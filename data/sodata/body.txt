"I have a DataFrame from Pandas:\n\nimport pandas as pd\ninp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\ndf = pd.DataFrame(inp)\nprint df\n\n\nOutput:\n\n   c1   c2\n0  10  100\n1  11  110\n2  12  120\n\n\nNow I want to iterate over the rows of this frame. For every row I want to be able to access its elements (values in cells) by the name of the columns. For example:\n\nfor row in df.rows:\n   print row['c1'], row['c2']\n\n\nIs it possible to do that in Pandas?\n\nI found this similar question. But it does not give me the answer I need. For example, it is suggested there to use:\n\nfor date, row in df.T.iteritems():\n\n\nor\n\nfor row in df.iterrows():\n\n\nBut I do not understand what the row object is and how I can work with it.\n"
"How can I select rows from a DataFrame based on values in some column in Pandas?\nIn SQL, I would use:\nSELECT *\nFROM table\nWHERE colume_name = some_value\n\nI tried to look at Pandas' documentation, but I did not immediately find the answer.\n"
"I have a DataFrame using Pandas and column labels that I need to edit to replace the original column labels.\nI'd like to change the column names in a DataFrame A where the original column names are:\n['$a', '$b', '$c', '$d', '$e']\n\nto\n['a', 'b', 'c', 'd', 'e'].\n\nI have the edited column names stored it in a list, but I don't know how to replace the column names.\n"
"When deleting a column in a DataFrame I use:\n\ndel df['column_name']\n\n\nAnd this works great. Why can't I use the following?\n\ndel df.column_name\n\n\nSince it is possible to access the column/Series as df.column_name, I expected this to work.\n"
"I have data in different columns, but I don't know how to extract it to save it in another variable.\nindex  a   b   c\n1      2   3   4\n2      3   4   5\n\nHow do I select 'a', 'b' and save it in to df1?\nI tried\ndf1 = df['a':'b']\ndf1 = df.ix[:, 'a':'b']\n\nNone seem to work.\n"
"I want to get a list of the column headers from a pandas DataFrame.  The DataFrame will come from user input so I won't know how many columns there will be or what they will be called.\nFor example, if I'm given a DataFrame like this:\n&gt;&gt;&gt; my_dataframe\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n\nI would get a list like this:\n&gt;&gt;&gt; header_list\n['y', 'gdp', 'cap']\n\n"
"I have the following indexed DataFrame with named columns and rows not- continuous numbers:\n\n          a         b         c         d\n2  0.671399  0.101208 -0.181532  0.241273\n3  0.446172 -0.243316  0.051767  1.577318\n5  0.614758  0.075793 -0.451460 -0.012493\n\n\nI would like to add a new column, 'e', to the existing data frame and do not want to change anything in the data frame (i.e., the new column always has the same length as the DataFrame). \n\n0   -0.335485\n1   -1.166658\n2   -0.385571\ndtype: float64\n\n\nHow can I add column e to the above example? \n"
'I have tried to puzzle out an answer to this question for many months while learning pandas.  I use SAS for my day-to-day work and it is great for it\'s out-of-core support.  However, SAS is horrible as a piece of software for numerous other reasons.\n\nOne day I hope to replace my use of SAS with python and pandas, but I currently lack an out-of-core workflow for large datasets.  I\'m not talking about "big data" that requires a distributed network, but rather files too large to fit in memory but small enough to fit on a hard-drive.\n\nMy first thought is to use HDFStore to hold large datasets on disk and pull only the pieces I need into dataframes for analysis.  Others have mentioned MongoDB as an easier to use alternative.  My question is this:\n\nWhat are some best-practice workflows for accomplishing the following:\n\n\nLoading flat files into a permanent, on-disk database structure\nQuerying that database to retrieve data to feed into a pandas data structure\nUpdating the database after manipulating pieces in pandas\n\n\nReal-world examples would be much appreciated, especially from anyone who uses pandas on "large data".\n\nEdit -- an example of how I would like this to work:\n\n\nIteratively import a large flat-file and store it in a permanent, on-disk database structure.  These files are typically too large to fit in memory.\nIn order to use Pandas, I would like to read subsets of this data (usually just a few columns at a time) that can fit in memory.\nI would create new columns by performing various operations on the selected columns.\nI would then have to append these new columns into the database structure.\n\n\nI am trying to find a best-practice way of performing these steps. Reading links about pandas and pytables it seems that appending a new column could be a problem.\n\nEdit -- Responding to Jeff\'s questions specifically:\n\n\nI am building consumer credit risk models. The kinds of data include phone, SSN and address characteristics; property values; derogatory information like criminal records, bankruptcies, etc... The datasets I use every day have nearly 1,000 to 2,000 fields on average of mixed data types: continuous, nominal and ordinal variables of both numeric and character data.  I rarely append rows, but I do perform many operations that create new columns.\nTypical operations involve combining several columns using conditional logic into a new, compound column. For example, if var1 &gt; 2 then newvar = \'A\' elif var2 = 4 then newvar = \'B\'.  The result of these operations is a new column for every record in my dataset.\nFinally, I would like to append these new columns into the on-disk data structure.  I would repeat step 2, exploring the data with crosstabs and descriptive statistics trying to find interesting, intuitive relationships to model.\nA typical project file is usually about 1GB.  Files are organized into such a manner where a row consists of a record of consumer data.  Each row has the same number of columns for every record.  This will always be the case.\nIt\'s pretty rare that I would subset by rows when creating a new column.  However, it\'s pretty common for me to subset on rows when creating reports or generating descriptive statistics.  For example, I might want to create a simple frequency for a specific line of business, say Retail credit cards.  To do this, I would select only those records where the line of business = retail in addition to whichever columns I want to report on.  When creating new columns, however, I would pull all rows of data and only the columns I need for the operations.\nThe modeling process requires that I analyze every column, look for interesting relationships with some outcome variable, and create new compound columns that describe those relationships.  The columns that I explore are usually done in small sets.  For example, I will focus on a set of say 20 columns just dealing with property values and observe how they relate to defaulting on a loan.  Once those are explored and new columns are created, I then move on to another group of columns, say college education, and repeat the process.  What I\'m doing is creating candidate variables that explain the relationship between my data and some outcome.  At the very end of this process, I apply some learning techniques that create an equation out of those compound columns.\n\n\nIt is rare that I would ever add rows to the dataset.  I will nearly always be creating new columns (variables or features in statistics/machine learning parlance).\n'
"I understand that pandas is designed to load fully populated DataFrame but I need to create an empty DataFrame then add rows, one by one.\nWhat is the best way to do this ?\n\nI successfully created an empty DataFrame with :\n\nres = DataFrame(columns=('lib', 'qty1', 'qty2'))\n\n\nThen I can add a new row and fill a field with :\n\nres = res.set_value(len(res), 'qty1', 10.0)\n\n\nIt works but seems very odd :-/ (it fails for adding string value)\n\nHow can I add a new row to my DataFrame (with different columns type) ?\n"
"I want to convert a table, represented as a list of lists, into a Pandas DataFrame. As an extremely simplified example:\n\na = [['a', '1.2', '4.2'], ['b', '70', '0.03'], ['x', '5', '0']]\ndf = pd.DataFrame(a)\n\n\nWhat is the best way to convert the columns to the appropriate types, in this case columns 2 and 3 into floats? Is there a way to specify the types while converting to DataFrame? Or is it better to create the DataFrame first and then loop through the columns to change the type for each column? Ideally I would like to do this in a dynamic way because there can be hundreds of columns and I don't want to specify exactly which columns are of which type. All I can guarantee is that each columns contains values of the same type.\n"
"Lets say I have the following pandas dataframe:\n\ndf = DataFrame({'A' : [5,6,3,4], 'B' : [1,2,3, 5]})\ndf\n\n     A   B\n0    5   1\n1    6   2\n2    3   3\n3    4   5\n\n\nI can subset based on a specific value:\n\nx = df[df['A'] == 3]\nx\n\n     A   B\n2    3   3\n\n\nBut how can I subset based on a list of values? - something like this:\n\nlist_of_values = [3,6]\n\ny = df[df['A'] in list_of_values]\n\n"
"I have a dataframe in pandas which I would like to write to a CSV file. I am doing this using:\n\ndf.to_csv('out.csv')\n\n\nAnd getting the error:\n\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u03b1' in position 20: ordinal not in range(128)\n\n\nIs there any way to get around this easily (i.e. I have unicode characters in my data frame)? And is there a way to write to a tab delimited file instead of a CSV using e.g. a 'to-tab' method (that I dont think exists)?\n"
"Background\nI just upgraded my Pandas from 0.11 to 0.13.0rc1. Now, the application is popping out many new warnings. One of them like this:\nE:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n\nI want to know what exactly it means?  Do I need to change something?\nHow should I suspend the warning if I insist to use quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE?\nThe function that gives errors\ndef _decode_stock_quote(list_of_150_stk_str):\n    &quot;&quot;&quot;decode the webpage and return dataframe&quot;&quot;&quot;\n\n    from cStringIO import StringIO\n\n    str_of_all = &quot;&quot;.join(list_of_150_stk_str)\n\n    quote_df = pd.read_csv(StringIO(str_of_all), sep=',', names=list('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefg')) #dtype={'A': object, 'B': object, 'C': np.float64}\n    quote_df.rename(columns={'A':'STK', 'B':'TOpen', 'C':'TPCLOSE', 'D':'TPrice', 'E':'THigh', 'F':'TLow', 'I':'TVol', 'J':'TAmt', 'e':'TDate', 'f':'TTime'}, inplace=True)\n    quote_df = quote_df.ix[:,[0,3,2,1,4,5,8,9,30,31]]\n    quote_df['TClose'] = quote_df['TPrice']\n    quote_df['RT']     = 100 * (quote_df['TPrice']/quote_df['TPCLOSE'] - 1)\n    quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\n    quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\n    quote_df['STK_ID'] = quote_df['STK'].str.slice(13,19)\n    quote_df['STK_Name'] = quote_df['STK'].str.slice(21,30)#.decode('gb2312')\n    quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n    \n    return quote_df\n\nMore error messages\nE:\\FinReporter\\FM_EXT.py:449: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TVol']   = quote_df['TVol']/TVOL_SCALE\nE:\\FinReporter\\FM_EXT.py:450: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TAmt']   = quote_df['TAmt']/TAMT_SCALE\nE:\\FinReporter\\FM_EXT.py:453: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n  quote_df['TDate']  = quote_df.TDate.map(lambda x: x[0:4]+x[5:7]+x[8:10])\n\n"
'I have a list of dictionaries like this:\n\n[{\'points\': 50, \'time\': \'5:00\', \'year\': 2010}, \n{\'points\': 25, \'time\': \'6:00\', \'month\': "february"}, \n{\'points\':90, \'time\': \'9:00\', \'month\': \'january\'}, \n{\'points_h1\':20, \'month\': \'june\'}]\n\n\nAnd I want to turn this into a pandas DataFrame like this:\n\n      month  points  points_h1  time  year\n0       NaN      50        NaN  5:00  2010\n1  february      25        NaN  6:00   NaN\n2   january      90        NaN  9:00   NaN\n3      june     NaN         20   NaN   NaN\n\n\nNote: Order of the columns does not matter.\n\nHow can I turn the list of dictionaries into a pandas DataFrame as shown above?\n'
'I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.\n\nIs there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.\n'
'Is there a way to widen the display of output in either interactive or script-execution mode?\n\nSpecifically, I am using the describe() function on a pandas DataFrame.  When the DataFrame is 5 columns (labels) wide, I get the descriptive statistics that I want.  However, if the DataFrame has any more columns, the statistics are suppressed and something like this is returned:\n\n&gt;&gt; Index: 8 entries, count to max  \n&gt;&gt; Data columns:  \n&gt;&gt; x1          8  non-null values  \n&gt;&gt; x2          8  non-null values  \n&gt;&gt; x3          8  non-null values  \n&gt;&gt; x4          8  non-null values  \n&gt;&gt; x5          8  non-null values  \n&gt;&gt; x6          8  non-null values  \n&gt;&gt; x7          8  non-null values  \n\n\nThe "8" value is given whether there are 6 or 7 columns.  What does the "8" refer to?\n\nI have already tried dragging the IDLE window larger, as well as increasing the "Configure IDLE" width options, to no avail.\n\nMy purpose in using pandas and describe() is to avoid using a second program like Stata to do basic data manipulation and investigation.\n'
"Can someone explain how these two methods of slicing are different?\nI've seen the docs,\nand I've seen these answers, but I still find myself unable to understand how the three are different. To me, they seem interchangeable in large part, because they are at the lower levels of slicing.\nFor example, say we want to get the first five rows of a DataFrame.  How is it that these two work?\ndf.loc[:5]\ndf.iloc[:5]\n\nCan someone present three cases where the distinction in uses are clearer?\n\nOnce upon a time, I also wanted to know how these two functions differ from df.ix[:5] but ix has been removed from pandas 1.0, so I don't care anymore.\n"
"I have the following DataFrame:\n\n             daysago  line_race rating        rw    wrating\n line_date                                                 \n 2007-03-31       62         11     56  1.000000  56.000000\n 2007-03-10       83         11     67  1.000000  67.000000\n 2007-02-10      111          9     66  1.000000  66.000000\n 2007-01-13      139         10     83  0.880678  73.096278\n 2006-12-23      160         10     88  0.793033  69.786942\n 2006-11-09      204          9     52  0.636655  33.106077\n 2006-10-22      222          8     66  0.581946  38.408408\n 2006-09-29      245          9     70  0.518825  36.317752\n 2006-09-16      258         11     68  0.486226  33.063381\n 2006-08-30      275          8     72  0.446667  32.160051\n 2006-02-11      475          5     65  0.164591  10.698423\n 2006-01-13      504          0     70  0.142409   9.968634\n 2006-01-02      515          0     64  0.134800   8.627219\n 2005-12-06      542          0     70  0.117803   8.246238\n 2005-11-29      549          0     70  0.113758   7.963072\n 2005-11-22      556          0     -1  0.109852  -0.109852\n 2005-11-01      577          0     -1  0.098919  -0.098919\n 2005-10-20      589          0     -1  0.093168  -0.093168\n 2005-09-27      612          0     -1  0.083063  -0.083063\n 2005-09-07      632          0     -1  0.075171  -0.075171\n 2005-06-12      719          0     69  0.048690   3.359623\n 2005-05-29      733          0     -1  0.045404  -0.045404\n 2005-05-02      760          0     -1  0.039679  -0.039679\n 2005-04-02      790          0     -1  0.034160  -0.034160\n 2005-03-13      810          0     -1  0.030915  -0.030915\n 2004-11-09      934          0     -1  0.016647  -0.016647\n\n\nI need to remove the rows where line_race is equal to 0. What's the most efficient way to do this?\n"
"I'm starting from the pandas DataFrame docs here: http://pandas.pydata.org/pandas-docs/stable/dsintro.html\n\nI'd like to iteratively fill the DataFrame with values in a time series kind of calculation.\nSo basically, I'd like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.\n\nI'd then add initial values and go over this data calculating the new row from the row before, say row[A][t] = row[A][t-1]+1 or so.\n\nI'm currently using the code as below, but I feel it's kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.\nNote: I'm using Python 2.7.\n\nimport datetime as dt\nimport pandas as pd\nimport scipy as s\n\nif __name__ == '__main__':\n    base = dt.datetime.today().date()\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\n    dates.sort()\n\n    valdict = {}\n    symbols = ['A','B', 'C']\n    for symb in symbols:\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\n\n    for thedate in dates:\n        if thedate &gt; dates[0]:\n            for symb in valdict:\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\n\n    print valdict\n\n"
"I've created a Pandas DataFrame\n\ndf = DataFrame(index=['A','B','C'], columns=['x','y'])\n\n\nand got this\n\n\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  NaN  NaN\n\n\n\nThen I want to assign value to particular cell, for example for row 'C' and column 'x'.\nI've expected to get such result:\n\n\n    x    y\nA  NaN  NaN\nB  NaN  NaN\nC  10  NaN\n\n\nwith this code:\n\ndf.xs('C')['x'] = 10\n\n\nbut contents of df haven't changed. It's again only NaNs in DataFrame. \n\nAny suggestions?\n"
"This seems rather obvious, but I can't seem to figure out how to convert an index of data frame to a column?\n\nFor example:\n\ndf=\n        gi       ptt_loc\n 0  384444683      593  \n 1  384444684      594 \n 2  384444686      596  \n\n\nTo,\n\ndf=\n    index1    gi       ptt_loc\n 0  0     384444683      593  \n 1  1     384444684      594 \n 2  2     384444686      596  \n\n"
'I\'m starting with input data like this\n\ndf1 = pandas.DataFrame( { \n    "Name" : ["Alice", "Bob", "Mallory", "Mallory", "Bob" , "Mallory"] , \n    "City" : ["Seattle", "Seattle", "Portland", "Seattle", "Seattle", "Portland"] } )\n\n\nWhich when printed appears as this:\n\n   City     Name\n0   Seattle    Alice\n1   Seattle      Bob\n2  Portland  Mallory\n3   Seattle  Mallory\n4   Seattle      Bob\n5  Portland  Mallory\n\n\nGrouping is simple enough:\n\ng1 = df1.groupby( [ "Name", "City"] ).count()\n\n\nand printing yields a GroupBy object:\n\n                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\n        Seattle      1     1\n\n\nBut what I want eventually is another DataFrame object that contains all the rows in the GroupBy object. In other words I want to get the following result:\n\n                  City  Name\nName    City\nAlice   Seattle      1     1\nBob     Seattle      2     2\nMallory Portland     2     2\nMallory Seattle      1     1\n\n\nI can\'t quite see how to accomplish this in the pandas documentation. Any hints would be welcome.\n'
"How can I achieve the equivalents of SQL's IN and NOT IN?\nI have a list with the required values.\nHere's the scenario:\ndf = pd.DataFrame({'country': ['US', 'UK', 'Germany', 'China']})\ncountries_to_keep = ['UK', 'China']\n\n# pseudo-code:\ndf[df['country'] not in countries_to_keep]\n\nMy current way of doing this is as follows:\ndf = pd.DataFrame({'country': ['US', 'UK', 'Germany', 'China']})\ndf2 = pd.DataFrame({'country': ['UK', 'China'], 'matched': True})\n\n# IN\ndf.merge(df2, how='inner', on='country')\n\n# NOT IN\nnot_in = df.merge(df2, how='left', on='country')\nnot_in = not_in[pd.isnull(not_in['matched'])]\n\nBut this seems like a horrible kludge. Can anyone improve on it?\n"
"I have the following DataFrame:\n\n    Col1  Col2  Col3  Type\n0      1     2     3     1\n1      4     5     6     1\n...\n20     7     8     9     2\n21    10    11    12     2\n...\n45    13    14    15     3\n46    16    17    18     3\n...\n\n\nThe DataFrame is read from a csv file. All rows which have Type 1 are on top, followed by the rows with Type 2, followed by the rows with Type 3, etc.\n\nI would like to shuffle the order of the DataFrame's rows, so that all Type's are mixed. A possible result could be:\n\n    Col1  Col2  Col3  Type\n0      7     8     9     2\n1     13    14    15     3\n...\n20     1     2     3     1\n21    10    11    12     2\n...\n45     4     5     6     1\n46    16    17    18     3\n...\n\n\nHow can I achieve this?\n"
"I have a data frame df and I use several columns from it to groupby:\n\ndf['col1','col2','col3','col4'].groupby(['col1','col2']).mean()\n\n\nIn the above way I almost get the table (data frame) that I need. What is missing is an additional column that contains number of rows in each group. In other words, I have mean but I also would like to know how many number were used to get these means. For example in the first group there are 8 values and in the second one 10 and so on.\n\nIn short: How do I get group-wise statistics for a dataframe?\n"
'I have a Pandas Dataframe as below:\n      itm Date                  Amount \n67    420 2012-09-30 00:00:00   65211\n68    421 2012-09-09 00:00:00   29424\n69    421 2012-09-16 00:00:00   29877\n70    421 2012-09-23 00:00:00   30990\n71    421 2012-09-30 00:00:00   61303\n72    485 2012-09-09 00:00:00   71781\n73    485 2012-09-16 00:00:00     NaN\n74    485 2012-09-23 00:00:00   11072\n75    485 2012-09-30 00:00:00  113702\n76    489 2012-09-09 00:00:00   64731\n77    489 2012-09-16 00:00:00     NaN\n\nWhen I try to apply a function to the Amount column, I get the following error:\nValueError: cannot convert float NaN to integer\n\nI have tried applying a function using .isnan from the Math Module\nI have tried the pandas .replace attribute\nI tried the .sparse data attribute from pandas 0.9\nI have also tried if NaN == NaN statement in a function.\nI have also looked at this article How do I replace NA values with zeros in an R dataframe? whilst looking at some other articles.\nAll the methods I have tried have not worked or do not recognise NaN.\nAny Hints or solutions would be appreciated.\n'
'Can you tell me when to use these vectorization methods with basic examples? \n\nI see that map is a Series method whereas the rest are DataFrame methods. I got confused about apply and applymap methods though. Why do we have two methods for applying a function to a DataFrame? Again, simple examples which illustrate the usage would be great!\n'
"\nHow to perform a (INNER| (LEFT|RIGHT|FULL) OUTER) JOIN with pandas?\nHow do I add NaNs for missing rows after merge?\nHow do I get rid of NaNs after merging?\nCan I merge on the index?\nHow do I merge multiple DataFrames?\nCross join with pandas?\nmerge? join? concat? update? Who? What? Why?!\n\n... and more. I've seen these recurring questions asking about various facets of the pandas merge functionality. Most of the information regarding merge and its various use cases today is fragmented across dozens of badly worded, unsearchable posts. The aim here is to collate some of the more important points for posterity.\nThis QnA is meant to be the next installment in a series of helpful  user-guides on common pandas idioms (see this post on pivoting, and this post on concatenation, which I will be touching on, later).\nPlease note that this post is not meant to be a replacement for the documentation, so please read that as well! Some of the examples are taken from there.\n\n\nTable of Contents\nFor ease of access.\n\nMerging basics - basic types of joins (read this first)\n\nIndex-based joins\n\nGeneralizing to multiple DataFrames\n\nCross join\n\n\n"
'I would like to read several csv files from a directory into pandas and concatenate them into one big DataFrame. I have not been able to figure it out though. Here is what I have so far:\n\nimport glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\'\nfilenames = glob.glob(path + "/*.csv")\n\ndfs = []\nfor filename in filenames:\n    dfs.append(pd.read_csv(filename))\n\n# Concatenate all data into one DataFrame\nbig_frame = pd.concat(dfs, ignore_index=True)\n\n\nI guess I need some help within the for loop???\n'
"Having issue filtering my result dataframe with an or condition. I want my result df to extract all column var values that are above 0.25 and below -0.25.\nThis logic below gives me an ambiguous truth value however it work when I split this filtering in two separate operations. What is happening here? not sure where to use the suggested a.empty(), a.bool(), a.item(),a.any() or a.all().\nresult = result[(result['var']&gt;0.25) or (result['var']&lt;-0.25)]\n\n"
"This may be a simple question, but I can not figure out how to do this. Lets say that I have two variables as follows.\n\na = 2\nb = 3\n\n\nI want to construct a DataFrame from this:\n\ndf2 = pd.DataFrame({'A':a,'B':b})\n\n\nThis generates an error:  \n\n\n  ValueError: If using all scalar values, you must pass an index\n\n\nI tried this also:\n\ndf2 = (pd.DataFrame({'a':a,'b':b})).reset_index()\n\n\nThis gives the same error message.\n"
"I have constructed a condition that extract exactly one row from my data frame:\n\nd2 = df[(df['l_ext']==l_ext) &amp; (df['item']==item) &amp; (df['wn']==wn) &amp; (df['wd']==1)]\n\n\nNow I would like to take a value from a particular column:\n\nval = d2['col_name']\n\n\nBut as a result I get a data frame that contains one row and one column (i.e. one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?\n"
'I am curious as to why df[2] is not supported, while df.ix[2] and df[2:3] both work. \n\nIn [26]: df.ix[2]\nOut[26]: \nA    1.027680\nB    1.514210\nC   -1.466963\nD   -0.162339\nName: 2000-01-03 00:00:00\n\nIn [27]: df[2:3]\nOut[27]: \n                  A        B         C         D\n2000-01-03  1.02768  1.51421 -1.466963 -0.162339\n\n\nI would expect df[2] to work the same way as df[2:3] to be consistent with Python indexing convention. Is there a design reason for not supporting indexing row by single integer?\n'
"\nWhat is pivot?\nHow do I pivot?\nIs this a pivot?\nLong format to wide format?\n\nI've seen a lot of questions that ask about pivot tables.  Even if they don't know that they are asking about pivot tables, they usually are.  It is virtually impossible to write a canonical question and answer that encompasses all aspects of pivoting...\n... But I'm going to give it a go.\n\nThe problem with existing questions and answers is that often the question is focused on a nuance that the OP has trouble generalizing in order to use a number of the existing good answers.  However, none of the answers attempt to give a comprehensive explanation (because it's a daunting task)\nLook a few examples from my google search\n\nHow to pivot a dataframe in Pandas?\n\n\nGood question and answer.  But the answer only answers the specific question with little explanation.\n\n\npandas pivot table to data frame\n\n\nIn this question, the OP is concerned with the output of the pivot.  Namely how the columns look.  OP wanted it to look like R.  This isn't very helpful for pandas users.\n\n\npandas pivoting a dataframe, duplicate rows\n\n\nAnother decent question but the answer focuses on one method, namely pd.DataFrame.pivot\n\nSo whenever someone searches for pivot they get sporadic results that are likely not going to answer their specific question.\n\nSetup\nYou may notice that I conspicuously named my columns and relevant column values to correspond with how I'm going to pivot in the answers below.\nimport numpy as np\nimport pandas as pd\nfrom numpy.core.defchararray import add\n\nnp.random.seed([3,1415])\nn = 20\n\ncols = np.array(['key', 'row', 'item', 'col'])\narr1 = (np.random.randint(5, size=(n, 4)) // [2, 1, 2, 1]).astype(str)\n\ndf = pd.DataFrame(\n    add(cols, arr1), columns=cols\n).join(\n    pd.DataFrame(np.random.rand(n, 2).round(2)).add_prefix('val')\n)\nprint(df)\n\n     key   row   item   col  val0  val1\n0   key0  row3  item1  col3  0.81  0.04\n1   key1  row2  item1  col2  0.44  0.07\n2   key1  row0  item1  col0  0.77  0.01\n3   key0  row4  item0  col2  0.15  0.59\n4   key1  row0  item2  col1  0.81  0.64\n5   key1  row2  item2  col4  0.13  0.88\n6   key2  row4  item1  col3  0.88  0.39\n7   key1  row4  item1  col1  0.10  0.07\n8   key1  row0  item2  col4  0.65  0.02\n9   key1  row2  item0  col2  0.35  0.61\n10  key2  row0  item2  col1  0.40  0.85\n11  key2  row4  item1  col2  0.64  0.25\n12  key0  row2  item2  col3  0.50  0.44\n13  key0  row4  item1  col4  0.24  0.46\n14  key1  row3  item2  col3  0.28  0.11\n15  key0  row3  item1  col1  0.31  0.23\n16  key0  row0  item2  col3  0.86  0.01\n17  key0  row4  item0  col3  0.64  0.21\n18  key2  row2  item2  col0  0.13  0.45\n19  key0  row2  item0  col4  0.37  0.70\n\nQuestion(s)\n\nWhy do I get ValueError: Index contains duplicate entries, cannot reshape\n\nHow do I pivot df such that the col values are columns, row values are the index, and mean of val0 are the values?\n col   col0   col1   col2   col3  col4\n row                                  \n row0  0.77  0.605    NaN  0.860  0.65\n row2  0.13    NaN  0.395  0.500  0.25\n row3   NaN  0.310    NaN  0.545   NaN\n row4   NaN  0.100  0.395  0.760  0.24\n\n\nHow do I pivot df such that the col values are columns, row values are the index, mean of val0 are the values, and missing values are 0?\n col   col0   col1   col2   col3  col4\n row                                  \n row0  0.77  0.605  0.000  0.860  0.65\n row2  0.13  0.000  0.395  0.500  0.25\n row3  0.00  0.310  0.000  0.545  0.00\n row4  0.00  0.100  0.395  0.760  0.24\n\n\nCan I get something other than mean, like maybe sum?\n col   col0  col1  col2  col3  col4\n row                               \n row0  0.77  1.21  0.00  0.86  0.65\n row2  0.13  0.00  0.79  0.50  0.50\n row3  0.00  0.31  0.00  1.09  0.00\n row4  0.00  0.10  0.79  1.52  0.24\n\n\nCan I do more that one aggregation at a time?\n        sum                          mean                           \n col   col0  col1  col2  col3  col4  col0   col1   col2   col3  col4\n row                                                                \n row0  0.77  1.21  0.00  0.86  0.65  0.77  0.605  0.000  0.860  0.65\n row2  0.13  0.00  0.79  0.50  0.50  0.13  0.000  0.395  0.500  0.25\n row3  0.00  0.31  0.00  1.09  0.00  0.00  0.310  0.000  0.545  0.00\n row4  0.00  0.10  0.79  1.52  0.24  0.00  0.100  0.395  0.760  0.24\n\n\nCan I aggregate over multiple value columns?\n       val0                             val1                          \n col   col0   col1   col2   col3  col4  col0   col1  col2   col3  col4\n row                                                                  \n row0  0.77  0.605  0.000  0.860  0.65  0.01  0.745  0.00  0.010  0.02\n row2  0.13  0.000  0.395  0.500  0.25  0.45  0.000  0.34  0.440  0.79\n row3  0.00  0.310  0.000  0.545  0.00  0.00  0.230  0.00  0.075  0.00\n row4  0.00  0.100  0.395  0.760  0.24  0.00  0.070  0.42  0.300  0.46\n\n\nCan Subdivide by multiple columns?\n item item0             item1                         item2                   \n col   col2  col3  col4  col0  col1  col2  col3  col4  col0   col1  col3  col4\n row                                                                          \n row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.605  0.86  0.65\n row2  0.35  0.00  0.37  0.00  0.00  0.44  0.00  0.00  0.13  0.000  0.50  0.13\n row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.000  0.28  0.00\n row4  0.15  0.64  0.00  0.00  0.10  0.64  0.88  0.24  0.00  0.000  0.00  0.00\n\n\nOr\n item      item0             item1                         item2                  \n col        col2  col3  col4  col0  col1  col2  col3  col4  col0  col1  col3  col4\n key  row                                                                         \n key0 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.86  0.00\n      row2  0.00  0.00  0.37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50  0.00\n      row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.00  0.00  0.00\n      row4  0.15  0.64  0.00  0.00  0.00  0.00  0.00  0.24  0.00  0.00  0.00  0.00\n key1 row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.81  0.00  0.65\n      row2  0.35  0.00  0.00  0.00  0.00  0.44  0.00  0.00  0.00  0.00  0.00  0.13\n      row3  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.28  0.00\n      row4  0.00  0.00  0.00  0.00  0.10  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n key2 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.00  0.00\n      row2  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.00  0.00\n      row4  0.00  0.00  0.00  0.00  0.00  0.64  0.88  0.00  0.00  0.00  0.00  0.00\n\n\nCan I aggregate the frequency in which the column and rows occur together, aka &quot;cross tabulation&quot;?\n col   col0  col1  col2  col3  col4\n row                               \n row0     1     2     0     1     1\n row2     1     0     2     1     2\n row3     0     1     0     2     0\n row4     0     1     2     2     1\n\n\nHow do I convert a DataFrame from long to wide by pivoting on ONLY two columns? Given,\nnp.random.seed([3, 1415])\ndf2 = pd.DataFrame({'A': list('aaaabbbc'), 'B': np.random.choice(15, 8)})        \ndf2        \n   A   B\n0  a   0\n1  a  11\n2  a   2\n3  a  11\n4  b  10\n5  b  10\n6  b  14\n7  c   7\n\nThe expected should look something like\n      a     b    c\n0   0.0  10.0  7.0\n1  11.0  10.0  NaN\n2   2.0  14.0  NaN\n3  11.0   NaN  NaN\n\n\nHow do I flatten the multiple index to single index after pivot\nFrom\n   1  2   \n   1  1  2        \na  2  1  1\nb  2  1  0\nc  1  0  0\n\nTo\n   1|1  2|1  2|2               \na    2    1    1\nb    2    1    0\nc    1    0    0\n\n\n\n"
"I'm trying to use pandas to manipulate a .csv file but I get this error:\n\n\n  pandas.parser.CParserError: Error tokenizing data. C error: Expected 2 fields in line 3,  saw 12\n\n\nI have tried to read the pandas docs, but found nothing.\n\nMy code is simple:\n\npath = 'GOOG Key Ratios.csv'\n#print(open(path).read())\ndata = pd.read_csv(path)\n\n\nHow can I resolve this? Should I use the csv module or another language ?\n\nFile is from Morningstar\n"
'I have a dictionary which looks like this: di = {1: "A", 2: "B"}\n\nI would like to apply it to the "col1" column of a dataframe similar to:\n\n     col1   col2\n0       w      a\n1       1      2\n2       2    NaN\n\n\nto get:\n\n     col1   col2\n0       w      a\n1       A      2\n2       B    NaN\n\n\nHow can I best do this? For some reason googling terms relating to this only shows me links about how to make columns from dicts and vice-versa :-/ \n'
"I have a dataframe from which I remove some rows. As a result, I get a dataframe in which index is something like that: [1,5,6,10,11] and I would like to reset it to [0,1,2,3,4]. How can I do it?\n\n\n\nThe following seems to work:\n\ndf = df.reset_index()\ndel df['index']\n\n\nThe following does not work:\n\ndf = df.reindex()\n\n"
'I want to apply my custom function (it uses an if-else ladder) to these six columns (ERI_Hispanic, ERI_AmerInd_AKNatv, ERI_Asian, ERI_Black_Afr.Amer, ERI_HI_PacIsl, ERI_White) in each row of my dataframe.\n\nI\'ve tried different methods from other questions but still can\'t seem to find the right answer for my problem.  The critical piece of this is that if the person is counted as Hispanic they can\'t be counted as anything else.  Even if they have a "1" in another ethnicity column they still are counted as Hispanic not two or more races.  Similarly, if the sum of all the ERI columns is greater than 1 they are counted as two or more races and can\'t be counted as a unique ethnicity(except for Hispanic).  Hopefully this makes sense.  Any help will be greatly appreciated. \n\nIts almost like doing a for loop through each row and if each record meets a criterion they are added to one list and eliminated from the original.  \n\nFrom the dataframe below I need to calculate a new column based on the following spec in SQL:\n\n=========================  CRITERIA  ===============================\n\nIF [ERI_Hispanic] = 1 THEN RETURN “Hispanic”\nELSE IF SUM([ERI_AmerInd_AKNatv] + [ERI_Asian] + [ERI_Black_Afr.Amer] + [ERI_HI_PacIsl] + [ERI_White]) &gt; 1 THEN RETURN “Two or More”\nELSE IF [ERI_AmerInd_AKNatv] = 1 THEN RETURN “A/I AK Native”\nELSE IF [ERI_Asian] = 1 THEN RETURN “Asian”\nELSE IF [ERI_Black_Afr.Amer] = 1 THEN RETURN “Black/AA”\nELSE IF [ERI_HI_PacIsl] = 1 THEN RETURN “Haw/Pac Isl.”\nELSE IF [ERI_White] = 1 THEN RETURN “White”\n\n\nComment: If the ERI Flag for Hispanic is True (1), the employee is classified as “Hispanic”\n\nComment: If more than 1 non-Hispanic ERI Flag is true, return “Two or More”\n\n======================  DATAFRAME ===========================\n\n     lname          fname       rno_cd  eri_afr_amer    eri_asian   eri_hawaiian    eri_hispanic    eri_nat_amer    eri_white   rno_defined\n0    MOST           JEFF        E       0               0           0               0               0               1           White\n1    CRUISE         TOM         E       0               0           0               1               0               0           White\n2    DEPP           JOHNNY              0               0           0               0               0               1           Unknown\n3    DICAP          LEO                 0               0           0               0               0               1           Unknown\n4    BRANDO         MARLON      E       0               0           0               0               0               0           White\n5    HANKS          TOM         0                       0           0               0               0               1           Unknown\n6    DENIRO         ROBERT      E       0               1           0               0               0               1           White\n7    PACINO         AL          E       0               0           0               0               0               1           White\n8    WILLIAMS       ROBIN       E       0               0           1               0               0               0           White\n9    EASTWOOD       CLINT       E       0               0           0               0               0               1           White\n\n'
"When calling\n\ndf = pd.read_csv('somefile.csv')\n\n\nI get:\n\n\n  /Users/josh/anaconda/envs/py27/lib/python2.7/site-packages/pandas/io/parsers.py:1130:\n  DtypeWarning: Columns (4,5,7,16) have mixed types.  Specify dtype\n  option on import or set low_memory=False.\n\n\nWhy is the dtype option related to low_memory, and why would making it False help with this problem?\n"
"I have a pandas DataFrame and I want to delete rows from it where the length of the string in a particular column is greater than 2.\n\nI expect to be able to do this (per this answer):\n\ndf[(len(df['column name']) &lt; 2)]\n\n\nbut I just get the error:\n\nKeyError: u'no item named False'\n\n\nWhat am I doing wrong?\n\n(Note: I know I can use df.dropna() to get rid of rows that contain any NaN, but I didn't see how to remove rows based on a conditional expression.)\n"
"I have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):\n\n     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       \n                                     sum   sum   sum    sum   amax   amin\n0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94\n\n\nI want to flatten it, so that it looks like this (names aren't critical - I could rename):\n\n     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   \n0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98\n1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98\n2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98\n3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92\n4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94\n\n\nHow do I do this? (I've tried a lot, to no avail.) \n\nPer a suggestion, here is the head in dict form\n\n{('USAF', ''): {0: '702730',\n  1: '702730',\n  2: '702730',\n  3: '702730',\n  4: '702730'},\n ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},\n ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},\n ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},\n ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},\n ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},\n ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},\n ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},\n ('tempf', 'amax'): {0: 30.920000000000002,\n  1: 32.0,\n  2: 23.0,\n  3: 10.039999999999999,\n  4: 19.939999999999998},\n ('tempf', 'amin'): {0: 24.98,\n  1: 24.98,\n  2: 6.9799999999999969,\n  3: 3.9199999999999982,\n  4: 10.940000000000001},\n ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}\n\n"
'I have a fairly large dataset in the form of a dataframe and I was wondering how I would be able to split the dataframe into two random samples (80% and 20%) for training and testing.\n\nThanks!\n'
'I would like to create views or dataframes from an existing dataframe based on column selections.\n\nFor example, I would like to create a dataframe df2 from a dataframe df1 that holds all columns from it except two of them. I tried doing the following, but it didn\'t work:\n\nimport numpy as np\nimport pandas as pd\n\n# Create a dataframe with columns A,B,C and D\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list(\'ABCD\'))\n\n# Try to create a second dataframe df2 from df with all columns except \'B\' and D\nmy_cols = set(df.columns)\nmy_cols.remove(\'B\').remove(\'D\')\n\n# This returns an error ("unhashable type: set")\ndf2 = df[my_cols]\n\n\nWhat am I doing wrong? Perhaps more generally, what mechanisms does pandas have to support the picking and exclusions of arbitrary sets of columns from a dataframe?\n'
"This seems like a ridiculously easy question... but I'm not seeing the easy answer I was expecting.\n\nSo, how do I get the value at an nth row of a given column in Pandas? (I am particularly interested in the first row, but would be interested in a more general practice as well).\n\nFor example, let's say I want to pull the 1.2 value in Btime as a variable. \n\nWhats the right way to do this?\n\ndf_test = \n\n  ATime   X   Y   Z   Btime  C   D   E\n0    1.2  2  15   2    1.2  12  25  12\n1    1.4  3  12   1    1.3  13  22  11\n2    1.5  1  10   6    1.4  11  20  16\n3    1.6  2   9  10    1.7  12  29  12\n4    1.9  1   1   9    1.9  11  21  19\n5    2.0  0   0   0    2.0   8  10  11\n6    2.4  0   0   0    2.4  10  12  15\n\n"
'How to check whether a pandas DataFrame is empty? In my case I want to print some message in terminal if the DataFrame is empty. \n'
"Right now I'm importing a fairly large CSV as a dataframe every time I run the script. Is there a good solution for keeping that dataframe constantly available in between runs so I don't have to spend all that time waiting for the script to run?\n"
"Most operations in pandas can be accomplished with operator chaining (groupby, aggregate, apply, etc), but the only way I've found to filter rows is via normal bracket indexing\n\ndf_filtered = df[df['column'] == value]\n\n\nThis is unappealing as it requires I assign df to a variable before being able to filter on its values.  Is there something more like the following?\n\ndf_filtered = df.mask(lambda x: x['column'] == value)\n\n"
"I have a dataframe along the lines of the below:\n    Type       Set\n1    A          Z\n2    B          Z           \n3    B          X\n4    C          Y\n\nI want to add another column to the dataframe (or generate a series) of the same length as the dataframe (equal number of records/rows) which sets a colour 'green' if Set == 'Z' and 'red' if Set equals anything else.\nWhat's the best way to do this?\n"
"I have a dataframe look like this:\n\nimport pandas\nimport numpy as np\ndf = DataFrame(np.random.rand(4,4), columns = list('abcd'))\ndf\n      a         b         c         d\n0  0.418762  0.042369  0.869203  0.972314\n1  0.991058  0.510228  0.594784  0.534366\n2  0.407472  0.259811  0.396664  0.894202\n3  0.726168  0.139531  0.324932  0.906575\n\n\nHow I can get all columns except column b?\n"
"I have a dataset\ncategory\ncat a\ncat b\ncat a\n\nI'd like to be able to return something like (showing unique values and frequency)\ncategory   freq \ncat a       2\ncat b       1\n\n"
'I have a pandas data frame df like:\n\na b\nA 1\nA 2\nB 5\nB 5\nB 4\nC 6\n\n\nI want to group by the first column and get second column as lists in rows:\n\nA [1,2]\nB [5,5,4]\nC [6]\n\n\nIs it possible to do something like this using pandas groupby?\n'
"I have a Python dictionary like the following:\n\n{u'2012-06-08': 388,\n u'2012-06-09': 388,\n u'2012-06-10': 388,\n u'2012-06-11': 389,\n u'2012-06-12': 389,\n u'2012-06-13': 389,\n u'2012-06-14': 389,\n u'2012-06-15': 389,\n u'2012-06-16': 389,\n u'2012-06-17': 389,\n u'2012-06-18': 390,\n u'2012-06-19': 390,\n u'2012-06-20': 390,\n u'2012-06-21': 390,\n u'2012-06-22': 390,\n u'2012-06-23': 390,\n u'2012-06-24': 390,\n u'2012-06-25': 391,\n u'2012-06-26': 391,\n u'2012-06-27': 391,\n u'2012-06-28': 391,\n u'2012-06-29': 391,\n u'2012-06-30': 391,\n u'2012-07-01': 391,\n u'2012-07-02': 392,\n u'2012-07-03': 392,\n u'2012-07-04': 392,\n u'2012-07-05': 392,\n u'2012-07-06': 392}\n\n\nThe keys are Unicode dates and the values are integers. I would like to convert this into a pandas dataframe by having the dates and their corresponding values as two separate columns. Example: col1: Dates col2: DateValue (the dates are still Unicode and datevalues are still integers)\n\n     Date         DateValue\n0    2012-07-01    391\n1    2012-07-02    392\n2    2012-07-03    392\n.    2012-07-04    392\n.    ...           ...\n.    ...           ...\n\n\nAny help in this direction would be much appreciated. I am unable to find resources on the pandas docs to help me with this.\n\nI know one solution might be to convert each key-value pair in this dict, into a dict so the entire structure becomes a dict of dicts, and then we can add each row individually to the dataframe. But I want to know if there is an easier way and a more direct way to do this.\n\nSo far I have tried converting the dict into a series object but this doesn't seem to maintain the relationship between the columns:\n\ns  = Series(my_dict,index=my_dict.keys())\n\n"
"Is there a way to check if a column exists in a Pandas DataFrame?\n\nSuppose that I have the following DataFrame:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from random import randint\n&gt;&gt;&gt; df = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                       'B': [randint(1, 9)*10 for x in xrange(10)],\n                       'C': [randint(1, 9)*100 for x in xrange(10)]})\n&gt;&gt;&gt; df\n   A   B    C\n0  3  40  100\n1  6  30  200\n2  7  70  800\n3  3  50  200\n4  7  50  400\n5  4  10  400\n6  3  70  500\n7  8  30  200\n8  3  40  800\n9  6  60  200\n\n\nand I want to calculate df['sum'] = df['A'] + df['C']\n\nBut first I want to check if df['A'] exists, and if not, I want to calculate df['sum'] = df['B'] + df['C'] instead.\n"
"I want to perform my own complex operations on financial data in dataframes in a sequential manner.\n\nFor example I am using the following MSFT CSV file taken from Yahoo Finance:\n\nDate,Open,High,Low,Close,Volume,Adj Close\n2011-10-19,27.37,27.47,27.01,27.13,42880000,27.13\n2011-10-18,26.94,27.40,26.80,27.31,52487900,27.31\n2011-10-17,27.11,27.42,26.85,26.98,39433400,26.98\n2011-10-14,27.31,27.50,27.02,27.27,50947700,27.27\n\n....\n\n\nI then do the following:\n\n#!/usr/bin/env python\nfrom pandas import *\n\ndf = read_csv('table.csv')\n\nfor i, row in enumerate(df.values):\n    date = df.index[i]\n    open, high, low, close, adjclose = row\n    #now perform analysis on open/close based on date, etc..\n\n\nIs that the most efficient way? Given the focus on speed in pandas, I would assume there must be some special function to iterate through the  values in a manner that one also retrieves the index (possibly through a generator to be memory efficient)? df.iteritems unfortunately only iterates column by column.\n"
"I have a dataframe with over 200 columns. The issue is as they were generated the order is\n\n['Q1.3','Q6.1','Q1.2','Q1.1',......]\n\n\nI need to re-order the columns as follows:\n\n['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]\n\n\nIs there some way for me to do this within Python?\n"
'How can I convert a DataFrame column of strings (in dd/mm/yyyy format) to datetimes?\n'
'I have a pandas data frame with two columns. I need to change the values of the first column without affecting the second one and get back the whole data frame with just first column values changed. How can I do that using apply in pandas?\n'
"How do I convert a numpy.datetime64 object to a datetime.datetime (or Timestamp)?\n\nIn the following code, I create a datetime, timestamp and datetime64 objects.\n\nimport datetime\nimport numpy as np\nimport pandas as pd\ndt = datetime.datetime(2012, 5, 1)\n# A strange way to extract a Timestamp object, there's surely a better way?\nts = pd.DatetimeIndex([dt])[0]\ndt64 = np.datetime64(dt)\n\nIn [7]: dt\nOut[7]: datetime.datetime(2012, 5, 1, 0, 0)\n\nIn [8]: ts\nOut[8]: &lt;Timestamp: 2012-05-01 00:00:00&gt;\n\nIn [9]: dt64\nOut[9]: numpy.datetime64('2012-05-01T01:00:00.000000+0100')\n\n\nNote: it's easy to get the datetime from the Timestamp:\n\nIn [10]: ts.to_datetime()\nOut[10]: datetime.datetime(2012, 5, 1, 0, 0)\n\n\nBut how do we extract the datetime or Timestamp from a numpy.datetime64 (dt64)?\n\n.\n\nUpdate: a somewhat nasty example in my dataset (perhaps the motivating example) seems to be:\n\ndt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100')\n\n\nwhich should be datetime.datetime(2002, 6, 28, 1, 0), and not a long (!) (1025222400000000000L)...\n"
"I have a Numpy array consisting of a list of lists, representing a two-dimensional array with row labels and column names as shown below:\n\ndata = array([['','Col1','Col2'],['Row1',1,2],['Row2',3,4]])\n\n\nI'd like the resulting DataFrame to have Row1 and Row2 as index values, and Col1, Col2 as header values\n\nI can specify the index as follows:\n\ndf = pd.DataFrame(data,index=data[:,0]),\n\n\nhowever I am unsure how to best assign column headers.\n"
'Given a DataFrame with a column "BoolCol", we want to find the indexes of the DataFrame in which the values for "BoolCol" == True\n\nI currently have the iterating way to do it, which works perfectly:\n\nfor i in range(100,3000):\n    if df.iloc[i][\'BoolCol\']== True:\n         print i,df.iloc[i][\'BoolCol\']\n\n\nBut this is not the correct panda\'s way to do it.\nAfter some research, I am currently using this code:\n\ndf[df[\'BoolCol\'] == True].index.tolist()\n\n\nThis one gives me a list of indexes, but they dont match, when I check them by doing:\n\ndf.iloc[i][\'BoolCol\']\n\n\nThe result is actually False!!\n\nWhich would be the correct Pandas way to do this?\n'
'I am using pandas as a db substitute as I have multiple databases (oracle, mssql, etc) and I am unable to make a sequence of commands to a SQL equivalent.\n\nI have a table loaded in a DataFrame with some columns:\n\nYEARMONTH, CLIENTCODE, SIZE, .... etc etc\n\n\nIn SQL, to count the amount of different clients per year would be:\n\nSELECT count(distinct CLIENTCODE) FROM table GROUP BY YEARMONTH;\n\n\nAnd the result would be \n\n201301    5000\n201302    13245\n\n\nHow can I do that in pandas? \n'
"I have a data frame like this: \n\nprint(df)\n\n        0          1     2\n0   354.7      April   4.0\n1    55.4     August   8.0\n2   176.5   December  12.0\n3    95.5   February   2.0\n4    85.6    January   1.0\n5     152       July   7.0\n6   238.7       June   6.0\n7   104.8      March   3.0\n8   283.5        May   5.0\n9   278.8   November  11.0\n10  249.6    October  10.0\n11  212.7  September   9.0\n\n\nAs you can see, months are not in calendar order. So I created a second column to get the month number corresponding to each month (1-12). From there, how can I sort this data frame according to  calendar months' order?\n"
'In order to test some functionality I would like to create a DataFrame from a string. Let\'s say my test data looks like:\n\nTESTDATA="""col1;col2;col3\n1;4.4;99\n2;4.5;200\n3;4.7;65\n4;3.2;140\n"""\n\n\nWhat is the simplest way to read that data into a Pandas DataFrame?\n'
"What's the easiest way to add an empty column to a pandas DataFrame object?  The best I've stumbled upon is something like\n\ndf['foo'] = df.apply(lambda _: '', axis=1)\n\n\nIs there a less perverse method?\n"
'I want to know if it is possible to use the pandas to_csv() function to add a dataframe to an existing csv file. The csv file has the same structure as the loaded data. \n'
'I have two Series s1 and s2 with the same (non-consecutive) indices. How do I combine s1 and s2 to being two columns in a DataFrame and keep one of the indices as a third column?\n'
'Suppose I have a dataframe with columns a, b and c, I want to sort the dataframe by column b in ascending order, and by column c in descending order, how do I do this?\n'
"Here is my code to generate a dataframe:\n\nimport pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(1,2),columns=list('AB'))\n\n\nthen I got the dataframe:\n\n+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|\n+------------+---------+--------+\n\n\nWhen I type the commmand :\n\ndff.mean(axis=1)\n\n\nI got :\n\n0    1.074821\ndtype: float64\n\n\nAccording to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be\n\nA    0.626386\nB    1.523255\ndtype: float64\n\n\nSo here is my question: what does axis in pandas mean?\n"
'How do I get the index column name in python pandas?  Here\'s an example dataframe:\n\n             Column 1\nIndex Title          \nApples              1\nOranges             2\nPuppies             3\nDucks               4  \n\n\nWhat I\'m trying to do is get/set the dataframe index title.  Here is what i tried:\n\nimport pandas as pd\ndata = {\'Column 1\'     : [1., 2., 3., 4.],\n        \'Index Title\'  : ["Apples", "Oranges", "Puppies", "Ducks"]}\ndf = pd.DataFrame(data)\ndf.index = df["Index Title"]\ndel df["Index Title"]\nprint df\n\n\nAnyone know how to do this? \n'
"How to remove rows with duplicate index values?\nIn the weather DataFrame below, sometimes a scientist goes back and corrects observations -- not by editing the erroneous rows, but by appending a duplicate row to the end of a file.\nI'm reading some automated weather data from the web (observations occur every 5 minutes, and compiled into monthly files for each weather station.) After parsing a file, the DataFrame looks like:\n                      Sta  Precip1hr  Precip5min  Temp  DewPnt  WindSpd  WindDir  AtmPress\nDate                                                                                      \n2001-01-01 00:00:00  KPDX          0           0     4       3        0        0     30.31\n2001-01-01 00:05:00  KPDX          0           0     4       3        0        0     30.30\n2001-01-01 00:10:00  KPDX          0           0     4       3        4       80     30.30\n2001-01-01 00:15:00  KPDX          0           0     3       2        5       90     30.30\n2001-01-01 00:20:00  KPDX          0           0     3       2       10      110     30.28\n\nExample of a duplicate case:\nimport pandas \nimport datetime\n\nstartdate = datetime.datetime(2001, 1, 1, 0, 0)\nenddate = datetime.datetime(2001, 1, 1, 5, 0)\nindex = pandas.DatetimeIndex(start=startdate, end=enddate, freq='H')\ndata1 = {'A' : range(6), 'B' : range(6)}\ndata2 = {'A' : [20, -30, 40], 'B' : [-50, 60, -70]}\ndf1 = pandas.DataFrame(data=data1, index=index)\ndf2 = pandas.DataFrame(data=data2, index=index[:3])\ndf3 = df2.append(df1)\n\ndf3\n                       A   B\n2001-01-01 00:00:00   20 -50\n2001-01-01 01:00:00  -30  60\n2001-01-01 02:00:00   40 -70\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n\nAnd so I need df3 to eventually become:\n                       A   B\n2001-01-01 00:00:00    0   0\n2001-01-01 01:00:00    1   1\n2001-01-01 02:00:00    2   2\n2001-01-01 03:00:00    3   3\n2001-01-01 04:00:00    4   4\n2001-01-01 05:00:00    5   5\n\nI thought that adding a column of row numbers (df3['rownum'] = range(df3.shape[0])) would help me select the bottom-most row for any value of the DatetimeIndex, but I am stuck on figuring out the group_by or pivot (or ???) statements to make that work.\n"
"I have one field in a pandas DataFrame that was imported as string format. \nIt should be a datetime variable.\nHow do I convert it to a datetime column and then filter based on date.\n\nExample:\n\n\nDataFrame Name: raw_data    \nColumn Name: Mycol    \nValue\nFormat in Column: '05SEP2014:00:00:00.000'\n\n"
'I have a dataframe in pandas where each column has different value range. For example:\n\ndf:\n\nA     B   C\n1000  10  0.5\n765   5   0.35\n800   7   0.09\n\n\nAny idea how I can normalize the columns of this dataframe where each value is between 0 and 1?\n\nMy desired output is:\n\nA     B    C\n1     1    1\n0.765 0.5  0.7\n0.8   0.7  0.18(which is 0.09/0.5)\n\n'
"I load some machine learning data from a CSV file. The first 2 columns are observations and the remaining columns are features.\n\nCurrently, I do the following:\n\ndata = pandas.read_csv('mydata.csv')\n\n\nwhich gives something like:\n\ndata = pandas.DataFrame(np.random.rand(10,5), columns = list('abcde'))\n\n\nI'd like to slice this dataframe in two dataframes: one containing the columns a and b and one containing the columns c, d and e.\n\nIt is not possible to write something like \n\nobservations = data[:'c']\nfeatures = data['c':]\n\n\nI'm not sure what the best method is. Do I need a pd.Panel?\n\nBy the way, I find dataframe indexing pretty inconsistent: data['a'] is permitted, but data[0] is not. On the other side, data['a':] is not permitted but data[0:] is.\nIs there a practical reason for this? This is really confusing if columns are indexed by Int, given that data[0] != data[0:1]\n"
'I have a dataframe df :\n\n&gt;&gt;&gt; df\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20060630   6.590       NaN      6.590   5.291\n       20060930  10.103       NaN     10.103   7.981\n       20061231  15.915       NaN     15.915  12.686\n       20070331   3.196       NaN      3.196   2.710\n       20070630   7.907       NaN      7.907   6.459\n\n\nThen I want to drop rows with certain sequence numbers which indicated in a list, suppose here is [1,2,4], then left:\n\n                  sales  discount  net_sales    cogs\nSTK_ID RPT_Date                                     \n600141 20060331   2.709       NaN      2.709   2.245\n       20061231  15.915       NaN     15.915  12.686\n       20070630   7.907       NaN      7.907   6.459\n\n\nHow or what function can do that ?\n'
"How do I find all rows in a pandas data frame which have the max value for count column, after grouping by ['Sp','Mt'] columns?\nExample 1: the following dataFrame, which I group by ['Sp','Mt']:\n   Sp   Mt Value   count\n0  MM1  S1   a     **3**\n1  MM1  S1   n       2\n2  MM1  S3   cb    **5**\n3  MM2  S3   mk    **8**\n4  MM2  S4   bg    **10**\n5  MM2  S4   dgd     1\n6  MM4  S2   rd      2\n7  MM4  S2   cb      2\n8  MM4  S2   uyi   **7**\n\nExpected output: get the result rows whose count is max between the groups, like:\n0  MM1  S1   a      **3**\n2  MM1  S3   cb     **5**\n3  MM2  S3   mk     **8**\n4  MM2  S4   bg     **10** \n8  MM4  S2   uyi    **7**\n\nExample 2: this dataframe, which I group by ['Sp','Mt']:\n   Sp   Mt   Value  count\n4  MM2  S4   bg     10\n5  MM2  S4   dgd    1\n6  MM4  S2   rd     2\n7  MM4  S2   cb     8\n8  MM4  S2   uyi    8\n\nFor the above example, I want to get all the rows where count equals max, in each group e.g :\nMM2  S4   bg     10\nMM4  S2   cb     8\nMM4  S2   uyi    8\n\n"
'In R when you need to retrieve a column index based on the name of the column you could do\n\nidx &lt;- which(names(my_data)==my_colum_name)\n\n\nIs there a way to do the same with pandas dataframes?\n'
'I am having trouble with some of pandas functionalities. How do I check what is my installation version?\n'
'Do you know how to get the index or column of a DataFrame as a NumPy array or python list? \n'
"I need to count unique ID values in every domain\nI have data\n\nID, domain\n123, 'vk.com'\n123, 'vk.com'\n123, 'twitter.com'\n456, 'vk.com'\n456, 'facebook.com'\n456, 'vk.com'\n456, 'google.com'\n789, 'twitter.com'\n789, 'vk.com'\n\n\nI try df.groupby(['domain', 'ID']).count()\nBut I want to get\n\ndomain, count\nvk.com   3\ntwitter.com   2\nfacebook.com   1\ngoogle.com   1\n\n"
'How can I read in a .csv file (with no headers) and when I only want a subset of the columns (say 4th and 7th out of a total of 20 columns), using pandas? I cannot seem to be able to do usecols\n'
"I've two pandas data frames that have some rows in common.\nSuppose dataframe2 is a subset of dataframe1.\nHow can I get the rows of dataframe1 which are not in dataframe2?\ndf1 = pandas.DataFrame(data = {'col1' : [1, 2, 3, 4, 5], 'col2' : [10, 11, 12, 13, 14]}) \ndf2 = pandas.DataFrame(data = {'col1' : [1, 2, 3], 'col2' : [10, 11, 12]})\n\ndf1\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14\n\ndf2\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n\nExpected result:\n   col1  col2\n3     4    13\n4     5    14\n\n"
'I converted a pandas dataframe to an html output using the DataFrame.to_html function. When I save this to a separate html file, the file shows truncated output.\n\nFor example, in my TEXT column, \n\ndf.head(1) will show \n\nThe film was an excellent effort...\n\ninstead of \n\nThe film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.\n\nThis rendition is fine in the case of a screen-friendly format of a massive pandas dataframe, but I need an html file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet. \n\nHow would I be able to show the complete, non-truncated text data for each element in my TEXT column in the html version of the information? I would imagine that the html table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the DataFrame.to_html function.\n'
'I am using this data frame:\n\nFruit   Date      Name  Number\nApples  10/6/2016 Bob    7\nApples  10/6/2016 Bob    8\nApples  10/6/2016 Mike   9\nApples  10/7/2016 Steve 10\nApples  10/7/2016 Bob    1\nOranges 10/7/2016 Bob    2\nOranges 10/6/2016 Tom   15\nOranges 10/6/2016 Mike  57\nOranges 10/6/2016 Bob   65\nOranges 10/7/2016 Tony   1\nGrapes  10/7/2016 Bob    1\nGrapes  10/7/2016 Tom   87\nGrapes  10/7/2016 Bob   22\nGrapes  10/7/2016 Bob   12\nGrapes  10/7/2016 Tony  15\n\n\nI want to aggregate this by name and then by fruit to get a total number of fruit per name.\n\nBob,Apples,16 ( for example )\n\n\nI tried grouping by Name and Fruit but how do I get the total number of fruit.\n'
"The docs show how to apply multiple functions on a groupby object at a time using a dict with the output column names as the keys:\n\nIn [563]: grouped['D'].agg({'result1' : np.sum,\n   .....:                   'result2' : np.mean})\n   .....:\nOut[563]: \n      result2   result1\nA                      \nbar -0.579846 -1.739537\nfoo -0.280588 -1.402938\n\n\nHowever, this only works on a Series groupby object. And when a dict is similarly passed to a groupby DataFrame, it expects the keys to be the column names that the function will be applied to.\n\nWhat I want to do is apply multiple functions to several columns (but certain columns will be operated on multiple times). Also, some functions will depend on other columns in the groupby object (like sumif functions). My current solution is to go column by column, and doing something like the code above, using lambdas for functions that depend on other rows. But this is taking a long time, (I think it takes a long time to iterate through a groupby object). I'll have to change it so that I iterate through the whole groupby object in a single run, but I'm wondering if there's a built in way in pandas to do this somewhat cleanly.\n\nFor example, I've tried something like \n\ngrouped.agg({'C_sum' : lambda x: x['C'].sum(),\n             'C_std': lambda x: x['C'].std(),\n             'D_sum' : lambda x: x['D'].sum()},\n             'D_sumifC3': lambda x: x['D'][x['C'] == 3].sum(), ...)\n\n\nbut as expected I get a KeyError (since the keys have to be a column if agg is called from a DataFrame).\n\nIs there any built in way to do what I'd like to do, or a possibility that this functionality may be added, or will I just need to iterate through the groupby manually?\n\nThanks\n"
"I have a pandas Series object containing boolean values. How can I get a series containing the logical NOT of each value?\n\nFor example, consider a series containing:\n\nTrue\nTrue\nTrue\nFalse\n\n\nThe series I'd like to get would contain:\n\nFalse\nFalse\nFalse\nTrue\n\n\nThis seems like it should be reasonably simple, but apparently I've misplaced my mojo =(\n"
"I've been working with data imported from a CSV. Pandas changed some columns to float, so now the numbers in these columns get displayed as floating points! However, I need them to be displayed as integers, or, without comma. Is there a way to convert them to integers or not display the comma?\n"
'what is the quickest/simplest way to drop nan and inf/-inf values from a pandas DataFrame without resetting mode.use_inf_as_null? I\'d like to be able to use the subset and how arguments of dropna, except with inf values considered missing, like:\n\ndf.dropna(subset=["col1", "col2"], how="all", with_inf=True)\n\n\nis this possible? Is there a way to tell dropna to include inf in its definition of missing values?\n'
"For example I have simple DF:\n\nimport pandas as pd\nfrom random import randint\n\ndf = pd.DataFrame({'A': [randint(1, 9) for x in xrange(10)],\n                   'B': [randint(1, 9)*10 for x in xrange(10)],\n                   'C': [randint(1, 9)*100 for x in xrange(10)]})\n\n\nCan I select values from 'A' for which corresponding values for 'B' will be greater than 50, and for 'C' - not equal 900, using methods and idioms of Pandas?\n"
"I have a Dataframe, df, with the following column:\n\ndf['ArrivalDate'] =\n...\n936   2012-12-31\n938   2012-12-29\n965   2012-12-31\n966   2012-12-31\n967   2012-12-31\n968   2012-12-31\n969   2012-12-31\n970   2012-12-29\n971   2012-12-31\n972   2012-12-29\n973   2012-12-29\n...\n\n\nThe elements of the column are pandas.tslib.Timestamp.\n\nI want to just include the year and month.  I thought there would be simple way to do it, but I can't figure it out.\n\nHere's what I've tried:\n\ndf['ArrivalDate'].resample('M', how = 'mean')\n\n\nI got the following error:\n\nOnly valid with DatetimeIndex or PeriodIndex \n\n\nThen I tried:\n\ndf['ArrivalDate'].apply(lambda(x):x[:-2])\n\n\nI got the following error:\n\n'Timestamp' object has no attribute '__getitem__' \n\n\nAny suggestions?\n\nEdit: I sort of figured it out.  \n\ndf.index = df['ArrivalDate']\n\n\nThen, I can resample another column using the index.\n\nBut I'd still like a method for reconfiguring the entire column.  Any ideas?\n"
'I have a Pandas Dataframe as shown below:\n\n    1    2       3\n 0  a  NaN    read\n 1  b    l  unread\n 2  c  NaN    read\n\n\nI want to remove the NaN values with an empty string so that it looks like so:\n\n    1    2       3\n 0  a   ""    read\n 1  b    l  unread\n 2  c   ""    read\n\n'
"How to do this in pandas:\n\nI have a function extract_text_features on a single text column, returning multiple output columns. Specifically, the function returns 6 values.\n\nThe function works, however there doesn't seem to be any proper return type (pandas DataFrame/ numpy array/ Python list) such that the output can get correctly assigned df.ix[: ,10:16] = df.textcol.map(extract_text_features)\n\nSo I think I need to drop back to iterating with df.iterrows(), as per this?\n\nUPDATE: \nIterating with df.iterrows() is at least 20x slower, so I surrendered and split out the function into six distinct .map(lambda ...) calls.\n\nUPDATE 2: this question was asked back around v0.11.0. Hence much of the question and answers are not too relevant.\n"
"I have a pandas dataframe in which one column of text strings contains comma-separated values. I want to split each CSV field and create a new row per entry (assume that CSV are clean and need only be split on ','). For example, a should become b:\n\nIn [7]: a\nOut[7]: \n    var1  var2\n0  a,b,c     1\n1  d,e,f     2\n\nIn [8]: b\nOut[8]: \n  var1  var2\n0    a     1\n1    b     1\n2    c     1\n3    d     2\n4    e     2\n5    f     2\n\n\nSo far, I have tried various simple functions, but the .apply method seems to only accept one row as return value when it is used on an axis, and I can't get .transform to work. Any suggestions would be much appreciated!\n\nExample data: \n\nfrom pandas import DataFrame\nimport numpy as np\na = DataFrame([{'var1': 'a,b,c', 'var2': 1},\n               {'var1': 'd,e,f', 'var2': 2}])\nb = DataFrame([{'var1': 'a', 'var2': 1},\n               {'var1': 'b', 'var2': 1},\n               {'var1': 'c', 'var2': 1},\n               {'var1': 'd', 'var2': 2},\n               {'var1': 'e', 'var2': 2},\n               {'var1': 'f', 'var2': 2}])\n\n\nI know this won't work because we lose DataFrame meta-data by going through numpy, but it should give you a sense of what I tried to do: \n\ndef fun(row):\n    letters = row['var1']\n    letters = letters.split(',')\n    out = np.array([row] * len(letters))\n    out['var1'] = letters\na['idx'] = range(a.shape[0])\nz = a.groupby('idx')\nz.transform(fun)\n\n"
'I have some problems with the Pandas apply function, when using multiple columns with the following dataframe\n\ndf = DataFrame ({\'a\' : np.random.randn(6),\n                 \'b\' : [\'foo\', \'bar\'] * 3,\n                 \'c\' : np.random.randn(6)})\n\n\nand the following function\n\ndef my_test(a, b):\n    return a % b\n\n\nWhen I try to apply this function with :\n\ndf[\'Value\'] = df.apply(lambda row: my_test(row[a], row[c]), axis=1)\n\n\nI get the error message:\n\nNameError: ("global name \'a\' is not defined", u\'occurred at index 0\')\n\n\nI do not understand this message, I defined the name properly. \n\nI would highly appreciate any help on this issue\n\nUpdate\n\nThanks for your help. I made indeed some syntax mistakes with the code, the index should be put \'\'. However I still get the same issue using a more complex function such as:\n\ndef my_test(a):\n    cum_diff = 0\n    for ix in df.index():\n        cum_diff = cum_diff + (a - df[\'a\'][ix])\n    return cum_diff \n\n'
'I have a data set with huge number of features, so analysing the correlation matrix has become very difficult. I want to plot a correlation matrix which we get using dataframe.corr() function from pandas library. Is there any built-in function provided by the pandas library to plot this matrix?\n'
"I am creating a DataFrame from a csv as follows:\n\nstock = pd.read_csv('data_in/' + filename + '.csv', skipinitialspace=True)\n\n\nThe DataFrame has a date column. Is there a way to create a new DataFrame (or just overwrite the existing one) which only contains rows with date values that fall within a specified date range or between two specified date values?\n"
"Is it possible to append to an empty data frame that doesn't contain any indices or columns?\n\nI have tried to do this, but keep getting an empty dataframe at the end.\n\ne.g.\n\ndf = pd.DataFrame()\ndata = ['some kind of data here' --&gt; I have checked the type already, and it is a dataframe]\ndf.append(data)\n\n\nThe result looks like this:\n\nEmpty DataFrame\nColumns: []\nIndex: []\n\n"
"Suppose I have two DataFrames like so:\n\nleft = pd.DataFrame({'key1': ['foo', 'bar'], 'lval': [1, 2]})\n\nright = pd.DataFrame({'key2': ['foo', 'bar'], 'rval': [4, 5]})\n\n\nI want to merge them, so I try something like this:\n\npd.merge(left, right, left_on='key1', right_on='key2')\n\n\nAnd I'm happy\n\n    key1    lval    key2    rval\n0   foo     1       foo     4\n1   bar     2       bar     5\n\n\nBut I'm trying to use the join method, which I've been lead to believe is pretty similar. \n\nleft.join(right, on=['key1', 'key2'])\n\n\nAnd I get this:\n\n//anaconda/lib/python2.7/site-packages/pandas/tools/merge.pyc in _validate_specification(self)\n    406             if self.right_index:\n    407                 if not ((len(self.left_on) == self.right.index.nlevels)):\n--&gt; 408                     raise AssertionError()\n    409                 self.right_on = [None] * n\n    410         elif self.right_on is not None:\n\nAssertionError: \n\n\nWhat am I missing?\n"
"How can I find the row for which the value of a specific column is maximal?\n\ndf.max() will give me the maximal value for each column, I don't know how to get the corresponding row.\n"
"I have a pandas DataFrame with 4 columns and I want to create a new DataFrame that only has three of the columns.  This question is similar to: Extracting specific columns from a data frame but for pandas not R.  The following code does not work, raises an error, and is certainly not the pandasnic way to do it. \n\nimport pandas as pd\nold = pd.DataFrame({'A' : [4,5], 'B' : [10,20], 'C' : [100,50], 'D' : [-30,-50]})\nnew = pd.DataFrame(zip(old.A, old.C, old.D)) # raises TypeError: data argument can't be an iterator \n\n\nWhat is the pandasnic way to do it?  \n"
"When selecting a sub dataframe from a parent dataframe, I noticed that some programmers make a copy of the data frame using the .copy() method. For example,\nX = my_dataframe[features_list].copy()\n\n...instead of just\nX = my_dataframe[features_list]\n\nWhy are they making a copy of the data frame? What will happen if I don't make a copy?\n"
"I have an existing dataframe which I need to add an additional column to which will contain the same value for every row.\n\nExisting df:\n\nDate, Open, High, Low, Close\n01-01-2015, 565, 600, 400, 450\n\n\nNew df:\n\nName, Date, Open, High, Low, Close\nabc, 01-01-2015, 565, 600, 400, 450\n\n\nI know how to append an existing series / dataframe column. But this is a different situation, because all I need is to add the 'Name' column and set every row to the same value, in this case 'abc'.\n"
"Can I insert a column at a specific column index in pandas? \n\nimport pandas as pd\ndf = pd.DataFrame({'l':['a','b','c','d'], 'v':[1,2,1,2]})\ndf['n'] = 0\n\n\nThis will put column n as the last column of df, but isn't there a way to tell df to put n at the beginning?\n"
"I have a data frame with one (string) column and I'd like to split it into two (string) columns, with one column header as 'fips' and the other 'row'\nMy dataframe df looks like this:\n          row\n0    00000 UNITED STATES\n1    01000 ALABAMA\n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n\nI do not know how to use df.row.str[:] to achieve my goal of splitting the row cell. I can use df['fips'] = hello to add a new column and populate it with hello. Any ideas?\n         fips       row\n0    00000 UNITED STATES\n1    01000 ALABAMA \n2    01001 Autauga County, AL\n3    01003 Baldwin County, AL\n4    01005 Barbour County, AL\n\n"
"Having spent a decent amount of time watching both the r and pandas tags on SO, the impression that I get is that pandas questions are less likely to contain reproducible data. This is something that the R community has been pretty good about encouraging, and thanks to guides like this, newcomers are able to get some help on putting together these examples. People who are able to read these guides and come back with reproducible data will often have much better luck getting answers to their questions.\n\nHow can we create good reproducible examples for pandas questions? Simple dataframes can be put together, e.g.:\n\nimport pandas as pd\ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n                   'income': [40000, 50000, 42000]})\n\n\nBut many example datasets need more complicated structure, e.g.:\n\n\ndatetime indices or data\nMultiple categorical variables (is there an equivalent to R's expand.grid() function, which produces all possible combinations of some given variables?)\nMultiIndex or Panel data\n\n\nFor datasets that are hard to mock up using a few lines of code, is there an equivalent to R's dput() that allows you to generate copy-pasteable code to regenerate your datastructure?\n"
"Suppose I have the following code that plots something very simple using pandas:\n\nimport pandas as pd\nvalues = [[1, 2], [2, 5]]\ndf2 = pd.DataFrame(values, columns=['Type A', 'Type B'], \n                   index=['Index 1', 'Index 2'])\ndf2.plot(lw=2, colormap='jet', marker='.', markersize=10, \n         title='Video streaming dropout by category')\n\n\n\n\nHow do I easily set x and y-labels while preserving my ability to use specific colormaps? I noticed that the plot() wrapper for pandas DataFrames doesn't take any parameters specific for that.\n"
"I have a large spreadsheet file (.xlsx) that I'm processing using python pandas. It happens that I need data from two tabs in that large file. One of the tabs has a ton of data and the other is just a few square cells.\n\nWhen I use pd.read_excel() on any worksheet, it looks to me like the whole file is loaded (not just the worksheet I'm interested in). So when I use the method twice (once for each sheet), I effectively have to suffer the whole workbook being read in twice (even though we're only using the specified sheet).\n\nAm I using it wrong or is it just limited in this way?\n\nThank you!\n"
"I have the following DataFrame from a SQL query:\n\n(Pdb) pp total_rows\n     ColumnID  RespondentCount\n0          -1                2\n1  3030096843                1\n2  3030096845                1\n\n\nand I want to pivot it like this:\n\ntotal_data = total_rows.pivot_table(cols=['ColumnID'])\n\n(Pdb) pp total_data\nColumnID         -1            3030096843   3030096845\nRespondentCount            2            1            1\n\n[1 rows x 3 columns]\n\n\ntotal_rows.pivot_table(cols=['ColumnID']).to_dict('records')[0]\n\n{3030096843: 1, 3030096845: 1, -1: 2}\n\n\nbut I want to make sure the 303 columns are casted as strings instead of integers so that I get this:\n\n{'3030096843': 1, '3030096845': 1, -1: 2}\n\n"
"I have a DataFrame with four columns. I want to convert this DataFrame to a python dictionary. I want the elements of first column be keys and the elements of other columns in same row be values. \n\nDataFrame:   \n\n    ID   A   B   C\n0   p    1   3   2\n1   q    4   3   2\n2   r    4   0   9  \n\n\nOutput should be like this:\n\nDictionary:\n\n{'p': [1,3,2], 'q': [4,3,2], 'r': [4,0,9]}\n\n"
"Without using groupby how would I filter out data without NaN?\nLet say I have a matrix where customers will fill in 'N/A','n/a' or any of its variations and others leave it blank:\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'movie': ['thg', 'thg', 'mol', 'mol', 'lob', 'lob'],\n                  'rating': [3., 4., 5., np.nan, np.nan, np.nan],\n                  'name': ['John', np.nan, 'N/A', 'Graham', np.nan, np.nan]})\n\nnbs = df['name'].str.extract('^(N/A|NA|na|n/a)')\nnms=df[(df['name'] != nbs) ]\n\noutput:\n&gt;&gt;&gt; nms\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n\nHow would I filter out NaN values so I can get results to work with like this:\n  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n\nI am guessing I need something like ~np.isnan but the tilda does not work with strings.\n"
'I have 3 CSV files. Each has the first column as the (string) names of people, while all the other columns in each dataframe are attributes of that person. \n\nHow can I "join" together all three CSV documents to create a single CSV with each row having all the attributes for each unique value of the person\'s string name?\n\nThe join() function in pandas specifies that I need a multiindex, but I\'m confused about what a hierarchical indexing scheme has to do with making a join based on a single index. \n'
"I am trying to join two pandas data frames using two columns:\n\nnew_df = pd.merge(A_df, B_df,  how='left', left_on='[A_c1,c2]', right_on = '[B_c1,c2]')\n\n\nbut got the following error:\n\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4164)()\n\npandas/index.pyx in pandas.index.IndexEngine.get_loc (pandas/index.c:4028)()\n\npandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13166)()\n\npandas/src/hashtable_class_helper.pxi in pandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13120)()\n\nKeyError: '[B_1, c2]'\n\n\nAny idea what should be the right way to do this? Thanks!\n"
"I've got a dataframe called data. How would I rename the only one column header? For example gdp to log(gdp)?\n\ndata =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n5   4    8    3\n6   8    2    8\n7   9    9   10\n8   6    6    4\n9  10   10    7\n\n"
"I was looking for an elegant way to change a specified column name in a DataFrame.\n\nplay data ...\n\nimport pandas as pd\nd = {\n         'one': [1, 2, 3, 4, 5],\n         'two': [9, 8, 7, 6, 5],\n         'three': ['a', 'b', 'c', 'd', 'e']\n    }\ndf = pd.DataFrame(d)\n\n\nThe most elegant solution I have found so far ...\n\nnames = df.columns.tolist()\nnames[names.index('two')] = 'new_name'\ndf.columns = names\n\n\nI was hoping for a simple one-liner ... this attempt failed ...\n\ndf.columns[df.columns.tolist().index('one')] = 'another_name'\n\n\nAny hints gratefully received.\n"
"I've got a pandas DataFrame filled mostly with real numbers, but there is a few nan values in it as well.\n\nHow can I replace the nans with averages of columns where they are?\n\nThis question is very similar to this one: numpy array: replace nan values with average of columns  but, unfortunately, the solution given there doesn't work for a pandas DataFrame.\n"
"I need to delete the first three rows of a dataframe in pandas.\n\nI know df.ix[:-1] would remove the last row, but I can't figure out how to remove first n rows.\n"
"I want to group my dataframe by two columns and then sort the aggregated results within the groups.\n\nIn [167]:\ndf\n\nOut[167]:\ncount   job source\n0   2   sales   A\n1   4   sales   B\n2   6   sales   C\n3   3   sales   D\n4   7   sales   E\n5   5   market  A\n6   3   market  B\n7   2   market  C\n8   4   market  D\n9   1   market  E\n\nIn [168]:\ndf.groupby(['job','source']).agg({'count':sum})\n\nOut[168]:\n            count\njob     source  \nmarket  A   5\n        B   3\n        C   2\n        D   4\n        E   1\nsales   A   2\n        B   4\n        C   6\n        D   3\n        E   7\n\n\nI would now like to sort the count column in descending order within each of the groups. And then take only the top three rows. To get something like:\n\n            count\njob     source  \nmarket  A   5\n        D   4\n        B   3\nsales   E   7\n        C   6\n        B   4\n\n"
"I am trying to determine whether there is an entry in a Pandas column that has a particular value. I tried to do this with if x in df['id']. I thought this was working, except when I fed it a value that I knew was not in the column 43 in df['id'] it still returned True. When I subset to a data frame only containing entries matching the missing id df[df['id'] == 43] there are, obviously, no entries in it. How to I determine if a column in a Pandas data frame contains a particular value and why doesn't my current method work? (FYI, I have the same problem when I use the implementation in this answer to a similar question).\n"
'How do you programmatically retrieve the number of columns in a pandas dataframe? I was hoping for something like:\n\ndf.num_columns\n\n'
"I'm sure this is simple, but as a complete newbie to python, I'm having trouble figuring out how to iterate over variables in a pandas dataframe and run a regression with each.\n\nHere's what I'm doing:\n\nall_data = {}\nfor ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')\n\nprices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  \nreturns = prices.pct_change()\n\n\nI know I can run a regression like this:\n\nregs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()\n\n\nbut suppose I want to do this for each column in the dataframe. In particular, I want to regress FIUIX on FSTMX, and then FSAIX on FSTMX, and then FSAVX on FSTMX. After each regression I want to store the residuals.\n\nI've tried various versions of the following, but I must be getting the syntax wrong:\n\nresids = {}\nfor k in returns.keys():\n    reg = sm.OLS(returns[k],returns.FSTMX).fit()\n    resids[k] = reg.resid\n\n\nI think the problem is I don't know how to refer to the returns column by key, so returns[k] is probably wrong.\n\nAny guidance on the best way to do this would be much appreciated. Perhaps there's a common pandas approach I'm missing.\n"
"I have the following DataFrame:\n\n\ncustomer    item1      item2    item3\n1           apple      milk     tomato\n2           water      orange   potato\n3           juice      mango    chips\n\n\nwhich I want to translate it to list of dictionaries per row\n\nrows = [{'customer': 1, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n    {'customer': 2, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n    {'customer': 3, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n\n"
'If I have a dataframe with the following columns: \n\n1. NAME                                     object\n2. On_Time                                      object\n3. On_Budget                                    object\n4. %actual_hr                                  float64\n5. Baseline Start Date                  datetime64[ns]\n6. Forecast Start Date                  datetime64[ns] \n\n\nI would like to be able to say: here is a dataframe, give me a list of the columns which are of type Object or of type DateTime?\n\nI have a function which converts numbers (Float64) to two decimal places, and I would like to use this list of dataframe columns, of a particular type, and run it through this function to convert them all to 2dp.\n\nMaybe:\n\nFor c in col_list: if c.dtype = "Something"\nlist[]\nList.append(c)?\n\n'
"How do I take multiple lists and put them as different columns in a python dataframe? I tried this solution but had some trouble.\n\nAttempt 1:\n\n\nHave three lists, and zip them together and use that res = zip(lst1,lst2,lst3)\nYields just one column\n\n\nAttempt 2:\n\npercentile_list = pd.DataFrame({'lst1Tite' : [lst1],\n                                'lst2Tite' : [lst2],\n                                'lst3Tite' : [lst3] }, \n                                columns=['lst1Tite','lst1Tite', 'lst1Tite'])\n\n\n\nyields either one row by 3 columns (the way above) or if I transpose it is 3 rows and 1 column\n\n\nHow do I get a 100 row (length of each independent list) by 3 column (three lists) pandas dataframe? \n"
"I have the following dataframes:\n&gt; df1\n  id begin conditional confidence discoveryTechnique  \n0 278    56       false        0.0                  1   \n1 421    18       false        0.0                  1 \n\n&gt; df2\n   concept \n0  A  \n1  B\n   \n\nHow do I merge on the indices to get:\n  id begin conditional confidence discoveryTechnique   concept \n0 278    56       false        0.0                  1  A \n1 421    18       false        0.0                  1  B\n\nI ask because it is my understanding that merge() i.e. df1.merge(df2) uses columns to do the matching. In fact, doing this I get:\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py&quot;, line 4618, in merge\n    copy=copy, indicator=indicator)\n  File &quot;/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py&quot;, line 58, in merge\n    copy=copy, indicator=indicator)\n  File &quot;/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py&quot;, line 491, in __init__\n    self._validate_specification()\n  File &quot;/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py&quot;, line 812, in _validate_specification\n    raise MergeError('No common columns to perform merge on')\npandas.tools.merge.MergeError: No common columns to perform merge on\n\nIs it bad practice to merge on index? Is it impossible? If so, how can I shift the index into a new column called &quot;index&quot;?\n"
"I have a dataframe with repeat values in column A.  I want to drop duplicates, keeping the row with the highest value in column B.\n\nSo this:\n\nA B\n1 10\n1 20\n2 30\n2 40\n3 10\n\n\nShould turn into this:\n\nA B\n1 20\n2 40\n3 10\n\n\nWes has added some nice functionality to drop duplicates: http://wesmckinney.com/blog/?p=340.  But AFAICT, it's designed for exact duplicates, so there's no mention of criteria for selecting which rows get kept.\n\nI'm guessing there's probably an easy way to do this---maybe as easy as sorting the dataframe before dropping duplicates---but I don't know groupby's internal logic well enough to figure it out.  Any suggestions?\n"
"So I completely understand how to use resample, but the documentation does not do a good job explaining the options.\n\nSo most options in the resample function are pretty straight forward except for these two:\n\n\nrule : the offset string or object representing target conversion\nhow : string, method for down- or re-sampling, default to ‘mean’\n\n\nSo from looking at as many examples as I found online I can see for rule you can do 'D' for day, 'xMin' for minutes, 'xL' for milliseconds, but that is all I could find.\n\nfor how I have seen the following: 'first', np.max, 'last', 'mean', and 'n1n2n3n4...nx' where nx is the first letter of each column index.\n\nSo is there somewhere in the documentation that I am missing that displays every option for pandas.resample's rule and how inputs? If yes, where because I could not find it. If no, what are all the options for them?\n"
'I am reading a csv file into pandas. This csv file constists of four columns and some rows, but does not have a header row, which I want to add. I have been trying the following: \n\nCov = pd.read_csv("path/to/file.txt", sep=\'\\t\')\nFrame=pd.DataFrame([Cov], columns = ["Sequence", "Start", "End", "Coverage"])\nFrame.to_csv("path/to/file.txt", sep=\'\\t\')\n\n\nBut when I apply the code, I get the following Error:\n\nValueError: Shape of passed values is (1, 1), indices imply (4, 1)\n\n\nWhat exactly does the error mean? And what would be a clean way in python to add a header row to my csv file/pandas df?\n'
"I regularly perform pandas operations on data frames in excess of 15 million or so rows and I'd love to have access to a progress indicator for particular operations.\n\nDoes a text based progress indicator for pandas split-apply-combine operations exist?\n\nFor example, in something like:\n\ndf_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n\n\nwhere feature_rollup is a somewhat involved function that take many DF columns and creates new user columns through various methods.  These operations can take a while for large data frames so I'd like to know if it is possible to have text based output in an iPython notebook that updates me on the progress.\n\nSo far, I've tried canonical loop progress indicators for Python but they don't interact with pandas in any meaningful way.\n\nI'm hoping there's something I've overlooked in the pandas library/documentation that allows one to know the progress of a split-apply-combine.  A simple implementation would maybe look at the total number of data frame subsets upon which the apply function is working and report progress as the completed fraction of those subsets.\n\nIs this perhaps something that needs to be added to the library?\n"
'What are the most common pandas ways to select/filter rows of a dataframe whose index is a MultiIndex?\n\n\nSlicing based on a single value/label\nSlicing based on multiple labels from one or more levels\nFiltering on boolean conditions and expressions\nWhich methods are applicable in what circumstances\n\n\nAssumptions for simplicity:\n\n\ninput dataframe does not have duplicate index keys\ninput dataframe below only has two levels. (Most solutions shown here generalize to N levels)\n\n\n\n\nExample input:\n\n\nmux = pd.MultiIndex.from_arrays([\n    list(\'aaaabbbbbccddddd\'),\n    list(\'tuvwtuvwtuvwtuvw\')\n], names=[\'one\', \'two\'])\n\ndf = pd.DataFrame({\'col\': np.arange(len(mux))}, mux)\n\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\nb   t      4\n    u      5\n    v      6\n    w      7\n    t      8\nc   u      9\n    v     10\nd   w     11\n    t     12\n    u     13\n    v     14\n    w     15\n\n\n\nQuestion 1: Selecting a Single Item\n\nHow do I select rows having "a" in level "one"? \n\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n\n\nAdditionally, how would I be able to drop level "one" in the output?\n\n     col\ntwo     \nt      0\nu      1\nv      2\nw      3\n\n\nQuestion 1b\nHow do I slice all rows with value "t" on level "two"?\n\n         col\none two     \na   t      0\nb   t      4\n    t      8\nd   t     12\n\n\nQuestion 2: Selecting Multiple Values in a Level\n\nHow can I select rows corresponding to items "b" and "d" in level "one"?\n\n         col\none two     \nb   t      4\n    u      5\n    v      6\n    w      7\n    t      8\nd   w     11\n    t     12\n    u     13\n    v     14\n    w     15\n\n\nQuestion 2b\nHow would I get all values corresponding to "t" and "w" in level "two"?\n\n         col\none two     \na   t      0\n    w      3\nb   t      4\n    w      7\n    t      8\nd   w     11\n    t     12\n    w     15\n\n\nQuestion 3: Slicing a Single Cross Section (x, y)\n\nHow do I retrieve a cross section, i.e., a single row having a specific values for the index from df? Specifically, how do I retrieve the cross section of (\'c\', \'u\'), given by\n\n         col\none two     \nc   u      9\n\n\nQuestion 4: Slicing Multiple Cross Sections [(a, b), (c, d), ...]\n\nHow do I select the two rows corresponding to (\'c\', \'u\'), and (\'a\', \'w\')?\n\n         col\none two     \nc   u      9\na   w      3\n\n\nQuestion 5: One Item Sliced per Level\n\nHow can I retrieve all rows corresponding to "a" in level "one" or "t" in level "two"?\n\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\nb   t      4\n    t      8\nd   t     12\n\n\nQuestion 6: Arbitrary Slicing\n\nHow can I slice specific cross sections? For "a" and "b", I would like to select all rows with sub-levels "u" and "v", and for "d", I would like to select rows with sub-level "w".\n\n         col\none two     \na   u      1\n    v      2\nb   u      5\n    v      6\nd   w     11\n    w     15\n\n\n\n  Question 7 will use a unique setup consisting of a numeric level:\n\nnp.random.seed(0)\nmux2 = pd.MultiIndex.from_arrays([\n    list(\'aaaabbbbbccddddd\'),\n    np.random.choice(10, size=16)\n], names=[\'one\', \'two\'])\n\ndf2 = pd.DataFrame({\'col\': np.arange(len(mux2))}, mux2)\n\n         col\none two     \na   5      0\n    0      1\n    3      2\n    3      3\nb   7      4\n    9      5\n    3      6\n    5      7\n    2      8\nc   4      9\n    7     10\nd   6     11\n    8     12\n    8     13\n    1     14\n    6     15\n\n\n\nQuestion 7: Filtering by numeric inequality on individual levels of the multiindex\n\nHow do I get all rows where values in level "two" are greater than 5?\n\n         col\none two     \nb   7      4\n    9      5\nc   7     10\nd   6     11\n    8     12\n    8     13\n    6     15\n\n\n\n\nNote: This post will not go through how to create MultiIndexes, how to perform assignment operations on them, or any performance related discussions (these are separate topics for another time). \n'
"I have a pandas dataframe in the following format:\n\ndf = pd.DataFrame([[1.1, 1.1, 1.1, 2.6, 2.5, 3.4,2.6,2.6,3.4,3.4,2.6,1.1,1.1,3.3], list('AAABBBBABCBDDD'), [1.1, 1.7, 2.5, 2.6, 3.3, 3.8,4.0,4.2,4.3,4.5,4.6,4.7,4.7,4.8], ['x/y/z','x/y','x/y/z/n','x/u','x','x/u/v','x/y/z','x','x/u/v/b','-','x/y','x/y/z','x','x/u/v/w'],['1','3','3','2','4','2','5','3','6','3','5','1','1','1']]).T\ndf.columns = ['col1','col2','col3','col4','col5']\n\n\ndf:\n\n   col1 col2 col3     col4 col5\n0   1.1    A  1.1    x/y/z    1\n1   1.1    A  1.7      x/y    3\n2   1.1    A  2.5  x/y/z/n    3\n3   2.6    B  2.6      x/u    2\n4   2.5    B  3.3        x    4\n5   3.4    B  3.8    x/u/v    2\n6   2.6    B    4    x/y/z    5\n7   2.6    A  4.2        x    3\n8   3.4    B  4.3  x/u/v/b    6\n9   3.4    C  4.5        -    3\n10  2.6    B  4.6      x/y    5\n11  1.1    D  4.7    x/y/z    1\n12  1.1    D  4.7        x    1\n13  3.3    D  4.8  x/u/v/w    1\n\n\nNow I want to group this by two columns like following:\n\ndf.groupby(['col5','col2']).reset_index()\n\n\nOutPut:\n\n             index col1 col2 col3     col4 col5\ncol5 col2                                      \n1    A    0      0  1.1    A  1.1    x/y/z    1\n     D    0     11  1.1    D  4.7    x/y/z    1\n          1     12  1.1    D  4.7        x    1\n          2     13  3.3    D  4.8  x/u/v/w    1\n2    B    0      3  2.6    B  2.6      x/u    2\n          1      5  3.4    B  3.8    x/u/v    2\n3    A    0      1  1.1    A  1.7      x/y    3\n          1      2  1.1    A  2.5  x/y/z/n    3\n          2      7  2.6    A  4.2        x    3\n     C    0      9  3.4    C  4.5        -    3\n4    B    0      4  2.5    B  3.3        x    4\n5    B    0      6  2.6    B    4    x/y/z    5\n          1     10  2.6    B  4.6      x/y    5\n6    B    0      8  3.4    B  4.3  x/u/v/b    6\n\n\nI want to get the count by each row like following.\nExpected Output:\n\ncol5 col2 count\n1    A      1\n     D      3\n2    B      2\netc...\n\n\nHow to get my expected output? And I want to find largest count for each 'col2' value?\n"
'I want to create a new column in a pandas data frame by applying a function to two existing columns. Following this answer I\'ve been able to create a new column when I only need one column as an argument:\n\nimport pandas as pd\ndf = pd.DataFrame({"A": [10,20,30], "B": [20, 30, 10]})\n\ndef fx(x):\n    return x * x\n\nprint(df)\ndf[\'newcolumn\'] = df.A.apply(fx)\nprint(df)\n\n\nHowever, I cannot figure out how to do the same thing when the function requires multiple arguments. For example, how do I create a new column by passing column A and column B to the function below?\n\ndef fxy(x, y):\n    return x * y\n\n'
"I would like to display a pandas dataframe with a given format using print() and the IPython display(). For example:\n\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint df\n\n         cost\nfoo   123.4567\nbar   234.5678\nbaz   345.6789\nquux  456.7890\n\n\nI would like to somehow coerce this into printing\n\n         cost\nfoo   $123.46\nbar   $234.57\nbaz   $345.68\nquux  $456.79\n\n\nwithout having to modify the data itself or create a copy, just change the way it is displayed.\n\nHow can I do this?\n"
"I am loading a txt file containig a mix of float and string data. I want to store them in an array where I can access each element. Now I am just doing \n\nimport pandas as pd\n\ndata = pd.read_csv('output_list.txt', header = None)\nprint data\n\n\nThis is the structure of the input file: 1 0 2000.0 70.2836942112 1347.28369421 /file_address.txt. \n\nNow the data are imported as a unique column. How can I divide it, so to store different elements separately (so I can call data[i,j])? And how can I define a header?\n"
"I have a dataframe where some cells contain lists of multiple values. Rather than storing multiple\nvalues in a cell, I'd like to expand the dataframe so that each item in the list gets its own row (with the same values in all other columns). So if I have:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {'trial_num': [1, 2, 3, 1, 2, 3],\n     'subject': [1, 1, 1, 2, 2, 2],\n     'samples': [list(np.random.randn(3).round(2)) for i in range(6)]\n    }\n)\n\ndf\nOut[10]: \n                 samples  subject  trial_num\n0    [0.57, -0.83, 1.44]        1          1\n1    [-0.01, 1.13, 0.36]        1          2\n2   [1.18, -1.46, -0.94]        1          3\n3  [-0.08, -4.22, -2.05]        2          1\n4     [0.72, 0.79, 0.53]        2          2\n5    [0.4, -0.32, -0.13]        2          3\n\n\nHow do I convert to long form, e.g.:\n\n   subject  trial_num  sample  sample_num\n0        1          1    0.57           0\n1        1          1   -0.83           1\n2        1          1    1.44           2\n3        1          2   -0.01           0\n4        1          2    1.13           1\n5        1          2    0.36           2\n6        1          3    1.18           0\n# etc.\n\n\nThe index is not important, it's OK to set existing\ncolumns as the index and the final ordering isn't\nimportant.\n"
"Consider the following dataframe:\ncolumns = ['A', 'B', 'C', 'D']\nrecords = [\n    ['foo', 'one', 0.162003, 0.087469],\n    ['bar', 'one', -1.156319, -1.5262719999999999],\n    ['foo', 'two', 0.833892, -1.666304],     \n    ['bar', 'three', -2.026673, -0.32205700000000004],\n    ['foo', 'two', 0.41145200000000004, -0.9543709999999999],\n    ['bar', 'two', 0.765878, -0.095968],\n    ['foo', 'one', -0.65489, 0.678091],\n    ['foo', 'three', -1.789842, -1.130922]\n]\ndf = pd.DataFrame.from_records(records, columns=columns)\n\n&quot;&quot;&quot;\n     A      B         C         D\n0  foo    one  0.162003  0.087469\n1  bar    one -1.156319 -1.526272\n2  foo    two  0.833892 -1.666304\n3  bar  three -2.026673 -0.322057\n4  foo    two  0.411452 -0.954371\n5  bar    two  0.765878 -0.095968\n6  foo    one -0.654890  0.678091\n7  foo  three -1.789842 -1.130922\n&quot;&quot;&quot;\n\nThe following commands work:\ndf.groupby('A').apply(lambda x: (x['C'] - x['D']))\ndf.groupby('A').apply(lambda x: (x['C'] - x['D']).mean())\n\nbut none of the following work:\ndf.groupby('A').transform(lambda x: (x['C'] - x['D']))\n# KeyError or ValueError: could not broadcast input array from shape (5) into shape (5,3)\n\ndf.groupby('A').transform(lambda x: (x['C'] - x['D']).mean())\n# KeyError or TypeError: cannot concatenate a non-NDFrame object\n\nWhy? The example on the documentation seems to suggest that calling transform on a group allows one to do row-wise operation processing:\n# Note that the following suggests row-wise operation (x.mean is the column mean)\nzscore = lambda x: (x - x.mean()) / x.std()\ntransformed = ts.groupby(key).transform(zscore)\n\nIn other words, I thought that transform is essentially a specific type of apply (the one that does not aggregate). Where am I wrong?\nFor reference, below is the construction of the original dataframe above:\ndf = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n                          'foo', 'bar', 'foo', 'foo'],\n                   'B' : ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'C' : randn(8), 'D' : randn(8)})\n\n"
'I am reading contents of a spreadsheet into pandas.   DataNitro has a method that returns a rectangular selection of cells as a list of lists.   So\n\ntable = Cell("A1").table\n\n\ngives\n\ntable = [[\'Heading1\', \'Heading2\'], [1 , 2], [3, 4]]\n\nheaders = table.pop(0) # gives the headers as list and leaves data\n\n\nI am busy writing code to translate this, but my guess is that it is such a simple use that there must be method to do this.    Cant seem to find it in documentation.   Any pointers to the method that would simplify this?\n'
'Assume we have a data frame in Python Pandas that looks like this:\n\ndf = pd.DataFrame({\'vals\': [1, 2, 3, 4], \'ids\': [u\'aball\', u\'bball\', u\'cnut\', u\'fball\']})\n\n\nOr, in table form:\n\nids    vals\naball   1\nbball   2\ncnut    3\nfball   4\n\n\nHow do I filter rows which contain the key word "ball?" For example, the output should be:\n\nids    vals\naball   1\nbball   2\nfball   4\n\n'
"I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. \n\nfile.csv\n\n\n\n,A,B,C\n0,1,2,3\n1,4,5,6\n2,7,8,9\n\n\nThe CSV is read with this:\n\npd.read_csv('file.csv')\n\n   Unnamed: 0  A  B  C\n0           0  1  2  3\n1           1  4  5  6\n2           2  7  8  9\n\n\nThis is very annoying! Does anyone have an idea on how to get rid of this?\n"
'I have a dynamic DataFrame which works fine, but when there are no data to be added into the DataFrame I get an error. And therefore I need a solution to create an empty DataFrame with only the column names.\n\nFor now I have something like this:\n\ndf = pd.DataFrame(columns=COLUMN_NAMES) # Note that there are now row data inserted.\n\n\nPS: It is important that the column names would still appear in a DataFrame.\n\nBut when I use it like this I get something like that as a result:\n\nIndex([], dtype=\'object\')\nEmpty DataFrame\n\n\nThe "Empty DataFrame" part is good! But instead of the Index thing I need to still display the columns.\n\nEdit:\n\nAn important thing that I found out: I am converting this DataFrame to a PDF using Jinja2, so therefore I\'m calling out a method to first output it to HTML like that:\n\ndf.to_html()\n\n\nThis is where the columns get lost I think.\n\nEdit2:\nIn general, I followed this example: http://pbpython.com/pdf-reports.html. The css is also from the link.\nThat\'s what I do to send the dataframe to the PDF:\n\nenv = Environment(loader=FileSystemLoader(\'.\'))\ntemplate = env.get_template("pdf_report_template.html")\ntemplate_vars = {"my_dataframe": df.to_html()}\n\nhtml_out = template.render(template_vars)\nHTML(string=html_out).write_pdf("my_pdf.pdf", stylesheets=["pdf_report_style.css"])\n\n\nEdit3:\n\nIf I print out the dataframe right after creation I get the followin:\n\n[0 rows x 9 columns]\nEmpty DataFrame\nColumns: [column_a, column_b, column_c, column_d, \ncolumn_e, column_f, column_g, \ncolumn_h, column_i]\nIndex: []\n\n\nThat seems reasonable, but if I print out the template_vars:\n\n\'my_dataframe\': \'&lt;table border="1" class="dataframe"&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Index([], dtype=\\\'object\\\')&lt;/td&gt;\\n      &lt;td&gt;Empty DataFrame&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\'\n\n\nAnd it seems that the columns are missing already.\n\nE4:\nIf I print out the following:\n\nprint(df.to_html())\n\n\nI get the following result already:\n\n&lt;table border="1" class="dataframe"&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;td&gt;Index([], dtype=\'object\')&lt;/td&gt;\n      &lt;td&gt;Empty DataFrame&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n'
"Suppose I have pandas DataFrame like this:\n\n&gt;&gt;&gt; df = pd.DataFrame({'id':[1,1,1,2,2,2,2,3,4],'value':[1,2,3,1,2,3,4,1,1]})\n&gt;&gt;&gt; df\n   id  value\n0   1      1\n1   1      2\n2   1      3\n3   2      1\n4   2      2\n5   2      3\n6   2      4\n7   3      1\n8   4      1\n\n\nI want to get a new DataFrame with top 2 records for each id, like this:\n\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n\n\nI can do it with numbering records within group after group by:\n\n&gt;&gt;&gt; dfN = df.groupby('id').apply(lambda x:x['value'].reset_index()).reset_index()\n&gt;&gt;&gt; dfN\n   id  level_1  index  value\n0   1        0      0      1\n1   1        1      1      2\n2   1        2      2      3\n3   2        0      3      1\n4   2        1      4      2\n5   2        2      5      3\n6   2        3      6      4\n7   3        0      7      1\n8   4        0      8      1\n&gt;&gt;&gt; dfN[dfN['level_1'] &lt;= 1][['id', 'value']]\n   id  value\n0   1      1\n1   1      2\n3   2      1\n4   2      2\n7   3      1\n8   4      1\n\n\nBut is there more effective/elegant approach to do this? And also is there more elegant approach to number records within each group (like SQL window function row_number()).\n"
'I am trying to highlight exactly what changed between two dataframes.\n\nSuppose I have two Python Pandas dataframes:\n\n"StudentRoster Jan-1":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.11                     False                Graduated\n113  Zoe    4.12                     True       \n\n"StudentRoster Jan-2":\nid   Name   score                    isEnrolled           Comment\n111  Jack   2.17                     True                 He was late to class\n112  Nick   1.21                     False                Graduated\n113  Zoe    4.12                     False                On vacation\n\n\nMy goal is to output an HTML table that:\n\n\nIdentifies rows that have changed (could be int, float, boolean, string)\nOutputs rows with same, OLD and NEW values (ideally into an HTML table) so the consumer can clearly see what changed between two dataframes: \n\n"StudentRoster Difference Jan-1 - Jan-2":  \nid   Name   score                    isEnrolled           Comment\n112  Nick   was 1.11| now 1.21       False                Graduated\n113  Zoe    4.12                     was True | now False was "" | now   "On   vacation"\n\n\n\nI suppose I could do a row by row and column by column comparison, but is there an easier way?\n'
"How can one modify the format for the output from a groupby operation in pandas that produces scientific notation for very large numbers? \n\nI know how to do string formatting in python but I'm at a loss when it comes to applying it here. \n\ndf1.groupby('dept')['data1'].sum()\n\ndept\nvalue1       1.192433e+08\nvalue2       1.293066e+08\nvalue3       1.077142e+08\n\n\nThis suppresses the scientific notation if I convert to string but now I'm just wondering how to string format and add decimals. \n\nsum_sales_dept.astype(str)\n\n"
"I can't get the average or mean of a column in pandas. A have a dataframe. Neither of things I tried below gives me the average of the column weight\n\n&gt;&gt;&gt; allDF \n         ID           birthyear  weight\n0        619040       1962       0.1231231\n1        600161       1963       0.981742\n2      25602033       1963       1.3123124     \n3        624870       1987       0.94212\n\n\nThe following returns several values, not one:\n\nallDF[['weight']].mean(axis=1)\n\n\nSo does this:\n\nallDF.groupby('weight').mean()\n\n"
'This is obviously simple, but as a numpy newbe I\'m getting stuck.\n\nI have a CSV file that contains 3 columns, the State, the Office ID, and the Sales for that office.\n\nI want to calculate the percentage of sales per office in a given state (total of all percentages in each state is 100%).\n\ndf = pd.DataFrame({\'state\': [\'CA\', \'WA\', \'CO\', \'AZ\'] * 3,\n                   \'office_id\': range(1, 7) * 2,\n                   \'sales\': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\n\ndf.groupby([\'state\', \'office_id\']).agg({\'sales\': \'sum\'})\n\n\nThis returns:\n\n                  sales\nstate office_id        \nAZ    2          839507\n      4          373917\n      6          347225\nCA    1          798585\n      3          890850\n      5          454423\nCO    1          819975\n      3          202969\n      5          614011\nWA    2          163942\n      4          369858\n      6          959285\n\n\nI can\'t seem to figure out how to "reach up" to the state level of the groupby to total up the sales for the entire state to calculate the fraction.\n'
'The pandas drop_duplicates function is great for "uniquifying" a dataframe. However, one of the keyword arguments to pass is take_last=True or take_last=False, while I would like to drop all rows which are duplicates across a subset of columns. Is this possible?\n\n    A   B   C\n0   foo 0   A\n1   foo 1   A\n2   foo 1   B\n3   bar 1   A\n\n\nAs an example, I would like to drop rows which match on columns A and C so this should drop rows 0 and 1.\n'
'I have a pandas DataFrame with one column:\n\nimport pandas as pd\n\ndf = pd.DataFrame(\n    data={\n        "teams": [\n            ["SF", "NYG"],\n            ["SF", "NYG"],\n            ["SF", "NYG"],\n            ["SF", "NYG"],\n            ["SF", "NYG"],\n            ["SF", "NYG"],\n            ["SF", "NYG"],\n        ]\n    }\n)\n\nprint(df)\n\n\nOutput:\n\n       teams\n0  [SF, NYG]\n1  [SF, NYG]\n2  [SF, NYG]\n3  [SF, NYG]\n4  [SF, NYG]\n5  [SF, NYG]\n6  [SF, NYG]\n\n\nHow can split this column of lists into 2 columns?\n'
"The new version of Pandas uses the following interface to load Excel files:\n\nread_excel('path_to_file.xls', 'Sheet1', index_col=None, na_values=['NA'])\n\n\nbut what if I don't know the sheets that are available? \n\nFor example, I am working with excel files that the following sheets\n\n\n  Data 1, Data 2 ..., Data N, foo, bar\n\n\nbut I don't know N a priori.\n\nIs there any way to get the list of sheets from an excel document in Pandas?\n"
"I know this is a very basic question but for some reason I can't find an answer. How can I get the index of certain element of a Series in python pandas? (first occurrence would suffice)\n\nI.e., I'd like something like:\n\nimport pandas as pd\nmyseries = pd.Series([1,4,0,7,5], index=[0,1,2,3,4])\nprint myseries.find(7) # should output 3\n\n\nCertainly, it is possible to define such a method with a loop:\n\ndef find(s, el):\n    for i in s.index:\n        if s[i] == el: \n            return i\n    return None\n\nprint find(myseries, 7)\n\n\nbut I assume there should be a better way. Is there?\n"
"I'm working with boolean index in Pandas.\nThe question is why the statement:\n\na[(a['some_column']==some_number) &amp; (a['some_other_column']==some_other_number)]\n\n\nworks fine whereas\n\na[(a['some_column']==some_number) and (a['some_other_column']==some_other_number)]\n\n\nexits with error?\n\nExample:\n\na=pd.DataFrame({'x':[1,1],'y':[10,20]})\n\nIn: a[(a['x']==1)&amp;(a['y']==10)]\nOut:    x   y\n     0  1  10\n\nIn: a[(a['x']==1) and (a['y']==10)]\nOut: ValueError: The truth value of an array with more than one element is ambiguous.     Use a.any() or a.all()\n\n"
"DataFrame:\n\n  c_os_family_ss c_os_major_is l_customer_id_i\n0      Windows 7                         90418\n1      Windows 7                         90418\n2      Windows 7                         90418\n\n\nCode:\n\nprint df\nfor name, group in df.groupby('l_customer_id_i').agg(lambda x: ','.join(x)):\n    print name\n    print group\n\n\nI'm trying to just loop over the aggregated data, but I get the error:\n\n\n  ValueError: too many values to unpack\n\n\n@EdChum, here's the expected output:\n\n                                                    c_os_family_ss  \\\nl_customer_id_i\n131572           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n135467           Windows 7,Windows 7,Windows 7,Windows 7,Window...\n\n                                                     c_os_major_is\nl_customer_id_i\n131572           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n135467           ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...\n\n\nThe output is not the problem, I wish to loop over every group.\n"
'Given a pandas dataframe containing possible NaN values scattered here and there:\n\nQuestion: How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?\n'
"I want to find all values in a Pandas dataframe that contain whitespace (any arbitrary amount) and replace those values with NaNs.\n\nAny ideas how this can be improved?\n\nBasically I want to turn this:\n\n                   A    B    C\n2000-01-01 -0.532681  foo    0\n2000-01-02  1.490752  bar    1\n2000-01-03 -1.387326  foo    2\n2000-01-04  0.814772  baz     \n2000-01-05 -0.222552         4\n2000-01-06 -1.176781  qux     \n\n\nInto this:\n\n                   A     B     C\n2000-01-01 -0.532681   foo     0\n2000-01-02  1.490752   bar     1\n2000-01-03 -1.387326   foo     2\n2000-01-04  0.814772   baz   NaN\n2000-01-05 -0.222552   NaN     4\n2000-01-06 -1.176781   qux   NaN\n\n\nI've managed to do it with the code below, but man is it ugly. It's not Pythonic and I'm sure it's not the most efficient use of pandas either. I loop through each column and do boolean replacement against a column mask generated by applying a function that does a regex search of each value, matching on whitespace.\n\nfor i in df.columns:\n    df[i][df[i].apply(lambda i: True if re.search('^\\s*$', str(i)) else False)]=None\n\n\nIt could be optimized a bit by only iterating through fields that could contain empty strings:\n\nif df[i].dtype == np.dtype('object')\n\n\nBut that's not much of an improvement\n\nAnd finally, this code sets the target strings to None, which works with Pandas' functions like fillna(), but it would be nice for completeness if I could actually insert a NaN directly instead of None.\n"
'I\'ve done some searching and can\'t figure out how to filter a dataframe by df["col"].str.contains(word), however I\'m wondering if there is a way to do the reverse: filter a dataframe by that set\'s compliment. eg: to the effect of !(df["col"].str.contains(word)). \n\nCan this be done through a DataFrame method?\n'
"How do I access the corresponding groupby dataframe in a groupby object by the key?\n\nWith the following groupby:\n\nrand = np.random.RandomState(1)\ndf = pd.DataFrame({'A': ['foo', 'bar'] * 3,\n                   'B': rand.randn(6),\n                   'C': rand.randint(0, 20, 6)})\ngb = df.groupby(['A'])\n\n\nI can iterate through it to get the keys and groups:\n\nIn [11]: for k, gp in gb:\n             print 'key=' + str(k)\n             print gp\nkey=bar\n     A         B   C\n1  bar -0.611756  18\n3  bar -1.072969  10\n5  bar -2.301539  18\nkey=foo\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n\n\nI would like to be able to access a group by its key:\n\nIn [12]: gb['foo']\nOut[12]:  \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n\n\nBut when I try doing that with gb[('foo',)] I get this weird pandas.core.groupby.DataFrameGroupBy object thing which doesn't seem to have any methods that correspond to the DataFrame I want.\n\nThe best I could think of is:\n\nIn [13]: def gb_df_key(gb, key, orig_df):\n             ix = gb.indices[key]\n             return orig_df.ix[ix]\n\n         gb_df_key(gb, 'foo', df)\nOut[13]:\n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14  \n\n\nbut this is kind of nasty, considering how nice pandas usually is at these things.\nWhat's the built-in way of doing this?\n"
'Today I was positively surprised by the fact that while reading data from a data file (for example) pandas is able to recognize types of values:\n\ndf = pandas.read_csv(\'test.dat\', delimiter=r"\\s+", names=[\'col1\',\'col2\',\'col3\'])\n\n\nFor example it can be checked in this way:\n\nfor i, r in df.iterrows():\n    print type(r[\'col1\']), type(r[\'col2\']), type(r[\'col3\'])\n\n\nIn particular integer, floats and strings were recognized correctly. However, I have a column that has dates in the following format: 2013-6-4. These dates were recognized as strings (not as python date-objects). Is there a way to "learn" pandas to recognized dates?\n'
"I have a dataframe with 2 index levels:\n\n                         value\nTrial    measurement\n    1              0        13\n                   1         3\n                   2         4\n    2              0       NaN\n                   1        12\n    3              0        34 \n\n\nWhich I want to turn into this:\n\nTrial    measurement       value\n\n    1              0        13\n    1              1         3\n    1              2         4\n    2              0       NaN\n    2              1        12\n    3              0        34 \n\n\nHow can I best do this?   \n\nI need this because I want to aggregate the data as instructed here, but I can't select my columns like that if they are in use as indices.\n"
'Suppose I have a DataFrame with some NaNs:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n&gt;&gt;&gt; df\n    0   1   2\n0   1   2   3\n1   4 NaN NaN\n2 NaN NaN   9\n\n\nWhat I need to do is replace every NaN with the first non-NaN value in the same column above it. It is assumed that the first row will never contain a NaN. So for the previous example the result would be\n\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n\n\nI can just loop through the whole DataFrame column-by-column, element-by-element and set the values directly, but is there an easy (optimally a loop-free) way of achieving this?\n'
"I have a pandas dataframe and I wish to divide it to 3 separate sets. I know that using train_test_split from sklearn.cross_validation, one can divide the data in two sets (train and test). However, I couldn't find any solution about splitting the data into three sets. Preferably, I'd like to have the indices of the original data. \n\nI know that a workaround would be to use train_test_split two times and somehow adjust the indices. But is there a more standard / built-in way to split the data into 3 sets instead of 2?\n"
'Is there a way to select random rows from a DataFrame in Pandas.\nIn R, using the car package, there is a useful function some(x, n) which is similar to head but selects, in this example, 10 rows at random from x.\nI have also looked at the slicing documentation and there seems to be nothing equivalent.\nUpdate\nNow using version 20. There is a sample method.\ndf.sample(n)\n'
'What I am trying to do is extract elevation data from a google maps API along a path specified by latitude and longitude coordinates as follows:\n\nfrom urllib2 import Request, urlopen\nimport json\n\npath1 = \'42.974049,-81.205203|42.974298,-81.195755\'\nrequest=Request(\'http://maps.googleapis.com/maps/api/elevation/json?locations=\'+path1+\'&amp;sensor=false\')\nresponse = urlopen(request)\nelevations = response.read()\n\n\nThis gives me a data that looks like this:\n\nelevations.splitlines()\n\n[\'{\',\n \'   "results" : [\',\n \'      {\',\n \'         "elevation" : 243.3462677001953,\',\n \'         "location" : {\',\n \'            "lat" : 42.974049,\',\n \'            "lng" : -81.205203\',\n \'         },\',\n \'         "resolution" : 19.08790397644043\',\n \'      },\',\n \'      {\',\n \'         "elevation" : 244.1318664550781,\',\n \'         "location" : {\',\n \'            "lat" : 42.974298,\',\n \'            "lng" : -81.19575500000001\',\n \'         },\',\n \'         "resolution" : 19.08790397644043\',\n \'      }\',\n \'   ],\',\n \'   "status" : "OK"\',\n \'}\']\n\n\nwhen putting into as DataFrame here is what I get:\n\n\n\npd.read_json(elevations)\n\n\nand here is what I want:\n\n\n\nI\'m not sure if this is possible, but mainly what I am looking for is a way to be able to put the elevation, latitude and longitude data together in a pandas dataframe (doesn\'t have to have fancy mutiline headers).\n\nIf any one can help or give some advice on working with this data that would be great! If you can\'t tell I haven\'t worked much with json data before...\n\nEDIT:\n\nThis method isn\'t all that attractive but seems to work:\n\ndata = json.loads(elevations)\nlat,lng,el = [],[],[]\nfor result in data[\'results\']:\n    lat.append(result[u\'location\'][u\'lat\'])\n    lng.append(result[u\'location\'][u\'lng\'])\n    el.append(result[u\'elevation\'])\ndf = pd.DataFrame([lat,lng,el]).T\n\n\nends up dataframe having columns latitude, longitude, elevation\n\n\n'
"I have the following DataFrame:\n\nIn [1]:\n\nimport pandas as pd\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf\nOut [1]:\n   a  b   c  d\n0  1  2  dd  5\n1  2  3  ee  9\n2  3  4  ff  1\n\n\nI would like to add a column 'e' which is the sum of column 'a', 'b' and 'd'.\n\nGoing across forums, I thought something like this would work:\n\ndf['e'] = df[['a','b','d']].map(sum)\n\n\nBut it didn't.\n\nI would like to know the appropriate operation with the list of columns ['a','b','d'] and df as inputs.\n"
"I have a simple DataFrame like the following:\n\n\n\nI want to select all values from the 'First Season' column and replace those that are over 1990 by 1. In this example, only Baltimore Ravens would have the 1996 replaced by 1 (keeping the rest of the data intact).\n\nI have used the following:\n\ndf.loc[(df['First Season'] &gt; 1990)] = 1\n\n\nBut, it replaces all the values in that row by 1, and not just the values in the 'First Season' column.\n\nHow can I replace just the values from that column?\n"
'I have a scenario where a user wants to apply several filters to a Pandas DataFrame or Series object.  Essentially, I want to efficiently chain a bunch of filtering (comparison operations) together that are specified at run-time by the user.\n\nThe filters should be additive (aka each one applied should narrow results).\n\nI\'m currently using reindex() but this creates a new object each time and copies the underlying data (if I understand the documentation correctly).  So, this could be really inefficient when filtering a big Series or DataFrame.\n\nI\'m thinking that using apply(), map(), or something similar might be better.  I\'m pretty new to Pandas though so still trying to wrap my head around everything.\n\nTL;DR\n\nI want to take a dictionary of the following form and apply each operation to a given Series object and return a \'filtered\' Series object.\n\nrelops = {\'&gt;=\': [1], \'&lt;=\': [1]}\n\n\nLong Example\n\nI\'ll start with an example of what I have currently and just filtering a single Series object.  Below is the function I\'m currently using:\n\n   def apply_relops(series, relops):\n        """\n        Pass dictionary of relational operators to perform on given series object\n        """\n        for op, vals in relops.iteritems():\n            op_func = ops[op]\n            for val in vals:\n                filtered = op_func(series, val)\n                series = series.reindex(series[filtered])\n        return series\n\n\nThe user provides a dictionary with the operations they want to perform:\n\n&gt;&gt;&gt; df = pandas.DataFrame({\'col1\': [0, 1, 2], \'col2\': [10, 11, 12]})\n&gt;&gt;&gt; print df\n&gt;&gt;&gt; print df\n   col1  col2\n0     0    10\n1     1    11\n2     2    12\n\n&gt;&gt;&gt; from operator import le, ge\n&gt;&gt;&gt; ops ={\'&gt;=\': ge, \'&lt;=\': le}\n&gt;&gt;&gt; apply_relops(df[\'col1\'], {\'&gt;=\': [1]})\ncol1\n1       1\n2       2\nName: col1\n&gt;&gt;&gt; apply_relops(df[\'col1\'], relops = {\'&gt;=\': [1], \'&lt;=\': [1]})\ncol1\n1       1\nName: col1\n\n\nAgain, the \'problem\' with my above approach is that I think there is a lot of possibly unnecessary copying of the data for the in-between steps.\n\nAlso, I would like to expand this so that the dictionary passed in can include the columns to operator on and filter an entire DataFrame based on the input dictionary.  However, I\'m assuming whatever works for the Series can be easily expanded to a DataFrame.\n'
"I have a problem viewing the following DataFrame: \n\nn = 100\nfoo = DataFrame(index=range(n))\nfoo['floats'] = np.random.randn(n)\nfoo\n\n\nThe problem is that it does not print all rows per default in ipython notebook, but I have to slice to view the resulting rows. Even the following option does not change the output:\n\npd.set_option('display.max_rows', 500)\n\n\nDoes anyone know how to display the whole array?\n"
"What is the most efficient way to organise the following pandas Dataframe:\n\ndata =\n\nPosition    Letter\n1           a\n2           b\n3           c\n4           d\n5           e\n\n\ninto a dictionary like alphabet[1 : 'a', 2 : 'b', 3 : 'c', 4 : 'd', 5 : 'e']?\n"
'I have a pandas DataFrame like following.\n\ndf = pd.DataFrame({\'id\' : [1,1,1,2,2,3,3,3,3,4,4,5,6,6,6,7,7],\n                \'value\'  : ["first","second","second","first",\n                            "second","first","third","fourth",\n                            "fifth","second","fifth","first",\n                            "first","second","third","fourth","fifth"]})\n\n\nI want to group this by ["id","value"] and get the first row of each group.\n\n        id   value\n0        1   first\n1        1  second\n2        1  second\n3        2   first\n4        2  second\n5        3   first\n6        3   third\n7        3  fourth\n8        3   fifth\n9        4  second\n10       4   fifth\n11       5   first\n12       6   first\n13       6  second\n14       6   third\n15       7  fourth\n16       7   fifth\n\n\nExpected outcome\n\n    id   value\n     1   first\n     2   first\n     3   first\n     4  second\n     5  first\n     6  first\n     7  fourth\n\n\nI tried following which only gives the first row of the DataFrame. Any help regarding this is appreciated.\n\nIn [25]: for index, row in df.iterrows():\n   ....:     df2 = pd.DataFrame(df.groupby([\'id\',\'value\']).reset_index().ix[0])\n\n'
"I am trying to fill none values in a Pandas dataframe with 0's for only some subset of columns.\n\nWhen I do:\n\nimport pandas as pd\ndf = pd.DataFrame(data={'a':[1,2,3,None],'b':[4,5,None,6],'c':[None,None,7,8]})\nprint df\ndf.fillna(value=0, inplace=True)\nprint df\n\n\nThe output:\n\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  5.0  NaN\n2  3.0  NaN  7.0\n3  NaN  6.0  8.0\n     a    b    c\n0  1.0  4.0  0.0\n1  2.0  5.0  0.0\n2  3.0  0.0  7.0\n3  0.0  6.0  8.0\n\n\nIt replaces every None with 0's. What I want to do is, only replace Nones in columns a and b, but not c.\n\nWhat is the best way of doing this?\n"
"I've a csv file without header, with a DateTime index. I want to rename the index and column name, but with df.rename() only the column name is renamed. Bug? I'm on version 0.12.0\n\nIn [2]: df = pd.read_csv(r'D:\\Data\\DataTimeSeries_csv//seriesSM.csv', header=None, parse_dates=[[0]], index_col=[0] )\n\nIn [3]: df.head()\nOut[3]: \n                   1\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\nIn [4]: df.rename(index={0:'Date'}, columns={1:'SM'}, inplace=True)\n\nIn [5]: df.head()\nOut[5]: \n                  SM\n0                   \n2002-06-18  0.112000\n2002-06-22  0.190333\n2002-06-26  0.134000\n2002-06-30  0.093000\n2002-07-04  0.098667\n\n"
"I have a dataframe with column names, and I want to find the one that contains a certain string, but does not exactly match it. I'm searching for 'spike' in column names like 'spike-2', 'hey spike', 'spiked-in' (the 'spike' part is always continuous). \n\nI want the column name to be returned as a string or a variable, so I access the column later with df['name'] or df[name] as normal. I've tried to find ways to do this, to no avail. Any tips?\n"
"I have a pandas dataframe with mixed type columns, and I'd like to apply sklearn's min_max_scaler to some of the columns.  Ideally, I'd like to do these transformations in place, but haven't figured out a way to do that yet.  I've written the following code that works:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\n\ndfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\nmin_max_scaler = preprocessing.MinMaxScaler()\n\ndef scaleColumns(df, cols_to_scale):\n    for col in cols_to_scale:\n        df[col] = pd.DataFrame(min_max_scaler.fit_transform(pd.DataFrame(dfTest[col])),columns=[col])\n    return df\n\ndfTest\n\n    A   B   C\n0    14.00   103.02  big\n1    90.20   107.26  small\n2    90.95   110.35  big\n3    96.27   114.23  small\n4    91.21   114.68  small\n\nscaled_df = scaleColumns(dfTest,['A','B'])\nscaled_df\n\nA   B   C\n0    0.000000    0.000000    big\n1    0.926219    0.363636    small\n2    0.935335    0.628645    big\n3    1.000000    0.961407    small\n4    0.938495    1.000000    small\n\n\nI'm curious if this is the preferred/most efficient way to do this transformation.  Is there a way I could use df.apply that would be better?  \n\nI'm also surprised I can't get the following code to work:\n\nbad_output = min_max_scaler.fit_transform(dfTest['A'])\n\nIf I pass an entire dataframe to the scaler it works:\n\ndfTest2 = dfTest.drop('C', axis = 1)\ngood_output = min_max_scaler.fit_transform(dfTest2)\ngood_output\n\nI'm confused why passing a series to the scaler fails.  In my full working code above I had hoped to just pass a series to the scaler then set the dataframe column = to the scaled series.  I've seen this question asked a few other places, but haven't found a good answer.  Any help understanding what's going on here would be greatly appreciated!\n"
"I'm trying to read a csv-file from given URL, using Python 3.x:\nimport pandas as pd\nimport requests\n\nurl = &quot;https://github.com/cs109/2014_data/blob/master/countries.csv&quot;\ns = requests.get(url).content\nc = pd.read_csv(s)\n\nI have the following error\n\n&quot;Expected file path name or file-like object, got &lt;class 'bytes'&gt; type&quot;\n\nHow can I fix this? I'm using Python 3.4\n"
'What is the easiest way to remove duplicate columns from a dataframe?\n\nI am reading a text file that has duplicate columns via:\n\nimport pandas as pd\n\ndf=pd.read_table(fname)\n\n\nThe column names are:\n\nTime, Time Relative, N2, Time, Time Relative, H2, etc...\n\n\nAll the Time and Time Relative columns contain the same data. I want:\n\nTime, Time Relative, N2, H2\n\n\nAll my attempts at dropping, deleting, etc  such as:\n\ndf=df.T.drop_duplicates().T\n\n\nResult in uniquely valued index errors:\n\nReindexing only valid with uniquely valued index objects\n\n\nSorry for being a Pandas noob. Any Suggestions would be appreciated.\n\n\n\nAdditional Details\n\nPandas version: 0.9.0\nPython Version: 2.7.3\nWindows 7\n(installed via Pythonxy 2.7.3.0)\n\ndata file (note: in the real file, columns are separated by tabs, here they are separated by 4 spaces):\n\nTime    Time Relative [s]    N2[%]    Time    Time Relative [s]    H2[ppm]\n2/12/2013 9:20:55 AM    6.177    9.99268e+001    2/12/2013 9:20:55 AM    6.177    3.216293e-005    \n2/12/2013 9:21:06 AM    17.689    9.99296e+001    2/12/2013 9:21:06 AM    17.689    3.841667e-005    \n2/12/2013 9:21:18 AM    29.186    9.992954e+001    2/12/2013 9:21:18 AM    29.186    3.880365e-005    \n... etc ...\n2/12/2013 2:12:44 PM    17515.269    9.991756+001    2/12/2013 2:12:44 PM    17515.269    2.800279e-005    \n2/12/2013 2:12:55 PM    17526.769    9.991754e+001    2/12/2013 2:12:55 PM    17526.769    2.880386e-005\n2/12/2013 2:13:07 PM    17538.273    9.991797e+001    2/12/2013 2:13:07 PM    17538.273    3.131447e-005\n\n'
'I have manipulated some data using pandas and now I want to carry out a batch save back to the database. This requires me to convert the dataframe into an array of tuples, with each tuple corresponding to a "row" of the dataframe.\n\nMy DataFrame looks something like:\n\nIn [182]: data_set\nOut[182]: \n  index data_date   data_1  data_2\n0  14303 2012-02-17  24.75   25.03 \n1  12009 2012-02-16  25.00   25.07 \n2  11830 2012-02-15  24.99   25.15 \n3  6274  2012-02-14  24.68   25.05 \n4  2302  2012-02-13  24.62   24.77 \n5  14085 2012-02-10  24.38   24.61 \n\n\nI want to convert it to an array of tuples like:\n\n[(datetime.date(2012,2,17),24.75,25.03),\n(datetime.date(2012,2,16),25.00,25.07),\n...etc. ]\n\n\nAny suggestion on how I can efficiently do this?\n'
'Is there a pandas built-in way to apply two different aggregating functions f1, f2 to the same column df["returns"], without having to call agg() multiple times?\n\nExample dataframe:\n\nimport pandas as pd\nimport datetime as dt\n\npd.np.random.seed(0)\ndf = pd.DataFrame({\n         "date"    :  [dt.date(2012, x, 1) for x in range(1, 11)], \n         "returns" :  0.05 * np.random.randn(10), \n         "dummy"   :  np.repeat(1, 10)\n}) \n\n\nThe syntactically wrong, but intuitively right, way to do it would be:\n\n# Assume `f1` and `f2` are defined for aggregating.\ndf.groupby("dummy").agg({"returns": f1, "returns": f2})\n\n\nObviously, Python doesn\'t allow duplicate keys. Is there any other manner for expressing the input to agg()? Perhaps a list of tuples [(column, function)] would work better, to allow multiple functions applied to the same column? But agg() seems like it only accepts a dictionary.\n\nIs there a workaround for this besides defining an auxiliary function that just applies both of the functions inside of it? (How would this work with aggregation anyway?)\n'
"I am filtering rows in a dataframe by values in two columns.\n\nFor some reason the OR operator behaves like I would expect AND operator to behave and vice versa.\n\nMy test code:\n\nimport pandas as pd\n\ndf = pd.DataFrame({'a': range(5), 'b': range(5) })\n\n# let's insert some -1 values\ndf['a'][1] = -1\ndf['b'][1] = -1\ndf['a'][3] = -1\ndf['b'][4] = -1\n\ndf1 = df[(df.a != -1) &amp; (df.b != -1)]\ndf2 = df[(df.a != -1) | (df.b != -1)]\n\nprint pd.concat([df, df1, df2], axis=1,\n                keys = [ 'original df', 'using AND (&amp;)', 'using OR (|)',])\n\n\nAnd the result:\n\n      original df      using AND (&amp;)      using OR (|)    \n             a  b              a   b             a   b\n0            0  0              0   0             0   0\n1           -1 -1            NaN NaN           NaN NaN\n2            2  2              2   2             2   2\n3           -1  3            NaN NaN            -1   3\n4            4 -1            NaN NaN             4  -1\n\n[5 rows x 6 columns]\n\n\nAs you can see, the AND operator drops every row in which at least one value equals -1. On the other hand, the OR operator requires both values to be equal to -1 to drop them. I would expect exactly the opposite result. Could anyone explain this behavior, please?\n\nI am using pandas 0.13.1.\n"
'when my function f is called with a variable I want to check if var is a pandas dataframe:\n\ndef f(var):\n    if var == pd.DataFrame():\n        print "do stuff"\n\n\nI guess the solution might be quite simple but even with \n\ndef f(var):\n    if var.values != None:\n        print "do stuff"\n\n\nI can\'t get it to work like expected. \n'
"I'm trying to replace the values in one column of a dataframe. The column ('female') only contains the values 'female' and 'male'. \n\nI have tried the following:\n\nw['female']['female']='1'\nw['female']['male']='0' \n\n\nBut receive the exact same copy of the previous results.\n\nI would ideally like to get some output which resembles the following loop element-wise.\n\nif w['female'] =='female':\n    w['female'] = '1';\nelse:\n    w['female'] = '0';\n\n\nI've looked through the gotchas documentation (http://pandas.pydata.org/pandas-docs/stable/gotchas.html) but cannot figure out why nothing happens.\n\nAny help will be appreciated.\n"
"I want to apply a function with arguments to a series in python pandas:\n\nx = my_series.apply(my_function, more_arguments_1)\ny = my_series.apply(my_function, more_arguments_2)\n...\n\n\nThe documentation describes support for an apply method, but it doesn't accept any arguments.  Is there a different method that accepts arguments?  Alternatively, am I missing a simple workaround?\n\nUpdate (October 2017):  Note that since this question was originally asked that pandas apply() has been updated to handle positional and keyword arguments and the documentation link above now reflects that and shows how to include either type of argument.\n"
"I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.\n\nAny ideas? \n"
"My data can have multiple events on a given date or NO events on a date. I take these events, get a count by date and plot them.  However, when I plot them, my two series don't always match.    \n\nidx = pd.date_range(df['simpleDate'].min(), df['simpleDate'].max())\ns = df.groupby(['simpleDate']).size()\n\n\nIn the above code idx becomes a range of say 30 dates. 09-01-2013 to 09-30-2013\nHowever S may only have 25 or 26 days because no events happened for a given date. I then get an AssertionError as the sizes dont match when I try to plot:\n\nfig, ax = plt.subplots()    \nax.bar(idx.to_pydatetime(), s, color='green')\n\n\nWhat's the proper way to tackle this? Do I want to remove dates with no values from IDX or (which I'd rather do) is add to the series the missing date with a count of 0. I'd rather have a full graph of 30 days with 0 values. If this approach is right, any suggestions on how to get started? Do I need some sort of dynamic reindex function?\n\nHere's a snippet of S ( df.groupby(['simpleDate']).size()  ), notice no entries for 04 and 05.\n\n09-02-2013     2\n09-03-2013    10\n09-06-2013     5\n09-07-2013     1\n\n"
"I have following 2 data frames:\n\ndf_a =\n\n     mukey  DI  PI\n0   100000  35  14\n1  1000005  44  14\n2  1000006  44  14\n3  1000007  43  13\n4  1000008  43  13\n\ndf_b = \n    mukey  niccdcd\n0  190236        4\n1  190237        6\n2  190238        7\n3  190239        4\n4  190240        7\n\n\nWhen I try to join these 2 dataframes:\n\njoin_df = df_a.join(df_b,on='mukey',how='left')\n\n\nI get the error:\n\n*** ValueError: columns overlap but no suffix specified: Index([u'mukey'], dtype='object')\n\n\nWhy is this so? The dataframes do have common 'mukey' values.\n"
"I have a few Pandas DataFrames sharing the same value scale, but having different columns and indices. When invoking df.plot(), I get separate plot images. what I really want is to have them all in the same plot as subplots, but I'm unfortunately failing to come up with a solution to how and would highly appreciate some help. \n"
'I have a list of Pandas dataframes that I would like to combine into one Pandas dataframe.  I am using Python 2.7.10 and Pandas 0.16.2\n\nI created the list of dataframes from:\n\nimport pandas as pd\ndfs = []\nsqlall = "select * from mytable"\n\nfor chunk in pd.read_sql_query(sqlall , cnxn, chunksize=10000):\n    dfs.append(chunk)\n\n\nThis returns a list of dataframes\n\ntype(dfs[0])\nOut[6]: pandas.core.frame.DataFrame\n\ntype(dfs)\nOut[7]: list\n\nlen(dfs)\nOut[8]: 408\n\n\nHere is some sample data\n\n# sample dataframes\nd1 = pd.DataFrame({\'one\' : [1., 2., 3., 4.], \'two\' : [4., 3., 2., 1.]})\nd2 = pd.DataFrame({\'one\' : [5., 6., 7., 8.], \'two\' : [9., 10., 11., 12.]})\nd3 = pd.DataFrame({\'one\' : [15., 16., 17., 18.], \'two\' : [19., 10., 11., 12.]})\n\n# list of dataframes\nmydfs = [d1, d2, d3]\n\n\nI would like to combine d1, d2, and d3 into one pandas dataframe.  Alternatively, a method of reading a large-ish table directly into a dataframe when using the chunksize option would be very helpful.  \n'
"Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?\n\nI tried the following\n\ndf['A'==0]['B'] = np.nan\n\n\nand\n\ndf['A'==0]['B'].values.fill(np.nan)\n\n\nwithout success.\n"
"I am looking for an efficient way to remove unwanted parts from strings in a DataFrame column.\n\nData looks like:\n\n    time    result\n1    09:00   +52A\n2    10:00   +62B\n3    11:00   +44a\n4    12:00   +30b\n5    13:00   -110a\n\n\nI need to trim these data to:\n\n    time    result\n1    09:00   52\n2    10:00   62\n3    11:00   44\n4    12:00   30\n5    13:00   110\n\n\nI tried .str.lstrip('+-') and .str.rstrip('aAbBcC'), but got an error:  \n\nTypeError: wrapper() takes exactly 1 argument (2 given)\n\n\nAny pointers would be greatly appreciated!\n"
"I am trying to read an excel file this way :\n\nnewFile = pd.ExcelFile(PATH\\FileName.xlsx)\nParsedData = pd.io.parsers.ExcelFile.parse(newFile)\n\n\nwhich throws an error that says two arguments expected, I don't know what the second argument is and also what I am trying to achieve here is to convert an Excel file to a DataFrame, Am I doing it the right way? or is there any other way to do this using pandas?\n"
"I am sure there is an obvious way to do this but cant think of anything slick right now.\n\nBasically instead of raising exception I would like to get True or False to see if a value exists in pandas df index.\n\nimport pandas as pd\ndf = pd.DataFrame({'test':[1,2,3,4]}, index=['a','b','c','d'])\ndf.loc['g']  # (should give False)\n\n\nWhat I have working now is the following\n\nsum(df.index == 'g')\n\n"
'I\'m new to python and pandas.  I\'m trying to get a tsv file loaded into a pandas DataFrame.  \n\nThis is what I\'m trying and the error I\'m getting:\n\n&gt;&gt;&gt; df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n\nTraceback (most recent call last):\n  File "&lt;pyshell#28&gt;", line 1, in &lt;module&gt;\n    df1 = DataFrame(csv.reader(open(\'c:/~/trainSetRel3.txt\'), delimiter=\'\\t\'))\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py", line 318, in __init__\n    raise PandasError(\'DataFrame constructor not properly called!\')\nPandasError: DataFrame constructor not properly called!\n\n'
'I tried:\n\nx=pandas.DataFrame(...)\ns = x.take([0], axis=1)\n\n\nAnd s gets a DataFrame, not a Series.\n'
"df = pd.DataFrame({'Col1': ['Bob', 'Joe', 'Bill', 'Mary', 'Joe'],\n                   'Col2': ['Joe', 'Steve', 'Bob', 'Bob', 'Steve'],\n                   'Col3': np.random.random(5)})\n\n\nWhat is the best way to return the unique values of 'Col1' and 'Col2'?\n\nThe desired output is \n\n'Bob', 'Joe', 'Bill', 'Mary', 'Steve'\n\n"
"I'm new to pandas and trying to figure out how to add multiple columns to pandas simultaneously.  Any help here is appreciated.  Ideally I would like to do this in one step rather than multiple repeated steps...\n\nimport pandas as pd\n\ndf = {'col_1': [0, 1, 2, 3],\n        'col_2': [4, 5, 6, 7]}\ndf = pd.DataFrame(df)\n\ndf[[ 'column_new_1', 'column_new_2','column_new_3']] = [np.nan, 'dogs',3]  #thought this would work here...\n\n"
"I have the following DataFrame where one of the columns is an object (list type cell):\n\ndf=pd.DataFrame({'A':[1,2],'B':[[1,2],[1,2]]})\ndf\nOut[458]: \n   A       B\n0  1  [1, 2]\n1  2  [1, 2]\n\n\nMy expected output is: \n\n   A  B\n0  1  1\n1  1  2\n3  2  1\n4  2  2\n\n\nWhat should I do to achieve this?\n\n\n\nRelated question \n\npandas: When cell contents are lists, create a row for each element in the list\n\nGood question and answer but only handle one column with list(In my answer the self-def function will work for multiple columns, also the accepted answer is use the most time consuming apply , which is not recommended, check more info When should I ever want to use pandas apply() in my code?) \n"
'I have a list of items that likely has some export issues.  I would like to get a list of the duplicate items so I can manually compare them.  When I try to use pandas duplicated method, it only returns the first duplicate.  Is there a a way to get all of the duplicates and not just the first one?\n\nA small subsection of my dataset looks like this:\n\nID,ENROLLMENT_DATE,TRAINER_MANAGING,TRAINER_OPERATOR,FIRST_VISIT_DATE\n1536D,12-Feb-12,"06DA1B3-Lebanon NH",,15-Feb-12\nF15D,18-May-12,"06405B2-Lebanon NH",,25-Jul-12\n8096,8-Aug-12,"0643D38-Hanover NH","0643D38-Hanover NH",25-Jun-12\nA036,1-Apr-12,"06CB8CF-Hanover NH","06CB8CF-Hanover NH",9-Aug-12\n8944,19-Feb-12,"06D26AD-Hanover NH",,4-Feb-12\n1004E,8-Jun-12,"06388B2-Lebanon NH",,24-Dec-11\n11795,3-Jul-12,"0649597-White River VT","0649597-White River VT",30-Mar-12\n30D7,11-Nov-12,"06D95A3-Hanover NH","06D95A3-Hanover NH",30-Nov-11\n3AE2,21-Feb-12,"06405B2-Lebanon NH",,26-Oct-12\nB0FE,17-Feb-12,"06D1B9D-Hartland VT",,16-Feb-12\n127A1,11-Dec-11,"064456E-Hanover NH","064456E-Hanover NH",11-Nov-12\n161FF,20-Feb-12,"0643D38-Hanover NH","0643D38-Hanover NH",3-Jul-12\nA036,30-Nov-11,"063B208-Randolph VT","063B208-Randolph VT",\n475B,25-Sep-12,"06D26AD-Hanover NH",,5-Nov-12\n151A3,7-Mar-12,"06388B2-Lebanon NH",,16-Nov-12\nCA62,3-Jan-12,,,\nD31B,18-Dec-11,"06405B2-Lebanon NH",,9-Jan-12\n20F5,8-Jul-12,"0669C50-Randolph VT",,3-Feb-12\n8096,19-Dec-11,"0649597-White River VT","0649597-White River VT",9-Apr-12\n14E48,1-Aug-12,"06D3206-Hanover NH",,\n177F8,20-Aug-12,"063B208-Randolph VT","063B208-Randolph VT",5-May-12\n553E,11-Oct-12,"06D95A3-Hanover NH","06D95A3-Hanover NH",8-Mar-12\n12D5F,18-Jul-12,"0649597-White River VT","0649597-White River VT",2-Nov-12\nC6DC,13-Apr-12,"06388B2-Lebanon NH",,\n11795,27-Feb-12,"0643D38-Hanover NH","0643D38-Hanover NH",19-Jun-12\n17B43,11-Aug-12,,,22-Oct-12\nA036,11-Aug-12,"06D3206-Hanover NH",,19-Jun-12\n\n\nMy code looks like this currently:\n\ndf_bigdata_duplicates = df_bigdata[df_bigdata.duplicated(cols=\'ID\')]\n\n\nThere area a couple duplicate items. But, when I use the above code, I only get the first item.  In the API reference, I see how I can get the last item, but I would like to have all of them so I can visually inspect them to see why I am getting the discrepancy.  So, in this example I would like to get all three A036 entries and both 11795 entries and any other duplicated entries, instead of the just first one.  Any help is most appreciated.\n'
"I've got a Pandas DataFrame and I want to combine the 'lat' and 'long' columns to form a tuple.\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 205482 entries, 0 to 209018\nData columns:\nMonth           205482  non-null values\nReported by     205482  non-null values\nFalls within    205482  non-null values\nEasting         205482  non-null values\nNorthing        205482  non-null values\nLocation        205482  non-null values\nCrime type      205482  non-null values\nlong            205482  non-null values\nlat             205482  non-null values\ndtypes: float64(4), object(5)\n\n\nThe code I tried to use was:\n\ndef merge_two_cols(series): \n    return (series['lat'], series['long'])\n\nsample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n\n\nHowever, this returned the following error:\n\n---------------------------------------------------------------------------\n AssertionError                            Traceback (most recent call last)\n&lt;ipython-input-261-e752e52a96e6&gt; in &lt;module&gt;()\n      2     return (series['lat'], series['long'])\n      3 \n----&gt; 4 sample['lat_long'] = sample.apply(merge_two_cols, axis=1)\n      5\n\n\n...\n\nAssertionError: Block shape incompatible with manager \n\n\nHow can I solve this problem?\n"
'I am receiving the following error when importing pandas in a Python program\n\nmonas-mbp:book mona$ sudo pip install python-dateutil\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python\nCleaning up...\nmonas-mbp:book mona$ python t1.py\nNo module named dateutil.parser\nTraceback (most recent call last):\n  File "t1.py", line 4, in &lt;module&gt;\n    import pandas as pd\n  File "/Library/Python/2.7/site-packages/pandas/__init__.py", line 6, in &lt;module&gt;\n    from . import hashtable, tslib, lib\n  File "tslib.pyx", line 31, in init pandas.tslib (pandas/tslib.c:48782)\nImportError: No module named dateutil.parser\n\n\nAlso here\'s the program:\n\nimport codecs \nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\n\nusers = {"Angelica": {"Blues Traveler": 3.5, "Broken Bells": 2.0,\n                      "Norah Jones": 4.5, "Phoenix": 5.0,\n                      "Slightly Stoopid": 1.5,\n                      "The Strokes": 2.5, "Vampire Weekend": 2.0},\n\n         "Bill":{"Blues Traveler": 2.0, "Broken Bells": 3.5,\n                 "Deadmau5": 4.0, "Phoenix": 2.0,\n                 "Slightly Stoopid": 3.5, "Vampire Weekend": 3.0},\n\n         "Chan": {"Blues Traveler": 5.0, "Broken Bells": 1.0,\n                  "Deadmau5": 1.0, "Norah Jones": 3.0, "Phoenix": 5,\n                  "Slightly Stoopid": 1.0},\n\n         "Dan": {"Blues Traveler": 3.0, "Broken Bells": 4.0,\n                 "Deadmau5": 4.5, "Phoenix": 3.0,\n                 "Slightly Stoopid": 4.5, "The Strokes": 4.0,\n                 "Vampire Weekend": 2.0},\n\n         "Hailey": {"Broken Bells": 4.0, "Deadmau5": 1.0,\n                    "Norah Jones": 4.0, "The Strokes": 4.0,\n                    "Vampire Weekend": 1.0},\n\n         "Jordyn":  {"Broken Bells": 4.5, "Deadmau5": 4.0,\n                     "Norah Jones": 5.0, "Phoenix": 5.0,\n                     "Slightly Stoopid": 4.5, "The Strokes": 4.0,\n                     "Vampire Weekend": 4.0},\n\n         "Sam": {"Blues Traveler": 5.0, "Broken Bells": 2.0,\n                 "Norah Jones": 3.0, "Phoenix": 5.0,\n                 "Slightly Stoopid": 4.0, "The Strokes": 5.0},\n\n         "Veronica": {"Blues Traveler": 3.0, "Norah Jones": 5.0,\n                      "Phoenix": 4.0, "Slightly Stoopid": 2.5,\n                      "The Strokes": 3.0}\n        }\n\n\n\nclass recommender:\n\n    def __init__(self, data, k=1, metric=\'pearson\', n=5):\n        """ initialize recommender\n        currently, if data is dictionary the recommender is initialized\n        to it.\n        For all other data types of data, no initialization occurs\n        k is the k value for k nearest neighbor\n        metric is which distance formula to use\n        n is the maximum number of recommendations to make"""\n        self.k = k\n        self.n = n\n        self.username2id = {}\n        self.userid2name = {}\n        self.productid2name = {}\n        # for some reason I want to save the name of the metric\n        self.metric = metric\n        if self.metric == \'pearson\':\n            self.fn = self.pearson\n        #\n        # if data is dictionary set recommender data to it\n        #\n        if type(data).__name__ == \'dict\':\n            self.data = data\n\n    def convertProductID2name(self, id):\n        """Given product id number return product name"""\n        if id in self.productid2name:\n            return self.productid2name[id]\n        else:\n            return id\n\n\n    def userRatings(self, id, n):\n        """Return n top ratings for user with id"""\n        print ("Ratings for " + self.userid2name[id])\n        ratings = self.data[id]\n        print(len(ratings))\n        ratings = list(ratings.items())\n        ratings = [(self.convertProductID2name(k), v)\n                   for (k, v) in ratings]\n        # finally sort and return\n        ratings.sort(key=lambda artistTuple: artistTuple[1],\n                     reverse = True)\n        ratings = ratings[:n]\n        for rating in ratings:\n            print("%s\\t%i" % (rating[0], rating[1]))\n\n\n\n\n    def loadBookDB(self, path=\'\'):\n        """loads the BX book dataset. Path is where the BX files are\n        located"""\n        self.data = {}\n        i = 0\n        #\n        # First load book ratings into self.data\n        #\n        f = codecs.open(path + "BX-Book-Ratings.csv", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            user = fields[0].strip(\'"\')\n            book = fields[1].strip(\'"\')\n            rating = int(fields[2].strip().strip(\'"\'))\n            if user in self.data:\n                currentRatings = self.data[user]\n            else:\n                currentRatings = {}\n            currentRatings[book] = rating\n            self.data[user] = currentRatings\n        f.close()\n        #\n        # Now load books into self.productid2name\n        # Books contains isbn, title, and author among other fields\n        #\n        f = codecs.open(path + "BX-Books.csv", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #separate line into fields\n            fields = line.split(\';\')\n            isbn = fields[0].strip(\'"\')\n            title = fields[1].strip(\'"\')\n            author = fields[2].strip().strip(\'"\')\n            title = title + \' by \' + author\n            self.productid2name[isbn] = title\n        f.close()\n        #\n        #  Now load user info into both self.userid2name and\n        #  self.username2id\n        #\n        f = codecs.open(path + "BX-Users.csv", \'r\', \'utf8\')\n        for line in f:\n            i += 1\n            #print(line)\n            #separate line into fields\n            fields = line.split(\';\')\n            userid = fields[0].strip(\'"\')\n            location = fields[1].strip(\'"\')\n            if len(fields) &gt; 3:\n                age = fields[2].strip().strip(\'"\')\n            else:\n                age = \'NULL\'\n            if age != \'NULL\':\n                value = location + \'  (age: \' + age + \')\'\n            else:\n                value = location\n            self.userid2name[userid] = value\n            self.username2id[location] = userid\n        f.close()\n        print(i)\n\n\n    def pearson(self, rating1, rating2):\n        sum_xy = 0\n        sum_x = 0\n        sum_y = 0\n        sum_x2 = 0\n        sum_y2 = 0\n        n = 0\n        for key in rating1:\n            if key in rating2:\n                n += 1\n                x = rating1[key]\n                y = rating2[key]\n                sum_xy += x * y\n                sum_x += x\n                sum_y += y\n                sum_x2 += pow(x, 2)\n                sum_y2 += pow(y, 2)\n        if n == 0:\n            return 0\n        # now compute denominator\n        denominator = (sqrt(sum_x2 - pow(sum_x, 2) / n)\n                       * sqrt(sum_y2 - pow(sum_y, 2) / n))\n        if denominator == 0:\n            return 0\n        else:\n            return (sum_xy - (sum_x * sum_y) / n) / denominator\n\n\n    def computeNearestNeighbor(self, username):\n        """creates a sorted list of users based on their distance to\n        username"""\n        distances = []\n        for instance in self.data:\n            if instance != username:\n                distance = self.fn(self.data[username],\n                                   self.data[instance])\n                distances.append((instance, distance))\n        # sort based on distance -- closest first\n        distances.sort(key=lambda artistTuple: artistTuple[1],\n                       reverse=True)\n        return distances\n\n    def recommend(self, user):\n       """Give list of recommendations"""\n       recommendations = {}\n       # first get list of users  ordered by nearness\n       nearest = self.computeNearestNeighbor(user)\n       #\n       # now get the ratings for the user\n       #\n       userRatings = self.data[user]\n       #\n       # determine the total distance\n       totalDistance = 0.0\n       for i in range(self.k):\n          totalDistance += nearest[i][1]\n       # now iterate through the k nearest neighbors\n       # accumulating their ratings\n       for i in range(self.k):\n          # compute slice of pie \n          weight = nearest[i][1] / totalDistance\n          # get the name of the person\n          name = nearest[i][0]\n          # get the ratings for this person\n          neighborRatings = self.data[name]\n          # get the name of the person\n          # now find bands neighbor rated that user didn\'t\n          for artist in neighborRatings:\n             if not artist in userRatings:\n                if artist not in recommendations:\n                   recommendations[artist] = (neighborRatings[artist]\n                                              * weight)\n                else:\n                   recommendations[artist] = (recommendations[artist]\n                                              + neighborRatings[artist]\n                                              * weight)\n       # now make list from dictionary\n       recommendations = list(recommendations.items())\n       recommendations = [(self.convertProductID2name(k), v)\n                          for (k, v) in recommendations]\n       # finally sort and return\n       recommendations.sort(key=lambda artistTuple: artistTuple[1],\n                            reverse = True)\n       # Return the first n items\n       return recommendations[:self.n]\n\nr = recommender(users)\n# The author implementation\nr.loadBookDB(\'/Users/mona/Downloads/BX-Dump/\')\n\nratings = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Book-Ratings.csv\', sep=";", quotechar="\\"", escapechar="\\\\")\nbooks = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Books.csv\', sep=";", quotechar="\\"", escapechar="\\\\")\nusers = pd.read_csv(\'/Users/danialt/BX-CSV-Dump/BX-Users.csv\', sep=";", quotechar="\\"", escapechar="\\\\")\n\n\n\npivot_rating = ratings.pivot(index=\'User-ID\', columns=\'ISBN\', values=\'Book-Rating\')\n\n'
"I have a dataframe that consist of hundreds of columns, and I need to see all column names.\n\nWhat I did:\n\nIn[37]:\ndata_all2.columns\n\n\nThe output is:\n\nOut[37]:\nIndex(['customer_id', 'incoming', 'outgoing', 'awan', 'bank', 'family', 'food',\n       'government', 'internet', 'isipulsa',\n       ...\n       'overdue_3months_feature78', 'overdue_3months_feature79',\n       'overdue_3months_feature80', 'overdue_3months_feature81',\n       'overdue_3months_feature82', 'overdue_3months_feature83',\n       'overdue_3months_feature84', 'overdue_3months_feature85',\n       'overdue_3months_feature86', 'loan_overdue_3months_total_y'],\n      dtype='object', length=102)\n\n\nHow do I show all columns, instead of a truncated list?\n"
'Here is my code:\n\nimport pandas as pd\n\ndata = pd.DataFrame({\'Odd\':[1,3,5,6,7,9], \'Even\':[0,2,4,6,8,10]})\n\nfor i in reversed(data):\n    print(data[\'Odd\'], data[\'Even\'])\n\n\nWhen I run this code, i get the following error:\n\nTraceback (most recent call last):\n  File "C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py", line 665, in _get_item_cache\n    return cache[item]\nKeyError: 5\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File "C:\\Users\\*****\\Documents\\******\\********\\****.py", line 5, in &lt;module&gt;\n    for i in reversed(data):\n  File "C:\\Python33\\lib\\site-packages\\pandas\\core\\frame.py", line 2003, in __getitem__\n    return self._get_item_cache(key)\n  File "C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py", line 667, in _get_item_cache\n    values = self._data.get(item)\n  File "C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py", line 1656, in get\n    _, block = self._find_block(item)\n  File "C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py", line 1936, in _find_block\n    self._check_have(item)\n  File "C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py", line 1943, in _check_have\n    raise KeyError(\'no item named %s\' % com.pprint_thing(item))\nKeyError: \'no item named 5\'\n\n\nWhy am I getting this error?\nHow can I fix that?\nWhat is the right way to reverse pandas.DataFrame?\n'
"I need to use different functions to treat numeric columns and string columns. What I am doing now is really dumb:\n\nallc = list((agg.loc[:, (agg.dtypes==np.float64)|(agg.dtypes==np.int)]).columns)\nfor y in allc:\n    treat_numeric(agg[y])    \n\nallc = list((agg.loc[:, (agg.dtypes!=np.float64)&amp;(agg.dtypes!=np.int)]).columns)\nfor y in allc:\n    treat_str(agg[y])    \n\n\nIs there a more elegant way to do this? E.g.\n\nfor y in agg.columns:\n    if(dtype(agg[y]) == 'string'):\n          treat_str(agg[y])\n    elif(dtype(agg[y]) != 'string'):\n          treat_numeric(agg[y])\n\n"
"I'm working with a large csv file and the next to last column has a string of text that I want to split by a specific delimiter. I was wondering if there is a simple way to do this using pandas or python?\n\nCustNum  CustomerName     ItemQty  Item   Seatblocks                 ItemExt\n32363    McCartney, Paul      3     F04    2:218:10:4,6                   60\n31316    Lennon, John        25     F01    1:13:36:1,12 1:13:37:1,13     300\n\n\nI want to split by the space(' ') and then the colon(':') in the Seatblocks column, but each cell would result in a different number of columns. I have a function to rearrange the columns so the Seatblocks column is at the end of the sheet, but I'm not sure what to do from there. I can do it in excel with the built in text-to-columns function and a quick macro, but my dataset has too many records for excel to handle.\n\nUltimately, I want to take records such John Lennon's and create multiple lines, with the info from each set of seats on a separate line.\n"
"I have the following pandas dataframe Top15:\n    \n\nI create a column that estimates the number of citable documents per person:\n\nTop15['PopEst'] = Top15['Energy Supply'] / Top15['Energy Supply per Capita']\nTop15['Citable docs per Capita'] = Top15['Citable documents'] / Top15['PopEst']\n\n\nI want to know the correlation between the number of citable documents per capita and the energy supply per capita. So I use the .corr() method (Pearson's correlation):\n\ndata = Top15[['Citable docs per Capita','Energy Supply per Capita']]\ncorrelation = data.corr(method='pearson')\n\n\nI want to return a single number, but the result is:\n\n"
"I would like to merge two DataFrames, and keep the index from the first frame as the index on the merged dataset.  However, when I do the merge, the resulting DataFrame has integer index.  How can I specify that I want to keep the index from the left data frame?\n\nIn [4]: a = pd.DataFrame({'col1': {'a': 1, 'b': 2, 'c': 3}, \n                          'to_merge_on': {'a': 1, 'b': 3, 'c': 4}})\n\nIn [5]: b = pd.DataFrame({'col2': {0: 1, 1: 2, 2: 3}, \n                          'to_merge_on': {0: 1, 1: 3, 2: 5}})\n\nIn [6]: a\nOut[6]:\n   col1  to_merge_on\na     1            1\nb     2            3\nc     3            4\n\nIn [7]: b\nOut[7]:\n   col2  to_merge_on\n0     1            1\n1     2            3\n2     3            5\n\nIn [8]: a.merge(b, how='left')\nOut[8]:\n   col1  to_merge_on  col2\n0     1            1   1.0\n1     2            3   2.0\n2     3            4   NaN\n\nIn [9]: _.index\nOut[9]: Int64Index([0, 1, 2], dtype='int64')\n\n\nEDIT: Switched to example code that can be easily reproduced\n"
"The simple task of adding a row to a pandas.DataFrame object seems to be hard to accomplish. There are 3 stackoverflow questions relating to this, none of which give a working answer.\n\nHere is what I'm trying to do. I have a DataFrame of which I already know the shape as well as the names of the rows and columns.\n\n&gt;&gt;&gt; df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n&gt;&gt;&gt; df\n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny  NaN  NaN  NaN  NaN\nz  NaN  NaN  NaN  NaN\n\n\nNow, I have a function to compute the values of the rows iteratively. How can I fill in one of the rows with either a dictionary or a pandas.Series ? Here are various attempts that have failed:\n\n&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} \n&gt;&gt;&gt; df['y'] = y\nAssertionError: Length of values does not match length of index\n\n\nApparently it tried to add a column instead of a row.\n\n&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} \n&gt;&gt;&gt; df.join(y)\nAttributeError: 'builtin_function_or_method' object has no attribute 'is_unique'\n\n\nVery uninformative error message.\n\n&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} \n&gt;&gt;&gt; df.set_value(index='y', value=y)\nTypeError: set_value() takes exactly 4 arguments (3 given)\n\n\nApparently that is only for setting individual values in the dataframe.\n\n&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} \n&gt;&gt;&gt; df.append(y)\nException: Can only append a Series if ignore_index=True\n\n\nWell, I don't want to ignore the index, otherwise here is the result:\n\n&gt;&gt;&gt; df.append(y, ignore_index=True)\n     a    b    c    d\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3    1    5    2    3\n\n\nIt did align the column names with the values, but lost the row labels.\n\n&gt;&gt;&gt; y = {'a':1, 'b':5, 'c':2, 'd':3} \n&gt;&gt;&gt; df.ix['y'] = y\n&gt;&gt;&gt; df\n                                  a                                 b  \\\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n                                  c                                 d\nx                               NaN                               NaN\ny  {'a': 1, 'c': 2, 'b': 5, 'd': 3}  {'a': 1, 'c': 2, 'b': 5, 'd': 3}\nz                               NaN                               NaN\n\n\nThat also failed miserably.\n\nSo how do you do it ?\n"
"Is there any function that would be the equivalent of a combination of df.isin() and df[col].str.contains()? \n\nFor example, say I have the series\ns = pd.Series(['cat','hat','dog','fog','pet']), and I want to find all places where s contains any of ['og', 'at'], I would want to get everything but 'pet'.\n\nI have a solution, but it's rather inelegant:\n\nsearchfor = ['og', 'at']\nfound = [s.str.contains(x) for x in searchfor]\nresult = pd.DataFrame[found]\nresult.any()\n\n\nIs there a better way to do this?\n"
'So my dataset has some information by location for n dates. The problem is each date is actually a different column header. For example the CSV looks like\n\nlocation    name    Jan-2010    Feb-2010    March-2010\nA           "test"  12          20          30\nB           "foo"   18          20          25\n\n\nWhat I would like is for it to look like\n\nlocation    name    Date        Value\nA           "test"  Jan-2010    12       \nA           "test"  Feb-2010    20\nA           "test"  March-2010  30\nB           "foo"   Jan-2010    18       \nB           "foo"   Feb-2010    20\nB           "foo"   March-2010  25\n\n\nproblem is I don\'t know how many dates are in the column (though I know they will always start after name)\n'
"\n\nUsing Python Pandas I am trying to find the Country &amp; Place with the maximum value.\n\nThis returns the maximum value:\n\ndata.groupby(['Country','Place'])['Value'].max()\n\n\nBut how do I get the corresponding Country and Place name?\n"
"I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).\nI already figured out how to kind-of do this and I am currently using:\n\ndf.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\n\n\nThis seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)?\n\nIn case this is yet unclear, I would like to turn:\n\n    col \n1     a\n2     0\n\n\ninto:\n\n       col \n1     stra\n2     str0\n\n"
"I am struggling with the seemingly very simple thing.I have a pandas data frame containing very long string.\n\ndf = pd.DataFrame({'one' : ['one', 'two', \n      'This is very long string very long string very long string veryvery long string']})\n\n\nNow when I try to print the same, I do not see the full string I rather see only part of the string.\n\nI tried following options \n\n\nusing print(df.iloc[2]) \nusing to_html\nusing to_string\nOne of the stackoverflow answer suggested to increase column width by\nusing pandas display option, that did not work either.\nI also did not get how set_printoptions will help me.\n\n\nAny ideas appreciated. Looks very simple, but not able to get it!  \n"
'I have seen many answers posted to questions on Stack Overflow involving the use of the Pandas method apply. I have also seen users commenting under them saying that "apply is slow, and should be avoided". \n\nI have read many articles on the topic of performance that explain apply is slow. I have also seen a disclaimer in the docs about how apply is simply a convenience function for passing UDFs (can\'t seem to find that now). So, the general consensus is that apply should be avoided if possible. However, this raises the following questions: \n\n\nIf apply is so bad, then why is it in the API?\nHow and when should I make my code apply-free?\nAre there ever any situations where apply is good (better than other possible solutions)? \n\n'
'I want to find rows that contain a string, like so:\n\nDF[DF.col.str.contains("foo")]\n\n\nHowever, this fails because some elements are NaN:\n\n\n  ValueError: cannot index with vector containing NA / NaN values\n\n\nSo I resort to the obfuscated\n\nDF[DF.col.notnull()][DF.col.dropna().str.contains("foo")]\n\n\nIs there a better way?\n'
"Target\n\nI have a Pandas data frame, as shown below, with multiple columns and would like to get the total of column, MyColumn.\n\n\n\nData Frame - df:\n\nprint df\n\n           X           MyColumn  Y              Z   \n0          A           84        13.0           69.0   \n1          B           76         77.0          127.0   \n2          C           28         69.0           16.0   \n3          D           28         28.0           31.0   \n4          E           19         20.0           85.0   \n5          F           84        193.0           70.0   \n\n\n\n\nMy attempt:\n\nI have attempted to get the sum of the column using groupby and .sum():\n\nTotal = df.groupby['MyColumn'].sum()\n\nprint Total\n\n\nThis causes  the following error:\n\nTypeError: 'instancemethod' object has no attribute '__getitem__'\n\n\n\n\nExpected Output\n\nI'd have expected the output to be as followed:\n\n319\n\n\nOr alternatively, I would like df to be edited with a new row entitled TOTAL containing the total: \n\n           X           MyColumn  Y              Z   \n0          A           84        13.0           69.0   \n1          B           76         77.0          127.0   \n2          C           28         69.0           16.0   \n3          D           28         28.0           31.0   \n4          E           19         20.0           85.0   \n5          F           84        193.0           70.0   \nTOTAL                  319\n\n"
"I have a DataFrame, and I want to replace the values in a particular column that exceed a value with zero. I had thought this was a way of achieving this:\ndf[df.my_channel &gt; 20000].my_channel = 0\n\nIf I copy the channel into a new data frame it's simple:\ndf2 = df.my_channel \n\ndf2[df2 &gt; 20000] = 0\n\nThis does exactly what I want, but seems not to work with the channel as part of the original DataFrame.\n"
"It's easy to turn a list of lists into a pandas dataframe:\n\nimport pandas as pd\ndf = pd.DataFrame([[1,2,3],[3,4,5]])\n\n\nBut how do I turn df back into a list of lists?\n\nlol = df.what_to_do_now?\nprint lol\n# [[1,2,3],[3,4,5]]\n\n"
"I have an existing plot that was created with pandas like this:\n\ndf['myvar'].plot(kind='bar')\n\n\nThe y axis is format as float and I want to change the y axis to percentages.  All of the solutions I found use ax.xyz syntax and I can only place code below the line above that creates the plot (I cannot add ax=ax to the line above.) \n\nHow can I format the y axis as percentages without changing the line above?\n\nHere is the solution I found but requires that I redefine the plot:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as mtick\n\ndata = [8,12,15,17,18,18.5]\nperc = np.linspace(0,100,len(data))\n\nfig = plt.figure(1, (7,4))\nax = fig.add_subplot(1,1,1)\n\nax.plot(perc, data)\n\nfmt = '%.0f%%' # Format you want the ticks, e.g. '40%'\nxticks = mtick.FormatStrFormatter(fmt)\nax.xaxis.set_major_formatter(xticks)\n\nplt.show()\n\n\nLink to the above solution: Pyplot: using percentage on x axis\n"
"Suppose I have a pandas data frame df: \n\nI want to calculate the column wise mean of a data frame.\n\nThis is easy: \n\ndf.apply(average) \n\n\nthen the column wise range max(col) - min(col). This is easy again: \n\ndf.apply(max) - df.apply(min)\n\n\nNow for each element I want to subtract its column's mean and divide by its column's range. I am not sure how to do that\n\nAny help/pointers are much appreciated. \n"
"I am trying to access the index of a row in a function applied across an entire DataFrame in Pandas. I have something like this:\n\ndf = pandas.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\n&gt;&gt;&gt; df\n   a  b  c\n0  1  2  3\n1  4  5  6\n\n\nand I'll define a function that access elements with a given row\n\ndef rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n\n\nI can apply it like so:\n\ndf['d'] = df.apply(rowFunc, axis=1)\n&gt;&gt;&gt; df\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n\n\nAwesome! Now what if I want to incorporate the index into my function?\nThe index of any given row in this DataFrame before adding d would be Index([u'a', u'b', u'c', u'd'], dtype='object'), but I want the 0 and 1. So I can't just access row.index.\n\nI know I could create a temporary column in the table where I store the index, but I'm wondering if it is stored in the row object somewhere.\n"
'I have a DataFrame like this one:\n\nIn [7]:\nframe.head()\nOut[7]:\nCommunications and Search   Business    General Lifestyle\n0   0.745763    0.050847    0.118644    0.084746\n0   0.333333    0.000000    0.583333    0.083333\n0   0.617021    0.042553    0.297872    0.042553\n0   0.435897    0.000000    0.410256    0.153846\n0   0.358974    0.076923    0.410256    0.153846\n\n\nIn here, I want to ask how to get column name which has maximum value for each row, the desired output is like this:\n\nIn [7]:\n    frame.head()\n    Out[7]:\n    Communications and Search   Business    General Lifestyle   Max\n    0   0.745763    0.050847    0.118644    0.084746           Communications \n    0   0.333333    0.000000    0.583333    0.083333           Business  \n    0   0.617021    0.042553    0.297872    0.042553           Communications \n    0   0.435897    0.000000    0.410256    0.153846           Communications \n    0   0.358974    0.076923    0.410256    0.153846           Business \n\n'
"I know that if I use randn,\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\n\ngives me what I am looking for, but with elements from a normal distribution. But what if I just wanted random integers?\n\nrandint works by providing a range, but not an array like randn does. So how do I do this with random integers between some range?\n"
"My dataframe has a DOB column (example format 1/1/2016) which by default gets converted to pandas dtype 'object': DOB   object\n\nConverting this to date format with df['DOB'] = pd.to_datetime(df['DOB']), the date gets converted to: 2016-01-26 and its dtype is: DOB   datetime64[ns].\n\nNow I want to convert this date format to 01/26/2016 or in any other general date formats. How do I do it?\n\nWhatever the method I try, it always shows the date in 2016-01-26 format.\n"
"I have a spreadsheet like this:\n\nLocality    2005    2006    2007    2008    2009\n\nABBOTSFORD  427000  448000  602500  600000  638500\nABERFELDIE  534000  600000  735000  710000  775000\nAIREYS INLET459000  440000  430000  517500  512500\n\n\nI don't want to manually swap the column with the row. Could it be possible        to use pandas reading data to a list as this:\n\ndata['ABBOTSFORD']=[427000,448000,602500,600000,638500]\ndata['ABERFELDIE']=[534000,600000,735000,710000,775000]\ndata['AIREYS INLET']=[459000,440000,430000,517500,512500]\n\n"
'When I run the program, Pandas gives \'Future warning\' like below every time.\n\nD:\\Python\\lib\\site-packages\\pandas\\core\\frame.py:3581: FutureWarning: rename with inplace=True  will return None from pandas 0.11 onward\n  " from pandas 0.11 onward", FutureWarning) \n\n\nI got the msg, but I just want to stop Pandas showing such msg again and again, is there any buildin parameter that I can set to let Pandas not pop up the \'Future warning\' ?\n'
'I use pandas to write to excel file in the following fashion:\n\nimport pandas\n\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\') \n\ndata_filtered.to_excel(writer, "Main", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n\n\nMasterfile.xlsx already consists of number of different tabs. However, it does not yet contain "Main".\n\nPandas correctly writes to "Main" sheet, unfortunately it also deletes all other tabs.\n'
'I\'m confused about the rules Pandas uses when deciding that a selection from a dataframe is a copy of the original dataframe, or a view on the original.\n\nIf I have, for example,\n\ndf = pd.DataFrame(np.random.randn(8,8), columns=list(\'ABCDEFGH\'), index=range(1,9))\n\n\nI understand that a query returns a copy so that something like\n\nfoo = df.query(\'2 &lt; index &lt;= 5\')\nfoo.loc[:,\'E\'] = 40\n\n\nwill have no effect on the original dataframe, df. I also understand that scalar or named slices return a view, so that assignments to these, such as \n\ndf.iloc[3] = 70\n\n\nor \n\ndf.ix[1,\'B\':\'E\'] = 222\n\n\nwill change df. But I\'m lost when it comes to more complicated cases. For example, \n\ndf[df.C &lt;= df.B] = 7654321\n\n\nchanges df, but\n\ndf[df.C &lt;= df.B].ix[:,\'B\':\'E\']\n\n\ndoes not.\n\nIs there a simple rule that Pandas is using that I\'m just missing? What\'s going on in these specific cases; and in particular, how do I change all values (or a subset of values) in a dataframe that satisfy a particular query (as I\'m attempting to do in the last example above)?\n\n\n\nNote: This is not the same as this question; and I have read the documentation, but am not enlightened by it. I\'ve also read through the "Related" questions on this topic, but I\'m still missing the simple rule Pandas is using, and how I\'d apply it to — for example —\xa0modify the values (or a subset of values) in a dataframe that satisfy a particular query.\n'
'I have a pandas DataFrame, df_test.  It contains a column \'size\' which represents size in bytes.  I\'ve calculated KB, MB, and GB using the following code:\n\ndf_test = pd.DataFrame([\n    {\'dir\': \'/Users/uname1\', \'size\': 994933},\n    {\'dir\': \'/Users/uname2\', \'size\': 109338711},\n])\n\ndf_test[\'size_kb\'] = df_test[\'size\'].astype(int).apply(lambda x: locale.format("%.1f", x / 1024.0, grouping=True) + \' KB\')\ndf_test[\'size_mb\'] = df_test[\'size\'].astype(int).apply(lambda x: locale.format("%.1f", x / 1024.0 ** 2, grouping=True) + \' MB\')\ndf_test[\'size_gb\'] = df_test[\'size\'].astype(int).apply(lambda x: locale.format("%.1f", x / 1024.0 ** 3, grouping=True) + \' GB\')\n\ndf_test\n\n\n             dir       size       size_kb   size_mb size_gb\n0  /Users/uname1     994933      971.6 KB    0.9 MB  0.0 GB\n1  /Users/uname2  109338711  106,776.1 KB  104.3 MB  0.1 GB\n\n[2 rows x 5 columns]\n\n\nI\'ve run this over 120,000 rows and time it takes about 2.97 seconds per column * 3 = ~9 seconds according to %timeit.\n\nIs there anyway I can make this faster?  For example, can I instead of returning one column at a time from apply and running it 3 times, can I return all three columns in one pass to insert back into the original dataframe?\n\nThe other questions I\'ve found all want to take multiple values and return a single value. I want to take a single value and return multiple columns.\n'
'I am kind of getting stuck on extracting value of one variable conditioning on another variable. For example, the following dataframe:\n\nA  B\np1 1\np1 2\np3 3\np2 4\n\n\nHow can I get the value of A when B=3? Every time when I extracted the value of A, I got an object, not a string. \n'
'Any help on this problem will be greatly appreciated.\n\nSo basically I want to run a query to my SQL database and store the returned data as Pandas data structure.\n\nI have attached code for query.\n\nI am reading the documentation on Pandas, but I have problem to identify the return type of my query.\n\nI tried to print the query result, but it doesn\'t give any useful information.\n\nThanks!!!! \n\nfrom sqlalchemy import create_engine\n\nengine2 = create_engine(\'mysql://THE DATABASE I AM ACCESSING\')\nconnection2 = engine2.connect()\ndataid = 1022\nresoverall = connection2.execute("\n  SELECT \n      sum(BLABLA) AS BLA,\n      sum(BLABLABLA2) AS BLABLABLA2,\n      sum(SOME_INT) AS SOME_INT,\n      sum(SOME_INT2) AS SOME_INT2,\n      100*sum(SOME_INT2)/sum(SOME_INT) AS ctr,\n      sum(SOME_INT2)/sum(SOME_INT) AS cpc\n   FROM daily_report_cooked\n   WHERE campaign_id = \'%s\'", %dataid)\n\n\nSo I sort of want to understand what\'s the format/datatype of my variable "resoverall" and how to put it with PANDAS data structure.\n'
'Is it possible to only merge some columns? I have a DataFrame df1 with columns x, y, z, and df2 with columns x, a ,b, c, d, e, f, etc.\n\nI want to merge the two DataFrames on x, but I only want to merge columns df2.a, df2.b - not the entire DataFrame. \n\nThe result would be a DataFrame with x, y, z, a, b.\n\nI could merge then delete the unwanted columns, but it seems like there is a better method.\n'
'So I have initialized an empty pandas DataFrame and I would like to iteratively append lists (or Series) as rows in this DataFrame. What is the best way of doing this?\n'
'I have a dataframe with unix times and prices in it. I want to convert the index column so that it shows in human readable dates. \n\nSo for instance I have date as 1349633705 in the index column but I\'d want it to show as 10/07/2012 (or at least 10/07/2012 18:15). \n\nFor some context, here is the code I\'m working with and what I\'ve tried already:\n\nimport json\nimport urllib2\nfrom datetime import datetime\nresponse = urllib2.urlopen(\'http://blockchain.info/charts/market-price?&amp;format=json\')\ndata = json.load(response)   \ndf = DataFrame(data[\'values\'])\ndf.columns = ["date","price"]\n#convert dates \ndf.date = df.date.apply(lambda d: datetime.strptime(d, "%Y-%m-%d"))\ndf.index = df.date   \n\n\nAs you can see I\'m using\ndf.date = df.date.apply(lambda d: datetime.strptime(d, "%Y-%m-%d")) here which doesn\'t work since I\'m working with integers, not strings. I think I need to use datetime.date.fromtimestamp but I\'m not quite sure how to apply this to the whole of df.date. \n\nThanks.\n'
'Dataframe.resample() works only with timeseries data. I cannot find a way of getting every nth row from non-timeseries data. What is the best method?\n'
'I have a dataframe like this:\n\n   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n\n\nCalling \n\nIn [10]: print df.groupby("A")["B"].sum()\n\n\nwill return \n\nA\n1    1.615586\n2    0.421821\n3    0.463468\n4    0.643961\n\n\nNow I would like to do "the same" for column "C". Because that column contains strings, sum() doesn\'t work (although you might think that it would concatenate the strings). What I would really like to see is a list or set of the strings for each group, i.e. \n\nA\n1    {This, string}\n2    {is, !}\n3    {a}\n4    {random}\n\n\nI have been trying to find ways to do this. \n\nSeries.unique() (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unique.html) doesn\'t work, although\n\ndf.groupby("A")["B"]\n\n\nis a\n\npandas.core.groupby.SeriesGroupBy object\n\n\nso I was hoping any Series method would work. Any ideas?\n'
"I have a pandas dataframe with the following columns;\n\nDate              Time\n01-06-2013      23:00:00\n02-06-2013      01:00:00\n02-06-2013      21:00:00\n02-06-2013      22:00:00\n02-06-2013      23:00:00\n03-06-2013      01:00:00\n03-06-2013      21:00:00\n03-06-2013      22:00:00\n03-06-2013      23:00:00\n04-06-2013      01:00:00\n\n\nHow do I combine data['Date'] &amp; data['Time']  to get the following? Is there a way of doing it using pd.to_datetime?\n\nDate\n01-06-2013 23:00:00\n02-06-2013 01:00:00\n02-06-2013 21:00:00\n02-06-2013 22:00:00\n02-06-2013 23:00:00\n03-06-2013 01:00:00\n03-06-2013 21:00:00\n03-06-2013 22:00:00\n03-06-2013 23:00:00\n04-06-2013 01:00:00\n\n"
'I\'m trying to reprogram my Stata code into Python for speed improvements, and I was pointed in the direction of PANDAS.  I am, however, having a hard time wrapping my head around how to process the data.\n\nLet\'s say I want to iterate over all values in the column head \'ID.\' If that ID matches a specific number, then I want to change two corresponding values FirstName and LastName.\n\nIn Stata it looks like this:\n\nreplace FirstName = "Matt" if ID==103\nreplace LastName =  "Jones" if ID==103\n\n\nSo this replaces all values in FirstName that correspond with values of ID == 103 to Matt.  \n\nIn PANDAS, I\'m trying something like this\n\ndf = read_csv("test.csv")\nfor i in df[\'ID\']:\n    if i ==103:\n          ...\n\n\nNot sure where to go from here.  Any ideas?\n'
"I have a dataframe generated from Python's Pandas package. How can I generate  heatmap using DataFrame from pandas package. \n\nimport numpy as np \nfrom pandas import *\n\nIndex= ['aaa','bbb','ccc','ddd','eee']\nCols = ['A', 'B', 'C','D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index= Index, columns=Cols)\n\n&gt;&gt;&gt; df\n          A         B         C         D\naaa  2.431645  1.248688  0.267648  0.613826\nbbb  0.809296  1.671020  1.564420  0.347662\nccc  1.501939  1.126518  0.702019  1.596048\nddd  0.137160  0.147368  1.504663  0.202822\neee  0.134540  3.708104  0.309097  1.641090\n&gt;&gt;&gt; \n\n"
'I want to read a .xlsx file using the Pandas Library of python and port the data to a postgreSQL table. \n\nAll I could do up until now is:\n\nimport pandas as pd\ndata = pd.ExcelFile("*File Name*")\n\n\n\nNow I know that the step got executed successfully, but I want to know how i can parse the excel file that has been read so that I can understand how the data in the excel maps to the data in the variable data. \nI learnt that data is a Dataframe object if I\'m not wrong. So How do i parse this dataframe object to extract each line row by row.\n'
'I have a pandas dataframe with the following column names:\n\nResult1, Test1, Result2, Test2, Result3, Test3, etc...\n\nI want to drop all the columns whose name contains the word "Test". The numbers of such columns is not static but depends on a previous function.\n\nHow can I do that?\n'
'I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.\n\nFor example, I want to drop all rows which have the string "XYZ" as a substring in the column C of the data frame.\n\nCan this be implemented in an efficient way using .drop() method?\n'
"I have a dataframe in pandas and I'm trying to figure out what the types of its values are. I am unsure what the type is of column 'Test'. However, when I run myFrame['Test'].dtype, I get;\n\ndtype('O')\n\n\nWhat does this mean?\n"
"I'm using Pandas data frames. I have a initial data frame, say D. I extract two data frames from it like this:\nA = D[D.label == k]\nB = D[D.label != k]\n\nI want to combine A and B so I can have them as one DataFrame, something like a union operation. The order of the data is not important. However, when we sample A and B from D, they retain their indexes from D.\n"
"I have a DataFrame:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'foo.aa': [1, 2.1, np.nan, 4.7, 5.6, 6.8],\n                   'foo.fighters': [0, 1, np.nan, 0, 0, 0],\n                   'foo.bars': [0, 0, 0, 0, 0, 1],\n                   'bar.baz': [5, 5, 6, 5, 5.6, 6.8],\n                   'foo.fox': [2, 4, 1, 0, 0, 5],\n                   'nas.foo': ['NA', 0, 1, 0, 0, 0],\n                   'foo.manchu': ['NA', 0, 0, 0, 0, 0],})\n\n\nI want to select values of 1 in columns starting with foo.. Is there a better way to do it other than:\n\ndf2 = df[(df['foo.aa'] == 1)|\n(df['foo.fighters'] == 1)|\n(df['foo.bars'] == 1)|\n(df['foo.fox'] == 1)|\n(df['foo.manchu'] == 1)\n]\n\n\nSomething similar to writing something like:\n\ndf2= df[df.STARTS_WITH_FOO == 1]\n\n\nThe answer should print out a DataFrame like this:\n\n   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo\n0      5.0     1.0         0             0        2         NA      NA\n1      5.0     2.1         0             1        4          0       0\n2      6.0     NaN         0           NaN        1          0       1\n5      6.8     6.8         1             0        5          0       0\n\n[4 rows x 7 columns]\n\n"
"Say I have the following DataFrame\n\n\nLetter    Number\nA          1\nB          2\nC          3\nD          4\n\n\nWhich can be obtained through the following code\n\nimport pandas as pd\n\nletters=pd.Series(('A', 'B', 'C', 'D'))\nnumbers=pd.Series((1, 2, 3, 4))\nkeys=('Letters', 'Numbers')\ndf=pd.concat((letters, numbers), axis=1, keys=keys)\n\n\nNow I want to get the value C from the column Letters.\n\nThe command line\n\ndf[df.Letters=='C'].Letters\n\n\nwill return\n\n\n2    C\nName: Letters, dtype: object\n\n\nHow can I get only the value C and not the whole two line output?\n"
'How do I convert data from a Scikit-learn Bunch object to a Pandas DataFrame?\n\nfrom sklearn.datasets import load_iris\nimport pandas as pd\ndata = load_iris()\nprint(type(data))\ndata1 = pd. # Is there a Pandas method to accomplish this?\n\n'
"This topic hasn't been addressed in a while, here or elsewhere. Is there a solution converting a SQLAlchemy &lt;Query object&gt; to a pandas DataFrame?\n\nPandas has the capability to use pandas.read_sql but this requires use of raw SQL. I have two reasons for wanting to avoid it: 1) I already have everything using the ORM (a good reason in and of itself) and 2) I'm using python lists as part of the query (eg: .db.session.query(Item).filter(Item.symbol.in_(add_symbols) where Item is my model class and add_symbols is a list). This is the equivalent of SQL SELECT ... from ... WHERE ... IN. \n\nIs anything possible?\n"
"Is there a way to reorder columns in pandas dataframe based on my personal preference (i.e. not alphabetically or numerically sorted, but more like following certain conventions)?\n\nSimple example:\n\nframe = pd.DataFrame({\n        'one thing':[1,2,3,4],\n        'second thing':[0.1,0.2,1,2],\n        'other thing':['a','e','i','o']})\n\n\nproduces this:\n\n   one thing other thing  second thing\n0          1           a           0.1\n1          2           e           0.2\n2          3           i           1.0\n3          4           o           2.0\n\n\nBut instead, I would like this:\n\n   one thing second thing  other thing\n0          1           0.1           a\n1          2           0.2           e\n2          3           1.0           i\n3          4           2.0           o\n\n\n(Please, provide a generic solution rather than specific to this case. Many thanks.)\n"
"I'm looking for a way to do the equivalent to the SQL \n\nSELECT DISTINCT col1, col2 FROM dataframe_table\n\n\nThe pandas sql comparison doesn't have anything about distinct.\n\n.unique() only works for a single column, so I suppose I could concat the columns, or put them in a list/tuple and compare that way, but this seems like something pandas should do in a more native way.  \n\nAm I missing something obvious, or is there no way to do this?\n"
"The data I have to work with is a bit messy.. It has header names inside of its data. How can I choose a row from an existing pandas dataframe and make it (rename it to) a column header?\n\nI want to do something like:\n\nheader = df[df['old_header_name1'] == 'new_header_name1']\n\ndf.columns = header\n\n"
"I want to set the dtypes of multiple columns in pd.Dataframe (I have a file that I've had to manually parse into a list of lists, as the file was not amenable for pd.read_csv)\n\nimport pandas as pd\nprint pd.DataFrame([['a','1'],['b','2']],\n                   dtype={'x':'object','y':'int'},\n                   columns=['x','y'])\n\n\nI get\n\nValueError: entry not a 2- or 3- tuple\n\n\nThe only way I can set them is by looping through each column variable and recasting with astype. \n\ndtypes = {'x':'object','y':'int'}\nmydata = pd.DataFrame([['a','1'],['b','2']],\n                      columns=['x','y'])\nfor c in mydata.columns:\n    mydata[c] = mydata[c].astype(dtypes[c])\nprint mydata['y'].dtype   #=&gt; int64\n\n\nIs there a better way?\n"
'There is DataFrame.to_sql method, but it works only for mysql, sqlite and oracle databases. I cant pass to this method postgres connection or sqlalchemy engine.\n'
"I have my data in pandas data frame as follows:\n\ndf1 = pd.DataFrame({'A':['yes','yes','yes','yes','no','no','yes','yes','yes','no'],\n                   'B':['yes','no','no','no','yes','yes','no','yes','yes','no']})\n\n\nSo, my data looks like this\n\n----------------------------\nindex         A        B\n0           yes      yes\n1           yes       no\n2           yes       no\n3           yes       no\n4            no      yes\n5            no      yes\n6           yes       no\n7           yes      yes\n8           yes      yes\n9            no       no\n-----------------------------\n\n\nI would like to transform it to another data frame.  The expected output can be shown in the following python script:\n\noutput = pd.DataFrame({'A':['no','no','yes','yes'],'B':['no','yes','no','yes'],'count':[1,2,4,3]})\n\n\nSo, my expected output looks like this\n\n--------------------------------------------\nindex      A       B       count\n--------------------------------------------\n0         no       no        1\n1         no      yes        2\n2        yes       no        4\n3        yes      yes        3\n--------------------------------------------\n\n\nActually, I can achieve to find all combinations and count them by using the following command: mytable = df1.groupby(['A','B']).size()\n\nHowever, it turns out that such combinations are in a single column.  I would like to separate each value in a combination into different column and also add one more column for the result of counting.  Is it possible to do that?  May I have your suggestions?  Thank you in advance.\n"
"I have two pandas dataframes:\n\nfrom pandas import DataFrame\ndf1 = DataFrame({'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'col3':[5,6]})     \n\n\nWhat is the best practice to get their cartesian product (of course without writing it explicitly like me)?\n\n#df1, df2 cartesian product\ndf_cartesian = DataFrame({'col1':[1,2,1,2],'col2':[3,4,3,4],'col3':[5,5,6,6]})\n\n"
"In the pandas library many times there is an option to change the object inplace such as with the following statement...\n\ndf.dropna(axis='index', how='all', inplace=True)\n\n\nI am curious what is being returned as well as how the object is handled when inplace=True is passed vs. when inplace=False.\n\nAre all operations modifying self when inplace=True? And when inplace=False is a new object created immediately such as new_df = self and then new_df is returned?\n"
"I have a Data Frame column with numeric values:\n\ndf['percentage'].head()\n46.5\n44.2\n100.0\n42.12\n\n\nI want to see the column as bin counts:\n\nbins = [0, 1, 5, 10, 25, 50, 100]\n\n\nHow can I get the result as bins with their value counts?\n\n[0, 1] bin amount\n[1, 5] etc \n[5, 10] etc \n......\n\n"
'I have a data frame with three string columns. I know that the only one value in the 3rd column is valid for every combination of the first two. To clean the data I have to group by data frame by first two columns and select most common value of the third column for each combination.\n\nMy code:\n\nimport pandas as pd\nfrom scipy import stats\n\nsource = pd.DataFrame({\'Country\' : [\'USA\', \'USA\', \'Russia\',\'USA\'], \n                  \'City\' : [\'New-York\', \'New-York\', \'Sankt-Petersburg\', \'New-York\'],\n                  \'Short name\' : [\'NY\',\'New\',\'Spb\',\'NY\']})\n\nprint source.groupby([\'Country\',\'City\']).agg(lambda x: stats.mode(x[\'Short name\'])[0])\n\n\nLast line of code doesn\'t work, it says "Key error \'Short name\'" and if I try to group only by City, then I got an AssertionError. What can I do fix it?\n'
"This is probably easy, but I have the following data:\n\nIn data frame 1:\n\nindex dat1\n0     9\n1     5\n\n\nIn data frame 2:\n\nindex dat2\n0     7\n1     6\n\n\nI want a data frame with the following form:\n\nindex dat1  dat2\n0     9     7\n1     5     6\n\n\nI've tried using the append method, but I get a cross join (i.e. cartesian product).\n\nWhat's the right way to do this?\n"
"Using this as a starting point:\n\na = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\nOut[8]: \n  one  two three\n0   10  1.2   4.2\n1   15  70   0.03\n2    8   5     0\n\n\nI want to use something like an if statement within pandas. \n\nif df['one'] &gt;= df['two'] and df['one'] &lt;= df['three']:\n    df['que'] = df['one']\n\n\nBasically, check each row via the if statement, create new column. \n\nThe docs say to use .all but there is no example...\n"
'Are for loops really "bad"? If not, in what situation(s) would they be better than using a more conventional "vectorized" approach?1\n\nI am familiar with the concept of "vectorization", and how pandas employs vectorized techniques to speed up computation. Vectorized functions broadcast operations over the entire series or DataFrame to achieve speedups much greater than conventionally iterating over the data. \n\nHowever, I am quite surprised to see a lot of code (including from answers on Stack Overflow) offering solutions to problems that involve looping through data using for loops and list comprehensions. The documentation and API say that loops are "bad", and that one should "never" iterate over arrays, series, or DataFrames. So, how come I sometimes see users suggesting loop-based solutions?\n\n\n\n1 - While it is true that the question sounds somewhat broad, the truth is that there are very specific situations when for loops are usually better than conventionally iterating over data. This post aims to capture this for posterity.  \n'
"I have a column in my dataframe like this:\nrange\n&quot;(2,30)&quot;\n&quot;(50,290)&quot;\n&quot;(400,1000)&quot;\n... \n\nand I want to replace the , comma with - dash. I'm currently using this method but nothing is changed.\norg_info_exc['range'].replace(',', '-', inplace=True)\n\nCan anybody help?\n"
'I have two data frames df1 and df2, where df2 is a subset of df1. How do I get a new data frame (df3) which is the difference between the two data frames?\n\nIn other word, a data frame that has all the rows/columns in df1 that are not in df2?\n\n\n'
'I have a csv file with the name params.csv. I opened up ipython qtconsole and created a pandas dataframe using:\n\nimport pandas\nparamdata = pandas.read_csv(\'params.csv\', names=paramnames)\n\n\nwhere, paramnames is a python list of string objects. Example of paramnames (the length of actual list is 22):\n\nparamnames = ["id",\n"fc",\n"mc",\n"markup",\n"asplevel",\n"aspreview",\n"reviewpd"]\n\n\nAt the ipython prompt if I type paramdata and press enter then I do not get the dataframe with columns and values as shown in examples on Pandas website. Instead, I get information about the dataframe. I get:\n\nIn[35]: paramdata\nOut[35]: \n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 59 entries, 0 to 58\nData columns:\nid                    59  non-null values\nfc                    59  non-null values\nmc                    59  non-null values\nmarkup                59  non-null values\nasplevel              59  non-null values\naspreview             59  non-null values\nreviewpd              59  non-null values\n\n\nIf I type paramdata[\'mc\'] then I do get the values as expected for the mc column. I have two questions:\n\n(1) In the examples on the pandas website (see, for example, the output of df here: http://pandas.sourceforge.net/indexing.html#additional-column-access) typing the name of the dataframe gives the actual data. Why am I getting information about the dataframe as shown above instead of the actual data? Do I need to set some output options somewhere?\n\n(2) How do I output all columns in the dataframe to the screen without having to type their names, i.e., without having to type something like paramdata[[\'id\',\'fc\',\'mc\']]. \n\nI am using pandas version 0.8. \n\nThank you.\n'
"I'm simply trying to access named pandas columns by an integer. \n\nYou can select a row by location using df.ix[3].\n\nBut how to select a column by integer?\n\nMy dataframe:\n\ndf=pandas.DataFrame({'a':np.random.rand(5), 'b':np.random.rand(5)})\n\n"
"I can use pandas dropna() functionality to remove rows with some or all columns set as NA's. Is there an equivalent function for dropping rows with all columns having value 0?\n\nP   kt  b   tt  mky depth\n1   0   0   0   0   0\n2   0   0   0   0   0\n3   0   0   0   0   0\n4   0   0   0   0   0\n5   1.1 3   4.5 2.3 9.0\n\n\nIn this example, we would like to drop the first 4 rows from the data frame.\n\nthanks!\n"
"I have a dataframe with this type of data (too many columns):\n\ncol1        int64\ncol2        int64\ncol3        category\ncol4        category\ncol5        category\n\n\nColumns seems like this:\n\nName: col3, dtype: category\nCategories (8, object): [B, C, E, G, H, N, S, W]\n\n\nI want to convert all value in columns to integer like this:\n\n[1, 2, 3, 4, 5, 6, 7, 8]\n\n\nI solved this for one column by this:\n\ndataframe['c'] = pandas.Categorical.from_array(dataframe.col3).codes\n\n\nNow I have two columns in my dataframe - old col3 and new c and need to drop old columns. \n\nThat's bad practice. It's work but in my dataframe many columns and I don't want do it manually.  \n\nHow do this pythonic and just cleverly?\n"
'I have an array of floats (some normal numbers, some nans) that is coming out of an apply on a pandas dataframe.\n\nFor some reason, numpy.isnan is failing on this array, however as shown below, each element is a float, numpy.isnan runs correctly on each element, the type of the variable is definitely a numpy array.\n\nWhat\'s going on?!\n\nset([type(x) for x in tester])\nOut[59]: {float}\n\ntester\nOut[60]: \narray([-0.7000000000000001, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n   nan, nan], dtype=object)\n\nset([type(x) for x in tester])\nOut[61]: {float}\n\nnp.isnan(tester)\nTraceback (most recent call last):\n\nFile "&lt;ipython-input-62-e3638605b43c&gt;", line 1, in &lt;module&gt;\nnp.isnan(tester)\n\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\n\nset([np.isnan(x) for x in tester])\nOut[65]: {False, True}\n\ntype(tester)\nOut[66]: numpy.ndarray\n\n'
'As of August 2017, Pandas DataFame.apply() is unfortunately still limited to working with a single core, meaning that a multi-core machine will waste the majority of its compute-time when you run df.apply(myfunc, axis=1). \n\nHow can you use all your cores to run apply on a dataframe in parallel? \n'
"If I have a table like this:  \n\ndf = pd.DataFrame({\n         'hID': [101, 102, 103, 101, 102, 104, 105, 101],\n         'dID': [10, 11, 12, 10, 11, 10, 12, 10],\n         'uID': ['James', 'Henry', 'Abe', 'James', 'Henry', 'Brian', 'Claude', 'James'],\n         'mID': ['A', 'B', 'A', 'B', 'A', 'A', 'A', 'C']\n})\n\n\nI can do count(distinct hID) in Qlik to come up with count of 5 for unique hID.  How do I do that in python using a pandas dataframe?  Or maybe a numpy array?  Similarly, if were to do count(hID) I will get 8 in Qlik.  What is the equivalent way to do it in pandas?\n"
"I've taken my Series and coerced it to a datetime column of dtype=datetime64[ns] (though only need day resolution...not sure how to change). \n\nimport pandas as pd\ndf = pd.read_csv('somefile.csv')\ncolumn = df['date']\ncolumn = pd.to_datetime(column, coerce=True)\n\n\nbut plotting doesn't work:\n\nipdb&gt; column.plot(kind='hist')\n*** TypeError: ufunc add cannot use operands with types dtype('&lt;M8[ns]') and dtype('float64')\n\n\nI'd like to plot a histogram that just shows the count of dates by week, month, or year.\n\nSurely there is a way to do this in pandas?\n"
"I have a DataFrame with a MultiIndex created after some grouping:\n\nimport numpy as np\nimport pandas as p\nfrom numpy.random import randn\n\ndf = p.DataFrame({\n    'A' : ['a1', 'a1', 'a2', 'a3']\n  , 'B' : ['b1', 'b2', 'b3', 'b4']\n  , 'Vals' : randn(4)\n}).groupby(['A', 'B']).sum()\n\ndf\n\nOutput&gt;            Vals\nOutput&gt; A  B           \nOutput&gt; a1 b1 -1.632460\nOutput&gt;    b2  0.596027\nOutput&gt; a2 b3 -0.619130\nOutput&gt; a3 b4 -0.002009\n\n\nHow do I prepend a level to the MultiIndex so that I turn it into something like:\n\nOutput&gt;                       Vals\nOutput&gt; FirstLevel A  B           \nOutput&gt; Foo        a1 b1 -1.632460\nOutput&gt;               b2  0.596027\nOutput&gt;            a2 b3 -0.619130\nOutput&gt;            a3 b4 -0.002009\n\n"
'To filter a dataframe (df) by a single column, if we consider data with male and females we might:\n\nmales = df[df[Gender]==\'Male\']\n\n\nQuestion 1 - But what if the data spanned multiple years and i wanted to only see males for 2014?\n\nIn other languages I might do something like: \n\nif A = "Male" and if B = "2014" then \n\n\n(except I want to do this and get a subset of the original dataframe in a new dataframe object)\n\nQuestion 2. How do I do this in a loop, and create a dataframe object for each unique sets of year and gender (i.e. a df for: 2013-Male, 2013-Female, 2014-Male, and 2014-Female\n\nfor y in year:\n\nfor g in gender:\n\ndf = .....\n\n'
'What is the best way to create a zero-filled pandas data frame of a given size?\n\nI have used: \n\nzero_data = np.zeros(shape=(len(data),len(feature_list)))\nd = pd.DataFrame(zero_data, columns=feature_list)\n\n\nIs there a better way to do it?\n'
'Given this dataframe, how to select only those rows that have "Col2" equal to NaN?\n\nIn [56]: df = pd.DataFrame([range(3), [0, np.NaN, 0], [0, 0, np.NaN], range(3), range(3)], columns=["Col1", "Col2", "Col3"])\n\nIn [57]: df\nOut[57]: \n   0   1   2\n0  0   1   2\n1  0 NaN   0\n2  0   0 NaN\n3  0   1   2\n4  0   1   2\n\n\nThe result should be this one:\n\nOut[57]: \n   0   1   2\n1  0 NaN   0\n\n'
"I've noticed that installing Pandas and Numpy (it's dependency) in a Docker container using the base OS Alpine vs. CentOS or Debian takes much longer. I created a little test below to demonstrate the time difference. Aside from the few seconds Alpine takes to update and download the build dependencies to install Pandas and Numpy, why does the setup.py take around 70x more time than on Debian install?\n\nIs there any way to speed up the install using Alpine as the base image or is there another base image of comparable size to Alpine that is better to use for packages like Pandas and Numpy?\n\nDockerfile.debian\n\nFROM python:3.6.4-slim-jessie\n\nRUN pip install pandas\n\n\nBuild Debian image with Pandas &amp; Numpy:\n\n[PandasDockerTest] time docker build -t debian-pandas -f Dockerfile.debian . --no-cache\n    Sending build context to Docker daemon  3.072kB\n    Step 1/2 : FROM python:3.6.4-slim-jessie\n     ---&gt; 43431c5410f3\n    Step 2/2 : RUN pip install pandas\n     ---&gt; Running in 2e4c030f8051\n    Collecting pandas\n      Downloading pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)\n    Collecting numpy&gt;=1.9.0 (from pandas)\n      Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n    Collecting pytz&gt;=2011k (from pandas)\n      Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)\n    Collecting python-dateutil&gt;=2 (from pandas)\n      Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\n    Collecting six&gt;=1.5 (from python-dateutil&gt;=2-&gt;pandas)\n      Downloading six-1.11.0-py2.py3-none-any.whl\n    Installing collected packages: numpy, pytz, six, python-dateutil, pandas\n    Successfully installed numpy-1.14.1 pandas-0.22.0 python-dateutil-2.6.1 pytz-2018.3 six-1.11.0\n    Removing intermediate container 2e4c030f8051\n     ---&gt; a71e1c314897\n    Successfully built a71e1c314897\n    Successfully tagged debian-pandas:latest\n    docker build -t debian-pandas -f Dockerfile.debian . --no-cache  0.07s user 0.06s system 0% cpu 13.605 total\n\n\nDockerfile.alpine\n\nFROM python:3.6.4-alpine3.7\n\nRUN apk --update add --no-cache g++\n\nRUN pip install pandas\n\n\nBuild Alpine image with Pandas &amp; Numpy:\n\n[PandasDockerTest] time docker build -t alpine-pandas -f Dockerfile.alpine . --no-cache\nSending build context to Docker daemon   16.9kB\nStep 1/3 : FROM python:3.6.4-alpine3.7\n ---&gt; 4b00a94b6f26\nStep 2/3 : RUN apk --update add --no-cache g++\n ---&gt; Running in 4b0c32551e3f\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\nfetch http://dl-cdn.alpinelinux.org/alpine/v3.7/community/x86_64/APKINDEX.tar.gz\n(1/17) Upgrading musl (1.1.18-r2 -&gt; 1.1.18-r3)\n(2/17) Installing libgcc (6.4.0-r5)\n(3/17) Installing libstdc++ (6.4.0-r5)\n(4/17) Installing binutils-libs (2.28-r3)\n(5/17) Installing binutils (2.28-r3)\n(6/17) Installing gmp (6.1.2-r1)\n(7/17) Installing isl (0.18-r0)\n(8/17) Installing libgomp (6.4.0-r5)\n(9/17) Installing libatomic (6.4.0-r5)\n(10/17) Installing pkgconf (1.3.10-r0)\n(11/17) Installing mpfr3 (3.1.5-r1)\n(12/17) Installing mpc1 (1.0.3-r1)\n(13/17) Installing gcc (6.4.0-r5)\n(14/17) Installing musl-dev (1.1.18-r3)\n(15/17) Installing libc-dev (0.7.1-r0)\n(16/17) Installing g++ (6.4.0-r5)\n(17/17) Upgrading musl-utils (1.1.18-r2 -&gt; 1.1.18-r3)\nExecuting busybox-1.27.2-r7.trigger\nOK: 184 MiB in 50 packages\nRemoving intermediate container 4b0c32551e3f\n ---&gt; be26c3bf4e42\nStep 3/3 : RUN pip install pandas\n ---&gt; Running in 36f6024e5e2d\nCollecting pandas\n  Downloading pandas-0.22.0.tar.gz (11.3MB)\nCollecting python-dateutil&gt;=2 (from pandas)\n  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194kB)\nCollecting pytz&gt;=2011k (from pandas)\n  Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)\nCollecting numpy&gt;=1.9.0 (from pandas)\n  Downloading numpy-1.14.1.zip (4.9MB)\nCollecting six&gt;=1.5 (from python-dateutil&gt;=2-&gt;pandas)\n  Downloading six-1.11.0-py2.py3-none-any.whl\nBuilding wheels for collected packages: pandas, numpy\n  Running setup.py bdist_wheel for pandas: started\n  Running setup.py bdist_wheel for pandas: still running...\n  Running setup.py bdist_wheel for pandas: still running...\n  Running setup.py bdist_wheel for pandas: still running...\n  Running setup.py bdist_wheel for pandas: still running...\n  Running setup.py bdist_wheel for pandas: still running...\n  Running setup.py bdist_wheel for pandas: still running...\n  Running setup.py bdist_wheel for pandas: finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/e8/ed/46/0596b51014f3cc49259e52dff9824e1c6fe352048a2656fc92\n  Running setup.py bdist_wheel for numpy: started\n  Running setup.py bdist_wheel for numpy: still running...\n  Running setup.py bdist_wheel for numpy: still running...\n  Running setup.py bdist_wheel for numpy: still running...\n  Running setup.py bdist_wheel for numpy: finished with status 'done'\n  Stored in directory: /root/.cache/pip/wheels/9d/cd/e1/4d418b16ea662e512349ef193ed9d9ff473af715110798c984\nSuccessfully built pandas numpy\nInstalling collected packages: six, python-dateutil, pytz, numpy, pandas\nSuccessfully installed numpy-1.14.1 pandas-0.22.0 python-dateutil-2.6.1 pytz-2018.3 six-1.11.0\nRemoving intermediate container 36f6024e5e2d\n ---&gt; a93c59e6a106\nSuccessfully built a93c59e6a106\nSuccessfully tagged alpine-pandas:latest\ndocker build -t alpine-pandas -f Dockerfile.alpine . --no-cache  0.54s user 0.33s system 0% cpu 16:08.47 total\n\n"
"I would like to import the following csv as strings not as int64. Pandas read_csv automatically converts it to int64, but I need this column as string.\n\nID\n00013007854817840016671868\n00013007854817840016749251\n00013007854817840016754630\n00013007854817840016781876\n00013007854817840017028824\n00013007854817840017963235\n00013007854817840018860166\n\n\ndf = read_csv('sample.csv')\n\ndf.ID\n&gt;&gt;\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n\n\nUnfortunately using converters gives the same result. \n\ndf = read_csv('sample.csv', converters={'ID': str})\ndf.ID\n&gt;&gt;\n\n0   -9223372036854775808\n1   -9223372036854775808\n2   -9223372036854775808\n3   -9223372036854775808\n4   -9223372036854775808\n5   -9223372036854775808\n6   -9223372036854775808\nName: ID\n\n"
"How can I filter which lines of a CSV to be loaded into memory using pandas?  This seems like an option that one should find in read_csv.  Am I missing something?\n\nExample: we've a CSV with a timestamp column and we'd like to load just the lines that with a timestamp greater than a given constant.\n"
"Is there a built-in way to use read_csv to read only the first n lines of a file without knowing the length of the lines ahead of time? I have a large file that takes a long time to read, and occasionally only want to use the first, say, 20 lines to get a sample of it (and prefer not to load the full thing and take the head of it).\n\nIf I knew the total number of lines I could do something like footer_lines = total_lines - n and pass this to the skipfooter keyword arg. My current solution is to manually grab the first n lines with python and StringIO it to pandas:\n\nimport pandas as pd\nfrom StringIO import StringIO\n\nn = 20\nwith open('big_file.csv', 'r') as f:\n    head = ''.join(f.readlines(n))\n\ndf = pd.read_csv(StringIO(head))\n\n\nIt's not that bad, but is there a more concise, 'pandasic' (?) way to do it with keywords or something?\n"
"I have a Pandas data frame, one of the column contains date strings in the format YYYY-MM-DD\nFor e.g. '2013-10-28'\nAt the moment the dtype of the column is object.\nHow do I convert the column values to Pandas date format?\n"
'I\'m having trouble with Pandas\' groupby functionality. I\'ve read the documentation, but I can\'t see to figure out how to apply aggregate functions to multiple columns and have custom names for those columns.\n\nThis comes very close, but the data structure returned has nested column headings:\n\ndata.groupby("Country").agg(\n        {"column1": {"foo": sum()}, "column2": {"mean": np.mean, "std": np.std}})\n\n\n(ie. I want to take the mean and std of column2, but return those columns as "mean" and "std")\n\nWhat am I missing?\n'
"I want to drop rows from a pandas dataframe when the value of the date column is in a list of dates. The following code doesn't work:\n\na=['2015-01-01' , '2015-02-01']\n\ndf=df[df.datecolumn not in a]\n\n\nI get the following error:\n\n\n  ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n"
"I am trying to modify a DataFrame df to only contain rows for which the values in the column closing_price are between 99 and 101 and trying to do this with the code below. \n\nHowever, I get the error \n\n\n  ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()\n\n\nand I am wondering if there is a way to do this without using loops.\n\ndf = df[(99 &lt;= df['closing_price'] &lt;= 101)]\n\n"
"I'd like to replace bad values in a column of a dataframe by NaN's.\n\nmydata = {'x' : [10, 50, 18, 32, 47, 20], 'y' : ['12', '11', 'N/A', '13', '15', 'N/A']}\ndf = pd.DataFrame(mydata)\n\ndf[df.y == 'N/A']['y'] = np.nan\n\n\nThough, the last line fails and throws a warning because it's working on a copy of df. So, what's the correct way to handle this? I've seen many solutions with iloc or ix but here, I need to use a boolean condition.\n"
"I would like to shift a column in a Pandas DataFrame, but I haven't been able to find a method to do it from the documentation without rewriting the whole DF. Does anyone know how to do it? \nDataFrame:\n\n##    x1   x2\n##0  206  214\n##1  226  234\n##2  245  253\n##3  265  272\n##4  283  291\n\n\nDesired output:\n\n##    x1   x2\n##0  206  nan\n##1  226  214\n##2  245  234\n##3  265  253\n##4  283  272\n##5  nan  291\n\n"
'I have a dataframe with columns A,B. I need to create a column C such that for every record / row:\n\nC = max(A, B).\n\nHow should I go about doing this?\n'
'I want to make all column headers in my pandas data frame lower case\n\nExample\n\nIf I have:\n\ndata =\n\n  country country isocode  year     XRAT          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n....\n\n\nI would like to change XRAT to xrat by doing something like:\n\ndata.headers.lowercase()\n\n\nSo that I get:\n\n  country country isocode  year     xrat          tcgdp\n0  Canada             CAN  2001  1.54876   924909.44207\n1  Canada             CAN  2002  1.56932   957299.91586\n2  Canada             CAN  2003  1.40105  1016902.00180\n3  Canada             CAN  2004  1.30102  1096000.35500\n....\n\n\nI will not know the names of each column header ahead of time.\n'
"Given a DataFrame:\n\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randn(3, 3), columns=list('ABC'), index=[1, 2, 3])\ndf\n\n          A         B         C\n1  1.764052  0.400157  0.978738\n2  2.240893  1.867558 -0.977278\n3  0.950088 -0.151357 -0.103219\n\n\nWhat is the simplest way to add a new column containing a constant value eg 0?\n\n          A         B         C  new\n1  1.764052  0.400157  0.978738    0\n2  2.240893  1.867558 -0.977278    0\n3  0.950088 -0.151357 -0.103219    0\n\n\n\n\nThis is my solution, but I don't know why this puts NaN into 'new' column?\n\ndf['new'] = pd.Series([0 for x in range(len(df.index))])\n\n          A         B         C  new\n1  1.764052  0.400157  0.978738  0.0\n2  2.240893  1.867558 -0.977278  0.0\n3  0.950088 -0.151357 -0.103219  NaN\n\n"
'You can use the function tz_localize to make a Timestamp or DateTimeIndex timezone aware, but how can you do the opposite: how can you convert a timezone aware Timestamp to a naive one, while preserving its timezone?\n\nAn example:\n\nIn [82]: t = pd.date_range(start="2013-05-18 12:00:00", periods=10, freq=\'s\', tz="Europe/Brussels")\n\nIn [83]: t\nOut[83]: \n&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\n\nI could remove the timezone by setting it to None, but then the result is converted to UTC (12 o\'clock became 10):\n\nIn [86]: t.tz = None\n\nIn [87]: t\nOut[87]: \n&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2013-05-18 10:00:00, ..., 2013-05-18 10:00:09]\nLength: 10, Freq: S, Timezone: None\n\n\nIs there another way I can convert a DateTimeIndex to timezone naive, but while preserving the timezone it was set in?\n\n\n\nSome context on the reason I am asking this: I want to work with timezone naive timeseries (to avoid the extra hassle with timezones, and I do not need them for the case I am working on).\nBut for some reason, I have to deal with a timezone-aware timeseries in my local timezone (Europe/Brussels). As all my other data are timezone naive (but represented in my local timezone), I want to convert this timeseries to naive to further work with it, but it also has to be represented in my local timezone (so just remove the timezone info, without converting the user-visible time to UTC).  \n\nI know the time is actually internal stored as UTC and only converted to another timezone when you represent it, so there has to be some kind of conversion when I want to "delocalize" it. For example, with the python datetime module you can "remove" the timezone like this:\n\nIn [119]: d = pd.Timestamp("2013-05-18 12:00:00", tz="Europe/Brussels")\n\nIn [120]: d\nOut[120]: &lt;Timestamp: 2013-05-18 12:00:00+0200 CEST, tz=Europe/Brussels&gt;\n\nIn [121]: d.replace(tzinfo=None)\nOut[121]: &lt;Timestamp: 2013-05-18 12:00:00&gt; \n\n\nSo, based on this, I could do the following, but I suppose this will not be very efficient when working with a larger timeseries:\n\nIn [124]: t\nOut[124]: \n&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: S, Timezone: Europe/Brussels\n\nIn [125]: pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\nOut[125]: \n&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2013-05-18 12:00:00, ..., 2013-05-18 12:00:09]\nLength: 10, Freq: None, Timezone: None\n\n'
"How do you find the top correlations in a correlation matrix with Pandas? There are many answers on how to do this with R (Show correlations as an ordered list, not as a large matrix or Efficient way to get highly correlated pairs from large data set in Python or R), but I am wondering how to do it with pandas? In my case the matrix is 4460x4460, so can't do it visually.\n"
'I am working with this Pandas DataFrame in Python.\n\nFile    heat    Farheit Temp_Rating\n   1    YesQ         75         N/A\n   1    NoR         115         N/A\n   1    YesA         63         N/A\n   1    NoT          83          41\n   1    NoY         100          80\n   1    YesZ         56          12\n   2    YesQ        111         N/A\n   2    NoR          60         N/A\n   2    YesA         19         N/A\n   2    NoT         106          77\n   2    NoY          45          21\n   2    YesZ         40          54\n   3    YesQ         84         N/A\n   3    NoR          67         N/A\n   3    YesA         94         N/A\n   3    NoT          68          39\n   3    NoY          63          46\n   3    YesZ         34          81\n\n\nI need to replace all NaNs in the Temp_Rating column with the value from the Farheit column.\n\nThis is what I need:\n\nFile        heat    Temp_Rating\n   1        YesQ             75\n   1         NoR            115\n   1        YesA             63\n   1        YesQ             41\n   1         NoR             80\n   1        YesA             12\n   2        YesQ            111\n   2         NoR             60\n   2        YesA             19\n   2         NoT             77\n   2         NoY             21\n   2        YesZ             54\n   3        YesQ             84\n   3         NoR             67\n   3        YesA             94\n   3         NoT             39\n   3         NoY             46\n   3        YesZ             81\n\n\nIf I do a Boolean selection, I can pick out only one of these columns at a time. The problem is if I then try to join them, I am not able to do this while preserving the correct order.\n\nHow can I only find Temp_Rating rows with the NaNs and replace them with the value in the same row of the Farheit column?\n'
'Take the following data-frame:\n\nx = np.tile(np.arange(3),3)\ny = np.repeat(np.arange(3),3)\ndf = pd.DataFrame({"x": x, "y": y})\n\n\n   x  y\n0  0  0\n1  1  0\n2  2  0\n3  0  1\n4  1  1\n5  2  1\n6  0  2\n7  1  2\n8  2  2\n\n\nI need to sort it by x first, and only second by y:\n\ndf2 = df.sort(["x", "y"])\n\n   x  y\n0  0  0\n3  0  1\n6  0  2\n1  1  0\n4  1  1\n7  1  2\n2  2  0\n5  2  1\n8  2  2\n\n\nHow can I change the index such that it is ascending again. I.e. how do I get this:\n\n   x  y\n0  0  0\n1  0  1\n2  0  2\n3  1  0\n4  1  1\n5  1  2\n6  2  0\n7  2  1\n8  2  2\n\n\nI have tried the following. Unfortunately, it doesn\'t change the index at all:\n\ndf2.reindex(np.arange(len(df2.index)))\n\n'
'I have a DataFrame df:\n\n    A    B\na   2    2 \nb   3    1\nc   1    3\n\n\nI want to create a new column based on the following criteria:\n\nif row A == B: 0\n\nif rowA &gt; B: 1\n\nif row A &lt; B: -1 \n\nso given the above table, it should be:\n\n    A    B    C\na   2    2    0\nb   3    1    1\nc   1    3   -1 \n\n\nFor typical if else cases I do np.where(df.A &gt; df.B, 1, -1), does pandas provide a special syntax for solving my problem with one step (without the necessity of creating 3 new columns and then combining the result)?  \n'
"I have table x:\n\n        website\n0   http://www.google.com/\n1   http://www.yahoo.com\n2   None\n\n\nI want to replace python None with pandas NaN. I tried:\n\nx.replace(to_replace=None, value=np.nan)\n\n\nBut I got:\n\nTypeError: 'regex' must be a string or a compiled regular expression or a list or dict of strings or regular expressions, you passed a 'bool'\n\n\nHow should I go about it? \n"
"I have this data frame diamonds which is composed of variables like (carat, price, color), and I want to draw a scatter plot of price to carat for each color, which means different color has different color in the plot.\n\nThis is easy in R with ggplot:\n\nggplot(aes(x=carat, y=price, color=color),  #by setting color=color, ggplot automatically draw in different colors\n       data=diamonds) + geom_point(stat='summary', fun.y=median)\n\n\n\n\nI wonder how could this be done in Python using matplotlib ?\n\nPS:\n\nI know about auxiliary plotting packages, such as seaborn and ggplot for python, and I donot prefer them, just want to find out if it is possible to do the job using matplotlib alone, ;P\n"
'I want to merge several strings in a dataframe based on a groupedby in Pandas. \n\nThis is my code so far:\n\nimport pandas as pd\nfrom io import StringIO\n\ndata = StringIO("""\n"name1","hej","2014-11-01"\n"name1","du","2014-11-02"\n"name1","aj","2014-12-01"\n"name1","oj","2014-12-02"\n"name2","fin","2014-11-01"\n"name2","katt","2014-11-02"\n"name2","mycket","2014-12-01"\n"name2","lite","2014-12-01"\n""")\n\n# load string as stream into dataframe\ndf = pd.read_csv(data,header=0, names=["name","text","date"],parse_dates=[2])\n\n# add column with month\ndf["month"] = df["date"].apply(lambda x: x.month)\n\n\nI want the end result to look like this:\n\n\n\nI don\'t get how I can use groupby and apply some sort of concatenation of the strings in the column "text". Any help appreciated!\n'
'My numpy arrays use np.nan to designate missing values. As I iterate over the data set, I need to detect such missing values and handle them in special ways.\n\nNaively I used numpy.isnan(val), which works well unless val isn\'t among the subset of types supported by numpy.isnan(). For example, missing data can occur in string fields, in which case I get:\n\n&gt;&gt;&gt; np.isnan(\'some_string\')\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nTypeError: Not implemented for this type\n\n\nOther than writing an expensive wrapper that catches the exception and returns False, is there a way to handle this elegantly and efficiently?\n'
'I\'m trying to import a .csv file using pandas.read_csv(), however I don\'t want to import the 2nd row of the data file (the row with index = 1 for 0-indexing).\n\nI can\'t see how not to import it because the arguments used with the command seem ambiguous:\n\nFrom the pandas website: \n\n\n  skiprows : list-like or integer\n  \n  Row numbers to skip (0-indexed) or number of rows to skip (int) at the\n  start of the file."\n\n\nIf I put skiprows=1 in the arguments, how does it know whether to skip the first row or skip the row with index 1?\n'
'Here is my df:\n\n                             Net   Upper   Lower  Mid  Zsore\nAnswer option                                                \nMore than once a day          0%   0.22%  -0.12%   2    65 \nOnce a day                    0%   0.32%  -0.19%   3    45\nSeveral times a week          2%   2.45%   1.10%   4    78\nOnce a week                   1%   1.63%  -0.40%   6    65\n\n\nHow can I move a column by name ("Mid") to the front of the table, index 0. This is what the result should look like:\n\n                             Mid   Upper   Lower  Net  Zsore\nAnswer option                                                \nMore than once a day          2   0.22%  -0.12%   0%    65 \nOnce a day                    3   0.32%  -0.19%   0%    45\nSeveral times a week          4   2.45%   1.10%   2%    78\nOnce a week                   6   1.63%  -0.40%   1%    65\n\n\nMy current code moves the column by index using df.columns.tolist() but I\'d like to shift it by name. \n'
"I am trying to plot some data using pandas in Ipython Notebook, and while it gives me the object, it doesn't actually plot the graph itself. So it looks like this:\n\nIn [7]:\n\npledge.Amount.plot()\n\nOut[7]:\n\n&lt;matplotlib.axes.AxesSubplot at 0x9397c6c&gt;\n\n\nThe graph should follow after that, but it simply doesn't appear. I have imported matplotlib, so that's not the problem. Is there any other module I need to import?\n"
'I want to subtract dates in \'A\' from dates in \'B\' and add a new column with the difference.\n\ndf\n          A        B\none 2014-01-01  2014-02-28 \ntwo 2014-02-03  2014-03-01\n\n\nI\'ve tried the following, but get an error when I try to include this in a for loop...\n\nimport datetime\ndate1=df[\'A\'][0]\ndate2=df[\'B\'][0]\nmdate1 = datetime.datetime.strptime(date1, "%Y-%m-%d").date()\nrdate1 = datetime.datetime.strptime(date2, "%Y-%m-%d").date()\ndelta =  (mdate1 - rdate1).days\nprint delta\n\n\nWhat should I do?\n'
'I\'m somewhat new to pandas. I have a pandas data frame that is 1 row by 23 columns.\n\nI want to convert this into a series? I\'m wondering what the most pythonic way to do this is?\n\nI\'ve tried pd.Series(myResults) but it complains ValueError: cannot copy sequence with size 23 to array axis with dimension 1. It\'s not smart enough to realize it\'s still a "vector" in math terms. \n\nThanks!\n'
'I have a csv file which isn\'t coming in correctly with pandas.read_csv when I  filter the columns with usecols and use multiple indexes.\n\n\nimport pandas as pd\ncsv = r"""dummy,date,loc,x\n   bar,20090101,a,1\n   bar,20090102,a,3\n   bar,20090103,a,5\n   bar,20090101,b,1\n   bar,20090102,b,3\n   bar,20090103,b,5"""\n\nf = open(\'foo.csv\', \'w\')\nf.write(csv)\nf.close()\n\ndf1 = pd.read_csv(\'foo.csv\',\n        header=0,\n        names=["dummy", "date", "loc", "x"], \n        index_col=["date", "loc"], \n        usecols=["dummy", "date", "loc", "x"],\n        parse_dates=["date"])\nprint df1\n\n# Ignore the dummy columns\ndf2 = pd.read_csv(\'foo.csv\', \n        index_col=["date", "loc"], \n        usecols=["date", "loc", "x"], # &lt;----------- Changed\n        parse_dates=["date"],\n        header=0,\n        names=["dummy", "date", "loc", "x"])\nprint df2\n\n\nI expect that df1 and df2 should be the same except for the missing dummy column, but the columns come in mislabeled.  Also the date is getting parsed as a date.  \n\nIn [118]: %run test.py\n               dummy  x\ndate       loc\n2009-01-01 a     bar  1\n2009-01-02 a     bar  3\n2009-01-03 a     bar  5\n2009-01-01 b     bar  1\n2009-01-02 b     bar  3\n2009-01-03 b     bar  5\n              date\ndate loc\na    1    20090101\n     3    20090102\n     5    20090103\nb    1    20090101\n     3    20090102\n     5    20090103\n\n\nUsing column numbers instead of names give me the same problem.  I can workaround the issue by dropping the dummy column after the read_csv step, but I\'m trying to understand what is going wrong.  I\'m using pandas 0.10.1.\n\nedit: fixed bad header usage.\n'
"I have a pandas data frame with multiple columns and I would like to construct a dict from two columns: one as the dict's keys and the other as the dict's values. How can I do that?\n\nDataframe:\n\n           area  count\nco tp\nDE Lake      10      7\nForest       20      5\nFR Lake      30      2\nForest       40      3\n\n\nI need to define area as key, count as value in dict. Thank you in advance.\n"
"I have a very large data set and I can't afford to read the entire data set in. So, I'm thinking of reading only one chunk of it to train but I have no idea how to do it. Any thought will be appreciated.\n"
'I have a pandas DataFrame like this:\n\n                    a         b\n2011-01-01 00:00:00 1.883381  -0.416629\n2011-01-01 01:00:00 0.149948  -1.782170\n2011-01-01 02:00:00 -0.407604 0.314168\n2011-01-01 03:00:00 1.452354  NaN\n2011-01-01 04:00:00 -1.224869 -0.947457\n2011-01-01 05:00:00 0.498326  0.070416\n2011-01-01 06:00:00 0.401665  NaN\n2011-01-01 07:00:00 -0.019766 0.533641\n2011-01-01 08:00:00 -1.101303 -1.408561\n2011-01-01 09:00:00 1.671795  -0.764629\n\n\nIs there an efficient way to find the "integer" index of rows with NaNs? In this case the desired output should be [3, 6].\n'
'What\'s the Python way to read in a CSV file into a pandas DataFrame (which I can then use for statistical operations, can have differently-typed columns, etc.)?  \n\nMy CSV file "value.txt" has the following content: \n\nDate,"price","factor_1","factor_2"\n2012-06-11,1600.20,1.255,1.548\n2012-06-12,1610.02,1.258,1.554\n2012-06-13,1618.07,1.249,1.552\n2012-06-14,1624.40,1.253,1.556\n2012-06-15,1626.15,1.258,1.552\n2012-06-16,1626.15,1.263,1.558\n2012-06-17,1626.15,1.264,1.572\n\n\nIn R we would read this file in using: \n\nprice &lt;- read.csv("value.txt")  \n\n\nand that would return an R data.frame:\n\n&gt; price &lt;- read.csv("value.txt")\n&gt; price\n     Date   price factor_1 factor_2\n1  2012-06-11 1600.20    1.255    1.548\n2  2012-06-12 1610.02    1.258    1.554\n3  2012-06-13 1618.07    1.249    1.552\n4  2012-06-14 1624.40    1.253    1.556\n5  2012-06-15 1626.15    1.258    1.552\n6  2012-06-16 1626.15    1.263    1.558\n7  2012-06-17 1626.15    1.264    1.572\n\n\nIs there a Pythonic way to get the same functionality?\n'
'I am reading two columns of a csv file using pandas readcsv() and then assigning the values to a dictionary. The columns contain strings of numbers and letters. Occasionally there are cases where a cell is empty. In my opinion, the value read to that dictionary entry should be None but instead nan is assigned. Surely None is more descriptive of an empty cell as it has a null value, whereas nan just says that the value read is not a number.\n\nIs my understanding correct, what IS the difference between None and nan? Why is nan assigned instead of None?\n\nAlso, my dictionary check for any empty cells has been using numpy.isnan():\n\nfor k, v in my_dict.iteritems():\n    if np.isnan(v):\n\n\nBut this gives me an error saying that I cannot use this check for v. I guess it is because an integer or float variable, not a string is meant to be used. If this is true, how can I check v for an "empty cell"/nan case?\n'
'I am parsing data from an Excel file that has extra white space in some of the column headings.\n\nWhen I check the columns of the resulting dataframe, with df.columns, I see:\n\nIndex([\'Year\', \'Month \', \'Value\'])\n                     ^\n#                    Note the unwanted trailing space on \'Month \'\n\n\nConsequently, I can\'t do: \n\ndf["Month"]\n\nBecause it will tell me the column is not found, as I asked for "Month", not "Month ".\n\nMy question, then, is how can I strip out the unwanted white space from the column headings?\n'
"I have noticed very poor performance when using iterrows from pandas.\n\nIs this something that is experienced by others? Is it specific to iterrows and should this function be avoided for data of a certain size (I'm working with 2-3 million rows)?\n\nThis discussion on GitHub led me to believe it is caused when mixing dtypes in the dataframe, however the simple example below shows it is there even when using one dtype (float64). This takes 36 seconds on my machine:\n\nimport pandas as pd\nimport numpy as np\nimport time\n\ns1 = np.random.randn(2000000)\ns2 = np.random.randn(2000000)\ndfa = pd.DataFrame({'s1': s1, 's2': s2})\n\nstart = time.time()\ni=0\nfor rowindex, row in dfa.iterrows():\n    i+=1\nend = time.time()\nprint end - start\n\n\nWhy are vectorized operations like apply so much quicker? I imagine there must be some row by row iteration going on there too. \n\nI cannot figure out how to not use iterrows in my case (this I'll save for a future question). Therefore I would appreciate hearing if you have consistently been able to avoid this iteration. I'm making calculations based on data in separate dataframes. Thank you!\n\n---Edit: simplified version of what I want to run has been added below---\n\nimport pandas as pd\nimport numpy as np\n\n#%% Create the original tables\nt1 = {'letter':['a','b'],\n      'number1':[50,-10]}\n\nt2 = {'letter':['a','a','b','b'],\n      'number2':[0.2,0.5,0.1,0.4]}\n\ntable1 = pd.DataFrame(t1)\ntable2 = pd.DataFrame(t2)\n\n#%% Create the body of the new table\ntable3 = pd.DataFrame(np.nan, columns=['letter','number2'], index=[0])\n\n#%% Iterate through filtering relevant data, optimizing, returning info\nfor row_index, row in table1.iterrows():   \n    t2info = table2[table2.letter == row['letter']].reset_index()\n    table3.ix[row_index,] = optimize(t2info,row['number1'])\n\n#%% Define optimization\ndef optimize(t2info, t1info):\n    calculation = []\n    for index, r in t2info.iterrows():\n        calculation.append(r['number2']*t1info)\n    maxrow = calculation.index(max(calculation))\n    return t2info.ix[maxrow]\n\n"
"I'm creating a heatmap from a pandas pivot_table as below:\n\ntable2 = pd.pivot_table(df,values='control',columns='Year',index='Region',aggfunc=np.sum)\nsns.heatmap(table2,annot=True,cmap='Blues')\n\n\nIt creates a heat map as shown below. You can see the numbers are not huge (max 750), but it's showing them in scientific notation. If I view the table itself this is not the case. Any idea on how I could get it to show the numbers in plain notation?\n\n\n"
"Say I have the following dataframe:\n\n\n\nWhat is the most efficient way to update the values of the columns feat and another_feat where the stream is number 2?\n\nIs this it?\n\nfor index, row in df.iterrows():\n    if df1.loc[index,'stream'] == 2:\n       # do something\n\n\nUPDATE:\nWhat to do if I have more than a 100 columns? I don't want to explicitly name the columns that I want to update. I want to divide the value of each column by 2 (except for the stream column).\n\nSo to be clear what my goal is:\n\nDividing all values by 2 of all rows that have stream 2, but not changing the stream column\n"
'I have a large amount of data in a collection in mongodb which I need to analyze. How do i import that data to pandas?\n\nI am new to pandas and numpy.\n\nEDIT:\nThe mongodb collection contains sensor values tagged with date and time. The sensor values are of float datatype. \n\nSample Data:\n\n{\n"_cls" : "SensorReport",\n"_id" : ObjectId("515a963b78f6a035d9fa531b"),\n"_types" : [\n    "SensorReport"\n],\n"Readings" : [\n    {\n        "a" : 0.958069536790466,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:26:35.297Z"),\n        "b" : 6.296118156595,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 0.95574014778624,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:27:09.963Z"),\n        "b" : 6.29651468650064,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 0.953648289182713,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:27:37.545Z"),\n        "b" : 7.29679823731148,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 0.955931884300997,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:28:21.369Z"),\n        "b" : 6.29642922525632,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 0.95821381,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:41:20.801Z"),\n        "b" : 7.28956613,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 4.95821335,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:41:36.931Z"),\n        "b" : 6.28956574,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 9.95821341,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:42:09.971Z"),\n        "b" : 0.28956488,\n        "_cls" : "Reading"\n    },\n    {\n        "a" : 1.95667927,\n        "_types" : [\n            "Reading"\n        ],\n        "ReadingUpdatedDate" : ISODate("2013-04-02T08:43:55.463Z"),\n        "b" : 0.29115237,\n        "_cls" : "Reading"\n    }\n],\n"latestReportTime" : ISODate("2013-04-02T08:43:55.463Z"),\n"sensorName" : "56847890-0",\n"reportCount" : 8\n}\n\n'
"Consider a csv file:\n\nstring,date,number\na string,2/5/11 9:16am,1.0\na string,3/5/11 10:44pm,2.0\na string,4/22/11 12:07pm,3.0\na string,4/22/11 12:10pm,4.0\na string,4/29/11 11:59am,1.0\na string,5/2/11 1:41pm,2.0\na string,5/2/11 2:02pm,3.0\na string,5/2/11 2:56pm,4.0\na string,5/2/11 3:00pm,5.0\na string,5/2/14 3:02pm,6.0\na string,5/2/14 3:18pm,7.0\n\n\nI can read this in, and reformat the date column into datetime format:\n\nb=pd.read_csv('b.dat')\nb['date']=pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\n\n\nI have been trying to group the data by month. It seems like there should be an obvious way of accessing the month and grouping by that. But I can't seem to do it. Does anyone know how?\n\nWhat I am currently trying is re-indexing by the date:\n\nb.index=b['date']\n\n\nI can access the month like so:\n\nb.index.month\n\n\nHowever I can't seem to find a function to lump together by month.\n"
"I have a Series like this after doing groupby('name') and used mean() function on other column\n\nname\n383      3.000000\n663      1.000000\n726      1.000000\n737      9.000000\n833      8.166667\n\n\nCould anyone please show me how to filter out the rows with 1.000000 mean values? Thank you and I greatly appreciate your help.\n"
'With the following code:\n\nimport matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':["foo","bar","qux","woz"], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[["celltype","s1","s2"]]\ndf.set_index(["celltype"],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75)\nplt.xlabel("")\n\n\nI made this plot:\n\n\n\nHow can I rotate the x-axis tick labels to 0 degrees?\n\nI tried adding this but did not work:\n\nplt.set_xticklabels(df.index,rotation=90)\n\n'
"Questions\n\n\nHow do I use pd.concat?\nWhat is the levels argument for?\nWhat is the keys argument for?\nAre there a bunch of examples to help explain how to use all the arguments?\n\n\nPandas' concat function is the Swiss Army knife of the merging utilities.  The variety of situations in which it is useful are numerous. The existing documentation leaves out a few details on some of the optional arguments. Among them are the levels and keys arguments. I set out to figure out what those arguments do.\n\nI'll pose a question that will act as a gateway into many aspects of pd.concat.\n\nConsider the data frames d1, d2, and d3:\n\nimport pandas as pd\n\nd1 = pd.DataFrame(dict(A=.1, B=.2, C=.3), [2, 3])\nd2 = pd.DataFrame(dict(B=.4, C=.5, D=.6), [1, 2])\nd3 = pd.DataFrame(dict(A=.7, B=.8, D=.9), [1, 3])\n\n\nIf I were to concatenate these together with\n\npd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'])\n\n\nI get the expected result with a pandas.MultiIndex for my columns object:\n\n        A    B    C    D\nd1 2  0.1  0.2  0.3  NaN\n   3  0.1  0.2  0.3  NaN\nd2 1  NaN  0.4  0.5  0.6\n   2  NaN  0.4  0.5  0.6\nd3 1  0.7  0.8  NaN  0.9\n   3  0.7  0.8  NaN  0.9\n\n\nHowever, I wanted to use the levels argument documentation:\n\n\n  levels: list of sequences, default None.\n  Specific levels (unique values) to use for constructing a MultiIndex. Otherwise, they will be inferred from the keys.\n\n\nSo I passed\n\npd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2']])\n\n\nAnd get a KeyError\n\n\n  ValueError: Key d3 not in level Index(['d1', 'd2'], dtype='object')\n\n\nThis made sense. The levels I passed were inadequate to describe the necessary levels indicated by the keys. Had I not passed anything, as I did above, the levels are inferred (as stated in the documentation). But how else can I use this argument to better effect?\n\nIf I tried this instead:\n\npd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2', 'd3']])\n\n\nI and got the same results as above. But when I add one more value to the levels,\n\ndf = pd.concat([d1, d2, d3], keys=['d1', 'd2', 'd3'], levels=[['d1', 'd2', 'd3', 'd4']])\n\n\nI end up with the same looking data frame, but the resulting MultiIndex has an unused level.\n\ndf.index.levels[0]\n\nIndex(['d1', 'd2', 'd3', 'd4'], dtype='object')\n\n\nSo what is the point of the level argument and should I be using keys differently?\n\nI'm using Python 3.6 and Pandas 0.22.\n"
"I am working with the pandas library and I want to add two new columns to a dataframe df with n columns (n > 0).\nThese new columns result from the application of a function to one of the columns in the dataframe.\n\nThe function to apply is like:\n\ndef calculate(x):\n    ...operate...\n    return z, y\n\n\nOne method for creating a new column for a function returning only a value is:\n\ndf['new_col']) = df['column_A'].map(a_function)\n\n\nSo, what I want, and tried unsuccesfully (*), is something like:\n\n(df['new_col_zetas'], df['new_col_ys']) = df['column_A'].map(calculate)\n\n\nWhat the best way to accomplish this could be ? I scanned the documentation with no clue. \n\n**df['column_A'].map(calculate) returns a pandas Series each item consisting of a tuple z, y. And trying to assign this to two dataframe columns produces a ValueError.* \n"
"I have a dataframe with some columns like this:\n\nA   B   C  \n0   \n4\n5\n6\n7\n7\n6\n5\n\n\nThe possible range of values in A are only from 0 to 7. \n\nAlso, I have a list of 8 elements like this:\n\nList=[2,5,6,8,12,16,26,32]  //There are only 8 elements in this list\n\n\nIf the element in column A is n, I need to insert the n th element from the List in a new column, say 'D'.\n\nHow can I do this in one go without looping over the whole dataframe? \n\nThe resulting dataframe would look like this:\n\nA   B   C   D\n0           2\n4           12\n5           16\n6           26\n7           32\n7           32\n6           26\n5           16\n\n\nNote: The dataframe is huge and iteration is the last option option. But I can also arrange the elements in 'List' in any other data structure like dict if necessary.\n"
"I've been reading a tab-delimited data file in Windows with Pandas/Python without any problems. The data file contains notes in first three lines and then follows with a header. \n\ndf = pd.read_csv(myfile,sep='\\t',skiprows=(0,1,2),header=(0))\n\n\nI'm now trying to read this file with my Mac. (My first time using Python on Mac.) I get the following error.\n\npandas.parser.CParserError: Error tokenizing data. C error: Expected 1\nfields in line 8, saw 39\n\n\nIf set the error_bad_lines argument for read_csv to False, I get the following information, which continues until the end of the last row.\n\nSkipping line 8: expected 1 fields, saw 39\nSkipping line 9: expected 1 fields, saw 125\nSkipping line 10: expected 1 fields, saw 125\nSkipping line 11: expected 1 fields, saw 125\nSkipping line 12: expected 1 fields, saw 125\nSkipping line 13: expected 1 fields, saw 125\nSkipping line 14: expected 1 fields, saw 125\nSkipping line 15: expected 1 fields, saw 125\nSkipping line 16: expected 1 fields, saw 125\nSkipping line 17: expected 1 fields, saw 125\n...\n\n\nDo I need to specify a value for the encoding argument? It seems as though I shouldn't have to because reading the file works fine on Windows.\n"
'I am trying to read in a JSON file into Python pandas (0.14.0) data frame. Here is the first line line of the JSON file:\n\n{"votes": {"funny": 0, "useful": 0, "cool": 0}, "user_id": "P_Mk0ygOilLJo4_WEvabAA", "review_id": "OeT5kgUOe3vcN7H6ImVmZQ", "stars": 3, "date": "2005-08-26", "text": "This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course.", "type": "review", "business_id": "Jp9svt7sRT4zwdbzQ8KQmw"}\n\n\nI am trying do the following:df = pd.read_json(path).\n\nI am getting the following error (with full traceback):\n\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py", line 198, in read_json\n    date_unit).parse()\n  File "/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py", line 266, in parse\n    self._parse_no_numpy()\n  File "/Users/d/anaconda/lib/python2.7/site-packages/pandas/io/json.py", line 483, in _parse_no_numpy\n    loads(json, precise_float=self.precise_float), dtype=None)\nValueError: Trailing data\n\n\nWhat is the Trailing data error? How do I read it into a data frame?\n\nFollowing some suggestions, here are few lines of the .json file:\n\n{"votes": {"funny": 0, "useful": 0, "cool": 0}, "user_id": "P_Mk0ygOilLJo4_WEvabAA", "review_id": "OeT5kgUOe3vcN7H6ImVmZQ", "stars": 3, "date": "2005-08-26", "text": "This is a pretty typical cafe.  The sandwiches and wraps are good but a little overpriced and the food items are the same.  The chicken caesar salad wrap is my favorite here but everything else is pretty much par for the course.", "type": "review", "business_id": "Jp9svt7sRT4zwdbzQ8KQmw"}\n{"votes": {"funny": 0, "useful": 0, "cool": 0}, "user_id": "TNJRTBrl0yjtpAACr1Bthg", "review_id": "qq3zF2dDUh3EjMDuKBqhEA", "stars": 3, "date": "2005-11-23", "text": "I agree with other reviewers - this is a pretty typical financial district cafe.  However, they have fantastic pies.  I ordered three pies for an office event (apple, pumpkin cheesecake, and pecan) - all were delicious, particularly the cheesecake.  The sucker weighed in about 4 pounds - no joke.\\n\\nNo surprises on the cafe side - great pies and cakes from the catering business.", "type": "review", "business_id": "Jp9svt7sRT4zwdbzQ8KQmw"}\n{"votes": {"funny": 0, "useful": 0, "cool": 0}, "user_id": "H_mngeK3DmjlOu595zZMsA", "review_id": "i3eQTINJXe3WUmyIpvhE9w", "stars": 3, "date": "2005-11-23", "text": "Decent enough food, but very overpriced. Just a large soup is almost $5. Their specials are $6.50, and with an overpriced soda or juice, it\'s approaching $10. A bit much for a cafe lunch!", "type": "review", "business_id": "Jp9svt7sRT4zwdbzQ8KQmw"}\n\n\nThis .json file I am using contains one JSON object in each line as per the specification.\n\nI tried the jsonlint.com website as suggested and it gives the following error:\n\nParse error on line 14:\n...t7sRT4zwdbzQ8KQmw"}{    "votes": {\n----------------------^\nExpecting \'EOF\', \'}\', \',\', \']\'\n\n'
'I would like to fill missing values in one column with values from another column, using fillna method. \n\n(I read that looping through each row would be very bad practice and that it would be better to do everything in one go but I could not find out how to do it with fillna.)\n\nData before:\n\nDay  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    NaN   ant\n\n\nData after:\n\nDay  Cat1  Cat2\n1    cat   mouse\n2    dog   elephant\n3    cat   giraf\n4    ant   ant\n\n'
'I have the following dataframe:\n\n Index_Date    A    B    C    D\n ===============================\n 2015-01-31    10   10   Nan  10\n 2015-02-01     2    3   Nan  22 \n 2015-02-02    10   60   Nan  280\n 2015-02-03    10   100   Nan  250\n\n\nRequire:\n\n Index_Date    A    B    C    D\n ===============================\n 2015-01-31    10   10   10   10\n 2015-02-01     2    3   23   22\n 2015-02-02    10   60   290  280\n 2015-02-03    10   100  3000 250\n\n\nColumn C is derived for 2015-01-31 by taking value of D.\n\nThen I need to use the value of C for 2015-01-31 and multiply by the value of A on 2015-02-01 and add B.\n\nI have attempted an apply and a shift using an if else by this gives a key error.\n'
'I am new to Pandas, and am trying to use date_range.  I came across all kinds of good things for freq, like BME and BMS and I would like to be able to quickly look up the proper strings to get what I want.  Yesterday I found a nicely formatted table somewhere in the documentation, but the title of the table was so obtuse that I can not use search to find it again today.    \n'
"When selecting a single column from a pandas DataFrame(say df.iloc[:, 0], df['A'], or df.A, etc), the resulting vector is automatically converted to a Series instead of a single-column DataFrame. However, I am writing some functions that takes a DataFrame as an input argument. Therefore, I prefer to deal with single-column DataFrame instead of Series so that the function can assume say df.columns is accessible. Right now I have to explicitly convert the Series into a DataFrame by using something like pd.DataFrame(df.iloc[:, 0]). This doesn't seem like the most clean method. Is there a more elegant way to index from a DataFrame directly so that the result is a single-column DataFrame instead of Series?\n"
"I have a pandas data frame and would like to plot values from one column versus the values from another column. Fortunately, there is plot method associated with the data-frames that seems to do what I need:\n\ndf.plot(x='col_name_1', y='col_name_2')\n\n\nUnfortunately, it looks like among the plot styles (listed here after the kind parameter) there are not points. I can use lines or bars or even density but not points. Is there a work around that can help to solve this problem.\n"
"Let's say I have a log of user activity and I want to generate a report of total duration and the number of unique users per day.\n\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'date': ['2013-04-01','2013-04-01','2013-04-01','2013-04-02', '2013-04-02'],\n    'user_id': ['0001', '0001', '0002', '0002', '0002'],\n    'duration': [30, 15, 20, 15, 30]})\n\n\nAggregating duration is pretty straightforward:\n\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg\n            duration\ndate\n2013-04-01        65\n2013-04-02        45\n\n\nWhat I'd like to do is sum the duration and count distincts at the same time, but I can't seem to find an equivalent for count_distinct:\n\nagg = group.aggregate({ 'duration': np.sum, 'user_id': count_distinct})\n\n\nThis works, but surely there's a better way, no?\n\ngroup = df.groupby('date')\nagg = group.aggregate({'duration': np.sum})\nagg['uv'] = df.groupby('date').user_id.nunique()\nagg\n            duration  uv\ndate\n2013-04-01        65   2\n2013-04-02        45   1\n\n\nI'm thinking I just need to provide a function that returns the count of distinct items of a Series object to the aggregate function, but I don't have a lot of exposure to the various libraries at my disposal. Also, it seems that the groupby object already knows this information, so wouldn't I just be duplicating effort?\n"
'In Pandas, when I select a label that only has one entry in the index I get back a Series, but when I select an entry that has more then one entry I get back a data frame.\n\nWhy is that?  Is there a way to ensure I always get back a data frame?\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(data=range(5), index=[1, 2, 3, 3, 3])\n\nIn [3]: type(df.loc[3])\nOut[3]: pandas.core.frame.DataFrame\n\nIn [4]: type(df.loc[1])\nOut[4]: pandas.core.series.Series\n\n'
"I have some data and when I import it I get the following unneeded columns I'm looking for an easy way to delete all of these\n\n   'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27',\n   'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31',\n   'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35',\n   'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39',\n   'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43',\n   'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47',\n   'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51',\n   'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55',\n   'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59',\n   'Unnamed: 60'\n\n\nThey are indexed by 0-indexing so I tried something like \n\n    df.drop(df.columns[[22, 23, 24, 25, \n    26, 27, 28, 29, 30, 31, 32 ,55]], axis=1, inplace=True)\n\n\nBut this isn't very efficient. I tried writing some for loops but this struck me as bad Pandas behaviour. Hence i ask the question here.\n\nI've seen some examples which are similar (Drop multiple columns pandas) but this doesn't answer my question. \n"
'The documentation says: \n\nhttp://pandas.pydata.org/pandas-docs/dev/basics.html \n\n"Continuous values can be discretized using the cut (bins based on values) and qcut (bins based on sample quantiles) functions"\n\nSounds very abstract to me... I can see the differences in the example below but what does qcut (sample quantile) actually do/mean? When would you use qcut versus cut?\n\nThanks.\n\nfactors = np.random.randn(30)\n\nIn [11]:\npd.cut(factors, 5)\nOut[11]:\n[(-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (-0.411, 0.575], (0.575, 1.561], ..., (-0.411, 0.575], (-1.397, -0.411], (0.575, 1.561], (-2.388, -1.397], (-0.411, 0.575]]\nLength: 30\nCategories (5, object): [(-2.388, -1.397] &lt; (-1.397, -0.411] &lt; (-0.411, 0.575] &lt; (0.575, 1.561] &lt; (1.561, 2.547]]\n\nIn [14]:\npd.qcut(factors, 5)\nOut[14]:\n[(-0.348, 0.0899], (-0.348, 0.0899], (0.0899, 1.19], (0.0899, 1.19], (0.0899, 1.19], ..., (0.0899, 1.19], (-1.137, -0.348], (1.19, 2.547], [-2.383, -1.137], (-0.348, 0.0899]]\nLength: 30\nCategories (5, object): [[-2.383, -1.137] &lt; (-1.137, -0.348] &lt; (-0.348, 0.0899] &lt; (0.0899, 1.19] &lt; (1.19, 2.547]]`\n\n'
'I have a dataframe like this:\n\ncluster  org      time\n   1      a       8\n   1      a       6\n   2      h       34\n   1      c       23\n   2      d       74\n   3      w       6 \n\n\nI would like to calculate the average of time per org per cluster.\n\nExpected result:\n\ncluster mean(time)\n1       15 ((8+6)/2+23)/2\n2       54   (74+34)/2\n3       6\n\n\nI do not know how to do it in Pandas, can anybody help?\n'
"One last newbie pandas question for the day:  How do I generate a table for a single Series?\n\nFor example:\n\nmy_series = pandas.Series([1,2,2,3,3,3])\npandas.magical_frequency_function( my_series )\n\n&gt;&gt; {\n     1 : 1,\n     2 : 2, \n     3 : 3\n   }\n\n\nLots of googling has led me to Series.describe() and pandas.crosstabs, but neither of these does quite what I need: one variable, counts by categories.  Oh, and it'd be nice if it worked for different data types: strings, ints, etc.\n"
"I'm probably using poor search terms when trying to find this answer. Right now, before indexing a DataFrame, I'm getting a list of values in a column this way...\n\n list = list(df['column']) \n\n\n...then I'll set_index on the column. This seems like a wasted step. When trying the above on an index, I get a key error.\n\nHow can I grab the values in an index (both single and multi) and put them in a list or a list of tuples?\n"
"I am attempting a merge between two data frames.  Each data frame has two index levels (date, cusip).  In the columns, some columns match between the two (currency, adj date) for example.\n\nWhat is the best way to merge these by index, but to not take two copies of currency and adj date.\n\nEach data frame is 90 columns, so I am trying to avoid writing everything out by hand.\n\ndf:                 currency  adj_date   data_col1 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\ndf2:                currency  adj_date   data_col2 ...\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45\n...\n\n\nIf I do:\n\ndfNew = merge(df, df2, left_index=True, right_index=True, how='outer')\n\n\nI get \n\ndfNew:              currency_x  adj_date_x   data_col2 ... currency_y adj_date_y\ndate        cusip\n2012-01-01  XSDP      USD      2012-01-03   0.45             USD         2012-01-03\n\n\nThank you!\n    ...\n"
"Environment: Python 2.7, matplotlib 1.3, IPython notebook 1.1, linux, chrome. The code is in one single input cell, using --pylab=inline\n\nI want to use IPython notebook and pandas to consume a stream and dynamically update a plot every 5 seconds. \n\nWhen I just use print statement to print the data in text format, it works perfectly fine: the output cell just keeps printing data and adding new rows. But when I try to plot the data (and then update it in a loop), the plot never show up in the output cell. But if I remove the loop, just plot it once. It works fine.\n\nThen I did some simple test:\n\ni = pd.date_range('2013-1-1',periods=100,freq='s')\nwhile True:\n    plot(pd.Series(data=np.random.randn(100), index=i))\n    #pd.Series(data=np.random.randn(100), index=i).plot() also tried this one\n    time.sleep(5)\n\n\nThe output will not show anything until I manually interrupt the process (ctrl+m+i). And after I interrupt it, the plot shows correctly as multiple overlapped lines. But what I really want is a plot that shows up and gets updated every 5 seconds (or whenever the plot() function gets called, just like what print statement outputs I mentioned above, which works well). Only showing the final chart after the cell is completely done is NOT what i want.\n\nI even tried to explicitly add draw() function after each plot(), etc. None of them works. Wonder how to dynamically update a plot by a for/while loop within one cell in IPython notebook.\n"
"I have a pandas dataframe as follows:\n\nSymbol  Date\nA       02/20/2015\nA       01/15/2016\nA       08/21/2015\n\n\nI want to sort it by Date, but the column is just an object.\n\nI tried to make the column a date object, but I ran into an issue where that format is not the format needed. The format needed is 2015-02-20, etc.\n\nSo now I'm trying to figure out how to have numpy convert the 'American' dates into the ISO standard, so that I can make them date objects, so that I can sort by them.\n\nHow would I convert these american dates into ISO standard, or is there a more straight forward method I'm missing within pandas?\n"
"I have a data frame with categorical data:\n\n     colour  direction\n1    red     up\n2    blue    up\n3    green   down\n4    red     left\n5    red     right\n6    yellow  down\n7    blue    down\n\n\nI want to generate some graphs, like pie charts and histograms based on the categories. Is it possible without creating dummy numeric variables? Something like\n\ndf.plot(kind='hist')\n\n"
'Hello I have the following dataframe.   \n\n    Group           Size\n\n    Short          Small\n    Short          Small\n    Moderate       Medium\n    Moderate       Small\n    Tall           Large\n\n\nI want to count the frequency of how many time the same row appears in the dataframe.\n\n    Group           Size      Time\n\n    Short          Small        2\n    Moderate       Medium       1 \n    Moderate       Small        1\n    Tall           Large        1\n\n'
'That is the difference between groupby("x").count and groupby("x").size in pandas ?\n\nDoes size just exclude nil ?\n'
"I have a temperature file with many years temperature records, in a format as below:\n\n2012-04-12,16:13:09,20.6\n2012-04-12,17:13:09,20.9\n2012-04-12,18:13:09,20.6\n2007-05-12,19:13:09,5.4\n2007-05-12,20:13:09,20.6\n2007-05-12,20:13:09,20.6\n2005-08-11,11:13:09,20.6\n2005-08-11,11:13:09,17.5\n2005-08-13,07:13:09,20.6\n2006-04-13,01:13:09,20.6\n\n\nEvery year has different numbers, time of the records, so the pandas datetimeindices are all different.\n\nI want to plot the different year's data in the same figure for comparing . The X-axis is Jan to Dec, the Y-axis is temperature. How should I go about doing this? \n"
"I want to get both horizontal and vertical grid lines on my plot but only the horizontal grid lines are appearing by default. I am using a pandas.DataFrame from an sql query in python to generate a line plot with dates on the x-axis. I'm not sure why they do not appear on the dates and I have tried to search for an answer to this but couldn't find one.\n\nAll I have used to plot the graph is the simple code below. \n\ndata.plot()\ngrid('on')\n\n\ndata is the DataFrame which contains the dates and the data from the sql query. \n\nI have also tried adding the code below but I still get the same output with no vertical grid lines.\n\nax = plt.axes()        \nax.yaxis.grid() # horizontal lines\nax.xaxis.grid() # vertical lines\n\n\nAny suggestions?\n\n\n"
"I have a very large dataframe (around 1 million rows) with data from an experiment (60 respondents).\nI would like to split the dataframe into 60 dataframes (a dataframe for each participant).\nIn the dataframe, data, there is a variable called 'name', which is the unique code for each participant.\nI have tried the following, but nothing happens (or execution does not stop within an hour). What I intend to do is to split the data into smaller dataframes, and append these to a list (datalist):\nimport pandas as pd\n\ndef splitframe(data, name='name'):\n    \n    n = data[name][0]\n\n    df = pd.DataFrame(columns=data.columns)\n\n    datalist = []\n\n    for i in range(len(data)):\n        if data[name][i] == n:\n            df = df.append(data.iloc[i])\n        else:\n            datalist.append(df)\n            df = pd.DataFrame(columns=data.columns)\n            n = data[name][i]\n            df = df.append(data.iloc[i])\n        \n    return datalist\n\nI do not get an error message, the script just seems to run forever!\nIs there a smart way to do it?\n"
"Is there an easy method in pandas to invoke groupby on a range of values increments? For instance given the example below can I bin and group column B with a 0.155 increment so that for example, the first couple of groups in column B are divided into ranges between '0 - 0.155, 0.155 - 0.31 ...`\n\nimport numpy as np\nimport pandas as pd\ndf=pd.DataFrame({'A':np.random.random(20),'B':np.random.random(20)})\n\n     A         B\n0  0.383493  0.250785\n1  0.572949  0.139555\n2  0.652391  0.401983\n3  0.214145  0.696935\n4  0.848551  0.516692\n\n\nAlternatively I could first categorize the data by those increments into a new column and subsequently use groupby to determine any relevant statistics that may be applicable in column A?\n"
"I have a Pandas series sf:\n\nemail\nemail1@email.com    [1.0, 0.0, 0.0]\nemail2@email.com    [2.0, 0.0, 0.0]\nemail3@email.com    [1.0, 0.0, 0.0]\nemail4@email.com    [4.0, 0.0, 0.0]\nemail5@email.com    [1.0, 0.0, 3.0]\nemail6@email.com    [1.0, 5.0, 0.0]\n\n\nAnd I would like to transform it to the following DataFrame:\n\nindex | email             | list\n_____________________________________________\n0     | email1@email.com  | [1.0, 0.0, 0.0]\n1     | email2@email.com  | [2.0, 0.0, 0.0]\n2     | email3@email.com  | [1.0, 0.0, 0.0]\n3     | email4@email.com  | [4.0, 0.0, 0.0]\n4     | email5@email.com  | [1.0, 0.0, 3.0]\n5     | email6@email.com  | [1.0, 5.0, 0.0]\n\n\nI found a way to do it, but I doubt it's the more efficient one:\n\ndf1 = pd.DataFrame(data=sf.index, columns=['email'])\ndf2 = pd.DataFrame(data=sf.values, columns=['list'])\ndf = pd.merge(df1, df2, left_index=True, right_index=True)\n\n"
'I have a pd.DataFrame that was created by parsing some excel spreadsheets. A column of which has empty cells. For example, below is the output for the frequency of that column, 32320 records have missing values for Tenant.\n\n&gt;&gt;&gt; value_counts(Tenant, normalize=False)\n                              32320\n    Thunderhead                8170\n    Big Data Others            5700\n    Cloud Cruiser              5700\n    Partnerpedia               5700\n    Comcast                    5700\n    SDP                        5700\n    Agora                      5700\n    dtype: int64\n\n\nI am trying to drop rows where Tenant is missing, however .isnull() option does not recognize the missing values. \n\n&gt;&gt;&gt; df[\'Tenant\'].isnull().sum()\n    0\n\n\nThe column has data type "Object". What is happening in this case? How can I drop records where Tenant is missing?\n'
'I have two pandas dataframes and I would like to display them in Jupyter notebook.\n\nDoing something like: \n\ndisplay(df1)\ndisplay(df2)\n\n\nShows them one below another:\n\n\n\nI would like to have a second dataframe on the right of the first one. There is a similar question, but it looks like there a person is satisfied either with merging them in one dataframe of showing the difference between them.\n\nThis will not work for me. In my case dataframes can represent completely different (non-comparable elements) and the size of them can be different. Thus my main goal is to save space.\n'
"I have a multi-index data frame with columns  'A' and 'B'. \n\nIs there is a way to select rows by filtering on one column of the multi-index without resetting the index to a single column index? \n\nFor Example.\n\n# has multi-index (A,B)\ndf\n#can I do this? I know this doesn't work because the index is multi-index so I need to     specify a tuple\n\ndf.ix[df.A ==1]\n\n"
"When there is an DataFrame like the following:\n\nimport pandas as pd\ndf = pd.DataFrame([1, 1, 1, 1, 1], index=[100, 29, 234, 1, 150], columns=['A'])\n\n\nHow can I sort this dataframe by index with each combination of index and column value intact?\n"
"I was looking for a way to annotate my bars in a Pandas bar plot with the rounded numerical values from my DataFrame.\n\n&gt;&gt;&gt; df=pd.DataFrame({'A':np.random.rand(2),'B':np.random.rand(2)},index=['value1','value2'] )         \n&gt;&gt;&gt; df\n                 A         B\n  value1  0.440922  0.911800\n  value2  0.588242  0.797366\n\n\nI would like to get something like this:\n\n\n\nI tried with this code sample, but the annotations are all centered on the x ticks:\n\n&gt;&gt;&gt; ax = df.plot(kind='bar') \n&gt;&gt;&gt; for idx, label in enumerate(list(df.index)): \n        for acc in df.columns:\n            value = np.round(df.ix[idx][acc],decimals=2)\n            ax.annotate(value,\n                        (idx, value),\n                         xytext=(0, 15), \n                         textcoords='offset points')\n\n"
'I have a pandas dataframe (this is only a little piece)\n\n&gt;&gt;&gt; d1\n   y norm test  y norm train  len(y_train)  len(y_test)  \\\n0    64.904368    116.151232          1645          549   \n1    70.852681    112.639876          1645          549   \n\n                                    SVR RBF  \\\n0   (35.652207342877873, 22.95533537448393)   \n1  (39.563683797747622, 27.382483096332511)   \n\n                                        LCV  \\\n0  (19.365430594452338, 13.880062435173587)   \n1  (19.099614489458364, 14.018867136617146)   \n\n                                   RIDGE CV  \\\n0  (4.2907610988480362, 12.416745648065584)   \n1    (4.18864306788194, 12.980833914392477)   \n\n                                         RF  \\\n0   (9.9484841581029428, 16.46902345373697)   \n1  (10.139848213735391, 16.282141345406522)   \n\n                                           GB  \\\n0  (0.012816232716538605, 15.950164822266007)   \n1  (0.012814519804493328, 15.305745202851712)   \n\n                                             ET DATA  \n0  (0.00034337162272515505, 16.284800366214057)  j2m  \n1  (0.00024811554516431878, 15.556506191784194)  j2m  \n&gt;&gt;&gt; \n\n\nI want to split all the columns that contain tuples. For example I want to replace the column LCV with the columns LCV-a and LCV-b . \n\nHow can I do that?\n'
"I'm looking to turn a pandas cell containing a list into rows for each of those values.\n\nSo, take this:\n\n  \n\nIf I'd like to unpack and stack the values in the nearest_neighbors column so that each value would be a row within each opponent index, how would I best go about this? Are there pandas methods that are meant for operations like this?\n"
'I\'m doing some code practice and applying merging of data frames while doing this \ngetting user warning \n\n\n  /usr/lib64/python2.7/site-packages/pandas/core/frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n  of pandas will change to not sort by default.\n  To accept the future behavior, pass \'sort=True\'.\n  To retain the current behavior and silence the warning, pass sort=False\n\n\nOn these lines of code: Can you please help to get the solution of this warning.\n\nplacement_video = [self.read_sql_vdx_summary, self.read_sql_video_km]\nplacement_video_summary = reduce(lambda left, right: pd.merge(left, right, on=\'PLACEMENT\', sort=False), placement_video)\n\n\nplacement_by_video = placement_video_summary.loc[:, ["PLACEMENT", "PLACEMENT_NAME", "COST_TYPE", "PRODUCT",\n                                                     "VIDEONAME", "VIEW0", "VIEW25", "VIEW50", "VIEW75",\n                                                     "VIEW100",\n                                                     "ENG0", "ENG25", "ENG50", "ENG75", "ENG100", "DPE0",\n                                                     "DPE25",\n                                                     "DPE50", "DPE75", "DPE100"]]\n\n# print (placement_by_video)\n\nplacement_by_video["Placement# Name"] = placement_by_video[["PLACEMENT",\n                                                            "PLACEMENT_NAME"]].apply(lambda x: ".".join(x),\n                                                                                     axis=1)\n\nplacement_by_video_new = placement_by_video.loc[:,\n                         ["PLACEMENT", "Placement# Name", "COST_TYPE", "PRODUCT", "VIDEONAME",\n                          "VIEW0", "VIEW25", "VIEW50", "VIEW75", "VIEW100",\n                          "ENG0", "ENG25", "ENG50", "ENG75", "ENG100", "DPE0", "DPE25",\n                          "DPE50", "DPE75", "DPE100"]]\n\nplacement_by_km_video = [placement_by_video_new, self.read_sql_km_for_video]\nplacement_by_km_video_summary = reduce(lambda left, right: pd.merge(left, right, on=[\'PLACEMENT\', \'PRODUCT\'], sort=False),\n                                       placement_by_km_video)\n\n#print (list(placement_by_km_video_summary))\n#print(placement_by_km_video_summary)\n#exit()\n# print(placement_by_video_new)\n"""Conditions for 25%view"""\nmask17 = placement_by_km_video_summary["PRODUCT"].isin([\'Display\', \'Mobile\'])\nmask18 = placement_by_km_video_summary["COST_TYPE"].isin(["CPE", "CPM", "CPCV"])\nmask19 = placement_by_km_video_summary["PRODUCT"].isin(["InStream"])\nmask20 = placement_by_km_video_summary["COST_TYPE"].isin(["CPE", "CPM", "CPE+", "CPCV"])\nmask_video_video_completions = placement_by_km_video_summary["COST_TYPE"].isin(["CPCV"])\nmask21 = placement_by_km_video_summary["COST_TYPE"].isin(["CPE+"])\nmask22 = placement_by_km_video_summary["COST_TYPE"].isin(["CPE", "CPM"])\nmask23 = placement_by_km_video_summary["PRODUCT"].isin([\'Display\', \'Mobile\', \'InStream\'])\nmask24 = placement_by_km_video_summary["COST_TYPE"].isin(["CPE", "CPM", "CPE+"])\n\nchoice25video_eng = placement_by_km_video_summary["ENG25"]\nchoice25video_vwr = placement_by_km_video_summary["VIEW25"]\nchoice25video_deep = placement_by_km_video_summary["DPE25"]\n\nplacement_by_km_video_summary["25_pc_video"] = np.select([mask17 &amp; mask18, mask19 &amp; mask20, mask17 &amp; mask21],\n                                                  [choice25video_eng, choice25video_vwr, choice25video_deep])\n\n\n"""Conditions for 50%view"""\nchoice50video_eng = placement_by_km_video_summary["ENG50"]\nchoice50video_vwr = placement_by_km_video_summary["VIEW50"]\nchoice50video_deep = placement_by_km_video_summary["DPE50"]\n\nplacement_by_km_video_summary["50_pc_video"] = np.select([mask17 &amp; mask18, mask19 &amp; mask20, mask17 &amp; mask21],\n                                                  [choice50video_eng,\n                                                   choice50video_vwr, choice50video_deep])\n\n"""Conditions for 75%view"""\n\nchoice75video_eng = placement_by_km_video_summary["ENG75"]\nchoice75video_vwr = placement_by_km_video_summary["VIEW75"]\nchoice75video_deep = placement_by_km_video_summary["DPE75"]\n\nplacement_by_km_video_summary["75_pc_video"] = np.select([mask17 &amp; mask18, mask19 &amp; mask20, mask17 &amp; mask21],\n                                                  [choice75video_eng,\n                                                   choice75video_vwr,\n                                                   choice75video_deep])\n\n"""Conditions for 100%view"""\n\nchoice100video_eng = placement_by_km_video_summary["ENG100"]\nchoice100video_vwr = placement_by_km_video_summary["VIEW100"]\nchoice100video_deep = placement_by_km_video_summary["DPE100"]\nchoicecompletions = placement_by_km_video_summary[\'COMPLETIONS\']\n\nplacement_by_km_video_summary["100_pc_video"] = np.select([mask17 &amp; mask22, mask19 &amp; mask24, mask17 &amp; mask21, mask23 &amp; mask_video_video_completions],\n                                                          [choice100video_eng, choice100video_vwr, choice100video_deep, choicecompletions])\n\n\n\n"""conditions for 0%view"""\n\nchoice0video_eng = placement_by_km_video_summary["ENG0"]\nchoice0video_vwr = placement_by_km_video_summary["VIEW0"]\nchoice0video_deep = placement_by_km_video_summary["DPE0"]\n\nplacement_by_km_video_summary["Views"] = np.select([mask17 &amp; mask18, mask19 &amp; mask20, mask17 &amp; mask21],\n                                                   [choice0video_eng,\n                                                    choice0video_vwr,\n                                                    choice0video_deep])\n\n\n#print (placement_by_km_video_summary)\n#exit()\n\n#final Table\n\nplacement_by_video_summary = placement_by_km_video_summary.loc[:,\n                             ["PLACEMENT", "Placement# Name", "PRODUCT", "VIDEONAME", "COST_TYPE",\n                              "Views", "25_pc_video", "50_pc_video", "75_pc_video","100_pc_video",\n                              "ENGAGEMENTS","IMPRESSIONS", "DPEENGAMENTS"]]\n\n#placement_by_km_video = [placement_by_video_summary, self.read_sql_km_for_video]\n#placement_by_km_video_summary = reduce(lambda left, right: pd.merge(left, right, on=[\'PLACEMENT\', \'PRODUCT\']),\n                                       #placement_by_km_video)\n\n\n#print(placement_by_video_summary)\n#exit()\n# dup_col =["IMPRESSIONS","ENGAGEMENTS","DPEENGAMENTS"]\n\n# placement_by_video_summary.loc[placement_by_video_summary.duplicated(dup_col),dup_col] = np.nan\n\n# print ("Dhar",placement_by_video_summary)\n\n\'\'\'adding views based on conditions\'\'\'\n#filter maximum value from videos\n\nplacement_by_video_summary_new = placement_by_km_video_summary.loc[\n    placement_by_km_video_summary.reset_index().groupby([\'PLACEMENT\', \'PRODUCT\'])[\'Views\'].idxmax()]\n#print (placement_by_video_summary_new)\n#exit()\n# print (placement_by_video_summary_new)\n# mask22 = (placement_by_video_summary_new.PRODUCT.str.upper ()==\'DISPLAY\') &amp; (placement_by_video_summary_new.COST_TYPE==\'CPE\')\n\nplacement_by_video_summary_new.loc[mask17 &amp; mask18, \'Views\'] = placement_by_video_summary_new[\'ENGAGEMENTS\']\nplacement_by_video_summary_new.loc[mask19 &amp; mask20, \'Views\'] = placement_by_video_summary_new[\'IMPRESSIONS\']\nplacement_by_video_summary_new.loc[mask17 &amp; mask21, \'Views\'] = placement_by_video_summary_new[\'DPEENGAMENTS\']\n\n#print (placement_by_video_summary_new)\n#exit()\nplacement_by_video_summary = placement_by_video_summary.drop(placement_by_video_summary_new.index).append(\n    placement_by_video_summary_new).sort_index()\n\nplacement_by_video_summary["Video Completion Rate"] = placement_by_video_summary["100_pc_video"] / \\\n                                                      placement_by_video_summary["Views"]\n\nplacement_by_video_final = placement_by_video_summary.loc[:,\n                           ["Placement# Name", "PRODUCT", "VIDEONAME", "Views",\n                            "25_pc_video", "50_pc_video", "75_pc_video", "100_pc_video",\n                            "Video Completion Rate"]]\n\n'
"I have python pandas dataframe, in which a column contains month name.\n\nHow can I  do a custom sort using a dictionary, for example:\n\ncustom_dict = {'March':0, 'April':1, 'Dec':3}  \n\n"
"Is it possible to add some meta-information/metadata to a pandas DataFrame?\n\nFor example, the instrument's name used to measure the data, the instrument responsible, etc.\n\nOne workaround would be to create a column with that information, but it seems wasteful to store a single piece of information in every row!\n"
'I have a large dataframe with 423244 lines. I want to split this in to 4. I tried the following code which gave an error? ValueError: array split does not result in an equal division\n\nfor item in np.split(df, 4):\n    print item\n\n\nHow to split this dataframe in to 4 groups?\n'
'I want to use excel files to store data elaborated with python. My problem is that I can\'t add sheets to an existing excel file. Here I suggest a sample code to work with in order to reach this issue\n\nimport pandas as pd\nimport numpy as np\n\npath = r"C:\\Users\\fedel\\Desktop\\excelData\\PhD_data.xlsx"\n\nx1 = np.random.randn(100, 2)\ndf1 = pd.DataFrame(x1)\n\nx2 = np.random.randn(100, 2)\ndf2 = pd.DataFrame(x2)\n\nwriter = pd.ExcelWriter(path, engine = \'xlsxwriter\')\ndf1.to_excel(writer, sheet_name = \'x1\')\ndf2.to_excel(writer, sheet_name = \'x2\')\nwriter.save()\nwriter.close()\n\n\nThis code saves two DataFrames to two sheets, named "x1" and "x2" respectively. If I create two new DataFrames and try to use the same code to add two new sheets, \'x3\' and \'x4\', the original data is lost.\n\nimport pandas as pd\nimport numpy as np\n\npath = r"C:\\Users\\fedel\\Desktop\\excelData\\PhD_data.xlsx"\n\nx3 = np.random.randn(100, 2)\ndf3 = pd.DataFrame(x3)\n\nx4 = np.random.randn(100, 2)\ndf4 = pd.DataFrame(x4)\n\nwriter = pd.ExcelWriter(path, engine = \'xlsxwriter\')\ndf3.to_excel(writer, sheet_name = \'x3\')\ndf4.to_excel(writer, sheet_name = \'x4\')\nwriter.save()\nwriter.close()\n\n\nI want an excel file with four sheets: \'x1\', \'x2\', \'x3\', \'x4\'.\nI know that \'xlsxwriter\' is not the only "engine", there is \'openpyxl\'. I also saw there are already other people that have written about this issue, but still I can\'t understand how to do that.\n\nHere a code taken from this link\n\nimport pandas\nfrom openpyxl import load_workbook\n\nbook = load_workbook(\'Masterfile.xlsx\')\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\', engine=\'openpyxl\') \nwriter.book = book\nwriter.sheets = dict((ws.title, ws) for ws in book.worksheets)\n\ndata_filtered.to_excel(writer, "Main", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n\n\nThey say that it works, but it is hard to figure out how. I don\'t understand what "ws.title", "ws", and "dict" are in this context. \n\nWhich is the best way to save "x1" and "x2", then close the file, open it again and add "x3" and "x4"?\n'
"I'm reading a CSV with float numbers like this:\n\nBob,0.085\nAlice,0.005\n\n\nAnd import into a dataframe, and write this dataframe to a new place\n\ndf = pd.read_csv(orig)\ndf.to_csv(pandasfile)\n\n\nNow this pandasfile has:\n\nBob,0.085000000000000006\nAlice,0.0050000000000000001\n\n\nWhat happen? maybe I have to cast to a different type like float32 or something? \n\nIm using pandas 0.9.0 and numpy 1.6.2.\n"
"I've got a pandas dataframe. I want to 'lag' one of my columns. Meaning, for example, shifting the entire column 'gdp' up by one, and then removing all the excess data at the bottom of the remaining rows so that all columns are of equal length again.\n\ndf =\n    y  gdp  cap\n0   1    2    5\n1   2    3    9\n2   8    7    2\n3   3    4    7\n4   6    7    7\n\ndf_lag =\n    y  gdp  cap\n0   1    3    5\n1   2    7    9\n2   8    4    2\n3   3    7    7\n\n\nAnyway to do this?\n"
"I want to add _x suffix to each column name like so:\n\nfeaturesA = myPandasDataFrame.columns.values + '_x'\n\n\nHow do I do this? Additionally, if I wanted to add x_ as a suffix, how would the solution change?\n"
'Suppose I have a nested dictionary \'user_dict\' with structure:\n\n\nLevel 1: UserId (Long Integer)\nLevel 2: Category (String)\nLevel 3: Assorted Attributes (floats, ints, etc..)\n\n\nFor example, an entry of this dictionary would be:\n\nuser_dict[12] = {\n    "Category 1": {"att_1": 1, \n                   "att_2": "whatever"},\n    "Category 2": {"att_1": 23, \n                   "att_2": "another"}}\n\n\neach item in user_dict has the same structure and user_dict contains a large number of items which I want to feed to a pandas DataFrame, constructing the series from the attributes. In this case a hierarchical index would be useful for the purpose.\n\nSpecifically, my question is whether there exists a way to to help the DataFrame constructor understand that the series should be built from the values of the "level 3" in the dictionary?\n\nIf I try something like:\n\ndf = pandas.DataFrame(users_summary)\n\n\nThe items in "level 1" (the UserId\'s) are taken as columns, which is the opposite of what I want to achieve (have UserId\'s as index). \n\nI know I could construct the series after iterating over the dictionary entries, but if there is a more direct way this would be very useful. A similar question would be asking whether it is possible to construct a pandas DataFrame from json objects listed in a file. \n'
"I have two dataframes with the following column names:\n\nframe_1:\nevent_id, date, time, county_ID\n\nframe_2:\ncountyid, state\n\n\nI would like to get a dataframe with the following columns by joining (left) on county_ID = countyid:\n\njoined_dataframe\nevent_id, date, time, county, state\n\n\nI cannot figure out how to do it if the columns on which I want to join are not the index. What's the easiest way? Thanks!\n"
'I have a DataFrame that contains numbers as strings with commas for the thousands marker. I need to convert them to floats.\n\na = [[\'1,200\', \'4,200\'], [\'7,000\', \'-0.03\'], [ \'5\', \'0\']]\ndf=pandas.DataFrame(a)\n\n\nI am guessing I need to use locale.atof. Indeed \n\ndf[0].apply(locale.atof)\n\n\nworks as expected. I get a Series of floats.\n\nBut when I apply it to the DataFrame, I get an error.\n\ndf.apply(locale.atof)\n\n\n\n  TypeError: ("cannot convert the series to ", u\'occurred at index 0\')\n\n\nand\n\ndf[0:1].apply(locale.atof)\n\n\ngives another error:\n\n\n  ValueError: (\'invalid literal for float(): 1,200\', u\'occurred at index 0\')\n\n\nSo, how do I convert this DataFrame of strings to a DataFrame of floats?\n'
"I have a pandas dataframe df as illustrated below:\n\nBrandName Specialty\nA          H\nB          I\nABC        J\nD          K\nAB         L\n\n\nI want to replace 'ABC' and 'AB' in column BrandName by A.\nCan someone help with this?\n"
'When using R it\'s handy to load "practice" datasets using \n\ndata(iris)\n\n\nor\n\ndata(mtcars)\n\n\nIs there something similar for Pandas? I know I can load using any other method, just curious if there\'s anything builtin.\n'
"I have a pandas dataframe. I want to print the unique values of one of its columns in ascending order. This is how I am doing it:\n\nimport pandas as pd\ndf = pd.DataFrame({'A':[1,1,3,2,6,2,8]})\na = df['A'].unique()\nprint a.sort()\n\n\nThe problem is that I am getting a None for the output.\n"
"How can I export a list of DataFrames into one Excel spreadsheet?\nThe docs for to_excel state:\n\n\n  Notes\n  If passing an existing ExcelWriter object, then the sheet will be added\n  to the existing workbook.  This can be used to save different\n  DataFrames to one workbook\n  \n  writer = ExcelWriter('output.xlsx')\n  df1.to_excel(writer, 'sheet1')\n  df2.to_excel(writer, 'sheet2')\n  writer.save()\n\n\nFollowing this, I thought I could write a function which saves a list of DataFrames to one spreadsheet as follows:\n\nfrom openpyxl.writer.excel import ExcelWriter\ndef save_xls(list_dfs, xls_path):\n    writer = ExcelWriter(xls_path)\n    for n, df in enumerate(list_dfs):\n        df.to_excel(writer,'sheet%s' % n)\n    writer.save()\n\n\nHowever (with a list of two small DataFrames, each of which can save to_excel individually), an exception is raised (Edit: traceback removed):\n\nAttributeError: 'str' object has no attribute 'worksheets'\n\n\nPresumably I am not calling ExcelWriter correctly, how should I be in order to do this?\n"
"I currently have a dataframe consisting of columns with 1's and 0's as values, I would like to iterate through the columns and delete the ones that are made up of only 0's. Here's what I have tried so far:\n\nones = []\nzeros = []\nfor year in years:\n    for i in range(0,599):\n        if year[str(i)].values.any() == 1:\n            ones.append(i)\n        if year[str(i)].values.all() == 0:\n            zeros.append(i)\n    for j in ones:\n        if j in zeros:\n            zeros.remove(j)\n    for q in zeros:\n        del year[str(q)]\n\n\nIn which years is a list of dataframes for the various years I am analyzing, ones consists of columns with a one in them and zeros is a list of columns containing all zeros. Is there a better way to delete a column based on a condition? For some reason I have to check whether the ones columns are in the zeros list as well and remove them from the zeros list to obtain a list of all the zero columns. \n"
"I am trying to make a simple scatter plot in pyplot using a Pandas DataFrame object, but want an efficient way of plotting two variables but have the symbols dictated by a third column (key). I have tried various ways using df.groupby, but not successfully. A sample df script is below. This colours the markers according to 'key1', but Id like to see a legend with 'key1' categories. Am I close? Thanks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range('2010-01-01', freq = 'M', periods = 10), columns = ('one', 'two', 'three'))\ndf['key1'] = (4,4,4,6,6,6,8,8,8,8)\nfig1 = plt.figure(1)\nax1 = fig1.add_subplot(111)\nax1.scatter(df['one'], df['two'], marker = 'o', c = df['key1'], alpha = 0.8)\nplt.show()\n\n"
'I have two dataframes.  Examples:\n\ndf1:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n\ndf2:\nDate       Fruit  Num  Color \n2013-11-24 Banana 22.1 Yellow\n2013-11-24 Orange  8.6 Orange\n2013-11-24 Apple   7.6 Green\n2013-11-24 Celery 10.2 Green\n2013-11-25 Apple  22.1 Red\n2013-11-25 Orange  8.6 Orange\n\n\nEach dataframe has the Date as an index. Both dataframes have the same structure. \n\nWhat i want to do, is compare these two dataframes and find which rows are in df2 that aren\'t in df1. I want to compare the date (index) and the first column (Banana, APple, etc) to see if they exist in df2 vs df1.\n\nI have tried the following:\n\n\nOutputting difference in two Pandas dataframes side by side - highlighting the difference\nComparing two pandas dataframes for differences\n\n\nFor the first approach I get this error: "Exception: Can only compare identically-labeled DataFrame objects".  I have tried removing the Date as index but get the same error.\n\nOn the third approach, I get the assert to return False but cannot figure out how to actually see the different rows.\n\nAny pointers would be welcome\n'
'The following code does not work. \n\nimport pandas as pd\nimport numpy as np\ndf=pd.DataFrame([\'ONE\',\'Two\', np.nan],columns=[\'x\']) \nxLower = df["x"].map(lambda x: x.lower())\n\n\nHow should I tweak it to get xLower = [\'one\',\'two\',np.nan] ?\nEfficiency is important since the real data frame is huge.\n'
"I have a dataframe that may look like this:\n\nA        B        C\nfoo      bar      foo bar\nbar foo  foo      bar\n\n\nI want to look through every element of each row (or every element of each column) and apply the following function to get the subsequent DF:\n\ndef foo_bar(x):\n    return x.replace('foo', 'wow')\n\nA        B        C\nwow      bar      wow bar\nbar wow  wow      bar\n\n\nIs there a simple one-liner that can apply a function to each cell? \n\nThis is a simplistic example so there may be an easier way to execute this specific example other than applying a function, but what I am really asking about is how to apply a function in every cell within a dataframe. \n"
"If I pass a dataframe to a function and modify it inside the function, is it pass-by-value or pass-by-reference?\n\nI run the following code\n\na = pd.DataFrame({'a':[1,2], 'b':[3,4]})\ndef letgo(df):\n    df = df.drop('b',axis=1)\nletgo(a)\n\n\nthe value of a does not change after the function call. Does it mean it is pass-by-value?\n\nI also tried the following\n\nxx = np.array([[1,2], [3,4]])\ndef letgo2(x):\n    x[1,1] = 100\ndef letgo3(x):\n    x = np.array([[3,3],[3,3]])\n\n\nIt turns out letgo2() does change xx and letgo3() does not. Why is it like this?\n"
'It seems that dtype only work for pandas.DataFrame.Series, right? Is there a function to display data types of all columns at once?\n'
"I have started my IPython Notebook with \n\nipython notebook --pylab inline\n\n\nThis is my code in one cell\n\ndf['korisnika'].plot()\ndf['osiguranika'].plot()\n\n\nThis is working fine, it will draw two lines, but on the same chart.\n\nI would like to draw each line on a separate chart.\nAnd it would be great if the charts would be next to each other, not one after the other.\n\nI know that I can put the second line in the next cell, and then I would get two charts. But I would like the charts close to each other, because they represent the same logical unit.\n"
"I have two columns, fromdate and todate, in a dataframe.\nimport pandas as pd\n\ndata = {'todate': [pd.Timestamp('2014-01-24 13:03:12.050000'), pd.Timestamp('2014-01-27 11:57:18.240000'), pd.Timestamp('2014-01-23 10:07:47.660000')],\n        'fromdate': [pd.Timestamp('2014-01-26 23:41:21.870000'), pd.Timestamp('2014-01-27 15:38:22.540000'), pd.Timestamp('2014-01-23 18:50:41.420000')]}\n\ndf = pd.DataFrame(data)\n\nI add a new column, diff, to find the difference between the two dates using\ndf['diff'] = df['fromdate'] - df['todate']\n\nI get the diff column, but it contains days, when there's more than 24 hours.\n                   todate                fromdate                   diff\n0 2014-01-24 13:03:12.050 2014-01-26 23:41:21.870 2 days 10:38:09.820000\n1 2014-01-27 11:57:18.240 2014-01-27 15:38:22.540 0 days 03:41:04.300000\n2 2014-01-23 10:07:47.660 2014-01-23 18:50:41.420 0 days 08:42:53.760000\n\nHow do I convert my results to only hours and minutes (i.e. days are converted to hours)?\n"
"I'm using Seaborn's lmplot to plot a linear regression, dividing my dataset into two groups with a categorical variable.\n\nFor both x and y, I'd like to manually set the lower bound on both plots, but leave the upper bound at the Seaborn default.\nHere's a simple example:\n\nimport pandas as pd\nimport seaborn as sns\nimport random\n\nn = 200\nrandom.seed(2014)\nbase_x = [random.random() for i in range(n)]\nbase_y = [2*i for i in base_x]\nerrors = [random.uniform(0,1) for i in range(n)]\ny = [i+j for i,j in zip(base_y,errors)]\n\ndf = pd.DataFrame({'X': base_x,\n                   'Y': y,\n                   'Z': ['A','B']*(n/2)})\n\nmask_for_b = df.Z == 'B'\ndf.loc[mask_for_b,['X','Y']] = df.loc[mask_for_b,] *2\n\nsns.lmplot('X','Y',df,col='Z',sharex=False,sharey=False)\n\n\nThis outputs the following:\n\n\nBut in this example, I'd like the xlim and the ylim to be (0,*) . I tried using sns.plt.ylim and sns.plt.xlim but those only affect the right-hand plot. \nExample:\n\nsns.plt.ylim(0,)\nsns.plt.xlim(0,)\n\n\n\n\nHow can I access the xlim and ylim for each plot in the FacetGrid?\n"
"I currently have a dataframe that looks like this:\n\n           Unnamed: 1    Unnamed: 2   Unnamed: 3  Unnamed: 4\n0   Sample Number  Group Number  Sample Name  Group Name\n1             1.0           1.0          s_1         g_1\n2             2.0           1.0          s_2         g_1\n3             3.0           1.0          s_3         g_1\n4             4.0           2.0          s_4         g_2\n\n\nI'm looking for a way to delete the header row and make the first row the new header row, so the new dataframe would look like this:\n\n    Sample Number  Group Number  Sample Name  Group Name\n0             1.0           1.0          s_1         g_1\n1             2.0           1.0          s_2         g_1\n2             3.0           1.0          s_3         g_1\n3             4.0           2.0          s_4         g_2\n\n\nI've tried stuff along the lines of if 'Unnamed' in df.columns: then make the dataframe without the header df.to_csv(newformat,header=False,index=False) but I don't seem to be getting anywhere.\n"
"I just started using pandas/matplotlib as a replacement for Excel to generate stacked bar charts.  I am running into an issue  \n\n(1) there are only 5 colors in the default colormap, so if I have more than 5 categories then the colors repeat.  How can I specify more colors?  Ideally, a gradient with a start color and an end color, and a way to dynamically generate n colors in between?\n\n(2) the colors are not very visually pleasing.  How do I specify a custom set of n colors?  Or, a gradient would also work.\n\nAn example which illustrates both of the above points is below:\n\n  4 from matplotlib import pyplot\n  5 from pandas import *\n  6 import random\n  7 \n  8 x = [{i:random.randint(1,5)} for i in range(10)]\n  9 df = DataFrame(x)\n 10 \n 11 df.plot(kind='bar', stacked=True)\n\n\nAnd the output is this:\n\n\n"
"I have a dataset with multi-index columns in a pandas df that I would like to sort by values in a specific column.  I have tried using sortindex and sortlevel but haven't been able get the results I am looking for.  My dataset looks like:\n\n    Group1    Group2\n    A B C     A B C\n1   1 0 3     2 5 7\n2   5 6 9     1 0 0\n3   7 0 2     0 3 5 \n\n\nI want to sort all data and the index by column C in Group 1 in descending order so my results look like:\n\n    Group1    Group2\n    A B C     A B C\n 2  5 6 9     1 0 0\n 1  1 0 3     2 5 7\n 3  7 0 2     0 3 5 \n\n\nIs it possible to do this sort with the structure that my data is in, or should I be swapping Group1 to the index side?\n"
"I'm new to Pandas.... I've got a bunch of polling data; I want to compute a rolling mean to get an estimate for each day based on a three-day window. As I understand from this question, the rolling_* functions compute the window based on a specified number of values, and not a specific datetime range. \n\nIs there a different function that implements this functionality? Or am I stuck writing my own?\n\nEDIT: \n\nSample input data: \n\npolls_subset.tail(20)\nOut[185]: \n            favorable  unfavorable  other\n\nenddate                                  \n2012-10-25       0.48         0.49   0.03\n2012-10-25       0.51         0.48   0.02\n2012-10-27       0.51         0.47   0.02\n2012-10-26       0.56         0.40   0.04\n2012-10-28       0.48         0.49   0.04\n2012-10-28       0.46         0.46   0.09\n2012-10-28       0.48         0.49   0.03\n2012-10-28       0.49         0.48   0.03\n2012-10-30       0.53         0.45   0.02\n2012-11-01       0.49         0.49   0.03\n2012-11-01       0.47         0.47   0.05\n2012-11-01       0.51         0.45   0.04\n2012-11-03       0.49         0.45   0.06\n2012-11-04       0.53         0.39   0.00\n2012-11-04       0.47         0.44   0.08\n2012-11-04       0.49         0.48   0.03\n2012-11-04       0.52         0.46   0.01\n2012-11-04       0.50         0.47   0.03\n2012-11-05       0.51         0.46   0.02\n2012-11-07       0.51         0.41   0.00\n\n\nOutput would have only one row for each date. \n\nEDIT x2: fixed typo \n"
"I am using pandas/python and I have two date time series s1 and s2, that have been generated using the 'to_datetime' function on a field of the df containing dates/times.\n\nWhen I subtract s1 from s2\n\n\n  s3 = s2 - s1\n\n\nI get a series, s3, of type \n\n\n  timedelta64[ns]\n\n\n0    385 days, 04:10:36\n1     57 days, 22:54:00\n2    642 days, 21:15:23\n3    615 days, 00:55:44\n4    160 days, 22:13:35\n5    196 days, 23:06:49\n6     23 days, 22:57:17\n7      2 days, 22:17:31\n8    622 days, 01:29:25\n9     79 days, 20:15:14\n10    23 days, 22:46:51\n11   268 days, 19:23:04\n12                  NaT\n13                  NaT\n14   583 days, 03:40:39\n\n\nHow do I look at 1 element of the series:\n\n\n  s3[10]\n\n\nI get something like this:\n\n\n  numpy.timedelta64(2069211000000000,'ns')\n\n\nHow do I extract days from s3 and maybe keep them as integers(not so interested in hours/mins etc.)?\n\nThanks in advance for any help.\n"
'In ipython Notebook, first create a pandas Series object, then by calling the instance method .hist(), the browser displays the figure. \n\nI am wondering how to save this figure to a file (I mean not by right click and save as, but the commands needed in the script).\n'
"Suppose I have a dataframe with countries that goes as:\n\ncc | temp\nUS | 37.0\nCA | 12.0\nUS | 35.0\nAU | 20.0\n\n\nI know that there is a pd.get_dummies function to convert the countries to 'one-hot encodings'. However, I wish to convert them to indices instead such that I will get cc_index = [1,2,1,3] instead. \n\nI'm assuming that there is a faster way than using the get_dummies along with a numpy where clause as shown below:\n\n[np.where(x) for x in df.cc.get_dummies().values]\n\nThis is somewhat easier to do in R using 'factors' so I'm hoping pandas has something similar.\n"
"I would like to filter rows by a function of each row, e.g.\n\ndef f(row):\n  return sin(row['velocity'])/np.prod(['masses']) &gt; 5\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, f)]\n\n\nOr for another more complex, contrived example,\n\ndef g(row):\n  if row['col1'].method1() == 1:\n    val = row['col1'].method2() / row['col1'].method3(row['col3'], row['col4'])\n  else:\n    val = row['col2'].method5(row['col6'])\n  return np.sin(val)\n\ndf = pandas.DataFrame(...)\nfiltered = df[apply_to_all_rows(df, g)]\n\n\nHow can I do so?\n"
'I want to be able to set the major and minor xticks and their labels for a time series graph plotted from a Pandas time series object.  \n\nThe Pandas 0.9 "what\'s new" page says: \n\n\n  "you can either use to_pydatetime or register a converter for the\n  Timestamp type"\n\n\nbut I can\'t work out how to do that so that I can use the matplotlib ax.xaxis.set_major_locator and ax.xaxis.set_major_formatter (and minor) commands.\n\nIf I use them without converting the pandas times, the x-axis ticks and labels end up wrong.\n\nBy using the \'xticks\' parameter I can pass the major ticks to pandas.plot, and then set the major tick labels. I can\'t work out how to do the minor ticks using this approach. (I can set the labels on the default minor ticks set by pandas.plot)\n\nHere is my test code:\n\nimport pandas\nprint \'pandas.__version__ is \', pandas.__version__\nprint \'matplotlib.__version__ is \', matplotlib.__version__    \n\ndStart = datetime.datetime(2011,5,1) # 1 May\ndEnd = datetime.datetime(2011,7,1) # 1 July    \n\ndateIndex = pandas.date_range(start=dStart, end=dEnd, freq=\'D\')\nprint "1 May to 1 July 2011", dateIndex      \n\ntestSeries = pandas.Series(data=np.random.randn(len(dateIndex)),\n                           index=dateIndex)    \n\nax = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\ntestSeries.plot(ax=ax, style=\'v-\', label=\'first line\')    \n\n# using MatPlotLib date time locators and formatters doesn\'t work with new\n# pandas datetime index\nax.xaxis.set_minor_locator(matplotlib.dates.WeekdayLocator(byweekday=(1),\n                                                           interval=1))\nax.xaxis.set_minor_formatter(matplotlib.dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which="minor")\nax.xaxis.grid(False, which="major")\nax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter(\'\\n\\n\\n%b%Y\'))\nplt.show()    \n\n# set the major xticks and labels through pandas\nax2 = plt.figure(figsize=(7,4), dpi=300).add_subplot(111)\nxticks = pandas.date_range(start=dStart, end=dEnd, freq=\'W-Tue\')\nprint "xticks: ", xticks\ntestSeries.plot(ax=ax2, style=\'-v\', label=\'second line\',\n                xticks=xticks.to_pydatetime())\nax2.set_xticklabels([x.strftime(\'%a\\n%d\\n%h\\n%Y\') for x in xticks]);\n# set the text of the first few minor ticks created by pandas.plot\n#    ax2.set_xticklabels([\'a\',\'b\',\'c\',\'d\',\'e\'], minor=True)\n# remove the minor xtick labels set by pandas.plot \nax2.set_xticklabels([], minor=True)\n# turn the minor ticks created by pandas.plot off \n# plt.minorticks_off()\nplt.show()\nprint testSeries[\'6/4/2011\':\'6/7/2011\']\n\n\nand its output:\n\npandas.__version__ is  0.9.1.dev-3de54ae\nmatplotlib.__version__ is  1.1.1\n1 May to 1 July 2011 &lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2011-05-01 00:00:00, ..., 2011-07-01 00:00:00]\nLength: 62, Freq: D, Timezone: None\n\n\n\n\nxticks:  &lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2011-05-03 00:00:00, ..., 2011-06-28 00:00:00]\nLength: 9, Freq: W-TUE, Timezone: None\n\n\n\n\n2011-06-04   -0.199393\n2011-06-05   -0.043118\n2011-06-06    0.477771\n2011-06-07   -0.033207\nFreq: D\n\n\nUpdate: I\'ve been able to get closer to the layout I wanted by using a loop to build the major xtick labels:\n\n# only show month for first label in month\nmonth = dStart.month - 1\nxticklabels = []\nfor x in xticks:\n    if  month != x.month :\n        xticklabels.append(x.strftime(\'%d\\n%a\\n%h\'))\n        month = x.month\n    else:\n        xticklabels.append(x.strftime(\'%d\\n%a\'))\n\n\nHowever, this is a bit like doing the x-axis using ax.annotate: possible but not ideal.\n'
"I've looking around for this but I can't seem to find it (though it must be extremely trivial).\n\nThe problem that I have is that I would like to retrieve the value of a column for the first and last entries of a data frame. But if I do:\n\ndf.ix[0]['date']\n\n\nI get:\n\ndatetime.datetime(2011, 1, 10, 16, 0)\n\n\nbut if I do:\n\ndf[-1:]['date']\n\n\nI get:\n\nmyIndex\n13         2011-12-20 16:00:00\nName: mydate\n\n\nwith a different format. Ideally, I would like to be able to access the value of the last index of the data frame, but I can't find how.\n\nI even tried to create a column (IndexCopy) with the values of the index and try:\n\ndf.ix[df.tail(1)['IndexCopy']]['mydate']\n\n\nbut this also yields a different format (since df.tail(1)['IndexCopy'] does not output a simple integer). \n\nAny ideas? \n"
'I have a Pandas data frame object of shape (X,Y) that looks like this:\n\n[[1, 2, 3],\n[4, 5, 6],\n[7, 8, 9]]\n\n\nand a numpy sparse matrix (CSC) of shape (X,Z) that looks something like this\n\n[[0, 1, 0],\n[0, 0, 1],\n[1, 0, 0]]\n\n\nHow can I add the content from the matrix to the data frame in a new named column such that the data frame will end up like this:\n\n[[1, 2, 3, [0, 1, 0]],\n[4, 5, 6, [0, 0, 1]],\n[7, 8, 9, [1, 0, 0]]]\n\n\nNotice the data frame now has shape (X, Y+1) and rows from the matrix are elements in the data frame.\n'
"I have 2 dataframes:\n\nrestaurant_ids_dataframe \n\nData columns (total 13 columns):\nbusiness_id      4503  non-null values\ncategories       4503  non-null values\ncity             4503  non-null values\nfull_address     4503  non-null values\nlatitude         4503  non-null values\nlongitude        4503  non-null values\nname             4503  non-null values\nneighborhoods    4503  non-null values\nopen             4503  non-null values\nreview_count     4503  non-null values\nstars            4503  non-null values\nstate            4503  non-null values\ntype             4503  non-null values\ndtypes: bool(1), float64(3), int64(1), object(8)`\n\n\nand \n\nrestaurant_review_frame\n\nInt64Index: 158430 entries, 0 to 229905\nData columns (total 8 columns):\nbusiness_id    158430  non-null values\ndate           158430  non-null values\nreview_id      158430  non-null values\nstars          158430  non-null values\ntext           158430  non-null values\ntype           158430  non-null values\nuser_id        158430  non-null values\nvotes          158430  non-null values\ndtypes: int64(1), object(7)\n\n\nI would like to join these two DataFrames to make them into a single dataframe using the DataFrame.join() command in pandas.\n\nI have tried the following line of code:\n\n#the following line of code creates a left join of restaurant_ids_frame and   restaurant_review_frame on the column 'business_id'\nrestaurant_review_frame.join(other=restaurant_ids_dataframe,on='business_id',how='left')\n\n\nBut when I try this I get the following error:\n\nException: columns overlap: Index([business_id, stars, type], dtype=object)\n\n\nI am very new to pandas and have no clue what I am doing wrong as far as executing the join statement is concerned.\n\nany help would be much appreciated.\n"
"Is there a way to select all but one column in a pandas DataFrame object? I've seen ways to delete a column, but I don't want to do that.\n"
"I need to Convert my list into a one column pandas dataframe \n\nCurrent List (len=3):\n\n['Thanks You',\n 'Its fine no problem',\n 'Are you sure']\n\n\nRequired Pandas DF (shape =3,):\n\n0 Thank You\n1 Its fine no problem\n2 Are you sure\n\n\nPlease note the numbers represent index in Required Pandas DF above.\n"
"I've noticed three methods of selecting a column in a Pandas DataFrame:\n\nFirst method of selecting a column using loc:\n\ndf_new = df.loc[:, 'col1']\n\n\nSecond method - seems simpler and faster:\n\ndf_new = df['col1']\n\n\nThird method - most convenient:\n\ndf_new = df.col1\n\n\nIs there a difference between these three methods? I don't think so, in which case I'd rather use the third method.\n\nI'm mostly curious as to why there appear to be three methods for doing the same thing.\n"
"\n  If you came here looking for information on how to\n  merge a DataFrame and Series on the index, please look at this\n  answer.\n  \n  The OP's original intention was to ask how to assign series elements\n  as columns to another DataFrame. If you are interested in knowing the\n  answer to this, look at the accepted answer by EdChum.\n\n\n\n\nBest I can come up with is\n\ndf = pd.DataFrame({'a':[1, 2], 'b':[3, 4]})  # see EDIT below\ns = pd.Series({'s1':5, 's2':6})\n\nfor name in s.index:\n    df[name] = s[name]\n\n   a  b  s1  s2\n0  1  3   5   6\n1  2  4   5   6\n\n\nCan anybody suggest better syntax / faster method? \n\nMy attempts:\n\ndf.merge(s)\nAttributeError: 'Series' object has no attribute 'columns'\n\n\nand\n\ndf.join(s)\nValueError: Other Series must have a name\n\n\nEDIT The first two answers posted highlighted a problem with my question, so please use the following to construct df:\n\ndf = pd.DataFrame({'a':[np.nan, 2, 3], 'b':[4, 5, 6]}, index=[3, 5, 6])\n\n\nwith the final result\n\n    a  b  s1  s2\n3 NaN  4   5   6\n5   2  5   5   6\n6   3  6   5   6\n\n"
"I have pandas DataFrame like this\n\n        X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n\n\nI want to write this data to a text file that looks like this:\n\n18 55 1 70   \n18 55 2 67 \n18 57 2 75     \n18 58 1 35  \n19 54 2 70 \n\n\nI have tried something like \n\nf = open(writePath, 'a')\nf.writelines(['\\n', str(data['X']), ' ', str(data['Y']), ' ', str(data['Z']), ' ', str(data['Value'])])\nf.close()\n\n\nbut it's not working. How to do this?      \n"
"I have DataFrame with column Sales.\n\nHow can I split it into 2 based on Sales value?\n\nFirst DataFrame will have data with 'Sales' &lt; s and second with 'Sales' &gt;= s \n"
"I am trying to re-index a pandas DataFrame object, like so,\n\nFrom:\n            a   b   c\n        0   1   2   3\n        1  10  11  12\n        2  20  21  22\n\nTo :\n           b   c\n       1   2   3\n      10  11  12\n      20  21  22\n\n\nI am going about this as shown below and am getting the wrong answer. Any clues on how to do this?\n\n&gt;&gt;&gt; col = ['a','b','c']\n&gt;&gt;&gt; data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n&gt;&gt;&gt; data\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n&gt;&gt;&gt; idx2 = data.a.values\n&gt;&gt;&gt; idx2\narray([ 1, 10, 20], dtype=int64)\n&gt;&gt;&gt; data2 = DataFrame(data,index=idx2,columns=col[1:])\n&gt;&gt;&gt; data2\n     b   c\n1   11  12\n10 NaN NaN\n20 NaN NaN\n\n\nAny idea why this is happening?\n"
'A Pandas DataFrame contains column named "date" that contains non-unique datetime values. \nI can group the lines in this frame using:\n\ndata.groupby(data[\'date\'])\n\n\nHowever, this splits the data by the datetime values. I would like to group these data by the year stored in the "date" column. This page shows how to group by year in cases where the time stamp is used as an index, which is not true in my case.\n\nHow do I achieve this grouping?\n'
'I have the following data frame in IPython, where each row is a single stock:\n\nIn [261]: bdata\nOut[261]:\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 21210 entries, 0 to 21209\nData columns:\nBloombergTicker      21206  non-null values\nCompany              21210  non-null values\nCountry              21210  non-null values\nMarketCap            21210  non-null values\nPriceReturn          21210  non-null values\nSEDOL                21210  non-null values\nyearmonth            21210  non-null values\ndtypes: float64(2), int64(1), object(4)\n\n\nI want to apply a groupby operation that computes cap-weighted average return across everything, per each date in the "yearmonth" column.\n\nThis works as expected:\n\nIn [262]: bdata.groupby("yearmonth").apply(lambda x: (x["PriceReturn"]*x["MarketCap"]/x["MarketCap"].sum()).sum())\nOut[262]:\nyearmonth\n201204      -0.109444\n201205      -0.290546\n\n\nBut then I want to sort of "broadcast" these values back to the indices in the original data frame, and save them as constant columns where the dates match.\n\nIn [263]: dateGrps = bdata.groupby("yearmonth")\n\nIn [264]: dateGrps["MarketReturn"] = dateGrps.apply(lambda x: (x["PriceReturn"]*x["MarketCap"]/x["MarketCap"].sum()).sum())\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/mnt/bos-devrnd04/usr6/home/espears/ws/Research/Projects/python-util/src/util/&lt;ipython-input-264-4a68c8782426&gt; in &lt;module&gt;()\n----&gt; 1 dateGrps["MarketReturn"] = dateGrps.apply(lambda x: (x["PriceReturn"]*x["MarketCap"]/x["MarketCap"].sum()).sum())\n\nTypeError: \'DataFrameGroupBy\' object does not support item assignment\n\n\nI realize this naive assignment should not work. But what is the "right" Pandas idiom for assigning the result of a groupby operation into a new column on the parent dataframe?\n\nIn the end, I want a column called "MarketReturn" than will be a repeated constant value for all indices that have matching date with the output of the groupby operation.\n\nOne hack to achieve this would be the following:\n\nmarketRetsByDate  = dateGrps.apply(lambda x: (x["PriceReturn"]*x["MarketCap"]/x["MarketCap"].sum()).sum())\n\nbdata["MarketReturn"] = np.repeat(np.NaN, len(bdata))\n\nfor elem in marketRetsByDate.index.values:\n    bdata["MarketReturn"][bdata["yearmonth"]==elem] = marketRetsByDate.ix[elem]\n\n\nBut this is slow, bad, and unPythonic.\n'
'I am accessing a series of Excel files in a for loop. I then read the data in the excel file to a pandas dataframe. I cant figure out how to append these dataframes together to then save the dataframe (now containing the data from all the files) as a new Excel file.\n\nHere\'s what I tried:\n\nfor infile in glob.glob("*.xlsx"):\n    data = pandas.read_excel(infile)\n    appended_data = pandas.DataFrame.append(data) # requires at least two arguments\nappended_data.to_excel("appended.xlsx")\n\n\nThanks!\n'
'I have a dataframe like this one:\n\nIn [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n\n\nHow to remove index name foo from that dataframe?\nThe desired output is like this:\n\nIn [10]: df\nOut[10]: \n         Column 1             \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n\n'
'The pandas read_csv() method interprets \'NA\' as nan (not a number) instead of a valid string.\n\nIn the simple case below note that the output in row 1, column 2 (zero based count) is \'nan\' instead of \'NA\'.\n\nsample.tsv (tab delimited)\n\n\n  PDB   CHAIN   SP_PRIMARY  RES_BEG RES_END PDB_BEG PDB_END SP_BEG  SP_END\n  5d8b  N   P60490  1   146 1   146 1   146\n  5d8b  NA  P80377  1   126 1   126 1   126\n  5d8b  O   P60491  1   118 1   118 1   118\n\n\nread_sample.py\n\nimport pandas as pd\n\ndf = pd.read_csv(\n    \'sample.tsv\',\n    sep=\'\\t\',\n    encoding=\'utf-8\',\n)\n\nfor df_tuples in df.itertuples(index=True):\n    print(df_tuples)\n\n\noutput\n\n\n  (0, u\'5d8b\', u\'N\', u\'P60490\', 1, 146, 1, 146, 1, 146)\n  (1, u\'5d8b\', nan, u\'P80377\', 1, 126, 1, 126, 1, 126)\n  (2, u\'5d8b\', u\'O\', u\'P60491\', 1, 118, 1, 118, 1, 118)\n\n\nAdditional Information\n\nRe-writing the file with quotes for data in the \'CHAIN\' column and then using the quotechar parameter quotechar=\'\\\'\' has the same result. And passing a dictionary of types via the dtype parameter dtype=dict(valid_cols) does not change the result.\n\nAn old answer to Prevent pandas from automatically inferring type in read_csv suggests first using a numpy record array to parse the file, but given the ability to now specify column dtypes, this shouldn\'t be necessary.\n\nNote that itertuples() is used to preserve dtypes as described in the iterrows documentation: "To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns tuples of the values and which is generally faster as iterrows."\n\nExample was tested on Python 2 and 3 with pandas version 0.16.2, 0.17.0, and 0.17.1.\n\n\n\nIs there a way to capture a valid string \'NA\' instead of it being converted to nan?\n'
'I have done some searching for the answer to this question, but all I can figure out is this: \n\ndf[df.columns[len(df.columns)-1]]\n\n\nwhich to me seems unweildy, and un-pythonic (and slow?). \n\nWhat is the easiest way to select the data for the last column in a pandas dataframe without specifying the name of the column?\n'
'I have a dataframe in which some rows contain missing values. \n\nIn [31]: df.head()\nOut[31]: \n                             alpha1  alpha2    gamma1    gamma2       chi2min  \nfilename                                                                        \nM66_MI_NSRh35d32kpoints.dat  0.8016  0.9283  1.000000  0.074804  3.985599e+01   \nF71_sMI_DMRI51d.dat          0.0000  0.0000       NaN  0.000000  1.000000e+25   \nF62_sMI_St22d7.dat           1.7210  3.8330  0.237480  0.150000  1.091832e+01   \nF41_Car_HOC498d.dat          1.1670  2.8090  0.364190  0.300000  7.966335e+00   \nF78_MI_547d.dat              1.8970  5.4590  0.095319  0.100000  2.593468e+01 \n\n\nI want to display those rows on the screen. If I try df.isnull(), it gives a long dataframe with True and False. Is there any way by which I can select these rows and print them on the screen?\n'
"I have diferent dataframes and need to merge them together based on the date column. If I only had two dataframes, I could use df1.merge(df2, on='date'), to do it with three dataframes, I use df1.merge(df2.merge(df3, on='date'), on='date'), however it becomes really complex and unreadable to do it with multiple dataframes. \n\nAll dataframes have one column in common -date, but they don't have the same number of rows nor columns and I only need those rows in which each date is common to every dataframe.\n\nSo, I'm trying to write a recursion function that returns a dataframe with all data but it didn't work. How should I merge multiple dataframes then?\n\nI tried diferent ways and got errors like out of range, keyerror 0/1/2/3 and can not merge DataFrame with instance of type &lt;class 'NoneType'&gt;.\n\nThis is the script I wrote:\n\ndfs = [df1, df2, df3] # list of dataframes\n\ndef mergefiles(dfs, countfiles, i=0):\n    if i == (countfiles - 2): # it gets to the second to last and merges it with the last\n        return\n\n    dfm = dfs[i].merge(mergefiles(dfs[i+1], countfiles, i=i+1), on='date')\n    return dfm\n\nprint(mergefiles(dfs, len(dfs)))\n\n\nAn example:\ndf_1:\n\nMay 19, 2017;1,200.00;0.1%\nMay 18, 2017;1,100.00;0.1%\nMay 17, 2017;1,000.00;0.1%\nMay 15, 2017;1,901.00;0.1%\n\n\ndf_2:\n\nMay 20, 2017;2,200.00;1000000;0.2%\nMay 18, 2017;2,100.00;1590000;0.2%\nMay 16, 2017;2,000.00;1230000;0.2%\nMay 15, 2017;2,902.00;1000000;0.2%\n\n\ndf_3:\n\nMay 21, 2017;3,200.00;2000000;0.3%\nMay 17, 2017;3,100.00;2590000;0.3%\nMay 16, 2017;3,000.00;2230000;0.3%\nMay 15, 2017;3,903.00;2000000;0.3%\n\n\nExpected merge result:\n\nMay 15, 2017;  1,901.00;0.1%;  2,902.00;1000000;0.2%;   3,903.00;2000000;0.3%   \n\n"
'I have a column in a pandas DataFrame that I would like to split on a single space. The splitting is simple enough with DataFrame.str.split(\' \'), but I can\'t make a new column from the last entry. When I .str.split() the column I get a list of arrays and I don\'t know how to manipulate this to get a new column for my DataFrame.\n\nHere is an example. Each entry in the column contains \'symbol data price\' and I would like to split off the price (and eventually remove the "p"... or "c" in half the cases).\n\nimport pandas as pd\ntemp = pd.DataFrame({\'ticker\' : [\'spx 5/25/2001 p500\', \'spx 5/25/2001 p600\', \'spx 5/25/2001 p700\']})\ntemp2 = temp.ticker.str.split(\' \')\n\n\nwhich yields\n\n0    [\'spx\', \'5/25/2001\', \'p500\']\n1    [\'spx\', \'5/25/2001\', \'p600\']\n2    [\'spx\', \'5/25/2001\', \'p700\']\n\n\nBut temp2[0] just gives one list entry\'s array and temp2[:][-1] fails. How can I convert the last entry in each array to a new column? Thanks!\n'
"Is there any method to replace values with None in Pandas in Python?\n\nYou can use df.replace('pre', 'post') and can replace a value with another, but this can't be done if you want to replace with None value, which if you try, you get a strange result.\n\nSo here's an example:\n\ndf = DataFrame(['-',3,2,5,1,-5,-1,'-',9])\ndf.replace('-', 0)\n\n\nwhich returns a successful result.\n\nBut,\n\ndf.replace('-', None)\n\n\nwhich returns a following result:\n\n0\n0   - // this isn't replaced\n1   3\n2   2\n3   5\n4   1\n5  -5\n6  -1\n7  -1 // this is changed to `-1`...\n8   9\n\n\nWhy does such a strange result be returned?\n\nSince I want to pour this data frame into MySQL database, I can't put NaN values into any element in my data frame and instead want to put None. Surely, you can first change '-' to NaN and then convert NaN to None, but I want to know why the dataframe acts in such a terrible way.\n\n\n  Tested on pandas 0.12.0 dev on Python 2.7 and OS X 10.8. Python is a\n  pre-installed version on OS X and I installed pandas by using SciPy\n  Superpack script, for your information.\n\n"
"Supposely, I have the bar chart as below:\n\n\n\nAny ideas on how to set different colors for each carrier? As for example, AK would be Red, GA would be Green, etc?\n\nI am using Pandas and matplotlib in Python\n\n&gt;&gt;&gt; f=plt.figure()\n&gt;&gt;&gt; ax=f.add_subplot(1,1,1)\n&gt;&gt;&gt; ax.bar([1,2,3,4], [1,2,3,4])\n&lt;Container object of 4 artists&gt;\n&gt;&gt;&gt; ax.get_children()\n[&lt;matplotlib.axis.XAxis object at 0x6529850&gt;, &lt;matplotlib.axis.YAxis object at 0x78460d0&gt;,  &lt;matplotlib.patches.Rectangle object at 0x733cc50&gt;, &lt;matplotlib.patches.Rectangle object at 0x733cdd0&gt;, &lt;matplotlib.patches.Rectangle object at 0x777f290&gt;, &lt;matplotlib.patches.Rectangle object at 0x777f710&gt;, &lt;matplotlib.text.Text object at 0x7836450&gt;, &lt;matplotlib.patches.Rectangle object at 0x7836390&gt;, &lt;matplotlib.spines.Spine object at 0x6529950&gt;, &lt;matplotlib.spines.Spine object at 0x69aef50&gt;, &lt;matplotlib.spines.Spine object at 0x69ae310&gt;, &lt;matplotlib.spines.Spine object at 0x69aea50&gt;]\n&gt;&gt;&gt; ax.get_children()[2].set_color('r') #You can also try to locate the first patches.Rectangle object instead of direct calling the index.\n\n\nFor the suggestions above, how do exactly we could enumerate ax.get_children() and check if the object type is rectangle? So if the object is rectangle, we would assign different random color?\n"
"From what I understand about a left outer join, the resulting table should never have more rows than the left table...Please let me know if this is wrong... \n\nMy left table is 192572 rows and 8 columns.\n\nMy right table is 42160 rows and 5 columns.\n\nMy Left table has a field called 'id' which matches with a column in my right table called 'key'.\n\nTherefore I merge them as such:\n\ncombined = pd.merge(a,b,how='left',left_on='id',right_on='key')\n\n\nBut then the combined shape is 236569.\n\nWhat am I misunderstanding? \n"
"Is that any way that I can get first element of Seires without have information on index.\n\nFor example,We have a Series\n\n    import pandas as pd\n    key='MCS096'\n    SUBJECTS=pd.DataFrame({'ID':Series([146],index=[145]),\\\n                   'study':Series(['MCS'],index=[145]),\\\n                   'center':Series(['Mag'],index=[145]),\\\n                   'initials':Series(['MCS096'],index=[145])\n                   })\n\n\nprints out SUBJECTS:\n\n    print (SUBJECTS[SUBJECTS.initials==key]['ID'])\n    145    146\n    Name: ID, dtype: int64\n\n\nHow can I get the value here 146 without using index 145?\n\nThank you very much\n"
"When using this in a script (not IPython), nothing happens, i.e. the plot window doesn't appear :\n\nimport numpy as np\nimport pandas as pd\nts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\nts.plot()\n\n\nEven when adding time.sleep(5), there is still nothing. Why?\n\nIs there a way to do it, without having to manually call matplotlib ?\n"
'I am running this cell in IPython Notebook:\n\n# salaries and teams are Pandas dataframe\nsalaries.head()\nteams.head()\n\n\nThe result is that I am only getting the output of teams data-frame rather than of both salaries and teams. If I just run salaries.head() I get the result for salaries data-frame but on running both the statement I just see the output of teams.head(). How can I correct this?\n'
"I have a dataframe that looks like this:\n\n              Company Name              Organisation Name  Amount\n10118  Vifor Pharma UK Ltd  Welsh Assoc for Gastro &amp; Endo 2700.00\n10119  Vifor Pharma UK Ltd    Welsh IBD Specialist Group,  169.00\n10120  Vifor Pharma UK Ltd             West Midlands AHSN 1200.00\n10121  Vifor Pharma UK Ltd           Whittington Hospital   63.00\n10122  Vifor Pharma UK Ltd                 Ysbyty Gwynedd   75.93\n\n\nHow do I sum the Amount and count the Organisation Name, to get a new dataframe that looks like this?\n\n              Company Name             Organisation Count   Amount\n10118  Vifor Pharma UK Ltd                              5 11000.00\n\n\nI know how to sum or count:\n\ndf.groupby('Company Name').sum()\ndf.groupby('Company Name').count()\n\n\nBut not how to do both!\n"
'How to read a modestly sized Parquet data-set into an in-memory Pandas DataFrame without setting up a cluster computing infrastructure such as Hadoop or Spark? This is only a moderate amount of data that I would like to read in-memory with a simple Python script on a laptop. The data does not reside on HDFS. It is either on the local file system or possibly in S3. I do not want to spin up and configure other services like Hadoop, Hive or Spark.\n\nI thought Blaze/Odo would have made this possible: the Odo documentation mentions Parquet, but the examples seem all to be going through an external Hive runtime.\n'
"How do i convert a pandas index of strings to datetime format\n\nmy dataframe 'df' is like this\n\n                     value          \n2015-09-25 00:46    71.925000\n2015-09-25 00:47    71.625000\n2015-09-25 00:48    71.333333\n2015-09-25 00:49    64.571429\n2015-09-25 00:50    72.285714\n\n\nbut the index is of type string, but i need it a datetime format because i get the error\n\n'Index' object has no attribute 'hour'\n\n\nwhen using \n\n df['A'] = df.index.hour\n\n"
'How to group values of pandas dataframe and select the latest(by date) from each group? \n\nFor example, given a dataframe sorted by date:\n\n    id     product   date\n0   220    6647     2014-09-01 \n1   220    6647     2014-09-03 \n2   220    6647     2014-10-16\n3   826    3380     2014-11-11\n4   826    3380     2014-12-09\n5   826    3380     2015-05-19\n6   901    4555     2014-09-01\n7   901    4555     2014-10-05\n8   901    4555     2014-11-01\n\n\ngrouping by id or product, and selecting the earliest gives:\n\n    id     product   date\n2   220    6647     2014-10-16\n5   826    3380     2015-05-19\n8   901    4555     2014-11-01\n\n'
"I am using Pandas dataframes and want to create a new column as a function of existing columns. I have not seen a good discussion of the speed difference between df.apply() and np.vectorize(), so I thought I would ask here. \n\nThe Pandas apply() function is slow. From what I measured (shown below in some experiments), using np.vectorize() is 25x faster (or more) than using the DataFrame function apply() , at least on my 2016 MacBook Pro. Is this an expected result, and why? \n\nFor example, suppose I have the following dataframe with N rows:\n\nN = 10\nA_list = np.random.randint(1, 100, N)\nB_list = np.random.randint(1, 100, N)\ndf = pd.DataFrame({'A': A_list, 'B': B_list})\ndf.head()\n#     A   B\n# 0  78  50\n# 1  23  91\n# 2  55  62\n# 3  82  64\n# 4  99  80\n\n\nSuppose further that I want to create a new column as a function of the two columns A and B. In the example below, I'll use a simple function divide(). To apply the function, I can use either df.apply() or np.vectorize():\n\ndef divide(a, b):\n    if b == 0:\n        return 0.0\n    return float(a)/b\n\ndf['result'] = df.apply(lambda row: divide(row['A'], row['B']), axis=1)\n\ndf['result2'] = np.vectorize(divide)(df['A'], df['B'])\n\ndf.head()\n#     A   B    result   result2\n# 0  78  50  1.560000  1.560000\n# 1  23  91  0.252747  0.252747\n# 2  55  62  0.887097  0.887097\n# 3  82  64  1.281250  1.281250\n# 4  99  80  1.237500  1.237500\n\n\nIf I increase N to real-world sizes like 1 million or more, then I observe that np.vectorize() is 25x faster or more than df.apply().\n\nBelow is some complete benchmarking code:\n\nimport pandas as pd\nimport numpy as np\nimport time\n\ndef divide(a, b):\n    if b == 0:\n        return 0.0\n    return float(a)/b\n\nfor N in [1000, 10000, 100000, 1000000, 10000000]:    \n\n    print ''\n    A_list = np.random.randint(1, 100, N)\n    B_list = np.random.randint(1, 100, N)\n    df = pd.DataFrame({'A': A_list, 'B': B_list})\n\n    start_epoch_sec = int(time.time())\n    df['result'] = df.apply(lambda row: divide(row['A'], row['B']), axis=1)\n    end_epoch_sec = int(time.time())\n    result_apply = end_epoch_sec - start_epoch_sec\n\n    start_epoch_sec = int(time.time())\n    df['result2'] = np.vectorize(divide)(df['A'], df['B'])\n    end_epoch_sec = int(time.time())\n    result_vectorize = end_epoch_sec - start_epoch_sec\n\n\n    print 'N=%d, df.apply: %d sec, np.vectorize: %d sec' % \\\n            (N, result_apply, result_vectorize)\n\n    # Make sure results from df.apply and np.vectorize match.\n    assert(df['result'].equals(df['result2']))\n\n\nThe results are shown below:\n\nN=1000, df.apply: 0 sec, np.vectorize: 0 sec\n\nN=10000, df.apply: 1 sec, np.vectorize: 0 sec\n\nN=100000, df.apply: 2 sec, np.vectorize: 0 sec\n\nN=1000000, df.apply: 24 sec, np.vectorize: 1 sec\n\nN=10000000, df.apply: 262 sec, np.vectorize: 4 sec\n\n\nIf np.vectorize() is in general always faster than df.apply(), then why is np.vectorize() not mentioned more? I only ever see StackOverflow posts related to df.apply(), such as:\n\npandas create new column based on values from other columns\n\nHow do I use Pandas &#39;apply&#39; function to multiple columns?\n\nHow to apply a function to two columns of Pandas dataframe\n"
'I have a list of tuples like\n\ndata = [\n(\'r1\', \'c1\', avg11, stdev11),\n(\'r1\', \'c2\', avg12, stdev12),\n(\'r2\', \'c1\', avg21, stdev21),\n(\'r2\', \'c2\', avg22, stdev22)\n]\n\n\nand I would like to put them into a pandas DataFrame with rows named by the first column and columns named by the 2nd column. It seems the way to take care of the row names is something like pandas.DataFrame([x[1:] for x in data], index = [x[0] for x in data]) but how do I take care of the columns to get a 2x2 matrix (the output from the previous set is 3x4)? Is there a more intelligent way of taking care of row labels as well, instead of explicitly omitting them?\n\nEDIT It seems I will need 2 DataFrames - one for averages and one for standard deviations, is that correct? Or can I store a list of values in each "cell"?\n'
'I would like the element-wise logical OR operator. I know "or" itself is not what I am looking for.\n\nI am aware that AND corresponds to &amp; and NOT, ~. But what about OR? \n'
'I have a Python Pandas DataFrame object containing textual data. My problem is, that when I use to_html() function, it truncates the strings in the output.\n\nFor example:\n\nimport pandas\ndf = pandas.DataFrame({\'text\': [\'Lorem ipsum dolor sit amet, consectetur adipiscing elit.\']})\nprint (df.to_html())\n\n\nThe output is truncated at adapis...\n\n&lt;table border="1" class="dataframe"&gt;\n  &lt;thead&gt;\n    &lt;tr style="text-align: right;"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;text&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n    &lt;tr&gt;\n      &lt;th&gt;0&lt;/th&gt;\n      &lt;td&gt; Lorem ipsum dolor sit amet, consectetur adipis...&lt;/td&gt;\n    &lt;/tr&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n\n\nThere is a related question on SO, but it uses placeholders and search/replace functionality to postprocess the HTML, which I would like to avoid:\n\n\nWriting full contents of Pandas dataframe to HTML table\n\n\nIs there a simpler solution to this problem? I could not find anything related from the documentation.\n'
'I would like to know if there is someway of replacing all DataFrame negative numbers by zeros?\n'
"I would like to add a column 'D' to a dataframe like this:\n\nU,L\n111,en\n112,en\n112,es\n113,es\n113,ja\n113,zh\n114,es\n\n\nbased on the following Dictionary:\n\nd = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\n\n\nso that the resulting dataframe appears as:\n\nU,L,D\n111,en,en\n112,en,en\n112,es,en\n113,es,es\n113,ja,es\n113,zh,es\n114,es,es\n\n\nSo far I tried the pd.join() method but I can't figured out how it works with Dictionaries.\n"
'I want to apply scaling (using StandardScaler() from sklearn.preprocessing) to a pandas dataframe. The following code returns a numpy array, so I lose all the column names and indeces. This is not what I want.\n\nfeatures = df[["col1", "col2", "col3", "col4"]]\nautoscaler = StandardScaler()\nfeatures = autoscaler.fit_transform(features)\n\n\nA "solution" I found online is:\n\nfeatures = features.apply(lambda x: autoscaler.fit_transform(x))\n\n\nIt appears to work, but leads to a deprecationwarning:\n\n\n  /usr/lib/python3.5/site-packages/sklearn/preprocessing/data.py:583:\n  DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17\n  and will raise ValueError in 0.19. Reshape your data either using\n  X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1)\n  if it contains a single sample.\n\n\nI therefore tried:\n\nfeatures = features.apply(lambda x: autoscaler.fit_transform(x.reshape(-1, 1)))\n\n\nBut this gives:\n\n\n  Traceback (most recent call last):   File "./analyse.py", line 91, in\n  \n      features = features.apply(lambda x: autoscaler.fit_transform(x.reshape(-1, 1)))   File\n  "/usr/lib/python3.5/site-packages/pandas/core/frame.py", line 3972, in\n  apply\n      return self._apply_standard(f, axis, reduce=reduce)   File "/usr/lib/python3.5/site-packages/pandas/core/frame.py", line 4081, in\n  _apply_standard\n      result = self._constructor(data=results, index=index)   File "/usr/lib/python3.5/site-packages/pandas/core/frame.py", line 226, in\n  init\n      mgr = self._init_dict(data, index, columns, dtype=dtype)   File "/usr/lib/python3.5/site-packages/pandas/core/frame.py", line 363, in\n  _init_dict\n      dtype=dtype)   File "/usr/lib/python3.5/site-packages/pandas/core/frame.py", line 5163, in\n  _arrays_to_mgr\n      arrays = _homogenize(arrays, index, dtype)   File "/usr/lib/python3.5/site-packages/pandas/core/frame.py", line 5477, in\n  _homogenize\n      raise_cast_failure=False)   File "/usr/lib/python3.5/site-packages/pandas/core/series.py", line 2885,\n  in _sanitize_array\n      raise Exception(\'Data must be 1-dimensional\') Exception: Data must be 1-dimensional\n\n\nHow do I apply scaling to the pandas dataframe, leaving the dataframe intact? Without copying the data if possible.\n'
"Cleaning the values of a multitype data frame in python/pandas, I want to trim the strings. I am currently doing it in two instructions :\n\nimport pandas as pd\n\ndf = pd.DataFrame([['  a  ', 10], ['  c  ', 5]])\n\ndf.replace('^\\s+', '', regex=True, inplace=True) #front\ndf.replace('\\s+$', '', regex=True, inplace=True) #end\n\ndf.values\n\n\nThis is quite slow, what could I improve ?\n"
'I have an OHLC price data set, that I have parsed from CSV into a Pandas dataframe and resampled to 15 min bars:\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 500047 entries, 1998-05-04 04:45:00 to 2012-08-07 00:15:00\nFreq: 15T\nData columns:\nClose    363152  non-null values\nHigh     363152  non-null values\nLow      363152  non-null values\nOpen     363152  non-null values\ndtypes: float64(4)\n\n\nI would like to add various calculated columns, starting with simple ones such as period Range (H-L) and then booleans to indicate the occurrence of price patterns that I will define - e.g. a hammer candle pattern, for which a sample definition:\n\ndef closed_in_top_half_of_range(h,l,c):\n    return c &gt; l + (h-l)/2\n\ndef lower_wick(o,l,c):\n    return min(o,c)-l\n\ndef real_body(o,c):\n    return abs(c-o)\n\ndef lower_wick_at_least_twice_real_body(o,l,c):\n    return lower_wick(o,l,c) &gt;= 2 * real_body(o,c)\n\ndef is_hammer(row):\n    return lower_wick_at_least_twice_real_body(row["Open"],row["Low"],row["Close"]) \\\n    and closed_in_top_half_of_range(row["High"],row["Low"],row["Close"])\n\n\nBasic problem: how do I map the function to the column, specifically where I would like to reference more than one other column or the whole row or whatever? \n\nThis post deals with adding two calculated columns off of a single source column, which is close, but not quite it.\n\nAnd slightly more advanced: for price patterns that are determined with reference to more than a single bar (T), how can I reference different rows (e.g. T-1, T-2 etc.) from within the function definition?\n'
"I have two DataFrames which I want to merge based on a column. However, due to alternate spellings, different number of spaces, absence/presence of diacritical marks, I would like to be able to merge as long as they are similar to one another.\n\nAny similarity algorithm will do (soundex, Levenshtein, difflib's). \n\nSay one DataFrame has the following data:\n\ndf1 = DataFrame([[1],[2],[3],[4],[5]], index=['one','two','three','four','five'], columns=['number'])\n\n       number\none         1\ntwo         2\nthree       3\nfour        4\nfive        5\n\ndf2 = DataFrame([['a'],['b'],['c'],['d'],['e']], index=['one','too','three','fours','five'], columns=['letter'])\n\n      letter\none        a\ntoo        b\nthree      c\nfours      d\nfive       e\n\n\nThen I want to get the resulting DataFrame\n\n       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n\n"
'What is the best way to make a series of scatter plots using matplotlib from a pandas dataframe in Python? \n\nFor example, if I have a dataframe df that has some columns of interest, I find myself typically converting everything to arrays:\n\nimport matplotlib.pylab as plt\n# df is a DataFrame: fetch col1 and col2 \n# and drop na rows if any of the columns are NA\nmydata = df[["col1", "col2"]].dropna(how="any")\n# Now plot with matplotlib\nvals = mydata.values\nplt.scatter(vals[:, 0], vals[:, 1])\n\n\nThe problem with converting everything to array before plotting is that it forces you to break out of dataframes.\n\nConsider these two use cases where having the full dataframe is essential to plotting:\n\n\nFor example, what if you wanted to now look at all the values of col3 for the corresponding values that you plotted in the call to scatter, and color each point (or size) it by that value? You\'d have to go back, pull out the non-na values of col1,col2 and check what their corresponding values.\n\nIs there a way to plot while preserving the dataframe? For example:\n\nmydata = df.dropna(how="any", subset=["col1", "col2"])\n# plot a scatter of col1 by col2, with sizes according to col3\nscatter(mydata(["col1", "col2"]), s=mydata["col3"])\n\nSimilarly, imagine that you wanted to filter or color each point differently depending on the values of some of its columns. E.g. what if you wanted to automatically plot the labels of the points that meet a certain cutoff on col1, col2 alongside them (where the labels are stored in another column of the df), or color these points differently, like people do with dataframes in R.  For example:\n\nmydata = df.dropna(how="any", subset=["col1", "col2"]) \nmyscatter = scatter(mydata[["col1", "col2"]], s=1)\n# Plot in red, with smaller size, all the points that \n# have a col2 value greater than 0.5\nmyscatter.replot(mydata["col2"] &gt; 0.5, color="red", s=0.5)\n\n\n\nHow can this be done?\n\nEDIT Reply to crewbum:\n\nYou say that the best way is to plot each condition (like subset_a, subset_b) separately. What if you have many conditions, e.g. you want to split up the scatters into 4 types of points or even more, plotting each in different shape/color. How can you elegantly apply condition a, b, c, etc. and make sure you then plot "the rest" (things not in any of these conditions) as the last step? \n\nSimilarly in your example where you plot col1,col2 differently based on col3, what if there are NA values that break the association between col1,col2,col3? For example if you want to plot all col2 values based on their col3 values, but some rows have an NA value in either col1 or col3, forcing you to use dropna first. So you would do:\n\nmydata = df.dropna(how="any", subset=["col1", "col2", "col3")\n\n\nthen you can plot using mydata like you show -- plotting the scatter between col1,col2 using the values of col3. But mydata will be missing some points that have values for col1,col2 but are NA for col3, and those still have to be plotted... so how would you basically plot "the rest" of the data, i.e. the points that are not in the filtered set mydata?\n'
"data = {'name' : ['bill', 'joe', 'steve'],\n    'test1' : [85, 75, 85],\n    'test2' : [35, 45, 83],\n     'test3' : [51, 61, 45]}\nframe = pd.DataFrame(data)\n\n\nI would like to add a new column that shows the max value for each row.\n\ndesired output:\n\n name test1 test2 test3 HighScore\n bill  75    75    85    85\n joe   35    45    83    83 \n steve  51   61    45    61 \n\n\nSometimes  \n\nframe['HighScore'] = max(data['test1'], data['test2'], data['test3'])\n\n\nworks but most of the time gives this error:\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nWhy does it only work sometimes?  Is there another way of doing it?\n"
"How do I add multiple empty columns to a DataFrame from a list?\nI can do:\n    df[&quot;B&quot;] = None\n    df[&quot;C&quot;] = None\n    df[&quot;D&quot;] = None\n\nBut I can't do:\n    df[[&quot;B&quot;, &quot;C&quot;, &quot;D&quot;]] = None\n\n\nKeyError: &quot;['B' 'C' 'D'] not in index&quot;\n\n"
'Say we have used pandas dataframe[column].value_counts() which outputs:\n\n apple   5 \n sausage 2\n banana  2\n cheese  1\n\n\nHow do you extract the values in the order same as shown above from max to min ?\n\ne.g: [apple,sausage,banana,cheese]\n'
"What's a simple and efficient way to shuffle a dataframe in pandas, by rows or by columns? I.e. how to write a function shuffle(df, n, axis=0) that takes a dataframe, a number of shuffles n, and an axis (axis=0 is rows, axis=1 is columns) and returns a copy of the dataframe that has been shuffled n times. \n\nEdit: key is to do this without destroying the row/column labels of the dataframe. If you just shuffle df.index that loses all that information. I want the resulting df to be the same as the original except with the order of rows or order of columns different.\n\nEdit2: My question was unclear. When I say shuffle the rows, I mean shuffle each row independently. So if you have two columns a and b, I want each row shuffled on its own, so that you don't have the same associations between a and b as you do if you just re-order each row as a whole. Something like: \n\nfor 1...n:\n  for each col in df: shuffle column\nreturn new_df\n\n\nBut hopefully more efficient than naive looping. This does not work for me:\n\ndef shuffle(df, n, axis=0):\n        shuffled_df = df.copy()\n        for k in range(n):\n            shuffled_df.apply(np.random.shuffle(shuffled_df.values),axis=axis)\n        return shuffled_df\n\ndf = pandas.DataFrame({'A':range(10), 'B':range(10)})\nshuffle(df, 5)\n\n"
"I use Pandas 'ver 0.12.0' with Python 2.7 and have a dataframe as below:\n\ndf = pd.DataFrame({'id' : [123,512,'zhub1', 12354.3, 129, 753, 295, 610],\n                    'colour': ['black', 'white','white','white',\n                            'black', 'black', 'white', 'white'],\n                    'shape': ['round', 'triangular', 'triangular','triangular','square',\n                                        'triangular','round','triangular']\n                    },  columns= ['id','colour', 'shape'])\n\n\nThe id Series consists of some integers and strings. Its dtype by default is object. I want to convert all contents of id to strings. I tried astype(str), which produces the output below.\n\ndf['id'].astype(str)\n0    1\n1    5\n2    z\n3    1\n4    1\n5    7\n6    2\n7    6\n\n\n1) How can I convert all elements of id to String? \n\n2) I will eventually use id for indexing for dataframes. Would having String indices in a dataframe slow things down, compared to having an integer index?\n"
"I have an n-by-m Pandas DataFrame df defined as follows. (I know this is not the best way to do it. It makes sense for what I'm trying to do in my actual code, but that would be TMI for this post so just take my word that this approach works in my particular scenario.)\n\n&gt;&gt;&gt; df = DataFrame(columns=['col1'])\n&gt;&gt;&gt; df.append(Series([None]), ignore_index=True)\n&gt;&gt;&gt; df\nEmpty DataFrame\nColumns: [col1]\nIndex: []\n\n\nI stored lists in the cells of this DataFrame as follows.\n\n&gt;&gt;&gt; df['column1'][0] = [1.23, 2.34]\n&gt;&gt;&gt; df\n     col1\n0  [1, 2]\n\n\nFor some reason, the DataFrame stored this list as a string instead of a list.\n\n&gt;&gt;&gt; df['column1'][0]\n'[1.23, 2.34]'\n\n\nI have 2 questions for you.\n\n\nWhy does the DataFrame store a list as a string and is there a way around this behavior?\nIf not, then is there a Pythonic way to convert this string into a list?\n\n\n\n\nUpdate\n\nThe DataFrame I was using had been saved and loaded from a CSV format. This format, rather than the DataFrame itself, converted the list from a string to a literal.\n"
"I'm having trouble figuring out how to skip n rows in a csv file but keep the header which is the 1 row.\n\nWhat I want to do is iterate but keep the header from the first row.  skiprows makes the header the first row after the skipped rows.  What is the best way of doing this?\n\ndata = pd.read_csv('test.csv', sep='|', header=0, skiprows=10, nrows=10)\n\n"
"I have a data file from columns A-G like below but when I am reading it with pd.read_csv('data.csv') it prints an extra unnamed column at the end for no reason. \n\ncolA    ColB    colC    colD    colE    colF    colG    Unnamed: 7\n44      45      26      26      40      26      46        NaN\n47      16      38      47      48      22      37        NaN\n19      28      36      18      40      18      46        NaN\n50      14      12      33      12      44      23        NaN\n39      47      16      42      33      48      38        NaN\n\n\nI have seen my data file various times but I have no extra data in any other column. How I should remove this extra column while reading ? Thanks\n"
"How Do I add a single item to a serialized panda series. I know it's not the most efficient way memory wise, but i still need to do that.\n\nSomething along:\n\n&gt;&gt; x = Series()\n&gt;&gt; N = 4\n&gt;&gt; for i in xrange(N):\n&gt;&gt;     x.some_appending_function(i**2)    \n&gt;&gt; print x\n\n0 | 0\n1 | 1\n2 | 4\n3 | 9\n\n\nalso, how can i add a single row to a pandas DataFrame? \n"
'In R, you can combine two dataframes by sticking the columns of one onto the bottom of the columns of the other using rbind. In pandas, how do you accomplish the same thing? It seems bizarrely difficult. \n\nUsing append results in a horrible mess including NaNs and things for reasons I don\'t understand. I\'m just trying to "rbind" two identical frames that look like this:\n\nEDIT: I was creating the DataFrames in a stupid way, which was causing issues. Append=rbind to all intents and purposes. See answer below.\n\n        0         1       2        3          4          5        6                    7\n0   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n\n\nBut I\'m getting something horrible a la this: \n\n        0         1        2        3          4         5        6                    7       0         1       2        3          4          5        6                    7\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  AMEC.L  20130220  1030.0  1040.00  1024.0000  1035.0000  1972517  2013-02-20 18:47:43\n4     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AAL.L  20130220  1998.0  2014.50  1942.4999  1951.0000  3666033  2013-02-20 18:47:44\n5     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  ANTO.L  20130220  1093.0  1097.00  1064.7899  1068.0000  2183931  2013-02-20 18:47:44\n6     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ARM.L  20130220   941.5   965.10   939.4250   951.5001  2994652  2013-02-20 18:47:45\n0     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADN.L  20130220   437.4   442.37   436.5000   441.9000  2775364  2013-02-20 18:47:42\n1     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   ADM.L  20130220  1279.0  1300.00  1272.0000  1285.0000   967730  2013-02-20 18:47:42\n2     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN   AGK.L  20130220  1717.0  1749.00  1709.0000  1739.0000   834534  2013-02-20 18:47:43\n3     NaN       NaN      NaN      NaN        NaN       NaN      NaN                  NaN  \n\n\nAnd I don\'t understand why. I\'m starting to miss R :(\n'
'I need to create a data frame by reading in data from a file, using read_csv method. However, the separators are not very regular: some columns are separated by tabs (\\t), other are separated by spaces. Moreover, some columns can be separated by 2 or 3 or more spaces or even by a combination of spaces and tabs (for example 3 spaces, two tabs and then 1 space).\n\nIs there a way to tell pandas to treat these files properly?\n\nBy the way, I do not have this problem if I use Python. I use:\n\nfor line in file(file_name):\n   fld = line.split()\n\n\nAnd it works perfect. It does not care if there are 2 or 3 spaces between the fields. Even combinations of spaces and tabs do not cause any problem. Can pandas do the same?\n'
'I have a data frame that looks like this:\n\ncompany  Amazon  Apple  Yahoo\nname\nA             0    130      0\nC           173      0      0\nZ             0      0    150\n\n\nIt was created using this code:\n\nimport pandas as pd\ndf = pd.DataFrame({\'name\' : [\'A\', \'Z\',\'C\'],\n                   \'company\' : [\'Apple\', \'Yahoo\',\'Amazon\'],\n                   \'height\' : [130, 150,173]})\n\ndf = df.pivot(index="name", columns="company", values="height").fillna(0)\n\n\nWhat I want to do is to sort the row (with index name) according to a predefined list ["Z", "C", "A"].  Resulting in this :\n\ncompany  Amazon  Apple  Yahoo\nname\nZ             0      0    150\nC           173      0      0\nA             0    130      0\n\n\nHow can I achieve that?\n'
'I have two dataframes and each one has two index columns. I would like to merge them. For example, the first dataframe is the following:\n\n                   V1\n\nA      1/1/2012    12\n       2/1/2012    14\nB      1/1/2012    15\n       2/1/2012    8\nC      1/1/2012    17\n       2/1/2012    9\n\n\nThe second dataframe is the following:\n\n                   V2\n\nA      1/1/2012    15\n       3/1/2012    21             \nB      1/1/2012    24\n       2/1/2012    9\nD      1/1/2012    7\n       2/1/2012    16\n\n\nand as result I would like to get the following:\n\n                   V1   V2\n\nA      1/1/2012    12   15\n       2/1/2012    14   N/A\n       3/1/2012    N/A  21           \nB      1/1/2012    15   24\n       2/1/2012    8    9\nC      1/1/2012    7    N/A\n       2/1/2012    16   N/A\nD      1/1/2012    N/A  7\n       2/1/2012    N/A  16\n\n\nI have tried a few versions using the pd.merge and .join methods, but nothing seems to work. Do you have any suggestions?\n'
'I have a data set that looks like this (at most 5 columns - but can be less)\n\n1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n....\n\n\nI am trying to use pandas read_table to read this into a 5 column data frame. I would like to read this in without additional massaging. \n\nIf I try\n\nimport pandas as pd\nmy_cols=[\'A\',\'B\',\'C\',\'D\',\'E\']\nmy_df=pd.read_table(path,sep=\',\',header=None,names=my_cols)\n\n\nI get an error - "column names have 5 fields, data has 3 fields". \n\nIs there any way to make pandas fill in NaN for the missing columns while reading the data?\n'
'I\'ve got a data frame df1 with multiple columns and rows. Simple example:\n\n    TIME T1  T2 \n       1 10 100\n       2 20 200\n       3 30 300\n\n\nI\'d like to create an empty data frame df2 and later on, add new columns with the calculation results.\n\nFor this moment my code looks like this:\n\n\n     df1=pd.read_csv("1.txt",index_col="TIME")\n\n     df2=df1.copy()[[]] #copy df1 and erase all columns\n\n\n...adding two new columns:\n\n\n     df2["results1"],df2["results2"]=df1["T1"]*df["T2"]*3,df1["T2"]+100\n\n\nIs there any better/safer/faster way to do this ? \nIs it possible to create an empty data frame df2 and only copy index from df1 ?\n'
"If the data look like:\n\nStore,Dept,Date,Weekly_Sales,IsHoliday\n1,1,2010-02-05,24924.5,FALSE\n1,1,2010-02-12,46039.49,TRUE\n1,1,2010-02-19,41595.55,FALSE\n1,1,2010-02-26,19403.54,FALSE\n1,1,2010-03-05,21827.9,FALSE\n1,1,2010-03-12,21043.39,FALSE\n1,1,2010-03-19,22136.64,FALSE\n1,1,2010-03-26,26229.21,FALSE\n1,1,2010-04-02,57258.43,FALSE\n\n\nAnd I wanna duplicate rows with IsHoliday equal to TRUE, I can do:\n\nis_hol = df['IsHoliday'] == True\ndf_try = df[is_hol]\ndf=df.append(df_try*10)\n\n\nBut is there a better way to do this as I need to duplicate holiday rows by 5 times, and I have to append 5 times if using above way.\n"
'Suppose I have a column like so:\n\na   b  \n1   5   \n1   7\n2   3\n1   3\n2   5\n\n\nI want to sum up the values for b where a = 1, for example. This would give me 5 + 7 + 3 = 15.\n\nHow do I do this in pandas?\n'
'I would like to add a column to the second level of a multiindex column dataframe.\n\nIn [151]: df\nOut[151]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813 \nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596 \n\n\nThe usual trick of direct assignment does not work:\n\nIn [152]: df[\'bar\'][\'three\'] = [0, 1, 2]\n\nIn [153]: df\nOut[153]: \nfirst        bar                 baz           \nsecond       one       two       one       two \nA       0.487880 -0.487661 -1.030176  0.100813\nB       0.267913  1.918923  0.132791  0.178503\nC       1.550526 -0.312235 -1.177689 -0.081596\n\n\nHow can I add the third row to under "bar"?\n'
"I'm using Pandas to compare the outputs of two files loaded into two data frames (uat, prod):\n...\n\nuat = uat[['Customer Number','Product']]\nprod = prod[['Customer Number','Product']]\nprint uat['Customer Number'] == prod['Customer Number']\nprint uat['Product'] == prod['Product']\nprint uat == prod\n\nThe first two match exactly:\n74357    True\n74356    True\nName: Customer Number, dtype: bool\n74357    True\n74356    True\nName: Product, dtype: bool\n\n\nFor the third print, I get an error:\nCan only compare identically-labeled DataFrame objects. If the first two compared fine, what's wrong with the 3rd?\n\nThanks\n"
'Are there single functions in pandas to perform the equivalents of SUMIF, which sums over a specific condition and COUNTIF, which counts values of specific conditions from Excel?\n\nI know that there are many multiple step functions that can be used for\n\nfor example for sumif I can use (df.map(lambda x: condition), or df.size()) then use .sum()\n\nand for countif I can use (groupby functions and look for my answer or use a filter and the .count()) \n\nIs there simple one step process to do these functions where you enter the condition and the data frame and you get the sum or counted results?\n'
'Pandas is really great, but I am really surprised by how inefficient it is to retrieve values from a Pandas.DataFrame.  In the following toy example, even the DataFrame.iloc method is more than 100 times slower than a dictionary.  \n\nThe question: Is the lesson here just that dictionaries are the better way to look up values?  Yes, I get that that is precisely what they were made for.  But I just wonder if there is something I am missing about DataFrame lookup performance.\n\nI realize this question is more "musing" than "asking" but I will accept an answer that provides insight or perspective on this.  Thanks.\n\nimport timeit\n\nsetup = \'\'\'\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 10]))\ndictionary = df.to_dict()\n\'\'\'\n\nf = [\'value = dictionary[5][5]\', \'value = df.loc[5, 5]\', \'value = df.iloc[5, 5]\']\n\nfor func in f:\n    print func\n    print min(timeit.Timer(func, setup).repeat(3, 100000))\n\n\n\n  value = dictionary[5][5]\n  \n  0.130625009537\n  \n  value = df.loc[5, 5]\n  \n  19.4681699276\n  \n  value = df.iloc[5, 5]\n  \n  17.2575249672\n\n'
"I have created a TimeSeries in pandas:\n\nIn [346]: from datetime import datetime\n\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\n\n .....: datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\n\nIn [348]: ts = Series(np.random.randn(6), index=dates)\n\nIn [349]: ts\n\nOut[349]: \n\n2011-01-02 0.690002\n\n2011-01-05 1.001543\n\n2011-01-07 -0.503087\n\n2011-01-08 -0.622274\n\n2011-01-10 -0.921169\n\n2011-01-12 -0.726213\n\n\nI'm following on the example from 'Python for Data Analysis' book. \n\nIn the following paragraph, the author checks the index type:\n\nIn [353]: ts.index.dtype\n\nOut[353]: dtype('datetime64[ns]')\n\n\nWhen I do exactly the same operation in the console I get:\n\nts.index.dtype\ndtype('&lt;M8[ns]')\n\n\nWhat is the difference between two types 'datetime64[ns]' and '&lt;M8[ns]' ?\n\nAnd why do I get a different type?\n"
"I am importing an excel file into a pandas dataframe with the pandas.read_excel() function.\n\nOne of the columns is the primary key of the table: it's all numbers, but it's stored as text (the little green triangle in the top left of the Excel cells confirms this). \n\nHowever, when I import the file into a pandas dataframe, the column gets imported as a float. This means that, for example, '0614' becomes 614.\n\nIs there a way to specify the datatype when importing a column? I understand this is possible when importing CSV files but couldn't find anything in the syntax of read_excel(). \n\nThe only solution I can think of is to add an arbitrary letter at the beginning of the text (converting '0614' into 'A0614') in Excel, to make sure the column is imported as text, and then chopping off the 'A' in python, so I can match it to other tables I am importing from SQL.\n"
"How do I multiply each element of a given column of my dataframe with a scalar?\n(I have tried looking on SO, but cannot seem to find the right solution)\n\nDoing something like: \n\ndf['quantity'] *= -1 # trying to multiply each row's quantity column with -1\n\n\ngives me a warning: \n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\n\nNote: If possible, I do not want to be iterating over the dataframe and do something like this...as I think any standard math operation on an entire column should be possible w/o having to write a loop:\n\nfor idx, row in df.iterrows():\n    df.loc[idx, 'quantity'] *= -1\n\n\nEDIT: \n\nI am running 0.16.2 of Pandas\n\nfull trace:\n\n SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self.obj[item] = s\n\n"
'I have read multiple posts regarding this error, but I still can\'t figure it out. When I try to loop through my function:\n\ndef fix_Plan(location):\n    letters_only = re.sub("[^a-zA-Z]",  # Search for all non-letters\n                          " ",          # Replace all non-letters with spaces\n                          location)     # Column and row to search    \n\n    words = letters_only.lower().split()     \n    stops = set(stopwords.words("english"))      \n    meaningful_words = [w for w in words if not w in stops]      \n    return (" ".join(meaningful_words))    \n\ncol_Plan = fix_Plan(train["Plan"][0])    \nnum_responses = train["Plan"].size    \nclean_Plan_responses = []\n\nfor i in range(0,num_responses):\n    clean_Plan_responses.append(fix_Plan(train["Plan"][i]))\n\n\nHere is the error:\n\nTraceback (most recent call last):\n  File "C:/Users/xxxxx/PycharmProjects/tronc/tronc2.py", line 48, in &lt;module&gt;\n    clean_Plan_responses.append(fix_Plan(train["Plan"][i]))\n  File "C:/Users/xxxxx/PycharmProjects/tronc/tronc2.py", line 22, in fix_Plan\n    location)  # Column and row to search\n  File "C:\\Users\\xxxxx\\AppData\\Local\\Programs\\Python\\Python36\\lib\\re.py", line 191, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or bytes-like object\n\n'
'I know pandas supports a secondary Y axis, but Im curious if anyone knows a way to put a tertiary Y axis on plots... currently I am achieving this with numpy+pyplot ... but it is slow with large data sets.\n\nthis is to plot different measurements with distinct units on the same graph for easy comparison (eg Relative Humidity/Temperature/ and Electrical Conductivity)\n\nso really just curious if anyone knows if this is possible in pandas without too much work.\n\n[Edit] I doubt that there is a way to do this(without too much overhead) however I hope to be proven wrong , this may be a limitation of matplotlib...\n'
"I have a problem making histograms from pandas series objects and I can't understand why it does not work. The code has worked fine before but now it does not.\n\nHere is a bit of my code (specifically, a pandas series object I'm trying to make a histogram of):\n\ntype(dfj2_MARKET1['VSPD2_perc'])\n\n\nwhich outputs the result: \n    pandas.core.series.Series\n\nHere's my plotting code:\n\nfig, axes = plt.subplots(1, 7, figsize=(30,4))\naxes[0].hist(dfj2_MARKET1['VSPD1_perc'],alpha=0.9, color='blue')\naxes[0].grid(True)\naxes[0].set_title(MARKET1 + '  5-40 km / h')\n\n\nError message:\n\n    AttributeError                            Traceback (most recent call last)\n    &lt;ipython-input-75-3810c361db30&gt; in &lt;module&gt;()\n      1 fig, axes = plt.subplots(1, 7, figsize=(30,4))\n      2 \n    ----&gt; 3 axes[1].hist(dfj2_MARKET1['VSPD2_perc'],alpha=0.9, color='blue')\n      4 axes[1].grid(True)\n      5 axes[1].set_xlabel('Time spent [%]')\n\n    C:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in hist(self, x, bins, range, normed,          weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label,    stacked, **kwargs)\n   8322             # this will automatically overwrite bins,\n   8323             # so that each histogram uses the same bins\n-&gt; 8324             m, bins = np.histogram(x[i], bins, weights=w[i], **hist_kwargs)\n   8325             m = m.astype(float) # causes problems later if it's an int\n   8326             if mlast is None:\n\n    C:\\Python27\\lib\\site-packages\\numpy\\lib\\function_base.pyc in histogram(a, bins, range,     normed, weights, density)\n    158         if (mn &gt; mx):\n    159             raise AttributeError(\n--&gt; 160                 'max must be larger than min in range parameter.')\n    161 \n    162     if not iterable(bins):\n\nAttributeError: max must be larger than min in range parameter.\n\n"
'given a dataframe that logs uses of some books like this:\n\nName   Type   ID\nBook1  ebook  1\nBook2  paper  2\nBook3  paper  3\nBook1  ebook  1\nBook2  paper  2\n\n\nI need to get the count of all the books, keeping the other columns and get this:\n\nName   Type   ID    Count\nBook1  ebook  1     2\nBook2  paper  2     2\nBook3  paper  3     1\n\n\nHow can this be done?\n\nThanks!\n'
'I have the following for loop:\n\nfor i in links:\n     data = urllib2.urlopen(str(i)).read()\n     data = json.loads(data)\n     data = pd.DataFrame(data.items())\n     data = data.transpose()\n     data.columns = data.iloc[0]\n     data = data.drop(data.index[[0]])\n\n\nEach dataframe so created has most columns in common with the others but not all of them. Moreover, they all have just one row. What I need to to is to add to the dataframe all the distinct columns and each row from each dataframe produced by the for loop\n\nI tried pandas concatenate or similar but nothing seemed to work. Any idea? Thanks.\n'
"How can I extract the first and last rows of a given dataframe as a new dataframe in pandas?\n\nI've tried to use iloc to select the desired rows and then concat as in:\n\ndf=pd.DataFrame({'a':range(1,5), 'b':['a','b','c','d']})\npd.concat([df.iloc[0,:], df.iloc[-1,:]])\n\n\nbut this does not produce a pandas dataframe:\n\na    1\nb    a\na    4\nb    d\ndtype: object\n\n"
"What's the most efficient way to drop only consecutive duplicates in pandas?\n\ndrop_duplicates gives this:\n\nIn [3]: a = pandas.Series([1,2,2,3,2], index=[1,2,3,4,5])\n\nIn [4]: a.drop_duplicates()\nOut[4]: \n1    1\n2    2\n4    3\ndtype: int64\n\n\nBut I want this:\n\nIn [4]: a.something()\nOut[4]: \n1    1\n2    2\n4    3\n5    2\ndtype: int64\n\n"
'Say I import the following Excel spreadsheet into a dataframe:\n\nVal1 Val2 Val3\n1     2    3 \n5     6    7 \n9     1    2\n\n\nHow do I delete the column name row (in this case Val1, Val2, Val3) so that I can export a csv with no column names, just the data?\n\nI have tried df.drop() and df.ix[1:] and have not been successful with either.\n'
'I like to filter out data whose string length is not equal to 10.\n\nIf I try to filter out any row whose column A\'s or B\'s string length is not equal to 10, I tried this.\n\ndf=pd.read_csv(\'filex.csv\')\ndf.A=df.A.apply(lambda x: x if len(x)== 10 else np.nan)\ndf.B=df.B.apply(lambda x: x if len(x)== 10 else np.nan)\ndf=df.dropna(subset=[\'A\',\'B\'], how=\'any\')\n\n\nThis works slow, but is working.\n\nHowever, it sometimes produce error when the data in A is not a string but a number (interpreted as a number when read_csv read the input file).\n\n  File "&lt;stdin&gt;", line 1, in &lt;lambda&gt;\nTypeError: object of type \'float\' has no len()\n\n\nI believe there should be more efficient and elegant code instead of this.\n\n\n\nBased on the answers and comments below, the simplest solution I found are:\n\ndf=df[df.A.apply(lambda x: len(str(x))==10]\ndf=df[df.B.apply(lambda x: len(str(x))==10]\n\n\nor\n\ndf=df[(df.A.apply(lambda x: len(str(x))==10) &amp; (df.B.apply(lambda x: len(str(x))==10)]\n\n\nor\n\ndf=df[(df.A.astype(str).str.len()==10) &amp; (df.B.astype(str).str.len()==10)]\n\n'
"I have a dataframe, something like:\n\n     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n\n\nand I would like to add a 'total' row to the end of the dataframe:\n\n     foo  bar  qux\n0    a    1    3.14\n1    b    3    2.72\n2    c    2    1.62\n3    d    9    1.41\n4    e    3    0.58\n5    tot  15   9.47\n\n\nI've tried to use the sum command but I end up with a Series, which although I can convert back to a Dataframe, doesn't maintain the data types:\n\ntot_row = pd.DataFrame(df.sum()).T\ntot_row['foo'] = 'tot'\ntot_row.dtypes:\n     foo    object\n     bar    object\n     qux    object\n\n\nI would like to maintain the data types from the original data frame as I need to apply other operations to the total row, something like:\n\nbaz = 2*tot_row['qux'] + 3*tot_row['bar']\n\n"
"Here is a simple example of the code I am running, and I would like the results put into a pandas dataframe (unless there is a better option):\n\nfor p in game.players.passing():\n    print p, p.team, p.passing_att, p.passer_rating()\n\nR.Wilson SEA 29 55.7\nJ.Ryan SEA 1 158.3\nA.Rodgers GB 34 55.8\n\n\nUsing this code:\n\nd = []\nfor p in game.players.passing():\n    d = [{'Player': p, 'Team': p.team, 'Passer Rating':\n        p.passer_rating()}]\n\npd.DataFrame(d)\n\n\nI can get:\n\n    Passer Rating   Player      Team\n  0 55.8            A.Rodgers   GB\n\n\nWhich is a 1x3 dataframe, and I understand why it is only one row but I can't figure out how to make it multi-row with the columns in the correct order. Ideally the solution would be able to deal with n number of rows (based on p) and it would be wonderful (although not essential) if the number of columns would be set by the number of stats requested. Any suggestions? Thanks in advance!\n"
'I am importing data from a MySQL database into a Pandas data frame. The following excerpt is the code that I am using:\n\nimport mysql.connector as sql\nimport pandas as pd\n\ndb_connection = sql.connect(host=\'hostname\', database=\'db_name\', user=\'username\', password=\'password\')\ndb_cursor = db_connection.cursor()\ndb_cursor.execute(\'SELECT * FROM table_name\')\n\ntable_rows = db_cursor.fetchall()\n\ndf = pd.DataFrame(table_rows)\n\n\nWhen I print the data frame it does properly represent the data but my question is, is it possible to also keep the column names? Here is an example output:\n\n                          0   1   2     3     4     5     6     7     8\n0  :ID[giA0CqQcx+(9kbuSKV== NaN NaN  None  None  None  None  None  None\n1  lXB+jIS)DN!CXmj&gt;0(P8^]== NaN NaN  None  None  None  None  None  None   \n2  lXB+jIS)DN!CXmj&gt;0(P8^]== NaN NaN  None  None  None  None  None  None   \n3  lXB+jIS)DN!CXmj&gt;0(P8^]== NaN NaN  None  None  None  None  None  None   \n4  lXB+jIS)DN!CXmj&gt;0(P8^]== NaN NaN  None  None  None  None  None  None   \n\n\nWhat I would like to do is keep the column name, which would replace the pandas column indexes. For example, instead of having 0, the column name would be: "First_column" as in the MySQL table. Is there a good way to go about this? or is there a more efficient approach of importing data from MySQL into a Pandas data frame than mine?\n'
"I have the following pd.DataFrame:\n\nName    0                       1                      ...\nCol     A           B           A            B         ...\n0       0.409511    -0.537108   -0.355529    0.212134  ...\n1       -0.332276   -1.087013    0.083684    0.529002  ...\n2       1.138159    -0.327212    0.570834    2.337718  ...\n\n\nIt has MultiIndex columns with names=['Name', 'Col'] and hierarchical levels. The Name label goes from 0 to n, and for each label, there are two A and B columns. \n\nI would like to subselect all the A (or B) columns of this DataFrame. \n"
"I am doing some geocoding work that I used selenium to screen scrape the x-y coordinate I need for address of a location, I imported an xls file to panda dataframe and want to use explicit loop to update the rows which do not have the x-y coordinate, like below:\n\nfor index, row in rche_df.iterrows():\n    if isinstance(row.wgs1984_latitude, float):\n        row = row.copy()\n        target = row.address_chi        \n        dict_temp = geocoding(target)\n        row.wgs1984_latitude = dict_temp['lat']\n        row.wgs1984_longitude = dict_temp['long']\n\n\nI have read Why doesn&#39;t this function &quot;take&quot; after I iterrows over a pandas DataFrame? and am fully aware that iterrow only gives us a view rather than a copy for editing, but what if I really to update the value row by row? Is lambda feasible?\n"
"I am trying to drop multiple columns (column 2 and 70 in my data set, indexed as 1 and 69 respectively) by index number in a pandas data frame with the following code:\n\ndf.drop([df.columns[[1, 69]]], axis=1, inplace=True)\n\n\nI get the following error: \n\nTypeError: unhashable type: 'Index'\n\n\nAnd in my code the [1, 69] is highlighted and says:\n\nExpected type 'Integral', got 'list[int]' instead\n\n\nThe following code does what I want it to do successfully, but on two lines of repetitive code (first dropping col index 69, then 1, and order does matter because dropping earlier columns changes the index of later columns). I thought I could specify more than one column index simply as a list, but perhaps I have something wrong above?\n\ndf.drop([df.columns[69]], axis=1, inplace=True)\ndf.drop([df.columns[1]], axis=1, inplace=True)\n\n\nIs there a way that I can do this on one line similar to the first code snippet above?\n"
"I have a pandas data frame with 50k rows.  I'm trying to add a new column that is a randomly generated integer from 1 to 5.  \n\nIf I want 50k random numbers I'd use:\n\ndf1['randNumCol'] = random.sample(xrange(50000), len(df1))\n\n\nbut for this I'm not sure how to do it.\n\nSide note in R, I'd do:\n\nsample(1:5, 50000, replace = TRUE)\n\n\nAny suggestions?\n"
"I'm working with the following df:\n\nc.sort_values('2005', ascending=False).head(3)\n      GeoName ComponentName     IndustryId IndustryClassification Description                                2004 2005  2006  2007  2008  2009 2010 2011 2012 2013 2014\n37926 Alabama Real GDP by state 9          213                    Support activities for mining              99   98    117   117   115   87   96   95   103  102  (NA)\n37951 Alabama Real GDP by state 34         42                     Wholesale trade                            9898 10613 10952 11034 11075 9722 9765 9703 9600 9884 10199\n37932 Alabama Real GDP by state 15         327                    Nonmetallic mineral products manufacturing 980  968   940   1084  861   724  714  701  589  641  (NA)\n\n\nI want to force numeric on all of the years:\n\nc['2014'] = pd.to_numeric(c['2014'], errors='coerce')\n\n\nis there an easy way to do this or do I have to type them all out?\n"
"Why do we use 'loc' for pandas dataframes? it seems the following code with or without using loc both compile anr run at a simulular speed\n\n%timeit df_user1 = df.loc[df.user_id=='5561']\n\n100 loops, best of 3: 11.9 ms per loop\n\n\nor\n\n%timeit df_user1_noloc = df[df.user_id=='5561']\n\n100 loops, best of 3: 12 ms per loop\n\n\nSo why use loc?\n\nEdit: This has been flagged as a duplicate question. But although pandas iloc vs ix vs loc explanation? does mention that *\n\n\n  you can do column retrieval just by using the data frame's\n  getitem:\n\n\n*\n\ndf['time']    # equivalent to df.loc[:, 'time']\n\n\nit does not say why we use loc, although it does explain lots of features of loc, my specific question is 'why not just omit loc altogether'? for which i have accepted a very detailed answer below.\n\nAlso that other post the answer (which i do not think is an answer) is very hidden in the discussion and any person searching for what i was looking for would find it hard to locate the information and would be much better served by the answer provided to my question.\n"
'I\'m using groupby on a pandas dataframe to drop all rows that don\'t have the minimum of a specific column. Something like this: \n\ndf1 = df.groupby("item", as_index=False)["diff"].min()\n\n\nHowever, if I have more than those two columns, the other columns (e.g. otherstuff in my example) get dropped. Can I keep those columns using groupby, or am I going to have to find a different way to drop the rows?\n\nMy data looks like: \n\n    item    diff   otherstuff\n   0   1       2            1\n   1   1       1            2\n   2   1       3            7\n   3   2      -1            0\n   4   2       1            3\n   5   2       4            9\n   6   2      -6            2\n   7   3       0            0\n   8   3       2            9\n\n\nand should end up like:\n\n    item   diff  otherstuff\n   0   1      1           2\n   1   2     -6           2\n   2   3      0           0\n\n\nbut what I\'m getting is:\n\n    item   diff\n   0   1      1           \n   1   2     -6           \n   2   3      0                 \n\n\nI\'ve been looking through the documentation and can\'t find anything. I tried:\n\ndf1 = df.groupby(["item", "otherstuff"], as_index=false)["diff"].min()\n\ndf1 = df.groupby("item", as_index=false)["diff"].min()["otherstuff"]\n\ndf1 = df.groupby("item", as_index=false)["otherstuff", "diff"].min()\n\n\nBut none of those work (I realized with the last one that the syntax is meant for aggregating after a group is created).\n'
"I've got pandas data with some columns of text type. There are some NaN values along with these text columns. What I'm trying to do is to impute those NaN's by sklearn.preprocessing.Imputer (replacing NaN by the most frequent value). The problem is in implementation.\nSuppose there is a Pandas dataframe df with 30 columns, 10 of which are of categorical nature.\nOnce I run:\n\nfrom sklearn.preprocessing import Imputer\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\nimp.fit(df) \n\n\nPython generates an error: 'could not convert string to float: 'run1'', where 'run1' is an ordinary (non-missing) value from the first column with categorical data.\n\nAny help would be very welcome\n"
"I have a list of 4 pandas dataframes containing a day of tick data that I want to merge into a single data frame. I cannot understand the behavior of concat on my timestamps. See details below:\n\ndata\n\n[&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 35228 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-03-28 18:59:20.357000+02:00\nData columns:\nPrice       4040  non-null values\nVolume      4040  non-null values\nBidQty      35228  non-null values\nBidPrice    35228  non-null values\nAskPrice    35228  non-null values\nAskQty      35228  non-null values\ndtypes: float64(6),\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nDatetimeIndex: 33088 entries, 2013-04-01 00:03:17.047000+02:00 to 2013-04-01 18:59:58.175000+02:00\nData columns:\nPrice       3969  non-null values\nVolume      3969  non-null values\nBidQty      33088  non-null values\nBidPrice    33088  non-null values\nAskPrice    33088  non-null values\nAskQty      33088  non-null values\ndtypes: float64(6),\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nDatetimeIndex: 50740 entries, 2013-04-02 00:03:27.470000+02:00 to 2013-04-02 18:59:58.172000+02:00\nData columns:\nPrice       7326  non-null values\nVolume      7326  non-null values\nBidQty      50740  non-null values\nBidPrice    50740  non-null values\nAskPrice    50740  non-null values\nAskQty      50740  non-null values\ndtypes: float64(6),\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\nDatetimeIndex: 60799 entries, 2013-04-03 00:03:06.994000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nPrice       8258  non-null values\nVolume      8258  non-null values\nBidQty      60799  non-null values\nBidPrice    60799  non-null values\nAskPrice    60799  non-null values\nAskQty      60799  non-null values\ndtypes: float64(6)]\n\n\nUsing append I get:\n\npd.DataFrame().append(data)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 179855 entries, 2013-03-28 00:00:07.089000+02:00 to 2013-04-03 18:59:58.180000+02:00\nData columns:\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\nBidPrice    179855  non-null values\nBidQty      179855  non-null values\nPrice       23593  non-null values\nVolume      23593  non-null values\ndtypes: float64(6)\n\n\nUsing concat I get:\n\npd.concat(data)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 179855 entries, 2013-03-27 22:00:07.089000+02:00 to 2013-04-03 16:59:58.180000+02:00\nData columns:\nPrice       23593  non-null values\nVolume      23593  non-null values\nBidQty      179855  non-null values\nBidPrice    179855  non-null values\nAskPrice    179855  non-null values\nAskQty      179855  non-null values\ndtypes: float64(6)\n\n\nNotice how the index changes when using concat. Why is that happening and how would I go about using concat to reproduce the results obtained using append? (Since concat seems so much faster; 24.6 ms per loop vs 3.02 s per loop)\n"
"I need the index to start at 1 rather than 0 when writing a Pandas DataFrame to CSV.\n\nHere's an example:\n\nIn [1]: import pandas as pd\n\nIn [2]: result = pd.DataFrame({'Count': [83, 19, 20]})\n\nIn [3]: result.to_csv('result.csv', index_label='Event_id')                               \n\n\nWhich produces the following output:\n\nIn [4]: !cat result.csv\nEvent_id,Count\n0,83\n1,19\n2,20\n\n\nBut my desired output is this:\n\nIn [5]: !cat result2.csv\nEvent_id,Count\n1,83\n2,19\n3,20\n\n\nI realize that this could be done by adding a sequence of integers shifted by 1 as a column to my data frame, but I'm new to Pandas and I'm wondering if a cleaner way exists.\n"
"I have a csv file from this webpage.\nI want to read some of the columns in the downloaded file (the csv version can be downloaded in the upper right corner).\n\nLet's say I want 2 columns:\n\n\n59 which in the header is star_name\n60 which in the header is ra.\n\n\nHowever, for some reason the authors of the webpage sometimes decide to move the columns around.\n\nIn the end I want something like this, keeping in mind that values can be missing.\n\ndata = #read data in a clever way\nnames = data['star_name']\nras = data['ra']\n\n\nThis will prevent my program to malfunction when the columns are changed again in the future, if they keep the name correct.\n\nUntil now I have tried various ways using the csv module and resently the pandas module. Both without any luck.\n\nEDIT (added two lines + the header of my datafile. Sorry, but it's extremely long.)\n\n# name, mass, mass_error_min, mass_error_max, radius, radius_error_min, radius_error_max, orbital_period, orbital_period_err_min, orbital_period_err_max, semi_major_axis, semi_major_axis_error_min, semi_major_axis_error_max, eccentricity, eccentricity_error_min, eccentricity_error_max, angular_distance, inclination, inclination_error_min, inclination_error_max, tzero_tr, tzero_tr_error_min, tzero_tr_error_max, tzero_tr_sec, tzero_tr_sec_error_min, tzero_tr_sec_error_max, lambda_angle, lambda_angle_error_min, lambda_angle_error_max, impact_parameter, impact_parameter_error_min, impact_parameter_error_max, tzero_vr, tzero_vr_error_min, tzero_vr_error_max, K, K_error_min, K_error_max, temp_calculated, temp_measured, hot_point_lon, albedo, albedo_error_min, albedo_error_max, log_g, publication_status, discovered, updated, omega, omega_error_min, omega_error_max, tperi, tperi_error_min, tperi_error_max, detection_type, mass_detection_type, radius_detection_type, alternate_names, molecules, star_name, ra, dec, mag_v, mag_i, mag_j, mag_h, mag_k, star_distance, star_metallicity, star_mass, star_radius, star_sp_type, star_age, star_teff, star_detected_disc, star_magnetic_field\n11 Com b,19.4,1.5,1.5,,,,326.03,0.32,0.32,1.29,0.05,0.05,0.231,0.005,0.005,0.011664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2008,2011-12-23,94.8,1.5,1.5,2452899.6,1.6,1.6,Radial Velocity,,,,,11 Com,185.1791667,17.7927778,4.74,,,,,110.6,-0.35,2.7,19.0,G8 III,,4742.0,,\n11 UMi b,10.5,2.47,2.47,,,,516.22,3.25,3.25,1.54,0.07,0.07,0.08,0.03,0.03,0.012887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1,2009,2009-08-13,117.63,21.06,21.06,2452861.05,2.06,2.06,Radial Velocity,,,,,11 UMi,229.275,71.8238889,5.02,,,,,119.5,0.04,1.8,24.08,K4III,1.56,4340.0,,\n\n"
'Let\'s assume that I have an XML like this:\n\n&lt;author type="XXX" language="EN" gender="xx" feature="xx" web="foobar.com"&gt;\n    &lt;documents count="N"&gt;\n        &lt;document KEY="e95a9a6c790ecb95e46cf15bee517651" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="bc360cfbafc39970587547215162f0db" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="19e71144c50a8b9160b3f0955e906fce" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="21d4af9021a174f61b884606c74d9e42" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="28a45eb2460899763d709ca00ddbb665" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="a0c0712a6a351f85d9f5757e9fff8946" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="626726ba8d34d15d02b6d043c55fe691" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="2cb473e0f102e2e4a40aa3006e412ae4" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...] [...]\n]]&gt;\n        &lt;/document&gt;\n    &lt;/documents&gt;\n&lt;/author&gt;\n\n\nI would like to read this XML file and convert it to a pandas DataFrame:\n\nkey                                         type     language    feature            web                         data\ne95324a9a6c790ecb95e46cf15bE232ee517651      XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\ne95324a9a6c790ecb95e46cf15bE232ee517651      XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n19e71144c50a8b9160b3cvdf2324f0955e906fce     XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n21d4af9021a174f61b8erf284606c74d9e42         XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n28a45eb2460823499763d70vdf9ca00ddbb665       XXX        EN          xx      www.foo_bar_exmaple.com     A large text with lots of strings and punctuations symbols [...]\n\n\nThis is what I already tried, but I am getting some errors and probably there is a more efficient way of doing this task:\n\nfrom lxml import objectify\nimport pandas as pd\n\npath = \'file_path\'\nxml = objectify.parse(open(path))\nroot = xml.getroot()\nroot.getchildren()[0].getchildren()\ndf = pd.DataFrame(columns=(\'key\',\'type\', \'language\', \'feature\', \'web\', \'data\'))\n\nfor i in range(0,len(xml)):\n    obj = root.getchildren()[i].getchildren()\n    row = dict(zip([\'key\',\'type\', \'language\', \'feature\', \'web\', \'data\'], [obj[0].text, obj[1].text]))\n    row_s = pd.Series(row)\n    row_s.name = i\n    df = df.append(row_s)\n\n\nCould anybody provide me a better aproach for this problem?\n'
"Is there any way to specify the index that I want for a new row, when appending the row to a dataframe?\n\nThe original documentation provides the following example:\n\nIn [1301]: df = DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\nIn [1302]: df\nOut[1302]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n\nIn [1303]: s = df.xs(3)\n\nIn [1304]: df.append(s, ignore_index=True)\nOut[1304]: \n          A         B         C         D\n0 -1.137707 -0.891060 -0.693921  1.613616\n1  0.464000  0.227371 -0.496922  0.306389\n2 -2.290613 -1.134623 -1.561819 -0.260838\n3  0.281957  1.523962 -0.902937  0.068159\n4 -0.057873 -0.368204 -1.144073  0.861209\n5  0.800193  0.782098 -1.069094 -1.099248\n6  0.255269  0.009750  0.661084  0.379319\n7 -0.008434  1.952541 -1.056652  0.533946\n8  0.281957  1.523962 -0.902937  0.068159\n\n\nwhere the new row gets the index label automatically. Is there any way to control the new label?\n"
"I have the following:\n\n&gt; date1\nTimestamp('2014-01-23 00:00:00', tz=None)\n\n&gt; date2\ndatetime.date(2014, 3, 26)\n\n\nand I read on this answer that I could use pandas.to_datetime() to convert from Timestamps to datetime objects, but it doesn't seem to work:\n\n&gt; pd.to_datetime(date1)   \nTimestamp('2014-01-23 00:00:00', tz=None)\n\n\nWhy? How can I convert between these two formats?\n"
'Lets say I have a dataframe like this\n\n    A   B\n0   a   b\n1   c   d\n2   e   f \n3   g   h\n\n\n0,1,2,3 are times, a, c, e, g is one time series and b, d, f, h is another time series.\nI need to be able to add two columns to the orignal dataframe which is got by computing the differences of consecutive rows for certain columns. \n\nSo i need something like this\n\n    A   B   dA\n0   a   b  (a-c)\n1   c   d  (c-e)\n2   e   f  (e-g)\n3   g   h   Nan\n\n\nI saw something called diff on the dataframe/series but that does it slightly differently as in first element will become Nan.\n'
'I am trying to write a paper in IPython notebook, but encountered some issues with display format. Say I have following dataframe df, is there any way to format var1 and var2 into 2 digit decimals and var3 into percentages.\n\n       var1        var2         var3    \nid                                              \n0    1.458315    1.500092   -0.005709   \n1    1.576704    1.608445   -0.005122    \n2    1.629253    1.652577   -0.004754    \n3    1.669331    1.685456   -0.003525   \n4    1.705139    1.712096   -0.003134   \n5    1.740447    1.741961   -0.001223   \n6    1.775980    1.770801   -0.001723    \n7    1.812037    1.799327   -0.002013    \n8    1.853130    1.822982   -0.001396    \n9    1.943985    1.868401    0.005732\n\n\nThe numbers inside are not multiplied by 100, e.g.  -0.0057=-0.57%.\n'
"Say you have this MultiIndex-ed DataFrame:\n\ndf = pd.DataFrame({'co':['DE','DE','FR','FR'],\n                   'tp':['Lake','Forest','Lake','Forest'],\n                   'area':[10,20,30,40],\n                   'count':[7,5,2,3]})\ndf = df.set_index(['co','tp'])\n\n\nWhich looks like this:\n\n           area  count\nco tp\nDE Lake      10      7\n   Forest    20      5\nFR Lake      30      2\n   Forest    40      3\n\n\nI would like to retrieve the unique values per index level. This can be accomplished using\n\ndf.index.levels[0]  # returns ['DE', 'FR]\ndf.index.levels[1]  # returns ['Lake', 'Forest']\n\n\nWhat I would really like to do, is to retrieve these lists by addressing the levels by their name, i.e. 'co' and 'tp'. The shortest two ways I could find looks like this:\n\nlist(set(df.index.get_level_values('co')))  # returns ['DE', 'FR']\ndf.index.levels[df.index.names.index('co')]  # returns ['DE', 'FR']\n\n\nBut non of them are very elegant. Is there a shorter way?\n"
"I need to concatenate two dataframes df_a anddf_b having equal number of rows (nRow) one after another without any consideration of keys. This function is similar to cbind in R programming language. The number of columns in each dataframe may be different. \n\nThe resultant dataframe will have the same number of rows nRow and number of columns equal to the sum of number of columns in both the dataframes. In othe words, this is a blind columnar concatenation of two dataframes. \n\nimport pandas as pd\ndict_data = {'Treatment': ['C', 'C', 'C'], 'Biorep': ['A', 'A', 'A'], 'Techrep': [1, 1, 1], 'AAseq': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'mz':[500.0, 500.5, 501.0]}\ndf_a = pd.DataFrame(dict_data)\ndict_data = {'Treatment1': ['C', 'C', 'C'], 'Biorep1': ['A', 'A', 'A'], 'Techrep1': [1, 1, 1], 'AAseq1': ['ELVISLIVES', 'ELVISLIVES', 'ELVISLIVES'], 'inte1':[1100.0, 1050.0, 1010.0]}\ndf_b = pd.DataFrame(dict_data)\n\n"
"I have a dataframe that looks like the following\n   color  x   y\n0    red  0   0\n1    red  1   1\n2    red  2   2\n3    red  3   3\n4    red  4   4\n5    red  5   5\n6    red  6   6\n7    red  7   7\n8    red  8   8\n9    red  9   9\n10  blue  0   0\n11  blue  1   1\n12  blue  2   4\n13  blue  3   9\n14  blue  4  16\n15  blue  5  25\n16  blue  6  36\n17  blue  7  49\n18  blue  8  64\n19  blue  9  81\n\nI ultimately want two lines, one blue, one red.  The red line should essentially be y=x and the blue line should be y=x^2\nWhen I do the following:\ndf.plot(x='x', y='y')\n\nThe output is this:\n\nIs there a way to make pandas know that there are two sets?  And group them accordingly.  I'd like to be able to specify the column color as the set differentiator\n"
"This is the example of my dataset.\n\n&gt;&gt;&gt; user1 = pd.read_csv('dataset/1.csv')\n&gt;&gt;&gt; print(user1)\n          0  0.69464   3.1735   7.5048\n0  0.030639  0.14982  3.48680   9.2755\n1  0.069763 -0.29965  1.94770   9.1120\n2  0.099823 -1.68890  1.41650  10.1200\n3  0.129820 -2.17930  0.95342  10.9240\n4  0.159790 -2.30180  0.23155  10.6510\n5  0.189820 -1.41650  1.18500  11.0730\n\n\nHow to push down the first column and add the names column [TIME, X, Y, and Z] on the first column.\n\nThe desired output is like this:\n\n       TIME        X        Y        Z\n0         0  0.69464   3.1735   7.5048\n1  0.030639  0.14982  3.48680   9.2755\n2  0.069763 -0.29965  1.94770   9.1120\n3  0.099823 -1.68890  1.41650  10.1200\n4  0.129820 -2.17930  0.95342  10.9240\n5  0.159790 -2.30180  0.23155  10.6510\n6  0.189820 -1.41650  1.18500  11.0730\n\n"
"I constructed a pandas dataframe of results. This data frame acts as a table. There are MultiIndexed columns and each row represents a name, ie index=['name1','name2',...] when creating the DataFrame. I would like to display this table and save it as a png (or any graphic format really). At the moment, the closest I can get is converting it to html, but I would like a png. It looks like similar questions have been asked such as How to save the Pandas dataframe/series data as a figure?\n\nHowever, the marked solution converts the dataframe into a line plot (not a table) and the other solution relies on PySide which I would like to stay away simply because I cannot pip install it on linux. I would like this code to be easily portable. I really was expecting table creation to png to be easy with python. All help is appreciated.\n"
"Let's take a simple function that takes a str and returns a dataframe:\n\nimport pandas as pd\ndef csv_to_df(path):\n    return pd.read_csv(path, skiprows=1, sep='\\t', comment='#')\n\n\nWhat is the recommended pythonic way of adding type hints to this function?\n\nIf I ask python for the type of a DataFrame it returns pandas.core.frame.DataFrame.\nThe following won't work though, as it'll tell me that pandas is not defined.\n\n def csv_to_df(path: str) -&gt; pandas.core.frame.DataFrame:\n     return pd.read_csv(path, skiprows=1, sep='\\t', comment='#')\n\n"
'I would like to break down a pandas column consisting of a list of elements into as many columns as there are unique elements i.e. one-hot-encode them (with value 1 representing a given element existing in a row and 0 in the case of absence). \n\nFor example, taking dataframe df\n\nCol1   Col2         Col3\n C      33     [Apple, Orange, Banana]\n A      2.5    [Apple, Grape]\n B      42     [Banana] \n\n\nI would like to convert this to:\n\ndf \n\nCol1   Col2   Apple   Orange   Banana   Grape\n C      33     1        1        1       0\n A      2.5    1        0        0       1\n B      42     0        0        1       0\n\n\nHow can I use pandas/sklearn to achieve this?\n'
"I need to set the value of one column based on the value of another in a Pandas dataframe. This is the logic:\n\nif df['c1'] == 'Value':\n    df['c2'] = 10\nelse:\n    df['c2'] = df['c3']\n\n\nI am unable to get this to do what I want, which is to simply create a column with new values (or change the value of an existing column: either one works for me). \n\nIf I try to run the code above or if I write it as a function and use the apply method, I get the following:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n"
"I'm trying to read a fairly large CSV file with Pandas and split it up into two random chunks, one of which being 10% of the data and the other being 90%.\n\nHere's my current attempt:\n\nrows = data.index\nrow_count = len(rows)\nrandom.shuffle(list(rows))\n\ndata.reindex(rows)\n\ntraining_data = data[row_count // 10:]\ntesting_data = data[:row_count // 10]\n\n\nFor some reason, sklearn throws this error when I try to use one of these resulting DataFrame objects inside of a SVM classifier:\n\nIndexError: each subindex must be either a slice, an integer, Ellipsis, or newaxis\n\n\nI think I'm doing it wrong. Is there a better way to do this?\n"
"I have some values in a Python Pandas Series (type: pandas.core.series.Series)\n\nIn [1]: series = pd.Series([0.0,950.0,-70.0,812.0,0.0,-90.0,0.0,0.0,-90.0,0.0,-64.0,208.0,0.0,-90.0,0.0,-80.0,0.0,0.0,-80.0,-48.0,840.0,-100.0,190.0,130.0,-100.0,-100.0,0.0,-50.0,0.0,-100.0,-100.0,0.0,-90.0,0.0,-90.0,-90.0,63.0,-90.0,0.0,0.0,-90.0,-80.0,0.0,])\n\nIn [2]: series.min()\nOut[2]: -100.0\n\nIn [3]: series.max()\nOut[3]: 950.0\n\n\nI would like to get values of histogram (not necessary plotting histogram)... I just need to get the frequency for each interval.\n\nLet's say that my intervals are going from [-200; -150] to [950; 1000]\n\nso lower bounds are\n\nlwb = range(-200,1000,50)\n\n\nand upper bounds are\n\nupb = range(-150,1050,50)\n\n\nI don't know how to get frequency (the number of values that are inside each interval) now...\nI'm sure that defining lwb and upb is not necessary... but I don't know what\nfunction I should use to perform this!\n(after diving in Pandas doc, I think cut function can help me because it's a discretization problem... but I'm don't understand how to use it)\n\nAfter being able to do this, I will have a look at the way to display histogram (but that's an other problem)\n"
"I'm trying to multiply two existing columns in a pandas Dataframe (orders_df) - Prices (stock close price) and Amount (stock quantities) and add the calculation to a new column called 'Value'. For some reason when I run this code, all the rows under the 'Value' column are positive numbers, while some of the rows should be negative. Under the Action column in the DataFrame there are seven rows with the 'Sell' string and seven with the 'Buy' string.\n\nfor i in orders_df.Action:\n if i  == 'Sell':\n  orders_df['Value'] = orders_df.Prices*orders_df.Amount\n elif i == 'Buy':\n  orders_df['Value'] = -orders_df.Prices*orders_df.Amount)\n\n\nPlease let me know what i'm doing wrong !\n"
"I am using .size() on a groupby result in order to count how many items are in each group.\nI would like the result to be saved to a new column name without manually editing the column names array, how can it be done?\nThis is what I have tried:\ngrpd = df.groupby(['A','B'])\ngrpd['size'] = grpd.size()\ngrpd\n\nand the error I got:\n\nTypeError: 'DataFrameGroupBy' object does not support item assignment\n(on the second line)\n\n"
"After performing a groupby.sum() on a DataFrame I'm having some trouble trying to create my intended plot.\n\n\n\nHow can I create a subplot (kind='bar') for each Code, where the x-axis is the Month and the bars are ColA and ColB?\n"
'I want to query a PostgreSQL database and return the output as a Pandas dataframe.\n\nI created a connection to the database with \'SqlAlchemy\':\n\nfrom sqlalchemy import create_engine\nengine = create_engine(\'postgresql://user@localhost:5432/mydb\')\n\n\nI write a Pandas dataframe to a database table:\n\ni=pd.read_csv(path)\ni.to_sql(\'Stat_Table\',engine,if_exists=\'replace\')\n\n\nBased on the docs, looks like pd.read_sql_query() should accept a SQLAlchemy engine:\n\na=pd.read_sql_query(\'select * from Stat_Table\',con=engine)\n\n\nBut it throws an error:\n\nProgrammingError: (ProgrammingError) relation "stat_table" does not exist\n\n\nI\'m using Pandas version 0.14.1. \n\nWhat\'s the right way to do this?\n'
"The default behavior of pandas groupby is to turn the group by columns into index and remove them from the list of columns of the dataframe. For instance, say I have a dataFrame with these columns \n\ncol1|col2|col3|col4\n\n\nif I apply a groupby say with columns col2 and col3 this way\n\ndf.groupby(['col2','col3']).sum()\n\n\nThe dataframe df no longer has the ['col2','col3'] in the list of columns. They are automatically turned into the indices of the resulting dataframe. \n\nMy question is how can I perform groupby on a column and yet keep that column in the dataframe?  \n"
"I've been exploring how to optimize my code and ran across pandas .at method. Per the documentation\n\nFast label-based scalar accessor\nSimilarly to loc, at provides label based scalar lookups. You can also set using these indexers.\n\nSo I ran some samples:\nSetup\nimport pandas as pd\nimport numpy as np\nfrom string import letters, lowercase, uppercase\n\nlt = list(letters)\nlc = list(lowercase)\nuc = list(uppercase)\n\ndef gdf(rows, cols, seed=None):\n    &quot;&quot;&quot;rows and cols are what you'd pass\n    to pd.MultiIndex.from_product()&quot;&quot;&quot;\n    gmi = pd.MultiIndex.from_product\n    df = pd.DataFrame(index=gmi(rows), columns=gmi(cols))\n    np.random.seed(seed)\n    df.iloc[:, :] = np.random.rand(*df.shape)\n    return df\n\nseed = [3, 1415]\ndf = gdf([lc, uc], [lc, uc], seed)\n\nprint df.head().T.head().T\n\ndf looks like:\n            a                                        \n            A         B         C         D         E\na A  0.444939  0.407554  0.460148  0.465239  0.462691\n  B  0.032746  0.485650  0.503892  0.351520  0.061569\n  C  0.777350  0.047677  0.250667  0.602878  0.570528\n  D  0.927783  0.653868  0.381103  0.959544  0.033253\n  E  0.191985  0.304597  0.195106  0.370921  0.631576\n\nLets use .at and .loc and ensure I get the same thing\nprint &quot;using .loc&quot;, df.loc[('a', 'A'), ('c', 'C')]\nprint &quot;using .at &quot;, df.at[('a', 'A'), ('c', 'C')]\n\nusing .loc 0.37374090276\nusing .at  0.37374090276\n\nTest speed using .loc\n%%timeit\ndf.loc[('a', 'A'), ('c', 'C')]\n\n10000 loops, best of 3: 180 µs per loop\n\nTest speed using .at\n%%timeit\ndf.at[('a', 'A'), ('c', 'C')]\n\nThe slowest run took 6.11 times longer than the fastest. This could mean that an intermediate result is being cached.\n100000 loops, best of 3: 8 µs per loop\n\nThis looks to be a huge speed increase.  Even at the caching stage 6.11 * 8 is a lot faster than 180\nQuestion\nWhat are the limitations of .at?  I'm motivated to use it.  The documentation says it's similar to .loc but it doesn't behave similarly.  Example:\n# small df\nsdf = gdf([lc[:2]], [uc[:2]], seed)\n\nprint sdf.loc[:, :]\n\n          A         B\na  0.444939  0.407554\nb  0.460148  0.465239\n\nwhere as print sdf.at[:, :] results in TypeError: unhashable type\nSo obviously not the same even if the intent is to be similar.\nThat said, who can provide guidance on what can and cannot be done with the .at method?\n"
'I have pandas DataFrame which I have composed from concat. One row consists of 96 values, I would like to split the DataFrame from the value 72.\n\nSo that the first 72 values of a row are stored in Dataframe1, and the next 24 values of a row in Dataframe2.\n\nI create my DF as follows:\n\ntemps = DataFrame(myData)\ndatasX = concat(\n[temps.shift(72), temps.shift(71), temps.shift(70), temps.shift(69), temps.shift(68), temps.shift(67),\n temps.shift(66), temps.shift(65), temps.shift(64), temps.shift(63), temps.shift(62), temps.shift(61),\n temps.shift(60), temps.shift(59), temps.shift(58), temps.shift(57), temps.shift(56), temps.shift(55),\n temps.shift(54), temps.shift(53), temps.shift(52), temps.shift(51), temps.shift(50), temps.shift(49),\n temps.shift(48), temps.shift(47), temps.shift(46), temps.shift(45), temps.shift(44), temps.shift(43),\n temps.shift(42), temps.shift(41), temps.shift(40), temps.shift(39), temps.shift(38), temps.shift(37),\n temps.shift(36), temps.shift(35), temps.shift(34), temps.shift(33), temps.shift(32), temps.shift(31),\n temps.shift(30), temps.shift(29), temps.shift(28), temps.shift(27), temps.shift(26), temps.shift(25),\n temps.shift(24), temps.shift(23), temps.shift(22), temps.shift(21), temps.shift(20), temps.shift(19),\n temps.shift(18), temps.shift(17), temps.shift(16), temps.shift(15), temps.shift(14), temps.shift(13),\n temps.shift(12), temps.shift(11), temps.shift(10), temps.shift(9), temps.shift(8), temps.shift(7),\n temps.shift(6), temps.shift(5), temps.shift(4), temps.shift(3), temps.shift(2), temps.shift(1), temps,\n temps.shift(-1), temps.shift(-2), temps.shift(-3), temps.shift(-4), temps.shift(-5), temps.shift(-6),\n temps.shift(-7), temps.shift(-8), temps.shift(-9), temps.shift(-10), temps.shift(-11), temps.shift(-12),\n temps.shift(-13), temps.shift(-14), temps.shift(-15), temps.shift(-16), temps.shift(-17), temps.shift(-18),\n temps.shift(-19), temps.shift(-20), temps.shift(-21), temps.shift(-22), temps.shift(-23)], axis=1)\n\n\nQuestion is: How can split them? :)\n'
'I am trying to get a new dataset, or change the value of the current dataset columns to their unique values. \nHere is an example of what I am trying to get : \n\n   A B\n -----\n0| 1 1\n1| 2 5\n2| 1 5\n3| 7 9\n4| 7 9\n5| 8 9\n\nWanted Result    Not Wanted Result\n       A B            A B\n     -----          -----\n    0| 1 1         0| 1 1\n    1| 2 5         1| 2 5\n    2| 7 9         2| \n    3| 8           3| 7 9\n                   4|\n                   5| 8\n\n\nI don\'t really care about the index but it seems to be the problem. \nMy code so far is pretty simple, I tried 2 approaches, 1 with a new dataFrame and one without. \n\n#With New DataFrame\n def UniqueResults(dataframe):\n    df = pd.DataFrame()\n    for col in dataframe:\n        S=pd.Series(dataframe[col].unique())\n        df[col]=S.values\n    return df\n\n#Without new DataFrame\ndef UniqueResults(dataframe):\n    for col in dataframe:\n        dataframe[col]=dataframe[col].unique()\n    return dataframe\n\n\nI have the error "Length of Values does not match length of index" both times.\n'
"I'm using the Pandas package and it creates a DataFrame object, which is basically a labeled matrix. Often I have columns that have long string fields, or dataframes with many columns, so the simple print command doesn't work well. I've written some text output functions, but they aren't great.\n\nWhat I'd really love is a simple GUI that lets me interact with a dataframe / matrix / table. Just like you would find in a SQL tool. Basically a window that has a read-only spreadsheet like view into the data. I can expand columns, page up and down through long tables, etc.\n\nI would suspect something like this exists, but I must be Googling with the wrong terms. It would be great if it is pandas specific, but I would guess I could use any matrix-accepting tool. (BTW - I'm on Windows.)\n\nAny pointers?\n\nOr, conversely, if someone knows this space well and knows this probably doesn't exist, any suggestions on if there is a simple GUI framework / widget I could use to roll my own? (But since my needs are limited, I'm reluctant to have to learn a big GUI framework and do a bunch of coding for this one piece.)\n"
'I want to get the count of dataframe rows based on conditional selection. I tried the following code.\n\nprint df[(df.IP == head.idxmax()) &amp; (df.Method == \'HEAD\') &amp; (df.Referrer == \'"-"\')].count()\n\n\noutput:\n\nIP          57\nTime        57\nMethod      57\nResource    57\nStatus      57\nBytes       57\nReferrer    57\nAgent       57\ndtype: int64\n\n\nThe output shows the count for each an every column in the dataframe. Instead I need to get a single count where all of the above conditions satisfied? How to do this? If you need more explanation about my dataframe please let me know.\n'
'Given a sparse matrix listing, what\'s the best way to calculate the cosine similarity between each of the columns (or rows) in the matrix? I would rather not iterate n-choose-two times.\n\nSay the input matrix is:\n\nA= \n[0 1 0 0 1\n 0 0 1 1 1\n 1 1 0 1 0]\n\n\nThe sparse representation is:\n\nA = \n0, 1\n0, 4\n1, 2\n1, 3\n1, 4\n2, 0\n2, 1\n2, 3\n\n\nIn Python, it\'s straightforward to work with the matrix-input format:\n\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom scipy.spatial.distance import cosine\n\nA = np.array(\n[[0, 1, 0, 0, 1],\n[0, 0, 1, 1, 1],\n[1, 1, 0, 1, 0]])\n\ndist_out = 1-pairwise_distances(A, metric="cosine")\ndist_out\n\n\nGives:\n\narray([[ 1.        ,  0.40824829,  0.40824829],\n       [ 0.40824829,  1.        ,  0.33333333],\n       [ 0.40824829,  0.33333333,  1.        ]])\n\n\nThat\'s fine for a full-matrix input, but I really want to start with the sparse representation (due to the size and sparsity of my matrix). Any ideas about how this could best be accomplished? Thanks in advance.\n'
"I have a dataframe where the first 3 columns are 'MONTH', 'DAY', 'YEAR'\n\nIn each column there is an integer.\nIs there a Pythonic way to convert all three columns into datetimes while there are in the dataframe?\n\nFrom:\n\nM    D    Y    Apples   Oranges\n5    6  1990      12        3\n5    7  1990      14        4\n5    8  1990      15       34\n5    9  1990      23       21\n\n\ninto:\n\nDatetimes    Apples   Oranges\n1990-6-5        12        3\n1990-7-5        14        4\n1990-8-5        15       34\n1990-9-5        23       21\n\n"
'I cannot figure out how to do "reverse melt" using Pandas in python.\nThis is my starting data\n\nimport pandas as pd\n\nfrom StringIO import StringIO\n\norigin = pd.read_table(StringIO(\'\'\'label    type    value\nx   a   1\nx   b   2\nx   c   3\ny   a   4\ny   b   5\ny   c   6\nz   a   7\nz   b   8\nz   c   9\'\'\'))\n\norigin\nOut[5]: \n  label type  value\n0     x    a      1\n1     x    b      2\n2     x    c      3\n3     y    a      4\n4     y    b      5\n5     y    c      6\n6     z    a      7\n7     z    b      8\n8     z    c      9\n\n\nThis is the output I would like to have:\n\n    label   a   b   c\n        x   1   2   3\n        y   4   5   6\n        z   7   8   9\n\n\nI\'m sure there is an easy way to do this, but I don\'t know how.\n'
"I have a large dataframe (several million rows).\n\nI want to be able to do a groupby operation on it, but just grouping by arbitrary consecutive (preferably equal-sized) subsets of rows, rather than using any particular property of the individual rows to decide which group they go to.\n\nThe use case: I want to apply a function to each row via a parallel map in IPython. It doesn't matter which rows go to which back-end engine, as the function calculates a result based on one row at a time. (Conceptually at least; in reality it's vectorized.)\n\nI've come up with something like this:\n\n# Generate a number from 0-9 for each row, indicating which tenth of the DF it belongs to\nmax_idx = dataframe.index.max()\ntenths = ((10 * dataframe.index) / (1 + max_idx)).astype(np.uint32)\n\n# Use this value to perform a groupby, yielding 10 consecutive chunks\ngroups = [g[1] for g in dataframe.groupby(tenths)]\n\n# Process chunks in parallel\nresults = dview.map_sync(my_function, groups)\n\n\nBut this seems very long-winded, and doesn't guarantee equal sized chunks. Especially if the index is sparse or non-integer or whatever.\n\nAny suggestions for a better way?\n\nThanks!\n"
"I've got a DataFrame who's index is just datetime.time and there's no method in  DataFrame.Index and datetime.time to shift the time. datetime.time has replace but that'll only work on individual items of the Series? \n\nHere's an example of the index used:\n\nIn[526]:  dfa.index[:5]\nOut[526]: Index([21:12:19, 21:12:20, 21:12:21, 21:12:21, 21:12:22], dtype='object')\n\nIn[527]:  type(dfa.index[0])\nOut[527]: datetime.time\n\n"
'I have a pandas dataframe and I want to filter the whole df based on the value of two columns in the data frame.  I want to get back all rows and columns where IBRD or IMF != 0.  \n\nalldata_balance = alldata[(alldata[IBRD] !=0) or (alldata[IMF] !=0)]\n\n\nbut this gives me a ValueError\n\n\n  ValueError: The truth value of a Series is ambiguous. Use a.empty,\n  a.bool(),       a.item(), a.any() or a.all().\n\n\nSo I know I am not using the or statement correctly, is there a way to do this?\n'
'I want to create a Pandas DataFrame filled with NaNs. During my research I found an answer:\n\nimport pandas as pd\n\ndf = pd.DataFrame(index=range(0,4),columns=[\'A\'])\n\n\nThis code results in a DataFrame filled with NaNs of type "object". So they cannot be used later on for example with the interpolate() method. Therefore, I created the DataFrame with this complicated code (inspired by this answer):\n\nimport pandas as pd\nimport numpy as np\n\ndummyarray = np.empty((4,1))\ndummyarray[:] = np.nan\n\ndf = pd.DataFrame(dummyarray)\n\n\nThis results in a DataFrame filled with NaN of type "float", so it can be used later on with interpolate(). Is there a more elegant way to create the same result?\n'
"I import a dataframe via read_csv, but for some reason can't extract the year or month from the series df['date'], trying that gives AttributeError: 'Series' object has no attribute 'year':\ndate    Count\n6/30/2010   525\n7/30/2010   136\n8/31/2010   125\n9/30/2010   84\n10/29/2010  4469\n\ndf = pd.read_csv('sample_data.csv', parse_dates=True)\n\ndf['date'] = pd.to_datetime(df['date'])\n\ndf['year'] = df['date'].year\ndf['month'] = df['date'].month\n\nUPDATE:\nand when I try solutions with df['date'].dt on my pandas version 0.14.1, I get   &quot;AttributeError: 'Series' object has no attribute 'dt' &quot;:\ndf = pd.read_csv('sample_data.csv',parse_dates=True)\n\ndf['date'] = pd.to_datetime(df['date'])\n\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\n\nSorry for this question that seems repetitive - I expect the answer will make me feel like a bonehead... but I have not had any luck using answers to the similar questions on SO.\n\nFOLLOWUP: I can't seem to update my pandas 0.14.1 to a newer release in my Anaconda environment, each of the attempts below generates an invalid syntax error. I'm using Python 3.4.1 64bit.\nconda update pandas\n\nconda install pandas==0.15.2\n\nconda install -f pandas\n\nAny ideas?\n"
"Given a Pandas DataFrame that has multiple columns with categorical values (0 or 1), is it possible to conveniently get the value_counts for every column at the same time?\n\nFor example, suppose I generate a DataFrame as follows:\n\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randint(0, 2, (10, 4)), columns=list('abcd'))\n\n\nI can get a DataFrame like this:\n\n   a  b  c  d\n0  0  1  1  0\n1  1  1  1  1\n2  1  1  1  0\n3  0  1  0  0\n4  0  0  0  1\n5  0  1  1  0\n6  0  1  1  1\n7  1  0  1  0\n8  1  0  1  1\n9  0  1  1  0\n\n\nHow do I conveniently get the value counts for every column and obtain the following conveniently?\n\n   a  b  c  d\n0  6  3  2  6\n1  4  7  8  4\n\n\nMy current solution is:\n\npieces = []\nfor col in df.columns:\n    tmp_series = df[col].value_counts()\n    tmp_series.name = col\n    pieces.append(tmp_series)\ndf_value_counts = pd.concat(pieces, axis=1)\n\n\nBut there must be a simpler way, like stacking, pivoting, or groupby?\n"
'I just started learning Pandas and was wondering if there is any difference between groupby() and pivot_table() functions. Can anyone help me understand the difference between them.\nHelp would be appreciated.\n'
'I have the following questions about HDF5 performance and concurrency:\n\n\nDoes HDF5 support concurrent write access? \nConcurrency considerations aside, how is HDF5 performance in terms of I/O performance (does compression rates affect the performance)?\nSince I use HDF5 with Python, how does its performance compare to Sqlite?\n\n\nReferences:\n\n\nhttp://www.sqlite.org/faq.html#q5\nLocking sqlite file on NFS filesystem possible?\nhttp://pandas.pydata.org/\n\n'
'I have a freshly installed Ubuntu on a freshly built computer. I just installed python-pip using apt-get. Now when I try to pip install Numpy and Pandas, it gives the following error.\n\nI\'ve seen this error mentioned in quite a few places on SO and Google, but I haven\'t been able to find a solution. Some people mention it\'s a bug, some threads are just dead... What\'s going on?\n\nTraceback (most recent call last):\n  File "/usr/bin/pip", line 9, in &lt;module&gt;\n    load_entry_point(\'pip==1.5.4\', \'console_scripts\', \'pip\')()\n  File "/usr/lib/python2.7/dist-packages/pip/__init__.py", line 185, in main\n    return command.main(cmd_args)\n  File "/usr/lib/python2.7/dist-packages/pip/basecommand.py", line 161, in main\n    text = \'\\n\'.join(complete_log)\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xe2 in position 72: ordinal not in range(128)\n\n'
"I have the following dataframe:\n\nDate        abc    xyz\n01-Jun-13   100    200\n03-Jun-13   -20    50\n15-Aug-13   40     -5\n20-Jan-14   25     15\n21-Feb-14   60     80\n\n\nI need to group the data by year and month. ie: Group by Jan 2013, Feb 2013, Mar 2013 etc...\nI will be using the newly grouped data to create a plot showing abc vs xyz per year/month.\n\nI've tried various combinations of groupby and sum but just can't seem to get anything to work.\n\nThank you for any assistance.\n"
"I am trying to unstack a multi-index with pandas and I am keep getting:\n\nValueError: Index contains duplicate entries, cannot reshape\n\n\nGiven a dataset with four columns:\n\n\nid (string)\ndate (string)\nlocation (string)\nvalue (float)\n\n\nI first set a three-level multi-index:\n\nIn [37]: e.set_index(['id', 'date', 'location'], inplace=True)\n\nIn [38]: e\nOut[38]: \n                                    value\nid           date       location       \nid1          2014-12-12 loc1        16.86\n             2014-12-11 loc1        17.18\n             2014-12-10 loc1        17.03\n             2014-12-09 loc1        17.28\n\n\nThen I try to unstack the location:\n\nIn [39]: e.unstack('location')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-39-bc1e237a0ed7&gt; in &lt;module&gt;()\n----&gt; 1 e.unstack('location')\n...\nC:\\Anaconda\\envs\\sandbox\\lib\\site-packages\\pandas\\core\\reshape.pyc in _make_selectors(self)\n    143 \n    144         if mask.sum() &lt; len(self.index):\n--&gt; 145             raise ValueError('Index contains duplicate entries, '\n    146                              'cannot reshape')\n    147 \n\nValueError: Index contains duplicate entries, cannot reshape\n\n\nWhat is going on here?\n"
"I'm trying to generate a date range of monthly data where the day is always at the beginning of the month:\n\npd.date_range(start='1/1/1980', end='11/1/1991', freq='M')\n\n\nThis generates 1/31/1980, 2/29/1980, and so on. Instead, I just want 1/1/1980, 2/1/1980,...\n\nI've seen other question ask about generating data that is always on a specific day of the month, with answers saying it wasn't possible, but beginning of month surely must be possible!\n"
'Normally when a dataframe undergoes a reset_index() the new column is assigned the name index or level_i depending on the level.\n\nIs it possible to assign the new column a name?\n'
"If I want to calculate the mean of two categories in Pandas, I can do it like this:\n\ndata = {'Category': ['cat2','cat1','cat2','cat1','cat2','cat1','cat2','cat1','cat1','cat1','cat2'],\n        'values': [1,2,3,1,2,3,1,2,3,5,1]}\nmy_data = DataFrame(data)\nmy_data.groupby('Category').mean()\n\nCategory:     values:   \ncat1     2.666667\ncat2     1.600000\n\n\nI have a lot of data formatted this way, and now I need to do a T-test to see if the mean of cat1 and cat2 are statistically different. How can I do that?\n"
"Code example:\n\nIn [171]: A = np.array([1.1, 1.1, 3.3, 3.3, 5.5, 6.6])\n\nIn [172]: B = np.array([111, 222, 222, 333, 333, 777])\n\nIn [173]: C = randint(10, 99, 6)\n\nIn [174]: df = pd.DataFrame(zip(A, B, C), columns=['A', 'B', 'C'])\n\nIn [175]: df.set_index(['A', 'B'], inplace=True)\n\nIn [176]: df\nOut[176]: \n          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n\n\nNow, I want to retrieve A values:\nQ1: in range [3.3, 6.6] - expected return value: [3.3, 5.5, 6.6] or [3.3, 3.3, 5.5, 6.6] in case last inclusive, and [3.3, 5.5] or [3.3, 3.3, 5.5] if not.\nQ2: in range [2.0, 4.0] - expected return value: [3.3] or [3.3, 3.3]  \n\nSame for any other MultiIndex dimension, for example B values:\nQ3: in range [111, 500] with repetitions, as number of data rows in range - expected return value: [111, 222, 222, 333, 333]    \n\nMore formal:\n\nLet us assume T is a table with columns A, B and C. The table includes n rows. Table cells are numbers, for example A double, B and C integers. Let's create a DataFrame of table T, let us name it DF. Let's set columns A and B indexes of DF (without duplication, i.e. no separate columns A and B as indexes, and separate as data), i.e. A and B in this case MultiIndex.  \n\nQuestions:  \n\n\nHow to write a query on the index, for example, to query the index A (or B), say in the labels interval [120.0, 540.0]? Labels 120.0 and 540.0 exist. I must clarify that I am interested only in the list of indices as a response to the query!\nHow to the same, but in case of the labels 120.0 and 540.0 do not exist, but there are labels by value lower than 120, higher than 120 and less than 540, or higher than 540?\nIn case the answer for Q1 and Q2 was unique index values, now the same, but with repetitions, as number of data rows in index range.\n\n\nI know the answers to the above questions in the case of columns which are not indexes, but in the indexes case, after a long research in the web and experimentation with the functionality of pandas, I did not succeed. The only method (without additional programming) I see now is to have a duplicate of A and B as data columns in addition to index.\n"
'So here is how my data set looks like :\n\nIn [1]: df1=pd.DataFrame(np.random.rand(4,2),index=["A","B","C","D"],columns=["I","J"])\n\nIn [2]: df2=pd.DataFrame(np.random.rand(4,2),index=["A","B","C","D"],columns=["I","J"])\n\nIn [3]: df1\nOut[3]: \n          I         J\nA  0.675616  0.177597\nB  0.675693  0.598682\nC  0.631376  0.598966\nD  0.229858  0.378817\n\nIn [4]: df2\nOut[4]: \n          I         J\nA  0.939620  0.984616\nB  0.314818  0.456252\nC  0.630907  0.656341\nD  0.020994  0.538303\n\n\nI want to have stacked bar plot for each dataframe but since they have same index, I\'d like to have 2 stacked bars per index.\n\nI\'ve tried to plot both on the same axes :\n\nIn [5]: ax = df1.plot(kind="bar", stacked=True)\n\nIn [5]: ax2 = df2.plot(kind="bar", stacked=True, ax = ax)\n\n\nBut it overlaps.\n\nThen I tried to concat the two dataset first :\n\npd.concat(dict(df1 = df1, df2 = df2),axis = 1).plot(kind="bar", stacked=True)\n\n\nbut here everything is stacked\n\nMy best try is :\n\n pd.concat(dict(df1 = df1, df2 = df2),axis = 0).plot(kind="bar", stacked=True)\n\n\nWhich gives :\n\n\n\nThis is basically what I want, except that I want the bar ordered as\n\n(df1,A) (df2,A) (df1,B) (df2,B) etc...\n\nI guess there is a trick but I can\'t found it !\n\n\n\nAfter @bgschiller\'s answer I got this :\n\n \n\nWhich is almost what I want. I would like the bar to be clustered by index, in order to have something visually clear.\n\nBonus : Having the x-label not redundant, something like : \n\ndf1 df2    df1 df2\n_______    _______ ...\n   A          B\n\n\nThanks for helping.\n'
"How do I get the min and max Dates from a dataframe's major axis?\n           value\nDate                                           \n2014-03-13  10000.000 \n2014-03-21   2000.000 \n2014-03-27   2000.000 \n2014-03-17    200.000 \n2014-03-17      5.000 \n2014-03-17     70.000 \n2014-03-21    200.000 \n2014-03-27      5.000 \n2014-03-27     25.000 \n2014-03-31      0.020 \n2014-03-31     12.000 \n2014-03-31      0.022\n\nEssentially I want a way to get the min and max dates, i.e. 2014-03-13 and 2014-03-31. I tried using numpy.min or df.min(axis=0), I'm able to get the min or max value but that's not what I want\n"
"Can someone point me to a link or provide an explanation of the benefits of indexing in pandas? I routinely deal with tables and join them based on columns, and this joining/merging process seems to re-index things anyway, so it's a bit cumbersome to apply index criteria considering I don't think I need to.\n\nAny thoughts on best-practices around indexing?\n"
"Create a day-of-week column in a Pandas dataframe using Python\n\nI’d like to read a csv file into a pandas dataframe, parse a column of dates from string format to a date object, and then generate a new column that indicates the day of the week.\n\nThis is what I’m trying:\n\nWhat I’d like to do is something like:\n\nimport pandas as pd\n\nimport csv\n\ndf = pd.read_csv('data.csv', parse_dates=['date']))\n\ndf['day-of-week'] = df['date'].weekday()\n\n\nAttributeError: 'Series' object has no attribute 'weekday'\n\n\n\n\nThank you for your help.\nJames\n"
'I have a pandas dataframe "df". In this dataframe I have multiple columns, one of which I have to substring.\nLets say the column name is "col".\nI can run a "for" loop like below and substring the column:\n\nfor i in range(0,len(df)):\n  df.iloc[i].col = df.iloc[i].col[:9]\n\n\nBut I wanted to know, if there is an option where I don\'t have to use a "for" loop, and do it directly using an attribute.I have huge amount of data, and if I do this, the data will take a very long time process.\n'
"I've read an SQL query into Pandas and the values are coming in as dtype 'object', although they are strings, dates and integers. I am able to convert the date 'object' to a Pandas datetime dtype, but I'm getting an error when trying to convert the string and integers.\n\nHere is an example:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_sql_query('select * from my_table', conn)\n&gt;&gt;&gt; df\n    id    date          purchase\n 1  abc1  2016-05-22    1\n 2  abc2  2016-05-29    0\n 3  abc3  2016-05-22    2\n 4  abc4  2016-05-22    0\n\n&gt;&gt;&gt; df.dtypes\n id          object\n date        object\n purchase    object\n dtype: object\n\n\nConverting the df['date'] to a datetime works:\n\n&gt;&gt;&gt; pd.to_datetime(df['date'])\n 1  2016-05-22\n 2  2016-05-29\n 3  2016-05-22\n 4  2016-05-22\n Name: date, dtype: datetime64[ns] \n\n\nBut I get an error when trying to convert the df['purchase'] to an integer:\n\n&gt;&gt;&gt; df['purchase'].astype(int)\n ....\n pandas/lib.pyx in pandas.lib.astype_intsafe (pandas/lib.c:16667)()\n pandas/src/util.pxd in util.set_value_at (pandas/lib.c:67540)()\n\n TypeError: long() argument must be a string or a number, not 'java.lang.Long'\n\n\nNOTE: I get a similar error when I tried .astype('float')\n\nAnd when trying to convert to a string, nothing seems to happen.\n\n&gt;&gt;&gt; df['id'].apply(str)\n 1 abc1\n 2 abc2\n 3 abc3\n 4 abc4\n Name: id, dtype: object\n\n"
'I have a massive dataframe, and I\'m getting the error:\n\nTypeError: ("Empty \'DataFrame\': no numeric data to plot", \'occurred at index 159220\')\n\nI\'ve already dropped nulls, and checked dtypes for the DataFrame so I have no guess as to why it\'s failing on that row.\n\nHow do I print out just that row (at index 159220) of the data frame?\n\nThanks\n'
'I want to apply a lambda function to a DataFrame column using if...elif...else within the lambda function.\n\nThe df and the code are smth. like:\n\ndf=pd.DataFrame({"one":[1,2,3,4,5],"two":[6,7,8,9,10]})\n\ndf["one"].apply(lambda x: x*10 if x&lt;2 elif x&lt;4 x**2 else x+10)\n\n\nobviously this way it is not working.\nIs there a way to apply if....elif....else to lambda?\nHow can I relize the same result with List Comprehension?\n\nThanks for any response.  \n'
"Hi I want to get the counts of unique values of the dataframe. count_values implements this however I want to use its output somewhere else. How can I convert .count_values output to a pandas dataframe. here is an example code:\n\nimport pandas as pd\ndf = pd.DataFrame({'a':[1, 1, 2, 2, 2]})\nvalue_counts = df['a'].value_counts(dropna=True, sort=True)\nprint(value_counts)\nprint(type(value_counts))\n\n\noutput is:\n\n2    3\n1    2\nName: a, dtype: int64\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nWhat I need is a dataframe like this:\n\nunique_values  counts\n2              3\n1              2\n\n\nThank you. \n"
"df2 = pd.DataFrame({'X' : ['X1', 'X1', 'X1', 'X1'], 'Y' : ['Y2','Y1','Y1','Y1'], 'Z' : ['Z3','Z1','Z1','Z2']})\n\n    X   Y   Z\n0  X1  Y2  Z3\n1  X1  Y1  Z1\n2  X1  Y1  Z1\n3  X1  Y1  Z2\n\ng=df2.groupby('X')\n\npd.pivot_table(g, values='X', rows='Y', cols='Z', margins=False, aggfunc='count')\n\n\n\n  Traceback (most recent call last): ... AttributeError: 'Index' object\n  has no attribute 'index'\n\n\nHow do I get a Pivot Table with counts of unique values of one DataFrame column for two other columns?\nIs there aggfunc for count unique? Should I be using np.bincount()?\n\nNB. I am aware of 'Series' values_counts() however I need a pivot table.\n\n\n\nEDIT: The output should be:\n\nZ   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n\n"
'I was experimenting with the kaggle.com Titanic data set (data on every person on the Titanic) and came up with a gender breakdown like this:\n\ngender = df.sex.value_counts()\ngender\n\nmale   577\nfemale 314 \n\n\nI would like to find out the percentage of each gender on the Titanic.\n\nMy approach is slightly less than ideal:\n\nfrom __future__ import division\npcts = gender / gender.sum()\npcts\n\nmale      0.647587\nfemale    0.352413\n\n\nIs there a better (more idiomatic) way?\n\nThanks!\n'
'I would like to compare two histograms by having the Y axis show the percentage of each column from the overall dataset size instead of an absolute value. Is that possible? I am using Pandas and matplotlib.\nThanks\n'
'I am transitioning from R to Python. I just began using Pandas. I have an R code that subsets nicely:\n\nk1 &lt;- subset(data, Product = p.id &amp; Month &lt; mn &amp; Year == yr, select = c(Time, Product))\n\n\nNow, I want to do similar stuff in Python. this is what I have got so far:\n\nimport pandas as pd\ndata = pd.read_csv("../data/monthly_prod_sales.csv")\n\n\n#first, index the dataset by Product. And, get all that matches a given \'p.id\' and time.\n data.set_index(\'Product\')\n k = data.ix[[p.id, \'Time\']]\n\n# then, index this subset with Time and do more subsetting..\n\n\nI am beginning to feel that I am doing this the wrong way. perhaps, there is an elegant solution. Can anyone help? I need to extract month and year from the timestamp I have and do subsetting. Perhaps there is a one-liner that will accomplish all this:\n\nk1 &lt;- subset(data, Product = p.id &amp; Time &gt;= start_time &amp; Time &lt; end_time, select = c(Time, Product))\n\n\nthanks.\n'
'I have a data frame df which looks like this. Date and Time are 2 multilevel index\n                           observation1   observation2\ndate          Time                             \n2012-11-02    9:15:00      79.373668      224\n              9:16:00      130.841316     477\n2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n2012-11-04    9:15:00      115.449437     122\n              9:16:00      123.776946     555\n              9:17:00      153.76646      344\n              9:18:00      463.276946     212\n\nI want to run some complex process over daily data block.\nPseudo code would look like\n for count in df(level 0 index) :\n     new_df = get only chunk for count\n     complex_process(new_df)\n\nSo, first of all, I could not find a way to access only blocks for a date\n2012-11-03    9:15:00      45.312814      835\n              9:16:00      123.776946     623\n              9:17:00      153.76646      624\n              9:18:00      463.276946     626\n              9:19:00      663.176934     622\n              9:20:00      763.77333      621\n\nand then send it for processing. I am doing this in for loop as I am not sure if there is any way to do it without mentioning exact value of level 0 column. I did some basic search and able to get df.index.get_level_values(0), but it returns me all the values and that causes loop to run multiple times for a day. I want to create a dataframe per day and send it for processing.\n'
'Is there a way to round a single column in pandas without affecting the rest of the dataframe?\n\n df:\n      item  value1  value2\n    0    a    1.12     1.3\n    1    a    1.50     2.5\n    2    a    0.10     0.0\n    3    b    3.30    -1.0\n    4    b    4.80    -1.0\n\n\ndf.value1.apply(np.round)\ngives\n\n0    1\n1    2\n2    0\n3    3\n4    5\n5    5\n\n\nWhat is the correct way to make data look like this:\n\n  item  value1  value2\n0    a       1     1.3\n1    a       2     2.5\n2    a       0     0.0\n3    b       3    -1.0\n4    b       5    -1.0\n5    c       5     5.0\n\n'
"I received a DataFrame from somewhere and want to create another DataFrame with the same number and names of columns and rows (indexes). For example, suppose that the original data frame was created as\n\nimport pandas as pd\ndf1 = pd.DataFrame([[11,12],[21,22]], columns=['c1','c2'], index=['i1','i2'])\n\n\nI copied the structure by explicitly defining the columns and names:\n\ndf2 = pd.DataFrame(columns=df1.columns, index=df1.index)    \n\n\nI don't want to copy the data, otherwise I could just write df2 = df1.copy(). In other words, after df2 being created it must contain only NaN elements:\n\nIn [1]: df1\nOut[1]: \n    c1  c2\ni1  11  12\ni2  21  22\n\nIn [2]: df2\nOut[2]: \n     c1   c2\ni1  NaN  NaN\ni2  NaN  NaN\n\n\nIs there a more idiomatic way of doing it?\n"
"I am trying to find the number of times a certain value appears in one column.\n\nI have made the dataframe with data = pd.DataFrame.from_csv('data/DataSet2.csv')\n\nand now I want to find the number of times something appears in a column. How is this done?\n\nI thought it was the below, where I am looking in the education column and counting the number of time ? occurs.\n\nThe code below shows that I am trying to find the number of times 9th appears and the error is what I am getting when I run the code\n\nCode\n\nmissing2 = df.education.value_counts()['9th']\nprint(missing2)\n\n\nError\n\nKeyError: '9th'\n\n"
"I am trying to count the duplicates of each type of row in my dataframe. For example, say that I have a dataframe in pandas as follows:\n\ndf = pd.DataFrame({'one': pd.Series([1., 1, 1]),\n                   'two': pd.Series([1., 2., 1])})\n\n\nI get a df that looks like this:\n\n    one two\n0   1   1\n1   1   2\n2   1   1\n\n\nI imagine the first step is to find all the different unique rows, which I do by:\n\ndf.drop_duplicates()\n\n\nThis gives me the following df:\n\n    one two\n0   1   1\n1   1   2\n\n\nNow I want to take each row from the above df ([1 1] and [1 2]) and get a count of how many times each is in the initial df. My result would look something like this:\n\nRow     Count\n[1 1]     2\n[1 2]     1\n\n\nHow should I go about doing this last step?\n\nEdit:\n\nHere's a larger example to make it more clear:\n\ndf = pd.DataFrame({'one': pd.Series([True, True, True, False]),\n                   'two': pd.Series([True, False, False, True]),\n                   'three': pd.Series([True, False, False, False])})\n\n\ngives me:\n\n    one three   two\n0   True    True    True\n1   True    False   False\n2   True    False   False\n3   False   False   True\n\n\nI want a result that tells me:\n\n       Row           Count\n[True True True]       1\n[True False False]     2\n[False False True]     1\n\n"
'Here is how I encountered the error:\n\ndf.loc[a_list][df.a_col.isnull()]\n\n\nThe type of a_list is Int64Index， it contains a list of row indexes. All of these row indexes belong to df.\n\nThe df.a_col.isnull() part is a condition I need for filtering.\n\nIf I execute the following commands individually, I do not get any warnings:\n\ndf.loc[a_list]\ndf[df.a_col.isnull()]\n\n\nBut if I put them together df.loc[a_list][df.a_col.isnull()], I get the warning message (but I can see the result):\n\n\n  Boolean Series key will be reindexed to match DataFrame index\n\n\nWhat is the meaning of this error message? Does it affect the result that it returned?\n'
'I have the following dataframe:\n\nYear    Country          medal    no of medals\n1896    Afghanistan      Gold        5\n1896    Afghanistan      Silver      4\n1896    Afghanistan      Bronze      3\n1896    Algeria          Gold        1\n1896    Algeria          Silver      2\n1896    Algeria          Bronze      3\n\n\nI want it this way.\n\nYear    Country      Gold   Silver   Bronze\n1896    Afghanistan    5      4         3\n1896    Algeria        1      2         3\n\n\nStack/Unstack dont seem to work.\n'
'With this DataFrame, how can I conditionally set rating to 0 when line_race is equal to zero? \n\n    line_track  line_race  rating foreign\n 25        MTH         10     84    False\n 26        MTH          6     88    False\n 27        TAM          5     87    False\n 28         GP          2     86    False\n 29         GP          7     59    False\n 30        LCH          0    103     True\n 31        LEO          0    125     True\n 32        YOR          0    126     True\n 33        ASC          0    124     True\n\n\nIn other words, what is the proper way on a DataFrame to say if ColumnA = x then ColumnB = y else ColumnB = ColumnB\n'
"Here is an example of what I am trying to get:\n\nI have:\n\nimport pandas as pd \ndf = pd.DataFrame({'A' : [0, 1], 'B' : [1, 6]})\n\n\nMy goal is:\n\n',A,B\\n0,0,1\\n1,1,6\\n'\n\n\nI can achieve this with lazy and horrible:\n\ndf.to_csv('temp.csv') # create unnecessary file\nbody = open('temp.csv').read()\n\n\nAlso to_string() methods looks very promising; however, the best I can come up with is this:\n\nbody = df.to_string()[1:].replace('  ', ',') + '\\n'\n\n\nThis does not create an unnecessary file, but seems sloppy and perhaps not very reliable.\n\nAm I missing a simpler solution?\n"
"I have a pandas data frame where the first 3 columns are strings:\n\n         ID        text1    text 2\n0       2345656     blah      blah\n1          3456     blah      blah\n2        541304     blah      blah        \n3        201306       hi      blah        \n4   12313201308    hello      blah         \n\n\nI want to add leading zeros to the ID:\n\n                ID    text1    text 2\n0  000000002345656     blah      blah\n1  000000000003456     blah      blah\n2  000000000541304     blah      blah        \n3  000000000201306       hi      blah        \n4  000012313201308    hello      blah \n\n\nI have tried:\n\ndf['ID'] = df.ID.zfill(15)\ndf['ID'] = '{0:0&gt;15}'.format(df['ID'])\n\n"
'Say I have data about 3 trading strategies, each with and without transaction costs.  I want to plot, on the same axes, the time series of each of the 6 variants (3 strategies * 2 trading costs).  I would like the "with transaction cost" lines to be plotted with alpha=1 and linewidth=1 while I want the "no transaction costs" to be plotted with alpha=0.25 and linewidth=5.  But I would like the color to be the same for both versions of each strategy. \n\nI would like something along the lines of:\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 10))\n\nfor c in with_transaction_frame.columns:\n    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)\n\n****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE\n\nfor c in no_transaction_frame.columns:\n    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)\n\nax.legend()\n\n\nWhat is the appropriate code to put on the indicated line to reset the color cycle so it is "back to the start" when the second loop is invoked?\n'
"I have used rosetta.parallel.pandas_easy to parallelize apply after groupby, for example:\n\nfrom rosetta.parallel.pandas_easy import groupby_to_series_to_frame\ndf = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\ngroupby_to_series_to_frame(df, np.mean, n_jobs=8, use_apply=True, by=df.index)\n\n\nHowever, has anyone figured out how to parallelize a function that returns a DataFrame? This code fails for rosetta, as expected.\n\ndef tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndf.groupby(df.index).apply(tmpFunc)\ngroupby_to_series_to_frame(df, tmpFunc, n_jobs=1, use_apply=True, by=df.index)\n\n"
'I\'m surely missing something simple here. Trying to merge two dataframes in pandas that have mostly the same column names, but the right dataframe has some columns that the left doesn\'t have, and vice versa. \n\n&gt;df_may\n\n  id  quantity  attr_1  attr_2\n0  1        20       0       1\n1  2        23       1       1\n2  3        19       1       1\n3  4        19       0       0\n\n&gt;df_jun\n\n  id  quantity  attr_1  attr_3\n0  5         8       1       0\n1  6        13       0       1\n2  7        20       1       1\n3  8        25       1       1\n\n\nI\'ve tried joining with an outer join:\n\nmayjundf = pd.DataFrame.merge(df_may, df_jun, how="outer")\n\n\nBut that yields:\n\nLeft data columns not unique: Index([....\n\n\nI\'ve also specified a single column to join on (on = "id", e.g.), but that duplicates all columns except "id" like attr_1_x, attr_1_y, which is not ideal. I\'ve also passed the entire list of columns (there are many) to "on":\n\nmayjundf = pd.DataFrame.merge(df_may, df_jun, how="outer", on=list(df_may.columns.values))\n\n\nWhich yields:\n\nValueError: Buffer has wrong number of dimensions (expected 1, got 2)\n\n\nWhat am I missing? I\'d like to get a df with all rows appended, and attr_1, attr_2, attr_3 populated where possible, NaN where they don\'t show up. This seems like a pretty typical workflow for data munging, but I\'m stuck.\n\nThanks in advance.\n'
"I am trying to column-bind dataframes and having issue with pandas concat, as ignore_index=True doesn't seem to work:\n\ndf1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                    'B': ['B0', 'B1', 'B2', 'B3'],\n                    'D': ['D0', 'D1', 'D2', 'D3']},\n                    index=[0, 2, 3,4])\n\ndf2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],\n                    'C': ['C4', 'C5', 'C6', 'C7'],\n                    'D2': ['D4', 'D5', 'D6', 'D7']},\n                    index=[ 5, 6, 7,3])\ndf1\n#     A   B   D\n# 0  A0  B0  D0\n# 2  A1  B1  D1\n# 3  A2  B2  D2\n# 4  A3  B3  D3\n\ndf2\n#    A1   C  D2\n# 5  A4  C4  D4\n# 6  A5  C5  D5\n# 7  A6  C6  D6\n# 3  A7  C7  D7\n\ndfs = [df1,df2]\ndf = pd.concat( dfs,axis=1,ignore_index=True)     \nprint df   \n\n\nand the result is \n\n     0    1    2    3    4    5    \n0   A0   B0   D0  NaN  NaN  NaN  \n2   A1   B1   D1  NaN  NaN  NaN    \n3   A2   B2   D2   A7   C7   D7   \n4   A3   B3   D3  NaN  NaN  NaN  \n5  NaN  NaN  NaN   A4   C4   D4  \n6  NaN  NaN  NaN   A5   C5   D5  \n7  NaN  NaN  NaN   A6   C6   D6           \n\n\nEven if I reset index using\n\n df1.reset_index()    \n df2.reset_index() \n\n\nand then try \n\npd.concat([df1,df2],axis=1) \n\n\nit still produces the same result!\n"
'I have a column with consecutive digits in a Pandas DataFrame.\n\nA\n1\n2\n3\n4\n\n\nI would like to change all those values to a simple string, say "foo", resulting in\n\nA\nfoo\nfoo\nfoo\nfoo\n\n'
"How can the length of the lists in the column be determine without iteration?\nI have a dataframe like this:\n                                                    CreationDate\n2013-12-22 15:25:02                  [ubuntu, mac-osx, syslinux]\n2009-12-14 14:29:32  [ubuntu, mod-rewrite, laconica, apache-2.2]\n2013-12-22 15:42:00               [ubuntu, nat, squid, mikrotik]\n\nI am calculation length of lists in the CreationDate column and making a new Length column like this:\ndf['Length'] = df.CreationDate.apply(lambda x: len(x))\n\nWhich gives me this:\n                                                    CreationDate  Length\n2013-12-22 15:25:02                  [ubuntu, mac-osx, syslinux]       3\n2009-12-14 14:29:32  [ubuntu, mod-rewrite, laconica, apache-2.2]       4\n2013-12-22 15:42:00               [ubuntu, nat, squid, mikrotik]       4\n\nIs there a more pythonic way to do this?\n"
'I\'m using Pandas to explore some datasets. I have this dataframe:\n\n\n\nI want to exclude any row that has a city value. So I\'ve tried:\n\nnew_df = all_df[(all_df["City"] == "None") ]\nnew_df\n\n\nBut then I got an empty dataframe:\n\n\n\nIt works whenever I use any value other than None. Any idea how to filter this dataframe?\n'
'This is a self-answered post. Below I outline a common problem in the NLP domain and propose a few performant methods to solve it.\n\nOftentimes the need arises to remove punctuation during text cleaning and pre-processing. Punctuation is defined as any character in string.punctuation:\n\n&gt;&gt;&gt; import string\nstring.punctuation\n\'!"#$%&amp;\\\'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\'\n\n\nThis is a common enough problem and has been asked before ad nauseam. The most idiomatic solution uses pandas str.replace. However, for situations which involve a lot of text, a more performant solution may need to be considered. \n\nWhat are some good, performant alternatives to str.replace when dealing with hundreds of thousands of records?\n'
'I have a DataFrame object similar to this one:\n\n       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     41.815    1.3\n4     61.606    1.3\n...\n\n\nWhat I would like to do is insert a row at a position specified by some index value and update the following indices accordingly. E.g.:\n\n       onset    length\n1      2.215    1.3\n2     23.107    1.3\n3     30.000    1.3  # new row\n4     41.815    1.3\n5     61.606    1.3\n...\n\n\nWhat would be the best way to do this?\n'
'Lets say I have a MultiIndex Series s:\n\n&gt;&gt;&gt; s\n     values\na b\n1 2  0.1 \n3 6  0.3\n4 4  0.7\n\n\nand I want to apply a function which uses the index of the row:\n\ndef f(x):\n   # conditions or computations using the indexes\n   if x.index[0] and ...: \n   other = sum(x.index) + ...\n   return something\n\n\nHow can I do s.apply(f) for such a function? What is the recommended way to make this kind of operations? I expect to obtain a new Series with the values resulting from this function applied on each row and the same MultiIndex.\n'
"Assume I have two dataframes of this format (call them df1 and df2):\n\n+------------------------+------------------------+--------+\n|        user_id         |      business_id       | rating |\n+------------------------+------------------------+--------+\n| rLtl8ZkDX5vH5nAx9C3q5Q | eIxSLxzIlfExI6vgAbn2JA |      4 |\n| C6IOtaaYdLIT5fWd7ZYIuA | eIxSLxzIlfExI6vgAbn2JA |      5 |\n| mlBC3pN9GXlUUfQi1qBBZA | KoIRdcIfh3XWxiCeV1BDmA |      3 |\n+------------------------+------------------------+--------+\n\n\nI'm looking to get a dataframe of all the rows that have a common user_id in df1 and df2. (ie. if a user_id is in both df1 and df2, include the two rows in the output dataframe)\n\nI can think of many ways to approach this, but they all strike me as clunky. For example, we could find all the unique user_ids in each dataframe, create a set of each, find their intersection, filter the two dataframes with the resulting set and concatenate the two filtered dataframes.\n\nMaybe that's the best approach, but I know Pandas is clever. Is there a simpler way to do this? I've looked at merge but I don't think that's what I need.\n"
'Using the pandas library in python and using \n\n.plot()\n\n\non a dataframe,  how do I display the plot without a legend?\n'
"I have a large dataframe in pandas that apart from the column used as index is supposed to have only numeric values:\n\ndf = pd.DataFrame({'a': [1, 2, 3, 'bad', 5],\n                   'b': [0.1, 0.2, 0.3, 0.4, 0.5],\n                   'item': ['a', 'b', 'c', 'd', 'e']})\ndf = df.set_index('item')\n\n\nHow can I find the row of the dataframe df that has a non-numeric value in it? \n\nIn this example it's the fourth row in the dataframe, which has the string 'bad' in the a column. How can this row be found programmatically?\n"
"I am new to using DataFrame and I would like to know how to perform a SQL equivalent of left outer join on multiple columns on a series of tables\n\nExample:\n\ndf1: \nYear    Week    Colour    Val1 \n2014       A       Red      50\n2014       B       Red      60\n2014       B     Black      70\n2014       C       Red      10\n2014       D     Green      20\n\ndf2:\nYear    Week    Colour    Val2\n2014       A     Black      30\n2014       B     Black     100\n2014       C     Green      50\n2014       C       Red      20\n2014       D       Red      40\n\ndf3:\nYear    Week    Colour    Val3\n2013       B       Red      60\n2013       C     Black      80\n2013       B     Black      10\n2013       D     Green      20\n2013       D       Red      50\n\n\nEssentially I want to do something like this SQL code (Notice that df3 is not joined on Year):\n\nSELECT df1.*, df2.Val2, df3.Val3\nFROM df1\n  LEFT OUTER JOIN df2\n    ON df1.Year = df2.Year\n    AND df1.Week = df2.Week\n    AND df1.Colour = df2.Colour\n  LEFT OUTER JOIN df3\n    ON df1.Week = df3.Week\n    AND df1.Colour = df3.Colour\n\n\nThe result should look like:\n\nYear    Week    Colour    Val1    Val2    Val3\n2014       A       Red      50    Null    Null\n2014       B       Red      60    Null      60\n2014       B     Black      70     100    Null\n2014       C       Red      10      20    Null\n2014       D     Green      20    Null    Null\n\n\nI have tried using merge and join but can't figure out how to do it on multiple tables and when there are multiple joints involved. Could someone help me on this please?\n\nThanks\n"
"I have a dataframe with a timeindex and 3 columns containing the coordinates of a 3D vector:\n\n                         x             y             z\nts\n2014-05-15 10:38         0.120117      0.987305      0.116211\n2014-05-15 10:39         0.117188      0.984375      0.122070\n2014-05-15 10:40         0.119141      0.987305      0.119141\n2014-05-15 10:41         0.116211      0.984375      0.120117\n2014-05-15 10:42         0.119141      0.983398      0.118164\n\n\nI would like to apply a transformation to each row that also returns a vector\n\ndef myfunc(a, b, c):\n    do something\n    return e, f, g\n\n\nbut if I do:\n\ndf.apply(myfunc, axis=1)\n\n\nI end up with a Pandas series whose elements are tuples. This is beacause apply will take the result of myfunc without unpacking it. How can I change myfunc so that I obtain a new df with 3 columns?\n\nEdit:\n\nAll solutions below work. The Series solution does allow for column names, the List solution seem to execute faster.\n\ndef myfunc1(args):\n    e=args[0] + 2*args[1]\n    f=args[1]*args[2] +1\n    g=args[2] + args[0] * args[1]\n    return pd.Series([e,f,g], index=['a', 'b', 'c'])\n\ndef myfunc2(args):\n    e=args[0] + 2*args[1]\n    f=args[1]*args[2] +1\n    g=args[2] + args[0] * args[1]\n    return [e,f,g]\n\n%timeit df.apply(myfunc1 ,axis=1)\n\n100 loops, best of 3: 4.51 ms per loop\n\n%timeit df.apply(myfunc2 ,axis=1)\n\n100 loops, best of 3: 2.75 ms per loop\n\n"
'I need to merge two pandas dataframes on an identifier and a condition where a date in one dataframe is between two dates in the other dataframe.\n\nDataframe A has a date ("fdate") and an ID ("cusip"):\n\n\n\nI need to merge this with this dataframe B:\n\n\n\non A.cusip==B.ncusip and A.fdate is between B.namedt and B.nameenddt.\n\nIn SQL this would be trivial, but the only way I can see how to do this in pandas is to first merge unconditionally on the identifier, and then filter on the date condition:\n\ndf = pd.merge(A, B, how=\'inner\', left_on=\'cusip\', right_on=\'ncusip\')\ndf = df[(df[\'fdate\']&gt;=df[\'namedt\']) &amp; (df[\'fdate\']&lt;=df[\'nameenddt\'])]\n\n\nIs this really the best way to do this? It seems that it would be much better if one could filter within the merge so as to avoid having a potentially very large dataframe after the merge but before the filter has completed.\n'
"I have a dataframe df1 which looks like:\n   c  k  l\n0  A  1  a\n1  A  2  b\n2  B  2  a\n3  C  2  a\n4  C  2  d\n\nand another called df2 like:\n   c  l\n0  A  b\n1  C  a\n\nI would like to filter df1 keeping only the values that ARE NOT in df2. Values to filter are expected to be as (A,b) and (C,a) tuples. So far I tried to apply the isin method:\nd = df[~(df['l'].isin(dfc['l']) &amp; df['c'].isin(dfc['c']))]\n\nThat seems to me too complicated, it returns:\n   c  k  l\n2  B  2  a\n4  C  2  d\n\nbut I'm expecting:\n   c  k  l\n0  A  1  a\n2  B  2  a\n4  C  2  d\n\n"
"I want to change a dataframes' index (rows) from float64 to string or unicode. \n\nI thought this would work but apparently not:\n\n#check type\ntype(df.index)\n'pandas.core.index.Float64Index'\n\n#change type to unicode\nif not isinstance(df.index, unicode):\n    df.index = df.index.astype(unicode)\n\n\nerror message:\n\nTypeError: Setting &lt;class 'pandas.core.index.Float64Index'&gt; dtype to anything other than float64 or object is not supported\n\n"
"Given the following data frame:\nimport pandas as pd\nimport numpy as np\ndf=pd.DataFrame({'A':['A','A','A','B','B','B'],\n                'B':['a','a','b','a','a','a'],\n                })\ndf\n\n    A   B\n0   A   a \n1   A   a \n2   A   b \n3   B   a \n4   B   a \n5   B   a\n\nI'd like to create column 'C', which numbers the rows within each group in columns A and B like this:\n    A   B   C\n0   A   a   1\n1   A   a   2\n2   A   b   1\n3   B   a   1\n4   B   a   2\n5   B   a   3\n\nI've tried this so far:\ndf['C']=df.groupby(['A','B'])['B'].transform('rank')\n\n...but it doesn't work!\n"
'I have this simplified dataframe:\n\nID   Fruit\nF1   Apple\nF2   Orange\nF3   Banana \n\n\nI want to add in the begining of the dataframe a new column df[\'New_ID\']  which has the number 880 that increments by one in each row.\n\nThe output should be simply like:\n\nNew_ID   ID   Fruit\n880      F1   Apple\n881      F2   Orange\n882      F3   Banana  \n\n\nI tried the following:\n\ndf[\'New_ID\'] = ["880"] # but I want to do this without assigning it the list of numbers literally\n\n\nAny idea how to solve this?\n\nThanks!\n'
"I have a pandas data frame my_df, where I can find the mean(), median(), mode() of a given column:\n\nmy_df['field_A'].mean()\nmy_df['field_A'].median()\nmy_df['field_A'].mode()\n\n\nI am wondering is it possible to find more detailed stats such as 90 percentile? Thanks!\n"
"Assume an easy dataframe, for example\n\n    A         B\n0   1  0.810743\n1   2  0.595866\n2   3  0.154888\n3   4  0.472721\n4   5  0.894525\n5   6  0.978174\n6   7  0.859449\n7   8  0.541247\n8   9  0.232302\n9  10  0.276566\n\n\nHow can I retrieve an index value of a row, given a condition?\nFor example:\ndfb = df[df['A']==5].index.values.astype(int)\nreturns [4], but what I would like to get is just 4. This is causing me troubles later in the code.\n\nBased on some conditions, I want to have a record of the indexes where that condition is fulfilled, and then select rows between. \n\nI tried\n\ndfb = df[df['A']==5].index.values.astype(int)\ndfbb = df[df['A']==8].index.values.astype(int)\ndf.loc[dfb:dfbb,'B']\n\n\nfor a desired output\n\n    A         B\n4   5  0.894525\n5   6  0.978174\n6   7  0.859449\n\n\nbut I get TypeError: '[4]' is an invalid key\n"
"I'm having this data frame:\n\nName   Date    Quantity\nApple  07/11/17  20\norange 07/14/17  20\nApple  07/14/17  70\nOrange 07/25/17  40\nApple  07/20/17  30\n\n\nI want to aggregate this by Name and Date to get sum of quantities\nDetails:\n\nDate: Group, the result should be at the beginning of the week (or just on Monday)\n\nQuantity: Sum, if two or more record have same Name and Date(if falls on same interval)\n\nThe desired output is given below:\n\nName   Date    Quantity\nApple  07/10/17  90\norange 07/10/17  20\nApple  07/17/17  30\norange 07/24/17  40\n\n\nThanks in advance\n"
"I have a dataframe with values like \n\nA B\n1 4\n2 6\n3 9\n\n\nI need to add a new column by adding values from column A and B, like\n\nA B C\n1 4 5\n2 6 8\n3 9 12\n\n\nI believe this can be done using lambda function, but I can't figure out how to do it.\n"
"I have a time series object grouped of the type &lt;pandas.core.groupby.SeriesGroupBy object at 0x03F1A9F0&gt;. grouped.sum() gives the desired result but I cannot get rolling_sum to work with the groupby object. Is there any way to apply rolling functions to groupby objects? For example:\n\nx = range(0, 6)\nid = ['a', 'a', 'a', 'b', 'b', 'b']\ndf = DataFrame(zip(id, x), columns = ['id', 'x'])\ndf.groupby('id').sum()\nid    x\na    3\nb   12\n\n\nHowever, I would like to have something like:\n\n  id  x\n0  a  0\n1  a  1\n2  a  3\n3  b  3\n4  b  7\n5  b  12\n\n"
"For example, I have:\n\nIn [1]: df = pd.DataFrame([8, 9],\n                          index=pd.MultiIndex.from_tuples([(1, 1, 1),\n                                                           (1, 3, 2)]),\n                          columns=['A'])\n\nIn [2] df\nOut[2]: \n       A\n1 1 1  8\n  3 2  9\n\n\nIs there a better way to remove the last level from the index than this:\n\nIn [3]: pd.DataFrame(df.values,\n                     index=df.index.droplevel(2),\n                     columns=df.columns)\nOut[3]: \n     A\n1 1  8\n  3  9\n\n"
'This works (using Pandas 12 dev)\n\ntable2=table[table[\'SUBDIVISION\'] ==\'INVERNESS\']\n\n\nThen I realized I needed to select the field using "starts with" Since I was missing a bunch.\nSo per the Pandas doc as near as I could follow I tried \n\ncriteria = table[\'SUBDIVISION\'].map(lambda x: x.startswith(\'INVERNESS\'))\ntable2 = table[criteria]\n\n\nAnd got AttributeError: \'float\' object has no attribute \'startswith\'\n\nSo I tried an alternate syntax with the same result\n\ntable[[x.startswith(\'INVERNESS\') for x in table[\'SUBDIVISION\']]]\n\n\nReference http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\nSection 4: List comprehensions and map method of Series can also be used to produce more complex criteria:\n\nWhat am I missing?\n'
'I need some guidance in working out how to plot a block of histograms from grouped data in a pandas dataframe. Here\'s an example to illustrate my question:\n\nfrom pandas import DataFrame\nimport numpy as np\nx = [\'A\']*300 + [\'B\']*400 + [\'C\']*300\ny = np.random.randn(1000)\ndf = DataFrame({\'Letter\':x, \'N\':y})\ngrouped = df.groupby(\'Letter\')\n\n\nIn my ignorance I tried this code command:\n\ndf.groupby(\'Letter\').hist()\n\n\nwhich failed with the error message "TypeError: cannot concatenate \'str\' and \'float\' objects"\n\nAny help most appreciated.\n'
"I have a DataFrame like this:\n\ndf:\n\n fruit    val1 val2\n0 orange    15    3\n1 apple     10   13\n2 mango     5    5 \n\n\nHow do I get Pandas to give me a cumulative sum and percentage column on only val1?\n\nDesired output:\n\ndf_with_cumsum:\n\n fruit    val1 val2   cum_sum    cum_perc\n0 orange    15    3    15          50.00\n1 apple     10   13    25          83.33\n2 mango     5    5     30          100.00\n\n\nI tried df.cumsum(), but it's giving me this error:\n\n\n  TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n"
"I'm trying to create N balanced random subsamples of my large unbalanced dataset. Is there a way to do this simply with scikit-learn / pandas or do I have to implement it myself? Any pointers to code that does this?\n\nThese subsamples should be random and can be overlapping as I feed each to separate classifier in a very large ensemble of classifiers.\n\nIn Weka there is tool called spreadsubsample, is there equivalent in sklearn? \nhttp://wiki.pentaho.com/display/DATAMINING/SpreadSubsample\n\n(I know about weighting but that's not what I'm looking for.)\n"
"I'm trying to create a csv with pandas, but when I export the data to csv it gives me an extra column\n\nd = {'one' : pd.Series([1., 2., 3.]),'two' : pd.Series([1., 2., 3., 4.])}\ndf0_fa = pd.DataFrame(d)\ndf_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w')\n\n\nThus, my result is: \n\n ,one,two\n0,1.0,1.0\n1,2.0,2.0\n2,3.0,3.0\n3,4.0,4.0\n\n\nBut, the expected results are:\n\none,two\n1.0,1.0\n2.0,2.0\n3.0,3.0\n4.0,4.0\n\n"
'I have fed the following CSV file into iPython Notebook:\n\npublic = pd.read_csv("categories.csv")\npublic\n\n\nI\'ve also imported pandas as pd, numpy as np and matplotlib.pyplot as plt.  The following data types are present (the below is a summary - there are about 100 columns)\n\nIn [36]:   public.dtypes\nOut[37]:   parks          object\n           playgrounds    object\n           sports         object\n           roading        object               \n           resident       int64\n           children       int64\n\n\nI want to change \'parks\', \'playgrounds\', \'sports\' and \'roading\' to categories (they have likert scale responses in them - each column has different types of likert responses though (e.g. one has "strongly agree", "agree" etc., another has "very important", "important" etc.), leaving the remainder as int64.  \n\nI was able to create a separate dataframe - public1 - and change one of the columns to a category type using the following code:\n\npublic1 = {\'parks\': public.parks}\npublic1 = public1[\'parks\'].astype(\'category\')\n\n\nHowever, when I tried to change a number at once using this code, I was unsuccessful:\n\npublic1 = {\'parks\': public.parks,\n           \'playgrounds\': public.parks}\npublic1 = public1[\'parks\', \'playgrounds\'].astype(\'category\')\n\n\nNotwithstanding this, I don\'t want to create a separate dataframe with just the categories columns.  I would like them changed in the original dataframe.\n\nI tried numerous ways to achieve this, then tried the code here: Pandas: change data type of columns...\n\npublic[[\'parks\', \'playgrounds\', \'sports\', \'roading\']] = public[[\'parks\', \'playgrounds\', \'sports\', \'roading\']].astype(\'category\')\n\n\nand got the following error:\n\n NotImplementedError: &gt; 1 ndim Categorical are not supported at this time\n\n\nIs there a way to change \'parks\', \'playgrounds\', \'sports\', \'roading\' to categories (so the likert scale responses can then be analysed), leaving \'resident\' and \'children\' (and the 94 other columns that are string, int + floats) untouched please?  Or, is there a better way to do this?  If anyone has any suggestions and/or feedback I would be most grateful....am slowly going bald ripping my hair out!\n\nMany thanks in advance.\n\nedited to add - I am using Python 2.7.\n'
'I have a simple stacked line plot that has exactly the date format I want magically set when using the following code.\n\ndf_ts = df.resample("W", how=\'max\')\ndf_ts.plot(figsize=(12,8), stacked=True)\n\n\n\n\nHowever, the dates mysteriously transform themselves to an ugly and unreadable format when plotting the same data as a bar plot.\n\ndf_ts = df.resample("W", how=\'max\')\ndf_ts.plot(kind=\'bar\', figsize=(12,8), stacked=True)\n\n\n\n\nThe original data was transformed a bit to have the weekly max. Why is this radical change in automatically set dates happening? How can I have the nicely formatted dates as above?\n\nHere is some dummy data\n\nstart = pd.to_datetime("1-1-2012")\nidx = pd.date_range(start, periods= 365).tolist()\ndf=pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\ndf_ts.plot(kind=\'bar\', stacked=True)\n\n'
'I have a pandas dataframe which contains duplicates values according to two columns (A and B):\n\nA B C\n1 2 1\n1 2 4\n2 7 1\n3 4 0\n3 4 8\n\n\nI want to remove duplicates keeping the row with max value in column C. This would lead to: \n\nA B C\n1 2 4\n2 7 1\n3 4 8\n\n\nI cannot figure out how to do that. Should I use drop_duplicates(), something else?\n'
"I am always bothered when I make a bar plot with pandas and I want to change the names of the labels in the legend. Consider for instance the output of this code:\n\nimport pandas as pd\nfrom matplotlib.pyplot import *\n\ndf = pd.DataFrame({'A':26, 'B':20}, index=['N'])\ndf.plot(kind='bar')\n\n\n\nNow, if I want to change the name in the legend, I would usually try to do:\n\nlegend(['AAA', 'BBB'])\n\n\nBut I end up with this:\n\n\n\nIn fact, the first dashed line seems to correspond to an additional patch. \n\nSo I wonder if there is a simple trick here to change the labels, or do I need to plot each of the columns independently with matplotlib and set the labels myself. Thanks.\n"
'I have a pandas column of Timestamp data\n\nIn [27]: train["Original_Quote_Date"][6] \nOut[27]: Timestamp(\'2013-12-25 00:00:00\')\n\n\nHow can check equivalence of these objects to datetime.date objects of the type\n\ndatetime.date(2013, 12, 25)\n\n'
'I have downloaded some datas as a sqlite database (data.db) and I want to open this database in python and then convert it into pandas dataframe.\n\nThis is so far I have done\n\nimport sqlite3\nimport pandas    \ndat = sqlite3.connect(\'data.db\') #connected to database with out error\npandas.DataFrame.from_records(dat, index=None, exclude=None, columns=None, coerce_float=False, nrows=None)\n\n\nBut its throwing this error\n\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py", line 980, in from_records\n    coerce_float=coerce_float)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py", line 5353, in _to_arrays\n    if not len(data):\nTypeError: object of type \'sqlite3.Connection\' has no len()\n\n\nHow to convert sqlite database to pandas dataframe\n'
'I am trying to find some way of appending multiple pandas data frames at once rather than appending them one by one using \n\ndf.append(df)\n\n\nLet us say there are 5 pandas data frames t1, t2, t3, t4, t5. How do I append them at once? Something equivalent of \n\ndf = rbind(t1,t2,t3,t4,t5)\n\n'
'I was just wondering if I can rename column names by their positions.\nI know how to rename them by their actual names using:\n\ndf.rename(columns = {}) \n\nHow do I do it if I do not know the column names and know only their positions?\n'
"Is there a faster way to find the length of the longest string in a Pandas DataFrame than what's shown in the example below?\n\nimport numpy as np\nimport pandas as pd\n\nx = ['ab', 'bcd', 'dfe', 'efghik']\nx = np.repeat(x, 1e7)\ndf = pd.DataFrame(x, columns=['col1'])\n\nprint df.col1.map(lambda x: len(x)).max()\n# result --&gt; 6\n\n\nIt takes about 10 seconds to run df.col1.map(lambda x: len(x)).max() when timing it with IPython's %timeit.\n"
'Are there any examples of how to pass parameters with an SQL query in Pandas?\n\nIn particular I\'m using an SQLAlchemy engine to connect to a PostgreSQL database.  So far I\'ve found that the following works:\n\ndf = psql.read_sql((\'select "Timestamp","Value" from "MyTable" \'\n                     \'where "Timestamp" BETWEEN %s AND %s\'),\n                   db,params=[datetime(2014,6,24,16,0),datetime(2014,6,24,17,0)],\n                   index_col=[\'Timestamp\'])\n\n\nThe Pandas documentation says that params can also be passed as a dict, but I can\'t seem to get this to work having tried for instance:\n\ndf = psql.read_sql((\'select "Timestamp","Value" from "MyTable" \'\n                     \'where "Timestamp" BETWEEN :dstart AND :dfinish\'),\n                   db,params={"dstart":datetime(2014,6,24,16,0),"dfinish":datetime(2014,6,24,17,0)},\n                   index_col=[\'Timestamp\'])\n\n\nWhat is the recommended way of running these types of queries from Pandas?\n'
'I\'m trying to inner join DataFrame A to DataFrame B and am running into an error.\n\nHere\'s my join statement:\n\nmerged = DataFrameA.join(DataFrameB, on=[\'Code\',\'Date\'])\n\n\nAnd here\'s the error:\n\nValueError: len(left_on) must equal the number of levels in the index of "right"\n\n\nI\'m not sure the column order matters (they aren\'t truly "ordered" are they?), but just in case, the DataFrames are organized like this:\n\nDataFrameA:  Code, Date, ColA, ColB, ColC, ..., ColG, ColH (shape: 80514, 8 - no index)\nDataFrameB:  Date, Code, Col1, Col2, Col3, ..., Col15, Col16 (shape: 859, 16 - no index)\n\n\nDo I need to correct my join statement?  Or is there another, better way to get the intersection (or inner join) of these two DataFrames?\n'
"I am confused how pandas blew out of bounds for datetime objects with these lines:\n\nimport pandas as pd\nBOMoffset = pd.tseries.offsets.MonthBegin()\n# here some code sets the all_treatments dataframe and the newrowix, micolix, mocolix counters\nall_treatments.iloc[newrowix,micolix] = BOMoffset.rollforward(all_treatments.iloc[i,micolix] + pd.tseries.offsets.DateOffset(months = x))\nall_treatments.iloc[newrowix,mocolix] = BOMoffset.rollforward(all_treatments.iloc[newrowix,micolix]+ pd.tseries.offsets.DateOffset(months = 1))\n\n\nHere all_treatments.iloc[i,micolix] is a datetime set by pd.to_datetime(all_treatments['INDATUMA'], errors='coerce',format='%Y%m%d'), and INDATUMA is date information in the format 20070125.\n\nThis logic seems to work on mock data (no errors, dates make sense), so at the moment I cannot reproduce while it fails in my entire data with the following error:\n\npandas.tslib.OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2262-05-01 00:00:00\n\n"
"I'm looking for a method that behaves similarly to coalesce in T-SQL. I have 2 columns (column A and B) that are sparsely populated in a pandas dataframe. I'd like to create a new column using the following rules:\n\n\nIf the value in column A is not null, use that value for the new column C\nIf the value in column A is null, use the value in column B for the new column C\n\n\nLike I mentioned, this can be accomplished in MS SQL Server via the coalesce function. I haven't found a good pythonic method for this; does one exist?\n"
"So I have a dataframe, df1, that looks like the following:\n\n       A      B      C\n1     foo    12    California\n2     foo    22    California\n3     bar    8     Rhode Island\n4     bar    32    Rhode Island\n5     baz    15    Ohio\n6     baz    26    Ohio\n\n\nI want to group by column A and then sum column B while keeping the value in column C. Something like this:\n\n      A       B      C\n1    foo     34    California\n2    bar     40    Rhode Island\n3    baz     41    Ohio\n\n\nThe issue is, when I say df.groupby('A').sum() column C gets removed returning\n\n      B\nA\nbar  40\nbaz  41\nfoo  34\n\n\nHow can I get around this and keep column C when I group and sum?\n"
'I\'m trying to test if one of my variables is pd.NaT. I know it is NaT, and still it won\'t pass the test. As an example, the following code prints nothing :\n\na=pd.NaT\n\nif a == pd.NaT:\n    print("a not NaT")\n\n\nDoes anyone have a clue ? Is there a way to effectively test if a is NaT?\n'
"I have two series s1 and s2 in pandas and want to compute the intersection i.e. where all of the values of the series are common.\n\nHow would I use the concat function to do this? I have been trying to work it out but have been unable to (I don't want to compute the intersection on the indices of s1 and s2, but on the values).\n"
"I've had success using the groupby function to sum or average a given variable by groups, but is there a way to aggregate into a list of values, rather than to get a single result? (And would this still be called aggregation?) \n\nI am not entirely sure this is the approach I should be taking anyhow, so below is an example of the transformation I'd like to make, with toy data. \n\nThat is, if the data look something like this:\n\n    A    B    C  \n    1    10   22\n    1    12   20\n    1    11   8\n    1    10   10\n    2    11   13\n    2    12   10 \n    3    14   0\n\n\nWhat I am trying to end up with is something like the following. I am not totally sure whether this can be done through groupby aggregating into lists, and am rather lost as to where to go from here. \n\nHypothetical output:\n\n     A    B    C  New1  New2  New3  New4  New5  New6\n    1    10   22  12    20    11    8     10    10\n    2    11   13  12    10 \n    3    14   0\n\n\nPerhaps I should be pursuing pivots instead? The order by which the data are put into columns does not matter - all columns B through New6 in this example are equivalent. All suggestions/corrections are much appreciated.\n"
"Given a dataframe with different categorical variables, how do I return a cross-tabulation with percentages instead of frequencies?\n\ndf = pd.DataFrame({'A' : ['one', 'one', 'two', 'three'] * 6,\n                   'B' : ['A', 'B', 'C'] * 8,\n                   'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 4,\n                   'D' : np.random.randn(24),\n                   'E' : np.random.randn(24)})\n\n\npd.crosstab(df.A,df.B)\n\n\nB       A    B    C\nA               \none     4    4    4\nthree   2    2    2\ntwo     2    2    2\n\n\nUsing the margins option in crosstab to compute row and column totals gets us close enough to think that it should be possible using an aggfunc or groupby, but my meager brain can't think it through.\n\nB       A     B    C\nA               \none     .33  .33  .33\nthree   .33  .33  .33\ntwo     .33  .33  .33\n\n"
"I am trying to create a stacked bar graph that replicates the picture, all my data is separate from that excel spreadsheet.\n\n\n\nI cant figure out how to make a dataframe for it like pictured, nor can I figure out how to make the stacked bar chart. All examples I locate work in different ways to what I'm trying to create.\n\nMy dataframe is a csv of all values narrowed down to the following with a pandas dataframe.\n\n      Site Name    Abuse/NFF\n0    NORTH ACTON       ABUSE\n1    WASHINGTON         -\n2    WASHINGTON        NFF\n3    BELFAST            -\n4    CROYDON            - \n\n\nI have managed to count the data with totals and get individual counts for each site, I just cant seem to combine it in a way to graph.\n\nWould really appreciate some strong guidance.\n\nCompleted code, many thanks for the assistance completing.\n\ntest5 = faultdf.groupby(['Site Name', 'Abuse/NFF'])['Site Name'].count().unstack('Abuse/NFF').fillna(0)\n\ntest5.plot(kind='bar', stacked=True)\n\n"
"I have a question similar to this and this. The difference is that I have to select row by position, as I do not know the index. \n\nI want to do something like df.iloc[0, 'COL_NAME'] = x, but iloc does not allow this kind of access. If I do df.iloc[0]['COL_NAME] = x the warning about chained indexing appears.\n"
'I have the following dataframe:\n\n   a  b   x  y\n0  1  2   3 -1\n1  2  4   6 -2\n2  3  6   9 -3\n3  4  8  12 -4\n\n\nHow can I move columns b and x such that they are the last 2 columns in the dataframe? I would like to specify b and x by name, but not the other columns.\n'
"I have the following dataframe:\n\ndf = pd.DataFrame([\n    (1, 1, 'term1'),\n    (1, 2, 'term2'),\n    (1, 1, 'term1'),\n    (1, 1, 'term2'),\n    (2, 2, 'term3'),\n    (2, 3, 'term1'),\n    (2, 2, 'term1')\n], columns=['id', 'group', 'term'])\n\n\nI want to group it by id and group and calculate the number of each term for this id, group pair.\n\nSo in the end I am going to get something like this:\n\n\n\nI was able to achieve what I want by looping over all the rows with df.iterrows() and creating a new dataframe, but this is clearly inefficient. (If it helps, I know the list of all terms beforehand and there are ~10 of them).\n\nIt looks like I have to group by and then count values, so I tried that with df.groupby(['id', 'group']).value_counts() which does not work because value_counts operates on the groupby series and not a dataframe.\n\nAnyway I can achieve this without looping?\n"
"I would like to perform arithmetic on one or more dataframes columns using pd.eval. Specifically, I would like to port the following code that evaluates a formula:\nx = 5\ndf2['D'] = df1['A'] + (df1['B'] * x) \n\n...to code using pd.eval. The reason for using pd.eval is that I would like to automate many workflows, so creating them dynamically will be useful to me.\nMy two input DataFrames are:\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\ndf1 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\n\ndf1\n   A  B  C  D\n0  5  0  3  3\n1  7  9  3  5\n2  2  4  7  6\n3  8  8  1  6\n4  7  7  8  1\n\ndf2\n   A  B  C  D\n0  5  9  8  9\n1  4  3  0  3\n2  5  0  2  3\n3  8  1  3  3\n4  3  7  0  1\n\nI am trying to better understand pd.eval's engine and parser arguments to determine how best to solve my problem. I have gone through the documentation but the difference was not made clear to me.\n\nWhat arguments should be used to ensure my code is working at max performance?\nIs there a way to assign the result of the expression back to df2?\nAlso, to make things more complicated, how do I pass x as an argument inside the string expression?\n\n"
'I have a dataFrame in pandas and several of the columns have all null values. Is there a built in function which will let me remove those columns?\n'
'I have a problem with some groupy code which I\'m quite sure once ran (on an older pandas version). On 0.9, I get No numeric types to aggregate errors. Any ideas?\n\nIn [31]: data\nOut[31]: \n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00\nFreq: &lt;1 DateOffset&gt;\nColumns: 360 entries, -89.75 to 89.75\ndtypes: object(360)\n\nIn [32]: latedges = linspace(-90., 90., 73)\n\nIn [33]: lats_new = linspace(-87.5, 87.5, 72)\n\nIn [34]: def _get_gridbox_label(x, bins, labels):\n   ....:             return labels[searchsorted(bins, x) - 1]\n   ....: \n\nIn [35]: lat_bucket = lambda x: _get_gridbox_label(x, latedges, lats_new)\n\nIn [36]: data.T.groupby(lat_bucket).mean()\n---------------------------------------------------------------------------\nDataError                                 Traceback (most recent call last)\n&lt;ipython-input-36-ed9c538ac526&gt; in &lt;module&gt;()\n----&gt; 1 data.T.groupby(lat_bucket).mean()\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in mean(self)\n    295         """\n    296         try:\n--&gt; 297             return self._cython_agg_general(\'mean\')\n    298         except DataError:\n    299             raise\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_general(self, how, numeric_only)\n   1415 \n   1416     def _cython_agg_general(self, how, numeric_only=True):\n-&gt; 1417         new_blocks = self._cython_agg_blocks(how, numeric_only=numeric_only)\n   1418         return self._wrap_agged_blocks(new_blocks)\n   1419 \n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.py in _cython_agg_blocks(self, how, numeric_only)\n   1455 \n   1456         if len(new_blocks) == 0:\n-&gt; 1457             raise DataError(\'No numeric types to aggregate\')\n   1458 \n   1459         return new_blocks\n\nDataError: No numeric types to aggregate\n\n'
"Given the following dataframe\n\nIn [31]: rand = np.random.RandomState(1)\n         df = pd.DataFrame({'A': ['foo', 'bar', 'baz'] * 2,\n                            'B': rand.randn(6),\n                            'C': rand.rand(6) &gt; .5})\n\nIn [32]: df\nOut[32]:      A         B      C\n         0  foo  1.624345  False\n         1  bar -0.611756   True\n         2  baz -0.528172  False\n         3  foo -1.072969   True\n         4  bar  0.865408  False\n         5  baz -2.301539   True \n\n\nI would like to sort it in groups (A) by the aggregated sum of B, and then by the value in C (not aggregated). So basically get the order of the A groups with\n\nIn [28]: df.groupby('A').sum().sort('B')\nOut[28]:             B  C\n         A               \n         baz -2.829710  1\n         bar  0.253651  1\n         foo  0.551377  1\n\n\nAnd then by True/False, so that it ultimately looks like this:\n\nIn [30]: df.ix[[5, 2, 1, 4, 3, 0]]\nOut[30]: A         B      C\n    5  baz -2.301539   True\n    2  baz -0.528172  False\n    1  bar -0.611756   True\n    4  bar  0.865408  False\n    3  foo -1.072969   True\n    0  foo  1.624345  False\n\n\nHow can this be done?                         \n"
'I have some hierarchical data which bottoms out into time series data which looks something like this:\n\ndf = pandas.DataFrame(\n    {\'value_a\': values_a, \'value_b\': values_b},\n    index=[states, cities, dates])\ndf.index.names = [\'State\', \'City\', \'Date\']\ndf\n\n                               value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        0       10\n                   2012-01-02        1       11\n                   2012-01-03        2       12\n                   2012-01-04        3       13\n        Savanna    2012-01-01        4       14\n                   2012-01-02        5       15\n                   2012-01-03        6       16\n                   2012-01-04        7       17\nAlabama Mobile     2012-01-01        8       18\n                   2012-01-02        9       19\n                   2012-01-03       10       20\n                   2012-01-04       11       21\n        Montgomery 2012-01-01       12       22\n                   2012-01-02       13       23\n                   2012-01-03       14       24\n                   2012-01-04       15       25\n\n\nI\'d like to perform time resampling per city, so something like\n\ndf.resample("2D", how="sum")\n\n\nwould output\n\n                             value_a  value_b\nState   City       Date                        \nGeorgia Atlanta    2012-01-01        1       21\n                   2012-01-03        5       25\n        Savanna    2012-01-01        9       29\n                   2012-01-03       13       33\nAlabama Mobile     2012-01-01       17       37\n                   2012-01-03       21       41\n        Montgomery 2012-01-01       25       45\n                   2012-01-03       29       49\n\n\nas is, df.resample(\'2D\', how=\'sum\') gets me\n\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n\n\nFair enough, but I\'d sort of expect this to work:\n\n&gt;&gt;&gt; df.swaplevel(\'Date\', \'State\').resample(\'2D\', how=\'sum\')\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n\n\nat which point I\'m really running out of ideas... is there some way stack and unstack might be able to help me?\n'
"Say I have a dataframe df with a column value holding some float values and some NaN. How can I get the part of the dataframe where we have NaN using the query syntax?\n\nThe following, for example, does not work:\n\ndf.query( '(value &lt; 10) or (value == NaN)' )\n\n\nI get name NaN is not defined (same for df.query('value ==NaN'))\n\nGenerally speaking, is there any way to use numpy names in query, such as inf, nan, pi, e, etc.?\n"
'I have a pandas dataframe, df.\n\nI want to select all indices in df that are not in a list, blacklist.\n\nNow, I use list comprehension to create the desired labels to slice.\n\nix=[i for i in df.index if i not in blacklist]  \ndf_select=df.loc[ix]\n\n\nWorks fine, but may be clumsy if I need to do this often.\n\nIs there a better way to do this?\n'
"For example, I have DataFrame now as\nid score1   score2  score3  score4  score5\n1  0.000000     0.108659    0.000000    0.078597    1\n2  0.053238     0.308253    0.286353    0.446433    1\n3  0.000000     0.083979    0.808983    0.233052    1\n\nI want to convert it as\nid scoreDict\n1  {'1': 0, '2': 0.1086, ...}\n2  {...}\n3  {...}\n\nAnyway to do that?\n"
"This is a very basic question, I just can not seem to find an answer.\n\nI have a dataframe like this, called df:\n\n  A     B     C\n a.1   b.1   c.1\n a.2   b.2   c.2\n a.3   b.3   c.3\n\n\nThen I extract all the rows from df, where column 'B' has a value of 'b.2'.  I assign these results to df_2.  \n\ndf_2 = df[df['B'] == 'b.2']\n\n\ndf_2 becomes:\n\n  A     B     C\n a.2   b.2   c.2\n\n\nThen, I copy all the values in column 'B' to a new column named 'D'. Causing df_2 to become: \n\n  A     B     C     D\n a.2   b.2   c.2   b.2\n\n\nWhen I preform an assignment like this:\n\ndf_2['D'] = df_2['B']\n\n\nI get the following warning:\n\n\n  A value is trying to be set on a copy of a slice from a DataFrame. Try\n  using .loc[row_indexer,col_indexer] = value instead\n  \n  See the the caveats in the documentation:\n  http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\n\n\n\nI have also tried using .loc when creating df_2 like this:\n\ndf_2 = df.loc[df['B'] == 'b.2']\n\n\nHowever, I still get the warning. \n\nAny help is greatly appreciated.\n"
'I have a pandas dataframe: data. it has columns ["name", \'A\', \'B\'] \n\nWhat I want to do (and works) is:\n\nd2 = data[data[\'name\'] == \'fred\'] #This gives me multiple rows\nd2[\'A\'] = 0\n\n\nThis will set the column A on the fred rows to 0. \nI\'ve also done:\n\nindexes = d2.index\ndata[\'A\'][indexes] = 0\n\n\nHowever, both give me the same warning: \n\n/Users/brianp/work/cyan/venv/lib/python2.7/site-packages/pandas/core/indexing.py:128: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\n\nHow does pandas WANT me to do this?\n'
'If I do \n\nmt = mobile.PattLen.value_counts()   # sort True by default\n\n\nI get\n\n4    2831\n3    2555 \n5    1561\n[...]\n\n\nIf I do\n\nmt = mobile.PattLen.value_counts(sort=False) \n\n\nI get\n\n8    225\n9    120\n2   1234 \n[...]\n\n\nWhat I am trying to do is get the output in 2, 3, 4 ascending order (the left numeric column).  Can I change value_counts somehow or do I need to use a different function. \n'
"I have a function, which returns a dictionary like this:\n\n{'truth': 185.179993, 'day1': 197.22307753038834, 'day2': 197.26118010160317, 'day3': 197.19846975345905, 'day4': 197.1490578795196, 'day5': 197.37179265011116}\n\n\nI am trying to append this dictionary to a dataframe like so:\n\noutput = pd.DataFrame()\noutput.append(dictionary, ignore_index=True)\nprint(output.head())\n\n\nUnfortunately, the printing of the dataframe results in an empty dataframe. Any ideas?\n"
"When I run this code\n\nimport pandas as pd\nimport numpy as np\ndef add_prop(group):\n    births = group.births.astype(float)\n    group['prop'] = births/births.sum()\n    return group\n\npieces = []\ncolumns = ['name', 'sex', 'births']\n\nfor year in range(1880, 2012):\n    path = 'yob%d.txt' % year\n    frame = pd.read_csv(path, names = columns)\n    frame['year'] = year\n    pieces.append(frame)\n    names = pd.concat(pieces, ignore_index = True)\n\ntotal_births = names.pivot_table('births', rows = 'year', cols = 'sex', aggfunc = sum)\ntotal_births.plot(title = 'Total Births by sex and year')\n\n\nI get no plot.  This is from Wes McKinney's book on using Python for data analysis.\nCan anyone point me in the right direction?\n"
'Q1) I want to do a groupby, SQL-style aggregation and rename the output column:\n\nExample dataset:\n\n&gt;&gt;&gt; df\n    ID     Region  count\n0  100       Asia      2\n1  101     Europe      3\n2  102         US      1\n3  103     Africa      5\n4  100     Russia      5\n5  101  Australia      7\n6  102         US      8\n7  104       Asia     10\n8  105     Europe     11\n9  110     Africa     23\n\n\nI want to group the observations of this dataset by ID and Region and summing the count for each group. So I used something like this...\n\n&gt;&gt;&gt; print(df.groupby([\'ID\',\'Region\'],as_index=False).count().sum())\n\n    ID     Region  count\n0  100       Asia      2\n1  100     Russia      5\n2  101  Australia      7\n3  101     Europe      3\n4  102         US      9\n5  103     Africa      5\n6  104       Asia     10\n7  105     Europe     11\n8  110     Africa     23\n\n\nOn using as_index=False I am able to get "SQL-Like" output. My problem is that I am unable to rename the aggregate variable count here. So in SQL if wanted to do the above thing I would do something like this:\n\nselect ID, Region, sum(count) as Total_Numbers\nfrom df\ngroup by ID, Region\norder by ID, Region\n\n\nAs we see, it\'s very easy for me to rename the aggregate variable count to Total_Numbers in SQL. I wanted to do the same thing in Pandas but unable to find such an option in group-by function. Can somebody help?\n\nThe second question (more of an observation) is whether...\n\nQ2) Is it possible to directly use column names in Pandas dataframe functions without enclosing them in quotes?\n\nI understand that the variable names are strings, so have to be inside quotes, but I see if use them outside dataframe function and as an attribute we don\'t require them to be inside quotes. Like df.ID.sum() etc. It\'s only when we use it in a DataFrame function like df.sort() or df.groupby we have to use it inside quotes. This is actually a bit of pain as in SQL or in SAS or other languages we simply use the variable name without quoting them. Any suggestion on this? \n\nKindly reply to both questions (Q1 is the main, Q2 more of an opinion).\n'
'I uploaded a file to Google spreadsheets (to make a publically accessible example IPython Notebook, with data) I was using the file in it\'s native form could be read into a Pandas Dataframe. So now I use the following code to read the spreadsheet, works fine but just comes in as string,, and I\'m not having any luck trying to get it back into a dataframe (you can get the data)\n\nimport requests\nr = requests.get(\'https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&amp;output=csv\')\ndata = r.content\n\n\nThe data ends up looking like: (1st row headers)\n\n\',City,region,Res_Comm,mkt_type,Quradate,National_exp,Alabama_exp,Sales_exp,Inventory_exp,Price_exp,Credit_exp\\n0,Dothan,South_Central-Montgomery-Auburn-Wiregrass-Dothan,Residential,Rural,1/15/2010,2,2,3,2,3,3\\n10,Foley,South_Mobile-Baldwin,Residential,Suburban_Urban,1/15/2010,4,4,4,4,4,3\\n12,Birmingham,North_Central-Birmingham-Tuscaloosa-Anniston,Commercial,Suburban_Urban,1/15/2010,2,2,3,2,2,3\\n\n\n\nThe native pandas code that brings in the disk resident file looks like:\n\ndf = pd.io.parsers.read_csv(\'/home/tom/Dropbox/Projects/annonallanswerswithmaster1012013.csv\',index_col=0,parse_dates=[\'Quradate\'])\n\n\nA "clean" solution would be helpful to many to provide an easy way to share datasets for Pandas use! I tried a bunch of alternative with no success and I\'m pretty sure I\'m missing something obvious again.\n\nJust a Update note The new Google spreadsheet has a different URL pattern Just use this in place of the URL in the above example and or the below answer and you should be fine here is an example: \n\nhttps://docs.google.com/spreadsheets/d/177_dFZ0i-duGxLiyg6tnwNDKruAYE-_Dd8vAQziipJQ/export?format=csv&amp;id\n\n\nsee solution below from @Max Ghenis which just used pd.read_csv, no need for StringIO or requests...\n'
"Given a dictionary of data frames like:\n\ndict = {'ABC': df1, 'XYZ' : df2}   # of any length...\n\n\nwhere each data frame has the same columns and similar index, for example:\n\ndata           Open     High      Low    Close   Volume\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149\n\n\nWhat is the simplest way to combine all the data frames into one, with a multi-index like:\n\nsymbol         ABC                                       XYZ\ndata           Open     High      Low    Close   Volume  Open ...\nDate                                                   \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  ...\n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  ...\n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  ...\n\n\nI've tried a few methods - eg for each data frame replace the columns with a multi-index like .from_product(['ABC', columns]) and then concatenate along axis=1, without success.\n"
'It is quite easy to add many pandas dataframes into excel work book as long as it is different worksheets. But, it is somewhat tricky to get many dataframes into one worksheet if you want to use pandas built-in df.to_excel functionality. \n\n# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter(\'test.xlsx\',engine=\'xlsxwriter\')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet(\'Validation\') \ndf.to_excel(writer,sheet_name=\'Validation\',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name=\'Validation\',startrow=20, startcol=0) \n\n\nThe above code won\'t work. You will get the error of \n\n Sheetname \'Validation\', with case ignored, is already in use.\n\n\nNow, I have experimented enough that I found a way to make it work. \n\nwriter = pd.ExcelWriter(\'test.xlsx\',engine=\'xlsxwriter\')   # Creating Excel Writer Object from Pandas  \nworkbook=writer.book\ndf.to_excel(writer,sheet_name=\'Validation\',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name=\'Validation\',startrow=20, startcol=0) \n\n\nThis will work. So, my purpose of posting this question on stackoverflow is twofold. Firstly, I hope this will help someone if he/she is trying to put many dataframes into a single work sheet at excel. \n\nSecondly,  Can someone help me understand the difference between those two blocks of code? It appears to me that they are pretty much the same except the first block of code created worksheet called "Validation" in advance while the second does not. I get that part. \n\nWhat I don\'t understand is why should it be any different ? Even if I don\'t create the worksheet in advance, this line, the line right before the last one, \n\n df.to_excel(writer,sheet_name=\'Validation\',startrow=0 , startcol=0)  \n\n\nwill create a worksheet anyway. Consequently, by the time we reached the last line of code the worksheet "Validation" is already created as well in the second block of code.  So, my question basically, why should the second block of code  work while the first doesn\'t?  \n\nPlease also share if there is another way to put many dataframes into excel using the built-in df.to_excel functionality !! \n'
'I\'ve got a dataframe df_a with id information:\n\n    unique_id lacet_number \n15    5570613  TLA-0138365 \n24    5025490  EMP-0138757 \n36    4354431  DXN-0025343 \n\n\nand another dataframe df_b, with the same number of rows that I know correspond to the rows in df_a:\n\n     latitude  longitude \n0  -93.193560  31.217029  \n1  -93.948082  35.360874  \n2 -103.131508  37.787609  \n\n\nWhat I want to do is simply cbind the two and get:\n\n    unique_id lacet_number      latitude  longitude \n0     5570613  TLA-0138365    -93.193560  31.217029  \n1     5025490  EMP-0138757    -93.948082  35.360874  \n2     4354431  DXN-0025343   -103.131508  37.787609  \n\n\nWhat I have tried:\n\ndf_c = pd.concat([df_a, df_b], axis=1)\n\n\nwhich gives me an outer join. \n\n    unique_id lacet_number    latitude  longitude\n0         NaN          NaN  -93.193560  31.217029\n1         NaN          NaN  -93.948082  35.360874\n2         NaN          NaN -103.131508  37.787609\n15    5570613  TLA-0138365         NaN        NaN\n24    5025490  EMP-0138757         NaN        NaN\n36    4354431  DXN-0025343         NaN        NaN\n\n\nThe problem is that the indices for the two dataframes do not match. I read the documentation for pandas.concat, and saw that there is an option "ignore_index". But that only applies to the concatenation axis, in my case the columns and it certainly is not the right choice for me. So my question is: is there a simple way to achieve this?\n'
"I've got a 'DataFrame` which has occasional missing values, and looks something like this:\n\n          Monday         Tuesday         Wednesday \n      ================================================\nMike        42             NaN               12\nJenna       NaN            NaN               15\nJon         21              4                 1\n\n\nI'd like to add a new column to my data frame where I'd calculate the average across all columns for every row.\n\nMeaning, for Mike, I'd need \n(df['Monday'] + df['Wednesday'])/2, but for Jenna, I'd simply use df['Wednesday amt.']/1\n\nDoes anyone know the best way to account for this variation that results from missing values and calculate the average?\n"
"np.where has the semantics of a vectorized if/else (similar to Apache Spark's when/otherwise DataFrame method). I know that I can use np.where on pandas Series, but pandas often defines its own API to use instead of raw numpy functions, which is usually more convenient with pd.Series/pd.DataFrame.\n\nSure enough, I found pandas.DataFrame.where. However, at first glance, it has a completely different semantics. I could not find a way to rewrite the most basic example of np.where using pandas where:\n\n# df is pd.DataFrame\n# how to write this using df.where?\ndf['C'] = np.where((df['A']&lt;0) | (df['B']&gt;0), df['A']+df['B'], df['A']/df['B'])\n\n\nAm I missing something obvious? Or is pandas where intended for a completely different use case, despite same name as np.where? \n"
'I have the following file named \'data.csv\':\n\n    1997,Ford,E350\n    1997, Ford , E350\n    1997,Ford,E350,"Super, luxurious truck"\n    1997,Ford,E350,"Super ""luxurious"" truck"\n    1997,Ford,E350," Super luxurious truck "\n    "1997",Ford,E350\n    1997,Ford,E350\n    2000,Mercury,Cougar\n\n\nAnd I would like to parse it into a pandas DataFrame so that the DataFrame looks as follows:\n\n       Year     Make   Model              Description\n    0  1997     Ford    E350                     None\n    1  1997     Ford    E350                     None\n    2  1997     Ford    E350   Super, luxurious truck\n    3  1997     Ford    E350  Super "luxurious" truck\n    4  1997     Ford    E350    Super luxurious truck\n    5  1997     Ford    E350                     None\n    6  1997     Ford    E350                     None\n    7  2000  Mercury  Cougar                     None\n\n\nThe best I could do was:\n\n    pd.read_table("data.csv", sep=r\',\', names=["Year", "Make", "Model", "Description"])\n\n\nWhich gets me:\n\n    Year     Make   Model              Description\n 0  1997     Ford    E350                     None\n 1  1997    Ford     E350                     None\n 2  1997     Ford    E350   Super, luxurious truck\n 3  1997     Ford    E350  Super "luxurious" truck\n 4  1997     Ford    E350   Super luxurious truck \n 5  1997     Ford    E350                     None\n 6  1997     Ford    E350                     None\n 7  2000  Mercury  Cougar                     None\n\n\nHow can I get the DataFrame without those whitespaces?\n'
'I\'m having trouble getting the pandas dataframe.to_csv(...) output quoting strings right.\n\nimport pandas as pd\n\ntext = \'this is "out text"\'\ndf = pd.DataFrame(index=[\'1\'],columns=[\'1\',\'2\'])\ndf.loc[\'1\',\'1\']=123\ndf.loc[\'1\',\'2\']=text\ndf.to_csv(\'foo.txt\',index=False,header=False)\n\n\nThe output is:\n\n\n  123,"this is ""out text"""\n\n\nBut I would like:\n\n\n  123,this is "out text"\n\n\nDoes anyone know how to get this right? \n'
"I have data in long format and am trying to reshape to wide, but there doesn't seem to be a straightforward way to do this using melt/stack/unstack:\n\nSalesman  Height   product      price\n  Knut      6        bat          5\n  Knut      6        ball         1\n  Knut      6        wand         3\n  Steve     5        pen          2\n\n\nBecomes:\n\nSalesman  Height    product_1  price_1  product_2 price_2 product_3 price_3  \n  Knut      6        bat          5       ball      1        wand      3\n  Steve     5        pen          2        NA       NA        NA       NA\n\n\nI think Stata can do something like this with the reshape command.\n"
'A pandas DataFrame column duration contains timedelta64[ns] as shown. How can you convert them to seconds?\n\n0   00:20:32\n1   00:23:10\n2   00:24:55\n3   00:13:17\n4   00:18:52\nName: duration, dtype: timedelta64[ns]\n\n\nI tried the following\n\nprint df[:5][\'duration\'] / np.timedelta64(1, \'s\')\n\n\nbut got the error\n\nTraceback (most recent call last):\n  File "test.py", line 16, in &lt;module&gt;\n    print df[0:5][\'duration\'] / np.timedelta64(1, \'s\')\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py", line 130, in wrapper\n    "addition and subtraction, but the operator [%s] was passed" % name)\nTypeError: can only operate on a timedeltas for addition and subtraction, but the operator [__div__] was passed\n\n\nAlso tried\n\nprint df[:5][\'duration\'].astype(\'timedelta64[s]\')\n\n\nbut received the error\n\nTraceback (most recent call last):\n  File "test.py", line 17, in &lt;module&gt;\n    print df[:5][\'duration\'].astype(\'timedelta64[s]\')\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py", line 934, in astype\n    values = com._astype_nansafe(self.values, dtype)\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\common.py", line 1653, in _astype_nansafe\n    raise TypeError("cannot astype a timedelta from [%s] to [%s]" % (arr.dtype,dtype))\nTypeError: cannot astype a timedelta from [timedelta64[ns]] to [timedelta64[s]]\n\n'
"I've got a dataset with a big number of rows. Some of the values are NaN, like this:\n\nIn [91]: df\nOut[91]:\n 1    3      1      1      1\n 1    3      1      1      1\n 2    3      1      1      1\n 1    1    NaN    NaN    NaN\n 1    3      1      1      1\n 1    1      1      1      1\n\n\nAnd I want to count the number of NaN values in each string, it would be like this:\n\nIn [91]: list = &lt;somecode with df&gt;\nIn [92]: list\n    Out[91]:\n     [0,\n      0,\n      0,\n      3,\n      0,\n      0]\n\n\nWhat is the best and fastest way to do it?\n"
'I have a series within a DataFrame that I read in initially as an object, and then need to convert it to a date in the form of yyyy-mm-dd where dd is the end of the month.\n\nAs an example, I have DataFrame df with a column Date as an object:\n\n...      Date    ...\n...     200104   ...\n...     200508   ...\n\n\nWhat I want when this is all said and done is a date object:\n\n...      Date    ...\n...  2001-04-30  ...\n...  2005-08-31  ...\n\n\nsuch that df[\'Date\'].item() returns\n\ndatetime.date(2001, 04, 30)\n\n\nI\'ve used the following code to get almost there, but all my dates are at the beginning of the month, not the end. Please advise.\n\ndf[\'Date\'] = pd.to_datetime(df[\'Date\'], format="%Y%m").dt.date\n\n\nNote: I\'ve already imported Pandas ad pd, and datetime as dt\n'
'Given two dataframes df_1 and df_2, how to join them such that datetime column df_1 is in between start and end  in dataframe df_2:\n\nprint df_1\n\n  timestamp              A          B\n0 2016-05-14 10:54:33    0.020228   0.026572\n1 2016-05-14 10:54:34    0.057780   0.175499\n2 2016-05-14 10:54:35    0.098808   0.620986\n3 2016-05-14 10:54:36    0.158789   1.014819\n4 2016-05-14 10:54:39    0.038129   2.384590\n\n\nprint df_2\n\n  start                end                  event    \n0 2016-05-14 10:54:31  2016-05-14 10:54:33  E1\n1 2016-05-14 10:54:34  2016-05-14 10:54:37  E2\n2 2016-05-14 10:54:38  2016-05-14 10:54:42  E3\n\n\nGet corresponding event where df1.timestamp is between df_2.start and df2.end \n\n  timestamp              A          B          event\n0 2016-05-14 10:54:33    0.020228   0.026572   E1\n1 2016-05-14 10:54:34    0.057780   0.175499   E2\n2 2016-05-14 10:54:35    0.098808   0.620986   E2\n3 2016-05-14 10:54:36    0.158789   1.014819   E2\n4 2016-05-14 10:54:39    0.038129   2.384590   E3\n\n'
'I am working in a virtual environment. I am able to import and work in pandas without any error but when I am trying to import pandas_datareader \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom matplotlib import style\nimport pandas_datareader as web\n\n\nit is giving following error -\n\nTraceback (most recent call last):\n  File "stock.py", line 6, in &lt;module&gt;\n    import pandas_datareader as web\n  File "/home/xxxxx/django-apps/env/lib/python3.5/site-packages/pandas_datareader/__init__.py", line 2, in &lt;module&gt;\n    from .data import (DataReader, Options, get_components_yahoo,\n  File "/home/xxxxx/django-apps/env/lib/python3.5/site-packages/pandas_datareader/data.py", line 14, in &lt;module&gt;\n    from pandas_datareader.fred import FredReader\n  File "/home/xxxxx/django-apps/env/lib/python3.5/site-packages/pandas_datareader/fred.py", line 1, in &lt;module&gt;\n    from pandas.core.common import is_list_like\nImportError: cannot import name \'is_list_like\'\n(env) xxxxx@xxxxx-yyyyy ~/pyt $ python stock.py\nTraceback (most recent call last):\n  File "stock.py", line 6, in &lt;module&gt;\n    import pandas_datareader\n  File "/home/xxxxx/django-apps/env/lib/python3.5/site-packages/pandas_datareader/__init__.py", line 2, in &lt;module&gt;\n    from .data import (DataReader, Options, get_components_yahoo,\n  File "/home/xxxxx/django-apps/env/lib/python3.5/site-packages/pandas_datareader/data.py", line 14, in &lt;module&gt;\n    from pandas_datareader.fred import FredReader\n  File "/home/xxxxx/django-apps/env/lib/python3.5/site-packages/pandas_datareader/fred.py", line 1, in &lt;module&gt;\n    from pandas.core.common import is_list_like\nImportError: cannot import name \'is_list_like\'\n\n'
"In python, how can I reference previous row and calculate something against it?  Specifically, I am working with dataframes in pandas - I have a data frame full of stock price information that looks like this:\n\n           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n\n\nHere is how I created this dataframe:\n\nimport pandas\n\nurl = 'http://ichart.finance.yahoo.com/table.csv?s=IBM&amp;a=00&amp;b=1&amp;c=2011&amp;d=11&amp;e=31&amp;f=2011&amp;g=d&amp;ignore=.csv'\ndata = data = pandas.read_csv(url)\n\n## now I sorted the data frame ascending by date \ndata = data.sort(columns='Date')\n\n\nStarting with row number 2, or in this case, I guess it's 250 (PS - is that the index?), I want to calculate the difference between 2011-01-03 and 2011-01-04, for every entry in this dataframe.  I believe the appropriate way is to write a function that takes the current row, then figures out the previous row, and calculates the difference between them, the use the pandas apply function to update the dataframe with the value.  \n\nIs that the right approach?  If so, should I be using the index to determine the difference?  (note - I'm still in python beginner mode, so index may not be the right term, nor even the correct way to implement this)\n"
"In pandas, how can I convert a column of a DataFrame into dtype object?\nOr better yet, into a factor? (For those who speak R, in Python, how do I as.factor()?)\n\nAlso, what's the difference between pandas.Factor and pandas.Categorical?\n"
"I'm relatively new to using the PyCharm IDE, and have been unable to find a way to better shape the output when in a built-in console session. I'm typically working with pretty wide dataframes, that would fit easily across my monitor, but the display is cutting and wrapping them much sooner than needed.\n\nDoes anyone know of a setting to change this behavior to take advantage of the full width of my screen?\n\nEdit: I don't have enough reputation to post a screenshot, but link is below:\nhttp://imgur.com/iiBK3iU\n\nI would like to prevent it from wrapping after only a few columns (for example, the column 'ReadmitRate' should be immediately to the right of 'SNFDaysPerSNFCase')\n"
"I have a DataFrame df like the following (excerpt, 'Timestamp' are the index):\n\nTimestamp              Value\n2012-06-01 00:00:00     100\n2012-06-01 00:15:00     150\n2012-06-01 00:30:00     120\n2012-06-01 01:00:00     220\n2012-06-01 01:15:00      80\n...and so on.\n\n\nI need a new column df['weekday'] with the respective weekday/day-of-week of the timestamps.\n\nHow can I get this?\n"
'I would like to convert everything but the first column of a pandas dataframe into a numpy array. For some reason using the columns= parameter of DataFrame.to_matrix() is not working.\n\ndf:\n\n  viz  a1_count  a1_mean     a1_std\n0   n         3        2   0.816497\n1   n         0      NaN        NaN \n2   n         2       51  50.000000\n\n\nI tried X=df.as_matrix(columns=[df[1:]]) but this yields an array of all NaNs\n'
'I have a column I_DATE of type string(object) in a dataframe called train as show below.\n\nI_DATE\n28-03-2012  2:15:00 PM\n28-03-2012  2:17:28 PM\n28-03-2012  2:50:50 PM\n\n\nHow to convert I_DATE  from string to datatime format &amp; specify the format of input string. I saw some answers to this but its not for AM/PM format.\n\nAlso, how to filter rows based on a range of dates in pandas?\n'
'I have a pandas data frame like this:\n\n   Column1  Column2  Column3  Column4  Column5\n 0    a        1        2        3        4\n 1    a        3        4        5\n 2    b        6        7        8\n 3    c        7        7        \n\n\nWhat I want to do now is getting a new dataframe containing Column1 and a new columnA. This columnA should contain all values from columns 2 -(to) n (where n is the number of columns from Column2 to the end of the row) like this:\n\n  Column1  ColumnA\n0   a      1,2,3,4\n1   a      3,4,5\n2   b      6,7,8\n3   c      7,7\n\n\nHow could I best approach this issue? Any advice would be helpful. Thanks in advance!\n'
"I would like to use Pandas df.apply but only for certain rows\n\nAs an example, I want to do something like this, but my actual issue is a little more complicated:\n\nimport pandas as pd\nimport math\nz = pd.DataFrame({'a':[4.0,5.0,6.0,7.0,8.0],'b':[6.0,0,5.0,0,1.0]})\nz.where(z['b'] != 0, z['a'] / z['b'].apply(lambda l: math.log(l)), 0)\n\n\nWhat I want in this example is the value in 'a' divided by the log of the value in 'b' for each row, and for rows where 'b' is 0, I simply want to return 0.\n"
"I have a set of dataframes where one of the columns contains a categorical variable. I'd like to convert it to several dummy variables, in which case I'd normally use get_dummies.\n\nWhat happens is that get_dummies looks at the data available in each dataframe to find out how many categories there are, and thus create the appropriate number of dummy variables. However, in the problem I'm working right now, I actually know in advance what the possible categories are. But when looking at each dataframe individually, not all categories necessarily appear.\n\nMy question is: is there a way to pass to get_dummies (or an equivalent function) the names of the categories, so that, for the categories that don't appear in a given dataframe, it'd just create a column of 0s?\n\nSomething that would make this:\n\ncategories = ['a', 'b', 'c']\n\n   cat\n1   a\n2   b\n3   a\n\n\nBecome this:\n\n  cat_a  cat_b  cat_c\n1   1      0      0\n2   0      1      0\n3   1      0      0\n\n"
"let say I have a dataframe that looks like this:\n\ndf = pd.DataFrame(index=list('abcde'), data={'A': range(5), 'B': range(5)})\n df\nOut[92]: \n   A  B\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n\n\nAsumming that this dataframe already exist, how can I simply add a level 'C' to the column index so I get this:\n\n df\nOut[92]: \n   A  B\n   C  C\na  0  0\nb  1  1\nc  2  2\nd  3  3\ne  4  4\n\n\nI saw SO anwser like this python/pandas: how to combine two dataframes into one with hierarchical column index? but this concat different dataframe instead of adding a column level to an already existing dataframe.\n\n-\n"
"i have this dataframe:\n\n0 name data\n1 alex asd\n2 helen sdd\n3 alex dss\n4 helen sdsd\n5 john sdadd\n\n\nso i am trying to get the most frequent value or values(in this case its values)\nso what i do is:\n\ndataframe['name'].value_counts().idxmax()\n\n\nbut it returns only the value: Alex even if it Helen appears two times as well.\n"
"I'm using Pandas to read a bunch of CSVs. Passing an options json to dtype parameter to tell pandas which columns to read as string instead of the default:\n\ndtype_dic= { 'service_id':str, 'end_date':str, ... }\nfeedArray = pd.read_csv(feedfile , dtype = dtype_dic)\n\n\nIn my scenario, all the columns except a few specific ones are to be read as strings. So instead of defining several columns as str in dtype_dic, I'd like to set just my chosen few as int or float. Is there a way to do that?\n\nIt's a loop cycling through various CSVs with differing columns, so a direct column conversion after having read the whole csv as string (dtype=str), would not be easy as I would not immediately know which columns that csv is having. (I'd rather spend that effort in defining all the columns in the dtype json!) \n\nEdit: But if there's a way to process the list of column names to be converted to number without erroring out if that column isn't present in that csv, then yes that'll be a valid solution, if there's no other way to do this at csv reading stage itself.\n\nNote: this sounds like a previously asked question but the answers there went down a very different path (bool related) which doesn't apply to this question. Pls don't mark as duplicate!\n"
"I want to pass the numpy percentile() function through pandas' agg() function as I do below with various other numpy statistics functions.\n\nRight now I have a dataframe that looks like this:\n\nAGGREGATE   MY_COLUMN\nA           10\nA           12\nB           5\nB           9\nA           84\nB           22\n\n\nAnd my code looks like this:\n\ngrouped = dataframe.groupby('AGGREGATE')\ncolumn = grouped['MY_COLUMN']\ncolumn.agg([np.sum, np.mean, np.std, np.median, np.var, np.min, np.max])\n\n\nThe above code works, but I want to do something like \n\ncolumn.agg([np.sum, np.mean, np.percentile(50), np.percentile(95)])\n\n\ni.e. specify various percentiles to return from agg()\n\nHow should this be done?\n"
"I've got a script updating 5-10 columns worth of data , but sometimes the start csv will be identical to the end csv so instead of writing an identical csvfile I want it to do nothing... \n\nHow can I compare two dataframes to check if they're the same or not?\n\ncsvdata = pandas.read_csv('csvfile.csv')\ncsvdata_old = csvdata\n\n# ... do stuff with csvdata dataframe\n\nif csvdata_old != csvdata:\n    csvdata.to_csv('csvfile.csv', index=False)\n\n\nAny ideas?\n"
'I\'d like to know if there is a memory efficient way of reading multi record JSON file ( each line is a JSON dict) into a pandas dataframe. Below is a 2 line example with working solution, I need it for potentially very large number of records. Example use would be to process output from Hadoop Pig JSonStorage function.\n\nimport json\nimport pandas as pd\n\ntest=\'\'\'{"a":1,"b":2}\n{"a":3,"b":4}\'\'\'\n#df=pd.read_json(test,orient=\'records\') doesn\'t work, expects []\n\nl=[ json.loads(l) for l in test.splitlines()]\ndf=pd.DataFrame(l)\n\n'
'I have read some pricing data into a pandas dataframe the values appear as:\n\n$40,000*\n$40000 conditions attached\n\n\nI want to strip it down to just the numeric values.\nI know I can loop through and apply regex \n\n[0-9]+\n\n\nto each field then join the resulting list back together but is there a not loopy way?\n\nThanks\n'
"I have a pandas dataframe that has two datetime64 columns and one timedelta64 column that is the difference between the two columns. I'm trying to plot a histogram of the timedelta column to visualize the time differences between the two events.\n\nHowever, just using df['time_delta'] results in:\nTypeError: ufunc add cannot use operands with types dtype('&lt;m8[ns]') and dtype('float64')\n\nTrying to convert the timedelta column to : float--&gt; df2 = df1['time_delta'].astype(float) \nresults in:\nTypeError: cannot astype a timedelta from [timedelta64[ns]] to [float64]\n\nHow would one create a histogram of pandas timedelta data?\n"
'How can one idiomatically run a function like get_dummies, which expects a single column and returns several, on multiple DataFrame columns?\n'
"I am running 'describe()' on a dataframe and getting summaries of only int columns (pandas 14.0). \n\nThe documentation says that for object columns frequency of most common value, and additional statistics would be returned. What could be wrong? (no error message is returned by the way)\n\nEdit:\n\nI think it's how the function is set to behave on mixed column types in a dataframe. Although the documentation fails to mention it.\n\nExample code:\n\ndf_test = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\ndf_test.dtypes\ndf_test.describe()\ndf_test['$a'] = df_test['$a'].astype(str)\ndf_test.describe()\ndf_test['$a'].describe()\ndf_test['$b'].describe()\n\n\nMy ugly work around in the meanwhile:\n\ndef my_df_describe(df):\n    objects = []\n    numerics = []\n    for c in df:\n        if (df[c].dtype == object):\n            objects.append(c)\n        else:\n            numerics.append(c)\n\n    return df[numerics].describe(), df[objects].describe()\n\n"
'Is there a way to do this?  I cannot seem an easy way to interface pandas series with plotting a CDF.  \n'
"Not quite sure why I can't figure this out.  I'm looking to slice a Pandas dataframe by using index numbers.   I have a list/core index with the index numbers that i do NOT need, shown below\n\n pandas.core.index.Int64Index\n\n Int64Index([2340, 4840, 3163, 1597, 491 , 5010, 911 , 3085, 5486, 5475, 1417, 2663, 4204, 156 , 5058, 1990, 3200, 1218, 3280, 793 , 824 , 3625, 1726, 1971, 2845, 4668, 2973, 3039, 376 , 4394, 3749, 1610, 3892, 2527, 324 , 5245, 696 , 1239, 4601, 3219, 5138, 4832, 4762, 1256, 4437, 2475, 3732, 4063, 1193], dtype=int64)\n\n\nHow can I create a new dataframe excluding these index numbers.  I tried \n\ndf.iloc[combined_index]\n\n\nand obviously this just shows the rows with those index number (the opposite of what I want). any help will be greatly appreciated\n"
"Python pandas has a pct_change function which I use to calculate the returns for stock prices in a dataframe:\n\nndf['Return']= ndf['TypicalPrice'].pct_change()\n\n\nI am using the following code to get logarithmic returns, but it gives the exact same values as the pct.change() function:\n\nndf['retlog']=np.log(ndf['TypicalPrice'].astype('float64')/ndf['TypicalPrice'].astype('float64').shift(1))\n#np is for numpy\n\n"
'I have a dataframe ,\n\n    Out[78]: \n   contract month year  buys  adjusted_lots    price\n0         W     Z    5  Sell             -5   554.85\n1         C     Z    5  Sell             -3   424.50\n2         C     Z    5  Sell             -2   424.00\n3         C     Z    5  Sell             -2   423.75\n4         C     Z    5  Sell             -3   423.50\n5         C     Z    5  Sell             -2   425.50\n6         C     Z    5  Sell             -3   425.25\n7         C     Z    5  Sell             -2   426.00\n8         C     Z    5  Sell             -2   426.75\n9        CC     U    5   Buy              5  3328.00\n10       SB     V    5   Buy              5    11.65\n11       SB     V    5   Buy              5    11.64\n12       SB     V    5   Buy              2    11.60\n\n\nI need a sum of adjusted_lots   , price which is weighted average , of price and ajusted_lots , grouped by all the other columns , ie. grouped by (contract, month , year and buys)\n\nSimiliar solution on R was achieved by following code, using dplyr, however unable to do the same in pandas. \n\n&gt; newdf = df %&gt;%\n  select ( contract , month , year , buys , adjusted_lots , price ) %&gt;%\n  group_by( contract , month , year ,  buys) %&gt;%\n  summarise(qty = sum( adjusted_lots) , avgpx = weighted.mean(x = price , w = adjusted_lots) , comdty = "Comdty" )\n\n&gt; newdf\nSource: local data frame [4 x 6]\n\n  contract month year comdty qty     avgpx\n1        C     Z    5 Comdty -19  424.8289\n2       CC     U    5 Comdty   5 3328.0000\n3       SB     V    5 Comdty  12   11.6375\n4        W     Z    5 Comdty  -5  554.8500\n\n\nis the same possible by groupby or any other solution ?\n'
"In questions and answers, users very often post an example DataFrame which their question/answer works with:\n\nIn []: x\nOut[]: \n   bar  foo\n0    4    1\n1    5    2\n2    6    3\n\n\nIt'd be really useful to be able to get this DataFrame into my Python interpreter so I can start debugging the question, or testing the answer.\n\nHow can I do this?\n"
"I have a dictionary of dictionaries of the form:\n\n{'user':{movie:rating} }\n\n\nFor example, \n\n{Jill': {'Avenger: Age of Ultron': 7.0,\n                            'Django Unchained': 6.5,\n                            'Gone Girl': 9.0,\n                            'Kill the Messenger': 8.0}\n'Toby': {'Avenger: Age of Ultron': 8.5,\n                                'Django Unchained': 9.0,\n                                'Zoolander': 2.0}}\n\n\nI want to convert this dict of dicts into a pandas dataframe with column 1 the user name and the other columns the movie ratings i.e. \n\nuser  Gone_Girl  Horrible_Bosses_2  Django_Unchained  Zoolander etc. \\\n\n\nHowever, some users did not rate the movies and so these movies are not included in the values() for that user key(). It would be nice in these cases to just fill the entry with NaN. \n\nAs of now, I iterate over the keys, fill a list, and then use this list to create a dataframe:\n\ndata=[] \nfor i,key in enumerate(movie_user_preferences.keys() ):\n    try:            \n        data.append((key\n                    ,movie_user_preferences[key]['Gone Girl']\n                    ,movie_user_preferences[key]['Horrible Bosses 2']\n                    ,movie_user_preferences[key]['Django Unchained']\n                    ,movie_user_preferences[key]['Zoolander']\n                    ,movie_user_preferences[key]['Avenger: Age of Ultron']\n                    ,movie_user_preferences[key]['Kill the Messenger']))\n    # if no entry, skip\n    except:\n        pass \ndf=pd.DataFrame(data=data,columns=['user','Gone_Girl','Horrible_Bosses_2','Django_Unchained','Zoolander','Avenger_Age_of_Ultron','Kill_the_Messenger'])\n\n\nBut this only gives me a dataframe of users who rated all the movies in the set. \n\nMy goal is to append to the data list by iterating over the movie labels (rather than the brute force approach shown above) and, secondly, create a dataframe that includes all users and that places null values in the elements that do not have movie ratings. \n"
"i have a small sample data:\n\nimport pandas as pd\n\ndf = {'ID': [3009, 129,119,120,121,122,130,3014,266,849,174,844 ],\n  'V': ['IGHV7-B*01','IGHV7-B*01','IGHV6-A*01','GHV6-A*01','IGHV6-A*01','IGHV6-A*01','IGHV4-L*03','IGHV4-L*03','IGHV5-A*01','IGHV5-A*04','IGHV6-A*02','IGHV6-A*02'],\n  'Prob': [1,1,0.8,0.8056,0.9,0.805 ,1,1,0.997,0.401,1,1]}\n\n\ndf = pd.DataFrame(df)\n\n\nlooks like\n\ndf    \n\nOut[25]: \n      ID    Prob           V\n0    3009  1.0000  IGHV7-B*01\n1     129  1.0000  IGHV7-B*01\n2     119  0.8000  IGHV6-A*01\n3     120  0.8056  IGHV6-A*01\n4     121  0.9000  IGHV6-A*01\n5     122  0.8050  IGHV6-A*01\n6     130  1.0000  IGHV4-L*03\n7    3014  1.0000  IGHV4-L*03\n8     266  0.9970  IGHV5-A*01\n9     849  0.4010  IGHV5-A*04\n10    174  1.0000  IGHV6-A*02\n11    844  1.0000  IGHV6-A*02\n\n\nI want to split the column 'V' by the '-' delimiter and move it to another column named 'allele' \n\n    Out[25]: \n      ID    Prob      V    allele\n0    3009  1.0000  IGHV7    B*01\n1     129  1.0000  IGHV7    B*01\n2     119  0.8000  IGHV6    A*01\n3     120  0.8056  IGHV6    A*01\n4     121  0.9000  IGHV6    A*01\n5     122  0.8050  IGHV6    A*01\n6     130  1.0000  IGHV4    L*03\n7    3014  1.0000  IGHV4    L*03\n8     266  0.9970  IGHV5    A*01\n9     849  0.4010  IGHV5    A*04\n10    174  1.0000  IGHV6    A*02\n11    844  1.0000  IGHV6    A*02\n\n\nthe code i have tried so far is incomplete and didn't work:\n\ndf1 = pd.DataFrame()\ndf1[['V']] = pd.DataFrame([ x.split('-') for x in df['V'].tolist() ])\n\n\nor \n\ndf.add(Series, axis='columns', level = None, fill_value = None)\nnewdata = df.DataFrame({'V':df['V'].iloc[::2].values, 'Allele': df['V'].iloc[1::2].values})\n\n"
'How to get merged data frame from two data frames having common column value such that only those rows make merged data frame having common value in a particular column. \n\nI have 5000 rows of df1 as format : -\n\n    director_name   actor_1_name    actor_2_name    actor_3_name    movie_title\n0   James Cameron   CCH Pounder Joel David Moore    Wes Studi     Avatar\n1   Gore Verbinski  Johnny Depp Orlando Bloom   Jack Davenport   Pirates \n    of the Caribbean: At World\'s End\n2   Sam Mendes   Christoph Waltz    Rory Kinnear    Stephanie Sigman Spectre\n\n\nand 10000 rows of df2 as\n\nmovieId                   genres                        movie_title\n    1       Adventure|Animation|Children|Comedy|Fantasy   Toy Story\n    2       Adventure|Children|Fantasy                    Jumanji\n    3       Comedy|Romance                             Grumpier Old Men\n    4       Comedy|Drama|Romance                      Waiting to Exhale\n\n\nA common column \'movie_title\' have common values and based on them, I want to get all rows where \'movie_title\' is same. Other rows to be deleted.\n\nAny help/suggestion would be appreciated. \n\nNote:  I already tried \n\npd.merge(dfinal, df1, on=\'movie_title\')\n\n\nand output comes like one row \n\ndirector_name   actor_1_name    actor_2_name    actor_3_name    movie_title movieId title   genres\n\n\nand on how ="outer"/"left", "right", I tried all and didn\'t get any row after dropping NaN although many common coloumn do exist.\n'
"I'm trying to set the entire column of a dataframe to a specific value.\n\nIn  [1]: df\nOut [1]: \n     issueid   industry\n0        001        xxx\n1        002        xxx\n2        003        xxx\n3        004        xxx\n4        005        xxx\n\n\nFrom what I've seen, loc is the best practice when replacing values in a dataframe (or isn't it?):\n\nIn  [2]: df.loc[:,'industry'] = 'yyy'\n\n\nHowever, I still received this much talked-about warning message:\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n\n\nIf I do\n\nIn  [3]: df['industry'] = 'yyy'\n\n\nI got the same warning message.\n\nAny ideas?  Working with Python 3.5.2 and pandas 0.18.1.\n"
"I have a dataframe with this index:\n\nindex = pd.MultiIndex.from_product([['stock1','stock2'...],['price','volume'...]])\n\n\nIt's a useful structure for being able to do df['stock1'], but how do I select all the price data? I can't make any sense of the documentation.\n\nI've tried the following with no luck: df[:,'price'] df[:]['price'] df.loc(axis=1)[:,'close'] df['price]\n\nIf this index style is generally agreed to be a bad idea for whatever reason, then what would be a better choice? Should I go for a multi-indexed index for the stocks as labels on the time series instead of at the column level?\n\nMany thanks\n\nEDIT - I am using the multiindex for the columns, not the index (the wording got the better of me). The examples in the documentation focus on multi-level indexes rather than column structures.\n"
'I have some data from log files and would like to group entries by a minute:\n\n def gen(date, count=10):\n     while count &gt; 0:\n         yield date, "event{}".format(randint(1,9)), "source{}".format(randint(1,3))\n         count -= 1\n         date += DateOffset(seconds=randint(40))\n\n df = DataFrame.from_records(list(gen(datetime(2012,1,1,12, 30))), index=\'Time\', columns=[\'Time\', \'Event\', \'Source\'])\n\n\ndf:\n\n Event  Source\n 2012-01-01 12:30:00     event3  source1\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:12     event2  source2\n 2012-01-01 12:30:29     event6  source1\n 2012-01-01 12:30:38     event1  source1\n 2012-01-01 12:31:05     event4  source2\n 2012-01-01 12:31:38     event4  source1\n 2012-01-01 12:31:44     event5  source1\n 2012-01-01 12:31:48     event5  source2\n 2012-01-01 12:32:23     event6  source1\n\n\nI tried these options:\n\n\ndf.resample(\'Min\') is too high level and wants to aggregate.\ndf.groupby(date_range(datetime(2012,1,1,12, 30), freq=\'Min\',\nperiods=4)) fails with exception.\ndf.groupby(TimeGrouper(freq=\'Min\')) works fine and returns a DataFrameGroupBy object for further processing, e.g.:\n\ngrouped = df.groupby(TimeGrouper(freq=\'Min\'))\ngrouped.Source.value_counts()\n2012-01-01 12:30:00  source1    1\n2012-01-01 12:31:00  source2    2\n                     source1    2\n2012-01-01 12:32:00  source2    2\n                     source1    2\n2012-01-01 12:33:00  source1    1\n\n\n\nHowever, the TimeGrouper class is not documented.\n\nWhat is the correct way to group by a period of time? How can I group the data by a minute AND by the Source column, e.g. groupby([TimeGrouper(freq=\'Min\'), df.Source])?\n'
'The default output format of to_csv() is:\n\n12/14/2012  12:00:00 AM\n\n\nI cannot figure out how to output only the date part with specific format:\n\n20121214\n\n\nor date and time in two separate columns in the csv file:\n\n20121214,  084530\n\n\nThe documentation is too brief to give me any clue as to how to do these. Can anyone help?\n'
'I have a dataframe containing a single column of IDs and all other columns are numerical values for which I want to compute z-scores. Here\'s a subsection of it:\n\nID      Age    BMI    Risk Factor\nPT 6    48     19.3    4\nPT 8    43     20.9    NaN\nPT 2    39     18.1    3\nPT 9    41     19.5    NaN\n\n\nSome of my columns contain NaN values which I do not want to include into the z-score calculations so I intend to use a solution offered to this question: how to zscore normalize pandas column with nans?\n\ndf[\'zscore\'] = (df.a - df.a.mean())/df.a.std(ddof=0)\n\n\nI\'m interested in applying this solution to all of my columns except the ID column to produce a new dataframe which I can save as an Excel file using\n\ndf2.to_excel("Z-Scores.xlsx")\n\n\nSo basically; how can I compute z-scores for each column (ignoring NaN values) and push everything into a new dataframe?\n\nSIDENOTE: there is a concept in pandas called "indexing" which intimidates me because I do not understand it well. If indexing is a crucial part of solving this problem, please dumb down your explanation of indexing.\n'
"Say I have a dictionary that looks like this:\n\ndictionary = {'A' : {'a': [1,2,3,4,5],\n                     'b': [6,7,8,9,1]},\n\n              'B' : {'a': [2,3,4,5,6],\n                     'b': [7,8,9,1,2]}}\n\n\nand I want a dataframe that looks something like this:\n\n     A   B\n     a b a b\n  0  1 6 2 7\n  1  2 7 3 8\n  2  3 8 4 9\n  3  4 9 5 1\n  4  5 1 6 2\n\n\nIs there a convenient way to do this? If I try:\n\nIn [99]:\n\nDataFrame(dictionary)\n\nOut[99]:\n     A               B\na   [1, 2, 3, 4, 5] [2, 3, 4, 5, 6]\nb   [6, 7, 8, 9, 1] [7, 8, 9, 1, 2]\n\n\nI get a dataframe where each element is a list. What I need is a multiindex where each level corresponds to the keys in the nested dict and the rows corresponding to each element in the list as shown above. I think I can work a very crude solution but I'm hoping there might be something a bit simpler. \n"
"I was wondering if there are classifiers that handle nan/null values in scikit-learn.  I thought random forest regressor handles this but I got an error when I call predict.\n\nX_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])\ny_train = np.array([1, 2])\nclf = RandomForestRegressor(X_train, y_train)\nX_test = np.array([7, 8, np.nan])\ny_pred = clf.predict(X_test) # Fails!\n\n\nCan I not call predict with any scikit-learn algorithm with missing values?\n\nEdit.\nNow that I think about this, it makes sense.  It's not an issue during training but when you predict how do you branch when the variable is null?  maybe you could just split both ways and average the result?  It seems like k-NN should work fine as long as the distance function ignores nulls though.\n\nEdit 2 (older and wiser me)\nSome gbm libraries (such as xgboost) use a ternary tree instead of a binary tree precisely for this purpose: 2 children for the yes/no decision and 1 child for the missing decision. sklearn is using a binary tree\n"
'How do we get a particular filtered row as series?\n\nExample dataframe:\n\n&gt;&gt;&gt; df = pd.DataFrame({\'date\': [20130101, 20130101, 20130102], \'location\': [\'a\', \'a\', \'c\']})\n&gt;&gt;&gt; df\n       date location\n0  20130101        a\n1  20130101        a\n2  20130102        c\n\n\nI need to select the row where location is c as a series.\n\nI tried:\n\nrow = df[df["location"] == "c"].head(1)  # gives a dataframe\nrow = df.ix[df["location"] == "c"]       # also gives a dataframe with single row\n\n\nIn either cases I can\'t the row as series.\n'
"I need to add 1 day to each date I want to get the begining date of the following month eg 2014-01-2014 for the 1st item in the dataframe.\nTried: \n\nmontdist['date'] + pd.DateOffset(1)\n\n\nWhich gives me: \n\nTypeError: cannot use a non-absolute DateOffset in datetime/timedelta operations [&lt;DateOffset&gt;]\n\n\nHave a Dataframe:\n\n    Units   mondist                date\n1    6491  0.057785 2013-12-31 00:00:00\n2    7377  0.065672 2014-01-31 00:00:00\n3    9990  0.088934 2014-02-28 00:00:00\n4   10362  0.092245 2014-03-31 00:00:00\n5   11271  0.100337 2014-04-30 00:00:00\n6   11637  0.103596 2014-05-31 00:00:00\n7   10199  0.090794 2014-06-30 00:00:00\n8   10486  0.093349 2014-07-31 00:00:00\n9    9282  0.082631 2014-08-31 00:00:00\n10   8632  0.076844 2014-09-30 00:00:00\n11   8204  0.073034 2013-10-31 00:00:00\n12   8400  0.074779 2013-11-30 00:00:00\n\n"
"I'm making a fairly simple histogram in with pandas using\n\nresults.val1.hist(bins=120)\n\nwhich works fine, but I really want to have a log scale on the y axis, which I normally (probably incorrectly) do like this:\n\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nplt.plot(np.random.rand(100))\nax.set_yscale('log')\nplt.show()\n\n\nIf I replace the plt command with the pandas command, so I have:\n\nfig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(111)\nresults.val1.hist(bins=120)\nax.set_yscale('log')\nplt.show()\n\n\nresults in many copies of the same error:\n\nJan  9 15:53:07 BLARG.local python[6917] &lt;Error&gt;: CGContextClosePath: no current point.\n\n\nI do get a log scale histogram, but it only has the top lines of the bars, but no vertical bars or colors. Am doing something horribly wrong or is this just not supported by pandas?\n\nFrom Paul H's code I added bottom=0.1 to hist call fixes the problem, I guess there is some kind of divide by zero thing, or something.\n"
"I have a pandas dataframe with a column named 'City, State, Country'. I want to separate this column into three new columns, 'City, 'State' and 'Country'.\n\n0                 HUN\n1                 ESP\n2                 GBR\n3                 ESP\n4                 FRA\n5             ID, USA\n6             GA, USA\n7    Hoboken, NJ, USA\n8             NJ, USA\n9                 AUS\n\n\nSplitting the column into three columns is trivial enough:\n\nlocation_df = df['City, State, Country'].apply(lambda x: pd.Series(x.split(',')))\n\n\nHowever, this creates left-aligned data:\n\n     0       1       2\n0    HUN     NaN     NaN\n1    ESP     NaN     NaN\n2    GBR     NaN     NaN\n3    ESP     NaN     NaN\n4    FRA     NaN     NaN\n5    ID      USA     NaN\n6    GA      USA     NaN\n7    Hoboken  NJ     USA\n8    NJ      USA     NaN\n9    AUS     NaN     NaN\n\n\nHow would one go about creating the new columns with the data right-aligned? Would I need to iterate through every row, count the number of commas and handle the contents individually?\n"
"This is my data frame that should be repeated for 5 times:\n\n&gt;&gt;&gt; x = pd.DataFrame({'a':1,'b':2},index = range(1))\n&gt;&gt;&gt; x\n   a  b\n0  1  2\n\n\nI wanna have the result like this:\n\n&gt;&gt;&gt; x.append(x).append(x).append(x)\n   a  b\n0  1  2\n0  1  2\n0  1  2\n0  1  2\n\n\nBut there must be a way smarter than keep appending.. Actually the data frame Im working on should be repeated for 50 times..\n\nI haven't found anything practical, including those like np.repeat ---- it just doesnt work on data frame.\n\nCould anyone help?\n"
'I have the following table:\n\nNote: Both NSRCODE and PBL_AWI are index\'s\n\nNote: the % Of area column would be filled out just have not done so yet.\n\nNSRCODE  PBL_AWI          Area           % Of Area\nCM       BONS             44705.492941\n         BTNN            253854.591990\n         FONG             41625.590370\n         FONS             16814.159680\n         Lake             57124.819333\n         River             1603.906642\n         SONS            583958.444751\n         STNN             45603.837177\n         clearcut        106139.013930\n         disturbed       127719.865675\n         lowland         118795.578059\n         upland         2701289.270193\nLBH      BFNN            289207.169650\n         BONS           9140084.716743\n         BTNI             33713.160390\n         BTNN          19748004.789040\n         FONG           1687122.469691\n         FONS           5169959.591270\n         FTNI            317251.976160\n         FTNN           6536472.869395\n         Lake            258046.508310\n         River            44262.807900\n         SONS           4379097.677405\n         burn regen      744773.210860\n         clearcut         54066.756790\n         disturbed       597561.471686\n         lowland       12591619.141842\n         upland        23843453.638117\n\n\nHow do I filter out item in the "PBL_AWI" index? \nFor example I want to keep [\'Lake\', \'River\', \'Upland\']\n'
"I've seen a few variations on the theme of exploding a column/series into multiple columns of a Pandas dataframe, but I've been trying to do something and not really succeeding with the existing approaches.\n\nGiven a DataFrame like so:\n\n    key       val\nid\n2   foo   oranges\n2   bar   bananas\n2   baz    apples\n3   foo    grapes\n3   bar     kiwis\n\n\nI want to convert the items in the key series into columns, with the val values serving as the values, like so:\n\n        foo        bar        baz\nid\n2   oranges    bananas     apples\n3    grapes      kiwis        NaN\n\n\nI feel like this is something that should be relatively straightforward, but I've been bashing my head against this for a few hours now with increasing levels of convolution, and no success.\n"
"I have a dataframe where one column is a list of groups each of my users belongs to. Something like:\n\nindex groups  \n0     ['a','b','c']\n1     ['c']\n2     ['b','c','e']\n3     ['a','c']\n4     ['b','e']\n\n\nAnd what I would like to do is create a series of dummy columns to identify which groups each user belongs to in order to run some analyses\n\nindex  a   b   c   d   e\n0      1   1   1   0   0\n1      0   0   1   0   0\n2      0   1   1   0   1\n3      1   0   1   0   0\n4      0   1   0   0   0\n\n\npd.get_dummies(df['groups'])\n\n\nwon't work because that just returns a column for each different list in my column.\n\nThe solution needs to be efficient as the dataframe will contain 500,000+ rows. Any advice would be appreciated!\n"
"I need to make this simple thing:\ndates = p.to_datetime(p.Series(['20010101', '20010331']), format = '%Y%m%d')\ndates.str\n\nBut I get an error. How should I transform from datetime to string?\n"
'I have a .csv file in such format\n\ntimestmp, p\n2014/12/31 00:31:01:9200, 0.7\n2014/12/31 00:31:12:1700, 1.9\n...\n\n\nand when read via pd.read_csv and convert the time str to datetime using pd.to_datetime, the performance drops dramatically. Here is a minimal example.\n\nimport re\nimport pandas as pd\n\nd = \'2014-12-12 01:02:03.0030\'\nc = re.sub(\'-\', \'/\', d)\n\n%timeit pd.to_datetime(d)\n%timeit pd.to_datetime(c)\n%timeit pd.to_datetime(c, format="%Y/%m/%d %H:%M:%S.%f")\n\n\nand the performances are:\n\n10000 loops, best of 3: 62.4 µs per loop\n10000 loops, best of 3: 181 µs per loop\n10000 loops, best of 3: 82.9 µs per loop\n\n\nso, how could I improve the performance of pd.to_datetime when reading date from a csv file?\n'
"I have some data I'm trying to organize into a DataFrame in Pandas.  I was trying to make each row a Series and append it to the DataFrame.  I found a way to do it by appending the Series to an empty list and then converting the list of Series to a DataFrame \n\ne.g. DF = DataFrame([series1,series2],columns=series1.index)\n\nThis list to DataFrame step seems to be excessive.  I've checked out a few examples on here but none of the Series preserved the Index labels from the Series to use them as column labels.\n\nMy long way where columns are id_names and rows are type_names:\n \n\nIs it possible to append Series to rows of DataFrame without making a list first?\n\n#!/usr/bin/python\n\nDF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value)\n    DF.append(SR_row)\nDF.head()\n\nTypeError: Can only append a Series if ignore_index=True or if the Series has a name\n\n\nThen I tried\n\nDF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value,name=sample)\n    DF.append(SR_row)\nDF.head()\n\n\nEmpty DataFrame\n\nTried Insert a row to pandas dataframe\nStill getting an empty dataframe :/ \n\nI am trying to get the Series to be the rows, where the index of the Series becomes the column labels of the DataFrame\n"
"There is a dataframe like the following, and it has one unclean column 'id' which it sholud be numeric column\n\nid, name\n1,  A\n2,  B\n3,  C\ntt, D\n4,  E\n5,  F\nde, G\n\n\nIs there a concise way to remove the rows because tt and de are not numeric values\n\ntt,D\nde,G\n\n\nto make the dataframe clean?\n\nid, name\n1,  A\n2,  B\n3,  C\n4,  E\n5,  F\n\n"
"I have an array of objects of this class\n\nclass CancerDataEntity(Model):\n\n    age = columns.Text(primary_key=True)\n    gender = columns.Text(primary_key=True)\n    cancer = columns.Text(primary_key=True)\n    deaths = columns.Integer()\n    ...\n\n\nWhen printed, array looks like this\n\n[CancerDataEntity(age=u'80-85+', gender=u'Female', cancer=u'All cancers (C00-97,B21)', deaths=15306), CancerDataEntity(...\n\n\nI want to convert this to a data frame so I can play with it in a more suitable way to me - to aggregate, count, sum and similar.\nHow I wish this data frame to look, would be something like this:\n\n     age     gender     cancer     deaths\n0    80-85+  Female     ...        15306\n1    ...\n\n\nIs there a way to achieve this using numpy/pandas easily, without manually processing the input array?\n"
"I am trying to find the union of two polygons in GeoPandas and output a single geometry that encompasses points from both polygons as its vertices.  The geopandas.overlay function gives me polygons for each individual union but I would like a single polygon.  \n\nFor context, I'm using this to combine two administrative areas together into a single area (i.e. include a town district within a country).  \n\nThe following example is from the geopandas website and illustrates what I'd like:\n\nfrom matplotlib import pyplot as plt\nimport geopandas as gpd\nfrom shapely.geometry import Polygon\n\npolys1 = gpd.GeoSeries([Polygon([(0,0), (2,0), (2,2), (0,2)]),\n                         Polygon([(2,2), (4,2), (4,4), (2,4)])])\n\npolys2 = gpd.GeoSeries([Polygon([(1,1), (3,1), (3,3), (1,3)]),\n                         Polygon([(3,3), (5,3), (5,5), (3,5)])])\n\ndf1 = gpd.GeoDataFrame({'geometry': polys1, 'df1':[1,2]})\ndf2 = gpd.GeoDataFrame({'geometry': polys2, 'df2':[1,2]})\n\nres_union = gpd.overlay(df1, df2, how='union')\nres_union.plot()\n\n\n\n\nNone of the output geometries are what I was expected, which is the following:\n\npoly_union = gpd.GeoSeries([Polygon([(0,0), (0,2), (1,2), (1,3), \\\n    (2,3), (2,4), (3, 4), (3, 5), (5, 5), (5, 3), (4, 3), (4, 2), \\\n    (3,2), (3,1), (2, 1), (2, 0), (0, 0)])])\n\npoly_union.plot(color = 'red')\nplt.show()\n\n\n\n\nFirstly, how do I output the above polygon (poly_union) from the input polygons (df1, df2) using GeoPandas or shapely?  \n\nSecondly, what is the correct nomenclature associated with the geometry (poly_union) that I'm trying to find?  I would call it a 'union' but every example I find that refers to 'unions' does not output this geometry.  \n\nNote: This example does not seem to output a single polygon either:\n\npoly1 = df1['geometry']; poly2 = df2['geometry']\nmergedpoly = poly1.union(poly2)\nmergedpoly.plot()\n\n\n\n"
"I have a Pandas Data Frame object that has 1000 rows and 10 columns. I would simply like to slice the Data Frame and take the first 10 rows. How can I do this? I've been trying to use this:\n\n&gt;&gt;&gt; df.shape\n(1000,10)\n&gt;&gt;&gt; my_slice = df.ix[10,:]\n&gt;&gt;&gt; my_slice.shape\n(10,)\n\n\nShouldn't my_slice be the first ten rows, ie. a 10 x 10 Data Frame? How can I get the first ten rows, such that my_slice is a 10x10 Data Frame object? Thanks.\n"
'I can connect to my local mysql database from python, and I can create, select from, and insert individual rows.\n\nMy question is: can I directly instruct mysqldb to take an entire dataframe and insert it into an existing table, or do I need to iterate over the rows? \n\nIn either case, what would the python script look like for a very simple table with ID and two data columns, and a matching dataframe?\n'
"Say I have a data table\n\n    1  2  3  4  5  6 ..  n\nA   x  x  x  x  x  x ..  x\nB   x  x  x  x  x  x ..  x\nC   x  x  x  x  x  x ..  x\n\n\nAnd I want to slim it down so that I only have, say, columns 3 and 5 deleting all other and maintaining the structure. How could I do this with pandas? I think I understand how to delete a single column, but I don't know how to save a select few and delete all others.\n"
'I have a dataframe, df, that has some columns of type float64, while the others are of object. Due to the mixed nature, I cannot use \n\ndf.fillna(\'unknown\') #getting error "ValueError: could not convert string to float:"\n\n\nas the error happened with the columns whose type is float64 (what a misleading error message!)\n\nso I\'d wish that I could do something like \n\nfor col in df.columns[&lt;dtype == object&gt;]:\n    df[col] = df[col].fillna("unknown")\n\n\nSo my question is if there is any such filter expression that I can use with df.columns?\n\nI guess alternatively, less elegantly, I could do:  \n\n for col in df.columns:\n        if (df[col].dtype == dtype(\'O\')): # for object type\n            df[col] = df[col].fillna(\'\') \n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of "ValueError: Error parsing datetime string "unknown" at position 0" \n\n\nI also would like to know why in the above code replacing \'\' with \'unknown\' the code would work for certain cells but failed with a cell with the error of "ValueError: Error parsing datetime string "unknown" at position 0" \n\nThanks a lot!\n\nYu\n'
'what would be the most efficient way to use groupby and in parallel apply a filter in pandas?\n\nBasically I am asking for the equivalent in SQL of\n\nselect *\n...\ngroup by col_name\nhaving condition\n\n\nI think there are many uses cases ranging from conditional means, sums, conditional probabilities, etc. which would make such a command very powerful.\n\nI need a very good performance, so ideally such a command would not be the result of several layered operations done in python.\n'
"I have a df like this:\n\n    a b           c\n    1 NaT         w\n    2 2014-02-01  g\n    3 NaT         x   \n\n    df=df[df.b=='2014-02-01']\n\n\nwill give me\n\n    a  b          c\n    2 2014-02-01  g\n\n\nI want a database of all rows with NaT in column b?\n\n   df=df[df.b==None] #Doesn't work\n\n\nI want this:\n\n    a b           c\n    1 NaT         w\n    3 NaT         x    \n\n"
'I have an Excel file (.xls format) with 5 sheets, I want to replace the contents of sheet 5 with contents of my pandas data frame.\n'
"I've read loads of SO answers but can't find a clear solution.\n\nI have this data in a df called day1 which represents hours:\n\n1    10:53\n2    12:17\n3    14:46\n4    16:36\n5    18:39\n6    20:31\n7    22:28\nName: time, dtype: object&gt;\n\n\nI want to convert it into a time format. But when I do this:\n\nday1.time = pd.to_datetime(day1.time, format='H%:M%')\n\nThe result includes today's date:\n\n1   2015-09-03 10:53:00\n2   2015-09-03 12:17:00\n3   2015-09-03 14:46:00\n4   2015-09-03 16:36:00\n5   2015-09-03 18:39:00\n6   2015-09-03 20:31:00\n7   2015-09-03 22:28:00\nName: time, dtype: datetime64[ns]&gt;\n\n\nIt seems the format argument isn't working - how do I get the time as shown here without the date?\n\n\n\nUpdate\n\nThe following formats the time correctly, but somehow the column is still an object type. Why doesn't it convert to datetime64?\n\nday1['time'] = pd.to_datetime(day1['time'], format='%H:%M').dt.time\n\n1    10:53:00\n2    12:17:00\n3    14:46:00\n4    16:36:00\n5    18:39:00\n6    20:31:00\n7    22:28:00\nName: time, dtype: object&gt;\n\n"
"I know this must have been answered some where but I just could not find it.\n\nProblem: Sample each group after groupby operation.\n\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1,2,3,4,5,6,7],\n                   'b': [1,1,1,0,0,0,0]})\n\ngrouped = df.groupby('b')\n\n# now sample from each group, e.g., I want 30% of each group\n\n"
'I\'m using TfidfVectorizer from scikit-learn to do some feature extraction from text data. I have a CSV file with a Score (can be +1 or -1) and a Review (text). I pulled this data into a DataFrame so I can run the Vectorizer.\n\nThis is my code: \n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndf = pd.read_csv("train_new.csv",\n             names = [\'Score\', \'Review\'], sep=\',\')\n\n# x = df[\'Review\'] == np.nan\n#\n# print x.to_csv(path=\'FindNaN.csv\', sep=\',\', na_rep = \'string\', index=True)\n#\n# print df.isnull().values.any()\n\nv = TfidfVectorizer(decode_error=\'replace\', encoding=\'utf-8\')\nx = v.fit_transform(df[\'Review\'])\n\n\nThis is the traceback for the error I get: \n\nTraceback (most recent call last):\n  File "/home/PycharmProjects/Review/src/feature_extraction.py", line 16, in &lt;module&gt;\nx = v.fit_transform(df[\'Review\'])\n File "/home/b/hw1/local/lib/python2.7/site-   packages/sklearn/feature_extraction/text.py", line 1305, in fit_transform\n   X = super(TfidfVectorizer, self).fit_transform(raw_documents)\n File "/home/b/work/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py", line 817, in fit_transform\nself.fixed_vocabulary_)\n File "/home/b/work/local/lib/python2.7/site- packages/sklearn/feature_extraction/text.py", line 752, in _count_vocab\n   for feature in analyze(doc):\n File "/home/b/work/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py", line 238, in &lt;lambda&gt;\ntokenize(preprocess(self.decode(doc))), stop_words)\n File "/home/b/work/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.py", line 118, in decode\n raise ValueError("np.nan is an invalid document, expected byte or "\n ValueError: np.nan is an invalid document, expected byte or unicode string.\n\n\nI checked the CSV file and DataFrame for anything that\'s being read as NaN but I can\'t find anything. There are 18000 rows, none of which return isnan as True. \n\nThis is what df[\'Review\'].head() looks like: \n\n  0    This book is such a life saver.  It has been s...\n  1    I bought this a few times for my older son and...\n  2    This is great for basics, but I wish the space...\n  3    This book is perfect!  I\'m a first time new mo...\n  4    During your postpartum stay at the hospital th...\n  Name: Review, dtype: object\n\n'
"I would like to make supervised learning.\nUntil now I know to do supervised learning to all features.\nHowever, I would like also to conduct experiment with the K best features.\nI read the documentation and found the in Scikit learn there is SelectKBest method.\nUnfortunately, I am not sure how to create new dataframe after finding those best features:\nLet's assume I would like to conduct experiment with 5 best features:\nfrom sklearn.feature_selection import SelectKBest, f_classif\nselect_k_best_classifier = SelectKBest(score_func=f_classif, k=5).fit_transform(features_dataframe, targeted_class)\n\nNow if I would add the next line:\ndataframe = pd.DataFrame(select_k_best_classifier)\n\nI will receive a new dataframe without feature names (only index starting from 0 to 4).\nI should replace it to:\ndataframe = pd.DataFrame(fit_transofrmed_features, columns=features_names)\n\nMy question is how to create the features_names list??\nI know that I should use:\n select_k_best_classifier.get_support()\n\nWhich returns array of boolean values.\nThe true value in the array represent the index in the right column.\nHow should I use this boolean array with the array of all features names I can get via the method:\nfeature_names = list(features_dataframe.columns.values)\n\n"
"If I have a dataframe similar to this one\n\nApples   Bananas   Grapes   Kiwis\n2        3         nan      1\n1        3         7        nan\nnan      nan       2        3\n\n\nI would like to add a column like this\n\nApples   Bananas   Grapes   Kiwis   Fruit Total\n2        3         nan      1        6\n1        3         7        nan      11\nnan      nan       2        3        5\n\n\nI guess you could use df['Apples'] + df['Bananas'] and so on, but my actual dataframe is much larger than this. I was hoping a formula like df['Fruit Total']=df[-4:-1].sum could do the trick in one line of code. That didn't work however. Is there any way to do it without explicitly summing up all columns?\n"
"I am using the following code to plot a bar-chart:\n\nimport matplotlib.pyplot as pls \nmy_df.plot(x='my_timestampe', y='col_A', kind='bar') \nplt.show()\n\n\nThe plot works fine. However, I want to improve the graph by having 3 columns: 'col_A', 'col_B', and 'col_C' all on the plot. Like in the example figure below:\n\n\n\nI would like the col_A displayed in blue above x-axis, col_B in red below x-axis, and col_C in green above x-axis. Is this something possible in matplotlib? How do I make changes to plot all the three columns? Thanks!\n"
"\n  The contents of this post were originally meant to be a part of\n  Pandas Merging 101,\n  but due to the nature and size of the content required to fully do\n  justice to this topic, it has been moved to its own QnA.\n\n\nGiven two simple DataFrames; \n\nleft = pd.DataFrame({'col1' : ['A', 'B', 'C'], 'col2' : [1, 2, 3]})\nright = pd.DataFrame({'col1' : ['X', 'Y', 'Z'], 'col2' : [20, 30, 50]})\n\nleft\n\n  col1  col2\n0    A     1\n1    B     2\n2    C     3\n\nright\n\n  col1  col2\n0    X    20\n1    Y    30\n2    Z    50\n\n\nThe cross product of these frames can be computed, and will look something like:\n\nA       1      X      20\nA       1      Y      30\nA       1      Z      50\nB       2      X      20\nB       2      Y      30\nB       2      Z      50\nC       3      X      20\nC       3      Y      30\nC       3      Z      50\n\n\nWhat is the most performant method of computing this result?\n"
"Say I have a column in a dataframe that has some numbers and some non-numbers\n\n&gt;&gt; df['foo']\n0       0.0\n1     103.8\n2     751.1\n3       0.0\n4       0.0\n5         -\n6         -\n7       0.0\n8         -\n9       0.0\nName: foo, Length: 9, dtype: object\n\n\nHow can I convert this column to np.float, and have everything else that is not float convert it to NaN?\n\nWhen I try:\n\n&gt;&gt; df['foo'].astype(np.float)\n\n\nor\n\n&gt;&gt; df['foo'].apply(np.float)\n\n\nI get ValueError: could not convert string to float: -\n"
"I'd like to concatenate two dataframes A, B to a new one without duplicate rows (if rows in B already exist in A, don't add):\n\nDataframe A:   Dataframe B:\n\n   I    II    I    II\n0  1    2     5    6\n1  3    1     3    1\n\n\nNew Dataframe:\n\n     I    II\n  0  1    2\n  1  3    1\n  2  5    6\n\n\nHow can I do this?\n"
"I have the following pandas dataframe:\n\ndfalph.head()\n\ntoken    year    uses  books\n  386   xanthos  1830    3     3\n  387   xanthos  1840    1     1\n  388   xanthos  1840    2     2\n  389   xanthos  1868    2     2\n  390   xanthos  1875    1     1\n\n\nI aggregate the rows with duplicate token and years like so:\n\ndfalph = dfalph[['token','year','uses','books']].groupby(['token', 'year']).agg([np.sum])\ndfalph.columns = dfalph.columns.droplevel(1)\ndfalph.head()\n\n               uses  books\ntoken    year       \nxanthos  1830    3     3\n         1840    3     3\n         1867    2     2\n         1868    2     2\n         1875    1     1\n\n\nInstead of having the 'token' and 'year' fields in the index, I would like to return them to columns and have an integer index. \n"
'Problem\n\nGiven data in a Pandas DataFrame like the following:\n\nName     Amount\n---------------\nAlice       100\nBob          50\nCharlie     200\nAlice        30\nCharlie      10\n\n\nI want to select all rows where the Name is one of several values in a collection {Alice, Bob}\n\nName     Amount\n---------------\nAlice       100\nBob          50\nAlice        30\n\n\nQuestion\n\nWhat is an efficient way to do this in Pandas?\n\nOptions as I see them\n\n\nLoop through rows, handling the logic with Python\nSelect and merge many statements like the following \n\nmerge(df[df.name = specific_name] for specific_name in names) # something like this\n\nPerform some sort of join\n\n\nWhat are the performance trade-offs here?  When is one solution better than the others?  What solutions am I missing?\n\nWhile the example above uses strings my actual job uses matches on 10-100 integers over millions of rows and so fast NumPy operations may be relevant. \n'
"I want to split the following dataframe based on column ZZ\n\ndf = \n        N0_YLDF  ZZ        MAT\n    0  6.286333   2  11.669069\n    1  6.317000   6  11.669069\n    2  6.324889   6  11.516454\n    3  6.320667   5  11.516454\n    4  6.325556   5  11.516454\n    5  6.359000   6  11.516454\n    6  6.359000   6  11.516454\n    7  6.361111   7  11.516454\n    8  6.360778   7  11.516454\n    9  6.361111   6  11.516454\n\n\nAs output, I want a new dataframe with the 'N0_YLDF' column split into 4, one new column for each unique value of ZZ. How do I go about this? I can do groupby, but do not know what to do with the grouped object.\n"
"Assume I have a DataFrame sales of timestamp values:\n\ntimestamp               sales_office\n2014-01-01 09:01:00     Cincinnati\n2014-01-01 09:11:00     San Francisco\n2014-01-01 15:22:00     Chicago\n2014-01-01 19:01:00     Chicago\n\n\nI would like to create a new column time_hour. I can create it by writing a short function as so and using apply() to apply it iteratively:\n\ndef hr_func(ts):\n    return ts.hour\n\nsales['time_hour'] = sales['timestamp'].apply(hr_func)\n\n\nI would then see this result:\n\ntimestamp               sales_office         time_hour\n2014-01-01 09:01:00     Cincinnati           9\n2014-01-01 09:11:00     San Francisco        9\n2014-01-01 15:22:00     Chicago              15\n2014-01-01 19:01:00     Chicago              19\n\n\nWhat I'd like to achieve is some shorter transformation like this (which I know is erroneous but gets at the spirit):\n\nsales['time_hour'] = sales['timestamp'].hour\n\n\nObviously the column is of type Series and as such doesn't have those attributes, but it seems there's a simpler way to make use of matrix operations.\n\nIs there a more-direct approach?\n"
"According to this documentation I can only make a join between fields having the same name.\n\nDo you know if it's possible to join two DataFrames on a field having different names?\n\nThe equivalent in SQL would be:\n\nSELECT *\nFROM df1\nLEFT OUTER JOIN df2\n  ON df1.id_key = df2.fk_key\n\n"
'I can\'t seem to get a simple dtype check working with Pandas\' improved Categoricals in v0.15+. Basically I just want something like is_categorical(column) -&gt; True/False.\n\nimport pandas as pd\nimport numpy as np\nimport random\n\ndf = pd.DataFrame({\n    \'x\': np.linspace(0, 50, 6),\n    \'y\': np.linspace(0, 20, 6),\n    \'cat_column\': random.sample(\'abcdef\', 6)\n})\ndf[\'cat_column\'] = pd.Categorical(df2[\'cat_column\'])\n\n\nWe can see that the dtype for the categorical column is \'category\':\n\ndf.cat_column.dtype\nOut[20]: category\n\n\nAnd normally we can do a dtype check by just comparing to the name\nof the dtype:\n\ndf.x.dtype == \'float64\'\nOut[21]: True\n\n\nBut this doesn\'t seem to work when trying to check if the x column\nis categorical:\n\ndf.x.dtype == \'category\'\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-22-94d2608815c4&gt; in &lt;module&gt;()\n----&gt; 1 df.x.dtype == \'category\'\n\nTypeError: data type "category" not understood\n\n\nIs there any way to do these types of checks in pandas v0.15+?\n'
'For example I have following table:\n\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n3,1,0\n4,1,5\n\n\nAfter grouping by A:\n\n0:\nindex,A,B\n0,0,0\n1,0,8\n2,0,8\n\n1:\nindex,A,B\n3,1,5\n4,1,3\n\n\nWhat I need is to drop rows from each group, where the number in column B is less than maximum value from all rows from group\'s column B. Well I have a problem translating and formulating this problem to English so here is the example:\n\nMaximum value from rows in column B in group 0: 8\n\nSo I want to drop row with index 0 and keep rows with indexes 1 and 2\n\nMaximum value from rows in column B in group 1: 5\n\nSo I want to drop row with index 4 and keep row with index 3\n\nI have tried to use pandas filter function, but the problem is that it is operating on all rows in group at one time:\n\ndata = &lt;example table&gt;\ngrouped = data.groupby("A")\nfiltered = grouped.filter(lambda x: x["B"] == x["B"].max())\n\n\nSo what I ideally need is some filter, which iterates through all rows in group. \n\nThanks for help!\n\nP.S. Is there also way to only delete rows in groups and do not return DataFrame object?\n'
'I would like to use pd.write_csv to write "filename" (with headers) if "filename" doesn\'t exist, otherwise to append to "filename" if it exists.    If I simply use command: \n\n     df.to_csv(\'filename.csv\',mode = \'a\',header =\'column_names\')\n\n\nThe write or append succeeds, but it seems like the header is written every time an append takes place.   \n\nHow can I only add the header if the file doesn\'t exist, and append without header if the file does exist? \n'
"I used Counter on a list to compute this variable:\n\nfinal = Counter(event_container)\n\n\nprint final gives:\n\nCounter({'fb_view_listing': 76, 'fb_homescreen': 63, 'rt_view_listing': 50, 'rt_home_start_app': 46, 'fb_view_wishlist': 39, 'fb_view_product': 37, 'fb_search': 29, 'rt_view_product': 23, 'fb_view_cart': 22, 'rt_search': 12, 'rt_view_cart': 12, 'add_to_cart': 2, 'create_campaign': 1, 'fb_connect': 1, 'sale': 1, 'guest_sale': 1, 'remove_from_cart': 1, 'rt_transaction_confirmation': 1, 'login': 1})\n\n\nNow I want to convert final into a Pandas DataFrame, but when I'm doing:\n\nfinal_df = pd.DataFrame(final)\n\n\nbut I got an error.\n\nI guess final is not a proper dictionary, so how can I convert final to a dictionary? Or is it an other way to convert final to a DataFrame?\n"
'I\'m trying to convert Pandas DF into Spark one.\nDF head:\n\n10000001,1,0,1,12:35,OK,10002,1,0,9,f,NA,24,24,0,3,9,0,0,1,1,0,0,4,543\n10000001,2,0,1,12:36,OK,10002,1,0,9,f,NA,24,24,0,3,9,2,1,1,3,1,3,2,611\n10000002,1,0,4,12:19,PA,10003,1,1,7,f,NA,74,74,0,2,15,2,0,2,3,1,2,2,691\n\n\nCode:\n\ndataset = pd.read_csv("data/AS/test_v2.csv")\nsc = SparkContext(conf=conf)\nsqlCtx = SQLContext(sc)\nsdf = sqlCtx.createDataFrame(dataset)\n\n\nAnd I got an error:\n\nTypeError: Can not merge type &lt;class \'pyspark.sql.types.StringType\'&gt; and &lt;class \'pyspark.sql.types.DoubleType\'&gt;\n\n'
"I have two columns in a Pandas data frame that are dates. \n\nI am looking to subtract one column from another and the result being the difference in numbers of days as an integer.\n\nA peek at the data:\n\ndf_test.head(10)\nOut[20]: \n  First_Date Second Date\n0 2016-02-09  2015-11-19\n1 2016-01-06  2015-11-30\n2        NaT  2015-12-04\n3 2016-01-06  2015-12-08\n4        NaT  2015-12-09\n5 2016-01-07  2015-12-11\n6        NaT  2015-12-12\n7        NaT  2015-12-14\n8 2016-01-06  2015-12-14\n9        NaT  2015-12-15\n\n\nI have created a new column successfully with the difference:\n\ndf_test['Difference'] = df_test['First_Date'].sub(df_test['Second Date'], axis=0)\ndf_test.head()         \nOut[22]: \n  First_Date Second Date  Difference\n0 2016-02-09  2015-11-19     82 days\n1 2016-01-06  2015-11-30     37 days\n2        NaT  2015-12-04         NaT\n3 2016-01-06  2015-12-08     29 days\n4        NaT  2015-12-09         NaT\n\n\nHowever I am unable to get a numeric version of the result:\n\ndf_test['Difference'] = df_test[['Difference']].apply(pd.to_numeric)     \n\ndf_test.head()\nOut[25]: \n  First_Date Second Date    Difference\n0 2016-02-09  2015-11-19  7.084800e+15\n1 2016-01-06  2015-11-30  3.196800e+15\n2        NaT  2015-12-04           NaN\n3 2016-01-06  2015-12-08  2.505600e+15\n4        NaT  2015-12-09           NaN\n\n"
"energy.loc['Republic of Korea']\n\nI want to change the value of index from 'Republic of Korea' to 'South Korea'.\nBut the dataframe is too large and it is not possible to change every index value. How do I change only this single value?\n"
"Using pandas I can easily make a line plot:  \n\nimport pandas as pd\nimport numpy as np\n%matplotlib inline # to use it in jupyter notebooks\n\ndf = pd.DataFrame(np.random.randn(50, 4), \n        index=pd.date_range('1/1/2000', periods=50), columns=list('ABCD'))\ndf = df.cumsum()\ndf.plot();\n\n\n\n\nBut I can't figure out how to also plot the data as points over the lines, as in this example: \n\n\n\nThis matplotlib example seems to suggest the direction, but I can't find how to do it using pandas plotting capabilities. And I am specially interested in learning how to do it with pandas because I am always working with dataframes.\n\nAny clues?\n"
'I have DataFrame:\n\n    time_diff   avg_trips\n0   0.450000    1.0\n1   0.483333    1.0\n2   0.500000    1.0\n3   0.516667    1.0\n4   0.533333    2.0\n\n\nI want to get 1st quartile, 3rd quartile and median for the column time_diff. To obtain median, I use np.median(df["time_diff"].values).\n\nHow can I calculate quartiles?\n'
"I want to get a percentage of a particular value in a df column. Say I have a df with (col1, col2 , col3, gender)  gender column has values of M, F, or Other. I want to get the percentage of M, F, Other values in the df.\nI have tried this, which gives me the number M, F, Other instances, but I want these as a percentage of the total number of values in the df.\ndf.groupby('gender').size()\n\nCan someone help?\n"
"\nHow to perform aggregation with pandas?\nNo DataFrame after aggregation! What happened?\nHow to aggregate mainly strings columns (to lists, tuples, strings with separator)?\nHow to aggregate counts?\nHow to create new column filled by aggregated values?\n\n\nI've seen these recurring questions asking about various faces of the pandas aggregate functionality. \nMost of the information regarding aggregation and its various use cases today is fragmented across dozens of badly worded, unsearchable posts. \nThe aim here is to collate some of the more important points for posterity.\n\nThis Q/A is meant to be the next instalment in a series of helpful user-guides:\n\n\nHow to pivot a dataframe, \nPandas concat\nHow do I operate on a DataFrame with a Series for every column\nPandas Merging 101\n\n\nPlease note that this post is not meant to be a replacement for the documentation about aggregation and about groupby, so please read that as well! \n"
'If I import or create a pandas column that contains no spaces, I can access it as such:\n\ndf1 = DataFrame({\'key\': [\'b\', \'b\', \'a\', \'c\', \'a\', \'a\', \'b\'],\n                 \'data1\': range(7)})\n\ndf1.data1\n\n\nwhich would return that series for me.  If, however, that column has a space in its name, it isn\'t accessible via that method:\n\ndf2 = DataFrame({\'key\': [\'a\',\'b\',\'d\'],\n                 \'data 2\': range(3)})\n\ndf2.data 2      # &lt;--- not the droid i\'m looking for.\n\n\nI know I can access it using .xs():\n\ndf2.xs(\'data 2\', axis=1)\n\n\nThere\'s got to be another way.  I\'ve googled it like mad and can\'t think of any other way to google it.  I\'ve read all 96 entries here on SO that contain "column" and "string" and "pandas" and could find no previous answer.  Is this the only way, or is there something better?\n\nThanks!\n'
"A simple pandas question: \n\nIs there a drop_duplicates() functionality to drop every row involved in the duplication? \n\nAn equivalent question is the following: Does pandas have a set difference for dataframes? \n\nFor example:\n\nIn [5]: df1 = pd.DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\n\nIn [6]: df2 = pd.DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})\n\nIn [7]: df1\nOut[7]: \n   col1  col2\n0     1     2\n1     2     3\n2     3     4\n\nIn [8]: df2\nOut[8]: \n   col1  col2\n0     4     6\n1     2     3\n2     5     5\n\n\nso maybe something like df2.set_diff(df1) will produce this:\n\n   col1  col2\n0     4     6\n2     5     5\n\n\nHowever, I don't want to rely on indexes because in my case, I have to deal with dataframes that have distinct indexes.\n\nBy the way, I initially thought about an extension of the current drop_duplicates() method, but now I realize that the second approach using properties of set theory would be far more useful in general. Both approaches solve my current problem, though.\n\nThanks!\n"
'I have dataframe in Pandas for example:\n\nCol1 Col2\nA     1 \nB     2\nC     3\n\n\nNow if I would like to add one more column named Col3 and the value is based on Col2. In formula, if Col2 > 1, then Col3 is 0, otherwise would be 1. So, in the example above. The output would be:\n\nCol1 Col2 Col3\nA    1    1\nB    2    0\nC    3    0\n\n\nAny idea on how to achieve this?\n'
"I have a pandas-Dataframe and use resample() to calculate means (e.g. daily or monthly means).\nHere is a small example. \n\nimport pandas as pd  \nimport numpy as np\n\ndates = pd.date_range('1/1/2000', periods=100)\ndf = pd.DataFrame(np.random.randn(100, 1), index=dates, columns=['A'])\n\nmonthly_mean = df.resample('M', how='mean')\n\n\nHow do I plot the monthly_mean now?\nHow do I manage to use the index of my new created DataFrame monthly_mean as the x-axis?\nThanks in advance.\n"
"I know how to do this in R. But, is there any function in pandas that transforms a dataframe to an nxn co-occurrence matrix containing the counts of two aspects co-occurring. \n\nFor example a matrix df:\n\nimport pandas as pd\n\ndf = pd.DataFrame({'TFD' : ['AA', 'SL', 'BB', 'D0', 'Dk', 'FF'],\n                    'Snack' : ['1', '0', '1', '1', '0', '0'],\n                    'Trans' : ['1', '1', '1', '0', '0', '1'],\n                    'Dop' : ['1', '0', '1', '0', '1', '1']}).set_index('TFD')\n\nprint df\n\n&gt;&gt;&gt; \n    Dop Snack Trans\nTFD                \nAA    1     1     1\nSL    0     0     1\nBB    1     1     1\nD0    0     1     0\nDk    1     0     0\nFF    1     0     1\n\n[6 rows x 3 columns]\n\n\nwould yield:\n\n    Dop Snack Trans\n\nDop   0     2     3\nSnack 2     0     2\nTrans 3     2     0\n\n\nSince the matrix is mirrored on the diagonal I guess there would be a way to optimize code.\n"
"I would like to have:\n\ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n\n\nreturn those columns multiplied by df['mtaz_proportion']\n\nso that I can set \n\ndf[['mtaz_income_1', 'mtaz_income_2']] = \ndf[['income_1', 'income_2']] * df['mtaz_proportion']\n\n\nbut instead I get: \n\nincome_1    income_2    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \n0   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n1   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n2   NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ...\n\n\nect...\n\nwhat simple thing am I missing?\n\nThank you!\n"
"I have a df like so:\n\nimport pandas\na=[['1/2/2014', 'a', '6', 'z1'], \n   ['1/2/2014', 'a', '3', 'z1'], \n   ['1/3/2014', 'c', '1', 'x3'],\n   ]\ndf = pandas.DataFrame.from_records(a[1:],columns=a[0])\n\n\nI want to flatten the df so it is one continuous list like so:\n\n['1/2/2014', 'a', '6', 'z1', '1/2/2014', 'a', '3', 'z1','1/3/2014', 'c', '1', 'x3']\n\nI can loop through the rows and extend to a list, but is a much easier way to do it?\n"
'I have a pandas dataframe with about 20 columns.\n\nIt is possible to replace all occurrences of a string (here a newline) by manually writing all column names:\n\ndf[\'columnname1\'] = df[\'columnname1\'].str.replace("\\n","&lt;br&gt;")\ndf[\'columnname2\'] = df[\'columnname2\'].str.replace("\\n","&lt;br&gt;")\ndf[\'columnname3\'] = df[\'columnname3\'].str.replace("\\n","&lt;br&gt;")\n...\ndf[\'columnname20\'] = df[\'columnname20\'].str.replace("\\n","&lt;br&gt;")\n\n\nThis unfortunately does not work:\n\ndf = df.replace("\\n","&lt;br&gt;")\n\n\nIs there any other, more elegant solution?\n'
"I have a pandas dataframe:\n\n    lat         lng         alt days              date        time\n0   40.003834   116.321462  211 39745.175405      2008-10-24  04:12:35\n1   40.003783   116.321431  201 39745.175463  2008-10-24      04:12:40\n2   40.003690   116.321429  203 39745.175521      2008-10-24      04:12:45\n3   40.003589   116.321427  194 39745.175579      2008-10-24      04:12:50\n4   40.003522   116.321412  190 39745.175637      2008-10-24      04:12:55\n5   40.003509   116.321484  188 39745.175694      2008-10-24      04:13:00\n\n\nFor which I am trying to convert the df['date'] and df['time'] columns into a datetime.  I can do:\n\ndf['Datetime'] = pd.to_datetime(df['date']+df['time'])\ndf = df.set_index(['Datetime'])\ndel df['date']\ndel df['time']\n\n\nAnd I get:\n\n                    lat         lng         alt days\nDatetime                            \n2008-10-2404:12:35  40.003834   116.321462  211 39745.175405    \n2008-10-2404:12:40  40.003783   116.321431  201 39745.175463\n2008-10-2404:12:45  40.003690   116.321429  203 39745.175521    \n2008-10-2404:12:50  40.003589   116.321427  194 39745.175579    \n2008-10-2404:12:55  40.003522   116.321412  190 39745.175637\n\n\nBut then if I try:\n\ndf.between_time(time(1),time(22,59,59))['lng'].std()\n\n\nI get an error - 'TypeError: Index must be DatetimeIndex'\n\nSo, I've also tried setting the DatetimeIndex:\n\ndf['Datetime'] = pd.to_datetime(df['date']+df['time'])\n#df = df.set_index(['Datetime'])\ndf = df.set_index(pd.DatetimeIndex(df['Datetime']))\ndel df['date']\ndel df['time']\n\n\nAnd this throws an error also - 'DateParseError: unknown string format'\n\nHow do I create the datetime column and DatetimeIndex correctly so that df.between_time() works right?\n"
"For the following dataframe:\n\nStationID  HoursAhead    BiasTemp  \nSS0279           0          10\nSS0279           1          20\nKEOPS            0          0\nKEOPS            1          5\nBB               0          5\nBB               1          5\n\n\nI'd like to get something like:\n\nStationID  BiasTemp  \nSS0279     15\nKEOPS      2.5\nBB         5\n\n\nI know I can script something like this to get the desired result:\n\ndef transform_DF(old_df,col):\n    list_stations = list(set(old_df['StationID'].values.tolist()))\n    header = list(old_df.columns.values)\n    header.remove(col)\n    header_new = header\n    new_df = pandas.DataFrame(columns = header_new)\n    for i,station in enumerate(list_stations):\n        general_results = old_df[(old_df['StationID'] == station)].describe()\n        new_row = []\n        for column in header_new:\n            if column in ['StationID']: \n                new_row.append(station)\n                continue\n            new_row.append(general_results[column]['mean'])\n        new_df.loc[i] = new_row\n    return new_df\n\n\nBut I wonder if there is something more straightforward in pandas.\n"
"Given the below pandas DataFrame:\n\nIn [115]: times = pd.to_datetime(pd.Series(['2014-08-25 21:00:00','2014-08-25 21:04:00',\n                                            '2014-08-25 22:07:00','2014-08-25 22:09:00']))\n          locations = ['HK', 'LDN', 'LDN', 'LDN']\n          event = ['foo', 'bar', 'baz', 'qux']\n          df = pd.DataFrame({'Location': locations,\n                             'Event': event}, index=times)\n          df\nOut[115]:\n                               Event Location\n          2014-08-25 21:00:00  foo   HK\n          2014-08-25 21:04:00  bar   LDN\n          2014-08-25 22:07:00  baz   LDN\n          2014-08-25 22:09:00  qux   LDN\n\n\nI would like resample the data to aggregate it hourly by count while grouping by location to produce a data frame that looks like this:\n\nOut[115]:\n                               HK    LDN\n          2014-08-25 21:00:00  1     1\n          2014-08-25 22:00:00  0     2\n\n\nI've tried various combinations of resample() and groupby() but with no luck. How would I go about this?\n"
'       Y1961      Y1962      Y1963      Y1964      Y1965  Region\n0  82.567307  83.104757  83.183700  83.030338  82.831958  US\n1   2.699372   2.610110   2.587919   2.696451   2.846247  US\n2  14.131355  13.690028  13.599516  13.649176  13.649046  US\n3   0.048589   0.046982   0.046583   0.046225   0.051750  US\n4   0.553377   0.548123   0.582282   0.577811   0.620999  US\n\n\nIn the above dataframe, I would like to get average of each row. currently, I am doing this:\n\ndf.mean(axis=0)\n\n\nHowever, this does away with the Region column as well. how can I compute mean and also retain Region column\n'
"My question is very similar to this one, but I need to convert my entire dataframe instead of just a series. The to_numeric function only works on one series at a time and is not a good replacement for the deprecated convert_objects command. Is there a way to get similar results to the convert_objects(convert_numeric=True) command in the new pandas release?\n\nThank you Mike Müller for your example. df.apply(pd.to_numeric) works very well if the values can all be converted to integers. What if in my dataframe I had strings that could not be converted into integers? \nExample: \n\ndf = pd.DataFrame({'ints': ['3', '5'], 'Words': ['Kobe', 'Bryant']})\ndf.dtypes\nOut[59]: \nWords    object\nints     object\ndtype: object\n\n\nThen I could run the deprecated function and get:\n\ndf = df.convert_objects(convert_numeric=True)\ndf.dtypes\nOut[60]: \nWords    object\nints      int64\ndtype: object\n\n\nRunning the apply command gives me errors, even with try and except handling.\n"
"I am curious why a simple concatenation of two data frames in pandas:\n\nshape: (66441, 1)\ndtypes: prediction    int64\ndtype: object\nisnull().sum(): prediction    0\ndtype: int64\n\nshape: (66441, 1)\nCUSTOMER_ID    int64\ndtype: object\nisnull().sum() CUSTOMER_ID    0\ndtype: int64\n\n\nof the same shape and both without NaN values \n\nfoo = pd.concat([initId, ypred], join='outer', axis=1)\nprint(foo.shape)\nprint(foo.isnull().sum())\n\n\ncan result in a lot of NaN values if joined.\n\n(83384, 2)\nCUSTOMER_ID    16943\nprediction     16943\n\n\nHow can I fix this problem and prevent NaN values being introduced?\n\nTrying to reproduce it like \n\naaa  = pd.DataFrame([0,1,0,1,0,0], columns=['prediction'])\nprint(aaa)\nbbb  = pd.DataFrame([0,0,1,0,1,1], columns=['groundTruth'])\nprint(bbb)\npd.concat([aaa, bbb], axis=1)\n\n\nfailed e.g. worked just fine as no NaN values were introduced.\n"
'I\'m trying to do some aggregations on a pandas data frame. Here is a sample code:\n\nimport pandas as pd\n\ndf = pd.DataFrame({"User": ["user1", "user2", "user2", "user3", "user2", "user1"],\n                  "Amount": [10.0, 5.0, 8.0, 10.5, 7.5, 8.0]})\n\ndf.groupby(["User"]).agg({"Amount": {"Sum": "sum", "Count": "count"}})\n\nOut[1]: \n      Amount      \n         Sum Count\nUser              \nuser1   18.0     2\nuser2   20.5     3\nuser3   10.5     1\n\n\nWhich generates the following warning:\n\n\n  FutureWarning: using a dict with renaming is deprecated and will be\n  removed in a future version   return super(DataFrameGroupBy,\n  self).aggregate(arg, *args, **kwargs)\n\n\nHow can I avoid this?\n'
'Is there an easy way to export a data frame (or even a part of it) to LaTeX?  \n\nI searched in google and was only able to find solutions using asciitables.\n'
'One of my favorite aspects of using the ggplot2 library in R is the ability to easily specify aesthetics. I can quickly make a scatterplot and apply color associated with a specific column and I would love to be able to do this with python/pandas/matplotlib.  I\'m wondering if there are there any convenience functions that people use to map colors to values using pandas dataframes and Matplotlib?\n\n##ggplot scatterplot example with R dataframe, `df`, colored by col3\nggplot(data = df, aes(x=col1, y=col2, color=col3)) + geom_point()\n\n##ideal situation with pandas dataframe, \'df\', where colors are chosen by col3\ndf.plot(x=col1,y=col2,color=col3)\n\n\nEDIT:\nThank you for your responses but I want to include a sample dataframe to clarify what I am asking. Two columns contain numerical data and the third is a categorical variable. The script I am thinking of will assign colors based on this value.\n\nimport pandas as pd\ndf = pd.DataFrame({\'Height\':np.random.normal(10),\n                   \'Weight\':np.random.normal(10),\n                   \'Gender\': ["Male","Male","Male","Male","Male",\n                              "Female","Female","Female","Female","Female"]})\n\n'
"I am able to read and slice pandas dataframe using python datetime objects, however I am forced to use only existing dates in index.  For example, this works:\n\n&gt;&gt;&gt; data\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 252 entries, 2010-12-31 00:00:00 to 2010-04-01 00:00:00\nData columns:\nAdj Close    252  non-null values\ndtypes: float64(1)\n\n&gt;&gt;&gt; st = datetime.datetime(2010, 12, 31, 0, 0)\n&gt;&gt;&gt; en = datetime.datetime(2010, 12, 28, 0, 0)\n\n&gt;&gt;&gt; data[st:en]\n            Adj Close\nDate                 \n2010-12-31     593.97\n2010-12-30     598.86\n2010-12-29     601.00\n2010-12-28     598.92\n\n\nHowever if I use a start or end date that is not present in the DF, I get python KeyError.\n\nMy Question : How do I query the dataframe object for a date range; even when the start and end dates are not present in the DataFrame.  Does pandas allow for range based slicing?\n\nI am using pandas version 0.10.1\n"
'import pandas as pd\ndate_stngs = (\'2008-12-20\',\'2008-12-21\',\'2008-12-22\',\'2008-12-23\')\n\na = pd.Series(range(4),index = (range(4)))\n\nfor idx, date in enumerate(date_stngs):\n    a[idx]= pd.to_datetime(date)\n\n\nThis code bit produces error: \n\n\n  TypeError:" \'int\' object is not iterable"\n\n\nCan anyone tell me how to get this series of date time strings into a DataFrame as DateTime objects?\n'
"I have a DataFrame in pandas where some of the numbers are expressed in scientific notation (or exponent notation) like this:\n\n                  id        value\nid              1.00    -4.22e-01\nvalue          -0.42     1.00e+00\npercent        -0.72     1.00e-01\nplayed          0.03    -4.35e-02\nmoney          -0.22     3.37e-01\nother            NaN          NaN\nsy             -0.03     2.19e-04\nsz             -0.33     3.83e-01\n\n\nAnd the scientific notation makes what should be an easy comparison, needlessly difficult. I assume it's the 21900 value that's screwing it up for the others. I mean 1.0 is encoded. ONE! \n\nThis doesn't work:\n\nnp.set_printoptions(supress=True) \n\n\nAnd pandas.set_printoptions doesn't implement suppress either, and I've looked all at pd.describe_options() in despair, and pd.core.format.set_eng_float_format() only seems to turn it on for all the other float values, with no ability to turn it off.\n"
'I have a Pandas dataframe and I want to find all the unique values in that dataframe...irrespective of row/columns. If I have a 10 x 10 dataframe, and suppose they have 84 unique values, I need to find them - Not the count.\n\nI can create a set and add the values of each rows by iterating over the rows of the dataframe. But, I feel that it may be inefficient (cannot justify that). Is there an efficient way to find it? Is there a predefined function?\n'
'I am working with CSV files where several of the columns have a simple json object (several key value pairs) while other columns are normal. Here is an example:\n\nname,dob,stats\njohn smith,1/1/1980,"{""eye_color"": ""brown"", ""height"": 160, ""weight"": 76}"\ndave jones,2/2/1981,"{""eye_color"": ""blue"", ""height"": 170, ""weight"": 85}"\nbob roberts,3/3/1982,"{""eye_color"": ""green"", ""height"": 180, ""weight"": 94}"\n\n\nAfter using df = pandas.read_csv(\'file.csv\'), what\'s the most efficient way to parse and split the stats column into additional columns?\n\nAfter about an hour, the only thing I could come up with was:\n\nimport json\nstdf = df[\'stats\'].apply(json.loads)\nstlst = list(stdf)\nstjson = json.dumps(stlst)\ndf.join(pandas.read_json(stjson))\n\n\nThis seems like I\'m doing it wrong, and it\'s quite a bit of work considering I\'ll need to do this on three columns regularly.\n\nDesired output is the dataframe object below. Added following lines of code to get there in my (crappy) way:\n\ndf = df.join(pandas.read_json(stjson))\ndel(df[\'stats\'])\nIn [14]: df\n\nOut[14]:\n          name       dob eye_color  height  weight\n0   john smith  1/1/1980     brown     160      76\n1   dave jones  2/2/1981      blue     170      85\n2  bob roberts  3/3/1982     green     180      94\n\n'
'in a pandas dataframe how can I apply a sort of excel left(\'state\',2) to only take the first two letters. Ideally I want to learn how to use left,right and mid in a dataframe too. So need an equivalent and not a "trick" for this specific example.\n\ndata = {\'state\': [\'Auckland\', \'Otago\', \'Wellington\', \'Dunedin\', \'Hamilton\'],\n\'year\': [2000, 2001, 2002, 2001, 2002],\n\'pop\': [1.5, 1.7, 3.6, 2.4, 2.9]}\ndf = pd.DataFrame(data)\n\nprint df\n\n     pop       state  year\n 0  1.5    Auckland  2000\n 1  1.7       Otago  2001\n 2  3.6  Wellington  2002\n 3  2.4     Dunedin  2001\n 4  2.9    Hamilton  2002\n\n\nI want to get this:\n\n    pop       state     year  StateInitial\n 0  1.5       Auckland    2000     Au\n 1  1.7       Otago       2001     Ot\n 2  3.6       Wellington  2002     We\n 3  2.4       Dunedin     2001     Du\n 4  2.9       Hamilton    2002     Ha\n\n'
"when I use this syntax it creates a series rather than adding a column to my new dataframe (sum). Please help.\n\nMy code: \n\nsum = data['variance'] = data.budget + data.actual\n\n\nMy Data (in dataframe df): (currently has everything except the budget - actual, I want to create a variance column?\n\n    cluster     date    budget  actual          | budget - actual\n0   a   2014-01-01 00:00:00     11000   10000       1000\n1   a   2014-02-01 00:00:00     1200    1000\n2   a   2014-03-01 00:00:00     200     100\n3   b   2014-04-01 00:00:00     200     300\n4   b   2014-05-01 00:00:00     400     450\n5   c   2014-06-01 00:00:00     700     1000\n6   c   2014-07-01 00:00:00     1200    1000\n7   c   2014-08-01 00:00:00     200     100\n8   c   2014-09-01 00:00:00     200     300\n\n"
'I have a DataFrame that looks like\n\n  Emp1    Empl2           date       Company\n0    0        0     2012-05-01         apple\n1    0        1     2012-05-29         apple\n2    0        1     2013-05-02         apple\n3    0        1     2013-11-22         apple\n18   1        0     2011-09-09        google\n19   1        0     2012-02-02        google\n20   1        0     2012-11-26        google\n21   1        0     2013-05-11        google\n\n\nI want to pass the company and date for setting a MultiIndex for this DataFrame. Currently it has a default index. I am using df.set_index([\'Company\', \'date\'], inplace=True)\n\ndf = pd.DataFrame()\nfor c in company_list:\n        row = pd.DataFrame([dict(company = \'%s\' %s, date = datetime.date(2012, 05, 01))])\n        df = df.append(row, ignore_index = True)\n        for e in emp_list:\n            dataset  = pd.read_sql("select company, emp_name, date(date), count(*) from company_table where  = \'"+s+"\' and emp_name = \'"+b+"\' group by company, date, name LIMIT 5 ", con)\n                if len(dataset) == 0:\n                row = pd.DataFrame([dict(sitename=\'%s\' %s, name = \'%s\' %b, date = datetime.date(2012, 05, 01), count = np.nan)])\n                dataset = dataset.append(row, ignore_index=True)\n            dataset = dataset.rename(columns = {\'count\': \'%s\' %b})\n            dataset = dataset.groupby([\'company\', \'date\', \'emp_name\'], as_index = False).sum()\n\n            dataset = dataset.drop(\'emp_name\', 1)\n            df = pd.merge(df, dataset, how = \'\')\n            df = df.sort(\'date\', ascending = True)\n            df.fillna(0, inplace = True)\n\ndf.set_index([\'Company\', \'date\'], inplace=True)            \nprint df\n\n\nBut when I print this DataFrame, it prints None. I saw this solution from stackoverflow it self. Is this not the correct way of doing it. Also I want to shuffle the positions of the columns company and date so that company becomes the first index, and date becomes the second in Hierarchy. Any ideas on this?\n'
'I have a dataframe, grouped, with multiindex columns as below:\n\nimport pandas as pd\ncodes = ["one","two","three"];\ncolours = ["black", "white"];\ntextures = ["soft", "hard"];\nN= 100 # length of the dataframe\ndf = pd.DataFrame({ \'id\' : range(1,N+1),\n                    \'weeks_elapsed\' : [random.choice(range(1,25)) for i in range(1,N+1)],\n                    \'code\' : [random.choice(codes) for i in range(1,N+1)],\n                    \'colour\': [random.choice(colours) for i in range(1,N+1)],\n                    \'texture\': [random.choice(textures) for i in range(1,N+1)],\n                    \'size\': [random.randint(1,100) for i in range(1,N+1)],\n                    \'scaled_size\': [random.randint(100,1000) for i in range(1,N+1)]\n                   },  columns= [\'id\', \'weeks_elapsed\', \'code\',\'colour\', \'texture\', \'size\', \'scaled_size\'])\ngrouped = df.groupby([\'code\', \'colour\']).agg( {\'size\': [np.sum, np.average, np.size, pd.Series.idxmax],\'scaled_size\': [np.sum, np.average, np.size, pd.Series.idxmax]}).reset_index()\n\n&gt;&gt; grouped\n    code colour     size                           scaled_size                         \n                    sum    average  size  idxmax            sum    average  size  idxmax\n0    one  black    1031  60.647059    17      81     185.153944  10.891408    17      47\n1    one  white     481  37.000000    13      53     204.139249  15.703019    13      53\n2  three  black     822  48.352941    17       6     123.269405   7.251141    17      31\n3  three  white    1614  57.642857    28      50     285.638337  10.201369    28      37\n4    two  black     523  58.111111     9      85      80.908912   8.989879     9      88\n5    two  white     669  41.812500    16      78      82.098870   5.131179    16      78\n[6 rows x 10 columns]\n\n\nHow can I flatten/merge the column index levels as: "Level1|Level2", e.g. size|sum, scaled_size|sum. etc? If this is not possible, is there a way to groupby() as I did above without creating multi-index columns?\n'
"I've been looking around for ways to select columns through the python documentation and the forums but every example on indexing columns are too simplistic. \n\nSuppose I have a 10 x 10 dataframe\n\ndf = DataFrame(randn(10, 10), index=range(0,10), columns=['A', 'B', 'C', 'D','E','F','G','H','I','J'])\n\n\nSo far, all the documentations gives is just a simple example of indexing like\n\nsubset = df.loc[:,'A':'C']\n\n\nor\n\nsubset = df.loc[:,'C':]\n\n\nBut I get an error when I try index multiple, non-sequential columns, like this\n\nsubset = df.loc[:,('A':'C', 'E')]\n\n\nHow would I index in Pandas if I wanted to select column A to C, E, and G to I? It appears that this logic will not work\n\nsubset = df.loc[:,('A':'C', 'E', 'G':'I')]\n\n\nI feel that the solution is pretty simple, but I can't get around this error. Thanks!\n"
"trying to write pandas dataframe to MySQL table using to_sql.  Previously been using flavor='mysql', however it will be depreciated in the future and wanted to start the transition to using SQLAlchemy engine.\n\nsample code:\n\nimport pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ncnx = engine.raw_connection()\ndata = pd.read_sql('SELECT * FROM sample_table', cnx)\ndata.to_sql(name='sample_table2', con=cnx, if_exists = 'append', index=False)\n\n\nThe read works fine but the to_sql has an error:\n\n\n  DatabaseError: Execution failed on sql 'SELECT name FROM sqlite_master\n  WHERE type='table' AND name=?;': Wrong number of arguments during\n  string formatting\n\n\nWhy does it look like it is trying to use sqlite? What is the correct use of a sqlalchemy connection with mysql and specifically mysql.connector?\n\nI also tried passing the engine in as the connection as well, and that gave me an error referencing no cursor object.\n\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n&gt;&gt;AttributeError: 'Engine' object has no attribute 'cursor'\n\n"
"I would like to extract a week number from data in a pandas dataframe.\n\nThe date format is datetime64[ns]\n\nI have normalized the date to remove the time from it\n\ndf['Date'] = df['Date'].apply(pd.datetools.normalize_date)\n\n\nso the date now looks like - 2015-06-17 in the data frame column\n\nand now I like to convert that to a week number.\n\nThanks in advance\n"
"I would like to run a pivot on a pandas DataFrame, with the index being two columns, not one. For example, one field for the year, one for the month, an 'item' field which shows 'item 1' and 'item 2' and a 'value' field with numerical values. I want the index to be year + month.\n\nThe only way I managed to get this to work was to combine the two fields into one, then separate them again. is there a better way?\n\nMinimal code copied below. Thanks a lot!\n\nPS Yes, I am aware there are other questions with the keywords 'pivot' and 'multi-index', but I did not understand if/how they can help me with this question.\n\nimport pandas as pd\nimport numpy as np\n\ndf= pd.DataFrame()\nmonth = np.arange(1, 13)\nvalues1 = np.random.randint(0, 100, 12)\nvalues2 = np.random.randint(200, 300, 12)\n\n\ndf['month'] = np.hstack((month, month))\ndf['year'] = 2004\ndf['value'] = np.hstack((values1, values2))\ndf['item'] = np.hstack((np.repeat('item 1', 12), np.repeat('item 2', 12)))\n\n# This doesn't work: \n# ValueError: Wrong number of items passed 24, placement implies 2\n# mypiv = df.pivot(['year', 'month'], 'item', 'value')\n\n# This doesn't work, either:\n# df.set_index(['year', 'month'], inplace=True)\n# ValueError: cannot label index with a null key\n# mypiv = df.pivot(columns='item', values='value')\n\n# This below works but is not ideal: \n# I have to first concatenate then separate the fields I need\ndf['new field'] = df['year'] * 100 + df['month']\n\nmypiv = df.pivot('new field', 'item', 'value').reset_index()\nmypiv['year'] = mypiv['new field'].apply( lambda x: int(x) / 100)  \nmypiv['month'] = mypiv['new field'] % 100\n\n"
"How do you iterate over a Pandas Series generated from a .groupby('...').size() command and get both the group name and count.\n\nAs an example if I have:\n\nfoo\n-1     7\n 0    85\n 1    14\n 2     5\n\n\nhow can I loop over them so the that each iteration I would have -1 &amp; 7, 0 &amp; 85, 1 &amp; 14 and 2 &amp; 5 in variables?  \n\nI tried the enumerate option but it doesn't quite work.  Example:\n\nfor i, row in enumerate(df.groupby(['foo']).size()):\n    print(i, row)\n\n\nit doesn't return -1, 0, 1, and 2 for i but rather 0, 1, 2, 3.\n"
"I am using pyspark to read a parquet file like below:\n\nmy_df = sqlContext.read.parquet('hdfs://myPath/myDB.db/myTable/**')\n\n\nThen when I do my_df.take(5), it will show [Row(...)], instead of a table  format like when we use the pandas data frame.\n\nIs it possible to display the data frame in a table format like pandas data frame? Thanks!\n"
"Example Problem\n\nAs a simple example, consider the numpy array arr as defined below:\n\nimport numpy as np\narr = np.array([[5, np.nan, np.nan, 7, 2],\n                [3, np.nan, 1, 8, np.nan],\n                [4, 9, 6, np.nan, np.nan]])\n\n\nwhere arr looks like this in console output:\n\narray([[  5.,  nan,  nan,   7.,   2.],\n       [  3.,  nan,   1.,   8.,  nan],\n       [  4.,   9.,   6.,  nan,  nan]])\n\n\nI would now like to row-wise 'forward-fill' the nan values in array arr. By that I mean replacing each nan value with the nearest valid value from the left. The desired result would look like this:\n\narray([[  5.,   5.,   5.,  7.,  2.],\n       [  3.,   3.,   1.,  8.,  8.],\n       [  4.,   9.,   6.,  6.,  6.]])\n\n\n\n\nTried thus far\n\nI've tried using for-loops:\n\nfor row_idx in range(arr.shape[0]):\n    for col_idx in range(arr.shape[1]):\n        if np.isnan(arr[row_idx][col_idx]):\n            arr[row_idx][col_idx] = arr[row_idx][col_idx - 1]\n\n\nI've also tried using a pandas dataframe as an intermediate step (since pandas dataframes have a very neat built-in method for forward-filling):\n\nimport pandas as pd\ndf = pd.DataFrame(arr)\ndf.fillna(method='ffill', axis=1, inplace=True)\narr = df.as_matrix()\n\n\nBoth of the above strategies produce the desired result, but I keep on wondering: wouldn't a strategy that uses only numpy vectorized operations be the most efficient one?\n\n\n\nSummary\n\nIs there another more efficient way to 'forward-fill' nan values in numpy arrays? (e.g. by using numpy vectorized operations)\n\n\n\nUpdate: Solutions Comparison\n\nI've tried to time all solutions thus far. This was my setup script:\n\nimport numba as nb\nimport numpy as np\nimport pandas as pd\n\ndef random_array():\n    choices = [1, 2, 3, 4, 5, 6, 7, 8, 9, np.nan]\n    out = np.random.choice(choices, size=(1000, 10))\n    return out\n\ndef loops_fill(arr):\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\n@nb.jit\ndef numba_loops_fill(arr):\n    '''Numba decorator solution provided by shx2.'''\n    out = arr.copy()\n    for row_idx in range(out.shape[0]):\n        for col_idx in range(1, out.shape[1]):\n            if np.isnan(out[row_idx, col_idx]):\n                out[row_idx, col_idx] = out[row_idx, col_idx - 1]\n    return out\n\ndef pandas_fill(arr):\n    df = pd.DataFrame(arr)\n    df.fillna(method='ffill', axis=1, inplace=True)\n    out = df.as_matrix()\n    return out\n\ndef numpy_fill(arr):\n    '''Solution provided by Divakar.'''\n    mask = np.isnan(arr)\n    idx = np.where(~mask,np.arange(mask.shape[1]),0)\n    np.maximum.accumulate(idx,axis=1, out=idx)\n    out = arr[np.arange(idx.shape[0])[:,None], idx]\n    return out\n\n\nfollowed by this console input:\n\n%timeit -n 1000 loops_fill(random_array())\n%timeit -n 1000 numba_loops_fill(random_array())\n%timeit -n 1000 pandas_fill(random_array())\n%timeit -n 1000 numpy_fill(random_array())\n\n\nresulting in this console output:\n\n1000 loops, best of 3: 9.64 ms per loop\n1000 loops, best of 3: 377 µs per loop\n1000 loops, best of 3: 455 µs per loop\n1000 loops, best of 3: 351 µs per loop\n\n"
"I have data of the following form:\n\ndf = pd.DataFrame({\n    'group': [1, 1, 2, 3, 3, 3, 4],\n    'param': ['a', 'a', 'b', np.nan, 'a', 'a', np.nan]\n})\nprint(df)\n\n#    group param\n# 0      1     a\n# 1      1     a\n# 2      2     b\n# 3      3   NaN\n# 4      3     a\n# 5      3     a\n# 6      4   NaN\n\n\nNon-null values within groups are always the same. I want to count the non-null value for each group (where it exists) once, and then find the total counts for each value. \n\nI'm currently doing this in the following (clunky and inefficient) way:\n\nparam = []\nfor _, group in df[df.param.notnull()].groupby('group'):\n    param.append(group.param.unique()[0])\nprint(pd.DataFrame({'param': param}).param.value_counts())\n\n# a    2\n# b    1\n\n\nI'm sure there's a way to do this more cleanly and without using a loop, but I just can't seem to work it out. Any help would be much appreciated.\n"
'I have a dataframe which is structured as:\n\n          Date   ticker  adj_close \n0   2016-11-21     AAPL    111.730     \n1   2016-11-22     AAPL    111.800    \n2   2016-11-23     AAPL    111.230    \n3   2016-11-25     AAPL    111.790     \n4   2016-11-28     AAPL    111.570    \n...          \n8   2016-11-21      ACN    119.680            \n9   2016-11-22      ACN    119.480              \n10  2016-11-23      ACN    119.820              \n11  2016-11-25      ACN    120.740 \n...             \n\n\nHow can I plot based on the ticker the adj_close versus Date?  \n'
"I have a data frame like this which is imported from a CSV.\n\n              stock  pop\nDate\n2016-01-04  325.316   82\n2016-01-11  320.036   83\n2016-01-18  299.169   79\n2016-01-25  296.579   84\n2016-02-01  295.334   82\n2016-02-08  309.777   81\n2016-02-15  317.397   75\n2016-02-22  328.005   80\n2016-02-29  315.504   81\n2016-03-07  328.802   81\n2016-03-14  339.559   86\n2016-03-21  352.160   82\n2016-03-28  348.773   84\n2016-04-04  346.482   83\n2016-04-11  346.980   80\n2016-04-18  357.140   75\n2016-04-25  357.439   77\n2016-05-02  356.443   78\n2016-05-09  365.158   78\n2016-05-16  352.160   72\n2016-05-23  344.540   74\n2016-05-30  354.998   81\n2016-06-06  347.428   77\n2016-06-13  341.053   78\n2016-06-20  363.515   80\n2016-06-27  349.669   80\n2016-07-04  371.583   82\n2016-07-11  358.335   81\n2016-07-18  362.021   79\n2016-07-25  368.844   77\n...             ...  ...\n\n\nI wanted to add a new column MA which calculates Rolling mean for the column pop. I tried the following\n\ndf['MA']=data.rolling(5,on='pop').mean()\n\n\nI get an error \n\nValueError: Wrong number of items passed 2, placement implies 1\n\n\nSo I thought let me try if it just works without adding a column. I used \n\n data.rolling(5,on='pop').mean()\n\n\nI got the output \n\n               stock  pop\nDate\n2016-01-04       NaN   82\n2016-01-11       NaN   83\n2016-01-18       NaN   79\n2016-01-25       NaN   84\n2016-02-01  307.2868   82\n2016-02-08  304.1790   81\n2016-02-15  303.6512   75\n2016-02-22  309.4184   80\n2016-02-29  313.2034   81\n2016-03-07  319.8970   81\n2016-03-14  325.8534   86\n2016-03-21  332.8060   82\n2016-03-28  336.9596   84\n2016-04-04  343.1552   83\n2016-04-11  346.7908   80\n2016-04-18  350.3070   75\n2016-04-25  351.3628   77\n2016-05-02  352.8968   78\n2016-05-09  356.6320   78\n2016-05-16  357.6680   72\n2016-05-23  355.1480   74\n2016-05-30  354.6598   81\n2016-06-06  352.8568   77\n2016-06-13  348.0358   78\n2016-06-20  350.3068   80\n2016-06-27  351.3326   80\n2016-07-04  354.6496   82\n2016-07-11  356.8310   81\n2016-07-18  361.0246   79\n2016-07-25  362.0904   77\n...              ...  ...\n\n\nI can't seem to apply Rolling mean on the column pop. What am I doing wrong?\n"
"Objective and Motivation\n\nI've seen this kind of question several times over and have seen many other questions that involve some element of this.  Most recently, I had to spend a bit of time explaining this concept in comments while looking for an appropriate canonical Q&amp;A.  I did not find one and so I thought I'd write one.\n\nThis question usually arises with respect to a specific operation but equally applies to most arithmetic operations.\n\n\nHow do I subtract a Series from every column in a DataFrame?\nHow do I add a Series from every column in a DataFrame?\nHow do I multiply a Series from every column in a DataFrame?\nHow do I divide a Series from every column in a DataFrame?\n\n\nThe Question\n\nGiven a Series s and DataFrame df.  How do I operate on each column of df with s?\n\ndf = pd.DataFrame(\n    [[1, 2, 3], [4, 5, 6]],\n    index=[0, 1],\n    columns=['a', 'b', 'c']\n)\n\ns = pd.Series([3, 14], index=[0, 1])\n\n\nWhen I attempt to add them, I get all np.nan\n\ndf + s\n\n    a   b   c   0   1\n0 NaN NaN NaN NaN NaN\n1 NaN NaN NaN NaN NaN\n\n\nWhat I thought I should get is\n\n    a   b   c\n0   4   5   6\n1  18  19  20\n\n"
"I have two dataframes, both indexed by timeseries.  I need to add the elements together to form a new dataframe, but only if the index and column are the same.  If the item does not exist in one of the dataframes then it should be treated as a zero.\n\nI've tried using .add but this sums regardless of index and column.  Also tried a simple combined_data = dataframe1 + dataframe2 but this give a NaN if both dataframes don't have the element.\n\nAny suggestions?\n"
"I know that I can get the unique values of a DataFrame by resetting the index but is there a way to avoid this step and get the unique values directly?\n\nGiven I have:\n\n        C\n A B     \n 0 one  3\n 1 one  2\n 2 two  1\n\n\nI can do:\n\ndf = df.reset_index()\nuniq_b = df.B.unique()\ndf = df.set_index(['A','B'])\n\n\nIs there a way built in pandas to do this?\n"
'I have trouble querying a table of > 5 million records from MS SQL Server database. I want to select all of the records, but my code seems to fail when selecting to much data into memory. \n\nThis works:\n\nimport pandas.io.sql as psql\nsql = "SELECT TOP 1000000 * FROM MyTable" \ndata = psql.read_frame(sql, cnxn)\n\n\n...but this does not work:\n\nsql = "SELECT TOP 2000000 * FROM MyTable" \ndata = psql.read_frame(sql, cnxn)\n\n\nIt returns this error:\n\nFile "inference.pyx", line 931, in pandas.lib.to_object_array_tuples\n(pandas\\lib.c:42733) Memory Error\n\n\nI have read here that a similar problem exists when creating a dataframe from a csv file, and that the work-around is to use the \'iterator\' and \'chunksize\' parameters like this:\n\nread_csv(\'exp4326.csv\', iterator=True, chunksize=1000)\n\n\nIs there a similar solution for querying from an SQL database?  If not, what is the preferred work-around?  Should I use some other methods to read the records in chunks?  I read a bit of discussion here about working with large datasets in pandas, but it seems like a lot of work to execute a SELECT * query.  Surely there is a simpler approach.\n'
'Say that I have a dataframe that looks like:\n\nName Group_Id\nAAA  1\nABC  1\nCCC  2\nXYZ  2\nDEF  3 \nYYH  3\n\n\nHow could I randomly select one (or more) row for each Group_Id? Say that I want one random draw per Group_Id, I would get:\n\nName Group_Id\nAAA  1\nXYZ  2\nDEF  3\n\n'
"I have a two dimensional (or more) pandas DataFrame like this:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame([[0,1],[2,3],[4,5]], columns=['A', 'B'])\n&gt;&gt;&gt; df\n   A  B\n0  0  1\n1  2  3\n2  4  5\n\n\nNow suppose I have a numpy array like np.array([2,3]) and want to check if there is any row in df that matches with the contents of my array. Here the answer should obviously true but eg. np.array([1,2]) should return false as there is no row with both 1 in column A and 2 in column B.\n\nSure this is easy but don't see it right now.\n"
"I've frequented used pandas' agg() function to run summary statistics on every column of a data.frame.  For example, here's how you would produce the mean and standard deviation:\n\ndf = pd.DataFrame({'A': ['group1', 'group1', 'group2', 'group2', 'group3', 'group3'],\n                   'B': [10, 12, 10, 25, 10, 12],\n                   'C': [100, 102, 100, 250, 100, 102]})\n\n&gt;&gt;&gt; df\n[output]\n        A   B    C\n0  group1  10  100\n1  group1  12  102\n2  group2  10  100\n3  group2  25  250\n4  group3  10  100\n5  group3  12  102\n\n\nIn both of those cases, the order that individual rows are sent to the agg function does not matter.  But consider the following example, which:\n\ndf.groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\n\n[output]\n\n        mean  &lt;lambda&gt;  mean  &lt;lambda&gt;\nA                                     \ngroup1  11.0        12   101       102\ngroup2  17.5        25   175       250\ngroup3  11.0        12   101       102\n\n\nIn this case the lambda functions as intended, outputting the second row in each group.  However, I have not been able to find anything in the pandas documentation that implies that this is guaranteed to be true in all cases.  I want use agg() along with a weighted average function, so I want to be sure that the rows that come into the function will be in the same order as they appear in the original data frame.\n\nDoes anyone know, ideally via somewhere in the docs or pandas source code, if this is guaranteed to be the case?\n"
"I'm working in Python with a pandas DataFrame of video games, each with a genre. I'm trying to remove any video game with a genre that appears less than some number of times in the DataFrame, but I have no clue how to go about this. I did find a StackOverflow question that seems to be related, but I can't decipher the solution at all (possibly because I've never heard of R and my memory of functional programming is rusty at best).\n\nHelp?\n"
'Is there a way to plot the CDF + cumulative histogram of a Pandas Series in Python using Seaborn only? I have the following:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ns = pd.Series(np.random.normal(size=1000))\n\n\nI know I can plot the cumulative histogram with s.hist(cumulative=True, normed=1), and I know I can then plot the CDF using sns.kdeplot(s, cumulative=True), but I want something that can do both in Seaborn, just like when plotting a distribution with sns.distplot(s), which gives both the kde fit and the histogram. Is there a way?\n'
"In my code, I have several variables which can either contain a pandas DataFrame or nothing at all.  Let's say I want to test and see if a certain DataFrame has been created yet or not.  My first thought would be to test for it like this:\n\nif df1:\n    # do something\n\n\nHowever, that code fails in this way:\n\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\nFair enough.  Ideally, I would like to have a presence test that works for either a DataFrame or Python None.  \n\nHere is one way this can work:\n\nif not isinstance(df1, type(None)):\n    # do something\n\n\nHowever, testing for type is really slow.\n\nt = timeit.Timer('if None: pass')\nt.timeit()\n# approximately 0.04\nt = timeit.Timer('if isinstance(x, type(None)): pass', setup='x=None')\nt.timeit()\n# approximately 0.4\n\n\nOuch.  Along with being slow, testing for NoneType isn't very flexible, either.  \n\nA different solution would be to initialize df1 as an empty DataFrame, so that the type would be the same in both the null and non-null cases.  I could then just test using len(), or any(), or something like that.  Making an empty DataFrame seems kind of silly and wasteful, though.\n\nAnother solution would be to have an indicator variable: df1_exists, which is set to False until df1 is created.  Then, instead of testing df1, I would be testing df1_exists.  But this doesn't seem all that elegant, either.\n\nIs there a better, more Pythonic way of handling this issue?  Am I missing something, or is this just an awkward side effect all the awesome things about pandas? \n"
"I'm currently using pandas to read an Excel file and present its sheet names to the user, so he can select which sheet he would like to use. The problem is that the files are really big (70 columns x 65k rows), taking up to 14s to load on a notebook (the same data in a CSV file is taking 3s).\n\nMy code in panda goes like this:\n\nxls = pandas.ExcelFile(path)\nsheets = xls.sheet_names\n\n\nI tried xlrd before, but obtained similar results. This was my code with xlrd:\n\nxls = xlrd.open_workbook(path)\nsheets = xls.sheet_names\n\n\nSo, can anybody suggest a faster way to retrieve the sheet names from an Excel file than reading the whole file?\n"
"I have read a csv file and pivoted it to get to following structure:\n\npivoted = df.pivot('user_id', 'group', 'value')\nlookup = df.drop_duplicates('user_id')[['user_id', 'group']]\nlookup.set_index(['user_id'], inplace=True)\nresult = pivoted.join(lookup)\nresult = result.fillna(0) \n\n\nSection of the result:\n\n             0     1     2    3     4    5   6  7    8   9  10  11  12  13  group\nuser_id                                                                      \n2        33653  2325   916  720   867  187  31  0    6   3  42  56  92  15    l-1\n4        18895   414  1116  570  1190   55  92  0  122  23  78   6   4   2    l-2 \n16        1383    70    27   17    17    1   0  0    0   0   1   0   0   0    l-2\n50         396    72    34    5    18    0   0  0    0   0   0   0   0   0    l-3\n51        3915  1170   402  832  2791  316  12  5  118  51  32   9  62  27    l-4\n\n\nI want to sum across column 0 to column 13 by each row and divide each cell by the sum of that row. I am still getting used to pandas; if I understand correctly, we should try to avoid for loops when doing things like this? In other words, how can I do this in a  'pandas' way? \n"
'I am trying to search through a Pandas Dataframe to find where it has a missing entry or a NaN entry.\n\nHere is a dataframe that I am working with:\n\ncl_id       a           c         d         e        A1              A2             A3\n    0       1   -0.419279  0.843832 -0.530827    text76        1.537177      -0.271042\n    1       2    0.581566  2.257544  0.440485    dafN_6        0.144228       2.362259\n    2       3   -1.259333  1.074986  1.834653    system                       1.100353\n    3       4   -1.279785  0.272977  0.197011     Fifty       -0.031721       1.434273\n    4       5    0.578348  0.595515  0.553483   channel        0.640708       0.649132\n    5       6   -1.549588 -0.198588  0.373476     audio       -0.508501               \n    6       7    0.172863  1.874987  1.405923    Twenty             NaN            NaN\n    7       8   -0.149630 -0.502117  0.315323  file_max             NaN            NaN\n\n\nNOTE: The blank entries are empty strings - this is because there was no alphanumeric content in the file that the dataframe came from.\n\nIf I have this dataframe, how can I find a list with the indexes where the NaN or blank entry occurs?\n'
"I have data frames with column names (coming from .csv files) containing ( and ) and I'd like to replace them with _.\n\nHow can I do that in place for all columns?\n"
"I am using the following code to create a data frame from a list:\n\ntest_list = ['a','b','c','d']\ndf_test = pd.DataFrame.from_records(test_list, columns=['my_letters'])\ndf_test\n\n\nThe above code works fine. Then I tried the same approach for another list:\n\nimport pandas as pd\nq_list = ['112354401', '116115526', '114909312', '122425491', '131957025', '111373473']\ndf1 = pd.DataFrame.from_records(q_list, columns=['q_data'])\ndf1\n\n\nBut it gave me the following errors this time:\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-24-99e7b8e32a52&gt; in &lt;module&gt;()\n      1 import pandas as pd\n      2 q_list = ['112354401', '116115526', '114909312', '122425491', '131957025', '111373473']\n----&gt; 3 df1 = pd.DataFrame.from_records(q_list, columns=['q_data'])\n      4 df1\n\n/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py in from_records(cls, data, index, exclude, columns, coerce_float, nrows)\n   1021         else:\n   1022             arrays, arr_columns = _to_arrays(data, columns,\n-&gt; 1023                                              coerce_float=coerce_float)\n   1024 \n   1025             arr_columns = _ensure_index(arr_columns)\n\n/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py in _to_arrays(data, columns, coerce_float, dtype)\n   5550         data = lmap(tuple, data)\n   5551         return _list_to_arrays(data, columns, coerce_float=coerce_float,\n-&gt; 5552                                dtype=dtype)\n   5553 \n   5554 \n\n/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py in _list_to_arrays(data, columns, coerce_float, dtype)\n   5607         content = list(lib.to_object_array(data).T)\n   5608     return _convert_object_array(content, columns, dtype=dtype,\n-&gt; 5609                                  coerce_float=coerce_float)\n   5610 \n   5611 \n\n/usr/local/lib/python3.4/dist-packages/pandas/core/frame.py in _convert_object_array(content, columns, coerce_float, dtype)\n   5666             # caller's responsibility to check for this...\n   5667             raise AssertionError('%d columns passed, passed data had %s '\n-&gt; 5668                                  'columns' % (len(columns), len(content)))\n   5669 \n   5670     # provide soft conversion of object dtypes\n\nAssertionError: 1 columns passed, passed data had 9 columns\n\n\nWhy would the same approach work for one list but not another? Any idea what might be wrong here? Thanks a lot!\n"
"I have a pandas dataframe with a column called my_labels which contains strings: 'A', 'B', 'C', 'D', 'E'. I would like to count the number of occurances of each of these strings then divide the number of counts by the sum of all the counts. I'm trying to do this in Pandas like this:\n\nfunc = lambda x: x.size() / x.sum()\ndata = frame.groupby('my_labels').apply(func)\n\n\nThis code throws an error, 'DataFrame object has no attribute 'size'. How can I apply a function to calculate this in Pandas?\n"
'I have a very large dataset were I want to replace strings with numbers. I would like to operate on the dataset without typing a mapping function for each key (column) in the dataset. (similar to the fillna method, but replace specific string with assosiated value).\nIs there anyway to do this?\n\nHere is an example of my dataset\n\ndata\n   resp          A          B          C\n0     1       poor       poor       good\n1     2       good       poor       good\n2     3  very good  very good  very good\n3     4       bad        poor       bad \n4     5   very bad   very bad   very bad\n5     6       poor       good   very bad\n6     7       good       good       good\n7     8  very good  very good  very good\n8     9       bad        bad    very bad\n9    10   very bad   very bad   very bad\n\n\nThe desired result:\n\n data\n   resp  A  B  C\n0      1  3  3  4\n1     2  4  3  4\n2     3  5  5  5\n3     4  2  3  2\n4     5  1  1  1\n5     6  3  4  1\n6     7  4  4  4\n7     8  5  5  5\n8     9  2  2  1\n9    10  1  1  1\n\n\nvery bad=1, bad=2, poor=3, good=4, very good=5\n\n//Jonas\n'
"this is a rather similar question to this question but with one key difference: I'm selecting the data I want to change not by its index but by some criteria.\n\nIf the criteria I apply return a single row, I'd expect to be able to set the value of a certain column in that row in an easy way, but my first attempt doesn't work:\n\n&gt;&gt;&gt; d = pd.DataFrame({'year':[2008,2008,2008,2008,2009,2009,2009,2009], \n...                   'flavour':['strawberry','strawberry','banana','banana',\n...                   'strawberry','strawberry','banana','banana'],\n...                   'day':['sat','sun','sat','sun','sat','sun','sat','sun'],\n...                   'sales':[10,12,22,23,11,13,23,24]})\n\n&gt;&gt;&gt; d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n\n&gt;&gt;&gt; d[d.sales==24]\n   day flavour  sales  year\n7  sun  banana     24  2009\n\n&gt;&gt;&gt; d[d.sales==24].sales = 100\n&gt;&gt;&gt; d\n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana     24  2009\n\n\nSo rather than setting 2009 Sunday's Banana sales to 100, nothing happens! What's the nicest way to do this? Ideally the solution should use the row number, as you normally don't know that in advance!\n"
'Looks ugly:\n\ndf_cut = df_new[\n             (\n             (df_new[\'l_ext\']==31) |\n             (df_new[\'l_ext\']==22) |\n             (df_new[\'l_ext\']==30) |\n             (df_new[\'l_ext\']==25) |\n             (df_new[\'l_ext\']==64)\n             )\n            ]\n\n\nDoes not work:\n\ndf_cut = df_new[(df_new[\'l_ext\'] in [31, 22, 30, 25, 64])]\n\n\nIs there an elegant and working solution of the above "problem"?\n'
'First I\'m new to pandas, but I\'m already falling in love with it.  I\'m trying to implement the equivalent of the Lag function from Oracle.\n\nLet\'s suppose you have this DataFrame:\n\nDate                   Group      Data\n2014-05-14 09:10:00        A         1\n2014-05-14 09:20:00        A         2\n2014-05-14 09:30:00        A         3\n2014-05-14 09:40:00        A         4\n2014-05-14 09:50:00        A         5\n2014-05-14 10:00:00        B         1\n2014-05-14 10:10:00        B         2\n2014-05-14 10:20:00        B         3\n2014-05-14 10:30:00        B         4\n\n\nIf this was an oracle database and I wanted to create a lag function grouped by the "Group" column and ordered by the Date I could easily use this function:\n\n LAG(Data,1,NULL) OVER (PARTITION BY Group ORDER BY Date ASC) AS Data_lagged\n\n\nThis would result in the following Table:\n\nDate                   Group     Data    Data lagged\n2014-05-14 09:10:00        A        1           Null\n2014-05-14 09:20:00        A        2            1\n2014-05-14 09:30:00        A        3            2\n2014-05-14 09:40:00        A        4            3\n2014-05-14 09:50:00        A        5            4\n2014-05-14 10:00:00        B        1           Null\n2014-05-14 10:10:00        B        2            1\n2014-05-14 10:20:00        B        3            2\n2014-05-14 10:30:00        B        4            3\n\n\nIn pandas I can set the date to be an index and use the shift method:\n\ndb["Data_lagged"] = db.Data.shift(1)\n\n\nThe only issue is that this doesn\'t group by a column.  Even if I set the two columns Date and Group as indexes, I would still get the "5" in the lagged column.\n\nIs there a way to implement the equivalent of the Lead and lag functions in Pandas?\n'
"I am trying to get the max value from a panda dataframe as whole. I am not interested in what row or column it came from. I am just interested in a single max value within the dataframe.\n\nHere is my dataframe:\n\ndf = pd.DataFrame({'group1': ['a','a','a','b','b','b','c','c','d','d','d','d','d'],\n                        'group2': ['c','c','d','d','d','e','f','f','e','d','d','d','e'],\n                        'value1': [1.1,2,3,4,5,6,7,8,9,1,2,3,4],\n                        'value2': [7.1,8,9,10,11,12,43,12,34,5,6,2,3]})\n\n\nThis is what it looks like:\n\n   group1 group2  value1  value2\n0       a      c     1.1     7.1\n1       a      c     2.0     8.0\n2       a      d     3.0     9.0\n3       b      d     4.0    10.0\n4       b      d     5.0    11.0\n5       b      e     6.0    12.0\n6       c      f     7.0    43.0\n7       c      f     8.0    12.0\n8       d      e     9.0    34.0\n9       d      d     1.0     5.0\n10      d      d     2.0     6.0\n11      d      d     3.0     2.0\n12      d      e     4.0     3.0\n\n\nExpected output:\n\n43.0\n\n\nI was under the assumption that df.max() would do this job but it returns a max value for each column but I am not interested in that. I need the max from an entire dataframe. \n"
"I have a Pandas DF where I need to filter out some rows that contains values == 0 for feature 'a' and feature 'b'.\n\nIn order to inspect the values, I run the following:\n\nDF1 = DF[DF['a'] == 0]\n\n\nWhich returns the right values. Similarly, by doing this:\n\nDF2 = DF[DF['b'] == 0]\n\n\nI can see the 0 values for feature 'b'.\n\nHowever, if I try to combine these 2 in a single line of code using the OR operand:\n\nDF3 = DF[DF['a'] == 0 |  DF['b'] == 0]\n\n\nI get this:\n\nTypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n\n\nWhat's happening here?\n"
'I have a function which processes a DataFrame, largely to process data into buckets create a binary matrix of features in a particular column using pd.get_dummies(df[col]).\n\nTo avoid processing all of my data using this function at once (which goes out of memory and causes iPython to crash), I have broken the large DataFrame into chunks using:\n\nchunks = (len(df) / 10000) + 1\ndf_list = np.array_split(df, chunks)\n\n\npd.get_dummies(df) will automatically create new columns based on the contents of df[col] and these are likely to differ for each df in df_list.\n\nAfter processing, I am concatenating the DataFrames back together using:\n\nfor i, df_chunk in enumerate(df_list):\n    print "chunk", i\n    [x, y] = preprocess_data(df_chunk)\n    super_x = pd.concat([super_x, x], axis=0)\n    super_y = pd.concat([super_y, y], axis=0)\n    print datetime.datetime.utcnow()\n\n\nThe processing time of the first chunk is perfectly acceptable, however, it grows per chunk! This is not to do with the preprocess_data(df_chunk) as there is no reason for it to increase. Is this increase in time occurring as a result of the call to pd.concat()?\n\nPlease see log below:\n\nchunks 6\nchunk 0\n2016-04-08 00:22:17.728849\nchunk 1\n2016-04-08 00:22:42.387693 \nchunk 2\n2016-04-08 00:23:43.124381\nchunk 3\n2016-04-08 00:25:30.249369\nchunk 4\n2016-04-08 00:28:11.922305\nchunk 5\n2016-04-08 00:32:00.357365\n\n\nIs there a workaround to speed this up? I have 2900 chunks to process so any help is appreciated!\n\nOpen to any other suggestions in Python!\n'
'I am querying a SQL database and I want to use pandas to process the data. However, I am not sure how to move the data. Below is my input and output.  \n\nimport pyodbc\nimport pandas\nfrom pandas import DataFrame\n\ncnxn = pyodbc.connect(r\'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=C:\\users\\bartogre\\desktop\\CorpRentalPivot1.accdb;UID="";PWD="";\')\ncrsr = cnxn.cursor()\nfor table_name in crsr.tables(tableType=\'TABLE\'):\n    print(table_name)\ncursor = cnxn.cursor()\nsql = "Select sum(CYTM), sum(PYTM), BRAND From data Group By BRAND"\ncursor.execute(sql)\nfor data in cursor.fetchall():\n    print (data)\n\n\n\n\n(\'C:\\\\users\\\\bartogre\\\\desktop\\\\CorpRentalPivot1.accdb\', None, \'Data\', \'TABLE\', None)\n(\'C:\\\\users\\\\bartogre\\\\desktop\\\\CorpRentalPivot1.accdb\', None, \'SFDB\', \'TABLE\', None)\n(Decimal(\'78071898.71\'), Decimal(\'82192672.29\'), \'A\')\n(Decimal(\'12120663.79\'), Decimal(\'13278814.52\'), \'B\')\n\n'
'df = pd.DataFrame([[1,2,3], [10,20,30], [100,200,300]])\ndf.columns = pd.MultiIndex.from_tuples((("a", "b"), ("a", "c"), ("d", "f")))\ndf\n\n\nreturns\n\n     a         d\n     b    c    f\n0    1    2    3\n1   10   20   30\n2  100  200  300\n\n\nand\n\ndf.columns.levels[1]\n\n\nreturns\n\nIndex([u\'b\', u\'c\', u\'f\'], dtype=\'object\')\n\n\nI want to rename "f" to "e". According to pandas.MultiIndex.rename I run:\n\ndf.columns.rename(["b1", "c1", "f1"], level=1)\n\n\nBut it raises\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-110-b171a2b5706c&gt; in &lt;module&gt;()\n----&gt; 1 df.columns.rename(["b1", "c1", "f1"], level=1)\n\nC:\\Users\\USERNAME\\AppData\\Local\\Continuum\\Miniconda2\\lib\\site-packages\\pandas\\indexes\\base.pyc in set_names(self, names, level, inplace)\n    994         if level is not None and not is_list_like(level) and is_list_like(\n    995                 names):\n--&gt; 996             raise TypeError("Names must be a string")\n    997 \n    998         if not is_list_like(names) and level is None and self.nlevels &gt; 1:\n\nTypeError: Names must be a string\n\n\nI use Python 2.7.12 |Continuum Analytics, Inc.| (default, Jun 29 2016, 11:07:13) [MSC v.1500 64 bit (AMD64)]\' and pandas 0.19.1\n'
"df\n     A     B  \n0   a=10   b=20.10\n1   a=20   NaN\n2   NaN    b=30.10\n3   a=40   b=40.10\n\n\nI tried :\n\ndf['A'] = df['A'].str.extract('(\\d+)').astype(int)\ndf['B'] = df['B'].str.extract('(\\d+)').astype(float)\n\n\nBut I get the following error:\n\n\n  ValueError: cannot convert float NaN to integer\n\n\nAnd:\n\n\n  AttributeError: Can only use .str accessor with string values, which use np.object_ dtype in pandas\n\n\nHow do I fix this ?\n"
"how do I add 'd' to the index below without having to reset it first?\n\nfrom pandas import DataFrame\ndf = DataFrame( {'a': range(6), 'b': range(6), 'c': range(6)} )\ndf.set_index(['a','b'], inplace=True)\ndf['d'] = range(6)\n\n# how do I set index to 'a b d' without having to reset it first?\ndf.reset_index(['a','b','d'], inplace=True)\ndf.set_index(['a','b','d'], inplace=True)\n\ndf\n\n"
"On Pandas documentation of the pivot method, we have:\n\nExamples\n--------\n&gt;&gt;&gt; df\n    foo   bar  baz\n0   one   A    1.\n1   one   B    2.\n2   one   C    3.\n3   two   A    4.\n4   two   B    5.\n5   two   C    6.\n\n&gt;&gt;&gt; df.pivot('foo', 'bar', 'baz')\n     A   B   C\none  1   2   3\ntwo  4   5   6\n\n\nMy DataFrame is structured like this:\n\n   name   id     x\n----------------------\n0  john   1      0\n1  john   2      0\n2  mike   1      1\n3  mike   2      0\n\n\nAnd I want something like this:\n\n      1    2   # (this is the id as columns)\n----------------------\nmike  0    0   # (and this is the 'x' as values)\njohn  1    0\n\n\nBut when I run the pivot method, it is saying:\n\n*** ReshapeError: Index contains duplicate entries, cannot reshape\n\n\nWhich doesn't makes sense, even in example there are repeated entries on the foo column. I'm using the name column as the index of the pivot, the first argument of the pivot method call.\n"
'Is there a way to automatically download historical prices of stocks from yahoo finance or google finance (csv format)? Preferably in Python.\n'
'I have a dataframe looks like this:\n\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0JOINED_CO GENDER \xa0\xa0\xa0EXEC_FULLNAME \xa0GVKEY \xa0YEAR \xa0CONAME \xa0BECAMECEO \xa0REJOIN \xa0\xa0LEFTOFC \xa0\xa0\xa0LEFTCO \xa0RELEFT \xa0\xa0\xa0REASON \xa0PAGE\nCO_PER_ROL \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01992 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01993 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01994 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01995 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01996 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01997 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5622 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0\xa0Ira A. Eichner \xa0\xa01004 \xa01998 \xa0AAR CORP \xa0\xa019550101 \xa0\xa0\xa0\xa0NaN \xa019961001 \xa019990531 \xa0\xa0\xa0\xa0NaN \xa0RESIGNED \xa0\xa0\xa079\n5623 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0David P. Storch \xa0\xa01004 \xa01992 \xa0AAR CORP \xa0\xa019961009 \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa057\n5623 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0David P. Storch \xa0\xa01004 \xa01993 \xa0AAR CORP \xa0\xa019961009 \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa057\n5623 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0David P. Storch \xa0\xa01004 \xa01994 \xa0AAR CORP \xa0\xa019961009 \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa057\n5623 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0David P. Storch \xa0\xa01004 \xa01995 \xa0AAR CORP \xa0\xa019961009 \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa057\n5623 \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0MALE \xa0David P. Storch \xa0\xa01004 \xa01996 \xa0AAR CORP \xa0\xa019961009 \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0NaN \xa0\xa0\xa0\xa0\xa0\xa0NaN \xa0\xa0\xa057\n \n\nFor the YEAR value, I like to add year columns (1993,1994...,2009) to the original dataframe, If the value in YEAR is 1992, then the value in the 1992 column should be 1 otherwise 0.\n\nI used a very stupid for loop, but it seems to run forever as I have a large dataset.\nCould anyone help me with it, thanks a lot!\n'
"I have a problem with adding columns in pandas.\nI have DataFrame, dimensional is nxk. And in process I wiil need add columns with dimensional mx1, where m = [1,n], but I don't know m.\n\nWhen I try do it:   \n\ndf['Name column'] = data    \n# type(data) = list\n\n\nresult:\n\nAssertionError: Length of values does not match length of index   \n\n\nCan I add columns with different length?\n"
"How can I create a DataFrame from multiple numpy arrays, Pandas Series, or Pandas DataFrame's while preserving the order of the columns?\n\nFor example, I have these two numpy arrays and I want to combine them as a Pandas DataFrame.\n\nfoo = np.array( [ 1, 2, 3 ] )\nbar = np.array( [ 4, 5, 6 ] )\n\n\nIf I do this, the bar column would come first because dict doesn't preserve order.\n\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } )\n\n    bar foo\n0   4   1\n1   5   2\n2   6   3\n\n\nI can do this, but it gets tedious when I need to combine many variables.\n\npd.DataFrame( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) }, columns = [ 'foo', 'bar' ] )\n\n\nEDIT: Is there a way to specify the variables to be joined and to organize the column order in one operation? That is, I don't mind using multiple lines to complete the entire operation, but I'd rather not having to specify the variables to be joined multiple times (since I will be changing the code a lot and this is pretty error prone).\n\nEDIT2: One more point. If I want to add or remove one of the variables to be joined, I only want to add/remove in one place.\n"
"I tried to convert a column from data type float64 to int64 using:\n\ndf['column name'].astype(int64)\n\n\nbut got an error:\n\n\n  NameError: name 'int64' is not defined\n\n\nThe column has number of people but was formatted as 7500000.0, any idea how I can simply change this float64 into int64?\n"
'Lets say I have following pandas DataFrame:\n\nimport pandas as pd\ndf = pd.DataFrame({"A":[1,pd.np.nan,2], "B":[5,6,0]})\n\n\nWhich would look like:\n\n&gt;&gt;&gt; df\n     A  B\n0  1.0  5\n1  NaN  6\n2  2.0  0\n\n\nFirst option\n\nI know one way to check if a particular value is NaN, which is as follows:\n\n&gt;&gt;&gt; df.isnull().ix[1,0]\nTrue\n\n\nSecond option (not working)\n\nI thought below option, using ix, would work as well, but it\'s not:\n\n&gt;&gt;&gt; df.ix[1,0]==pd.np.nan\nFalse\n\n\nI also tried iloc with same results:\n\n&gt;&gt;&gt; df.iloc[1,0]==pd.np.nan\nFalse\n\n\nHowever if I check for those values using ix or iloc I get:\n\n&gt;&gt;&gt; df.ix[1,0]\nnan\n&gt;&gt;&gt; df.iloc[1,0]\nnan\n\n\nSo, why is the second option not working? Is it possible to check for NaN values using ix or iloc?\n'
"I'm using sci-kit learn linear regression algorithm. \nWhile scaling Y target feature with:\n\nYs = scaler.fit_transform(Y)\n\n\nI got\n\n\n  ValueError: Expected 2D array, got 1D array instead:\n\n\nAfter that I reshaped using:\n\nYs = scaler.fit_transform(Y.reshape(-1,1))\n\n\nBut got error again:\n\n\n  AttributeError: 'Series' object has no attribute 'reshape'\n\n\nSo I checked pandas.Series documentation page and it says:\n\n\n  reshape(*args, **kwargs)     Deprecated since version 0.19.0.\n\n"
'On a concrete problem, say I have a DataFrame DF\n\n     word  tag count\n0    a     S    30\n1    the   S    20\n2    a     T    60\n3    an    T    5\n4    the   T    10 \n\n\nI want to find, for every "word", the "tag" that has the most "count". So the return would be something like\n\n     word  tag count\n1    the   S    20\n2    a     T    60\n3    an    T    5\n\n\nI don\'t care about the count column or if the order/Index is original or messed up. Returning a dictionary {\'the\' : \'S\', ...} is just fine.\n\nI hope I can do\n\nDF.groupby([\'word\']).agg(lambda x: x[\'tag\'][ x[\'count\'].argmax() ] )\n\n\nbut it doesn\'t work. I can\'t access column information.\n\nMore abstractly, what does the function in agg(function) see as its argument?\n\nbtw, is .agg() the same as .aggregate() ?\n\nMany thanks.\n'
"Is there anyway to use the mapping function or something better to replace values in an entire dataframe?\n\nI only know how to perform the mapping on series.\n\nI would like to replace the strings in the 'tesst' and 'set' column with a number\nfor example set = 1, test =2\n\nHere is a example of my dataset: (Original dataset is very large)\n\nds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age tesst   set\n0          a  volvo      p      swe      1        0        1   23   set   set\n1          b  volvo   None      swe      0        0        1   45   set   set\n2          c    bmw      p       us      0        0        1   56  test  test\n3          d    bmw      p       us      0        1        1   43  test  test\n4          e    bmw      d  germany      1        0        1   34   set   set\n5          f   audi      d  germany      1        0        1   59   set   set\n6          g  volvo      d      swe      1        0        0   65  test   set\n7          h   audi      d      swe      1        0        0   78  test   set\n8          i  volvo      d       us      1        1        1   32   set   set\n\n\nFinal result should be \n\n ds_r\n  respondent  brand engine  country  aware  aware_2  aware_3  age  tesst  set\n0          a  volvo      p      swe      1        0        1   23      1    1\n1          b  volvo   None      swe      0        0        1   45      1    1\n2          c    bmw      p       us      0        0        1   56      2    2\n3          d    bmw      p       us      0        1        1   43      2    2\n4          e    bmw      d  germany      1        0        1   34      1    1\n5          f   audi      d  germany      1        0        1   59      1    1\n6          g  volvo      d      swe      1        0        0   65      2    1\n7          h   audi      d      swe      1        0        0   78      2    1\n8          i  volvo      d       us      1        1        1   32      1    1\n\n\ngrateful for advise,\n"
'If I have a frame like this\n\nframe = pd.DataFrame({\'a\' : [\'the cat is blue\', \'the sky is green\', \'the dog is black\']})\n\n\nand I want to check if any of those rows contain a certain word I just have to do this.\n\nframe[\'b\'] = frame.a.str.contains("dog") | frame.a.str.contains("cat") | frame.a.str.contains("fish")\n\n\nframe[\'b\'] outputs:\n\nTrue\nFalse\nTrue\n\n\nIf I decide to make a list \n\nmylist =[\'dog\', \'cat\', \'fish\']\n\n\nhow would I check that the rows contain a certain word in the list?  \n'
"Example\n\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nprint s \n1    5\n2    4\n3    3\n4    2\n5    1\n\n\nIs there an efficient way to create a series. e.g. containing in each row the lagged values (in this example up to lag 2)\n\n3    [3, 4, 5]\n4    [2, 3, 4]\n5    [1, 2, 3]\n\n\nThis corresponds to s=pd.Series([[3,4,5],[2,3,4],[1,2,3]], index=[3,4,5])\n\nHow can this be done in an efficient way for dataframes with a lot of timeseries which are very long?\n\nThanks\n\nEdited after seeing the answers\n\nok, at the end I implemented this function:\n\ndef buildLaggedFeatures(s,lag=2,dropna=True):\n'''\nBuilds a new DataFrame to facilitate regressing over all possible lagged features\n'''\nif type(s) is pd.DataFrame:\n    new_dict={}\n    for col_name in s:\n        new_dict[col_name]=s[col_name]\n        # create lagged Series\n        for l in range(1,lag+1):\n            new_dict['%s_lag%d' %(col_name,l)]=s[col_name].shift(l)\n    res=pd.DataFrame(new_dict,index=s.index)\n\nelif type(s) is pd.Series:\n    the_range=range(lag+1)\n    res=pd.concat([s.shift(i) for i in the_range],axis=1)\n    res.columns=['lag_%d' %i for i in the_range]\nelse:\n    print 'Only works for DataFrame or Series'\n    return None\nif dropna:\n    return res.dropna()\nelse:\n    return res \n\n\nit produces the wished outputs and manages the naming of columns in the resulting DataFrame.\n\nFor a Series as input:\n\ns=pd.Series([5,4,3,2,1], index=[1,2,3,4,5])\nres=buildLaggedFeatures(s,lag=2,dropna=False)\n   lag_0  lag_1  lag_2\n1      5    NaN    NaN\n2      4      5    NaN\n3      3      4      5\n4      2      3      4\n5      1      2      3\n\n\nand for a DataFrame as input:\n\ns2=s=pd.DataFrame({'a':[5,4,3,2,1], 'b':[50,40,30,20,10]},index=[1,2,3,4,5])\nres2=buildLaggedFeatures(s2,lag=2,dropna=True)\n\n   a  a_lag1  a_lag2   b  b_lag1  b_lag2\n3  3       4       5  30      40      50\n4  2       3       4  20      30      40\n5  1       2       3  10      20      30\n\n"
'I am trying to calculate time based aggregations in Pandas based on date values stored in a separate tables.\n\nThe top of the first table table_a looks like this:\n\n    COMPANY_ID  DATE            MEASURE\n    1   2010-01-01 00:00:00     10\n    1   2010-01-02 00:00:00     10\n    1   2010-01-03 00:00:00     10\n    1   2010-01-04 00:00:00     10\n    1   2010-01-05 00:00:00     10\n\n\nHere is the code to create the table:\n\n    table_a = pd.concat(\\\n    [pd.DataFrame({\'DATE\': pd.date_range("01/01/2010", "12/31/2010", freq="D"),\\\n    \'COMPANY_ID\': 1 , \'MEASURE\': 10}),\\\n    pd.DataFrame({\'DATE\': pd.date_range("01/01/2010", "12/31/2010", freq="D"),\\\n    \'COMPANY_ID\': 2 , \'MEASURE\': 10})])\n\n\nThe second table, table_b looks like this:\n\n        COMPANY     END_DATE\n        1   2010-03-01 00:00:00\n        1   2010-06-02 00:00:00\n        2   2010-03-01 00:00:00\n        2   2010-06-02 00:00:00\n\n\nand the code to create it is:\n\n    table_b = pd.DataFrame({\'END_DATE\':pd.to_datetime([\'03/01/2010\',\'06/02/2010\',\'03/01/2010\',\'06/02/2010\']),\\\n                    \'COMPANY\':(1,1,2,2)})\n\n\nI want to be able to get the sum of the measure column for each COMPANY_ID for each 30 day period prior to the END_DATE in table_b.\n\nThis is (I think) the SQL equivalent:\n\n      select\n b.COMPANY_ID,\n b.DATE\n sum(a.MEASURE) AS MEASURE_TO_END_DATE\n from table_a a, table_b b\n where a.COMPANY = b.COMPANY and\n       a.DATE &lt; b.DATE and\n       a.DATE &gt; b.DATE - 30  \n group by b.COMPANY;\n\n\nThanks for any help\n'
"With the nice indexing methods in Pandas I have no problems extracting data in various ways. On the other hand I am still confused about how to change data in an existing DataFrame. \n\nIn the following code I have two DataFrames and my goal is to update values in a specific row in the first df from values of the second df. How can I achieve this?\n\nimport pandas as pd\ndf = pd.DataFrame({'filename' :  ['test0.dat', 'test2.dat'], \n                                  'm': [12, 13], 'n' : [None, None]})\ndf2 = pd.DataFrame({'filename' :  'test2.dat', 'n':16}, index=[0])\n\n# this overwrites the first row but we want to update the second\n# df.update(df2)\n\n# this does not update anything\ndf.loc[df.filename == 'test2.dat'].update(df2)\n\nprint(df)\n\n\ngives \n\n   filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  None\n\n[2 rows x 3 columns]\n\n\nbut how can I achieve this:\n\n    filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13  16\n\n[2 rows x 3 columns]\n\n"
'While working in Pandas in Python...\n\nI\'m working with a dataset that contains some missing values, and I\'d like to return a dataframe which contains only those rows which have missing data.  Is there a nice way to do this?\n\n(My current method to do this is an inefficient "look to see what index isn\'t in the dataframe without the missing values, then make a df out of those indices.")\n'
"When using panda's resample function on a DataFrame in order to convert tick data to OHLCV, a resampling error is encountered.\n\nHow should we solve the error?\n\ndata = pd.read_csv('tickdata.csv', header=None, names=['Timestamp','Price','Volume']).set_index('Timestamp')\ndata.head()\n\n\n\n\n# Resample data into 30min bins\nticks = data.ix[:, ['Price', 'Volume']]\nbars = ticks.Price.resample('30min', how='ohlc')\nvolumes = ticks.Volume.resample('30min', how='sum')\n\n\nThis gives the error:\n\nTypeError: Only valid with DatetimeIndex or PeriodIndex\n\n"
'I am importing a CSV file like the one below, using pandas.read_csv:\n\ndf = pd.read_csv(Input, delimiter=";")\n\n\nExample of CSV file:\n\n10;01.02.2015 16:58;01.02.2015 16:58;-0.59;0.1;-4.39;NotApplicable;0.79;0.2\n11;01.02.2015 16:58;01.02.2015 16:58;-0.57;0.2;-2.87;NotApplicable;0.79;0.21\n\n\nThe problem is that when I later on in my code try to use these values I get this error: TypeError: can\'t multiply sequence by non-int of type \'float\' \n\nThe error is because the number I\'m trying to use is not written with a dot (.) as a decimal separator but a comma(,). After manually changing the commas to a dots my program works.\n\nI can\'t change the format of my input, and thus have to replace the commas in my DataFrame in order for my code to work, and I want python to do this without the need of doing it manually. Do you have any suggestions?\n'
'I know that I can reset the indices like so\n\ndf.reset_index(inplace=True)\n\n\nbut this will start the index from 0. I want to start it from 1.  How do I do that without creating any extra columns and by keeping the index/reset_index functionality and options? I do not want to create a new dataframe, so inplace=True should still apply.\n'
"I have loaded a data file into a Python pandas dataframe. I has a datetime column of the format 2015-07-18 13:53:33.280. \n\nWhat I need to do is create a new column that rounds this out to its nearest quarter hour. So, the date above will be rounded to 2015-07-18 13:45:00.000.\n\nHow do I do this in pandas? I tried using the solution from here, but get an 'Series' object has no attribute 'year' error. \n"
'I want to add a title to a seaborn heatmap. Using Pandas and iPython Notebook\n\ncode is below, \n\na1_p = a1.pivot_table( index=\'Postcode\', columns=\'Property Type\', values=\'Count\', aggfunc=np.mean, fill_value=0)\n\nsns.heatmap(a1_p, cmap="YlGnBu")\n\n\nthe data is pretty straight forward:\n\nIn [179]: a1_p\n\nOut [179]:\nProperty Type   Flat    Terraced house  Unknown\nPostcode            \nE1  11  0   0\nE14 12  0   0\nE1W 6   0   0\nE2  6   0   0\n\n'
'This is my dataframe:  \n\n          date                          ids\n0     2011-04-23  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n1     2011-04-24  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n2     2011-04-25  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n3     2011-04-26  Nan\n4     2011-04-27  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n5     2011-04-28  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n\n\nI want to replace Nan with []. How to do that? Fillna([]) did not work. I even tried replace(np.nan, []) but it gives error:  \n\n TypeError(\'Invalid "to_replace" type: \\\'float\\\'\',)\n\n'
"I have a pandas dataFrame of mixed types, some are strings and some are numbers. I would like to replace the NAN values in string columns by '.', and the NAN values in float columns by 0.\n\nConsider this small fictitious example:\n\ndf = pd.DataFrame({'Name':['Jack','Sue',pd.np.nan,'Bob','Alice','John'],\n    'A': [1, 2.1, pd.np.nan, 4.7, 5.6, 6.8],\n    'B': [.25, pd.np.nan, pd.np.nan, 4, 12.2, 14.4],\n    'City':['Seattle','SF','LA','OC',pd.np.nan,pd.np.nan]})\n\n\nNow, I can do it in 3 lines:\n\ndf['Name'].fillna('.',inplace=True)\ndf['City'].fillna('.',inplace=True)\ndf.fillna(0,inplace=True)\n\n\nSince this is a small dataframe, 3 lines is probably ok. In my real example (which I cannot share here due to data confidentiality reasons), I have many more string columns and numeric columns. SO I end up writing many lines just for fillna. Is there a concise way of doing this? \n"
"I want to count number of times each values is appearing in dataframe.\n\nHere is my dataframe - df:\n\n    status\n1     N\n2     N\n3     C\n4     N\n5     S\n6     N\n7     N\n8     S\n9     N\n10    N\n11    N\n12    S\n13    N\n14    C\n15    N\n16    N\n17    N\n18    N\n19    S\n20    N\n\n\nI want to dictionary of counts: \n\nex. counts = {N: 14, C:2, S:4}\n\nI have tried df['status']['N'] but it gives keyError and also df['status'].value_counts but no use.   \n"
'On my own I found a way to drop nan rows from a pandas dataframe. Given a dataframe dat with column x which contains nan values,is there a more elegant way to do drop each row of dat which has a nan value in the x column?\n\ndat = dat[np.logical_not(np.isnan(dat.x))]\ndat = dat.reset_index(drop=True)\n\n'
"I would like to drop all data in a pandas dataframe, but am getting TypeError: drop() takes at least 2 arguments (3 given). I essentially want a blank dataframe with just my columns headers.\n\nimport pandas as pd\n\nweb_stats = {'Day': [1, 2, 3, 4, 2, 6],\n             'Visitors': [43, 43, 34, 23, 43, 23],\n             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}\ndf = pd.DataFrame(web_stats)\n\ndf.drop(axis=0, inplace=True)\nprint df\n\n"
"I have a column Date_Time that I wish to groupby date time without creating a new column. Is this possible the current code I have does not work.\n\ndf = pd.groupby(df,by=[df['Date_Time'].date()])\n\n"
'I recently discovered pandas "assign" method which I find very elegant.\nMy issue is that the name of the new column is assigned as keyword, so it cannot have spaces or dashes in it. \n\ndf = DataFrame({\'A\': range(1, 11), \'B\': np.random.randn(10)})\ndf.assign(ln_A = lambda x: np.log(x.A))\n        A         B      ln_A\n0   1  0.426905  0.000000\n1   2 -0.780949  0.693147\n2   3 -0.418711  1.098612\n3   4 -0.269708  1.386294\n4   5 -0.274002  1.609438\n5   6 -0.500792  1.791759\n6   7  1.649697  1.945910\n7   8 -1.495604  2.079442\n8   9  0.549296  2.197225\n9  10 -0.758542  2.302585\n\n\nbut what if I want to name the new column "ln(A)" for example?\nE.g. \n\ndf.assign(ln(A) = lambda x: np.log(x.A))\ndf.assign("ln(A)" = lambda x: np.log(x.A))\n\n\nFile "&lt;ipython-input-7-de0da86dce68&gt;", line 1\ndf.assign(ln(A) = lambda x: np.log(x.A))\nSyntaxError: keyword can\'t be an expression\n\n\nI know I could rename the column right after the .assign call, but I want to understand more about this method and its syntax.\n'
"I saw this code in someone's iPython notebook, and I'm very confused as to how this code works. As far as I understood, pd.loc[] is used as a location based indexer where the format is:\n\ndf.loc[index,column_name]\n\n\nHowever, in this case, the first index seems to be a series of boolean values. Could someone please explain to me how this selection works. I tried to read through the documentation but I couldn't figure out an explanation. Thanks!\n\niris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n\n\n\n"
"I have a hacky way of achieving this using boto3 (1.4.4), pyarrow (0.4.1) and pandas (0.20.3).\n\nFirst, I can read a single parquet file locally like this:\n\nimport pyarrow.parquet as pq\n\npath = 'parquet/part-r-00000-1e638be4-e31f-498a-a359-47d017a0059c.gz.parquet'\ntable = pq.read_table(path)\ndf = table.to_pandas()\n\n\nI can also read a directory of parquet files locally like this:\n\nimport pyarrow.parquet as pq\n\ndataset = pq.ParquetDataset('parquet/')\ntable = dataset.read()\ndf = table.to_pandas()\n\n\nBoth work like a charm. Now I want to achieve the same remotely with files stored in a S3 bucket. I was hoping that something like this would work:\n\ndataset = pq.ParquetDataset('s3n://dsn/to/my/bucket')\n\n\nBut it does not:\n\nOSError: Passed non-file path: s3n://dsn/to/my/bucket\n\nAfter reading pyarrow's documentation thoroughly, this does not seem possible at the moment. So I came out with the following solution:\n\nReading a single file from S3 and getting a pandas dataframe:\n\nimport io\nimport boto3\nimport pyarrow.parquet as pq\n\nbuffer = io.BytesIO()\ns3 = boto3.resource('s3')\ns3_object = s3.Object('bucket-name', 'key/to/parquet/file.gz.parquet')\ns3_object.download_fileobj(buffer)\ntable = pq.read_table(buffer)\ndf = table.to_pandas()\n\n\nAnd here my hacky, not-so-optimized, solution to create a pandas dataframe from a S3 folder path:\n\nimport io\nimport boto3\nimport pandas as pd\nimport pyarrow.parquet as pq\n\nbucket_name = 'bucket-name'\ndef download_s3_parquet_file(s3, bucket, key):\n    buffer = io.BytesIO()\n    s3.Object(bucket, key).download_fileobj(buffer)\n    return buffer\n\nclient = boto3.client('s3')\ns3 = boto3.resource('s3')\nobjects_dict = client.list_objects_v2(Bucket=bucket_name, Prefix='my/folder/prefix')\ns3_keys = [item['Key'] for item in objects_dict['Contents'] if item['Key'].endswith('.parquet')]\nbuffers = [download_s3_parquet_file(s3, bucket_name, key) for key in s3_keys]\ndfs = [pq.read_table(buffer).to_pandas() for buffer in buffers]\ndf = pd.concat(dfs, ignore_index=True)\n\n\nIs there a better way to achieve this? Maybe some kind of connector for pandas using pyarrow? I would like to avoid using pyspark, but if there is no other solution, then I would take it.\n"
'given the following dataframe in pandas:\n\nimport numpy as np\ndf = pandas.DataFrame({"a": np.random.random(100), "b": np.random.random(100), "id": np.arange(100)})\n\n\nwhere id is an id for each point consisting of an a and b value, how can I bin a and b into a specified set of bins (so that I can then take the median/average value of a and b in each bin)?  df might have NaN values for a or b (or both) for any given row in df. thanks.\n\nHere\'s a better example using Joe Kington\'s solution with a more realistic df. The thing I\'m unsure about is how to access the df.b elements for each df.a group below:\n\na = np.random.random(20)\ndf = pandas.DataFrame({"a": a, "b": a + 10})\n# bins for df.a\nbins = np.linspace(0, 1, 10)\n# bin df according to a\ngroups = df.groupby(np.digitize(df.a,bins))\n# Get the mean of a in each group\nprint groups.mean()\n## But how to get the mean of b for each group of a?\n# ...\n\n'
"I have the following DataFrame:\n\n   a  b  c\nb\n2  1  2  3\n5  4  5  6\n\n\nAs you can see, column b is used as an index. I want to get the ordinal number of the row fulfilling ('b' == 5), which in this case would be 1.\n\nThe column being tested can be either an index column (as with b in this case) or a regular column, e.g. I may want to find the index of the row fulfilling ('c' == 6).\n"
"I have a pandas DataFrame called data with a column called ms.  I want to eliminate all the rows where data.ms is above the 95% percentile.  For now, I'm doing this:\n\nlimit = data.ms.describe(90)['95%']\nvalid_data = data[data['ms'] &lt; limit]\n\n\nwhich works, but I want to generalize that to any percentile.  What's the best way to do that?\n"
"What is the quickest way to insert a pandas DataFrame into mongodb using PyMongo?\n\nAttempts\n\ndb.myCollection.insert(df.to_dict())\n\n\ngave an error \n\n\n  InvalidDocument: documents must have only string keys, the key was\n  Timestamp('2013-11-23 13:31:00', tz=None)\n\n\n\n\n db.myCollection.insert(df.to_json())\n\n\ngave an error \n\n\n  TypeError: 'str' object does not support item assignment\n\n\n\n\n db.myCollection.insert({id: df.to_json()})\n\n\ngave an error \n\nInvalidDocument: documents must have only string a keys, key was &lt;built-in function id&gt;\n\n\n\ndf\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 150 entries, 2013-11-23 13:31:26 to 2013-11-23 13:24:07\nData columns (total 3 columns):\namount    150  non-null values\nprice     150  non-null values\ntid       150  non-null values\ndtypes: float64(2), int64(1)\n\n"
"I have the following structure to my dataFrame:\n\nIndex: 1008 entries, Trial1.0 to Trial3.84\nData columns (total 5 columns):\nCHUNK_NAME                    1008  non-null values\nLAMBDA                        1008  non-null values\nBETA                          1008  non-null values\nHIT_RATE                      1008  non-null values\nAVERAGE_RECIPROCAL_HITRATE    1008  non-null values\n\nchunks=['300_321','322_343','344_365','366_387','388_408','366_408','344_408','322_408','300_408']\nlam_beta=[(lambda1,beta1),(lambda1,beta2),(lambda1,beta3),...(lambda1,beta_n),(lambda2,beta1),(lambda2,beta2)...(lambda2,beta_n),........]\n\nmy_df.ix[my_df.CHUNK_NAME==chunks[0]&amp;my_df.LAMBDA==lam_beta[0][0]]\n\n\nI want to get the rows of the Dataframe for a particular chunk lets say chunks[0] and particular lambda value. So in this case the output should be all rows in the dataframe having CHUNK_NAME='300_321' and LAMBDA=lambda1. There would be n rows one for each beta value that would be returned. But instead I get the follwoing error. Any help in solving this problem would be appreciated.\n\nTypeError: cannot compare a dtyped [float64] array with a scalar of type [bool]\n\n"
'I have a dataframe of the following form (for example)\n\nshopper_num,is_martian,number_of_items,count_pineapples,birth_country,tranpsortation_method\n1,FALSE,0,0,MX,\n2,FALSE,1,0,MX,\n3,FALSE,0,0,MX,\n4,FALSE,22,0,MX,\n5,FALSE,0,0,MX,\n6,FALSE,0,0,MX,\n7,FALSE,5,0,MX,\n8,FALSE,0,0,MX,\n9,FALSE,4,0,MX,\n10,FALSE,2,0,MX,\n11,FALSE,0,0,MX,\n12,FALSE,13,0,MX,\n13,FALSE,0,0,CA,\n14,FALSE,0,0,US,\n\n\nHow can I use Pandas to calculate summary statistics of each column (column data types are variable, some columns have no information \n\nAnd then return the a dataframe of the form:\n\ncolumnname, max, min, median,\n\nis_martian, NA, NA, FALSE\n\n\nSo on and so on\n'
"I have a DataFrame looking like this:\n\n     amount     price\nage\nA     40929   4066443\nB     93904   9611272\nC    188349  19360005\nD    248438  24335536\nE    205622  18888604\nF    140173  12580900\nG     76243   6751731\nH     36859   3418329\nI     29304   2758928\nJ     39768   3201269\nK     30350   2867059\n\n\nNow I'd like to plot a bar-plot with the age on the x-axis as labels. For each x-tick there should be two bars, one bar for the amount, and one for the price. I can get this working by using simply:\n\ndf.plot(kind='bar')\n\n\nThe problem is the scaling. The prices are so much higher that I can not really identify the amount in that graph, see:\n\n\n\nThus I'd like a second y-axis. I tried it using:\n\ndf.loc[:,'amount'].plot(kind='bar')\ndf.loc[:,'price'].plot(kind='bar',secondary_y=True)\n\n\nbut this just overwrites the bars and does NOT place them side-by-side.\nIs there any way to do this without having to access the lower-level matplotlib (which would be possible obviously by placing the bars side by side manually)?\n\nFor now, I'm using two single plots within subplots:\n\ndf.plot(kind='bar',grid=True,subplots=True,sharex=True); \n\n\nresulting in:\n\n\n"
'I want to remove the stop words from my column "tweets". How do I iterative over each row and each item?\n\npos_tweets = [(\'I love this car\', \'positive\'),\n    (\'This view is amazing\', \'positive\'),\n    (\'I feel great this morning\', \'positive\'),\n    (\'I am so excited about the concert\', \'positive\'),\n    (\'He is my best friend\', \'positive\')]\n\ntest = pd.DataFrame(pos_tweets)\ntest.columns = ["tweet","class"]\ntest["tweet"] = test["tweet"].str.lower().str.split()\n\nfrom nltk.corpus import stopwords\nstop = stopwords.words(\'english\')\n\n'
"I'm looking for a simple way to sort a pandas dataframe by the absolute value of a particular column, but without actually changing the values within the dataframe. Something similar to sorted(df, key=abs). So if I had a dataframe like:\n\n    a   b\n0   1   -3\n1   2   5 \n2   3   -1\n3   4   2\n4   5   -9\n\n\nThe resultant sorted data when sorting on 'b' would look like:\n\n    a   b\n2   3   -1\n3   4   2\n0   1   -3\n1   2   5 \n4   5   -9\n\n"
"I am trying to find the count of distinct values in each column using Pandas. This is what I did.\n\nimport pandas as pd\nimport numpy as np\n\n# Generate data.\nNROW = 10000\nNCOL = 100\ndf = pd.DataFrame(np.random.randint(1, 100000, (NROW, NCOL)),\n                  columns=['col' + x for x in np.arange(NCOL).astype(str)])\n\n\nI need to count the number of distinct elements for each column, like this:\n\ncol0    9538\ncol1    9505\ncol2    9524\n\n\nWhat would be the most efficient way to do this, as this method will be applied to files which have size greater than 1.5GB?\n\n\n\nBased upon the answers, df.apply(lambda x: len(x.unique())) is the fastest (notebook).\n\n%timeit df.apply(lambda x: len(x.unique()))\n10 loops, best of 3: 49.5 ms per loop\n%timeit df.nunique()\n10 loops, best of 3: 59.7 ms per loop\n%timeit df.apply(pd.Series.nunique)\n10 loops, best of 3: 60.3 ms per loop\n%timeit df.T.apply(lambda x: x.nunique(), axis=1)\n10 loops, best of 3: 60.5 ms per loop\n\n"
"I'm fighting with pandas and for now I'm loosing. I have source table similar to this:\n\nimport pandas as pd\n\na=pd.Series([123,22,32,453,45,453,56])\nb=pd.Series([234,4353,355,453,345,453,56])\ndf=pd.concat([a, b], axis=1)\ndf.columns=['First', 'Second']\n\n\nI would like to add new column to this data frame with first digit from values in column 'First':\na) change number to string from column 'First'\nb) extracting first character from newly created string\nc) Results from b save as new column in data frame\n\nI don't know how to apply this to the pandas data frame object. I would be grateful for helping me with that.\n"
'I have to merge two dataframes:\n\ndf1\n\ncompany,standard\ntata,A1\ncts,A2\ndell,A3\n\n\ndf2\n\ncompany,return\ntata,71\ndell,78\ncts,27\nhcl,23\n\n\nI have to unify both dataframes to one dataframe. I need output like:\n\ncompany,standard,return\ntata,A1,71\ncts,A2,27\ndell,A3,78\n\n'
"This question has been asked so many times, and it seemed to work for others, however, I am getting NaN values when I copy a column from a different DataFrame(df1 and df2 are same length). \n\ndf1\n\n        date     hour      var1\na   2017-05-01  00:00:00   456585\nb   2017-05-01  01:00:00   899875\nc   2017-05-01  02:00:00   569566\nd   2017-05-01  03:00:00   458756\ne   2017-05-01  04:00:00   231458\nf   2017-05-01  05:00:00   986545\n\n\ndf2\n\n      MyVar1     MyVar2 \n 0  6169.719338 3688.045368\n 1  5861.148007 3152.238704\n 2  5797.053347 2700.469871\n 3  5779.102340 2730.471948\n 4  6708.219647 3181.298291\n 5  8550.380343 3793.580394\n\n\nI need like this in my df2\n\n       MyVar1    MyVar2        date        hour\n 0  6169.719338 3688.045368  2017-05-01  00:00:00\n 1  5861.148007 3152.238704  2017-05-01  01:00:00\n 2  5797.053347 2700.469871  2017-05-01  02:00:00\n 3  5779.102340 2730.471948  2017-05-01  03:00:00\n 4  6708.219647 3181.298291  2017-05-01  04:00:00\n 5  8550.380343 3793.580394  2017-05-01  05:00:00\n\n\nI tried the following,\n\ndf2['date'] = df1['date']\ndf2['hour'] = df1['hour']\n\ntype(df1)\n&gt;&gt; pandas.core.frame.DataFrame\n\ntype(df2)\n&gt;&gt; pandas.core.frame.DataFrame\n\n\nI am getting the following,\n\n       MyVar1    MyVar2      date       hour\n 0  6169.719338 3688.045368  NaN        NaN\n 1  5861.148007 3152.238704  NaN        NaN\n 2  5797.053347 2700.469871  NaN        NaN\n\n\nWhy is this happening? There is another post that discusses merge, but I just need to copy it. Any help would be appreciated.  \n"
"I want to perform a join/merge/append operation on a dataframe with datetime index.\n\nLet's say I have df1 and I want to add df2 to it.  df2 can have fewer or more columns, and overlapping indexes.  For all rows where the indexes match, if df2 has the same column as df1, I want the values of df1 be overwritten with those from df2. \n\nHow can I obtain the desired result? \n"
'The index that I have in the dataframe (with 30 rows) is of the form:\n\nInt64Index([171, 174,173, 172, 199..............\n        ....175, 200])\n\n\nThe index is not strictly increasing because the data frame is the output of a sort().\nI want to have add a column which is the series:\n\n[1, 2, 3, 4, 5......................., 30]\n\n\nHow should I go about doing that?\n\nThanks.\n'
"From the pandas documentation, I've gathered that unique-valued indices make certain operations efficient, and that non-unique indices are occasionally tolerated.\n\nFrom the outside, it doesn't look like non-unique indices are taken advantage of in any way. For example, the following ix query is slow enough that it seems to be scanning the entire dataframe\n\nIn [23]: import numpy as np\nIn [24]: import pandas as pd\nIn [25]: x = np.random.randint(0, 10**7, 10**7)\nIn [26]: df1 = pd.DataFrame({'x':x})\nIn [27]: df2 = df1.set_index('x', drop=False)\nIn [28]: %timeit df2.ix[0]\n1 loops, best of 3: 402 ms per loop\nIn [29]: %timeit df1.ix[0]\n10000 loops, best of 3: 123 us per loop\n\n\n(I realize the two ix queries don't return the same thing -- it's just an example that calls to ix on a non-unique index appear much slower)\n\nIs there any way to coax pandas into using faster lookup methods like binary search on non-unique and/or sorted indices?\n"
'Is there a way to generate time range in pandas similar to date_range?\nsomething like:\n\npandas.time_range("11:00", "21:30", freq="30min")\n\n'
"In the context of unit testing some functions, I'm trying to establish the equality of 2 DataFrames using python pandas:\n\nipdb&gt; expect\n                            1   2\n2012-01-01 00:00:00+00:00 NaN   3\n2013-05-14 12:00:00+00:00   3 NaN\n\nipdb&gt; df\nidentifier                  1   2\ntimestamp\n2012-01-01 00:00:00+00:00 NaN   3\n2013-05-14 12:00:00+00:00   3 NaN\n\nipdb&gt; df[1][0]\nnan\n\nipdb&gt; df[1][0], expect[1][0]\n(nan, nan)\n\nipdb&gt; df[1][0] == expect[1][0]\nFalse\n\nipdb&gt; df[1][1] == expect[1][1]\nTrue\n\nipdb&gt; type(df[1][0])\n&lt;type 'numpy.float64'&gt;\n\nipdb&gt; type(expect[1][0])\n&lt;type 'numpy.float64'&gt;\n\nipdb&gt; (list(df[1]), list(expect[1]))\n([nan, 3.0], [nan, 3.0])\n\nipdb&gt; df1, df2 = (list(df[1]), list(expect[1])) ;; df1 == df2\nFalse\n\n\nGiven that I'm trying to test the entire of expect against the entire of df, including NaN positions, what am I doing wrong?\n\nWhat is the simplest way to compare equality of Series/DataFrames including NaNs?\n"
"I'm looking for advice on how to show a title at the top of a collection of histogram plots that have been generated by a pandas df.hist() command. For instance, in the histogram figure block generated by the code below I'd like to place a general title (e.g. 'My collection of histogram plots') at the top of the figure:\n\ndata = DataFrame(np.random.randn(500).reshape(100,5), columns=list('abcde'))\naxes = data.hist(sharey=True, sharex=True)\n\n\nI've tried using the title keyword in the hist command (i.e. title='My collection of histogram plots'), but that didn't work.\n\nThe following code does work (in an ipython notebook) by adding text to one of the axes, but is a bit of a kludge.\n\naxes[0,1].text(0.5, 1.4,'My collection of histogram plots', horizontalalignment='center',\n               verticalalignment='center', transform=axes[0,1].transAxes)\n\n\nIs there a better way?\n"
"It sounds somewhat weird， but I need to save the Pandas console output string to png pics. For example:\n\n&gt;&gt;&gt; df\n                   sales  net_pft     ROE    ROIC\nSTK_ID RPT_Date                                  \n600809 20120331  22.1401   4.9253  0.1651  0.6656\n       20120630  38.1565   7.8684  0.2567  1.0385\n       20120930  52.5098  12.4338  0.3587  1.2867\n       20121231  64.7876  13.2731  0.3736  1.2205\n       20130331  27.9517   7.5182  0.1745  0.3723\n       20130630  40.6460   9.8572  0.2560  0.4290\n       20130930  53.0501  11.8605  0.2927  0.4369 \n\n\nIs there any way like df.output_as_png(filename='df_data.png') to generate a pic file which just display above content inside?\n"
'Say I have a dataframe with 3 columns: Date, Ticker, Value (no index, at least to start with).  I have many dates and many tickers, but each (ticker, date) tuple is unique.  (But obviously the same date will show up in many rows since it will be there for multiple tickers, and the same ticker will show up in multiple rows since it will be there for many dates.)\n\nInitially, my rows in a specific order, but not sorted by any of the columns.\n\nI would like to compute first differences (daily changes) of each ticker (ordered by date) and put these in a new column in my dataframe.  Given this context, I cannot simply do\n\ndf[\'diffs\'] = df[\'value\'].diff()\n\n\nbecause adjacent rows do not come from the same ticker.  Sorting like this:\n\ndf = df.sort([\'ticker\', \'date\'])\ndf[\'diffs\'] = df[\'value\'].diff()\n\n\ndoesn\'t solve the problem because there will be "borders".  I.e. after that sort, the last value for one ticker will be above the first value for the next ticker.  And computing differences then would take a difference between two tickers.  I don\'t want this.  I want the earliest date for each ticker to wind up with an NaN in its diff column.\n\nThis seems like an obvious time to use groupby but for whatever reason, I can\'t seem to get it to work properly.  To be clear, I would like to perform the following process:\n\n\nGroup rows based on their ticker\nWithin each group, sort rows by their date\nWithin each sorted group, compute differences of the value column\nPut these differences into the original dataframe in a new diffs column (ideally leaving the original dataframe order in tact.)\n\n\nI have to imagine this is a one-liner.  But what am I missing?\n\n\n\nEdit at 9:00pm 2013-12-17\n\nOk...some progress.  I can do the following to get a new dataframe:\n\nresult = df.set_index([\'ticker\', \'date\'])\\\n    .groupby(level=\'ticker\')\\\n    .transform(lambda x: x.sort_index().diff())\\\n    .reset_index()\n\n\nBut if I understand the mechanics of groupby, my rows will now be sorted first by ticker and then by date.  Is that correct?  If so, would I need to do a merge to append the differences column (currently in result[\'current\'] to the original dataframe df?\n'
"I have 2 DataFrames df1 and df2 with the same column names ['a','b','c'] and indexed by dates.\nThe date index can have similar values.\nI would like to create a DataFrame df3 with only the data from columns ['c'] renamed respectively 'df1' and 'df2' and with the correct date index. My problem is that I cannot get how to merge the index properly.\n\ndf1 = pd.DataFrame(np.random.randn(5,3), index=pd.date_range('01/02/2014',periods=5,freq='D'), columns=['a','b','c'] )\ndf2 = pd.DataFrame(np.random.randn(8,3), index=pd.date_range('01/01/2014',periods=8,freq='D'), columns=['a','b','c'] )\ndf1\n                 a        b            c\n2014-01-02   0.580550    0.480814    1.135899\n2014-01-03  -1.961033    0.546013    1.093204\n2014-01-04   2.063441   -0.627297    2.035373\n2014-01-05   0.319570    0.058588    0.350060\n2014-01-06   1.318068   -0.802209   -0.939962\n\ndf2\n                 a        b            c\n2014-01-01   0.772482    0.899337    0.808630\n2014-01-02   0.518431   -1.582113    0.323425\n2014-01-03   0.112109    1.056705   -1.355067\n2014-01-04   0.767257   -2.311014    0.340701\n2014-01-05   0.794281   -1.954858    0.200922\n2014-01-06   0.156088    0.718658   -1.030077\n2014-01-07   1.621059    0.106656   -0.472080\n2014-01-08  -2.061138   -2.023157    0.257151\n\n\nThe df3 DataFrame should have the following form : \n\ndf3\n                 df1        df2\n2014-01-01   NaN        0.808630\n2014-01-02   1.135899   0.323425\n2014-01-03   1.093204   -1.355067\n2014-01-04   2.035373   0.340701\n2014-01-05   0.350060   0.200922\n2014-01-06   -0.939962  -1.030077\n2014-01-07   NaN        -0.472080\n2014-01-08   NaN        0.257151\n\n\nBut with NaN in the df1 column as the date index of df2 is wider. (In this example, I would get NaN for the ollowing dates : 2014-01-01, 2014-01-07 and 2014-01-08)\n\nThanks for your help.\n"
'I was wondering if there is an elegant and shorthand way in Pandas DataFrames to select columns by data type (dtype). i.e. Select only int64 columns from a DataFrame.\n\nTo elaborate, something along the lines of\n\ndf.select_columns(dtype=float64)\n\n\nThanks in advance for the help\n'
"Is there a way to look back to a previous row, and calculate a new variable?  so as long as the previous row is the same case what is the (previous change) - (current change), and attribute it to the previous 'ChangeEvent' in new columns?\n\nhere is my DataFrame\n\n&gt;&gt;&gt; df\n  ChangeEvent StartEvent  case              change      open  \n0    Homeless   Homeless     1 2014-03-08 00:00:00 2014-02-08  \n1       other   Homeless     1 2014-04-08 00:00:00 2014-02-08     \n2    Homeless   Homeless     1 2014-05-08 00:00:00 2014-02-08      \n3        Jail   Homeless     1 2014-06-08 00:00:00 2014-02-08     \n4        Jail       Jail     2 2014-06-08 00:00:00 2014-02-08   \n\n\nto add columns\n\nJail  Homeless case\n 0    6        1\n 0    30       1\n 0    0        1\n\n\n... and so on\n\nhere is the df build\n\nimport pandas as pd\nimport datetime as DT\nd = {'case' : pd.Series([1,1,1,1,2]),\n'open' : pd.Series([DT.datetime(2014, 3, 2), DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2),DT.datetime(2014, 3, 2)]),\n'change' : pd.Series([DT.datetime(2014, 3, 8), DT.datetime(2014, 4, 8),DT.datetime(2014, 5, 8),DT.datetime(2014, 6, 8),DT.datetime(2014, 6, 8)]),\n'StartEvent' : pd.Series(['Homeless','Homeless','Homeless','Homeless','Jail']),\n'ChangeEvent' : pd.Series(['Homeless','irrelivant','Homeless','Jail','Jail']),\n'close' : pd.Series([DT.datetime(2015, 3, 2), DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2),DT.datetime(2015, 3, 2)])}\ndf=pd.DataFrame(d)\n\n"
"I'm generating a number of dataframes with the same shape, and I want to compare them to one another. I want to be able to get the mean and median across the dataframes.\n\n         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.001182  0.184535  0.814230  0.000054\n1        0.000001  0.160490  0.839508  0.000001\n2        0.000001  0.173829  0.826114  0.000055\n3        0.000432  0.180065  0.819502  0.000001\n4        0.000152  0.157041  0.842694  0.000113\n5        0.000183  0.174142  0.825674  0.000001\n6        0.000001  0.151556  0.848405  0.000038\n7        0.000771  0.177583  0.821645  0.000001\n8        0.000001  0.202059  0.797939  0.000001\n9        0.000025  0.189537  0.810410  0.000028\n10       0.006142  0.003041  0.493912  0.496905\n11       0.003739  0.002367  0.514216  0.479678\n12       0.002334  0.001517  0.529041  0.467108\n13       0.003458  0.000001  0.532265  0.464276\n14       0.000405  0.005655  0.527576  0.466364\n15       0.002557  0.003233  0.507954  0.486256\n16       0.004161  0.000001  0.491271  0.504568\n17       0.001364  0.001330  0.528311  0.468996\n18       0.002886  0.000001  0.506392  0.490721\n19       0.001823  0.002498  0.509620  0.486059\n\n         Source.0  Source.1  Source.2  Source.3\ncluster                                        \n0        0.000001  0.197108  0.802495  0.000396\n1        0.000001  0.157860  0.842076  0.000063\n2        0.094956  0.203057  0.701662  0.000325\n3        0.000001  0.181948  0.817841  0.000210\n4        0.000003  0.169680  0.830316  0.000001\n5        0.000362  0.177194  0.822443  0.000001\n6        0.000001  0.146807  0.852924  0.000268\n7        0.001087  0.178994  0.819564  0.000354\n8        0.000001  0.202182  0.797333  0.000485\n9        0.000348  0.181399  0.818252  0.000001\n10       0.003050  0.000247  0.506777  0.489926\n11       0.004420  0.000001  0.513927  0.481652\n12       0.006488  0.001396  0.527197  0.464919\n13       0.001510  0.000001  0.525987  0.472502\n14       0.000001  0.000001  0.520737  0.479261\n15       0.000001  0.001765  0.515658  0.482575\n16       0.000001  0.000001  0.492550  0.507448\n17       0.002855  0.000199  0.526535  0.470411\n18       0.000001  0.001952  0.498303  0.499744\n19       0.001232  0.000001  0.506612  0.492155\n\n\nThen I want to get the mean of these two dataframes.\n\nWhat is the easiest way to do this?\n\nJust to clarify I want to get the mean for each particular cell when the indexes and columns of all the dataframes are exactly the same.\n\nSo in the example I gave, the average for [0,Source.0] would be (0.001182 + 0.000001) / 2 = 0.0005915.\n"
"If I have a dataframe with multiple columns ['x', 'y', 'z'], how do I forward fill only one column 'x'? Or a group of columns ['x','y']?\nI only know how to do it by axis.\n"
"I have a very big csv file so that I can not read them all into the memory. I only want to read and process a few lines in it. So I am seeking a function in Pandas which could handle this task, which the basic python can handle this well:\n\nwith open('abc.csv') as f:\n    line = f.readline()\n    # pass until it reaches a particular line number....\n\n\nHowever, if I do this in pandas, I always read the first line:\n\ndatainput1 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\ndatainput2 = pd.read_csv('matrix.txt',sep=',', header = None, nrows = 1 )\n\n\nI am looking for some easier way to handle this task in pandas. For example, if I want to read rows from 1000 to 2000. How can I do this quickly? \n\nI want to use pandas because I want to read data into the dataframe.\n"
"I have two separate dataframes that share a project number. In type_df, the project number is the index. In time_df, the project number is a column. I would like to count the number of rows in type_df that have a Project Type of 2. I am trying to do this with pandas.merge(). It works great when using both columns, but not indices. I'm not sure how to reference the index and if merge is even the right way to do this.\n\nimport pandas as pd\ntype_df = pd.DataFrame(data = [['Type 1'], ['Type 2']], \n                       columns=['Project Type'], \n                       index=['Project2', 'Project1'])\ntime_df = pd.DataFrame(data = [['Project1', 13], ['Project1', 12], \n                               ['Project2', 41]], \n                       columns=['Project', 'Time'])\nmerged = pd.merge(time_df,type_df, on=[index,'Project'])\nprint merged[merged['Project Type'] == 'Type 2']['Project Type'].count()\n\n\nError:\n\n\n  Name 'Index' is not defined.\n\n\nDesired Output:\n\n2\n\n"
"Given a square pandas DataFrame of the following form:\n   a  b  c\na  1 .5 .3\nb .5  1 .4\nc .3 .4  1\n\nHow can the upper triangle be melted to get a matrix of the following form\n Row     Column    Value\n  a        a       1\n  a        b       .5 \n  a        c       .3\n  b        b       1\n  b        c       .4\n  c        c       1 \n\n#Note the combination a,b is only listed once.  There is no b,a listing     \n\nI'm more interested in an idiomatic pandas solution, a custom indexer would be easy enough to write by hand...\nThank you in advance for your consideration and response.\n"
"I am trying to filter the columns in a pandas dataframe based on whether they are of type date or not.  I can figure out which ones are, but then would have to parse that output or manually select columns.  I want to select date columns automatically.  Here's what I have so far as an example - I'd want to only select the 'date_col' column in this case.\n\nimport pandas as pd\ndf = pd.DataFrame([['Feb-2017', 1, 2],\n                   ['Mar-2017', 1, 2],\n                   ['Apr-2017', 1, 2],\n                   ['May-2017', 1, 2]], \n                  columns=['date_str', 'col1', 'col2'])\ndf['date_col'] = pd.to_datetime(df['date_str'])\ndf.dtypes\n\n\nOut:\n\ndate_str            object\ncol1                 int64\ncol2                 int64\ndate_col    datetime64[ns]\ndtype: object\n\n"
'I have a pandas data set, called \'df\'.\n\nHow can I do something like below;\n\ndf.query("select * from df")\n\n\nThank you.\n\nFor those who know R, there is a library called sqldf where you can execute SQL code in R, my question is basically, is there some library like sqldf in python\n'
"Hi: I am trying to use the Pandas DataFrame.to_csv method to save a dataframe to a csv file:\n\nfilename = './dir/name.csv'\n\ndf.to_csv(filename)\n\n\nHowever I am getting the error:\n\nIOError: [Errno 2] No such file or directory: './dir/name.csv'\n\n\nShouldn't the to_csv method be able to create the file if it doesn't exist? This is what I am intending for it to do.\n"
'I get ValueError: cannot convert float NaN to integer for following:\n\ndf = pandas.read_csv(\'zoom11.csv\')\ndf[[\'x\']] = df[[\'x\']].astype(int)\n\n\n\nThe "x" is obviously a column in the csv file, but I cannot spot any float NaN in the file, and dont get what does it mean by this. \nWhen I read the column as String, then it has values like -1,0,1,...2000, all look very nice int numbers to me.\nWhen I read the column as float, then this can be loaded. Then it shows values as -1.0,0.0 etc, still there are no any NaN-s\nI tried with error_bad_lines = False and dtype parameter in read_csv to no avail. It just cancels loading with same exception.\nThe file is not small (10+ M rows), so cannot inspect it manually, when I extract a small header part, then there is no error, but it happens with full file. So it is something in the file, but cannot detect what.\nLogically the csv should not have missing values, but even if there is some garbage then I would be ok to skip the rows. Or at least identify them, but I do not see way to scan through file and report conversion errors.\n\n\nUpdate: Using the hints in comments/answers I got my data clean with this:\n\n# x contained NaN\ndf = df[~df[\'x\'].isnull()]\n\n# Y contained some other garbage, so null check was not enough\ndf = df[df[\'y\'].str.isnumeric()]\n\n# final conversion now worked\ndf[[\'x\']] = df[[\'x\']].astype(int)\ndf[[\'y\']] = df[[\'y\']].astype(int)\n\n'
'I have a textfile where columns are separated by variable amounts of whitespace. Is it possible to load this file directly as a pandas dataframe without pre-processing the file? In the pandas documentation the delimiter section says that I can use a \'s*\' construct but I couldn\'t get this to work. \n\n## sample data\nhead sample.txt\n\n#                                                                            --- full sequence --- -------------- this domain -------------   hmm coord   ali coord   env coord\n# target name        accession   tlen query name           accession   qlen   E-value  score  bias   #  of  c-Evalue  i-Evalue  score  bias  from    to  from    to  from    to  acc description of target\n#------------------- ---------- ----- -------------------- ---------- ----- --------- ------ ----- --- --- --------- --------- ------ ----- ----- ----- ----- ----- ----- ----- ---- ---------------------\nABC_membrane         PF00664.18   275 AAF67494.2_AF170880  -            615     8e-29  100.7  11.4   1   1     3e-32     1e-28  100.4   7.9     3   273    42   313    40   315 0.95 ABC transporter transmembrane region\nABC_tran             PF00005.22   118 AAF67494.2_AF170880  -            615   2.6e-20   72.8   0.0   1   1   1.9e-23   6.4e-20   71.5   0.0     1   118   402   527   402   527 0.93 ABC transporter\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   1   2    0.0036        12    4.9   0.0    27    40   391   404   383   408 0.86 RecF/RecN/SMC N terminal domain\nSMC_N                PF02463.14   220 AAF67494.2_AF170880  -            615   3.8e-08   32.7   0.2   2   2   1.8e-09   6.1e-06   25.4   0.0   116   210   461   568   428   575 0.85 RecF/RecN/SMC N terminal domain\nAAA_16               PF13191.1    166 AAF67494.2_AF170880  -            615   3.1e-06   27.5   0.3   1   1     2e-09     7e-06   26.4   0.2    20   158   386   544   376   556 0.72 AAA ATPase domain\nYceG                 PF02618.11   297 AAF67495.1_AF170880  -            284   3.4e-64  216.6   0.0   1   1   2.9e-68     4e-64  216.3   0.0    68   296    53   274    29   275 0.85 YceG-like family\nPyr_redox_3          PF13738.1    203 AAF67496.2_AF170880  -            352   2.9e-28   99.1   0.0   1   2   2.8e-30   4.8e-27   95.2   0.0     1   201     4   198     4   200 0.85 Pyridine nucleotide-disulphide oxidoreductase\n\n#load data\nfrom pandas import *\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep=" ")\n\nValueError: Expecting 83 columns, got 91 in row 4\n\n#load data part 2\ndata = read_table(\'sample.txt\', skiprows=3, header=None, sep="\'s*\' ")\n#this mushes some of the columns into the first column and drops the rest.\n    X.1\n1    ABC_tran PF00005.22 118 AAF67494.2_\n2    SMC_N PF02463.14 220 AAF67494.2_\n3    SMC_N PF02463.14 220 AAF67494.2_\n4    AAA_16 PF13191.1 166 AAF67494.2_\n5    YceG PF02618.11 297 AAF67495.1_\n6    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n7    Pyr_redox_3 PF13738.1 203 AAF67496.2_\n8    FMO-like PF00743.14 532 AAF67496.2_\n9    FMO-like PF00743.14 532 AAF67496.2_\n\n\nWhile I can preprocess the files to change the whitespace to commas/tabs it would be nice to load them directly.\n\n(FYI this is the *.hmmdomtblout output from the hmmscan program)\n'
'Is there a way to preserve the order of the columns in a csv file when read and the write with Python Pandas? For example, in this code\n\nimport pandas as pd\n\ndata = pd.read_csv(filename)\ndata.to_csv(filename)\n\n\nthe output files might be different because the columns are not preserved.\n'
'I\'m wondering if there is a simpler, memory efficient way to select a subset of rows and columns from a pandas DataFrame.\n\nFor instance, given this dataframe:\n\n\ndf = DataFrame(np.random.rand(4,5), columns = list(\'abcde\'))\nprint df\n\n          a         b         c         d         e\n0  0.945686  0.000710  0.909158  0.892892  0.326670\n1  0.919359  0.667057  0.462478  0.008204  0.473096\n2  0.976163  0.621712  0.208423  0.980471  0.048334\n3  0.459039  0.788318  0.309892  0.100539  0.753992\n\n\nI want only those rows in which the value for column \'c\' is greater than 0.5, but I only need columns \'b\' and \'e\' for those rows.\n\nThis is the method that I\'ve come up with - perhaps there is a better "pandas" way?\n\n\nlocs = [df.columns.get_loc(_) for _ in [\'a\', \'d\']]\nprint df[df.c > 0.5][locs]\n\n          a         d\n0  0.945686  0.892892\n\n\nMy final goal is to convert the result to a numpy array to pass into an sklearn regression algorithm, so I will use the code above like this:\n\n\ntraining_set = array(df[df.c > 0.5][locs])\n\n\n... and that peeves me since I end up with a huge array copy in memory. Perhaps there\'s a better way for that too?\n'
'Can some please explain the difference between the asfreq and resample methods in pandas? When should one use what?\n'
"I recently found dask module that aims to be an easy-to-use python parallel processing module. Big selling point for me is that it works with pandas.\n\nAfter reading a bit on its manual page, I can't find a way to do this trivially parallelizable task: \n\nts.apply(func) # for pandas series\ndf.apply(func, axis = 1) # for pandas DF row apply\n\n\nAt the moment, to achieve this in dask, AFAIK,\n\nddf.assign(A=lambda df: df.apply(func, axis=1)).compute() # dask DataFrame\n\n\nwhich is ugly syntax and is actually slower than outright\n\ndf.apply(func, axis = 1) # for pandas DF row apply\n\n\nAny suggestion?\n\nEdit: Thanks @MRocklin for the map function. It seems to be slower than plain pandas apply. Is this related to pandas GIL releasing issue or am I doing it wrong?\n\nimport dask.dataframe as dd\ns = pd.Series([10000]*120)\nds = dd.from_pandas(s, npartitions = 3)\n\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a &gt; 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n\ns.apply(slow_func) # 0.43 sec\nds.map(slow_func).compute() # 2.04 sec\n\n"
'I am trying to append an empty row at the end of dataframe but unable to do so, even trying to understand how pandas work with append function and still not getting it.\n\nHere\'s the code:\n\nimport pandas as pd\n\nexcel_names = ["ARMANI+EMPORIO+AR0143-book.xlsx"]\nexcels = [pd.ExcelFile(name) for name in excel_names]\nframes = [x.parse(x.sheet_names[0], header=None,index_col=None).dropna(how=\'all\') for x in excels]\nfor f in frames:\n    f.append(0, float(\'NaN\'))\n    f.append(2, float(\'NaN\'))\n\n\nThere are two columns and random number of row.\n\nwith "print f" in for loop i Get this:\n\n                             0                 1\n0                   Brand Name    Emporio Armani\n2                 Model number            AR0143\n4                  Part Number            AR0143\n6                   Item Shape       Rectangular\n8   Dial Window Material Type           Mineral\n10               Display Type          Analogue\n12                 Clasp Type            Buckle\n14               Case Material   Stainless steel\n16              Case Diameter    31 millimetres\n18               Band Material           Leather\n20                 Band Length  Women\'s Standard\n22                 Band Colour             Black\n24                 Dial Colour             Black\n26            Special Features       second-hand\n28                    Movement            Quartz\n\n'
"import pandas as pd\ndata={'col1':[1,3,3,1,2,3,2,2]}\ndf=pd.DataFrame(data,columns=['col1'])\nprint df\n\n\n         col1  \n    0     1          \n    1     3          \n    2     3          \n    3     1          \n    4     2          \n    5     3          \n    6     2          \n    7     2          \n\n\nI have the following Pandas DataFrame and I want to create another column that compares the previous row of col1 to see if they are equal.  What would be the best way to do this?  It would be like the following DataFrame.  Thanks\n\n    col1  match  \n0     1   False     \n1     3   False     \n2     3   True     \n3     1   False     \n4     2   False     \n5     3   False     \n6     2   False     \n7     2   True     \n\n"
'Pretty sure this is very simple.\n\nI am reading a csv file and have the dataframe:\n\nAttribute    A   B   C\na            1   4   7\nb            2   5   8\nc            3   6   9\n\n\nI want to do a transpose to get\n\nAttribute    a   b   c\nA            1   2   3\nB            4   5   6\nC            7   8   9\n\n\nHowever, when I do df.T, it results in\n\n             0   1   2 \nAttribute    a   b   c\nA            1   2   3\nB            4   5   6\nC            7   8   9`\n\n\nHow do I get rid of the indexes on top?\n'
'I am a newbie to pandas, tried searching this on google but still no luck. How can I get the rows by distinct values in column2?\n\nFor example, I have the dataframe bellow:\n\n&gt;&gt;&gt; df\nCOL1   COL2\na.com  22\nb.com  45\nc.com  34\ne.com  45\nf.com  56\ng.com  22\nh.com  45\n\n\nI want to get the rows based on unique values in COL2\n\n&gt;&gt;&gt; df\nCOL1  COL2\na.com 22\nb.com 45\nc.com 34\nf.com 56\n\n\nSo, how can I get that? I would appreciate it very much if anyone can provide any help.\n'
"I have a dataframe in which I would like to store 'raw' numpy.array:\n\ndf['COL_ARRAY'] = df.apply(lambda r: np.array(do_something_with_r), axis=1)\n\n\nbut it seems that pandas tries to 'unpack' the numpy.array.\n\nIs there a workaround? Other than using a wrapper (see edit below)?\n\nI tried reduce=False with no success.\n\nEDIT\n\nThis works, but I have to use the 'dummy' Data class to wrap around the array, which is unsatisfactory and not very elegant.\n\nclass Data:\n    def __init__(self, v):\n        self.v = v\n\nmeas = pd.read_excel(DATA_FILE)\nmeas['DATA'] = meas.apply(\n    lambda r: Data(np.array(pd.read_csv(r['filename'])))),\n    axis=1\n)\n\n"
'I have a data frame and I would like to group it by a particular column (or, in other words, by values from a particular column). I can do it in the following way: grouped = df.groupby([\'ColumnName\']).\n\nI imagine the result of this operation as a table in which some cells can contain sets of values instead of single values. To get a usual table (i.e. a table in which every cell contains only one a single value) I need to indicate what function I want to use to transform the sets of values in the cells into single values.\n\nFor example I can replace sets of values by their sum, or by their minimal or maximal value. I can do it in the following way: grouped.sum() or grouped.min() and so on.\n\nNow I want to use different functions for different columns. I figured out that I can do it in the following way: grouped.agg({\'ColumnName1\':sum, \'ColumnName2\':min}).\n\nHowever, because of some reasons I cannot use first. In more details, grouped.first() works, but grouped.agg({\'ColumnName1\':first, \'ColumnName2\':first}) does not work. As a result I get a NameError: NameError: name \'first\' is not defined. So, my question is: Why does it happen and how to resolve this problem.\n\nADDED\n\nHere I found the following example:\n\ngrouped[\'D\'].agg({\'result1\' : np.sum, \'result2\' : np.mean})\n\n\nMay be I also need to use np? But in my case python does not recognize "np". Should I import it? \n'
'I am just getting started with pandas in the IPython Notebook and encountering the following problem: When a DataFrame read from a CSV file is small, the IPython Notebook displays it in a nice table view. When the DataFrame is large, something like this is ouput:\n\nIn [27]:\n\nevaluation = readCSV("evaluation_MO_without_VNS_quality.csv").filter(["solver", "instance", "runtime", "objective"])\n\nIn [37]:\n\nevaluation\n\nOut[37]:\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 333 entries, 0 to 332\nData columns:\nsolver       333  non-null values\ninstance     333  non-null values\nruntime      333  non-null values\nobjective    333  non-null values\ndtypes: int64(1), object(3)\n\n\nI would like to see a small portion of the data frame as a table just to make sure it is in the right format. What options do I have?\n'
"I'm attempting to read a simple space-separated file with pandas read_csv method.  However, pandas doesn't seem to be obeying my dtype argument.  Maybe I'm incorrectly specifying it?\n\nI've distilled down my somewhat complicated call to read_csv to this simple test case.  I'm actually using the converters argument in my 'real' scenario but I removed this for simplicity.\n\nBelow is my ipython session:\n\n&gt;&gt;&gt; cat test.out\na b\n0.76398 0.81394\n0.32136 0.91063\n&gt;&gt;&gt; import pandas\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; x = pandas.read_csv('test.out', dtype={'a': numpy.float32}, delim_whitespace=True)\n&gt;&gt;&gt; x\n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n&gt;&gt;&gt; x.a.dtype\ndtype('float64')\n\n\nI've also tried this using this with a dtype of numpy.int32 or numpy.int64.  These choices result in an exception:\n\nAttributeError: 'NoneType' object has no attribute 'dtype'\n\n\nI'm assuming the AttributeError is because pandas will not automatically try to convert/truncate the float values into an integer?\n\nI'm running on a 32-bit machine with a 32-bit version of Python.\n\n&gt;&gt;&gt; !uname -a\nLinux ubuntu 3.0.0-13-generic #22-Ubuntu SMP Wed Nov 2 13:25:36 UTC 2011 i686 i686 i386 GNU/Linux\n&gt;&gt;&gt; import platform\n&gt;&gt;&gt; platform.architecture()\n('32bit', 'ELF')\n&gt;&gt;&gt; pandas.__version__\n'0.10.1'\n\n"
"I have a data frame with alpha-numeric keys which I want to save as a csv and read back later. For various reasons I need to explicitly read this key column as a string format, I have keys which are strictly numeric or even worse, things like: 1234E5 which Pandas interprets as a float. This obviously makes the key completely useless.\n\nThe problem is when I specify a string dtype for the data frame or any column of it I just get garbage back. I have some example code here:\n\ndf = pd.DataFrame(np.random.rand(2,2),\n                  index=['1A', '1B'],\n                  columns=['A', 'B'])\ndf.to_csv(savefile)\n\n\nThe data frame looks like:\n\n           A         B\n1A  0.209059  0.275554\n1B  0.742666  0.721165\n\n\nThen I read it like so:\n\ndf_read = pd.read_csv(savefile, dtype=str, index_col=0)\n\n\nand the result is:\n\n   A  B\nB  (  &lt;\n\n\nIs this a problem with my computer, or something I'm doing wrong here, or just a bug?\n"
"I come from a sql background and I use the following data processing step frequently:\n\n\nPartition the table of data by one or more fields\nFor each partition, add a rownumber to each of its rows that ranks the row by one or more other fields, where the analyst specifies ascending or descending\n\n\nEX:  \n\ndf = pd.DataFrame({'key1' : ['a','a','a','b','a'],\n           'data1' : [1,2,2,3,3],\n           'data2' : [1,10,2,3,30]})\ndf\n     data1        data2     key1    \n0    1            1         a           \n1    2            10        a        \n2    2            2         a       \n3    3            3         b       \n4    3            30        a        \n\n\nI'm looking for how to do the PANDAS equivalent to this sql window function:\n\nRN = ROW_NUMBER() OVER (PARTITION BY Key1 ORDER BY Data1 ASC, Data2 DESC)\n\n\n    data1        data2     key1    RN\n0    1            1         a       1    \n1    2            10        a       2 \n2    2            2         a       3\n3    3            3         b       1\n4    3            30        a       4\n\n\nI've tried the following which I've gotten to work where there are no 'partitions':\n\ndef row_number(frame,orderby_columns, orderby_direction,name):\n    frame.sort_index(by = orderby_columns, ascending = orderby_direction, inplace = True)\n    frame[name] = list(xrange(len(frame.index)))\n\n\nI tried to extend this idea to work with partitions (groups in pandas) but the following didn't work:\n\ndf1 = df.groupby('key1').apply(lambda t: t.sort_index(by=['data1', 'data2'], ascending=[True, False], inplace = True)).reset_index()\n\ndef nf(x):\n    x['rn'] = list(xrange(len(x.index)))\n\ndf1['rn1'] = df1.groupby('key1').apply(nf)\n\n\nBut I just got a lot of NaNs when I do this.\n\nIdeally, there'd be a succinct way to replicate the window function capability of sql (i've figured out the window based aggregates...that's a one liner in pandas)...can someone share with me the most idiomatic way to number rows like this in PANDAS?\n"
"I want to create a new column in Pandas using a string sliced for another column in the dataframe.\n\nFor example.\n\nSample  Value  New_sample\nAAB     23     A\nBAB     25     B\n\n\nWhere New_sample is a new column formed from a simple [:1] slice of Sample\n\nI've tried a number of things to no avail - I feel I'm missing something simple.\n\nWhat's the most efficient way of doing this?\n"
'I have a Pandas dataframe, and I want to create a new column whose values are that of another column, shifted down by one row. The last row should show NaN.\n\nThe catch is that I want to do this by group, with the last row of each group showing NaN. NOT have the last row of a group "steal" a value from a group that happens to be adjacent in the dataframe.\n\nMy attempted implementation is quite shamefully broken, so I\'m clearly misunderstanding something fundamental.\n\ndf[\'B_shifted\'] = df.groupby([\'A\'])[\'B\'].transform(lambda x:x.values[1:])\n\n'
"Have a look at the graph below:\n\n\nIt's a subplot of this larger figure:\n\n\nI see two problems with it.  First, the x-axis labels overlap with one another (this is my major issue).  Second. the location of the x-axis minor gridlines seems a bit wonky.  On the left of the graph, they look properly spaced.  But on the right, they seem to be crowding the major gridlines...as if the major gridline locations aren't proper multiples of the minor tick locations.\n\nMy setup is that I have a DataFrame called df which has a DatetimeIndex on the rows and a column called value which contains floats.  I can provide an example of the df contents in a gist if necessary.  A dozen or so lines of df are at the bottom of this post for reference.\n\nHere's the code that produces the figure:\n\nnow = dt.datetime.now()\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 8), dpi=200)\nfor i, d in enumerate([360, 30, 7, 1]):\n    ax = axes.flatten()[i]\n    earlycut = now - relativedelta(days=d)\n    data = df.loc[df.index&gt;=earlycut, :]\n    ax.plot(data.index, data['value'])\n    ax.xaxis_date()\n\n    ax.get_xaxis().set_minor_locator(mpl.ticker.AutoMinorLocator())\n    ax.get_yaxis().set_minor_locator(mpl.ticker.AutoMinorLocator())\n\n    ax.grid(b=True, which='major', color='w', linewidth=1.5)\n    ax.grid(b=True, which='minor', color='w', linewidth=0.75)\n\n\nWhat is my best option here to get the x-axis labels to stop overlapping each other (in each of the four subplots)?  Also, separately (but less urgently), what's up with the minor tick issue in the top-left subplot?\n\nI am on Pandas 0.13.1, numpy 1.8.0, and matplotlib 1.4.x.\n\nHere's a small snippet of df for reference:\n\n                                    id scale  tempseries_id    value\ntimestamp                                                           \n2014-11-02 14:45:10.302204+00:00  7564     F              1  68.0000\n2014-11-02 14:25:13.532391+00:00  7563     F              1  68.5616\n2014-11-02 14:15:12.102229+00:00  7562     F              1  68.9000\n2014-11-02 14:05:13.252371+00:00  7561     F              1  69.0116\n2014-11-02 13:55:11.792191+00:00  7560     F              1  68.7866\n2014-11-02 13:45:10.782227+00:00  7559     F              1  68.6750\n2014-11-02 13:35:10.972248+00:00  7558     F              1  68.4500\n2014-11-02 13:25:10.362213+00:00  7557     F              1  68.1116\n2014-11-02 13:15:10.822247+00:00  7556     F              1  68.2250\n2014-11-02 13:05:10.102200+00:00  7555     F              1  68.5616\n2014-11-02 12:55:10.292217+00:00  7554     F              1  69.0116\n2014-11-02 12:45:10.382226+00:00  7553     F              1  69.3500\n2014-11-02 12:35:10.642245+00:00  7552     F              1  69.2366\n2014-11-02 12:25:12.642255+00:00  7551     F              1  69.1250\n2014-11-02 12:15:11.122382+00:00  7550     F              1  68.7866\n2014-11-02 12:05:11.332224+00:00  7549     F              1  68.5616\n2014-11-02 11:55:11.662311+00:00  7548     F              1  68.2250\n2014-11-02 11:45:11.122193+00:00  7547     F              1  68.4500\n2014-11-02 11:35:11.162271+00:00  7546     F              1  68.7866\n2014-11-02 11:25:12.102211+00:00  7545     F              1  69.2366\n2014-11-02 11:15:10.422226+00:00  7544     F              1  69.4616\n2014-11-02 11:05:11.412216+00:00  7543     F              1  69.3500\n2014-11-02 10:55:10.772212+00:00  7542     F              1  69.1250\n2014-11-02 10:45:11.332220+00:00  7541     F              1  68.7866\n2014-11-02 10:35:11.332232+00:00  7540     F              1  68.5616\n2014-11-02 10:25:11.202411+00:00  7539     F              1  68.2250\n2014-11-02 10:15:11.932326+00:00  7538     F              1  68.5616\n2014-11-02 10:05:10.922229+00:00  7537     F              1  68.9000\n2014-11-02 09:55:11.602357+00:00  7536     F              1  69.3500\n\n\nEdit: Trying fig.autofmt_xdate():\nI don't think this going to do the trick.  This seems to use the same x-tick labels for both graphs on the left and also for both graphs on the right.  Which is not correct given my data.  Please see the problematic output below:\n\n\n"
"I read my data\n\nimport pandas as pd\ndf = pd.read_csv('/path/file.tsv', header=0, delimiter='\\t')\nprint df\n\n\nand get:\n\n          id    text\n0    361.273    text1...\n1    374.350    text2...\n2    374.350    text3...\n\n\nHow can I delete the id column from the above data frame?. I tried the following:\n\nimport pandas as pd\ndf = pd.read_csv('/path/file.tsv', header=0, delimiter='\\t')\nprint df.drop('id', 1)\n\n\nBut it raises this exception:\n\nValueError: labels ['id'] not contained in axis\n\n"
"I'm trying to apply a function to all rows of a pandas DataFrame (actually just one column in that DataFrame)\n\nI'm sure this is a syntax error but I'm know sure what I'm doing wrong\n\ndf['col'].apply(lambda x, y:(x - y).total_seconds(), args=[d1], axis=1)\n\n\nThe col column contains a bunch a datetime.datetime objects and and d1 is the earliest of them.  I'm trying to get a column of the total number of seconds for each of the rows\n\nEDIT I keep getting the following error\n\nTypeError: &lt;lambda&gt;() got an unexpected keyword argument 'axis'\n\n\nI don't understand why axis is getting passed to my lambda function\n\nEDIT 2\n\nI've also tried doing\n\ndef diff_dates(d1, d2):\n    return (d1-d2).total_seconds()\n\ndf['col'].apply(diff_dates, args=[d1], axis=1)\n\n\nAnd I get the same error\n"
"I have the dataframe shown below. I need to get the scalar value of column B, dependent on the value of A (which is a variable in my script). I'm trying the loc() function but it returns a Series instead of a scalar value. How do I get the scalar value()?\n\n&gt;&gt;&gt; x = pd.DataFrame({'A' : [0,1,2], 'B' : [4,5,6]})\n&gt;&gt;&gt; x\n   A  B\n0  0  4\n1  1  5\n2  2  6\n\n&gt;&gt;&gt; x.loc[x['A'] == 2]['B']\n2    6\nName: B, dtype: int64\n\n&gt;&gt;&gt; type(x.loc[x['A'] == 2]['B'])\n&lt;class 'pandas.core.series.Series'&gt;\n\n"
"How can I get the total number of hours in a Pandas timedelta?\n\nFor example:\n\n&gt;&gt;&gt; td = pd.Timedelta('1 days 2 hours')\n&gt;&gt;&gt; td.get_total_hours()\n26\n\n\nNote: as per the documentation, the .hours attribute will return the hours component:\n\n&gt;&gt;&gt; td.hours\n2\n\n"
'I have a Pandas Dataframe generated from a database, which has data with mixed encodings. For example:\n\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n| ID | path                    | language | date       | longest_sentence                               | shortest_sentence                                      | number_words | readability_consensus |\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n| 0  | data/Eng/Sagitarius.txt | Eng      | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...  | 306          | 11th and 12th grade   |\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n| 31 | data/Nor/Høylandet.txt  | Nor      | 2015-07-22 | Høgskolen i Østfold er et eksempel...          | Som skuespiller har jeg både...                        | 253          | 15th and 16th grade   |\n+----+-------------------------+----------+------------+------------------------------------------------+--------------------------------------------------------+--------------+-----------------------+\n\n\nAs seen there is a mix of English and Norwegian (encoded as ISO-8859-1 in the database I think). I need to get the contents of this Dataframe output as a Markdown table, but without getting problems with encoding. I followed this answer (from the question Generate Markdown tables?) and got the following:\n\nimport sys, sqlite3\n\ndb = sqlite3.connect("Applications.db")\ndf = pd.read_sql_query("SELECT path, language, date, longest_sentence, shortest_sentence, number_words, readability_consensus FROM applications ORDER BY date(date) DESC", db)\ndb.close()\n\nrows = []\nfor index, row in df.iterrows():\n    items = (row[\'date\'], \n             row[\'path\'], \n             row[\'language\'], \n             row[\'shortest_sentence\'],\n             row[\'longest_sentence\'], \n             row[\'number_words\'], \n             row[\'readability_consensus\'])\n    rows.append(items)\n\nheadings = [\'Date\', \n            \'Path\', \n            \'Language\',\n            \'Shortest Sentence\', \n            \'Longest Sentence since\', \n            \'Words\',\n            \'Grade level\']\n\nfields = [0, 1, 2, 3, 4, 5, 6]\nalign = [(\'^\', \'&lt;\'), (\'^\', \'^\'), (\'^\', \'&lt;\'), (\'^\', \'^\'), (\'^\', \'&gt;\'),\n         (\'^\',\'^\'), (\'^\',\'^\')]\n\ntable(sys.stdout, rows, fields, headings, align)\n\n\nHowever, this yields an UnicodeEncodeError: \'ascii\' codec can\'t encode character u\'\\xe5\' in position 72: ordinal not in range(128) error. How can I output the Dataframe as a Markdown table? That is, for the purpose of storing this code in a file for use in writing a Markdown document. I need the output to look like this:\n\n| ID | path                    | language | date       | longest_sentence                               | shortest_sentence                                      | number_words | readability_consensus |\n|----|-------------------------|----------|------------|------------------------------------------------|--------------------------------------------------------|--------------|-----------------------|\n| 0  | data/Eng/Sagitarius.txt | Eng      | 2015-09-17 | With administrative experience in the prepa... | I am able to relocate internationally on short not...  | 306          | 11th and 12th grade   |\n| 31 | data/Nor/Høylandet.txt  | Nor      | 2015-07-22 | Høgskolen i Østfold er et eksempel...          | Som skuespiller har jeg både...                        | 253          | 15th and 16th grade   |\n\n'
'I have a problem where I produce a pandas dataframe by concatenating along the row axis (stacking vertically). \n\nEach of the constituent dataframes has an autogenerated index (ascending numbers). \n\nAfter concatenation, my index is screwed up: it counts up to n (where n is the shape[0] of the corresponding dataframe), and restarts at zero at the next dataframe. \n\nI am trying to "re-calculate the index, given the current order", or "re-index" (or so I thought).  Turns out that isn\'t exactly what DataFrame.reindex seems to be doing. \n\n\n\nHere is what I tried to do:\n\ntrain_df = pd.concat(train_class_df_list)\ntrain_df = train_df.reindex(index=[i for i in range(train_df.shape[0])])\n\n\nIt failed with "cannot reindex from a duplicate axis." I don\'t want to change the order of my data... just need to delete the old index and set up a new one, with the order of rows preserved. \n'
"Let's say that I have a dataframe like this one\n\nimport pandas as pd\ndf = pd.DataFrame([[1, 2, 1], [1, 3, 2], [4, 6, 3], [4, 3, 4], [5, 4, 5]], columns=['A', 'B', 'C'])\n\n&gt;&gt; df\n   A  B  C\n0  1  2  1\n1  1  3  2\n2  4  6  3\n3  4  3  4\n4  5  4  5\n\n\nThe original table is more complicated with more columns and rows.\n\nI want to get the first row that fulfil some criteria. Examples:\n\n\nGet first row where A > 3 (returns row 2)\nGet first row where A > 4 AND B > 3 (returns row 4)\nGet first row where A > 3 AND (B > 3 OR C > 2) (returns row 2)\n\n\nBut, if there isn't any row that fulfil the specific criteria, then I want to get the first one after I just sort it descending by A (or other cases by B, C etc)\n\n\nGet first row where A > 6 (returns row 4 by ordering it by A desc and get the first one)\n\n\nI was able to do it by iterating on the dataframe (I know that craps :P). So, I prefer a more pythonic way to solve it.\n"
"I have a large dataframe with ID numbers:\n\nID.head()\nOut[64]: \n0    4806105017087\n1    4806105017087\n2    4806105017087\n3    4901295030089\n4    4901295030089\n\n\nThese are all strings at the moment.\n\nI want to convert to int without using loops - for this I use ID.astype(int).\n\nThe problem is that some of my lines contain dirty data which cannot be converted to int, for e.g.\n\nID[154382]\nOut[58]: 'CN414149'\n\n\nHow can I (without using loops) remove these type of occurrences so that I can use astype with peace of mind?\n"
'I have a DataFrame with a few columns. One columns contains a symbol for which currency is being used, for instance a euro or a dollar sign. Another column contains a budget value. So for instance in one row it could mean a budget of 5000 in euro and in the next row it could say a budget of 2000 in dollar.\n\nIn pandas I would like to add an extra column to my DataFrame, normalizing the budgets in euro. So basically, for each row the value in the new column should be the value from the budget column * 1 if the symbol in the currency column is a euro sign, and the value in the new column should be the value of the budget column * 0.78125 if the symbol in the currency column is a dollar sign.\n\nI know how to add a column, fill it with values, copy values from another column etc. but not how to fill the new column conditionally based on the value of another column. \n\nAny suggestions?\n'
'I am creating a matrix from a Pandas dataframe as follows:\n\ndense_matrix = np.array(df.as_matrix(columns = None), dtype=bool).astype(np.int)\n\n\nAnd then into a sparse matrix with:\n\nsparse_matrix = scipy.sparse.csr_matrix(dense_matrix)\n\n\nIs there any way to go from a df straight to a sparse matrix?\n\nThanks in advance.\n'
'I am using Flask but this probably applies to a lot of similar frameworks.\n\nI construct a pandas Dataframe, e.g.\n\n@app.route(\'/analysis/&lt;filename&gt;\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template("analysis.html", name=filename, data=x)\n\n\nThe template analysis.html looks like\n\n{% extends "base.html" %}\n{% block content %}\n&lt;h1&gt;{{name}}&lt;/h1&gt;\n{{data}}\n{% endblock %}\n\n\nThis works but the output looks horrible. It doesn\'t use linebreaks etc.\nI have played with data.to_html() and data.to_string()\nWhat\'s the easiest way to display a frame?\n'
'So far I have been able to label the subplots just fine but I\'m having an issue with the main one.\n\nHere\'s the relevant part of my code:\n\ndata_BS_P = data[channels[0]]\ndata_BS_R = data[channels[1]]\ndata_BS_Y = data[channels[2]]\nplot_BS_P = data_BS_P.plot() #data_BS_P is a pandas dataframe\naxBS = plot_BS_P.gca()\naxBS.plot(data_BS_R, label=\'Roll\')\naxBS.plot(data_BS_Y, label=\'Yaw\')\naxBS.set_ylabel(\'Amplitude (urad)\')\naxBS.legend(loc=\'upper center\', bbox_to_anchor=(0.5, 1.05), ncol=3,\n            fancybox=True, shadow=True)\nml1 = MultipleLocator(10)\nml2 = MultipleLocator(3600)\naxBS.yaxis.set_minor_locator(ml1)\naxBS.xaxis.set_minor_locator(ml2)\nplot_BS_P.save(\'L1-SUS-BS_M1_DAMP_PRY_INMON.jpg\')\n\n\nAnd this is what I have so far: \nNotice the lengthy label for the blue line. I\'d like that to be labeled as "Pitch" instead of the file name. In which line can I do that?\n'
"I feel like there is a better way than this:\n\nimport pandas as pd\ndf = pd.DataFrame(\n    [['A', 'X', 3], ['A', 'X', 5], ['A', 'Y', 7], ['A', 'Y', 1],\n     ['B', 'X', 3], ['B', 'X', 1], ['B', 'X', 3], ['B', 'Y', 1],\n     ['C', 'X', 7], ['C', 'Y', 4], ['C', 'Y', 1], ['C', 'Y', 6]],\n    columns=['c1', 'c2', 'v1'])\ndef callback(x):\n    x['seq'] = range(1, x.shape[0] + 1)\n    return x\ndf = df.groupby(['c1', 'c2']).apply(callback)\nprint df\n\n\nTo achieve this:\n\n   c1 c2  v1  seq\n0   A  X   3    1\n1   A  X   5    2\n2   A  Y   7    1\n3   A  Y   1    2\n4   B  X   3    1\n5   B  X   1    2\n6   B  X   3    3\n7   B  Y   1    1\n8   C  X   7    1\n9   C  Y   4    1\n10  C  Y   1    2\n11  C  Y   6    3\n\n\nIs there a way to do it that avoids the callback?\n"
"I have a dataframe that looks like:\n\ndata = {'index': ['2014-06-22 10:46:00', '2014-06-24 19:52:00', '2014-06-25 17:02:00', '2014-06-25 17:55:00', '2014-07-02 11:36:00', '2014-07-06 12:40:00', '2014-07-05 12:46:00', '2014-07-27 15:12:00'],\n    'type': ['A', 'B', 'C', 'A', 'B', 'C', 'A', 'C'],\n    'sum_col': [1, 2, 3, 1, 1, 3, 2, 1]}\ndf = pd.DataFrame(data, columns=['index', 'type', 'sum_col'])\ndf['index'] = pd.to_datetime(df['index'])\ndf = df.set_index('index')\ndf['weekofyear'] = df.index.weekofyear\ndf['date'] = df.index.date\ndf['date'] = pd.to_datetime(df['date'])\n\n\n\n                     type sum_col weekofyear   date\nindex               \n2014-06-22 10:46:00    A    1       25      2014-06-22\n2014-06-24 19:52:00    B    2       26      2014-06-24\n2014-06-25 17:02:00    C    3       26      2014-06-25\n2014-06-25 17:55:00    A    1       26      2014-06-25\n2014-07-02 11:36:00    B    1       27      2014-07-02\n2014-07-06 12:40:00    C    3       27      2014-07-06\n2014-07-05 12:46:00    A    2       27      2014-07-05\n2014-07-27 15:12:00    C    1       30      2014-07-27\n\n\nI'm looking to groupby the weekofyear, then sum up the sum_col. In addition, I need to find the earliest, and the latest date for the week. The first part is pretty easy:\n\ngb = df.groupby(['type', 'weekofyear'])\ngb['sum_col'].agg({'sum_col' : np.sum})\n\n\nI've tried to find the min/max date with this, but haven't been successful:\n\ngb = df.groupby(['type', 'weekofyear'])\ngb.agg({'sum_col' : np.sum,\n        'date' : np.min,\n        'date' : np.max})\n\n\nHow would one find the earliest/latest date that appears?\n"
"Here is my problem, I have a dataframe like this :\n\n    Depr_1  Depr_2  Depr_3\nS3  0   5   9\nS2  4   11  8\nS1  6   11  12\nS5  0   4   11\nS4  4   8   8\n\n\nand I just want to calculate the mean over the full dataframe, as the following doesn't work :\n\ndf.mean()\n\n\nThen I came up with :\n\ndf.mean().mean()\n\n\nBut this trick won't work for computing the standard deviation.  My final attempts were :\n\ndf.get_values().mean()\ndf.get_values().std()\n\n\nExcept that in the latter case, it uses mean() and std() function from numpy. It's not a problem for the mean, but it is for std, as the pandas function uses by default ddof=1, unlike the numpy one where ddof=0.\n"
"I have the following 2 dataframes \n\nExample1\nsku loc flag  \n122  61 True \n123  61 True\n113  62 True \n122  62 True \n123  62 False\n122  63 False\n301  63 True \n\nExample2 \nsku dept \n113 a\n122 b\n123 b\n301 c \n\n\nI want to perform a merge, or join opertation using Pandas (or whichever Python operator is best) to produce the below data frame. \n\nExample3\nsku loc flag   dept  \n122  61 True   b\n123  61 True   b\n113  62 True   a\n122  62 True   b\n123  62 False  b\n122  63 False  b\n301  63 True   c\n\nBoth \ndf_Example1.join(df_Example2,lsuffix='_ProdHier')\ndf_Example1.join(df_Example2,how='outer',lsuffix='_ProdHier')\n\n\nAren't working.\nWhat am I doing wrong? \n"
"I want to bring some data into a pandas DataFrame and I want to assign dtypes for each column on import.  I want to be able to do this for larger datasets with many different columns, but, as an example:\n\nmyarray = np.random.randint(0,5,size=(2,2))\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype=[float,int])\nmydf.dtypes\n\n\nresults in:\n\n\n  TypeError: data type not understood\n\n\nI tried a few other methods such as:\n\nmydf = pd.DataFrame(myarray,columns=['a','b'], dtype={'a': int})\n\n\n\n  TypeError: object of type 'type' has no len()\n\n\nIf I put dtype=(float,int) it applies a float format to both columns.\n\nIn the end I would like to just be able to pass it a list of datatypes the same way I can pass it a list of column names.\n"
"Thought this would be straight forward but had some trouble tracking down an elegant way to search all columns in a dataframe at same time for a partial string match. Basically how would I apply df['col1'].str.contains('^') to an entire dataframe at once and filter down to any rows that have records containing the match? \n"
'I have a list of dicts in the following form that I generate from pandas. I want to convert it to a json format.\n\nlist_val = [{1.0: 685}, {2.0: 8}]\noutput = json.dumps(list_val)\n\n\nHowever, json.dumps throws an error: TypeError: 685 is not JSON serializable\n\nI am guessing it\'s a type conversion issue from numpy to python(?).\n\nHowever, when I convert the values v of each dict in the array using np.int32(v) it still throws the error.\n\nEDIT: Here\'s the full code\n\n            new = df[df[label] == label_new] \n            ks_dict = json.loads(content)\n            ks_list = ks_dict[\'variables\']\n            freq_counts = []\n\n            for ks_var in ks_list:\n\n                    freq_var = dict()\n                    freq_var["name"] = ks_var["name"]\n                    ks_series = new[ks_var["name"]]\n                    temp_df = ks_series.value_counts().to_dict()\n                    freq_var["new"] = [{u: np.int32(v)} for (u, v) in temp_df.iteritems()]            \n                    freq_counts.append(freq_var)\n\n           out = json.dumps(freq_counts)\n\n'
"I have a Pandas DataFrame as below \n\n        ReviewID       ID      Type               TimeReviewed\n205     76032930  51936827  ReportID 2015-01-15 00:05:27.513000\n232     76032930  51936854  ReportID 2015-01-15 00:06:46.703000\n233     76032930  51936855  ReportID 2015-01-15 00:06:56.707000\n413     76032930  51937035  ReportID 2015-01-15 00:14:24.957000\n565     76032930  51937188  ReportID 2015-01-15 00:23:07.220000\n\n&gt;&gt;&gt; type(df)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\nTimeReviewed is a series type \n\n&gt;&gt;&gt; type(df.TimeReviewed)\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nI've tried below, but it still doesn't change the Series type \n\nimport pandas as pd\nreview = pd.to_datetime(pd.Series(df.TimeReviewed))\n&gt;&gt;&gt; type(review)\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nHow can I change the df.TimeReviewed to DateTime type and pull out year, month, day, hour, min, sec separately? \nI'm kinda new to python, thanks for your help. \n"
'I am trying to create a new DataFrame using only one index from a multi-indexed DataFrame.  \n\n                   A         B         C\nfirst second                              \nbar   one     0.895717  0.410835 -1.413681\n      two     0.805244  0.813850  1.607920\nbaz   one    -1.206412  0.132003  1.024180\n      two     2.565646 -0.827317  0.569605\nfoo   one     1.431256 -0.076467  0.875906\n      two     1.340309 -1.187678 -2.211372\nqux   one    -1.170299  1.130127  0.974466\n      two    -0.226169 -1.436737 -2.006747\n\n\nIdeally, I would like something like this:\n\nIn: df.ix[level="first"]\n\n\nand:\n\nOut:\n\n               A         B         C\nfirst                               \nbar        0.895717  0.410835 -1.413681\n           0.805244  0.813850  1.607920\nbaz       -1.206412  0.132003  1.024180\n           2.565646 -0.827317  0.569605\nfoo        1.431256 -0.076467  0.875906\n           1.340309 -1.187678 -2.211372\nqux       -1.170299  1.130127  0.974466\n          -0.226169 -1.436737 -2.006747\n`\n\n\nEssentially I want to drop all the other indexes of the multi-index other than level first.  Is there an easy way to do this?\n'
"I have a pandas.DataFrame called df which has an automatically generated index, with a column dt:\n\ndf['dt'].dtype, df['dt'][0]\n# (dtype('&lt;M8[ns]'), Timestamp('2014-10-01 10:02:45'))\n\n\nWhat I'd like to do is create a new column truncated to hour precision. I'm currently using:\n\ndf['dt2'] = df['dt'].apply(lambda L: datetime(L.year, L.month, L.day, L.hour))\n\n\nThis works, so that's fine. However, I've an inkling there's some nice way using pandas.tseries.offsets or creating a DatetimeIndex or similar. \n\nSo if possible, is there some pandas wizardry to do this? \n"
"                       NI\nYEAR MONTH datetime        \n2000 1     2000-01-01   NaN\n           2000-01-02   NaN\n           2000-01-03   NaN\n           2000-01-04   NaN\n           2000-01-05   NaN\n\n\nIn the dataframe above, I have a multilevel index consisting of the columns:\n\nnames=[u'YEAR', u'MONTH', u'datetime']\n\n\nHow do I revert to a dataframe with 'datetime' as index and 'YEAR' and 'MONTH' as normal columns?\n"
'I have a Pandas DataFrame with two columns – one with the filename and one with the hour in which it was generated: \n\n File       Hour\n  F1         1\n  F1         2\n  F2         1\n  F3         1\n\n\nI am trying to convert it to a JSON file with the following format:\n\n{"File":"F1","Hour":"1"} \n{"File":"F1","Hour":"2"}\n{"File":"F2","Hour":"1"}\n{"File":"F3","Hour":"1"}\n\n\nWhen I use the command DataFrame.to_json(orient = "records"), I get the records in the below format:\n\n[{"File":"F1","Hour":"1"},\n {"File":"F1","Hour":"2"},\n {"File":"F2","Hour":"1"},\n {"File":"F3","Hour":"1"}]\n\n\nI\'m just wondering whether there is an option to get the JSON file in the desired format. Any help would be appreciated.\n'
'I\'m looking to see how to do two things in Seaborn with using a bar chart to display values that are in the dataframe, but not in the graph\n\n1) I\'m looking to display the values of one field in a dataframe while graphing another.  For example, below, I\'m graphing \'tip\', but I would like to place the value of \'total_bill\' centered above each of the bars (i.e.325.88 above Friday, \n1778.40 above Saturday, etc.)\n\n2) Is there a way to scale the colors of the bars, with the lowest value of \'total_bill\' having the lightest color (in this case Friday) and the highest value of \'total_bill\' having the darkest.  Obviously, I\'d stick with one color (i.e. blue) when I do the scaling.\n\nThanks! I\'m sure this is easy, but i\'m missing it..\n\nWhile I see that others think that this is a duplicate of another problem (or two), I am missing the part of how I use a value that is not in the graph as the basis for the label or the shading.  How do I say, use total_bill as the basis.  I\'m sorry, but I just can\'t figure it out based on those answers.\n\nStarting with the following code,\n\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\ndf=pd.read_csv("https://raw.githubusercontent.com/wesm/pydata-    book/master/ch08/tips.csv", sep=\',\')\ngroupedvalues=df.groupby(\'day\').sum().reset_index()\ng=sns.barplot(x=\'day\',y=\'tip\',data=groupedvalues)\n\n\nI get the following result:\n\n\n\nInterim Solution:\n\nfor index, row in groupedvalues.iterrows():\n    g.text(row.name,row.tip, round(row.total_bill,2), color=\'black\', ha="center")\n\n\n\n\nOn the shading, using the example below, I tried the following:\n\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\ndf=pd.read_csv("https://raw.githubusercontent.com/wesm/pydata-book/master/ch08/tips.csv", sep=\',\')\ngroupedvalues=df.groupby(\'day\').sum().reset_index()\n\npal = sns.color_palette("Greens_d", len(data))\nrank = groupedvalues.argsort().argsort() \ng=sns.barplot(x=\'day\',y=\'tip\',data=groupedvalues)\n\nfor index, row in groupedvalues.iterrows():\n    g.text(row.name,row.tip, round(row.total_bill,2), color=\'black\', ha="center")\n\n\nBut that gave me the following error:\n\nAttributeError: \'DataFrame\' object has no attribute \'argsort\'\n\nSo I tried a modification:\n\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\ndf=pd.read_csv("https://raw.githubusercontent.com/wesm/pydata-book/master/ch08/tips.csv", sep=\',\')\ngroupedvalues=df.groupby(\'day\').sum().reset_index()\n\npal = sns.color_palette("Greens_d", len(data))\nrank=groupedvalues[\'total_bill\'].rank(ascending=True)\ng=sns.barplot(x=\'day\',y=\'tip\',data=groupedvalues,palette=np.array(pal[::-1])[rank])\n\n\nand that leaves me with \n\nIndexError: index 4 is out of bounds for axis 0 with size 4\n'
'I want to train a simple neural network on PyTorch using a personal database. This database is imported from an Excel file and stored in df.\n\nOne of the columns is named "Target", and it is the target variable of the network. How can i use this data frame as an input for the PyTorch neural network?\n\nI tried this, but it doesn\'t work:\n\ntarget = pd.DataFrame(data = df[\'Target\'])\ntrain = data_utils.TensorDataset(df, target)\ntrain_loader = data_utils.DataLoader(train, batch_size = 10, shuffle = True)\n\n'
'I am trying to build a ARIMA for anomaly detection. I need to find the moving average of the time series graph I am trying to use pandas 0.23 for this  \n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams[\'figure.figsize\'] = 15, 6\n\ndateparse = lambda dates: pd.datetime.strptime(dates, \'%Y-%m\')\ndata = pd.read_csv(\'AirPassengers.csv\', parse_dates=[\'Month\'], index_col=\'Month\',date_parser=dateparse)\n\ndata.index\nts = data[\'#Passengers\']\nts.head(10)\n\nplt.plot(ts)\nts_log = np.log(ts)\nplt.plot(ts_log)\nmoving_avg = pd.rolling_mean(ts_log,12)  # here is the error\n\npd.rolling_mean  \nplt.plot(ts_log)\nplt.plot(moving_avg, color=\'red\') \n\n\n\n  error:Traceback (most recent call last):   File "C:\\Program\n  Files\\Python36\\lastmainprogram.py", line 74, in \n      moving_avg = pd.rolling_mean(ts_log,12) AttributeError: module \'pandas\' has no attribute \'rolling_mean\'\n\n'
'Is there a way to convert a Spark Df (not RDD) to pandas DF\n\nI tried the following:\n\nvar some_df = Seq(\n ("A", "no"),\n ("B", "yes"),\n ("B", "yes"),\n ("B", "no")\n\n ).toDF(\n"user_id", "phone_number")\n\n\nCode:\n\n%pyspark\npandas_df = some_df.toPandas()\n\n\nError:\n\n NameError: name \'some_df\' is not defined\n\n\nAny suggestions.\n'
'Pandas has both isna() and isnull(). I usually use isnull() to detect missing values and have never met the case so that I had to use other than that.\nSo, when to use isna()?\n'
"I have a times series with temperature and radiation in a pandas dataframe. The time resolution is 1 minute in regular steps.\n\nimport datetime\nimport pandas as pd\nimport numpy as np\n\ndate_times = pd.date_range(datetime.datetime(2012, 4, 5, 8, 0),\n                           datetime.datetime(2012, 4, 5, 12, 0),\n                           freq='1min')\ntamb = np.random.sample(date_times.size) * 10.0\nradiation = np.random.sample(date_times.size) * 10.0\nframe = pd.DataFrame(data={'tamb': tamb, 'radiation': radiation},\n                     index=date_times)\nframe\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 241 entries, 2012-04-05 08:00:00 to 2012-04-05 12:00:00\nFreq: T\nData columns:\nradiation    241  non-null values\ntamb         241  non-null values\ndtypes: float64(2)\n\n\nHow can I down-sample this dataframe to a resolution of one hour, computing the hourly mean for the temperature and the hourly sum for radiation? \n"
'Very weird bug here: I\'m using pandas to merge several dataframes.  As part of the merge, I have to call reset_index several times.  But when I do, it crashes unexpectedly on the second or third use of reset_index.\n\nHere\'s minimal code to reproduce the error:\n\nimport pandas\nA = pandas.DataFrame({\n    \'val\' :  [\'aaaaa\', \'acaca\', \'ddddd\', \'zzzzz\'],\n    \'extra\' : range(10,14),\n})\nA = A.reset_index()\nA = A.reset_index()\nA = A.reset_index()\n\n\nHere\'s the relevant part of the traceback:\n\n....\n    A = A.reset_index()\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py", line 2393, in reset_index\n    new_obj.insert(0, name, _maybe_cast(self.index.values))\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py", line 1787, in insert\n    self._data.insert(loc, column, value)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/internals.py", line 893, in insert\n    raise Exception(\'cannot insert %s, already exists\' % item)\nException: cannot insert level_0, already exists\n\n\nAny idea what\'s going wrong here?  How do I work around it?\n'
"I am making a stacked bar plot using:\n\nDataFrame.plot(kind='bar',stacked=True)\n\n\nI want to control width of bars so that the bars are connected to each other like a histogram.\n\nI've looked through the documentation but to no avail - any suggestions? Is it possible to do it this way?\n"
"I'm using Pandas 0.10.1\n\nConsidering this Dataframe:\n\nDate       State   City    SalesToday  SalesMTD  SalesYTD\n20130320     stA    ctA            20       400      1000\n20130320     stA    ctB            30       500      1100\n20130320     stB    ctC            10       500       900\n20130320     stB    ctD            40       200      1300\n20130320     stC    ctF            30       300       800\n\n\nHow can i group subtotals per state?\n\nState   City  SalesToday  SalesMTD  SalesYTD\n  stA    ALL          50       900      2100\n  stA    ctA          20       400      1000\n  stA    ctB          30       500      1100\n\n\nI tried with a pivot table but i only can have subtotals in columns\n\ntable = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State','City'], aggfunc=np.sum, margins=True)\n\n\nI can achieve this on excel, with a pivot table.\n"
"I wonder how to add new DataFrame data onto the end of an existing csv file? The to_csv doesn't mention such functionality.\n"
"I've a Pandas data frame, where one column contains text. I'd like to get a list of unique words appearing across the entire column (space being the only split).\n\nimport pandas as pd\n\nr1=['My nickname is ft.jgt','Someone is going to my place']\n\ndf=pd.DataFrame(r1,columns=['text'])\n\n\nThe output should look like this:\n\n['my','nickname','is','ft.jgt','someone','going','to','place']\n\n\nIt wouldn't hurt to get a count as well, but it is not required.\n"
'How can I square each element of a column/series of a DataFrame in pandas (and create another column to hold the result)?\n'
"I have a Pandas data frame 'df' in which I'd like to perform some scalings column by column.\n\n\nIn column 'a', I need the maximum number to be 1, the minimum number to be 0, and all other to be spread accordingly.\nIn column 'b', however, I need the minimum number to be 1, the maximum number to be 0, and all other to be spread accordingly.\n\n\nIs there a Pandas function to perform these two operations? If not, numpy would certainly do.\n\n    a    b\nA   14   103\nB   90   107\nC   90   110\nD   96   114\nE   91   114\n\n"
'The quantile functions gives us the quantile of a given pandas series s,\n\nE.g.\n\n\n  s.quantile(0.9) is 4.2\n\n\nIs there the inverse function (i.e. cumulative distribution) which finds the value x such that \n\n\n  s.quantile(x)=4\n\n\nThanks\n'
"I need to filter a data frame with a dict, constructed with the key being the column name and the value being the value that I want to filter:\n\nfilter_v = {'A':1, 'B':0, 'C':'This is right'}\n# this would be the normal approach\ndf[(df['A'] == 1) &amp; (df['B'] ==0)&amp; (df['C'] == 'This is right')]\n\n\nBut I want to do something on the lines\n\nfor column, value in filter_v.items():\n    df[df[column] == value]\n\n\nbut this will filter the data frame several times, one value at a time, and not apply all filters at the same time. Is there a way to do it programmatically?\n\nEDIT: an example:\n\ndf1 = pd.DataFrame({'A':[1,0,1,1, np.nan], 'B':[1,1,1,0,1], 'C':['right','right','wrong','right', 'right'],'D':[1,2,2,3,4]})\nfilter_v = {'A':1, 'B':0, 'C':'right'}\ndf1.loc[df1[filter_v.keys()].isin(filter_v.values()).all(axis=1), :]\n\n\ngives\n\n    A   B   C   D\n0   1   1   right   1\n1   0   1   right   2\n3   1   0   right   3\n\n\nbut the expected result was\n\n    A   B   C   D\n3   1   0   right   3\n\n\nonly the last one should be selected.\n"
"Can you make a python pandas function with values in two different columns as arguments?\n\nI have a function that returns a 1 if two columns have values in the same range. otherwise it returns 0:\n\ndef segmentMatch(RealTime, ResponseTime):\n    if RealTime &lt;= 566 and ResponseTime &lt;= 566:\n        matchVar = 1\n    elif 566 &lt; RealTime &lt;= 1132 and 566 &lt; ResponseTime &lt;= 1132:\n        matchVar = 1\n    elif 1132 &lt; RealTime &lt;= 1698 and 1132 &lt; ResponseTime &lt;= 1698:\n        matchVar = 1\n    else:\n        matchVar = 0\n    return matchVar\n\n\nI want the first argument, RealTime, to be a column in my data frame, such that the function will take the value of each row in that column. e.g. RealTime is df['TimeCol'] and the second argument is df['ResponseCol']`. And I'd like the result to be a new column in the dataframe. I came across several threads that have answered a similar question, but it looks like those arguments were variables, not values in rows of the dataframe.\n\nI tried the following but it didn't work:\n\ndf['NewCol'] = df.apply(segmentMatch, args=(df['TimeCol'], df['ResponseCol']), axis=1)\n\n"
"Given a 1.5 Gb list of pandas dataframes, which format is fastest for loading compressed data:\npickle (via cPickle), hdf5, or something else in Python?\n\n\nI only care about fastest speed to load the data into memory\nI don't care about dumping the data, it's slow but I only do this once.\nI don't care about file size on disk\n\n"
"Tring to remove the commas and dollars signs from the columns. But when I do, the table prints them out and still has them in there. Is there a different way to remove the commans and dollars signs using a pandas function. I was unuable to find anything in the API Docs or maybe i was looking in the wrong place\n\n import pandas as pd\n    import pandas_datareader.data as web\n\nplayers = pd.read_html('http://www.usatoday.com/sports/mlb/salaries/2013/player/p/')\n\n\ndf1 = pd.DataFrame(players[0])\n\n\ndf1.drop(df1.columns[[0,3,4, 5, 6]], axis=1, inplace=True)\ndf1.columns = ['Player', 'Team', 'Avg_Annual']\ndf1['Avg_Annual'] = df1['Avg_Annual'].replace(',', '')\n\nprint (df1.head(10))\n\n"
"I'm having issues transitioning to pandas from R where dplyr package can easily group-by and perform multiple summarizations. \n\nPlease help improve my existing Python pandas code for multiple aggregations:\n\nimport pandas as pd\ndata = pd.DataFrame(\n    {'col1':[1,1,1,1,1,2,2,2,2,2],\n    'col2':[1,2,3,4,5,6,7,8,9,0],\n     'col3':[-1,-2,-3,-4,-5,-6,-7,-8,-9,0]\n    }\n)\nresult = []\nfor k,v in data.groupby('col1'):\n    result.append([k, max(v['col2']), min(v['col3'])])\nprint pd.DataFrame(result, columns=['col1', 'col2_agg', 'col3_agg'])\n\n\nIssues:\n\n\ntoo verbose\nprobably can be optimized and efficient. (I rewrote a for-loop groupby implementation into groupby.agg and the performance enhancement was huge). \n\n\nIn R the equivalent code would be:\n\ndata %&gt;% groupby(col1) %&gt;% summarize(col2_agg=max(col2), col3_agg=min(col3))\n\n\n\n\nUPDATE: @ayhan solved my question, here is a follow-up question that I will post here instead of as comment: \n\nQ2) What is the equivalent of groupby().summarize(newcolumn=max(col2 * col3)) i.e. an aggregation/summarization where the function is a compound function of 2+ columns? \n"
"How do I get the exponential weighted moving average in NumPy just like the following in pandas?\n\nimport pandas as pd\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n# Declare variables\nibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1)).reset_index(drop=True)['Adj Close']\nwindowSize = 20\n\n# Get PANDAS exponential weighted moving average\newm_pd = pd.DataFrame(ibm).ewm(span=windowSize, min_periods=windowSize).mean().as_matrix()\n\nprint(ewm_pd)\n\n\nI tried the following with NumPy\n\nimport numpy as np\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\n# From this post: http://stackoverflow.com/a/40085052/3293881 by @Divakar\ndef strided_app(a, L, S): # Window len = L, Stride len/stepsize = S\n    nrows = ((a.size - L) // S) + 1\n    n = a.strides[0]\n    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n))\n\ndef numpyEWMA(price, windowSize):\n    weights = np.exp(np.linspace(-1., 0., windowSize))\n    weights /= weights.sum()\n\n    a2D = strided_app(price, windowSize, 1)\n\n    returnArray = np.empty((price.shape[0]))\n    returnArray.fill(np.nan)\n    for index in (range(a2D.shape[0])):\n        returnArray[index + windowSize-1] = np.convolve(weights, a2D[index])[windowSize - 1:-windowSize + 1]\n    return np.reshape(returnArray, (-1, 1))\n\n# Declare variables\nibm = pdr.get_data_yahoo(symbols='IBM', start=datetime(2000, 1, 1), end=datetime(2012, 1, 1)).reset_index(drop=True)['Adj Close']\nwindowSize = 20\n\n# Get NumPy exponential weighted moving average\newma_np = numpyEWMA(ibm, windowSize)\n\nprint(ewma_np)\n\n\nBut the results are not similar as the ones in pandas.\n\nIs there maybe a better approach to calculate the exponential weighted moving average directly in NumPy and get the exact same result as the pandas.ewm().mean()?\n\nAt 60,000 requests on pandas solution, I get about 230 seconds. I am sure that with a pure NumPy, this can be decreased significantly.\n"
"I am trying to iterate over the rows of a Python Pandas dataframe. Within each row of the dataframe, I am trying to to refer to each value along a row by its column name. \n\nHere is what I have:\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\nprint df\n          A         B         C         D\n0  0.351741  0.186022  0.238705  0.081457\n1  0.950817  0.665594  0.671151  0.730102\n2  0.727996  0.442725  0.658816  0.003515\n3  0.155604  0.567044  0.943466  0.666576\n4  0.056922  0.751562  0.135624  0.597252\n5  0.577770  0.995546  0.984923  0.123392\n6  0.121061  0.490894  0.134702  0.358296\n7  0.895856  0.617628  0.722529  0.794110\n8  0.611006  0.328815  0.395859  0.507364\n9  0.616169  0.527488  0.186614  0.278792\n\n\nI used this approach to iterate, but it is only giving me part of the solution - after selecting a row in each iteration, how do I access row elements by their column name?\n\nHere is what I am trying to do:\n\nfor row in df.iterrows():\n    print row.loc[0,'A']\n    print row.A\n    print row.index()\n\n\nMy understanding is that the row is a Pandas series. But I have no way to index into the Series.\n\nIs it possible to use column names while simultaneously iterating over rows?\n"
"I have a large dataframe containing lots of columns.\n\nFor each row/index in the dataframe I do some operations, read in some ancilliary ata, etc and get a new value. Is there a way to add that new value into a new column at the correct row/index?\n\nI can use .assign to add a new column but as I'm looping over the rows and only generating the data to add for one value at a time (generating it is quite involved). When it's generated I'd like to immediately add it to the dataframe rather than waiting until I've generated the entire series.\n\nThis doesn't work and gives a key error:\n\ndf['new_column_name'].iloc[this_row]=value\n\n\nDo I need to initialise the column first or something?\n"
'This is a self-answered post. A common problem is to randomly generate dates between a given start and end date. \n\nThere are two cases to consider:\n\n\nrandom dates with a time component, and \nrandom dates without time\n\n\nFor example, given some start date 2015-01-01 and an end date 2018-01-01, how can I sample N random dates between this range using pandas?\n'
"I'm trying to create a series of dummy variables from a categorical variable using pandas in python. I've come across the get_dummies function, but whenever I try to call it I receive an error that the name is not defined. \n\nAny thoughts or other ways to create the dummy variables would be appreciated.\n\nEDIT: Since others seem to be coming across this, the get_dummies function in pandas now works perfectly fine. This means the following should work:\n\nimport pandas as pd\n\ndummies = pd.get_dummies(df['Category'])\n\n\nSee http://blog.yhathq.com/posts/logistic-regression-and-python.html for further information.\n"
'I have a pandas DataFrame, st containing multiple columns:\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_1020                            53716  non-null values\nAOT_870                             53732  non-null values\nAOT_675                             53188  non-null values\nAOT_500                             51687  non-null values\nAOT_440                             53727  non-null values\nAOT_380                             51864  non-null values\nAOT_340                             52852  non-null values\nWater(cm)                           51687  non-null values\n%TripletVar_1020                    53710  non-null values\n%TripletVar_870                     53726  non-null values\n%TripletVar_675                     53182  non-null values\n%TripletVar_500                     51683  non-null values\n%TripletVar_440                     53721  non-null values\n%TripletVar_380                     51860  non-null values\n%TripletVar_340                     52846  non-null values\n440-870Angstrom                     53732  non-null values\n380-500Angstrom                     52253  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\n340-440Angstrom                     53277  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ndtypes: datetime64[ns](1), float64(22), object(1)\n\n\nI want to create two new columns for this dataframe based on applying a function to each row of the dataframe. I don\'t want to have to call the function multiple times (eg. by doing two separate apply calls) as it is rather computationally intensive. I have tried doing this in two ways, and neither of them work:\n\n\n\nUsing apply:\n\nI have written a function which takes a Series and returns a tuple of the values I want:\n\ndef calculate(s):\n    a = s[\'path\'] + 2*s[\'row\'] # Simple calc for example\n    b = s[\'path\'] * 0.153\n    return (a, b)\n\n\nTrying to apply this to the DataFrame gives an error:\n\nst.apply(calculate, axis=1)\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\n&lt;ipython-input-248-acb7a44054a7&gt; in &lt;module&gt;()\n----&gt; 1 st.apply(calculate, axis=1)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in apply(self, func, axis, broadcast, raw, args, **kwds)\n   4191                     return self._apply_raw(f, axis)\n   4192                 else:\n-&gt; 4193                     return self._apply_standard(f, axis)\n   4194             else:\n   4195                 return self._apply_broadcast(f, axis)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _apply_standard(self, func, axis, ignore_failures)\n   4274                 index = None\n   4275 \n-&gt; 4276             result = self._constructor(data=results, index=index)\n   4277             result.rename(columns=dict(zip(range(len(res_index)), res_index)),\n   4278                           inplace=True)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in __init__(self, data, index, columns, dtype, copy)\n    390             mgr = self._init_mgr(data, index, columns, dtype=dtype, copy=copy)\n    391         elif isinstance(data, dict):\n--&gt; 392             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    393         elif isinstance(data, ma.MaskedArray):\n    394             mask = ma.getmaskarray(data)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _init_dict(self, data, index, columns, dtype)\n    521 \n    522         return _arrays_to_mgr(arrays, data_names, index, columns,\n--&gt; 523                               dtype=dtype)\n    524 \n    525     def _init_ndarray(self, values, index, columns, dtype=None,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\frame.pyc in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5411 \n   5412     # consolidate for now\n-&gt; 5413     mgr = BlockManager(blocks, axes)\n   5414     return mgr.consolidate()\n   5415 \n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in __init__(self, blocks, axes, do_integrity_check)\n    802 \n    803         if do_integrity_check:\n--&gt; 804             self._verify_integrity()\n    805 \n    806         self._consolidate_check()\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\internals.pyc in _verify_integrity(self)\n    892                                      "items")\n    893             if block.values.shape[1:] != mgr_shape[1:]:\n--&gt; 894                 raise AssertionError(\'Block shape incompatible with manager\')\n    895         tot_items = sum(len(x.items) for x in self.blocks)\n    896         if len(self.items) != tot_items:\n\nAssertionError: Block shape incompatible with manager\n\n\nI was then going to assign the values returned from apply to two new columns using the method shown in this question. However, I can\'t even get to this point! This all works fine if I just return one value.\n\n\n\nUsing a loop:\n\nI first created two new columns of the dataframe and set them to None:\n\nst[\'a\'] = None\nst[\'b\'] = None\n\n\nThen looped over all of the indices and tried to modify these None values that I\'d got in there, but the modifications I did didn\'t seem to work. That is, no error was generated, but the DataFrame didn\'t seem to be modified.\n\nfor i in st.index:\n    # do calc here\n    st.ix[i][\'a\'] = a\n    st.ix[i][\'b\'] = b\n\n\n\n\nI thought that both of these methods would work, but neither of them did. So, what am I doing wrong here? And what is the best, most \'pythonic\' and \'pandaonic\' way to do this?\n'
"I'm still kinda new with Python, using Pandas, and I've got some issues debugging my Python script.\n\nI've got the following warning message :\n\n[...]\\pandas\\core\\index.py:756: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\nreturn self._engine.get_loc(key)\n\n\nAnd can't find where it's from.\n\nAfter some research, I tried to do that in the Pandas lib file (index.py):\n\ntry:\n    return self._engine.get_loc(key)\nexcept UnicodeWarning:\n    warnings.warn('Oh Non', stacklevel=2)\n\n\nBut that didn't change anything about the warning message.\n"
'I have some csv text data in a package which I want to read using read_csv. I was doing this by \n\nfrom pkgutil import get_data\nfrom StringIO import StringIO\n\ndata = read_csv(StringIO(get_data(\'package.subpackage\', \'path/to/data.csv\')))\n\n\nHowever, StringIO.StringIO disappears in Python 3, and io.StringIO only accepts Unicode. Is there a simple way to do this?\n\nEdit: the following does not appear to work \n\nimport pandas as pd\n\nimport pkgutil\nfrom io import StringIO\n\ndef get_data_file(pkg, path):\n    f = StringIO()\n    contents = unicode(pkgutil.get_data(\'pymc.examples\', \'data/wells.dat\'))\n    f.write(contents)\n    return f\n\nwells = get_data_file(\'pymc.examples\', \'data/wells.dat\')\n\ndata = pd.read_csv(wells, delimiter=\' \', index_col=\'id\',\n                   dtype={\'switch\': np.int8})\n\n\nfailing with \n\n  File "/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py", line 401, in parser_f\n    return _read(filepath_or_buffer, kwds)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py", line 209, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py", line 509, in __init__\n    self._make_engine(self.engine)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py", line 611, in _make_engine\n    self._engine = CParserWrapper(self.f, **self.options)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py", line 893, in __init__\n    self._reader = _parser.TextReader(src, **kwds)\n  File "parser.pyx", line 441, in pandas._parser.TextReader.__cinit__ (pandas/src/parser.c:3940)\n  File "parser.pyx", line 551, in pandas._parser.TextReader._get_header (pandas/src/parser.c:5096)\npandas._parser.CParserError: Passed header=0 but only 0 lines in file\n\n'
"Here is what I am doing:\n\n$ python\nPython 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\n&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; statsmodels.__version__\n'0.5.0'\n&gt;&gt;&gt; import numpy \n&gt;&gt;&gt; y = numpy.array([1,2,3,4,5,6,7,8,9])\n&gt;&gt;&gt; X = numpy.array([1,1,2,2,3,3,4,4,5])\n&gt;&gt;&gt; res_ols = sm.OLS(y, X).fit()\n&gt;&gt;&gt; res_ols.params\narray([ 1.82352941])\n\n\nI had expected an array with two elements?!?\nThe intercept and the slope coefficient?\n"
"I have reported this as an issue on pandas issues.\nIn the meanwhile I post this here hoping to save others time, in case they encounter similar issues.\nUpon profiling a process which needed to be optimized I found that renaming columns NOT inplace improves performance (execution time) by  x120.\nProfiling indicates this is related to garbage collection (see below).\nFurthermore, the expected performance is recovered by avoiding the dropna method.\nThe following short example demonstrates a factor x12:\nimport pandas as pd\nimport numpy as np\n\ninplace=True\n%%timeit\nnp.random.seed(0)\nr,c = (7,3)\nt = np.random.rand(r)\ndf1 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\nindx = np.random.choice(range(r),r/3, replace=False)\nt[indx] = np.random.rand(len(indx))\ndf2 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\ndf = (df1-df2).dropna()\n## inplace rename:\ndf.rename(columns={col:'d{}'.format(col) for col in df.columns}, inplace=True)\n\n\n100 loops, best of 3: 15.6 ms per loop\n\nfirst output line of %%prun:\n\nncalls   tottime  percall  cumtime  percall  filename:lineno(function)\n1  0.018 0.018 0.018 0.018 {gc.collect}\n\n\ninplace=False\n%%timeit\nnp.random.seed(0)\nr,c = (7,3)\nt = np.random.rand(r)\ndf1 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\nindx = np.random.choice(range(r),r/3, replace=False)\nt[indx] = np.random.rand(len(indx))\ndf2 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\ndf = (df1-df2).dropna()\n## avoid inplace:\ndf = df.rename(columns={col:'d{}'.format(col) for col in df.columns})\n\n\n1000 loops, best of 3: 1.24 ms per loop\n\navoid dropna\nThe expected performance is recovered by avoiding the dropna method:\n%%timeit\nnp.random.seed(0)\nr,c = (7,3)\nt = np.random.rand(r)\ndf1 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\nindx = np.random.choice(range(r),r/3, replace=False)\nt[indx] = np.random.rand(len(indx))\ndf2 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\n#no dropna:\ndf = (df1-df2)#.dropna()\n## inplace rename:\ndf.rename(columns={col:'d{}'.format(col) for col in df.columns}, inplace=True)\n\n\n1000 loops, best of 3: 865 µs per loop\n\n%%timeit\nnp.random.seed(0)\nr,c = (7,3)\nt = np.random.rand(r)\ndf1 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\nindx = np.random.choice(range(r),r/3, replace=False)\nt[indx] = np.random.rand(len(indx))\ndf2 = pd.DataFrame(np.random.rand(r,c), columns=range(c), index=t)\n## no dropna\ndf = (df1-df2)#.dropna()\n## avoid inplace:\ndf = df.rename(columns={col:'d{}'.format(col) for col in df.columns})\n\n\n1000 loops, best of 3: 902 µs per loop\n\n"
"I'm using df.columns.values to make a list of column names which I then iterate over and make charts, etc... but when I set this up I overlooked the non-numeric columns in the df. Now, I'd much rather not simply drop those columns from the df (or a copy of it). Instead, I would like to find a slick way to eliminate them from the list of column names. \n\nNow I have: \n\nnames = df.columns.values \n\n\nwhat I'd like to get to is something that behaves like: \n\nnames = df.columns.values(column_type=float64) \n\n\nIs there any slick way to do this? I suppose I could make a copy of the df, and drop those non-numeric columns before doing columns.values, but that strikes me as clunky.\n\nWelcome any inputs/suggestions. Thanks. \n"
"I have a huge CSV with many tables with many rows. I would like to simply split each dataframe into 2 if it contains more than 10 rows. \n\nIf true, I would like the first dataframe to contain the first 10 and the rest in the second dataframe. \n\nIs there a convenient function for this? I've looked around but found nothing useful... \n\ni.e. split_dataframe(df, 2(if &gt; 10))? \n"
'I have the following table.  I want to calculate a weighted average grouped by each date based on the formula below.  I can do this using some standard conventional code, but assuming that this data is in a pandas dataframe, is there any easier way to achieve this rather than through iteration?\n\nDate        ID      wt      value   w_avg\n01/01/2012  100     0.50    60      0.791666667\n01/01/2012  101     0.75    80\n01/01/2012  102     1.00    100\n01/02/2012  201     0.50    100     0.722222222\n01/02/2012  202     1.00    80\n\n\n\n  01/01/2012 w_avg = 0.5 * ( 60/ sum(60,80,100)) + .75 * (80/\n  sum(60,80,100)) + 1.0 * (100/sum(60,80,100))\n  \n  01/02/2012 w_avg = 0.5 * ( 100/ sum(100,80)) + 1.0 * ( 80/\n  sum(100,80))\n\n'
'Assuming i have a DataFrame that looks like this:\n\nHour | V1 | V2 | A1 | A2\n 0   | 15 | 13 | 25 | 37  \n 1   | 26 | 52 | 21 | 45 \n 2   | 18 | 45 | 45 | 25 \n 3   | 65 | 38 | 98 | 14\n\n\nIm trying to create a bar plot to compare columns V1 and V2 by the Hour.\nWhen I do:\n\nimport matplotlib.pyplot as plt\nax = df.plot(kind=\'bar\', title ="V comp",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel("Hour",fontsize=12)\nax.set_ylabel("V",fontsize=12)\n\n\nI get a plot and a legend with all the columns\' values and names. How can I modify my code so the plot and legend only displays the columns V1 and V2  \n'
"I need to process a huge amount of CSV files where the time stamp is always a string representing the unix timestamp in milliseconds. I could not find a method yet to modify these columns efficiently.\n\nThis is what I came up with, however this of course duplicates only the column and I have to somehow put it back to the original dataset. I'm sure it can be done when creating the DataFrame?\n\nimport sys\nif sys.version_info[0] &lt; 3:\n    from StringIO import StringIO\nelse:\n    from io import StringIO\nimport pandas as pd\n\ndata = 'RUN,UNIXTIME,VALUE\\n1,1447160702320,10\\n2,1447160702364,20\\n3,1447160722364,42'\n\ndf = pd.read_csv(StringIO(data))\n\nconvert = lambda x: datetime.datetime.fromtimestamp(x / 1e3)\nconverted_df = df['UNIXTIME'].apply(convert)\n\n\nThis will pick the column 'UNIXTIME' and change it from\n\n0    1447160702320\n1    1447160702364\n2    1447160722364\nName: UNIXTIME, dtype: int64\n\n\ninto this\n\n0   2015-11-10 14:05:02.320\n1   2015-11-10 14:05:02.364\n2   2015-11-10 14:05:22.364\nName: UNIXTIME, dtype: datetime64[ns]\n\n\nHowever, I would like to use something like pd.apply() to get the whole dataset returned with the converted column or as I already wrote, simply create datetimes when generating the DataFrame from CSV.\n"
'I have a pandas dataframe with several rows that are near duplicates of each other, except for one value. My goal is to merge or "coalesce" these rows into a single row, without summing the numerical values. \n\nHere is an example of what I\'m working with:\n\nName   Sid   Use_Case  Revenue\nA      xx01  Voice     $10.00\nA      xx01  SMS       $10.00\nB      xx02  Voice     $5.00\nC      xx03  Voice     $15.00\nC      xx03  SMS       $15.00\nC      xx03  Video     $15.00\n\n\nAnd here is what I would like:\n\nName   Sid   Use_Case            Revenue\nA      xx01  Voice, SMS          $10.00\nB      xx02  Voice               $5.00\nC      xx03  Voice, SMS, Video   $15.00\n\n\nThe reason I don\'t want to sum the "Revenue" column is because my table is the result of doing a pivot over several time periods where "Revenue" simply ends up getting listed multiple times instead of having a different value per "Use_Case". \n\nWhat would be the best way to tackle this issue? I\'ve looked into the groupby() function but I still don\'t understand it very well.\n'
"I want to replace all strings that contain a specific substring. So for example if I have this dataframe:\n\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Bob', 'Jane', 'Alice'], \n                   'sport': ['tennis', 'football', 'basketball']})\n\n\nI could replace football with the string 'ball sport' like this:\n\ndf.replace({'sport': {'football': 'ball sport'}})\n\n\nWhat I want though is to replace everything that contains ball (in this case football and basketball) with 'ball sport'. Something like this:\n\ndf.replace({'sport': {'[strings that contain ball]': 'ball sport'}})\n\n"
"I have a data frame similar to the one below:\n\nName    Volume  Value\nMay21   23      21321\nJames   12      12311\nAdi22   11      4435\nHello   34      32454\nGirl90  56      654654\n\n\nI want the output to be in the format:\n\nName    Volume  Value\nMay     23      21321\nJames   12      12311\nAdi     11      4435\nHello   34      32454\nGirl    56      654654\n\n\nWant to remove all the numbers from the Name column. \n\nClosest I have come is doing it at a cell level with the following code:\n\nresult = ''.join([i for i in df['Name'][1] if not i.isdigit()])\n\n\nAny idea how to  do it in a better way at the series/dataframe level.\n"
'I have the following dataframe:\n\nuser_id    purchase_date \n  1        2015-01-23 14:05:21\n  2        2015-02-05 05:07:30\n  3        2015-02-18 17:08:51\n  4        2015-03-21 17:07:30\n  5        2015-03-11 18:32:56\n  6        2015-03-03 11:02:30\n\n\nand purchase_date is a datetime64[ns] column. I need to add a new column df[month] that contains first day of the month of the purchase date: \n\ndf[\'month\']\n2015-01-01\n2015-02-01\n2015-02-01\n2015-03-01\n2015-03-01\n2015-03-01\n\n\nI\'m looking for something like DATE_FORMAT(purchase_date, "%Y-%m-01") m in SQL. I have tried the following code:\n\n     df[\'month\']=df[\'purchase_date\'].apply(lambda x : x.replace(day=1))\n\n\nIt works somehow but returns: 2015-01-01 14:05:21.\n'
"Im new to python and came across a code snippet.\n\ndf = df[~df['InvoiceNo'].str.contains('C')]\n\n\nWould be much obliged if I could know whats the tilde signs usage in this context ?\n"
'Consider a small MWE, taken from another question:\n\nDateTime                Data\n2017-11-21 18:54:31     1\n2017-11-22 02:26:48     2\n2017-11-22 10:19:44     3\n2017-11-22 15:11:28     6\n2017-11-22 23:21:58     7\n2017-11-28 14:28:28    28\n2017-11-28 14:36:40     0\n2017-11-28 14:59:48     1\n\n\nThe goal is to clip all values with an upper bound of 1. My answer uses np.clip, which works fine.\n\nnp.clip(df.Data, a_min=None, a_max=1)\narray([1, 1, 1, 1, 1, 1, 0, 1])\n\n\nOr, \n\nnp.clip(df.Data.values, a_min=None, a_max=1)\narray([1, 1, 1, 1, 1, 1, 0, 1])\n\n\nBoth of which return the same answer. My question is about the relative performance of these two methods. Consider - \n\ndf = pd.concat([df]*1000).reset_index(drop=True)\n\n%timeit np.clip(df.Data, a_min=None, a_max=1)\n1000 loops, best of 3: 270 µs per loop\n\n%timeit np.clip(df.Data.values, a_min=None, a_max=1)\n10000 loops, best of 3: 23.4 µs per loop\n\n\nWhy is there such a massive difference between the two, just by calling values on the latter? In other words...\n\nWhy are numpy functions so slow on pandas objects?\n'
'I\'m having trouble applying "classes" argument with Pandas "to_html" method to style a DataFrame.\n\n"classes : str or list or tuple, default None\nCSS class(es) to apply to the resulting html table"\nfrom: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_html.html\n\nI am able to render a styled DataFrame like this (for example):\n\ndf = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=[\'A\', \'B\'])\n\nmyhtml = df.style.set_properties(**{\'font-size\': \'11pt\', \'font-family\': \'Calibri\',\'border-collapse\': \'collapse\',\'border\': \'1px solid black\'}).render()\n\nwith open(\'myhtml.html\',\'w\') as f:\n    f.write(myhtml)        \n\n\nHow can I style html output from a DataFrame using "classes" with "to_html" like this:\n\ndf.to_html(\'myhtml.html\',classes=&lt;something here&gt;)\n\n'
"Here is a simplified example of my df:\n\nds = pd.DataFrame(np.abs(randn(3, 4)), index=[1,2,3], columns=['A','B','C','D'])\nds\n      A         B         C         D\n1  1.099679  0.042043  0.083903  0.410128\n2  0.268205  0.718933  1.459374  0.758887\n3  0.680566  0.538655  0.038236  1.169403\n\n\nI would like to sum the data in the columns row wise:\n\nds['sum']=ds.sum(axis=1)\nds\n      A         B         C         D       sum\n1  0.095389  0.556978  1.646888  1.959295  4.258550\n2  1.076190  2.668270  0.825116  1.477040  6.046616\n3  0.245034  1.066285  0.967124  0.791606  3.070049\n\n\nNow, here comes my question! I would like to create 4 new columns and calculate the percentage value from the total (sum) in every row. So first value in the first new column should be (0.095389/4.258550), first value in the second new column (0.556978/4.258550)...and so on...\nHelp please \n"
"I have a set of data that I load into python using a pandas dataframe. What I would like to do is create a loop that will print a plot for all the elements in their own frame, not all on one. My data is in an excel file structured in this fashion:\n\nIndex | DATE  | AMB CO 1 | AMB CO 2 |...|AMB CO_n | TOTAL\n1     | 1/1/12|  14      | 33       |...|  236    | 1600\n.     | ...   | ...      | ...      |...|  ...    | ...\n.     | ...   | ...      | ...      |...|  ...    | ...\n.     | ...   | ...      | ...      |...|  ...    | ...\nn\n\n\nThis is what I have for code so far:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nambdf = pd.read_excel('Ambulance.xlsx', \n                      sheetname='Sheet2', index_col=0, na_values=['NA'])\nprint type(ambdf)\nprint ambdf\nprint ambdf['EAS']\n\namb_plot = plt.plot(ambdf['EAS'], linewidth=2)\nplt.title('EAS Ambulance Numbers')\nplt.xlabel('Month')\nplt.ylabel('Count of Deliveries')\nprint amb_plot\n\nfor i in ambdf:\n    print plt.plot(ambdf[i], linewidth = 2)\n\n\nI am thinking of doing something like this:\n\nfor i in ambdf:\n    ambdf_plot = plt.plot(ambdf, linewidth = 2)\n\n\nThe above was not remotely what i wanted and it stems from my unfamiliarity with Pandas, MatplotLib etc, looking at some documentation though to me it looks like matplotlib is not even needed (question 2)\n\nSo A) How can I produce a plot of data for every column in my df\nand B) do I need to use matplotlib or should I just use pandas to do it all?\n\nThank you,\n"
'I just want to check if a single cell in Pandas series is null or not i.e. to check if a value is NaN.\n\nAll other answers are for series and arrays, but not for single value. \n\nI have tried pandas.notnull, pandas.isnull, numpy.isnan. Is there a solution for a single value only?\n'
"There is a Pandas DataFrame:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 300 entries, 5220 to 5519\nData columns (total 3 columns):\nDate             300 non-null datetime64[ns]\nA                300 non-null float64\nB                300 non-null float64\ndtypes: datetime64[ns](1), float64(2)\nmemory usage: 30.5 KB\n\n\nI want to plot A and B series vs Date.\n\nplt.plot_date(data['Date'], data['A'], '-')\nplt.plot_date(data['Date'], data['B'], '-')\n\n\nThen I want apply fill_between() on area between A and B series:\n\nplt.fill_between(data['Date'], data['A'], data['B'],\n                where=data['A'] &gt;= data['B'],\n                facecolor='green', alpha=0.2, interpolate=True)\n\n\nWhich outputs:\n\nTypeError: ufunc 'isfinite' not supported for the input types, and the inputs\ncould not be safely coerced to any supported types according to the casting \nrule ''safe''\n\n\nDoes matplotlib accept pandas datetime64 object in fill_between() function? Should I convert it to different date type?\n"
"I have a dataframe 'gt' like this:\n\norg     group\norg1      1\norg2      1\norg3      2\norg4      3\norg5      3\norg6      3\n\n\nand I would like to add column 'count' to gt dataframe to counts number member of the groups, expected results like this:\n\norg     group   count\norg1      1       2\norg2      1       2\norg3      2       1\norg4      3       3\norg5      3       3\norg6      3       3\n\n\nI know how to do it per one item of the group, but do not know how to make the count repeated for all of the group items, here is the code I have used: \n\ngtcounts = gt.groupby('group').count()\n\n\nCan anybody help?\n"
'I have tried passing the dtype parameter with read_csv as dtype={n: pandas.Categorical} but this does not work properly (the result is an Object). The manual is unclear.\n'
'I would like to know how to read several json files from a single folder (without specifying the files names, just that they are json files). \n\nAlso, it is possible to turn them into a pandas DataFrame?\n\nCan you give me a basic example?\n'
'Hi I am using pandas to convert a column to month. \nWhen I read my data they are objects:\n\nDate           object\ndtype: object\n\n\nSo I am first making them to date time and then try to make them as months:\n\nimport pandas as pd\nfile = \'/pathtocsv.csv\'\ndf = pd.read_csv(file, sep = \',\', encoding=\'utf-8-sig\', usecols= [\'Date\', \'ids\'])    \ndf[\'Date\'] = pd.to_datetime(df[\'Date\'])\ndf[\'Month\'] = df[\'Date\'].dt.month\n\n\nAlso if that helps: \n\nIn [10]: df[\'Date\'].dtype\nOut[10]: dtype(\'O\')\n\n\nSo, the error I get is like this:\n\n/Library/Frameworks/Python.framework/Versions/2.7/bin/User/lib/python2.7/site-packages/pandas/core/series.pyc in _make_dt_accessor(self)\n   2526             return maybe_to_datetimelike(self)\n   2527         except Exception:\n-&gt; 2528             raise AttributeError("Can only use .dt accessor with datetimelike "\n   2529                                  "values")\n   2530 \n\nAttributeError: Can only use .dt accessor with datetimelike values\n\n\nEDITED: \n\nDate columns are like this: \n\n0         2014-01-01         \n1         2014-01-01         \n2         2014-01-01         \n3         2014-01-01         \n4         2014-01-03       \n5         2014-01-03         \n6         2014-01-03         \n7         2014-01-07         \n8         2014-01-08         \n9         2014-01-09 \n\n\nDo you have any ideas?\nThank you very much! \n'
"I would like to replace an entire column on a Pandas DataFrame with another column taken from another DataFrame, an example will clarify what I am looking for\nimport pandas as pd\ndic = {'A': [1, 4, 1, 4], 'B': [9, 2, 5, 3], 'C': [0, 0, 5, 3]}\ndf = pd.DataFrame(dic)\n\ndf is\n'A' 'B' 'C'\n 1   9   0\n 4   2   0\n 1   5   5\n 4   3   3\n\nNow I have another dataframe called df1 with a column &quot;E&quot; that is\ndf1['E'] = [ 4, 4, 4, 0]\n\nand I would like to replace column &quot;B&quot; of df with column &quot;E&quot; of df1\n'A' 'E' 'C'\n 1   4   0\n 4   4   0\n 1   4   5\n 4   0   3\n\nI tried to use the .replace() method in many ways but I didn't get anything good. Can you help me?\n"
"I'm trying to upload a csv file, which is 250MB. Basically 4 million rows and 6 columns of time series data (1min). The usual procedure is:\n\nlocation = r'C:\\Users\\Name\\Folder_1\\Folder_2\\file.csv'\ndf = pd.read_csv(location)\n\n\nThis procedure takes about 20 minutes !!!. Very preliminary I have explored the following options\n\n\nUpload in chunks and then put the chunks together.\nHDF5\n'feather'\n'pickle'\n\n\nI wonder if anybody has compared these options (or more) and there's a clear winner. If nobody answers, In the future I will post my results. I just don't have time right now. \n"
'I have two tables and I would like to append them so that only all the data in table A is retained and data from table B is only added if its key is unique (Key values are unique in table A and B however in some cases a Key will occur in both table A and B). \n\nI think the way to do this will involve some sort of filtering join (anti-join) to get values in table B that do not occur in table A then append the two tables. \n\nI am familiar with R and this is the code I would use to do this in R.\n\nlibrary("dplyr")\n\n## Filtering join to remove values already in "TableA" from "TableB"\nFilteredTableB &lt;- anti_join(TableB,TableA, by = "Key")\n\n## Append "FilteredTableB" to "TableA"\nCombinedTable &lt;- bind_rows(TableA,FilteredTableB)\n\n\nHow would I achieve this in python?\n'
'I want to add a Series (s) to a Pandas DataFrame (df) as a new column. The series has more values than there are rows in the dataframe, so I am using the concat method along axis 1.\n\ndf = pd.concat((df, s), axis=1)\n\nThis works, but the new column of the dataframe representing the series is given an arbitrary numerical column name, and I would like this column to have a specific name instead.\n\nIs there a way to add a series to a dataframe, when the series is longer than the rows of the dataframe, and with a specified column name in the resulting dataframe?\n'
"Suppose I have pandas dataframe as:\n\ndf=pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})\n\n\nWhen I convert it into dask dataframe what should name and divisions parameter consist of:\n\nfrom dask import dataframe as dd \nsd=dd.DataFrame(df.to_dict(),divisions=1,meta=pd.DataFrame(columns=df.columns,index=df.index))\n\n\n\n  TypeError: init() missing 1 required positional argument: 'name'\n\n\nEdit :\nSuppose I create a pandas dataframe like:\n\npd.DataFrame({'a':[1,2,3],'b':[4,5,6]})\n\n\nSimilarly how to create dask dataframe as it needs three additional arguments as name,divisions and meta.\n\nsd=dd.Dataframe({'a':[1,2,3],'b':[4,5,6]},name=,meta=,divisions=)\n\n\nThank you for your reply.\n"
"I have a pandas DataFrame object named xiv which has a column of int64 Volume measurements.  \n\nIn[]: xiv['Volume'].head(5)\nOut[]: \n\n0    252000\n1    484000\n2     62000\n3    168000\n4    232000\nName: Volume, dtype: int64\n\n\nI have read other posts (like this and this) that suggest the following solutions.  But when I use either approach, it doesn't appear to change the dtype of the underlying data:\n\nIn[]: xiv['Volume'] = pd.to_numeric(xiv['Volume'])\n\nIn[]: xiv['Volume'].dtypes\nOut[]: \ndtype('int64')\n\n\nOr...\n\nIn[]: xiv['Volume'] = pd.to_numeric(xiv['Volume'])\nOut[]: ###omitted for brevity###\n\nIn[]: xiv['Volume'].dtypes\nOut[]: \ndtype('int64')\n\nIn[]: xiv['Volume'] = xiv['Volume'].apply(pd.to_numeric)\n\nIn[]: xiv['Volume'].dtypes\nOut[]: \ndtype('int64')\n\n\nI've also tried making a separate pandas Series and using the methods listed above on that Series and reassigning to the x['Volume'] obect, which is a pandas.core.series.Series object.\n\nI have, however, found a solution to this problem using the numpy package's float64 type - this works but I don't know why it's different.\n\nIn[]: xiv['Volume'] = xiv['Volume'].astype(np.float64)\n\nIn[]: xiv['Volume'].dtypes\nOut[]: \ndtype('float64') \n\n\nCan someone explain how to accomplish with the pandas library what the numpy library seems to do easily with its float64 class; that is, convert the column in the xiv DataFrame to a float64 in place.\n"
'Trying to create a new column in the netc df but i get the warning\n\nnetc["DeltaAMPP"] = netc.LOAD_AM - netc.VPP12_AM\n\nC:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\n\nwhats the proper way to create a field in the newer version of Pandas to avoid getting the warning?\n\npd.__version__\nOut[45]:\nu\'0.19.2+0.g825876c.dirty\'\n\n'
"I have time from epochs timestamps\nI use data.Time_req = pd.to_datetime(data.Time_req)\nBut I get UTC time, I need +5:30 from the given time. How do I tell pandas to use 'IST' timezone or just 5hrs 30 mins further to the time it currently shows me. eg. 7 hrs should become 12:30 hrs and so on.\n"
'I am new to pandas and python. My input data is like\n\ncategory   text\n1   hello iam fine. how are you\n1   iam good. how are you doing.\n\ninputData= pd.read_csv(Input\', sep=\'\\t\', names=[\'category\',\'text\'])\nX = inputData["text"]\nY = inputData["category"]\n\n\nhere Y is the panda series object, which i want to convert into numpy array. so i tried .as_matrix\n\nYArray= Y.as_matrix(columns=None)\nprint YArray\n\n\nBut i got the output as [1,1] (which is wrong since i have only one column category and two rows). I want the result as 2x1 matrix.\n'
"I have the following dataframe\n\n           time       X    Y  X_t0     X_tp0  X_t1     X_tp1  X_t2     X_tp2\n0         0.002876    0   10     0       NaN   NaN       NaN   NaN       NaN\n1         0.002986    0   10     0       NaN     0       NaN   NaN       NaN\n2         0.037367    1   10     1  1.000000     0       NaN     0       NaN\n3         0.037374    2   10     2  0.500000     1  1.000000     0       NaN\n4         0.037389    3   10     3  0.333333     2  0.500000     1  1.000000\n5         0.037393    4   10     4  0.250000     3  0.333333     2  0.500000\n\n....\n1030308   9.962213  256  268   256  0.000000   256  0.003906   255  0.003922\n1030309  10.041799    0  268     0      -inf   256  0.000000   256  0.003906\n1030310  10.118960    0  268     0       NaN     0      -inf   256  0.000000\n\n\nI tried with the following\n\ndf.dropna(inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40)\n\nX_train = X_train.drop('time', axis=1)\nX_train = X_train.drop('X_t1', axis=1)\nX_train = X_train.drop('X_t2', axis=1)\nX_test = X_test.drop('time', axis=1)\nX_test = X_test.drop('X_t1', axis=1)\nX_test = X_test.drop('X_t2', axis=1)\nX_test.fillna(X_test.mean(), inplace=True)\nX_train.fillna(X_train.mean(), inplace=True)\ny_train.fillna(y_train.mean(), inplace=True)\n\n\nHowever, I am still getting this error ValueError: Input contains NaN, infinity or a value too large for dtype('float32'). whenever i try to fit a regression model  fit(X_train, y_train)\n\nHow can we remove both the NaN and -inf values at the same time? \n"
'I know that seaborn.countplot has the attribute order which can be set to determine the order of the categories. But what I would like to do is have the categories be in order of descending count. I know that I can accomplish this by computing the count manually (using a groupby operation on the original dataframe, etc.) but I am wondering if this functionality exists with seaborn.countplot. Surprisingly, I cannot find an answer to this question anywhere.\n'
"I'm trying to select a subset of a subset of a dataframe, selecting only some columns, and filtering on the rows.\n\ndf.loc[df.a.isin(['Apple', 'Pear', 'Mango']), ['a', 'b', 'f', 'g']]\n\n\nHowever, I'm getting the error:\n\nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\n\nWhat 's the correct way to slice and filter now?\n"
"I need to filter rows in a pandas dataframe so that a specific string column contains at least one of a list of provided substrings. The substrings may have unusual / regex characters. The comparison should not involve regex and is case insensitive.\n\nFor example:\n\nlst = ['kdSj;af-!?', 'aBC+dsfa?\\-', 'sdKaJg|dksaf-*']\n\n\nI currently apply the mask like this:\n\nmask = np.logical_or.reduce([df[col].str.contains(i, regex=False, case=False) for i in lst])\ndf = df[mask]\n\n\nMy dataframe is large (~1mio rows) and lst has length 100. Is there a more efficient way? For example, if the first item in lst is found, we should not have to test any subsequent strings for that row.\n"
"I'm starting to learn Pandas and am trying to find the most Pythonic (or panda-thonic?) ways to do certain tasks.\n\nSuppose we have a DataFrame with columns A, B, and C.\n\n\nColumn A contains boolean values: each row's A value is either true or false.\nColumn B has some important values we want to plot.\n\n\nWhat we want to discover is the subtle distinctions between B values for rows that have A set to false, vs. B values for rows that have A is true.\n\nIn other words, how can I group by the value of column A (either true or false), then plot the values of column B for both groups on the same graph? The two datasets should be colored differently to be able to distinguish the points.\n\n\n\nNext, let's add another feature to this program: before graphing, we want to compute another value for each row and store it in column D. This value is the mean of all data stored in B for the entire five minutes before a record - but we only include rows that have the same boolean value stored in A.\n\nIn other words, if I have a row where A=True and time=t, I want to compute a value for column D that is the mean of B for all records from time t-5 to t that have the same A=True.\n\nIn this case, how can we execute the groupby on values of A, then apply this computation to each individual group, and finally plot the D values for the two groups?\n"
"I have a dataframe in pandas called 'munged_data' with two columns 'entry_date' and 'dob' which i have converted to Timestamps using pd.to_timestamp.I am trying to figure out how to calculate ages of people based on the time difference between 'entry_date' and 'dob' and to do this i need to get the difference in days between the two columns ( so that i can then do somehting like round(days/365.25). I do not seem to be able to find a way to do this using a vectorized operation. When I do munged_data.entry_date-munged_data.dob i get the following : \n\ninternal_quote_id\n2                    15685977 days, 23:54:30.457856\n3                    11651985 days, 23:49:15.359744\n4                     9491988 days, 23:39:55.621376\n7                     11907004 days, 0:10:30.196224\n9                    15282164 days, 23:30:30.196224\n15                  15282227 days, 23:50:40.261632  \n\n\nHowever i do not seem to be able to extract the days as an integer so that i can continue with my calculation. \nAny help appreciated.\n"
"I am producing some plots in matplotlib and would like to add explanatory text for some of the data. I want to have a string inside my legend as a separate legend item above the '0-10' item. Does anyone know if there is a possible way to do this?\n\n\n\nThis is the code for my legend:\nax.legend(['0-10','10-100','100-500','500+'],loc='best')\n"
"Does anyone know if it is possible to use the DataFrame.loc method to select from a MultiIndex? I have the following DataFrame and would like to be able to access the values located in the 'Dwell' columns, at the indices of ('at', 1), ('at', 3), ('at', 5), and so on (non-sequential).\n\nI'd love to be able to do something like data.loc[['at',[1,3,5]], 'Dwell'], similar to the data.loc[[1,3,5], 'Dwell'] syntax for a regular index (which returns a 3-member series of Dwell values).\n\nMy purpose is to select an arbitrary subset of the data, perform some analysis only on that subset, and then update the new values with the results of the analysis. I plan on using the same syntax to set new values for these data, so chaining selectors wouldn't really work in this case.\n\nHere is a slice of the DataFrame I'm working with: \n\n         Char    Dwell  Flight  ND_Offset  Offset\nQGram                                                           \nat    0     a      100     120   0.000000       0  \n      1     t      180       0   0.108363       5  \n      2     a      100     120   0.000000       0 \n      3     t      180       0   0.108363       5 \n      4     a       20     180   0.000000       0  \n      5     t       80     120   0.108363       5\n      6     a       20     180   0.000000       0   \n      7     t       80     120   0.108363       5  \n      8     a       20     180   0.000000       0  \n      9     t       80     120   0.108363       5   \n      10    a      120     180   0.000000       0  \n\n\nThanks!\n"
"I'm trying to sort a dataframe by descending.\nI put 'False' in the ascending argument, but my order is still ascending.\n\nMy code is:\n\nfrom pandas import DataFrame\nimport pandas as pd\n\nd = {'one':[2,3,1,4,5],\n     'two':[5,4,3,2,1],\n     'letter':['a','a','b','b','c']}\n\ndf = DataFrame(d)\n\ntest = df.sort(['one'], ascending=[False])\n\n\nbut the output is\n\n  letter  one  two\n2      b    1    3\n0      a    2    5\n1      a    3    4\n3      b    4    2\n4      c    5    1\n\n"
'I\'m reading through the Pandas documentation, and the term "broadcasting" is used extensively, but never really defined or explained.\n\nWhat does it mean?\n'
'I want to use multiprocessing on a large dataset to find the distance between two gps points. I constructed a test set, but I have been unable to get multiprocessing to work on this set.\n\nimport pandas as pd\nfrom geopy.distance import vincenty\nfrom itertools import combinations\nimport multiprocessing as mp\n\ndf = pd.DataFrame({\'ser_no\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 0],\n                \'co_nm\': [\'aa\', \'aa\', \'aa\', \'bb\', \'bb\', \'bb\', \'bb\', \'cc\', \'cc\', \'cc\'],\n                \'lat\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n                \'lon\': [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]})\n\n\n\ndef calc_dist(x):\n    return pd.DataFrame(\n               [ [grp,\n                  df.loc[c[0]].ser_no,\n                  df.loc[c[1]].ser_no,\n                  vincenty(df.loc[c[0], x], \n                           df.loc[c[1], x])\n                 ]\n                 for grp,lst in df.groupby(\'co_nm\').groups.items()\n                 for c in combinations(lst, 2)\n               ],\n               columns=[\'co_nm\',\'machineA\',\'machineB\',\'distance\'])\n\nif __name__ == \'__main__\':\n    pool = mp.Pool(processes = (mp.cpu_count() - 1))\n    pool.map(calc_dist, [\'lat\',\'lon\'])\n    pool.close()\n    pool.join()\n\n\nI am using Python 2.7.11 and Ipython 4.1.2 with Anaconda 2.5.0 64-bit on Windows7 Professional when this error occurs.\n\n\n  runfile(\'C:/.../Desktop/multiprocessing test.py\', wdir=\'C:/.../Desktop\')\n  Traceback (most recent call last):\n  \n  File "", line 1, in \n      runfile(\'C:/.../Desktop/multiprocessing test.py\', wdir=\'C:/.../Desktop\')\n  \n  File "C:...\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py", line 699, in runfile\n      execfile(filename, namespace)\n  \n  File "C:...\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py", line 74, in execfile\n      exec(compile(scripttext, filename, \'exec\'), glob, loc)\n  \n  File "C:/..../multiprocessing test.py", line 33, in \n      pool.map(calc_dist, [\'lat\',\'lon\'])\n  \n  File "C:...\\AppData\\Local\\Continuum\\Anaconda2\\lib\\multiprocessing\\pool.py", line 251, in map\n      return self.map_async(func, iterable, chunksize).get()\n  \n  File "C:...\\Local\\Continuum\\Anaconda2\\lib\\multiprocessing\\pool.py", line 567, in get\n      raise self._value\n  \n  TypeError: Failed to create Point instance from 1.\n\n\ndef get(self, timeout=None):\n    self.wait(timeout)\n    if not self._ready:\n        raise TimeoutError\n    if self._success:\n        return self._value\n    else:\n        raise self._value\n\n'
'I have a dataframe with some (hundreds of) million of rows. And I want to convert datetime to timestamp effectively. How can I do it?\n\nMy sample df:\n\ndf = pd.DataFrame(index=pd.DatetimeIndex(start=dt.datetime(2016,1,1,0,0,1),\n    end=dt.datetime(2016,1,2,0,0,1), freq=\'H\'))\\\n    .reset_index().rename(columns={\'index\':\'datetime\'})\n\ndf.head()\n\n             datetime\n0 2016-01-01 00:00:01\n1 2016-01-01 01:00:01\n2 2016-01-01 02:00:01\n3 2016-01-01 03:00:01\n4 2016-01-01 04:00:01\n\n\nNow I convert datetime to timestamp value-by-value with .apply() but it takes a very long time (some hours) if I have some (hundreds of) million rows:\n\ndf[\'ts\'] = df[[\'datetime\']].apply(lambda x: x[0].timestamp(), axis=1).astype(int)\n\ndf.head()\n\n             datetime          ts\n0 2016-01-01 00:00:01  1451602801\n1 2016-01-01 01:00:01  1451606401\n2 2016-01-01 02:00:01  1451610001\n3 2016-01-01 03:00:01  1451613601\n4 2016-01-01 04:00:01  1451617201\n\n\nThe above result is what I want.\n\nIf I try to use the .dt accessor of pandas.Series then I get error message:\n\ndf[\'ts\'] = df[\'datetime\'].dt.timestamp\n\n\n\n  AttributeError: \'DatetimeProperties\' object has no attribute\n  \'timestamp\'\n\n\nIf I try to create eg. the date parts of datetimes with the .dt accessor then it is much more faster then using .apply():\n\ndf[\'date\'] = df[\'datetime\'].dt.date\n\ndf.head()\n\n             datetime          ts        date\n0 2016-01-01 00:00:01  1451602801  2016-01-01\n1 2016-01-01 01:00:01  1451606401  2016-01-01\n2 2016-01-01 02:00:01  1451610001  2016-01-01\n3 2016-01-01 03:00:01  1451613601  2016-01-01\n4 2016-01-01 04:00:01  1451617201  2016-01-01\n\n\nI want something similar with timestamps...\n\nBut I don\'t really understand the official documentation: it talks about "Converting to Timestamps" but I don\'t see any timestamps there; it just talks about converting to datetime with pd.to_datetime() but not to timestamp...\n\npandas.Timestamp constructor also doesn\'t work (returns with the below error):\n\ndf[\'ts2\'] = pd.Timestamp(df[\'datetime\'])\n\n\n\n  TypeError: Cannot convert input to Timestamp\n\n\npandas.Series.to_timestamp also makes something totally different that I want:\n\ndf[\'ts3\'] = df[\'datetime\'].to_timestamp\n\ndf.head()\n\n             datetime          ts                                                ts3\n0 2016-01-01 00:00:01  1451602801  &lt;bound method Series.to_timestamp of 0    2016...\n1 2016-01-01 01:00:01  1451606401  &lt;bound method Series.to_timestamp of 0    2016...\n2 2016-01-01 02:00:01  1451610001  &lt;bound method Series.to_timestamp of 0    2016...\n3 2016-01-01 03:00:01  1451613601  &lt;bound method Series.to_timestamp of 0    2016...\n4 2016-01-01 04:00:01  1451617201  &lt;bound method Series.to_timestamp of 0    2016...\n\n\nThank you!!\n'
"I've looked at the Sklearn stratified sampling docs as well as the pandas docs and also Stratified samples from Pandas and sklearn stratified sampling based on a column but they do not address this issue.\n\nIm looking for a fast pandas/sklearn/numpy way to generate stratified samples of size n from a dataset. However, for rows with less than the specified sampling number, it should take all of the entries.\n\nConcrete example:\n\n\n\nThank you! :)\n"
'I have a dataframe df in pandas that was built using pandas.read_table from a csv file. The dataframe has several columns and it is indexed by one of the columns (which is unique, in that each row has a unique value for that column used for indexing.) \n\nHow can I select rows of my dataframe based on a "complex" filter applied to multiple columns? I can easily select out the slice of the dataframe where column colA is greater than 10 for example:\n\ndf_greater_than10 = df[df["colA"] &gt; 10]\n\n\nBut what if I wanted a filter like: select the slice of df where any of the columns are greater than 10? \n\nOr where the value for colA is greater than 10 but the value for colB is less than 5?\n\nHow are these implemented in pandas?\nThanks.\n'
"I have a DataFrame that has duplicated rows. I'd like to get a DataFrame with a unique index and no duplicates. It's ok to discard the duplicated values. Is this possible? Would it be a done by groupby?\n"
"I want to plot multiple lines from a pandas dataframe and setting different options for each line. I would like to do something like\n\ntestdataframe=pd.DataFrame(np.arange(12).reshape(4,3))\ntestdataframe.plot(style=['s-','o-','^-'],color=['b','r','y'],linewidth=[2,1,1])\n\n\nThis will raise some error messages:\n\n\nlinewidth is not callable with a list\nIn style I can't use 's' and 'o' or any other alphabetical symbol, when defining colors in a list\n\n\nAlso there is some more stuff which seems weird to me\n\n\nwhen I add another plot command to the above code testdataframe[0].plot() it will plot this line in the same plot, if I add the command testdataframe[[0,1]].plot() it will create a new plot\nIf i would call testdataframe[0].plot(style=['s-','o-','^-'],color=['b','r','y']) it is fine with a list in style, but not with a list in color\n\n\nHope somebody can help, thanks.\n"
"I am stymied at the moment.  I am sure that I am missing something simple, but how do you move a series of dates forward by x units?  In my more specific case I want to add 180 days to a date series within a dataframe.\n\nHere is what I have so far:\n\nimport pandas, numpy, StringIO, datetime\n\n\ntxt = '''ID,DATE\n002691c9cec109e64558848f1358ac16,2003-08-13 00:00:00\n002691c9cec109e64558848f1358ac16,2003-08-13 00:00:00\n0088f218a1f00e0fe1b94919dc68ec33,2006-05-07 00:00:00\n0088f218a1f00e0fe1b94919dc68ec33,2006-06-03 00:00:00\n00d34668025906d55ae2e529615f530a,2006-03-09 00:00:00\n00d34668025906d55ae2e529615f530a,2006-03-09 00:00:00\n0101d3286dfbd58642a7527ecbddb92e,2007-10-13 00:00:00\n0101d3286dfbd58642a7527ecbddb92e,2007-10-27 00:00:00\n0103bd73af66e5a44f7867c0bb2203cc,2001-02-01 00:00:00\n0103bd73af66e5a44f7867c0bb2203cc,2008-01-20 00:00:00\n'''\ndf = pandas.read_csv(StringIO.StringIO(txt))\ndf = df.sort('DATE')\ndf.DATE = pandas.to_datetime(df.DATE)\ndf['X_DATE'] = df['DATE'].shift(180, freq=pandas.datetools.Day)\n\n\nThis code generates a type error.  For reference I am using:\n\nPython 2.7.4\nPandas '0.12.0.dev-6e7c4d6'\nNumpy '1.7.1'\n"
'I have data with a time-stamp in UTC. I\'d like to convert the timezone of this timestamp to \'US/Pacific\' and add it as a hierarchical index to a pandas DataFrame. I\'ve been able to convert the timestamp as an Index, but it loses the timezone formatting when I try to add it back into the DataFrame, either as a column or as an index.\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; dat = pd.DataFrame({\'label\':[\'a\', \'a\', \'a\', \'b\', \'b\', \'b\'], \'datetime\':[\'2011-07-19 07:00:00\', \'2011-07-19 08:00:00\', \'2011-07-19 09:00:00\', \'2011-07-19 07:00:00\', \'2011-07-19 08:00:00\', \'2011-07-19 09:00:00\'], \'value\':range(6)})\n&gt;&gt;&gt; dat.dtypes\n#datetime    object\n#label       object\n#value        int64\n#dtype: object\n\n\nNow if I try to convert the Series directly I run into an error.\n\n&gt;&gt;&gt; times = pd.to_datetime(dat[\'datetime\'])\n&gt;&gt;&gt; times.tz_localize(\'UTC\')\n#Traceback (most recent call last):\n#  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n#  File "/Users/erikshilts/workspace/schedule-detection/python/pysched/env/lib/python2.7/site-packages/pandas/core/series.py", line 3170, in tz_localize\n#    raise Exception(\'Cannot tz-localize non-time series\')\n#Exception: Cannot tz-localize non-time series\n\n\nIf I convert it to an Index then I can manipulate it as a timeseries. Notice that the index now has the Pacific timezone.\n\n&gt;&gt;&gt; times_index = pd.Index(times)\n&gt;&gt;&gt; times_index_pacific = times_index.tz_localize(\'UTC\').tz_convert(\'US/Pacific\')\n&gt;&gt;&gt; times_index_pacific\n#&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n#[2011-07-19 00:00:00, ..., 2011-07-19 02:00:00]\n#Length: 6, Freq: None, Timezone: US/Pacific\n\n\nHowever, now I run into problems adding the index back to the dataframe as it loses its timezone formatting:\n\n&gt;&gt;&gt; dat_index = dat.set_index([dat[\'label\'], times_index_pacific])\n&gt;&gt;&gt; dat_index\n#                                      datetime label  value\n#label                                                      \n#a     2011-07-19 07:00:00  2011-07-19 07:00:00     a      0\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     a      1\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     a      2\n#b     2011-07-19 07:00:00  2011-07-19 07:00:00     b      3\n#      2011-07-19 08:00:00  2011-07-19 08:00:00     b      4\n#      2011-07-19 09:00:00  2011-07-19 09:00:00     b      5\n\n\nYou\'ll notice the index is back on the UTC timezone instead of the converted Pacific timezone.\n\nHow can I change the timezone and add it as an index to a DataFrame?\n'
'despite there being at least two good tutorials on how to index a DataFrame in Python\'s pandas library, I still can\'t work out an elegant way of SELECTing on more than one column.\n\n&gt;&gt;&gt; d = pd.DataFrame({\'x\':[1, 2, 3, 4, 5], \'y\':[4, 5, 6, 7, 8]})\n&gt;&gt;&gt; d\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n4  5  8\n&gt;&gt;&gt; d[d[\'x\']&gt;2] # This works fine\n   x  y\n2  3  6\n3  4  7\n4  5  8\n&gt;&gt;&gt; d[d[\'x\']&gt;2 &amp; d[\'y\']&gt;7] # I had expected this to work, but it doesn\'t\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\nI have found (what I think is) a rather inelegant way of doing it, like this\n\n&gt;&gt;&gt; d[d[\'x\']&gt;2][d[\'y\']&gt;7]\n\n\nBut it\'s not pretty, and it scores fairly low for readability (I think).\n\nIs there a better, more Python-tastic way?\n'
'What is the most idiomatic way to normalize each row of a pandas DataFrame? Normalizing the columns is easy, so one (very ugly!) option is:\n\n(df.T / df.T.sum()).T\n\n\nPandas broadcasting rules prevent df / df.sum(axis=1) from doing this\n'
"I've create a tuple generator that extract information from a file filtering only the records of interest and converting it to a tuple that generator returns.\n\nI've try to create a DataFrame from:\n\nimport pandas as pd\ndf = pd.DataFrame.from_records(tuple_generator, columns = tuple_fields_name_list)\n\n\nbut throws an error:\n\n... \nC:\\Anaconda\\envs\\py33\\lib\\site-packages\\pandas\\core\\frame.py in from_records(cls, data, index, exclude, columns, coerce_float, nrows)\n   1046                 values.append(row)\n   1047                 i += 1\n-&gt; 1048                 if i &gt;= nrows:\n   1049                     break\n   1050 \n\nTypeError: unorderable types: int() &gt;= NoneType()\n\n\nI managed it to work consuming the generator in a list, but uses twice memory:\n\ndf = pd.DataFrame.from_records(list(tuple_generator), columns = tuple_fields_name_list)\n\n\nThe files I want to load are big, and memory consumption matters. The last try my computer spends two hours trying to increment virtual memory :(\n\nThe question: Anyone knows a method to create a DataFrame from a record generator directly, without previously convert it to a list?\n\nNote: I'm using python 3.3 and pandas 0.12 with Anaconda on Windows.\n\nUpdate:\n\nIt's not problem of reading the file, my tuple generator do it well, it scan a text compressed file of intermixed records line by line and convert only the wanted data to the correct types, then it yields fields in a generator of tuples form.\nSome numbers, it scans 2111412 records on a 130MB gzip file, about 6.5GB uncompressed, in about a minute and with little memory used.\n\nPandas 0.12 does not allow generators, dev version allows it but put all the generator in a list and then convert to a frame. It's not efficient but it's something that have to deal internally pandas. Meanwhile I've must think about buy some more memory.\n"
"I have a dataframe that may or may not have columns that are the same value. For example\n\n    row    A    B\n    1      9    0\n    2      7    0\n    3      5    0\n    4      2    0\n\n\nI'd like to return just\n\n   row    A  \n   1      9    \n   2      7    \n   3      5    \n   4      2\n\n\nIs there a simple way to identify if any of these columns exist and then remove them?\n"
"I have a DataFrame with a column containing labels for each row (in addition to some relevant data for each row).  I have a dictionary with keys equal to the possible labels and values equal to 2-tuples of information related to that label.  I'd like to tack two new columns onto my frame, one for each part of the 2-tuple corresponding to the label for each row.\n\nHere is the setup:\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(1)\nn = 10\n\nlabels = list('abcdef')\ncolors = ['red', 'green', 'blue']\nsizes = ['small', 'medium', 'large']\n\nlabeldict = {c: (np.random.choice(colors), np.random.choice(sizes)) for c in labels}\n\ndf = pd.DataFrame({'label': np.random.choice(labels, n), \n                   'somedata': np.random.randn(n)})\n\n\nI can get what I want by running:\n\ndf['color'], df['size'] = zip(*df['label'].map(labeldict))\nprint df\n\n  label  somedata  color    size\n0     b  0.196643    red  medium\n1     c -1.545214  green   small\n2     a -0.088104  green   small\n3     c  0.852239  green   small\n4     b  0.677234    red  medium\n5     c -0.106878  green   small\n6     a  0.725274  green   small\n7     d  0.934889    red  medium\n8     a  1.118297  green   small\n9     c  0.055613  green   small\n\n\nBut how can I do this if I don't want to manually type out the two columns on the left side of the assignment?  I.e. how can I create multiple new columns on the fly.  For example, if I had 10-tuples in labeldict instead of 2-tuples, this would be a real pain as currently written.  Here are a couple things that don't work:\n\n# set up attrlist for later use\nattrlist = ['color', 'size']\n\n# non-working idea 1)\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n# non-working idea 2)\ndf.loc[:, attrlist] = zip(*df['label'].map(labeldict))\n\n\nThis does work, but seems like a hack:\n\nfor a in attrlist:\n    df[a] = 0\ndf[attrlist] = zip(*df['label'].map(labeldict))\n\n\nBetter solutions?\n"
"I find myself often having to check whether a column or row exists in a dataframe before trying to reference it. For example I end up adding a lot of code like:\n\nif 'mycol' in df.columns and 'myindex' in df.index: x = df.loc[myindex, mycol]\nelse: x = mydefault\n\n\nIs there any way to do this more nicely? For example on an arbitrary object I can do x = getattr(anobject, 'id', default) - is there anything similar to this in pandas? Really any way to achieve what I'm doing more gracefully?\n"
"pd.get_dummies allows to convert a categorical variable into dummy variables. Besides the fact that it's trivial to reconstruct the categorical variable, is there a preferred/quick way to do it?\n"
"I have the following code to plot a line and a point:\n\ndf = pd.DataFrame({'x': [1, 2, 3], 'y': [3, 4, 6]})\npoint = pd.DataFrame({'x': [2], 'y': [5]})\nax = df.plot(x='x', y='y', label='line')\nax = point.plot(x='x', y='y', ax=ax, style='r-', label='point')\n\n\nHow do I get the single data point to show up?\n\n\n"
'I am very new to Json files. If I have a json file with multiple json objects such as following:\n\n{"ID":"12345","Timestamp":"20140101", "Usefulness":"Yes",\n "Code":[{"event1":"A","result":"1"},…]}\n{"ID":"1A35B","Timestamp":"20140102", "Usefulness":"No",\n "Code":[{"event1":"B","result":"1"},…]}\n{"ID":"AA356","Timestamp":"20140103", "Usefulness":"No",\n "Code":[{"event1":"B","result":"0"},…]}\n…\n\n\nI want to extract all "Timestamp" and "Usefulness" into a data frames:\n\n    Timestamp    Usefulness\n 0   20140101      Yes\n 1   20140102      No\n 2   20140103      No\n …\n\n\nDoes anyone know a general way to deal with such problems?\n'
"suppose I have two dataframes: \n\nimport pandas\n....\n....\ntest1 = pandas.DataFrame([1,2,3,4,5])\n....\n....\ntest2 = pandas.DataFrame([4,2,1,3,7])\n....\n\n\nI tried test1.append(test2) but it is the equivalent of R's rbind.\n\nHow can I combine the two as two columns of a dataframe similar to the cbind function in R? \n"
'I am new to Pandas... I want to a simple and generic way to find which columns are categorical in my DataFrame, when I don\'t manually specify each column type, unlike in this SO question. The df is created with:\n\nimport pandas as pd\ndf = pd.read_csv("test.csv", header=None)\n\n\ne.g.\n\n           0         1         2         3        4\n0   1.539240  0.423437 -0.687014   Chicago   Safari\n1   0.815336  0.913623  1.800160    Boston   Safari\n2   0.821214 -0.824839  0.483724  New York   Safari\n\n\n.\n\nUPDATE (2018/02/04) The question assumes numerical columns are NOT categorical, @Zero\'s accepted answer solves this.\n\nBE CAREFUL - As @Sagarkar\'s comment points out that\'s not always true. The difficulty is that Data Types and Categorical/Ordinal/Nominal types are orthogonal concepts, thus mapping between them isn\'t straightforward. @Jeff\'s answer below specifies the precise manner to achieve the manual mapping.\n'
'I have a df that looks like the following:\n\nid        item        color\n01        truck       red\n02        truck       red\n03        car         black\n04        truck       blue\n05        car         black\n\n\nI am trying to create a df that looks like this:\n\nitem      color       count\ntruck     red          2\ntruck     blue         1\ncar       black        2\n\n\nI have tried \n\ndf["count"] = df.groupby("item")["color"].transform(\'count\')\n\n\nBut it is not quite what I am searching for.\n\nAny guidance is appreciated\n'
'I have an excel sheet that looks like so:\n\nColumn1 Column2 Column3\n0       23      1\n1       5       2\n1       2       3\n1       19      5\n2       56      1\n2       22      2\n3       2       4\n3       14      5\n4       59      1\n5       44      1\n5       1       2\n5       87      3\n\n\nAnd I\'m looking to extract that data, group it by column 1, and add it to a dictionary so it appears like this:\n\n{0: [1],\n1: [2,3,5],\n2: [1,2],\n3: [4,5],\n4: [1],\n5: [1,2,3]}\n\n\nThis is my code so far\n\nexcel = pandas.read_excel(r"e:\\test_data.xlsx", sheetname=\'mySheet\', parse_cols\'A,C\')\nmyTable = excel.groupby("Column1").groups\nprint myTable\n\n\nHowever, my output looks like this:\n\n{0: [0L], 1: [1L, 2L, 3L], 2: [4L, 5L], 3: [6L, 7L], 4: [8L], 5: [9L, 10L, 11L]}\n\n\nThanks!\n'
"I'm having trouble applying upper case to a column in my DataFrame.\n\ndataframe is df.\n\n1/2 ID is the column head that need to apply UPPERCASE.\n\nThe problem is that the values are made up of three letters and three numbers. For example rrr123 is one of the values.\n\ndf['1/2 ID'] = map(str.upper, df['1/2 ID'])\n\n\nI got an error:\n\nTypeError: descriptor 'upper' requires a 'str' object but received a 'unicode' error.\n\nHow can I apply upper case to the first three letters in the column of the DataFrame df?\n"
'i need to format the contents of a Json file in a certain format in a pandas DataFrame so that i can run pandassql to transform the data and run it through a scoring model.\n\nfile = C:\\scoring_model\\json.js   (contents of \'file\' are below)\n\n{\n"response":{\n  "version":"1.1",\n  "token":"dsfgf",\n   "body":{\n     "customer":{\n         "customer_id":"1234567",\n         "verified":"true"\n       },\n     "contact":{\n         "email":"mr@abc.com",\n         "mobile_number":"0123456789"\n      },\n     "personal":{\n         "gender": "m",\n         "title":"Dr.",\n         "last_name":"Muster",\n         "first_name":"Max",\n         "family_status":"single",\n         "dob":"1985-12-23",\n     }\n   }\n }\n\n\nI need the dataframe to look like this (obviously all values on same row, tried to format it best as possible for this question):\n\nversion | token | customer_id | verified | email      | mobile_number | gender |\n1.1     | dsfgf | 1234567     | true     | mr@abc.com | 0123456789    | m      |\n\ntitle | last_name | first_name |family_status | dob\nDr.   | Muster    | Max        | single       | 23.12.1985\n\n\nI have looked at all the other questions on this topic, have tried various ways to load Json file into pandas \n\n`with open(r\'C:\\scoring_model\\json.js\', \'r\') as f:`\n    c = pd.read_json(f.read())\n\n `with open(r\'C:\\scoring_model\\json.js\', \'r\') as f:`\n    c = f.readlines()\n\n\ntried pd.Panel() in this solution Python Pandas: How to split a sorted dictionary in a column of a dataframe \n\nwith dataframe results from [yo = f.readlines()] thought about trying to split contents of each cell based on ("") and find a way to put the split contents into different columns but no luck so far. Your expertise is greatly appreciated. Thank you in advance.\n'
"I am trying but not able to remove nan while combining two columns of a DataFrame.\n\nData is like:\n\nfeedback_id                  _id\n568a8c25cac4991645c287ac     nan    \n568df45b177e30c6487d3603     nan    \nnan                          568df434832b090048f34974       \nnan                          568cd22e9e82dfc166d7dff1   \n568df3f0832b090048f34711     nan\nnan                          568e5a38b4a797c664143dda   \n\n\nI want:\n\nfeedback_request_id\n568a8c25cac4991645c287ac\n568df45b177e30c6487d3603\n568df434832b090048f34974\n568cd22e9e82dfc166d7dff1\n568df3f0832b090048f34711\n568e5a38b4a797c664143dda\n\n\nHere is my code:\n\ndf3['feedback_request_id'] = ('' if df3['_id'].empty else df3['_id'].map(str)) + ('' if df3['feedback_id'].empty else df3['feedback_id'].map(str))\n\n\nOutput I'm getting:\n\nfeedback_request_id\n568a8c25cac4991645c287acnan\n568df45b177e30c6487d3603nan\nnan568df434832b090048f34974\nnan568cd22e9e82dfc166d7dff1\n568df3f0832b090048f34711nan\nnan568e5a38b4a797c664143dda\n\n\nI have tried this, also: \n\ndf3['feedback_request_id'] = ('' if df3['_id']=='nan' else df3['_id'].map(str)) + ('' if df3['feedback_id']=='nan' else df3['feedback_id'].map(str))\n\n\nBut it's giving the error:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n"
"I'm trying to get a bigger chart. However, the figure method from matplotlib does not seem to be working properly.\n\nI get a message, which is not an error: \n\n\n\nimport pandas.io.data as web\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n...\nplt.figure(figsize=(20,10))\ndf2['media']= df2['SPY']*.6 + df2['TLT']*.4\ndf2.plot()\nplt.show()\n\n\nWhat's wrong with my code?\n"
"I have a dataframe extracted from Kaggle's San Fransico Salaries: https://www.kaggle.com/kaggle/sf-salaries\nand I wish to create a set of the values of a column, for instance 'Status'.\n\nThis is what I have tried but it brings a list of all the records instead of the set (sf is how I name the data frame).\n\na=set(sf['Status'])\nprint a\n\n\nAccording to this webpage, this should work.\nHow to construct a set out of list items in python?\n"
"I am trying to implement an LSTM with Keras.\n\nI know that LSTM's in Keras require a 3D tensor with shape (nb_samples, timesteps, input_dim) as an input. However, I am not entirely sure how the input should look like in my case, as I have just one sample of T observations for each input, not multiple samples, i.e. (nb_samples=1, timesteps=T, input_dim=N). Is it better to split each of my inputs into samples of length T/M? T is around a few million observations for me, so how long should each sample in that case be, i.e., how would I choose M?\n\nAlso, am I right in that this tensor should look something like:\n\n[[[a_11, a_12, ..., a_1M], [a_21, a_22, ..., a_2M], ..., [a_N1, a_N2, ..., a_NM]], \n [[b_11, b_12, ..., b_1M], [b_21, b_22, ..., b_2M], ..., [b_N1, b_N2, ..., b_NM]], \n ..., \n [[x_11, x_12, ..., a_1M], [x_21, x_22, ..., x_2M], ..., [x_N1, x_N2, ..., x_NM]]]\n\n\nwhere M and N defined as before and x corresponds to the last sample that I would have obtained from splitting as discussed above? \n\nFinally, given a pandas dataframe with T observations in each column, and N columns, one for each input, how can I create such an input to feed to Keras?\n"
'How do I suppress scientific notation output from dataframe.describe(): \n\ncontrib_df["AMNT"].describe()\n\ncount    1.979680e+05\nmean     5.915134e+02\nstd      1.379618e+04\nmin     -1.750000e+05\n25%      4.000000e+01\n50%      1.000000e+02\n75%      2.500000e+02\nmax      3.000000e+06\nName: AMNT, dtype: float64\n\n\nMy data is of type float64:\n\ncontrib_df["AMNT"].dtypes\n\ndtype(\'float64\')\n\n'
"Could somebody explain to me a difference between\n\ndf2 = df1\n\ndf2 = df1.copy()\n\ndf3 = df1.copy(deep=False)\n\n\nI have tried all options and did as follows:\n\ndf1 = pd.DataFrame([1,2,3,4,5])\ndf2 = df1\ndf3 = df1.copy()\ndf4 = df1.copy(deep=False)\ndf1 = pd.DataFrame([9,9,9])\n\n\nand returned as follows:\n\ndf1: [9,9,9]\ndf2: [1,2,3,4,5]\ndf3: [1,2,3,4,5]\ndf4: [1,2,3,4,5]\n\n\nSo, I observe no difference in the output between .copy() and .copy(deep=False). Why? \n\nI would expect one of the options '=', copy(), copy(deep=False) to return [9,9,9]\n\nWhat am I missing please?\n"
'I have a dataframe in python pandas with several columns taken from a CSV file.\n\nFor instance, data =:\n\nDay P1S1 P1S2 P1S3 P2S1 P2S2 P2S3\n1   1    2    2    3    1    2\n2   2    2    3    5    4    2\n\n\nAnd what I need is to get the sum of all columns which name starts with P1... something like P1* with a wildcard.\n\nSomething like the following which gives an error:\n\n\n  P1Sum = data["P1*"]\n\n\nIs there any why to do this with pandas?\n'
"I'm using pandas and I'm wondering what's the easiest way to get the business days between a start and end date using pandas?\n\nThere are a lot of posts out there regarding doing this in Python (for example), but I would be interested to use directly pandas as I think that pandas can probably handle this quite easy.\n"
"I have a DataFrame with an index called city_id of cities in the format [city],[state] (e.g., new york,ny containing integer counts in the columns. The problem is that I have multiple rows for the same city, and I want to collapse the rows sharing a city_id by adding their column values. I looked at groupby() but it wasn't immediately obvious how to apply it to this problem.\n\nEdit:\n\nAn example: I'd like to change this:\n\ncity_id    val1 val2 val3\nhouston,tx    1    2    0\nhouston,tx    0    0    1\nhouston,tx    2    1    1\n\n\ninto this:\n\ncity_id    val1 val2 val3\nhouston,tx    3    3    2\n\n\nif there are ~10-20k rows.\n"
"Question\n\nIs it possible to specify a float precision specifically for each column to be printed by the Python pandas package method pandas.DataFrame.to_csv?\n\nBackground\n\nIf I have a pandas dataframe that is arranged like this:\n\nIn [53]: df_data[:5]\nOut[53]: \n    year  month  day       lats       lons  vals\n0   2012      6   16  81.862745 -29.834254   0.0\n1   2012      6   16  81.862745 -29.502762   0.1\n2   2012      6   16  81.862745 -29.171271   0.0\n3   2012      6   16  81.862745 -28.839779   0.2\n4   2012      6   16  81.862745 -28.508287   0.0\n\n\nThere is the float_format option that can be used to specify a precision, but this applys that precision to all columns of the dataframe when printed.\n\nWhen I use that like so:\n\ndf_data.to_csv(outfile, index=False,\n                   header=False, float_format='%11.6f')\n\n\nI get the following, where vals is given an inaccurate precision:\n\n2012,6,16,  81.862745, -29.834254,   0.000000\n2012,6,16,  81.862745, -29.502762,   0.100000\n2012,6,16,  81.862745, -29.171270,   0.000000\n2012,6,16,  81.862745, -28.839779,   0.200000\n2012,6,16,  81.862745, -28.508287,   0.000000\n\n"
"I have two pandas dataframes.\n\nnoclickDF = DataFrame([[0,123,321],[0,1543,432]], columns=['click', 'id','location'])\nclickDF = DataFrame([[1,123,421],[1,1543,436]], columns=['click', 'location','id'])\n\n\nI simply want to join such that the final DF will look like:\n\nclick  |  id   |   location\n0         123        321\n0         1543       432\n1         421        123\n1         436       1543\n\n\nAs you can see the column names of both original DF's are the same, but not in the same order. Also there is no join in a column.\n"
'I\'m writing a script to reduce a large .xlsx file with headers into a csv, and then write a new csv file with only the required columns based on header name.\n\nimport pandas\nimport csv\n\ndf = pandas.read_csv(\'C:\\\\Python27\\\\Work\\\\spoofing.csv\')\n\ntime = df["InviteTime (Oracle)"]\norignum = df["Orig Number"]\norigip = df["Orig IP Address"]\ndestnum = df["Dest Number"]\n\ndf.to_csv(\'output.csv\', header=[time,orignum,origip,destnum])\n\n\nThe error I\'m getting is with that last bit of code, and it says\n\nValueError: Writing 102 cols but got 4 aliases\n\n\nI\'m sure i\'m overlooking something stupid, but I\'ve read over the to_csv documentation on the pandas website and I\'m still at a loss. I know I\'m using the to_csv parameters incorrectly but I can\'t seem to get my head around the documentation I guess.\n\nAny help is appreciated, thanks!\n'
"I'm finding it difficult to understand how to fix a Pipeline I created (read: largely pasted from a tutorial). It's python 3.4.2:\n\ndf = pd.DataFrame\ndf = DataFrame.from_records(train)\n\ntest = [blah1, blah2, blah3]\n\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', RandomForestClassifier())])\n\npipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1]))\npredicted = pipeline.predict(test)\n\n\nWhen I run it, I get:\n\nTypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.\n\n\nThis is for the line pipeline.fit(numpy.asarray(df[0]), numpy.asarray(df[1])).\n\nI've experimented a lot with solutions through numpy, scipy, and so forth, but I still don't know how to fix it. And yes, similar questions have come up before, but not inside a pipeline.\nWhere is it that I have to apply toarray or todense?\n"
"Consider the following code running in iPython/Jupyter Notebook:\n\nfrom pandas import *\n%matplotlib inline\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\nfor y_ax in ys:\n    ts = Series(y_ax,index=x_ax)\n    ts.plot(kind='bar', figsize=(15,5))\n\n\nI would expect to have 2 separate plots as output, instead, I get the two series merged in one single plot.\nWhy is that? How can I get two separate plots keeping the for loop?\n"
'I\'m relatively new with numpy and pandas (I\'m an experimental physicist so I\'ve been using ROOT for years...).\nA common plot in ROOT is a 2D scatter plot where, given a list of x- and y- values, makes a "heatmap" type scatter plot of one variable versus the other.\n\nHow is this best accomplished with numpy and Pandas? I\'m trying to use the Dataframe.plot() function, but I\'m struggling to even create the Dataframe.\n\nimport numpy as np\nimport pandas as pd\nx = np.random.randn(1,5)\ny = np.sin(x)\ndf = pd.DataFrame(d)\n\n\nFirst off, this dataframe has shape (1,2), but I would like it to have shape (5,2).\nIf I can get the dataframe the right shape, I\'m sure I can figure out the DataFrame.plot() function to draw what I want.\n'
'I try to add a new column "energy_class" to a dataframe "df_energy" which it contains the string "high" if the "consumption_energy" value > 400, "medium" if the "consumption_energy" value is between 200 and 400, and "low" if the "consumption_energy" value is under 200.\nI try to use  np.where from numpy, but I see that numpy.where(condition[, x, y]) treat only two condition not 3 like in my case.\n\nAny idea to help me please?\n\nThank you in advance\n'
'Hi I have a dataframe like this:\n\n    A             B \n0:  some value    [[L1, L2]]\n\n\nI want to change it into:\n\n    A             B \n0:  some value    L1\n1:  some value    L2\n\n\nHow can I do that?\n'
"I have a Series, like this:\n\nseries = pd.Series({'a': 1, 'b': 2, 'c': 3})\n\n\nI want to convert it to a dataframe like this:\n\n    a   b   c\n0   1   2   3\n\n\npd.Series.to_frame does't work, it got result like,\n\n    0\na   1\nb   2\nc   3\n\n\nHow to construct a DataFrame from Series, with index of Series as columns?\n"
'I"m not sure how to reset index after dropna() \n\ndf_all = df_all.dropna()\n\ndf_all.reset_index(drop=True)\n\n\nbut after drop row index  would skip  for example jump from 0,1,2,4  ..\n'
"I am trying to install pandas using pip to run some pandas-based Python programs. I already installed pip. I tried googling and SO'ing but didn't find a solution to this error. Can somebody share your inputs on this?\n\nC:\\&gt; pip install pandas\n\n\nError:\n\npip is not recognized as an internal or external command, operable program or batch file.\n\n"
"I have a question that is basically the same as a question back from 2014 (see here). However, my script still throws an error.\n\nHere is what I do: I have a pandas dataframe with a few columns. I plot a simple boxplot comparison. \n\ng = sns.boxplot(x='categories', y='oxygen', hue='target', data=df)\ng.set_xticklabels(rotation=30)\n\n\nThe graph looks like this:\n\n\n\nI'd like to rotate the x-labels by 30 degrees. Hence I use g.set_xticklabels(rotation=30). However, I get the following error: \n\nset_xticklabels() missing 1 required positional argument: 'labels'\n\nI don't know how to pass the matplotlib labels argument to seaborns sns.boxplot. Any ideas? \n"
"Is there a way in pandas to check if a dataframe column has duplicate values, without actually dropping rows? I have a function that will remove duplicate rows, however, I only want it to run if there are actually duplicates in a specific column.\n\nCurrently I compare the number of unique values in the column to the number of rows: if there are less unique values than rows then there are duplicates and the code runs.\n\n if len(df['Student'].unique()) &lt; len(df.index):\n    # Code to remove duplicates based on Date column runs\n\n\nIs there an easier or more efficient way to check if duplicate values exist in a specific column, using pandas?\n\nSome of the sample data I am working with (only two columns shown). If duplicates are found then another function identifies which row to keep (row with oldest date):\n\n    Student Date\n0   Joe     December 2017\n1   James   January 2018\n2   Bob     April 2018\n3   Joe     December 2017\n4   Jack    February 2018\n5   Jack    March 2018\n\n"
'I try to compare below two dataframe with "check_index_type" set to False. According to the documentation, if it set to False, it shouldn\'t "check the Index class, dtype and inferred_type are identical". Did I misunderstood the documentation? how to compare ignoring the index and return True for below test?\n\nI know I can reset the index but prefer not to.\n\nhttps://pandas.pydata.org/pandas-docs/stable/generated/pandas.testing.assert_frame_equal.html\n\nfrom pandas.util.testing import assert_frame_equal\nimport pandas as pd\nd1 = pd.DataFrame([[1,2], [10, 20]], index=[0,2])\nd2 = pd.DataFrame([[1, 2], [10, 20]], index=[0, 1])\nassert_frame_equal(d1, d2, check_index_type=False)\n\n\nAssertionError: DataFrame.index are different\nDataFrame.index values are different (50.0 %)\n[left]:  Int64Index([0, 2], dtype=\'int64\')\n[right]: Int64Index([0, 1], dtype=\'int64\')\n\n'
"I have a dataframe df as follows:\n\n| name  | coverage |\n|-------|----------|\n| Jason | 25.1     |\n\n\nI want to convert it to a dictionary.\nI used the following command in pandas :\n\ndict=df.to_dict()\n\n\nThe output of dict gave me the following:\n\n{'coverage': {0: 25.1}, 'name': {0: 'Jason'}} \n\n\nI do not want the 0 in my output. I believe this is captured because of the column index in my dataframe df.\nWhat can I do to eliminate 0 in my output\n( I do not want index to be captured.) expected output :\n\n{'coverage': 25.1, 'name': 'Jason'} \n\n"
'So I learned that I can use DataFrame.groupby without having a MultiIndex to do subsampling/cross-sections.\n\nOn the other hand, when I have a MultiIndex on a DataFrame, I still need to use DataFrame.groupby to do sub-sampling/cross-sections.\n\nSo what is a MultiIndex good for apart from the quite helpful and pretty display of the hierarchies when printing?\n'
"I had a dataframe and did a groupby in FIPS and summed the groups that worked fine.\n\nkl = ks.groupby('FIPS')\n\nkl.aggregate(np.sum)\n\n\nI just want a normal Dataframe back but I have a pandas.core.groupby.DataFrameGroupBy object. \n"
"How to apply conditional logic to a Pandas DataFrame. \n\nSee DataFrame shown below,\n\n   data desired_output\n0     1          False\n1     2          False\n2     3           True\n3     4           True\n\n\nMy original data is show in the 'data' column and the desired_output is shown next to it. If the number in 'data' is below 2.5, the desired_output is False.\n\nI could apply a loop and do re-construct the DataFrame... but that would be 'un-pythonic'\n"
"I have a pandas DataFrame with multiple columns.\n\n2u    2s    4r     4n     4m   7h   7v\n0     1     1      0      0     0    1\n0     1     0      1      0     0    1\n1     0     0      1      0     1    0\n1     0     0      0      1     1    0\n1     0     1      0      0     1    0\n0     1     1      0      0     0    1\n\n\nWhat I want to do is to convert this pandas.DataFrame into a list like following\n\nX = [\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1],\n     [1, 0, 0, 0, 1, 1],\n     [0, 1, 1, 0, 0, 0],\n     [0, 0, 0, 1, 0, 0],\n     [0, 0, 1, 1, 1, 0],\n     [1, 1, 0, 0, 0, 1]\n    ]\n\n\n2u    2s    4r     4n     4m   7h   7v   are column headings. It will change in different situations, so don't bother about it.\n"
"I have a pandas.DataFrame that I wish to export to a CSV file. However, pandas seems to write some of the values as float instead of int types. I couldn't not find how to change this behavior.\n\nBuilding a data frame:\n\ndf = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'], dtype=int)\nx = pandas.Series([10,10,10], index=['a','b','d'], dtype=int)\ny = pandas.Series([1,5,2,3], index=['a','b','c','d'], dtype=int)\nz = pandas.Series([1,2,3,4], index=['a','b','c','d'], dtype=int)\ndf.loc['x']=x; df.loc['y']=y; df.loc['z']=z\n\n\nView it:\n\n&gt;&gt;&gt; df\n    a   b    c   d\nx  10  10  NaN  10\ny   1   5    2   3\nz   1   2    3   4\n\n\nExport it:\n\n&gt;&gt;&gt; df.to_csv('test.csv', sep='\\t', na_rep='0', dtype=int)\n&gt;&gt;&gt; for l in open('test.csv'): print l.strip('\\n')\n        a       b       c       d\nx       10.0    10.0    0       10.0\ny       1       5       2       3\nz       1       2       3       4\n\n\nWhy do the tens have a dot zero ?\n\nSure, I could just stick this function into my pipeline to reconvert the whole CSV file, but it seems unnecessary:\n\ndef lines_as_integer(path):\n    handle = open(path)\n    yield handle.next()\n    for line in handle:\n        line = line.split()\n        label = line[0]\n        values = map(float, line[1:])\n        values = map(int, values)\n        yield label + '\\t' + '\\t'.join(map(str,values)) + '\\n'\nhandle = open(path_table_int, 'w')\nhandle.writelines(lines_as_integer(path_table_float))\nhandle.close()\n\n"
"What's the difference between:\n\nMaand['P_Sanyo_Gesloten']\nOut[119]: \nTime\n2012-08-01 00:00:11    0\n2012-08-01 00:05:10    0\n2012-08-01 00:10:11    0\n2012-08-01 00:20:10    0\n2012-08-01 00:25:10    0\n2012-08-01 00:30:09    0\n2012-08-01 00:40:10    0\n2012-08-01 00:50:09    0\n2012-08-01 01:05:10    0\n2012-08-01 01:10:10    0\n2012-08-01 01:15:10    0\n2012-08-01 01:25:10    0\n2012-08-01 01:30:10    0\n2012-08-01 01:35:09    0\n2012-08-01 01:40:10    0\n...\n2012-08-30 22:35:09    0\n2012-08-30 22:45:10    0\n2012-08-30 22:50:09    0\n2012-08-30 22:55:10    0\n2012-08-30 23:00:09    0\n2012-08-30 23:05:10    0\n2012-08-30 23:10:09    0\n2012-08-30 23:15:10    0\n2012-08-30 23:20:09    0\n2012-08-30 23:25:10    0\n2012-08-30 23:35:09    0\n2012-08-30 23:40:10    0\n2012-08-30 23:45:09    0\n2012-08-30 23:50:10    0\n2012-08-30 23:55:11    0\nName: P_Sanyo_Gesloten, Length: 7413, dtype: int64\n\n\nAnd\n\nMaand[[1]]\nOut[120]: \n&amp;ltclass 'pandas.core.frame.DataFrame'&amp;gt\nDatetimeIndex: 7413 entries, 2012-08-01 00:00:11 to 2012-08-30 23:55:11\nData columns (total 1 columns):\nP_Sanyo_Gesloten    7413  non-null values\ndtypes: int64(1)\n\n\nHow can I get data by column-indexnumber?\nAnd not by an Index-string?\n"
"Using Pandas to plot in I-Python Notebook, I have several plots and because Matplotlib decides the Y axis it is setting them differently and we need to compare that data using the same range.\nI have tried several variants on: (I assume I'll need to apply the limits to each plot.. but since I can't get one working... From the Matplotlib doc it seems that I need to set ylim, but can't figure the syntax to do so.\n\ndf2250.plot(); plt.ylim((100000,500000)) &lt;&lt;&lt;&lt; if I insert the ; I get int not callable and  if I leave it out I get invalid syntax. anyhow, neither is right...\ndf2260.plot()\ndf5.plot()\n\n"
'I\'m wondering if there is a more efficient way to use the str.contains() function in Pandas, to search for two partial strings at once. I want to search a given column in a dataframe for data that contains either "nt" or "nv". Right now, my code looks like this:\n\n    df[df[\'Behavior\'].str.contains("nt", na=False)]\n    df[df[\'Behavior\'].str.contains("nv", na=False)]\n\n\nAnd then I append one result to another. What I\'d like to do is use a single line of code to search for any data that includes "nt" OR "nv" OR "nf." I\'ve played around with some ways that I thought should work, including just sticking a pipe between terms, but all of these result in errors. I\'ve checked the documentation, but I don\'t see this as an option. I get errors like this: \n\n    ---------------------------------------------------------------------------\n    TypeError                                 Traceback (most recent call last)\n    &lt;ipython-input-113-1d11e906812c&gt; in &lt;module&gt;()\n    3 \n    4 \n    ----&gt; 5 soctol = f_recs[f_recs[\'Behavior\'].str.contains("nt"|"nv", na=False)]\n    6 soctol\n\n    TypeError: unsupported operand type(s) for |: \'str\' and \'str\'\n\n\nIs there a fast way to do this? Thanks for any help, I am a beginner but am LOVING pandas for data wrangling.\n'
"How to convert a column consisting of datetime64 objects to a strings that would read\n01-11-2013 for today's date of November 1.\n\nI have tried \n\ndf['DateStr'] = df['DateObj'].strftime('%d%m%Y')\n\n\nbut I get this error\n\nAttributeError: 'Series' object has no attribute 'strftime'\n"
'I\'d like to insert a link (to a web page) inside a pandas table, so when it is displayed in ipython notebook, I could press the link.\n\nI tried the following:\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(range(5), columns=[\'a\'])\n\nIn [3]: df[\'b\'] = df[\'a\'].apply(lambda x: \'http://example.com/{0}\'.format(x))\n\nIn [4]: df\nOut[4]:\n   a                     b\n0  0  http://example.com/0\n1  1  http://example.com/1\n2  2  http://example.com/2\n3  3  http://example.com/3\n4  4  http://example.com/4\n\n\nbut the url is just displayed as text. \n\nI also tried using ipython HTML object:\n\nIn [5]: from IPython.display import HTML\n\nIn [6]: df[\'b\'] = df[\'a\'].apply(lambda x:HTML(\'http://example.com/{0}\'.format(x)))\n\nIn [7]: df\nOut[7]:\n   a                                                 b\n0  0  &lt;IPython.core.display.HTML object at 0x0481E530&gt;\n1  1  &lt;IPython.core.display.HTML object at 0x0481E770&gt;\n2  2  &lt;IPython.core.display.HTML object at 0x0481E7B0&gt;\n3  3  &lt;IPython.core.display.HTML object at 0x0481E810&gt;\n4  4  &lt;IPython.core.display.HTML object at 0x0481EA70&gt;\n\n\nbut it will only display the repr of the object.\n\nAny other ideas?\n\nEDIT:\nalko got the right answer, just wanted to add that the cell width is limited by default, and long html code will be truncated, ie:\n\n&lt;a href="aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa0"&gt;xxx&lt;/a&gt;\n\n\nwill become this:\n\n&lt;a href="aaaaaaaaaaaaaaaaaaaaaa...\n\n\nand won\'t be displayed correctly.\n(even though the text xxx is short and can fit in the cell)\n\nI\'ve bypassed it by setting:\n\npd.set_printoptions(max_colwidth=-1)\n\n'
'Sometimes I end up with a series of tuples/lists when using Pandas. This is common when, for example, doing a group-by and passing a function that has multiple return values:\n\nimport numpy as np\nfrom scipy import stats\ndf = pd.DataFrame(dict(x=np.random.randn(100),\n                       y=np.repeat(list("abcd"), 25)))\nout = df.groupby("y").x.apply(stats.ttest_1samp, 0)\nprint out\n\ny\na       (1.3066417476, 0.203717485506)\nb    (0.0801133382517, 0.936811414675)\nc      (1.55784329113, 0.132360504653)\nd     (0.267999459642, 0.790989680709)\ndtype: object\n\n\nWhat is the correct way to "unpack" this structure so that I get a DataFrame with two columns?\n\nA related question is how I can unpack either this structure or the resulting dataframe into two Series/array objects. This almost works:\n\nt, p = zip(*out)\n\n\nbut it t is\n\n (array(1.3066417475999257),\n array(0.08011333825171714),\n array(1.557843291126335),\n array(0.267999459641651))\n\n\nand one needs to take the extra step of squeezing it.\n'
'I have a pandas dataframe I would like to se the diagonal to 0\n\nimport numpy\nimport pandas\n\ndf = pandas.DataFrame(numpy.random.rand(5,5))\ndf\n\nOut[6]:\n     0           1           2           3               4\n0    0.536596    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.954506    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.901891    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.521104    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.075738\n5 rows × 5 columns\n\n\nnow I want to set the diagonal to 0:\n\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        if i==j:\n            df.loc[i,j] = 0\ndf\nOut[9]:\n     0           1           2           3           4\n0    0.000000    0.674319    0.032815    0.908086    0.215334\n1    0.735022    0.000000    0.889162    0.711610    0.415118\n2    0.119985    0.979056    0.000000    0.687829    0.947549\n3    0.186921    0.899178    0.296294    0.000000    0.638924\n4    0.354053    0.060022    0.275224    0.635054    0.000000\n5 rows × 5 columns\n\n\nbut there must be a more pythonic way than that!?\n'
'From Pandas data frame, how to get index of non "NaN" values?\n\nMy data frame is\n\n    A    b     c\n0   1    q1    1\n1   2    NaN   3\n2   3    q2    3\n3   4    q1    NaN\n4   5    q2    7\n\n\nAnd I want the index of the rows in which column b is not NaN. (there can be NaN values in other column e.g. c )\n\n\n  non_nana_index = [0,2,3,4]\n\n\nUsing this non "NaN" index list I want to create new data frame which column b do not have "Nan"\n\ndf2=\n\n    A    b     c\n0   1    q1    1\n1   3    q2    3\n2   4    q1    NaN\n3   5    q2    7\n\n'
"Using the Pandas package in python, I would like to sum (marginalize) over one level in a series with a 3-level multiindex to produce a series with a 2 level multiindex. For example, if I have the following:\n\nind = [tuple(x) for x in ['ABC', 'ABc', 'AbC', 'Abc', 'aBC', 'aBc', 'abC', 'abc']]\nmi = pd.MultiIndex.from_tuples(ind)\ndata = pd.Series([264, 13, 29, 8, 152, 7, 15, 1], index=mi)\n\nA  B  C    264\n      c     13\n   b  C     29\n      c      8\na  B  C    152\n      c      7\n   b  C     15\n      c      1\n\n\nI would like to sum over the variable C to produce the following output:\n\nA  B    277\n   b     37\na  B    159\n   b     16\n\n\nWhat is the best way in Pandas to do this?\n"
"Apologies in advance for this, but after two hours of searching and trying I cannot get the right answer here. I have a data frame, populated via pandas io sql.read_frame(). \nThe column that is proving to be too much for me is of dtype int64. The integers is of the format YYYYMMDD. for example 20070530 - 30th of may 2007. I have tried a range of approaches, the most obvious being; \n\npd.to_datetime(dt['Date']) and pd.to_datetime(str(dt['Date']))\n\nwith multiple variations on the functions different parameters. \n\nThe result has been, at best, that the date interpreted as being the time. The date is set to 1970-01-01 - outcome as per above example 1970-01-01 00:00:00.020070530\n\nI also tried various .map() functions found in simular posts. \n\nI have noticed that according to np.date_range() can interpret string values of the format YYYYMMDD, but that is the closest I have come to seeing a solution. \n\nIf anyone has an answer, I would be very greatful!\n\nEDIT: In view of the answer from Ed Chum, the problem is most likely related to encoding. rep() on a subset of the dataFrame yields:\n\n\n  OrdNo    LstInvDt\\n0\n  9    20070620\\n1\n  11    20070830\\n2\n  19    20070719\\n3\n  21    20070719\\n4\n  23    20070719\\n5\n  26    20070911\\n7\n  29    20070918\\n8\n  31    0070816\\n9\n  34    20070925\\n10     \n\n\nThis is when LstInvDt is dtype int64.\n"
'I have a pandas data frame mydf that has two columns,and both columns are datetime datatypes: mydate and mytime. I want to add three more columns: hour, weekday, and weeknum. \n\ndef getH(t): #gives the hour\n    return t.hour\ndef getW(d): #gives the week number\n    return d.isocalendar()[1] \ndef getD(d): #gives the weekday\n    return d.weekday() # 0 for Monday, 6 for Sunday\n\nmydf["hour"] = mydf.apply(lambda row:getH(row["mytime"]), axis=1)\nmydf["weekday"] = mydf.apply(lambda row:getD(row["mydate"]), axis=1)\nmydf["weeknum"] = mydf.apply(lambda row:getW(row["mydate"]), axis=1)\n\n\nThe snippet works, but it\'s not computationally efficient as it loops through the data frame at least three times. I would just like to know if there\'s a faster and/or more optimal way to do this. For example, using zip or merge? If, for example, I just create one function that returns three elements, how should I implement this? To illustrate, the function would be:\n\ndef getHWd(d,t):\n    return t.hour, d.isocalendar()[1], d.weekday()\n\n'
"I have a datetime column as below -\n&gt;&gt;&gt; df['ACC_DATE'].head(2)\n538   2006-04-07\n550   2006-04-12\nName: ACC_DATE, dtype: datetime64[ns]\n\nNow, I want to subtract an year from each row of this column. How can I achieve the same &amp; which library can I use?\nThe expected field -\n        ACC_DATE    NEW_DATE\n538   2006-04-07  2005-04-07\n549   2006-04-12  2005-04-12\n\n"
"I have a small dataframe, say this one :\n\n    Mass32      Mass44  \n12  0.576703    0.496159\n13  0.576658    0.495832\n14  0.576703    0.495398    \n15  0.576587    0.494786\n16  0.576616    0.494473\n...\n\n\nI would like to have a rolling mean of column Mass32, so I do this:\n\nx['Mass32s'] = pandas.rolling_mean(x.Mass32, 5).shift(-2)\n\n\nIt works as in I have a new column named Mass32s which contains what I expect it to contain but I also get the warning message:\n\n\n  A value is trying to be set on a copy of a slice from a DataFrame. Try\n  using .loc[row_indexer,col_indexer] = value instead\n  \n  See the the caveats in the documentation:\n  http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\n\nI'm wondering if there's a better way to do it, notably to avoid getting this warning message.\n"
"I am confused by the performance in Pandas when building a large dataframe chunk by chunk. In Numpy, we (almost) always see better performance by preallocating a large empty array and then filling in the values. As I understand it, this is due to Numpy grabbing all the memory it needs at once instead of having to reallocate memory with every append operation.\n\nIn Pandas, I seem to be getting better performance by using the df = df.append(temp) pattern.\n\nHere is an example with timing. The definition of the Timer class follows. As you, see I find that preallocating is roughly 10x slower than using append! Preallocating a dataframe with np.empty values of the appropriate dtype helps a great deal, but the append method is still the fastest.\n\nimport numpy as np\nfrom numpy.random import rand\nimport pandas as pd\n\nfrom timer import Timer\n\n# Some constants\nnum_dfs = 10  # Number of random dataframes to generate\nn_rows = 2500\nn_cols = 40\nn_reps = 100  # Number of repetitions for timing\n\n# Generate a list of num_dfs dataframes of random values\ndf_list = [pd.DataFrame(rand(n_rows*n_cols).reshape((n_rows, n_cols)), columns=np.arange(n_cols)) for i in np.arange(num_dfs)]\n\n##\n# Define two methods of growing a large dataframe\n##\n\n# Method 1 - append dataframes\ndef method1():\n    out_df1 = pd.DataFrame(columns=np.arange(4))\n    for df in df_list:\n        out_df1 = out_df1.append(df, ignore_index=True)\n    return out_df1\n\ndef method2():\n# # Create an empty dataframe that is big enough to hold all the dataframes in df_list\nout_df2 = pd.DataFrame(columns=np.arange(n_cols), index=np.arange(num_dfs*n_rows))\n#EDIT_1: Set the dtypes of each column\nfor ix, col in enumerate(out_df2.columns):\n    out_df2[col] = out_df2[col].astype(df_list[0].dtypes[ix])\n# Fill in the values\nfor ix, df in enumerate(df_list):\n    out_df2.iloc[ix*n_rows:(ix+1)*n_rows, :] = df.values\nreturn out_df2\n\n# EDIT_2: \n# Method 3 - preallocate dataframe with np.empty data of appropriate type\ndef method3():\n    # Create fake data array\n    data = np.transpose(np.array([np.empty(n_rows*num_dfs, dtype=dt) for dt in df_list[0].dtypes]))\n    # Create placeholder dataframe\n    out_df3 = pd.DataFrame(data)\n    # Fill in the real values\n    for ix, df in enumerate(df_list):\n        out_df3.iloc[ix*n_rows:(ix+1)*n_rows, :] = df.values\n    return out_df3\n\n##\n# Time both methods\n##\n\n# Time Method 1\ntimes_1 = np.empty(n_reps)\nfor i in np.arange(n_reps):\n    with Timer() as t:\n       df1 = method1()\n    times_1[i] = t.secs\nprint 'Total time for %d repetitions of Method 1: %f [sec]' % (n_reps, np.sum(times_1))\nprint 'Best time: %f' % (np.min(times_1))\nprint 'Mean time: %f' % (np.mean(times_1))\n\n#&gt;&gt;  Total time for 100 repetitions of Method 1: 2.928296 [sec]\n#&gt;&gt;  Best time: 0.028532\n#&gt;&gt;  Mean time: 0.029283\n\n# Time Method 2\ntimes_2 = np.empty(n_reps)\nfor i in np.arange(n_reps):\n    with Timer() as t:\n        df2 = method2()\n    times_2[i] = t.secs\nprint 'Total time for %d repetitions of Method 2: %f [sec]' % (n_reps, np.sum(times_2))\nprint 'Best time: %f' % (np.min(times_2))\nprint 'Mean time: %f' % (np.mean(times_2))\n\n#&gt;&gt;  Total time for 100 repetitions of Method 2: 32.143247 [sec]\n#&gt;&gt;  Best time: 0.315075\n#&gt;&gt;  Mean time: 0.321432\n\n# Time Method 3\ntimes_3 = np.empty(n_reps)\nfor i in np.arange(n_reps):\n    with Timer() as t:\n        df3 = method3()\n    times_3[i] = t.secs\nprint 'Total time for %d repetitions of Method 3: %f [sec]' % (n_reps, np.sum(times_3))\nprint 'Best time: %f' % (np.min(times_3))\nprint 'Mean time: %f' % (np.mean(times_3))\n\n#&gt;&gt;  Total time for 100 repetitions of Method 3: 6.577038 [sec]\n#&gt;&gt;  Best time: 0.063437\n#&gt;&gt;  Mean time: 0.065770\n\n\nI use a nice Timer courtesy of Huy Nguyen:\n\n# credit: http://www.huyng.com/posts/python-performance-analysis/\n\nimport time\n\nclass Timer(object):\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n\n    def __enter__(self):\n        self.start = time.clock()\n        return self\n\n    def __exit__(self, *args):\n        self.end = time.clock()\n        self.secs = self.end - self.start\n        self.msecs = self.secs * 1000  # millisecs\n        if self.verbose:\n            print 'elapsed time: %f ms' % self.msecs\n\n\nIf you are still following, I have two questions:\n\n1) Why is the append method faster? (NOTE: for very small dataframes, i.e. n_rows = 40, it is actually slower).\n\n2) What is the most efficient way to build a large dataframe out of chunks? (In my case, the chunks are all large csv files).\n\nThanks for your help!\n\nEDIT_1: \nIn my real world project, the columns have different dtypes. So I cannot use the pd.DataFrame(.... dtype=some_type) trick to improve the performance of preallocation, per BrenBarn's recommendation. The dtype parameter forces all the columns to be the same dtype [Ref. issue 4464]\n\nI added some lines to method2() in my code to change the dtypes column-by-column to match in the input dataframes. This operation is expensive and negates the benefits of having the appropriate dtypes when writing blocks of rows.\n\nEDIT_2: Try preallocating a dataframe using placeholder array np.empty(... dtyp=some_type). Per @Joris's suggestion. \n"
'So i tried reading all the csv files from a folder and then concatenate them to create a big csv(structure of all the files was same), save it and read it again. All this was done using Pandas. The Error occurs while reading. I am Attaching the code and the Error below.\n\nimport pandas as pd\nimport numpy as np\nimport glob\n\npath =r\'somePath\' # use your path\nallFiles = glob.glob(path + "/*.csv")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=0)\n    list_.append(df)\nstore = pd.concat(list_)\nstore.to_csv("C:\\work\\DATA\\Raw_data\\\\store.csv", sep=\',\', index= False)\nstore1 = pd.read_csv("C:\\work\\DATA\\Raw_data\\\\store.csv", sep=\',\')\n\n\nError:-\n\nCParserError                              Traceback (most recent call last)\n&lt;ipython-input-48-2983d97ccca6&gt; in &lt;module&gt;()\n----&gt; 1 store1 = pd.read_csv("C:\\work\\DATA\\Raw_data\\\\store.csv", sep=\',\')\n\nC:\\Users\\armsharm\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc in parser_f(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\n    472                     skip_blank_lines=skip_blank_lines)\n    473 \n--&gt; 474         return _read(filepath_or_buffer, kwds)\n    475 \n    476     parser_f.__name__ = name\n\nC:\\Users\\armsharm\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc in _read(filepath_or_buffer, kwds)\n    258         return parser\n    259 \n--&gt; 260     return parser.read()\n    261 \n    262 _parser_defaults = {\n\nC:\\Users\\armsharm\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n    719                 raise ValueError(\'skip_footer not supported for iteration\')\n    720 \n--&gt; 721         ret = self._engine.read(nrows)\n    722 \n    723         if self.options.get(\'as_recarray\'):\n\nC:\\Users\\armsharm\\AppData\\Local\\Continuum\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.pyc in read(self, nrows)\n   1168 \n   1169         try:\n-&gt; 1170             data = self._reader.read(nrows)\n   1171         except StopIteration:\n   1172             if nrows is None:\n\npandas\\parser.pyx in pandas.parser.TextReader.read (pandas\\parser.c:7544)()\n\npandas\\parser.pyx in pandas.parser.TextReader._read_low_memory (pandas\\parser.c:7784)()\n\npandas\\parser.pyx in pandas.parser.TextReader._read_rows (pandas\\parser.c:8401)()\n\npandas\\parser.pyx in pandas.parser.TextReader._tokenize_rows (pandas\\parser.c:8275)()\n\npandas\\parser.pyx in pandas.parser.raise_parser_error (pandas\\parser.c:20691)()\n\nCParserError: Error tokenizing data. C error: Buffer overflow caught - possible malformed input file.\n\n\nI tried using csv reader as well:-\n\nimport csv\nwith open("C:\\work\\DATA\\Raw_data\\\\store.csv", \'rb\') as f:\n    reader = csv.reader(f)\n    l = list(reader)\n\n\nError:-\n\nError                                     Traceback (most recent call last)\n&lt;ipython-input-36-9249469f31a6&gt; in &lt;module&gt;()\n      1 with open(\'C:\\work\\DATA\\Raw_data\\\\store.csv\', \'rb\') as f:\n      2     reader = csv.reader(f)\n----&gt; 3     l = list(reader)\n\nError: new-line character seen in unquoted field - do you need to open the file in universal-newline mode?\n\n'
"I have a dataframe of taxi data with two columns that looks like this:\n\nNeighborhood    Borough        Time\nMidtown         Manhattan      X\nMelrose         Bronx          Y\nGrant City      Staten Island  Z\nMidtown         Manhattan      A\nLincoln Square  Manhattan      B\n\n\nBasically, each row represents a taxi pickup in that neighborhood in that borough. Now, I want to find the top 5 neighborhoods in each borough with the most number of pickups. I tried this:\n\ndf['Neighborhood'].groupby(df['Borough']).value_counts()\n\n\nWhich gives me something like this:\n\nborough                          \nBronx          High  Bridge          3424\n               Mott Haven            2515\n               Concourse Village     1443\n               Port Morris           1153\n               Melrose                492\n               North Riverdale        463\n               Eastchester            434\n               Concourse              395\n               Fordham                252\n               Wakefield              214\n               Kingsbridge            212\n               Mount Hope             200\n               Parkchester            191\n......\n\nStaten Island  Castleton Corners        4\n               Dongan Hills             4\n               Eltingville              4\n               Graniteville             4\n               Great Kills              4\n               Castleton                3\n               Woodrow                  1\n\n\nHow do I filter it so that I get only the top 5 from each? I know there are a few questions with a similar title but they weren't helpful to my case.\n"
"I understand that OHLC re-sampling of time series data in Pandas, using one column of data, will work perfectly, for example on the following dataframe:\n\n&gt;&gt;df\nctime       openbid\n1443654000  1.11700\n1443654060  1.11700\n...\n\ndf['ctime']  = pd.to_datetime(df['ctime'], unit='s')\ndf           = df.set_index('ctime')\ndf.resample('1H',  how='ohlc', axis=0, fill_method='bfill')\n\n\n&gt;&gt;&gt;\n                     open     high     low       close\nctime                                                   \n2015-09-30 23:00:00  1.11700  1.11700  1.11687   1.11697\n2015-09-30 24:00:00  1.11700  1.11712  1.11697   1.11697\n...\n\n\nBut what do I do if the data is already in an OHLC format? From what I can gather the OHLC method of the API calculates an OHLC slice for every column, hence if my data is in the format:\n\n             ctime  openbid  highbid   lowbid  closebid\n0       1443654000  1.11700  1.11700  1.11687   1.11697\n1       1443654060  1.11700  1.11712  1.11697   1.11697\n2       1443654120  1.11701  1.11708  1.11699   1.11708\n\n\nWhen I try to re-sample I get an OHLC for each of the columns, like so:\n\n                     openbid                             highbid           \\\n                        open     high      low    close     open     high   \nctime                                                                       \n2015-09-30 23:00:00  1.11700  1.11700  1.11700  1.11700  1.11700  1.11712   \n2015-09-30 23:01:00  1.11701  1.11701  1.11701  1.11701  1.11708  1.11708 \n...\n                                        lowbid                             \\\n                         low    close     open     high      low    close   \nctime                                                                       \n2015-09-30 23:00:00  1.11700  1.11712  1.11687  1.11697  1.11687  1.11697   \n2015-09-30 23:01:00  1.11708  1.11708  1.11699  1.11699  1.11699  1.11699  \n...\n\n                    closebid                             \n                        open     high      low    close  \nctime                                                    \n2015-09-30 23:00:00  1.11697  1.11697  1.11697  1.11697  \n2015-09-30 23:01:00  1.11708  1.11708  1.11708  1.11708  \n\n\nIs there a quick(ish) workaround for this that someone is willing to share please, without me having to get knee-deep in pandas manual?\n\nThanks.\n\nps, there is this answer - Converting OHLC stock data into a different timeframe with python and pandas -  but it was 4 years ago, so I am hoping there has been some progress.\n"
'I have used the\n\nsklearn.preprocessing.OneHotEncoder\n\n\nto transform some data the output is scipy.sparse.csr.csr_matrix\nhow can I merge it back into my original dataframe along with the other columns?\n\nI tried to use pd.concat but I get \n\nTypeError: cannot concatenate a non-NDFrame object\n\n\nThanks\n'
"I am appending rows to a pandas DataFrame within a for loop, but at the end the dataframe is always empty. I don't want to add the rows to an array and then call the DataFrame constructer, because my actual for loop handles lots of data. I also tried pd.concat without success. Could anyone highlight what I am missing to make the append statement work? Here's a dummy example:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame([])\n\nfor i in np.arange(0, 4):\n    if i % 2 == 0:\n        data.append(pd.DataFrame({'A': i, 'B': i + 1}, index=[0]), ignore_index=True)\n    else:\n        data.append(pd.DataFrame({'A': i}, index=[0]), ignore_index=True)\n\nprint data.head()\n\nEmpty DataFrame\nColumns: []\nIndex: []\n[Finished in 0.676s]\n\n"
'I have to read  several files some in Excel format and some in CSV format. Some of the files have hundreds of columns.\n\nIs there a way to select several ranges of columns without specifying all the column names or positions? For example something like selecting columns 1 -10, 15, 17 and 50-100:\n\ndf = df.ix[1:10, 15, 17, 50:100]\n\n\nI need to know how to do this both when creating dataframe from Excel files and CSV files and after the data framers created.\n'
"I am converting strings to categorical values in my dataset using the following piece of code.\n\ndata['weekday'] = pd.Categorical.from_array(data.weekday).labels \n\n\nFor eg,\n\nindex    weekday\n0        Sunday\n1        Sunday\n2        Wednesday\n3        Monday\n4        Monday\n5        Thursday\n6        Tuesday\n\n\nAfter encoding the weekday, my dataset appears like this:\n\nindex    weekday\n    0       3\n    1       3\n    2       6\n    3       1\n    4       1\n    5       4\n    6       5\n\n\nIs there any way I can know that Sunday has been mapped to 3, Wednesday to 6 and so on?\n"
"I'm looking for a way to get a list of all the keys in a GroupBy object, but I can't seem to find one via the docs nor through Google. \n\nThere is definitely a way to access the groups through their keys, like so:\n\ndf_gb = df.groupby(['EmployeeNumber'])\ndf_gb.get_group(key)\n\n\n...so I figure there's a way to access a list (or the like) of the keys in a GroupBy object. I'm looking for something like this:\n\ndf_gb.keys\nOut: [1234, 2356, 6894, 9492]\n\n\nI figure I could just loop through the GroupBy object and get the keys that way, but I think there's got to be a better way.\n"
"I would like to use df.groupby() in combination with apply() to apply a function to each row per group.\n\nI normally use the following code, which usually works (note, that this is without groupby()):\n\ndf.apply(myFunction, args=(arg1,))\n\n\nWith the groupby() I tried the following:\n\ndf.groupby('columnName').apply(myFunction, args=(arg1,))\n\n\nHowever, I get the following error:\n\n\n  TypeError: myFunction() got an unexpected keyword argument 'args'\n\n\nHence, my question is: How can I use groupby() and apply() with a function that needs arguments?\n"
"I've a data frame that looks like the following\n\nx = pd.DataFrame({'user': ['a','a','b','b'], 'dt': ['2016-01-01','2016-01-02', '2016-01-05','2016-01-06'], 'val': [1,33,2,1]})\n\n\nWhat I would like to be able to do is find the minimum and maximum date within the date column and expand that column to have all the dates there while simultaneously filling in 0 for the val column. So the desired output is\n\n            dt user  val\n0   2016-01-01    a    1\n1   2016-01-02    a   33\n2   2016-01-03    a    0\n3   2016-01-04    a    0\n4   2016-01-05    a    0\n5   2016-01-06    a    0\n6   2016-01-01    b    0\n7   2016-01-02    b    0\n8   2016-01-03    b    0\n9   2016-01-04    b    0\n10  2016-01-05    b    2\n11  2016-01-06    b    1\n\n\nI've tried the solution mentioned here and here but they aren't what I'm after.\nAny pointers much appreciated.\n"
"I realize that dropping NaNs from a dataframe is as easy as df.dropna but for some reason that isn't working on mine and I'm not sure why.\n\nHere is my original dataframe:\n\nfish_frame1:                       0   1   2         3   4       5   6          7\n0               #0915-8 NaN NaN       NaN NaN     NaN NaN        NaN\n1                   NaN NaN NaN  LIVE WGT NaN  AMOUNT NaN      TOTAL\n2               GBW COD NaN NaN     2,280 NaN   $0.60 NaN  $1,368.00\n3               POLLOCK NaN NaN     1,611 NaN   $0.01 NaN     $16.11\n4                 WHAKE NaN NaN       441 NaN   $0.70 NaN    $308.70\n5           GBE HADDOCK NaN NaN     2,788 NaN   $0.01 NaN     $27.88\n6           GBW HADDOCK NaN NaN    16,667 NaN   $0.01 NaN    $166.67\n7               REDFISH NaN NaN       932 NaN   $0.01 NaN      $9.32\n8    GB WINTER FLOUNDER NaN NaN       145 NaN   $0.25 NaN     $36.25\n9   GOM WINTER FLOUNDER NaN NaN    25,070 NaN   $0.35 NaN  $8,774.50\n10        GB YELLOWTAIL NaN NaN        26 NaN   $1.75 NaN     $45.50\n\n\nThe code that follows is an attempt to drop all NaNs as well as any columns with more than 3 NaNs (either one, or both, should work I think):\n\nfish_frame.dropna()\nfish_frame.dropna(thresh=len(fish_frame) - 3, axis=1)\n\n\nThis produces: \n\nfish_frame1 after dropna:                       0   1   2         3   4       5   6          7\n0               #0915-8 NaN NaN       NaN NaN     NaN NaN        NaN\n1                   NaN NaN NaN  LIVE WGT NaN  AMOUNT NaN      TOTAL\n2               GBW COD NaN NaN     2,280 NaN   $0.60 NaN  $1,368.00\n3               POLLOCK NaN NaN     1,611 NaN   $0.01 NaN     $16.11\n4                 WHAKE NaN NaN       441 NaN   $0.70 NaN    $308.70\n5           GBE HADDOCK NaN NaN     2,788 NaN   $0.01 NaN     $27.88\n6           GBW HADDOCK NaN NaN    16,667 NaN   $0.01 NaN    $166.67\n7               REDFISH NaN NaN       932 NaN   $0.01 NaN      $9.32\n8    GB WINTER FLOUNDER NaN NaN       145 NaN   $0.25 NaN     $36.25\n9   GOM WINTER FLOUNDER NaN NaN    25,070 NaN   $0.35 NaN  $8,774.50\n10        GB YELLOWTAIL NaN NaN        26 NaN   $1.75 NaN     $45.50\n\n\nI am a novice with Pandas so I'm not sure if this isn't working because I'm doing something wrong or I'm misunderstanding something or misusing a command. Any help is appreciated thanks. \n"
'I want to create a python pandas DataFrame with a single row, to use further pandas functionality like dumping to *.csv.\n\nI have seen code like the following being used, but I only end up with the column structure, but empty data\n\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf[\'A\'] = 1\ndf[\'B\'] = 1.23\ndf[\'C\'] = "Hello"\ndf.columns = [[\'A\',\'B\',\'C\']]\n\nprint df\n\nEmpty DataFrame\nColumns: [A, B, C]\nIndex: []\n\n\nWhile I know there are other ways to do it (like from a dictionary), I want to understand why this piece of code is not working for me!? Is this a version issue? (using pandas==0.19.2)\n'
"I am reading multiple JSON objects into one DataFrame. The problem is that some of the columns are lists. Also, the data is very big and because of that I cannot use the available solutions on the internet. They are very slow and memory-inefficient \n\nHere is how my data looks like:\n\ndf = pd.DataFrame({'A': ['x1','x2','x3', 'x4'], 'B':[['v1','v2'],['v3','v4'],['v5','v6'],['v7','v8']], 'C':[['c1','c2'],['c3','c4'],['c5','c6'],['c7','c8']],'D':[['d1','d2'],['d3','d4'],['d5','d6'],['d7','d8']], 'E':[['e1','e2'],['e3','e4'],['e5','e6'],['e7','e8']]})\n    A       B          C           D           E\n0   x1  [v1, v2]    [c1, c2]    [d1, d2]    [e1, e2]\n1   x2  [v3, v4]    [c3, c4]    [d3, d4]    [e3, e4]\n2   x3  [v5, v6]    [c5, c6]    [d5, d6]    [e5, e6]\n3   x4  [v7, v8]    [c7, c8]    [d7, d8]    [e7, e8]\n\n\nAnd this is the shape of my data: (441079, 12)\n\nMy desired output is:\n\n    A       B          C           D           E\n0   x1      v1         c1         d1          e1\n0   x1      v2         c2         d2          e2\n1   x2      v3         c3         d3          e3\n1   x2      v4         c4         d4          e4\n.....\n\n\nEDIT: After being marked as duplicate, I would like to stress on the fact that in this question I was looking for an efficient method of exploding multiple columns. Therefore the approved answer is able to explode an arbitrary number of columns on very large datasets efficiently. Something that the answers to the other question failed to do (and that was the reason I asked this question after testing those solutions).\n"
'I have a DataFrame df:\n\ndf = pd.DataFrame(columns=["App","Feature1", "Feature2","Feature3",\n                           "Feature4","Feature5",\n                           "Feature6","Feature7","Feature8"], \n                  data=[["SHA",0,0,1,1,1,0,1,0],\n                        ["LHA",1,0,1,1,0,1,1,0],\n                        ["DRA",0,0,0,0,0,0,1,0],\n                        ["FRA",1,0,1,1,1,0,1,1],\n                        ["BRU",0,0,1,0,1,0,0,0],\n                        ["PAR",0,1,1,1,1,0,1,0],\n                        ["AER",0,0,1,1,0,1,1,0],\n                        ["SHE",0,0,0,1,0,0,1,0]])\n\n\nI want to create a stacked bar chart so that each stack would correspond to App while the Y axis would contain the count of 1 values and the X axis would be Feature.\n\nIt should be similar to this bar chart with the only difference that now I want to see stack bars and a legend with colors:\n\ndf_c = df.iloc[:, 1:].eq(1).sum().rename_axis(\'Feature\').reset_index(name=\'Cou\u200c\u200bnt\')\ndf_c = df_c.sort_values(\'Count\')\n\nplt.figure(figsize=(12,8))\nax = sns.barplot(x="Feature", y="Count", data=df_c, palette=sns.color_palette("GnBu", 10))\nplt.xticks(rotation=\'vertical\')\nax.grid(b=True, which=\'major\', color=\'#d3d3d3\', linewidth=1.0)\nax.grid(b=True, which=\'minor\', color=\'#d3d3d3\', linewidth=0.5)\nplt.show()\n\n'
'I noticed Pandas now has support for Sparse Matrices and Arrays.  Currently, I create DataFrame()s like this:\n\nreturn DataFrame(matrix.toarray(), columns=features, index=observations)\n\n\nIs there a way to create a SparseDataFrame() with a scipy.sparse.csc_matrix() or csr_matrix()? Converting to dense format kills RAM badly. Thanks!\n'
"I have a simple 2 column csv file called st1.csv:\n\nGRID    St1  \n1457    614  \n1458    657  \n1459    679  \n1460    732  \n1461    754  \n1462    811  \n1463    748  \n\n\nHowever, when I try to read the csv file, the first column is not loaded:\n\na = pandas.DataFrame.from_csv('st1.csv')  \na.columns\n\n\noutputs:\n\n Index([u'ST1'], dtype=object)\n\n\nWhy is the first column not being read?\n"
"\nHow could I convert the values of column 'count' to absolute value?\n\nA summary of my dataframe this:\n\n             datetime   count\n0   2011-01-20 00:00:00 14.565996\n1   2011-01-20 01:00:00 10.204177\n2   2011-01-20 02:00:00 -1.261569\n3   2011-01-20 03:00:00 1.938322\n4   2011-01-20 04:00:00 1.938322\n5   2011-01-20 05:00:00 -5.963259\n6   2011-01-20 06:00:00 73.711525\n\n"
'I have a pandas data frame like df with a column construct_name\n\nconstruct_name\naaaa_t1_2    \ncccc_t4_10\nbbbb_g3_3\n\n\nand so on. I want to first split all the names at the underscore and store the first element (aaaa,cccc, etc.) as another column name.\n\nExpected output\n\nconstruct_name  name\naaaa_t1_2       aaaa\ncccc_t4_10      bbbb\n\n\nand so on.\n\nI tried the following\ndf[\'construct_name\'].map(lambda row:row.split("_")) and it gives me a list like\n\n[aaaa,t1,2]\n[cccc,t4,10]\n\n\nand so on\n\nBut when I do \n\ndf[\'construct_name\'].map(lambda row:row.split("_"))[0] to get the first element of the list I get an error. Can you suggest a fix. Thanks\n'
"Apologies if this has been asked before, I can't seem to find an answer.\n\nIf I create a dataframe like so:\n\nimport pandas as pd, numpy as np\n\ndf = pd.DataFrame(np.random.randint(0,100,size=(100, 2)), columns=list('AB'))\n\n\nHow would I change the entry in column A to be the number 16 from row 0 -15, for example? In other words, how do I replace cells based purely on index?\n"
'I have some difficulty in importing a JSON file with pandas.\n\nimport pandas as pd\nmap_index_to_word = pd.read_json(\'people_wiki_map_index_to_word.json\')\n\n\nThis is the  error that I get: \n\nValueError: If using all scalar values, you must pass an index\n\n\nThe file structure is simplified like this:\n\n{"biennials": 522004, "lb915": 116290, "shatzky": 127647, "woode": 174106, "damfunk": 133206, "nualart": 153444, "hatefillot": 164111, "missionborn": 261765, "yeardescribed": 161075, "theoryhe": 521685}\n\n\nIt is from the machine learning course of University of Washington on Coursera. You can find the file here.\n'
'I have a dataframe df that loads data from a database. Most of the columns are json strings while some are even list of jsons. For example:\n\nid     name     columnA                               columnB\n1     John     {"dist": "600", "time": "0:12.10"}    [{"pos": "1st", "value": "500"},{"pos": "2nd", "value": "300"},{"pos": "3rd", "value": "200"}, {"pos": "total", "value": "1000"}]\n2     Mike     {"dist": "600"}                       [{"pos": "1st", "value": "500"},{"pos": "2nd", "value": "300"},{"pos": "total", "value": "800"}]\n...\n\n\nAs you can see, not all the rows have the same number of elements in the json strings for a column. \n\nWhat I need to do is keep the normal columns like id and name as it is and flatten the json columns like so:\n\nid    name   columnA.dist   columnA.time   columnB.pos.1st   columnB.pos.2nd   columnB.pos.3rd     columnB.pos.total\n1     John   600            0:12.10        500               300               200                 1000 \n2     Mark   600            NaN            500               300               Nan                 800 \n\n\nI have tried using json_normalize like so:\n\nfrom pandas.io.json import json_normalize\njson_normalize(df)\n\n\nBut there seems to be some problems with keyerror. What is the correct way of doing this?\n'
"I have a dataframe like this: \n\nmainid  pidx    pidy   score\n  1      a        b      2\n  1      a        c      5\n  1      c        a      7\n  1      c        b      2\n  1      a        e      8\n  2      x        y      1\n  2      y        z      3\n  2      z        y      5\n  2      x        w      12\n  2      x        v      1\n  2      y        x      6   \n\n\nI want to groupby on column 'pidx'  and then sort score in descending order in each group i.e for each pidx\n\nand then select head(2) i.e top 2 from each group.\n\nThe result I am looking for is like this:\n\nmainid   pidx    pidy    score\n  1        a      e        8\n  1        a      c        5\n  1        c      a        7\n  1        c      b        2\n  2        x      w        12\n  2        x      y        1\n  2        y      x        6\n  2        y      z        3\n  2        z      y        5\n\n\nWhat I tried was:\n\ndf.sort(['pidx','score'],ascending = False).groupby('pidx').head(2) \n\n\nand this seems to work, but I dont know if it's the right approach if working on a huge dataset. What other best method can I use to get such result? \n"
'From this question and others it seems that it is not recommended to use concat or append to build a pandas dataframe because it is recopying the whole dataframe each time. \n\nMy project involves retrieving a small amount of data every 30 seconds. This might run for a 3 day weekend, so someone could easily expect over 8000 rows to be created one row at a time. What would be the most efficient way to add rows to this dataframe?\n'
'I followed the following procedure: In Python, how do I convert all of the items in a list to floats? because each column of my Dataframe is list, but instead of floats I chose to change all the values to strings.\n\ndf = [str(i) for i in df]\n\nBut this failed.\n\nIt simply erased all the data except for the first row of column names.\n\nThen, trying df = [str(i) for i in df.values] resulted in changing the entire Dataframe into one big list, but that messes up the data too much to be able to meet the goal of my script which is to export the Dataframe to my Oracle table. \n\nIs there a way to convert all the items that are in my Dataframe that are NOT strings into strings? \n'
'Is there a shorter way of dropping a column MultiIndex level (in my case, basic_amt) except transposing it twice?\n\nIn [704]: test\nOut[704]: \n           basic_amt               \nFaculty          NSW  QLD  VIC  All\nAll                1    1    2    4\nFull Time          0    1    0    1\nPart Time          1    0    2    3\n\nIn [705]: test.reset_index(level=0, drop=True)\nOut[705]: \n         basic_amt               \nFaculty        NSW  QLD  VIC  All\n0                1    1    2    4\n1                0    1    0    1\n2                1    0    2    3\n\nIn [711]: test.transpose().reset_index(level=0, drop=True).transpose()\nOut[711]: \nFaculty    NSW  QLD  VIC  All\nAll          1    1    2    4\nFull Time    0    1    0    1\nPart Time    1    0    2    3\n\n'
'I would like to annotate the data points with their values next to the points on the plot. The examples I found only deal with x and y as vectors. However, I would like to do this for a pandas DataFrame that contains multiple columns. \n\nax = plt.figure().add_subplot(1, 1, 1)\ndf.plot(ax = ax)\nplt.show()\n\n\nWhat is the best way to annotate all the points for a multi-column DataFrame?\n'
'Is there any way to merge on a single level of a MultiIndex without resetting the index?\n\nI have a "static" table of time-invariant values, indexed by an ObjectID, and I have a "dynamic" table of time-varying fields, indexed by ObjectID+Date. I\'d like to join these tables together.\n\nRight now, the best I can think of is:\n\ndynamic.reset_index().merge(static, left_on=[\'ObjectID\'], right_index=True)\n\n\nHowever, the dynamic table is very big, and I don\'t want to have to muck around with its index in order to combine the values.\n'
"I have a DataFrame named df as\n\n  Order Number       Status\n1         1668  Undelivered\n2        19771  Undelivered\n3    100032108  Undelivered\n4         2229    Delivered\n5        00056  Undelivered\n\n\nI would like to convert the Status column to boolean (True when Status is Delivered and False when Status is Undelivered)\nbut if Status is neither 'Undelivered' neither 'Delivered' it should be considered as NotANumber or something like that.\n\nI would like to use a dict\n\nd = {\n  'Delivered': True,\n  'Undelivered': False\n}\n\n\nso I could easily add other string which could be either considered as True or False.\n"
"I have a dataframe df, with two columns, I want to groupby one column and join the lists belongs to same group, example: \n\ncolumn_a, column_b\n1,         [1,2,3]\n1,         [2,5]\n2,         [5,6]\n\n\nafter the process: \n\ncolumn_a, column_b\n1,         [1,2,3,2,5]\n2,         [5,6]\n\n\nI want to keep all the duplicates. I have the following questions: \n\n\nThe dtypes of the dataframe are object(s). convert_objects() doesn't convert column_b to list automatically. How can I do this? \nwhat does the function in df.groupby(...).apply(lambda x: ...) apply to ? what is the form of x ? list? \nthe solution to my main problem? \n\n\nThanks in advance. \n"
"I have a df time series. I extracted the indexes and want to convert them each to datetime. How do you go about doing that? I tried to use pandas.to_datetime(x) but it doesn't convert it when I check after using type()\n"
"Consider the following situation:\n\nIn [2]: a = pd.Series([1,2,3,4,'.'])\n\nIn [3]: a\nOut[3]: \n0    1\n1    2\n2    3\n3    4\n4    .\ndtype: object\n\nIn [8]: a.astype('float64', raise_on_error = False)\nOut[8]: \n0    1\n1    2\n2    3\n3    4\n4    .\ndtype: object\n\n\nI would have expected an option that allows conversion while turning erroneous values (such as that .) to NaNs. Is there a way to achieve this?\n"
"I want to replicate rows in a Pandas Dataframe. Each row should be repeated n times, where n is a field of each row. \n\nimport pandas as pd\n\nwhat_i_have = pd.DataFrame(data={\n  'id': ['A', 'B', 'C'],\n  'n' : [  1,   2,   3],\n  'v' : [ 10,  13,   8]\n})\n\nwhat_i_want = pd.DataFrame(data={\n  'id': ['A', 'B', 'B', 'C', 'C', 'C'],\n  'v' : [ 10,  13,  13,   8,   8,   8]\n})\n\n\nIs this possible?\n"
'I\'m trying to take a dataframe and transform it into a partcular json format.\n\nHere\'s my dataframe example:\n\nDataFrame name: Stops\nid    location\n0     [50, 50]\n1     [60, 60]\n2     [70, 70]\n3     [80, 80]\n\n\nHere\'s the json format I\'d like to transform into:\n\n"stops":\n[\n{\n    "id": 1,\n    "location": [50, 50]\n},\n{\n    "id": 2,\n    "location": [60, 60]\n},\n... (and so on)\n]\n\n\nNotice it\'s a list of dicts.  I have it nearly there with the following code:\n\ndf.reset_index().to_json(orient=\'index)\n\nHowever, that line also includes the index like this:\n\n"stops":\n{\n"0":\n    {\n        "id": 0,\n        "location": [50, 50]\n    },\n"1":\n    {\n        "id": 1,\n        "location": [60, 60]\n    },\n... (and so on)\n}\n\n\nNotice this is a dict of dicts and also includes the index twice (in the first dict and as the "id" in the second dict!  Any help would be appreciated.\n'
'I have two dataframes with the same index but different columns. How do I combine them into one with the same index but containing all the columns? \n\nI have:\n\n  A \n1 10 \n2 11\n\n  B\n1 20\n2 21\n\n\nand I need the following output:\n\n  A  B\n1 10 20\n2 11 21\n\n'
'I have this DataFrame (df1) in Pandas:\n\ndf1 = pd.DataFrame(np.random.rand(10,4),columns=list(\'ABCD\'))\nprint df1\n\n       A         B         C         D\n0.860379  0.726956  0.394529  0.833217\n0.014180  0.813828  0.559891  0.339647\n0.782838  0.698993  0.551252  0.361034\n0.833370  0.982056  0.741821  0.006864\n0.855955  0.546562  0.270425  0.136006\n0.491538  0.445024  0.971603  0.690001\n0.911696  0.065338  0.796946  0.853456\n0.744923  0.545661  0.492739  0.337628\n0.576235  0.219831  0.946772  0.752403\n0.164873  0.454862  0.745890  0.437729\n\n\nI would like to check if any row (all columns) from another dataframe (df2) are present in df1. Here is df2:\n\ndf2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\nprint df2\n\n           A         B         C         D\n    0.855955  0.546562  0.270425  0.136006\n    0.491538  0.445024  0.971603  0.690001\n    0.911696  0.065338  0.796946  0.853456\n    0.744923  0.545661  0.492739  0.337628\n    0.576235  0.219831  0.946772  0.752403\n    2.000000  3.000000  4.000000  5.000000\n   14.000000 15.000000 16.000000 17.000000\n\n\nI tried using df.lookup to search for one row at a time. I did it this way:\n\nlist1 = df2.ix[0].tolist()\ncols = df1.columns.tolist()\nprint df1.lookup(list1, cols)\n\n\nbut I got this error message:\n\n  File "C:\\Users\\test.py", line 19, in &lt;module&gt;\n    print df1.lookup(list1, cols)\n  File "C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py", line 2217, in lookup\n    raise KeyError(\'One or more row labels was not found\')\nKeyError: \'One or more row labels was not found\'\n\n\nI also tried .all() using:\n\nprint (df2 == df1).all(1).any()\n\n\nbut I got this error message:\n\n  File "C:\\Users\\test.py", line 12, in &lt;module&gt;\n    print (df2 == df1).all(1).any()\n  File "C:\\python27\\lib\\site-packages\\pandas\\core\\ops.py", line 884, in f\n    return self._compare_frame(other, func, str_rep)\n  File "C:\\python27\\lib\\site-packages\\pandas\\core\\frame.py", line 3010, in _compare_frame\n    raise ValueError(\'Can only compare identically-labeled \'\nValueError: Can only compare identically-labeled DataFrame objects\n\n\nI also tried isin() like this:\n\nprint df2.isin(df1)\n\n\nbut I got False everywhere, which is not correct:\n\n    A      B      C      D\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\nFalse  False  False  False\n\n\nIs it possible to search for a set of rows in a DataFrame, by comparing it to another dataframe\'s rows?\n\nEDIT:\nIs is possible to drop df2 rows if those rows are also present in df1?\n'
"Similar to this question How to add an empty column to a dataframe?, I am interested in knowing the best way to add a column of empty lists to a DataFrame.\n\nWhat I am trying to do is basically initialize a column and as I iterate over the rows to process some of them, then add a filled list in this new column to replace the initialized value.\n\nFor example, if below is my initial DataFrame:\n\ndf = pd.DataFrame(d = {'a': [1,2,3], 'b': [5,6,7]}) # Sample DataFrame\n\n&gt;&gt;&gt; df\n   a  b\n0  1  5\n1  2  6\n2  3  7\n\n\nThen I want to ultimately end up with something like this, where each row has been processed separately (sample results shown):\n\n&gt;&gt;&gt; df\n   a  b          c\n0  1  5     [5, 6]\n1  2  6     [9, 0]\n2  3  7  [1, 2, 3]\n\n\nOf course, if I try to initialize like df['e'] = [] as I would with any other constant, it thinks I am trying to add a sequence of items with length 0, and hence fails.\n\nIf I try initializing a new column as None or NaN, I run in to the following issues when trying to assign a list to a location.\n\ndf['d'] = None\n\n&gt;&gt;&gt; df\n   a  b     d\n0  1  5  None\n1  2  6  None\n2  3  7  None\n\n\nIssue 1 (it would be perfect if I can get this approach to work! Maybe something trivial I am missing):\n\n&gt;&gt;&gt; df.loc[0,'d'] = [1,3]\n\n...\nValueError: Must have equal len keys and value when setting with an iterable\n\n\nIssue 2 (this one works, but not without a warning because it is not guaranteed to work as intended):\n\n&gt;&gt;&gt; df['d'][0] = [1,3]\n\nC:\\Python27\\Scripts\\ipython:1: SettingWithCopyWarning:\nA value is trying to be set on a copy of a slice from a DataFrame\n\n\nHence I resort to initializing with empty lists and extending them as needed. There are a couple of methods I can think of to initialize this way, but is there a more straightforward way?\n\nMethod 1:\n\ndf['empty_lists1'] = [list() for x in range(len(df.index))]\n\n&gt;&gt;&gt; df\n   a  b   empty_lists1\n0  1  5             []\n1  2  6             []\n2  3  7             []\n\n\nMethod 2:\n\n df['empty_lists2'] = df.apply(lambda x: [], axis=1)\n\n&gt;&gt;&gt; df\n   a  b   empty_lists1   empty_lists2\n0  1  5             []             []\n1  2  6             []             []\n2  3  7             []             []\n\n\nSummary of questions:\n\nIs there any minor syntax change that can be addressed in Issue 1 that can allow a list to be assigned to a None/NaN initialized field?\n\nIf not, then what is the best way to initialize a new column with empty lists?\n"
'I have the following data in pandas dataframe:\n\n    state        1st        2nd             3rd\n0   California  $11,593,820 $109,264,246    $8,496,273\n1   New York    $10,861,680 $45,336,041     $6,317,300\n2   Florida     $7,942,848  $69,369,589     $4,697,244\n3   Texas       $7,536,817  $61,830,712     $5,736,941\n\n\nI want to perform some simple analysis (e.g., sum, groupby) with three columns (1st, 2nd, 3rd), but the data type of those three columns is object (or string).\n\nSo I used the following code for data conversion:\n\ndata = data.convert_objects(convert_numeric=True)\n\n\nBut, conversion does not work, perhaps, due to the dollar sign. Any suggestion?\n'
'I\'m trying to left join multiple pandas dataframes on a single Id column, but when I attempt the merge I get warning: \n\n\n  KeyError: \'Id\'. \n\n\nI think it might be because my dataframes have offset columns resulting from a groupby statement, but I could very well be wrong. Either way I can\'t figure out how to "unstack" my dataframe column headers. None of the answers at this question seem to work.\n\nMy groupby code:\n\nstep1 = pd.DataFrame(step3.groupby([\'Id\', \'interestingtabsplittest2__grp\'])[\'applications\'].sum())\nstep1.sort(\'applications\', ascending=False).head(3)\n\n\nReturns:\n\n\n\nHow to get those offset headers into the top level?\n'
'I have a Pandas DataFrame with a column called "AXLES", which can take an integer value between 3-12. I am trying to use Seaborn\'s countplot() option to achieve the following plot:\n\n\nleft y axis shows the frequencies of these values occurring in the data. The axis extends are [0%-100%], tick marks at every 10%.\nright y axis shows the actual counts, values correspond to tick marks determined by the left y axis (marked at every 10%.)\nx axis shows the categories for the bar plots [3, 4, 5, 6, 7, 8, 9, 10, 11, 12].\nAnnotation on top of the bars show the actual percentage of that category.\n\n\nThe following code gives me the plot below, with actual counts, but I could not find a way to convert them into frequencies. I can get the frequencies using df.AXLES.value_counts()/len(df.index) but I am not sure about how to plug this information into Seaborn\'s countplot().\n\nI also found a workaround for the annotations, but I am not sure if that is the best implementation.\n\nAny help would be appreciated!\n\nThanks\n\nplt.figure(figsize=(12,8))\nax = sns.countplot(x="AXLES", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nfor p in ax.patches:\n        ax.annotate(\'%{:.1f}\'.format(p.get_height()), (p.get_x()+0.1, p.get_height()+50))\n\n\n\n\nEDIT:\n\nI got closer to what I need with the following code, using Pandas\' bar plot, ditching Seaborn. Feels like I\'m using so many workarounds, and there has to be an easier way to do it. The issues with this approach:\n\n\nThere is no order keyword in Pandas\' bar plot function as Seaborn\'s countplot() has, so I cannot plot all categories from 3-12 as I did in the countplot(). I need to have them shown even if there is no data in that category.\nThe secondary y-axis messes up the bars and the annotation for some reason (see the white gridlines drawn over the text and bars).\n\nplt.figure(figsize=(12,8))\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\nplt.ylabel(\'Frequency [%]\')\n\nax = (dfWIM.AXLES.value_counts()/len(df)*100).sort_index().plot(kind="bar", rot=0)\nax.set_yticks(np.arange(0, 110, 10))\n\nax2 = ax.twinx()\nax2.set_yticks(np.arange(0, 110, 10)*len(df)/100)\n\nfor p in ax.patches:\n    ax.annotate(\'{:.2f}%\'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))\n\n\n\n\n'
'I am new to Python and I am not sure how to solve the following problem.\n\nI have a function:\n\ndef EOQ(D,p,ck,ch):\n    Q = math.sqrt((2*D*ck)/(ch*p))\n    return Q\n\n\nSay I have the dataframe\n\ndf = pd.DataFrame({"D": [10,20,30], "p": [20, 30, 10]})\n\n    D   p\n0   10  20\n1   20  30\n2   30  10\n\nch=0.2\nck=5\n\n\nAnd ch and ck are float types. Now I want to apply the formula to every row on the dataframe and return it as an extra row \'Q\'. An example (that does not work) would be:\n\ndf[\'Q\']= map(lambda p, D: EOQ(D,p,ck,ch),df[\'p\'], df[\'D\']) \n\n\n(returns only \'map\' types)\n\nI will need this type of processing more in my project and I hope to find something that works.\n'
'I have a DataFrame that looks like this:\n\n+----------+---------+-------+\n| username | post_id | views |\n+----------+---------+-------+\n| john     |       1 |     3 |\n| john     |       2 |    23 |\n| john     |       3 |    44 |\n| john     |       4 |    82 |\n| jane     |       7 |     5 |\n| jane     |       8 |    25 |\n| jane     |       9 |    46 |\n| jane     |      10 |    56 |\n+----------+---------+-------+\n\n\nand I would like to transform it to count views that belong to certain bins like this:\n\n+------+------+-------+-------+--------+\n|      | 1-10 | 11-25 | 25-50 | 51-100 |\n+------+------+-------+-------+--------+\n| john |    1 |     1 |     1 |      1 |\n| jane |    1 |     1 |     1 |      1 |\n+------+------+-------+-------+--------+\n\n\nI tried:\n\nbins = [1, 10, 25, 50, 100]\ngroups = df.groupby(pd.cut(df.views, bins))\ngroups.username.count()\n\n\nBut it only gives aggregate counts and not counts by user. How can I get bin counts by user?\n\nThe aggregate counts (using my real data) looks like this:\n\nimpressions\n(2500, 5000]         2332\n(5000, 10000]        1118\n(10000, 50000]        570\n(50000, 10000000]      14\nName: username, dtype: int64\n\n'
"I have a pandas dataframe with over 1000 timestamps (below) that I would like to loop through:\n\n2016-02-22 14:59:44.561776\n\n\nI'm having a hard time splitting this time stamp into 2 columns- 'date' and 'time'.  The date format can stay the same, but the time needs to be converted to CST (including milliseconds).\n\nThanks for the help\n"
"I know that this kind of question was asked before and I've checked all the answers and I have tried several times to find a solution but in vain. \nIn fact I call a Dataframe using Pandas. I've uploaded a csv.file.\n\n\n\nWhen I type data.Country and data.Year, I get the 1st Column and the second one displayed. However when I type data.Number, everytime it gives me this error: \n\n\n  AttributeError: 'DataFrame' object has no attribute 'Number'.\n\n"
"I have a list of items like this:\n\nA = ['1', 'd', 'p', 'bab', '']\n\n\nMy goal is to convert such list into a dataframe of 1 row and 5 columns. If I type pd.DataFrame(A) I get 5 rows and 1 column. What should I do in order to get the result I want?\n"
"I can use .map(func) on any column in a df, like:\n\ndf=DataFrame({'a':[1,2,3,4,5,6],'b':[2,3,4,5,6,7]})\n\ndf['a']=df['a'].map(lambda x: x &gt; 1)\n\n\nI could also:\n\ndf['a'],df['b']=df['a'].map(lambda x: x &gt; 1),df['b'].map(lambda x: x &gt; 1)\n\n\nIs there a more pythonic way to apply a function to all columns or the entire frame (without a loop)?\n"
'I have a list of stockmarket data pulled from Yahoo in a pandas DataFrame (see format below). The date is serving as the index in the DataFrame. I want to write the data (including the index) out to a SQLite database. \n\n             AAPL     GE\nDate\n2009-01-02  89.95  14.76\n2009-01-05  93.75  14.38\n2009-01-06  92.20  14.58\n2009-01-07  90.21  13.93\n2009-01-08  91.88  13.95\n\n\nBased on my reading of the write_frame code for Pandas, it does not currently support writing the index. I\'ve attempted to use to_records instead, but ran into the issue with Numpy 1.6.2 and datetimes. Now I\'m trying to write tuples using .itertuples, but SQLite throws an error that the data type isn\'t supported (see code and result below). I\'m relatively new to Python, Pandas and Numpy, so it is entirely possible I\'m missing something obvious. I think I\'m running into a problem trying to write a datetime to SQLite, but I think I might be overcomplicating this. \n\nI think I may be able to fix the issue by upgrading to Numpy 1.7 or the development version of Pandas, which has a fix posted on GitHub. I\'d prefer to develop using release versions of software - I\'m new to this and I don\'t want stability issues confusing matters further. \n\nIs there a way to accomplish this using Python 2.7.2, Pandas 0.10.0, and Numpy 1.6.2? Perhaps cleaning the datetimes somehow? I\'m in a bit over my head, any help would be appreciated. \n\nCode:\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport sqlite3 as db\n\n# download data from yahoo\nall_data = {}\n\nfor ticker in [\'AAPL\', \'GE\']:\n    all_data[ticker] = pd.io.data.get_data_yahoo(ticker, \'1/1/2009\',\'12/31/2012\')\n\n# create a data frame\nprice = DataFrame({tic: data[\'Adj Close\'] for tic, data in all_data.iteritems()})\n\n# get output ready for database export\noutput = price.itertuples()\ndata = tuple(output)\n\n# connect to a test DB with one three-column table titled "Demo"\ncon = db.connect(\'c:/Python27/test.db\')\nwildcards = \',\'.join([\'?\'] * 3)\ninsert_sql = \'INSERT INTO Demo VALUES (%s)\' % wildcards\ncon.executemany(insert_sql, data)\n\n\nResult:\n\n---------------------------------------------------------------------------\nInterfaceError                            Traceback (most recent call last)\n&lt;ipython-input-15-680cc9889c56&gt; in &lt;module&gt;()\n----&gt; 1 con.executemany(insert_sql, data)\n\nInterfaceError: Error binding parameter 0 - probably unsupported type.\n\n'
"I need to remove all rows in which elements from column 3 onwards are all NaN\n\ndf = DataFrame(np.random.randn(6, 5), index=['a', 'c', 'e', 'f', 'g','h'], columns=['one', 'two', 'three', 'four', 'five'])\n\ndf2 = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf2.ix[1][0] = 111\ndf2.ix[1][1] = 222\n\n\nIn the example above, my final data frame would not be having rows 'b' and 'c'. \n\nHow to use df.dropna() in this case?\n"
'What is the closest equivalent to an R Factor variable in Python pandas?\n'
'I have a DataFrame with column named date. How can we convert/parse the \'date\' column to a DateTime object?\n\nI loaded the date column from a Postgresql database using sql.read_frame(). An example of the date column is 2013-04-04.\n\nWhat I am trying to do is to select all rows in a dataframe that has their date columns within a certain period, like after 2013-04-01 and before 2013-04-04.\n\nMy attempt below gives the error \'Series\' object has no attribute \'read\'\n\nAttempt\n\nimport dateutil\n\ndf[\'date\'] = dateutil.parser.parse(df[\'date\'])\n\n\nError\n\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-636-9b19aa5f989c&gt; in &lt;module&gt;()\n     15 \n     16 # Parse \'Date\' Column to Datetime\n---&gt; 17 df[\'date\'] = dateutil.parser.parse(df[\'date\'])\n     18 \n     19 # SELECT RECENT SALES\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(timestr, parserinfo, **kwargs)\n    695         return parser(parserinfo).parse(timestr, **kwargs)\n    696     else:\n--&gt; 697         return DEFAULTPARSER.parse(timestr, **kwargs)\n    698 \n    699 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in parse(self, timestr, default, ignoretz, tzinfos, **kwargs)\n    299             default = datetime.datetime.now().replace(hour=0, minute=0,\n    300                                                       second=0, microsecond=0)\n--&gt; 301         res = self._parse(timestr, **kwargs)\n    302         if res is None:\n    303             raise ValueError, "unknown string format"\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in _parse(self, timestr, dayfirst, yearfirst, fuzzy)\n    347             yearfirst = info.yearfirst\n    348         res = self._result()\n--&gt; 349         l = _timelex.split(timestr)\n    350         try:\n    351 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in split(cls, s)\n    141 \n    142     def split(cls, s):\n--&gt; 143         return list(cls(s))\n    144     split = classmethod(split)\n    145 \n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in next(self)\n    135 \n    136     def next(self):\n--&gt; 137         token = self.get_token()\n    138         if token is None:\n    139             raise StopIteration\n\nC:\\Python27\\lib\\site-packages\\dateutil\\parser.pyc in get_token(self)\n     66                 nextchar = self.charstack.pop(0)\n     67             else:\n---&gt; 68                 nextchar = self.instream.read(1)\n     69                 while nextchar == \'\\x00\':\n     70                     nextchar = self.instream.read(1)\n\nAttributeError: \'Series\' object has no attribute \'read\'\n\n\n\n\ndf[\'date\'].apply(dateutil.parser.parse) gives me the error AttributeError: \'datetime.date\' object has no attribute \'read\'\n\ndf[\'date\'].truncate(after=\'2013/04/01\') gives the error TypeError: can\'t compare datetime.datetime to long\n\ndf[\'date\'].dtype returns dtype(\'O\'). Is it already a datetime object?\n'
"I have DataFrame with MultiIndex columns that looks like this:\n\n# sample data\ncol = pd.MultiIndex.from_arrays([['one', 'one', 'one', 'two', 'two', 'two'],\n                                ['a', 'b', 'c', 'a', 'b', 'c']])\ndata = pd.DataFrame(np.random.randn(4, 6), columns=col)\ndata\n\n\n\n\nWhat is the proper, simple way of selecting only specific columns (e.g. ['a', 'c'], not a range) from the second level?\n\nCurrently I am doing it like this:\n\nimport itertools\ntuples = [i for i in itertools.product(['one', 'two'], ['a', 'c'])]\nnew_index = pd.MultiIndex.from_tuples(tuples)\nprint(new_index)\ndata.reindex_axis(new_index, axis=1)\n\n\n\n\nIt doesn't feel like a good solution, however, because I have to bust out itertools, build another MultiIndex by hand and then reindex (and my actual code is even messier, since the column lists aren't so simple to fetch). I am pretty sure there has to be some ix or xs way of doing this, but everything I tried resulted in errors.\n"
"I have a data frame called followers_df as below:\n\n followers_df\n\n             0\n0         oasikhia \n0     LEANEnergyUS\n0  _johannesngwako\n0     jamesbreenre\n0   CaitlinFecteau\n0  mantequillaFACE\n0         apowersb\n0       ecoprinter\n0        tsdesigns\n0      GreenBizDoc\n0        JimHarris\n0    Jmarti11Julia\n0         JAslat63\n0            prAna\n0    GrantLundberg \n0        Jitasa_Is\n0     ChoosePAWind\n0  cleanpowerperks\n0          WoWEorg\n0      Laura_Chuck\n\n\nI want to change this data frame into something like this:\n\n followers_df\n\n             0\n0          oasikhia \n1      LEANEnergyUS\n2   _johannesngwako\n3      jamesbreenre\n4    CaitlinFecteau\n5   mantequillaFACE\n6          apowersb\n7        ecoprinter\n8         tsdesigns\n9       GreenBizDoc\n10        JimHarris\n11    Jmarti11Julia\n12         JAslat63\n13            prAna\n14    GrantLundberg \n15        Jitasa_Is\n16     ChoosePAWind\n17  cleanpowerperks\n18          WoWEorg\n19      Laura_Chuck\n\n\nhow can I do this? I tried:\n\n     index = pandas.Index(range(20))\n     followers_df = pandas.DataFrame(followers_df, index=index)\n\n\nbut it's giving me the following error:\n\n  ValueError: Shape of passed values is (1, 39), indices imply (1, 20)\n\n\nthanks,\n"
"I am new to pandas. What is the best way to calculate the relative strength part in the RSI indicator in pandas? So far I got the following:\n\nfrom pylab import *\nimport pandas as pd\nimport numpy as np\n\n\n\ndef Datapull(Stock):\n    try:\n        df = (pd.io.data.DataReader(Stock,'yahoo',start='01/01/2010'))\n        return df\n        print 'Retrieved', Stock\n        time.sleep(5)\n    except Exception, e:\n        print 'Main Loop', str(e)\n\n\ndef RSIfun(price, n=14):\n    delta = price['Close'].diff()\n    #-----------\n    dUp=\n    dDown=\n\n    RolUp=pd.rolling_mean(dUp, n)\n    RolDown=pd.rolling_mean(dDown, n).abs()\n\n    RS = RolUp / RolDown\n    rsi= 100.0 - (100.0 / (1.0 + RS))\n    return rsi\n\nStock='AAPL'\ndf=Datapull(Stock)\nRSIfun(df)\n\n\nAm I doing it correctly so far? I am having trouble with the difference part of the equation where you separate out upward and downward calculations\n"
'I\'ve got a dataframe, and I\'m trying to append a column of sequential differences to it.  I have found a method that I like a lot (and generalizes well for my use case).  But I noticed one weird thing along the way.  Can you help me make sense of it?\n\nHere is some data that has the right structure (code modeled on an answer here):\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom itertools import product\n\nrandom.seed(1)       # so you can play along at home\nnp.random.seed(2)    # ditto\n\n# make a list of dates for a few periods\ndates = pd.date_range(start=\'2013-10-01\', periods=4).to_native_types()\n# make a list of tickers\ntickers = [\'ticker_%d\' % i for i in range(3)]\n# make a list of all the possible (date, ticker) tuples\npairs = list(product(dates, tickers))\n# put them in a random order\nrandom.shuffle(pairs)\n# exclude a few possible pairs\npairs = pairs[:-3]\n# make some data for all of our selected (date, ticker) tuples\nvalues = np.random.rand(len(pairs))\n\nmydates, mytickers = zip(*pairs)\ndata = pd.DataFrame({\'date\': mydates, \'ticker\': mytickers, \'value\':values})\n\n\nOk, great.  This gives me a frame like so:\n\n     date        ticker      value\n0    2013-10-03  ticker_2    0.435995\n1    2013-10-04  ticker_2    0.025926\n2    2013-10-02  ticker_1    0.549662\n3    2013-10-01  ticker_0    0.435322\n4    2013-10-02  ticker_2    0.420368\n5    2013-10-03  ticker_0    0.330335\n6    2013-10-04  ticker_1    0.204649\n7    2013-10-02  ticker_0    0.619271\n8    2013-10-01  ticker_2    0.299655\n\n\nMy goal is to add a new column to this dataframe that will contain sequential changes.  The data needs to be in order to do this, but the ordering and the differencing needs to be done "ticker-wise" so that gaps in another ticker don\'t cause NA\'s for a given ticker. I want to do this without perturbing the dataframe in any other way (i.e. I do not want the resulting DataFrame to be reordered based on what was necessary to do the differencing).  The following code works:\n\ndata1 = data.copy() #let\'s leave the original data alone for later experiments\ndata1.sort([\'ticker\', \'date\'], inplace=True)\ndata1[\'diffs\'] = data1.groupby([\'ticker\'])[\'value\'].transform(lambda x: x.diff())\ndata1.sort_index(inplace=True)\ndata1\n\n\nand returns:\n\n     date        ticker      value       diffs\n0    2013-10-03  ticker_2    0.435995    0.015627\n1    2013-10-04  ticker_2    0.025926   -0.410069\n2    2013-10-02  ticker_1    0.549662    NaN\n3    2013-10-01  ticker_0    0.435322    NaN\n4    2013-10-02  ticker_2    0.420368    0.120713\n5    2013-10-03  ticker_0    0.330335   -0.288936\n6    2013-10-04  ticker_1    0.204649   -0.345014\n7    2013-10-02  ticker_0    0.619271    0.183949\n8    2013-10-01  ticker_2    0.299655    NaN\n\n\nSo far, so good.  If I replace the middle line above with the more concise code shown here, everything still works:\n\ndata2 = data.copy()\ndata2.sort([\'ticker\', \'date\'], inplace=True)\ndata2[\'diffs\'] = data2.groupby(\'ticker\')[\'value\'].diff()\ndata2.sort_index(inplace=True)\ndata2\n\n\nA quick check shows that, in fact, data1 is equal to data2.  However, if I do this:\n\ndata3 = data.copy()\ndata3.sort([\'ticker\', \'date\'], inplace=True)\ndata3[\'diffs\'] = data3.groupby(\'ticker\')[\'value\'].transform(np.diff)\ndata3.sort_index(inplace=True)\ndata3\n\n\nI get a strange result:\n\n     date        ticker     value       diffs\n0    2013-10-03  ticker_2    0.435995    0\n1    2013-10-04  ticker_2    0.025926   NaN\n2    2013-10-02  ticker_1    0.549662   NaN\n3    2013-10-01  ticker_0    0.435322   NaN\n4    2013-10-02  ticker_2    0.420368   NaN\n5    2013-10-03  ticker_0    0.330335    0\n6    2013-10-04  ticker_1    0.204649   NaN\n7    2013-10-02  ticker_0    0.619271   NaN\n8    2013-10-01  ticker_2    0.299655    0\n\n\nWhat\'s going on here?  When you call the .diff method on a Pandas object, is it not just calling np.diff?  I know there\'s a diff method on the DataFrame class, but I couldn\'t figure out how to pass that to transform without the lambda function syntax I used to make data1 work.  Am I missing something?  Why is the diffs column in data3 screwy?  How can I have call the Pandas diff method within transform without needing to write a lambda to do it?\n'
"pandas has support for multi-level column names:\n\n&gt;&gt;&gt;  x = pd.DataFrame({'instance':['first','first','first'],'foo':['a','b','c'],'bar':rand(3)})\n&gt;&gt;&gt; x = x.set_index(['instance','foo']).transpose()\n&gt;&gt;&gt; x.columns\nMultiIndex\n[(u'first', u'a'), (u'first', u'b'), (u'first', u'c')]\n&gt;&gt;&gt; x\ninstance     first                    \nfoo              a         b         c\nbar       0.102885  0.937838  0.907467\n\n\nThis feature is very useful since it allows multiple versions of the same dataframe to be appended 'horizontally' with the 1st level of the column names (in my example instance) distinguishing the instances.\n\nImagine I already have a dataframe like this:\n\n                 a         b         c\nbar       0.102885  0.937838  0.907467\n\n\nIs there a nice way to add another level to the column names, similar to this for row index:\n\nx['instance'] = 'first'\nx.set_level('instance',append=True)\n\n"
"I'm using the apply method on a panda's DataFrame object.  When my DataFrame has a single column, it appears that the applied function is being called twice.  The questions are why? And, can I stop that behavior?\nCode:\nimport pandas as pd\n\ndef mul2(x):\n    print ('hello')\n    return 2*x\n\ndf = pd.DataFrame({'a': [1,2,0.67,1.34]})\ndf.apply(mul2)\n\nOutput:\nhello\nhello\n\n0  2.00\n1  4.00\n2  1.34\n3  2.68\n\nI'm printing 'hello' from within the function being applied.  I know it's being applied twice because 'hello' printed twice.  What's more is that if I had two columns, 'hello' prints 3 times.  Even more still is when I call applied to just the column 'hello' prints 4 times.\nCode:\ndf.a.apply(mul2)\n\nOutput:\nhello\nhello\nhello\nhello\n0    2.00\n1    4.00\n2    1.34\n3    2.68\nName: a, dtype: float64\n\n"
'What is the method to convert a Python list of strings to a pd.Series object?\n\n(pandas Series objects can be converted to list using tolist() method--but how to do the reverse conversion?)\n'
'wowee.....how to use iterrows with python and pandas?  If I do a row iteration should I not be able to access a col with row[\'COL_NAME\']?\n\nHere are the col names: \n\nprint df\nInt64Index: 152 entries, 0 to 151\nData columns:\nDate          152  non-null values\nTime          152  non-null values\nTime Zone     152  non-null values\nCurrency      152  non-null values\nEvent         152  non-null values\nImportance    152  non-null values\nActual        127  non-null values\nForecast      86  non-null values\nPrevious      132  non-null values\ndtypes: object(9)\n\nfor row in df.iterrows():\n    print row[\'Date\']\n\nTraceback (most recent call last):\n  File "/home/ubuntu/workspace/calandar.py", line 34, in &lt;module&gt;\n    print row[\'Date\']\nTypeError: tuple indices must be integers, not str\n\n\nif I print 1 row:\n\n(0, Date                                                 Sun Apr 13\nTime                                                      17:30\nTime Zone                                                   GMT\nCurrency                                                    USD\nEvent         USD Fed\'s Stein Speaks on Financial Stability ...\nImportance                                                  Low\nActual                                                      NaN\nForecast                                                    NaN\nPrevious                                                    NaN\nName: 0)\n\n'
"I'm trying to read in a somewhat large dataset using pandas read_csv or read_stata functions, but I keep running into Memory Errors. What is the maximum size of a dataframe? My understanding is that dataframes should be okay as long as the data fits into memory, which shouldn't be a problem for me. What else could cause the memory error?\n\nFor context, I'm trying to read in the Survey of Consumer Finances 2007, both in ASCII format (using read_csv) and in Stata format (using read_stata). The file is around 200MB as dta and around 1.2GB as ASCII, and opening it in Stata tells me that there are 5,800 variables/columns for 22,000 observations/rows. \n"
"I have a table in csv format that looks like this.  I would like to transpose the table so that the values in the indicator name column are the new columns,\n\nIndicator       Country         Year   Value    \n1               Angola          2005    6\n2               Angola          2005    13\n3               Angola          2005    10\n4               Angola          2005    11\n5               Angola          2005    5\n1               Angola          2006    3\n2               Angola          2006    2\n3               Angola          2006    7\n4               Angola          2006    3\n5               Angola          2006    6\n\n\nI would like the end result to like like this:\n\nCountry    Year     1     2     3     4     5\nAngola     2005     6     13    10    11    5\nAngola     2006     3     2     7     3     6\n\n\nI have tried using a pandas data frame with not much success.\n\nprint(df.pivot(columns = 'Country', 'Year', 'Indicator', values = 'Value'))\n\n\nAny thoughts on how to accomplish this?\n\nThanks \n"
"I have a pandas series \n\nobject x\nEzh2   2\nHmgb   7\nIrf1   1\n\n\nI want to save this as a dataframe with column names Gene and Count respectively\nI tried \n\nx_df = pd.DataFrame(x,columns = ['Gene','count'])\n\n\nbut it does not work.The final form I want is\n\nGene Count\nEzh2   2\nHmgb   7\nIrf1   1\n\n\nCan you suggest how to do this\n"
"I have a 14MB Excel file with five worksheets that I'm reading into a Pandas dataframe, and although the code below works, it takes 9 minutes!\n\nDoes anyone have suggestions for speeding it up?\n\nimport pandas as pd\n\ndef OTT_read(xl,site_name):\n    df = pd.read_excel(xl.io,site_name,skiprows=2,parse_dates=0,index_col=0,\n                       usecols=[0,1,2],header=None,\n                       names=['date_time','%s_depth'%site_name,'%s_temp'%site_name])\n    return df\n\ndef make_OTT_df(FILEDIR,OTT_FILE):\n    xl = pd.ExcelFile(FILEDIR + OTT_FILE)\n    site_names = xl.sheet_names\n    df_list = [OTT_read(xl,site_name) for site_name in site_names]\n    return site_names,df_list\n\nFILEDIR='c:/downloads/'\nOTT_FILE='OTT_Data_All_stations.xlsx'\nsite_names_OTT,df_list_OTT = make_OTT_df(FILEDIR,OTT_FILE)\n\n"
"I'm trying to make a table, and the way Pandas formats its indices is exactly what I'm looking for. That said, I don't want the actual data, and I can't figure out how to get Pandas to print out just the indices without the corresponding data.\n"
'I\'ve read the docs about slicers a million times, but have never got my head round it, so I\'m still trying to figure out how to use loc to slice a DataFrame with a MultiIndex.\n\nI\'ll start with the DataFrame from this SO answer:\n\n                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n             C2    D0          6\n                   D1          7\n      B1     C1    D0         10\n                   D1         11\n             C2    D0         14\n                   D1         15\nA1    B0     C1    D0         18\n                   D1         19\n             C2    D0         22\n                   D1         23\n      B1     C1    D0         26\n                   D1         27\n             C2    D0         30\n                   D1         31\nA2    B0     C1    D0         34\n                   D1         35\n             C2    D0         38\n                   D1         39\n      B1     C1    D0         42\n                   D1         43\n             C2    D0         46\n                   D1         47\nA3    B0     C1    D0         50\n                   D1         51\n             C2    D0         54\n                   D1         55\n      B1     C1    D0         58\n                   D1         59\n             C2    D0         62\n                   D1         63\n\n\nTo select just A0 and C1 values, I can do:\n\nIn [26]: df.loc[\'A0\', :, \'C1\', :]\nOut[26]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D0          2\n                   D1          3\n      B1     C1    D0         10\n                   D1         11\n\n\nWhich also works selecting from three levels, and even with tuples:\n\nIn [28]: df.loc[\'A0\', :, (\'C1\', \'C2\'), \'D1\']\nOut[28]: \n                           value\nfirst second third fourth       \nA0    B0     C1    D1          3\n             C2    D1          5\n      B1     C1    D1         11\n             C2    D1         13\n\n\nSo far, intuitive and brilliant.\n\nSo why can\'t I select all values from the first index level?\n\nIn [30]: df.loc[:, :, \'C1\', :]\n---------------------------------------------------------------------------\nIndexingError                             Traceback (most recent call last)\n&lt;ipython-input-30-57b56108d941&gt; in &lt;module&gt;()\n----&gt; 1 df.loc[:, :, \'C1\', :]\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in __getitem__(self, key)\n   1176     def __getitem__(self, key):\n   1177         if type(key) is tuple:\n-&gt; 1178             return self._getitem_tuple(key)\n   1179         else:\n   1180             return self._getitem_axis(key, axis=0)\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _getitem_tuple(self, tup)\n    694 \n    695         # no multi-index, so validate all of the indexers\n--&gt; 696         self._has_valid_tuple(tup)\n    697 \n    698         # ugly hack for GH #836\n\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc in _has_valid_tuple(self, key)\n    125         for i, k in enumerate(key):\n    126             if i &gt;= self.obj.ndim:\n--&gt; 127                 raise IndexingError(\'Too many indexers\')\n    128             if not self._has_valid_type(k, i):\n    129                 raise ValueError("Location based indexing can only have [%s] "\n\nIndexingError: Too many indexers\n\n\nSurely this is not intended behaviour?\n\nNote: I know this is possible with df.xs(\'C1\', level=\'third\') but the current .loc behaviour seems inconsistent.\n'
"I would like to create a MySQL table with Pandas' to_sql function which has a primary key (it is usually kind of good to have a primary key in a mysql table) as so:\n\ngroup_export.to_sql(con = db, name = config.table_group_export, if_exists = 'replace', flavor = 'mysql', index = False)\n\n\nbut this creates a table without any primary key, (or even without any index).\n\nThe documentation mentions the parameter 'index_label' which combined with the 'index' parameter could be used to create an index but doesn't mention any option for primary keys.\n\nDocumentation\n"
"I have the following problem: I have two pandas data frames of different length containing some rows and columns that have common values and some that are different, like this:\n\ndf1:                                 df2:\n\n      Column1  Column2  Column3           ColumnA  ColumnB ColumnC\n    0    a        x        x            0    c        y       y\n    1    c        x        x            1    e        z       z\n    2    e        x        x            2    a        s       s\n    3    d        x        x            3    d        f       f\n    4    h        x        x\n    5    k        x        x            \n\n\nWhat I want to do now is merging the two dataframes so that if ColumnA and Column1 have the same value the rows from df2 are appended to the corresponding row in df1, like this:\n\ndf1:\n    Column1  Column2  Column3  ColumnB  ColumnC\n  0    a        x        x        s        s\n  1    c        x        x        y        y\n  2    e        x        x        z        z\n  3    d        x        x        f        f\n  4    h        x        x        NaN      NaN\n  5    k        x        x        NaN      NaN\n\n\nI know that the merge is doable through\n\ndf1.merge(df2,left_on='Column1', right_on='ColumnA')\n\n\nbut this command drops all rows that are not the same in Column1 and ColumnA in both files. Instead of that I want to keep these rows in df1 and just assign NaN to them in the columns where other rows have a value from df2, as shown above. Is there a smooth way to do this in pandas?\n\nThanks in advance!\n"
'Is there anyway to hide E1101 errors for objects that are created from a specific library?  Our large repository is littered with #pylint: disable=E1101 around various objects created by pandas.\n\nFor example, pylint will throw a no member error on the following code:\n\nimport pandas.io.data\nimport pandas as pd\nspy = pandas.io.data.DataReader("SPY", "yahoo")\nspy.to_csv("test.csv")\nspy = pd.read_csv("test.csv")\nclose_px = spy.ix["2012":]\n\n\nWill have the following errors:\n\nE:  6,11: Instance of \'tuple\' has no \'ix\' member (no-member)\nE:  6,11: Instance of \'TextFileReader\' has no \'ix\' member (no-member)\n\n'
"I'm trying to read in a CSV file into a pandas dataframe and select a column, but keep getting a key error.\n\nThe file reads in successfully and I can view the dataframe in an iPython notebook, but when I want to select a column any other than the first one, it throws a key error.\n\nI am using this code:\n\nimport pandas as pd\n\ntransactions = pd.read_csv('transactions.csv',low_memory=False, delimiter=',', header=0, encoding='ascii')\ntransactions['quarter']\n\n\nThis is the file I'm working on:\nhttps://www.dropbox.com/s/81iwm4f2hsohsq3/transactions.csv?dl=0\n\nThank you! \n"
"I'm calculating the Autocorrelation Function for a stock's returns. To do so I tested two functions, the autocorr function built into Pandas, and the acf function supplied by statsmodels.tsa. This is done in the following MWE:\n\nimport pandas as pd\nfrom pandas_datareader import data\nimport matplotlib.pyplot as plt\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nfrom statsmodels.tsa.stattools import acf, pacf\n\nticker = 'AAPL'\ntime_ago = datetime.datetime.today().date() - relativedelta(months = 6)\n\nticker_data = data.get_data_yahoo(ticker, time_ago)['Adj Close'].pct_change().dropna()\nticker_data_len = len(ticker_data)\n\nticker_data_acf_1 =  acf(ticker_data)[1:32]\nticker_data_acf_2 = [ticker_data.autocorr(i) for i in range(1,32)]\n\ntest_df = pd.DataFrame([ticker_data_acf_1, ticker_data_acf_2]).T\ntest_df.columns = ['Pandas Autocorr', 'Statsmodels Autocorr']\ntest_df.index += 1\ntest_df.plot(kind='bar')\n\n\nWhat I noticed was the values they predicted weren't identical:\n\n\n\nWhat accounts for this difference, and which values should be used?\n"
"I have two dataframes , the first one has 1000 rows and looks like:\n\nDate            Group         Family       Bonus\n2011-06-09      tri23_1       Laavin       456\n2011-07-09      hsgç_T2       Grendy       679\n2011-09-10      bbbj-1Y_jn    Fantol       431\n2011-11-02      hsgç_T2       Gondow       569\n\n\nThe column Group has different values, sometimes repeated, but in general about 50 unique values.\n\nThe second dataframe contains all these 50 unique values (50 rows) and also the hotels, that are associated to these values:\n\nGroup             Hotel\ntri23_1           Jamel\nhsgç_T2           Frank\nbbbj-1Y_jn        Luxy\nmlkl_781          Grand Hotel\nvchs_94           Vancouver\n\n\nMy goal is to replace the value in the column Group of the first dataframe by the the corresponding values of the column Hotel of the second dataframe/or create the column Hotel with the corresponding values. When I try to make it just by assignment like \n\ndf1.loc[(df1.Group=df2.Group), 'Hotel']=df2.Hotel\n\n\nI have an error that the dataframes are not of equal size, so the comparison is not possible\n"
"I want to go from this data frame which is basically one hot encoded.\n In [2]: pd.DataFrame({&quot;monkey&quot;:[0,1,0],&quot;rabbit&quot;:[1,0,0],&quot;fox&quot;:[0,0,1]})\n\n    Out[2]:\n       fox  monkey  rabbit\n    0    0       0       1\n    1    0       1       0\n    2    1       0       0\n    3    0       0       0\n    4    0       0       0\n\nTo this one which is 'reverse' one-hot encoded.\n    In [3]: pd.DataFrame({&quot;animal&quot;:[&quot;monkey&quot;,&quot;rabbit&quot;,&quot;fox&quot;]})\n    Out[3]:\n       animal\n    0  monkey\n    1  rabbit\n    2     fox\n\nI imagine there's some sort of clever use of apply or zip to do thins but I'm not sure how... Can anyone help?\nI've not had much success using indexing etc to try to solve this problem.\n"
"I have a dataframe, which contains info about movies. It has a column called genre, which contains a list of genres it belongs to. For example:\n\ndf['genre']\n\n## returns \n\n0       ['comedy', 'sci-fi']\n1       ['action', 'romance', 'comedy']\n2       ['documentary']\n3       ['crime','horror']\n...\n\n\nI want to know how can I query the dataframe, so it returns the movie belongs to a cerain genre?\n\nFor example, something may like df['genre'].contains('comedy') returns 0 or 1.\n\nI know for a list, I can do things like:\n\n'comedy' in  ['comedy', 'sci-fi']\n\n\nHowever, in pandas, I didn't find something similar, the only thing I know is df['genre'].str.contains(), but it didn't work for the list type.\n"
"I have a large dataframe which looks as:\n\ndf1['A'].ix[1:3]\n2017-01-01 02:00:00    [33, 34, 39]\n2017-01-01 03:00:00    [3, 43, 9]\n\n\nI want to replace each element greater than 9 with 11.\n\nSo, the desired output for above example is:\n\ndf1['A'].ix[1:3]\n2017-01-01 02:00:00    [11, 11, 11]\n2017-01-01 03:00:00    [3, 11, 9]\n\n\nEdit:\n\nMy actual dataframe has about 20,000 rows and each row has list of size 2000.\n\nIs there a way to use numpy.minimum function for each row? I assume that it will be faster than list comprehension method? \n"
'I got good use out of pandas\' MovingOLS class (source here) within the deprecated stats/ols module.  Unfortunately, it was gutted completely with pandas 0.20.\n\nThe question of how to run rolling OLS regression in an efficient manner has been asked several times (here, for instance), but phrased a little broadly and left without a great answer, in my view.\n\nHere are my questions:  \n\n\nHow can I best mimic the basic framework of pandas\' MovingOLS?  The most attractive feature of this class was the ability to view multiple methods/attributes as separate time series--i.e. coefficients, r-squared, t-statistics, etc without needing to re-run regression.  For example, you could create something like model = pd.MovingOLS(y, x) and then call .t_stat, .rmse, .std_err, and the like.  In the example below, conversely, I don\'t see a way around being forced to compute each statistic separately.  Is there a method that doesn\'t involve creating sliding/rolling "blocks" (strides) and running regressions/using linear algebra to get model parameters for each?  \nMore broadly, what\'s going on under the hood in pandas that makes rolling.apply not able to take more complex functions?*  When you create a .rolling object, in layman\'s terms, what\'s going on internally--is it fundamentally different from looping over each window and creating a higher-dimensional array as I\'m doing below?\n\n\n*Namely, func passed to .apply:\n\n\n  Must produce a single value from an ndarray input *args and **kwargs\n  are passed to the function\n\n\nHere\'s where I\'m currently at with some sample data, regressing percentage changes in the trade weighted dollar on interest rate spreads and the price of copper.  (This doesn\'t make a ton of sense; just picked these randomly.)  I\'ve taken it out of a class-based implementation and tried to strip it down to a simpler script.\n\nfrom datetime import date\nfrom pandas_datareader.data import DataReader\nimport statsmodels.formula.api as smf\n\nsyms = {\'TWEXBMTH\' : \'usd\', \n        \'T10Y2YM\' : \'term_spread\', \n        \'PCOPPUSDM\' : \'copper\'\n       }\n\nstart = date(2000, 1, 1)\ndata = (DataReader(syms.keys(), \'fred\', start)\n        .pct_change()\n        .dropna())\ndata = data.rename(columns = syms)\ndata = data.assign(intercept = 1.) # required by statsmodels OLS\n\ndef sliding_windows(x, window):\n    """Create rolling/sliding windows of length ~window~.\n\n    Given an array of shape (y, z), it will return "blocks" of shape\n    (x - window + 1, window, z)."""\n\n    return np.array([x[i:i + window] for i \n                    in range(0, x.shape[0] - window + 1)])\n\ndata.head(3)\nOut[33]: \n                 usd  term_spread    copper  intercept\nDATE                                                  \n2000-02-01  0.012573    -1.409091 -0.019972        1.0\n2000-03-01 -0.000079     2.000000 -0.037202        1.0\n2000-04-01  0.005642     0.518519 -0.033275        1.0\n\nwindow = 36\nwins = sliding_windows(data.values, window=window)\ny, x = wins[:, :, 0], wins[:, :, 1:]\n\ncoefs = []\n\nfor endog, exog in zip(y, x):\n    model = smf.OLS(endog, exog).fit()\n        # The full set of model attributes gets lost with each loop\n    coefs.append(model.params)\n\ndf = pd.DataFrame(coefs, columns=data.iloc[:, 1:].columns,\n                  index=data.index[window - 1:])\n\ndf.head(3) # rolling 36m coefficients\nOut[70]: \n            term_spread    copper  intercept\nDATE                                        \n2003-01-01    -0.000122 -0.018426   0.001937\n2003-02-01     0.000391 -0.015740   0.001597\n2003-03-01     0.000655 -0.016811   0.001546\n\n'
'Pandas documentation lists a bunch of "expanding window functions" :\n\nhttp://pandas.pydata.org/pandas-docs/version/0.17.0/api.html#standard-expanding-window-functions\n\nBut I couldn\'t figure out what they do from the documentation. \n'
'I have the following dataframe:\n\n   key1  key2\n0    a   one\n1    a   two\n2    b   one\n3    b   two\n4    a   one\n5    c   two\n\n\nNow, I want to group the dataframe by the key1 and count the column key2 with the value "one" to get this result:\n\n   key1  \n0    a   2\n1    b   1\n2    c   0\n\n\n\n\nI just get the usual count with:\n\ndf.groupby([\'key1\']).size()\n\n\nBut I don\'t know how to insert the condition.\n\nI tried things like this:\n\ndf.groupby([\'key1\']).apply(df[df[\'key2\'] == \'one\'])\n\n\nBut I can\'t get any further.  How can I do this?\n'
'This should be an easy one, but somehow I couldn\'t find a solution that works.\n\nI have a pandas dataframe which looks like this:\n\nindex col1   col2   col3   col4   col5\n0     a      c      1      2      f \n1     a      c      1      2      f\n2     a      d      1      2      f\n3     b      d      1      2      g\n4     b      e      1      2      g\n5     b      e      1      2      g\n\n\nI want to group by col1 and col2 and get the sum() of col3 and col4. Col5 can be dropped, since the data can not be aggregated.\n\nHere is how the output should look like. I am interested in having both col3 and col4 in the resulting dataframe. It doesn\'t really matter if col1 and col2 are part of the index or not.\n\nindex col1   col2   col3   col4   \n0     a      c      2      4          \n1     a      d      1      2      \n2     b      d      1      2      \n3     b      e      2      4      \n\n\nHere is what I tried:\n\ndf_new = df.groupby([\'col1\', \'col2\'])["col3", "col4"].sum()\n\n\nThat however only returns the aggregated results of col4.\n\nI am lost here. Every example I found only aggregates one column, where the issue obviously doesn\'t occur.\n'
'I\'m trying to figure out how to add 3 months to a date in a Pandas dataframe, while keeping it in the date format, so I can use it to lookup a range.\n\nThis is what I\'ve tried:\n\n#create dataframe\ndf = pd.DataFrame([pd.Timestamp(\'20161011\'),\n                   pd.Timestamp(\'20161101\') ], columns=[\'date\'])\n\n#create a future month period\nplus_month_period = 3\n\n#calculate date + future period\ndf[\'future_date\'] = plus_month_period.astype("timedelta64[M]")\n\n\nHowever, I get the following error:\n\nAttributeError: \'int\' object has no attribute \'astype\'\n\n'
'I have a table like below:\n\n      URN                   Firm_Name\n0  104472               R.X. Yah &amp; Co\n1  104873        Big Building Society\n2  109986          St James\'s Society\n3  114058  The Kensington Society Ltd\n4  113438      MMV Oil Associates Ltd\n\n\nAnd I want to count the frequency of all the words within the Firm_Name column, to get an output like below:\n\n\n\nI have tried the following code:\n\nimport pandas as pd\nimport nltk\ndata = pd.read_csv("X:\\Firm_Data.csv")\ntop_N = 20\nword_dist = nltk.FreqDist(data[\'Firm_Name\'])\nprint(\'All frequencies\')\nprint(\'=\'*60)\nrslt=pd.DataFrame(word_dist.most_common(top_N),columns=[\'Word\',\'Frequency\'])\n\nprint(rslt)\nprint (\'=\'*60)\n\n\nHowever the following code does not produce a unique word count. \n'
"I feel I am probably not thinking of something obvious. I want to put in the same figure, the box plot of every column of a dataframe, where on the x-axis I have the columns' names. In the seaborn.boxplot() this would be equal to groupby by every column. \n\nIn pandas I would do \n\ndf = pd.DataFrame(data = np.random.random(size=(4,4)), columns = ['A','B','C','D'])\ndf.boxplot()\n\n\nwhich yields\n\n\n\nNow I would like to get the same thing in seaborn. But when I try sns.boxplot(df), I get only one grouped boxplot. How do I reproduce the same figure in seaborn? \n"
"I am working with survey data loaded from an h5-file as hdf = pandas.HDFStore('Survey.h5') through the pandas package. Within this DataFrame, all rows are the results of a single survey, whereas the columns are the answers for all questions within a single survey. \n\nI am aiming to reduce this dataset to a smaller DataFrame including only the rows with a certain depicted answer on a certain question, i.e. with all the same value in this column. I am able to determine the index values of all rows with this condition, but I can't find how to delete this rows or make a new df with these rows only.\n"
"I'm suspicious that this is trivial, but I yet to discover the incantation that will let me select rows from a Pandas dataframe based on the values of a hierarchical key. So, for example, imagine we have the following dataframe:\n\nimport pandas\ndf = pandas.DataFrame({'group1': ['a','a','a','b','b','b'],\n                       'group2': ['c','c','d','d','d','e'],\n                       'value1': [1.1,2,3,4,5,6],\n                       'value2': [7.1,8,9,10,11,12]\n})\ndf = df.set_index(['group1', 'group2'])\n\n\ndf looks as we would expect: \n\n\n\nIf df were not indexed on group1 I could do the following:\n\ndf['group1' == 'a']\n\n\nBut that fails on this dataframe with an index. So maybe I should think of this like a Pandas series with a hierarchical index:\n\ndf['a','c']\n\n\nNope. That fails as well. \n\nSo how do I select out all the rows where:\n\n\ngroup1 == 'a'\ngroup1 == 'a' &amp; group2 == 'c'\ngroup2 == 'c'\ngroup1 in ['a','b','c']\n\n"
'I want to mark some quantiles in my data, and for each row of the DataFrame, I would like the entry in a new column called e.g. "xtile" to hold this value.\n\nFor example, suppose I create a data frame like this:\n\nimport pandas, numpy as np\ndfrm = pandas.DataFrame({\'A\':np.random.rand(100), \n                         \'B\':(50+np.random.randn(100)), \n                         \'C\':np.random.randint(low=0, high=3, size=(100,))})\n\n\nAnd let\'s say I write my own function to compute the quintile of each element in an array. I have my own function for this, but for example just refer to scipy.stats.mstats.mquantile.\n\nimport scipy.stats as st\ndef mark_quintiles(x, breakpoints):\n    # Assume this is filled in, using st.mstats.mquantiles.\n    # This returns an array the same shape as x, with an integer for which\n    # breakpoint-bucket that entry of x falls into.\n\n\nNow, the real question is how to use transform to add a new column to the data. Something like this:\n\ndef transformXtiles(dataFrame, inputColumnName, newColumnName, breaks):\n    dataFrame[newColumnName] = mark_quintiles(dataFrame[inputColumnName].values, \n                                              breaks)\n    return dataFrame\n\n\nAnd then:\n\ndfrm.groupby("C").transform(lambda x: transformXtiles(x, "A", "A_xtile", [0.2, 0.4, 0.6, 0.8, 1.0]))\n\n\nThe problem is that the above code will not add the new column "A_xtile". It just returns my data frame unchanged. If I first add a column full of dummy values, like NaN, called "A_xtile", then it does successfully over-write this column to include the correct quintile markings.\n\nBut it is extremely inconvenient to have to first write in the column for anything like this that I may want to add on the fly.\n\nNote that a simple apply will not work here, since it won\'t know how to make sense of the possibly differently-sized result arrays for each group.\n'
"Consider the following example:\nPrepare the data:\nimport string\nimport random\nimport pandas as pd\n\nmatrix = np.random.random((100, 3000))\nmy_cols = [random.choice(string.ascii_uppercase) for x in range(matrix.shape[1])]\nmydf = pd.DataFrame(matrix, columns=my_cols)\nmydf['something'] = 'hello_world'\n\nSet the highest compression possible for HDF5:\nstore = pd.HDFStore('myfile.h5',complevel=9, complib='bzip2')\nstore['mydf'] = mydf\nstore.close()\n\nSave also to CSV:\nmydf.to_csv('myfile.csv', sep=':')\n\nThe result is:\n\nmyfile.csv is 5.6 MB big\nmyfile.h5 is 11 MB big\n\nThe difference grows bigger as the datasets get larger.\nI have tried with other compression methods and levels. Is this a bug? (I am using Pandas 0.11 and the latest stable version of HDF5 and Python).\n"
"Using sample data:\n\ndf = pd.DataFrame({'key1' : ['a','a','b','b','a'],\n               'key2' : ['one', 'two', 'one', 'two', 'one'],\n               'data1' : np.random.randn(5),\n               'data2' : np. random.randn(5)})\n\n\ndf\n\n    data1        data2     key1  key2\n0    0.361601    0.375297    a   one\n1    0.069889    0.809772    a   two\n2    1.468194    0.272929    b   one\n3   -1.138458    0.865060    b   two\n4   -0.268210    1.250340    a   one\n\n\nI'm trying to figure out how to group the data by key1 and sum only the data1 values where key2 equals 'one'.\n\nHere's what I've tried\n\ndef f(d,a,b):\n    d.ix[d[a] == b, 'data1'].sum()\n\ndf.groupby(['key1']).apply(f, a = 'key2', b = 'one').reset_index()\n\n\nBut this gives me a dataframe with 'None' values \n\nindex   key1    0\n0       a       None\n1       b       None\n\n\nAny ideas here?  I'm looking for the Pandas equivalent of the following SQL:\n\nSELECT Key1, SUM(CASE WHEN Key2 = 'one' then data1 else 0 end)\nFROM df\nGROUP BY key1\n\n\nFYI - I've seen conditional sums for pandas aggregate  but couldn't transform the answer provided there to work with sums rather than counts.\n\nThanks in advance\n"
"Say I create a pandas DataFrame with two columns, b (a DateTime) and c (an integer). Now I want to make a DatetimeIndex from the values in the first column (b):\n\nimport pandas as pd\nimport datetime as dt\n\na=[1371215423523845, 1371215500149460, 1371215500273673, 1371215500296504, 1371215515568529, 1371215531603530, 1371215576463339, 1371215579939113, 1371215731215054, 1371215756231343, 1371215756417484, 1371215756519690, 1371215756551645, 1371215756578979, 1371215770164647, 1371215820891387, 1371215821305584, 1371215824925723, 1371215878061146, 1371215878173401, 1371215890324572, 1371215898024253, 1371215926634930, 1371215933513122, 1371216018210826, 1371216080844727, 1371216080930036, 1371216098471787, 1371216111858392, 1371216326271516, 1371216326357836, 1371216445401635, 1371216445401635, 1371216481057049, 1371216496791894, 1371216514691786, 1371216540337354, 1371216592180666, 1371216592339578, 1371216605823474, 1371216610332627, 1371216623042903, 1371216624749566, 1371216630631179, 1371216654267672, 1371216714011662, 1371216783761738, 1371216783858402, 1371216783858402, 1371216783899118, 1371216976339169, 1371216976589850, 1371217028278777, 1371217028560770, 1371217170996479, 1371217176184425, 1371217176318245, 1371217190349372, 1371217190394753, 1371217272797618, 1371217340235667, 1371217340358197, 1371217340433146, 1371217340463797, 1371217340490876, 1371217363797722, 1371217363797722, 1371217363890678, 1371217363922929, 1371217523548405, 1371217523548405, 1371217551181926, 1371217551181926, 1371217551262975, 1371217652579855, 1371218091071955, 1371218295006690, 1371218370005139, 1371218370133637, 1371218370133637, 1371218370158096, 1371218370262823, 1371218414896836, 1371218415013417, 1371218415050485, 1371218415050485, 1371218504396524, 1371218504396524, 1371218504481537, 1371218504517462, 1371218586980079, 1371218719953887, 1371218720621245, 1371218738776732, 1371218937926310, 1371218954785466, 1371218985347070, 1371218985421615, 1371219039790991, 1371219171650043]\nb=[dt.datetime.fromtimestamp(t/1000000.) for t in a]\nc = {'b':b, 'c':a[:]}\n\n\ndf = pd.DataFrame(c)\ndf.set_index(pd.DatetimeIndex(df['b']))\nprint df\n\n\nEverything seems to work fine, except that when I print the DataFrame, it says that it has an Int64Index.\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 100 entries, 0 to 99\nData columns (total 2 columns):\nb    100  non-null values\nc    100  non-null values\ndtypes: datetime64[ns](1), int64(1)\n\n\nAm I doing something wrong or do I not understand the concept of Indeces properly?\n"
'I am trying to read in a csv file with numpy.genfromtxt but some of the fields are strings which contain commas.  The strings are in quotes, but numpy is not recognizing the quotes as defining a single string.  For example, with the data in \'t.csv\':\n\n2012, "Louisville KY", 3.5\n2011, "Lexington, KY", 4.0\n\n\nthe code\n\nnp.genfromtxt(\'t.csv\', delimiter=\',\')\n\n\nproduces the error:\n\n\n  ValueError: Some errors were detected !\n      Line #2 (got 4 columns instead of 3)\n\n\nThe data structure I am looking for is:\n\narray([[\'2012\', \'Louisville KY\', \'3.5\'],\n       [\'2011\', \'Lexington, KY\', \'4.0\']], \n      dtype=\'|S13\')\n\n\nLooking over the documentation, I don\'t see any options to deal with this.  Is there a way do to it with numpy, or do I just need to read in the data with the csv module and then convert it to a numpy array?\n'
"I am looking for for a pythonic way to handle the following problem.\n\nThe pandas.get_dummies() method is great to create dummies from a categorical column of a dataframe. For example, if the column has values in ['A', 'B'], get_dummies() creates 2 dummy variables and assigns 0 or 1 accordingly.\n\nNow, I need to handle this situation. A single column, let's call it 'label', has values like ['A', 'B', 'C', 'D', 'A*C', 'C*D'] . get_dummies() creates 6 dummies, but I only want 4 of them, so that a row could have multiple 1s. \n\nIs there a way to handle this in a pythonic way? I could only think of some step-by-step algorithm to get it, but that would not include get_dummies(). \nThanks\n\nEdited, hope it is more clear!\n"
"Is it possible to hide the index when displaying pandas dataframes, so that only the column names appear at the top of the table?\n\nThis would need to work for both the html representation in ipython notebook and to_latex() function (which I'm using with nbconvert).\n\nTa.\n"
"I am still new to Python pandas' pivot_table and would like to ask a way to count frequencies of values in one column, which is also linked to another column of ID. The DataFrame looks like the following.\n\nimport pandas as pd\ndf = pd.DataFrame({'Account_number':[1,1,2,2,2,3,3],\n                   'Product':['A', 'A', 'A', 'B', 'B','A', 'B']\n                  })\n\n\nFor the output, I'd like to get something like the following:\n\n                Product\n                A      B\nAccount_number           \n      1         2      0\n      2         1      2\n      3         1      1\n\n\nSo far, I tried this code:\n\ndf.pivot_table(rows = 'Account_number', cols= 'Product', aggfunc='count')\n\n\nThis code gives me the two same things. What is the problems with the code above? A part of the reason why I am asking this question is that this DataFrame is just an example. The real data that I am working on has tens of thousands of account_numbers. Thanks for your help in advance!\n"
"I'm trying to write a function to aggregate and perform various stats calcuations on a dataframe in Pandas and then merge it to the original dataframe however, I'm running to issues.  This is code equivalent in SQL:\n\nSELECT EID,\n       PCODE,\n       SUM(PVALUE) AS PVALUE,\n       SUM(SQRT(SC*EXP(SC-1))) AS SC,\n       SUM(SI) AS SI,\n       SUM(EE) AS EE\nINTO foo_bar_grp\nFROM foo_bar\nGROUP BY EID, PCODE \n\n\nAnd then join on the original table:\n\nSELECT *\nFROM foo_bar_grp INNER JOIN \nfoo_bar ON foo_bar.EID = foo_bar_grp.EID \n        AND foo_bar.PCODE = foo_bar_grp.PCODE\n\n\nHere are the steps:  Loading the data\nIN:>>\n\npol_dict = {'PID':[1,1,2,2],\n             'EID':[123,123,123,123],\n             'PCODE':['GU','GR','GU','GR'],\n             'PVALUE':[100,50,150,300],\n             'SI':[400,40,140,140],\n             'SC':[230,23,213,213],\n             'EE':[10000,10000,2000,30000],\n             }\n\n\npol_df = DataFrame(pol_dict)\n\npol_df\n\n\nOUT:>>\n\n   EID    EE PCODE  PID  PVALUE   SC   SI\n0  123  10000    GU    1     100  230  400\n1  123  10000    GR    1      50   23   40\n2  123   2000    GU    2     150  213  140\n3  123  30000    GR    2     300  213  140\n\n\nStep 2:  Calculating and Grouping on the data:\n\nMy pandas code is as follows:\n\n#create aggregation dataframe\npoagg_df = pol_df\ndel poagg_df['PID']\npo_grouped_df = poagg_df.groupby(['EID','PCODE'])\n\n#generate acc level aggregate\nacc_df = po_grouped_df.agg({\n    'PVALUE' : np.sum,\n    'SI' : lambda x: np.sqrt(np.sum(x * np.exp(x-1))),\n    'SC' : np.sum,\n    'EE' : np.sum\n})\n\n\nThis works fine until I want to join on the original table:\n\nIN:>>\n\npo_account_df = pd.merge(acc_df, po_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\n\n\nOUT:>>\nKeyError: u'no item named EID'\n\nFor some reason, the grouped dataframe can't join back to the original table.  I've looked at ways of trying to convert the groupby columns to actual columns but that doesn't seem to work. \n\nPlease note, the end goal is to be able to find the percentage for each column (PVALUE, SI, SC, EE)  IE:\n\npol_acc_df['PVALUE_PCT'] = np.round(pol_acc_df.PVALUE_Po/pol_acc_df.PVALUE_Acc,4)\n\n\nThanks!\n"
"I'm new to using pandas and am writing a script where I read in a dataframe and then do some computation on some of the columns.\nSometimes I will have the column called &quot;Met&quot;:\ndf = pd.read_csv(File, \n  sep='\\t', \n  compression='gzip', \n  header=0, \n  names=[&quot;Chrom&quot;, &quot;Site&quot;, &quot;coverage&quot;, &quot;Met&quot;]\n)\n\nOther times I will have:\ndf = pd.read_csv(File, \n  sep='\\t', \n  compression='gzip', \n  header=0, \n  names=[&quot;Chrom&quot;, &quot;Site&quot;, &quot;coverage&quot;, &quot;freqC&quot;]\n)\n\nI need to do some computation with the &quot;Met&quot; column so if it isn't present I will need to calculate it using:\ndf['Met'] = df['freqC'] * df['coverage'] \n\nis there a way to check if the &quot;Met&quot; column is present in the dataframe, and if not add it?\n"
"I have a dataframe that has auction IDs and bid prices. The dataframe is sorted by auction id (ascending) and bid price (descending):\n\nAuction_ID    Bid_Price\n123           9\n123           7\n123           6\n123           2\n124           3\n124           2\n124           1\n125           1\n\n\nI'd like to add a column called 'Auction_Rank' that ranks auction id's by bid prices:\n\nAuction_ID    Bid_Price    Auction_Rank\n123           9            1\n123           7            2\n123           6            3\n123           2            4\n124           3            1\n124           2            2\n124           1            3\n125           1            1\n\n"
"I have a Pandas Series of lists of strings:\n\n0                           [slim, waist, man]\n1                                [slim, waistline]\n2                                     [santa]\n\n\nAs you can see, the lists vary by length. I want an efficient way to collapse this into one series\n\n0 slim\n1 waist\n2 man\n3 slim\n4 waistline\n5 santa\n\n\nI know I can break up the lists using\n\nseries_name.split(' ')\n\n\nBut I am having a hard time putting those strings back into one list.\n\nThanks!\n"
"Suppose I have a DataFrame such as: \n\ndf = pd.DataFrame(np.random.randn(10,5), columns = ['a','b','c','d','e'])\n\n\nand I would like to retrieve the last value in column e. I could do:\n\ndf['e'].tail(1)\n\n\nbut this would return a series which has index 9 with it. ideally, i just want to obtain the value as a number i can work with directly. i could also do:\n\nnp.array(df['e'].tail(1))\n\n\nbut this would then require me to access/call the 0'th element of it before i can really work with it. is there a a more direct/easy way to do this? \n"
"In pandas, I'd like to create a computed column that's a boolean operation on two other columns. \n\nIn pandas, it's easy to add together two numerical columns. I'd like to do something similar with logical operator AND. Here's my first try:\n\nIn [1]: d = pandas.DataFrame([{'foo':True, 'bar':True}, {'foo':True, 'bar':False}, {'foo':False, 'bar':False}])\n\nIn [2]: d\nOut[2]: \n     bar    foo\n0   True   True\n1  False   True\n2  False  False\n\nIn [3]: d.bar and d.foo   ## can't\n...\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\nSo I guess logical operators don't work quite the same way as numeric operators in pandas. I tried doing what the error message suggests and using bool():\n\nIn [258]: d.bar.bool() and d.foo.bool()  ## spoiler: this doesn't work either\n...\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\nI found a way that works by casting the boolean columns to int, adding them together and evaluating as a boolean.\n\nIn [4]: (d.bar.apply(int) + d.foo.apply(int)) &gt; 0  ## Logical OR\nOut[4]: \n0     True\n1     True\n2    False\ndtype: bool\n\nIn [5]: (d.bar.apply(int) + d.foo.apply(int)) &gt; 1  ## Logical AND\nOut[5]: \n0     True\n1    False\n2    False\ndtype: bool\n\n\nThis is convoluted. Is there a better way?\n"
"I have two columns in my Pandas dataframe. I'd like to divide column a by column b, value by value, and show it as follows:\nimport pandas as pd\n\ncsv1=pd.read_csv('auto$0$0.csv')\ncsv2=pd.read_csv('auto$0$8.csv')\n\ndf1 = pd.DataFrame(csv1, columns = ['Column A','Column B'])\ndf2 = pd.DataFrame(csv2, columns = ['Column A','Column B'])\n\ndfnew = pd.concat([df1, df2])\n\nThe columns:\nColumn A | Column B |\n12-------|--2-------|\n14-------|--7-------|\n16-------|--8-------|\n20-------|--5-------|\n\nand the expected result\nResult\n6\n2\n2\n4\n\nHow do I do this?\n"
"What is an efficient way to get the diagonal of a square DataFrame.  I would expect the result to be a Series with a MultiIndex with two levels, the first being the index of the DataFrame the second level being the columns of the DataFrame.\n\nSetup\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3, 1415])\ndf = pd.DataFrame(np.random.rand(3, 3) * 5,\n                  columns = list('abc'),\n                  index = list('ABC'),\n                  dtype=np.int64\n                 )\n\n\nI want to see this:\n\nprint df.stack().loc[[('A', 'a'), ('B', 'b'), ('C', 'c')]]\n\nA  a    2\nB  b    2\nC  c    3\n\n"
"I have a pandas data frame, sample, with one of the columns called PR to which am applying a lambda function as follows:\n\nsample['PR'] = sample['PR'].apply(lambda x: NaN if x &lt; 90)\n\n\nI then get the following syntax error message:\n\nsample['PR'] = sample['PR'].apply(lambda x: NaN if x &lt; 90)\n                                                         ^\nSyntaxError: invalid syntax\n\n\nWhat am I doing wrong?\n"
'I\'m using sqlalchemy in pandas to query postgres database and then insert results of a transformation to another table on the same database. But when I do \ndf.to_sql(\'db_table2\', engine) I get this error message:\nValueError: Table \'db_table2\' already exists. I noticed it want to create a new table. How to insert pandas dataframe to an already existing table ? \n\ndf = pd.read_sql_query(\'select * from "db_table1"\',con=engine)\n#do transformation then save df to db_table2\ndf.to_sql(\'db_table2\', engine)\n\nValueError: Table \'db_table2\' already exists\n\n'
"I figured out these two methods.  Is there a better one?\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'A': [5, 6, 7], 'B': [7, 8, 9]})\n&gt;&gt;&gt; print df.sum().sum()\n42\n&gt;&gt;&gt; print df.values.sum()\n42\n\n\nJust want to make sure I'm not missing something more obvious.\n"
'I have a pandas dataframe that looks like this:\n\n    class       men       woman   children\n0   first   0.91468    0.667971   0.660562\n1   second  0.30012    0.329380   0.882608\n2   third   0.11899    0.189747   0.121259\n\n\nHow would I create a plot using seaborn that looks like this? Do I have to rearrange my data in some way?\n\n\n(source: mwaskom at stanford.edu)  \n'
"Below is my dataframe\n\nimport pandas as pd\ndf = pd.DataFrame({'name': ['jon','sam','jane','bob'],\n           'age': [30,25,18,26],\n           'sex':['male','male','female','male']})\n\n\n   age  name     sex\n0   30   jon    male\n1   25   sam    male\n2   18  jane  female\n3   26   bob    male\n\n\nI want to insert a new row at the first position\n\nname: dean, age: 45, sex: male\n\n   age  name     sex\n0   45  dean    male\n1   30   jon    male\n2   25   sam    male\n3   18  jane  female\n4   26   bob    male\n\n\nWhat is the best way to do this in pandas?\n"
'There are many questions on this, but there has been no simple answer on how to read an xlsb file into pandas. Is there an easy way to do this?\n'
'I am new to Python, I am trying to read csv file using below script.\n\nPast=pd.read_csv("C:/Users/Admin/Desktop/Python/Past.csv",encoding=\'utf-8\')\n\n\nBut, getting error "UnicodeDecodeError: \'utf-8\' codec can\'t decode byte 0x96 in position 35: invalid start byte", Please help me to know issue here, I used encoding in script thought it will resolve error.\n'
'i have a dataframe\n\nid  store    address\n1    100        xyz\n2    200        qwe\n3    300        asd\n4    400        zxc\n5    500        bnm\n\n\ni have another dataframe df2\n\nserialNo    store_code  warehouse\n    1          300         Land\n    2          500         Sea\n    3          100         Land\n    4          200         Sea\n    5          400         Land\n\n\nI want my final dataframe to look like:\n\nid  store    address  warehouse\n1    100        xyz     Land\n2    200        qwe     Sea\n3    300        asd     Land\n4    400        zxc     Land\n5    500        bnm     Sea\n\n\ni.e map from one dataframe onto another creating new column\n'
"I'm trying to add a new row to the DataFrame with a specific index name 'e'.\n\n    number   variable       values\na    NaN       bank          true   \nb    3.0       shop          false  \nc    0.5       market        true   \nd    NaN       government    true   \n\n\nI have tried the following but it's creating a new column instead of a new row. \n\nnew_row = [1.0, 'hotel', 'true']\ndf = df.append(new_row)\n\n\nStill don't understand how to insert the row with a specific index. Will be grateful for any suggestions.\n"
'I have been trying to normalize a very nested json file I will later analyze. What I am struggling with is how to go more than one level deep to normalize.\n\nI went through the pandas.io.json.json_normalize documentation, since it does exactly what I want it to do.\n\nI have been able to normalize part of it and now understand how dictionaries work, but I am still not there.\n\nWith below code I am able to get only the first level.\n\nimport json\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\nwith open(\'authors_sample.json\') as f:\n    d = json.load(f)\n\nraw = json_normalize(d[\'hits\'][\'hits\'])\n\nauthors = json_normalize(data = d[\'hits\'][\'hits\'], \n                         record_path = \'_source\', \n                         meta = [\'_id\', [\'_source\', \'journal\'], [\'_source\', \'title\'], \n                                 [\'_source\', \'normalized_venue_name\']\n                                 ])\n\n\nI am trying to \'dig\' into the \'authors\' dictionary with below code, but the record_path = [\'_source\', \'authors\'] throws me TypeError: string indices must be integers. As far as I understand json_normalize the logic should be good, but I still don\'t quite understand how to dive into a json with dict vs list.\n\nI even went through this simple example.\n\nauthors = json_normalize(data = d[\'hits\'][\'hits\'], \n                         record_path = [\'_source\', \'authors\'], \n                         meta = [\'_id\', [\'_source\', \'journal\'], [\'_source\', \'title\'], \n                                 [\'_source\', \'normalized_venue_name\']\n                                 ])\n\n\nBelow is a chunk of the json file (5 records).\n\n{u\'_shards\': {u\'failed\': 0, u\'successful\': 5, u\'total\': 5},\n u\'hits\': {u\'hits\': [{u\'_id\': u\'7CB3F2AD\',\n    u\'_index\': u\'scibase_listings\',\n    u\'_score\': 1.0,\n    u\'_source\': {u\'authors\': None,\n     u\'deleted\': 0,\n     u\'description\': None,\n     u\'doi\': u\'\',\n     u\'is_valid\': 1,\n     u\'issue\': None,\n     u\'journal\': u\'Physical Review Letters\',\n     u\'link\': None,\n     u\'meta_description\': None,\n     u\'meta_keywords\': None,\n     u\'normalized_venue_name\': u\'phys rev lett\',\n     u\'pages\': None,\n     u\'parent_keywords\': [u\'Chromatography\',\n      u\'Quantum mechanics\',\n      u\'Particle physics\',\n      u\'Quantum field theory\',\n      u\'Analytical chemistry\',\n      u\'Quantum chromodynamics\',\n      u\'Physics\',\n      u\'Mass spectrometry\',\n      u\'Chemistry\'],\n     u\'pub_date\': u\'1987-03-02 00:00:00\',\n     u\'pubtype\': None,\n     u\'rating_avg_weighted\': 0,\n     u\'rating_clarity\': 0.0,\n     u\'rating_clarity_weighted\': 0.0,\n     u\'rating_innovation\': 0.0,\n     u\'rating_innovation_weighted\': 0.0,\n     u\'rating_num_weighted\': 0,\n     u\'rating_reproducability\': 0,\n     u\'rating_reproducibility_weighted\': 0.0,\n     u\'rating_versatility\': 0.0,\n     u\'rating_versatility_weighted\': 0.0,\n     u\'review_count\': 0,\n     u\'tag\': [u\'mass spectra\', u\'elementary particles\', u\'bound states\'],\n     u\'title\': u\'Evidence for a new meson: A quasinuclear NN-bar bound state\',\n     u\'userAvg\': 0.0,\n     u\'user_id\': None,\n     u\'venue_name\': u\'Physical Review Letters\',\n     u\'views_count\': 0,\n     u\'volume\': None},\n    u\'_type\': u\'listing\'},\n   {u\'_id\': u\'7AF8EBC3\',\n    u\'_index\': u\'scibase_listings\',\n    u\'_score\': 1.0,\n    u\'_source\': {u\'authors\': [{u\'affiliations\': [u\'Punjabi University\'],\n       u\'author_id\': u\'780E3459\',\n       u\'author_name\': u\'munish puri\'},\n      {u\'affiliations\': [u\'Punjabi University\'],\n       u\'author_id\': u\'48D92C79\',\n       u\'author_name\': u\'rajesh dhaliwal\'},\n      {u\'affiliations\': [u\'Punjabi University\'],\n       u\'author_id\': u\'7D9BD37C\',\n       u\'author_name\': u\'r s singh\'}],\n     u\'deleted\': 0,\n     u\'description\': None,\n     u\'doi\': u\'\',\n     u\'is_valid\': 1,\n     u\'issue\': None,\n     u\'journal\': u\'Journal of Industrial Microbiology &amp; Biotechnology\',\n     u\'link\': None,\n     u\'meta_description\': None,\n     u\'meta_keywords\': None,\n     u\'normalized_venue_name\': u\'j ind microbiol biotechnol\',\n     u\'pages\': None,\n     u\'parent_keywords\': [u\'Nuclear medicine\',\n      u\'Psychology\',\n      u\'Hydrology\',\n      u\'Chromatography\',\n      u\'X-ray crystallography\',\n      u\'Nuclear fusion\',\n      u\'Medicine\',\n      u\'Fluid dynamics\',\n      u\'Thermodynamics\',\n      u\'Physics\',\n      u\'Gas chromatography\',\n      u\'Radiobiology\',\n      u\'Engineering\',\n      u\'Organic chemistry\',\n      u\'High-performance liquid chromatography\',\n      u\'Chemistry\',\n      u\'Organic synthesis\',\n      u\'Psychotherapist\'],\n     u\'pub_date\': u\'2008-04-04 00:00:00\',\n     u\'pubtype\': None,\n     u\'rating_avg_weighted\': 0,\n     u\'rating_clarity\': 0.0,\n     u\'rating_clarity_weighted\': 0.0,\n     u\'rating_innovation\': 0.0,\n     u\'rating_innovation_weighted\': 0.0,\n     u\'rating_num_weighted\': 0,\n     u\'rating_reproducability\': 0,\n     u\'rating_reproducibility_weighted\': 0.0,\n     u\'rating_versatility\': 0.0,\n     u\'rating_versatility_weighted\': 0.0,\n     u\'review_count\': 0,\n     u\'tag\': [u\'flow rate\',\n      u\'operant conditioning\',\n      u\'packed bed reactor\',\n      u\'immobilized enzyme\',\n      u\'specific activity\'],\n     u\'title\': u\'Development of a stable continuous flow immobilized enzyme reactor for the hydrolysis of inulin\',\n     u\'userAvg\': 0.0,\n     u\'user_id\': None,\n     u\'venue_name\': u\'Journal of Industrial Microbiology &amp; Biotechnology\',\n     u\'views_count\': 0,\n     u\'volume\': None},\n    u\'_type\': u\'listing\'},\n   {u\'_id\': u\'7521A721\',\n    u\'_index\': u\'scibase_listings\',\n    u\'_score\': 1.0,\n    u\'_source\': {u\'authors\': [{u\'author_id\': u\'7FF872BC\',\n       u\'author_name\': u\'barbara eileen ryan\'}],\n     u\'deleted\': 0,\n     u\'description\': None,\n     u\'doi\': u\'\',\n     u\'is_valid\': 1,\n     u\'issue\': None,\n     u\'journal\': u\'The American Historical Review\',\n     u\'link\': None,\n     u\'meta_description\': None,\n     u\'meta_keywords\': None,\n     u\'normalized_venue_name\': u\'american historical review\',\n     u\'pages\': None,\n     u\'parent_keywords\': [u\'Social science\',\n      u\'Politics\',\n      u\'Sociology\',\n      u\'Law\'],\n     u\'pub_date\': u\'1992-01-01 00:00:00\',\n     u\'pubtype\': None,\n     u\'rating_avg_weighted\': 0,\n     u\'rating_clarity\': 0.0,\n     u\'rating_clarity_weighted\': 0.0,\n     u\'rating_innovation\': 0.0,\n     u\'rating_innovation_weighted\': 0.0,\n     u\'rating_num_weighted\': 0,\n     u\'rating_reproducability\': 0,\n     u\'rating_reproducibility_weighted\': 0.0,\n     u\'rating_versatility\': 0.0,\n     u\'rating_versatility_weighted\': 0.0,\n     u\'review_count\': 0,\n     u\'tag\': [u\'social movements\'],\n     u\'title\': u"Feminism and the women\'s movement : dynamics of change in social movement ideology, and activism",\n     u\'userAvg\': 0.0,\n     u\'user_id\': None,\n     u\'venue_name\': u\'The American Historical Review\',\n     u\'views_count\': 0,\n     u\'volume\': None},\n    u\'_type\': u\'listing\'},\n   {u\'_id\': u\'7DAEB9A4\',\n    u\'_index\': u\'scibase_listings\',\n    u\'_score\': 1.0,\n    u\'_source\': {u\'authors\': [{u\'author_id\': u\'0299B8E9\',\n       u\'author_name\': u\'fraser j harbutt\'}],\n     u\'deleted\': 0,\n     u\'description\': None,\n     u\'doi\': u\'\',\n     u\'is_valid\': 1,\n     u\'issue\': None,\n     u\'journal\': u\'The American Historical Review\',\n     u\'link\': None,\n     u\'meta_description\': None,\n     u\'meta_keywords\': None,\n     u\'normalized_venue_name\': u\'american historical review\',\n     u\'pages\': None,\n     u\'parent_keywords\': [u\'Superconductivity\',\n      u\'Nuclear fusion\',\n      u\'Geology\',\n      u\'Chemistry\',\n      u\'Metallurgy\'],\n     u\'pub_date\': u\'1988-01-01 00:00:00\',\n     u\'pubtype\': None,\n     u\'rating_avg_weighted\': 0,\n     u\'rating_clarity\': 0.0,\n     u\'rating_clarity_weighted\': 0.0,\n     u\'rating_innovation\': 0.0,\n     u\'rating_innovation_weighted\': 0.0,\n     u\'rating_num_weighted\': 0,\n     u\'rating_reproducability\': 0,\n     u\'rating_reproducibility_weighted\': 0.0,\n     u\'rating_versatility\': 0.0,\n     u\'rating_versatility_weighted\': 0.0,\n     u\'review_count\': 0,\n     u\'tag\': [u\'iron\'],\n     u\'title\': u\'The iron curtain : Churchill, America, and the origins of the Cold War\',\n     u\'userAvg\': 0.0,\n     u\'user_id\': None,\n     u\'venue_name\': u\'The American Historical Review\',\n     u\'views_count\': 0,\n     u\'volume\': None},\n    u\'_type\': u\'listing\'},\n   {u\'_id\': u\'7B3236C5\',\n    u\'_index\': u\'scibase_listings\',\n    u\'_score\': 1.0,\n    u\'_source\': {u\'authors\': [{u\'author_id\': u\'7DAB7B72\',\n       u\'author_name\': u\'richard m freeland\'}],\n     u\'deleted\': 0,\n     u\'description\': None,\n     u\'doi\': u\'\',\n     u\'is_valid\': 1,\n     u\'issue\': None,\n     u\'journal\': u\'The American Historical Review\',\n     u\'link\': None,\n     u\'meta_description\': None,\n     u\'meta_keywords\': None,\n     u\'normalized_venue_name\': u\'american historical review\',\n     u\'pages\': None,\n     u\'parent_keywords\': [u\'Political Science\', u\'Economics\'],\n     u\'pub_date\': u\'1985-01-01 00:00:00\',\n     u\'pubtype\': None,\n     u\'rating_avg_weighted\': 0,\n     u\'rating_clarity\': 0.0,\n     u\'rating_clarity_weighted\': 0.0,\n     u\'rating_innovation\': 0.0,\n     u\'rating_innovation_weighted\': 0.0,\n     u\'rating_num_weighted\': 0,\n     u\'rating_reproducability\': 0,\n     u\'rating_reproducibility_weighted\': 0.0,\n     u\'rating_versatility\': 0.0,\n     u\'rating_versatility_weighted\': 0.0,\n     u\'review_count\': 0,\n     u\'tag\': [u\'foreign policy\'],\n     u\'title\': u\'The Truman Doctrine and the origins of McCarthyism : foreign policy, domestic politics, and internal security, 1946-1948\',\n     u\'userAvg\': 0.0,\n     u\'user_id\': None,\n     u\'venue_name\': u\'The American Historical Review\',\n     u\'views_count\': 0,\n     u\'volume\': None},\n    u\'_type\': u\'listing\'}],\n  u\'max_score\': 1.0,\n  u\'total\': 36429433},\n u\'timed_out\': False,\n u\'took\': 170}\n\n'
'I have a dataframe contains orders data, each order has multiple packages stored as comma separated string [package &amp; package_code] columns\n\nI want to split the packages data and create a row for each package including its order details\n\nHere is a sample input dataframe:\n\nimport pandas as pd\ndf = pd.DataFrame({"order_id":[1,3,7],"order_date":["20/5/2018","22/5/2018","23/5/2018"], "package":["p1,p2,p3","p4","p5,p6"],"package_code":["#111,#222,#333","#444","#555,#666"]})\n\n\n\n\nAnd this is what I am trying to achieve as output:\n\n\nHow can I do that with pandas?\n'
'This may sound a noob question, but I\'m stuck with it as Python is not one of my best languages.\n\nI have a html page with a table inside it, and I would like to show a pandas dataframe in it.\nWhat is the best way to do it? Use pandasdataframe.to_html?\n\npy\n\nfrom flask import Flask;\nimport pandas as pd;\nfrom pandas import DataFrame, read_csv;\n\nfile = r\'C:\\Users\\myuser\\Desktop\\Test.csv\'\ndf = pd.read_csv(file)\ndf.to_html(header="true", table_id="table")\n\n\nhtml\n\n&lt;div class="table_entrances" style="overflow-x: auto;"&gt;\n\n  &lt;table id="table"&gt;\n\n    &lt;thead&gt;&lt;/thead&gt; \n    &lt;tr&gt;&lt;/tr&gt;\n\n  &lt;/table&gt;\n\n&lt;/div&gt;\n\n'
'In the following, male_trips is a big pandas data frame and stations is a small pandas data frame. For each station id I\'d like to know how many male trips took place. The following does the job, but takes a long time:\n\nmc = [ sum( male_trips[\'start_station_id\'] == id ) for id in stations[\'id\'] ]\n\n\nhow should I go about this instead?\n\n\n\nUpdate! So there were two main approaches: groupby() followed by size(), and the simpler .value_counts(). I did a quick timeit, and the groupby approach wins by quite a large margin! Here is the code:\n\nfrom timeit import Timer\nsetup = "import pandas; male_trips=pandas.load(\'maletrips\')"\na  = "male_trips.start_station_id.value_counts()"\nb = "male_trips.groupby(\'start_station_id\').size()"\nTimer(a,setup).timeit(100)\nTimer(b,setup).timeit(100)\n\n\nand here is the result:\n\nIn [4]: Timer(a,setup).timeit(100) # &lt;- this is value_counts\nOut[4]: 9.709594964981079\n\nIn [5]: Timer(b,setup).timeit(100) # &lt;- this is groupby / size\nOut[5]: 1.5574288368225098\n\n\nNote that, at this speed, for exploring data typing value_counts is marginally quicker and less remembering!\n'
'Is there a way to convert values like \'34%\' directly to int or float when using read_csv in pandas? I would like that it is directly read as 0.34. \n\nUsing this in read_csv did not work:\n\nread_csv(..., dtype={\'col\':np.float})\n\n\nAfter loading the csv as \'df\' this also did not work with the error "invalid literal for float(): 34%"\n\ndf[\'col\'] = df[\'col\'].astype(float)\n\n\nI ended up using this which works but is long winded:\n\ndf[\'col\'] = df[\'col\'].apply(lambda x: np.nan if x in [\'-\'] else x[:-1]).astype(float)/100\n\n'
'I would like to create an empty DataFrame with a MultiIndex before assigning rows to it. I already found that empty DataFrames don\'t like to be assigned MultiIndexes on the fly, so I\'m setting the MultiIndex names during creation. However, I don\'t want to assign levels, as this will be done later. This is the best code I got to so far:\n\ndef empty_multiindex(names):\n    """\n    Creates empty MultiIndex from a list of level names.\n    """\n    return MultiIndex.from_tuples(tuples=[(None,) * len(names)], names=names)\n\n\nWhich gives me\n\nIn [2]:\n\nempty_multiindex([\'one\',\'two\', \'three\'])\n\nOut[2]:\n\nMultiIndex(levels=[[], [], []],\n           labels=[[-1, -1, -1], [-1, -1, -1], [-1, -1, -1]],\n           names=[u\'one\', u\'two\', u\'three\'])\n\n\nand\n\nIn [3]:\nDataFrame(index=empty_multiindex([\'one\',\'two\', \'three\']))\n\nOut[3]:\none two three\nNaN NaN NaN\n\n\nWell, I have no use for these NaNs. I can easily drop them later, but this is obviously a hackish solution. Anyone has a better one?\n'
"I'm trying to reorder/swaplevel/pivot/something columns in a pandas dataframe.\nThe columns are a MultiIndex, but I can't find the sauce to do what I want.\n\nThe fastest varying column in my multiIndex is month, but I would like it to be the slowest varying column. \n\nI've got a nbviewer notebook if you would like to try it out yourself:\nhttp://nbviewer.ipython.org/gist/flamingbear/4cfac24c80fe34a67474\n\nWhat I have:\n\n+-------------------------------------------------------------------+\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||     |weight             |extent            |rank                ||\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||month|'1Jan'|'Feb' |'Mar'|'1Jan'|'Feb'|'Mar'|'1Jan'|'Feb'|'Mar'|  |\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||year |      |      |     |      |     |     |      |     |     |  |\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||2000 |45.1  |46.1  |25.1 |13.442|14.94|15.02|13    |17   |14   |  |\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||2001 |85.0  |16.0  |49.0 |13.380|14.81|15.14|12    |15   |17   |  |\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||2002 |90.0  |33.0  |82.0 |13.590|15.13|14.88|15    |22   |10   |  |\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n||2003 |47.0  |34.0  |78.0 |13.640|14.83|15.27|17    |16   |22   |  |\n|+-----+------+------+-----+------+-----+-----+------+-----+-----+  |\n+-------------------------------------------------------------------+\n\n\nWhat I want\n\n+------------------------------------------------------------------+\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||month|1Jan              |Feb                |Mar                ||\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||     |weight|extent|rank|weight|extent|rank |weight|extent|rank| |\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||year |      |      |    |      |      |     |      |      |    | |\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||2000 |45.1  |13.442|13  |46.1  |14.94 |17   | 25.1 |15.02 |14  | |\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||2001 |85.0  |13.380|12  |16.0  |14.81 |15   | 49.0 |15.14 |17  | |\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||2002 |90.0  |13.590|15  |33.0  |15.13 |22   | 82.0 |14.88 |10  | |\n|+-----+------+------+----+------+------+-----+------+------+----+ |\n||2003 |47.0  |13.640|17  |34.0  |14.83 |16   | 78.0 |15.27 |22  | |\n|+-----+------+------+-----------+------+-----+------+------+----+ |\n+------------------------------------------------------------------+\n\n\nAny help would be appreciated.  I can work with my original DataFrame, but writing to a CSV with the desired ordering would be fantastic.\n\nThanks in advance,\nMatt\n"
'The question is how to fill NaNs with most frequent levels for category column in pandas dataframe?\n\nIn R randomForest package there is \nna.roughfix option : A completed data matrix or data frame. For numeric variables, NAs are replaced with column medians. For factor variables, NAs are replaced with the most frequent levels (breaking ties at random). If object contains no NAs, it is returned unaltered.\n\nin Pandas for numeric variables I can fill NaN values with :\n\ndf = df.fillna(df.median())\n\n'
"I have the following dataframe:\n\n   amount  catcode    cid      cycle      date     di  feccandid    type\n0   1000    E1600   N00029285   2014    2014-05-15  D   H8TX22107   24K\n1   5000    G4600   N00026722   2014    2013-10-22  D   H4TX28046   24K\n2      4    C2100   N00030676   2014    2014-03-26  D   H0MO07113   24Z\n\n\nI want to make dummy variables for the values in column type. There about 15. I have tried this:\n\npd.get_dummies(df['type'])\n\nAnd it returns this:\n\n           24A  24C  24E  24F  24K  24N  24P  24R  24Z\ndate                                    \n2014-05-15  0    0    0    0    1    0    0    0    0\n2013-10-22  0    0    0    0    1    0    0    0    0\n2014-03-26  0    0    0    0    0    0    0    0    1\n\n\nWhat I would like is to have a dummy variable column for each unique value in Type\n"
'I would like to install Python Pandas library (0.8.1) on Mac OS X 10.6.8. This library needs Numpy>=1.6.\n\nI tried this\n\n$ sudo easy_install pandas\nSearching for pandas\nReading http://pypi.python.org/simple/pandas/\nReading http://pandas.pydata.org\nReading http://pandas.sourceforge.net\nBest match: pandas 0.8.1\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.8.1.zip#md5=d2c5c5bea971cd760b0ae6f6850fcb74\nProcessing pandas-0.8.1.zip\nRunning pandas-0.8.1/setup.py -q bdist_egg --dist-dir /tmp/easy_install-ckAMym/pandas-0.8.1/egg-dist-tmp-0mlL7t\nerror: Setup script exited with pandas requires NumPy &gt;= 1.6 due to datetime64 dependency\n\n\nSo I tried to install Numpy\n\n$ sudo easy_install numpy\nSearching for numpy\nBest match: numpy 1.6.2\nAdding numpy 1.6.2 to easy-install.pth file\n\nUsing /Library/Python/2.6/site-packages\nProcessing dependencies for numpy\nFinished processing dependencies for numpy\n\n\nSo I tried again\n\n$ sudo easy_install pandas\n\n\nBut the problem is still the same !\n\nerror: Setup script exited with pandas requires NumPy &gt;= 1.6 due to datetime64 dependency\n\n\nI run Python \n\n$ python\nPython 2.6.1 (r261:67515, Jun 24 2010, 21:47:49) \n[GCC 4.2.1 (Apple Inc. build 5646)] on darwin\nType "help", "copyright", "credits" or "license" for more information.\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.__version__\n\'1.2.1\'\n\n\nSo Numpy 1.6 doesn\'t seems to be installed correctly !\n\nI tried to install Numpy 1.6 with pip (instead of easy_install)...\n\n$ sudo pip install numpy\nRequirement already satisfied (use --upgrade to upgrade): numpy in /Library/Python/2.6/site-packages\nCleaning up...\n\n\nI added --upgrade flag\n\n$ sudo pip install numpy --upgrade\nRequirement already up-to-date: numpy in /Library/Python/2.6/site-packages\nCleaning up...\n\n$ sudo pip install pandas\nDownloading/unpacking pandas\n  Downloading pandas-0.8.1.zip (1.9MB): 1.9MB downloaded\n  Running setup.py egg_info for package pandas\n    pandas requires NumPy &gt;= 1.6 due to datetime64 dependency\n    Complete output from command python setup.py egg_info:\n    pandas requires NumPy &gt;= 1.6 due to datetime64 dependency\n\n----------------------------------------\nCommand python setup.py egg_info failed with error code 1 in /tmp/pip-build/pandas\nStoring complete log in /Users/MyUsername/Library/Logs/pip.log\n\n\nI also tried to install binary version of Numpy http://sourceforge.net/projects/numpy/files/\nnumpy-1.6.2-py2.6-python.org-macosx10.3.dmg but it fails !!! (installer said me that numpy 1.6.2 can\'t be install on this disk. Numpy requires python.org Python 2.6 to install.\n'
"I have a series with a MultiIndex like this:\n\nimport numpy as np\nimport pandas as pd\n\nbuckets = np.repeat(['a','b','c'], [3,5,1])\nsequence = [0,1,5,0,1,2,4,50,0]\n\ns = pd.Series(\n    np.random.randn(len(sequence)), \n    index=pd.MultiIndex.from_tuples(zip(buckets, sequence))\n)\n\n# In [6]: s\n# Out[6]: \n# a  0    -1.106047\n#    1     1.665214\n#    5     0.279190\n# b  0     0.326364\n#    1     0.900439\n#    2    -0.653940\n#    4     0.082270\n#    50   -0.255482\n# c  0    -0.091730\n\n\nI'd like to get the s['b'] values where the second index ('sequence') is between 2 and 10.\n\nSlicing on the first index works fine:\n\ns['a':'b']\n# Out[109]: \n# bucket  value\n# a       0        1.828176\n#         1        0.160496\n#         5        0.401985\n# b       0       -1.514268\n#         1       -0.973915\n#         2        1.285553\n#         4       -0.194625\n#         5       -0.144112\n\n\nBut not on the second, at least by what seems to be the two most obvious ways:\n\n1) This returns elements 1 through 4, with nothing to do with the index values\n\ns['b'][1:10]\n\n# In [61]: s['b'][1:10]\n# Out[61]: \n# 1     0.900439\n# 2    -0.653940\n# 4     0.082270\n# 50   -0.255482\n\n\nHowever, if I reverse the index and the first index is integer and the second index is a string, it works:\n\nIn [26]: s\nOut[26]: \n0   a   -0.126299\n1   a    1.810928\n5   a    0.571873\n0   b   -0.116108\n1   b   -0.712184\n2   b   -1.771264\n4   b    0.148961\n50  b    0.089683\n0   c   -0.582578\n\nIn [25]: s[0]['a':'b']\nOut[25]: \na   -0.126299\nb   -0.116108\n\n"
"I'm trying to figure out how to count by number of rows per unique pair of columns (ip, useragent), e.g.\n\nd = pd.DataFrame({'ip': ['192.168.0.1', '192.168.0.1', '192.168.0.1', '192.168.0.2'], 'useragent': ['a', 'a', 'b', 'b']})\n\n     ip              useragent\n0    192.168.0.1     a\n1    192.168.0.1     a\n2    192.168.0.1     b\n3    192.168.0.2     b\n\n\nTo produce:\n\nip           useragent  \n192.168.0.1  a           2\n192.168.0.1  b           1\n192.168.0.2  b           1\n\n\nIdeas?\n"
'I\'m pretty new to pandas, so I guess I\'m doing something wrong - \n\nI have a DataFrame:\n\n     a     b\n0  0.5  0.75\n1  0.5  0.75\n2  0.5  0.75\n3  0.5  0.75\n4  0.5  0.75\n\n\ndf.corr() gives me: \n\n    a   b\na NaN NaN\nb NaN NaN\n\n\nbut np.correlate(df["a"], df["b"]) gives: 1.875\n\nWhy is that? \nI want to have the correlation matrix for my DataFrame and thought that corr() does that (at least according to the documentation). Why does it return NaN?\n\nWhat\'s the correct way to calculate?\n\nMany thanks!\n'
'I created a Series from a DataFrame, when I resampled some data with a count\nlike so: where H2 is a DataFrame:\n\nH3=H2[[\'SOLD_PRICE\']]\nH5=H3.resample(\'Q\',how=\'count\')\nH6=pd.rolling_mean(H5,4)\n\n\nThis yielded a series that looks like this:\n\n1999-03-31  SOLD_PRICE     NaN\n1999-06-30  SOLD_PRICE     NaN\n1999-09-30  SOLD_PRICE     NaN\n1999-12-31  SOLD_PRICE    3.00\n2000-03-31  SOLD_PRICE    3.00\n\n\nwith an index that looks like:\n\nMultiIndex\n[(1999-03-31 00:00:00, u\'SOLD_PRICE\'), (1999-06-30 00:00:00, u\'SOLD_PRICE\'), (1999-09-30 00:00:00, u\'SOLD_PRICE\'), (1999-12-31 00:00:00, u\'SOLD_PRICE\'),.....\n\n\nI don\'t want the second column as an index. Ideally I\'d have a DataFrame with column 1 as "Date" and column 2 as "Sales" (dropping the second level of the index). I don\'t quite see how to reconfigure the index.\n'
"I have a 719mb CSV file that looks like:\n\nfrom, to, dep, freq, arr, code, mode   (header row)\nRGBOXFD,RGBPADTON,127,0,27,99999,2\nRGBOXFD,RGBPADTON,127,0,33,99999,2\nRGBOXFD,RGBRDLEY,127,0,1425,99999,2\nRGBOXFD,RGBCHOLSEY,127,0,52,99999,2\nRGBOXFD,RGBMDNHEAD,127,0,91,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,46,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,3,99999,2\nRGBDIDCOTP,RGBCHOLSEY,127,0,61,99999,2\nRGBDIDCOTP,RGBRDLEY,127,0,1430,99999,2\nRGBDIDCOTP,RGBPADTON,127,0,115,99999,2\nand so on... \n\n\nI want to load in to a pandas DataFrame. Now I know there is a load from csv method:\n\n r = pd.DataFrame.from_csv('test_data2.csv')\n\n\nBut I specifically want to load it as a 'MultiIndex' DataFrame where from and to are the indexes:\n\nSo ending up with:\n\n                   dep, freq, arr, code, mode\nRGBOXFD RGBPADTON  127     0   27  99999    2\n        RGBRDLEY   127     0   33  99999    2\n        RGBCHOLSEY 127     0 1425  99999    2\n        RGBMDNHEAD 127     0 1525  99999    2\n\n\netc. I'm not sure how to do that?\n"
"I understand that passing a function as a group key calls the function once per index value with the return values being used as the group names. What I can't figure out is how to call the function on column values. \n\nSo I can do this:\n\npeople = pd.DataFrame(np.random.randn(5, 5), \n                      columns=['a', 'b', 'c', 'd', 'e'],\n                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\ndef GroupFunc(x):\n    if len(x) &gt; 3:\n        return 'Group1'\n    else:\n        return 'Group2'\n\npeople.groupby(GroupFunc).sum()\n\n\nThis splits the data into two groups, one of which has index values of length 3 or less, and the other with length three or more. But how can I pass one of the column values? So for example if column d value for each index point is greater than 1. I realise I could just do the following:\n\npeople.groupby(people.a &gt; 1).sum()\n\n\nBut I want to know how to do this in a user defined function for future reference. \n\nSomething like: \n\ndef GroupColFunc(x):\nif x &gt; 1:\n    return 'Group1'\nelse:\n    return 'Group2'\n\n\nBut how do I call this?\nI tried \n\npeople.groupby(GroupColFunc(people.a))\n\n\nand similar variants but this does not work.\n\nHow do I pass the column values to the function?\nHow would I pass multiple column values e.g. to group on whether people.a > people.b for example?\n"
"In R I can quickly see a count of missing data using the summary command, but the equivalent pandas DataFrame method, describe does not report these values.\n\nI gather I can do something like\n\nlen(mydata.index) - mydata.count()\n\n\nto compute the number of missing values for each column, but I wonder if there's a better idiom (or if my approach is even right).\n"
'I have Excel files with multiple sheets, each of which looks a little like this (but much longer):\n\n        Sample  CD4     CD8\nDay 1   8311    17.3    6.44\n        8312    13.6    3.50\n        8321    19.8    5.88\n        8322    13.5    4.09\nDay 2   8311    16.0    4.92\n        8312    5.67    2.28\n        8321    13.0    4.34\n        8322    10.6    1.95\n\n\nThe first column is actually four cells merged vertically.  \n\nWhen I read this using pandas.read_excel, I get a DataFrame that looks like this:\n\n       Sample    CD4   CD8\nDay 1    8311  17.30  6.44\nNaN      8312  13.60  3.50\nNaN      8321  19.80  5.88\nNaN      8322  13.50  4.09\nDay 2    8311  16.00  4.92\nNaN      8312   5.67  2.28\nNaN      8321  13.00  4.34\nNaN      8322  10.60  1.95\n\n\nHow can I either get Pandas to understand merged cells, or quickly and easily remove the NaN and group by the appropriate value?  (One approach would be to reset the index, step through to find the values and replace NaNs with values, pass in the list of days, then set the index to the column.  But it seems like there should be a simpler approach.)\n'
'If I add two columns to create a third, any columns containing NaN (representing missing data in my world) cause the resulting output column to be NaN as well. Is there a way to skip NaNs without explicitly setting the values to 0 (which would lose the notion that those values are "missing")?\n\nIn [42]: frame = pd.DataFrame({\'a\': [1, 2, np.nan], \'b\': [3, np.nan, 4]})\n\nIn [44]: frame[\'c\'] = frame[\'a\'] + frame[\'b\']\n\nIn [45]: frame\nOut[45]: \n    a   b   c\n0   1   3   4\n1   2 NaN NaN\n2 NaN   4 NaN\n\n\nIn the above, I would like column c to be [4, 2, 4].\n\nThanks...\n'
"So I have a 'Date' column in my data frame where the dates have the format like this\n\n0    1998-08-26 04:00:00 \n\n\nIf I only want the Year month and day how do I drop the trivial hour?\n"
"I'm new to Python and Pandas so there might be a simple solution which I don't see. \n\nI have a number of discontinuous datasets which look like this:  \n\nind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   3.5  2  0  \n4   4.0  4  5  \n5   4.5  3  3  \n\n\nI now look for a solution to get the following:  \n\nind A    B  C  \n0   0.0  1  3  \n1   0.5  4  2  \n2   1.0  6  1  \n3   1.5  NAN NAN  \n4   2.0  NAN NAN  \n5   2.5  NAN NAN  \n6   3.0  NAN NAN  \n7   3.5  2  0  \n8   4.0  4  5  \n9   4.5  3  3  \n\n\nThe problem is,that the gap in A varies from dataset to dataset in position and length...\n"
"I have a dataframe like this\n\nd={}\nd['z']=['Q8','Q8','Q7','Q9','Q9']\nd['t']=['10:30','10:31','10:38','10:40','10:41']\nd['qty']=[20,20,9,12,12]\n\n\nI want compare first row with second row\n\n\nis qty same as next row AND\nis t greater in the next row AND\nis z value same as next row\n\n\nThe desired value is\n\n   qty                   t   z  valid\n0   20 2015-06-05 10:30:00  Q8  False\n1   20 2015-06-05 10:31:00  Q8   True\n2    9 2015-06-05 10:38:00  Q7  False\n3   12 2015-06-05 10:40:00  Q9  False\n4   12 2015-06-05 10:41:00  Q9   True\n\n"
"I have \n\ndf = pd.DataFrame.from_dict({'id': ['A', 'B', 'A', 'C', 'D', 'B', 'C'], 'val': [1,2,-3,1,5,6,-2], 'stuff':['12','23232','13','1234','3235','3236','732323']})\n\n  id   stuff  val\n0  A      12    1\n1  B   23232    2\n2  A      13   -3\n3  C    1234    1\n4  D    3235    5\n5  B    3236    6\n6  C  732323   -2\n\n\nI'd like to get running some of val for each id, so the desired output looks like this:\n\n  id   stuff  val  cumsum\n0  A      12    1   1\n1  B   23232    2   2\n2  A      13   -3   -2\n3  C    1234    1   1\n4  D    3235    5   5\n5  B    3236    6   8\n6  C  732323   -2  -1\n\n\nThis is what I tried:\n\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n\n\nand\n\ndf['cumsum'] = df.groupby('id').cumsum(['val'])\n\n\nThis is the error I got:\n\nValueError: Wrong number of items passed 0, placement implies 1\n\n"
"I have a pandas dataframe containing spectral data and metadata. The columns are labeled with a multiindex so that df['wvl'] gives the spectra and df['meta'] gives the metadata. Within df['wvl'] the column labels are the wavelength values for the spectrometer channels. \n\nWhat I want to do is normalize each row of df['wvl'] by the sum of that row so that adding up the values in the row gives a total of 1.0.\n\nHere's what one row of the dataframe looks like: \n\ndf['wvl'].iloc[0]\n246.050003     128.533035\n246.102005     102.756321\n246.156006      99.930775\n...    \n848.697205     121.313347\n848.896423     127.011662\n849.095703     123.234168\nName: 0, dtype: float64\n\n\nBut when I do something like:\n\ndf['wvl'].iloc[0]=df['wvl'].iloc[0]/df['wvl'].iloc[0].sum()\n\n\nNothing happens! I get the exact same values:\n\ndf['wvl'].iloc[0]\n246.050003     128.533035\n246.102005     102.756321\n246.156006      99.930775\n...    \n848.697205     121.313347\n848.896423     127.011662\n849.095703     123.234168\nName: 0, dtype: float64\n\n\nIf I create a temporary variable to hold the row, I can do the normalization just fine:\n\ntemp=df['wvl'].iloc[0]\n\ntemp=temp/temp.sum()\n\ntemp\n246.050003    0.000027\n246.102005    0.000022\n246.156006    0.000021\n                ...   \n848.697205    0.000026\n848.896423    0.000027\n849.095703    0.000026\nName: 0, dtype: float64\n\n\nBut if I try to replace the dataframe row with the normalized temporary variable, nothing happens:\n\ndf['wvl'].iloc[0]=temp\n\ndf['wvl'].iloc[0]\n246.050003     128.533035\n246.102005     102.756321\n246.156006      99.930775\n                 ...     \n848.697205     121.313347\n848.896423     127.011662\n849.095703     123.234168\nName: 0, dtype: float64\n\n\nI'm obviously missing something here, but I can't figure out what and it's driving me insane. Help? Thanks in advance!\n"
'df (Pandas Dataframe) has three rows.\n\nsome_col_name\n"apple is delicious"\n"banana is delicious"\n"apple and banana both are delicious"\n\n\ndf.col_name.str.contains("apple|banana")\n\nwill catch all of the rows:\n\n"apple is delicious",\n"banana is delicious",\n"apple and banana both are delicious".\n\n\nHow do I apply AND operator on str.contains method, so that it only grabs strings that contain BOTH apple &amp; banana?\n\n"apple and banana both are delicious"\n\n\nI\'d like to grab strings that contains 10-20 different words (grape, watermelon, berry, orange, ..., etc.) \n'
'I have the following code, \n\ndf = pd.read_csv(CsvFileName)\n\np = df.pivot_table(index=[\'Hour\'], columns=\'DOW\', values=\'Changes\', aggfunc=np.mean).round(0)\np.fillna(0, inplace=True)\n\np[["1Sun", "2Mon", "3Tue", "4Wed", "5Thu", "6Fri", "7Sat"]] = p[["1Sun", "2Mon", "3Tue", "4Wed", "5Thu", "6Fri", "7Sat"]].astype(int)\n\n\nIt has always been working until the csv file doesn\'t have enough coverage (of all week days). For e.g., with the following .csv file, \n\nDOW,Hour,Changes\n4Wed,01,237\n3Tue,07,2533\n1Sun,01,240\n3Tue,12,4407\n1Sun,09,2204\n1Sun,01,240\n1Sun,01,241\n1Sun,01,241\n3Tue,11,662\n4Wed,01,4\n2Mon,18,4737\n1Sun,15,240\n2Mon,02,4\n6Fri,01,1\n1Sun,01,240\n2Mon,19,2300\n2Mon,19,2532\n\n\nI\'ll get the following error:\n\nKeyError: "[\'5Thu\' \'7Sat\'] not in index"\n\n\nIt seems to have a very easy fix, but I\'m just too new to Python to know how to fix it.\n'
"I have a pandas dataframe with mixed column names:\n\n1,2,3,4,5, 'Class'\n\nWhen I save this dataframe to h5file, it says that the performance will be affected due to mixed types. How do I convert the integer to string in pandas?\n"
"I have a data frame A like this:\n\n\n\nAnd another data frame B which looks like this:\n\n\n\nI want to add a column 'Exist' to data frame A so that if User and Movie both exist in data frame B then 'Exist' is True, otherwise it is False.\nSo A should become like this:\n\n"
"I have a dataframe like the one displayed below: \n\n# Create an example dataframe about a fictional army\nraw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks'],\n            'company': ['1st', '1st', '2nd', '2nd'],\n            'deaths': ['kkk', 52, '25', 616],\n            'battles': [5, '42', 2, 2],\n            'size': ['l', 'll', 'l', 'm']}\ndf = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'deaths', 'battles', 'size'])\n\n\n\n\nMy goal is to transform every single string inside of the dataframe to upper case so that it looks like this:\n\n\n\nNotice: all data types are objects and must not be changed; the output must contain all objects. I want to avoid to convert every single column one by one... I would like to do it generally over the whole dataframe possibly.\n\nWhat I tried so far is to do this but without success\n\ndf.str.upper()\n\n"
"How can I flag a row in a dataframe every time a column change its string value?\n\nEx:\n\nInput\n\nColumnA   ColumnB\n1            Blue\n2            Blue\n3            Red\n4            Red\n5            Yellow\n\n\n#  diff won't work here with strings....  only works in numerical values\ndataframe['changed'] = dataframe['ColumnB'].diff()        \n\n\nColumnA   ColumnB      changed\n1            Blue         0\n2            Blue         0\n3            Red          1\n4            Red          0\n5            Yellow       1\n\n"
'I am curious how I can use pandas to read nested json of the following structure:\n\n{\n    "number": "",\n    "date": "01.10.2016",\n    "name": "R 3932",\n    "locations": [\n        {\n            "depTimeDiffMin": "0",\n            "name": "Spital am Pyhrn Bahnhof",\n            "arrTime": "",\n            "depTime": "06:32",\n            "platform": "2",\n            "stationIdx": "0",\n            "arrTimeDiffMin": "",\n            "track": "R 3932"\n        },\n        {\n            "depTimeDiffMin": "0",\n            "name": "Windischgarsten Bahnhof",\n            "arrTime": "06:37",\n            "depTime": "06:40",\n            "platform": "2",\n            "stationIdx": "1",\n            "arrTimeDiffMin": "1",\n            "track": ""\n        },\n        {\n            "depTimeDiffMin": "",\n            "name": "Linz/Donau Hbf",\n            "arrTime": "08:24",\n            "depTime": "",\n            "platform": "1A-B",\n            "stationIdx": "22",\n            "arrTimeDiffMin": "1",\n            "track": ""\n        }\n    ]\n}\n\n\nThis here keeps the array as json. I would rather prefer it to be expanded into columns.\n\npd.read_json("/myJson.json", orient=\'records\')\n\n\nedit\n\nThanks for the first answers.\nI should refine my question:\nA flattening of the nested attributes in the array is not mandatory.\nIt would be ok to just [A, B, C] concatenate the df.locations[\'name\'].\n\nMy file contains multiple JSON objects (1 per line) I would like to keep number, date, name, and locations column. However, I would need to join the locations.\n\nallLocations = ""\nisFirst = True\nfor location in result.locations:\n    if isFirst:\n        isFirst = False\n        allLocations = location[\'name\']\n    else:\n        allLocations += "; " + location[\'name\']\nallLocations\n\n\nMy approach here does not seem to be efficient / pandas style.\n'
'Suppose I have a structured dataframe as follows:\n\ndf = pd.DataFrame({"A":[\'a\',\'a\',\'a\',\'b\',\'b\'],\n                   "B":[1]*5})\n\n\nThe A column has previously been sorted. I wish to find the first row index of where df[df.A!=\'a\']. The end goal is to use this index to break the data frame into groups based on A. \n\nNow I realise that there is a groupby functionality. However, the dataframe is quite large and this is a simplified toy example. Since A has been sorted already, it would be faster if I can just find the 1st index of where df.A!=\'a\'. Therefore it is important that whatever method that you use the scanning stops once the first element is found.\n'
'I have a function that takes in a variable that would work if it is any of the following three types\n\n 1. pandas Series\n 2. numpy array (ndarray)\n 3. python list\n\n\nAny other type should be rejected. What is the most efficient way to check this?\n'
'I am passing a list from flask function to another function, and getting this value error.\n\nMy code at sending end:\n\n@app.route(\'/process\', methods=[\'POST\'])\ndef process():\n    name = request.form[\'name\']\n    comment = request.form[\'comment\']\n    wickets = request.form[\'wickets\']\n    ga = request.form[\'ga\']\n    ppballs = request.form[\'ppballs\']\n    overs = request.form[\'overs\']\n\n    score = [name,comment,wickets,ga,ppballs,overs]\n    results = []\n    results = eval_score(score)\n    print results\n\n\nReceiver end :\n\ndef ml_model(data):\n    col = pd.DataFrame(data,columns=[\'runs\',\'balls\', \'wickets\', \'ground_average\', \'pp_balls_left\', \'total_overs\'])\n    predicted = predictor(col)\n\n\nTrace of Error:\n\n ...\n line 1598, in dispatch_request\n return self.view_functions[rule.endpoint](**req.view_args)\n\n File "/Users/sbk/guestbook/guestbook.py", line 26, in process\n results = eval_score(score)\n\n File "/Users/sbk/guestbook/eval_score.py", line 6, in eval_score\n col = pd.DataFrame(data,columns=[\'runs\',\'balls\', \'wickets\',  \'ground_average\', \'pp_balls_left\', \'total_overs\'])\n\n File "/Users/sbk/anaconda2/lib/python2.7/site-  packages/pandas/core/frame.py", line 385, in __init__\n copy=copy)\n\n File "/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/frame.py", line 533, in _init_ndarray\n return create_block_manager_from_blocks([values], [columns, index])\n\n File "/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/internals.py", line 4631, in  create_block_manager_from_blocks\n construction_error(tot_items, blocks[0].shape[1:], axes, e)\n\n File "/Users/sbk/anaconda2/lib/python2.7/site-packages/pandas/core/internals.py", line 4608, in construction_error\n Open an interactive python shell in this framepassed, implied))\n\n\nPlease let me know where I am going wrong.\n'
'There are many questions (1, 2, 3) dealing with counting values in a single series.\n\nHowever, there are fewer questions looking at the best way to count combinations of two or more series. Solutions are presented (1, 2), but when and why one should use each is not discussed.\n\nBelow is some benchmarking for three potential methods. I have two specific questions:\n\n\nWhy is grouper more efficient than count? I expected count to be the more efficient, as it is implemented in C. The superior performance of grouper persists even if number of columns is increased from 2 to 4.\nWhy does value_counter underperform grouper by so much? Is this due to the cost of constructing a list, or series from list?\n\n\nI understand the outputs are different, and this should also inform choice. For example, filtering by count is more efficient with contiguous numpy arrays versus a dictionary comprehension:\n\nx, z = grouper(df), count(df)\n%timeit x[x.values &gt; 10]                        # 749µs\n%timeit {k: v for k, v in z.items() if v &gt; 10}  # 9.37ms\n\n\nHowever, the focus of my question is on performance of building comparable results in a series versus dictionary. My C knowledge is limited, yet I would appreciate any answer which can point to the logic underlying these methods.\n\nBenchmarking code\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\nnp.random.seed(0)\n\nm, n = 1000, 100000\n\ndf = pd.DataFrame({\'A\': np.random.randint(0, m, n),\n                   \'B\': np.random.randint(0, m, n)})\n\ndef grouper(df):\n    return df.groupby([\'A\', \'B\'], sort=False).size()\n\ndef value_counter(df):\n    return pd.Series(list(zip(df.A, df.B))).value_counts(sort=False)\n\ndef count(df):\n    return Counter(zip(df.A.values, df.B.values))\n\nx = value_counter(df).to_dict()\ny = grouper(df).to_dict()\nz = count(df)\n\nassert (x == y) &amp; (y == z), "Dictionary mismatch!"\n\nfor m, n in [(100, 10000), (1000, 10000), (100, 100000), (1000, 100000)]:\n\n    df = pd.DataFrame({\'A\': np.random.randint(0, m, n),\n                       \'B\': np.random.randint(0, m, n)})\n\n    print(m, n)\n\n    %timeit grouper(df)\n    %timeit value_counter(df)\n    %timeit count(df)\n\n\nBenchmarking results\n\nRun on python 3.6.2, pandas 0.20.3, numpy 1.13.1\n\nMachine specs: Windows 7 64-bit, Dual-Core 2.5 GHz, 4GB RAM.\n\nKey: g = grouper, v = value_counter, c = count.\n\nm           n        g        v       c\n100     10000     2.91    18.30    8.41\n1000    10000     4.10    27.20    6.98[1]\n100    100000    17.90   130.00   84.50\n1000   100000    43.90   309.00   93.50\n\n\n1 This is not a typo.\n'
"import pandas as pd\ndf = pd.read_csv('https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0')\npercent= 100*(len(df.loc[:,df.isnull().sum(axis=0)&gt;=1 ].index) / len(df.index))\nprint(round(percent,2))\n\n\ninput is https://query.data.world/s/Hfu_PsEuD1Z_yJHmGaxWTxvkz7W_b0\n\nand the output should be\n\nOrd_id                 0.00\nProd_id                0.00\nShip_id                0.00\nCust_id                0.00\nSales                  0.24\nDiscount               0.65\nOrder_Quantity         0.65\nProfit                 0.65\nShipping_Cost          0.65\nProduct_Base_Margin    1.30\ndtype: float64\n\n"
"I have a time-series that is not recognized as a DatetimeIndex despite being indexed by standard YYYY-MM-DD strings with valid dates. Coercing them to a valid DatetimeIndex seems to be inelegant enough to make me think I'm doing something wrong.\n\nI read in (someone else's lazily formatted) data that contains invalid datetime values and remove these invalid observations.\n\nIn [1]: df = pd.read_csv('data.csv',index_col=0)\nIn [2]: print df['2008-02-27':'2008-03-02']\nOut[2]: \n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-02-30   0\n2008-02-31   0\n2008-03-01   0\n2008-03-02  17\n\nIn [3]: def clean_timestamps(df):\n    # remove invalid dates like '2008-02-30' and '2009-04-31'\n    to_drop = list()\n    for d in df.index:\n        try:\n            datetime.date(int(d[0:4]),int(d[5:7]),int(d[8:10]))\n        except ValueError:\n            to_drop.append(d)\n    df2 = df.drop(to_drop,axis=0)\n    return df2\n\nIn [4]: df2 = clean_timestamps(df)\nIn [5] :print df2['2008-02-27':'2008-03-02']\nOut[5]:\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\n\nThis new index is still only recognized as a 'object' dtype rather than a DatetimeIndex. \n\nIn [6]: df2.index\nOut[6]: Index([2008-01-01, 2008-01-02, 2008-01-03, ..., 2012-11-27, 2012-11-28,\n   2012-11-29], dtype=object)\n\n\nReindexing produces NaNs because they're different dtypes.\n\nIn [7]: i = pd.date_range(start=min(df2.index),end=max(df2.index))\nIn [8]: df3 = df2.reindex(index=i,columns=['count'])\nIn [9]: df3['2008-02-27':'2008-03-02']\nOut[9]: \n            count\n2008-02-27 NaN\n2008-02-28 NaN\n2008-02-29 NaN\n2008-03-01 NaN\n2008-03-02 NaN\n\n\nI create a fresh dataframe with the appropriate index, drop the data to a dictionary, then populate the new dataframe based on the dictionary values (skipping missing values).\n\nIn [10]: df3 = pd.DataFrame(columns=['count'],index=i)\nIn [11]: values = dict(df2['count'])\nIn [12]: for d in i:\n    try:\n        df3.set_value(index=d,col='count',value=values[d.isoformat()[0:10]])\n    except KeyError:\n        pass\nIn [13]: print df3['2008-02-27':'2008-03-02']\nOut[13]: \n\n             count\n2008-02-27  20\n2008-02-28   0\n2008-02-29  27\n2008-03-01   0\n2008-03-02  17\n\nIn [14]: df3.index\nOut[14];\n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2008-01-01 00:00:00, ..., 2012-11-29 00:00:00]\nLength: 1795, Freq: D, Timezone: None\n\n\nThis last part of setting values based on lookups to a dictionary keyed by strings seems especially hacky and makes me think I've missed something important.\n"
'A similar question is asked here:\nPython : Getting the Row which has the max value in groups using groupby\n\nHowever, I just need one record per group even if there are more than one record with maximum value in that group. \n\nIn the example below, I need one record for "s2". For me it doesn\'t matter which one. \n\n&gt;&gt;&gt; df = DataFrame({\'Sp\':[\'a\',\'b\',\'c\',\'d\',\'e\',\'f\'], \'Mt\':[\'s1\', \'s1\', \'s2\',\'s2\',\'s2\',\'s3\'], \'Value\':[1,2,3,4,5,6], \'count\':[3,2,5,10,10,6]})\n&gt;&gt;&gt; df\n   Mt Sp  Value  count\n0  s1  a      1      3\n1  s1  b      2      2\n2  s2  c      3      5\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n&gt;&gt;&gt; idx = df.groupby([\'Mt\'])[\'count\'].transform(max) == df[\'count\']\n&gt;&gt;&gt; df[idx]\n   Mt Sp  Value  count\n0  s1  a      1      3\n3  s2  d      4     10\n4  s2  e      5     10\n5  s3  f      6      6\n&gt;&gt;&gt; \n\n'
"I'm new to pandas, therefore perhaps I'm asking a very stupid question. Normally initialization of data frame in pandas would be column-wise, where I put in dict with key of column names and values of list-like object with same length.\n\nBut I would love to initialize row-wise without dynamically concat-ing rows. Say I have a list of namedtuple, is there a optimized operation that will give me a pandas data frame directly from it?\n\nMany many thanks\n"
"Given a dataframe, I want to get the duplicated indexes, which do not have duplicate values in the columns, and see which values are different.\n\nSpecifically, I have this dataframe:\n\nimport pandas as pd\nwget https://www.dropbox.com/s/vmimze2g4lt4ud3/alt_exon_repeatmasker_intersect.bed\nalt_exon_repeatmasker = pd.read_table('alt_exon_repeatmasker_intersect.bed', header=None, index_col=3)\n\nIn [74]: alt_exon_repeatmasker.index.is_unique\nOut[74]: False\n\n\nAnd some of the indexes have duplicate values in the 9th column (the type of DNA repetitive element in this location), and I want to know what are the different types of repetitive elements for individual locations (each index = a genome location).\n\nI'm guessing this will require some kind of groupby and hopefully some groupby ninja can help me out.\n\nTo simplify even further, if we only have the index and the repeat type,\n\ngenome_location1    MIR3\ngenome_location1    AluJb\ngenome_location2    Tigger1\ngenome_location3    AT_rich\n\n\nSo the output I'd like to see all duplicate indexes and their repeat types, as such:\n\ngenome_location1    MIR3\ngenome_location1    AluJb\n\n\nEDIT: added toy example\n"
'I have something like this in my code:\n\ndf2 = df[df[\'A\'].str.contains("Hello|World")]\n\nHowever, I want all the rows that don\'t contain either of Hello or World.  How do I most efficiently reverse this?\n'
"How can I force a suffix on a merge or join.  I understand it's possible to provide one if there is a collision but in my case I'm merging df1 with df2 which doesn't cause any collision but then merging again on df2 which uses the suffixes but I would prefer for each merge to have a suffix because it gets confusing if I do different combinations as you could imagine.\n"
"I have data:\n\n                             Symbol      bid      ask\nTimestamp                                            \n2014-01-01 21:55:34.378000  EUR/USD  1.37622  1.37693\n2014-01-01 21:55:40.410000  EUR/USD  1.37624  1.37698\n2014-01-01 21:55:47.210000  EUR/USD  1.37619  1.37696\n2014-01-01 21:55:57.963000  EUR/USD  1.37616  1.37696\n2014-01-01 21:56:03.117000  EUR/USD  1.37616  1.37694\n\n\nThe timestamp is of GMT. Is there a way to convert that to Eastern?\n\nNote when I do:\n\ndata.index\n\n\nI get output:\n\n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2014-01-01 21:55:34.378000, ..., 2014-01-01 21:56:03.117000]\nLength: 5, Freq: None, Timezone: None\n\n"
"I have a df in pandas\n\nimport pandas as pd\ndf = pd.DataFrame(['AA', 'BB', 'CC'], columns = ['value'])\n\n\nI want to iterate over rows in df. For each row i want rows value and next rows value\nSomething like(it does not work):\n\nfor i, row in df.iterrows():\n     print row['value']\n     i1, row1 = next(df.iterrows())\n     print row1['value']\n\n\nAs a result I want \n\n'AA'\n'BB'\n'BB'\n'CC'\n'CC'\n*Wrong index error here  \n\n\nAt this point i have mess way to solve this\n\nfor i in range(0, df.shape[0])\n   print df.irow(i)['value']\n   print df.irow(i+1)['value']\n\n\nIs there more efficient way to solve this issue? \n"
'I am trying to build a DataFrame in pandas, using the results of a very basic query to ElasticSearch.  I am getting the Data I need, but its a matter of slicing the results in a way to build the proper data frame. I really only care about getting the timestamp, and path, of each result.  I have tried a few different es.search patterns.\n\nCode:\n\nfrom datetime import datetime\nfrom elasticsearch import Elasticsearch\nfrom pandas import DataFrame, Series\nimport pandas as pd\nimport matplotlib.pyplot as plt\nes = Elasticsearch(host="192.168.121.252")\nres = es.search(index="_all", doc_type=\'logs\', body={"query": {"match_all": {}}}, size=2, fields=(\'path\',\'@timestamp\'))\n\n\nThis gives 4 chunks of data.  [u\'hits\', u\'_shards\', u\'took\', u\'timed_out\']. My results are inside the hits.\n\nres[\'hits\'][\'hits\']\nOut[47]: \n[{u\'_id\': u\'a1XHMhdHQB2uV7oq6dUldg\',\n  u\'_index\': u\'logstash-2014.08.07\',\n  u\'_score\': 1.0,\n  u\'_type\': u\'logs\',\n  u\'fields\': {u\'@timestamp\': u\'2014-08-07T12:36:00.086Z\',\n   u\'path\': u\'app2.log\'}},\n {u\'_id\': u\'TcBvro_1QMqF4ORC-XlAPQ\',\n  u\'_index\': u\'logstash-2014.08.07\',\n  u\'_score\': 1.0,\n  u\'_type\': u\'logs\',\n  u\'fields\': {u\'@timestamp\': u\'2014-08-07T12:36:00.200Z\',\n   u\'path\': u\'app1.log\'}}]\n\n\nThe only things I care about, are getting the timestamp, and path for each hit.\n\nres[\'hits\'][\'hits\'][0][\'fields\']\nOut[48]: \n{u\'@timestamp\': u\'2014-08-07T12:36:00.086Z\',\n u\'path\': u\'app1.log\'}\n\n\nI can not for the life of me figure out who to get that result, into a dataframe in pandas.  So for the 2 results I have returned, I would expect a dataframe like.\n\n   timestamp                   path\n0  2014-08-07T12:36:00.086Z    app1.log\n1  2014-08-07T12:36:00.200Z    app2.log\n\n'
"I have a dataframe (in Python 2.7, pandas 0.15.0):\n\ndf=\n       A    B               C\n0    NaN   11             NaN\n1    two  NaN  ['foo', 'bar']\n2  three   33             NaN\n\n\nI want to apply a simple function for rows that does not contain NULL values in a specific column. My function is as simple as possible:\n\ndef my_func(row):\n    print row\n\n\nAnd my apply code is the following:\n\ndf[['A','B']].apply(lambda x: my_func(x) if(pd.notnull(x[0])) else x, axis = 1)\n\n\nIt works perfectly. If I want to check column 'B' for NULL values the pd.notnull() works perfectly as well. But if I select column 'C' that contains list objects:\n\ndf[['A','C']].apply(lambda x: my_func(x) if(pd.notnull(x[1])) else x, axis = 1)\n\n\nthen I get the following error message: ValueError: ('The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()', u'occurred at index 1')\n\nDoes anybody know why pd.notnull() works only for integer and string columns but not for 'list columns'?\n\nAnd is there a nicer way to check for NULL values in column 'C' instead of this:\n\ndf[['A','C']].apply(lambda x: my_func(x) if(str(x[1]) != 'nan') else x, axis = 1)\n\n\nThank you!\n"
'I am running a basic script that loops over a nested dictionary, grabs data from each record, and appends it to a Pandas DataFrame. The data looks something like this:\n\ndata = {"SomeCity": {"Date1": {record1, record2, record3, ...}, "Date2": {}, ...}, ...}\n\n\nIn total it has a few million records. The script itself looks like this:\n\ncity = ["SomeCity"]\ndf = DataFrame({}, columns=[\'Date\', \'HouseID\', \'Price\'])\nfor city in cities:\n    for dateRun in data[city]:\n        for record in data[city][dateRun]:\n            recSeries = Series([record[\'Timestamp\'], \n                                record[\'Id\'], \n                                record[\'Price\']],\n                                index = [\'Date\', \'HouseID\', \'Price\'])\n            FredDF = FredDF.append(recSeries, ignore_index=True)\n\n\nThis runs painfully slow, however. Before I look for a way to parallelize it, I just want to make sure I\'m not missing something obvious that would make this perform faster as it is, as I\'m still quite new to Pandas.\n'
'dataset is pandas dataframe. This is sklearn.cluster.KMeans\n\n km = KMeans(n_clusters = n_Clusters)\n\n km.fit(dataset)\n\n prediction = km.predict(dataset)\n\n\nThis is how I decide which entity belongs to which cluster:\n\n for i in range(len(prediction)):\n     cluster_fit_dict[dataset.index[i]] = prediction[i]\n\n\nThis is how dataset looks:\n\n A 1 2 3 4 5 6\n B 2 3 4 5 6 7\n C 1 4 2 7 8 1\n ...\n\n\nwhere A,B,C are indices\n\nIs this the correct way of using k-means?\n'
"I have output file like this from a pandas function. \n\nSeries([], name: column, dtype: object)\n311     race\n317     gender\nName: column, dtype: object\n\n\nI'm trying to get an output with just the second column, i.e., \n\nrace\ngender\n\n\nby deleting top and bottom rows, first column. How do I do that?\n"
'I\'m trying to create a Trading calendar using Pandas. I\'m able to create a cal instance based on the USFederalHolidayCalendar. The USFederalHolidayCalendar is not consistent with the Trading calendar in that the Trading calendar doesn\'t include Columbus Day and Veteran\'s Day. However, the Trading calendar includes Good Friday (not included in the USFederalHolidayCalendar).  Everything except for the last line in following code works:\n\nfrom pandas.tseries.holiday import get_calendar, HolidayCalendarFactory, GoodFriday\nfrom datetime import datetime\n\ncal = get_calendar(\'USFederalHolidayCalendar\')  # Create calendar instance\ncal.rules.pop(7)                                # Remove Veteran\'s Day rule\ncal.rules.pop(6)                                # Remove Columbus Day rule\ntradingCal = HolidayCalendarFactory(\'TradingCalendar\', cal, GoodFriday)\n\n\nThe tradingCal instance seems to work in that I\'m able to view the Holiday rules.\n\nIn[10]: tradingCal.rules\nOut[10]: \n[Holiday: Labor Day (month=9, day=1, offset=&lt;DateOffset: kwds={\'weekday\': MO(+1)}&gt;),\n Holiday: Presidents Day (month=2, day=1, offset=&lt;DateOffset: kwds={\'weekday\': MO(+3)}&gt;),\n Holiday: Good Friday (month=1, day=1, offset=[&lt;Easter&gt;, &lt;-2 * Days&gt;]),\n Holiday: Dr. Martin Luther King Jr. (month=1, day=1, offset=&lt;DateOffset: kwds={\'weekday\': MO(+3)}&gt;),\n Holiday: New Years Day (month=1, day=1, observance=&lt;function nearest_workday at 0x000000000A190BA8&gt;),\n Holiday: Thanksgiving (month=11, day=1, offset=&lt;DateOffset: kwds={\'weekday\': TH(+4)}&gt;),\n Holiday: July 4th (month=7, day=4, observance=&lt;function nearest_workday at 0x000000000A190BA8&gt;),\n Holiday: Christmas (month=12, day=25, observance=&lt;function nearest_workday at 0x000000000A190BA8&gt;),\n Holiday: MemorialDay (month=5, day=31, offset=&lt;DateOffset: kwds={\'weekday\': MO(-1)}&gt;)]\n\n\nWhen I try to list the holidays in a date range, I get the following error:\n\nIn[11]: tradingCal.holidays(datetime(2014, 12, 31), datetime(2016, 12, 31))\nTraceback (most recent call last):\n  File "C:\\Python27\\lib\\site-packages\\IPython\\core\\interactiveshell.py", line 3035, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File "&lt;ipython-input-12-2708cd2db7a0&gt;", line 1, in &lt;module&gt;\n    tradingCal.holidays(datetime(2014, 12, 31), datetime(2016, 12, 31))\nTypeError: unbound method holidays() must be called with TradingCalendar instance as first argument (got datetime instance instead)\n\n\nAny ideas?\n'
'I want to append (merge) all the csv files in a folder using Python pandas.\n\nFor example: Say folder has two csv files test1.csv and test2.csv as follows:\n\nA_Id    P_Id    CN1         CN2         CN3\nAAA     111     702         709         740\nBBB     222     1727        1734        1778\n\n\nand\n\nA_Id    P_Id    CN1         CN2         CN3\nCCC     333     710        750          750\nDDD     444     180        734          778\n\n\nSo the python script I wrote was as follows:\n\n#!/usr/bin/python\nimport pandas as pd\nimport glob\n\nall_data = pd.DataFrame()\nfor f in glob.glob("testfolder/*.csv"):\n    df = pd.read_csv(f)\n    all_data = all_data.append(df)\n\nall_data.to_csv(\'testfolder/combined.csv\')\n\n\nThough the combined.csv seems to have all the appended rows, it looks as follows:  \n\n      CN1       CN2         CN3    A_Id    P_Id\n  0   710      750         750     CCC     333\n  1   180       734         778     DDD     444     \n  0   702       709         740     AAA     111\n  1  1727       1734        1778    BBB     222\n\n\nWhere as it should look like this:\n\nA_ID   P_Id   CN1    CN2    CN2\nAAA    111    702    709    740\nBBB    222    1727   1734   1778\nCCC    333    110    356    123\nDDD    444    220    256    223\n\n\n\nWhy are the first two columns moved to the end?\nWhy is it appending in the first line rather than at the last line?\n\n\nWhat am I missing? And how can I get get of 0s and 1s in the first column?\n\nP.S: Since these are large csv files, I thought of using pandas.\n'
'I was wondering if it is possible to create a Seaborn count plot, but instead of actual counts on the y-axis, show the relative frequency (percentage) within its group (as specified with the hue parameter).\n\nI sort of fixed this with the following approach, but I can\'t imagine this is the easiest approach:\n\n# Plot percentage of occupation per income class\ngrouped = df.groupby([\'income\'], sort=False)\noccupation_counts = grouped[\'occupation\'].value_counts(normalize=True, sort=False)\n\noccupation_data = [\n    {\'occupation\': occupation, \'income\': income, \'percentage\': percentage*100} for \n    (income, occupation), percentage in dict(occupation_counts).items()\n]\n\ndf_occupation = pd.DataFrame(occupation_data)\n\np = sns.barplot(x="occupation", y="percentage", hue="income", data=df_occupation)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n\n\nResult:\n\n\n\nI\'m using the well known adult data set from the UCI machine learning repository. The pandas dataframe is created like this:\n\n# Read the adult dataset\ndf = pd.read_csv(\n    "data/adult.data",\n    engine=\'c\',\n    lineterminator=\'\\n\',\n\n    names=[\'age\', \'workclass\', \'fnlwgt\', \'education\', \'education_num\',\n           \'marital_status\', \'occupation\', \'relationship\', \'race\', \'sex\',\n           \'capital_gain\', \'capital_loss\', \'hours_per_week\',\n           \'native_country\', \'income\'],\n    header=None,\n    skipinitialspace=True,\n    na_values="?"\n)\n\n\nThis question is sort of related, but does not make use of the hue parameter. And in my case I cannot just change the labels on the y-axis, because the height of the bar must depend on the group.\n'
"I'm trying to read a CSV file from a private S3 bucket to a pandas dataframe:\n\ndf = pandas.read_csv('s3://mybucket/file.csv')\n\n\nI can read a file from a public bucket, but reading a file from a private bucket results in HTTP 403: Forbidden error.\n\nI have configured the AWS credentials using aws configure.\n\nI can download a file from a private bucket using boto3, which uses aws credentials. It seems that I need to configure pandas to use AWS credentials, but don't know how.\n"
'Importing pandas didn\'t throw the error, but rather trying to read a picked pandas dataframe as such:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nsns.set(style="white")\n\ncontrol_data = pd.read_pickle(\'null_report.pickle\')\ntest_data = pd.read_pickle(\'test_report.pickle\')\n\n\nThe traceback is 165 lines with three concurrent exceptions (whatever that means).  Is read_pickle not compatible with pandas version 17.1 I\'m running?  How do I unpickle my dataframe for use?\n\nBelow is a copy of the traceback:\n\nImportError                               Traceback (most recent call last)\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in try_read(path, encoding)\n     45             with open(path, \'rb\') as fh:\n---&gt; 46                 return pkl.load(fh)\n     47         except (Exception) as e:\n\nImportError: No module named \'pandas.indexes\'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in try_read(path, encoding)\n     51                 with open(path, \'rb\') as fh:\n---&gt; 52                     return pc.load(fh, encoding=encoding, compat=False)\n     53 \n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\compat\\pickle_compat.py in load(fh, encoding, compat, is_verbose)\n    115 \n--&gt; 116         return up.load()\n    117     except:\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load(self)\n   1038                 assert isinstance(key, bytes_types)\n-&gt; 1039                 dispatch[key[0]](self)\n   1040         except _Stop as stopinst:\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load_stack_global(self)\n   1342             raise UnpicklingError("STACK_GLOBAL requires str")\n-&gt; 1343         self.append(self.find_class(module, name))\n   1344     dispatch[STACK_GLOBAL[0]] = load_stack_global\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in find_class(self, module, name)\n   1383                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1384         __import__(module, level=0)\n   1385         if self.proto &gt;= 4:\n\nImportError: No module named \'pandas.indexes\'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in read_pickle(path)\n     59     try:\n---&gt; 60         return try_read(path)\n     61     except:\n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in try_read(path, encoding)\n     56                 with open(path, \'rb\') as fh:\n---&gt; 57                     return pc.load(fh, encoding=encoding, compat=True)\n     58 \n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\compat\\pickle_compat.py in load(fh, encoding, compat, is_verbose)\n    115 \n--&gt; 116         return up.load()\n    117     except:\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load(self)\n   1038                 assert isinstance(key, bytes_types)\n-&gt; 1039                 dispatch[key[0]](self)\n   1040         except _Stop as stopinst:\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load_stack_global(self)\n   1342             raise UnpicklingError("STACK_GLOBAL requires str")\n-&gt; 1343         self.append(self.find_class(module, name))\n   1344     dispatch[STACK_GLOBAL[0]] = load_stack_global\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in find_class(self, module, name)\n   1383                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1384         __import__(module, level=0)\n   1385         if self.proto &gt;= 4:\n\nImportError: No module named \'pandas.indexes\'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in try_read(path, encoding)\n     45             with open(path, \'rb\') as fh:\n---&gt; 46                 return pkl.load(fh)\n     47         except (Exception) as e:\n\nImportError: No module named \'pandas.indexes\'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in try_read(path, encoding)\n     51                 with open(path, \'rb\') as fh:\n---&gt; 52                     return pc.load(fh, encoding=encoding, compat=False)\n     53 \n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\compat\\pickle_compat.py in load(fh, encoding, compat, is_verbose)\n    115 \n--&gt; 116         return up.load()\n    117     except:\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load(self)\n   1038                 assert isinstance(key, bytes_types)\n-&gt; 1039                 dispatch[key[0]](self)\n   1040         except _Stop as stopinst:\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load_stack_global(self)\n   1342             raise UnpicklingError("STACK_GLOBAL requires str")\n-&gt; 1343         self.append(self.find_class(module, name))\n   1344     dispatch[STACK_GLOBAL[0]] = load_stack_global\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in find_class(self, module, name)\n   1383                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1384         __import__(module, level=0)\n   1385         if self.proto &gt;= 4:\n\nImportError: No module named \'pandas.indexes\'\n\nDuring handling of the above exception, another exception occurred:\n\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-17-3b05fe7d20a4&gt; in &lt;module&gt;()\n      3 # test_data = np.genfromtxt(fh, usecols=2)\n      4 \n----&gt; 5 control_data = pd.read_pickle(\'null_report.pickle\')\n      6 test_data = pd.read_pickle(\'test_report.pickle\')\n      7 \n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in read_pickle(path)\n     61     except:\n     62         if PY3:\n---&gt; 63             return try_read(path, encoding=\'latin1\')\n     64         raise\n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\io\\pickle.py in try_read(path, encoding)\n     55             except:\n     56                 with open(path, \'rb\') as fh:\n---&gt; 57                     return pc.load(fh, encoding=encoding, compat=True)\n     58 \n     59     try:\n\nC:\\Users\\test\\Anaconda3\\lib\\site-packages\\pandas\\compat\\pickle_compat.py in load(fh, encoding, compat, is_verbose)\n    114         up.is_verbose = is_verbose\n    115 \n--&gt; 116         return up.load()\n    117     except:\n    118         raise\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load(self)\n   1037                     raise EOFError\n   1038                 assert isinstance(key, bytes_types)\n-&gt; 1039                 dispatch[key[0]](self)\n   1040         except _Stop as stopinst:\n   1041             return stopinst.value\n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in load_stack_global(self)\n   1341         if type(name) is not str or type(module) is not str:\n   1342             raise UnpicklingError("STACK_GLOBAL requires str")\n-&gt; 1343         self.append(self.find_class(module, name))\n   1344     dispatch[STACK_GLOBAL[0]] = load_stack_global\n   1345 \n\nC:\\Users\\test\\Anaconda3\\lib\\pickle.py in find_class(self, module, name)\n   1382             elif module in _compat_pickle.IMPORT_MAPPING:\n   1383                 module = _compat_pickle.IMPORT_MAPPING[module]\n-&gt; 1384         __import__(module, level=0)\n   1385         if self.proto &gt;= 4:\n   1386             return _getattribute(sys.modules[module], name)[0]\n\nImportError: No module named \'pandas.indexes\'\n\n\nI also tried loading the pickle file from pickle directly:\n\nvia_pickle = pickle.load( open( \'null_report.pickle\', "rb" ) )\n\nand got the same error:\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-23-ba2e3adae1c4&gt; in &lt;module&gt;()\n      1 \n----&gt; 2 via_pickle = pickle.load( open( \'null_report.pickle\', "rb" ) )\n      3 \n      4 # control_data = pd.read_pickle(\'null_report.pickle\')\n      5 # test_data = pd.read_pickle(\'test_report.pickle\')\n\nImportError: No module named \'pandas.indexes\'\n\n'
"I'm looking for an efficient way to convert a series to a tuple of its index with its values.\n\ns = pd.Series([1, 2, 3], ['a', 'b', 'c'])\n\n\nI want an array, list, series, some iterable:\n\n[(1, 'a'), (2, 'b'), (3, 'c')]\n\n"
"Suppose I've data similar to following:\n  index id   name  value  value2  value3  data1  val5\n    0  345  name1    1      99      23     3      66\n    1   12  name2    1      99      23     2      66\n    5    2  name6    1      99      23     7      66\n\nHow can we drop all those columns like (value, value2, value3) where all rows have the same values, in one command or couple of commands using python?\nConsider we have many columns similar to value, value2, value3...value200.\nOutput:\n   index    id  name   data1\n       0   345  name1    3\n       1    12  name2    2\n       5     2  name6    7\n\n"
"I have a dataframe for values form a file by which I have grouped by two columns, which return a count of the aggregation. Now I want to sort by the max count value, however I get the following error:\n\n\n  KeyError: 'count'\n\n\nLooks the group by agg count column is some sort of index so not sure how to do this, I'm a beginner to Python and Panda.\nHere's the actual code, please let me know if you need more detail:\n\ndef answer_five():\n    df = census_df#.set_index(['STNAME'])\n    df = df[df['SUMLEV'] == 50]\n    df = df[['STNAME','CTYNAME']].groupby(['STNAME']).agg(['count']).sort(['count'])\n    #df.set_index(['count'])\n    print(df.index)\n    # get sorted count max item\n    return df.head(5)\n\n"
'I am using the Jupyter notebook with Python 3 selected. On the first line of a cell I am entering:\n\nimport pandas as pd\n\n\nThe error I get from the notebook is, ImportError: No module named \'pandas\'. How can I install pandas to the jupyter notebook? The computer I launched the Jupyter notebook from definitely has pandas.\n\nI tried doing:\n\n!pip install pandas\n\n\nAnd it says it is already installed but for Python 2.7 at the bottom. My script shows it is a Python 3 script at the top though.\n\nWhen I do echo $PATH in Ubuntu is shows that \'/home/user/anaconda2/bin\' is on the first entry. I think I may need to change this to be anaconda3?\n\nUPDATE: When I try and launch a Python3 script through jupyter the command line which launched Jupyter gives me the error "ImportError: No module named \'IPython.paths\'. Then there is a timeout waiting for \'kernel_info\' reply. Additionally, I tried removing anaconda but still experience the same error. I have tried to make so many quick fixes now, that I am not sure what the next step is to get this working.\n'
"I have a Python dataframe with about 1,500 rows and 15 columns. With one specific column I would like to remove the first 3 characters of each row. As a simple example here is a dataframe:\n\nimport pandas as pd\n\nd = {\n    'Report Number':['8761234567', '8679876543','8994434555'],\n    'Name'         :['George', 'Bill', 'Sally']\n     }\n\nd = pd.DataFrame(d)\n\n\nI would like to remove the first three characters from each field in the Report Number column of dataframe d.\n"
"I think this should be simple but what I've seen are techniques that involve iterating over a dataframe date fields to determine the diff between two dates. And I'm having trouble with it. I'm familiar with MSSQL DATEDIFF so I thought Pandas datetime would have something similar. I perhaps it does but I'm missing it. \n\nIs there a Pandonic way of determing the number of months as an integer between two dates (datetime) without the need to iterate? Keeping in mind that there potentially are millions of rows so performance is a consideration.\n\nThe dates are datetime objects and the result would like this - new column being Month:\n\nDate1           Date2         Months\n2016-04-07      2017-02-01    11\n2017-02-01      2017-03-05    1\n\n"
"In Python, I have a pandas DataFrame similar to the following:\n\nItem | shop1 | shop2 | shop3 | Category\n------------------------------------\nShoes| 45    | 50    | 53    | Clothes\nTV   | 200   | 300   | 250   | Technology\nBook | 20    | 17    | 21    | Books\nphone| 300   | 350   | 400   | Technology\n\n\nWhere shop1, shop2 and shop3 are the costs of every item in different shops.\nNow, I need to  return a DataFrame, after some data cleaning, like this one:\n\nCategory (index)| size| sum| mean | std\n----------------------------------------\n\n\nwhere size is the number of items in each Category and sum, mean and std are related to the same functions applied to the 3 shops. How can I do these operations with the split-apply-combine pattern (groupby, aggregate, apply,...) ?\n\nCan someone help me out? I'm going crazy with this one...thank you!\n"
'I have a dataframe like this:\n\nRecID| A  |B\n----------------\n1    |a   | abc \n2    |b   | cba \n3    |c   | bca\n4    |d   | bac \n5    |e   | abc \n\n\nAnd want to create another column, C, out of A and B such that for the same row, if the string in column A is contained in the string of column B, then C = True and if not then C = False.\n\nThe example output I am looking for is this:\n\nRecID| A  |B    |C \n--------------------\n1    |a   | abc |True\n2    |b   | cba |True\n3    |c   | bca |True\n4    |d   | bac |False\n5    |e   | abc |False\n\n\nIs there a way to do this in pandas quickly and without using a loop? Thanks\n'
"I have two dataframes in python. I want to update rows in first dataframe using  matching values from another dataframe. Second dataframe serves as an override. \n\nHere is an example with same data and code:  \n\nDataFrame 1 : \n\n\n\nDataFrame 2: \n\n\n\nI want to update update dataframe 1 based on matching code and name. In this example Dataframe 1 should be updated as below: \n\n\n\nNote : Row with Code =2 and Name= Company2 is updated with value 1000 (coming from Dataframe 2) \n\nimport pandas as pd\n\ndata1 = {\n         'Code': [1, 2, 3],\n         'Name': ['Company1', 'Company2', 'Company3'],\n         'Value': [200, 300, 400],\n\n    }\ndf1 = pd.DataFrame(data1, columns= ['Code','Name','Value'])\n\ndata2 = {\n         'Code': [2],\n         'Name': ['Company2'],\n         'Value': [1000],\n    }\n\ndf2 = pd.DataFrame(data2, columns= ['Code','Name','Value'])\n\n\nAny pointers or hints? \n"
"I often use Pandas mask and where methods for cleaner logic when updating values in a series conditionally. However, for relatively performance-critical code I notice a significant performance drop relative to numpy.where.\n\nWhile I'm happy to accept this for specific cases, I'm interested to know:\n\n\nDo Pandas mask / where methods offer any additional functionality, apart from inplace / errors / try-cast parameters? I understand those 3 parameters but rarely use them. For example, I have no idea what the level parameter refers to.\nIs there any non-trivial counter-example where mask / where outperforms numpy.where? If such an example exists, it could influence how I choose appropriate methods going forwards.\n\n\nFor reference, here's some benchmarking on Pandas 0.19.2 / Python 3.6.0:\n\nnp.random.seed(0)\n\nn = 10000000\ndf = pd.DataFrame(np.random.random(n))\n\nassert (df[0].mask(df[0] &gt; 0.5, 1).values == np.where(df[0] &gt; 0.5, 1, df[0])).all()\n\n%timeit df[0].mask(df[0] &gt; 0.5, 1)       # 145 ms per loop\n%timeit np.where(df[0] &gt; 0.5, 1, df[0])  # 113 ms per loop\n\n\nThe performance appears to diverge further for non-scalar values:\n\n%timeit df[0].mask(df[0] &gt; 0.5, df[0]*2)       # 338 ms per loop\n%timeit np.where(df[0] &gt; 0.5, df[0]*2, df[0])  # 153 ms per loop\n\n"
'I have a pandas DataFrame and I want to plot a bar chart that includes a legend.\n\nimport pylab as pl\nfrom pandas import *\n\nx = DataFrame({"Alpha": Series({1: 1, 2: 3, 3:2.5}), "Beta": Series({1: 2, 2: 2, 3:3.5})})\n\n\nIf I call plot directly, then it puts the legend above the plot:\n\nx.plot(kind="bar")\n\n\nIf I turn of the legend in the plot and try to add it later, then it doesn\'t retain the colors associated with the two columns in the DataFrame (see below):\n\nx.plot(kind="bar", legend=False)\nl = pl.legend((\'Alpha\',\'Beta\'), loc=\'best\')\n\n\nWhat\'s the right way to include a legend in a matplotlib plot from a Pandas DataFrame?\n\n'
"How can I format IPython html display of pandas dataframes so that\n\n\nnumbers are right justified\nnumbers have commas as thousands separator\nlarge floats have no decimal places\n\n\nI understand that numpy has the facility of set_printoptions where I can do:\n\nint_frmt:lambda x : '{:,}'.format(x)\nnp.set_printoptions(formatter={'int_kind':int_frmt})\n\n\nand similarly for other data types. \n\nBut IPython does not pick up these formatting options when displaying dataframes in html. I still need to have\n\npd.set_option('display.notebook_repr_html', True)\n\n\nbut with 1, 2, 3 as in above.\n\nEdit: Below is my solution for 2 &amp; 3 ( not sure this is the best way ), but I still need to figure out how to make number columns right justified.\n\nfrom IPython.display import HTML\nint_frmt = lambda x: '{:,}'.format(x)\nfloat_frmt = lambda x: '{:,.0f}'.format(x) if x &gt; 1e3 else '{:,.2f}'.format(x)\nfrmt_map = {np.dtype('int64'):int_frmt, np.dtype('float64'):float_frmt}\nfrmt = {col:frmt_map[df.dtypes[col]] for col in df.columns if df.dtypes[col] in frmt_map.keys()}\nHTML(df.to_html(formatters=frmt))\n\n"
'If I have a pandas DataFrame object, how do I simply access a cell?  In R, assuming my data.frame is called df, I can access the 3rd row and 4th column by\n\ndf[3,4]\n\n\nWhat is the equivalent in python?\n'
"I have a dataframe that has characters in it - I want a boolean result by row that tells me if all columns for that row have the same value.\n\nFor example, I have\n\ndf = [  a   b   c   d\n\n0  'C'   'C'   'C'   'C' \n\n1  'C'   'C'   'A'   'A'\n\n2  'A'   'A'   'A'   'A' ]\n\n\nand I want the result to be\n\n0  True\n\n1  False\n\n2  True\n\n\nI've tried .all but it seems I can only check if all are equal to one letter.  The only other way I can think of doing it is by doing a unique on each row and see if that equals 1?  Thanks in advance.\n"
"I'm trying to merge two DataFrames summing columns value.\n\n&gt;&gt;&gt; print(df1)\n   id name  weight\n0   1    A       0\n1   2    B      10\n2   3    C      10\n\n&gt;&gt;&gt; print(df2)\n   id name  weight\n0   2    B      15\n1   3    C      10\n\n\nI need to sum weight values during merging for similar values in the common column.\n\nmerge = pd.merge(df1, df2, how='inner')\n\n\nSo the output will be something like following.\n\n   id name  weight\n1   2    B      25\n2   3    C      20\n\n"
"Note:for simplicity's sake, i'm using a toy example, because copy/pasting dataframes is difficult in stack overflow (please let me know if there's an easy way to do this).\n\nIs there a way to merge the values from one dataframe onto another without getting the _X, _Y columns? I'd like the values on one column to replace all zero values of another column. \n\ndf1: \n\nName   Nonprofit    Business    Education\n\nX      1             1           0\nY      0             1           0   &lt;- Y and Z have zero values for Nonprofit and Educ\nZ      0             0           0\nY      0             1           0\n\ndf2:\n\nName   Nonprofit    Education\nY       1            1     &lt;- this df has the correct values. \nZ       1            1\n\n\n\npd.merge(df1, df2, on='Name', how='outer')\n\nName   Nonprofit_X    Business    Education_X     Nonprofit_Y     Education_Y\nY       1                1          1                1               1\nY      1                 1          1                1               1\nX      1                 1          0               nan             nan   \nZ      1                 1          1                1               1\n\n\nIn a previous post, I tried combine_First and dropna(), but these don't do the job. \n\nI want to replace zeros in df1 with the values in df2. \nFurthermore, I want all rows with the same Names to be changed according to df2. \n\nName    Nonprofit     Business    Education\nY        1             1           1\nY        1             1           1 \nX        1             1           0\nZ        1             0           1\n\n\n(need to clarify: The value in 'Business' column where name = Z should 0.)\n\nMy existing solution does the following:\nI subset based on the names that exist in df2, and then replace those values with the correct value. However, I'd like a less hacky way to do this. \n\npubunis_df = df2\nsdf = df1 \n\nregex = str_to_regex(', '.join(pubunis_df.ORGS))\n\npubunis = searchnamesre(sdf, 'ORGS', regex)\n\nsdf.ix[pubunis.index, ['Education', 'Public']] = 1\nsearchnamesre(sdf, 'ORGS', regex)\n\n"
"I have a line of code: \n\ng = x.groupby('Color')\n\n\nThe colors are Red, Blue, Green, Yellow, Purple, Orange, and Black. How do I return this list? For similar attributes, I use x.Attribute and it works fine, but x.Color doesn't behave the same way.\n"
'I have the following DataFrame, where Track ID is the row index. How can I split the string in the stats column into 5 columns of numbers?\n\nTrack ID    stats\n14.0    (-0.00924175824176, 0.41, -0.742016492568, 0.0036830094242, 0.00251748449963)\n28.0    (0.0411538461538, 0.318230769231, 0.758717081514, 0.00264000622468, 0.0106535783677)\n42.0    (-0.0144351648352, 0.168438461538, -0.80870348637, 0.000816872566404, 0.00316572586742)\n56.0    (0.0343461538462, 0.288730769231, 0.950844962874, 6.1608706775e-07, 0.00337262030771)\n70.0    (0.00905164835165, 0.151030769231, 0.670257006716, 0.0121790506745, 0.00302182567957)\n84.0    (-0.0047967032967, 0.171615384615, -0.552879463981, 0.0500316517755, 0.00217970256969)\n\n'
"I am new to pandas and can't seem to get this to work with merge function:\n\n&gt;&gt;&gt; left       &gt;&gt;&gt; right\n   a  b   c       a  c   d \n0  1  4   9    0  1  7  13\n1  2  5  10    1  2  8  14\n2  3  6  11    2  3  9  15\n3  4  7  12    \n\n\nWith a left join on column a, I would like to update common columns BY THE JOINED KEYS. Note last value in column c is from LEFT table since there is no match. \n\n&gt;&gt;&gt; final       \n   a  b   c   d \n0  1  4   7   13\n1  2  5   8   14\n2  3  6   9   15\n3  4  7   12  NAN \n\n\nHow should I do this with Pandas merge function? Thank you.\n"
"I have a hopefully straightforward question that has been giving me a lot of difficulty for the last 3 hours.  It should be easy.  \n\nHere's the challenge.\n\nI have a pandas dataframe:\n\n+--------------------------+\n|     Col 'X'    Col 'Y'  |\n+--------------------------+\n|     class 1      cat 1  |\n|     class 2      cat 1  |\n|     class 3      cat 2  |\n|     class 2      cat 3  |\n+--------------------------+\n\n\nWhat I am looking to transform the dataframe into:\n\n+------------------------------------------+\n|                  cat 1    cat 2    cat 3 |\n+------------------------------------------+\n|     class 1         1        0        0  |\n|     class 2         1        0        1  |\n|     class 3         0        1        0  |\n+------------------------------------------+\n\n\nWhere the values are value counts.  Anybody have any insight?  Thanks!\n"
'I cleaned 400 excel files and read them into python using pandas and appended all the raw data into one big df.\n\nThen when I try to export it to a csv:\n\ndf.to_csv("path",header=True,index=False)\n\n\nI get this error:\n\nUnicodeEncodeError: \'ascii\' codec can\'t encode character u\'\\xc7\' in position 20: ordinal not in range(128)\n\n\nCan someone suggest a way to fix this and what it means?\n\nThanks\n'
"I'm working turning a list of records with two columns (A and B) into a matrix representation. I have been using the pivot function within pandas, but the result ends up being fairly large. Does pandas support pivoting into a sparse format? I know I can pivot it and then turn it into some kind of sparse representation, but isn't as elegant as I would like. My end goal is to use it as the input for a predictive model.\n\nAlternatively, is there some kind of sparse pivot capability outside of pandas?\n\nedit: here is an example of a non-sparse pivot\n\nimport pandas as pd\nframe=pd.DataFrame()\nframe['person']=['me','you','him','you','him','me']\nframe['thing']=['a','a','b','c','d','d']\nframe['count']=[1,1,1,1,1,1]\n\nframe\n\n  person thing  count\n0     me     a      1\n1    you     a      1\n2    him     b      1\n3    you     c      1\n4    him     d      1\n5     me     d      1\n\nframe.pivot('person','thing')\n\n        count            \nthing       a   b   c   d\nperson                   \nhim       NaN   1 NaN   1\nme          1 NaN NaN   1\nyou         1 NaN   1 NaN\n\n\nThis creates a matrix that could contain all possible combinations of persons and things, but it is not sparse.\n\nhttp://docs.scipy.org/doc/scipy/reference/sparse.html\n\nSparse matrices take up less space because they can imply things like NaN or 0. If I have a very large data set, this pivoting function can generate a matrix that should be sparse due to the large number of NaNs or 0s. I was hoping that I could save a lot of space/memory by generating something that was sparse right off the bat rather than creating a dense matrix and then converting it to sparse.\n"
'I am getting an warning from matplotlib every time I import pandas:\n\n/usr/local/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n\n\n warnings.warn(self.msg_depr % (key, alt_key))\n\n\nWhat is the best way to suppress it? All packages are up-to-date.\n\nConf: OSX with a brew Python 2.7.10 (default, Jul 13 2015, 12:05:58), and pandas==0.17.0 and matplotlib==1.5.0\n'
'So I want to use isin() method with df.query(), to select rows with id in a list: id_list. Similar question was asked before, but they used typical df[df[\'id\'].isin(id_list)] method. I\'m wondering if there is a way to use df.query() instead.\n\ndf = pd.DataFrame({\'a\': list(\'aabbccddeeff\'), \'b\': list(\'aaaabbbbcccc\'),\n                   \'c\': np.random.randint(5, size=12),\n                   \'d\': np.random.randint(9, size=12)})\n\nid_list = ["a", "b", "c"]\n\n\nAnd this yields an error\n\ndf.query(\'a == id_list\')\n\n'
'I\'m trying to mix StringIO and BytesIO with pandas and struggling with some basic stuff.  For example, I can\'t get "output" below to work, whereas "output2" below does work.  But "output" is closer to the real world example I\'m trying to do.  The way in "output2" is from an old pandas example but not really a useful way for me to do it.\n\nimport io   # note for python 3 only\n            # in python2 need to import StringIO\n\noutput = io.StringIO()\noutput.write(\'x,y\\n\')\noutput.write(\'1,2\\n\')\n\noutput2 = io.StringIO("""x,y\n1,2\n""")\n\n\nThey seem to be the same in terms of type and contents:\n\ntype(output) == type(output2)\nOut[159]: True\n\noutput.getvalue() == output2.getvalue()\nOut[160]: True\n\n\nBut no, not the same:\n\noutput == output2\nOut[161]: False\n\n\nMore to the point of the problem I\'m trying to solve:\n\npd.read_csv(output)   # ValueError: No columns to parse from file\npd.read_csv(output2)  # works fine, same as reading from a file\n\n'
"I'm struggle to read a excel sheet with pd.read_excel(). \n\nMy excel table looks like this in it's raw form:\n\n\n\nI expected the dataframe to look like this:\n\n                bar                 baz                 foo\n                one       two       one       two       one       two\n                A         B         C         D         E         F\nbaz one         0.085930 -0.848468  0.911572 -0.705026 -1.284458 -0.602760\n    two         0.385054  2.539314  0.589164  0.765126  0.210199 -0.481789\n    three      -0.352475 -0.975200 -0.403591  0.975707  0.533924 -0.195430\n\n\nis this even possible?\n\nMy failed attempt:\n\nxls_file = pd.read_excel(data_file, header=[0,1,2], index_col=None)\n\n\nLink to the raw excel file:\n\nhttps://www.dropbox.com/s/ek646ab4yb1fvdq/ipsos_excel_tables_type_2_trimed_nosig.xlsx?dl=0\n\n"
"I'm saving pandas DataFrame to_excel using xlsxwriter. I've managed to format all of my data (set column width, font size etc) except for changing header's font and I can't find the way to do it. Here's my example:\n\nimport pandas as pd\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n\n\nThe penultimate line that tries to set format for the header does nothing.\n"
"This is a very simple and practical question. I have the feeling that it must be a silly detail and that there should be similar questions. I wasn't able to find them tho. If someone does I'll happily delete this one.\n\nThe closest I found were these:\npandas: iterating over DataFrame index with loc\n\nHow to select rows within a pandas dataframe based on time only when index is date and time\n\nanyway, the thing is, I have a datetime indexed panda dataframe as follows:\n\nIn[81]: y\nOut[81]: \n            PETR4  CSNA3  VALE5\n2008-01-01    0.0    0.0    0.0\n2008-01-02    1.0    1.0    1.0\n2008-01-03    7.0    7.0    7.0\n\nIn[82]: y.index\nOut[82]: DatetimeIndex(['2008-01-01', '2008-01-02', '2008-01-03'], dtype='datetime64[ns]', freq=None)\n\n\nOddly enough, I can't access its values using none of the following methods:\n\nIn[83]: y[datetime.datetime(2008,1,1)]\nIn[84]: y['2008-1-1']\nIn[85]: y['1/1/2008']\n\n\nI get the KeyError error.\n\nEven more weird is that the following methods DO work:\n\nIn[86]: y['2008']\nOut[86]: \n            PETR4  CSNA3  VALE5\n2008-01-01    0.0    0.0    0.0\n2008-01-02    1.0    1.0    1.0\n2008-01-03    7.0    7.0    7.0\nIn[87]: y['2008-1']\nOut[87]: \n            PETR4  CSNA3  VALE5\n2008-01-01    0.0    0.0    0.0\n2008-01-02    1.0    1.0    1.0\n2008-01-03    7.0    7.0    7.0\n\n\nI'm fairly new to pandas so maybe I'm missing something here?\n"
'I have a very simple csv, with the following data, compressed inside the tar.gz file. I need to read that in dataframe using pandas.read_csv.  \n\n   A  B\n0  1  4\n1  2  5\n2  3  6\n\nimport pandas as pd\npd.read_csv("sample.tar.gz",compression=\'gzip\')\n\n\nHowever, I am getting error:\n\nCParserError: Error tokenizing data. C error: Expected 1 fields in line 440, saw 2\n\n\nFollowing are the set of read_csv commands and the different errors I get with them:\n\npd.read_csv("sample.tar.gz",compression=\'gzip\',  engine=\'python\')\nError: line contains NULL byte\n\npd.read_csv("sample.tar.gz",compression=\'gzip\', header=0)\nCParserError: Error tokenizing data. C error: Expected 1 fields in line 440, saw 2\n\npd.read_csv("sample.tar.gz",compression=\'gzip\', header=0, sep=" ")\nCParserError: Error tokenizing data. C error: Expected 2 fields in line 94, saw 14    \n\npd.read_csv("sample.tar.gz",compression=\'gzip\', header=0, sep=" ", engine=\'python\')\nError: line contains NULL byte\n\n\nWhat\'s going wrong here? How can I fix this?\n'
'I have a pandas DataFrame with mixed data types.  I would like to replace all null values with None (instead of default np.nan).  For some reason, this appears to be nearly impossible.  \n\nIn reality my DataFrame is read in from a csv, but here is a simple DataFrame with mixed data types to illustrate my problem.  \n\ndf = pd.DataFrame(index=[0], columns=range(5))\ndf.iloc[0] = [1, \'two\', np.nan, 3, 4] \n\n\nI can\'t do:\n\n&gt;&gt;&gt; df.fillna(None)\nValueError: must specify a fill method or value\n\n\nnor:\n\n&gt;&gt;&gt; df[df.isnull()] = None\nTypeError: Cannot do inplace boolean setting on mixed-types with a non np.nan value\n\n\nnor:\n\n&gt;&gt;&gt; df.replace(np.nan, None)\nTypeError: cannot replace [nan] with method pad on a DataFrame\n\n\nI used to have a DataFrame with only string values, so I could do:\n\n&gt;&gt;&gt; df[df == ""] = None\n\n\nwhich worked.  But now that I have mixed datatypes, it\'s a no go.\n\nFor various reasons about my code, it would be helpful to be able to use None as my null value.  Is there a way I can set the null values to None?  Or do I just have to go back through my other code and make sure I\'m using np.isnan or pd.isnull everywhere? \n'
"I am trying to groupby a column and compute value counts on another column.\n\nimport pandas as pd\ndftest = pd.DataFrame({'A':[1,1,1,1,1,1,1,1,1,2,2,2,2,2], \n               'Amt':[20,20,20,30,30,30,30,40, 40,10, 10, 40,40,40]})\n\nprint(dftest)\n\n\ndftest looks like\n\n    A  Amt\n0   1   20\n1   1   20\n2   1   20\n3   1   30\n4   1   30\n5   1   30\n6   1   30\n7   1   40\n8   1   40\n9   2   10\n10  2   10\n11  2   40\n12  2   40\n13  2   40\n\n\nperform grouping\n\ngrouper = dftest.groupby('A')\ndf_grouped = grouper['Amt'].value_counts()\n\n\nwhich gives\n\n   A  Amt\n1  30     4\n   20     3\n   40     2\n2  40     3\n   10     2\nName: Amt, dtype: int64\n\n\nwhat I want is to keep top two rows of each group\n\nAlso, I was perplexed by an error when I tried to reset_index\n\ndf_grouped.reset_index()\n\n\nwhich gives following error\n\n\n  df_grouped.reset_index()\n  ValueError: cannot insert Amt, already exists\n\n"
"I have a Pandas DataFrame with a mix of screen names, tweets, fav's etc.  I want find the max value of 'favcount' (which i have already done) and also return the screen name of that 'tweet'\n\ndf = pd.DataFrame()\ndf['timestamp'] = timestamp\ndf['sn'] = sn\ndf['text'] = text\ndf['favcount'] = fav_count\n\n\nprint df\nprint '------'\nprint df['favcount'].max()\n\n\nI cant seem to find anything on this, can anyone help guide me in the right direction?\n"
'I have a DataFrame. Two relevant columns are the following: one is a column of int and another is a column of str. \n\nI understand that if I insert NaN into the int column, Pandas will convert all the int into float because there is no NaN value for an int.\n\nHowever, when I insert None into the str column, Pandas converts all my int to float as well. This doesn\'t make sense to me - why does the value I put in column 2 affect column 1?\n\nHere\'s a simple working example (Python 2):\n\nimport pandas as pd\ndf = pd.DataFrame()\ndf["int"] = pd.Series([], dtype=int)\ndf["str"] = pd.Series([], dtype=str)\ndf.loc[0] = [0, "zero"]\nprint df\nprint\ndf.loc[1] = [1, None]\nprint df\n\n\nThe output is \n\n   int   str\n0    0  zero\n\n   int   str\n0  0.0  zero\n1  1.0   NaN\n\n\nIs there any way to make the output the following:\n\n   int   str\n0    0  zero\n\n   int   str\n0    0  zero\n1    1   NaN\n\n\nwithout recasting the first column to int.\n\n\nI prefer using int instead of float because the actual data in\nthat column are integers. If there\'s not workaround, I\'ll just\nuse float though.\nI prefer not having to recast because in my actual code, I don\'t\nstore the actual dtype.\nI also need the data inserted row-by-row.\n\n'
'What exactly is the function of as_index in groupby in Pandas?\n'
"I have a df with two columns and I want to combine both columns ignoring the NaN values. The catch is that sometimes both columns have NaN values in which case I want the new column to also have NaN. Here's the example: \n\ndf = pd.DataFrame({'foodstuff':['apple-martini', 'apple-pie', None, None, None], 'type':[None, None, 'strawberry-tart', 'dessert', None]})\n\ndf\nOut[10]:\nfoodstuff   type\n0   apple-martini   None\n1   apple-pie   None\n2   None    strawberry-tart\n3   None    dessert\n4   None    None\n\n\nI tried to use fillna and solve this : \n\ndf['foodstuff'].fillna('') + df['type'].fillna('')\n\n\nand I got : \n\n0      apple-martini\n1          apple-pie\n2    strawberry-tart\n3            dessert\n4                   \ndtype: object\n\n\nThe row 4 has become a blank value. What I wan't in this situation is a NaN value since both the combining columns are NaNs. \n\n0      apple-martini\n1          apple-pie\n2    strawberry-tart\n3            dessert\n4            None       \ndtype: object\n\n"
"I feel like this question must have been answered by someone before, but I can't find an answer on stack overflow!\n\nI have a dataframe result that looks like this and I want to remove all the values less than or equal to 10\n\n&gt;&gt;&gt; result\n                       Name              Value      Date\n189                   Sall                19.0  11/14/15\n191                     Sam               10.0  11/14/15\n192                 Richard               21.0  11/14/15\n193                  Ingrid                4.0  11/14/15 \n\n\nThis command works and removes all the values that are 10:\n\ndf2 = result[result['Value'] != 10]\n\n\nBut when I try to add the &lt;= qualifier I get the error message SyntaxError: invalid syntax\n\ndf3 = result[result['Value'] ! &lt;= 10]  \n\n\nI feel like there is probably a really simple solution. Thanks in advance!\n"
'If I have a series that has either NULL or some non-null value.  How can I find the 1st row where the value is not NULL so I can report back the datatype to the user.  If the value is non-null all values are the same datatype in that series.\n\nThanks\n'
'I have a pandas dataframe and a numpy array of values of that dataframe.\nI have the index of a specific column and I already have the row index of an important value. Now I need to get the column name of that particular value from my dataframe.\n\nAfter searching through the documentations, I found out that I can do the opposite but not what I want. \n'
'I am using python csvkit to compare 2 files like this:\n\ndf1 = pd.read_csv(\'input1.csv\', sep=\',\\s+\', delimiter=\',\', encoding="utf-8")\ndf2 = pd.read_csv(\'input2.csv\', sep=\',\\s,\', delimiter=\',\', encoding="utf-8")\ndf3 = pd.merge(df1,df2, on=\'employee_id\', how=\'right\')\ndf3.to_csv(\'output.csv\', encoding=\'utf-8\', index=False)\n\n\nCurrently I am running the file through a script before hand that strips spaces from the employee_id column.\n\nAn example of employee_ids:\n\n37 78973 3\n23787\n2 22 3\n123\n\n\nIs there a way to get csvkit to do it and save me a step?\n'
"I have a large dataframe (>3MM rows) that I'm trying to pass through a function (the one below is largely simplified), and I keep getting a Memory Error message. \n\nI think I'm passing too large of a dataframe into the function, so I'm trying to:\n\n1) Slice the dataframe into smaller chunks (preferably sliced by AcctName)\n\n2) Pass the dataframe into the function\n\n3) Concatenate the dataframes back into one large dataframe\n\ndef trans_times_2(df):\n    df['Double_Transaction'] = df['Transaction'] * 2\n\nlarge_df \nAcctName   Timestamp    Transaction\nABC        12/1         12.12\nABC        12/2         20.89\nABC        12/3         51.93    \nDEF        12/2         13.12\nDEF        12/8          9.93\nDEF        12/9         92.09\nGHI        12/1         14.33\nGHI        12/6         21.99\nGHI        12/12        98.81\n\n\nI know that my function works properly, since it will work on a smaller dataframe (e.g. 40,000 rows). I tried the following, but I was unsuccessful with concatenating the small dataframes back into one large dataframe.\n\ndef split_df(df):\n    new_df = []\n    AcctNames = df.AcctName.unique()\n    DataFrameDict = {elem: pd.DataFrame for elem in AcctNames}\n    key_list = [k for k in DataFrameDict.keys()]\n    new_df = []\n    for key in DataFrameDict.keys():\n        DataFrameDict[key] = df[:][df.AcctNames == key]\n        trans_times_2(DataFrameDict[key])\n    rejoined_df = pd.concat(new_df)\n\n\nHow I envision the dataframes being split:\n\ndf1\nAcctName   Timestamp    Transaction  Double_Transaction\nABC        12/1         12.12        24.24\nABC        12/2         20.89        41.78\nABC        12/3         51.93        103.86\n\ndf2\nAcctName   Timestamp    Transaction  Double_Transaction\nDEF        12/2         13.12        26.24\nDEF        12/8          9.93        19.86\nDEF        12/9         92.09        184.18\n\ndf3\nAcctName   Timestamp    Transaction  Double_Transaction\nGHI        12/1         14.33        28.66\nGHI        12/6         21.99        43.98\nGHI        12/12        98.81        197.62\n\n"
'List with attributes of persons loaded into pandas dataframe df2. For cleanup I want to replace value zero (0 or \'0\') by np.nan. \n\ndf2.dtypes\n\nID                   object\nName                 object\nWeight              float64\nHeight              float64\nBootSize             object\nSuitSize             object\nType                 object\ndtype: object\n\n\nWorking code to set value zero to np.nan:\n\ndf2.loc[df2[\'Weight\'] == 0,\'Weight\'] = np.nan\ndf2.loc[df2[\'Height\'] == 0,\'Height\'] = np.nan\ndf2.loc[df2[\'BootSize\'] == \'0\',\'BootSize\'] = np.nan\ndf2.loc[df2[\'SuitSize\'] == \'0\',\'SuitSize\'] = np.nan\n\n\nBelieve this can be done in a similar/shorter way:\n\ndf2[["Weight","Height","BootSize","SuitSize"]].astype(str).replace(\'0\',np.nan)\n\n\nHowever the above does not work. The zero\'s remain in df2. How to tackle this?\n'
"This should be incredibly easy, but I can't get it to work. \n\nI want to filter my dataset on two or more values. \n\n#this works, when I filter for one value\ndf.loc[df['channel'] == 'sale'] \n\n#if I have to filter, two separate columns, I can do this\ndf.loc[(df['channel'] == 'sale')&amp;(df['type']=='A')] \n\n#but what if I want to filter one column by more than one value?\ndf.loc[df['channel'] == ('sale','fullprice')] \n\n\nWould this have to be an OR statement? I can do something like in SQL using in?\n"
"Here's an array of datetime values:\n\narray = np.array(['2016-05-01T00:00:59.3+10:00', '2016-05-01T00:02:59.4+10:00',\n                  '2016-05-01T00:03:59.4+10:00', '2016-05-01T00:13:00.1+10:00',\n                  '2016-05-01T00:22:00.5+10:00', '2016-05-01T00:31:01.1+10:00'],\n        dtype=object)\n\n\npd.to_datetime is very good at inferring datetime formats.\n\narray = pd.to_datetime(array)\n\nprint(array)\nDatetimeIndex(['2016-04-30 14:00:59.300000', '2016-04-30 14:02:59.400000',\n               '2016-04-30 14:03:59.400000', '2016-04-30 14:13:00.100000',\n               '2016-04-30 14:22:00.500000', '2016-04-30 14:31:01.100000'],\n              dtype='datetime64[ns]', freq=None)\n\n\nHow can I dynamically figure out what datetime format pd.to_datetime inferred? Something like: %Y-%m-%dT... (sorry, my datetime foo is really bad).\n"
"How do you append/update to a parquet file with pyarrow? \n\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\n table2 = pd.DataFrame({'one': [-1, np.nan, 2.5], 'two': ['foo', 'bar', 'baz'], 'three': [True, False, True]})\n table3 = pd.DataFrame({'six': [-1, np.nan, 2.5], 'nine': ['foo', 'bar', 'baz'], 'ten': [True, False, True]})\n\n\npq.write_table(table2, './dataNew/pqTest2.parquet')\n#append pqTest2 here?  \n\n\nThere is nothing I found in the docs about appending parquet files. And, Can you use pyarrow  with multiprocessing to insert/update the data. \n"
"I searched almost all over the internet and somehow none of the approaches seem to work in my case. \n\nI have two large csv files (each with a million+ rows and about 300-400MB in size). They are loading fine into data frames using the read_csv function without having to use the chunksize parameter.\nI even performed certain minor operations on this data like new column generation, filtering, etc.\n\nHowever, when I try to merge these two frames, I get a MemoryError. I have even tried to use SQLite to accomplish the merge, but in vain. The operation takes forever. \n\nMine is a Windows 7 PC with 8GB RAM. The Python version is 2.7\n\nThank you.\n\nEdit: I tried chunking methods too. When I do this, I don't get MemoryError, but the RAM usage explodes and my system crashes.\n"
'I am having issues using pandas groupby with categorical data. Theoretically, it should be super efficient: you are grouping and indexing via integers rather than strings. But it insists that, when grouping by multiple categories, every combination of categories must be accounted for.\n\nI sometimes use categories even when there\'s a low density of common strings, simply because those strings are long and it saves memory / improves performance. Sometimes there are thousands of categories in each column. When grouping by 3 columns, pandas forces us to hold results for 1000^3 groups.\n\nMy question: is there a convenient way to use groupby with categories while avoiding this untoward behaviour? I\'m not looking for any of these solutions:\n\n\nRecreating all the functionality via numpy.\nContinually converting to strings/codes before groupby, reverting to categories later.\nMaking a tuple column from group columns, then group by the tuple column.\n\n\nI\'m hoping there\'s a way to modify just this particular pandas idiosyncrasy. A simple example is below. Instead of 4 categories I want in the output, I end up with 12.\n\nimport pandas as pd\n\ngroup_cols = [\'Group1\', \'Group2\', \'Group3\']\n\ndf = pd.DataFrame([[\'A\', \'B\', \'C\', 54.34],\n                   [\'A\', \'B\', \'D\', 61.34],\n                   [\'B\', \'A\', \'C\', 514.5],\n                   [\'B\', \'A\', \'A\', 765.4],\n                   [\'A\', \'B\', \'D\', 765.4]],\n                  columns=(group_cols+[\'Value\']))\n\nfor col in group_cols:\n    df[col] = df[col].astype(\'category\')\n\ndf.groupby(group_cols, as_index=False).sum()\n\nGroup1  Group2  Group3  Value\n#   A   A   A   NaN\n#   A   A   C   NaN\n#   A   A   D   NaN\n#   A   B   A   NaN\n#   A   B   C   54.34\n#   A   B   D   826.74\n#   B   A   A   765.40\n#   B   A   C   514.50\n#   B   A   D   NaN\n#   B   B   A   NaN\n#   B   B   C   NaN\n#   B   B   D   NaN\n\n\nBounty update\n\nThe issue is poorly addressed by pandas development team (cf github.com/pandas-dev/pandas/issues/17594). Therefore, I am looking for responses that address any of the following:\n\n\nWhy, with reference to pandas source code, is categorical data treated differently in groupby operations?\nWhy would the current implementation be preferred? I appreciate this is subjective, but I am struggling to find any answer to this question. Current behaviour is prohibitive in many situations without cumbersome, potentially expensive, workarounds.\nIs there a clean solution to override pandas treatment of categorical data in groupby operations? Note the 3 no-go routes (dropping down to numpy; conversions to/from codes; creating and grouping by tuple columns). I would prefer a solution that is "pandas-compliant" to minimise / avoid loss of other pandas categorical functionality.\nA response from pandas development team to support and clarify existing treatment. Also, why should considering all category combinations not be configurable as a Boolean parameter?\n\n\nBounty update #2\n\nTo be clear, I\'m not expecting answers to all of the above 4 questions. The main question I am asking is whether it\'s possible, or advisable, to overwrite pandas library methods so that categories are treated in a way that facilitates groupby / set_index operations.\n'
"In Python generally, membership of a hashable collection is best tested via set. We know this because the use of hashing gives us O(1) lookup complexity versus O(n) for list or np.ndarray.\n\nIn Pandas, I often have to check for membership in very large collections. I presumed that the same would apply, i.e. checking each item of a series for membership in a set is more efficient than using list or np.ndarray. However, this doesn't seem to be the case:\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(0)\n\nx_set = {i for i in range(100000)}\nx_arr = np.array(list(x_set))\nx_list = list(x_set)\n\narr = np.random.randint(0, 20000, 10000)\nser = pd.Series(arr)\nlst = arr.tolist()\n\n%timeit ser.isin(x_set)                   # 8.9 ms\n%timeit ser.isin(x_arr)                   # 2.17 ms\n%timeit ser.isin(x_list)                  # 7.79 ms\n%timeit np.in1d(arr, x_arr)               # 5.02 ms\n%timeit [i in x_set for i in lst]         # 1.1 ms\n%timeit [i in x_set for i in ser.values]  # 4.61 ms\n\n\nVersions used for testing:\n\nnp.__version__  # '1.14.3'\npd.__version__  # '0.23.0'\nsys.version     # '3.6.5'\n\n\nThe source code for pd.Series.isin, I believe, utilises numpy.in1d, which presumably means a large overhead for set to np.ndarray conversion.\n\nNegating the cost of constructing the inputs, the implications for Pandas:\n\n\nIf you know your elements of x_list or x_arr are unique, don't bother converting to x_set. This will be costly (both conversion and membership tests) for use with Pandas.\nUsing list comprehensions are the only way to benefit from O(1) set lookup.\n\n\nMy questions are:\n\n\nIs my analysis above correct? This seems like an obvious, yet undocumented, result of how pd.Series.isin has been implemented.\nIs there a workaround, without using a list comprehension or pd.Series.apply, which does utilise O(1) set lookup? Or is this an unavoidable design choice and/or corollary of having NumPy as the backbone of Pandas?\n\n\nUpdate: On an older setup (Pandas / NumPy versions) I see x_set outperform x_arr with pd.Series.isin. So an additional question: has anything fundamentally changed from old to new to cause performance with set to worsen?\n\n%timeit ser.isin(x_set)                   # 10.5 ms\n%timeit ser.isin(x_arr)                   # 15.2 ms\n%timeit ser.isin(x_list)                  # 9.61 ms\n%timeit np.in1d(arr, x_arr)               # 4.15 ms\n%timeit [i in x_set for i in lst]         # 1.15 ms\n%timeit [i in x_set for i in ser.values]  # 2.8 ms\n\npd.__version__  # '0.19.2'\nnp.__version__  # '1.11.3'\nsys.version     # '3.6.0'\n\n"
"I have two dataframes. df1 is multi-indexed:\n\n                value\nfirst second    \na     x         0.471780\n      y         0.774908\n      z         0.563634\nb     x         -0.353756\n      y         0.368062\n      z         -1.721840\n\n\nand df2:\n\n      value\nfirst   \na     10\nb     20\n\n\nHow can I merge the two data frames with only one of the multi-indexes, in this case the 'first' index? The desired output would be:\n\n                value1      value2\nfirst second    \na     x         0.471780    10\n      y         0.774908    10\n      z         0.563634    10\nb     x         -0.353756   20\n      y         0.368062    20\n      z         -1.721840   20\n\n"
"I'm having trouble applying a regex function a column in a python dataframe.  Here is the head of my dataframe:\n\n               Name   Season          School   G    MP  FGA  3P  3PA    3P%\n 74       Joe Dumars  1982-83   McNeese State  29   NaN  487   5    8  0.625   \n 84      Sam Vincent  1982-83  Michigan State  30  1066  401   5   11  0.455   \n 176  Gerald Wilkins  1982-83     Chattanooga  30   820  350   0    2  0.000   \n 177  Gerald Wilkins  1983-84     Chattanooga  23   737  297   3   10  0.300   \n 243    Delaney Rudd  1982-83     Wake Forest  32  1004  324  13   29  0.448  \n\n\nI thought I had a pretty good grasp of applying functions to Dataframes, so maybe my Regex skills are lacking.\n\nHere is what I put together:\n\nimport re\n\ndef split_it(year):\n    return re.findall('(\\d\\d\\d\\d)', year)\n\n df['Season2'] = df['Season'].apply(split_it(x))\n\nTypeError: expected string or buffer\n\n\nOutput would be a column called Season2 that contains the year before the hyphen.  I'm sure theres an easier way to do it without regex, but more importantly, i'm trying to figure out what I did wrong\n\nThanks for any help in advance.\n"
"If I have a dataframe df with column x and want to create column y based on values of x using this in pseudo code:\n\n if df['x'] &lt;-2 then df['y'] = 1 \n else if df['x'] &gt; 2 then df['y']= -1 \n else df['y'] = 0\n\n\nHow would I achieve this?  I assume np.where is the best way to do this but not sure how to code it correctly.\n"
"If I've got a DataFrame in pandas which looks something like:\n\n    A   B   C\n0   1 NaN   2\n1 NaN   3 NaN\n2 NaN   4   5\n3 NaN NaN NaN\n\n\nHow can I get the first non-null value from each row? E.g. for the above, I'd like to get: [1, 3, 4, None] (or equivalent Series).\n"
'When doing:\n\nimport pandas\nx = pandas.read_csv(\'data.csv\', parse_dates=True, index_col=\'DateTime\', \n                                names=[\'DateTime\', \'X\'], header=None, sep=\';\')\n\n\nwith this data.csv file:\n\n1449054136.83;15.31\n1449054137.43;16.19\n1449054138.04;19.22\n1449054138.65;15.12\n1449054139.25;13.12\n\n\n(the 1st colum is a UNIX timestamp, i.e. seconds elapsed since 1/1/1970), I get this error when resampling the data every 15 second with x.resample(\'15S\'):\n\nTypeError: Only valid with DatetimeIndex, TimedeltaIndex or PeriodIndex\n\n\nIt\'s like the "datetime" information has not been parsed:\n\n                 X\nDateTime      \n1.449054e+09  15.31                \n1.449054e+09  16.19\n...\n\n\nHow to import a .CSV  with date stored as timestamp with pandas module?\n\nThen once I will be able to import the CSV, how to access to the lines for which date > 2015-12-02 12:02:18 ?\n'
'import pandas as pd\nimport numpy as np\ndata = \'filename.csv\'\ndf = pd.DataFrame(data)\ndf \n\n        one       two     three  four   five\na  0.469112 -0.282863 -1.509059  bar   True\nb  0.932424  1.224234  7.823421  bar  False\nc -1.135632  1.212112 -0.173215  bar  False\nd  0.232424  2.342112  0.982342  unbar True\ne  0.119209 -1.044236 -0.861849  bar   True\nf -2.104569 -0.494929  1.071804  bar  False\n\n\nI would like to select a range for a certain column, let\'s say column two. I would like to select all values between -0.5 and +0.5. How does one do this? \n\nI expected to use \n\n-0.5 &lt; df["two"] &lt; 0.5\n\n\nBut this (naturally) gives a ValueError: \n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\nI tried \n\n-0.5 (&lt; df["two"] &lt; 0.5)\n\n\nBut this outputs all True. \n\nThe correct output should be \n\n0    True\n1    False\n2    False\n3    False\n4    False\n5    True\n\n\nWhat is the correct way to find a range of values in a pandas dataframe column?\n\nEDIT: Question\n\nUsing .between() with \n\ndf[\'two\'].between(-0.5, 0.5, inclusive=False)\n\n\nwould would be the difference between \n\n -0.5 &lt; df[\'two\'] &lt; 0.5\n\n\nand inequalities like \n\n -0.5 =&lt; df[\'two\'] &lt; 0.5\n\n\n?\n'
'I have the following numpy array:\nimport numpy as np\n\npair_array = np.array([(205, 254), (205, 382), (254, 382), (18, 69), (205, 382), \n                       (31, 183), (31, 267), (31, 382), (183, 267), (183, 382)])\n\nprint(pair_array)\n\n#[[205 254]\n# [205 382]\n# [254 382]\n# [ 18  69]\n# [205 382]\n# [ 31 183]\n# [ 31 267]\n# [ 31 382]\n# [183 267]\n# [183 382]]\n\nIs there a way to transform this array to a symmetric pandas Dataframe that contains the count of occurences for all possible combinations?\nI expect something along the lines of this:\n#     18  31  69 183 205 254 267 382 \n#  18  0   0   1   0   0   0   0   0\n#  31  0   0   0   1   0   0   1   1\n#  69  1   0   0   0   0   0   0   0\n# 183  0   1   0   0   0   0   1   1\n# 205  0   0   0   0   0   1   0   2\n# 254  0   0   0   0   1   0   0   1\n# 267  0   1   0   1   0   0   0   0\n# 382  0   1   0   1   2   1   0   0\n\n'
"I have a DataFrame, say a volatility surface with index as time and column as strike. How do I do two dimensional interpolation? I can reindex but how do i deal with NaN? I know we can fillna(method='pad') but it is not even linear interpolation. Is there a way we can plug in our own method to do interpolation?\n"
'I have a data set like so in a pandas dataframe:\n\n                                  score\ntimestamp                                 \n2013-06-29 00:52:28+00:00        -0.420070\n2013-06-29 00:51:53+00:00        -0.445720\n2013-06-28 16:40:43+00:00         0.508161\n2013-06-28 15:10:30+00:00         0.921474\n2013-06-28 15:10:17+00:00         0.876710\n\n\nI need to get counts for the number of measurements, that occur so I am looking for something like this:\n\n                                    count\n   timestamp\n   2013-06-29                       2\n   2013-06-28                       3\n\n\nI do not care about the sentiment column I want the count of the occurrences per day. \n'
'I\'m loading a csv file, which has the following columns:\ndate, textA, textB, numberA, numberB\n\nI want to group by the columns: date, textA and textB - but want to apply "sum" to numberA, but "min" to numberB. \n\ndata = pd.read_table("file.csv", sep=",", thousands=\',\')\ngrouped = data.groupby(["date", "textA", "textB"], as_index=False)\n\n\n...but I cannot see how to then apply two different aggregate functions, to two different columns?\nI.e. sum(numberA), min(numberB)\n'
"Is there a way to structure Pandas groupby and qcut commands to return one column that has nested  tiles? Specifically, suppose I have 2 groups of data and I want qcut applied to each group and then return the output to one column. This would be similar to MS SQL Server's ntile() command that allows Partition by().\n\n     A    B  C\n0  foo  0.1  1\n1  foo  0.5  2\n2  foo  1.0  3\n3  bar  0.1  1\n4  bar  0.5  2\n5  bar  1.0  3\n\n\nIn the dataframe above I would like to apply the qcut function to B while partitioning on A to return C.\n"
"I have a dictionary object of the form:\n\nmy_dict = {id1: val1, id2: val2, id3: val3, ...}\n\n\nI want to create this into a DataFrame where I want to name the 2 columns 'business_id' and 'business_code'.  \n\nI tried:\n\nbusiness_df = DataFrame.from_dict(my_dict,orient='index',columns=['business_id','business_code'])\n\n\nBut it says from_dict doesn't take in a columns argument.\n\n\n  TypeError: from_dict() got an unexpected keyword argument 'columns' \n\n"
'I have some cvs data that has an empty column at the end of each row. I would like to leave it out of the import or alternatively delete it after import. My cvs data\'s have a varying number of columns. I\'ve tried using df.tail(), but haven\'t managed to choose the last column with it.\n\nemployment=pd.read_csv(\'./data/spanish/employment1976-1987thousands.csv\',index_col=0,header=[7,8],encoding=\'latin-1\')\n\n\nData:\n\n4.- Resultados provinciales\nEncuesta de Población Activa. Principales Resultados\n\nActivos por provincia y grupo de edad (4).\nUnidades:miles de personas\n\n\n,Álava,,,,Albacete,,,,Alicante,,,,Almería,,,,Asturias,,,,Ávila,,,,Badajoz,,,,Balears (Illes),,,,Barcelona,,,,Burgos,,,,Cáceres,,,,Cádiz,,,,Cantabria,,,,Castellón de la Plana,,,,Ciudad Real,,,,Córdoba,,,,Coruña (A),,,,Cuenca,,,,Girona,,,,Granada,,,,Guadalajara,,,,Guipúzcoa,,,,Huelva,,,,Huesca,,,,Jaén,,,,León,,,,Lleida,,,,Lugo,,,,Madrid,,,,Málaga,,,,Murcia,,,,Navarra,,,,Orense,,,,Palencia,,,,Palmas (Las),,,,Pontevedra,,,,Rioja (La),,,,Salamanca,,,,Santa Cruz de Tenerife,,,,Segovia,,,,Sevilla,,,,Soria,,,,Tarragona,,,,Teruel,,,,Toledo,,,,Valencia,,,,Valladolid,,,,Vizcaya,,,,Zamora,,,,Zaragoza,,,,Ceuta y Melilla,,,,\n,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,de 16 a 19 años,de 20 a 24 años,de 25 a 54 años,de 55 y más años,\n1976TIII,"8.9","11.6","60.4","11.8","16.4","14.4","65.2","14.9","47.9","49.9","246.0","60.1","20.5","14.3","88.9","11.2","34.5","42.5","278.0","91.3","6.6","7.2","41.5","13.3","25.3","22.8","135.3","37.5","19.8","24.4","153.0","43.0","166.8","203.7","1079.0","230.7","14.1","16.4","86.0","23.8","17.0","18.3","86.6","28.6","31.0","38.7","180.4","29.8","15.3","19.2","120.6","30.4","19.9","15.3","104.2","23.4","19.7","19.5","97.5","29.7","28.0","23.9","140.5","30.1","29.1","46.1","263.8","70.0","8.9","6.2","45.7","14.6","19.7","19.7","123.0","35.3","26.8","22.5","141.0","36.2","4.8","6.0","33.1","13.4","23.1","31.6","174.5","33.8","11.9","14.3","83.8","18.8","7.0","9.3","50.3","20.0","22.4","23.4","125.8","28.6","22.7","21.6","143.1","50.9","12.5","13.7","89.5","33.2","14.3","14.7","134.0","54.7","136.6","207.5","1067.6","218.6","34.7","41.1","196.4","38.4","37.2","35.0","200.5","46.1","15.6","23.8","111.6","30.7","14.0","16.8","120.2","74.9","5.7","6.4","39.2","8.0","24.5","25.6","135.3","27.1","36.4","39.4","246.1","74.0","10.2","11.3","63.9","13.4","10.5","11.0","74.1","19.6","19.3","23.9","140.3","31.7","5.5","6.0","35.6","11.3","55.2","55.6","262.5","68.1","3.1","3.2","24.4","5.4","21.8","18.4","116.7","37.1","4.6","3.4","37.3","12.0","20.3","16.7","102.2","23.1","73.5","85.5","454.6","101.5","19.2","23.4","90.7","20.5","41.3","54.7","272.2","57.0","6.0","7.1","56.5","28.9","29.2","32.1","192.7","49.8","0.0","0.0","0.0","0.0",\n1976TIV,"8.7","11.7","60.8","11.4","14.4","13.6","63.3","14.5","49.1","50.6","244.9","54.2","19.0","16.9","86.8","11.4","33.2","42.3","271.8","86.0","5.8","7.5","40.3","13.9","25.1","24.7","132.7","38.4","18.8","23.4","151.8","43.9","172.2","201.7","1070.7","228.1","11.1","15.7","82.5","21.1","16.4","18.0","89.2","26.6","32.6","40.0","176.5","30.5","15.8","18.1","121.3","30.2","19.0","17.3","106.3","24.1","19.9","19.0","101.7","26.9","25.3","22.3","142.7","28.9","30.0","42.4","267.6","70.1","7.3","7.0","44.4","13.0","17.8","21.4","122.8","34.0","28.4","21.6","140.5","36.8","4.7","6.6","32.6","10.8","24.8","32.7","177.2","32.3","11.9","12.5","85.4","20.5","6.9","8.5","48.8","19.9","22.4","22.1","127.6","25.1","18.5","21.1","137.8","48.7","12.4","11.1","84.9","31.5","13.6","15.6","132.7","52.0","144.0","202.3","1054.0","222.5","35.6","40.1","194.1","37.5","36.7","34.7","203.8","47.1","15.6","23.6","114.3","31.3","14.0","15.9","118.3","76.7","5.5","7.3","36.9","9.3","25.5","25.1","138.7","26.8","34.8","42.9","250.3","74.9","9.9","11.8","62.8","14.0","10.0","13.2","74.5","19.2","19.5","24.2","142.7","31.0","4.0","5.9","35.5","12.0","55.0","56.7","264.7","63.3","2.8","3.5","23.9","5.1","20.0","21.6","116.4","34.9","4.5","3.7","36.5","12.1","21.1","17.6","100.6","25.7","74.6","87.5","455.5","102.1","18.9","22.9","90.0","21.6","40.2","57.1","273.9","58.5","5.6","8.3","57.6","23.9","28.3","31.4","192.2","46.4","0.0","0.0","0.0","0.0",\n1977TI,"9.2","11.8","59.9","11.2","14.2","13.2","65.9","14.7","48.2","50.4","251.1","50.8","17.8","15.4","86.5","11.8","30.6","42.9","272.6","84.1","5.8","7.4","37.2","12.8","24.1","22.8","131.3","38.2","17.8","23.5","151.1","42.5","168.1","200.4","1077.2","223.3","11.6","12.8","80.9","17.6","14.4","16.4","88.2","23.9","34.5","37.5","176.3","30.8","15.2","19.7","121.3","31.6","18.4","19.4","107.4","24.7","20.0","18.1","98.3","26.6","24.9","23.6","150.7","27.5","29.5","40.3","267.4","70.5","5.6","7.5","44.2","12.8","17.1","21.1","122.8","33.6","29.6","23.3","142.1","37.9","4.6","5.5","33.7","11.2","23.5","30.4","175.2","32.8","12.0","12.7","84.8","21.3","7.3","9.3","46.6","17.8","30.2","26.0","147.1","25.2","15.9","22.7","133.2","45.1","12.8","12.1","84.3","28.0","12.4","16.5","131.2","55.6","150.9","202.9","1065.4","223.7","36.6","44.0","194.3","39.9","36.7","31.5","196.7","45.7","14.8","22.5","115.1","29.4","11.7","17.2","114.2","75.8","5.0","7.7","38.0","9.4","24.0","26.8","143.5","27.0","35.3","43.0","247.4","73.5","9.7","12.1","61.6","13.3","9.5","11.9","73.9","18.9","20.4","26.7","143.0","31.6","4.0","5.0","35.5","12.3","52.3","58.0","266.0","62.5","2.6","2.7","24.2","6.0","17.3","21.0","113.0","33.3","4.5","5.2","33.8","10.6","18.7","18.8","98.3","24.8","77.4","87.6","446.6","100.3","20.5","23.4","90.2","20.4","38.7","50.7","277.6","57.3","6.4","8.7","60.1","21.5","28.6","31.0","194.8","45.7","0.0","0.0","0.0","0.0",\n\n'
"Is there a way to do something similar to SQL's LIKE syntax on a pandas text DataFrame column, such that it returns a list of indices, or a list of booleans that can be used for indexing the dataframe? For example, I would like to be able to match all rows where the column starts with 'prefix_', similar to WHERE &lt;col&gt; LIKE prefix_% in SQL.\n"
'I have the below code\n\nimport pandas as pd\nprivate = pd.read_excel("file.xlsx","Pri")\npublic = pd.read_excel("file.xlsx","Pub")\nprivate["ISH"] = private.HolidayName.str.lower().contains("holiday|recess")\npublic["ISH"] = public.HolidayName.str.lower().contains("holiday|recess")\n\n\nI get the following error:\n\nAttributeError: \'Series\' object has no attribute \'contains\'\n\n\nIs there anyway to convert the \'HolidayName\' column to lower case and then check the regular expression ("Holiday|Recess")using .contains in one step?\n'
'With the following DataFrame, how can I shift the "beyer" column based on the index without having Pandas assign the shifted value to a different index value?\n\n                  line_date  line_race  beyer\nhorse                                        \nLast Gunfighter  2013-09-28         10     99\nLast Gunfighter  2013-08-18         10    102\nLast Gunfighter  2013-07-06          8    103\n.....\nPaynter          2013-09-28         10    103\nPaynter          2013-08-31         10     88\nPaynter          2013-07-27          8    100\n\n\ndf[\'beyer\'].shift(1) produces...\n\n                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103             71\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n\n\nThe problem is that Paynter was given a beyer that Last Gunfighter (his first record) was assigned. Instead I want it to go like this...\n\n                  line_date  line_race  beyer  beyer_shifted\nhorse                                                       \nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\n.....\nPaynter          2013-09-28         10    103            NaN\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n\n'
'I am trying to understand how python could pull data from an FTP server into pandas then move this into SQL server.  My code here is very rudimentary to say the least and I am looking for any advice or help at all.  I have tried to load the data from the FTP server first which works fine.... If I then remove this code and change it to a select from ms sql server it is fine so the connection string works, but the insertion into the SQL server seems to be causing problems.\n\nimport pyodbc\nimport pandas\nfrom ftplib import FTP\nfrom StringIO import StringIO\nimport csv\n\nftp = FTP (\'ftp.xyz.com\',\'user\',\'pass\' )\nftp.set_pasv(True)\nr = StringIO()\nftp.retrbinary(\'filname.csv\', r.write)\n\npandas.read_table (r.getvalue(), delimiter=\',\')\n\n\nconnStr = (\'DRIVER={SQL Server Native Client 10.0};SERVER=localhost;DATABASE=TESTFEED;UID=sa;PWD=pass\')\nconn = pyodbc.connect(connStr)\n\ncursor = conn.cursor()\ncursor.execute("INSERT INTO dbo.tblImport(Startdt, Enddt, x,y,z,)" "VALUES                  (x,x,x,x,x,x,x,x,x,x.x,x)")\ncursor.close()\nconn.commit()\nconn.close()\nprint"Script has successfully run!"\n\n\nWhen I remove the ftp code this runs perfectly, but I do not understand how to make the next jump to get this into Microsoft SQL server, or even if it is possible without saving into a file first.\n'
"I currently came up with some work arounds to count the number of missing values in a pandas DataFrame. Those are quite ugly and I am wondering if there is a better way to do it.\n\nLet's create an example DataFrame:\n\nfrom numpy.random import randn\ndf = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n               columns=['one', 'two', 'three'])\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\n\n\n\n\nWhat I currently have is\n\na) Counting cells with missing values:\n\n&gt;&gt;&gt; sum(df.isnull().values.ravel())\n9\n\n\nb) Counting rows that have missing values somewhere:\n\n&gt;&gt;&gt; sum([True for idx,row in df.iterrows() if any(row.isnull())])\n3\n\n"
"This is a fairly trivial problem, but its triggering my OCD and I haven't been able to find a suitable solution for the past half hour. \n\nFor background, I'm looking to calculate a value (let's call it F) for each group in a DataFrame derived from different aggregated measures of columns in the existing DataFrame.\n\nHere's a toy example of what I'm trying to do:\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'A': ['X', 'Y', 'X', 'Y', 'Y', 'Y', 'Y', 'X', 'Y', 'X'],\n                'B': ['N', 'N', 'N', 'M', 'N', 'M', 'M', 'N', 'M', 'N'],\n                'C': [69, 83, 28, 25, 11, 31, 14, 37, 14,  0],\n                'D': [ 0.3,  0.1,  0.1,  0.8,  0.8,  0. ,  0.8,  0.8,  0.1,  0.8],\n                'E': [11, 11, 12, 11, 11, 12, 12, 11, 12, 12]\n                })\n\ndf_grp = df.groupby(['A','B'])\ndf_grp.apply(lambda x: x['C'].sum() * x['D'].mean() / x['E'].max())\n\n\nWhat I'd like to do is assign a name to the result of apply (or lambda). Is there anyway to do this without moving lambda to a named function or renaming the column after running the last line?\n"
"I'm reading a huge CSV with a date field in the format YYYYMMDD and I'm using the following lambda to convert it when reading:\n\nimport pandas as pd\n\ndf = pd.read_csv(filen,\n                 index_col=None,\n                 header=None,\n                 parse_dates=[0],\n                 date_parser=lambda t:pd.to_datetime(str(t),\n                                            format='%Y%m%d', coerce=True))\n\n\nThis function is very slow though. \n\nAny suggestion to improve it?\n"
'I have to create a function which would split provided dataframe into chunks of needed size. For instance if dataframe contains 1111 rows, I want to be able to specify chunk size of 400 rows, and get three smaller dataframes with sizes of 400, 400 and 311. Is there a convenience function to do the job? What would be the best way to store and iterate over sliced dataframe?\n\nExample DataFrame\n\nimport numpy as np\nimport pandas as pd\n\ntest = pd.concat([pd.Series(np.random.rand(1111)), pd.Series(np.random.rand(1111))], axis = 1)\n\n'
'My goal is comparing between two columns and add the result column. R uses ifelse but I need to know pandas\'s way. \n\nR\n\n&gt; head(mau.payment)\n  log_month user_id install_month payment\n1   2013-06       1       2013-04       0\n2   2013-06       2       2013-04       0\n3   2013-06       3       2013-04   14994\n\n&gt; mau.payment$user.type &lt;-ifelse(mau.payment$install_month == mau.payment$log_month, "install", "existing")\n&gt; head(mau.payment)\n  log_month user_id install_month payment user.type\n1   2013-06       1       2013-04       0  existing\n2   2013-06       2       2013-04       0  existing\n3   2013-06       3       2013-04   14994  existing\n4   2013-06       4       2013-04       0  existing\n5   2013-06       6       2013-04       0  existing\n6   2013-06       7       2013-04       0  existing\n\n\nPandas\n\n&gt;&gt;&gt; maupayment\nuser_id  log_month  install_month\n1        2013-06    2013-04              0\n         2013-07    2013-04              0\n2        2013-06    2013-04              0\n3        2013-06    2013-04          14994\n\n\nI tried some cases but did not work. It seems that string comparison does not work.\n\n&gt;&gt;&gt;np.where(maupayment[\'log_month\'] == maupayment[\'install_month\'], \'install\', \'existing\')\n\nTypeError: \'str\' object cannot be interpreted as an integer \n\n\nCould you help me please?\n\n\n\nPandas and numpy version.\n\n&gt;&gt;&gt; pd.version.version\n\'0.16.2\'\n&gt;&gt;&gt; np.version.full_version\n\'1.9.2\'\n\n\n\n\nAfter update the versions, it worked!\n\n&gt;&gt;&gt; np.where(maupayment[\'log_month\'] == maupayment[\'install_month\'], \'install\', \'existing\')\narray([\'existing\', \'install\', \'existing\', ..., \'install\', \'install\',\n       \'install\'], \n      dtype=\'&lt;U8\')\n\n'
'How do I compare a pandas DataFrame with None? I have a constructor that takes one of a parameter_file or a pandas_df but never both. \n\ndef __init__(self,copasi_file,row_to_insert=0,parameter_file=None,pandas_df=None):\n    self.copasi_file=copasi_file\n    self.parameter_file=parameter_file\n    self.pandas_df=pandas_df      \n\n\nHowever, when I later try to compare the pandas_df against None, (i.e. when self.pandas_df actually contains a pandas dataframe):\n\n    if self.pandas_df!=None:\n        print \'Do stuff\'\n\n\nI get the following TypeError: \n\n  File "C:\\Anaconda1\\lib\\site-packages\\pandas\\core\\internals.py", line 885, in eval\n    % repr(other))\n\nTypeError: Could not compare [None] with block values\n\n'
"I have data like this in a csv file\n\nSymbol  Action  Year\n  AAPL     Buy  2001\n  AAPL     Buy  2001\n   BAC    Sell  2002\n   BAC    Sell  2002\n\n\nI am able to read it and groupby like this\n\ndf.groupby(['Symbol','Year']).count()\n\n\nI get \n\n             Action\nSymbol Year        \nAAPL   2001       2\nBAC    2002       2\n\n\nI desire this (order does not matter)\n\n             Action\nSymbol Year        \nAAPL   2001       2\nAAPL   2002       0\nBAC    2001       0\nBAC    2002       2\n\n\nI want to know if its possible to count for zero occurances\n"
"df = pd.DataFrame({'A': ['x', 'y', 'x'], 'B': ['z', 'u', 'z'],\n                  'C': ['1', '2', '3'],\n                  'D':['j', 'l', 'j']})\n\n\nI just want Column A and D to get dummies not for Column B. If I used pd.get_dummies(df), all columns turned into dummies. \n\nI want the final result containing all of columns , which means column C and column B exit,like 'A_x','A_y','B','C','D_j','D_l'.\n"
"I am trying to print or to get list of columns name with missing values. E.g.   \n\ndata1 data2 data3  \n1     3     3  \n2     NaN   5  \n3     4     NaN  \n\n\nI want to get ['data2', 'data3'].\nI wrote following code:\n\nprint('\\n'.join(map(\n    lambda x : str(x[1])\n    ,(filter(lambda z: z[0] != False, zip(train.isnull().any(axis=0), train.columns.values)))\n)))\n\n\nIt works well, but I think should be simpler way.\n"
"My data sets looks like:\n\n       Date    Value\n    1/1/1988    0.62\n    1/2/1988    0.64\n    1/3/1988    0.65\n    1/4/1988    0.66\n    1/5/1988    0.67\n    1/6/1988    0.66\n    1/7/1988    0.64\n    1/8/1988    0.66\n    1/9/1988    0.65\n    1/10/1988   0.65\n    1/11/1988   0.64\n    1/12/1988   0.66\n    1/13/1988   0.67\n    1/14/1988   0.66\n    1/15/1988   0.65\n    1/16/1988   0.64\n    1/17/1988   0.62\n    1/18/1988   0.64\n    1/19/1988   0.62\n    1/20/1988   0.62\n    1/21/1988   0.64\n    1/22/1988   0.62\n    1/23/1988   0.60\n\n\nI used this code to read this data \n\ndf.set_index(df['Date'], drop=False, append=False, inplace=False, verify_integrity=False).drop('Date', 1)\n\n\nbut the problem is index is not in date format. So the question is how to set this column as date index?\n"
"There has been many similar questions but none specifically to this.\n\nI have a list of data frames and I need to merge them together using a unique column (date). Field names are different so concat is out. \n\nI can manually use df[0].merge(df[1],on='Date').merge(df[3],on='Date) etc. to merge each df one by one, but the issue is that the number of data frames in the list differs with user input.\n\nIs there any way to merge that just combines all data frames in a list at one go? Or perhaps some for in loop at does that?\n\nI am using Python 2.7.\n"
"I had following data frame (the real data frame is much more larger than this one ) :\n\nsale_user_id    sale_product_id count\n1                 1              1\n1                 8              1\n1                 52             1\n1                 312            5\n1                 315            1\n\n\nThen reshaped it to move the values in sale_product_id as column headers using the following code:\n\nreshaped_df=id_product_count.pivot(index='sale_user_id',columns='sale_product_id',values='count')\n\n\nand the resulting data frame is:\n\nsale_product_id -1057   1   2   3   4   5   6   8   9   10  ... 98  980 981 982 983 984 985 986 987 99\nsale_user_id                                                                                    \n1                NaN    1.0 NaN NaN NaN NaN NaN 1.0 NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n3                NaN    1.0 NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n4                NaN    NaN 1.0 NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n\nas you can see we have a multililevel index , what i need is to have sale_user_is in the first column without multilevel indexing:\n\ni take the following approach : \n\nreshaped_df.reset_index()\n\n\nthe the result would be like this i still have the sale_product_id column , but i do not need it anymore:\n\nsale_product_id sale_user_id    -1057   1   2   3   4   5   6   8   9   ... 98  980 981 982 983 984 985 986 987 99\n0                          1    NaN 1.0 NaN NaN NaN NaN NaN 1.0 NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n1                          3    NaN 1.0 NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n2                          4    NaN NaN 1.0 NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN \n\n\ni can subset this data frame to get rid of sale_product_id but i don't think it would be efficient.I am looking for an efficient way to get rid of multilevel indexing while reshaping the original data frame\n"
"I would like to parallelize the following code:\n\nfor row in df.iterrows():\n    idx = row[0]\n    k = row[1]['Chromosome']\n    start,end = row[1]['Bin'].split('-')\n\n    sequence = sequence_from_coordinates(k,1,start,end) #slow download form http\n\n    df.set_value(idx,'GC%',gc_content(sequence,percent=False,verbose=False))\n    df.set_value(idx,'G4 repeats', sum([len(list(i)) for i in g4_scanner(sequence)]))\n    df.set_value(idx,'max flexibility',max([item[1] for item in dna_flex(sequence,verbose=False)]))\n\n\nI have tried to use multiprocessing.Pool() since each row can be processed independently, but I can't figure out how to share the DataFrame. I am also not sure that this is the best approach to do parallelization with pandas. Any help?\n"
'I\'m just starting to use Bokeh. Below I create some args I use for the rect figure.\n\nx_length = var_results.index * 5.5\n\n\nMultiplying the index by 5.5 gave me more room between labels.\n\nnames = var_results.Feature.tolist()\ny_length = var_results.Variance\ny_center = var_results.Variance/2\n\n\nvar_results is a Pandas dataframe that has a typical, sequential, non-repeating index. var_results also has a column Features that is strings of non-repeated, names, and finally it has a column Variance which is dtype float.\n\nr = figure(x_range = names, \n           y_range = (-0.05,.3), \n           active_scroll = \'wheel_zoom\', \n           x_axis_label = \'Features\', \n           y_axis_label = \'Variance\')\n\n\n\nr.rect(x_length, \n       y_center, \n       width=1, \n       height=y_length, \n       color = "#ff1200")\noutput_notebook()\nshow(r)\n\n\nI\'m essentially making a bar chart with rectangles. Bokeh seems to be very customizable. But my graph looks rough around the edges, literally.\n\n\n\nAs you can see there is an ugly smudge just below the chart and above the x-axis title \'Features\'. This is the label titles (technically the rectangle titles). How do I create space for and perhaps rotate to 45 degrees the labels so that they are readable and not just an overlapping mess?\n'
"I currently have an existing Pandas DataFrame with a date index, and columns each with a specific name.\n\nAs for the data cells, they are filled with various float values.\n\nI would like to copy my DataFrame, but replace all these values with zero.\n\nThe objective is to reuse the structure of the DataFrame (dimensions, index, column names), but clear all the current values by replacing them with zeroes.\n\nThe way I'm currently achieving this is as follow:\n\ndf[df &gt; 0] = 0\n\n\nHowever, this would not replace any negative value in the DataFrame.\n\nIsn't there a more general approach to filling an entire existing DataFrame with a single common value?\n\nThank you in advance for your help.\n"
"I have a dictionary with each key holding a list of float values. These lists are not of same size. \n\nI'd like to convert this dictionary to a pandas dataframe so that I can perform some analysis functions on the data easily such as (min, max, average, standard deviation, more).\n\nMy dictionary looks like this:\n\n{\n    'key1': [10, 100.1, 0.98, 1.2],\n    'key2': [72.5],\n    'key3': [1, 5.2, 71.2, 9, 10.11, 12.21, 65, 7]\n}\n\n\nWhat is the best way to get this into a dataframe so that I can utilize basic functions like sum, mean, describe, std? \n\nThe examples I find (like the link above), all assume each of the keys have the same number of values in the list.\n"
"Here is the snippet:\n\ntest = pd.DataFrame({'days': [0,31,45]})\ntest['range'] = pd.cut(test.days, [0,30,60])\n\n\nOutput:\n\n    days    range\n0   0       NaN\n1   31      (30, 60]\n2   45      (30, 60]\n\n\nI am surprised that 0 is not in (0, 30], what should I do to categorize 0 as (0, 30]?\n"
'I have a machine learning application written in Python which includes a data processing step. When I wrote it, I initially did the data processing on Pandas DataFrames, but when this lead to abysmal performance I eventually rewrote it using vanilla Python, with for loops instead of vectorized operations and lists and dicts instead of DataFrames and Series. To my surprise, the performance of the code written in vanilla Python ended up being far higher than that of the code written using Pandas.\n\nAs my handcoded data processing code is substantially bigger and messier than the original Pandas code, I haven\'t quite given up on using Pandas, and I\'m currently trying to optimize the Pandas code without much success.\n\nThe core of the data processing step consists of the following: I first divide the rows into several groups, as the data consists of several thousand time series (one for each "individual"), and I then do the same data processing on each group: a lot of summarization, combining different columns into new ones, etc.\n\nI profiled my code using Jupyter Notebook\'s lprun, and the bulk of the time is spent on the following and other similar lines:\n\ngrouped_data = data.groupby(\'pk\')\ndata[[v + \'Diff\' for v in val_cols]] = grouped_data[val_cols].transform(lambda x: x - x.shift(1)).fillna(0)\ndata[[v + \'Mean\' for v in val_cols]] = grouped_data[val_cols].rolling(4).mean().shift(1).reset_index()[val_cols]\n(...)\n\n\n...a mix of vectorized and non-vectorized processing. I understand that the non-vectorized operations won\'t be faster than my handwritten for loops, since that\'s basically what they are under the hood, but how can they be so much slower? We\'re talking about a performance degradation of 10-20x between my handwritten code and the Pandas code.\n\nAm I doing something very, very wrong?\n'
"I came across this line of code\n\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n\nhere app_train_poly and app_test_poly are the pandas dataframe.\n\nI know that with align() you are able to perform some sort of combining of the two dataframes but I am not able to visualize how does it actually work.\n\nI searched the documentation but could not find any illustrative example.\n"
'I\'m trying to create a plotly graph with some data I\'ve got from my PostgreSQL server, but when I try to graph I\'m getting an error: "TypeError: Object of type \'DataFrame\' is not JSON serializable"\n\nHere\'s the code so far:\n\nimport dash\nimport numpy as np\nimport pandas as pd\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport psycopg2 as pg2\nimport datetime\n\nconn = pg2.connect(database=\'X\',user=\'X\',password=secret)\n\ncur = conn.cursor()\n\ncur.execute("SELECT * FROM times;")\na = cur.fetchall()\nstr(a)\n\n\ndf = pd.DataFrame([[ij for ij in i] for i in a])\ndf.to_json()\ndf.rename(columns={0: "Serial Number", 1: "Status", 2: "Date", 3: "Time", 4: "Number"}, inplace=True);\n\nx = df["Date"]\ndata = [go.Scatter(\n            x=x,\n            y=df["Status"])]\n\nlayout = go.Layout(title="Server Data Visualization",\n                   xaxis = dict(\n                   range = [df.head(1),\n                            df.tail(1)]),\n                    yaxis=dict(title = "Status"))\n\nfig = go.Figure(data = data, layout = layout)\npy.plot(fig)\n\n\nThe df["Date"] is the date in format of "2018-08-03" and the df["Status"] is either "Uptime" or "Downtime."\n\nCan someone tell me what I\'m doing incorrectly? I\'m trying to have this graph basically be dates on the x-axis read in from the sql server, and then two values on the y-axis that represent either the value of "Uptime" or "Downtime."\n\nTraceback (most recent call last):\n  File "\\\\srv31data1\\users$\\User\\Desktop\\basic.py", line 37, in &lt;module&gt;\n    py.plot(fig)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plotly\\offline\\offline.py", line 469, in plot\n    \'100%\', \'100%\', global_requirejs=False)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plotly\\offline\\offline.py", line 184, in _plot_html\n    cls=utils.PlotlyJSONEncoder)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\json\\__init__.py", line 238, in dumps\n    **kw).encode(obj)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plotly\\utils.py", line 161, in encode\n    encoded_o = super(PlotlyJSONEncoder, self).encode(o)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\json\\encoder.py", line 199, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\json\\encoder.py", line 257, in iterencode\n    return _iterencode(o, 0)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\plotly\\utils.py", line 229, in default\n    return _json.JSONEncoder.default(self, obj)\n  File "C:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\json\\encoder.py", line 180, in default\n    o.__class__.__name__)\nTypeError: Object of type \'DataFrame\' is not JSON serializable\n\n\nEdit: Sorry, forgot to post the traceback!\n'
"With the DataFrame below as an example, \n\nIn [83]:\ndf = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\ndf\nOut[83]:\n   A  B  values\n0  1  1      10\n1  1  2      15\n2  2  1      20\n3  2  2      25\n\n\nWhat would be a simple way to generate a new column containing some aggregation of the data over one of the columns?\n\nFor example, if I sum values over items in A \n\nIn [84]:\ndf.groupby('A').sum()['values']\nOut[84]:\nA\n1    25\n2    45\nName: values\n\n\nHow can I get \n\n   A  B  values  sum_values_A\n0  1  1      10            25\n1  1  2      15            25\n2  2  1      20            45\n3  2  2      25            45\n\n"
"                    A        B\nDATE                 \n2013-05-01        473077    71333\n2013-05-02         35131    62441\n2013-05-03           727    27381\n2013-05-04           481     1206\n2013-05-05           226     1733\n2013-05-06           NaN     4064\n2013-05-07           NaN    41151\n2013-05-08           NaN     8144\n2013-05-09           NaN       23\n2013-05-10           NaN       10\n\n\nsay i have the dataframe above.  what is the easiest way to get a series with the same index which is the average of the columns A and B?  the average needs to ignore NaN values. the twist is that this solution needs to be flexible to the addition of new columns to the dataframe.\n\nthe closest i have come was \n\ndf.sum(axis=1) / len(df.columns)\n\n\nhowever, this does not seem to ignore the NaN values\n\n(note:  i am still a bit new to the pandas library, so i'm guessing there's an obvious way to do this that my limited brain is simply not seeing)\n"
"Obviously new to Pandas. How can i simply count the number of records in a dataframe.\n\nI would have thought some thing as simple as this would do it and i can't seem to even find the answer in searches...probably because it is too simple.\n\ncnt = df.count\nprint cnt\n\n\nthe above code actually just prints the whole df\n"
'I am new to pandas (well, to all things "programming"...), but have been encouraged to give it a try. \nI have a mongodb database - "test" - with a collection called "tweets".\nI access the database in ipython:\n\nimport sys\nimport pymongo\nfrom pymongo import Connection\nconnection = Connection()\ndb = connection.test\ntweets = db.tweets\n\n\nthe document structure of documents in tweets is as follows:\n\nentities\': {u\'hashtags\': [],\n  u\'symbols\': [],\n  u\'urls\': [],\n  u\'user_mentions\': []},\n u\'favorite_count\': 0,\n u\'favorited\': False,\n u\'filter_level\': u\'medium\',\n u\'geo\': {u\'coordinates\': [placeholder coordinate, -placeholder coordinate], u\'type\': u\'Point\'},\n u\'id\': 349223842700472320L,\n u\'id_str\': u\'349223842700472320\',\n u\'in_reply_to_screen_name\': None,\n u\'in_reply_to_status_id\': None,\n u\'in_reply_to_status_id_str\': None,\n u\'in_reply_to_user_id\': None,\n u\'in_reply_to_user_id_str\': None,\n u\'lang\': u\'en\',\n u\'place\': {u\'attributes\': {},\n  u\'bounding_box\': {u\'coordinates\': [[[placeholder coordinate, placeholder coordinate],\n     [-placeholder coordinate, placeholder coordinate],\n     [-placeholder coordinate, placeholder coordinate],\n     [-placeholder coordinate, placeholder coordinate]]],\n   u\'type\': u\'Polygon\'},\n  u\'country\': u\'placeholder country\',\n  u\'country_code\': u\'example\',\n  u\'full_name\': u\'name, xx\',\n  u\'id\': u\'user id\',\n  u\'name\': u\'name\',\n  u\'place_type\': u\'city\',\n  u\'url\': u\'http://api.twitter.com/1/geo/id/1820d77fb3f65055.json\'},\n u\'retweet_count\': 0,\n u\'retweeted\': False,\n u\'source\': u\'&lt;a href="http://twitter.com/download/iphone" rel="nofollow"&gt;Twitter for iPhone&lt;/a&gt;\',\n u\'text\': u\'example text\',\n u\'truncated\': False,\n u\'user\': {u\'contributors_enabled\': False,\n  u\'created_at\': u\'Sat Jan 22 13:42:59 +0000 2011\',\n  u\'default_profile\': False,\n  u\'default_profile_image\': False,\n  u\'description\': u\'example description\',\n  u\'favourites_count\': 100,\n  u\'follow_request_sent\': None,\n  u\'followers_count\': 100,\n  u\'following\': None,\n  u\'friends_count\': 100,\n  u\'geo_enabled\': True,\n  u\'id\': placeholder_id,\n  u\'id_str\': u\'placeholder_id\',\n  u\'is_translator\': False,\n  u\'lang\': u\'en\',\n  u\'listed_count\': 0,\n  u\'location\': u\'example place\',\n  u\'name\': u\'example name\',\n  u\'notifications\': None,\n  u\'profile_background_color\': u\'000000\',\n  u\'profile_background_image_url\': u\'http://a0.twimg.com/images/themes/theme19/bg.gif\',\n  u\'profile_background_image_url_https\': u\'https://si0.twimg.com/images/themes/theme19/bg.gif\',\n  u\'profile_background_tile\': False,\n  u\'profile_banner_url\': u\'https://pbs.twimg.com/profile_banners/241527685/1363314054\',\n  u\'profile_image_url\':       u\'http://a0.twimg.com/profile_images/378800000038841219/8a71d0776da0c48dcc4ef6fee9f78880_normal.jpeg\',\n  u\'profile_image_url_https\':     u\'https://si0.twimg.com/profile_images/378800000038841219/8a71d0776da0c48dcc4ef6fee9f78880_normal.jpeg\', \n  u\'profile_link_color\': u\'000000\',\n  u\'profile_sidebar_border_color\': u\'FFFFFF\',\n  u\'profile_sidebar_fill_color\': u\'000000\',\n  u\'profile_text_color\': u\'000000\',\n  u\'profile_use_background_image\': False,\n  u\'protected\': False,\n  u\'screen_name\': placeholder screen_name\',\n  u\'statuses_count\': xxxx,\n  u\'time_zone\': u\'placeholder time_zone\',\n  u\'url\': None,\n  u\'utc_offset\': -21600,\n  u\'verified\': False}}\n\n\nNow, as far as I understand, pandas\' main data structure - a spreadsheet-like table - is called DataFrame. How can I load the data from my "tweets" collection into pandas\' DataFrame? And how can I query for a subdocument within the database?\n'
"I have to dataframes, df1 has columns A, B, C, D... and df2 has columns A, B, E, F...\n\nThe keys I want to merge with are in column A. B is also (most likely) the same in both dataframes. Though this is a big data set I am working on cleaning so I do not have a extremely good overview of everything yet.\n\nI do\n\nmerge(df1, df2, on='A')\n\n\nAnd the results contains a column called B_x. Since the data set is big and messy I haven't tried to investigate how B_x differs from B in df1 and B in df2\n\nSo my question is just in general: what does Pandas mean when it has appended the _x to a column name in the merged dataframe?\n\nThank you\n"
"I have a pandas DataFrame, eg:\n\nx = DataFrame.from_dict({'farm' : ['A','B','A','B'], \n                         'fruit':['apple','apple','pear','pear'], \n                         '2014':[10,12,6,8], \n                         '2015':[11,13,7,9]})\n\n\nie:\n\n   2014  2015 farm  fruit\n0    10    11    A  apple\n1    12    13    B  apple\n2     6     7    A   pear\n3     8     9    B   pear\n\n\nHow can I convert it to this: ?\n\n  farm  fruit  value  year\n0    A  apple     10  2014\n1    B  apple     12  2014\n2    A   pear      6  2014\n3    B   pear      8  2014\n4    A  apple     11  2015\n5    B  apple     13  2015\n6    A   pear      7  2015\n7    B   pear      9  2015\n\n\nI have tried stack and unstack but haven't been able to make it work.\n\nThanks!\n"
"I'm having difficulty constructing a 3D DataFrame in Pandas. I want something like this\n\nA               B               C\nstart    end    start    end    start    end ...\n7        20     42       52     90       101\n11       21                     213      34\n56       74                     9        45\n45       12\n\n\nWhere A, B, etc are the top-level descriptors and start and end are subdescriptors. The numbers that follow are in pairs and there aren't the same number of pairs for A, B etc. Observe that A has four such pairs, B has only 1, and C has 3.\n\nI'm not sure how to proceed in constructing this DataFrame. Modifying this example didn't give me the designed output:\n\nimport numpy as np\nimport pandas as pd\n\nA = np.array(['one', 'one', 'two', 'two', 'three', 'three'])\nB = np.array(['start', 'end']*3)\nC = [np.random.randint(10, 99, 6)]*6\ndf = pd.DataFrame(zip(A, B, C), columns=['A', 'B', 'C'])\ndf.set_index(['A', 'B'], inplace=True)\ndf\n\n\nyielded:\n\n                C\n A          B   \n one        start   [22, 19, 16, 20, 63, 54]\n              end   [22, 19, 16, 20, 63, 54]\n two        start   [22, 19, 16, 20, 63, 54]\n              end   [22, 19, 16, 20, 63, 54]\n three      start   [22, 19, 16, 20, 63, 54]\n              end   [22, 19, 16, 20, 63, 54]\n\n\nIs there any way of breaking up the lists in C into their own columns?\n\nEDIT: The structure of my C is important. It looks like the following:\n\n C = [[7,11,56,45], [20,21,74,12], [42], [52], [90,213,9], [101, 34, 45]]\n\n\nAnd the desired output is the one at the top. It represents the starting and ending points of subsequences within a certain sequence (A, B. C are the different sequences). Depending on the sequence itself, there are a differing number of subsequences that satisfy a given condition I'm looking for. As a result, there are a differing number of start:end pairs for A, B, etc\n"
'I am new to python and i am facing problem in creating the Dataframe in the format of key and value i.e. \n\ndata = [{\'key\':\'\\[GlobalProgramSizeInThousands\\]\',\'value\':\'1000\'},]\n\n\nHere is my My code  \n\ncolumnsss = [\'key\',\'value\'];\nquery = "select * from bparst_tags where tag_type = 1 ";\nresult = database.cursor(db.cursors.DictCursor);\nresult.execute(query);\nresult_set = result.fetchall();\ndata = "[";\nfor row in result_set:\n`row["tag_expression"]`)\n    data +=  "{\'value\': %s , \'key\': %s }," % ( `row["tag_expression"]`, `row["tag_name"]` )\ndata += "]" ;    \ndf = DataFrame(data , columns=columnsss); \n\n\nBut when i pass the data in DataFrame it shows me pandas.core.common.PandasError: DataFrame constructor not properly called!.\n\nwhile if i print the data and assign the same value to data variable then it works.\n'
'How do I drop a row if any of the values in the row equal zero?\n\nI would normally use df.dropna() for NaN values but not sure how to do it with "0" values.\n'
"I have a 100M line csv file (actually many separate csv files) totaling 84GB. I need to convert it to a HDF5 file with a single float dataset. I used h5py in testing without any problems, but now I can't do the final dataset without running out of memory.\n\nHow can I write to HDF5 without having to store the whole dataset in memory? I'm expecting actual code here, because it should be quite simple.\n\nI was just looking into pytables, but it doesn't look like the array class (which corresponds to a HDF5 dataset) can be written to iteratively. Similarly, pandas has read_csv and to_hdf methods in its io_tools, but I can't load the whole dataset at one time so that won't work. Perhaps you can help me solve the problem correctly with other tools in pytables or pandas.\n"
'I am trying to save defined in Python Pandas Data Frame as HTML page. In addition i would like to make this table saved as HTML table ability to be filtered by value of any column. Can you please provide possible solution? At the final this should be table saved as HTML page. I would like to incorporate this code in my Python code. Thank you\n'
"I have a python pandas dataframe df with a lot of rows.  From those rows, I want to slice out and only use the rows that contain the word 'ball' in the 'body' column.  To do that, I can do:\n\ndf[df['body'].str.contains('ball')]\n\nThe issue is, I want it to be case insensitive, meaning that if the word Ball or bAll showed up, I'll want those as well.  One way to do case insensitive search is to turn the string to lowercase and then search that way.  I'm wondering how to go about doing that. I tried\n\ndf[df['body'].str.lower().contains('ball')]\n\nBut that doesn't work.  I'm not sure if I'm supposed to use a lambda function on this or something of that nature.\n"
'For example I read excel file into DataFrame with 2 columns(id and URL). URLs in input file are like text(without hyperlinks):\n\ninput_f = pd.read_excel("input.xlsx")\n\n\nWatch what inside this DataFrame - everything was successfully read, all URLs are ok in input_f. After that when I wan\'t to save this file to_excel\n\ninput_f.to_excel("output.xlsx", index=False)\n\n\nI got warning. \n\n\n  Path\\worksheet.py:836: UserWarning: Ignoring URL \'http:// here long URL\' with\n  link or location/anchor > 255 characters since it exceeds Excel\'s\n  limit for URLS   force_unicode(url))\n\n\nAnd in output.xlsx cells with long URL were empty, and URLs become hyperlinks.\n\nHow to fix this?\n'
"I am using the label encoder to convert categorical data into numeric values.\n\nHow does LabelEncoder handle missing values?\n\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport numpy as np\na = pd.DataFrame(['A','B','C',np.nan,'D','A'])\nle = LabelEncoder()\nle.fit_transform(a)\n\n\nOutput:\n\narray([1, 2, 3, 0, 4, 1])\n\n\nFor the above example, label encoder changed NaN values to a category. How would I know which category represents missing values?\n"
'Most of the info I found was not in python>pandas>dataframe hence the question.\n\nI want to transform an integer between 1 and 12 into an abbrieviated month name.\n\nI have a df which looks like:\n\n   client Month\n1  sss    02\n2  yyy    12\n3  www    06\n\n\nI want the df to look like this:\n\n   client Month\n1  sss    Feb\n2  yyy    Dec\n3  www    Jun\n\n'
"I am using pandas qcut to split some data into 20 bins as part of data prep for training of a binary classification model like so:\n\ndata['VAR_BIN'] = pd.qcut(cc_data[var], 20, labels=False)\n\n\nMy question is, how can I apply the same binning logic derived from the qcut statement above to a new set of data, say for model validation purposes. Is there an easy way to do this?\n\nThanks\n"
'How can I pick out the difference between to columns of the same name in two dataframes?\nI mean I have dataframe A with a column named X and dataframe B with column named X, if i do pd.merge(A, B, on=[\'X\']), i\'ll get the common X values of A and B, but how can i get the "non-common" ones?\n'
"I'm trying to do a simple merge between two dataframes. These come from two different SQL tables, where the joining keys are strings:\n\n&gt;&gt;&gt; df1.col1.dtype\ndtype('O')\n&gt;&gt;&gt; df2.col2.dtype\ndtype('O')\n\n\nI try to merge them using this:\n\n&gt;&gt;&gt; merge_res = pd.merge(df1, df2, left_on='col1', right_on='col2')\n\n\nThe result of the inner join is empty, which first prompted me that there might not be any entries in the intersection:\n\n&gt;&gt;&gt; merge_res.shape\n(0, 19)\n\n\nBut when I try to match a single element, I see this really odd behavior.\n\n# Pick random element in second dataframe\n&gt;&gt;&gt; df2.iloc[5,:].col2\n'95498208100000'\n\n# Manually look for it in the first dataframe\n&gt;&gt;&gt; df1[df1.col1 == '95498208100000']\n0 rows × 19 columns\n# Empty, which makes sense given the above merge result\n\n# Now look for the same value as an integer\n&gt;&gt;&gt; df1[df1.col1 == 95498208100000]\n1 rows × 19 columns\n# FINDS THE ELEMENT!?!\n\n\nSo, the columns are defined with the 'object' dtype. Searching for them as strings don't yield any results. Searching for them as integers does return a result, and I think this is the reason why the merge doesn't work above..\n\nAny ideas what's going on?\n\nIt's almost as thought Pandas converts df1.col1 to an integer just because it can, even though it should be treated as a string while matching. \n\n(I tried to replicate this using sample dataframes, but for small examples, I don't see this behavior. Any suggestions on how I can find a more descriptive example would be appreciated as well.)\n"
"I've got several columns with long strings of text in a pandas data frame, but am only interested in examining one of them. Is there a way to use something along the lines of pd.set_option('max_colwidth', 60) but for a single column only, rather than expanding the width of all the columns in my df?\n"
'I have 2 Data Frames, one named USERS and another named EXCLUDE. Both of them have a field named "email".\n\nBasically, I want to remove every row in USERS that has an email contained in EXCLUDE.\n\nHow can I do it?\n'
'I have a Python3.x pandas DataFrame whereby certain columns are strings which as expressed as bytes (like in Python2.x)\n\nimport pandas as pd\ndf = pd.DataFrame(...)\ndf\n       COLUMN1         ....\n0      b\'abcde\'        ....\n1      b\'dog\'          ....\n2      b\'cat1\'         ....\n3      b\'bird1\'        ....\n4      b\'elephant1\'    ....\n\n\nWhen I access by column with df.COLUMN1, I see Name: COLUMN1, dtype: object\n\nHowever, if I access by element, it is a "bytes" object\n\ndf.COLUMN1.ix[0].dtype\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nAttributeError: \'bytes\' object has no attribute \'dtype\'\n\n\nHow do I convert these into "regular" strings? That is, how can I get rid of this b\'\' prefix? \n'
'I am new to Python and Pandas. I am trying to convert a Pandas Dataframe to a nested JSON. The function .to_json() doens\'t give me enough flexibility for my aim.\n\nHere are some data points of the dataframe (in csv, comma separated):\n\n,ID,Location,Country,Latitude,Longitude,timestamp,tide  \n0,1,BREST,FRA,48.383,-4.495,1807-01-01,6905.0  \n1,1,BREST,FRA,48.383,-4.495,1807-02-01,6931.0  \n2,1,BREST,FRA,48.383,-4.495,1807-03-01,6896.0  \n3,1,BREST,FRA,48.383,-4.495,1807-04-01,6953.0  \n4,1,BREST,FRA,48.383,-4.495,1807-05-01,7043.0  \n2508,7,CUXHAVEN 2,DEU,53.867,8.717,1843-01-01,7093.0  \n2509,7,CUXHAVEN 2,DEU,53.867,8.717,1843-02-01,6688.0  \n2510,7,CUXHAVEN 2,DEU,53.867,8.717,1843-03-01,6493.0  \n2511,7,CUXHAVEN 2,DEU,53.867,8.717,1843-04-01,6723.0  \n2512,7,CUXHAVEN 2,DEU,53.867,8.717,1843-05-01,6533.0  \n4525,9,MAASSLUIS,NLD,51.918,4.25,1848-02-01,6880.0  \n4526,9,MAASSLUIS,NLD,51.918,4.25,1848-03-01,6700.0  \n4527,9,MAASSLUIS,NLD,51.918,4.25,1848-04-01,6775.0  \n4528,9,MAASSLUIS,NLD,51.918,4.25,1848-05-01,6580.0  \n4529,9,MAASSLUIS,NLD,51.918,4.25,1848-06-01,6685.0  \n6540,8,WISMAR 2,DEU,53.898999999999994,11.458,1848-07-01,6957.0  \n6541,8,WISMAR 2,DEU,53.898999999999994,11.458,1848-08-01,6944.0  \n6542,8,WISMAR 2,DEU,53.898999999999994,11.458,1848-09-01,7084.0  \n6543,8,WISMAR 2,DEU,53.898999999999994,11.458,1848-10-01,6898.0  \n6544,8,WISMAR 2,DEU,53.898999999999994,11.458,1848-11-01,6859.0  \n8538,10,SAN FRANCISCO,USA,37.806999999999995,-122.465,1854-07-01,6909.0  \n8539,10,SAN FRANCISCO,USA,37.806999999999995,-122.465,1854-08-01,6940.0  \n8540,10,SAN FRANCISCO,USA,37.806999999999995,-122.465,1854-09-01,6961.0  \n8541,10,SAN FRANCISCO,USA,37.806999999999995,-122.465,1854-10-01,6952.0  \n8542,10,SAN FRANCISCO,USA,37.806999999999995,-122.465,1854-11-01,6952.0  \n\n\nThere is a lot of repetitive information and I would like to have a JSON like this:\n\n[\n{\n    "ID": 1,\n    "Location": "BREST",\n    "Latitude": 48.383,\n    "Longitude": -4.495,\n    "Country": "FRA",\n    "Tide-Data": {\n        "1807-02-01": 6931,\n        "1807-03-01": 6896,\n        "1807-04-01": 6953,\n        "1807-05-01": 7043\n    }\n},\n{\n    "ID": 5,\n    "Location": "HOLYHEAD",\n    "Latitude": 53.31399999999999,\n    "Longitude": -4.62,\n    "Country": "GBR",\n    "Tide-Data": {\n        "1807-02-01": 6931,\n        "1807-03-01": 6896,\n        "1807-04-01": 6953,\n        "1807-05-01": 7043\n    }\n}\n]\n\n\nHow could I achieve this?\n\nEDIT: \n\nCode to reproduce the dataframe:\n\n# input json\njson_str = \'[{"ID":1,"Location":"BREST","Country":"FRA","Latitude":48.383,"Longitude":-4.495,"timestamp":"1807-01-01","tide":6905},{"ID":1,"Location":"BREST","Country":"FRA","Latitude":48.383,"Longitude":-4.495,"timestamp":"1807-02-01","tide":6931},{"ID":1,"Location":"BREST","Country":"DEU","Latitude":48.383,"Longitude":-4.495,"timestamp":"1807-03-01","tide":6896},{"ID":7,"Location":"CUXHAVEN 2","Country":"DEU","Latitude":53.867,"Longitude":-8.717,"timestamp":"1843-01-01","tide":7093},{"ID":7,"Location":"CUXHAVEN 2","Country":"DEU","Latitude":53.867,"Longitude":-8.717,"timestamp":"1843-02-01","tide":6688},{"ID":7,"Location":"CUXHAVEN 2","Country":"DEU","Latitude":53.867,"Longitude":-8.717,"timestamp":"1843-03-01","tide":6493}]\'\n\n# load json object\ndata_list = json.loads(json_str)\n\n# create dataframe\ndf = json_normalize(data_list, None, None)\n\n'
'I\'m doing some analysis with pandas in a jupyter notebook and since my apply function takes a long time I would like to see a progress bar.\nThrough this post here I found the tqdm library that provides a simple progress bar for pandas operations. \nThere is also a Jupyter integration that provides a really nice progress bar where the bar itself changes over time.\n\nHowever, I would like to combine the two and don\'t quite get how to do that. \nLet\'s just take the same example as in the documentation\n\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\ndf = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))\n\n# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\ntqdm.pandas(desc="my bar!")\n\n# Now you can use `progress_apply` instead of `apply`\n# and `progress_map` instead of `map`\ndf.progress_apply(lambda x: x**2)\n# can also groupby:\n# df.groupby(0).progress_apply(lambda x: x**2)\n\n\nIt even says "can use \'tqdm_notebook\' " but I don\'t find a way how. \nI\'ve tried a few things like\n\ntqdm_notebook(tqdm.pandas(desc="my bar!"))\n\n\nor\n\ntqdm_notebook.pandas\n\n\nbut they don\'t work.\nIn the definition it looks to me like  \n\ntqdm.pandas(tqdm_notebook(desc="my bar!"))\n\n\nshould work, but the bar doesn\'t properly show the progress and there is still additional output.\n\nAny other ideas?\n'
"I have a dataframe of 13 columns and 55,000 rows I am trying to convert 5 of those rows to datetime, right now they are returning the type 'object' and I need to transform this data for machine learning I know that if I do \n\ndata['birth_date'] = pd.to_datetime(data[birth_date], errors ='coerce')\n\n\nit will return a datetime column but I want to do it for 4 other columns as well, is there one line that I can write to call all of them? I dont think I can index like \n\ndata[:,7:12]\n\n\nthanks!\n"
"I have a dataframe and am trying to set the index to the column 'Timestamp'. Currently the index is just a row number. An example of Timestamp's format is: 2015-09-03 16:35:00\n\nI've tried to set the index:\n\ndf.set_index('Timestamp')\n\n\nI don't get an error, but when I print the dataframe, the index is still the row number. How can I use Timestamp as the index?\n"
"I am trying to plot two countplots showing the counts of batting and bowling. I tried the following code:\n\nl=['batting_team','bowling_team']\nfor i in l:\n    sns.countplot(high_scores[i])\n    mlt.show()\n\n\nBut by using this , I am getting two plots one below the other. How can i make them order side by side?\n"
"I have a Python Pandas dataframe df:\n\nd=[['hello',1,'GOOD','long.kw'],\n   [1.2,'chipotle',np.nan,'bingo'],\n   ['various',np.nan,3000,123.456]]                                                    \nt=pd.DataFrame(data=d, columns=['A','B','C','D']) \n\n\nwhich looks like this:\n\nprint(t)\n         A         B     C        D\n0    hello         1  GOOD  long.kw\n1      1.2  chipotle   NaN    bingo\n2  various       NaN  3000  123.456\n\n\nI am trying to create a new column which is a list of the values in A, B, C, and D.  So it would look like this:\n\nt['combined']                                             \n\nOut[125]: \n0        [hello, 1, GOOD, long.kw]\n1        [1.2, chipotle, nan, bingo]\n2        [various, nan, 3000, 123.456]\nName: combined, dtype: object\n\n\nI am trying this code:\n\nt['combined'] = t.apply(lambda x: list([x['A'],\n                                        x['B'],\n                                        x['C'],\n                                        x['D']]),axis=1)    \n\n\nWhich returns this error:\n\nValueError: Wrong number of items passed 4, placement implies 1 \n\n\nWhat is puzzling to me is if remove one of the columns that I want to put in the list (or add another column to the dataframe that I DON'T add to the list), my code works.  \n\nFor instance, run this code:\n\nt['combined'] = t.apply(lambda x: list([x['A'],\n                                        x['B'],\n                                        x['D']]),axis=1)      \n\n\nReturns this which is perfect if I only wanted the 3 columns:\n\nprint(t)\n         A         B     C        D                 combined\n0    hello         1  GOOD  long.kw      [hello, 1, long.kw]\n1      1.2  chipotle   NaN    bingo   [1.2, chipotle, bingo]\n2  various       NaN  3000  123.456  [various, nan, 123.456]\n\n\nI am at a complete loss as to why requesting the 'combined' list be made of all columns in the dataframe would create an error, but selecting all but 1 column to create the 'combined' list and the list is created as expected.                                        \n"
"I'm frequently using pandas for merge (join) by using a range condition.\n\nFor instance if there are 2 dataframes:\n\nA (A_id, A_value)\n\nB (B_id,B_low, B_high, B_name)\n\nwhich are big and approximately of the same size (let's say 2M records each).\n\nI would like to make an inner join between A and B, so A_value would be between B_low and B_high.\n\nUsing SQL syntax that would be:\n\nSELECT *\nFROM A,B\nWHERE A_value between B_low and B_high\n\n\nand that would be really easy, short and efficient.\n\nMeanwhile in pandas the only way (that's not using loops that I found), is by creating a dummy column in both tables, join on it (equivalent to cross-join) and then filter out unneeded rows. That sounds heavy and complex:\n\nA['dummy'] = 1\nB['dummy'] = 1\nTemp = pd.merge(A,B,on='dummy')\nResult = Temp[Temp.A_value.between(Temp.B_low,Temp.B_high)]\n\n\nAnother solution that I had is by applying on each of A value a search function on B by usingB[(x&gt;=B.B_low) &amp; (x&lt;=B.B_high)] mask, but it sounds inefficient as well and might require index optimization.\n\nIs there a more elegant and/or efficient way to perform this action?\n"
"I have a pandas data frame with two columns one is temperature the other is time. \n\nI would like to make third and fourth columns called min and max. Each of these columns would be filled with nan's except where there is a local min or max, then it would have the value of that extrema.  \n\nHere is a sample of what the data looks like, essentially I am trying to identify all the peaks and low points in the figure. \n\n\n\nAre there any built in tools with pandas that can accomplish this?\n"
"I have a this data frame:\n\nand I would like to calculate a new columns as de the mean of salary_1, salary_2and salary_3.\n\ndf = pd.DataFrame({'salary_1':[230,345,222],'salary_2':[235,375,292],'salary_3':[210,385,260]})\n\n      salary_1     salary_2    salary_3\n0        230           235        210\n1        345           375        385\n2        222           292        260\n\n\nHow can I do it in pandas in the most efficient way? Actually I have many more columns and I don't want to write this one by one.\n\nSomething like this:\n\n      salary_1     salary_2    salary_3     salary_mean\n0        230           235        210     (230+235+210)/3\n1        345           375        385       ...\n2        222           292        260       ...\n\n\nThank you!\n"
"I have a csv-file with a column with strings and I want to read it with pandas. In this file the string null occurs as an actual value and should not be regarded as a missing value.\n\nExample:\n\nimport pandas as pd\nfrom io import StringIO\n\ndata = u'strings,numbers\\nfoo,1\\nbar,2\\nnull,3'\nprint(pd.read_csv(StringIO(data)))\n\n\nThis gives the following output:\n\n  strings  numbers\n0     foo        1\n1     bar        2\n2     NaN        3\n\n\nWhat can I do to get the value null as it is (and not as NaN) into the DataFrame? The file can be assumed to not contain any actually missing values.\n"
"I have searched S/O but I couldn't find a answer for this. \n\nWhen I try to plot a distribution plot using seaborn I am getting a futurewarning. I was wondering what could be the issue here.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n% matplotlib inline\nfrom sklearn import datasets\n\niris = datasets.load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['class'] = iris.target\ndf['species'] = df['class'].map({idx:s for idx, s in enumerate(iris.target_names)})\n\n\nfig, ((ax1,ax2),(ax3,ax4))= plt.subplots(2,2, figsize =(13,9))\nsns.distplot(a = df.iloc[:,0], ax=ax1)\nsns.distplot(a = df.iloc[:,1], ax=ax2)\nsns.distplot(a = df.iloc[:,2], ax=ax3)\nsns.distplot(a = df.iloc[:,3], ax=ax4)\nplt.show()\n\n\nThis is the warning:\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713:\nFutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; \nuse `arr[tuple(seq)]` instead of `arr[seq]`. \nIn the future this will be interpreted as an array index, `arr[np.array(seq)]`,\nwhich will result either in an error or a different result.\nreturn np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n\n\nAny help? You can run the above code. You'll get the warning.\n\nPandas : 0.23.4, seaborn : 0.9.0, matplotlib : 2.2.3, scipy : 1.1.0, numpy: 1.15.0'\n"
'I am having a really hard time trying to install a stable data science package configuration in docker. This should be easier with such mainstream, relevant tools. \n\nThe following is the Dockerfile that used to work, with a bit of a hack, removing pandas from the package core and installing it separately, specifying pandas&lt;0.21.0, because, allegedly, higher versions conflict with numpy.\n\n    FROM alpine:3.6\n\n    ENV PACKAGES="\\\n    dumb-init \\\n    musl \\\n    libc6-compat \\\n    linux-headers \\\n    build-base \\\n    bash \\\n    git \\\n    ca-certificates \\\n    freetype \\\n    libgfortran \\\n    libgcc \\\n    libstdc++ \\\n    openblas \\\n    tcl \\\n    tk \\\n    libssl1.0 \\\n    "\n\nENV PYTHON_PACKAGES="\\\n    numpy \\\n    matplotlib \\\n    scipy \\\n    scikit-learn \\\n    nltk \\\n    " \n\nRUN apk add --no-cache --virtual build-dependencies python3 \\\n    &amp;&amp; apk add --virtual build-runtime \\\n    build-base python3-dev openblas-dev freetype-dev pkgconfig gfortran \\\n    &amp;&amp; ln -s /usr/include/locale.h /usr/include/xlocale.h \\\n    &amp;&amp; python3 -m ensurepip \\\n    &amp;&amp; rm -r /usr/lib/python*/ensurepip \\\n    &amp;&amp; pip3 install --upgrade pip setuptools \\\n    &amp;&amp; ln -sf /usr/bin/python3 /usr/bin/python \\\n    &amp;&amp; ln -sf pip3 /usr/bin/pip \\\n    &amp;&amp; rm -r /root/.cache \\\n    &amp;&amp; pip install --no-cache-dir $PYTHON_PACKAGES \\\n    &amp;&amp; pip3 install \'pandas&lt;0.21.0\' \\    #&lt;---------- PANDAS\n    &amp;&amp; apk del build-runtime \\\n    &amp;&amp; apk add --no-cache --virtual build-dependencies $PACKAGES \\\n    &amp;&amp; rm -rf /var/cache/apk/*\n\n# set working directory\nWORKDIR /usr/src/app\n\n# add and install requirements\nCOPY ./requirements.txt /usr/src/app/requirements.txt # other than data science packages go here\nRUN pip install -r requirements.txt\n\n# add entrypoint.sh\nCOPY ./entrypoint.sh /usr/src/app/entrypoint.sh\n\nRUN chmod +x /usr/src/app/entrypoint.sh\n\n# add app\nCOPY . /usr/src/app\n\n# run server\nCMD ["/usr/src/app/entrypoint.sh"]\n\n\n\n\nThe configuration above used to work. What happens now is that build does go through, but pandas fails at import with the following error:\n\nImportError: Missing required dependencies [\'numpy\']\n\n\nSince numpy 1.16.1 was installed, I don\'t know which numpy pandas is trying to find anymore... \n\nDoes anyone know how to obtain a stable solution for this?\n\nNOTE: A solution consisting of a pull from a turnkey docker image for data science with at least the packages mentioned above, into Dockerfile above, would be also very welcomed.\n\n\n\n\n  EDIT 1:\n\n\nIf I move install of data packages into requirements.txt, as suggested in the comments, like so:\n\nrequirements.txt\n\n(...)\nnumpy==1.16.1 # or numpy==1.16.0\nscikit-learn==0.20.2\nscipy==1.2.1\nnltk==3.4   \npandas==0.24.1 # or pandas== 0.23.4\nmatplotlib==3.0.2 \n(...)\n\n\nand Dockerfile:\n\n# add and install requirements\nCOPY ./requirements.txt /usr/src/app/requirements.txt\nRUN pip install -r requirements.txt\n\n\nIt breaks again at pandas, complaining about numpy.\n\nCollecting numpy==1.16.1 (from -r requirements.txt (line 61))\n  Downloading https://files.pythonhosted.org/packages/2b/26/07472b0de91851b6656cbc86e2f0d5d3a3128e7580f23295ef58b6862d6c/numpy-1.16.1.zip (5.1MB)\nCollecting scikit-learn==0.20.2 (from -r requirements.txt (line 62))\n  Downloading https://files.pythonhosted.org/packages/49/0e/8312ac2d7f38537361b943c8cde4b16dadcc9389760bb855323b67bac091/scikit-learn-0.20.2.tar.gz (10.3MB)\nCollecting scipy==1.2.1 (from -r requirements.txt (line 63))\n  Downloading https://files.pythonhosted.org/packages/a9/b4/5598a706697d1e2929eaf7fe68898ef4bea76e4950b9efbe1ef396b8813a/scipy-1.2.1.tar.gz (23.1MB)\nCollecting nltk==3.4 (from -r requirements.txt (line 64))\n  Downloading https://files.pythonhosted.org/packages/6f/ed/9c755d357d33bc1931e157f537721efb5b88d2c583fe593cc09603076cc3/nltk-3.4.zip (1.4MB)\nCollecting pandas==0.24.1 (from -r requirements.txt (line 65))\n  Downloading https://files.pythonhosted.org/packages/81/fd/b1f17f7dc914047cd1df9d6813b944ee446973baafe8106e4458bfb68884/pandas-0.24.1.tar.gz (11.8MB)\n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File "/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py", line 359, in get_provider\n        module = sys.modules[moduleOrReq]\n    KeyError: \'numpy\'\n\n    During handling of the above exception, another exception occurred:\n\n    Traceback (most recent call last):\n      File "&lt;string&gt;", line 1, in &lt;module&gt;\n      File "/tmp/pip-install-_e5z6o6_/pandas/setup.py", line 732, in &lt;module&gt;\n        ext_modules=maybe_cythonize(extensions, compiler_directives=directives),\n      File "/tmp/pip-install-_e5z6o6_/pandas/setup.py", line 475, in maybe_cythonize\n        numpy_incl = pkg_resources.resource_filename(\'numpy\', \'core/include\')\n      File "/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py", line 1144, in resource_filename\n        return get_provider(package_or_requirement).get_resource_filename(\n      File "/usr/local/lib/python3.7/site-packages/pkg_resources/__init__.py", line 361, in get_provider\n        __import__(moduleOrReq)\n    ModuleNotFoundError: No module named \'numpy\'\n\nCommand "python setup.py egg_info" failed with error code 1 in /tmp/pip-install-_e5z6o6_/pandas/\n\n\n\n\n\n  EDIT 2: \n\n\nThis seems like an open pandas issue. For more details please refer to:\n\npandas-dev github\n\n\n  "Unfortunately, this means that a requirements.txt file is insufficient for setting up a new environment with pandas installed (like in a docker container)".\n\n\n  **ImportError**:\n\n  IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\n  Importing the multiarray numpy extension module failed.  Most\n  likely you are trying to import a failed build of numpy.\n  Here is how to proceed:\n  - If you\'re working with a numpy git repository, try `git clean -xdf`\n    (removes all files not under version control) and rebuild numpy.\n  - If you are simply trying to use the numpy version that you have installed:\n    your installation is broken - please reinstall numpy.\n  - If you have already reinstalled and that did not fix the problem, then:\n    1. Check that you are using the Python you expect (you\'re using /usr/local/bin/python),\n       and that you have no directories in your PATH or PYTHONPATH that can\n       interfere with the Python and numpy versions you\'re trying to use.\n    2. If (1) looks fine, you can open a new issue at\n       https://github.com/numpy/numpy/issues.  Please include details on:\n       - how you installed Python\n       - how you installed numpy\n       - your operating system\n       - whether or not you have multiple versions of Python installed\n       - if you built from source, your compiler versions and ideally a build log\n\n\n\n\n\n  EDIT 3\n\n\nrequirements.txt ---> https://pastebin.com/0icnx0iu\n\n\n\n\n  EDIT 4\n\n\nAs of 01/12/20, the accepted solution started not to work anymore.\nNow, build breaks not at pandas, but at scipy but after numpy, while building scipy\'s wheel. This is the log:\n\n  ----------------------------------------\n  ERROR: Failed building wheel for scipy\n  Running setup.py clean for scipy\n  ERROR: Command errored out with exit status 1:\n   command: /usr/bin/python3.6 -u -c \'import sys, setuptools, tokenize; sys.argv[0] = \'"\'"\'/tmp/pip-install-s6nahssd/scipy/setup.py\'"\'"\'; __file__=\'"\'"\'/tmp/pip-install-s6nahssd/scipy/setup.py\'"\'"\';f=getattr(tokenize, \'"\'"\'open\'"\'"\', open)(__file__);code=f.read().replace(\'"\'"\'\\r\\n\'"\'"\', \'"\'"\'\\n\'"\'"\');f.close();exec(compile(code, __file__, \'"\'"\'exec\'"\'"\'))\' clean --all\n       cwd: /tmp/pip-install-s6nahssd/scipy\n  Complete output (9 lines):\n\n  `setup.py clean` is not supported, use one of the following instead:\n\n    - `git clean -xdf` (cleans all files)\n    - `git clean -Xdf` (cleans all versioned files, doesn\'t touch\n                        files that aren\'t checked into the git repo)\n\n  Add `--force` to your command to use it anyway if you must (unsupported).\n\n  ----------------------------------------\n  ERROR: Failed cleaning build dir for scipy\nSuccessfully built numpy\nFailed to build scipy\nERROR: Could not build wheels for scipy which use PEP 517 and cannot be installed directly\n\n\nFrom the error it seems that building process is using python3.6, while I use FROM alpine:3.7.\n\nFull log here -> https://pastebin.com/Tw4ubxSA\n\nAnd this is the current Dockerfile:\n\nhttps://pastebin.com/3SftEufx\n'
"I was wondering if there is a way to groupby consecutive index numbers and move the groups in different columns. Here is an example of the DataFrame I'm using:\n\n                 0\n0     19218.965703\n1     19247.621650\n2     19232.651322\n9     19279.216956\n10    19330.087371\n11    19304.316973\n\n\nAnd my idea is to gruoup by sequential index numbers and get something like this:\n\n                 0             1\n0     19218.965703  19279.216956    \n1     19247.621650  19330.087371\n2     19232.651322  19304.316973\n\n\nIve been trying to split my data by blocks of 3 and then groupby but I was looking more about something that can be used to group and rearrange sequential index numbers.\nThank you!\n"
"Problem:\nHow to remove duplicate cell values from each row, considering each row separately (and perhaps replace them with NaNs) in a Pandas dataframe?\nIt would be even better if we could shift all newly created NaNs to the end of each row.\n\n\nReferences: related but different posts:\n\nPosts on how to remove entire rows which are deemed duplicate:\n\nhow do I remove rows with duplicate values of columns in pandas data frame?\nDrop all duplicate rows across multiple columns in Python Pandas\nRemove duplicate rows from Pandas dataframe where only some columns have the same value\n\n\nPost on how to remove duplicates from a list which is in a Pandas column:\n\nRemove duplicates from rows and columns (cell) in a dataframe, python\n\n(that answer returns a series of strings, not a dataframe)\n\n\n\n\n\n\n\nExample:\nimport pandas as pd\ndf = pd.DataFrame({'a': ['A', 'A', 'C', 'B'],\n                   'b': ['B', 'D', 'B', 'B'],\n                   'c': ['C', 'C', 'C', 'A'],\n                   'd': ['D', 'D', 'B', 'A']},\n                   index=[0, 1, 2, 3])\n\nwhich creates this df:\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\nA\nB\nC\nD\n\n\n1\nA\nD\nC\nD\n\n\n2\nC\nB\nC\nB\n\n\n3\nB\nB\nA\nA\n\n\n\n\n(Printed using this.)\n\nOne solution:\nOne way of dropping duplicates from each row, considering each row separately:\ndf = df.apply(lambda row: pd.Series(row).drop_duplicates(keep='first'),axis='columns')\n\nusing apply(), a lambda function, pd.Series(), &amp; Series.drop_duplicates().\nShove all NaNs to the end of each row, using Shift NaNs to the end of their respective rows:\ndf.apply(lambda x : pd.Series(x[x.notnull()].values.tolist()+x[x.isnull()].values.tolist()),axis='columns') \n\nOutput (as desired):\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\nA\nB\nC\nD\n\n\n1\nA\nD\nC\nnan\n\n\n2\nC\nB\nnan\nnan\n\n\n3\nB\nA\nnan\nnan\n\n\n\n\nQuestion: Is there a more efficient way to do this? Perhaps with some built-in Pandas functions?\n"
'I would like to modify a pandas MultiIndex DataFrame such that each index group includes Dates between a specified range. I would like each group to fill in missing dates 2013-06-11 to 2013-12-31 with the value 0 (or NaN).\n\nGroup A, Group B, Date,           Value\nloc_a    group_a  2013-06-11      22\n                  2013-07-02      35\n                  2013-07-09      14\n                  2013-07-30       9\n                  2013-08-06       4\n                  2013-09-03      40\n                  2013-10-01      18\n         group_b  2013-07-09       4\n                  2013-08-06       2\n                  2013-09-03       5\n         group_c  2013-07-09       1\n                  2013-09-03       2\nloc_b    group_a  2013-10-01       3\n\n\nI\'ve seen a few discussions of reindexing, but that is for a simple (non-grouped) time-series data. \n\nIs there an easy way to do this?\n\n\n\nFollowing are some attempts I\'ve made at accomplishing this.  For example: Once I\'ve unstacked by [\'A\', \'B\'], I can then reindex.\n\ndf = pd.DataFrame({\'A\': [\'loc_a\'] * 12 + [\'loc_b\'],\n                \'B\': [\'group_a\'] * 7 + [\'group_b\'] * 3 + [\'group_c\'] * 2 + [\'group_a\'],\n                \'Date\': ["2013-06-11",\n                        "2013-07-02",\n                        "2013-07-09",\n                        "2013-07-30",\n                        "2013-08-06",\n                        "2013-09-03",\n                        "2013-10-01",\n                        "2013-07-09",\n                        "2013-08-06",\n                        "2013-09-03",\n                        "2013-07-09",\n                        "2013-09-03",\n                        "2013-10-01"],\n                 \'Value\': [22, 35, 14,  9,  4, 40, 18, 4, 2, 5, 1, 2, 3]})\n\ndf.Date = df[\'Date\'].apply(lambda x: pd.to_datetime(x).date())\ndf = df.set_index([\'A\', \'B\', \'Date\'])\n\ndt_start = dt.datetime(2013,6,1)\nall_dates = [(dt_start + dt.timedelta(days=x)).date() for x in range(0,60)]\n\ndf2 = df.unstack([\'A\', \'B\'])\ndf3 = df2.reindex(index=all_dates).fillna(0)\ndf4 = df3.stack([\'A\', \'B\'])\n\n## df4 is about where I want to get, now I\'m trying to get it back in the form of df...\n\ndf5 = df4.reset_index()\ndf6 = df5.rename(columns={\'level_0\' : \'Date\'})\ndf7 = df6.groupby([\'A\', \'B\', \'Date\'])[\'Value\'].sum()\n\n\nThe last few lines make me a little sad. I was hoping that at df6 I could simply set_index back to [\'A\', \'B\', \'Date\'], but that did not group the values as they are grouped in the initial df DataFrame.\n\nAny thoughts on how I can reindex the unstacked DataFrame, restack, and have the DataFrame in the same format as the original?\n'
"I keep getting different attribute errors when trying to run this file in ipython...beginner with pandas so maybe I'm missing something\n\nCode:\n\nfrom pandas import Series, DataFrame\n\nimport pandas as pd\n\nimport json\n\nnan=float('NaN')\ndata = []\nwith open('file.json') as f:\nfor line in f:\n    data.append(json.loads(line))\n\ndf = DataFrame(data, columns=['accepted', 'user', 'object', 'response'])\nclean = df.replace('NULL', nan)\nclean = clean.dropna()\n\nprint clean.value_counts() \n\nAttributeError: 'DataFrame' object has no attribute 'value_counts'\n\n\nAny ideas?\n"
"Another Pandas question!\n\nI am writing some unit tests that test two data frames for equality, however, the test does not appear to look at the values of the data frame, only the structure:\n\ndates = pd.date_range('20130101', periods=6)\n\ndf1 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n\nprint df1\nprint df2\nself.assertItemsEqual(df1, df2)\n\n\n-->True\n\nDo I need to convert the data frames to another data structure before asserting equality?\n"
"I have a pandas DataFrame with positive and negative values as a bar chart. I want to plot the positive colors 'green' and the negative values 'red' (very original...lol).  I'm not sure how to pass if > 0 'green' else &lt; 0 'red'?\n\ndata = pd.DataFrame([[-15], [10], [8], [-4.5]],\n                    index=['a', 'b', 'c', 'd'],\n                    columns=['values'])\ndata.plot(kind='barh')\n\n\n\n"
"On the following series:\n\n0    1411161507178\n1    1411138436009\n2    1411123732180\n3    1411167606146\n4    1411124780140\n5    1411159331327\n6    1411131745474\n7    1411151831454\n8    1411152487758\n9    1411137160544\nName: my_series, dtype: int64\n\n\nThis command (convert to timestamp, localize and convert to EST) works:\n\npd.to_datetime(my_series, unit='ms').apply(lambda x: x.tz_localize('UTC').tz_convert('US/Eastern'))\n\n\nbut this one fails:\n\npd.to_datetime(my_series, unit='ms').tz_localize('UTC').tz_convert('US/Eastern')\n\n\nwith:\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-3-58187a4b60f8&gt; in &lt;module&gt;()\n----&gt; 1 lua = pd.to_datetime(df[column], unit='ms').tz_localize('UTC').tz_convert('US/Eastern')\n\n/Users/josh/anaconda/envs/py34/lib/python3.4/site-packages/pandas/core/generic.py in tz_localize(self, tz, axis, copy, infer_dst)\n   3492                 ax_name = self._get_axis_name(axis)\n   3493                 raise TypeError('%s is not a valid DatetimeIndex or PeriodIndex' %\n-&gt; 3494                                 ax_name)\n   3495             else:\n   3496                 ax = DatetimeIndex([],tz=tz)\n\nTypeError: index is not a valid DatetimeIndex or PeriodIndex\n\n\nand so does this one:\n\nmy_series.tz_localize('UTC').tz_convert('US/Eastern')\n\n\nwith: \n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-4-0a7cb1e94e1e&gt; in &lt;module&gt;()\n----&gt; 1 lua = df[column].tz_localize('UTC').tz_convert('US/Eastern')\n\n/Users/josh/anaconda/envs/py34/lib/python3.4/site-packages/pandas/core/generic.py in tz_localize(self, tz, axis, copy, infer_dst)\n   3492                 ax_name = self._get_axis_name(axis)\n   3493                 raise TypeError('%s is not a valid DatetimeIndex or PeriodIndex' %\n-&gt; 3494                                 ax_name)\n   3495             else:\n   3496                 ax = DatetimeIndex([],tz=tz)\n\nTypeError: index is not a valid DatetimeIndex or PeriodIndex\n\n\nAs far as I understand, the second approach above (the first one that fails) should work. Why does it fail?\n"
"I'm trying to count consecutive up days in equity return data - so if a positive day is 1 and a negative is 0, a list y=[0,0,1,1,1,0,0,1,0,1,1] should return z=[0,0,1,2,3,0,0,1,0,1,2].\n\nI've come to a solution which is neat in terms of number of lines of code, but is very slow:\n\nimport pandas\ny=pandas.Series([0,0,1,1,1,0,0,1,0,1,1])\ndef f(x):\n    return reduce(lambda a,b:reduce((a+b)*b,x)\nz=pandas.expanding_apply(y,f)\n\n\nI'm guessing I'm looping through the whole list y too many times. Is there a nice Pythonic way of achieving what I want while only going through the data once? I could write a loop myself but wondering if there's a better way.\n\nThanks!\n"
'I want to use a boolean to select the columns with more than 4000 entries from a dataframe comb which has over 1,000 columns. This expression gives me a Boolean (True/False) result: \n\ncriteria = comb.ix[:,\'c_0327\':].count()&gt;4000\n\n\nI want to use it to select only the True columns to a new Dataframe.\nThe following just gives me "Unalignable boolean Series key provided":\n\ncomb.loc[criteria,]\n\n\nI also tried:\n\ncomb.ix[:, comb.ix[:,\'c_0327\':].count()&gt;4000] \n\n\nSimilar to this question answer dataframe boolean selection along columns instead of row\nbut that gives me the same error: "Unalignable boolean Series key provided"\n\ncomb.ix[:,\'c_0327\':].count()&gt;4000\n\n\nyields:\n\nc_0327    False\nc_0328    False\nc_0329    False\nc_0330    False\nc_0331    False\nc_0332    False\nc_0333    False\nc_0334    False\nc_0335    False\nc_0336    False\nc_0337     True\nc_0338    False\n.....\n\n'
"I've seen this and this on formatting floating-point numbers for display in pandas, but I'm interested in doing the same thing for integers. \n\nRight now, I have: \n\npd.options.display.float_format = '{:,.2f}'.format\n\n\nThat works on the floats in my data, but will either leave annoying trailing zeroes on integers that are cast to floats, or I'll have plain integers that don't get formatted with commas.\n\nThe pandas docs mention a SeriesFormatter class about which I haven't been able to find any information.\n\nAlternatively, if there's a way to write a single string formatter that will format floats as '{:,.2f}' and floats with zero trailing decimal as '{:,d}', that'd work too.\n"
'I want to pass a datetime array to a Numba function (which cannot be vectorised and would otherwise be very slow). I understand Numba supports numpy.datetime64. However, it seems it supports datetime64[D] (day precision) but not datetime64[ns] (millisecond precision) (I learnt this the hard way: is it documented?).\n\nI tried to convert from datetime64[ns] to datetime64[D], but can\'t seem to find a way! Any ideas?\n\nI have summarised my problem with the minimal code below. If you run testdf(mydates), which is datetime64[D], it works fine. If you run testdf(dates_input), which is datetime64[ns], it doesn\'t. Note that this example simply passes the dates to the Numba function, which doesn\'t (yet) do anything with them. I try to convert dates_input to datetime64[D], but the conversion doesn\'t work. In my original code I read from a SQL table into a pandas dataframe, and need a column which changes the day of each date to the 15th.\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport datetime\n\nmydates =np.array([\'2010-01-01\',\'2011-01-02\']).astype(\'datetime64[D]\')\ndf=pd.DataFrame()\ndf["rawdate"]=mydates\ndf["month_15"] = df["rawdate"].apply(lambda r: datetime.date( r.year, r.month,15 ) )\n\ndates_input = df["month_15"].astype(\'datetime64[D]\')\nprint dates_input.dtype # Why datetime64[ns] and not datetime64[D] ??\n\n\n@numba.jit(nopython=True)\ndef testf(dates):\n    return 1\n\nprint testf(mydates)\n\n\nThe error I get if I run testdf(dates_input) is:\n\nnumba.typeinfer.TypingError: Failed at nopython (nopython frontend)\nVar \'dates\' unified to object: dates := {pyobject}\n\n'
'Consider a dataframe with three columns: group_ID, item_ID and value. Say we have 10 itemIDs total.\n\nI need to rank each item_ID (1 to 10) within each group_ID based on value, and then see the mean rank (and other stats) across groups (e.g. the IDs with the highest value across groups would get ranks closer to 1). How can I do this in \nPandas?\n\nThis answer does something very close with qcut, but not exactly the same.\n\n\n\nA data example would look like:\n\n      group_ID   item_ID  value\n0   0S00A1HZEy        AB     10\n1   0S00A1HZEy        AY      4\n2   0S00A1HZEy        AC     35\n3   0S03jpFRaC        AY     90\n4   0S03jpFRaC        A5      3\n5   0S03jpFRaC        A3     10\n6   0S03jpFRaC        A2      8\n7   0S03jpFRaC        A4      9\n8   0S03jpFRaC        A6      2\n9   0S03jpFRaC        AX      0\n\n\nwhich would result in:\n\n      group_ID   item_ID   rank\n0   0S00A1HZEy        AB      2\n1   0S00A1HZEy        AY      3\n2   0S00A1HZEy        AC      1\n3   0S03jpFRaC        AY      1\n4   0S03jpFRaC        A5      5\n5   0S03jpFRaC        A3      2\n6   0S03jpFRaC        A2      4\n7   0S03jpFRaC        A4      3\n8   0S03jpFRaC        A6      6\n9   0S03jpFRaC        AX      7\n\n'
"I am trying to check if a certain value is contained in a python column. I'm using df.date.isin(['07311954']), which I do not doubt to be a good tool. The problem is that I have over 350K rows and the output won't show \nall of them so that I can see if the value is actually contained. Put simply, I just want to know (Y/N) whether or not a specific value is contained in a column. My code follows:\n\nimport numpy as np\nimport pandas as pd\nimport glob\n\n\ndf = (pd.read_csv('/home/jayaramdas/anaconda3/Thesis/FEC_data/itpas2_data/itpas214.txt',\\\n    sep='|', header=None, low_memory=False, names=['1', '2', '3', '4', '5', '6', '7', \\\n    '8', '9', '10', '11', '12', '13', 'date', '15', '16', '17', '18', '19', '20', \\\n    '21', '22']))\n\ndf.date.isin(['07311954'])\n\n"
"I have to plot pie-chart and a table side by side using matplotlib.\n\nFor drawing the pie-chart, I use the below code:\n\nimport matplotlib.pyplot as plt\ndf1.EventLogs.value_counts(sort=False).plot.pie()\nplt.show()\n\n\nFor drawing a table, I use the below code:\n\n%%chart table --fields MachineName --data df_result2\n\n\ndf_result2 is a table with the list of MachineName's in it.\n\nNot sure whether we can place both pie chart and table side by side. Any help would be appreciated.\n"
"I have a dataframe df\n\ndf = pd.DataFrame(np.arange(20).reshape(10, -1),\n                  [['a', 'a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd'],\n                   ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']],\n                  ['X', 'Y'])\n\n\nHow do I get the first and last rows, grouped by the first level of the index?\n\nI tried\n\ndf.groupby(level=0).agg(['first', 'last']).stack()\n\n\nand got\n\n          X   Y\na first   0   1\n  last    6   7\nb first   8   9\n  last   12  13\nc first  14  15\n  last   16  17\nd first  18  19\n  last   18  19\n\n\nThis is so close to what I want.  How can I preserve the level 1 index and get this instead:\n\n      X   Y\na a   0   1\n  d   6   7\nb e   8   9\n  g  12  13\nc h  14  15\n  i  16  17\nd j  18  19\n  j  18  19\n\n"
'I\'m very new in python and would like to learn using pandas, but I can\'t even use the package properly in my python.\n\nIn the terminal I tried\n\n$ conda install pandas\n\n\nThe result is:\n\nFetching package metadata .......\nSolving package specifications: ..........\n\n# All requested packages already installed.\n# packages in environment at /Users/rudyyuan/anaconda:\n#\npandas                    0.18.1              np111py27_0  \nRudy-Yuans-MBP:~ rudyyuan$ \n\n\nThen I tried to install using pip:\n\nRudy-Yuans-MBP:~ rudyyuan$ pip install pandas\nRequirement already satisfied (use --upgrade to upgrade): pandas in        \n./anaconda/lib/python2.7/site-packages\nRequirement already satisfied (use --upgrade to upgrade): python-dateutil in     \n./anaconda/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied (use --upgrade to upgrade): pytz&gt;=2011k in    \n./anaconda/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.7.0 in \n./anaconda/lib/python2.7/site-packages (from pandas)\nRequirement already satisfied (use --upgrade to upgrade): six&gt;=1.5 in\n./anaconda/lib/python2.7/site-packages (from python-dateutil-&gt;pandas)\n\n\nThen I enter \n\n$ python\n\n\nThen I enter \n\n$ import pandas\n\n\nThe result of the commands entered:\n\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-  \npackages/pandas/__init__.py", line 39, in &lt;module&gt;\n    from pandas.core.api import *\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-  packages/pandas/core/api.py", line 10, in &lt;module&gt;\nfrom pandas.core.groupby import Grouper\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/pandas/core/groupby.py", line 18, in &lt;module&gt;\n    from pandas.core.frame import DataFrame\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/pandas/core/frame.py", line 39, in &lt;module&gt;\n    from pandas.core.series import Series\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/pandas/core/series.py", line 2944, in &lt;module&gt;\n    import pandas.tools.plotting as _gfx  # noqa\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/pandas/tools/plotting.py", line 27, in &lt;module&gt;\n    import pandas.tseries.converter as conv\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-   packages/pandas/tseries/converter.py", line 7, in &lt;module&gt;\n    import matplotlib.units as units\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py", line 1131, in &lt;module&gt;\n    rcParams = rc_params()\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-  \npackages/matplotlib/__init__.py", line 975, in rc_params\n    return rc_params_from_file(fname, fail_on_error)\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py", line 1100, in rc_params_from_file\n    config_from_file = _rc_params_in_file(fname, fail_on_error)\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-packages/matplotlib/__init__.py", line 1018, in _rc_params_in_file\n     with _open_file_or_url(fname) as fd:\n  File "/Users/rudyyuan/anaconda/lib/python2.7/contextlib.py", line 17, in __enter__\n    return self.gen.next()\n  File "/Users/rudyyuan/anaconda/lib/python2.7/site-   packages/matplotlib/__init__.py", line 1000, in _open_file_or_url\n    encoding = locale.getdefaultlocale()[1]\n  File "/Users/rudyyuan/anaconda/lib/python2.7/locale.py", line 545, in getdefaultlocale\n    return _parse_localename(localename)\n  File "/Users/rudyyuan/anaconda/lib/python2.7/locale.py", line 477, in   _parse_localename\n    raise ValueError, \'unknown locale: %s\' % localename\nValueError: unknown locale: UTF-8\n\n\nBut when I tried to check packages locally using this:\n\nimport pip\nsorted(["%s==%s" % (i.key, i.version) for i in pip.get_installed_distributions()])\n\n\nit shown pandas already inside....:\n\n[\'alabaster==0.7.8\', \'anaconda-client==1.4.0\', \'anaconda-navigator==1.2.1\',    \n\'appnope==0.1.0\', \'appscript==1.0.1\', \'argcomplete==1.0.0\', \'astropy==1.2.1\',   \n\'babel==2.3.3\', \'backports-abc==0.4\', \'backports.shutil-get-terminal-\nsize==1.0.0\', \'backports.ssl-match-hostname==3.4.0.2\', \n\'beautifulsoup4==4.4.1\', \'bitarray==0.8.1\', \'blaze==0.10.1\', \'bokeh==0.12.0\', \n\'boto==2.40.0\', \'bottleneck==1.1.0\', \'bz2file==0.98\', \'cdecimal==2.3\', \'      \ncffi==1.6.0\', \'chest==0.2.3\', \'click==6.6\', \'cloudpickle==0.2.1\', \'clyent==1.2.2\', \'colorama==0.3.7\', \'conda-build==1.21.3\', \'conda-env==2.5.0a0\', \'conda==4.1.11\', \'configobj==5.0.6\', \'configparser==3.5.0b2\', \'contextlib2==0.5.3\', \'cryptography==1.4\', \'cycler==0.10.0\', \'cymem==1.31.2\', \'cython==0.24\', \'cytoolz==0.8.0\', \'dask==0.10.0\', \'datashape==0.5.2\', \'decorator==4.0.10\', \'dill==0.2.5\', \'docutils==0.12\', \'dynd==0.7.3.dev1\', \'enum34==1.1.6\', \'et-xmlfile==1.0.1\', \'fastcache==1.0.2\', \'flake8-docstrings==1.0.2\', \'flake8==3.0.4\', \'flask-cors==2.1.2\', \'flask==0.11.1\', \'funcsigs==1.0.2\', \'functools32==3.2.3.post2\', \'futures==3.0.5\', \'gensim==0.12.4\', \'gevent==1.1.1\', \'greenlet==0.4.10\', \'grin==1.2.1\', \'h5py==2.6.0\', \'heapdict==1.0.0\', \'idna==2.1\', \'imagesize==0.7.1\', \'ipaddress==1.0.16\', \'ipykernel==4.3.1\', \'ipython-genutils==0.1.0\', \'ipython==4.2.0\', \'ipywidgets==4.1.1\', \'itsdangerous==0.24\', \'jdcal==1.2\', \'jedi==0.9.0\', \'jinja2==2.8\', \'jsonschema==2.5.1\', \'jupyter-client==4.3.0\', \'jupyter-console==4.1.1\', \'jupyter-core==4.1.0\', \'jupyter==1.0.0\', \'llvmlite==0.11.0\', \'locket==0.2.0\', \'lxml==3.6.0\', \'markupsafe==0.23\', \'matplotlib==1.5.1\', \'mccabe==0.5.2\', \'mistune==0.7.2\', \'mpmath==0.19\', \'multipledispatch==0.4.8\', \'murmurhash==0.26.4\', \'nb-anacondacloud==1.1.0\', \'nb-conda-kernels==1.0.3\', \'nb-conda==1.1.0\', \'nbconvert==4.2.0\', \'nbformat==4.0.1\', \'nbpresent==3.0.2\', \'networkx==1.11\', \'nltk==3.2.1\', \'nose==1.3.7\', \'notebook==4.2.1\', \'numba==0.26.0\', \'numexpr==2.6.0\', \'numpy==1.11.1\', \'odo==0.5.0\',  \n`enter code here`\'openpyxl==2.3.2\', \'pandas==0.18.1\', \'partd==0.3.4\', \'path.py==0.0.0\', \'pathlib2==2.1.0\', \'patsy==0.4.1\', \'pep8==1.7.0\', \'pexpect==4.0.1\', \'pickleshare==0.7.2\', \'pillow==3.2.0\', \'pip==8.1.2\', \'plac==0.9.6\', \'ply==3.8\', \'preshed==0.46.4\', \'psutil==4.3.0\', \'ptyprocess==0.5.1\', \'py==1.4.31\', \'pyasn1==0.1.9\', \'pyaudio==0.2.7\', \'pycodestyle==2.0.0\', \'pycosat==0.6.1\', \'pycparser==2.14\', \'pycrypto==2.6.1\', \'pycurl==7.43.0\', \'pydocstyle==1.0.0\', \'pyflakes==1.2.3\', \'pygments==2.1.3\', \'pyopenssl==16.0.0\', \'pyparsing==2.1.4\', \'pytest==2.9.2\', \'python-\',\n\n\nWhats the issue and how to fix this? Anything related with path or something?\n\nI\'m very new to python.\n\n\nWhats wrong with it?\nHow can we fix it? \n\n'
"consider the pd.Series s\n\ns = pd.Series(list('abcdefghij'), list('ABCDEFGHIJ'))\ns\n\nA    a\nB    b\nC    c\nD    d\nE    e\nF    f\nG    g\nH    h\nI    i\nJ    j\ndtype: object\n\n\nWhat is the quickest way to swap index and values and get the following\n\na    A\nb    B\nc    C\nd    D\ne    E\nf    F\ng    G\nh    H\ni    I\nj    J\ndtype: object\n\n"
'How can I calculate the elapsed months using pandas? I have write the following, but this code is not elegant. Could you tell me a better way?\n\nimport pandas as pd\n\ndf = pd.DataFrame([pd.Timestamp(\'20161011\'),\n                   pd.Timestamp(\'20161101\') ], columns=[\'date\'])\ndf[\'today\'] = pd.Timestamp(\'20161202\')\n\ndf = df.assign(\n    elapsed_months=(12 *\n                    (df["today"].map(lambda x: x.year) -\n                     df["date"].map(lambda x: x.year)) +\n                    (df["today"].map(lambda x: x.month) -\n                     df["date"].map(lambda x: x.month))))\n# Out[34]: \n#         date      today  elapsed_months\n# 0 2016-10-11 2016-12-02               2\n# 1 2016-11-01 2016-12-02               1\n\n'
'How do i query for the closest index from a Pandas DataFrame? The index is DatetimeIndex\n\n2016-11-13 20:00:10.617989120   7.0 132.0\n2016-11-13 22:00:00.022737152   1.0 128.0\n2016-11-13 22:00:28.417561344   1.0 132.0\n\n\nI tried this:\n\ndf.index.get_loc(df.index[0], method=\'nearest\')\n\n\nbut it give me InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n\nSame error if I tried this:\n\ndt =datetime.datetime.strptime("2016-11-13 22:01:25", "%Y-%m-%d %H:%M:%S")\ndf.index.get_loc(dt, method=\'nearest\')\n\n\nBut if I remove method=\'nearest\' it works, but that is not I want, I want to find the closest index from my query datetime\n'
"I have a pandas.Series containing integers, but I need to convert these to strings for some downstream tools. So suppose I had a Series object:\n\nimport numpy as np\nimport pandas as pd\n\nx = pd.Series(np.random.randint(0, 100, 1000000))\n\n\nOn StackOverflow and other websites, I've seen most people argue that the best way to do this is:\n\n%% timeit\nx = x.astype(str)\n\n\nThis takes about 2 seconds. \n\nWhen I use x = x.apply(str), it only takes 0.2 seconds.\n\nWhy is x.astype(str) so slow? Should the recommended way be x.apply(str)? \n\nI'm mainly interested in python 3's behavior for this. \n"
'df = df[~df["column"].str.contains("Total")]\n\nTypeError: bad operand type for unary ~: \'float\'\n\n\nWhy does .str.contains() return a float? What should I be doing here?\n'
"I have two pandas dataframes one called 'orders' and another one called 'daily_prices'.\ndaily_prices is as follows:\n\n              AAPL    GOOG     IBM    XOM\n2011-01-10  339.44  614.21  142.78  71.57\n2011-01-13  342.64  616.69  143.92  73.08\n2011-01-26  340.82  616.50  155.74  75.89\n2011-02-02  341.29  612.00  157.93  79.46\n2011-02-10  351.42  616.44  159.32  79.68\n2011-03-03  356.40  609.56  158.73  82.19\n2011-05-03  345.14  533.89  167.84  82.00\n2011-06-03  340.42  523.08  160.97  78.19\n2011-06-10  323.03  509.51  159.14  76.84\n2011-08-01  393.26  606.77  176.28  76.67\n2011-12-20  392.46  630.37  184.14  79.97\n\n\norders is as follows:\n\n           direction  size ticker  prices\n2011-01-10       Buy  1500   AAPL  339.44\n2011-01-13      Sell  1500   AAPL  342.64\n2011-01-13       Buy  4000    IBM  143.92\n2011-01-26       Buy  1000   GOOG  616.50\n2011-02-02      Sell  4000    XOM   79.46\n2011-02-10       Buy  4000    XOM   79.68\n2011-03-03      Sell  1000   GOOG  609.56\n2011-03-03      Sell  2200    IBM  158.73\n2011-06-03      Sell  3300    IBM  160.97\n2011-05-03       Buy  1500    IBM  167.84\n2011-06-10       Buy  1200   AAPL  323.03\n2011-08-01       Buy    55   GOOG  606.77\n2011-08-01      Sell    55   GOOG  606.77\n2011-12-20      Sell  1200   AAPL  392.46\n\n\nindex of both dataframes is datetime.date.\n'prices' column in the 'orders' dataframe was added by using a list comprehension to loop through all the orders and look up the specific ticker for the specific date in the 'daily_prices' data frame and then adding that list as a column to the 'orders' dataframe. I would like to do this using an array operation rather than something that loops. can it be done? i tried to use:\n\ndaily_prices.ix[dates,tickers] \n\nbut this returns a matrix of cartesian product of the two lists. i want it to return a column vector of only the  price of a specified ticker for a specified date.\n"
'I have a pandas.DataFrame with measurements taken at consecutive points in time. Along with each measurement the system under observation had a distinct state at each point in time. Hence, the DataFrame also contains a column with the state of the system at each measurement. State changes are much slower than the measurement interval. As a result, the column indicating the states might look like this (index: state):\n\n1:  3\n2:  3\n3:  3\n4:  3\n5:  4\n6:  4\n7:  4\n8:  4\n9:  1\n10: 1\n11: 1\n12: 1\n13: 1\n\n\nIs there an easy way to retrieve the indices of each segment of consecutively equal states. That means I would like to get something like this:\n\n[[1,2,3,4], [5,6,7,8], [9,10,11,12,13]]\n\n\nThe result might also be in something different than plain lists.\n\nThe only solution I could think of so far is manually iterating over the rows, finding segment change points and reconstructing the indices from these change points, but I have the hope that there is an easier solution.\n'
'I have two dataframes which look like this:\n\n&gt;&gt;&gt; df1\n              A    B\n2000-01-01  1.4  1.4\n2000-01-02  1.7 -1.9\n2000-01-03 -0.2 -0.8\n\n&gt;&gt;&gt; df2\n              A    B\n2000-01-01  0.6 -0.3\n2000-01-02 -0.4  0.6\n2000-01-03  1.1 -1.0\n\n\nHow can I make one dataframe out of this two with hierarchical column index like below?\n\n            df1       df2\n              A    B    A    B\n2000-01-01  1.4  1.4  0.6 -0.3\n2000-01-02  1.7 -1.9 -0.4  0.6\n2000-01-03 -0.2 -0.8  1.1 -1.0\n\n'
"I want to replace negative values in a pandas DataFrame column with zero.\n\nIs there a more concise way to construct this expression?\n\ndf['value'][df['value'] &lt; 0] = 0\n\n"
"I'm using python 2.7.3 and Pandas version 0.12.0.\n\nI want to drop the row with the NaN index so that I only have valid site_id values.\n\nprint df.head()\n            special_name\nsite_id\nNaN          Banana\nOMG          Apple\n\ndf.drop(df.index[0])\n\nTypeError: 'NoneType' object is not iterable\n\n\nIf I try dropping a range, like this:\n\ndf.drop(df.index[0:1])\n\n\nI get this error:\n\nAttributeError: 'DataFrame' object has no attribute 'special_name'\n\n"
"The View is a very useful function to allow me to see cross-section of large data frames in R. \n\nIs there any equivalent of R's View function for Python's pandas DataFrame? \n\nI use RStudio for R and PyCharm for Python. \n"
"I have two Pandas DataFrames that I'm hoping to plot in single figure. I'm using IPython notebook.\n\nI would like the legend to show the label for both of the DataFrames, but so far I've been able to get only the latter one to show. Also any suggestions as to how to go about writing the code in a more sensible way would be appreciated. I'm new to all this and don't really understand object oriented plotting.\n\n%pylab inline\nimport pandas as pd\n\n#creating data\n\nprng = pd.period_range('1/1/2011', '1/1/2012', freq='M')\nvar=pd.DataFrame(randn(len(prng)),index=prng,columns=['total'])\nshares=pd.DataFrame(randn(len(prng)),index=index,columns=['average'])\n\n#plotting\n\nax=var.total.plot(label='Variance')\nax=shares.average.plot(secondary_y=True,label='Average Age')\nax.left_ax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\nplt.legend(loc='upper center')\nplt.title('Wage Variance and Mean Age')\nplt.show()\n\n\n\n"
"I have a DataFrame (df1) with a dimension 2000 rows x 500 columns (excluding the index) for which I want to divide each row by another DataFrame (df2) with dimension 1 rows X 500 columns. Both have the same column headers. I tried:\n\ndf.divide(df2) and \ndf.divide(df2, axis='index') and multiple other solutions and I always get a df with nan values in every cell. What argument am I missing in the function df.divide?\n"
'I have a multi level column table like this:\n\n    a\n   ---+---+---\n    b | c | f\n--+---+---+---\n0 | 1 | 2 | 7\n1 | 3 | 4 | 9\n\n\nHow can I drop column "c" by name? to look like this:\n\n    a\n   ---+---\n    b | f\n--+---+---\n0 | 1 | 7\n1 | 3 | 9\n\n\nI tried this:\n\ndel df[\'c\']\n\n\nbut I get the following error, which makes sense:\n\n\n  KeyError: \'Key length (1) was greater than MultiIndex lexsort depth (0)\'\n\n'
'What is the python equivalent of this in operator?  I am trying to filter down a pandas database by having rows only remain if a column in the row has a value found in my list. \n\nI tried using any() and am having immense difficulty with this. \n'
'I would like to create the following histogram (see image below) taken from the book "Think Stats". However, I cannot get them on the same plot. Each DataFrame takes its own subplot.  \n\nI have the following code: \n\nimport nsfg\nimport matplotlib.pyplot as plt\ndf = nsfg.ReadFemPreg()\npreg = nsfg.ReadFemPreg()\nlive = preg[preg.outcome == 1]\n\nfirst = live[live.birthord == 1]\nothers = live[live.birthord != 1]\n\n#fig = plt.figure()\n#ax1 = fig.add_subplot(111)\n\nfirst.hist(column = \'prglngth\', bins = 40, color = \'teal\', \\\n           alpha = 0.5)\nothers.hist(column = \'prglngth\', bins = 40, color = \'blue\', \\\n            alpha = 0.5)\nplt.show()\n\n\nThe above code does not work when I use ax = ax1 as suggested in: pandas multiple plots not working as hists nor this example does what I need: Overlaying multiple histograms using pandas. When I use the code as it is, it creates two windows with histograms. Any ideas how to combine them?  \n\nHere\'s an example of how I\'d like the final figure to look:\n\n'
"I have a normal df.index that I would like to add some hours to it.\n\nIn [1]: test[1].index\nOut[2]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2010-03-11, ..., 2014-08-14]\nLength: 52, Freq: None, Timezone: None\n\n\nThis is how the first element looks like:\n\nIn [1]: test[1].index[0]\nOut[2]: Timestamp('2010-03-11 00:00:00')\n\n\nSo I try this to add the hours:\n\nIn [1]: test[1].index[0] + pd.tseries.timedeltas.to_timedelta(16, unit='h')\n\n\nHowever I get this:\n\nOut[2]: Timestamp('2010-03-11 00:00:00.000000016')\n\n\nBut I would like to get this:\n\nOut[2]: Timestamp('2010-03-11 16:00:00')\n\n\nWhat I am missing?. The enviroment is Anaconda (latest) Python 2.7.7, iPython 2.2\n\nThanks a lot\n"
'I am going around in circles and tried so many different ways so I guess my core understanding is wrong. I would be grateful for help in understanding my encoding/decoding issues.\n\nI import the dataframe from SQL and it seems that some datatypes:float64 are converted to Object. Thus, I cannot do any calculation. I fail to convert the Object back to float64.\n\ndf.head()\n\nDate        WD  Manpower 2nd     CTR    2ndU    T1  \u3000\u3000T2    \u3000\u3000T3    \u3000\u3000T4 \n\n2013/4/6    6   NaN     2,645   5.27%   0.29    407     533     454     368\n2013/4/7    7   NaN     2,118   5.89%   0.31    257     659     583     369\n2013/4/13   6   NaN     2,470   5.38%   0.29    354     531     473 \u3000\u3000383\n2013/4/14   7   NaN     2,033   6.77%   0.37    396     748     681     458\n2013/4/20   6   NaN     2,690   5.38%   0.29    361     528     541     381\n\n\ndf.dtypes\n\nWD             float64\nManpower       float64\n2nd             object\nCTR             object\n2ndU           float64\nT1              object\nT2              object\nT3              object\nT4              object\nT5              object\n\ndtype: object\n\n\nSQL table:\n\n\n'
"I have a large time series data frame (called df), and the first 5 records look like this:\n\ndf\n\n         stn     years_of_data  total_minutes avg_daily TOA_daily   K_daily\ndate                        \n1900-01-14  AlberniElementary      4    5745    34.100  114.600 0.298\n1900-01-14  AlberniWeather         6    7129    29.500  114.600 0.257\n1900-01-14  Arbutus                8    11174   30.500  114.600 0.266\n1900-01-14  Arrowview              7    10080   27.600  114.600 0.241\n1900-01-14  Bayside                7    9745    33.800  114.600 0.295\n\n\nGoal:\n\n\n  I am trying to remove rows where any of the strings in a list\n  are present in the 'stn' column. So,I am basically trying to filter this dataset to not include rows containing any of the strings in following list.\n\n\nAttempt:\n\nremove_list = ['Arbutus','Bayside']\n\ncleaned = df[df['stn'].str.contains('remove_list')]\n\n\nReturns:\n\nOut[78]:\n\nstn years_of_data   total_minutes   avg_daily   TOA_daily   K_daily\ndate    \n\n\nNothing!\n\nI have tried a few combinations of quotes, brackets, and even a lambda function; though I am fairly new, so probably not using syntax properly..\n"
'I installed Anaconda with python 2.7.7. However, whenever I run "import pandas" I get the error: "ImportError: C extension: y not built. If you want to import pandas from the source directory, you may need to run \'python setup.py build_ext --inplace\' to build the C extensions first." I tried running the suggested command but it stated that\n\nskipping \'pandas\\index.c\' Cython extension (up-to-date)      \nskipping \'pandas\\src\\period.c\' Cython extension (up-to-date) \nskipping \'pandas\\algos.c\' Cython extension (up-to-date)      \nskipping \'pandas\\lib.c\' Cython extension (up-to-date)        \nskipping \'pandas\\tslib.c\' Cython extension (up-to-date)      \nskipping \'pandas\\parser.c\' Cython extension (up-to-date)     \nskipping \'pandas\\hashtable.c\' Cython extension (up-to-date)  \nskipping \'pandas\\src\\sparse.c\' Cython extension (up-to-date) \nskipping \'pandas\\src\\testing.c\' Cython extension (up-to-date)\nskipping \'pandas\\msgpack.cpp\' Cython extension (up-to-date)\n\n\nHas anyone encountered this before and found a solution?\n'
"same as this python pandas: how to find rows in one dataframe but not in another?\nbut with multiple columns\n\nThis is the setup:\n\nimport pandas as pd\n\ndf = pd.DataFrame(dict(\n    col1=[0,1,1,2],\n    col2=['a','b','c','b'],\n    extra_col=['this','is','just','something']\n))\n\nother = pd.DataFrame(dict(\n    col1=[1,2],\n    col2=['b','c']\n))\n\n\nNow, I want to select the rows from df which don't exist in other. I want to do the selection by col1 and col2\n\nIn SQL I would do:\n\nselect * from df \nwhere not exists (\n    select * from other o \n    where df.col1 = o.col1 and \n    df.col2 = o.col2\n)\n\n\nAnd in Pandas I can do something like this but it feels very ugly. Part of the ugliness could be avoided if df had id-column but it's not always available. \n\nkey_col = ['col1','col2']\ndf_with_idx = df.reset_index()\ncommon = pd.merge(df_with_idx,other,on=key_col)['index']\nmask = df_with_idx['index'].isin(common)\n\ndesired_result =  df_with_idx[~mask].drop('index',axis=1)\n\n\nSo maybe there is some more elegant way?\n"
"I have the following data frame just single column.\n\nimport pandas as pd\ntdf =  pd.DataFrame({'s1' : [0,1,23.4,10,23]})\n\n\nCurrently it has the following shape.\n\nIn [54]: tdf.shape\nOut[54]: (5, 1)\n\n\nHow can I convert it to a Series or a numpy vector so that the shape is simply (5,)\n"
'TL\'DR, the vertical bar charts are shown in a conventional way -- things line up from left to right. However, when it is converted to horizontal bar chart (from bar to barh), everything is upside-down. I.e., for a grouped bar chart, not only the order of the grouped bar is wrong, the order of the each group is wrong as well.\n\nFor e.g., the graph from http://dwheelerau.com/2014/05/28/pandas-data-analysis-new-zealanders-and-their-sheep/\n\n\n\nIf you look closely, you will find that the the bar and legend are in reverse order -- Beef shows on top in legend but on bottom in the graph. \n\nAs the simplest demo, I changed kind=\'bar\', to kind=\'barh\',\nfrom this graph\nhttps://plot.ly/pandas/bar-charts/#pandas-grouped-bar-chart\nand the result looks like this:\nhttps://plot.ly/7/~xpt/\n\nI.e., the bars in the horizontal grouped bar chart is ordered upside-down.  \n\nHow to fix it?\n\nEDIT: @Ajean, it is actually not only the order of the grouped bar is wrong, the order of the each group is wrong as well. The graph from Simple customization of matplotlib/pandas bar chart (labels, ticks, etc.) shows it clearly:\n\n\n\nWe can see that the order is unconventional too, because people would expect the graph to be top-down, with "AAA" at the top, not the bottom. \n\nIf you search for "Excel upside-down", you will find people are complaining about this in Excel all over the places. The Microsoft Excel has a fix for it, do Matplotlib/Panda/Searborn/Ploty/etc has a fix for it? \n'
'I plot a piechart using pyplot.\n\nimport pylab\nimport pandas as pd\ntest = pd.Series([\'male\', \'male\', \'male\', \'male\', \'female\'], name="Sex")\ntest = test.astype("category")\ngroups = test.groupby([test]).agg(len)\ngroups.plot(kind=\'pie\', shadow=True)\npylab.show()\n\n\nThe result:\n\n\n\nHowever, I\'m unable to remove the label on the left (marked red in the picture). I already tried\n\nplt.axes().set_xlabel(\'\')\n\n\nand\n\nplt.axes().set_ylabel(\'\')\n\n\nbut that did not work.\n'
'I have the foll. dataframe:\n\ndf\n\n   A   B\n0  23  12\n1  21  44\n2  98  21\n\n\nHow do I remove the column names A and B from this dataframe? One way might be to write it into a csv file and then read it in specifying header=None. is there a way to do that without writing out to csv and re-reading?\n'
'I have a Pandas DataFrame with a column containing lists objects\n\n      A\n0   [1,2]\n1   [3,4]\n2   [8,9] \n3   [2,6]\n\n\nHow can I access the first element of each list and save it into a new column of the DataFrame? To get a result like this:\n\n      A     new_col\n0   [1,2]      1\n1   [3,4]      3\n2   [8,9]      8\n3   [2,6]      2\n\n\nI know this could be done via iterating over each row, but is there any "pythonic" way? \n'
"I have the following Pandas dataframe in Python 2.7.\n\nimport pandas as pd\ntrial_num = [1,2,3,4,5]\nsail_rem_time = ['11:33:11','16:29:05','09:37:56','21:43:31','17:42:06']\ndfc = pd.DataFrame(zip(*[trial_num,sail_rem_time]),columns=['Temp_Reading','Time_of_Sail'])\nprint dfc\n\n\nThe dataframe looks like this:\n\n  Temp_Reading Time_of_Sail\n             1     11:33:11\n             2     16:29:05\n             3     09:37:56\n             4     21:43:31\n             5     17:42:06\n\n\nThis dataframe comes from a *.csv file. I use Pandas to read in the *.csv file as a Pandas dataframe. When I use print dfc.dtypes, it shows me that the column Time_of_Sail has a datatype object. I would like to convert this column to datetime datatype BUT I only want the time part - I don't want the year, month, date.\n\nI can try this:\n\ndfc['Time_of_Sail'] = pd.to_datetime(dfc['Time_of_Sail'])\ndfc['Time_of_Sail'] = [time.time() for time in dfc['Time_of_Sail']]\n\n\nbut the problem is that the when I run print dfc.dtypes it still shows that the column Time_of_Sail is object.\n\nIs there a way to convert this column into a datetime format that only has the time?\n\nAdditional Information:\n\nTo create the above dataframe and output, this also works:\n\nimport pandas as pd\ntrial_num = [1,2,3,4,5]\nsail_rem_time = ['11:33:11','16:29:05','09:37:56','21:43:31','17:42:06']\ndata = [\n    [trial_num[0],sail_rem_time[0]],\n    [trial_num[1],sail_rem_time[1]],[trial_num[2],sail_rem_time[2]],\n    [trial_num[3],sail_rem_time[3]]\n    ]\ndfc = pd.DataFrame(data,columns=['Temp_Reading','Time_of_Sail'])\ndfc['Time_of_Sail'] = pd.to_datetime(dfc['Time_of_Sail'])\ndfc['Time_of_Sail'] = [time.time() for time in dfc['Time_of_Sail']]\nprint dfc\nprint dfc.dtypes\n\n"
"After setting a DataFrame to redis, then getting it back, redis returns a string and I can't figure out a way to convert this str to a DataFrame.\n\nHow can I do these two appropriately?\n"
"I am trying to use django with pandas for data analysis. There seem to be no simple step by step tutorial on this. All the ones i have seen online just explain how to write the code in your django views.py file but none shows how to display the final product in the browser.\nHere is the code in my views.py\ndef index2(request):\n    qs = Product.objects.all()\n    df = read_frame(qs)\n    html= df.to_html\n    return HttpResponse(html)\n\nbut this does not work. Any detailed help will be appreciated. Please dont just point me to some documentation. In fact, most of django's documentation is not written in simple plain english --- it is even more confusing to some of us. Thank you.\n"
"How can I transform my resulting dask.DataFrame into pandas.DataFrame (let's say I am done with heavy lifting, and just want to apply sklearn to my aggregate result)?\n"
'in one of my scripts I\'m selecting several columns of a dataframe, by a list of the column names. The following code works:\n\ndata = df[lst]\n\n\nIt works fine as long as all elements of the list are included in the dataframe. If that\'s not the case, it will return the error "\'....\' not in index".\n\nIs there a possibility to still select all columns which column name is included in that list, even if not all elements of the list are included in the dataframe? \n'
'I have data frames which contain e.g.:\n&quot;vendor a::ProductA&quot;\n&quot;vendor b::ProductA&quot;\n&quot;vendor a::Productb&quot;\n\nI need to remove everything (and including) the two :: so that I end up with:\n&quot;vendor a&quot;\n&quot;vendor b&quot;\n&quot;vendor a&quot;\n\nI tried str.trim (which seems to not exist) and str.split without success.\nwhat would be the easiest way to accomplish this?\n'
'I can clear the text of the xlabel in a Pandas plot with:\n\nplt.xlabel("")\n\n\nInstead, is it possible to hide the label?\n\nMay be something like .xaxis.label.set_visible(False).\n'
"print('http://google.com') outputs a clickable url.\n\nHow do I get clickable URLs for pd.DataFrame(['http://google.com', 'http://duckduckgo.com']) ?\n"
'I have a DataFrame df filled with rows and columns where there are duplicate Id\'s:\n\nIndex   Id   Type\n0       a1   A\n1       a2   A\n2       b1   B\n3       b3   B\n4       a1   A\n...\n\n\nWhen I use:\n\nuniqueId = df["Id"].unique() \n\n\nI get a list of unique IDs.\n\nHow can I however apply this filtering on the whole DataFrame such that it keeps the structure but that the duplicates (based on "Id") are removed?\n'
"I am iterating over a pandas dataframe using itertuples. I also want to capture the row number while iterating:\n\nfor row in df.itertuples():\n    print row['name']\n\n\nExpected output :\n\n1 larry\n2 barry\n3 michael\n\n\n1, 2, 3 are row numbers. I want to avoid using a counter and getting the row number. Is there an easy way to achieve this using pandas?\n"
"Why doesn't df.index.map(dict) work like df['column_name'].map(dict)?\n\nHere's a little example of trying to use index.map:\n\nimport pandas as pd\n\ndf = pd.DataFrame({'one': {'A': 10, 'B': 20, 'C': 30, 'D': 40, 'E': 50}})\nmap_dict = {'A': 'every', 'B': 'good', 'C': 'boy', 'D': 'does', 'E': 'fine'}\ndf\n'''\n    one\nA   10\nB   20\nC   30\nD   40\nE   50\n'''\n\ndf['two'] = df.index.map(mapper=map_dict)\n\n\nThis raises TypeError: 'dict' object is not callable\n\nFeeding it a lambda works:\n\ndf['two'] = df.index.map(mapper=(lambda x: map_dict[x])); df\n'''\n   one    two\nA   10  every\nB   20   good\nC   30    boy\nD   40   does\nE   50   fine\n'''\n\n\nHowever, resetting the index and mapping on a column works as expected without complaint:\n\ndf.reset_index(inplace=True)\ndf.rename(columns={'index': 'old_ndx'}, inplace=True) #so there's no index name confusion\ndf['two'] = df.old_ndx.map(map_dict); df\n\n'''\n  old_ndx  one    two\n0       A   10  every\n1       B   20   good\n2       C   30    boy\n3       D   40   does\n4       E   50   fine\n'''\n\n"
'I ended up figuring it out while writing out this question so I\'ll just post anyway and answer my own question in case someone else needs a little help.\n\nProblem\n\nSuppose we have a DataFrame, df, containing this data. \n\nimport pandas as pd\nfrom io import StringIO\n\ndata = StringIO(\n"""\\\ndate          spendings  category\n2014-03-25    10         A\n2014-04-05    20         A\n2014-04-15    10         A\n2014-04-25    10         B\n2014-05-05    10         B\n2014-05-15    10         A\n2014-05-25    10         A\n"""\n)\n\ndf = pd.read_csv(data,sep="\\s+",parse_dates=True,index_col="date")\n\n\nGoal\n\nFor each row, sum the spendings over every row that is within one month of it, ideally using DataFrame.rolling as it\'s a very clean syntax.\n\nWhat I have tried\n\ndf = df.rolling("M").sum()\n\n\nBut this throws an exception\n\nValueError: &lt;MonthEnd&gt; is a non-fixed frequency\n\n\nversion: pandas==0.19.2\n'
"I am trying to use drop_duplicates method on my dataframe, but I am getting an \nerror. See the following:\n\n\n  error: TypeError: unhashable type: 'list'\n\n\nThe code I am using:\n\ndf = db.drop_duplicates()\n\n\nMy DB is huge and contains strings, floats, dates, NaN's, booleans, integers... Any help is appreciated.\n"
"suppose I have DataFrame with columns ['X_Axis','col_2','col_3',...,'col_n',]\n\nI need to plot the first column on X-Axis and rest on Y-Axis.\nFYI : all the values have been grouped according to X-Axis, the X-Axis values range from 0-25 and all other column values have been normalized to the scale of 0 - 1. I want it on same graph plot, not subplots.\n\nPreferred : FactorPlot , normal line graph.\n"
"When trying to save plot image created with 'pandas.DataFrame.plot' from ' pandas.core.series.Series' object :\n\n%matplotlib inline\ntype(class_counts) # pandas.core.series.Series\nclass_counts.plot(kind='bar',  figsize=(20, 16), fontsize=26)\n\n\nLike this:\n\nimport matplotlib.pyplot as plt\nplt.savefig('figure_1.pdf', dpi=300)\n\n\nresults in empty pdf file. How to save image created with 'pandas.DataFrame.plot'? \n"
'I am trying to filter a pandas data frame using thresholds for three columns\n\nimport pandas as pd\ndf = pd.DataFrame({"A" : [6, 2, 10, -5, 3],\n                   "B" : [2, 5, 3, 2, 6],\n                   "C" : [-5, 2, 1, 8, 2]})\ndf = df.loc[(df.A &gt; 0) &amp; (df.B &gt; 2) &amp; (df.C &gt; -1)].reset_index(drop = True)\n\ndf\n    A  B  C\n0   2  5  2\n1  10  3  1\n2   3  6  2\n\n\nHowever, I want to do this inside a function where the names of the columns and their thresholds are given to me in a dictionary. Here\'s my first try that works ok. Essentially I am putting the filter inside cond variable and just run it: \n\ndf = pd.DataFrame({"A" : [6, 2, 10, -5, 3],\n                   "B" : [2, 5, 3, 2, 6],\n                   "C" : [-5, 2, 1, 8, 2]})\nlimits_dic = {"A" : 0, "B" : 2, "C" : -1}\ncond = "df = df.loc["\nfor key in limits_dic.keys():\n    cond += "(df." + key + " &gt; " + str(limits_dic[key])+ ") &amp; "\ncond = cond[:-2] + "].reset_index(drop = True)"\nexec(cond)\ndf\n    A  B  C\n0   2  5  2\n1  10  3  1\n2   3  6  2\n\n\nNow, finally I put everything inside a function and it stops working (perhaps exec function does not like to be used inside a function!):\n\ndf = pd.DataFrame({"A" : [6, 2, 10, -5, 3],\n                   "B" : [2, 5, 3, 2, 6],\n                   "C" : [-5, 2, 1, 8, 2]})\nlimits_dic = {"A" : 0, "B" : 2, "C" : -1}\ndef filtering(df, limits_dic):\n    cond = "df = df.loc["\n    for key in limits_dic.keys():\n        cond += "(df." + key + " &gt; " + str(limits_dic[key])+ ") &amp; "\n    cond = cond[:-2] + "].reset_index(drop = True)"\n    exec(cond)\n    return(df)\n\ndf = filtering(df, limits_dic)\ndf\n    A  B  C\n0   6  2 -5\n1   2  5  2\n2  10  3  1\n3  -5  2  8\n4   3  6  2\n\n\nI know that exec function acts differently when used inside a function but was not sure how to address the problem. Also, I am wondering there must be a more elegant way to define a function to do the filtering given two input: 1)df and 2)limits_dic = {"A" : 0, "B" : 2, "C" : -1}. I would appreciate any thoughts on this.\n'
"I have a csv file. I read it:\n\nimport pandas as pd\ndata = pd.read_csv('my_data.csv', sep=',')\ndata.head()\n\n\nIt has output like:\n\nid    city    department    sms    category\n01    khi      revenue      NaN       0\n02    lhr      revenue      good      1\n03    lhr      revenue      NaN       0\n\n\nI want to remove all the rows where sms column is empty/NaN. What is efficient way to do it?\n"
"I think this is a fairly basic question, but I can't seem to find the solution.\n\nI have a pandas dataframe similar to the following:\n\nimport pandas as pd\n\ndf = pd.DataFrame({'A' : ['x','x','y','z','z'],\n                   'B' : ['p','p','q','r','r']})\ndf\n\n\nwhich creates a table like this:\n\n    A   B\n0   x   p\n1   x   p\n2   y   q\n3   z   r\n4   z   r\n\n\nI'm trying to create a table that represents the number of distinct values in that dataframe. So my goal is something like this:\n\n    A   B   c\n0   x   p   2\n1   y   q   1\n2   z   r   2\n\n\nI can't find the correct functions to achieve this, though. I've tried:\n\ndf.groupby(['A','B']).agg('count')\n\n\nThis produces a table with 3 rows (as expected) but without a 'count' column. I don't know how to add in that count column. Could someone point me in the right direction?\n"
'I have a simple question related with csv files and parsing datetime.\n\nI have a csv file that look like this:\n\nYYYYMMDD, HH,    X\n20110101,  1,   10\n20110101,  2,   20\n20110101,  3,   30\n\n\nI would like to read it using pandas (read_csv) and have it in a dataframe indexed by the datetime. So far I\'ve tried to implement the following:\n\nimport pandas as pnd\npnd.read_csv("..\\\\file.csv",  parse_dates = True, index_col = [0,1])\n\n\nand the result I get is:\n\n                         X\nYYYYMMDD    HH            \n2011-01-01 2012-07-01   10\n           2012-07-02   20\n           2012-07-03   30\n\n\nAs you see the parse_dates in converting the HH into a different date.\n\nIs there a simple and efficient way to combine properly the column "YYYYMMDD" with the column "HH" in order to have something like this? :\n\n                      X\nDatetime              \n2011-01-01 01:00:00  10\n2011-01-01 02:00:00  20\n2011-01-01 03:00:00  30\n\n\nThanks in advance for the help.\n'
"As part of a unit test, I need to test two DataFrames for equality.  The order of the columns in the DataFrames is not important to me.  However, it seems to matter to Pandas:\n\nimport pandas\ndf1 = pandas.DataFrame(index = [1,2,3,4])\ndf2 = pandas.DataFrame(index = [1,2,3,4])\ndf1['A'] = [1,2,3,4]\ndf1['B'] = [2,3,4,5]\ndf2['B'] = [2,3,4,5]\ndf2['A'] = [1,2,3,4]\ndf1 == df2\n\n\nResults in:\n\nException: Can only compare identically-labeled DataFrame objects\n\n\nI believe the expression df1 == df2 should evaluate to a DataFrame containing all True values.  Obviously it's debatable what the correct functionality of == should be in this context.  My question is: Is there a Pandas method that does what I want?  That is, is there a way to do equality comparison that ignores column order?\n"
"I have a similar problem to the one posted here: \n\nPandas DataFrame: remove unwanted parts from strings in a column\n\nI need to remove newline characters from within a string in a DataFrame. Basically, I've accessed an api using python's json module and that's all ok. Creating the DataFrame works amazingly, too. However, when I want to finally output the end result into a csv, I get a bit stuck, because there are newlines that are creating false 'new rows' in the csv file.\n\nSo basically I'm trying to turn this: \n\n'...this is a paragraph.\n\nAnd this is another paragraph...'\n\ninto this:\n\n'...this is a paragraph. And this is another paragraph...'\n\nI don't care about preserving any kind of '\\n' or any special symbols for the paragraph break. So it can be stripped right out.\n\nI've tried a few variations:\n\nmisc['product_desc'] = misc['product_desc'].strip('\\n')\n\nAttributeError: 'Series' object has no attribute 'strip'\n\n\nhere's another\n\nmisc['product_desc'] = misc['product_desc'].str.strip('\\n')\n\nTypeError: wrapper() takes exactly 1 argument (2 given)\n\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n'))\nmisc['product_desc'] = misc['product_desc'].map(lambda x: x.strip('\\n\\t'))\n\n\nThere is no error message, but the newline characters don't go away, either. Same thing with this:\n\nmisc = misc.replace('\\n', '')\n\n\nThe write to csv line is this:\n\nmisc_id.to_csv('C:\\Users\\jlalonde\\Desktop\\misc_w_id.csv', sep=' ', na_rep='', index=False, encoding='utf-8')\n\n\nVersion of Pandas is 0.9.1\n\nThanks! :)\n"
'I have written a function to convert pandas datetime dates to month-end:\n\nimport pandas\nimport numpy\nimport datetime\nfrom pandas.tseries.offsets import Day, MonthEnd\n\ndef get_month_end(d):\n    month_end = d - Day() + MonthEnd() \n    if month_end.month == d.month:\n        return month_end # 31/March + MonthEnd() returns 30/April\n    else:\n        print "Something went wrong while converting dates to EOM: " + d + " was converted to " + month_end\n        raise\n\n\nThis function seems to be quite slow, and I was wondering if there is any faster alternative? The reason I noticed it\'s slow is that I am running this on a dataframe column with 50\'000 dates, and I can see that the code is much slower since introducing that function (before I was converting dates to end-of-month).\n\ndf = pandas.read_csv(inpath, na_values = nas, converters = {open_date: read_as_date})\ndf[open_date] = df[open_date].apply(get_month_end)\n\n\nI am not sure if that\'s relevant, but I am reading the dates in as follows:\n\ndef read_as_date(x):\n    return datetime.datetime.strptime(x, fmt)\n\n'
'Below is my dataframe.  I made some transformations to create the category column and dropped the original column it was derived from.  Now I need to do a group-by to remove the dups e.g. Love and Fashion can be rolled up via a groupby sum.\n\ndf.colunms = array([category, clicks, revenue, date, impressions, size], dtype=object)\ndf.values=\n[[Love 0 0.36823 2013-11-04 380 300x250]\n [Love 183 474.81522 2013-11-04 374242 300x250]\n [Fashion 0 0.19434 2013-11-04 197 300x250]\n [Fashion 9 18.26422 2013-11-04 13363 300x250]]\n\n\nHere is the index that is created when I created the dataframe\n\nprint df.index\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48])\n\n\nI assume I want to drop the index, and create date, and category as a multiindex then do a groupby sum of the metrics.  How do I do this in pandas dataframe?\n\ndf.head(15).to_dict()= {\'category\': {0: \'Love\', 1: \'Love\', 2: \'Fashion\', 3: \'Fashion\', 4: \'Hair\', 5: \'Movies\', 6: \'Movies\', 7: \'Health\', 8: \'Health\', 9: \'Celebs\', 10: \'Celebs\', 11: \'Travel\', 12: \'Weightloss\', 13: \'Diet\', 14: \'Bags\'}, \'impressions\': {0: 380, 1: 374242, 2: 197, 3: 13363, 4: 4, 5: 189, 6: 60632, 7: 269, 8: 40189, 9: 138, 10: 66590, 11: 2227, 12: 22668, 13: 21707, 14: 229}, \'date\': {0: \'2013-11-04\', 1: \'2013-11-04\', 2: \'2013-11-04\', 3: \'2013-11-04\', 4: \'2013-11-04\', 5: \'2013-11-04\', 6: \'2013-11-04\', 7: \'2013-11-04\', 8: \'2013-11-04\', 9: \'2013-11-04\', 10: \'2013-11-04\', 11: \'2013-11-04\', 12: \'2013-11-04\', 13: \'2013-11-04\', 14: \'2013-11-04\'}, \'cpc_cpm_revenue\': {0: 0.36823, 1: 474.81522000000001, 2: 0.19434000000000001, 3: 18.264220000000002, 4: 0.00080000000000000004, 5: 0.23613000000000001, 6: 81.391139999999993, 7: 0.27171000000000001, 8: 51.258200000000002, 9: 0.11536, 10: 83.966859999999997, 11: 3.43248, 12: 31.695889999999999, 13: 28.459320000000002, 14: 0.43524000000000002}, \'clicks\': {0: 0, 1: 183, 2: 0, 3: 9, 4: 0, 5: 1, 6: 20, 7: 0, 8: 21, 9: 0, 10: 32, 11: 1, 12: 12, 13: 9, 14: 2}, \'size\': {0: \'300x250\', 1: \'300x250\', 2: \'300x250\', 3: \'300x250\', 4: \'300x250\', 5: \'300x250\', 6: \'300x250\', 7: \'300x250\', 8: \'300x250\', 9: \'300x250\', 10: \'300x250\', 11: \'300x250\', 12: \'300x250\', 13: \'300x250\', 14: \'300x250\'}}\n\n\nPython is 2.7 and pandas is 0.7.0 on ubuntu 12.04.  Below is the error I get if I run the below\n\nimport pandas\nprint pandas.__version__\ndf = pandas.DataFrame.from_dict(\n    {\n     \'category\': {0: \'Love\', 1: \'Love\', 2: \'Fashion\', 3: \'Fashion\', 4: \'Hair\', 5: \'Movies\', 6: \'Movies\', 7: \'Health\', 8: \'Health\', 9: \'Celebs\', 10: \'Celebs\', 11: \'Travel\', 12: \'Weightloss\', 13: \'Diet\', 14: \'Bags\'}, \n     \'impressions\': {0: 380, 1: 374242, 2: 197, 3: 13363, 4: 4, 5: 189, 6: 60632, 7: 269, 8: 40189, 9: 138, 10: 66590, 11: 2227, 12: 22668, 13: 21707, 14: 229}, \n     \'date\': {0: \'2013-11-04\', 1: \'2013-11-04\', 2: \'2013-11-04\', 3: \'2013-11-04\', 4: \'2013-11-04\', 5: \'2013-11-04\', 6: \'2013-11-04\', 7: \'2013-11-04\', 8: \'2013-11-04\', 9: \'2013-11-04\', 10: \'2013-11-04\', 11: \'2013-11-04\', 12: \'2013-11-04\', 13: \'2013-11-04\', 14: \'2013-11-04\'}, \'cpc_cpm_revenue\': {0: 0.36823, 1: 474.81522000000001, 2: 0.19434000000000001, 3: 18.264220000000002, 4: 0.00080000000000000004, 5: 0.23613000000000001, 6: 81.391139999999993, 7: 0.27171000000000001, 8: 51.258200000000002, 9: 0.11536, 10: 83.966859999999997, 11: 3.43248, 12: 31.695889999999999, 13: 28.459320000000002, 14: 0.43524000000000002}, \'clicks\': {0: 0, 1: 183, 2: 0, 3: 9, 4: 0, 5: 1, 6: 20, 7: 0, 8: 21, 9: 0, 10: 32, 11: 1, 12: 12, 13: 9, 14: 2}, \'size\': {0: \'300x250\', 1: \'300x250\', 2: \'300x250\', 3: \'300x250\', 4: \'300x250\', 5: \'300x250\', 6: \'300x250\', 7: \'300x250\', 8: \'300x250\', 9: \'300x250\', 10: \'300x250\', 11: \'300x250\', 12: \'300x250\', 13: \'300x250\', 14: \'300x250\'}\n    }\n)\ndf.set_index([\'date\', \'category\'], inplace=True)\ndf.groupby(level=[0,1]).sum()\n\n\nTraceback (most recent call last):\n  File "/home/ubuntu/workspace/devops/reports/groupby_sub.py", line 9, in &lt;module&gt;\n    df.set_index([\'date\', \'category\'], inplace=True)\n  File "/usr/lib/pymodules/python2.7/pandas/core/frame.py", line 1927, in set_index\n    raise Exception(\'Index has duplicate keys: %s\' % duplicates)\nException: Index has duplicate keys: [(\'2013-11-04\', \'Celebs\'), (\'2013-11-04\', \'Fashion\'), (\'2013-11-04\', \'Health\'), (\'2013-11-04\', \'Love\'), (\'2013-11-04\', \'Movies\')]\n\n'
"Currently there is a median method on the Pandas's GroupBy objects.\n\nIs there is a way to calculate an arbitrary percentile (see: http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html) on the groupings? \n\nMedian would be the calcuation of percentile with q=50.\n"
'I have a csv file that has a few hundred rows and 26 columns, but the last few columns only have a value in a few rows and they are towards the middle or end of the file. When I try to read it in using read_csv() I get the following error. \n"ValueError: Expecting 23 columns, got 26 in row 64"\n\nI can\'t see where to explicitly state the number of columns in the file, or how it determines how many columns it thinks the file should have. \nThe dump is below\n\nIn [3]:\n\ninfile =open(easygui.fileopenbox(),"r")\npledge = read_csv(infile,parse_dates=\'true\')\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-3-b35e7a16b389&gt; in &lt;module&gt;()\n      1 infile =open(easygui.fileopenbox(),"r")\n      2 \n----&gt; 3 pledge = read_csv(infile,parse_dates=\'true\')\n\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in read_csv(filepath_or_buffer, sep, dialect, header, index_col, names, skiprows, na_values, thousands, comment, parse_dates, keep_date_col, dayfirst, date_parser, nrows, iterator, chunksize, skip_footer, converters, verbose, delimiter, encoding, squeeze)\n    234         kwds[\'delimiter\'] = sep\n    235 \n--&gt; 236     return _read(TextParser, filepath_or_buffer, kwds)\n    237 \n    238 @Appender(_read_table_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in _read(cls, filepath_or_buffer, kwds)\n    189         return parser\n    190 \n--&gt; 191     return parser.get_chunk()\n    192 \n    193 @Appender(_read_csv_doc)\n\nC:\\Python27\\lib\\site-packages\\pandas-0.8.1-py2.7-win32.egg\\pandas\\io\\parsers.pyc in get_chunk(self, rows)\n    779             msg = (\'Expecting %d columns, got %d in row %d\' %\n    780                    (col_len, zip_len, row_num))\n--&gt; 781             raise ValueError(msg)\n    782 \n    783         data = dict((k, v) for k, v in izip(self.columns, zipped_content))\n\nValueError: Expecting 23 columns, got 26 in row 64\n\n'
'I would like to read several excel files from a directory into pandas and concatenate them into one big dataframe. I have not been able to figure it out though. I need some help with the for loop and building a concatenated dataframe:\nHere is what I have so far: \n\nimport sys\nimport csv\nimport glob\nimport pandas as pd\n\n# get data file names\npath =r\'C:\\DRO\\DCL_rawdata_files\\excelfiles\'\nfilenames = glob.glob(path + "/*.xlsx")\n\ndfs = []\n\nfor df in dfs: \n    xl_file = pd.ExcelFile(filenames)\n    df=xl_file.parse(\'Sheet1\')\n    dfs.concat(df, ignore_index=True)\n\n'
'When I look at the plotting style in the Pandas documentation, the plots look different from the default one. It seems to mimic the ggplot "look and feel".\n\nSame thing with the seaborn\'s package.\n\nHow can I load that style? (even if I am not using a notebook?)\n'
"I am trying to read data from a csv file into a pandas dataframe, and access the first column 'Date'\n\nimport pandas as pd\ndf_ticks=pd.read_csv('values.csv', delimiter=',')\nprint(df_ticks.columns)\ndf_ticks['Date']\n\n\nproduces the following result\n\nIndex([u'Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\nKeyError: u'no item named Date'\n\n\nIf I try to acces any other column like 'Open' or 'Volume' it is working as expected\n"
"The following code can't parse my date column into dates from csv file.\n\ndata=pd.read_csv('c:/data.csv',parse_dates=True,keep_date_col = True) \n\n\nor \n\ndata=pd.read_csv('c:/data.csv',parse_dates=[0]) \n\n\ndata is like following\n\ndate          value \n30MAR1990    140000 \n30JUN1990    30000  \n30SEP1990    120000  \n30DEC1990    34555\n\n\nWhat did I do wrong? Please help!\n\nThanks.\n"
"I have a Pandas DataFrame as below:\n\n   a      b      c      d\n0  Apple  3      5      7\n1  Banana 4      4      8\n2  Cherry 7      1      3\n3  Apple  3      4      7\n\n\nI would like to group the rows by column 'a' while replacing values in column 'c' by the mean of values in grouped rows and add another column with std deviation of the values in column 'c' whose mean has been calculated. The values in column 'b' or 'd' are constant for all rows being grouped. So, the desired output would be:\n\n   a      b      c      d      e\n0  Apple  3      4.5    7      0.707107\n1  Banana 4      4      8      0\n2  Cherry 7      1      3      0\n\n\nWhat is the best way to achieve this?\n"
"I'm not sure where I am astray but I cannot seem to reset the index on a dataframe.\n\nWhen I run test.head(), I get the output below:\n\n\n\nAs you can see, the dataframe is a slice, so the index is out of bounds.\nWhat I'd like to do is to reset the index for this dataframe. So I run test.reset_index(drop=True). This outputs the following:\n\n\n\nThat looks like a new index, but it's not. Running test.head again, the index is still the same. Attempting to use lambda.apply or iterrows() creates problems with the dataframe.\n\nHow can I really reset the index?\n"
"Let's suppose I have following Time Series:\n\nTimestamp              Category\n2014-10-16 15:05:17    Facebook\n2014-10-16 14:56:37    Vimeo\n2014-10-16 14:25:16    Facebook\n2014-10-16 14:15:32    Facebook\n2014-10-16 13:41:01    Facebook\n2014-10-16 12:50:30    Orkut\n2014-10-16 12:28:54    Facebook\n2014-10-16 12:26:56    Facebook\n2014-10-16 12:25:12    Facebook\n...\n2014-10-08 15:52:49    Youtube\n2014-10-08 15:04:50    Youtube\n2014-10-08 15:03:48    Vimeo\n2014-10-08 15:02:27    Youtube\n2014-10-08 15:01:56    DailyMotion\n2014-10-08 13:27:28    Facebook\n2014-10-08 13:01:08    Vimeo\n2014-10-08 12:52:06    Facebook\n2014-10-08 12:43:27    Facebook\nName: summary, Length: 600\n\n\nI would like to make a count of each category (Unique Value/Factor in the Time Series) per week and year.\n\nExample:\n\n    Week/Year      Category      Count\n    1/2014         Facebook      12\n    1/2014         Google        5\n    1/2014         Youtube       2\n...    \n    2/2014         Facebook      2\n    2/2014         Google        5\n    2/2014         Youtube       20\n...\n\n\nHow can this be achieved using Python pandas?\n"
"I have seen a lot of posts about how you can do it with a date string but I am trying something for a dataframe column and haven't got any luck so far.\nMy current method is : Get the weekday from 'myday' and then offset to get monday.\n\ndf['myday'] is column of dates. \nmydays = pd.DatetimeIndex(df['myday']).weekday\ndf['week_start'] = pd.DatetimeIndex(df['myday']) - pd.DateOffset(days=mydays)\n\n\nBut I get \nTypeError: unsupported type for timedelta days component: numpy.ndarray\n\nHow can I get week start date from a df column?\n"
"Assuming I have a dataframe similar to the below, how would I get the correlation between 2 specific columns and then group by the 'ID' column?  I believe the Pandas 'corr' method finds the correlation between all columns.  If possible I would also like to know how I could find the 'groupby' correlation using the .agg function (i.e. np.correlate).\n\nWhat I have:\n\nID  Val1    Val2    OtherData   OtherData\nA   5       4       x           x\nA   4       5       x           x\nA   6       6       x           x\nB   4       1       x           x\nB   8       2       x           x\nB   7       9       x           x\nC   4       8       x           x\nC   5       5       x           x\nC   2       1       x           x\n\n\nWhat I need:\n\nID  Correlation_Val1_Val2\nA   0.12\nB   0.22\nC   0.05\n\n\nThanks!\n"
"I have a list, with each entry being a company name\n\ncompanies = ['AA', 'AAPL', 'BA', ....., 'YHOO']\n\n\nI want to create a new dataframe for each entry in the list.\n\nSomething like\n\n(pseudocode)\n\nfor c in companies:\n     c = pd.DataFrame()\n\n\nI have searched for a way to do this but can't find it. Any ideas? \n"
"How can I remove values from a column in pandas.DataFrame, that occurs rarely, i.e. with a low frequency? Example:\n\nIn [4]: df[col_1].value_counts()\n\nOut[4]: 0       189096\n        1       110500\n        2        77218\n        3        61372\n              ...\n        2065         1\n        2067         1\n        1569         1\n        dtype: int64\n\n\nSo, my question is: how to remove values like 2065, 2067, 1569 and others? And how can I do this for ALL columns, that contain .value_counts() like this?\n\nUPDATE: About 'low' I mean values like 2065. This value occurs in col_1 1 (one) times and I want to remove values like this.\n"
'I want to calculate the running sum in a given column(without using loops, of course). The caveat is that I have this other column that specifies when to reset the running sum to the value present in that row. Best explained by the following example:\n\n   reset  val   desired_col\n0      0    1   1\n1      0    5   6\n2      0    4   10\n3      1    2   2\n4      1   -1   -1\n5      0    6   5\n6      0    4   9\n7      1    2   2\n\n\ndesired_col is the value I want to be calculated.\n'
"I am working on a program that involves large amounts of data. I am using the python pandas module to look for errors in my data. This usually works very fast. However this current piece of code I wrote seems to be way slower than it should be and I am looking for a way to speed it up.\n\nIn order for you guys to properly test it I uploaded a rather large piece of code. You should be able to run it as is. The comments in the code should explain what I am trying to do here. Any help would be greatly appreciated.\n\n# -*- coding: utf-8 -*-\n\nimport pandas as pd\nimport numpy as np\n\n# Filling dataframe with data\n# Just ignore this part for now, real data comes from csv files, this is an example of how it looks\nTimeOfDay_options = ['Day','Evening','Night']\nTypeOfCargo_options = ['Goods','Passengers']\nnp.random.seed(1234)\nn = 10000\n\ndf = pd.DataFrame()\ndf['ID_number'] = np.random.randint(3, size=n)\ndf['TimeOfDay'] = np.random.choice(TimeOfDay_options, size=n)\ndf['TypeOfCargo'] = np.random.choice(TypeOfCargo_options, size=n)\ndf['TrackStart'] = np.random.randint(400, size=n) * 900\ndf['SectionStart'] = np.nan\ndf['SectionStop'] = np.nan\n\ngrouped_df = df.groupby(['ID_number','TimeOfDay','TypeOfCargo','TrackStart'])\nfor index, group in grouped_df:\n    if len(group) == 1:\n        df.loc[group.index,['SectionStart']] = group['TrackStart']\n        df.loc[group.index,['SectionStop']] = group['TrackStart'] + 899\n\n    if len(group) &gt; 1:\n        track_start = group.loc[group.index[0],'TrackStart']\n        track_end = track_start + 899\n        section_stops = np.random.randint(track_start, track_end, size=len(group))\n        section_stops[-1] = track_end\n        section_stops = np.sort(section_stops)\n        section_starts = np.insert(section_stops, 0, track_start)\n\n        for i,start,stop in zip(group.index,section_starts,section_stops):\n            df.loc[i,['SectionStart']] = start\n            df.loc[i,['SectionStop']] = stop\n\n#%% This is what a random group looks like without errors\n#Note that each section neatly starts where the previous section ended\n#There are no gaps (The whole track is defined)\ngrouped_df.get_group((2, 'Night', 'Passengers', 323100))\n\n#%% Introducing errors to the data\ndf.loc[2640,'SectionStart'] += 100\ndf.loc[5390,'SectionStart'] += 7\n\n#%% This is what the same group looks like after introducing errors \n#Note that the 'SectionStop' of row 1525 is no longer similar to the 'SectionStart' of row 2640\n#This track now has a gap of 100, it is not completely defined from start to end\ngrouped_df.get_group((2, 'Night', 'Passengers', 323100))\n\n#%% Try to locate the errors\n#This is the part of the code I need to speed up\n\ndef Full_coverage(group):\n    if len(group) &gt; 1:\n        #Sort the grouped data by column 'SectionStart' from low to high\n\n        #Updated for newer pandas version\n        #group.sort('SectionStart', ascending=True, inplace=True)\n        group.sort_values('SectionStart', ascending=True, inplace=True)\n\n        #Some initial values, overwritten at the end of each loop  \n        #These variables correspond to the first row of the group\n        start_km = group.iloc[0,4]\n        end_km = group.iloc[0,5]\n        end_km_index = group.index[0]\n\n        #Loop through all the rows in the group\n        #index is the index of the row\n        #i is the 'SectionStart' of the row\n        #j is the 'SectionStop' of the row\n        #The loop starts from the 2nd row in the group\n        for index, (i, j) in group.iloc[1:,[4,5]].iterrows():\n\n            #The start of the next row must be equal to the end of the previous row in the group\n            if i != end_km: \n\n                #Add the faulty data to the error list\n                incomplete_coverage.append(('Expected startpoint: '+str(end_km)+' (row '+str(end_km_index)+')', \\\n                                    'Found startpoint: '+str(i)+' (row '+str(index)+')'))                \n\n            #Overwrite these values for the next loop\n            start_km = i\n            end_km = j\n            end_km_index = index\n\n    return group\n\n#Check if the complete track is completely defined (from start to end) for each combination of:\n    #'ID_number','TimeOfDay','TypeOfCargo','TrackStart'\nincomplete_coverage = [] #Create empty list for storing the error messages\ndf_grouped = df.groupby(['ID_number','TimeOfDay','TypeOfCargo','TrackStart']).apply(lambda x: Full_coverage(x))\n\n#Print the error list\nprint('\\nFound incomplete coverage in the following rows:')\nfor i,j in incomplete_coverage:\n    print(i)\n    print(j)\n    print() \n\n#%%Time the procedure -- It is very slow, taking about 6.6 seconds on my pc\n%timeit df.groupby(['ID_number','TimeOfDay','TypeOfCargo','TrackStart']).apply(lambda x: Full_coverage(x))\n\n"
"I have the following dataframe:\n\n       actual_credit    min_required_credit\n   0   0.3              0.4\n   1   0.5              0.2\n   2   0.4              0.4\n   3   0.2              0.3\n\n\nI need to add a column indicating where actual_credit >= min_required_credit. The result would be:\n\n       actual_credit    min_required_credit   result\n   0   0.3              0.4                   False\n   1   0.5              0.2                   True\n   2   0.4              0.4                   True\n   3   0.1              0.3                   False\n\n\nI am doing the following:\n\ndf['result'] = abs(df['actual_credit']) &gt;= abs(df['min_required_credit'])\n\n\nHowever the 3rd row (0.4 and 0.4) is constantly resulting in False. After researching this issue at various places including: What is the best way to compare floats for almost-equality in Python? I still can't get this to work. Whenever the two columns have an identical value, the result is False which is not correct.\n\nI am using python 3.3\n"
"I have a data frame with some columns with empty lists and others with lists of strings:\n\n       donation_orgs                              donation_context\n0            []                                           []\n1   [the research of Dr. ...]   [In lieu of flowers , memorial donations ...]\n\n\nI'm trying to return a data set without any of the rows where there are empty lists.\n\nI've tried just checking for null values:\n\ndfnotnull = df[df.donation_orgs != []]\ndfnotnull\n\n\nand\n\ndfnotnull = df[df.notnull().any(axis=1)]\npd.options.display.max_rows=500\ndfnotnull\n\n\nAnd I've tried looping through and checking for values that exist, but I think the lists aren't returning Null or None like I thought they would:\n\ndfnotnull = pd.DataFrame(columns=('donation_orgs', 'donation_context'))\nfor i in range(0,len(df)):\n    if df['donation_orgs'].iloc(i):\n        dfnotnull.loc[i] = df.iloc[i]\n\n\nAll three of the above methods simply return every row in the original data frame.=\n"
'Is there an analog for reduce for a pandas Series?\n\nFor example, the analog for map is pd.Series.apply, but I can\'t find any analog for reduce. \n\n\n\nMy application is, I have a pandas Series of lists:\n\n&gt;&gt;&gt; business["categories"].head()\n\n0                      [\'Doctors\', \'Health &amp; Medical\']\n1                                        [\'Nightlife\']\n2                 [\'Active Life\', \'Mini Golf\', \'Golf\']\n3    [\'Shopping\', \'Home Services\', \'Internet Servic...\n4    [\'Bars\', \'American (New)\', \'Nightlife\', \'Loung...\nName: categories, dtype: object\n\n\nI\'d like to merge the Series of lists together using reduce, like so:\n\ncategories = reduce(lambda l1, l2: l1 + l2, categories)\n\n\nbut this takes a horrific time because merging two lists together is O(n) time in Python. I\'m hoping that pd.Series has a vectorized way to perform this faster.\n'
"I have read the docs of DataFrame.apply\n\n\n  DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, args=(), **kwds)¶\n  Applies function along input axis of DataFrame.\n\n\nSo, How can I apply a function to a specific column?\n\nIn [1]: import pandas as pd\nIn [2]: data = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}\nIn [3]: df = pd.DataFrame(data)\nIn [4]: df\nOut[4]: \n   A  B  C\n0  1  4  7\n1  2  5  8\n2  3  6  9\nIn [5]: def addOne(v):\n...:        v += 1\n...:        return v\n...: \nIn [6]: df.apply(addOne, axis=1)\nOut[6]: \n   A  B   C\n0  2  5   8\n1  3  6   9\n2  4  7  10\n\n\nI want to addOne to every value in df['A'], not all columns. How can I do that with DataFrame.apply.\n\nThanks for help!  \n"
'I have a pandas data frame, df, which looks like this:\n\nCut-off             &lt;=35   &gt;35                   \nCalcium              0.0   1.0\nCopper               1.0   0.0\nHelium               0.0   8.0\nHydrogen             0.0   1.0\n\n\nHow can I remove the decimal point so that the data frame looks like this:\n\nCut-off             &lt;= 35  &gt; 35                   \nCalcium              0     1\nCopper               1     0\nHelium               0     8\nHydrogen             0     1\n\n\nI have tried df.round(0) without success.\n'
"I have a pandas DataFrame with a column of integers. I want the rows containing numbers greater than 10. I am able to evaluate True or False but not the actual value, by doing:\n\ndf['ints'] = df['ints'] &gt; 10\n\n\nI don't use Python very often so I'm going round in circles with this. \n\nI've spent 20 minutes Googling but haven't been able to find what I need....\n\nEdit:\n\n    observationID   recordKey   gridReference   siteKey siteName    featureKey  startDate   endDate pTaxonVersionKey    taxonName   authority   commonName  ints\n0   463166539   1767    SM90    NaN NaN 150161  12/02/2006  12/02/2006  NBNSYS0100004720    Pipistrellus pygmaeus   (Leach, 1825)   Soprano Pipistrelle 2006\n1   463166623   4325    TL65    NaN NaN 168651  21/12/2008  21/12/2008  NHMSYS0020001355    Pipistrellus pipistrellus sensu stricto (Schreber, 1774)    Common Pipistrelle  2008\n2   463166624   4326    TL65    NaN NaN 168651  18/01/2009  18/01/2009  NHMSYS0020001355    Pipistrellus pipistrellus sensu stricto (Schreber, 1774)    Common Pipistrelle  2009\n3   463166625   4327    TL65    NaN NaN 168651  15/02/2009  15/02/2009  NHMSYS0020001355    Pipistrellus pipistrellus sensu stricto (Schreber, 1774)    Common Pipistrelle  2009\n4   463166626   4328    TL65    NaN NaN 168651  19/12/2009  19/12/2009  NHMSYS0020001355    Pipistrellus pipistrellus sensu stricto (Schreber, 1774)    Common Pipistrelle  2009\n\n"
'I have a pandas dataframe like this: \n\n    \'\'     count\nsugar      420\nmilk       108\nvanilla    450\n...\n\n\nThe first column has no header and I would like to give it the name: \'ingredient\'. \n\nI created the dataframe from a csv file: \n\ndf = pd.read_csv(\'./data/file_name.csv\', index_col=False, encoding="ISO-8859-1")  \ndf = df[\'ingredient_group\']  #selecting column \ndf = df.value_counts()       #calculating string occurance which return series obj\ndf = pd.DataFrame(df)        #creating dataframe from series obj\n\n\nHow do I assign the name \'ingredient\' to the first column which has currently no name? \n\nI already tried: \n\ndf_count.rename(columns={\'\': \'ingredient\'}, inplace=True)\n\ndf = pd.DataFrame(df, columns = [\'ingredient\',\'count\']\n\n\nHow do I prevent this from happening? \n\n\'\'        count\ningredient  \'\'\nsugar      420\nmilk       108\nvanilla    450\n...\n\n'
'We can use the following to iterate rows of a data frame.\n\nfor index, row in df.iterrows():\n\n\nWhat if I want to begin from a different row index?  (not from first row)?\n'
"I have the following data frame:\n\ndata = pd.DataFrame({'user_id' : ['a1', 'a1', 'a1', 'a2','a2','a2','a3','a3','a3'], 'product_id' : ['p1','p1','p2','p1','p1','p1','p2','p2','p3']})\n\nproduct_id  user_id\n    p1       a1\n    p1       a1\n    p2       a1\n    p1       a2\n    p1       a2\n    p1       a2\n    p2       a3\n    p2       a3\n    p3       a3\n\n\nin real case there might be some other columns as well, but what i need to do is to group by data frame by product_id and user_id columns and count number of each combination and add it as a new column in a new dat frame\n\noutput should be something like this:\n\nuser_id product_id  count\na1       p1            2\na1       p2            1\na2       p1            3\na3       p2            2\na3       p3            1\n\n\nI have tried the following code:\n\ngrouped=data.groupby(['user_id','product_id']).count()\n\n\nbut the result is:\n\nuser_id product_id\n a1       p1\n          p2\n a2       p1\n a3       p2\n          p3\n\n\nactually the most important thing for me is to have a column names count that has the number of occurrences , i need to use the column later.\n"
"I have a list of 18 data frames:\n\ndfList = [df1, df2, df3, df4, df5, df6.....df18]\n\n\nAll of the data frames have a common id column so it's easy to join them each together with pd.merge 2 at a time. Is there a way to join them all at once so that dfList comes back as a single dataframe?\n"
'This is an extension to this question, where OP wanted to know how to drop rows where the values in a single column are NaN.\n\nI\'m wondering how I can drop rows where the values in 2 (or more) columns are both NaN. Using the second answer\'s created Data Frame:\n\nIn [1]: df = pd.DataFrame(np.random.randn(10,3))\n\nIn [2]: df.ix[::2,0] = np.nan; df.ix[::4,1] = np.nan; df.ix[::3,2] = np.nan;\n\nIn [3]: df\nOut[3]:\n          0         1         2\n0       NaN       NaN       NaN\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n\n\nIf I use the drop.na() command, specifically the drop.na(subset=[1,2]), then it completes an "or" type drop and leaves:\n\nIn[4]: df.dropna(subset=[1,2])\nOut[4]: \n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n5 -1.250970  0.030561 -2.678622\n7  0.049896 -0.308003  0.823295\n\n\nWhat I want is an "and" type drop, where it drops rows where there is an NaN in column index 1 and 2. This would leave:\n\n          0         1         2\n1  2.677677 -1.466923 -0.750366\n2       NaN  0.798002 -0.906038\n3  0.672201  0.964789       NaN\n4       NaN       NaN  0.050742\n5 -1.250970  0.030561 -2.678622\n6       NaN  1.036043       NaN\n7  0.049896 -0.308003  0.823295\n8       NaN       NaN  0.637482\n9 -0.310130  0.078891       NaN\n\n\nwhere only the first row is dropped.\n\nAny ideas?\n\nEDIT: changed data frame values for consistency\n'
'I have monthly data.  I want to convert it to "periods" of 3 months where q1 starts in January.  So in the example below, the first 3 month aggregation would translate into start of q2 (desired format: 1996q2). And the data value that results from mushing together 3 monthly values is a mean (average) of 3 columns.   Conceptually, not complicated.  Does anyone know how to do it in one swoop? Potentially, I could do a lot of hard work through looping and just hardcode the hell out of it, but I am new to pandas and looking for something more clever than brute force.\n\n\n1996-04   1996-05 1996-06  1996-07 .....\n25          19       37      40\n\n\nSo I am looking for:\n\n\n1996q2  1996q3   1996q4  1997q1  1997q2 .....\n avg      avg      avg     ...     ...\n\n'
"Working with PANDAS to try and summarise a dataframe as a count of certain categories, as well as the means sentiment score for these categories.\n\nThere is table full of strings which have different sentiment scores, and I want to group each text source by saying how many posts they have, as well as the average sentiment of these posts.\n\nMy (simplified) dataframe looks like this:\n\nsource    text              sent\n--------------------------------\nbar       some string       0.13\nfoo       alt string        -0.8\nbar       another str       0.7\nfoo       some text         -0.2\nfoo       more text         -0.5\n\n\nThe output from this should be something like this:\n\nsource    count     mean_sent\n-----------------------------\nfoo       3         -0.5\nbar       2         0.415\n\n\nThe answer is somewhere along the lines of:\n\ndf['sent'].groupby(df['source']).mean()\n\n\nYet only gives each source and it's mean, with no column headers.\n\nThanks in advance!\n"
"I'm trying to get the index of 6th item in a Series I have.\nThis is how the head looks like:\nUnited States    1.536434e+13\nChina            6.348609e+12\nJapan            5.542208e+12\nGermany          3.493025e+12\nFrance           2.681725e+12\n\nFor getting the 6th index name (6th Country after being sorted), I usually use s.head(6) and get the 6th index from there.\ns.head(6) gives me:\nUnited States     1.536434e+13\nChina             6.348609e+12\nJapan             5.542208e+12\nGermany           3.493025e+12\nFrance            2.681725e+12\nUnited Kingdom    2.487907e+12\n\nand looking at this, I'm getting the index as United Kingdom.\nSo, is there any better way for getting the index other than this? And also, for a dataframe, is there any function to get the 6th index on basis of a respective column after sorting.\nIf it's a dataframe, I usually, sort, create a new column named index, and use reset_index, and then use iloc attribute to get the 6th (since it will be using a range in the index after reset).\nIs there any better way to do this with pd.Series and pd.DataFrame?\n"
"New to Pandas, so maybe I'm missing a big idea? \nI have a Pandas DataFrame of register transactions with shape like (500,4):\n\nTime              datetime64[ns]\nNet Total                float64\nTax                      float64\nTotal Due                float64\n\n\nI'm working through my code in a Python3 Jupyter notebook. I can't get past sorting any column. Working through the different code examples for sort, I'm not seeing the output reorder when I inspect the df. So, I've reduced the problem to trying to order just one column:\n\ndf.sort_values(by='Time')\n# OR\ndf.sort_values(['Total Due'])\n# OR\ndf.sort_values(['Time'], ascending=True)\n\n\nNo matter which column title, or which boolean argument I use, the displayed results never change order. \n\nThinking it could be a Jupyter thing, I've previewed the results using print(df), df.head(), and HTML(df.to_html()) (the last example is for Jupyter notebooks). I've also rerun the whole notebook from import CSV to this code. And, I'm also new to Python3 (from 2.7), so I get stuck with that sometimes, but I don't see how that's relevant in this case.\n\nAnother post has a similar problem, Python pandas dataframe sort_values does not work. In that instance, the ordering was on a column type string. But as you can see all of the columns here are unambiguously sortable.\n\nWhy does my Pandas DataFrame not display new order using sort_values?\n"
"I have two dataframes, which both have an Order ID and a date. \n\nI wanted to add a flag into the first dataframe df1: if a record with the same order id and date is in dataframe df2, then add a Y:\n\n[ df1['R'] = np.where(orders['key'].isin(df2['key']), 'Y', 0)]\n\n\nTo accomplish that, I was going to create a key, which would be the concatenation of the order_id and date, but when I try the following code:\n\ndf1['key']=df1['Order_ID']+'_'+df1['Date']\n\n\nI get this error\n\nufunc 'add' did not contain a loop with signature matching types dtype('S21') dtype('S21') dtype('S21')\n\n\ndf1 looks like this: \n\nDate | Order_ID | other data points ... \n201751 4395674  ...\n201762 3487535  ...\n\n\nThese are the datatypes:\n\ndf1.info()\nRangeIndex: 157443 entries, 0 to 157442\nData columns (total 6 columns):\nOrder_ID                                 157429 non-null object\nDate                                     157443 non-null int64\n...\ndtypes: float64(2), int64(2), object(2)\nmemory usage: 7.2+ MB\n\ndf1['Order_ID'].values\narray(['782833030', '782834969', '782836416', ..., '783678018',\n       '783679806', '783679874'], dtype=object)\n\n"
"I have two Pandas DataFrames, each with different columns.  I want to basically glue them together horizontally (they each have the same number of rows so this shouldn't be an issue).\n\nThere must be a simple way of doing this but I've gone through the docs and concat isn't what I'm looking for (I don't think).\n\nAny ideas?\n\nThanks!\n"
'I would like to group rows in a dataframe, given one column. Then I would like to receive an edited dataframe for which I can decide which aggregation function makes sense. The default should be just the value of the first entry in the group.\n\n(it would be nice if the solution also worked for a combination of two columns)\n\nExample\n\n#!/usr/bin/env python\n\n"""Test data frame grouping."""\n\n# 3rd party modules\nimport pandas as pd\n\n\ndf = pd.DataFrame([{\'id\': 1, \'price\': 123, \'name\': \'anna\', \'amount\': 1},\n                   {\'id\': 1, \'price\':   7, \'name\': \'anna\', \'amount\': 2},\n                   {\'id\': 2, \'price\':  42, \'name\': \'bob\', \'amount\': 30},\n                   {\'id\': 3, \'price\':   1, \'name\': \'charlie\', \'amount\': 10},\n                   {\'id\': 3, \'price\':   2, \'name\': \'david\', \'amount\': 100}])\nprint(df)\n\n\ngives the dataframe:\n\n   amount  id     name  price\n0       1   1     anna    123\n1       2   1     anna      7\n2      30   2      bob     42\n3      10   3  charlie      1\n4     100   3    david      2\n\n\nAnd I would like to get:\n\namount  id     name  price\n     3   1     anna    130\n    30   2      bob     42\n   110   3  charlie      3\n\n\nSo:\n\n\nEntries with the same value in the id column belong together. After that operation, there should still be an id column, but it should have only unique values.\nAll values in amount and price which have the same id get summed up\nFor name, just the first one (by the current order of the dataframe) is taken.\n\n\nIs this possible with Pandas?\n'
"In the example from the pandas documentation about the new .pipe() method for GroupBy objects, an .apply() method accepting the same lambda would return the same results. \n\nIn [195]: import numpy as np\n\nIn [196]: n = 1000\n\nIn [197]: df = pd.DataFrame({'Store': np.random.choice(['Store_1', 'Store_2'], n),\n   .....:                    'Product': np.random.choice(['Product_1', 'Product_2', 'Product_3'], n),\n   .....:                    'Revenue': (np.random.random(n)*50+10).round(2),\n   .....:                    'Quantity': np.random.randint(1, 10, size=n)})\n\nIn [199]: (df.groupby(['Store', 'Product'])\n   .....:    .pipe(lambda grp: grp.Revenue.sum()/grp.Quantity.sum())\n   .....:    .unstack().round(2))\n\nOut[199]: \nProduct  Product_1  Product_2  Product_3\nStore                                   \nStore_1       6.93       6.82       7.15\nStore_2       6.69       6.64       6.77\n\n\nI can see how the pipe functionality differs from apply for DataFrame objects, but not for GroupBy objects. Does anyone have an explanation or examples of what can be done with pipe but not with apply for a GroupBy?\n"
"I find the result is a little bit random. Sometimes it's a copy sometimes it's a view. For example:\n\ndf = pd.DataFrame([{'name':'Marry', 'age':21},{'name':'John','age':24}],index=['student1','student2'])\n\ndf\n              age   name\n   student1   21  Marry\n   student2   24   John\n\n\nNow, Let me try to modify it a little bit. \n\ndf2= df.loc['student1']\ndf2 [0] = 23\ndf\n              age   name\n   student1   21  Marry\n   student2   24   John\n\n\nAs you can see, nothing changed. df2 is a copy. However, if I add another student into the dataframe...\n\ndf.loc['student3'] = ['old','Tom']\ndf\n               age   name\n    student1   21  Marry\n    student2   24   John\n    student3  old    Tom\n\n\nTry to change the age again..\n\ndf3=df.loc['student1']\ndf3[0]=33\ndf\n               age   name\n    student1   33  Marry\n    student2   24   John\n    student3  old    Tom\n\n\nNow df3 suddenly became a view. What is going on? I guess the value 'old' is the key?\n"
'I\'ve the following dataframe\n\ndf1 = df[[\'tripduration\',\'starttime\',\'stoptime\',\'start station name\',\'end station name\',\'bikeid\',\'usertype\',\'birth year\',\'gender\']]\nprint(df1.head(2))\n\n\nwhich prints the following\n\ntripduration            starttime             stoptime start station name  \\\n0           364  2017-09-01 00:02:01  2017-09-01 00:08:05     Exchange Place   \n1           357  2017-09-01 00:08:12  2017-09-01 00:14:09          Warren St   \n\n   end station name  bikeid    usertype  birth year  gender  \n0  Marin Light Rail   29670  Subscriber      1989.0       1  \n1      Newport Pkwy   26163  Subscriber      1980.0       1\n\n\nI am using the following code to convert "birth year" column type from float to int.\n\ndf1[[\'birth year\']] = df1[[\'birth year\']].astype(int)\nprint df1.head(2)\n\n\nBut I get the following error. How to fix this?\n\nValueErrorTraceback (most recent call last)\n&lt;ipython-input-25-0fe766e4d4a7&gt; in &lt;module&gt;()\n----&gt; 1 df1[[\'birth year\']] = df1[[\'birth year\']].astype(int)\n      2 print df1.head(2)\n      3 __zeppelin__._displayhook()\n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/util/_decorators.pyc in wrapper(*args, **kwargs)\n    116                 else:\n    117                     kwargs[new_arg_name] = new_arg_value\n--&gt; 118             return func(*args, **kwargs)\n    119         return wrapper\n    120     return _deprecate_kwarg\n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/core/generic.pyc in astype(self, dtype, copy, errors, **kwargs)\n   4002         # else, only a single dtype is given\n   4003         new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n-&gt; 4004                                      **kwargs)\n   4005         return self._constructor(new_data).__finalize__(self)\n   4006 \n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/core/internals.pyc in astype(self, dtype, **kwargs)\n   3460 \n   3461     def astype(self, dtype, **kwargs):\n-&gt; 3462         return self.apply(\'astype\', dtype=dtype, **kwargs)\n   3463 \n   3464     def convert(self, **kwargs):\n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/core/internals.pyc in apply(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\n   3327 \n   3328             kwargs[\'mgr\'] = self\n-&gt; 3329             applied = getattr(b, f)(**kwargs)\n   3330             result_blocks = _extend_blocks(applied, result_blocks)\n   3331 \n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/core/internals.pyc in astype(self, dtype, copy, errors, values, **kwargs)\n    542     def astype(self, dtype, copy=False, errors=\'raise\', values=None, **kwargs):\n    543         return self._astype(dtype, copy=copy, errors=errors, values=values,\n--&gt; 544                             **kwargs)\n    545 \n    546     def _astype(self, dtype, copy=False, errors=\'raise\', values=None,\n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/core/internals.pyc in _astype(self, dtype, copy, errors, values, klass, mgr, **kwargs)\n    623 \n    624                 # _astype_nansafe works fine with 1-d only\n--&gt; 625                 values = astype_nansafe(values.ravel(), dtype, copy=True)\n    626                 values = values.reshape(self.shape)\n    627 \n\n/usr/miniconda2/lib/python2.7/site-packages/pandas/core/dtypes/cast.pyc in astype_nansafe(arr, dtype, copy)\n    685 \n    686         if not np.isfinite(arr).all():\n--&gt; 687             raise ValueError(\'Cannot convert non-finite values (NA or inf) to \'\n    688                              \'integer\')\n    689 \n\nValueError: Cannot convert non-finite values (NA or inf) to integer\n\n'
'I\'ve got pandas DataFrame, df, with index named date and the columns columnA, columnB and columnC\n\nI am trying to scatter plot index on a x-axis and columnA on a y-axis using the DataFrame syntax.\n\nWhen I try:\n\ndf.plot(kind=\'scatter\', x=\'date\', y=\'columnA\')\n\n\nI ma getting an error KeyError: \'date\' probably because the date is not column\n\ndf.plot(kind=\'scatter\', y=\'columnA\')\n\n\nI am getting an error:\n\nValueError: scatter requires and x and y column\n\n\nso no default index on x-axis.\n\ndf.plot(kind=\'scatter\', x=df.index, y=\'columnA\')\n\n\nI am getting error \n\nKeyError: "DatetimeIndex([\'1818-01-01\', \'1818-01-02\', \'1818-01-03\', \'1818-01-04\',\\n\n                          \'1818-01-05\', \'1818-01-06\', \'1818-01-07\', \'1818-01-08\',\\n\n                          \'1818-01-09\', \'1818-01-10\',\\n               ...\\n  \n                          \'2018-03-22\', \'2018-03-23\', \'2018-03-24\', \'2018-03-25\',\\n\n                          \'2018-03-26\', \'2018-03-27\', \'2018-03-28\', \'2018-03-29\',\\n \n                          \'2018-03-30\', \'2018-03-31\'],\\n  \ndtype=\'datetime64[ns]\', name=\'date\', length=73139, freq=None) not in index"\n\n\n\n\nI can plot it if I use matplotlib.pyplot directly\n\nplt.scatter(df.index, df[\'columnA\'])\n\n\nIs there a way to plot index as x-axis using the DataFrame kind syntax?\n'
"I have a database generated by a survey to evaluate university professors. What I want is a python script that takes the information from that database, generates a graphing table for each user, creates graphs for each user, and then renders it in a template to export it to a pdf.\n\nWhat does the database look like?\n\nUser    Professor_evaluated  Category       Question    Answer\n_________________________________________________________________\nMike    Professor Criss       respect           1         3\nMike    Professor Criss       respect           2         4\nMike    Professor Criss       wisdom            3         5\nMike    Professor Criss       wisdom            4         3\nCharles Professor Criss       respect           1         3\nCharles Professor Criss       respect           2         4\nCharles Professor Criss       wisdom            3         5\nCharles Professor Criss       wisdom            4         3\n\n\nEach teacher has several categories assigned  to be evaluated (respect, wisdom, etc.) and in turn each category has  associated questions. In other words, a category has several questions. Each row of the DB is the answer to a question from a student evaluating a teacher\n\nWhat do I need?\n\nI need to create a script for automatically generate pdf reports that summarizes this information through charts, for example a chart with the overall score of each teacher, another chart with the score of each teacher by category, another chart with the average of each student, etc..Finally, every teacher would have a report.I want a report like this\n\nWhat is my question?\n\nmy question is about which python packages and modules I would need to do this task. And what would be the general process of doing so. I don't need the code, because I know the answer is very general, but the knowledge of how I could do it.\n\nFor example: you would first need to process the information with pandas, to create a table that summarizes the information you want to graph, then plot it, then create a template of your report with XYZ module and then export it to pdf with XYZ module.\n"
"This seems like a simple question, but I couldn't find it asked before (this and this are close but the answers aren't great). \n\nThe question is: if I want to search for a value somewhere in my df (I don't know which column it's in) and return all rows with a match. \n\nWhat's the most Pandaic way to do it? Is there anything better than:\n\nfor col in list(df):\n    try:    \n        df[col] == var\n        return df[df[col] == var]\n    except TypeError:\n        continue \n\n\n?\n"
"I want to do a quick and easy check if all column values for counts are the same in a dataframe:\n\nIn:\n\nimport pandas as pd\n\nd = {'names': ['Jim', 'Ted', 'Mal', 'Ted'], 'counts': [3, 4, 3, 3]}\npd.DataFrame(data=d)\n\n\nOut: \n\n  names  counts\n0   Jim       3\n1   Ted       4\n2   Mal       3\n3   Ted       3\n\n\nI want just a simple condition that if all counts = same value then print('True').\n\nIs there a fast way to do this?\n"
"Consider this simple setup:\n\nx = pd.Series([1, 2, 3], index=list('abc'))\ny = pd.Series([2, 3, 3], index=list('bca'))\n\nx\n\na    1\nb    2\nc    3\ndtype: int64\n\ny\n\nb    2\nc    3\na    3\ndtype: int64\n\n\nAs you can see, the indexes are the same, just in a different order. \n\nNow, consider a simple logical comparison using the equality (==) operator:\n\nx == y\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n\n\nThis throws a ValueError, most likely because the indexes do not match. On the other hand, calling the equivalent eq operator works:\n\nx.eq(y)\n\na    False\nb     True\nc     True\ndtype: bool\n\n\nOTOH, the operator method works given y is first reordered...\n\nx == y.reindex_like(x)\n\na    False\nb     True\nc     True\ndtype: bool\n\n\nMy understanding was that the function and operator comparison should do the same thing, all other things equal. What is eq doing that the operator comparison doesn't? \n"
'I have a data frame (sample, not real):\n\ndf = \n\n    A   B   C    D   E     F       \n\n0   3   4   NaN  NaN NaN   NaN  \n1   9   8   NaN  NaN NaN   NaN      \n2   5   9   4    7   NaN   NaN  \n3   5   7   6    3   NaN   NaN  \n4   2   6   4    3   NaN   NaN  \n\n\nNow I want to fill NaN values with previous couple(!!!) values of row (fill Nan with left existing couple of numbers and apply to the whole row) and apply this to the whole dataset.\n\n\nThere are a lot of answers concerning filling the columns. But in\nthis case I need to fill based on rows.\nThere are also answers related to fill NaN based on other column, but\nin my case number of columns are more than 2000. This is sample data\n\n\nDesired output is:\n\ndf = \n\n   A  B   C  D  E  F  \n\n0  3  4   3  4  3  4  \n1  9  8   9  8  9  8  \n2  5  9   4  7  4  7      \n3  5  7   6  3  6  3  \n4  2  6   4  3  4  3  \n\n'
'If I have a dataframe that has columns that include the same name, is there a way to combine the columns that have the same name with some sort of function (i.e. sum)?\n\nFor instance with:\n\nIn [186]:\n\ndf["NY-WEB01"].head()\nOut[186]:\n                NY-WEB01    NY-WEB01\nDateTime        \n2012-10-18 16:00:00  5.6     2.8\n2012-10-18 17:00:00  18.6    12.0\n2012-10-18 18:00:00  18.4    12.0\n2012-10-18 19:00:00  18.2    12.0\n2012-10-18 20:00:00  19.2    12.0\n\n\nHow might I collapse the NY-WEB01 columns (there are a  bunch of duplicate columns, not just NY-WEB01) by summing each row where the column name is the same?\n'
'Say I have a dataframe\n\nimport pandas as pd\nimport numpy as np\nfoo = pd.DataFrame(np.random.random((10,5)))\n\n\nand I create another dataframe from a subset of my data:\n\nbar = foo.iloc[3:5,1:4]\n\n\ndoes bar hold a copy of those elements from foo? Is there any way to create a view of that data instead? If so, what would happen if I try to modify data in this view? Does Pandas provide any sort of copy-on-write mechanism?\n'
"I'd like to generate a series that's the incremental mean of a timeseries.  Meaning that, starting from the first date (index 0), the mean stored in row x is the average of values [0:x]\n\ndata\nindex   value   mean          formula\n0       4\n1       5\n2       6\n3       7       5.5           average(0-3)\n4       4       5.2           average(0-4)\n5       5       5.166666667   average(0-5)\n6       6       5.285714286   average(0-6)\n7       7       5.5           average(0-7)\n\n\nI'm hoping there's a way to do this without looping to take advantage of pandas.\n"
"I want to simply reverse the column order of a given DataFrame. \n\nMy DataFrame:\n\ndata = {'year': [2010, 2011, 2012, 2011, 2012, 2010, 2011, 2012],\n    'team': ['Bears', 'Bears', 'Bears', 'Packers', 'Packers', 'Lions', 'Lions', 'Lions'],\n    'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n    'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\nfootball = pd.DataFrame(data, columns=['year', 'team', 'wins', 'losses'])\n\n\nActual output:\n\n   year     team  wins  losses\n0  2010    Bears    11       5\n1  2011    Bears     8       8\n2  2012    Bears    10       6\n3  2011  Packers    15       1\n4  2012  Packers    11       5\n5  2010    Lions     6      10\n6  2011    Lions    10       6\n7  2012    Lions     4      12\n\n\nI thought this would work but it reverses the row order not column order:\n\nfootball[::-1] \n\n\nI also tried:\n\nfootball.columns = football.columns[::-1]\n\n\nbut that reversed the column labels and not the entire column itself. \n"
"I'm having trouble getting a barplot in seaborn.  Here's my reproducible data:\n\npeople = ['Hannah', 'Bethany', 'Kris', 'Alex', 'Earl', 'Lori']\nreputation = ['awesome', 'cool', 'brilliant', 'meh', 'awesome', 'cool']\ndictionary = dict(zip(people, reputation))\ndf = pd.DataFrame(dictionary.values(), dictionary.keys())\ndf = df.rename(columns={0:'reputation'})\n\n\nThen I want to get a bar plot showing the value counts of different reputation.  I've tried:\n\nsns.barplot(x = 'reputation', y = df['reputation'].value_counts(), data = df, ci = None)\n\n\nand \n\nsns.barplot(x = 'reputation', y = df['reputation'].value_counts().values, data = df, ci = None)\n\n\nbut both return blank plots.  \n\nAny idea what I can do to get this?  \n"
"Numpy seems to make a distinction between str and object types.  For instance I can do ::\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.dtype(str)\ndtype('S')\n&gt;&gt;&gt; np.dtype(object)\ndtype('O')\n\n\nWhere dtype('S') and dtype('O') corresponds to str and object respectively.\n\nHowever pandas seem to lack that distinction and coerce str to object. ::\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': np.arange(5)})\n&gt;&gt;&gt; df.a.dtype\ndtype('int64')\n&gt;&gt;&gt; df.a.astype(str).dtype\ndtype('O')\n&gt;&gt;&gt; df.a.astype(object).dtype\ndtype('O')\n\n\nForcing the type to dtype('S') does not help either. ::\n\n&gt;&gt;&gt; df.a.astype(np.dtype(str)).dtype\ndtype('O')\n&gt;&gt;&gt; df.a.astype(np.dtype('S')).dtype\ndtype('O')\n\n\nIs there any explanation for this behavior? \n"
"I currently have this code. It works perfectly.\n\nIt loops through excel files in a folder,\nremoves the first 2 rows, then saves them as individual excel files,\nand it also saves the files in the loop as an appended file.\n\nCurrently the appended file overwrites the existing file each time I run the code.\n\nI need to append the new data to the bottom of the already existing excel sheet ('master_data.xlsx)\n\ndfList = []\npath = 'C:\\\\Test\\\\TestRawFile' \nnewpath = 'C:\\\\Path\\\\To\\\\New\\\\Folder'\n\nfor fn in os.listdir(path): \n  # Absolute file path\n  file = os.path.join(path, fn)\n  if os.path.isfile(file): \n    # Import the excel file and call it xlsx_file \n    xlsx_file = pd.ExcelFile(file) \n    # View the excel files sheet names \n    xlsx_file.sheet_names \n    # Load the xlsx files Data sheet as a dataframe \n    df = xlsx_file.parse('Sheet1',header= None) \n    df_NoHeader = df[2:] \n    data = df_NoHeader \n    # Save individual dataframe\n    data.to_excel(os.path.join(newpath, fn))\n\n    dfList.append(data) \n\nappended_data = pd.concat(dfList)\nappended_data.to_excel(os.path.join(newpath, 'master_data.xlsx'))\n\n\nI thought this would be a simple task, but I guess not.\nI think I need to bring in the master_data.xlsx file as a dataframe, then match the index up with the new appended data, and save it back out. Or maybe there is an easier way. Any Help is appreciated.\n"
'Could someone please point me in the right direction with respect to OHLC data timeframe conversion with Pandas? What I\'m trying to do is build a Dataframe with data for higher timeframes, given data with lower timeframe. \n\nFor example, given I have the following one-minute (M1) data:\n\n                       Open    High     Low   Close  Volume\nDate                                                       \n1999-01-04 10:22:00  1.1801  1.1819  1.1801  1.1817       4\n1999-01-04 10:23:00  1.1817  1.1818  1.1804  1.1814      18\n1999-01-04 10:24:00  1.1817  1.1817  1.1802  1.1806      12\n1999-01-04 10:25:00  1.1807  1.1815  1.1795  1.1808      26\n1999-01-04 10:26:00  1.1803  1.1806  1.1790  1.1806       4\n1999-01-04 10:27:00  1.1801  1.1801  1.1779  1.1786      23\n1999-01-04 10:28:00  1.1795  1.1801  1.1776  1.1788      28\n1999-01-04 10:29:00  1.1793  1.1795  1.1782  1.1789      10\n1999-01-04 10:31:00  1.1780  1.1792  1.1776  1.1792      12\n1999-01-04 10:32:00  1.1788  1.1792  1.1788  1.1791       4\n\n\nwhich has Open, High, Low, Close (OHLC) and volume values for every minute I would like to build a set of 5-minute readings (M5) which would look like so:\n\n                       Open    High     Low   Close  Volume\nDate                                                       \n1999-01-04 10:25:00  1.1807  1.1815  1.1776  1.1789      91\n1999-01-04 10:30:00  1.1780  1.1792  1.1776  1.1791      16\n\n\nSo the workflow is that:\n\n\nOpen is the Open of the first row in the timewindow\nHigh is the highest High in the timewindow\nLow is the lowest Low\nClose is the last Close\nVolume is simply a sum of Volumes\n\n\nThere are few issues though:\n\n\nthe data has gaps ( note there is no 10:30:00 row)\nthe 5-minute intervals have to start at round time, e.g. M5 starts at 10:25:00 not 10:22:00\nfirst, incomplete set can be omitted like in this example, or included (so we could have 10:20:00 5-minute entry)\n\n\nThe Pandas documentation on up-down sampling gives an example, but they use mean value as the value of up-sampled row, which won\'t work here. I have tried using groupby and agg but to no avail. For one getting highest High and lowest Low might be not so hard, but I have no idea how to get first Open and last Close.\n\nWhat I tried is something along the lines of:\n\ngrouped = slice.groupby( dr5minute.asof ).agg( \n    { \'Low\': lambda x : x.min()[ \'Low\' ], \'High\': lambda x : x.max()[ \'High\' ] } \n)\n\n\nbut it results in following error, which I don\'t understand:\n\nIn [27]: grouped = slice.groupby( dr5minute.asof ).agg( { \'Low\' : lambda x : x.min()[ \'Low\' ], \'High\' : lambda x : x.max()[ \'High\' ] } )\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n/work/python/fxcruncher/&lt;ipython-input-27-df50f9522a2f&gt; in &lt;module&gt;()\n----&gt; 1 grouped = slice.groupby( dr5minute.asof ).agg( { \'Low\' : lambda x : x.min()[ \'Low\' ], \'High\' : lambda x : x.max()[ \'High\' ] } )\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in agg(self, func, *args, **kwargs)\n    242         See docstring for aggregate\n    243         """\n--&gt; 244         return self.aggregate(func, *args, **kwargs)\n    245 \n    246     def _iterate_slices(self):\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in aggregate(self, arg, *args, **kwargs)\n   1153                     colg = SeriesGroupBy(obj[col], column=col,\n   1154                                          grouper=self.grouper)\n-&gt; 1155                     result[col] = colg.aggregate(func)\n   1156 \n   1157             result = DataFrame(result)\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in aggregate(self, func_or_funcs, *args, **kwargs)\n    906                 return self._python_agg_general(func_or_funcs, *args, **kwargs)\n    907             except Exception:\n--&gt; 908                 result = self._aggregate_named(func_or_funcs, *args, **kwargs)\n    909 \n    910             index = Index(sorted(result), name=self.grouper.names[0])\n\n/usr/lib/python2.7/site-packages/pandas/core/groupby.pyc in _aggregate_named(self, func, *args, **kwargs)\n    976             grp = self.get_group(name)\n    977             grp.name = name\n--&gt; 978             output = func(grp, *args, **kwargs)\n    979             if isinstance(output, np.ndarray):\n    980                 raise Exception(\'Must produce aggregated value\')\n\n/work/python/fxcruncher/&lt;ipython-input-27-df50f9522a2f&gt; in &lt;lambda&gt;(x)\n----&gt; 1 grouped = slice.groupby( dr5minute.asof ).agg( { \'Low\' : lambda x : x.min()[ \'Low\' ], \'High\' : lambda x : x.max()[ \'High\' ] } )\n\nIndexError: invalid index to scalar variable.\n\n\nSo any help on doing that would be greatly appreciated. If the path I chose is not going to work, please suggest other relatively efficient approach (I have millions of rows). Some resources on using Pandas for financial processing would also be nice.\n'
"I'm trying to remove entries from a data frame which occur less than 100 times. \nThe data frame data looks like this:\n\npid   tag\n1     23    \n1     45\n1     62\n2     24\n2     45\n3     34\n3     25\n3     62\n\n\nNow I count the number of tag occurrences like this:\n\nbytag = data.groupby('tag').aggregate(np.count_nonzero)\n\n\nBut then I can't figure out how to remove those entries which have low count...\n"
"I have minute based OHLCV data for the opening range/first hour (9:30-10:30 AM EST).  I'm looking to resample this data so I can get one 60-minute value and then calculate the range.\n\nWhen I call the dataframe.resample() function on the data I get two rows and the initial row starts at 9:00 AM.  I'm looking to get only one row which starts at 9:30 AM. \n\nNote: the initial data begins at 9:30. \n\n\n\nEdit:  Adding code:\n\n# Extract data for regular trading hours (rth) from the 24 hour data set\nrth = data.between_time(start_time = '09:30:00', end_time = '16:15:00', include_end = False)\n\n# Extract data for extended trading hours (eth) from the 24 hour data set\neth = data.between_time(start_time = '16:30:00', end_time = '09:30:00', include_end = False)\n\n# Extract data for initial balance (rth) from the 24 hour data set\ninitial_balance = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end =      False)\n\n\nGot stuck tried to separate the opening range by individual date and get the Initial Balance\n\nconversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last', 'Volume' : 'sum'}\nsample = data.between_time(start_time = '09:30:00', end_time = '10:30:00', include_end = False)\nsample = sample.ix['2007-05-07']\nsample.tail()\n\nsample.resample('60Min', how = conversion) \n\n\nBy default resample starts at the beggining of the hour.  I would like it to start from where the data starts.\n"
"If I define a hierarchically-indexed dataframe like this:\n\nimport itertools\nimport pandas as pd\nimport numpy as np\na = ('A', 'B')\ni = (0, 1, 2)\nb = (True, False)\nidx = pd.MultiIndex.from_tuples(list(itertools.product(a, i, b)),\n                                names=('Alpha', 'Int', 'Bool'))\ndf = pd.DataFrame(np.random.randn(len(idx), 7), index=idx,\n                  columns=('I', 'II', 'III', 'IV', 'V', 'VI', 'VII'))\n\n\nthe contents look like this:\n\nIn [19]: df\nOut[19]: \n                        I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -0.462924  1.210442  0.306737  0.325116 -1.320084 -0.831699  0.892865\n          False -0.850570 -0.949779  0.022074 -0.205575 -0.684794 -0.214307 -1.133833\n      1   True   0.603602  1.387020 -0.830780 -1.242000 -0.321938  0.484271  0.171738\n          False -1.591730  1.282136  0.095159 -1.239882  0.760880 -0.606444 -0.485957\n      2   True  -1.346883  1.650247 -1.476443  2.092067  1.344689  0.177083  0.100844\n          False  0.001407 -1.127299 -0.417828  0.143595 -0.277838 -0.478262 -0.350906\nB     0   True   0.722781 -1.093182  0.237536  0.457614 -2.500885  0.338257  0.009128\n          False  0.321022  0.419357  1.161140 -1.371035  1.093696  0.250517 -1.125612\n      1   True   0.237441  1.739933  0.029653  0.327823 -0.384647  1.523628 -0.009053\n          False -0.459148 -0.598577 -0.593486 -0.607447  1.478399  0.504028 -0.329555\n      2   True  -0.583052 -0.986493 -0.057788 -0.639798  1.400311  0.076471 -0.212513\n          False  0.896755  2.583520  1.520151  2.367336 -1.084994 -1.233548 -2.414215\n\n\nI know how to extract the data corresponding to a given column.  E.g. for column 'VII':\n\nIn [20]: df['VII']\nOut[20]: \nAlpha  Int  Bool \nA      0    True     0.892865\n            False   -1.133833\n       1    True     0.171738\n            False   -0.485957\n       2    True     0.100844\n            False   -0.350906\nB      0    True     0.009128\n            False   -1.125612\n       1    True    -0.009053\n            False   -0.329555\n       2    True    -0.212513\n            False   -2.414215\nName: VII\n\n\nHow do I extract the data matching the following sets of criteria:\n\n\nAlpha=='B'\nAlpha=='B', Bool==False\nAlpha=='B', Bool==False, column 'I'\nAlpha=='B', Bool==False, columns 'I' and 'III'\nAlpha=='B', Bool==False, columns 'I', 'III', and all columns from 'V' onwards\nInt is even\n\n\n(BTW, I did rtfm, more than once even, but I really find it incomprehensible.)\n"
'How can I drop or disable the indices in a pandas Data Frame?\n\nI am learning the pandas from the book "python for data analysis" and I already know I can use the dataframe.drop to drop one column or one row. But I did not find anything about disabling the all the indices in place.\n'
"I grouped my dataframe by the two columns below\n\ndf = pd.DataFrame({'a': [1, 1, 3],\n                   'b': [4.0, 5.5, 6.0],\n                   'c': [7L, 8L, 9L],\n                   'name': ['hello', 'hello', 'foo']})\ndf.groupby(['a', 'name']).median()\n\n\nand the result is:\n\n            b    c\na name            \n1 hello  4.75  7.5\n3 foo    6.00  9.0\n\n\nHow can I access the name field of the resulting median (in this case hello, foo)? This fails:\n\ndf.groupby(['a', 'name']).median().name\n\n"
"Suppose I have the following DataFrame:\n\nIn [1]: df\nOut[1]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\n\n\nThis works as expected:\n\nIn [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1   NaN      4    bad\n2     2      5   good\n\n\nBut this doesn't:\n\nIn [2]: df[['apple', 'banana']][df.cherry == 'bad'] = np.nan\nIn [3]: df\nOut[3]:\n  apple banana cherry\n0     0      3   good\n1     1      4    bad\n2     2      5   good\n\n\nWhy?  How can I achieve the conversion of both the 'apple' and 'banana' values without having to write out two lines, as in\n\nIn [2]: df['apple'][df.cherry == 'bad'] = np.nan\nIn [3]: df['banana'][df.cherry == 'bad'] = np.nan\n\n"
"I have a very large data frame df that looks  like:\n\nID       Value1    Value2\n1345      3.2      332\n1355      2.2      32\n2346      1.0      11\n3456      8.9      322\n\n\nAnd I have a list that contains a subset of IDs ID_list. I need to have a subset of df for the ID contained in ID_list. \n\nCurrently, I am using df_sub=df[df.ID.isin(ID_list)] to do it. But it takes a lot time. IDs contained in ID_list doesn't have any pattern, so it's not within certain range. (And I need to apply the same operation to many similar dataframes. I was wondering if there is any faster way to do this. Will it help a lot if make ID as the index? \n\nThanks!\n"
"I have a pandas data frame that has is composed of different subgroups. \n\n    df = pd.DataFrame({\n    'id':[1, 2, 3, 4, 5, 6, 7, 8], \n    'group':['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'], \n    'value':[.01, .4, .2, .3, .11, .21, .4, .01]\n    })\n\n\nI want to find the rank of each id in its group with say, lower values being better.  In the example above, in group A, Id 1 would have a rank of 1, Id 2 would have a rank of 4.  In group B, Id 5 would have a rank of 2, Id 8 would have a rank of 1 and so on. \n\nRight now I assess the ranks by:\n\n\nSorting by value.\n\ndf.sort('value', ascending = True, inplace=True)\nCreate a ranker function (it assumes variables already sorted)\n\ndef ranker(df):\n    df['rank'] = np.arange(len(df)) + 1\n    return df\nApply the ranker function on each group separately:\n\ndf = df.groupby(['group']).apply(ranker)\n\n\nThis process works but it is really slow when I run it on millions of rows of data.  Does anyone have any ideas on how to make a faster ranker function.   \n"
"I have a sorting request per example below.\n\nDo i need to reset_index(), then sort() and then set_index() or is there a slick way to do this?\n\nl = [[1,'A',99],[1,'B',102],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\n\n# assume data has been received like this...\nprint df\n\n           col1\nidx1 idx2      \n1    A       99\n     B      102\n     C      105\n     D       97\n2    A       19\n     B       14\n     C       10\n     D       17\n\n# I'd like to sort descending on col1, partitioning within index level = 'idx2'\n\n           col1\nidx1 idx2      \n1    C      105\n     B      102\n     A       99\n     D       97\n\n2    A       19\n     D       17\n     B       14\n     C       10\n\n\n\n\nThank you for the answer\nNote I change the data slightly:\n\nl = [[1,'A',99],[1,'B',11],[1,'C',105],[1,'D',97],[2,'A',19],[2,'B',14],[2,'C',10],[2,'D',17]]\ndf = pd.DataFrame(l,columns = ['idx1','idx2','col1'])\ndf.set_index(['idx1','idx2'],inplace=True)\ndf = df.sort_index(by='col1', ascending=False)\n\n\nhowever the output is \n\nidx1 idx2      \n1    C      105\n     A       99\n     D       97\n2    A       19\n     D       17\n     B       14\n1    B       11\n2    C       10\n\n\ni would have wanted it to be \n\nidx1 idx2      \n1    C      105\n     A       99\n     D       97\n     B       11\n\n2    A       19\n     D       17\n     B       14\n     C       10\n\n"
'When using pandas read_csv() method, does it keep the file open or it closes it (discard the file descriptor)?\n\nIf it keeps it, how do I close it after I finish using the dataframe?\n'
"I have a Pandas dataframe:\n\ntype(original)\npandas.core.frame.DataFrame\n\n\nwhich includes the series object original['user']:\n\ntype(original['user'])\npandas.core.series.Series\n\n\noriginal['user'] points to a number of dicts:\n\ntype(original['user'].ix[0])\ndict\n\n\nEach dict has the same keys:\n\noriginal['user'].ix[0].keys()\n\n[u'follow_request_sent',\n u'profile_use_background_image',\n u'profile_text_color',\n u'id',\n u'verified',\n u'profile_location',\n # ... keys removed for brevity\n]\n\n\nAbove is (part of) one of the dicts of user fields in a tweet from tweeter API. I want to build a data frame from these dicts.\n\nWhen I try to make a data frame directly, I get only one column for each row and this column contains the whole dict:\n\npd.DataFrame(original['user'][:2])\n    user\n0   {u'follow_request_sent': False, u'profile_use_...\n1   {u'follow_request_sent': False, u'profile_use_..\n\n\nWhen I try to create a data frame using from_dict() I get the same result:\n\npd.DataFrame.from_dict(original['user'][:2])\n\n    user\n0   {u'follow_request_sent': False, u'profile_use_...\n1   {u'follow_request_sent': False, u'profile_use_..\n\n\nNext I tried a list comprehension which returned an error:\n\nitem = [[k, v] for (k,v) in users]\nValueError: too many values to unpack\n\n\nWhen I create a data frame from a single row, it nearly works:\n\ndf = pd.DataFrame.from_dict(original['user'].ix[0])\ndf.reset_index()\n\n    index   contributors_enabled    created_at  default_profile     default_profile_image   description     entities    favourites_count    follow_request_sent     followers_count     following   friends_count   geo_enabled     id  id_str  is_translation_enabled  is_translator   lang    listed_count    location    name    notifications   profile_background_color    profile_background_image_url    profile_background_image_url_https  profile_background_tile     profile_image_url   profile_image_url_https     profile_link_color  profile_location    profile_sidebar_border_color    profile_sidebar_fill_color  profile_text_color  profile_use_background_image    protected   screen_name     statuses_count  time_zone   url     utc_offset  verified\n0   description     False   Mon May 26 11:58:40 +0000 2014  True    False       {u'urls': []}   0   False   157\n\n\nIt works almost like I want it to, except it sets the description field as the default index.\n\nEach of the dicts has 40 keys but I only need about 10 of them and I have 28734 rows in data frame.\n\nHow can I filter out the keys which I do not need?\n"
"I have a pandas data frame with date column, and I am trying to add a new column of boolean values indicating whether a given date is a holiday or not.\n\nFollowing is the code, but it does not work (all the values are False) because the types seem to be different, and I can't figure out how to get the 'date' in the pandas data frame to be of the same type as the holidays:\n\ncal = USFederalHolidayCalendar()\nholidays = cal.holidays(start=train_df['date'].min(),\n                        end=train_df['date'].max()).to_pydatetime()\ntrain_df['holiday'] = train_df['date'].isin(holidays)\nprint type(train_df['date'][1])\nprint type(holidays[0])\n\n"
"I'm trying to use pandas in order to change one of my columns in-place, using simple function.\n\nAfter reading the whole Dataframe, I tried to apply function on one Serie:\n\nwanted_data.age.apply(lambda x: x+1)\n\n\nAnd it's working great. The only problem occurs when I try to put it back into my DataFrame:\n\nwanted_data.age = wanted_data.age.apply(lambda x: x+1)\n\n\nor:\n\nwanted_data['age'] = wanted_data.age.apply(lambda x: x+1)\n\n\nThrowing the following warning:\n\n&gt; C:\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py:1974:\n&gt; SettingWithCopyWarning: A value is trying to be set on a copy of a\n&gt; slice from a DataFrame. Try using .loc[row_indexer,col_indexer] =\n&gt; value instead\n&gt; \n&gt; See the the caveats in the documentation:\n&gt; http://pandas.pydata.org/pandas-docs/stable\n&gt; /indexing.html#indexing-view-versus-copy   self[name] = value\n\n\nOf Course, I can set the DataFrame using the long form of:\n\nwanted_data.loc[:, 'age'] = wanted_data.age.apply(lambda x: x+1)\n\n\nBut is there no other, easier and more syntactic-nicer way to do it?\n\nThanks!\n"
"Basically the same as Select first row in each GROUP BY group? only in pandas.\n\ndf = pd.DataFrame({'A' : ['foo', 'foo', 'foo', 'foo', 'bar', 'bar', 'bar', 'bar'],\n                'B' : ['3', '1', '2', '4','2', '4', '1', '3'],\n                    })\n\n\nSorting looks promising:\n\ndf.sort('B')\n\n     A  B\n1  foo  1\n6  bar  1\n2  foo  2\n4  bar  2\n0  foo  3\n7  bar  3\n3  foo  4\n5  bar  4\n\n\nBut then first won't give the desired result... \n    df.groupby('A').first()\n\n     B\nA     \nbar  2\nfoo  3\n\n"
'I know there are a number of topics on this question, but none of the methods worked for me so I\'m posting about my specific situation\n\nI have a dataframe that looks like this:\n\ndata = pd.DataFrame([[1,0],[0,1],[1,0],[0,1]], columns=["sex", "split"])\ndata[\'sex\'].replace(0, \'Female\')\ndata[\'sex\'].replace(1, \'Male\')\ndata\n\n\nWhat I want to do is replace all 0\'s in the sex column with \'Female\', and all 1\'s with \'Male\', but the values within the dataframe don\'t seem to change when I use the code above\n\nAm I using replace() incorrectly? Or is there a better way to do conditional replacement of values?\n'
"I'm building a small graphing utility using Pandas and MatPlotLib to parse data and output graphs from a machine at work.\n\nWhen I output the graph using \n\nplt.show()\n\n\nI end up with an unclear image that has legends and labels crowding each other out like so.\n\n\nHowever, expanding the window to full-screen resolves my problem, repositioning everything in a way that allows the graph to be visible.\n\nI then save the graph to a .png like so\n\nplt.savefig('sampleFileName.png')\n\n\nBut when it saves to the image, the full-screen, correct version of the plot isn't saved, but instead the faulty default version.\n\nHow can I save the full-screen plt.show() of the plot to .png?\n\nI hope I'm not too confusing.\n\nThank you for your help!\n"
'I am looking for an elegant way to append all the rows from one DataFrame to another DataFrame (both DataFrames having the same index and column structure), but in cases where the same index value appears in both DataFrames, use the row from the second data frame.\n\nSo, for example, if I start with:\n\ndf1:\n                    A      B\n    date\n    \'2015-10-01\'  \'A1\'   \'B1\'\n    \'2015-10-02\'  \'A2\'   \'B2\'\n    \'2015-10-03\'  \'A3\'   \'B3\'\n\ndf2:\n    date            A      B\n    \'2015-10-02\'  \'a1\'   \'b1\'\n    \'2015-10-03\'  \'a2\'   \'b2\'\n    \'2015-10-04\'  \'a3\'   \'b3\'\n\n\nI would like the result to be:\n\n                    A      B\n    date\n    \'2015-10-01\'  \'A1\'   \'B1\'\n    \'2015-10-02\'  \'a1\'   \'b1\'\n    \'2015-10-03\'  \'a2\'   \'b2\'\n    \'2015-10-04\'  \'a3\'   \'b3\'\n\n\nThis is analogous to what I think is called "upsert" in some SQL systems --- a combination of update and insert, in the sense that each row from df2 is either (a) used to update an existing row in df1 if the row key already exists in df1, or (b) inserted into df1 at the end if the row key does not already exist.\n\nI have come up with the following \n\npd.concat([df1, df2])     # concat the two DataFrames\n    .reset_index()        # turn \'date\' into a regular column\n    .groupby(\'date\')      # group rows by values in the \'date\' column\n    .tail(1)              # take the last row in each group\n    .set_index(\'date\')    # restore \'date\' as the index\n\n\nwhich seems to work, but this relies on the order of the rows in each groupby group always being the same as the original DataFrames, which I haven\'t checked on, and seems displeasingly convoluted.\n\nDoes anyone have any ideas for a more straightforward solution?\n'
"Assuming the following DataFrame:\n\n  key.0 key.1 key.2  topic\n1   abc   def   ghi      8\n2   xab   xcd   xef      9\n\n\nHow can I combine the values of all the key.* columns into a single column 'key', that's associated with the topic value corresponding to the key.* columns?  This is the result I want:  \n\n   topic  key\n1      8  abc\n2      8  def\n3      8  ghi\n4      9  xab\n5      9  xcd\n6      9  xef\n\n\nNote that the number of key.N columns is variable on some external N.\n"
"I have a DataFrame indexed on the month column (set using df = df.set_index('month'), in case that's relevant): \n\n             org_code  ratio_cost   \nmonth\n2010-08-01   1847      8.685939     \n2010-08-01   1848      7.883951     \n2010-08-01   1849      6.798465     \n2010-08-01   1850      7.352603     \n2010-09-01   1847      8.778501     \n\n\nI want to add a new column called quantile, which will assign a quantile value to each row, based on the value of its ratio_cost for that month. \n\nSo the example above might look like this:\n\n             org_code  ratio_cost   quantile\nmonth\n2010-08-01   1847      8.685939     100 \n2010-08-01   1848      7.883951     66.6 \n2010-08-01   1849      6.798465     0  \n2010-08-01   1850      7.352603     33.3\n2010-09-01   1847      8.778501     100\n\n\nHow can I do this? I've tried this:\n\ndf['quantile'] = df.groupby('month')['ratio_cost'].rank(pct=True)\n\n\nBut I get KeyError: 'month'. \n\nUPDATE: I can reproduce the bug. \n\nHere is my CSV file: http://pastebin.com/raw/6xbjvEL0\n\nAnd here is the code to reproduce the error:\n\ndf = pd.read_csv('temp.csv')\ndf.month = pd.to_datetime(df.month, unit='s')\ndf = df.set_index('month')\ndf['percentile'] = df.groupby(df.index)['ratio_cost'].rank(pct=True)\nprint df['percentile']\n\n\nI'm using Pandas 0.17.1 on OSX. \n"
"I'm trying to add a column to my DataFrame which is the product of division of two other columns, like so:\n\ndf['$/hour'] = df['$']/df['hours']\n\n\nThis works fine, but if the value in ['hours'] is less than 1, then the ['$/hour'] value is greater than the value in ['$'], which is not what I want.\n\nIs there a way of controlling the operation so that if ['hours'] &lt; 1 then df['$/hour'] = df['$']?\n"
"Given the following dataframe:\n\nimport pandas as pd\ndf = pd.DataFrame({'COL1': ['A', np.nan,'A'], \n                   'COL2' : [np.nan,'A','A']})\ndf\n    COL1    COL2\n0    A      NaN\n1    NaN    A\n2    A      A\n\n\nI would like to create a column ('COL3') that uses the value from COL1 per row unless that value is null (or NaN). If the value is null (or NaN), I'd like for it to use the value from COL2.\n\nThe desired result is:\n\n    COL1    COL2   COL3\n0    A      NaN    A\n1    NaN    A      A\n2    A      A      A\n\n\nThanks in advance!\n"
"I have this simple dataframe df:\n\ndf = pd.DataFrame({'c':[1,1,1,2,2,2,2],'type':['m','n','o','m','m','n','n']})\n\n\nmy goal is to count values of type for each c, and then add a column with the size of c. So starting with:\n\nIn [27]: g = df.groupby('c')['type'].value_counts().reset_index(name='t')\n\nIn [28]: g\nOut[28]: \n   c type  t\n0  1    m  1\n1  1    n  1\n2  1    o  1\n3  2    m  2\n4  2    n  2\n\n\nthe first problem is solved. Then I can also:\n\nIn [29]: a = df.groupby('c').size().reset_index(name='size')\n\nIn [30]: a\nOut[30]: \n   c  size\n0  1     3\n1  2     4\n\n\nHow can I add the size column directly to the first dataframe? So far I used map as:\n\nIn [31]: a.index = a['c']\n\nIn [32]: g['size'] = g['c'].map(a['size'])\n\nIn [33]: g\nOut[33]: \n   c type  t  size\n0  1    m  1     3\n1  1    n  1     3\n2  1    o  1     3\n3  2    m  2     4\n4  2    n  2     4\n\n\nwhich works, but is there a more straightforward way to do this?\n"
"I'm trying to update a couple fields at once - I have two data sources and I'm trying to reconcile them.  I know I could do some ugly merging and then delete columns, but was expecting this code below to work:\n\ndf = pd.DataFrame([['A','B','C',np.nan,np.nan,np.nan],\n                  ['D','E','F',np.nan,np.nan,np.nan],[np.nan,np.nan,np.nan,'a','b','d'],\n                  [np.nan,np.nan,np.nan,'d','e','f']], columns = ['Col1','Col2','Col3','col1_v2','col2_v2','col3_v2'])\n\nprint df\n\n Col1 Col2 Col3 col1_v2 col2_v2 col3_v2\n0    A    B    C     NaN     NaN     NaN\n1    D    E    F     NaN     NaN     NaN\n2  NaN  NaN  NaN       a       b       d\n3  NaN  NaN  NaN       d       e       f\n\n#update \ndf.loc[df['Col1'].isnull(),['Col1','Col2', 'Col3']] = df[['col1_v2','col2_v2','col3_v2']]\n\nprint df\n\n Col1 Col2 Col3 col1_v2 col2_v2 col3_v2\n0    A    B    C     NaN     NaN     NaN\n1    D    E    F     NaN     NaN     NaN\n2  NaN  NaN  NaN       a       b       d\n3  NaN  NaN  NaN       d       e       f\n\n\nMy desired output would be:\n\n Col1 Col2 Col3 col1_v2 col2_v2 col3_v2\n0    A    B    C     NaN     NaN     NaN\n1    D    E    F     NaN     NaN     NaN\n2    a    b    c       a       b       d\n3    d    e    f       d       e       f\n\n\nI'm betting it has to do with updating/setting on a slice, but I always use .loc to update values, just not on multiple columns at once.\n\nI feel like there's an easy way to do this that I'm just missing, any thoughts/suggestions would be welcome!\n\nEdit to reflect solution below\nThanks for the comment on the indexes.  However, I have a question about this as it relates to series.  If I wanted to update an individual series in a similar manner, I could do something like this:\n\ndf.loc[df['Col1'].isnull(),['Col1']] = df['col1_v2']\n\nprint df\n\n  Col1 Col2 Col3 col1_v2 col2_v2 col3_v2\n0    A    B    C     NaN     NaN     NaN\n1    D    E    F     NaN     NaN     NaN\n2    a  NaN  NaN       a       b       d\n3    d  NaN  NaN       d       e       f\n\n\nNote that I didn't account for the indexes here, I filtered to a 2x1 series and set that equal to a 4x1 series, yet it handled it correctly.  Thoughts?  I'm trying to understand the functionality a bit better of something I've used for a while, but I guess don't have a full grasp of the underlying mechanism/rule\n"
"I faced a problem while printing pandas dataframes in jupyter notebook. If the column names are really long it breaks the dataframe structure in different lines. \n\nHow can I print it like the way jupyter notebook does it by default(Shown in image - third cell)? As far as I know, only way to print the dataframe in bordered table style, you have to leave the variable name as the last command of the notebooks cell.\n\n\n\nHere's the code if you want to check it,\n\nd = pd.DataFrame({'A1_column':[1, 2, 4], 'B1_column':['a', 'b', 'd'],\n                 'A2_column':[1, 2, 4], 'B2_column':['a', 'b', 'd'],\n                 'A3_column':[1, 2, 4], 'B3_column':['a', 'b', 'd'],\n                 'A4_column':[1, 2, 4], 'B4_column':['a', 'b', 'd'],\n                 'A5_column':[1, 2, 4], 'B5_column':['a', 'b', 'd'],\n                 'A6_column':[1, 2, 4], 'B6_column':['a', 'b', 'd'],\n                 'A7_column':[1, 2, 4], 'B7_column':['a', 'b', 'd']})\nprint(d)\nd\n\n"
'If I have a DataFrame such that:\n\npd.DataFrame( {"name" : "John", \n               "days" : [[1, 3, 5, 7]]\n              })\n\n\ngives this structure:\n\n           days  name\n0  [1, 3, 5, 7]  John\n\n\nHow do expand it to the following?\n\n   days  name\n0     1  John\n1     3  John\n2     5  John\n3     7  John\n\n'
"I want to use sklearn's StandardScaler. Is it possible to apply it to some feature columns but not others?\n\nFor instance, say my data is:\n\ndata = pd.DataFrame({'Name' : [3, 4,6], 'Age' : [18, 92,98], 'Weight' : [68, 59,49]})\n\n   Age  Name  Weight\n0   18     3      68\n1   92     4      59\n2   98     6      49\n\n\ncol_names = ['Name', 'Age', 'Weight']\nfeatures = data[col_names]\n\n\nI fit and transform the data\n\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\nscaled_features = pd.DataFrame(features, columns = col_names)\n\n       Name       Age    Weight\n0 -1.069045 -1.411004  1.202703\n1 -0.267261  0.623041  0.042954\n2  1.336306  0.787964 -1.245657\n\n\nBut of course the names are not really integers but strings and I don't want to standardize them. How can I apply the fit and transform methods only on the columns Age and Weight?\n"
"I am using Python3.5 and I am working with pandas. I have loaded stock data from yahoo finance and have saved the files to csv. My DataFrames load this data from the csv.  This is a copy of the ten rows of the csv file that is my DataFrame\n\n  Date       Open       High      Low     Close    Volume   Adj Close  \n1990-04-12  26.875000  26.875000  26.625  26.625      6100  250.576036\n1990-04-16  26.500000  26.750000  26.375  26.750       500  251.752449\n1990-04-17  26.750000  26.875000  26.750  26.875      2300  252.928863\n1990-04-18  26.875000  26.875000  26.500  26.625      3500  250.576036\n1990-04-19  26.500000  26.750000  26.500  26.750       700  251.752449\n1990-04-20  26.750000  26.875000  26.750  26.875      2100  252.928863\n1990-04-23  26.875000  26.875000  26.750  26.875       700  252.928863\n1990-04-24  27.000000  27.000000  26.000  26.000      2400  244.693970\n1990-04-25  25.250000  25.250000  24.875  25.125      9300  236.459076\n1990-04-26  25.000000  25.250000  24.750  25.000      1200  235.282663\n\n\nI know that I can use iloc, loc, ix but these values that I index will only give my specific rows and columns and will not perform the operation on every row.\nFor example: Row one of the data in the open column has a value of 26.875 and the row below it has 26.50. The price dropped .375 cents. I want to be able to capture the % of Increase or Decrease from the previous day so to finish this example .375 divided by 26.875 = 1.4% decrease from one day to the next. I want to be able to run this calculation on every row so I know how much it has increased or decreased from the previous day. The index functions I have tried but they are absolute, and I don't want to use a loop. Is there a way I can do this with the ix, iloc, loc or another function?\n"
'I have a dataframe and I try to get string, where on of column contain some string\nDf looks like\n\nmember_id,event_path,event_time,event_duration\n30595,"2016-03-30 12:27:33",yandex.ru/,1\n30595,"2016-03-30 12:31:42",yandex.ru/,0\n30595,"2016-03-30 12:31:43",yandex.ru/search/?lr=10738&amp;msid=22901.25826.1459330364.89548&amp;text=%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D1%8B+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&amp;suggest_reqid=168542624144922467267026838391360&amp;csg=3381%2C3938%2C2%2C3%2C1%2C0%2C0,0\n30595,"2016-03-30 12:31:44",yandex.ru/search/?lr=10738&amp;msid=22901.25826.1459330364.89548&amp;text=%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D1%8B+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&amp;suggest_reqid=168542624144922467267026838391360&amp;csg=3381%2C3938%2C2%2C3%2C1%2C0%2C0,0\n30595,"2016-03-30 12:31:45",yandex.ru/search/?lr=10738&amp;msid=22901.25826.1459330364.89548&amp;text=%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D1%8B+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&amp;suggest_reqid=168542624144922467267026838391360&amp;csg=3381%2C3938%2C2%2C3%2C1%2C0%2C0,0\n30595,"2016-03-30 12:31:46",yandex.ru/search/?lr=10738&amp;msid=22901.25826.1459330364.89548&amp;text=%D1%84%D0%B8%D0%BB%D1%8C%D0%BC%D1%8B+%D0%BE%D0%BD%D0%BB%D0%B0%D0%B9%D0%BD&amp;suggest_reqid=168542624144922467267026838391360&amp;csg=3381%2C3938%2C2%2C3%2C1%2C0%2C0,0\n30595,"2016-03-30 12:31:49",kinogo.co/,1\n30595,"2016-03-30 12:32:11",kinogo.co/melodramy/,0\n\n\nAnd another df with urls\n\nurl\n003\\.ru\\/[a-zA-Z0-9-_%$#?.:+=|()]+\\/mobilnyj_telefon_bq_phoenix\n003\\.ru\\/[a-zA-Z0-9-_%$#?.:+=|()]+\\/mobilnyj_telefon_fly_\n003\\.ru\\/sonyxperia\n003\\.ru\\/[a-zA-Z0-9-_%$#?.:+=|()]+\\/mobilnye_telefony_smartfony\n003\\.ru\\/[a-zA-Z0-9-_%$#?.:+=|()]+\\/mobilnye_telefony_smartfony\\/brands5D5Bbr_23\n1click\\.ru\\/sonyxperia\n1click\\.ru\\/[a-zA-Z0-9-_%$#?.:+=|()]+\\/chasy-motorola\n\n\nI use \n\nurls = pd.read_csv(\'relevant_url1.csv\', error_bad_lines=False)\nsubstr = urls.url.values.tolist()\ndata = pd.read_csv(\'data_nts2.csv\', error_bad_lines=False, chunksize=50000)\nresult = pd.DataFrame()\nfor i, df in enumerate(data):\n    res = df[df[\'event_time\'].str.contains(\'|\'.join(substr), regex=True)]\n\n\nbut it return me\n\nUserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n\n\nHow can I fix that?\n'
"I have an excel sheet with multiple header like:\n\n_________________________________________________________________________\n____|_____|        Header1    |        Header2     |        Header3      |\nColX|ColY |ColA|ColB|ColC|ColD||ColD|ColE|ColF|ColG||ColH|ColI|ColJ|ColDK|\n1   | ds  | 5  | 6  |9   |10  | .......................................\n2   | dh  |  ..........................................................\n3   | ge  |  ..........................................................\n4   | ew  |  ..........................................................\n5   | er  |  ..........................................................\n\n\nNow here you can see that first two columns do not have headers they are blank but other columns have headers like Header1, Header2 and Header3. So I want to read this sheet and merge it with other sheet with similar structure.\n\nI want to merge it on first column 'ColX'. Right now I am doing this:\n\nimport pandas as pd\n\ntotalMergedSheet = pd.DataFrame([1,2,3,4,5], columns=['ColX'])\nfile = pd.ExcelFile('ExcelFile.xlsx')\nfor i in range (1, len(file.sheet_names)):\n    df1 = file.parse(file.sheet_names[i-1])\n    df2 = file.parse(file.sheet_names[i])\n    newMergedSheet = pd.merge(df1, df2, on='ColX')\n    totalMergedSheet = pd.merge(totalMergedSheet, newMergedSheet, on='ColX')\n\n\nBut I don't know its neither reading columns correctly and I think will not return the results in the way I want. So, I want the resulting frame should be like:\n\n________________________________________________________________________________________________________\n____|_____|        Header1    |        Header2     |        Header3      |        Header4     |        Header5      |\nColX|ColY |ColA|ColB|ColC|ColD||ColD|ColE|ColF|ColG||ColH|ColI|ColJ|ColK| ColL|ColM|ColN|ColO||ColP|ColQ|ColR|ColS|\n1   | ds  | 5  | 6  |9   |10  | ..................................................................................\n2   | dh  |  ...................................................................................\n3   | ge  |  ....................................................................................\n4   | ew  |  ...................................................................................\n5   | er  |  ......................................................................................\n\n\nAny suggestions please. Thanks.\n"
"I am doing some statistical work using Python's pandas and I am having the following code to print out the data description (mean, count, median, etc).\n\ndata=pandas.read_csv(input_file)\nprint(data.describe())\n\n\nBut my data is pretty big (around 4 million rows) and each rows has very small data. So inevitably, the count would be big and the mean would be pretty small and thus Python print it like this.\n\n\n\nI just want to print these numbers entirely just for ease of use and understanding, for example it better be 4393476 instead of 4.393476e+06. I have googled it around and the most I can find is Display a float with two decimal places in Python and some other similar posts. But that will only work only if I have the numbers in a variable already. Not in my case though. In my case I haven't got those numbers. The numbers are created by the describe() function, so I don't know what numbers I will get.\n\nSorry if this seems like a very basic question, I am still new to Python. Any response is appreaciated. Thanks.\n"
'I have a dataframe df:\n\nid   name   count\n1    a       10\n2    b       20\n3    c       30\n4    d       40\n5    e       50\n\n\nHere I have another dataframe df2:\n\nid1  price   rating\n 1     100     1.0\n 2     200     2.0\n 3     300     3.0\n 5     500     5.0\n\n\nI want to join these two dataframes on column id and id1(both refer same). Here is an example of df3: \n\nid   name   count   price   rating\n1    a       10      100      1.0\n2    b       20      200      2.0\n3    c       30      300      3.0\n4    d       40      Nan      Nan\n5    e       50      500      5.0\n\n\nShould I use df.merge or pd.concat?\n'
'I have this data:\n\nID   TIME\n1    2\n1    4\n1    2\n2    3\n\n\nI want to group the data by ID and calculate the mean time and the size of each group.\n\nID   MEAN_TIME COUNT\n1    2.67      3\n2    3.00      1\n\n\nIf I run this code, then I get an error "ValueError: cannot insert ID, already exists":\n\nresult = df.groupby([\'ID\']).agg({\'TIME\': \'mean\', \'ID\': \'count\'}).reset_index()\n\n'
"When using groupby(), how can I create a DataFrame with a new column containing an index of the group number, similar to dplyr::group_indices in R.  For example, if I have\n\n&gt;&gt;&gt; df=pd.DataFrame({'a':[1,1,1,2,2,2],'b':[1,1,2,1,1,2]})\n&gt;&gt;&gt; df\n   a  b\n0  1  1\n1  1  1\n2  1  2\n3  2  1\n4  2  1\n5  2  2\n\n\nHow can I get a DataFrame like\n\n   a  b  idx\n0  1  1  1\n1  1  1  1\n2  1  2  2\n3  2  1  3\n4  2  1  3\n5  2  2  4\n\n\n(the order of the idx indexes doesn't matter)\n"
'I have the following Python pandas dataframe:\n\n     fruits | numFruits\n---------------------\n0  | apples |   10\n1  | grapes |   20\n2  |  figs  |   15\n\n\nI want:\n\n                 apples | grapes | figs\n-----------------------------------------\nMarket 1 Order |    10  |   20   |  15\n\n\nI have looked at pivot(), pivot_table(), Transpose and unstack() and none of them seem to give me this. Pandas newbie, so all help appreciated.\n'
"I have a dataFrame like this, I would like to group every 60 minutes and start grouping at 06:30.\n\n                           data\nindex\n2017-02-14 06:29:57    11198648\n2017-02-14 06:30:01    11198650\n2017-02-14 06:37:22    11198706\n2017-02-14 23:11:13    11207728\n2017-02-14 23:21:43    11207774\n2017-02-14 23:22:36    11207776\n\n\nI am using:\n\ndf.groupby(pd.TimeGrouper(freq='60Min'))\n\n\nI get this grouping:\n\n                      data\nindex       \n2017-02-14 06:00:00     x1\n2017-02-14 07:00:00     x2\n2017-02-14 08:00:00     x3\n2017-02-14 09:00:00     x4\n2017-02-14 10:00:00     x5\n\n\nbut I am looking for this result:\n\n                      data\nindex       \n2017-02-14 06:30:00     x1\n2017-02-14 07:30:00     x2\n2017-02-14 08:30:00     x3\n2017-02-14 09:30:00     x4\n2017-02-14 10:30:00     x5\n\n\nHow can I tell the function to start grouping at 6:30 at one-hour intervals?\n\nIf it can not be done by the .groupby(pd.TimeGrouper(freq='60Min')), how is the best way to do it?\n\nA salute and thanks very much in advance\n"
'I am trying to convert my output into a pandas data frame and I am struggling. I have this list\n\nmy_list = [1,2,3,4,5,6,7,8,9]\n\n\nI want to create a pandas data frame that would have 3 columns and three rows. I try using \n\ndf = pd.DataFrame(my_list, columns = list("abc"))\n\n\nbut it doesn\'t seem to be working for me. Any help would be appreciated.\n'
'There are countless questions about the dreaded SettingWithCopyWarning\n\nI\'ve got a good handle on how it comes about. (Notice I said good, not great)\n\nIt happens when a dataframe df is "attached" to another dataframe via an attribute stored in is_copy.\n\nHere\'s an example\n\ndf = pd.DataFrame([[1]])\n\nd1 = df[:]\n\nd1.is_copy\n\n&lt;weakref at 0x1115a4188; to \'DataFrame\' at 0x1119bb0f0&gt;\n\n\nWe can either set that attribute to None or\n\nd1 = d1.copy()\n\n\nI\'ve seen devs like @Jeff and I can\'t remember who else, warn about doing that.  Citing that the SettingWithCopyWarning has a purpose.\n\nQuestion\nOk, so what is a concrete example that demonstrates why ignoring the warning by assigning a copy back to the original is a bad idea.\n\nI\'ll define "bad idea" for clarification.\n\nBad Idea\nIt is a bad idea to place code into production that will lead to getting a phone call in the middle of a Saturday night saying your code is broken and needs to be fixed.\n\nNow how can using df = df.copy() in order to bypass the SettingWithCopyWarning lead to getting that kind of phone call.  I want it spelled out because this is a source of confusion and I\'m attempting to find clarity.  I want to see the edge case that blows up!\n'
'I am iterating over many exported security event logs pulled from a windows host, example dataframe like below:\n\n"MachineName","EventID","EntryType","Source","TimeGenerated","TimeWritten","UserName","Message"\n"mycompname","5156","SuccessAudit","Microsoft-Windows-Security-Auditing","4/26/2017 10:47:41 AM","4/26/2017 10:47:41 AM",,"The Windows Filtering Platform has permitted a connection.    Application Information:   Process ID:  4   Application Name: System    Network Information:   Direction:  %%14592   Source Address:  192.168.10.255   Source Port:  137   Destination Address: 192.168.10.238   Destination Port:  137   Protocol:  17    Filter Information:   Filter Run-Time ID: 83695   Layer Name:  %%14610   Layer Run-Time ID: 44"\n"mycompname","4688","SuccessAudit","Microsoft-Windows-Security-Auditing","4/26/2014 10:47:03 AM","4/26/2014 10:47:03 AM",,"A new process has been created.    Subject:   Security ID:  S-1-5-18   Account Name:  mycompname$   Account Domain:  mydomain   Logon ID:  0x3e7    Process Information:   New Process ID:  0x1b04   New Process Name: C:\\Windows\\SysWOW64\\Macromed\\Flash\\FlashPlayerUpdateService.exe   Token Elevation Type: %%1936   Creator Process ID: 0x300   Process Command Line: C:\\windows\\SysWOW64\\Macromed\\Flash\\FlashPlayerUpdateService.exe    Token Elevation Type indicates the type of token that was assigned to the new process in accordance with User Account Control policy.    Type 1 is a full token with no privileges removed or groups disabled.  A full token is only used if User Account Control is disabled or if the user is the built-in Administrator account or a service account.    Type 2 is an elevated token with no privileges removed or groups disabled.  An elevated token is used when User Account Control is enabled and the user chooses to start the program using Run as administrator.  An elevated token is also used when an application is configured to always require administrative privilege or to always require maximum privilege, and the user is a member of the Administrators group.    Type 3 is a limited token with administrative privileges removed and administrative groups disabled.  The limited token is used when User Account Control is enabled, the application does not require administrative privilege, and the user does not choose to start the program using Run as administrator."\n"mycompname","4673","SuccessAudit","Microsoft-Windows-Security-Auditing","4/26/2014 10:47:00 AM","4/26/2014 10:47:00 AM",,"A privileged service was called.    Subject:   Security ID:  S-1-5-18   Account Name:  mycompname$   Account Domain:  mydomain   Logon ID:  0x3e7    Service:   Server: NT Local Security Authority / Authentication Service   Service Name: LsaRegisterLogonProcess()    Process:   Process ID: 0x308   Process Name: C:\\Windows\\System32\\lsass.exe    Service Request Information:   Privileges:  SeTcbPrivilege"\n\n\nI am converting it to extract key:value pairs out of the "Message" Column and convert the keys to columns like below \n\ndef myfunc(folder):\n    file = \'\'.join(glob2.glob(folders + "\\\\*security*"))\n    df = pd.read_csv(file) \n    df.message = df.message.replace(["[ ]{6}", "[ ]{3}"],[","," ||"], regex=True)\n    message_results = df.message.str.extractall(r"\\|([^\\|]*?):(.*?)\\|").reset_index()\n    message_results.columns = ["org_index", "match", "keys", "vals"]\n    # PART THAT TAKES THE LONGEST\n    p = pd.pivot_table(message_results, values="vals", columns=[\'keys\'], index=["org_index"], aggfunc=np.sum)\n    df = df.join(p).fillna("NONE")\n\n\nOutput of above function:\n\nMachineName,EventID,EntryType,Source,TimeGenerated,TimeWritten,UserName,Message, Application Information, Filter Information, Network Information, Process, Process Information, Service, Service Request Information, Subject,Account Domain,Account Name,Application Name,Creator Process ID,Destination Address,Destination Port,Direction,Filter Run-Time ID,Layer Name,Logon ID,New Process ID,New Process Name,Process Command Line,Process ID,Process Name,Protocol,Security ID,Server,Service Name,Source Address,Source Port,Token Elevation Type\nmycompname,5156,SuccessAudit,Microsoft-Windows-Security-Auditing,4/26/2017 10:47:41 AM,4/26/2017 10:47:41 AM,NONE,The Windows Filtering Platform has permitted a connection. || Application Information: ||Process ID:  4 ||Application Name: System || Network Information: ||Direction:  %%14592 ||Source Address:  192.168.10.255 ||Source Port:  137 ||Destination Address: 192.168.10.238 ||Destination Port:  137 ||Protocol:  17 || Filter Information: ||Filter Run-Time ID: 83695 ||Layer Name:  %%14610 ||Layer Run-Time ID: 44, , , ,NONE,NONE,NONE,NONE,NONE,NONE,NONE, System ,NONE, 192.168.10.238 ,  137 ,  %%14592 , 83695 ,  %%14610 ,NONE,NONE,NONE,NONE,  4 ,NONE,  17 ,NONE,NONE,NONE,  192.168.10.255 ,  137 ,NONE\nmycompname,4688,SuccessAudit,Microsoft-Windows-Security-Auditing,4/26/2017 10:47:03 AM,4/26/2017 10:47:03 AM,NONE,"A new process has been created. || Subject: ||Security ID:  S-1-5-18 ||Account Name:  mycompname$ ||Account Domain:  mydomain ||Logon ID:  0x3e7 || Process Information: ||New Process ID:  0x1b04 ||New Process Name: C:\\Windows\\SysWOW64\\Macromed\\Flash\\FlashPlayerUpdateService.exe ||Token Elevation Type: %%1936 ||Creator Process ID: 0x300 ||Process Command Line: C:\\windows\\SysWOW64\\Macromed\\Flash\\FlashPlayerUpdateService.exe || Token Elevation Type indicates the type of token that was assigned to the new process in accordance with User Account Control policy. || Type 1 is a full token with no privileges removed or groups disabled.  A full token is only used if User Account Control is disabled or if the user is the built-in Administrator account or a service account. || Type 2 is an elevated token with no privileges removed or groups disabled.  An elevated token is used when User Account Control is enabled and the user chooses to start the program using Run as administrator.  An elevated token is also used when an application is configured to always require administrative privilege or to always require maximum privilege, and the user is a member of the Administrators group. || Type 3 is a limited token with administrative privileges removed and administrative groups disabled.  The limited token is used when User Account Control is enabled, the application does not require administrative privilege, and the user does not choose to start the program using Run as administrator.",NONE,NONE,NONE,NONE, ,NONE,NONE, ,  mydomain ,  MEADWK4216DC190$ ,NONE, 0x300 ,NONE,NONE,NONE,NONE,NONE,  0x3e7 ,  0x1b04 , C:\\Windows\\SysWOW64\\Macromed\\Flash\\FlashPlayerUpdateService.exe , C:\\windows\\SysWOW64\\Macromed\\Flash\\FlashPlayerUpdateService.exe ,NONE,NONE,NONE,  S-1-5-18 ,NONE,NONE,NONE,NONE, %%1936 \nmycompname,4673,SuccessAudit,Microsoft-Windows-Security-Auditing,4/26/2017 10:47:00 AM,4/26/2017 10:47:00 AM,NONE,A privileged service was called. || Subject: ||Security ID:  S-1-5-18 ||Account Name:  mycompname$ ||Account Domain:  mydomain ||Logon ID:  0x3e7 || Service: ||Server: NT Local Security Authority / Authentication Service ||Service Name: LsaRegisterLogonProcess() || Process: ||Process ID: 0x308 ||Process Name: C:\\Windows\\System32\\lsass.exe || Service Request Information: ||Privileges:  SeTcbPrivilege,NONE,NONE,NONE, ,NONE, , , ,  mydomain ,  mycompname$ ,NONE,NONE,NONE,NONE,NONE,NONE,NONE,  0x3e7 ,NONE,NONE,NONE, 0x308 , C:\\Windows\\System32\\lsass.exe ,NONE,  S-1-5-18 , NT Local Security Authority / Authentication Service , LsaRegisterLogonProcess() ,NONE,NONE,NONE\n\n\nThe functionality of the program works but is incredibly slow on the p = pivot_table portion of code on larger sets of data (roughly 150000 lines). \n\nI am currently using concurrent.futures.ThreadPoolExecutor(maxworkers=1000) iterating over each reading of the file like below: \n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=1000) as pool:\n    for folder in path:\n        if os.path.isdir(folder):\n            try:\n                print(folder)\n                pool.submit(myfunc(folder), 1000)\n            except:\n                print(\'error\') \n\n\nHow can I speed up my the pivot table portion of my function?\n\nAlso, is there any method to speed up the pivot_table call from pandas? \n\nAny assistance with this would be greatly appreciated. Thank you. \n'
"df =\n\nCol1 Col2 Col3\n1    nan  4\n2    5    4\n3    3    nan\n\n\nGiven the dataframe df, I want to obtain a new dataframe df2 that does not contain nan in the column Col2. This is the expected result:\n    df2 =\n\nCol1 Col2 Col3\n2    5    4\n3    3    nan\n\n\nI know that it's possible to use pandas.isnull and dropna, however how to specify only particular column to which filtering should be applied?\n"
'I use Pandas a lot and its great. I use TimeGrouper as well, and its great. I actually dont know where is the documentation about TimeGrouper. Is there any?\n\nThanks!\n'
"I'm trying to run what I think is simple code to eliminate any columns with all NaNs, but can't get this to work (axis = 1 works just fine when eliminating rows):\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'a':[1,2,np.nan,np.nan], 'b':[4,np.nan,6,np.nan], 'c':[np.nan, 8,9,np.nan], 'd':[np.nan,np.nan,np.nan,np.nan]})\n\ndf = df[df.notnull().any(axis = 0)]\n\nprint df\n\n\nFull error:\n\nraise IndexingError('Unalignable boolean Series provided as 'pandas.core.indexing.IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match\n\nExpected output:\n\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  NaN  8.0\n2  NaN  6.0  9.0\n3  NaN  NaN  NaN\n\n"
'I am new on pandas and for now i don\'t get how to arrange my time serie, take a look at it :\n\ndate &amp; time of connection\n19/06/2017 12:39\n19/06/2017 12:40\n19/06/2017 13:11\n20/06/2017 12:02\n20/06/2017 12:04\n21/06/2017 09:32\n21/06/2017 18:23\n21/06/2017 18:51\n21/06/2017 19:08\n21/06/2017 19:50\n22/06/2017 13:22\n22/06/2017 13:41\n22/06/2017 18:01\n23/06/2017 16:18\n23/06/2017 17:00\n23/06/2017 19:25\n23/06/2017 20:58\n23/06/2017 21:03\n23/06/2017 21:05\n\n\nThis is a sample of a dataset of 130 k raws,I tried :\ndf.groupby(\'date &amp; time of connection\')[\'date &amp; time of connection\'].apply(list)\n\nNot enough i guess\n\nI think i should :\n\n\nCreate a dictionnary with index from dd/mm/yyyy to dd/mm/yyyy \nConvert "date &amp; time of connection" type dateTime to Date\nGroup and count Date of "date &amp; time of connection"\nPut the numbers i count inside the dictionary ?\n\n\nWhat do you think about my logic ? Do you know some tutos ?\nThank you very much\n'
"consider a data frame defined like so:\nimport pandas as pd\ntest = pd.DataFrame({\n    'id' : ['a', 'b', 'c', 'd'],\n    'times' : [2, 3, 1, 5]\n})\n\nIs it possible to create a new data frame from this in which each row is repeated times times, such that the result looks like this:\n&gt;&gt;&gt; result\n   id  times\n0   a      2\n1   a      2\n2   b      3\n3   b      3\n4   b      3\n5   c      1\n6   d      5\n7   d      5\n8   d      5\n9   d      5\n10  d      5\n\n"
'I have a pandas series features that has the following values (features.values)\n\narray([array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n       array([0, 0, 0, ..., 0, 0, 0]), ...,\n       array([0, 0, 0, ..., 0, 0, 0]), array([0, 0, 0, ..., 0, 0, 0]),\n       array([0, 0, 0, ..., 0, 0, 0])], dtype=object)\n\n\nNow I really want this to be recognized as matrix, but if I do\n\n&gt;&gt;&gt; features.values.shape\n(10000,)\n\n\nrather than (10000, 3000) which is what I would expect.\n\nHow can I get this to be recognized as 2d rather than a 1d array with arrays as values. Also why does it not automatically detect it as a 2d array?\n'
"Most of the Numpy's function will enable multithreading by default.\n\nfor example, I work on a 8-cores intel cpu workstation, if I run a script\n\nimport numpy as np    \nx=np.random.random(1000000)\nfor i in range(100000):\n    np.sqrt(x)\n\n\nthe linux top will show 800% cpu usage during running like\n \nWhich means numpy automatically detects that my workstation has 8 cores, and np.sqrt automatically use all 8 cores to accelerate computation.\n\nHowever, I found a weird bug. If I run a script\n\nimport numpy as np\nimport pandas as pd\ndf=pd.DataFrame(np.random.random((10,10)))\ndf+df\nx=np.random.random(1000000)\nfor i in range(100000):\n    np.sqrt(x)\n\n\nthe cpu usage is 100%!!.\n It means that if you plus two pandas DataFrame before running any numpy function, the auto multithreading feature of numpy is gone without any warning! This is absolutely not reasonable, why would Pandas dataFrame calculation affect Numpy threading setting? Is it a bug? How to work around this?\n\n\n\nPS:\n\nI dig further using Linux perf tool.\n\nrunning first script shows\n\n\n\nWhile running second script shows\n\n\n\nSo both script involves libmkl_vml_avx2.so, while the first script involves additional libiomp5.so which seems to be related to openMP.\n\nAnd since vml means intel vector math library, so according to vml doc I guess at least below functions are all automatically multithreaded\n\n\n"
"I have a pandas dataframe where one column is a bunch of strings with certain travel details. My goal is to parse each string to extract the city of origin and destination city (I would like to ultimately have two new columns titled 'origin' and 'destination').\n\nThe data:\n\ndf_col = [\n    'new york to venice, italy for usd271',\n    'return flights from brussels to bangkok with etihad from â‚¬407',\n    'from los angeles to guadalajara, mexico for usd191',\n    'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags'\n]\n\n\nThis should result in:\n\nOrigin: New York, USA; Destination: Venice, Italy\nOrigin: Brussels, BEL; Destination: Bangkok, Thailand\nOrigin: Los Angeles, USA; Destination: Guadalajara, Mexico\nOrigin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)\n\n\nThus far I have tried:\nA variety of NLTK methods, but what has gotten me closest is using the nltk.pos_tag method to tag each word in the string. The result is a list of tuples with each word and associated tag. Here's an example...\n\n[('Fly', 'NNP'), ('to', 'TO'), ('Australia', 'NNP'), ('&amp;', 'CC'), ('New', 'NNP'), ('Zealand', 'NNP'), ('from', 'IN'), ('Paris', 'NNP'), ('from', 'IN'), ('â‚¬422', 'NNP'), ('return', 'NN'), ('including', 'VBG'), ('2', 'CD'), ('checked', 'VBD'), ('bags', 'NNS'), ('!', '.')]\n\n\nI am stuck at this stage and am unsure how to best implement this. Can anyone point me in the right direction, please? Thanks.\n"
"I stumbled across pandas and it looks ideal for simple calculations that I'd like to do. I have a SAS background and was thinking it'd replace proc freq -- it looks like it'll scale to what I may want to do in the future. However, I just can't seem to get my head around a simple task (I'm not sure if I'm supposed to look at pivot/crosstab/indexing - whether I should have a Panel or DataFrames etc...). Could someone give me some pointers on how to do the following:\n\nI have two CSV files (one for year 2010, one for year 2011 - simple transactional data) - The columns are category and amount\n\n2010:\n\nAB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n\n\n2011:\n\nAB,500.00\nAC,250.00\nAX,900.00\n\n\nThese are loaded into separate DataFrame objects.\n\nWhat I'd like to do is get the category, the sum of the category, and the frequency of the category, eg:\n\n2010:\n\nAB,300.00,2\nAC,150.00,1\nAD,500.00,1\n\n\n2011:\n\nAB,500.00,1\nAC,250.00,1\nAX,900.00,1\n\n\nI can't work out whether I should be using pivot/crosstab/groupby/an index\netc... I can get either the sum or the frequency - I can't seem to get both... It gets a bit more complex because I would like to do it on a month by month basis, but I think if someone would be so kind to point me to the right technique/direction I'll be able to go from there.\n"
"&gt;&gt;&gt; df =DataFrame({'a':[1,2,3,4],'b':[2,4,6,8]})\n&gt;&gt;&gt; df['x']=df.a + df.b\n&gt;&gt;&gt; df['y']=df.a - df.b\n&gt;&gt;&gt; df\n   a  b   x  y\n0  1  2   3 -1\n1  2  4   6 -2\n2  3  6   9 -3\n3  4  8  12 -4\n\n\nNow I want to rearrange the column sequence, which makes 'x','y' column to be the first &amp; second columns by :\n\n&gt;&gt;&gt; df = df[['x','y','a','b']]\n&gt;&gt;&gt; df\n    x  y  a  b\n0   3 -1  1  2\n1   6 -2  2  4\n2   9 -3  3  6\n3  12 -4  4  8\n\n\nBut if I have a long coulmns 'a','b','c','d'....., and I don't want to explictly list the columns. How can I do that ?\n\nOr Does Pandas provide a function like set_column_sequence(dataframe,col_name, seq) so that I can do  :  set_column_sequence(df,'x',0) and set_column_sequence(df,'y',1) ?\n"
'I\'m looking to decrease density of tick labels on differing subplot\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom StringIO import StringIO\ndata = """\\\n    a   b   c   d\nz   54.65   6.27    19.53   4.54\nw   -1.27   4.41    11.74   3.06\nd   5.51    3.39    22.98   2.29\nt   76284.53    -0.20   28394.93    0.28\n"""\ndf = pd.read_csv(StringIO(data), sep=\'\\s+\')\ngs = gridspec.GridSpec(3, 1,height_ratios=[1,1,4] )\nax0 = plt.subplot(gs[0])\nax1 = plt.subplot(gs[1])\nax2 = plt.subplot(gs[2])\ndf.plot(kind=\'bar\', ax=ax0,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax1,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'))\ndf.plot(kind=\'bar\', ax=ax2,color=(\'Blue\',\'DeepSkyBlue\',\'Red\',\'DarkOrange\'),rot=45)\nax0.set_ylim(69998, 78000)\nax1.set_ylim(19998, 29998)\nax2.set_ylim(-2, 28)\nax0.legend().set_visible(False)\nax1.legend().set_visible(False)\nax2.legend().set_visible(False)\nax0.spines[\'bottom\'].set_visible(False)\nax1.spines[\'bottom\'].set_visible(False)\nax1.spines[\'top\'].set_visible(False)\nax2.spines[\'top\'].set_visible(False)\nax0.xaxis.set_ticks_position(\'none\')\nax1.xaxis.set_ticks_position(\'none\')\nax0.xaxis.set_label_position(\'top\')\nax1.xaxis.set_label_position(\'top\')\nax0.tick_params(labeltop=\'off\')\nax1.tick_params(labeltop=\'off\', pad=15)\nax2.tick_params(pad=15)\nax2.xaxis.tick_bottom()\nd = .015\nkwargs = dict(transform=ax0.transAxes, color=\'k\', clip_on=False)\nax0.plot((-d,+d),(-d,+d), **kwargs)\nax0.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax1.transAxes)\nax1.plot((-d,+d),(1-d,1+d), **kwargs)\nax1.plot((1-d,1+d),(1-d,1+d), **kwargs)\nax1.plot((-d,+d),(-d,+d), **kwargs)\nax1.plot((1-d,1+d),(-d,+d), **kwargs)\nkwargs.update(transform=ax2.transAxes)\nax1.plot((-d,+d),(1-d/4,1+d/4), **kwargs)\nax1.plot((1-d,1+d),(1-d/4,1+d/4), **kwargs)\nplt.show()\n\n\nwhich results in \n\n\nI would like to decrease tick labels in the two upper subplots. How to do that ? Thanks.\n\nBonus: 1) how to get rid of the dotted line on y=0 at the basis of the bars?\n2) how to get rid of x-trick label between subplot 0 and 1?\n3) how to set the back of the plot to transparency? (see the right-bottom broken y-axis line that disappears behind the back of the plot)\n'
"I realize Dataframe takes a map of {'series_name':Series(data, index)}.  However, it automatically sorts that map even if the map is an OrderedDict().\n\nIs there a simple way to pass a list of Series(data, index, name=name) such that the order is preserved and the column names are the series.name?  Is there an easy way if all the indices are the same for all the series? \n\nI normally do this by just passing a numpy column_stack of series.values and specifying the column names.  However, this is ugly and in this particular case the data is strings not floats.\n"
'Pandas has the following examples for how to store Series, DataFrames and Panelsin HDF5 files:\n\nPrepare some data:\n\nIn [1142]: store = HDFStore(\'store.h5\')\n\nIn [1143]: index = date_range(\'1/1/2000\', periods=8)\n\nIn [1144]: s = Series(randn(5), index=[\'a\', \'b\', \'c\', \'d\', \'e\'])\n\nIn [1145]: df = DataFrame(randn(8, 3), index=index,\n   ......:                columns=[\'A\', \'B\', \'C\'])\n   ......:\n\nIn [1146]: wp = Panel(randn(2, 5, 4), items=[\'Item1\', \'Item2\'],\n   ......:            major_axis=date_range(\'1/1/2000\', periods=5),\n   ......:            minor_axis=[\'A\', \'B\', \'C\', \'D\'])\n   ......:\n\n\nSave it in a store:\n\nIn [1147]: store[\'s\'] = s\n\nIn [1148]: store[\'df\'] = df\n\nIn [1149]: store[\'wp\'] = wp\n\n\nInspect what\'s in the store:\n\nIn [1150]: store\nOut[1150]: \n&lt;class \'pandas.io.pytables.HDFStore\'&gt;\nFile path: store.h5\n/df            frame        (shape-&gt;[8,3])  \n/s             series       (shape-&gt;[5])    \n/wp            wide         (shape-&gt;[2,5,4])\n\n\nClose the store:\n\nIn [1151]: store.close()\n\n\nQuestions:\n\n\nIn the code above, when is the data actually written to disk? \nSay I want to add thousands of large dataframes living in .csv files to a single .h5 file. I would need to load them and add them to the .h5 file one by one since I cannot afford to have them all in memory at once as they would take too much memory. Is this possible with HDF5? What would be the correct way to do it?\nThe Pandas documentation says the following: \n\n\n  "These stores are not appendable once written (though you simply\n  remove them and rewrite). Nor are they queryable; they must be\n  retrieved in their entirety."\n\n\nWhat does it mean by not appendable nor queryable? Also, shouldn\'t it say once closed instead of written?\n\n'
"how do I import excel data into a dataframe in python.\n\nBasically the current excel workbook runs some vba on opening which refreshes a pivot table and does some other stuff.\n\nThen I wish to import the results of the pivot table refresh into a dataframe in python for further analysis.\n\nimport xlrd\n\nwb = xlrd.open_workbook('C:\\Users\\cb\\Machine_Learning\\cMap_Joins.xlsm')\n\n#sheetnames\nprint wb.sheet_names()\n\n#number of sheets\nprint wb.nsheets\n\n\nThe refreshing and opening of the file works fine. But how do i select the data from the first sheet from say row 5 including header down to last record n.\n"
'I want to count number of occurrences of certain words in a data frame. I know using "str.contains"\n\na = df2[df2[\'col1\'].str.contains("sample")].groupby(\'col2\').size()\nn = a.apply(lambda x: 1).sum()\n\n\nCurrently I\'m using the above code. Is there a method to match regular expression and get the count of occurrences? In my case I have a large dataframe and I want to match around 100 strings.\n'
'I have a Pandas DataFrame like following:\n\n               A              B              C\n0   192.168.2.85   192.168.2.85  124.43.113.22\n1  192.248.8.183  192.248.8.183   192.168.2.85\n2  192.168.2.161            NaN  192.248.8.183\n3   66.249.74.52            NaN  192.168.2.161\n4            NaN            NaN   66.249.74.52\n\n\nI want to get the count of a certain values across columns. So my expected output is something like:\n\nIP          Count\n192.168.2.85 3 #Since this value is there in all coulmns\n192.248.8.183 3\n192.168.2.161 2\n66.249.74.52 2\n124.43.113.22 1\n\n\nI know how to this across rows, but doing this for columns is bit strange?Help me to solve this? Thanks.\n'
"Ok so I have a dataframe which contains timeseries data that has a multiline index for each columns. Here is a sample of what the data looks like and it is in csv format. Loading the data is not an issue here.\n\n \n\nWhat I want to do is to be able to create a boxplot with this data grouped according to different catagories in a specific line of the multiindex. For example if I were to group by 'SPECIES' I would have the groups, 'aq', 'gr', 'mix', 'sed' and a box for each group at a specific time in the timeseries.\n\nI've tried this:\n\ngrouped = data['2013-08-17'].groupby(axis=1, level='SPECIES')\ngrouped.boxplot()\n\n\nbut it gives me a boxplot (flat line) for each point in the group rather than for the grouped set. Is there an easy way to do this? I don't have any problems grouping as I can aggregate the groups any which way I want, but I can't get them to boxplot.\n"
"I have a pandas dataframe and would like to drop all columns save the index and a column named 'bob'\n\nHow would I do this?\n"
"I'm starting to tear my hair out with this - so I hope someone can help. I have a pandas DataFrame that was created from an Excel spreadsheet using openpyxl. The resulting DataFrame looks like:\n\nprint image_name_data\n     id           image_name\n0  1001  1001_mar2014_report\n1  1002  1002_mar2014_report\n2  1003  1003_mar2014_report\n\n[3 rows x 2 columns]\n\n\n…with the following datatypes:\n\nprint image_name_data.dtypes\nid            float64\nimage_name     object\ndtype: object\n\n\nThe issue is that the numbers in the id column are, in fact, identification numbers and I need to treat them as strings. I've tried converting the id column to strings using:\n\nimage_name_data['id'] = image_name_data['id'].astype('str')\n\n\nThis seems a bit ugly but it does produce a variable of type 'object' rather than 'float64':\n\nprint image_name_data.dyptes\nid            object\nimage_name    object\ndtype: object\n\n\nHowever, the strings that are created have a decimal point, as shown:\n\nprint image_name_data\n       id           image_name\n0  1001.0  1001_mar2014_report\n1  1002.0  1002_mar2014_report\n2  1003.0  1003_mar2014_report\n\n[3 rows x 2 columns]\n\n\nHow can I convert a float64 column in a pandas DataFrame to a string with a given format (in this case, for example, '%10.0f')?\n"
"When drawing a pandas boxplot, grouped by another column, pandas automatically adds a title to the plot, saying 'Boxplot grouped by....'. Is there a way to remove that? I tried using \n\nsuptitle('')\n\n\nas per Pandas: boxplot of one column based on another column\n\nbut this does not seem to work. I am using latest pandas (0.13.1) version.\n"
"I have a fixed-width data file containing dates, but when I try to plot the data the dates are not displayed properly on the x-axis.\n\nMy files looks like\n\n2014-07-10 11:49:14.377102    45\n2014-07-10 11:50:14.449150    45\n2014-07-10 11:51:14.521168    21\n2014-07-10 11:52:14.574241     8\n2014-07-10 11:53:14.646137    11\n2014-07-10 11:54:14.717688    14\n\n\netc\n\nand I use pandas to read in the file\n\n#! /usr/bin/env python\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_fwf('myfile.log',header=None,names=['time','amount'],widths=[27,5])\ndata.time = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S.%f')\nplt.plot(data.time,data.amount)\nplt.show()\n\n\nSo I suppose the issue here is conversion from pandas to matplotlib datetime, How would one do a conversion?\n\nI also tried with pandas directly:\n\ndata.time = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S.%f')\ndata.set_index('time') # Fails!!\ndata.time.plot()\n\n\nbut this fails with\n\n\n  TypeError: Empty 'Series': no numeric data to plot\n\n"
"If I have two lists \n\nl1 = [ 'A', 'B' ]\n\nl2 = [ 1, 2 ]\n\n\nwhat is the most elegant way to get a pandas data frame which looks like:\n\n+-----+-----+-----+\n|     | l1  | l2  |\n+-----+-----+-----+\n|  0  | A   | 1   |\n+-----+-----+-----+\n|  1  | A   | 2   |\n+-----+-----+-----+\n|  2  | B   | 1   |\n+-----+-----+-----+\n|  3  | B   | 2   |\n+-----+-----+-----+\n\n\nNote, the first column is the index.\n"
'fig = plt.figure()\nax = fig.gca()\nts.plot(ax=ax)\n\n\nI know I can set xlim inside pandas plotting routine: ts.plot(xlim = ...), but how to change it after pandas plotting is done?\n\nax.set_xlim(( t0.toordinal(), t1.toordinal() )\n\n\nworks sometimes, but if pandas is formatting the xaxis as months from epoch, not days, this will fail hard.\n\nIs there anyway to know how pandas has converted the dates to xaxis and then convert my xlim in the same way?\n\nThanks.\n'
"I have a problem with reading CSV(or txt file) on pandas module\nBecause numpy's loadtxt function takes too much time, I decided to use pandas read_csv instead.\n\nI want to make a numpy array from txt file with four columns separated by space, and has very large number of rows (like, 256^3. In this example, it is 64^3).\n\nThe problem is that I don't know why but it seems that pandas's read_csv always skips the first line (first row) of the csv (txt) file, resulting one less data.\n\nhere is the code.\n\nfrom __future__ import division\nimport numpy as np\nimport pandas as pd\nngridx = 4\nngridy = 4\nngridz = 4\nsize = ngridx*ngridy*ngridz\nf = np.zeros((size,4))\na = np.arange(size)\nf[:, 0] = np.floor_divide(a, ngridy*ngridz)\nf[:, 1] = np.fmod(np.floor_divide(a, ngridz), ngridy)\nf[:, 2] = np.fmod(a, ngridz)\nf[:, 3] = np.random.rand(size)\nprint f[0]\nnp.savetxt('Testarray.txt',f,fmt='%6.16f')\ng = pd.read_csv('Testarray.txt',delimiter=' ').values\nprint g[0]\nprint len(g[:,3])\n\n\nf[0] and g[0] that are displayed in the output have to match but it doesn't, indicating that pandas is skipping the first line of the Testarray.txt.\nAlso, length of loaded file g is less than the length of the array f.\n\nI need help.\n\nThanks in advance.\n"
'Suppose I have a pandas data frame surveyData:\n\nI want to normalize the data in each column by performing:\n\nsurveyData_norm = (surveyData - surveyData.mean()) / (surveyData.max() - surveyData.min())\n\n\nThis would work fine if my data table only contained the columns I wanted to normalize. However, I have some columns containing string data preceding like:\n\nName  State  Gender  Age  Income  Height\nSam   CA     M        13   10000    70\nBob   AZ     M        21   25000    55\nTom   FL     M        30   100000   45\n\n\nI only want to normalize the Age, Income, and Height columns but my above method does not work becuase of the string data in the name state and gender columns.\n'
"I would like to save a python list in a .csv file, for example I have a list like this:\n\n['hello','how','are','you']\n\n\nI would like to save it like this:\n\ncolummn,\nhello,\nhow,\nare,\nyou,\n\n\nI tried the following:\n\nmyfile = open('/Users/user/Projects/list.csv', 'wb')\nwr = csv.writer(myfile, quoting=csv.QUOTE_ALL,'\\n')\nwr.writerow(pos_score)\n\n"
"I cannot find a pandas function (which I had seen before) to substitute the NaN's in a dataframe with values from another dataframe (assuming a common index which can be specified). Any help?\n"
"I have an adjacency matrix stored as a pandas.DataFrame:\n\nnode_names = ['A', 'B', 'C']\na = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],\n    index=node_names, columns=node_names)\na_numpy = a.as_matrix()\n\n\nI'd like to create an igraph.Graph from either the pandas or the numpy adjacency matrices. In an ideal world the nodes would be named as expected.\n\nIs this possible? The tutorial seems to be silent on the issue.\n"
'Given a dataframe that looks like this:\n\n            A   B      \n2005-09-06  5  -2  \n2005-09-07 -1   3  \n2005-09-08  4   5 \n2005-09-09 -8   2\n2005-09-10 -2  -5\n2005-09-11 -7   9 \n2005-09-12  2   8  \n2005-09-13  6  -5  \n2005-09-14  6  -5  \n\n\nIs there a pythonic way to create a 2x2 matrix like this:\n\n    1  0\n 1  a  b\n 0  c  d\n\n\nWhere:\n\na = number of obs where the corresponding elements of column A and B are both positive.\n\nb = number of obs where the corresponding elements of column A are positive and negative in column B.\n\nc = number of obs where the corresponding elements of column A are negative and positive in column B.\n\nd = number of obs where the corresponding elements of column A and B are both negative.\n\nFor this example the output would be:\n\n    1  0\n 1  2  3\n 0  3  1\n\n\nThanks\n'
'I was trying to split the sample dataset using Scikit-learn\'s Stratified Shuffle Split. I followed the example shown on the Scikit-learn documentation here  \n\nimport pandas as pd\nimport numpy as np\n# UCI\'s wine dataset\nwine = pd.read_csv("https://s3.amazonaws.com/demo-datasets/wine.csv")\n\n# separate target variable from dataset\ntarget = wine[\'quality\']\ndata = wine.drop(\'quality\',axis = 1)\n\n# Stratified Split of train and test data\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(target, n_iter=3, test_size=0.2)\n\nfor train_index, test_index in sss:\n    xtrain, xtest = data[train_index], data[test_index]\n    ytrain, ytest = target[train_index], target[test_index]\n\n# Check target series for distribution of classes\nytrain.value_counts()\nytest.value_counts()\n\n\nHowever, upon running this script, I get the following error:\n\nIndexError: indices are out-of-bounds\n\n\nCould someone please point out what I am doing wrong here? Thanks!\n'
'For pandas, I\'m looking for a way to write conditional values to each row in column B, based on substrings for corresponding rows in column A.\n\nSo if cell in A contains "BULL", write "Long" to B. Or if cell in A contains "BEAR", write "Short" to B.\n\nDesired output:\n\nA                  B\n"BULL APPLE X5"    "Long"\n"BEAR APPLE X5"    "Short"\n"BULL APPLE X5"    "Long"\n\n\nB is initially empty: df = pd.DataFrame([[\'BULL APPLE X5\',\'\'],[\'BEAR APPLE X5\',\'\'],[\'BULL APPLE X5\',\'\']],columns=[\'A\',\'B\'])\n'
'I have a dataframe that looks like\n\ndf\n\nviz  a1_count  a1_mean     a1_std\nn         3        2   0.816497\ny         0      NaN        NaN \nn         2       51  50.000000\n\n\nI want to convert the "viz" column to 0 and 1, based on a conditional. I\'ve tried: \n\ndf[\'viz\'] = 0 if df[\'viz\'] == "n" else 1\n\n\nbut I get:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n'
'I\'m just trying to do a simple RandomForestRegressor example. But while testing the accuracy I get this error \n\n\n/Users/noppanit/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\n\n  \n  in accuracy_score(y_true, y_pred, normalize, sample_weight)\n          177 \n          178     # Compute accuracy for each possible representation\n      --> 179     y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n          180     if y_type.startswith(\'multilabel\'):\n          181         differing_labels = count_nonzero(y_true - y_pred, axis=1)\n\n/Users/noppanit/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\n\n  \n  in _check_targets(y_true, y_pred)\n           90     if (y_type not in ["binary", "multiclass", "multilabel-indicator",\n           91                        "multilabel-sequences"]):\n      ---> 92         raise ValueError("{0} is not supported".format(y_type))\n           93 \n           94     if y_type in ["binary", "multiclass"]:\n\nValueError: continuous is not supported\n\n\n\nThis is the sample of the data. I can\'t show the real data. \n\ntarget, func_1, func_2, func_2, ... func_200\nfloat, float, float, float, ... float\n\n\nHere\'s my code.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\n\ntrain = pd.read_csv(\'data.txt\', sep=\'\\t\')\n\nlabels = train.target\ntrain.drop(\'target\', axis=1, inplace=True)\ncat = [\'cat\']\ntrain_cat = pd.get_dummies(train[cat])\n\ntrain.drop(train[cat], axis=1, inplace=True)\ntrain = np.hstack((train, train_cat))\n\nimp = Imputer(missing_values=\'NaN\', strategy=\'mean\', axis=0)\nimp.fit(train)\ntrain = imp.transform(train)\n\nx_train, x_test, y_train, y_test = train_test_split(train, labels.values, test_size = 0.2)\n\nclf = RandomForestRegressor(n_estimators=10)\n\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\naccuracy_score(y_test, y_pred) # This is where I get the error.\n\n'
'I have the following dataframe:\n\nindex = range(14)\ndata = [1, 0, 0, 2, 0, 4, 6, 8, 0, 0, 0, 0, 2, 1]\ndf = pd.DataFrame(data=data, index=index, columns = [\'A\'])\n\n\nHow can I fill the zeros with the previous non-zero value using pandas? Is there a fillna that is not just for "NaN"?.  \n\nThe output should look like:\n\n[1, 1, 1, 2, 2, 4, 6, 8, 8, 8, 8, 8, 2, 1]\n\n\n(This question was asked before here Fill zero values of 1d numpy array with last non-zero values but he was asking exclusively for a numpy solution)\n'
'No offence, if the questions is too basic. Let me know if you need more information.\n\nI am looking for an idea to convert square-form tuple of tuples to pandas.DataFrame in a clean/efficient/pythonic way,\ni.e. from \n\ns =((1,0,0,0,),(2,3,0,0,),(4,5,6,0,),(7,8,9,10,))\n\n\nto pandas.DataFrame like\n\n   1  2  3   4\n1  1  0  0   0\n2  2  3  0   0\n3  4  5  6   0\n4  7  8  9  10\n\n\nNaturally, this list can grow with more zeros appended in the upper-triangular (if we think of s as a tuple of rows).\n\nDataFrame(t) seems to fail.\n'
"I want to import OData XML datafeeds from the Dutch Bureau of Statistics (CBS) into our database. Using lxml and pandas I thought this should be straigtforward. By using OrderDict I want to preserve the order of the columns for readability, but somehow I can't get it right.\n\nfrom collections import OrderedDict\nfrom lxml import etree\nimport requests\nimport pandas as pd\n\n\n# CBS URLs\nbase_url = 'http://opendata.cbs.nl/ODataFeed/odata'\ndatasets = ['/37296ned', '/82245NED']\n\nfeed = requests.get(base_url + datasets[1] + '/TypedDataSet')\nroot = etree.fromstring(feed.content)\n\n# all record entries start at tag m:properties, parse into data dict\ndata = []\nfor record in root.iter('{{{}}}properties'.format(root.nsmap['m'])):\n    row = OrderedDict()\n    for element in record:\n        row[element.tag.split('}')[1]] = element.text\n    data.append(row)\n\ndf = pd.DataFrame.from_dict(data)\ndf.columns\n\n\nInspecting data, the OrderDict is in the right order. But looking at df.head() the columns have been sorted alphabetically with CAPS first?\n\nHelp, anyone?\n"
'I\'m new to python and I began to teach myself how to use pandas on jupyter using the exercise from this link:\n\nhttp://nbviewer.jupyter.org/github/jvns/pandas-cookbook/blob/v0.1/cookbook/Chapter%201%20-%20Reading%20from%20a%20CSV.ipynb\n\nI have the problem that the plot at 1.3 won\'t appear when I do it in Jupyter, I only get the following output:\n\nmatplotlib.axes._subplots.AxesSubplot at 0x8ad24a8&gt;"\n\n\nHowever it does appear when I run the same code in Spyder. Does anyone know why this is?  This is my code:\n\n import pandas as pd\nimport os\nfixed_df = pd.read_csv(\'bikes.csv\', sep=\';\', encoding=\'latin1\', parse_dates=[\'Date\'], dayfirst=True, index_col=\'Date\')\nfixed_df[\'Berri1\'].plot()\n\n'
'I have a dataframe with second timeseries data of wheat in df.\n\ndf = wt["WHEAT_USD"]\n\n2016-05-02 02:00:00+02:00    4.780\n2016-05-02 02:01:00+02:00    4.777\n2016-05-02 02:02:00+02:00    4.780\n2016-05-02 02:03:00+02:00    4.780\n2016-05-02 02:04:00+02:00    4.780\nName: closeAsk, dtype: float64\n\n\nWhen I plot the data it has theese annoying horizontal lines because of weekends. Are there any simple way of simply removing the non-business days from the dataframe itself. \n\nSomething like\n\ndf = df.BDays()\n\n'
"I would like to know if there is a function to change specific column names but without selecting a specific name or without changing all of them. \n\nI have the code: \n\ndf=df.rename(columns = {'nameofacolumn':'newname'})\n\n\nBut with it i have to manually change each one of them writing each name. \nAlso to change all of them I have \n\ndf = df.columns['name1','name2','etc']\n\n\nI would like to have a function to change columns 1 and 3 without writing their names just stating their location. Thanks!\n"
'I have a DataFrame like this (first column is index (786...) and second day (25...) and Rainfall amount is empty):    \n\nDay Rainfall amount (millimetres)  \n786   25                              \n787   26                              \n788   27                              \n789   28                              \n790   29                              \n791    1                              \n792    2                              \n793    3                              \n794    4                              \n795    5 \n\n\nand I want to delete the row 790. I tried so many things with df.drop but nothin happend.\n\nI hope you can help me.\n'
"I do as below:\n\ndata1 = pd.DataFrame({ 'b' : [1, 1, 1], 'a' : [2, 2, 2]})\ndata2 = pd.DataFrame({ 'b' : [1, 1, 1], 'a' : [2, 2, 2]})\nframes = [data1, data2]\ndata = pd.concat(frames)\ndata\n\n\n   a    b\n0   2   1\n1   2   1\n2   2   1\n0   2   1\n1   2   1\n2   2   1\n\n\nThe data column order is in alphabet order. Why is it so?\nand how to keep the original order?\n"
"So there's a DataFrame say:\n\n&gt;&gt;&gt; df = pd.DataFrame({\n...                 'A':[1,2,'Three',4],\n...                 'B':[1,'Two',3,4]})\n&gt;&gt;&gt; df\n       A    B\n0      1    1\n1      2  Two\n2  Three    3\n3      4    4\n\n\nI want to select the rows whose datatype of particular row of a particular column is of type str.\n\nFor example I want to select the row where type of data in the column A is a str.\n so it should print something like:\n\n   A      B\n2  Three  3\n\n\nWhose intuitive code would be like:\n\ndf[type(df.A) == str]\n\n\nWhich obviously doesn't works!\n\nThanks please help!\n"
"I'm trying to figure out if there is a good way to manage units in my pandas data. For example, I have a DataFrame that looks like this:\n\n   length (m)  width (m)  thickness (cm)\n0         1.2        3.4             5.6\n1         7.8        9.0             1.2\n2         3.4        5.6             7.8\n\n\nCurrently, the measurement units are encoded in column names. Downsides include:\n\n\ncolumn selection is awkward -- df['width (m)'] vs. df['width']\nthings will likely break if the units of my source data change\n\n\nIf I wanted to strip the units out of the column names, is there somewhere else that the information could be stored?\n"
"Given the following array, I want to replace commas with dots:\n\narray(['0,140711', '0,140711', '0,0999', '0,0999', '0,001', '0,001',\n       '0,140711', '0,140711', '0,140711', '0,140711', '0,140711',\n       '0,140711', 0L, 0L, 0L, 0L, '0,140711', '0,140711', '0,140711',\n       '0,140711', '0,140711', '0,1125688', '0,140711', '0,1125688',\n       '0,140711', '0,1125688', '0,140711', '0,1125688', '0,140711',\n       '0,140711', '0,140711', '0,140711', '0,140711', '0,140711',\n       '0,140711', '0,140711', '0,140711', '0,140711', '0,140711',\n       '0,140711', '0,140711', '0,140711', '0,140711', '0,140711',\n       '0,140711', '0,140711', '0,140711', '0,140711'], dtype=object)\n\n\nI've been trying different ways but I can't figure out how to do this.\nAlso, I have imported it as a pandas DataFrame but can't apply the function:\n\ndf\n      1-8        1-7\nH0   0,140711   0,140711\nH1     0,0999     0,0999\nH2      0,001      0,001\nH3   0,140711   0,140711\nH4   0,140711   0,140711\nH5   0,140711   0,140711\nH6          0          0\nH7          0          0\nH8   0,140711   0,140711\nH9   0,140711   0,140711\nH10  0,140711  0,1125688\nH11  0,140711  0,1125688\nH12  0,140711  0,1125688\nH13  0,140711  0,1125688\nH14  0,140711   0,140711\nH15  0,140711   0,140711\nH16  0,140711   0,140711\nH17  0,140711   0,140711\nH18  0,140711   0,140711\nH19  0,140711   0,140711\nH20  0,140711   0,140711\nH21  0,140711   0,140711\nH22  0,140711   0,140711\nH23  0,140711   0,140711 \n\ndf.applymap(lambda x: str(x.replace(',','.')))\n\n\nAny suggestions how to solve this? \n"
'I have a column in a DataFrame with values:\n\n[1, 1, -1, 1, -1, -1]\n\n\nHow can I group them like this?\n\n[1,1] [-1] [1] [-1, -1]\n\n'
'So in R when I have a data frame consisting of say 4 columns, call it df and I want to compute the ratio by sum product of a group, I can it in such a way:\n\n// generate data\ndf = data.frame(a=c(1,1,0,1,0),b=c(1,0,0,1,0),c=c(10,5,1,5,10),d=c(3,1,2,1,2));\n| a   b   c    d |\n| 1   1   10   3 |\n| 1   0   5    1 |\n| 0   0   1    2 |\n| 1   1   5    1 |\n| 0   0   10   2 |\n// compute sum product ratio\ndf = df%&gt;% group_by(a,b) %&gt;%\n      mutate(\n          ratio=c/sum(c*d)\n      );\n| a   b   c    d  ratio |\n| 1   1   10   3  0.286 |\n| 1   1   5    1  0.143 |\n| 1   0   5    1  1     |\n| 0   0   1    2  0.045 |\n| 0   0   10   2  0.454 |\n\n\nBut in python I need to resort to loops.\nI know there should be a more elegant way than raw loops in python, anyone got any ideas? \n'
'How can I  convert a JSON File as such  into a dataframe to do some transformations.\n\nFor Example if the JSON file reads:\n\n{"FirstName":"John",\n\n"LastName":"Mark",\n\n"MiddleName":"Lewis",\n\n"username":"johnlewis2",\n\n"password":"2910"}\n\n\nHow can I convert it to a table like such\n\nColumn -&gt; FirstName | LastName | MiddleName | username | password\n\n\n\nRow -----&gt;    John | Mark |Lewis | johnlewis2 |2910\n\n'
'Say I have the following Pandas Dataframe:\n\ndf = pd.DataFrame({"a" : [1,2,3], "b" : [[1,2],[2,3,4],[5]]})\n   a          b\n0  1     [1, 2]\n1  2  [2, 3, 4]\n2  3        [5]\n\n\nHow would I "unstack" the lists in the "b" column in order to transform it into the dataframe:\n\n   a  b\n0  1  1\n1  1  2\n2  2  2\n3  2  3\n4  2  4\n5  3  5\n\n'
"For certain columns of df, if 80% of the column is NAN.\n\nWhat's the simplest code to drop such columns?\n"
'I have a pandas dataframe that has two columns. \n\nI need the plot ordered by the "Count" Column.\n\ndicti=({\'37\':99943,\'25\':47228,\'36\':16933,\'40\':14996,\'35\':11791,\'34\':8030,\'24\' : 6319 ,\'2\'  :5055 ,\'39\' :4758 ,\'38\' :4611  })\npd_df = pd.DataFrame(list(dicti.iteritems()))\npd_df.columns =["Dim","Count"]\nplt.figure(figsize=(12,8))\nax = sns.barplot(x="Dim", y= "Count",data=pd_df )\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "\n{:,}".format(int(x))))\nax.set(xlabel="Dim", ylabel=\'Count\')\nfor item in ax.get_xticklabels():\n    item.set_rotation(90)\nfor i, v in enumerate(pd_df["Count"].iteritems()):        \n    ax.text(i ,v[1], "{:,}".format(v[1]), color=\'m\', va =\'bottom\', \n    rotation=45)\nplt.tight_layout()\n\n\nRight now the plot is getting ordered by the "Dim" column, I need it ordered by the "Count" column,How can I do this?\n'
'I created a DatetimeIndex from a "date" column:\n\nsales.index = pd.DatetimeIndex(sales["date"])\n\n\nNow the index looks as follows:\n\nDatetimeIndex([\'2003-01-02\', \'2003-01-03\', \'2003-01-04\', \'2003-01-06\',\n                   \'2003-01-07\', \'2003-01-08\', \'2003-01-09\', \'2003-01-10\',\n                   \'2003-01-11\', \'2003-01-13\',\n                   ...\n                   \'2016-07-22\', \'2016-07-23\', \'2016-07-24\', \'2016-07-25\',\n                   \'2016-07-26\', \'2016-07-27\', \'2016-07-28\', \'2016-07-29\',\n                   \'2016-07-30\', \'2016-07-31\'],\n                  dtype=\'datetime64[ns]\', name=\'date\', length=4393, freq=None)\n\n\nAs you see, the freq attribute is None. I suspect that errors down the road are caused by the missing freq. However, if I try to set the frequency explicitly:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-148-30857144de81&gt; in &lt;module&gt;()\n      1 #### DEBUG\n----&gt; 2 sales_train = disentangle(df_train)\n      3 sales_holdout = disentangle(df_holdout)\n      4 result = sarima_fit_predict(sales_train.loc[5002, 9990]["amount_sold"], sales_holdout.loc[5002, 9990]["amount_sold"])\n\n&lt;ipython-input-147-08b4c4ecdea3&gt; in disentangle(df_train)\n      2     # transform sales table to disentangle sales time series\n      3     sales = df_train[["date", "store_id", "article_id", "amount_sold"]]\n----&gt; 4     sales.index = pd.DatetimeIndex(sales["date"], freq="d")\n      5     sales = sales.pivot_table(index=["store_id", "article_id", "date"])\n      6     return sales\n\n/usr/local/lib/python3.6/site-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)\n     89                 else:\n     90                     kwargs[new_arg_name] = new_arg_value\n---&gt; 91             return func(*args, **kwargs)\n     92         return wrapper\n     93     return _deprecate_kwarg\n\n/usr/local/lib/python3.6/site-packages/pandas/core/indexes/datetimes.py in __new__(cls, data, freq, start, end, periods, copy, name, tz, verify_integrity, normalize, closed, ambiguous, dtype, **kwargs)\n    399                                          \'dates does not conform to passed \'\n    400                                          \'frequency {1}\'\n--&gt; 401                                          .format(inferred, freq.freqstr))\n    402 \n    403         if freq_infer:\n\nValueError: Inferred frequency None from passed dates does not conform to passed frequency D\n\n\nSo apparently a frequency has been inferred, but is stored neither in the freq nor inferred_freq attribute of the DatetimeIndex - both are None. Can someone clear up the confusion?\n'
"I'm using XGBoost with Python and have successfully trained a model using the XGBoost train() function called on DMatrix data. The matrix was created from a Pandas dataframe, which has feature names for the columns.\n\nXtrain, Xval, ytrain, yval = train_test_split(df[feature_names], y, \\\n                                    test_size=0.2, random_state=42)\ndtrain = xgb.DMatrix(Xtrain, label=ytrain)\n\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=60, \\\n                  early_stopping_rounds=50, maximize=False, verbose_eval=10)\n\nfig, ax = plt.subplots(1,1,figsize=(10,10))\nxgb.plot_importance(model, max_num_features=5, ax=ax)\n\n\nI want to now see the feature importance using the xgboost.plot_importance() function, but the resulting plot doesn't show the feature names. Instead, the features are listed as f1, f2, f3, etc. as shown below.\n\n\n\nI think the problem is that I converted my original Pandas data frame into a DMatrix. How can I associate feature names properly so that the feature importance plot shows them?\n"
'I\'m looking for a simple way of parsing complex text files into a pandas DataFrame. Below is a sample file, what I want the result to look like after parsing, and my current method. \n\nIs there any way to make it more concise/faster/more pythonic/more readable?\n\nI\'ve also put this question on Code Review.\n\nI eventually wrote a blog article to explain this to beginners.\n\nHere is a sample file:\n\nSample text\n\nA selection of students from Riverdale High and Hogwarts took part in a quiz. This is a record of their scores.\n\nSchool = Riverdale High\nGrade = 1\nStudent number, Name\n0, Phoebe\n1, Rachel\n\nStudent number, Score\n0, 3\n1, 7\n\nGrade = 2\nStudent number, Name\n0, Angela\n1, Tristan\n2, Aurora\n\nStudent number, Score\n0, 6\n1, 3\n2, 9\n\nSchool = Hogwarts\nGrade = 1\nStudent number, Name\n0, Ginny\n1, Luna\n\nStudent number, Score\n0, 8\n1, 7\n\nGrade = 2\nStudent number, Name\n0, Harry\n1, Hermione\n\nStudent number, Score\n0, 5\n1, 10\n\nGrade = 3\nStudent number, Name\n0, Fred\n1, George\n\nStudent number, Score\n0, 0\n1, 0\n\n\nHere is what I want the result to look like after parsing:\n\n                                         Name  Score\nSchool         Grade Student number                 \nHogwarts       1     0                  Ginny      8\n                     1                   Luna      7\n               2     0                  Harry      5\n                     1               Hermione     10\n               3     0                   Fred      0\n                     1                 George      0\nRiverdale High 1     0                 Phoebe      3\n                     1                 Rachel      7\n               2     0                 Angela      6\n                     1                Tristan      3\n                     2                 Aurora      9\n\n\nHere is how I currently parse it:\n\nimport re\nimport pandas as pd\n\n\ndef parse(filepath):\n    """\n    Parse text at given filepath\n\n    Parameters\n    ----------\n    filepath : str\n        Filepath for file to be parsed\n\n    Returns\n    -------\n    data : pd.DataFrame\n        Parsed data\n\n    """\n\n    data = []\n    with open(filepath, \'r\') as file:\n        line = file.readline()\n        while line:\n            reg_match = _RegExLib(line)\n\n            if reg_match.school:\n                school = reg_match.school.group(1)\n\n            if reg_match.grade:\n                grade = reg_match.grade.group(1)\n                grade = int(grade)\n\n            if reg_match.name_score:\n                value_type = reg_match.name_score.group(1)\n                line = file.readline()\n                while line.strip():\n                    number, value = line.strip().split(\',\')\n                    value = value.strip()\n                    dict_of_data = {\n                        \'School\': school,\n                        \'Grade\': grade,\n                        \'Student number\': number,\n                        value_type: value\n                    }\n                    data.append(dict_of_data)\n                    line = file.readline()\n\n            line = file.readline()\n\n        data = pd.DataFrame(data)\n        data.set_index([\'School\', \'Grade\', \'Student number\'], inplace=True)\n        # consolidate df to remove nans\n        data = data.groupby(level=data.index.names).first()\n        # upgrade Score from float to integer\n        data = data.apply(pd.to_numeric, errors=\'ignore\')\n    return data\n\n\nclass _RegExLib:\n    """Set up regular expressions"""\n    # use https://regexper.com to visualise these if required\n    _reg_school = re.compile(\'School = (.*)\\n\')\n    _reg_grade = re.compile(\'Grade = (.*)\\n\')\n    _reg_name_score = re.compile(\'(Name|Score)\')\n\n    def __init__(self, line):\n        # check whether line has a positive match with all of the regular expressions\n        self.school = self._reg_school.match(line)\n        self.grade = self._reg_grade.match(line)\n        self.name_score = self._reg_name_score.search(line)\n\n\nif __name__ == \'__main__\':\n    filepath = \'sample.txt\'\n    data = parse(filepath)\n    print(data)\n\n'
'I have written the program (below) to:\n\n\nread a huge text file as pandas dataframe\nthen groupby using a specific column value to split the data and store as list of dataframes.\nthen pipe the data to multiprocess Pool.map() to process each dataframe in parallel.\n\n\nEverything is fine, the program works well on my small test dataset. But, when I pipe in my large data (about 14 GB), the memory consumption exponentially increases and then freezes the computer or gets killed (in HPC cluster). \n\nI have added codes to clear the memory as soon as the data/variable isn\'t useful. I am also closing the pool as soon as it is done. Still with 14 GB input I was only expecting 2*14 GB memory burden, but it seems like lot is going on. I also tried to tweak using chunkSize and maxTaskPerChild, etc but I am not seeing any difference in optimization in both test vs. large file.\n\nI think improvements to this code is/are required at this code position, when I start multiprocessing. \n\np = Pool(3)  # number of pool to run at once; default at 1\n    result = p.map(matrix_to_vcf, list(gen_matrix_df_list.values()))\nbut, I am posting the whole code.\n\nTest example: I created a test file ("genome_matrix_final-chr1234-1mb.txt") of upto 250 mb and ran the program. When I check the system monitor I can see that memory consumption increased by about 6 GB. I am not so clear why so much memory space is taken by 250 mb file plus some outputs. I have shared that file via drop box if it helps in seeing the real problem. https://www.dropbox.com/sh/coihujii38t5prd/AABDXv8ACGIYczeMtzKBo0eea?dl=0 \n\nCan someone suggest, How I can get rid of the problem?\n\nMy python script:\n\n#!/home/bin/python3\n\nimport pandas as pd\nimport collections\nfrom multiprocessing import Pool\nimport io\nimport time\nimport resource\n\nprint()\nprint(\'Checking required modules\')\nprint()\n\n\n\'\'\' change this input file name and/or path as need be \'\'\'\ngenome_matrix_file = "genome_matrix_final-chr1n2-2mb.txt"   # test file 01\ngenome_matrix_file = "genome_matrix_final-chr1234-1mb.txt"  # test file 02\n#genome_matrix_file = "genome_matrix_final.txt"    # large file \n\ndef main():\n    with open("genome_matrix_header.txt") as header:\n        header = header.read().rstrip(\'\\n\').split(\'\\t\')\n        print()\n\n    time01 = time.time()\n    print(\'starting time: \', time01)\n\n    \'\'\'load the genome matrix file onto pandas as dataframe.\n    This makes is more easy for multiprocessing\'\'\'\n    gen_matrix_df = pd.read_csv(genome_matrix_file, sep=\'\\t\', names=header)\n\n    # now, group the dataframe by chromosome/contig - so it can be multiprocessed\n    gen_matrix_df = gen_matrix_df.groupby(\'CHROM\')\n\n    # store the splitted dataframes as list of key, values(pandas dataframe) pairs\n    # this list of dataframe will be used while multiprocessing\n    gen_matrix_df_list = collections.OrderedDict()\n    for chr_, data in gen_matrix_df:\n        gen_matrix_df_list[chr_] = data\n\n    # clear memory\n    del gen_matrix_df\n\n    \'\'\'Now, pipe each dataframe from the list using map.Pool() \'\'\'\n    p = Pool(3)  # number of pool to run at once; default at 1\n    result = p.map(matrix_to_vcf, list(gen_matrix_df_list.values()))\n\n    del gen_matrix_df_list  # clear memory\n\n    p.close()\n    p.join()\n\n\n    # concat the results from pool.map() and write it to a file\n    result_merged = pd.concat(result)\n    del result  # clear memory\n\n    pd.DataFrame.to_csv(result_merged, "matrix_to_haplotype-chr1n2.txt", sep=\'\\t\', header=True, index=False)\n\n    print()\n    print(\'completed all process in "%s" sec. \' % (time.time() - time01))\n    print(\'Global maximum memory usage: %.2f (mb)\' % current_mem_usage())\n    print()\n\n\n\'\'\'function to convert the dataframe from genome matrix to desired output \'\'\'\ndef matrix_to_vcf(matrix_df):\n\n    print()\n    time02 = time.time()\n\n    # index position of the samples in genome matrix file\n    sample_idx = [{\'10a\': 33, \'10b\': 18}, {\'13a\': 3, \'13b\': 19},\n                    {\'14a\': 20, \'14b\': 4}, {\'16a\': 5, \'16b\': 21},\n                    {\'17a\': 6, \'17b\': 22}, {\'23a\': 7, \'23b\': 23},\n                    {\'24a\': 8, \'24b\': 24}, {\'25a\': 25, \'25b\': 9},\n                    {\'26a\': 10, \'26b\': 26}, {\'34a\': 11, \'34b\': 27},\n                    {\'35a\': 12, \'35b\': 28}, {\'37a\': 13, \'37b\': 29},\n                    {\'38a\': 14, \'38b\': 30}, {\'3a\': 31, \'3b\': 15},\n                    {\'8a\': 32, \'8b\': 17}]\n\n    # sample index stored as ordered dictionary\n    sample_idx_ord_list = []\n    for ids in sample_idx:\n        ids = collections.OrderedDict(sorted(ids.items()))\n        sample_idx_ord_list.append(ids)\n\n\n    # for haplotype file\n    header = [\'contig\', \'pos\', \'ref\', \'alt\']\n\n    # adding some suffixes "PI" to available sample names\n    for item in sample_idx_ord_list:\n        ks_update = \'\'\n        for ks in item.keys():\n            ks_update += ks\n        header.append(ks_update+\'_PI\')\n        header.append(ks_update+\'_PG_al\')\n\n\n    #final variable store the haplotype data\n    # write the header lines first\n    haplotype_output = \'\\t\'.join(header) + \'\\n\'\n\n\n    # to store the value of parsed the line and update the "PI", "PG" value for each sample\n    updated_line = \'\'\n\n    # read the piped in data back to text like file\n    matrix_df = pd.DataFrame.to_csv(matrix_df, sep=\'\\t\', index=False)\n\n    matrix_df = matrix_df.rstrip(\'\\n\').split(\'\\n\')\n    for line in matrix_df:\n        if line.startswith(\'CHROM\'):\n            continue\n\n        line_split = line.split(\'\\t\')\n        chr_ = line_split[0]\n        ref = line_split[2]\n        alt = list(set(line_split[3:]))\n\n        # remove the alleles "N" missing and "ref" from the alt-alleles\n        alt_up = list(filter(lambda x: x!=\'N\' and x!=ref, alt))\n\n        # if no alt alleles are found, just continue\n        # - i.e : don\'t write that line in output file\n        if len(alt_up) == 0:\n            continue\n\n        #print(\'\\nMining data for chromosome/contig "%s" \' %(chr_ ))\n        #so, we have data for CHR, POS, REF, ALT so far\n        # now, we mine phased genotype for each sample pair (as "PG_al", and also add "PI" tag)\n        sample_data_for_vcf = []\n        for ids in sample_idx_ord_list:\n            sample_data = []\n            for key, val in ids.items():\n                sample_value = line_split[val]\n                sample_data.append(sample_value)\n\n            # now, update the phased state for each sample\n            # also replacing the missing allele i.e "N" and "-" with ref-allele\n            sample_data = (\'|\'.join(sample_data)).replace(\'N\', ref).replace(\'-\', ref)\n            sample_data_for_vcf.append(str(chr_))\n            sample_data_for_vcf.append(sample_data)\n\n        # add data for all the samples in that line, append it with former columns (chrom, pos ..) ..\n        # and .. write it to final haplotype file\n        sample_data_for_vcf = \'\\t\'.join(sample_data_for_vcf)\n        updated_line = \'\\t\'.join(line_split[0:3]) + \'\\t\' + \',\'.join(alt_up) + \\\n            \'\\t\' + sample_data_for_vcf + \'\\n\'\n        haplotype_output += updated_line\n\n    del matrix_df  # clear memory\n    print(\'completed haplotype preparation for chromosome/contig "%s" \'\n          \'in "%s" sec. \' %(chr_, time.time()-time02))\n    print(\'\\tWorker maximum memory usage: %.2f (mb)\' %(current_mem_usage()))\n\n    # return the data back to the pool\n    return pd.read_csv(io.StringIO(haplotype_output), sep=\'\\t\')\n\n\n\'\'\' to monitor memory \'\'\'\ndef current_mem_usage():\n    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / 1024.\n\n\nif __name__ == \'__main__\':\n    main()\n\n\nUpdate for bounty hunters:\n\nI have achieved multiprocessing using Pool.map() but the code is causing a big memory burden (input test file ~ 300 mb, but memory burden is about 6 GB). I was only expecting 3*300 mb memory burden at max. \n\n\nCan somebody explain, What is causing such a huge memory requirement for such a small file and for such small length computation. \nAlso, i am trying to take the answer and use that to improve multiprocess in my large program. So, addition of any method, module that doesn\'t change the structure of computation part (CPU bound process) too much should be fine. \nI have included two test files for the test purposes to play with the code. \nThe attached code is full code so it should work as intended as it is when copied-pasted. Any changes should be used only to improve optimization in multiprocessing steps.\n\n'
"Suppose I have a select roughly like this:\n\nselect instrument, price, date from my_prices;\n\n\nHow can I unpack the prices returned into a single dataframe with a series for each instrument and indexed on date?\n\nTo be clear: I'm looking for:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninst_1    ...\ninst_2    ...\ndtypes: float64(1), object(1) \n\n\nI'm NOT looking for:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: ...\nData columns (total 2 columns):\ninstrument    ...\nprice         ...\ndtypes: float64(1), object(1)\n\n\n...which is easy ;-)\n"
'Bar plots:\n\nmatplotlib offers the function bar and barh to do vertical and horizontal bar plots. \n\nBox plots:\n\nmatplotlib also offers the function boxplot to do vertical box plots. \n\nAnd Pandas offers its own function for vertical box plots.\n\nBut is there any way in matplotlib or Pandas to get a horizontal box plot? \n'
"I would like to write some comments in my CSV file created with pandas. I haven't found any option for this in DataFrame.to_csv (even though read_csv can skip comments) neither in the standard csv module. I can open the file, write the comments (line starting with #) and then pass it to to_csv. Does any body have a better option?\n"
"I would like to create a new column with a numerical value based on the following conditions:\n\na. if gender is male &amp; pet1=pet2, points = 5\n\nb. if gender is female &amp; (pet1 is 'cat' or pet1='dog'), points = 5\n\nc. all other combinations, points = 0\n\n    gender    pet1      pet2\n0   male      dog       dog\n1   male      cat       cat\n2   male      dog       cat\n3   female    cat       squirrel\n4   female    dog       dog\n5   female    squirrel  cat\n6   squirrel  dog       cat\n\n\nI would like the end result to be as follows:\n\n    gender    pet1      pet2      points\n0   male      dog       dog       5\n1   male      cat       cat       5\n2   male      dog       cat       0\n3   female    cat       squirrel  5\n4   female    dog       dog       5\n5   female    squirrel  cat       0\n6   squirrel  dog       cat       0\n\n\nHow do I accomplish this?\n"
'I have the following code:\n\nimport pandas as pd\nimport matplotlib\nmatplotlib.style.use(\'ggplot\')\ndf = pd.DataFrame({ \'sample1\':[\'foo\',\'bar\',\'bar\',\'qux\'], \'score\':[5,9,1,7]})\nsum_df = df.groupby("sample1").sum()\npie = sum_df.plot(kind="pie", figsize=(6,6), legend = False, use_index=False, subplots=True, colormap="Pastel1")\n\n\nWhich makes the pie chart. What I want to do then is to save it to a file.\nBut why this fail?\n\nfig = pie.get_figure()\nfig.savefig("~/Desktop/myplot.pdf")\n\n\nI get this error:\n\n\'numpy.ndarray\' object has no attribute \'get_figure\'\n\n'
"Given the following data frame:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A':['1a',np.nan,'10a','100b','0b'],\n                   })\ndf\n\n    A\n0   1a\n1   NaN\n2   10a\n3   100b\n4   0b\n\n\nI'd like to extract the numbers from each cell (where they exist).\nThe desired result is:\n\n    A\n0   1\n1   NaN\n2   10\n3   100\n4   0\n\n\nI know it can be done with str.extract, but I'm not sure how.\n"
"I have the following problem\n\nI have a dataframe master that contains sentences, such as\n\nmaster\nOut[8]: \n                  original\n0  this is a nice sentence\n1      this is another one\n2    stackoverflow is nice\n\n\nFor every row in Master, I lookup into another Dataframe slave for the best match using fuzzywuzzy. I use fuzzywuzzy because the matched sentences between the two dataframes could differ a bit (extra characters, etc).\n\nFor instance, slave could be\n\nslave\nOut[10]: \n   my_value                      name\n0         2               hello world\n1         1           congratulations\n2         2  this is a nice sentence \n3         3       this is another one\n4         1     stackoverflow is nice\n\n\nHere is a fully-functional, wonderful, compact working example :)\n\nfrom fuzzywuzzy import fuzz\nimport pandas as pd\nimport numpy as np\nimport difflib\n\n\nmaster= pd.DataFrame({'original':['this is a nice sentence',\n'this is another one',\n'stackoverflow is nice']})\n\n\nslave= pd.DataFrame({'name':['hello world',\n'congratulations',\n'this is a nice sentence ',\n'this is another one',\n'stackoverflow is nice'],'my_value': [2,1,2,3,1]})\n\ndef fuzzy_score(str1, str2):\n    return fuzz.token_set_ratio(str1, str2)\n\ndef helper(orig_string, slave_df):\n    #use fuzzywuzzy to see how close original and name are\n    slave_df['score'] = slave_df.name.apply(lambda x: fuzzy_score(x,orig_string))\n    #return my_value corresponding to the highest score\n    return slave_df.ix[slave_df.score.idxmax(),'my_value']\n\nmaster['my_value'] = master.original.apply(lambda x: helper(x,slave))\n\n\nThe 1 million dollars question is: can I parallelize my apply code above? \n\nAfter all, every row in master is compared to all the rows in slave (slave is a small dataset and I can hold many copies of the data into the RAM). \n\nI dont see why I could not run multiple comparisons (i.e. process multiple rows at the same time). \n\nProblem: I dont know how to do that or if thats even possible.\n\nAny help greatly appreciated!\n"
'This seems simple, but I can not find any information on it on the internet\n\nI have a dataframe like below\n\nCity    State Zip           Date        Description       \nEarlham IA    50072-1036    2014-10-10  Postmarket Assurance: Devices\nEarlham IA    50072-1036    2014-10-10  Compliance: Devices\nMadrid  IA    50156-1748    2014-09-10  Drug Quality Assurance\n\n\nHow can I eliminate duplicates that match 4 of 5 columns? The column not matching being Description.\n\nThe result would be\n\nCity    State Zip           Date        Description       \nEarlham IA    50072-1036    2014-10-10  Postmarket Assurance: Devices\nMadrid  IA    50156-1748    2014-09-10  Drug Quality Assurance\n\n\nI found online that drop_dupilcates with the subset parameter could work, but I am unsure of how I can apply it to multiple columns.\n'
'I have a Pandas df [see below].\nHow do I add values from a function to a new column "price"?\n\nfunction:\n\n    def getquotetoday(symbol):\n        yahoo = Share(symbol)\n        return yahoo.get_prev_close()\n\ndf:\n\nSymbol    Bid      Ask\nMSFT     10.25   11.15\nAAPL     100.01  102.54\n\n\n  (...)\n\n'
"If I have an existing pandas dataframe, is there a way to generate the python code, which when executed in another python script, will reproduce that dataframe.\n\ne.g. \n\nIn[1]: df\n\nOut[1]:\n   income   user\n0   40000    Bob\n1   50000   Jane\n2   42000  Alice\n\nIn[2]: someFunToWriteDfCode(df)\n\nOut[2]: \ndf = pd.DataFrame({'user': ['Bob', 'Jane', 'Alice'], \n    ...:                    'income': [40000, 50000, 42000]})\n\n"
"I want to sort by name length. There doesn't appear to be a key parameter for sort_values so I'm not sure how to accomplish this. Here is a test df:\n\nimport pandas as pd\ndf = pd.DataFrame({'name': ['Steve', 'Al', 'Markus', 'Greg'], 'score': [2, 4, 2, 3]})\n\n"
"I would like to calculate the mean and standard deviation of a timedelta by bank from a dataframe with two columns shown below. When I run the code (also shown below) I get the below error:\n\npandas.core.base.DataError: No numeric types to aggregate\n\n\nMy dataframe:\n\n   bank                          diff\n   Bank of Japan                 0 days 00:00:57.416000\n   Reserve Bank of Australia     0 days 00:00:21.452000\n   Reserve Bank of New Zealand  55 days 12:39:32.269000\n   U.S. Federal Reserve          8 days 13:27:11.387000\n\n\nMy code:\n\nmeans = dropped.groupby('bank').mean()\nstd = dropped.groupby('bank').std()\n\n"
"This is my original dataframe.\n\nThis is my second dataframe containing one column.\n\nI want to add the column of second dataframe to the original dataframe at the end.Indices are different for both dataframes.\nI did like this\nfeature_file_df['RESULT']=RESULT_df['RESULT']\n\nResult column got added but all values are NaN's\n\nHow to add columns with value\n"
'I\'m trying to do boolean indexing with a couple conditions using Pandas.  My original DataFrame is called df.  If I perform the below, I get the expected result:\n\ntemp = df[df["bin"] == 3]\ntemp = temp[(~temp["Def"])]\ntemp = temp[temp["days since"] &gt; 7]\ntemp.head()\n\n\nHowever, if I do this (which I think should be equivalent), I get no rows back:\n\ntemp2 = df[df["bin"] == 3]\ntemp2 = temp2[~temp2["Def"] &amp; temp2["days since"] &gt; 7]\ntemp2.head()\n\n\nAny idea what accounts for the difference? \n'
'I have a pandas DataFrame that has multiple columns in it:\n\nIndex: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nfoo                   11516  non-null values\nbar                   228381  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n\n\nwhere foo and bar are columns which contain the same data yet are named differently. Is there are a way to move the rows which make up foo into bar, ideally whilst maintaining the name of bar? \n\nIn the end the DataFrame should appear as:\n\nIndex: 239897 entries, 2012-05-11 15:20:00 to 2012-06-02 23:44:51\nData columns:\nbar                   239897  non-null values\nTime_UTC              239897  non-null values\ndtstamp               239897  non-null values\ndtypes: float64(4), object(1)\n\n\nThat is the NaN values that made up bar were replaced by the values from foo.\n'
"I have GPS data of ice speed from three different GPS receivers. The data are in a pandas dataframe with an index of julian day (incremental from the start of 2009).\n\nThis is a subset of the data (the main dataset is 3487235 rows...):\n\n                    R2          R7         R8\n1235.000000 116.321959  100.805197  96.519977\n1235.000116 NaN         100.771133  96.234957\n1235.000231 NaN         100.584559  97.249262\n1235.000347 118.823610  100.169055  96.777833\n1235.000463 NaN         99.753551   96.598350\n1235.000579 NaN         99.338048   95.283989\n1235.000694 113.995003  98.922544   95.154067\n\n\nThe dataframe has form:\n\n\nIndex: 6071320 entries, 127.67291667 to 1338.51805556\nData columns:\nR2    3487235  non-null values\nR7    3875864  non-null values\nR8    1092430  non-null values\ndtypes: float64(3)\n\n\nR2 sampled at a different rate to R7 and R8 hence the NaNs which appear systematically at that spacing.\n\nTrying df.plot() to plot the whole dataframe (or indexed row locations thereof) works fine in terms of plotting R7 and R8, but doesn't plot R2. Similarly, just doing df.R2.plot() also doesn't work. The only way to plot R2 is to do df.R2.dropna().plot(), but this also removes NaNs which signify periods of no data (rather than just a coarser sampling frequency than the other receivers).\n\nHas anyone else come across this? Any ideas on the problem would be gratefully received :)\n"
"Another pandas question.\n\nReading Wes Mckinney's excellent book about Data Analysis and Pandas, I encountered the following thing that I thought should work:\n\nSuppose I have some info about tips.\n\nIn [119]:\n\ntips.head()\nOut[119]:\ntotal_bill  tip      sex     smoker    day   time    size  tip_pct\n0    16.99   1.01    Female  False   Sun     Dinner  2   0.059447\n1    10.34   1.66    Male    False   Sun     Dinner  3   0.160542\n2    21.01   3.50    Male    False   Sun     Dinner  3   0.166587\n3    23.68   3.31    Male    False   Sun     Dinner  2   0.139780\n4    24.59   3.61    Female  False   Sun     Dinner  4   0.146808\n\n\nand I want to know the five largest tips in relation to the total bill, that is, tip_pct for smokers and non-smokers separately. So this works:\n\ndef top(df, n=5, column='tip_pct'): \n    return df.sort_index(by=column)[-n:]\n\nIn [101]:\n\ntips.groupby('smoker').apply(top)\nOut[101]:\n           total_bill   tip sex smoker  day time    size    tip_pct\nsmoker                                  \nFalse   88   24.71   5.85    Male    False   Thur    Lunch   2   0.236746\n185  20.69   5.00    Male    False   Sun     Dinner  5   0.241663\n51   10.29   2.60    Female  False   Sun     Dinner  2   0.252672\n149  7.51    2.00    Male    False   Thur    Lunch   2   0.266312\n232  11.61   3.39    Male    False   Sat     Dinner  2   0.291990\n\nTrue    109  14.31   4.00    Female  True    Sat     Dinner  2   0.279525\n183  23.17   6.50    Male    True    Sun     Dinner  4   0.280535\n67   3.07    1.00    Female  True    Sat     Dinner  1   0.325733\n178  9.60    4.00    Female  True    Sun     Dinner  2   0.416667\n172  7.25    5.15    Male    True    Sun     Dinner  2   0.710345\n\n\nGood enough, but then I wanted to use pandas' transform to do the same like this:\n\ndef top_all(df):\n    return df.sort_index(by='tip_pct')\n\ntips.groupby('smoker').transform(top_all)\n\n\nbut instead I get this:\n\nTypeError: Transform function invalid for data types\n\n\nWhy? I know that transform requires to return an array of the same dimensions that it accepts as input, so I thought I'd be complying with that requirement just sorting both slices (smokers and non-smokers) of the original DataFrame without changing their respective dimensions. Can anyone explain why it failed? \n"
"I'm trying to find, at each timestamp, the column name in a dataframe for which the value matches with the one in a timeseries  at the same timestamp.\n\nHere is my dataframe:\n\n&gt;&gt;&gt; df\n                            col5        col4        col3        col2        col1\n1979-01-01 00:00:00  1181.220328  912.154923  648.848635  390.986156  138.185861\n1979-01-01 06:00:00  1190.724461  920.767974  657.099560  399.395338  147.761352\n1979-01-01 12:00:00  1193.414510  918.121482  648.558837  384.632475  126.254342\n1979-01-01 18:00:00  1171.670276  897.585930  629.201469  366.652033  109.545607\n1979-01-02 00:00:00  1168.892579  900.375126  638.377583  382.584568  132.998706\n\n&gt;&gt;&gt; df.to_dict()\n{'col4': {&lt;Timestamp: 1979-01-01 06:00:00&gt;: 920.76797370744271, &lt;Timestamp: 1979-01-01 00:00:00&gt;: 912.15492332839756, &lt;Timestamp: 1979-01-01 18:00:00&gt;: 897.58592995700656, &lt;Timestamp: 1979-01-01 12:00:00&gt;: 918.1214819496729}, 'col5': {&lt;Timestamp: 1979-01-01 06:00:00&gt;: 1190.7244605667831, &lt;Timestamp: 1979-01-01 00:00:00&gt;: 1181.2203275146587, &lt;Timestamp: 1979-01-01 18:00:00&gt;: 1171.6702763228691, &lt;Timestamp: 1979-01-01 12:00:00&gt;: 1193.4145103184442}, 'col2': {&lt;Timestamp: 1979-01-01 06:00:00&gt;: 399.39533771666561, &lt;Timestamp: 1979-01-01 00:00:00&gt;: 390.98615646597591, &lt;Timestamp: 1979-01-01 18:00:00&gt;: 366.65203285812231, &lt;Timestamp: 1979-01-01 12:00:00&gt;: 384.63247469269874}, 'col3': {&lt;Timestamp: 1979-01-01 06:00:00&gt;: 657.09956023625466, &lt;Timestamp: 1979-01-01 00:00:00&gt;: 648.84863460462293, &lt;Timestamp: 1979-01-01 18:00:00&gt;: 629.20146872682449, &lt;Timestamp: 1979-01-01 12:00:00&gt;: 648.55883747413225}, 'col1': {&lt;Timestamp: 1979-01-01 06:00:00&gt;: 147.7613518219286, &lt;Timestamp: 1979-01-01 00:00:00&gt;: 138.18586102094068, &lt;Timestamp: 1979-01-01 18:00:00&gt;: 109.54560722575859, &lt;Timestamp: 1979-01-01 12:00:00&gt;: 126.25434189361377}}\n\n\nAnd the time series with values I want to match at each timestamp:\n\n&gt;&gt;&gt; ts\n1979-01-01 00:00:00    1181.220328\n1979-01-01 06:00:00    657.099560\n1979-01-01 12:00:00    126.254342\n1979-01-01 18:00:00    109.545607\nFreq: 6H\n\n&gt;&gt;&gt; ts.to_dict()\n{&lt;Timestamp: 1979-01-01 06:00:00&gt;: 657.09956023625466, &lt;Timestamp: 1979-01-01 00:00:00&gt;: 1181.2203275146587, &lt;Timestamp: 1979-01-01 18:00:00&gt;: 109.54560722575859, &lt;Timestamp: 1979-01-01 12:00:00&gt;: 126.25434189361377}\n\n\nThen the result would be:\n\n&gt;&gt;&gt; df_result\n                             value  Column\n1979-01-01 00:00:00    1181.220328  col5\n1979-01-01 06:00:00    657.099560   col3\n1979-01-01 12:00:00    126.254342   col1\n1979-01-01 18:00:00    109.545607   col1\n\n\nI hope my question is clear enough. Anyone has an idea how to get df_result?\n\nThanks\n\nGreg\n"
"For dataframe\n\nIn [2]: df = pd.DataFrame({'Name': ['foo', 'bar'] * 3,\n   ...:                    'Rank': np.random.randint(0,3,6),\n   ...:                    'Val': np.random.rand(6)})\n   ...: df\nOut[2]: \n  Name  Rank       Val\n0  foo     0  0.299397\n1  bar     0  0.909228\n2  foo     0  0.517700\n3  bar     0  0.929863\n4  foo     1  0.209324\n5  bar     2  0.381515\n\n\nI'm interested in grouping by Name and Rank and possibly getting aggregate values\n\nIn [3]: group = df.groupby(['Name', 'Rank'])\nIn [4]: agg = group.agg(sum)\nIn [5]: agg\nOut[5]: \n                Val\nName Rank          \nbar  0     1.839091\n     2     0.381515\nfoo  0     0.817097\n     1     0.209324\n\n\nBut I would like to get a field in the original df that contains the group number for that row, like\n\nIn [13]: df['Group_id'] = [2, 0, 2, 0, 3, 1]\nIn [14]: df\nOut[14]: \n  Name  Rank       Val  Group_id\n0  foo     0  0.299397         2\n1  bar     0  0.909228         0\n2  foo     0  0.517700         2\n3  bar     0  0.929863         0\n4  foo     1  0.209324         3\n5  bar     2  0.381515         1\n\n\nIs there a good way to do this in pandas?\n\nI can get it with python, \n\nIn [16]: from itertools import count\nIn [17]: c = count()\nIn [22]: group.transform(lambda x: c.next())\nOut[22]: \n   Val\n0    2\n1    0\n2    2\n3    0\n4    3\n5    1\n\n\nbut it's pretty slow on a large dataframe, so I figured there may be a better built in pandas way to do this. \n"
"I have a data frame. Then I have a logical condition using which I create another data frame by removing some rows. The new data frame however skips indices for removed rows. How can I get it to reindex sequentially without skipping? Here's a sample coded to clarify\n\nimport pandas as pd\nimport numpy as np\n\njjarray = np.array(range(5))\neq2 = jjarray == 2\nneq2 = np.logical_not(eq2)\n\njjdf = pd.DataFrame(jjarray)\njjdfno2 = jjdf[neq2]\n\njjdfno2\n\n\nOut: \n\n  0\n0 0\n1 1\n3 3\n4 4\n\n\nI want it to look like this:\n\n  0\n0 0\n1 1\n2 3\n3 4\n\n\nThanks.\n"
'Given a dataframe how to find out all the columns that only have 0 as the values?\n\ndf\n   0  1  2  3  4  5  6  7\n0  0  0  0  1  0  0  1  0\n1  1  1  0  0  0  1  1  1\n\n\nExpected output\n\n   2  4\n0  0  0\n1  0  0\n\n'
"I have data in a csv file with dates stored as strings in a standard UK format - %d/%m/%Y - meaning they look like:\n\n12/01/2012\n30/01/2012\n\n\nThe examples above represent 12 January 2012 and 30 January 2012.\n\nWhen I import this data with pandas version 0.11.0 I applied the following transformation:\n\nimport pandas as pd\n...\ncpts.Date = cpts.Date.apply(pd.to_datetime)\n\n\nbut it converted dates inconsistently. To use my existing example, 12/01/2012 would convert as a datetime object representing 1 December 2012 but 30/01/2012 converts as 30 January 2012, which is what I want.\n\nAfter looking at this question I tried:\n\ncpts.Date = cpts.Date.apply(pd.to_datetime, format='%d/%m/%Y')\n\n\nbut the results are exactly the same. The source code suggests I'm doing things right so I'm at a loss. Does anyone know what I'm doing wrong?\n"
'I have a dataframe in Pandas, I would like to sort its columns (i.e. get a new dataframe, or a view) according to the mean value of its columns (or e.g. by their std value). The documentation talks about sorting by label or value, but I could not find anything on custom sorting methods.\n\nHow can I do this? \n'
"I'm new to pandas and that's my first question on stackoverflow, I'm trying to do some analytics with pandas.\n\nI have some text files with data records that I want to process. Each line of the file match to a record which fields are in a fixed place and have a length of a fixed number of characters. There are different kinds of records on the same file, all records share the first field that are two characters depending of the type of record. As an example:\n\nSome file:\n01Jhon      Smith     555-1234                                        \n03Cow            Bos primigenius taurus        00401                  \n01Jannette  Jhonson           00100000000                             \n...\n\n\nfield    start  length   \ntype         1       2   *common to all records, example: 01 = person, 03 = animal\nname         3      10\nsurname     13      10\nphone       23       8\ncredit      31      11\nfill of spaces\n\n\nI'm writing some code to convert one record to a dictionary:\n\nperson1 = {'type': 01, 'name': = 'Jhon', 'surname': = 'Smith', 'phone': '555-1234'}\nperson2 = {'type': 01, 'name': 'Jannette', 'surname': 'Jhonson', 'credit': 1000000.00}\nanimal1 = {'type': 03, 'cname': 'cow', 'sciname': 'Bos....', 'legs': 4, 'tails': 1 }\n\n\nIf a field is empty (filled with spaces) there will not be in the dictionary).\n\nWith all records of one kind I want to create a pandas DataFrame with the dicts keys as columns names, I've try with pandas.DataFrame.from_dict() without success. \n\nAnd here comes my question: Is any way to do this with pandas so dict keys become column names? Are any other standard method to deal with this kind of files?\n"
'I have a pandas data frame and group it by two columns (for example col1 and col2). For fixed values of col1 and col2 (i.e. for a group) I can have several different values in the col3. I would like to count the number of distinct values from the third columns.\n\nFor example, If I have this as my input:\n\n1  1  1\n1  1  1\n1  1  2\n1  2  3\n1  2  3\n1  2  3\n2  1  1\n2  1  2\n2  1  3\n2  2  3\n2  2  3\n2  2  3\n\n\nI would like to have this table (data frame) as the output:\n\n1  1  2\n1  2  1\n2  1  3\n2  2  1\n\n'
"I'm Looking for a generic way of turning a DataFrame to a nested dictionary\n\nThis is a sample data frame \n\n    name    v1  v2  v3\n0   A       A1  A11 1\n1   A       A2  A12 2\n2   B       B1  B12 3\n3   C       C1  C11 4\n4   B       B2  B21 5\n5   A       A2  A21 6\n\n\nThe number of columns may differ and so does the column names.\n\nlike this : \n\n{\n'A' : { \n    'A1' : { 'A11' : 1 }\n    'A2' : { 'A12' : 2 , 'A21' : 6 }} , \n'B' : { \n    'B1' : { 'B12' : 3 } } , \n'C' : { \n    'C1' : { 'C11' : 4}}\n}\n\n\nWhat is best way to achieve this ? \n\nclosest I got was with the zip function but haven't managed to make it work for more then one level (two columns).\n"
"If I want to create a new DataFrame with several columns, I can add all the columns at once -- for example, as follows:\n\ndata = {'col_1': [0, 1, 2, 3],\n        'col_2': [4, 5, 6, 7]}\ndf = pd.DataFrame(data)\n\n\nBut now suppose farther down the road I want to add a set of additional columns to this DataFrame.  Is there a way to add them all simultaneously, as in\n\nadditional_data = {'col_3': [8, 9, 10, 11],\n                   'col_4': [12, 13, 14, 15]}\n#Below is a made-up function of the kind I desire.\ndf.add_data(additional_data)\n\n\nI'm aware I could do this:\n\nfor key, value in additional_data.iteritems():\n    df[key] = value\n\n\nOr this:\n\ndf2 = pd.DataFrame(additional_data, index=df.index)\ndf = pd.merge(df, df2, on=df.index)\n\n\nI was just hoping for something cleaner.  If I'm stuck with these two options, which is preferred?\n"
'Here is my Pandas data frame:\n\nprices = pandas.DataFrame([1035.23, 1032.47, 1011.78, 1010.59, 1016.03, 1007.95, \n              1022.75, 1021.52, 1026.11, 1027.04, 1030.58, 1030.42,\n              1036.24, 1015.00, 1015.20])\n\n\nHere is my daily_return function:\n\ndef daily_return(prices):\n    return prices[:-1] / prices[1:] - 1\n\n\nHere is output that comes from this function:\n\n0    NaN\n1      0\n2      0\n3      0\n4      0\n5      0\n6      0\n7      0\n8      0\n9      0\n10     0\n11     0\n12     0\n13     0\n14   NaN\n\n\nWhy am I having this output?\n'
"I'm trying to configure my IPython output in my OS X terminal, but it would seem that none of the changes I'm trying to set are taking effect. I'm trying to configure the display settings such that wider outputs like a big DataFrame will output without any truncation or as the summary info.\n\nAfter importing pandas into my script, I have a few options set where I tried a whole bunch, but any one (or all, for that matter) does not seem to take effect. I'm running the script from IPython using %run. Am I doing something wrong here?\n\nimport pandas as pd\n\npd.set_option('display.expand_max_repr', False)\npd.set_option('display.max_columns', 30)\npd.set_option('display.width', None)\npd.set_option('display.line_width', 200)\n\n\nI've looked at some threads on Stack and the pandas FAQ to no avail, even when using these under the display namespace (or without), as I've attempted here. \n\nI understand that there are some ways around this, such as calling to_string() or describe() methods on your output, but these are very manual, and don't always work as intended in some cases, like one where I have calling to_string() on a groupby object yields:\n\n    id       type\n106125       puzzle       gameplay_id  sitting_id  user_id           ...\n106253       frames       gameplay_id  sitting_id  user_id           ...\n106260       trivia       gameplay_id  sitting_id  user_id           ...\n\n\nMy terminal window size is more than sufficient to accommodate the width, and calling pd.util.terminal.get_terminal_size() is correctly finding the window size tuple, so it would seem that auto detecting the size isn't working either. Any insight would be appreciated!\n"
'I have two columns in my dataset, col1 and col2. I want group the data as per col1 and then sort the data as per the size of each group. That is, I want to display groups in ascending order of their size.\n\nI have written the code for grouping and displaying the data as follows:\n\ngrouped_data = df.groupby(\'col1\')\n"""code for sorting comes here"""\nfor name,group in grouped_data:\n          print (name)\n          print (group)\n\n\nBefore displaying the data, I need to sort it as per group size, which I am not able to do. \n'
"What is the Pythonic/pandas way of sorting 'levels' within a column in pandas to give a specific ordering of bars in bar plot.\n\nFor example, given:\n\nimport pandas as pd\ndf = pd.DataFrame({\n    'group': ['a', 'a', 'a', 'a', 'a', 'a', 'a', \n              'b', 'b', 'b', 'b', 'b', 'b', 'b'],\n    'day': ['Mon', 'Tues', 'Fri', 'Thurs', 'Sat', 'Sun', 'Weds',\n            'Fri', 'Sun', 'Thurs', 'Sat', 'Weds', 'Mon', 'Tues'],\n    'amount': [1, 2, 4, 2, 1, 1, 2, 4, 5, 3, 4, 2, 1, 3]})\ndfx = df.groupby(['group'])\ndfx.plot(kind='bar', x='day')\n\n\nI can generate the following pair of plots:\n\n\n\nThe order of the bars follows the row order.\n\nWhat's the best way of reordering the data so that the bar charts have bars ordered Mon-Sun?\n\nUPDATE: this rubbish solution works - but it's far from elegant in the way it uses an extra sorting column:\n\ndf2 = pd.DataFrame({\n    'day': ['Mon', 'Tues', 'Weds', 'Thurs', 'Fri', 'Sat', 'Sun'],\n    'num': [0, 1, 2, 3, 4, 5, 6]})\ndf = pd.merge(df, df2, on='day')\ndf = df.sort_values('num')\ndfx = df.groupby(['group'])\ndfx.plot(kind='bar', x='day')\n\n\nFURTHER GENERALISATION:\n\nIs there a solution that also fixes the order of bars in a 'dodged' bar plot:\n\ndf.pivot('day', 'group', 'amount').plot(kind='bar')\n\n\n\n"
'I have a DataFrame which looks like below. I am trying to count the number of elements less than 2.0 in each column, then I will visualize the result in a bar plot. I did it using lists and loops, but I wonder if there is a "Pandas way" to do this quickly. Thanks!\n\nx = []\nfor i in range(6):\n    x.append(df[df.ix[:,i]&lt;2.0].count()[i])\n\n\nthen I can get a bar plot using list x.\n\n          A          B          C          D          E          F \n0       2.142      1.929      1.674      1.547      3.395      2.382  \n1       2.077      1.871      1.614      1.491      3.110      2.288  \n2       2.098      1.889      1.610      1.487      3.020      2.262    \n3       1.990      1.760      1.479      1.366      2.496      2.128    \n4       1.935      1.765      1.656      1.530      2.786      2.433\n\n'
'I have a DataFrame where I would like to keep the rows when a particular variable has a NaN value and drop the non-missing values.\nExample:\nticker  opinion  x1       x2  \naapl    GC       100      70  \nmsft    NaN      50       40  \ngoog    GC       40       60  \nwmt     GC       45       15  \nabm     NaN      80       90  \n\nIn the above DataFrame, I would like to drop all observations where opinion is not missing (so, I would like to drop the rows where ticker is aapl, goog, and wmt).\nIs there anything in pandas that is the opposite to .dropna()?\n'
'What is the best way, given a pandas dataframe, df, to get the correlation between its columns df.1 and df.2? \n\nI do not want the output to count rows with NaN, which pandas built-in correlation does. But I also want it to output a pvalue or a standard error, which the built-in does not.\n\nSciPy seems to get caught up by the NaNs, though I believe it does report significance.\n\nData example:\n\n     1           2\n0    2          NaN\n1    NaN         1\n2    1           2\n3    -4          3\n4    1.3         1\n5    NaN         NaN\n\n'
'I am using pandas version 0.14.1 with Python 2.7.5, and I have a data frame with three columns, e.g.:\n\nimport pandas as pd\n\nd = {\'L\':  [\'left\', \'right\', \'left\', \'right\', \'left\', \'right\'],\n     \'R\': [\'right\', \'left\', \'right\', \'left\', \'right\', \'left\'],\n     \'VALUE\': [-1, 1, -1, 1, -1, 1]}\ndf = pd.DataFrame(d)\n\nidx = (df[\'VALUE\'] == 1)\n\n\nresults in a data frame which looks like this:\n\n       L      R  VALUE\n0   left  right     -1\n1  right   left      1\n2   left  right     -1\n3  right   left      1\n4   left  right     -1\n5  right   left      1\n\n\nFor rows where VALUE == 1, I would like to swap the contents of the left and right columns, so that all of the "left" values will end up under the "L" column, and the "right" values end up under the "R" column.\n\nHaving already defined the idx variable above, I can easily do this in just three more lines, by using a temporary variable as follows:\n\ntmp = df.loc[idx,\'L\']\ndf.loc[idx,\'L\'] = df.loc[idx,\'R\']\ndf.loc[idx,\'R\'] = tmp\n\n\nhowever this seems like really clunky and inelegant syntax to me; surely pandas supports something more succinct?  I\'ve noticed that if I swap the column order in the input to the data frame .loc attribute, then I get the following swapped output:\n\nIn [2]: print(df.loc[idx,[\'R\',\'L\']])\n      R      L\n1  left  right\n3  left  right\n5  left  right\n\n\nThis suggests to me that I should be able to implement the same swap as above, by using just the following single line:\n\ndf.loc[idx,[\'L\',\'R\']] = df.loc[idx,[\'R\',\'L\']]\n\n\nHowever when I actually try this, nothing happens--the columns remain unswapped.  It\'s as if pandas automatically recognizes that I\'ve put the columns in the wrong order on the right hand side of the assignment statement, and it automatically corrects for the problem.  Is there a way that I can disable this "column order autocorrection" in pandas assignment statements, in order to implement the swap without creating unnecessary temporary variables?\n'
'I\'m trying to generate a new column in a pandas DataFrame that equals values in another pandas DataFrame.  When I attempt to create the new column I just get NaNs for the new column values.\n\nFirst I use an API call to get some data, and the \'mydata\' DataFrame is one column of data indexed by dates\n\nmydata = Quandl.get(["YAHOO/INDEX_MXX.4"],\n                    trim_start="2001-04-01", trim_end="2014-03-31",\n                    collapse="monthly")\n\n\nThe next DataFrame I get from a CSV with the following code, and it contains many columns of data with the same number of rows as \'mydata\'\n\nDWDATA = pandas.DataFrame.from_csv("filename",\n                                   header=0,\n                                   sep=\',\',\n                                   index_col=0,\n                                   parse_dates=True,\n                                   infer_datetime_format=True)\n\n\nI then try to generate the new column like this:\n\nDWDATA[\'MXX\'] = mydata.iloc[:,0]\n\n\nAgain, I just get NaN values.  Can someone help me understand why it\'s doing this and how to resolve? From what I\'ve read it looks like I might have something wrong with my indexes.  The indexes are dates in each DataFrame, but \'mydata\' have end-of-month dates while \'DWDATA\' has beginning-of-month dates.\n'
"Python 3.4 and Pandas 0.15.0\n\ndf is a dataframe and col1 is a column. With the code below, I'm checking for the presence of the value 10 and replacing such values with 1000. \n\ndf.col1[df.col1 == 10] = 1000\n\n\nHere's another example. This time, I'm changing values in col2 based on index.\n\ndf.col2[df.index == 151] = 500\n\n\nBoth these produce the warning below:\n\n-c:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\n\nFinally,\n\ncols = ['col1', 'col2', 'col3']\ndf[cols] = df[cols].applymap(some_function)\n\n\nThis produces a similar warning, with an added suggestion:\n\nTry using .loc[row_indexer,col_indexer] = value instead\n\n\nI'm not sure I understand the discussion pointed to in the warnings. What would be a better way to write these three lines of code?\n\nNote that the operations worked.\n"
"I have a series with some datetimes (as strings) and some nulls as 'nan':\n\nimport pandas as pd, numpy as np, datetime as dt\ndf = pd.DataFrame({'Date':['2014-10-20 10:44:31', '2014-10-23 09:33:46', 'nan', '2014-10-01 09:38:45']})\n\n\nI'm trying to convert these to datetime:\n\ndf['Date'] = df['Date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n\n\nbut I get the error:\n\ntime data 'nan' does not match format '%Y-%m-%d %H:%M:%S'\n\n\nSo I try to turn these into actual nulls:\n\ndf.ix[df['Date'] == 'nan', 'Date'] = np.NaN\n\n\nand repeat:\n\ndf['Date'] = df['Date'].apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n\n\nbut then I get the error:\n\n\n  must be string, not float\n\n\nWhat is the quickest way to solve this problem?\n"
'How do I stack the following 2 dataframes:\n\ndf1\n    hzdept_r    hzdepb_r    sandtotal_r\n0   0           114         0\n1   114         152         92.1\n\ndf2\n    hzdept_r    hzdepb_r    sandtotal_r\n0   0           23          83.5\n1   23          152         45\n\n\nto give the following result:\n\n    hzdept_r    hzdepb_r    sandtotal_r\n0   0           114         0\n1   114         152         92.1\n2   0           23          83.5\n3   23          152         45\n\n\nUsing the pandas merge operations does not work since it just lines the dataframes horizontally (and not vertically, which is what I want)\n'
'I am looking to write a quick script that will run through a csv file with two columns and provide me the rows in which the values in column B switch from one value to another: \n\neg: \n\ndataframe:\n\n# |  A  |  B  \n--+-----+-----\n1 |  2  |  3\n2 |  3  |  3\n3 |  4  |  4\n4 |  5  |  4\n5 |  5  |  4\n\n\nwould tell me that the change happened between row 2 and row 3. I know how to get these values using for loops but I was hoping there was a more pythonic way of approaching this problem.\n'
'I am trying to use a loop function to create a matrix of whether a product was seen in a particular week.\n\nEach row in the df (representing a product) has a close_date (the date the product closed) and a week_diff (the number of weeks the product was listed).\n\nimport pandas\nmydata = [{\'subid\' : \'A\', \'Close_date_wk\': 25, \'week_diff\':3},\n          {\'subid\' : \'B\', \'Close_date_wk\': 26, \'week_diff\':2},\n          {\'subid\' : \'C\', \'Close_date_wk\': 27, \'week_diff\':2},]\ndf = pandas.DataFrame(mydata)\n\n\nMy goal is to see how many alternative products were listed for each product in each date_range\n\nI have set up the following loop:\n\nfor index, row in df.iterrows():\n    i = 0\n    max_range = row[\'Close_date_wk\']    \n    min_range = int(row[\'Close_date_wk\'] - row[\'week_diff\'])\n    for i in range(min_range,max_range):\n        col_head = \'job_week_\'  +  str(i)\n        row[col_head] = 1\n\n\nCan you please help explain why the "row[col_head] = 1" line is neither adding a column, nor adding a value to that column for that row. \n\nFor example, if:\n\nrow A has date range 1,2,3 \nrow B has date range 2,3  \nrow C has date range 3,4,5\'\n\n\nthen ideally I would like to end up with\n\nrow A has 0 alternative products in week 1\n          1 alternative products in week 2\n          2 alternative products in week 3\nrow B has 1 alternative products in week 2\n          2 alternative products in week 3\n&amp;c..\n\n'
'I would like to give a pandas dataframe to Bokeh to plot a line chart with multiple lines.\n\nThe x-axis should be the df.index and each df.columns should be a separate line.\n\nThis is what I would like to do:\n\nimport pandas as pd\nimport numpy as np\nfrom bokeh.plotting import figure, show\n\ntoy_df = pd.DataFrame(data=np.random.rand(5,3), columns = (\'a\', \'b\' ,\'c\'), index = pd.DatetimeIndex(start=\'01-01-2015\',periods=5, freq=\'d\'))   \n\np = figure(width=1200, height=900, x_axis_type="datetime") \np.multi_line(df)\nshow(p)\n\n\nHowever, I get the error: \n\nRuntimeError: Missing required glyph parameters: ys\n\n\nInstead, I\'ve managed to do this:\n\nimport pandas as pd\nimport numpy as np\nfrom bokeh.plotting import figure, show\n\ntoy_df = pd.DataFrame(data=np.random.rand(5,3), columns = (\'a\', \'b\' ,\'c\'), index = pd.DatetimeIndex(start=\'01-01-2015\',periods=5, freq=\'d\'))   \n\nts_list_of_list = []\nfor i in range(0,len(toy_df.columns)):\n    ts_list_of_list.append(toy_df.index)\n\nvals_list_of_list = toy_df.values.T.tolist()\n\np = figure(width=1200, height=900, x_axis_type="datetime") \np.multi_line(ts_list_of_list, vals_list_of_list)\nshow(p)\n\n\nThat (ineligantly) does the job but it uses the same color for all 3 lines, see below:\n\n\n\nQuestions:\n\n1) How can I pass a pandas dataframe to bokeh\'s multi_line?\n\n2) If not possible directly, how can I manipulate the dataframe data so that multi_line will create each line with a different color?\n\nThanks in advance.\n'
'I have the following code\n\n from sklearn.ensemble import ExtraTreesClassifier\n from sklearn.cross_validation import cross_val_score\n #split the dataset for train and test\n combnum[\'is_train\'] = np.random.uniform(0, 1, len(combnum)) &lt;= .75\n train, test = combnum[combnum[\'is_train\']==True], combnum[combnum[\'is_train\']==False]\n\n et = ExtraTreesClassifier(n_estimators=200, max_depth=None, min_samples_split=10, random_state=0)\n min_samples_split=10, random_state=0  )\n\n labels = train[list(label_columns)].values\n tlabels = test[list(label_columns)].values\n\n features = train[list(columns)].values\n tfeatures = test[list(columns)].values\n\n et_score = cross_val_score(et, features, labels, n_jobs=-1)\n print("{0} -&gt; ET: {1})".format(label_columns, et_score))\n\n\nChecking the shape of the arrays:\n\n features.shape\n Out[19]:(43069, 34)\n\n\nAnd\n\nlabels.shape\nOut[20]:(43069, 1)\n\n\nand I\'m getting:\n\nIndexError: too many indices for array\n\n\nand this relevant part of the traceback:\n\n---&gt; 22 et_score = cross_val_score(et, features, labels, n_jobs=-1)\n\n\nI\'m creating the data from Pandas dataframes and I searched here and saw some reference to possible errors via this method but can\'t figure out how to correct?\nWhat the data arrays look like:\nfeatures\n\nOut[21]:\narray([[ 0.,  1.,  1., ...,  0.,  0.,  1.],\n   [ 0.,  1.,  1., ...,  0.,  0.,  1.],\n   [ 1.,  1.,  1., ...,  0.,  0.,  1.],\n   ..., \n   [ 0.,  0.,  1., ...,  0.,  0.,  1.],\n   [ 0.,  0.,  1., ...,  0.,  0.,  1.],\n   [ 0.,  0.,  1., ...,  0.,  0.,  1.]])\n\n\nlabels\n\nOut[22]:\narray([[1],\n   [1],\n   [1],\n   ..., \n   [1],\n   [1],\n   [1]])\n\n'
"I have a dataframe which was created via a df.pivot:\n\ntype                             start  end\nF_Type         to_date                     \nA              20150908143000    345    316\nB              20150908140300    NaN    480\n               20150908140600    NaN    120\n               20150908143000  10743   8803\nC              20150908140100    NaN   1715\n               20150908140200    NaN   1062\n               20150908141000    NaN    145\n               20150908141500    418    NaN\n               20150908141800    NaN    450\n               20150908142900   1973   1499\n               20150908143000  19522  16659\nD              20150908143000    433     65\nE              20150908143000   7290   7375\nF              20150908143000      0      0\nG              20150908143000   1796    340\n\n\nI would like to filter and return a single row for each 'F_TYPE' only returning the row with the Maximum 'to_date'. I would like to return the following dataframe:\n\ntype                             start  end\nF_Type         to_date                     \nA              20150908143000    345    316\nB              20150908143000  10743   8803\nC              20150908143000  19522  16659\nD              20150908143000    433     65\nE              20150908143000   7290   7375\nF              20150908143000      0      0\nG              20150908143000   1796    340\n\n\nThanks..\n"
'Hello I have the following dataframe\n\ndf = \n\n       Record_ID       Time\n        94704   2014-03-10 07:19:19.647342\n        94705   2014-03-10 07:21:44.479363\n        94706   2014-03-10 07:21:45.479581\n        94707   2014-03-10 07:21:54.481588\n        94708   2014-03-10 07:21:55.481804\n\n\nIs it possible to the have following?\n\ndf1 = \n\n       Record_ID       Time\n        94704   2014-03-10 07:19:19\n        94705   2014-03-10 07:21:44\n        94706   2014-03-10 07:21:45\n        94707   2014-03-10 07:21:54\n        94708   2014-03-10 07:21:55\n\n'
"I am  trying to learn pandas but i have been puzzled with the following please. I want to replace NaNs is a dataframe with the row average. Hence something like df.fillna(df.mean(axis=1)) should work but for some reason it fails for me. Am I missing anything please, something I'm doing wrong? Is is because its not implemented; see link here\n\nimport pandas as pd\nimport numpy as np\n\u200b\npd.__version__\nOut[44]:\n'0.15.2'\n\nIn [45]:\ndf = pd.DataFrame()\ndf['c1'] = [1, 2, 3]\ndf['c2'] = [4, 5, 6]\ndf['c3'] = [7, np.nan, 9]\ndf\n\nOut[45]:\n    c1  c2  c3\n0   1   4   7\n1   2   5   NaN\n2   3   6   9\n\nIn [46]:  \ndf.fillna(df.mean(axis=1)) \n\nOut[46]:\n    c1  c2  c3\n0   1   4   7\n1   2   5   NaN\n2   3   6   9\n\n\nHowever something like this looks to work fine\n\ndf.fillna(df.mean(axis=0)) \n\nOut[47]:\n    c1  c2  c3\n0   1   4   7\n1   2   5   8\n2   3   6   9\n\n"
'I have a DataFrame with multiple rows. Is there any way in which they can be combined to form one string?\n\nFor example: \n\n     words\n0    I, will, hereby\n1    am, gonna\n2    going, far\n3    to\n4    do\n5    this\n\n\nExpected output:\n\nI, will, hereby, am, gonna, going, far, to, do, this\n\n'
"I have a large csv file, about 600mb with 11 million rows and I want to create statistical data like pivots, histograms, graphs etc. Obviously trying to just to read it normally:\n\ndf = pd.read_csv('Check400_900.csv', sep='\\t')\n\n\ndoesn't work so I found iterate and chunksize in a similar post so I used \n\ndf = pd.read_csv('Check1_900.csv', sep='\\t', iterator=True, chunksize=1000)\n\n\nAll good, i can for example print df.get_chunk(5)  and search the whole file with just \n\nfor chunk in df:\n    print chunk\n\n\nMy problem is I don't know how to use stuff like these below for the whole df and not for just one chunk\n\nplt.plot()\nprint df.head()\nprint df.describe()\nprint df.dtypes\ncustomer_group3 = df.groupby('UserID')\ny3 = customer_group.size()\n\n\nI hope my question is not so confusing\n"
"I have a long string of pandas chained commands, for example:\n\ndf.groupby[['x','y']].apply(lambda x: (np.max(x['z'])-np.min(x['z']))).sort_values(ascending=False)\n\n\nAnd I would like to be able to present it across multiple lines but still as a one liner (without saving results to a temporary object, or defining the lambda as a function)\n\nan example of how I would like it to look:\n\ndf.groupby[['x','y']]\n.apply(lambda x: (np.max(x['z'])-np.min(x['z'])))\n.sort_values(ascending=False)\n\n\nIs it possible to do so? (I know '_' has this functionality in python, but it doesn't seem to work with chained commands)\n"
'I\'m trying to extract US states from wiki URL, and for which I\'m using Python Pandas. \n\nimport pandas as pd\nimport html5lib\nf_states = pd.read_html(\'https://simple.wikipedia.org/wiki/List_of_U.S._states\') \n\n\nHowever, the above code is giving me an error L\n\n\n  ImportError                               Traceback (most recent call last)\n   in ()\n        1 import pandas as pd\n  ----> 2 f_states = pd.read_html(\'https://simple.wikipedia.org/wiki/List_of_U.S._states\')\n  \n  if flavor in (\'bs4\', \'html5lib\'):\n      662         if not _HAS_HTML5LIB:\n  --> 663             raise ImportError("html5lib not found, please install it")\n      664         if not _HAS_BS4:\n      665             raise ImportError("BeautifulSoup4 (bs4) not found, please install it")\n  ImportError: html5lib not found, please install it\n\n\nI installed html5lib and beautifulsoup4 as well, but it is not working. \nCan someone help pls.\n'
"I have an indexed pandas dataframe. By searching through its index, I find a row of interest. How do I find out the iloc of this row?\n\nExample:\n\ndates = pd.date_range('1/1/2000', periods=8)\ndf = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])\ndf\n                   A         B         C         D\n2000-01-01 -0.077564  0.310565  1.112333  1.023472\n2000-01-02 -0.377221 -0.303613 -1.593735  1.354357\n2000-01-03  1.023574 -0.139773  0.736999  1.417595\n2000-01-04 -0.191934  0.319612  0.606402  0.392500\n2000-01-05 -0.281087 -0.273864  0.154266  0.374022\n2000-01-06 -1.953963  1.429507  1.730493  0.109981\n2000-01-07  0.894756 -0.315175 -0.028260 -1.232693\n2000-01-08 -0.032872 -0.237807  0.705088  0.978011\n\nwindow_stop_row = df[df.index &lt; '2000-01-04'].iloc[-1]\nwindow_stop_row\nTimestamp('2000-01-08 00:00:00', offset='D')\n#which is the iloc of window_stop_row?\n\n"
"I'm trying to use the pivot_table method of a pandas DataFrame;\n\nmean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\n\nHowever, I receive the following error:\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-55-cb4d494f2f39&gt; in &lt;module&gt;()\n----&gt; 1 mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\nTypeError: pivot_table() got an unexpected keyword argument 'rows'\n\n\nThe above command was taken from the book 'Python for Data Analysis' by Wes McKinney (the creator of pandas)\n"
"Lets say I have a dataframe df as \n\nA B\n1 V2\n3 W42\n1 S03\n2 T02\n3 U71\n\n\nI want to have a new column (either at it the end of df or replace column B with it, as it doesn't matter) that only extracts the int from the column B. That is I want column C to look like\n\nC\n2\n42\n3\n2\n71\n\n\nSo if there is a 0 in front of the number, such as for 03, then I want to return 3 not 03\n\nHow can I do this?\n"
"I have the following two dataframes that I have set date to DateTime Index df.set_index(pd.to_datetime(df['date']), inplace=True) and would like to merge or join on date:\n\ndf.head(5)\n        catcode_amt type    feccandid_amt   amount\ndate                \n1915-12-31  A5000   24K     H6TX08100   1000\n1916-12-31  T6100   24K     H8CA52052   500\n1954-12-31  H3100   24K     S8AK00090   1000\n1985-12-31  J7120   24E     H8OH18088   36\n1997-12-31  z9600   24K     S6ND00058   2000\n\n\nd.head(5)\n         catcode_disp disposition   feccandid_disp  bills\ndate                \n2007-12-31  A0000   support     S4HI00011               1\n2007-12-31  A1000   oppose      S4IA00020', 'P20000741  1\n2007-12-31  A1000   support     S8MT00010               1\n2007-12-31  A1500   support     S6WI00061               2\n2007-12-31  A1600   support     S4IA00020', 'P20000741  3\n\n\nI have tried the following two methods but both return a MemoryError:\n\ndf.join(d, how='right')\n\n\nI use the code below on dataframes that dont have date set to index.\n\nmerge=pd.merge(df,d, how='inner', on='date')\n\n"
"I need to combine multiple rows into a single row, that would be simple concat with space\n\n    View of my dataframe:\n  tempx        value\n0  picture1         1.5\n1  picture555       1.5\n2  picture255       1.5\n3  picture365       1.5\n4  picture112       1.5\n\n\nI want the dataframe to be converted like this: (space separated)\ntempx values\n\n  Expected output:\n  tempx                                                       value\n  0     picture1 picture555 picture255 picture365 picture112  1.5\n\n  or\n  as a python dict\n  {1.5:{picture1 picture555 picture255 picture365 picture112}}\n\n\nWhat I have tried :\n\n df_test['tempx']=df_test['tempx'].str.cat(sep=' ')\n\n\nthis works but it combines the rows in all the columns like this:\n\n      tempx        value\n0  picture1 picture555 picture255 picture365 picture112 1.5\n1  picture1 picture555 picture255 picture365 picture112 1.5\n2  picture1 picture555 picture255 picture365 picture112 1.5\n3  picture1 picture555 picture255 picture365 picture112 1.5\n4  picture1 picture555 picture255 picture365 picture112 1.5\n\n\nIs there any elegant solution?\n"
"I've got a pandas DataFrame with a boolean column sorted by another column and need to calculate reverse cumulative sum of the boolean column, that is, amount of true values from current row to bottom.\n\nExample\n\nIn [13]: df = pd.DataFrame({'A': [True] * 3 + [False] * 5, 'B': np.random.rand(8) })\n\nIn [15]: df = df.sort_values('B')\n\nIn [16]: df\nOut[16]:\n       A         B\n6  False  0.037710\n2   True  0.315414\n4  False  0.332480\n7  False  0.445505\n3  False  0.580156\n1   True  0.741551\n5  False  0.796944\n0   True  0.817563\n\n\nI need something that will give me a new column with values\n\n3\n3\n2\n2\n2\n2\n1\n1\n\n\nThat is, for each row it should contain amount of True values on this row and rows below.\n\nI've tried various methods using .iloc[::-1] but result is not that is desired.\n\nIt looks like I'm missing some obvious bit of information. I've starting using Pandas only yesterday.\n"
'How to assert that the following two dataframes df1 and df2 are equal?\n\nimport pandas as pd\ndf1 = pd.DataFrame([1, 2, 3])\ndf2 = pd.DataFrame([1.0, 2, 3])\n\n\nThe output of df1.equals(df2) is False.\nAs of now, I know two ways:\n\nprint (df1 == df2).all()[0]\n\n\nor\n\ndf1 = df1.astype(float)\nprint df1.equals(df2)\n\n\nIt seems a little bit messy. Is there a better way to do this comparison?\n'
"I have a dataframe\n\ndf = pd.DataFrame([\n        ['2', '3', 'nan'],\n        ['0', '1', '4'],\n        ['5', 'nan', '7']\n    ])\n\nprint df\n\n   0    1    2\n0  2    3  nan\n1  0    1    4\n2  5  nan    7\n\n\nI want to convert these strings to numbers and sum the columns and convert back to strings.\n\nUsing astype(float) seems to get me to the number part.  Then summing is easy with sum().  Then back to strings should be easy too with astype(str)\n\ndf.astype(float).sum().astype(str)\n\n0     7.0\n1     4.0\n2    11.0\ndtype: object\n\n\nThat's almost what I wanted.  I wanted the string version of integers.  But floats have decimals.  How do I get rid of them?\n\nI want this\n\n0     7\n1     4\n2    11\ndtype: object\n\n"
"I have a dataset with personal data such as name, height, weight and date of birth. I would build a graph with the number of people born in a particular month and year. I'm using python pandas to accomplish this and my strategy was to try to group by year and month and add using count. But the closest I got is to get the count of people by year or by month but not by both.\n\ndf['birthdate'].groupby(df.birthdate.dt.year).agg('count')\n\n\nOther questions in stackoverflow point to a Grouper called TimeGrouper but searching in pandas documentation found nothing. Any idea?\n"
'I\'ve looked through a bunch of questions and answers related to this issue, but I\'m still finding that I\'m getting this copy of slice warning in places where I don\'t expect it. Also, it\'s cropping up in code that was running fine for me previously, leading me to wonder if some sort of update may be the culprit.  \n\nFor example, this is a set of code where all I\'m doing is reading in an Excel file into a pandas DataFrame, and cutting down the set of columns included with the df[[]] syntax.  \n\n izmir = pd.read_excel(filepath)\n izmir_lim = izmir[[\'Gender\',\'Age\',\'MC_OLD_M&gt;=60\',\'MC_OLD_F&gt;=60\',\'MC_OLD_M&gt;18\',\'MC_OLD_F&gt;18\',\'MC_OLD_18&gt;M&gt;5\',\'MC_OLD_18&gt;F&gt;5\',\n               \'MC_OLD_M_Child&lt;5\',\'MC_OLD_F_Child&lt;5\',\'MC_OLD_M&gt;0&lt;=1\',\'MC_OLD_F&gt;0&lt;=1\',\'Date to Delivery\',\'Date to insert\',\'Date of Entery\']]\n\n\nNow, any further changes I make to this izmir_lim file raise the copy of slice warning.  \n\nizmir_lim[\'Age\'] = izmir_lim.Age.fillna(0)\nizmir_lim[\'Age\'] = izmir_lim.Age.astype(int)\n\n\n\n  /Users/samlilienfeld/anaconda/lib/python3.5/site-packages/ipykernel/main.py:2:\n  SettingWithCopyWarning:   A value is trying to be set on a copy of a\n  slice from a DataFrame.   Try using .loc[row_indexer,col_indexer] =\n  value instead\n\n\nI\'m confused because I thought the df[[]] column subsetting returned a copy by default.  The only way I\'ve found to suppress the errors is by explicitly adding df[[]].copy(). I could have sworn that in the past I did not have to do that and did not raise the copy of slice error.\n\nSimilarly, I have some other code that runs a function on a dataframe to filter it in certain ways:\n\ndef lim(df):\nif (geography == "All"):\n    df_geo = df\nelse:\n    df_geo = df[df.center_JO == geography]\n\ndf_date = df_geo[(df_geo.date_survey &gt;= start_date) &amp; (df_geo.date_survey &lt;= end_date)]\n\nreturn df_date\n\ndf_lim = lim(df)\n\n\nFrom this point forward, any changes I make to any of the values of df_lim raise the copy of slice error. The only way around it that i\'ve found is to change the function call to:\n\ndf_lim = lim(df).copy()\n\n\nThis just seems wrong to me. What am I missing? It seems like these use cases should return copies by default, and I could have sworn that the last time I ran these scripts I was not running in to these errors.\nDo I just need to start adding .copy() all over the place? Seems like there should be a cleaner way to do this. Any insight or help is much appreciated.\n'
'The pandas style option to add a background gradient is great for quickly inspecting my output table. However, it is applied either row-wise or columns-wise. Would it be possible to apply it to the whole dataframe at once?\n\nEDIT: A minimum working example:\n\ndf = pd.DataFrame([[3,2,10,4],[20,1,3,2],[5,4,6,1]])\ndf.style.background_gradient()\n\n'
"I am currently merging 2 dataframes with an outer join, but after merging, I see all the rows are duplicated even when the columns I did the merge upon contain the same values. In detail:\n\nlist_1 = pd.read_csv('list_1.csv')\nlist_2 = pd.read_csv('list_2.csv')\n\nmerged_list = pd.merge(list_1 , list_2 , on=['email_address'], how='inner')\n\n\nwith the following input and results:\n\nlist_1:\n\nemail_address, name, surname\njohn.smith@email.com, john, smith\njohn.smith@email.com, john, smith\nelvis@email.com, elvis, presley\n\n\nlist_2:\n\nemail_address, street, city\njohn.smith@email.com, street1, NY\njohn.smith@email.com, street1, NY\nelvis@email.com, street2, LA\n\n\nmerged_list:\n\nemail_address, name, surname, street, city\njohn.smith@email.com, john, smith, street1, NY\njohn.smith@email.com, john, smith, street1, NY\njohn.smith@email.com, john, smith, street1, NY\njohn.smith@email.com, john, smith, street1, NY\nelvis@email.com, elvis, presley, street2, LA\nelvis@email.com, elvis, presley, street2, LA\n\n\nMy question is, shouldn't it be like this?\n\nmerged_list (how I would like it to be :D):\n\nemail_address, name, surname, street, city\njohn.smith@email.com, john, smith, street1, NY\njohn.smith@email.com, john, smith, street1, NY\nelvis@email.com, elvis, presley, street2, LA\n\n\nHow can I make it so that it becomes like this?\nThanks a lot for your help!\n"
"For a list of numbers ranging from x to y that may contain NaN, how can I normalise between 0 and 1, ignoring the NaN values (they stay as NaN).\n\nTypically I would use MinMaxScaler (ref page) from sklearn.preprocessing, but this cannot handle NaN and recommends imputing the values based on mean or median etc. it doesn't offer the option to ignore all the NaN values.\n"
'I want to apply a custom function and create a derived column called population2050 that is based on two columns already present in my data frame.\n\nimport pandas as pd\nimport sqlite3\nconn = sqlite3.connect(\'factbook.db\')\nquery = "select * from facts where area_land =0;"\nfacts = pd.read_sql_query(query,conn)\nprint(list(facts.columns.values))\n\ndef final_pop(initial_pop,growth_rate):\n    final = initial_pop*math.e**(growth_rate*35)\n    return(final)\n\nfacts[\'pop2050\'] = facts[\'population\',\'population_growth\'].apply(final_pop,axis=1)\n\n\nWhen I run the above code, I get an error. Am I not using the \'apply\' function correctly?\n'
"I have a date column (called 'Time') which contains days/hours/mins etc (timedelta). I have created a new column in my dataframe and I want to convert the 'Time' column into seconds and put it in the new column for each row.\n\nDoes anyone have any pointers? All I can find on the internet is how to convert your column, not create a new column and convert another one.\n\nThank you in advance!\n"
"I want to select rows that the values do not start with some str. For example, I have a pandas df, and I want to select data do not start with t, and c. In this sample, the output should be mext1 and okl1.\n\nimport pandas as pd\n\ndf=pd.DataFrame({'col':['text1','mext1','cext1','okl1']})\ndf\n\n    col\n0   text1\n1   mext1\n2   cext1\n3   okl1\n\n\nI want this:\n\n    col\n0   mext1\n1   okl1\n\n"
'I have a dataframe (df) that looks like this:\n\n+---------+-------+------------+----------+\n| subject | pills |    date    | strength |\n+---------+-------+------------+----------+\n|       1 |     4 | 10/10/2012 |      250 |\n|       1 |     4 | 10/11/2012 |      250 |\n|       1 |     2 | 10/12/2012 |      500 |\n|       2 |     1 | 1/6/2014   |     1000 |\n|       2 |     1 | 1/7/2014   |      250 |\n|       2 |     1 | 1/7/2014   |      500 |\n|       2 |     3 | 1/8/2014   |      250 |\n+---------+-------+------------+----------+\n\n\nWhen I use reshape in R, I get what I want: \n\nreshape(df, idvar = c("subject","date"), timevar = \'strength\', direction = "wide")\n\n+---------+------------+--------------+--------------+---------------+\n| subject |    date    | strength.250 | strength.500 | strength.1000 |\n+---------+------------+--------------+--------------+---------------+\n|       1 | 10/10/2012 | 4            | NA           | NA            |\n|       1 | 10/11/2012 | 4            | NA           | NA            |\n|       1 | 10/12/2012 | NA           | 2            | NA            |\n|       2 | 1/6/2014   | NA           | NA           | 1             |\n|       2 | 1/7/2014   | 1            | 1            | NA            |\n|       2 | 1/8/2014   | 3            | NA           | NA            |\n+---------+------------+--------------+--------------+---------------+\n\n\nUsing pandas:\n\ndf.pivot_table(df, index=[\'subject\',\'date\'],columns=\'strength\')\n\n+---------+------------+-------+----+-----+\n|         |            | pills            |\n+---------+------------+-------+----+-----+\n|         | strength   | 250   | 500| 1000|\n+---------+------------+-------+----+-----+\n| subject | date       |       |    |     |\n+---------+------------+-------+----+-----+\n| 1       | 10/10/2012 | 4     | NA | NA  |\n|         | 10/11/2012 | 4     | NA | NA  |\n|         | 10/12/2012 | NA    | 2  | NA  |\n+---------+------------+-------+----+-----+\n| 2       | 1/6/2014   | NA    | NA | 1   |\n|         | 1/7/2014   | 1     | 1  | NA  |\n|         | 1/8/2014   | 3     | NA | NA  |\n+---------+------------+-------+----+-----+\n\n\nHow do I get exactly the same output as in R with pandas? I only want 1 header.\n'
"I have a Data-frame df which is as follows:\n\n| date      | Revenue |\n|-----------|---------|\n| 6/2/2017  | 100     |\n| 5/23/2017 | 200     |\n| 5/20/2017 | 300     |\n| 6/22/2017 | 400     |\n| 6/21/2017 | 500     |\n\n\nI need to group the above data by month to get output as:\n\n| date | SUM(Revenue) |\n|------|--------------|\n| May  | 500          |\n| June | 1000         |\n\n\nI tried this code but it did not work:\n\ndf.groupby(month('date')).agg({'Revenue': 'sum'})\n\n\nI want to only use Pandas or Numpy and no additional libraries\n"
"I am trying to filter a df using several Boolean variables that are a part of the df, but have been unable to do so. \n\nSample data:\n\nA | B | C | D\nJohn Doe | 45 | True | False\nJane Smith | 32 | False | False\nAlan Holmes | 55 | False | True\nEric Lamar | 29 | True | True\n\n\nThe dtype for columns C and D is Boolean. I want to create a new df (df1)  with only the rows where either C or D is True. It should look like this:\n\nA | B | C | D\nJohn Doe | 45 | True | False\nAlan Holmes | 55 | False | True\nEric Lamar | 29 | True | True\n\n\nI've tried something like this, which faces issues because it cant handle the  Boolean type: \n\ndf1 = df[(df['C']=='True') or (df['D']=='True')]\n\n\nAny ideas?\n"
"Consider this simple example\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nimport matplotlib.dates as mdates\n\npd.__version__\nOut[147]: u'0.22.0'\n\nidx = pd.date_range('2017-01-01 05:03', '2017-01-01 18:03', freq = 'min')\n\ndf = pd.Series(np.random.randn(len(idx)),  index = idx)\ndf.head()\nOut[145]: \n2017-01-01 05:03:00   0.4361\n2017-01-01 05:04:00   0.9737\n2017-01-01 05:05:00   0.8430\n2017-01-01 05:06:00   0.4292\n2017-01-01 05:07:00   0.5739\nFreq: T, dtype: float64\n\n\nI want to plot this, and have ticks every hour. I use:\n\nfig, ax = plt.subplots()\nhours = mdates.HourLocator(interval = 1)  #\nh_fmt = mdates.DateFormatter('%H:%M:%S')\n\ndf.plot(ax = ax, color = 'black', linewidth = 0.4)\n\nax.xaxis.set_major_locator(hours)\nax.xaxis.set_major_formatter(h_fmt)\n\n\nwhich gives\n\n\n\nwhy dont the ticks appear every hour here? Thanks for your help!\n"
'I need to remove a column with label name at the time of loading a csv using pandas. I am reading csv as follows and want to add parameters inside it to do so. Thanks.\n\npd.read_csv("sample.csv")\n\nI know this to do after reading csv:\n\ndf.drop(\'name\', axis=1)\n\n'
"\nI'm new to python pandas. Need some help with deleting a few rows where there are null values. In the screenshot, I need to delete rows where charge_per_line == &quot;-&quot; using python pandas.\n"
'I have a problem filtering a pandas dataframe.\n\ncity \nNYC \nNYC \nNYC \nNYC \nSYD \nSYD \nSEL \nSEL\n...\n\ndf.city.value_counts()\n\n\nI would like to remove rows of cities that has less than 4 count frequency, which would be SYD and SEL for instance.\n\nWhat would be the way to do so without manually dropping them city by city?\n'
"I've got a table of clients (coper) and asset allocation (asset)\n\nA = [[1,2],[3,4],[5,6]]\nidx = ['coper1','coper2','coper3']\ncols = ['asset1','asset2']\n\ndf = pd.DataFrame(A,index = idx, columns = cols)\n\n\nso my data look like\n\n        asset1  asset2\ncoper1       1       2\ncoper2       3       4\ncoper3       5       6\n\n\nand I want to run them through a linear optimization (i've got constraints- somtehing like sum of all of asset_i &lt;= amount_on_hand_i and sum of coper_j = price_j)\n\nso I have to turn this 2D matrix into a 1D vector.  Which is easy with melt\n\ndf2 = pd.melt(df,value_vars=['asset1','asset2'])\n\n\nBut now, when I try to unmelt it, I get a 6-row array with lots of blanks!\n\ndf2.pivot(columns = 'variable', values = 'value')\n\n\nvariable  asset1  asset2\n0            1.0     NaN\n1            3.0     NaN\n2            5.0     NaN\n3            NaN     2.0\n4            NaN     4.0\n5            NaN     6.0\n\n\nIs there any way to preserve the 'coper' part of my indexing while using melt?\n"
"I have a df like this:\n\nframe = pd.DataFrame({'a' : ['a,b,c', 'a,c,f', 'b,d,f','a,z,c']})\n\n\nAnd a list of items:\n\nletters = ['a','c']\n\n\nMy goal is to get all the rows from frame that contain at least the 2 elements in letters\n\nI came up with this solution:\n\nfor i in letters:\n    subframe = frame[frame['a'].str.contains(i)]\n\n\nThis gives me what I want, but it might not be the best solution in terms of scalability.\nIs there any 'vectorised' solution?\nThanks\n"
"I have the following pandas dataframe :\ndf = pd.DataFrame([\n    ['A', 2017, 1],\n    ['A', 2019, 1],\n    ['B', 2017, 1],\n    ['B', 2018, 1],\n    ['C', 2016, 1],\n    ['C', 2019, 1],\n], columns=['ID', 'year', 'number'])\n\nand am looking for the most efficient way to fill the missing years with a default value of 0 for the column number\nThe expected output is:\n  ID  year  number\n0  A  2017       1\n1  A  2018       0\n2  A  2019       1\n3  B  2017       1\n4  B  2018       1\n5  C  2016       1\n6  C  2017       0\n7  C  2018       0\n8  C  2019       1\n\nThe dataframe that I have is relatively big, so I am looking for an efficient solution.\nEdit:\nThis is the code that I have so far:\nmin_max_dict = df[['ID', 'year']].groupby('ID').agg([min, max]).to_dict('index')\n\nnew_ix = [[], []]\nfor id_ in df['ID'].unique():\n    for year in range(min_max_dict[id_][('year', 'min')], min_max_dict[id_][('year', 'max')]+1): \n        new_ix[0].append(id_)\n        new_ix[1].append(year)\n\n\ndf.set_index(['ID', 'year'], inplace=True)\ndf = df.reindex(new_ix, fill_value=0).reset_index()\n\nResult\n  ID  year  number\n0  A  2017       1\n1  A  2018       0\n2  A  2019       1\n3  B  2017       1\n4  B  2018       1\n5  C  2016       1\n6  C  2017       0\n7  C  2018       0\n8  C  2019       1\n\n"
"I'm trying to do what I think is a straight froward operation in pandas but I can't seem to make it work.\n\nI have two pandas Series with different numbers of indices, I would like to add values together if they share an index, otherwise I would just like to pass the values that don't have corresponding indices along.\n\nFor example\n\nSr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\nSr2 = pd.Series([5,6], index = ['A', 'C'])\nSr1        Sr2\nA     1    A     5\nB     2    C     6\nC     3\nD     4\n\n\nSr1 + Sr2 or Sr1.add(Sr2) give\n\nA     6\nB   NaN\nC     9\nD   NaN\n\n\nBut what I want is\n\nA     6\nB     2\nC     9\nD     4\n\n\nwhere the B and D values for Sr1 are just passed along.\n\nAny suggestions?\n"
'It\'s pretty easy to write a function that computes the maximum drawdown of a time series.  It takes a small bit of thinking to write it in O(n) time instead of O(n^2) time.  But it\'s not that bad.  This will work:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef max_dd(ser):\n    max2here = pd.expanding_max(ser)\n    dd2here = ser - max2here\n    return dd2here.min()\n\n\nLet\'s set up a brief series to play with to try it out:\n\nnp.random.seed(0)\nn = 100\ns = pd.Series(np.random.randn(n).cumsum())\ns.plot()\nplt.show()\n\n\n\n\nAs expected, max_dd(s) winds up showing something right around -17.6.  Good, great, grand.  Now say I\'m interested in computing the rolling drawdown of this Series.  I.e. for each step, I want to compute the maximum drawdown from the preceding sub series of a specified length.  This is easy to do using pd.rolling_apply.  It works like so:\n\nrolling_dd = pd.rolling_apply(s, 10, max_dd, min_periods=0)\ndf = pd.concat([s, rolling_dd], axis=1)\ndf.columns = [\'s\', \'rol_dd_10\']\ndf.plot()\n\n\n\n\nThis works perfectly.  But it feels very slow.  Is there a particularly slick algorithm in pandas or another toolkit to do this fast?  I took a shot at writing something bespoke: it keeps track of all sorts of intermediate data (locations of observed maxima, locations of previously found drawdowns) to cut down on lots of redundant calculations.  It does save some time, but not a whole lot, and not nearly as much as should be possible.  \n\nI think it\'s because of all the looping overhead in Python/Numpy/Pandas.  But I\'m not currently fluent enough in Cython to really know how to begin attacking this from that angle.  I was hoping someone had tried this before.  Or, perhaps, that someone might want to have a look at my "handmade" code and be willing to help me convert it to Cython.\n\n\n\nEdit: \nFor anyone who wants a review of all the functions mentioned here (and some others!) have a look at the iPython notebook at: http://nbviewer.ipython.org/gist/8one6/8506455\n\nIt shows how some of the approaches to this problem relate, checks that they give the same results, and shows their runtimes on data of various sizes.\n\nIf anyone is interested, the "bespoke" algorithm I alluded to in my post is rolling_dd_custom.  I think that could be a very fast solution if implemented in Cython.\n'
'Given a DataFrame I would like to compute number of zeros per each row. How can I compute it with Pandas?\n\nThis is presently what I ve done, this returns indices of zeros\n\ndef is_blank(x):\n    return x == 0 \n\nindexer = train_df.applymap(is_blank)\n\n'
"I am trying to use pandas.Series.value_counts to get the frequency of values in a dataframe, so  I go through each column and get values_count , which gives me a series:\n\nI am struggling to convert this resultant series to a dict:\n\n groupedData = newData.groupby('class')\nfor k, group in groupedData:\n    dictClass[k] = {}\n    for eachlabel in dataLabels:\n        myobj = group[eachlabel].value_counts()\n        for eachone in myobj:\n            print type(myobj)\n            print myobj\n\n\n\n\nwhat I need is a dict :\n\n{'high': 3909 , 'average': 3688, 'less': '182 , 'veryless' : 62}\n"
"The pandas cut() documentation states that: &quot;Out of bounds values will be NA in the resulting Categorical object.&quot; This makes it difficult when the upper bound is not necessarily clear or important. For example:\ncut (weight, bins=[10,50,100,200])\n\nWill produce the bins:\n[(10, 50] &lt; (50, 100] &lt; (100, 200]]\n\nSo cut (250, bins=[10,50,100,200]) will produce a NaN, as will cut (5, bins=[10,50,100,200]). What I'm trying to do is produce something like &gt; 200 for the first example and &lt; 10 for the second.\nI realize I could do cut (weight, bins=[float(&quot;inf&quot;),10,50,100,200,float(&quot;inf&quot;)]) or the equivalent, but the report style I am following doesn't allow things like (200, inf]. I realize too I could actually specify custom labels via the labels parameter on cut(), but that means remembering to adjust them every time I adjust bins, which could be often.\nHave I exhausted all the possibilities, or is there something in cut() or elsewhere in pandas that would help me do this? I'm thinking about writing a wrapper function for cut() that would automatically generate the labels in desired format from the bins, but I wanted to check here first.\n"
'i have one data frame suppose:\n\nname age hb\nali  34  14\njex  16  13\naja  24  16\njoy  23  12\n\n\ni have a value say "5" that i want to substract from each member of column "hb"\n\nnew column could be:\n\nhb\n9\n8\n11\n7\n\n\nWhat is the best method to do this...\n\nthanks and regards. \n'
'Is there a direct way to calculate the mean of a dataframe column in pandas but not taking into account data that has zero as a value? Like a parameter inside the .mean() function?\nWas currently doing it like this: \n\nx = df[df[A]!=0]\nx.mean()\n\n'
'The seaborn stripplot has a function which allows hue. \n\nUsing the example from https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.stripplot.html\n\nimport seaborn as sns\nsns.set_style("whitegrid")\ntips = sns.load_dataset("tips")\nax = sns.stripplot(x=tips["total_bill"])\nax = sns.stripplot(x="sex", y="total_bill", hue="day", data=tips, jitter=True)\n\n\n\n\nIn this case, the legend is quite small, showing a different hue for each day. However, I would like to remove the legend. \n\nNormally, one includes a parameter legend=False. However, for stripplot, this appears to output an attribute error: \n\nAttributeError: Unknown property legend\n\n\nCan one remove the legend for stripplots? If so, how does one do this? \n'
'I have a big dataframe and I try to split that and after concat that.\nI use\n\ndf2 = pd.read_csv(\'et_users.csv\', header=None, names=names2, chunksize=100000)\nfor chunk in df2:\n    chunk[\'ID\'] = chunk.ID.map(rep.set_index(\'member_id\')[\'panel_mm_id\'])\n\ndf2 = pd.concat(chunk, ignore_index=True)\n\n\nBut it return an error\n\nTypeError: first argument must be an iterable of pandas objects, you passed an object of type "DataFrame"\n\n\nHow can I fix that?\n'
'I\'m converting a large textfile to a hdf storage in hopes of a faster data access. The conversion works allright, however reading from the csv file is not done in parallel. It is really slow (takes about 30min for a 1GB textfile on an SSD, so my guess is that it is not IO-bound). \n\nIs there a way to have it read in multiple threads in parralel?\nSice it might be important, I\'m currently forced to run under Windows -- just in case that makes any difference.\n\nfrom dask import dataframe as ddf\ndf = ddf.read_csv("data/Measurements*.csv",\n             sep=\';\', \n             parse_dates=["DATETIME"], \n             blocksize=1000000,\n             )\n\ndf.categorize([ \'Type\',\n                \'Condition\',               \n          ])\n\ndf.to_hdf("data/data.hdf", "Measurements", \'w\')\n\n'
"I am trying to merge the results of a predict method back with the original data in a pandas.DataFrame object.\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport pandas as pd\nimport numpy as np\n\ndata = load_iris()\n\n# bear with me for the next few steps... I'm trying to walk you through\n# how my data object landscape looks... i.e. how I get from raw data \n# to matrices with the actual data I have, not the iris dataset\n# put feature matrix into columnar format in dataframe\ndf = pd.DataFrame(data = data.data)\n\n# add outcome variable\ndf['class'] = data.target\n\nX = np.matrix(df.loc[:, [0, 1, 2, 3]])\ny = np.array(df['class'])\n\n# finally, split into train-test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(X_train, y_train)\n\n# I've got my predictions now\ny_hats = model.predict(X_test)\n\n\nTo merge these predictions back with the original df, I try this:\n\ndf['y_hats'] = y_hats\n\n\nBut that raises:\n\n\n  ValueError: Length of values does not match length of index\n\n\nI know I could split the df into train_df and test_df and this problem would be solved, but in reality I need to follow the path above to create the matrices X and y (my actual problem is a text classification problem in which I normalize the entire feature matrix before splitting into train and test). How can I align these predicted values with the appropriate rows in my df, since the y_hats array is zero-indexed and seemingly all information about which rows were included in the X_test and y_test is lost? Or will I be relegated to splitting dataframes into train-test first, and then building feature matrices? I'd like to just fill the rows included in train with np.nan values in the dataframe.\n"
"Problem\n\nI want to calculate diff by group. And I don’t know how to sort the time column so that each group results are sorted and positive. \n\nThe original data :\n\nIn [37]: df \nOut[37]:\n  id                time\n0  A 2016-11-25 16:32:17\n1  A 2016-11-25 16:36:04\n2  A 2016-11-25 16:35:29\n3  B 2016-11-25 16:35:24\n4  B 2016-11-25 16:35:46\n\n\nThe result I want \n\nOut[40]:\n   id   time\n0  A   00:35\n1  A   03:12\n2  B   00:22\n\n\nnotice: the type of time col is timedelta64[ns]\n\nTrying\n\nIn [38]: df['time'].diff(1)\nOut[38]:\n0                 NaT\n1            00:03:47\n2   -1 days +23:59:25\n3   -1 days +23:59:55\n4            00:00:22\nName: time, dtype: timedelta64[ns]\n\n\nDon't get desired result.\n\nHope\n\nNot only solve the problem but the code can run fast because there are 50 million rows.\n"
"I'm just learning python/pandas and like how powerful and concise it is.\n\nDuring data cleaning I want to use replace on a column in a dataframe with regex but I want to reinsert parts of the match (groups).\n\nSimple Example:\nlastname, firstname -> firstname lastname\n\nI tried something like the following (actual case is more complex so excuse the simple regex):\n\ndf['Col1'].replace({'([A-Za-z])+, ([A-Za-z]+)' : '\\2 \\1'}, inplace=True, regex=True)\n\n\nHowever, this results in empty values. The match part works as expected, but the value part doesn't.\nI guess this could be achieved by some splitting and merging, but I am looking for a general answer as to whether the regex group can be used in replace.\n"
"I am working with pandas, but I don't have so much experience. I have the following DataFrame:\n\n          A\n0       NaN\n1      0.00\n2      0.00\n3      3.33\n4     10.21\n5      6.67\n6      7.00\n7      8.27\n8      6.07\n9      2.17\n10     3.38\n11     2.48\n12     2.08\n13     6.95\n14     0.00\n15     1.75\n16     6.66\n17     9.69\n18     6.73\n19     6.20\n20     3.01\n21     0.32\n22     0.52\n\n\nand I need to compute the cumulative sum of the previous 11 rows. When there is less than 11 previously, they remaining are assumed to be 0.\n\n        B\n0     NaN\n1    0.00\n2    0.00\n3    0.00\n4    3.33\n5    13.54\n6    20.21\n7    27.20\n8    35.47\n9    41.54\n10    43.72\n11   47.09\n12   49.57 \n13   51.65\n14   58.60\n15   58.60\n16   57.02\n17   53.48\n18   56.49\n19   56.22\n20   54.16\n21   51.10\n22   49.24\n\n\nI have tried:\n\ndf['B'] = df.A.cumsum().shift(-11).fillna(0)\n\n\nHowever, this is not achieving what I want, but this is rotating the result of a cumulative sum. How can I achieve this?\n"
'When I import pandas, everything is fine and working. Yet, when I try to import something from pandas.plotting im getting an error. What could be the source of this? \n\nHere is how the output looks like:\n\n&gt;&gt;&gt; import pandas\n&gt;&gt;&gt; from pandas.plotting import scatter_matrix\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nImportError: No module named plotting\n\n\nThe pandas version Im using is: 0.19.2\n'
"I'm trying to remove a row from my data frame in which one of the columns has a value of null. Most of the help I can find relates to removing NaN values which hasn't worked for me so far. \n\nHere I've created the data frame:\n\n  # successfully crated data frame\n df1 = ut.get_data(symbols, dates) # column heads are 'SPY', 'BBD'\n\n# can't get rid of row containing null val in column BBD\n# tried each of these with the others commented out but always had an \n# error or sometimes I was able to get a new column of boolean values\n# but i just want to drop the row\ndf1 = pd.notnull(df1['BBD']) # drops rows with null val, not working\ndf1 = df1.drop(2010-05-04, axis=0)\ndf1 = df1[df1.'BBD' != null]\ndf1 = df1.dropna(subset=['BBD'])\ndf1 = pd.notnull(df1.BBD)\n\n\n# I know the date to drop but still wasn't able to drop the row\ndf1.drop([2015-10-30])\ndf1.drop(['2015-10-30'])\ndf1.drop([2015-10-30], axis=0)\ndf1.drop(['2015-10-30'], axis=0)\n\n\nwith pd.option_context('display.max_row', None):\n    print(df1)\n\n\nHere is my output:\n\n\n\nCan someone please tell me how I can drop this row, preferably both by identifying the row by the null value and how to drop by date? \n\nI haven't been working with pandas very long and I've been stuck on this for an hour. Any advice would be much appreciated. \n"
'Suppose a Pandas data frame looks like:\n\nX_test.head(4)\n    BoxRatio  Thrust  Velocity  OnBalRun  vwapGain\n5     -0.163  -0.817     0.741     1.702     0.218\n8      0.000   0.000     0.732     1.798     0.307\n11     0.417  -0.298     2.036     4.107     1.793\n13     0.054  -0.574     1.323     2.553     1.185\n\n\nHow can I extract the third row (as row3) as a pd data frame?\nIn other words, row3.shape should be (1,5) and row3.head() should be:\n\n 0.417  -0.298     2.036     4.107     1.793\n\n'
"Suppose \n\ns = pd.Series(range(50))\n\n0      0\n1      1\n2      2\n3      3\n...\n48     48\n49     49\n\n\nHow can I get the new series that consists of sum of every n rows?\n\nExpected result is like below, when n = 5;\n\n0      10\n1      35\n2      60\n3      85\n...\n8      210\n9      235\n\n\nIf using loc or iloc and loop by python, of course it can be accomplished, however I believe it could be done simply in Pandas way.\n\nAlso, this is a very simplified example, I don't expect the explanation of the sequences:). Actual data series I'm trying has the time index and the the number of events occurred in every second as the values.\n"
'I have the following df:\n\ncode . role    . persons\n123 .  Janitor . 3\n123 .  Analyst . 2\n321 .  Vallet  . 2\n321 .  Auditor . 5\n\n\nThe first line means that I have 3 persons with the role Janitors.\nMy problem is that I would need to have one line for each person. My df should look like this:\n\ndf:\n\ncode . role    . persons\n123 .  Janitor . 3\n123 .  Janitor . 3\n123 .  Janitor . 3\n123 .  Analyst . 2\n123 .  Analyst . 2\n321 .  Vallet  . 2\n321 .  Vallet  . 2\n321 .  Auditor . 5\n321 .  Auditor . 5\n321 .  Auditor . 5\n321 .  Auditor . 5\n321 .  Auditor . 5\n\n\nHow could I do that using pandas?\n'
'I\'m not sure why I\'m getting slightly different results for a simple OLS, depending on whether I go through panda\'s experimental rpy interface to do the regression in R or whether I use statsmodels in Python.\n\nimport pandas\nfrom rpy2.robjects import r\n\nfrom functools import partial\n\nloadcsv = partial(pandas.DataFrame.from_csv,\n                  index_col="seqn", parse_dates=False)\n\ndemoq = loadcsv("csv/DEMO.csv")\nrxq = loadcsv("csv/quest/RXQ_RX.csv")\n\nnum_rx = {}\nfor seqn, num in rxq.rxd295.iteritems():\n    try:\n        val = int(num)\n    except ValueError:\n        val = 0\n    num_rx[seqn] = val\n\nseries = pandas.Series(num_rx, name="num_rx")\ndemoq = demoq.join(series)\n\nimport pandas.rpy.common as com\ndf = com.convert_to_r_dataframe(demoq)\nr.assign("demoq", df)\nr(\'lmout &lt;- lm(demoq$num_rx ~ demoq$ridageyr)\')  # run the regression\nr(\'print(summary(lmout))\')  # print from R\n\n\nFrom R, I get the following summary:\n\nCall:\nlm(formula = demoq$num_rx ~ demoq$ridageyr)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9086 -0.6908 -0.2940  0.1358 15.7003 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.1358216  0.0241399  -5.626 1.89e-08 ***\ndemoq$ridageyr  0.0358161  0.0006232  57.469  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 \n\nResidual standard error: 1.545 on 9963 degrees of freedom\nMultiple R-squared: 0.249,  Adjusted R-squared: 0.2489 \nF-statistic:  3303 on 1 and 9963 DF,  p-value: &lt; 2.2e-16\n\n\nUsing statsmodels.api to do the OLS:\n\nimport statsmodels.api as sm\nresults = sm.OLS(demoq.num_rx, demoq.ridageyr).fit()\nresults.summary()\n\n\nThe results are similar to R\'s output but not the same:\n\nOLS Regression Results\nAdj. R-squared:  0.247\nLog-Likelihood:  -18488.\nNo. Observations:    9965    AIC:   3.698e+04\nDf Residuals:    9964    BIC:   3.698e+04\n             coef   std err  t     P&gt;|t|    [95.0% Conf. Int.]\nridageyr     0.0331  0.000   82.787    0.000        0.032 0.034\n\n\nThe install process is a a bit cumbersome. But, there is an ipython notebook here, that can reproduce the inconsistency.\n'
"In R, there is a rather useful replace function.\nEssentially, it does conditional re-assignment in a given column of a data frame.\nIt can be used as so:\nreplace(df$column, df$column==1,'Type 1');\n\nWhat is a good way to achieve the same in pandas?\n\nShould I use a lambda with apply? (If so, how do I get a reference to the given column, as opposed to a whole row).\n\nShould I use np.where on data_frame.values?\nIt seems like I am missing a very obvious thing here.\n\nAny suggestions are appreciated.\n"
'I get a KeyError when I try to plot a slice of a pandas DataFrame column with datetimes in it. Does anybody know what could cause this?\n\nI managed to reproduce the error in a little self contained example (which you can also view here: http://nbviewer.ipython.org/3714142/):\n\nimport numpy as np\nfrom pandas import DataFrame\nimport datetime\nfrom pylab import *\n\ntest = DataFrame({\'x\' : [datetime.datetime(2012,9,10) + datetime.timedelta(n) for n in range(10)], \n                  \'y\' : range(10)})\n\n\nNow if I plot:\n\nplot(test[\'x\'][0:5])\n\n\nthere is not problem, but when I plot:\n\nplot(test[\'x\'][5:10])\n\n\nI get the KeyError below (and the error message is not very helpfull to me). This only happens with datetime columns, not with other columns (as far as I experienced). E.g. plot(test[\'y\'][5:10]) is not a problem.\n\nTher error message:\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-7-aa076e3fc4e0&gt; in &lt;module&gt;()\n----&gt; 1 plot(test[\'x\'][5:10])\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\pyplot.pyc in plot(*args, **kwargs)\n   2456         ax.hold(hold)\n   2457     try:\n-&gt; 2458         ret = ax.plot(*args, **kwargs)\n   2459         draw_if_interactive()\n   2460     finally:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in plot(self, *args, **kwargs)\n   3846         lines = []\n   3847 \n-&gt; 3848         for line in self._get_lines(*args, **kwargs):\n   3849             self.add_line(line)\n   3850             lines.append(line)\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _grab_next_args(self, *args, **kwargs)\n    321                 return\n    322             if len(remaining) &lt;= 3:\n--&gt; 323                 for seg in self._plot_args(remaining, kwargs):\n    324                     yield seg\n    325                 return\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _plot_args(self, tup, kwargs)\n    298             x = np.arange(y.shape[0], dtype=float)\n    299 \n--&gt; 300         x, y = self._xy_from_xy(x, y)\n    301 \n    302         if self.command == \'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axes.pyc in _xy_from_xy(self, x, y)\n    215         if self.axes.xaxis is not None and self.axes.yaxis is not None:\n    216             bx = self.axes.xaxis.update_units(x)\n--&gt; 217             by = self.axes.yaxis.update_units(y)\n    218 \n    219             if self.command!=\'plot\':\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\axis.pyc in update_units(self, data)\n   1277         neednew = self.converter!=converter\n   1278         self.converter = converter\n-&gt; 1279         default = self.converter.default_units(data, self)\n   1280         #print \'update units: default=%s, units=%s\'%(default, self.units)\n   1281         if default is not None and self.units is None:\n\nC:\\Python27\\lib\\site-packages\\matplotlib\\dates.pyc in default_units(x, axis)\n   1153         \'Return the tzinfo instance of *x* or of its first element, or None\'\n   1154         try:\n-&gt; 1155             x = x[0]\n   1156         except (TypeError, IndexError):\n   1157             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __getitem__(self, key)\n    374     def __getitem__(self, key):\n    375         try:\n--&gt; 376             return self.index.get_value(self, key)\n    377         except InvalidIndexError:\n    378             pass\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\index.pyc in get_value(self, series, key)\n    529         """\n    530         try:\n--&gt; 531             return self._engine.get_value(series, key)\n    532         except KeyError, e1:\n    533             if len(self) &gt; 0 and self.inferred_type == \'integer\':\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1479)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.IndexEngine.get_value (pandas\\src\\engines.c:1374)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2498)()\n\nC:\\Python27\\lib\\site-packages\\pandas\\_engines.pyd in pandas._engines.DictIndexEngine.get_loc (pandas\\src\\engines.c:2460)()\n\nKeyError: 0\n\n'
'I have a DataFrame which contains a lot of intraday data, the DataFrame has several days of data, dates are not continuous. \n\n 2012-10-08 07:12:22            0.0    0          0  2315.6    0     0.0    0\n 2012-10-08 09:14:00         2306.4   20  326586240  2306.4  472  2306.8    4\n 2012-10-08 09:15:00         2306.8   34  249805440  2306.8  361  2308.0   26\n 2012-10-08 09:15:01         2308.0    1   53309040  2307.4   77  2308.6    9\n 2012-10-08 09:15:01.500000  2308.2    1  124630140  2307.0  180  2308.4    1\n 2012-10-08 09:15:02         2307.0    5   85846260  2308.2  124  2308.0    9\n 2012-10-08 09:15:02.500000  2307.0    3  128073540  2307.0  185  2307.6   11\n ......\n 2012-10-10 07:19:30            0.0    0          0  2276.6    0     0.0    0\n 2012-10-10 09:14:00         2283.2   80   98634240  2283.2  144  2283.4    1\n 2012-10-10 09:15:00         2285.2   18  126814260  2285.2  185  2285.6    3\n 2012-10-10 09:15:01         2285.8    6   98719560  2286.8  144  2287.0   25\n 2012-10-10 09:15:01.500000  2287.0   36  144759420  2288.8  211  2289.0    4\n 2012-10-10 09:15:02         2287.4    6  109829280  2287.4  160  2288.6    5\n ......\n\n\nHow can I extract the unique date in the datetime format from the above DataFrame? To have result like [2012-10-08, 2012-10-10]\n'
'I just recently made the switch from R to python and have been having some trouble getting used to data frames again as opposed to using R\'s data.table. The problem I\'ve been having is that I\'d like to take a list of strings, check for a value, then sum the count of that string- broken down by user. So I would like to take this data:\n\n   A_id       B    C\n1:   a1    "up"  100\n2:   a2  "down"  102\n3:   a3    "up"  100\n3:   a3    "up"  250\n4:   a4  "left"  100\n5:   a5 "right"  102\n\n\nAnd return:\n\n   A_id_grouped   sum_up   sum_down  ...  over_200_up\n1:           a1        1          0  ...            0\n2:           a2        0          1                 0\n3:           a3        2          0  ...            1\n4:           a4        0          0                 0\n5:           a5        0          0  ...            0\n\n\nBefore I did it with the R code (using data.table)\n\n&gt;DT[ ,list(A_id_grouped, sum_up = sum(B == "up"),\n+  sum_down = sum(B == "down"), \n+  ...,\n+  over_200_up = sum(up == "up" &amp; &lt; 200), by=list(A)];\n\n\nHowever all of my recent attempts with Python have failed me:\n\nDT.agg({"D": [np.sum(DT[DT["B"]=="up"]),np.sum(DT[DT["B"]=="up"])], ...\n    "C": np.sum(DT[(DT["B"]=="up") &amp; (DT["C"]&gt;200)])\n    })\n\n\nThank you in advance! it seems like a simple question however I couldn\'t find it anywhere.\n'
"I want to have the x-tick date labels centered between the tick marks, instead of centered about the tick marks as shown in the photo below.\n\nI have read the documentation but to no avail - does anyone know a way to do this? \n\n\n\nHere is everything that I've used for my x-axis tick formatting if it helps:    \n\nday_fmt = '%d'   \nmyFmt = mdates.DateFormatter(day_fmt)\nax.xaxis.set_major_formatter(myFmt)    \nax.xaxis.set_major_locator(matplotlib.dates.DayLocator(interval=1))     \n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.tick1line.set_markersize(0)\n    tick.tick2line.set_markersize(0)\n    tick.label1.set_horizontalalignment('center')\n\n"
"I have the following python pandas data frame:\n\ndf = pd.DataFrame( {\n   'A': [1,1,1,1,2,2,2,3,3,4,4,4],\n   'B': [5,5,6,7,5,6,6,7,7,6,7,7],\n   'C': [1,1,1,1,1,1,1,1,1,1,1,1]\n    } );\n\ndf\n    A  B  C\n0   1  5  1\n1   1  5  1\n2   1  6  1\n3   1  7  1\n4   2  5  1\n5   2  6  1\n6   2  6  1\n7   3  7  1\n8   3  7  1\n9   4  6  1\n10  4  7  1\n11  4  7  1\n\n\nI would like to have another column storing a value of a sum over C values for fixed (both) A and B. That is, something like:\n\n    A  B  C  D\n0   1  5  1  2\n1   1  5  1  2\n2   1  6  1  1\n3   1  7  1  1\n4   2  5  1  1\n5   2  6  1  2\n6   2  6  1  2\n7   3  7  1  2\n8   3  7  1  2\n9   4  6  1  1\n10  4  7  1  2\n11  4  7  1  2\n\n\nI have tried with pandas groupby and it kind of works:\n\nres = {}\nfor a, group_by_A in df.groupby('A'):\n    group_by_B = group_by_A.groupby('B', as_index = False)\n    res[a] = group_by_B['C'].sum()\n\n\nbut I don't know how to 'get' the results from res into df in the orderly fashion. Would be very happy with any advice on this. Thank you. \n"
"Question\n\nI am having trouble figuring out how to create new DataFrame column based on the values in two other columns.  I need to use if/elif/else logic.  But all of the documentation and examples I have found only show if/else logic.  Here is a sample of what I am trying to do:  \n\nCode\n\ndf['combo'] = 'mobile' if (df['mobile'] == 'mobile') elif (df['tablet'] =='tablet') 'tablet' else 'other')\n\n\nI am open to using where() also.  Just having trouble finding the right syntax.  \n"
'import pandas as pd\n\npath1 = "/home/supertramp/Desktop/100&amp;life_180_data.csv"\n\nmydf =  pd.read_csv(path1)\n\nnumcigar = {"Never":0 ,"1-5 Cigarettes/day" :1,"10-20 Cigarettes/day":4}\n\nprint mydf[\'Cigarettes\']\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\nprint mydf[\'CigarNum\']\n\nmydf.to_csv(\'/home/supertramp/Desktop/powerRangers.csv\')\n\n\nThe csv file "100&amp;life_180_data.csv" contains columns like age, bmi,Cigarettes,Alocohol etc.\n\nNo                int64\nAge               int64\nBMI             float64\nAlcohol          object\nCigarettes       object\ndtype: object\n\n\nCigarettes column contains "Never" "1-5 Cigarettes/day","10-20 Cigarettes/day".\nI want to assign weights to these object (Never,1-5 Cigarettes/day ,....)\n\nThe expected output is new column CigarNum appended which consists only numbers 0,1,2\nCigarNum is as expected till 8 rows and then shows Nan till last row in CigarNum column\n\n0                     Never\n1                     Never\n2        1-5 Cigarettes/day\n3                     Never\n4                     Never\n5                     Never\n6                     Never\n7                     Never\n8                     Never\n9                     Never\n10                    Never\n11                    Never\n12     10-20 Cigarettes/day\n13       1-5 Cigarettes/day\n14                    Never\n...\n167                    Never\n168                    Never\n169     10-20 Cigarettes/day\n170                    Never\n171                    Never\n172                    Never\n173                    Never\n174                    Never\n175                    Never\n176                    Never\n177                    Never\n178                    Never\n179                    Never\n180                    Never\n181                    Never\nName: Cigarettes, Length: 182, dtype: object\n\n\nThe output I get shoudln\'t give NaN after few first rows.\n\n0      0\n1      0\n2      1\n3      0\n4      0\n5      0\n6      0\n7      0\n8      0\n9      0\n10   NaN\n11   NaN\n12   NaN\n13   NaN\n14     0\n...\n167   NaN\n168   NaN\n169   NaN\n170   NaN\n171   NaN\n172   NaN\n173   NaN\n174   NaN\n175   NaN\n176   NaN\n177   NaN\n178   NaN\n179   NaN\n180   NaN\n181   NaN\nName: CigarNum, Length: 182, dtype: float64\n\n'
'I often use pandas groupby to generate stacked tables. But then I often want to output the resulting nested relations to json.  Is there any way to extract a nested json filed from the stacked table it produces?  \n\nLet\'s say I have a df like: \n\nyear office candidate  amount\n2010 mayor  joe smith  100.00\n2010 mayor  jay gould   12.00\n2010 govnr  pati mara  500.00\n2010 govnr  jess rapp   50.00\n2010 govnr  jess rapp   30.00\n\n\nI can do: \n\ngrouped = df.groupby(\'year\', \'office\', \'candidate\').sum()\n\nprint grouped\n                       amount\nyear office candidate \n2010 mayor  joe smith   100\n            jay gould    12\n     govnr  pati mara   500\n            jess rapp    80\n\n\nBeautiful!  Of course, what I\'d real like to do is get nested json via a command along the lines of grouped.to_json. But that feature isn\'t available. Any workarounds? \n\nSo, what I really want is something like: \n\n{"2010": {"mayor": [\n                    {"joe smith": 100},\n                    {"jay gould": 12}\n                   ]\n         }, \n          {"govnr": [\n                     {"pati mara":500}, \n                     {"jess rapp": 80}\n                    ]\n          }\n}\n\n\nDon\n'
"I want to preview a Pandas dataframe.  I would use head(mymatrix) in R, but I do not know how to do this in Pandas Python.\n\nWhen I type \n\ndf.head(10) I get...\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 10 entries, 0 to 9\nData columns (total 14 columns):\n#Book_Date            10  non-null values\nItem_Qty              10  non-null values\nItem_id               10  non-null values\nLocation_id           10  non-null values\nMFG_Discount          10  non-null values\nSale_Revenue          10  non-null values\nSales_Flg             10  non-null values\nSell_Unit_Cost        5  non-null values\nStore_Discount        10  non-null values\nTransaction_Id        10  non-null values\nUnit_Cost_Amt         10  non-null values\nUnit_Received_Cost    5  non-null values\nUnnamed: 0            10  non-null values\nWeight                10  non-null values\n\n"
"I have a time series in pandas that looks like this:\n\n                     Values\n1992-08-27 07:46:48    28.0  \n1992-08-27 08:00:48    28.2  \n1992-08-27 08:33:48    28.4  \n1992-08-27 08:43:48    28.8  \n1992-08-27 08:48:48    29.0  \n1992-08-27 08:51:48    29.2  \n1992-08-27 08:53:48    29.6  \n1992-08-27 08:56:48    29.8  \n1992-08-27 09:03:48    30.0\n\n\nI would like to resample it to a regular time series with 15 min times steps where the values are linearly interpolated. Basically I would like to get:\n\n                     Values\n1992-08-27 08:00:00    28.2  \n1992-08-27 08:15:00    28.3  \n1992-08-27 08:30:00    28.4  \n1992-08-27 08:45:00    28.8  \n1992-08-27 09:00:00    29.9\n\n\nHowever using the resample method (df.resample('15Min')) from Pandas I get:\n\n                     Values\n1992-08-27 08:00:00   28.20  \n1992-08-27 08:15:00     NaN  \n1992-08-27 08:30:00   28.60  \n1992-08-27 08:45:00   29.40  \n1992-08-27 09:00:00   30.00  \n\n\nI have tried the resample method with different 'how' and 'fill_method' parameters but never got exactly the results I wanted. Am I using the wrong method?\n\nI figure this is a fairly simple query, but I have searched the web for a while and couldn't find an answer.\n\nThanks in advance for any help I can get.\n"
"Let's say that I have some data generated as follows:\n\nN = 20\nm = 3\ndata = np.random.normal(size=(N,m)) + np.random.normal(size=(N,m))**3\n\n\nand then I create some categorization variable:\n\nindx = np.random.randint(0,3,size=N).astype(np.int32)\n\n\nand generate a DataFrame:\n\nimport pandas as pd\ndf = pd.DataFrame(np.hstack((data, indx[:,None])), \n             columns=['a%s' % k for k in range(m)] + [ 'indx'])\n\n\nI can get the mean value, per group as:\n\ndf.groubpy('indx').mean()\n\n\nWhat I'm unsure of how to do is to then subtract the mean off of each group, per-column in the original data, so that the data in each column is normalized by the mean within group. Any suggestions would be appreciated.\n"
"I'd like to be able to compute descriptive statistics on data in a Pandas DataFrame, but I only care about duplicated entries. For example, let's say I have the DataFrame created by:\n\nimport pandas as pd\ndata={'key1':[1,2,3,1,2,3,2,2],'key2':[2,2,1,2,2,4,2,2],'data':[5,6,2,6,1,6,2,8]}\nframe=pd.DataFrame(data,columns=['key1','key2','data'])\nprint frame\n\n\n     key1  key2  data\n0     1     2     5\n1     2     2     6\n2     3     1     2\n3     1     2     6\n4     2     2     1\n5     3     4     6\n6     2     2     2\n7     2     2     8\n\n\nAs you can see, rows 0,1,3,4,6, and 7 are all duplicates (using 'key1' and 'key2'. However, if I index this DataFrame like so:\n\nframe[frame.duplicated(['key1','key2'])]\n\n\nI get \n\n   key1  key2  data\n3     1     2     6\n4     2     2     1\n6     2     2     2\n7     2     2     8\n\n\n(i.e., the 1st and 2nd rows do not show up because they are not indexed to True by the duplicated method). \n\nThat is my first problem. My second problems deals with how to extract the descriptive statistics from this information. Forgetting the missing duplicate for the moment, let's say I want to compute the .min() and .max() for the duplicate entries (so that I can get a range). I can use groupby and these methods on the groupby object like so:\n\na.groupby(['key1','key2']).min()\n\n\nwhich gives \n\n           key1  key2  data\nkey1 key2                  \n1    2        1     2     6\n2    2        2     2     1\n\n\nThe data I want is obviously here, but what's the best way for me to extract it?  How do I index the resulting object to get what I want (which is the key1,key2,data info)?\n"
"Is there a way to test whether a dataframe is sorted by a given column that's not an index (i.e. is there an equivalent to is_monotonic() for non-index columns) without calling a sort all over again, and without converting a column into an index?\n"
"I've checked out map, apply, mapapply, and combine, but can't seem to find a simple way of doing the following:\n\nI have a dataframe with 10 columns. I need to pass three of them into a function that takes scalars and returns a scalar ...\n\nsome_func(int a, int b, int c) returns int d\n\n\nI want to apply this and create a new column in the dataframe with the result.\n\ndf['d'] = some_func(a = df['a'], b = df['b'], c = df['c'])\n\n\nAll the solutions that I've found seem to suggest to rewrite some_func to work with Series instead of scalars, but this is not possible as it is part of another package. How do I elegantly do the above?\n"
"I'm trying to convert a datetime column back to a string in Pandas dataframe.\n\nthe syntax I have so far is:\n\nall_data['Order Day new'] = dt.date.strftime(all_data['Order Day new'], '%d/%m/%Y')\n\n\nbut this returns the error:\n\ndescriptor 'strftime' requires a 'datetime.date' object but received a 'Series'.\n\nCan anyone tell me where I'm going wrong.\n"
"I want to delete rows when a few conditions are met:\n\nFor instance, a random DataFrame is generated:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.randn(10, 4), columns=['one', 'two', 'three', 'four'])\nprint df\n\n\none instance of table is shown as below:\n\n        one       two     three      four\n0 -0.225730 -1.376075  0.187749  0.763307\n1  0.031392  0.752496 -1.504769 -1.247581\n2 -0.442992 -0.323782 -0.710859 -0.502574\n3 -0.948055 -0.224910 -1.337001  3.328741\n4  1.879985 -0.968238  1.229118 -1.044477\n5  0.440025 -0.809856 -0.336522  0.787792\n6  1.499040  0.195022  0.387194  0.952725\n7 -0.923592 -1.394025 -0.623201 -0.738013\n8 -1.775043 -1.279997  0.194206 -1.176260\n9 -0.602815  1.183396 -2.712422 -0.377118\n\n\nI want to delete rows based on the conditions that:\n\nRow with value of col 'one', 'two', or 'three' greater than 0; and value of col 'four' less than 0 should be deleted. \n\nThen I tried to implement as follows:\n\ndf = df[df.one &gt; 0 or df.two &gt; 0 or df.three &gt; 0 and df.four &lt; 1]\n\n\nHowever, resulting in a error message as follow:\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\nCould someone help me on how to delete based on multiple conditions?\n"
'I have a pandas DataFrame df:\n\nimport pandas as pd\n\ndata = {"Name": ["AAAA", "BBBB"],\n        "C1": [25, 12],\n        "C2": [2, 1],\n        "C3": [1, 10]}\n\ndf = pd.DataFrame(data)\ndf.set_index("Name")\n\n\nwhich looks like this when printed (for reference):\n\n      C1  C2  C3\nName            \nAAAA  25   2   1\nBBBB  12   1  10\n\n\nI would like to choose rows for which C1, C2 and C3 have values between 0 and 20.\n\nCan you suggest an elegant way to select those rows?\n'
'i have a variable consisting of 300k records with dates and the date look like\n2015-02-21 12:08:51\nfrom that date i want to remove time \n\ntype of date variable is pandas.core.series.series \n\nThis is the way i tried\n\nfrom datetime import datetime,date\ndate_str = textdata[\'vfreceiveddate\']  \nformat_string = "%Y-%m-%d"\nthen = datetime.strftime(date_str,format_string)   \n\n\nsome Random ERROR\n\nIn the above code textdata is my datasetname and vfreceived date is a variable consisting of dates\nHow can i write the code to remove the time from the datetime.\n'
"So I have a pandas DataFrame that looks like this:\n\nr vals    positions\n1.2       1\n1.8       2\n2.3       1\n1.8       1\n2.1       3\n2.0       3\n1.9       1\n...       ...\n\n\nI would like the filter out all rows by position that do not appear at least 20 times. I have seen something like this\n\ng=df.groupby('positions')\ng.filter(lambda x: len(x) &gt; 20)\n\n\nbut this does not seem to work and I do not understand how to get the original dataframe back from this. Thanks in advance for the help.\n"
'I am loading data from various sources (csv, xls, json etc...) into Pandas dataframes and I would like to generate statements to create and fill a SQL database with this data. Does anyone know of a way to do this?\n\nI know pandas has a to_sql function, but that only works on a database connection, it can not generate a string.\n\nExample\n\nWhat I would like is to take a dataframe like so:\n\nimport pandas as pd\nimport numpy as np\n\ndates = pd.date_range(\'20130101\',periods=6)\ndf = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list(\'ABCD\'))\n\n\nAnd a function that would generate this (this example is PostgreSQL but any would be fine):\n\nCREATE TABLE data\n(\n  index timestamp with time zone,\n  "A" double precision,\n  "B" double precision,\n  "C" double precision,\n  "D" double precision\n)\n\n'
"I have a pandas dataframe with two id variables:\n\ndf = pd.DataFrame({'id': [1,1,1,2,2,3], \n               'num': [10,10,12,13,14,15],\n               'q': ['a', 'b', 'd', 'a', 'b', 'z'],\n               'v': [2,4,6,8,10,12]})\n\n   id  num  q   v\n0   1   10  a   2\n1   1   10  b   4\n2   1   12  d   6\n3   2   13  a   8\n4   2   14  b  10\n5   3   15  z  12\n\n\nI can pivot the table with:\n\ndf.pivot('id','q','v')\n\n\nAnd end up with something close:\n\nq    a   b   d   z\nid                \n1    2   4   6 NaN\n2    8  10 NaN NaN\n3  NaN NaN NaN  12\n\n\nHowever, what I really want is (the original unmelted form):\n\nid   num   a   b   d   z               \n1    10   2   4 NaN NaN\n1    12 NaN NaN   6 NaN  \n2    13   8 NaN NaN NaN\n2    14 NaN  10 NaN NaN\n3    15 NaN NaN NaN  12\n\n\nIn other words:\n\n\n'id' and 'num' my indices (normally, I've only seen either 'id' or 'num' being the index but I need both since I'm trying to retrieve the original unmelted form)\n'q' are my columns\n'v' are my values in the table\n\n\nUpdate\n\nI found a close solution from Wes McKinney's blog:\n\ndf.pivot_table(index=['id','num'], columns='q')\n\n         v            \nq        a   b   d   z\nid num                \n1  10    2   4 NaN NaN\n   12  NaN NaN   6 NaN\n2  13    8 NaN NaN NaN\n   14  NaN  10 NaN NaN\n3  15  NaN NaN NaN  12\n\n\nHowever, the format is not quite the same as what I want above.\n"
"I have a df with currency:\n\ndf = pd.DataFrame({'Currency':['$1.00','$2,000.00','(3,000.00)']})\n\n     Currency\n0       $1.00\n1   $2,000.00\n2  (3,000.00)\n\n\nI want to convert the 'Currency' dtype to float but I am having trouble with the parentheses string (which indicate a negative amount). This is my current code:\n\ndf[['Currency']] = df[['Currency']].replace('[\\$,]','',regex=True).astype(float)\n\n\nwhich produces an error:\n\nValueError: could not convert string to float: (3000.00)\n\n\nWhat I want as dtype float is:\n\n     Currency\n0       1.00\n1   2000.00\n2  -3000.00\n\n"
'I have multiple CSV files with values like this in a folder:\n\nThe GroupID.csv is the filename. There are multiple files like this, but the value ranges are defined in the same XML file. I\'m trying to group them\nHow can I do that?\n\nUPDATE1:\nBased on BobHaffner\'s comments, I\'ve done this \n\nimport pandas as pd \nimport glob path =r\'path/to/files\' \nallFiles = glob.glob(path + "/*.csv")\nframe = pd.DataFrame()\nlist_ = []\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None, header=None)\n    df[\'file\'] = os.path.basename(\'path/to/files/\'+file_)\n    list_.append(df)\nframe = pd.concat(list_)\nprint frame\n\n\nto get something like this: \n\nI need to group the values based on the bins from the XML file. I\'d truly appreciate any help.\n'
"I have this serie:\n\nprint series.head()\nprint type(series)\nprint series.index\n\nyear\n1992    36.222222\n1993    53.200000\n1994    49.400000\n1995    34.571429\n1996    39.200000\nName: ranking, dtype: float64\n&lt;class 'pandas.core.series.Series'&gt;\n\nInt64Index([1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014], dtype='int64', name=u'year')\n\n\nI'm trying to do a scatter plot, but I'm having trouble accessing the the index and values from the series.\n\nAny pointers will be appreciated.\n"
"When stacking a pandas DataFrame, a Series is returned. Normally after I stack a DataFrame, I convert it back into a DataFrame. However, the default names coming from the stacked data make renaming the columns a bit hacky. What I'm looking for is an easier/built-in way to give columns sensible names after stacking.\n\nE.g., for the following DataFrame:\n\nIn [64]: df = pd.DataFrame({'id':[1,2,3], \n    ...:                    'date':['2015-09-31']*3, \n    ...:                    'value':[100, 95, 42], \n    ...:                    'value2':[200, 57, 27]}).set_index(['id','date'])\n\nIn [65]: df\nOut[65]: \n               value  value2\nid date                     \n1  2015-09-31    100     200\n2  2015-09-31     95      57\n3  2015-09-31     42      27\n\n\nI stack and convert it back to a DataFrame like so:\n\nIn [68]: df.stack().reset_index()\nOut[68]: \n   id        date level_2    0\n0   1  2015-09-31   value  100\n1   1  2015-09-31  value2  200\n2   2  2015-09-31   value   95\n3   2  2015-09-31  value2   57\n4   3  2015-09-31   value   42\n5   3  2015-09-31  value2   27\n\n\nSo in order to name these columns appropriately I would need to do something like this:\n\nIn [72]: stacked = df.stack()\n\nIn [73]: stacked\nOut[73]: \nid  date              \n1   2015-09-31  value     100\n                value2    200\n2   2015-09-31  value      95\n                value2     57\n3   2015-09-31  value      42\n                value2     27\ndtype: int64\n\nIn [74]: stacked.index.set_names('var_name', level=len(stacked.index.names)-1, inplace=True)\n\nIn [88]: stacked.reset_index().rename(columns={0:'value'})\nOut[88]: \n   id        date var_name  value\n0   1  2015-09-31    value    100\n1   1  2015-09-31   value2    200\n2   2  2015-09-31    value     95\n3   2  2015-09-31   value2     57\n4   3  2015-09-31    value     42\n5   3  2015-09-31   value2     27\n\n\nIdeally, the solution would look something like this:\n\ndf.stack(new_index_name='var_name', new_col_name='value')\n\n\nBut looking at the docs it doesn't look like stack takes any such arguments. Is there an easier/built-in way in pandas to deal with this workflow?\n"
'The following code will print True because the Series contains at least one element that is greater than 1. However, it seems a bit un-Pythonic. Is there a more Pythonic way to return True if a Series contains a number that is > a particular value?\n\nimport pandas as pd\n\ns = pd.Series([0.5, 2])\nprint True in (s &gt; 1)\n\n\n\n  True\n\n\nEDIT:\nNot only is the above answer un-Pythonic, it will sometimes return an incorrect result for some reason. For example:\n\ns = pd.Series([0.5])\nprint True in (s &lt; 1)\n\n\n\n  False\n\n'
'What is the best way to account for (not a number) nan values in a pandas DataFrame?\n\nThe following code:\n\nimport numpy as np\nimport pandas as pd\ndfd = pd.DataFrame([1, np.nan, 3, 3, 3, np.nan], columns=[\'a\'])\ndfv = dfd.a.value_counts().sort_index()\nprint("nan: %d" % dfv[np.nan].sum())\nprint("1: %d" % dfv[1].sum())\nprint("3: %d" % dfv[3].sum())\nprint("total: %d" % dfv[:].sum())\n\n\nOutputs:\n\nnan: 0\n1: 1\n3: 3\ntotal: 4\n\n\nWhile the desired output is:\n\nnan: 2\n1: 1\n3: 3\ntotal: 6\n\n\nI am using pandas 0.17 with Python 3.5.0 with Anaconda 2.4.0.\n'
'I am trying to concat the following:\n\ndf1\n\n    price   side    timestamp\ntimestamp           \n2016-01-04 00:01:15.631331072   0.7286  2   1451865675631331\n2016-01-04 00:01:15.631399936   0.7286  2   1451865675631400\n2016-01-04 00:01:15.631860992   0.7286  2   1451865675631861\n2016-01-04 00:01:15.631866112   0.7286  2   1451865675631866\n\n\nand\n\ndf2\n\n    bid bid_size    offer   offer_size\ntimestamp               \n2016-01-04 00:00:31.331441920   0.7284  4000000 0.7285  1000000\n2016-01-04 00:00:53.631324928   0.7284  4000000 0.7290  4000000\n2016-01-04 00:01:03.131234048   0.7284  5000000 0.7286  4000000\n2016-01-04 00:01:12.131444992   0.7285  1000000 0.7286  4000000\n2016-01-04 00:01:15.631364096   0.7285  4000000 0.7290  4000000\n\n\nWith \n\n data = pd.concat([df1,df2], axis=1)  \n\n\nBut I get the follwing output:\n\nInvalidIndexError                         Traceback (most recent call last)\n&lt;ipython-input-38-2e88458f01d7&gt; in &lt;module&gt;()\n----&gt; 1 data = pd.concat([df1,df2], axis=1)\n      2 data = data.fillna(method=\'pad\')\n      3 data = data.fillna(method=\'bfill\')\n      4 data[\'timestamp\'] =  data.index.values#converting to datetime\n      5 data[\'timestamp\'] = pd.to_datetime(data[\'timestamp\'])#converting to datetime\n\n/usr/local/lib/python2.7/site-packages/pandas/tools/merge.pyc in concat(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\n    810                        keys=keys, levels=levels, names=names,\n    811                        verify_integrity=verify_integrity,\n--&gt; 812                        copy=copy)\n    813     return op.get_result()\n    814 \n\n/usr/local/lib/python2.7/site-packages/pandas/tools/merge.pyc in __init__(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\n    947         self.copy = copy\n    948 \n--&gt; 949         self.new_axes = self._get_new_axes()\n    950 \n    951     def get_result(self):\n\n/usr/local/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_new_axes(self)\n   1013                 if i == self.axis:\n   1014                     continue\n-&gt; 1015                 new_axes[i] = self._get_comb_axis(i)\n   1016         else:\n   1017             if len(self.join_axes) != ndim - 1:\n\n/usr/local/lib/python2.7/site-packages/pandas/tools/merge.pyc in _get_comb_axis(self, i)\n   1039                 raise TypeError("Cannot concatenate list of %s" % types)\n   1040 \n-&gt; 1041         return _get_combined_index(all_indexes, intersect=self.intersect)\n   1042 \n   1043     def _get_concat_axis(self):\n\n/usr/local/lib/python2.7/site-packages/pandas/core/index.pyc in _get_combined_index(indexes, intersect)\n   6120             index = index.intersection(other)\n   6121         return index\n-&gt; 6122     union = _union_indexes(indexes)\n   6123     return _ensure_index(union)\n   6124 \n\n/usr/local/lib/python2.7/site-packages/pandas/core/index.pyc in _union_indexes(indexes)\n   6149 \n   6150         if hasattr(result, \'union_many\'):\n-&gt; 6151             return result.union_many(indexes[1:])\n   6152         else:\n   6153             for other in indexes[1:]:\n\n/usr/local/lib/python2.7/site-packages/pandas/tseries/index.pyc in union_many(self, others)\n    959             else:\n    960                 tz = this.tz\n--&gt; 961                 this = Index.union(this, other)\n    962                 if isinstance(this, DatetimeIndex):\n    963                     this.tz = tz\n\n/usr/local/lib/python2.7/site-packages/pandas/core/index.pyc in union(self, other)\n   1553                 result.extend([x for x in other._values if x not in value_set])\n   1554         else:\n-&gt; 1555             indexer = self.get_indexer(other)\n   1556             indexer, = (indexer == -1).nonzero()\n   1557 \n\n/usr/local/lib/python2.7/site-packages/pandas/core/index.pyc in get_indexer(self, target, method, limit, tolerance)\n   1890 \n   1891         if not self.is_unique:\n-&gt; 1892             raise InvalidIndexError(\'Reindexing only valid with uniquely\'\n   1893                                     \' valued Index objects\')\n   1894 \n\nInvalidIndexError: Reindexing only valid with uniquely valued Index objects  \n\n\nI have removed additional columns and removed duplicates and NA where there could be a conflict - but I simply do not know whats wrong.  \n\nPlease help\nThanks\n'
"Starting from this dataframe df:\n\ndf = pd.DataFrame({'c':[1,1,1,2,2,2],'l1':['a','a','b','c','c','b'],'l2':['b','d','d','f','e','f']})\n\n   c l1 l2\n0  1  a  b\n1  1  a  d\n2  1  b  d\n3  2  c  f\n4  2  c  e\n5  2  b  f\n\n\nI would like to perform a groupby over the c column to get unique values of the l1 and l2 columns. For one columns I can do:\n\ng = df.groupby('c')['l1'].unique()\n\n\nthat correctly returns:\n\nc\n1    [a, b]\n2    [c, b]\nName: l1, dtype: object\n\n\nbut using:\n\ng = df.groupby('c')['l1','l2'].unique()\n\n\nreturns:\n\nAttributeError: 'DataFrameGroupBy' object has no attribute 'unique'\n\n\nI know I can get the unique values for the two columns with (among others):\n\nIn [12]: np.unique(df[['l1','l2']])\nOut[12]: array(['a', 'b', 'c', 'd', 'e', 'f'], dtype=object)\n\n\nIs there a way to apply this method to the groupby in order to get something like:\n\nc\n1    [a, b, d]\n2    [c, b, e, f]\nName: l1, dtype: object\n\n"
"Let's assume that I have the following dataframe in pandas:\n\n             AA  BB  CC     \n   date\n   05/03     1   2   3  \n   06/03     4   5   6  \n   07/03     7   8   9  \n   08/03     5   7   1  \n\n\nand I want to transform it to the following:\n\n   AA 05/03    1\n   AA 06/03    4\n   AA 07/03    7\n   AA 08/03    5\n   BB 05/03    2\n   BB 06/03    5\n   BB 07/03    8\n   BB 08/03    7\n   CC 05/03    3\n   CC 06/03    6\n   CC 07/03    9\n   CC 08/03    1\n\n\nHow can I do it?\n\nThe reason of the transformation from wide to long is that, in the next stage, I would like to merge this dataframe with another one, based on dates and the initial column names (AA, BB, CC).\n"
'Have a pandas dataframe:\n\nidx Event\n0   abc/def\n1   abc\n2   abc/def/hij\n\n\nRun: df[\'EventItem\'] = df[\'Event\'].str.split("/")\n\nGot:\n\nidx EventItem\n0   [\'abc\',\'def\']\n1   [\'abc\']\n2   [\'abc\',\'def\',\'hij\']\n\n\nWant to get the length of each cell, run df[\'EventCount\'] = len(df[\'EventItem\'])\n\nGot:\n\nidx EventCount\n0   6\n1   6\n2   6\n\n\nHow can I get the correct count as follow?\n\nidx EventCount\n0   2\n1   1\n2   3\n\n'
"Suppose we have simple Dataframe\n\ndf = pd.DataFrame(['one apple','banana','box of oranges','pile of fruits outside', 'one banana', 'fruits'])\ndf.columns = ['fruits']\n\n\nhow to calculate number of words in keywords, similar to:\n\n1 word: 2\n2 words: 2\n3 words: 1\n4 words: 1\n\n"
"Consider my series as below: First column is article_id and the second column is frequency count.\n\narticle_id  \n1         39 \n2         49 \n3        187 \n4        159 \n5        158 \n        ...  \n16947     14 \n16948      7 \n16976      2 \n16977      1 \n16978      1 \n16980      1 \n\nName: article_id, dtype: int64\n\n\nI got this series from a dataframe with the following command: \n\nlogs.loc[logs['article_id'] &lt;= 17029].groupby('article_id')['article_id'].count()\n\n\nlogs is the dataframe here and article_id is one of the columns in it.\n\nHow do I plot a bar chart(using Matlplotlib) such that the article_id is on the X-axis and the frequency count on the Y-axis ? \n\nMy natural instinct was to convert it into a list using .tolist() but that doesn't preserve the article_id.\n"
'Using pandas 0.18.1, I\'d like to take the rolling average of a one-column dataframe. Since version 0.18.0, this is done with rolling() objects. The default for these rolling objects is to be right-justified. There is a boolean argument you can pass, center=True, to align the rolling object to the center value, but there doesn\'t seem to be a way to left-align it. Here\'s an example:\n\ndf = pandas.DataFrame({\'A\': [2,3,6,8,20, 27]})\ndf\n    A\n0   2\n1   3\n2   6\n3   8\n4  20\n5  27\n\n\nThe standard method automatically aligns to the right, so there\'s no value at the first two indecies with a window of size three:\n\n df.rolling(window=3).mean()\n           A\n0        NaN\n1        NaN\n2   3.666667\n3   5.666667\n4  11.333333\n5  18.333333\n\n\nWe can center-align the operation like this:\n\ndf.rolling(window=3).mean(center=True)\n           A\n0        NaN\n1   3.666667\n2   5.666667\n3  11.333333\n4  18.333333\n5        NaN\n\n\nBut what I\'m looking for is this:\n\ndf.rolling(3).mean()\n            A\n 0   3.666667\n 1   5.666667\n 2  11.333333\n 3  18.333333\n 4        NaN\n 5        NaN\n\n\nI can accomplish this by doing it with the default right alignment, and then re-indexing it, or by reversing the order of the rows and then doing it "right-aligned" but these are work-arounds for what should be a straight-forward operation.\n'
"I have had to do this several times and I'm always frustrated.  I have a dataframe:\n\ndf = pd.DataFrame([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'], ['A', 'B', 'C', 'D'])\n\nprint df\n\n   A  B  C  D\na  1  2  3  4\nb  5  6  7  8\n\n\nI want to turn df into:\n\npd.Series([[1, 2, 3, 4], [5, 6, 7, 8]], ['a', 'b'])\n\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndtype: object\n\n\nI've tried\n\ndf.apply(list, axis=1)\n\n\nWhich just gets me back the same df\n\nWhat is a convenient/effective way to do this?\n"
"I am using pandas to analyse some election results. I have a DF, Results, which has a row for each constituency and columns representing the votes for the various parties (over 100 of them):\n\nIn[60]: Results.columns\nOut[60]: \nIndex(['Constituency', 'Region', 'Country', 'ID', 'Type', 'Electorate',\n       'Total', 'Unnamed: 9', '30-50', 'Above',\n       ...\n       'WP', 'WRP', 'WVPTFP', 'Yorks', 'Young', 'Zeb', 'Party', 'Votes',\n       'Share', 'Turnout'],\n      dtype='object', length=147) \n\n\nSo...\n\nIn[63]: Results.head()\nOut[63]: \n                         Constituency    Region   Country         ID    Type  \\\nPAID                                                                           \n1                            Aberavon     Wales     Wales  W07000049  County   \n2                           Aberconwy     Wales     Wales  W07000058  County   \n3                      Aberdeen North  Scotland  Scotland  S14000001   Burgh   \n4                      Aberdeen South  Scotland  Scotland  S14000002   Burgh   \n5     Aberdeenshire West &amp; Kincardine  Scotland  Scotland  S14000058  County   \n\n      Electorate  Total  Unnamed: 9  30-50  Above    ...     WP  WRP  WVPTFP  \\\nPAID                                                 ...                       \n1          49821  31523         NaN    NaN    NaN    ...    NaN  NaN     NaN   \n2          45525  30148         NaN    NaN    NaN    ...    NaN  NaN     NaN   \n3          67745  43936         NaN    NaN    NaN    ...    NaN  NaN     NaN   \n4          68056  48551         NaN    NaN    NaN    ...    NaN  NaN     NaN   \n5          73445  55196         NaN    NaN    NaN    ...    NaN  NaN     NaN   \n\n      Yorks  Young  Zeb  Party  Votes     Share   Turnout  \nPAID                                                       \n1       NaN    NaN  NaN    Lab  15416  0.489040  0.632725  \n2       NaN    NaN  NaN    Con  12513  0.415052  0.662230  \n3       NaN    NaN  NaN    SNP  24793  0.564298  0.648550  \n4       NaN    NaN  NaN    SNP  20221  0.416490  0.713398  \n5       NaN    NaN  NaN    SNP  22949  0.415773  0.751528  \n\n[5 rows x 147 columns]\n\n\nThe per-constituency results for each party are given in the columns Results.ix[:, 'Unnamed: 9': 'Zeb']\n\nI can find the winning party (i.e. the party which polled highest number of votes) and the number of votes it polled using:\n\nRawResults = Results.ix[:, 'Unnamed: 9': 'Zeb']\nResults['Party'] = RawResults.idxmax(axis=1)\nResults['Votes'] = RawResults.max(axis=1).astype(int)\n\n\nBut, I also need to know how many votes the second-place party got (and ideally its index/name). So is there any way in pandas to return the second highest value/index in a set of columns for each row?\n"
'how do you capitalize the first letter of each word in the column? I am using python pandas by the way. For example, \n\n         Column1\n         The apple\n         the Pear\n         Green tea\n\n\nMy desire result will be:\n\n         Column1\n         The Apple\n         The Pear\n         Green Tea\n\n'
"What is the pythonic way to slice a dataframe by more index ranges (eg. by 10:12 and 25:28)?\n\nI want this in a more elegant way:\n\ndf = pd.DataFrame({'a':range(10,100)})\ndf.iloc[[i for i in range(10,12)] + [i for i in range(25,28)]]\n\n\nResult:\n\n     a\n10  20\n11  21\n25  35\n26  36\n27  37\n\n\nSomething like this would be more elegant:\n\ndf.iloc[(10:12, 25:28)]\n\n"
'I\'m trying to drop pandas dataframe row based on its index (not location).\n\nThe data frame looks like \n\n                      DO  \n129518  a developer and   \n20066   responsible for   \n571     responsible for   \n85629   responsible for   \n5956    by helping them   \n\n\n(FYI: "DO" is a column name)\n\nI want to delete the row where its index is 571 so I did:\n\ndf=df.drop(df.index[571])\n\n\nthen I check\n    df.ix[571]\n\nthen what the hell it\'s still there!\n\nSo I thought "ok, maybe index and ix are different!"\n\nIn [539]: df.index[571]\n17002\n\n\nMy question is\n\n1) What is index? (compared to ix)\n\n2) How do I delete the index row 571 using ix?\n'
'I have a Pandas DataFrame like this: \n\n   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3     0  0.4  0.4\n4     0    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n\n\nI want to replace the col1 values with the values in the second column (col2) only if col1 values are equal to 0, and after (for the zero values remaining),  do it again but with the third column (col3). The Desired Result is the next one:\n\n   col1 col2 col3\n1   0.2  0.3  0.3\n2   0.2  0.3  0.3\n3   0.4  0.4  0.4\n4   0.3    0  0.3\n5     0    0    0\n6   0.1  0.4  0.4\n\n\nI did it using the pd.replace function, but it seems too slow.. I think must be a faster way to accomplish that. \n\ndf.col1.replace(0,df.col2,inplace=True)\ndf.col1.replace(0,df.col3,inplace=True)\n\n\nis there a faster way to do that?, using some other function instead of the pd.replace function?\n'
"I have a dataFrame which has several coulmns, so i choosed some of its coulmns to create a variable like this xtrain = df[['Age','Fare', 'Group_Size','deck', 'Pclass', 'Title' ]] i want to drop from these coulmns all raws that the Survive coulmn in the main dataFrame is nan.\n"
"Beginner with panda dataframes. I have this data set below with missing values for column A and B (Test.csv):\n\nDateTime              A             B\n01-01-2017 03:27        \n01-01-2017 03:28        \n01-01-2017 03:29    0.18127718  -0.178835737\n01-01-2017 03:30    0.186923018 -0.183260853\n01-01-2017 03:31        \n01-01-2017 03:32        \n01-01-2017 03:33    0.18127718  -0.178835737\n\n\nI can use this code to fill in values using forward propagation, but this only fills in for 03:31 and 03:32, and not 03:27 and 03:28.\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('test.csv', index_col = 0)\ndata = df.fillna(method='ffill')\nndata = data.to_csv('test1.csv')\n\n\nresults in:\n\n   DateTime              A             B\n    01-01-2017 03:27        \n    01-01-2017 03:28        \n    01-01-2017 03:29    0.18127718  -0.178835737\n    01-01-2017 03:30    0.186923018 -0.183260853\n    01-01-2017 03:31    0.186923018 -0.183260853\n    01-01-2017 03:32    0.186923018 -0.183260853\n    01-01-2017 03:33    0.18127718  -0.178835737\n\n\nHow could I include the 'Bfill' to fill in the missing values for 03:27 and 03:28 using the backfil?\n"
"I think there are many questions on plotting multiple graphs but not specifically for this case as shown below.\n\nThe pandas documentation says to 'repeat plot method' to plot multiple column groups in a single axes. However, how would this work for 3 or more column groups? For example if we define a third column:\n\nbx = df.plot(kind='scatter', x='a',y='f',color = 'Green',label ='f')\n\n\nWhere would this bx be passed into?\n\nAlso, if the plot is the same graph, shouldn't the x-axis be consistently either 'a' or 'c'? but the documentation has 2 different x axis: 'a' and 'c'\n\n\n"
"Question\n\nHow do I measure the performance of the various functions below in a concise and comprehensive way.\n\nExample\n\nConsider the dataframe df\n\ndf = pd.DataFrame({\n        'Group': list('QLCKPXNLNTIXAWYMWACA'),\n        'Value': [29, 52, 71, 51, 45, 76, 68, 60, 92, 95,\n                  99, 27, 77, 54, 39, 23, 84, 37, 99, 87]\n    })\n\n\nI want to sum up the Value column grouped by distinct values in Group.  I have three methods for doing it.\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\ndef sum_pd(df):\n    return df.groupby('Group').Value.sum()\n\ndef sum_fc(df):\n    f, u = pd.factorize(df.Group.values)\n    v = df.Value.values\n    return pd.Series(np.bincount(f, weights=v).astype(int), pd.Index(u, name='Group'), name='Value').sort_index()\n\n@njit\ndef wbcnt(b, w, k):\n    bins = np.arange(k)\n    bins = bins * 0\n    for i in range(len(b)):\n        bins[b[i]] += w[i]\n    return bins\n\ndef sum_nb(df):\n    b, u = pd.factorize(df.Group.values)\n    w = df.Value.values\n    bins = wbcnt(b, w, u.size)\n    return pd.Series(bins, pd.Index(u, name='Group'), name='Value').sort_index()\n\n\nAre they the same?\n\nprint(sum_pd(df).equals(sum_nb(df)))\nprint(sum_pd(df).equals(sum_fc(df)))\n\nTrue\nTrue\n\n\nHow fast are they?\n\n%timeit sum_pd(df)\n%timeit sum_fc(df)\n%timeit sum_nb(df)\n\n1000 loops, best of 3: 536 µs per loop\n1000 loops, best of 3: 324 µs per loop\n1000 loops, best of 3: 300 µs per loop\n\n"
"I am new to python (coming from R), and I am trying to understand how I can convert a timestamp series in a pandas dataframe (in my case this is called df['timestamp']) into what I would call a string vector in R.  is this possible?  How would this be done?\n\nI tried df['timestamp'].apply('str'), but this seems to simply put the entire column df['timestamp'] into one long string.  I'm looking to convert each element into a string and preserve the structure, so that it's still a vector (or maybe this a called an array?)\n"
"Suppose I have pandas data frame with 2 columns:\n\ndf: Col1  Col2\n      1     1\n      1     2\n      1     2\n      1     2\n      3     4\n      3     4\n\n\nThen I want to keep only the unique couple values (col1, col2) of these two columns and give their frequncy:\n\ndf2: Col1  Col2  Freq\n      1     1     1\n      1     2     3\n      3     4     2\n\n\nI think to use df['Col1', 'Col2'].value_counts() but it works only for one column. \nDoes it exist a function to deal with many columns?\n"
"I'm confused about the syntax regarding the following line of code: \n\nx_values = dataframe[['Brains']]\n\n\nThe dataframe object consists of 2 columns (Brains and Bodies)\n\nBrains Bodies\n42     34\n32     23\n\n\nWhen I print x_values I get something like this:\n\nBrains\n0  42\n1  32\n\n\nI'm aware of the pandas documentation as far as attributes and methods of the dataframe object are concerned, but the double bracket syntax is confusing me.\n"
"I just discovered the assign method for pandas dataframes, and it looks nice and very similar to dplyr's mutate in R. However, I've always gotten by by just initializing a new column 'on the fly'. Is there a reason why assign is better?\n\nFor instance (based on the example in the pandas documentation), to create a new column in a dataframe, I could just do this:\n\ndf = DataFrame({'A': range(1, 11), 'B': np.random.randn(10)})\ndf['ln_A'] = np.log(df['A'])\n\n\nbut the pandas.DataFrame.assign documentation recommends doing this:\n\ndf.assign(ln_A = lambda x: np.log(x.A))\n# or \nnewcol = np.log(df['A'])\ndf.assign(ln_A=newcol)\n\n\nBoth methods return the same dataframe. In fact, the first method (my 'on the fly' method) is significantly faster (0.20225788200332318 seconds for 1000 iterations) than the .assign method (0.3526602769998135 seconds for 1000 iterations). \n\nSo is there a reason I should stop using my old method in favour of df.assign? \n"
"So my dataframe looks like this:\nfrom pandas.compat import StringIO\nd = StringIO('''\ndate,site,country,score\n2018-01-01,google,us,100\n2018-01-01,google,ch,50\n2018-01-02,google,us,70\n2018-01-03,google,us,60\n2018-01-02,google,ch,10\n2018-01-01,fb,us,50\n2018-01-02,fb,us,55\n2018-01-03,fb,us,100\n2018-01-01,fb,es,100\n2018-01-02,fb,gb,100\n''')\n\ndf = pd.read_csv(d, sep=&quot;,&quot;)\n\nEach site has a different score depending on the country. I'm trying to find the 1/3/5-day difference of scores for each site/country combination.\nOutput should be:\ndate,site,country,score,1_day_diff\n2018-01-01,google,ch,50,0\n2018-01-02,google,ch,10,-40\n2018-01-01,google,us,100,0\n2018-01-02,google,us,70,-30\n2018-01-03,google,us,60,-10\n2018-01-01,fb,es,100,0\n2018-01-02,fb,gb,100,0\n2018-01-01,fb,us,50,0\n2018-01-02,fb,us,55,5\n2018-01-03,fb,us,100,45\n\nI first tried sorting by site/country/date, then grouping by site and country but I'm not able to wrap my head around getting a difference from a grouped object.\n"
"I'm trying to write a pandas dataframe as a pickle file into an s3 bucket in AWS. I know that I can write dataframe new_df as a csv to an s3 bucket as follows:\n\nbucket='mybucket'\nkey='path'\n\ncsv_buffer = StringIO()\ns3_resource = boto3.resource('s3')\n\nnew_df.to_csv(csv_buffer, index=False)\ns3_resource.Object(bucket,path).put(Body=csv_buffer.getvalue())\n\n\nI've tried using the same code as above with to_pickle() but with no success.\n"
'I\'m trying to create a new column in a dataframe that contains the word count for the respective row.  I\'m looking to the total number of words, not frequencies of each distinct word. I assumed there would be a simple/quick way to do this common task, but after googling around and reading a handful of SO posts (1, 2, 3, 4) I\'m stuck. I\'ve tried the solutions put forward in the linked SO posts, but get lots of attribute errors back.\n\nwords = df[\'col\'].split()\ndf[\'totalwords\'] = len(words)\n\n\nresults in\n\nAttributeError: \'Series\' object has no attribute \'split\'\n\n\nand\n\nf = lambda x: len(x["col"].split()) -1\ndf[\'totalwords\'] = df.apply(f, axis=1)\n\n\nresults in\n\nAttributeError: ("\'list\' object has no attribute \'split\'", \'occurred at index 0\')\n\n'
"I have 2 dataframes.\n\nDf1 = pd.DataFrame({'name': ['Marc', 'Jake', 'Sam', 'Brad']\nDf2 = pd.DataFrame({'IDs': ['Jake', 'John', 'Marc', 'Tony', 'Bob']\n\n\nI want to loop over every row in Df1['name'] and check if each name is somewhere in Df2['IDs'].\n\nThe result should return 1 if the name is in there, 0 if it is not like so:\n\nMarc  1 \nJake  1\nSam   0 \nBrad  0\n\n\nThank you.\n"
'I\'m trying to scrape the data from the coins catalog. \n\nThere is one of the pages. I need to scrape this data into Dataframe \n\nSo far I have this code:\n\nimport bs4 as bs\nimport urllib.request\nimport pandas as pd\n\nsource = urllib.request.urlopen(\'http://www.gcoins.net/en/catalog/view/45518\').read()\nsoup = bs.BeautifulSoup(source,\'lxml\')\n\ntable = soup.find(\'table\', attrs={\'class\':\'subs noBorders evenRows\'})\ntable_rows = table.find_all(\'tr\')\n\nfor tr in table_rows:\n    td = tr.find_all(\'td\')\n    row = [tr.text for tr in td]\n    print(row)                    # I need to save this data instead of printing it \n\n\nIt produces following output:\n\n[]\n[\'\', \'\', \'1882\', \'\', \'108,000\', \'UNC\', \'—\']\n[\' \', \'\', \'1883\', \'\', \'786,000\', \'UNC\', \'~ $3.99\']\n[\' \', " \\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t$(\'subGraph55337\').on(\'click\', function(event) {\\n\\t\\t\\t\\t\\t\\t\\t\\tLightview.show({\\n\\t\\t\\t\\t\\t\\t\\t\\t\\thref : \'/en/catalog/ajax/subgraph?id=55337\',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\trel : \'ajax\',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\toptions : {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tautosize : true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttopclose : true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tajax : {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tevalScripts : true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t} \\n\\t\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t\\t\\tevent.stop();\\n\\t\\t\\t\\t\\t\\t\\t\\treturn false;\\n\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t", \'1884\', \'\', \'4,604,000\', \'UNC\', \'~ $2.08–$4.47\']\n[\' \', \'\', \'1885\', \'\', \'1,314,000\', \'UNC\', \'~ $3.20\']\n[\'\', \'\', \'1886\', \'\', \'444,000\', \'UNC\', \'—\']\n[\' \', \'\', \'1888\', \'\', \'413,000\', \'UNC\', \'~ $2.88\']\n[\' \', \'\', \'1889\', \'\', \'568,000\', \'UNC\', \'~ $2.56\']\n[\' \', " \\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t$(\'subGraph55342\').on(\'click\', function(event) {\\n\\t\\t\\t\\t\\t\\t\\t\\tLightview.show({\\n\\t\\t\\t\\t\\t\\t\\t\\t\\thref : \'/en/catalog/ajax/subgraph?id=55342\',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\trel : \'ajax\',\\n\\t\\t\\t\\t\\t\\t\\t\\t\\toptions : {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tautosize : true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttopclose : true,\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tajax : {\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tevalScripts : true\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t} \\n\\t\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t\\t\\tevent.stop();\\n\\t\\t\\t\\t\\t\\t\\t\\treturn false;\\n\\t\\t\\t\\t\\t\\t\\t});\\n\\t\\t\\t\\t\\t\\t", \'1890\', \'\', \'2,137,000\', \'UNC\', \'~ $1.28–$4.79\']\n[\'\', \'\', \'1891\', \'\', \'605,000\', \'UNC\', \'—\']\n[\' \', \'\', \'1892\', \'\', \'205,000\', \'UNC\', \'~ $4.47\']\n[\' \', \'\', \'1893\', \'\', \'754,000\', \'UNC\', \'~ $4.79\']\n[\' \', \'\', \'1894\', \'\', \'532,000\', \'UNC\', \'~ $3.20\']\n[\' \', \'\', \'1895\', \'\', \'423,000\', \'UNC\', \'~ $2.40\']\n[\'\', \'\', \'1896\', \'\', \'174,000\', \'UNC\', \'—\']\n\n\nBut when I\'m trying to save it to Dataframe and export to excel it contains just the last value:   \n\n         0\n0         \n1         \n2     1896\n3         \n4  174,000\n5      UNC\n6        —\n\n'
"My pandas dataframe looks like this:\n   Person  ID   ZipCode   Gender\n0  12345   882  38182     Female\n1  32917   271  88172     Male\n2  18273   552  90291     Female\n\nI want to replicate every row 3 times like:\n   Person  ID   ZipCode   Gender\n0  12345   882  38182     Female\n0  12345   882  38182     Female\n0  12345   882  38182     Female\n1  32917   271  88172     Male\n1  32917   271  88172     Male\n1  32917   271  88172     Male\n2  18273   552  90291     Female\n2  18273   552  90291     Female\n2  18273   552  90291     Female\n\nAnd of course, reset the index so it is:\n0\n1\n2\n...\n\nI tried solutions such as:\npd.concat([df[:5]]*3, ignore_index=True)\n\nAnd:\ndf.reindex(np.repeat(df.index.values, df['ID']), method='ffill')\n\nBut none of them worked.\n"
'I have a pandas data frame which looks like this. \n\n  Column1  Column2 Column3\n0     cat        1       C\n1     dog        1       A\n2     cat        1       B\n\n\nI want to identify that cat and bat are same values which have been repeated and hence want to remove one record and preserve only the first record. The resulting data frame should only have. \n\n  Column1  Column2 Column3\n0     cat        1       C\n1     dog        1       A\n\n'
"I am raising this question for my self learning. As far as I know, followings are the different methods to remove columns in pandas dataframe.\n\nOption - 1:\n\ndf=pd.DataFrame({'a':[1,2,3,4,5],'b':[6,7,8,9,10],'c':[11,12,13,14,15]})\ndel df['a']\n\n\nOption - 2:\n\ndf=pd.DataFrame({'a':[1,2,3,4,5],'b':[6,7,8,9,10],'c':[11,12,13,14,15]})\ndf=df.drop('a',1)\n\n\nOption - 3:\n\ndf=pd.DataFrame({'a':[1,2,3,4,5],'b':[6,7,8,9,10],'c':[11,12,13,14,15]})\ndf=df[['b','c']]\n\n\n\nWhat is the best approach among these? \nAny other approaches to achieve the same?\n\n"
"I'm trying to run pd.scatter_matrix() function in Jupyter Notebook with my code below:\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Load some data\niris = datasets.load_iris()\niris_df = pd.DataFrame(iris['data'], columns=iris['feature_names'])\niris_df['species'] = iris['target']\n\npd.scatter_matrix(iris_df, alpha=0.2, figsize=(10, 10))\nplt.show()\n\nBut I'm getting\nAttributeError: module 'pandas' has no attribute 'scatter_matrix'.\nEven after executing conda update pandas and conda update matplotlib commands in Terminal, this is still occurring.\nI executed pd.__version__ command to check my pandas version and it's '0.24.2'. What could be the problem?\n"
'I have a script that generates a pandas data frame with a varying number of value columns. As an example, this df might be\n\nimport pandas as pd\ndf = pd.DataFrame({\n\'group\': [\'A\', \'A\', \'A\', \'B\', \'B\'],\n\'group_color\' : [\'green\', \'green\', \'green\', \'blue\', \'blue\'],\n\'val1\': [5, 2, 3, 4, 5], \n\'val2\' : [4, 2, 8, 5, 7]\n})\n\n  group group_color  val1  val2\n0     A       green     5     4\n1     A       green     2     2\n2     A       green     3     8\n3     B        blue     4     5\n4     B        blue     5     7\n\n\nMy goal is to get the grouped mean for each of the value columns. In this specific case (with 2 value columns), I can use\n\ndf.groupby(\'group\').agg({"group_color": "first", "val1": "mean", "val2": "mean"})\n\n      group_color      val1      val2\ngroup                                \nA           green  3.333333  4.666667\nB            blue  4.500000  6.000000\n\n\nbut that does not work when the data frame in question has more value columns (val3, val4 etc.).\nIs there a way to dynamically take the mean of "all the other columns" or "all columns containing val in their names"?\n'
"I'm looking for a way to do something like the various rolling_* functions of pandas, but I want the window of the rolling computation to be defined by a range of values (say, a range of values of a column of the DataFrame), not by the number of rows in the window.\n\nAs an example, suppose I have this data:\n\n&gt;&gt;&gt; print d\n   RollBasis  ToRoll\n0          1       1\n1          1       4\n2          1      -5\n3          2       2\n4          3      -4\n5          5      -2\n6          8       0\n7         10     -13\n8         12      -2\n9         13      -5\n\n\nIf I do something like rolling_sum(d, 5), I get a rolling sum in which each window contains 5 rows.  But what I want is a rolling sum in which each window contains a certain range of values of RollBasis.  That is, I'd like to be able to do something like d.roll_by(sum, 'RollBasis', 5), and get a result where the first window contains all rows whose RollBasis is between 1 and 5, then the second window contains all rows whose RollBasis is between 2 and 6, then the third window contains all rows whose RollBasis is between 3 and 7, etc.  The windows will not have equal numbers of rows, but the range of RollBasis values selected in each window will be the same.  So the output should be like:\n\n&gt;&gt;&gt; d.roll_by(sum, 'RollBasis', 5)\n    1    -4    # sum of elements with 1 &lt;= Rollbasis &lt;= 5\n    2    -4    # sum of elements with 2 &lt;= Rollbasis &lt;= 6\n    3    -6    # sum of elements with 3 &lt;= Rollbasis &lt;= 7\n    4    -2    # sum of elements with 4 &lt;= Rollbasis &lt;= 8\n    # etc.\n\n\nI can't do this with groupby, because groupby always produces disjoint groups.  I can't do it with the rolling functions, because their windows always roll by number of rows, not by values.  So how can I do it?\n"
"Is it possible to suppress the array output when plotting a histogram in ipython?:\nFor example:\nplt.hist(OIR['Range'], bins, named=True, histtype='bar')\n\noutputs/prints the array information before displaying the graph.\n\n\n"
'I have about 7 million rows in an HDFStore with more than 60 columns. The data is more than I can fit into memory. I\'m looking to aggregate the data into groups based on the value of a column "A". The documentation for pandas splitting/aggregating/combining assumes that I have all my data in a DataFrame already, however I can\'t read the entire store into an in-memory DataFrame. What is the correct approach for grouping data in an HDFStore?\n'
"I know how to do element by element multiplication between two Pandas dataframes. However, things get more complicated when the dimensions of the two dataframes are not compatible. For instance below df * df2 is straightforward, but df * df3 is a problem:\n\ndf = pd.DataFrame({'col1' : [1.0] * 5, \n                   'col2' : [2.0] * 5, \n                   'col3' : [3.0] * 5 }, index = range(1,6),)\ndf2 = pd.DataFrame({'col1' : [10.0] * 5, \n                    'col2' : [100.0] * 5, \n                    'col3' : [1000.0] * 5 }, index = range(1,6),)\ndf3 = pd.DataFrame({'col1' : [0.1] * 5}, index = range(1,6),)\n\ndf.mul(df2, 1) # element by element multiplication no problems\n\ndf.mul(df3, 1) # df(row*col) is not equal to df3(row*col)\n   col1  col2  col3\n1   0.1   NaN   NaN\n2   0.1   NaN   NaN\n3   0.1   NaN   NaN\n4   0.1   NaN   NaN\n5   0.1   NaN   NaN\n\n\nIn the above situation, how can I multiply every column of df with df3.col1?\n\nMy attempt: I tried to replicate df3.col1  len(df.columns.values) times to get a dataframe that is of the same dimension as df:\n\ndf3 = pd.DataFrame([df3.col1 for n in range(len(df.columns.values)) ])\ndf3\n        1    2    3    4    5\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\ncol1  0.1  0.1  0.1  0.1  0.1\n\n\nBut this creates a dataframe of dimensions 3 * 5, whereas I am after 5*3. I know I can take the transpose with df3.T() to get what I need but I think this is not that the fastest way.\n"
'I have two dataframes, both of which contain an irregularly spaced, millisecond resolution timestamp column. My goal here is to match up the rows so that for each matched row, 1) the first time stamp is always smaller or equal to the second timestamp, and 2) the matched timestamps are the closest for all pairs of timestamps satisfying 1). \n\nIs there any way to do this with pandas.merge?\n'
"If I have a DataFrame:\n\nmyDF = DataFrame(data=[[11,11],[22,'2A'],[33,33]], columns = ['A','B'])\n\n\nGives the following dataframe (Starting out on stackoverflow and don't have enough reputation for an image of the DataFrame)\n\n   | A  | B  |\n\n0  | 11 | 11 |\n\n1  | 22 | 2A |\n\n2  | 33 | 33 |\n\n\nIf i want to convert column B to int values and drop values that can't be converted I have to do:\n\ndef convertToInt(cell):\n    try:\n        return int(cell)\n    except:\n        return None\nmyDF['B'] = myDF['B'].apply(convertToInt)\n\n\nIf I only do:\n\n\n  myDF['B'].apply(int)\n\n\nthe error obviously is:\n\n\n  C:\\WinPython-32bit-2.7.5.3\\python-2.7.5\\lib\\site-packages\\pandas\\lib.pyd\n  in pandas.lib.map_infer (pandas\\lib.c:42840)()\n  \n  ValueError: invalid literal for int() with base 10: '2A'\n\n\nIs there a way to add exception handling to myDF['B'].apply()\n\nThank you in advance!\n"
'I have searched a bit, but can not find a good answer. I want to create an empty dataframe with same dimensions as another dataframe so I can add new columns. Today I create an empty dataframe filled with zeroes, and then I delete the zero column. I hope there is a better way, but can not find the answer. Can someone help me?\n\nI do like this today and it works, but it is very ugly.\n\ndf_copy = pandas.DataFrame(numpy.zeros(len(df_original.index))) \ndf_copy = df_copy.drop([0],axis=1) \n\n\nAnd now I can add new columns as I process data. So basically I want an empty dataframe with same dimensions as another dataframe.\n\ndf_copy["price"] = pricesList\ndf_copy["size"] = sizesList\n\n\nEDIT: Another closely related question: how do I create an empty Dataframe with dimensions mxn? I have got the answer below how to create an empty dataframe with dimensions 1xn, which is by setting the index. But how do I create an empty nxm dataframe filled with zeroes? The reason I am asking, is because I suspect(?) it is faster to create a zero filled dataframe, and then replace each element as needed. The alternative is to create an empty dataframe with dimensions 1xn and then add columns as needed - which I am told is slow. So it might be faster to create an empty dataframe with nxm dimensions and then replace elements as needed (by copying a list to each column). Say a column has 100 rows, and I create a sublist with 25 rows, so I just copy this list to the correct subcolumn, and repeat. This is faster than adding a new column? \n'
"I frequently have a dataframe with a large multiindex, and a secondary DataFrame with a MultiIndex that is a subset of the larger one. The secondary dataframe is usually some kind of lookup table. I often want to add the columns from the lookup table to the larger dataframe. The primary DataFrame is often very large, so I want to do this efficiently.\n\nHere is an imaginary example, where I construct two dataframes df1 and df2\n\nimport pandas as pd\nimport numpy as np\n\narrays = [['sun', 'sun', 'sun', 'moon', 'moon', 'moon', 'moon', 'moon'],\n          ['summer', 'winter', 'winter', 'summer', 'summer', 'summer', 'winter', 'winter'],\n          ['one', 'one', 'two', 'one', 'two', 'three', 'one', 'two']]\n\ntuples = list(zip(*arrays))\nindex = pd.MultiIndex.from_tuples(tuples, names=['Body', 'Season','Item'])\ndf1 = pd.DataFrame(np.random.randn(8,2), index=index,columns=['A','B'])\n\nindex2= pd.MultiIndex.from_tuples([('sun','summer'),('sun','winter'),('moon','summer'),('moon','winter')],\n                                  names=['Body','Season'])\n\ndf2 = pd.DataFrame(['Good','Bad','Ugly','Confused'],index=index2,columns = ['Mood'])\n\n\nGiving the dataframes:\n\ndf1\n\n                    A         B\nBody Season Item                     \nsun  summer one   -0.409372  0.638502\n     winter one    1.448772 -1.460596\n            two   -0.495634 -0.839063\nmoon summer one    1.296035 -1.439349\n            two   -1.002667  0.508394\n            three -1.247748 -0.645782\n     winter one   -1.848857 -0.858759\n            two    0.559172  2.202957\n\n\ndf2\n\n                 Mood\nBody Season          \nsun  summer      Good\n     winter       Bad\nmoon summer      Ugly\n     winter  Confused\n\n\nNow, suppose I want to add the columns from df2 to df1? This line is the only way I could find to do the job:\n\ndf1 = df1.reset_index().join(df2,on=['Body','Season']).set_index(df1.index.names)\n\n\nresulting in:\n\n           A         B      Mood\nBody Season Item\nsun  summer one   -0.121588  0.272774      Good\n     winter one    0.233562 -2.005623       Bad\n            two   -1.034642  0.315065       Bad\nmoon summer one    0.184548  0.820873      Ugly\n            two    0.838290  0.495047      Ugly\n            three  0.450813 -2.040089      Ugly\n     winter one   -1.149993 -0.498148  Confused\n            two    2.406824 -2.031849  Confused\n\n[8 rows x 3 columns]\n\n\nIt works, but there are two problems with this method. First, the line is ugly. Needing to reset the index, then recreate the multiindex, makes this simple operation seem needlessly complicated. Second, if I understand correctly, every time I run reset_index() and set_index(), a copy of the dataframe is created. I am often working with very large dataframes, and this seems very inefficient. \n\nIs there a better way to do this?\n"
'I a importing a .csv file in python with pandas.\n\nHere is the file format from the .csv :\n\na1;b1;c1;d1;e1;...\na2;b2;c2;d2;e2;...   \n.....\n\n\nhere is how  get it :\n\nfrom pandas import *\ncsv_path = "C:...."\ndata = read_csv(csv_path)\n\n\nNow when I print the file I get that :\n\n0  a1;b1;c1;d1;e1;...\n1  a2;b2;c2;d2;e2;...   \n\n\nAnd so on... So I need help to read the file and split the values in columns, with the semi color character ;.\n'
"How can I calculate the age of a person (based off the dob column) and add a column to the dataframe with the new value?\n\ndataframe looks like the following:\n\n    lname      fname     dob\n0    DOE       LAURIE    03011979\n1    BOURNE    JASON     06111978\n2    GRINCH    XMAS      12131988\n3    DOE       JOHN      11121986\n\n\nI tried doing the following:\n\nnow = datetime.now()\ndf1['age'] = now - df1['dob']\n\n\nBut, received the following error:\n\nTypeError: unsupported operand type(s) for -: 'datetime.datetime' and 'str'\n"
"I am developing a set of python scripts to pre-process a dataset then produce a series of machine learning models using scikit-learn. I would like to develop a set of unittests to check the data pre-processing functions, and would like to be able to use a small test pandas dataframe for which I can determine the answers for and use it in assert statements.\n\nI cannot seem to get it to load the dataframe and to pass it to the unit tests using self. My code looks something like this;\n\ndef setUp(self):\n    TEST_INPUT_DIR = 'data/'\n    test_file_name =  'testdata.csv'\n    try:\n        data = pd.read_csv(INPUT_DIR + test_file_name,\n            sep = ',',\n            header = 0)\n    except IOError:\n        print 'cannot open file'\n    self.fixture = data\n\ndef tearDown(self):\n    del self.fixture\n\ndef test1(self):    \n    self.assertEqual(somefunction(self.fixture), somevalue)\n\nif __name__ == '__main__':\n    unittest.main()\n\n\nThanks for the help.\n"
'I am new to pandas , I am trying to load the csv in Dataframe. My data has missing values represented as ? , and I am trying to replace it with standard Missing values - NaN\nKindly help me with this . I have tried reading through Pandas docs, but I am not able to follow.\ndef readData(filename):\n    DataLabels =[&quot;age&quot;, &quot;workclass&quot;, &quot;fnlwgt&quot;, &quot;education&quot;, &quot;education-num&quot;, &quot;marital-status&quot;,\n               &quot;occupation&quot;, &quot;relationship&quot;, &quot;race&quot;, &quot;sex&quot;, &quot;capital-gain&quot;,\n               &quot;capital-loss&quot;, &quot;hours-per-week&quot;, &quot;native-country&quot;, &quot;class&quot;] \n\n    # ==== trying to replace ? with Nan using na_values\n    rawfile = pd.read_csv(filename, header=None, names=DataLabels, na_values=[&quot;?&quot;])\n    age = rawfile[&quot;age&quot;]\n    print(age)\n    print(rawfile[25:40])\n\n    #========trying to replace ?\n    rawfile.replace(&quot;?&quot;, &quot;NaN&quot;)\n    print(rawfile[25:40])\n    return rawfile\n\n    age   workclass  fnlwgt      education  education-num       marital-status        occupation    relationship                 race    sex  capital-gain  capital-loss  hours-per-week  native-country   class\n25   56   Local-gov  216851      Bachelors             13   Married-civ-spouse      Tech-support         Husband                White   Male             0             0              40   United-States    &gt;50K\n26   19     Private  168294        HS-grad              9        Never-married      Craft-repair       Own-child                White   Male             0             0              40   United-States   &lt;=50K\n27   54           ?  180211   Some-college             10   Married-civ-spouse                 ?         Husband   Asian-Pac-Islander   Male             0             0              60           South    &gt;50K\n28   39     Private  367260        HS-grad              9             Divorced   Exec-managerial   Not-in-family                White   Male             0             0              80   United-States   &lt;=50K\n29   49     Private  193366        HS-grad              9   Married-civ-spouse      Craft-repair         Husband                White   Male             0             0              40   United-States   &lt;=50K\n\nData\nadult.data\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, &lt;=50K\n50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, &lt;=50K\n38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, &lt;=50K\n53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, &lt;=50K\n28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, &lt;=50K\n37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, &lt;=50K\n49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, &lt;=50K\n52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, &gt;50K\n31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, &gt;50K\n42, Private, 159449, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 5178, 0, 40, United-States, &gt;50K\n37, Private, 280464, Some-college, 10, Married-civ-spouse, Exec-managerial, Husband, Black, Male, 0, 0, 80, United-States, &gt;50K\n30, State-gov, 141297, Bachelors, 13, Married-civ-spouse, Prof-specialty, Husband, Asian-Pac-Islander, Male, 0, 0, 40, India, &gt;50K\n23, Private, 122272, Bachelors, 13, Never-married, Adm-clerical, Own-child, White, Female, 0, 0, 30, United-States, &lt;=50K\n32, Private, 205019, Assoc-acdm, 12, Never-married, Sales, Not-in-family, Black, Male, 0, 0, 50, United-States, &lt;=50K\n40, Private, 121772, Assoc-voc, 11, Married-civ-spouse, Craft-repair, Husband, Asian-Pac-Islander, Male, 0, 0, 40, ?, &gt;50K\n34, Private, 245487, 7th-8th, 4, Married-civ-spouse, Transport-moving, Husband, Amer-Indian-Eskimo, Male, 0, 0, 45, Mexico, &lt;=50K\n25, Self-emp-not-inc, 176756, HS-grad, 9, Never-married, Farming-fishing, Own-child, White, Male, 0, 0, 35, United-States, &lt;=50K\n32, Private, 186824, HS-grad, 9, Never-married, Machine-op-inspct, Unmarried, White, Male, 0, 0, 40, United-States, &lt;=50K\n38, Private, 28887, 11th, 7, Married-civ-spouse, Sales, Husband, White, Male, 0, 0, 50, United-States, &lt;=50K\n43, Self-emp-not-inc, 292175, Masters, 14, Divorced, Exec-managerial, Unmarried, White, Female, 0, 0, 45, United-States, &gt;50K\n40, Private, 193524, Doctorate, 16, Married-civ-spouse, Prof-specialty, Husband, White, Male, 0, 0, 60, United-States, &gt;50K\n54, Private, 302146, HS-grad, 9, Separated, Other-service, Unmarried, Black, Female, 0, 0, 20, United-States, &lt;=50K\n35, Federal-gov, 76845, 9th, 5, Married-civ-spouse, Farming-fishing, Husband, Black, Male, 0, 0, 40, United-States, &lt;=50K\n43, Private, 117037, 11th, 7, Married-civ-spouse, Transport-moving, Husband, White, Male, 0, 2042, 40, United-States, &lt;=50K\n59, Private, 109015, HS-grad, 9, Divorced, Tech-support, Unmarried, White, Female, 0, 0, 40, United-States, &lt;=50K\n56, Local-gov, 216851, Bachelors, 13, Married-civ-spouse, Tech-support, Husband, White, Male, 0, 0, 40, United-States, &gt;50K\n19, Private, 168294, HS-grad, 9, Never-married, Craft-repair, Own-child, White, Male, 0, 0, 40, United-States, &lt;=50K\n54, ?, 180211, Some-college, 10, Married-civ-spouse, ?, Husband, Asian-Pac-Islander, Male, 0, 0, 60, South, &gt;50K\n39, Private, 367260, HS-grad, 9, Divorced, Exec-managerial, Not-in-family, White, Male, 0, 0, 80, United-States, &lt;=50K\n49, Private, 193366, HS-grad, 9, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n23, Local-gov, 190709, Assoc-acdm, 12, Never-married, Protective-serv, Not-in-family, White, Male, 0, 0, 52, United-States, &lt;=50K\n20, Private, 266015, Some-college, 10, Never-married, Sales, Own-child, Black, Male, 0, 0, 44, United-States, &lt;=50K\n45, Private, 386940, Bachelors, 13, Divorced, Exec-managerial, Own-child, White, Male, 0, 1408, 40, United-States, &lt;=50K\n30, Federal-gov, 59951, Some-college, 10, Married-civ-spouse, Adm-clerical, Own-child, White, Male, 0, 0, 40, United-States, &lt;=50K\n22, State-gov, 311512, Some-college, 10, Married-civ-spouse, Other-service, Husband, Black, Male, 0, 0, 15, United-States, &lt;=50K\n48, Private, 242406, 11th, 7, Never-married, Machine-op-inspct, Unmarried, White, Male, 0, 0, 40, Puerto-Rico, &lt;=50K\n21, Private, 197200, Some-college, 10, Never-married, Machine-op-inspct, Own-child, White, Male, 0, 0, 40, United-States, &lt;=50K\n19, Private, 544091, HS-grad, 9, Married-AF-spouse, Adm-clerical, Wife, White, Female, 0, 0, 25, United-States, &lt;=50K\n31, Private, 84154, Some-college, 10, Married-civ-spouse, Sales, Husband, White, Male, 0, 0, 38, ?, &gt;50K\n48, Self-emp-not-inc, 265477, Assoc-acdm, 12, Married-civ-spouse, Prof-specialty, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n31, Private, 507875, 9th, 5, Married-civ-spouse, Machine-op-inspct, Husband, White, Male, 0, 0, 43, United-States, &lt;=50K\n53, Self-emp-not-inc, 88506, Bachelors, 13, Married-civ-spouse, Prof-specialty, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n24, Private, 172987, Bachelors, 13, Married-civ-spouse, Tech-support, Husband, White, Male, 0, 0, 50, United-States, &lt;=50K\n49, Private, 94638, HS-grad, 9, Separated, Adm-clerical, Unmarried, White, Female, 0, 0, 40, United-States, &lt;=50K\n25, Private, 289980, HS-grad, 9, Never-married, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 35, United-States, &lt;=50K\n57, Federal-gov, 337895, Bachelors, 13, Married-civ-spouse, Prof-specialty, Husband, Black, Male, 0, 0, 40, United-States, &gt;50K\n53, Private, 144361, HS-grad, 9, Married-civ-spouse, Machine-op-inspct, Husband, White, Male, 0, 0, 38, United-States, &lt;=50K\n44, Private, 128354, Masters, 14, Divorced, Exec-managerial, Unmarried, White, Female, 0, 0, 40, United-States, &lt;=50K\n41, State-gov, 101603, Assoc-voc, 11, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n29, Private, 271466, Assoc-voc, 11, Never-married, Prof-specialty, Not-in-family, White, Male, 0, 0, 43, United-States, &lt;=50K\n25, Private, 32275, Some-college, 10, Married-civ-spouse, Exec-managerial, Wife, Other, Female, 0, 0, 40, United-States, &lt;=50K\n18, Private, 226956, HS-grad, 9, Never-married, Other-service, Own-child, White, Female, 0, 0, 30, ?, &lt;=50K\n47, Private, 51835, Prof-school, 15, Married-civ-spouse, Prof-specialty, Wife, White, Female, 0, 1902, 60, Honduras, &gt;50K\n50, Federal-gov, 251585, Bachelors, 13, Divorced, Exec-managerial, Not-in-family, White, Male, 0, 0, 55, United-States, &gt;50K\n47, Self-emp-inc, 109832, HS-grad, 9, Divorced, Exec-managerial, Not-in-family, White, Male, 0, 0, 60, United-States, &lt;=50K\n43, Private, 237993, Some-college, 10, Married-civ-spouse, Tech-support, Husband, White, Male, 0, 0, 40, United-States, &gt;50K\n46, Private, 216666, 5th-6th, 3, Married-civ-spouse, Machine-op-inspct, Husband, White, Male, 0, 0, 40, Mexico, &lt;=50K\n35, Private, 56352, Assoc-voc, 11, Married-civ-spouse, Other-service, Husband, White, Male, 0, 0, 40, Puerto-Rico, &lt;=50K\n41, Private, 147372, HS-grad, 9, Married-civ-spouse, Adm-clerical, Husband, White, Male, 0, 0, 48, United-States, &lt;=50K\n30, Private, 188146, HS-grad, 9, Married-civ-spouse, Machine-op-inspct, Husband, White, Male, 5013, 0, 40, United-States, &lt;=50K\n30, Private, 59496, Bachelors, 13, Married-civ-spouse, Sales, Husband, White, Male, 2407, 0, 40, United-States, &lt;=50K\n32, ?, 293936, 7th-8th, 4, Married-spouse-absent, ?, Not-in-family, White, Male, 0, 0, 40, ?, &lt;=50K\n48, Private, 149640, HS-grad, 9, Married-civ-spouse, Transport-moving, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n42, Private, 116632, Doctorate, 16, Married-civ-spouse, Prof-specialty, Husband, White, Male, 0, 0, 45, United-States, &gt;50K\n29, Private, 105598, Some-college, 10, Divorced, Tech-support, Not-in-family, White, Male, 0, 0, 58, United-States, &lt;=50K\n36, Private, 155537, HS-grad, 9, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n28, Private, 183175, Some-college, 10, Divorced, Adm-clerical, Not-in-family, White, Female, 0, 0, 40, United-States, &lt;=50K\n53, Private, 169846, HS-grad, 9, Married-civ-spouse, Adm-clerical, Wife, White, Female, 0, 0, 40, United-States, &gt;50K\n49, Self-emp-inc, 191681, Some-college, 10, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 50, United-States, &gt;50K\n25, ?, 200681, Some-college, 10, Never-married, ?, Own-child, White, Male, 0, 0, 40, United-States, &lt;=50K\n19, Private, 101509, Some-college, 10, Never-married, Prof-specialty, Own-child, White, Male, 0, 0, 32, United-States, &lt;=50K\n31, Private, 309974, Bachelors, 13, Separated, Sales, Own-child, Black, Female, 0, 0, 40, United-States, &lt;=50K\n29, Self-emp-not-inc, 162298, Bachelors, 13, Married-civ-spouse, Sales, Husband, White, Male, 0, 0, 70, United-States, &gt;50K\n23, Private, 211678, Some-college, 10, Never-married, Machine-op-inspct, Not-in-family, White, Male, 0, 0, 40, United-States, &lt;=50K\n79, Private, 124744, Some-college, 10, Married-civ-spouse, Prof-specialty, Other-relative, White, Male, 0, 0, 20, United-States, &lt;=50K\n\n'
"the pie chart example on pandas plotting tutorial http://pandas.pydata.org/pandas-docs/version/0.15.0/visualization.html generates the following figure:\n\n\n\nwith this code: \n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport numpy as np\nnp.random.seed(123456)\n\n\nimport pandas as pd\ndf = pd.DataFrame(3 * np.random.rand(4, 2), index=['a', 'b', 'c', 'd'], columns=['x', 'y'])\n\nf, axes = plt.subplots(1,2, figsize=(10,5))\nfor ax, col in zip(axes, df.columns):\n    df[col].plot(kind='pie', autopct='%.2f', labels=df.index,  ax=ax, title=col, fontsize=10)\n    ax.legend(loc=3)\n\nplt.show()\n\n\nI want to remove the text label (a,b,c,d) from both subplots, because for my application those label are long, so I only want to show them in legend. \n\nAfter read this: How to add a legend to matplotlib pie chart?, I figure out an way with matplotlib.pyplot.pie but the figure is not as fancy even if i am still using ggplot. \n\nf, axes = plt.subplots(1,2, figsize=(10,5))\nfor ax, col in zip(axes, df.columns):\n    patches, text, _ = ax.pie(df[col].values, autopct='%.2f')\n    ax.legend(patches, labels=df.index, loc='best')\n\n\n\n\nMy question is, is there a way that can combine the things I want from both side? to be clear, I want the fanciness from pandas, but remove the text from the wedges. \n\nThank you \n"
"I have a dataframe with about 500 columns and that's why I am wondering if there is anyway that I could use head() function but want to see the first 50 columns for example.\n\nThanks\n"
"Background\n\nI am doing some simulations resp. a system analysis by variing parameters (in this case rpm only) and append every last line of a results dataframe results_df to a summarizing dataframe df containing giving the baviour of my system in depencence of the varied rpm.\n\nIn order to get an appropriate index for plotting and data analysis I converted the varied values (here rpm) from the list into a pandas series ser and concat this series with the summarizing dataframe df containing the results I am interested in.\n\nSince the results of each calculation I am interested in is only last line of each calculation I am extracting this data from the results dataframe results_df by using .tail(1).\n\nWhat I have done so far is shown in the following snippet:\n\nrpm = [0.25, 0.3, 0.5, 0.75, 1.0, 1.5, 2.0]\n\nser = pd.Series(rpm, name='rpm')\ndf = pd.DataFrame()\ndf_list = list()\n\nfor i, val in enumerate(rpm):\n    results_df = get_some_data_from_somwhere()\n    df_list.append(results_df.tail(1))\n\ndf = df.append(df_list, ignore_index=True)\ndf = pd.concat([df, ser], axis=1)\ndf.set_index('rpm', inplace=True)\n\n\nwith open('foo.csv', 'w') as f:\n    data.to_csv(f, index=True, header=True, decimal=',', sep=' ', float_format='%.3f')\n\n\nProblem\n\nThis csv-file what I get has the follwing format:\n\nrpm cooling_inner heating_inner cooling_outlet heating_outlet\n0.25 303,317 323,372 302,384 324,332\n\n\nHowever, I expected having three decimal digits and a comma as decimal sign on my index column, like shown here:\n\nrpm cooling_inner heating_inner cooling_outlet heating_outlet\n0,250 303,317 323,372 302,384 324,332\n\n\nSo it seems that the index and decimal sign options are not applied to the index column when exporting dataframes to csv-files using the .to_csv command.\n\nHow could I achieve this behaviour since the index option is set True and all values (with exception to the index column) have the right format and decimal sign?\n\nDo I have to handle the index column somehow seperate?\n"
'Is there a faster way to drop columns that only contain one distinct value than the code below?\n\ncols=df.columns.tolist()\nfor col in cols:\n    if len(set(df[col].tolist()))&lt;2:\n        df=df.drop(col, axis=1)\n\n\nThis is really quite slow for large dataframes. Logically, this counts the number of values in each column when in fact it could just stop counting after reaching 2 different values.\n'
"I have two columns with strings. I would like to combine them and ignore nan values. Such that:\n\nColA, Colb, ColA+ColB\nstr   str    strstr\nstr   nan    str\nnan   str    str\n\n\nI tried df['ColA+ColB'] = df['ColA'] + df['ColB'] but that creates a nan value if either column is nan. I've also thought about using concat.\n\nI suppose I could just go with that, and then use some df.ColA+ColB[df[ColA] = nan] = df[ColA] but that seems like quite the workaround.\n"
"Here is the code that I am working with:\n\nimport pandas as pd\n\ntest3 = pd.Series([1,2,3], index = ['a','b','c'])\ntest3 = test3.reindex(index = ['f','g','z'])\n\n\nSo originally every thing is fine and test3 has an index of 'a' 'b' 'c' and values 1,2,3. But then when I got to reindex test3 I get that my values 1 2 3 are lost. Why is that? The desired output would be:\n\nf 1\ng 2\nz 3\n\n"
"I have created a dataframe and set an index:\n\ndf = pd.DataFrame(np.random.randn(8, 4),columns=['A', 'B', 'C', 'D'])\ndf = df.set_index('A')\n\n\nThe dataframe looks like this:\n\n                  B         C         D\nA\n 0.687263 -1.700568  0.140175  1.420394\n-0.212621 -0.700442 -0.041497 -1.034021\n-0.614214 -0.437313 -0.464493 -1.182492\n-0.885062  0.203892 -0.412400 -0.578346\n-1.222661  2.014908 -0.463674 -0.378910\n 0.132472 -0.389512  0.623531 -0.788556\n-1.083620  1.167158 -0.558217 -0.222078\n 1.066270 -0.215586 -0.884757 -0.878557\n\n\nHow do I get the value of B in the row for which A is 0.687263?\n\nI've tried:\n\ne = df.loc(0.687263)\n\n\nThis gives me a LocIndexer object, rather than the row I'd expect (also I'd like to specify that it should be a single row if possible):\n\n&lt;pandas.core.indexing._LocIndexer object at 0x10385e210&gt;\n\n\nAnd if I now try e['B'] I get an error. \n\nHow do I get the value of B?\n"
"I have a panda data frame.  One of the columns contains a list.  I want that column to be a single string.\n\nFor example my list ['one','two','three'] should simply be 'one, two, three'\n\ndf['col'] = df['col'].astype(str).apply(lambda x: ', '.join(df['col'].astype(str)))\n\n\ngives me ['one, two, three],['four','five','six']  where the second list is from the next row. Needless to say with millions of rows this concatenation across rows is not only incorrect, it kills my memory.\n"
"I have a Pandas dataframe in my Flask app that I want to return as a CSV file.\n\nreturn Response(df.to_csv())\n\n\nThe problem is that the output appears in the browser instead of downloading as a separate file. How can I change that?\n\nI tried the following as well but it just gave empty output.\n\nresponse = make_response(df.to_csv())\nresponse.headers['Content-Type'] = 'text/csv'\nreturn Response(response)\n\n"
"I have a pandas dataframe as below. For each Id I can have multiple Names and Sub-ids.\n\nId      NAME   SUB_ID\n276956  A      5933\n276956  B      5934\n276956  C      5935\n287266  D      1589\n\n\nI want to condense the dataframe such that there is only one row for each id and all the names and sub_ids under each id appear as a singular set on that row\n\nId      NAME           SUB_ID\n276956  set(A,B,C)     set(5933,5934,5935)\n287266  set(D)         set(1589) \n\n\nI tried to groupby id and then aggregate over all the other columns \n\ndf.groupby('Id').agg(lambda x: set(x))\n\n\nBut in doing so the resulting dataframe does not have the Id column. When you do groupby the id is returned as the first value of the tuple but I guess when you aggregate that is lost. Is there a way to get the dataframe that I am looking for. That is to groupby and aggregate without losing the column which was grouped.\n"
"I have 2 Series, given by:\n\nimport pandas as pd\n\nr = pd.Series()\nfor i in range(0, 10):\n    r = r.set_value(i,i*3)\nr.name = 'rrr'\n\ns = pd.Series()\nfor i in range(0, 10):\n    s = s.set_value(i,i*5)\ns.name = 'sss'\n\n\nHow to I create a DataFrame from them?\n"
"The pandas.DataFrame.query() method is of great usage for (pre/post)-filtering data when loading or plotting. It comes particularly handy for method chaining.\nI find myself often wanting to apply the same logic to a pandas.Series, e.g. after having done a  method such as df.value_counts which returns a pandas.Series.\nExample\nLets assume there is a huge table with the columns Player, Game, Points and I want to plot a histogram of the players with more than 14 times 3 points. I first have to sum the points of each player (groupby -&gt; agg) which will return a Series of ~1000 players and their overall points. Applying the .query logic it would look something like this:\ndf = pd.DataFrame({\n    'Points': [random.choice([1,3]) for x in range(100)], \n    'Player': [random.choice([&quot;A&quot;,&quot;B&quot;,&quot;C&quot;]) for x in range(100)]})\n\n(df\n     .query(&quot;Points == 3&quot;)\n     .Player.values_count()\n     .query(&quot;&gt; 14&quot;)\n     .hist())\n\nThe only solutions I find force me to do an unnecessary assignment and break the method chaining:\n(points_series = df\n     .query(&quot;Points == 3&quot;)\n     .groupby(&quot;Player&quot;).size()\npoints_series[points_series &gt; 100].hist()\n\nMethod chaining as well as the query method help to keep the code legible meanwhile the subsetting-filtering can get messy quite quickly.\n# just to make my point :)\nseries_bestplayers_under_100[series_prefiltered_under_100 &gt; 0].shape\n\nPlease help me out of my dilemma! Thanks\n"
"Im using pandas datareader to get stock data. \n\nimport pandas as pd\nimport pandas_datareader.data as web\nABB = web.DataReader(name='ABB.ST', \n                     data_source='yahoo',\n                     start='2000-1-1')\n\n\nHowever by default freq is not set on the resulting dataframe.\nI need freq to be able to navigate using the index like this:\n\nfor index, row in ABB.iterrows():\n    ABB.loc[[index + 1]]\n\n\nIf freq is not set on DatetimeIndex im not able to use +1 etc to navigate.\n\nWhat I have found are two functions astype and resample. Since I already know to freq resample looks like overkill, I just want to set freq to daily.\n\nNow my question is how can i use astype on ABB to set freq to daily?\n"
'Given that I have the following two vectors:\n\nIn [99]: time_index\nOut[99]: \n[1484942413,\n 1484942712,\n 1484943012,\n 1484943312,\n 1484943612,\n 1484943912,\n 1484944212,\n 1484944511,\n 1484944811,\n 1484945110]\n\nIn [100]: bytes_in\nOut[100]: \n[1293981210388,\n 1293981379944,\n 1293981549960,\n 1293981720866,\n 1293981890968,\n 1293982062261,\n 1293982227492,\n 1293982391244,\n 1293982556526,\n 1293982722320]\n\n\nWhere bytes_in is an incremental only counter, and time_index is a list to unix timestamps (epoch).\n\nObjective: What I would like to calculate is the bitrate.\n\nThat means that I will build a data frame like\n\nIn [101]: timeline = pandas.to_datetime(time_index, unit="s")\n\nIn [102]: recv = pandas.Series(bytes_in, timeline).resample("300S").mean().ffill().apply(lambda i: i*8)\n\nIn [103]: recv\nOut[103]: \n2017-01-20 20:00:00    10351849683104\n2017-01-20 20:05:00    10351851039552\n2017-01-20 20:10:00    10351852399680\n2017-01-20 20:15:00    10351853766928\n2017-01-20 20:20:00    10351855127744\n2017-01-20 20:25:00    10351856498088\n2017-01-20 20:30:00    10351857819936\n2017-01-20 20:35:00    10351859129952\n2017-01-20 20:40:00    10351860452208\n2017-01-20 20:45:00    10351861778560\nFreq: 300S, dtype: int64\n\n\nQuestion: Now, what is strange, calculating the gradient manually gives me :\n\nIn [104]: (bytes_in[1]-bytes_in[0])*8/300\nOut[104]: 4521.493333333333\n\n\nwhich is the correct value ..\n\nwhile calculating the gradient with pandas gives me\n\nIn [124]: recv.diff()\nOut[124]: \n2017-01-20 20:00:00          NaN\n2017-01-20 20:05:00    1356448.0\n2017-01-20 20:10:00    1360128.0\n2017-01-20 20:15:00    1367248.0\n2017-01-20 20:20:00    1360816.0\n2017-01-20 20:25:00    1370344.0\n2017-01-20 20:30:00    1321848.0\n2017-01-20 20:35:00    1310016.0\n2017-01-20 20:40:00    1322256.0\n2017-01-20 20:45:00    1326352.0\nFreq: 300S, dtype: float64\n\n\nwhich is not the same as above, 1356448.0 is different than 4521.493333333333\n\nCould you please enlighten on what I am doing wrong ?\n'
'&gt;&gt;&gt; data = data.drop(data.columns[[1,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19]], axis=1)\n&gt;&gt;&gt; data = data.drop(data.index[[0,1]],axis = 0)\n&gt;&gt;&gt; print(data.head())\n             0         2    3    4    20\n2  500292014600       .00  .00  .00  NaN\n3  500292014600    100.00  .00  .00  NaN\n4  500292014600  11202.00  .00  .00  NaN\n&gt;&gt;&gt; data = data.reset_index(drop = True)\n&gt;&gt;&gt; print(data.head())\n              0         2    3    4    20\n 0  500292014600       .00  .00  .00  NaN\n 1  500292014600    100.00  .00  .00  NaN\n 2  500292014600  11202.00  .00  .00  NaN\n\n\nHow come when i use df.reset_index the index of my columns is not reset?\nHow do I go about resetting this index to 0,1,2,3,4?\n'
"I have a pandas DataFrame consisting of some sensor readings taken over time like this:\n\n       diode1  diode2  diode3  diode4\nTime\n0.530       7       0      10      16\n1.218      17       7      14      19\n1.895      13       8      16      17\n2.570       8       2      16      17\n3.240      14       8      17      19\n3.910      13       6      17      18\n4.594      13       5      16      19\n5.265       9       0      12      16\n5.948      12       3      16      17\n6.632      10       2      15      17\n\n\nI have written code to add another row with the means of each column:\n\n# List of the averages for the test. \naverages = [df[key].describe()['mean'] for key in df]\nindexes = df.index.tolist()\nindexes.append('mean')\ndf.reindex(indexes)\n# Adding the mean row to the bottom of the DataFrame\n\ni = 0\nfor key in df:\n    df.set_value('mean', key, averages[i])\n    i += 1\n\n\nThis gives me the result I want, which is a DataFrame like this:\n\n       diode1  diode2  diode3  diode4\nTime\n0.53      7.0     0.0    10.0    16.0\n1.218    17.0     7.0    14.0    19.0\n1.895    13.0     8.0    16.0    17.0\n2.57      8.0     2.0    16.0    17.0\n3.24     14.0     8.0    17.0    19.0\n3.91     13.0     6.0    17.0    18.0\n4.594    13.0     5.0    16.0    19.0\n5.265     9.0     0.0    12.0    16.0\n5.948    12.0     3.0    16.0    17.0\n6.632    10.0     2.0    15.0    17.0\nmean     11.6     4.1    14.9    17.5\n\n\nHowever, I am sure that this is not the most efficient way of adding the row. I have tried using append with the means saved as a pandas Series but ended up with something like this:\n\n    diode1  diode2  diode3  diode4                     mean\n0      7.0     0.0    10.0    14.0                      NaN\n1      9.0     0.0    10.0    15.0                      NaN\n2     10.0     5.0    14.0    20.0                      NaN\n3      6.0     0.0     7.0    14.0                      NaN\n4      7.0     0.0    10.0    15.0                      NaN\n5      7.0     0.0     8.0    14.0                      NaN\n6      7.0     0.0    11.0    14.0                      NaN\n7      7.0     0.0     2.0    11.0                      NaN\n8      2.0     0.0     4.0    12.0                      NaN\n9      4.0     0.0     0.0     6.0                      NaN\n10     NaN     NaN     NaN     NaN  [11.6, 4.1, 14.9, 17.5]\n\n\nI was wondering if there was a more efficient means of adding a row with the index 'mean' and the averages of each column to the bottom of a pandas DataFrame. \n"
"How to change figsize for matshow() in jupyter notebook?\n\nFor example this code change figure size\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nd = pd.DataFrame({'one' : [1, 2, 3, 4, 5],\n                  'two' : [4, 3, 2, 1, 5]})\nplt.figure(figsize=(10,5))\nplt.plot(d.one, d.two)\n\n\nBut code below doesn't work\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nd = pd.DataFrame({'one' : [1, 2, 3, 4, 5],\n                  'two' : [4, 3, 2, 1, 5]})\nplt.figure(figsize=(10,5))\nplt.matshow(d.corr())\n\n"
'I have two pandas.DataFrames which I would like to combine into one. The dataframes have the same number of columns, in the same order, but have column headings in different languages. How can I efficiently combine these dataframes?\n\ndf_ger\nindex  Datum   Zahl1   Zahl2\n0      1-1-17  1       2\n1      2-1-17  3       4\n\ndf_uk\nindex  Date    No1     No2\n0      1-1-17  5       6\n1      2-1-17  7       8\n\ndesired output\nindex  Datum   Zahl1   Zahl2\n0      1-1-17  1       2\n1      2-1-17  3       4\n2      1-1-17  5       6\n3      2-1-17  7       8\n\n\nThe only approach I came up with so far is to rename the column headings and then use pd.concat([df_ger, df_uk], axis=0, ignore_index=True). However, I hope to find a more general approach.\n'
"I have a dataframe df and it has a Date column. I want to create two new data frames. One which contains all of the rows from df where the year equals some_year and another data frame which contains all of the rows of df where the year does not equal some_year. I know you can do df.ix['2000-1-1' : '2001-1-1'] but in order to get all of the rows which are not in 2000 requires creating 2 extra data frames and then concatenating/joining them.\n\nIs there some way like this?\n\ninclude = df[df.Date.year == year]\nexclude = df[df['Date'].year != year]\n\n\nThis code doesn't work, but is there any similar sort of way?\n"
'I know how to count the number of unique values in pandas series (one column in pandas dataframe).\n\npandas.Series.value_counts\n\n\nBut how do I check if they are all unique? Should I just compare value_counts with its length?\n'
"I have this Dataframe:\n\nimport pandas as pd\ndf = pd.DataFrame({'Hugo' : {'age' : 21, 'weight' : 75},\n                   'Bertram': {'age' : 45, 'weight' : 65},\n                   'Donald' : {'age' : 75, 'weight' : 85}}).T\ndf.index.names = ['name']\n\n\n         age  weight\nname                \nBertram   45      65\nDonald    75      85\nHugo      21      75\n\n\nI want to change the index to the column 'age':\n\ndf.set_index('age', inplace=True)\n\n     weight\nage        \n45       65\n75       85\n21       75\n\n\nThe old index-column name gets lost. Is there a way to change the index without losing the original index-column and getting the old column as 'normal' column again, so that it looks like this?\n\n     name       weight\nage        \n45   Bertram    65\n75   Donald     85\n21   Hugo       75\n\n"
"I am trying to write a dataframe to an Excel spreadsheet using ExcelWriter, but it keeps returning an error:\n\nopenpyxl.utils.exceptions.IllegalCharacterError\n\n\nI'm guessing there's some character in the dataframe that ExcelWriter doesn't like. It seems odd, because the dataframe is formed from three Excel spreadsheets, so I can't see how there could be a character that Excel doesn't like!\n\nIs there any way to iterate through a dataframe and replace characters that ExcelWriter doesn't like? I don't even mind if it simply deletes them.\n\nWhat's the best way or removing or replacing illegal characters from a dataframe?\n"
"I am having issues with joins in pandas and I am trying to figure out what is wrong. \n   Say I have a dataframe x:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\nData columns:\nclose    1941  non-null values\nhigh     1941  non-null values\nlow      1941  non-null values\nopen     1941  non-null values\ndtypes: float64(4)\n\n\nshould I be able to join it with y on index with a simple join command where y = x except colnames have +2.  \n\n &lt;class 'pandas.core.frame.DataFrame'&gt;\n DatetimeIndex: 1941 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00\n Data columns:\n close2    1941  non-null values\n high2     1941  non-null values\n low2      1941  non-null values\n open2     1941  non-null values\n dtypes: float64(4)\n\n y.join(x) or pandas.DataFrame.join(y,x):\n &lt;class 'pandas.core.frame.DataFrame'&gt;\n DatetimeIndex: 34879 entries, 2004-12-16 00:00:00 to 2012-07-12 00:00:00\n Data columns:\n close2    34879  non-null values\n high2     34879  non-null values\n low2      34879  non-null values\n open2     34879  non-null values\n close     34879  non-null values\n high      34879  non-null values\n low       34879  non-null values\n open      34879  non-null values\n dtypes: float64(8)\n\n\nI expect the final to have 1941 non-values for both. I tried merge as well but I have the same issue.\n\nI had thought the right answer was pandas.concat([x,y]), but this does not do what I intend either.\n\nIn [83]: pandas.concat([x,y]) \nOut[83]: &lt;class 'pandas.core.frame.DataFrame'&gt; \nDatetimeIndex: 3882 entries, 2004-10-19 00:00:00 to 2012-07-23 00:00:00 \nData columns: \nclose2 3882 non-null values \nhigh2 3882 non-null values \nlow2 3882 non-null values \nopen2 3882 non-null values \ndtypes: float64(4) \n\n\nedit: \nIf you are having issues with join, read Wes's answer below. I had one time stamp that was duplicated.\n"
"I have a dataframe with monthly financial data:\n\nIn [89]: vfiax_monthly.head()\nOut[89]: \n            year  month  day       d   open  close   high    low  volume  aclose\n2003-01-31  2003      1   31  731246  64.95  64.95  64.95  64.95       0   64.95\n2003-02-28  2003      2   28  731274  63.98  63.98  63.98  63.98       0   63.98\n2003-03-31  2003      3   31  731305  64.59  64.59  64.59  64.59       0   64.59\n2003-04-30  2003      4   30  731335  69.93  69.93  69.93  69.93       0   69.93\n2003-05-30  2003      5   30  731365  73.61  73.61  73.61  73.61       0   73.61\n\n\nI'm trying to calculate the returns like that:\n\nIn [90]: returns = (vfiax_monthly.open[1:] - vfiax_monthly.open[:-1])/vfiax_monthly.open[1:]\n\n\nBut I'm getting only zeroes:\n\nIn [91]: returns.head()\nOut[91]: \n2003-01-31   NaN\n2003-02-28     0\n2003-03-31     0\n2003-04-30     0\n2003-05-30     0\nFreq: BM, Name: open\n\n\nI think that's because the arithmetic operations get aligned on the index and that makes the [1:] and [:-1] useless.\n\nMy workaround is:\n\nIn [103]: returns = (vfiax_monthly.open[1:].values - vfiax_monthly.open[:-1].values)/vfiax_monthly.open[1:].values\n\nIn [104]: returns = pd.Series(returns, index=vfiax_monthly.index[1:])\n\nIn [105]: returns.head()\nOut[105]: \n2003-02-28   -0.015161\n2003-03-31    0.009444\n2003-04-30    0.076362\n2003-05-30    0.049993\n2003-06-30    0.012477\nFreq: BM\n\n\nIs there a better way to calculate the returns? I don't like the conversion to array and then back to Series.\n"
'I have a pandas dataframe with datetime index \n\nDate\n2013-02-22 00:00:00+00:00    0.280001\n2013-02-25 00:00:00+00:00    0.109999\n2013-02-26 00:00:00+00:00   -0.150000\n2013-02-27 00:00:00+00:00    0.130001\n2013-02-28 00:00:00+00:00    0.139999\nName: MOM12\n\n\nand want to evaluate the previous three values of the given datetime index.\n\ndate = "2013-02-27 00:00:00+00:00"\ndf.ix[date]\n\n\nI searched for this but since my index is a date I can\'t do\n\ndf.ix[int-1]\n\n'
"I have a pandas.DataFrame with a column called name containing strings.\nI would like to get a list of the names which occur more than once in the column. How do I do that?\n\nI tried:\n\nfuncs_groups = funcs.groupby(funcs.name)\nfuncs_groups[(funcs_groups.count().name&gt;1)]\n\n\nBut it doesn't filter out the singleton names.\n"
"I have the following 15 minute data as a dataframe for 3 years. With the first two columns being the index.\n\n2014-01-01 00:15:00  1269.6      \n2014-01-01 00:30:00  1161.6      \n2014-01-01 00:45:00  1466.4      \n2014-01-01 01:00:00  1365.6      \n2014-01-01 01:15:00  1362.6      \n2014-01-01 01:30:00  1064.0      \n2014-01-01 01:45:00  1171.2      \n2014-01-01 02:00:00  1171.0      \n2014-01-01 02:15:00  1330.4      \n2014-01-01 02:30:00  1309.6      \n2014-01-01 02:45:00  1308.4      \n2014-01-01 03:00:00  1494.0    \n\n\nI have used resample to get a second series with monthly averages.\n\ndata_Monthly = data.resample('1M', how='mean')\n\n\nHow can I divide the values in the last column by their monthly average with the result being still a time series on 15 minute granularity?\n"
'I have a DataFrame "df" with (time,ticker) Multiindex and bid/ask/etc data columns: \n\n\n\n                          tod    last     bid      ask      volume\n    time        ticker                  \n    2013-02-01  SPY       1600   149.70   150.14   150.17   1300\n                SLV       1600   30.44    30.38    30.43    3892\n                GLD       1600   161.20   161.19   161.21   3860\n\n\n\nI would like to select a second-level (level=1) cross section using multiple keys. Right now, I can do it using one key, i.e.\n\n\n\n    df.xs(\'SPY\', level=1)\n\n\n\nwhich gives me a timeseries of SPY. What is the best way to select a multi-key cross section, i.e. a combined cross-section of both SPY and GLD, something like:\n\n\n\n    df.xs([\'SPY\', \'GLD\'], level=1)\n\n\n\n?\n'
"I have a dataframe:\n\n&gt;&gt;&gt; dt\n                   COL000   COL001   QT\nSTK_ID  RPT_Date                       \nSTK000  20120331   2.6151   2.1467    1\n        20120630   4.0589   2.3442    2\n        20120930   4.4547   3.9204    3\n        20121231   4.1360   3.8559    4\nSTK001  20120331  -0.2178   0.9184    1\n        20120630  -1.9639   0.7900    2\n        20120930  -2.9147   1.0189    3\n        20121231  -2.5648   2.3743    4\nSTK002  20120331  -0.6426   0.9543    1\n        20120630  -0.3575   1.6085    2\n        20120930  -2.3549   0.7174    3\n        20121231  -3.4860   1.6324    4\n\n\nAnd I want the columns values divided by 'QT' column, somewhat like this:\n\ndt =  dt/dt.QT     # pandas does not accept this syntax\n\n\nThe desired output is:\n\nSTK_ID  RPT_Date        COL000       COL001  QT\nSTK000  20120331   2.615110188  2.146655745   1\n        20120630   2.029447265  1.172093561   1\n        20120930   1.484909881  1.306795608   1\n        20121231   1.034008443  0.963970609   1\nSTK001  20120331  -0.217808111  0.918355842   1\n        20120630  -0.981974837  0.394977675   1\n        20120930  -0.97157148   0.339633733   1\n        20121231  -0.641203355  0.593569537   1\nSTK002  20120331  -0.642567516  0.954323016   1\n        20120630  -0.178759288  0.804230898   1\n        20120930  -0.784982521  0.239117442   1\n        20121231  -0.871501505  0.408094317   1\n\n\nHow to do that?\n"
'I have a pandas Dataframe with N columns representing the coordinates of a vector (for example X, Y, Z, but could be more than 3D).\n\nI would like to aggregate the dataframe along the rows with an arbitrary function that combines the columns, for example the norm: (X^2 + Y^2 + Y^2).\n\nI want to do something similar to what is done here and here and here but I want to keep it general enough that the number of columns can change and it behaves like\n\nDataFrame.mean(axis = 1)\n\n\nor\n\nDataFrame.sum(axis = 1)\n\n'
'Is it possible to groupby a multi-index (2 levels) pandas dataframe  by one of the multi-index levels ?  \n\nThe only way I know of doing it is to reset_index on a multiindex and then set index again. I am sure there is a better way to do it, and I want to know how. \n'
'Was trying to generate a pivot table with multiple "values" columns. I know I can use aggfunc to aggregate values the way I want to, but what if I don\'t want to sum or avg both columns but instead I want sum of one column while mean of the other one. So is it possible to do so using pandas?\n\ndf = pd.DataFrame({\n          \'A\' : [\'one\', \'one\', \'two\', \'three\'] * 6,\n          \'B\' : [\'A\', \'B\', \'C\'] * 8,\n          \'C\' : [\'foo\', \'foo\', \'foo\', \'bar\', \'bar\', \'bar\'] * 4,\n          \'D\' : np.random.randn(24),\n          \'E\' : np.random.randn(24)\n})\n\n\nNow this will get a pivot table with sum:\n\npd.pivot_table(df, values=[\'D\',\'E\'], rows=[\'B\'], aggfunc=np.sum)\n\n\nAnd this for mean:\n\npd.pivot_table(df, values=[\'D\',\'E\'], rows=[\'B\'], aggfunc=np.mean)\n\n\nHow can I get sum for D and mean for E?\n\nHope my question is clear enough.\n'
"Say I have a DataFrame df with date as index and some values. How can I select the rows where the date is larger than some value x? \n\nI know I can convert the index to a column and then do the select df[df['date']&gt;x], but is that slower than doing the operation on the index?\n"
"I have several columns named the same in a df. I need to rename them but the problem is that the df.rename method renames them all the same way. How I can rename the below blah(s) to blah1, blah4, blah5?\ndf = pd.DataFrame(np.arange(2*5).reshape(2,5))\ndf.columns = ['blah','blah2','blah3','blah','blah']\ndf\n\n#     blah  blah2  blah3  blah  blah\n# 0   0     1      2      3     4\n# 1   5     6      7      8     9\n\nHere is what happens when using the df.rename method:\ndf.rename(columns={'blah':'blah1'})\n\n#     blah1  blah2  blah3  blah1  blah1\n# 0   0      1      2      3      4\n# 1   5      6      7      8      9\n\n"
"I intend to plot multiple columns in a pandas dataframe, all grouped by another column using groupby inside seaborn.boxplot. There is a nice answer here, for a similar problem in matplotlib matplotlib: Group boxplots but given the fact that seaborn.boxplot comes with groupby option I thought it could be much easier to do this in seaborn. \n\nHere we go with a reproducible example that fails:\n\nimport seaborn as sns\nimport pandas as pd\ndf = pd.DataFrame(\n[\n[2, 4, 5, 6, 1],\n[4, 5, 6, 7, 2],\n[5, 4, 5, 5, 1],\n[10, 4, 7, 8, 2],\n[9, 3, 4, 6, 2],\n[3, 3, 4, 4, 1]\n], columns=['a1', 'a2', 'a3', 'a4', 'b'])\n\n#Plotting by seaborn\nsns.boxplot(df[['a1','a2', 'a3', 'a4']], groupby=df.b)\n\n\nWhat I get is something that completely ignores groupby option: \n\n\n\nWhereas if I do this with one column it works thanks to another SO question Seaborn groupby pandas Series :\n\nsns.boxplot(df.a1, groupby=df.b)\n\n\n\n\nSo I would like to get all my columns in one plot (all columns come in a similar scale).\n\nEDIT:\n\nThe above SO question was edited and now includes a 'not clean' answer to this problem, but it would be nice if someone has a better idea for this problem. \n"
'I have a categorical variable in a series. I want to assign integer ids to each unique value and create a new series with the ids, effectively turning a string variable into an integer variable. What is the most compact/efficient way to do this?\n'
'I am following the Pandas tutorials\n\nThe tutorials are written using python 2.7 and I am doing them in python 3.4\n\nHere is my version details.\n\nIn [11]: print(\'Python version \' + sys.version)\nPython version 3.4.1 |Anaconda 2.0.1 (64-bit)| (default, Jun 11 2014, 17:27:11)\n[MSC v.1600 64 bit (AMD64)]\n\nIn [12]: print(\'Pandas version \' + pd.__version__)\nPandas version 0.14.1\n\n\nI create the zip as per  the tutorial\n\nIn [13]: names = [\'Bob\',\'Jessica\',\'Mary\',\'John\',\'Mel\']\n\nIn [14]: births = [968, 155, 77, 578, 973]\n\nIn [15]: zip?\nType:            type\nString form:     &lt;class \'zip\'&gt;\nNamespace:       Python builtin\nInit definition: zip(self, *args, **kwargs)\nDocstring:\nzip(iter1 [,iter2 [...]]) --&gt; zip object\n\nReturn a zip object whose .__next__() method returns a tuple where\nthe i-th element comes from the i-th iterable argument.  The .__next__()\nmethod continues until the shortest iterable in the argument sequence\nis exhausted and then it raises StopIteration.\n\nIn [16]: BabyDataSet = zip(names,births)\n\n\nBut after creation the first error shows that I cannot see the contents of the zip.\n\nIn [17]: BabyDataSet\nOut[17]: &lt;zip at 0x4f28848&gt;\n\nIn [18]: print(BabyDataSet)\n&lt;zip object at 0x0000000004F28848&gt;\n\n\nThen when I go to create the dataframe I get this iterator error.\n\nIn [21]: df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-21-636a49c94b6e&gt; in &lt;module&gt;()\n----&gt; 1 df = pd.DataFrame(data = BabyDataSet, columns=[\'Names\', \'Births\'])\n\nc:\\Users\\Sayth\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self\n, data, index, columns, dtype, copy)\n    255                                          copy=copy)\n    256         elif isinstance(data, collections.Iterator):\n--&gt; 257             raise TypeError("data argument can\'t be an iterator")\n    258         else:\n    259             try:\n\nTypeError: data argument can\'t be an iterator\n\nIn [22]:\n\n\nIs  this a python 3 gotcha where I need to do it differently? Or other?\n'
"I have some data that looks like this:\n\nc stuff\nc more header\nc begin data         \n 1 1:.5\n 1 2:6.5\n 1 3:5.3\n\n\nI want to import it into a 3 column data frame, with columns e.g.\n\na , b, c\n1,  1, 0.5\netc\n\n\nI have been trying to read in the data as 2 columns split on ':', and then to split the first column on ' '. However I'm finding it irksome.\nIs there a better way to sort it out on import directly?\n\ncurrently:\n\ndata1 = pd.read_csv(file_loc, skiprows = 3, delimiter = ':', names = ['AB', 'C'])\ndata2 = pd.DataFrame(data1.AB.str.split(' ',1).tolist(), names = ['A','B'])\n\n\nHowever this is further complicated by the fact my data has a leading space...\n\nI feel like this should be a simple task, but currently I'm thinking of reading it line by line and using some find replace to sanitise the data before importing.\n"
"Suppose I have a data frame data with strings that I want converted to indicators. I use pandas.get_dummies(data) to convert this to a dataset that I can now use for building a model.\n\nNow I have a single new observation that I want to run through my model. Obviously I can't use pandas.get_dummies(new_data) because it doesn't contain all of the classes and won't make the same indicator matrices. Is there a good way to do this?\n"
"I need to create a pivot table of 2000 columns by around 30-50 million rows from a dataset of around 60 million rows.  I've tried pivoting in chunks of 100,000 rows, and that works, but when I try to recombine the DataFrames by doing a .append() followed by .groupby('someKey').sum(), all my memory is taken up and python eventually crashes.\n\nHow can I do a pivot on data this large with a limited ammount of RAM?\n\nEDIT: adding sample code\n\nThe following code includes various test outputs along the way, but the last print is what we're really interested in.  Note that if we change segMax to 3, instead of 4, the code will produce a false positive for correct output.  The main issue is that if a shipmentid entry is not in each and every chunk that sum(wawa) looks at, it doesn't show up in the output.\n\nimport pandas as pd\nimport numpy as np\nimport random\nfrom pandas.io.pytables import *\nimport os\n\npd.set_option('io.hdf.default_format','table') \n\n# create a small dataframe to simulate the real data.\ndef loadFrame():\n    frame = pd.DataFrame()\n    frame['shipmentid']=[1,2,3,1,2,3,1,2,3] #evenly distributing shipmentid values for testing purposes\n    frame['qty']= np.random.randint(1,5,9) #random quantity is ok for this test\n    frame['catid'] = np.random.randint(1,5,9) #random category is ok for this test\n    return frame\n\ndef pivotSegment(segmentNumber,passedFrame):\n    segmentSize = 3 #take 3 rows at a time\n    frame = passedFrame[(segmentNumber*segmentSize):(segmentNumber*segmentSize + segmentSize)] #slice the input DF\n\n    # ensure that all chunks are identically formatted after the pivot by appending a dummy DF with all possible category values\n    span = pd.DataFrame() \n    span['catid'] = range(1,5+1)\n    span['shipmentid']=1\n    span['qty']=0\n\n    frame = frame.append(span)\n\n    return frame.pivot_table(['qty'],index=['shipmentid'],columns='catid', \\\n                             aggfunc='sum',fill_value=0).reset_index()\n\ndef createStore():\n\n    store = pd.HDFStore('testdata.h5')\n    return store\n\nsegMin = 0\nsegMax = 4\n\nstore = createStore()\nframe = loadFrame()\n\nprint('Printing Frame')\nprint(frame)\nprint(frame.info())\n\nfor i in range(segMin,segMax):\n    segment = pivotSegment(i,frame)\n    store.append('data',frame[(i*3):(i*3 + 3)])\n    store.append('pivotedData',segment)\n\nprint('\\nPrinting Store')   \nprint(store)\nprint('\\nPrinting Store: data') \nprint(store['data'])\nprint('\\nPrinting Store: pivotedData') \nprint(store['pivotedData'])\n\nprint('**************')\nprint(store['pivotedData'].set_index('shipmentid').groupby('shipmentid',level=0).sum())\nprint('**************')\nprint('$$$')\nfor df in store.select('pivotedData',chunksize=3):\n    print(df.set_index('shipmentid').groupby('shipmentid',level=0).sum())\n\nprint('$$$')\nstore['pivotedAndSummed'] = sum((df.set_index('shipmentid').groupby('shipmentid',level=0).sum() for df in store.select('pivotedData',chunksize=3)))\nprint('\\nPrinting Store: pivotedAndSummed') \nprint(store['pivotedAndSummed'])\n\nstore.close()\nos.remove('testdata.h5')\nprint('closed')\n\n"
"Say I have an dictionary of dataframes:\n\n  {'df1': name         color    type\n          Apple        Yellow   Fruit,\n   'df2': name         color    type\n          Banana       Red      Fruit,\n   'df3': name         color    type\n          Chocolate    Brown    Sweet\n    ......}\n\n\nAnd I want to merge them all into one like this:\n\n  name         color    type\n  Apple        Red      Fruit \n  Banana       Yellow   Fruit\n  Chocolate    Brown    Sweet\n\n\nI can do it manually as follows:\n\n  merge1=pd.merge('df1','df2')\n  merge2=pd.merge('merge1','df3')\n  ...\n\n\nBut is there a way to automatically zip through the dictionary and merge?\nAny help is appreciated.\n"
'pandas.get_dummies emits a dummy variable per categorical value. Is there some automated, easy way to ask it to create only N-1 dummy variables? (just get rid of one "baseline" variable arbitrarily)? \n\nNeeded to avoid co-linearity in our dataset. \n'
'Is there away to specify to the groupby() call to use the group name in the apply() lambda function?\n\nSimilar to  if I iterate through groups I can get the group key via the following tuple decomposition:\n\nfor group_name, subdf in temp_dataframe.groupby(level=0, axis=0):\n    print group_name\n\n\n...is there a way to also get the group name in the apply function, such as:\n\ntemp_dataframe.groupby(level=0,axis=0).apply(lambda group_name, subdf: foo(group_name, subdf)\n\n\nHow can I get the group name as an argument for the apply lambda function?\n'
"I am able to add a new column in Panda by defining user function and then using apply. However, I want to do this using lambda; is there a way around?\n\nFor Example, df has two columns a and b. I want to create a new column c which is equal to the longest length between a and b.\n\nSome thing like:\n\ndf['c'] = df.apply(lambda x, len(df['a']) if len(df['a']) &gt; len(df['b']) or len(df['b']) )\n\n\nOne approach:\n\ndf = pd.DataFrame({'a':['dfg','f','fff','fgrf','fghj'], 'b' : ['sd','dfg','edr','df','fghjky']})\n\ndf['c'] = df.apply(lambda x: max([len(x) for x in [df['a'], df['b']]]))\nprint df\n      a       b   c\n0   dfg      sd NaN\n1     f     dfg NaN\n2   fff     edr NaN\n3  fgrf      df NaN\n4  fghj  fghjky NaN\n\n"
"Hi have a multiindex dataframe:\n\ntuples = [('YTA_Q3', 1), ('YTA_Q3', 2), ('YTA_Q3', 3), ('YTA_Q3', 4), ('YTA_Q3', 99), ('YTA_Q3', 96)]\n# Index\nindex = pd.MultiIndex.from_tuples(tuples, names=['Questions', 'Values'])\n# Columns\ncolumns = pd.MultiIndex.from_tuples([('YTA_Q3', '@')], names=['Questions', 'Values'])\n# Data\ndata = [29.014949,5.0260590000000001,\n  6.6269119999999999,\n  1.3565260000000001,\n  41.632221999999999,\n  21.279499999999999]\n\ndf1 = pd.DataFrame(data=data, index=index, columns=columns)\n\n\nHow do I convert the inner values of the df's index to str? \n\nMy attempt:\n\ndf1.index.astype(str) \n\n\nreturns a TypeError\n"
'I have been struggling with removing the time zone info from a column in a pandas dataframe. I have checked the following question, but it does not work for me:\n\nCan I export pandas DataFrame to Excel stripping tzinfo?\n\nI used tz_localize to assign a timezone to a datetime object, because I need to convert to another timezone using tz_convert. This adds an UTC offset, in the way "-06:00". I need to get rid of this offset, because it results in an error when I try to export the dataframe to Excel. \n\nActual output\n\n2015-12-01 00:00:00-06:00\n\n\nDesired output\n\n2015-12-01 00:00:00\n\n\nI have tried to get the characters I want using the str() method, but it seems the result of tz_localize is not a string. My solution so far is to export the dataframe to csv, read the file, and to use the str() method to get the characters I want.\n\nIs there an easier solution?\n'
"I have a dataframe:\n\ndf = pd.DataFrame([[2, 4, 7, 8, 1, 3, 2013], [9, 2, 4, 5, 5, 6, 2014]], columns=['Amy', 'Bob', 'Carl', 'Chris', 'Ben', 'Other', 'Year'])\n\n\n   Amy  Bob  Carl  Chris  Ben  Other  Year\n0    2    4     7      8    1      3  2013\n1    9    2     4      5    5      6  2014\n\n\nAnd a dictionary:\n\nd = {'A': ['Amy'], 'B': ['Bob', 'Ben'], 'C': ['Carl', 'Chris']}\n\n\nI would like to reshape my dataframe to look like this:\n\n    Group   Name  Year  Value\n 0      A    Amy  2013      2\n 1      A    Amy  2014      9\n 2      B    Bob  2013      4\n 3      B    Bob  2014      2\n 4      B    Ben  2013      1\n 5      B    Ben  2014      5\n 6      C   Carl  2013      7\n 7      C   Carl  2014      4\n 8      C  Chris  2013      8\n 9      C  Chris  2014      5\n10  Other         2013      3\n11  Other         2014      6\n\n\nNote that Other doesn't have any values in the Name column and the order of the rows does not matter. I think I should be using the melt function but the examples that I've come across aren't too clear.\n"
"I am passing a dictionary to the map function to recode values in the column of a Pandas dataframe. However, I noticed that if there is a value in the original series that is not explicitly in the dictionary, it gets recoded to NaN. Here is a simple example:\n\nTyping...\n\ns = pd.Series(['one','two','three','four'])\n\n\n...creates the series\n\n0      one\n1      two\n2    three\n3     four\ndtype: object\n\n\nBut applying the map...\n\nrecodes = {'one':'A', 'two':'B', 'three':'C'}\ns.map(recodes)\n\n\n...returns the series\n\n0      A\n1      B\n2      C\n3    NaN\ndtype: object\n\n\nI would prefer that if any element in series s is not in the recodes dictionary, it remains unchanged. That is, I would prefer to return the series below (with the original four instead of NaN).\n\n0      A\n1      B\n2      C\n3   four\ndtype: object\n\n\nIs there an easy way to do this, for example an option to pass to the map function? The challenge I am having is that I can't always anticipate all possible values that will be in the series I'm recoding - the data will be updated in the future and new values could appear. \n\nThanks!\n"
"I have a UTF-8 file with twitter data and I am trying to read it into a Python data frame but I can only get an 'object' type instead of unicode strings:\n\n# file 1459966468_324.csv\n#1459966468_324.csv: UTF-8 Unicode English text\ndf = pd.read_csv('1459966468_324.csv', dtype={'text': unicode})\ndf.dtypes\ntext               object\nAirline            object\nname               object\nretweet_count     float64\nsentiment          object\ntweet_location     object\ndtype: object\n\n\nWhat is the right way of reading and coercing UTF-8 data into unicode with Pandas?\n\nThis does not solve the problem:\n\ndf = pd.read_csv('1459966468_324.csv', encoding = 'utf8')\ndf.apply(lambda x: pd.lib.infer_dtype(x.values))\n\n\nText file is here:\nhttps://raw.githubusercontent.com/l1x/nlp/master/1459966468_324.csv\n"
"I need to iterate over each row of a pandas df and turn this into a comma separated string.\n\nexample:\n\ndf3 = DataFrame(np.random.randn(10, 5),\n              columns=['a', 'b', 'c', 'd', 'e'])\n\n\n          a         b         c         d         e\n0 -0.158897 -0.749799  0.268921  0.070035  0.099600\n1 -0.863654 -0.086814 -0.614562 -1.678850  0.980292\n2 -0.098168  0.710652 -0.456274 -0.373153 -0.533463\n3  1.001634 -0.736187 -0.812034  0.223062 -1.337972\n4  0.173549 -0.576412 -1.016063 -0.217242  0.443794\n5  0.273695  0.335562  0.778393 -0.668368  0.438880\n6 -0.783824  1.439888  1.057639 -1.825481 -0.770953\n7 -1.025004  0.155974  0.645023  0.993379 -0.812133\n8  0.953448 -1.355628 -1.918317 -0.966472 -0.618744\n9 -0.479297  0.295150 -0.294449  0.679416 -1.813078\n\n\nI'd like to get for each row:\n\n '-0.158897,-0.749799,0.268921,0.070035,0.099600'\n '0.863654,-0.086814,-0.614562,-1.678850,0.980292'\n... and so on\n\n"
"After experimenting with timing various types of lookups on a Pandas (0.17.1) DataFrame I am left with a few questions. \n\nHere is the set up...\n\nimport pandas as pd\nimport numpy as np\nimport itertools\n\nletters = [chr(x) for x in range(ord('a'), ord('z'))]\nletter_combinations = [''.join(x) for x in itertools.combinations(letters, 3)]\n\ndf1 = pd.DataFrame({\n        'value': np.random.normal(size=(1000000)), \n        'letter': np.random.choice(letter_combinations, 1000000)\n    })\ndf2 = df1.sort_values('letter')\ndf3 = df1.set_index('letter')\ndf4 = df3.sort_index()\n\n\nSo df1 looks something like this...\n\nprint(df1.head(5))\n\n\n&gt;&gt;&gt;\n  letter     value\n0    bdh  0.253778\n1    cem -1.915726\n2    mru -0.434007\n3    lnw -1.286693\n4    fjv  0.245523\n\n\nHere is the code to test differences in lookup performance...\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df1[df1.letter == 'ben']\n%timeit df1[df1.letter == 'amy']\n%timeit df1[df1.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df2[df2.letter == 'ben']\n%timeit df2[df2.letter == 'amy']\n%timeit df2[df2.letter == 'abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df3.loc['ben']\n%timeit df3.loc['amy']\n%timeit df3.loc['abe']\n\nprint('~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n%timeit df4.loc['ben']\n%timeit df4.loc['amy']\n%timeit df4.loc['abe']\n\n\nAnd the results...\n\n~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / UNSORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n10 loops, best of 3: 59.7 ms per loop\n~~~~~~~~~~~~~~~~~NON-INDEXED LOOKUPS / SORTED DATASET~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 192 ms per loop\n10 loops, best of 3: 193 ms per loop\n~~~~~~~~~~~~~~~~~~~~~INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 4.66 times longer than the fastest. This could mean that an intermediate result is being cached \n10 loops, best of 3: 40.9 ms per loop\n10 loops, best of 3: 41 ms per loop\n10 loops, best of 3: 40.9 ms per loop\n~~~~~~~~~~~~~~~~~~~~~SORTED INDEXED LOOKUPS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe slowest run took 1621.00 times longer than the fastest. This could mean that an intermediate result is being cached \n1 loops, best of 3: 259 µs per loop\n1000 loops, best of 3: 242 µs per loop\n1000 loops, best of 3: 243 µs per loop\n\n\nQuestions...\n\n\nIt's pretty clear why the lookup on the sorted index is so much faster, binary search to get O(log(n)) performance vs O(n) for a full array scan. But, why is the lookup on the sorted non-indexed df2 column SLOWER than the lookup on the unsorted non-indexed column df1?\nWhat is up with the The slowest run took x times longer than the fastest. This could mean that an intermediate result is being cached. Surely, the results aren't being cached. Is it because the created index is lazy and isn't actually reindexed until needed? That would explain why it is only on the first call to .loc[].\nWhy isn't an index sorted by default? The fixed cost of the sort can be too much?\n\n"
"I know how to create a mask to filter a dataframe when querying a single column:\n\nimport pandas as pd\nimport datetime\nindex = pd.date_range('2013-1-1',periods=100,freq='30Min')\ndata = pd.DataFrame(data=list(range(100)), columns=['value'], index=index)\ndata['value2'] = 'A'\ndata['value2'].loc[0:10] = 'B'\n\ndata\n\n    value   value2\n2013-01-01 00:00:00 0   B\n2013-01-01 00:30:00 1   B\n2013-01-01 01:00:00 2   B\n2013-01-01 01:30:00 3   B\n2013-01-01 02:00:00 4   B\n2013-01-01 02:30:00 5   B\n2013-01-01 03:00:00 6   B\n\n\nI use a simple mask here:\n\nmask = data['value'] &gt; 4\ndata[mask]\n    value   value2\n2013-01-01 02:30:00 5   B\n2013-01-01 03:00:00 6   B\n2013-01-01 03:30:00 7   B\n2013-01-01 04:00:00 8   B\n2013-01-01 04:30:00 9   B\n2013-01-01 05:00:00 10  A\n\n\nMy question is how to create a mask with multiple columns?  So if I do this:\n\ndata[data['value2'] == 'A' ][data['value'] &gt; 4]\n\n\nThis filters as I would expect but how do I create a bool mask from this as per my other example?  I have provided the test data for this but I often want to create a mask on other types of data so Im looking for any pointers please. \n"
"I have a dataframe and would like to truncate each field to up to 20 characters. I've been naively trying the following:\n\ndf = df.astype(str).apply(lambda x: x[:20])\n\n\nhowever it has no effect whatsoever. If, however, I wanted to add an 'Y' to each field, this works like a charm:\n\ndf = df.astype(str).apply(lambda x: x+'Y')\n\n\nWhat am I doing wrong?\n"
'a pyspark.sql.DataFrame displays messy with DataFrame.show() - lines wrap instead of a scroll.\n\n\n\nbut displays with pandas.DataFrame.head \n\n\nI tried these options \n\nimport IPython\nIPython.auto_scroll_threshold = 9999\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = "all"\nfrom IPython.display import display\n\n\nbut no luck. Although the scroll works when used within Atom editor with jupyter plugin:\n\n\n'
'I\'ve used multiple ways of splitting and stripping the strings in my pandas dataframe to remove all the \'\\n\'characters, but for some reason it simply doesn\'t want to delete the characters that are attached to other words, even though I split them. I have a pandas dataframe with a column that captures text from web pages using Beautifulsoup. The text has been cleaned a bit already by beautifulsoup, but it failed in removing the newlines attached to other characters.  My strings look a bit like this:\n\n"hands-on\\ndevelopment of games. We will study a variety of software technologies\\nrelevant to games including programming languages, scripting\\nlanguages, operating systems, file systems, networks, simulation\\nengines, and multi-media design systems. We will also study some of\\nthe underlying scientific concepts from computer science and related\\nfields including"\n\nIs there an easy python way to remove these "\\n" characters? \n\nThanks in advance!\n'
"This is my dataframe df\n\na     b     c\n1.2   2    0.1\n2.1   1.1  3.2\n0.2   1.9  8.8\n3.3   7.8  0.12\n\n\nI'm trying to get max value from each row of a dataframe, I m expecting output like this\n\nmax_value\n   2\n  3.2\n  8.8\n  7.8 \n\n\nThis is what I have tried\n\ndf[len(df.columns)].argmax()\n\n\nI'm not getting proper output, any help would be much appreciated. Thanks\n"
"I'd like to take a dataset with a bunch of different unique individuals, each with multiple entries, and assign each individual a unique id for all of their entries. Here's an example of the df:\n\n      FirstName LastName  id\n0     Tom       Jones     1\n1     Tom       Jones     1\n2     David     Smith     1\n3     Alex      Thompson  1\n4     Alex      Thompson  1\n\n\nSo, basically I want all entries for Tom Jones to have id=1, all entries for David Smith to have id=2, all entries for Alex Thompson to have id=3, and so on. \n\nSo I already have one solution, which is a dead simple python loop iterating two values (One for id, one for index) and assigning the individual an id based on whether they match the previous individual:\n\nx = 1\ni = 1\n\nwhile i &lt; len(df_test):\n    if (df_test.LastName[i] == df_test.LastName[i-1]) &amp; \n    (df_test.FirstName[i] == df_test.FirstName[i-1]):\n        df_test.loc[i, 'id'] = x\n        i = i+1\n    else:\n        x = x+1\n        df_test.loc[i, 'id'] = x\n        i = i+1\n\n\nThe problem I'm running into is that the dataframe has about 9 million entries, so with that loop it would have taken a huge amount of time to run. Can anyone think of a more efficient way to do this? I've been looking at groupby and multiindexing as potential solutions, but haven't quite found the right solution yet. Thanks! \n"
"I have a dataframe which has aggregated data for some days. I want to add in the missing days \n\nI was following another post, Add missing dates to pandas dataframe, unfortunately, it overwrote my results (maybe functionality was changed slightly?)... the code is below\n\nimport random\nimport datetime as dt\nimport numpy as np\nimport pandas as pd\n\ndef generate_row(year, month, day):\n    while True:\n        date = dt.datetime(year=year, month=month, day=day)\n        data = np.random.random(size=4)\n        yield [date] + list(data)\n\n# days I have data for\ndates = [(2000, 1, 1), (2000, 1, 2), (2000, 2, 4)]\ngenerators = [generate_row(*date) for date in dates]\n\n# get 5 data points for each\ndata = [next(generator) for generator in generators for _ in range(5)]\n\ndf = pd.DataFrame(data, columns=['date'] + ['f'+str(i) for i in range(1,5)])\n\n# df\ngroupby_day = df.groupby(pd.PeriodIndex(data=df.date, freq='D'))\nresults = groupby_day.sum()\n\nidx = pd.date_range(min(df.date), max(df.date))\nresults.reindex(idx, fill_value=0)\n\n\nResults before filling in missing date indices\n\n\nResults after\n\n"
"I have a dataframe say like this\n\n&gt;&gt;&gt; df = pd.DataFrame({'user_id':['a','a','s','s','s'],\n                    'session':[4,5,4,5,5],\n                    'revenue':[-1,0,1,2,1]})\n\n&gt;&gt;&gt; df\n   revenue  session user_id\n0       -1        4       a\n1        0        5       a\n2        1        4       s\n3        2        5       s\n4        1        5       s\n\n\nAnd each value of session and revenue represents a kind of type, and I want to count the number of each kind say the number of revenue=-1 and session=4 of user_id=a is 1.\n\nAnd I found simple call count() function after groupby() can't output the result I want.\n\n&gt;&gt;&gt; df.groupby('user_id').count()\n         revenue  session\nuser_id\na              2        2\ns              3        3\n\n\nHow can I do that?\n"
'I have a DataFrame df with a non-numerical column CatColumn.\n\n   A         B         CatColumn\n0  381.1396  7.343921  Medium\n1  481.3268  6.786945  Medium\n2  263.3766  7.628746  High\n3  177.2400  5.225647  Medium-High\n\n\nI want to include CatColumn in the correlation analysis with other columns in the Dataframe. I tried DataFrame.corr but it does not include columns with nominal values in the correlation analysis.\n'
"I have a dataframe\n\ndf = pd.DataFrame(data=np.arange(10),columns=['v']).astype(float)\n\n\nHow to make sure that the numbers in v are whole numbers?\nI am very concerned about rounding/truncation/floating point representation errors\n"
'I\'ve just started coding in python, and my general coding skills are fairly rusty :(     so please be a bit patient\n\nI have a pandas dataframe:\n\n\n\nIt has around 3m rows.  There are 3 kinds of age_units: Y, D, W  for years, Days &amp; Weeks.  Any individual over 1 year old has an age unit of Y and my first grouping I want is &lt;2y old so all I have to test for in Age Units is Y...\n\nI want to create a new column AgeRange and populate with the following ranges:\n\n\n&lt;2 \n2 - 18 \n18 - 35 \n35 - 65 \n65+\n\n\nso I wrote a function\n\ndef agerange(values):\n    for i in values:\n        if complete.Age_units == \'Y\':\n            if complete.Age &gt; 1 AND &lt; 18 return \'2-18\'\n            elif complete.Age &gt; 17 AND &lt; 35 return \'18-35\'\n            elif complete.Age &gt; 34 AND &lt; 65 return \'35-65\'\n            elif complete.Age &gt; 64 return \'65+\'\n        else return \'&lt; 2\'\n\n\nI thought if I passed in the dataframe as a whole I would get back what I needed  and then could create the column I wanted something like this:\n\nagedetails[\'age_range\'] = ageRange(agedetails)\n\n\nBUT when I try to run the first code to create the function I get:\n\n  File "&lt;ipython-input-124-cf39c7ce66d9&gt;", line 4\n    if complete.Age &gt; 1 AND complete.Age &lt; 18 return \'2-18\'\n                          ^\nSyntaxError: invalid syntax\n\n\nClearly it is not accepting the AND - but I thought I heard in class I could use AND like this?  I must be mistaken but then what would be the right way to do this?\n\nSo after getting that error, I\'m not even sure the method of passing in a dataframe will throw an error either.  I am guessing probably yes.  In which case - how would I make that work as well?\n\nI am looking to learn the best method, but part of the best method for me is keeping it simple even if that means doing things in a couple of steps...\n'
'def stack_plot(data, xtick, col2=\'project_is_approved\', col3=\'total\'):\n    ind = np.arange(data.shape[0])\n\n    plt.figure(figsize=(20,5))\n    p1 = plt.bar(ind, data[col3].values)\n    p2 = plt.bar(ind, data[col2].values)\n\n    plt.ylabel(\'Projects\')\n    plt.title(\'Number of projects aproved vs rejected\')\n    plt.xticks(ind, list(data[xtick].values))\n    plt.legend((p1[0], p2[0]), (\'total\', \'accepted\'))\n    plt.show()\n\ndef univariate_barplots(data, col1, col2=\'project_is_approved\', top=False):\n    # Count number of zeros in dataframe python: https://stackoverflow.com/a/51540521/4084039\n    temp = pd.DataFrame(project_data.groupby(col1)[col2].agg(lambda x: x.eq(1).sum())).reset_index()\n\n    # Pandas dataframe grouby count: https://stackoverflow.com/a/19385591/4084039\n    temp[\'total\'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({\'total\':\'count\'})).reset_index()[\'total\']\n\n    temp[\'Avg\'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({\'Avg\':\'mean\'})).reset_index()[\'Avg\']\n\n    temp.sort_values(by=[\'total\'],inplace=True, ascending=False)\n\n    if top:\n        temp = temp[0:top]\n\n    stack_plot(temp, xtick=col1, col2=col2, col3=\'total\')\n    print(temp.head(5))\n    print("="*50)\n    print(temp.tail(5))\n\nunivariate_barplots(project_data, \'school_state\', \'project_is_approved\', False)\n\n\nError:\n\nSpecificationError                        Traceback (most recent call last)\n&lt;ipython-input-21-2cace8f16608&gt; in &lt;module&gt;()\n----&gt; 1 univariate_barplots(project_data, \'school_state\', \'project_is_approved\', False)\n\n&lt;ipython-input-20-856fcc83737b&gt; in univariate_barplots(data, col1, col2, top)\n      4 \n      5     # Pandas dataframe grouby count: https://stackoverflow.com/a/19385591/4084039\n----&gt; 6     temp[\'total\'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({\'total\':\'count\'})).reset_index()[\'total\']\n      7     print (temp[\'total\'].head(2))\n      8     temp[\'Avg\'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({\'Avg\':\'mean\'})).reset_index()[\'Avg\']\n\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\groupby\\generic.py in aggregate(self, func, *args, **kwargs)\n    251             # but not the class list / tuple itself.\n    252             func = _maybe_mangle_lambdas(func)\n--&gt; 253             ret = self._aggregate_multiple_funcs(func)\n    254             if relabeling:\n    255                 ret.columns = columns\n\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\pandas\\core\\groupby\\generic.py in _aggregate_multiple_funcs(self, arg)\n    292             # GH 15931\n    293             if isinstance(self._selected_obj, Series):\n--&gt; 294                 raise SpecificationError("nested renamer is not supported")\n    295 \n    296             columns = list(arg.keys())\n\nSpecificationError: **nested renamer is not supported**\n\n'
'In [35]: test = pd.DataFrame({\'a\':range(4),\'b\':range(4,8)})\n\nIn [36]: test\nOut[36]: \n   a  b\n0  0  4\n1  1  5\n2  2  6\n3  3  7\n\nIn [37]: for i in test[\'a\']:\n   ....:  print i\n   ....: \n0\n1\n2\n3\n\nIn [38]: for i,j in test:\n   ....:  print i,j\n   ....: \n------------------------------------------------------------\nTraceback (most recent call last):\n  File "&lt;ipython console&gt;", line 1, in &lt;module&gt;\nValueError: need more than 1 value to unpack\n\n\nIn [39]: for i,j in test[[\'a\',\'b\']]:\n   ....:  print i,j\n   ....: \n------------------------------------------------------------\nTraceback (most recent call last):\n  File "&lt;ipython console&gt;", line 1, in &lt;module&gt;\nValueError: need more than 1 value to unpack\n\n\nIn [40]: for i,j in [test[\'a\'],test[\'b\']]:\n   ....:  print i,j\n   ....: \n------------------------------------------------------------\nTraceback (most recent call last):\n  File "&lt;ipython console&gt;", line 1, in &lt;module&gt;\nValueError: too many values to unpack\n\n'
'What\'s the best way to handle zero denominators when dividing pandas DataFrame columns by each other in Python? for example:\n\ndf = pandas.DataFrame({"a": [1, 2, 0, 1, 5], "b": [0, 10, 20, 30, 50]})\ndf.a / df.b  # yields error\n\n\nI\'d like the ratios where the denominator is zero to be registered as NA (numpy.nan). How can this be done efficiently in pandas? \n\nCasting to float64 does not work at level of columns:\n\nIn [29]: df\nOut[29]: \n   a   b\n0  1   0\n1  2  10\n2  0  20\n3  1  30\n4  5  50\n\nIn [30]: df["a"].astype("float64") / df["b"].astype("float64")\n...\n\nFloatingPointError: divide by zero encountered in divide\n\n\nHow can I do it just for particular columns and not entire df?\n'
"I am dealing with pandas DataFrames like this:\n\n   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1  NaN\n5   2  NaN\n6   1  300\n7   1  NaN\n\n\nI would like to replace each NAN 'x' with the previous non-NAN 'x' from a row with the same 'id' value:\n\n   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1   20\n5   2  200\n6   1  300\n7   1  300\n\n\nIs there some slick way to do this without manually looping over rows?\n"
'I have some binary data and I was wondering how I can load that into pandas.\n\nCan I somehow load it specifying the format it is in, and what the individual columns are called?\n\nEdit:\nFormat is \n\nint, int, int, float, int, int[256]\n\n\neach comma separation represents a column in the data, i.e. the last 256 integers is one column.\n'
"Is there an option not drop the the indices with 'nan' in them? I think silently dropping these rows from the pivot will at some point cause someone serious pain.\n\nimport pandas\nimport numpy\n\na = [['a', 'b', 12, 12, 12], ['a', numpy.nan, 12.3, 233., 12], ['b', 'a', 123.23, 123, 1], ['a', 'b', 1, 1, 1.]]\n\ndf = pandas.DataFrame(a, columns=['a', 'b', 'c', 'd', 'e'])\n\ndf_pivot = df.pivot_table(rows=['a', 'b'], values=['c', 'd', 'e'], aggfunc=sum)\nprint(df)\nprint(df_pivot)\n\n\nOutput:\n\n   a    b       c    d   e\n0  a    b   12.00   12  12\n1  a  NaN   12.30  233  12\n2  b    a  123.23  123   1\n3  a    b    1.00    1   1\n          c    d   e\na b                 \na b   13.00   13  13\nb a  123.23  123   1\n\n"
"Simple example:\n\n&gt;&gt;&gt; from collections import namedtuple\n&gt;&gt;&gt; import pandas\n\n&gt;&gt;&gt; Price = namedtuple('Price', 'ticker date price')\n&gt;&gt;&gt; a = Price('GE', '2010-01-01', 30.00)\n&gt;&gt;&gt; b = Price('GE', '2010-01-02', 31.00)\n&gt;&gt;&gt; l = [a, b]\n&gt;&gt;&gt; df = pandas.DataFrame.from_records(l, index='ticker')\nTraceback (most recent call last)\n...\nKeyError: 'ticker'\n\n\nHarder example:\n\n&gt;&gt;&gt; df2 = pandas.DataFrame.from_records(l, index=['ticker', 'date'])\n&gt;&gt;&gt; df2\n\n         0           1   2\nticker  GE  2010-01-01  30\ndate    GE  2010-01-02  31\n\n\nNow it thinks that ['ticker', 'date'] is the index itself, rather than the columns I want to use as the index.\n\nIs there a way to do this without resorting to an intermediate numpy ndarray or using set_index after the fact?\n"
"There is a method to plot Series histograms, but is there a function to retrieve the histogram counts to do further calculations on top of it? \n\nI keep using numpy's functions to do this and converting the result to a DataFrame or Series when I need this. It would be nice to stay with pandas objects the whole time. \n"
'I\'m creating tables using the pandas to_html function, and I\'d like to be able to highlight the bottom row of the outputted table, which is of variable length. I don\'t have any real experience of html to speak of, and all I found online was this \n    \n    \n    \n\n&lt;table border="1"&gt;\n  &lt;tr style="background-color:#FF0000"&gt;\n    &lt;th&gt;Month&lt;/th&gt;\n    &lt;th&gt;Savings&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;January&lt;/td&gt;\n    &lt;td&gt;$100&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n\n\nSo I know that the final row must have &lt;tr style=""background-color:#FF0000"&gt; (or whatever colour I want) rather than just &lt;tr&gt;, but what I don\'t really know how to do is get this to occur with the tables I\'m making. I don\'t think I can do it with the to_html function itself, but how can I do it after the table has been created?\n\nAny help is appreciated. \n\n\n\n'
'is there a simple way to take a pandas/df table:\n\nfield_1 field_2 field_3 field_4\ncat     15,263  2.52    00:03:00\ndog     1,652   3.71    00:03:47\ntest     312    3.27    00:03:41\nbook     300    3.46    00:02:40\n\n\nAnd convert it to XML along the lines of:\n\n&lt;item&gt;\n  &lt;field name="field_1"&gt;cat&lt;/field&gt;\n  &lt;field name="field_2"&gt;15263&lt;/field&gt;\n  &lt;field name="filed_3"&gt;2.52&lt;/field&gt;\n\n...\n\n&lt;item&gt;\n      &lt;field name="field_1"&gt;dog&lt;/field&gt;\n\nand so on...\n\n\nThanks in advance for any help.\n'
"I'd like to create some NetworkX graphs from a simple Pandas DataFrame:\n\n        Loc 1   Loc 2   Loc 3   Loc 4   Loc 5   Loc 6   Loc 7\nFoo     0       0       1       1       0       0           0\nBar     0       0       1       1       0       1           1\nBaz     0       0       1       0       0       0           0\nBat     0       0       1       0       0       1           0\nQuux    1       0       0       0       0       0           0\n\n\nWhere Foo… is the index, and Loc 1 to Loc 7 are the columns. But converting to Numpy matrices or recarrays doesn't seem to work for generating input for nx.Graph(). Is there a standard strategy for achieving this? I'm not averse the reformatting the data in Pandas --> dumping to CSV --> importing to NetworkX, but it seems as if I should be able to generate the edges from the index and the nodes from the values.\n"
'Is there a idiomatic way to plot the histogram of a feature for two classes?\nIn pandas, I basically want\n\ndf.feature[df.class == 0].hist()\ndf.feature[df.class == 1].hist()\n\n\nTo be in the same plot. I could do\n\ndf.feature.hist(by=df.class)\n\n\nbut that gives me two separate plots.\n\nThis seems to be a common task so I would imagine there to be an idiomatic way to do this. Of course I could manipulate the histograms manually to fit next to each other but usually pandas does that quite nicely.\n\nBasically I want this matplotlib example in one line of pandas: http://matplotlib.org/examples/pylab_examples/barchart_demo.html\n\nI thought I was missing something, but maybe it is not possible (yet).\n'
"I'd like to drop all values from a table if the rows = nan or 0.\n\nI know there's a way to do this using pandas i.e pandas.dropna(how = 'all') but I'd like a numpy method to remove rows with all nan or 0.\n\nIs there an efficient implementation of this?\n"
"Is there a way to apply a list of functions to each column in a DataFrame like the DataFrameGroupBy.agg function does?  I found an ugly way to do it like this:\n\ndf=pd.DataFrame(dict(one=np.random.uniform(0,10,100), two=np.random.uniform(0,10,100)))\ndf.groupby(np.ones(len(df))).agg(['mean','std'])\n\n        one                 two\n       mean       std      mean       std\n1  4.802849  2.729528  5.487576  2.890371\n\n"
"I have a timeseries dataframe  df looks like this (the time seris happen within same day, but across different hours:\n\n                                id               val \n time                    \n2014-04-03 16:01:53             23              14389      \n2014-04-03 16:01:54             28              14391             \n2014-04-03 16:05:55             24              14393             \n2014-04-03 16:06:25             23              14395             \n2014-04-03 16:07:01             23              14395             \n2014-04-03 16:10:09             23              14395             \n2014-04-03 16:10:23             26              14397             \n2014-04-03 16:10:57             26              14397             \n2014-04-03 16:11:10             26              14397              \n\n\nI need to create group every 5 minutes from starting from 16:00:00. That is all the rows with in the range 16:00:00 to 16:05:00, its value of the new column period is 1. (the number of rows within each group is irregular, so i can't simply cut the group)\n\nEventually, the data should look like this:\n\n                                id               val           period \ntime            \n2014-04-03 16:01:53             23              14389             1\n2014-04-03 16:01:54             28              14391             1\n2014-04-03 16:05:55             24              14393             2\n2014-04-03 16:06:25             23              14395             2\n2014-04-03 16:07:01             23              14395             2\n2014-04-03 16:10:09             23              14395             3\n2014-04-03 16:10:23             26              14397             3\n2014-04-03 16:10:57             26              14397             3\n2014-04-03 16:11:10             26              14397             3\n\n\nThe purpose is to perform some groupby operation, but the operation I need to do is not included in pd.resample(how=' ') method. So I have to create a period column to identify each group, then do df.groupby('period').apply(myfunc).\n\nAny help or comments are highly appreciated.\n\nThanks!\n"
'I have the following data in a pandas dataframe\n\n       date  template     score\n0  20140605         0  0.138786\n1  20140605         1  0.846441\n2  20140605         2  0.766636\n3  20140605         3  0.259632\n4  20140605         4  0.497366\n5  20140606         0  0.138139\n6  20140606         1  0.845320\n7  20140606         2  0.762876\n8  20140606         3  0.261035\n9  20140606         4  0.498010\n\n\nFor every day there will be 5 templates and each template will have a score.\n\nI want to plot the date in the x axis and score in the y axis and a separate line graph for each template in the same figure.\n\nIs it possible to do this using matplotlib?\n'
'I have a time-series A holding several values. I need to obtain a series B that is defined algebraically as follows:\n\nB[t] = a * A[t] + b * B[t-1]\n\n\nwhere we can assume B[0] = 0, and a and b are real numbers.\n\nIs there any way to do this type of recursive computation in Pandas? Or do I have no choice but to loop in Python as suggested in this answer?\n\nAs an example of input:\n\n&gt; A = pd.Series(np.random.randn(10,))\n\n0   -0.310354\n1   -0.739515\n2   -0.065390\n3    0.214966\n4   -0.605490\n5    1.293448\n6   -3.068725\n7   -0.208818\n8    0.930881\n9    1.669210\n\n'
'I have an input file where every value is stored as a string.\nIt is inside a csv file with each entry inside double quotes.\n\nExample file:\n\n"column1","column2", "column3", "column4", "column5", "column6"\n"AM", "07", "1", "SD", "SD", "CR"\n"AM", "08", "1,2,3", "PR,SD,SD", "PR,SD,SD", "PR,SD,SD"\n"AM", "01", "2", "SD", "SD", "SD"\n\n\nThere are only six columns. What options do I need to enter to pandas read_csv to read this correctly?\n\nI currently am trying:\n\nimport pandas as pd\ndf = pd.read_csv(file, quotechar=\'"\')\n\n\nbut this gives me the error message:\nCParserError: Error tokenizing data. C error: Expected 6 fields in line 3, saw 14\n\nWhich obviously means that it is ignoring the \'"\' and parsing every comma as a field.\nHowever, for line 3, columns 3 through 6 should be strings with commas in them. ("1,2,3", "PR,SD,SD", "PR,SD,SD", "PR,SD,SD")\n\nHow do I get pandas.read_csv to parse this correctly?\n\nThanks.\n'
"I've got a dataset which needs to omit a few rows whilst preserving the order of the rows. My idea was to use a mask with a random number between 0 and the length of my dataset but I'm not sure how to setup a mask without shuffling the rows around i.e. a method similar to sampling a dataset. \n\nExample: Dataset has 5 rows and 2 columns and I would like to remove a row at random.\n\nCol1 | Col2\n  A  |  1\n  B  |  2 \n  C  |  5     \n  D  |  4\n  E  |  0\n\n\ntransforms to:\n\nCol1 | Col2\n  A  |  1\n  B  |  2   \n  D  |  4\n  E  |  0\n\n\nwith the third row (Col1='C') omitted by a random choice.\n\nHow should I go about this? \n"
"I used the read_csv command following below: \n\n    In [20]:\n    dataframe = pd.read_csv('D:/UserInterest/output/ENFP_0719/Bookmark.csv', index_col=None)\n    dataframe.head()\n    Out[20]:\n    Unnamed: 0  timestamp   url visits\n    0   0   1.404028e+09    http://m.blog.naver.com/PostView.nhn?blogId=mi...   2\n    1   1   1.404028e+09    http://m.facebook.com/l.php?u=http%3A%2F%2Fblo...   1\n    2   2   1.404028e+09    market://details?id=com.kakao.story 1\n    3   3   1.404028e+09    https://story-api.kakao.com/upgrade/install 4\n    4   4   1.403889e+09    http://m.cafe.daum.net/WorldcupLove/Knj/173424...   1\n\n\nThe result shows column Unnamed:0 and it is simillar when I used index_col=False, but when I used index_col=0, the result is following below:\n\ndataframe = pd.read_csv('D:/UserInterest/output/ENFP_0719/Bookmark.csv', index_col=0)\ndataframe.head()\nOut[21]:\ntimestamp   url visits\n0   1.404028e+09    http://m.blog.naver.com/PostView.nhn?blogId=mi...   2\n1   1.404028e+09    http://m.facebook.com/l.php?u=http%3A%2F%2Fblo...   1\n2   1.404028e+09    market://details?id=com.kakao.story 1\n3   1.404028e+09    https://story-api.kakao.com/upgrade/install 4\n4   1.403889e+09    http://m.cafe.daum.net/WorldcupLove/Knj/173424...   1\n\n\nThe result did show the column Unnamed:0, In here I want to ask, what is the difference between index_col=None, index_col=0, and index_col=False, I have read the documentation in this, but I still did not get the idea.\n"
'I have a ndarray like this one:\n\nIn [75]:\nz_r\nOut[75]:\narray([[ 0.00909254],\n       [ 0.02390291],\n       [ 0.02998752]])\n\n\nIn here, I want to ask how to convert those things to series, the desired output is like this:\n\n0   0.00909254\n1   0.02390291\n2   0.02998752\n\n'
"What is the recommended way (if any) for doing linear regression using a pandas dataframe? I can do it, but my method seems very elaborate. Am I making things unnecessarily complicated?\n\nThe R code, for comparison:\n\nx &lt;- c(1,2,3,4,5)\ny &lt;- c(2,1,3,5,4)\nM &lt;- lm(y~x)\nsummary(M)$coefficients\n            Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n\n\nNow, my python (2.7.10), rpy2 (2.6.0), and pandas (0.16.1)\n version:\n\nimport pandas\nimport pandas.rpy.common as common\nfrom rpy2 import robjects\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\nstats = importr('stats')\n\ndataframe = pandas.DataFrame({'x': [1,2,3,4,5], \n                              'y': [2,1,3,5,4]})\n\nrobjects.globalenv['dataframe']\\\n   = common.convert_to_r_dataframe(dataframe) \n\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\nprint(base.summary(M).rx2('coefficients'))\n\n            Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n\n\nBy the way, I do get a FutureWarning on the import of pandas.rpy.common. However, when I tried the pandas2ri.py2ri(dataframe) to convert a dataframe from pandas to R (as mentioned here), I get \n\nNotImplementedError: Conversion 'py2ri' not defined for objects of type '&lt;class 'pandas.core.series.Series'&gt;'\n\n"
'In the doc of Series, the use parameter of name and fastpath is not explained. What do they do?\n'
'This code generates error:\n\nIndexError: invalid index to scalar variable.\n\n\nat the line: results.append(RMSPE(np.expm1(y_train[testcv]), [y[1] for y in y_test]))\n\nHow to fix it?\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import ensemble\nfrom sklearn import cross_validation\n\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1./(y[ind]**2)\n    return w\n\ndef RMSPE(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe\n\nforest = ensemble.RandomForestRegressor(n_estimators=10, min_samples_split=2, n_jobs=-1)\n\nprint ("Cross validations")\ncv = cross_validation.KFold(len(train), n_folds=5)\n\nresults = []\nfor traincv, testcv in cv:\n    y_test = np.expm1(forest.fit(X_train[traincv], y_train[traincv]).predict(X_train[testcv]))\n    results.append(RMSPE(np.expm1(y_train[testcv]), [y[1] for y in y_test]))\n\n\ntestcv is:\n\n[False False False ...,  True  True  True]\n\n'
"I would like create new column for given dataframe where I calculate minimum between the column value and some global value (in this example 7). so my df has the columns session and note and my desired output column is minValue :\n\nsession     note     minValue\n1       0.726841     0.726841\n2       3.163402     3.163402  \n3       2.844161     2.844161\n4       NaN          NaN\n\n\nI'm using the built in Python method min :\n\ndf['minValue']=min(7, df['note'])\n\n\nand I have this error:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n"
"I have a dataframe in python. One of its columns is labelled time, which is a timestamp. Using the following code, I have converted the timestamp to datetime:\n\nmilestone['datetime'] = milestone.apply(lambda x: datetime.datetime.fromtimestamp(x['time']), axis = 1)\n\n\nNow I want to separate (tokenize) date and time and have two different columns like milestone['only_date'] and milestone['only_time']. How do I do this?\n"
"I have a big dataframe, and I'm grouping by one to n columns, and want to apply a function on these groups across two columns (e.g. foo and bar).\n\nHere's an example dataframe:\n\nfoo_function = lambda x: np.sum(x.a+x.b)\n\ndf = pd.DataFrame({'a':[1,2,3,4,5,6],\n                   'b':[1,2,3,4,5,6],\n                   'c':['q', 'q', 'q', 'q', 'w', 'w'],  \n                   'd':['z','z','z','o','o','o']})\n\n# works with apply, but I want transform:\ndf.groupby(['c', 'd'])[['a','b']].apply(foo_function)\n# transform doesn't work!\ndf.groupby(['c', 'd'])[['a','b']].transform(foo_function)\nTypeError: cannot concatenate a non-NDFrame object\n\n\nBut transform apparently isn't able to combine multiple columns together because it looks at each column separately (unlike apply). What is the next best alternative in terms of speed /  elegance? e.g. I could use apply and then create df['new_col'] by using pd.match, but that would necessitate matching over sometimes multiple groupby columns (col1 and col2) which seems really hacky / would take a fair amount of code.\n\n--> Is there a function that is like groupby().transform that can use functions that work over multiple columns? If this doesn't exist, what's the best hack?\n"
"pandas function read_csv() reads a .csv file. Its documentation is here \n\nAccording to documentation, we know:\n\n\n  dtype : Type name or dict of column -> type, default None Data type\n  for data or columns. E.g. {‘a’: np.float64, ‘b’: np.int32}\n  (Unsupported with engine=’python’)\n\n\nand \n\n\n  converters : dict, default None Dict of functions for converting\n  values in certain columns. Keys can either be integers or column\n  labels\n\n\nWhen using this function, I can call either\npandas.read_csv('file',dtype=object) or pandas.read_csv('file',converters=object). Obviously, converter, its name can says that data type will be converted but I wonder the case of dtype?\n"
'I have a pandas dataframe that is dynamically created with columns names that vary.  I\'m trying to push them to sql, but don\'t want them to go to mssqlserver as the default datatype "text" (can anyone explain why this is the default?  Wouldn\'t it make sense to use a more common datatype?) \n\nDoes anyone know how I can specify a datatype for all columns? \n\ncolumn_errors.to_sql(\'load_errors\',push_conn, if_exists = \'append\', index = False, dtype = #Data type for all columns#)\n\n\nthe dtype argument takes a dict, and since I don\'t know what the columns will be it is hard to set them all to be \'sqlalchemy.types.NVARCHAR\'\n\nThis is what I would like to do:\n\ncolumn_errors.to_sql(\'load_errors\',push_conn, if_exists = \'append\', index = False, dtype = \'sqlalchemy.types.NVARCHAR\')\n\n\nAny help/understanding of how best to specify all column types would be much appreciated!\n'
"I have been using pandas in python and I usually write a dataframe to my db table as below. I am now now migrating to Django, how can I write the same dataframe to a table through a model called MyModel? Assistance really appreciated.\n\n# Original pandas code\n    engine = create_engine('postgresql://myuser:mypassword@localhost:5432/mydb', echo=False)\n    mydataframe.to_sql('mytable', engine,if_exists='append',index=True)\n\n"
"I have pandas df with say, 100 rows, 10 columns, (actual data is huge). I also have row_index list which contains, which rows to be considered to take mean. I want to calculate mean on say columns 2,5,6,7 and 8. Can we do it with some function for dataframe object?\n\nWhat I know is do a for loop, get value of row for each element in row_index and keep doing mean. Do we have some direct function where we can pass row_list, and column_list and axis, for ex df.meanAdvance(row_list,column_list,axis=0) ?\n\nI have seen DataFrame.mean() but it didn't help I guess.\n\n  a b c d q \n0 1 2 3 0 5\n1 1 2 3 4 5\n2 1 1 1 6 1\n3 1 0 0 0 0\n\n\nI want mean of 0, 2, 3 rows for each a, b, d columns \n\n  a b d\n0 1 1 2\n\n"
"I want to plot RESULT vs TIME based on a testresult.csv file that has following format, and I have trouble to get the TIME column's datatype defined properly.\nTIME,RESULT  \n03/24/2016 12:27:11 AM,2  \n03/24/2016 12:28:41 AM,76  \n03/24/2016 12:37:23 AM,19  \n03/24/2016 12:38:44 AM,68  \n03/24/2016 12:42:02 AM,44  \n...\n\nTo read the csv file, this is the code I wrote:\nraw_df = pd.read_csv('testresult.csv', index_col=None, parse_dates=['TIME'], infer_datetime_format=True)\nThis code works, but it is extremely slow, and I assume that the infer_datetime_format takes time. So I tried to read in the csv by default first, and then convert the object dtype 'TIME' to datetime dtype by using to_datetime(), and I hope by defining the format, it might expedite the speed.\nraw_df =  pd.read_csv('testresult.csv')\nraw_df.loc['NEWTIME'] = pd.to_datetime(raw_df['TIME'], format='%m/%d%Y %-I%M%S %p')\n\nThis code complained error:\n\n&quot;ValueError: '-' is a bad directive in format '%m/%d%Y %-I%M%S %p'&quot;\n\n"
'I was following the style guide for pandas and it worked pretty well. \n\nHow can I keep these styles using the to_html command through Outlook? The documentation seems a bit lacking for me.\n\n(df.style\n   .format(percent)\n   .applymap(color_negative_red, subset=[\'col1\', \'col2\'])\n   .set_properties(**{\'font-size\': \'9pt\', \'font-family\': \'Calibri\'})\n   .bar(subset=[\'col4\', \'col5\'], color=\'lightblue\'))\n\nimport win32com.client as win32\noutlook = win32.Dispatch(\'outlook.application\')\nmail = outlook.CreateItem(0)\nmail.Subject = subject_name\nmail.HTMLbody = (\'&lt;html&gt;&lt;body&gt;&lt;p&gt;&lt;body style="font-size:11pt; \nfont-family:Calibri"&gt;Hello,&lt;/p&gt; + \'&lt;p&gt;Title of Data&lt;/p&gt;\' + df.to_html(\n            index=False, classes=????????) \'&lt;/body&gt;&lt;/html&gt;\')\nmail.send\n\n\nThe to_html documentation shows that there is a classes command that I can put inside of the to_html method, but I can\'t figure it out. It also seems like my dataframe does not carry the style that I specified up top. \n\nIf I try:\n\n df = (df.style\n       .format(percent)\n       .applymap(color_negative_red, subset=[\'col1\', \'col2\'])\n       .set_properties(**{\'font-size\': \'9pt\', \'font-family\': \'Calibri\'})\n       .bar(subset=[\'col4\', \'col5\'], color=\'lightblue\'))\n\n\nThen df is now a Style object and you can\'t use to_html.\n\nEdit - this is what I am currently doing to modify my tables. This works, but I can\'t keep the cool features of the .style method that pandas offers.\n\nemail_paragraph = """\n&lt;body style= "font-size:11pt; font-family:Calibri; text-align:left; margin: 0px auto" &gt;\n"""\n\nemail_caption = """\n&lt;body style= "font-size:10pt; font-family:Century Gothic; text-align:center; margin: 0px auto" &gt;\n"""\n\n\nemail_style = \'\'\'&lt;style type="text/css" media="screen" style="width:100%"&gt;\n    table, th, td {border: 0px solid black;  background-color: #eee; padding: 10px;}\n    th {background-color: #C6E2FF; color:black; font-family: Tahoma;font-size : 13; text-align: center;}\n    td {background-color: #fff; padding: 10px; font-family: Calibri; font-size : 12; text-align: center;}\n  &lt;/style&gt;\'\'\'\n\n'
"I'm doing a simple group by operation, trying to compare group means. As you can see below, I have selected specific columns from a larger dataframe, from which all missing values have been removed.\n\n\n\nBut when I group by, I am losing a couple of columns:\n\n\n\nI have never encountered this with pandas, and I'm not finding anything else on stack overflow that is all that similar. Does anybody have any insight?\n"
'Is there a standard way to convert matlab .mat (matlab formated data) files to Panda DataFrame? \n\nI am aware that a workaround is possible by using scipy.io but I am wondering whether there is a straightforward way to do it.\n'
'Say I have data in following format:\n\nRegion   Men   Women\nCity1    10   5\nCity2    50   89\n\n\nWhen I load it in Dataframe and plot graph, it shows index as X-axis labels instead of Region name. How do I get names on X-axis?\n\nSo far I tried:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt    \nplt.style.use(\'ggplot\')\nax = df[[\'Men\',\'Women\']].plot(kind=\'bar\', title ="Population",figsize=(15,10),legend=True, fontsize=12)\nax.set_xlabel("Areas",fontsize=12)\nax.set_ylabel("Population",fontsize=12)\nplt.show()\n\n\nCurrently it shows x ticks as 0,1,2..\n'
'In the excel sheet , i have two columns with large numbers.\n\nBut when i read the excel file with read_excel() and display the dataframe,\n\nthose two columns are printed in scientific format with exponential.\n\nHow can  get rid of this format?\n\nThanks \n\nOutput in Pandas\n\n\n'
'I have csv data and created Pandas dataframe using read_csv and forcing all columns as string.\nThen when I try to create Spark dataframe from the Pandas dataframe, I get the error message below.\n\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\nz=pd.read_csv("mydata.csv", dtype=str)\nz.info()\n\n\n\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 74044003 entries, 0 to 74044002\nData columns (total 12 columns):\nprimaryid       object\nevent_dt        object\nage             object\nage_cod         object\nage_grp         object\nsex             object\noccr_country    object\ndrug_seq        object\ndrugname        object\nroute           object\noutc_cod        object\npt              object\n\n\n\n\nq= sqlContext.createDataFrame(z)\n\n\n\n\nFile "&lt;stdin&gt;", line 1, in &lt;module&gt;\nFile "/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/context.py", line 425, in createDataFrame\nrdd, schema = self._createFromLocal(data, schema)\n File "/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/context.py", line 341, in _createFromLocal\nstruct = self._inferSchemaFromList(data)\n File "/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/context.py", line 241, in _inferSchemaFromList\nschema = reduce(_merge_type, map(_infer_schema, data))\n File "/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/types.py", line 862, in _merge_type\nfor f in a.fields]\n File "/usr/hdp/2.4.2.0-258/spark/python/pyspark/sql/types.py", line 856, in _merge_type\nraise TypeError("Can not merge type %s and %s" % (type(a), type(b)))\nTypeError: Can not merge type &lt;class \'pyspark.sql.types.DoubleType\'&gt; and &lt;class \'pyspark.sql.types.StringType\'&gt;\n\n\nHere is an example. I am downloading public data and creating pandas dataframe but spark does not create spark dataframe from the pandas dataframe.\n\nimport pandas as pd\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nurl ="http://www.nber.org/fda/faers/2016/demo2016q1.csv.zip"\n\nimport requests, zipfile, StringIO\nr = requests.get(url, stream=True)\nz = zipfile.ZipFile(StringIO.StringIO(r.content))\nz.extractall()\n\n\nz=pd.read_csv("demo2016q1.csv") # creates pandas dataframe\n\nData_Frame = sqlContext.createDataFrame(z)\n\n'
"At first, I tried writing some code that looked like this:\n\nimport numpy as np\nimport pandas as pd\nnp.random.seed(2016)\ntrain = pd.DataFrame(np.random.choice([np.nan, 1, 2], size=(10, 3)), \n                     columns=['Age', 'SibSp', 'Parch'])\n\ncomplete = train.dropna()    \ncomplete['AgeGt15'] = complete['Age'] &gt; 15\n\n\nAfter getting SettingWithCopyWarning, I tried using.loc:\n\ncomplete.loc[:, 'AgeGt15'] = complete['Age'] &gt; 15\ncomplete.loc[:, 'WithFamily'] = complete['SibSp'] + complete['Parch'] &gt; 0\n\n\nHowever, I still get the same warning. What gives?\n"
'I\'m working on creating a Python generated report that uses Pandas DataFrames. Currently I am using the DataFrame.to_string() method. However this writes to the file as a string. Is there a way for me to achieve this while keeping it as a table so I can use table formating.\n\nCode:\n\nSEMorgkeys = client.domain_organic(url, database = "us", display_limit = 10, export_columns=["Ph,Pp,Pd,Nq,Cp,Ur,Tr"])\norg_df = pd.DataFrame(SEMorgkeys)\n\nf = open(name, \'w\')\nf.write("\\nOrganic:\\n")\nf.write(org_df.to_string(index=False,justify="left"))\nf.close()\n\n\nCurrent Printout (as string):\n\nCPC    Keyword                        Position Difference Previous Position Search Volume Traffic (%) Url                                               \n75.92       small business factoring   0                   1                 210          11.69       https://www..com/small-business-f...\n80.19              factoring company   0                   8                1600           5.72       https://www..com/factoring-vs-ban...\n\n'
"I'm trying to set a maximum value of a pandas DataFrame column.  For example:\n\nmy_dict = {'a':[10,12,15,17,19,20]}\ndf = pd.DataFrame(my_dict)\n\ndf['a'].set_max(15)\n\n\nwould yield:\n\n    a\n0   10\n1   12\n2   15\n3   15\n4   15\n5   15\n\n\nBut it doesn't. \n\nThere are a million solutions to find the maximum value, but nothing to set the maximum value... at least that I can find.  \n\nI could iterate through the list, but I suspect there is a faster way to do it with pandas.  My lists will be significantly longer and thus I would expect iteration to take relatively longer amount of time.  Also, I'd like whatever solution to be able to handle NaN.\n"
"I am using python 3.4 on Jupyter Notebook, trying to merge two data frame like below:\n\ndf_A.shape\n(204479, 2)\n\ndf_B.shape\n(178, 3)\n\nnew_df = pd.merge(df_A, df_B,  how='inner', on='my_icon_number')\nnew_df.shape\n(266788, 4)\n\n\nI thought the new_df merged above should have few rows than df_A since merge is like an inner join. But why new_df here actually has more rows than df_A? \n\nHere is what I actually want:\n\nmy df_A is like:\n\n id           my_icon_number\n-----------------------------\n A1             123             \n B1             234\n C1             123\n D1             235\n E1             235\n F1             400\n\n\nand my df_B is like:\n\nmy_icon_number    color      size\n-------------------------------------\n  123              blue      small\n  234              red       large \n  235              yellow    medium\n\n\nThen I want new_df to be:\n\n id           my_icon_number     color       size\n--------------------------------------------------\n A1             123              blue        small\n B1             234              red         large\n C1             123              blue        small\n D1             235              yellow      medium\n E1             235              yellow      medium\n\n\nI don't really want to remove duplicates of my_icon_number in df_A. Any idea what I missed here?\n"
"You can calculate skew and kurtosis with the the methods\n\n\npd.Series.skew\npd.Series.kurt\npd.DataFrame.skew\npd.DataFrame.kurt\n\n\nHowever, there is no convenient way to calculate the coskew or cokurtosis between variables.  Or even better, the coskew or cokurtosis matrix.\n\n\n\nConsider the pd.DataFrame df\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(10, 2), columns=list('ab'))\n\ndf\n\n          a         b\n0  0.444939  0.407554\n1  0.460148  0.465239\n2  0.462691  0.016545\n3  0.850445  0.817744\n4  0.777962  0.757983\n5  0.934829  0.831104\n6  0.879891  0.926879\n7  0.721535  0.117642\n8  0.145906  0.199844\n9  0.437564  0.100702\n\n\nHow do I calculate the coskew and cokurtosis of a and b?\n"
'QUESTION\n\nHow can assign be used to return a copy of the original DataFrame with multiple new columns added?\n\nDESIRED RESULT\n\ndf = pd.DataFrame({\'A\': range(1, 5), \'B\': range(11, 15)})\n&gt;&gt;&gt; df.assign({\'C\': df.A.apply(lambda x: x ** 2), \'D\': df.B * 2})\n   A   B   C   D\n0  1  11   1  22\n1  2  12   4  24\n2  3  13   9  26\n3  4  14  16  28\n\n\nATTEMPTS\n\nThe example above results in:\n\nValueError: Wrong number of items passed 2, placement implies 1.\n\nBACKGROUND\n\nThe assign function in Pandas takes a copy of the relevant dataframe joined to the newly assigned column, e.g.\n\ndf = df.assign(C=df.B * 2)\n&gt;&gt;&gt; df\n   A   B   C\n0  1  11  22\n1  2  12  24\n2  3  13  26\n3  4  14  28\n\n\nThe 0.19.2 documentation for this function implies that more than one column can be added to the dataframe.\n\n\n  Assigning multiple columns within the same assign is possible, but you cannot reference other columns created within the same assign call.\n\n\nIn addition:\n\n\n  Parameters:\n  kwargs : keyword, value pairs\n  \n  keywords are the column names.\n\n\nThe source code for the function states that it accepts a dictionary:\n\ndef assign(self, **kwargs):\n    """\n    .. versionadded:: 0.16.0\n    Parameters\n    ----------\n    kwargs : keyword, value pairs\n        keywords are the column names. If the values are callable, they are computed \n        on the DataFrame and assigned to the new columns. If the values are not callable, \n        (e.g. a Series, scalar, or array), they are simply assigned.\n\n    Notes\n    -----\n    Since ``kwargs`` is a dictionary, the order of your\n    arguments may not be preserved. The make things predicatable,\n    the columns are inserted in alphabetical order, at the end of\n    your DataFrame. Assigning multiple columns within the same\n    ``assign`` is possible, but you cannot reference other columns\n    created within the same ``assign`` call.\n    """\n\n    data = self.copy()\n\n    # do all calculations first...\n    results = {}\n    for k, v in kwargs.items():\n\n        if callable(v):\n            results[k] = v(data)\n        else:\n            results[k] = v\n\n    # ... and then assign\n    for k, v in sorted(results.items()):\n        data[k] = v\n\n    return data\n\n'
"I have pandas data frame in which I need to replace one part of the vale with another value\n\nfor Example. I have \n\nHF - Antartica\nHF - America\nHF - Asia\n\n\nout of which I'd like to replace ony the HF - part\nthus the result would be \n\nHi Funny Antartica\nHi Funny America\nHi Funny Asia\n\n\nI have tried pd.replace() but it doesnt work as I need only one part of the string replaced, rather than the entire string\n"
"I have a list of names like:\n\nnames = ['A', 'B', 'C', 'D']\n\n\nand a list of documents, that in each documents some of these names are mentioned.\n\ndocument =[['A', 'B'], ['C', 'B', 'K'],['A', 'B', 'C', 'D', 'Z']]\n\n\nI would like to get an output as a matrix of co-occurrences like:\n\n  A  B  C  D\nA 0  2  1  1\nB 2  0  2  1\nC 1  2  0  1\nD 1  1  1  0\n\n\nThere is a solution (Creating co-occurrence matrix) for this problem in R, but I couldn't do it in Python. I am thinking of doing it in Pandas, but yet no progress!\n"
'I want to see the datatype of all columns stored in my dataframe without iterating over them. What is the way?\n'
"I've been reading through the documentation and many explanations and examples use levels as something taken for granted. Imho the docs lack a bit on a fundamental explanation of the data structure and definitions. \n\nWhat are levels in a data frame? What are levels in a MultiIndex index? \n"
"I understand how to display two plots next to each other (horizontally) in Jupyter Notebook, but I don't know if there is a way to display a plot with a dataframe next to it. I imagine it could look something like this:\n\n\n\nHowever, I'm not able to do this, and whenever I print out the dataframe, it appears below my plot...\n\nHere is a similar question, but I am also outputting plots within this same cell that I want to be vertically oriented. \n\nI currently have this:\n\n# line plots\ndf_plot[['DGO %chg','DLM %chg']].plot(figsize=(15,5),grid=True)\nplt.ylim((-ylim,ylim))\n\ndf_plot[['Diff']].plot(kind='area',color='lightgrey',figsize=(15,1))\nplt.xticks([])\nplt.xlabel('')\nplt.ylim((0,ylim_diff))\nplt.show()\n\n# scatter plots\nplt.scatter(x=df_scat[:-7]['DGO'],y=df_scat[:-7]['DLM'])\nplt.scatter(x=df_scat[-7:]['DGO'],y=df_scat[-7:]['DLM'],color='red')\nplt.title('%s Cluster Last 7 Days'%asset)\nplt.show()\n\n# display dataframe\n# display(df_scat[['DGO','DLM']][:10]) &lt;-- prints underneath, not working\n\n\n\n\nWhere the red box shows where I want my dataframe to appear. Does anyone have any ideas about how to do this?\n\nThanks for your thoughts!\n"
"My df looks as follows:\n\nIndex    Country    Val1  Val2 ... Val10\n1        Australia  1     3    ... 5\n2        Bambua     12    33   ... 56\n3        Tambua     14    34   ... 58\n\n\nI'd like to substract Val10 from Val1 for each country, so output looks like:\n\nCountry    Val10-Val1\nAustralia  4\nBambua     23\nTambua     24\n\n\nSo far I've got:\n\ndef myDelta(row):\n    data = row[['Val10', 'Val1']]\n    return pd.Series({'Delta': np.subtract(data)})\n\ndef runDeltas():\n    myDF = getDF() \\\n        .apply(myDelta, axis=1) \\\n        .sort_values(by=['Delta'], ascending=False)\n    return myDF\n\n\nrunDeltas results in this error:\n\nValueError: ('invalid number of arguments', u'occurred at index 9')\n\n\nWhat's the proper way to fix this?\n"
"I have a pandas dataframe of shape (39, 67). When I plot it's seaborn heatmap, I don't get as many labels on the X and Y axes. .get_xticklabels() method also returns only 23 labels.\n\n\n\nmatplotlib doesn't show any labels (only numbers) as well.\n\n\n\nBoth these heatmaps are for the same dataframe (39, 67).\n"
"\nfollowing code is giving me error.\n\n\nimport pandas as pd\ndf = pd.DataFrame({'a' : [1,2,3]})\ndf.to_hdf('temp.h5', key='df', mode='w')\n\n\n\nThis is giving me error.\n\n\n  Missing optional dependency 'tables'.  Use pip or conda to install tables.\n\nI already tried \nImportError HDFStore requires PyTables No module named tables. Still the same error.\nI am getting the same error when reading hdf file. And tables are already installed for my python.\n\n\nSome version info.\n\n\n  \n  python 3.7.4\n  pandas 0.25.2\n  windows10\n  \n\n\nPS: You can reproduce this in repl https://repl.it/.\n\nUpdate:\n\n\nI tried runnig following.\n\n\nimport tables\n\n\nand got this error:\n\n\n  ImportError: Could not load any of ['hdf5.dll', 'hdf5dll.dll'], please ensure that it can be found in the system path.\n\n\n\nIt looks like pandas is not giving accurate message for this. Its just saying missing dependency when its actually present. \nIf anyone knows how to resolve this. That will help.\n\n"
"I like to think I'm not an idiot, but maybe I'm wrong. Can anyone explain to me why this isn't working? I can achieve the desired results using 'merge'. But I eventually need to join multiple pandas DataFrames so I need to get this method working.\n\nIn [2]: left = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value': [4.685, 2.491]})\n\nIn [3]: right = pandas.DataFrame({'ST_NAME': ['Oregon', 'Nebraska'], 'value2': [6.218, 0.001]})\n\nIn [4]: left.join(right, on='ST_NAME', lsuffix='_left', rsuffix='_right')\nOut[4]: \n  ST_NAME_left  value ST_NAME_right  value2\n0       Oregon  4.685           NaN     NaN\n1     Nebraska  2.491           NaN     NaN\n\n"
"How do I resample a time series in pandas to a weekly frequency where the weeks start on an arbitrary day? I see that there's an optional keyword base but it only works for intervals shorter than a day.\n"
"First I have the following empty DataFrame preallocated:\n\ndf=DataFrame(columns=range(10000),index=range(1000))\n\n\nThen I want to update the df row by row (efficiently) with a length-10000 numpy array as data. My problem is: I don't even have an idea what method of DataFrame I should use to accomplish this task.\n\nThank you!\n"
"I read a csv file containing 150,000 lines into a pandas dataframe. This dataframe has a field, Date, with the dates in yyyy-mm-dd format. I want to extract the month, day and year from it and copy into the dataframes' columns, Month, Day and Year respectively. For a few hundred records the below two methods work ok, but for 150,000 records both take a ridiculously long time to execute. Is there a faster way to do this for 100,000+ records?\n\nFirst method: \n\ndf = pandas.read_csv(filename)\nfor i in xrange(len(df)): \n   df.loc[i,'Day'] = int(df.loc[i,'Date'].split('-')[2])\n\n\nSecond method: \n\ndf = pandas.read_csv(filename)\nfor i in xrange(len(df)):\n   df.loc[i,'Day'] = datetime.strptime(df.loc[i,'Date'], '%Y-%m-%d').day\n\n\nThank you.\n"
'I am trying to join two numpy arrays. In one I have a set of columns/features after running TF-IDF on a single column of text. In the other I have one column/feature which is an integer. So I read in a column of train and test data, run TF-IDF on this, and then I want to add another integer column because I think this will help my classifier learn more accurately how it should behave. \n\nUnfortunately, I am getting the error in the title when I try and run hstack to add this single column to my other numpy array.\n\nHere is my code : \n\n  #reading in test/train data for TF-IDF\n  traindata = list(np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter=";"))[:,2])\n  testdata = list(np.array(p.read_csv(\'FinalTestCSVFin.csv\', delimiter=";"))[:,2])\n\n  #reading in labels for training\n  y = np.array(p.read_csv(\'FinalCSVFin.csv\', delimiter=";"))[:,-2]\n\n  #reading in single integer column to join\n  AlexaTrainData = p.read_csv(\'FinalCSVFin.csv\', delimiter=";")[["alexarank"]]\n  AlexaTestData = p.read_csv(\'FinalTestCSVFin.csv\', delimiter=";")[["alexarank"]]\n  AllAlexaAndGoogleInfo = AlexaTestData.append(AlexaTrainData)\n\n  tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents=\'unicode\',  \n        analyzer=\'word\',token_pattern=r\'\\w{1,}\',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1) #tf-idf object\n  rd = lm.LogisticRegression(penalty=\'l2\', dual=True, tol=0.0001, \n                             C=1, fit_intercept=True, intercept_scaling=1.0, \n                             class_weight=None, random_state=None) #Classifier\n  X_all = traindata + testdata #adding test and train data to put into tf-idf\n  lentrain = len(traindata) #find length of train data\n  tfv.fit(X_all) #fit tf-idf on all our text\n  X_all = tfv.transform(X_all) #transform it\n  X = X_all[:lentrain] #reduce to size of training set\n  AllAlexaAndGoogleInfo = AllAlexaAndGoogleInfo[:lentrain] #reduce to size of training set\n  X_test = X_all[lentrain:] #reduce to size of training set\n\n  #printing debug info, output below : \n  print "X.shape =&gt; " + str(X.shape)\n  print "AllAlexaAndGoogleInfo.shape =&gt; " + str(AllAlexaAndGoogleInfo.shape)\n  print "X_all.shape =&gt; " + str(X_all.shape)\n\n  #line we get error on\n  X = np.hstack((X, AllAlexaAndGoogleInfo))\n\n\nBelow is the output and error message :\n\nX.shape =&gt; (7395, 238377)\nAllAlexaAndGoogleInfo.shape =&gt; (7395, 1)\nX_all.shape =&gt; (10566, 238377)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-12-2b310887b5e4&gt; in &lt;module&gt;()\n     31 print "X_all.shape =&gt; " + str(X_all.shape)\n     32 #X = np.column_stack((X, AllAlexaAndGoogleInfo))\n---&gt; 33 X = np.hstack((X, AllAlexaAndGoogleInfo))\n     34 sc = preprocessing.StandardScaler().fit(X)\n     35 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\core\\shape_base.pyc in hstack(tup)\n    271     # As a special case, dimension 0 of 1-dimensional arrays is "horizontal"\n    272     if arrs[0].ndim == 1:\n--&gt; 273         return _nx.concatenate(arrs, 0)\n    274     else:\n    275         return _nx.concatenate(arrs, 1)\n\nValueError: all the input arrays must have same number of dimensions\n\n\nWhat is causing my problem here? How can I fix this? As far as I can see I should be able to join these columns? What have I misunderstood?\n\nThank you.\n\nEdit : \n\nUsing the method in the answer below gets the following error : \n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-16-640ef6dd335d&gt; in &lt;module&gt;()\n---&gt; 36 X = np.column_stack((X, AllAlexaAndGoogleInfo))\n     37 sc = preprocessing.StandardScaler().fit(X)\n     38 X = sc.transform(X)\n\nC:\\Users\\Simon\\Anaconda\\lib\\site-packages\\numpy\\lib\\shape_base.pyc in column_stack(tup)\n    294             arr = array(arr,copy=False,subok=True,ndmin=2).T\n    295         arrays.append(arr)\n--&gt; 296     return _nx.concatenate(arrays,1)\n    297 \n    298 def dstack(tup):\n\nValueError: all the input array dimensions except for the concatenation axis must match exactly\n\n\nInterestingly, I tried to print the dtype of X and this worked fine : \n\nX.dtype =&gt; float64\n\n\nHowever, trying to print the dtype of AllAlexaAndGoogleInfo like so : \n\nprint "AllAlexaAndGoogleInfo.dtype =&gt; " + str(AllAlexaAndGoogleInfo.dtype) \n\n\nproduces :\n\n\'DataFrame\' object has no attribute \'dtype\'\n\n'
'I am trying to join to dataframe on the same column "Date", the code is as follow:\n\nimport pandas as pd\nfrom datetime import datetime\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq="W-FRI")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\n\n\nIt complains dataframe df_train_csv has no column named "Date". I\'d like to set "Date" in both dataframe as index and I am wondering what is the best way to join dataframe with date as the index?\n\nUPDATE:\n\nThat is the sample data\n\nDate,Weekly_Sales\n2010-02-05,24924.5\n2010-02-12,46039.49\n2010-02-19,41595.55\n2010-02-26,19403.54\n2010-03-05,21827.9\n2010-03-12,21043.39\n2010-03-19,22136.64\n2010-03-26,26229.21\n2010-04-02,57258.43\n2010-04-09,42960.91\n2010-04-16,17596.96\n2010-04-23,16145.35\n2010-04-30,16555.11\n2010-05-07,17413.94\n2010-05-14,18926.74\n2010-05-21,14773.04\n2010-05-28,15580.43\n2010-06-04,17558.09\n2010-06-11,16637.62\n2010-06-18,16216.27\n2010-06-25,16328.72\n2010-07-02,16333.14\n2010-07-09,17688.76\n2010-07-16,17150.84\n2010-07-23,15360.45\n2010-07-30,15381.82\n2010-08-06,17508.41\n2010-08-13,15536.4\n2010-08-20,15740.13\n2010-08-27,15793.87\n2010-09-03,16241.78\n2010-09-10,18194.74\n2010-09-17,19354.23\n2010-09-24,18122.52\n2010-10-01,20094.19\n2010-10-08,23388.03\n2010-10-15,26978.34\n2010-10-22,25543.04\n2010-10-29,38640.93\n2010-11-05,34238.88\n2010-11-12,19549.39\n2010-11-19,19552.84\n2010-11-26,18820.29\n2010-12-03,22517.56\n2010-12-10,31497.65\n2010-12-17,44912.86\n2010-12-24,55931.23\n2010-12-31,19124.58\n2011-01-07,15984.24\n2011-01-14,17359.7\n2011-01-21,17341.47\n2011-01-28,18461.18\n2011-02-04,21665.76\n2011-02-11,37887.17\n2011-02-18,46845.87\n2011-02-25,19363.83\n2011-03-04,20327.61\n2011-03-11,21280.4\n2011-03-18,20334.23\n2011-03-25,20881.1\n2011-04-01,20398.09\n2011-04-08,23873.79\n2011-04-15,28762.37\n2011-04-22,50510.31\n2011-04-29,41512.39\n2011-05-06,20138.19\n2011-05-13,17235.15\n2011-05-20,15136.78\n2011-05-27,15741.6\n2011-06-03,16434.15\n2011-06-10,15883.52\n2011-06-17,14978.09\n2011-06-24,15682.81\n2011-07-01,15363.5\n2011-07-08,16148.87\n2011-07-15,15654.85\n2011-07-22,15766.6\n2011-07-29,15922.41\n2011-08-05,15295.55\n2011-08-12,14539.79\n2011-08-19,14689.24\n2011-08-26,14537.37\n2011-09-02,15277.27\n2011-09-09,17746.68\n2011-09-16,18535.48\n2011-09-23,17859.3\n2011-09-30,18337.68\n2011-10-07,20797.58\n2011-10-14,23077.55\n2011-10-21,23351.8\n2011-10-28,31579.9\n2011-11-04,39886.06\n2011-11-11,18689.54\n2011-11-18,19050.66\n2011-11-25,20911.25\n2011-12-02,25293.49\n2011-12-09,33305.92\n2011-12-16,45773.03\n2011-12-23,46788.75\n2011-12-30,23350.88\n2012-01-06,16567.69\n2012-01-13,16894.4\n2012-01-20,18365.1\n2012-01-27,18378.16\n2012-02-03,23510.49\n2012-02-10,36988.49\n2012-02-17,54060.1\n2012-02-24,20124.22\n2012-03-02,20113.03\n2012-03-09,21140.07\n2012-03-16,22366.88\n2012-03-23,22107.7\n2012-03-30,28952.86\n2012-04-06,57592.12\n2012-04-13,34684.21\n2012-04-20,16976.19\n2012-04-27,16347.6\n2012-05-04,17147.44\n2012-05-11,18164.2\n2012-05-18,18517.79\n2012-05-25,16963.55\n2012-06-01,16065.49\n2012-06-08,17666\n2012-06-15,17558.82\n2012-06-22,16633.41\n2012-06-29,15722.82\n2012-07-06,17823.37\n2012-07-13,16566.18\n2012-07-20,16348.06\n2012-07-27,15731.18\n2012-08-03,16628.31\n2012-08-10,16119.92\n2012-08-17,17330.7\n2012-08-24,16286.4\n2012-08-31,16680.24\n2012-09-07,18322.37\n2012-09-14,19616.22\n2012-09-21,19251.5\n2012-09-28,18947.81\n2012-10-05,21904.47\n2012-10-12,22764.01\n2012-10-19,24185.27\n2012-10-26,27390.81\n\n\nI will read it from a csv file. But sometimes, some weeks may be missing. Therefore, I am trying to generate a date range like this:\n\ndf_train_fly = pd.date_range(start, end, freq="W-FRI")\n\n\nThis generated dataframe contains all weeks in the range so I need to merge those two dataframe into one.\n\nIf I check df_train_csv[\'Date\'] and df_train_fly[\'Date\'] from the iPython console, they both showed as dtype: datetime64[ns]\n'
"Is there a pandas function that allows selection from different columns based on a condition? This is analogous to a CASE statement in a SQL Select clause. For example, say I have the following DataFrame:\n\nfoo = DataFrame(\n    [['USA',1,2],\n    ['Canada',3,4],\n    ['Canada',5,6]], \n    columns = ('Country', 'x', 'y')\n)\n\n\nI want to select from column 'x' when Country=='USA', and from column 'y' when Country=='Canada', resulting in something like the following:\n\n  Country  x  y  z\n0     USA  1  2  1\n1  Canada  3  4  4\n2  Canada  5  6  6\n\n[3 rows x 4 columns]\n\n"
"I would like to sort the following dataframe:\n\nRegion           LSE          North      South\n0                   Cn     33.330367   9.178917\n1               Develd     -36.157025 -27.669988\n2               Wetnds    -38.480206 -46.089908\n3                Oands    -47.986764 -32.324991\n4               Otherg    323.209834  28.486310\n5                 Soys      34.936147   4.072872\n6                  Wht     0.983977 -14.972555\n\n\nI would like to sort it so the LSE column is reordered based on the list:\n\nlst = ['Oands','Wetnds','Develd','Cn','Soys','Otherg','Wht']\n\n\nof, course the other columns will need to be reordered accordingly as well. Is there any way to do this in pandas?\n"
"I am trying to output all columns of a data frame .\nHere is the code below:\ndf_advertiser_activity_part_qa  = df_advertiser_activity_part.loc[(df_advertiser_activity_part['advertiser_id']==209988 )]\ndf_advertiser_activity_part_qa.sort(columns ='date_each_day_et')\n\ndf_advertiser_activity_part_qa\n\nwhen I output the data frame not all columns gets displayed . This has 21 columns and between some columns there is just there dots &quot;...&quot; I am using ipython notebook . Is there a way by  which this can be ignored.\n"
'I recall from my MatLab days using structured arrays wherein you could store different data as an attribute of the main structure.  Something like:\n\na = {}\na.A = magic(10)\na.B = magic(50); etc.\n\n\nwhere a.A and a.B are completely separate from each other allowing you to store different types within a and operate on them as desired.  Pandas allows us to do something similar, but not quite the same.  \n\nI am using Pandas and want to store attributes of a dataframe without actually putting it within a dataframe.  This can be done via:\n\nimport pandas as pd\n\na = pd.DataFrame(data=pd.np.random.randint(0,100,(10,5)),columns=list(\'ABCED\')\n\n# now store an attribute of &lt;a&gt;\na.local_tz = \'US/Eastern\'\n\n\nNow, the local timezone is stored in a, but I cannot save this attribute when I save the dataframe (i.e. after re-loading a there is no a.local_tz).  Is there a way to save these attributes?  \n\nCurrently, I am just making new columns in the dataframe to hold information like timezone, latitude, longituded, etc., but this seems to be a bit of a waste.  Further, when I do analysis on the data I run into problems of having to exclude these other columns.\n\n##################\nBEGIN EDIT\n##################\n\nUsing unutbu\'s advice, I now store the data in h5 format.  As mentioned, loading metadata back in as attributes of the dataframe is risky.  However, since I am the creator of these files (and the processing algorithms) I can choose what is stored as metadata and what is not.  When processing the data that will go into the h5 files, I choose to store the metadata in a dictionary that is initialized as an attribute of my classes.  I made a simple IO class to import the h5 data, and made the metadata as class attributes.  Now I can work on my dataframes without risk of losing the metadata.  \n\nclass IO():\n    def __init__(self):\n        self.dtfrmt = \'dummy_str\'\n\n    def h5load(self,filename,update=False):\n        \'\'\'h5load loads the stored HDF5 file.  Both the dataframe (actual data) and \n        the associated metadata are stored in the H5file\n\n        NOTE: This does not load "any" H5 \n        file, it loads H5 files specifically created to hold dataframe data and \n        metadata.\n\n        When multi-indexed dataframes are stored in the H5 format the date \n        values (previously initialized with timezone information) lose their\n        timezone localization.  Therefore, &lt;h5load&gt; re-localizes the \'DATE\' \n        index as UTC.\n\n        Parameters\n        ----------\n        filename : string/path\n            path and filename of H5 file to be loaded.  H5 file must have been \n            created using &lt;h5store&gt; below.\n\n        udatedf : boolean True/False\n            default: False\n            If the selected dataframe is to be updated then it is imported \n            slightly different.  If update==True, the &lt;metadata&gt; attribute is\n            returned as a dictionary and &lt;data&gt; is returned as a dataframe \n            (i.e., as a stand-alone dictionary with no attributes, and NOT an \n            instance of the IO() class).  Otherwise, if False, &lt;metadata&gt; is \n            returned as an attribute of the class instance.\n\n        Output\n        ------\n        data : Pandas dataframe with attributes\n            The dataframe contains only the data as collected by the instrument.  \n            Any metadata (e.g. timezone, scaling factor, basically anything that\n            is constant throughout the file) is stored as an attribute (e.g. lat \n            is stored as &lt;data.lat&gt;).\'\'\'\n\n        with pd.HDFStore(filename,\'r\') as store:\n            self.data = store[\'mydata\']\n            self.metadata = store.get_storer(\'mydata\').attrs.metadata    # metadata gets stored as attributes, so no need to make &lt;metadata&gt; an attribute of &lt;self&gt;\n\n            # put metadata into &lt;data&gt; dataframe as attributes\n            for r in self.metadata:\n                setattr(self,r,self.metadata[r])\n\n        # unscale data\n        self.data, self.metadata = unscale(self.data,self.metadata,stringcols=[\'routine\',\'date\'])\n\n        # when pandas stores multi-index dataframes as H5 files the timezone\n        # initialization is lost.  Remake index with timezone initialized: only\n        # for multi-indexed dataframes\n        if isinstance(self.data.index,pd.core.index.MultiIndex):\n            # list index-level names, and identify \'DATE\' level\n            namen = self.data.index.names\n            date_lev = namen.index(\'DATE\')\n\n            # extract index as list and remake tuples with timezone initialized\n            new_index = pd.MultiIndex.tolist(self.data.index)\n            for r in xrange( len(new_index) ):\n                tmp = list( new_index[r] )\n                tmp[date_lev] = utc.localize( tmp[date_lev] )\n\n                new_index[r] = tuple(tmp)\n\n            # reset multi-index\n            self.data.index = pd.MultiIndex.from_tuples( new_index, names=namen )\n\n\n        if update:\n            return self.metadata, self.data\n        else:\n            return self\n\n\n\n\n\n    def h5store(self,data, filename, **kwargs):\n        \'\'\'h5store stores the dataframe as an HDF5 file.  Both the dataframe \n        (actual data) and the associated metadata are stored in the H5file\n\n        Parameters\n        ----------\n        data : Pandas dataframe NOT a class instance\n            Must be a dataframe, not a class instance (i.e. cannot be an instance \n            named &lt;data&gt; that has an attribute named &lt;data&gt; (e.g. the Pandas \n            data frame is stored in data.data)).  If the dataframe is under\n            data.data then the input variable must be data.data.\n\n        filename : string/path\n            path and filename of H5 file to be loaded.  H5 file must have been \n            created using &lt;h5store&gt; below.\n\n        **kwargs : dictionary\n            dictionary containing metadata information.\n\n\n        Output\n        ------\n        None: only saves data to file\'\'\'\n\n        with pd.HDFStore(filename,\'w\') as store:\n            store.put(\'mydata\',data)\n            store.get_storer(\'mydata\').attrs.metadata = kwargs\n\n\nH5 files are then loaded via data = IO().h5load(\'filename.h5\')\nthe dataframe is stored under data.data\nI retain the metadata dictionary under data.metadata and have created individual metadata attributes (e.g. data.lat created from data.metadata[\'lat\']).\n\nMy index time stamps are localized to pytz.utc().  However, when a multi-indexed dataframe is stored to h5 the timezone localization is lost (using Pandas 15.2), so I correct for this in IO().h5load.\n'
"Because\n\n\nI don't need double precision\nMy machine has limited memory and I want to process bigger datasets\nI need to pass the extracted data (as matrix) to BLAS libraries, and BLAS calls for single precision are 2x faster than for double precision equivalence.\n\n\nNote that not all columns in the raw csv file have float types. I only need to set float32 as the default for float columns.\n"
'I have a DataFrame ave_data that contains the following:\n\nave_data\n\nTime        F7           F8            F9  \n00:00:00    43.005593    -56.509746    25.271271  \n01:00:00    55.114918    -59.173852    31.849262  \n02:00:00    63.990762    -64.699492    52.426017\n\n\nI want to add another column to this dataframe, containing the average of the values under column F7, F8 and F9 for each row.  \n\nThe ave_data DataFrame might change size as my code reads from different Excel files later, so the method needs to be generic (i.e add the column containing the average always as the last column in the DataFrame, not in column number 4)\n\ndesired output\n\nTime        F7           F8            F9           Average\n00:00:00    43.005593    -56.509746    25.271271    4.25  \n01:00:00    55.114918    -59.173852    31.849262    9.26\n02:00:00    63.990762    -64.699492    52.426017    17.24\n\n'
'df:\n\nname score\nA      1\nA      2\nA      3\nA      4\nA      5\nB      2\nB      4\nB      6 \nB      8\n\n\nWant to get the following new dataframe in the form of below:\n\n   name count mean std min 25% 50% 75% max\n    A     5    3    .. ..  ..  ..  ..  ..\n    B     4    5    .. ..  ..  ..  ..  ..\n\n\nHow to exctract the information from df.describe() and reformat it?\nThanks\n'
"I'm trying to adjust the formatting of the date tick labels of the x-axis so that it only shows the Year and Month values. From what I've found online, I have to use mdates.DateFormatter, but it's not taking effect at all with my current code as is. Anyone see where the issue is? (the dates are the index of the pandas Dataframe)\n\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\nfig = plt.figure(figsize = (10,6))\nax = fig.add_subplot(111)\n\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n\nbasicDF['some_column'].plot(ax=ax, kind='bar', rot=75)\n\nax.xaxis_date()\n\n\n\n\nReproducible scenario code:\n\nimport numpy as np\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\nrng = pd.date_range('1/1/2014', periods=20, freq='m')\n\nblah = pd.DataFrame(data = np.random.randn(len(rng)), index=rng)\n\nfig = plt.figure(figsize = (10,6))\nax = fig.add_subplot(111)\n\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n\nblah.plot(ax=ax, kind='bar')\n\nax.xaxis_date()\n\n\nStill can't get just the year and month to show up.\n\nIf I set the format after .plot , get an error like this:\n\n\n  ValueError: DateFormatter found a value of x=0, which is an illegal date. This usually occurs because you have not informed the axis that it is plotting dates, e.g., with ax.xaxis_date(). \n\n\nIt's the same for if I put it before ax.xaxis_date() or after.\n"
"There must be an easy way to do this, but I was unable to find an elegant solution for on SO or work it out by myself.\n\nI'm trying to count the number of duplicate values based on set of columns in a DataFrame.\n\nExample:\n\nprint df\n\n    Month   LSOA code   Longitude   Latitude    Crime type\n0   2015-01 E01000916   -0.106453   51.518207   Bicycle theft\n1   2015-01 E01000914   -0.111497   51.518226   Burglary\n2   2015-01 E01000914   -0.111497   51.518226   Burglary\n3   2015-01 E01000914   -0.111497   51.518226   Other theft\n4   2015-01 E01000914   -0.113767   51.517372   Theft from the person\n\n\nMy workaround:\n\ncounts = dict()\nfor i, row in df.iterrows():\n    key = (\n            row['Longitude'],\n            row['Latitude'],\n            row['Crime type']\n        )\n\n    if counts.has_key(key):\n        counts[key] = counts[key] + 1\n    else:\n        counts[key] = 1\n\n\nAnd I get the counts:\n\n{(-0.11376700000000001, 51.517371999999995, 'Theft from the person'): 1,\n (-0.111497, 51.518226, 'Burglary'): 2,\n (-0.111497, 51.518226, 'Other theft'): 1,\n (-0.10645299999999999, 51.518207000000004, 'Bicycle theft'): 1}\n\n\nAside from the fact this code could be improved as well (feel free to comment how), what would be the way to do it through Pandas?\n\nFor those interested I'm working on a dataset from https://data.police.uk/\n"
"I have a sql file which consists of the data below which I read into pandas.\n\ndf = pandas.read_sql('Database count details', con=engine,\n                     index_col='id', parse_dates='newest_available_date')\n\n\nOutput\n\nid       code   newest_date_available\n9793708  3514   2015-12-24\n9792282  2399   2015-12-25\n9797602  7452   2015-12-25\n9804367  9736   2016-01-20\n9804438  9870   2016-01-20\n\n\nThe next line of code is to get last week's date\n\ndate_before = datetime.date.today() - datetime.timedelta(days=7) # Which is 2016-01-20\n\n\nWhat I am trying to do is, to compare date_before with df and print out all rows that is less than date_before\n\nif (df['newest_available_date'] &lt; date_before):\n     print(#all rows) \n\nObviously this returns me an error The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\nHow should I do this?\n"
"df1 = pd.DataFrame({'a':[1,2,3],'x':[4,5,6],'y':[7,8,9]})\ndf2 = pd.DataFrame({'b':[10,11,12],'x':[13,14,15],'y':[16,17,18]})\n\n\nI'm trying to merge the two data frames using the keys from the df1. I think I should use pd.merge for this, but I how can I tell pandas to place the values in the b column of df2 in the a column of df1. This is the output I'm trying to achieve: \n\n    a   x   y\n0   1   4   7\n1   2   5   8\n2   3   6   9\n3   10  13  16\n4   11  14  17\n5   12  15  18\n\n"
'I am merging two data frames using pandas.merge. Even after specifying how = left option, I found the number of rows of merged data frame is larger than the original. Why does this happen?\n\npanel = pd.read_csv(file1, encoding =\'cp932\')\nbefore_len = len(panel)\n\nprof_2000 = pd.read_csv(file2, encoding =\'cp932\').drop_duplicates()\n\ntemp_2000 = pd.merge(panel, prof_2000, left_on=\'Candidate_u\', right_on="name2", how="left")\n\nafter_len =  len(temp_2000)\nprint(before_len, after_len)\n&gt; 12661 13915\n\n'
"I have looked up this issue and most questions are for more complex replacements. However in my case I have a very simple dataframe as a test dummy.\n\nThe aim is to replace a string anywhere in the dataframe with an nan, however this does not seem to work (i.e. does not replace; no errors whatsoever). I've tried replacing with another string and it does not work either. E.g.\n\nd = {'color' : pd.Series(['white', 'blue', 'orange']),\n   'second_color': pd.Series(['white', 'black', 'blue']),\n   'value' : pd.Series([1., 2., 3.])}\ndf = pd.DataFrame(d)\ndf.replace('white', np.nan)\n\n\nThe output is still:\n\n      color second_color  value\n  0   white        white      1\n  1    blue        black      2\n  2  orange         blue      3\n\n"
'How do I change the special characters to the usual alphabet letters?\nThis is my dataframe:\n\nIn [56]: cities\nOut[56]:\n\nTable Code  Country         Year        City        Value       \n240         Åland Islands   2014.0      MARIEHAMN   11437.0 1\n240         Åland Islands   2010.0      MARIEHAMN   5829.5  1\n240         Albania         2011.0      Durrës      113249.0\n240         Albania         2011.0      TIRANA      418495.0\n240         Albania         2011.0      Durrës      56511.0 \n\n\nI want it to look like this:\n\nIn [56]: cities\nOut[56]:\n\nTable Code  Country         Year        City        Value       \n240         Aland Islands   2014.0      MARIEHAMN   11437.0 1\n240         Aland Islands   2010.0      MARIEHAMN   5829.5  1\n240         Albania         2011.0      Durres      113249.0\n240         Albania         2011.0      TIRANA      418495.0\n240         Albania         2011.0      Durres      56511.0 \n\n'
'I came up with values in square bracket(more like a list) after applying str.findall() to column of a pandas dataframe. How can I remove the square bracket ?\n\nprint df\n\nid     value                 \n1      [63]        \n2      [65]       \n3      [64]        \n4      [53]       \n5      [13]      \n6      [34]  \n\n'
'I have a pandas dataframe with two columns, a date column and an int column, and I\'d simply like to add the int column (in days) to the date column. I found a solution using df.apply(), but that was too slow on my full dataset. I don\'t see a ton of documentation on doing this in a vectorized manner (the closest I could find was this ), so I wanted to make sure the solution I found was the best way to go forward.\n\nMy raw data is just a column of strings as a column of ints (days).\n\nimport pandas as pd\nfrom datetime import timedelta\n\ndf = pd.DataFrame([[\'2016-01-10\',28],[\'2016-05-11\',28],[\'2016-02-23\',15],[\'2015-12-08\',30]], \n                  columns = [\'ship_string\',\'days_supply\'])\nprint df  \n\nship_string  days_supply\n0  2016-01-10           28\n1  2016-05-11           28\n2  2016-02-23           15\n3  2015-12-08           30\n\n\nMy first thought (which worked) was to use .apply as follows:\n\ndef f(x):    \n    return x[\'ship_date\'] + timedelta(days=x[\'days_supply\'] )\n\ndf[\'ship_date\'] = pd.to_datetime(df[\'ship_string\'])\n\ndf[\'supply_ended\'] = df.apply(f,axis = 1)\n\n\nThat worked, but is exceedingly slow.  I\'ve posted my alternate solution below as an answer to the question, but I\'d like to get confirmation that it is "best practice".  I couldn\'t find many good threads on adding timedelta columns to dates in pandas (especially in a vectorized manner), so thought I\'d add one that is a little bit more user friendly and hopefully it will help the next poor soul trying to do this.\n'
'pandas.factorize encodes input values as an enumerated type or categorical variable. \n\nBut how can I easily and efficiently convert many columns of a data frame? What about the reverse mapping step?\n\nExample: This data frame contains columns with string values such as "type 2" which I would like to convert to numerical values - and possibly translate them back later.\n\n\n'
'I\'m trying to write a pandas DataFrame containing unicode to json, but the built in .to_json function escapes the characters. How do I fix this?\n\nExample:\n\nimport pandas as pd\ndf = pd.DataFrame([[\'τ\', \'a\', 1], [\'π\', \'b\', 2]])\ndf.to_json(\'df.json\')\n\n\nThis gives:\n\n{"0":{"0":"\\u03c4","1":"\\u03c0"},"1":{"0":"a","1":"b"},"2":{"0":1,"1":2}}\n\n\nWhich differs from the desired result:\n\n{"0":{"0":"τ","1":"π"},"1":{"0":"a","1":"b"},"2":{"0":1,"1":2}}\n\n\n\nI have tried adding the force_ascii=False argument:\n\nimport pandas as pd\ndf = pd.DataFrame([[\'τ\', \'a\', 1], [\'π\', \'b\', 2]])\ndf.to_json(\'df.json\', force_ascii=False)\n\n\nBut this gives the following error:\n\nUnicodeEncodeError: \'charmap\' codec can\'t encode character \'\\u03c4\' in position 11: character maps to &lt;undefined&gt;\n\n\n\nI\'m using WinPython 3.4.4.2 64bit with pandas 0.18.0\n'
'All four functions seem really similar to me. In some situations some of them might give the same result, some not. Any help will be thankfully appreciated!\n\nNow I know and I assume that internally, factorize and LabelEncoder work the same way and having no big differences in terms of results. I am not sure whether they will take up similar time with large magnitudes of data.\n\nget_dummies and OneHotEncoder will yield the same result but OneHotEncoder can only handle numbers but get_dummies will take all kinds of input. get_dummies will generate new column names automatically for each column input, but OneHotEncoder will not (it rather will assign new column names 1,2,3....). So get_dummies is better in all respectives.\n\nPlease correct me if I am wrong! Thank you!\n'
"When I am analyzing data, I save my dataframes into a csv-file and use pd.to_csv() for that. However, the function (over)writes the new file, without checking whether there exists one with the same name. Is there a way to check whether the file already exists, and if so, ask for a new filename?\n\nI know I can add the system's datetime to the filename, which will prevent any overwriting, but I would like to know when I made the mistake. \n"
"I have the following dataframe in pandas (python):\n\n      B.  X.  Y.  \nA\nalpha 3. 5.  5\nbeta  9. 9.  11\n\n\nI want to change 'alpha' for another name, like 'mu'. What should I do?\n"
"I have a dictionary that looks like the below\n\ndefaultdict(list,\n        {'Open': ['47.47', '47.46', '47.38', ...],\n         'Close': ['47.48', '47.45', '47.40', ...],\n         'Date': ['2016/11/22 07:00:00', '2016/11/22 06:59:00','2016/11/22 06:58:00', ...]})\n\n\nMy purpose is to convert this dictionary to a dataframe and to set the 'Date' key values as the index of the dataframe.\n\nI can do this job by the below commands\n\ndf = pd.DataFrame(dictionary, columns=['Date', 'Open', 'Close'])\n\n     0  Date                  Open    Close\n     1  2016/11/22 07:00:00   47.47   47.48\n     2  2016/11/22 06:59:00   47.46   47.45\n     3  2016/11/22 06:58:00   47.38   47.38\n\ndf.index = df.Date\n\n     Date                  Date                  Open    Close\n     2016/11/22 07:00:00   2016/11/22 07:00:00   47.47   47.48\n     2016/11/22 06:59:00   2016/11/22 06:59:00   47.46   47.45\n     2016/11/22 06:58:00   2016/11/22 06:58:00   47.38   47.38\n\n\nbut, then I have two 'Date' columns, one of which is the index and the other of which is the original column.\n\nIs there any way to set index while converting dictionary to dataframe, without having overlapping columns like the below?\n\n     Date                  Close       Open\n     2016/11/22 07:00:00   47.48       47.47\n     2016/11/22 06:59:00   47.45       47.46\n     2016/11/22 06:58:00   47.38       47.38\n\n\nThank you for reading this! :)\n"
'Problem Statement:\n\nI have some nice data in a pandas dataframe. I\'d like to run simple linear regression on it:\n\n\n\nUsing statsmodels, I perform my regression. Now, how do I get my plot? I\'ve tried statsmodels\' plot_fit method, but the plot is a little funky: \n\n\n\nI was hoping to get a horizontal line which represents the actual result of the regression. \n\nStatsmodels has a variety of methods for plotting regression (a few more details about them here) but none of them seem to be the super simple "just plot the regression line on top of your data" -- plot_fit seems to be the closest thing. \n\nQuestions:\n\n\nThe first picture above is from pandas\' plot function, which returns a matplotlib.axes._subplots.AxesSubplot. Can I overlay a regression line easily onto that plot? \nIs there a function in statsmodels I\'ve overlooked? \nIs there a better way to put together this figure? \n\n\nTwo related questions: \n\n\nPlotting Pandas OLS linear regression results\nGetting the regression line to plot from a Pandas regression\n\n\nNeither seems to have a good answer. \n\nSample data\n\nAs requested by @IgorRaush\n\n        motifScore  expression\n6870    1.401123    0.55\n10456   1.188554    -1.58\n12455   1.476361    -1.75\n18052   1.805736    0.13\n19725   1.110953    2.30\n30401   1.744645    -0.49\n30716   1.098253    -1.59\n30771   1.098253    -2.04\n\n\nabline_plot\n\nI had tried this, but it doesn\'t seem to work... not sure why:\n\n\n'
'I have been struggling with this question for a long while, and I tried different methods.\n\nI have a simple DataFrame as shown,\n\n\n\nI can use code to replace NaN with None (Not String "None"),\n\n[![dfTest2 = dfTest.where(pd.notnull(dfTest), None)][2]][2]\n\n\n\nI support  that NaT is also classified as \'Null\' because the following,\n \n\nHowever, NaT is not replaced with None.\n\nI have been searching for answers but got no luck. Anyone could Help?\n\nThank you in advance.\n'
"Is there a way to create a bar plot from continuous data binned into predefined intervals? For example, \n\nIn[1]: df\nOut[1]: \n0      0.729630\n1      0.699620\n2      0.710526\n3      0.000000\n4      0.831325\n5      0.945312\n6      0.665428\n7      0.871845\n8      0.848148\n9      0.262500\n10     0.694030\n11     0.503759\n12     0.985437\n13     0.576271\n14     0.819742\n15     0.957627\n16     0.814394\n17     0.944649\n18     0.911111\n19     0.113333\n20     0.585821\n21     0.930131\n22     0.347222\n23     0.000000\n24     0.987805\n25     0.950570\n26     0.341317\n27     0.192771\n28     0.320988\n29     0.513834\n\n231    0.342541\n232    0.866279\n233    0.900000\n234    0.615385\n235    0.880597\n236    0.620690\n237    0.984375\n238    0.171429\n239    0.792683\n240    0.344828\n241    0.288889\n242    0.961686\n243    0.094402\n244    0.960526\n245    1.000000\n246    0.166667\n247    0.373494\n248    0.000000\n249    0.839416\n250    0.862745\n251    0.589873\n252    0.983871\n253    0.751938\n254    0.000000\n255    0.594937\n256    0.259615\n257    0.459916\n258    0.935065\n259    0.969231\n260    0.755814\n\n\nand instead of a simple histogram:\n\ndf.hist()\n\n\n\n\nI need to create a bar plot, where each bar will count a number of instances within a predefined range. \nFor example, the following plot should have three bars with the number of points which fall into: [0 0.35], [0.35 0.7] [0.7 1.0] \n\nEDIT\n\nMany thanks for your answers. Another question, how to order bins?\nFor example, I get the following result:\n\nIn[349]: out.value_counts()\nOut[349]:  \n[0, 0.001]      104\n(0.001, 0.1]     61\n(0.1, 0.2]       32\n(0.2, 0.3]       20\n(0.3, 0.4]       18\n(0.7, 0.8]        6\n(0.4, 0.5]        6\n(0.5, 0.6]        5\n(0.6, 0.7]        4\n(0.9, 1]          3\n(0.8, 0.9]        2\n(1, 1.001]        0\n\n\nas you can see, the last three bins are not ordered. How to sort the data frame based on 'categories' or my bins?\n\nEDIT 2\n\nJust found how to solve it, simply with 'reindex()':\n\nIn[355]: out.value_counts().reindex(out.cat.categories)\nOut[355]: \n[0, 0.001]      104\n(0.001, 0.1]     61\n(0.1, 0.2]       32\n(0.2, 0.3]       20\n(0.3, 0.4]       18\n(0.4, 0.5]        6\n(0.5, 0.6]        5\n(0.6, 0.7]        4\n(0.7, 0.8]        6\n(0.8, 0.9]        2\n(0.9, 1]          3\n(1, 1.001]        0\n\n"
"I am trying to iterate over a pandas dataframe and update the value if condition is met but i am getting an error.\n\nfor line, row in enumerate(df.itertuples(), 1):\n    if row.Qty:\n        if row.Qty == 1 and row.Price == 10:\n            row.Buy = 1\nAttributeError: can't set attribute\n\n"
"The following code only shows the main category ['one', 'two', 'three', 'four', 'five', 'six'] as the x axis labels. Is there a way show subcategory ['A', 'B', 'C', 'D'] as secondary x axis labels?\n\n\ndf = pd.DataFrame(np.random.rand(6, 4),\n                 index=['one', 'two', 'three', 'four', 'five', 'six'],\n                 columns=pd.Index(['A', 'B', 'C', 'D'], \n                 name='Genus')).round(2)\n\n\ndf.plot(kind='bar',figsize=(10,4))\n\n"
'I have a series whose index is datetime that I wish to plot. I want to plot the values of the series on the y axis and the index of the series on the x axis. The Series looks as follows:\n\n2014-01-01     7\n2014-02-01     8\n2014-03-01     9\n2014-04-01     8\n...\n\n\nI generate a graph using plt.plot(series.index, series.values). But the graph looks like:\n\n\n\nThe problem is that I would like to have only year and month. However, the graph contains hours, minutes and seconds. How can I remove them so that I get my desired formatting?\n'
"I have a dataframe with a column of lists which can be created with:\n\nimport pandas as pd\nlists={1:[[1,2,12,6,'ABC']],2:[[1000,4,'z','a']]}\n#create test dataframe\ndf=pd.DataFrame.from_dict(lists,orient='index')\ndf=df.rename(columns={0:'lists'})\n\n\nThe dataframe df looks like:\n\n                lists\n1  [1, 2, 12, 6, ABC]\n2     [1000, 4, z, a]\n\n\nI need to create a new column called 'liststring' which takes every element of each list in lists and creates a string with each element separated by commas.  The elements of each list can be int, float, or string.  So the result would be:\n\n                lists    liststring\n1  [1, 2, 12, 6, ABC]  1,2,12,6,ABC\n2     [1000, 4, z, a]    1000,4,z,a\n\n\nI have tried various things, including from Converting a Panda DF List into a string:\n\ndf['liststring']=df.lists.apply(lambda x: ', '.join(str(x)))\n\n\nbut unfortunately the result takes every character and seperates by comma:\n\n                lists                                         liststring\n1  [1, 2, 12, 6, ABC]  [, 1, ,,  , 2, ,,  , 1, 2, ,,  , 6, ,,  , ', A...\n2     [1000, 4, z, a]  [, 1, 0, 0, 0, ,,  , 4, ,,  , ', z, ', ,,  , '...\n\n\nThanks in advance for the help!\n"
'Note: This question is not the same as an answer here: "Pandas: sample each group after groupby"\n\nTrying to figure out how to use pandas.DataFrame.sample or any other function to balance this data:\n\ndf[class].value_counts()\n\nc1    9170\nc2    5266\nc3    4523\nc4    2193\nc5    1956\nc6    1896\nc7    1580\nc8    1407\nc9    1324\n\n\nI need to get a random sample of each class (c1, c2, .. c9) where sample size is equal to the size of a class with min number of instances. In this example sample size should be the size of class c9 = 1324.\n\nAny simple way to do this with Pandas?\n\nUpdate\n\nTo clarify my question, in the table above :\n\nc1    9170\nc2    5266\nc3    4523\n...\n\n\nNumbers are counts of instances of c1,c2,c3,... classes, so actual data looks like this:\n\nc1 \'foo\'\nc2 \'bar\'\nc1 \'foo-2\'\nc1 \'foo-145\'\nc1 \'xxx-07\'\nc2 \'zzz\'\n...\n\n\netc.\n\nUpdate 2\n\nTo clarify more:\n\nd = {\'class\':[\'c1\',\'c2\',\'c1\',\'c1\',\'c2\',\'c1\',\'c1\',\'c2\',\'c3\',\'c3\'],\n     \'val\': [1,2,1,1,2,1,1,2,3,3]\n    }\n\ndf = pd.DataFrame(d)\n\n    class   val\n0   c1  1\n1   c2  2\n2   c1  1\n3   c1  1\n4   c2  2\n5   c1  1\n6   c1  1\n7   c2  2\n8   c3  3\n9   c3  3\n\ndf[\'class\'].value_counts()\n\nc1    5\nc2    3\nc3    2\nName: class, dtype: int64\n\ng = df.groupby(\'class\')\ng.apply(lambda x: x.sample(g.size().min()))\n\n        class   val\nclass           \nc1  6   c1  1\n    5   c1  1\nc2  4   c2  2  \n    1   c2  2\nc3  9   c3  3\n    8   c3  3\n\n\nLooks like this works. Main questions:\n\nHow g.apply(lambda x: x.sample(g.size().min())) works? I know what \'lambda` is, but: \n\n\nWhat is passed to lambda in x in this case? \nWhat is g in g.size()? \nWhy output contains  6,5,4, 1,8,9 numbers? What do they\nmean?\n\n'
"Scenario: I have a dataframe with multiple columns retrieved from excel worksheets. Some of these columns are dates: some have just the date (yyyy:mm:dd) and some have date and timestamp (yyyy:mm:dd 00.00.000000).\n\nQuestion: How can I remove the time stamp from the dates when they are not the index of my dataframe?\n\nWhat I already tried: From other posts here in SO (working with dates in pandas - remove unseen characters in datetime and convert to string and How to strip a pandas datetime of date, hours and seconds) I found:\n\npd.DatetimeIndex(dfST['timestamp']).date\n\n\nand\n\nstrfitme (df['timestamp'].apply(lambda x: x.strftime('%Y-%m-%d'))\n\n\nBut I can't seem to find a way to use those directly to the wanted column when it is not the index of my dataframe.\n"
"I have a dataframe pd. I would like to change a value of column irr depending on whether it is above or below a thresh hold.\n\nHow can  I do this in a single line? Now I have \n\npd['irr'] = pd['irr'][pd['cs']*0.63 &gt; pd['irr']] = 1.0\npd['irr'] = pd['irr'][pd['cs']*0.63 &lt;=  pd['irr']] = 0.0\n\n\nThe problem of course is that I change irr and check it again in the next line.\n\nIs there something like a ternary conditional operator for pandas?\n"
'My categorical variable case_satus takes on four unique values. I have data from 2014 to 2016. I would like to plot the distribution of case_status grouped by year. I try to this using:\n\ndf.groupby(\'year\').case_status.value_counts().plot.barh()\n\n\nAnd I get the following plot:\n\n\n\nWhat I would like to have is a nicer represenation. For example where I have one color for each year, and all the "DENIED" would stand next to each other.\n\nI think it can be achieved since the groupby object is a multi-index, but I don\'t understand it well enough to create the plot I want. \n\n\n\nThe solution is: \n\ndf.groupby(\'year\').case_status.value_counts().unstack(0).plot.barh()\n\n\nand results in \n\n\n'
'I am using python 3.6 and trying to download json file (350 MB) as pandas dataframe using the code below. However, I get the following error:\n\n\ndata_json_str = "[" + ",".join(data) + "]\n"TypeError: sequence item 0: expected str instance, bytes found\n\n\n\nHow can I fix the error?\n\nimport pandas as pd\n\n# read the entire file into a python array\nwith open(\'C:/Users/Alberto/nutrients.json\', \'rb\') as f:\n   data = f.readlines()\n\n# remove the trailing "\\n" from each line\ndata = map(lambda x: x.rstrip(), data)\n\n# each element of \'data\' is an individual JSON object.\n# i want to convert it into an *array* of JSON objects\n# which, in and of itself, is one large JSON object\n# basically... add square brackets to the beginning\n# and end, and have all the individual business JSON objects\n# separated by a comma\ndata_json_str = "[" + ",".join(data) + "]"\n\n# now, load it into pandas\ndata_df = pd.read_json(data_json_str)\n\n'
"I have a dataframe containing time series for 100 objects:\n\nobject  period  value \n1       1       24\n1       2       67\n...\n1       1000    56\n2       1       59\n2       2       46\n...\n2       1000    64\n3       1       54\n...\n100     1       451\n100     2       153\n...\n100     1000    21\n\n\nI want to calculate moving average with window 10 for the value column. I guess I have to do something like\n\ndf.groupby('object').apply(lambda ~calculate MA~) \n\n\nand then merge this Series to the original dataframe by object? Can't figure out exact commands\n"
'Using \n\ndd = {\'ID\': [\'H576\',\'H577\',\'H578\',\'H600\', \'H700\'],\n      \'CD\': [\'AAAAAAA\', \'BBBBB\', \'CCCCCC\',\'DDDDDD\', \'EEEEEEE\']}\ndf = pd.DataFrame(dd)\n\n\nPre Pandas 0.25, this below worked.  \n\nset:  redisConn.set("key", df.to_msgpack(compress=\'zlib\'))\nget:  pd.read_msgpack(redisConn.get("key"))\n\n\nNow, there are deprecated warnings.. \n\nFutureWarning: to_msgpack is deprecated and will be removed in a future version.\nIt is recommended to use pyarrow for on-the-wire transmission of pandas objects.\n\nThe read_msgpack is deprecated and will be removed in a future version.\nIt is recommended to use pyarrow for on-the-wire transmission of pandas objects.\n\n\nHow does pyarrow work? And, how do I get pyarrow objects into and back from Redis. \n\nreference:\nHow to set/get pandas.DataFrame to/from Redis?\n'
'Is there a grep like built-in function in Pandas to drop a row if it has some string or value?\nThanks in advance.\n'
"Given:\n\nser = Series(['one', 'two', 'three', 'two', 'two'])\n\n\nHow do I plot a basic histogram of these values?\n\nHere is an ASCII version of what I'd want to see in matplotlib:\n\n     X\n X   X   X\n-------------\none two three\n\n\nI'm tired of seeing:\n\nTypeError: cannot concatenate 'str' and 'float' objects\n\n"
"I've got a data frame and want to filter or bin by a range of values and then get the counts of values in each bin. \n\nCurrently, I'm doing this:\n\nx = 5\ny = 17\nz = 33\nfilter_values = [x, y, z]\nfiltered_a = df[df.filtercol &lt;= x]\na_count = filtered_a.filtercol.count()\n\nfiltered_b = df[df.filtercol &gt; x]\nfiltered_b = filtered_b[filtered_b &lt;= y]\nb_count = filtered_b.filtercol.count()\n\nfiltered_c = df[df.filtercol &gt; y]\nc_count = filtered_c.filtercol.count()\n\n\nBut is there a more concise way to accomplish the same thing?\n"
'I have a set of data from which I want to plot the number of keys per unique id count  (x=unique_id_count, y=key_count), and I\'m trying to learn how to take advantage of pandas.\n\nIn this case:\n\nunique_ids 1 = key count 2\n\nunique_ids 2 = key count 1\n\nfrom pandas import *\nkey_items = ("a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "c", "c", "c")\nid_data = ("X", "X", "X", "X", "X", "X", "X", "Y", "Y", "Y", "X", "X", "X")\n\ndf = DataFrame({\'keys\': key_items, \'ids\': id_data})\n\n\nI\'ve managed to mangle the data into what I want by pulling out the data from the dataframe and restructuring it, and rebuilding a new dataframe.  In this case it\'s probably better to do it all in python without pandas...\n\nunique_values = defaultdict(list)\nfor items in df.itertuples(index=False):\n    key = items[1]\n    v = items[0]\n    unique_values[key].append(v)\n\nunique_values_count = {}\nfor k, values in unique_values.iteritems():\n    unique_values_count[k] = [len(set(values))]\n\n# reformat for plotting\nkey_col = ("a", "b", "c")\nid_col = [unique_values_count[k][0] for k in key_col]\n\n\n\ndf2 = DataFrame({"keys":key_col, "unique_id_count": id_col})\ndf2.groupby("unique_id_count").size().plot(kind="bar")\n\n\nIs there a better way to do this more directly using the initial dataframe?\n'
'I want to drop m number of rows from the bottom of a data frame.  It is integer indexed (with holes).  How can this be done?  \n\npandas == 0.10.1\npython == 2.7.3\n'
"I am new to python and pandas, and have the following DataFrame. \n\nHow can I plot the DataFrame where each ModelID is a separate plot, saledate is the x-axis and MeanToDate is the y-axis?\n\nAttempt\n\ndata[40:76].groupby('ModelID').plot()\n\n\n\n\nDataFrame\n\n\n"
'I\'m trying to do some data work in Python pandas and having trouble writing out my results.\nI read my data in as a CSV file and been exporting each script as it\'s own CSV file which works fine. Lately though I\'ve tried exporting everything in 1 Excel file with worksheets and a few of the sheets give me an error \n\n"\'utf8\' codec can\'t decode byte 0xe9 in position 1: invalid continuation byte"\n\nI have no idea how to even start finding any characters that could be causing problems exporting to Excel. Not sure why it exports to CSV just fine though :(\n\nrelevant lines\n\nfrom pandas import ExcelWriter\ndata = pd.read_csv(input)\nwriter = ExcelWriter(output) #output is just the filename\nfundraisers.to_excel(writer, "fundraisers")\nlocations.to_excel(writer, "locations") #error\nlocations.to_csv(outputcsv) #works\nwriter.save()\n\n\nprinting head of offending dataframe\n\nEvent ID    Constituent ID  Email Address   First Name  \\   Last Name\nf       1       A       A       1\nF       4       L       R       C\nM       1       1       A       D\nF       4       A       A       G\nM       2       0       R       G\nM       3       O       O       H\nM       2       T       E       H\nM       2       A       A       H\nM       2       M       M       K\nF       3       J       E       K\nLocation ID raised  raised con  raised email\na   0   0   0\na   8   0   0\no   0   0   0\no   0   0   0\no   0   0   0\nt   5   0   0\no   1   0   0\no   6   a   0\no   6   0   0\nd   0   0   0\n\n\nlooking at the excel sheet I do actually get a partial print out. Anything in the first name column and beyond are blank, but event, constituent and email all print. \n\nedit: Trying to read the csv in as utf8 fails, but reading it in as latin1 works. Is there a way to specify the to_excel encoding? Or decode and encode my dataframe to utf8? \n'
"I am loading a csv file into a Pandas DataFrame. For each column, how do I specify what type of data it contains using the dtype argument?\n\n\nI can do it with numeric data (code at bottom)...\nBut how do I specify time data...\nand categorical data such as factors or booleans? I have tried np.bool_ and pd.tslib.Timestamp without luck.\n\n\nCode:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.read_csv(&lt;file-name&gt;, dtype={'A': np.int64, 'B': np.float64})\n\n"
"Is there a way to control grid format when doing pandas.DataFrame.plot()?\n\nSpecifically i would like to show the minor gridlines for plotting a DataFrame with a x-axis which has a DateTimeIndex.\n\nIs this possible through the DataFrame.plot()?\n\ndf = pd.DataFrame.from_csv(csv_file, parse_dates=True, sep=' ')\n\n"
'I am starting to learn Pandas, and I was following the question here and could not get the solution proposed to work for me and I get an indexing error. This is what I have \n\nfrom pandas import *\nimport pandas as pd\nd = {\'L1\' : Series([\'X\',\'X\',\'Z\',\'X\',\'Z\',\'Y\',\'Z\',\'Y\',\'Y\',]),\n     \'L2\' : Series([1,2,1,3,2,1,3,2,3]),\n     \'L3\' : Series([50,100,15,200,10,1,20,10,100])}\ndf = DataFrame(d)  \ndf.groupby(\'L1\', as_index=False).apply(lambda x : pd.expanding_sum(x.sort(\'L3\', ascending=False)[\'L3\'])/x[\'L3\'].sum())\n\n\nwhich outputs the following (I am using iPython)\n\nL1   \nX   3    0.571429\n    1    0.857143\n    0    1.000000\nY   8    0.900901\n    7    0.990991\n    5    1.000000\nZ   6    0.444444\n    2    0.777778\n    4    1.000000\ndtype: float64\n\n\nThen, I try to append the cumulative number calculation under the label "new" as suggested in the post\n\ndf["new"] = df.groupby("L1", as_index=False).apply(lambda x : pd.expanding_sum(x.sort("L3", ascending=False)["L3"])/x["L3"].sum())\n\n\nI get this:\n\n   2196                         value = value.reindex(self.index).values\n   2197                     except:\n-&gt; 2198                         raise TypeError(\'incompatible index of inserted column \'\n   2199                                         \'with frame index\')\n   2200 \nTypeError: incompatible index of inserted column with frame index\n\n\nDoes anybody knows what the problem is? How can I reinsert the calculated value into the dataframe so it shows the values in order (descending by "new" for each label X, Y, Z.)\n'
"If I use type on a DataFrame which I know has a datetime index, I get:\n\nIn [17]: type(df.index)\nOut[17]: pandas.tseries.index.DatetimeIndex\n\n\nbut when I test it, I get:\n\nIn [18]: type(df.index) == 'pandas.tseries.index.DatetimeIndex'\nOut[18]: False\n\n\nI know I assumed the type of type is a string, but I really don't know what else to try, and searching has resulted in nothing.\n"
"Is there an existing function to estimate fixed effect (one-way or two-way) from Pandas or Statsmodels.\n\nThere used to be a function in Statsmodels but it seems discontinued. And in Pandas, there is something called plm, but I can't import it or run it using pd.plm().\n"
"I need to find the quickest way to sort each row in a dataframe with millions of rows and around a hundred columns.\n\nSo something like this:\n\nA   B   C   D\n3   4   8   1\n9   2   7   2\n\n\nNeeds to become:\n\nA   B   C   D\n8   4   3   1\n9   7   2   2\n\n\nRight now I'm applying sort to each row and building up a new dataframe row by row. I'm also doing a couple of extra, less important things to each row (hence why I'm using pandas and not numpy). Could it be quicker to instead create a list of lists and then build the new dataframe at once? Or do I need to go cython?\n"
"This is my first question on stackoverflow. Go easy on me!\n\nI have two data sets acquired simultaneously by different acquisition systems with different sampling rates. One is very regular, and the other is not. I would like to create a single dataframe containing both data sets, using the regularly spaced timestamps (in seconds) as the reference for both. The irregularly sampled data should be interpolated on the regularly spaced timestamps.\n\nHere's some toy data demonstrating what I'm trying to do:\n\nimport pandas as pd\nimport numpy as np\n\n# evenly spaced times\nt1 = np.array([0,0.5,1.0,1.5,2.0])\ny1 = t1\n\n# unevenly spaced times\nt2 = np.array([0,0.34,1.01,1.4,1.6,1.7,2.01])\ny2 = 3*t2\n\ndf1 = pd.DataFrame(data={'y1':y1,'t':t1})\ndf2 = pd.DataFrame(data={'y2':y2,'t':t2})\n\n\ndf1 and df2 look like this:\n\ndf1:\n    t   y1\n0  0.0  0.0\n1  0.5  0.5\n2  1.0  1.0\n3  1.5  1.5\n4  2.0  2.0\n\ndf2:\n    t    y2\n0  0.00  0.00\n1  0.34  1.02\n2  1.01  3.03\n3  1.40  4.20\n4  1.60  4.80\n5  1.70  5.10\n6  2.01  6.03\n\n\nI'm trying to merge df1 and df2, interpolating y2 on df1.t. The desired result is:\n\ndf_combined:\n     t   y1   y2\n0  0.0  0.0  0.0\n1  0.5  0.5  1.5\n2  1.0  1.0  3.0\n3  1.5  1.5  4.5\n4  2.0  2.0  6.0\n\n\nI've been reading documentation for pandas.resample, as well as searching previous stackoverflow questions, but haven't been able to find a solution to my particular problem. Any ideas? Seems like it should be easy.\n\nUPDATE:\nI figured out one possible solution: interpolate the second series first, then append to the first data frame:\n\nfrom scipy.interpolate import interp1d\nf2 = interp1d(t2,y2,bounds_error=False)\ndf1['y2'] = f2(df1.t)\n\n\nwhich gives:\n\ndf1:\n    t   y1   y2\n0  0.0  0.0  0.0\n1  0.5  0.5  1.5\n2  1.0  1.0  3.0\n3  1.5  1.5  4.5\n4  2.0  2.0  6.0\n\n\nThat works, but I'm still open to other solutions if there's a better way.\n"
'This is more of a conceptual question, I do not have a specific problem.\n\nI am learning python for data analysis, but I am very familiar with R - one of the great things about R is plyr (and of course ggplot2) and even better dplyr. Pandas of course has split-apply as well however in R I can do things like (in dplyr, a bit different in plyr, and I can see now how dplyr mimics the . notation from object programming)\n\n   data %.% group_by(c(.....)) %.% summarise(new1 = ...., new2 = ...., ..... newn=....)\n\n\nin which I create multiple summary calculations at the same time\n\nHow do I do that in python, because \n\ndf[...].groupby(.....).sum() only sums columns, \n\n\nwhile on R I can have one mean, one sum, one special function, etc. on one call\n\nI realize I can do all my operations separately and merge them, and that is fine if I am using python, but when it comes down to choosing a tool, any line of code you do not have to type and check and validate adds up in time\n\nin addition, in dplyr you can also add mutate statements as well, so it seems to me it is way more powerful - so what am I missing about pandas or python - \n\nMy goal is to learn, I have spent a lot of effort to learn python and it is a worthy investment, but still the question remains\n'
'I have a column of a pandas dataframe that I got from a database query with blank cells. The blank cells become "None" and I want to check if each of the rows is None:         \n\nIn [325]: yes_records_sample[\'name\']\nOut[325]: \n41055    John J Murphy Professional Building\n25260                                   None\n41757             Armand Bayou Nature Center\n31397                                   None\n33104               Hubert Humphrey Building\n16891                         Williams Hall\n29618                                   None\n3770                          Covenant House\n39618                                   None\n1342       Bhathal Student Services Building\n20506                                   None\n\n\nMy understanding per the documentation is that I can check if each row is null with isnull() command http://pandas.pydata.org/pandas-docs/dev/missing_data.html#values-considered-missing\n\nThat function, however, is not working for me:  \n\nIn [332]: isnull(yes_records_sample[\'name\'])\n\n\nI get the following error:\n\nNameError Traceback (most recent call last)\n&lt;ipython-input-332-55873906e7e6&gt; in &lt;module&gt;()\n----&gt; 1 isnull(yes_records_sample[\'name\'])\nNameError: name \'isnull\' is not defined\n\n\nI also saw that someone just replaced the "None" strings, but neither of these variations on that approach worked for me:\nRename &quot;None&quot; value in Pandas\n\nyes_records_sample[\'name\'].replace(\'None\', "--no value--")\nyes_records_sample[\'name\'].replace(None, "--no value--")\n\n\nI was ultimately able to use the fillna function and fill each of those rows with an empty string yes_records_sample.fillna(\'\') as a workaround and then I could check yes_records_sample[\'name\']==\'\' But I am profoundly confused by how \'None\' works and what it means.  Is there a way to easily just check if a cell in a dataframe is \'None\'? \n'
'Suppose I have two DataFrames like so:\n\n&gt;&gt;dfA\n             S                      T            prob\n0        ! ! !                ! ! ! !   8.1623999e-05\n1        ! ! !                ! ! ! "   0.00354090007\n2        ! ! !                ! ! ! .   0.00210241997\n3        ! ! !                ! ! ! ?  6.55684998e-05\n4        ! ! !                  ! ! !     0.203119993\n5        ! ! !                ! ! ! ”  6.62070015e-05\n6        ! ! !                    ! !   0.00481862016\n7        ! ! !                      !    0.0274260994\n8        ! ! !                " ! ! !  7.99940026e-05\n9        ! ! !                    " !  1.51188997e-05\n10       ! ! !                      "  8.50678989e-05\n\n&gt;&gt;dfB\n             S                      T                                 knstats\n0        ! ! !                ! ! ! !                 knstats=2,391,104,64,25\n1        ! ! !                ! ! ! "                    knstats=4,391,6,64,2\n2        ! ! !                ! ! ! .                    knstats=4,391,5,64,2\n3        ! ! !                ! ! ! ?                    knstats=1,391,4,64,4\n4        ! ! !                  ! ! !               knstats=220,391,303,64,55\n5        ! ! !                    ! !               knstats=16,391,957,64,115\n6        ! ! !                      !              knstats=28,391,5659,64,932\n7        ! ! !                " ! ! !                    knstats=2,391,2,64,1\n8        ! ! !                    " !                  knstats=1,391,37,64,13\n9        ! ! !                      "     knstats=2,391,1.11721e+06,64,180642\n10       ! ! !                    . "           knstats=2,391,120527,64,20368\n\n\nI want to create a new DataFrame which is composed of the rows which have matching "S" and "T" entries in both matrices, along with the prob column from dfA and the knstats column from dfB. The result should look something like the following, and it is important that the order is the same:\n\n             S                      T            prob                             knstats\n0        ! ! !                ! ! ! !   8.1623999e-05             knstats=2,391,104,64,25\n1        ! ! !                ! ! ! "   0.00354090007                knstats=4,391,6,64,2\n2        ! ! !                ! ! ! .   0.00210241997                knstats=4,391,5,64,2\n3        ! ! !                ! ! ! ?  6.55684998e-05                knstats=1,391,4,64,4\n4        ! ! !                  ! ! !     0.203119993           knstats=220,391,303,64,55\n5        ! ! !                    ! !   0.00481862016           knstats=16,391,957,64,115\n6        ! ! !                      !    0.0274260994          knstats=28,391,5659,64,932\n7        ! ! !                " ! ! !  7.99940026e-05                knstats=2,391,2,64,1\n8        ! ! !                    " !  1.51188997e-05              knstats=1,391,37,64,13\n9        ! ! !                      "  8.50678989e-05 knstats=2,391,1.11721e+06,64,180642\n\n'
"I'm not sure of how to do this without chained assignments (which probably wouldn't work anyways because I'd be setting a copy).\n\nI wan't to take a subset of a multiindex pandas dataframe, test for values less than zero and set them to zero.\n\nFor example:\n\ndf = pd.DataFrame({('A','a'): [-1,-1,0,10,12],\n                   ('A','b'): [0,1,2,3,-1],\n                   ('B','a'): [-20,-10,0,10,20],\n                   ('B','b'): [-200,-100,0,100,200]})\n\ndf[df['A']&lt;0] = 0.0\n\n\ngives\n\nIn [37]:\n\ndf\n\nOut[37]:\n    A   B\n    a   b   a   b\n0   -1  0   -20 -200\n1   -1  1   -10 -100\n2   0   2   0   0\n3   10  3   10  100\n4   12  -1  20  200\n\n\nWhich shows that it was not able to set based on the condition. Alternatively if I did a chained assignment:\n\ndf.loc[:,'A'][df['A']&lt;0] = 0.0\n\n\nThis gives the same result (and setting with copy warning)\n\nI could loop through each column based on the condition that the first level is the one that I want:\n\nfor one,two in df.columns.values:\n    if one == 'A':\n        df.loc[df[(one,two)]&lt;0, (one,two)] = 0.0\n\n\nwhich gives the desired result:\n\nIn [64]:\n\ndf\n\nOut[64]:\n    A   B\n    a   b   a   b\n0   0   0   -20 -200\n1   0   1   -10 -100\n2   0   2   0   0\n3   10  3   10  100\n4   12  0   20  200\n\n\nBut somehow I feel there is a better way to do this than looping through the columns. What is the best way to do this in pandas?\n"
"When I'm fitting sklearn's LogisticRegression using a 1 column python pandas DataFrame (not a Series object), I get this warning:\n\n/Library/Python/2.7/site-packages/sklearn/preprocessing/label.py:125:         \nDataConversionWarning: A column-vector y was passed when a 1d array was \nexpected. Please change the shape of y to (n_samples, ), for example using \nravel().\ny = column_or_1d(y, warn=True)\n\n\nI know I could easily advert this warning in my code, but how can I turn off these warnings?\n"
"Getting the following error when trying to install Pandas (0.16.0), which is in my requirements.txt file, on AWS Elastic Beanstalk EC2 instance:\n\n  building 'pandas.msgpack' extension\n\n  gcc -pthread -fno-strict-aliasing -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -DNDEBUG -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector --param=ssp-buffer-size=4 -m64 -mtune=generic -D_GNU_SOURCE -fPIC -fwrapv -fPIC -D__LITTLE_ENDIAN__=1 -Ipandas/src/klib -Ipandas/src -I/opt/python/run/venv/local/lib/python2.7/site-packages/numpy/core/include -I/usr/include/python2.7 -c pandas/msgpack.cpp -o build/temp.linux-x86_64-2.7/pandas/msgpack.o\n\n  gcc: error trying to exec 'cc1plus': execvp: No such file or directory\n\n  error: command 'gcc' failed with exit status 1\n\n\nI'm running on 64bit Amazon Linux 2015.03 v1.3.0 running Python 2.7 and previously ran into this same error on a t1.micro instance, which was resolved when I change to a m3.medium, but I'm running an m3.xlarge so can't be a memory issue.\n\nI have also ensured that gcc is installed as a package in .ebextensions/00_gcc.config:\n\npackages:\n   yum:\n      gcc: []\n      gcc-c++: []\n\n"
"My question is simple, I have a dataframe and I groupby the results based on a column and get the size like this:\n\ndf.groupby('column').size()\n\n\nNow the problem is that I only want the ones where size is greater than X. I am wondering if I can do it using a lambda function or anything similar? I have already tried this:\n\ndf.groupby('column').size() &gt; X\n\n\nand it prints out some True and False values.\n"
'I am trying to compute a correlation matrix of several values.  These values include some \'nan\' values.  I\'m using numpy.corrcoef.  For element(i,j) of the output correlation matrix I\'d like to have the correlation calculated using all values that exist for both variable i and variable j.\n\nThis is what I have now:\n\nIn[20]: df_counties = pd.read_sql("SELECT Median_Age, Rpercent_2008, overall_LS, population_density FROM countyVotingSM2", db_eng)\nIn[21]: np.corrcoef(df_counties, rowvar = False)\nOut[21]: \narray([[ 1.        ,         nan,         nan, -0.10998411],\n       [        nan,         nan,         nan,         nan],\n       [        nan,         nan,         nan,         nan],\n       [-0.10998411,         nan,         nan,  1.        ]])\n\n\nToo many nan\'s :(\n'
"I am struggling to convert a comma separated list into a multi column (7) data-frame. \n\nprint (type(mylist))\n\n&lt;type 'list'&gt;\nPrint(mylist)\n\n\n['AN,2__AAS000,26,20150826113000,-283.000,20150826120000,-283.000',         'AN,2__AE000,26,20150826113000,0.000,20150826120000,0.000',.........\n\n\nThe following creates a frame of a single column:\n\ndf = pd.DataFrame(mylist)\n\n\nI have reviewed the inbuilt csv functionality for Pandas, however my csv data is held in a list. How can I simply covert the list into a 7 column data-frame.\n\nThanks in advance.\n"
'I\'m trying to create a basic scatter plot based on a Pandas dataframe. But when I call the scatter routine I get an error "TypeError: invalid type promotion". Sample code to reproduce the problem is shown below:\n\nt1 = pd.to_datetime(\'2015-11-01 00:00:00\')\nt2 = pd.to_datetime(\'2015-11-02 00:00:00\')\n\nTime = pd.Series([t1, t2])\nr = pd.Series([-1, 1])\n\ndf = pd.DataFrame({\'Time\': Time, \'Value\': r})\nprint(df)\n\nprint(type(df.Time))\nprint(type(df.Time[0]))\n\nfig = plt.figure(figsize=(x_size,y_size))\nax = fig.add_subplot(111)\nax.scatter(df.Time, y=df.Value, marker=\'o\')\n\n\nThe resulting output is\n\n        Time  Value\n0 2015-11-01     -1\n1 2015-11-02      1\n&lt;class \'pandas.core.series.Series\'&gt;\n&lt;class \'pandas.tslib.Timestamp\'&gt;\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-285-f4ed0443bf4d&gt; in &lt;module&gt;()\n     15 fig = plt.figure(figsize=(x_size,y_size))\n     16 ax = fig.add_subplot(111)\n---&gt; 17 ax.scatter(df.Time, y=df.Value, marker=\'o\')\n\nC:\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py in scatter(self, x,    y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, **kwargs)\n   3635             edgecolors = \'face\'\n   3636 \n-&gt; 3637         offsets = np.dstack((x, y))\n   3638 \n   3639         collection = mcoll.PathCollection(\n\nC:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py in dstack(tup)\n    365 \n    366     """\n--&gt; 367     return _nx.concatenate([atleast_3d(_m) for _m in tup], 2)\n    368 \n    369 def _replace_zero_by_x_arrays(sub_arys):\n\nTypeError: invalid type promotion\n\n\nSearching around I\'ve found a similar post Pandas Series TypeError and ValueError when using datetime which suggests that the error is caused by having multiple data types in the series. But that does not appear to be the issue in my example, as evidenced by the type information I\'m printing.\n\nNote that if I stop using pandas datetime objects and make the \'Time\' a float instead this works fine, e.g.\n\nt1 = 1.1 #\nt2 = 1.2\n\nTime = pd.Series([t1, t2])\nr = pd.Series([-1, 1])\n\ndf = pd.DataFrame({\'Time\': Time, \'Value\': r})\nprint(df)\n\nprint(type(df.Time))\nprint(type(df.Time[0]))\n\nfig = plt.figure(figsize=(x_size,y_size))\nax = fig.add_subplot(111)\nax.scatter(df.Time, y=df.Value, marker=\'o\')\n\n\nwith output\n\n   Time  Value\n0   1.1     -1\n1   1.2      1\n&lt;class \'pandas.core.series.Series\'&gt;\n&lt;class \'numpy.float64\'&gt;\n\n\nand the graph looks just fine. I\'m at a loss as to why the use of a datetime is causing the invalid type promotion error? I\'m using Python 3.4.3 and pandas 0.16.2. \n'
'I have CSV files which I read in in pandas with:\n\n#!/usr/bin/env python\n\nimport pandas as pd\nimport sys\n\nfilename = sys.argv[1]\ndf = pd.read_csv(filename)\n\n\nUnfortunately, the last line of these files is often corrupt (has the wrong number of commas).  Currently I open each file in a text editor and remove the last line.\n\nIs it possible to remove the last line in the same python/pandas script that loads the CSV to save having to take this extra non-automated step?\n'
'I can\'t find anything about cross join include the  merge/join or some other.\nI need deal with two dataframe using {my function} as  myfunc .\nthe equivalent of :\n\n{\n    for itemA in df1.iterrows():\n           for itemB in df2.iterrows():\n                       t["A"] = myfunc(itemA[1]["A"],itemB[1]["A"])\n }      \n\n\nthe equivalent of :\n\n{\n select myfunc(df1.A,df2.A),df1.A,df2.A from df1,df2;\n}\n\n\nbut I need more efficient solution:\nif used apply i will  be how to implement them  thx;^^\n'
"I am using the pandas.DataFrame.dropna method to drop rows that contain NaN. This function returns a dataframe that excludes the dropped rows, as shown in the documentation.\n\nHow can I store a copy of the dropped rows as a separate dataframe? Is:\n\nmydataframe[pd.isnull(['list', 'of', 'columns'])]\n\n\nalways guaranteed to return the same rows that dropna drops, assuming that dropna is called with subset=['list', 'of', 'columns'] ?\n"
'I have the following Pandas DataFrame object df. It is a train schedule listing the date of departure, scheduled time of departure, and train company.\n\nimport pandas as pd\ndf = \n\n            Year  Month DayofMonth  DayOfWeek  DepartureTime Train    Origin\nDatetime\n1988-01-01  1988    1     1         5        1457      BritishRail   Leeds\n1988-01-02  1988    1     2         6        1458      DeutscheBahn  Berlin\n1988-01-03  1988    1     3         7        1459      SNCF           Lyons\n1988-01-02  1988    1     2         6        1501      BritishRail   Ipswich\n1988-01-02  1988    1     2         6        1503      NMBS          Brussels\n....\n\n\nNow, let\'s say I wanted to select all items "DeutscheBahn" in the column "Train". \n\nI would use \n\nDB = df[df[\'Train\'] == \'DeutscheBahn\']\n\n\nNow, how can I select all trains except DeutscheBahn and British Rails and SNCF. How can I simultaneously choose the items not these? \n\nnotDB = df[df[\'Train\'] != \'DeutscheBahn\']\n\n\nand \n\nnotSNCF = df[df[\'Train\'] != \'SNCF\']\n\n\nbut I am not sure how to combine these into one command. \n\ndf[df[\'Train\'] != \'DeutscheBahn\', \'SNCF\']\n\n\ndoesn\'t work. \n'
'I\'m currently having an issue with Python. I have a Pandas DataFrame and one of the columns is a string with a date.\nThe format is :\n\n\n  "%Y-%m-%d %H:%m:00.000". For example : "2011-04-24 01:30:00.000"\n\n\nI need to convert the entire column to integers. I tried to run this code, but it is extremely slow and I have a few million rows.\n\n    for i in range(calls.shape[0]):\n        calls[\'dateint\'][i] = int(time.mktime(time.strptime(calls.DATE[i], "%Y-%m-%d %H:%M:00.000")))\n\n\nDo you guys know how to convert the whole column to epoch time ?\n\nThanks in advance !\n'
"I am using the data present here to construct this heat map using seaborn and pandas.\n\nCode:\n\n    import pandas\n    import seaborn.apionly as sns\n\n    # Read in csv file\n    df_trans = pandas.read_csv('LUH2_trans_matrix.csv')\n\n    sns.set(font_scale=0.8)\n    cmap = sns.cubehelix_palette(start=2.8, rot=.1, light=0.9, as_cmap=True)\n    cmap.set_under('gray')  # 0 values in activity matrix are shown in gray (inactive transitions)\n    df_trans = df_trans.set_index(['Unnamed: 0'])\n    ax = sns.heatmap(df_trans, cmap=cmap, linewidths=.5, linecolor='lightgray')\n\n    # X - Y axis labels\n    ax.set_ylabel('FROM')\n    ax.set_xlabel('TO')\n\n    # Rotate tick labels\n    locs, labels = plt.xticks()\n    plt.setp(labels, rotation=0)\n    locs, labels = plt.yticks()\n    plt.setp(labels, rotation=0)\n\n    # revert matplotlib params\n    sns.reset_orig()\n\n\nAs you can see from csv file, it contains 3 discrete values: 0, -1 and 1. I want a discrete legend instead of the colorbar. Labeling 0 as A, -1 as B and 1 as C. How can I do that?\n"
'So I have a dataframe with 5 columns. I would like to pull the indices where all of the columns are NaN. I was using this code:\n\nnan = pd.isnull(df.all)\n\n\nbut that is just returning false because it is logically saying no not all values in the dataframe are null. There are thousands of entries so I would prefer to not have to loop through and check each entry. Thanks!\n'
"I have a dataframe with a lot of columns in it. Now I want to select only certain columns. I have saved all the names of the columns that I want to select into a Python list and now I want to filter my dataframe according to this list. \n\nI've been trying to do:\n\ndf_new = df[[list]]\n\n\nwhere list includes all the column names that I want to select.\n\nHowever I get the error:\n\nTypeError: unhashable type: 'list'\n\n\nAny help on this one?\n"
'I am using pyspark on Jupyter notebook. Here is how Spark setup:\n\nimport findspark\nfindspark.init(spark_home=\'/home/edamame/spark/spark-2.0.0-bin-spark-2.0.0-bin-hadoop2.6-hive\', python_path=\'python2.7\')\n\n    import pyspark\n    from pyspark.sql import *\n\n    sc = pyspark.sql.SparkSession.builder.master("yarn-client").config("spark.executor.memory", "2g").config(\'spark.driver.memory\', \'1g\').config(\'spark.driver.cores\', \'4\').enableHiveSupport().getOrCreate()\n\n    sqlContext = SQLContext(sc)\n\n\nThen when I do:\n\nspark_df = sqlContext.createDataFrame(df_in)\n\n\nwhere df_in is a pandas dataframe. I then got the following errors:\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-9-1db231ce21c9&gt; in &lt;module&gt;()\n----&gt; 1 spark_df = sqlContext.createDataFrame(df_in)\n\n\n/home/edamame/spark/spark-2.0.0-bin-spark-2.0.0-bin-hadoop2.6-hive/python/pyspark/sql/context.pyc in createDataFrame(self, data, schema, samplingRatio)\n    297         Py4JJavaError: ...\n    298         """\n--&gt; 299         return self.sparkSession.createDataFrame(data, schema, samplingRatio)\n    300 \n    301     @since(1.3)\n\n/home/edamame/spark/spark-2.0.0-bin-spark-2.0.0-bin-hadoop2.6-hive/python/pyspark/sql/session.pyc in createDataFrame(self, data, schema, samplingRatio)\n    520             rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n    521         else:\n--&gt; 522             rdd, schema = self._createFromLocal(map(prepare, data), schema)\n    523         jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n    524         jdf = self._jsparkSession.applySchemaToPythonRDD(jrdd.rdd(), schema.json())\n\n/home/edamame/spark/spark-2.0.0-bin-spark-2.0.0-bin-hadoop2.6-hive/python/pyspark/sql/session.pyc in _createFromLocal(self, data, schema)\n    400         # convert python objects to sql data\n    401         data = [schema.toInternal(row) for row in data]\n--&gt; 402         return self._sc.parallelize(data), schema\n    403 \n    404     @since(2.0)\n\nAttributeError: \'SparkSession\' object has no attribute \'parallelize\'\n\n\nDoes anyone know what I did wrong? Thanks!\n'
'I am plotting time series using pandas .plot() and want to see every month shown as an x-tick. \n\nHere is the dataset structure\n\n\nHere is the result of the .plot()\n\n\n\nI was trying to use examples from other posts and matplotlib documentation and do something like\n\nax.xaxis.set_major_locator(\n   dates.MonthLocator(revenue_pivot.index, bymonthday=1,interval=1))\n\n\nBut that removed all the ticks :(\n\nI also tried to pass xticks = df.index, but it has not changed anything.\n\nWhat would be the rigth way to show more ticks on x-axis?\n'
"I am reading from two different CSVs each having date values in their columns. After read_csv I want to convert the data to datetime with the to_datetime method. The formats of the dates in each CSV are slightly different, and although the differences are noted and specified in the to_datetime format argument, the one converts fine, while the other returns the following value error.\n\nValueError: to assemble mappings requires at least that [year, month, day] be sp\necified: [day,month,year] is missing\n\n\nfirst dte.head()\n\n0  10/14/2016  10/17/2016  10/19/2016    8/9/2016  10/17/2016   7/20/2016\n1   7/15/2016   7/18/2016   7/20/2016    6/7/2016   7/18/2016   4/19/2016\n2   4/15/2016   4/14/2016   4/18/2016   3/15/2016   4/18/2016   1/14/2016\n3   1/15/2016   1/19/2016   1/19/2016  10/19/2015   1/19/2016  10/13/2015\n4  10/15/2015  10/14/2015  10/19/2015   7/23/2015  10/14/2015   7/15/2015\n\n\nthis dataframe converts fine using the following code:\n\ndte = pd.to_datetime(dte, infer_datetime_format=True)\n\n\nor \n\ndte = pd.to_datetime(dte[x], format='%m/%d/%Y')\n\n\nthe second dtd.head()\n\n0   2004-01-02 2004-01-02  2004-01-09 2004-01-16  2004-01-23  2004-01-30\n1   2004-01-05 2004-01-09  2004-01-16 2004-01-23  2004-01-30  2004-02-06\n2   2004-01-06 2004-01-09  2004-01-16 2004-01-23  2004-01-30  2004-02-06\n3   2004-01-07 2004-01-09  2004-01-16 2004-01-23  2004-01-30  2004-02-06\n4   2004-01-08 2004-01-09  2004-01-16 2004-01-23  2004-01-30  2004-02-06\n\n\nthis csv doesn't convert using either: \n\ndtd = pd.to_datetime(dtd, infer_datetime_format=True)\n\n\nor \n\ndtd = pd.to_datetime(dtd, format='%Y-%m-%d')\n\n\nIt returns the value error above. Interestingly, however, using the parse_dates and infer_datetime_format as arguments of the read_csv method work fine. What is going on here? \n"
'My question is about groupby operation with pandas. I have the following DataFrame :\n\nIn [4]: df = pd.DataFrame({"A": range(4), "B": ["PO", "PO", "PA", "PA"], "C": ["Est", "Est", "West", "West"]})\n\nIn [5]: df\nOut[5]: \n   A   B     C\n0  0  PO   Est\n1  1  PO   Est\n2  2  PA  West\n3  3  PA  West\n\n\nThis is what I would like to do : I want to group by column B and do a sum on column A. But at the end, I would like column C to still be in the DataFrame. If I do :\n\nIn [8]: df.groupby(by="B").aggregate(pd.np.sum)\nOut[8]: \n    A\nB    \nPA  5\nPO  1\n\n\nIt does the job but column C is missing. I can also do this :\n\nIn [9]: df.groupby(by=["B", "C"]).aggregate(pd.np.sum)\nOut[9]: \n         A\nB  C      \nPA West  5\nPO Est   1\n\n\nor \n\nIn [11]: df.groupby(by=["B", "C"], as_index=False).aggregate(pd.np.sum)\nOut[11]: \n    B     C  A\n0  PA  West  5\n1  PO   Est  1\n\n\nBut in both cases it group by B AND C and not just B and keeps the C value. Is what I want to do irrelevant or is there a way to do it ?\n'
"I'm reading in a pandas DataFrame using pd.read_csv. I want to keep the first row as data, however it keeps getting converted to column names.\n\n\nI tried header=False but this just deleted it entirely. \n\n\n(Note on my input data: I have a string (st = '\\n'.join(lst)) that I convert to a file-like object (io.StringIO(st)), then build the csv from that file object.)\n"
'I supposed that\n\ndata[data.agefm.isnull()]\n\n\nand \n\ndata[data.agefm == numpy.nan]\n\n\nare equivalent. But no, the first truly returns rows where agefm is NaN, but the second returns an empty DataFrame. I thank that omitted values are always equal to np.nan, but it seems wrong.\n\nagefm column has float64 type:\n\n(Pdb) data.agefm.describe()\ncount    2079.000000\nmean       20.686388\nstd         5.002383\nmin        10.000000\n25%        17.000000\n50%        20.000000\n75%        23.000000\nmax        46.000000\nName: agefm, dtype: float64\n\n\nCould you explain me please, what does data[data.agefm == np.nan] mean exactly?\n'
'I\'m trying to use the convenience of the plot method of a pandas dataframe while adjusting the size of the figure produced.  (I\'m saving the figures to file as well as displaying them inline in a Jupyter notebook).  I found the method below successful most of the time, except when I plot two lines on the same chart - then the figure goes back to the default size.\n\nI suspect this might be due to the differences between plot on a series and plot on a dataframe.\n\nSetup example code:\n\ndata = {\n    \'A\': 90 + np.random.randn(366),\n    \'B\': 85 + np.random.randn(366)\n}\n\ndate_range = pd.date_range(\'2016-01-01\', \'2016-12-31\')\n\nindex = pd.Index(date_range, name=\'Date\')\n\ndf = pd.DataFrame(data=data, index=index)\n\n\nControl - this code produces the expected result (a wide plot):\n\nfig = plt.figure(figsize=(10,4))\n\ndf[\'A\'].plot()\nplt.savefig("plot1.png")\nplt.show()\n\n\nResult:\n\n\n\nPlotting two lines - figure size is not (10,4)\n\nfig = plt.figure(figsize=(10,4))\n\ndf[[\'A\', \'B\']].plot()\nplt.savefig("plot2.png")\nplt.show()\n\n\nResult:\n\n\n\nWhat\'s the right way to do this so that the figure size is consistency set regardless of number of series selected?\n'
"I checked this post: finding non-numeric rows in dataframe in pandas? \nbut it doesn't really answer my question. \n\nmy sample data:\n\nimport pandas as pd\n\n\nd = {\n 'unit': ['UD', 'UD', 'UD', 'UD', 'UD','UD'],\n 'N-D': [ 'Q1', 'Q2', 'Q3', 'Q4','Q5','Q6'],\n 'num' : [ -1.48, 1.7, -6.18, 0.25, 'sum(d)', 0.25]\n\n}\ndf = pd.DataFrame(d)\n\n\nit looks like this:\n\n  N-D   num   unit\n0  Q1  -1.48   UD\n1  Q2   1.70   UD\n2  Q3  -6.18   UD\n3  Q4   0.25   UD\n4  Q5   sum(d) UD\n5  Q6   0.25   UD\n\n\nI want to filter out only the rows in column 'num' that are NON-NUMERIC. I want all of the columns for only the rows that contain non-numeric values for column 'num'.\n\ndesired output:\n\n  N-D   num   unit\n4  Q5   sum(d) UD\n\n\nmy attempts:\n\nnonnumeric=df[~df.applymap(np.isreal).all(1)] #didn't work, it pulled out everything, besides i want the condition to check only column 'num'. \n\nnonnumeric=df['num'][~df.applymap(np.isreal).all(1)] #didn't work, it pulled out all the rows for column 'num' only.\n\n"
"I am trying to create a pandas dataframe from an ordereddict to preserve the order of the values. But for some reason after creating the dataframe the fields are messed up again.\n\nHere's the list of ordereddicts:\n\n[OrderedDict([\n  ('key_a',\n  'value_a'),\n  ('key_b',\n  'value_b'),\n]),\nOrderedDict([\n  ('key_a',\n  'value_c'),\n  ('key_b',\n  'value_d'),\n])\n]\n\n\nNow how should I create a pandas DataFrame from these? What I am looking for is something like that (the important thing is the key_a and key_b etc column name order):\n\n  key_a    key_b\n0 value_a  value_b\n1 value_c  value_d\n\n\nI have tried:\n\npd.DataFrame.from_records(orderedDictList)\npd.DataFrame.from_dict(orderedDictList)\n\n\nFeel free to ask any additional questions.\n"
'I have the following dictionary:\n\nfillna(value={\'first_name\':\'Andrii\', \'last_name\':\'Furmanets\', \'created_at\':None})\n\nWhen I pass that dictionary to fillna I see:\n\n\n  raise ValueError(\'must specify a fill method or value\')\\nValueError: must specify a fill method or value\\n"\n\n\nIt seems to me that it fails on None value.\n\nI use pandas version 0.20.3.\n'
'I would like to convert \'bytes\' data into a Pandas dataframe. \n\nThe data looks like this (few first lines):\n\n    (b\'#Settlement Date,Settlement Period,CCGT,OIL,COAL,NUCLEAR,WIND,PS,NPSHYD,OCGT\'\n b\',OTHER,INTFR,INTIRL,INTNED,INTEW,BIOMASS\\n2017-01-01,1,7727,0,3815,7404,3\'\n b\'923,0,944,0,2123,948,296,856,238,\\n2017-01-01,2,8338,0,3815,7403,3658,16,\'\n b\'909,0,2124,998,298,874,288,\\n2017-01-01,3,7927,0,3801,7408,3925,0,864,0,2\'\n b\'122,998,298,816,286,\\n2017-01-01,4,6996,0,3803,7407,4393,0,863,0,2122,998\'\n\n\nThe columns headers appear at the top. each subsequent line is a timestamp and numbers.\n\nIs there a straightforward way to do this?\n\nThank you very much\n\n@Paula Livingstone:\n\nThis seems to work: \n\ns=str(bytes_data,\'utf-8\')\n\nfile = open("data.txt","w") \n\nfile.write(s)\ndf=pd.read_csv(\'data.txt\')\n\n\nmaybe this can be done without using a file in between.\n'
"I've heard in Pandas there's often multiple ways to do the same thing, but I was wondering – \n\nIf I'm trying to group data by a value within a specific column and count the number of items with that value, when does it make sense to use df.groupby('colA').count() and when does it make sense to use df['colA'].value_counts() ?\n"
"I have a GeoDataFrame of polygons (~30) and a GeoDataFrame of Points (~10k)\n\nI'm looking to create 30 new columns (with appropriate polygon names) in my GeoDataFrame of Points with a simple boolean True/False if the point is present in the polygon.\n\nAs an example, the GeoDataFrame of Polygons is this:\n\nid  geometry\nfoo POLYGON ((-0.18353,51.51022, -0.18421,51.50767, -0.18253,51.50744, -0.1794,51.50914))\nbar POLYGON ((-0.17003,51.50739, -0.16904,51.50604, -0.16488,51.50615, -0.1613,51.5091))\n\n\nThe GeoDataFrame of Points is like this:\n\ncounter     points\n   1     ((-0.17987,51.50974))\n   2     ((-0.16507,51.50925))\n\n\nExpected output:\n\ncounter          points        foo    bar\n   1    ((-0.17987,51.50974))  False  False\n   1    ((-0.16507,51.50925))  False  False\n\n\nI can do this manually by:\n\nfoo = df_poly.loc[df_poly.id=='foo']\ndf_points['foo'] = df_points['points'].map(lambda x: True if foo.contains(x).any()==True else False\n\n\nBut given that I have 30 polygons, I was wondering if there is a better way.\nAppreciate any help!\n"
"I've got a pandas DataFrame with a float (on decimal) index which I use to look up values (similar to a dictionary). As floats are not exactly the value they are supposed to be multiplied everything by 10 and converted it to integers .astype(int) before setting it as index. However this seems to do a floor instead of rounding. Thus 1.999999999999999992 is converted to 1 instead of 2. Rounding with the pandas.DataFrame.round() method before does not avoid this problem as the values are still stored as floats.\n\nThe original idea (which obviously rises a key error) was this:\n\nidx = np.arange(1,3,0.001)\ns = pd.Series(range(2000))\ns.index=idx\nprint(s[2.022])\n\n\ntrying with converting to integers:\n\nidx_int = idx*1000\nidx_int = idx_int.astype(int)\ns.index = idx_int\nfor i in range(1000,3000):\n    print(s[i])\n\n\nthe output is always a bit random as the 'real' value of an integer can be slightly above or below the wanted value. In this case the index contains two times the value 1000 and does not contain the value 2999.\n"
'Suppose I have the following:\n\ndf = pd.DataFrame({\'a\':range(2), \'b\':range(2), \'c\':range(2), \'d\':range(2)})\n\n\nI\'d like to "pop" two columns (\'c\' and \'d\') off the dataframe, into a new dataframe, leaving \'a\' and \'b\' behind in the original df. The following does not work:\n\ndf2 = df.pop([\'c\', \'d\'])\n\n\nHere\'s my error:\n\nTypeError: \'[\'c\', \'d\']\' is an invalid key\n\n\nDoes anyone know a quick, classy solution, besides doing the following?\n\ndf2 = df[[\'c\', \'d\']]\ndf3 = df[[\'a\', \'b\']]\n\n\nI know the above code is not that tedious to type, but this is why DataFrame.pop was invented--to save us a step when popping one column off a database.\n'
"I have a dask dataframe grouped by the index (first_name).\n\nimport pandas as pd\nimport numpy as np\n\nfrom multiprocessing import cpu_count\n\nfrom dask import dataframe as dd\nfrom dask.multiprocessing import get \nfrom dask.distributed import Client\n\n\nNCORES = cpu_count()\nclient = Client()\n\nentities = pd.DataFrame({'first_name':['Jake','John','Danae','Beatriz', 'Jacke', 'Jon'],'last_name': ['Del Toro', 'Foster', 'Smith', 'Patterson', 'Toro', 'Froster'], 'ID':['X','U','X','Y', '12','13']})\n\ndf = dd.from_pandas(entities, npartitions=NCORES)\ndf = client.persist(df.set_index('first_name'))\n\n\n(Obviously entities in the real life is several thousand rows)\n\nI want to apply a user defined function to each grouped dataframe. I want to compare each row with all the other rows in the group (something similar to Pandas compare each row with all rows in data frame and save results in list for each row).\n\nThe following is the function that I try to apply:\n\ndef contraster(x, DF):\n    matches = DF.apply(lambda row: fuzz.partial_ratio(row['last_name'], x) &gt;= 50, axis = 1) \n    return [i for i, x in enumerate(matches) if x]\n\n\nFor the test entities data frame, you could apply the function as usual:\n\nentities.apply(lambda row: contraster(row['last_name'], entities), axis =1)\n\n\nAnd the expected result is:\n\nOut[35]: \n0    [0, 4]\n1    [1, 5]\n2       [2]\n3       [3]\n4    [0, 4]\n5    [1, 5]\ndtype: object\n\n\nWhen entities is huge, the solution is use dask.  Note that DF in the contraster function must be the groupped dataframe.\n\nI am trying to use the following:\n\ndf.groupby('first_name').apply(func=contraster, args=????)\n\n\nBut How should I specify the grouped dataframe (i.e. DF in contraster?)\n"
'I am trying to read in the JSON structure below into pandas dataframe, but it throws out the error message: \n\n\n  ValueError: Mixing dicts with non-Series may lead to ambiguous\n  ordering.\n\n\nJson data:\n\n{\n    "status": {\n        "statuscode": 200,\n        "statusmessage": "Everything OK"\n    },\n\n    "result": [{\n        "id": 22,\n        "club_id": 16182\n    }, {\n        "id": 23,\n        "club_id": 16182\n    }, {\n        "id": 24,\n        "club_id": 16182\n    }, {\n        "id": 25,\n        "club_id": 16182\n    }, {\n        "id": 26,\n        "club_id": 16182\n    }, {\n        "id": 27,\n        "club_id": 16182\n    }]\n}\n\n\nHow do I get this right? I have tried the script below...\n\nj_df = pd.read_json(\'json_file.json\')\nj_df\n\nwith open(j_file) as jsonfile:\n    data = json.load(jsonfile)\n\n'
'For some reason, the following 2 calls to iloc / loc produce different behavior:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(dict(A=range(3), B=range(3)))\n&gt;&gt;&gt; df.iloc[:1]\n   A  B\n0  0  0\n&gt;&gt;&gt; df.loc[:1]\n   A  B\n0  0  0\n1  1  1\n\n\nI understand that loc considers the row labels, while iloc considers the integer-based indices of the rows. But why is the upper bound for the loc call considered inclusive, while the iloc bound is considered exclusive?\n'
'Column names are: ID,1,2,3,4,5,6,7,8,9.\n\nThe col values are either 0 or 1\n\nMy dataframe looks like this:\n\n ID     1    2    3    4    5    6   7   8   9 \n\n1002    0    1    0    1    0    0   0   0   0\n1003    0    0    0    0    0    0   0   0   0 \n1004    1    1    0    0    0    0   0   0   0\n1005    0    0    0    0    1    0   0   0   0\n1006    0    0    0    0    0    1   0   0   0\n1007    1    0    1    0    0    0   0   0   0\n1000    0    0    0    0    0    0   0   0   0\n1009    0    0    1    0    0    0   1   0   0\n\n\nI want the column names in front of the ID where the value in a row is 1.\n\nThe Dataframe i want should look like this:\n\n ID      Col2\n1002       2    // has 1 at Col(2) and Col(4)\n1002       4    \n1004       1    // has 1 at col(1) and col(2)\n1004       2\n1005       5    // has 1 at col(5)\n1006       6    // has 1 at col(6)\n1007       1    // has 1 at col(1) and col(3)\n1007       3\n1009       3    // has 1 at col(3) and col(7)\n1009       7\n\n\nPlease help me in this, Thanks in advance\n'
"I have a data frame that records responses of 19717 people's choice of programing languages through multiple choice questions. The first column is of course the gender of the respondent while the rest are the choices they picked. The data frame is shown below, with each response being recorded as the same name as column. If no response is selected, then this results in a nan.\nID     Gender              Python    Bash    R    JavaScript    C++\n0      Male                Python    nan     nan  JavaScript    nan\n1      Female              nan       nan     R    JavaScript    C++\n2      Prefer not to say   Python    Bash    nan  nan           nan\n3      Male                nan       nan     nan  nan           nan\n\nWhat I want is a table that returns the count based on Gender. Hence if 5000 men coded in Python and 3000 women in JS, then I should get this:\nGender              Python    Bash    R    JavaScript    C++\nMale                5000      1000    800  1500          1000\nFemale              4000      500     1500 3000          800\nPrefer Not To Say   2000      ...   ...    ...           860\n\nI have tried some of the options:\ndf.iloc[:, [*range(0, 13)]].stack().value_counts()\n\nMale                       16138\nPython                     12841\nSQL                         6532\nR                           4588\nFemale                      3212\nJava                        2267\nC++                         2256\nJavascript                  2174\nBash                        2037\nC                           1672\nMATLAB                      1516\nOther                       1148\nTypeScript                   389\nPrefer not to say            318\nNone                          83\nPrefer to self-describe       49\ndtype: int64\n\nAnd it's not what is required as described above. Can this be done in pandas?\n"
'Is there a fast way to do serialization of a DataFrame?\n\nI have a grid system which can run pandas analysis in parallel. In the end, I want to collect all the results (as a DataFrame) from each grid job and aggregate them into a giant DataFrame.\n\nHow can I save data frame in a binary format that can be loaded rapidly?\n'
"I'm using Pandas library for remote sensing time series analysis. Eventually I would like to save my DataFrame to csv by using chunk-sizes, but I run into a little issue. My code generates 6 NumPy arrays that I convert to Pandas Series. Each of these Series contains a lot of items\n\n&gt;&gt;&gt; prcpSeries.shape\n(12626172,)\n\n\nI would like to add the Series into a Pandas DataFrame (df) so I can save them chunk by chunk to a csv file.\n\nd = {'prcp': pd.Series(prcpSeries),\n     'tmax': pd.Series(tmaxSeries),\n     'tmin': pd.Series(tminSeries),\n     'ndvi': pd.Series(ndviSeries),\n     'lstm': pd.Series(lstmSeries),\n     'evtm': pd.Series(evtmSeries)}\n\ndf = pd.DataFrame(d)\noutFile ='F:/data/output/run1/_'+str(i)+'.out'\ndf.to_csv(outFile, header = False, chunksize = 1000)\nd = None\ndf = None\n\n\nBut my code get stuck at following line giving a Memory Error\n\ndf = pd.DataFrame(d)\n\n\nAny suggestions? Is it possible to fill the Pandas DataFrame chunk by chunk?\n"
"I have a pandas dataframe:\n\nimport pandas as pnd\nd = pnd.Timestamp('2013-01-01 16:00')\ndates = pnd.bdate_range(start=d, end = d+pnd.DateOffset(days=10), normalize = False)\n\ndf = pnd.DataFrame(index=dates, columns=['a'])\ndf['a'] = 6\n\nprint(df)\n                     a\n2013-01-01 16:00:00  6\n2013-01-02 16:00:00  6\n2013-01-03 16:00:00  6\n2013-01-04 16:00:00  6\n2013-01-07 16:00:00  6\n2013-01-08 16:00:00  6\n2013-01-09 16:00:00  6\n2013-01-10 16:00:00  6\n2013-01-11 16:00:00  6\n\n\nI am interested in find the label location of one of the labels, say,\n\nds = pnd.Timestamp('2013-01-02 16:00')\n\n\nLooking at the index values, I know that is integer location of this label 1. How can get pandas to tell what the integer value of this label is?\n"
"I have a python dictionary of user-item ratings that looks something like this:\n\nsample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0}, \n'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0}, \n'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}\n\n\nI was looking to convert it into a pandas data frame that would be structured like\n\n     col1   col2  col3\n0   user1  item1   2.5\n1   user1  item2   3.5\n2   user1  item3   3.0\n3   user1  item4   3.5\n4   user1  item5   2.5\n5   user1  item6   3.0\n6   user2  item1   2.5\n7   user2  item2   3.0\n8   user2  item3   3.5\n9   user2  item4   4.0\n10  user3  item2   4.5\n11  user3  item5   1.0\n12  user3  item6   4.0\n\n\nAny ideas would be much appreciated :)\n"
"For the Series object (let's call it s), pandas offers three types of addressing.\n\ns.iloc[] -- for integer position addressing;\n\ns.loc[] -- for index label addressing; and\n\ns.ix[] -- for a hybrid of integer position and label addressing.\n\nThe pandas object also performs ix addressing directly. \n\n# play data ...\nimport string\nidx = [i for i in string.uppercase] # A, B, C .. Z\nt = pd.Series(range(26), index=idx) # 0, 1, 2 .. 25\n\n# examples ...\nt[0]              # --&gt; 0\nt['A']            # --&gt; 0\nt[['A','M']]      # --&gt; [0, 12]\nt['A':'D']        # --&gt; [0, 1, 2, 3]\nt.iloc[25]        # --&gt; 25\nt.loc['Z']        # --&gt; 25\nt.loc[['A','Z']]  # --&gt; [0, 25]\nt.ix['A':'C']     # --&gt; [0, 1, 2]\nt.ix[0:2]         # --&gt; [0, 1]\n\n\nSo to my question: is there a point to the .ix method of indexing? Am I missing something important here?\n\nNote: As of Pandas v0.20, .ix indexer is deprecated in favour of .iloc / .loc.\n"
"In pandas, given a DataFrame D:\n\n+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  0  | apple  | banana | banana |\n|  1  | orange | orange | orange |\n|  2  | banana | apple  | orange |\n|  3  | NaN    | NaN    | NaN    |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n\n\nHow do I return rows that have the same contents across all of its columns when there are three columns or more such that it returns this:\n\n+-----+--------+--------+--------+   \n|     |    1   |    2   |    3   |\n+-----+--------+--------+--------+\n|  1  | orange | orange | orange |\n|  4  | apple  | apple  | apple  |\n+-----+--------+--------+--------+\n\n\nNote that it skips rows when all values are NaN.\n\nIf this were only two columns, I usually do D[D[1]==D[2]] but I don't know how to generalize this for more than 2 column DataFrames. \n"
'I want to delete all the rows in a dataframe.\n\nThe reason I want to do this is so that I can reconstruct the dataframe with an iterative loop. I want to start with a completely empty dataframe.\n\nAlternatively, I could create an empty df from just the column / type information if that is possible\n'
'I was wondering if there is an equivalent way to add a row to a Series or DataFrame with a MultiIndex as there is with a single index, i.e. using .ix or .loc?\n\nI thought the natural way would be something like\n\nrow_to_add = pd.MultiIndex.from_tuples()\ndf.ix[row_to_add] = my_row\n\n\nbut that raises a KeyError. I know I can use .append(), but I would find it much neater to use .ix[] or .loc[].\n\nhere an example:\n\n&gt;&gt;&gt; df = pd.DataFrame({\'Time\': [dt.datetime(2013,2,3,9,0,1), dt.datetime(2013,2,3,9,0,1)], \'hsec\': [1,25], \'vals\': [45,46]})\n&gt;&gt;&gt; df\n                 Time  hsec  vals\n0 2013-02-03 09:00:01     1    45\n1 2013-02-03 09:00:01    25    46\n\n[2 rows x 3 columns]\n&gt;&gt;&gt; df.set_index([\'Time\',\'hsec\'],inplace=True)\n&gt;&gt;&gt; ind = pd.MultiIndex.from_tuples([(dt.datetime(2013,2,3,9,0,2),0)],names=[\'Time\',\'hsec\'])\n&gt;&gt;&gt; df.ix[ind] = 5\n\nTraceback (most recent call last):\n  File "&lt;pyshell#201&gt;", line 1, in &lt;module&gt;\n    df.ix[ind] = 5\n  File "C:\\Program Files\\Python27\\lib\\site-packages\\pandas\\core\\indexing.py", line 96, in __setitem__\n    indexer = self._convert_to_indexer(key, is_setter=True)\n  File "C:\\Program Files\\Python27\\lib\\site-packages\\pandas\\core\\indexing.py", line 967, in _convert_to_indexer\n    raise KeyError(\'%s not in index\' % objarr[mask])\nKeyError: "[(Timestamp(\'2013-02-03 09:00:02\', tz=None), 0L)] not in index"\n\n'
"I know I can rename single pandas.DataFrame columns with:\n\ndrugInfo.rename(columns = {'col_1': 'col_1_new_name'}, inplace = True)\n\n\nBut I'd like to rename a column without knowing its name (based on its index - although I know dictionaries don't have it). I would like rename column number 1 like this:\n\ndrugInfo.rename(columns = {1: 'col_1_new_name'}, inplace = True)\n\n\nBut in the DataFrame.columns dict there is no '1' entry, so no renaming is done. How could I achieve this?\n"
'I have a simple csv file with ten columns!\n\nWhen I set the following option in the notebook and print my csv file (which is in a pandas dataframe) it doesn\'t print all the columns from left to right, it prints the first two, the next two underneath and so on.\n\nI used this option, why isn\'t it working?\n\npd.option_context("display.max_rows",1,"display.max_columns",100)\n\n\nEven this doesn\'t seem to work:\n\npandas.set_option(\'display.max_columns\', None)\n\n'
"I have a Dataframe with a pandas MultiIndex:\n\nIn [1]: import pandas as pd\nIn [2]: multi_index = pd.MultiIndex.from_product([['CAN','USA'],['total']],names=['country','sex'])\nIn [3]: df = pd.DataFrame({'pop':[35,318]},index=multi_index)\nIn [4]: df\nOut[4]:\n               pop\ncountry sex\nCAN     total   35\nUSA     total  318\n\n\nThen I remove some rows from that DataFrame:\n\nIn [5]: df = df.query('pop &gt; 100')\n\nIn [6]: df\nOut[6]:\n               pop\ncountry sex\nUSA     total  318\n\n\nBut when I consult the MutliIndex, it still has both countries in its levels.\n\nIn [7]: df.index.levels[0]\nOut[7]: Index([u'CAN', u'USA'], dtype='object')\n\n\nI can fix this myself in a rather strange way:\n\nIn [8]: idx_names = df.index.names\n\nIn [9]: df = df.reset_index(drop=False)\n\nIn [10]: df = df.set_index(idx_names)\n\nIn [11]: df\nOut[11]:\n               pop\ncountry sex\nUSA     total  318\n\nIn [12]: df.index.levels[0]\nOut[12]: Index([u'USA'], dtype='object')\n\n\nBut this seems rather messy. Is there a better way I'm missing?\n"
'I\'m looking for solutions to speed up a function I have written to loop through a pandas dataframe and compare column values between the current row and the previous row.\n\nAs an example, this is a simplified version of my problem:\n\n   User  Time                 Col1  newcol1  newcol2  newcol3  newcol4\n0     1     6     [cat, dog, goat]        0        0        0        0\n1     1     6         [cat, sheep]        0        0        0        0\n2     1    12        [sheep, goat]        0        0        0        0\n3     2     3          [cat, lion]        0        0        0        0\n4     2     5  [fish, goat, lemur]        0        0        0        0\n5     3     9           [cat, dog]        0        0        0        0\n6     4     4          [dog, goat]        0        0        0        0\n7     4    11                [cat]        0        0        0        0\n\n\nAt the moment I have a function which loops through and calculates values for \'newcol1\' and \'newcol2\' based on whether the \'User\' has changed since the previous row and also whether the difference in the \'Time\' values is greater than 1. It also looks at the first value in the arrays stored in \'Col1\' and \'Col2\' and updates \'newcol3\' and \'newcol4\' if these values have changed since the previous row.\n\nHere\'s the pseudo-code for what I\'m doing currently (since I\'ve simplified the problem I haven\'t tested this but it\'s pretty similar to what I\'m actually doing in ipython notebook):\n\n def myJFunc(df):\n...     #initialize jnum counter\n...     jnum = 0;\n...     #loop through each row of dataframe (not including the first/zeroeth)\n...     for i in range(1,len(df)):\n...             #has user changed?\n...             if df.User.loc[i] == df.User.loc[i-1]:\n...                     #has time increased by more than 1 (hour)?\n...                     if abs(df.Time.loc[i]-df.Time.loc[i-1])&gt;1:\n...                             #update new columns\n...                             df[\'newcol2\'].loc[i-1] = 1;\n...                             df[\'newcol1\'].loc[i] = 1;\n...                             #increase jnum\n...                             jnum += 1;\n...                     #has content changed?\n...                     if df.Col1.loc[i][0] != df.Col1.loc[i-1][0]:\n...                             #record this change\n...                             df[\'newcol4\'].loc[i-1] = [df.Col1.loc[i-1][0], df.Col2.loc[i][0]];\n...             #different user?\n...             elif df.User.loc[i] != df.User.loc[i-1]:\n...                     #update new columns\n...                     df[\'newcol1\'].loc[i] = 1; \n...                     df[\'newcol2\'].loc[i-1] = 1;\n...                     #store jnum elsewhere (code not included here) and reset jnum\n...                     jnum = 1;\n\n\nI now need to apply this function to several million rows and it\'s impossibly slow so I\'m trying to figure out the best way to speed it up. I\'ve heard that Cython can increase the speed of functions but I have no experience with it (and I\'m new to both pandas and python). Is it possible to pass two rows of a dataframe as arguments to the function and then use Cython to speed it up or would it be necessary to create new columns with "diff" values in them so that the function only reads from and writes to one row of the dataframe at a time, in order to benefit from using Cython?  Any other speed tricks would be greatly appreciated!\n\n(As regards using .loc, I compared .loc, .iloc and .ix and this one was marginally faster so that\'s the only reason I\'m using that currently)\n\n(Also, my User column in reality is unicode not int, which could be problematic for speedy comparisons)\n'
"I have a pandas DataFrame with indices I want to sort naturally.  Natsort doesn't seem to work.  Sorting the indices prior to building the DataFrame doesn't seem to help because the manipulations I do to the DataFrame seem to mess up the sorting in the process.  Any thoughts on how I can resort the indices naturally?\n\nfrom natsort import natsorted\nimport pandas as pd\n\n# An unsorted list of strings\na = ['0hr', '128hr', '72hr', '48hr', '96hr']\n# Sorted incorrectly\nb = sorted(a)\n# Naturally Sorted \nc = natsorted(a)\n\n# Use a as the index for a DataFrame\ndf = pd.DataFrame(index=a)\n# Sorted Incorrectly\ndf2 = df.sort()\n# Natsort doesn't seem to work\ndf3 = natsorted(df)\n\nprint(a)\nprint(b)\nprint(c)\nprint(df.index)\nprint(df2.index)\nprint(df3.index)\n\n"
"I have text reviews in one column in Pandas dataframe and I want to count the N-most frequent words with their frequency counts (in whole column - NOT in single cell). One approach is Counting the words using a counter, by iterating through each row. Is there a better alternative?\n\nRepresentative data.\n\n0    a heartening tale of small victories and endu\n1    no sophomore slump for director sam mendes  w\n2    if you are an actor who can relate to the sea\n3    it's this memory-as-identity obviation that g\n4    boyd's screenplay ( co-written with guardian\n\n"
"I am trying to use Cython to speed up a Pandas DataFrame computation which is relatively simple:  iterating over each row in the DataFrame, add that row to itself and to all remaining rows in the DataFrame, sum these across each row, and yield the list of these sums.  The length of these series will decrease as the rows in the DataFrame are exhausted.  These series are stored as a dictionary keyed on the index row number.\n\ndef foo(df):\n    vals = {i: (df.iloc[i, :] + df.iloc[i:, :]).sum(axis=1).values.tolist()\n            for i in range(df.shape[0])}   \n    return vals\n\n\nAside from adding %%cython at the top of this function, does anyone have a recommendation on how I'd go about using cdefs to convert the DataFrame values to doubles and then cythonize this code?\n\nBelow is some dummy data:\n\n&gt;&gt;&gt; df\n\n          A         B         C         D         E\n0 -0.326403  1.173797  1.667856 -1.087655  0.427145\n1 -0.797344  0.004362  1.499460  0.427453 -0.184672\n2 -1.764609  1.949906 -0.968558  0.407954  0.533869\n3  0.944205  0.158495 -1.049090 -0.897253  1.236081\n4 -2.086274  0.112697  0.934638 -1.337545  0.248608\n5 -0.356551 -1.275442  0.701503  1.073797 -0.008074\n6 -1.300254  1.474991  0.206862 -0.859361  0.115754\n7 -1.078605  0.157739  0.810672  0.468333 -0.851664\n8  0.900971  0.021618  0.173563 -0.562580 -2.087487\n9  2.155471 -0.605067  0.091478  0.242371  0.290887\n\n\nand expected output:\n\n&gt;&gt;&gt; foo(df)\n\n{0: [3.7094795101205236,\n  2.8039983729106,\n  2.013301815968468,\n  2.24717712931852,\n  -0.27313665495940964,\n  1.9899718844711711,\n  1.4927321304935717,\n  1.3612155622947018,\n  0.3008239883773878,\n  4.029880107986906],\n\n. . .\n\n 6: [-0.72401524913338,\n  -0.8555318173322499,\n  -1.9159233912495635,\n  1.813132728359954],\n 7: [-0.9870483855311194, -2.047439959448434, 1.6816161601610844],\n 8: [-3.107831533365748, 0.6212245862437702],\n 9: [4.350280705853288]}\n\n"
'It appears that the pandas read_csv function only allows single character delimiters/separators. Is there some way to allow for a string of characters to be used like, "*|*" or "%%" instead?\n'
"I have a pandas dataframe looking like this:\n\nName    start        end\nA       2000-01-10   1970-04-29\n\n\nI want to add a new column providing the difference between the start and end column in years, months, days.\n\nSo the result should look like:\n\nName    start        end          diff\nA       2000-01-10   1970-04-29   29y9m etc.\n\n\nthe diff column may also be a datetime object or a timedelta object, but the key point for me is, that I can easily get the Year and Month out of it.\n\nWhat I tried until now is:\n\ndf['diff'] = df['end'] - df['start']\n\n\nThis results in the new column containing 10848 days. However, I do not know how to convert the days to 29y9m etc.\n"
"I have a dataset in a relational database format (linked by ID's over various .csv files).\n\nI know that each data frame contains only one value of an ID, and I'd like to know the simplest way to extract values from that row.\n\nWhat I'm doing now:\n\n# the group has only one element\npurchase_group = purchase_groups.get_group(user_id)\nprice = list(purchase_group['Column_name'])[0]\n\n\nThe third row is bothering me as it seems ugly, however I'm not sure what is the workaround. The grouping (I guess) assumes that there might be multiple values and returns a &lt;class 'pandas.core.frame.DataFrame'&gt; object, while I'd like just a row returned.\n"
"I have a DataFrame with some time series. I created a correlation matrix from those time series and I'd like to create a hierarchical clustering on this correlation matrix. How can I do that? \n\n#\n# let't pretend this DataFrame contains some time series\n#\ndf = pd.DataFrame((np.random.randn(150)).reshape(10,15))\n\n         0         1         2               13           14    \n0  0.369746  0.093882 -0.656211 ....  -0.596936  0  0.095960  \n1  0.641457  1.120405 -0.468639 ....  -2.070802  1 -1.254159  \n2  0.360756 -0.222554  0.367893 ....   0.566299  2  0.932898  \n3  0.733130  0.666270 -0.624351 ....  -0.377017  3  0.340360  \n4 -0.263967  1.143818  0.554947 ....   0.220406  4 -0.585353  \n5  0.082964 -0.311667  1.323161 ....  -1.190672  5 -0.828039  \n6  0.173685  0.719818 -0.881854 ....  -1.048066  6 -1.388395  \n7  0.118301 -0.268945  0.909022 ....   0.094301  7  1.111376  \n8 -1.341381  0.599435 -0.318425 ....   1.053272  8 -0.763416  \n9 -1.146692  0.453125  0.150241 ....   0.454584  9  1.506249\n\n#\n# I can create a correlation matrix like this \n#\ncorrelation_matrix = df.corr(method='spearman')\n\n          0         1  ...          13         14 \n0   1.000000 -0.139394 ...    0.090909   0.309091 \n1  -0.139394  1.000000 ...   -0.636364   0.115152 \n2   0.175758  0.733333 ...   -0.515152  -0.163636 \n3   0.309091  0.163636 ...   -0.248485  -0.127273 \n4   0.600000 -0.103030 ...    0.151515   0.175758 \n5  -0.078788  0.054545 ...   -0.296970  -0.187879 \n6  -0.175758 -0.272727 ...    0.151515  -0.139394 \n7   0.163636 -0.042424 ...    0.187879   0.248485 \n8   0.030303  0.915152 ...   -0.430303   0.296970 \n9  -0.696970  0.321212 ...   -0.236364  -0.151515 \n10  0.163636  0.115152 ...   -0.163636   0.381818 \n11  0.321212 -0.236364 ...   -0.127273  -0.224242 \n12 -0.054545 -0.200000 ...    0.078788   0.236364 \n13  0.090909 -0.636364 ...    1.000000   0.381818 \n14  0.309091  0.115152 ...    0.381818   1.000000 \n\n\nNow, how can build the Hierarchical clustering on this matrix?\n"
'I have a pandas dataframe like the following\n\nidx, f1, f2, f3\n1,   a,  a,  b\n2,   b,  a,  c\n3,   a,  b,  c\n.\n.\n.\n87   e,  e,  e\n\n\nI need to convert the other columns to list of dictionaries based on idx column. so, final result should be:\n\nidx, features\n1 ,  [{f1:a, f2:a, f3:b}, {f1:b, f2:a, f3:c}, {f1:a, f2:b, f3:c}]\n.\n.\n.\n87,  [{f1: e, f2:e, f3:e}]\n\n\nIs it possible to do something like this using groupby in pandas?\n'
"I have a fairly large CSV file containing amazon review data which I read into a pandas data frame. I want to split the data 80-20(train-test) but while doing so I want to ensure that the split data is proportionally representing the values of one column (Categories), i.e all the different category of reviews are present both in train and test data proportionally.\n\nThe data looks like this:\n\n**ReviewerID**       **ReviewText**        **Categories**       **ProductId**\n\n1212                   good product         Mobile               14444425\n1233                   will buy again       drugs                324532\n5432                   not recomended       dvd                  789654123 \n\n\nIm using the following code to do so:\n\nimport pandas as pd\nMeta = pd.read_csv('C:\\\\Users\\\\xyz\\\\Desktop\\\\WM Project\\\\Joined.csv')\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\n\ntrain, test = train_test_split(Meta.categories, test_size = 0.2, stratify=y)\n\n\nit gives the following error\n\nNameError: name 'y' is not defined\n\n\nAs I'm relatively new to python I cant figure out what I'm doing wrong or whether this code will stratify based on column categories. It seems to work fine when i remove the stratify option as well as the categories column from train-test split.\n\nAny help will be appreciated.\n"
'I\'ve got a dataframe with the following information:\n\n    filename    val1    val2\nt                   \n1   file1.csv   5       10\n2   file1.csv   NaN     NaN\n3   file1.csv   15      20\n6   file2.csv   NaN     NaN\n7   file2.csv   10      20\n8   file2.csv   12      15\n\n\nI would like to interpolate the values in the dataframe based on the indices, but only within each file group.\n\nTo interpolate, I would normally do\n\ndf = df.interpolate(method="index")\n\n\nAnd to group, I do\n\ngrouped = df.groupby("filename")\n\n\nI would like the interpolated dataframe to look like this:\n\n    filename    val1    val2\nt                   \n1   file1.csv   5       10\n2   file1.csv   10      15\n3   file1.csv   15      20\n6   file2.csv   NaN     NaN\n7   file2.csv   10      20\n8   file2.csv   12      15\n\n\nWhere the NaN\'s are still present at t = 6 since they are the first items in the file2 group.\n\nI suspect I need to use "apply", but haven\'t been able to figure out exactly how...\n\ngrouped.apply(interp1d)\n...\nTypeError: __init__() takes at least 3 arguments (2 given)\n\n\nAny help would be appreciated.\n'
'I have a .csv file with three columns and many rows. I am trying to use pandas to read only the third column.\n\nright now I have:\n\nimport pandas as pd\n\npd.read_csv(r"C:\\test.csv",usecols=(3))\n\n'
'I have a df which contains my main data which has one million rows. My main data also has 30 columns. Now I want to add another column to my df called category. The category is a column in df2 which contains around 700 rows and two other columns that will match with two columns in df.\n\nI begin with setting an index in df2 and df that will match between the frames, however some of the index in df2 doesn\'t exist in df.\n\nThe remaining columns in df2 are called AUTHOR_NAME and CATEGORY.\n\nThe relevant column in df is called AUTHOR_NAME. \n\nSome of the AUTHOR_NAME in df doesn\'t exist in df2 and vice versa.\n\nThe instruction I want is: when index in df matches with index in df2 and title in df matches with title in df2, add category to df, else add NaN in category.\n\nExample data:\n\ndf2\n           AUTHOR_NAME              CATEGORY\nIndex       \nPub1        author1                 main\nPub2        author1                 main\nPub3        author1                 main\nPub1        author2                 sub\nPub3        author2                 sub\nPub2        author4                 sub\n\n\ndf\n            AUTHOR_NAME     ...n amount of other columns        \nIndex       \nPub1        author1                 \nPub2        author1     \nPub1        author2 \nPub1        author3\nPub2        author4 \n\nexpected_result\n            AUTHOR_NAME             CATEGORY   ...n amount of other columns\nIndex\nPub1        author1                 main\nPub2        author1                 main\nPub1        author2                 sub\nPub1        author3                 NaN\nPub2        author4                 sub\n\n\nIf I use df2.merge(df,left_index=True,right_index=True,how=\'left\', on=[\'AUTHOR_NAME\']) my df becomes three times bigger than it is supposed to be.\n\nSo I thought maybe merging was the wrong way to go about this. What I am really trying to do is use df2 as a lookup table and then return type values to df depending on if certain conditions are met.\n\ndef calculate_category(df2, d):\n    category_row = df2[(df2["Index"] == d["Index"]) &amp; (df2["AUTHOR_NAME"] == d["AUTHOR_NAME"])]\n    return str(category_row[\'CATEGORY\'].iat[0])\n\ndf.apply(lambda d: calculate_category(df2, d), axis=1)\n\n\nHowever, this throws me an error:\n\nIndexError: (\'index out of bounds\', u\'occurred at index 7614\')\n\n'
"I want to read a dbf file of an ArcGIS shapefile and dump it into a pandas dataframe. I am currently using the dbf package.\n\nI have apparently been able to load the dbf file as a Table, but have not been able to figure out how to parse it and turn it into a pandas dataframe. What is the way to do it?\n\nThis is where I am stuck at:\n\nimport dbf\nthisTable = dbf.Table('C:\\\\Users\\\\myfolder\\\\project\\\\myfile.dbf')\nthisTable.open(mode='read-only')\n\n\nPython returns this statement as output, which I frankly don't know what to make of:\n\ndbf.ver_2.Table('C:\\\\Users\\\\myfolder\\\\project\\\\myfile.dbf', status='read-only')\n\n\n\nEDIT\n\nSample of my original dbf:\n\nFID   Shape    E              N\n0     Point    90089.518711   -201738.245555\n1     Point    93961.324059   -200676.766517\n2     Point    97836.321204   -199614.270439\n...   ...      ...            ...\n\n"
'I am trying to make histograms for all columns of a dataframe through pandas 0.11.0 but the figure size is very small and hence the histograms are coming even smaller. We have the figsize property in 0.19.0 but how can we achieve the same in 0.11.0.\n'
'i have this plot of a dataframe with seaborn\'s facetgrid:\n\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport pandas\nimport numpy as np\n\nplt.figure()\ndf = pandas.DataFrame({"a": map(str, np.arange(1001, 1001 + 30)),\n                       "l": ["A"] * 15 + ["B"] * 15,\n                       "v": np.random.rand(30)})\ng = sns.FacetGrid(row="l", data=df)\ng.map(sns.pointplot, "a", "v")\nplt.show()\n\n\nseaborn plots all the xtick labels instead of just picking a few and it looks horrible:\n\n\n\nIs there a way to customize it so that it plots every n-th tick on x-axis instead of all of them?\n'
'Can I "store" instances of class in pandas/numpy Series-DataFrame/ndarray\njust like I do in list? Or these libraries support on built-in types (numerics, strings).\n\nFor example I have Point with x,y coordinates, and I want to store Points in Plane, that would return Point with given coordinates.\n\n#my class\nclass MyPoint:\n\n    def __init__(self, x,y):\n        self.x = x\n        self.y = y\n\n    @property\n    def x(self):\n        return self.x\n\n    @property\n    def y(self):\n        return self.y\n\n\nHere I create instances:\n\nfirst_point = MyClass(1,1)\nsecond_point = MyClass(2,2)\n\n\nI can store instances in some list\n\nmy_list = []\nmy_list.append(first_point)\nmy_list.append(second_point)\n\n\nThe problem in list is that it\'s indexes do not correspond to x,y properties.\n\nDictionary/DataFrame approach:\n\nPlane = {"x" : [first_point.x, second_point.x], "y" : [first_point.y, second_point.y], "some_reference/id_to_point_instance" = ???}\nPlane_pd = pd.DataFrame(Plane)\n\n\nI\'ve read posts, that using "id" of instance as third column value in DataFrame could cause problems with the garbage collector. \n'
'I have a dataframe:\n\n  a     b     c\n0 nan   Y     nan\n1  23   N      3\n2 nan   N      2\n3  44   Y     nan\n\n\nI wish to have this output:\n\n  a     b     c      d\n0 nan   Y     nan   nan\n1  23   N      3     96\n2 nan   N      2    nan\n3  44   Y     nan    44\n\n\nI wish to have a condition which is when column a is null, then d will be null else if column b is N and column c is not null then column d is equal to column a * column c else column d equal column a\n\nI have done this code but i get the error:\n\ndef f4(row):\n    if row[\'a\']==np.nan:\n       return np.nan\n    elif row[\'b\']=="N" &amp; row(row[\'c\'].notnull()):\n       return row[\'a\']*row[\'c\']\n    else:\n       return row[\'a\']\n\n DF[\'P1\']=DF.apply(f4,axis=1)\n\n\ncan anyone help me point out where is my mistake? I have refer to this and try this but also get the error Creating a new column based on if-elif-else condition\n'
'On Jupter Notebook, i was trying to compare time taken between the two methods for finding the index with max value.\n\n\n\nIn the Image, the first function took, 1000 loops, and the second took 10000 loops, is this increase in loops due to the method itself OR Jupyter Just added more loops to get more accurate time per loop even though the second function maybe took 1000 only, is that the case?\n'
'I would like to aggregate user transactions into lists in pandas. I can\'t figure out how to make a list comprised of more than one field. For example,\n\ndf = pd.DataFrame({\'user\':[1,1,2,2,3], \n                   \'time\':[20,10,11,18, 15], \n                   \'amount\':[10.99, 4.99, 2.99, 1.99, 10.99]})\n\n\nwhich looks like\n\n    amount  time  user\n0   10.99    20     1\n1    4.99    10     1\n2    2.99    11     2\n3    1.99    18     2\n4   10.99    15     3\n\n\nIf I do\n\nprint(df.groupby(\'user\')[\'time\'].apply(list))\n\n\nI get \n\nuser\n1    [20, 10]\n2    [11, 18]\n3        [15]\n\n\nbut if I do\n\ndf.groupby(\'user\')[[\'time\', \'amount\']].apply(list)\n\n\nI get\n\nuser\n1    [time, amount]\n2    [time, amount]\n3    [time, amount]\n\n\nThanks to an answer below, I learned I can do this\n\ndf.groupby(\'user\').agg(lambda x: x.tolist()))\n\n\nto get\n\n             amount      time\nuser                         \n1     [10.99, 4.99]  [20, 10]\n2      [2.99, 1.99]  [11, 18]\n3           [10.99]      [15]\n\n\nbut I\'m going to want to sort time and amounts in the same order - so I can go through each users transactions in order. \n\nI was looking for a way to produce this:\n\n             amount-time-tuple\nuser                         \n1     [(20, 10.99), (10, 4.99)]\n2     [(11,  2.99), (18, 1.99)]\n3     [(15, 10.99)]\n\n\nbut maybe there is a way to do the sort without "tupling" the two columns?\n'
"I have a dataframe and I would select only rows that contain index value into df1.index.\n\nfor Example:\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n2  4  5  0  2\n3  5  5  1  0\n22 1  3  9  6\n\n\nand these indexes\n\nIn[96]:df1.index\nOut[96]:\nInt64Index([  1,   3,   4,   5,   6,   7,  22,  28,  29,  32,], dtype='int64', length=253)\n\n\nI would like this output:\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\n1  1  4  9  1\n3  5  5  1  0\n22 1  3  9  6\n\n\nthanks\n"
"I have a pandas dataframe which looks like the following:\n\nName    Missed    Credit    Grade\nA       1         3         10\nA       1         1         12      \nB       2         3         10\nB       1         2         20\n\n\nAnd my desired output is:\n\nName    Sum1   Sum2    Average\nA       2      4      11\nB       3      5      15   \n\n\nBasically to get the sum of column Credit and Missed and to do average on Grade. What I am doing right now is two groupby on Name and then get sum and average and finally merge the two output dataframes which does not seem to be the best way of doing this. I have also found this on SO which makes sense if I want to work only on one column:\n\ndf.groupby('Name')['Credit'].agg(['sum','average'])\n\n\nBut not sure how to do a one-liner for both columns?   \n"
"What is the most efficient way to convert a geopandas geodataframe into a pandas dataframe?  Below is the method I use, is there another method which is more efficient or better in general at not generating errors?\n\nimport geopandas as gpd\nimport pandas as pd\n\n# assuming I have a shapefile named shp1.shp\ngdf1 = gpd.read_file('shp1.shp')\n\n# then for the conversion, I drop the last column (geometry) and specify the column names for the new df\ndf1 = pd.DataFrame(gdf1.iloc[:,:-1].values, columns = list(gdf1.columns.values)[:-1] )\n\n"
"Discription: both features are in categorical dtypes. and i used this code in a different kernal of  same \ndateset was working fine, the only difference is the features are in flote64. later i have converted these feature dtypes into Categorical\nbecause all the features in the dataset represents categories.\n\nBelow is the code:\n\nAM_train['product_category_2'].fillna('Unknown', inplace =True)\nAM_train['city_development_index'].fillna('Missing', inplace =True)\n\n"
'Consider\n\nnp.random.seed(0)\ns1 = pd.Series([1, 2, \'a\', \'b\', [1, 2, 3]])\ns2 = np.random.randn(len(s1))\ns3 = np.random.choice(list(\'abcd\'), len(s1))\n\n\ndf = pd.DataFrame({\'A\': s1, \'B\': s2, \'C\': s3})\ndf\n           A         B  C\n0          1  1.764052  a\n1          2  0.400157  d\n2          a  0.978738  c\n3          b  2.240893  a\n4  [1, 2, 3]  1.867558  a\n\n\nColumn "A" has mixed data types. I would like to come up with a really quick way of determining this. It would not be as simple as checking whether type == object, because that would identify "C" as a false positive.\n\nI can think of doing this with \n\ndf.applymap(type).nunique() &gt; 1\n\nA     True\nB    False\nC    False\ndtype: bool\n\n\nBut calling type atop applymap is pretty slow. Especially for larger frames.\n\n%timeit df.applymap(type).nunique() &gt; 1\n3.95 ms ± 88 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nCan we do better (perhaps with NumPy)? I can accept "No" if your argument is convincing enough. :-)\n'
"In a pandas DataFrame, I have a series of boolean values. In order to filter to rows where the boolean is True, I can use: df[df.column_x]\n\nI thought in order to filter to only rows where the column is False, I could use: df[~df.column_x]. I feel like I have done this before, and have seen it as the accepted answer. \n\nHowever, this fails because ~df.column_x converts the values to integers. See below. \n\nimport pandas as pd . # version 0.24.2\n\na = pd.Series(['a', 'a', 'a', 'a', 'b', 'a', 'b', 'b', 'b', 'b'])\nb = pd.Series([True, True, True, True, True, False, False, False, False, False], dtype=bool)\n\nc = pd.DataFrame(data=[a, b]).T\nc.columns = ['Classification', 'Boolean']```\n\nprint(~c.Boolean)\n\n0    -2\n1    -2\n2    -2\n3    -2\n4    -2\n5    -1\n6    -1\n7    -1\n8    -1\n9    -1\nName: Boolean, dtype: object\n\nprint(~b)\n\n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\n6     True\n7     True\n8     True\n9     True\ndtype: bool\n\n\n\nBasically, I can use c[~b], but not c[~c.Boolean]\n\nAm I just dreaming that this use to work?\n"
'There are so many posts like this about how to extract sklearn decision tree rules but I could not find any about using pandas.\n\nTake this data and model for example, as below \n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion="entropy", max_depth=3)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n\nThe result:\n\n\n\nExpected:\n\nThere\'re 8 rules about this example.\n\nFrom left to right,notice that dataframe is df\n\nr1 = (df[\'glucose\']&lt;=127.5) &amp; (df[\'bmi\']&lt;=26.45) &amp; (df[\'bmi\']&lt;=9.1)\n……\nr8 =  (df[\'glucose\']&gt;127.5) &amp; (df[\'bmi\']&gt;28.15) &amp; (df[\'glucose\']&gt;158.5)\n\n\nI\'m not a master of extracting sklearn decision tree rules. Getting the pandas boolean conditions will help me calculate samples and other metrics for each rule. So I want to extract each rule to a pandas boolean condition.\n'
"I would like to convert two arrays (x and y) into a frequency n x n matrix (n = 5), indicating each cell the number of point that contains. It consists on resampling both variables into five intervals and count the existing number of points per cell.\nI have tried using pandas pivot_table but don't know the way of referencing to each axis coordinate.\nX and Y arrays are two dependent variables that contain values between 0 and 100.\nI would really appreciate some one's aid.\nThank you very much in advance.\nThis is an example of the code:\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Arrays example. They are always float type and ranging 0-100. (n_size array = 15)\nx = 100 * np.random.random(15)\ny = 100 * np.random.random(15)\n\n# Df created for trying to pivot and counting values per cell\ndf = pd.DataFrame({'X':x,'Y':y})\n\n# Plot the example data:\ndf.plot(x = 'X',y = 'Y', style = 'o')\n\n\nThis is what I have:\n\nThis is the objetive matrix, saved as a df:\n\n"
"I'm using Python 2.7.3 in 64-bit. I installed pandas as well as matplotlib 1.1.1, both for 64-bit. Right now, none of my plots are showing. After attempting to plot from several different dataframes, I gave up in frustration and tried the following first example from http://pandas.pydata.org/pandas-docs/dev/visualization.html:\n\nINPUT:\n\nimport matplotlib.pyplot as plt\nts = Series(randn(1000), index=date_range ('1/1/2000', periods=1000))\nts = ts.cumsum()\nts.plot()\npylab.show()\n\n\nOUTPUT: \n\nAxes(0.125,0.1;0.775x0.8)\n\n\nAnd no plot window appeared. Other StackOverflow threads I've read suggested I might be missing DLLs. Any suggestions?\n"
'It looks to me like a bug in pandas.Series.\n\na = pd.Series([1,2,3,4])\nb = a.reshape(2,2)\nb\n\n\nb has type Series but can not be displayed, the last statement gives exception, very lengthy, the last line is "TypeError: %d format: a number is required, not numpy.ndarray". b.shape returns (2,2), which contradicts its type Series. I am guessing perhaps pandas.Series does not implement reshape function and I am calling the version from np.array? Anyone see this error as well? I am at pandas 0.9.1. \n'
"I have numeric data stored in two DataFrames x and y. The inner product from numpy works but the dot product from pandas does not.\n\nIn [63]: x.shape\nOut[63]: (1062, 36)\n\nIn [64]: y.shape\nOut[64]: (36, 36)\n\nIn [65]: np.inner(x, y).shape\nOut[65]: (1062L, 36L)\n\nIn [66]: x.dot(y)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-66-76c015be254b&gt; in &lt;module&gt;()\n----&gt; 1 x.dot(y)\n\nC:\\Programs\\WinPython-64bit-2.7.3.3\\python-2.7.3.amd64\\lib\\site-packages\\pandas\\core\\frame.pyc in dot(self, other)\n    888             if (len(common) &gt; len(self.columns) or\n    889                     len(common) &gt; len(other.index)):\n--&gt; 890                 raise ValueError('matrices are not aligned')\n    891 \n    892             left = self.reindex(columns=common, copy=False)\n\nValueError: matrices are not aligned\n\n\nIs this a bug or am I using pandas wrong?\n"
"After performing calculations on an entire pandas dataframe, I need to go back and override variable calculations (often setting to zero) based on the value of another variable(s). Is there a more succinct/idiomatic way to perform this kind of operation?\n\ndf['var1000'][df['type']==7] = 0\ndf['var1001'][df['type']==7] = 0\ndf['var1002'][df['type']==7] = 0\n...\ndf['var1099'][df['type']==7] = 0\n\n\nIs there a pandas-y way to do something like this?\n\nif (df['type']==7):\n    df['var1000'] = 0\n    df['var1001'] = 0\n    df['var1002'] = 0\n    ...\n    df['var1099'] = 0\n\n"
"I have a multi-index DataFrame created via a groupby operation.  I'm trying to do a compound sort using several levels of the index, but I can't seem to find a sort function that does what I need.\n\nInitial dataset looks something like this (daily sales counts of various products):\n\n         Date Manufacturer Product Name Product Launch Date  Sales\n0  2013-01-01        Apple         iPod          2001-10-23     12\n1  2013-01-01        Apple         iPad          2010-04-03     13\n2  2013-01-01      Samsung       Galaxy          2009-04-27     14\n3  2013-01-01      Samsung   Galaxy Tab          2010-09-02     15\n4  2013-01-02        Apple         iPod          2001-10-23     22\n5  2013-01-02        Apple         iPad          2010-04-03     17\n6  2013-01-02      Samsung       Galaxy          2009-04-27     10\n7  2013-01-02      Samsung   Galaxy Tab          2010-09-02      7\n\n\nI use groupby to get a sum over the date range:\n\n&gt; grouped = df.groupby(['Manufacturer', 'Product Name', 'Product Launch Date']).sum()\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPad         2010-04-03              30\n             iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n\n\nSo far so good!\n\nNow the last thing I want to do is sort each manufacturer's products by launch date, but keep them grouped hierarchically under Manufacturer - here's all I am trying to do:\n\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\n             iPad         2010-04-03              30\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n\n\nWhen I try sortlevel() I lose the nice per-company hierarchy I had before:\n\n&gt; grouped.sortlevel('Product Launch Date')\n                                               Sales\nManufacturer Product Name Product Launch Date       \nApple        iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\nApple        iPad         2010-04-03              30\nSamsung      Galaxy Tab   2010-09-02              22\n\n\nsort() and sort_index() just fail:\n\ngrouped.sort(['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n\ngrouped.sort_index(by=['Manufacturer','Product Launch Date'])\nKeyError: u'no item named Manufacturer'\n\n\nSeems like a simple operation, but I can't quite figure it out.\n\nI'm not tied to using a MultiIndex for this, but since that's what groupby() returns, that's what I've been working with.\n\nBTW the code to produce the initial DataFrame is:\n\ndata = {\n  'Date': ['2013-01-01', '2013-01-01', '2013-01-01', '2013-01-01', '2013-01-02', '2013-01-02', '2013-01-02', '2013-01-02'],\n  'Manufacturer' : ['Apple', 'Apple', 'Samsung', 'Samsung', 'Apple', 'Apple', 'Samsung', 'Samsung',],\n  'Product Name' : ['iPod', 'iPad', 'Galaxy', 'Galaxy Tab', 'iPod', 'iPad', 'Galaxy', 'Galaxy Tab'], \n  'Product Launch Date' : ['2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02','2001-10-23', '2010-04-03', '2009-04-27', '2010-09-02'],\n  'Sales' : [12, 13, 14, 15, 22, 17, 10, 7]\n}\ndf = DataFrame(data, columns=['Date', 'Manufacturer', 'Product Name', 'Product Launch Date', 'Sales'])\n\n"
"If I have a pandas.core.series.Series named ts of either 1's or NaN's like this:\n\n3382   NaN\n3381   NaN\n...\n3369   NaN\n3368   NaN\n...\n15     1\n10   NaN\n11     1\n12     1\n13     1\n9    NaN\n8    NaN\n7    NaN\n6    NaN\n3    NaN\n4      1\n5      1\n2    NaN\n1    NaN\n0    NaN\n\n\nI would like to calculate cumsum of this serie but it should be reset (set to zero) at the location of the NaNs like below:\n\n3382   0\n3381   0\n...\n3369   0\n3368   0\n...\n15     1\n10     0\n11     1\n12     2\n13     3\n9      0\n8      0\n7      0\n6      0\n3      0\n4      1\n5      2\n2      0\n1      0\n0      0\n\n\nIdeally I would like to have a vectorized solution !\n\nI ever see a similar question with Matlab : \nMatlab cumsum reset at NaN?\n\nbut I don't know how to translate this line d = diff([0 c(n)]); \n"
"dates seem to be a tricky thing in python, and I am having a lot of trouble simply stripping the date out of the pandas TimeStamp. I would like to get from 2013-09-29 02:34:44 to simply 09-29-2013\n\nI have a dataframe with a column Created_date:\n\nName: Created_Date, Length: 1162549, dtype: datetime64[ns]`\n\n\nI have tried applying the .date() method on this Series, eg: df.Created_Date.date(), but I get the error AttributeError: 'Series' object has no attribute 'date'\n\nCan someone help me out?\n"
'Suppose I have a time series:\n\nIn[138] rng = pd.date_range(\'1/10/2011\', periods=10, freq=\'D\')\nIn[139] ts = pd.Series(randn(len(rng)), index=rng)\nIn[140]\nOut[140]:\n2011-01-10    0\n2011-01-11    1\n2011-01-12    2\n2011-01-13    3\n2011-01-14    4\n2011-01-15    5\n2011-01-16    6\n2011-01-17    7\n2011-01-18    8\n2011-01-19    9\nFreq: D, dtype: int64\n\n\nIf I use one of the rolling_* functions, for instance rolling_sum, I can get the behavior I want for backward looking rolling calculations:\n\nIn [157]: pd.rolling_sum(ts, window=3, min_periods=0)\nOut[157]: \n2011-01-10     0\n2011-01-11     1\n2011-01-12     3\n2011-01-13     6\n2011-01-14     9\n2011-01-15    12\n2011-01-16    15\n2011-01-17    18\n2011-01-18    21\n2011-01-19    24\nFreq: D, dtype: float64\n\n\nBut what if I want to do a forward-looking sum? I\'ve tried something like this: \n\nIn [161]: pd.rolling_sum(ts.shift(-2, freq=\'D\'), window=3, min_periods=0)\nOut[161]: \n2011-01-08     0\n2011-01-09     1\n2011-01-10     3\n2011-01-11     6\n2011-01-12     9\n2011-01-13    12\n2011-01-14    15\n2011-01-15    18\n2011-01-16    21\n2011-01-17    24\nFreq: D, dtype: float64\n\n\nBut that\'s not exactly the behavior I want. What I am looking for as an output is:\n\n2011-01-10    3\n2011-01-11    6\n2011-01-12    9\n2011-01-13    12\n2011-01-14    15\n2011-01-15    18\n2011-01-16    21\n2011-01-17    24\n2011-01-18    17\n2011-01-19    9\n\n\nie - I want the sum of the "current" day plus the next two days. My current solution is not sufficient because I care about what happens at the edges. I know I could solve this manually by setting up two additional columns that are shifted by 1 and 2 days respectively and then summing the three columns, but there\'s got to be a more elegant solution. \n'
"I'd like to replace values in a Pandas DataFrame larger than an arbitrary number (100 in this case) with NaN (as values this large are indicative of a failed experiment). Previously I've used this to replace unwanted values:\n\nsve2_all[sve2_all[' Hgtot ng/l'] &gt; 100] = np.nan\n\n\nHowever, I got the following error:\n\n-c:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nC:\\Users\\AppData\\Local\\Enthought\\Canopy32\\User\\lib\\site-packages\\pandas\\core\\indexing.py:346: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\nself.obj[item] = s\n\n\nFrom this StackExchange question, it seems that sometimes this warning can be ignored, but I can't follow the discussion well enough to be certain whether this applies to my situation. Is the warning basically letting me know that I'll be overwriting some of the values in my DataFrame?\n\nEdit: As far as I can tell, everything behaved as it should. As a follow up is my method of replacing values non-standard? Is there a better way to replace values?\n"
"I am preparing a pandas df for output, and would like to remove the NaN and NaT in the table, and leave those table locations blank.  An example would be\n\nmydataframesample \n\ncol1    col2     timestamp\na       b        2014-08-14\nc       NaN      NaT\n\n\nwould become\n\ncol1    col2     timestamp\na       b        2014-08-14\nc       \n\n\nMost of the values are dtypes object, with the timestamp column being datetime64[ns].  In order to fix this, I attempted to use panda's mydataframesample.fillna(' ') to effectively leave a space in the location.  However, this doesn't work with the datetime types.  In order to get around this, I'm trying to convert the timestamp column back to object or string type.\n\nIs it possible to remove the NaN/NaT without doing the type conversion?  If not, how do I do the type conversion (tried str() and astype(str) but difficulty with datetime being the original format)?\n"
'What exactly happens when Pandas issues this warning? Should I worry about it?\n\nIn [1]: read_csv(path_to_my_file)\n/Users/josh/anaconda/envs/py3k/lib/python3.3/site-packages/pandas/io/parsers.py:1139: \nDtypeWarning: Columns (4,13,29,51,56,57,58,63,87,96) have mixed types. Specify dtype option on import or set low_memory=False.              \n\n  data = self._reader.read(nrows)\n\n\nI assume that this means that Pandas is unable to infer the type from values on those columns. But if that is the case, what type does Pandas end up using for those columns?  \n\nAlso, can the type always be recovered after the fact? (after getting the warning), or are there cases where I may not be able to recover the original info correctly, and I should pre-specify the type?\n\nFinally, how exactly does low_memory=False fix the problem?\n'
"I have a date time column in a Pandas DataFrame and I'd like to convert it to minutes or seconds. \n\nFor example: I want to convert 00:27:00 to 27 mins.\n\nexample = data['duration'][0]\nexample\n\n\nresult: numpy.timedelta64(1620000000000,'ns')\n\nWhat's the best way to achieve this?\n"
"I have a pandas dataframe. One of my columns should only be floats. When I try to convert that column to floats, I'm alerted that there are strings in there. I'd like to delete all rows where values in this column are strings... \n"
"What's the difference between:\n\npandas df.loc[:,('col_a','col_b')]\n\nand \n\ndf.loc[:,['col_a','col_b']]\n\nThe link below doesn't mention the latter, though it works. Do both pull a view? Does the first pull a view and the second pull a copy? Love learning Pandas.\n\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\nThanks\n"
"I have a series in datetime format, and need to change the day to 1 for each entry. I have thought of numerous simple solutions, but none of them works for me. For now, the only thing that actually works is\n\n\nset the series as the index\nQuery month and year from the index\nReconstruct a new time series using year, month and 1\n\n\nIt can't really be that complicated, can it? There is month start, but is unfortunately an offset, that's of no use here. There seems to be no set() function for the method, and even less functionality while the series is a column, and not (part of) the index itself. \n\nThe only related question was this, but the trick used there is not applicable here.\n"
'I have a multi class classification problem and my dataset is skewed, I have 100 instances of a particular class and say 10 of some different class, so I want to split my dataset keeping ratio between classes, if I have 100 instances of a particular class and I want 30% of records to go in the training set I want to have there 30 instances of my 100 record represented class and 3 instances of my 10 record represented class and so on.\n'
'An absolute basic read_csv question. \n\nI have data that looks like the following in a csv file -\n\nDate,Open Price,High Price,Low Price,Close Price,WAP,No.of Shares,No. of Trades,Total Turnover (Rs.),Deliverable Quantity,% Deli. Qty to Traded Qty,Spread High-Low,Spread Close-Open\n28-February-2015,2270.00,2310.00,2258.00,2294.85,2279.192067772602217319,73422,8043,167342840.00,11556,15.74,52.00,24.85\n27-February-2015,2267.25,2280.85,2258.00,2266.35,2269.239841485775122730,50721,4938,115098114.00,12297,24.24,22.85,-0.90\n26-February-2015,2314.90,2314.90,2250.00,2259.50,2277.198324862194860047,69845,8403,159050917.00,22046,31.56,64.90,-55.40\n25-February-2015,2290.00,2332.00,2278.35,2318.05,2315.100614216488163214,161995,10174,375034724.00,102972,63.56,53.65,28.05\n24-February-2015,2276.05,2295.00,2258.00,2278.15,2281.058946240263344242,52251,7726,119187611.00,13292,25.44,37.00,2.10\n23-February-2015,2303.95,2311.00,2253.25,2270.70,2281.912259219760108491,75951,7344,173313518.00,24969,32.88,57.75,-33.25\n20-February-2015,2324.00,2335.20,2277.00,2284.30,2301.631421152326354478,79717,10233,183479152.00,23045,28.91,58.20,-39.70\n19-February-2015,2304.00,2333.90,2292.00,2326.60,2321.485466301625211160,85835,8847,199264705.00,29728,34.63,41.90,22.60\n18-February-2015,2284.00,2305.00,2261.10,2295.75,2282.060986778089405300,69884,6639,159479550.00,26665,38.16,43.90,11.75\n16-February-2015,2281.00,2305.85,2266.00,2278.50,2284.961866239581019628,85541,10149,195457923.00,22164,25.91,39.85,-2.50\n13-February-2015,2311.00,2324.90,2286.95,2296.40,2311.371235111317676864,109731,5570,253629077.00,69039,62.92,37.95,-14.60\n12-February-2015,2280.00,2322.85,2275.00,2315.45,2301.372038211769425569,79766,9095,183571242.00,33981,42.60,47.85,35.45\n11-February-2015,2275.00,2295.00,2258.25,2287.20,2279.587966250020639664,60563,7467,138058686.00,20058,33.12,36.75,12.20\n10-February-2015,2244.90,2297.40,2225.00,2280.30,2269.562228214830293104,141656,13026,321497107.00,55577,39.23,72.40,35.40\n\n\n--\n\nI am trying to read this data in a pandas dataframe using the following variations of read_csv. I am only interested in two columns. \n\nz = pd.read_csv(\'file.csv\', parse_dates=True, index_col="Date", usecols=["Date", "Open Price", "Close Price"], names=["Date", "O", "C"], header=0)\n\n\nWhat I get is \n\n     O    C\n\nDate                \n2015-02-28  NaN  NaN\n2015-02-27  NaN  NaN\n2015-02-26  NaN  NaN\n2015-02-25  NaN  NaN\n2015-02-24  NaN  NaN\n\nOr \nz = pd.read_csv(\'file.csv\', parse_dates=True, index_col="Date", usecols=["Date", "Open", "Close"], names=["Date", "Open Price", "Close Price"], header=0)\n\n\nThe result is - \n\n    Open Price Close Price\nDate                             \n2015-02-28        NaN         NaN\n2015-02-27        NaN         NaN\n2015-02-26        NaN         NaN\n2015-02-25        NaN         NaN\n\n\nAm I missing something fundamental or is there an issue with read_csv of pandas 0.13.1 - my version on Debian Wheezy?\n'
'I\'m using Pandas to manipulate a csv file with several rows and columns that looks like the following\n\nFullname     Amount     Date           Zip    State .....\nJohn Joe        1        1/10/1900     55555    Confusion\nBetty White     5         .             .       Alaska \nBruce Wayne     10        .             .       Frustration\nJohn Joe        20        .             .       .\nBetty White     25        .             .       .\n\n\nI\'d like to create a new column entitled "Total" with a total sum of amount for each person. (Identified by fullname and zip). I\'m having difficulty in finding the correct solution.\n\nLet\'s just call my csv import csvfile. Here is what I have.\n\nimport Pandas\ndf = pandas.read_csv(\'csvfile.csv\', header = 0) \ndf.sort([\'fullname\'])\n\n\nI think I have to use the iterrows to do what I want as an object. The problem with dropping duplicates is that I will lose the amount or the amount may be different.\n'
"Let's say I have the following:\n\nIn [1]: import pandas as pd\n        import numpy as np\n        df = pd.DataFrame(data=np.random.rand(11),index=pd.date_range('2015-04-20','2015-04-30'),columns=['A'])\nOut[1]: \n               A\n2015-04-20  0.694983\n2015-04-21  0.393851\n2015-04-22  0.690138\n2015-04-23  0.674222\n2015-04-24  0.763175\n2015-04-25  0.761917\n2015-04-26  0.999274\n2015-04-27  0.907871\n2015-04-28  0.464818\n2015-04-29  0.005733\n2015-04-30  0.806351\n\n\nI have some complicated method that identifies a single index as being interesting, for example '2015-04-25'. I can retrieve the row with that index using:\n\nIn [2]: df.loc['2015-04-25']\nOut[2]: \nA    0.761917\nName: 2015-04-25 00:00:00, dtype: float64\n\n\nWhat would be the nicest way to obtain a number of n rows before and/or after that index value?\n\nWhat I would like to do is something like:\n\nIn[3]: df.getRowsBeforeLoc('2015-04-25',3)\nOut[3]:\n2015-04-22  0.690138\n2015-04-23  0.674222\n2015-04-24  0.763175\n2015-04-25  0.761917\n\n\nOr equivalently:\n\nIn[3]: df.getRowsAfterLoc('2015-04-25',3)\nOut[3]:\n2015-04-25  0.761917\n2015-04-26  0.999274\n2015-04-27  0.907871\n2015-04-28  0.464818\n\n\n(I don't have a strong opinion on whether or not the row that corresponds to the target index value itself is included.)\n"
'I have a Pandas DataFrame containing the date that a stream gage started measuring flow and the date that the station was decommissioned.  I want to generate a plot showing these dates graphically. Here is a sample of my DataFrame: \n\nindex       StationId                 amin                 amax\n40623  UTAHDWQ-5932100  1994-07-19 13:15:00  1998-06-30 14:51:00\n40637  UTAHDWQ-5932230  2006-03-16 13:55:00  2007-01-24 12:55:00\n40666  UTAHDWQ-5932240  1980-10-31 16:00:00  2007-07-31 11:35:00\n40697  UTAHDWQ-5932250  1981-06-11 17:45:00  1990-08-01 08:30:00\n40728  UTAHDWQ-5932253  2006-06-28 13:15:00  2007-01-24 13:35:00\n40735  UTAHDWQ-5932254  2006-06-28 13:55:00  2007-01-24 14:05:00\n40742  UTAHDWQ-5932280  1981-06-11 15:30:00  2006-08-22 16:00:00\n40773  UTAHDWQ-5932290  1992-06-10 15:45:00  1998-06-30 11:33:00\n40796  UTAHDWQ-5932750  2005-10-03 16:30:00  2005-10-22 15:00:00\n40819  UTAHDWQ-5983753  2006-04-25 09:56:00  2006-04-25 10:00:00\n40823  UTAHDWQ-5983754  2006-04-25 11:05:00  2008-04-08 12:16:00\n40845  UTAHDWQ-5983755  2006-04-25 13:50:00  2008-04-08 09:10:00\n40867  UTAHDWQ-5983756  2006-04-25 14:20:00  2008-04-08 09:30:00\n40887  UTAHDWQ-5983757  2006-04-25 12:45:00  2008-04-08 11:27:00\n40945  UTAHDWQ-5983759  2008-04-08 13:03:00  2008-04-08 13:05:00\n40964  UTAHDWQ-5983760  2008-04-08 13:15:00  2008-04-08 13:23:00\n40990  UTAHDWQ-5983775  2008-04-15 12:47:00  2009-04-07 13:15:00\n41040  UTAHDWQ-5989066  2005-10-04 10:15:00  2005-10-05 11:40:00\n41091  UTAHDWQ-5996780  1995-03-09 13:59:00  1996-03-14 10:40:00\n41100  UTAHDWQ-5996800  1995-03-09 15:13:00  1996-03-14 11:05:00\n\nI want to create a plot similar to this (please note that I did not make this plot using the above data):\n\n\nThe plot does not have to have the text shown along each line, just the y-axis with station names. \n\nWhile this may seem like a niche application of pandas, I know several scientists that would benefit from this plotting ability.\n\nThe closest answer I could find is here: \n\n\nHow to plot stacked proportional graph?\nHow to plot two columns of a pandas data frame using points?\nMatplotlib timelines\nCreate gantt Plot with python matplotlib\n\n\nThe last answer is closest to suiting my needs.\n\nWhile I would prefer a way to do it through the Pandas wrapper, I would be open and grateful to a straight matplotlib solution.\n'
'I have pandas dataframe that looks similar to this:\n\n                     TIMESTAMP  EVENT_COUNT\n0          2014-07-23 04:28:23            1 \n1          2014-07-23 04:28:24            1\n2   2014-07-23 04:28:25.999000            4\n3          2014-07-23 04:28:27            1\n4   2014-07-23 04:28:28.999000            2\n5          2014-07-23 04:28:30            1\n6          2014-07-23 04:29:31            7\n7          2014-07-23 04:29:33            1\n8          2014-07-23 04:29:34            1\n9          2014-07-23 04:29:36            1\n10         2014-07-23 04:40:37            2\n11         2014-07-23 04:40:39            1\n12         2014-07-23 04:40:40            1\n13         2014-07-23 04:40:42            1\n14         2014-07-23 04:40:43            1\n15  2014-07-23 04:40:44.999000            4\n16         2014-07-23 04:41:46            1\n17         2014-07-23 04:41:47            1\n18         2014-07-23 04:41:49            1\n19         2014-07-23 04:41:50            1\n20         2014-07-23 04:50:52            9\n21         2014-07-23 04:50:53            4\n22         2014-07-23 04:50:55            6\n23         2014-07-27 01:12:13            1\n\n\nMy end goal is to be able to find gaps in this that exceed 5 minutes. So, from above, I\'d find a gap between:\n\n2014-07-23 04:29:36 and 2014-07-23 04:40:37\n2014-07-23 04:41:50 and 2014-07-23 04:50:52\n2014-07-23 04:50:55 and 2014-07-27 01:12:13\n\n\nGaps of less than 5 minutes do not need to be identified. So the following wouldn\'t be recognized as a "gap".\n\n2014-07-23 04:28:30 and 2014-07-23 04:29:31    (Only 61 seconds)\n2014-07-23 04:40:37 and 2014-07-23 04:40:39    (Only 2 seconds)\n2014-07-23 04:40:44.999000 and 2014-07-23 04:41:46 (Just over 61 seconds)\n\n\nHow can I find the gaps mentioned above? When I tried the solution mentioned in this answer, nothing seems to have changed. I used the following command:\n\ndf.reindex(pd.date_range(min(df[\'TIMESTAMP\']), max(df[\'TIMESTAMP\']), freq=\'5min\')).fillna(0)\n\n\nThe dataframe looks the same after this command is run. \n'
"I have an API that returns a single row of data as a Python dictionary. Most of the keys have a single value, but some of the keys have values that are lists (or even lists-of-lists or lists-of-dictionaries).\nWhen I throw the dictionary into pd.DataFrame to try to convert it to a pandas DataFrame, it throws a &quot;Arrays must be the same length&quot; error. This is because it cannot process the keys which have multiple values (i.e. the keys which have values of lists).\nHow do I get pandas to treat the lists as 'single values'?\nAs a hypothetical example:\ndata = { 'building': 'White House', 'DC?': True,\n         'occupants': ['Barack', 'Michelle', 'Sasha', 'Malia'] }\n\nI want to turn it into a DataFrame like this:\nix   building         DC?      occupants\n0    'White House'    True     ['Barack', 'Michelle', 'Sasha', 'Malia']\n\n"
'How can I plot a Python Pandas multiindex dataframe as a bar chart with group labels? Do any of the plotting libraries directly support this? This SO post shows a custom solution using matplotlib, but is there direct support for it?\n\nAs an example:\n\nquarter  company\nQ1       Blue       100\n         Green      300\nQ2       Blue       200\n         Green      350\nQ3       Blue       300\n         Green      400\nQ4       Blue       400\n         Green      450\nName: count, dtype: int64\n\n\n...can this dataframe be plotted with group labels like this?\n\nThanks in advance,\n\nRafi\n'
'dataset = pd.read_csv("dataset.csv").fillna(" ")[:100]\ndataset[\'Id\']=0\ndataset[\'i\']=0\ndataset[\'j\']=0\n#...\nentries=dataset[dataset[\'Id\']==0]\nprint type(entries)  # Prints &lt;class \'pandas.core.frame.DataFrame\'&gt;\nentries=entries.sort_values([\'i\',\'j\',\'ColumnA\',\'ColumnB\'])\n\n\nWhat might be the possible reason of the following error message at the last line?:\n\nAttributeError: \'DataFrame\' object has no attribute \'sort_values\'\n\n'
'I have a CSV that looks like this:\n\ngene,stem1,stem2,stem3,b1,b2,b3,special_col\nfoo,20,10,11,23,22,79,3\nbar,17,13,505,12,13,88,1\nqui,17,13,5,12,13,88,3\n\n\nAnd as data frame it looks like this:\n\nIn [17]: import pandas as pd\nIn [20]: df = pd.read_table("http://dpaste.com/3PQV3FA.txt",sep=",")\nIn [21]: df\nOut[21]:\n  gene  stem1  stem2  stem3  b1  b2  b3  special_col\n0  foo     20     10     11  23  22  79            3\n1  bar     17     13    505  12  13  88            1\n2  qui     17     13      5  12  13  88            3\n\n\nWhat I want to do is to perform pearson correlation from last column (special_col) with every columns between gene column and special column, i.e. colnames[1:number_of_column-1]\n\nAt the end of the day we will have length 6 data frame.\n\nColn   PearCorr\nstem1  0.5\nstem2 -0.5\nstem3 -0.9999453506011533\nb1    0.5\nb2    0.5\nb3    -0.5\n\n\nThe above value is computed manually:\n\nIn [27]: import scipy.stats\nIn [39]: scipy.stats.pearsonr([3, 1, 3], [11,505,5])\nOut[39]: (-0.9999453506011533, 0.0066556395400007278)\n\n\nHow can I do that?\n'
"Python: 2.7.11\n\nDjango: 1.9\n\nPandas: 0.17.1\n\nHow should I go about creating a potentially large xlsx file download? I'm creating a xlsx file with pandas from a list of dictionaries and now need to give the user possibility to download it. The list is in a variable and is not allowed to be saved locally (on server).\n\nExample:\n\ndf = pandas.DataFrame(self.csvdict)\nwriter = pandas.ExcelWriter('pandas_simple.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1')\nwriter.save()\n\n\nThis example would just create the file and save it where the executing script is located. What I need is to create it to a http response so that the user would get a download prompt.\n\nI have found a few posts about doing this for a xlsxwriter but non for pandas. I also think that I should be using 'StreamingHttpResponse' for this and not a 'HttpResponse'.\n"
'Using Pandas DataFrame, let\'s say I have a bunch of columns in a csv file, and I want to be able to access any one of them via case insensitive name.\n\nimport pandas as pd\n\ndf = pd.read_csv(path_to_csv, delimiter=",")\n\ndf2 = df["Size"]\n\n\nThe actual column name is "Size". What can I do so that df2 = df["sIZE"] can also be accepted?\n'
'I am loading about 2 - 2.5 million records into a Postgres database every day.\n\nI then read this data with pd.read_sql to turn it into a dataframe and then I do some column manipulation and some minor merging. I am saving this modified data as a separate table for other people to use.\n\nWhen I do pd.to_sql it takes forever. If I save a csv file and use COPY FROM in Postgres, the whole thing only takes a few minutes but the server is on a separate machine and it is a pain to transfer files there.\n\nUsing psycopg2, it looks like I can use copy_expert to benefit from the bulk copying, but still use python. I want to, if possible, avoid writing an actual csv file. Can I do this in memory with a pandas dataframe?\n\nHere is an example of my pandas code. I would like to add the copy_expert or something to make saving this data much faster if possible.\n\n    for date in required_date_range:\n        df = pd.read_sql(sql=query, con=pg_engine, params={\'x\' : date})\n        ...\n        do stuff to the columns\n        ...\n        df.to_sql(\'table_name\', pg_engine, index=False, if_exists=\'append\',  dtype=final_table_dtypes)\n\n\nCan someone help me with example code? I would prefer to use pandas still and it would be nice to do it in memory. If not, I will just write a csv temporary file and do it that way.\n\nEdit- here is my final code which works. It only takes a couple of hundred seconds per date (millions of rows) instead of a couple of hours.\n\nto_sql = """COPY %s FROM STDIN WITH CSV HEADER"""\n\ndef process_file(conn, table_name, file_object):\n    fake_conn = cms_dtypes.pg_engine.raw_connection()\n    fake_cur = fake_conn.cursor()\n    fake_cur.copy_expert(sql=to_sql % table_name, file=file_object)\n    fake_conn.commit()\n    fake_cur.close()\n\n\n#after doing stuff to the dataframe\n    s_buf = io.StringIO()\n    df.to_csv(s_buf) \n    process_file(cms_dtypes.pg_engine, \'fact_cms_employee\', s_buf)\n\n'
"I have reweightTarget as follows and I want to convert it to a pandas Dataframe. However, I got following error:\n\n\n  TypeError: Index(...) must be called with a collection of some kind,\n  't' was passed\n\n\nIf I remove columns='t', it works fine. Can anyone please explain what's going on?\n\nreweightTarget\n\n\nTrading dates\n2004-01-31    4.35\n2004-02-29    4.46\n2004-03-31    4.44\n2004-04-30    4.39\n2004-05-31    4.50\n2004-06-30    4.53\n2004-07-31    4.63\n2004-08-31    4.58\ndtype: float64\npd.DataFrame(reweightTarget, columns='t')\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-334-bf438351aaf2&gt; in &lt;module&gt;()\n----&gt; 1 pd.DataFrame(reweightTarget, columns='t')\n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self, data, index, columns, dtype, copy)\n    253             else:\n    254                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n--&gt; 255                                          copy=copy)\n    256         elif isinstance(data, (list, types.GeneratorType)):\n    257             if isinstance(data, types.GeneratorType):\n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in _init_ndarray(self, values, index, columns, dtype, copy)\n    421                     raise_with_traceback(e)\n    422 \n--&gt; 423         index, columns = _get_axes(*values.shape)\n    424         values = values.T\n    425 \n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in _get_axes(N, K, index, columns)\n    388                 columns = _default_index(K)\n    389             else:\n--&gt; 390                 columns = _ensure_index(columns)\n    391             return index, columns\n    392 \n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py in _ensure_index(index_like, copy)\n   3407             index_like = copy(index_like)\n   3408 \n-&gt; 3409     return Index(index_like)\n   3410 \n   3411 \n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py in __new__(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\n    266                          **kwargs)\n    267         elif data is None or lib.isscalar(data):\n--&gt; 268             cls._scalar_data_error(data)\n    269         else:\n    270             if (tupleize_cols and isinstance(data, list) and data and\n\nC:\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py in _scalar_data_error(cls, data)\n    481         raise TypeError('{0}(...) must be called with a collection of some '\n    482                         'kind, {1} was passed'.format(cls.__name__,\n--&gt; 483                                                       repr(data)))\n    484 \n    485     @classmethod\n\nTypeError: Index(...) must be called with a collection of some kind, 't' was passed\n\n"
'I have a 2D Numpy array that I would like to put in a pandas Series (not a DataFrame):\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.zeros((5, 2))\n&gt;&gt;&gt; a\narray([[ 0.,  0.],\n       [ 0.,  0.],\n       [ 0.,  0.],\n       [ 0.,  0.],\n       [ 0.,  0.]])\n\n\nBut this throws an error:\n\n&gt;&gt;&gt; s = pd.Series(a)\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/miniconda/envs/pyspark/lib/python3.4/site-packages/pandas/core/series.py", line 227, in __init__\n    raise_cast_failure=True)\n  File "/miniconda/envs/pyspark/lib/python3.4/site-packages/pandas/core/series.py", line 2920, in _sanitize_array\n    raise Exception(\'Data must be 1-dimensional\')\nException: Data must be 1-dimensional\n\n\nIt is possible with a hack:\n\n&gt;&gt;&gt; s = pd.Series(map(lambda x:[x], a)).apply(lambda x:x[0])\n&gt;&gt;&gt; s\n0    [0.0, 0.0]\n1    [0.0, 0.0]\n2    [0.0, 0.0]\n3    [0.0, 0.0]\n4    [0.0, 0.0]\n\n\nIs there a better way?\n'
'I have many (4000+) CSVs of stock data (Date, Open, High, Low, Close) which I import into individual Pandas dataframes to perform analysis.  I am new to python and want to calculate a rolling 12month beta for each stock, I found a post to calculate rolling beta (Python pandas calculate rolling stock beta using rolling apply to groupby object in vectorized fashion) however when used in my code below takes over 2.5 hours!  Considering I can run the exact same calculations in SQL tables in under 3 minutes this is too slow.\n\nHow can I improve the performance of my below code to match that of SQL?  I understand Pandas/python has that capability. My current method loops over each row which I know slows performance but I am unaware of any aggregate way to perform a rolling window beta calculation on a dataframe.\n\nNote: the first 2 steps of loading the CSVs into individual dataframes and calculating daily returns only takes ~20seconds.  All my CSV dataframes are stored in the dictionary called \'FilesLoaded\' with names such as \'XAO\'.\n\nYour help would be much appreciated!\nThank you :)\n\nimport pandas as pd, numpy as np\nimport datetime\nimport ntpath\npd.set_option(\'precision\',10)  #Set the Decimal Point precision to DISPLAY\nstart_time=datetime.datetime.now()\n\nMarketIndex = \'XAO\'\nperiod = 250\nMinBetaPeriod = period\n# ***********************************************************************************************\n# CALC RETURNS \n# ***********************************************************************************************\nfor File in FilesLoaded:\n    FilesLoaded[File][\'Return\'] = FilesLoaded[File][\'Close\'].pct_change()\n# ***********************************************************************************************\n# CALC BETA\n# ***********************************************************************************************\ndef calc_beta(df):\n    np_array = df.values\n    m = np_array[:,0] # market returns are column zero from numpy array\n    s = np_array[:,1] # stock returns are column one from numpy array\n    covariance = np.cov(s,m) # Calculate covariance between stock and market\n    beta = covariance[0,1]/covariance[1,1]\n    return beta\n\n#Build Custom "Rolling_Apply" function\ndef rolling_apply(df, period, func, min_periods=None):\n    if min_periods is None:\n        min_periods = period\n    result = pd.Series(np.nan, index=df.index)\n    for i in range(1, len(df)+1):\n        sub_df = df.iloc[max(i-period, 0):i,:]\n        if len(sub_df) &gt;= min_periods:  \n            idx = sub_df.index[-1]\n            result[idx] = func(sub_df)\n    return result\n\n#Create empty BETA dataframe with same index as RETURNS dataframe\ndf_join = pd.DataFrame(index=FilesLoaded[MarketIndex].index)    \ndf_join[\'market\'] = FilesLoaded[MarketIndex][\'Return\']\ndf_join[\'stock\'] = np.nan\n\nfor File in FilesLoaded:\n    df_join[\'stock\'].update(FilesLoaded[File][\'Return\'])\n    df_join  = df_join.replace(np.inf, np.nan) #get rid of infinite values "inf" (SQL won\'t take "Inf")\n    df_join  = df_join.replace(-np.inf, np.nan)#get rid of infinite values "inf" (SQL won\'t take "Inf")\n    df_join  = df_join.fillna(0) #get rid of the NaNs in the return data\n    FilesLoaded[File][\'Beta\'] = rolling_apply(df_join[[\'market\',\'stock\']], period, calc_beta, min_periods = MinBetaPeriod)\n\n# ***********************************************************************************************\n# CLEAN-UP\n# ***********************************************************************************************\nprint(\'Run-time: {0}\'.format(datetime.datetime.now() - start_time))\n\n'
"I'm reading data from a database (50k+ rows) where one column is stored as JSON. I want to extract that into a pandas dataframe. \nThe snippet below works fine but is fairly inefficient and really takes forever when run against the whole db.\nNote that not all the items have the same attributes and that the JSON have some nested attributes.\n\nHow could I make this faster?\n\nimport pandas as pd\nimport json\n\ndf = pd.read_csv('http://pastebin.com/raw/7L86m9R2', \\\n                 header=None, index_col=0, names=['data'])\n\ndf.data.apply(json.loads) \\\n       .apply(pd.io.json.json_normalize)\\\n       .pipe(lambda x: pd.concat(x.values))\n###this returns a dataframe where each JSON key is a column\n\n"
'I have a dataframe which contains the characters ((( I would like to replace. But I get error after doing this:\n\ndata = [{\'Title\': \'set1((("a", "b", "c")))\'},\n     {\'Title\': \'set2((("d", "e", "f")))\'},\n     {\'Title\': \'set3((("g", "h", "i")))\'},\n     {\'Title\': \'set4((("j", "k", "l")))\'},\n     {\'Title\': \'set5((("m", "n", "o")))\'},\n     {\'Title\': \'set6((("p", "q", "r")))\'}]\n\ndf = pd.DataFrame(data)\ndf\n\n# df[\'Title\'] = df[\'Title\'].str.replace(\'set\', \'M\') # Works correctly\ndf[\'Title\'] = df[\'Title\'].str.replace(\'(((\', \'&gt;&gt;\') # Not working\n\n\nHow do I solve this error in order to to replace ((( by &gt;&gt; and ))) by &lt;&lt;?\n'
'I have a pandas dataframe defined as:\n\nA   B   SUM_C      \n1   1   10     \n1   2   20   \n\n\nI would like to do a cumulative sum of SUM_C and add it as a new column to the same dataframe. In other words, my end goal is to have a dataframe that looks like below:\n\nA   B   SUM_C   CUMSUM_C       \n1   1   10      10     \n1   2   20      30   \n\n\nUsing cumsum in pandas on group() shows the possibility of generating a new dataframe where column name SUM_C is replaced with cumulative sum. However, my ask is to add the cumulative sum as a new column to the existing dataframe.\n\nThank you\n'
"I am using the following code to convert .xlsx files into .csv files.  \n\nimport pandas as pd\ndata_xls = pd.read_excel('excelfile.xlsx', 'Sheet2', index_col=None)\ndata_xls.to_csv('csvfile.csv', encoding='utf-8')\n\n\nThe code is working, however I am getting an index column with the cell numbers which I do not want.  Is there anyway to not include or remove that index column?  \n\nFile output\n\n Unnamed  Data\n    0   0.99319613\n    1   0.99319613\n    2   0.99319613\n    3   0.99319613\n    4   0.99319613\n    5   0.99319613\n\n"
"I created a dataframe using the following:\n\ndf = pd.DataFrame(np.random.rand(10, 3), columns=['alp1', 'alp2', 'bet1'])\n\n\nI'd like to get a dataframe containing every columns from df that have alp in their names. This is only a light version of my problem, so my real dataframe will have more columns.\n"
"I have a Pandas dataframe that looks like the below:\n\n                   codes\n1                  [71020]\n2                  [77085]\n3                  [36415]\n4                  [99213, 99287]\n5                  [99233, 99233, 99233]\n\n\nI'm trying to split the lists in df['codes'] into columns, like the below:\n\n                   code_1      code_2      code_3   \n1                  71020\n2                  77085\n3                  36415\n4                  99213       99287\n5                  99233       99233       99233\n\n\nwhere columns that don't have a value (because the list was not that long) are filled with blanks or NaNs or something.\n\nI've seen answers like this one and others similar to it, and while they work on lists of equal length, they all throw errors when I try to use the methods on lists of unequal length. Is there a good way do to this?\n"
'I have two dataframes:\n\n df1 = row1;row2;row3\n df2 = row4;row5;row6;row2\n\n\nI want my output dataframe to only contain the rows unique in df1, i.e.:\n\ndf_out = row1;row3\n\n\nHow do I get this most efficiently?\n\nThis code does what I want, but using 2 for-loops:\n\na = pd.DataFrame({0:[1,2,3],1:[10,20,30]})\nb = pd.DataFrame({0:[0,1,2,3],1:[0,1,20,3]})\n\nmatch_ident = []\nfor i in range(0,len(a)):\n    found=False\n    for j in range(0,len(b)):\n        if a[0][i]==b[0][j]:\n            if a[1][i]==b[1][j]:\n                found=True\n    match_ident.append(not(found))\n\na = a[match_ident]\n\n'
'I want to delete to just column name (x,y,z)\nuse only data\n\nIn [68]: df\nOut[68]: \n   x  y  z\n0  1  0  1  \n1  2  0  0 \n2  2  1  1 \n3  2  0  1 \n4  2  1  0\n\n\ni want to print result to same as below.\n\nOut[68]: \n\n0  1  0  1  \n1  2  0  0 \n2  2  1  1 \n3  2  0  1 \n4  2  1  0\n\n\nis it possible?\nhow i can do this?\n'
'I had obtained the index of training set and testing set with code below.\n\ndf = pandas.read_pickle(filepath + filename)\nkf = KFold(n_splits = n_splits, shuffle = shuffle, random_state = \nrandomState)\n\nresult = next(kf.split(df), None)\n\n#train can be accessed with result[0]\n#test can be accessed with result[1]\n\n\nI wonder if there is any faster way to separate them into 2 dataframe respectively with the row indexes I retrieved.\n'
"I have a DataFrame and I would like to drop the last column of it. Until now I had just been dropping what I believed to be the final column with \n\nif len(fish_frame.columns) == 4:\n    del fish_frame[3]. \n\nRight before this command, however, I drop all columns of NaNs. So that removes column [3] because it is filled with NaNs, so it fails. \n\nI would like to say just drop the final column of the entire DataFrame. I feel like that would work perfectly. \n\nI tried fish_frame([:-1], axis=1) but that's invalid syntax.\n\nAny help would be appreciated thanks. \n\nThe DataFrame:\n\nfish_frame after dropna:\n\n                              0        1      2           4\n0                         #0721      NaN    NaN         NaN\n1                       GBE COD      746  $2.00   $1,492.00\n2                       GBW COD   13,894  $0.50   $6,947.00\n3                       GOM COD       60  $2.00     $120.00\n4            GB WINTER FLOUNDER   94,158  $0.25  $23,539.50\n5           GOM WINTER FLOUNDER    3,030  $0.50   $1,515.00\n6                   GBE HADDOCK   18,479  $0.02     $369.58\n7                   GOM HADDOCK        0  $0.02       $0.00\n8                   GBW HADDOCK  110,470  $0.02   $2,209.40\n9                          HAKE      259  $1.30     $336.70\n10                       PLAICE    3,738  $0.40   $1,495.20\n11                      POLLOCK    3,265  $0.02      $65.30\n12               WITCH FLOUNDER    1,134  $1.30   $1,474.20\n13                       SNE YT    1,458  $0.65     $947.70\n14                        GB YT    4,499  $0.70   $3,149.30\n15                      REDFISH      841  $0.02      $16.82\n16  54 DAS @ $8.00/DAY = 432.00      NaN    NaN        None\n\n"
"There's this interesting API called Intervalindex new in 0.20 that lets you create an index of intervals.\n\nGiven some sample data:\n\ndata = [(893.1516130000001, 903.9187099999999),\n (882.384516, 893.1516130000001),\n (817.781935, 828.549032)]\n\n\nYou can create the index like this:\n\nidx = pd.IntervalIndex.from_tuples(data)\n\nprint(idx)\nIntervalIndex([(893.151613, 903.91871], (882.384516, 893.151613], (817.781935, 828.549032]]\n              closed='right',\n              dtype='interval[float64]')\n\n\nAn interesting property of Intervals is that you can perform interval checks with in:\n\nprint(y[-1])\nInterval(817.78193499999998, 828.54903200000001, closed='right')\n\nprint(820 in y[-1])\nTrue\n\nprint(1000 in y[-1])\nFalse\n\n\nI would like to know how to apply this operation to the entire index. For example, given some number 900, how could I retrieve a boolean mask of intervals for which this number fits in? \n\nI can think of:\n\nm = [900 in y for y in idx]\nprint(m)\n[True, False, False]\n\n\nAre there better ways to do this?\n"
"In my dataframe, I have a categorical variable that I'd like to convert into dummy variables. This column however has multiple values separated by commas:\n\n0    'a'\n1    'a,b,c'\n2    'a,b,d'\n3    'd'\n4    'c,d'\n\n\nUltimately, I'd want to have binary columns for each possible discrete value; in other words, final column count equals number of unique values in the original column. I imagine I'd have to use split() to get each separate value but not sure what to do afterwards. Any hint much appreciated!\n\nEdit: Additional twist. Column has null values. And in response to comment, the following is the desired output. Thanks!\n\n   a  b  c  d\n0  1  0  0  0\n1  1  1  1  0\n2  1  1  0  1\n3  0  0  0  1\n4  0  0  1  1\n\n"
"I have dataframe in pandas:\n\nIn [10]: df\nOut[10]:\n    col_a    col_b  col_c  col_d\n0  France    Paris      3      4\n1      UK    Londo      4      5\n2      US  Chicago      5      6\n3      UK  Bristol      3      3\n4      US    Paris      8      9\n5      US   London     44      4\n6      US  Chicago     12      4\n\n\nI need to count unique cities. I can count unique states\n\nIn [11]: df['col_a'].nunique()\nOut[11]: 3\n\n\nand I can try to count unique cities\n\nIn [12]: df['col_b'].nunique()\nOut[12]: 5\n\n\nbut it is wrong because US Paris and Paris in France are different cities. So now I'm doing in like this:\n\nIn [13]: df['col_a_b'] = df['col_a'] + ' - ' + df['col_b']\n\nIn [14]: df\nOut[14]:\n    col_a    col_b  col_c  col_d         col_a_b\n0  France    Paris      3      4  France - Paris\n1      UK    Londo      4      5      UK - Londo\n2      US  Chicago      5      6    US - Chicago\n3      UK  Bristol      3      3    UK - Bristol\n4      US    Paris      8      9      US - Paris\n5      US   London     44      4     US - London\n6      US  Chicago     12      4    US - Chicago\n\nIn [15]: df['col_a_b'].nunique()\nOut[15]: 6\n\n\nMaybe there is a better way? Without creating an additional column.\n"
"I have a csv file containing numerical values such as 1524.449677. There are always exactly 6 decimal places.\n\nWhen I import the csv file (and other columns) via pandas read_csv, the column automatically gets the datatype object. My issue is that the values are shown as 2470.6911370000003 which actually should be 2470.691137. Or the value 2484.30691 is shown as 2484.3069100000002.\n\nThis seems to be a datatype issue in some way. I tried to explicitly provide the data type when importing via read_csv by giving the dtype argument as {'columnname': np.float64}. Still the issue did not go away.\n\nHow can I get the values imported and shown exactly as they are in the source csv file?\n"
' result = sm.OLS(gold_lookback, silver_lookback ).fit()\n\n\nAfter I get the result, how can I get the coefficient and the constant?\n\nIn other words, if\ny = ax + c\nhow to get the values a and c?\n'
"I have a pandas DataFrame that has the following values in a Series\n\nx = [2, 1, 76, 140, 286, 267, 60, 271, 5, 13, 9, 76, 77, 6, 2, 27, 22, 1, 12, 7, 19, 81, 11, 173, 13, 7, 16, 19, 23, 197, 167, 1]\n\n\nI was instructed to plot two histograms in a Jupyter notebook with Python 3.6. No sweat right?\n\nx.plot.hist(bins=8)\nplt.show()\n\n\nI chose 8 bins because that looked best to me.\nI have also been instructed to plot another histogram with the log of x.\n\nx.plot.hist(bins=8)\nplt.xscale('log')\nplt.show()\n\n\nThis histogram looks TERRIBLE. Am I not doing something right? I've tried fiddling around with the plot, but everything I've tried just seems to make the histogram look even worse. Example:\n\nx.plot(kind='hist', logx=True)\n\n\nI was not given any instructions other than plot the log of X as a histogram. \n\nI really appreciate any help!!! \n\nFor the record, I have imported pandas, numpy, and matplotlib and specified that the plot should be inline. \n"
'The rolling window function pandas.DataFrame.rolling of pandas 0.22 takes a window argument that is described as follows:\n\n\n  window : int, or offset\n  \n  Size of the moving window. This is the number of observations used for\n  calculating the statistic. Each window will be a fixed size.\n  \n  If its an offset then this will be the time period of each window.\n  Each window will be a variable sized based on the observations\n  included in the time-period. This is only valid for datetimelike\n  indexes. This is new in 0.19.0\n\n\nWhat actually is an offset in this context?\n'
"I have a dataframe where each row contains various meta-data pertaining to a single Reddit comment (e.g. author, subreddit, comment text).\n\nI want to do the following: for each author, I want to grab a list of all the subreddits they have comments in, and transform this data into a pandas dataframe where each row corresponds to an author, and a list of all the unique subreddits they comment in.\n\nI am currently trying some combination of the following, but can't get it down:\n\nAttempt 1:\n\ngroup = df['subreddit'].groupby(df['author']).unique()\nlist(group) \n\n\nAttempt 2:\n\nfrom collections import defaultdict\nsubreddit_dict  = defaultdict(list)\n\nfor index, row in df.iterrows():\n    author = row['author']\n    subreddit = row['subreddit']\n    subreddit_dict[author].append(subreddit)\n\nfor key, value in subreddit_dict.items():\n    subreddit_dict[key] = set(value)\n\nsubreddit_df = pd.DataFrame.from_dict(subreddit_dict, \n                            orient = 'index')\n\n"
"I have the below case statement in python, \n\npd_df['difficulty'] = 'Unknown'\npd_df['difficulty'][(pd_df['Time']&lt;30) &amp; (pd_df['Time']&gt;0)] = 'Easy'\npd_df['difficulty'][(pd_df['Time']&gt;=30) &amp; (pd_df['Time']&lt;=60)] = 'Meduim'\npd_df['difficulty'][pd_df['Time']&gt;60] = 'Hard'\n\n\nBut when I run the code, it throws an error.\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\n"
"I have an empty dataframe.\n\ndf=pd.DataFrame(columns=['a'])\n\n\nfor some reason I want to generate df2, another empty dataframe, with two columns 'a' and 'b'.\n\nIf I do \n\ndf.columns=df.columns+'b'\n\n\nit does not work (I get the columns renamed to 'ab')\nand neither does the following \n\ndf.columns=df.columns.tolist()+['b']\n\n\nHow to add a separate column 'b' to df, and df.emtpy keep on being True?\n\nUsing .loc is also not possible\n\n   df.loc[:,'b']=None\n\n\nas it returns \n\n  Cannot set dataframe with no defined index and a scalar\n\n"
"I want to filter a data frame by more complex function based on different values in the row.\n\nIs there a possibility to filter DF rows by a boolean function like you can do it e.g. in ES6 filter function?\n\nExtreme simplified example to illustrate the problem:\n\nimport pandas as pd\n\ndef filter_fn(row):\n    if row['Name'] == 'Alisa' and row['Age'] &gt; 24:\n        return False\n\n    return row\n\nd = {\n    'Name': ['Alisa', 'Bobby', 'jodha', 'jack', 'raghu', 'Cathrine',\n             'Alisa', 'Bobby', 'kumar', 'Alisa', 'Alex', 'Cathrine'],\n    'Age': [26, 24, 23, 22, 23, 24, 26, 24, 22, 23, 24, 24],\n\n    'Score': [85, 63, 55, 74, 31, 77, 85, 63, 42, 62, 89, 77]}\n\ndf = pd.DataFrame(d, columns=['Name', 'Age', 'Score'])\n\ndf = df.apply(filter_fn, axis=1, broadcast=True)\n\nprint(df)\n\n\nI found something using apply() bit this actually returns only False/True filled rows using a bool function, which is expected.\n\nMy workaround would be returning the row itself when the function result would be True and returning False if not. But this would require a additional filtering after that.\n\n        Name    Age  Score\n0      False  False  False\n1      Bobby     24     63\n2      jodha     23     55\n3       jack     22     74\n4      raghu     23     31\n5   Cathrine     24     77\n6      False  False  False\n7      Bobby     24     63\n8      kumar     22     42\n9      Alisa     23     62\n10      Alex     24     89\n11  Cathrine     24     77\n\n"
'As you can see the code produces a barplot that is not as clear and I want to make the figure larger so the values can be seen better. This doesn\'t do it. What is the correct way?\nx is a dataframe and x[\'user\'] is the x axis in the plot and x[\'number\'] is the y.\n\nimport matplotlib.pyplot as plt\n%matplotlib inline  \nplt.bar(x[\'user\'], x[\'number\'], color="blue")\nplt.figure(figsize=(20,10)) \n\n\nThe line with the plt.figure doesn\'t change the initial dimensions.\n'
'I recently asked a question regarding missing values in pandas here and was directed to a github issue. After reading through that page and the missing data documentation.\n\nI am wondering why merge and join treat NaNs as a match when "they don\'t compare equal": np.nan != np.nan\n\n# merge example\ndf = pd.DataFrame({\'col1\':[np.nan, \'match\'], \'col2\':[1,2]})\ndf2 = pd.DataFrame({\'col1\':[np.nan, \'no match\'], \'col3\':[3,4]})\npd.merge(df,df2, on=\'col1\')\n\n    col1    col2    col3\n0   NaN      1       3\n\n# join example with same dataframes from above\ndf.set_index(\'col1\').join(df2.set_index(\'col1\'))\n\n      col2  col3\ncol1        \nNaN     1   3.0\nmatch   2   NaN\n\n\nHowever, NaNs in groupby are excluded:\n\ndf = pd.DataFrame({\'col1\':[np.nan, \'match\', np.nan], \'col2\':[1,2,1]})\ndf.groupby(\'col1\').sum()\n\n       col2\ncol1    \nmatch   2\n\n\nOf course you can dropna() or df[df[\'col1\'].notnull()] but I am curious as to why NaNs are excluded in some pandas operations like groupby and not others like merge, join, update, and map?\n\nEssentially, as I asked above, why does merge and join match on np.nan when they do not compare equal?\n'
"I'm trying to query a Pandas dataframe like this:\n\n        inv = pd.read_csv(infile)\n        inv.columns = ['County','Site','Role','Hostname'] \n        clist = inv.County.unique() # Get list of counties\n        for county in clist: # for each county\n            csub=inv.query('County == county') # create a county subset\n            ... do stuff on subset\n\n\nBut I get an error:\n\npandas.core.computation.ops.UndefinedVariableError: name 'county' is not defined\n\n\nI'm sure it's a trivial error, but I can't figure it out.  How do I pass a variable to the query method?\n"
"I want to resample a TimeSeries in daily (exactly 24 hours) frequence starting at a certain hour.\n\nLike:\n\nindex = date_range(datetime(2012,1,1,17), freq='H', periods=60)\n\nts = Series(data=[1]*60, index=index)\n\nts.resample(rule='D', how='sum', closed='left', label='left')\n\n\nResult i get:\n\n2012-01-01  7\n2012-01-02 24\n2012-01-03 24\n2012-01-04  5\nFreq: D\n\n\nResult i wish:\n\n2012-01-01 17:00:00 24\n2012-01-02 17:00:00 24\n2012-01-03 17:00:00 12\nFreq: D\n\n\nSome weeks ago you could pass '24H' to the freq argument and it worked totally fine.\nBut now it combines '24H' to '1D'.\n\nWas I using a bug with '24H' which is fixed now?\nAnd how can i get the wished result in a efficient and pythonic (or pandas) way back?\n\nversions:\n\n\npython 2.7.3\npandas 0.9.0rc1 (but doesn't work in 0.8.1, too)\nnumpy 1.6.1\n\n"
"How can I retrieve specific columns from a pandas HDFStore?  I regularly work with very large data sets that are too big to manipulate in memory.  I would like to read in a csv file iteratively, append each chunk into HDFStore object, and then work with subsets of the data.  I have read in a simple csv file and loaded it into an HDFStore with the following code:    \n\ntmp = pd.HDFStore('test.h5')\nchunker = pd.read_csv('cars.csv', iterator=True, chunksize=10, names=['make','model','drop'])\ntmp.append('df', pd.concat([chunk for chunk in chunker], ignore_index=True))\n\n\nAnd the output:\n\nIn [97]: tmp\nOut[97]:\n&lt;class 'pandas.io.pytables.HDFStore'&gt;\nFile path: test.h5\n/df     frame_table (typ-&gt;appendable,nrows-&gt;1930,indexers-&gt;[index])\n\n\nMy Question is how do I access specific columns from tmp['df']?  The documenation makes mention of a select() method and some Term objects.  The examples provided are applied to Panel data; however, and I'm too much of a novice to extend it to the simpler data frame case.  My guess is that I have to create an index of the columns somehow.  Thanks!\n"
"I see that Pandas has read_fwf, but does it have something like DataFrame.to_fwf?  I'm looking for support for field width, numerical precision, and string justification.  It seems that DataFrame.to_csv doesn't do this.  numpy.savetxt does, but I wouldn't want to do:\n\nnumpy.savetxt('myfile.txt', mydataframe.to_records(), fmt='some format')\n\n\nThat just seems wrong.  Your ideas are much appreciated.\n"
"I am analyzing a data set that is similar in shape to the following example. I have two different types of data (abc data and xyz data):\n\n   abc1  abc2  abc3  xyz1  xyz2  xyz3\n0     1     2     2     2     1     2\n1     2     1     1     2     1     1\n2     2     2     1     2     2     2\n3     1     2     1     1     1     1\n4     1     1     2     1     2     1\n\n\nI want to create a function that adds a categorizing column for each abc column that exists in the dataframe. Using lists of column names and a category mapping dictionary, I was able to get my desired result.\n\nabc_columns = ['abc1', 'abc2', 'abc3']\nxyz_columns = ['xyz1', 'xyz2', 'xyz3']\nabc_category_columns = ['abc1_category', 'abc2_category', 'abc3_category']\ncategories = {1: 'Good', 2: 'Bad', 3: 'Ugly'}\n\nfor i in range(len(abc_category_columns)):\n    df3[abc_category_columns[i]] = df3[abc_columns[i]].map(categories)\n\nprint df3\n\n\nThe end result:\n\n   abc1  abc2  abc3  xyz1  xyz2  xyz3 abc1_category abc2_category abc3_category\n0     1     2     2     2     1     2          Good           Bad           Bad\n1     2     1     1     2     1     1           Bad          Good          Good\n2     2     2     1     2     2     2           Bad           Bad          Good\n3     1     2     1     1     1     1          Good           Bad          Good\n4     1     1     2     1     2     1          Good          Good           Bad\n\n\nWhile the for loop at the end works fine, I feel like I should be using Python's lambda function, but can't seem to figure it out. \n\nIs there a more efficient way to map in a dynamic number of abc-type columns?\n"
'since I have installed the updated version of pandas every time I type in the name of a dataframe, e.g.\n\ndf[0:5]\n\n\nTo see the first few rows, it gives me a summary of the columns, the number of values in them and the data types instead.\n\nHow do I get to see the tabular view instead? (I am using iPython btw).\n\nThanks in advance!\n'
"I am getting an error and I'm not sure how to fix it. \n\nThe following seems to work:\n\ndef random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\n\ndf.apply(func = random, axis = 1)\n\n\nand my output is:\n\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n[1,2,3,4]\n\n\nHowever, when I change one of the of the columns to a value such as 1 or None:\n\ndef random(row):\n   return [1,2,3,4]\n\ndf = pandas.DataFrame(np.random.randn(5, 4), columns=list('ABCD'))\ndf['E'] = 1\n\ndf.apply(func = random, axis = 1)\n\n\nI get the the error:\n\nValueError: Shape of passed values is (5,), indices imply (5, 5)\n\n\nI've been wrestling with this for a few days now and nothing seems to work. What is interesting is that when I change \n\ndef random(row):\n   return [1,2,3,4]\n\n\nto \n\ndef random(row):\n   print [1,2,3,4]\n\n\neverything seems to work normally. \n\nThis question is a clearer way of asking this question, which I feel may have been confusing.\n\nMy goal is to compute a list for each row and then create a column out of that.  \n\nEDIT: I originally start with a dataframe that hase one column. I add 4 columns in 4 difference apply steps, and then when I try to add another column I get this error. \n"
'I want to draw a boxplot of column Z in dataframe df by the categories X and Y. How can I sort the boxplot by the median, in descending order?\n\nimport pandas as pd\nimport random\nn = 100\n# this is probably a strange way to generate random data; please feel free to correct it\ndf = pd.DataFrame({"X": [random.choice(["A","B","C"]) for i in range(n)], \n                   "Y": [random.choice(["a","b","c"]) for i in range(n)],\n                   "Z": [random.gauss(0,1) for i in range(n)]})\ndf.boxplot(column="Z", by=["X", "Y"])\n\n\nNote that this question is very similar, but they use a different data structure. I\'m relatively new to pandas (and have only done some tutorials on python in general), so I couldn\'t figure out how to make my data work with the answer posted there. This may well be more of a reshaping than a plotting question. Maybe there is a solution using groupby?\n'
'I am a little confused with datatype "object" in Pandas. What exactly is "object"? \n\nI would like to change the variable "SpT" (see below) from object to String. \n\n&gt; df_cleaned.dtypes\n    Vmag        float64\n    RA          float64\n    DE          float64\n    Plx         float64\n    pmRA        float64\n    pmDE        float64\n    B-V         float64\n    SpT          object\n    M_V         float64\n    distance    float64\n    dtype: object\n\n\nFor this I do the following: \n\ndf_cleaned[\'SpT\'] = df_cleaned[\'SpT\'].astype(str)\n\n\nBut that has no effect on the dtype of SpT. \n\nThe reason for doing is when I do the following: \n\nf = lambda s: (len(s) &gt;= 2)  and (s[0].isalpha()) and (s[1].isdigit())\ni  = df_cleaned[\'SpT\'].apply(f)\ndf_cleaned = df_cleaned[i]\n\n\nI get: \n\nTypeError: object of type \'float\' has no len()\n\n\nHence, I believe if I convert "object" to "String", I will get to do what I want. \n\nMore info: This is how SpT looks like: \n\nHIP\n1                F5\n2               K3V\n3                B9\n4               F0V\n5             G8III\n6              M0V:\n7                G0\n8      M6e-M8.5e Tc\n9                G5\n10              F6V\n11               A2\n12            K4III\n13            K0III\n14               K0\n15               K2\n...\n118307    M2III:\n118308        K:\n118309        A2\n118310        K5\n118312        G5\n118313        F0\n118314        K0\n118315     K0III\n118316        F2\n118317        F8\n118318        K2\n118319       G2V\n118320        K0\n118321       G5V\n118322      B9IV\nName: SpT, Length: 114472, dtype: object\n\n'
'I have a dataframe with numerous columns (≈30) from an external source (csv file) but several of them have no value or always the same. Thus, I would to see quickly the value_counts for each column, how can i do that?\n\nFor example\n\n  Id, temp, name\n1 34, null, mark\n2 22, null, mark\n3 34, null, mark\n\n\nWould return me an object stating that\n\n\nId: 34 -> 2,  22 -> 1\ntemp: null -> 3\nname: mark -> 3\n\n\nSo I would know that temp is irrelevant and name is not interesting (always the same)\n'
'The operation that I want to do is similar to merger. For example, with the inner merger we get a data frame that contains rows that are present in the first AND second data frame. With the outer merger we get a data frame that are present EITHER in the first OR in the second data frame.\n\nWhat I need is a data frame that contains rows that are present in the first data frame AND NOT present in the second one? Is there a fast and elegant way to do it?\n'
'I have a pandas dataframe with a column of real values that I want to zscore normalize:\n\n&gt;&gt; a\narray([    nan,  0.0767,  0.4383,  0.7866,  0.8091,  0.1954,  0.6307,\n        0.6599,  0.1065,  0.0508])\n&gt;&gt; df = pandas.DataFrame({"a": a})\n\n\nThe problem is that a single nan value makes all the array nan:\n\n&gt;&gt; from scipy.stats import zscore\n&gt;&gt; zscore(df["a"])\narray([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])\n\n\nWhat\'s the correct way to apply zscore (or an equivalent function not from scipy) to a column of a pandas dataframe and have it ignore the nan values? I\'d like it to be same dimension as original column with np.nan for values that can\'t be normalized\n\nedit: maybe the best solution is to use scipy.stats.nanmean and scipy.stats.nanstd? I don\'t see why the degrees of freedom need to be changed for std for this purpose:\n\nzscore = lambda x: (x - scipy.stats.nanmean(x)) / scipy.stats.nanstd(x)\n\n'
"I am trying to plot a chart with the 1st and 2nd columns of data as bars and then a line overlay for the 3rd column of data.\n\nI have tried the following code but this creates 2 separate charts but I would like this all on one chart.\n\nleft_2013 = pd.DataFrame({'month': ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'],\n                     '2013_val': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 6]})\n\nright_2014 = pd.DataFrame({'month': ['jan', 'feb'], '2014_val': [4, 5]})\n\nright_2014_target = pd.DataFrame({'month': ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'],\n                                   '2014_target_val': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]})\n\n\ndf_13_14 = pd.merge(left_2013, right_2014, how='outer')\ndf_13_14_target = pd.merge(df_13_14, right_2014_target, how='outer')\ndf_13_14_target[['month','2013_val','2014_val','2014_target_val']].head(12)\n\nplt.figure()\ndf_13_14_target[['month','2014_target_val']].plot(x='month',linestyle='-', marker='o')\ndf_13_14_target[['month','2013_val','2014_val']].plot(x='month', kind='bar')\n\n\nThis is what I currently get\n\n\n"
"This may seem to be a useless feature but it would be very helpful for me. I would like to save the output I get inside Canopy IDE. I would not think this is specific to Canopy but for the sake of clarity that is what I use. For example, my console Out[2] is what I would want from this:\n\n\n\nI think that the formatting is quite nice and to reproduce this each time instead of just saving the output would be a waste of time. So my question is, how can I get a handle on this figure? Ideally the implimentation would be similar to standard methods, such that it could be done like this:\n\nfrom matplotlib.backends.backend_pdf import PdfPages\n\npp = PdfPages('Output.pdf')\nfig = plt.figure() \nax = fig.add_subplot(1, 1, 1)\ndf.plot(how='table')\npp.savefig()\npp.close()\n\n\nNOTE: I realize that a very similar question has been asked before ( How to save the Pandas dataframe/series data as a figure? ) but it never received an answer and I think I have stated the question more clearly.\n"
"I have lists which I want to insert it as column labels.\nBut when I use read_excel of pandas, they always consider 0th row as column label.\nHow could I read the file as pandas dataframe and then put the list as column label\n\n  orig_index = pd.read_excel(basic_info, sheetname = 'KI12E00')\n\n    0.619159  0.264191  0.438849  0.465287  0.445819  0.412582  0.397366  \\\n0   0.601379  0.303953  0.457524  0.432335  0.415333  0.382093  0.382361   \n1   0.579914  0.343715  0.418294  0.401129  0.385508  0.355392  0.355123  \n\n\nHere is my personal list for column name\n\n   print set_index\n[20140109, 20140213, 20140313, 20140410, 20140508, 20140612]\n\n\nAnd I want to make dataframe as below\n\n    20140109  20140213  20140313  20140410  20140508  20140612\n0   0.619159  0.264191  0.438849  0.465287  0.445819  0.412582  0.397366  \\\n1   0.601379  0.303953  0.457524  0.432335  0.415333  0.382093  0.382361   \n2   0.579914  0.343715  0.418294  0.401129  0.385508  0.355392  0.355123\n\n"
'I am looking for a a way to read just the header row of a large number of large CSV files. \n\nUsing Pandas, I have this method available, for each csv file:\n\n&gt;&gt;&gt; df = pd.read_csv(PATH_TO_CSV)\n&gt;&gt;&gt; df.columns\n\n\nI could do this with just the csv module:\n\n&gt;&gt;&gt; reader = csv.DictReader(open(PATH_TO_CSV))\n&gt;&gt;&gt; reader.fieldnames\n\n\nThe problem with these is that each CSV file is 500MB+ in size, and it seems to be a gigantic waste to read in the entire file of each just to pull the header lines.\n\nMy end goal of all of this is to pull out unique column names. I can do that once I have a list of column headers that are in each of these files.\n\nHow can I extract only the header row of a CSV file, quickly?\n'
'Say I have a multi-index dataframe in Pandas, e.g:\n\n                         A         B         C\nX      Y     Z                                \nbar   one    a   -0.007381 -0.365315 -0.024817\n             b   -1.219794  0.370955 -0.795125\nbaz   three  a    0.145578  1.428502 -0.408384\n             b   -0.249321 -0.292967 -1.849202\n      two    a   -0.249321 -0.292967 -1.849202\n      four   a    0.211234 -0.967123  1.202234\nfoo   one    b   -1.046479 -1.250595  0.781722\n             a    1.314373  0.333150  0.133331\nqux   one    c    0.716789  0.616471 -0.298493\n      two    b    0.385795 -0.915417 -1.367644\n\n\nHow can I count how many levels are contained within another level? (e.g. level Y within X)\n\nE.g. in the case above the answer would be:\n\nX    Y \nbar  1\nbaz  3\nfoo  1\nqux  2\n\n\nUpdate\n\nWhen I try df.groupby(level=[0, 1]).count()[0] I get:\n\n            C  D  E\nA    B             \nbar  one    1  1  1\n     three  1  1  1\nflux six    1  1  1\n     three  1  1  1\nfoo  five   1  1  1\n     one    1  1  1\n     two    2  2  2\n\n'
"I want to create a DateTimeIndex at 1 minute intervals based on a start and end timestamp (given in microseconds since epoch) with pd_date_range().  To do this, I need to round the starting timestamp up and the ending timestamp down.  Here is what I have so far:\n\nimport pandas as pd\nstart = 1406507532491431\nend = 1406535228420914\n\nstart_ts = pd.to_datetime(start, unit='us') # Timestamp('2014-07-28 00:32:12.491431')\nend_ts = pd.to_datetime(end, unit='us') # Timestamp('2014-07-28 08:13:48.420914')\n\n\nI want to round:\n\nstart_ts to Timestamp('2014-07-28 00:32') and \n\nend_ts to Timestamp('2014-07-28 08:14').  \n\nHow can I do this?\n"
'I have a long json like this: http://pastebin.com/gzhHEYGy\n\nI would like to place it into a pandas datframe in order to play with it, so by the documentation I do the following:\n\ndf = pd.read_json(\'/user/file.json\')\nprint df\n\n\nI got this traceback:\n\n  File "/Users/user/PycharmProjects/PAN-pruebas/json_2_dataframe.py", line 6, in &lt;module&gt;\n    df = pd.read_json(\'/Users/user/Downloads/54db3923f033e1dd6a82222aa2604ab9.json\')\n  File "/usr/local/lib/python2.7/site-packages/pandas/io/json.py", line 198, in read_json\n    date_unit).parse()\n  File "/usr/local/lib/python2.7/site-packages/pandas/io/json.py", line 266, in parse\n    self._parse_no_numpy()\n  File "/usr/local/lib/python2.7/site-packages/pandas/io/json.py", line 483, in _parse_no_numpy\n    loads(json, precise_float=self.precise_float), dtype=None)\n  File "/usr/local/lib/python2.7/site-packages/pandas/core/frame.py", line 203, in __init__\n    mgr = self._init_dict(data, index, columns, dtype=dtype)\n  File "/usr/local/lib/python2.7/site-packages/pandas/core/frame.py", line 327, in _init_dict\n    dtype=dtype)\n  File "/usr/local/lib/python2.7/site-packages/pandas/core/frame.py", line 4620, in _arrays_to_mgr\n    index = extract_index(arrays)\n  File "/usr/local/lib/python2.7/site-packages/pandas/core/frame.py", line 4668, in extract_index\n    raise ValueError(\'arrays must all be same length\')\nValueError: arrays must all be same length\n\n\nThen from a previous question I found that I need to do something like this:\n\nd = dict( A = np.array([1,2]), B = np.array([1,2,3,4]) )\n\n\nBut I dont get how should I obtain the contents like a numpy array. How can I preserve the length of the arrays in a big file like this?. Thanks in advance. \n'
"I am trying to understand pandas MultiIndex DataFrames and how to assign data to them. Specifically I'm interested in assigning entire blocks that match another smaller data frame.\n\nix = pd.MultiIndex.from_product([['A', 'B'], ['a', 'b', 'c', 'd']])\ndf = pd.DataFrame(index=ix, columns=['1st', '2nd', '3rd'], dtype=np.float64)\ndf_ = pd.DataFrame(index=['a', 'b', 'c', 'd'], columns=['1st', '2nd', '3rd'], data=np.random.rand(4, 3))\ndf_\n\n    1st     2nd     3rd\na   0.730251    0.468134    0.876926\nb   0.104990    0.082461    0.129083\nc   0.993608    0.117799    0.341811\nd   0.784950    0.840145    0.016777\n\n\ndf is the same except that all the values are NaN and there are two blocks A and B. Now if I want to assign the values from df_ to df I would imagine I can do something like\n\ndf.loc['A',:] = df_                # Runs, does not work\ndf.loc[('A','a'):('A','d')] = df_  # AssertionError (??) 'Start slice bound is non-scalar'\ndf.loc[('A','a'):('A','d')]        # No AssertionError (??)\n\nidx = pd.IndexSlice\ndf.loc[idx['A', :]] = df_          # Runs, does not work\n\n\nNone of these work, they leave all the values in df as NaN, although df.loc[idx['A', :]] gives me a slice of the data frame that exactly matches that of the sub frame (df_). So is this a case of setting values on a view? Explicitly iterating over the index in df_ works\n\n# this is fine\nfor v in df_.index:\n    df.loc[idx['A', v]] = df_.loc[v]\n\n# this is also fine\nfor v in df_.index:\n    df.loc['A', v] = df_.loc[v]\n\n\nIs it even possible to assign whole blocks like this (sort of like NumPy)? If not, that's fine, I am simply trying to understand how the system works.\n\nThere's a related question about index slicers, but it's about assigning a single value to a masked portion of the DataFrame, not about assigning blocks.\nPandas : Proper way to set values based on condition for subset of multiindex dataframe\n"
"I have a pandas dataframe with 3 levels of a MultiIndex. I am trying to pull out rows of this dataframe according to a list of values that correspond to two of the levels.\n\nI have something like this:\n\nix = pd.MultiIndex.from_product([[1, 2, 3], ['foo', 'bar'], ['baz', 'can']], names=['a', 'b', 'c'])\ndata = np.arange(len(ix))\ndf = pd.DataFrame(data, index=ix, columns=['hi'])\nprint(df)\n\n           hi\na b   c      \n1 foo baz   0\n      can   1\n  bar baz   2\n      can   3\n2 foo baz   4\n      can   5\n  bar baz   6\n      can   7\n3 foo baz   8\n      can   9\n  bar baz  10\n      can  11\n\n\nNow I want to take all rows where index levels 'b' and 'c' are in this index:\n\nix_use = pd.MultiIndex.from_tuples([('foo', 'can'), ('bar', 'baz')], names=['b', 'c'])\n\n\ni.e. values of hi having ('foo', 'can') or ('bar', 'baz') in levels b and c respectively: (1, 2, 5, 6, 9, 10).\n\nSo I'd like to take a slice(None) on the first level, and pull out specific tuples on the second and third levels.\n\nInitially I thought that passing a multi-index object to .loc would pull out the values / levels that I wanted, but this isn't working. What's the best way to do something like this?\n"
"I'm trying to set a number of different in a pandas DataFrame all to the same value. I thought I understood boolean indexing for pandas, but I haven't found any resources on this specific error.\n\nimport pandas as pd \ndf = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})\nmask = df.isin([1, 3, 12, 'a'])\ndf[mask] = 30\nTraceback (most recent call last):\n...\nTypeError: Cannot do inplace boolean setting on mixed-types with a non np.nan value\n\n\nAbove, I want to replace all of the True entries in the mask with the value 30.\n\nI could do df.replace instead, but masking feels a bit more efficient and intuitive here. Can someone explain the error, and provide an efficient way to set all of the values?\n"
"I am using Python Pandas for the first time. I have 5-min lag traffic data in csv format:\n\n...\n2015-01-04 08:29:05,271238\n2015-01-04 08:34:05,329285\n2015-01-04 08:39:05,-1\n2015-01-04 08:44:05,260260\n2015-01-04 08:49:05,263711\n...\n\n\nThere are several issues: \n\n\nfor some timestamps there's missing data (-1)\nmissing entries (also 2/3 consecutive hours)\nthe frequency of the observations is not exactly 5 minutes, but actually loses some seconds once in a while\n\n\nI would like to obtain a regular time series, so with entries every (exactly) 5 minutes (and no missing valus). I have successfully interpolated the time series with the following code to approximate the -1 values with this code:\n\nts = pd.TimeSeries(values, index=timestamps)\nts.interpolate(method='cubic', downcast='infer')\n\n\nHow can I both interpolate and regularize the frequency of the observations? Thank you all for the help.\n"
"I love using the .head() and .tail() functions in pandas to circumstantially display a certain amount of rows (sometimes I want less, sometimes I want more!).  But is there a way to do this with the columns of a DataFrame?\n\nYes, I know that I can change the display options, as in:\npd.set_option('display.max_columns', 20)\n\nBut that is too clunky to keep having to change on-the-fly, and anyway, it would only replace the .head() functionality, but not the .tail() functionality.\n\nI also know that this could be done using an accessor:\nyourDF.iloc[:,:20] to emulate .head(20) and yourDF.iloc[:,-20:] to emulate .tail(20).\n\nIt may look like a short amount of code, but honestly it's not as intuitive nor swift as when I use .head().\n\nDoes such a command exist? I couldn't find one!\n"
"I'm having a problem with a data set that has 400,000 rows and 300 variables. I have to get dummy variables for a categorical variable with 3,000+ different items. At the end I want to end up with a data set with 3,300 variables or features so that I can train a RandomForest model.\n\nHere is what I've tried to do:\n\n df = pd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_')], axis=1)\n\n\nWhen I do that I'll always get an memory error. Is there a limit to the number of variables i can have?\n\nIf I do that with only the first 1,000 rows (which got 374 different categories) it just works fine.\n\nDoes anyone have a solution for my problem?  The computer I'm using has 8 GB of memory.\n"
"I've got a pandas DataFrame that looks like this:\n\n       sum\n1948   NaN\n1949   NaN\n1950     5\n1951     3\n1952   NaN\n1953     4\n1954     8\n1955   NaN\n\n\nand I would like to cut off the NaNs at the beginning and at the end ONLY (i.e. only the values incl. NaN from 1950 to 1954 should remain).\nI already tried .isnull() and dropna(), but somehow I couldn't find a proper solution.\nCan anyone help?\n"
"Say I have two dataframes, df1 and df2 that share the same index. df1 is sorted in the order that I want df2 to be sorted.\n\ndf=pd.DataFrame(index=['Arizona','New Mexico', 'Colorado'],columns=['A','B','C'], data=[[1,2,3],[4,5,6],[7,8,9]])\nprint df\n\n            A  B  C\nArizona     1  2  3\nNew Mexico  4  5  6\nColorado    7  8  9\n\n\ndf2=pd.DataFrame(index=['Arizona','Colorado', 'New Mexico'], columns=['D'], data=['Orange','Blue','Green'])\nprint df2\n                 D\nArizona     Orange\nColorado      Blue\nNew Mexico   Green\n\n\nWhat is the best / most efficient way of sorting the second dataframe by the index of the first?\n\nOne option is just joining them, sorting, and then dropping the columns:\n\ndf.join(df2)[['D']]\n\n                 D\nArizona     Orange\nNew Mexico   Green\nColorado      Blue\n\n\nIs there a more elegant way of doing this?\n\nThanks!\n"
'I have an example dataframe (df) for 2 products:\n\n                         BBG.XAMS.FUR.S               BBG.XAMS.MT.S\ndate                                                               \n2014-10-23                 -2368.850388                    0.000000\n2014-10-24                  6043.456178                    0.000000\n2015-03-23                    -0.674996                   -0.674997\n2015-03-24                    82.704951                   11.868748\n2015-03-25                   -11.027327                   84.160210\n\n\nIs there a way to retrieve the last index value of a dataframe only.  So in this example the date value I need retrieved is 2015-03-25?\n'
'I have two dataframes, one 18x30 (called df1) and one 2x30 (called df2), both of them have exactly the same index values.\n\nI want to be able to add one of the columns from df2 to the end of df1.\n\nThe data types in df1 are all integer and the data type for df2 is string. Whenever I merge/concat/join, I get NaN instead of the right data.\n\nAny help would be greatly appreciated\n\nThanks :D\n'
"I have two separate pandas dataframes (df1 and df2) which have multiple columns, but only one in common ('text'). \n\nI would like to do find every row in df2 that does not have a match in any of the rows of the column that df2 and df1 have in common. \n\ndf1\n\nA    B    text\n45   2    score\n33   5    miss\n20   1    score\n\n\ndf2\n\nC    D    text\n.5   2    shot\n.3   2    shot\n.3   1    miss\n\n\nResult df (remove row containing miss since it occurs in df1)\n\nC    D    text\n.5   2    shot\n.3   2    shot\n\n\nIs it possible to use the isin method in this scenario?\n"
"I have a column in my (pandas) dataframe:\n\ndata['Start Date'].head()\ntype(data['Start Date'])\nOutput:\n1/7/13\n1/7/13\n1/7/13\n16/7/13\n16/7/13\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nWhen I convert it into a date format (as follows) I am getting the error ValueError: Unknown string format\n\ndata['Start Date']= pd.to_datetime(data['Start Date'],dayfirst=True)\n...\n...\n/Library/Python/2.7/site-packages/pandas/tseries/tools.pyc in _convert_listlike(arg, box, format, name)\n    381                 return DatetimeIndex._simple_new(values, name=name, tz=tz)\n    382             except (ValueError, TypeError):\n--&gt; 383                 raise e\n    384 \n    385     if arg is None:\n\nValueError: Unknown string format\n\n\nWhat am I missing here?\n"
'I have a dataframe which I want to plot with matplotlib, but the index column is the time and I cannot plot it.\n\nThis is the dataframe (df3):\n\n\n\nbut when I try the following:\n\nplt.plot(df3[\'magnetic_mag mean\'], df3[\'YYYY-MO-DD HH-MI-SS_SSS\'], label=\'FDI\')\n\n\nI\'m getting an error obviously:\n\nKeyError: \'YYYY-MO-DD HH-MI-SS_SSS\'\n\n\nSo what I want to do is to add a new extra column to my dataframe (named \'Time) which is just a copy of the index column.\n\nHow can I do it?\n\nThis is the entire code:\n\n#Importing the csv file into df\ndf = pd.read_csv(\'university2.csv\', sep=";", skiprows=1)\n\n#Changing datetime\ndf[\'YYYY-MO-DD HH-MI-SS_SSS\'] = pd.to_datetime(df[\'YYYY-MO-DD HH-MI-SS_SSS\'], \n                                               format=\'%Y-%m-%d %H:%M:%S:%f\')\n\n#Set index from column\ndf = df.set_index(\'YYYY-MO-DD HH-MI-SS_SSS\')\n\n#Add Magnetic Magnitude Column\ndf[\'magnetic_mag\'] = np.sqrt(df[\'MAGNETIC FIELD X (μT)\']**2 + df[\'MAGNETIC FIELD Y (μT)\']**2 + df[\'MAGNETIC FIELD Z (μT)\']**2)\n\n#Subtract Earth\'s Average Magnetic Field from \'magnetic_mag\'\ndf[\'magnetic_mag\'] = df[\'magnetic_mag\'] - 30\n\n#Copy interesting values\ndf2 = df[[ \'ATMOSPHERIC PRESSURE (hPa)\',\n          \'TEMPERATURE (C)\', \'magnetic_mag\']].copy()\n\n#Hourly Average and Standard Deviation for interesting values \ndf3 = df2.resample(\'H\').agg([\'mean\',\'std\'])\ndf3.columns = [\' \'.join(col) for col in df3.columns]\n\ndf3.reset_index()\nplt.plot(df3[\'magnetic_mag mean\'], df3[\'YYYY-MO-DD HH-MI-SS_SSS\'], label=\'FDI\')  \n\n\nThank you !!\n'
"I have multiple dataframes each with a multi-level-index and a value column. I want to add up all the dataframes on the value columns. \n\ndf1 + df2\n\nNot all the indexes are complete in each dataframe, hence I am getting nan on a row which is not present in all the dataframes. \n\nHow can I overcome this and treat rows which are not present in any dataframe as having a value of 0?\n\nEg. I want to get\n\n   val\na    2\nb    4\nc    3\nd    3\n\n\nfrom pd.DataFrame({'val':{'a': 1, 'b':2, 'c':3}}) + pd.DataFrame({'val':{'a': 1, 'b':2, 'd':3}}) instead of \n\n   val\na    2\nb    4\nc  NaN\nd  NaN\n\n"
"I'm working with pandas 0.18 in Jupyter. \n\nI'd like to configure Jupyter/pandas to display 2 decimal places throughout, and to use comma separators in thousands. \n\nHow can I do this?\n"
'Need some help on processing data inside a pandas dataframe.\nAny help is most welcome.\n\nI have OHCLV data in CSV format. I have loaded the file in to pandas dataframe.\n\nHow do I convert the volume column from  2.90K to 2900 or 5.2M to 5200000.\nThe column can contain both K in form of thousands and M in millions.\n\nimport pandas as pd\n\nfile_path = \'/home/fatjoe/UCHM.csv\'\ndf = pd.read_csv(file_path, parse_dates=[0], index_col=0)\ndf.columns = [\n"closing_price", \n"opening_price", \n"high_price", \n"low_price",\n"volume",\n"change"]\n\ndf[\'opening_price\'] = df[\'closing_price\']\ndf[\'opening_price\'] = df[\'opening_price\'].shift(-1)\ndf = df.replace(\'-\', 0)\ndf = df[:-1]\nprint(df.head())\n\nConsole:\n Date\n 2016-09-23          0\n 2016-09-22      9.60K\n 2016-09-21     54.20K\n 2016-09-20    115.30K\n 2016-09-19     18.90K\n 2016-09-16    176.10K\n 2016-09-15     31.60K\n 2016-09-14     10.00K\n 2016-09-13      3.20K\n\n'
'I have a csv file with 50 columns of data. I am using Pandas read_csv function to pull in a subset of these columns, using  the usecols parameter to choose the ones I want:\n\ncols_to_use = [0,1,5,16,8]\ndf_ret = pd.read_csv(filepath, index_col=False, usecols=cols_to_use)\n\n\nThe trouble is df_ret contains the correct columns, but not in the order I specified. They are in ascending order, so [0,1,5,8,16]. (By the way the column numbers can change from run to run, this is just an example.) This is a problem because the rest of the code has arrays which are in the "correct" order and I would rather not have to reorder all of them. \n\nIs there any clever pandas way of pulling in the columns in the order specified? Any help would be much appreciated!\n'
'I have a pandas series of the form [0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0 , 0 , 1].\n\n0: indicates economic increase.\n1: indicates economic decline.\n\n\nA recession is signaled by two consecutive declines (1).\n\nThe end of the recession is signaled by two consecutive increase (0).\n\nIn the above dataset I have two recessions, begin at index 3, end at index 5 and begin at index 8 end at index 11.\n\nI am at a lost for how to approach this with pandas.  I would like to identify the index for the start and end of the recession.  Any assistance would be appreciated.\n\nHere is my python attempt at a soln.\n\nnp_decline =  np.array([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0 , 0 , 1])\nrecession_start_flag = 0\nrecession_end_flag = 0\nrecession_start = []\nrecession_end = []\n\nfor i in range(len(np_decline) - 1):\n    if recession_start_flag == 0 and np_decline[i] == 1 and np_decline[i + 1] == 1:\n        recession_start.append(i)\n        recession_start_flag = 1\n    if recession_start_flag == 1 and np_decline[i] == 0 and np_decline[i + 1] == 0:\n        recession_end.append(i - 1)\n        recession_start_flag = 0\n\nprint(recession_start)\nprint(recession_end)\n\n\nIs the a more pandas centric approach?\nLeon \n'
"I have the Yelp dataset and I want to count all reviews which have greater than 3 stars. I get the count of reviews by doing this:\n\nreviews.groupby('business_id')['stars'].count()\n\n\nNow I want to get the count of reviews which had more than 3 stars, so I tried this by taking inspiration from here:\n\nreviews.groupby('business_id')['stars'].agg({'greater':lambda val: (val &gt; 3).count()})\n\n\nBut this just gives me the count of all stars like before. I am not sure if this is the right way to do it? What am I doing incorrectly here. Does the lambda expression not go through each value of the stars column?\n\nEDIT:\nOkay I feel stupid. I should have used the sum function instead of count to get the value of elements greater than 3, like this:\n\nreviews.groupby('business_id')['stars'].agg({'greater':lambda val: (val &gt; 3).sum()})\n\n"
"consider the following pandas series s and plot\n\nimport pandas as pd\nimport numpy as np\n\ns = pd.Series(np.random.lognormal(.001, .01, 100))\nax = s.cumprod().plot()\nax.set_title('My Log Normal Example', position=(.5, 1.02),\n             backgroundcolor='black', color='white')\n\n\n\n\nHow do I get the box that contains the title to span the entire plot?\n"
"I'm looking at the tutorials on window functions, but I don't quite understand why the following code produces NaNs.\n\nIf I understand correctly, the code creates a rolling window of size 2. Why do the first, fourth, and fifth rows have NaN? At first, I thought it's because adding NaN with another number would produce NaN, but then I'm not sure why the second row wouldn't be NaN.\n\ndft = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}, \n                   index=pd.date_range('20130101 09:00:00', periods=5, freq='s'))\n\n\nIn [58]: dft.rolling(2).sum()\nOut[58]: \n                       B\n2013-01-01 09:00:00  NaN\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  NaN\n2013-01-01 09:00:04  NaN\n\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfile = 'd:\\\\a\\\\pandas\\\\test.xlsx'\ndata = pd.ExcelFile(file)\ndf1 = data.parse('Link')\ndf2 = df1[['dataFor', 'total']]\ndf2\n\n\nreturns:\n\n\n\n print (type(df2))\n\n\ntells me\n\nclass 'pandas.core.frame.DataFrame'\n\n\ntrying\n\ndf2.plot(kind='line')\n\n\nreturns\n\nmatplotlib.axes._subplots.AxesSubplot at 0xe4241d0\n\n\nCould it be the environment?\n\nJupyter notebook &gt; Help &gt; About\n\nThe version of the notebook server is 4.2.3 and is running on:\nPython 3.5.2 |Anaconda 4.2.0 (32-bit)| (default, Jul  5 2016, 11:45:57) [MSC    v.1900 32 bit (Intel)]\n\n\nWhere is the fault? Is matplotlib still the standard or should beginners go for Bokeh or for both?\n"
'I have a dataframe that looks like this:\n\nfrom    to         datetime              other\n-------------------------------------------------\n11      1     2016-11-06 22:00:00          -\n11      1     2016-11-06 20:00:00          -\n11      1     2016-11-06 15:45:00          -\n11      12    2016-11-06 15:00:00          -\n11      1     2016-11-06 12:00:00          -\n11      18    2016-11-05 10:00:00          -\n11      12    2016-11-05 10:00:00          -\n12      1     2016-10-05 10:00:59          -\n12      3     2016-09-06 10:00:34          -\n\n\nI want to groupby "from" and then "to" columns and then sort the "datetime" in descending order and then finally want to calculate the time difference within these grouped by objects between the current time and the next time. For eg, in this case,\nI would like to have a dataframe like the following:\n\nfrom    to     timediff in minutes                                          others\n11      1            120\n11      1            255\n11      1            225\n11      1            0 (preferrably subtract this date from the epoch)\n11      12           300\n11      12           0\n11      18           0\n12      1            25\n12      3            0\n\n\nI can\'t get my head around figuring this out!! Is there a way out for this?\nAny help will be much much appreciated!!\nThank you so much in advance!\n'
'I have a pandas dataframe with dates and strings similar to this:\n\nStart        End           Note    Item\n2016-10-22   2016-11-05    Z       A\n2017-02-11   2017-02-25    W       B\n\n\n\n\nI need to expand/transform it to the below, filling in weeks (W-SAT) in between the Start and End columns and forward filling the data in Note and Items:\n\nStart        Note    Item\n2016-10-22   Z       A\n2016-10-29   Z       A\n2016-11-05   Z       A\n2017-02-11   W       B\n2017-02-18   W       B\n2017-02-25   W       B\n\n\n\n\nWhats the best way to do this with pandas? Some sort of multi-index apply?\n'
"I have some columns in my dataframe for which I just want to keep the date part and remove the time part. I have made a list of these columns:\n\nlist_of_cols_to_change = ['col1','col2','col3','col4']\n\n\nI have written a function for doing this. It takes a list of columns and applies  dt.date to each column in the list.\n\ndef datefunc(x):\n    for column in x:\n        df[column] = df[column].dt.date\n\n\nI then call this function passing the list as parameter:\n\ndatefunc(list_of_cols_to_change )\n\n\nI want to accomplish this using something like map(). Basically use a function what takes a column as parameter and makes changes to it. I then want to use map() to apply this function to the list of columns that I have. Something like this:\n\ndef datefunc_new(column):\n    df[column] = df[column].dt.date\n\nmap(datefunc_new,list_of_cols_to_change)\n\n\nThis does not work however. How can I make this work ?\n"
"I have the following dict, with keys as tuples:\n\nd = {('first', 'row'): 3, ('second', 'row'): 1}\n\n\nI'd like to create a dataframe with 3 columns: Col1, Col2 and Col3 which should look like this:\n\nCol1   Col2  Col3\nfirst  row   3\nsecond row   4\n\n\nI can't figure out how to split the tuples other than parsing the dict pair by pair.\n"
'I have a dataframe that provides two integer columns with the Year and Week of the year:\n\nimport pandas as pd\nimport numpy as np\nL1 = [43,44,51,2,5,12]\nL2 = [2016,2016,2016,2017,2017,2017]\ndf = pd.DataFrame({"Week":L1,"Year":L2})\n\ndf\nOut[72]: \n   Week  Year\n0    43  2016\n1    44  2016\n2    51  2016\n3     2  2017\n4     5  2017\n5    12  2017\n\n\nI need to create a datetime-object from these two numbers.\n\nI tried this, but it throws an error:\n\ndf["DT"] = df.apply(lambda x: np.datetime64(x.Year,\'Y\') + np.timedelta64(x.Week,\'W\'),axis=1)\n\n\nThen I tried this, it works but gives the wrong result, that is it ignores the week completely:\n\ndf["S"] = df.Week.astype(str)+\'-\'+df.Year.astype(str)\ndf["DT"] = df["S"].apply(lambda x: pd.to_datetime(x,format=\'%W-%Y\'))\n\ndf\nOut[74]: \n   Week  Year        S         DT\n0    43  2016  43-2016 2016-01-01\n1    44  2016  44-2016 2016-01-01\n2    51  2016  51-2016 2016-01-01\n3     2  2017   2-2017 2017-01-01\n4     5  2017   5-2017 2017-01-01\n5    12  2017  12-2017 2017-01-01\n\n\nI\'m really getting lost between Python\'s datetime, Numpy\'s datetime64, and pandas Timestamp, can you tell me how it\'s done correctly?\n\nI\'m using Python 3, if that is relevant in any way.\n\nEDIT:\n\nStarting with Python 3.8 the problem is easily solved with a newly introduced method on datetime.date objects: https://docs.python.org/3/library/datetime.html#datetime.date.fromisocalendar\n'
"I have the following dict:\n\ntd = {'q1':(111,222), 'q2':(333,444)}\n\n\nI would like to convert it to a dataframe that looks like this:\n\nQuery    Value1     Value2\nq1       111       222\nq2       333       444\n\n\nI have tried the following:\n\ndf = pd.DataFrame(td.items())\n\n\nThe result looks like this:\n\n    0         1\n0   q1  (111,222)\n1   q2  (333,444) \n\n\nIf it wasn't entirely obvious, I am new to python and pandas. How can I get a dictionary with values as tuples to behave as separate columns in a dataframe?\n\nMy end goal is to display percent difference between value1 and value2.  \n"
"I'm trying to upload a pandas.DataFrame to google big query using the pandas.DataFrame.to_gbq() function documented here. The problem is that to_gbq() takes 2.3 minutes while uploading directly to Google Cloud Storage GUI takes less than a minute. I'm planing to upload a bunch of dataframes (~32) each one with a similar size, so i want to know what its the faster alternative.\n\nThis is the script that i'm using:\n\ndataframe.to_gbq('my_dataset.my_table', \n                 'my_project_id',\n                 chunksize=None, # i've tryed with several chunksizes, it runs faster when is one big chunk (at least for me)\n                 if_exists='append',\n                 verbose=False\n                 )\n\ndataframe.to_csv(str(month) + '_file.csv') # the file size its 37.3 MB, this takes almost 2 seconds \n# manually upload the file into GCS GUI\nprint(dataframe.shape)\n(363364, 21)\n\n\nmy question is, what is faster?\n\n\nUpload Dataframe using pandas.DataFrame.to_gbq() function\nSaving Dataframe as csv and then upload as a file to BigQuery using the Python API\nSaving Dataframe as csv and then upload the file to Google Cloud Storage using this procedure and then reading it from BigQuery\n\n\nupdate:\n\nalternative 2, using pd.DataFrame.to_csv() and load_data_from_file() seems to take longer than alternative 1 ( 17.9 sec more in average with 3 loops):\n\ndef load_data_from_file(dataset_id, table_id, source_file_name):\n    bigquery_client = bigquery.Client()\n    dataset_ref = bigquery_client.dataset(dataset_id)\n    table_ref = dataset_ref.table(table_id)\n\n    with open(source_file_name, 'rb') as source_file:\n        # This example uses CSV, but you can use other formats.\n        # See https://cloud.google.com/bigquery/loading-data\n        job_config = bigquery.LoadJobConfig()\n        job_config.source_format = 'text/csv'\n        job_config.autodetect=True\n        job = bigquery_client.load_table_from_file(\n            source_file, table_ref, job_config=job_config)\n\n    job.result()  # Waits for job to complete\n\n    print('Loaded {} rows into {}:{}.'.format(\n        job.output_rows, dataset_id, table_id))\n\n\nthank you!\n"
"I know there are tons of posts about this warning, but I couldn't find a solution to my situation. Here's my code:\n\ndf.loc[:, 'my_col'] = df.loc[:, 'my_col'].astype(int)\n#df.loc[:, 'my_col'] = df.loc[:, 'my_col'].astype(int).copy()\n#df.loc[:, 'my_col'] = df['my_col'].astype(int)\n\n\nIt produces the warning:\n\n\n  SettingWithCopyWarning:  A value is trying to be set on a copy of a\n  slice from a DataFrame. Try using .loc[row_indexer,col_indexer] =\n  value instead\n\n\nEven though I changed the code as suggested, I still get this warning? All I need to do is to convert the data type of one column. \n\n**Remark: ** Originally the column is of type float having one decimal (example: 4711.0). Therefore I change it to integer (4711) and then to string ('4711') - just to remove the decimal.\n\nAppreciate your help!\n\nUpdate: The warning was a side effect on a filtering of the original data that was done just before. I was missing the DataFrame.copy(). Using the copy instead, solved the problem!\n\ndf = df[df['my_col'].notnull()].copy()\ndf.loc[:, 'my_col'] = df['my_col'].astype(int).astype(str)\n#df['my_col'] = df['my_col'].astype(int).astype(str) # works too!\n\n"
'I am doing multiple linear regression with statsmodels.formula.api (ver 0.9.0) on Windows 10. After fitting the model and getting the summary with following lines i get summary in summary object format.\n\n\nX_opt  = X[:, [0,1,2,3]]\nregressor_OLS = sm.OLS(endog= y, exog= X_opt).fit()\nregressor_OLS.summary()\n\n\n                          OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.951\nModel:                            OLS   Adj. R-squared:                  0.948\nMethod:                 Least Squares   F-statistic:                     296.0\nDate:                Wed, 08 Aug 2018   Prob (F-statistic):           4.53e-30\nTime:                        00:46:48   Log-Likelihood:                -525.39\nNo. Observations:                  50   AIC:                             1059.\nDf Residuals:                      46   BIC:                             1066.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       5.012e+04   6572.353      7.626      0.000    3.69e+04    6.34e+04\nx1             0.8057      0.045     17.846      0.000       0.715       0.897\nx2            -0.0268      0.051     -0.526      0.602      -0.130       0.076\nx3             0.0272      0.016      1.655      0.105      -0.006       0.060\n==============================================================================\nOmnibus:                       14.838   Durbin-Watson:                   1.282\nProb(Omnibus):                  0.001   Jarque-Bera (JB):               21.442\nSkew:                          -0.949   Prob(JB):                     2.21e-05\nKurtosis:                       5.586   Cond. No.                     1.40e+06\n==============================================================================\n\n\nI want to do backward elimination for P values for significance level 0.05. For this i need to remove the predictor with highest P values and run the code again.\n\nI wanted to know if there is a way to extract the P values from the summary object, so that i can run a loop with conditional statement and find the significant variables without repeating the steps manually.\n\nThank you.\n'
"I have a  NxM dataframe and a NxL numpy matrix. I'd like to add the matrix to the dataframe to create L new columns by simply appending the columns and rows the same order they appear. I tried merge() and join(), but I end up with errors:\n\n\n  assign() keywords must be strings \n\n\nand \n\n\n  columns overlap but no suffix specified\n\n\nrespectively.\n\nIs there a way I can add a numpy matrix as dataframe columns?\n"
'I have a dataframe c with lots of different columns. Also, arr is a dataframe that corresponds to a subset of c: arr = c[c[\'A_D\'] == \'A\'].\n\nThe main idea of my code is to iterate over all rows in the c-dataframe and search for all the possible cases (in the arr dataframe) where some specific conditions should happen:\n\n\nIt is only necessary to iterate over rows were c[\'A_D\'] == D and c[\'Already_linked\'] == 0\nThe hour in the arr dataframe must be less than the hour_aux in the c dataframe\nThe column Already_linked of the arr dataframe must be zero: arr.Already_linked == 0\nThe Terminal and the Operator needs to be the same in the c and arr dataframe\n\n\nRight now, the conditions are stored using both Boolean indexing and groupby get_group:\n\n\nGroupby the arr dataframe in order to choose the same Operator and Terminal: g = groups.get_group((row.Operator, row.Terminal))\nChoose only the arrivals where the hour is smaller than the hour in the c dataframe and where Already_linked==0: vb = g[(g.Already_linked==0) &amp; (g.hour&lt;row.hour_aux)]\n\n\nFor each of the rows in the c dataframe that verify all conditions, a vb dataframe is created. Naturally, this dataframe has different lengths in each iteration. After creating the vb dataframe, my goal is to choose the index of the vb dataframe that minimises the time between vb.START and  c[x]. The FightID that corresponds to this index is then stored in the c dataframe on column a. Additionally, since the arrival was linked to a departure, the column Already_linked in the arr dataframe is changed from 0 to 1.\n\nIt is important to notice that the column Already_linked of the arr dataframe may change in every iteration (and arr.Already_linked == 0 is one of the conditions to create the vb dataframe). Therefore, it is not possible to parallelize this code.\n\nI have already used c.itertuples() for efficiency, however since c has millions of rows, this code is still too time consuming. \n\nOther option would also be to use pd.apply to every row. Nonetheless, this is not really straightforward since in each loop there are values that change in both c and arr (also, I believe that even with pd.apply it would be extremely slow).\n\nIs there any possible way to convert this for loop in a vectorized solution (or decrease the running time by 10X(if possible even more) )?  \n\nInitial dataframe:\n\nSTART     END       A_D     Operator     FlightID    Terminal   TROUND_ID   tot\n0   2017-03-26 16:55:00 2017-10-28 16:55:00 A   QR  QR001   4   QR002       70\n1   2017-03-26 09:30:00 2017-06-11 09:30:00 D   DL  DL001   3   "        "  84\n2   2017-03-27 09:30:00 2017-10-28 09:30:00 D   DL  DL001   3   "        "  78\n3   2017-10-08 15:15:00 2017-10-22 15:15:00 D   VS  VS001   3   "        "  45\n4   2017-03-26 06:50:00 2017-06-11 06:50:00 A   DL  DL401   3   "        "  9\n5   2017-03-27 06:50:00 2017-10-28 06:50:00 A   DL  DL401   3   "        "  19\n6   2017-03-29 06:50:00 2017-04-19 06:50:00 A   DL  DL401   3   "        "  3\n7   2017-05-03 06:50:00 2017-10-25 06:50:00 A   DL  DL401   3   "        "  32\n8   2017-06-25 06:50:00 2017-10-22 06:50:00 A   DL  DL401   3   "        "  95\n9   2017-03-26 07:45:00 2017-10-28 07:45:00 A   DL  DL402   3   "        "  58\n\n\nDesired Output (some of the columns were excluded in the dataframe below. Only the a and Already_linked columns are relevant):\n\n    START                    END             A_D  Operator  a   Already_linked\n0   2017-03-26 16:55:00 2017-10-28 16:55:00 A   QR  0               1\n1   2017-03-26 09:30:00 2017-06-11 09:30:00 D   DL  DL402           1\n2   2017-03-27 09:30:00 2017-10-28 09:30:00 D   DL  DL401           1\n3   2017-10-08 15:15:00 2017-10-22 15:15:00 D   VS  No_link_found   0\n4   2017-03-26 06:50:00 2017-06-11 06:50:00 A   DL  0               0\n5   2017-03-27 06:50:00 2017-10-28 06:50:00 A   DL  0               1\n6   2017-03-29 06:50:00 2017-04-19 06:50:00 A   DL  0               0\n7   2017-05-03 06:50:00 2017-10-25 06:50:00 A   DL  0               0\n8   2017-06-25 06:50:00 2017-10-22 06:50:00 A   DL  0               0\n9   2017-03-26 07:45:00 2017-10-28 07:45:00 A   DL  0               1\n\n\nCode:\n\ngroups = arr.groupby([\'Operator\', \'Terminal\'])\nfor row in c[(c.A_D == "D") &amp; (c.Already_linked == 0)].itertuples():\n    try:\n        g = groups.get_group((row.Operator, row.Terminal))\n        vb = g[(g.Already_linked==0) &amp; (g.hour&lt;row.hour_aux)]\n        aux = (vb.START - row.x).abs().idxmin()\n        c.loc[row.Index, \'a\'] = vb.loc[aux].FlightID\n        arr.loc[aux, \'Already_linked\'] = 1\n        continue\n    except:\n        continue\n\nc[\'Already_linked\'] = np.where((c.a != 0) &amp; (c.a != \'No_link_found\') &amp; (c.A_D == \'D\'), 1, c[\'Already_linked\'])\nc.Already_linked.loc[arr.Already_linked.index] = arr.Already_linked\nc[\'a\'] = np.where((c.Already_linked  == 0) &amp; (c.A_D == \'D\'),\'No_link_found\',c[\'a\'])\n\n\nCode for the initial c dataframe:\n\nimport numpy as np\nimport pandas as pd\nimport io\n\ns = \'\'\'\n A_D     Operator     FlightID    Terminal   TROUND_ID   tot\n A   QR  QR001   4   QR002       70\n D   DL  DL001   3   "        "  84\n D   DL  DL001   3   "        "  78\n D   VS  VS001   3   "        "  45\n A   DL  DL401   3   "        "  9\n A   DL  DL401   3   "        "  19\n A   DL  DL401   3   "        "  3\n A   DL  DL401   3   "        "  32\n A   DL  DL401   3   "        "  95\n A   DL  DL402   3   "        "  58\n\'\'\'\n\ndata_aux = pd.read_table(io.StringIO(s), delim_whitespace=True)\ndata_aux.Terminal = data_aux.Terminal.astype(str)\ndata_aux.tot= data_aux.tot.astype(str)\n\nd = {\'START\': [\'2017-03-26 16:55:00\', \'2017-03-26 09:30:00\',\'2017-03-27 09:30:00\',\'2017-10-08 15:15:00\',\n           \'2017-03-26 06:50:00\',\'2017-03-27 06:50:00\',\'2017-03-29 06:50:00\',\'2017-05-03 06:50:00\',\n           \'2017-06-25 06:50:00\',\'2017-03-26 07:45:00\'], \'END\': [\'2017-10-28 16:55:00\' ,\'2017-06-11 09:30:00\' ,\n           \'2017-10-28 09:30:00\' ,\'2017-10-22 15:15:00\',\'2017-06-11 06:50:00\' ,\'2017-10-28 06:50:00\', \n           \'2017-04-19 06:50:00\' ,\'2017-10-25 06:50:00\',\'2017-10-22 06:50:00\' ,\'2017-10-28 07:45:00\']}    \n\naux_df = pd.DataFrame(data=d)\naux_df.START = pd.to_datetime(aux_df.START)\naux_df.END = pd.to_datetime(aux_df.END)\nc = pd.concat([aux_df, data_aux], axis = 1)\nc[\'A_D\'] = c[\'A_D\'].astype(str)\nc[\'Operator\'] = c[\'Operator\'].astype(str)\nc[\'Terminal\'] = c[\'Terminal\'].astype(str)\n\nc[\'hour\'] = pd.to_datetime(c[\'START\'], format=\'%H:%M\').dt.time\nc[\'hour_aux\'] = pd.to_datetime(c[\'START\'] - pd.Timedelta(15, unit=\'m\'), \nformat=\'%H:%M\').dt.time\nc[\'start_day\'] = c[\'START\'].astype(str).str[0:10]\nc[\'end_day\'] = c[\'END\'].astype(str).str[0:10]\nc[\'x\'] = c.START -  pd.to_timedelta(c.tot.astype(int), unit=\'m\')\nc["a"] = 0\nc["Already_linked"] = np.where(c.TROUND_ID != "        ", 1 ,0)\n\narr = c[c[\'A_D\'] == \'A\']\n\n'
"I have a script that assigns a value based off two columns in a pandas df. The code below is able to implement the 1st step but I'm struggling with the second.\n\nSo the script should initially:\n\n1) Assign a Person for each individual string in [Area] and the first 3 unique values in [Place]\n\n2) Look to reassign People with less than 3 unique values\nExample. The df below have 6 unique values in [Area] and [Place]. But 3 People are assigned. Ideally, 2 people will 2 unique values each\n\nd = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','10:15:00','10:15:00','11:48:00','12:00:00','12:10:00'],                 \n   'Place' : ['House 1','House 2','House 1','House 3','House 4','House 5','House 1','House 1'],                 \n    'Area' : ['X','X','Y','X','X','X','X','X'],    \n     })\n\ndf = pd.DataFrame(data=d)\n\ndef g(gps):\n        s = gps['Place'].unique()\n        d = dict(zip(s, np.arange(len(s)) // 3 + 1))\n        gps['Person'] = gps['Place'].map(d)\n        return gps\n\ndf = df.groupby('Area', sort=False).apply(g)\ns = df['Person'].astype(str) + df['Area']\ndf['Person'] = pd.Series(pd.factorize(s)[0] + 1).map(str).radd('Person ')\n\n\nOutput:\n\n       Time    Place Area    Person\n0   8:03:00  House 1    X  Person 1\n1   8:17:00  House 2    X  Person 1\n2   8:20:00  House 1    Y  Person 2\n3  10:15:00  House 3    X  Person 1\n4  10:15:00  House 4    X  Person 3\n5  11:48:00  House 5    X  Person 3\n6  12:00:00  House 1    X  Person 1\n7  12:10:00  House 1    X  Person 1\n\n\nAs you can see, the first step works fine. or each individual string in [Area], the first 3 unique values in [Place] are assigned to a Person. This leaves Person 1 with 3 values, Person 2 with 1 value and Person 3 with 2 values.\n\nThe second step is where I'm struggling. \n\nIf a Person has less than 3 unique values assigned to them, alter this so each Person has up to 3 unique values\n\nIntended Output:\n\n       Time    Place Area    Person\n0   8:03:00  House 1    X  Person 1\n1   8:17:00  House 2    X  Person 1\n2   8:20:00  House 1    Y  Person 2\n3  10:15:00  House 3    X  Person 1\n4  10:15:00  House 4    X  Person 2\n5  11:48:00  House 5    X  Person 2\n6  12:00:00  House 1    X  Person 1\n7  12:10:00  House 1    X  Person 1\n\n"
'I have a data frame df with thousands of rows, and a sample is this:\n\n    Index           A   B   C   D   E   F               \n    EX-A.1.A.B-1A  18   7   2   2   9   8       \n    EX-A.1.A.B-1C   0   0   0   0   0   0       \n    EX-A.1.A.B-4A   6   4   8   6   1   1   \n    EX-A.1.A.B-4C   0   0   0   0   0   0   \n    EX-A.1.A.B-4F   0   0   0   0   0   0\n\n\nI also have a list my_list = ["EX-A.1.A.B-1A","EX-A.1.A.B-4A","EX-A.1.A.B-4F"]\n\nand I want to filter the df based on this list, therefore I want to keep the rows for which the index value is in the list my_list.\n\nI tried this in order to create a new filtered df: Filter_df = df[df.index in my_list] and I get this error:\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all().\n\n\nAny ideas on how I could do this? Thanks\n'
"From the official documentation of pandas.to_datetime we can say,\n\nunit : string, default ‘ns’\n\n\n\n  unit of the arg (D,s,ms,us,ns) denote the unit, which is an integer or\n  float number. This will be based off the origin. Example, with\n  unit=’ms’ and origin=’unix’ (the default), this would calculate the\n  number of milliseconds to the unix epoch start.\n\n\nSo when I try like this way,\n\nimport pandas as pd\ndf = pd.DataFrame({'time': [pd.to_datetime('2019-01-15 13:25:43')]})\ndf_unix_sec = pd.to_datetime(df['time'],unit='ms',origin='unix')\nprint(df)\nprint(df_unix_sec)\n\n                 time\n0   2019-01-15 13:25:43\n0   2019-01-15 13:25:43\nName: time, dtype: datetime64[ns]\n\n\nOutput is not changing for the later one. Every time it is showing the datetime value not  number of milliseconds to the unix epoch start for the 2nd one. Why is that? Am I missing something? \n"
'I have a Dataframe like:\n\nSequence    Duration1   Value1  Duration2   Value2  Duration3   Value3\n1001        145         10      125         53      458         33\n1002        475         20      175         54      652         45\n1003        685         57      687         87      254         88\n1004        125         54      175         96      786         96\n1005        475         21      467         32      526         32\n1006        325         68      301         54      529         41\n1007        125         97      325         85      872         78\n1008        129         15      429         41      981         82\n1009        547         47      577         52      543         83\n1010        666         65      722         63      257         87\n\n\nI want to find the maximum value of Duration in (Duration1,Duration2,Duration3) and return the corresponding Value &amp; Sequence.\n\nMy Desired Output:\n\nSequence,Duration3,Value3\n1008,    981,      82\n\n'
'I have two pandas data frames, a and b:\n\na1   a2   a3   a4   a5   a6   a7\n1    3    4    5    3    4    5\n0    2    0    3    0    2    1\n2    5    6    5    2    1    2\n\n\nand\n\nb1   b2   b3   b4   b5   b6   b7\n3    5    4    5    1    4    3\n0    1    2    3    0    0    2\n2    2    1    5    2    6    5\n\n\nThe two data frames contain exactly the same data, but in a different order and with different column names. Based on the numbers in the two data frames, I would like to be able to match each column name in a to each column name in b.\n\nIt is not as easy as simply comparing the first row of a with the first row of b as there are duplicated values, for example both a4 and a7 have the value 5 so it is not possible to immediately match them to either b2 or b4.\n\nWhat is the best way to do this?\n'
"I am trying to transform DataFrame, such that some of the rows will be replicated a given number of times. For example:\n\ndf = pd.DataFrame({'class': ['A', 'B', 'C'], 'count':[1,0,2]})\n\n  class  count\n0     A      1\n1     B      0\n2     C      2\n\n\nshould be transformed to:\n\n  class \n0     A   \n1     C   \n2     C \n\n\nThis is the reverse of aggregation with count function. Is there an easy way to achieve it in pandas (without using for loops or list comprehensions)?   \n\nOne possibility might be to allow DataFrame.applymap function return multiple rows (akin apply method of GroupBy). However, I do not think it is possible in pandas now.\n"
'I have a Pandas dataframe which is indexed by a DatetimeIndex:\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 53732 entries, 1993-01-07 12:23:58 to 2012-12-02 20:06:23\nData columns:\nDate(dd-mm-yy)_Time(hh-mm-ss)       53732  non-null values\nJulian_Day                          53732  non-null values\nAOT_870                             53732  non-null values\n440-870Angstrom                     53732  non-null values\n440-675Angstrom                     53732  non-null values\n500-870Angstrom                     53732  non-null values\nLast_Processing_Date(dd/mm/yyyy)    53732  non-null values\nSolar_Zenith_Angle                  53732  non-null values\ntime                                53732  non-null values\ndtypes: datetime64[ns](2), float64(6), object(1)\n\n\nI want to find the row that is closest to a certain time:\n\nimage_time = dateutil.parser.parse(\'2009-07-28 13:39:02\')\n\n\nand find how close it is. So far, I have tried various things based upon the idea of subtracting the time I want from all of the times and finding the smallest absolute value, but none quite seem to work.\n\nFor example:\n\naeronet.index - image_time\n\n\nGives an error which I think is due to +/- on a Datetime index shifting things, so I tried putting the index into another column and then working on that:\n\naeronet[\'time\'] = aeronet.index\naeronet.time - image_time\n\n\nThis seems to work, but to do what I want, I need to get the ABSOLUTE time difference, not the relative difference. However, just running abs or np.abs on it gives an error:\n\nabs(aeronet.time - image_time)\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __repr__(self)\n   1061         Yields Bytestring in Py2, Unicode String in py3.\n   1062         """\n-&gt; 1063         return str(self)\n   1064 \n   1065     def _tidy_repr(self, max_vals=20):\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __str__(self)\n   1021         if py3compat.PY3:\n   1022             return self.__unicode__()\n-&gt; 1023         return self.__bytes__()\n   1024 \n   1025     def __bytes__(self):\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __bytes__(self)\n   1031         """\n   1032         encoding = com.get_option("display.encoding")\n-&gt; 1033         return self.__unicode__().encode(encoding, \'replace\')\n   1034 \n   1035     def __unicode__(self):\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in __unicode__(self)\n   1044                     else get_option("display.max_rows"))\n   1045         if len(self.index) &gt; (max_rows or 1000):\n-&gt; 1046             result = self._tidy_repr(min(30, max_rows - 4))\n   1047         elif len(self.index) &gt; 0:\n   1048             result = self._get_repr(print_header=True,\n\nC:\\Python27\\lib\\site-packages\\pandas\\core\\series.pyc in _tidy_repr(self, max_vals)\n   1069         """\n   1070         num = max_vals // 2\n-&gt; 1071         head = self[:num]._get_repr(print_header=True, length=False,\n   1072                                     name=False)\n   1073         tail = self[-(max_vals - num):]._get_repr(print_header=False,\n\nAttributeError: \'numpy.ndarray\' object has no attribute \'_get_repr\'\n\n\nAm I approaching this the right way? If so, how should I get abs to work, so that I can then select the minimum absolute time difference, and thus get the closest time. If not, what is the best way to do this with a Pandas time-series?\n'
"I have a csv file that shows parts on order. The columns include days late, qty and commodity.\n\nI need to group the data by days late and commodity with a sum of the qty. However the days late needs to be grouped into ranges.\n\n&gt;56\n&gt;35 and &lt;= 56\n&gt;14 and &lt;= 35\n&gt;0 and &lt;=14\n\n\nI was hoping I could use a dict some how. Something like this \n\n{'Red':'&gt;56,'Amber':'&gt;35 and &lt;= 56','Yellow':'&gt;14 and &lt;= 35','White':'&gt;0 and &lt;=14'}\n\n\nI am looking for a result like this\n\n        Red  Amber  Yellow  White\nSTRSUB  56   60     74      40\nBOTDWG  20   67     87      34\n\n\nI am new to pandas so I don't know if this is possible at all. Could anyone provide some advice.\n\nThanks\n"
"I am trying to classify my data in percentile buckets based on their values. My data looks like, \n\na = pnd.DataFrame(index = ['a','b','c','d','e','f','g','h','i','j'], columns=['data'])\na.data = np.random.randn(10)\nprint a\nprint '\\nthese are ranked as shown'\nprint a.rank()\n\n       data\na -0.310188\nb -0.191582\nc  0.860467\nd -0.458017\ne  0.858653\nf -1.640166\ng -1.969908\nh  0.649781\ni  0.218000\nj  1.887577\n\nthese are ranked as shown\n   data\na     4\nb     5\nc     9\nd     3\ne     8\nf     2\ng     1\nh     7\ni     6\nj    10\n\n\nTo rank this data, I am using the rank function. However, I am interested in the creating a bucket of the top 20%. In the example shown above, this would be a list containing labels ['c', 'j']\n\ndesired result : ['c','j']\n\n\nHow do I get the desired result\n"
"How do I convert an existing dataframe with single-level columns to have hierarchical index columns (MultiIndex)?\n\nExample dataframe:\n\nIn [1]:\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\ndf = DataFrame(np.arange(6).reshape((2,3)),\n               index=['A','B'],\n               columns=['one','two','three'])\ndf\nOut [1]:\n   one  two  three\nA    0    1      2\nB    3    4      5\n\n\nI'd have thought that reindex() would work, but I get NaN's:\n\nIn [2]:\ndf.reindex(columns=[['odd','even','odd'],df.columns])\nOut [2]:\n   odd  even    odd\n   one   two  three\nA  NaN   NaN    NaN\nB  NaN   NaN    NaN\n\n\nSame if I use DataFrame():\n\nIn [3]:\nDataFrame(df,columns=[['odd','even','odd'],df.columns])\nOut [3]:\n   odd  even    odd\n   one   two  three\nA  NaN   NaN    NaN\nB  NaN   NaN    NaN\n\n\nThis last approach actually does work if I specify df.values:\n\nIn [4]:\nDataFrame(df.values,index=df.index,columns=[['odd','even','odd'],df.columns])\nOut [4]:\n   odd  even    odd\n   one   two  three\nA    0     1      2\nB    3     4      5\n\n\nWhat's the proper way to do this? Why does reindex() give NaN's?\n"
"Is there a way to omit some of the output from the pandas describe?\nThis command gives me exactly what I want with a table output (count and mean of executeTime's by a simpleDate)\n\ndf.groupby('simpleDate').executeTime.describe().unstack(1)\n\n\nHowever that's all I want, count and mean. I want to drop std, min, max, etc... So far I've only read how to modify column size.\n\nI'm guessing the answer is going to be to re-write the line, not using describe, but I haven't had any luck grouping by simpleDate and getting the count with a mean on executeTime.\n\nI can do count by date:\n\ndf.groupby(['simpleDate']).size()\n\n\nor executeTime by date:\n\ndf.groupby(['simpleDate']).mean()['executeTime'].reset_index()\n\n\nBut can't figure out the syntax to combine them.\n\nMy desired output:\n\n            count  mean  \n09-10-2013      8  20.523   \n09-11-2013      4  21.112  \n09-12-2013      3  18.531\n...            ..  ...\n\n"
"I want to create a simple bar chart for pandas DataFrame object. However, the xtick on the chart appears to be too granular, whereas if I change the plot to line chart, xtick is optimized for better viewing. I was wondering if I can bring the same line chart xtick frequency to bar chart? Thanks.\n\nlocks.plot(kind='bar',y='SUM')\n\n\nEDIT\n\nResultant plot:\n"
"I have a pandas DataFrame that looks like this training.head()\n\n\n\nThe DataFrame has been sorted by date. I'd like to make a scatterplot where the date of the campaign is on the x axis and the rate of success is on the y axis. I was able to get a line graph by using training.plot(x='date',y='rate'). However, when I changed that to training.plot(kind='scatter',x='date',y='rate') I get an error: KeyError: u'no item named date'\n\nWhy does my index column go away when I try to make a scatterplot? Also, I bet I need to do something with that date field so that it doesn't get treated like a simple string, don't I?\n\nExtra credit, what would I do if I wanted each of the account numbers to plot with a different color?\n"
"Variations of this question have been asked before, I'm still having trouble understanding how  to actually slice a python series/pandas dataframe based on conditions that I'd like to set.\n\nIn R, what I'm trying to do is:\n\ndf[which(df[,colnumber] &gt; somenumberIchoose),]\n\n\nThe which() function finds indices of row entries in a column in the dataframe which are greater than somenumberIchoose, and returns this as a vector.  Then, I slice the dataframe by using these row indices to indicate which rows of the dataframe I would like to look at in the new form.\n\nIs there an equivalent way to do this in python? I've seen references to enumerate, which I don't fully understand after reading the documentation.  My sample in order to get the row indices right now looks like this:\n\nindexfuture = [ x.index(), x in enumerate(df['colname']) if x &gt; yesterday]  \n\n\nHowever, I keep on getting an invalid syntax error.  I can hack a workaround by for looping through the values, and manually doing the search myself, but that seems extremely non-pythonic and inefficient.\n\nWhat exactly does enumerate() do?  What is the pythonic way of finding indices of values in a vector that fulfill desired parameters?\n\nNote: I'm using Pandas for the dataframes\n"
'I have a data frame with a column containing Investment which represents the amount invested by a trader. I would like to create 2 new columns in the data frame; one giving a decile rank and the other a quintile rank based on the Investment size. I want 1 to represent the decile with the largest Investments and 10 representing the smallest. Smilarly, I want 1 to represent the quintile with the largest investments and 5 representing the smallest.\n\nI am new to Pandas, so is there a way that I can easily do this? \nThanks!\n'
"Here is a time series data like this,call it df:\n\n      'No'       'Date'       'Value'\n0     600000     1999-11-10    1\n1     600000     1999-11-11    1\n2     600000     1999-11-12    1\n3     600000     1999-11-15    1\n4     600000     1999-11-16    1\n5     600000     1999-11-17    1\n6     600000     1999-11-18    0\n7     600000     1999-11-19    1\n8     600000     1999-11-22    1\n9     600000     1999-11-23    1\n10    600000     1999-11-24    1\n11    600000     1999-11-25    0\n12    600001     1999-11-26    1\n13    600001     1999-11-29    1\n14    600001     1999-11-30    0\n\n\nI want to get the date range of the consecutive 'Value' of 1, so how can I get the final result as follows:\n\n   'No'     'BeginDate'    'EndDate'   'Consecutive'\n0 600000    1999-11-10    1999-11-17    6\n1 600000    1999-11-19    1999-11-24    4\n2 600001    1999-11-26    1999-11-29    2\n\n"
"I have a dataframe with some columns containing nan. I'd like to drop those columns with certain number of nan. For example, in the following code, I'd like to drop any column with 2 or more nan. In this case, column 'C' will be dropped and only 'A' and 'B' will be kept. How can I implement it?\n\nimport pandas as pd\nimport numpy as np\n\ndff = pd.DataFrame(np.random.randn(10,3), columns=list('ABC'))\ndff.iloc[3,0] = np.nan\ndff.iloc[6,1] = np.nan\ndff.iloc[5:8,2] = np.nan\n\nprint dff\n\n"
"Can anybody explain why is loc used in python pandas with examples like shown below?\n\nfor i in range(0, 2):\n  for j in range(0, 3):\n    df.loc[(df.Age.isnull()) &amp; (df.Gender == i) &amp; (df.Pclass == j+1),\n            'AgeFill'] = median_ages[i,j]\n\n"
'I am new to python and have recently learnt to create a series in python using Pandas. I can define a series eg:  x = pd.Series([1, 2, 3, 4, 5]) but how to define the series for a range, say 1 to 100 rather than typing all elements from 1 to 100? \n'
"I have a pandas dataframe with three columns and I am plotting each column separately using the following code:\n\ndata.plot(y='value')\n\n\nWhich generates a figure like this one:\n\n\n\nWhat I need is a subset of these values and not all of them. For example, I want to plot values at rows 500 to 1000 and not from 0 to 3500. Any idea how I can tell the plot function to only pick those?\n\nThanks\n"
"I am struggling to massage a dataframe in pandas into the correct format for seaborn's heatmap (or matplotlib really) to make a heatmap.\n\nMy current dataframe (called data_yule) is:\n\n     Unnamed: 0  SymmetricDivision         test  MutProb      value\n3             3                1.0  sackin_yule    0.100  -4.180864\n8             8                1.0  sackin_yule    0.050  -9.175349\n13           13                1.0  sackin_yule    0.010 -11.408114\n18           18                1.0  sackin_yule    0.005 -10.502450\n23           23                1.0  sackin_yule    0.001  -8.027475\n28           28                0.8  sackin_yule    0.100  -0.722602\n33           33                0.8  sackin_yule    0.050  -6.996394\n38           38                0.8  sackin_yule    0.010 -10.536340\n43           43                0.8  sackin_yule    0.005  -9.544065\n48           48                0.8  sackin_yule    0.001  -7.196407\n53           53                0.6  sackin_yule    0.100  -0.392256\n58           58                0.6  sackin_yule    0.050  -6.621639\n63           63                0.6  sackin_yule    0.010  -9.551801\n68           68                0.6  sackin_yule    0.005  -9.292469\n73           73                0.6  sackin_yule    0.001  -6.760559\n78           78                0.4  sackin_yule    0.100  -0.652147\n83           83                0.4  sackin_yule    0.050  -6.885229\n88           88                0.4  sackin_yule    0.010  -9.455776\n93           93                0.4  sackin_yule    0.005  -8.936463\n98           98                0.4  sackin_yule    0.001  -6.473629\n103         103                0.2  sackin_yule    0.100  -0.964818\n108         108                0.2  sackin_yule    0.050  -6.051482\n113         113                0.2  sackin_yule    0.010  -9.784686\n118         118                0.2  sackin_yule    0.005  -8.571063\n123         123                0.2  sackin_yule    0.001  -6.146121\n\n\nand my attempts using matplotlib was:\n\nplt.pcolor(data_yule.SymmetricDivision, data_yule.MutProb, data_yule.value)\n\n\nwhich threw the error: \n\nValueError: not enough values to unpack (expected 2, got 1)\n\n\nand the seaborn attempt was:\n\nsns.heatmap(data_yule.SymmetricDivision, data_yule.MutProb, data_yule.value)\n\n\nwhich threw:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n\nIt seems trivial as both functions want rectangular dataset, but I'm missing something, clearly. \n"
'How do I write out a large data file to a CSV file in chunks?\n\nI have a set of large data files (1M rows x 20 cols). However, only 5 or so columns of that data is of interest to me. \n\nI want to make things easier by making copies of these files with only the columns of interest so I have smaller files to work with for post-processing. So I plan to read the file into a dataframe, then write to csv file. \n\nI\'ve been looking into reading large data files in chunks into a dataframe. However, I haven\'t been able to find anything on how to write out the data to a csv file in chunks.\n\nHere is what I\'m trying now, but this doesn\'t append the csv file:\n\nwith open(os.path.join(folder, filename), \'r\') as src:\n    df = pd.read_csv(src, sep=\'\\t\',skiprows=(0,1,2),header=(0), chunksize=1000)\n    for chunk in df:\n        chunk.to_csv(os.path.join(folder, new_folder,\n                                  "new_file_" + filename), \n                                  columns = [[\'TIME\',\'STUFF\']])\n\n'
"With:\n\nimport pandas as pd\ndf = pd.read_csv('pima-data.csv')\nprint df.head(2)\n\n\nthe print is automatically formatted across multiple lines:\n\n   num_preg  glucose_conc  diastolic_bp  thickness  insulin   bmi  diab_pred  \\\n0         6           148            72         35        0  33.6      0.627   \n1         1            85            66         29        0  26.6      0.351   \n\n   age    skin diabetes  \n0   50  1.3790     True  \n1   31  1.1426    False \n\n\nI wonder if there is a way to avoid the multi-line formatting. I would rather have it printed in a single line like so:\n\n   num_preg  glucose_conc  diastolic_bp  thickness  insulin       bmi      diab_pred     age       skin      diabetes  \n0         6           148            72         35        0      33.6          0.627      50     1.3790          True  \n1         1            85            66         29        0      26.6          0.351      31     1.1426         False \n\n"
'I have the following in a Pandas DataFrame in Python 2.7:\n\nSer_Numb        LAT      LONG\n       1  74.166061 30.512811\n       2  72.249672 33.427724\n       3  67.499828 37.937264\n       4  84.253715 69.328767\n       5  72.104828 33.823462\n       6  63.989462 51.918173\n       7  80.209112 33.530778\n       8  68.954132 35.981256\n       9  83.378214 40.619652\n       10 68.778571 6.607066\n\n\nI am looking to calculate the distance between successive rows in the dataframe. The output should look something like this:\n\nSer_Numb          LAT        LONG   Distance\n       1    74.166061   30.512811          0\n       2    72.249672   33.427724          d_between_Ser_Numb2 and Ser_Numb1\n       3    67.499828   37.937264          d_between_Ser_Numb3 and Ser_Numb2\n       4    84.253715   69.328767          d_between_Ser_Numb4 and Ser_Numb3\n       5    72.104828   33.823462          d_between_Ser_Numb5 and Ser_Numb4\n       6    63.989462   51.918173          d_between_Ser_Numb6 and Ser_Numb5\n       7    80.209112   33.530778   .\n       8    68.954132   35.981256   .\n       9    83.378214   40.619652   .\n       10   68.778571   6.607066    .\n\n\nAttempt\n\nThis post looks somewhat similar but it is calculating the distance between fixed points. I need the distance between successive points.\n\nI tried to adapt this as follows:\n\ndf[\'LAT_rad\'], df[\'LON_rad\'] = np.radians(df[\'LAT\']), np.radians(df[\'LONG\'])\ndf[\'dLON\'] = df[\'LON_rad\'] - np.radians(df[\'LON_rad\'].shift(1))\ndf[\'dLAT\'] = df[\'LAT_rad\'] - np.radians(df[\'LAT_rad\'].shift(1))\ndf[\'distance\'] = 6367 * 2 * np.arcsin(np.sqrt(np.sin(df[\'dLAT\']/2)**2 + math.cos(df[\'LAT_rad\'].astype(float).shift(-1)) * np.cos(df[\'LAT_rad\']) * np.sin(df[\'dLON\']/2)**2))\n\n\nHowever, I get the following error:\n\nTraceback (most recent call last):\n  File "C:\\Python27\\test.py", line 115, in &lt;module&gt;\n    df[\'distance\'] = 6367 * 2 * np.arcsin(np.sqrt(np.sin(df[\'dLAT\']/2)**2 + math.cos(df[\'LAT_rad\'].astype(float).shift(-1)) * np.cos(df[\'LAT_rad\']) * np.sin(df[\'dLON\']/2)**2))\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\series.py", line 78, in wrapper\n    "{0}".format(str(converter)))\nTypeError: cannot convert the series to &lt;type \'float\'&gt;\n[Finished in 2.3s with exit code 1]\n\n\nThis error was fixed from MaxU\'s comment. With the fix, the output of this calculation is not making sense - the distance is nearly 8000 km:\n\n   Ser_Numb        LAT       LONG   LAT_rad   LON_rad      dLON      dLAT     distance\n0         1  74.166061  30.512811  1.294442  0.532549       NaN       NaN          NaN\n1         2  72.249672  33.427724  1.260995  0.583424  0.574129  1.238402  8010.487211\n2         3  67.499828  37.937264  1.178094  0.662130  0.651947  1.156086  7415.364469\n3         4  84.253715  69.328767  1.470505  1.210015  1.198459  1.449943  9357.184623\n4         5  72.104828  33.823462  1.258467  0.590331  0.569212  1.232802  7992.087820\n5         6  63.989462  51.918173  1.116827  0.906143  0.895840  1.094862  7169.812123\n6         7  80.209112  33.530778  1.399913  0.585222  0.569407  1.380421  8851.558260\n7         8  68.954132  35.981256  1.203477  0.627991  0.617777  1.179044  7559.609520\n8         9  83.378214  40.619652  1.455224  0.708947  0.697986  1.434220  9194.371978\n9        10  68.778571   6.607066  1.200413  0.115315  0.102942  1.175014          NaN\n\n\nAccording to:\n\n\nthis online calculator: If I use Latitude1 = 74.166061,\nLongitude1 = 30.512811, Latitude2 = 72.249672, Longitude2 = 33.427724\nthen I get 233 km\nhaversine function found\nhere as: print haversine(30.512811, 74.166061, 33.427724, 72.249672) then I\nget 232.55 km\n\n\nThe answer should be 233 km, but my approach is giving ~8000 km. I think there is something wrong with how I am trying to iterate between successive rows.\n\nQuestion:\nIs there a way to do this in Pandas? Or do I need to loop through the dataframe one row at a time?\n\nAdditional Information:\n\nTo create the above DF, select it and copy to clipboard. Then:\n\nimport pandas as pd\ndf = pd.read_clipboard()\nprint df\n\n'
'shift converts my column from integer to float. It turns out that np.nan is float only. Is there any ways to keep shifted column as integer?\n\ndf = pd.DataFrame({"a":range(5)})\ndf[\'b\'] = df[\'a\'].shift(1)\n\ndf[\'a\']\n# 0    0\n# 1    1\n# 2    2\n# 3    3\n# 4    4\n# Name: a, dtype: int64\n\ndf[\'b\']\n\n# 0   NaN\n# 1     0\n# 2     1\n# 3     2\n# 4     3\n# Name: b, dtype: float64\n\n'
"I am trying to display a pair plot by creating from scatter_matrix in pandas dataframe. This is how the pair plot is created:\n\n# Create dataframe from data in X_train\n# Label the columns using the strings in iris_dataset.feature_names\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n# Create a scatter matrix from the dataframe, color by y_train\ngrr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\nhist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\n\n\nI want to display the pair plot to look something like this;\n\n\n\nI am using Python v3.6 and PyCharm and am not using Jupyter Notebook.\n"
"After grouping and counting a dataframe I'm trying to remove the multiindex like this:\ndf = df[['CID','FE', 'FID']].groupby(by=['CID','FE']).count()\n              .unstack().reset_index()\n\nPrinting the columns (df.colums) shows that it is still a MultiIndex.\nMultiIndex(levels=[['FID', 'CID'], [...]]\n\nI can't access the column CID via df['CID'].\n"
'I have a problem with the line below self.tableView.set??????????(df) that supposed to display the data frame in PyQt5. I put ??? there where I am missing the code I need.\n\ndef btn_clk(self):\n        path = self.lineEdit.text()\n        df = pd.read_csv(path)\n        self.tableView.set??????????(df)\n\n\nThe rest of the code works, because if I use print(df) in the above code, the data frame is printed in the IPython console. So, Pandas reads the CSV and prints it.\n\nBut, I tried many things to get it displayed in PyQt5 and nothing works. I am not very familiar with PyQt, just started to play around with it and I am stuck here.\n\nHere is my code:\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\nimport pandas as pd\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName("MainWindow")\n        MainWindow.resize(662, 512)\n        self.centralwidget = QtWidgets.QWidget(MainWindow)\n        self.centralwidget.setObjectName("centralwidget")\n        self.horizontalLayout = QtWidgets.QHBoxLayout(self.centralwidget)\n        self.horizontalLayout.setObjectName("horizontalLayout")\n        self.verticalLayout = QtWidgets.QVBoxLayout()\n        self.verticalLayout.setObjectName("verticalLayout")\n        self.lineEdit = QtWidgets.QLineEdit(self.centralwidget)\n        self.lineEdit.setObjectName("lineEdit")\n        self.verticalLayout.addWidget(self.lineEdit)\n        self.tableView = QtWidgets.QTableView(self.centralwidget)\n        self.tableView.setObjectName("tableView")\n        self.verticalLayout.addWidget(self.tableView)\n        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n        self.pushButton.setObjectName("pushButton")\n        self.verticalLayout.addWidget(self.pushButton)\n        self.horizontalLayout.addLayout(self.verticalLayout)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtWidgets.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 662, 21))\n        self.menubar.setObjectName("menubar")\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtWidgets.QStatusBar(MainWindow)\n        self.statusbar.setObjectName("statusbar")\n        MainWindow.setStatusBar(self.statusbar)\n\n        self.retranslateUi(MainWindow)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        _translate = QtCore.QCoreApplication.translate\n        MainWindow.setWindowTitle(_translate("MainWindow", "MainWindow"))\n        self.pushButton.setText(_translate("MainWindow", "PushButton"))\n\n\n        self.pushButton.clicked.connect(self.btn_clk)\n\n        MainWindow.show()\n\n    def btn_clk(self):\n        path = self.lineEdit.text()\n        df = pd.read_csv(path)\n        self.tableView.set????????????(df)\n\n\nif __name__ == "__main__":\n    import sys\n    app = QtWidgets.QApplication(sys.argv)\n    MainWindow = QtWidgets.QMainWindow()\n    ui = Ui_MainWindow()\n    ui.setupUi(MainWindow)\n    MainWindow.show()\n    sys.exit(app.exec_())\n\n'
"I've searched for an answer for the past 30 min, but the only solutions are either for a single column or in R. I have a dataset in which I want to change the ('Y/N') values to 1 and 0 respectively. I feel like copying and pasting the code below 17 times is very inefficient. \n\ndf.loc[df.infants == 'n', 'infants'] = 0\ndf.loc[df.infants == 'y', 'infants'] = 1\ndf.loc[df.infants == '?', 'infants'] = 1\n\n\nMy solution is the following. This doesn't cause an error, but the values in the dataframe doesn't change. I'm assuming I need to do something like df = df_new. But how to do this? \n\nfor coln in df:\nfor value in coln: \n        if value == 'y':\n            value = '1'\n        elif value == 'n':\n            value = '0'\n        else: \n            value = '1'\n\n\nEDIT: There are 17 columns in this dataset, but there is another dataset I'm hoping to tackle which contains 56 columns. \n\nrepublican  n   y   n.1 y.1 y.2 y.3 n.2 n.3 n.4 y.4 ?   y.5 y.6 y.7 n.5 y.8\n0   republican  n   y   n   y   y   y   n   n   n   n   n   y   y   y   n   ?\n1   democrat    ?   y   y   ?   y   y   n   n   n   n   y   n   y   y   n   n\n2   democrat    n   y   y   n   ?   y   n   n   n   n   y   n   y   n   n   y\n3   democrat    y   y   y   n   y   y   n   n   n   n   y   ?   y   y   y   y\n4   democrat    n   y   y   n   y   y   n   n   n   n   n   n   y   y   y   y\n\n"
"I'm using seaborn to plot some biology data. \n\nI just want a distribution one gene against another (expression in ~300 patients), and that's all worked fine and dandy with graph = sns.jointplot(x='Gene1',y='Gene2',data=data,kind='reg')\n\nI like that the graph gives me a nice linear fit and a PearsonR and a P value. \n\n\n\nAll I want is to plot my data on a log scale, which is the way that such gene data is usually represented. \n\nI've looked at a few solutions online, but they all get rid of my PearsonR value or my linear fit or they just don't look as good. I'm new to this, but it seems like graphing on a log scale shouldn't be too much trouble.\n\nAny comments or solutions?\n\nThanks!\n\nEdit: In response to comments, I've gotten closer to my answer. I now have a plot (shown below), but I need a line of fit and to do some statistics. Working on that now, but any answers/suggestions in the meantime are more than welcome.\n\n\n"
'How to plot a histogram with pandas DataFrame.hist() using group by?\nI have a data frame with 5 columns: "A", "B", "C", "D" and "Group"\n\nThere are two Groups classes: "yes" and "no"\n\nUsing:\n\ndf.hist() \n\n\nI get the hist for each of the 4 columns.\n\n\n\nNow I would like to get the same 4 graphs but with blue bars (group="yes") and red bars (group = "no").\n\nI tried this withouth success:\n\ndf.hist(by = "group")\n\n\n\n'
"Given a Series like\n\nimport pandas as pd\n\ns = pd.Series(['foo', 'bar', 42])\n\n\nI would like to obtain a 'sub-series' pd.Series(['foo', 'bar']) in which all values are strings. I've tried Boolean indexing like so:\n\ns[isinstance(s, str)]\n\n\nbut this gives a\n\n\n  KeyError: False\n\n\nIn my search for suitable methods so far I came across select, but this imposes a criterion on the labels, not the values. How can I filter based on (the type of) the values in this case?\n"
"I need to get the frequency of each element in a list when the list is in a pandas data frame columns \n\nIn data:\n\ndin=pd.DataFrame({'x':[['a','b','c'],['a','e','d', 'c']]})`\n\n              x\n0     [a, b, c]\n1  [a, e, d, c]\n\n\nDesired Output:\n\n   f  x\n0  2  a\n1  1  b\n2  2  c\n3  1  d\n4  1  e\n\n\nI can expand the list into rows and then perform a group by but this data could be large ( million plus records ) and was wondering if there is a more efficient/direct way.\n\nThanks\n"
"Answering this question it turned out that df.groupby(...).agg(set) and df.groupby(...).agg(lambda x: set(x)) are producing different results.\n\nData:\n\ndf = pd.DataFrame({\n       'user_id': [1, 2, 3, 4, 1, 2, 3], \n       'class_type': ['Krav Maga', 'Yoga', 'Ju-jitsu', 'Krav Maga', \n                      'Ju-jitsu','Krav Maga', 'Karate'], \n       'instructor': ['Bob', 'Alice','Bob', 'Alice','Alice', 'Alice','Bob']})\n\n\nDemo:\n\nIn [36]: df.groupby('user_id').agg(lambda x: set(x))\nOut[36]:\n                    class_type    instructor\nuser_id\n1        {Krav Maga, Ju-jitsu}  {Alice, Bob}\n2            {Yoga, Krav Maga}       {Alice}\n3           {Ju-jitsu, Karate}         {Bob}\n4                  {Krav Maga}       {Alice}\n\nIn [37]: df.groupby('user_id').agg(set)\nOut[37]:\n                                class_type                         instructor\nuser_id\n1        {user_id, class_type, instructor}  {user_id, class_type, instructor}\n2        {user_id, class_type, instructor}  {user_id, class_type, instructor}\n3        {user_id, class_type, instructor}  {user_id, class_type, instructor}\n4        {user_id, class_type, instructor}  {user_id, class_type, instructor}\n\n\nI would expect the same behaviour here - do you know what I am missing?\n"
"I have two dataframes, A and B, and I want to get those in A but not in B, just like the one right below the top left corner.\n\n\n\nDataframe A has columns ['a','b' + others] and B has columns ['a','b' + others]. There are no NaN values. I tried the following:\n\n1.\n\ndfm = dfA.merge(dfB, on=['a','b'])\ndfe = dfA[(~dfA['a'].isin(dfm['a']) | (~dfA['b'].isin(dfm['b'])\n\n\n2.\n\ndfm = dfA.merge(dfB, on=['a','b'])\ndfe = dfA[(~dfA['a'].isin(dfm['a']) &amp; (~dfA['b'].isin(dfm['b'])\n\n\n3.\n\ndfe = dfA[(~dfA['a'].isin(dfB['a']) | (~dfA['b'].isin(dfB['b'])\n\n\n4.\n\ndfe = dfA[(~dfA['a'].isin(dfB['a']) &amp; (~dfA['b'].isin(dfB['b'])\n\n\nbut when I get len(dfm) and len(dfe), they don't sum up to dfA (it's off by a few numbers). I've tried doing this on dummy cases and #1 works, so maybe my dataset may have some peculiarities I am unable to reproduce.\n\nWhat's the right way to do this?\n"
'I have a dataframe with 3 columns in Python:\n\nName1 Name2 Value\nJuan  Ale   1\nAle   Juan  1\n\n\nand would like to eliminate the duplicates based on columns Name1 and Name2 combinations.\n\nIn my example both rows are equal (but they are in different order), and I would like to delete the second row and just keep the first one, so the end result should be:\n\nName1 Name2 Value\nJuan  Ale   1\n\n\nAny idea will be really appreciated!\n'
"I have a dataframe with just two columns, Date, and ClosingPrice. I am trying to plot them using df.plot() but keep getting this error:\n\n\n  ValueError: view limit minimum -36785.37852 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units\n\n\nI have found documentation about this from matplotlib but that says how to make sure that the format is datetime. Here is code that I have to make sure the format is datetime and also printing the data type for each column before attempting to plot.\n\ndf.Date = pd.to_datetime(df.Date)\n\nprint(df['ClosingPrice'].dtypes)\nprint(df['Date'].dtypes)\n\n\nThe output for these print statements are:\n\n\n  float64\n  datetime64[ns]\n\n\nI am not sure what the problem is since I am verifying the data type before plotting. Here is also what the first few rows of the data set look like:\n\nDate  ClosingPrice\n0    2013-09-10       64.7010\n1    2013-09-11       61.1784\n2    2013-09-12       61.8298\n3    2013-09-13       60.8108\n4    2013-09-16       58.8776\n5    2013-09-17       59.5577\n6    2013-09-18       60.7821\n7    2013-09-19       61.7788\n\n Any help is appreciated. \n"
'I have a pandas DataFrame with a column representing a categorical variable. How can I get a list of the categories? I tried .values on the column but that does not return the unique levels.\n\nThanks!\n'
"This question is related to rostering or staffing. I'm trying to assign various jobs to individuals (employees). Using the df below, \n\n`[Person]` = Individuals (employees)\n`[Area]` and `[Place]` = unique jobs\n`[On]` = How many unique jobs are occurring at each point in time\n\n\nSo [Area] and [Place] together will make up unique values that are different jobs. These values will be assigned to individuals with the overall aim to use the least amount of individuals possible. The most unique values assigned to any one individual is 3. [On] displays how many current unique values for [Place] and [Area] are occurring. So this provides a concrete guide on how many individuals I need. For example, \n\n1-3 unique values occurring = 1 individual\n4-6 unique values occurring = 2 individuals\n7-9 unique values occurring = 3 individuals etc\n\n\nQuestion:\nWhere the amount of unique values in [Area] and [Place] is greater than 3 is causing me trouble. I can't do a groupby where I assign the first 3 unique values to individual 1 and the next 3 unique values to individual 2 etc. I want to group unique values in [Area] and [Place] by [Area]. So look to assign same values in [Area] to an individual (up to 3). Then, if there are leftover values (&lt;3), they should be combined to make a group of 3, where possible.\n\nThe way I envisage this working is: see into the future by an hour. For each new row of values the script should see how many values will be [On](this provides an indication of how many total individuals are required). Where unique values are >3, they should be assigned by grouping the same value in [Area]. If there are leftover values they should be combined anyhow to make up to a group of 3.\n\nPutting that into a step by step process:\n\n1) Use the [On] Column to determine how many individuals are required by looking into the future for an hour\n\n2) Where there are more than 3 unique values occurring assign the identical values in [Area] first.\n\n3) If there are any leftover values then look to combine anyway possible.\n\nFor the df below, there are 9 unique values occurring for [Place] and [Area] with an hour. So we should have 3 individuals assigned. When unique values >3 it should be assigned by [Area] and seeing if the same value occurs. The leftover values should be combined with other individuals that have less than 3 unique values.\n\nimport pandas as pd\nimport numpy as np\n\nd = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','08:40:00','08:42:00','08:45:00','08:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 5','House 1','House 2','House 3','House 2'],                 \n    'Area' : ['A','B','C','D','E','D','E','F','G'],     \n    'On' : ['1','2','3','4','5','6','7','8','9'], \n    'Person' : ['Person 1','Person 2','Person 3','Person 4','Person 5','Person 4','Person 5','Person 6','Person 7'],   \n     })\n\ndf = pd.DataFrame(data=d)\n\n\nThis is my attempt:\n\ndef reduce_df(df):\n    values = df['Area'] + df['Place']\n    df1 = df.loc[~values.duplicated(),:] # ignore duplicate values for this part..\n    person_count = df1.groupby('Person')['Person'].agg('count')\n    leftover_count = person_count[person_count &lt; 3] # the 'leftovers'\n\n    # try merging pairs together\n    nleft = leftover_count.shape[0]\n    to_try = np.arange(nleft - 1)\n    to_merge = (leftover_count.values[to_try] + \n                leftover_count.values[to_try + 1]) &lt;= 3\n    to_merge[1:] = to_merge[1:] &amp; ~to_merge[:-1]\n    to_merge = to_try[to_merge]\n    merge_dict = dict(zip(leftover_count.index.values[to_merge+1], \n                leftover_count.index.values[to_merge]))\n    def change_person(p):\n        if p in merge_dict.keys():\n            return merge_dict[p]\n        return p\n    reduced_df = df.copy()\n    # update df with the merges you found\n    reduced_df['Person'] = reduced_df['Person'].apply(change_person)\n    return reduced_df\n\ndf1 = (reduce_df(reduce_df(df)))\n\n\nThis is the Output:\n\n       Time    Place Area On    Person\n0   8:03:00  House 1    A  1  Person 1\n1   8:17:00  House 2    B  2  Person 1\n2   8:20:00  House 3    C  3  Person 1\n3   8:28:00  House 4    D  4  Person 4\n4   8:35:00  House 5    E  5  Person 5\n5   8:40:00  House 1    D  6  Person 4\n6   8:42:00  House 2    E  7  Person 5\n7   8:45:00  House 3    F  8  Person 5\n8   8:50:00  House 2    G  9  Person 7\n\n\nThis is my Intended Output:\n\n       Time    Place Area On    Person\n0   8:03:00  House 1    A  1  Person 1\n1   8:17:00  House 2    B  2  Person 1\n2   8:20:00  House 3    C  3  Person 1\n3   8:28:00  House 4    D  4  Person 2\n4   8:35:00  House 5    E  5  Person 3\n5   8:40:00  House 6    D  6  Person 2\n6   8:42:00  House 2    E  7  Person 3\n7   8:45:00  House 3    F  8  Person 2\n8   8:50:00  House 2    G  9  Person 3\n\n\nDescription on how I want to get this output:\n\nIndex 0: One `unique` value occurring. So `assign` to individual 1\nIndex 1: Two `unique` values occurring. So `assign` to individual 1\nIndex 2: Three `unique` values occurring. So `assign` to individual 1\nIndex 3: Four `unique` values on. So `assign` to individual 2\nIndex 4: Five `unique` values on. This one is a bit tricky and hard to conceptualise. But there is another `E` within an `hour`. So `assign` to a new individual so it can be combined with the other `E`\nIndex 5: Six `unique` values on. Should be `assigned` with the other `D`. So individual 2\nIndex 6: Seven `unique` values on. Should be `assigned` with other `E`. So individual 3\nIndex 7: Eight `unique` values on. New value in `[Area]`, which is a _leftover_. `Assign` to either individual 2 or 3\nIndex 8: Nine `unique` values on. New value in `[Area]`, which is a _leftover_. `Assign` to either individual 2 or 3\n\n\nExample No2:\n\nd = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','8:40:00','8:42:00','8:45:00','8:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 1','House 2','House 3','House 1','House 2','House 3'],                 \n    'Area' : ['X','X','X','X','X','X','X','X','X'],     \n    'On' : ['1','2','3','3','3','3','3','3','3'], \n    'Person' : ['Person 1','Person 1','Person 1','Person 1','Person 1','Person 1','Person 1','Person 1','Person 1'],   \n    })\n\n    df = pd.DataFrame(data=d)\n\n\nI am getting an error:\n\n IndexError: index 1 is out of bounds for axis 1 with size 1\n\n\nOn this line:\n\ndf.loc[:,'Person'] = df['Person'].unique()[assignedPeople]\n\n\nHowever, if I change the Person to 1,2,3 repeating, it returns the following:\n\n'Person' : ['Person 1','Person 2','Person 3','Person 1','Person 2','Person 3','Person 1','Person 2','Person 3'], \n\n      Time    Place Area On    Person\n0  8:03:00  House 1    X  1  Person 1\n1  8:17:00  House 2    X  2  Person 1\n2  8:20:00  House 3    X  3  Person 1\n3  8:28:00  House 1    X  3  Person 2\n4  8:35:00  House 2    X  3  Person 2\n5  8:40:00  House 3    X  3  Person 2\n6  8:42:00  House 1    X  3  Person 3\n7  8:45:00  House 2    X  3  Person 3\n8  8:50:00  House 3    X  3  Person 3\n\n\nIntended Output:\n\n      Time    Place Area On    Person\n0  8:03:00  House 1    X  1  Person 1\n1  8:17:00  House 2    X  2  Person 1\n2  8:20:00  House 3    X  3  Person 1\n3  8:28:00  House 1    X  3  Person 1\n4  8:35:00  House 2    X  3  Person 1\n5  8:40:00  House 3    X  3  Person 1\n6  8:42:00  House 1    X  3  Person 1\n7  8:45:00  House 2    X  3  Person 1\n8  8:50:00  House 3    X  3  Person 1\n\n\nThe main takeaway from Example 2 is:\n\n1) There are &lt;3 unique values on so assign to individual 1\n\n"
"This might be considered as a duplicate of a thorough explanation of various approaches, however I can't seem to find a solution to my problem there due to a higher number of Data Frames.\n\nI have multiple Data Frames (more than 10), each differing in one column VARX. This is just a quick and oversimplified example:\n\nimport pandas as pd\n\ndf1 = pd.DataFrame({'depth': [0.500000, 0.600000, 1.300000],\n       'VAR1': [38.196202, 38.198002, 38.200001],\n       'profile': ['profile_1', 'profile_1','profile_1']})\n\ndf2 = pd.DataFrame({'depth': [0.600000, 1.100000, 1.200000],\n       'VAR2': [0.20440, 0.20442, 0.20446],\n       'profile': ['profile_1', 'profile_1','profile_1']})\n\ndf3 = pd.DataFrame({'depth': [1.200000, 1.300000, 1.400000],\n       'VAR3': [15.1880, 15.1820, 15.1820],\n       'profile': ['profile_1', 'profile_1','profile_1']})\n\n\nEach df has same or different depths for the same profiles, so\n\nI need to create a new DataFrame which would merge all separate ones, where the key columns for the operation are depth and profile, with all appearing depth values for each profile.\n\nThe VARX value should be therefore NaN where there is no depth measurement of that variable for that profile.\n\nThe result should be a thus a new, compressed DataFrame with all VARX as additional columns to the depth and profile ones, something like this:\n\nname_profile    depth   VAR1        VAR2        VAR3\nprofile_1   0.500000    38.196202   NaN         NaN\nprofile_1   0.600000    38.198002   0.20440     NaN\nprofile_1   1.100000    NaN         0.20442     NaN\nprofile_1   1.200000    NaN         0.20446     15.1880\nprofile_1   1.300000    38.200001   NaN         15.1820\nprofile_1   1.400000    NaN         NaN         15.1820\n\n\nNote that the actual number of profiles is much, much bigger.\n\nAny ideas?\n"
'I\'ve inherited a data file saved in the Stata .dta format. I can load it in with scikits.statsmodels genfromdta() function. This puts my data into a 1-dimensional NumPy array, where each entry is a row of data, stored in a 24-tuple.\n\nIn [2]: st_time = time.time(); initialload = sm.iolib.genfromdta("/home/myfile.dta"); ed_time = time.time(); print (ed_time - st_time)\n666.523324013\n\nIn [3]: type(initialload)\nOut[3]: numpy.ndarray\n\nIn [4]: initialload.shape\nOut[4]: (4809584,)\n\nIn [5]: initialload[0]\nOut[5]: (19901130.0, 289.0, 1990.0, 12.0, 19901231.0, 18.0, 40301000.0, \'GB\', 18242.0, -2.368063, 1.0, 1.7783716290878204, 4379.355, 66.17669677734375, -999.0, -999.0, -0.60000002, -999.0, -999.0, -999.0, -999.0, -999.0, 0.2, 371.0)\n\n\nI am curious if there\'s an efficient way to arrange this into a Pandas DataFrame. From what I\'ve read, building up a DataFrame row-by-row seems quite inefficient... but what are my options?\n\nI\'ve written a pretty slow first-pass that just reads each tuple as a single-row DataFrame and appends it. Just wondering if anything else is known to be better.\n'
"I have a dictionary name date_dict keyed by datetime dates with values corresponding to integer counts of observations. I convert this to a sparse series/dataframe with censored observations that I would like to join or convert to a series/dataframe with continuous dates. The nasty list comprehension is my hack to get around the fact that pandas apparently won't automatically covert datetime date objects to an appropriate DateTime index.\n\ndf1 = pd.DataFrame(data=date_dict.values(),\n                   index=[datetime.datetime.combine(i, datetime.time()) \n                          for i in date_dict.keys()],\n                   columns=['Name'])\ndf1 = df1.sort(axis=0)\n\n\nThis example has 1258 observations and the DateTime index runs from 2003-06-24 to 2012-11-07.\n\ndf1.head()\n             Name\nDate\n2003-06-24   2\n2003-08-13   1\n2003-08-19   2\n2003-08-22   1\n2003-08-24   5\n\n\nI can create an empty dataframe with a continuous DateTime index, but this introduces an unneeded column and seems clunky. I feel as though I'm missing a more elegant solution involving a join.\n\ndf2 = pd.DataFrame(data=None,columns=['Empty'],\n                   index=pd.DateRange(min(date_dict.keys()),\n                                      max(date_dict.keys())))\ndf3 = df1.join(df2,how='right')\ndf3.head()\n            Name    Empty\n2003-06-24   2   NaN\n2003-06-25  NaN  NaN\n2003-06-26  NaN  NaN\n2003-06-27  NaN  NaN\n2003-06-30  NaN  NaN\n\n\nIs there a simpler or more elegant way to fill a continuous dataframe from a sparse dataframe so that there is (1) a continuous index, (2) the NaNs are 0s, and (3) there is no left-over empty column in the dataframe?\n\n            Name\n2003-06-24   2\n2003-06-25   0\n2003-06-26   0\n2003-06-27   0\n2003-06-30   0\n\n"
"I'd like to build a running sum over a pandas dataframe. I have something like:\n\n10/10/2012:  50,  0\n10/11/2012: -10, 90\n10/12/2012: 100, -5\n\n\nAnd I would like to get:\n\n10/10/2012:  50,  0\n10/11/2012:  40, 90\n10/12/2012: 140, 85\n\n\nSo every cell should be the sum of itself and all previous cells, how should I do this without using a loop.\n"
"I have a df:\n\n&gt;&gt;&gt; df\n                   sales     cash\nSTK_ID RPT_Date                  \n000568 20120930   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n\n\nAnd want to change first row's index value from ('000568','20120930') to ('000999','20121231'). Final result will be:\n\n&gt;&gt;&gt; df\n                   sales     cash\nSTK_ID RPT_Date                  \n000999 20121231   80.093   57.488\n000596 20120930   32.585   26.177\n000799 20120930   14.784    8.157\n\n\nHow to achieve this?\n"
'I know about these column slice methods:\n\ndf2 = df[["col1", "col2", "col3"]] and df2 = df.ix[:,0:2]\n\nbut I\'m wondering if there is a way to slice columns from the front/middle/end of a dataframe in the same slice without specifically listing each one.\n\nFor example, a dataframe df with columns: col1, col2, col3, col4, col5 and col6.\n\nIs there a way to do something like this?\n\ndf2 = df.ix[:, [0:2, "col5"]]\n\nI\'m in the situation where I have hundreds of columns and routinely need to slice specific ones for different requests.  I\'ve checked through the documentation and haven\'t seen something like this.  Have I overlooked something? \n'
"I would like to modify some values from a column in my DataFrame. At the moment I have  a view from select via the multi index of my original df (and modifying does change df).\n\nHere's an example: \n\nIn [1]: arrays = [np.array(['bar', 'bar', 'baz', 'qux', 'qux', 'bar']),\n                  np.array(['one', 'two', 'one', 'one', 'two', 'one']),\n                  np.arange(0, 6, 1)]\nIn [2]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])\n\nIn [3]: df\n                  A         B         C\nbar one 0 -0.088671  1.902021 -0.540959\n    two 1  0.782919 -0.733581 -0.824522\nbaz one 2 -0.827128 -0.849712  0.072431\nqux one 3 -0.328493  1.456945  0.587793\n    two 4 -1.466625  0.720638  0.976438\nbar one 5 -0.456558  1.163404  0.464295\n\n\nI try to modify a slice of df to a scalar value:\n\nIn [4]: df.ix['bar', 'two', :]['A']\nOut[4]:\n1    0.782919\nName: A, dtype: float64\n\nIn [5]: df.ix['bar', 'two', :]['A'] = 9999\n# df is unchanged\n\n\nI really want to modify several values in the column (and since indexing returns a vector, not a scalar value, I think this would make more sense):\n\nIn [6]: df.ix['bar', 'one', :]['A'] = [999, 888]\n# again df remains unchanged\n\n\nI'm using pandas 0.11. Is there is a simple way to do this?\n\nThe current solution is to recreate df from a new one and modify values I want to. But it's not elegant and can be very heavy on complex dataframe. In my opinion the problem should come from .ix and .loc not returning a view but a copy.\n"
'A few methods to do this:\n\n\nRead the entire CSV and then use df.tail\nSomehow reverse the file (whats the best way to do this for large files?) and then use nrows argument to read\nSomehow find the number of rows in the CSV, then use skiprows and read required number of rows.\nMaybe do chunk read discarding initial chunks (though not sure how this would work)\n\n\nCan it be done in some easier way? If not, which amongst these three should be prefered and why?\n\nPossibly related: \n\n\nEfficiently finding the last line in a text file\nReading parts of ~13000 row CSV file with pandas read_csv and nrows\n\n\nNot directly related:\n\n\nHow to get the last n row of pandas dataframe?\n\n'
"As Python newbie I recently discovered that with Py 2.7 I can do something like:\n\nprint '{:20,.2f}'.format(123456789)\n\n\nwhich will give the resulting output:\n\n123,456,789.00\n\n\nI'm now looking to have a similar outcome for a pandas df so my code was like:\n\nimport pandas as pd\nimport random\ndata = [[random.random()*10000 for i in range(1,4)] for j in range (1,8)]\ndf = pd.DataFrame (data)\nprint '{:20,.2f}'.format(df)\n\n\nIn this case I have the error:\n\n Unknown format code 'f' for object of type 'str'\n\n\nAny suggestions to perform something like '{:20,.2f}'.format(df) ?\n\nAs now my idea is to index the dataframe (it's a small one), then format each individual float within it, might be assign astype(str), and rebuild the DF ... but looks so looks ugly :-( and I'm not even sure it'll work ..\n\nWhat do you think ? I'm stuck ... and would like to have a better format for my dataframes when these are converted to reportlabs grids.\n"
'I have data sampled at essentially random intervals. I would like to compute a weighted moving average using numpy (or other python package). I have a crude implementation of a moving average, but I am having trouble finding a good way to do a weighted moving average, so that the values towards the center of the bin are weighted more than values towards the edges.\n\nHere I generate some sample data and then take a moving average. How can I most easily implement a weighted moving average? Thanks!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#first generate some datapoint for a randomly sampled noisy sinewave\nx = np.random.random(1000)*10\nnoise = np.random.normal(scale=0.3,size=len(x))\ny = np.sin(x) + noise\n\n#plot the data\nplt.plot(x,y,\'ro\',alpha=0.3,ms=4,label=\'data\')\nplt.xlabel(\'Time\')\nplt.ylabel(\'Intensity\')\n\n#define a moving average function\ndef moving_average(x,y,step_size=.1,bin_size=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        items_in_bin = y[(x&gt;(bin_center-bin_size*0.5) ) &amp; (x&lt;(bin_center+bin_size*0.5))]\n        bin_avg[index] = np.mean(items_in_bin)\n\n    return bin_centers,bin_avg\n\n#plot the moving average\nbins, average = moving_average(x,y)\nplt.plot(bins, average,label=\'moving average\')\n\nplt.show()\n\n\nThe output:\n\n\nUsing the advice from crs17 to use "weights=" in the np.average function, I came up weighted average function, which uses a Gaussian function to weight the data:\n\ndef weighted_moving_average(x,y,step_size=0.05,width=1):\n    bin_centers  = np.arange(np.min(x),np.max(x)-0.5*step_size,step_size)+0.5*step_size\n    bin_avg = np.zeros(len(bin_centers))\n\n    #We\'re going to weight with a Gaussian function\n    def gaussian(x,amp=1,mean=0,sigma=1):\n        return amp*np.exp(-(x-mean)**2/(2*sigma**2))\n\n    for index in range(0,len(bin_centers)):\n        bin_center = bin_centers[index]\n        weights = gaussian(x,mean=bin_center,sigma=width)\n        bin_avg[index] = np.average(y,weights=weights)\n\n    return (bin_centers,bin_avg)\n\n\nResults look good:\n\n'
"I'm having some trouble sorting and then resetting my Index in Pandas:\n\ndfm = dfm.sort(['delt'],ascending=False)\ndfm = dfm.reindex(index=range(1,len(dfm)))\n\n\nThe dataframe returns unsorted after I reindex.  My ultimate goal is to have a sorted dataframe with index numbers from 1 --> len(dfm) so if there's a better way to do that, I wouldn't mind,\n\nThanks!\n"
"I'm trying to Assign multiple values to a single row in a DataFrame and I need the correct syntax.  \n\nSee the code below.\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n'A': range(10),\n'B' : '',\n'C' : 0.0,\n'D' : 0.0,\n'E': 0.0,\n})\n\n#Works fine\ndf['A'][2] = 'tst'\n\n#Is there a way to assign multiple values in a single line and if so what is the correct syntax\ndf[['A', 'B', 'C', 'D', 'E']][3] = ['V1', 4.3, 2.2, 2.2, 20.2]\n\n\nThanks for the help\n"
'Looking for a fast way to get a row in a pandas dataframe into a ordered dict with out using list. List are fine but with large data sets will take to long. I am using fiona GIS reader and the rows are ordereddicts with the schema giving the data type. I use pandas to join data. I many cases the rows will have different types so I was thinking turning into a numpy array with type string might do the trick. \n'
"I have a set of DataFrames with numeric values and partly overlapping indices. I would like to merge them an take the mean if an index occurs in more than one DataFrame.\n\nimport pandas as pd\nimport numpy as np\n\ndf1 = pd.DataFrame([1,2,3], columns=['col'], index=['a','b','c'])\ndf2 = pd.DataFrame([4,5,6], columns=['col'], index=['b','c','d'])\n\n\nThis gives me two DataFrames:\n\n   col            col\na    1        b     4\nb    2        c     5\nc    3        d     6\n\n\nNow I would like to merge the DataFrames and take the mean for each index (if applicable, i.e. if it occurs more than once).\n\nShould look like this:\n\n    col\na     1\nb     3\nc     4\nd     6\n\n\nCan I do this with some advanced merging/joining?\n"
"I have a pandas df as below:\n\n&gt;&gt;&gt; df\n                   sales  net_pft  sales_gr  net_pft_gr\nSTK_ID RPT_Date                                        \n600809 20120331  22.1401   4.9253    0.1824     -0.0268\n       20120630  38.1565   7.8684    0.3181      0.1947\n       20120930  52.5098  12.4338    0.4735      0.7573\n       20121231  64.7876  13.2731    0.4435      0.7005\n       20130331  27.9517   7.5182    0.2625      0.5264\n       20130630  40.6460   9.8572    0.0652      0.2528\n       20130930  53.0501  11.8605    0.0103     -0.0461\n\n\nThen df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True) create bar chart.\n\nAnd df[['sales_gr','net_pft_gr']].plot(kind='line', use_index=True) create line chart:\n\nNow I want to put them together in a chart of two y-axes, using twinx(). \n\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = df[['sales','net_pft']].unstack('STK_ID').plot(kind='bar', use_index=True)\nax2 = ax.twinx()\nax2.plot(df[['sales_gr','net_pft_gr']].values, linestyle='-', marker='o', linewidth=2.0)\n\n\nThe result is like this :\n\n\nMy issues are:\n\n\nHow to shift the line to align with the bar at the same x-tickers ?\nHow to let the left and right y_axis tickers aligned at the same line?  \n\n"
'This is a string I\'m getting out of pandas.DataFrame.to_json(), putting it into redis, getting it out of redis elsewhere, and trying to read it via pandas.read_json():\n\nDFJ {"args":{"0":"[]","1":"[]","2":"[]","3":"[]","4":"[]","5":"[]","6":"[]","7":"[]"},"date":{"0":1385944439000000000,"1":1385944439000000000,"2":1385944440000000000,"3":1385944440000000000,"4":1385944440000000000,"5":1385944440000000000,"6":1385944440000000000,"7":1385944440000000000},"host":{"0":"yy38.segm1.org","1":"kyy1.segm1.org","2":"yy10.segm1.org","3":"yy24.segm1.org","4":"yy24.segm1.org","5":"yy34.segm1.org","6":"yy15.segm1.org","7":"yy15.segm1.org"},"kwargs":{"0":"{}","1":"{}","2":"{}","3":"{}","4":"{}","5":"{}","6":"{}","7":"{}"},"operation":{"0":"x_gbinf","1":"x_initobj","2":"x_gobjParams","3":"gtfull","4":"x_gbinf","5":"gxyzinf","6":"deletemfg","7":"gxyzinf"},"thingy":{"0":"a13yy38","1":"a19kyy1","2":"a14yy10","3":"a14yy24","4":"a14yy24","5":"a12yy34","6":"a15yy15","7":"a15yy15"},"status":{"0":-101,"1":1,"2":-101,"3":-101,"4":-101,"5":-101,"6":1,"7":-101},"time":{"0":0.000801,"1":0.003244,"2":0.002247,"3":0.002787,"4":0.001067,"5":0.002652,"6":0.004371,"7":0.000602}}\n\n\nIt seems like it does not have any unicode in it. Yet on trying to .read_json() it I get:\n\nTraceback (most recent call last):\n  File "./sqlprofile.py", line 160, in &lt;module&gt;\n    maybe_save_dataframes(rconn, configd, results)\n  File "./sqlprofile.py", line 140, in maybe_save_dataframes\n    h5store.append(out_queue, df)\n  File "/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py", line 658, in append\n    self._write_to_group(key, value, table=True, append=True, **kwargs)\n  File "/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py", line 923, in _write_to_group\n    s.write(obj = value, append=append, complib=complib, **kwargs)\n  File "/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py", line 2985, in write\n    **kwargs)\n  File "/home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py", line 2717, in create_axes\n    raise e\nTypeError: [unicode] is not implemented as a table column\n&gt; /home/username/anaconda/lib/python2.7/site-packages/pandas/io/pytables.py(2717)create_axes()\n-&gt; raise e\n(Pdb) locals()\n\n\nThis is what I\'m getting in locals() - it seems that append_axis (column names?) values are unicode. Why?\n\n{\'append_axis\': [u\'args\', u\'date\', u\'host\', u\'kwargs\', u\'operation\', u\'thingy\', u\'status\', u\'time\'], \'existing_table\': None, \'blocks\': [FloatBlock: [time], 1 x 8, dtype float64, ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, IntBlock: [status], 1 x 8, dtype int64, DatetimeBlock: [date], 1 x 8, dtype datetime64[ns]], \'axis\': 1, \'self\': frame_table  (typ-&gt;appendable,nrows-&gt;None,ncols-&gt;1,indexers-&gt;[index]), \'axes\': [0], \'kwargs\': {}, \'klass\': &lt;class \'pandas.io.pytables.DataCol\'&gt;, \'block_obj\':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, \'axis_labels\': [u\'args\', u\'date\', u\'host\', u\'kwargs\', u\'operation\', u\'thingy\', u\'status\', u\'time\'], \'nan_rep\': \'nan\', \'data_columns\': [], \'obj\':   args                date            host kwargs              operation      thingy  status      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf  a13yy38    -101  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}         x_initobj  a19kyy1       1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}    x_gobjParams  a14yy10    -101  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull  a14yy24    -101  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf  a14yy24    -101  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}           gxyzinf  a12yy34    -101  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}  deletemfg  a15yy15       1  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}           gxyzinf  a15yy15    -101  0.000602, \'validate\': True, \'a\': (1, [u\'args\', u\'date\', u\'host\', u\'kwargs\', u\'operation\', u\'thingy\', u\'status\', u\'time\']), \'index_axes_map\': {0: name-&gt;index,cname-&gt;index,axis-&gt;0,pos-&gt;0,kind-&gt;integer}, \'b\': ObjectBlock: [args, host, kwargs, operation, thingy], 5 x 8, dtype object, \'e\': TypeError(\'[unicode] is not implemented as a table column\',), \'name\': None, \'existing_col\': None, \'j\': 2, \'i\': 1, \'min_itemsize\': None, \'col\': name-&gt;values_block_1,cname-&gt;values_block_1,dtype-&gt;None,shape-&gt;None}\n\n\nHow can I fix that? Is this a bug in Pandas / pytables?\n\nEnvironment:\n\nPython 2.7\n\npandas==0.12.0\n\ntables==3.0.0\n'
'Problem description\n\nIn writing a Monte Carlo particle simulator (brownian motion and photon emission) in python/numpy. I need to save the simulation output (>>10GB) to a file and process the data in a second step. Compatibility with both Windows and Linux is important.\n\nThe number of particles (n_particles) is 10-100. The number of time-steps (time_size) is ~10^9.\n\nThe simulation has 3 steps (the code below is for an all-in-RAM version):\n\n\nSimulate (and store) an emission rate array (contains many almost-0 elements): \n\n\nshape (n_particles x time_size), float32, size 80GB\n\nCompute counts array, (random values from a Poisson process with previously computed rates):\n\n\nshape (n_particles x time_size), uint8, size 20GB\n\ncounts = np.random.poisson(lam=emission).astype(np.uint8)\n\n\nFind timestamps (or index) of counts. Counts are almost always 0, so the timestamp arrays  will fit in RAM.\n\n# Loop across the particles\ntimestamps = [np.nonzero(c) for c in counts]\n\n\n\nI do step 1 once, then repeat step 2-3 many (~100) times. In the future I may need to pre-process emission (apply cumsum or other functions) before computing counts.\n\nQuestion\n\nI have a working in-memory implementation and I\'m trying to understand what is the best approach to implement an out-of-core version that can scale to (much) longer simulations.\n\nWhat I would like it exist\n\nI need to save arrays to a file, and I would like to use a single file for a simulation. I also need a "simple" way to store and recall a dictionary of simulation parameter (scalars).\n\nIdeally I would like a file-backed numpy array that I can preallocate and fill in chunks. Then, I would like the numpy array methods (max, cumsum, ...) to work transparently, requiring only a chunksize keyword to specify how much of the array to load at each iteration.\n\nEven better, I would like a Numexpr that operates not between cache and RAM but between RAM and hard drive.\n\nWhat are the practical options\n\nAs a first option\nI started experimenting with pyTables, but I\'m not happy with its complexity and abstractions (so different from numpy). Moreover my current solution (read below) is UGLY and not very efficient.\n\nSo my options for which I seek an answer are\n\n\nimplement a numpy array with required functionality (how?)\nuse pytable in a smarter way (different data-structures/methods)\nuse another library: h5py, blaze, pandas... (I haven\'t tried any of them so far).\n\n\nTentative solution (pyTables)\n\nI save the simulation parameters in \'/parameters\' group: each parameter is converted to a numpy array scalar. Verbose solution but it works.\n\nI save emission as an Extensible array (EArray), because I generate the data in chunks and I need to append each new chunk (I know the final size though). Saving counts is more problematic. If a save it like a pytable array it\'s difficult to perform queries like "counts >= 2". Therefore I saved counts as multiple tables (one per particle) [UGLY] and I query with .get_where_list(\'counts &gt;= 2\'). I\'m not sure this is space-efficient, and\ngenerating all these tables instead of using a single array, clobbers significantly the HDF5 file. Moreover, strangely enough, creating those tables require creating a custom dtype (even for standard numpy dtypes):\n\n    dt = np.dtype([(\'counts\', \'u1\')])        \n    for ip in xrange(n_particles):\n        name = "particle_%d" % ip\n        data_file.create_table(\n                    group, name, description=dt, chunkshape=chunksize,\n                    expectedrows=time_size,\n                    title=\'Binned timetrace of emitted ph (bin = t_step)\'\n                        \' - particle_%d\' % particle)\n\n\nEach particle-counts "table" has a different name (name = "particle_%d" % ip) and that I need to put them in a python list for easy iteration.\n\nEDIT: The result of this question is a Brownian Motion simulator called PyBroMo.\n'
"I have a long time series, eg. \n\nimport pandas as pd\nindex=pd.date_range(start='2012-11-05', end='2012-11-10', freq='1S').tz_localize('Europe/Berlin')\ndf=pd.DataFrame(range(len(index)), index=index, columns=['Number'])\n\n\nNow I want to extract all sub-DataFrames for each day, to get the following output:\n\ndf_2012-11-05: data frame with all data referring to day 2012-11-05\ndf_2012-11-06: etc.\ndf_2012-11-07\ndf_2012-11-08\ndf_2012-11-09\ndf_2012-11-10\n\n\nWhat is the most effective way to do this avoiding to check if the index.date==give_date which is very slow. Also, the user does not know a priory the range of days in the frame.\n\nAny hint do do this with an iterator? \n\nMy current solution is this, but it is not so elegant and has two issues defined below:\n\ntime_zone='Europe/Berlin'\n# find all days\na=np.unique(df.index.date) # this can take a lot of time\na.sort()\nresults=[]\nfor i in range(len(a)-1):\n    day_now=pd.Timestamp(a[i]).tz_localize(time_zone)\n    day_next=pd.Timestamp(a[i+1]).tz_localize(time_zone)\n    results.append(df[day_now:day_next]) # how to select if I do not want day_next included?\n\n# last day\nresults.append(df[day_next:])\n\n\nThis approach has the following problems:\n\n\na=np.unique(df.index.date) can take a lot of time\ndf[day_now:day_next] includes the day_next, but I need to exclude it in the range\n\n"
"I am trying to achieve differentiation by hatch pattern instead of by (just) colour. How do I do it using pandas?\n\nIt's possible in matplotlib, by passing the hatch optional argument as discussed here. I know I can also pass that option to a pandas plot, but I don't know how to tell it to use a different hatch pattern for each DataFrame column.\n\ndf = pd.DataFrame(rand(10, 4), columns=['a', 'b', 'c', 'd'])\ndf.plot(kind='bar', hatch='/');\n\n\n\n\nFor colours, there is the colormap option described here. Is there something similar for hatching? Or can I maybe set it manually by modifying the Axes object returned by plot?\n"
'I am trying to write a pandas DataFrame to a PostgreSQL database,\nusing a schema-qualified table.\n\nI use the following code:\n\nimport pandas.io.sql as psql\nfrom sqlalchemy import create_engine\n\nengine = create_engine(r\'postgresql://some:user@host/db\')\n\nc = engine.connect()\nconn = c.connection\n\ndf = psql.read_sql("SELECT * FROM xxx", con=conn)    \ndf.to_sql(\'a_schema.test\', engine)\n\nconn.close()\n\n\nWhat happens is that pandas writes in schema "public", in a table named \'a_schema.test\',\ninstead of writing in the "test" table in the "a_schema" schema.\n\nHow can I instruct pandas to use a schema different than public?\n\nThanks\n'
"EDIT\n\nI found a quite nice solution and posted it below as an answer.\nThe result will look like this:\n\n\n\n\n\nSome example data you can generate for this problem:\n\ncodes = list('ABCDEFGH'); \ndates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); \ndates = dates.append(dates)\ndates.sort()\ndf = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates})\n\n\nresulting in:\n\nIn [55]: df\nOut[55]:\n    amount col1 col2       date\n0        1    D    E 2013-11-01\n0        5    E    B 2013-11-01\n1        5    G    A 2013-11-02\n1        7    D    H 2013-11-02\n2        5    E    G 2013-11-03\n2        4    H    G 2013-11-03\n3        7    A    F 2013-11-04\n3        3    A    A 2013-11-04\n4        1    E    G 2013-11-05\n4        7    D    C 2013-11-05\n5        5    C    A 2013-11-06\n5        7    H    F 2013-11-06\n6        1    G    B 2013-11-07\n6        8    D    A 2013-11-07\n7        1    B    H 2013-11-08\n7        8    F    H 2013-11-08\n8        3    A    E 2013-11-09\n8        1    H    D 2013-11-09\n9        3    B    D 2013-11-10\n9        1    H    G 2013-11-10\n10       6    E    E 2013-11-11\n10       6    F    E 2013-11-11\n11       2    G    B 2013-11-12\n11       5    H    H 2013-11-12\n12       5    F    G 2013-11-13\n12       5    G    B 2013-11-13\n13       8    H    B 2013-11-14\n13       6    G    F 2013-11-14\n14       9    F    C 2013-11-15\n14       4    H    A 2013-11-15\n..     ...  ...  ...        ...\n77       9    A    B 2014-01-17\n77       7    E    B 2014-01-17\n78       4    F    E 2014-01-18\n78       6    B    E 2014-01-18\n79       6    A    H 2014-01-19\n79       3    G    D 2014-01-19\n80       7    E    E 2014-01-20\n80       6    G    C 2014-01-20\n81       9    H    G 2014-01-21\n81       9    C    B 2014-01-21\n82       2    D    D 2014-01-22\n82       7    D    A 2014-01-22\n83       6    G    B 2014-01-23\n83       1    A    G 2014-01-23\n84       9    B    D 2014-01-24\n84       7    G    D 2014-01-24\n85       7    A    F 2014-01-25\n85       9    B    H 2014-01-25\n86       9    C    D 2014-01-26\n86       5    E    B 2014-01-26\n87       3    C    H 2014-01-27\n87       7    F    D 2014-01-27\n88       3    D    G 2014-01-28\n88       4    A    D 2014-01-28\n89       2    F    A 2014-01-29\n89       8    D    A 2014-01-29\n90       1    A    G 2014-01-30\n90       6    C    A 2014-01-30\n91       6    H    C 2014-01-31\n91       2    G    F 2014-01-31\n\n[184 rows x 4 columns]\n\n\nI'd like to group by calendar-week and by value of col1. Like this:\n\nkw = lambda x: x.isocalendar()[1]\ngrouped = df.groupby([df['date'].map(kw), 'col1'], sort=False).agg({'amount': 'sum'})\n\n\nresulting in:\n\nIn [58]: grouped\nOut[58]:\n           amount\ndate col1\n44   D          8\n     E         10\n     G          5\n     H          4\n45   D         15\n     E          1\n     G          1\n     H          9\n     A         13\n     C          5\n     B          4\n     F          8\n46   E          7\n     G         13\n     H         17\n     B          9\n     F         23\n47   G         14\n     H          4\n     A         40\n     C          7\n     B         16\n     F         13\n48   D          7\n     E         16\n     G          9\n     H          2\n     A          7\n     C          7\n     B          2\n...           ...\n1    H         14\n     A         14\n     B         15\n     F         19\n2    D         13\n     H         13\n     A         13\n     B         10\n     F         32\n3    D          8\n     E         18\n     G          3\n     H          6\n     A         30\n     C          9\n     B          6\n     F          5\n4    D          9\n     E         12\n     G         19\n     H          9\n     A          8\n     C         18\n     B         18\n5    D         11\n     G          2\n     H          6\n     A          5\n     C          9\n     F          9\n\n[87 rows x 1 columns]\n\n\nThen I want a plot to be generated like this:\n\nThat means: calendar-week and year (datetime) on the x-axis and for each of the grouped col1 one bar.\n\nThe problem I'm facing is: I only have integers describing the calendar week (KW in the plot), but I somehow have to merge back the date on it to get the ticks labeled by year as well. Furthermore I can't only plot the grouped calendar week because I need a correct order of the items (kw 47, kw 48 (year 2013) have to be on the left side of kw 1 (because this is 2014)).\n\n\n\nEDIT\n\nI figured out from here:\nhttp://pandas.pydata.org/pandas-docs/stable/visualization.html#visualization-barplot that grouped bars need to be columns instead of rows. So I thought about how to transform the data and found the method pivot which turns out to be a great function. reset_index is needed to transform the multiindex into columns. At the end I fill NaNs by zero:\n\nA = grouped.reset_index().pivot(index='date', columns='col1', values='amount').fillna(0)\n\n\ntransforms the data into:\n\ncol1   A   B   C   D   E   F   G   H\ndate\n1      4  31   0   0   0  18  13   8\n2      0  12  13  22   1  17   0   8\n3      3  10   4  13  12   8   7   6\n4     17   0  10   7   0  25   7   4\n5      7   0   7   9   8   6   0   7\n44     0   0   2  11   7   0   0   2\n45     9   3   2  14   0  16  21   2\n46     0  14   7   2  17  13  11   8\n47     5  13   0  15  19   7   5  10\n48    15   8  12   2  20   4   7   6\n49    20   0   0  18  22  17  11   0\n50     7  11   8   6   5   6  13  10\n51     8  26   0   0   5   5  16   9\n52     8  13   7   5   4  10   0  11\n\n\nwhich looks like the example data in the docs to be plotted in grouped bars:\n\nA. plot(kind='bar')\n\n\ngets this:\n\n\n\nwhereas I have the problem with the axis as it is now sorted (from 1-52), which is actually wrong, because calendar week 52 belongs to year 2013 in this case... Any ideas on how to merge back the real datetime for the calendar-weeks and use them as x-axis ticks?\n"
'I am trying to merge two dataframes on both name and the closest date (WRT the left hand dataframe). In my research I found one similar question here but it doesn\'t account for the name as well. From the above question it doesn\'t seem like there is a way to do this with merge but I can\'t see another way to do the two argument join that doesn\'t use the pandas merge function. \n\nIs there a way to do this with merge? And if not what would be the appropriate way to do this? \n\nI will post a copy of what I have tried but this was trying it with an exact merge on date which will not work. The most important line is the last one where I make the data3 dataframe.\n\ndata=pd.read_csv("edgar14Afacts.csv", parse_dates={"dater": [2]}, infer_datetime_format=True)\ndata2=pd.read_csv("sdcmergersdata.csv", parse_dates={"dater": [17]}, infer_datetime_format=True)\nlist(data2.columns.values)\n\ndata2.rename(columns=lambda x: x.replace(\'\\r\\n\', \'\'), inplace=True)\ndata2.rename(columns=lambda x: x.replace(\'\\n\', \'\'), inplace=True)\ndata2.rename(columns=lambda x: x.replace(\'\\r\', \'\'), inplace=True)\ndata2=data2.rename(columns = {\'Acquiror Name\':\'name\'})\ndata2=data2.rename(columns = {\'dater\':\'date\'})\ndata=data.rename(columns = {\'dater\':\'date\'})\n\nlist(data2.columns.values)\n\ndata["name"]=data[\'name\'].map(str.lower)\ndata2["name"]=data2[\'name\'].map(str.lower)\ndata2[\'date\'].fillna(method=\'pad\')\ndata[\'namer1\']=data[\'name\']\ndata[\'dater1\']=data[\'date\']\ndata2[\'namer2\']=data2[\'name\']\ndata2[\'dater2\']=data2[\'date\']\n\nprint data.head()\nprint data2.head()\ndata[\'name\'] = data[\'name\'].map(lambda x: str(x)[:4])\ndata2[\'name\'] = data2[\'name\'].map(lambda x: str(x)[:4])\n\ndata3 = pd.merge(data, data2, how=\'left\', on=[\'date\',\'name\'])\ndata3.to_csv("check.csv")\n\n'
"I set up a simple DataFrame in pandas:\n\na = pandas.DataFrame([[1,2,3], [4,5,6], [7,8,9]], columns=['a','b','c'])\n&gt;&gt;&gt; print a\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n\n\nI would like to be able to alter a single element in the last row of. In pandas==0.13.1 I could use the following:\n\na.iloc[-1]['a'] = 77\n&gt;&gt;&gt; print a\n    a  b  c\n0   1  2  3\n1   4  5  6\n2  77  8  9\n\n\nbut after updating to pandas==0.14.1, I get the following warning when doing this: \n\nSettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_index,col_indexer] = value instead\n\n\nThe problem of course being that -1 is not an index of a, so I can't use loc. As the warning indicates, I have not changed column 'a' of the last row, I've only altered a discarded local copy. \n\nHow do I do this in the newer version of pandas? I realize I could use the index of the last row like:\n\na.loc[2,'a'] = 77\n\n\nBut I'll be working with tables where multiple rows have the same index, and I don't want to reindex my table every time. Is there a way to do this without knowing the index of the last row before hand?\n"
'This works p_table.apply(pd.Series.round) however it has no decimal places\n\nDocumentation says\n\nimport pandas as pd\n\nSeries.round(decimals=0, out=None)\n\n\ni tried this p_table.apply(pd.Series.round(2)) but get this error:\n\nunbound method round() must be called with Series instance as first argument (got int instance instead)\n\n\nHow do I round all elements in the data frame to two decimal places?\n\n[EDIT] Figured it out. \n\nimport numpy as np\nnp.round(p_table, decimals=2)\n\n'
"I am familiar with python but new to panda DataFrames. I have a dictionary like this:\n\na={'b':100,'c':300}\n\n\nAnd I would like to convert it to a DataFrame, where b and c are the column names, and the first row is 100,300 (100 is underneath b and 300 is underneath c). I would like a solution that can be generalized to a much longer dictionary, with many more items. Thank you!\n"
'I have a dataframe df with the following:\n\nIn [10]: df.index.unique()\nOut[10]: array([u\'DC\', nan, u\'BS\', u\'AB\', u\'OA\'], dtype=object)\n\n\nI can easily select out df.ix["DC"], df.ix["BS"], etc.  But I\'m having trouble selecting the nan indexes.\n\ndf.ix[nan], df.ix["nan"], df.ix[np.nan] all won\'t work.\n\n\nHow do I select the rows with nan as the index?\n'
"I have a dataframe that looks like this: \n\nOut[14]:\n    impwealth  indweight\n16     180000     34.200\n21     384000     37.800\n26     342000     39.715\n30    1154000     44.375\n31     421300     44.375\n32    1210000     45.295\n33    1062500     45.295\n34    1878000     46.653\n35     876000     46.653\n36     925000     53.476\n\n\nI want to calculate the weighted median of the column impwealth using the frequency weights in indweight. My pseudo code looks like this: \n\n# Sort `impwealth` in ascending order \ndf.sort('impwealth', 'inplace'=True)\n\n# Find the 50th percentile weight, P\nP = df['indweight'].sum() * (.5)\n\n# Search for the first occurrence of `impweight` that is greater than P \ni = df.loc[df['indweight'] &gt; P, 'indweight'].last_valid_index()\n\n# The value of `impwealth` associated with this index will be the weighted median\nw_median = df.ix[i, 'impwealth']\n\n\nThis method seems clunky, and I'm not sure it's correct. I didn't find a built in way to do this in pandas reference. What is the best way to go about finding weighted median?\n"
"I would like to delete the current row during iteration - using df.iterrows(), if it its certain column fails on my if condition.\n\nex.\n\nfor index, row in df:\n     if row['A'] == 0:\n          #remove/drop this row from the df\n          del df[index] #I tried this but it gives me an error\n\n\nThis might be a very easy one, but i still can't figure out how to do it.\nYour help will be very much appreciated!\n"
"I basically face the same problem posted here:Converting between datetime, Timestamp and datetime64\n\nbut I couldn't find satisfying answer from it, my question how to extract datetime from numpy.datetime64 type:\n\nif I try:\n\nnp.datetime64('2012-06-18T02:00:05.453000000-0400').astype(datetime.datetime)\n\n\nit gave me:\n     1339999205453000000L\n\nmy current solution is convert datetime64 into a string and then turn to datetime again. but it seems quite a silly method.\n"
'Im triying to obtain the most informative features from a textual corpus. From this well answered question I know that this task could be done as follows:\n\ndef most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\n\n\nThen:\n\nmost_informative_feature_for_class(tfidf_vect, clf, 5)\n\n\nFor this classfier:\n\nX = tfidf_vect.fit_transform(df[\'content\'].values)\ny = df[\'label\'].values\n\n\nfrom sklearn import cross_validation\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X,\n                                                    y, test_size=0.33)\nclf = SVC(kernel=\'linear\', C=1)\nclf.fit(X, y)\nprediction = clf.predict(X_test)\n\n\nThe problem is the output of most_informative_feature_for_class:\n\n5 a_base_de_bien bastante   (0, 2451)   -0.210683496368\n  (0, 3533) -0.173621065386\n  (0, 8034) -0.135543062425\n  (0, 10346)    -0.173621065386\n  (0, 15231)    -0.154148294738\n  (0, 18261)    -0.158890483047\n  (0, 21083)    -0.297476572586\n  (0, 434)  -0.0596263855375\n  (0, 446)  -0.0753492277856\n  (0, 769)  -0.0753492277856\n  (0, 1118) -0.0753492277856\n  (0, 1439) -0.0753492277856\n  (0, 1605) -0.0753492277856\n  (0, 1755) -0.0637950312345\n  (0, 3504) -0.0753492277856\n  (0, 3511) -0.115802483001\n  (0, 4382) -0.0668983049212\n  (0, 5247) -0.315713152154\n  (0, 5396) -0.0753492277856\n  (0, 5753) -0.0716096348446\n  (0, 6507) -0.130661516772\n  (0, 7978) -0.0753492277856\n  (0, 8296) -0.144739048504\n  (0, 8740) -0.0753492277856\n  (0, 8906) -0.0753492277856\n  : :\n  (0, 23282)    0.418623443832\n  (0, 4100) 0.385906085143\n  (0, 15735)    0.207958503155\n  (0, 16620)    0.385906085143\n  (0, 19974)    0.0936828782325\n  (0, 20304)    0.385906085143\n  (0, 21721)    0.385906085143\n  (0, 22308)    0.301270427482\n  (0, 14903)    0.314164150621\n  (0, 16904)    0.0653764031957\n  (0, 20805)    0.0597723455204\n  (0, 21878)    0.403750815828\n  (0, 22582)    0.0226150073272\n  (0, 6532) 0.525138162099\n  (0, 6670) 0.525138162099\n  (0, 10341)    0.525138162099\n  (0, 13627)    0.278332617058\n  (0, 1600) 0.326774799211\n  (0, 2074) 0.310556919237\n  (0, 5262) 0.176400451433\n  (0, 6373) 0.290124806858\n  (0, 8593) 0.290124806858\n  (0, 12002)    0.282832270298\n  (0, 15008)    0.290124806858\n  (0, 19207)    0.326774799211\n\n\nIt is not returning the label nor the words. Why this is happening and how can I print the words and the labels?. Do you guys this is happening since I am using pandas to read the data?. Another thing I tried is the following, form this question:\n\ndef print_top10(vectorizer, clf, class_labels):\n    """Prints features with the highest coefficient values, per class"""\n    feature_names = vectorizer.get_feature_names()\n    for i, class_label in enumerate(class_labels):\n        top10 = np.argsort(clf.coef_[i])[-10:]\n        print("%s: %s" % (class_label,\n              " ".join(feature_names[j] for j in top10)))\n\n\nprint_top10(tfidf_vect,clf,y)\n\n\nBut I get this traceback:\n\nTraceback (most recent call last):\n\n  File "/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py", line 237, in &lt;module&gt;\n    print_top10(tfidf_vect,clf,5)\n  File "/Users/user/PycharmProjects/TESIS_FINAL/Classification/Supervised_learning/Final/experimentos/RBF/SVM_con_rbf.py", line 231, in print_top10\n    for i, class_label in enumerate(class_labels):\nTypeError: \'int\' object is not iterable\n\n\nAny idea of how to solve this, in order to get the features with the highest coefficient values?.\n'
'I am experiencing some really weird interactions between h5py, PyTables (via Pandas), and C++ generated HDF5 files. It seems that, h5check and h5py seem to cope with type names containing \'/\' but pandas/PyTables cannot. Clearly, there is a gap in my understanding, so:\n\nWhat have I not understood here?\n\n\n\nThe gory details\n\nI have the following data in a HDF5 file:\n\n   [...]\n   DATASET "log" {\n      DATATYPE  H5T_COMPOUND {\n         H5T_COMPOUND {\n            H5T_STD_U32LE "sec";\n            H5T_STD_U32LE "usec";\n         } "time";\n         H5T_IEEE_F32LE "CIF/align/aft_port_end/extend_pressure";\n         [...]\n\n\nThis was created via the C++ API. The h5check utility says the file is valid.\n\nNote that CIF/align/aft_port_end/extend_pressure is not meant as a path to a group/node/leaf.  It is a label, that we use internally which happens to have some internal structure that contains \'/\' as delimiters.  We do not want the HDF5 file to know anything about that: it should not care. Clearly, if \'/\' are illegal in any HDF5 name, then we have to change that delimiter to something else.\n\nUsing PyTables (okay, Pandas but it uses PyTables internally) to read the file, I get an \n\n &gt;&gt;&gt; import pandas as pd\n &gt;&gt;&gt; store = pd.HDFStore(\'data/XXX-20150423-071618.h5\')\n &gt;&gt;&gt; store\n/home/XXX/virt/env/develop/lib/python2.7/site-packages/tables/group. py:1156: UserWarning: problems loading leaf ``/log``::\n\n  the ``/`` character is not allowed in object names: \'XXX/align/aft_port_end/extend_pressure\'\n\nThe leaf will become an ``UnImplemented`` node. \n\n\nI asked about this in this question and got told that \'/\' are illegal in the specification. However, things get stranger with h5py...\n\nUsing h5py to read the file, I get what I want:\n\n&gt;&gt;&gt; f[\'/log\'].dtype\n&gt;&gt;&gt;\xa0dtype([(\'time\', [(\'sec\', \'&lt;u4\'), (\'usec\', \'&lt;u4\')]), (\'CI\nF/align/aft_port_end/extend_pressure\', \'&lt;f4\')[...]\n\n\nWhich is more or less what I set out with.\n\nNeedless to say, I am confused. Have I managed to create an illegal HDF5 file that somehow passes h5check?  Is PyTables not supporting this edge case? ... I am confused.\n\n\n\nClearly, I could write a simple wrapper something like this:\n\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; silly = pd.DataFrame(f[\'/log\'][\'CIF/align/aft_port_end/extend_pressure\'])\n&gt;&gt;&gt; silly.plot()\n&gt;&gt;&gt; plt.show()\n\n\nto get all the data from the HDF5 file into Pandas. However, I am not sure if this is a good idea because of the confusion earlier. My biggest worry is the conversion might not scale if the data is very large...\n'
"I face to a modification of a dataframe inside a function that I have never observed previously. Is there a method to deal with this and no modify the initial dataframe ?\n\nIn[30]: def test(df):\n    df['tt'] = np.nan\n    return df\n\nIn[31]: dff = pd.DataFrame(data=[])\n\nIn[32]: dff\n\nOut[32]: \nEmpty DataFrame\nColumns: []\nIndex: []\nIn[33]: df = test(dff)\n\nIn[34]: dff\n\nOut[34]: \nEmpty DataFrame\nColumns: [tt]\nIndex: []\n\n"
"For a dataframe like this:\n\nd = {'id': [1,1,1,2,2], 'Month':[1,2,3,1,3],'Value':[12,23,15,45,34], 'Cost':[124,214,1234,1324,234]}\ndf = pd.DataFrame(d)\n\n     Cost  Month  Value  id  \n0    124       1     12   1  \n1    214       2     23   1  \n2    1234      3     15   1  \n3    1324      1     45   2  \n4    234       3     34   2  \n\n\nto which I apply pivot_table\n\ndf2 =    pd.pivot_table(df, \n                        values=['Value','Cost'],\n                        index=['id'],\n                        columns=['Month'],\n                        aggfunc=np.sum,\n                        fill_value=0)\n\n\nto get df2:\n\n       Cost            Value          \nMonth     1    2     3     1   2   3   \nid                                  \n1       124  214  1234    12  23  15\n2      1324    0   234    45   0  34\n\n\nis there an easy way to format resulting dataframe column names like\n\nid     Cost1    Cost2     Cost3 Value1   Value2   Value3   \n1       124      214      1234    12        23       15\n2      1324       0       234     45         0       34\n\n\nIf I do:\n\ndf2.columns =[s1 + str(s2) for (s1,s2) in df2.columns.tolist()]\n\n\nI get:\n\n    Cost1  Cost2  Cost3  Value1  Value2  Value3\nid                                             \n1     124    214   1234      12      23      15\n2    1324      0    234      45       0      34\n\n\nHow to get rid of the extra level?\n\nthanks!\n"
"I really like pandas to handle and analyze big datasets. So far, I have mostly used matplotlib for plotting but now want to use pandas own plot  functionalities (based on matplotlib) since it needs less code and seems to be sufficient for me in most cases. Especially the subplots to have a guick glance at big dataframes like in the following example..\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate random data\ndf = pd.DataFrame(np.random.randn(96,12),\n                  columns=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n                           'K', 'L'])\n\n# Plotting\ndf.plot(kind='line', subplots=True, grid=True, title=&quot;Sample Data (Unit)&quot;,\n        layout=(4, 3), sharex=True, sharey=False, legend=True,    \n        style=['r', 'r', 'r', 'g', 'g', 'g', 'b', 'b', 'b', 'r', 'r', 'r'],\n        xticks=np.arange(0, len(df), 16))\n\n\n..which brings me to my questions:\n1.) How can I place all legends in the subplots at the same place (e. g. centered, outside, topright)?\n2.) Can I somehow use matplotlibs &quot;Tight Layout&quot; (http://matplotlib.org/users/tight_layout_guide.html) for the plot?\nThanks in advance!\n"
'I have a Pandas DataFrame in which one of the columns contains string elements, and those string elements contain new lines that I would like to print literally. But they just appear as \\n in the output.\n\nThat is, I want to print this:\n\n  pos     bidder\n0   1\n1   2\n2   3  &lt;- alice\n       &lt;- bob\n3   4\n\n\nbut this is what I get:\n\n  pos            bidder\n0   1\n1   2\n2   3  &lt;- alice\\n&lt;- bob\n3   4\n\n\nHow can I accomplish what I want? Can I use a DataFrame, or will I have to revert to manually printing padded columns one row at a time?\n\nHere\'s what I have so far:\n\nn = 4\noutput = pd.DataFrame({\n    \'pos\': range(1, n+1),\n    \'bidder\': [\'\'] * n\n})\nbids = {\'alice\': 3, \'bob\': 3}\nused_pos = []\nfor bidder, pos in bids.items():\n    if pos in used_pos:\n        arrow = output.ix[pos, \'bidder\']\n        output.ix[pos, \'bidder\'] = arrow + "\\n&lt;- %s" % bidder\n    else:\n        output.ix[pos, \'bidder\'] = "&lt;- %s" % bidder\nprint(output)\n\n'
'In pandas and seaborn, it is possible to temporarily change the display/plotting options by using the with keyword, which applies the specified setting only to the indented code, while leaving the global settings untouched:\n\nprint(pd.get_option("display.max_rows"))\n\nwith pd.option_context("display.max_rows",10):\n    print(pd.get_option("display.max_rows"))\n\nprint(pd.get_option("display.max_rows"))\n\n\nOut:\n\n60\n10\n60\n\n\nWhen I similarly try with mpl.rcdefaults(): or with mpl.rc(\'lines\', linewidth=2, color=\'r\'):, I receive AttributeError: __exit__.\n\nIs there a way to temporarily change the rcParams in matplotlib, so that they only apply to a selected subset of the code, or do I have to keep switching back and forth manually?\n'
'Consider following dataframe which has columns with same name (Apparently this does happens, currently I have a dataset like this! :( )\n\n&gt;&gt;&gt; df = pd.DataFrame({"a":range(10,15),"b":range(5,10)})\n&gt;&gt;&gt; df.rename(columns={"b":"a"},inplace=True)\ndf\n\n    a   a\n0   10  5\n1   11  6\n2   12  7\n3   13  8\n4   14  9\n\n&gt;&gt;&gt; df.columns\nIndex([\'a\', \'a\'], dtype=\'object\')\n\n\nI would expect that when dropping by index , only the column with the respective index would be gone, but apparently this is not the case.\n\n&gt;&gt;&gt; df.drop(df.columns[-1],1)\n\n0\n1\n2\n3\n4\n\n\nIs there a way to get rid of columns with duplicated column names? \n\nEDIT: I choose missleading values for the first column, fixed now\n\nEDIT2: the expected outcome is\n\n  a\n0 10\n1 11\n2 12 \n3 13\n4 14\n\n'
"Consider the following pandas dataframe:\n\nIn [114]:\n\ndf['movie_title'].head()\n\n\u200b\nOut[114]:\n\n0     Toy Story (1995)\n1     GoldenEye (1995)\n2    Four Rooms (1995)\n3    Get Shorty (1995)\n4       Copycat (1995)\n...\nName: movie_title, dtype: object\n\n\nUpdate:\nI would like to extract with a regular expression just the titles of the movies. So, let's use the following regex: \\b([^\\d\\W]+)\\b. So I tried the following:\n\ndf_3['movie_title'] = df_3['movie_title'].str.extract('\\b([^\\d\\W]+)\\b')\ndf_3['movie_title']\n\n\nHowever, I get the following:\n\n0       NaN\n1       NaN\n2       NaN\n3       NaN\n4       NaN\n5       NaN\n6       NaN\n7       NaN\n8       NaN\n\n\nAny idea of how to extract specific features from text in a pandas dataframe?. More specifically, how can I extract just the titles of the movies in a completely new dataframe?. For instance, the desired output should be:\n\nOut[114]:\n\n0     Toy Story\n1     GoldenEye\n2    Four Rooms\n3    Get Shorty\n4       Copycat\n...\nName: movie_title, dtype: object\n\n"
'I have been trying to work on this issue for a while.I am trying to remove non ASCII characters form DB_user column and trying to replace them with spaces. But I keep getting some errors. This is how my data frame looks:\n\n\n\n+-----------------------------------------------------------\n|      DB_user                            source   count  |                                             \n+-----------------------------------------------------------\n| ???/"Ò|Z?)?]??C %??J                      A        10   |                                       \n| ?D$ZGU   ;@D??_???T(?)                    B         3   |                                       \n| ?Q`H??M\'?Y??KTK$?Ù‹???Ð©JL4??*?_??        C         2   |                                        \n+-----------------------------------------------------------\n\n\n\nI was using this function, which I had come across while researching the problem on SO.\n\ndef filter_func(string):\n   for i in range(0,len(string)):\n\n\n      if (ord(string[i])&lt; 32 or ord(string[i])&gt;126\n           break\n\n      return \'\'\n\nAnd then using the apply function:\n\ndf[\'DB_user\'] = df.apply(filter_func,axis=1)\n\n\nI keep getting the error:\n\n\n\n\'ord() expected a character, but string of length 66 found\', u\'occurred at index 2\'\n\n\nHowever, I thought by using the loop in the filter_func function, I was dealing with this by inputing a char into \'ord\'. Therefore the moment it hits a non-ASCII character, it should be replaced by a space.\n\nCould somebody help me out?\n\nThanks!\n'
'I am new to pandas and matplotlib. Couldn\'t able to get exact reference to plot my DataFrame whose schema is as follows\n\nschema = StructType([\nStructField("x", IntegerType(), True),\nStructField("y", IntegerType(), True),\nStructField("z", IntegerType(), True)])\n\n\nLike to plot 3d graph w.r.t. x, y and z\n\nHere is the sample code i used\n\nimport matplotlib.pyplot as pltt\n\ndfSpark = sqlContext.createDataFrame(tupleRangeRDD, schema) // reading as spark df\ndf = dfSpark.toPandas()\nfig = pltt.figure();\nax = fig.add_subplot(111, projection=\'3d\')\nax.plot_surface(df[\'x\'], df[\'y\'], df[\'z\']) \n\n\nI am getting a empty graph plot. definitely missing something. Any pointers?\n\n-Thx\n\nRequest-1: Print df\n\ndef print_full(x):\npd.set_option(\'display.max_rows\', len(x))\nprint(x)\npd.reset_option(\'display.max_rows\')\n\n\nprint_full(df)\n\n\nResult of top 10\n\n         x    y       z\n0      301  301      10\n1      300  301      16\n2      300  300       6\n3      299  301      30\n4      299  300      20\n5      299  299      14\n6      298  301      40\n7      298  300      30\n8      298  299      24\n9      298  298      10\n10     297  301      48\n\n'
'Maximum Drawdown is a common risk metric used in quantitative finance to assess the largest negative return that has been experienced.\n\nRecently, I became impatient with the time to calculate max drawdown using my looped approach.\n\ndef max_dd_loop(returns):\n    """returns is assumed to be a pandas series"""\n    max_so_far = None\n    start, end = None, None\n    r = returns.add(1).cumprod()\n    for r_start in r.index:\n        for r_end in r.index:\n            if r_start &lt; r_end:\n                current = r.ix[r_end] / r.ix[r_start] - 1\n                if (max_so_far is None) or (current &lt; max_so_far):\n                    max_so_far = current\n                    start, end = r_start, r_end\n    return max_so_far, start, end\n\n\nI\'m familiar with the common perception that a vectorized solution would be better.\n\nThe questions are:\n\n\ncan I vectorize this problem?\nWhat does this solution look like?\nHow beneficial is it?\n\n\n\n\nEdit\n\nI modified Alexander\'s answer into the following function:\n\ndef max_dd(returns):\n    """Assumes returns is a pandas Series"""\n    r = returns.add(1).cumprod()\n    dd = r.div(r.cummax()).sub(1)\n    mdd = dd.min()\n    end = dd.argmin()\n    start = r.loc[:end].argmax()\n    return mdd, start, end\n\n'
"My index:\n\nIndex([u'Newal', u'Saraswati Khera', u'Tohana'], dtype='object')\n\n\nI have to convert this format into list with  following format:\n\n['Newal','SaraswatiKhera','Tohana']\n\n"
'I was motivated to use pandas rolling feature to perform a rolling multi-factor regression (This question is NOT about rolling multi-factor regression).  I expected that I\'d be able to use apply after a df.rolling(2) and take the resulting pd.DataFrame extract the ndarray with .values and perform the requisite matrix multiplication.  It didn\'t work out that way.\n\nHere is what I found:\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(5, 2).round(2), columns=[\'A\', \'B\'])\nX = np.random.rand(2, 1).round(2)\n\n\nWhat do objects look like:\n\nprint "\\ndf = \\n", df\nprint "\\nX = \\n", X\nprint "\\ndf.shape =", df.shape, ", X.shape =", X.shape\n\ndf = \n      A     B\n0  0.44  0.41\n1  0.46  0.47\n2  0.46  0.02\n3  0.85  0.82\n4  0.78  0.76\n\nX = \n[[ 0.93]\n [ 0.83]]\n\ndf.shape = (5, 2) , X.shape = (2L, 1L)\n\n\nMatrix multiplication behaves normally:\n\ndf.values.dot(X)\n\narray([[ 0.7495],\n       [ 0.8179],\n       [ 0.4444],\n       [ 1.4711],\n       [ 1.3562]])\n\n\nUsing apply to perform row by row dot product behaves as expected:\n\ndf.apply(lambda x: x.values.dot(X)[0], axis=1)\n\n0    0.7495\n1    0.8179\n2    0.4444\n3    1.4711\n4    1.3562\ndtype: float64\n\n\nGroupby -> Apply behaves as I\'d expect:\n\ndf.groupby(level=0).apply(lambda x: x.values.dot(X)[0, 0])\n\n0    0.7495\n1    0.8179\n2    0.4444\n3    1.4711\n4    1.3562\ndtype: float64\n\n\nBut when I run:\n\ndf.rolling(1).apply(lambda x: x.values.dot(X))\n\n\nI get:\n\n\n  AttributeError: \'numpy.ndarray\' object has no attribute \'values\'\n\n\nOk, so pandas is using straight ndarray within its rolling implementation.  I can handle that.  Instead of using .values to get the ndarray, let\'s try:\n\ndf.rolling(1).apply(lambda x: x.dot(X))\n\n\n\n  shapes (1,) and (2,1) not aligned: 1 (dim 0) != 2 (dim 0)\n\n\nWait!  What?!\n\nSo I created a custom function to look at the what rolling is doing.\n\ndef print_type_sum(x):\n    print type(x), x.shape\n    return x.sum()\n\n\nThen ran:\n\nprint df.rolling(1).apply(print_type_sum)\n\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n&lt;type \'numpy.ndarray\'&gt; (1L,)\n      A     B\n0  0.44  0.41\n1  0.46  0.47\n2  0.46  0.02\n3  0.85  0.82\n4  0.78  0.76\n\n\nMy resulting pd.DataFrame is the same, that\'s good.  But it printed out 10 single dimensional ndarray objects.  What about rolling(2)\n\nprint df.rolling(2).apply(print_type_sum)\n\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n&lt;type \'numpy.ndarray\'&gt; (2L,)\n      A     B\n0   NaN   NaN\n1  0.90  0.88\n2  0.92  0.49\n3  1.31  0.84\n4  1.63  1.58\n\n\nSame thing, expect output but it printed 8 ndarray objects.  rolling is producing a single dimensional ndarray of length window for each column as opposed to what I expected which was an ndarray of shape (window, len(df.columns)).\n\nQuestion is Why?\n\nI now don\'t have a way to easily run a rolling multi-factor regression.\n'
"I have a csv file as follows:\n\n0 5\n1 10\n2 15\n3 20\n4 25\n\n\nI want to save it as a dataframe with x,y axes as names, then plot it. However when I assign x,y I get a messed up DataFrame, what is happening?\n\ncolumn_names = ['x','y']\nx = pd.read_csv('csv-file.csv', header = None, names = column_names)\nprint(x)\n\n          x   y\n0   0 5 NaN\n1  1 10 NaN\n2  2 15 NaN\n3  3 20 NaN\n4  4 25 NaN\n\n\nI've tried without specifying None for header, to no avail.\n"
'Is anyone can provide example how to create zip file from csv file using Python/Pandas package?\nThank you\n'
"I met a problem that when I use pandas to read Mysql table, some columns (see 'to_nlc') used to be integer became a float number (automatically add .0 after that).\nCan anyone figure it out? Or some guessings? Thanks very much!\n\n\n\n"
"I am trying to multiply each row of a pandas dataframe by a different value and wondering what the best way to do this is.\n\nFor example if I have the following dataframe:\n\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame(np.random.randn(2, 3))\ndf\n     0          1           2\n0   -1.283316   0.849488    1.936060\n1   -2.078575   -0.871570   -0.970261\n\n\nI want to multiply each element of each row by a different in a list or array\n\nvals = [1, 100]\n\n\nIn this example I want each item in the first row to be multiplied by 1 and each item in the second row to be multiplied by 100\n\nthe result should therefore be:\n\n     0          1           2\n0   -1.283316   0.849488    1.936060\n1   -207.8575   -87.1570    -97.0261\n\n\nI have tried:\n\ndf * vals\ndf.multiply(vals)\ndf.multiply(vals, axis=1)\n\n\nNone of which work, although I was not expecting them too, based on my understanding of what that code should do. I can't figure out a concise way to do this with pandas, any help is appreciated, thanks.\n"
"Let's say I have a dataframe df and I would like to create a new column filled with 0, I use:\n\ndf['new_col'] = 0\n\n\nThis far, no problem. But if the value I want to use is a list, it doesn't work:\n\ndf['new_col'] = my_list\n\nValueError: Length of values does not match length of index\n\n\nI understand why this doesn't work (pandas is trying to assign one value of the list per cell of the column), but how can we avoid this behavior? (if it isn't clear I would like every cell of my new column to contain the same predefined list)\n\nNote: I also tried: df.assign(new_col = my_list), same problem\n"
'I have to parse an xml file which gives me datetimes in Excel style; for example: 42580.3333333333.\n\nDoes Pandas provide a way to convert that number into a regular datetime object?\n'
"Setup\n\nconsider the following dataframe (note the strings):\n\ndf = pd.DataFrame([['3', '11'], ['0', '2']], columns=list('AB'))\ndf\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 2 columns):\nA    2 non-null object\nB    2 non-null object\ndtypes: object(2)\nmemory usage: 104.0+ bytes\n\n\nQuestion\n\nI'm going to sum.  I expect the strings to be concatenated.\n\ndf.sum()\n\nA     30.0\nB    112.0\ndtype: float64\n\n\nIt looks as though the strings were concatenated then converted to float.  Is there a good reason for this?  Is this a bug?  Anything enlightening will be up voted.\n"
"How to multiply all the numeric values in the data frame by a constant without having to specify column names explicitly? Example:\n\nIn [13]: df = pd.DataFrame({'col1': ['A','B','C'], 'col2':[1,2,3], 'col3': [30, 10,20]})\n\nIn [14]: df\nOut[14]: \n  col1  col2  col3\n0    A     1    30\n1    B     2    10\n2    C     3    20\n\n\nI tried df.multiply but it affects the string values as well by concatenating them several times.\n\nIn [15]: df.multiply(3)\nOut[15]: \n  col1  col2  col3\n0  AAA     3    90\n1  BBB     6    30\n2  CCC     9    60\n\n\nIs there a way to preserve the string values intact while multiplying only the numeric values by a constant? \n"
"I have a lot of different table (and other unstructured data in an excel sheet) .. I need to create a dataframe out of range 'A3:D20' from 'Sheet2' of Excel sheet 'data'.\n\nAll examples that I come across drilldown up to sheet level, but not how to pick it from an exact range.\n\nimport openpyxl\nimport pandas as pd\n\nwb = openpyxl.load_workbook('data.xlsx')\nsheet = wb.get_sheet_by_name('Sheet2')\nrange = ['A3':'D20']   #&lt;-- how to specify this?\nspots = pd.DataFrame(sheet.range) #what should be the exact syntax for this?\n\nprint (spots)\n\n\nOnce I get this, I plan to look up data in column A and find its corresponding value in column B.\n\nEdit 1: I realised that openpyxl takes too long, and so have changed that to pandas.read_excel('data.xlsx','Sheet2') instead, and it is much faster at that stage at least.\n\nEdit 2: For the time being, I have put my data in just one sheet and:\n\n\nremoved all other info\nadded column names,  \napplied index_col on my leftmost column\nthen used wb.loc[]\n\n"
"Based on python, sort descending dataframe with pandas:\n\nGiven:\n\nfrom pandas import DataFrame\nimport pandas as pd\n\nd = {'x':[2,3,1,4,5],\n     'y':[5,4,3,2,1],\n     'letter':['a','a','b','b','c']}\n\ndf = DataFrame(d)\n\n\ndf then looks like this:\n\ndf:\n      letter    x    y\n    0      a    2    5\n    1      a    3    4\n    2      b    1    3\n    3      b    4    2\n    4      c    5    1\n\n\nI would like to have something like:\n\nf = lambda x,y: x**2 + y**2\ntest = df.sort(f('x', 'y'))\n\n\nThis should order the complete dataframe with respect to the sum of the squared values of column 'x' and 'y' and give me:\n\ntest:\n      letter    x    y\n    2      b    1    3\n    3      b    4    2\n    1      a    3    4\n    4      c    5    1\n    0      a    2    5\n\n\nAscending or descending order does not matter. Is there a nice and simple way to do that? I could not yet find a solution.\n"
'I am trying calculate a regression output using python library but I am unabl;e to get the intercept value when I use the library:\n\nimport statsmodels.api as sm\n\n\nIt prints all the regression analysis except the intercept. \n\nbut when I use:\n\nfrom pandas.stats.api import ols\n\n\nMy code for pandas:\n\nRegression = ols(y= Sorted_Data3[\'net_realization_rate\'],x = Sorted_Data3[[\'Cohort_2\',\'Cohort_3\']])\nprint Regression  \n\n\nI get the the intercept with a warning that this librabry will be deprecated in the future so I am trying to use Statsmodels.\n\nthe warning that I get while using pandas.stats.api:\n\n\n  Warning (from warnings module):\n   File "C:\\Python27\\lib\\idlelib\\run.py", line 325\n      exec code in self.locals\n  FutureWarning: The pandas.stats.ols module is deprecated and will be removed in a future version. We refer to external packages like statsmodels, see some examples here: http://statsmodels.sourceforge.net/stable/regression.html\n\n\nMy code for Statsmodels:\n\nimport pandas as pd\nimport numpy as np\nfrom pandas.stats.api import ols\nimport statsmodels.api as sm\n\nData1 = pd.read_csv(\'C:\\Shank\\Regression.csv\')  #Importing CSV\nprint Data1\n\n\nrunning some cleaning code\n\nsm_model = sm.OLS(Sorted_Data3[\'net_realization_rate\'],Sorted_Data3[[\'Cohort_2\',\'Cohort_3\']])\nresults = sm_model.fit()\nprint \'\\n\'\nprint results.summary()\n\n\nI even tried statsmodels.formula.api:\nas:\n\nsm_model = sm.OLS(formula ="net_realization_rate ~ Cohort_2 + Cohort_3", data = Sorted_Data3)\nresults = sm_model.fit()\nprint \'\\n\'\nprint result.params\nprint \'\\n\'\nprint results.summary()\n\n\nbut I get the error: \n\n\n  TypeError: init() takes at least 2 arguments (1 given)\n\n\nFinal output:\n1st is from pandas 2nd is from Stats.... I want the intercept vaule as the one from pandas from stats also:\n\n'
"Using requests I am creating an object which is in .csv format. How can I then write that object to a DataFrame with pandas?\n\nTo get the requests object in text format:\n\nimport requests\nimport pandas as pd\nurl = r'http://test.url' \nr = requests.get(url)\nr.text  #this will return the data as text in csv format\n\n\nI tried (doesn't work):\n\npd.read_csv(r.text)\npd.DataFrame.from_csv(r.text)\n\n"
"The docs provide good examples, how metadata can be provided. However I still feel unsure, when it comes to picking the right dtypes for my dataframe.\n\n\nCould I do something like meta={'x': int 'y': float,\n'z': float} instead of meta={'x': 'i8', 'y': 'f8', 'z': 'f8'}?\nCould somebody hint me to a list of possible values like 'i8'?  What\ndtypes exist? \nHow    can I specify a column, that contains arbitrary objects? How can I    specify a column, that contains only instances of one class?\n\n"
'I have a problem with appending of dataframe.\nI try to execute this code\n\ndf_all = pd.read_csv(\'data.csv\', error_bad_lines=False, chunksize=1000000)\nurls = pd.read_excel(\'url_june.xlsx\')\nsubstr = urls.url.values.tolist()\ndf_res = pd.DataFrame()\nfor df in df_all:\n    for i in substr:\n        res = df[df[\'url\'].str.contains(i)]\n        df_res.append(res)\n\n\nAnd when I try to save df_res I get empty dataframe.\ndf_all looks like \n\nID,"url","used_at","active_seconds"\nb20f9412f914ad83b6611d69dbe3b2b4,"mobiguru.ru/phones/apple/comp/32gb/apple_iphone_5s.html",2015-10-01 00:00:25,1\nb20f9412f914ad83b6611d69dbe3b2b4,"mobiguru.ru/phones/apple/comp/32gb/apple_iphone_5s.html",2015-10-01 00:00:31,30\nf85ce4b2f8787d48edc8612b2ccaca83,"4pda.ru/forum/index.php?showtopic=634566&amp;view=getnewpost",2015-10-01 00:01:49,2\nd3b0ef7d85dbb4dbb75e8a5950bad225,"shop.mts.ru/smartfony/mts/smartfon-smart-sprint-4g-sim-lock-white.html?utm_source=admitad&amp;utm_medium=cpa&amp;utm_content=300&amp;utm_campaign=gde_cpa&amp;uid=3",2015-10-01 00:03:19,34\n078d388438ebf1d4142808f58fb66c87,"market.yandex.ru/product/12675734/spec?hid=91491&amp;track=char",2015-10-01 00:03:48,2\nd3b0ef7d85dbb4dbb75e8a5950bad225,"avito.ru/yoshkar-ola/telefony/mts",2015-10-01 00:04:21,4\nd3b0ef7d85dbb4dbb75e8a5950bad225,"shoppingcart.aliexpress.com/order/confirm_order",2015-10-01 00:04:25,1\nd3b0ef7d85dbb4dbb75e8a5950bad225,"shoppingcart.aliexpress.com/order/confirm_order",2015-10-01 00:04:26,9\n\n\nand urls looks like\n\nurl\nshoppingcart.aliexpress.com/order/confirm_order\nozon.ru/?context=order_done&amp;number=\nlk.wildberries.ru/basket/orderconfirmed\nlamoda.ru/checkout/onepage/success/quick\nmvideo.ru/confirmation?_requestid=\neldorado.ru/personal/order.php?step=confirm\n\n\nWhen I print res in a loop it doesn\'t empty. But when I try print in a loop df_res after append, it return empty dataframe.\nI can\'t find my error. How can I fix it?\n'
'I have the following code in PyCharm\n\nimport pandas as pd\n\nimport numpy as np\n\nimport matplotlib as plt\n\ndf = pd.read_csv("c:/temp/datafile.txt", sep=\'\\t\')\n\ndf.head(10)\n\n\nI get the following output:\n\nProcess finished with exit code 0\n\n\nI am supposed to get the first ten rows of my datafile, but these do not appear in PyCharm.\n\nI checked the Project interpreter and all settings seem to be alright there. The right packages are installed (numpy, pandas, matplotlib) under the right Python version. \n\nWhat am I doing wrong? Thanks.\n'
'I have a pattern:\n\npatternDel = "( \\\\((MoM|QoQ)\\\\))";\n\n\nAnd I want to delete all rows in pandas dataframe where column df[\'Event Name\'] matches this pattern. Which is the best way to do it?  There are more than 100k rows in dataframe.\n'
'I want to predict a value at a date in the future with simple linear regression, but I can\'t due to the date format. \n\nThis is the dataframe I have: \n\ndata_df = \ndate          value\n2016-01-15    1555\n2016-01-16    1678\n2016-01-17    1789\n...  \n\ny = np.asarray(data_df[\'value\'])\nX = data_df[[\'date\']]\nX_train, X_test, y_train, y_test = train_test_split             \n(X,y,train_size=.7,random_state=42)\n\nmodel = LinearRegression() #create linear regression object\nmodel.fit(X_train, y_train) #train model on train data\nmodel.score(X_train, y_train) #check score\n\nprint (‘Coefficient: \\n’, model.coef_)\nprint (‘Intercept: \\n’, model.intercept_) \ncoefs = zip(model.coef_, X.columns)\nmodel.__dict__\nprint "sl = %.1f + " % model.intercept_ + \\\n     " + ".join("%.1f %s" % coef for coef in coefs) #linear model\n\n\nI tried to convert the date unsuccessfully\n\ndata_df[\'conv_date\'] = data_df.date.apply(lambda x: x.toordinal())\n\ndata_df[\'conv_date\'] = pd.to_datetime(data_df.date, format="%Y-%M-%D")\n\n'
"consider the list of lists l\n\nl = [[1, 2, 3], [1, 2]]\n\n\nif I convert this to a np.array I'll get a one dimensional object array with [1, 2, 3] in the first position and [1, 2] in the second position.\n\nprint(np.array(l))\n\n[[1, 2, 3] [1, 2]]\n\n\nI want this instead\n\nprint(np.array([[1, 2, 3], [1, 2, np.nan]]))\n\n[[  1.   2.   3.]\n [  1.   2.  nan]]\n\n\n\n\nI can do this with a loop, but we all know how unpopular loops are\n\ndef box_pir(l):\n    lengths = [i for i in map(len, l)]\n    shape = (len(l), max(lengths))\n    a = np.full(shape, np.nan)\n    for i, r in enumerate(l):\n        a[i, :lengths[i]] = r\n    return a\n\nprint(box_pir(l))\n\n[[  1.   2.   3.]\n [  1.   2.  nan]]\n\n\n\n\nhow do I do this in a fast, vectorized way?\n\n\n\ntiming  \n\n\n\n\n\nsetup functions  \n\n%%cython\nimport numpy as np\n\ndef box_pir_cython(l):\n    lengths = [len(item) for item in l]\n    shape = (len(l), max(lengths))\n    a = np.full(shape, np.nan)\n    for i, r in enumerate(l):\n        a[i, :lengths[i]] = r\n    return a\n\n\n\n\ndef box_divikar(v):\n    lens = np.array([len(item) for item in v])\n    mask = lens[:,None] &gt; np.arange(lens.max())\n    out = np.full(mask.shape, np.nan)\n    out[mask] = np.concatenate(v)\n    return out\n\ndef box_hpaulj(LoL):\n    return np.array(list(zip_longest(*LoL, fillvalue=np.nan))).T\n\ndef box_simon(LoL):\n    max_len = len(max(LoL, key=len))\n    return np.array([x + [np.nan]*(max_len-len(x)) for x in LoL])\n\ndef box_dawg(LoL):\n    cols=len(max(LoL, key=len))\n    rows=len(LoL)\n    AoA=np.empty((rows,cols, ))\n    AoA.fill(np.nan)\n    for idx in range(rows):\n        AoA[idx,0:len(LoL[idx])]=LoL[idx]\n    return AoA\n\ndef box_pir(l):\n    lengths = [len(item) for item in l]\n    shape = (len(l), max(lengths))\n    a = np.full(shape, np.nan)\n    for i, r in enumerate(l):\n        a[i, :lengths[i]] = r\n    return a\n\ndef box_pandas(l):\n    return pd.DataFrame(l).values\n\n"
'I somehow got a pandas.Series which contains a bunch of arrays in it, as the s in the code below.\n\ndata = [[1,2,3],[2,3,4],[3,4,5],[2,3,4],[3,4,5],[2,3,4],\n        [3,4,5],[2,3,4],[3,4,5],[2,3,4],[3,4,5]]\ns = pd.Series(data = data)\ns.shape # output ---&gt; (11L,)\n# try to convert s to matrix\nsm = s.as_matrix()\n# but...\nsm.shape # output ---&gt; (11L,)\n\n\nHow can I convert the s into a matrix with shape (11,3)? Thanks!\n'
'I have this dataframe:\n\n        id       text\n 0      12       boats\n 1      14       bicycle\n 2      15       car\n\n\nNow I want to make a select dropdown in jinja2. But I cannot find a way to loop over the dataframe in jinja2.\n\nI tried using to_dict(). But with\n\n{% for key,value in x.items() %}\n\n\nit loops over id and text instead of the rows. How can I change this so I can do something like this in the template?\n\n{% for key,value in x.items() %}\n    &lt;option value="{{ id }}"&gt;{{ text }}&lt;/option&gt;\n{% endfor %}\n\n'
"I have a really simple Pandas dataframe where each cell contains a list. I'd like to split each element of the list into it's own column. I can do that by exporting the values and then creating a new dataframe. This doesn't seem like a good way to do this especially, if my dataframe had a column aside from the list column.\n\nimport pandas as pd\n\ndf = pd.DataFrame(data=[[[8,10,12]],\n                        [[7,9,11]]])\n\ndf = pd.DataFrame(data=[x[0] for x in df.values])\n\n\nDesired output:\n\n   0   1   2\n0  8  10  12\n1  7   9  11\n\n\nFollow-up based on @Psidom answer:\n\nIf I did have a second column:\n\ndf = pd.DataFrame(data=[[[8,10,12], 'A'],\n                        [[7,9,11], 'B']])\n\n\nHow do I not loose the other column?\n\nDesired output:\n\n   0   1   2  3 \n0  8  10  12  A\n1  7   9  11  B\n\n"
"I have a data frame that looks something like this:\n\ndefaultdict(&lt;class 'list'&gt;, {'XYF':             TimeUS           GyrX           GyrY           GyrZ         AccX  \\\n0        207146570    0.000832914    0.001351716  -0.0004189798    -0.651183   \n1        207186671    0.001962787    0.001242457  -0.0001859666   -0.6423497   \n2        207226791   9.520243E-05    0.001076498  -0.0005664826   -0.6360412   \n3        207246474   0.0001093059    0.001616917   0.0003615251   -0.6342875   \n4        207286244    0.001412051   0.0007565815  -0.0003780428    -0.637755   \n\n\n[103556 rows x 12 columns], 'DAR':           TimeUS RSSI RemRSSI TxBuf Noise RemNoise RxErrors Fixed\n0      208046965  159     161    79    25       29        0     0\n1      208047074  159     161    79    25       29        0     0\n2      208927455  159     159    91    28       28        0     0\n3      208927557  159     159    91    28       28        0     0\n\n\n[4136 rows x 8 columns], 'NK2':            TimeUS    IVN    IVE   IVD    IPN   IPE    IPD IMX  IMY IMZ  IYAW  \\\n0       207147350  -0.02   0.02  0.00  -0.02  0.01   0.20   0    0   0  1.94   \n1       207187259  -0.02   0.02  0.00  -0.02  0.01   0.20   0    0   0  1.94   \n2       207227559  -0.02   0.02  0.00  -0.02  0.01   0.14   0    0   0  1.77   \n3       207308304   0.02   0.02  0.00  -0.01  0.01  -0.05   0    0   0  1.77   \n4       207347766   0.02   0.02  0.00  -0.01  0.01  -0.05   0    0   0  0.82  \n\n\nI first separated the column I want to do math with:\n\nnew_time = dfs['XYF']['TimeUS']\n\n\nThen I have tried several things to do some math on it but I had no luck.\nFirst I just treated it like a list. so\n\nnew_time_F = new_time / 1000000\n\n\nThat didn't work, gave me a float error of:\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\nso I did this:\n\nnew_time_F = float (new_time) / 1000000\n\n\nThis give me an error:\n\nTypeError: cannot convert the series to &lt;class 'float'&gt;\n\n\nI have no idea where to go from here. \n"
'I am trying to append values to a pandas Series obtained by finding the difference between the nth and nth + 1 element:\n\nq = pd.Series([])\n\nwhile i &lt; len(other array):\n    diff = some int value\n    a = pd.Series([diff], ignore_index=True)\n    q.append(a)\n    i+=1\n\n\nThe output I get is:\n\nSeries([], dtype: float64)\n\n\nWhy am I not getting an array with all the appended values?\n\n--\n\nP.S. This is a data science question where I have to find state with the most counties by searching through a dataframe. I am using the index values where one state ends and the next one begins (the values in the array that I am using to find the difference) to determine how many counties are in that state. If anyone knows how to solve this problem better than I am above, please let me know!\n'
'I am trying to send multiple dataframes as tables in an email. Using df.to_html() I am able to render a HTML string for the table which I am attaching as the part of the email body. I am successfully able to get the tables in the email.\n\nhtml.append(table.to_html(na_rep = " ",index = False))\nbody = \'\\r\\n\\n&lt;br&gt;\'.join(\'%s\'%item for item in html)\nmsg.attach(MIMEText(body, \'html\'))\n\n\nBut how do I add background color to the header of these tables?\n'
'I have a Pandas dataframe in Python. The contents of the dataframe are from here. I modified the case of the first alphabet in the "Single" column slightly. Here is what I have:\n\nimport pandas as pd\ndf = pd.read_csv(\'test.csv\')\nprint df\n\nPosition                       Artist                  Single               Year     Weeks\n       1                Frankie Laine               I Believe               1953  18 weeks\n       2                  Bryan Adams         I Do It for You               1991  16 weeks\n       3                  Wet Wet Wet      love Is All Around               1994  15 weeks\n       4  Drake (feat. Wizkid &amp; Kyla)               One Dance               2016  15 weeks\n       5                        Queen       bohemian Rhapsody  1975/76 &amp; 1991/92  14 weeks\n       6                 Slim Whitman              Rose Marie               1955  11 weeks\n       7              Whitney Houston  i Will Always Love You               1992  10 weeks\n\n\nI would like to sort by the Single column in ascending order (a to z). When I run\n\ndf.sort_values(by=\'Single\',inplace=True)\n\n\nit seems that the sort is not able to combine upper and lowercase. Here is what I get:\n\nPosition                       Artist                  Single               Year     Weeks\n       1                Frankie Laine               I Believe               1953  18 weeks\n       2                  Bryan Adams         I Do It for You               1991  16 weeks\n       4  Drake (feat. Wizkid &amp; Kyla)               One Dance               2016  15 weeks\n       6                 Slim Whitman              Rose Marie               1955  11 weeks\n       5                        Queen       bohemian Rhapsody  1975/76 &amp; 1991/92  14 weeks\n       7              Whitney Houston  i Will Always Love You               1992  10 weeks\n       3                  Wet Wet Wet      love Is All Around               1994  15 weeks\n\n\nSo, it is sorting by uppercase first and then performing a separate sort by lower case. I want a combined sort, regardless of the case of the starting alphabet in the Single column. The row with "bohemian Rhapsody" is in the wrong location after sorting. It should be first; instead it is appearing as the 5th row after the sort.\n\nIs there a way to do sort a Pandas DataFrame while ignoring the case of the text in the Single column?\n'
"I have a dataframe with 4 columns an ID and three categories that results fell into \n\n  &lt;80% 80-90 &gt;90\nid\n1   2     4    4\n2   3     6    1\n3   7     0    3\n\n\nI would like to convert it to percentages ie:\n\n   &lt;80% 80-90 &gt;90\nid\n1   20%   40%  40%\n2   30%   60%  10%\n3   70%    0%  30%\n\n\nthis seems like it should be within pandas capabilities but I just can't figure it out.\n\nThanks in advance!\n"
'Not sure how to fix . Any help much appreciate. I saw thi Vectorization: Not a valid collection but not sure if i understood this\n\ntrain = df1.iloc[:,[4,6]]\ntarget =df1.iloc[:,[0]]\n\ndef train(classifier, X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n    classifier.fit(X_train, y_train)\n    print ("Accuracy: %s" % classifier.score(X_test, y_test))\n    return classifier\n\ntrial1 = Pipeline([\n         (\'vectorizer\', TfidfVectorizer()),\n         (\'classifier\', MultinomialNB()),])\n\ntrain(trial1, train, target)\n\n\nerror below :\n\n    ----&gt; 6 train(trial1, train, target)\n\n    &lt;ipython-input-140-ac0e8d32795e&gt; in train(classifier, X, y)\n          1 def train(classifier, X, y):\n    ----&gt; 2     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n          3 \n          4     classifier.fit(X_train, y_train)\n          5     print ("Accuracy: %s" % classifier.score(X_test, y_test))\n\n    /home/manisha/anaconda3/lib/python3.5/site-packages/sklearn/model_selection/_split.py in train_test_split(*arrays, **options)\n       1687         test_size = 0.25\n       1688 \n    -&gt; 1689     arrays = indexable(*arrays)\n       1690 \n       1691     if stratify is not None:\n\n    /home/manisha/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py in indexable(*iterables)\n        204         else:\n        205             result.append(np.array(X))\n    --&gt; 206     check_consistent_length(*result)\n        207     return result\n        208 \n\n    /home/manisha/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)\n        175     """\n        176 \n    --&gt; 177     lengths = [_num_samples(X) for X in arrays if X is not None]\n        178     uniques = np.unique(lengths)\n        179     if len(uniques) &gt; 1:\n\n    /home/manisha/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py in &lt;listcomp&gt;(.0)\n        175     """\n        176 \n    --&gt; 177     lengths = [_num_samples(X) for X in arrays if X is not None]\n        178     uniques = np.unique(lengths)\n        179     if len(uniques) &gt; 1:\n\n    /home/manisha/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py in _num_samples(x)\n        124         if len(x.shape) == 0:\n        125             raise TypeError("Singleton array %r cannot be considered"\n    --&gt; 126                             " a valid collection." % x)\n        127         return x.shape[0]\n        128     else:\n\n    TypeError: Singleton array array(&lt;function train at 0x7f3a311320d0&gt;, dtype=object) cannot be considered a valid collection.\n\n ____\n\n\nNot sure how to fix . Any help much appreciate. I saw thi Vectorization: Not a valid collection but not sure if i understood this\n'
'I have two dataframes that I need to merge based on whether or not a date value fits in between two other dates. Basically, I need to perform an outer join where B.event_date is between A.start_date and A.end_date. It seems that merge and join always assume a common column which in this case, I do not have. \n\n    A                           B\n    start_date  end_date        event_date  price\n0   2017-03-27  2017-04-20  0   2017-01-20  100\n1   2017-01-10  2017-02-01  1   2017-01-27  200\n\nResult \n    start_date  end_date        event_date  price\n0   2017-03-27  2017-04-20  \n1   2017-01-10  2017-02-01      2017-01-20  100\n2   2017-01-10  2017-02-01      2017-01-27  200\n\n'
'I am wondering if there are any efficient methods or one-liner that, given a pandas DatetimeIndex date1, return a DatetimeIndex date2 that is the first day of the next month?\n\nFor example, if date1 is \'2011-09-30\' then date2 is \'2011-10-01\'?\n\nI have tried this one liner \n\n    df.index.to_period("M").to_timestamp(\'M\')\n\n\nBut this seems only able to return the "last day of the same month". Is it possible to do some datetime arithmetic here?\n\nThanks!\n'
"I have read loaded a csv file into a pandas dataframe and want to do some simple manipulations on the dataframe. I can not figure out how to create a new dataframe based on selected columns from my original dataframe. My attempt:\n\nnames = ['A','B','C','D']\ndataset = pandas.read_csv('file.csv', names=names)\nnew_dataset = dataset['A','D']\n\n\nI would like to create a new dataframe with the columns A and D from the original dataframe.\n"
'I have a dataframe object df of over 15000 rows like:\n\nanime_id          name              genre    rating\n1234      Kimi no nawa    Romance, Comedy     9.31\n5678       Stiens;Gate             Sci-fi     8.92\n\n\nAnd I am trying to find the row with a particular anime_id.\n\na_id = "5678"\ntemp = (df.query("anime_id == "+a_id).genre)\n\n\nI just wanted to know if this search was done in constant time (like dictionaries) or linear time(like lists).\n'
'I have this \'file.csv\' file to read with pandas:\n\nTitle|Tags\nT1|"[Tag1,Tag2]"\nT1|"[Tag1,Tag2,Tag3]"\nT2|"[Tag3,Tag1]"\n\n\nusing\n\ndf = pd.read_csv(\'file.csv\', sep=\'|\')\n\n\nthe output is:\n\n  Title              Tags\n0    T1       [Tag1,Tag2]\n1    T1  [Tag1,Tag2,Tag3]\n2    T2       [Tag3,Tag1]\n\n\nI know that the column Tags is a full string, since:\n\nIn [64]: df[\'Tags\'][0][0]\nOut[64]: \'[\'\n\n\nI need to read it as a list of strings like ["Tag1","Tag2"]. I tried the solution provided in this question but no luck there, since I have the [ and ] characters that actually mess up the things.\n\nThe expecting output should be:\n\nIn [64]: df[\'Tags\'][0][0]\nOut[64]: \'Tag1\'\n\n'
'I have pandas dataframe that I want to execute on it query function with isnull() or not isnull() condition like that:\n\nIn [67]: df_data = pd.DataFrame({\'a\':[1,20,None,40,50]})\nIn [68]: df_data\nOut[68]:       a\n         0   1.0\n         1  20.0\n         2   NaN\n         3  40.0\n         4  50.0\n\n\nif I use this command:\n\ndf_data.query(\'a isnull\', engine=\'python\')\n\n\nor this command:\n\ndf_data.query(\'a isnull()\', engine=\'python\')\n\n\nI get an error:\n\nIn [75]: df_data.query(\'a isnull\', engine=\'python\')  \nFile "&lt;unknown&gt;", line 1    a isnull           \nSyntaxError: invalid syntax\n\nIn [76]: df_data.query(\'a isnull()\', engine=\'python\')  \nFile "&lt;unknown&gt;", line 1    a isnull ()           \nSyntaxError: invalid syntax\n\n\nWhat is the right way to do that?\n\nThank you.\n'
"I have dataframe total_year, which contains three columns (year, action, comedy) .\ntotal_year\n\nI want to plot the year column on the x-axis, and action &amp; comedy both on the y-axis.\nHow can I plot two columns (action and comedy) on y-axis?\nMy code plots only one column on y-axis.\ntotal_year[-15:].plot(x='year', y='action', figsize=(10,5), grid=True)\n\n\n"
'I am trying to insert some data in a table I have created.\nI have a data frame that looks like this:\n\n\n\nI created a table:\n\ncreate table online.ds_attribution_probabilities\n(\nattribution_type text,\nchannel text,\ndate date ,\nvalue float\n)\n\n\nAnd I am running this python script:\n\nengine = create_engine("postgresql://@e.eu-central-1.redshift.amazonaws.com:5439/mdhclient_encoding=utf8")\nconnection = engine.raw_connection()\nresult.to_sql(\'online.ds_attribution_probabilities\', con=engine, index = False, if_exists = \'append\')\n\n\nI get no error, but when I check there are no data in my table. What can be wrong? Do I have to commit or do an extra step?\n'
"How to replace values in a Pandas series s via a dictionary d has been asked and re-asked many times.\n\nThe recommended method (1, 2, 3, 4) is to either use s.replace(d) or, occasionally, use s.map(d) if all your series values are found in the dictionary keys.\n\nHowever, performance using s.replace is often unreasonably slow, often 5-10x slower than a simple list comprehension.\n\nThe alternative, s.map(d) has good performance, but is only recommended when all keys are found in the dictionary.\n\nWhy is s.replace so slow and how can performance be improved?\n\nimport pandas as pd, numpy as np\n\ndf = pd.DataFrame({'A': np.random.randint(0, 1000, 1000000)})\nlst = df['A'].values.tolist()\n\n##### TEST 1 #####\n\nd = {i: i+1 for i in range(1000)}\n\n%timeit df['A'].replace(d)                          # 1.98s\n%timeit [d[i] for i in lst]                         # 134ms\n\n##### TEST 2 #####\n\nd = {i: i+1 for i in range(10)}\n\n%timeit df['A'].replace(d)                          # 20.1ms\n%timeit [d.get(i, i) for i in lst]                  # 243ms\n\n\nNote: This question is not marked as a duplicate because it is looking for specific advice on when to use different methods given different datasets. This is explicit in the answer and is an aspect not usually addressed in other questions.\n"
"I would like to slice two columns in my data frame.\n\nThis is my code for doing this:\n\nimport pandas as pd\ndf = pd.read_csv('source.txt',header=0)\ncidf=df.loc[:,['vocab','sumCI']]\nprint(cidf)\n\n\nThis is a sample of data:\n\nID  vocab   sumCI   sumnextCI   new_diff\n450      statu    3.0        0.0       3.0\n391     provid    4.0        1.0       3.0\n382  prescript    3.0        0.0       3.0\n300   lymphoma    2.0        0.0       2.0\n405      renew    2.0        0.0       2.0\n\n\n**Firstly I got this error: **\n\nKeyError: “None of [['', '']] are in the [columns]”'\n\n\nWhat I have tried:\n\n\nI tried putting a header with index 0 while reading the file,\nI tried to rename columns with this code:\n\ndf.rename(columns=df.iloc[0],inplace=True)\n\nI also tried this:\n\ndf.columns = df.iloc[1]\ndf=df.reindex(df.index.drop(0))\n\nAlso tried comments in this link\n\n\nNone of the above resolved the issue.\n"
"datum = soup.findAll('a', {'class': 'result-title'})\nfor data in datum:\n    print(data.text)\n    print(data.get('href'))\n    df = {'Title': data.text, 'Url': data.get('href')}\n    houseitems.append(df, ignore_index=True)\n\n\nWhat is wrong with my Code? Why when I asked for my houseitems, it gives me empty data.\n\nEmpty DataFrame\n\nColumns: [Title, Url, Price]\nIndex: []\n\n"
"I have the following code:\n\nx = pd.DataFrame(np.zeros((4, 1)), columns=['A'])\ny = np.random.randn(4, 2)\nx['A'] = y\n\n\nI expect it to throw an exception because of shape mismatch. But pandas silently accepted the assignment: y's first column is assigned to x.\n\nIs this an intentional design? If yes, what is the rationale behind?\n\nI tried both pandas 0.21 and 0.23.\n\n\n\nThanks for those who tried to help. However, nobody gives a satisfactory answer although the bounty is going to expire. \n\nLet me emphasis what is expected as an answer: \n\n\nwhether this design is intentional? Is it a bug ? Is it a false design?\nwhat is the rationale to design it in this way?\n\n\n\n\nSince the bounty is going to expiry, I accepted the most voted answer. But it does not provide a answer to the above questions.\n"
"I wanted to ask a questions regarding merging multiindex dataframe in pandas, here is a hypothetical scenario: \n\narrays = [['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'],\n            ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]\ntuples = list(zip(*arrays))\nindex1 = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])\nindex2 = pd.MultiIndex.from_tuples(tuples, names=['third', 'fourth'])\n\ns1 = pd.DataFrame(np.random.randn(8), index=index1, columns=['s1'])\ns2 = pd.DataFrame(np.random.randn(8), index=index2, columns=['s2'])\n\n\nThen either \n\ns1.merge(s2, how='left', left_index=True, right_index=True)\n\n\nor \n\ns1.merge(s2, how='left', left_on=['first', 'second'], right_on=['third', 'fourth'])\n\n\nwill result in error.\n\nDo I have to do reset_index() on either s1/s2 to make this work?\n\nThanks\n"
"I have a dataframe as follows:\n\nPLEASE_REMOVE  2013  2014  2015\n THIS_IS_EASY\n-------------------------------\n          Bob     0     3     4\n         Mary     2     3     6\n\n\nThe years (2013, 2014, 2015) are the column index labels.\nThe names (Mary, Bob) are the row index labels.\n\nI have somehow managed to get labels for the row indices and column indices.\n\nUsing df.index.names = [''] I can remove the THIS_IS_EASY bit.\n\nHow can I remove the PLEASE_REMOVE bit?\n\nDesired output is:\n\n               2013  2014  2015\n -------------------------------\n          Bob     0     3     4\n         Mary     2     3     6\n\n"
"It's possible with xlsxwriter to save variables to existing excel files and read them after, though the problem is that the variables are stored as strings in my excel file. \n\nLet's say I have a list of many different variables with various types (pd.datetimerange, pd.df, np.arrays, etc.), if I save them to the excel file the variable type would be lost.\n\nAlso the Python script is called from my Excel file so I can't change anything in it without writing a VBA script. Which would temporarily close my workbook, dump the chars (with say pickle strings) and reopen it.\n\nIs it possible to retrieve the types from the excel file without writing a parsing function first (which would take the excel string and yield me with the equivalent variable type)?\n"
'I\'m trying to create a SVM model from what I found in github here, but it keeps returning this error.\n\nTraceback (most recent call last):\n  File "C:\\Users\\Me\\Documents\\#e\\projects\\Sign-Language-Glove-master\\modeling.py", line 22, in &lt;module&gt;\n    train_features = train[[\'F1\',\'F2\',\'F3\',\'F4\',\'F5\',\'X\',\'Y\',\'Z\',\'C1\',\'C2\']]\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\frame.py", line 2934, in __getitem__\n    raise_missing=True)\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\indexing.py", line 1354, in _convert_to_indexer\n    return self._get_listlike_indexer(obj, axis, **kwargs)[1]\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\indexing.py", line 1161, in _get_listlike_indexer\n    raise_missing=raise_missing)\n  File "C:\\Python27\\lib\\site-packages\\pandas\\core\\indexing.py", line 1246, in _validate_read_indexer\n    key=key, axis=self.obj._get_axis_name(axis)))\nKeyError: u"None of [Index([u\'F1\', u\'F2\', u\'F3\', u\'F4\', u\'F5\', u\'X\', u\'Y\', u\'Z\', u\'C1\', u\'C2\'], dtype=\'object\')] are in the [columns]"\n\n\nThis is my code.\n\nimport pandas as pd\ndataframe= pd.read_csv("lettera.csv", delimiter=\',\')\ndf=pd.DataFrame(dataframe)\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.2)\n\ntrain_features = train[[\'F1\',\'F2\',\'F3\',\'F4\',\'F5\',\'X\',\'Y\',\'Z\',\'C1\',\'C2\']]\n\n\nAnd these are the contents of the csv file.\n\nLABEL, F1, F2, F3, F4, F5, X, Y, Z, C1, C2\n\n1, 631, 761, 739, 751, 743, 14120, -5320, 7404, 0, 0\n\n1, 632, 759, 740, 751, 744, 14108, -5276, 7444, 0, 0\n\n1, 630, 761, 740, 752, 743, 14228, -5104, 7680, 0, 0\n\n1, 630, 761, 738, 750, 743, 14256, -5148, 7672, 0, 0\n\n1, 632, 759, 740, 751, 744, 14172, -5256, 7376, 0, 0\n\n1, 632, 759, 742, 751, 746, 14288, -5512, 7412, 0, 0\n\n1, 632, 759, 742, 751, 744, 14188, -5200, 7416, 0, 0\n\n1, 634, 759, 738, 751, 743, 14252, -5096, 7524, 0, 0\n\n1, 630, 759, 739, 751, 743, 14364, -5124, 7612, 0, 0\n\n1, 630, 759, 740, 751, 744, 14192, -5316, 7424, 0, 0\n\n1, 631, 760, 739, 752, 743, 14292, -5100, 7404, 0, 0\n\n1, 634, 759, 738, 751, 742, 14232, -5188, 7468, 0, 0\n\n1, 632, 759, 740, 751, 744, 14288, -5416, 7552, 0, 0\n\n1, 630, 760, 739, 752, 743, 14344, -5072, 7816, 0, 0\n\n1, 631, 760, 739, 752, 743, 14320, -4992, 7444, 0, 0\n\n1, 630, 762, 739, 751, 746, 14220, -5172, 7544, 0, 0\n\n1, 630, 759, 739, 751, 742, 14280, -5176, 7416, 0, 0\n\n1, 630, 760, 738, 752, 740, 14360, -5028, 7468, 0, 0\n\n1, 632, 759, 738, 752, 741, 14384, -5108, 7364, 0, 0\n\n1, 629, 757, 737, 751, 741, 14224, -5108, 7536, 0, 0\n\n1, 629, 758, 740, 751, 744, 14412, -5136, 7956, 0, 0\n\n1, 629, 761, 740, 750, 744, 14468, -4868, 7100, 0, 0\n\n1, 629, 760, 738, 752, 741, 14504, -4964, 6600, 0, 0\n\n1, 629, 758, 738, 749, 741, 14440, -5112, 6828, 0, 0\n\n1, 629, 760, 738, 752, 741, 14484, -5016, 7556, 0, 0\n\n\nThank you.\n'
"I have following dataset\nItem Count\nA    60\nA    20\nA    21\nB    33\nB    33\nB    32\n\nCode to reproduce:\nimport pandas as pd\ndf = pd.DataFrame([\n    ['A', 60],\n    ['A', 20],\n    ['A', 21],\n    ['B', 33],\n    ['B', 33],\n    ['B', 32],\n], \n    columns=['Item', 'Count'])\n\nSuppose I have to Change only the maximum value of each group of &quot;Item&quot; column by adding 1.\nthe output should be like this:\nItem Count New_Count\nA    60    61\nA    20    20\nA    21    21\nB    33    34\nB    33    34\nB    32    32\n\nI tried df['New_Count']=df.groupby(['Item'])['Count'].transform(lambda x: max(x)+1) but all the values in &quot;Count&quot; was replaced by max value of each group +1.\nItem Count New_Count\nA    60    61\nA    20    61\nA    21    61\nB    33    34\nB    33    34\nB    32    34\n\n"
"I have an intra day series of log returns over multiple days that I would like to downsample to daily ohlc. I can do something like\n\nhi = series.resample('B', how=lambda x: np.max(np.cumsum()))\nlow = series.resample('B', how=lambda x: np.min(np.cumsum())) \n\n\nBut it seems inefficient to compute cumsum on each call. Is there a way to first compute the cumsums and then apply 'ohcl' to the data?\n\n1999-08-09 12:30:00-04:00   -0.000486\n1999-08-09 12:31:00-04:00   -0.000606\n1999-08-09 12:32:00-04:00   -0.000120\n1999-08-09 12:33:00-04:00   -0.000037\n1999-08-09 12:34:00-04:00   -0.000337\n1999-08-09 12:35:00-04:00    0.000100\n1999-08-09 12:36:00-04:00    0.000219\n1999-08-09 12:37:00-04:00    0.000285\n1999-08-09 12:38:00-04:00   -0.000981\n1999-08-09 12:39:00-04:00   -0.000487\n1999-08-09 12:40:00-04:00    0.000476\n1999-08-09 12:41:00-04:00    0.000362\n1999-08-09 12:42:00-04:00   -0.000038\n1999-08-09 12:43:00-04:00   -0.000310\n1999-08-09 12:44:00-04:00   -0.000337\n...\n1999-09-28 06:45:00-04:00    0.000000\n1999-09-28 06:46:00-04:00    0.000000\n1999-09-28 06:47:00-04:00    0.000000\n1999-09-28 06:48:00-04:00    0.000102\n1999-09-28 06:49:00-04:00   -0.000068\n1999-09-28 06:50:00-04:00    0.000136\n1999-09-28 06:51:00-04:00    0.000566\n1999-09-28 06:52:00-04:00    0.000469\n1999-09-28 06:53:00-04:00    0.000000\n1999-09-28 06:54:00-04:00    0.000000\n1999-09-28 06:55:00-04:00    0.000000\n1999-09-28 06:56:00-04:00    0.000000\n1999-09-28 06:57:00-04:00    0.000000\n1999-09-28 06:58:00-04:00    0.000000\n1999-09-28 06:59:00-04:00    0.000000\n\n"
'In a pandas DataFrame, is it possible to collapse columns which have identical values, and sum up the values in another column?\n\nCode\n\ndata = {"score":{"0":9.397,"1":9.397,"2":9.397995,"3":9.397996,"4":9.3999},"type":{"0":"advanced","1":"advanced","2":"advanced","3":"newbie","4":"expert"},"count":{"0":394.18930604,"1":143.14226729,"2":9.64172783,"3":0.1,"4":19.65413734}}\ndf = pd.DataFrame(data)\ndf\n\n\nOutput\n\n     count       score       type\n0    394.189306  9.397000    advanced\n1    143.142267  9.397000    advanced\n2    9.641728    9.397995    advanced\n3    0.100000    9.397996    newbie\n4    19.654137   9.399900    expert\n\n\nIn the example above, the first two rows have the same score and type , so these rows should be merged together and their scores added up.\n\nDesired Output\n\n     count       score       type\n0    537.331573  9.397000    advanced\n1    9.641728    9.397995    advanced\n2    0.100000    9.397996    newbie\n3    19.654137   9.399900    expert\n\n'
'I took a new clean install of OSX 10.9.3 and installed pip, and then did\n\n\npip install pandas\npip install numpy\n\n\nBoth installs seemed to be perfectly happy, and ran without any errors (though there were a zillion warnings).  When I tried to run a python script with import pandas, I got the following error:\n\n\n\n    numpy.dtype has the wrong size, try recompiling Traceback (most recent call last): \n    File "./moen.py", line 7, in  import pandas File "/Library/Python/2.7/site-packages/pandas/__init__.py", line 6, in  from . import hashtable, tslib, lib \n    File "numpy.pxd", line 157, in init pandas.hashtable (pandas/hashtable.c:22331) \n    ValueError: numpy.dtype has the wrong size, try recompiling\n\n\n\nHow do I fix this error and get pandas to load properly?\n'
"I have a dataframe of transactions. Each row represents a transaction of two item (think of it like a transaction of 2 event tickets or something). I want to duplicate each row based on the quantity sold. \n\nHere's example code:\n\n# dictionary of transactions\n\nd = {\n    '1': ['20',  'NYC', '2'],\n    '2': ['30',  'NYC', '2'],\n    '3': ['5',   'NYC', '2'],\n    '4': ['300', 'LA',  '2'],\n    '5': ['30',  'LA',  '2'],\n    '6': ['100', 'LA',  '2']\n}\n\ncolumns=['Price', 'City', 'Quantity']\n\n# create dataframe and rename columns\n\ndf = pd.DataFrame.from_dict(\n    data=d, orient='index'\n)\ndf.columns = columns\n\n\nThis produces a dataframe that looks like this\n\nPrice   City    Quantity\n20       NYC         2\n30       NYC         2\n5        NYC         2\n300      LA          2\n30       LA          2\n100      LA          2\n\n\nSo in the case above, each row will transform into two duplicate rows. If the 'quantity' column was 3, then that row would transform into three duplicate rows.\n"
'I have two numpy arrays light_points and time_points and would like to use some time series analysis methods on those data.\n\nI then tried this :\n\nimport statsmodels.api as sm\nimport pandas as pd\ntdf = pd.DataFrame({\'time\':time_points[:]})\nrdf =  pd.DataFrame({\'light\':light_points[:]})\nrdf.index = pd.DatetimeIndex(freq=\'w\',start=0,periods=len(rdf.light))\n#rdf.index = pd.DatetimeIndex(tdf[\'time\'])\n\n\nThis works but is not doing the correct thing.\nIndeed, the measurements are not evenly time-spaced and if I just declare the time_points pandas DataFrame as the index of my frame, I get an error :\n\nrdf.index = pd.DatetimeIndex(tdf[\'time\'])\n\ndecomp = sm.tsa.seasonal_decompose(rdf)\n\nelif freq is None:\nraise ValueError("You must specify a freq or x must be a pandas object with a timeseries index")\n\nValueError: You must specify a freq or x must be a pandas object with a timeseries index\n\n\nI don\'t know how to correct this.\nAlso, it seems that pandas\' TimeSeries are deprecated.\n\nI tried this :\n\nrdf = pd.Series({\'light\':light_points[:]})\nrdf.index = pd.DatetimeIndex(tdf[\'time\'])\n\n\nBut it gives me a length mismatch :\n\nValueError: Length mismatch: Expected axis has 1 elements, new values have 122 elements\n\n\nNevertheless, I don\'t understand where it comes from, as rdf[\'light\'] and \ntdf[\'time\'] are of same length...\n\nEventually, I tried by defining my rdf as a pandas Series :\n\nrdf = pd.Series(light_points[:],index=pd.DatetimeIndex(time_points[:]))\n\n\nAnd I get this :\n\nValueError: You must specify a freq or x must be a pandas object with a timeseries index\n\n\nThen, I tried instead replacing the index by \n\n pd.TimeSeries(time_points[:])\n\n\nAnd it gives me an error on the seasonal_decompose method line :\n\nAttributeError: \'Float64Index\' object has no attribute \'inferred_freq\'\n\n\nHow can I work with unevenly spaced data ?\nI was thinking about creating an approximately evenly spaced time array by adding many unknown values between the existing values and using interpolation to "evaluate" those points, but I think there could be a cleaner and easier solution.\n'
"I have a dataframe which looks like this:\n\n    a1    b1    c1    a2    b2    c2    a3    ...\nx   1.2   1.3   1.2   ...   ...   ...   ...\ny   1.4   1.2   ...   ...   ...   ...   ...\nz   ...\n\n\nWhat I want is grouping by every nth column. In other words, I want a dataframe with all the as, one with bs and one with cs\n\n    a1     a2     a4\nx   1.2    ...    ...\ny\nz\n\n\nIn another SO question I saw that is possibile to do df.iloc[::5,:], for example, to get every 5th raw. I could do of course df.iloc[:,::3] to get the c cols but it doesn't work for getting a and b.\n\nAny ideas?\n"
'I have the following pandas DataFrame.\n\nimport pandas as pd\ndf = pd.read_csv(\'filename.csv\')\n\nprint(df)\n\n     dog      A         B           C\n0     dog1    0.787575  0.159330    0.053095\n1     dog10   0.770698  0.169487    0.059815\n2     dog11   0.792689  0.152043    0.055268\n3     dog12   0.785066  0.160361    0.054573\n4     dog13   0.795455  0.150464    0.054081\n5     dog14   0.794873  0.150700    0.054426\n..    ....\n8     dog19   0.811585  0.140207    0.048208\n9     dog2    0.797202  0.152033    0.050765\n10    dog20   0.801607  0.145137    0.053256\n11    dog21   0.792689  0.152043    0.055268\n    ....\n\n\nI create a new column by summing columns "A", "B", "C" as follows:\n\ndf[\'total_ABC\'] = df[["A", "B", "B"]].sum(axis=1)\n\n\nNow I would like to do this based on a conditional, i.e. if "A" &lt; 0.78 then create a new summed column df[\'smallA_sum\'] = df[["A", "B", "B"]].sum(axis=1). Otherwise, the value should be zero. \n\nHow does one create conditional statements like this? \n\nMy thought would be to use \n\ndf[\'smallA_sum\'] = df1.apply(lambda row: (row[\'A\']+row[\'B\']+row[\'C\']) if row[\'A\'] &lt; 0.78))\n\n\nHowever, this doesn\'t work and I\'m not able to specify axis. \n\nHow do you create a column based on the values of other columns? \n\nYou could also do something like for each df[\'dog\'] == \'dog2\', create column dog2_sum, i.e. \n\n df[\'dog2_sum\'] = df1.apply(lambda row: (row[\'A\']+row[\'B\']+row[\'C\']) if df[\'dog\'] == \'dog2\'))\n\n\nbut my approach is incorrect. \n\n`\n'
"New to unittest package. \nI'm trying to verify the DataFrame returned by a function through the following code. Even though I hardcoded the inputs of assert_frame_equal to be equal (pd.DataFrame([0,0,0,0])), the unittest still fails. Anyone would like to explain why it happens?\n\nimport unittest\nfrom pandas.util.testing import assert_frame_equal\nclass TestSplitWeight(unittest.TestCase):\n    def test_allZero(self):\n        #splitWeight(pd.DataFrame([0,0,0,0]),10)\n        self.assert_frame_equal(pd.DataFrame([0,0,0,0]),pd.DataFrame([0,0,0,0]))\n\nsuite = unittest.TestLoader().loadTestsFromTestCase(TestSplitWeight)\nunittest.TextTestRunner(verbosity=2).run(suite)\n\n\nError: AttributeError: 'TestSplitWeight' object has no attribute 'assert_frame_equal'\n"
"Now there are a lot of similar questions but most of them answer how to delete the duplicate columns. However, I want to know how can I make a list of tuples where each tuple contains the column names of duplicate columns. I am assuming that each column has a unique name. Just to further illustrate my question:\n\ndf = pd.DataFrame({'A': [1, 2, 3, 4, 5],'B': [2, 4, 2, 1, 9],\n                   'C': [1, 2, 3, 4, 5],'D': [2, 4, 2, 1, 9],\n                   'E': [3, 4, 2, 1, 2],'F': [1, 1, 1, 1, 1]},\n                   index = ['a1', 'a2', 'a3', 'a4', 'a5'])\n\n\nthen I want the output:\n\n[('A', 'C'), ('B', 'D')]\n\n\nAnd if you are feeling great today then also extend the same question to rows. How to get a list of tuples where each tuple contains duplicate rows.\n"
'I worked now for quite some time using python and pandas for analysing a set of hourly data and find it quite nice (Coming from Matlab.)\n\nNow I am kind of stuck. I created my DataFrame like that:\n\nSamplingRateMinutes=60\nindex = DateRange(initialTime,finalTime, offset=datetools.Minute(SamplingRateMinutes))\nts=DataFrame(data, index=index)\n\n\nWhat I want to do now is to select the Data for all days at the hours 10 to 13 and 20-23 to use the data for further calculations.\nSo far I sliced the data using\n\n selectedData=ts[begin:end]\n\n\nAnd I am sure to get some kind of dirty looping to select the data needed. But there must be a more elegant way to index exacly what I want. I am sure this is a common problem and the solution in pseudocode should look somewhat like that:\n\nmyIndex=ts.index[10&lt;=ts.index.hour&lt;=13 or 20&lt;=ts.index.hour&lt;=23]\nselectedData=ts[myIndex]\n\n\nTo mention I am an engineer and no programer :) ... yet\n'
"I can't figure out how to write/read a Series correctly...The following (and many variations of it) results in the read series being different than the written series...note that the series is read into a DataFrame rather than a series.\n\nIn [55]: s = pd.Series({'a': 1, 'b': 2})\n\nIn [56]: s\nOut[56]: \na    1\nb    2\n\nIn [57]: s.to_csv('/tmp/s.csv')\n\nIn [58]: !cat /tmp/s.csv\na,1\nb,2\n\nIn [59]: pd.read_csv('/tmp/s.csv')\nOut[59]: \n   a  1\n0  b  2\n\n"
'If I have the following Dataframe\n\n&gt;&gt;&gt; df = pd.DataFrame({\'Name\': [\'Bob\'] * 3 + [\'Alice\'] * 3, \\\n\'Destination\': [\'Athens\', \'Rome\'] * 3, \'Length\': np.random.randint(1, 6, 6)}) \n&gt;&gt;&gt; df    \n  Destination  Length   Name\n0      Athens       3    Bob\n1        Rome       5    Bob\n2      Athens       2    Bob\n3        Rome       1  Alice\n4      Athens       3  Alice\n5        Rome       5  Alice\n\n\nI can goup by name and destination...\n\n&gt;&gt;&gt; grouped = df.groupby([\'Name\', \'Destination\'])\n&gt;&gt;&gt; for nm, gp in grouped:\n&gt;&gt;&gt;     print nm\n&gt;&gt;&gt;     print gp\n(\'Alice\', \'Athens\')\n  Destination  Length   Name\n4      Athens       3  Alice\n(\'Alice\', \'Rome\')\n  Destination  Length   Name\n3        Rome       1  Alice\n5        Rome       5  Alice\n(\'Bob\', \'Athens\')\n  Destination  Length Name\n0      Athens       3  Bob\n2      Athens       2  Bob\n(\'Bob\', \'Rome\')\n  Destination  Length Name\n1        Rome       5  Bob\n\n\nbut I would like a new multi-indexed dataframe out of it that looks something like\n\n                Length\nAlice   Athens       3\n        Rome         1\n        Rome         5\nBob     Athens       3\n        Athens       2\n        Rome         5\n\n\nIt seems there should be a way to do something like Dataframe(grouped) to get my multi-indexed Dataframe, but instead I get a PandasError ("DataFrame constructor not properly called!").\n\nWhat is the easiest way to get this? Also, anyone know if there will ever be an option to pass a groupby object to the constructor, or if I\'m just doing it wrong?\n\nThanks\n'
'I am trying to go through every row in a DataFrame index and remove all rows that are not between a certain time.\n\nI have been looking for solutions but none of them separate the Date from the Time, and all I want to do is drop the rows that are outside of a Time range.\n'
"I've read about the to_latex method, but it's not clear how to use the formatters argument.\n\nI have some numbers which are too long and some which I want thousand separators.\n\nA side issue for the to_latex method on multi-indexed tables, the indices are parsed together and it issues some &amp;s in the latex output.\n"
"I am trying to create a term document matrix with NLTK and pandas. \nI wrote the following function:\n\ndef fnDTM_Corpus(xCorpus):\n    import pandas as pd\n    '''to create a Term Document Matrix from a NLTK Corpus'''\n    fd_list = []\n    for x in range(0, len(xCorpus.fileids())):\n        fd_list.append(nltk.FreqDist(xCorpus.words(xCorpus.fileids()[x])))\n    DTM = pd.DataFrame(fd_list, index = xCorpus.fileids())\n    DTM.fillna(0,inplace = True)\n    return DTM.T\n\n\nto run it \n\nimport nltk\nfrom nltk.corpus import PlaintextCorpusReader\ncorpus_root = 'C:/Data/'\n\nnewcorpus = PlaintextCorpusReader(corpus_root, '.*')\n\nx = fnDTM_Corpus(newcorpus)\n\n\nIt works well for few small files in the corpus but gives me a MemoryError  when I try to run it with a corpus of 4,000 files (of about 2 kb each). \n\nAm I missing something? \n\nI am using a 32 bit python. \n(am on windows 7, 64-bit OS, Core Quad CPU, 8 GB RAM). Do I really need to use 64 bit for corpus of this size ? \n"
"I have series of measurements which are time stamped and irregularly spaced. Values in these series always represent changes of the measurement -- i.e. without a change no new value. A simple example of such a series would be:\n23:00:00.100     10\n23:00:01.200      8\n23:00:01.600      0\n23:00:06.300      4\n\nWhat I want to reach is an equally spaced series of time-weighted averages. For the given example I might aim at a frequency based on seconds and hence a result like the following:\n23:00:01     NaN ( the first 100ms are missing )\n23:00:02     5.2 ( 10*0.2 + 8*0.4 + 0*0.4 )\n23:00:03       0\n23:00:04       0\n23:00:05       0\n23:00:06     2.8 ( 0*0.3 + 4*0.7 )\n\nI am searching for a Python library solving that problem. For me, this seems to be a standard problem, but I couldn't find such a functionality so far in standard libraries like pandas.\nThe algorithm needs to take two things into account:\n\ntime-weighted averaging\nconsidering values ahead of the current interval ( and possibly even ahead of the lead ) when forming the average\n\nUsing pandas\ndata.resample('S', fill_method='pad')          # forming a series of seconds\n\ndoes parts of the work. Providing a user-defined function for aggregation will allow to form time-weighted averages, but because the beginning of the interval is ignored, this average will be incorrect too. Even worse: the holes in the series are filled with the average values, leading in the example from above to the values of seconds 3, 4 and 5 to be non zero.\ndata = data.resample('L', fill_method='pad')   # forming a series of milliseconds\ndata.resample('S')\n\ndoes the trick with a certain accurateness, but is -- depending on the accurateness -- very expensive. In my case, too expensive.\nEdit: Solution\nimport pandas as pa\nimport numpy as np\nfrom datetime import datetime\nfrom datetime import timedelta\n\ntime_stamps=[datetime(2013,04,11,23,00,00,100000), \n             datetime(2013,04,11,23,00,1,200000),\n             datetime(2013,04,11,23,00,1,600000),\n             datetime(2013,04,11,23,00,6,300000)]\nvalues = [10, 8, 0, 4]\nraw = pa.TimeSeries(index=time_stamps, data=values)\n\ndef round_down_to_second(dt):\n    return datetime(year=dt.year, month=dt.month, day=dt.day, \n                    hour=dt.hour, minute=dt.minute, second=dt.second)\n\ndef round_up_to_second(dt):\n    return round_down_to_second(dt) + timedelta(seconds=1)\n\ndef time_weighted_average(data):\n    end = pa.DatetimeIndex([round_up_to_second(data.index[-1])])\n    return np.average(data, weights=np.diff(data.index.append(end).asi8))\n\nstart = round_down_to_second(time_stamps[0])\nend = round_down_to_second(time_stamps[-1])\nrange = pa.date_range(start, end, freq='S')\ndata = raw.reindex(raw.index + range)\ndata = data.ffill()\n\ndata = data.resample('S', how=time_weighted_average)\n\n"
"The following piece of code:\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame({'date': ('13/02/2012', '14/02/2012')})\ndata['date'] = data['date'].astype('datetime64')\n\n\nworks fine on one machine (windows) and doesn't work on another (linux). Both numpy and pandas are installed on both. \n\nThe error I get is:\n\nValueError: Cannot create a NumPy datetime other than NaT with generic units\n\n\nWhat does this error mean? I see it for the first time ever and there is not much on the web I can find. Is it some missing dependency? \n"
"How is it possible to retrieve the labe of a particular value in a pandas Series object:\n\nFor example:\n\nlabels = ['a', 'b', 'c', 'd', 'e']\ns = Series (arange(5) * 4 , labels)\n\n\nWhich produces the Series:\n\na     0\nb     4\nc     8\nd    12\ne    16\ndtype: int64\n\n\nHow is it possible to get the label of value '12'? Thanks\n"
'In the following snippet data is a pandas.DataFrame and indices is a set of columns of the data. After grouping the data with groupby I am interested in the ids of the groups, but only those with a size greater than a threshold (say: 3).\n\ngroup_ids=data.groupby(list(data.columns[list(indices)])).grouper.group_info[0]\n\n\nNow, how can I find which group has a size greater than or equal 3 knowing the id of the group? I only want ids of groups with a certain size.\n\n#TODO: filter out ids from group_ids which correspond to groups with sizes &lt; 3 \n\n'
'I have a DataFrame loaded from a .tsv file. I wanted to generate some exploratory plots. The problem is that the data set is large (~1 million rows), so there are too many points on the plot to see a trend. Plus, it is taking a while to plot.\n\nI wanted to sub-sample 10000 randomly distributed rows. This should be reproducible so the same sequence of random numbers is generated in each run.\n\nThis: Sample two pandas dataframes the same way seems to be on the right track, but I cannot guarantee the subsample size. \n'
'I would like to use pandas to plot a barplot with diffrent colors for category in column.\n\nHere is a simple example: (index is variable)\n\ndf:\n         value   group\nvariable               \na             10      1\nb              9      1\nc              8      1\nd              7      2\nf              6      2\ng              5      3\nh              4      3\n\n\nI would like to make a barplot with coloring on group. I would also like to specify the colors. In my original dataset I have many goups.\nCould someone help me with this?\n'
'My Pandas Dataframe frame looks something like this\n\n 1. 2013-10-09 09:00:05\n 2. 2013-10-09 09:05:00\n 3. 2013-10-09 10:00:00\n 4.  ............\n 5.   ............\n 6.   ............\n 7. 2013-10-10 09:00:05\n 8. 2013-10-10 09:05:00 \n 9. 2013-10-10 10:00:00\n\n\nI want the data lying in between hours 9 and 10 ...if anyone has worked on something like this ,it would be really helpful.\n'
"Using python 2.7.5 and pandas 0.12.0, I'm trying to import fixed-width-font text files into a DataFrame with 'pd.io.parsers.read_fwf()'.  The values I'm importing are all numeric, but it's important that leading zeros be preserved, so I'd like to specify the dtype as string rather than int.\n\nAccording to the documentation for this function, the dtype attribute is supported in read_fwf, but when I try to use it:\n\ndata= pd.io.parsers.read_fwf(file, colspecs = ([79,81], [87,90]), header = None, dtype = {0: np.str, 1: np.str})\n\nI get the error:\n\nValueError: dtype is not supported with python-fwf parser\n\nI've tried as many variations as I can think of for setting 'dtype = something', but all of them return the same message.  \n\nAny help would be much appreciated!    \n"
"Is it possible to store arbitrary numpy arrays as the values of a single column in a dataframe of Pandas?\n\nThe arrays are all 2-dimensional, and I intend to use them to calculate values for other columns in the same dataframe.\n\nTo provide some context of what I'm trying to do here:\n\nEach array is an adjacency matrix of some network, and for each network I want to calculate its various characteristics (e.g. density, centralities, clustering coefficient, etc) which are in fact other columns in the same dataframe.\n"
"A lot of times, I have a big dataframe df to hold the basic data, and need to create many more columns to hold the derivative data calculated by basic data columns.\n\nI can do that in Pandas like:\n\ndf['derivative_col1'] = df['basic_col1'] + df['basic_col2']\ndf['derivative_col2'] = df['basic_col1'] * df['basic_col2']\n....\ndf['derivative_coln'] = func(list_of_basic_cols)\n\n\netc. Pandas will calculate and allocate the memory for all derivative columns all at once.\n\nWhat I want now is to have a lazy evaluation mechanism to postpone the calculation and memory allocation of derivative columns to the actual need moment. Somewhat define the lazy_eval_columns as:\n\ndf['derivative_col1'] = pandas.lazy_eval(df['basic_col1'] + df['basic_col2'])\ndf['derivative_col2'] = pandas.lazy_eval(df['basic_col1'] * df['basic_col2'])\n\n\nThat will save the time/memory like Python 'yield' generator, for if I issue df['derivative_col2'] command will only triger the specific calculation and memory allocation.\n\nSo how to do lazy_eval() in Pandas ? Any tip/thought/ref are welcome.\n"
"I'm looking for a way to replicate the encode behaviour in Stata, which will convert a categorical string column into a number column.\n\nx = pd.DataFrame({'cat':['A','A','B'], 'val':[10,20,30]})\nx = x.set_index('cat')\n\n\nWhich results in:\n\n     val\ncat     \nA     10\nA     20\nB     30\n\n\nI'd like to convert the cat column from strings to integers, mapping each unique string to an (arbitrary) integer 1-to-1. It would result in:\n\n     val\ncat     \n1     10\n1     20\n2     30\n\n\nOr, just as good:\n\n  cat  val\n0   1   10\n1   1   20\n2   2   30\n\n\nAny suggestions?\n\nMany thanks as always,\nRob\n"
"I want to create a pandas dataframe with default values of zero, but one column of integers and the other of floats.  I am able to create a numpy array with the correct types, see the values variable below.  However, when I pass that into the dataframe constructor, it only returns NaN values (see df below).  I have include the untyped code that returns an array of floats(see df2)\n\nimport pandas as pd\nimport numpy as np\n\nvalues = np.zeros((2,3), dtype='int32,float32')\nindex = ['x', 'y']\ncolumns = ['a','b','c']\n\ndf = pd.DataFrame(data=values, index=index, columns=columns)\ndf.values.dtype\n\nvalues2 = np.zeros((2,3))\ndf2 = pd.DataFrame(data=values2, index=index, columns=columns)\ndf2.values.dtype\n\n\nAny suggestions on how to construct the dataframe?\n"
"I would like to have the 3 columns of a numpy array\n\npx[:,:,0]\npx[:,:,1]\npx[:,:,0]\n\n\ninto a pandas Dataframe.\n\nShould I use?\n\ndf = pd.DataFrame(px, columns=['R', 'G', 'B'])\n\n\nThank you\n\nHugo\n"
'I am going to convert Python pandas dataframe to dataframe in R.\nI found out few libraries for this problem\n\nhttp://pandas.pydata.org/pandas-docs/stable/r_interface.html\n\nwhich is rpy2\n\nBut I couldn\'t find the methods for saving or transfer it to R.\n\nFirstly I tried "to_csv"\n\ndf_R = com.convert_to_r_dataframe(df_total)\ndf_R.to_csv(direc+"/qap/detail_summary_R/"+"distance_"+str(gp_num)+".csv",sep = ",")\n\n\nBut it gives me an error\n\n"AttributeError: \'DataFrame\' object has no attribute \'to_csv\'  "\n\n\nSo I tried to see its data type\nit was \n\n&lt;class \'rpy2.robjects.vectors.DataFrame\'&gt;\n\n\nhow could I save this type object to csv file or transfer to R?\n'
'I\'m trying to preform recursive feature elimination using scikit-learn and a random forest classifier, with OOB ROC as the method of scoring each subset created during the recursive process.\n\nHowever, when I try to use the RFECV method, I get an error saying AttributeError: \'RandomForestClassifier\' object has no attribute \'coef_\'  \n\nRandom Forests don\'t have coefficients per se, but they do have rankings by Gini score.  So, I\'m wondering how to get arround this problem.\n\nPlease note that I want to use a method that will explicitly tell me what features from my pandas DataFrame were selected in the optimal grouping as I am using recursive feature selection to try to minimize the amount of data I will input into the final classifier. \n\nHere\'s some example code: \n\nfrom sklearn import datasets\nimport pandas as pd\nfrom pandas import Series\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFECV\n\niris = datasets.load_iris()\nx=pd.DataFrame(iris.data, columns=[\'var1\',\'var2\',\'var3\', \'var4\'])\ny=pd.Series(iris.target, name=\'target\')\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=5, n_jobs=-1)\nrfecv = RFECV(estimator=rf, step=1, cv=10, scoring=\'ROC\', verbose=2)\nselector=rfecv.fit(x, y)\n\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py", line 336, in fit\n    ranking_ = rfe.fit(X_train, y_train).ranking_\n  File "/Users/bbalin/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.py", line 148, in fit\n    if estimator.coef_.ndim &gt; 1:\nAttributeError: \'RandomForestClassifier\' object has no attribute \'coef_\'\n\n'
"I was wondering how I can remove all indexes that containing negative values inside their column. I am using Pandas DataFrames. \n\nDocumentation Pandas DataFrame\n\nFormat:\n\nMyid - valuecol1 - valuecol2 - valuecol3 -... valuecol30\n\nSo my DataFrame is called data\n\nI know how to do this for 1 column:\n\ndata2 = data.index[data['valuecol1'] &gt; 0]\ndata3 = data.ix[data3]\n\n\nSo I only get the ids where valuecol1 &gt; 0, how can I do some kind of and statement?\n\nvaluecol1 &amp;&amp; valuecol2 &amp;&amp; valuecol3 &amp;&amp; ... &amp;&amp; valuecol30 &gt; 0 ? \n"
'It seems that for OLS linear regression to work well in Pandas, the arguments must be floats. I\'m starting with a csv (called "gameAct.csv") of the form:\n\ndate, city, players, sales\n\n2014-04-28,London,111,1091.28\n\n2014-04-29,London,100,1100.44\n\n2014-04-28,Paris,87,1001.33\n\n...\n\n\nI want to perform linear regression of how sales depend on date (as time moves forward, how do sales move?). The problem with my code below seems to be with dates not being float values. I would appreciate help on how to resolve this indexing problem in Pandas.\n\nMy current (non-working, but compiling code):\n\nimport pandas as pd\n\nfrom pandas import DataFrame, Series\n\nimport statsmodels.formula.api as sm\n\ndf = pd.read_csv(\'gameAct.csv\')\n\ndf.columns = [\'date\', \'city\', \'players\', \'sales\']\n\ncity_data = df[df[\'city\'] == \'London\']\n\nresult = sm.ols(formula = \'sales ~ date\', data = city_data).fit()\n\n\nAs I vary the city value, I get R^2 = 1 results, which is wrong. I have also attempted index_col = 0, parse_dates == True\' in defining the dataframe df, but without success.\n\nI suspect there is a better way to read in such csv files to perform basic regression over dates, and also for more general time series analysis. Help, examples, and resources are appreciated!\n\nNote, with the above code, if I convert the dates index (for a given city) to an array, the values in this array are of the form:\n\n\'\\xef\\xbb\\xbf2014-04-28\'\n\n\nHow does one produce an AIC analysis over all of the non-sales parameters? (e.g. the result might be that sales depend most linearly on date and city).\n'
"I am learning Pandas package by replicating the outing from some of the R vignettes. Now I am using the dplyr package from R as an example:\n\nhttp://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\n\nR script\n\nplanes &lt;- group_by(hflights_df, TailNum)\ndelay &lt;- summarise(planes,\n  count = n(),\n  dist = mean(Distance, na.rm = TRUE))\ndelay &lt;- filter(delay, count &gt; 20, dist &lt; 2000)\n\n\nPython script\n\nplanes = hflights.groupby('TailNum')\nplanes['Distance'].agg({'count' : 'count',\n                        'dist' : 'mean'})\n\n\nHow can I state explicitly in python that NA needs to be skipped?\n"
"I am trying to format the output in an IPython notebook. I tried using the to_string function, and this neatly lets me eliminate the index column. But the textual data is right justified.\n\nIn [10]:\n\nimport pandas as pd\ncolumns = ['Text', 'Value']\na = pd.DataFrame ({'Text': ['abcdef', 'x'], 'Value': [12.34, 4.2]})\nprint (a.to_string (index=False))\n\n   Text  Value\n abcdef  12.34\n      x   4.20\n\n\nThe same is true when just printing the dataframe.\n\nIn [12]:\n\nprint (a)\n\n     Text  Value\n0  abcdef  12.34\n1       x   4.20\n\n\nThe justify argument in the to_string function, surprisingly, only justifies the column heading.\n\nIn [13]:\n\nimport pandas as pd\ncolumns = ['Text', 'Value']\na = pd.DataFrame ({'Text': ['abcdef', 'x'], 'Value': [12.34, 4.2]})\nprint (a.to_string (justify='left', index=False))\nText     Value\n abcdef  12.34\n      x   4.20\n\n\nHow can I control the justification settings for individual columns?\n"
'When importing pandas I would get the following error: \n\nNumpy.dtype has the wrong size, try recompiling\n\nI am running Python 2.7.5, with Pandas 0.14.1, and Numpy 1.9.0. I have tried installing older versions of both using pip, with major errors every time. I am a beginner when it comes to Python so any help here would be much appreciated. :)\n\nEDIT: running OS X 10.9.4\n\nEDIT 2: here is a link to a video of me uninstalling and reinstalling Numpy + Pandas, and then running a .py file: https://www.dropbox.com/s/sx9l288jijokrar/numpy%20issue.mov?dl=0\n'
"I have a dataframe with rows indexed by chemical element type and columns representing different samples. The values are floats representing the degree of presence of the row element in each sample.\n\nI want to compute the mean of each row and subtract it from each value in that specific row to normalize the data, and make a new dataframe of that dataset.\n\nI tried using mean(1), which give me a Series object with the mean for each chemical element, which is good, but then I tried using subtract, which didn't work.\n"
'So I have data that I am outputting to an excel file using pandas\' ExcelWriter. After the entire data is outputted to the Excel file, what is the easiest way to apply conditional formatting to it programmatically using Python? \n\nI want to be able to do the equivalent (through Python) of selecting (in Excel) all the filled cells in the Excel sheet and clicking on "Conditional Formatting" > Color Scales. The end result is a gradient of colors based on the values, a "heat map" if you will. \n\nThis is what I am doing to generate the data:\n\nwriter = ExcelWriter(\'Data\' + today +\'.xls\')\n... processing data ... \ndf.to_excel(writer, sheet_name = \'Models\', startrow = start_row, index=False)\n\n\nAfter the data is written, I need a way to apply the conditional formatting using python. To make it simple, I want the colors to be darker shades of blue the more positive (>0) the values are and to be darker shades of red the more negative the values are (&lt;0) and the cell to be white if the value is 0. \n\nI tried looking into xlsxwriter (in hopes of being able to modify the excel file after creating it) but in the documentation it says that "It [XLSXwriter] cannot read or modify existing Excel XLSX files." \n\nPlease let me know if you can think of a solution or point me in the right direction. \n'
'I have a CSV file  (tmp.csv) that looks like this:\n\n        x       y       z\nbar     0.55    0.55    0.0\nfoo     0.3     0.4     0.1\nqux     0.0     0.3     5.55\n\n\nIt was created with Pandas this way:\n\n    In [103]: df_dummy \n    Out[103]: \n          x     y     z\n    bar  0.55  0.55  0.00\n    foo  0.30  0.40  0.10\n    qux  0.00  0.30  5.55\n\n   In [104]: df_dummy.to_csv("tmp.csv",sep="\\t")   \n\n\nWhat I want to do is to read that CSV into the same dataframe representation.\nI tried this but doesn\'t give what I want:\n\nIn [108]: pd.io.parsers.read_csv("tmp.csv",sep="\\t")\nOut[108]: \n  Unnamed: 0     x     y     z\n0        bar  0.55  0.55  0.00\n1        foo  0.30  0.40  0.10\n2        qux  0.00  0.30  5.55\n\n\nWhat\'s the right way to do it?\n'
"I'm trying to delete rows of a dataframe based on one date column; [Delivery Date]\n\nI need to delete rows which are older than 6 months old but not equal to the year '1970'.\n\nI've created 2 variables:\n\nfrom datetime import date, timedelta\nsixmonthago = date.today() - timedelta(188)\n\nimport time\nnineteen_seventy = time.strptime('01-01-70', '%d-%m-%y')\n\n\nbut I don't know how to delete rows based on these two variables, using the [Delivery Date] column.\n\nCould anyone provide the correct solution?\n"
"I have the following dataset sample:\n\n     0         1\n0    0  0.040158\n1    2  0.500642\n2    0  0.005694\n3    1  0.065052\n4    0  0.034789\n5    2  0.128495\n6    1  0.088816\n7    1  0.056725\n8    0 -0.000193\n9    2 -0.070252\n10   2  0.138282\n11   2  0.054638\n12   2  0.039994\n13   2  0.060659\n14   0  0.038562\n\n\nAnd need a box and whisker plot, grouped by column 0. I have the following:\n\nplt.figure()\ngrouped = df.groupby(0)\ngrouped.boxplot(column=1)\nplt.savefig('plot.png')\n\n\nBut I end up with three subplots. How can place all three on one plot?\nThanks.\n\n"
'I have a fairly big pandas dataframe - 50 or so headers and a few hundred thousand rows of data - and I\'m looking to transfer this data to a database using the ceODBC module. Previously I was using pyodbc and using a simple execute statement in a for loop but this was taking ridiculously long (1000 records per 10 minutes)... \n\nI\'m now trying a new module and am trying to introduce executemany() although I\'m not quite sure what\'s meant by sequence of parameters in:\n\n    cursor.executemany("""insert into table.name(a, b, c, d, e, f) \nvalues(?, ?, ?, ?, ?), sequence_of_parameters)\n\n\nshould it look like a constant list working through each header like\n\n    [\'asdas\', \'1\', \'2014-12-01\', \'true\', \'asdasd\', \'asdas\', \'2\', \n\'2014-12-02\', \'true\', \'asfasd\', \'asdfs\', \'3\', \'2014-12-03\', \'false\', \'asdasd\']\n\n\n\nwhere this is an example of three rows\n\n\nor what is the format that\'s needed?\n\nas another related question, how then can I go about converting a regular pandas dataframe to this format?\n\nThanks!\n'
"Here is a sample data frame:\n\nimport pandas as pd\n\nNaN = float('nan')\nID = [1, 2, 3, 4, 5, 6, 7]\nA = [NaN, NaN, NaN, 0.1, 0.1, 0.1, 0.1]\nB = [0.2, NaN, 0.2, 0.2, 0.2, NaN, NaN]\nC = [NaN, 0.5, 0.5, NaN, 0.5, 0.5, NaN]\ncolumns = {'A':A, 'B':B, 'C':C}\ndf = pd.DataFrame(columns, index=ID)\ndf.index.name = 'ID'\nprint(df)\n\n      A    B    C\nID               \n1   NaN  0.2  NaN\n2   NaN  NaN  0.5\n3   NaN  0.2  0.5\n4   0.1  0.2  NaN\n5   0.1  0.2  0.5\n6   0.1  NaN  0.5\n7   0.1  NaN  NaN\n\n\nI know that pandas has the pytables based HDFStore, which is an easy way to efficiently serialize/deserialize a data frame. But those datasets are not very easy to load directly using a reader h5py or matlab. How can I save a data frame using h5py, so that I can easily load it back using another hdf5 reader?\n"
'Sometimes I want to show all of the rows in a pandas DataFrame, but only for a single command or code-block.\n\nOf course I can set the "max_rows" display option to a large number, but then I have to repeat the command afterwards in order to revert to my preferred setting. (I like 12 rows max, personally).\n\npd.options.display.max_rows=1000\nmyDF\npd.options.display.max_rows=12\n\n\nThat\'s annoying.\n\nI read in the documentation that I can use the pd.option_context() function to accomplish this if I combine my command with a "with" statement:\n\nwith pd.option_context("display.max_rows", 1000): myDF\n\n\nI couldn\'t get that to work, (no output is returned). \nBut I think such a solution would still be too much typing for routine incidental usage!\n\nI wish there was some quick pythonic way to override the display options!\nDoes one exist? Have I overlooked something?\n\nI like how one can alter the # of rows that the .head() function outputs by passing it an argument for the # of rows, but it still must be lower than the "display.max_rows" setting...\n\nI know I could keep the "display.max_rows" setting really high all the time, and then tack a .head(12) function on most of the time, but I think most people would agree on how annoying that would be.\n\nI am indeed aware that one can view all (or most of?) the values in a pandas Series by passing it to a core function such as list().  But that is tricky to do with a DF. Furthermore, it\'s hard to read when it\'s not in a tabular format.\n\nSimilar to the solution for my first question, I imagine there\'s probably a way to write my own function (to be placed in a startup script), but I\'m not sure the best way to write it.\n'
'i have  actually a pandas dataframe and i want to save it to json format.\nFrom the pandas docs it says:\n\n\n  Note NaN‘s, NaT‘s and None will be converted to null and datetime\n  objects will be converted based on the date_format and date_unit\n  parameters\n\n\nThen using the orient option records  i have something like this\n\n[{"A":1,"B":4,"C":7},{"A":null,"B":5,"C":null},{"A":3,"B":null,"C":null}]\n\n\nIs it possible to have this instead:\n\n[{"A":1,"B":4,"C":7},{"B":5},{"A":3}]\'\n\n\nThank you\n'
'I have a dataframe of data that I am trying to append to another dataframe. I have tried various ways with .append() and there has been no successful way. When I print the data from iterrows. I provide 2 possible ways I tried to solve the issue below, one creates an error, the other doesn\'t populate the dataframe with anything.\n\nThe workflow I am trying to create is create a dataframe based off of a file that contains transaction history of customer orders. I only want to create a single record per order and I am going to add other logic to update the order details based on updates in the history. By the end of the script, it will have a single record for all of the orders and the end state of those orders after iterating through the history file.\n\nclass om():\n"""Manages over the current state of orders"""\n\ndef __init__(self,dataF, desc=\'NONE\'):\n    self.df = pd.DataFrame\n    self.data = dataF\n    print type(dataF)\n    self.oD= self.df(data=None,columns=desc)\n\ndef add_data(self,df):\n    for i, row in self.data.iterrows():\n        print \'row \'+str(row)\n        print type(row)\n        df.append(self.data[i], ignore_index =True) """ This line creates and error"""\n        df.append(row, ignore_index =True) """This line doesn\'t append anything to the dataframe."""\n\ntest = order_manager(body,header)\ntest.add_data(test.orderData)\n\n'
"I have the following large dataframe (df) that looks like this:\n\n    ID     date        PRICE       \n1   10001  19920103  14.500    \n2   10001  19920106  14.500    \n3   10001  19920107  14.500     \n4   10002  19920108  15.125     \n5   10002  19920109  14.500   \n6   10002  19920110  14.500    \n7   10003  19920113  14.500 \n8   10003  19920114  14.500     \n9   10003  19920115  15.000 \n\n\nQuestion: What's the most efficient way to delete (or remove) the first row of each ID? I want this:\n\n        ID     date     PRICE       \n    2   10001  19920106  14.500    \n    3   10001  19920107  14.500     \n    5   10002  19920109  14.500   \n    6   10002  19920110  14.500    \n    8   10003  19920114  14.500     \n    9   10003  19920115  15.000 \n\n\nI can do a loop over each unique ID and remove the first row but I believe this is not very efficient.\n"
'I have a df:\n\nimport pandas as pd\nimport numpy as np\nimport datetime as DT\nimport hmac\nfrom geopy.geocoders import Nominatim\nfrom geopy.distance import vincenty\n\ndf\n\n\n     city_name  state_name  county_name\n0    WASHINGTON  DC  DIST OF COLUMBIA\n1    WASHINGTON  DC  DIST OF COLUMBIA\n2    WASHINGTON  DC  DIST OF COLUMBIA\n3    WASHINGTON  DC  DIST OF COLUMBIA\n4    WASHINGTON  DC  DIST OF COLUMBIA\n5    WASHINGTON  DC  DIST OF COLUMBIA\n6    WASHINGTON  DC  DIST OF COLUMBIA\n7    WASHINGTON  DC  DIST OF COLUMBIA\n8    WASHINGTON  DC  DIST OF COLUMBIA\n9    WASHINGTON  DC  DIST OF COLUMBIA\n\n\nI want to get the latitude and longitude coordinates for any one of the columns in the data frame below.  The documentation (http://geopy.readthedocs.org/en/latest/#data) is pretty straightforward when working with the documentation for individual locations. \n\n&gt;&gt;&gt; from geopy.geocoders import Nominatim\n&gt;&gt;&gt; geolocator = Nominatim()\n&gt;&gt;&gt; location = geolocator.geocode("175 5th Avenue NYC")\n&gt;&gt;&gt; print(location.address)\nFlatiron Building, 175, 5th Avenue, Flatiron, New York, NYC, New York,     ...\n&gt;&gt;&gt; print((location.latitude, location.longitude))\n(40.7410861, -73.9896297241625)\n&gt;&gt;&gt; print(location.raw)\n{\'place_id\': \'9167009604\', \'type\': \'attraction\', ...}\n\n\nHowever I want to apply the function to each row in the df and make a new column.  I\'ve tried the following \n\ndf[\'city_coord\'] = geolocator.geocode(lambda row: \'state_name\' (row))\n\n\nbut I think I\'m missing something in my code because I get the following:\n\n    city_name   state_name  county_name coordinates\n0    WASHINGTON  DC  DIST OF COLUMBIA    None\n1    WASHINGTON  DC  DIST OF COLUMBIA    None\n2    WASHINGTON  DC  DIST OF COLUMBIA    None\n3    WASHINGTON  DC  DIST OF COLUMBIA    None\n4    WASHINGTON  DC  DIST OF COLUMBIA    None\n5    WASHINGTON  DC  DIST OF COLUMBIA    None\n6    WASHINGTON  DC  DIST OF COLUMBIA    None\n7    WASHINGTON  DC  DIST OF COLUMBIA    None\n8    WASHINGTON  DC  DIST OF COLUMBIA    None\n9    WASHINGTON  DC  DIST OF COLUMBIA    None\n\n\nI would like something like this hopefully using the Lambda function:\n\n     city_name  state_name  county_name  city_coord\n0    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n1    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n2    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n3    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n4    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n5    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n6    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n7    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n8    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456 \n9    WASHINGTON  DC  DIST OF COLUMBIA    38.8949549, -77.0366456\n10   GLYNCO      GA  GLYNN               31.2224512, -81.5101023\n\n\nI appreciate any help.  After I get the coordinates I\'d like to map them.  Any recommended resources for mapping coordinates is greatly appreciated too. thanks\n'
'I\'m very new to PyQt and I am struggling to populate a QTableView control.\n\nMy code is the following:\n\ndef data_frame_to_ui(self, data_frame):\n        """\n        Displays a pandas data frame into the GUI\n        """\n        list_model = QtGui.QStandardItemModel()\n        i = 0\n        for val in data_frame.columns:\n            # for the list model\n            if i &gt; 0:\n                item = QtGui.QStandardItem(val)\n                #item.setCheckable(True)\n                item.setEditable(False)\n                list_model.appendRow(item)\n            i += 1\n        self.ui.profilesListView.setModel(list_model)\n\n        # for the table model\n        table_model = QtGui.QStandardItemModel()\n\n        # set table headers\n        table_model.setColumnCount(data_frame.columns.size)\n        table_model.setHorizontalHeaderLabels(data_frame.columns.tolist())\n        self.ui.profileTableView.horizontalHeader().setStretchLastSection(True)\n\n        # fill table model data\n        for row_idx in range(10): #len(data_frame.values)\n            row = list()\n            for col_idx in range(data_frame.columns.size):\n                val = QtGui.QStandardItem(str(data_frame.values[row_idx][col_idx]))\n                row.append(val)\n            table_model.appendRow(row)\n\n        # set table model to table object\n        self.ui.profileTableView.setModel(table_model)\n\n\nActually in the code I succeed to populate a QListView, but the values I set to the QTableView are not displayed, also you can see that I truncated the rows to 10 because it takes forever to display the hundreds of rows of the data frame.\n\nSo, What is the fastest way to populate the table model from a pandas data frame?\n\nThanks in advance.\n'
"I'm trying to replace some NaN values in my data with an empty list []. However the list is represented as a str and doesn't allow me to properly apply the len() function. is there anyway to replace a NaN value with an actual empty list in pandas?\n\nIn [28]: d = pd.DataFrame({'x' : [[1,2,3], [1,2], np.NaN, np.NaN], 'y' : [1,2,3,4]})\n\nIn [29]: d\nOut[29]:\n           x  y\n0  [1, 2, 3]  1\n1     [1, 2]  2\n2        NaN  3\n3        NaN  4\n\nIn [32]: d.x.replace(np.NaN, '[]', inplace=True)\n\nIn [33]: d\nOut[33]:\n           x  y\n0  [1, 2, 3]  1\n1     [1, 2]  2\n2         []  3\n3         []  4\n\nIn [34]: d.x.apply(len)\nOut[34]:\n0    3\n1    2\n2    2\n3    2\nName: x, dtype: int64\n\n"
'I have a .csv file of contact information that I import as a pandas data frame. \n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; \n&gt;&gt;&gt; df = pd.read_csv(\'data.csv\')\n&gt;&gt;&gt; df.head()\n\n  fName   lName                    email   title\n0  John   Smith         jsmith@gmail.com     CEO\n1   Joe   Schmo      jschmo@business.com  Bagger\n2  Some  Person  some.person@hotmail.com   Clerk\n\n\nAfter importing the data, I\'d like to drop rows where one field contains one of several substrings in a list. For example:\n\nto_drop = [\'Clerk\', \'Bagger\']\n\nfor i in range(len(df)):\n    for k in range(len(to_drop)):\n        if to_drop[k] in df.title[i]:\n            # some code to drop the rows from the data frame\n\ndf.to_csv("results.csv")\n\n\nWhat is the preferred way to do this in Pandas? Should this even be a post-processing step, or is it preferred to filter this prior to writing to the data frame in the first place? My thought was that this would be easier to manipulate once in a data frame object.\n'
'This pandas python code generates the error message, \n\n\n  "TypeError: bad operand type for unary ~: \'float\'"\n\n\nI have no idea why because I\'m trying to manipulate a str object\n\ndf_Anomalous_Vendor_Reasons[~df_Anomalous_Vendor_Reasons[\'V\'].str.contains("File*|registry*")] #sorts, leaving only cases where reason is NOT File or Registry\n\n\nAnybody got any ideas?\n'
'Is it possible to select the negation of a given list from pandas dataframe?. For instance, say I have the following dataframe \n\nT1_V2  T1_V3 T1_V4 T1_V5 T1_V6 T1_V7 T1_V8\n1     15      3      2     N     B     N         \n4     16     14      5     H     B     N            \n1     10     10      5     N     K     N  \n\n\nand I want to get out all columns but column T1_V6. I would normally do that this way: \n\ndf = df[["T1_V2","T1_V3","T1_V4","T1_V5","T1_V7","T1_V8"]]\n\n\nMy question is on whether there is a way to this the other way around, something like this \n\ndf = df[!["T1_V6"]]\n\n'
"I have recently started using the nltk module for text analysis. I am stuck at a point. I want to use word_tokenize on a dataframe, so as to obtain all the words used in a particular row of the dataframe.\n\ndata example:\n       text\n1.   This is a very good site. I will recommend it to others.\n2.   Can you please give me a call at 9983938428. have issues with the listings.\n3.   good work! keep it up\n4.   not a very helpful site in finding home decor. \n\nexpected output:\n\n1.   'This','is','a','very','good','site','.','I','will','recommend','it','to','others','.'\n2.   'Can','you','please','give','me','a','call','at','9983938428','.','have','issues','with','the','listings'\n3.   'good','work','!','keep','it','up'\n4.   'not','a','very','helpful','site','in','finding','home','decor'\n\n\nBasically, i want to separate all the words and find the length of each text in the dataframe.\n\nI know word_tokenize can for it for a string, but how to apply it onto the entire dataframe?\n\nPlease help!\n\nThanks in advance...\n"
'I decided to compare skew and kurtosis functions in pandas and scipy.stats, and don\'t understand why I\'m getting different results between libraries.\n\nAs far as I can tell from the documentation, both kurtosis functions compute using Fisher\'s definition, whereas for skew there doesn\'t seem to be enough of a description to tell if there any major differences with how they are computed.\n\nimport pandas as pd\nimport scipy.stats.stats as st\n\nheights = np.array([1.46, 1.79, 2.01, 1.75, 1.56, 1.69, 1.88, 1.76, 1.88, 1.78])\n\nprint "skewness:", st.skew(heights)\nprint "kurtosis:", st.kurtosis(heights)\n\n\nthis returns:\n\nskewness: -0.393524456473\nkurtosis: -0.330672097724\n\n\nwhereas if I convert to a pandas dataframe:\n\nheights_df = pd.DataFrame(heights)\nprint "skewness:", heights_df.skew()\nprint "kurtosis:", heights_df.kurtosis() \n\n\nthis returns:\n\nskewness: 0   -0.466663\nkurtosis: 0    0.379705\n\n\nApologies if I\'ve posted this in the wrong place; not sure if it\'s a stats or a programming question.\n'
'I new to Python and I\'m therefore having trouble converting a row in a DataFrame into a flat list.  To do this I use the following code:\n\nToy DataFrame:\n\nimport pandas as pd\nd = {\n     "a": [1, 2, 3, 4, 5],\n     "b": [9, 8, 7, 6, 5],\n     "n": ["a", "b", "c", "d", "e"]\n}\n\ndf = pd.DataFrame(d)\n\n\nMy code:\n\ndf_note = df.loc[df.n == "d", ["a", "b"]].values #convert to array\ndf_note = df_note.tolist() #convert to nested list\ndf_note = reduce(lambda x, y: x + y, df_note) #convert to flat list\n\n\nTo me this code appears to be both gross and inefficient.  The fact that I convert to an array before a list is what is causing the problem, i.e. the list to be nested.  That withstanding, I can not find a means of converting the row directly to a list.  Any advice?\n\nThis question is not a dupe of this.  In my case, I want the list to be flat.\n'
'I have a DataFrame with a few time series:\n\n         divida    movav12       var  varmovav12\nDate                                            \n2004-01       0        NaN       NaN         NaN\n2004-02       0        NaN       NaN         NaN\n2004-03       0        NaN       NaN         NaN\n2004-04      34        NaN       inf         NaN\n2004-05      30        NaN -0.117647         NaN\n2004-06      44        NaN  0.466667         NaN\n2004-07      35        NaN -0.204545         NaN\n2004-08      31        NaN -0.114286         NaN\n2004-09      30        NaN -0.032258         NaN\n2004-10      24        NaN -0.200000         NaN\n2004-11      41        NaN  0.708333         NaN\n2004-12      29  24.833333 -0.292683         NaN\n2005-01      31  27.416667  0.068966    0.104027\n2005-02      28  29.750000 -0.096774    0.085106\n2005-03      27  32.000000 -0.035714    0.075630\n2005-04      30  31.666667  0.111111   -0.010417\n2005-05      31  31.750000  0.033333    0.002632\n2005-06      39  31.333333  0.258065   -0.013123\n2005-07      36  31.416667 -0.076923    0.002660\n\n\nI want to decompose the first time series divida in a way that I can separate its trend from its seasonal and residual components.\n\nI found an answer here, and am trying to use the following code:\n\nimport statsmodels.api as sm\n\ns=sm.tsa.seasonal_decompose(divida.divida)\n\n\nHowever I keep getting this error:\n\nTraceback (most recent call last):\nFile "/Users/Pred_UnBR_Mod2.py", line 78, in &lt;module&gt; s=sm.tsa.seasonal_decompose(divida.divida)\nFile "/Library/Python/2.7/site-packages/statsmodels/tsa/seasonal.py", line 58, in seasonal_decompose _pandas_wrapper, pfreq = _maybe_get_pandas_wrapper_freq(x)\nFile "/Library/Python/2.7/site-packages/statsmodels/tsa/filters/_utils.py", line 46, in _maybe_get_pandas_wrapper_freq\nfreq = index.inferred_freq\nAttributeError: \'Index\' object has no attribute \'inferred_freq\'\n\n\nHow can I proceed?\n'
"I have a series data type which was generated by subtracting two columns from pandas data frame.\n\nI want to remove the first element from the series which would be x[-1] in R. I can get it to work in np array class but series class doesn't work.\n"
"I have a DataFrame:\n\n         Seasonal\nDate             \n2014-12 -1.089744\n2015-01 -0.283654\n2015-02  0.158974\n2015-03  0.461538\n\n\nI used a pd.to_period in the DataFrame, so its index has turned into a Pandas period type (type 'pandas._period.Period').\n\nNow, I want to turn that index to strings. I'm trying to apply the following:\n\ndf.index=df.index.astype(str)\n\n\nHowever that doesn't work...\n\nValueError: Cannot cast PeriodIndex to dtype |S0\n\n\nMy code has been frozen since then.\n\nS.O.S.\n"
"Seems strange, but I cannot find an easy way to find the local timezone using Pandas/pytz in Python.\n\nI can do:\n\n&gt;&gt;&gt; pd.Timestamp('now', tz='utc').isoformat()\nOut[47]: '2016-01-28T09:36:35.604000+00:00'\n&gt;&gt;&gt; pd.Timestamp('now').isoformat()\nOut[48]: '2016-01-28T10:36:41.830000'\n&gt;&gt;&gt; pd.Timestamp('now').tz_localize('utc') - pd.Timestamp('now', tz='utc')\nOut[49]: Timedelta('0 days 01:00:00')\n\n\nWhich will give me the timezone, but this is probably not the best way to do it...\nIs there a command in pytz or pandas to get the system time zone? (preferably in python 2.7 )\n"
'I am wondering how I can pass matplotlibs where="post" into a pandas plot.\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.randn(36, 3))\ndf.plot(drawstyle="steps", linewidth=2)\n\n# this doesn\'t work\ndf.plot(drawstyle="steps", where=\'post\')\n\n\nDoes anyone know how to realize this?\n\nThanks in advance!\n'
"I want to compute the time difference between times in a DateTimeIndex\n\nimport pandas as pd\np = pd.DatetimeIndex(['1985-11-14', '1985-11-28', '1985-12-14', '1985-12-28'], dtype='datetime64[ns]')\n\n\nI can compute the time difference of two times:\n\np[1] - p[0]\n\n\ngives\n\nTimedelta('14 days 00:00:00')\n\n\nBut p[1:] - p[:-1] doesn't work and gives\n\nDatetimeIndex(['1985-12-28'], dtype='datetime64[ns]', freq=None)\n\n\nand a future warning: \n\nFutureWarning: using '-' to provide set differences with datetimelike Indexes is deprecated, use .difference()\n\n\nAny thought on how how I can (easily) compute the time difference between values in a DateTimeIndex? And why does it work for 1 value, but not for the entire DateTimeIndex?\n"
'I\'ve been trying to test various methods for making my code to run. To begin with, I have this list:\n\nmember_list = [111,222,333,444,555,...]\n\nI tried to pass it into this query:\n\nquery = pd.read_sql_query(\n"""\nselect member id\n    ,yearmonth\nfrom queried_table\nwhere yearmonth between ? and ?\n    and member_id in ?\n""", db2conn, params = [201601, 201603, member_list])\n\n\nHowever, I get an error that says:\n\n\n  \'Invalid parameter type.  param-index=2 param-type=list\', \'HY105\'\n\n\nSo I looked around and tried using formatted strings:\n\nquery = pd.read_sql_query(\n"""\nselect member id\n    ,yearmonth\nfrom queried_table\nwhere yearmonth between ? and ?\n    and member_id in (%s)\n""" % \',\'.join([\'?\']*len(member_list), db2conn, params = [201601, 201603, tuple(member_list)])\n\n\nNow, I get the error:\n\n\n  \'The SQL contains 18622 parameter markers, but 3 parameters were supplied\', \'HY000\'\n\n\nbecause it\'s looking to fill in all the ? placeholders in the formatted string.\n\nSo, ultimately, is there a way to somehow evaluate the list and pass each individual element to bind to the ? or is there another method I could use to get this to work?\n\nBtw, I\'m using pyodbc as my connector.\n\nThanks in advance!\n'
"I have files of the below format in a text file which I am trying to read into a pandas dataframe.\n\n895|2015-4-23|19|10000|LA|0.4677978806|0.4773469340|0.4089938425|0.8224291972|0.8652525793|0.6829942860|0.5139162227|\n\n\nAs you can see there are 10 integers after the floating point in the input file.\n\ndf = pd.read_csv('mockup.txt',header=None,delimiter='|')\n\n\nWhen I try to read it into dataframe, I am not getting the last 4 integers\n\ndf[5].head()\n\n0    0.467798\n1    0.258165\n2    0.860384\n3    0.803388\n4    0.249820\nName: 5, dtype: float64\n\n\nHow can I get the complete precision as present in the input file? I have some matrix operations that needs to be performed so i cannot cast it as string. \n\nI figured out that I have to do something about dtype but I am not sure where I should use it.\n"
"I'm looking for a pandas equivalent of the resample method for a dataframe whose isn't a DatetimeIndex but an array of integers, or maybe even floats.\n\nI know that for some cases (this one, for example) the resample method can be substituted easily by a reindex and interpolation, but for some cases (I think) it can't.\n\nFor example, if I have \n\ndf = pd.DataFrame(np.random.randn(10,2))\nwithdates = df.set_index(pd.date_range('2012-01-01', periods=10))\nwithdates.resample('5D', np.std)\n\n\nthis gives me\n\n                   0         1\n2012-01-01  1.184582  0.492113\n2012-01-06  0.533134  0.982562\n\n\nbut I can't produce the same result with df and resample. So I'm looking for something that would work as\n\n df.resample(5, np.std)\n\n\nand that would give me \n\n          0         1\n0  1.184582  0.492113\n5  0.533134  0.982562\n\n\nDoes such a method exist? The only way I was able to create this method was by manually separating df into smaller dataframes, applying np.std and then concatenating everything back, which I find pretty slow and not smart at all.\n\nCheers\n"
"The pandas option max_colwidth controls how many characters will be included in the repr of a dataframe:\n\nimport string, random\nimport pandas as pd\ndf = pd.DataFrame([''.join(random.choice(string.ascii_lowercase + ' ') for j in range(1000)) for i in range(4)])\n\npd.options.display.max_colwidth = 10\nprint(df)\n\n\nyields\n\n           0\n0  lmftge...\n1  pqttqb...\n2  wi wgy...\n3  ow dip...\n\n\nand \n\npd.options.display.max_colwidth = 30\nprint(df)\n\n\nyields\n\n                               0\n0  lmftgenioerszvgzfaxorzciow...\n1  pqttqbqqe pykgguxnjsspbcti...\n2  wi wgybtgcbxkobrwnaxpxwsjc...\n3  ow dippaiamvvcofvousieckko...\n\n\nAnd you can set pd.options.display.max_colwidth = 0 to remove the limit altogether. Fine so far!\n\nBut if the dataframe is rendered in HTML inside a notebook, the notebook will wrap the table of the column to the width of the display, regardless of this setting:\n\n\n\nIs there any way to avoid this, i.e. to have the HTML table column rendered as wide as is necessary to fit the each row on a single line?\n\nMore generally, is it possible to control the width of HTML table columns in notebook output independent of the number of characters in the pandas output?\n"
'I have a working_df in pandas I\'d like to output to sqlite database. \n\nfrom sqlalchemy import create_engine\n\nsql_engine = create_engine(\'sqlite:///test.db\', echo=False)\nworking_df.to_sql(\'data\', sql_engine,index=False, if_exists=\'append\')\n\n\nreturns: AttributeError: \'Engine\' object has no attribute \'cursor\'\n\nAny thoughts?\n\nPandas version \'0.18.1\'\n\nEdit: Added full trace\n\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-41-4f64fc939721&gt; in &lt;module&gt;()\n----&gt; 1 working_df.to_sql(\'data\', engine, index=False, if_exists=\'append\')\n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/core/generic.py in to_sql(self, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\n   1163         sql.to_sql(self, name, con, flavor=flavor, schema=schema,\n   1164                    if_exists=if_exists, index=index, index_label=index_label,\n-&gt; 1165                    chunksize=chunksize, dtype=dtype)\n   1166 \n   1167     def to_pickle(self, path):\n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/io/sql.py in to_sql(frame, name, con, flavor, schema, if_exists, index, index_label, chunksize, dtype)\n    569     pandas_sql.to_sql(frame, name, if_exists=if_exists, index=index,\n    570                       index_label=index_label, schema=schema,\n--&gt; 571                       chunksize=chunksize, dtype=dtype)\n    572 \n    573 \n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/io/sql.py in to_sql(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype)\n   1659                             if_exists=if_exists, index_label=index_label,\n   1660                             dtype=dtype)\n-&gt; 1661         table.create()\n   1662         table.insert(chunksize)\n   1663 \n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/io/sql.py in create(self)\n    688 \n    689     def create(self):\n--&gt; 690         if self.exists():\n    691             if self.if_exists == \'fail\':\n    692                 raise ValueError("Table \'%s\' already exists." % self.name)\n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/io/sql.py in exists(self)\n    676 \n    677     def exists(self):\n--&gt; 678         return self.pd_sql.has_table(self.name, self.schema)\n    679 \n    680     def sql_schema(self):\n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/io/sql.py in has_table(self, name, schema)\n   1674         query = flavor_map.get(self.flavor)\n   1675 \n-&gt; 1676         return len(self.execute(query, [name, ]).fetchall()) &gt; 0\n   1677 \n   1678     def get_table(self, table_name, schema=None):\n\n/Users/tom/anaconda/envs/data_science/lib/python3.5/site-packages/pandas/io/sql.py in execute(self, *args, **kwargs)\n   1557             cur = self.con\n   1558         else:\n-&gt; 1559             cur = self.con.cursor()\n   1560         try:\n   1561             if kwargs:\n\nAttributeError: \'Engine\' object has no attribute \'cursor\'\n\n'
'I have read the manual here and saw this answer, but it is not working:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt; pd.Series([my_list]).to_csv(\'output.tsv\',sep=\'\\t\',index=False,header=False, quoting=csv.QUOTE_NONE)\n    Traceback (most recent call last):\n      File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n    TypeError: to_csv() got an unexpected keyword argument \'quoting\'\n\n\nWithout the quoting argument, it works. \n\npd.Series([my_list]).to_csv(\'output.tsv\',sep=\'\\t\',index=False,header=False)\n\n\nBut this is incompatible with my intended usage. \n\nTo make things even more confusing, when I wrote out a table this way, there were no quotes, and no errors:\n\nmy_dataframe.to_csv(\'output2.tsv\',sep=\'\\t\', quoting=csv.QUOTE_NONE)\n\n\nAny idea what is going on?\n'
"Since one column of my pandas dataframe has nan value, so when I want to get the max value of that column, it just return error. \n\n&gt;&gt;&gt; df.iloc[:, 1].max()\n'error:512'\n\n\nHow can I skip that nan value and get the max value of that column?\n"
"I have a dataframe with multiple levels, eg:\n\nidx = pd.MultiIndex.from_product((['foo', 'bar'], ['one', 'five', 'three' 'four']),\n                                 names=['first', 'second'])\ndf = pd.DataFrame({'A': [np.nan, 12, np.nan, 11, 16, 12, 11, np.nan]}, index=idx).dropna().astype(int)\n\n              A     \nfirst second\nfoo   five     12\n      four     11\nbar   one      16\n      five     12\n      three    11\n\n\nI want to create a new column using the index level titled second, so that I get\n\n              A    B  \nfirst second\nfoo   five     12   five\n      four     11   four\nbar   one      16   one\n      five     12   five\n      three    11   three\n\n\nI can do this by resetting the index, copying the column, then re-applying, but that seems more round-about. \n\nI tried df.index.levels[1], but that creates a sorted list, it doesn't preserve the order. \n\nIf it was a single index, I would use df.index but in a multiindex that creates a column of tuples. \n\nIf this is resolved elsewhere, please share as I haven't had any luck searching the stackoverflow archives.\n"
'code: df[\'review\'].head()\n        index         review\noutput: 0      These flannel wipes are OK, but in my opinion\n\n\nI want to remove punctuations from the column of the dataframe and create a new column.\n\ncode: import string \n      def remove_punctuations(text):\n          return text.translate(None,string.punctuation)\n\n      df["new_column"] = df[\'review\'].apply(remove_punctuations)\n\nError:\n  return text.translate(None,string.punctuation)\n  AttributeError: \'float\' object has no attribute \'translate\'\n\n\nI am using python 2.7. Any suggestions would be helpful.\n'
'I have a table in a pandas DataFrame named df:\n\n+--- -----+------------+-------------+----------+------------+-----------+\n|avg_views| avg_orders | max_views   |max_orders| min_views  |min_orders |\n+---------+------------+-------------+----------+------------+-----------+\n| 23       | 123       |   135       | 500      |    3       |    1      |\n+---------+------------+-------------+----------+------------+-----------+ \n\n\nWhat I am looking for now is to plot a grouped bar graph which shows me\n(avg, max, min) of views and orders in one single bar chart.\n\ni.e on x axis there would be Views and orders separated by a distance\nand 3 bars of (avg, max, min) for views and similarly for orders.\n\nI have attached a sample bar graph image, just to know how the bar graph should look.\n\n\nGreen color should be for avg, yellow for max and pink for avg.\n\nI took the following code from setting spacing between grouped bar plots in matplotlib but it is not working for me:\n\nplt.figure(figsize=(13, 7), dpi=300)\n\ngroups = [[23, 135, 3], [123, 500, 1]]\ngroup_labels = [\'views\', \'orders\']\nnum_items = len(group_labels)\nind = np.arange(num_items)\nmargin = 0.05\nwidth = (1. - 2. * margin) / num_items\n\ns = plt.subplot(1, 1, 1)\nfor num, vals in enumerate(groups):\n    print \'plotting: \', vals\n    # The position of the xdata must be calculated for each of the two data \n    # series.\n    xdata = ind + margin + (num * width)\n    # Removing the "align=center" feature will left align graphs, which is \n    # what this method of calculating positions assumes.\n    gene_rects = plt.bar(xdata, vals, width)\ns.set_xticks(ind + 0.5)\ns.set_xticklabels(group_labels)\n\n\n\n  plotting:  [23, 135, 3]\n  ...\n  ValueError: shape mismatch: objects cannot be broadcast to a single shape\n\n'
'Single row of a DataFrame prints value side by side, i.e. column_name then columne_value in one line and next line contains next column_name and columne_value. For example, below code    \n\nimport pandas as pd\n\ndf = pd.DataFrame([[100,200,300],[400,500,600]])\nfor index, row in df.iterrows():\n    # other operations goes here....\n    print row\n\n\nOutput for first row comes as \n\n0    100\n1    200\n2    300\nName: 0, dtype: int64    \n\n\nIs there a way to have each row printed horizontally and ignore the datatype, Name? Example for the first row:\n\n0    1    2\n100  200  300\n\n'
"I have two dataframes with only somewhat overlapping indices and columns.\n\nold = pd.DataFrame(index = ['A', 'B', 'C'],\n                   columns = ['k', 'l', 'm'],\n                   data = abs(np.floor(np.random.rand(3, 3)*10)))\n\nnew = pd.DataFrame(index = ['A', 'B', 'C', 'D'],\n                   columns = ['k', 'l', 'm', 'n'],\n                   data = abs(np.floor(np.random.rand(4, 4)*10)))\n\n\nI want to calculate the difference between them and tried\n\ndelta = new - old\n\n\nThis gives lots of NaNs where indices and columns do not match. I would like to treat the abscence of the indices and columns as zeroes, (old['n', 'D'] = 0). old will always be a subspace of new.\n\nAny ideas?\n\nEDIT:\nI guess I didn't explain it thoroughly enough. I don't want to fill the delta dataframe with zeroes. I want to treat missing indices and columns in old as if they were zeroes. I would then get the value in new['n', 'D'] in delta instead of a NaN.\n"
"Here's my code:\n\ndef topK(dataMat,sensitivity):\n    meanVals = np.mean(dataMat, axis=0)\n    meanRemoved = dataMat - meanVals\n    covMat = np.cov(meanRemoved, rowvar=0)\n    eigVals,eigVects = np.linalg.eig(np.mat(covMat))\n\n\nI get the error in the title on the last line above.  I suspect has something to do with the datatype, so, here's an image of the variable and datatype from the Variable Explorer in Spyder:\n\n\n\nI've tried changing np.linalg.eig(np.mat(covMat)) to np.linalg.eig(np.array(np.mat(covMat))) and to np.linalg.eig(np.array(covMat)), nothing works.  Any ideas?  (an example would be great!)\n"
"I make dataframe like this.\n\ndf = pd.DataFrame({\n    'class' : ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],\n    'number' : [1,2,3,4,5,1,2,3,4,5],\n    'math' : [90, 20, 50, 30, 57, 67, 89, 79, 45, 23],\n    'english' : [40, 21, 68, 89, 90, 87, 89, 54, 21, 23]\n})\n\n\nand i want to convert index to this by using some pandas methods.(ex. set_index, stack,,,)\n\ndf1 = pd.DataFrame(np.random.randint(1, 100, (5, 4)),\n             columns = [['A', 'A', 'B', 'B'],['english', 'math', 'english', 'math']],\n             index = [1, 2, 3, 4, 5])\n\n\nhow can i do this?\n"
"I have this dataframe \n\n       X    Y  Z    Value \n0      18   55  1      70   \n1      18   55  2      67 \n2      18   57  2      75     \n3      18   58  1      35  \n4      19   54  2      70   \n\n\nI want to save it as a text file with this format\n\n   X    Y  Z    Value \n   18   55  1      70   \n   18   55  2      67 \n   18   57  2      75     \n   18   58  1      35  \n   19   54  2      70   \n\n\nI tried this code but is not working:\n\nnp.savetxt('xgboost.txt', a.values, delimiter ='\\t')\n\nTypeError: Mismatch between array dtype ('object') and format specifier ('%.18e %.18e %.18e')\n\n"
'Suddenly, output for statements started to appear inside scrollable frames.\n\nI was playing with only one parameter\n\npd.options.display.max_rows = 1000\n\n\nbut after experiments, I commented this line out and restarted the kernel. \n\nNevertheless, one of my outputs appears inside frame.\n\nHow to avoid this?\n'
'I have a data set in which there is a column known as Native Country which contain around 30000 records. Some are missing represented by NaN so I thought to fill it with mode() value.  I wrote something like this:\n\ndata[\'Native Country\'].fillna(data[\'Native Country\'].mode(), inplace=True)\n\n\nHowever when I do a count of missing values: \n\nfor col_name in data.columns: \n    print ("column:",col_name,".Missing:",sum(data[col_name].isnull()))\n\n\nIt is still coming up with the same number of NaN values for the column Native Country.\n'
"I have a data table of data for a variety of genomic positions. The positions are represented as 3-tuples ('chromosome', 'srand', position) that I've turned into a multi-index. My goal is to look up various information about each position and add that to the table (for example gene name, etc.) I can do this with pybedtools.\n\ndf = pd.DataFrame(data={'A':range(1,8), 'B':range(1,8), 'C': range(1,8)},\n index=pd.MultiIndex.from_tuples([('chrom1', '-', 1234), ('chrom1', '+', 5678),\n ('chrom1', '+', 9876),  ('chrom2', '+', 13579), ('chrom2', '+', 8497), ('chrom2', '-', 98765),\n ('chrom2', '-', 76856)]))\n\ndf.index.rename(['chrom','strand','abs_pos'], inplace=True)\n\n                       A  B  C\nchrom  strand abs_pos         \nchrom1 -      1234     1  1  1\n       +      5678     2  2  2\n              9876     3  3  3\nchrom2 +      13579    4  4  4\n              8497     5  5  5\n       -      98765    6  6  6\n              76856    7  7  7\n\n\nMy issue is with adding columns to a data frame with a multi-index. This seems straight forward without a multi-index: pandas - add new column to dataframe from dictionary\n\nI have a dictionary of the look up information with 3-tuple keys corresponding to the multi-index. How can I add this data as a new column?\n\ngene_d = {('chrom1', '-', 1234) : 'geneA', ('chrom1', '+', 5678): 'geneB', \n    ('chrom1', '+', 9876): 'geneC', ('chrom2', '+', 13579): 'geneD',\n    ('chrom2', '+', 8497): 'geneE', ('chrom2', '-', 98765): 'geneF', \n    ('chrom2', '-', 76856): 'geneG'}\n\n\nI've tried map, but can't seem to figure out how to get it to work with a multi-index to yield the following:\n\n                                A  B  C\nchrom  strand abs_pos gene\nchrom1 -      1234    geneA     1  1  1\n       +      5678    geneB     2  2  2\n              9876    geneC     3  3  3\nchrom2 +      13579   geneD     4  4  4\n              8497    geneE     5  5  5\n       -      98765   geneF     6  6  6\n              76856   geneG     7  7  7\n\n"
"df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n               'B' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n               'C' : [np.nan, 'bla2', np.nan, 'bla3', np.nan, np.nan, np.nan, np.nan]})\n\n\nOutput:  \n\n     A      B     C\n0  foo    one   NaN\n1  bar    one  bla2\n2  foo    two   NaN\n3  bar  three  bla3\n4  foo    two   NaN\n5  bar    two   NaN\n6  foo    one   NaN\n7  foo  three   NaN\n\n\nI would like to use groupby in order to count the number of NaN's for the different combinations of foo.  \n\nExpected Output (EDIT):  \n\n     A      B     C    D\n0  foo    one   NaN    2\n1  bar    one  bla2    0\n2  foo    two   NaN    2\n3  bar  three  bla3    0\n4  foo    two   NaN    2\n5  bar    two   NaN    1\n6  foo    one   NaN    2\n7  foo  three   NaN    1\n\n\nCurrently I am trying this:\n\ndf['count']=df.groupby(['A'])['B'].isnull().transform('sum')\n\n\nBut this is not working...\n\nThank You\n"
'I would like to find out what is the most efficient way to achieve the following in Python:\n\nSuppose we have two lists a and b which are of equal length and contain up to 1e7 elements.\nHowever, for the ease of illustration we may consider the following:\n\na = [2, 1, 2, 3, 4, 5, 4, 6, 5, 7, 8, 9, 8,10,11]\nb = [1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14,15]\n\n\nThe goal is to create a strictly monotonic list a_new from a whereas only the first sample point of sample points with identical values is used.\nThe same indices that have to be deleted in a should also be deleted in b such that the final result will be:\n\na_new = [2, 3, 4, 5, 6, 7, 8, 9,10,11]\nb_new = [1, 4, 5, 6, 8,10,11,12,14,15]\n\n\nOf course this can be done using computationally expensive for loops which is however not suitable due to the huge amount of data.\n\nAny suggestions are very appreciated.\n'
"I have the following pandas data frame:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({\n               'fc': [100,100,112,1.3,14,125],\n               'sample_id': ['S1','S1','S1','S2','S2','S2'],\n               'gene_symbol': ['a', 'b', 'c', 'a', 'b', 'c'],\n               })\n\ndf = df[['gene_symbol', 'sample_id', 'fc']]\ndf\n\n\nWhich produces this:\n\nOut[11]:\n  gene_symbol sample_id     fc\n0           a        S1  100.0\n1           b        S1  100.0\n2           c        S1  112.0\n3           a        S2    1.3\n4           b        S2   14.0\n5           c        S2  125.0\n\n\nHow can I spread sample_id so that in the end I get this:\n\ngene_symbol    S1   S2\na             100   1.3\nb             100   14.0\nc             112   125.0\n\n"
'Here\'s an MWE of some code I\'m using. I slowly whittle down an initial dataframe via slicing and some conditions until I have only the rows that I need. Each block of five rows actually represents a different object so that, as I whittle things down, if any one row in each block of five meets the criteria, I want to keep it -- this is what the loop over keep.index accomplishes. No matter what, when I\'m done I can see that the final indices I want exist, but I get an error message saying "IndexError: positional indexers are out-of-bounds." What is happening here?\n\nimport pandas as pd\nimport numpy as np\n\ntemp = np.random.rand(100,5)\n\ndf = pd.DataFrame(temp, columns=[\'First\', \'Second\', \'Third\', \'Fourth\', \'Fifth\'])\n\ndf_cut = df.iloc[10:]\n\nkeep = df_cut.loc[(df_cut[\'First\'] &lt; 0.5) &amp; (df_cut[\'Second\'] &lt;= 0.6)]\n\nnew_indices_to_use = []\nfor item in keep.index:\n    remainder = (item % 5)\n    add = np.arange(0-remainder,5-remainder,1)\n    inds_to_use = item + add\n    new_indices_to_use.append(inds_to_use)\n\nnew_indices_to_use = [ind for sublist in new_indices_to_use for ind in sublist]\nfinal_indices_to_use = []\nfor item in new_indices_to_use:\n    if item not in final_indices_to_use:\n        final_indices_to_use.append(item)\n\nfinal = df_cut.iloc[final_indices_to_use]\n\n'
'Hi I have a DataFrame/Series with 2-level multi-index and one column. I would like to take the second-level index and use it as a column. For example (code taken from multi-index docs):\n\nimport pandas as pd\nimport numpy as np\n\narrays = [[\'bar\', \'bar\', \'baz\', \'baz\', \'foo\', \'foo\', \'qux\', \'qux\'],\n          [\'one\', \'two\', \'one\', \'two\', \'one\', \'two\', \'one\', \'two\']]\ntuples = list(zip(*arrays))\nindex = pd.MultiIndex.from_tuples(tuples, names=[\'first\', \'second\'])\ns = pd.DataFrame(np.random.randn(8), index=index, columns=["col"])\n\n\nWhich looks like:\n\nfirst  second\nbar    one      -0.982656\n       two      -0.078237\nbaz    one      -0.345640\n       two      -0.160661\nfoo    one      -0.605568\n       two      -0.140384\nqux    one       1.434702\n       two      -1.065408\ndtype: float64\n\n\nWhat I would like is to have a DataFrame with index [bar, baz, foo, qux] and columns [one, two].\n'
'I\'m wanting to do a frequency count on a single column of a dask dataframe. The code works, but I get an warning complaining that meta is not defined. If I try to define meta I get an error AttributeError: \'DataFrame\' object has no attribute \'name\'. For this particular use case it doesn\'t look like I need to define meta but I\'d like to know how to do that for future reference.\n\nDummy dataframe and the column frequencies\n\nimport pandas as pd\nfrom dask import dataframe as dd\n\ndf = pd.DataFrame([[\'Sam\', \'Alex\', \'David\', \'Sarah\', \'Alice\', \'Sam\', \'Anna\'],\n                   [\'Sam\', \'David\', \'David\', \'Alice\', \'Sam\', \'Alice\', \'Sam\'],\n                   [12, 10, 15, 23, 18, 20, 26]],\n                  index=[\'Column A\', \'Column B\', \'Column C\']).T\ndask_df = dd.from_pandas(df)\n\n\n\n\nIn [39]: dask_df.head()\nOut[39]: \n  Column A Column B Column C\n0      Sam      Sam       12\n1     Alex    David       10\n2    David    David       15\n3    Sarah    Alice       23\n4    Alice      Sam       18\n\n\n\n\n(dask_df.groupby(\'Column B\')\n        .apply(lambda group: len(group))\n       ).compute()\n\nUserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={\'x\': \'f8\', \'y\': \'f8\'}) for dataframe result\n  or:     .apply(func, meta=(\'x\', \'f8\'))            for series result\n  warnings.warn(msg)\nOut[60]: \nColumn B\nAlice    2\nDavid    2\nSam      3\ndtype: int64\n\n\n\n\nTrying to define meta produces AttributeError\n\n (dask_df.groupby(\'Column B\')\n         .apply(lambda d: len(d), meta={\'Column B\': \'int\'})).compute()\n\n\nsame for this\n\n (dask_df.groupby(\'Column B\')\n         .apply(lambda d: len(d), meta=pd.DataFrame({\'Column B\': \'int\'}))).compute()\n\n\nsame if I try having the dtype be int instead of "int" or for that matter \'f8\' or np.float64 so it doesn\'t seem like it\'s the dtype that is causing the problem.\n\nThe documentation on meta seems to imply that I should be doing exactly what I\'m trying to do (http://dask.pydata.org/en/latest/dataframe-design.html#metadata).\n\nWhat is meta? and how am I supposed to define it?\n\nUsing python 3.6 dask 0.14.3 and pandas 0.20.2\n'
"Assume I have a table like below\n\n    A   B   C   B\n0   0   1   2   3\n1   4   5   6   7\n\n\nI'd like to drop column B. I tried to use drop_duplicate, but it seems that it only works based on duplicated data not header.\nHope anyone know how to do this\n\nThanks\n"
"I have a CSV file that is as the following:\n\nindex,Avg,Min,Max\nBuild1,56.19,39.123,60.1039\nBuild2,57.11,40.102,60.2\nBuild3,55.1134,35.129404123,60.20121\n\n\nBased off my question here I am able to add some relevant information to this csv via this short script:\n\nimport pandas as pd\n\ndf = pd.read_csv('newdata.csv')\nprint(df)\n\ndf_out = pd.concat([df.set_index('index'),df.set_index('index').agg(['max','min','mean'])]).rename(index={'max':'Max','min':'Min','mean':'Average'}).reset_index()\n\nwith open('newdata.csv', 'w') as f:\n    df_out.to_csv(f,index=False)\n\n\nThis results in this CSV:\n\nindex,Avg,Min,Max\nBuild1,56.19,39.123,60.1039\nBuild2,57.11,40.102,60.2\nBuild3,55.1134,35.129404123,60.20121\nMax,57.11,40.102,60.20121\nMin,55.1134,35.129404123,60.1039\nAverage,56.1378,38.1181347077,60.16837\n\n\nI would like to now have it so I can update this csv. For example if I ran a new build (build4 for instance) I could add that in and then redo the Max, Min, Average rows. My idea is that I therefore delete the rows with labels Max, Min, Average, add my new row, redo the stats. I believe the code I need is as simple as (just for Max but would have lines for Min and Average as well):\n\ndf = pd.read_csv('newdata.csv')\ndf = df.drop('Max')\n\n\nHowever this always results in an ValueError: labels ['Max'] not contained in axis\n\nI have created the csv files in sublime text, could this be part of the issue? I have read other SO posts about this and none seem to help my issue. \n\nI am unsure if this allowed but here is a download link to my csv just in case something is wrong with the file itself. \n\nI would be okay with two possible answers:\n\n\nHow to fix this drop issue\nHow to add more builds and update the statistics (a method without drop)\n\n"
"I have a large pandas data fame df. It has quite a few missings. Dropping row/or col-wise is not an option. Imputing medians, means or the most frequent values is not an option either (hence imputation with pandas and/or scikit unfortunately doens't do the trick). \n\nI came across what seems to be a neat package called fancyimpute (you can find it here). But I have some problems with it.\n\nHere is what I do: \n\n#the neccesary imports\nimport pandas as pd\nimport numpy as np\nfrom fancyimpute import KNN\n\n# df is my data frame with the missings. I keep only floats\ndf_numeric = = df.select_dtypes(include=[np.float])\n\n# I now run fancyimpute KNN, \n# it returns a np.array which I store as a pandas dataframe\ndf_filled = pd.DataFrame(KNN(3).complete(df_numeric))\n\n\nHowever, df_filled is a single vector somehow, instead of the filled data frame. How do I get a hold of the data frame with imputations?\n\nUpdate\n\nI realized, fancyimpute needs a numpay array. I hence converted the df_numeric to a an array using as_matrix(). \n\n# df is my data frame with the missings. I keep only floats\ndf_numeric = df.select_dtypes(include=[np.float]).as_matrix()\n\n# I now run fancyimpute KNN, \n# it returns a np.array which I store as a pandas dataframe\ndf_filled = pd.DataFrame(KNN(3).complete(df_numeric))\n\n\nThe output is a dataframe with the column labels gone missing. Any way to retrieve the labels?\n"
"Suppose I have two dataframes:\n\n&gt;&gt; df1\n\n   0  1  2\n0  a  b  c\n1  d  e  f\n\n&gt;&gt; df2\n\n   0  1  2\n0  A  B  C\n1  D  E  F\n\n\nHow can I interleave the rows?  i.e. get this:\n\n&gt;&gt; interleaved_df\n\n   0  1  2\n0  a  b  c\n1  A  B  C\n2  d  e  f\n3  D  E  F\n\n\n(Note my real DFs have identical columns, but not the same number of rows).\n\n\n\nWhat I've tried\n\ninspired by this question (very similar, but asks on columns):\n\nimport pandas as pd\nfrom itertools import chain, zip_longest\n\ndf1 = pd.DataFrame([['a','b','c'], ['d','e','f']])  \ndf2 = pd.DataFrame([['A','B','C'], ['D','E','F']])\n\nconcat_df = pd.concat([df1,df2])\n\nnew_index = chain.from_iterable(zip_longest(df1.index, df2.index))\n# new_index now holds the interleaved row indices\n\ninterleaved_df = concat_df.reindex(new_index)\n\nValueError: cannot reindex from a duplicate axis\n\n\nThe last call fails because df1 and df2 have some identical index values (which is also the case with my real DFs). \n\nAny ideas?\n"
'Don\'t really understand is it a mistake or just my local problem, still have some issues with using tqdm progress bars with progress_apply in Jupyter.\n\nFirst try:\n\nfrom tqdm import tqdm\ntqdm_notebook.pandas(desc="Example Desc")\nkeywords_df[\'keyword\'] = keywords_df[\'keywird\'].progress_apply(lambda x: x.replace(\'*\',\'\'))\n\n\nOutput (without any bars):\n\nAttributeError: \'function\' object has no attribute \'pandas\'\n\n\nSecond try:\n\nfrom tqdm import tqdm\ntqdm_notebook().pandas(desc="Example Desc")\nkeywords_df[\'keyword\'] = keywords_df[\'keywird\'].progress_apply(lambda x: x.replace(\'*\',\'\'))\n\n\nOutput:\nTwo bars (need one). First bar is empty (0it [00:00, ?it/s]), second is OK.\n\nAny ideas how to change progress_apply description and display bar without empty initialization bar? :)\n\nP.S.\nDocumentation (https://github.com/tqdm/tqdm) says I can just use tqdm_notebook, but it\'s not working for me :)\n\n# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`\n# (can use `tqdm_gui`, `tqdm_notebook`, optional kwargs, etc.)\ntqdm.pandas(desc="my bar!")\n\n'
'I want to plot two time series on the same plot with same x-axis and secondary y-axis. I have somehow achieved this, but two legends are overlapping and is unable to give label to x-axis and secondary y-axis.I tried putting two legend at upper-left and upper-right, but it is still not working. \n\nCode:\n\nplt.figure(figsize=(12,5))\n\n# Number of request every 10 minutes\nlog_10minutely_count_Series = log_df[\'IP\'].resample(\'10min\').count()\nlog_10minutely_count_Series.name="Count"\nlog_10minutely_count_Series.plot(color=\'blue\', grid=True)\nplt.legend(loc=\'upper left\')\nplt.xlabel(\'Number of request ever 10 minute\')\n\n# Sum of response size over each 10 minute\nlog_10minutely_sum_Series = log_df[\'Bytes\'].resample(\'10min\').sum()\nlog_10minutely_sum_Series.name = \'Sum\'\nlog_10minutely_sum_Series.plot(color=\'red\',grid=True, secondary_y=True)\nplt.legend(loc=\'upper right\')\nplt.show()\n\n\n\n\nThanks in advance\n'
"I have a two column data set depicting multiple child-parent relationships that form a large tree. I would like to use this to build an updated list of every descendant for each node. \n\nOriginal Input:\n\n   child  parent\n1   2010    1000\n7   2100    1000\n5   2110    1000\n3   3000    2110\n2   3011    2010\n4   3033    2100\n0   3102    2010\n6   3111    2110\n\n\nGraphical depiction of relationships:\n\n\n\nExpected output:\n\n    descendant  ancestor\n0         2010      1000\n1         2100      1000\n2         2110      1000\n3         3000      1000\n4         3011      1000\n5         3033      1000\n6         3102      1000\n7         3111      1000\n8         3011      2010\n9         3102      2010\n10        3033      2100\n11        3000      2110\n12        3111      2110\n\n\nOriginally I decided to use a recursive solution with DataFrames. It works as intended, but Pandas is awfully inefficient. My research has led me to believe that an implementation using NumPy arrays (or other simple data structures) would be much faster on large data sets (of 10's of thousands of records).\n\nSolution using data frames:\n\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {\n        'child':     [3102, 2010, 3011, 3000, 3033, 2110, 3111, 2100],\n        'parent':    [2010, 1000, 2010, 2110, 2100, 1000, 2110, 1000]\n    },  columns=['child', 'parent']\n)\n\n\ndef get_ancestry_dataframe_flat(df):\n\n    def get_child_list(parent_id):\n\n        list_of_children = list()\n        list_of_children.append(df[df['parent'] == parent_id]['child'].values)\n\n        for i, r in df[df['parent'] == parent_id].iterrows():\n            if r['child'] != parent_id:\n                list_of_children.append(get_child_list(r['child']))\n\n        # flatten list\n        list_of_children = [item for sublist in list_of_children for item in sublist]\n        return list_of_children\n\n    new_df = pd.DataFrame(columns=['descendant', 'ancestor']).astype(int)\n    for index, row in df.iterrows():\n        temp_df = pd.DataFrame(columns=['descendant', 'ancestor'])\n        temp_df['descendant'] = pd.Series(get_child_list(row['parent']))\n        temp_df['ancestor'] = row['parent']\n        new_df = new_df.append(temp_df)\n\n    new_df = new_df\\\n        .drop_duplicates()\\\n        .sort_values(['ancestor', 'descendant'])\\\n        .reset_index(drop=True)\n\n    return new_df\n\n\nBecause using pandas DataFrames in this way is very inefficient on large data sets, I need to improve the performance of this operation. My understanding is that this can be done by using more efficient data structures better suited for looping and recursion. I want to perform this same operation in the most efficient way possible. \n\nSpecifically, I'm asking for optimization of speed.\n"
"Lately I'm constantly finding myself asking questions in Pandas which depend on data that I'm using , so far it takes me quite a while to create a data frame with similarity to my data (reproducible data frame) so that SO users could easily copy it to their machine.\n\nI would prefer to find a convenient way so i could just print my small DF within my question, and other users could easily collect it, hence creating it with minimum effort.\n\nIn R I'm used to print a small sample of my data within the dput function in the console, and then printing the output within my question (example):\nGetting the error &quot;level sets of factors are different&quot; when running a for loop\n\nI've noticed this explanation, but i don't think its suitable for printing a sample of data for other SO users:\nPython&#39;s equivalent for R&#39;s dput() function\n\nIs there an equivalent method in Pandas for doing that?\n\nThanks in advance!\n"
"The goal here is to create a grouped bar plot, not subplots like the image below\n\nIs there a simple way to create a grouped bar plot in Python? Right now I get separate bar plots, instead of separate bars on one plot.\n\ndf = pd.DataFrame([['g1','c1',10],['g1','c2',12],['g1','c3',13],['g2','c1',8],['g2','c2',10],['g2','c3',12]],columns=['group','column','val'])\n\n%matplotlib inline\ndf.groupby(['group']).plot(kind='bar')\n\n\n\n"
"\n\nI have a data set made of 22 categorical variables (non-ordered). I would like to visualize their correlation in a nice heatmap. Since the Pandas built-in function\n\nDataFrame.corr(method='pearson', min_periods=1)\n\n\nonly implement correlation coefficients for numerical variables (Pearson, Kendall, Spearman), I have to aggregate it myself to perform a chi-square or something like it and I am not quite sure which function use to do it in one elegant step (rather than iterating through all the cat1*cat2 pairs). To be clear, this is what I would like to end up with (a dataframe):  \n\n         cat1  cat2  cat3  \n  cat1|  coef  coef  coef  \n  cat2|  coef  coef  coef\n  cat3|  coef  coef  coef\n\n\nAny ideas with pd.pivot_table or something in the same vein?\n\nthanks in advance\nD.\n"
"I have a package that uses pandas Panels to generate MultiIndex pandas DataFrames.  However, whenever I use pandas.Panel, I get the following DeprecationError:\n\n\n  DeprecationWarning: \n  Panel is deprecated and will be removed in a future version.\n  The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method.\n  Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n      Pandas provides a .to_xarray() method to help automate this conversion.\n\n\nHowever, I can't understand what the first recommendation here is actually recommending in order to create MultiIndex DataFrames.  If Panel is going to be removed, how am I going to be able to use Panel.to_frame?\n\n\n\nTo clarify: I am not asking what deprecation is, or how to convert my Panels to DataFrames.  What I am asking is, if I am using pandas.Panel and then pandas.Panel.to_frame in a library to create MultiIndex DataFrames from 3D ndarrays, and Panels are going to be deprecated, then what is the best option for making those DataFrames without using the Panel API?\n\nEg, if I'm doing the following, with X as a ndarray with shape (N,J,K):\n\np = pd.Panel(X, items=item_names, major_axis=names0, minor_axis=names1)\ndf = p.to_frame()\n\n\nthis is clearly no longer a viable future-proof option for DataFrame construction, though it was the recommended method in this question.\n"
'There is a way to shift a dataframe column dependently on the condition on two other columns? something like:\n\ndf["cumulated_closed_value"] = df.groupby("user").[\'close_cumsum\'].shiftWhile(df[\'close_time\']&gt;df[\'open_time])\n\n\nI have figured out a way to do this but it\'s inefficient:\n\n1)Load data and create the column to shift \n\ndf=pd.read_csv(\'data.csv\')\ndf.sort_values([\'user\',\'close_time\'],inplace=True)\ndf[\'close_cumsum\']=df.groupby(\'user\')[\'value\'].cumsum()\ndf.sort_values([\'user\',\'open_time\'],inplace=True)\nprint(df)\n\n\noutput:\n\n   user  open_time close_time  value  close_cumsum\n0     1 2017-01-01 2017-03-01      5            18\n1     1 2017-01-02 2017-02-01      6             6\n2     1 2017-02-03 2017-02-05      7            13\n3     1 2017-02-07 2017-04-01      3            21\n4     1 2017-09-07 2017-09-11      1            22\n5     2 2018-01-01 2018-02-01     15            15\n6     2 2018-03-01 2018-04-01      3            18\n\n\n2) shift the column with a self-join and some filters\n\nSelf-join (this is memory inefficient)   df2=pd.merge(df[[\'user\',\'open_time\']],df[[\'user\',\'close_time\',\'close_cumsum\']], on=\'user\')\n\nfilter for \'close_time\' &lt; \'open_time\'. Then get the row with the max close_time\n\ndf2=df2[df2[\'close_time\']&lt;df2[\'open_time\']]\nidx = df2.groupby([\'user\',\'open_time\'])[\'close_time\'].transform(max) == df2[\'close_time\']\ndf2=df2[idx]\n\n\n3)merge with the original dataset:\n\ndf3=pd.merge(df[[\'user\',\'open_time\',\'close_time\',\'value\']],df2[[\'user\',\'open_time\',\'close_cumsum\']],how=\'left\')\nprint(df3)\n\n\noutput:\n\n   user  open_time close_time  value  close_cumsum\n0     1 2017-01-01 2017-03-01      5           NaN\n1     1 2017-01-02 2017-02-01      6           NaN\n2     1 2017-02-03 2017-02-05      7           6.0\n3     1 2017-02-07 2017-04-01      3          13.0\n4     1 2017-09-07 2017-09-11      1          21.0\n5     2 2018-01-01 2018-02-01     15           NaN\n6     2 2018-03-01 2018-04-01      3          15.0\n\n\nThere is a more pandas way to get the same result?\n\nEdit: I have added one data line to make the case more clear.\nMy goal is to get the sum of all transactions closed before the opening time of the new transaction\n'
"I was confused by this, which is very simple but I didn't immediately find the answer on StackOverflow: \n\n\ndf.set_index('xcol') makes the column 'xcol' become the index (when it is a column of df).\ndf.reindex(myList), however, takes indexes from outside the dataframe, for example, from a list named myList that we defined somewhere else.\n\n\nI hope this post clarifies it! Additions to this post are also welcome!\n"
'I am trying to convert JSON to CSV file, that I can use for further analysis. Issue with my structure is that I have quite some nested dict/lists when I convert my JSON file.\n\nI tried to use pandas json_normalize(), but it only flattens first level.\n\nimport json\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nfrom cs import CloudStack\n\napi_key = xxxx\nsecret = xxxx\nendpoint = xxxx\n\ncs = CloudStack(endpoint=endpoint,\n                key=api_key,\n                secret=secret)\n\nvirtual_machines = cs.virtMach()\n\ntest = json_normalize(virtual_machines["virtualmachine"])\n\ntest.to_csv("test.csv", sep="|", index=False)\n\n\nAny idea how to flatter whole JSON file, so I can create single line input to CSV file for single (in this case virtual machine) entry? I have tried couple of solutions posted here, but my result was always only first level was flattened.\n\nThis is sample JSON (in this case, I still get "securitygroup" and "nic" output as JSON format:\n\n{\n    "count": 13,\n    "virtualmachine": [\n        {\n            "id": "1082e2ed-ff66-40b1-a41b-26061afd4a0b",\n            "name": "test-2",\n            "displayname": "test-2",\n            "securitygroup": [\n                {\n                    "id": "9e649fbc-3e64-4395-9629-5e1215b34e58",\n                    "name": "test",\n                    "tags": []\n                }\n            ],\n            "nic": [\n                {\n                    "id": "79568b14-b377-4d4f-b024-87dc22492b8e",\n                    "networkid": "05c0e278-7ab4-4a6d-aa9c-3158620b6471"\n                },\n                {\n                    "id": "3d7f2818-1f19-46e7-aa98-956526c5b1ad",\n                    "networkid": "b4648cfd-0795-43fc-9e50-6ee9ddefc5bd"\n                    "traffictype": "Guest"\n                }\n            ],\n            "hypervisor": "KVM",\n            "affinitygroup": [],\n            "isdynamicallyscalable": false\n        }\n    ]\n}\n\n\nThank you and best regards,\nBostjan\n'
'In a Pandas DataFrame, I want to create a new column conditionally based on the value of another column. In my application, the DataFrame typically has a few million lines, and the number of unique conditional values is small, on the order of unity. Performance is extremely important: what is the fastest way to generate the new column? \n\nI created an example case below, and tried and compared different methods already. \nIn the example, the conditional filling is represented by a \ndictionary lookup based on the value of the column label  (here: one of 1, 2, 3). \n\nlookup_dict = {\n    1: 100,   # arbitrary\n    2: 200,   # arbitrary\n    3: 300,   # arbitrary\n    }\n\n\nI then expect my DataFrame to be filled as:\n\n       label  output\n0      3     300\n1      2     200\n2      3     300\n3      3     300\n4      2     200\n5      2     200\n6      1     100\n7      1     100\n\n\n\n\nBelow are 6 different methods tested on 10M lines (parameter Nlines in the test code):\n\n\nmethod 1: pandas.groupby().apply()\nmethod 2: pandas.groupby().indices.items()\nmethod 3: pandas.Series.map\nmethod 4: for loop on labels\nmethod 5: numpy.select\nmethod 6: numba \n\n\nThe full code is available at the end of the answer, with the runtimes of all methods. The output of every method is asserted to be equal before performances are compared. \n\nmethod 1: pandas.groupby().apply()\n\nI use pandas.groupby() on the label, then fills each block with the same value using apply().\n\ndef fill_output(r):\n    \'\'\' called by groupby().apply(): all r.label values are the same \'\'\'\n    r.loc[:, \'output\'] = lookup_dict[r.iloc[0][\'label\']]\n    return r\n\ndf = df.groupby(\'label\').apply(fill_output)\n\n\nI get \n\n&gt;&gt;&gt; method_1_groupby ran in 2.29s (average over 3 iterations)\n\n\nNote that groupby().apply() is ran twice on the first group to determine which code path to use (see Pandas #2936). This can slow things down for a small number of groups. I tricked the Method 1 can adding a first dummy group, but I didn\'t get much improvement.\n\nmethod 2: pandas.groupby().indices.items()\n\nSecond is a variant: instead of using apply I access the indices directy with groupby().indices.items(). This ends up to be twice as fast as Method 1, and it\'s the method I\'ve used for a long time\n\ndgb = df.groupby(\'label\')\nfor label, idx in dgb.indices.items():\n    df.loc[idx, \'output\'] = lookup_dict[label]\n\n\nGot:\n\nmethod_2_indices ran in 1.21s (average over 3 iterations)\n\n\nmethod 3: pandas.Series.map\n\nI used Pandas.Series.map. \n\ndf[\'output\'] = df.label.map(lookup_dict.get)\n\n\nI had very good results in similar cases where the number of looked up values was comparable with the number of lines. In the present case, map ends up being twice as slow as Method 1. \n\nmethod_3_map ran in 3.07s (average over 3 iterations)\n\nI attribute that to the small number of look up values, but there may just be an issue with the way I implemented it. \n\nmethod 4: for loop on labels\n\nThe 4th method is quite naive: I just loop over all labels and select the matching part of the DataFrame. \n\nfor label, value in lookup_dict.items():\n    df.loc[df.label == label, \'output\'] = value\n\n\nSurprisingly, though, I ended up with much faster results that in the previous cases. I expected the groupby based solutions to be faster than this one, because Pandas has to make three comparisons with df.label == label here. Results prove me wrong: \n\nmethod_4_forloop ran in 0.54s (average over 3 iterations)\n\n\nmethod 5: numpy.select\n\nFifth method uses the numpy select function, based on this StackOverflow answer.\n\nconditions = [df.label == k for k in lookup_dict.keys()]\nchoices = list(lookup_dict.values())\n\ndf[\'output\'] = np.select(conditions, choices)\n\n\nThis yields the best results:\n\nmethod_5_select ran in 0.29s (average over 3 iterations)\n\n\nEventually, I tried a numba approach in Method 6. \n\nmethod 6: numba\n\nJust for the sake of the example, the conditional filling values are hardcode in the compiled function. I don\'t know how to give Numba a list as a runtime constant:\n\n@jit(int64[:](int64[:]), nopython=True)\ndef hardcoded_conditional_filling(column):\n    output = np.zeros_like(column)\n    i = 0\n    for c in column:\n        if c == 1:\n            output[i] = 100\n        elif c == 2:\n            output[i] = 200\n        elif c == 3:\n            output[i] = 300\n        i += 1\n    return output\n\ndf[\'output\'] = hardcoded_conditional_filling(df.label.values)\n\n\nI ended up with the best time, faster than Method 5 by 50%. \n\nmethod_6_numba ran in 0.19s (average over 3 iterations)\n\n\nI haven\'t implemented this one for the reason stated above: I don\'t know how to give Numba a list as a runtime constant without a major drop in performances. \n\n\n\nFull code\n\nimport pandas as pd\nimport numpy as np\nfrom timeit import timeit\nfrom numba import jit, int64\n\nlookup_dict = {\n        1: 100,   # arbitrary\n        2: 200,   # arbitrary\n        3: 300,   # arbitrary\n        }\n\nNlines = int(1e7)\n\n# Generate \nlabel = np.round(np.random.rand(Nlines)*2+1).astype(np.int64)\ndf0 = pd.DataFrame(label, columns=[\'label\'])\n\n# Now the goal is to assign the look_up_dict values to a new column \'output\' \n# based on the value of label\n\n# Method 1\n# using groupby().apply()\n\ndef method_1_groupby(df):\n\n    def fill_output(r):\n        \'\'\' called by groupby().apply(): all r.label values are the same \'\'\'\n        #print(r.iloc[0][\'label\'])   # activate to reveal the #2936 issue in Pandas\n        r.loc[:, \'output\'] = lookup_dict[r.iloc[0][\'label\']]\n        return r\n\n    df = df.groupby(\'label\').apply(fill_output)\n    return df \n\ndef method_2_indices(df):\n\n    dgb = df.groupby(\'label\')\n    for label, idx in dgb.indices.items():\n        df.loc[idx, \'output\'] = lookup_dict[label]\n\n    return df\n\ndef method_3_map(df):\n\n    df[\'output\'] = df.label.map(lookup_dict.get)\n\n    return df\n\ndef method_4_forloop(df):\n    \'\'\' naive \'\'\'\n\n    for label, value in lookup_dict.items():\n        df.loc[df.label == label, \'output\'] = value\n\n    return df\n\ndef method_5_select(df):\n    \'\'\' Based on answer from \n    https://stackoverflow.com/a/19913845/5622825\n    \'\'\'\n\n    conditions = [df.label == k for k in lookup_dict.keys()]\n    choices = list(lookup_dict.values())\n\n    df[\'output\'] = np.select(conditions, choices)\n\n    return df\n\ndef method_6_numba(df):\n    \'\'\' This works, but it is hardcoded and i don\'t really know how\n    to make it compile with list as runtime constants\'\'\'\n\n\n    @jit(int64[:](int64[:]), nopython=True)\n    def hardcoded_conditional_filling(column):\n        output = np.zeros_like(column)\n        i = 0\n        for c in column:\n            if c == 1:\n                output[i] = 100\n            elif c == 2:\n                output[i] = 200\n            elif c == 3:\n                output[i] = 300\n            i += 1\n        return output\n\n    df[\'output\'] = hardcoded_conditional_filling(df.label.values)\n\n    return df\n\ndf1 = method_1_groupby(df0)\ndf2 = method_2_indices(df0.copy())\ndf3 = method_3_map(df0.copy())\ndf4 = method_4_forloop(df0.copy())\ndf5 = method_5_select(df0.copy())\ndf6 = method_6_numba(df0.copy())\n\n# make sure we havent modified the input (would bias the results)\nassert \'output\' not in df0.columns \n\n# Test validity\nassert (df1 == df2).all().all()\nassert (df1 == df3).all().all()\nassert (df1 == df4).all().all()\nassert (df1 == df5).all().all()\nassert (df1 == df6).all().all()\n\n# Compare performances\nNites = 3\nprint(\'Compare performances for {0:.1g} lines\'.format(Nlines))\nprint(\'-\'*30)\nfor method in [\n               \'method_1_groupby\', \'method_2_indices\', \n               \'method_3_map\', \'method_4_forloop\', \n               \'method_5_select\', \'method_6_numba\']:\n    print(\'{0} ran in {1:.2f}s (average over {2} iterations)\'.format(\n            method, \n            timeit("{0}(df)".format(method), setup="from __main__ import df0, {0}; df=df0.copy()".format(method), number=Nites)/Nites,\n            Nites))\n\n\nOutput:\n\nCompare performances for 1e+07 lines\n------------------------------\nmethod_1_groupby ran in 2.29s (average over 3 iterations)\nmethod_2_indices ran in 1.21s (average over 3 iterations)\nmethod_3_map ran in 3.07s (average over 3 iterations)\nmethod_4_forloop ran in 0.54s (average over 3 iterations)\nmethod_5_select ran in 0.29s (average over 3 iterations)\nmethod_6_numba ran in 0.19s (average over 3 iterations)\n\n\n\n\nI\'d be interested in any other solution that could yield better performances. \nI was originally looking for Pandas based methods, but I accept numba/cython based solutions too.\n\n\n\nEdit\n\nAdding Chrisb\'s methods for comparison:\n\ndef method_3b_mapdirect(df):\n    \'\'\' Suggested by https://stackoverflow.com/a/51388828/5622825\'\'\'\n\n    df[\'output\'] = df.label.map(lookup_dict)\n\n    return df\n\ndef method_7_take(df):\n    \'\'\' Based on answer from \n    https://stackoverflow.com/a/19913845/5622825\n\n    Exploiting that labels are continuous integers\n    \'\'\'\n\n    lookup_arr = np.array(list(lookup_dict.values()))\n    df[\'output\'] = lookup_arr.take(df[\'label\'] - 1)\n\n    return df\n\n\nWith runtimes of:\n\nmethod_3_mapdirect ran in 0.23s (average over 3 iterations)\nmethod_7_take ran in 0.11s (average over 3 iterations)\n\n\nWhich makes #3 faster than any other method (#6 aside), and the most elegant too. Use #7 if your user case is compatible. \n'
"I want to call a function from a class in which I want to plot several figures.\nThere is no error thrown but I did not receive the plot but only:\n\n#############################################\nHistograms of the continuous data:\n#############################################\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n&lt;Figure size 640x480 with 1 Axes&gt;\n\n\nThe code I use is:\n\nclass Pipeline:\n    import matplotlib.pyplot as plt\n    global plt\n    from matplotlib import style\n    style.use('ggplot')  \n\n\n    def __init__(self,goal):\n        self.goal = goal\n\n\n    def examine(self,dataset):\n        # Check for categorical and continous data\n        continuous = []\n        categorical = []\n        for n,i in enumerate(dataset.columns):\n            if isinstance(dataset[i][1],str):\n                categorical.append(dataset.columns[n])\n            else:\n                continuous.append(dataset.columns[n])\n\n        continuous_data = dataset[continuous]\n        categorical_data = dataset[categorical]\n\n        #Plot the histograms of the continuous data\n        print('#############################################')\n        print('Histograms of the continuous data:')\n        print('#############################################')\n\n        for col in continuous_data.columns:\n            fig = plt.figure()\n            ax = continuous_data[col].hist()\n            ax.set_title(col)\n            plt.show()\n\n\n\n\n\n\n\npipe = Pipeline('C')\npipe.examine(data)\n\n\nI wonder because if I run the same code a second time it plots the figures just as proposed.\nAppreciate any help!\n"
"I would like to get only horizontal grid using pandas plot. \n\nThe integrated parameter of pandas only has grid=True or grid=False, so I tried with matplotlib pyplot, changing the axes parameters, specifically with this code:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax2 = plt.subplot()\nax2.grid(axis='x')\ndf.plot(kind='bar',ax=ax2, fontsize=10, sort_columns=True)\nplt.show(fig)\n\n\nBut I get no grid, neither horizontal nor vertical. Is Pandas overwriting the axes? Or am I doing something wrong?\n"
'I have one column containing all the data which looks something like this (values that need to be separated have a mark like (c)):\n\nUK (c)\nLondon\nWales\nLiverpool\nUS (c)\nChicago\nNew York\nSan Francisco\nSeattle\nAustralia (c)\nSydney\nPerth\n\n\nAnd I want it split into two columns looking like this:\n\nLondon          UK\nWales           UK\nLiverpool       UK\nChicago         US\nNew York        US\nSan Francisco   US\nSeattle         US\nSydney          Australia\nPerth           Australia\n\n\nQuestion 2: What if the countries did not have a pattern like (c)?\n'
'For my application, I need to read multiple files with 15 M lines each, store them in a DataFrame, and save the DataFrame in HDFS5 format. \n\nI\'ve already tried different approaches, notably pandas.read_csv with chunksize and dtype specifications, and dask.dataframe. They both take around 90 seconds to treat 1 file, and so I\'d like to know if there\'s a way to efficiently treat these files in the described way. In the following, I show some code of the tests I\'ve done.\n\nimport pandas as pd\nimport dask.dataframe as dd\nimport numpy as np\nimport re \n\n# First approach\nstore = pd.HDFStore(\'files_DFs.h5\')\n\nchunk_size = 1e6\n\ndf_chunk = pd.read_csv(file,\n                sep="\\t",\n                chunksize=chunk_size,\n                usecols=[\'a\', \'b\'],\n                converters={"a": lambda x: np.float32(re.sub(r"[^\\d.]", "", x)),\\\n                            "b": lambda x: np.float32(re.sub(r"[^\\d.]", "", x))},\n                skiprows=15\n           )              \nchunk_list = [] \n\n\nfor chunk in df_chunk:\n      chunk_list.append(chunk)\n\n\ndf = pd.concat(chunk_list, ignore_index=True)\n\nstore[dfname] = df\nstore.close()\n\n# Second approach\n\ndf = dd.read_csv(\n        file,\n        sep="\\t",\n        usecols=[\'a\', \'b\'],\n        converters={"a": lambda x: np.float32(re.sub(r"[^\\d.]", "", x)),\\\n                    "b": lambda x: np.float32(re.sub(r"[^\\d.]", "", x))},\n        skiprows=15\n     )\nstore.put(dfname, df.compute())\nstore.close()\n\n\nHere is what the files look like (whitespace consists of a literal tab):\n\na   b\n599.998413  14.142895\n599.998413  20.105534\n599.998413  6.553850\n599.998474  27.116098\n599.998474  13.060312\n599.998474  13.766775\n599.998596  1.826706\n599.998596  18.275938\n599.998718  20.797491\n599.998718  6.132450)\n599.998718  41.646194\n599.998779  19.145775\n\n'
"When having a Pandas DataFrame like this: \n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'today': [['a', 'b', 'c'], ['a', 'b'], ['b']], \n                   'yesterday': [['a', 'b'], ['a'], ['a']]})\n\n\n                 today        yesterday\n0      ['a', 'b', 'c']       ['a', 'b']\n1           ['a', 'b']            ['a']\n2                ['b']            ['a']                          \n... etc\n\n\nBut with about 100 000 entries, I am looking to find the additions and removals of those lists in the two columns on a row-wise basis.\n\nIt is comparable to this question: Pandas: How to Compare Columns of Lists Row-wise in a DataFrame with Pandas (not for loop)?  but I am looking at the differences, and Pandas.apply method seems not to be that fast for such many entries.\nThis is the code that I am currently using. Pandas.apply with numpy's setdiff1d method:\n\nadditions = df.apply(lambda row: np.setdiff1d(row.today, row.yesterday), axis=1)\nremovals  = df.apply(lambda row: np.setdiff1d(row.yesterday, row.today), axis=1)\n\n\nThis works fine, however it takes about a minute for 120 000 entries. So is there a faster way to accomplish this?\n"
"DataFrame I have:\n\n            A   B   C \n2012-01-01  1   2   3 \n2012-01-05  4   5   6 \n2012-01-10  7   8   9 \n2012-01-15  10  11  12 \n\n\nWhat I am using now:\n\ndate_after = dt.datetime( 2012, 1, 7 )\nframe.ix[date_after:].ix[0:1]\nOut[1]: \n            A  B  C\n2012-01-10  7  8  9\n\n\nIs there any better way of doing this?  I do not like that I have to specify .ix[0:1] instead of .ix[0], but if I don't the output changes to a TimeSeries instead of a single row in a DataFrame.  I find it harder to work with a rotated TimeSeries back on top of the original DataFrame.\n\nWithout .ix[0:1]:\n\nframe.ix[date_after:].ix[0]\nOut[1]: \nA    7\nB    8\nC    9\nName: 2012-01-10 00:00:00\n\n\nThanks,\n\nJohn\n"
'I\'d like to filter out weekend data and only look at data for weekdays (mon(0)-fri(4)).  I\'m new to pandas, what\'s the best way to accomplish this in pandas?\n\nimport datetime\nfrom pandas import *\n\ndata = read_csv("data.csv")\ndata.my_dt \n\nOut[52]:\n0     2012-10-01 02:00:39\n1     2012-10-01 02:00:38\n2     2012-10-01 02:01:05\n3     2012-10-01 02:01:07\n4     2012-10-01 02:02:03\n5     2012-10-01 02:02:09\n6     2012-10-01 02:02:03\n7     2012-10-01 02:02:35\n8     2012-10-01 02:02:33\n9     2012-10-01 02:03:01\n10    2012-10-01 02:08:53\n11    2012-10-01 02:09:04\n12    2012-10-01 02:09:09\n13    2012-10-01 02:10:20\n14    2012-10-01 02:10:45\n...\n\n\nI\'d like to do something like:\n\nweekdays_only = data[data.my_dt.weekday() &lt; 5]\n\n\nAttributeError: \'numpy.int64\' object has no attribute \'weekday\'\n\nbut this doesn\'t work, I haven\'t quite grasped how column datetime objects are accessed.\n\nThe eventual goal being to arrange hierarchically to weekday hour-range, something like:\n\nmonday, 0-6, 7-12, 13-18, 19-23\ntuesday, 0-6, 7-12, 13-18, 19-23\n\n'
"Is it possible to have stdin data go into a pandas DataFrame?\n\nCurrently I'm saving the data in an intermediate json file and then doing:\n\npandas.read_json('my_json_file.json')\n\n\nbut was wondering if it's possible to pipe the stdin directly in the python script.\nI found this: How to read from stdin or from a file if no data is piped in Python?\nbut not sure how to do a line-by-line insertion in a pandas DF.\n"
"I have a dataframe which Im loading from a csv file and then setting the index to few of its columns (usually two or three) by the set_index method. The idea is to then access parts of the dataframe using several key combination, as such:\n\ndf.set_index(['fileName','phrase'])\ndf.ix['somePath','somePhrase']\n\n\nApparently, this type of selection with multiple keys is only possible if the MultiIndex of the dataframe is sorted to sufficient depth. In this case, since im supplying two keys, the .ix operation will not fail only if the dataframe MultiIndex is sorted to depth of at least 2. \n\nfor some reason, when Im setting the index as shown, while to me it seems both layers are sorted, calling  df.index.lexsort_depth command returns 1 , and I get the following error when trying to access with two keys:\n\n\n  MultiIndex lexsort depth 1, key was length 2\n\n\nAny help?\n"
'I have been reading this link on "Returning a view versus a copy". I do not really get how the chained assignment concept in Pandas works and how the usage of .ix(), .iloc(), or .loc() affects it.\n\nI get the SettingWithCopyWarning warnings for the following lines of codes, where data is a Panda dataframe and amount is a column (Series) name in that dataframe:\n\ndata[\'amount\'] = data[\'amount\'].astype(float)\n\ndata["amount"].fillna(data.groupby("num")["amount"].transform("mean"), inplace=True)\n\ndata["amount"].fillna(mean_avg, inplace=True)\n\n\nLooking at this code, is it obvious that I am doing something suboptimal? If so, can you let me know the replacement code lines?\n\nI am aware of the below warning and like to think that the warnings in my case are false positives: \n\n\n  The chained assignment warnings / exceptions are aiming to inform the\n  user of a possibly invalid assignment. There may be false positives;\n  situations where a chained assignment is inadvertantly reported.\n\n\nEDIT : the code leading to the first copy warning error.\n\ndata[\'amount\'] = data.apply(lambda row: function1(row,date,qty), axis=1) \ndata[\'amount\'] = data[\'amount\'].astype(float)\n\ndef function1(row,date,qty):\n    try:\n        if(row[\'currency\'] == \'A\'):\n            result = row[qty]\n        else:\n            rate = lookup[lookup[\'Date\']==row[date]][row[\'currency\'] ]\n            result = float(rate) * float(row[qty])\n        return result\n    except ValueError: # generic exception clause\n        print "The current row causes an exception:"\n\n'
'Say I create a fully random Dataframe using the following:\n\nfrom pandas.util import testing\nfrom random import randrange\n\ndef random_date(start, end):\n    delta = end - start\n    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n    random_second = randrange(int_delta)\n    return start + timedelta(seconds=random_second)\n\ndef rand_dataframe():\n  df = testing.makeDataFrame()\n  df[\'date\'] = [random_date(datetime.date(2014,3,18),datetime.date(2014,4,1)) for x in xrange(df.shape[0])]\n  df.sort(columns=[\'date\'], inplace=True)      \n  return df\n\ndf = rand_dataframe()\n\n\nwhich results in the dataframe shown at the bottom of this post. I would like to plot my columns A, B, C and D using the timeseries visualization features in seaborn so that I get something along these lines:\n\n\n\nHow can I approach this problem? From what I read on this notebook, the call should be:\n\nsns.tsplot(df, time="time", unit="unit", condition="condition", value="value")\n\n\nbut this seems to require that the dataframe is represented in a different way, with the columns somehow encoding time, unit, condition and value, which is not my case. How can I convert my dataframe (shown below) into this format?\n\nHere is my dataframe:\n\n      date         A         B         C         D\n\n2014-03-18  1.223777  0.356887  1.201624  1.968612\n2014-03-18  0.160730  1.888415  0.306334  0.203939\n2014-03-18 -0.203101 -0.161298  2.426540  0.056791\n2014-03-18 -1.350102  0.990093  0.495406  0.036215\n2014-03-18 -1.862960  2.673009 -0.545336 -0.925385\n2014-03-19  0.238281  0.468102 -0.150869  0.955069\n2014-03-20  1.575317  0.811892  0.198165  1.117805\n2014-03-20  0.822698 -0.398840 -1.277511  0.811691\n2014-03-20  2.143201 -0.827853 -0.989221  1.088297\n2014-03-20  0.299331  1.144311 -0.387854  0.209612\n2014-03-20  1.284111 -0.470287 -0.172949 -0.792020\n2014-03-22  1.031994  1.059394  0.037627  0.101246\n2014-03-22  0.889149  0.724618  0.459405  1.023127\n2014-03-23 -1.136320 -0.396265 -1.833737  1.478656\n2014-03-23 -0.740400 -0.644395 -1.221330  0.321805\n2014-03-23 -0.443021 -0.172013  0.020392 -2.368532\n2014-03-23  1.063545  0.039607  1.673722  1.707222\n2014-03-24  0.865192 -0.036810 -1.162648  0.947431\n2014-03-24 -1.671451  0.979238 -0.701093 -1.204192\n2014-03-26 -1.903534 -1.550349  0.267547 -0.585541\n2014-03-27  2.515671 -0.271228 -1.993744 -0.671797\n2014-03-27  1.728133 -0.423410 -0.620908  1.430503\n2014-03-28 -1.446037 -0.229452 -0.996486  0.120554\n2014-03-28 -0.664443 -0.665207  0.512771  0.066071\n2014-03-29 -1.093379 -0.936449 -0.930999  0.389743\n2014-03-29  1.205712 -0.356070 -0.595944  0.702238\n2014-03-29 -1.069506  0.358093  1.217409 -2.286798\n2014-03-29  2.441311  1.391739 -0.838139  0.226026\n2014-03-31  1.471447 -0.987615  0.201999  1.228070\n2014-03-31 -0.050524  0.539846  0.133359 -0.833252\n\n\nIn the end, what I am looking for is an overlay of of plots (one per column), where each of them looks as follows (note that different values of CI get different values of alphas):\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \n'
'I have the following Pandas data frame:\n\nprint(df)\n\n     head1  head2  head3\nbar     32      3    100\nbix     22    NaN    NaN\nfoo     11      1    NaN\nqux    NaN     10    NaN\nxoo    NaN      2     20\n\n\nWhat I want to do is to convert the row names bar, bix, ... into columns such that \nin the end I have something like this:\n\n    newhead     head1  head2  head3\n0   bar         32      3    100\n1   bix         22    NaN    NaN\n2   foo         11      1    NaN\n3   qux         NaN    10    NaN\n4   xoo         NaN     2     20\n\n\nHow can I achieve that?\n'
"Say I have a Pandas DataFrame and I want to obtain a list of tuples of the form [(index1, column1), (index2, column2) ...] describing the locations of all elements of the DataFrame where some condition is true. For example:\n\nx = pd.DataFrame(np.random.normal(0, 1, (4,4)), index=['a', 'b', 'c', 'd'],\n                 columns=['e', 'f', 'g', 'h'])\nx\n\n\n     e           f           g           h\na   -1.342571   -0.274879   -0.903354   -1.458702\nb   -1.521502   -1.135800   -1.147913   1.829485\nc   -1.199857   0.458135    -1.993701   -0.878301\nd   0.485599    0.286608    -0.436289   -0.390755\n\ny = x &gt; 0\n\n\nIs there any way to obtain:\n\nx.loc[y]\n\n\nTo return:\n\n[(b, h), (c,f), (d, e), (d,f)]\n\n\nOr some equivalent? Evidently, I can do:\n\npostup = []\nfor i in x.index:\n    for j in x.columns:\n        if x.loc[i, j] &gt; 0:\n            postup.append((i, j))\n\n\nBut I imagine something better may be possible / already implemented. In matlab the function find combined with sub2ind do the job.\n"
"I have a dataframe of individuals who each have multiple records.  I want to enumerate the record in the sequence for each individual in python.  Essentially I would like to create the 'sequence' column in the following table:\n\npatient  date      sequence\n145      20Jun2009        1\n145      24Jun2009        2\n145      15Jul2009        3\n582      09Feb2008        1\n582      21Feb2008        2\n987      14Mar2010        1\n987      02May2010        2\n987      12May2010        3\n\n\nThis is essentially the same question as here, but I am working in python and unable to implement the sql solution. I suspect I can use a groupby statement with an iterable count, but have so far been unsuccessful. Thanks!\n"
'Ever so often I get this warning when parsing data files:\n\nWARNING:py.warnings:/usr/local/python3/miniconda/lib/python3.4/site-\npackages/pandas-0.16.0_12_gdcc7431-py3.4-linux-x86_64.egg/pandas\n/io/parsers.py:1164: DtypeWarning: Columns (0,2,14,20) have mixed types. \nSpecify dtype option on import or set low_memory=False.\n          data = self._reader.read(nrows)\n\n\nBut if the data is large (I have 50k rows), how can I find WHERE in the data the change of dtype occurs?\n'
"I have two dataframes in Pandas which are being merged together df.A and df.B, df.A is the original, and df.B has the new data I want to bring over.  The merge works fine and as expected I get two columns col_x and col_y in the merged df.\n\nHowever, in some rows, the original df.A has values where the other df.B does not.  My question is, how can I selectively take the values from col_x and col_y and place them into a new col such as col_z ?\n\nHere's what I mean, how can I merge df.A:\n\ndate   impressions    spend    col\n1/1/15 100000         3.00     ABC123456\n1/2/15 145000         5.00     ABCD00000\n1/3/15 300000         15.00    (null)\n\n\nwith df.B\n\ndate    col\n1/1/15  (null)\n1/2/15  (null)\n1/3/15  DEF123456\n\n\nTo get:\n\ndate   impressions    spend    col_z\n1/1/15 100000         3.00     ABC123456\n1/2/15 145000         5.00     ABCD00000\n1/3/15 300000         15.00    DEF123456\n\n\nAny help or point in the right direction would be really appreciated!\n\nThanks\n"
'I am using the shift method for a data series in pandas \n (documentation).\n\nIs it possible do a cyclic shift, i.e. the first value become the last value, in one step?\n\n&gt;&gt;&gt; input\nOut[20]: \n5     0.995232\n15    0.999794\n25    1.006853\n35    0.997781\n45    0.981553\nName: vRatio, dtype: float64\n\n&gt;&gt;&gt; input.shift()\nOut[21]: \n5          NaN\n15    0.995232\n25    0.999794\n35    1.006853\n45    0.997781\nName: vRatio, dtype: float64\n\n\ndesired output:\n\nOut[21]: \n5     0.981553\n15    0.995232\n25    0.999794\n35    1.006853\n45    0.997781\nName: vRatio, dtype: float64\n\n'
'I have a dataframe as follows: the shape of the frame is (1510, 1399). The columns represents products, the rows represents the values (0 or 1) assigned by an user for a given product. How can I can compute a jaccard_similarity_score?\n\n\n\nI created a placeholder dataframe listing product vs. product \n\ndata_ibs = pd.DataFrame(index=data_g.columns,columns=data_g.columns)\n\n\nI am not sure how to iterate though data_ibs to compute similarities.\n\nfor i in range(0,len(data_ibs.columns)) :\n    # Loop through the columns for each column\n    for j in range(0,len(data_ibs.columns)) :\n.........\n\n'
"as the title suggests, where has the rolling function option in the ols command in Pandas migrated to in statsmodels? I can't seem to find it.\nPandas tells me doom is in the works:\n\nFutureWarning: The pandas.stats.ols module is deprecated and will be removed in a future version. We refer to external packages like statsmodels, see some examples here: http://statsmodels.sourceforge.net/stable/regression.html\n  model = pd.ols(y=series_1, x=mmmm, window=50)\n\n\nin fact, if you do something like:\n\nimport statsmodels.api as sm\n\nmodel = sm.OLS(series_1, mmmm, window=50).fit()\n\nprint(model.summary())\n\n\nyou get results (window does not impair the running of the code) but you get only the parameters of the regression run on the entire period, not the series of parameters for each of the rolling period it should be supposed to work on.\n"
"I work with python-pandas dataframes, and I have a large dataframe containing users and their data. Each user can have multiple rows. I want to sample 1-row per user. \nMy current solution seems not efficient:\n\ndf1 = pd.DataFrame({'User': ['user1', 'user1', 'user2', 'user3', 'user2', 'user3'],\n                 'B': ['B', 'B1', 'B2', 'B3','B4','B5'],\n                 'C': ['C', 'C1', 'C2', 'C3','C4','C5'],\n                 'D': ['D', 'D1', 'D2', 'D3','D4','D5'],\n                 'E': ['E', 'E1', 'E2', 'E3','E4','E5']},\n                 index=[0, 1, 2, 3,4,5])\n\ndf1\n&gt;&gt;  B   C   D   E   User\n0   B   C   D   E   user1\n1   B1  C1  D1  E1  user1\n2   B2  C2  D2  E2  user2\n3   B3  C3  D3  E3  user3\n4   B4  C4  D4  E4  user2\n5   B5  C5  D5  E5  user3\n\nuserList = list(df1.User.unique())\nuserList\n&gt; ['user1', 'user2', 'user3']\n\n\nThe I loop over unique users list and sample one row per user, saving them to a different dataframe\n\nusersSample = pd.DataFrame() # empty dataframe, to save samples\nfor i in userList:\n    usersSample=usersSample.append(df1[df1.User == i].sample(1)) \n\n&gt; usersSample   \nB   C   D   E   User\n0   B   C   D   E   user1\n4   B4  C4  D4  E4  user2\n3   B3  C3  D3  E3  user3\n\n\nIs there a more efficient way of achieving that? I'd really like to:\n1) avoid appending to dataframe usersSample. This is gradually growing object and it seriously kills performance. \nAnd 2) avoid looping over users one at a time. Is there a way to sample 1-per-user more efficiently?\n"
"I am trying to convert one column of my dataframe to datetime. Following the discussion here https://github.com/dask/dask/issues/863 I tried the following code:\n\nimport dask.dataframe as dd\ndf['time'].map_partitions(pd.to_datetime, columns='time').compute()\n\n\nBut I am getting the following error message\n\nValueError: Metadata inference failed, please provide `meta` keyword\n\n\nWhat exactly should I put under meta? should I put a dictionary of ALL the columns in df or only of the 'time' column? and what type should I put? I have tried dtype and datetime64 but none of them work so far.\n\nThank you and I appreciate your guidance,\n\nUpdate\n\nI will include here the new error messages:\n\n1) Using Timestamp\n\ndf['trd_exctn_dt'].map_partitions(pd.Timestamp).compute()\n\nTypeError: Cannot convert input to Timestamp\n\n\n2) Using datetime and meta\n\nmeta = ('time', pd.Timestamp)\ndf['time'].map_partitions(pd.to_datetime,meta=meta).compute()\nTypeError: to_datetime() got an unexpected keyword argument 'meta'\n\n\n3) Just using date time: gets stuck at 2%\n\n    In [14]: df['trd_exctn_dt'].map_partitions(pd.to_datetime).compute()\n[                                        ] | 2% Completed |  2min 20.3s\n\n\nAlso, I would like to be able to specify the format in the date, as i would do in pandas:\n\npd.to_datetime(df['time'], format = '%m%d%Y'\n\n\nUpdate 2\n\nAfter updating to Dask 0.11, I no longer have problems with the meta keyword. Still, I can't get it past 2% on a 2GB dataframe.\n\ndf['trd_exctn_dt'].map_partitions(pd.to_datetime, meta=meta).compute()\n    [                                        ] | 2% Completed |  30min 45.7s\n\n\nUpdate 3\n\nworked better this way:\n\ndef parse_dates(df):\n  return pd.to_datetime(df['time'], format = '%m/%d/%Y')\n\ndf.map_partitions(parse_dates, meta=meta)\n\n\nI'm not sure whether it's the right approach or not\n"
"I have the following data frame\n\n    prod_type\n0   responsive\n1   responsive\n2   respon\n3   r\n4   respon\n5   r\n6   responsive\n\n\nI would like to replace respon and r with responsive, so the final data frame is\n\n    prod_type\n0   responsive\n1   responsive\n2   responsive\n3   responsive\n4   responsive\n5   responsive\n6   responsive\n\n\nI tried the following but it did not work:\n\ndf['prod_type'] = df['prod_type'].replace({'respon' : 'responsvie'}, regex=True)\ndf['prod_type'] = df['prod_type'].replace({'r' : 'responsive'}, regex=True)\n\n"
"I want to extract all unique combinations of values of columns Col1, Col2 and Col3. Let's say there is the following dataframe df:\n\ndf =\n\nCol1    Col2    Col3\n12      AB      13\n11      AB      13\n12      AB      13\n12      AC      14\n\n\nThe answer is:\n\nunique =\n\nCol1    Col2    Col3\n12      AB      13\n11      AB      13\n12      AC      14\n\n\nI know how to obtain unique values of a particular column, i.e. df.Col1.unique(), however not sure about unique combinations.\n"
'I don\'t understand why apply and transform return different dtypes when called on the same data frame. The way I explained the two functions to myself before went something along the lines of "apply collapses the data, and transform does exactly the same thing as apply but preserves the original index and doesn\'t collapse." Consider the following.\n\ndf = pd.DataFrame({\'id\': [1,1,1,2,2,2,2,3,3,4],\n                   \'cat\': [1,1,0,0,1,0,0,0,0,1]})\n\n\nLet\'s identify those ids which have a nonzero entry in the cat column.\n\n&gt;&gt;&gt; df.groupby(\'id\')[\'cat\'].apply(lambda x: (x == 1).any())\nid\n1     True\n2     True\n3    False\n4     True\nName: cat, dtype: bool\n\n\nGreat. If we wanted to create an indicator column, however, we could do the following.\n\n&gt;&gt;&gt; df.groupby(\'id\')[\'cat\'].transform(lambda x: (x == 1).any())\n0    1\n1    1\n2    1\n3    1\n4    1\n5    1\n6    1\n7    0\n8    0\n9    1\nName: cat, dtype: int64\n\n\nI don\'t understand why the dtype is now int64 instead of the boolean returned by the any() function.\n\nWhen I change the original data frame to contain some booleans (note that the zeros remain), the transform approach returns booleans in an object column. This is an extra mystery to me since all of the values are boolean, but it\'s listed as object apparently to match the dtype of the original mixed-type column of integers and booleans.\n\ndf = pd.DataFrame({\'id\': [1,1,1,2,2,2,2,3,3,4],\n                   \'cat\': [True,True,0,0,True,0,0,0,0,True]})\n\n&gt;&gt;&gt; df.groupby(\'id\')[\'cat\'].transform(lambda x: (x == 1).any())\n0     True\n1     True\n2     True\n3     True\n4     True\n5     True\n6     True\n7    False\n8    False\n9     True\nName: cat, dtype: object\n\n\nHowever, when I use all booleans, the transform function returns a boolean column.\n\ndf = pd.DataFrame({\'id\': [1,1,1,2,2,2,2,3,3,4],\n                   \'cat\': [True,True,False,False,True,False,False,False,False,True]})\n\n&gt;&gt;&gt; df.groupby(\'id\')[\'cat\'].transform(lambda x: (x == 1).any())\n0     True\n1     True\n2     True\n3     True\n4     True\n5     True\n6     True\n7    False\n8    False\n9     True\nName: cat, dtype: bool\n\n\nUsing my acute pattern-recognition skills, it appears that the dtype of the resulting column mirrors that of the original column. I would appreciate any hints about why this occurs or what\'s going on under the hood in the transform function. Cheers.\n'
'I am currently using the below code to import 6,000 csv files (with headers) and export them into a single csv file (with a single header row).\n\n#import csv files from folder\npath =r\'data/US/market/merged_data\'\nallFiles = glob.glob(path + "/*.csv")\nstockstats_data = pd.DataFrame()\nlist_ = []\n\nfor file_ in allFiles:\n    df = pd.read_csv(file_,index_col=None,)\n    list_.append(df)\n    stockstats_data = pd.concat(list_)\n    print(file_ + " has been imported.")\n\n\nThis code works fine, but it is slow. It can take up to 2 days to process. \n\nI was given a single line script for Terminal command line that does the same (but with no headers). This script takes 20 seconds.\n\n for f in *.csv; do cat "`pwd`/$f" | tail -n +2 &gt;&gt; merged.csv; done \n\n\nDoes anyone know how I can speed up the first Python script? To cut the time down, I have thought about not importing it into a DataFrame and just concatenating the CSVs, but I cannot figure it out. \n\nThanks.\n'
"Suppose I have two dataframes d1 and d2\n\nd1 = pd.DataFrame(np.ones((3, 3), dtype=int), list('abc'), [0, 1, 2])\nd2 = pd.DataFrame(np.zeros((3, 2), dtype=int), list('abc'), [3, 4])\n\n\n\n\nd1\n\n   0  1  2\na  1  1  1\nb  1  1  1\nc  1  1  1\n\n\n\n\nd2\n\n   3  4\na  0  0\nb  0  0\nc  0  0\n\n\n\n\nWhat is an easy and generalized way to interweave two dataframes' columns.  We can assume that the number of columns in d2 is always one less than the number of columns in d1.  And, the indices are the same.\n\nI want this:\n\npd.concat([d1[0], d2[3], d1[1], d2[4], d1[2]], axis=1)\n\n   0  3  1  4  2\na  1  0  1  0  1\nb  1  0  1  0  1\nc  1  0  1  0  1\n\n"
"I came across something curious (to me) while trying to answer this question.\n\nSay I want to compare a series of shape (10,) to a df of shape (10,10):\n\nnp.random.seed(0)\nmy_ser = pd.Series(np.random.randint(0, 100, size=10))\nmy_df = pd.DataFrame(np.random.randint(0, 100, size=100).reshape(10,10))\nmy_ser &gt; 10 * my_df\n\n\nyields, as expected, a matrix of the shape of the df (10,10). The comparison seems to be row-wise.\n\nHowever consider this case:\n\ndf = pd.DataFrame({'cell1':[0.006209, 0.344955, 0.004521, 0, 0.018931, 0.439725, 0.013195, 0.009045, 0, 0.02614, 0],\n              'cell2':[0.048043, 0.001077, 0,0.010393, 0.031546, 0.287264, 0.016732, 0.030291, 0.016236, 0.310639,0], \n              'cell3':[0,0,0.020238, 0, 0.03811, 0.579348, 0.005906, 0,0,0.068352, 0.030165],\n              'cell4':[0.016139, 0.009359, 0,0,0.025449, 0.47779, 0, 0.01282, 0.005107, 0.004846, 0],\n              'cell5': [0,0,0,0.012075, 0.031668, 0.520258, 0,0,0,2.728218, 0.013418]})\ni = 0\ndf.iloc[:,i].shape\n&gt;(11,)\n(10 * df.drop(df.columns[i], axis=1)).shape\n&gt;(11,4)\n(df.iloc[:,i] &gt; (10 * df.drop(df.columns[i], axis=1))).shape\n&gt;(11,15)\n\n\nAs far as I can tell, here Pandas broadcasts the Series with the df. Why is this?\n\nThe desired behaviour can be gotten with:\n\n(10 * df.drop(df.columns[i], axis=1)).lt(df.iloc[:,i], axis=0).shape\n&gt;(11,4)\n\npd.__version__\n'0.24.0'\n\n"
'I am trying to concat multiple Pandas DataFrame columns with different tokens.\n\nFor example, my dataset looks like this :\n\ndataframe = pd.DataFrame({\'col_1\' : [\'aaa\',\'bbb\',\'ccc\',\'ddd\'], \n                          \'col_2\' : [\'name_aaa\',\'name_bbb\',\'name_ccc\',\'name_ddd\'], \n                          \'col_3\' : [\'job_aaa\',\'job_bbb\',\'job_ccc\',\'job_ddd\']})\n\n\nI want to output something like this:\n\n    features\n0   aaa &lt;0&gt; name_aaa &lt;1&gt; job_aaa\n1   bbb &lt;0&gt; name_bbb &lt;1&gt; job_bbb\n2   ccc &lt;0&gt; name_ccc &lt;1&gt; job_ccc\n3   ddd &lt;0&gt; name_ddd &lt;1&gt; job_ddd\n\n\nExplanation :\n\nconcat each column with "&lt;{}>" where {} will be increasing numbers.\n\nWhat I\'ve tried so far:\n\nI don\'t want to modify original DataFrame so I created two new dataframe:\n\nfeatures_df = pd.DataFrame()\nfinal_df    = pd.DataFrame()\nfor iters in range(len(dataframe.columns)):\n    features_df[dataframe.columns[iters]] = dataframe[dataframe.columns[iters]] + \' \' + "&lt;{}&gt;".format(iters)\nfinal_df[\'features\'] = features_df[features_df.columns].agg(\' \'.join, axis=1)\n\n\nThere is an issue I am facing, It\'s adding &lt;2> at last but I want output like above, also this is not panda\'s way to do this task, How I can make it more efficient?\n'
'I have the following Pandas Dataframe with a MultiIndex(Z,A):\n\n             H1       H2  \n   Z    A \n0  100  200  0.3112   -0.4197   \n1  100  201  0.2967   0.4893    \n2  100  202  0.3084   -0.4873   \n3  100  203  0.3069   NaN        \n4  101  203  -0.4956  NaN       \n\n\nQuestion: How can I select all items with A=203?\nI tried df[:,\'A\']  but it doesn\'t work. Then I found this in the online documentation so I tried:\ndf.xs(203,level=\'A\')\nbut I get:\n"TypeError: xs() got an unexpected keyword argument \'level\'"\nAlso I dont see this parameter in the installed doc(df.xs?):\n"Parameters ---------- key : object Some label contained in the index, or partially in a MultiIndex axis : int, default 0 Axis to retrieve cross-section on copy : boolean, default True Whether to make a copy of the data"\nNote:I have the development version.\n\nEdit: I found this thread. They recommend something like:\n\ndf.select(lambda x: x[1]==200, axis=0)  \n\n\nI still would like to know what happened with df.xs with the level parameter or what is the recommended way in the current version.\n'
"I am trying to create a pandas DataFrame and it works fine for a single file. If I need to build it for multiple files which have the same data structure. So instead of single file name I have a list of file names from which I would like to create the DataFrame.\n\nNot sure what's the way to append to current DataFrame in pandas or is there a way for pandas to suck a list of files into a DataFrame.\n"
"I have a #-separated file with three columns: the first is integer, the second looks like a float, but isn't, and the third is a string.  I attempt to load this directly into python with pandas.read_csv\n\nIn [149]: d = pandas.read_csv('resources/names/fos_names.csv',  sep='#', header=None, names=['int_field', 'floatlike_field', 'str_field'])\n\nIn [150]: d\nOut[150]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1673 entries, 0 to 1672\nData columns:\nint_field          1673  non-null values\nfloatlike_field    1673  non-null values\nstr_field          1673  non-null values\ndtypes: float64(1), int64(1), object(1)\n\n\npandas tries to be smart and automatically convert fields to a useful type.  The issue is that I don't actually want it to do so (if I did, I'd used the converters argument).  How can I prevent pandas from converting types automatically?\n"
'How to implement the composition pattern? I have a class Container which has an attribute object Contained. I would like to  redirect/allow access to all methods of Contained class from Container by simply calling my_container.some_contained_method(). Am I doing the right thing in the right way?\n\nI use something like:\n\nclass Container:\n   def __init__(self):\n       self.contained = Contained()\n   def __getattr__(self, item):\n       if item in self.__dict__: # some overridden\n           return self.__dict__[item] \n       else:\n           return self.contained.__getattr__(item) # redirection\n\n\nBackground:\n\nI am trying to build a class (Indicator) that adds to the functionality of an existing class (pandas.DataFrame). Indicator will have all the methods of DataFrame. I could use inheritance, but I am following the "favor composition over inheritance" advice (see, e.g., the answers in: python: inheriting or composition). One reason not to inherit is because the base class is not serializable and I need to serialize.\n\nI have found this, but I am not sure if it fits my needs.\n'
"Q is similar to this:\nuse a list of values to select rows from a pandas dataframe\n\nI want to dataframe if either value in two columns are in a list.\nReturn both columns (combine results of #1 and #4. \n\nimport numpy as np\nfrom pandas import *\n\n\nd = {'one' : [1., 2., 3., 4] ,'two' : [5., 6., 7., 8.],'three' : [9., 16., 17., 18.]}\n\ndf = DataFrame(d)\nprint df\n\ncheckList = [1,7]\n\nprint df[df.one == 1 ]#1\nprint df[df.one == 7 ]#2\nprint df[df.two == 1 ]#3\nprint df[df.two == 7 ]#4\n\n#print df[df.one == 1 or df.two ==7]\nprint df[df.one.isin(checkList)]\n\n"
'I have a Series s with duplicate index :\n\n&gt;&gt;&gt; s\nSTK_ID  RPT_Date\n600809  20061231    demo_str\n        20070331    demo_str\n        20070630    demo_str\n        20070930    demo_str\n        20071231    demo_str\n        20060331    demo_str\n        20060630    demo_str\n        20060930    demo_str\n        20061231    demo_str\n        20070331    demo_str\n        20070630    demo_str\nName: STK_Name, Length: 11\n\n\nAnd I just want to keep the unique rows and only one copy of the duplicate rows by:\n\ns[s.index.unique()]\n\n\nPandas 0.10.1.dev-f7f7e13  give the below error msg\n\n&gt;&gt;&gt; s[s.index.unique()]\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "d:\\Python27\\lib\\site-packages\\pandas\\core\\series.py", line 515, in __getitem__\n    return self._get_with(key)\n  File "d:\\Python27\\lib\\site-packages\\pandas\\core\\series.py", line 558, in _get_with\n    return self.reindex(key)\n  File "d:\\Python27\\lib\\site-packages\\pandas\\core\\series.py", line 2361, in reindex\n    level=level, limit=limit)\n  File "d:\\Python27\\lib\\site-packages\\pandas\\core\\index.py", line 2063, in reindex\n    limit=limit)\n  File "d:\\Python27\\lib\\site-packages\\pandas\\core\\index.py", line 2021, in get_indexer\n    raise Exception(\'Reindexing only valid with uniquely valued Index \'\nException: Reindexing only valid with uniquely valued Index objects\n&gt;&gt;&gt; \n\n\nSo how to drop extra duplicate rows of series, keep the unique rows and only one copy of the duplicate rows in an efficient way ? (better in one line)\n'
'I have this dataframe in pandas:\n\nd=pandas.DataFrame([{"a": 1, "b": 1}, {"c": 2, "b": 4}])\nd["name"] = ["Hello", "World"]\n\n\nI want to select an element based on its string value in "name" column and then get the value as a string. To select the element:\n\nd[d["name"] == "World"]["name"]\nOut:\n1    World\nName: name\n\n\nThe problem is that it doesn\'t give a simple string but a series. Casting to a string won\'t help -- how can I just get the string "World" out of this? Is this the only way?\n\nd[d["name"] == "World"]["name"].values[0]\n\n\nthanks.\x10\n'
'Basically I want to rotate a pandas DataFrame by 90 degrees (clockwise), such that if it were\n\ndf:\n    A B C D\n  0 5 3 6 7\n  1 6 3 5 2\n\n\nit would turn into\n\ndf:\n   6 5 A\n   3 3 B\n   5 6 C\n   2 7 D\n\n\nIs there some way to do this with pivots, or some other way? Thanks!\n'
"Some of my data looks like:\n\ndate, name, value1, value2, value3, value4\n1/1/2001,ABC,1,1,,\n1/1/2001,ABC,,,2,\n1/1/2001,ABC,,,,35\n\n\nI am trying to get to the point where I can run\n\ndata.set_index(['date', 'name'])\n\n\nBut, with the data as-is, there are of course duplicates (as shown in the above), so I cannot do this (and I don't want an index with duplicates, and I can't simply drop_duplicates(), since this would lose data).\n\nI would like to be able to force rows which have the same [date, name] values into a single rows, if they can be successfully converged based on certain values being NaN (similar to the behavior of combine_first()).  E.g., the above would end up at\n\ndate, name, value1, value2, value3, value4\n1/1/2001,ABC,1,1,2,35\n\n\nIf two values are different and one is not NaN, the two rows should not be converged (this would probably be an error that I would need to follow up on).\n\n(To extend the above example, there may in fact be an arbitrary number of lines--given an arbitrary number of columns--which should be able to be converged into one single line.)\n\nThis feels like a problem that should be very solvable via pandas, but I am having trouble figuring out an elegant solution.  \n"
"I'm starting with a dictionary like this:\n\ndict = {(100000550L, u'ActivityA'): {'bar__sum': 14.0, 'foo__sum': 12.0},\n        (100001799L, u'ActivityB'): {'bar__sum': 7.0, 'foo__sum': 3.0}}\n\n\nWhich, when converted to a DataFrame, puts as column headers the tuples of (id, activitytype):\n\ndf = DataFrame(dict).transpose()\n\n                        bar__sum  foo__sum\n(100000550, ActivityA)        14        12\n(100001799, ActivityB)         7         3\n\n\nHow can I convert the tuples in the index to a MultiIndex? Ie, so that the end result looks like this instead:\n\n                        bar__sum  foo__sum\nid        act_type\n100000550 ActivityA        14        12\n100001799 ActivityB         7         3\n\n\nWhat's the best way to do this? Is there some option on the DataFrame creation that I'm missing? Or should it happen via a list comprehension, which feels inefficient to me.\n"
"pandas for python is neat. I'm trying to replace a list-of-dictionaries with a pandas-dataframe. However, I'm wondering of there's a way to change values row-by-row in a for-loop just as easy?\n\nHere's the non-pandas dict-version:\n\ntrialList = [\n    {'no':1, 'condition':2, 'response':''},\n    {'no':2, 'condition':1, 'response':''},\n    {'no':3, 'condition':1, 'response':''}\n]  # ... and so on\n\nfor trial in trialList:\n    # Do something and collect response\n    trial['response'] = 'the answer!'\n\n\n... and now trialList contains the updated values because trial refers back to that. Very handy! But the list-of-dicts is very unhandy, especially because I'd like to be able to compute stuff column-wise which pandas excel at. \n\nSo given trialList from above, I though I could make it even better by doing something pandas-like:\n\nimport pandas as pd    \ndfTrials = pd.DataFrame(trialList)  # makes a nice 3-column dataframe with 3 rows\n\nfor trial in dfTrials.iterrows():\n   # do something and collect response\n   trials[1]['response'] = 'the answer!'\n\n\n... but trialList remains unchanged here. Is there an easy way to update values row-by-row, perhaps equivalent to the dict-version? It is important that it's row-by-row as this is for an experiment where participants are presented with a lot of trials and various data is collected on each single trial.\n"
'I\'m trying to read this csv into pandas \n\nHK,"[u\'5328.1\', u\'5329.3\', \'2013-12-27 13:58:57.973614\']"\nHK,"[u\'5328.1\', u\'5329.3\', \'2013-12-27 13:58:59.237387\']"\nHK,"[u\'5328.1\', u\'5329.3\', \'2013-12-27 13:59:00.346325\']"\n\n\nAs you can see there are only 2 columns and the second one is a list, is there a way to interpret it correctly ( meaning reading the values in the list as columns) when using pd.read_csv() with arguments ?\n\nthank you\n'
"When I create the following Pandas Series:\n\n    pandas.Series(['a', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa']\n\n\nI get this as a result:\n\n    0                                                    a\n    1    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n    2    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n    3                                     aaaaaaaaaaaaaaaa\n    4    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...\n\n\nHow can I instead get a Series without the ellipsis that looks like this:\n\n    0                                                                         a\n    1    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n    2         aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n    3                                                          aaaaaaaaaaaaaaaa\n    4        aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n\n"
"I am building a new method to parse a DataFrame into a Vincent-compatible format. This requires a standard Index (Vincent can't parse a MultiIndex).\n\nIs there a way to detect whether a Pandas DataFrame has a MultiIndex?\n\nIn: type(frame)\nOut: pandas.core.index.MultiIndex\n\n\nI've tried:\n\nIn: if type(result.index) is 'pandas.core.index.MultiIndex':\n        print True\n    else:\n        print False\nOut: False\n\n\nIf I try without quotations I get:\n\nNameError: name 'pandas' is not defined\n\n\nAny help appreciated.\n\n(Once I have the MultiIndex, I'm then resetting the index and merging the two columns into a single string value for the presentation stage.)\n"
'In Pandas, I can use df.dropna() to drop any NaN entries. Is there anything similar in Pandas to drop non-finite (e.g. Inf) entries?\n'
'I have the following dataframe:\n\n    fsq digits  digits_type\n0    1   1       odd\n1    2   1       odd\n2    3   1       odd\n3    11  2       even\n4    22  2       even\n5    101 3       odd\n6    111 3       odd\n\n\nand I want to add a last column, count, containing the number of fsq belonging to the digits group, i.e:\n\n    fsq digits  digits_type   count\n0    1   1       odd          3\n1    2   1       odd          3\n2    3   1       odd          3\n3    11  2       even         2\n4    22  2       even         2\n5    101 3       odd          2\n6    111 3       odd          2\n\n\nSince there are 3 fsq rows that has digits equal to 1, 2 fsq rows that has digits equal to 2, etc.\n'
'Seaborn allows defining color palettes that contain multiple colors, useful for charts with many lines.  However, when setting the palette to one with multiple colors, only the first six are used, after which colors recycle, making it hard to distinguish lines. This can be overridden by explicitly calling the palette, but that\'s not convenient.  Is there a way to force the Seaborn current palette not to recycle colors, when more than 6 are defined?\n\nExample:\n\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sb\n\n# Define a palette with 8 colors\ncmap = sb.blend_palette(["firebrick", "palegreen"], 8) \nsb.palplot(cmap)\n\n\n\n\n# Set the current palette to this; only 6 colors are used\nsb.set_palette(cmap)\nsb.palplot(sb.color_palette() )\n\n\n\n\ndf = pd.DataFrame({x:[x*10, x*10+5, x*10+10] for x in range(8)})\nfig, (ax1, ax2) = plt.subplots(2,1,figsize=(4,6))\n# Using the current palette, colors repeat \ndf.plot(ax=ax1) \nax1.legend(bbox_to_anchor=(1.2, 1)) \n# using the palette that defined the current palette, colors don\'t repeat\ndf.plot(ax=ax2, color=cmap) \nax2.legend(bbox_to_anchor=(1.2, 1))  ;\n\n\n\n'
"Consider the following dataframe:\n\n         item_id  hour    when        date      quantity\n110   0YrKNYeEoa     1  before  2015-01-26        247286\n111   0UMNiXI7op     1  before  2015-01-26        602001\n112   0QBtIMN3AH     1  before  2015-01-26        981630\n113   0GuKXLiWyV     1  after   2015-01-26       2203913\n114   0SoFbjvXTs     1  after   2015-01-26        660183\n115   0UkT257SXj     1  before  2015-01-26        689332\n116   0RPjXnkiGx     1  after   2015-01-26        283090\n117   0FhJ9RGsLT     1  before  2015-01-26       2024256\n118   0FhGJ4MFlg     1  before  2015-01-26         74524\n119   0FQhHZRXhB     1  before  2015-01-26             0\n120   0FsSdJQlTB     1  before  2015-01-26             0\n121   0FrrAzTFHE     1  before  2015-01-26             0\n122   0FfkgBdMHi     1  before  2015-01-26             0\n123   0FOnJNexRn     1  before  2015-01-26             0\n124   0FcWhIdBds     1  before  2015-01-26             0\n125   0F2lr0cL9t     1  before  2015-01-26       1787659\n\n\nI would like to pivot it to get the table arranged as:\n\nIndex                     before           after\n(item_id, hour, date)   quantityB      quantityA\n\n\nWhen I try with:\n\ndf.pivot(index=['item_id', 'hour', 'date'], columns='when', values='quanty')\n\n\nI get:\n\nValueError: Wrong number of items passed 8143, placement implies 3\n\n\nWhy?\n"
"I started by reading a CSV into a Pandas Data Frame via the pandas read_csv() function. Now that the data is in an actual data frame, I tried to write something like this:\n\nfor row in df.iterrows():\n    row[1].to_json(path_to_file)\n\n\nThis works but only the last row is saved to disk because I've been rewriting the file each time I make a call to row[1].to_json(path_to_file). I've tried a few other file handling options but to no avail. Can anyone shed some insight on how to proceed? \n\nThank you!\n"
'I am reading in a bunch of CSV files (measurement data for water levels over time) to do various analysis and visualizations on them.\n\nDue to various reasons beyond my control, these time series often have missing data, so I do two things:\n\nI count them in total with\n\nRlength=len(RainD)   #counts everything, including NaN\nRcount=RainD.count() #counts only valid numbers\nNaN_Number=Rlength-Rcount\n\n\nand discard the dataset if i have more missing data than a certain threshold:\n\nPercent_Data=Rlength/100\nFive_Percent=Percent_Data*5\nif NaN_Number &gt; Five_Percent:\n    ...\n\n\nIf the number of NaN is sufficiently small, I would like to fill the gaps with \n\nRainD.level=RainD.level.fillna(method=\'pad\',limit=2)\n\n\nAnd now for the issue: Its monthly data, so if I have more than 2 consecutive NaN, I also want to discard the data, since that would mean that I "guess" a whole season, or even more.\n\nThe documentation for fillna doesn\'t really mention what happens when there is more consecutive NaN\'s than my specified limit=2, but when I look at RainD.describe() before and after ...fillna... and compare it with the base CSV, its clear that it fills the first 2 NaN, and then leaves the rest as it is, instead of erroring out.\n\nSo, long story short:\n\nHow do I identify a number of consecutive NaN\'s with pandas, without some complicated and time consuming non-pandas loop?\n'
"I'd like to plot a factorplot in seaborn but manually provide the error bars instead of having seaborn calculate them.\n\nI have a pandas dataframe that looks roughly like this:\n\n     model output feature  mean   std\n0    first    two       a  9.00  2.00\n1    first    one       b  0.00  0.00\n2    first    one       c  0.00  0.00\n3    first    two       d  0.60  0.05\n...\n77   third   four       a  0.30  0.02\n78   third   four       b  0.30  0.02\n79   third   four       c  0.10  0.01\n\n\nand I'm outputting a plot that looks roughly like this:\n\n\nI'm using this seaborn commands to generate the plot:\n\ng = sns.factorplot(data=pltdf, x='feature', y='mean', kind='bar',\n                   col='output', col_wrap=2, sharey=False, hue='model')\ng.set_xticklabels(rotation=90)\n\n\nHowever, I can't figure out how to have seaborn use the 'std' column as the error bars. Unfortunately, it would be quite time consuming to recompute the output for the data frame in question.\n\nThis is a little similar to this q:\nPlotting errors bars from dataframe using Seaborn FacetGrid\n\nExcept I can't figure out how to get it to work with the matplotlib.pyplot.bar function.\n\nIs there a way to do this using seaborn factorplot or FacetGrid combined with matplotlib?\n\nThanks!\n"
"I am new with pandas and I am trying to join two dataframes based on the equality of one specific column. For example suppose that I have the followings:\n\ndf1\nA    B    C\n1    2    3\n2    2    2\n\ndf2\nA    B    C\n5    6    7\n2    8    9\n\n\nBoth dataframes have the same columns and the value of only one column (say A) might be equal. What I want as output is this:\n\ndf3\nA    B    C   B    C\n2    8    9   2    2\n\n\nThe values for column 'A' are unique in both dataframes.\n\nThanks\n"
'I have some DataFrame which I want to group by the ID, e. g.:\n\nimport pandas as pd\ndf = pd.DataFrame({\'item_id\': [\'a\', \'a\', \'b\', \'b\', \'b\', \'c\', \'d\'], \'user_id\': [1,2,1,1,3,1,5]})\nprint df\n\n\nWhich generates:\n\n  item_id  user_id\n0       a        1\n1       a        2\n2       b        1\n3       b        1\n4       b        3\n5       c        1\n6       d        5\n\n[7 rows x 2 columns]\n\n\nI can easily group by the id:\n\ngrouped = df.groupby("item_id")\n\n\nBut how can I return only the first N group-by objects? E. g. I want only the first 3 unique item_ids.\n'
'I am working with pandas dataframes that are essentially time series like this:\n\n             level\nDate              \n1976-01-01  409.67\n1976-02-01  409.58\n1976-03-01  409.66\n…\n\n\nWhat I want to have, is multiple indexes/headers for the level column, like so:\n\n           Station1                   #Name of the datasource\n           43.1977317,-4.6473648,5    #Lat/Lon of the source\n           Precip                     #Type of data\nDate              \n1976-01-01  409.67\n1976-02-01  409.58\n1976-03-01  409.66\n…\n\n\nSo essentially I am searching for something like Mydata.columns.level1 = [\'Station1\'], Mydata.columns.level2 = [Lat,Lon], Mydata.columns.level3 = [\'Precip\'].\n\nReason being that a single location can have multiple datasets, and that I want to be able to pick either all data from one location, or all data of a certain type from all locations, from a subsequent merged, big dataframe.\n\nI can set up an example dataframe from the pandas documentation, and test my selection, but with my real data, I need a different way to set the indexes as in the example.\n\nExample:\n\nBuilt a small dataframe\n\nheader = [np.array([\'location\',\'location\',\'location\',\'location2\',\'location2\',\'location2\']), \nnp.array([\'S1\',\'S2\',\'S3\',\'S1\',\'S2\',\'S3\'])] \ndf = pd.DataFrame(np.random.randn(5, 6), index=[\'a\',\'b\',\'c\',\'d\',\'e\'], columns = header )   \n\ndf\n    location                      location2                    \n         S1        S2        S3         S1        S2        S3\na -1.469932 -1.544511 -1.373463  -0.317262  0.024832 -0.641000\nb  0.047170 -0.339423  1.351253   0.601172 -1.607339  0.035932\nc -0.257479  1.140829  0.188291  -0.242490  1.019315 -1.163429\nd  0.832949  0.098170 -0.818513  -0.070383  0.557419 -0.489839\ne -0.628549 -0.158419  0.366167  -2.319316 -0.474897 -0.319549\n\n\nPick datatype or location:\n\ndf.loc(axis=1)[:,\'S1\']\n\n   location  location2\n         S1         S1\na -1.469932  -0.317262\nb  0.047170   0.601172\nc -0.257479  -0.242490\nd  0.832949  -0.070383\ne -0.628549  -2.319316\n\ndf[\'location\']\n\n         S1        S2        S3\na -1.469932 -1.544511 -1.373463\nb  0.047170 -0.339423  1.351253\nc -0.257479  1.140829  0.188291\nd  0.832949  0.098170 -0.818513\ne -0.628549 -0.158419  0.366167\n\n\nOr am I just looking for the wrong terminology? Because 90% of all examples in the documentation, and the questions here only treat the vertical "stuff" (dates or abcde in my case) as index, and a quick df.index.values on my test data also just gets me the vertical array([\'a\', \'b\', \'c\', \'d\', \'e\'], dtype=object).\n'
"Using Pandas how would I filter rows and take just a subset of columns from a pandas dataframe please in one command.\n\nI am trying to apply something like this....\n\nframe[(frame.DESIGN_VALUE &gt; 20) &amp; (frame['mycol3','mycol6']))]\n\n\nThanks.\n"
'I have the following input file:\n\n"Name",97.7,0A,0A,65M,0A,100M,5M,75M,100M,90M,90M,99M,90M,0#,0N#,\n\n\nAnd I am reading it in with:\n\n#!/usr/bin/env python\n\nimport pandas as pd\nimport sys\nimport numpy as np\n\nfilename = sys.argv[1]\ndf = pd.read_csv(filename,header=None)\nfor col in df.columns[2:]:\n    df[col] = df[col].str.extract(r\'(\\d+\\.*\\d*)\').astype(np.float)\n\nprint df\n\n\nHowever, I get the error\n\n    df[col] = df[col].str.extract(r\'(\\d+\\.*\\d*)\').astype(np.float)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/generic.py", line 2241, in __getattr__\n    return object.__getattribute__(self, name)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/base.py", line 188, in __get__\n    return self.construct_accessor(instance)\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/base.py", line 528, in _make_str_accessor\n    raise AttributeError("Can only use .str accessor with string "\nAttributeError: Can only use .str accessor with string values, which use np.object_ dtype in pandas\n\n\nThis worked OK in pandas 0.14 but does not work in pandas 0.17.0.\n'
"I am trying to create a function that iterates through a pandas dataframe row by row. I want to create a new column based on row values of other columns. My original dataframe could look like this: \n\ndf:\n\n   A   B\n0  1   2\n1  3   4\n2  2   2\n\n\nNow I want to create a new column filled with the row values of Column A - Column B at each index position, so that the result looks like this:\n\n df:\n\n       A   B   A-B\n    0  1   2   -1\n    1  3   4   -1\n    2  2   2    0\n\n\nthe solution I have works, but only when I do NOT use it in a function:\n\nfor index, row in df.iterrows():\n        print index\n        df['A-B']=df['A']-df['B']\n\n\nThis gives me the desired output, but when I try to use it as a function, I get an error.\n\ndef test(x):\n    for index, row in df.iterrows():\n        print index\n        df['A-B']=df['A']-df['B']\n    return df\ndf.apply(test)\n\nValueError: cannot copy sequence with size 4 to array axis with dimension 3\n\n\nWhat am I doing wrong here and how can I get it to work?\n"
"Pandas Pivot Table Dictionary of Agg function\n\nI am trying to calculate 3 aggregative functions during pivoting:\n\n\nCount\nMean\nStDev\n\n\nThis is the code:\n\nn_page = (pd.pivot_table(Main_DF, \n                         values='SPC_RAW_VALUE',  \n                         index=['ALIAS', 'SPC_PRODUCT', 'LABLE', 'RAW_PARAMETER_NAME'], \n                         columns=['LOT_VIRTUAL_LINE'],\n                         aggfunc={'N': 'count', 'Mean': np.mean, 'Sigma': np.std})\n          .reset_index()\n         )\n\n\nError I am getting is: KeyError: 'Mean'\n\nHow can I calculate those 3 functions?\n"
"I have groupings of values in the data and within each group, I would like to check if a value within the group is below 8. If this condition is met, the entire group is removed from the data set.\n\nPlease note the value I'm referring to lies in another column to the groupings column.\n\nExample Input:\n\nGroups Count\n  1      7\n  1      11\n  1      9 \n  2      12\n  2      15\n  2      21 \n\n\nOutput:\n\nGroups Count\n  2      12\n  2      15\n  2      21 \n\n"
"I have two datetime columns which are naive when I read them into memory but which are in US/Eastern actually. I simply want to convert both of these columns to US/Central.\n\nI found a method which works but it seems like I am doing a workaround.\nI changed my call_start and call_end columns to be named 'start' and 'end' instead so I don't end up with duplicate column names. I then created a separate datetimeindex for each of these columns and reset the index.\n\naht.set_index(pd.DatetimeIndex(aht['start']).tz_localize('US/Eastern').tz_convert('US/Central'), inplace = True, drop = True)\naht.index.names = ['call_start']\naht = aht.reset_index()\naht.set_index(pd.DatetimeIndex(aht['end']).tz_localize('US/Eastern').tz_convert('US/Central'), inplace = True, drop = True)\naht.index.names = ['call_end']\naht = aht.reset_index()\n\n\nI end up getting:\n\n                 call_end                  call_start              start                 end\n2016-01-13 06:05:01-06:00   2016-01-13 06:02:00-06:00   01/13/2016 07:02    01/13/2016 07:05\n2016-01-13 06:07:00-06:00   2016-01-13 06:03:16-06:00   01/13/2016 07:03    01/13/2016 07:07\n2016-01-13 06:09:13-06:00   2016-01-13 06:06:02-06:00   01/13/2016 07:06    01/13/2016 07:09\n2016-01-13 06:17:51-06:00   2016-01-13 06:06:20-06:00   01/13/2016 07:06    01/13/2016 07:17\n\n\nIs this the best method? All other data is in central time so I just want to make sure that this file is too so when I merge files together it makes more sense. I do not care about having the actual timezone stamp there though - is there a way to easily strip it after I created my new columns?\n"
'I have a DataFrame with the following structure:\n\ninterval    segment  variable        value\n4   02:00:00      Night  weekdays   154.866667\n5   02:30:00      Night  weekdays   100.666667\n6   03:00:00      Night  weekdays    75.400000\n7   03:30:00      Night  weekdays    56.533333\n8   04:00:00      Night  weekdays    55.000000\n9   04:30:00      Night  weekends    53.733333\n10  05:00:00      Night  weekends    81.200000\n11  05:30:00      Night  weekends   125.933333\n14  07:00:00    Morning  weekdays   447.200000\n15  07:30:00    Morning  weekends   545.200000\n16  08:00:00    Morning  weekends   668.733333\n17  08:30:00    Morning  weekends   751.333333\n18  09:00:00    Morning  weekdays   793.800000\n19  09:30:00    Morning  weekdays   781.125000\n23  11:30:00       Noon  weekdays   776.375000\n24  12:00:00       Noon  weekdays   741.812500\n25  12:30:00       Noon  weekends   723.000000\n26  13:00:00       Noon  weekends   734.562500\n27  13:30:00       Noon  weekends   763.882353\n28  14:00:00  Afternoon  weekdays   810.411765\n31  15:30:00  Afternoon  weekdays   855.411765\n32  16:00:00  Afternoon  weekdays   824.882353\n33  16:30:00  Afternoon  weekends   768.529412\n34  17:00:00  Afternoon  weekends   790.812500\n35  17:30:00  Afternoon  weekends   809.125000\n\n\nI want to produce a faceted grid of bar plots, one for each variable (weekdays/weekends) and then colour the bars according to the "segment" column.\n\nProducing the two bar plots is very straightforward:\n\ng = sns.FacetGrid(melted, col="variable")\ng.map(sns.barplot,\'interval\',\'value\')\n\n\nThis produces (I know the xlabels are wrong, I can correct that):\n\n\nI\'m stuck at colouring the bars according to "segment". Per the docs, all I need to do is add the variable when instantiating the FacetGrid and set some palette:\n\ng = sns.FacetGrid(melted, col="variable",hue="segment",palette="Set3")\ng.map(sns.barplot,\'interval\',\'value\')\n\n\nBut this produces:\n\n\nThe bars are stacked in front of each other instead of being spread across the whole interval. What am I missing here?\n\nI\'ve created a gist with the dataset. \n'
"How can I reverse the order of the rows in my pandas.dataframe?\n\nI've looked everywhere and the only thing people are talking about is sorting the columns, reversing the order of the columns...\n\nWhat I want is simple :\n\nIf my DataFrame looks like this:\n\n   A      B     C\n ------------------\n  LOVE    IS   ALL\n  THAT   MAT   TERS\n\n\nI want it to become this: \n\n   A      B     C\n ------------------\n  THAT   MAT   TERS\n  LOVE    IS   ALL\n\n\nI know I can iterate over my data in reverse order but that's not what I want.\n"
"I have a dataframe in pandas and my goal is to write each row of the dataframe as a new json file.\n\nI'm a bit stuck right now. My intuition was to iterate over the rows of the dataframe (using df.iterrows) and use json.dumps to dump the file but to no avail.\n\nAny thoughts? \n"
"I have a Pandas data frame like this:\n\ntest = pd.DataFrame({ 'Date' : ['2016-04-01','2016-04-01','2016-04-02',\n                             '2016-04-02','2016-04-03','2016-04-04',\n                             '2016-04-05','2016-04-06','2016-04-06'],\n                      'User' : ['Mike','John','Mike','John','Mike','Mike',\n                             'Mike','Mike','John'],\n                      'Value' : [1,2,1,3,4.5,1,2,3,6]\n                })\n\n\nAs you can see below, the data set does not have observations for every day necessarily:\n\n         Date  User  Value\n0  2016-04-01  Mike    1.0\n1  2016-04-01  John    2.0\n2  2016-04-02  Mike    1.0\n3  2016-04-02  John    3.0\n4  2016-04-03  Mike    4.5\n5  2016-04-04  Mike    1.0\n6  2016-04-05  Mike    2.0\n7  2016-04-06  Mike    3.0\n8  2016-04-06  John    6.0\n\n\nI'd like to add a new column which shows the average value for each user for the past n days (in this case n = 2) if at least one day is available, else it would have nan value. For example, on 2016-04-06 John gets a nan because he has no data for 2016-04-05 and 2016-04-04. So the result will be something like this:\n\n         Date  User  Value  Value_Average_Past_2_days\n0  2016-04-01  Mike    1.0                        NaN\n1  2016-04-01  John    2.0                        NaN\n2  2016-04-02  Mike    1.0                       1.00\n3  2016-04-02  John    3.0                       2.00\n4  2016-04-03  Mike    4.5                       1.00\n5  2016-04-04  Mike    1.0                       2.75\n6  2016-04-05  Mike    2.0                       2.75\n7  2016-04-06  Mike    3.0                       1.50\n8  2016-04-06  John    6.0                        NaN\n\n\nIt seems that I should a combination of group_by and customized rolling_mean after reading several posts in the forum, but I couldn't quite figure out how to do it.\n"
"I have a pandas Dataframe with one column a list of files\n\nimport pandas as pd\ndf = pd.read_csv('fname.csv')\n\ndf.head()\n\nfilename    A    B    C\nfn1.txt   2    4    5\nfn2.txt   1    2    1\nfn3.txt   ....\n....\n\n\nI would like to delete the file extension .txt from each entry in filename. How do I accomplish this? \n\nI tried:\n\ndf['filename'] = df['filename'].map(lambda x: str(x)[:-4])\n\n\nbut when I look at the column entries afterwards with df.head(), nothing has changed. \n\nHow does one do this? \n"
"I have created a Dataframe df by merging 2 lists using the following command: \n\nimport pandas as pd\ndf=pd.DataFrame({'Name' : list1,'Probability' : list2})\n\n\nBut I'd like to remove the first column (The index column) and make the column called Name the first column. I tried using del df['index'] and index_col=0. But they didn't work. I also checked reset_index() and that is not what I need. I would like to completely remove the whole index column from a Dataframe that has been created like this (As mentioned above). Someone please help!\n"
"I have a pandas dataframe whose entries are all strings:\n\n   A     B      C\n1 apple  banana pear\n2 pear   pear   apple\n3 banana pear   pear\n4 apple  apple  pear\n\n\netc. I want to select all the rows that contain a certain string, say, 'banana'. I don't know which column it will appear in each time. Of course, I can write a for loop and iterate over all rows. But is there an easier or faster way to do this?\n"
'I have a dataframe which looks like this:\n\n\n  \n\n\nEach user has 10 records. Now, I want to create a dataframe which looks like this:\n\nuserid  name1  name2  ... name10\n\n\nwhich means I need to invert every 10 records of the column name and append to a new dataframe.\n\nSo, how do it do it? Is there any way I can do it in Pandas?\n'
'I am getting the following error while I am trying to plot a pandas dataframe:\n\n\n  ValueError: num must be 1 &lt;= num &lt;= 0, not 1\n\n\nCode:\n\nimport matplotlib.pyplot as plt\n\nnames = [\'buying\', \'maint\', \'doors\', \'persons\', \'lug_boot\', \'safety\']\ncustom = pd.DataFrame(x_train)  //only a portion of the csv\ncustom.columns = names\ncustom.hist()\nplt.show()\n\n\nI have tried to read the file again from the csv and I am getting the exact same error.\n\nEdit:\n\nprint x_train output:\n\n\n  [[0.0 0.0 0.0 0.0 0.0 0.0]\n  \n  [1.0 1.0 0.0 0.0 0.0 0.0]\n  \n  [0.0 0.0 0.0 0.0 0.0 0.0]\n  \n  ..., \n  \n  [0.0 0.0 0.0 0.0 0.0 0.0]\n  \n  [0.3333333333333333 0.3333333333333333 2.0 2.0 2.0 2.0]\n  \n  [0.0 0.0 3.0 3.0 3.0 3.0]]\n\n\nEdit2:\n\nComplete list of errors(Traceback): \n\n\n  Traceback (most recent call last):\n  \n  File "temp.py", line 104, in \n      custom.dropna().hist()\n  \n  File "/home/kostas/anaconda2/lib/python2.7/site-packages/pandas/tools/plotting.py", line 2893, in hist_frame\n      layout=layout)\n  \n  File "/home/kostas/anaconda2/lib/python2.7/site-packages/pandas/tools/plotting.py", line 3380, in _subplots\n      ax0 = fig.add_subplot(nrows, ncols, 1, **subplot_kw)\n  \n  File "/home/kostas/anaconda2/lib/python2.7/site-packages/matplotlib/figure.py", line 1005, in add_subplot\n      a = subplot_class_factory(projection_class)(self, *args, **kwargs)\n  \n  File "/home/kostas/anaconda2/lib/python2.7/site-packages/matplotlib/axes/_subplots.py", line 64, in init\n      maxn=rows*cols, num=num))\n\n'
"After running a Variance Threshold from Scikit-Learn on a set of data, it removes a couple of features. I feel I'm doing something simple yet stupid, but I'd like to retain the names of the remaining features. The following code: \n\ndef VarianceThreshold_selector(data):\n    selector = VarianceThreshold(.5) \n    selector.fit(data)\n    selector = (pd.DataFrame(selector.transform(data)))\n    return selector\nx = VarianceThreshold_selector(data)\nprint(x)\n\n\nchanges the following data (this is just a small subset of the rows):\n\nSurvived    Pclass  Sex Age SibSp   Parch   Nonsense\n0             3      1  22   1        0        0\n1             1      2  38   1        0        0\n1             3      2  26   0        0        0\n\n\ninto this (again just a small subset of the rows)\n\n     0         1      2     3\n0    3      22.0      1     0\n1    1      38.0      1     0\n2    3      26.0      0     0\n\n\nUsing the get_support method, I know that these are Pclass, Age, Sibsp, and Parch, so I'd rather this return something more like :\n\n     Pclass         Age      Sibsp     Parch\n0        3          22.0         1         0\n1        1          38.0         1         0\n2        3          26.0         0         0\n\n\nIs there an easy way to do this? I'm very new with Scikit Learn, so I'm probably just doing something silly. \n"
"I want to fill empty cells with with previous row value if they start with number. For example, I have \n\n    Text    Text    \n    30      Text    Text    \n            Text    Text    \n            Text    Text    \n    31      Text    Text\n    Text    Text    \n    31      Text    Text    \n            Text    Text    \n            Text    Text    \n    32      Text    Text\n    Text    Text    \n            Text    Text    \n            Text    Text    \n            Text    Text    \n            Text    Text\n\n\nI however, want to have \n\nText    Text    \n30      Text    Text    \n30      Text    Text    \n30      Text    Text    \n31      Text    Text\nText    Text    \n31      Text    Text    \n31      Text    Text    \n31      Text    Text    \n32      Text    Text\nText    Text    \n        Text    Text    \n        Text    Text    \n        Text    Text    \n        Text    Text\n\n\nI tried to reach this by using this code:\n\ndata = pd.read_csv('DATA.csv',sep='\\t', dtype=object, error_bad_lines=False)\ndata = data.fillna(method='ffill', inplace=True)\nprint(data)\n\n\nbut it did not work. \n\nIs there anyway to do this?\n"
"Sometimes when data is imported to Pandas Dataframe, it always imports as type object.  This is fine and well for doing most operations, but I am trying to create a custom export function, and my question is this:\n\n\nIs there a way to force Pandas to infer the data types of the input data?\nIf not, is there a way after the data is loaded to infer the data types somehow?\n\n\nI know I can tell Pandas that this is of type int, str, etc.. but I don't want to do that, I was hoping pandas could be smart enough to know all the data types when a user imports or adds a column.\n\nEDIT - example of import\n\na = ['a']\ncol = ['somename']\ndf = pd.DataFrame(a, columns=col)\nprint(df.dtypes)\n&gt;&gt;&gt; somename    object\ndtype: object\n\n\nThe type should be string? \n"
"I want to draw a scatter trend line on matplot. How can I do that?\n\nPython\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ncsv = pd.read_csv('/tmp/test.csv')\ndata = csv[['fee', 'time']]\nx = data['fee']\ny = data['time']\nplt.scatter(x, y)\nplt.show()\n\n\nCSV\n\nfee,time\n100,650\n90,700\n80,860\n70,800\n60,1000\n50,1200\n\n\ntime is integer value.\n\nScatter chart\n"
"I'm trying to find an efficient way to generate rolling counts or sums in pandas given a grouping and a date range. Eventually, I want to be able to add conditions, ie. evaluating a 'type' field, but I'm not there just yet. I've written something to get the job done, but feel that there could be a more direct way of getting to the desired result.\n\nMy pandas data frame currently looks like this, with the desired output being put in the last column 'rolling_sales_180'.\n\n    name       date  amount  rolling_sales_180\n0  David 2015-01-01     100              100.0\n1  David 2015-01-05     500              600.0\n2  David 2015-05-30      50              650.0\n3  David 2015-07-25      50              100.0\n4   Ryan 2014-01-04     100              100.0\n5   Ryan 2015-01-19     500              500.0\n6   Ryan 2016-03-31      50               50.0\n7    Joe 2015-07-01     100              100.0\n8    Joe 2015-09-09     500              600.0\n9    Joe 2015-10-15      50              650.0\n\n\nMy current solution and environment can be sourced below. I've been modeling my solution from this R Q&amp;A in stackoverflow. Efficient way to perform running total in the last 365 day window\n\nimport pandas as pd\nimport numpy as np \n\ndef trans_date_to_dist_matrix(date_col):  #  used to create a distance matrix\n    x = date_col.tolist()\n    y = date_col.tolist()\n    data = []\n    for i in x:\n        tmp = []\n        for j in y:\n            tmp.append(abs((i - j).days))\n        data.append(tmp)\n        del tmp\n\n    return pd.DataFrame(data=data, index=date_col.values, columns=date_col.values)\n\n\ndef lower_tri(x_col, date_col, win):  # x_col = column user wants a rolling sum of ,date_col = dates, win = time window\n    dm = trans_date_to_dist_matrix(date_col=date_col)  # dm = distance matrix\n    dm = dm.where(dm &lt;= win)  # find all elements of the distance matrix that are less than window(time)\n    lt = dm.where(np.tril(np.ones(dm.shape)).astype(np.bool))  # lt = lower tri of distance matrix so we get only future dates\n    lt[lt &gt;= 0.0] = 1.0  # cleans up our lower tri so that we can sum events that happen on the day we are evaluating\n    lt = lt.fillna(0)  # replaces NaN with 0's for multiplication\n     return pd.DataFrame(x_col.values * lt.values).sum(axis=1).tolist()\n\n\ndef flatten(x):\n    try:\n        n = [v for sl in x for v in sl]\n        return [v for sl in n for v in sl]\n    except:\n        return [v for sl in x for v in sl]\n\n\ndata = [\n['David', '1/1/2015', 100], ['David', '1/5/2015', 500], ['David', '5/30/2015', 50], ['David', '7/25/2015', 50],\n['Ryan', '1/4/2014', 100], ['Ryan', '1/19/2015', 500], ['Ryan', '3/31/2016', 50],\n['Joe', '7/1/2015', 100], ['Joe', '9/9/2015', 500], ['Joe', '10/15/2015', 50]\n]\n\nlist_of_vals = []\n\ndates_df = pd.DataFrame(data=data, columns=['name', 'date', 'amount'], index=None)\ndates_df['date'] = pd.to_datetime(dates_df['date'])\nlist_of_vals.append(dates_df.groupby('name', as_index=False).apply(\nlambda x: lower_tri(x_col=x.amount, date_col=x.date, win=180)))\n\nnew_data = flatten(list_of_vals)\ndates_df['rolling_sales_180'] = new_data\n\nprint dates_df\n\n\nYour time and feedback are appreciated.\n"
'My python code works correctly in the below example. My code combines a directory of CSV files and matches the headers. However, I want to take it a step further - how do I add a column that appends the filename of the CSV that was used?\n\nimport pandas as pd\nimport glob\n\nglobbed_files = glob.glob("*.csv") #creates a list of all csv files\n\ndata = [] # pd.concat takes a list of dataframes as an agrument\nfor csv in globbed_files:\n    frame = pd.read_csv(csv)\n    data.append(frame)\n\nbigframe = pd.concat(data, ignore_index=True) #dont want pandas to try an align row indexes\nbigframe.to_csv("Pandas_output2.csv")\n\n'
"I have a Pandas dataframe that I am writing out to an XLSX using openpyxl. Many of the cells in the spreadsheet contain long sentences, and i want to set 'wrap_text' on all the contents of the sheet (i.e. every cell).\n\nIs there a way to do this? I have seen openpyxl has an 'Alignment' option for 'wrap_text', but I cannot see how to apply this to all cells.\n\nEdit:\n\nThanks to feedback, the following does the trick. Note - copy due to styles being immutable.\n\nfor row in ws.iter_rows():\n    for cell in row:      \n        cell.alignment =  cell.alignment.copy(wrapText=True)\n\n"
'I want to replace some values in a column of a dataframe using a dictionary that maps the old codes to the new codes. \n\ndi = dict( { "myVar": {11:0, 204:11} } )\nmydata.replace( to_replace = di, inplace = True )\n\n\nBut some of the new codes and old codes overlap. When using the .replace method of the dataframe I encounter the error \'Replacement not allowed with overlapping keys and values\'\n\nMy current workaround is to replace replace the offending keys manually and then apply the dictionary to the remaining non-overlapping cases. \n\nmydata.loc[ mydata.myVar == 11, "myVar" ] = 0 \ndi = dict( { "myVar": {204:11} } )\nmydata.replace( to_replace = di, inplace = True )\n\n\nIs there a more compact way to do this? \n'
'I have multiple pandas dataframe which may have different number of columns and the number of these columns typically vary from 50 to 100. I need to create a final column that is simply all the columns concatenated. Basically the string in the first row of the column should be the sum(concatenation) of the strings on the first row of all the columns. I wrote the loop below but I feel there might be a better more efficient way to do this. Any ideas on how to do this\n\nnum_columns = df.columns.shape[0]\ncol_names = df.columns.values.tolist()\ndf.loc[:, \'merged\'] = ""\nfor each_col_ind in range(num_columns):\n    print(\'Concatenating\', col_names[each_col_ind])\n    df.loc[:, \'merged\'] = df.loc[:, \'merged\'] + df[col_names[each_col_ind]]\n\n'
"I want to add an additional column to an existing dataframe that has the length of the 'seller_name' column as its value. \n\nThe output should be like so:\n\nseller_name    name_length\n-------------|-------------\nRick         |      4\nHannah       |      6\n\n\nHowever, I'm having difficulty getting the code right. \n\ndf['name_length']  = len(df['seller_name'])\n\n\njust gives me the length of the entire column (6845)\nAnd \n\ndf['nl']  = df[len('seller_name')]\n\n\nThrows a KeyError. \n\nDoes anyone know the correct command to achieve my goal?\n\nMany thanks!\n"
"I have a huge dataframe which has values and blanks/NA's in it. I want to remove the blanks from the dataframe and move the next values up in the column. Consider below sample dataframe.\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.random.randn(5,4))\ndf.iloc[1,2] = np.NaN\ndf.iloc[0,1] = np.NaN\ndf.iloc[2,1] = np.NaN\ndf.iloc[2,0] = np.NaN\ndf\n\n       0           1           2         3\n0   1.857476      NaN      -0.462941   -0.600606\n1   0.000267   -0.540645    NaN        0.492480\n2   NaN           NaN      -0.803889   0.527973\n3   0.566922    0.036393   -1.584926   2.278294\n4   -0.243182   -0.221294   1.403478   1.574097\n\n\nI want my output to be as below\n\n       0             1             2           3\n0   1.857476    -0.540645     -0.462941   -0.600606\n1   0.000267     0.036393     -0.803889    0.492480\n2   0.566922    -0.221294     -1.584926    0.527973\n3   -0.243182                  1.403478    2.278294\n4                                          1.574097\n\n\nI want the NaN to be removed and the next value to move up. df.shift was not helpful. I tried with multiple loops and if statements and achieved the desired result but is there any better way to get it done.\n"
'I have following dataframe.  \n\nid int_date  \n1  20160228  \n2  20161231  \n3  20160618  \n4  20170123  \n5  20151124\n\n\nHow to convert above date in int format to date format of mm/dd/yyyy? Want this in particular format for further excel operations?\n\nid int_date  \n1  02/28/2016  \n2  12/31/2016  \n3  06/18/2016\n4  01/23/2017\n5  11/24/2015\n\n\nIS it also possible to generate third column with only Month in words? like January, February etc from int_date?\n\nI tried following\n\ndate = datetime(year=int(s[0:4]), month=int(s[4:6]), day=int(s[6:8]))\n\n\nbut date is in datetime object, how to put it as date in pandas DF?\n'
'I have a dataframe customers with some "bad" rows, the key in this dataframe is CustomerID. I know I should drop these rows. I have a list called badcu that says [23770, 24572, 28773, ...] each value corresponds to a different "bad" customer.\n\nThen I have another dataframe, lets call it sales, so I want to drop all the records for the bad customers, the ones in the badcu list.\n\nIf I do the following\n\nsales[sales.CustomerID.isin(badcu)]\n\n\nI got a dataframe with precisely the records I want to drop, but if I do a \n\nsales.drop(sales.CustomerID.isin(badcu))\n\n\nIt returns a dataframe with the first row dropped (which is a legitimate order), and the rest of the rows intact (it doesn\'t delete the bad ones), I think I know why this happens, but I still don\'t know how to drop the incorrect customer id rows.\n'
'I am creating a python module with a function to display a pandas DataFrame (my_df).\n\nIf the user imports the module into a Jupyter notebook, I would want to deliver "pretty" formatting for the DataFrame by using something like:\n\nfrom IPython.display import display, HTML\ndisplay(my_df)\n\n\nIf the user is not in a Jupyter notebook, I would want to display the text form of the DataFrame:\n\nprint(my_df)\n\n\nHow can I check if the code is being run from a Jupyter notebook? Or, how can I display the DataFrame in text form from the commandline, vs display the HTML form if it is imported into a Jupyter notebook?\n\nfrom IPython.display import display, HTML\n\ndef my_func(my_df):\n    if [... code to check for Jupyter notebook here ...]:\n        display(my_df)\n    else:\n        print(my_df)\n\n'
"There is something wrong when I use groupby method:\n\ndata = pd.Series(np.random.randn(100),index=pd.date_range('01/01/2001',periods=100))\nkeys = lambda x: [x.year,x.month]\ndata.groupby(keys).mean()\n\n\nbut it has an error: TypeError: unhashable type: 'list'.\nI want group by year and month, then calculate the means,why it has wrong?\n"
"Python version: Python 2.7.13 :: Anaconda custom (64-bit)\nPandas version: pandas 0.20.2\n\nHello,\n\nI have a quite simple requirement.\nI would like to read an excel file and write a specific sheet to a csv file.\nBlank values in the source Excel file should be treated / written as blank when writing the csv file.\nHowever, my blank records are always written as 'nan' to the output file. (without the quotes)\n\nI read the Excel file via method\n\nread_excel(xlsx, sheetname='sheet1', dtype = str)\n\nI am specifying dtype because I have some columns that are numbers but should be treated as string. (Otherwise they might lose leading 0s etc)\ni.e. I would like to read the exact value from every cell.\n\nNow I write the output .csv file via\nto_csv(output_file,index=False,mode='wb',sep=',',encoding='utf-8')\n\nHowever, my result csv file contains nan for all blank cells from the excel file.\n\nWhat am I missing? I already tried .fillna('', inplace=True) function but it seems to be doing nothing to my data.\nI also tried to add parameter na_rep ='' to the to_csv method but without success.\n\nThanks for any help!\n\nAddendum: Please find hereafter a reproducible example.\n\nPlease find hereafter a reproducible example code.\nPlease first create a new Excel file with 2 columns with the following content:\nCOLUMNA COLUMNB COLUMNC\n01      test\n02  test\n03      test\n\n(I saved this Excel file to c:\\test.xls\nPlease note that 1st and 3rd row for column B as well as the 2nd row for Column C is blank/empty)\n\nNow here is my code:\n\nimport pandas as pd\nxlsx = pd.ExcelFile('c:\\\\test.xlsx')\ndf = pd.read_excel(xlsx, sheetname='Sheet1', dtype = str)\ndf.fillna('', inplace=True)\ndf.to_csv('c:\\\\test.csv', index=False,mode='wb',sep=',',encoding='utf-8', na_rep ='')\n\n\nMy result is:\n COLUMNA,COLUMNB,COLUMNC\n 01,nan,test\n 02,test,nan\n 03,nan,test  \n\nMy desired result would be:\nCOLUMNA,COLUMNB,COLUMNC\n01,,test\n02,test,\n03,,test  \n"
"I have the following data frame and want to:\n\n\nGroup records by month\nSum QTY_SOLDand NET_AMT of each unique UPC_ID(per month)\nInclude the rest of the columns as well in the resulting dataframe\n\n\nThe way I thought I can do this is 1st: create a month column to aggregate the D_DATES, then sum QTY_SOLD by UPC_ID. \n\nScript:\n\n# Convert date to date time object\ndf['D_DATE'] = pd.to_datetime(df['D_DATE'])\n\n# Create aggregated months column\ndf['month'] = df['D_DATE'].apply(dt.date.strftime, args=('%Y.%m',))\n\n# Group by month and sum up quantity sold by UPC_ID\ndf = df.groupby(['month', 'UPC_ID'])['QTY_SOLD'].sum()\n\n\n\n\nCurrent data frame:\n\nUPC_ID | UPC_DSC | D_DATE | QTY_SOLD | NET_AMT\n----------------------------------------------\n111      desc1    2/26/2017   2         10 (2 x $5)\n222      desc2    2/26/2017   3         15\n333      desc3    2/26/2017   1         4\n111      desc1    3/1/2017    1         5\n111      desc1    3/3/2017    4         20\n\n\nDesired Output:\n\nMONTH | UPC_ID | QTY_SOLD | NET_AMT | UPC_DSC\n----------------------------------------------\n2017-2      111     2         10       etc...\n2017-2      222     3         15\n2017-2      333     1         4\n2017-3      111     5         25\n\n\nActual Output:\n\nMONTH | UPC_ID  \n----------------------------------------------\n2017-2      111     2\n            222     3\n            333     1\n2017-3      111     5\n...  \n\n\nQuestions:\n\n\nHow do I include the month for each row?\nHow do I include the rest of the columns of the dataframe?\nHow do also sum NET_AMT in addition to QTY_SOLD? \n\n"
"Having pandas data frame df with at least columns C1,C2,C3 how would you get all the unique C1,C2,C3 values as a new DataFrame? \n\nin other words, similiar to :\n\nSELECT C1,C2,C3\nFROM T\nGROUP BY C1,C2,C3\n\n\nTried that \n\nprint df.groupby(by=['C1','C2','C3'])\n\n\nbut im getting \n\n&lt;pandas.core.groupby.DataFrameGroupBy object at 0x000000000769A9E8&gt;\n\n"
'I have a dictionary of python pandas dataframes. The total size of this dictionary is about 2GB. However, when I share it across 16 multiprocessing (in the subprocesses I only read the data of the dict  without modifying it), it takes 32GB ram. So I would like to ask if it is possible for me to share this dictionary across multiprocessing without copying it. I tried to convert it to manager.dict(). But it seems it takes too long. What would be the most standard way to achieve this? Thank you.\n'
'i have a dataframe with following data :\n\ninvoice_no  dealer  billing_change_previous_month        date\n       110       1                              0  2016-12-31\n       100       1                         -41981  2017-01-30\n      5505       2                              0  2017-01-30\n      5635       2                          58730  2016-12-31\n\n\ni want to have only one dealer with the maximum date . The desired output should be like this :\n\ninvoice_no  dealer  billing_change_previous_month        date\n       100       1                         -41981  2017-01-30\n      5505       2                              0  2017-01-30\n\n\neach dealer should be distinct with maximum date,\nthanks in advance for your help.\n'
"I'm looking to add a uuid for every row in a single new column in a pandas DataFrame. This obviously fills the column with the same uuid:\n\nimport uuid\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(4,3), columns=list('abc'),\n                  index=['apple', 'banana', 'cherry', 'date'])\ndf['uuid'] = uuid.uuid4()\nprint(df)\n\n               a         b         c                                  uuid\napple   0.687601 -1.332904 -0.166018  34115445-c4b8-4e64-bc96-e120abda1653\nbanana -2.252191 -0.844470  0.384140  34115445-c4b8-4e64-bc96-e120abda1653\ncherry -0.470388  0.642342  0.692454  34115445-c4b8-4e64-bc96-e120abda1653\ndate   -0.943255  1.450051 -0.296499  34115445-c4b8-4e64-bc96-e120abda1653\n\n\nWhat I am looking for is a new uuid in each row of the 'uuid' column. I have also tried using .apply() and .map() without success. \n"
'I have a Pandas Dataframe that has date values stored in 2 columns in the below format:\n\nColumn 1: 04-APR-2018 11:04:29\nColumn 2: 2018040415203 \n\n\nHow could I convert this to a time stamp. Datatype of both these column is Object.\n'
"I have a Dataframe that has a time stamp column of type 'datetime64[ns]'. When I try to insert it to Salesforce platform get an error 'TypeError: Object of type 'Timestamp' is not JSON serializable'. How could I change this timestamp column to have it updated properly. Given below is the view of the Dataframe.\n\nId,Name,Date,Type\n1,ProdA,2018-05-18 04:45:08,S\n1,ProdB,2018-05-18 02:15:00,S\n1,ProdC,2018-05-16 10:20:00,S\n\n\nDatatype for each of these 4 columns:\n\nId                                     object\nName                                   object\nDate                           datetime64[ns]\nType                                   object\ndtype: object\n\n\nCould anyone assist on this. Thanks.\n"
'I understand that Pandas can read and write to and from Parquet files using different backends: pyarrow and fastparquet.\n\nI have a Conda distribution with the Intel distribution and "it works": I can use pandas.DataFrame.to_parquet. However I do not have pyarrow installed so I guess that fastparquet is used (which I cannot find either).\n\nIs there a way to identify which backend is used?\n'
'I have a very simple function like this one:\n\nimport numpy as np\nfrom numba import jit\nimport pandas as pd\n\n@jit\ndef f_(n, x, y, z):\n    for i in range(n):\n        z[i] = x[i] * y[i] \n\nf_(df.shape[0], df["x"].values, df["y"].values, df["z"].values)\n\n\nTo which I pass\n\ndf = pd.DataFrame({"x": [1, 2, 3], "y": [3, 4, 5], "z": np.NaN})\n\n\nI expected that function will modify data z column in place like this:\n\n&gt;&gt;&gt; f_(df.shape[0], df["x"].values, df["y"].values, df["z"].values)\n&gt;&gt;&gt; df\n\n   x  y     z\n0  1  3   3.0\n1  2  4   8.0\n2  3  5  15.0\n\n\nThis works fine most of the time, but somehow fails to modify data in others.\n\nI double checked things and:\n\n\nI haven\'t determined any problems with data points which could cause this problem.\nI see that data is modified as expected when I print the result.\nIf I return z array from the function it is modified as expected.\n\n\nUnfortunately I couldn\'t reduce the problem to a minimal reproducible case. For example removing unrelated columns seems to "fix" the problem making reduction impossible. \n\nDo I use jit in a way that is not intended to be used? Are there any border cases I should be aware of? Or is it likely to be a bug?\n\nEdit:\n\nI found the source of the problem. It occurs when data contains duplicated column names:\n\n&gt;&gt;&gt; df_ = pd.read_json(\'{"schema": {"fields":[{"name":"index","type":"integer"},{"name":"v","type":"integer"},{"name":"y","type":"integer"},\n... {"name":"v","type":"integer"},{"name":"x","type":"integer"},{"name":"z","type":"number"}],"primaryKey":["index"],"pandas_version":"0.20.\n... 0"}, "data": [{"index":0,"v":0,"y":3,"v":0,"x":1,"z":null}]}\', orient="table")\n&gt;&gt;&gt; f_(df_.shape[0], df_["x"].values, df_["y"].values, df_["z"].values)\n&gt;&gt;&gt; df_\n   v  y  v  x   z\n0  0  3  0  1 NaN\n\n\nIf duplicate is removed the function works like expected:\n\n&gt;&gt;&gt; df_.drop("v", axis="columns", inplace=True)\n&gt;&gt;&gt; f_(df_.shape[0], df_["x"].values, df_["y"].values, df_["z"].values)\n&gt;&gt;&gt; df_\n   y  x    z\n0  3  1  3.0\n\n'
"I'm using AWS Athena to query raw data from S3. Since Athena writes the query output into S3 output bucket I used to do:\n\ndf = pd.read_csv(OutputLocation)\n\n\nBut this seems like an expensive way. Recently I noticed the get_query_results method of boto3 which returns a complex dictionary of the results. \n\nclient = boto3.client('athena')\nresponse = client.get_query_results(\n        QueryExecutionId=res['QueryExecutionId']\n        )\n\n\nI'm facing two main issues:\n\n\nHow can I format the results of get_query_results into pandas data frame?\nget_query_results only returns 1000 rows. How can I use it to get two million rows? \n\n"
"I want to select rows from a dataframe based on values in the index combined with values in a specific column:\n\ndf = pd.DataFrame([[0, 2, 3], [0, 4, 1], [0, 20, 30], [40, 20, 30]], \n                  index=[4, 5, 6, 7], columns=['A', 'B', 'C'])\n\n\n    A   B   C\n4   0   2   3\n5   0   4   1\n6   0  20  30\n7  40  20  30\n\n\nwith\n\ndf.loc[df['A'] == 0, 'C'] = 99\n\n\ni can select all rows with column A = 0 and replace the value in column C with 99, but how can i select all rows with column A = 0 and the index &lt; 6 (i want to combine selection on the index with selection on the column)?\n"
"I want the use the index of a pandas DataFrame as x value for seaborn and is raised a value error.\nA small test example:\n\nimport pandas as pd\nimport seaborn as sns\nsns.lineplot(x='index',y='test',hue='test2',data=pd.DataFrame({'test':range(9),'test2':range(9)}))\n\n\nIt raises:\n\nValueError: Could not interpret input 'index'\n\n\nis it not possible to use the index as x values? What am I doing wrong?\nPython 2.7, seaborn 0.9\n"
"I have a Dataframe which I want to transform into a multidimensional array using one of the columns as the 3rd dimension.\nAs an example:\n\ndf = pd.DataFrame({\n'id': [1, 2, 2, 3, 3, 3],\n'date': np.random.randint(1, 6, 6),\n'value1': [11, 12, 13, 14, 15, 16],\n'value2': [21, 22, 23, 24, 25, 26]\n })\n\n\n\n\nI would like to transform it into a 3D array with dimensions (id, date, values) like this:\n\nThe problem is that the 'id's do not have the same number of occurrences so I cannot use np.reshape().  \n\nFor this simplified example, I was able to use:  \n\nra = np.full((3, 3, 3), np.nan)\n\nfor i, value in enumerate(df['id'].unique()):\n    rows = df.loc[df['id'] == value].shape[0]\n    ra[i, :rows, :] = df.loc[df['id'] == value, 'date':'value2']\n\n\nTo produce the needed result:\n\nbut the original DataFrame contains millions of rows.\n\nIs there a vectorized way to accomplice the same result?\n"
"I want to use the apply function that:\n- Takes 2 columns as inputs - Outputs two new columns based on a function.\n\nAn example is with this add_multiply function. \n\n#function with 2 column inputs and 2 outputs\ndef add_multiply (a,b):\n  return (a+b, a*b )\n\n#example dataframe\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n\n#this doesn't work\ndf[['add', 'multiply']] = df.apply(lambda x: add_multiply(x['col1'], x['col2']), axis=1)\n\n\nideal result: \n\ncol1  col2  add  multiply\n1     3     4    3\n2     4     6    8\n\n\n"
"I have a reference list\n\nref = ['September', 'August', 'July', 'June', 'May', 'April', 'March']\n\n\nAnd a dataframe\n\ndf = pd.DataFrame({'Month_List': [['July'], ['August'], ['July', 'June'], ['May', 'April', 'March']]})\ndf\n    Month_List\n0   [July]\n1   [August]\n2   [July, June]\n3   [May, April, March]\n\n\nI want to check which elements from reference list is present in each row, and convert into binary list\n\nI can achieve this using apply\n\ndef convert_month_to_binary(ref,lst):\n    s = pd.Series(ref)\n    return s.isin(lst).astype(int).tolist()  \n\ndf['Binary_Month_List'] = df['Month_List'].apply(lambda x: convert_month_to_binary(ref, x))\ndf\n\n    Month_List          Binary_Month_List\n0   [July]              [0, 0, 1, 0, 0, 0, 0]\n1   [August]            [0, 1, 0, 0, 0, 0, 0]\n2   [July, June]        [0, 0, 1, 1, 0, 0, 0]\n3   [May, April, March] [0, 0, 0, 0, 1, 1, 1]\n\n\nHowever, using apply on large datasets is very slow and hence I am looking to use numpy vectorization. How can I improve my performance?\n\nExtension:\n\nI wanted to use numpy vectorization because I need to now apply another function on this list\n\nI am trying like this, but performance is very slow. Similar results with apply\n\ndef count_one(lst):\n    index = [i for i, e in enumerate(lst) if e != 0] \n    return len(index)\n\nvfunc = np.vectorize(count_one)\ndf['Value'] = vfunc(df['Binary_Month_List']) \n\n"
'I\'d like to store JSON data in a Python Pandas DataFrame\n\nmy JSON data is a dict of dicts of dicts like this\n\nd = {\n  "col1": {\n    "row1": {\n      "data1": "0.87", \n      "data2": "Title col1", \n      "data3": "14.4878", \n      "data4": "Title row1"\n    }, \n    "row2": {\n      "data1": "15352.3", \n      "data2": "Title col1", \n      "data3": "14.9561", \n      "data4": "Title row2"\n    }, \n    "row3": {\n      "data1": "0", \n      "data2": "Title col1", \n      "data3": "16.8293", \n      "data4": "Title row3"\n    }\n  }, \n  "col2": {\n    "row1": {\n      "data1": "0.87", \n      "data2": "Title col2", \n      "data3": "24.4878", \n      "data4": "Title row1"\n    }, \n    "row2": {\n      "data1": "15352.3", \n      "data2": "Title col2", \n      "data3": "24.9561", \n      "data4": "Title row2"\n    }, \n    "row3": {\n      "data1": "0", \n      "data2": "Title col2", \n      "data3": "26.8293", \n      "data4": "Title row3"\n    }\n  }\n}\n\n\nI did this to put my data in a DataFrame\n\nimport pandas as pd\ndf=pd.DataFrame(d)\n\n\nI get this\n\nIn [1]: df\nOut[1]: \n                                                   col1                                               col2\nrow1  {\'data4\': \'Title col1\', \'data1\': \'0.87\', \'data3\':  {\'data4\': \'Title col1\', \'data1\': \'0.87\', \'data3\':\nrow2  {\'data4\': \'Title col2\', \'data1\': \'15352.3\', \'data  {\'data4\': \'Title col2\', \'data1\': \'15352.3\', \'data\nrow3  {\'data4\': \'Title col3\', \'data1\': \'0\', \'data3\': \'1  {\'data4\': \'Title col3\', \'data1\': \'0\', \'data3\': \'2\n\n\nMy problem is that my DataFrame contains dicts instead of values.\n\nI wonder how I can manage multidimensionnal data (more than 2 dimensions... 3 dimensions here) with a Pandas DataFrame.\n\nEach dict inside DataFrame have the same keys.\n'
"I have a pandas Series which presently looks like this:\n\n14    [Yellow, Pizza, Restaurants]\n...\n160920                  [Automotive, Auto Parts &amp; Supplies]\n160921       [Lighting Fixtures &amp; Equipment, Home Services]\n160922                 [Food, Pizza, Candy Stores]\n160923           [Hair Removal, Nail Salons, Beauty &amp; Spas]\n160924           [Hair Removal, Nail Salons, Beauty &amp; Spas]\n\n\nAnd I want to radically reshape it into a dataframe that looks something like this...\n\n      Yellow  Automotive  Pizza\n14       1         0        1\n…           \n160920   0         1        0\n160921   0         0        0\n160922   0         0        1\n160923   0         0        0\n160924   0         0        0\n\n\nie. a logical construction noting which categories each observation(row) falls into.\n\nI'm capable of writing for loop based code to tackle the problem, but given the large number of rows I need to handle, that's going to be very slow. \n\nDoes anyone know a vectorised solution to this kind of problem? I'd be very grateful.\n\nEDIT: there are 509 categories, which I do have a list of.\n"
'Is it possible to delete a group (by group name) from a groupby object in pandas? That is, after performing a groupby, delete a resulting group based on its name.\n'
'First off, I\'m a novice... I\'m a newbie to Python, pandas, and Linux.  \n\nI\'m getting some errors when trying to populate a DataFrame (sql.read_frame() gives an exception when trying to read from my MySQL DB, but I am able to execute and fetch a query / stored proc).  I noticed that pandas is at version 0.7.0, and running "sudo apt-get install python-pandas" just says that it\'s up to date (no errors): "... python-pandas is already the newest version. 0 upgraded..."\n\nBased on some other posts I found on the web, I think my DataFrame problem may be due to the older version of pandas (something about a pandas bug involving tuples of tuples?).  Why won\'t pandas update to a more current version? \n\nSetup:\n\nUbuntu: 12.04.2 LTS Desktop (virtual workstation on VMWare)\nsudo apt-get update, sudo apt-get upgrade, and sudo apt-get dist-upgrade all current\nPython: 2.7.3 (default, April 10 2013, 06:20:15) /n [GCC 4.6.3] on Linux2\n$ "which python" only show a single instance: /usr/bin/python\npandas.__version__ = 0.7.0\nnumpy.__version__ = 1.6.1\n\n\nI tried installing Anaconda previously, but that turned into a big nightmare, with conflicting versions of Python.  I finally rolled back to previous VM snapshot and started over, installing all of the MySQL, pandas, and iPython using apt-get on the individual packages.  \n\nI\'m not having any other problems on this workstation... apt-get seems to be working fine in general, and all other apps (MySQL Workbench, Kettle / spoon, etc.) are all working properly and up to date.  \n\nAny ideas why Python pandas won\'t upgrade to 0.11.0?  Thank you.\n'
"Pandas has a nice interface that facilitates storing things like Dataframes and Series in an HDF5:\n\nrandom_matrix  = np.random.random_integers(0,10, m_size)\nmy_dataframe =  pd.DataFrame(random_matrix)\n\nstore = pd.HDFStore('some_file.h5',complevel=9, complib='bzip2')\nstore['my_dataframe'] = my_dataframe\nstore.close()\n\n\nBut if I try to save some other regular Python objects in the same file, it complains:\n\nmy_dictionary = dict()\nmy_dictionary['a'] = 2           # &lt;--- ERROR\nmy_dictionary['b'] = [2,3,4]\n\nstore['my_dictionary'] = my_dictionary\nstore.close()\n\n\nwith \n\nTypeError: cannot properly create the storer for: [_TYPE_MAP] [group-&gt;/par\nameters (Group) u'',value-&gt;&lt;type 'dict'&gt;,table-&gt;None,append-&gt;False,kwargs-\n&gt;{}]                                   \n\n\nHow can I store regular Python data structures in the same HDF5 where I store other Pandas objects                                 ?\n"
"I have a pandas DataFrame df:\n\n+------+---------+  \n| team | user    |  \n+------+---------+  \n| A    | elmer   |  \n| A    | daffy   |  \n| A    | bugs    |  \n| B    | dawg    |  \n| A    | foghorn |  \n| B    | speedy  |  \n| A    | goofy   |  \n| A    | marvin  |  \n| B    | pepe    |  \n| C    | petunia |  \n| C    | porky   |  \n+------+---------  \n\n\nI want to find or write a function to return a DataFrame that I would return in MySQL using the following:\n\nSELECT\n  team,\n  GROUP_CONCAT(user)\nFROM\n  df\nGROUP BY\n  team\n\n\nfor the following result:\n\n+------+---------------------------------------+  \n| team | group_concat(user)                    |  \n+------+---------------------------------------+  \n| A    | elmer,daffy,bugs,foghorn,goofy,marvin |  \n| B    | dawg,speedy,pepe                      |  \n| C    | petunia,porky                         |  \n+------+---------------------------------------+  \n\n\nI can think of nasty ways to do this by iterating over rows and adding to a dictionary, but there's got to be a better way.\n"
"I have the table below in a Pandas DataFrame:\n\n    q_string    q_visits    q_date\n0   nucleus         1790        2012-10-02 00:00:00\n1   neuron          364         2012-10-02 00:00:00\n2   current         280         2012-10-02 00:00:00\n3   molecular       259         2012-10-02 00:00:00\n4   stem            201         2012-10-02 00:00:00\n\n\nThe table contains query volume from a server log, by day. I would like to do 2 things:\n\n\nI would like to group queries by month summing the query volume of a query for the whole month e.g. if 'molecular' was present on the 2012-10-02 with volume 1000 and on the 2012-10-03 with volume 500, then it should have an entry in the new table of 1500 (volume) with date 2012-10-31 (end of the month end-point representing the month &ndash; all dates in the transformed table will be month ends representing the whole month to which they relate).\nI want to add a 5th column which contains the month-normalized q_visits.  I.e., a term's monthly query volume divided by the total query volume for the month across all terms.\n\n\nWhat is the best way of doing this?\n"
"I have a pandas dataFrame like this:\n\n                     content\ndate                        \n2013-12-18 12:30:00        1\n2013-12-19 10:50:00        1\n2013-12-24 11:00:00        0\n2014-01-02 11:30:00        1\n2014-01-03 11:50:00        0\n2013-12-17 16:40:00       10\n2013-12-18 10:00:00        0\n2013-12-11 10:00:00        0\n2013-12-18 11:45:00        0\n2013-12-11 14:40:00        4\n2010-05-25 13:05:00        0\n2013-11-18 14:10:00        0\n2013-11-27 11:50:00        3\n2013-11-13 10:40:00        0\n2013-11-20 10:40:00        1\n2008-11-04 14:49:00        1\n2013-11-18 10:05:00        0\n2013-08-27 11:00:00        0\n2013-09-18 16:00:00        0\n2013-09-27 11:40:00        0\n\n\ndate being the index.\nI reduce the values to months using:\n\ndataFrame = dataFrame.groupby([lambda x: x.year, lambda x: x.month]).agg([sum])\n\n\nwhich outputs:\n\n         content\n             sum\n2006 3        66\n     4        65\n     5        48\n     6        87\n     7        37\n     8        54\n     9        73\n     10       74\n     11       53\n     12       45\n2007 1        28\n     2        40\n     3        95\n     4        63\n     5        56\n     6        66\n     7        50\n     8        49\n     9        18\n     10       28\n\n\nNow when I plot this dataFrame, I want the x-axis show every month/year as a tick. I have tries setting xticks but it doesn't seem to work. How could this be achieved? This is my current plot using dataFrame.plot():\n\n\n"
"I used to read my data with numpy.loadtxt(). However, lately I found out in SO, that pandas.read_csv() is much more faster.\n\nTo read these data I use: \n\npd.read_csv(filename, sep=' ',header=None)\n\n\nThe problem that I encounter right now is that in my case the separator can differ from one space, x spaces to even a tab. \n\nHere how my data could look like:\n\n56.00     101.85 52.40 101.85 56.000000 101.850000 1\n56.00 100.74 50.60 100.74 56.000000 100.740000 2\n56.00 100.74 52.10 100.74 56.000000 100.740000 3\n56.00 102.96 52.40 102.96 56.000000 102.960000 4\n56.00 100.74 55.40 100.74 56.000000 100.740000 5\n\n\nThat leads to results like:\n\n     0       1     2       3     4       5   6       7   8\n0   56     NaN   NaN  101.85  52.4  101.85  56  101.85   1\n1   56  100.74  50.6  100.74  56.0  100.74   2     NaN NaN\n2   56  100.74  52.1  100.74  56.0  100.74   3     NaN NaN\n3   56  102.96  52.4  102.96  56.0  102.96   4     NaN NaN\n4   56  100.74  55.4  100.74  56.0  100.74   5     NaN NaN\n\n\nI have to specify that my data are >100 MB. So I can not preprocess the data or clean them first.\nAny ideas how to get this fixed?\n"
"I am using Seaborn to plot some data in Pandas.\n\nI am making some very large plots (factorplots).\n\nTo see them, I am using some visualisation facilities at my university.\nI am using a Compound screen made up of 4 by 4 monitors with small (but nonzero) bevel -- the gap between the screens. \nThis gap is black.\nTo minimise the disconnect between the screen i want the graph backgound to be black.\nI have been digging around the documentation and playing around and I can't work it out..\nSurely this is simple.\n\nI can get grey background using set_style('darkgrid')\n\ndo i need to access the plot in matplotlib directly?\n"
'How do I convert this dataframe\n\n\n                                          location  value                       \n0                   (Richmond, Virginia, nan, USA)    100                       \n1              (New York City, New York, nan, USA)    200                       \n\n\n\nto this:\n\n\n    city            state       region    country   value\n0   Richmond        Virginia    nan       USA       100\n1   New York City   New York    nan       USA       200\n\n\n\nNote that the location column in the first dataframe contains tuples. I want to create four columns out of the location column.\n'
'When saving a Pandas DataFrame to csv, some integers are getting converted in floats.\nIt happens where a column of floats has missing values (np.nan). \n\nIs there a simple way to avoid it?\n(Especially in an automatic way - I often deal with many columns of various data types.)\n\nFor example\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1,2],[3,np.nan],[5,6]],\n                  columns=["a","b"],\n                  index=["i_1","i_2","i_3"])\ndf.to_csv("file.csv")\n\n\nyields\n\n,a,b\ni_1,1,2.0\ni_2,3,\ni_3,5,6.0\n\n\nWhat I would like to get is\n\n,a,b\ni_1,1,2\ni_2,3,\ni_3,5,6\n\n\nEDIT: I am fully aware of Support for integer NA - Pandas Caveats and Gotchas. The question is what is a nice workaround (especially in case if there are many other columns of various types and I do not know in advance which "integer" columns have missing values).\n'
'I am pretty new to Pandas and trying to find out where my code breaks. Say, I am doing a type conversion:\n\ndf[\'x\']=df[\'x\'].astype(\'int\')\n\n\n...and I get an error "ValueError: invalid literal for long() with base 10: \'1.0692e+06\'\n\nIn general, if I have 1000 entries in the dataframe, how can I find out what entry causes a break. Is there anything in ipdb to output the current location (i.e. where the code broke)? Basically, I am trying to pinpoint what value cannot be converted to Int.\n'
"I have the following dataframe:\n\n     col\n0    pre\n1    post\n2    a\n3    b\n4    post\n5    pre\n6    pre\n\n\nI want to replace all rows in the dataframe which do not contain 'pre' to become 'nonpre', so dataframe looks like:        \n\n     col\n0    pre\n1    nonpre\n2    nonpre\n3    nonpre\n4    nonpre\n5    pre\n6    pre\n\n\nI can do this using a dictionary and pandas replace, however I want to just select the elements which are not 'pre' and replace them with 'nonpre'. is there a better way to do that without listing all possible col values in a dictionary?\n"
'I can pass a StringIO object to pd.to_csv() just fine:\n\nio = StringIO.StringIO()\npd.DataFrame().to_csv(io)\n\n\nBut when using the excel writer, I am having a lot more trouble. \n\nio = StringIO.StringIO()\nwriter = pd.ExcelWriter(io)\npd.DataFrame().to_excel(writer,"sheet name")\nwriter.save()   \n\n\nReturns an \n\nAttributeError: StringIO instance has no attribute \'rfind\'\n\n\nI\'m trying to create an ExcelWriter object without calling pd.ExcelWriter() but am having some trouble. This is what I\'ve tried so far:\n\nfrom xlsxwriter.workbook import Workbook\nwriter = Workbook(io)\npd.DataFrame().to_excel(writer,"sheet name")\nwriter.save()\n\n\nBut now I am getting an AttributeError: \'Workbook\' object has no attribute \'write_cells\' \n\nHow can I save a pandas dataframe in excel format to a StringIO object?\n'
"I have a df that looks like this:\n\ndf = pd.DataFrame(np.random.random((4,4)))\ndf.columns = pd.MultiIndex.from_product([['1','2'],['A','B']])\nprint df\n          1                   2          \n          A         B         A         B\n0  0.030626  0.494912  0.364742  0.320088\n1  0.178368  0.857469  0.628677  0.705226\n2  0.886296  0.833130  0.495135  0.246427\n3  0.391352  0.128498  0.162211  0.011254\n\n\nHow can I rename column '1' and '2' as 'One' and 'Two'? \n\nI thought df.rename() would've helped but it doesn't. Have no idea how to do this?\n"
'I\'m trying to visualize a pandas dataframe as a heatmap, and am getting weird errors with all plotting functions I try (I tried with both the DataFrame object, and DataFrame.values array, and nothing changes). I don\'t understand what could be the reason for it. This is the dataframe:\n\n       1     2     3     4     5     6     7     8     9    10    11    12  \\\n1      0  1163   986  1105  1315  1472   844   560  1033   867   610   703   \n2   1163     0  1774   803  1091   899   704   806   891   648  1082  1199   \n3    986  1774     0   679   880   798  1268   931   560  1128   774   481   \n4   1105   803   679     0   742   654   887   765  1113  1079   605   928   \n5   1315  1091   880   742     0   924   580   658  1073  1008   719   699   \n6   1472   899   798   654   924     0   619   991  1290  1002  1290   540   \n7    844   704  1268   887   580   619     0   639   611   717   812   469   \n8    560   806   931   765   658   991   639     0   788  1469   498   588   \n9   1033   891   560  1113  1073  1290   611   788     0   665  1172   621   \n10   867   648  1128  1079  1008  1002   717  1469   665     0   728  1060   \n11   610  1082   774   605   719  1290   812   498  1172   728     0  1146   \n12   703  1199   481   928   699   540   469   588   621  1060  1146     0   \n13  1193  1065  1163   458   364   741   713   663   777   529   492   548   \n14   909   902   894   369   439   768  1043   350   975  1058   630   558   \n15   877   957  1303   516   604   954   403   784   987   510  1126   617   \n16  1035   662   456   584   812  1049   856   856   684   358  1098   685   \n17  1191   496   770  1083   417   809   635   669   912   366   336   418   \n18  1118  1118   590   550   930   665   501   664   577   918   458   574   \n19   429   526   324   786   658   215   308   518   522   331   540   487   \nX   1333  1273   589   908   691   604  1243  1085   853   591   696   921   \n\n      13    14    15    16    17    18   19     X  \n1   1193   909   877  1035  1191  1118  429  1333  \n2   1065   902   957   662   496  1118  526  1273  \n3   1163   894  1303   456   770   590  324   589  \n4    458   369   516   584  1083   550  786   908  \n5    364   439   604   812   417   930  658   691  \n6    741   768   954  1049   809   665  215   604  \n7    713  1043   403   856   635   501  308  1243  \n8    663   350   784   856   669   664  518  1085  \n9    777   975   987   684   912   577  522   853  \n10   529  1058   510   358   366   918  331   591  \n11   492   630  1126  1098   336   458  540   696  \n12   548   558   617   685   418   574  487   921  \n13     0   807   504   672   421   446  726   847  \n14   807     0   408   616   304   781  270   715  \n15   504   408     0   495   331   614  239   577  \n16   672   616   495     0   199   324  358   597  \n17   421   304   331   199     0   622  678   303  \n18   446   781   614   324   622     0  501   373  \n19   726   270   239   358   678   501    0   237  \nX    847   715   577   597   303   373  237     0\n\n\nI tried different ways to plot it. For example, pcolor:\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-16-88566b82f0f0&gt; in &lt;module&gt;()\n      1 inter_chr = a.make_intrachromosomal_map(BD.dataDict[\'26769\'])\n----&gt; 2 plt.pcolor(inter_chr)\n      3 plt.show()\n\n/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc in pcolor(*args, **kwargs)\n   3016         ax.hold(hold)\n   3017     try:\n-&gt; 3018         ret = ax.pcolor(*args, **kwargs)\n   3019         draw_if_interactive()\n   3020     finally:\n\n/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc in pcolor(self, *args, **kwargs)\n   4965         collection.set_norm(norm)\n   4966         collection.set_clim(vmin, vmax)\n-&gt; 4967         collection.autoscale_None()\n   4968         self.grid(False)\n   4969 \n\n/usr/local/lib/python2.7/dist-packages/matplotlib/cm.pyc in autoscale_None(self)\n    333         if self._A is None:\n    334             raise TypeError(\'You must first set_array for mappable\')\n--&gt; 335         self.norm.autoscale_None(self._A)\n    336         self.changed()\n    337 \n\n/usr/local/lib/python2.7/dist-packages/matplotlib/colors.pyc in autoscale_None(self, A)\n    952         \' autoscale only None-valued vmin or vmax\'\n    953         if self.vmin is None and np.size(A) &gt; 0:\n--&gt; 954             self.vmin = ma.min(A)\n    955         if self.vmax is None and np.size(A) &gt; 0:\n    956             self.vmax = ma.max(A)\n\n/usr/local/lib/python2.7/dist-packages/numpy/ma/core.pyc in min(obj, axis, out, fill_value)\n   6025         # If obj doesn\'t have a min method,\n   6026         # ...or if the method doesn\'t accept a fill_value argument\n-&gt; 6027         return asanyarray(obj).min(axis=axis, fill_value=fill_value, out=out)\n   6028 min.__doc__ = MaskedArray.min.__doc__\n   6029 \n\n/usr/local/lib/python2.7/dist-packages/numpy/ma/core.pyc in min(self, axis, out, fill_value)\n   5179         # No explicit output\n   5180         if out is None:\n-&gt; 5181             result = self.filled(fill_value).min(axis=axis, out=out).view(type(self))\n   5182             if result.ndim:\n   5183                 # Set the mask\n\nAttributeError: \'int\' object has no attribute \'view\'\n\n\nOr imshow:\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-17-1d4365bb4ce8&gt; in &lt;module&gt;()\n      1 inter_chr = a.make_intrachromosomal_map(BD.dataDict[\'26769\'])\n----&gt; 2 plt.imshow(inter_chr)\n      3 plt.show()\n\n/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.pyc in imshow(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, hold, **kwargs)\n   2959                         vmax=vmax, origin=origin, extent=extent, shape=shape,\n   2960                         filternorm=filternorm, filterrad=filterrad,\n-&gt; 2961                         imlim=imlim, resample=resample, url=url, **kwargs)\n   2962         draw_if_interactive()\n   2963     finally:\n\n/usr/local/lib/python2.7/dist-packages/matplotlib/axes/_axes.pyc in imshow(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\n   4642                        filterrad=filterrad, resample=resample, **kwargs)\n   4643 \n-&gt; 4644         im.set_data(X)\n   4645         im.set_alpha(alpha)\n   4646         if im.get_clip_path() is None:\n\n/usr/local/lib/python2.7/dist-packages/matplotlib/image.pyc in set_data(self, A)\n    432         if (self._A.dtype != np.uint8 and\n    433             not np.can_cast(self._A.dtype, np.float)):\n--&gt; 434             raise TypeError("Image data can not convert to float")\n    435 \n    436         if (self._A.ndim not in (2, 3) or\n\nTypeError: Image data can not convert to float\n\n\nAnd seaborn.heatmap gives this:\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-18-7ce156cf69d4&gt; in &lt;module&gt;()\n      1 inter_chr = a.make_intrachromosomal_map(BD.dataDict[\'26769\'])\n----&gt; 2 sns.heatmap(inter_chr)\n      3 plt.show()\n\n/usr/local/lib/python2.7/dist-packages/seaborn/matrix.pyc in heatmap(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, ax, xticklabels, yticklabels, mask, **kwargs)\n    330     plotter = _HeatMapper(data, vmin, vmax, cmap, center, robust, annot, fmt,\n    331                           annot_kws, cbar, cbar_kws, xticklabels, yticklabels,\n--&gt; 332                           mask)\n    333 \n    334     # Add the pcolormesh kwargs here\n\n/usr/local/lib/python2.7/dist-packages/seaborn/matrix.pyc in __init__(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\n    145         # Determine good default values for the colormapping\n    146         self._determine_cmap_params(plot_data, vmin, vmax,\n--&gt; 147                                     cmap, center, robust)\n    148 \n    149         # Save other attributes to the object\n\n/usr/local/lib/python2.7/dist-packages/seaborn/matrix.pyc in _determine_cmap_params(self, plot_data, vmin, vmax, cmap, center, robust)\n    159                                cmap, center, robust):\n    160         """Use some heuristics to set good defaults for colorbar and range."""\n--&gt; 161         calc_data = plot_data.data[~np.isnan(plot_data.data)]\n    162         if vmin is None:\n    163             vmin = np.percentile(calc_data, 2) if robust else calc_data.min()\n\nTypeError: ufunc \'isnan\' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule \'\'safe\'\'\n\n\nSo, it\'s some kind of type problem, but I can\'t see where it comes from. I tried putting both int and float values into the DF, but nothing changes.\n'
'I have a data file, apples.csv, that has headers like:\n\n"id","str1","str2","str3","num1","num2"\n\n\nI read it into a dataframe with pandas:\n\napples = pd.read_csv(\'apples.csv\',delimiter=",",sep=r"\\s+")\n\n\nThen I do some stuff to it, but ignore that (I have it all commented out, and my overall issues still occurs, so said stuff is irrelevant here).\n\nI then save it out:\n\napples.to_csv(\'bananas.csv\',columns=["id","str1","str2","str3","num1","num2"])\n\n\nNow, looking at bananas.csv, its headers are:\n\n,id,str1,str2,str3,num1,num2\n\n\nNo more quotes (which I don\'t really care about, as it doesn\'t impact anything in the file), and then that leading comma. \nThe ensuing rows are now with an additional column in there, so it saves out 7 columns. But if I do:\n\nprint(len(apples.columns))\n\n\nImmediately prior to saving, it shows 6 columns...\n\nI am normally in Java/Perl/R, and less experienced with Python and particularly Pandas, so I am not sure if this is "yeah, it just does that" or what the issue is - but I have spent amusingly long trying to figure this out and cannot find it via searching.\n\nHow can I get it to not do that prepending of a comma, and maybe as important - why is it doing it?\n'
"I have a pandas.dataframe like this ('col' column has two formats):\n\n    col                            val\n'12/1/2013'                       value1\n'1/22/2014 12:00:01 AM'           value2\n'12/10/2013'                      value3\n'12/31/2013'                      value4 \n\n\nI want to convert them into datetime, and I am considering using:\n\ntest_df['col']= test_df['col'].map(lambda x: datetime.strptime(x, '%m/%d/%Y'))    \ntest_df['col']= test_df['col'].map(lambda x: datetime.strptime(x, '%m/%d/%Y %H:%M %p'))\n\n\nObviously either of them works for the whole df. I'm thinking about using try and except but didn't get any luck, any suggestions?\n"
'Is it possible to convert frequencies represented by string such as "30T" (30 minutes) "2S" (2 seconds) to something that can be compared to a timedelta?\n\nI am looking for a mechanism internal to pandas if possible. Coding all possible string conversion using mechanism such as these would not be robust.\n'
'It seems strange to me that pandas.read_csv is not a direct reciprocal function to df.to_csv.  In this illustration, notice how when using all the default settings the original and final DataFrames differ by the "Unnamed" column.\n\nIn [1]: import pandas as pd\n\nIn [2]: orig_df = pd.DataFrame({\'AAA\' : [4,5,6,7], \'BBB\' : [10,20,30,40],\'CCC\' : [100,50,-30,-50]}); orig_df\nOut[2]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\n[4 rows x 3 columns]\n\nIn [3]: orig_df.to_csv(\'test.csv\')\n\nIn [4]: final_df = pd.read_csv(\'test.csv\'); final_df\nOut[4]: \n   Unnamed: 0  AAA  BBB  CCC\n0           0    4   10  100\n1           1    5   20   50\n2           2    6   30  -30\n3           3    7   40  -50\n\n[4 rows x 4 columns]\n\n\nIt seems the default read_csv should instead be \n\nIn [6]: final2_df = pd.read_csv(\'test.csv\', index_col=0); final2_df\nOut[7]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\n[4 rows x 3 columns]\n\n\nor the default to_csv should instead be\n\nIn [8]: df.to_csv(\'test2.csv\', index=False)\n\n\nwhich when read gives\n\nIn [9]: pd.read_csv(\'test2.csv\')\nOut[9]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\n\n[4 rows x 3 columns]\n\n(Perhaps this should instead be sent to the developer/s but I am genuinely interested why this is the default behavior.  Hopefully it also can help someone else avoid the confusion I had).\n'
'I have a number Pandas Series with 601 rows indexed by date as seen below. The values are zero up until a point, after which all the values are non zero. This point varies with each Series but I would like a way to remove all the rows where the value is zero while keeping the integrity of the date index.\n\nName: users, dtype: float64 dates\n2015-08-17 14:29:59-04:00    18\n2015-08-16 14:29:59-04:00     3\n2015-08-15 14:29:59-04:00    11\n2015-08-14 14:29:59-04:00    12\n2015-08-13 14:29:59-04:00     8\n2015-08-12 14:29:59-04:00    10\n2015-08-11 14:29:59-04:00     6\n2015-08-10 14:29:59-04:00     6\n2015-08-09 14:29:59-04:00     7\n2015-08-08 14:29:59-04:00     7\n2015-08-07 14:29:59-04:00    13\n2015-08-06 14:29:59-04:00    16\n2015-08-05 14:29:59-04:00    12\n2015-08-04 14:29:59-04:00    14\n2015-08-03 14:29:59-04:00     5\n2015-08-02 14:29:59-04:00     5\n2015-08-01 14:29:59-04:00     8\n2015-07-31 14:29:59-04:00     6\n2015-07-30 14:29:59-04:00     7\n2015-07-29 14:29:59-04:00     9\n2015-07-28 14:29:59-04:00     7\n2015-07-27 14:29:59-04:00     5\n2015-07-26 14:29:59-04:00     4\n2015-07-25 14:29:59-04:00     8\n2015-07-24 14:29:59-04:00     8\n2015-07-23 14:29:59-04:00     8\n2015-07-22 14:29:59-04:00     9\n2015-07-21 14:29:59-04:00     5\n2015-07-20 14:29:59-04:00     7\n2015-07-19 14:29:59-04:00     6\n                             ..\n2014-01-23 13:29:59-05:00     0\n2014-01-22 13:29:59-05:00     0\n2014-01-21 13:29:59-05:00     0\n2014-01-20 13:29:59-05:00     0\n2014-01-19 13:29:59-05:00     0\n2014-01-18 13:29:59-05:00     0\n2014-01-17 13:29:59-05:00     0\n2014-01-16 13:29:59-05:00     0\n2014-01-15 13:29:59-05:00     0\n2014-01-14 13:29:59-05:00     0\n2014-01-13 13:29:59-05:00     0\n2014-01-12 13:29:59-05:00     0\n2014-01-11 13:29:59-05:00     0\n2014-01-10 13:29:59-05:00     0\n2014-01-09 13:29:59-05:00     0\n2014-01-08 13:29:59-05:00     0\n2014-01-07 13:29:59-05:00     0\n2014-01-06 13:29:59-05:00     0\n2014-01-05 13:29:59-05:00     0\n2014-01-04 13:29:59-05:00     0\n2014-01-03 13:29:59-05:00     0\n2014-01-02 13:29:59-05:00     0\n2014-01-01 13:29:59-05:00     0\n2013-12-31 13:29:59-05:00     0\n2013-12-30 13:29:59-05:00     0\n2013-12-29 13:29:59-05:00     0\n2013-12-28 13:29:59-05:00     0\n2013-12-27 13:29:59-05:00     0\n2013-12-26 13:29:59-05:00     0\n2013-12-25 13:29:59-05:00     0\n\n'
"Try 1:\n\ndf[ df &gt; 1.0 ] : this returned all cells in NAN.\n\nTry2:\n\ndf.loc[ df &gt; 1.0 ] : this returned KeyError: 0\n\ndf[df['A']&gt; 1.0] : this works - But I want to apply the filter condition to all columns.\n"
"I'm iterating through a dataframe (called hdf) and applying changes on a row by row basis. hdf is sorted by group_id and assigned a 1 through n rank on some criteria.\n\n# Groupby function creates subset dataframes (a dataframe per distinct group_id).\ngrouped = hdf.groupby('group_id')\n\n# Iterate through each subdataframe. \nfor name, group in grouped:\n\n    # This grabs the top index for each subdataframe\n    index1 = group[group['group_rank']==1].index\n\n    # If criteria1 == 0, flag all rows for removal\n    if(max(group['criteria1']) == 0):    \n        for x in range(rank1, rank1 + max(group['group_rank'])):\n            hdf.loc[x,'remove_row'] = 1\n\n\nI'm getting the following error:\n\nTypeError: int() argument must be a string or a number, not 'Int64Index'\n\n\nI get the same error when I try to cast rank1 explicitly I get the same error:\n\nrank1 = int(group[group['auction_rank']==1].index)\n\n\nCan someone explain what is happening and provide an alternative?\n"
'Pandas df.describe() is a very useful method to have an overview of your df. However, it describes by columns and I would like to have an overview of the rows instead. Is there any way to make it work "by_row" without transposing the df?\n'
"There is a good number of questions about this error, but after looking around I'm still not able to find/wrap my mind around a solution yet.\nI'm trying to pivot a data frame with strings, to get some row data to become columns, but not working out so far.\n\nShape of my df \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 515932 entries, 0 to 515931\nData columns (total 5 columns):\nid                 515932 non-null object\ncc_contact_id      515932 non-null object\nNetwork_Name       515932 non-null object\nquestion           515932 non-null object\nresponse_answer    515932 non-null object\ndtypes: object(5)\nmemory usage: 23.6+ MB\n\n\nSample format\n\nid  contact_id  question    response_answer\n16  137519  2206    State   Ca\n17  137520  2206    State   Ca\n18  137521  2206    State   Ca\n19  137522  2206    State   Ca\n20  137523  2208    City    Lancaster\n21  137524  2208    City    Lancaster\n22  137525  2208    City    Lancaster\n23  137526  2208    City    Lancaster\n24  137527  2208    Trip_End Location   Home\n25  137528  2208    Trip_End Location   Home\n26  137529  2208    Trip_End Location   Home\n27  137530  2208    Trip_End Location   Home\n\n\nWhat I would like to pivot to\n\nid  contact_id      State   City       Trip_End Location\n16  137519  2206    Ca      None       None None\n20  137523  2208    None    Lancaster  None None\n24  137527  2208    None    None       None Home\netc. etc. \n\n\nWhere the question values become the columns, with the response_answer being in it's corresponding column, and retaining the ids\n\nWhat I have tried\n\nunified_df = pd.DataFrame(unified_data, columns=target_table_headers, dtype=object)\n\npivot_table = unified_df.pivot_table('response_answer',['id','cc_contact_id'],'question')\n# OR\npivot_table = unified_df.pivot_table('response_answer','question')\n\n\nDataError: No numeric types to aggregate\n\nWhat is the way to pivot a data frame with string values?\n"
'If we have a Pandas data frame consisting of a column of categories and a column of values, we can remove the mean in each category by doing the following:\n\ndf["DemeanedValues"] = df.groupby("Category")["Values"].transform(lambda g: g - numpy.mean(g))\n\n\nAs far as I understand, Spark dataframes do not directly offer this group-by/transform operation (I am using PySpark on Spark 1.5.0). So, what is the best way to implement this computation? \n\nI have tried using a group-by/join as follows:\n\ndf2 = df.groupBy("Category").mean("Values")\ndf3 = df2.join(df)\n\n\nBut it is very slow since, as I understand, each category requires a full scan of the DataFrame. \n\nI think (but have not verified) that I can speed this up a great deal if I collect the result of the group-by/mean into a dictionary, and then use that dictionary in a UDF as follows:\n\nnameToMean = {...}\nf = lambda category, value: value - nameToMean[category]\ncategoryDemeaned = pyspark.sql.functions.udf(f, pyspark.sql.types.DoubleType())\ndf = df.withColumn("DemeanedValue", categoryDemeaned(df.Category, df.Value))\n\n\nIs there an idiomatic way to express this type of operation without sacrificing performance?\n'
"I apologize in advance for asking such a basic question but I am stumped.\n\nThis is a very simple, dummy example. I'm having some issue matching dates in Pandas and I can't figure out why.\n\ndf = pd.DataFrame([[1,'2016-01-01'], \n                   [2,'2016-01-01'],\n                   [3,'2016-01-02'],\n                   [4,'2016-01-03']],\n                   columns=['ID', 'Date'])\n\ndf['Date'] = df['Date'].astype('datetime64')\n\n\nSay I want to match row 1 in the above df.\nI know beforehand that I want to match ID 1.\nAnd I know the date I want as well, and as a matter of fact, I'll extract that date directly from row 1 of the df to make it bulletproof.\n\nsome_id = 1\nsome_date = df.iloc[1:2]['Date']  # gives 2016-01-01\n\n\nSo why doesn't this line work to return me row 1??\n\ndf[(df['ID']==some_id) &amp; (df['Date'] == some_date)] \n\n\nInstead I get \nValueError: Series lengths must match to compare\nwhich I understand, and makes sense...but leaves me wondering...how else can I compare dates in pandas if I can't compare one to many?\n"
'I\'ve got a DataFrame storing daily-based data which is as below:\n\nDate              Open        High         Low       Close   Volume\n2010-01-04   38.660000   39.299999   38.509998   39.279999  1293400   \n2010-01-05   39.389999   39.520000   39.029999   39.430000  1261400   \n2010-01-06   39.549999   40.700001   39.020000   40.250000  1879800   \n2010-01-07   40.090000   40.349998   39.910000   40.090000   836400   \n2010-01-08   40.139999   40.310001   39.720001   40.290001   654600   \n2010-01-11   40.209999   40.520000   40.040001   40.290001   963600   \n2010-01-12   40.160000   40.340000   39.279999   39.980000  1012800   \n2010-01-13   39.930000   40.669998   39.709999   40.560001  1773400   \n2010-01-14   40.490002   40.970001   40.189999   40.520000  1240600   \n2010-01-15   40.570000   40.939999   40.099998   40.450001  1244200   \n\n\nWhat I intend to do is to merge it into weekly-based data. After grouping:\n\n\nthe Date should be every Monday (at this point, holidays scenario should be considered when Monday is not a trading day, we should apply the first trading day in current week as the Date).\nOpen should be Monday\'s (or the first trading day of current week) Open.\nClose should be Friday\'s (or the last trading day of current week) Close.\nHigh should be the highest High of trading days in current week.\nLow should be the lowest Low of trading days in current week.\nVolumn should be the sum of all Volumes of trading days in current week.\n\n\nwhich should look like this:\n\nDate              Open        High         Low       Close   Volume\n2010-01-04   38.660000   40.700001   38.509998   40.290001  5925600   \n2010-01-11   40.209999   40.970001   39.279999   40.450001  6234600   \n\n\nCurrently, my code snippet is as below, which function should I use to mapping daily-based data to the expected weekly-based data? Many thanks!\n\nimport pandas_datareader.data as web\n\nstart = datetime.datetime(2010, 1, 1)\nend = datetime.datetime(2016, 12, 31)\nf = web.DataReader("MNST", "yahoo", start, end, session=session)\nprint f\n\n'
"I've got a data frame, df, with three columns: count_a, count_b and date; the counts are floats, and the dates are consecutive days in 2015.\n\nI'm trying to figure out the difference between each day's counts in both the count_a and count_b columns —\xa0meaning, I'm trying to calculate the difference between each row and the preceding row for both of those columns. I've set the date as the index, but am having trouble figuring out how to do this; there were a couple of hints about using pd.Series and pd.DataFrame.diff but I haven't had any luck finding an applicable answer or set of instructions. \n\nI'm a bit stuck, and would appreciate some guidance here. \n\nHere's what my data frame looks like: \n\ndf=pd.Dataframe({'count_a': {Timestamp('2015-01-01 00:00:00'): 34175.0,\n  Timestamp('2015-01-02 00:00:00'): 72640.0,\n  Timestamp('2015-01-03 00:00:00'): 109354.0,\n  Timestamp('2015-01-04 00:00:00'): 144491.0,\n  Timestamp('2015-01-05 00:00:00'): 180355.0,\n  Timestamp('2015-01-06 00:00:00'): 214615.0,\n  Timestamp('2015-01-07 00:00:00'): 250096.0,\n  Timestamp('2015-01-08 00:00:00'): 287880.0,\n  Timestamp('2015-01-09 00:00:00'): 332528.0,\n  Timestamp('2015-01-10 00:00:00'): 381460.0,\n  Timestamp('2015-01-11 00:00:00'): 422981.0,\n  Timestamp('2015-01-12 00:00:00'): 463539.0,\n  Timestamp('2015-01-13 00:00:00'): 505395.0,\n  Timestamp('2015-01-14 00:00:00'): 549027.0,\n  Timestamp('2015-01-15 00:00:00'): 595377.0,\n  Timestamp('2015-01-16 00:00:00'): 649043.0,\n  Timestamp('2015-01-17 00:00:00'): 707727.0,\n  Timestamp('2015-01-18 00:00:00'): 761287.0,\n  Timestamp('2015-01-19 00:00:00'): 814372.0,\n  Timestamp('2015-01-20 00:00:00'): 867096.0,\n  Timestamp('2015-01-21 00:00:00'): 920838.0,\n  Timestamp('2015-01-22 00:00:00'): 983405.0,\n  Timestamp('2015-01-23 00:00:00'): 1067243.0,\n  Timestamp('2015-01-24 00:00:00'): 1164421.0,\n  Timestamp('2015-01-25 00:00:00'): 1252178.0,\n  Timestamp('2015-01-26 00:00:00'): 1341484.0,\n  Timestamp('2015-01-27 00:00:00'): 1427600.0,\n  Timestamp('2015-01-28 00:00:00'): 1511549.0,\n  Timestamp('2015-01-29 00:00:00'): 1594846.0,\n  Timestamp('2015-01-30 00:00:00'): 1694226.0,\n  Timestamp('2015-01-31 00:00:00'): 1806727.0,\n  Timestamp('2015-02-01 00:00:00'): 1899880.0,\n  Timestamp('2015-02-02 00:00:00'): 1987978.0,\n  Timestamp('2015-02-03 00:00:00'): 2080338.0,\n  Timestamp('2015-02-04 00:00:00'): 2175775.0,\n  Timestamp('2015-02-05 00:00:00'): 2279525.0,\n  Timestamp('2015-02-06 00:00:00'): 2403306.0,\n  Timestamp('2015-02-07 00:00:00'): 2545696.0,\n  Timestamp('2015-02-08 00:00:00'): 2672464.0,\n  Timestamp('2015-02-09 00:00:00'): 2794788.0},\n 'count_b': {Timestamp('2015-01-01 00:00:00'): nan,\n  Timestamp('2015-01-02 00:00:00'): nan,\n  Timestamp('2015-01-03 00:00:00'): nan,\n  Timestamp('2015-01-04 00:00:00'): nan,\n  Timestamp('2015-01-05 00:00:00'): nan,\n  Timestamp('2015-01-06 00:00:00'): nan,\n  Timestamp('2015-01-07 00:00:00'): nan,\n  Timestamp('2015-01-08 00:00:00'): nan,\n  Timestamp('2015-01-09 00:00:00'): nan,\n  Timestamp('2015-01-10 00:00:00'): nan,\n  Timestamp('2015-01-11 00:00:00'): nan,\n  Timestamp('2015-01-12 00:00:00'): nan,\n  Timestamp('2015-01-13 00:00:00'): nan,\n  Timestamp('2015-01-14 00:00:00'): nan,\n  Timestamp('2015-01-15 00:00:00'): nan,\n  Timestamp('2015-01-16 00:00:00'): nan,\n  Timestamp('2015-01-17 00:00:00'): nan,\n  Timestamp('2015-01-18 00:00:00'): nan,\n  Timestamp('2015-01-19 00:00:00'): nan,\n  Timestamp('2015-01-20 00:00:00'): nan,\n  Timestamp('2015-01-21 00:00:00'): nan,\n  Timestamp('2015-01-22 00:00:00'): nan,\n  Timestamp('2015-01-23 00:00:00'): nan,\n  Timestamp('2015-01-24 00:00:00'): 71.0,\n  Timestamp('2015-01-25 00:00:00'): 150.0,\n  Timestamp('2015-01-26 00:00:00'): 236.0,\n  Timestamp('2015-01-27 00:00:00'): 345.0,\n  Timestamp('2015-01-28 00:00:00'): 1239.0,\n  Timestamp('2015-01-29 00:00:00'): 2228.0,\n  Timestamp('2015-01-30 00:00:00'): 7094.0,\n  Timestamp('2015-01-31 00:00:00'): 16593.0,\n  Timestamp('2015-02-01 00:00:00'): 27190.0,\n  Timestamp('2015-02-02 00:00:00'): 37519.0,\n  Timestamp('2015-02-03 00:00:00'): 49003.0,\n  Timestamp('2015-02-04 00:00:00'): 63323.0,\n  Timestamp('2015-02-05 00:00:00'): 79846.0,\n  Timestamp('2015-02-06 00:00:00'): 101568.0,\n  Timestamp('2015-02-07 00:00:00'): 127120.0,\n  Timestamp('2015-02-08 00:00:00'): 149955.0,\n  Timestamp('2015-02-09 00:00:00'): 171440.0}})\n\n"
'I have this large dataframe I\'ve imported into pandas and I want to chop it down via a filter.  Here is my basic sample code:\n\nimport pandas as pd\nimport numpy as np\nfrom pandas import Series, DataFrame\n\ndf = DataFrame({\'A\':[12345,0,3005,0,0,16455,16454,10694,3005],\'B\':[0,0,0,1,2,4,3,5,6]})\n\ndf2= df[df["A"].map(lambda x: x &gt; 0) &amp; (df["B"] &gt; 0)]\n\n\nBasically this displays bottom 4 results which is semi-correct.  But I need to display everything BUT these results.  So essentially, I\'m looking for a way to use this filter but in a "not" version if that\'s possible.  So if column A is greater than 0 AND column B is greater than 0 then we want to disqualify these values from the dataframe.  Thanks\n'
'I have the following pandas DataFrame:\n\n     time      Group      blocks\n0     1        A           4\n1     2        A           7\n2     3        A           12\n3     4        A           17\n4     5        A           21 \n5     6        A           26\n6     7        A           33\n7     8        A           39\n8     9        A           48\n9     10       A           59\n    ....        ....          ....\n36     35      A           231\n37     1       B           1\n38     2       B           1.5\n39     3       B           3\n40     4       B           5\n41     5       B           6\n    ....        ....          ....\n911    35      Z           349\n\n\nThis is a dataframe with multiple time series-ques data, from min=1 to max=35. Each Group has a time series like this. \n\nI would like to plot each individual time series A through Z against an x-axis of 1 to 35. The y-axis would be the blocks at each time. \n\nI was thinking of using something like an Andrews Curves plot, which would plot each series against one another. Each "hue" would be set to a different group. (Other ideas are welcome.)\n\n\n\nMy problem: how do you format this dataframe to plot multiple series? Should the columns be GroupA, GroupB, etc.? \n\nHow do you get the dataframe to be in the format:\n\ntime GroupA blocksA GroupsB blocksB GroupsC blocksC....\n\n\nIs this the correct format for an Andrews plot as shown? \n\nEDIT\n\nIf I try:\n\ndf.groupby(\'Group\').plot(legend=False)\n\n\nthe x-axis is completely incorrect. All time series should be plotted from 0 to 35, all in one series. \n\n\n\nHow do I solve this? \n'
'I have two dataframes - df1 and df2.\n\ndf1 has row1,row2,row3,row4,row5\ndf2 has row2,row5\n\n\nI want to have a new dataframe such that df1-df2. That is, the resultant dataframe should have rows as - row1,row3,row4. \n'
'I have a large csv file and I open it with pd.read_csv as it follows:\n\ndf = pd.read_csv(path//fileName.csv, sep = \' \', header = None)\n\n\nAs the file is really large I would like to be able to open it in rows\n\nfrom 0 to 511\nfrom 512 to 1023\nfrom 1024 to 1535\n...\nfrom 512*n to 512*(n+1) - 1\n\n\nWhere n = 1, 2, 3 ...\n\nIf I add chunksize = 512 into the arguments of read_csv\n\ndf = pd.read_csv(path//fileName.csv, sep = \' \', header = None, chunksize = 512)\n\n\nand I type\n\ndf.get_chunk(5)\n\n\nThan I am able to open rows from 0 to 5 or I may be able to divide the file in parts of 512 rows using a for loop\n\ndata = []\nfor chunks in df:\n    data = data + [chunk]\n\n\nBut this is quite useless as still the file has to be completelly opened and takes time. How can I read only rows from 512*n to 512*(n+1).\n\nLooking around I often saw that "chunksize" is used together with "iterator" as it follows\n\n df = pd.read_csv(path//fileName.csv, sep = \' \', header = None, iterator = True, chunksize = 512)\n\n\nBut after many attempts I still don\'t understand which benefits provide me this boolean variable. Could you explain me it, please?\n'
'I would like to display the output from pandas.DataFrame.info() on a tkinter text widget so I need a string. However pandas.DataFrame.info() returns NoneType is there anyway I can change this?\n\nimport pandas as pd\nimport numpy as np\n\ndata = np.random.rand(10).reshape(5,2)\ncols = \'a\', \'b\'\ndf = pd.DataFrame(data, columns=cols)\ndf_info = df.info()\nprint(df_info)\ntype(df_info)\n\n\nI\'d like to do something like:\n\ninfo_str = ""\ndf_info = df.info(buf=info_str)\n\n\nIs it possible to get pandas to return a string object from DataFrame.info()?\n'
'The following is my dataframe which holds values from multiple Excel files. I wanted to do a time series analysis, so I made the index as datetimeindex. But my index is not arranged according to the date. The following is my dataframe:\n\n    Item Details    Unit    Op. Qty Price   Op. Amt.    Cl. Qty Price.1 Cl. Amt.\nMonth                               \n2013-04-01  5 In 1  Pcs -56.0   172.78  -9675.58    -68.0   175.79  -11953.96\n2013-04-01  Adaptor Pcs -17.0   9.00    -152.99 -17.0   9.00    -152.99\n2013-04-01  Agro Tape   Pcs -2.0    26.25   -52.50  -2.0    26.25   -52.50\n...\n2014-01-01  12" Angal   Pcs -6.0    31.50   -189.00 -6.0    31.50   -189.00\n2014-01-01  13 Mm Electrical Drill Check    Set -1.0    247.50  -247.50 -1.0    247.50  -247.50\n2014-01-01  14" Blad    Pcs -5.0    157.49  -787.45 -5.0    157.49  -787.45\n...\n2013-09-01  Zinc Bolt 1/4 X 2"(box) Box -1.0    899.99  -899.99 -1.0    899.99  -899.99\n2013-09-01  Zorik 88 32gram Pcs -1.0    45.00   -45.00  -1.0    45.00   -45.00\n2013-09-01  Zorrik 311 Gram Pcs -1.0    270.01  -270.01 -1.0    270.01  -270.01\n\n\nIt is not sorted according to the date. I wanted to sort the index and its respective rows also. I googled it and found that there is a way to sort the datetimeindex and is as follows:\n\nall_data.index.sort_values()\n\nDatetimeIndex([\'2013-04-01\', \'2013-04-01\', \'2013-04-01\', \'2013-04-01\',\n           \'2013-04-01\', \'2013-04-01\', \'2013-04-01\', \'2013-04-01\',\n           \'2013-04-01\', \'2013-04-01\',\n           ...\n           \'2014-02-01\', \'2014-02-01\', \'2014-02-01\', \'2014-02-01\',\n           \'2014-02-01\', \'2014-02-01\', \'2014-02-01\', \'2014-02-01\',\n           \'2014-02-01\', \'2014-02-01\'],\n          dtype=\'datetime64[ns]\', name=u\'Month\', length=71232, freq=None)\n\n\nBut it is sorting only the index, how can I sort the entire dataframe according to the sorted index? Kindly help.\n'
'I need to use a lambda function to do a row by row computation. For example create some dataframe\n\nimport pandas as pd\nimport numpy as np\n\ndef myfunc(x, y):\n    return x + y\n\ncolNames = [\'A\', \'B\']\ndata = np.array([np.arange(10)]*2).T\n\ndf = pd.DataFrame(data, index=[range(0, 10)], columns=colNames)\n\n\nusing \'myfunc\' this does work\n\ndf[\'D\'] = (df.apply(lambda x: myfunc(x.A, x.B), axis=1))\n\n\nbut this second case does not work!\n\ndf[\'D\'] = (df.apply(lambda x: myfunc(x.colNames[0], x.colNames[1]), axis=1))\n\n\ngiving the error\n\nAttributeError: ("\'Series\' object has no attribute \'colNames\'", u\'occurred at index 0\')\n\n\nI really need to use the second case (access the colNames using the list) which gives an error, any clues on how to do this?\n\nthanks\n'
"I have a pandas dataframe (that was created by importing a csv file). I want to replace blank values with NaN. Some of these blank values are empty and some contain a (variable number) of spaces '', ' ', '    ', etc.\n\nUsing the suggestion from this thread I have \n\ndf.replace(r'\\s+', np.nan, regex=True, inplace = True)\n\n\nwhich does replace all the strings that only contain spaces, but also replaces every string that has a space in it, which is not what I want.\n\nHow do I replace only strings with just spaces and empty strings?\n"
"i'm running a function in which a variable is of pandas.core.series.Series type.\n\ntype of the series shown below.\n\n&lt;class 'pandas.core.series.Series'&gt;\nproduct_id_y    1159730\ncount                 1\nName: 6159402, dtype: object\n\n\ni want to convert this into a dataframe,such that, i get\n\nproduct_id_y    count\n1159730           1\n\n\ni tried doing this:\n\nseries1 = series1.to_frame()\n\n\nbut getting wrong result\n\nafter converting to dataframe\n\n              6159402\nproduct_id_y  1159730\ncount               1\n\n\nafter doing reset index i'e series1 = series1.reset_index()\n\n           index  6159402\n 0  product_id_y  1159730\n 1         count        1\n\n\nis there anny other way to do this??\n"
"I am trying to use a linear regression on a group by pandas python dataframe: \n\nThis is the dataframe df:\n\n  group      date      value\n    A     01-02-2016     16 \n    A     01-03-2016     15 \n    A     01-04-2016     14 \n    A     01-05-2016     17 \n    A     01-06-2016     19 \n    A     01-07-2016     20 \n    B     01-02-2016     16 \n    B     01-03-2016     13 \n    B     01-04-2016     13 \n    C     01-02-2016     16 \n    C     01-03-2016     16 \n\n#import standard packages\nimport pandas as pd\nimport numpy as np\n\n#import ML packages\nfrom sklearn.linear_model import LinearRegression\n\n#First, let's group the data by group\ndf_group = df.groupby('group')\n\n#Then, we need to change the date to integer\ndf['date'] = pd.to_datetime(df['date'])  \ndf['date_delta'] = (df['date'] - df['date'].min())  / np.timedelta64(1,'D')\n\n\nNow I want to predict the value for each group for 01-10-2016. \n\nI want to get to a new dataframe like this: \n\ngroup      01-10-2016\n  A      predicted value\n  B      predicted value\n  C      predicted value\n\n\nThis How to apply OLS from statsmodels to groupby doesn't work\n\nfor group in df_group.groups.keys():\n      df= df_group.get_group(group)\n      X = df['date_delta'] \n      y = df['value']\n      model = LinearRegression(y, X)\n      results = model.fit(X, y)\n      print results.summary()\n\n\nI get the following error\n\nValueError: Found arrays with inconsistent numbers of samples: [ 1 52]\n\nDeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and   willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.DeprecationWarning)\n\n\nUPDATE: \n\nI changed it to \n\nfor group in df_group.groups.keys():\n      df= df_group.get_group(group)\n      X = df[['date_delta']]\n      y = df.value\n      model = LinearRegression(y, X)\n      results = model.fit(X, y)\n      print results.summary()\n\n\nand now I get this error: \n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n"
"Is there a built in function to rename a pandas dataframe by index? \n\nI thought I knew the name of my column headers, but it turns out the second column has some hexadecimal characters in it. I will likely come across this issue with column 2 in the future based on the way I receive my data, so I cannot hard code those specific hex characters into a dataframe.rename() call.\n\nIs there a function that would be appropriately named rename_col_by_index() that I have not been able to find?\n\nEx:\n\n&gt;&gt;&gt; df = pd.DataFrame({'a':[1,2], 'b':[3,4]})\n&gt;&gt;&gt; df.rename_col_by_index(1, 'new_name')\n&gt;&gt;&gt; df\n   a  new_name\n0  1         3\n1  2         4\n\n"
'I currently have code that reads in an Excel table (image below): \n\n# Read in zipcode input file\n\nus_zips = pd.read_excel("Zipcode.xls")\nus_zips\n\n\n\n\nI use the following code to convert the dataframe zip codes into a list: \n\nus_zips = list(us_zips.values.flatten())\n\n\nWhen I print us_zips it looks like this: \n\n[10601, 60047, 50301, 10606]\n\n...but I want it to look like this ["10601", "60047", "50301", "10606"]\n\nHow can I do that?  *Any help is greatly appreciated \n'
'I can successfully query and insert data using sqlalchemy and pandas:\n\nfrom sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine(\'mssql://myserver/mydb?driver=SQL+Server+Native+Client+11.0?trusted_connection=yes\')\n\n\nRead tempy table:\n\nsql_command = """\nselect top 100 * from tempy\n"""\n\ndf = pd.read_sql(sql_command, engine)\nprint df\n\n   tempID  tempValue\n0       1          2\n\n\nAppend new data:\n\ndf_append = pd.DataFrame( [[4,6]] , columns=[\'tempID\',\'tempValue\']) \ndf_append.to_sql(name=\'tempy\', con=engine, if_exists = \'append\', index=False)\n\ndf = pd.read_sql(sql_command, engine)\nprint df\n\n   tempID  tempValue\n0       1          2\n1       4          6\n\n\nTry to truncate data:\n\nconnection = engine.connect()\nconnection.execute( \'\'\'TRUNCATE TABLE tempy\'\'\' )\nconnection.close()\n\n\nRead table again, but truncate failed:\n\ndf = pd.read_sql(sql_command, engine)\nprint df\n\n   tempID  tempValue\n0       1          2\n1       4          6\n\n'
'When I\'m adding the c option to a scatterplot in matplotlib, the x axis labels dissapear. Here\'s an example: https://github.com/Kornel/scatterplot-matplotlib/blob/master/Scatter%20plot%20x%20axis%20labels.ipynb (pull requests are welcome:))\n\nHere\'s the same example as in the notebook:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ntest_df = pd.DataFrame({\n        "X": [1, 2, 3, 4],\n        "Y": [5, 4, 2, 1],\n        "C": [1, 2, 3, 4]\n    })\n\n\nNow compare the result of:\n\ntest_df.plot(kind="scatter", x="X", y="Y", s=50);\n\n\n\n\nTo:\n\ntest_df.plot(kind="scatter", x="X", y="Y", c="C");\n\n\n\n\nWhere are the x axis labels? Is this a feature I\'m missing?\n\nPandas version: 0.18.1\nMatplotlib: 1.5.3\nPython: 3.5.2\n\nThanks for any help,\n Kornel\n\nEDIT: The solution as pointed out by @Kewl is to call plt.subplots and specify the axes:\n\nfig, ax = plt.subplots()\ntest_df.plot(kind="scatter", x="X", y="Y", s=50, c="C", cmap="plasma", ax=ax);\n\n\ngives\n\n\n\nP.S. It looks like a jupyter issue, the label is fine when called without a jupyter notebook\n'
"I have multiple zip files containing different types of txt files. \nLike below:\n\nzip1 \n  - file1.txt\n  - file2.txt\n  - file3.txt\n\n\nHow can I use pandas to read in each of those files without extracting them?\n\nI know if they were 1 file per zip I could use the compression method with read_csv like below:\n\ndf = pd.read_csv(textfile.zip, compression='zip') \n\n\nAny help on how to do this would be great.\n"
"The documentation for the argument in this post's title says:\n\nfloat_precision : string, default None\nSpecifies which converter the C engine should use for floating-point values. The options are None for the ordinary converter, high for the high-precision converter, and round_trip for the round-trip converter.\n\nI'd like to learn more about the three algorithms mentioned, preferably without having to dig into the source code1.\n\nQ: Do these algorithms have names I can Google for to learn exactly what they do and how they differ?\n\n(Also, one side question: what exactly is &quot;the C engine&quot; in this context?  Is that a Pandas-specific thing, or a Python-wide thing?  None of the above?)\n\n1 Not being familiar with the code base in question, I expect it would take me a long time just to locate the relevant source code.  But even assuming I manage to find it, my experience with this sort of algorithm is that their implementations are so highly optimized, and at such a low level, that without some high-level description it is really difficult, at least for me, to follow what's going on.\n"
"Assume I have a pandas series with several consecutive NaNs. I know fillna has several methods to fill missing values (backfill and fill forward), but I want to fill them with the closest non NaN value. Here's an example of what I have:\n\n`s = pd.Series([0, 1, np.nan, np.nan, np.nan, np.nan, 3])`\n\n\nAnd an example of what I want:\n    s = pd.Series([0, 1, 1, 1, 3, 3, 3])\n\nDoes anyone know I could do that?\n\nThanks!\n"
"I'm trying to slice a dataframe based on list of values, how would I go about this?\n\nSay I have an expression or a list l = [0,1,0,0,1,1,0,0,0,1]\n\nHow to return those rows in a dataframe, df, when the corresponding value in the expression/list is 1? In this example, I would include rows where index is 1, 4, 5, and 9.\n"
"I want to check if a row exists in dataframe, following is my code:\n\ndf = pd.read_csv('dbo.Access_Stat_all.csv',error_bad_lines=False, usecols=['Name','Format','Resource_ID','Number'])\ndf1 = df[df['Resource_ID'] == 30957]\ndf1 = df1[['Format','Name','Number']]\ndf1 = df1.groupby(['Format','Name'], as_index=True).last()\npd.options.display.float_format = '{:,.0f}'.format\ndf1 = df1.unstack()\ndf1.columns = df1.columns.droplevel()\nif 'entry' in df1:\n    df2 = df1[1:4].sum(axis=0)\nelse:\n    df2 = df1[0:3].sum(axis=0)\ndf2.name = 'sum'\ndf2 = df1.append(df2)\nprint(df2)\n\n\nThis is the output:\n\nName    Apr 2013  Apr 2014  Apr 2015  Apr 2016  Apr 2017  Aug 2010  Aug 2013  \nFormat                                                                         \n\nentry          0         0         0         1         4         1         0   \npdf           13        12         4        23         7         1         9   \nsum           13        12         4        24        11         2         9 \n\n\nDoes  if 'entry' in df2:  only check if 'entry' exists as a column? It must be the case, I guess. We can see that the row 'entry' exists but we still land in the else condition(if it had landed in if the statement sum for Apr 2016 would be 23).\n\nIf I check it for the file which don't have the row 'entry', it again lands in else statement(as I expect), so I assume it always enters the else condition.\n\nHow do I check if a row exists in pandas?\n"
"Given a dataframe like this:\n          C\nA   B      \n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74 \n\nHow do I read it in using pd.read_clipboard? I've tried this:\ndf = pd.read_clipboard(index_col=[0, 1])\n\nBut it throws an error:\n\nParserError: Error tokenizing data. C error: Expected 2 fields in line 3, saw 3\n\n\nHow can I fix this?\n"
'Update: not sure if this is possible without some form of a loop, but np.where will not work here. If the answer is, "you can\'t", then so be it.  If it can be done, it may use something from scipy.signal.\n\n\n\nI\'d like to vectorize the loop in the code below, but unsure as to how, due to the recursive nature of the output.\n\nWalk-though of my current setup:\n\nTake a starting amount ($1 million) and a quarterly dollar distribution ($5,000):\n\ndist = 5000.\nv0 = float(1e6)\n\n\nGenerate some random security/account returns (decimal form) at monthly freq:\n\nr = pd.Series(np.random.rand(12) * .01,\n              index=pd.date_range(\'2017\', freq=\'M\', periods=12))\n\n\nCreate an empty Series that will hold the monthly account values:\n\nvalue = pd.Series(np.empty_like(r), index=r.index)\n\n\nAdd a "start month" to value.  This label will contain v0.\n\nfrom pandas.tseries import offsets\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)]))\n              .sort_index())\n\n\nThe loop I\'d like to get rid of is here:\n\nfor date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] \\\n                        * (1 + r.loc[date]) \n\n\nCombined code:\n\nimport pandas as pd\nfrom pandas.tseries import offsets\nfrom pandas import Series\nimport numpy as np\n\ndist = 5000.\nv0 = float(1e6)\nr = pd.Series(np.random.rand(12) * .01, index=pd.date_range(\'2017\', freq=\'M\', periods=12))\nvalue = pd.Series(np.empty_like(r), index=r.index)\nvalue = (value.append(Series(v0, index=[value.index[0] - offsets.MonthEnd(1)])).sort_index())\nfor date in value.index[1:]:\n    if date.is_quarter_end:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) - dist\n    else:\n        value.loc[date] = value.loc[date - offsets.MonthEnd(1)] * (1 + r.loc[date]) \n\n\nIn psuedocode, what is loop is doing is just:\n\nfor each date in index of value:\n    if the date is not a quarter end:\n        multiply previous value by (1 + r) for that month\n    if the date is a quarter end:\n        multiply previous value by (1 + r) for that month and subtract dist\n\n\nThe issue is, I don\'t currently see how vectorization is possible since the successive value depends on whether or not a distribution was taken in the month prior.  I get to the desired result, but pretty inefficiently for higher frequency data or larger time periods.\n\n'
'Let\'s say I have two dataframes, and the column names for both are:\n\ntable 1 columns:\n[ShipNumber, TrackNumber, ShipDate, Quantity, Weight]\ntable 2 columns:\n[ShipNumber, TrackNumber, AmountReceived]\n\n\nI want to merge the two tables based on both ShipNumber and TrackNumber. \nHowever, if i simply use merge in the following way (pseudo code, not real code):\n\ntab1.merge(tab2, "left", on=[\'ShipNumber\',\'TrackNumber\'])\n\n\nthen, that means the values in both ShipNumber and TrackNumber columns from both tables MUST MATCH. \n\nHowever, in my case, sometimes the ShipNumber column values will match, sometimes the TrackNumber column values will match; as long as one of the two values match for a row, I want the merge to happen. \n\nIn other words, if row 1 ShipNumber in tab 1 matches row 3 ShipNumber in tab 2, but the TrackNumber in two tables for the two records do not match, I still want to match the two rows from the two tables.\n\nSo basically this is a either/or match condition (pesudo code):\n\nif tab1.ShipNumber == tab2.ShipNumber OR tab1.TrackNumber == tab2.TrackNumber:\n    then merge\n\n\nI hope my question makes sense... \nAny help is really really appreciated!\n\nAs suggested, I looked into this post:\nPython pandas merge with OR logic\nBut it is not completely the same issue I think, as the OP from that post has a mapping file, and so they can simply do 2 merges to solve this. But I dont have a mapping file, rather, I have two df\'s with same key columns (ShipNumber, TrackNumber)\n'
"I need a datetime column in seconds, everywhere (including the docs) is saying that I should use Series.dt.total_seconds() but it can't find the function. I'm assuming I have the wrong version of something but I don't...\n\npip freeze | grep pandas\npandas==0.20.3\n\npython --version\nPython 3.5.3\n\n\nThis is all within a virtualenv that has worked without fault for a long time, and the other Series.dt functions work. Here's the code:\n\nfrom pandas import Series\nfrom datetime import datetime\n\ns = Series([datetime.now() for _ in range(10)])\n\n0   2017-08-25 15:55:25.079495\n1   2017-08-25 15:55:25.079504\n2   2017-08-25 15:55:25.079506\n3   2017-08-25 15:55:25.079508\n4   2017-08-25 15:55:25.079509\n5   2017-08-25 15:55:25.079510\n6   2017-08-25 15:55:25.079512\n7   2017-08-25 15:55:25.079513\n8   2017-08-25 15:55:25.079514\n9   2017-08-25 15:55:25.079516\ndtype: datetime64[ns]\n\ns.dt\n&lt;pandas.core.indexes.accessors.DatetimeProperties object at 0x7f5a686507b8&gt;\n\ns.dt.minute\n0    55\n1    55\n2    55\n3    55\n4    55\n5    55\n6    55\n7    55\n8    55\n9    55\ndtype: int64\n\ns.dt.total_seconds()\nAttributeError: 'DatetimeProperties' object has no attribute 'total_seconds'\n\n\nI've also tested this on a second machine and get the same result. Any ideas what I'm missing?\n"
"I have the following dataframe df:\n\ndata={'id':[1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2],\n      'value':[2,2,3,2,2,2,3,3,3,3,1,4,1,1,1,4,4,1,1,1,1,1]}\ndf=pd.DataFrame.from_dict(data)\ndf\nOut[8]: \n    id  value\n0    1      2\n1    1      2\n2    1      3\n3    1      2\n4    1      2\n5    1      2\n6    1      3\n7    1      3\n8    1      3\n9    1      3\n10   2      1\n11   2      4\n12   2      1\n13   2      1\n14   2      1\n15   2      4\n16   2      4\n17   2      1\n18   2      1\n19   2      1\n20   2      1\n21   2      1\n\n\nWhat I need to do is identify at the id level (df.groupby['id']) when the value shows the same number consecutively for 3 or more times.\n\nI would like to have the following result for the above:\n\ndf\nOut[12]: \n    id  value  flag\n0    1      2     0\n1    1      2     0\n2    1      3     0\n3    1      2     1\n4    1      2     1\n5    1      2     1\n6    1      3     1\n7    1      3     1\n8    1      3     1\n9    1      3     1\n10   2      1     0\n11   2      4     0\n12   2      1     1\n13   2      1     1\n14   2      1     1\n15   2      4     0\n16   2      4     0\n17   2      1     1\n18   2      1     1\n19   2      1     1\n20   2      1     1\n21   2      1     1\n\n\nI have tried variations of groupby and lambda using pandas rolling.mean to identify where the average of the rolling period is then compared to the 'value', and where they are the same this indicates a flag.  But this has several problems, including that you could have different values that will average to the value you are trying to flag.  Also, I can't figure out how to 'flag' all of the values of the rolling mean that created the initial flag.  See here, this identifies the 'right side' of the flag, but then I need to fill the previous values of the rolling mean length.  See my code here:\n\ntest=df.copy()\ntest['rma']=test.groupby('id')['value'].transform(lambda x: x.rolling(min_periods=3,window=3).mean())\ntest['flag']=np.where(test.rma==test.value,1,0)\n\n\nAnd the result here:\n\ntest\nOut[61]: \n    id  value       rma  flag\n0    1      2       NaN     0\n1    1      2       NaN     0\n2    1      3  2.333333     0\n3    1      2  2.333333     0\n4    1      2  2.333333     0\n5    1      2  2.000000     1\n6    1      3  2.333333     0\n7    1      3  2.666667     0\n8    1      3  3.000000     1\n9    1      3  3.000000     1\n10   2      1       NaN     0\n11   2      4       NaN     0\n12   2      1  2.000000     0\n13   2      1  2.000000     0\n14   2      1  1.000000     1\n15   2      4  2.000000     0\n16   2      4  3.000000     0\n17   2      1  3.000000     0\n18   2      1  2.000000     0\n19   2      1  1.000000     1\n20   2      1  1.000000     1\n21   2      1  1.000000     1\n\n\nCan't wait to see what I am missing!  Thanks\n"
'I have a series made of lists\n\nimport pandas as pd\ns = pd.Series([[1, 2, 3], [4, 5, 6]])\n\n\nand I want a DataFrame with each column a list. \n\nNone of from_items, from_records, DataFrame Series.to_frame seem to work. \n\nHow to do this?\n'
'When indexing a MultiIndex-ed DataFrame, it seems like .iloc assumes you\'re referencing the "inner level" of the index while .loc looks at the outer level.\n\nFor example:\n\nnp.random.seed(123)\niterables = [[\'bar\', \'baz\', \'foo\', \'qux\'], [\'one\', \'two\']]\nidx = pd.MultiIndex.from_product(iterables, names=[\'first\', \'second\'])\ndf = pd.DataFrame(np.random.randn(8, 4), index=idx)\n\n# .loc looks at the outer index:\n\nprint(df.loc[\'qux\'])\n# df.loc[\'two\'] would throw KeyError\n              0        1        2        3\nsecond                                    \none    -1.25388 -0.63775  0.90711 -1.42868\ntwo    -0.14007 -0.86175 -0.25562 -2.79859\n\n# while .iloc looks at the inner index:\n\nprint(df.iloc[-1])\n0   -0.14007\n1   -0.86175\n2   -0.25562\n3   -2.79859\nName: (qux, two), dtype: float64\n\n\nTwo questions:\n\nFirstly, why is this?  Is it a deliberate design decision?\n\nSecondly, can I use .iloc to reference the outer level of the index, to yield the result below?  I\'m aware I could first find the last member of the index with get_level_values and then .loc-index with that, but wandering if it can be done more directly, either with funky .iloc syntax or some existing function designed specifically for the case.\n\n# df.iloc[-1]\nqux   one     0.89071  1.75489  1.49564  1.06939\n      two    -0.77271  0.79486  0.31427 -1.32627\n\n'
"I have a large dataset of CSV files comprised of two distinct objects: object_a and object_b. Each of these entities has a numeric tick value as well.\n\nType,       Parent Name, Ticks\nobject_a,   4556421,     34\nobject_a,   4556421,     0\nobject_b,   4556421,     0\nobject_a,   3217863,     2\nobject_b,   3217863,     1\n......\n\n\nEach object shares a Parent Name value so in most cases, one of each object will share a Parent Name value but this is not always the case.\n\nI have two objectives with this dataset:\n\n\nextract all object_a's under a Parent Name where i) there are >1 object_a's and; ii) the object_a has 0 ticks but the other object_a has >0 ticks. i.e. just the one with zero ticks\nextract all object_b's under a Parent Name where i) there is >=1 object_a and; ii) the object_b has 0 ticks but the object_a has >0 ticks\n\n\nMy first approach is to have two separate functions for both tasks, read the CSV files (usually 1.5GB in size) in chunks and output the extracted rows to another csv file after grouping them according to Parent Name...\n\ndef objective_one(group_name, group_df):\n\n   group_df = group_df[group_df['Type'] == 'object_a']\n\n   if len(group_df) &gt; 1:        \n       zero_tick_object_a = group_df[group_df['Ticks'] == 0]        \n       if len(zero_click_object_a) &lt; len(group_df):        \n           return zero_click_object_a        \n       else:        \n           return pd.DataFrame(columns=group_df.columns)\n   else:        \n       return pd.DataFrame(columns=group_df.columns)\n\n\ndef objective_two(group_name, group_df):\n\n   object_a_in_group_df = group_df[group_df['Type'] == 'object_a']\n   object_b_has_no_clicks_in_group_df = group_df[(group_df['Type'] == 'object_b') &amp; (group_df['Ticks'] == 0)]\n\n   if len(object_a_in_group_df) &gt;= 1 and len(object_b_has_no_ticks_in_group_df) &gt;= 1:\n\n       has_ticks_objects = objects_in_group_df[object_a_in_group_df['Ticks'] &gt;= 1]\n\n       if len(has_ticks_object_a) &gt; 0:        \n           return object_B_has_no_ticks_in_group_df        \n       else:        \n           return pd.DataFrame(columns=group_df.columns)\n   else:        \n       return pd.DataFrame(columns=group_df.columns)\n\n\nHere are the calls to these functions in the main method:\n\nfor chunk in pd.read_csv(file, chunksize=500000):\n\n   #objective one\n   chunk_object_a = chunk.groupby(['Parent Name']).apply(lambda g: objective_one(g.name, g))\n   ....\n   ....\n   #objective two\n   chunk_object_b = chunk.groupby(['Parent Name']).apply(lambda g: objective_two(g.name, g))\n\n\n#  Then write the dataframes outputted by the apply methods to a csv file\n\nThe problem with this approach is that while it does get me the output I want, it is very slow in large files in the 1GB and above range. Another issue is that reading it in chunks from the CSV may effectively cut some groups in half( i.e. a Parent Name could be split over one chunk and the next, making for inaccurate number of objects extracted) \n\nIs there any way to optimize this to make it any faster and also, get around my chunk problem?\n"
"As per the title here's a reproducible example:\n\nraw_data = {'x': ['this', 'that', 'this', 'that', 'this'], \n            np.nan: [np.nan, np.nan, np.nan, np.nan, np.nan], \n            'y': [np.nan, np.nan, np.nan, np.nan, np.nan],\n            np.nan: [np.nan, np.nan, np.nan, np.nan, np.nan]}\n\ndf = pd.DataFrame(raw_data, columns = ['x', np.nan, 'y', np.nan])\ndf\n\n    x       nan  y      nan\n0   this    NaN  NaN    NaN\n1   that    NaN  NaN    NaN\n2   this    NaN  NaN    NaN\n3   that    NaN  NaN    NaN\n4   this    NaN  NaN    NaN\n\n\nAim is to drop only the columns with nan as the col name (so keep column y). dropna() doesn't work as it conditions on the nan values in the column, not nan as the col name. \n\ndf.drop(np.nan, axis=1, inplace=True) works if there's a single column in the data with nan as the col name. But not with multiple columns with nan as the col name, as in my data.\n\nSo how to drop multiple columns where the col name is nan?\n"
'I have issues with the merging of two large Dataframes since the merge returns NaN values though there are fitting values. The two dfs are shaped like:\n\ndf1\n\nMotor\n2232\n1524\n2230\n2230\n2224\n1516\n1724\n2224\n1524\n1624\n1724\n2224\n2224\n1524\n1524\n1516\n1524\n2224\n1624\n1724\n1724\n2224\n2224\n\n\ndf2 \n\nMotor   Output Torque (mNm)\n0615    0,17\n1219    0,72\n1516    0,59\n1624    2\n2230    4,7\n2233    5,9\n0816    0,7\n1016    0,92\n1024    1,6\n1224    1,7\n1319    1,4\n1331    3,8\n1516    0,97\n1524    2,9\n1717    2,2\n1724    4,5\n2224    6,8\n2232    10\n1336    3,6\n1727    4,9\n1741    8,8\n2237    12\n2642    26\n\n\nI use the code:\n\nMergeDat=MergeDat.merge(Motor,how="left")\nprint(MergeDat)\n\n\nwhere MergeDat= df1\nand   Motor= df2\n\nAs result it returns:\n\n  Motor  Output Torque (mNm)\n0      2232                  NaN\n1      1524                  NaN\n2      2230                  NaN\n3      2230                  NaN\n4      2224                  NaN\n5      1516                  NaN\n6      1724                  NaN\n7      2224                  NaN\n8      1524                  NaN\n9      1624                  NaN\n10     1724                  NaN\n11     2224                  NaN\n12     2224                  NaN\n13     1524                  NaN\n14     1524                  NaN\n15     1516                  NaN\n16     1524                  NaN\n17     2224                  NaN\n18     1624                  NaN\n19     1724                  NaN\n20     1724                  NaN\n21     2224                  NaN\n22     2224                  NaN\n23     1524                  NaN\n24     1724                  NaN\n25     1841                  NaN\n26     2224                  NaN\n\n\nI have no idea why the Output Torque column is not merged...\n\nAppreciate any help!\n'
'I have a Series object that has:\n\n    date   price\n    dec      12\n    may      15\n    apr      13\n    ..\n\n\nProblem statement: I want to make it appear by month and compute the mean price for each month and present it with a sorted manner by month.\n\nDesired Output:\n\n month mean_price\n  Jan    XXX\n  Feb    XXX\n  Mar    XXX\n\n\nI thought of making a list and passing it in a sort function:\n\nmonths = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]\n\n\nbut the sort_values doesn\'t support that for series.\n\nOne big problem I have is that even though\n\ndf = df.sort_values(by=\'date\',ascending=True,inplace=True) works\nto the initial df but after I did a groupby, it didn\'t maintain the order coming out from the sorted df.\n\nTo conclude, I needed from the initial data frame these two columns. Sorted the datetime column and through a groupby using the month (dt.strftime(\'%B\')) the sorting got messed up. Now I have to sort it by month name.\n\n\n\nMy code:\n\ndf # has 5 columns though I need the column \'date\' and \'price\'\n\ndf.sort_values(by=\'date\',inplace=True) #at this part it is sorted according to date, great\ntotal=(df.groupby(df[\'date\'].dt.strftime(\'%B\'))[\'price\'].mean()) # Though now it is not as it was but instead the months appear alphabetically\n\n'
"I want to scatter plot the first two columns of the following pandas.DataFrame, with the third column as color values. \n\n&gt;&gt;&gt;df = pd.DataFrame({'a': {'1128': -2, '1129': 0, '1146': -4, '1142': -3, '1154': -2,\n '1130': -1, '1125': -1, '1126': -2, '1127': -5, '1135': -2},\n 'c': {'1128': 5300, '1129': 6500, '1146': 8900, '1142': 8900,\n '1154': 9000, '1130': 5600, '1125': 9400, '1126': 6000, '1127': 7200,\n '1135': 7700}, 'b': {'1128': -3, '1129': -10, '1146': -6, '1142': -3,\n '1154': -7, '1130': -2, '1125': -7, '1126': -7, '1127': 0, '1135': -1}}\n\n\n&gt;&gt;&gt;df\n        a   b   c\ndid         \n1125    -1  -7  9400\n1126    -2  -7  6000\n1127    -5  0   7200\n1128    -2  -3  5300\n1129    0   -10 6500\n1130    -1  -2  5600\n1135    -2  -1  7700\n1142    -3  -3  8900\n1146    -4  -6  8900\n1154    -2  -7  9000\n\n\nIf I try:\n\n&gt;&gt;&gt;df.plot('a', 'b', kind='scatter', color=df['c'], colormap='YlOrRd')\n\n\nI get \n\n\n\nAnd the X-axis disappears.\n\nI tried ax.set_axis_on() and ax.axis('on') to no avail. \n"
'I have:\n\ndf = pd.DataFrame({\'col1\': [\'asdf\', \'xy\', \'q\'], \'col2\': [1, 2, 3]})\n\n   col1  col2\n0  asdf     1\n1    xy     2\n2     q     3\n\n\nI\'d like to take the "combinatoric product" of each letter from the strings in col1, with each elementwise int in col2.  I.e.:\n\n  col1  col2\n0    a    1\n1    s    1\n2    d    1\n3    f    1\n4    x    2\n5    y    2\n6    q    3\n\n\nCurrent method:\n\nfrom itertools import product\n\npieces = []\nfor _, s in df.iterrows():\n    letters = list(s.col1)\n    prods = list(product(letters, [s.col2]))\n    pieces.append(pd.DataFrame(prods))\n\npd.concat(pieces)\n\n\nAny more efficient workarounds?\n'
"I have a pandas dataframe and a list as follows\n\nmylist = ['nnn', 'mmm', 'yyy']\nmydata =\n   xxx   yyy zzz nnn ddd mmm\n0  0  10      5    5   5  5\n1  1   9      2    3   4  4\n2  2   8      8    7   9  0\n\n\nNow, I want to get only the columns mentioned in mylist and save it as a csv file.\n\ni.e.\n\n     yyy  nnn   mmm\n0    10     5     5\n1    9      3     4\n2    8      7     0\n\n\nMy current code is as follows.\n\nmydata = pd.read_csv( input_file, header=0)\n\nfor item in mylist:\n    mydata_new = mydata[item]\n\nprint(mydata_new)\nmydata_new.to_csv(file_name)\n\n\nIt seems to me that my new dataframe produces wrong results.Where I am making it wrong? Please help me!\n"
"Is there any way I can retain the original index of my large dataframe after I perform a groupby? The reason I need to this is because I need to do an inner merge back to my original df (after my groupby) to regain those lost columns. And the index value is the only 'unique' column to perform the merge back into. Does anyone know how I can achieve this?\n\nMy DataFrame is quite large. \nMy groupby looks like this: \n\ndf.groupby(['col1', 'col2']).agg({'col3': 'count'}).reset_index()\n\n\nThis drops my original indexes from my original dataframe, which I want to keep. \n"
"I have a list as follows.\n\n[['Andrew', '1', '9'], ['Peter', '1', '10'], ['Andrew', '1', '8'], ['Peter', '1', '11'], ['Sam', '4', '9'], ['Andrew', '2', '2']]\n\n\nI would like sum up the last column grouped by the other columns.The result is like this\n\n[['Andrew', '1', '17'], ['Peter', '1', '21'], ['Sam', '4', '9'], ['Andrew', '2', '2']]\n\n\nwhich is still a list.\n\nIn real practice, I would always like to sum up the last column grouped by many other columns. Is there a way I can do this in Python? Much appreciated.\n"
"Given a dataframe structured like:\n\nrule_id | ordering | sequence_id\n   1    |    0     |     12     \n   1    |    1     |     13\n   1    |    1     |     14\n   2    |    0     |     1\n   2    |    1     |     2\n   2    |    2     |     12 \n\n\nI need to transform it into:\n\nrule_id |  sequences\n   1    |  [[12],[13,14]]\n   2    |  [[1],[2],[12]]\n\n\nthat seems like easy groupby into groupby to list operation - I can not however make it work in pandas.\n\ndf.groupby(['rule_id', 'ordering'])['sequence_id'].apply(list)\n\n\nleaves me with\n\nrule_id  ordering\n1        0               [12]\n         1            [13,14]\n2        0                [1]\n         1                [2]\n         2               [12]\n\n\nHow does one apply another groupBy operation to furtherly concat results into one list?\n"
'I have a very simple problem. This is for a pandas dataframe ("df"). The answers are all more complex regarding string compare, which I have no use for. Here is the code that works for lowercase and returns only "apple":\n\ndf2 = df1[\'company_name\'].str.contains(("apple"), na=False)\n\n\nI need this to find "apple", "APPLE", "Apple", etc. Something like:\n\ndf2 = df1[\'company_name\'].str.contains.caseignore((("apple"), na=False))\n\n\nis there such a function anywhere?\n\nThanks.\n'
"I don't understand which functions are acceptable for groupby + transform operations. Often, I end up just guessing, testing, reverting until something works, but I feel there should be a systematic way of determining whether a solution will work.\n\nHere's a minimal example. First let's use groupby + apply with set:\n\ndf = pd.DataFrame({'a': [1,2,3,1,2,3,3], 'b':[1,2,3,1,2,3,3], 'type':[1,0,1,0,1,0,1]})\n\ng = df.groupby(['a', 'b'])['type'].apply(set)\n\nprint(g)\n\na  b\n1  1    {0, 1}\n2  2    {0, 1}\n3  3    {0, 1}\n\n\nThis works fine, but I want the resulting set calculated groupwise in a new column of the original dataframe. So I try and use transform:\n\ndf['g'] = df.groupby(['a', 'b'])['type'].transform(set)\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n---&gt; 23 df['g'] = df.groupby(['a', 'b'])['type'].transform(set)\n\nTypeError: int() argument must be a string, a bytes-like object or a number, not 'set'\n\n\nThis is the error I see in Pandas v0.19.0. In v0.23.0, I see TypeError: 'set' type is unordered. Of course, I can map a specifically defined index to achieve my result:\n\ng = df.groupby(['a', 'b'])['type'].apply(set)\ndf['g'] = df.set_index(['a', 'b']).index.map(g.get)\n\nprint(df)\n\n   a  b  type       g\n0  1  1     1  {0, 1}\n1  2  2     0  {0, 1}\n2  3  3     1  {0, 1}\n3  1  1     0  {0, 1}\n4  2  2     1  {0, 1}\n5  3  3     0  {0, 1}\n6  3  3     1  {0, 1}\n\n\nBut I thought the benefit of transform was to avoid such an explicit mapping. Where did I go wrong?\n"
"I have the following dataframe called language \n\n         lang          level\n0      english         intermediate\n1      spanish         intermediate\n2      spanish         basic\n3      english         basic\n4      english         advanced\n5      spanish         intermediate\n6      spanish         basic\n7      spanish         advanced\n\n\nI categorized each of my variables into numbers by using \n\nlanguage.lang.astype('category').cat.codes\n\nand\n\nlanguage.level.astype('category').cat.codes\n\nrespectively. Obtaining the following data frame:\n\n      lang   level\n0      0       1\n1      1       1\n2      1       0\n3      0       0\n4      0       2\n5      1       1\n6      1       0\n7      1       2\n\n\nNow, I would like to know if there is a way to obtain which original value corresponds to each value. I'd like to know that the 0 value in the lang column corresponds to english and so on. \n\nIs there any function that allows me to get back this information?\n"
"I have a table containing dates and the various cars sold on each dates in the following format (These are only 2 of many columns):\n\nDATE       CAR\n2012/01/01 BMW\n2012/01/01 Mercedes Benz\n2012/01/01 BMW\n2012/01/02 Volvo\n2012/01/02 BMW\n2012/01/03 Mercedes Benz\n...\n2012/09/01 BMW\n2012/09/02 Volvo\n\n\nI perform the following  operation to find the number of BMW cars sold everyday\n\ndf[df.CAR=='BMW']['DATE'].value_counts()\n\n\nThe result is something like this : \n\n2012/07/04 15\n2012/07/08 8\n...\n2012/01/02 1\n\n\nBut there are some days when no BMW car was sold. In the result, along with the above I want the days where there are zero occurrences of BMW. Therefore, the desired result is :\n\n2012/07/04 15\n2012/07/08 8\n...\n2012/01/02 1\n2012/01/09 0\n2012/08/11 0\n\n\nWhat can I do to attain such a result?\n"
'I have Three dataframes. All of them have a common column and I need to merge them based on the common column without missing any data\n\nInput\n\n\n>>>df1\n0 Col1  Col2  Col3\n1 data1  3      4\n2 data2  4      3\n3 data3  2      3\n4 data4  2      4\n5 data5  1      4\n\n>>>df2\n0 Col1  Col4  Col5\n1 data1  7      4\n2 data2  6      9\n3 data3  1      4\n\n>>>df3\n0 Col1  Col6  Col7\n1 data2  5      8\n2 data3  2      7\n3 data5  5      3\n\n\n\nExpected Output\n\n\n>>>df\n0 Col1  Col2  Col3  Col4 Col5  Col6  Col7\n1 data1  3      4    7    4\n2 data2  4      3    6    9     5     8\n3 data3  2      3    1    4     2     7\n4 data4  2      4\n5 data5  1      4               5     3\n\n'
'I have a Pandas dataframe containing two columns: a datetime column, and a column of integers representing station IDs. I need a new dataframe with the following modifications: \n\nFor each set of duplicate STATION_ID values, keep the row with the most recent entry for DATE_CHANGED. If the duplicate entries for the STATION_ID all contain the same DATE_CHANGED then drop the duplicates and retain a single row for the STATION_ID. If there are no duplicates for the STATION_ID value, simply retain the row.\n\nDataframe (sorted by STATION_ID):\n\n              DATE_CHANGED  STATION_ID\n0      2006-06-07 06:00:00           1\n1      2000-09-26 06:00:00           1\n2      2000-09-26 06:00:00           1\n3      2000-09-26 06:00:00           1\n4      2001-06-06 06:00:00           2\n5      2005-07-29 06:00:00           2\n6      2005-07-29 06:00:00           2\n7      2001-06-06 06:00:00           2\n8      2001-06-08 06:00:00           4\n9      2003-11-25 07:00:00           4\n10     2001-06-12 06:00:00           7\n11     2001-06-04 06:00:00           8\n12     2017-04-03 18:36:16           8\n13     2017-04-03 18:36:16           8\n14     2017-04-03 18:36:16           8\n15     2001-06-04 06:00:00           8\n16     2001-06-08 06:00:00          10\n17     2001-06-08 06:00:00          10\n18     2001-06-08 06:00:00          11\n19     2001-06-08 06:00:00          11\n20     2001-06-08 06:00:00          12\n21     2001-06-08 06:00:00          12\n22     2001-06-08 06:00:00          13\n23     2001-06-08 06:00:00          13\n24     2001-06-08 06:00:00          14\n25     2001-06-08 06:00:00          14\n26     2001-06-08 06:00:00          15\n27     2017-08-07 17:48:25          15\n28     2001-06-08 06:00:00          15\n29     2017-08-07 17:48:25          15\n...                    ...         ...\n157066 2018-08-06 14:11:28       71655\n157067 2018-08-06 14:11:28       71656\n157068 2018-08-06 14:11:28       71656\n157069 2018-09-11 21:45:05       71664\n157070 2018-09-11 21:45:05       71664\n157071 2018-09-11 21:45:05       71664\n157072 2018-09-11 21:41:04       71664\n157073 2018-08-09 15:22:07       71720\n157074 2018-08-09 15:22:07       71720\n157075 2018-08-09 15:22:07       71720\n157076 2018-08-23 12:43:12       71899\n157077 2018-08-23 12:43:12       71899\n157078 2018-08-23 12:43:12       71899\n157079 2018-09-08 20:21:43       71969\n157080 2018-09-08 20:21:43       71969\n157081 2018-09-08 20:21:43       71969\n157082 2018-09-08 20:21:43       71984\n157083 2018-09-08 20:21:43       71984\n157084 2018-09-08 20:21:43       71984\n157085 2018-09-05 18:46:18       71985\n157086 2018-09-05 18:46:18       71985\n157087 2018-09-05 18:46:18       71985\n157088 2018-09-08 20:21:44       71990\n157089 2018-09-08 20:21:44       71990\n157090 2018-09-08 20:21:44       71990\n157091 2018-09-08 20:21:43       72003\n157092 2018-09-08 20:21:43       72003\n157093 2018-09-08 20:21:43       72003\n157094 2018-09-10 17:06:18       72024\n157095 2018-09-10 17:15:05       72024\n\n[157096 rows x 2 columns]\n\n\nDATE_CHANGED is dtype: datetime64[ns]\n\nSTATION_ID is dtype: int64\n\npandas==0.23.4\n\npython==2.7.15\n'
"I know that there are ways to swap the column order in python pandas.\nLet say I have this example dataset:\n\nimport pandas as pd    \nemployee = {'EmployeeID' : [0,1,2],\n     'FirstName' : ['a','b','c'],\n     'LastName' : ['a','b','c'],\n     'MiddleName' : ['a','b', None],\n     'Contact' : ['(M) 133-245-3123', '(F)a123@gmail.com', '(F)312-533-2442 jimmy234@gmail.com']}\n\ndf = pd.DataFrame(employee)\n\n\nThe one basic way to do would be:\n\nneworder = ['EmployeeID','FirstName','MiddleName','LastName','Contact']\ndf=df.reindex(columns=neworder)\n\n\nHowever, as you can see, I only want to swap two columns. It was doable just because there are only 4 column, but what if I have like 100 columns? what would be an effective way to swap or reorder columns?\n\nThere might be 2 cases:\n\n\nwhen you just want 2 columns swapped.\nwhen you want 3 columns reordered. (I am pretty sure that this case can be applied to more than 3 columns.)\n\n\nThank you guys.\n"
"Is there a way to reindex two dataframes (of differing levels) so that they share a common index across all levels?\n\nDemo:\n\nCreate a basic Dataframe named 'A':\n\nindex = np.array(['AUD','BRL','CAD','EUR','INR'])\ndata = np.random.randint(1, 20, (5,5))\nA = pd.DataFrame(data=data, index=index, columns=index)  \n\n\nCreate a MultiIndex Dataframe named 'B':\n\nnp.random.seed(42)\nmidx1 = pd.MultiIndex.from_product([['Bank_1', 'Bank_2'], \n['AUD','CAD','EUR']], names=['Bank', 'Curency'])\nB = pd.DataFrame(np.random.randint(10,25,6), midx1)\nB.columns = ['Notional']\n\n\nBasic DF:\n\n&gt;&gt;&gt; Dataframe A:\n\n        AUD     BRL     CAD     EUR     INR\nAUD     7       19      11      11      4\nBRL     8       3       2       12      6\nCAD     2       1       12      12      17\nEUR     10      16      15      15      19\nINR     12      3       5       19      7\n\n\nMultiIndex DF:\n\n&gt;&gt;&gt; Dataframe B:\n\n                    Notional\nBank    Curency     \nBank_1  AUD         16\n        CAD         13\n        EUR         22\nBank_2  AUD         24\n        CAD         20\n        EUR         17\n\n\nThe goal is to:\n\n1) reindex B so that its currency level includes each currency in A's index. B would then look like this (see BRL and INR included, their Notional values are not important):\n\n                    Notional\nBank    Curency     \nBank_1  AUD         16\n        CAD         13\n        EUR         22\n        BRL         0\n        INR         0\nBank_2  AUD         24\n        CAD         20\n        EUR         17\n        BRL         0\n        INR         0\n\n\n2) reindex A so that it includes each Bank from the first level of B's index. A would then look like this:\n\n               AUD      BRL     CAD     EUR     INR\nBank_1  AUD     7       19      11      11      4\n        BRL     8       3       2       12      6\n        CAD     2       1       12      12      17\n        EUR     10      16      15      15      19\n        INR     12      3       5       19      7\nBank_2  AUD     7       19      11      11      4\n        BRL     8       3       2       12      6\n        CAD     2       1       12      12      17\n        EUR     10      16      15      15      19\n        INR     12      3       5       19      7\n\n\nThe application of this will be on much larger dataframes so I need a pythonic way to do this.\n\nFor context, ultimately I want to multiply A and B. I am trying to reindex to get matching indices as that was shown as a clean way to multiply dataframes of various index levels here:\nPandas multiply dataframes with multiindex and overlapping index levels\n\nThank you for any help. \n"
"I've got a pandas DataFrame that looks like this:\n\n  molecule            species\n0        a              [dog]\n1        b       [horse, pig]\n2        c         [cat, dog]\n3        d  [cat, horse, pig]\n4        e     [chicken, pig]\n\n\nand I like to extract a DataFrame containing only thoses rows, that contain any of selection = ['cat', 'dog']. So the result should look like this:\n\n  molecule            species\n0        a              [dog]\n1        c         [cat, dog]\n2        d  [cat, horse, pig]\n\n\nWhat would be the simplest way to do this?\n\nFor testing:\n\nselection = ['cat', 'dog']\ndf = pd.DataFrame({'molecule': ['a','b','c','d','e'], 'species' : [['dog'], ['horse','pig'],['cat', 'dog'], ['cat','horse','pig'], ['chicken','pig']]})\n\n"
"Considering a pandas dataframe in python having a column named time of type integer, I can convert it to a datetime format with the following instruction.\n\ndf['time'] = pandas.to_datetime(df['time'], unit='s')\n\n\nso now the column has entries like: 2019-01-15 13:25:43.\n\nWhat is the command to revert the string to an integer timestamp value (representing the number of seconds elapsed from 1970-01-01 00:00:00)?\n\nI checked pandas.Timestamp but could not find a conversion utility and I was not able to use pandas.to_timedelta for this.\n\nIs there any utility for this conversion?\n"
"I am grouping my dataset by column A and then would like to take the minimum value in column B and the corresponding value in column C.\n\ndata = pd.DataFrame({'A': [1, 2], 'B':[ 2, 4], 'C':[10, 4]})\ndata  \n    A   B   C\n0   1   4   3\n1   1   5   4\n2   1   2   10\n3   2   7   2\n4   2   4   4\n5   2   6   6  \n\n\nand I would like to get :\n\n    A   B   C\n0   1   2   10\n1   2   4   4\n\n\nFor the moment I am grouping by A, and creating a value that indicates me the rows I will keep in my dataset:\n\na = data.groupby('A').min()\na['A'] = a.index\nto_keep = [str(x[0]) + str(x[1]) for x in a[['A', 'B']].values]\ndata['id'] = data['A'].astype(str) + data['B'].astype('str')\ndata[data['id'].isin(to_keep)]\n\n\nI am sure that there is a much more straight forward way to do this.\nI have seen many answers here that use multi-indexing but I would like to do this without adding multi-index to my dataframe.\nThank you for your help.\n"
'I want to take the logarithm of every value in a pandas dataframe. I have tried this but it does not work:\n\n#Reading data from excel and rounding values on 2 decimal places\nimport math\nimport pandas as pd\n\ndata = pd.read_excel("DataSet.xls").round(2)\nlog_data= math.log10(data)\n\n\nIt gives me this error:\n\n\n  TypeError: must be real number, not DataFrame\n\n\nDo you have any idea what to do?\n'
'This may sound like a very broad question, but if you\'ll let me describe some details I can assure you it\'s very specific. As well as discouraging, frustrating and rage-inducing.\n\n\n\nThe following plot describes a scottish election and is based on code from plot.ly:\n\nPlot 1:\n\n\n\nDataset 1:\n\ndata = [[\'Source\',\'Target\',\'Value\',\'Color\',\'Node, Label\',\'Link Color\'],\n        [0,5,20,\'#F27420\',\'Remain+No – 28\',\'rgba(253, 227, 212, 0.5)\'],\n        [0,6,3,\'#4994CE\',\'Leave+No – 16\',\'rgba(242, 116, 32, 1)\'],\n        [0,7,5,\'#FABC13\',\'Remain+Yes – 21\',\'rgba(253, 227, 212, 0.5)\'],\n        [1,5,14,\'#7FC241\',\'Leave+Yes – 14\',\'rgba(219, 233, 246, 0.5)\'],\n        [1,6,1,\'#D3D3D3\',\'Didn’t vote in at least one referendum – 21\',\'rgba(73, 148, 206, 1)\'],\n        [1,7,1,\'#8A5988\',\'46 – No\',\'rgba(219, 233, 246,0.5)\'],\n        [2,5,3,\'#449E9E\',\'39 – Yes\',\'rgba(250, 188, 19, 1)\'],\n        [2,6,17,\'#D3D3D3\',\'14 – Don’t know / would not vote\',\'rgba(250, 188, 19, 0.5)\'],\n        [2,7,2,\'\',\'\',\'rgba(250, 188, 19, 0.5)\'],\n        [3,5,3,\'\',\'\',\'rgba(127, 194, 65, 1)\'],\n        [3,6,9,\'\',\'\',\'rgba(127, 194, 65, 0.5)\'],\n        [3,7,2,\'\',\'\',\'rgba(127, 194, 65, 0.5)\'],\n        [4,5,5,\'\',\'\',\'rgba(211, 211, 211, 0.5)\'],\n        [4,6,9,\'\',\'\',\'rgba(211, 211, 211, 0.5)\'],\n        [4,7,8,\'\',\'\',\'rgba(211, 211, 211, 0.5)\']\n        ]\n\n\nHow the plot is built:\n\nI\'ve picked up some important details about the behavior of sankey charts from various sources, like:\n\n\nSankey automatically orders the categories to minimize the amount of overlap\nLinks are assigned in the order they appear in dataset (row_wise)\nFor the nodes colors are assigned in the order plot is built.\n\n\nThe challenge:\n\nAs you\'ll see in the details below, nodes, labels and colors are not applied to the chart in the same order that the source dataframe is structured. Some of that makes perfect sence, since you have various elements that describe the same node like color, targets, values and link color. One  node \'Remain+No – 28\' looks like this:\n\n\n\nAnd the accompanying part of the dataset looks like this:\n\n[0,5,20,\'#F27420\',\'Remain+No – 28\',\'rgba(253, 227, 212, 0.5)\'],\n[0,6,3,\'#4994CE\',\'Leave+No – 16\',\'rgba(242, 116, 32, 1)\'],\n[0,7,5,\'#FABC13\',\'Remain+Yes – 21\',\'rgba(253, 227, 212, 0.5)\'],\n\n\nSo this part of the source describes a node [0] with three corresponding targets [5, 6, 7] and three links with the values [20, 3, 5]. \'#F27420\' is the orange(ish) color of the node, and the colors \'rgba(253, 227, 212, 0.5)\', \'rgba(242, 116, 32, 1)\' and \'rgba(253, 227, 212, 0.5)\' describe the colors of the links from the node to some targets. So far, the information that has not been used from the sample above is:\n\nData sample 2 (partial)\n\n[-,-,--\'-------\',\'---------------\',\'-------------------\'],\n[-,-,-,\'#4994CE\',\'Leave+No – 16\',\'-------------------\'],\n[-,-,-,\'#FABC13\',\'Remain+Yes – 21\',\'-------------------\'],\n\n\nAnd that information is used as the remaining elements of the diagram are indtroduced.\n\nSo, what\'s the question? In the further details below, you\'ll see that everything makes sense as long as a new row of data in the dataset inserts a new link, and makes other changes to other elements (colors, labels) if that information has not yet ben used. I\'ll be even more specific with the use of two screenshots from a setup I\'ve made with plot to the left and code to the right:\n\nThe following data sample produces the diagram below following the logic desbribed above:\n\nData sample 3\n\ndata = [[\'Source\',\'Target\',\'Value\',\'Color\',\'Node, Label\',\'Link Color\'],\n        [0,5,20,\'#F27420\',\'Remain+No – 28\',\'rgba(253, 227, 212, 0.5)\'],\n        [0,6,3,\'#4994CE\',\'Leave+No – 16\',\'rgba(242, 116, 32, 1)\'],\n        [0,7,5,\'#FABC13\',\'Remain+Yes – 21\',\'rgba(253, 227, 212, 0.5)\'],\n        [1,5,14,\'#7FC241\',\'Leave+Yes – 14\',\'rgba(219, 233, 246, 0.5)\'],\n        [1,6,1,\'#D3D3D3\',\'Didn’t vote in at least one referendum – 21\',\'rgba(73, 148, 206, 1)\']]\n\n\nScreenshot 1 - Partial plot with data sample 3\n\n\n\nTHE QUESTION:\n\nAdding the row [1,7,1,\'#8A5988\',\'46 – No\',\'rgba(219, 233, 246,0.5)\'] in the dataset produces a new link between source [5] and target [7] but applies color and label to a target 5 at the same time. I would think that the next label to be applied to the chart was \'Remain+Yes – 21\' since it hasn\'t been used. But what happens here is that the label \'46 – No\' is applied to Target 5. WHY?\n\nScreenshot 2 - Partial plot with data sample 3 + [1,7,1,\'#8A5988\',\'46 – No\',\'rgba(219, 233, 246,0.5)\'] :\n\n\n\nAnd how do you discern what is a source and what is a target based on that dataframe?\n\nI know that the question is both strange and hard to answer, but I\'m hoping someone has a suggestion. I also know that a dataframe may not be the best source for a sankey chart. Perhaps json instead?\n\n\n\nComplete code and data sample for an easy copy&amp;paste for a Jupyter Notebook:\n\n\n\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n# Original data\ndata = [[\'Source\',\'Target\',\'Value\',\'Color\',\'Node, Label\',\'Link Color\'],\n    [0,5,20,\'#F27420\',\'Remain+No – 28\',\'rgba(253, 227, 212, 0.5)\'],\n    [0,6,3,\'#4994CE\',\'Leave+No – 16\',\'rgba(242, 116, 32, 1)\'],\n    [0,7,5,\'#FABC13\',\'Remain+Yes – 21\',\'rgba(253, 227, 212, 0.5)\'],\n    [1,5,14,\'#7FC241\',\'Leave+Yes – 14\',\'rgba(219, 233, 246, 0.5)\'],\n    [1,6,1,\'#D3D3D3\',\'Didn’t vote in at least one referendum – 21\',\'rgba(73, 148, 206, 1)\'],\n    [1,7,1,\'#8A5988\',\'46 – No\',\'rgba(219, 233, 246,0.5)\'],\n    [2,5,3,\'#449E9E\',\'39 – Yes\',\'rgba(250, 188, 19, 1)\'],\n    [2,6,17,\'#D3D3D3\',\'14 – Don’t know / would not vote\',\'rgba(250, 188, 19, 0.5)\'],\n    [2,7,2,\'\',\'\',\'rgba(250, 188, 19, 0.5)\'],\n    [3,5,3,\'\',\'\',\'rgba(127, 194, 65, 1)\'],\n    [3,6,9,\'\',\'\',\'rgba(127, 194, 65, 0.5)\'],\n    [3,7,2,\'\',\'\',\'rgba(127, 194, 65, 0.5)\'],\n    [4,5,5,\'\',\'\',\'rgba(211, 211, 211, 0.5)\'],\n    [4,6,9,\'\',\'\',\'rgba(211, 211, 211, 0.5)\'],\n    [4,7,8,\'\',\'\',\'rgba(211, 211, 211, 0.5)\']\n    ]\n\n\n\nheaders = data.pop(0)\ndf = pd.DataFrame(data, columns = headers)\nscottish_df = df\n\ndata_trace = dict(\n    type=\'sankey\',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = "h",\n    valueformat = ".0f",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = "black",\n        width = 0\n      ),\n      label =  scottish_df[\'Node, Label\'].dropna(axis=0, how=\'any\'),\n      color = scottish_df[\'Color\']\n    ),\n    link = dict(\n      source = scottish_df[\'Source\'].dropna(axis=0, how=\'any\'),\n      target = scottish_df[\'Target\'].dropna(axis=0, how=\'any\'),\n      value = scottish_df[\'Value\'].dropna(axis=0, how=\'any\'),\n      color = scottish_df[\'Link Color\'].dropna(axis=0, how=\'any\'),\n  )\n)\n\nlayout =  dict(\n    title = "Scottish Referendum Voters who now want Independence",\n    height = 772,\n    font = dict(\n      size = 10\n    ),    \n)\n\nfig = dict(data=[data_trace], layout=layout)\niplot(fig, validate=False)\n\n'
'I have a text file with the following format:\n\n1: frack 0.733, shale 0.700, \n10: space 0.645, station 0.327, nasa 0.258, \n4: celebr 0.262, bahar 0.345 \n\n\nI need to covert this text to a DataFrame with the following format:\n\nId   Term    weight\n1    frack   0.733\n1    shale   0.700\n10   space   0.645\n10   station 0.327\n10   nasa    0.258\n4    celebr  0.262\n4    bahar   0.345\n\n\nHow I can do it?\n'
'So far I used this line of code here:\n\nmax_total_gross = event_data["max_total_gross"].loc[event_data["event_id"] == event_id].item()\n\n\nSince I updated Pandas I receive the future warning:\n\n\n  /opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12:\n  FutureWarning: item has been deprecated and will be removed in a\n  future version   if sys.path[0] == \'\':\n\n\nI tried to fix it that way, but it\'s not the same outcome:\n\nevent_data.loc[event_data.event_id == event_id, \'max_total_gross\']\n\n\nI expected a single integer.\n'
'How to get the most frequent row in a DataFrame?\nFor example, if I have the following table:\n   col_1  col_2 col_3\n0      1      1     A\n1      1      0     A\n2      0      1     A\n3      1      1     A\n4      1      0     B\n5      1      0     C\n\nExpected result:\n   col_1  col_2 col_3\n0      1      1     A\n\nEDIT: I need the most frequent row (as one unit) and not the most frequent column value that can be calculated with the mode() method.\n'
"Problem\nI have the following Pandas dataframe:\n    data = {\n        'ID':  [100, 100, 100, 100, 200, 200, 200, 200, 200, 300, 300, 300, 300, 300],\n        'value': [False, False, True, False, False, True, True, True, False, False, False, True, True, False],\n    }\n    df = pandas.DataFrame (data, columns = ['ID','value'])\n\nI want to get the following groups:\n\nGroup 1: for each ID, all False rows until the first True row of that ID\nGroup 2: for each ID, all False rows after the last True row of that ID\nGroup 3: all true rows\n\n\nCan this be done with pandas?\nWhat I've tried\nI've tried\ngroup = df.groupby((df['value'].shift() != df['value']).cumsum())\n\nbut this returns an incorrect result.\n"
"I have a pandas.DatetimeIndex, e.g.:\n\npd.date_range('2012-1-1 02:03:04.000',periods=3,freq='1ms')\n&gt;&gt;&gt; [2012-01-01 02:03:04, ..., 2012-01-01 02:03:04.002000]\n\n\nI would like to round the dates (Timestamps) to the nearest second. How do I do that? The expected result is similar to:\n\n[2012-01-01 02:03:04.000000, ..., 2012-01-01 02:03:04.000000]\n\n\nIs it possible to accomplish this by rounding a Numpy datetime64[ns] to seconds without changing the dtype [ns]?\n\nnp.array(['2012-01-02 00:00:00.001'],dtype='datetime64[ns]')\n\n"
"I'm trying to create a new Pandas dataframe column with ordinal day from a datetime column:\n\nimport pandas as pd\nfrom datetime import datetime\n\nprint df.ix[0:5]\n                              date\nfile                              \ngom3_197801.nc 2011-02-16 00:00:00\ngom3_197802.nc 2011-02-16 00:00:00\ngom3_197803.nc 2011-02-15 00:00:00\ngom3_197804.nc 2011-02-17 00:00:00\ngom3_197805.nc 2011-11-14 00:00:00\n\ndf['date'][0].toordinal()\n\nOut[6]:\n734184\n\ndf['date'].toordinal()\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-7-dbfd5e8b60f0&gt; in &lt;module&gt;()\n----&gt; 1 df['date'].toordinal()\n\nAttributeError: 'Series' object has no attribute 'toordinal'\n\n\nI guess this is a basic question, but I've struggled reading docs for last 30 minutes. \n\nHow can I create an ordinal time column for my dataframe?\n"
'I can query an explicit value:\n\nfills.query(\'Symbol=="BUD US"\')\n\n\nNow I want to query a variable:\n\nIn [40]: my_symbol\nOut[40]: \'BUD US\'\n\n\nIn pandas 0.13.1, I could simply use:\n\nfills.query(\'Symbol==my_symbol\')\n\n\nThis is no longer permitted in pandas 0.14.0 because local variables must be referred to explicitly. So I tried\n\nfills.query(\'Symbol==@my_symbol\')\n\n\nbut this returned an error\n\nKeyError: \'__pd_eval_local_my_symbol\'\n\n\nI was able to change some of my other code to use explicit local variables, but this one just will not take. Any thoughts? The dtype is object, if that helps.\n'
'I would like to plot two horizontal bar charts sharing same y axis. For example, the following question shows how to achieve this in R:\n\nTwo horizontal bar charts with shared axis in ggplot2 (similar to population pyramid)\n\nHow can I create a similar plot with Python?\n\nThe plot from the question above looks like this:\n\n\n\nHere is the list of states used in the graph above (the y axis):\n\n["AK", "TX", "CA", "MT", "NM", "AZ", "NV", "CO", "OR", "WY", \n "MI", "MN", "UT", "ID", "KS", "NE", "SD", "WA", "ND", "OK"]\n\n\nHere is the list of the numbers of sales staff for each state:\n\n[20,30,40,10,15,35,18,25,22,7,12,22,3,4,5,8,14,28,24,32]\n\n\nThe sales figures can be random.\n'
'How can I handle unknown values for label encoding in sk-learn?\nThe label encoder will only blow up with an exception that new labels were detected.\n\nWhat I want is the encoding of categorical variables via one-hot-encoder. However, sk-learn does not support strings for that. So I used a label encoder on each column. \n\nMy problem is that in my cross-validation step of the pipeline unknown labels show up.\nThe basic one-hot-encoder would have the option to ignore such cases.\nAn apriori pandas.getDummies /cat.codes is not sufficient as the pipeline should work with real-life, fresh incoming data which might contain unknown labels as well.\n\nWould it be possible to use a CountVectorizer for this purpose?\n'
"I have a Pandas data frame which is MultiIndexed. The second level contains a year ([2014,2015]) and the third contains the month number ([1, 2, .., 12]). I would like to merge these two into a single level like - [1/2014, 2/2014 ..., 6/2015]. How could this be done? \n\nI'm new to Pandas. Searched a lot but could not find any similar question/solution.\n\nEdit: I found a way to avoid having to do this altogether with the answer to this question. I should have been creating my data frame that way. This seems to be the way to go for indexing by DateTime.\n"
"I have a Pandas DataFrame with a DatetimeIndex and one column MSE Loss\nthe index is formatted as follows: \n\nDatetimeIndex(['2015-07-16 07:14:41', '2015-07-16 07:14:48',\n           '2015-07-16 07:14:54', '2015-07-16 07:15:01',\n           '2015-07-16 07:15:07', '2015-07-16 07:15:14',...]\n\n\nIt includes several days.\n\nI want to select all the rows (all times) of a particular days without specifically knowing the actual time intervals. \nFor example: Between 2015-07-16 07:00:00 and 2015-07-16 23:00:00\n\nI tried the approach outlined here: here\n\nBut df[date_from:date_to]\n\noutputs:\n\nKeyError: Timestamp('2015-07-16 07:00:00')\n\n\nSo it wants exact indices. Furthermore, I don't have a datecolumn. Only an index with the dates.\n\nWhat is the best way to select a whole day by just providing a date 2015-07-16 and then how could I select a specific time range within a particular day?\n"
'I have been trying to change the column names of a pandas dataframe using a list of names. The following code is being used:\n\ndf.rename(columns = list_of_names, inplace=True)\n\n\nHowever I got a Type Error each time, with an error message that says "list object is not callable".\n  I would like to know why does this happen? And What can I do to solve this problem? \nThank you for your help.\n'
'In Python Pandas, I have a DataFrame. I group this DataFrame by a column and want to assign the last value of a column to all rows of another column.\n\nI know that I am able to select the last row of the group by this command:\n\nimport pandas as pd\n\ndf = pd.DataFrame({\'a\': (1,1,2,3,3), \'b\':(20,21,30,40,41)})\nprint(df)\nprint("-")\nresult = df.groupby(\'a\').nth(-1)\nprint(result)\n\n\nResult:\n\n   a   b\n0  1  20\n1  1  21\n2  2  30\n3  3  40\n4  3  41\n-\n    b\na    \n1  21\n2  30\n3  41\n\n\nHow would it be possible to assign the result of this operation back to the original dataframe so that I have something like:\n\n   a   b b_new\n0  1  20 21\n1  1  21 21\n2  2  30 30\n3  3  40 41\n4  3  41 41\n\n'
