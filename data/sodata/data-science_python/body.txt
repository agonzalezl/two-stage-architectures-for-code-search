"If I want to use the BatchNormalization function in Keras, then do I need to call it once only at the beginning?\n\nI read this documentation for it: http://keras.io/layers/normalization/\n\nI don't see where I'm supposed to call it. Below is my code attempting to use it:\n\nmodel = Sequential()\nkeras.layers.normalization.BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None)\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n\n\nI ask because if I run the code with the second line including the batch normalization and if I run the code without the second line I get similar outputs. So either I'm not calling the function in the right place, or I guess it doesn't make that much of a difference.\n"
'I\'m facing an issue with allocating huge arrays in numpy on Ubuntu 18 while not facing the same issue on MacOS.\n\nI am trying to allocate memory for a numpy array with shape (156816, 36, 53806)\nwith \n\nnp.zeros((156816, 36, 53806), dtype=\'uint8\')\n\n\nand while I\'m getting an error on Ubuntu OS\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.zeros((156816, 36, 53806), dtype=\'uint8\')\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nnumpy.core._exceptions.MemoryError: Unable to allocate array with shape (156816, 36, 53806) and data type uint8\n\n\nI\'m not getting it on MacOS:\n\n&gt;&gt;&gt; import numpy as np \n&gt;&gt;&gt; np.zeros((156816, 36, 53806), dtype=\'uint8\')\narray([[[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       ...,\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]],\n\n       [[0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        ...,\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)\n\n\nI\'ve read somewhere that np.zeros shouldn\'t be really allocating the whole memory needed for the array, but only for the non-zero elements. Even though the Ubuntu machine has 64gb of memory, while my MacBook Pro has only 16gb.\n\nversions:\n\nUbuntu\nos -&gt; ubuntu mate 18\npython -&gt; 3.6.8\nnumpy -&gt; 1.17.0\n\nmac\nos -&gt; 10.14.6\npython -&gt; 3.6.4\nnumpy -&gt; 1.17.0\n\n\nPS: also failed on Google Colab\n'
'How to load a model from an HDF5 file in Keras?\n\nWhat I tried:\n\nmodel = Sequential()\n\nmodel.add(Dense(64, input_dim=14, init=\'uniform\'))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, init=\'uniform\'))\nmodel.add(LeakyReLU(alpha=0.3))\nmodel.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(2, init=\'uniform\'))\nmodel.add(Activation(\'softmax\'))\n\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=\'binary_crossentropy\', optimizer=sgd)\n\ncheckpointer = ModelCheckpoint(filepath="/weights.hdf5", verbose=1, save_best_only=True)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2, callbacks=[checkpointer])\n\n\nThe above code successfully saves the best model to a file named weights.hdf5. What I want to do is then load that model. The below code shows how I tried to do so:\n\nmodel2 = Sequential()\nmodel2.load_weights("/Users/Desktop/SquareSpace/weights.hdf5")\n\n\nThis is the error I get:\n\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-101-ec968f9e95c5&gt; in &lt;module&gt;()\n      1 model2 = Sequential()\n----&gt; 2 model2.load_weights("/Users/Desktop/SquareSpace/weights.hdf5")\n\n/Applications/anaconda/lib/python2.7/site-packages/keras/models.pyc in load_weights(self, filepath)\n    582             g = f[\'layer_{}\'.format(k)]\n    583             weights = [g[\'param_{}\'.format(p)] for p in range(g.attrs[\'nb_params\'])]\n--&gt; 584             self.layers[k].set_weights(weights)\n    585         f.close()\n    586 \n\nIndexError: list index out of range\n\n'
"I have just built my first model using Keras and this is the output. It looks like the standard output you get after building any Keras artificial neural network. Even after looking in the documentation, I do not fully understand what the epoch is and what the loss is which is printed in the output.\n\nWhat is epoch and loss in Keras? \n\n(I know it's probably an extremely basic question, but I couldn't seem to locate the answer online, and if the answer is really that hard to glean from the documentation I thought others would have the same question and thus decided to post it here.)\n\nEpoch 1/20\n1213/1213 [==============================] - 0s - loss: 0.1760     \nEpoch 2/20\n1213/1213 [==============================] - 0s - loss: 0.1840     \nEpoch 3/20\n1213/1213 [==============================] - 0s - loss: 0.1816     \nEpoch 4/20\n1213/1213 [==============================] - 0s - loss: 0.1915     \nEpoch 5/20\n1213/1213 [==============================] - 0s - loss: 0.1928     \nEpoch 6/20\n1213/1213 [==============================] - 0s - loss: 0.1964     \nEpoch 7/20\n1213/1213 [==============================] - 0s - loss: 0.1948     \nEpoch 8/20\n1213/1213 [==============================] - 0s - loss: 0.1971     \nEpoch 9/20\n1213/1213 [==============================] - 0s - loss: 0.1899     \nEpoch 10/20\n1213/1213 [==============================] - 0s - loss: 0.1957     \nEpoch 11/20\n1213/1213 [==============================] - 0s - loss: 0.1923     \nEpoch 12/20\n1213/1213 [==============================] - 0s - loss: 0.1910     \nEpoch 13/20\n1213/1213 [==============================] - 0s - loss: 0.2104     \nEpoch 14/20\n1213/1213 [==============================] - 0s - loss: 0.1976     \nEpoch 15/20\n1213/1213 [==============================] - 0s - loss: 0.1979     \nEpoch 16/20\n1213/1213 [==============================] - 0s - loss: 0.2036     \nEpoch 17/20\n1213/1213 [==============================] - 0s - loss: 0.2019     \nEpoch 18/20\n1213/1213 [==============================] - 0s - loss: 0.1978     \nEpoch 19/20\n1213/1213 [==============================] - 0s - loss: 0.1954     \nEpoch 20/20\n1213/1213 [==============================] - 0s - loss: 0.1949\n\n"
'This is my code that works if I use other activation layers like tanh:\n\nmodel = Sequential()\nact = keras.layers.advanced_activations.PReLU(init=\'zero\', weights=None)\nmodel.add(Dense(64, input_dim=14, init=\'uniform\'))\nmodel.add(Activation(act))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(64, init=\'uniform\'))\nmodel.add(Activation(\'softplus\'))\nmodel.add(Dropout(0.15))\nmodel.add(Dense(2, init=\'uniform\'))\nmodel.add(Activation(\'softmax\'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss=\'binary_crossentropy\', optimizer=sgd)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n\n\nIn this case, it doesn\'t work and says "TypeError: \'PReLU\' object is not callable" and the error is called at the model.compile line. Why is this the case? All the non-advanced activation functions works. However, neither of the advanced activation functions, including this one, works.\n'
'I want to to use numbers to indicate references in footnotes, so I was wondering inside of Jupyter Notebook how can I use superscripts and subscripts?\n'
"I am trying to groupby a column and compute value counts on another column.\n\nimport pandas as pd\ndftest = pd.DataFrame({'A':[1,1,1,1,1,1,1,1,1,2,2,2,2,2], \n               'Amt':[20,20,20,30,30,30,30,40, 40,10, 10, 40,40,40]})\n\nprint(dftest)\n\n\ndftest looks like\n\n    A  Amt\n0   1   20\n1   1   20\n2   1   20\n3   1   30\n4   1   30\n5   1   30\n6   1   30\n7   1   40\n8   1   40\n9   2   10\n10  2   10\n11  2   40\n12  2   40\n13  2   40\n\n\nperform grouping\n\ngrouper = dftest.groupby('A')\ndf_grouped = grouper['Amt'].value_counts()\n\n\nwhich gives\n\n   A  Amt\n1  30     4\n   20     3\n   40     2\n2  40     3\n   10     2\nName: Amt, dtype: int64\n\n\nwhat I want is to keep top two rows of each group\n\nAlso, I was perplexed by an error when I tried to reset_index\n\ndf_grouped.reset_index()\n\n\nwhich gives following error\n\n\n  df_grouped.reset_index()\n  ValueError: cannot insert Amt, already exists\n\n"
"I want to use sklearn's StandardScaler. Is it possible to apply it to some feature columns but not others?\n\nFor instance, say my data is:\n\ndata = pd.DataFrame({'Name' : [3, 4,6], 'Age' : [18, 92,98], 'Weight' : [68, 59,49]})\n\n   Age  Name  Weight\n0   18     3      68\n1   92     4      59\n2   98     6      49\n\n\ncol_names = ['Name', 'Age', 'Weight']\nfeatures = data[col_names]\n\n\nI fit and transform the data\n\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\nscaled_features = pd.DataFrame(features, columns = col_names)\n\n       Name       Age    Weight\n0 -1.069045 -1.411004  1.202703\n1 -0.267261  0.623041  0.042954\n2  1.336306  0.787964 -1.245657\n\n\nBut of course the names are not really integers but strings and I don't want to standardize them. How can I apply the fit and transform methods only on the columns Age and Weight?\n"
'I am doing a data cleaning exercise on python and the text that I am cleaning contains Italian words which I would like to remove. I have been searching online whether I would be able to do this on Python using a tool kit like nltk. \n\nFor example given some text : \n\n"Io andiamo to the beach with my amico."\n\n\nI would like to be left with : \n\n"to the beach with my" \n\n\nDoes anyone know of a way as to how this could be done?\nAny help would be much appreciated. \n'
'I have a dataframe that looks like this:\n\nfrom    to         datetime              other\n-------------------------------------------------\n11      1     2016-11-06 22:00:00          -\n11      1     2016-11-06 20:00:00          -\n11      1     2016-11-06 15:45:00          -\n11      12    2016-11-06 15:00:00          -\n11      1     2016-11-06 12:00:00          -\n11      18    2016-11-05 10:00:00          -\n11      12    2016-11-05 10:00:00          -\n12      1     2016-10-05 10:00:59          -\n12      3     2016-09-06 10:00:34          -\n\n\nI want to groupby "from" and then "to" columns and then sort the "datetime" in descending order and then finally want to calculate the time difference within these grouped by objects between the current time and the next time. For eg, in this case,\nI would like to have a dataframe like the following:\n\nfrom    to     timediff in minutes                                          others\n11      1            120\n11      1            255\n11      1            225\n11      1            0 (preferrably subtract this date from the epoch)\n11      12           300\n11      12           0\n11      18           0\n12      1            25\n12      3            0\n\n\nI can\'t get my head around figuring this out!! Is there a way out for this?\nAny help will be much much appreciated!!\nThank you so much in advance!\n'
"I have dataframe total_year, which contains three columns (year, action, comedy) .\ntotal_year\n\nI want to plot the year column on the x-axis, and action &amp; comedy both on the y-axis.\nHow can I plot two columns (action and comedy) on y-axis?\nMy code plots only one column on y-axis.\ntotal_year[-15:].plot(x='year', y='action', figsize=(10,5), grid=True)\n\n\n"
'I have a dataset from a number of users (nUsers). Each user is sampled randomly in time (non-constant nSamples for each user). Each sample has a number of features (nFeatures). \nFor example:\n\nnUsers = 3 ---> 3 users\n\nnSamples = [32, 52, 21] ---> first user was sampled 32 times second user was sampled 52 times etc.\n\nnFeatures = 10 ---> constant number of features for each sample.\n\nI would like the LSTM to produce a current prediction based on the current features and on previous predictions of the same user. \nCan I do that in Keras using LSTM layer? \nI have 2 problems: \n\n\nThe data has a different time series for each user. How do I incorporate this?\nHow do I deal with adding the previous predictions into the current time feature space in order to make a current prediction?\n\n\nThanks for your help!\n'
"I am trying to decompose a 3D matrix using python library scikit-tensor. I managed to decompose my Tensor (with dimensions 100x50x5) into three matrices. My question is how can I compose the initial matrix again using the decomposed matrix produced with Tensor factorization? I want to check if the decomposition has any meaning. My code is the following:\n\nimport logging\nfrom scipy.io.matlab import loadmat\nfrom sktensor import dtensor, cp_als\nimport numpy as np\n\n//Set logging to DEBUG to see CP-ALS information\nlogging.basicConfig(level=logging.DEBUG)\nT = np.ones((400, 50))\nT = dtensor(T)\nP, fit, itr, exectimes = cp_als(T, 10, init='random')\n// how can I re-compose the Matrix T? TA = np.dot(P.U[0], P.U[1].T)\n\n\nI am using the canonical decomposition as provided from the scikit-tensor library function cp_als. Also what is the expected dimensionality of the decomposed matrices?\n"
"An irregular time series data is stored in a pandas.DataFrame. A DatetimeIndex has been set. I need the time difference between consecutive entries in the index. \n\nI thought it would be as simple as\n\ndata.index.diff()\n\n\nbut got\n\nAttributeError: 'DatetimeIndex' object has no attribute 'diff'\n\n\nI tried\n\ndata.index - data.index.shift(1)\n\n\nbut got\n\nValueError: Cannot shift with no freq\n\n\nI do not want to infer or enforce a frequency first before doing this operation. There are large gaps in the time series that would be expanded to large runs of nan. The point is to find these gaps first.\n\nSo, what is a clean way to do this seemingly simple operation? \n"
"Applying pandas.to_numeric to a dataframe column which contains strings that represent numbers (and possibly other unparsable strings) results in an error message like this:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-66-07383316d7b6&gt; in &lt;module&gt;()\n      1 for column in shouldBeNumericColumns:\n----&gt; 2     trainData[column] = pandas.to_numeric(trainData[column])\n\n/usr/local/lib/python3.5/site-packages/pandas/tools/util.py in to_numeric(arg, errors)\n    113         try:\n    114             values = lib.maybe_convert_numeric(values, set(),\n--&gt; 115                                                coerce_numeric=coerce_numeric)\n    116         except:\n    117             if errors == 'raise':\n\npandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:53558)()\n\npandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:53344)()\n\nValueError: Unable to parse string\n\n\nWouldn't it be helpful to see which value failed to parse? \n"
"The title says it all.  When you are working R and using RStudio, its really easy and simple to debug something by dropping a browser() call anywhere in your code and seeing what goes wrong.  Is there a way to do that with Python? I'm slowly getting very sick of print statement debugging.\n"
"Hey I'm new to Pandas and I just came across df.query().\n\nWhy people would use df.query() when you can directly filter your Dataframes using brackets notation ? The official pandas tutorial also seems to prefer the latter approach.\n\nWith brackets notation :\n\ndf[df['age'] &lt;= 21]\n\n\nWith pandas query method :\n\ndf.query('age &lt;= 21')\n\n\nBesides some of the stylistic or flexibility differences that have been mentioned, is one canonically preferred - namely for performance of operations on large dataframes?\n"
"I am trying to drop NA values from a pandas dataframe.\n\nI have used dropna() (which should drop all NA rows from the dataframe). Yet, it does not work.\n\nHere is the code:\n\nimport pandas as pd\nimport numpy as np\nprison_data = pd.read_csv('https://andrewshinsuke.me/docs/compas-scores-two-years.csv')\n\n\nThat's how you get the data frame. As the following shows, the default read_csv method does indeed convert the NA data points to np.nan.\n\nnp.isnan(prison_data.head()['out_custody'][4])\n\nOut[2]: True\n\n\nConveniently, the head() of the DF already contains a NaN values (in the column out_custody), so printing prison_data.head() this, you get:\n\n   id                name   first         last compas_screening_date   sex  \n\n0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male\n1   3         kevon dixon   kevon        dixon            2013-01-27  Male\n2   4            ed philo      ed        philo            2013-04-14  Male\n3   5         marcu brown   marcu        brown            2013-01-13  Male\n4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male\n\n      dob  age          age_cat              race      ...        \n0  1947-04-18   69  Greater than 45             Other      ...\n1  1982-01-22   34          25 - 45  African-American      ...\n2  1991-05-14   24     Less than 25  African-American      ...\n3  1993-01-21   23     Less than 25  African-American      ...\n4  1973-01-22   43          25 - 45             Other      ...\n\n   v_decile_score  v_score_text  v_screening_date  in_custody  out_custody  \n\n0               1           Low        2013-08-14  2014-07-07   2014-07-14\n1               1           Low        2013-01-27  2013-01-26   2013-02-05\n2               3           Low        2013-04-14  2013-06-16   2013-06-16\n3               6        Medium        2013-01-13         NaN          NaN\n4               1           Low        2013-03-26         NaN          NaN\n\npriors_count.1 start   end event two_year_recid\n0               0     0   327     0              0\n1               0     9   159     1              1\n2               4     0    63     0              1\n3               1     0  1174     0              0\n4               2     0  1102     0              0\n\n\nHowever, running prison_data.dropna() does not change the dataframe in any way.\n\nprison_data.dropna()\nnp.isnan(prison_data.head()['out_custody'][4])\n\n\nOut[3]: True\n\n"
"I'm preprocessing my data before implementing a machine learning model. Some of the features are with high cardinality, like country and language.\nSince encoding those features as one-hot-vector can produce sparse data, I've decided to look into the hashing trick and used python's category_encoders like so:\nfrom category_encoders.hashing import HashingEncoder\nce_hash = HashingEncoder(cols = ['country'])\nencoded = ce_hash.fit_transform(df.country)\nencoded['country'] = df.country\nencoded.head()\n\nWhen looking at the result, I can see the collisions\n    col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7 country\n0       0      0      1      0      0      0      0      0      US &lt;━┓\n1       0      1      0      0      0      0      0      0      CA.  ┃ US and SE collides \n2       0      0      1      0      0      0      0      0      SE &lt;━┛\n3       0      0      0      0      0      0      1      0      JP\n\nFurther investigation lead me to this Kaggle article. The example of Hashing there include both X and y.\n\nWhat is the purpose of y, does it help to fight the collision problem?\nShould I add more columns to the encoder and encode more than one feature together (for example country and language)?\n\nWill appreciate an explanation of how to encode such categories using the hashing trick.\nUpdate:\nBased on the comments I got from @CoMartel, Iv'e looked at Sklearn FeatureHasher and written the following code to hash the country column:\nfrom sklearn.feature_extraction import FeatureHasher\nh = FeatureHasher(n_features=10,input_type='string')\nf = h.transform(df.country)\ndf1 = pd.DataFrame(f.toarray())\ndf1['country'] = df.country\ndf1.head()\n\nAnd got the following output:\n     0    1    2    3    4    5    6    7    8    9 country\n0 -1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0      US\n1 -1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0      US\n2 -1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 -1.0  0.0      US\n3  0.0 -1.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      CA\n4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0 -1.0  0.0      SE\n5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      JP\n6 -1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      AU\n7 -1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0      AU\n8 -1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0      DK\n9  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0 -1.0  0.0      SE\n\n\nIs that the way to use the library in order to encode high categorical\nvalues?\nWhy are some values negative?\nHow would you choose the &quot;right&quot; n_features value?\nHow can I check the collisions ratio?\n\n"
'I\'m trying to plot multiple series with two measurements (so it\'s actually num_of_time_series x 2 graphs) in one figure using pygal.\nFor instance, suppose mt data is:\n\nfrom collections import defaultdict\n\nmeasurement_1=defaultdict(None,[\n  ("component1", [11.83, 11.35, 0.55]), \n  ("component2", [2.19, 2.42, 0.96]),\n  ("component3", [1.98, 2.17, 0.17])])\n\nmeasurement_2=defaultdict(None,[\n  ("component1", [34940.57, 35260.41, 370.45]),\n  ("component2", [1360.67, 1369.58, 2.69]),\n  ("component3", [13355.60, 14790.81, 55.63])])\n\nx_labels=[\'2016-12-01\', \'2016-12-02\', \'2016-12-03\']\n\n\nand the graph rendering code is that:\n\nfrom pygal import graph\nimport pygal\ndef draw(measurement_1, measurement_2 ,x_labels):\n  graph = pygal.Line()\n  graph.x_labels = x_labels\n\n  for key, value in measurement_1.iteritems():\n      graph.add(key, value)\n  for key, value in measurement_2.iteritems():\n      graph.add(key, value, secondary=True)\n\n  return graph.render_data_uri()\n\n\nThe Current result is that.\n\nThe problem in the code above is that it\'s unclear which graph represents measurement 1 and which represents measurement 2.\n Second, I would like to see each component in a different color(or shape). \n\nThis graph aims to compare one component versus the two others, and to see the correlation between measurement 1 and 2.\n\nThanks for the help guys!\n'
'i have a list of points which are the inertia values of a kmeans algorithm.\nTo determine the optimum amount of clusters i need to find the point, where this curve starts to flatten.  \n\nData example\n\nHere is how my list of values is created and filled:\n\nsum_squared_dist = []\nK = range(1,50)\nfor k in K:\n    km = KMeans(n_clusters=k, random_state=0)\n    km = km.fit(normalized_modeling_data)\n    sum_squared_dist.append(km.inertia_)\n\nprint(sum_squared_dist)\n\n\nHow can i find a point, where the pitch of this curve increases (the curve is falling, so the first derivation is negative)?\n\nMy approach\n\nderivates = []\nfor i in range(len(sum_squared_dist)):\n    derivates.append(sum_squared_dist[i] - sum_squared_dist[i-1])\n\n\nI want to find the optimum number of clusters any given data using the elbow method. Could someone help me how i can find the point where the list of the inertia values starts to flatten?\n\nEdit\nDatapoints:\n\n[7342.1301373073857, 6881.7109460930769, 6531.1657905495022,  \n6356.2255554679778, 6209.8382535595829, 6094.9052166741121, \n5980.0191582610196, 5880.1869867848218, 5779.8957906367368, \n5691.1879324562778, 5617.5153566271356, 5532.2613232619951, \n5467.352265375117, 5395.4493783888756, 5345.3459908298091, \n5290.6769823693812, 5243.5271656371888, 5207.2501206569532, \n5164.9617535255456]\n\n\nGraph:\n\n'
'How to compare column names of 2 different Pandas data frame. I want to compare train and test data frames where there are some columns missing in test Data frames??\n'
'I am trying to predict the close price (1 or 0) based on the features present in the \'input_data\'. But when I try to run the code I am getting the below error, I am not sure how to fix this. Any help is much appreciated, thanks\n\nTraceback (most recent call last):\n  File "F:/Machine Learning/SK_Learn/SVM_Stock.py", line 71, in &lt;module&gt;\n    estimator.fit(x,y)\n  File "C:\\Python35\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py", line 210, in fit\n    return super(KerasClassifier, self).fit(x, y, **kwargs)\n  File "C:\\Python35\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py", line 139, in fit\n    **self.filter_sk_params(self.build_fn.__call__))\nTypeError: __call__() missing 1 required positional argument: \'inputs\'\n\n\nHere\'s the code:\n\nclass SVM_Stock:\n\n    def __init__(self):\n        pass\n\n    def create_model(self):\n\n        model = Sequential()\n        model.add(Dense(14, input_dim=16, kernel_initializer=\'normal\', activation=\'relu\'))\n        model.add(Dense(7, kernel_initializer=\'normal\', activation=\'relu\'))\n        model.add(Dense(1, kernel_initializer=\'normal\', activation=\'sigmoid\'))\n        model.compile(loss=\'binary_crossentropy\',optimizer=\'rmsprop\', metrics=[\'accuracy\'])\n        return model\n\n\nif __name__ == "__main__":\n\n    desired_width = 450\n    pd.set_option(\'display.width\', desired_width)\n    pd.set_option(\'display.max_columns\', 17)\n\n    ds = pd.read_csv(\'F:\\\\Machine Learning\\\\Linear Regression\\\\BIOCON-EQ.csv\')\n\n    ds = ds[[\'Date\',\'Open\',\'High\',\'Low\',\'Close\',\'Volume\',\'Slow VWMA\',\'Fast VWMA\']][14:].sort_values(\'Date\')\n\n    ds.loc[ds[\'Slow VWMA\'] &gt; ds[\'Fast VWMA\'], \'Trend UP\'] = 1\n    ds.loc[ds[\'Slow VWMA\'] &lt; ds[\'Fast VWMA\'], \'Trend UP\'] = 0\n    ds.loc[ds[\'Slow VWMA\'] == ds[\'Fast VWMA\'], \'Trend UP\'] = -1\n\n    ds.loc[ds[\'Slow VWMA\'] &lt; ds[\'Fast VWMA\'], \'Trend Down\'] = 1\n    ds.loc[ds[\'Slow VWMA\'] &gt; ds[\'Fast VWMA\'], \'Trend Down\'] = 0\n    ds.loc[ds[\'Slow VWMA\'] == ds[\'Fast VWMA\'], \'Trend Down\'] = -1\n\n    ds.loc[ds[\'Close\'] &gt; ds[\'Open\'], \'Close Price\'] = 1\n    ds.loc[ds[\'Close\'] &lt; ds[\'Open\'], \'Close Price\'] = 0\n    ds.loc[ds[\'Close\'] == ds[\'Open\'], \'Close Price\'] = -1\n\n    input_data = ds[[\'Date\',\'Open\',\'High\',\'Low\',\'Close\',\'Trend UP\', \'Trend \n    Down\']]\n    input_data.index = input_data.Date\n    input_data.drop(\'Date\', axis=1, inplace=True)\n    target = ds[[\'Close Price\']]\n\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    x = scaler.fit_transform(input_data)\n    y = target.values.ravel()\n\n    # clf = svm.SVC(gamma=0.1, C=100)\n    # clf.fit(x[:400], y[:400])\n    # print(clf.score(x[:400], y[:400]))\n    #\n    # for i in range(420, len(x)):\n    #     print("Prediction :", clf.predict(x[i].reshape(1, -1)))\n    #     print(i, y[i])\n\n    SS = SVM_Stock()\n    estimator = KerasClassifier(build_fn=SS.create_model(), nb_epoch=10, verbose=0)\n    estimator.fit(x,y)\n\n    \'\'\'Cross Validate\'\'\'\n    cv_scores = cross_val_score(estimator, x, y, cv=10)\n    print(cv_scores.mean())\n\n'
'Question\n\nIn this datafile, the United States is broken up into four regions using the "REGION" column. \n\nCreate a query that finds the counties that belong to regions 1 or 2, whose name starts with \'Washington\', and whose POPESTIMATE2015 was greater than their POPESTIMATE 2014.\n\nThis function should return a 5x2 DataFrame with the columns = [\'STNAME\', \'CTYNAME\'] and the same index ID as the census_df (sorted ascending by index).\n\nCODE\n\n    def answer_eight():\n    counties=census_df[census_df[\'SUMLEV\']==50]\n    regions = counties[(counties[counties[\'REGION\']==1]) | (counties[counties[\'REGION\']==2])]\n    washingtons = regions[regions[regions[\'COUNTY\']].str.startswith("Washington")]\n    grew = washingtons[washingtons[washingtons[\'POPESTIMATE2015\']]&gt;washingtons[washingtons[\'POPESTIMATES2014\']]]\n    return grew[grew[\'STNAME\'],grew[\'COUNTY\']]\n\noutcome = answer_eight()\nassert outcome.shape == (5,2)\nassert list (outcome.columns)== [\'STNAME\',\'CTYNAME\']\nprint(tabulate(outcome, headers=["index"]+list(outcome.columns),tablefmt="orgtbl"))\n\n\nERROR\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-77-546e58ae1c85&gt; in &lt;module&gt;()\n      6     return grew[grew[\'STNAME\'],grew[\'COUNTY\']]\n      7 \n----&gt; 8 outcome = answer_eight()\n      9 assert outcome.shape == (5,2)\n     10 assert list (outcome.columns)== [\'STNAME\',\'CTYNAME\']\n\n&lt;ipython-input-77-546e58ae1c85&gt; in answer_eight()\n      1 def answer_eight():\n      2     counties=census_df[census_df[\'SUMLEV\']==50]\n----&gt; 3     regions = counties[(counties[counties[\'REGION\']==1]) | (counties[counties[\'REGION\']==2])]\n      4     washingtons = regions[regions[regions[\'COUNTY\']].str.startswith("Washington")]\n      5     grew = washingtons[washingtons[washingtons[\'POPESTIMATE2015\']]&gt;washingtons[washingtons[\'POPESTIMATES2014\']]]\n\n/opt/conda/lib/python3.5/site-packages/pandas/core/frame.py in __getitem__(self, key)\n   1991             return self._getitem_array(key)\n   1992         elif isinstance(key, DataFrame):\n-&gt; 1993             return self._getitem_frame(key)\n   1994         elif is_mi_columns:\n   1995             return self._getitem_multilevel(key)\n\n/opt/conda/lib/python3.5/site-packages/pandas/core/frame.py in _getitem_frame(self, key)\n   2066     def _getitem_frame(self, key):\n   2067         if key.values.size and not com.is_bool_dtype(key.values):\n-&gt; 2068             raise ValueError(\'Must pass DataFrame with boolean values only\')\n   2069         return self.where(key)\n   2070 \n\nValueError: Must pass DataFrame with boolean values only\n\n\nI am clueless. Where am I going wrong?\n\nThanks\n'
"I need to have a MAPE function, however I was not able to find it in standard packages ... Below, my implementation of this function.\n\ndef mape(actual, predict): \n    tmp, n = 0.0, 0\n    for i in range(0, len(actual)):\n        if actual[i] &lt;&gt; 0:\n            tmp += math.fabs(actual[i]-predict[i])/actual[i]\n            n += 1\n    return (tmp/n)\n\n\nI don't like it, it's super not optimal in terms of speed. How to rewrite the code to be more Pythonic way and boost the speed?\n"
"I'm using Python and I need to split my .csv imported data in two parts, a training and test set, E.G 70% training and 30% test. \n\nI keep getting various errors, such as 'list' object is not callable and so on. \n\nIs there any easy way of doing this?\n\nThanks\n\nEDIT:\n\nThe code is basic, I'm just looking to split the dataset.\n\nfrom csv import reader\nwith open('C:/Dataset.csv', 'r') as f:\n    data = list(reader(f)) #Imports the CSV\n    data[0:1] ( data )\n\n\nTypeError: 'list' object is not callable\n"
"How can we change y axis to percent, instead of a fraction using Plotnine library in Python?\n\nA MWE of a barplot is as follows:\n\nfrom plotnine import *\nfrom plotnine.data import mpg\n\np = ggplot(mpg) + geom_bar(aes(x='manufacturer', fill='class'), position='fill')\nprint(p)\n\n\nWhich gives the following figure:\n\nStacked bar chart with y axis as fraction not percent\n\nWith ggplot2 in R it is simple, just need to add:\n\n+ scale_y_continuous(labels = scales::percent)\n\n\nHowever I have not been able to find how to do this in Plotnine. \n\nAny advise?\n"
"I need to create a function which would be the inverse of the np.gradient function.\nWhere the Vx,Vy arrays (Velocity component vectors) are the input and the output would be an array of anti-derivatives (Arrival Time) at the datapoints x,y.\nI have data on a (x,y) grid with scalar values (time) at each point.\nI have used the numpy gradient function and linear interpolation to determine the gradient vector Velocity (Vx,Vy) at each point (See below).\nI have achieved this by:\n #LinearTriInterpolator applied to a delaunay triangular mesh\n LTI= LinearTriInterpolator(masked_triang, time_array)\n\n #Gradient requested at the mesh nodes:\n (Vx, Vy) = LTI.gradient(triang.x, triang.y)\n\nThe first image below shows the velocity vectors at each point, and the point labels represent the time value which formed the derivatives (Vx,Vy)\n\nThe next image shows the resultant scalar value of the derivatives (Vx,Vy) plotted as a colored contour graph with associated node labels.\n\nSo my challenge is:\nI need to reverse the process!\nUsing the gradient vectors (Vx,Vy) or the resultant scalar value to determine the original Time-Value at that point.\nIs this possible?\nKnowing that the numpy.gradient function is computed using second order accurate central differences in the interior points and either first or second order accurate one-sides (forward or backwards) differences at the boundaries, I am sure there is a function which would reverse this process.\nI was thinking that taking a line derivative between the original point (t=0 at x1,y1) to any point (xi,yi) over the Vx,Vy plane would give me the sum of the velocity components. I could then divide this value by the distance between the two points to get the time taken..\nWould this approach work? And if so, which numpy integrate function would be best applied?\n\nAn example of my data can be found here [http://www.filedropper.com/calculatearrivaltimefromgradientvalues060820]\nYour help would be greatly appreciated\nEDIT:\nMaybe this simplified drawing might help understand where I'm trying to get to..\n\nEDIT:\nThanks to @Aguy who has contibuted to this code.. I Have tried to get a more accurate representation using a meshgrid of spacing 0.5 x 0.5m and calculating the gradient at each meshpoint, however I am not able to integrate it properly. I also have some edge affects which are affecting the results that I don't know how to correct.\nimport numpy as np\nfrom scipy import interpolate\nfrom matplotlib import pyplot\nfrom mpl_toolkits.mplot3d import Axes3D\n\n#Createmesh grid with a spacing of 0.5 x 0.5\nstepx = 0.5\nstepy = 0.5\nxx = np.arange(min(x), max(x), stepx)\nyy = np.arange(min(y), max(y), stepy)\nxgrid, ygrid = np.meshgrid(xx, yy)\ngrid_z1 = interpolate.griddata((x,y), Arrival_Time, (xgrid, ygrid), method='linear') #Interpolating the Time values\n\n#Formatdata\nX = np.ravel(xgrid)\nY= np.ravel(ygrid)\nzs = np.ravel(grid_z1)\nZ = zs.reshape(X.shape)\n\n#Calculate Gradient\n(dx,dy) = np.gradient(grid_z1) #Find gradient for points on meshgrid\n\nVelocity_dx= dx/stepx #velocity ms/m\nVelocity_dy= dy/stepx #velocity ms/m\n\nResultant = (Velocity_dx**2 + Velocity_dy**2)**0.5 #Resultant scalar value ms/m\n\nResultant = np.ravel(Resultant)\n\n#Plot Original Data F(X,Y) on the meshgrid\nfig = pyplot.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(x,y,Arrival_Time,color='r')\nax.plot_trisurf(X, Y, Z)\nax.set_xlabel('X-Coordinates')\nax.set_ylabel('Y-Coordinates')\nax.set_zlabel('Time (ms)')\npyplot.show()\n\n#Plot the Derivative of f'(X,Y) on the meshgrid\nfig = pyplot.figure()\nax = fig.add_subplot(projection='3d')\nax.scatter(X,Y,Resultant,color='r',s=0.2)\nax.plot_trisurf(X, Y, Resultant)\nax.set_xlabel('X-Coordinates')\nax.set_ylabel('Y-Coordinates')\nax.set_zlabel('Velocity (ms/m)')\npyplot.show()\n\n#Integrate to compare the original data input\ndxintegral = np.nancumsum(Velocity_dx, axis=1)*stepx\ndyintegral = np.nancumsum(Velocity_dy, axis=0)*stepy\n\nvalintegral = np.ma.zeros(dxintegral.shape)\nfor i in range(len(yy)):\n    for j in range(len(xx)):\n        valintegral[i, j] = np.ma.sum([dxintegral[0, len(xx) // 2], \n    dyintegral[i, len(yy)  // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])\nvalintegral = valintegral * np.isfinite(dxintegral)\n\n\nNow the np.gradient is applied at every meshnode (dx,dy) = np.gradient(grid_z1)\n\nNow in my process I would analyse the gradient values above and make some adjustments (There is some unsual edge effects that are being create which I need to rectify) and would then integrate the values to get back to a surface which would be very similar to f(x,y) shown above.\nI need some help adjusting the integration function:\n#Integrate to compare the original data input\ndxintegral = np.nancumsum(Velocity_dx, axis=1)*stepx\ndyintegral = np.nancumsum(Velocity_dy, axis=0)*stepy\n\nvalintegral = np.ma.zeros(dxintegral.shape)\nfor i in range(len(yy)):\n    for j in range(len(xx)):\n        valintegral[i, j] = np.ma.sum([dxintegral[0, len(xx) // 2], \n    dyintegral[i, len(yy)  // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])\nvalintegral = valintegral * np.isfinite(dxintegral)\n\nAnd now I need to calculate the new 'Time' values at the original (x,y) point locations.\nUPDATE (08-09-20) : I am getting some promising results using the help from @Aguy. The results can be seen below (with the blue contours representing the original data, and the red contours representing the integrated values).\nI am still working on an integration approach which can remove the inaccuarcies at the areas of min(y) and max(y)\n\nfrom matplotlib.tri import (Triangulation, UniformTriRefiner, \nCubicTriInterpolator,LinearTriInterpolator,TriInterpolator,TriAnalyzer)\nimport pandas as pd\nfrom scipy.interpolate import griddata\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import interpolate\n\n#-------------------------------------------------------------------------\n# STEP 1: Import data from Excel file, and set variables\n#-------------------------------------------------------------------------\ndf_initial = pd.read_excel(\nr'C:\\Users\\morga\\PycharmProjects\\venv\\Development\\Trial'\nr'.xlsx')\n\nInputdata can be found here link\ndf_initial = df_initial .sort_values(by='Delay', ascending=True) #Update dataframe and sort by Delay\nx = df_initial ['X'].to_numpy() \ny = df_initial ['Y'].to_numpy() \nArrival_Time = df_initial ['Delay'].to_numpy() \n\n# Createmesh grid with a spacing of 0.5 x 0.5\nstepx = 0.5\nstepy = 0.5\nxx = np.arange(min(x), max(x), stepx)\nyy = np.arange(min(y), max(y), stepy)\nxgrid, ygrid = np.meshgrid(xx, yy)\ngrid_z1 = interpolate.griddata((x, y), Arrival_Time, (xgrid, ygrid), method='linear')  # Interpolating the Time values\n\n# Calculate Gradient (velocity ms/m)\n(dy, dx) = np.gradient(grid_z1)  # Find gradient for points on meshgrid\n\n\nVelocity_dx = dx / stepx  # x velocity component ms/m\nVelocity_dy = dy / stepx  # y velocity component ms/m\n\n# Integrate to compare the original data input\ndxintegral = np.nancumsum(Velocity_dx, axis=1) * stepx\ndyintegral = np.nancumsum(Velocity_dy, axis=0) * stepy\n\nvalintegral = np.ma.zeros(dxintegral.shape)  # Makes an array filled with 0's the same shape as dx integral\nfor i in range(len(yy)):\n    for j in range(len(xx)):\n        valintegral[i, j] = np.ma.sum(\n        [dxintegral[0, len(xx) // 2], dyintegral[i, len(xx) // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])\nvalintegral[np.isnan(dx)] = np.nan\nmin_value = np.nanmin(valintegral)\n\nvalintegral = valintegral + (min_value * -1)\n\n##Plot Results\n\nfig = plt.figure()\nax = fig.add_subplot()\nax.scatter(x, y, color='black', s=7, zorder=3)\nax.set_xlabel('X-Coordinates')\nax.set_ylabel('Y-Coordinates')\nax.contour(xgrid, ygrid, valintegral, levels=50, colors='red', zorder=2)\nax.contour(xgrid, ygrid, grid_z1, levels=50, colors='blue', zorder=1)\nax.set_aspect('equal')\nplt.show()\n\n"
"Trying to use pyramid's auto arima function and getting nowhere. \n\nImporting the whole class:\n\nimport pyramid\n\n\n\nstepwise_fit = auto_arima(df.Weighted_Price, start_p=0, start_q=0, max_p=10, max_q=10, m=1,\n                      start_P=0, seasonal=True, trace=True,\n                      error_action='ignore',  # don't want to know if an order does not work\n                      suppress_warnings=True,  # don't want convergence warnings\n                      stepwise=True)  # set to stepwise\n\n\nI get error message:\n\nNameError: name 'auto_arima' is not defined\n\n\nFine, then let's import that specific package from pyramid.\n\nfrom pyramid.arima import auto_arima\n\n\n\n  --------------------------------------------------------------------------- RuntimeError                              Traceback (most recent call\n  last) RuntimeError: module compiled against API version 0xb but this\n  version of numpy is 0xa\n  \n  --------------------------------------------------------------------------- ImportError                               Traceback (most recent call\n  last)  in ()\n        1 #trying to import pyramid\n  ----> 2 from pyramid.arima import auto_arima\n  \n  /usr/local/lib/python2.7/site-packages/pyramid/arima/init.py in\n  ()\n        3 # Author: Taylor Smith \n        4 \n  ----> 5 from .approx import *\n        6 from .arima import *\n        7 from .auto import *\n  \n  /usr/local/lib/python2.7/site-packages/pyramid/arima/approx.py in\n  ()\n       16 # and since the platform might name the .so file something funky (like\n       17 # _arima.cpython-35m-darwin.so), import this absolutely and not relatively.\n  ---> 18 from pyramid.arima._arima import C_Approx\n       19 \n       20 all = [\n  \n  ImportError: numpy.core.multiarray failed to import\n\n\nAfter importing numpy, or even after just running the block again, I get this error message when running from pyramid.arima import auto_arima\n\n\n\n  --------------------------------------------------------------------------- ImportError                               Traceback (most recent call\n  last)  in ()\n        1 #trying to import pyramid\n  ----> 2 from pyramid import arima\n  \n  /usr/local/lib/python2.7/site-packages/pyramid/arima/init.py in\n  ()\n        3 # Author: Taylor Smith \n        4 \n  ----> 5 from .approx import *\n        6 from .arima import *\n        7 from .auto import *\n  \n  /usr/local/lib/python2.7/site-packages/pyramid/arima/approx.py in\n  ()\n       16 # and since the platform might name the .so file something funky (like\n       17 # _arima.cpython-35m-darwin.so), import this absolutely and not relatively.\n  ---> 18 from pyramid.arima._arima import C_Approx\n       19 \n       20 all = [\n  \n  ImportError: cannot import name C_Approx\n\n"
"I have a pandas dataframe and want to get rid of rows in which the column 'A' is negative. I know 2 ways to do this:\n\ndf = df[df['A'] &gt;= 0]\n\n\nor\n\nselRows = df[df['A'] &lt; 0].index\ndf = df.drop(selRows, axis=0)\n\n\nWhat is the recommended solution? Why?\n"
"I am currently performing multi class SVM with linear kernel using python's scikit library. \nThe sample training data and testing data are as given below:\n\nModel data:\n\nx = [[20,32,45,33,32,44,0],[23,32,45,12,32,66,11],[16,32,45,12,32,44,23],[120,2,55,62,82,14,81],[30,222,115,12,42,64,91],[220,12,55,222,82,14,181],[30,222,315,12,222,64,111]]\ny = [0,0,0,1,1,2,2]\n\n\nI want to plot the decision boundary and visualize the datasets. Can someone please help to plot this type of data.\n\nThe data given above is just mock data so feel free to change the values.\nIt would be helpful if at least if you could suggest the steps that are to followed. \nThanks in advance\n"
"I have this kind of data : \n\nID    x1   x2   x3    x4    x5    x6    x7   x8   x9   x10\n1   -0.18   5 -0.40 -0.26  0.53 -0.66  0.10   2 -0.20    1\n2   -0.58   5 -0.52 -1.66  0.65 -0.15  0.08   3  3.03   -2\n3   -0.62   5 -0.09 -0.38  0.65  0.22  0.44   4  1.49    1\n4   -0.22  -3  1.64 -1.38  0.08  0.42  1.24   5 -0.34    0\n5    0.00   5  1.76 -1.16  0.78  0.46  0.32   5 -0.51   -2\n\n\nwhat's the best method for visualizing this data, i'm using matplotlib to visualizing it, and read it from csv using pandas\n\nthanks\n"
'I\'m confused about using cross cross_val_predict in a test data set.\n\nI created a simple Random Forest model and used cross_val_predict to make predictions\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cross_validation import cross_val_predict, KFold\n\nlr = RandomForestClassifier(random_state=1, class_weight="balanced", n_estimators=25, max_depth=6)\nkf = KFold(train_df.shape[0], random_state=1)\npredictions = cross_val_predict(lr,train_df[features_columns], train_df["target"], cv=kf)\npredictions = pd.Series(predictions)\n\n\nI\'m confused on the next step here, How do I use is learnt above to make predictions on the test data set?\n'
'Working with census data, I want to replace NaNs in two columns ("workclass" and "native-country") with the respective modes of those two columns. I can get the modes easily:\n\nmode = df.filter(["workclass", "native-country"]).mode()\n\n\nwhich returns a dataframe:\n\n  workclass native-country\n0   Private  United-States\n\n\nHowever, \n\ndf.filter(["workclass", "native-country"]).fillna(mode)\n\n\ndoes not replace the NaNs in each column with anything, let alone the mode corresponding to that column. Is there a smooth way to do this?\n'
"I've got a dataframe with a multi index of Year and Month like the following\n\n     |     |Value\nYear |Month|  \n     |  1  |  3\n1992 |  2  |  5\n     |  3  |  8\n     | ... | ...\n1993 |  1  |  2\n     | ... | ...\n\n\nI'm trying to select the maximum Value for each year and put that in a DF like this: \n\n     | Max\nYear |  \n1992 |  5\n1993 |  2\n     | ... \n\n\nThere's not much info on multi-indexes, should I simply do a group by and apply or something similar to make it more simple?\n"
"I've searched the pandas documentation and cookbook recipes and it's clear you can round to the nearest decimal place easily using dataframe.columnName.round(decimalplace). \n\nHow do you do this with larger numbers?\n\nExample, I have a column of housing prices and I want them rounded to the nearest 10000 or 1000 or whatever. \n\ndf.SalesPrice.WhatDoIDo(1000)? \n\n"
"I'm plotting the missions ran by the USAF on North Korea during the Korean War.\n\nThe following is the map with 2800 plots.\n\n\n\nI have a total of about 7500 plots, but whenever I try to plot above 2800 a blank map renders. I'm rendering on a pc laptop. Would it render if I use a desktop? Or is this a limit with folium?\n\nI'm not speculating that it's an issue with the data. I'll share the coordinates data in case someone would like to explore it: link to public excel sheet.\n"
"Is it possible to convert a string vector into an indexed one using numpy ?\n\nSuppose I have an array of strings like ['ABC', 'DEF', 'GHI', 'DEF', 'ABC'] etc. I want it to be changed to an array of integers like [0,1,2,1,0]. Is it possible using numpy? I know that Pandas has a Series class that can do this, courtesy of this answer. Is there something similar for numpy as well?\n\nEdit : \n np.unique() returns unique value for all elements. What I'm trying to do is convert the labels in the Iris dataset to indices, such as 0 for Iris-setosa, 1 for Iris-versicolor and 2 for Iris-virginica respectively. Is there a way to do this using numpy?\n"
"Suppose I have dataframe df1 which includes two columns - A &amp; B. Value of A represents the lower range and value of B represents the upper range.\n\n  A     B\n10.5  20.5\n30.5  40.5\n50.5  60.5\n\n\nI've another dataframe which includes two columns - C &amp; D containing a different range of numbers.\n\n  C     D\n12.34  15.90\n13.68  19.13\n33.5   35.60\n35.12  38.76\n50.6   59.1\n\n\nNow I want to list all the pairs from df2 that fall under the groups (between the lower and upper range) in the df1. \n\nFinal output should be like this - \n\n     Key                Values\n(10.5, 20.5)  [(12.34, 15.90), (13.68, 19.13)]\n(30.5, 40.5)  [(33.5, 35.60), (35.12, 38.76)]\n(50.5, 60.5)  [(50.6, 59.1)]\n\n\nThe solution should be efficient as I have 5000 groups of range and 85000 numbers from different range.\n"
'I trained a model in keras (regression) on a Linux platform and saved the model with a model.save_weights("kwhFinal.h5")\n\nAnd then I was hoping to take my complete saved model to Python 3.6 on my Windows 10 laptop and just use it with IDLE:\n\nfrom keras.models import load_model\n\n# load weights into new model\nloaded_model.load_weights("kwhFinal.h5")\nprint("Loaded model from disk")\n\n\nExcept I am running into this read only mode ValueError with Keras. Thru pip I installed Keras &amp; Tensorflow on my Windows 10 laptop and researching more online it seems like this other SO post about the same issue, the answer states:\n\n\n  You have to set and define the architecture of your model and then use\n  model.load_weights\n\n\nBut I don\'t understand this enough to recreate the code from the answer (link to git gist). This is my Keras script below that I ran on a Linux OS to create the model. Can someone give me a tip on how define architecture so I can use this model to predict on my Windows 10 laptop?\n\n#https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/\n#https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n#https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras import backend\nfrom keras.models import model_from_json\nimport os\n\n\n\ndef rmse(y_true, y_pred):\n    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n\n# load dataset\ndataset = pd.read_csv("joinedRuntime2.csv", index_col=\'Date\', parse_dates=True)\n\nprint(dataset.shape)\nprint(dataset.dtypes)\nprint(dataset.columns)\n\n# shuffle dataset\ndf = dataset.sample(frac=1.0)\n\n# split into input (X) and output (Y) variables\nX = df.drop([\'kWh\'],1)\nY = df[\'kWh\']\n\noffset = int(X.shape[0] * 0.7)\nX_train, Y_train = X[:offset], Y[:offset]\nX_test, Y_test = X[offset:], Y[offset:]\n\n\nmodel = Sequential()\nmodel.add(Dense(60, input_dim=7, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(55, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(50, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(45, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(30, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(20, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(1, kernel_initializer=\'normal\'))\nmodel.summary()\n\nmodel.compile(loss=\'mse\', optimizer=\'adam\', metrics=[rmse])\n\n# train model\nhistory = model.fit(X_train, Y_train, epochs=5, batch_size=1,  verbose=2)\n\n# plot metrics\nplt.plot(history.history[\'rmse\'])\nplt.title("kWh RSME Vs Epoch")\nplt.show()\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open("model.json", "w") as json_file:\n    json_file.write(model_json)\n\nmodel.save_weights("kwhFinal.h5")\nprint("[INFO] Saved model to disk")\n\n\nOn machine learning mastery they demonstrate also saving YML &amp; Json but I am unsure if this will help to define model architecture...\n'
'I\'m loving Altair for creating choropleth maps! My biggest problem, however, is I cannot figure out how to change the size of the legend. I\'ve read through the documentation and tried several things to no avail. \n\nHere\'s an example using the unemployment map by county from Altair\'s docs. I added a \'config\' layer to change the font size for the title on both the map and the legend. Note the .configure_legend() part of the code within "config". \n\ncounties = alt.topo_feature(data.us_10m.url, \'counties\')\nsource = data.unemployment.url\n\nforeground = alt.Chart(counties).mark_geoshape(\n    ).encode(\n    color=alt.Color(\'rate:Q\', sort="descending",  scale=alt.Scale(scheme=\'plasma\'), legend=alt.Legend(title="Unemp Rate", tickCount=6))\n).transform_lookup(\n    lookup=\'id\',\n    from_=alt.LookupData(source, \'id\', [\'rate\'])\n).project(\n    type=\'albersUsa\'\n).properties(\n    title="Unemployment Rate by County",\n    width=500,\n    height=300\n)\n\nconfig = alt.layer(foreground).configure_title(fontSize=20, anchor="middle").configure_legend(titleColor=\'black\', titleFontSize=14) \n\nconfig\n\n\nHere\'s what the image should look like:\n\n\n\nIf I change the size of the map like this:\n\ncounties = alt.topo_feature(data.us_10m.url, \'counties\')\nsource = data.unemployment.url\n\nforeground = alt.Chart(counties).mark_geoshape(\n    ).encode(\n    color=alt.Color(\'rate:Q\', sort="descending",  scale=alt.Scale(scheme=\'plasma\'), legend=alt.Legend(title="Unemp Rate", tickCount=6))\n).transform_lookup(\n    lookup=\'id\',\n    from_=alt.LookupData(source, \'id\', [\'rate\'])\n).project(\n    type=\'albersUsa\'\n).properties(\n    title="Unemployment Rate by County",\n    width=900,\n    height=540\n)\n\nconfig = alt.layer(foreground).configure_title(fontSize=20, anchor="middle").configure_legend(titleColor=\'black\', titleFontSize=14) \n\nconfig\n\n\nThe legend stays the same size, so that it now looks tiny in comparison to the map:\n\n\n\nAlternatively, if I make the map size tiny, the legend will be huge!\n\n\n\nI\'ve tried about a dozen different things to no avail. \n\nAnyone have a solution to this? \n'
'I have the following table. Some values are NaNs. Let\'s assume that columns are highly correlated. Taking row 0 and row 5 I say that value in col2 will be 4.0. Same situation for row 1 and row 4. But in case of row 6, there is no perfectly matching sample so I should take most similar row - in this case, row 0 and change NaN to 3.0.\nHow should I approach it? Is there any pandas function that can do this?\n\nexample = pd.DataFrame({"col1": [3, 2, 8, 4, 2, 3, np.nan], \n                        "col2": [4, 3, 6, np.nan, 3, np.nan, 5], \n                        "col3": [7, 8, 9, np.nan, np.nan, 7, 7], \n                        "col4": [7, 8, 9, np.nan, np.nan, 7, 6]})\n\n\nOutput:\n\n    col1    col2    col3    col4\n0   3.0     4.0     7.0     7.0\n1   2.0     3.0     8.0     8.0\n2   8.0     6.0     9.0     9.0\n3   4.0     NaN     NaN     NaN\n4   2.0     3.0     NaN     NaN\n5   3.0     NaN     7.0     7.0\n6   NaN     5.0     7.0     6.0\n\n'
'My general problem is that I have a dataframe where columns correspond to feature values. There is also a date column in the dataframe. Each feature column may have missing NaN values. I want to fill a column with some fill logic such as "fill_mean" or "fill zero". \n\nBut I do not want to just apply the fill logic to the whole column because if one of the earlier values is a NaN, I do not want the average i fill for this specific NaN to be tainted by what the average was later on, when the model should have no knowledge about. Essentially it\'s the common problem of not leaking information about the future to your model - specifically when trying to fill my time series.\n\nAnyway, I have simplified my problem to a few lines of code. This is my simplified attempt at the above general problem:\n\n#assume ts_values is a time series where the first value in the list is the oldest value and the last value in the list is the most recent.\nts_values = [17.0, np.NaN, 12.0, np.NaN, 18.0]\nnan_inds = np.argwhere(np.isnan(ts_values))\nfor nan_ind in nan_inds:\n    nan_ind_value = nan_ind[0]\n    ts_values[nan_ind_value] = np.mean(ts_values[0:nan_ind_value])\n\n\nThe output of the above script is:\n\n[17.0, 17.0, 12.0, 15.333333333333334, 18.0]\n\n\nwhich is exactly what I would expect. \n\nMy only issue with this is that it will be linear time with respect to the number of NaNs in the data set. Is there a way to do this in constant or log time where I don\'t iterate through the nan index values. \n'
"The below code works perfectly okay. If I try to change all the 64s to 128s then I get an error about shape. Do I need to change the input data shape if I change the number of layers in an artificial neural network when using Keras? I didn't think so because it asks for input_dim which is correct. \n\nWorks: \n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd3 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd3)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n\n\nDoesn't Work:\n\nmodel = Sequential()\nmodel.add(Dense(128, input_dim=14, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd3 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd3)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n\n"
"I looked around online but couldn't find anything, but I may well have missed a piece of literature on this.  I am running a basic neural net on a 289 component vector to produce a 285 component vector.  In my input, the last 4 pieces of data are critical to change the rest of the input into the resultant 285 for the output.  That is to say, the input is 285 + 4, such that the 4 morph the rest of the input into the output.\n\nBut when running a neural network on this, I am not sure how to reflect this.  Would I need to use convolution on the rest of the input? I want my system to emphasize the 4 data points that critically affect the other 285.  I am still new to all of this, so a few pointers would be great!\n\nAgain, if there is something already written on this, then that would be awesome too.\n"
"I'm trying to set a new column (two columns in fact) in a pandas dataframe, with the data comes from other dataframe.\n\nI have the following two dataframes (they are example for this purpose, the original dataframes are so much bigger):\n\nIn [116]: df0\nOut[116]:     \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n\n\nIn [118]: df1\nOut[118]: \n   A  D  E\n0  2  7  2\n1  6  5  5\n2  4  3  2\n3  0  1  0\n4  5  4  6\n5  0  1  0\n\n\nAnd I want to have a new dataframe (or added to df0, whatever), as:\n\ndf2: \n   A  B  C  D  E\n0  0  1  0  1  0\n1  2  3  2  7  2\n2  4  5  4  3  2\n3  5  5  5  4  6\n\n\nAs you can see, in the resulting dataframe isn't present the row with A=6 which is present in df1 but not in df0. Also the row with A=0 is duplicated in df1, but not in the result df2.\n\nActually, I'm having trouble with the selection method. I can do this:\n\ndf1.loc[df1['A'].isin(df0['A'])]\n\n\nBut I'm not sure how to apply the part of keep with unique data (remember that df1 can contain duplicated data) and add the two columns to the df2 dataset (or add them to df0).\nI've search here and I don't know see how to apply something like groupby, or even map.\n\nAny idea?\n\nThanks!\n"
"I've create a pipeline as follows (using the Keras Scikit-Learn API)\n\nestimators = []\nestimators.append(('standardize', StandardScaler()))\nestimators.append(('mlp', KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=5, verbose=0)))\npipeline = Pipeline(estimators)\n\n\nand fit it with\n\npipeline.fit(trainX,trainY)\n\n\nIf I predict with pipline.predict(testX), I (believe) I get standardised predictions.\n\nHow do I predict on testX so that predictedY it at the same scale as the actual (untouched) testY (i.e. NOT standardised prediction, but instead the actual values)? I see there is an inverse_transform method for Pipeline, however appears to be for only reverting a transformed X.\n"
'I am doing a Kaggle tutorial for Titanic using the Datacamp platform.\n\nI understand the use of .loc within Pandas - to select values by row using column labels... \n\nMy confusion comes from the fact that in the Datacamp tutorial, we want to locate all the "Male" inputs within the "Sex" column, and replace it with the value of 0. They use the following piece of code to do it:\n\ntitanic.loc[titanic["Sex"] == "male", "Sex"] = 0\n\n\nCan someone please explain how this works? I thought .loc took inputs of row and column, so what is the == for?\n\nShouldn\'t it be:\n\ntitanic.loc["male", "Sex"] = 0\n\n\nThanks!\n'
'import pandas as pd\nimport os\nimport glob\n\n\nall_data = pd.DataFrame()\nfor f in glob.glob("output/test*.xlsx")\n    df = pd.read_excel(f)\n    all_data = all_data.append(df, ignore_index=True)\n\n\nI want to put multiple xlsx files into one xlsx. the excel files are in the output/test folder. The columns are the same, in all but I want concat the rows. the above code doesn\'t seem to work\n'
"I have a pandas dataframe as follows:\n\nuser_id product_id order_number\n1       1          1\n1       1          2\n1       1          3\n1       2          1\n1       2          5\n2       1          1\n2       1          3\n2       1          4\n2       1          5\n3       1          1\n3       1          2\n3       1          6\n\n\nI wanted to query this df for the longest streak (none order_number is skipped) and last streak (since last order_number). \n\nThe ideal result is as follows:\n\nuser_id product_id longest_streak last_streak\n1       1          3              3\n1       2          0              0\n2       1          3              3\n3       1          2              0\n\n\nI'd appreciate any insights on this.\n"
"I came across the below line of code, which gives an error when '.index' is not present in it.\n\nprint(df.drop(df[df['Quantity'] == 0].index).rename(columns={'Weight': 'Weight (oz.)'}))\n\n\nWhat is the purpose of '.index' while using drop in pandas?\n"
'So I currently have a dataframe that looks like:\n\n\n\nAnd I want to add a completely new column called "Predictors" with only one cell that contains an array.  \n\nSo [0, \'Predictors\'] should contain an array and everything below that cell in the same column should be empty.\n\nHere\'s my attempt, I tried to create a separate dataframe that just contained the "Predictors" column, and tried appending it to the current dataframe, but I get: \'Length mismatch: Expected axis has 3 elements, new values have 4 elements.\'\n\nHow do I append a single cell containing an array to my dataframe?\n\n# create a list and dataframe to hold the names of predictors\ndataframe=dataframe.drop([\'price\',\'Date\'],axis=1)  \npredictorsList = dataframe.columns.get_values().tolist()\npredictorsList = np.array(predictorsList, dtype=object)\n\n# Combine actual and forecasted lists to one dataframe\ncombinedResults = pd.DataFrame({\'Actual\': actual, \'Forecasted\': forecasted})\n\npredictorsDF = pd.DataFrame({\'Predictors\': [predictorsList]})\n\n# Add Predictors to dataframe\n#combinedResults.at[0, \'Predictors\'] = predictorsList\npd.concat([combinedResults,predictorsDF], ignore_index=True, axis=1)\n\n'
"I just started with GridSearchCV in Python, but I am confused what is scoring in this. Somewhere I have seen \n\nscorers = {\n    'precision_score': make_scorer(precision_score),\n    'recall_score': make_scorer(recall_score),\n    'accuracy_score': make_scorer(accuracy_score)\n}\n\ngrid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n                       cv=skf, return_train_score=True, n_jobs=-1)\n\n\nWhat is the intent of using these values, i.e. precision, recall, accuracy in scoring? \n\nIs this used by gridsearch in giving us the optimized parameters based on these scoring values.... like for the best precision score it finds the best parameters or something like that? \n\nIt calculate precision, recall, accuracy for the possible parameters and gives the result, now the question is if this is true, then it select best parameters based on precision, recall or accuracy? Is the above statement true?\n"
'What is the difference between Kernel Ridge (from sklearn.kernel_ridge) with polynomial kernel and using PolynomialFeatures + Ridge (from sklearn.linear_model)? \n'
"I'm using google Colab notebook for a project that requires me to plot GPS coordinates on a map. I want to use basemap for this purpose. I tried to import it on the Colab notebook by using\nfrom mpl_tools.basemap import Basemap\nand it showed up the following error: \n\nModuleNotFoundError                       Traceback (most recent call last)\n&lt;ipython-input-24-2cb85a2f9bb7&gt; in &lt;module&gt;()\n----&gt; 1 from mpl_tools.basemap import Basemap\n\nModuleNotFoundError: No module named 'mpl_tools'\n\n\nI need to install the basemap module in order to use it. I tried !pip install basemap and tried to run it on Colab and that did not work. \n"
"Problem\n\nHow can I dump a pickle object with its own dependencies?\n\nThe pickle object is generally generated from a notebook.\n\nI tried creating virtualenv for the notebook to track dependencies, however this way I don't get only the imports of the pickle object but many more that's used in other places of the application, which is fine enough but not the best solution.\n\nBackground\n\nWhat I'm trying to achieve\n\nI'm trying to build a MLOps flow. Quick explanation: MLOps is a buzzword that's synonymous with DevOps for machine learning. There are different PaaS/SaaS solutions for it offered by  different companies and they commonly solve following problems:\n\n\nAutomation of creating web API's from models\nHandling requirements/dependencies\nStoring &amp; running scripts used for model generation, model binary and data sets.\n\n\nI'll skip the storage part and focus on the first two.\n\nHow I'm trying to achieve\n\nIn my case I'm trying to set up this flow using good old TeamCity where models are pickle objects generated by sk-learn. The requirements are:\n\n\nThe dependencies must be explicitly defined\nOther pickle objects (rather than sk-learn) must be supported.\nThe workflow for a data scientists will look like:\n\n\nData scientist uploads the pickle model with requirements.txt.\nData scientist commits a definition file which look like this:\n\n\n apiPort: 8080\n apiName: name-tagger\n model: model-repository.internal/model.pickle\n requirements: model-repository.internal/model.requirements\n predicterVersion: 1.0\n\n\n\nwhere predicter is a FLASK application with own requirements.txt. It's an API wrapper/layer of a pickle model that loads the model in the memory and serves predictions from a rest endpoint.\n\n\n\nThen a build configuration in TeamCity parses the file and executes the following:\n\n\nParse the definition file.\nFind the predicter code\nCopy the pickle model as model.pickle in predicter applications root folder\nMerge requirements.txt of predicter with requirements.txt of pickle model\nCreate virtualenv, install dependencies, push it as wheel\n\n\nAs output of the flow I have a package including a REST API that consumes a pickle model and exposes to the defined port.\n"
"I was wondering if it is possible to groupby one column while counting the values of another column that fulfill a condition. Because my dataset is a bit weird, I created a similar one:\n\nimport pandas as pd\n\nraw_data = {'name': ['John', 'Paul', 'George', 'Emily', 'Jamie'], \n            'nationality': ['USA', 'USA', 'France', 'France', 'UK'],     \n            'books': [0, 15, 0, 14, 40]}  \ndf = pd.DataFrame(raw_data, columns = ['name', 'nationality', 'books'])\n\n\nSay, I want to groupby the nationality and count the number of people that don't have any books (books == 0) from that country.\n\nI would therefore expect something like the following as output:\n\nnationality\nUSA      1\nFrance   1\nUK       0\n\n\nI tried most variations of groupby, using filter, agg but don't seem to get anything that works.\n\nThanks in advance,\nBBQuercus :)\n"
'My JSON (~500mb) file has multiple JSON objetcs, actually i just need to use the "customer_id" colunm. When i execute the code below, it gives memory error.\n\nwith open(\'online_pageviews.json\') as f:\n     online_pageviews = pd.DataFrame(json.loads(line) for line in f)\n\n\nHere is a example of an JSON object in "online_pageviews.json"\n\n{\n"date": "2018-08-01",\n"visitor_id": "3832636531373538373137373",\n"deviceType": "mobile",\n"pageType": "product",\n"category_id": "6365313034",\n"on_product_id": "323239323839626",\n"customer_id": "33343163316564313264"\n}\n\n\nIs there a way to only use the "customer_id" column?\nWhat can i do to load this file?\n'
"I am trying to filter data from a dataframe which are less than a certain value. If there is no NaN then its working fine. But when there is a nan then it is ignoring the NaN value. I want to include all the time its doesn't matter its less than or bigger than the comparing value. \n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(\n    {\n        'index': [1, 2, 3,  4,  5,  6,   7,  8, 9],\n        'value': [5, 6, 7, np.nan, 9, 3, 11, 34, 78]\n    }\n)\n\ndf_chunked = df[(df['index'] &gt;= 1) &amp; (df['index'] &lt;= 5)]\n\nprint('df_chunked')\nprint(df_chunked)\n\ndf_result = df_chunked[(df_chunked['value'] &lt; 10)]\n# df_result = df_chunked[(df_chunked['value'] &lt; 10) | (df_chunked['value'] == np.isnan(df_chunked['value']))]\n\nprint('df_result')\nprint(df_result)\n\n\n\n\nIn the above result 5,6,7,9 is showing. but i want also the nan there. I tried with \n\ndf_result = df_chunked[(df_chunked['value'] &lt; 10) | (df_chunked['value'] == np.isnan(df_chunked['value']))]\n\n\nBut it is not working. \n\nHow can I do this?\n"
"I am training a tensorflow keras sequential model on around 20+ GB text based categorical data in a postgres db and i need to give class weights to the model.\nHere is what i am doing.\n\nclass_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes, y)\n\nmodel.fit(x, y, epochs=100, batch_size=32, class_weight=class_weights, validation_split=0.2, callbacks=[early_stopping])\n\n\nSince i can't load the whole thing in memory i figured i can use fit_generator method in keras model. \n\nHowever how can i calculate the class weights on this data? sklearn does not provide any special function for this, is it the right tool for this ?\n\nI thought of doing it on multiple random samples but is there a better approach where whole data can be used ?\n"
'How do these function works? I am using Python3.7 and OpenCv 4.2.0. Thanks in Advance.\n\napprox = cv2.approxPolyDP(cnt, 0.01*cv2.arcLength(cnt, True), True)\n\n'
"Most of the libraries like requests or matplotlib don't include proper documentation of kwargs/args. There are sometimes examples but mostly the specific use case is missing.\nMy Questions:\n\nWhere can I find that sorta information.\nWhy developers don't document the kwargs/args properly\n\n"
"I read a csv file, using DictReader.\n\nI have a list of dictionaries:\n\neg:\n\na = [{'Name':'A','Class':'1'},{'Name':'B','Class':'1'},{'Name':'C','Class':'2'}]\n\n\nI want to count the number of entries in the list that have 'Class' == 1.\n\nIs it possible to do it without a loop?\n\nEDIT:\n\nI have tried the following:\n\ncount = 0\nfor k in a:\n    if k['Class'] == '1':\n        count += 1\nprint(count)\n\n"
'I have multiple files with bits to analyse. First i read them into a list of BitString.Bits. Then i split each file bits into the specific parts i want to see and save them into a list of Pandas.DataFrames. One DF for each file.\n\nNow for further plotting and analysis purposes i want to store all data in one Xarray.Dataset, where i have the DataFrames stacked along the third axis with the name "dataset".\n\nI have tried to concat each DataFrame together to an DataSet:\n\nxr.concat(data_df[:], dim="dataset")\n\n\nbut i got an error saying that i cant concatenate other than DataArray or DataSets. Can i convert the DataFrames on the fly to DataArrays?\n\nThanks for your help!\n\nGreetings from Germany\n\nJan\n'
'New to pandas development. How do I forward fill a DataFrame with the value contained in one previously seen column?\n\nSelf-contained example:\n\nimport pandas as pd\nimport numpy as np\nO = [1, np.nan, 5, np.nan]\nH = [5, np.nan, 5, np.nan]\nL = [1, np.nan, 2, np.nan]\nC = [5, np.nan, 2, np.nan]\ntimestamps = ["2017-07-23 03:13:00", "2017-07-23 03:14:00", "2017-07-23 03:15:00", "2017-07-23 03:16:00"]\ndict = {\'Open\': O, \'High\': H, \'Low\': L, \'Close\': C}\ndf = pd.DataFrame(index=timestamps, data=dict)\nohlc = df[[\'Open\', \'High\', \'Low\', \'Close\']]\n\n\nThis yields the following DataFrame:\n\nprint(ohlc)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   NaN   NaN  NaN    NaN\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   NaN   NaN  NaN    NaN\n\n\nI want to go from that last DataFrame to something like this:\n\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n\n\nSo that the previously-seen value in \'Close\' forward fills entire rows until there\'s a new populated row seen. It\'s simple enough to fill column \'Close\' like so: \n\ncolumn2fill = \'Close\'\nohlc[column2fill] = ohlc[column2fill].ffill()\nprint(ohlc)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   NaN   NaN  NaN    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   NaN   NaN  NaN    2.0\n\n\nBut is there a way to fill across the  03:14:00 and 03:16:00 rows with the \'Close\' value of those rows? And is there a way to do it in one step using one forward fill instead of filling the \'Close\' column first?\n'
'Im new to Data Science and Analysis. \nAfter going through a lot of kernels on Kaggle, I made a model that predicts the price of a property. Ive tested this model using my training data, but now I want to run it on my test data. Ive got a test.csv file and I want to use it. How do I do that?\nWhat i previously did with my training dataset:\n\n#loading my train dataset into python\ntrain = pd.read_csv(\'/Users/sohaib/Downloads/test.csv\')\n\n#factors that will predict the price\ntrain_pr = [\'OverallQual\',\'GrLivArea\',\'GarageCars\',\'TotalBsmtSF\',\'FullBath\',\'YearBuilt\']\n\n#set my model to DecisionTree\nmodel = DecisionTreeRegressor()\n\n#set prediction data to factors that will predict, and set target to SalePrice\nprdata = train[train_pr]\ntarget = train.SalePrice\n\n#fitting model with prediction data and telling it my target\nmodel.fit(prdata, target)\n\nmodel.predict(prdata.head())\n\n\nNow what I tried to do is, copy the whole code, and change the "train" with "test", and "predate" with "testprdata", and I thought it will work, but sadly no. I know I\'m doing something wrong with this, idk what it is.\n'
'I\'m sorry if the title of the question is not that clear, I could not sum up the problem in one line. \n\nHere are the simplified datasets for an explanation. Basically, the number of categories in the training set is much larger than the categories in the test set, because of which there is a difference in the number of columns in the test and training set after OneHotEncoding. How can I handle this problem?\n\nTraining Set\n\n+-------+----------+\n| Value | Category |\n+-------+----------+\n| 100   | SE1      |\n+-------+----------+\n| 200   | SE2      |\n+-------+----------+\n| 300   | SE3      |\n+-------+----------+\n\n\nTraining set after OneHotEncoding\n\n+-------+-----------+-----------+-----------+\n| Value | DummyCat1 | DummyCat2 | DummyCat3 |\n+-------+-----------+-----------+-----------+\n| 100   | 1         | 0         | 0         |\n+-------+-----------+-----------+-----------+\n| 200   | 0         | 1         | 0         |\n+-------+-----------+-----------+-----------+\n| 300   | 0         | 0         | 1         |\n+-------+-----------+-----------+-----------+\n\n\nTest Set\n\n+-------+----------+\n| Value | Category |\n+-------+----------+\n| 100   | SE1      |\n+-------+----------+\n| 200   | SE1      |\n+-------+----------+\n| 300   | SE2      |\n+-------+----------+\n\n\nTest set after OneHotEncoding\n\n+-------+-----------+-----------+\n| Value | DummyCat1 | DummyCat2 |\n+-------+-----------+-----------+\n| 100   | 1         | 0         |\n+-------+-----------+-----------+\n| 200   | 1         | 0         |\n+-------+-----------+-----------+\n| 300   | 0         | 1         |\n+-------+-----------+-----------+\n\n\nAs you can notice, the training set after the OneHotEncoding is of shape (3,4) while the test set after OneHotEncoding is of shape (3,3).\nBecause of this, when I do the following code (y_train is a column vector of shape (3,))\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train, y_train)\n\nx_pred = regressor.predict(x_test)\n\n\nI get the error at the predict function. As you can see, the dimensions in the error are quite large, unlike the basic examples.\n\n  Traceback (most recent call last):\n\n  File "&lt;ipython-input-2-5bac76b24742&gt;", line 30, in &lt;module&gt;\n    x_pred = regressor.predict(x_test)\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py", line 256, in predict\n    return self._decision_function(X)\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py", line 241, in _decision_function\n    dense_output=True) + self.intercept_\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py", line 140, in safe_sparse_dot\n    return np.dot(a, b)\n\nValueError: shapes (4801,2236) and (4033,) not aligned: 2236 (dim 1) != 4033 (dim 0)\n\n'
'I\'m trying to calculate the Davies-Bouldin Index in Python. \n\nHere are the steps the code below tries to reproduce.\n\n5 Steps: \n\n\nFor each cluster, compute euclidean distances between each point to the centroid\nFor each cluster, compute the average of these distances\nFor each pair of clusters, compute the euclidean distance between their centroids\n\n\nThen,\n\n\nFor each pair of clusters, make the sum of the average distances to their respective centroid (computed at step 2) and divide it by the distance separating them (computed at step 3).\n\n\nFinally,\n\n\nCompute the mean of all these divisions (= all indexes) to get the Davies-Bouldin index of the whole clustering\n\n\nCode\n\ndef daviesbouldin(X, labels, centroids):\n\n    import numpy as np\n    from scipy.spatial.distance import pdist, euclidean\n\n    nbre_of_clusters = len(centroids) #Get the number of clusters\n    distances = [[] for e in range(nbre_of_clusters)] #Store intra-cluster distances by cluster\n    distances_means = [] #Store the mean of these distances\n    DB_indexes = [] #Store Davies_Boulin index of each pair of cluster\n    second_cluster_idx = [] #Store index of the second cluster of each pair\n    first_cluster_idx = 0 #Set index of first cluster of each pair to 0\n\n    # Step 1: Compute euclidean distances between each point of a cluster to their centroid\n    for cluster in range(nbre_of_clusters):\n        for point in range(X[labels == cluster].shape[0]):\n            distances[cluster].append(euclidean(X[labels == cluster][point], centroids[cluster]))\n\n    # Step 2: Compute the mean of these distances\n    for e in distances:\n        distances_means.append(np.mean(e))\n\n    # Step 3: Compute euclidean distances between each pair of centroid\n    ctrds_distance = pdist(centroids) \n\n    # Tricky step 4: Compute Davies-Bouldin index of each pair of cluster   \n    for i, e in enumerate(e for start in range(1, nbre_of_clusters) for e in range(start, nbre_of_clusters)):\n        second_cluster_idx.append(e)\n        if second_cluster_idx[i-1] == nbre_of_clusters - 1:\n            first_cluster_idx += 1\n        DB_indexes.append((distances_means[first_cluster_idx] + distances_means[e]) / ctrds_distance[i])\n\n    # Step 5: Compute the mean of all DB_indexes   \n    print("DAVIES-BOULDIN Index: %.5f" % np.mean(DB_indexes)) \n\n\nIn arguments:\n\n\nX is the data\nlabels, are the labels computed by a clustering algorithm (i.e: kmeans)\ncentroids are the coordinates of each cluster\'s centroid (i.e: cluster_centers_)\n\n\nAlso, note that I\'m using Python 3\n\nQUESTION1: Is the computation of euclidean distances between each pair of centroid correct (step 3)?\n\nQUESTION2: Is my implementation of step 4 correct?\n\nQUESTION3: Do I need to normalise intra and inter cluster distances ?\n\n\n\nFurther explanations on Step 4\n\nLet\'s say we have 10 clusters.\nThe loop should compute the DB index of each pair of cluster. \n\nAt the first iteration:\n\n\nsums intra-distances mean of cluster 1 (index 0 of distances_means) and intra-distances mean of cluster 2 (index 1 of distances_means)\ndivides this sum by the distance between the 2 clusters (index 0 of ctrds_distance)\n\n\nAt the second iteration:\n\n\nsums intra-distances mean of cluster 1 (index 0 of distances_means) and intra-distances mean of cluster 3 (index 2 of distances_means)\ndivides this sum by the distance between the 2 clusters (index 1 of ctrds_distance)\n\n\nand so on...\n\nWith the example of 10 clusters, the full iteration process should look like this:\n\nintra-cluster distance intra-cluster distance       distance between their\n      of cluster:             of cluster:           centroids(storage num):\n         0           +             1            /             0\n         0           +             2            /             1\n         0           +             3            /             2\n         0           +             4            /             3\n         0           +             5            /             4\n         0           +             6            /             5\n         0           +             7            /             6\n         0           +             8            /             7\n         0           +             9            /             8\n         1           +             2            /             9\n         1           +             3            /             10\n         1           +             4            /             11\n         1           +             5            /             12\n         1           +             6            /             13\n         1           +             7            /             14\n         1           +             8            /             15\n         1           +             9            /             16\n         2           +             3            /             17\n         2           +             4            /             18\n         2           +             5            /             19\n         2           +             6            /             20\n         2           +             7            /             21\n         2           +             8            /             22\n         2           +             9            /             23\n         3           +             4            /             24\n         3           +             5            /             25\n         3           +             6            /             26\n         3           +             7            /             27\n         3           +             8            /             28\n         3           +             9            /             29\n         4           +             5            /             30\n         4           +             6            /             31\n         4           +             7            /             32\n         4           +             8            /             33\n         4           +             9            /             34\n         5           +             6            /             35\n         5           +             7            /             36\n         5           +             8            /             37\n         5           +             9            /             38\n         6           +             7            /             39\n         6           +             8            /             40\n         6           +             9            /             41\n         7           +             8            /             42\n         7           +             9            /             43\n         8           +             9            /             44\n\n\nThe problem here is I\'m not quite sure that the index of distances_means matches the index of ctrds_distance. \n\nIn other words, I\'m not sure that the first inter-cluster distance computed corresponds to the distance between cluster 1 and cluster 2. And that the second inter-cluster distance computed corresponds to the distance between cluster 3 and cluster 1... and so on, following the pattern above.\n\nIn short: I\'m afraid I\'m dividing pairs of intra-cluster distances by an inter-cluster distance that is not corresponding.\n\nAny help is more than welcomed !\n'
'I have a dataframe which looks like this.\n\nid        YearReleased      Artist     count\n168             2015             Muse      1\n169             2015          Rihanna      3\n170             2015     Taylor Swift      2\n171             2016   Jennifer Lopez      1\n172             2016          Rihanna      3\n173             2016       Underworld      1\n174             2017         Coldplay      1\n175             2017       Ed Sheeran      2\n\nI want to get the maximum count for each year and then get the corresponding Artist name.\n\nSomething like this:\n\nYearReleased  Artist\n\n2015          Rihanna\n2016          Rihanna\n2017       Ed Sheeran\n\nI have tried using a loop to iterate over the rows of the dataframe and create another dictionary with key as year and value as artist. But when I try to convert that dictionary to a dataframe, the keys are mapped to columns instead of rows.\n\nCan somebody guide me to have a better approach to this without having to loop over the dataframe and instead use some inbuilt pandas method to achieve this?\n'
"I'm trying to create a 3-D Plot from a Dictionary and have hit a sticking point. The Z-access will be the key (MSE) and the X and Y axis will be the first and second values of the tuple e.g. X will be 2 and Y will be 5 in the first example of the example dataset below: \n\n80178.37739073468: (2, 5),\n81623.18006660603: (13, 14),\n82583.3021359235: (8, 16),\n83491.34222215343: (9, 8),\n83724.73005402873: (8, 14),\n83856.2891246289: (7, 8),\n83984.92825308126: (6, 5),\n84314.30519882984: (13, 16),\n84577.4110193269: (4, 11),\n86338.86146117302: (6, 20)\n\n\nI've found sample code to do it with a list but that's just for a 2-D plot.\n"
"Is there a convenient way to split an array such that regardless of the number of elements in each section, the range of values it contains is the same?\n\nSay we have data in range (0, 100). Let the size of the array be 1000. The first 500 elements are all in (0, 20), 300 elements in (20, 40) and so on. I'd like to manipulate values in the subsections split by 20, 40, 60 and 80.\n\nThe data could look something like this:\n\n1st div:  0,  0,  0, ... 17, 18\n2nd div: 22, 22, 24, ... 37, 39\n3rd div: 40, 41, 41, ... 55, 59\n4th div: 65, 68, 73, 76, 76\n5th div: 93, 96\n\n\nIt's very easy to split an array in equal size sections by the section size. But I'm plotting a trend line using some simple averaging, and the amount of data in each section varies. I know the split points.\n\nIt could be made with np.where with a condition like arr &gt; border1 taking only the first element, combining and then splitting, but that seems like a long-ish way of doing things.\n\nAny pointers would be greatly appreciated. I can't be the only one with this problem. Also, if another library does this kind of thing, I'd certainly be open to using it.\n"
"I'm successfully using the groupby() function to compute statistics on grouped data, however, I'd now like to do the same for subsets of each group. \n\nI can't seem to understand how to generate a subset for each group (as a groupby object) that can then be applied to a  groupby function such as mean(). The following line works as intended:\n\nd.groupby(['X','Y'])['Value'].mean()\n\n\nHow can I subset the values of the individual groups to then supply to the mean function?  I suspect transform() or filter() might be useful though I can't figure out how.\n\nEDIT to add reproducible example:\n\nrandom.seed(881)\nvalue = np.random.randn(15)\nletter = np.random.choice(['a','b','c'],15)\ndate = np.repeat(pd.date_range(start = '1/1/2001', periods=3), 5)\ndata = {'date':date,'letter':letter,'value':value}\ndf = pd.DataFrame(data)\ndf.groupby(['date','letter'])['value'].mean()\n\n date        letter\n2001-01-01  a        -0.039407\n            b        -0.350787\n            c         1.221200\n2001-01-02  a        -0.688744\n            b         0.346961\n            c        -0.702222\n2001-01-03  a         1.320947\n            b        -0.915636\n            c        -0.419655\nName: value, dtype: float64\n\n\nHere's an example of calculating the mean of the multi-level group.  Now I'd like to find the mean of a subset of each group.  For example, the mean of each groups data that is &lt; the groups 10th percentile.  The key take away being that the subsets must be performed on the groups and not the entire df first.\n"
'I have two numpy arrays of integers A and B. The values in array A and B correspond to time-points at which events A and B occurred. I would like to transform A to contain the time since the most recent event b occurred.\n\nI know I need to subtract each element of A by its nearest smaller the element of B but am unsure of how to do so. Any help would be greatly appreciated.\n\n&gt;&gt;&gt; import numpy as np\n\n&gt;&gt;&gt; A = np.array([11, 12, 13, 17, 20, 22, 33, 34])\n&gt;&gt;&gt; B = np.array([5, 10, 15, 20, 25, 30])\n\n\nDesired Result:\n\ncond_a = relative_timestamp(to_transform=A, reference=B)\ncond_a\n&gt;&gt;&gt; array([1, 2, 3, 2, 0, 2, 3, 4])\n\n'
'I need to sort viewers by hour to a histogram. I have some experience using Matplotlib to do that, but I can\'t find out what is the most pragmatic way to sort the dates by hour.\n\nFirst I read the data from a JSON file, then store the two relevant datatypes in a pandas Dataframe, like this:\n\ndata = pd.read_json(\'data/data.json\')\n\nsession_duration = pd.to_datetime(data.session_duration, unit=\'s\').dt.time\ntime = pd.to_datetime(data.time, format=\'%H:%M:%S\').dt.time\n\nviewers = []\n\nfor x, y in zip(time, session_duration):\n    viewers.append({str(x):str(y)})\n\n\nEDIT: The source file looks like this, leaving out the irrelevant parts.\n\n{\n    "time": "00:00:09",\n    "session_duration": 91\n},\n{\n    "time": "00:00:16",\n    "session_duration": 29\n},\n{\n    "time": "00:00:33",\n    "session_duration": 102\n},\n{\n    "time": "00:00:35",\n    "session_duration": 203\n}\n\n\n\nNote that the session_duration is in seconds.\n\nI have to distinguish two types of viewers:\n\n\nThose who spent &lt;= 1 minutes on the stream \nThose who spent >= 1    minutes on the stream\n\n\nFor that I do:\n\nimport datetime\nfor element in viewers:\n    for time, session_duration in element.items():\n        if datetime.strptime(session_duration, \'%H:%M:%S\').time() &gt;= datetime.strptime(\'00:01:00\', \'%H:%M:%S\').time():\n            viewers_more_than_1min.append(element)\n        else:\n            viewers_less_than_1min.append(element)\n\n\nAs a result I have my values in a dictionary like this: {session_duration:time}\nWhere the key is the time when the session ended the stream and the value is the time spent watching.\n\n[{\'00:00:09\': \'00:01:31\'},\n {\'00:00:16\': \'00:00:29\'},\n {\'00:00:33\': \'00:01:42\'},\n {\'00:00:35\': \'00:03:23\'},\n {\'00:00:36\': \'00:00:32\'},\n {\'00:00:37\': \'00:04:47\'},\n {\'00:00:47\': \'00:00:42\'},\n {\'00:00:53\': \'00:00:44\'},\n {\'00:00:56\': \'00:00:28\'},\n {\'00:00:58\': \'00:01:17\'},\n {\'00:01:04\': \'00:01:16\'},\n {\'00:01:09\': \'00:00:46\'},\n {\'00:01:29\': \'00:01:07\'},\n {\'00:01:31\': \'00:01:02\'},\n {\'00:01:32\': \'00:01:01\'},\n {\'00:01:32\': \'00:00:36\'},\n {\'00:01:37\': \'00:03:03\'},\n {\'00:01:49\': \'00:00:57\'},\n {\'00:02:01\': \'00:02:15\'},\n {\'00:02:18\': \'00:01:16\'}]\n\n\nAs a final step I wish to create a histogram withMatplotlib representing the viewercount for each our from the two viewertypes mentioned above per hour. I assume it would go something like this:\n\nimport matplotlib.pyplot as plt\nimport datetime as dt\nhours = [(dt.time(i).strftime(\'%H:00\')) for i in range(24)]\nplt.xlabel(\'Hour\')\nplt.ylabel(\'Viewer count\')\nplt.bar(hours, sorted_viewcount_byhour)\n\n\n'
'I have a Data frame with this shape:\n\n       x    y\n1     1510 -125\n2     1636 -125\n3     1637 -125\n4     1509 -124\n5     1510 -124\n...    ...  ...\n4210  1555   68\n4211  1556   68\n4212  1682   68\n4213  1554   69\n4214  1555   69\n\n\nI would like to find all the same values on x and keep just the one that has the highest absolute value on y. \n\nFor example on the given dataframe :\n\n       x    y\n1     1510 -125\n...    ...  ...\n5     1510 -124\n...    ...  ...\n4210  1555   68\n...    ...  ...\n4214  1555   69\n\n\nI would like to keep \n\n       x    y\n1     1510 -125\n...    ...  ...\n4214  1555   69\n\n\nand delete the other values with same x and lower y.\n'
'\nPremise: I have been working on this ML dataset and I found my ADA boost and SVM to be extremely good when it comes to detecting TP. The confusion matrix for both models is identical shown below.\nHere\'s the image:\n\n\n\nOut of the 10 models I have trained, 2 of them are ADA and SVM. The other 8, some have lower accuracy and others higher by ~+-2%\n\nMAIN QUESTION: \n\nHow do I chain/pipeline so that all my test cases are handled in the following manner?\n\n\nPass all the cases through SVM and ADA. If the either SVM or ADA has 80%+ confidence return the result\nElse, if SVM or ADA don\'t have a high confidence, have only those test cases evaluated by the other 8 models for a final decision\n\n\nPotential Solution:\nMy potential attempt involved the use of 2 voting classifiers. One classifier with just ADA and SVM, the second classifier with the other 8 models. But I don\'t know hot to make this work\n\n\nHere\'s the code for my approach:\n\nfrom sklearn.ensemble import VotingClassifier\nensemble1=VotingClassifier(estimators=[\n                                            (\'SVM\',model[5]),\n                                            (\'ADA\',model[7]),\n                                             ], voting=\'hard\').fit(X_train,Y_train)\nprint(\'The accuracy for ensembled model is:\',ensemble1.score(X_test, Y_test))\n\n\n#I was trying to make ensemble 1 the "first pass" if it was more than 80% confident in it\'s decision, return the result\n#ELSE, ensemble 2 jumps in to make a decision\n\n\nensemble2=VotingClassifier(estimators=[\n                                            (\'LR\',model[0]),\n                                            (\'DT\',model[1]),\n                                            (\'RFC\',model[2]),\n                                            (\'KNN\',model[3]),\n                                            (\'GBB\',model[4]),\n                                            (\'MLP\',model[6]),\n                                            (\'EXT\',model[8]),\n                                            (\'XG\',model[9])\n                                             ], voting=\'hard\').fit(X_train,Y_train)\n\n\n#I don\'t know how to make these two models work together though.\n\n\nExtra Questions:\nThese questions are to facilitate some extra concerns I had and are NOT the main question:\n\n\nIs what I am trying to do worth it?\nIs it normal to have a Confusion matrix with just True Positives and False Positives? Or is this indicative of incorrect training? As seen above in the picture for Model 5.\nAre the accuracies of my models on an individual level considered to be good? The models are predicting likelihood of developing heart disease. Accuracies below:\n\n\n\nSorry for the long post and thanks for all your input and suggestions. I\'m new to ML so I\'d appreciate any pointers.\n'
'We are trying to create new identification number (unique key) for finding unique customer using python pandas or python network graph(networkx lib):\n\nTwo columns depending on each other(vice-versa), need to group by both the columns and generate a new unique key.\n\nBelow is a list of the sample data in python pandas dataframe. \n\nInput data set:\n\n    r_vid   d_ph_nm    d_flg\n    DQLA853 6123340277  N\n    DQLA851 6999045706  N\n    DQLA851 6999340277  Y\n    DQLCT41 6999045706  N\n    DQLCT41 7123104672  N\n    DQLCT41 9123010121  N\n    DQLA852 6999290277  N\n    DQLA962 6999290277  Y\n    DQLC181 6222232026  N\n    DQLT381 6222232026  N\n    DQLC860 9912332326  N\n    DQLC860 9912336579  N\n\n\nOutput data set:\n\n    r_vid_group        d_ph_nm_group                              new_unique_id\n    DQLA851,DQLCT41   6999045706,6999340277,7123104672,9123010121     123\n    DQLA852,DQLA962   6999290277                                      124\n    DQLA853           6123340277                                      125\n    DQLC181,DQLT381   6222232026                                      126\n    DQLC860           9912332326,9912336579                           127\n\n\nCould you please suggest in python pandas or python pandas network graph(networkx lib).\n'
'I have a dataframe\n\ndf = \ntype value\n A     5\n A     7\n B     2\n B     6\n A     1\n\n\nI was to apply function per the groups of the column type , so the rank will apply per type and the new df will be\n\ndf = \ntype value rank\n A     5    1\n A     7    0\n B     2    1\n B     6    0\n A     1    2\n\n'
'Politics  Politics  Politics    Arts/Culture  Arts/Culture  Arts/Culture\n  nan       nan        c         nan            nan          c\n  nan        b         nan        a             nan          nan\n  nan        b         nan        a             nan          nan\n  a          nan       nan        nan           c            nan   \n\n\nBasically, this goes on throughout the dataframe.\nI want to merge the similar columns to the dataframe below\nPolitics    Arts/Culture  \n c              c\n b              a\n b              a\n a              c\n\n\n'
"I am troubling with counting the number of counties using famous cenus.csv data.\n\nTask:  Count number of counties in each state.\n\nFacing comparing (I think) / Please read below?\n\nI've tried this:\n\ndf = pd.read_csv('census.csv')\ndfd = df[:]['STNAME'].unique()  //Gives out names of state\n\nserr = pd.Series(dfd)  // converting to series (from array)\n\n\nAfter this, i've tried using two approaches:\n\n1:\n\n    df[df['STNAME'] == serr] **//ERROR: series length must match**\n\n\n2:   \n\ni = 0\nfor name in serr:                        //This generate error 'Alabama'\n    df['STNAME'] == name\n    for i in serr:\n        serr[i] == serr[name]\n        print(serr[name].count)\n        i+=1\n\n\nPlease guide me; it has been three days with this stuff.\n"
'I\'ve just discovered the Pipeline feature of scikit-learn, and I find it very useful for testing different combinations of preprocessing steps before training my model. \n\nA pipeline is a chain of objects that implement the fit and transform methods. Now, if I wanted to add a new preprocessing step, I used to write a  class that inherits from sklearn.base.estimator. However, I\'m thinking that there must be a simpler method. Do I really need to wrap every function I want to apply in an estimator class? \n\nExample:\n\nclass Categorizer(sklearn.base.BaseEstimator):\n    """\n    Converts given columns into pandas dtype \'category\'.\n    """\n\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y):\n        return self\n\n\n    def transform(self, X):\n        for column in self.columns:\n            X[column] = X[column].astype("category")\n        return X\n\n'
"I have df with column names: 'a', 'b', 'c' ... 'z'.\n\nprint(my_df.columns)\nIndex(['a', 'b', 'c', ... 'y', 'z'],\n  dtype='object', name=0)\n\n\nI have function which determine which columns should be displayed. For example:\n\nstart = con_start()\nstop = con_stop()\nprint(my_df.columns &gt;= start) &amp; (my_df &lt;= stop)\n\n\nMy result is:\n\n[False False ... False False False False  True  True\nTrue  True False False]\n\n\nMy goal is display dataframe only with columns that satisfy my condition.\nIf start = 'a' and stop = 'b', I want to have:\n\n0                                      a              b         \nindex1       index2                                                  \nNew York     New York           0.000000       0.000000          \nCalifornia   Los Angeles   207066.666667  214466.666667     \nIllinois     Chicago       138400.000000  143633.333333     \nPennsylvania Philadelphia   53000.000000   53633.333333      \nArizona      Phoenix       111833.333333  114366.666667 \n\n"
"I am new to data science and I am currently practicing to improve my skills. I used a data set from kaggle and planned how to present the data and came across a problem.\n\nWhat I was trying to achieve is to insert data to different data frames using a for loop. I have seen an example of this and used the dictionary to save data frames but the data on the data frame is overwritten.\n\nI have a list of data frames:\n\ncontinents_list = [african_countries, asian_countries, european_countries, north_american_countries,\n          south_american_countries, oceanian_countries]\n\n\nThis is an example of my data frame from one of the continents:\n\n    Continent   Country Name   Country Code    2010    2011    2012    2013    2014\n7    Oceania      Australia         AUS        11.4    11.4    11.7    12.2    13.1\n63   Oceania         Fiji           FJI        20.1    20.1    20.2    19.6    18.6\n149  Oceania     New Zealand        NZL        17.0    17.2    17.7    15.8    14.6\n157  Oceania   Papua New Guinea     PNG         5.4     5.3     5.4     5.5     5.4\n174  Oceania   Solomon Islands      SLB         9.1     8.9     9.3     9.4     9.5\n\n\nI first selected the whole row for the country which has the highest rate on a year:\n\ndef select_highest_rate(continent, year):\n    highest_rate_idx = continent[year].idxmax()\n    return continent.loc[highest_rate_idx]\n\n\nthen created a for loop which creates different data frames for each separate years which must contain all the continent and its corresponding country and rate on that year:\n\ndef show_highest_countries(continents_list):\n    df_highest_countries = {}\n    years_list = ['2010','2011','2012','2013','2014']\n    for continent in continents_list:\n        for year in years_list:\n            highest_country = select_highest_rate(continent, year)\n            highest_countries = highest_country[['Continent','Country Name',year]]\n            df_highest_countries[year] = pd.DataFrame(highest_countries)\n    return df_highest_countries\n\n\nhere is what it returns: different data frames but only for the last continent\n\nQuestion: How do I save all the data(continents) on the same data frame? Is it not possible with dictionaries?\n"
'Say n_repeats=5 and the number of fold is 3 (n_splits=3).\n\nDoes that mean the validator is creating 3-folds for our estimator/model to use every fold (like what KFold is for), then repeating that process for 5 times?\n\nThat means our model will use a total of 5 x 3 = 15 folds?\n'
'I\'m trying to create a 4x4 FacetGrid in seaborn for 4 boxplots, each of which is split into 3 boxplots based on the iris species in the iris dataset. Currently, my code looks like this:\n\nsns.set(style="whitegrid")\niris_vis = sns.load_dataset("iris")\n\nfig, axes = plt.subplots(2, 2)\n\nax = sns.boxplot(x="Species", y="SepalLengthCm", data=iris, orient=\'v\', \n    ax=axes[0])\nax = sns.boxplot(x="Species", y="SepalWidthCm", data=iris, orient=\'v\', \n    ax=axes[1])\nax = sns.boxplot(x="Species", y="PetalLengthCm", data=iris, orient=\'v\', \n    ax=axes[2])\nax = sns.boxplot(x="Species", y="PetalWidthCm", data=iris, orient=\'v\', \n    ax=axes[3])\n\n\nHowever, I\'m getting this error from my interpreter:\n\nAttributeError: \'numpy.ndarray\' object has no attribute \'boxplot\'\n\n\nI\'m confused on where the attribute error is exactly in here. What do I need to change?\n'
"Following up my previous question: Sorting datetime objects by hour to a pandas dataframe then visualize to histogram\n\nI need to plot 3 bars for one X-axis value representing viewer counts. Now they show those under one minute and above. I need one showing the overall viewers. I have the Dataframe but I can't seem to make them look right. With just 2 bars I have no problem, it looks just like I would want it with two bars:\n\n\nThe relevant part of the code for this:\n\n# Time and date stamp variables\nallviews = int(df['time'].dt.hour.count())\ndate = str(df['date'][0].date())\nhours = df_hist_short.index.tolist()\nhours[:] = [str(x) + ':00' for x in hours]\n\n\nThe hours variable that I use to represent the X-axis may be problematic, since I convert it to string so I can make the hours look like 23:00 instead of just the pandas index output 23 etc. I have seen examples where people add or subtract values from the X to change the bars position.\n\nfig, ax = plt.subplots(figsize=(20, 5))\nshort_viewers = ax.bar(hours, df_hist_short['time'], width=-0.35, align='edge')\nlong_viewers = ax.bar(hours, df_hist_long['time'], width=0.35, align='edge')\n\n\nNow I set the align='edge' and the two width values are absolutes and negatives. But I have no idea how to make it look right with 3 bars. I didn't find any positioning arguments for the bars. Also I have tried to work with the plt.hist() but I couldn't get the same output as with the plt.bar() function.\n\nSo as a result I wish to have a 3rd bar on the graph shown above on the left side, a bit wider than the other two.\n"
"I would like to create DataFrame, possibly sparse, which measure the correlations between users. Here, my definition of correlation between user_1 and user_2 is the number of times they both performed the same action on the same day.\n\nI will try to explain myself better using an example. Suppose I have the following Dataframe:\n\ndate    action  user\n6   2019-05-05  b   user_3\n9   2019-05-05  b   user_2\n1   2019-05-06  b   user_2\n5   2019-05-06  a   user_1\n0   2019-05-07  b   user_3\n7   2019-05-07  a   user_2\n8   2019-05-07  a   user_1\n2   2019-05-08  c   user_2\n4   2019-05-08  c   user_1\n3   2019-05-09  c   user_3\n\n\nwhich can be generated using this snippet:\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(12)\nusers = np.random.choice(['user_1', 'user_2', 'user_3'], size=10)\nactions = np.random.choice(['a', 'b', 'c'], size=10)\ndate = np.random.choice(pd.date_range(start='2019-05-05', end='2019-05-10', freq='D'), size=10)\n\ndf = pd.DataFrame(dict(date=date, action=actions, user=users))\ndf.date = pd.to_datetime(df.date)\ndf = df.sort_values('date')\n\n\nThe correlation between user_1 and user_2 is 2 since they both performed action a on the day 07 and action c on day 08. The correlation between user_2 and user_3 is 1 because they performed action b on day 05. All the rest is NaN. They output DataFrame I'm seeking is the following:\n\n        user_1  user_2  user_3\nuser_1  NaN     NaN     NaN\nuser_2  2.0     NaN     NaN\nuser_3  NaN     1.0     NaN\n\n\n\n\nMy inefficient way of creating this DataFrame is the following:\n\nfrom itertools import combinations\ndf_result = pd.DataFrame(columns=['user_1', 'user_2', 'user_3'],\n                         index=['user_1', 'user_2', 'user_3'], dtype=np.float64)    \n\nfor index, group in df.groupby(['date', 'action']):\n    for x, y in combinations(list(group.user.values), 2):\n        if np.isnan(df_result.loc[x,y]):\n            df_result.loc[x, y] = 1\n        else:\n            df_result.loc[x, y] = df_result.loc[x, y] + 1\n\n\nThe problem with this approach is being way to slow in my use-case.\n"
"When trying to read England's Covid_19 data into pandas, I've tried to use the URL provided by PHE https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv however, this file needs a http 308 redirect. I have tried the elegant solution:\n\nimport pandas as pd\ntabel = pd.read_csv('https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv')\n\n\nwhich trows the error HTTPError: HTTP Error 308: Permanent Redirect\n\nHowever, the URL works as\n\nimport pandas as pd\nimport requests\nimport io\ndatastr = requests.get('https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv',allow_redirects=True).text\ndata_file = io.StringIO(datastr)\ntable = pd.read_csv(data_file)\n\n\ngives the desired result.\n\nI would like something similar to the first solution, is this a problem of pandas or am I doing something wrong?\n"
"I'm looking for information on how should a Python Machine Learning project be organized. For Python usual projects there is Cookiecutter and for R ProjectTemplate. \n\nThis is my current folder structure, but I'm mixing Jupyter Notebooks with actual Python code and it does not seems very clear.\n\n.\n├── cache\n├── data\n├── my_module\n├── logs\n├── notebooks\n├── scripts\n├── snippets\n└── tools\n\n\nI work in the scripts folder and currently adding all the functions in files under my_module, but that leads to errors loading data(relative/absolute paths) and other problems.\n\nI could not find proper best practices or good examples on this topic besides this kaggle competition solution and some Notebooks that have all the functions condensed at the start of such Notebook.\n"
'I could manage to display total probabilities of my labels, for example after displaying my decision tree, I have a table :\n\nTotal Predictions :\n    65% impressions\n    30% clicks\n    5%  conversions\n\n\nBut my issue is to find probabilities (or to count) by features (by node), for example :\n\nif feature1 &gt; 5\n   if feature2 &lt; 10\n      Predict Impressions\n      samples : 30 Impressions\n   else feature2 &gt;= 10\n      Predict Clicks\n      samples : 5 Clicks\n\n\nScikit does it automatically , I am trying to find a way to do it with Spark\n'
'I am trying to find a way to convert IP addresses to integers or real values so they can be mathematically grouped and plotted in a meaningful way. Ideally IPs with the same leftmost octet would be similar, IPs sharing the leftmost two octets would be more similar, and so on. \n\nIs there a standard, accepted way to do this? Codified in, say, a Python library? Or am I just going to have to cook up my own algorithm?\n\nThanks.\n'
'I have the following data frame:\n\nid        day           total_amount\n 1       2015-07-09         1000\n 1       2015-10-22          100\n 1       2015-11-12          200\n 1       2015-11-27         2392\n 1       2015-12-16          123\n 7       2015-07-09          200\n 7       2015-07-09         1000\n 7       2015-08-27       100018\n 7       2015-11-25         1000\n 8       2015-08-27         1000\n 8       2015-12-07        10000\n 8       2016-01-18          796\n 8       2016-03-31        10000\n15       2015-09-10         1500\n15       2015-09-30         1000\n\n\nI need to subtract every two successive time in day column if they have the same id until reaching the last row of that id then start subtracting times in day column this time for new id, something similar to following lines in output is expected: \n\n 1       2015-08-09         1000 2015-11-22 - 2015-08-09\n 1       2015-11-22          100 2015-12-12 - 2015-11-22\n 1       2015-12-12          200 2015-12-16 - 2015-12-12\n 1       2015-12-16         2392 2015-12-27 - 2015-12-27\n 1       2015-12-27          123         NA\n 7       2015-08-09          200 2015-09-09 - 2015-08-09\n 7       2015-09-09         1000 2015-09-27 - 2015-09-09\n 7       2015-09-27       100018 2015-12-25 - 2015-09-27\n 7       2015-12-25         1000         NA\n 8       2015-08-27         1000  2015-12-07 - 2015-08-27\n 8       2015-12-07        10000  2016-02-18 - 2015-12-07\n 8       2016-02-18          796   2016-04-31- 2016-02-18     \n 8       2016-04-31        10000         NA\n15       2015-10-10         1500  2015-10-30 - 2015-10-10\n15       2015-10-30         1000         NA\n\n'
"I'm working with an airbnb dataset on Kaggle:\n\nhttps://www.kaggle.com/c/airbnb-recruiting-new-user-bookings\n\n\nand want to simplify the values for the language column into 2 groupings - english and non-english. \n\nFor instance:\n\nusers.language.value_counts()\nen    15011\nzh      101\nfr       99\nde       53\nes       53\nko       43\nru       21\nit       20\nja       19\npt       14\nsv       11\nno        6\nda        5\nnl        4\nel        2\npl        2\ntr        2\ncs        1\nfi        1\nis        1\nhu        1\nName: language, dtype: int64\n\n\nAnd the result I want it is:\n\nusers.language.value_counts()\n    english    15011\n    non-english 459\n    Name: language, dtype: int64\n\n\nThis is sort of the solution I want:\n\ndef language_groupings():\n    for i in users:\n        if users.language !='en':\n            replace(users.language.str, 'non-english')\n        else: \n            replace(users.language.str, 'english')\n    return users\n\nusers['language'] = users.apply(lambda row: language_groupings)\n\n\nExcept there's obviously something wrong with this as it returns an empty series when I run value_counts on the column. \n"
"I coded the for loop to enumerate a multidimensional ndarray containing n rows of 28x28 pixel values.\n\nI am looking for the index of each row that is duplicated and the indices of the duplicates without redundancies.\n\nI found this code here (thanks unutbu) and modified it to read the ndarray, it works 70% of the time, however 30% of the time it is identifying the wrong images as duplicates. \n\nHow can it be improved to detect the correct rows?\n\ndef overlap_same(arr):\nseen = []\ndups = collections.defaultdict(list)\nfor i, item in enumerate(arr):\n    for j, orig in enumerate(seen):\n        if np.array_equal(item, orig):\n            dups[j].append(i)\n            break\n    else:\n        seen.append(item)\nreturn dups\n\n\ne.g. return overlap_same(train) returns:\n\ndefaultdict(&lt;type 'list'&gt;, {34: [1388], 35: [1815], 583: [3045], 3208:\n[4426], 626: [824], 507: [4438], 188: [338, 431, 540, 757, 765, 806,\n808, 834, 882, 1515, 1539, 1715, 1725, 1789, 1841, 2038, 2081, 2165,\n2170, 2300, 2455, 2683, 2733, 2957, 3290, 3293, 3311, 3373, 3446, 3542,\n3565, 3890, 4110, 4197, 4206, 4364, 4371, 4734, 4851]})\n\n\nplotting some samples of the correct case on matplotlib gives:\n\nfig = plt.figure()\na=fig.add_subplot(1,2,1)\nplt.imshow(train[35])\na.set_title('train[35]')\na=fig.add_subplot(1,2,2)\nplt.imshow(train[1815])\na.set_title('train[1815]')\nplt.show\n\n\n\n\nwhich is correct\n\nHowever:\n\nfig = plt.figure()\na=fig.add_subplot(1,2,1)\nplt.imshow(train[3208])\na.set_title('train[3208]')\na=fig.add_subplot(1,2,2)\nplt.imshow(train[4426])\na.set_title('train[4426]')\nplt.show\n\n\n\n\nis incorrect as they do not match\n\nSample data (train[:3])\n\narray([[[-0.5       , -0.5       , -0.5       , ...,  0.48823529,\n      0.5       ,  0.17058824],\n    [-0.5       , -0.5       , -0.5       , ...,  0.48823529,\n      0.5       , -0.0372549 ],\n    [-0.5       , -0.5       , -0.5       , ...,  0.5       ,\n      0.47647059, -0.24509804],\n    ..., \n    [-0.49215686,  0.34705883,  0.5       , ..., -0.5       ,\n     -0.5       , -0.5       ],\n    [-0.31176472,  0.44901961,  0.5       , ..., -0.5       ,\n     -0.5       , -0.5       ],\n    [-0.11176471,  0.5       ,  0.49215686, ..., -0.5       ,\n     -0.5       , -0.5       ]],\n\n   [[-0.24509804,  0.2764706 ,  0.5       , ...,  0.5       ,\n      0.25294119, -0.36666667],\n    [-0.5       , -0.47254902, -0.02941176, ...,  0.20196079,\n     -0.46862745, -0.5       ],\n    [-0.49215686, -0.5       , -0.5       , ..., -0.47647059,\n     -0.5       , -0.49607843],\n    ..., \n    [-0.49215686, -0.49607843, -0.5       , ..., -0.5       ,\n     -0.5       , -0.49215686],\n    [-0.5       , -0.5       , -0.26862746, ...,  0.13137256,\n     -0.46470588, -0.5       ],\n    [-0.30000001,  0.11960784,  0.48823529, ...,  0.5       ,\n      0.28431374, -0.24117647]],\n\n   [[-0.5       , -0.5       , -0.5       , ..., -0.5       ,\n     -0.5       , -0.5       ],\n    [-0.5       , -0.5       , -0.5       , ..., -0.5       ,\n     -0.5       , -0.5       ],\n    [-0.5       , -0.5       , -0.5       , ..., -0.5       ,\n     -0.5       , -0.5       ],\n    ..., \n    [-0.5       , -0.5       , -0.5       , ...,  0.48431373,\n      0.5       ,  0.31568629],\n    [-0.5       , -0.49215686, -0.5       , ...,  0.49215686,\n      0.5       ,  0.04901961],\n    [-0.5       , -0.5       , -0.5       , ...,  0.04117647,\n     -0.17450981, -0.45686275]]], dtype=float32)\n\n"
'I am creating a dataframe name "salesdata" and it has a column name "Outlet_Size",this column contains some missing data.This is my code-:\n\n#defining a dictionary\ncat_dict ={}\n#getting all the values of the column\noutlet_size_values = salesdata.Outlet_Size.values\nunique_outlet_size_val = list(set(outlet_size_values))  \nprint(unique_outlet_size_val)\n\n\nthe output I am getting is\n    [nan,\'High\',\'Medium\',\'Small\']\nI don\'t want this missing data(nan) to be the part of my list and I don;t want to create a new list for this.\n'
"I have this:  \n\ndf = DataFrame(dict(person= ['andy', 'rubin', 'ciara', 'jack'], \n     item = ['a', 'b', 'a', 'c'], \n     group= ['c1', 'c2', 'c3', 'c1'], \n     age= [23, 24, 19, 49]))\ndf:\n\n    age group item person\n0   23  c1    a    andy\n1   24  c2    b    rubin\n2   19  c3    a    ciara\n3   49  c1    c    jack\n\n\nwhat I want to do, is to get the length of unique items in each column.\nNow I know I can do something like:  \n\nlen(df.person.unique())\n\n\nfor every column.\n\nIs there a way to do this in one go for all columns?  \n\nI tried to do:\n\nfor column in df.columns:\n    print(len(df.column.unique()))\n\n\nbut I know this is not right.\n\nHow can I accomplish this?\n"
'I have pandas data frame like this\n\na  b  c  d  e  f  label\n1  3  4  5  6  7    1\n2  2  5  7  5  7    0\n4  7  9  0  8  7    1\n6  9  4  7  3  8    1\n7  0  9  8  7  6    0\n\n\nI want a bar graph which looks something like this - : \n\nI have tried using hist() function from pandas but I am not able to figure out how do I include label in the bar graph to get the following graph like the one in the image.\n'
"I am new to data science and currently I'm exploring a bit further. I have over 600,000 columns of a data set and I'm currently cleaning and checking it for inconsistency or outliers. I came across a problem which I am not sure how to solve it. I have some solutions in mind but I am not sure how to do it with pandas.\n\nI have converted the data types of some columns from object to int. I got no errors and checked whether it's in int and it was. I checked the values of one column to check for the factual data. This involves age and I got an error saying my column has a string. so I checked it using this method:\n\nprint('if there is string in numeric column',np.any([isinstance(val, str) for val in homicide_df['Perpetrator Age']])\n\nNow, I wanted to print all indices and with their values and type only on this column which has the string data type.\n\ncurrently I came up with this solution that works fine:\n\ndef check_type(homicide_df):\n    for age in homicide_df['Perpetrator Age']:\n        if type(age) is str:\n            print(age, type(age))\ncheck_type(homicide_df)\n\n\nHere are some of the questions I have:\n\n\nis there a pandas way to do the same thing?\nhow should I convert these elements to int?\nwhy were some elements on the columns did not convert to int?\n\n\nI would appreciate any help. Thank you very much\n"
'For my bachelor thesis, I am working on a project where I want to perform a fit to some data. The problem is a bit more complex, but I tried to minimize the problem here:\n\nWe have three data points (very little theory data is available), but these points are highly correlated.\n\nUsing curve_fit to fit these points, we get a horrible fit result, as you can see in this picture. (The fit could be easily improved by altering the fit parameters by hand).\n\nOur fit results with correlations (blue) and with neglected correlations (orange):\n\n\n\nThe results get better when we use more parameters (as the fits essentially behave like solves by then).\n\nMy question: Why does this behaviour happen? (We use our own least-squares algorithm for our specific problem, but it suffers from the same problem). Is it a numerical problem, or is there any good reason for curve_fit to show this solution?\n\nI would be very happy to have a good explanation to say why we can\'t use "only 2" parameters to fit these highly correlated 3 datapoints.\n\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nplt.rcParams[\'lines.linewidth\'] = 1\n\ny = np.array([1.1994, 1.0941, 1.0047])\nw = np.array([1, 1.08, 1.16])\ncor = np.array([[1, 0.9674, 0.8812],[0.9674, 1, 0.9523], [0.8812, 0.9523, 1]])\ns = np.array([0.0095, 0.0104, 0.0072])\n\ndef f(x, a, b):\n    return a + b*x\n\ncov = np.zeros((3,3))\nfor i in range(3):\n    for j in range(3):\n        cov[i,j] = cor[i,j] * s[i] * s[j]\n\nA1, B1 = curve_fit(f, w, y, sigma=cov)\nA2, B2 = curve_fit(f, w, y)\n\nplt.plot(w, f(w, *A1))\nplt.plot(w, f(w, *A2))\n\nplt.scatter(w, y)\nplt.show()\n\n'
'I have a pyspark data frame with more than one million records, I need to subset in to 4 datafames. Like 1st 2.5 hundred thousand records in to one data frame and next 2.5 hundred thousand records in to next data frame. How can i do this? \n'
'I have a dataframe that has a row called "Hybridization REF". I would like to filter so that I only get the data for the items that have the same label as one of the items in my list.\n\nBasically, I\'d like to do the following: \n\ndataframe[dataframe["Hybridization REF\'].apply(lambda: x in list)] \n\n\nbut that syntax is not correct.\n'
"I am trying to find ROC curve and AUROC curve for decision tree. My code was something like\n\nclf.fit(x,y)\ny_score = clf.fit(x,y).decision_function(test[col])\npred = clf.predict_proba(test[col])\nprint(sklearn.metrics.roc_auc_score(actual,y_score))\nfpr,tpr,thre = sklearn.metrics.roc_curve(actual,y_score)\n\n\noutput:\n\n Error()\n'DecisionTreeClassifier' object has no attribute 'decision_function'\n\n\nbasically, the error is coming up while finding the y_score. Please explain what is y_score and how to solve this problem?\n"
'I have the following CSV data:\n\n+----------+-------------+-------+---------+\n| Category | Part Number | Units |  Cost   |\n+----------+-------------+-------+---------+\n| Axel     |          78 |   587 | $159.95 |\n| Rim      |          48 |   234 | $38.75  |\n| Nut      |          39 |  1234 | $0.15   |\n| Axel     |          79 |    67 | $110.95 |\n+----------+-------------+-------+---------+\n\n\nAnd the following code:\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndf = pd.read_csv(\'stock.csv\',engine="python")\n\n#Sum of values by category\ndf.groupby(\'Category\').sum()[\'Units\']\ndf.groupby(\'Category\').sum()[\'Cost\']\n\n\nWhen I run the second to last line, I get the following output:\n\ndf.groupby(\'Category\').sum()[\'Units\']\nOut[4]: \nCategory\nAxel     654\nNut     1234\nRim      234\nName: Units, dtype: int64\n\n\nWhen I run the last line, I get the following error:\n\nKeyError: \'Cost\'\n\n\nI\'m not sure if there is a simple way to sum the data without converting the data type to an integer and then converting it back.\n'
"I would like to do some tests with neural network final hidden activation layer outputs using sklearn's MLPClassifier after fitting the data.\n\nfor example, If I create a classifier, assuming data X_train with labels y_train and two hidden layers of sizes (300,100)\n\nclf = MLPClassifier(hidden_layer_sizes=(300,100))\nclf.fit(X_train,y_train)\n\n\nI would like to be able to call a function somehow to retrieve the final hidden activation layer vector of length 100 for use in additional tests. \n\nAssuming a test set X_test, y_test, normal prediction would be:\n\npreds = clf.predict(X_test)\n\n\nBut, I would like to do something like:\n\nactivation_layers_for_all_X_test = clf.get_final_activation_output(X_test)\n\n\nFunctions such as get_weights exist, but that would only help me on a per layer basis. Short of doing the transformation myself, is there another methodology to retrieve these final hidden layer activated outputs for the final hidden layer?\n\nLooking at this diagram as an example:\n\n\n\nThe output I would like is the Out Layer, i.e. the final activated output from the final hidden layer.\n"
"So first, I'm in a mission on AI's college group. I have a dataset with many faces in PGM P2(ASCII) format. Before starting Neural Network proccess, I need to extract the array of pixels from images, but I can't found a way to read these images in Python. \n\nI've already tried PIL but it doesn't work with PGM P2.\n\nCan I do this in Python? \nAny help would be much appreciated.\n"
'I have a list of text files in a directory.\n\nI\'d like to create a matrix with the frequency of each word in the entire corpus in every file. (The corpus is every unique word in every file in the directory.)\n\nExample:\n\nFile 1 - "aaa", "xyz", "cccc", "dddd", "aaa"  \nFile 2 - "abc", "aaa"\nCorpus - "aaa", "abc", "cccc", "dddd", "xyz"  \n\n\nOutput matrix:\n\n[[2, 0, 1, 1, 1],\n [1, 1, 0, 0, 0]]\n\n\nMy solution is to use collections.Counter over every file, get a dictionary with the count of every word, and initialize and a list of lists with size n × m (n = number of files, m = number of unique words in corpus). Then, I iterate over every file again to see the frequency of every word in the object, and fill each list with it.\n\nIs there a better way to solve this problem? Maybe in a single pass using collections.Counter?\n'
"In duration of reading about LinearDiscriminantAnalysis using python , I had got two different methods to implement it which are available here ,\nhttp://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n\nIn these method  the signature is here ,\n\nsklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver=’svd’, shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)\n\n\nNow again i found one more method with same kind of signature , which is available here ,\n\nhttp://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html\n\nsklearn.lda.LDA(solver='svd', shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)\n\n\nI just wanted to know that what is difference between both . which method we should use in projects and why ?\n"
'How do I delete a column from a DataFrame? I know this data is not reproducible as I have a CSV file and I am trying to build a pandas data frame to do some wrangling.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\'C:\\LoadProfiles\\CSV\\WillBaySchl 2013_2014 KW.csv\')\n\nprint(df)\n\n\nThis will return the head/tail and:[34944 rows x 3 columns]    \n\npos0 = 0\npos1 = 1\npos2 = 2\n\ncolname = df.columns[pos0]\nprint(colname)\n\n\nThis will return: Meter ID (I want to drop this column/dataframe)    \n\ncolname = df.columns[pos1]\nprint(colname)\n\n\nThis will return: Date / Time (I want this to be the pd data frame index)\n\ncolname = df.columns[pos2]\nprint(colname)\n\n\nThis will return: KW(ch: 1  set:0) (This is the data that I want to rename "kW" and do some wrangling...)\n\nIf I try this code below:\n\ndf = pd.DataFrame.drop([\'Meter ID\'], axis=1)\n\nprint(df)\n\n\nPython will return the error:TypeError: drop() missing 1 required positional argument: \'labels\'\n\nIf I try this code below:     \n\ndf = pd.DataFrame.drop(columns=[\'Meter ID\'])\nprint(df)\n\n\nPython will return the error: TypeError: drop() got an unexpected keyword argument \'columns\'\n\nAny help is greatly appreciated...\n'
'I have two CSV files(Training set and Test Set). Since there are visible NaN values in few of the columns (status, hedge_value, indicator_code, portfolio_id, desk_id, office_id).\n\nI start the process by replacing the NaN values with some huge value corresponding to the column.\nThen I am doing LabelEncoding to remove the text data and convert them into Numerical data.\nNow, when I try to do OneHotEncoding on the categorical data, I get the error. I tried giving input one by one into the OneHotEncoding constructor, but I get the same error for every column. \n\nBasically, my end goal is to predict the return values, but I am stuck in the data preprocessing part because of this. How do I solve this issue?\n\nI am using Python3.6 with Pandas and Sklearn for data processing.\n\nCode\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntest_data = pd.read_csv(\'test.csv\')\ntrain_data = pd.read_csv(\'train.csv\')\n\n# Replacing Nan values here\ntrain_data[\'status\']=train_data[\'status\'].fillna(2.0)\ntrain_data[\'hedge_value\']=train_data[\'hedge_value\'].fillna(2.0)\ntrain_data[\'indicator_code\']=train_data[\'indicator_code\'].fillna(2.0)\ntrain_data[\'portfolio_id\']=train_data[\'portfolio_id\'].fillna(\'PF99999999\')\ntrain_data[\'desk_id\']=train_data[\'desk_id\'].fillna(\'DSK99999999\')\ntrain_data[\'office_id\']=train_data[\'office_id\'].fillna(\'OFF99999999\')\n\nx_train = train_data.iloc[:, :-1].values\ny_train = train_data.iloc[:, 17].values\n\n# =============================================================================\n# from sklearn.preprocessing import Imputer\n# imputer = Imputer(missing_values="NaN", strategy="mean", axis=0)\n# imputer.fit(x_train[:, 15:17])\n# x_train[:, 15:17] = imputer.fit_transform(x_train[:, 15:17])\n# \n# imputer.fit(x_train[:, 12:13])\n# x_train[:, 12:13] = imputer.fit_transform(x_train[:, 12:13])\n# =============================================================================\n\n\n# Encoding categorical data, i.e. Text data, since calculation happens on numbers only, so having text like \n# Country name, Purchased status will give trouble\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nx_train[:, 0] = labelencoder_X.fit_transform(x_train[:, 0])\nx_train[:, 1] = labelencoder_X.fit_transform(x_train[:, 1])\nx_train[:, 2] = labelencoder_X.fit_transform(x_train[:, 2])\nx_train[:, 3] = labelencoder_X.fit_transform(x_train[:, 3])\nx_train[:, 6] = labelencoder_X.fit_transform(x_train[:, 6])\nx_train[:, 8] = labelencoder_X.fit_transform(x_train[:, 8])\nx_train[:, 14] = labelencoder_X.fit_transform(x_train[:, 14])\n\n\n# =============================================================================\n# import numpy as np\n# x_train[:, 3] = x_train[:, 3].reshape(x_train[:, 3].size,1)\n# x_train[:, 3] = x_train[:, 3].astype(np.float64, copy=False)\n# np.isnan(x_train[:, 3]).any()\n# =============================================================================\n\n\n# =============================================================================\n# from sklearn.preprocessing import StandardScaler\n# sc_X = StandardScaler()\n# x_train = sc_X.fit_transform(x_train)\n# =============================================================================\n\nonehotencoder = OneHotEncoder(categorical_features=[0,1,2,3,6,8,14])\nx_train = onehotencoder.fit_transform(x_train).toarray() # Replace Country Names with One Hot Encoding.\n\n\nError\n\nTraceback (most recent call last):\n\n  File "&lt;ipython-input-4-4992bf3d00b8&gt;", line 58, in &lt;module&gt;\n    x_train = onehotencoder.fit_transform(x_train).toarray() # Replace Country Names with One Hot Encoding.\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py", line 2019, in fit_transform\n    self.categorical_features, copy=True)\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py", line 1809, in _transform_selected\n    X = check_array(X, accept_sparse=\'csc\', copy=copy, dtype=FLOAT_DTYPES)\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py", line 453, in check_array\n    _assert_all_finite(array)\n\n  File "/Users/parthapratimneog/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py", line 44, in _assert_all_finite\n    " or a value too large for %r." % X.dtype)\n\nValueError: Input contains NaN, infinity or a value too large for dtype(\'float64\').\n\n'
'Say I have a pandas column as below\n\nType\ntype1\ntype2\n type3\n\nand now i will take dummies for above as follows: \ntype_dummies = pd.get_dummies(["Type"], prefix="type")\n\nThen after joing it with the main DataFrame the resulting df would be something like below:\n\ndf.drop([\'Type\'], axis=1, inplace=True)\ndf = df.join(type_dummies)\ndf.head()\n\ntype_type1    type_type2    type_type3\n   1              0             0\n   0              1             0\n   0              0             1\n\n\nBut what if in my training set there is an another category as type4 in Type column. So how would I use get_dummies() method to generate dummies as much as I want. That is, in this case I want to generate 4 dummy variables although there are only 3 categories in the desired column?\n'
'I am having dataset which is of the following shape:\n\ntconst  GreaterEuropean British WestEuropean    Italian French  Jewish  Germanic    Nordic  Asian   GreaterEastAsian    Japanese    Hispanic    GreaterAfrican  Africans    EastAsian   Muslim  IndianSubContinent  total_ethnicities\n0   tt0000001   3   1   2   0   1   0   0   1   0   0   0   0   0   0   0   0   0   8\n1   tt0000002   2   0   2   0   2   0   0   0   0   0   0   0   0   0   0   0   0   6\n2   tt0000003   4   0   3   0   3   1   0   0   0   0   0   0   0   0   0   0   0   11\n3   tt0000004   2   0   2   0   2   0   0   0   0   0   0   0   0   0   0   0   0   6\n4   tt0000005   3   2   1   0   0   0   1   0   0   0   0   0   0   0   0   0   0   7\n\n\nIt is IMDB data and after processing, I created these columns which represents there are this many number of ethnic actors in a movie (tcons).\n\nI want to create another column df["diversity"] which is:\n\n( diversity score "gini index") \n\nFor example:\nfor each movie lets say we have 10 actors; 3 asian, 3 British, 3 african american and 1 french. so we divide by total\n3/10     3/ 10   3/10     1/10\nthen     1  minus the summation of (  3/10 ) square   ( 3/ 10) square  ( 3/10) square (1/10) square \nadd the score of each actor to a column as diversity.\n\nI am trying simple pandas manipulation, but not getting there.\n\nEDIT:\n\nfor the first row,\nwe have total ethnicities as 8\n\n3 GreaterEuropean\n1 British\n2 WestEuropean\n1 French\n1 nordic\n\n\nso the score will be \n\n1- [(3/8)^2 + (1/8)^2 + (2/8)^2 + (1/8)^2  + (1/8)^2]\n'
"I'm following Andrew Ng Coursera course on Machine Learning and I tried to implement the Gradient Descent Algorithm in Python. I'm having trouble with the y-intercept parameter because it doesn't look like to go to the best value. Here's my code:\n\n# IMPORTS\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Acquiring Data\n# Source: https://github.com/mattnedrich/GradientDescentExample\ndata = pd.read_csv('data.csv')\n\ndef cost_function(a, b, x_values, y_values):\n    '''\n    Calculates the square mean error for a given dataset\n    with (x,y) pairs and the model y' = a + bx\n\n    a: y-intercept for the model\n    b: slope of the curve\n    x_values, y_values: points (x,y) of the dataset\n    '''\n    data_len = len(x_values)\n    total_error = sum([((a + b * x_values[i]) - y_values[i])**2\n                       for i in range(data_len)])\n    return total_error / (2 * float(data_len))\n\n\ndef a_gradient(a, b, x_values, y_values):\n    '''\n    Partial derivative of the cost_function with respect to 'a'\n\n    a, b: values for 'a' and 'b'\n    x_values, y_values: points (x,y) of the dataset\n    '''\n    data_len = len(x_values)\n    a_gradient = sum([((a + b * x_values[i]) - y_values[i])\n                      for i in range(data_len)])\n    return a_gradient / float(data_len)\n\n\ndef b_gradient(a, b, x_values, y_values):\n    '''\n    Partial derivative of the cost_function with respect to 'b'\n\n    a, b: values for 'a' and 'b'\n    x_values, y_values: points (x,y) of the dataset\n    '''\n    data_len = len(x_values)\n    b_gradient = sum([(((a + b * x_values[i]) - y_values[i]) * x_values[i])\n                      for i in range(data_len)])\n    return b_gradient / float(data_len)\n\n\ndef gradient_descent_step(a_current, b_current, x_values, y_values, alpha):\n    '''\n    Give a step in direction of the minimum of the cost_function using\n    the 'a' and 'b' gradiants. Return new values for 'a' and 'b'.\n\n    a_current, b_current: the current values for 'a' and 'b'\n    x_values, y_values: points (x,y) of the dataset\n\n    '''\n    new_a = a_current - alpha * a_gradient(a_current, b_current, x_values, y_values)\n    new_b = b_current - alpha * b_gradient(a_current, b_current, x_values, y_values)\n    return (new_a, new_b)\n\n\ndef run_gradient_descent(a, b, x_values, y_values, alpha, precision, plot=False, verbose=False):\n    '''\n    Runs the gradient_descent_step function and updates (a,b) until\n    the value of the cost function varies less than 'precision'.\n\n    a, b: initial values for the point a and b in the cost_function\n    x_values, y_values: points (x,y) of the dataset\n    alpha: learning rate for the algorithm\n    precision: value for the algorithm to stop calculation\n    '''\n    iterations = 0\n    delta_cost = cost_function(a, b, x_values, y_values)\n\n    error_list = [delta_cost]\n    iteration_list = [0]\n\n    # The loop runs until the delta_cost reaches the precision defined\n    # When the variation in cost_function is small it means that the\n    # the function is near its minimum and the parameters 'a' and 'b'\n    # are a good guess for modeling the dataset.\n    while delta_cost &gt; precision:\n        iterations += 1\n        iteration_list.append(iterations)\n\n        # Calculates the initial error with current a,b values\n        prev_cost = cost_function(a, b, x_values, y_values)\n\n        # Calculates new values for a and b\n        a, b = gradient_descent_step(a, b, x_values, y_values, alpha)\n\n        # Updates the value of the error\n        actual_cost = cost_function(a, b, x_values, y_values)\n        error_list.append(actual_cost)\n\n        # Calculates the difference between previous and actual error values.\n        delta_cost = prev_cost - actual_cost\n\n    # Plot the error in each iteration to see how it decreases\n    # and some information about our final results\n    if plot:\n        plt.plot(iteration_list, error_list, '-')\n        plt.title('Error Minimization')\n        plt.xlabel('Iteration',fontsize=12)\n        plt.ylabel('Error',fontsize=12)\n        plt.show()\n    if verbose:\n        print('Iterations = ' + str(iterations))\n        print('Cost Function Value = '+ str(cost_function(a, b, x_values, y_values)))\n        print('a = ' + str(a) + ' and b = ' + str(b))\n\n    return (actual_cost, a, b)\n\n\nWhen I run the algorithm with:\n\nrun_gradient_descent(0, 0, data['x'], data['y'], 0.0001, 0.01)\n\n\nI get (a = 0.0496688656535 and b = 1.47825808018)\n\nBut the best value for 'a' is around 7.9 (tried another resources for linear regression).\n\nAlso, if I change the initial guess for the parameter 'a' the algorithm simply try to adjust the parameter 'b'. \n\nFor example, if I set a = 200 and b = 0\n\nrun_gradient_descent(200, 0, data['x'], data['y'], 0.0001, 0.01)\n\n\nI get (a = 199.933763331 and b = -2.44824996193)\n\nI couldn't find anything wrong with the code and I realized that the problem is the initial guess for a parameter. See my own answer above where I defined a helper function to get a range for search some values for initial a guess.\n"
"I have alot of data in a dictionary format and I am attempting to use pandas print a string based on an IF ELSE statement. For my example ill make up some data in dict and covert to Pandas:\n\ndf = pd.DataFrame(dict(a=[1.5,2.8,9.3],b=[7.2,3.3,4.9],c=[13.1,4.9,15.9],d=[1.1,1.9,2.9]))\n\ndf\n\n\nThis returns:\n\n    a   b   c   d\n0   1.5 7.2 13.1 1.1\n1   2.8 3.3 4.9 1.9\n2   9.3 4.9 15.9 2.9\n\n\nMy IF ELSE statement:\n\nfor col in df.columns:\n    if (df[col] &lt; 4).any():\n        print('Zone %s does not make setpoint' % col)\n    else:\n        print('Zone %s is Normal' % col)\n\n\nReturns:\n\nZone a does not make setpoint\nZone b does not make setpoint\nZone c is Normal\nZone d does not make setpoint\n\n\nBut now I want to add in an extra to create a box plot where I am not making setpoint and also average the data frame where it is making setpoint. I know this is pandas series, but can pandas.Series.plot.box() be used?\n\nThis is my IF ELSE statement that I am using in a function with df.apply(lamba x:) and I am stuck trying to get the box box plot to work in pandas series... Any advice is greatly appreciated!\n\nimport matplotlib.pyplot as plt\n\ndef _print(x):\n    if (x &lt; 4).any():\n        print('Zone %s does not make setpoint' % x.name)\n        df.boxplot()\n        plt.show()\n    else:\n        print('Zone %s is Normal' % x.name)\n        print('The average is %s' % x.mean())\n\n\nIm getting an error when I am calling df.apply(lambda x: _print(x))\n\n\n  module 'matplotlib' has no attribute 'show'\n\n"
'Getting started with pyspark.ml and the pipelines API, I find myself writing custom transformers for typical preprocessing tasks in order to use them in a pipeline. Examples:\n\nfrom pyspark.ml import Pipeline, Transformer\n\n\nclass CustomTransformer(Transformer):\n    # lazy workaround - a transformer needs to have these attributes\n    _defaultParamMap = dict()\n    _paramMap = dict()\n    _params = dict()\n\nclass ColumnSelector(CustomTransformer):\n    """Transformer that selects a subset of columns\n    - to be used as pipeline stage"""\n\n    def __init__(self, columns):\n        self.columns = columns\n\n\n    def _transform(self, data):\n        return data.select(self.columns)\n\n\nclass ColumnRenamer(CustomTransformer):\n    """Transformer renames one column"""\n\n\n    def __init__(self, rename):\n        self.rename = rename\n\n    def _transform(self, data):\n        (colNameBefore, colNameAfter) = self.rename\n        return data.withColumnRenamed(colNameBefore, colNameAfter)\n\n\nclass NaDropper(CustomTransformer):\n    """\n    Drops rows with at least one not-a-number element\n    """\n\n    def __init__(self, cols=None):\n        self.cols = cols\n\n\n    def _transform(self, data):\n        dataAfterDrop = data.dropna(subset=self.cols) \n        return dataAfterDrop\n\n\nclass ColumnCaster(CustomTransformer):\n\n    def __init__(self, col, toType):\n        self.col = col\n        self.toType = toType\n\n    def _transform(self, data):\n        return data.withColumn(self.col, data[self.col].cast(self.toType))\n\n\nThey work, but I was wondering if this is a pattern or antipattern - are such transformers a good way to work with the pipeline API? Was it necessary to implement them, or is equivalent functionality provided somewhere else?\n'
"I want to apply a function row-wise to a dataframe that looks like this:\n\nname  value \n\n'foo' 2\n'bar' 4\n'bar' 3\n'foo' 1\n  .   .\n  .   .\n  .   .\n'bar' 8\n\n\nSpeed is important to me since I am operating on multiple 90GB datasets, so I have been attempting to vectorize the following operation for use in df.apply:\n\nConditioned on the 'name', I want to plug 'value' into a separate function, perform some arithmetic on the result, and write to a new column 'output'. Something like,\n\nfuncs = {'foo': &lt;FunctionObject&gt;, 'bar': &lt;FunctionObject&gt;}\n\ndef masterFunc(row):\n    correctFunction = funcs[row['name']]\n    row['output'] = correctFunction(row['value']) + 3*row['value']\n\ndf.apply(masterFunc, axis=1).\n\n\nIn my real problem, I have 32 different functions that could apply to the 'value' based on the 'name'. Each of those individual functions (fooFunc, barFunc, zooFunc, etc) are already vectorized; they are scipy.interp1d functions built like this:\n\nseparateFunc = scipy.interpolate.interp1d(x-coords=[2, 3, 4], y-coords=[3, 5, 7])\n#separateFunc is now a math function, y=2x-1. use case:\ny = separateFunc(3.5) # y == 6\n\n\nHowever, I am not sure how I can vectorize the masterFunc itself. It seems like choosing which function to 'pull out' to apply to the 'value' is very expensive, because it requires a memory access at each iteration (with my current method of storing the functions in hashtables). However, the alternative just seems to be a bunch of if-then statements, which also seems impossible to vectorize. How can I speed this up?\n\nActual code, with repetitive parts removed for brevity:\n\ninterpolationFunctions = {}\n#the 'interpolate.emissionsFunctions' are a separate function which does some scipy stuff\ninterpolationFunctions[2] = interpolate.emissionsFunctions('./roadtype_2_curve.csv')\ninterpolationFunctions[3] = interpolate.emissionsFunctions('./roadtype_3_curve.csv')\n\ndef compute_pollutants(row):\n    funcs = interpolationFunctions[row['roadtype']]\n    speed = row['speed']\n    length = row['length']\n    row['CO2-Atm'] = funcs['CO2-Atm'](speed)*length*speed*0.00310686368\n    row['CO2-Eq'] = funcs['CO2-Eq'](speed)*length*speed*0.00310686368\n    return row\n\n"
"I am trying to split a column into multiple columns based off comma/space seperation.\n\nmy dataframe currently looks like\n\n    Item                                          Colors\n0   ID-1                                          Red, Blue, Green\n1   ID-2                                          Red, Blue\n2   ID-3                                          Blue, Green\n3   ID-4                                          Blue\n4   ID-5                                          Red\n\n\nI would like to transform the 'Colors' column into Red, Blue and Green like this: \n\n    Item                                           Red  Blue  Green\n0   ID-1                                           1    1     1\n1   ID-2                                           1    1     0\n2   ID-3                                           0    1     1\n3   ID-4                                           0    1     0\n4   ID-5                                           1    0     1\n\n\nI really have no idea how to do this. \nAny help would be greatly appreciated. \n"
'I am working on the titanic kaggle competition, to deal with categorical data I’ve splited the data into 2 sets: one for numerical variables and the other for categorical variables.\nAfter working with sklearn one hot encoding on the set with categorical variables I tried the regroup the two datasets but since the categorical set is an ndarray and the other one is a dataframe I used:\n\nnp.hstack((X_train_num, X_train_cat))\n\n\nwhich works perfectly but I no longer have the names of my variables.\n\nIs there another way to do this while maintaining the names of the variables without using pd.get_dummies()?\n\nThanks\n'
'Used this code to genrate corelation table:\n\ndf1.drop([\'BC DataPlus\', \'AC Glossary\'], axis=1).corr(method=\'pearson\').style.format("{:.2}").background_gradient(cmap=plt.get_cmap(\'coolwarm\'), axis=1) \n\n\nThis is the table generated:\n\n\nI cant find any way to save this table as image. Thank you.\n'
"I've tried to use this technique to correct very imbalanced classes.\n\nMy data set has classes e.g.:\n\nIn [123]:\ndata['CON_CHURN_TOTAL'].value_counts()\n\nOut[123]:\n0    100\n1     10\nName: CON_CHURN_TOTAL, dtype: int64\n\n\nI wanted to use SMOTETomek to under sample 0-class and over sample 1-class to achieve ratio 80 : 20. However, I cannot find a way to correct the dictionary. Of course in full code the ratio 80:20 will be calculated based on number of rows. \n\nWhen I am trying:\n\nfrom imblearn.combine import SMOTETomek\nsmt = SMOTETomek(ratio={1:20, 0:80})\n\n\nI have error:\n\n\n  ValueError: With over-sampling methods, the number of samples in a\n  class should be greater or equal to the original number of samples.\n  Originally, there is 100 samples and 80 samples are asked.\n\n\nBut this method should be suitable for doing both under and over sampling at the same time. \n\nUnfortunately the documentary is not working now due to 404 error.\n"
"I am training my data skills in python which I have learned in R. Although, I have a doubt about a simple linear regression \n\nClimate_change Data:\n[link here] \n\nPython Script\n\nimport os\nimport pandas as pd\nimport statsmodels.api as sm\n\n\n\ntrain = df[df.Year&gt;=2006]\n\nX = train[['MEI', 'CO2', 'CH4', 'N2O', 'CFC.11', 'CFC.12', 'TSI', 'Aerosols']]\ny = train[['Temp']]\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\nmodel.summary()\n\n\nPython Result\n\n\n  Dep. Variable: Temp  R-squared: 0.972 \n  \n  Model: OLS  Adj. R-squared: 0.964 \n  \n  Method: Least Squares  F-statistic: 123.1 \n  \n  Date: Mon, 01 Oct 2018  Prob (F-statistic):9.54e-20 \n  \n  Time: 14:52:53  Log-Likelihood: 46.898 \n  \n  No. Observations: 36  AIC: -77.80 \n  \n  Df Residuals: 28  BIC: -65.13 \n  \n  Df Model: 8 \n  \n  Covariance Type: nonrobust \n  \n  MEI\n  0.0361 \n  \n  CO2\n  0.0046 \n  \n  CH4\n  -0.0023\n  \n  N2O\n  -0.0141 \n  \n  CFC-11\n  -0.0312  \n  \n  CFC-12\n  0.0358  \n  \n  TSI\n  -0.0033\n  \n  Aerosols\n  69.9680\n  \n  Omnibus:8.397\n  Durbin-Watson: 1.484 \n  \n  Prob(Omnibus):0.015\n  Jarque-Bera (JB):10.511 \n  \n  Skew: -0.546\n  Prob(JB): 0.00522 \n  \n  Kurtosis: 5.412\n  Cond. No. 6.35e+06\n\n\nR Script\n\ntrain &lt;- climate_change[climate_change$Year&gt;=2006,]\nprev &lt;- lm(Temp ~ ., data = train[,3:NCOL(train)])\nsummary(prev)\n\n\nR Result\n\n\n  Residuals:\n        Min        1Q    Median        3Q       Max \n  -0.221684 -0.032846  0.002042  0.037158  0.167887 \n  \n  Coefficients:\n  MEI 0.036056\n  CO2 0.004817\n  CH4 -0.002366\n  N2O -0.013007\n  CFC-11 -0.033194\n  CFC-12 0.037775\n  TSI   0.009100\n  Aerosols 70.463329\n  Residual standard error: 0.07594 on 27 degrees of freedom Multiple\n  R-squared:  0.5346,   Adjusted R-squared:  0.3967  F-statistic: 3.877 on\n  8 and 27 DF,  p-value: 0.003721\n\n\nQuestion\n\nThe R-squared has big difference between them, also the coefficients of independent variable has a bit difference. Someone could explain why?\n"
'Iam getting the error as \n\n\n  "ValueError: Expected 2D array, got 1D array instead: array=[  45000. \n  50000.   60000.   80000.  110000.  150000.  200000.  300000.\n    500000. 1000000.]. Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it\n  contains a single sample."\n\n\nwhile executing the following code: \n\n# SVR\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv(\'Position_S.csv\')\nX = dataset.iloc[:, 1:2].values\ny = dataset.iloc[:, 2].values\n\n # Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX = sc_X.fit_transform(X)\ny = sc_y.fit_transform(y)\n\n# Fitting SVR to the dataset\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = \'rbf\')\nregressor.fit(X, y)\n\n# Visualising the SVR results\nplt.scatter(X, y, color = \'red\')\nplt.plot(X, regressor.predict(X), color = \'blue\')\nplt.title(\'Truth or Bluff (SVR)\')\nplt.xlabel(\'Position level\')\nplt.ylabel(\'Salary\')\nplt.show()\n\n# Visualising the SVR results (for higher resolution and smoother curve)\nX_grid = np.arange(min(X), max(X), 0.01)\nX_grid = X_grid.reshape((len(X_grid), 1))\nplt.scatter(X, y, color = \'red\')\nplt.plot(X_grid, regressor.predict(X_grid), color = \'blue\')\nplt.title(\'Truth or Bluff (SVR)\')\nplt.xlabel(\'Position level\')\nplt.ylabel(\'Salary\')\nplt.show()\n\n'
'I am working with 3 data frames, out of which 2 data frames contains additional bin number assigned to column data based on the range in which they belong (in separate columns).\n\ndf_1\n\nA&emsp;&emsp;   B\n\n5&emsp;&emsp;   6\n\n8&emsp;&emsp;   1\n\n6&emsp;&emsp;   7\n\n4&emsp;&emsp;   9\n\n1&emsp;&emsp;   3\n\n9&emsp;&emsp;   2\n\n2&emsp;&emsp;   5\n\ndf_2\n\nA&emsp;&emsp;   B&emsp;&emsp;   A_bin&emsp;&emsp;   B_bin\n\n5&emsp;&emsp;   6&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   2\n\n8&emsp;&emsp;   1&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   1\n\n6&emsp;&emsp;   7&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;   2\n\n4&emsp;&emsp;   9&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;   3\n\n1&emsp;&emsp;   3&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   1\n\n9&emsp;&emsp;   2&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   1\n\n2&emsp;&emsp;   5&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   2\n\ndf_3\n\nA&emsp;&emsp;   B&emsp;&emsp;   C&emsp;&emsp;   D&emsp;&emsp;   A_bin&emsp;&emsp;   B_bin&emsp;&emsp;   C_bin&emsp;&emsp;   D_bin\n\n5&emsp;&emsp;   6&emsp;&emsp;   2&emsp;&emsp;   6&emsp;&emsp;&emsp; 2&emsp;&emsp;&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   2\n\n8&emsp;&emsp;   1&emsp;&emsp;   6&emsp;&emsp;   4&emsp;&emsp;&emsp; 1&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;\n\n6&emsp;&emsp;   7&emsp;&emsp;   3&emsp;&emsp;   1&emsp;&emsp;&emsp; 3&emsp;&emsp;&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;\n\n4&emsp;&emsp;   9&emsp;&emsp;   1&emsp;&emsp;   9&emsp;&emsp;&emsp; 3&emsp;&emsp;&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;\n\n1&emsp;&emsp;   3&emsp;&emsp;   8&emsp;&emsp;   7&emsp;&emsp;&emsp; 1&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;\n\n9&emsp;&emsp;   2&emsp;&emsp;   4&emsp;&emsp;   8&emsp;&emsp;&emsp; 1&emsp;&emsp;&emsp;&emsp;   1&emsp;&emsp;&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;\n\n2&emsp;&emsp;   5&emsp;&emsp;   9&emsp;&emsp;   2&emsp;&emsp;&emsp; 2&emsp;&emsp;&emsp;&emsp;   2&emsp;&emsp;&emsp;&emsp;   3&emsp;&emsp;&emsp;&emsp;   1\n\ndf_1 contain just two columns, df_2 have additional column which contain the bin assigned to column A and B according to the range in which the belong, similarly, df_3 contains columns with values and additional column with bin number assigned.\n\nI want to extract the rows from df_3 such that it only extract data where df_2 columns have bin value "2" for each column respectively in a separate data frame.\n\nThe Main problem i am facing is to do it WITHOUT mentioning the column names anywhere in the code.\n\nexpected output\n\ndf_output_1 (where bin values for column \'A\' in df_2 is 2)\n\nA&emsp;&emsp;   B&emsp;&emsp;   C&emsp;&emsp;   D&emsp;&emsp;   \n\n5&emsp;&emsp;   6&emsp;&emsp;   2&emsp;&emsp;   6\n\n2&emsp;&emsp;   5&emsp;&emsp;   9&emsp;&emsp;   2\n\ndf_output_2 (where bin values for column \'B\' in df_2 is 2)\n\nA&emsp;&emsp;   B&emsp;&emsp;   C&emsp;&emsp;   D&emsp;&emsp;\n\n5&emsp;&emsp;   6&emsp;&emsp;   2&emsp;&emsp;   6\n\n6&emsp;&emsp;   7&emsp;&emsp;   3&emsp;&emsp;   1\n\n2&emsp;&emsp;   5&emsp;&emsp;   9&emsp;&emsp;   2\n'
'I have two columns with datetime in gmt and I need subtract threee hours of this datetime. For example in line 4 I need subtract startdate in 3 hours, the result it was: 08/02/2018 17:20:0. And in the same line 4 a i need to subtract enddate in 3 hours, the result it was: 08/02/2018 21:50:0. \n\nINITIAL TABLE\n\ncpf  day  startdate              enddate\n1234  1   08/01/2018 12:50:0     08/01/2018 15:30:0\n1234  1   08/01/2018 14:30:0     08/01/2018 15:40:0\n1234  1   08/01/2018 14:50:0     08/01/2018 15:50:0\n1234  2   08/02/2018 20:20:0     08/03/2018 00:50:0\n1234  3   08/03/2018 01:00:0     08/03/2018 03:50:0\n1235  1   08/01/2018 11:50:0     08/01/2018 15:20:0\n5212  1   08/01/2018 14:50:0     08/01/2018 15:20:0\n\n\nRESULT TABLE\n\ncpf  day  startdate              enddate\n1234  1   08/01/2018 09:50:0     08/01/2018 10:30:0\n1234  1   08/01/2018 11:30:0     08/01/2018 10:40:0\n1234  1   08/01/2018 11:50:0     08/01/2018 10:50:0\n1234  2   08/02/2018 17:20:0     08/02/2018 21:50:0\n1234  3   08/02/2018 22:00:0     08/03/2018 00:50:0\n1235  1   08/01/2018 08:50:0     08/01/2018 10:20:0\n5212  1   08/01/2018 11:50:0     08/01/2018 10:20:0\n\n\nHow can I do that in python?\n\nP.S.: Please, pay attention in results. Thank you!\n'
"I have a dataset where I need to remove some huge outliers (10x the regular data) but I can't figure out a smart way to do it. I tried \n\nif df['pickup_latitude'] &gt;= 3*df['pickup_latitude'].mean():\n   df['pickup_latitude'] = df['pickup_latitude'].mean()\n\n\nBut that gives me: ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\nI have tried other methods \n\ndf[np.abs(df.Data-df.Data.mean()) &lt;= (3*df.Data.std())]\n\n\nbut they don't work because I have timestamps on my data which break the other solutions. \n\nAny smart way to filter the outliers away or replace them with other values?\n"
'In my column of the data frame i have version numbers like 6.3.5, 1.8, 5.10.0 saved as objects and thus likely as Strings. I want to remove the dots with nothing so i get 635, 18, 5100. My code idea was this:\n\nfor row in dataset.ver:\n    row.replace(".","",inplace=True)\n\n\nThe thing is it works if I don\'t set inplace to True, but we want to overwrite it and safe it.\n'
'I\'m currently using python3.7 in a Jupyter Notebook (v5.6.0) with pandas 0.23.4. \n\nI\'ve written code to tokenize some Japanese words and have successfully applied a word count function that returns the word counts from each row in a pandas Series like so:\n\n0       [(かげ, 20), (モリア, 17), (たち, 15), (お前, 14), (おれ,...\n1       [(お前, 11), (ゾロ, 10), (うっ, 10), (たち, 9), (サンジ, ...\n2       [(おれ, 11), (男, 6), (てめえ, 6), (お前, 5), (首, 5), ...\n3       [(おれ, 19), (たち, 14), (ヨホホホ, 12), (お前, 10), (みん...\n4       [(ラブーン, 32), (たち, 14), (おれ, 12), (お前, 12), (船長...\n5       [(ヨホホホ, 19), (おれ, 13), (ラブーン, 12), (船長, 11), (...\n6       [(わたし, 20), (おれ, 16), (海賊, 9), (お前, 9), (もう, 9...\n7       [(たち, 21), (あたし, 15), (宝石, 14), (おれ, 12), (ハッ,...\n8       [(おれ, 13), (あれ, 9), (もう, 7), (ヨホホホ, 7), (見え, 7...\n9       [(ケイミー, 23), (人魚, 20), (はっち, 14), (おれ, 13), (め...\n10      [(ケイミー, 18), (おれ, 17), (め, 14), (たち, 12), (はっち... \n\n\nFrom this previously asked question:\n\nCreating a dictionary of word count of multiple text files in a directory\n\nI thought I could use the answer to help with my objective.\n\nI want to consolidate all the above pairs in each row into a dictionary where the key is the Japanese text, and the value is the sum of all the instances of the text appearing within the data set. I thought I could accomplish this with the collections.Counter module by turning each row in the series into a dictionary, like this:\n\nvocab_list = []\nfor i in range(len(wordcount)):\n    vocab_list.append(dict(wordcount[i]))\n\n\nWhich gives me the dictionary format that I want, where each row in the Series is now a dictionary, like so:\n\n[{\'かげ\': 20,\n \'モリア\': 17,\n \'たち\': 15,\n \'お前\': 14,\n \'おれ\': 11,\n \'もう\': 9,\n \'船長\': 7,\n \'っ\': 7,\n \'七武海\': 7,\n \'言っ\': 6, ...\n\n\nMy problem comes when I try to use the sum() function and Counter() to aggregate the totals:\n\nvocab_list = sum(vocab_list, Counter())\nprint(vocab_list)\n\n\nInstead of getting the expected "aggregated dictionary", I receive the following error:\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-37-3c66e97f4559&gt; in &lt;module&gt;()\n      3     vocab_list.append(dict(wordcount[i]))\n      4 \n----&gt; 5 vocab_list = sum(vocab_list, Counter())\n      6 vocab_list\n\nTypeError: unsupported operand type(s) for +: \'Counter\' and \'dict\'\n\n\nCould you explain what exactly is wrong in the code and how to fix it?\n'
'I have the following dataframe.\n\ndf = pd.DataFrame(\n    {\n        "drive": [1,1,2,2,2,3,3,3,4,4,4,5,5,6,6,7,7],\n        "team": [\'home\',\'home\',\'away\',\'away\',\'away\',\'home\',\'home\',\'home\',\'away\',\n                 \'away\',\'away\',\'home\',\'home\',\'away\',\'away\',\'home\',\'home\'],\n        "home_comfy_lead": [0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,1],\n        "home_drives": [1,1,0,0,0,2,2,2,0,0,0,3,3,0,0,4,4],\n        \'home_drives_with_comfy_lead\': [0,0,0,0,0,0,0,1,0,0,0,2,2,0,0,3,3]\n    })\n\n\nI am trying to make two columns:\n\n\nA home_drives column that uniquely counts the drives from the\ndrive column based on the \'home\' designation from the team\ncolumn.\nA home_drives_with_comfy_lead column that uniquely counts the\nhome_drives values based on whether home_comfy_lead is true.\n\n\nMy desired output is:\n\n    drive  team  home_comfy_lead  home_drives  home_drives_with_comfy_lead\n0       1  home                0            1                            0\n1       1  home                0            1                            0\n2       2  away                0            0                            0\n3       2  away                0            0                            0\n4       2  away                0            0                            0\n5       3  home                0            2                            0\n6       3  home                0            2                            0\n7       3  home                1            2                            1\n8       4  away                0            0                            0\n9       4  away                0            0                            0\n10      4  away                0            0                            0\n11      5  home                1            3                            2\n12      5  home                1            3                            2\n13      6  away                0            0                            0\n14      6  away                0            0                            0\n15      7  home                1            4                            3\n16      7  home                1            4                            3\n\n\nCan anyone help with this? I\'ve been struggling with this for a few days now.\n'
"I have some time series data with three separate colums (Date, Time, kW) that looks like this:\n\nDate     Time        kW\n3/1/2011 12:15:00 AM 171.36\n3/1/2011 12:30:00 AM 181.44\n3/1/2011 12:45:00 AM 175.68\n3/1/2011 1:00:00 AM 180.00\n3/1/2011 1:15:00 AM 175.68\n\n\nAnd reading the csv file directly from Pandas I can parse the Date &amp; Time:\n\ndf= pd.read_csv('C:\\\\Users\\\\desktop\\\\master.csv', parse_dates=[['Date', 'Time']])\n\n\nWhich appears to work nicely, but the problem is I want to create another data frame in Pandas to represent the numerical value of the month. If I do a: \n\ndf['month'] = df.index.month\n\n\nAn error is thrown:\n\nAttributeError: 'Int64Index' object has no attribute 'month'\n\nI am also hoping to create additional dataframes to represent time stampt day, minute, hour... Any tips greatly appreciated..\n"
'I\'m an avid R user and am learning python along the way. One of the example code that I can easily run in R is perplexing me in Python.\n\nHere\'s the original data (constructed within R):\n\nlibrary(tidyverse)\n\n\ndf &lt;- tribble(~name, ~age, ~gender, ~height_in,\n        "john",20,\'m\',66,\n        \'mary\',NA,\'f\',62,\n        NA,38,\'f\',68,\n        \'larry\',NA,NA,NA\n)\n\n\nThe output of this looks like this:\n\ndf\n\n# A tibble: 4 x 4\n  name    age gender height_in\n  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 john     20 m             66\n2 mary     NA f             62\n3 NA       38 f             68\n4 larry    NA NA            NA\n\n\nI want to do 3 things:\n\n\nI want to replace the NA values in columns that are characters with the value "zz"\nI want to replace the NA values in columns that are numeric with the value 0\nI want to convert the character columns to factors.\n\n\nHere\'s how I did it in R (again, using the tidyverse package):\n\ntmp &lt;- df %&gt;%\n  mutate_if(is.character, function(x) ifelse(is.na(x),"zz",x)) %&gt;%\n  mutate_if(is.character, as.factor) %&gt;%\n  mutate_if(is.numeric, function(x) ifelse(is.na(x), 0, x))\n\n\nHere\'s the output of the dataframe tmp:\n\ntmp\n\n# A tibble: 4 x 4\n  name    age gender height_in\n  &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;\n1 john     20 m             66\n2 mary      0 f             62\n3 zz       38 f             68\n4 larry     0 zz             0\n\n\nI\'m familiar with if() and else() statements within Python. What I don\'t know is the correct and most readable way of executing the above code within Python. I\'m guessing that there is no mutate_if equivalent in the pandas package. My question is what is the similar code that I can use in python that mimics the mutate_if, is.character, is.numeric, and as.factor functions found within tidyverse and R?\n\nOn a side note, I\'m not as interested in speed/efficiency of code execution, but rather readability - which is why I really enjoy tidyverse. I would be grateful for any tips or suggestions.\n\nEdit 1: adding code to create a pandas dataframe\n\nHere is the code I used to create the dataframe within Python. This may assist others in getting started.\n\nimport pandas as pd\nimport numpy as np\n\nmy_dict = {\n    \'name\' : [\'john\',\'mary\', np.nan, \'larry\'],\n    \'age\' : [20, np.nan, 38,  np.nan],\n    \'gender\' : [\'m\',\'f\',\'f\', np.nan],\n    \'height_in\' : [66, 62, 68, np.nan]\n}\n\ndf = pd.DataFrame(my_dict)\n\n\nThe output of this should be similar:\n\nprint(df)\n    name   age gender  height_in\n0   john  20.0      m       66.0\n1   mary   NaN      f       62.0\n2    NaN  38.0      f       68.0\n3  larry   NaN    NaN        NaN\n\n'
"I need to resample timeseries data and interpolate missing values in 15 min intervals over the course of an hour. Each ID should have four rows of data per hour.\n\nIn:\n\nID            Time  Value\n1   1/1/2019 12:17      3\n1   1/1/2019 12:44      2\n2   1/1/2019 12:02      5\n2   1/1/2019 12:28      7\n\n\nOut:\n\nID                Time  Value\n1  2019-01-01 12:00:00    3.0\n1  2019-01-01 12:15:00    3.0\n1  2019-01-01 12:30:00    2.0\n1  2019-01-01 12:45:00    2.0\n2  2019-01-01 12:00:00    5.0\n2  2019-01-01 12:15:00    7.0\n2  2019-01-01 12:30:00    7.0\n2  2019-01-01 12:45:00    7.0\n\n\nI wrote a function to do this, however efficiency goes down drastically when trying to process a larger dataset.\n\nIs there a more efficient way to do this?\n\nimport datetime\nimport pandas as pd\n\n\ndata = pd.DataFrame({'ID': [1,1,2,2], \n                    'Time': ['1/1/2019 12:17','1/1/2019 12:44','1/1/2019 12:02','1/1/2019 12:28'], \n                    'Value': [3,2,5,7]})\n\n\ndef clean_dataset(data):\n    ids = data.drop_duplicates(subset='ID')\n    data['Time'] = pd.to_datetime(data['Time'])\n    data['Time'] = data['Time'].apply(\n    lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour,15*(dt.minute // 15)))\n    data = data.drop_duplicates(subset=['Time','ID']).reset_index(drop=True)\n    df = pd.DataFrame(columns=['Time','ID','Value'])\n    for i in range(ids.shape[0]):\n        times = pd.DataFrame(pd.date_range('1/1/2019 12:00','1/1/2019 13:00',freq='15min'),columns=['Time'])\n        id_data = data[data['ID']==ids.iloc[i]['ID']]\n        clean_data = times.join(id_data.set_index('Time'), on='Time')\n        clean_data = clean_data.interpolate(method='linear', limit_direction='both')\n        clean_data.drop(clean_data.tail(1).index,inplace=True)\n        df = df.append(clean_data)\n    return df\n\n\nclean_dataset(data)\n\n"
"Click to view data sampleI am trying to replace the Item_Visibility values with the Item_Visibility of each Item_Identifier (each item type). But it throws an error:\n\nValueError: Wrong number of items passed 5, placement implies 1\n\ndata['Item_Visibilty'] = data.groupby('Item_Identifier').transform(\n        lambda x: x.replace(x.mean()))\n\n"
"I had a df such as \n\nID  | Half Hour Bucket | clock in time  | clock out time  | Rate\n232 | 4/1/19 8:00 PM   | 4/1/19 7:12 PM | 4/1/19 10:45 PM | 0.54\n342 | 4/1/19 8:30 PM   | 4/1/19 7:12 PM | 4/1/19 7:22 PM  | 0.23\n232 | 4/1/19 7:00 PM   | 4/1/19 7:12 PM | 4/1/19 10:45 PM | 0.54\n\n\nI want my output to be \n\n ID | Half Hour Bucket | clock in time  | clock out time  | Rate | Mins\n232 | 4/1/19 8:00 PM   | 4/1/19 7:12 PM | 4/1/19 10:45 PM | 0.54 |\n342 | 4/1/19 8:30 PM   | 4/1/19 7:12 PM | 4/1/19 7:22 PM  | 0.23 |\n232 | 4/1/19 7:00 PM   | 4/1/19 7:12 PM | 4/1/19 10:45 PM | 0.54 |\n\n\nWhere minutes represents the difference between clock out time and clock in time.\n\nBut I can only contain the minutes value for the half hour bucket on the same row it corresponds to.\n\nFor example for id 342 it would be ten minutes and the 10 mins would be on that row. \n\nBut for ID 232 the clock in to clock out time spans 3 hours. I would only want the 30 mins for 8 to 830 in the first row and the 18 mins in the third row. for the minutes in the half hour bucket like 830-9 or 9-930 that dont exist in the first row, I would want to create a new row in that same df that contains nans for everything except the half hour bucket and mins field for the minutes that do not exist in the original row.\n\nthe 30 mins from 8-830 would stay in the first row, but I would want 5 new rows for all the half hour buckets that aren't 4/1/19 8:00 PM as new rows with only the half hour bucket and the rate carrying over from the row. Is this possible? \n\nI thank anyone for their time!\n"
'I\'m currently doing this data science problem and I keep running into an issue when trying to loop through each tweet that\'s stored in a filtered list of tweets that\'s mean\'t to send them to a new function to be cleaned up further.\n\nk1_tweets_filtered is just a list of tweets that\'s had any tweet that\'s less than 20 characters removed. What I\'m trying to do now is send that list to another function to process further but its only doing 1 tweet. The list are occupied from searching twitter.\n\nThe issue is that its only doing it for the first tweet and nothing else. I need it to process every tweet in that list. Looking at the len of k1_tweets_filtered, its 512 then len of processed only shows 14. Maybe my loop is wrong?\n\nThank you for the help!\n\nCode:\n\nk1_tweets_processed = []\nfor tweet in k1_tweets_filtered:\n    k1_tweets_processed = pre_process(tweet_k1)\n\n\ndef pre_process(doc):\n    doc = doc.lower()\n    # getting rid of non ascii codes\n    doc = remove_non_ascii(doc)\n\n    # replacing URLs\n    url_pattern = "http://[^\\s]+|https://[^\\s]+|www.[^\\s]+|[^\\s]+\\.com|bit.ly/[^\\s]+"\n    doc = re.sub(url_pattern, \'url\', doc) \n\n    punctuation = r"\\(|\\)|#|\\\'|\\"|-|:|\\\\|\\/|!|\\?|_|,|=|;|&gt;|&lt;|\\.|\\@"\n    doc = re.sub(punctuation, \' \', doc)\n\n    return [w for w in doc.split() if len(w) &gt; 2]\n\n\nIt works fine for one tweet but I\'m trying to send the entire list to it for every tweet in it to be processed properly. The final list should have every tweet processed properly instead of just the first 1.\n'
'I managed to extract data around an index number (in this case +/- 2) using the following code:\n\na = stim_onset[1:]\nss = [(num+1) for num,i in enumerate(zip(stim_onset,a)) if i == (False, True)]\nfin = [i for x in ss for i in range(x-2, x+3 ) if i in range(len(stim_onset))]\ndf = data[1:].loc[np.unique(fin)]\nprint(df)\n\n\n\nwhere stim_onset is a list of boolean False - True\n\ndf looks like this:\n\n           0           1             2             3    \n176    False  8333.912069  28698.791668  4.170312e+07\n177    False  8331.456998  28695.334820  4.170315e+07\n178     True  8326.858504  28695.763083  4.170319e+07\n179     True  8326.862988  28704.501836  4.170322e+07\n180     True  8326.804694  28700.394908  4.170325e+07\n897    False  8280.768191  28618.765863  4.172740e+07\n898    False  8279.306358  28621.403521  4.172744e+07\n899     True  8283.315187  28619.622388  4.172747e+07\n900     True  8278.514906  28631.908033  4.172750e+07\n901     True  8276.656227  28619.356645  4.172754e+07\n1595   False  8243.285199  28565.812841  4.175091e+07\n1596   False  8244.868103  28570.760921  4.175095e+07\n1597    True  8241.247154  28564.194228  4.175098e+07\n1598    True  8241.372710  28578.414742  4.175101e+07\n1599    True  8242.744859  28570.804845  4.175105e+07\n2351   False  8218.234507  28519.885522  4.177637e+07\n2352   False  8214.667367  28514.546515  4.177641e+07\n2353    True  8219.288282  28523.390687  4.177644e+07\n2354    True  8222.958557  28531.947153  4.177647e+07\n2355    True  8221.680575  28531.938369  4.177651e+07\n2906   False  8214.355719  28495.327408  4.179507e+07\n2907   False  8216.021580  28500.741086  4.179510e+07\n2908    True  8219.893642  28506.712604  4.179513e+07\n2909    True  8220.779261  28510.848083  4.179517e+07\n2910    True  8219.299492  28507.771181  4.179520e+07\n3408   False  8201.423437  28479.235716  4.181197e+07\n3409   False  8203.149834  28470.999897  4.181201e+07\n3410    True  8201.952566  28475.888679  4.181204e+07\n3411    True  8200.217201  28481.596651  4.181209e+07\n3412    True  8201.037800  28475.354998  4.181211e+07\n\n\nThis is just a pice of a large dataset and I would like to average and calculate standard error for column 3 of df with index numbers 176-897-1595-2351-2906-3408 and for the following numbers in the block extracted.\nThank you!\n'
"What i actually want to do is to fit all possible straight lines in some data and find the best group of fitted lines by measuring their average R squared.\n\nThe step that i got stuck, is how to obtain with a sufficient method all those possible sublists so that i can make the fit afterwards. That's also the reason why i want a minimum length of 3 because every line that passes through two points has a perfect fit and i don't want that.\n\nFor example my first try was something like that:\n\ndef sub_lists(lst):\n    lr = [lst[:i] for i in range(3,len(lst)-2)]\n    rl = [lst[i:] for i in range(len(lst)-3,2,-1)]\n    return [[lr[i], rl[-i-1]] for i in range(len(lr))]\n\n&gt;&gt;&gt; tst = [489, 495, 501, 506, 508, 514, 520, 522]\n&gt;&gt;&gt; sub_lists(tst)\n[[[489, 495, 501], [506, 508, 514, 520, 522]],\n[[489, 495, 501, 506], [508, 514, 520, 522]],\n[[489, 495, 501, 506, 508], [514, 520, 522]]]\n\n\nbut then i came across the below list with a length of 5 and it didn't work.Thus the expected output would be just the list:\n\n&gt;&gt;&gt; tst = [489, 495, 501, 506, 508]\n&gt;&gt;&gt; sub_lists_revised(tst)\n[489, 495, 501, 506, 508]\n\n\nand following the same logic when i have a bigger length of data, like 10 for example:\n\n&gt;&gt;&gt; tst = [489, 495, 501, 506, 508, 514, 520, 525, 527, 529]\n&gt;&gt;&gt; sub_lists_revised(tst)\n# the whole list\n[489, 495, 501, 506, 508, 514, 520, 525, 527, 529]\n# all possible pairs\n[[[489, 495, 501], [506, 508, 514, 520, 525, 527, 529]],\n[[489, 495, 501, 506], [508, 514, 520, 525, 527, 529]],\n[[489, 495, 501, 506, 508], [514, 520, 525, 527, 529]],\n[[489, 495, 501, 506, 508, 514], [520, 525, 527, 529]],\n[[489, 495, 501, 506, 508, 514, 520], [525, 527, 529]]]\n# and finally, all possible triplets which i couldn't figure out\n[[[489, 495, 501], [506, 508, 514], [520, 525, 527, 529]],\n[[489, 495, 501], [506, 508, 514, 520], [525, 527, 529]],\n[[489, 495, 501, 506], [508, 514, 520], [525, 527, 529]]]\n\n\nSo to conclude, what i want is a general approach that will work for even more data, although i don't think i would really need more than triplets at the moment.\n\nI also add the figures from the first example after the fit: fig1, fig2, \n fig3\n"
'I have a problem where I need to generate list of every combination possible based on given constraints and I am not sure of any approach that might help. I have a list with 12 slots available and I need to populate each slot x and y subject to constraints mentioned below.\n\nx &gt;= 7, x &lt;= 9\ny &gt;= 4, y &lt;= 9\nlist = [0,0,0,0,0,0,0,0,0,0,0,0]\n\n\nI am struggling to formulate the problem correctly but I need a solution where list can have continous value of x from 1 to atleast 7 and maximum 9 and y from 1 to atleast 4 and maximum 9 and list should not have any 0. So one of the solution could be:\n\n# Solution 1 as x has occupied 9 continuous slots and y occupied 4\nx = 1,1,1,1,1,1,1,1,1,0,0,0\ny = 0,0,0,0,0,0,0,0,1,1,1,1\nlist= [1,1,1,1,1,1,1,1,2,1,1,1] \n\n# Solution 2 as x has occupied 8 continuous slots and y occupied 7 with no zero left\nx = 0,0,0,0,1,1,1,1,1,1,1,1\ny = 1,1,1,1,1,1,1,0,0,0,0,0\nlist= [1,1,1,1,2,2,2,1,1,1,1,1] \n\n\nI need to generate every possible combination in the above fashion. Can someone please help or atleast give me some pointer on how can I achieve this in python ?\nThanks\n'
"I want to count the number of unique values in each column and select only those columns which have less than 32 unique values. \n\nI tried using \ndf.filter(nunique&lt;32)\n and \n\ndf[[ c for df.columns in df if c in c.nunique&lt;32]] \n\n\nbut because nunique is a method and not function they don't work. Thought len(set() would work and tried \n\ndf.apply(lambda x : len(set(x))\n\n\nbut doesn't work as well. Any ideas please? thanks in advance!\n"
'I am trying to extract a value in a span however the span is embedded into another. I was wondering how I get the value of only 1 span rather than both.\n\nfrom bs4 import BeautifulSoup\n\n\nsome_price = page_soup.find("div", {"class":"price_FHDfG large_3aP7Z"})\nsome_price.span\n\n# that code returns this:\n\n\'\'\'\n&lt;span&gt;$289&lt;span class="rightEndPrice_6y_hS"&gt;99&lt;/span&gt;&lt;/span&gt;\n\'\'\'\n\n# BUT I only want the $289 part, not the 99 associated with it\n\n\nAfter making this adjustment:\n\nsome_price.span.text\n\n\nthe interpreter returns\n\n$28999\n\n\nWould it be possible to somehow remove the \'99\' at the end? Or to only extract the first part of the span?\n\nAny help/suggestions would be appreciated!\n'
'I am porting code from tensorflow to numpy and i have trouble with this line of code:\n\ntensor_unstack = tf.unstack(some_tensor, axis=0)\n\n\nThe tf.unstack method is used and i was unable to find a equivalent in numpy. So my question is how would a tf.unstack be implemented when using numpy?\n'
"I am trying to implement a lightGBM classifier with a custom objective function. My target data has four classes and my data is divided into natural groups of 12 observations.\n\nThe custom objective function achieve two things:\n\n\nThe predicted model output must be probablistic and the probabilities must sum to one for each observation. This is also known as a softmax objective function and is relatively simple to implement\nThe probabilities for each class must sum to 1 within each group. This has been implemented in the binomial classification space and is known as a conditional logit model.\n\n\nIn summary, for each group (in my case 4 observations), the probabilities should sum to 1 in each column and each row. I have written a slightly hacky function to achieve this, but when I try and  run my custom objective functions within the xgb framework in python, I get the following error:\n\n\n  TypeError: cannot unpack non-iterable numpy.float64 object\n\n\nMy full code is as follows:\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\ndef standardiseProbs(preds, groupSize, eta = 0.1, maxIter = 100):\n\n    # add groupId to preds dataframe\n    n = preds.shape[0]\n    if n % groupSize != 0:\n        print('The selected group size paramter is not compatible with the data')\n    preds['groupId'] = np.repeat(np.arange(0, int(n/groupSize)), groupSize)\n\n    #initialise variables\n    error = 10000\n    i = 0\n\n    # perform loop while error exceeds set threshold (subject to maxIter)\n    while error &gt; eta and i&lt;maxIter:\n        i += 1\n        # get sum of probabilities by game\n        byGroup = preds.groupby('groupId')[0, 1, 2, 3].sum().reset_index()\n        byGroup.columns = ['groupId', '0G', '1G', '2G', '3G']\n\n        if '3G' in list(preds.columns):\n            preds = preds.drop(['3G', '2G', '1G', '0G'], axis=1)\n        preds = preds.merge(byGroup, how='inner', on='groupId')\n\n        # adjust probs to be consistent across a game\n        for v in [1, 2, 3]:\n            preds[v] = preds[v] / preds[str(v) + 'G']\n\n        preds[0] = (groupSize-3)* (preds[0] / preds['0G'])\n\n        # sum probabilities by player\n        preds['rowSum'] = preds[3] + preds[2] + preds[1] + preds[0]\n\n        # adjust probs to be consistent across a player\n        for v in [0, 1, 2, 3]:\n            preds[v] = preds[v] / preds['rowSum']\n\n        # get sum of probabilities by game\n        byGroup = preds.groupby('groupId')[0, 1, 2, 3].sum().reset_index()\n        byGroup.columns = ['groupId', '0G', '1G', '2G', '3G']\n\n        # calc error\n        errMat = abs(np.subtract(byGroup[['0G', '1G', '2G', '3G']].values, np.array([(groupSize-3), 1, 1, 1])))\n        error = sum(sum(errMat))\n\n    preds = preds[['groupId', 0, 1, 2, 3]]\n    return preds\n\ndef condObjective(preds, train):\n    labels = train.get_label()\n    preds = pd.DataFrame(np.reshape(preds, (int(preds.shape[0]/4), 4), order='C'), columns=[0,1,2,3])\n    n = preds.shape[0]\n    yy = np.zeros((n, 4))\n    yy[np.arange(n), labels] = 1\n    preds['matchId'] = np.repeat(np.arange(0, int(n/4)), 4)\n    preds = preds[['matchId', 0, 1, 2, 3]]\n    preds = standardiseProbs(preds, groupSize = 4, eta=0.001, maxIter=500)\n    preds = preds[[0, 1, 2, 3]].values\n    grad = (preds - yy).flatten()\n    hess = (preds * (1. - preds)).flatten()\n    return grad, hess\n\ndef mlogloss(preds, train):\n    labels = train.get_label()\n    preds = pd.DataFrame(np.reshape(preds, (int(preds.shape[0]/4), 4), order='C'), columns=[0,1,2,3])\n    n = preds.shape[0]\n    yy = np.zeros((n, 4))\n    yy[np.arange(n), labels] = 1\n    preds['matchId'] = np.repeat(np.arange(0, int(n/4)), 4)\n    preds = preds[['matchId', 0, 1, 2, 3]]\n    preds = standardiseProbs(preds, groupSize = 4, eta=0.001, maxIter=500)\n    preds = preds[[0, 1, 2, 3]].values\n    loss = -(np.sum(yy*np.log(preds)+(1-yy)*np.log(1-preds))/n)\n    return loss\n\nn, k = 880, 5\n\nxtrain = np.random.rand(n, k)\nytrain = np.random.randint(low=0, high=2, size=n)\nltrain = lgb.Dataset(xtrain, label=ytrain)\nxtest = np.random.rand(int(n/2), k)\nytest = np.random.randint(low=0, high=2, size=int(n/2))\nltest = lgb.Dataset(xtrain, label=ytrain)\n\nlgbmParams = {'boosting_type': 'gbdt', \n              'num_leaves': 250, \n              'max_depth': 3,\n              'min_data_in_leaf': 10, \n              'min_gain_to_split': 0.75, \n              'learning_rate': 0.01, \n              'subsample_for_bin': 120100, \n              'min_child_samples': 70, \n              'reg_alpha': 1.45, \n              'reg_lambda': 2.5, \n              'feature_fraction': 0.45, \n              'bagging_fraction': 0.55, \n              'is_unbalance': True, \n              'objective': 'multiclass', \n              'num_class': 4, \n              'metric': 'multi_logloss', \n              'verbose': 1}\n\nlgbmModel = lgb.train(lgbmParams, ltrain, valid_sets=ltest,fobj=condObjective, feval=mlogloss, num_boost_round=5000, early_stopping_rounds=100, verbose_eval=50)\n\n\nAssuming there isn't a better way to force my predictions to conform to the restrictive conditions I'm putting on them, what do I need to do to make the custom objective work?\n"
'Im really new to Python and Datascience.\n\nI have a large Dataset(with 100K+ rows), in this dataset i have two columns A and B. A is a Datetime column and B is string.\n\nColumn B has some NaN values, i want to fill those NaN values with latest known B column value, given the condition that my empty B column row and already filled B column row are in the same day, month and year (Column A).\n\nLemme explain my Self:\n\nLet\'s say that\'s my input:\n\ndf=pd.DataFrame({\'A\': ["2019-03-13 08:12:23", "2019-03-13 07:10:18", "2019-03-20 08:12:23", "2019-03-13 08:12:23", "2019-03-15 10:35:53", "2019-03-20 11:12:23"], \'B\': ["B1", "B0", "B13", np.nan, "B10", "B12"]})\n                     A    B\n0  2019-03-13 08:12:23   B1\n1  2019-03-13 07:10:18   B0\n2  2019-03-20 08:12:23  B13\n3  2019-03-13 08:12:23  NaN\n4  2019-03-15 10:35:53  B10\n5  2019-03-20 11:12:23  B12\n\n\nI want to fill the NaN value with B1(B value that occurs the same day and has the biggest time given the condition that this "Biggest time" isn\'t ahead of the actual A column value).\n\nSo my output should look like this:\n\n                     A    B\n0  2019-03-13 08:12:23   B1\n1  2019-03-13 07:10:18   B0\n2  2019-03-20 08:12:23  B13\n3  2019-03-13 08:12:23   B1\n4  2019-03-15 10:35:53  B10\n5  2019-03-20 11:12:23  B12\n\n\nI tried to achieve this with no success, the best i could do is making NaN Value to B13 using this :\n\ndf[\'B\']=df[\'B\'].replace({\'B\': {0: np.nan}}).ffill()\n\n\nCan you please guys tell me what\'s the fatest and most economic way to achieve this?\n'
'\n\nI have a dataframe as shown in the image and I want to convert it into multiple rows without changing the order.\n\n  RESP HR SPO2 PULSE\n1  46  122  0    0   \n2  46  122  0    0   \n3\n4\n\n'
'Basically I´m reading a pandas dataframe and converting it to Json. I´m a beginner in coding, but I know that is preferable to use apply function instead iterrows (and I already tried to use apply function, but some difficulties in understand the syntax and find out my solution arose)!!\n\n===============================\n\nData that I´m reading from excel \n\nid     label        id_customer     label_customer    part_number   number_customer   product   label_product    key    country  value_product\n\n6     Sao Paulo      CUST-99992         Brazil          982               10          sho1564       shoes       SH-99   Chile         1.5        \n\n6     Sao Paulo      CUST-99992         Brazil          982               10          sn47282       sneakers    SN-71   Germany       43.8 \n\n6     Sao Paulo      CUST-43535         Argentina       435               15          sk84393       skirt       SK-11   Netherlands   87.1  \n\n92    Hong Hong      CUST-88888         China           785               58          ca40349       cap         CA-82   Russia        3.95\n\n\n===============================\n\nCODE: \n\nimport pandas as pd \nimport json\n\ndf = pd.read_excel(path)\n\nresult = []\nfor labels, df1 in df.groupby([\'id\', \'label\'],sort=False):\n    id_, label = labels\n    record = {\'id\': int(id_), \'label\': label, \'Customer\': []}\n    for inner_labels, df2 in df1.groupby([\'id_customer\', \'label_customer\'],sort=False):\n        id_,label = inner_labels\n        record[\'Customer\'].append({\n            \'id\': id_,\n            \'label\': label,\n            \'Number\': [{\'part\': str(p), \'number_customer\': str(s)} for p, s in zip(df2[\'part_number\'], df2[\'number_customer\'])]  \n            })\n\n    result.append(record)\n\n\n===============================\n\nJson I\'m getting: \n\n[\n {\n  "id": 6,\n  "label": "Sao Paulo",\n  "Customer": [\n   {\n    "id": "CUST-99992",\n    "label": "Brazil",\n    "Number": [\n     {\n      "part": "982",\n      "number_customer": "10"\n     },\n     {\n      "part": "982",\n      "number_customer": "10"\n     }\n    ]\n   },\n   {\n    "id": "CUST-43535",\n    "label": "Argentina",\n    "Number": [\n     {\n      "part": "435",\n      "number_customer": "15"\n     }\n    ]\n   }\n  ]\n },\n {\n  "id": 92,\n  "label": "Hong Kong",\n  "Customer": [\n   {\n    "id": "CUST-88888",\n    "label": "China",\n    "Number": [\n     {\n      "part": "785",\n      "number_customer": "58"\n     }\n    ]\n   }\n  ]\n }\n]\n\n\n===============================\n\nJson expected:\n\n[\n {\n  "id": 6,\n  "label": "Sao Paulo",\n  "Customer": [\n   {\n    "id": "CUST-99992",\n    "label": "Brazil",\n    "Number": [\n     {\n      "part": "982",\n      "number_customer": "10",\n      "Procucts": [\n       {\n        "product": "sho1564",\n        "label_product": "shoes",\n        "Order": [\n        {\n         "key": "SH-99",\n         "country": "Chile",    \n         "value_product": "1.5"\n        }   \n       ]            \n     },\n     {\n        "product": "sn47282",\n        "label_product": "sneakers",\n        "Order": [\n        {\n         "key": "SN-71",\n         "country": "Germany",  \n         "value_product": "43.8"\n        }   \n       ] \n      }\n      ]\n     }\n    ] \n   },\n   {\n    "id": "CUST-43535",\n    "label": "Argentina",\n    "Number": [\n     {\n      "part": "435",\n      "number_customer": "15",\n      "Procucts": [\n       {\n        "product": "sk84393",\n        "label_product": "skirt",\n        "Order": [\n        {\n         "key": "SK-11",\n         "country": "Netherlands",  \n         "value_product": "87.1"\n        }   \n       ]            \n      }\n      ]\n     }\n    ]\n   }\n  ]\n },\n {\n  "id": 92,\n  "label": "Hong Kong",\n  "Customer": [\n   {\n    "id": "CUST-88888",\n    "label": "China",\n    "Number": [\n     {\n      "part": "785",\n      "number_customer": "58",\n      "Procucts": [\n       {\n        "product": "ca40349",\n        "label_product": "cap",\n        "Order": [\n        {\n         "key": "CA-82",\n         "country": "Russia",   \n         "value_product": "3.95"\n        }   \n       ]            \n      }\n      ]\n     }\n    ]\n   }\n  ]\n }\n]\n\n\n===============================\n\nLook that id and label is group of information even as id_customer and label customer is another group, part_number and number_customer is another, product and label_product another, key, country and value_product another. \n\nMy expected Json depends of my information inside my dataframe.\n\nCan somebody help me in any way pls?\n'
"I have a .csv file for to read with pd.read_csv(). Unfortunately, each line is entered in a single cell instead of multiple cells for each column as shown below:\n\n\n\nI am trying something like this to read that file:\n\nsheffield = pd.read_csv('data/sheffield_weather_station.csv', skiprows=8, delimiter='|', engine='python')\n\n\nIt gives me this output without separating each value/data. I checked the spaces between columns in the Microsoft Excel, they are arbitrary. Is there a specific option of pd.read_csv() to solve this problem?\n\n\n"
'Looking to get a continuous rolling mean of a dataframe. \n\ndf looks like this \n\nindex price\n\n0      4\n1      6\n2      10\n3      12\n\n\nlooking to get a continuous rolling of price\n\nthe goal is to have it look this a moving mean of all the prices.\n\nindex price  mean\n\n0      4      4\n1      6      5\n2      10     6.67\n3      12     8 \n\n\nthank you in advance!\n'
"Im working with times series data so for SO purposes Ill make some up..\n\nimport pandas as pd\nimport numpy as np\nfrom numpy.random import randint\nimport matplotlib.pyplot as plt\n\nrng = pd.date_range('10/9/2018 00:00', periods=10, freq='1H')\ndf = pd.DataFrame({'Random_Number':randint(1, 100, 10)}, index=rng)\n\n\nIf I plot this it looks like this: df.plot()\n\n\n\nI can print the values of df upper and lower percentiles:\ndf.quantile(0.025)\ndf.quantile(0.975)\n\nBut how would I add lines to my chart to represent the 2.5th percentile and 97.5th percentile of the dataset?\n"
"I have a pandas dataframe with two columns : locationid, geo_loc.\nlocationid column has missing values.\n\nI want to get the geo_loc value of the missing locationid row,\nthen search this geo_loc value in geo_loc column and get the loction id.\n\ndf1 = pd.DataFrame({'locationid':[111, np.nan, 145, np.nan, 189,np.nan, 158, 145],\n                     'geo_loc':['G12','K11','B16','G12','B22','B16', 'K11',he l 'B16']})\ndf\n\n\n\n\n\nI need the final output like this:\n\n\n\nindex 1 of locationid is missing and the corresponding geo_loc value is 'K11'.\nI would look this 'K11' in geo_loc column and index 6 has locationid 158. With this value \nI want to fill the missing value in index 1.\n\nI tried these codes and they didnt work.\n\ndf1['locationid'] = df1.locationid.fillna(df1.groupby('geo_loc')['locationid'].max())\n\n\ndf1['locationid'] = df1.locationid.fillna(df1.groupby('geo_loc').apply(lambda x: print(list(x.locationid)[0])))\n\n"
'I was going through its documentation and it says\n\n\n  Each sample’s missing values are imputed using the mean value from\n  n_neighbors nearest neighbors found in the training set. Two samples\n  are close if the features that neither are missing are close.\n\n\nNow, playing around with a toy dataset, i.e.\n\n&gt;&gt;&gt;X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n&gt;&gt;&gt;X\n\n   [[ 1.,  2., nan],\n    [ 3.,  4.,  3.],\n    [nan,  6.,  5.],\n    [ 8.,  8.,  7.]]\n\n\nAnd we make a KNNImputer as follows:\n\nimputer = KNNImputer(n_neighbors=2)\n\n\nThe question is, how does it fill the nans while having nans in 2 of the columns? For example, if it is to fill the nan in the 3rd column of the 1st row, how will it choose which features are the closest since one of the rows has nan in the first column as well? When I do imputer.fit_transform(X) it gives me\n\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n\nwhich means for filling out the nan in row 1, the nearest neighbors were the second and the third row. How did it calculate the euclidean distance between the first and the third row?\n'
'I have Data science-related project about a course students took in 2016. I have a column which shows at what dates did the students upgrade their course. If the course has not been upgraded the value is Null. What I want is to create a new data frame consisting of only this upgraded column consisting of "yes" or "no".\nI have attempted the following code and it works, Except I get the following warning:\n"SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame."\nI am putting a sample dataset, the code and the output I got. \nIf someone can tell me a more efficient way with an explanation, It will be great.\n\nimport pandas as pd\n\nregistration = pd.DataFrame({\'upgraded\':[\'2016-08-12 19:42:07+00:00\', \'2016-08-14 11:51:21+00:00\',\n    \'2016-07-22 17:24:59+00:00\', None, None, \'2016-07-12 10:33:02+00:00\']})\n\nupgraded_1 = registration[[\'upgraded\']]\nfor i in range(len(upgraded_1[\'upgraded\'])):\n    if pd.isnull(upgraded_1[\'upgraded\'][i]):\n        upgraded_1[\'upgraded\'][i] = "No"\n    else:\n        upgraded_1[\'upgraded\'][i] = "Yes"\n\n\nOutput:\n\n upgraded_1\n    0   Yes\n    1   Yes\n    2   Yes\n    3   No\n    4   No\n    5   Yes\n\n'
"I have a dataset of events sorted chronologically. I use pandas dataframes. This is what the dataframe looks like:\n\nTime                         Event   Location    ID\n2020-05-22 21:22:04.784622   start   UK          50\n2020-05-22 21:43:07.060629   end     UK          50\n2020-05-25 23:22:04.784622   start   UK          50\n2020-05-25 23:43:07.060629   end     UK          50\n2020-05-25 23:44:15.000566   start   US          30\n2020-05-25 23:48:23.416348   start   Italy       70\n2020-05-26 00:48:06.820164   end     US          30\n2020-05-26 01:33:42.454450   end     Italy       70\n2020-05-27 20:48:23.416348   start   Italy       30\n2020-05-27 00:33:42.454450   end     Italy       30\netc\n\n\nThis is what I would like to make of it :\n\nStart_Time                   End_Time                    Location    ID\n2020-05-22 21:22:04.784622   2020-05-22 21:43:07.060629  UK          50\n2020-05-25 23:22:04.784622   2020-05-25 23:43:07.060629  UK          50\n2020-05-25 23:44:15.000566   2020-05-26 00:48:06.820164  US          30\n2020-05-25 23:48:23.416348   2020-05-26 01:33:42.45445   Italy       70\n2020-05-27 20:48:23.416348   2020-05-27 00:33:42.454450  Italy       30\netc\n\n\n\nI've tried making separate dataframes (one for start, one for end) and merging them on Location and ID, but obviously it does not work. I've also looked at similar questions and wasn't able to figure it out from there. \nWould anyone have an idea of how I do that? \n\nEdit : Also, there would be several events with same locations, or IDs in the dataframe. Edited the data in example to reflect my dataset more accurately\n"
'Suppose I have a dataframe like:\n\n Column1    Column2    Column3    Column4\n 1          I          am         abc\n 3          on         weekend    holidays\n 1          I          do         business\n 2          I          am         xyz\n 3          I          do         nothing\n 2          I          do         job\n\n\nafter applying the groupby() using pandas expected result is:\n\nColumn1    Column2\n1          I am abc I do business\n2          I am Xyz I do job\n3          On weekend holidays I do nothing\n\n\nThe required aggregation is applicable first for the column than by rows. \n\nHow it can be performed?\n'
"After dropping a few rows of a DataFrame using Pandas, the header doesn't change; it keeps the way it was before the rows were dropped.\nHow can I get the updated header?\nfor row in range(rowStart): # rowStart is my index (int). It means it should drop all rows up to this\n    df.drop(row, inplace=True)\ndf = df.reset_index(drop=True)\n\nheader = list(df) # even assigning the header after the drop, it keeps returning the same as before\nprint(header)\nprint('')\nprint(df) # the DataFrame is ok, without the removed rows (as expected)\n\nMinimal example:\ndata = {\n    '': '',\n    'asd': '',\n    'bfdgfd': '',\n    'trytr': '',\n    'jlhj': '',\n    'Job': 'Revenue',\n    'abc123': 1000.00,\n    'hey098': 2000.00\n}\ndf = pd.DataFrame(data.items(),\n    columns=['Unnamed: 0', 'Unnamed: 1'])\nheader = list(df)\nprint(header)\nprint('')\nprint(df)\n\nstartRow = 5\n\nfor row in range(startRow):\n    df.drop(row, inplace=True)\ndf = df.reset_index(drop=True)\nheader = list(df)\nprint(header)\nprint('')\nprint(df)\n\n"
"I am running the following code:\nimport numpy as np\nimport pandas as pd\n\ndfTestExample = pd.DataFrame(np.random.randint(0,100,size=(1000, 4)), columns=list('ABCD'))\n\ndfTestExample = dfTestExample.sort_values([&quot;A&quot;, &quot;B&quot;], ascending = [True, False])\n\ndfTestExample.head(10)\n\nwhich produces\n    A   B   C   D\n303 0   84  13  96\n728 0   43  48  32\n558 0   35  49  49\n286 0   34  17  4\n652 0   29  53  4\n292 0   18  62  29\n139 0   17  63  99\n718 1   91  6   48\n611 1   83  19  75\n208 1   80  35  73\n\ndfTestExample.A.is_monotonic\nTrue\ndfTestExample.B.is_monotonic\nFalse\n\nHow do I check if the column B is also montonic for all values of A?\n"
'Is there a way to use awk or python to check patterns, in our group we have 120,000 lines messages but what to know who replies after who the most;\nConsider the following lines;\n\nAA\nBB\nAA\nCC\nAA\nBB\nAA\nAA\nAA\nBB\nCC\nAA\n\nI would want the result to show\n\nAA-&gt;BB = 3\nBB-&gt;AA = 2\nAA-&gt;CC = 1\nCC-&gt;AA = 2\nAA-&gt;CC = 1\nBB-&gt;CC = 1\nCC-&gt;BB = 0\n\n'
'I am currently exploring a dataset in CSV format having values like the following:\n\nexample 1, class 1\nexample 2, class 1, class 2\nexample 3, class 2,\nexample 4, class 1, class 2, class 4\n\n\nThe classes are assigned in variable length to each example as you can see. Is there any method (using numpy or pandas) that can help me transform this data to one class per instances? Just like the following:\n\nexample 1, class 1\nexample 2, class 1\nexample 2, class 2\nexample 3, class 2\nexample 4, class 1\nexample 4, class 2\nexample 4, class 4\n\n\nI am doing this so that it can be fed to Neural Network models easily. I have tried several ways in pandas but so far no luck. \n'
'I want to launch mask-rcnn model from visualize_cv2.py. My goal is train only on 1 element from class_names - person. For this I create class_names1(added full code from this python file for better understanding):  \n\nimport cv2\nimport numpy as np\nimport os\nimport sys\nimport coco\nimport utils\nimport model as modellib\n\nROOT_DIR = os.getcwd()\nMODEL_DIR = os.path.join(ROOT_DIR, "logs")\nCOCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")\nif not os.path.exists(COCO_MODEL_PATH):\n    utils.download_trained_weights(COCO_MODEL_PATH)\n\n\nclass InferenceConfig(coco.CocoConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\n\nconfig = InferenceConfig()\nconfig.display()\n\nmodel = modellib.MaskRCNN(\n    mode="inference", model_dir=MODEL_DIR, config=config\n)\nmodel.load_weights(COCO_MODEL_PATH, by_name=True)\nclass_names = [\n        \'BG\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\',\n        \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\',\n        \'fire hydrant\', \'stop sign\', \'parking meter\', \'bench\', \'bird\',\n        \'cat\', \'dog\', \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\',\n        \'zebra\', \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n        \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n        \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n        \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n        \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n        \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n        \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n        \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n        \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\',\n        \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n        \'teddy bear\', \'hair drier\', \'toothbrush\'\n    ]\nclass_names1 = class_names[1]\n\ndef random_colors(N):\n    np.random.seed(1)\n    colors = [tuple(255 * np.random.rand(3)) for _ in range(N)]\n    return colors\n\n\ncolors = random_colors(len(class_names))\nclass_dict = {\n    name: color for name, color in zip(class_names, colors)\n}\n\n\ndef apply_mask(image, mask, color, alpha=0.5):\n    """apply mask to image"""\n    for n, c in enumerate(color):\n        image[:, :, n] = np.where(\n            mask == 1,\n            image[:, :, n] * (1 - alpha) + alpha * c,\n            image[:, :, n]\n        )\n    return image\n\n\ndef display_instances(image, boxes, masks, ids, names, scores):\n    """\n        take the image and results and apply the mask, box, and Label\n    """\n    n_instances = boxes.shape[0]\n\nif not n_instances:\n    print(\'NO INSTANCES TO DISPLAY\')\nelse:\n    assert boxes.shape[0] == masks.shape[-1] == ids.shape[0]\n\nfor i in range(n_instances):\n    if not np.any(boxes[i]):\n        continue\n\n    y1, x1, y2, x2 = boxes[i]\n    label = names[ids[i]]\n    color = class_dict[label]\n    score = scores[i] if scores is not None else None\n    caption = \'{} {:.2f}\'.format(label, score) if score else label\n    mask = masks[:, :, i]\n\n    image = apply_mask(image, mask, color)\n    image = cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n    image = cv2.putText(\n        image, caption, (x1, y1), cv2.FONT_HERSHEY_COMPLEX, 0.7, color, 2\n    )\n\nreturn image\n\n\n if __name__ == __main__:\n\n\ncapture = cv2.VideoCapture(0)\n\n\n# these 2 lines can be removed if you dont have a 1080p camera.\n#capture.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n#capture.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n\nwhile True:\n    ret, frame = capture.read()\n    results = model.detect([frame], verbose=0)\n    r = results[0]\n    frame = display_instances(\n        frame, r[\'rois\'], r[\'masks\'], r[\'class_ids\'], class_names, r[\'scores\']\n    )\n    cv2.imshow(\'frame\', frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\'q\'):\n        break\n\ncapture.release()\ncv2.destroyAllWindows()\n\n\nBut if I run it, I get an error:\n\nTraceback (most recent call last):\n\nFile "visualize_cv2.py", line 86, in display_instances\nlabel = names[ids[i]]\nIndexError: string index out of range\n\n\nAs I thinking, I need to change this line(86) to something. But don\'t understand how (I`m newbie in python).\n'
'I would like to text mine an excel file. First I must concatenate all rows into one large text file. Then, scan the text for words in a dictionary. If the word is found, count it as the dictionary key name. Finally return the list of counted words in a relational table [word, count].\nI can count the words, but am unable to get the dictionary part to work. \nMy question is:\n\n\nam I going about this the right way?\nis it even possible, and how so?\n\n\ntweaked code from the internet\n\n\nimport collections\nimport re\nimport matplotlib.pyplot as plt\nimport pandas as pd\n#% matplotlib inline\n#file = open(\'PrideAndPrejudice.txt\', \'r\')\n#file = file.read()\n\n\'\'\' Convert excel column/ rows into a string of words\'\'\'\n#text_all = pd.read_excel(\'C:\\Python_Projects\\Rake\\data_file.xlsx\')\n#df=pd.DataFrame(text_all)\n#case_words= df[\'case_text\']\n#print(case_words)\n#case_concat= case_words.str.cat(sep=\' \')\n#print (case_concat)\ntext_all = ("Billy was glad to see jack. Jack was estatic to play with Billy. Jack and Billy were lonely without eachother. Jack is tall and Billy is clever.")\n\'\'\' done\'\'\'\nimport collections\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#% matplotlib inline\n# Read input file, note the encoding is specified here \n# It may be different in your text file\n\n# Startwords\nstartwords = {\'happy\':\'glad\',\'sad\': \'lonely\',\'big\': \'tall\', \'smart\': \'clever\'}\n#startwords = startwords.union(set([\'happy\',\'sad\',\'big\',\'smart\']))\n\n# Instantiate a dictionary, and for every word in the file, \n# Add to the dictionary if it doesn\'t exist. If it does, increase the count.\nwordcount = {}\n# To eliminate duplicates, remember to split by punctuation, and use case demiliters.\nfor word in text_all.lower().split():\n    word = word.replace(".","")\n    word = word.replace(",","")\n    word = word.replace(":","")\n    word = word.replace("\\"","")\n    word = word.replace("!","")\n    word = word.replace("â€œ","")\n    word = word.replace("â€˜","")\n    word = word.replace("*","")\n    if word  in startwords:\n        if word  in wordcount:\n            wordcount[word] = 1\n        else:\n            wordcount[word] += 1\n# Print most common word\nn_print = int(input("How many most common words to print: "))\nprint("\\nOK. The {} most common words are as follows\\n".format(n_print))\nword_counter = collections.Counter(wordcount)\nfor word, count in word_counter.most_common(n_print):\n    print(word, ": ", count)\n# Close the file\n#file.close()\n# Create a data frame of the most common words \n# Draw a bar chart\nlst = word_counter.most_common(n_print)\ndf = pd.DataFrame(lst, columns = [\'Word\', \'Count\'])\ndf.plot.bar(x=\'Word\',y=\'Count\')\n\n\nError: Empty \'DataFrame\': no numeric data to plot\n\nExpected output:\n\n\nhappy    1\nsad      1\nbig      1\nsmart    1\n\n'
'I am new to Neural Networks and went through the MNIST example for beginners.\n\nI am currently trying to use this example on another dataset from Kaggle that does not have test labels.\n\nIf I run the model on the test data set without corresponding labels and therefore unable to compute the accuracy like in the MNIST example, I would like to be able to see the predictions. Is it possible to access observations and their predicted labels somehow and print them out nicely?\n'
"I'm unable to find out the mistake here in indexing. I am sure this must be some silly mistake. I want to set 'td' values of those rows to 0 whose 'block' size is 1. I'm first finding out such rows and then using those indices to set the values of the column 'td' to 0.\nHere is the sample data set. Here, except block no 5,7,8 all the other block values should be set to 0 in 'td' column.\n\n\n    Sid     Itemid  Block       td\n0     1  214536502      1  180.591\n1     1  214536500      2    37.13\n2     1  214536506      3  133.308\n3     1  214577561      4      NaN\n4     2  214662742      5   41.759\n5     2  214662742      5   78.073\n6     3  214576500      6      NaN\n7     4  214821275      7   26.002\n8     4  214821275      7   28.199\n9     5  214821371      8   42.289\n10    5  214821371      8   45.193\n\n\nHere is my code. I'm getting unexpected output.   \n\nj=k.groupby('Block').Sid.count()==1\nte=k['Block'][j[j].index].index\nk['td'][te]=0\n\n\nExpected Output-\n\n\n    Sid     Itemid  Block       td\n0     1  214536502      1       0\n1     1  214536500      2       0\n2     1  214536506      3       0\n3     1  214577561      4       0\n4     2  214662742      5   41.759\n5     2  214662742      5   78.073\n6     3  214576500      6       0\n7     4  214821275      7   26.002\n8     4  214821275      7   28.199\n9     5  214821371      8   42.289\n10    5  214821371      8   45.193\n\n"
"I'm trying to implement Naive Bayes Classifier in python for the last few days with the iris data set from UCI (http://archive.ics.uci.edu/ml/datasets/Iris). When trying to classify 100 random samples i get only 30-40% accuracy. I think my probability function is right because I tested it with the example from wikipedia (https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Examples) \n\nNow here's what I do:\n\n\nI load the data\nI divide the data into 3 classes\nI calculate the mean and variance for each class\n\n\nThen for 100 random samples I:\n\n\ncalculate the probability for each feature to belong to a class\nCalculate the posterior numerator by multiplying each probability for that class\nStore the values in a list and get the index of the highest value\ncompare the highest value index to the real index (Check if prediction is right)\n\n\nAnd somehow I only get 30-40%, am I doing something wrong?\n\nIf you want to see the code, it's here: http://pastebin.com/sUYm97qi\n"
'I\'m working with a large data frame and I\'m struggling to find an efficient way to eliminate specific dates. Note that I\'m trying to eliminate any measurements from a specific date. \n\nPandas has this great function, where you can call: \n\ndf.ix[\'2016-04-22\'] \n\n\nand pull all rows from that day. But what if I want to eliminate all rows from \'2016-04-22\'? \n\nI want a function like this:\n\ndf.ix[~\'2016-04-22\']\n\n\n(but that doesn\'t work)\n\nAlso, what if I want to eliminate a list of dates? \n\nRight now, I have the following solution:\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import random\n\n###Create a sample data frame\n\ndates = [pd.Timestamp(\'2016-04-25 06:48:33\'), pd.Timestamp(\'2016-04-27 15:33:23\'), pd.Timestamp(\'2016-04-23 11:23:41\'), pd.Timestamp(\'2016-04-28    12:08:20\'), pd.Timestamp(\'2016-04-21 15:03:49\'), pd.Timestamp(\'2016-04-23 08:13:42\'), pd.Timestamp(\'2016-04-27 21:18:22\'), pd.Timestamp(\'2016-04-27 18:08:23\'), pd.Timestamp(\'2016-04-27 20:48:22\'), pd.Timestamp(\'2016-04-23 14:08:41\'), pd.Timestamp(\'2016-04-27 02:53:26\'), pd.Timestamp(\'2016-04-25 21:48:31\'), pd.Timestamp(\'2016-04-22 12:13:47\'), pd.Timestamp(\'2016-04-27 01:58:26\'), pd.Timestamp(\'2016-04-24 11:48:37\'), pd.Timestamp(\'2016-04-22 08:38:46\'), pd.Timestamp(\'2016-04-26 13:58:28\'), pd.Timestamp(\'2016-04-24 15:23:36\'), pd.Timestamp(\'2016-04-22 07:53:46\'), pd.Timestamp(\'2016-04-27 23:13:22\')]\n\nvalues = random.normal(20, 20, 20)\n\ndf = pd.DataFrame(index=dates, data=values, columns [\'values\']).sort_index()\n\n### This is the list of dates I want to remove\n\nremovelist = [\'2016-04-22\', \'2016-04-24\']\n\n\nThis for loop basically grabs the index for the dates I want to remove, then eliminates it from the index of the main dataframe, then positively selects the remaining dates (ie: the good dates) from the dataframe.\n\nfor r in removelist:\n    elimlist = df.ix[r].index.tolist()\n    ind = df.index.tolist()\n    culind = [i for i in ind if i not in elimlist]\n    df = df.ix[culind]\n\n\nIs there anything better out there? \n\nI\'ve also tried indexing by the rounded date+1 day, so something like this:\n\ndf[~((df[\'Timestamp\'] &lt; r+pd.Timedelta("1 day")) &amp; (df[\'Timestamp\'] &gt; r))]\n\n\nBut this gets really cumbersome and (at the end of the day) I\'ll still be using a for loop when I need to eliminate n specific dates. \n\nThere\'s got to be a better way! Right? Maybe?  \n'
'For grid search is always time consuming, so I want to see how much it run now. For example ,it might output\n\nparamsXXX processed\nparamsYYY processed\n...\n\n'
"Given I have a DataFrame with a column that contains lists of strings, like this:\n\n    Name    Fruit\n0   Curly   [Apple]\n1   Moe     [Orange]\n2   Larry   [Apple, Banana]\n\n\nHow would I turn that into something like this?\n\n    Name     Fruit_Apple   Fruit_Orange   Fruit_Banana\n0   Curly              1              0              0\n1   Moe                0              1              0\n2   Larry              1              0              1\n\n\nI have a feeling I'd somehow use pandas.get_dummies() but I can't seem to get it. Any help?\n"
"Suppose I have a set of data frames\n\ndf1 is \n\n   ID       C1\n0  0  0.000000\n1  1  0.538516\n2  2  0.509902\n3  3  0.648074\n4  4  0.141421\n\n\ndf2 is \n\n  ID        C1\n0  0  0.538516\n1  1  0.000000\n2  2  0.300000\n3  3  0.331662\n4  4  0.608276\n\n\nand df3 is \n\n  ID        C1\n0  0  0.509902\n1  1  0.300000\n2  2  0.000000\n3  3  0.244949\n4  4  0.509902\n\n\nI then go ahead and transpose these three data frames.\n\ndf1 = df1.T\ndf2 = df2.T\ndf3 = df3.T\n\n\nNow the data frames are : \n\ndf1 is \n\n          0         1         2         3         4\nID        0         1         2         3         4\nC1        0  0.538516  0.509902  0.648074  0.141421\n\n\ndf2 is :\n\n                 0         1    2         3         4\nID               0         1    2         3         4\nC1               0.538516  0  0.3  0.331662  0.608276\n\n\nand df3 is :\n\n                 0    1    2         3         4\nID               0    1    2         3         4\nC1          0.509902  0.3  0  0.244949  0.509902\n\n\nCan I somehow combine all data frames to have\n\n0            1         2         3         4\n0          0.538516  0.509902  0.648074  0.141421\n0.538516     0        0.3       0.331662  0.608276\n0.509902     0.3      0         0.244949  0.509902\n\n\nAnd then sort rows individually , so that each row in the resulting data frame is sorted ?\n\nFor instance the data frame with sorted rows would be \n\n0  0.141421  0.509902  0.538516  0.648074\n0  0.3       0.331662  0.538516  0.608276\n0  0.244949  0.3       0.509902  0.509902\n\n\nI'm having problems with concat since I've transposed the data frames.\n\nAll help is appreciated \n"
'I\'m trying to understand how can I address columns after using get_dummies.\nFor example, let\'s say I have three categorical variables.\nfirst variable has 2 levels.\nsecond variable has 5 levels.\nthird variable has 2 levels.\n\ndf=pd.DataFrame({"a":["Yes","Yes","No","No","No","Yes","Yes"], "b":["a","b","c","d","e","a","c"],"c":["1","2","2","1","2","1","1"]})\n\n\nI created dummies for all three variable in order to use them in sklearn regression in python. \n\ndf1 = pd.get_dummies(df,drop_first=True)\n\n\nNow I want to create two interactions (multiplication): bc , ba\n\nhow can I create the multiplication between each dummies variable to another one without using their specific names like that: \n\ndf1[\'a_yes_b\'] = df1[\'a_Yes\']*df1[\'b_b\']\ndf1[\'a_yes_c\'] = df1[\'a_Yes\']*df1[\'b_c\']\ndf1[\'a_yes_d\'] = df1[\'a_Yes\']*df1[\'b_d\']\ndf1[\'a_yes_e\'] = df1[\'a_Yes\']*df1[\'b_e\']\n\ndf1[\'c_2_b\'] = df1[\'c_2\']*df1[\'b_b\']\ndf1[\'c_2_c\'] = df1[\'c_2\']*df1[\'b_c\']\ndf1[\'c_2_d\'] = df1[\'c_2\']*df1[\'b_d\']\ndf1[\'c_2_e\'] = df1[\'c_2\']*df1[\'b_e\']\n\n\nThanks.\n'
'I have a pandas df like the following:\n\nUser    Purchase_Count    Location_Count\n1       2                 3\n2       10                5\n3       5                 1\n4       20                4\n5       2                 3\n6       2                 3\n7       10                5\n\n\nHow would I add a column that calculates the % of the coordinate pair (Purchse_Count[i], Location_Count[i]) of the total entries.\nso for example I would like the df to look like:\n\nUser    Purchase_Count    Location_Count    %\n1       2                 3                 42.85\n2       10                5                 28.57\n3       5                 1                 14.28\n4       20                4                 14.28\n5       2                 3                 42.85\n6       2                 3                 42.85\n7       10                5                 28.57\n\n'
"I group a pandas dataframe using groupby() function with multiple columns.\n\ndf_tr_mod = df_tr.groupby(['Col1','Col2']).aCol.agg(['count'])\n\n\nNow I want to access this count values (I want to multiply this all count values by 10)\nHow i can do this?\n"
'I have CSV data in the following format:\n\n+-----------------+--------+-------------+\n| reservation_num |  rate  | guest_name  |\n+-----------------+--------+-------------+\n| B874576         | 169.95 | Bob Smith   |\n| H786234         | 258.95 | Jane Doe    |\n| H786234         | 258.95 | John Doe    |\n| F987354         | 385.95 | David Jones |\n| N097897         | 449.95 | Mark Davis  |\n| H567349         | 482.95 | Larry Stein |\n| N097897         | 449.95 | Sue Miller  |\n+-----------------+--------+-------------+\n\n\nI would like to add a feature (column) to the DataFrame called \'rate_per_person\'. It would be calculated by taking the rate for a particular reservation number and dividing it by the total number of guests who have that same reservation number associated with their stay. \n\nHere is my code:\n\n#Importing Libraries\nimport pandas as pd\n\n# Importing the Dataset\nds = pd.read_csv(\'hotels.csv\')\n\nfor index, row in ds.iterrows():\n    row[\'rate_per_person\'] = row[\'rate\'] / ds[row[\'reservation_num\']].count\n\n\nAnd the error message:\n\nTraceback (most recent call last):\n\n  File "&lt;ipython-input-3-0668a3165e76&gt;", line 2, in &lt;module&gt;\n    row[\'rate_per_person\'] = row[\'rate\'] / ds[row[\'reservation_num\']].count\n\n  File "/Users/&lt;user_name&gt;/anaconda/lib/python3.6/site-packages/pandas/core/frame.py", line 2062, in __getitem__\n    return self._getitem_column(key)\n\n  File "/Users/&lt;user_name&gt;/anaconda/lib/python3.6/site-packages/pandas/core/frame.py", line 2069, in _getitem_column\n    return self._get_item_cache(key)\n\n  File "/Users/&lt;user_name&gt;/anaconda/lib/python3.6/site-packages/pandas/core/generic.py", line 1534, in _get_item_cache\n    values = self._data.get(item)\n\n  File "/Users/&lt;user_name&gt;/anaconda/lib/python3.6/site-packages/pandas/core/internals.py", line 3590, in get\n    loc = self.items.get_loc(item)\n\n  File "/Users/&lt;user_name&gt;/anaconda/lib/python3.6/site-packages/pandas/core/indexes/base.py", line 2395, in get_loc\n    return self._engine.get_loc(self._maybe_cast_indexer(key))\n\n  File "pandas/_libs/index.pyx", line 132, in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)\n\n  File "pandas/_libs/index.pyx", line 154, in pandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\n\n  File "pandas/_libs/hashtable_class_helper.pxi", line 1207, in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20405)\n\n  File "pandas/_libs/hashtable_class_helper.pxi", line 1215, in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas/_libs/hashtable.c:20359)\n\nKeyError: \'B874576\'\n\n\nBased on the error message, clearly there is an issue with the ds[row[\'reservation_num\']].count portion of the last line of code. However, I am unsure the right way to obtain the number of guests per reservation in a manner that will allow me to programmatically create the new feature. \n'
'I\'m have an excel document formatted like so (Columns are datasets, Rows are cell types, values are comma-delineated gene names)\n\n\nI would like to reformat the sheet like so (Columns are still datasets, but Rows are now gene names, and values are cell types):\n\n\n\nI was trying to do this in pandas. I imported the input as a dataFrame and called it \'test\'. My logic was to loop over each column, and within loop over each row, take the comma-delineated values, split those, and then make each of those a new index.\n\nThis approach is obviously pretty inefficient, but I\'m not even able to get it to work as expected yet (even though I\'m not getting an error, just no output, Edit Note: \'blank\' is the name of a new, blank Excel book)\n\nUnworking attempt:\n\nfor dataSet in test.columns:\n    for index, rows in test.iterrows():\n        geneList = test[dataSet].loc[index].split(",")\n        for gene in geneList:\n            blank[dataSet].reindex([gene])\n\n\nSo two questions:\n1. How can I get this example to work?\n2. How can I accomplish this transformation more efficiently?\n\nThanks!\n'
'I\'m looking for help with the Pandas .corr() method.\n\nAs is, I can use the .corr() method to calculate a heatmap of every possible combination of columns:\n\ncorr = data.corr()\nsns.heatmap(corr)\n\n\nWhich, on my dataframe of 23,000 columns, may terminate near the heat death of the universe.\n\nI can also do the more reasonable correlation between a subset of values\n\ndata2 = data[list_of_column_names]\ncorr = data2.corr(method="pearson")\nsns.heatmap(corr)\n\n\nThat gives me something that I can use--here\'s an example of what that looks like: \n\n\nWhat I would like to do is compare a list of 20 columns with the whole dataset. The normal .corr() function can give me a 20x20 or 23,000x23,000 heatmap, but essentially I would like a 20x23,000 heatmap.\n\nHow can I add more specificity to my correlations?\n\nThanks for the help! \n'
"I have a 2-column DataFrame, column-1 corresponds to customer, column-2 corresponds to the city this customer has visited. The DataFrame looks like the following:\n\nprint(df)\n\n    customer    visited_city\n0   John        London\n1   Mary        Melbourne\n2   Steve       Paris\n3   John        New_York\n4   Peter       New_York\n5   Mary        London\n6   John        Melbourne\n7   John        New_York\n\n\nI would like to convert the above DataFrame into a row-vector format, such that each row represents a unique user with the row vector indicating the cities visited.\n\nprint(wide_format_df)\n\n          London  Melbourne  New_York  Paris\nJohn      1.0        1.0       1.0      0.0\nMary      1.0        1.0       0.0      0.0\nSteve     0.0        0.0       0.0      1.0\nPeter     0.0        0.0       1.0      0.0\n\n\nBelow is the code I used to generate the wide format. It iterates through each user one by one. I was wondering is there any more efficient way to do so?\n\nimport pandas as pd\nimport numpy as np\n\nUNIQUE_CITIESS = np.sort(df['visited_city'].unique())\np = len(UNIQUE_CITIESS)\nunique_customers = df['customer'].unique().tolist()\n\nX = []\nfor customer in unique_customers:\n    x = np.zeros(p)    \n    city_visited = np.sort(df[df['customer'] == customer]['visited_city'].unique())\n    visited_idx = np.searchsorted(UNIQUE_CITIESS, city_visited)\n    x[visited_idx] = 1    \n    X.append(x)\nwide_format_df = pd.DataFrame(np.array(X), columns=UNIQUE_CITIESS, index=unique_customers)\nwide_format_df\n\n"
'I have two pandas dataframes (df_1, df_2) with the same columns, but in one dataframe (df_1) some values of one column are missing. So I want to fill in those missing values from df_2, but only when the the values of two columns match.\n\nHere is a little example what my data looks like:\n\ndf_1: \n\ndf_2: \n\nI tried to add the missing values with:\n\ndf_1.update(df_2, overwrite=False)\n\n\nBut the problem is, that it will fill in the values, even when just one column matches. I want to fill in the value when the columns "housenumber" AND "street" matches.\n'
'How to read this PGM image in python? please help \n\nPGM image link\n\nI tried this \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef readpgm(name):\n    with open(name) as f:\n         lines = f.readlines()\n\n    # Ignores commented lines\n    for l in list(lines):\n            if l[0] == \'#\':\n            lines.remove(l)\n\n    # Makes sure it is ASCII format (P2)\n    assert lines[0].strip() == \'P2\' \n\n    # Converts data to a list of integers\n    data = []\n    for line in lines[1:]:\n        data.extend([int(c) for c in line.split()])\n\n    return (np.array(data[3:]),(data[1],data[0]),data[2])\n\ndata = readpgm(\'514516.pgm\')\n\nplt.imshow(np.reshape(data[0],data[1])) # Usage example\n\n\nit gives me this error: \n\n"return codecs.charmap_decode(input,self.errors,decoding_table)[0]\n\n UnicodeDecodeError: \'charmap\' codec can\'t decode byte 0x90 in position \n 2075: character maps to &lt;undefined&gt;"\n\n\nwhat should i do? please help\n\nSolution: \n\nNumpy and 16-bit PGM\n'
"I have online user data with the following information, \n\ndf.head()\n\nUSER Timestamp  day_of_week Busi_days   Busi_hours\nAAD 2017-07-11 09:31:44 TRUE    TRUE\nAAD 2017-07-11 23:24:43 TRUE    FALSE\nAAD 2017-07-12 13:24:43 TRUE    TRUE\nSAP 2017-07-23 14:24:34 FALSE   FALSE\nSAP 2017-07-24 16:58:49 TRUE    TRUE\nYAS 2017-07-31 21:10:35 TRUE    FALSE\n\n\nI wanted to compute the activity of USER column and create three new columns namely: 1. Activity: With the information based on how active the user is, meaning if the same user clicked more than twice then call it TRUE else false. 2.Multiple_days: If the user clicked the website on the more one day, if the same user clicked more than 2 days call the column TRUE else FALSE. 3. Busniess_days:  whether the user clicked on the weekdays, if the user clicked the website on businesses days within business hours then call it True else FALSE\n\nI have the following script performing the above-mentioned task, but its really slow for my huge data frame my data frame is 117Mb in size. Any better solutions would be great\n\nMy try:\n\ndf.Timestamp = pd.to_datetime(df.Timestamp)\ndf['date'] = [x.date() for x in df.Timestamp]\ntarget_df = pd.DataFrame()\ntarget_df['USER'] = df.USER.unique()\na = df.groupby(['USER', 'date']).size()\na = a[a&gt;1]\nUID=pd.DataFrame(a).reset_index().USER.values\n\ntarget_df['Active'] = [True if x in UID else False for x in target_df.USER.values]\na = df.groupby('USER')['Timestamp'].nunique()\na = a[a&gt;1]\nUUID2=pd.DataFrame(a).reset_index().USER.values \ntarget_df['Multiple_days'] = [True if x in UUID2 else False for x in target_df.USER.values]\n\na = df[(df.Busi_days==True)&amp;(df.Busi_hours==True)].USER.unique()\n\ntarget_df['Busi_weekday'] = [True if x in a else False for x in target_df.USER.values]\n\ntarget_df.head()\n\n\nUSER Active  Multiple_days   Busi_weekday\nAAD TRUE    TRUE    TRUE\nSAP FALSE   TRUE    FALSE\nYAS FALSE   FALSE   FALSE\n\n"
'I have a dataframe in Pandas with collected data;\n\nimport pandas as pd\ndf = pd.DataFrame({\'Group\': [\'A\',\'A\',\'A\',\'A\',\'A\',\'A\',\'A\',\'B\',\'B\',\'B\',\'B\',\'B\',\'B\',\'B\'], \'Subgroup\': [\'Blue\', \'Blue\',\'Blue\',\'Red\',\'Red\',\'Red\',\'Red\',\'Blue\',\'Blue\',\'Blue\',\'Blue\',\'Red\',\'Red\',\'Red\'],\'Obs\':[1,2,4,1,2,3,4,1,2,3,6,1,2,3]})\n\n+-------+----------+-----+\n| Group | Subgroup | Obs |\n+-------+----------+-----+\n| A     | Blue     |   1 |\n| A     | Blue     |   2 |\n| A     | Blue     |   4 |\n| A     | Red      |   1 |\n| A     | Red      |   2 |\n| A     | Red      |   3 |\n| A     | Red      |   4 |\n| B     | Blue     |   1 |\n| B     | Blue     |   2 |\n| B     | Blue     |   3 |\n| B     | Blue     |   6 |\n| B     | Red      |   1 |\n| B     | Red      |   2 |\n| B     | Red      |   3 |\n+-------+----------+-----+\n\n\nThe Observations (\'Obs\') are supposed to be numbered without gaps, but you can see we have \'missed\' Blue 3 in group A and Blue 4 and 5 in group B. The desired outcome is a percentage of all \'missed\' Observations (\'Obs\') per group, so in the example:\n\n+-------+--------------------+--------+--------+\n| Group | Total Observations | Missed |   %    |\n+-------+--------------------+--------+--------+\n| A     |                  8 |      1 | 12.5%  |\n| B     |                  9 |      2 | 22.22% |\n+-------+--------------------+--------+--------+\n\n\nI tried both with for loops and by using groups (for example: \n\ndf.groupby([\'Group\',\'Subgroup\']).sum()\nprint(groups.head)\n\n\n) but I can\'t seem to get that to work in any way I try. Am I going about this the wrong way?\n\nFrom another answer (big shoutout to @Lie Ryan) I found a function to look for missing elements, however I don\'t quite understand how to implement this yet;\n\ndef window(seq, n=2):\n    "Returns a sliding window (of width n) over data from the iterable"\n    "   s -&gt; (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   "\n    it = iter(seq)\n    result = tuple(islice(it, n))\n    if len(result) == n:\n        yield result\n    for elem in it:\n        result = result[1:] + (elem,)\n        yield result\n\ndef missing_elements(L):\n    missing = chain.from_iterable(range(x + 1, y) for x, y in window(L) if (y - x) &gt; 1)\n    return list(missing)\n\n\nCan anyone give me a pointer is the right direction?\n'
'enter image description hereTarget:\nI have a pandas data frame, as shown below, with multiple of the column and would like to get the subtotal for few columns and write "Total" at Placement# Name.\n\nDataFrame:\nenter image description here\n\nMy Attempt:\n\n**adsize_sales_second_table.loc["Grand Total"] = pd.Series(adsize_sales_second_table.loc\n                                                             [:, ["Delivered Impressions",\n                                                                  "Clicks",\n                                                                  "Conversion", "Spend"]].sum(),\n                                                    index=["Delivered Impressions", "Clicks", "Conversion", "Spend"]**\n                                                             )\n\n\nThis is adding a row at last and can\'t be figuring out to fill subtotal:\nenter image description here\n\nExpected Output:\nI\'d have expected the output to be as followed\nenter image description here\n\nSee edited code:\n\n  adsize_sales_data = adsize_sales_second_table.loc[:, ["Placement# Name", "Adsize", "Delivered Impressions", "Clicks",\n                                                              "CTR", "Conversion", "Conversion Rate", "Spend", "eCPA"]]\n\n\n        cols = ["Delivered Impressions", "Clicks", "Conversion", "Spend"]\n\n        adsize_sales_data[\'Placement# Name\'] = adsize_sales_data[\'Placement# Name\'].ffill()\n        grand = adsize_sales_data[cols].sum()\n        grand.loc[\'Placement# Name\'] = \'Grand total\'\n\n        adsize_sales_data_new = adsize_sales_data.groupby(\'Placement# Name\')[cols].sum()\n\n        adsize_sales_data_new.index = adsize_sales_data.index.astype(str)+\'____\'\n\n        adsize_sales_data = (pd.concat([adsize_sales_data.set_index(\'Placement# Name\'), adsize_sales_data_new], keys=(\'a\', \'b\')).sort_index(level=1).reset_index())\n\n        adsize_sales_data[\'Placement# Name\'] = np.where(adsize_sales_data [\'level_0\'] == \'a\', adsize_sales_data[\'Placement# Name\'], \'Total\')\n\n        adsize_sales_data = adsize_sales_data.drop(\'level_0\', axis=1)\n\n        adsize_sales_data.loc[len(adsize_sales_data.index)] = grand\n\n        print (adsize_sales_data)\n\n\nit is now giving values error.enter image description here\n'
'I\'m preprocessing data and one column represents dates such as \'6/1/51\'\n\nI\'m trying to convert the string to a date object and so far what I have is:\n\n    date = row[2].strip()\n    format = "%m/%d/%y"\n    datetime_object = datetime.strptime(date, format)\n    date_object = datetime_object.date()\n    print(date_object)\n    print(type(date_object))\n\n\n\n\nThe problem I\'m facing is changing 2051 to 1951.\n\nI tried writing      \n\n    format = "%m/%d/19%y"\n\n\nBut it gives me a ValueError.\n\n    ValueError: time data \'6/1/51\' does not match format \'%m/%d/19%y\'\n\n\nI couldn\'t easily find the answer online so I\'m asking here. Can anyone please help me with this? \n\nThanks.\n'
'\nI am working with two csv files and imported as dataframe, df1 and df2 \ndf1 has 50000 rows and df2 has 150000 rows.\nI want to compare (iterate through each row) the \'time\' of df2 with\ndf1, find the difference in time and return the values of all column\ncorresponding to similar row, save it in df3 (time synchronization)\nFor example, 35427949712 (of \'time\' in df1) is nearest or equal to\n35427949712 (of \'time\' in df2), So I would like to return the\ncontents to df1 (\'velocity_x\' and \'yaw\') and df2 (\'velocity\' and\n\'yawrate\') and save in df3\nFor this i used two techniques, shown in code. \nCode 1 takes very long time to execute 72 hours which is not practice since i have lot of csv files\nCode 2 gives me "memory error" and kernel dies.\n\n\nWould be great, if I get a more robust solution for the problem considering computational time, memory and power(Intel Core i7-6700HQ, 8 GB Ram)\n\nHere is the sample data,\n\nimport pandas as pd\ndf1 = pd.DataFrame({\'time\': [35427889701, 35427909854, 35427929709,35427949712, 35428009860], \n                    \'velocity_x\':[12.5451, 12.5401,12.5351,12.5401,12.5251],\n                   \'yaw\' : [-0.0787806, -0.0784749, -0.0794889,-0.0795915,-0.0795472]})\n\ndf2 = pd.DataFrame({\'time\': [35427929709, 35427949712, 35427009860,35427029728, 35427049705], \n                    \'velocity\':[12.6583, 12.6556,12.6556,12.6556,12.6444],\n                    \'yawrate\' : [-0.0750492, -0.0750492, -0.074351,-0.074351,-0.074351]})\n\ndf3 = pd.DataFrame(columns=[\'time\',\'velocity_x\',\'yaw\',\'velocity\',\'yawrate\'])\n\n\nCode1\n\n for index, row in df1.iterrows():\n    min=100000\n    for indexer, rows in df2.iterrows():\n        if abs(float(row[\'time\'])-float(rows[\'time\']))&lt;min:\n            min = abs(float(row[\'time\'])-float(rows[\'time\']))\n            #storing the position \n            pos = indexer\n    df3.loc[index,\'time\'] = df1[\'time\'][pos]\n    df3.loc[index,\'velocity_x\'] = df1[\'velocity_x\'][pos]\n    df3.loc[index,\'yaw\'] = df1[\'yaw\'][pos]\n    df3.loc[index,\'velocity\'] = df2[\'velocity\'][pos]\n    df3.loc[index,\'yawrate\'] = df2[\'yawrate\'][pos]\n\n\nCode2\n\ndf1[\'key\'] = 1\ndf2[\'key\'] = 1\ndf1.rename(index=str, columns ={\'time\' : \'time_x\'}, inplace=True)\n\ndf = df2.merge(df1, on=\'key\', how =\'left\').reset_index()\ndf[\'diff\'] = df.apply(lambda x: abs(x[\'time\']  - x[\'time_x\']), axis=1)\ndf.sort_values(by=[\'time\', \'diff\'], inplace=True)\n\ndf=df.groupby([\'time\']).first().reset_index()[[\'time\', \'velocity_x\', \'yaw\', \'velocity\', \'yawrate\']]\n\n'
'The transform primitive works fine with additional arguments. Here is an example\n\ndef string_count(column, string=None):\n    \'\'\'\n    ..note:: this is a naive implementation used for clarity\n    \'\'\'\n    assert string is not None, "string to count needs to be defined"\n    counts = [str(element).lower().count(string) for element in column]\n    return counts\n\n\ndef string_count_generate_name(self):\n    return u"STRING_COUNT(%s, %s)" % (self.base_features[0].get_name(),\n                                      \'"\' + str(self.kwargs[\'string\'] + \'"\'))\n\n\nStringCount = make_trans_primitive(\n    function=string_count,\n    input_types=[Categorical],\n    return_type=Numeric,\n    cls_attributes={\n        "generate_name": string_count_generate_name\n    })\n\nes = ft.demo.load_mock_customer(return_entityset=True)\ncount_the_feat = StringCount(es[\'transactions\'][\'product_id\'], string="5")\nfm, fd = ft.dfs(\n    entityset=es,\n    target_entity=\'transactions\',\n    max_depth=1,\n    features_only=False,\n    seed_features=[count_the_feat])\n\n\nOutput:\n\n                product_id  STRING_COUNT(product_id, "5")\ntransaction_id                                           \n1                        5                              1\n2                        4                              0\n3                        3                              0\n4                        3                              0\n5                        4                              0\n\n\nHowever, if I modify and make into Aggregation Primitive like so:\n\ndef string_count(column, string=None):\n    \'\'\'\n    ..note:: this is a naive implementation used for clarity\n    \'\'\'\n    assert string is not None, "string to count needs to be defined"\n    counts = [str(element).lower().count(string) for element in column]\n    return sum(counts)\n\n\ndef string_count_generate_name(self):\n    return u"STRING_COUNT(%s, %s)" % (self.base_features[0].get_name(),\n                                      \'"\' + str(self.kwargs[\'string\'] + \'"\'))\n\n\nStringCount = make_agg_primitive(\n    function=string_count,\n    input_types=[Categorical],\n    return_type=Numeric,\n    cls_attributes={\n        "generate_name": string_count_generate_name\n    })\n\nes = ft.demo.load_mock_customer(return_entityset=True)\ncount_the_feat = StringCount(es[\'transactions\'][\'product_id\'], string="5")\n\n\nI get the following error:\n\nTypeError: new_class_init() missing 1 required positional argument: \'parent_entity\'\n\n\nAre custom Aggregation Primitives With Additional Arguments supported in featuretools?\n'
"How would I be able to write a rolling condition apply to a column in pandas?\n\nimport pandas as pd\nimport numpy as np    \n\nlst = np.random.random_integers(low = -10, high = 10, size = 10)\nlst2 = np.random.random_integers(low = -10, high = 10, size = 10)\n\n#lst = [ -2  10 -10  -6   4   2  -5   4   9   3]\n#lst2 = [-7  5  6 -4  7  1 -4 -6 -1 -4]\ndf = pandas.DataFrame({'a' : lst, 'b' : lst2})\n\n\nGiven a dataframe, namely 'df', I want to create a column 'C' such that it will display True if the element in a > 0 and b > 0 or False if a &lt; 0 and b &lt; 0. \n\nFor the rows that do not meet this condition, I want to roll the entry in the previous row to the current one (i.e. if the previous row has value 'True' but does not meet the specified conditions, it should have value 'True'.)\n\nHow can I do this?\n\nFollow-up Question : How would I do this for conditions a > 1 and b > 1 returns True or a &lt; -1 and b &lt; -1 returns False?\n"
"My dataframe looks like this:\n\n+-------+-----------------------------------------+\n| Image | Bounding Boxes                          |\n+-------+-----------------------------------------+\n| a.jpg | xyz 0.1 0.2 0.3 0.4                     |\n| b.jpg | xyz 0.1 0.2 0.3 0.4 ijk 0.4 0.3 0.2 0.1 |\n+-------+-----------------------------------------+\n\n\nThe values for those bounding boxes will always come in groups of five, where the values mean (in order)\n\n\nID of the label represented by the bounding box\nX coordinate of the bounding box (as a percentage of image width)\nY coordinate of the bounding box (as a percentage of image height)\nWidth of the image (as a percentage of whole image)\nHeight of the image (as a percentage of whole image)\n\n\nSince each row will only have at most 5 such pairs (and at least zero), I'd like to transform the dataframe to look as follows:\n\n+-------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+\n| Image | L1  | x1  | y1  | w1  | h1  | L2  | x2  | y2  | w2  | h2  | ... | h5 |\n+-------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+\n| a.jpg | xyz | 0.1 | 0.2 | 0.3 | 0.4 |     |     |     |     |     |     |    |\n| b.jpg | xyz | 0.1 | 0.2 | 0.3 | 0.4 | ijk | 0.4 | 0.3 | 0.2 | 0.1 | ... |    |\n+-------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+----+\n\n\nIn other words, each of the space-separated values that used to be in one column, I'd like them split into a new column.\n\nQuestion\n\nHow can this be done in Pandas?\n"
'Excuse me for my english,\nI\'m trying to recognize properties that come up frequently in a set of data to deduce a categorization using the apyori package of python. i\'m practicing on a dataframe of 20772 transactions and the largest transaction is 543 items.\n\nDataFrame\n\nI converted this DataFrame into a list :\n\nliste = df.astype(str).values.tolist()\n\n\nI got this list\n\nlist\n\nI used the apriori function of the library apyori to generate the association rules:\n\nfrom apyori import apriori\nrules = apriori(liste, min_support= 0.01, min_confidence= 0.2)\n\n\nto display the result I converted the rules variable to a list :\n\nMB = list(rules)\n\n\nThe problem is that instead of showing me the rules but it shows the RelationRecord "RelationRecord object of apyori module".\n\nlike here\n\nresult\n'
'I am working to extract all integer values from a specific column (left, top, length and width) in a csv file with multiple rows and columns. I have used pandas to isolate the columns I am interested in but Im stuck on how to use a specific parts of an array. \n\nLet me explain: I need to use the CSV file\'s column with "left, top, length and width" attributes to then obtain xmin, ymin, xmax and ymax (these are coordinated of boxes in images). Example of a row in this column looks like so:\n\n[{"left":171,"top":0,"width":163,"height":137,"label":"styrofoam container"},{"left":222,"top":42,"width":45,"height":70,"label":"chopstick"}]\n\nAnd I need to extract the 171, 0, 163 and 137 to do the necessary operations for finding my xmax, xmin, ymax and ymin\n\nThe above line is a single row in my pandas array, how do I extract the numbers I need for running my operations?\n\nHere is the code I wrote to extract the column and this is what I have so far:\n\nimport os\nimport csv\nimport pandas\nimport numpy as np\n\ncsvPath = "/path/of/my/csvfile/csvfile.csv"\n\ndata = pandas.read_csv(csvPath)\ncsv_coords = data[\'Answer.annotation_data\'].values #column with the coordinates\nimage_name = data [\'Input.image_url\'].values\nprint csv_coords[2]\n\n'
'I have a pandas dataframe (N = 1485) that looks like this:\n\nID          Intervention\n1           Blood Draw, Flushed, Locked\n1           Blood Draw, Port De-Accessed, Heparin-Locked, Tubing Changed\n1           Blood Draw, Flushed\n2           Blood return Verified, Flushed\n2           Cap Changed\n3           Port De-Accessed\n\n\nI want to be able to dummy code out each of the string before every comma so it looks similar to this:\n\nID          Blood Draw          Flushed          Locked      ....\n1              Yes                Yes             Yes\n1              Yes                No              No\n...\n\n\nThanks!\n'
'Let’s say I have two NumPy arrays, a and b:\n\n\n\na = np.array([\n    [1, 2, 3],\n    [2, 3, 4]\n    ])\n\nb = np.array([8,9])\n\n\nAnd I would like to append the same array b to every row (ie. adding multiple columns) to get an array, c:\n\n\n\nb = np.array([\n    [1, 2, 3, 8, 9],\n    [2, 3, 4, 8, 9]\n    ])\n\n\nHow can I do this easily and efficiently in NumPy?\n\nI am especially concerned about its behaviour with big datasets (where a is much bigger than b), is there any way around creating many copies (ie. a.shape[0]) of b?\n\nRelated to this question, but with multiple values. \n'
'How can I get the right return type for np.nditer? I need to iterate through ax object here:\n\nfig, ax = plt.subplots(figsize=(16,9), ncols=3, nrows=2)\nfor col, elem in zip(df.columns[:-1], np.nditer(ax, flags = [\'refs_ok\'])):\n    sns.countplot(x="CLASS", hue=col, data=df, ax=elem)\n\n\nI know I could have iterated here using the dimension of ax array, but I wonder, if I can make this work. Basically, ax=elem should look like ax=ax[i][j] in iteration. But it turns out they have different types:\n\nprint(type(elem))\nprint(type(ax[0][0]))\n\n\nreturns:\n\n&lt;class \'numpy.ndarray\'&gt;\n&lt;class \'matplotlib.axes._subplots.AxesSubplot\'&gt;\n\n'
"        shares    PC         Day\n0.01    546.68  NaN         Weekend\n0.02    695.34  27.193239   Weekend\n0.03    768.00  10.449564   Weekend\n0.04    809.56  5.411458    Weekend\n0.05    837.45  3.445081    Weekend\n0.06    865.34  3.330348    Weekend\n0.07    893.00  3.196431    Weekend\n0.08    917.00  2.687570    Weekend\n0.90    6200.00 8.771930    Weekday\n0.91    6700.00 8.064516    Weekday\n0.92    7500.00 11.940299   Weekday\n0.93    8300.00 10.666667   Weekday\n0.94    9400.00 13.253012   Weekday\n\n\nI need to create 2 box subplots for 'shares' column from the above dataframe using the categorical variable Day which has 2 values Weekday and Weekend.\n\n1st Sub Plot will be for Shares across Weekdays and the 2nd Sub Plot will be for Shares across Weekends.\n\nHow can it be done ?\n"
'I\'ve two binary lists that I\'m attempting to compare. To compare I sum where each corresponding value is equal and transform this to a percentage :\n\nimport numpy as np\n\nl1 = [1,0,1]\nl2 = [1,1,1]\n\nprint(np.dot(l1 , l2) / len(l1) * 100)\n\n\nprints 66.666\n\nSo in this case l1 and l2 are 61.666 in terms of closeness. As each list is less similar the closeness value decreases.\n\nFor example using values :\n\nl1 = [1,0,1]\nl2 = [0,1,0]\n\n\nreturns 0.0\n\nHow to plot l1 and l2 that describe the relationship between l1 and l2 ? Is there a name for using this method to measure similarity between binary values ?\n\nUsing a scatter :\n\nimport matplotlib.pyplot as plt\n\nplt.scatter( \'x\', \'y\', data=pd.DataFrame({\'x\': l1, \'y\': l2 }))\n\n\nproduces : \n\n\n\nBut this does not make sense ?\n\nUpdate : \n\n"if both entries are 0, this will not contribute to your "similarity"\n\nUsing updated code below in order to compute similarity, this updated similarity measure includes corresponding 0 values in computing final score. \n\nimport numpy as np\n\nl1 = [0,0,0]\nl2 = [0,1,0]\n\nprint(len([a for a in np.isclose(l1 , l2) if(a)]) / len(l1) * 100)\n\n\nwhich returns : \n\n66.66666666666666\n\n\nAlternatively, using below code with measure normalized_mutual_info_score returns 1.0 for lists that are the same or different, therefore normalized_mutual_info_score is not a suitable similarity measure ?\n\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\n\nl1 = [1,0,1]\nl2 = [0,1,0]\n\nprint(normalized_mutual_info_score(l1 , l2))\n\nl1 = [0,0,0]\nl2 = [0,0,0]\n\nprint(normalized_mutual_info_score(l1 , l2))\n\n\nprints : \n\n1.0\n1.0\n\n'
'I am trying to define a function that fits input x and y data of the form:\n\ndef nlvh(x,y, xi, yi, H,C):\n\n    return ((H-xi*C)/8.314)*((1/xi) - x) + (C/8.314)*np.log((1/x)/xi) + np.log(yi)\n\n\nThe x and y data are 1-D numpy arrays of the same length. I would like to slice the data so that I can select the first 5 points of x and y, fit those by optimizing C and H in the model, and then move one point ahead and repeat. I have some code that does this for a linear fit over the same data:\n\nfor i in np.arange(len(x)):\n    xdata = x[i:i + window]\n    ydata = y[i:i+window]\n    a[i], b[i] = np.polyfit(xdata, ydata,1)\n    xdata_avg[i] = np.mean(xdata)\n    if i == (lenx - window):\n        break\n\n\nbut doing the same thing over the equation defined above appears to be a bit more tricky. x and y appear as the independent and dependent variables, but there are also parameters xo and yo which are the first values of x and y in each window. \n\nThe end result I would like are two new arrays with H[i] and C[i], where i designates each subsequent window. Does anybody have some insight as to how I can get started?\n'
'For an user analysis I have a database with events and a timestamp for each.\n\nTimestamp   |   user    |    event code\n13:30:23    |   user1   |    event123\n13:30:45    |   user1   |    event123\n13:30:56    |   user3   |    event123\n13:40:15    |   user2   |    event123\n13:55:20    |   user1   |    event123\n\n\nNow I want to identify "sessions" for each user by annotating the events with a session id.\n\nEach session should be identified by a group of events for an user, that don\'t have a X seconds(or minutes) gap in between them. That means a X second gap after an event closes a session. \nSession  ids don\'t have to start by 0 for each user, but are later on used on a per user basis only. Here X = 5 Minutes is set.\n\nTimestamp   |   user    |    event code    | session_id\n13:30:23    |   user1   |    event123      | 1\n13:30:45    |   user1   |    event123      | 1\n13:30:56    |   user3   |    event123      | 2\n13:40:15    |   user2   |    event123      | 3\n13:55:20    |   user1   |    event123      | 4\n\n\nIs this possible with dataframes?\n'
"I'm struggling to adjust my plot legend after adding the axline/ hline on 100 level in the graph.(screenshot added)\n\nif there's a way to run this correctly so no information will be lost in legend, and maybe add another hline and adding it to the legend.\n\nadding the code here, maybe i'm not writing it properly.\n\nfig, ax1 = plt.subplots(figsize = (9,6),sharex=True)\n\nBundleFc_Outcome['Spend'].plot(kind = 'bar',color = 'blue',width = 0.4, ax = ax1,position = 1)\n#\n# Make the y-axis label, ticks and tick labels match the line color.\nax1.set_ylabel('SPEND', color='b', size = 18)\nax1.set_xlabel('Bundle FC',color='w',size = 18)\n\nax2 = ax1.twinx()\nax2.set_ylabel('ROAS', color='r',size = 18)\nax1.tick_params(axis='x', colors='w',size = 20)\nax2.tick_params(axis = 'y', colors='w',size = 20)\nax1.tick_params(axis = 'y', colors='w',size = 20)\n#ax1.text()\n#\n\nax2.axhline(100)\nBundleFc_Outcome['ROAS'].plot(kind = 'bar',color = 'red',width = 0.4, ax = ax2,position = 0.25)\nplt.grid()\n#ax2.set_ylim(0, 4000)\nax2.set_ylim(0,300)\nplt.title('ROAS &amp; SPEND By Bundle FC',color = 'w',size= 20)\nplt.legend([ax2,ax1],labels = ['SPEND','ROAS'],loc = 0)\n\n\nThe code gives me the following picture:\n\n\n\nAfter implementing the suggestion in the comments, the picture looks like this (does not solve the problem):\n\n\n"
"I am using matplotlib for the first time to visualize the following matrix:\n\n[[ 150.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.   ]\n [ 150.      69.388   35.36    18.211    7.851    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.      -0.03    -0.047   -0.044   -0.027    0.   ]\n [ 150.      92.192   53.842   29.633   13.192    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.      -0.075   -0.112   -0.104   -0.062    0.   ]\n [ 150.      95.538   58.184   33.287   15.285    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.      -0.156   -0.221   -0.199   -0.116    0.   ]\n [ 150.      81.776   50.068   30.045   14.659    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.      -0.331   -0.416   -0.357   -0.203    0.   ]\n [   0.      31.498   30.267   22.168   13.308    5.653    2.766    1.5      0.86     0.492    0.254    0.082   -0.058   -0.182   -0.298   -0.41    -0.515   -0.617   -0.751   -0.754   -0.609   -0.34     0.   ]\n [   0.      13.948   17.335   15.05    10.751    6.537    3.913    2.374    1.447    0.854    0.443    0.13    -0.132   -0.372   -0.602   -0.826   -1.034   -1.204   -1.301   -1.241   -0.985   -0.548    0.   ]\n [   0.       6.958   10.074    9.949    8.107    5.832    3.973    2.635    1.7      1.034    0.534    0.129   -0.229   -0.57    -0.913   -1.258   -1.59    -1.864   -2.007   -1.924   -1.543   -0.866    0.   ]\n [   0.       3.812    6.053    6.564    5.896    4.712    3.511    2.493    1.685    1.048    0.529    0.079   -0.342   -0.768   -1.22    -1.705   -2.203   -2.654   -2.941   -2.904   -2.395   -1.375    0.   ]\n [   0.       2.237    3.761    4.358    4.202    3.609    2.867    2.142    1.498    0.943    0.455    0.002   -0.452   -0.94    -1.495   -2.139   -2.864   -3.607   -4.2     -4.357   -3.76    -2.237    0.   ]\n [   0.       1.375    2.396    2.906    2.943    2.656    2.206    1.708    1.223    0.771    0.345   -0.076   -0.526   -1.045   -1.682   -2.491   -3.509   -4.71    -5.895   -6.563   -6.052   -3.812    0.   ]\n [   0.       0.867    1.543    1.925    2.009    1.865    1.592    1.261    0.915    0.573    0.232   -0.126   -0.531   -1.031   -1.698   -2.633   -3.971   -5.831   -8.105   -9.948  -10.073   -6.958    0.   ]\n [   0.       0.548    0.986    1.242    1.302    1.205    1.035    0.828    0.604    0.373    0.134   -0.128   -0.441   -0.852   -1.445   -2.372   -3.911   -6.536  -10.749  -15.049  -17.334  -13.947    0.   ]\n [   0.       0.34     0.609    0.755    0.751    0.618    0.516    0.411    0.299    0.183    0.059   -0.081   -0.253   -0.491   -0.859   -1.499   -2.766   -5.652  -13.307  -22.167  -30.267  -31.497    0.   ]\n [   0.       0.203    0.357    0.416    0.331    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.     -14.659  -30.045  -50.068  -81.776 -150.   ]\n [   0.       0.116    0.2      0.221    0.157    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.     -15.284  -33.286  -58.184  -95.538 -150.   ]\n [   0.       0.062    0.104    0.112    0.075    0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.     -13.192  -29.633  -53.842  -92.192 -150.   ]\n [   0.       0.027    0.044    0.047    0.03     0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.      -7.851  -18.211  -35.36   -69.388 -150.   ]\n [   0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.       0.    -150.   ]]\n\n\nThe problem I have is that the default precision of the rendering doesn't show the difference between the values in the center, which are the most important for my application:\n\n\nsince my values range from 150 to -150, matplotlib gives the same color to all the middle values, which are [-5,5]\n\nI would like to manipulate the scaling to generate some kind of heat map showing the progression throughout the figure.\n\nPertinent python code:\n\n    plt.matshow(mat, aspect='auto', cmap=plt.get_cmap('magma'))\n    plt.show()\n\n\nWhat would be the easiest way to achieve this result?\n"
"I have a pandas dataframe which has sections looking like this (the zeros are NaN's):\n\n...\n     18  19  20\n197  14  28  14\n198  14   0  14\n200   0   0   0\n201   0   0   0\n202  15  23  12\n203  16   0  18\n204   0   0   0\n205   0   0   0\n\n...\n\n\nI need to fill rows that have NaN's on specific columns with values from the last row which has no NaN's on those columns.\nIn my example, rows 200,201 would be filled with values from row 197, and rows 204,205 from row 202.\n\nLE: Rows 198&amp;203 don't have NaN's on all columns I'm interested in, so they're left alone.\n\nWhat would be a pythonic way of writing this?\n"
'Dataset\n0-9 columns: float features (parameters of a product)\n10 column: int labels (products)\n\nGoal\n\n\nCalculate an 0-1 classification certainty score for the labels (this is what my current code should do)\nCalculate the same certainty score for each “product_name”(300 columns) at each rows(22\'000)\n\n\nERROR I use sklearn.tree.DecisionTreeClassifier.\nI am trying to use "predict_proba" but it gives an error.\n\nPython CODE\n\ndata_train = pd.read_csv(\'data.csv\')\nfeatures = data_train.columns[:-1]\nlabels = data_train.columns[-1]\nx_features = data_train[features]\nx_label = data_train[labels]\nX_train, X_test, y_train, y_test = train_test_split(x_features, x_label, random_state=0)\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nclf = DecisionTreeClassifier(max_depth=3).fit(X_train, y_train)\nclass_probabilitiesDec = clf.predict_proba(y_train) \n#ERORR: ValueError: Number of features of the model must match the input. Model n_features is 10 and input n_features is 16722 \n\n\nprint(\'Decision Tree Classification Accuracy Training Score (max_depth=3): {:.2f}\'.format(clf.score(X_train, y_train)*100) + (\'%\'))\nprint(\'Decision Tree Classification Accuracy Test Score (max_depth=3): {:.2f}\'.format(clf.score(X_test, y_test)*100) + (\'%\'))\n\nprint(class_probabilitiesDec[:10])\n# if I use X_tranin than it jsut prints out a buch of 41 element vectors: [[ 0.00490808  0.00765327  0.01123035  0.00332751  0.00665502  0.00357707\n   0.05182597  0.03169453  0.04267532  0.02761833  0.01988187  0.01281091\n   0.02936528  0.03934781  0.02329257  0.02961484  0.0353548   0.02503951\n   0.03577073  0.04700108  0.07661592  0.04433907  0.03019715  0.02196157\n   0.0108976   0.0074869   0.0291989   0.03951418  0.01372598  0.0176358\n   0.02345895  0.0169703   0.02487314  0.01813493  0.0482489   0.01988187\n   0.03252641  0.01572249  0.01455786  0.00457533  0.00083188]\n [....\n\n\n\nFEATURES (COLUMNS)\n\n(last columns are the labels)\n0   1   1   1   1.0 1462293561  1462293561  0   0   0.0 0.0 1\n1   2   2   2   8.0 1460211580  1461091152  1   1   0.0 0.0 2\n2   3   3   3   1.0 1469869039  1470560880  1   1   0.0 0.0 3\n3   4   4   4   1.0 1461482675  1461482675  0   0   0.0 0.0 4\n4   5   5   5   5.0 1462173043  1462386863  1   1   0.0 0.0 5\n\nCLASSES COLUMNS (300 COLUMNS OF ITEMS)\n\nHEADER ROW: apple gameboy battery ....\nSCORE in 1st row: 0.763 0.346 0.345  ....\nSCORE in 2nd row: 0.256 0.732 0.935 ....\n\nex.: of similar scores used when someone image classify cat VS. dog and the classification gives confidence scores.\n'
'I am new to Python and posted my problem earlier on and received suggestion from others and still could not fix my problem.  I am re-posting my post with some modification, incorporated suggestions from others.  I am not only new to Python, but also have problems articulating my problems.\n\nI want to convert all the prices from string to numeric, from example, “3K” to “3000” to maintain consistency in data analysis. At the moment, K means thousand and it is sufficient, no need to go into millions or billions.\n\nThis is done on Python data frame and I am still not familiar to iteration, list, and encounter errors that I do not understand. \n\na)  I could not convert string to float. "ValueError: could not convert string to float:"\n\nb)  Then I decided to convert to string but I could not store it on the data frame as string. My output was empty cell.\n\nimport pandas as pd\nimport numpy as np\nimport re \n\ndef regex_filter(val):\n    new_price = val\n    if val:\n        price = \' \'\n        mo = re.search(\'\\d+[kK]\',val)\n        if mo:\n            price = str(price).replace(\'K\',\'000\')\n            print("The New value is ",price)\n            new_price = price\n            return new_price\n        else:\n            return new_price\n    else:\n        return new_price\n\n\nif __name__ == "__main__": \n\n    df = pd.read_csv(\'ProductID_price.csv\', encoding=\'utf8\')\n    df[\'price\'] = df[\'price\'].apply(regex_filter)\n\n\nINPUT\n\n    product_id  product_name                        price\n0   1           Mares XR Kevlar Diving Dry Suit     3K\n1   2           Beuchat Abyss Dry Diving Dry Suit   2050    \n2   3           Typhoon Scuba Dive Dry Suit     1.5K\n3   4           Scubapro Evertech Drysuit Men       4,059.99\n\n\nOUTPUT\n\n    product_id  product_name                        price\n0   1   Mares XR Kevlar Diving Dry Suit \n1   2   Beuchat Abyss Dry Diving Dry Suit           2050\n2   3   Typhoon Scuba Dive Dry Suit \n3   4   Scubapro Evertech Drysuit Men               4,059.99\n\n'
'I want to compare the number of survivors with gender.\nI have something like:\n\nName, Survived, Sex\nJohn, 0, male\nAna, 1, female\nLeo, 1, male\nPeter, 0, male\n\n\nAnd trying to create a bar chart which should look like here:\nhttps://pythonspot.com/wp-content/uploads/2015/07/barchart_python.png.webp\n\nI tried this way\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\'titanic_data/train.csv\')\n\n\nfigure1 = plt.bar(range(4), data[data[\'Sex\']==\'male\'][\'Survived\'].value_counts(), label=\'Male\')\nfigure2 = plt.bar(range(4), data[data[\'Sex\']==\'female\'][\'Survived\'].value_counts(), label=\'Female\')\nplt.legend()\nplt.xticks(np.arange(4), rotation=0)\nplt.title("Third class survivors")\n\nplt.show()\n\n\nBut it says "ValueError: shape mismatch: objects cannot be broadcast to a single shape". What should I do?\n'
"I'm building a tool for the company where I work and I've built a program that returns different statistics from any csv file, of course with an specific data structure. Now, my issue is that I don't know how to do is to request the user to upload a file. \nIn order to create this program I've been using as practice mode, \n\nthis: df = pd.read_csv('',delimiter=';', encoding='ISO-8859-1')\n\n\nAny ideas?\n"
"I'm using jupyter notebook to visualize some data with python.\nI got a dataframe with following columns: matchid, player, visionscore, win\n\ngiven table\n\nNow i want to sum up the visionscore. For every matchid there are 10 entries given, which go from player 1 to 10 (player 1-5 = team 1, 6-10 = team 2) and each player has it's own visionscore.\nNow i want to have something like this:\n\nexpected result\n\nObviously im new to data analytics. I allready have a table where the visionscore/matchid is summed up, but unfortunately there is no seperation between the teams. So it's the whole visionscore for the game.\n\ntable i allready got\n"
'How can I count the frequency of values \u200b\u200bin a column and calculate the percentage relative to the total? \n\nI got a dataframe: \n\n   range\n0   G-L\n1   M-R\n2   G-L\n3   M-R\n4   A-F\n5   S-Z\n6   A-F\n..   ..\n..   ..\n\n\nafter  df.range.value_counts()  i get this:\n\nA-F    1882\nG-L    3096\nM-R    3830\nS-Z    1017\n\n\nnow i want to get the percentage of each range in comparison to the total sum and show this in a plot, where the x-axis got the ranges(A-F; G-L;...) und the y-axis shows the percentage of these ranges.\n'
'I\'m struggling with slicing. I thought that generally it\'s easy and I understand it but when it comes to the below situation my ideas don\'t work.\n\nSituation:\nIn one of my columns in DF I want to remove in all rows some string that sometimes occurs and sometimes doesn\'t. \n\nThe problem looks like this:\n\n1.I don\'t know the exact position when this string starts (in each row it could be a different \n\n2.This string various, depending on each row, however, it always starts from the same structure - let\'s say: "¯main_"\n\n3.After "¯main_" usually, there\'re some numbers (it various) however the length always is the same (9 numbers)\n\n4.I\'m already after splitting and I have around ~40 columns (each with a similar problem). That\'s why I\'m looking for some more efficient way to solve it then splitting, generating ~40 more columns and then dropping them. \n\n5.Sometimes after this string with "¯main_" there\'s some additional string I\'d like to leave in the same column.\n\nExample:\n\nColumn1\nA1-19\nB2-52\nC3-1245¯main_123456789\nD4\nZ89028\nF7¯main_123456789,Z241\n\n\nLooking for a result like this:\n\nColumn1\nA1-19\nB2-52\nC3-1245\nD4\nZ89028\nF7,Z241\n\n\nThe best solution that I prepared up till now:\n\na = test.find("¯")\nb = a+14\ndf[0].str.slice(start = a, stop = b)\n\n\nBut:\n\n1.It doesn\'t work properly\n\n2.And I\'m aware that test.find() returns -1 when it won\'t find a character. I don\'t know how to escape from it - writing a loop? I believe that some better (more efficient) solution exists. However, after a few hours of looking for it, I decided to find help.\n'
"I currently have a dataframe which contains different stores and scores associated with each store. Each row represents one set of scores from a specific time period, so each store is represented multiple times in the dataset. I want to build a series of boxplots, one for each store, and show the spread of each score, for that specific score. \n\nSo far, the only approach I am confident will work is to create new filtered data frames, then individually boxplot each of those dataframes. However, I feel like there is a more elegant solution, perhaps involving a for loop, where I don't have to manually build each separate data frame. \n\ndf = pd.read_csv('Store_scores.csv')\n\n\ndf_storeA = df.loc['Store_A',:]\ndf_storeB = df.loc['Store_B',:]\ndf_storeC = df.loc['Store_C',:]\n\nsns.boxplot(data=df_storeA, y=values, x=categories)\nsns.boxplot(data=df_storeB, y=values, x=categories)\nsns.boxplot(data=df_storeC, y=values, x=categories)\n\n\nWhile this technically works, there are several stores within the dataset, so I'd love something a little less manual, and a little more robust.\n"
'How can I split this JSON data into groups according to \'name\' and sum the number of \'items\' in each group in order to find the most common name (based on number of items). The JSON data I am working with is as follows:\n\njson_data= [   \n {\'code\': \'0101010G0AAABAB\',\n  \'items\': 2,\n  \'practice\': \'N81013\',\n  \'name\': \'Co-Magaldrox_Susp 195mg/220mg/5ml S/F\',\n  \'nic\': 5.98,\n  \'act_cost\': 5.56,\n  \'quantity\': 1000},\n {\'code\': \'0101021B0AAAHAH\',\n  \'items\': 1,\n  \'practice\': \'N81013\',\n  \'name\': \'Alginate_Raft-Forming Oral Susp S/F\',\n  \'nic\': 1.95,\n  \'act_cost\': 1.82,\n  \'quantity\': 500},\n {\'code\': \'0101021B0AAALAL\',\n  \'items\': 12,\n  \'practice\': \'N81013\',\n  \'name\': \'Sod Algin/Pot Bicarb_Susp S/F\',\n  \'nic\': 64.51,\n  \'act_cost\': 59.95,\n  \'quantity\': 6300},\n {\'code\': \'0101021B0AAAPAP\',\n  \'items\': 3,\n  \'practice\': \'N81013\',\n  \'name\': \'Sod Alginate/Pot Bicarb_Tab Chble 500mg\',\n  \'nic\': 9.21,\n  \'act_cost\': 8.55,\n  \'quantity\': 180},\n {\'code\': \'0101021B0BEADAJ\',\n  \'items\': 6,\n  \'practice\': \'N81013\',\n  \'name\': \'Gaviscon Advance_Liq (Peppermint) S/F\',\n  \'nic\': 28.92,\n  \'act_cost\': 26.84,\n  \'quantity\': 90},\n {\'code\': \'0101021B0BEAIAL\',\n  \'items\': 15,\n  \'practice\': \'N81013\',\n  \'name\': \'Gaviscon Advance_Liq (Peppermint) S/F\',\n  \'nic\': 82.62,\n  \'act_cost\': 76.67,\n  \'quantity\': 7800},\n {\'code\': \'0101021B0BEAQAP\',\n  \'items\': 5,\n  \'practice\': \'N81013\',\n  \'name\': \'Gaviscon Advance_Liq (Peppermint) S/F\',\n  \'nic\': 13.47,\n  \'act_cost\': 12.93,\n  \'quantity\': 116},\n {\'code\': \'0101021B0BEBEAL\',\n  \'items\': 10,\n  \'practice\': \'N81013\',\n  \'name\': \'Gaviscon Advance_Liq (Peppermint) S/F\',\n  \'nic\': 64.0,\n  \'act_cost\': 59.45,\n  \'quantity\': 6250},\n {\'code\': \'0101021B0BIABAH\',\n  \'items\': 2,\n  \'practice\': \'N81013\',\n  \'name\': \'Sod Algin/Pot Bicarb_Susp S/F\',\n  \'nic\': 3.9,\n  \'act_cost\': 3.64,\n  \'quantity\': 1000},\n {\'code\': \'0102000A0AAAAAA\',\n  \'items\': 1,\n  \'practice\': \'N81013\',\n  \'name\': \'Alverine Cit_Cap 60mg\',\n  \'nic\': 19.48,\n  \'act_cost\': 18.05,\n  \'quantity\': 100}]\n\n\nI have been able to identify the number of unique values for \'name\' but I do not know how to proceed from there. Here is the code I used:\n\nnames =[]\n\nfor item in range(len(json_data)):\n    names.append(json_data[item][\'name\'])\n\nnames=set(names)\nnames=list(names)\n\nprint(len(names))\n\n\nI expect the output to be in the following format:\n\nmost_common = [("", 0)]\n\n\nwith the name in quotes followed by the total sum of items.\ne.g: \n\nmost_common = [("Gaviscon Advance_Liq (Peppermint) S/F", 36)]\n\n\nPlease bear with me. I am new to Stackoverflow and this is my first question so I\'m still trying to get used to how to ask a question here.\n'
"Would anyone have any tips to clean text data? The data I have is in a list (master_list) and I am trying to create a loop or function that would remove extra [] symbols as well as a None, or None so basically the data in master_list would just be strings separated by a ,\n\nAny help greatly appreciated..\n\nmaster_list = [['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.3.', 'the supply fan is running, the VFD speed output mean value is 94.3.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.2.', 'the supply fan is running, the VFD speed output mean value is 94.2.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.1.', 'the supply fan is running, the VFD speed output mean value is 94.1.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.0.', 'the supply fan is running, the VFD speed output mean value is 94.0.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 93.9.', 'the supply fan is running, the VFD speed output mean value is 93.9.'], None]\n\n"
"Consider we only have images as.npy file. Is it possible to resizing images  without converting their to images (because I'm looking for a way that is fast when run the code).\nfor more info, I asked the way without converting to image, I have images but i don't want use those in code, because my dataset is too large and running with images is so slow, on the other hand, Im not sure which size is better for my imeges, So Im looking for a way that first convert images to npy and save .npy file and then preprocess npy file, for example resize the dimension of images. \n"
'I have a Dataframe from pandas like this:\n\nimport pandas as pd\nraw_data = [{\'Date\': \'1-10-19\', \'Price\':7, \'Check\': 0}, \n            {\'Date\': \'2-10-19\',\'Price\':8.5, \'Check\': 0}, \n            {\'Date\': \'3-10-19\',\'Price\':9, \'Check\': 1}, \n            {\'Date\': \'4-10-19\',\'Price\':50, \'Check\': 1}, \n            {\'Date\': \'5-10-19\',\'Price\':80, \'Check\': 1}, \n            {\'Date\': \'6-10-19\',\'Price\':100, \'Check\': 1}]\ndf = pd.DataFrame(raw_data)\ndf.set_index(\'Date\')\n\n\nThis is what it looks like:\n\n           Price  Check\nDate        \n1-10-19     7.0      0\n2-10-19     8.5      0 \n3-10-19     9.0      1\n4-10-19     50.0     1 \n5-10-19     80.0     1\n6-10-19     100.0    1\n\n\nNow what I\'m trying to do is that for each row where \'Check" is 1, I want to check the number of rows prior to that row in which the price was less than 10% of that row\'s price. For example, for the 6th row where the price is 100, I want to iterate over the the previous rows and count the rows until the price is less than 10 (10% of 100), which in this case would 3 rows prior where the price is 9. Then want to save the results in a new column.\n\nThe final result would look like this:\n\n           Price  Check  Rows_till_small\nDate        \n1-10-19     7.0      0    NaN\n2-10-19     8.5      0    NaN\n3-10-19     9.0      1    Nan\n4-10-19     50.0     1    NaN\n5-10-19     80.0     1    4\n6-10-19     100.0    1    3\n\n\nI\'ve thought a lot about how I could do this using some kind of Rolling function, but I don\'t think it\'s possible. I\'ve also thought about iterating through the entire DataFrame using iterrows or itertuples, but I can\'t imagine of a way to do it without being extremely inefficient.\n'
"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import load_model\nfrom keras.preprocessing import image\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nmodel=tf.keras.models.load_model('model_ex-024_acc-0.996875.h5')\nimg_array = cv2.imread('30.jpg')  # convert to array\n\nimg_rgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n\nimg_rgb = cv2.resize(img_rgb,(224,224),3)\nplt.imshow(img_rgb)  # graph it\nplt.show()\nmodel.predict(img_rgb)\n\n\n\n\n  ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (224, 224, 3)\n\n"
"I want to create synthetic data for a classification problem. I'm using make_classification method of sklearn.datasets.\nI want the data to be in a specific range, let's say [80, 155], But it is generating negative numbers.\n\nI've tried lots of combinations of scale and class_sep parameters but got no desired output.\n\nimport pandas as pd\nfrom sklearn.datasets import make_classification\nweight = [0.2, 0.37, 0.21, 0.04, 0.11, 0.05, 0.02]\n\nX, y = make_classification(n_samples=100, n_features=3,\n            n_informative=3, n_redundant=0, n_repeated=0, \n            n_classes=7, n_clusters_per_class=1, weights=weight,\n            class_sep=1,shuffle=True, random_state=41, scale= 1)\n\npd.DataFrame(X).describe()\n\n\nOutput\n\n\n\nThe output should be in a specific range, but it is picking out random values with standard deviation of around 1.33.\n"
"I have a data frame like this:\n\nCategory Date_1       Score_1    Date_2           Score_2\n  A      13/11/2019    5        13/11/2019        10\n  A      13/11/2019    5        14/11/2019        55\n  A      13/11/2019    5        15/11/2019        45\n  A      13/11/2019    5        16/11/2019        80\n  A      14/11/2019    3        13/11/2019        10\n  A      14/11/2019    3        14/11/2019        55\n  A      14/11/2019    3        15/11/2019        45\n  A      14/11/2019    3        16/11/2019        80\n  A      15/11/2019    7        13/11/2019        10\n  A      15/11/2019    7        14/11/2019        55\n  A      15/11/2019    7        15/11/2019        45\n  A      15/11/2019    7        16/11/2019        80\n  B      13/11/2019    4        13/11/2019        18\n  B      13/11/2019    4        14/11/2019        65\n  B      13/11/2019    4        15/11/2019        75\n  B      13/11/2019    4        16/11/2019        89\n  B      14/11/2019    9        13/11/2019        18\n  B      14/11/2019    9        14/11/2019        65\n  B      14/11/2019    9        15/11/2019        75\n  B      14/11/2019    9        16/11/2019        89\n  B      15/11/2019    8        13/11/2019        18\n  B      15/11/2019    8        14/11/2019        65\n  B      15/11/2019    8        15/11/2019        75\n  B      15/11/2019    8        16/11/2019        89\n\n\nI want to keep the rows where both dates are same. \nI  was doing this:\n\ndf.drop_duplicates(subset=['Date_1', 'Date_2'])\n\n\nBut it do not work. Can`t figure out how to drop those extra rows?\n"
"I have data that looks like this:\n\ncurrency    country\nGBP         GB\nUSD         NaN\nUSD         US\nAUD         AUD\nGBP         NaN\n\n\nBasically I'm trying to make conditional replacements of NaN in the country column to a value that's dependent on what the currency in that row is. (E.g. for GBP it should be GB).\n\nI have a separate dict that maps currencies to country. But I'm not sure how to replace values in the country column\n"
"I have the following DataFrames\n\nimport pandas as pd\n\n\ndf = pd.DataFrame({'a': [201, 201, 201, 201, 202, 202, 202, 203, 203, 203],\n                   'b': [  1,   2,   3,   5,   1,   2,   6,   1,   3,   4]})\n\ndf_filter = pd.DataFrame({'a': [      201,    202, 203],\n                          'b': [[1, 2, 3], [1, 2], [1]]}).set_index('a')\n\n\nthey look like:\n\n&gt;&gt;&gt; df\n     a  b\n0  201  1\n1  201  2\n2  201  3\n3  201  5\n4  202  1\n5  202  2\n6  202  6\n7  203  1\n8  203  3\n9  203  4\n&gt;&gt;&gt;\n&gt;&gt;&gt; df_filter\n             b\na             \n201  [1, 2, 3]\n202     [1, 2]\n203        [1]\n\n\nI want to filter the df, using the df_filter. Namely, I want to keep for each element of 'a', the elements of the corresponding list in 'b'.\n\nWanted result:\n\n&gt;&gt;&gt; df_filtered\n     a  b\n0  201  1\n1  201  2\n2  201  3\n4  202  1\n5  202  2\n7  203  1\n\n\nAlso, I actually want to keep only the consecutive elements of 'b' for each of the elements on 'a'. I now can produce 'df_filter' and filter with that, but any suggestion to do this easier whould be more than welcome.\n"
'I\'m using plotly express and the gapminder dataset to plot a bar chart, to which I have some similar dataset. I found the instructions in their official website and here is the code and figure:\n\nimport plotly.express as px\n\ngapminder = px.data.gapminder()\n\nfig = px.bar(gapminder, x="continent", y="pop", color="continent",\n  animation_frame="year", animation_group="country", range_y=[0,4000000000])\nfig.show()\n\n\n\n\nHowever, I also wish to add another dataset for each bar as a comparison. Ideally, I wish to have a result as below:\n\n\nFor the result in each bar, I have a dataset as the "total number". \nFor example, for the first bar, the "yellow one" is the total number and "blue one" is the targeted number, just I wish to combine them in the same figure. But the example only shows a single dataset. I wonder how I can change the parameter of "y" to include the two datasets?\n'
'I\'m new to Machine Learning and have some problems with image classification. Using a simple classifier technique K Nearest Neighbours I\'m trying to distinguish Cats from Dogs. \n\nMy code so far:\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nDATADIR = "/Users/me/Desktop/ds2/ML_image_classification/kagglecatsanddogs_3367a/PetImages"\nCATEGORIES = [\'Dog\', \'Cat\']\n\nIMG_SIZE = 30\ndata = []\ncategories = []\n\nfor category in CATEGORIES:\n    path = os.path.join(DATADIR, category) \n    categ_id = CATEGORIES.index(category)\n    for img in os.listdir(path):\n        try:\n            img_array = cv2.imread(os.path.join(path,img), 0)\n            new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n            data.append(new_array)\n            categories.append(categ_id)\n        except Exception as e:\n            # print(e)\n            pass\n\nprint(data[0])\n\n\ns1 = pd.Series(data)\ns2 = pd.Series(categories)\nframe = {\'Img array\': s1, \'category\': s2}\ndf = pd.DataFrame(frame) \n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\n\nAnd here I get an error when trying to fit the data:\n\n   ---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-76-9d98d7b11202&gt; in &lt;module&gt;\n      2 from sklearn.neighbors import KNeighborsClassifier\n      3 \n----&gt; 4 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n      5 \n      6 print(X_train)\n\n~/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py in train_test_split(*arrays, **options)\n   2094         raise TypeError("Invalid parameters passed: %s" % str(options))\n   2095 \n-&gt; 2096     arrays = indexable(*arrays)\n   2097 \n   2098     n_samples = _num_samples(arrays[0])\n\n~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in indexable(*iterables)\n    228         else:\n    229             result.append(np.array(X))\n--&gt; 230     check_consistent_length(*result)\n    231     return result\n    232 \n\n~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py in check_consistent_length(*arrays)\n    203     if len(uniques) &gt; 1:\n    204         raise ValueError("Found input variables with inconsistent numbers of"\n--&gt; 205                          " samples: %r" % [int(l) for l in lengths])\n    206 \n    207 \n\nValueError: Found input variables with inconsistent numbers of samples: [24946, 22451400]\n\n\nHow to prepare the training the data properly?\nBtw. I don\'t want to use deep learning. This will be the next step for me.\n\nWould appreciate any help here..\n'
'My data set looks like this. Each row represents a car. Each car is located at an Auto Center, has a Model, Make, and a bunch of other attributes. This is a simplified version of the data frame. Extraneous rows and columns have been omitted for clarity.\n\n+===========+========+=======+====+=====+\n|Auto Center|Model   |Make   |Year|Color|\n+===========+========+=======+====+=====+\n|Roseville  |Corolla |Toyota |    |     |\n|Roseville  |Prius   |Toyota |    |     |\n|Rocklin    |Camry   |Toyota |    |     |\n|Rocklin    |Forester|Subaru |    |     |\n+===========+========+=======+====+=====+\n\n\nWhat do I want to do? I want to group the data by the Auto Center, and display a "list" of the top 5 cars in each Auto Center by quantify, and print their attributes Make, Model, Year, and Color.\n\nAfter grouping the data by the Auto Center, I want to count the number of occurrences of each Model, or even better a combination of Make and Model, in each Auto Center, and I want to get a list of the top 5 cars with the most occurrences. Then I want to print multiple columns of that car.\n\nAssume that the Year and the Color are the same for each car having the same Make and Model.\n\nFor example, the output should be something like this, a list of the top 5 cars in each auto center ordered by the number of occurrences.\n\nRosevile:\nthere are 12 red Toyota Prius 2009\nthere are 8 blue Toyota Cary 2010\n ...\n\n\nThis is what I have so far:\n\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext, SparkSession\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, IntegerType, StringType\n\nsc = SparkContext()\nsqlContext = SQLContext(sc)\nscSpark = SparkSession \\\n    .builder \\\n    .appName("Auto Center Big Data") \\\n    .config("spark.some.config.option", "some-value") \\\n    .getOrCreate()\n\ndata = scSpark.read.csv("autocenters.csv", header=True, inferSchema=True)\ndata.printSchema();\n\ndata.groupby(\'Auto Center\')\n\n\n\nIt seems that data.groupby() returns a GroupedData object. I have seem that .agg() function can be applied to it, but that only works for numerical data, such as finding the mean of some numbers, and here I have strings. I want to count the strings by number of occurrences in each group.\n\nWhat should I do? Is there a way to apply an aggregate function to multiple columns simultaneously, such as both Make and Model together? If not, that should be fine though, considering that there are no cars with the same Model having different Makes.\n'
"I am trying to do a Linear Regression on a data set on applications and the label I assigned was the rating of the application. When trying to split the label into classes, there was a third class 'NaN' even though it was not specified. \n\nbins = (2, 3, 5)\ngroup_names = ['bad', 'good']\nappStore['user_rating'] = pd.cut(appStore['user_rating'], bins = bins, labels = group_names)\nappStore['user_rating'].unique()\n\n\nresults to\n\n[good, bad, NaN]\nCategories (2, object): [bad &lt; good]\n\n\nI even ran the code\n\nappStore.isnull().sum()\n\n\nand it shows no null, the results for all fields were 0.\n\nEDIT:\nI even edited the bins to bins = (0, 3, 5) to split the data because the minimum in the column was 0 and the maximum was 5. It still had a NaN.\n\nThis is the result when I ran sns.countplot(appStore['user_rating']):\n\n\nAs you can see there would've been no outlier data.\n"
"I am taking the data science course from Udemy. After running the code to show the iris data set, it does not show. Instead, it downloads a data file.\n\nI am running the following code: \n\nfrom IPython.display import HTML\nHTML('&lt;iframe src=http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&gt;&lt;/iframe&gt;')\n\n\nIs the code correct? Could you please help how to show the iris dataset in the python using iframe?\n\nlink to the course: https://www.udemy.com/course/introduction-to-data-science-using-python/learn/lecture/9387344#questions\n"
'I am trying to split a parquet file using DASK with the following piece of code\n\nimport dask.dataframe as pd\ndf = pd.read_parquet(dataset_path, chunksize="100MB")\ndf.repartition(partition_size="100MB")\npd.to_parquet(df,output_path)\n\n\nI have only one physical file in input, i.e. file.parquet\n\nThe output of this script is as well only one file, i.e. part.0.parquet. \n\nBased on the partition_size &amp; chunksize parameters, I should have multiple files in output\n\nAny help would be appreciated\n'
'I\'m trying to create the most basic neural network from scratch to predict stocks for apple. The following code is what i have gotten to so far with assistance from looking at data science tutorials. However, I\'m at the bit of actually feeding in the data and making sure it does so correctly.I would to feed in a pandas data frame of a stock trade. This is my view of the NN.\n\n\n5 Input nodes (Open,Close,High,Low,Volume) *note - this will be in a pandas data frame with a datetime index\nAF that sums the weights of each input.\nSigmoid function to normalise the values\n1 output (adj close) *Not sure what I should use as the actual value \n\n\nThen the process is to move back using the back-propagation technique.\n\nimport pandas as pd\nimport pandas_datareader as web\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef sigmoid(x):\n    return 1.0/(1+ np.exp(-x))\n\ndef sigmoid_derivative(x):\n    return x * (1.0 - x)\n\nclass NeuralNetwork:\n    def __init__(self, x, y):\n        self.input      = x\n        self.weights1   = #will work out when i get the correct input\n        self.weights2   = #will work out when i get the correct input                \n        self.y          = y\n        self.output     = #will work out \n\n    def feedforward(self):\n        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n\n    def backprop(self):\n        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n\n        # update the weights with the derivative (slope) of the loss function\n        self.weights1 += d_weights1\n        self.weights2 += d_weights2\n\n\nif __name__ == "__main__":\n    X = #need help here\n    y = #need help here\n    nn = NeuralNetwork(X,y)\n\n    for i in range(1500):\n        nn.feedforward()\n        nn.backprop()\n\n    print(nn.output)\n\n\nIf you have any suggestions, corrections or anything please let me know because I am thrououghly invested into learning the neural networks.\n\nThanks.\n'
'I have a task, where I have to display plot and histogram in same graph for Rayleigh Distribution \n(which never reaches 1 on Y axis, for me highest point is 0.30).\n\nI have an array of x (linear space from 0 to 10), and calculated array of y (calculated from Rayleigh function with two arguments 1,5 and 2).\n\nI can plot it easily. But cannot display a histogram, as histogram never takes values less than 1 (as x for histogram is how often value of some x range is present on y scale).\n\nCould you please advise something?\n\nHere is the code sample:\n(I think I have to calculate x occasions on y somehow, right? That is what I miss?)\n\nUsing Distribution of Rayleigh: https://en.wikipedia.org/wiki/Rayleigh_distribution\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import rayleigh\n\nRAYLEIGH_LOW_LIMIT = 0\nRAYLEIGH_HIGH_LIMIT = 10\nRAYLEIGH_ARG_1 = 0.7\nRAYLEIGH_ARG_2 = 2\n\n\n# Part 1: Make selection of size 1000\n# Build histogramm and theoretical density\nselection = np.linspace(RAYLEIGH_LOW_LIMIT, RAYLEIGH_HIGH_LIMIT, num=1000)\ny = rayleigh.pdf(selection, RAYLEIGH_ARG_1, RAYLEIGH_ARG_2)\nplt.plot(selection, y, "r-",  label="Density")\n\nplt.hist(selection, bins=10, density=True, label="Histogram", rwidth=0.1)\n\nplt.show() # to display plot\n\n'
"I currently have these data points of date vs cumulative sum. I want to predict the cumulative sum for future dates using python. What prediction method should I use?\n\n\n\nMy dates series are in this format: ['2020-01-20', '2020-01-24', '2020-01-26', '2020-01-27', '2020-01-30', '2020-01-31'] dtype='datetime64[ns]'\n\n\nI tried spline but seems like spline can't handle date-time series\nI tried Exponential Smoothing for time series forecasting but the result is incorrect. I don't \nunderstand what predict(3) means and why it returns the predicted sum for dates I already have. I copied this code from an example. Here's my code for exp smoothing:\n\nfit1 = ExponentialSmoothing(date_cumsum_df).fit(smoothing_level=0.3,optimized=False)\n\nfcast1 = fit1.predict(3)\n\nfcast1\n\n\n\n2020-01-27       1.810000\n2020-01-30       2.467000\n2020-01-31       3.826900\n2020-02-01       5.978830\n2020-02-02       7.785181\n2020-02-04       9.949627\n2020-02-05      11.764739\n2020-02-06      14.535317\n2020-02-09      17.374722\n2020-02-10      20.262305\n2020-02-16      22.583614\n2020-02-18      24.808530\n2020-02-19      29.065971\n2020-02-20      39.846180\n2020-02-21      58.792326\n2020-02-22     102.054628\n2020-02-23     201.038240\n2020-02-24     321.026768\n2020-02-25     474.318737\n2020-02-26     624.523116\n2020-02-27     815.166181\n2020-02-28    1100.116327\n2020-02-29    1470.881429\n2020-03-01    1974.317000\n2020-03-02    2645.321900\n2020-03-03    3295.025330\n2020-03-04    3904.617731\n\n\n\nWhat method will be best suited for the sum values prediction that seems to be exponentially increasing?\nAlso I'm pretty new to data science with python so go easy on me. Thanks.\n"
'I am currently parsing historic delay data from a public transport network in Sweden. I have ~5700 files (one from every 15 seconds) from the 27th of January containing momentary delay data for vehicles on active trips in the network. It\'s, unfortunately, a lot of overhead / duplicate data, so I want to parse out the relevant stuff to do visualizations on it.\n\nHowever, when I try to parse and filter out the relevant delay data on a trip level using the script below it performs really slow. It has been running for over 1,5 hours now (on my 2019 Macbook Pro 15\') and isn\'t finished yet.\n\n\nHow can I optimize / improve this python parser? \nOr should I reduce the number of files, and i.e. the frequency of the data collection, for this task?\n\n\nThank you so much in advance. 💗\n\nfrom google.transit import gtfs_realtime_pb2\nimport gzip\nimport os\nimport datetime\nimport csv\nimport numpy as np\n\ndirectory = \'../data/tripu/27/\'\ndatapoints = np.zeros((0,3), int)\nread_trips = set()\n\n# Loop through all files in directory\nfor filename in os.listdir(directory)[::3]:\n\n    try:\n        # Uncompress and parse protobuff-file using gtfs_realtime_pb2\n        with gzip.open(directory + filename, \'rb\') as file:\n            response = file.read()\n            feed = gtfs_realtime_pb2.FeedMessage()\n            feed.ParseFromString(response)\n\n            print("Filename: " + filename, "Total entities: " + str(len(feed.entity)))\n\n            for trip in feed.entity:\n                if trip.trip_update.trip.trip_id not in read_trips:\n\n                    try:\n                        if len(trip.trip_update.stop_time_update) == len(stopsOnTrip[trip.trip_update.trip.trip_id]):\n                            print("\\t","Adding delays for",len(trip.trip_update.stop_time_update),"stops, on trip_id",trip.trip_update.trip.trip_id)\n\n                            for i, stop_time_update in enumerate(trip.trip_update.stop_time_update[:-1]):\n\n                                # Store the delay data point (arrival difference of two ascending nodes)\n                                delay = int(trip.trip_update.stop_time_update[i+1].arrival.delay-trip.trip_update.stop_time_update[i].arrival.delay)\n\n                                # Store contextual metadata (timestamp and edgeID) for the unique delay data point\n                                ts = int(trip.trip_update.stop_time_update[i+1].arrival.time)\n                                key = int(str(trip.trip_update.stop_time_update[i].stop_id) + str(trip.trip_update.stop_time_update[i+1].stop_id))\n\n                                # Append data to numpy array\n                                datapoints = np.append(datapoints, np.array([[key,ts,delay]]), axis=0)\n\n                            read_trips.add(trip.trip_update.trip.trip_id)\n                    except KeyError:\n                        continue\n                else:\n                    continue\n    except OSError:\n        continue\n\n'
'Here is a sample of my dictionary:\n\n{\'Fiction Books 2019\': [\'The Testaments by Margaret Atwood\',\n\'Normal People by Sally Rooney\',\n\'Where the Forest Meets the Stars by Glendy Vanderah\',\n\'Ask Again, Yes by Mary Beth Keane\',\n\'Queenie by Candice Carty-Williams\',\n"On Earth We\'re Briefly Gorgeous by Ocean Vuong",\n\'A Woman Is No Man by Etaf Rum\',\n\'The Overdue Life of Amy Byler by Kelly Harms\'... etc } \n\n\nHow can I do to only keep the Name of the books?\n\nI have tried the following but the loop adds all the books to every key in my dictionary:\n\nbooks_name_dict = dict.fromkeys((col_names), [])\n\nfor k in books_name_dict:\n    for i in range(len(nominee_list_dict_try[k])):\n        books_name_dict[k].append(nominee_list_dict_try[k][i].split(\' by \')[0])\n\n'
'I have a dictionary as follows:\n\ndict = {\n\'key-1\': [(\'blue\', \'-20\'), (\'red\', \'-67\')], \n\'key-2\': [(\'blue\', \'-77\'), (\'cyan\', \'-67\'), (\'white\', \'-57\')],\n\'key-3\': [(\'blue\', \'-39\'), (\'cyan , \'-35\'), (\'purple\', \'-60\')]\n}\n\n\n\nThe above dictionary contains keys with tuples("color", "weights"). I want to filter the list such that if a color is duplicated in the dictionary then the tuple with the highest weight should be kept and all other occurances of that color should be popped out of the dictionary.\n\nIn this case the filtered dictionary should look like this :\n\nfiltered_dict = {\n\'key-1\': [(\'blue\', \'-20\'), (\'red\', \'-67\')], \n\'key-2\': [(\'white\', \'-57\')],\n\'key-3\': [(\'purple\', \'-60\'), (\'cyan\', \'-35\')]\n}\n\n\n\nThe dictionary is generated dynamically with colors and weights. How should i approach this problem ? \n\nIf necessary the structure in which the dictionary is formed can be changed.\n\n[Edit: The weights are negative numbers]\n'
'I am using sklearn and I noticed that the arguments of sklearn.metrics.plot_confusion_matrix and sklearn.metrics.confusion_matrix are inconsistent. plot_confusion_matrix uses estimator and X to construct y_pred, while confusion_matrix has y_pred as argument directly. \n\nWhat may be the reason for this inconsistency?\n\nPartial function definitions:\n\n\nsklearn.metrics.plot_confusion_matrix(estimator, X, y_true, ...) [where X should be X_test]\nsklearn.metrics.confusion_matrix(y_true, y_pred, ...)\n\n\nSources:\n\n\nplot_confusion_matrix\nconfusion_matrix\n\n'
"I am new to numpy and python so please be gentle.\nSo I am working on a csv file popularnames.csv and it has different columns, I only want to load column number 3 which is titled 'Popular names in India' and find the names in that coloumn that have been repeated more than 10 times. I also only want to use numpy for the purpose and cant find any solution yet. My code is:\n\nBaby_names=np.genfromtxt('popularnames.csv', delimiter=',', usecols=(3), skip_header=1, dtype=str)\nfor Baby_names:\n    if np.unique(Baby_names)&gt;10:\n        print(Baby_names)\n\n\nI do understand that this code is wrong but that is all i could think of with the limited knowledge i have. Any help would be appreciated. Thanks in advance!\n"
'I\'m looking for an efficient way (without looping) to add a column to a dataframe, containing a sum over a column of that same dataframe, filtered by some values in the row. Example:\n\nDataframe:\n\nClientID   Date           Orders\n123        2020-03-01     23\n123        2020-03-05     10\n123        2020-03-10     7\n456        2020-02-22     3\n456        2020-02-25     15\n456        2020-02-28     5\n...\n\n\nI want to add a colum "orders_last_week" containing the total number of orders for that specific client in the 7 days before the given date.\nThe Excel equivalent would be something like:\n\nSUMIFS([orders],[ClientID],ClientID,[Date]&gt;=Date-7,[Date]&lt;Date)\n\n\nSo this would be the result:\n\nClientID   Date           Orders  Orders_Last_Week\n123        2020-03-01     23      0\n123        2020-03-05     10      23\n123        2020-03-10     7       10\n456        2020-02-22     3       0\n456        2020-02-25     15      3\n456        2020-02-28     5       18\n...\n\n\nI can solve this with a loop, but since my dataframe contains >20M records, this is not a feasible solution. Can anyone please help me out?\nMuch appreciated!\n'
"I have been using reliance datasets. My datasets look like this.\nDate        Open    High        Low         Close       Adj Close   year_month\n2019-04-15  1345.0  1348.949951 1335.000000 1340.150024 1332.769409 2019-04\n2019-04-16  1345.0  1360.000000 1340.000000 1343.750000 1336.349609 2019-04\n2019-04-18  1375.0  1389.750000 1365.000000 1385.949951 1378.317139 2019-04\n2019-04-22  1360.0  1367.000000 1341.300049 1345.349976 1337.940796 2019-04\n2019-04-23  1348.0  1373.000000 1346.000000 1363.849976 1356.338867 2019-04 \n\nThis is my code but the x axis looks messy if we see the graph\nf,ax1 = plt.subplots(figsize=(8, 5))\nax1.set_xlabel('Date')\nax1.set_ylabel('Price')\nax1.set_title('Original Plot')\nax1.plot('Date', 'Adj Close', data = df);\n\nOutput:-\n\nSo i use another feature but the figure is not as same as before.\nf,ax1 = plt.subplots(figsize=(15, 5))\nax1.set_xlabel('Date')\nax1.set_ylabel('Price')\nax1.set_title('Original Plot')\nax1.plot('year_month', 'Adj Close', data = df);\n\nOutput:-\n\nExpected Output:- (Note:- It is just an example. This figure is not related with my datasets)\n\nHope to get soon response. Thanks in advance guys.\n"
"\nI want to merge some rows if they are nearly similar.\nSimilarity can be checked by using spaCy.\n\ndf:\n\nstring                     \nyellow color       \nyellow color looks like \nyellow color bright\nred color okay\nred color blood\n\n\noutput:\n\nstring\nyellow color looks like bright\nred color okay blood\n\n\nsolution:\nbrute force approach is - for every item in string check similarity with other n-1 item if greater than some threshold value then merge.\n\nIs there any other approach ? As i am not in contact with much people idk how they do it\none idea coming into my mind is- can we pass some function to merge? if it is true then merge otherwise don't.\n\nAny other popular approaches are welcomed.\n"
"There is a column called country's '% Renewable'. I want to create a column consisting of 1 if the value in the column is above the median value of the column and 0 if it is below the median value. Here is my code\n\ndef answer_ten():\n    Top15 = answer_one()\n    avg=Top15['% Renewable'].median(axis=0)\n    print(avg)\n    #print(Top15['% Renewable'])\n    for value in Top15['% Renewable']:\n        if(value&gt;=avg):\n            Top15['HighRenew']=1\n        else:\n            Top15['HighRenew']=0\n    print(Top15[['HighRenew','% Renewable']])\n    Top15['HighRenew']=Top15['HighRenew'].sort_values(ascending = False)       \n\nanswer_ten()\n\n\nBut the output I got is :\n\n14.96908\n                    HighRenew  % Renewable\nCountry                                   \nAustralia                   0    11.810810\nBrazil                      0    69.648030\nCanada                      0    61.945430\nChina                       0     0.000000\nFrance                      0    17.020280\nGermany                     0    17.901530\nIndia                       0    14.969080\nIran                        0     5.707721\nItaly                       0    33.667230\nJapan                       0    10.232820\nRussian Federation          0    17.288680\nSouth Korea                 0     2.279353\nSpain                       0    37.968590\nUnited Kingdom              0    10.600470\nUnited States               0    11.570980\n\n\nThe avg value is 14.96908.\nThanx in advance.\n"
"For example lets say I have the following data set which goes on for 20,000 rows (large data set).\n\ntime      velocity\n0.000000  2.36949\n0.005217  2.36169             \n0.010434  2.35677\n0.015651  2.35299\n0.020869  2.35015\n\n\nI would want to take the second value in 'velocity' and subtract it from the first value. If the difference is less than say, 0.005, then continue to the third value. \n\nAnd continue to take the difference between third and second value, but if difference is greater than 0.005, divide the difference of these two values by another value and store the result.\n\nI would like to continue this process throughout the entire dataframe.\n\nUltimately I want to plot the 'delta values' which meet my condition versus the time at which they occur.\n\nAny help is appreciated. \n"
'Long time lurker here, and this community has been helping me a lot, thank you all.\n\nso I am trying to collect data from vivino.com and the DataFrame comes out empty, I can see that my soup is collecting the website info, but cant see where my error is.\n\nMy code:\n\n\n    headers = {"User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0", "Accept-Encoding":"gzip, deflate", "Accept":"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8", "DNT":"1","Connection":"close", "Upgrade-Insecure-Requests":"1"}\n\n    r = requests.get("https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1UMtNrLA1NTBQS660DQhRS7Z1DQ1SKwDKpqfZliUWZaaWJOao5SfZFhRlJqeq5dsmFierlZdExwJVJFcWA-mCEgC1YxlZ", headers=headers)#, proxies=proxies)\n    content = r.content\n    soup = BeautifulSoup(content, "html.parser")\n\n\nand as i need the wine maker, wine name and ratings this is how ive tried this:\n\n    for d in soup.findAll(\'div\', attrs={\'class\':\'explorerCard__titleColumn--28kWX\'}):\n\n        Winery = d.find_all("a", attrs={"class":"VintageTitle_winery--2YoIr"})\n        Wine = d.find_all("a", attrs={"class":"VintageTitle_wine--U7t9G"})\n        Rating = d.find_all("div", attrs={"class":"VivinoRatingWide_averageValue--1zL_5"})\n        num_Reviews = d.find_all("div", attrs={"class":"VivinoRatingWide__basedOn--s6y0t"})\n        Stars = d.find_all("div", attrs={"aria-label":"rating__rating--ZZb_x rating__vivino--1vGCy"})\n\n        alll=[]\n\n        if Winery is not None:\n            #print(n[0]["alt"])\n            alll.append(Winery.text)\n\n        else:\n            alll.append("unknown-winery")\n\n        if Wine is not None:\n            #print(wine.text)\n            alll.append(wine.text)\n        else:\n            alll.append("0")\n\n        if Rating is not None:\n            #print(rating.text)\n            alll.append(rating.text)\n\n        else:\n            alll.append("0")\n...\n\n\nand then getting the data into a DataFrame :\n\nfor i in range(1, no_pages+1):\n    results.append(get_data())\nflatten = lambda l: [item for sublist in l for item in sublist]\ndf = pd.DataFrame(flatten(results),columns=[\'Winery\',\'Wine\',\'Rating\',\'num_review\', \'Stars\'])\ndf.to_csv(\'redwines.csv\', index=False, encoding=\'utf-8\')\n\n\nthank you all\n'
"I am working with tips data set, and here is the head of data set.\n\n\n total_bill tip     sex    smoker day time  size\n0   16.99   1.01    Female  No  Sun Dinner  2\n1   10.34   1.66    Male    No  Sun Dinner  3\n2   21.01   3.50    Male    No  Sun Dinner  3\n3   23.68   3.31    Male    No  Sun Dinner  2\n4   24.59   3.61    Female  No  Sun Dinner  4\n\n\nMy code is\n\nsns.violinplot(x='day',y='total_bill',data=tips, hue=['sex','smoker'])\n\n\nI want a violinplot of day with total_bill in which hue is sex and smoker, but I can not find any option to set multiple values of hue. Is there any way?\n"
"I have got a dataframe of game releases and ratings\nname,platform,year_of_release,genre,na_sales,eu_sales,jp_sales,other_sales,critic_score,user_score,rating\nWii Sports,Wii,2006.0,Sports,41.36,28.96,3.77,8.45,76.0,8.0,E\nSuper Mario Bros.,NES,1985.0,Platform,29.08,3.58,6.81,0.77,,,\nMario Kart Wii,Wii,2008.0,Racing,15.68,12.76,3.79,3.29,82.0,8.3,E\nWii Sports Resort,Wii,2009.0,Sports,15.61,10.93,3.28,2.95,80.0,8.0,E\nPokemon Red/Pokemon Blue,GB,1996.0,Role-Playing,11.27,8.89,10.22,1.0,,,\n\nI want to fill NaN values in user_score column with the mean of the same genre. If a game has sports genre and in that row user_score is NaN i want replace the null value with sport's average user rating.\n"
"I need to write a function which calculates the number of tweets that were posted per day.\nFunction Specifications:\n\nIt should take a pandas dataframe as input.\nIt should return a new dataframe, grouped by day, with the number of tweets for that day.\nThe index of the new dataframe should be named Date, and the column of the new dataframe should be 'Tweets', corresponding to the date and number of tweets, respectively.\nThe date should be formated as yyyy-mm-dd, and should be a datetime object.\n\nMy code is :\n\ndef number_of_tweets_per_day(df):\n    \n    df = pd.DataFrame(twitter_df['Date','Tweets'], columns =['Date', 'Tweets'])\n    \n    df['Date'] = pd.to_datetime(df['Date'], format = '%Y%m%d')\n    df['Tweets'] = df['Tweets'].astype(str)\n    \n    grouped = pd.df.groupby(['Date'],['Tweets'])\n    final = pd.df.count()\n    \n    \n    \n    return final\n\n\nHowever when I try to run command 'number_of_tweets_per_day(twitter_df.copy())' I get an error saying twitter_df not defined.\nDF:\n\nExpected output:\n\nAttached links to an image of input dataframe and image of expected output.\nWhat am I doing wrong?\n"
"I have that df:\ngene  person  allele    allele2\nA1      p1       G          C\nA2      p1       A          C\nA3      p1       A          T\nA1      p2       G          C\nA2      p2       T          T\nA3      p2       G          C\nA4      p2       A          T\nA2      p1       G          C\nA3      p1       C          C\n...\n\nAs u can see, in table I can have same person few times (record form different laboratories). First p1 is different sample that second p1, and I need to pick only unique samples with best score (the highest number of rows), so it's that example it will be first p1 coz it's have 3 when another have 2.\nAnd I have no idea how to extract that table to got something like this:\ngene  person  allele    allele2\nA1      p1       G          C\nA2      p1       A          C\nA3      p1       A          T\nA1      p2       G          C\nA2      p2       T          T\nA3      p2       G          C\nA4      p2       A          T\n...\n\nI thinking about indexing it by for loop. For example, add to index i if person == above person. If not, i+1. And then I will have a groups. But... whole df has 3mln row, so before I start I decide to describe here my problem. Maybe it's better way to do this?\n"
"I have 2 datasets and a weight array.\n(train_X, validation_X, train_Y, validation_Y and sampleW)\nThe X sets are 3 dimensional, while the Y sets are 2 dimensional numpy-arrays.\nsampleW is a one dimensional numpy array.\nHow do I successfully migrate from fit_generator() to fit() function?\nIn terms of:\n\nare &quot; fit(x=None, y=None,&quot; for train_X, train_Y?\nhow to pass validation data seperately? (validation_X, validation_Y)\ncan I pass sampleW the same way as before?\nhow to train piecewise data on fit()?\nmost importantly: how to do this without generator?\n\nthis is a minimal reproducable (I am currently struggeling to find out why any other batchsize but 1 gives an error, but &gt;1 should also be usable)\n# -*- coding: utf-8 -*-\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,LSTM,BatchNormalization\nimport tensorflow as tf, numpy as np; from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint \ntensorboard_path= r&quot;C:\\Users\\user\\documents\\session&quot;  # &lt;--- your path\ncheckpoint_path = tensorboard_path \n\nBATCH_SIZE = 1\nEPOCHS, Input_shape, labels =  3, (20,4),6\ntrain_X,train_Y = np.asarray([np.random.random(Input_shape) for x in range(100)]), np.random.random((100,labels))\nvalidation_X,validation_Y = np.asarray([np.random.random(Input_shape) for x in range(50)]), np.random.random((50,labels))\nsampleW = np.random.random((100,1)) \n\nclass CustomGenerator_SampleW(tf.keras.utils.Sequence) :\n    def __init__(self, list_x, labels, batch_size, sample_weights=None) : \n        self.labels         = labels\n        self.batch_size     = batch_size\n        self.list_x         = list_x\n        self.sample_weights = sample_weights\n        \n    def __len__(self) :\n        return (np.ceil(len(self.list_x) / float(self.batch_size))).astype(np.int)\n    def __getitem__(self, idx) :\n        batch_x      = self.list_x[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_y      = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_weight = self.sample_weights[idx * self.batch_size : (idx+1) * self.batch_size]\n        return np.array(batch_x),np.array(batch_y), np.array(batch_weight)\n\nclass CustomGenerator(tf.keras.utils.Sequence) :\n    def __init__(self, list_x, labels, batch_size) : \n        self.labels         = labels\n        self.batch_size     = batch_size\n        self.list_x         = list_x \n        \n    def __len__(self) :\n        return (np.ceil(len(self.list_x) / float(self.batch_size))).astype(np.int)\n    def __getitem__(self, idx) :\n        batch_x      = self.list_x[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_y      = self.labels[idx * self.batch_size : (idx+1) * self.batch_size] \n        return np.array(batch_x),np.array(batch_y)\n \n\nmodel = Sequential()\nmodel.add(LSTM(242, input_shape=Input_shape, return_sequences=True))\nmodel.add(Dropout(0.3)); model.add(BatchNormalization())  \n\nmodel.add(LSTM(242, return_sequences=True))\nmodel.add(Dropout(0.3)); model.add(BatchNormalization())\n\nmodel.add(Dense(labels, activation='tanh')); model.add(Dropout(0.3))\n\nopt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\nmodel.compile(loss='mean_absolute_error',optimizer=opt,metrics=['mse'])\n\nif sampleW is not None:\n    train_batch_gen   = CustomGenerator_SampleW(train_X, train_Y, BATCH_SIZE, sample_weights=sampleW)\nelse: train_batch_gen = CustomGenerator(train_X, train_Y, BATCH_SIZE)\nvalidation_batch_gen  = CustomGenerator(validation_X, validation_Y, BATCH_SIZE)\n\ntensorboard = TensorBoard(tensorboard_path)\ncheckpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min') \n\nmodel.fit_generator(train_batch_gen, steps_per_epoch=None,  epochs=EPOCHS, \n                    validation_data = validation_batch_gen, callbacks=[tensorboard,checkpoint]) \n\n"
"I'm looking to clean a column ('Price') in a pandas data frame.\nI want to set any values in the 'Price' column that are larger than 24,000 and smaller than 28,000 to NaN.\nI tried to do this using np.where(), but I get an error message.\nThanks for any help!\nimport pandas as pd\nimport numpy as np\n\ncars = {'Brand': ['Honda Civic','Toyota Corolla','Ford Focus','Audi A4'],\n    'Price': [22000,25000,27000,35000]\n    }\n\ndf = pd.DataFrame(cars, columns = ['Brand', 'Price'])\n\n\ndf['Price'] = np.where(( 24000 &lt; df.Price &lt; 28000), np.nan,df.Price)\n\n# OUTPUT\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n# DESIRED OUTPUT \n            Brand  Price\n0     Honda Civic  22000\n1  Toyota Corolla  NaN\n2      Ford Focus  NaN\n3         Audi A4  35000\n\n"
'DataFrame 1:\n     Email            Mobile    \n  0  test1@test1.com  98989892     \n  1  test4@test4.com  98989895\n  2  test5@test5.com  98989894 \n  3  Otheruser@mail.com 98438348342343\n\nDataFrame 2:\n     Name        Email1           Email2          Email 3    \n  0  x_person    test1@test1.com  hello@hello.com Hello@Hello.com    \n  1  y_person    test4@test4.com                  test2@test2.com\n  2  z_person.   test5@test5.com  asasas@asas.com \n\nWhat is the best way to check Email column value of data frame 1 exist in any of email columns in data frame 2 ? if exists then i want to combine (left join) the records as matches..\nExpected result:\n Email               Mobile                Name       Email1    \n test1@test1.com     98989892              x_person   test1@test1.com \n test4@test4.com     98989895              y_person.  test4@test4.com  \n test5@test5.com     98989894              z_person.  test5@test5.com \n Otheruser@mail.com  98438348342343\n\n'
'I created a dataframe with the pandas library. I want to add a column to the dataframe. However I am getting the following error.But I think I have to enter as much data as the number of lines.How can I enter information in the row and column I want? How can I create a column without entering data?\nimport pandas as pd\n\nkd = pd.DataFrame(data) \ninsertColumns = kd.insert(0, &quot;Age&quot;, [21, 23, 24, 21],True ) \nprint(kd)\n\nerror:\nValueError: Length of values (4) does not match length of index (6)\n\n'
"I want to delete the data named volvo and dodge belonging to the make column. However, although I tried all kinds of functions, I still couldn't remove the data I didn't want from the column.how can I do that. I should delete or show multiple data at once. Very Thank you.\nimport csv\nimport pandas as pd\n\ncsv_path = &quot;traffic-crashes-vehicles-1.csv&quot;\ndata = pd.read_csv(csv_path)\n\nfilter_data1= data[['MAKE']]\n\nfilter_data2 = filter_data1[filter_data1.isin(['DODGE','VOLVO'])].dropna()\n\nDATAFRAME:\n                                                  MAKE  VEHICLE_YEAR\n                    0                            DODGE        2011.0\n                    1                            VOLVO        2020.0\n                    2                          UNKNOWN           NaN\n                    3       TOYOTA MOTOR COMPANY, LTD.        2015.0\n                    4                           SUBARU        2014.0\n                    ...                            ...           ...\n                    700461                      NISSAN        2013.0\n                    700462               MERCEDES-BENZ        2016.0\n                    700463                       DODGE        2018.0\n                    700464             KIA MOTORS CORP        2013.0\n                    700465                       LEXUS        2014.0\n\n"
"This page I want to parse - https://fbref.com/en/comps/9/gca/Premier-League-Stats\nIt has 2 tables, I am trying to get information from the second table, but it keeps displaying the first table every time I run this code.\nfrom bs4 import BeautifulSoup\nimport requests\n\nsource = requests.get('https://fbref.com/en/comps/9/gca/Premier-League-Stats').text\nsoup = BeautifulSoup(source, 'lxml')\nstattable = soup.find('table', class_= 'min_width sortable stats_table min_width shade_zero')[1]\n\nprint(stattable)\n\nmin_width sortable stats_table min_width shade_zero is the ID of the 'second' table.\nIt does not give me an error nor does it return anything. It's null.\n"
"so i have pasted my complete code for your reference, i want to know what's the use of ppf and cdf here? can you explain it? i did some research and found out that ppf(percent point function) is an inverse of CDF(comulative distribution function)\nif they really are, shouldn't this code work if i replaced ppf and cdf as 1/cdf and 1/ppf respectively?\nplease explain this to me, the difference between the two. and how to and when to use which\nthis is, btw, hypothesis testing.\nand sorry for so many comments, just a habit of explaining everything for my future reference.(do point me out if any of my comments is wrong regarding the same)\nball_bearing_radius = [2.99, 2.99, 2.70, 2.92, 2.88, 2.92, 2.82, 2.83, 3.06, 2.85]\n\n\n\n\nimport numpy as np\n\nfrom math import sqrt\nfrom scipy.stats import norm\n\n# h1 : u != U_0\n# h0 : u = u_0\n#case study : ball bearing example, claim is that radius = 3, do hypothesis testing \nmu_0 = 3\nsigma = 0.1\n\n#collect sample\nsample = ball_bearing_radius\n\n#compute mean\nmean = np.mean(sample)\n\n#compute n\nn = len(sample)\n\n#compute test statistic\nz = (mean - mu_0) /(sigma/sqrt(n))\n\n#set alpha\na = 0.01\n\n#-------------------------\n\n#calculate the z_a/2, by using percent point function of the norm of scipy\n#ppf = percent point function, inverse of CDF(comulative distribution function)\n#also, CDF = pr(X&lt;=x), i.e., probability to the left of the distribution\n\nz_critical = norm.ppf(1-a/2)    #this returns a value for which the probab to the left is 0.975\n\np_value = 2*(1 - norm.cdf(np.abs(z)))\n\np_value = float(&quot;{:.4f}&quot;.format(p_value))\n\n\nprint('z : ',z)\nprint('\\nz_critical :', z_critical)\nprint('\\nmean :', mean, &quot;\\n\\n&quot;)\n\n#test the hypothesis\n\nif (np.abs(z) &gt; z_critical):\n    print(&quot;\\nREJECT THE NULL HYPOTHESIS : \\n p-value = &quot;, p_value, &quot;\\n Alpha = &quot;, a )\n\nelse:\n    print(&quot;CANNOT REJECT THE NULL HYPOTHESIS. NOT ENOUGH EVIDENCE TO REJECT IT: \\n p-value = &quot;, p_value, &quot;\\n Alpha = &quot;, a )\n\n"
"Consider the following data frames in Python Pandas:\nDataframeA\n\n\n\n\nColA\nColB\nColC\n\n\n\n\n1\ndog\n439\n\n\n1\ncat\n932\n\n\n1\nfrog\n932\n\n\n2\ndog\n2122\n\n\n2\ncat\n454\n\n\n2\nfrog\n773\n\n\n3\ndog\n9223\n\n\n3\ncat\n3012\n\n\n3\nfrog\n898\n\n\n\nDataframeB\n\n\n\n\nColD\nColE\n\n\n\n\n1\n101\n\n\n2\n314\n\n\n3\n124\n\n\n\n\nTo note, ColB just repeats it's string values as ColA iterates upwards. ColC and ColE are random. ColA and ColD correspond. ColD values will never have repeats (like a map).\nI want to divide ColC by ColE wherever ColA == ColD and ideally put the resulting value in a new column in DataframeA (or just have it overwrite ColC). The resulting value should be able to have decimals.\nHow can I do this in Python Pandas?\n"
'I am fairly new to Pandas and I am working on project where I have a column that looks like the following:\n\nAverageTotalPayments\n    $7064.38\n    $7455.75\n    $6921.90\n     ETC\n\n\nI am trying to get the cost factor out of it where the cost could be anything above 7000. First, this column is an object. Thus, I know that I probably cannot do a comparison with it to a number. My code, that I have looks like the following:\n\nimport pandas as pd \nhealth_data = pd.read_csv("inpatientCharges.csv")\n\nstate = input("What is your state: ")\nissue = input("What is your issue: ")\n#This line of code will create a new dataframe based on the two letter state code\nstate_data = health_data[(health_data.ProviderState == state)]\n#With the new data set I search it for the injury the person has.\nissue_data=state_data[state_data.DRGDefinition.str.contains(issue.upper())]\n#I then make it replace the $ sign with a \'\' so I have a number. I also believe at this point my code may be starting to break down. \nissue_data = issue_data[\'AverageTotalPayments\'].str.replace(\'$\', \'\')\n#Since the previous line took out the $ I convert it from an object to a float\nissue_data = issue_data[[\'AverageTotalPayments\']].astype(float)\n#I attempt to print out the values. \ncost = issue_data[(issue_data.AverageTotalPayments &gt;= 10000)]\nprint(cost)\n\n\nWhen I run this code I simply get nan back. Not exactly what I want. Any help with what is wrong would be great! Thank you in advance.\n'
"My timestamp looks like below in the dataframe of my column but it is in 'object'. I want to convert this into 'timestamp'. How can I convert all values such in my dataframe column into timestamp?\n\n0    01/Jul/1995:00:00:01\n1    01/Jul/1995:00:00:06\n2    01/Jul/1995:00:00:09\n3    01/Jul/1995:00:00:09\n4    01/Jul/1995:00:00:09\nName: timestamp, dtype: object\n\n\nI tried below code referring this stackoverflow post but it gives me error:\n\npd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S.%f')\n\n\nBelow is the error:\n\nValueError: time data '01/Jul/1995:00:00:01' does not match format '%d/%b/%Y:%H:%M:%S.%f' (match)\n\n"
"I have a dataframe which is something like this\n\nVictim Sex       Female    Male  Unknown\nPerpetrator Sex                         \nFemale            10850   37618       24\nMale              99354  299781       92\nUnknown           33068  156545      148\n\n\nI'm planning to drop both the row indexed as 'Unknown' and the column named 'Unknown'. I know how to drop a row and a column but I was wondering whether you could drop a row and a column at the same time in pandas? If yes, how could it be done?\n"
"I have some N/A value in my dataframe\n\ndf = pd.DataFrame({'A':[1,1,1,3],\n              'B':[1,1,1,3],\n              'C':[1,np.nan,3,5],\n              'D':[2,np.nan, np.nan, 6]})\nprint(df)\n\n    A   B   C   D\n0   1   1   1.0 2.0\n1   1   1   NaN NaN\n2   1   1   3.0 NaN\n3   3   3   5.0 6.0\n\n\nHow can I fill in the n/a value with the mean of its previous non-empty value and next non-empty value in its column?\nFor example, the second value in column C should be filled in with (1+3)/2= 2\n\nDesired Output:\n\n    A   B   C   D\n0   1   1   1.0 2.0\n1   1   1   2.0 4.0\n2   1   1   3.0 4.0\n3   3   3   5.0 6.0\n\n\nThanks!\n"
"I was reading the book Data Science From scratch by Joel Grus. My question is specifically concerning Chapter 6, where the author was using binomial random variable to simulate the theorem.\n\nThe result would be a chart with the probability distribution of the binomial trials and an approximation plot using normal distribution. The two plots should be very similar to each other. The book shows a chart like this:\nAuthor's Chart\n\nThe codes he provided are:\n\nimport random\nfrom matplotlib import pyplot as plt\nfrom collections import Counter\n\ndef bernoulli_trial(p):\n    return 1 if random.random() &lt; p else 0\n\ndef binomial(n, p):\n    return sum(bernoulli_trial(p) for _ in range(n))\n\ndef make_hist(p, n, num_points):\n    data = [binomial(n, p) for _ in range(num_points)]\n    histogram = Counter(data)\n    plt.bar([x-0.4 for x in histogram.keys()],\n       [v / num_points for v in histogram.values()],\n       0.8,\n       color='0.75')\n\n    mu = p * n\n    sigma = math.sqrt(n * p * (1-p))\n\n    # use a line chart to show the normal approximation\n    xs = range(min(data), max(data) + 1)\n    ys = [normal_cdf(i+0.5, mu, sigma) - normal_cdf(i-0.5, mu, sigma) for i in xs]\n    plt.plot(xs, ys)\n    plt.title('Binomial Distribution vs. Normal Approximation')\n    plt.show()\n\nmake_hist(0.75, 100, 10000)\n\n\nMy question is, in this line:\n    [normal_cdf(i+0.5, mu, sigma) - normal_cdf(i-0.5, mu, sigma) for i in xs]\nwhy did the author use +0.5 and -0.5? Is there a specific reason for that?\n\nNot sure if anyone has encountered this question. \nThank you in advance!\n"
'Sorry if this seems like a stupid question,\nI have a dataset which looks like this \n\ntype    time    latitude    longitude   altitude (m)    speed (km/h)    name    desc    currentdistance timeelapsed\nT   2017-10-07 10:44:48 28.750766667    77.088805000    783.5   0.0 2017-10-07_10-44-48     0.0 00:00:00\nT   2017-10-07 10:44:58 28.752345000    77.087840000    853.5   7.8         198.70532   00:00:10\nT   2017-10-07 10:45:00 28.752501667    77.087705000    854.5   7.7         220.53915   00:00:12\n\n\nIm not exactly sure how to approach this,calculating acceleration requires taking difference of speed and time,any suggestions on what i may try?\n\nThanks in advance  \n'
"I have a dictionary where each key has a list of values.\nLength of the list associated with each key is different.\nI want to convert the dictionary into a pandas dataframe with two columns 'Key' and 'Values'. Each row having one dictionary key in the 'Key' column and the list of values associated with it in 'Values' column. The dataframe will look as follows:\n\nmapping_dict = {'A':['a', 'b', 'c', 'd'], 'B':['aa', 'bb', 'cc']}\n\ndf = \n    Key   Value\n0   A     ['a', 'b', 'c', 'd']\n1   B     ['aa', 'bb', 'cc']\n\n\nI tried using the answer provided here by modifying it as per my use case.\nBut it didn't output the required answer.\n"
"I have a .docx Microsoft Word file formatted roughly as follows:\n\nTAG    Lorem ipsum dolor sit amet, consectetur adipiscing \n       elit, sed do eiusmod tempor\nTAG_2  Lorem ipsum dolor sit amet, consectetur adipiscing \n       elit, sed do eiusmod tempor incididunt ut labore \n       et dolore magna aliqua. Ut enim ad minim veniam, \n       quis nostrud exercitation ullamco laboris nisi \nTAG    Text text text text text text text text text text\n\n\nWhere indentation is achieved by wrapping long lines automatically (if copy-pasted in a simple txt editor, the above text would result in 3 lines instead of 7).\n\nMy task is to automatically count the number of lines assigned to a tag, s.t. the above file would result in something like: \n\nTAG    2\nTAG_2  4\nTAG    1\n\n\nRight now I do it manually, by specifiying a font file, font size, and average line lenght, and dividing the lenght of a line (measured with PIL.ImageFont.getsize()) but this approach is really error-prone and does not cover all possible situations (like fonts changing mid-file).\nUnfortunately I have no control over the file, so I cannot properly format it before counting lines (as reason would demand). \n\nIs there a way to do this in Python? I've found the python-docx package but is seems kinda limited in its capabilities. \n\nAlso note that the .docx format is not necessary mandatory, I could also convert the file to .odt if necessary.\n\nAttaching a screenshot of my setup (in LibreOffice) to make it more clear. \n\n\n"
"I want one of my ONLY ONE of my features to be converted to a separate binary features:\ndf[&quot;pattern_id&quot;]\nOut[202]: \n0       3\n1       3\n...\n7440    2\n7441    2\n7442    3\nName: pattern_id, Length: 7443, dtype: int64 \ndf[&quot;pattern_id&quot;]\nOut[202]: \n0       0 0 1\n1       0 0 1\n...\n7440    0 1 0\n7441    0 1 0\n7442    0 0 1\nName: pattern_id, Length: 7443, dtype: int64 \n\nI want to use OneHotEncoder, data is int, so no need to encode it:\nonehotencoder = OneHotEncoder(categorical_features=[&quot;pattern_id&quot;])\ndf = onehotencoder.fit_transform(df).toarray()\n\nValueError: could not convert string to float: 'http://www.zaragoza.es/sedeelectronica/'\n\nInteresting enough I receive an error... sklearn tried to encode another column, not the one I wanted.\nWe have to encode pattern_id to be an integer value\nI used this link: Issue with OneHotEncoder for categorical features\n#transform the pattern_id feature to int\nencoding_feature = [&quot;pattern_id&quot;]\nenc = LabelEncoder()\nenc.fit(encoding_feature)\nworking_feature = enc.transform(encoding_feature)\nworking_feature = working_feature.reshape(-1, 1)\nohe = OneHotEncoder(sparse=False)\n\n\n#convert the pattern_id feature to separate binary features\nonehotencoder = OneHotEncoder(categorical_features=working_feature, sparse=False)\ndf = onehotencoder.fit_transform(df).toarray()\n\nAnd I get the same error. What am I doing wrong ?\nEdit\nsource:\nhttps://github.com/martin-varbanov96/scraper/blob/master/logo_scrape/logo_scrape/analysis.py\ndf\nOut[259]: \n      found_img  is_http                                           link_img  \\\n0          True        0                                  img/aahoteles.svg   \n//www.zaragoza.es/cont/paginas/img/sede/logo_e...   \n\n      pattern_id                                       current_link  site_id  \\\n0              3             https://www.aa-hoteles.com/es/reservas        3   \n6              3      https://www.aa-hoteles.com/es/ofertas-hoteles        3   \n7              2           http://about.pressreader.com/contact-us/        4   \n8              3           http://about.pressreader.com/contact-us/        4   \n\n      status                                   link_id  \n0        200               https://www.aa-hoteles.com/  \n1        200               https://www.365travel.asia/  \n2        200               https://www.365travel.asia/  \n3        200               https://www.365travel.asia/  \n4        200               https://www.aa-hoteles.com/  \n5        200               https://www.aa-hoteles.com/  \n6        200               https://www.aa-hoteles.com/  \n7        200              http://about.pressreader.com  \n8        200              http://about.pressreader.com  \n9        200               https://www.365travel.asia/  \n10       200               https://www.365travel.asia/  \n11       200               https://www.365travel.asia/  \n12       200               https://www.365travel.asia/  \n13       200               https://www.365travel.asia/  \n14       200               https://www.365travel.asia/  \n15       200               https://www.365travel.asia/  \n16       200               https://www.365travel.asia/  \n17       200               https://www.365travel.asia/  \n18       200              http://about.pressreade \n\n[7443 rows x 8 columns]\n\n"
"I have Dataset which has many columns and I want create a new column based on two columns on that dataset.  \n\n   train_data[['CtpJobId', 'SegmentId']]\n\n     CtpJobId     SegmentId\n0   qa1-9epx-dk1    347772\n1   qa1-9epx-dv1    347774\n2   qa1-9epx-dv1    347777\n3   qa1-9epx-dv1    347780\n4   qa1-9epx-dv1    347783\n5   qa1-9epx-dv1    347786\n6   qa1-9epx-dv1    347789\n7   qa1-9epx-dv1    347792\n8   qa1-9epx-e01    347794\n9   qa1-9epx-eb2    347795\n10  qa1-9epx-ez1    347796\n11  qa1-9epx-f32    347797\n12  qa1-9epx-fi1    347798\n\n\nNow I want create a new column called numberOfSegment such as if same jobId has multiple segmentId then aggregate that segmentId and insert that sum up to that new column.\n\n      CtpJobId        SegmentId    numberOfSegment\n0   qa1-9epx-dk1    347772             1\n1   qa1-9epx-dv1    347774             7\n2   qa1-9epx-dv1    347777             7\n3   qa1-9epx-dv1    347780             7\n4   qa1-9epx-dv1    347783             7\n5   qa1-9epx-dv1    347786             7\n6   qa1-9epx-dv1    347789             7\n7   qa1-9epx-dv1    347792             7\n8   qa1-9epx-e01    347794             1\n9   qa1-9epx-eb2    347795             1\n10  qa1-9epx-ez1    347796             1\n11  qa1-9epx-f32    347797             1\n\n\nI did in one way but it gives wrong \n\ntrain_data['NumberOfSegment'] = train_data.groupby('CtpJobId')['SegmentId'].sum()\n\n train_data[['CtpJobId','NumberOfSegment']]\n\n   CtpJobId    NumberOfSegment\n0   qa1-9epx-dk1    NaN\n1   qa1-9epx-dv1    NaN\n2   qa1-9epx-dv1    NaN\n3   qa1-9epx-dv1    NaN\n4   qa1-9epx-dv1    NaN\n5   qa1-9epx-dv1    NaN\n6   qa1-9epx-dv1    NaN\n7   qa1-9epx-dv1    NaN\n8   qa1-9epx-e01    NaN\n9   qa1-9epx-eb2    NaN\n10  qa1-9epx-ez1    NaN\n\n\nCan Anyone please help me? Thanks in advance\n"
'I\'m trying to:\n\n- read a KML file\n- remove the Placemark element if name = \'ZONE\'\n- write a new KML file without the element\n\n\nThis is my code:\n\nfrom pykml import parser\nkml_file_path = \'../Source/Lombardia.kml\'\n\nremoveList = list()\n\nwith open(kml_file_path) as f:\n folder = parser.parse(f).getroot().Document.Folder\n\nfor pm in folder.Placemark:\n    if pm.name == \'ZONE\':\n        removeList.append(pm)\n        print pm.name\n\nfor tag in removeList:\n    parent = tag.getparent()\n    parent.remove(tag)\n#Write the new file\n#I cannot reach the solution help me\n\n\nand this is the KML:\n\n&lt;?xml version="1.0" encoding="UTF-8"?&gt;\n&lt;kml xmlns="http://earth.google.com/kml/2.2"&gt;\n&lt;Document&gt;\n    &lt;name&gt;Lombardia&lt;/name&gt;\n    &lt;Style&gt;\n    ...\n    &lt;/Style&gt;\n    &lt;Folder&gt;\n&lt;Placemark&gt;\n            &lt;name&gt;ZOGNO&lt;/name&gt;\n            &lt;styleUrl&gt;#FEATURES_LABELS&lt;/styleUrl&gt;\n            &lt;Point&gt;\n                &lt;coordinates&gt;9.680530595139061,45.7941656233647,0&lt;/coordinates&gt;\n            &lt;/Point&gt;\n        &lt;/Placemark&gt;\n        &lt;Placemark&gt;\n            &lt;name&gt;ZONE&lt;/name&gt;\n            &lt;styleUrl&gt;#FEATURES_LABELS&lt;/styleUrl&gt;\n            &lt;Point&gt;\n                &lt;coordinates&gt;10.1315885854064,45.7592449779275,0&lt;/coordinates&gt;\n            &lt;/Point&gt;\n        &lt;/Placemark&gt;\n    &lt;/Folder&gt;\n&lt;/Document&gt;\n&lt;/kml&gt;\n\n\nThe problem is that when I write the new KML file this still has the element I want to delete.\nIn fact, with I want to delete the element that contains name = ZONE.\nWhat i\'m doing wrong?\nThank you.\n\n--- Final Code\nThis is the working code thanks to @Dawid Ferenczy:\n\nfrom lxml import etree\nimport pykml\nfrom pykml import parser\n\nkml_file_path = \'../Source/Lombardia.kml\'\n\n# parse the input file into an object tree\nwith open(kml_file_path) as f:\n  tree = parser.parse(f)\n\n# get a reference to the "Document.Folder" node\nfolder = tree.getroot().Document.Folder\n\n# iterate through all "Document.Folder.Placemark" nodes and find and remove all nodes\n# which contain child node "name" with content "ZONE"\nfor pm in folder.Placemark:\n    if pm.name == \'ZOGNO\':\n        parent = pm.getparent()\n        parent.remove(pm)\n\n# convert the object tree into a string and write it into an output file\nwith open(\'output.kml\', \'w\') as output:\n    output.write(etree.tostring(folder, pretty_print=True))\n\n'
"I've got a structure like the following one:\n\nproduct_type = np.dtype([('message_counter', np.int),\n                         ('alteration_time', 'U32'),\n                         ('area_states', status_type, (3,)),\n                        ])\n\n\nwith:\n\nstatus_type = np.dtype([('area', 'U32'),\n                        ('state', 'U32')])\n\n\nfurthermore I have an array of product_type like:\n\nproducts = np.array([product1, product2, ...], dtype=product_type)\n\n\nnow I want to select products which only have a status_type equal to ('area1', 'active'). How would I achieve this. I've tried something like:\n\nmask = np.isin(products['area_states'][['area', 'state']],\n              ('area1', 'active'))\nactive_products = products[mask]\n\n\nUnfortunately this didn't work out the way I hoped. Of course I only received a mask for the subarray (status_type) but I rather like to get a mask on products so that I can filter for the products which only have a status_type with ('area1', 'active'). \n\nso all in all the code would be the following:\n\nstatus_type = np.dtype([('area', 'U32'),\n                        ('state', 'U32')])\nproduct_type = np.dtype([('message_counter', np.int),\n                         ('alteration_time', 'U32'),\n                         ('area_states', status_type, (3,)),\n                         ])\nproducts = np.array([(253, '12:00', [('area1', 'active'), ('area2', 'inactive'), ('area3', 'inactive')]),\n                     (254, '13:00', [('area1', 'inactive'), ('area2', 'inactive'), ('area3', 'inactive')])],\n                    dtype=product_type)\nactive_products_in_area1 = '???'\n\n"
'I have implemented my own kNN-algorithm with the iris dataset in python. Now I would like to be able to report the training and test error for different kinds of k. I have calcultated the accuracy of my predictions, but don\'t really know how to get the training and test error from this. Any ideas?\n\nThank you in advance\n\nEDIT: Here\'s the code\n\nimport pandas as pd\nimport math\nimport operator\nfrom sklearn.model_selection import train_test_split\n\n\ndef euclideanDistance(instance1, instance2, length):\n    distance = 0\n    for x in range(length):\n        distance += pow((instance1[x] - instance2[x]), 2)\n    return math.sqrt(distance)\n\n\ndef getNeighbors(trainingSet, testInstance, k):\n    distances = []\n    length = len(testInstance) - 1\n    for x in range(len(trainingSet)):\n        dist = euclideanDistance(testInstance, trainingSet.iloc[x], length)\n        distances.append((trainingSet.iloc[x], dist))\n    distances.sort(key=operator.itemgetter(1))\n    neighbors = []\n    for x in range(k):\n        neighbors.append(distances[x][0])\n    return neighbors\n\n\ndef getResponse(neighbors):\n    classVotes = {}\n    for x in range(len(neighbors)):\n        response = neighbors[x][-1]\n        if response in classVotes:\n            classVotes[response] += 1\n        else:\n            classVotes[response] = 1\n        sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)\n\n    return sortedVotes[0][0]\n\n\ndef getAccuracy(testSet, predictions):\n    correct = 0\n    for x in range(len(testSet)):\n        if testSet.iloc[x][-1] == predictions[x]:\n            correct += 1\n    return (correct / float(len(testSet))) * 100.0\n\n\ndef main():\n    dataset = pd.read_csv(\'DataScience/iris.data.txt\',\n                          names=["Atr1", "Atr2", "Atr3", "Atr4", "Class"])\n\n    x = dataset.drop([\'Class\'], axis=1)\n    y = dataset.drop(["Atr1", "Atr2", "Atr3", "Atr4"], axis=1)\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=65, stratify=y)\n\ntrainingSet = pd.concat([x_train, y_train], axis=1)\ntestSet = pd.concat([x_test, y_test], axis=1)\n# prepare data\n\n# generate predictions\npredictions = []\nk = 5\nfor x in range(len(testSet)):\n    neighbors = getNeighbors(trainingSet, testSet.iloc[x], k)\n    result = getResponse(neighbors)\n    predictions.append(result)\n    print(\'&gt; predicted=\' + repr(result) + \', actual=\' + repr(testSet.iloc[x][-1]))\naccuracy = getAccuracy(testSet, predictions)\nprint(\'Accuracy: \' + repr(accuracy) + \'%\')\n\n\nmain()\n'
'I have the input dataframe:\n\ndf1 = pandas.DataFrame( { \n    "Name" : ["Alice", "Bob", "Mallory", "Mallory","Mallory", "Bob" ,"Bob", "Mallory", "Alice"] , \n    "City" : ["Seattle", "Seattle", "Portland", "Seattle", "Seattle", "Portland", "Portland", "Seattle", "Seattle"] } )\n\n\nAnd I want to groupby Name, but not unique, so the output should be:\n\n["Alice","Bob","Mallory","Bob","Mallory", "Alice"]\n\n\nI couldn\'t find any efficient way to do it - is there a way without iterating all rows?\n'
'Pandas beginner here.\nI have a .CSV file that I have opened using Pandas. The format of the file is follows:-\n\nPatientId    x    y    width    height    target\nA12kxk       23   45   10       20        1\nAldkd2       92   22   12       30        1\nAldkd2       29   11   98       34        1\nAlll34                 0\n\n\nI want to get a dictionary with the PatientId as key and the value would be an 2D array containing the x, y, width, height of one row of one patient row-wise and the various rows stacked down like this:-\n\nDictionary["Aldkd2"] =\n 92 22 12 30\n 29 11 98 34\n\nI want to discard those which have 0 in target.\nThere are one or more rows in the table for a single patientId. How can I do this?\n'
'I found this dataset on Kaggle containing transactions made by credit cards in September 2013 by European cardholders, over 2 days. The dataset is highly unbalanced, with frauds only taking 0.172% of all transactions. \n\nI want to implement a (Gaussian) Naive Bayes classifier on this dataset to identify fraudulent transactions. \n\nI\'ve done the following already:\n\n\nLoad data into data frame\nSplit data into X and y\nStandardize the data\nHandle the unbalanced dataset with ADASYN\nBuild the Gaussian Naive Bayes model\n\n\nNow, I want to evaluate the models:\n\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)\n# Output: 0.95973427712704695\n\nmetrics.confusion_matrix(y_test, y_pred_class)\n# Output: \n# array([[68219,  2855],\n#       [   12,   116]], dtype=int64)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred_class, digits=4))\n# Output:\n#              precision    recall  f1-score   support\n#\n#           0     0.9998    0.9598    0.9794     71074\n#           1     0.0390    0.9062    0.0749       128\n\n#   micro avg     0.9597    0.9597    0.9597     71202\n#   macro avg     0.5194    0.9330    0.5271     71202\n#weighted avg     0.9981    0.9597    0.9778     71202\n\n\nIt was noted however in the dataset that: \n\n"Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification."\n\nSo does this mean that I should measure accuracy with AUPRC even if I\'ve already done ADASYN and oversampled the data?\n\nI tried computing the accuracy for ROC_AUC (is this the same as AUPRC?) but received an error:\n\ny_pred_prob = gaussian.predict_proba(X_test)\nmetrics.roc_auc_score(y_test, y_pred_prob)\n\n\nValueError: bad input shape (71202, 2)\n\nHow do I properly calculate the accuracy for this?\n\nThank you!\n'
"I'm trying to plot 10 samples from the MNIST dataset. One of each digit. Here's the code:\n\nimport sklearn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\n\nmnist = datasets.fetch_mldata('MNIST original')\ny = mnist.target\nX = mnist.data\n\nfor i in range(10):\n    im_idx = np.argwhere(y == i)[0]\n    print(im_idx)\n    plottable_image = np.reshape(X[im_idx], (28, 28))\n    plt.imshow(plottable_image, cmap='gray_r')\n    plt.subplot(2, 5, i + 1)\n\nplt.plot()\n\n\nFor some reason, the zero digit is being skipped in the plot.\n\nWhy?\n"
"I have been working with a data frame in which data record have useful information in square brackets and non-useful information outside the square bracket.\n\nSample Data frame:   \n\n Record        Data\n      1          Rohan is [age:10] with [height:130 cm].\n      2          Girish is [age:12] with [height:140 cm].\n      3          Both kids live in [location:Punjab] and [location:Delhi].\n      4          They love to play [Sport:Cricket] and [Sport:Football].\n\n\nExpected Output:\n\n Record        Data\n      1          [age:10],[height:130 cm]\n      2          [age:12],[height:140 cm]\n      3          [location:Punjab],[location:Delhi]\n      4          [Sport:Cricket],[Sport:Football]\n\n\nI have been trying this but cannot get the desired output.\n\ndf['b'] = df['Record'].str.findall('([[][a-z \\s]+[]])', expand=False).str.strip()\nprint(df['b'])\n\n\nThat doesn't seems to work.\n\nI am new with Python.\n"
"Im trying to use this SO post to combine a date and time stamp but not having any luck..\n\n#df= pd.read_csv('C:\\\\Users\\\\desktop\\\\master.csv', index_col='Date', parse_dates=True)\ndf= pd.read_csv('C:\\\\Users\\\\desktop\\\\master.csv')\n\n\nThis is where Im stuck, I don't know how to import the package correctly..\n\nThis doesn't work:\nfrom datetime import combine\n\ndf['DateTime'] = df.apply(lambda x: combine(df['Date'], df['Time']), axis=1)\n\n\nWhen everything is all said and done, do I need to parse_dates=True? Usually all my data is a combined date time... and Finally I need to set a new index in my pandas data frame for the new combined date-time column.\n\nAny tips greatly appreciated thanks\n"
'import numpy as np\n\no = np.array([\n\n              [\n              [1,2,3,4],\n              [5,6,7,8]\n              ],\n\n              [\n              [9,10,11,12],\n              [13,14,15,16]\n              ]\n\n             ])\nprint(o.flatten())\n\n# array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\n\n\nIt first flattens the row of each matrix\n\nBut I want that it flattens the coluumn of each matrix first so that it prints\n[1,5,2,6,3,7,4,8,9,13,10,14,11,15,12,16]\n\nI tried searching and what I found was passing "F" as an argument but that gives [1,5,9,13,2,6,10,14,3,7,11,15,4,8,12,16]\nthat is it switches to another matrix without completing first.\n\nIn short, I would like to find python equivalent of R\'s indexing with double brackets\n\nsomeData &lt;- rep(0, 2*3*4);\nar &lt;- array(someData, c(2, 3, 4));  \nar[1,1,1] = 1\nar[1,2,1] = 2\nar[1,3,1] = 3\n\nar[2,1,1] = 4\nar[2,2,1] = 5\nar[2,3,1] = 6\n\nar[1,1,2] = 7\nar[1,2,2] = 8\nar[1,3,2] = 9\nprint(ar[[1]]) # 1\nprint(ar[[2]]) # 4\nprint(ar[[3]]) # 2\nprint(ar[[4]]) # 5 \nprint(ar[[5]]) # 3\nprint(ar[[6]]) # 6\n\n'
'I am trying to write a program to calculate the slope and the intercept of a linear regression model but when I am running more than 10 iterations, the gradient descent function gives the np.nan value for both intercept as well as slope.\n\nBelow is my implementation\n\ndef get_gradient_at_b(x, y, b, m):\n  N = len(x)\n  diff = 0\n  for i in range(N):\n    x_val = x[i]\n    y_val = y[i]\n    diff += (y_val - ((m * x_val) + b))\n  b_gradient = -(2/N) * diff  \n  return b_gradient\n\ndef get_gradient_at_m(x, y, b, m):\n  N = len(x)\n  diff = 0\n  for i in range(N):\n      x_val = x[i]\n      y_val = y[i]\n      diff += x_val * (y_val - ((m * x_val) + b))\n  m_gradient = -(2/N) * diff  \n  return m_gradient\n\ndef step_gradient(b_current, m_current, x, y, learning_rate):\n    b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n    m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n    b = b_current - (learning_rate * b_gradient)\n    m = m_current - (learning_rate * m_gradient)\n    return [b, m]\n\ndef gradient_descent(x, y, learning_rate, num_iterations):\n  b = 0\n  m = 0\n  for i in range(num_iterations):\n    b, m = step_gradient(b, m, x, y, learning_rate)\n  return [b,m]  \n\n\nI am running it on the following data:\n\na=[3.87656018e+11, 4.10320300e+11, 4.15730874e+11, 4.52699998e+11,\n       4.62146799e+11, 4.78965491e+11, 5.08068952e+11, 5.99592902e+11,\n       6.99688853e+11, 8.08901077e+11, 9.20316530e+11, 1.20111177e+12,\n       1.18695276e+12, 1.32394030e+12, 1.65661707e+12, 1.82304993e+12,\n       1.82763786e+12, 1.85672212e+12, 2.03912745e+12, 2.10239081e+12,\n       2.27422971e+12, 2.60081824e+12]\nb=[3.3469950e+10, 3.4784980e+10, 3.3218720e+10, 3.6822490e+10,\n       4.4560290e+10, 4.3826720e+10, 5.2719430e+10, 6.3842550e+10,\n       8.3535940e+10, 1.0309053e+11, 1.2641405e+11, 1.6313218e+11,\n       1.8529536e+11, 1.7875143e+11, 2.4981555e+11, 3.0596392e+11,\n       3.0040058e+11, 3.1440530e+11, 3.1033848e+11, 2.6229109e+11,\n       2.7585243e+11, 3.0352616e+11]\n\nprint(gradient_descent(a, b, 0.01, 100))\n#result --&gt; [nan, nan]\n\n\nWhen I run the gradient_descent function on a dataset with smaller values, it gives the correct answers. Also I was able to obtain the intercept and slope for the above data with from sklearn.linear_model import LinearRegression\n\nAny help will be appreciated in figuring out why the result is [nan, nan] instead of giving me the correct intercept and slope.\n'
"I am trying to find the right parameters for p,d,q in time series forecasting using SARIMA. I need to forecast house prices for 1000 zip codes. The problem is that grid search takes too much time and I can't manually look at ACF/PACF for each zip code as I need to automate it.  \n\nI tried using grid search for 8 different combinations of parameters and used the best set of params based on AIC.\n\np = d = q = range(0, 2)\n#d = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nparameters = []\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            model = sm.tsa.statespace.SARIMAX(y_new,method='css',\n                                            order=param,\n                                            seasonal_order=param_seasonal,\n                                            enforce_stationarity=False,\n                                            enforce_invertibility=False)\n            results = model.fit()\n            #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue\n        aic = results.aic\n        parameters.append([param,param_seasonal,aic])\nresult_table = pd.DataFrame(parameters)\nresult_table.columns = ['parameters','parameters_seasonal','aic']\n    # sorting in ascending order, the lower AIC is - the better\nresult_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n\n\nI can't get a model which can beat the naive forecast. Can you give me some direction on how to proceed?\n"
"I'm trying to subtract column df['date_of_admission'] from the column df['DOB'] to find the difference between then and store the age value in df['age'] column, however, I'm getting this error:\n\n\n  OverflowError: Overflow in int64 addition\n\n\n DOB          date_of_admission      age\n 2000-05-07   2019-01-19 12:26:00        \n 1965-01-30   2019-03-21 02:23:12        \n NaT          2018-11-02 18:30:10        \n 1981-05-01   2019-05-08 12:26:00       \n 1957-01-10   2018-12-31 04:01:15         \n 1968-07-14   2019-01-28 15:05:09            \n NaT          2018-04-13 06:20:01 \n NaT          2019-02-15 01:01:57 \n 2001-02-10   2019-03-21 08:22:00       \n 1990-03-29   2018-11-29 03:05:03\n.....         ......\n.....         .....\n.....         .....\n\n\nI've tried it with the following:\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import dt\n\ndf['age'] = (df['date_of_admission'] - df['DOB']).dt.days // 365\n\n\nExpected to get the following age column after finding the difference between: \n\nage\n26\n69\nNaN\n58\n.\n.\n.\n\n"
"My data looks like:\nSurvived,Pclass,Name,Sex\n0,3,&quot;Braund, Mr. Owen Harris&quot;,male\n1,1,&quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot;,female\n1,3,&quot;Heikkinen, Miss. Laina&quot;,female\n1,1,&quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot;,female\n0,3,&quot;Allen, Mr. William Henry&quot;,male\n0,3,&quot;Moran, Mr. James&quot;,male,\n\nWhen I'm trying to compare the number of survivors in first class and the gender it is showing me weird results.\nWhen I try do it like this\ndata[(data['Sex']=='female') &amp; (data['Pclass']== 1)]['Survived'].value_counts().plot(kind='bar')\n\n\nplt.legend()\nplt.xticks(np.arange(2), rotation=0)\nplt.title(&quot;Male and female survivors in first class&quot;)\n\nplt.show()\n\nIt shows that almost every female in first class did survive (and that is correct)\nBut when I try do it like this:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv('titanic_data/train.csv')\n\nmales = data[(data['Sex']=='male') &amp; (data['Pclass'] == 1)]['Survived'].value_counts()\nfemales = data[(data['Sex']=='female') &amp; (data['Pclass']== 1)]['Survived'].value_counts()\n\nplt.bar(range(len(females)), females, align='edge', width=-0.4, label='Female', color='red', alpha=0.5)\nplt.bar(range(len(males)), males, align='edge', width=0.4, label='Male', color='blue', alpha=0.5)\n\n\nplt.legend()\nplt.xticks(np.arange(len(males)), rotation=0)\nplt.title(&quot;Male and female survivors in first class&quot;)\n\nplt.show()\n\nIt shows that almost every female died (and that is wrong!)\nThe male results are correct.\n\n\n"
'I have a json file that gives the polygons of the neighborhoods of Chicago. Here is a small sample of the form.\n\n{\'type\': \'Feature\',\n \'properties\': {\'PRI_NEIGH\': \'Printers Row\',\n  \'SEC_NEIGH\': \'PRINTERS ROW\',\n  \'SHAPE_AREA\': 2162137.97139,\n  \'SHAPE_LEN\': 6864.247156},\n \'geometry\': {\'type\': \'Polygon\',\n  \'coordinates\': [[[-87.62760697485339, 41.87437097785366],\n    [-87.6275952566332, 41.873861712441126],\n    [-87.62756611032259, 41.873091933433905],\n    [-87.62755513014902, 41.872801941012725],\n    [-87.62754038267386, 41.87230261598636],\n    [-87.62752573582432, 41.8718067089444],\n    [-87.62751740010017, 41.87152447340544],\n    [-87.62749380061304, 41.87053328991345],\n    [-87.62748640976544, 41.87022285721281],\n    [-87.62747968351987, 41.86986997314866],\n    [-87.62746758964467, 41.86923545315858],\n    [-87.62746178584428, 41.868930955522266]\n\n\nI want to create a dataframe where I have each \'SEC_NEIGH\', linked to the coordinates such that \n\ndf[\'SEC_NEIGH\'] = \'coordinates\'\n\n\nI have tried using a for loop to loop through the dictionaries but when I do so, the dataframe comes out with only showing an \'_\'\n\ndf = {}\nfor item in data:\n    if \'features\' in item:\n        if \'properties\' in item:\n            nn = item.get("properties").get("PRI_NEIGH")\n        if \'geometry\' in item:\n            coords = item.get(\'geometry\').get(\'coordinates\')\n            df[nn] = coords\n\ndf_n=pd.DataFrame(df)\n\n\nI was expecting something where each column would be a separate neighborhood, with only one value, that being the list of coordinates. Instead, my dataframe outputs as a single underscore(\'_\'). Is there something wrong with my for loop?\n'
"\n\nI am wondering how the p value is calculated for various variables in a multiple linear regression. I am sure upon reading several resources that &lt;5% indicates the variable is significant for the model. But how is the p value calculated for each and every variable in the multiple linear regression?\n\nI tried to see the statsmodels summary using the summary() function. I can just see the values. I didn't find any resource on how p value for various variables in a multiple linear regression is calculated.\n\nimport statsmodels.api as sm\nnsample = 100\nx = np.linspace(0, 10, 100)\nX = np.column_stack((x, x**2))\nbeta = np.array([1, 0.1, 10])\ne = np.random.normal(size=nsample)\nX = sm.add_constant(X)\ny = np.dot(X, beta) + e\nmodel = sm.OLS(y, X)\nresults = model.fit() \nprint(results.summary())\n\n\nThis question has no error but requires an intuition on how p value is calculated for various variables in a multiple linear regression.\n"
'I have a dataset with like 100K+ rows, one column on this dataset is a Datetime column, let\'s name it A.\n\nMy Dataset is sorted by column A.\n\nI want to "Fill gaps" of my Dataset, i.e : if i have these two rows following each others :\n\n0  2019-03-13 08:12:20\n1  2019-03-13 08:12:25\n\n\nI want to make add missing seconds between them, as a result, i\'ll have this :\n\n0  2019-03-13 08:12:20\n1  2019-03-13 08:12:21\n2  2019-03-13 08:12:22\n3  2019-03-13 08:12:23\n4  2019-03-13 08:12:24\n5  2019-03-13 08:12:25\n\n\nI don\'t want to generate rows between two rows if they have different day, month or year. \n\nSo if have these two consecutive rows :\n\n0  2019-03-13 08:12:20\n1  2019-03-15 08:12:21\n\n\nI won\'t add anything.\n\nI can\'t also generate rows if the time difference between my two rows is greater than 2 hours.\n\nSo if have these two consecutive rows :\n\n0  2019-03-13 08:12:20\n1  2019-03-15 11:12:21\n\n\nI won\'t add anything.\n\nHere\'s an example to illustrate what i want :\n\ndf=pd.DataFrame({\'A\': ["2019-03-13 08:12:20", "2019-03-13 08:12:25", "2019-03-20 08:17:23", "2019-03-22 08:17:25", "2019-03-22 11:12:20", "2019-03-22 11:12:23", "2019-03-24 12:33:23"]})\n                     A\n0  2019-03-13 08:12:20\n1  2019-03-13 08:12:25\n2  2019-03-20 08:17:23\n3  2019-03-22 08:17:25\n4  2019-03-22 11:12:20\n5  2019-03-22 11:12:23\n6  2019-03-24 12:33:23\n\n\nAt the end, i want to have this result :\n\n                      A\n0   2019-03-13 08:12:20\n1   2019-03-13 08:12:21\n2   2019-03-13 08:12:22\n3   2019-03-13 08:12:23\n4   2019-03-13 08:12:24\n5   2019-03-13 08:12:25\n6   2019-03-20 08:17:23\n7   2019-03-22 08:17:25\n8   2019-03-22 11:12:20\n9   2019-03-22 11:12:21\n10  2019-03-22 11:12:22\n11  2019-03-22 11:12:23\n12  2019-03-24 12:33:23\n\n\nI tried with this :\n\nimport pandas as pd\n\ndf=pd.DataFrame({\'A\': ["2019-03-13 08:12:20", "2019-03-13 08:12:25", "2019-03-20 08:17:23", "2019-03-22 08:17:25", "2019-03-22 11:12:20", "2019-03-22 11:12:23", "2019-03-24 12:33:23"]})\ndf[\'A\']=pd.to_datetime(df[\'A\'])\nfill = [pd.date_range(df.iloc[i][\'A\'], df.iloc[i+1][\'A\'], freq=\'S\') for i in range(len(df)-1) if (df.iloc[i+1][\'A\']-df.iloc[i][\'A\']).total_seconds()&lt;=7200]\ndates = [item for sublist in fill for item in sublist]\ndf=df.set_index(\'A\').join(pd.DataFrame(index=pd.Index(dates, name=\'A\')), how=\'outer\').reset_index()\nprint(df)\n\n\nIt\'s doing the job, but it\'s slow, is there any faster way to do this ?\n'
'I am given that someone playing Dungeons and Dragons (D&amp;D) rolls a fair die with values 1-20 (one value per side) and that depending on the character I guess there is a modifier for each action that the character can make. In this case the modifier is 11 for opening a door. If the dice roll + modifier is greater than 15, the action is successful and the character opens the door if not, the action fails. This part of the question wants us to make an array of seven dice rolls and the score for that character with the modifier of 11.\n\nThis is what I have tried so far\n\nimport numpy as np\nfrom datascience import *\n\nmodifier = 11\npossible_rolls = np.arange(20)\nroll_result = np.random.choice(possible_rolls)\nmodified_result = roll_result + modifier\nnum_observations = 7\n\ndef simulate_observations():\n    """Produces an array of 7 simulated modified die rolls"""\n    possible_rolls = np.arange(20)\n    roll_result = np.random.choice(possible_rolls)\n    modified_result = roll_result + modifier\n    array = make_array()\n    for i in np.arange(num_observations):\n        array = np.append(array, modified_result)\n    return array\n\nobservation_array = simulate_observations()\nprint(observation_array)\n\n\nI\'m expecting to get a various range of outputs based on a random roll of the dice and then adding that value to the modifier, then finally placing that final value in to the array named array but all I am getting is an array that looks like [20.,20.,20.,20.,20.,20.,20.]. Any ideas as to where I may be off? I\'m 90% sure my issue is in my for loop as I have not quite grasped what exactly I am doing in them but I can\'t seem to pin down exactly what the problem is.\n'
"I am working with a large csv file that has information that looks something like\n\nid      year   decade  code  type\n3366    2014    2010    EM  Chemical\n3366    2014    2010    EM  Chemical\n3366    2014    2010    EM  Chemical\n3366    2014    2010    EM  Chemical\n3366    2014    2010    EM  Chemical\n427     1972    1970    DR  Coastal Storm\n337     1972    1970    DR  Coastal Storm\n337     1972    1970    DR  Coastal Storm\n\n\nI would like to sort by the number of unique occurrences in the 'id' column. My desired result would look something like\n\nid      year   decade  code  type          count\n3366    2014    2010    EM  Chemical        5\n427     1972    1970    DR  Coastal Storm   1\n337     1972    1970    DR  Coastal Storm   2\n\n\nHowever I was trying to settle for something like\n\nid      year   decade  code  type           count\n3366    2014    2010    EM  Chemical        5\n3366    2014    2010    EM  Chemical        5\n3366    2014    2010    EM  Chemical        5\n3366    2014    2010    EM  Chemical        5\n3366    2014    2010    EM  Chemical        5\n427     1972    1970    DR  Coastal Storm   1\n337     1972    1970    DR  Coastal Storm   1\n337     1972    1970    DR  Coastal Storm   2\n\n\nI attempted to do this by trying\n\ndf['count']=df.groupby('id').transform('count')\n\n\nBut I keep getting an error\n\nValueError: Wrong number of items passed 18, placement implies 1\n\n\nIs there a better way to go about accomplishing this?\n"
'If I use\n\nl = zip((1,2), (3,4))\nprint(l)\n\n\nthis does not print the output , why do I have to use\n\nprint(list(l))\n\n\nAlso what does below code mean, it assigns data to x and y but how\n\nl = zip((1,2), (3,4)) \nx, y = zip(*l)\n\n'
'I am implemented a KNN algorithm in python.\n\nimport math\n\n            #height,width,deepth,thickness,Label\ndata_set = [(2,9,8,4, "Good"),\n            (3,7,7,9, "Bad"),\n            (10,3,10,3, "Good"),\n            (2,9,6,10, "Good"),\n            (3,3,2,5, "Bad"),\n            (2,8,5,6, "Bad"),\n            (7,2,3,10, "Good"),\n            (1,10,8,10, "Bad"),\n            (2,8,1,10, "Good")\n            ]\n\n\nA = (3,2,1,5)\nB = (8,3,1,2)\nC = (6,10,8,3)\nD = (9,6,4,1)\n\n\ndistances = []\nlabels = []\n\ndef calc_distance(datas,test):\n    for data in datas:\n        distances.append(\n            ( round(math.sqrt(((data[0] - test[0])**2 + (data[1] - test[1])**2 + (data[2] - test[2])**2 + (data[3] - test[3])**2)), 3), data[4] )) \n    return distances\n\ndef most_frequent(list1): \n    return max(set(list1), key = list1.count) \n\ndef get_neibours(k):\n    distances.sort()\n    print(distances[:k])\n    for distance in distances[:k]:\n        labels.append(distance[1])\n    print("It can be classified as: ", end="")\n    print(most_frequent(labels))\n\n\n\ncalc_distance(data_set,D)\nget_neibours(7)\n\ncalc_distance(data_set,D)\nget_neibours(7)\n\n\nI works well mostly and I get the correct label. For example for D, i do get the label "Good". However i discovered a bug that when I call it twice for example:\n\n calc_distance(data_set,D)\nget_neibours(7)\n\ncalc_distance(data_set,D)\nget_neibours(7)\n\n\nand I run it few times, i get different outputs- "Good" and "Bad" when I run the program couple of times.. \n\nThere must be a bug somewhere I am unable to find out.\n'
"I'd like to ask for help for my problem. So, I have this dataframe with two columns and have a huge dataset of about 9500~ rows with 2 columns. Sometimes I have to take a subset from column A, sometimes from B - depending on the RegEx. But I have more than two of them (RegEx) but they are kinda unique. The result should be written into a third column with the 'right' value. It must be done with RegEx.\n\nI hope I can make it more clear with this (small) example:\n\nInput: \n\ndf = pd.DataFrame({'A': ['No animal', 'No animal', 'Zoo One', 'Zoo Two', 'Me-Lo-N', 'Ap-Pl-E'], 'B': ['EE.Elephant', 'SS.Penguin', 'EE.Elephant', 'SS.Penguin', 'GB One', 'GB Two']})\n\n&gt;&gt;&gt; df\n           A            B\n0  No animal  EE.Elephant\n1  No animal   SS.Penguin\n2    Zoo One  EE.Elephant\n3    Zoo Two   SS.Penguin\n4    Me-Lo-N       GB One\n5    Ap-Pl-E       GB Two\n\n\nNow I 'identified' several patterns.\n\n\nIf in column 'A' is 'No animal', take the value from column 'B' no matter what.\nIf in column 'A' is 'Zoo ...' and in column 'B' something like 'XX.Animalname', take the left value from 'A' (Zoo ...)\nIf in column 'A' is something like 'XX-YY-Z' and in column 'B' 'GB ...', take the value/s from column 'A'.\n\n\nThe output should look like:\n\n           A            B            C\n0  No animal  EE.Elephant  EE.Elephant\n1  No animal   SS.Penguin   SS.Penguin\n2    Zoo One  EE.Elephant      Zoo One\n3    Zoo Two   SS.Penguin      Zoo Two\n4    Me-Lo-N       GB One      Me-Lo-N\n5    Ap-Pl-E       GB Two      Ap-Pl-E\n\n\nI built follow RegEx for them:\n\n\n(No animal)\n(\\w{2}..*) f.e. for EE. Bla\n(Zoo.*) f.e. for Zoo...\n(\\w{2}-.+-.+) f.e. for Me-Lo-N\n(GB.+) ...\n\n\nThat's it. What's the best approach to compare specific RegEx to eachother between two columns and paste the answer into a own column?\n\nReally appreciated! Thank you!  \n"
"So I have a dataframe that consists of a datetime index and a boolean column;\nis there an effective (non-loop) way of doing the following operation:\n\nfor each row, find if any of the boolean values in the target column is True within the next 1 hour timeframe.\n\nFor example:\n\n\n\nNow I have a loop for this, but will be grateful for any suggestions on how to improve it.\n\npd.DataFrame({'target': {Timestamp('2019-03-08 10:02:24.705000'): False,\nTimestamp('2019-03-08 12:55:21.586000'): False,\nTimestamp('2019-03-08 13:01:36.574000'): True,\nTimestamp('2019-03-08 18:13:18.041000'): False,\nTimestamp('2019-03-08 18:54:29.286000'): False,\nTimestamp('2019-03-09 10:16:55.969000'): False,\nTimestamp('2019-03-09 13:00:41.357000'): False,\nTimestamp('2019-03-09 15:51:43.340000'): False,\nTimestamp('2019-03-09 16:37:31.487000'): True,\nTimestamp('2019-03-10 14:20:40.158000'): False,\nTimestamp('2019-03-10 15:12:52.386000'): False,\nTimestamp('2019-03-11 08:11:57.956000'): False,\nTimestamp('2019-03-11 12:17:35.041000'): False,\nTimestamp('2019-03-11 13:23:43.058000'): False,\nTimestamp('2019-03-11 20:31:03.062000'): False,\nTimestamp('2019-03-11 20:55:30.677000'): False,\nTimestamp('2019-03-12 10:38:44.177000'): False,\nTimestamp('2019-03-12 12:25:37.269000'): False,\nTimestamp('2019-03-12 13:40:54.494000'): False,\nTimestamp('2019-03-12 18:49:28.487000'): False,\nTimestamp('2019-03-13 09:58:23.547000'): True}})\n\n\nEDIT: added dataframe sample for ease of copy/paste and recreating it in Python\n"
"I have a dataframe:\n\ndf = \ncol1  col2  col3 \n1      2     3\n1      4     6\n3      7     2\n\n\nI want to edit df, such that when the value of col1 is smaller than 2 , take the value from col3.\n\nSo I will get:\n\nnew_df = \ncol1  col2  col3 \n3      2     3\n6      4     6\n3      7     2\n\n\nI tried to use assign and df.loc but it didn't work.\n\nWhat is the best way to do so?\n"
"i have 2 pandas dataframes with same columns [id, value].\n\nI want to create a new dataframe extracting 200 values for each id taking instances from the first df1 and if it hasn't enough values i should go to the second df2 to take remaining values. \n\nHow can i do? Thanks\n"
'I am working on workflows using Pipeline and GridSearchCV. \n\nMWE for RandomForest, as below,\n\n#################################################################\n# Libraries\n#################################################################\nimport time\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n#################################################################\n# Data loading and Symlinks\n#################################################################\ntrain = pd.read_csv("data_train.csv")\ntest = pd.read_csv("data_test.csv")\n\n#################################################################\n# Train Test Split\n#################################################################\n# Selected features - Training data\nX = train.drop(columns=\'fault_severity\')\n\n# Training data\ny = train.fault_severity\n\n# Test data\nx = test\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n#################################################################\n# Pipeline\n#################################################################\npipe_rf = Pipeline([\n    (\'clf\', RandomForestClassifier(random_state=0))\n    ])\n\nparameters_rf = {\n        \'clf__n_estimators\':[30,40], \n        \'clf__criterion\':[\'entropy\'], \n        \'clf__min_samples_split\':[15,20], \n        \'clf__min_samples_leaf\':[3,4]\n    }\n\ngrid_rf = GridSearchCV(pipe_rf,\n    param_grid=parameters_rf,\n    scoring=\'neg_mean_absolute_error\',\n    cv=5,\n    refit=True) \n\n#################################################################\n# Modeling\n#################################################################\nstart_time = time.time()\n\ngrid_rf.fit(X_train, y_train)\n\n#Calculate the score once and use when needed\nmae = grid_rf.score(X_valid,y_valid)\n\nprint("Best params                        : %s" % grid_rf.best_params_)\nprint("Best training data MAE score       : %s" % grid_rf.best_score_)    \nprint("Best validation data MAE score (*) : %s" % mae)\nprint("Modeling time                      : %s" % time.strftime("%H:%M:%S", time.gmtime(time.time() - start_time)))\n\n#################################################################\n# Prediction\n#################################################################\n#Predict using the test data with selected features\ny_pred = grid_rf.predict(x)\n\n# Transform numpy array to dataframe\ny_pred = pd.DataFrame(y_pred)\n\n# Rearrange dataframe\ny_pred.columns = [\'prediction\']\ny_pred.insert(0, \'id\', x[\'id\'])\n\n# Save to CSV\ny_pred.to_csv("data_predict.csv", index = False, header=True)\n#Output\n# id,prediction\n# 11066,0\n# 18000,2\n# 16964,0\n# ...., ....\n\n\nHave a MWE for XGBoost as below,\n\n#################################################################\n# Libraries\n#################################################################\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#################################################################\n# Data loading and Symlinks\n#################################################################\ntrain = pd.read_csv("data_train.csv")\ntest = pd.read_csv("data_test.csv")\n\n#################################################################\n# Train Test Split\n#################################################################\n\n# Selected features - Training data\nX = train.drop(columns=\'fault_severity\')\n\n# Training data\ny = train.fault_severity\n\n# Test data\nx = test\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n#################################################################\n# DMatrix\n#################################################################\ndtrain = xgb.DMatrix(data=X_train, label=y_train)\ndtest = xgb.DMatrix(data=test)\n\nparams = {\n    \'max_depth\': 6,\n    \'objective\': \'multi:softprob\',  # error evaluation for multiclass training\n    \'num_class\': 3,\n    \'n_gpus\': 0\n}\n\n#################################################################\n# Modeling\n#################################################################\nstart_time = time.time()\nbst = xgb.train(params, dtrain)\n\n#################################################################\n# Prediction\n#################################################################\n#Predict using the test data with selected features\ny_pred = bst.predict(dtest)\n\n# Transform numpy array to dataframe\ny_pred = pd.DataFrame(y_pred)\n\n# Rearrange dataframe\ny_pred.columns = [\'prediction_0\', \'prediction_1\', \'prediction_2\']\ny_pred.insert(0, \'id\', x[\'id\'])\n\n# Save to CSV\ny_pred.to_csv("data_predict_xgb.csv", index = False, header=True)\n# Expected Output:\n# id,prediction_0,prediction_1,prediction_2\n# 11066,0.4674369,0.46609518,0.06646795\n# 18000,0.7578633,0.19379888,0.048337903\n# 16964,0.9296321,0.04505246,0.025315404\n# ...., ...., ...., ....\n\n\nQuestions:\n\n\nHow does one convert the MWE for XGBoost using the Pipeline and GridSearchCV technique in MWE for RandomForest? Have to use \'num_class\' where XGBRegressor() does not support.\nHow to have a multi-class prediction output for RandomForrest as XGBoost (i.e predict_0, predict_1, predict_2)? The sample output are given in the MWEs above. I found num_class is is not supported by RandomForest Classifier.\n\n\nI have spent several days working on this and still been blocked. Appreciate some pointers to move forward.\n\nData:\n\n\ndata_train: https://www.dropbox.com/s/bnomyoidkcgyb2y/data_train.csv\ndata_test: https://www.dropbox.com/s/kn1bgde3hsf6ngy/data_test.csv\n\n'
"i am trying to train completely independent tasks using multiprocess pooling in python, which lightgbm for training(i am not sure if this is relevant for problem). Here is the code \n\nfrom sklearn.datasets import load_breast_cancer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nimport numpy as np\n\ndef functionToParallize(splitnumber=2):\n\n    data = load_breast_cancer()\n    X = pd.DataFrame(data.data, columns=data.feature_names)\n    y = pd.Series(data.target)\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    folds = KFold(splitnumber)\n    results = lgb.cv({}, lgb.Dataset(X_train, y_train), folds=folds,metrics=['rmse'])\n    return results\n\n\n\nto parallelize i use multiprocessing pooling with say 2 pool workers. But the it's really inefficient as it takes 1000 times more time to finish a task with 2 pools as it takes with just one. For eg.\n\nfrom multiprocessing import Pool \nimport psutil\nprint(psutil.cpu_count())\n\n\noutput\n\n\n  4\n\n\nstarttime=time.time()\npool = Pool(2)\nmultiple_results = [pool.apply_async(functionToParallize) for i in [3]]\np=[res.get() for res in multiple_results]\nprint((time.time()-starttime)/60)\n\n\noutput\n\n\n  0.007067755858103434\n\n\nbut with two pools\n\nstarttime=time.time()\npool = Pool(2)\nmultiple_results = [pool.apply_async(functionToParallize) for i in [2,3]]\np=[res.get() for res in multiple_results]\nprint((time.time()-starttime)/60)\n\n\n\n  1.026989181836446\n\n\nThis is actually not the original task, but i am doing something similar there. But for that single task takes about a minute and pool 2 process never ends there at all.\nAm i doing something wrong here ?? I am doing this on a jupyter notebook, If that's relevant. \n\nAny help is appreciated! Thanks!\n"
"I have used Tensorflow-GPU for object detection on my laptop.\nNow the management team wants to check it with URL at its own place.\nI never published/deployed the model on the web as I am not python developer but now I have to do that.For that I tried to go through some online tutorials for Flask but they weren't that helpful. \n\nHow can I publish the model using Flask API?\nAre there any guidance/blog/video to deploy the Object detection model on URL using Flask?\n\nmy project structure is something like this \n\n\n"
'I have a case where I have label names as columns of a DataFrame with value 0 or more like below.\n\n.net    2007    actionscript-3  activerecord    air ajax\n0   0   0   0   1   1   1\n1   0   0   0   1   1   1\n2   0   0   0   1   1   1\n3   2   2   2   2   0   0\n4   2   2   2   2   0   0\n5   2   2   2   2   0   0\n\n\nMy requirement is to put those column names whose value is more than zero, in a single column seperated by space for each row like below :\n\n0   activerecord air ajax\n1   activerecord air ajax\n2   activerecord air ajax\n3   .net 2007 actionscript-3 activerecord\n4   .net 2007 actionscript-3 activerecord\n5   .net 2007 actionscript-3 activerecord\n\n\nExample : values in columns activerecord, air, ajax has value of 1 in first row so these 3 should appear in one column of the dataframe separated by space.\n\nThis is an example of multi-label classification.\n'
"What I am trying:\n\nimport re\nnew_df = census_df.loc[(census_df['REGION']==1 | census_df['REGION']== 2) &amp; (census_df['CTYNAME'].str.contains('^Washington[a-z]*'))&amp; (census_df['POPESTIMATE2015']&gt;census_df['POPESTIMATE2014'])]\nnew_df\n\n\nIt returns this error:\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n"
"I have a list of index values. The values in the list can be repeated multiple times.\nindex_list = [1,3,4,3,2,20,55,30,45]\n\nI want to query a pandas dataframe and extract the the values with indexes matching the values in the index_list\n\nIf the same index is present multiple times I want to extract the value multiple times.\nThe order of selected value must match the order of the index_list.\n\nThe index_list can be pretty long (100000) and the df quite small (400 rows, 2 columns)\nThe solution I used is based on a loop:\nall_selected_values = []\nfor idx in index_list:\n  all_selected_values.append(df.loc[df.index == idx,'selected_column'].values[0])\n\nThis works but can be quite slow.\nIs there a more efficient way to do it?\nThanks!\n"
"I have the following data in a .csv file:\nOriginal Format\nI need to reshape the above mentioned data into the following format:\nRequired Format\nCan someone kindly explain how can I do that? I have an idea it can be done using df.pivot() function, however, the dates are in the 'headings' of the original format, which is confusing me a bit.\nI shall be grateful for the help.\nThank you\n"
'I have deployed an area chart using Streamlit for Python. Is it possible to change the labels for the axis X, as well as the labels for each data point plotted?\nimport streamlit as st\nimport pandas as pd\nst.write(&quot;&quot;&quot; My area chart &quot;&quot;&quot;)\ndf = pd.read_csv(&quot;my_data.csv&quot;)\nst.area_chart(df)\n\n'
"I have a large .csv file that has 11'000'000 rows and 3 columns: id ,magh , mixid2.\nWhat I have to do is to select the rows with the same id and then check if these rows have the same mixid2; if True I remove the rows, If False I initialize a class with the information of the selected rows.\nThat is my code:\nobs=obs.set_index('id')\nobs=obs.sort_index()\n#dropping elements with only one mixid2 and filling S\nID=obs.index.unique()\nS=[]\ngood_bye_list = []\nfor i in tqdm(ID):\n    app=obs.loc[i]\n    if len(np.unique([app['mixid2'],])) != 1:\n        #fill the class list\n        S.append(star(app['magh'].values,app['mixid2'].values,z_in))\n    else :\n    #drop\n        good_bye_list.append(i)\n\nobs=obs.drop(good_bye_list) \n\nThe .csv file is very large so it takes 40 min to compute everything.\nHow can I improve the speed??\nThank you for the help.\nThis is the .csv file:\nid,mixid2,magh\n3447001203296326,557,14.25\n3447001203296326,573,14.25\n3447001203296326,525,14.25\n3447001203296326,541,14.25\n3447001203296330,540,15.33199977874756\n3447001203296330,573,15.33199977874756\n3447001203296333,172,17.476999282836914\n3447001203296333,140,17.476999282836914\n3447001203296333,188,17.476999282836914\n3447001203296333,156,17.476999282836914\n3447001203296334,566,15.626999855041506\n3447001203296334,534,15.626999855041506\n3447001203296334,550,15.626999855041506\n3447001203296338,623,14.800999641418455\n3447001203296338,639,14.800999641418455\n3447001203296338,607,14.800999641418455\n3447001203296344,521,12.8149995803833\n3447001203296344,537,12.8149995803833\n3447001203296344,553,12.8149995803833\n3447001203296345,620,12.809000015258787\n3447001203296345,543,12.809000015258787\n3447001203296345,636,12.809000015258787\n3447001203296347,558,12.315999984741213\n3447001203296347,542,12.315999984741213\n3447001203296347,526,12.315999984741213\n3447001203296352,615,12.11299991607666\n3447001203296352,631,12.11299991607666\n3447001203296352,599,12.11299991607666\n3447001203296360,540,16.926000595092773\n3447001203296360,556,16.926000595092773\n3447001203296360,572,16.926000595092773\n3447001203296360,524,16.926000595092773\n3447001203296367,490,15.80799961090088\n3447001203296367,474,15.80799961090088\n3447001203296367,458,15.80799961090088\n3447001203296369,639,15.175000190734865\n3447001203296369,591,15.175000190734865\n3447001203296369,623,15.175000190734865\n3447001203296369,607,15.175000190734865\n3447001203296371,460,14.975000381469727\n3447001203296373,582,14.532999992370605\n3447001203296373,614,14.532999992370605\n3447001203296373,598,14.532999992370605\n3447001203296374,184,14.659000396728516\n3447001203296374,203,14.659000396728516\n3447001203296374,152,14.659000396728516\n3447001203296374,136,14.659000396728516\n3447001203296374,168,14.659000396728516\n3447001203296375,592,14.723999977111815\n3447001203296375,608,14.723999977111815\n3447001203296375,624,14.723999977111815\n3447001203296375,92,14.723999977111815\n3447001203296375,76,14.723999977111815\n3447001203296375,108,14.723999977111815\n3447001203296375,576,14.723999977111815\n3447001203296376,132,14.0649995803833\n3447001203296376,164,14.0649995803833\n3447001203296376,180,14.0649995803833\n3447001203296376,148,14.0649995803833\n3447001203296377,168,13.810999870300293\n3447001203296377,152,13.810999870300293\n3447001203296377,136,13.810999870300293\n3447001203296377,184,13.810999870300293\n3447001203296378,171,13.161999702453613\n3447001203296378,187,13.161999702453613\n3447001203296378,155,13.161999702453613\n3447001203296378,139,13.161999702453613\n3447001203296380,565,13.017999649047852\n3447001203296380,517,13.017999649047852\n3447001203296380,549,13.017999649047852\n3447001203296380,533,13.017999649047852\n3447001203296383,621,13.079999923706055\n3447001203296383,589,13.079999923706055\n3447001203296383,605,13.079999923706055\n3447001203296384,541,12.732000350952148\n3447001203296384,557,12.732000350952148\n3447001203296384,525,12.732000350952148\n3447001203296385,462,12.784000396728516\n3447001203296386,626,12.663999557495115\n3447001203296386,610,12.663999557495115\n3447001203296386,577,12.663999557495115\n3447001203296389,207,12.416000366210938\n3447001203296389,255,12.416000366210938\n3447001203296389,223,12.416000366210938\n3447001203296389,239,12.416000366210938\n3447001203296390,607,12.20199966430664\n3447001203296390,591,12.20199966430664\n3447001203296397,582,16.635000228881836\n3447001203296397,598,16.635000228881836\n3447001203296397,614,16.635000228881836\n3447001203296399,630,17.229999542236328\n3447001203296404,598,15.970000267028807\n3447001203296404,631,15.970000267028807\n3447001203296404,582,15.970000267028807\n3447001203296408,540,16.08799934387207\n3447001203296408,556,16.08799934387207\n3447001203296408,524,16.08799934387207\n3447001203296408,572,16.08799934387207\n3447001203296409,632,15.84000015258789\n3447001203296409,616,15.84000015258789\n\n"
"I am trying to clean my dataset. It has few columns which have all String/float/int (for name/pay/overtimepay) values but few rows contain 'Not provided' as a value.\nnewData = data[~data['BasePay'].isin(['Not Provided'])]\nnewData = data[~data['BasePay'].str.contains('Not Provided', na=False)]\nprint(newData['BasePay'].mean())\n\nand\ndf = data.select_dtypes(object)\nmask = ~df.apply(lambda series: series.str.contains('Not Provided')).any(axis=1)\nno_eco = data[mask]\n\nto clean the column of 'BasePay' and calculate mean but failed. I'd appreciate if you could help. Image of the concerned rows in my dataset\n"
"I don't understand why one has to use the fit_transform method when the transform method can give the same the output as using only fit transform method, whats the whole point of fit method?\nI have printed the x_train and x_test, both of them gave similar output.\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\nx_test[:, 3:] = sc.transform(x_test[:, 3:])\n\n"
"I'm trying to use featuretools to calculate time-series functions. Specifically, I'd like to subtract current(x) from previous(x) by a group-key (user_id), but I'm having trouble in adding this kind of relationship in the entityset.\ndf = pd.DataFrame({\n    &quot;user_id&quot;: [i % 2 for i in range(0, 6)],\n    'x': range(0, 6),\n    'time': pd.to_datetime(['2014-1-1 04:00', '2014-1-1 05:00', \n                            '2014-1-1 06:00', '2014-1-1 08:00', '2014-1-1 10:00', '2014-1-1 12:00'])\n     })\n\nprint(df.to_string())\n       user_id  x                time\n0        0      0 2014-01-01 04:00:00\n1        1      1 2014-01-01 05:00:00\n2        0      2 2014-01-01 06:00:00\n3        1      3 2014-01-01 08:00:00\n4        0      4 2014-01-01 10:00:00\n5        1      5 2014-01-01 12:00:00\n\n\nes = ft.EntitySet(id='test')\nes.entity_from_dataframe(entity_id='data', dataframe=df,\n                         variable_types={\n                             'user_id': ft.variable_types.Categorical,\n                             'x': ft.variable_types.Numeric,\n                             'time': ft.variable_types.Datetime\n                         },\n                         make_index=True, index='index',\n                         time_index='time'\n                         )\n\nI then try to invoke dfs, but I can't get the relationship right...\nfm, fl = ft.dfs(\n    target_entity=&quot;data&quot;,\n    entityset=es,\n    trans_primitives=[&quot;diff&quot;]\n)\nprint(fm.to_string())\n       user_id  x  DIFF(x)\nindex                     \n0            0  0      NaN\n1            1  1      1.0\n2            0  2      1.0\n3            1  3      1.0\n4            0  4      1.0\n5            1  5      1.0\n\nBut what I'd actually want to get is the difference by user. That is, from the last value for each user:\n       user_id  x  DIFF(x)\nindex                     \n0            0  0      NaN\n1            1  1      NaN\n2            0  2      2.0\n3            1  3      2.0\n4            0  4      2.0\n5            1  5      2.0\n\nHow do I get this kind of relationship in featuretools? I've tried several tutorial but to no avail. I'm stumped.\nThanks!\n"
"I'd like to perform a hyperparameter search for selecting preprocessing steps and models in sklearn as follows:\npipeline = Pipeline([(&quot;combiner&quot;, PolynomialFeatures()),\n                     (&quot;dimred&quot;, PCA()),\n                     (&quot;classifier&quot;, RandomForestClassifier())])\n\nparameters = [{&quot;combiner&quot;: [None]},\n              {&quot;combiner&quot;: [PolynomialFeatures()], &quot;combiner__degree&quot;: [2], &quot;combiner__interaction_only&quot;: [False, True]},\n\n              {&quot;dimred&quot;: [None]},\n              {&quot;dimred&quot;: [PCA()], &quot;dimred__n_components&quot;: [.95, .75]},\n\n              {&quot;classifier&quot;: [RandomForestClassifier(n_estimators=100, class_weight=&quot;balanced&quot;)],\n               &quot;classifier__max_depth&quot;: [5, 10, None]},\n              {&quot;classifier&quot;: [KNeighborsClassifier(weights=&quot;distance&quot;)],\n               &quot;classifier__n_neighbors&quot;: [3, 7, 11]}]\n\nCV = GridSearchCV(pipeline, parameters, cv=5, scoring=&quot;f1_weighted&quot;, refit=True, n_jobs=-1)\nCV.fit(train_X, train_y)\n\nOf course, I need the results with the best pipeline with the best parameters. However, when I request best estimators with CV.best_estimator_ I get only the winning components, not the hyperparameters:\nPipeline(steps=[('combiner', None), ('dimred', PCA()),\n                ('classifier', RandomForestClassifier())])\n\nWhen I print out the CV.best_params_, I get an even shorter info (only with the first element of the Pipeline, the combiner, no info about dimred, classifier whatsoever):\n{'combiner': None}\n\nHow could I get the best pipeline combination with components and their hyperparameters?\n"
'I would like to be able to perform operations on specific elements in a 2D array, if it meets a criterion. in the below example the code makes any value &lt; 0.5 = 0.\nDoes anyone know of a simple way of doing this? Below is my code, but im sure there is a simpler way.\nimport numpy as np\n\nx = 5\ny = 5\n\nx3 = np.random.rand(x,y)\n\ndef choice(arr):\n    row = -1\n    column = -1\n    for i in arr:\n        row += 1\n        for j in i:\n            column += 1\n            if j &gt;= 0.5:\n                arr[row,column] = j\n            else:\n                arr[row,column] = 0\n                \n            if column == y - 1:\n                column = -1\n    \n    return arr\n\ny3 = choice(x3.copy())\n\n'
'i read in a csv file with data. Everything works fine.\nI can do a task like\ndf.loc[(df[&quot;BID&quot;] == 7249)\n\nBut I would like to do the same with &quot;Testschritt&quot; like\ndf.loc[(df[&quot;Testschritt&quot;] == &quot;F1&quot;)\n\nBut all my entrys are false. But you can clearly see, that some entrys should be true.\nI use the jupyter notebook.\nHere the full code:\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndf = pd.read_csv(&quot;&lt;FILELOCATION&gt;&quot;, &quot;;&quot;)\ndf.loc[(df[&quot;Testschritt&quot;] == &quot;F1&quot;)\n\nOutput see in attached picturesOutput 1\nAnd here Output 2\nPlease give advise, Thank you\n'
"I have a data frame df_ss_g as\nent_id,WA,WB,WC,WD\n123,0.045251836,0.614582906,0.225930615,0.559766482\n124,0.722324239,0.057781167,,0.123603561\n125,,0.361074325,0.768542766,0.080434134\n126,0.085781742,0.698045853,0.763116684,0.029084545\n127,0.909758657,,0.760993759,0.998406211\n128,,0.32961283,,0.90038336\n129,0.714585519,,0.671905291,\n130,0.151888772,0.279261613,0.641133263,0.188231227\n\nnow I have to compute the average(AVG_WEIGHTAGE) which is based on a weightage i.e. =(WA*0.5+WB*1+WC*0.5+WD*1)/(0.5+1+0.5+1)\nbut while I am computing it using below method i.e.\ndf_ss_g['AVG_WEIGHTAGE']= df_ss_g.apply(lambda x:((x['WA']*0.5)+(x['WB']*1)+(x['WC']*0.5)+(x['WD']*1))/(0.5+1+0.5+1) , axis=1)\n\nIT output as i.e. for NaN value it is giving NaN as AVG_WEIGHTAGE as null which is wrong.\n\nall I wanted is that null should not be considered in denominator and numerator\ne.g.\nent_id,WA,WB,WC,WD,AVG_WEIGHTAGE\n128,,0.32961283,,0.90038336,0.614998095   i.e. (WB*1+WD*1)/1+1\n129,0.714585519,,0.671905291,,0.693245405 i.e. (WA*0.5+WC*0.5)/0.5+0.5\n\n"
"I have a dataframe containing leads (names).\nI am trying to search the web for relevant data regarding those leads.\nI am using beautifulsoup and urllib to scrape the data.\nThe url looks like this :\nurl = u'https://www.website.com/SearchResults?query=' + quote(str(df['name']))\n\nThe problem is that for each lead i get the exact same data, which is the data for the last lead in the dataframe of which data was retrieved.\nwhenever i use a string name instead of str(df['name']), i get for the specific lead the right data, and it looks like this :\nurl = u'https://www.website.com/SearchResults?query=' + quote('this+is+a+leads+name')\n\nThe reason i think the problem is specifically related to str(df['name']) is because whenever i remove it, i successfuly aquire data, otherwise, i get for 100,000 leads the same data. Only problem is, in order to use the leads from the dataframe i need to use str.\n"
'There is a DataFrame like this:\n           cost\n0   8762.000000\n1   -1\n2   7276.000000\n3   9574.000000\n4   -1\n..          ...\n59  5508.000000\n60  7193.750000\n61  5927.333333\n62  -1\n63  4972.000000\n\nThe -1 is the exception value in this case, so how to replace -1 with NaN. And then how to interpolate NaN for replacement.\nAfter that, the DataFrame was cleaned.But there may be some abnormal high and low values of the DataFrame, and then how to interpolate abnormal high and low values for replacement.\n'
"I'm trying to replicate R's fitdist() results (reference, cannot modify R code) in Python using scipy.stats. The results are quite close but still different (difference is at not acceptable level). Does anybody know why the results are different? How can I reduce the difference between the results?\nscipy_stats.weibull_min definition (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.weibull_min.html) seems to be the same as R's weibull (https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Weibull.html.\nData example:\ndata = [2457.145, 878.081, 855.118, 1157.135, 1099.82]\n\nR:\nparameters &lt;- fitdist(data, 'weibull',&quot;mle&quot;)$estimate\n\nR Results:\n     shape      scale \n   2.30804 1463.88528\n\nPython:\nimport scipy.stats as st\nst.weibull_min.fit(data, floc=0)\n\nPython results:\n(2.307899817944195, 0, 1463.7712925885176)\n\n"
'How can we split month column into different columns?\nSample data:\n    EmployeeId  City     join_month\n0   001        Mumbai        1\n1   001        Bangalore     3\n2   002        Pune          2\n3   002        Mumbai        6\n4   003        Delhi         9\n5   003        Mumbai        12\n6   004        Bangalore     11\n7   004        Pune          10\n8   005        Mumbai         5\n\nNeed an output like\n    EmployeeId  City     join_month    join_month_jan    jan_count  \n0   001        Mumbai        1                 1/True         1 \n1   001        Bangalore     3                 0/False      \n2   002        Pune          2                 0/False          \n3   002        Mumbai        6\n4   003        Delhi         9\n5   003        Mumbai        12\n6   004        Bangalore     11\n7   004        Pune          10\n8   005        Mumbai         5\n\n'
"I have a dataframe and am trying to calculate the time difference between two different topics while remaining within a call and not spilling over into a new call (i.e while ensuring it's not working out the time difference between topics in different calls). Where the interaction_id is a seperate call\nThis is an example Dataframe\ndf = pd.DataFrame([[1, 2, 'Cost'], [1, 5.72, NaN], [1, 8.83, 'Billing'], [1, 12.86, NaN], [2, 2, 'Cost'], [2, 6.75, NaN], [2, 8.54, NaN], [3, 1.5, 'Payments'],[3, 3.65, 'Products']], columns=['interaction_id', 'start_time', 'topic'])\n\n      interaction_id    start_time     topic \n           1               2           Cost\n           1              5.72          NaN\n           1              8.83         Billing\n           1              12.86         NaN\n           2               2            Cost\n           2              6.75          NaN\n           2              8.54          NaN\n           3              1.5          Payments\n           3              3.65         Products\n\nAn this is the Desired Output\ndf2 = pd.DataFrame([[1, 2, 'Cost',6.83], [1, 5.72, NaN, NaN], [1, 8.83, 'Billing',4.03], [1, 12.86, NaN,NaN], [2, 2, 'Cost',6.54], [2, 6.75, NaN, NaN], [2, 8.54, NaN, NaN], [3, 1.5, 'Payments', 2.15],[3, 3.65, 'Products','...']], columns=['interaction_id', 'start_time', 'topic','topic_length'])\n\n       interaction_id    start_time     topic     topic_length\n\n           1               2           Cost           6.83\n           1              5.72          NaN           NaN\n           1              8.83         Billing        4.03\n           1              12.86         NaN           NaN\n           2               2            Cost          6.54\n           2              6.75          NaN           NaN\n           2              8.54          NaN           NaN\n           3              1.5          Payments       2.15\n           3              3.65         Products       ....\n\n"
'I am fairly new to pandas so bear with me. I have a dataframe with interaction-data (begin time of the interaction, end time of the interaction, userA and userB that had interaction):\n\n\n  begin, end, userA, userB.\n\n\nNow I would like to transform this data into the following format (time from 0 to x, userId of one user, a boolean value yes or no if there was an interaction).\n\n\n  time, userId, interaction.\n\n\nI saw some posts about conditional dataframes using np.where but I am not yet sure how to stick this together. I am sorry for not providing a code-example.\n\nExample:\n(input):\n\nbegin, end, userA, userB\n\n130,     300, 1, 2\n\n\n(output):\n\ntime, user, interaction\n\n...\n\n130, 1, yes\n\n130, 2, yes\n\n131, 1, yes\n\n131, 2, yes\n\n...\n\n300, 1, yes\n\n300, 2, yes\n\n301, 1, no\n\n301, 2, no\n\n\nCould someone point me in the right direction, like: methods that I should look at?\n'
"I have this problem where I am stuck for quite a number of days.\n\nI have this function :\n\ndef cal_score(research, citations, teaching, international, income):\n     return .3 **research + .3 **citations + .3 **teaching +.075 **international + .025 **income\n\n\nwhere “research”, “citations”,  “teaching”, “international” and “income” are columns of the dataset.   I want to add a new column in the dataset whose values should be calculated based on the function mentioned above. I tried different procedures but none worked. \n\nExample :    If we have a row as below\n\nuniversity_name  Indian Institute of Technology Bombay\n\n\nteaching  43.8\n\ninternational  14.3\n\nresearch  24.2\n\ncitations  8,327\n\nincome   14.9\n\nTotal Score Ranking  \n\n\nThen the total score should be calculated as\n\nTotal Score =  .3 **research + .3 **citations + .3 **teaching +.075 **international + .025 **income.\n\n\nThis should apply for all the rows in the dataset.\n\nCan anyone please help me in implementing this requirement. I am stuck at this for quite sometime now. :-(\n\nIndian_univ.head(10).to_dict()\n\n{'citations': {510: 38.799999999999997,\n  832: 39.0,\n  856: 45.600000000000001,\n  959: 45.799999999999997,\n  1232: 84.700000000000003,\n  1360: 38.5,\n  1361: 41.799999999999997,\n  1362: 35.299999999999997,\n  1363: 53.600000000000001,\n  1679: 51.600000000000001},\n 'country': {510: 'India',\n  832: 'India',\n  856: 'India',\n  959: 'India',\n  1232: 'India',\n  1360: 'India',\n  1361: 'India',\n  1362: 'India',\n  1363: 'India',\n  1679: 'India'},\n 'female_male_ratio': {510: '16 : 84',\n  832: '15 : 85',\n  856: '16 : 84',\n  959: '17 : 83',\n  1232: '46 : 54',\n  1360: '18 : 82',\n  1361: '13 : 87',\n  1362: '15 : 85',\n  1363: '17 : 83',\n  1679: '19 : 81'},\n 'income': {510: '24.2',\n  832: '72.4',\n  856: '52.7',\n  959: '70.4',\n  1232: '28.4',\n  1360: '-',\n  1361: '42.4',\n  1362: '-',\n  1363: '64.8',\n  1679: '37.9'},\n 'international': {510: '14.3',\n  832: '16.1',\n  856: '19.9',\n  959: '15.6',\n  1232: '29.3',\n  1360: '15.3',\n  1361: '17.3',\n  1362: '14.7',\n  1363: '15.6',\n  1679: '18.2'},\n 'international_students': {510: '1%',\n  832: '0%',\n  856: '1%',\n  959: '1%',\n  1232: '1%',\n  1360: '1%',\n  1361: '0%',\n  1362: '0%',\n  1363: '1%',\n  1679: '1%'},\n 'num_students': {510: '8,327',\n  832: '9,928',\n  856: '8,327',\n  959: '8,061',\n  1232: '16,691',\n  1360: '8,371',\n  1361: '6,167',\n  1362: '9,928',\n  1363: '8,061',\n  1679: '3,318'},\n 'research': {510: 15.699999999999999,\n  832: 45.299999999999997,\n  856: 33.100000000000001,\n  959: 13.699999999999999,\n  1232: 14.0,\n  1360: 23.0,\n  1361: 25.199999999999999,\n  1362: 30.0,\n  1363: 12.300000000000001,\n  1679: 39.5},\n 'student_staff_ratio': {510: 14.9,\n  832: 17.5,\n  856: 14.9,\n  959: 18.699999999999999,\n  1232: 23.899999999999999,\n  1360: 17.300000000000001,\n  1361: 12.199999999999999,\n  1362: 17.5,\n  1363: 18.699999999999999,\n  1679: 8.1999999999999993},\n 'teaching': {510: 43.799999999999997,\n  832: 44.200000000000003,\n  856: 47.299999999999997,\n  959: 30.399999999999999,\n  1232: 25.800000000000001,\n  1360: 33.799999999999997,\n  1361: 31.300000000000001,\n  1362: 39.299999999999997,\n  1363: 25.100000000000001,\n  1679: 32.600000000000001},\n 'total_score': {510: 29.489999999999995,\n  832: 38.549999999999997,\n  856: 37.799999999999997,\n  959: 26.969999999999999,\n  1232: 37.350000000000001,\n  1360: 28.589999999999996,\n  1361: 29.489999999999998,\n  1362: 31.379999999999995,\n  1363: 27.299999999999997,\n  1679: 37.109999999999999},\n 'university_name': {510: 'Indian Institute of Technology Bombay',\n  832: 'Indian Institute of Technology Kharagpur',\n  856: 'Indian Institute of Technology Bombay',\n  959: 'Indian Institute of Technology Roorkee',\n  1232: 'Panjab University',\n  1360: 'Indian Institute of Technology Delhi',\n  1361: 'Indian Institute of Technology Kanpur',\n  1362: 'Indian Institute of Technology Kharagpur',\n  1363: 'Indian Institute of Technology Roorkee',\n  1679: 'Indian Institute of Science'},\n 'world_rank': {510: '301-350',\n  832: '226-250',\n  856: '251-275',\n  959: '351-400',\n  1232: '226-250',\n  1360: '351-400',\n  1361: '351-400',\n  1362: '351-400',\n  1363: '351-400',\n  1679: '276-300'},\n 'year': {510: 2012,\n  832: 2013,\n  856: 2013,\n  959: 2013,\n  1232: 2014,\n  1360: 2014,\n  1361: 2014,\n  1362: 2014,\n  1363: 2014,\n  1679: 2015}}\n\n"
"I'm training a model whose output is a softmax layer of size 19. When I try model.predict(x), for each input, I get what appears to be a probability distribution across the 19 classes. I tried model.predict_classes, and got a numpy array of the size of x, with each output equal to 0. How can I get one hot vectors for the output? \n"
"Say I have a python DataFrame with the following structure:\n\npd.DataFrame([[1,2,3,4],[1,2,3,4],[1,3,5,6],[1,4,6,7],[1,4,6,7],[1,4,6,7]])\nOut[262]: \n   0  1  2  3\n0  1  2  3  4\n1  1  2  3  4\n2  1  3  5  6\n3  1  4  6  7\n4  1  4  6  7\n5  1  4  6  7\n\n\nHow can I add a column called 'ct' that counts the instances of the DataFrame where column 1-3 match to each row that matches... so the DataFrame would look like this when all is completed.\n\n   0  1  2  3  ct\n0  1  2  3  4  2\n1  1  2  3  4  2\n2  1  3  5  6  1\n3  1  4  6  7  3\n4  1  4  6  7  3\n5  1  4  6  7  3\n\n"
'I have a data of bond market like this:\n\nId   row      Date       BuyPrice    SellPrice\n1    1      2017-10-30    94520       0\n1    2      2017-10-30    94538       0\n1    3      2017-10-30    94609       0\n1    4      2017-10-30    94615       0\n1    5      2017-10-30    94617       0\n1    1      2017-09-20    99100       99059\n1    1      2017-09-20    98100       99090\n2    1      2010-11-01    99890       100000\n2    2      2010-11-01    99899       100000\n2    3      2010-11-01    99901       99899\n2    4      2010-11-01    99920       99850\n2    5      2010-11-01    99933       99848\n\n\nI want to choose the lowest sell price and highest buy price for each id and calculate their subtraction but if any of minimum of sell or price is zero i want to make an exception and drop out that date.\n\nAnd also give each id an index by date. means for the first day of each give 1 and second day give 2 and so on.\n\nAt last data should be like this:\n\nId    Date    highest buy price     lowest sell price       NBBO(highest buy price - lowestSellPrice)Index\n\n1     2017-10-30    94520                  0                       NaN                                 1\n1     2017-09-20    99100                  99059                   41                                  2      \n2     2017-11-01    99890                  99848                   42                                  1\n\n'
'I have a data set which contains state code and its status.\n\n  code  status\n1   AZ  a\n2   CA  b\n3   KS  c\n4   MO  c\n5   NY  d\n6   AZ  d\n7   MO  a\n8   MO  b\n9   MN  b\n10  NV  a\n11  NV  e\n12  MO  f\n13  NY  a\n14  NY  a\n15  NY  b\n\n\nI want to filter out this data set which code contains only a status and count how many they have. Example output will be,\n\n  code  status  \n1   AZ  a   \n2   MO  a   \n3   NY  a   \n\n    AZ =1   MO = 1  NY =2\n\n\nI used df.groupyby("code").loc[df.status == \'a\'] but didn\'t have any luck.\nAny help appreciated!\n'
"I am attempting to filter some time series data without any luck in pandas.. Any tips for what I am doing wrong is greatly appreciated.. First I am attempting to filter just for the month of July 2013 and then filter the data again for taking hourly averages of the dataset samples.\n\nUltimately what I am wanting to do is filter the data as described above in addition for Weekdays, OR individual weekdays Mondays, Tuesdays, Wednesday, etc. with the CustomBusinessDay function.\n\nI am getting tripped up where the code is commented out for the CustomBusinessDay\n\nimport pandas as pd\nimport numpy as np\nfrom pandas.tseries.offsets import CustomBusinessDay\n\n\ntime = pd.date_range('6/28/2013', periods=2000, freq='45min')\ndata = pd.Series(np.random.randint(100, size=2000), index=time)\n\nprint(data)\n\n##weekmask = 'Mon Tue Wed Thu Fri'\n\n\ndf = data.truncate(before='7/1/2013', after='7/31/2013')\ndf = df.groupby(df.index.hour).mean()\nprint(df)\n\n##df = CustomBusinessDay(weekmask=weekmask)\n##df = pd.bdate_range(start=None, end=None, periods=None, freq='B')\n##\n##print(df)\n\n"
'I want to create a plot using Pandas to show the standard deviations of item prices on specific week days (in my case there are 6 relevant days of the week, each shown as 0-5 on the x axis).\n\nIt seems to work however there is another set of smaller bars next to each standard deviation bar that is literally also valued at 0-5. \n\nI think this means that I\'m also accidentally also plotting the day of the week.\n\nHow can I get rid of these smaller bars and only show the standard deviation bars?   \n\nsales_std=sales_std[[\'WeekDay\',\'price\']].groupby([\'WeekDay\']).std()\n    .reset_index()\n\n\nHere is where I try to plot the graph:\n\np = sales_std.plot(figsize= \n(15,5),legend=False,kind="bar",rot=45,color="orange",fontsize=16, \nyerr=sales_std);\np.set_title("Standard Deviation", fontsize=18);\np.set_xlabel("WeekDay", fontsize=18);\np.set_ylabel("Price", fontsize=18);\np.set_ylim(0,100);\n\n\nResulting Bar Plot:\n'
'I was implementing 10-fold cross validation from scratch in Python. The language is Python 3.6 and I wrote this in Spyder (Anaconda). My input shape is data=(1440,390),label=(1440,1).\n\nMy code:\n\ndef partitions(X,y):\n  np.random.shuffle(X)\n  foldx=[]\n  foldy=[]\n  j=0\n  for i in range(0,10):\n    foldx[i]=X[j:j+143,:]\n    foldy[i]=y[foldx[j]]\n    j+=144\n  return np.array(foldx),np.array(foldy)\n\ndef cv(X,y,model):\n  trainx,trainy=partitions(X,y)\n  scores=[]\n  for i in range(0,10):\n    xtest=trainx[i]\n    ytest=trainy[xtest]\n    xtrain=trainx[:i]+trainx[i+1:]\n    ytrain=trainy[xtrain]\n    model.fit(xtrain,ytrain)\n    preds=model.predict(xtest)\n    print(accuracy_score(np.ravel(ytest),preds))\n    scores.append(accuracy_score(np.ravel(ytest),preds))\n  return scores.mean()\n\n\nThe error comes at\n\nfoldx[i]=X[j:j+143,:]\n\n\nwhere it says \n\n\n  IndexError: list assignment index out of range. \n\n\nHow do I rectify this? I am not very experienced in implementing such problems from scratch.\n'
'I am new to Python. In a Data frame, there are two columns, DOB (Date of Birth) and DOD (Date of Death). I want to create a new categorical column in the dataframe with name "IS_ALIVE". The condition for this new field, IS_ALIVE should be "1" when DOB is populated and DOD is null; IS_ALIVE should be "0" when DOB and DOD both are not null, meaning, some date is populated the DOD.\n\n\nI searched and tried multiple ways, no luck. Please help me.\n'
"I'm a complete newbie to python dask (a little experience with pandas). I have a large Dask Dataframe (~10 to 20 million rows) that I have to separate based on a unique column value. \n\nFor exmaple if I have the following Dataframe with column C1 to Cn (sorry, don't know how to make a proper table in stackoverflow) and I want to create subset Dataframes for each unique value of the column C2\n\nBase Dataframe:\n\n\n|Ind| C1 | C2 |....| Cn |\n|-----------------------|\n| 1 |val1| AE |....|time|\n|-----------------------|\n| 2 |val2| FB |....|time|\n|-----------------------|\n|...|....| .. |....| ...|\n|-----------------------|\n| n |valn| QK |....|time|\n\n\nSubset Dataframes to be created:\n\nSubset 1:\n\n|Ind| C1 | C2 |....| Cn |\n|-----------------------|\n| 1 |val1| AE |....|time|\n|-----------------------|\n| 2 |val2| AE |....|time|\n|-----------------------|\n|...|....| .. |....| ...|\n|-----------------------|\n| n |valn| AE |....|time|\n\nSubset 2\n\n|Ind| C1 | C2 |....| Cn |\n|-----------------------|\n| 1 |val1| FB |....|time|\n|-----------------------|\n| 2 |val2| FB |....|time|\n|-----------------------|\n|...|....| .. |....| ...|\n|-----------------------|\n| n |valn| FB |....|time|\n\n\nand so on.\n\n\nMy current approach is getting all unique values of C2 and filtering the base dataframe for each of this values iteratively. But this takes way to long time. I'm doing research at the moment on how I can improve this process, but I would appreciate it a lot if any of you can give me some tips.\n"
'I want to modify only numeric variables in my data frame, i.e. impute missing values of numeric variables by median and those of factor variables by mode. To modify only numeric variables, I tried following:\n\nxTrain.select_dtypes(include=numerics) =  xTrain.select_dtypes(include=numerics).fillna(xTrain.mean(), inplace=True)\n\n\nbut it says: \n\n\n  SyntaxError: can\'t assign to function call\n\n\nIn fact, this solution just worked but I am not happy with it as it doesn\'t involve an assignment operation (\'=\'). Moreover, this is a "private method" (i.e., an implementation detail) and is subject to change or total removal in the future. Was recommended to use with caution by answer here :\n\nxTrain._get_numeric_data().fillna(xTrain.mean(), inplace=True)\n\n\nWas thinking if there are alternative ways to select just numeric columns and impute them in the whole data, meaning modifying only part of the dataframe? Thanks in advance!\n'
'I have a large data set containing many NaN values in multiple columns.\n\nI have tried the following code but it is not dropping Nan value from the data set\n\ndf = pd.read_excel(\'sec3_data.xlsx\')\ndf.dropna(subset=["Deviation from Partisanship"])\ndf[\'Deviation from Partisanship\'].unique()\n\n\nOutput:\n\narray([nan, \'Vote for opposing party\', \'Vote for own party\'], dtype=object)\n\n\nIt clearly shows there is still some nan values available. How can I remove them?\n'
"I have the following time series data of temperature readings:\n\nDT                Temperature\n01/01/2019 0:00     41\n01/01/2019 1:00     42\n01/01/2019 2:00     44\n......\n01/01/2019 23:00    41\n01/02/2019 0:00     44\n\n\nI am trying to write a function that compares the hourly change in temperature for a given day. Any change greater than 3 will increment quickChange counter. Something like this:\n\ndef countChange(day):\n    for dt in day:\n        if dt+1 - dt &gt; 3: quickChange = quickChange+1\n\n\nI can call the function for a day ex: countChange(df.loc['2018-01-01'])\n"
"I have a Dataframe series with 30s frequency. \n\ndf.head()\n\n\n\n\nI want to calculate the daily averages for all signals in that series but it doesnt seem to work. I tried both \n\ndf_average = df.to_period('D')\ndf.resample('D')\n\n\nAnd i get:\n\n\nI want to have only 1 line per day. Why do i get more?\nThank you\n"
'Below is part of my code in which I am trying to iterate over PE files. I am still getting the same error which is: \n\n\n  [Errno 2] No such file or directory: \'//FlickLearningWizard.exe\'\n\n\nTried using os.path.join(filepath) but it does not do anything since I am have already made the path. I got rid of \'/\' but it did not add much. Here is my code: \n\nB = 65521\nT = {}\nfor directories in datasetPath: # directories iterating over my datasetPath which contains list of my pe files\n    samples = [f for f in os.listdir(datasetPath) if isfile(join(datasetPath, f))]\n    for file in samples:\n        filePath = directories+"/"+file\n        fileByteSequence = readFile(filePath)\n        fileNgrams = byteSequenceToNgrams(filePath,N)\n        hashFileNgramsIntoDictionary(fileNgrams,T)\nK1 = 1000\nimport heapq\nK1_most_common_Ngrams_Using_Hash_Grams = heapq.nlargest(K1, T)\n\n\nAnd here is my complete error message: \n\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-63-eb8b9254ac6d&gt; in &lt;module&gt;\n      6     for file in samples:\n      7         filePath = directories+"/"+file\n----&gt; 8         fileByteSequence = readFile(filePath)\n      9         fileNgrams = byteSequenceToNgrams(filePath,N)\n     10         hashFileNgramsIntoDictionary(fileNgrams,T)\n\n&lt;ipython-input-3-4bdd47640108&gt; in readFile(filePath)\n      1 def readFile(filePath):\n----&gt; 2     with open(filePath, "rb") as binary_file:\n      3         data = binary_file.read()\n      4     return data\n      5 def byteSequenceToNgrams(byteSequence, n):\n\n\nA sample of the files I am trying to iterate through in which is in the datasetpath: \n\n [\'FlickLearningWizard.exe\', \'autochk.exe\', \'cmd.exe\', \'BitLockerWizard.exe\', \'iexplore.exe\', \'AxInstUI.exe\', \'fvenotify.exe\', \'DismHost.exe\', \'GameBarPresenceWriter.exe\', \'consent.exe\', \'fax_390392029_072514.exe\', \'Win32.AgentTesla.exe\', \'{71257279-042b-371d-a1d3-fbf8d2fadffa}.exe\', \'imecfmui.exe\', \'HxCalendarAppImm.exe\', \'CExecSvc.exe\', \'bootim.exe\', \'dumped.exe\', \'FXSSVC.exe\', \'drvinst.exe\', \'DW20.exe\', \'appidtel.exe\', \'baaupdate.exe\', \'AuthHost.exe\', \'last.exe\', \'BitLockerToGo.exe\', \'EhStorAuthn.exe\', \'IMTCLNWZ.EXE\', \'drvcfg.exe\', \'makecab.exe\', \'licensingdiag.exe\', \'ldp.exe\', \'win33.exe\', \'forfiles.exe\', \'DWWIN.EXE\', \'comp.exe\', \'coredpussvr.exe\', \'AddSuggestedFoldersToLibraryDialog.exe\', \'InetMgr6.exe\', \'3_4.exe\', \'CIDiag.exe\', \'win32.exe\', \'LanguageComponentsInstallerComHandler.exe\', \'sample.exe\', \'Win32.SofacyCarberp.exe\', \'EASPolicyManagerBrokerHost.exe\', \'131.exe\', \'AddInUtil.exe\', \'fixmapi.exe\', \'cmdl32.exe\', \'chkntfs.exe\', \'instnm.exe\', \'ImagingDevices.exe\', \'BitLockerWizardElev.exe\', \'bdechangepin.exe\', \'logman.exe\', \'.DS_Store\', \'bootcfg.exe\', \'DsmUserTask.exe\', \'find.exe\', \'LogCollector.exe\', \'HxTsr.exe\', \'lpq.exe\', \'ctfmon.exe\', \'AppInstaller.exe\', \'hvsimgr.exe\', \'Vcffipzmnipbxzdl.exe\', \'lpremove.exe\', \'hdwwiz.exe\', \'CastSrv.exe\', \'gpresult.exe\', \'hvix64.exe\', \'HvsiSettingsWorker.exe\', \'fodhelper.exe\', \'21.exe\', \'InspectVhdDialog6.2.exe\', \'798_abroad.exe\', \'doskey.exe\', \'AuditShD.exe\', \'alg.exe\', \'certutil.exe\', \'bitsadmin.exe\', \'help.exe\', \'fsquirt.exe\', \'PDFXCview.exe\', \'inetinfo.exe\', \'Win32.Wannacry.exe\', \'dcdiag.exe\', \'LsaIso.exe\', \'lpr.exe\', \'dtdump.exe\', \'FileHistory.exe\', \'LockApp.exe\', \'AppVShNotify.exe\', \'DeviceProperties.exe\', \'ilasm.exe\', \'CheckNetIsolation.exe\', \'FilePicker.exe\', \'choice.exe\', \'ComSvcConfig.exe\', \'Calculator.exe\', \'CredDialogHost.exe\', \'logagent.exe\', \'InspectVhdDialog6.3.exe\', \'junction.exe\', \'findstr.exe\', \'ktmutil.exe\', \'csvde.exe\', \'esentutl.exe\', \'Win32.GravityRAT.exe\', \'bootsect.exe\', \'BdeUISrv.exe\', \'ChtIME.exe\', \'ARP.EXE\', \'dsdbutil.exe\', \'iisreset.exe\', \'1003.exe\', \'getmac.exe\', \'dllhost.exe\', \'BOTBINARY.EXE\', \'cscript.exe\', \'dnscacheugc.exe\', \'aspnet_regbrowsers.exe\', \'hvax64.exe\', \'CredentialUIBroker.exe\', \'dpnsvr.exe\', \'ApplyTrustOffline.exe\', \'LxRun.exe\', \'credwiz.exe\', \'1002.exe\', \'FileExplorer.exe\', \'BackgroundTransferHost.exe\', \'convert.exe\', \'AppVClient.exe\', \'evntcmd.exe\', \'attrib.exe\', \'ClipUp.exe\', \'DmNotificationBroker.exe\', \'dcomcnfg.exe\', \'dvdplay.exe\', \'Dism.exe\', \'AtBroker.exe\', \'invoice_2318362983713_823931342io.pdf.exe\', \'DataSvcUtil.exe\', \'bdeunlock.exe\', \'DeviceCensus.exe\', \'dstokenclean.exe\', \'AndroRat Binder_Patched.exe\', \'iediagcmd.exe\', \'comrepl.exe\', \'dispdiag.exe\', \'FlashUtil_ActiveX.exe\', \'cliconfg.exe\', \'aitstatic.exe\', \'gpupdate.exe\', \'GetHelp.exe\', \'charmap.exe\', \'aspnet_regsql.exe\', \'IMEWDBLD.EXE\', \'AppVStreamingUX.exe\', \'dwm.exe\', \'Ransomware.Unnamed_0.exe\', \'csc.exe\', \'bridgeunattend.exe\', \'icacls.exe\', \'dialer.exe\', \'BdeHdCfg.exe\', \'fontdrvhost.exe\', \'027cc450ef5f8c5f653329641ec1fed9.exe\', \'LocationNotificationWindows.exe\', \'dpapimig.exe\', \'BitLockerDeviceEncryption.exe\', \'ftp.exe\', \'Eap3Host.exe\', \'dfsvc.exe\', \'LogonUI.exe\', \'Fake Intel (1).exe\', \'chglogon.exe\', \'fhmanagew.exe\', \'changepk.exe\', \'aspnetca.exe\', \'IMEPADSV.EXE\', \'browserexport.exe\', \'bcdboot.exe\', \'aspnet_wp.exe\', \'FXSCOVER.exe\', \'dllhst3g.exe\', \'CertEnrollCtrl.exe\', \'EduPrintProv.exe\', \'ielowutil.exe\', \'ADSchemaAnalyzer.exe\', \'cygrunsrv.exe\', \'HxAccounts.exe\', \'diskperf.exe\', \'certreq.exe\', \'bcdedit.exe\', \'efsui.exe\', \'klist.exe\', \'raffle.exe\', \'cacls.exe\', \'hvc.exe\', \'cmmon32.exe\', \'BioIso.exe\', \'AssignedAccessLockApp.exe\', \'DmOmaCpMo.exe\', \'AppLaunch.exe\', \'AddInProcess.exe\', \'dasHost.exe\', \'dmcertinst.exe\', \'IMJPSET.EXE\', \'cmbins.exe\', \'LicenseManagerShellext.exe\', \'diskpart.exe\', \'iscsicpl.exe\', \'chown.exe\', \'Magnify.exe\', \'aapt.exe\', \'false.exe\', \'BioEnrollmentHost.exe\', \'hvsirdpclient.exe\', \'c2wtshost.exe\', \'dplaysvr.exe\', \'ChsIME.exe\', \'fsavailux.exe\', \'Win32.WannaPeace.exe\', \'CasPol.exe\', \'icsunattend.exe\', \'fveprompt.exe\', \'expand.exe\', \'chgusr.exe\', \'hvsirpcd.exe\', \'MiniConfigBuilder.exe\', \'FirstLogonAnim.exe\', \'EDPCleanup.exe\', \'ksetup.exe\', \'AppVDllSurrogate.exe\', \'InstallUtil.exe\', \'immersivetpmvscmgrsvr.exe\', \'cmdkey.exe\', \'appcmd.exe\', \'Build.exe\', \'hostr.exe\', \'CloudStorageWizard.exe\', \'DWTRIG20.EXE\', \'file_4571518150a8181b403df4ae7ad54ce8b16ded0c.exe\', \'FsIso.exe\', \'chmod.exe\', \'imjpuexc.exe\', \'CHXSmartScreen.exe\', \'iissetup.exe\', \'7ZipSetup.exe\', \'svchost.exe\', \'ldifde.exe\', \'logoff.exe\', \'DiskSnapshot.exe\', \'fontview.exe\', \'LaunchWinApp.exe\', \'GamePanel.exe\', \'yfoye_dump.exe\', \'ls.exe\', \'HOSTNAME.EXE\', \'at.exe\', \'InetMgr.exe\', \'FaceFodUninstaller.exe\', \'InputPersonalization.exe\', \'AppVNice.exe\', \'ImeBroker.exe\', \'CameraSettingsUIHost.exe\', \'Defrag.exe\', \'lpksetup.exe\', \'djoin.exe\', \'irftp.exe\', \'DTUHandler.exe\', \'LockScreenContentServer.exe\', \'dsamain.exe\', \'lpkinstall.exe\', \'DataStoreCacheDumpTool.exe\', \'dmclient.exe\', \'dump1.exe\', \'Cain.exe\', \'AddInProcess32.exe\', \'appidcertstorecheck.exe\', \'IMJPUEX.EXE\', \'HxOutlook.exe\', \'FlashPlayerApp.exe\', \'diskraid.exe\', \'bthudtask.exe\', \'explorer.exe\', \'CompMgmtLauncher.exe\', \'malware.exe\', \'njRAT.exe\', \'CompatTelRunner.exe\', \'evntwin.exe\', \'Dxpserver.exe\', \'HelpPane.exe\', \'cvtres.exe\', \'dxdiag.exe\', \'hvsievaluator.exe\', \'signed.exe\', \'csrss.exe\', \'InstallBC201401.exe\', \'audiodg.exe\', \'dsregcmd.exe\', \'ApproveChildRequest.exe\', \'iisrstas.exe\', \'chkdsk.exe\', \'lodctr.exe\', \'aspnet_state.exe\', \'DiagnosticsHub.StandardCollector.Service.exe\', \'chgport.exe\', \'cleanmgr.exe\', \'GameBar.exe\', \'AgentService.exe\', \'InfDefaultInstall.exe\', \'IMESEARCH.EXE\', \'Fondue.exe\', \'iexpress.exe\', \'backgroundTaskHost.exe\', \'dfrgui.exe\', \'cofire.exe\', \'BrowserCore.exe\', \'clip.exe\', \'appidpolicyconverter.exe\', \'ed01ebfbc9eb5bbea545af4d01bf5f1071661840480439c6e5babe8e080e41aa.exe\', \'cipher.exe\', \'DeviceEject.exe\', \'cerber.exe\', \'5a765351046fea1490d20f25.exe\', \'CloudExperienceHostBroker.exe\', \'FXSUNATD.exe\', \'GenValObj.exe\', \'lsass.exe\', \'ddodiag.exe\', \'cmstp.exe\', \'wirelesskeyview.exe\', \'edpnotify.exe\', \'CameraBarcodeScannerPreview.exe\', \'bfsvc.exe\', \'eventcreate.exe\', \'driverquery.exe\', \'CCG.exe\', \'ConfigSecurityPolicy.exe\', \'ieUnatt.exe\', \'eshell.exe\', \'ipconfig.exe\', \'jsc.exe\', \'gpscript.exe\', \'LaunchTM.exe\', \'cttunesvr.exe\', \'curl.exe\', \'cttune.exe\', \'DevicePairingWizard.exe\', \'ByteCodeGenerator.exe\', \'IEChooser.exe\', \'LockAppHost.exe\', \'DataExchangeHost.exe\', \'dxgiadaptercache.exe\', \'dsacls.exe\', \'Locator.exe\', \'DpiScaling.exe\', \'DisplaySwitch.exe\', \'autoconv.exe\', \'IMJPDCT.EXE\', \'ieinstal.exe\', \'colorcpl.exe\', \'auditpol.exe\', \'dccw.exe\', \'DeviceEnroller.exe\', \'UpdateCheck.exe\', \'LicensingUI.exe\', \'ExtExport.exe\', \'easinvoker.exe\', \'ApplySettingsTemplateCatalog.exe\', \'eventvwr.exe\', \'browser_broker.exe\', \'extrac32.exe\', \'EaseOfAccessDialog.exe\', \'label.exe\', \'change.exe\', \'IMCCPHR.exe\', \'audit.exe\', \'aspnet_compiler.exe\', \'aspnet_regiis.exe\', \'desktopimgdownldr.exe\', \'dmcfghost.exe\', \'ComputerDefaults.exe\', \'control.exe\', \'DeviceCredentialDeployment.exe\', \'compact.exe\', \'InspectVhdDialog.exe\', \'EdmGen.exe\', \'cmak.exe\', \'AppHostRegistrationVerifier.exe\', \'DataUsageLiveTileTask.exe\', \'hcsdiag.exe\', \'gchrome.exe\', \'adamuninstall.exe\', \'CloudNotifications.exe\', \'dusmtask.exe\', \'fc.exe\', \'hh.exe\', \'eudcedit.exe\', \'iscsicli.exe\', \'DFDWiz.exe\', \'isoburn.exe\', \'IMTCPROP.exe\', \'CapturePicker.exe\', \'abba_-_happy_new_year_zaycev_net.exe\', \'finger.exe\', \'ApplicationFrameHost.exe\', \'calc.exe\', \'counter.exe\', \'editrights.exe\', \'fltMC.exe\', \'convertvhd.exe\', \'LegacyNetUXHost.exe\', \'grpconv.exe\', \'ie4uinit.exe\', \'dsmgmt.exe\', \'fsutil.exe\', \'AppResolverUX.exe\', \'BootExpCfg.exe\', \'conhost.exe\', \'bash.exe\', \'IcsEntitlementHost.exe\']\n\n\nCan anyone help please? \n'
'update: attached the link to the data in case you want to reproduce:\n\nhttps://github.com/amandawang-dev/credit-worthiness-analysis/blob/master/credit_train.csv\n\nhttps://github.com/amandawang-dev/credit-worthiness-analysis/blob/master/credit_test.csv\n\nI\'m trying to use logistic regression model of sklearn to predict whether the person\'s credit of a bank account is good or bad. The initial dataset looks like below:\n\n\n\n\n\n\n\nThen I binarized the first column "Class" (\'Good\'=1, \'Bad\'=0), and the dataset looks like below:\n\n\n\nSo I used sklearn logistic model to predict the test data (test data is same as predict dataset and the \'Class\' column is also binarized), and trying to calculate the confusion matrix, codes as below, then the confusion matrix I got is \n\n[[  0  54]\n [  0 138]]\n\n\naccuracy score is 0.71875, I think the confusion matrix result is wrong because there\'s no true positive value. Anybody have any idea how to fix this? Thanks!\n\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\n\ncredit_train = pd.read_csv(\'credit_train.csv\')\ncredit_test = pd.read_csv(\'credit_test.csv\')\ncredit_train["Class"] = (credit_train["Class"] =="Good").astype(int)\ncredit_test["Class"] = (credit_test["Class"] =="Good").astype(int)\nX=credit_train[[\'CreditHistory.Critical\']]\ny=credit_train[\'Class\']\nclf = LogisticRegression(random_state=0).fit(X, y)\n\nX_test=credit_test[[\'CreditHistory.Critical\']]\ny_test=credit_test[\'Class\']\ny_pred=clf.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm=confusion_matrix(y_pred=y_pred, y_true=y_test)\nscore = clf.score(X_test, y_test)\nprint(score)\nprint(cm)\n\n\n\n\ndata types of each column:\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nRangeIndex: 808 entries, 0 to 807\nData columns (total 17 columns):\nClass                             808 non-null int64\nDuration                          808 non-null int64\nAmount                            808 non-null int64\nInstallmentRatePercentage         808 non-null int64\nResidenceDuration                 808 non-null int64\nAge                               808 non-null int64\nNumberExistingCredits             808 non-null int64\nNumberPeopleMaintenance           808 non-null int64\nTelephone                         808 non-null int64\nForeignWorker                     808 non-null int64\nCheckingAccountStatus.lt.0        808 non-null int64\nCheckingAccountStatus.0.to.200    808 non-null int64\nCheckingAccountStatus.gt.200      808 non-null int64\nCreditHistory.ThisBank.AllPaid    808 non-null int64\nCreditHistory.PaidDuly            808 non-null int64\nCreditHistory.Delay               808 non-null int64\nCreditHistory.Critical            808 non-null int64\ndtypes: int64(17)\nmemory usage: 107.4 KB\n\n'
"How to match data column with its own regex in another column. Here is how it looks like.\n\n        Data            Regex\n0   HU13568812       ^HU[0-9]{8}\n1   78567899         ^NO[0-9]{5}\n2   AT1234567        ^HU[0-9]{7}\n\n\nThe output will be a new column for the result if its match (1)  or not match (0) like this.\n\n            Data            Regex     Match\n0   HU13568812       ^HU[0-9]{8}        1\n1   78567899         ^NO[0-9]{5}        0\n2   AT1234567        ^AT[0-9]{7}        1\n\n\nI tried to use re.match() but I can't seem to get it match for the whole row at once. Is there any better way to do this in a simple function or more pythonic way? Thank you.\n"
"I have cars,cities and routes.\nEvery city is a node.\nEvery route is a path generated by a car.\n\nDifferent cars will have different path, sometimes paths could be intersected (which means differents cars have found the same city in they path), sometimes not.\n\nI would rappresent a graph with all the cities and all the different path and plot the graph with plotly.\nExample:\n\nList of cities: CityA -CityB -CityD -CityZ -CityK\nList of cars: Car1, Car2\n\nRoutes:\nCar1 will have a path through   cityA - cityB - cityD  this path will be colored in red\nCar2 will have a path though    cityZ - cityA - cityK  this path will be colored in blue\n\n\nUsing networkx.classes.function.add_path I can't achive this because I will not preserve the information about different cars, there will be only the list of connected node:\n\nAs in the previous example add_path, G.edges(): [(CityA-CityB),(CityB-CityD),(CityZ-CityA),(CityA-CityK)]\n\n\nI am not sure if what I am looking for could be achived with networkx.\n\nA solution to plot is just passing the list to plotly but doing so I will not even use NetworkX and the next steps is to analize the graph.\n"
"I have a dataset that looks like this:\n    x       y           z\n0   Jan     28446000    110489.0    \n1   Feb     43267700    227900.0    \n\nWhen I plot a line chart like this:\npx.line(data,x = 'x', y = ['y','z'], line_shape = 'spline', title=&quot;My Chart&quot;)\n\nThe y axis scale comes from 0 to 90 M. The first line on the chart for y is good enough. However, the second line appears to be always at 0M. What can I do to improve my chart such that we can see clearly how the values of both column change over the x values?\nIs there any way I can normalize the data? Or perhaps I could change the scaling of the chart.\n"
'example =\nday | hours \nmonday  5\nmonday   6\ntuesday  5\ntuesday  6\ntuesday  7\nwednesday 5\nwednesday 6\nwednesday 7\n\nExpected Result:\nday | hours\nmonday 1\ntuesday 2\nwednesday 2\n\n'
"\nI wrote this code but it is not giving me the correct answer. Kindly help me.\nfor text in hiv2['Number of deaths due to HIV/AIDS']:\n   a=text.replace(' ','')\n   b=a.split('[')\nhiv2['Number of deaths due to HIV/AIDS']=b[0]\n\n"
"I need to calculate the accuracy for each category (NOT the overall accuracy) in a multi-label classification problem. It is easy to find the precision, recall and F-score for each category using classification_report from scikit-learn library. There are 13 categories distributed as follows:\n                   precision    recall  f1-score   support\n\n     Category 1       0.58      0.48      0.53       244\n     Category 2       0.91      0.85      0.88       728\n     Category 3       0.90      0.92      0.91      1319\n     Category 4       0.70      0.55      0.62       533\n     Category 5       1.00      0.10      0.18        20\n     Category 6       0.94      0.84      0.89      2038\n     Category 7       0.83      0.78      0.80      1930\n     Category 8       0.85      0.44      0.58       113\n     Category 9       0.88      0.87      0.87      1329\n     Category 10      0.79      0.54      0.64        61\n     Category 11      0.81      0.77      0.79       562\n     Category 12      0.71      0.62      0.66       416\n     Category 13      0.76      0.60      0.67       500\n\n      micro avg       0.86      0.78      0.82      9793\n      macro avg       0.82      0.64      0.69      9793\n   weighted avg       0.85      0.78      0.81      9793\n    samples avg       0.86      0.82      0.83      9793\n\nI know that accuracy can be found as follows:     Accuracy=(TP+TN)/(TP+TN+FP+FN) but finding TP and TN for this multi-lable classifcation problem was an issue for me.\nThere is a similar question to this one on stackoverflow Calculating accuracy from precision, recall, f1-score - scikit-learn\nbut for binary-classification problems only.\nNote:\nI have tried multilabel_confusion_matrix and confusion_matrix from sklearn.metrics to extract the confusion matrix, but both gave me the same following error: ValueError: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets\nAny Ideas?\n"
'I\'m learning data science and would like to make dummy variables for my dataset. \n\nI have a Dataframe that has "Product Category" column that is a list of matching categories looking like ["Category1", "Category2".."CategoryN"]\n\nI know that Pandas has nice function that makes dummy variables automatically (pandas.get_dummies) but in this case, I can\'t use it, I guess(?). \n\nI know how to loop over the each row to append 1 to matching elements of each columns.\nMy current code is this:\n\nfor column_name in df.columns[1:]: #first column is "Product Category" and appended dummy columns (product category names) to the right previously\n    for index, _ in enumerate(df[column_name][:10]): #limit 10 rows\n        if column_name in df["Product Category"][index]:\n            df[column_name][index] = 1    \n\n\nHowever, the above code is not efficient and I cannot use it since I have more than 100,000 rows. I\'d like to somehow do the operations on the whole array, but I can\'t figure out how to do it. \n\nCould someone help?\n'
'say I have two matrix original and reference \n\nimport pandas as pa\nprint "Original Data Frame"\n# Create a dataframe\noldcols = {\'col1\':[\'a\',\'a\',\'b\',\'b\'], \'col2\':[\'c\',\'d\',\'c\',\'d\'], \'col3\':[1,2,3,4]}\na = pa.DataFrame(oldcols)\nprint "Original Table:"\nprint a\n\nprint "Reference Table:"\nb = pa.DataFrame({\'col1\':[\'x\',\'x\'], \'col2\':[\'c\',\'d\'], \'col3\':[10,20]})\nprint b\n\n\nNow I want to subtract from the third column (col3) of the original table (a), the value in the reference table (c) in the row where the second columns of the two tables match. So the first row of table two should have the value 10 added to the third column, because the row of table b where the column is col2 is \'c\' has a value of 10 in col3. Make sense? Here\'s some code that does that:\n\ncol3 = []\nfor ix, row in a.iterrows():\n    col3 += [row[2] + b[b[\'col2\'] == row[1]][\'col3\']]\n\na[\'col3\'] = col3\nprint "Output Table:"\nprint a\n\n\nand want to make it look like this:\n\nOutput Table:\n  col1 col2  col3\n0    a    c   11\n1    a    d   22\n2    b    c   13\n3    b    d   24\n\n\nthe problem is col3 takes Name: and dtype in a array\n\n&gt;&gt;print col3\n[0    11\nName: col3, dtype: int64, 1    22\nName: col3, dtype: int64, 0    13\nName: col3, dtype: int64, 1    24\nName: col3, dtype: int64]\n\n\nCan you please help? \n'
"I have a MultiIndex Series (3 indices) that looks like this:\n\nWeek   ID_1    ID_2\n3       26     1182            39.0\n               4767            42.0\n               31393           20.0\n               31690           42.0\n               32962            3.0\n....................................\n\n\nI also have a dataframe df which contains all the columns (and more) used for indices in the Series above, and I want to create a new column in my dataframe df that contains the value matching the ID_1 and ID_2 and the Week - 2 from the Series.\n\nFor example, for the row in dataframe that has ID_1 = 26, ID_2 = 1182 and Week = 3, I want to match the value in the Series indexed by ID_1 = 26, ID_2 = 1182 and Week = 1 (3-2) and put it on that row in a new column. Further, my Series might not necessarily have the value required by the dataframe, in which case I'd like to just have 0.\n\nRight now, I am trying to do this by using:\n\n[multiindex_series.get((x[1].get('week', 2) - 2, x[1].get('ID_1', 0), x[1].get('ID_2', 0))) for x in df.iterrows()]\n\n\nThis however is very slow and memory hungry and I was wondering what are some better ways to do this.\n\nFWIW, the Series was created using\n\nsaved_groupby = df.groupby(['Week', 'ID_1', 'ID_2'])['Target'].median()\n\n\nand I'm willing to do it a different way if better paths exist to create what I'm looking for.\n"
"I'm basically trying to do this Monte Carlo kind of analysis where I randomly reassign the participants in my experiment to new groups, and then reanalyze the data given the random new groups. So here's what I want to do:\n\nParticipants are originally grouped into eight groups of four participants each. I want to randomly reassign each participant to a new group, but I don't want any participants to end up in a new group with another participant from their same original group.\n\nHere is how far I got with this:\n\nimport random\nimport pandas as pd\nimport itertools as it\n\ndata = list(it.product(range(8),range(4)))\ntest_df = pd.DataFrame(data=data,columns=['group','partid'])\ntest_df['new_group'] = None\n\nfor idx, row in test_df.iterrows():\n    start_group = row['group']\n    takens      = test_df.query('group == @start_group')['new_group'].values\n    fulls       = test_df.groupby('new_group').count().query('partid &gt;= 4').index.values\n    possibles   = [x for x in test_df['group'].unique() if (x not in takens)\n                                                      and (x not in fulls)]\n    test_df.loc[idx,'new_group'] = random.choice(possibles)\n\n\nThe basic idea here is that I randomly reassign a participant to a new group with the constraints that (a) the new group doesn't have one of their original group partners in, and (b) the new group doesn't have 4 or more participants already reassigned to it.\n\nThe problem with this approach is that, many times, by the time we try to reassign the last group, the only remaining group slots are in that same group. I could also just try to re-randomize when it fails until it succeeds, but that feels silly. Also, I want to make 100 random reassignments, so that approach could get very slow....\n\nSo there must be a smarter way to do this. I also feel like there should be a simpler way to solve this, given how simple the goal feels (but I realize that can be misleading...)\n"
"Below is what my dataframe looks like, as you would see one of my dataframe column is URL and other is timestamp count. When I am running this code: busiest_hosts[busiest_hosts['host'].str.contains('***.novo.dk')==True] I get an error: error: nothing to repeat at position 0. Which I think is because the first element of my URL is *. It seems like a python bug (my python version is 3.x). I would really appreciate if anyone could help me in resolving this.\n\n\n"
'I\'ve been building a web scraper in BS4 and have gotten stuck.  I am using Trip Advisor as a test for other data I will be going after, but am not able to isolate the tag of the \'entire\' reviews.  Here is an example:  \n\nhttps://www.tripadvisor.com/Restaurant_Review-g56010-d470148-Reviews-Chez_Nous-Humble_Texas.html\n\nNotice in the first review, there is an icon below "the wine list is...".  I am able to easily isolate the partial reviews, but have not been able to figure out a way to get BS4 to pull the reviews after a simulated \'More\' click.  I\'m trying to figure out what tool(s) are needed for this?  Do I need to use selenium instead?  \n\nThe original element looks like this:\n\n&lt;span class="partnerRvw"&gt;\n&lt;span class="taLnk hvrIE6 tr475091998 moreLink ulBlueLinks" onclick="  ta.util.cookie.setPIDCookie(4444); ta.call(\'ta.servlet.Reviews.expandReviews\', {type: \'dummy\'}, ta.id(\'review_475091998\'), \'review_475091998\', \'1\', 4444);\n  "&gt;\nMore&amp;nbsp; &lt;/span&gt;\n&lt;span class="ui_icon caret-down"&gt;&lt;/span&gt;\n&lt;/span&gt;\n\n\nLooking at the HTML after you click on the More link you would find a new dynamically added class that has a  with the information I need (see below):\n\n&lt;div class="review dyn_full_review inlineReviewUpdate provider0 first newFlag" style="display: block;"&gt;\n&lt;a name="UR475091998" class=""&gt;&lt;/a&gt;\n&lt;div id="UR475091998" class="extended provider0 first newFlag"&gt;\n&lt;div class="col1of2"&gt;\n&lt;div class="member_info"&gt;\n&lt;div id="UID_6875524F623CC948F4F9CA95BB4A9567-SRC_475091998" class="memberOverlayLink" onmouseover="requireCallIfReady(\'members/memberOverlay\', \'initMemberOverlay\', event, this, this.id, \'Reviews\', \'user_name_photo\');" data-anchorwidth="90"&gt;\n&lt;div class="avatar profile_6875524F623CC948F4F9CA95BB4A9567 "&gt;\n&lt;a onclick=""&gt;\n\n&lt;img src="https://media-cdn.tripadvisor.com/media/photo-l/0d/97/43/bf/joannecarpenter.jpg" class="avatar potentialFacebookAvatar avatarGUID:6875524F623CC948F4F9CA95BB4A9567" width="74" height="74"&gt;\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class="username mo"&gt;\n&lt;span class="expand_inline scrname mbrName_6875524F623CC948F4F9CA95BB4A9567" onclick="ta.trackEventOnPage(\'Reviews\', \'show_reviewer_info_window\', \'user_name_name_click\')"&gt;joannecarpenter&lt;/span&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="location"&gt;\nHumble, Texas\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="memberBadging g10n"&gt;\n&lt;div id="UID_6875524F623CC948F4F9CA95BB4A9567-CONT" class="no_cpu" onclick="ta.util.cookie.setPIDCookie(\'15984\'); requireCallIfReady(\'members/memberOverlay\', \'initMemberOverlay\', event, this, this.id, \'Reviews\', \'review_count\');" data-anchorwidth="90"&gt;\n&lt;div class="levelBadge badge lvl_02"&gt;\nLevel &lt;span&gt;&lt;img src="https://static.tacdn.com/img2/badges/20px/lvl_02.png" alt="" class="icon" width="20" height="20/"&gt;&lt;/span&gt; Contributor &lt;/div&gt;\n&lt;div class="reviewerBadge badge"&gt;\n&lt;img src="https://static.tacdn.com/img2/badges/20px/rev_03.png" alt="" class="icon" width="20" height="20"&gt;\n&lt;span class="badgeText"&gt;6 reviews&lt;/span&gt; &lt;/div&gt;\n&lt;div class="contributionReviewBadge badge"&gt;\n&lt;img src="https://static.tacdn.com/img2/badges/20px/Foodie.png" alt="" class="icon" width="20" height="20"&gt;\n&lt;span class="badgeText"&gt;6 restaurant reviews&lt;/span&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="col2of2"&gt;\n&lt;div class="innerBubble"&gt;\n&lt;div class="quote"&gt;&lt;a href="/ShowUserReviews-g56010-d470148-r475091998-Chez_Nous-Humble_Texas.html#CHECK_RATES_CONT" onclick="ta.setEvtCookie(\'Reviews\',\'title\',\'\',0,this.href); setPID();" id="r475091998"&gt;“&lt;span class="noQuotes"&gt;Dinner&lt;/span&gt;”&lt;/a&gt;&lt;/div&gt;\n&lt;div class="rating reviewItemInline"&gt;\n&lt;span class="rate sprite-rating_s rating_s"&gt; &lt;img class="sprite-rating_s_fill rating_s_fill s50" width="70" src="https://static.tacdn.com/img2/x.gif" alt="5 of 5 bubbles"&gt;\n&lt;/span&gt;\n&lt;span class="ratingDate relativeDate" title="April 12, 2017"&gt;Reviewed 3 days ago\n&lt;span class="new redesigned"&gt;NEW&lt;/span&gt; &lt;/span&gt;\n&lt;a class="viaMobile" href="/apps" target="_blank" onclick="ta.util.cookie.setPIDCookie(24687)"&gt;\n&lt;span class="ui_icon mobile-phone"&gt;&lt;/span&gt;\nvia mobile\n&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class="entry"&gt;\n&lt;p&gt;\nOur favorite restaurant in Houston. Definitely the best and friendliest service! The food is not only served with a flair, it is absolutely delicious. My favorite is the Lamb. It is the best! Also the duck moose, fois gras, the crispy salad and the French onion soup are all spectacular! This is a must try restaurant! The wine list is fantastic. Just ask Daniel for suggestions. He not only knows his wines; he loves what he does! We Love this place!\n&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class="rating-list"&gt;\n&lt;div class="recommend"&gt;\n&lt;span class="recommend-titleInline noRatings"&gt;Visited April 2017&lt;/span&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="expanded lessLink"&gt;\n&lt;span class="taLnk collapse ulBlueLinks no_cpu "&gt;\nLess&amp;nbsp;\n&lt;/span&gt;\n&lt;span class="textArrow_more ui_icon caret-up"&gt;&lt;/span&gt;\n&lt;/div&gt;\n&lt;div id="helpfulq475091998_expanded" class="helpful redesigned white_btn_container "&gt;\n&lt;span class="isHelpful"&gt;Helpful?&lt;/span&gt; &lt;div class="tgt_helpfulq475091998 rnd_white_thank_btn" onclick="ta.call(\'ta.servlet.Reviews.helpfulVoteHandlerOb\', event, this, \'LeJIVqd4EVIpECri1GII2t6mbqgqguuuxizSxiniaqgeVtIJpEJCIQQoqnQQeVsSVuqHyo3KUKqHMdkKUdvqHxfqHfGVzCQQoqnQQZiptqH5paHcVQQoqnQQrVxEJtxiGIac6XoXmqoTpcdkoKAUAAv0tEn1dkoKAUAAv0zH1o3KUK0pSM13vkooXdqn3XmffAdvqndqnAfbAo77dbAo3k0npEEeJIV1K0EJIVqiJcpV1U0Ii9VC1rZlU3XozxbZZxE2crHN2TDUJiqnkiuzsVEOxdkXqi7TxXpUgyR2xXvOfROwaqILkrzz9MvzCxMva7xEkq8xXNq8ymxbAq8AzzrhhzCxbx2vdNvEn2fnwEfq8alzCeqi53ZrgnMrHhshTtowGpNSmq89IwiVb7crUJxdevaCnJEqI33qiE5JGErJExXKx5ooItGCy5wnCTx2VA7RvxEsO3\'); ta.trackEventOnPage(\'HELPFUL_VOTE_TEST\', \'helpfulvotegiven_v2\');"&gt;\n&lt;img src="https://static.tacdn.com/img2/icons/icon_thumb_white.png" class="helpful_thumbs_up white"&gt;\n&lt;img src="https://static.tacdn.com/img2/icons/icon_thumb_green.png" class="helpful_thumbs_up green"&gt;\n&lt;span class="helpful_text"&gt;Thank joannecarpenter&lt;/span&gt; &lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="tooltips vertically_centered"&gt;\n&lt;div class="reportProblem"&gt;\n&lt;span id="ReportIAP_475091998" class="problem collapsed taLnk" onclick="ta.trackEventOnPage(\'Report_IAP\', \'Report_Button_Clicked\', \'member\'); ta.call(\'ta.servlet.Reviews.iapFlyout\', event, this, \'475091998\')" onmouseover="if (!this.getAttribute(\'data-first\')) {ta.trackEventOnPage(\'Reviews\', \'report_problem\', \'hover_over_flag\'); this.setAttribute(\'data-first\', 1)} uiOverlay(event, this)" data-tooltip="" data-position="above" data-content="Problem with this review?"&gt;\n&lt;img src="https://static.tacdn.com/img2/icons/gray_flag.png" width="13" height="14" alt=""&gt;\n&lt;span class="reportTxt"&gt;Report&lt;/span&gt; &lt;/span&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="userLinks"&gt;\n&lt;div class="sameGeoActivity"&gt;\n&lt;a href="/members-citypage/joannecarpenter/g56010" target="_blank" onclick="ta.setEvtCookie(\'Reviews\',\'more_reviews_by_user\',\'\',0,this.href); ta.util.cookie.setPIDCookie(19160)"&gt;\nSee all 5 reviews by joannecarpenter for Humble &lt;/a&gt;\n&lt;/div&gt;\n&lt;div class="askQuestion"&gt;\n&lt;span class="taLnk ulBlueLinks" onclick="ta.trackEventOnPage(\'answers_review\',\'ask_user_intercept_click\' ); ta.load(\'ta-answers\', (function() {require(\'answers/misc\').askReviewerIntercept(this, \'470148\', \'joannecarpenter\', \'6875524F623CC948F4F9CA95BB4A9567\', \'en\', \'475091998\',\'Chez Nous\', 39151)}).bind(this), true);"&gt;Ask joannecarpenter about Chez Nous&lt;/span&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="note"&gt;\nThis review is the subjective opinion of a TripAdvisor member and not of TripAdvisor LLC. &lt;/div&gt;\n&lt;div class="duplicateReviewsInline"&gt;\n&lt;div class="previous"&gt;joannecarpenter has 1 more review of Chez Nous&lt;/div&gt; &lt;ul class="dupReviews"&gt;\n&lt;li class="dupReviewItem"&gt;\n&lt;div class="reviewTitle"&gt;\n&lt;a href="/ShowUserReviews-g56010-d470148-r453237869-Chez_Nous-Humble_Texas.html#REVIEWS"&gt;“Joanne Carpenter”&lt;/a&gt;\n&lt;/div&gt;\n&lt;div class="rating"&gt;\n&lt;span class="rate sprite-rating_ss rating_ss"&gt; &lt;img class="sprite-rating_ss_fill rating_ss_fill ss50" width="50" src="https://static.tacdn.com/img2/x.gif" alt="5 of 5 bubbles"&gt;\n&lt;/span&gt;\n&lt;span class="date"&gt;Reviewed January 18, 2017&lt;/span&gt;\n&lt;/div&gt;\n&lt;/li&gt;\n&lt;/ul&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class="large"&gt;\n\n&lt;/div&gt;\n&lt;div class="ad iab_inlineBanner"&gt;\n&lt;div id="gpt-ad-468x60" class="adInner gptAd"&gt;&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n\n\nIs there a way for BS4 to handle this for me? \n'
'My CSV data file contains dates in the following format:\n\nIn: data["DayIndex"].unique()\n\nOut: array([\'04/23/17\', \'04/20/17\', \'04/21/17\', \'04/24/17\', \'04/22/17\',\n       \'05/02/17\', \'04/27/17\', \'05/06/17\', \'04/30/17\', \'04/25/17\',\n       \'04/26/17\', \'05/04/17\'], dtype=object)    \n\n\nI want to turn it into a proper pandas time series. I\'ve tried this:\n\ndata["DayIndex"] = pandas.DatetimeIndex(data["Day"])\n\n\nIt takes ages even for a few hundred thousand rows. What are my options to speed up the parsing?\n'
'So, I found a demo jupyter notebook online with toggles to adjust input to the graph. I would like to share this dashboard with someone without sharing the code which runs behind it.\n\nHow do I do it? I am very new to Jupyter Dashboards deploy.\n\nI have seen tutorials and I see there is this option "deploy" under ""File" (File->Deploy As->Local Dashboard.)\n\nJupyter notebook example I found online has Deploy As Local Dashboard option available here and My jupyter doesn\'t have that option.\n\nA Notebook I found online with "Deploy" as option which I don\'t have\n'
'Is there a way to find to find and rank rows in a Pandas Dataframe by their similarity to a row from another Dataframe?\n'
"There's an operation that is a little counter intuitive when using pandas apply() method. It took me a couple of hours of reading to solve, so here it is.\n\nSo here is what I was trying to accomplish. \n\nI have a pandas dataframe like so:\n\ntest = pd.DataFrame({'one': [[2],['test']], 'two': [[5],[10]]})\n      one   two\n0     [2]   [5]\n1  [test]  [10]\n\n\nand I want to add the columns per row to create a resulting list of length = to the DataFrame's original length like so:\n\ndef combine(row):\n    result = row['one'] + row['two']\n    return(result)\n\n\nWhen running it through the dataframe using the apply() method:\n\ntest.apply(lambda x: combine(x), axis=1)\n    one  two\n0     2    5\n1  test   10\n\n\nWhich isn't quite what we wanted. What we want is:\n\n       result\n0      [2, 5]\n1  [test, 10]\n\n\nEDIT\n\nI know there are simpler solutions to this example. But this is an abstraction from a much more complex operation.Here's an example of a more complex one:\n\ndf_one:\n\n    org_id     date       status     id\n0     2     2015/02/01     True      3\n1     10    2015/05/01     True      27\n2     10    2015/06/01     True      18\n3     10    2015/04/01     False     27\n4     10    2015/03/01     True      40\n\n\ndf_two:\n\n    org_id      date\n0     12     2015/04/01\n1     10     2015/02/01\n2     2      2015/08/01\n3     10     2015/08/01\n\n\nHere's a more complex operation:\n\ndef operation(row, df_one):\n    sel = (df_one.date &lt; pd.Timestamp(row['date'])) &amp; \\\n          (df_one['org_id'] == row['org_id'])\n    last_changes = df_one[sel].groupby(['org_id', 'id']).last()\n    id_list = last_changes[last_changes.status].reset_index().id.tolist()\n\n    return (id_list)\n\n\nthen finally run:\n\n    df_one.sort_values('date', inplace=True)\n\n    df_two['id_list'] = df_two.apply(\n        operation,\n        axis=1,\n        args=(df_one,)\n    )\n\n\nThis would be impossible with simpler solutions. Hence my proposed one below would be to re write operation to:\n\ndef operation(row, df_one):\n    sel = (df_one.date &lt; pd.Timestamp(row['date'])) &amp; \\\n          (df_one['org_id'] == row['org_id'])\n    last_changes = df_one[sel].groupby(['org_id', 'id']).last()\n    id_list = last_changes[last_changes.status].reset_index().id.tolist()\n\n    return pd.Series({'id_list': id_list})\n\n\nWe'd expect the following result:\n\n    id_list\n0     []\n1     []\n2     [3]      \n3     [27,18,40]\n\n"
'I am scraping data from a list of hundreds of URLs, each one containing a table with statistical baseball data. Within each unique URL in the list, there is a table for all of the seasons of a single baseball player\'s career, like this:\n\nhttps://www.baseball-reference.com/players/k/killeha01.shtml\n\nI have successfully created a script to append the data from a single URL into a single list/dataframe. However, here is my question:\n\nHow should I adjust my code to scrape a full list of hundreds of URLs from this domain and then append all of the table rows from all of the URLs into a single list/dataframe?\n\nMy general format for scraping a single URL is as follows:\n\nimport pandas as pd\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\n\nurl_baseball_players = [\'https://www.baseball-reference.com/players/k/killeha01.shtml\']\n\ndef scrape_baseball_data(url_parameter):\n\n    html = urlopen(url_parameter)\n\n    # create the BeautifulSoup object\n    soup = BeautifulSoup(html, "lxml")\n\n    column_headers = [SCRAPING COMMAND WITH CSS SELECTOR GADGET FOR GETTING COLUMN HEADERS]\n\n    table_rows = soup.select(SCRAPING COMMAND WITH CSS SELECTOR GADGET FOR GETTING ALL OF THE DATA FROM THE TABLES INCLUDING HTML CHARACTERS)\n\n    player_data = []\n\n    for row in table_rows:  \n\n        player_list = [COMMANDS FOR SCRAPING HTML DATA FROM THE TABLES INTO AN ORGANIZED LIST]\n\n        if not player_list:\n            continue\n\n        player_data.append(player_list)\n\n    return player_data\n\nlist_baseball_player_data = scrape_baseball_data(url_baseball_players)\n\ndf_baseball_player_data = pd.DataFrame(list_baseball_player_data)\n\n'
'I pass the edgecolor=none to the plot to make the edge of the graph disappear.\nThe whole code appears like this:\n\nsns.countplot(x="Survived", data=titanic_df, ax=ax1, palette=\'Pastel1\', edgecolor=\'none\')\n\n\nHowever, I have to do this everytime when I\'m visualizing graphs which is annoying. How can I set seaborn / matplolib up to default edgecolor to none???\n\nThanks!\n'
"I'm using extract_patches_2d and extract_patches for local patch extraction from a 2d image and I'd like to have the theory and references that explain the implemented method for patch extraction. \n"
"I'm stuck with one task on my learning path.\n\nFor the binomial distribution  X∼Bp,n  with mean  μ=np  and variance  σ**2=np(1−p), we would like to upper bound the probability  P(X≥c⋅μ) for  c≥1. \nThree bounds introduced:\n\nFormulas\n\nThe task is to write three functions respectively for each of the inequalities. They must take  n ,  p  and  c  as inputs and return the upper bounds for  P(X≥c⋅np) given by the above Markov, Chebyshev, and Chernoff inequalities as outputs.\n\nAnd there is an example of IO:\n\nCode:\n\nprint Markov(100.,0.2,1.5)\n\nprint Chebyshev(100.,0.2,1.5)\n\nprint Chernoff(100.,0.2,1.5)\n\nOutput\n\n0.6666666666666666\n\n0.16\n\n0.1353352832366127\n\n\nI'm completely stuck. I just can't figure out how to plug in all that math into functions (or how to think algorithmically here). If someone could help me out, that would be of great help!\n\np.s. and all libs are not allowed by task conditions except math.exp\n"
'I have extracted the list of columns from my df, which are of string type and look like this: 1999-1 (as in the first month of 1999). \n\nI want to drop all columns before 2000 so I extracted the column list and used list comprehension to check if the first character of the string is 1. \n\nThe code runs but my "columns_to_drop" list is empty.\n\ndef convert_housing_data_to_quarters():\n    data_source = pd.read_csv(\'City_Zhvi_AllHomes.csv\') #this is the df \n    data_source[\'State\'] = data_source[\'State\'].map(states) \n    data_source.drop([\'Metro\',\'CountyName\',\'RegionID\',\'SizeRank\'],axis=1,inplace=1)\n    data_source.set_index([\'State\',\'RegionName\'],inplace=True)\n    columns = list(data_source.columns)\n    columns_to_drop = [col for col in columns if col[0]=="1"]\n    data_source.drop(columns_to_drop,axis=1,inplace=1)\n    return data_source\n\nconvert_housing_data_to_quarters()\n\n'
"I'm new to data science and trying to do some data wrangling with python 2.7 in iPython notebook. A tutorial I was following for my first project asked me to replace all NaN intputs with 0 or 1. But I'd like to consider another approach where I can 1st look at the count for the rows with non-numerical values corresponding to all rows having credit_history as NaN...\n\nIdeal Output when Credit_History is NaN:\n\nSelf_Employed\nYes  532\nNo   32\n\nMarried\nNo   398\nYes  213\n\n\nAnd for the numerical values, I'd like to get the mean for all columns when credit_history is NaN\n\nIdeal output for non-numberical values when Credit_History is NaN:\n\nMean Applicant Income: 54003.1232\nLoanAmount: 35435.12\nLoan_Amount_Term: 360\n\n\nThanks in advance!\n"
'I\'d like to input certain value in a particular column. my data looks something like:\n\nLoanID  Married  ApplicantIncome  CoapplicantIncome  Credit_History\nLP00135   NaN        33460             16000               1.0\nLP00234   Yes        55000             70000               1.0\nLP00432   No         12000             0                   0.0\n\n\n(I also know the index no. corresponding to each row, in this case  for the 1st row its 104)\nAs we can see in the 1st line, the applicant is obviously married as there is an entry for coapplicant income.\n\nIs there a way I can directly impute "Yes" into in that column for that particular row using the Index no. or LoanID as these two are unique for each row?\n\nThanks in advance.\n'
"Im trying to figure out how to configure a neural network using Neupy. The problem is that I cant seem to find much options for a GRNN, only the sigma value as described here:\n\nThere is a parameter, y_i, that I want to be able to adjust, but there doesn't seem to be a way to do it on the package. I'm parsing through the code but i'm not a developer so i've trouble following all the steps, maybe a more experienced set of eyes can find a way to tweak that parameter.\n\nThanks\n"
"I have a dataframe created from a .CSV file. Each column should consist of numeric values only, however it can sometimes be a string ('>18','&lt;5','CANCELED',etc.)\n\nBefore using pd.to_numeric to convert the columns to numeric and coerce the non-numerics, I'd like to create a new dataframe or dictionary which contains unique non-numeric values found in each column. This will help understand what sorts of non-numeric inputs we are receiving for our features to be used in one or more predictive models.\n\nThis seems like a fairly simple task, but I am fairly new to Python and having a hard time figuring it out.\n\nSo far I've reduced the dataframe down to only columns of dtype object which seems like a good first step and removed the majority of the columns that have all numeric values:\n\ndf = df.select_dtypes(include='object')\n\n\nI'm thinking that I need to iterate over each element and apply some function using isnumeric() to create a new dataframe? Or a dictionary with each column name containing string values as key name and the dictionary value being a list of unique string values found in that column?\n\nAny help on the most efficient way to tackle this is much appreciated. \n\nSample Dataframe:\n\nFEATURE_1   FEATURE_2   FEATURE_3   FEATURE_4\n1               1         &lt;1.5        &gt;3.4\nNan             2           2           4\n4            CANCELED       3          4.5\n1.34            2         &lt;1.5         &lt;2\n\n\nDesired Output:\n\n{'FEATURE_2':['CANCELED'],'FEATURE_3':['&lt;1.5'],'FEATURE_4':['&gt;3.5','&lt;2']}\n\n\nThanks!\n"
"I'm trying to replace all the higher values than my limit in Pandas column like this:\n\ndf_train['IsAlone'].replace([&gt;0],1)\n\n\nbut this obviously is not working\n\nI got my code working like this:\n\nfor i in range(len(df_train)):\n    if df_train.iat[i,8] &gt; 0:\n        df_train.iat[i,8] = 1\n\n\nbut I'm wondering if there is a more universal way to replace values as the .iat solution is prone to bugs.\n"
'The problem\n\nI am trying to plot an image next to some data. However, I would like the image to expand to be flush with the plot labels. For example, the following code (using this tutorial image):\n\n# make the incorrect figure\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfig = plt.figure(figsize=(8,3))\nax_image = plt.subplot(1,2,1)\nplt.imshow(mpimg.imread(\'stinkbug.png\'))\nplt.subplot(1,2,2)\nplt.plot([0,1],[2,3])\nplt.ylabel("y")\nplt.xlabel("want image flush with bottom of this label")\nfig.tight_layout()\nax_image.axis(\'off\')\nfig.savefig("incorrect.png")\n\n\nyields this plot, with extra whitespace:\n\n\n\nThe hacky attempt at a solution\n\nI would like a plot that doesn\'t waste whitespace. The following hacky code (in the vein of this SO link) accomplishes this: \n\n# make the correct figure with manually changing the size\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfig = plt.figure(figsize=(8,3))\nax_image = fig.add_axes([0.02,0,0.45,1.0])\nplt.imshow(mpimg.imread(\'stinkbug.png\'))\nplt.subplot(1,2,2)\nplt.plot([0,1],[2,3])\nplt.ylabel("y")\nplt.xlabel("want image flush with bottom of this label")\nfig.tight_layout()\nax_image.axis(\'off\')\nfig.savefig("correct.png")\n\n\nyielding the following figure:\n\n\n\nThe question\n\nIs there any way to plot an image flush with other subplots, without having to resort to manual adjustment of figure sizes? \n'
"I am attempting to create a loop that will analyze time series data and average the data 'per day' in a seperate pandas dataframe.\n\nFor now if I make up some fake time series data to get a working program:\n\nimport pandas as pd\nimport numpy as np\n\ntime = pd.date_range('6/28/2013', periods=2000, freq='5min')\ndata = pd.Series(np.random.randint(100, size=2000), index=time)\n\n\nIm stuck in the loop portion of trying to iterate over each value on the data set. I think I need to create a blank pandas dataframe obs and then just keep appending this data frame thru the entire dataset... Any tips help! Thanks\n\nobs = pd.DataFrame()\n\nfor i in range(len(data)):\n    dfDaily = data.groupby(data.index.day).mean()\n    obs.append(dfDaily)\n\n"
"The workflow is as below:\n\n\nGroupby LineNum then\nMark values in LWS column greater than 50 as 'start'\nMark values in Text column containing ':'(colon) as 'end'\nMark values between start and end as 1 in 'ExpectedFlag'\n\n\nI have finished upto step 3 i.e upto column named 'end'\n\nI am not able to figure out how to mark values between start and end as in ExpectedFlag. Is there any way to mark this using pandas operation?\n\n        text  LWS LineNum   start   end     ExpectedFlag\n0   somethin    3       2       0     0                0\n1   somethin    3       2       0     0                0\n2   somethin    2       2       0     0                0\n3   value      70       2       1     0                1\n4   value       3       2       0     0                1\n5   value:      3       2       0     1                1\n6   val1      200       3       1     0                1\n7   val1:       3       3       0     1                1\n8   val2        3       3       0     0                0\n9   val2      100       3       1     0                1\n10  val2:       3       3       0     1                1\n11  djsal       3       3       0     0                0\n12  jdsal       3       3       0     0                0\n13  ajsd        3       3       0     0                0\n\n"
"i have a Pandas DataFrame with the following structure.\n\nFeature 1  | Feature 2  | Feature 3\n10         | 200        | True\n30         | 233        | False\n45         | 344        | True\n\n\nany idea how i can do normalization for feature 1 and feature 2 only? without changing the index of original DataFrame.\n\ni already try this following code, but it's normalize all columns and change the index of dataframe to 0,1,2\n\nx = df.values\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndataset = pd.DataFrame(x_scaled)\n\n"
"I have a Data(csv format) where the first column is an epoch timestamp(strictly increasing) and the other columns are cumulative rows(just increasing or equal).\nSample is as below:\n\ndf = pandas.DataFrame([[1515288240, 100, 50, 90, 70],[1515288241, 101, 60, 95, 75],[1515288242, 110, 70, 100, 80],[1515288239, 110, 70, 110, 85],[1515288241, 110, 75, 110, 85],[1515288243,110,70,110,85]],columns =['UNIX_TS','A','B','C','D'])\ndf =\nid    UNIX_TS  A   B   C  D\n 0 1515288240 100 50  90 70\n 1 1515288241 101 60  95 75\n 2 1515288242 110 70 100 80\n 3 1515288239 110 70 110 85\n 4 1515288241 110 75 110 85\n 5 1515288243 110 70 110 85\n\nimport pandas as pd\ndef clean(df,column_name,equl):\n    i=0\n    while(df.shape[0]-2&gt;=i):\n        if df[column_name].iloc[i]&gt;df[column_name].iloc[i+1]:\n            df.drop(df[column_name].iloc[[i+1]].index,inplace=True)\n            continue\n        elif df[column_name].iloc[i]==df[column_name].iloc[i+1] and equl==1:\n            df.drop(df[column_name].iloc[[i+1]].index,inplace=True)\n            continue\n        i+=1\n\nclean(df,'UNIX_TS',1)\nfor col in df.columns[1:]:\n    clean(df,col,0)\n\ndf =\n    id    UNIX_TS  A   B   C  D\n     0 1515288240 100 50  90 70\n     1 1515288241 101 60  95 75\n     2 1515288242 110 70 100 80\n\n\nMy script works as intended but its too slow, anybody has ideas about how to improve its speed.\n\nI wrote a script to remove all the invalid data based on 2 rules:\n\n\nUnix_TS must be strictly increasing(because its a time, it cannot flow back or  pause),\nother columns are increasing and can be constant for example is in one row it is 100 and the next row it can be >=100 but not less. \n\n\nBased on the rules the index 3 &amp; 4 are invalid because unix_ts 1515288239 is 1515288241 are less than the index 2.\nindex 5 is wrong because the value B decreased   \n"
'I am new to machine learning. I have one doubt: why use toarray() with onehotencoding while not with label encoding here. I am not getting any idea. pls someone help.\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabel_encoder_x = LabelEncoder()\nx[:, 0] = label_encoder_x.fit_transform(x[:, 0])\nonehotencoder = OneHotEncoder(categorical_features= [0])\nx = onehotencoder.fit_transform(x).toarray()\nlabel_encoder_y = LabelEncoder()\ny = label_encoder_y.fit_transform(y)\n\n'
'print("Someone has killed {:.4f} with headshot, have {} kills, while the most kills ever recorded is {}.".format(pubg_main_df[\'headshotKills\'].max(), pubg_main_df[pubg_main_df[\'headshotKills\']==pubg_main_df[\'headshotKills\'].max()][\'kills\'], pubg_main_df[\'kills\'].max()))\n\n\nWhat I want is to get the value of \'Kills\' where the headshotKills is max.\n\nBut I am getting:\n\nSomeone has killed 26.0000 with headshot, have 910050    60\nName: kills, dtype: int64 kills, while the most kills ever recorded is 60.\n\nIt should have been:\n\nSomeone has killed 26.0000 with headshot, have 42 kills, while the most kills ever recorded is 60.\n\nType of the variables are:\n\nheadshotKills      int64\n\nkills              int64\n\nPlease help. :)\n'
'A string is a WEAK password if:\neither, it is less than 8 characters long,\nor, it is an English word,which is function is_english_word( ) is True.\n\nA string is a STRONG password if:\nit contains at least 11 characters\nAND it contains at least 1 lower case letter\nAND it contains at least 1 capital letter\nAND it contains at least 1 numerical digit.\n\nA string is a MEDIUM password if it is NOT a WEAK password AND is NOT a STRONG password.\n\ndef is_english_word( string ):\n    with open("english_words.txt") as f:\n        word_list = []\n        for line in f.readlines():\n            word_list.append(line.strip())\n        if string in word_list: \n            return True\n        elif string == string.upper() and string.lower() in word_list:\n            return True\n        elif string == string.title() and string.lower() in word_list:\n            return True\n        else:\n            return False\n\ndef password_strength( string ):\n    lower = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m", "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"]\n    upper = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]\n    for item in string:\n        if item in lower:\n            string = string.replace(item, "x")\n        elif item in upper:\n            string = string.replace(item, "y")\n        elif item.isnumeric():\n            string = string.replace(item, "n")      \n    for item in string:        \n        if len( string ) &lt; 8 or is_english_word( string ) :\n            return \'WEAK\'\n        elif len( string ) &gt;= 11 and string.count("x") &gt;= 1 and string.count("y") &gt;= 1 and string.count("n") &gt;= 1: \n            return \'STRONG\'\n        else:\n            return \'MEDIUM\'\n\nprint( password_strength( \'Unimaginatively\' ) )\n\n\nThis password should be "WEAK",but the output is "MEDIUM",I don\'t know what\'s the problem of my codes.Many thanks.\n'
"I have to implement a classification algorithm on a medicinal dataset. So i thought it was crucial to have good recall on disease regognition. I wanted to implement scorer like this\n\nrecall_scorer = make_scorer(recall_score(y_true = , y_pred = , \\\nlabels =['compensated_hypothyroid', 'primary_hypothyroid'], average = 'macro'))\n\n\nBut then, I would like to use this scorer in GridSearchCV, so it will fit on KFold for me. So, i wouldn't know how to initialize scorer as it needs to be passed y_true and y_pred immediately.\n\nHow do i go around this problem? Am I to write my own hyperparameter tuning? \n"
'I\'m trying to extract the company, job description, reviews, and location for positions that are "data scientist" and "senior data scientist" separately. I started by trying to get an output for data scientist but wasn\'t able to. The dataset has duplicates of both jobs and I\'m trying to exact all the instances in which "data scientist" or "senior data scientist" are mentioned.\n\ndataset[\'position\']=dataset.position.str.lower()\ndataset\n        position            company     description      reviews     location\n0   data scientist lead     ALS TDI   This position is...  30.0    Atlanta\n1   NaN                      xyz        qualified candid.. 4000.0   Texas\n2   data scientist           xcv       python desireable..  232.0    toronto\n3   data scientist           intel     CS Degree needed..  322145.0   Newyork\n4   senior data scientist   amazon     python, excel....   23222.0     montreal\n.\n.\n5000  data scientist/machine  yahoo   sql needed plus...  Nan            Atlanta\n\n\nI\'m using the following to create a dataframe that has explicitly the records for positions that says \'data scientist\' only and not the variations found in index 0 and 5000. \n\nfiltered = dataset[dataset[\'position\'].str.contains(\'data scientist\', na=False)]\n\n\nbut it doesn\'t work as it provides all the records in which strings contain "data scientist" including "senior data scientist" and "data scientist/machine"\n'
"This is the sample of my dataset (in image enter image description heree), i need each unique placeID to have only one rated value by averaging all the ratings for each placeID, i tried doing it with python and pandas as lib but it doesn't work, help needed!!\n\n\n 1. \n\n\n\n"
"I have a dataframe (below, i.e, membership), one field (A) has some row with the value in a sorted manner. There is also a new field (new) which at the beginning of the process is a copy of the field C. What I would like to do is that, if the previous row in A is the same as the current row in A, and if either the current row of new or previous row of new is 1, the assign 1 to the current new. In the end, at the lasts of the repeated values of A, new will be 1 or 0 depending on the conditions in the function and the previous values where A is repeated will have new to be 0. I am able to accomplish that with the function below. \n\nmembership = pd.DataFrame.from_dict(dict([('A', ['20000000460', '20000000460', '20000000460','20000000460','20000000459','20000000461','20000000461','20000000462','20000000464','20000000464','20000000464','20000000464','20000000465','20000000465','20000000466']), ('B', [4,0, 5,0, 6,0,2,5,6,7,4,3,2,7,9]), ('C', [1,1,0,0,0,1,0,1,1,1,0,0,0,0,1])]))\n\ndef members(df, field):\ndf[field] = df.C\nprint(field)\nfor i in range(1, df.shape[0]):\n    if (df.loc[i, 'A'] == df.loc[i-1, 'A']) and\\\n    (df.loc[i-1, field] == 1 or df.loc[i, field] == 1):\n        df.loc[i, field] = 1\n        df.loc[i-1, field] = 0\n\n\nThe results of this function on the dataframe is in this enter image description here\n\nThe issue is that, I have a very large dataset and running this function on it is very slow. How can I improve the code to make it faster? I know if I am able to vectorize this function in pandas, the time will improve significantly. How can I vectorize this function?\n"
'I am trying to rename rows in my dataframe using a dictionary and the map function. Problem is some of the rows don\'t have the same text.\n\nHere is the code I have:\n\nfb_posts[\'title\'] = fb_posts[\'title\'].astype(str)\ndef converts(i):\n  if \'link\' in i:\n    i == \'link\'\n  elif \'post\' in i:\n    i == \'post\'\n  elif \'status\' in i:\n    i == \'stats\'\n  elif \'timeline\' in i:\n    i == \'timeline\'\n  return i\nfb_posts[\'title\'] = fb_posts[\'title\'].apply(converts(i))\n\n\nSo i started off by converting everything in the column into strings so I could find if a string contained a certain letter and convert the string according if it did.\n\nHowever this returns the following traceback:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-310-6ddc37cbbb4d&gt; in &lt;module&gt;()\n----&gt; 1 fb_posts[\'title\'] = fb_posts[\'title\'].apply(converts(i))\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/series.py in apply(self, func, convert_dtype, args, **kwds)\n   2532         # if we are a string, try to dispatch\n   2533         if isinstance(func, compat.string_types):\n-&gt; 2534             return self._try_aggregate_string_function(func, *args, **kwds)\n   2535 \n   2536         # handle ufuncs and lambdas\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/base.py in _try_aggregate_string_function(self, arg, *args, **kwargs)\n    307             return f(self, *args, **kwargs)\n    308 \n--&gt; 309         raise ValueError("{arg} is an unknown string function".format(arg=arg))\n    310 \n    311     def _aggregate(self, arg, *args, **kwargs):\n\nValueError: Person updated his status. is an unknown string function\n\n\nHere is a sample of my database:\n\n title   \nPerson shared a link.\nPerson shared a post.\nPerson posted on x\'s timeline\nPerson posted on y\'s timeline\nPerson posted on a\'s timeline\n\n'
"I need to calculate the mean of a certain column in DataFrame, so that means for each row is calculated excluding the previous values of the row for which it's calculated in certain group. Lets assume we have this dataframe, this is the expected output  \n\nis there any way that like iterate each row by index, adding previous row by index in every iteration, and then calculating mean. I wonder if there's a more efficient way of doing it\n\nunit    A      Expected \nT10     8      8\nT10     7      7.5\nT10     12     9\nT11     10     10\nT11     6      8\nT12     17     17\nT12     7      12\nT12     3      9\n\n"
'I am trying to split a large dataset into train/valid/test sets from Food101 dataset  for image classification \n\nand the structure of a dataset is like this and has all images in one folder\n\n\'\',\n\'Structure:\',\n\'----------\',\n\'pec/\',\n\'    images/\',\n\'        &lt;class_name&gt;/\',\n\'            &lt;image_id&gt;.jpg\',\n\'    meta/\',\n\'        classes.txt\',\n\'        labels.txt\',\n\'        test.json\',\n\'        test.txt\',\n\'        train.json\',\n\'        train.txt\',\n\'\',\n\'All images can be found in the "images" folder and are organized per class. All\',\n\'image ids are unique and correspond to the foodspotting.com review ids. \n\'\',\n\'The test/train splitting used in the experiment of our paper can be found in\',\n\'the "meta" directory.\', (edited) ```\n\n\n\nI want to divide images dataset to train/valid/test  with the list of filenames given in train.txt and test.txt, which author used \n\n\n\nthe Shape of train,valid, test lists:  (101, 600) , (101, 150) , 25250\n\nIn colab ,i run following code\n\n\nfor x in range(train.shape[0]):\n    for y in range(train.shape[1]):\n\n     temp = train[x,y] + ".jpg"\n\n     foldername = temp.split(\'/\')[0]\n\n     !mv /content/food-101/images/$temp /content/food101/train/$foldername/\n\n\nIndividually moving images by running nested loop by taking filenames in lists, is  taking forever time to create folders as there are 100100 images total so,\n\nI have a list of filenames for train/valid and test set but how to make them into folders so that we can feed it to image classifier in pytorch image folder format(i mean train / valid / test set are three different folders and each folder has subfolders of each class)\n\nPlease Do tell if anyone knows how to do this please, and please I really need your help here,  THanks:smile:\n'
'I have a DataFrame that contains months and years:\n\ndf:\n    month   year\n0   Jan     2012.0\n1   Feb     2012.0\n2   Mar     2012.0\n3   Apr     2012.0\n4   May     2012.0\n5   Jun     2012.0\n6   Jul     2012.0\n7   Aug     2012.0\n8   Sep     2012.0\n9   Oct     2012.0\n10  Nov     2012.0\n11  Dec     2012.0\n\n\nI want to add another column which determines a business-year which starts on Mar on every year Something like this:. \n\ndf:\n        month   year     business_year\n    0   Jan     2012.0     2011\n    1   Feb     2012.0     2011\n    2   Mar     2012.0     2012\n    3   Apr     2012.0     2012\n    4   May     2012.0     2012\n    5   Jun     2012.0     2012\n    6   Jul     2012.0     2012\n    7   Aug     2012.0     2012\n    8   Sep     2012.0     2012\n    9   Oct     2012.0     2012\n    10  Nov     2012.0     2012\n    11  Dec     2012.0     2012\n    12  Jan     2013.0     2012\n    13  Feb     2013.0     2012\n\n'
"I have a pandas DataFrame object with a column named 'order_id'. Rows with the same id belong to one and the same order (size can be anything between 1 and 1000), e.g.:\n\nsales_orders = {\n    'order_id': [1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4],\n    # multiple other fields \n}\ndf = pd.DataFrame(sales_orders)\n\n\nI need to split the groups into smaller chunks based on a cut-off value like for instance 3. Ideally, nothing else changes other than the suffix of the column, i.e.:\n\n'order_id': [1-0, 1-0, 1-0, 1-1, 1-1, 1-1, 1-2, 2, 2, 2, 3-0, 3-0, 3-0, 3-1, 3-1, 3-1, 3-2, 3-2, 3-2, 4]\n\n\nI assume that one can simply step through the groups and touch each group individually in a for loop like this:\n\nfor order_id, group in df.groupby(by=['order_id']):\n    if group.shape[0] &gt; 2:\n        # change column line by line\n\n\nBut this looks incredibly unpanda'ish and horrifically slow. Thus I would appreciate a sensible, performant and readable solution ;)\nThanks in advance for the help!\n"
'I have a Dataframe that currently looks like this:\n\nimage         source                               label\nbookshelf     A                      [flora, jar, plant]\nbookshelf     B                    [indoor, shelf, wall]\nbookshelf     C             [furniture, shelf, shelving]\ncactus        A                     [flora, plant, vine]\ncactus        B                [building, outdoor, tree]\ncactus        C                  [home, house, property]\ncars          A          [parking, parking lot, vehicle]\ncars          B                     [car, outdoor, tree]\ncars          C            [car, motor vehicle, vehicle]\n\n\nWhat I would like to get is the count of duplicate labels for each source per image, i.e.:\n\n\nfor the image bookshelf, sources B and C share the "shelf" label (B+=1; C+=1)\nfor the image cactus, no sources share the same labels\nfor the image cars, sources B and C share the label "car" (B+=1; C+=1) and sources A and C share the label "vehicle" (A+=1; C+=1)\n\n\nThe response object would be the number of times sources share labels. In the example above, (1) would increase the B and C counts by 1 each, and (3) would increase the B and C counts by 1 each and the A and C counts by 1 each:\n\n{ \'A\': 1, \'B\': 2, \'C\': 3 }\n\n\nReproducible example:\n\nfrom pandas import DataFrame\ndf = DataFrame({\n  \'image\': [\'bookshelf\', \'bookshelf\', \'bookshelf\',\n            \'cactus\', \'cactus\', \'cactus\',\n            \'cars\', \'cars\', \'cars\'],\n  \'source\': [\'A\', \'B\', \'C\',\n             \'A\', \'B\', \'C\',\n             \'A\', \'B\', \'C\'],\n  \'label\': [\n    [\'flora\', \'jar\', \'plant\'],\n    [\'indoor\', \'shelf\', \'wall\'],\n    [\'furniture\', \'shelf\', \'shelving\'],\n    [\'flora\', \'plant\', \'vine\'],\n    [\'building\', \'outdoor\', \'tree\'],\n    [\'home\', \'house\', \'property\'],\n    [\'parking\', \'parking lot\', \'vehicle\'],\n    [\'car\', \'outdoor\', \'tree\'],\n    [\'car\', \'motor vehicle\', \'vehicle\']]\n  },\n  columns = [\'image\', \'source\', \'label\']\n)\n\n\nWhile there are usually 3 labels per source/image, this isn\'t guaranteed.\n\nAny ideas on how I could achieve this with good performance? I have a few million records to process like it...\n'
"I am working on a dataset that has some 26 million rows and 13 columns including two datetime columns arr_date and dep_date. I am trying to create a new boolean column to check if there is any US holidays between these dates. \nI am using apply function to the entire dataframe but the execution time is too slow. The code has been running for more than 48 hours now on Goolge Cloud Platform (24GB ram, 4 core). Is there a faster way to do this?\n\nThe dataset looks like this:\nSample data\n\nThe code I am using is - \n\nimport pandas as pd\nimport numpy as np\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\ndf = pd.read_pickle('dataGT70.pkl')\ncal = calendar()\ndef mark_holiday(df):\n    df.apply(lambda x: True if (len(cal.holidays(start=x['dep_date'], end=x['arr_date']))&gt;0 and x['num_days']&lt;20) else False, axis=1)\n    return df\n\ndf = mark_holiday(df)\n\n"
"I have a precipitation data-series in a Pandas DataFrame with as index the dates between 2009-2018 (3652). I'm trying to find a way to decrease or increase proportional the cumulative precipitation relative to the mean by a given percentage (decrease for values &lt; mean, increase for values > mean).\n\nWhat I managed to do\n\nMaking a list with only non-zero values, sort it and increase or decrease the values proportional to the place of the mean value. Like this:\n\n\n\nThe blue line represents the original values. The orange line a 200% (for clear example) increase and decrease (there are now negative values but with a percentage of 20% that won't happen). The mean is 4.65 and in this case on place 1261.\n\nThe Problem\nNow I wan't to do this for a Dataframe like below, but keep all the precipitation values in place.\n\n[3652 rows x 1 columns]\n             Precipitation\n2009-01-01   0.000000\n2009-01-02   0.600000\n2009-01-03   0.000000\n2009-01-04   0.900000\n2009-01-05   2.000000\n2009-01-06   0.000000\n...\n\n\nQuestion\nIs someone familiar how to increase or decrease values proportional to the mean. So that the highest and lowest values actual increase and decrease 20% but all the values in between proportional. \n"
'I\'m trying to calculate the score of a DecisionTreeRegressor with the following code:\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\n\n# some features are better using LabelEncoder like HouseStyle but the chance that they will affect\n# the target LotFrontage are small so we just use HotEncoder and drop unwanted columns later\nencoded_df = pd.get_dummies(train_df, prefix_sep="_", columns=[\'MSZoning\', \'Street\', \'Alley\',\n                                                       \'LotShape\', \'LandContour\', \'Utilities\',\n                                                       \'LotConfig\', \'LandSlope\', \'Neighborhood\',\n                                                       \'Condition1\', \'Condition2\', \'BldgType\', \'HouseStyle\'])\nencoded_df = encoded_df[[\'LotFrontage\', \'LotArea\', \'LotShape_IR1\', \'LotShape_IR2\', \'LotShape_IR3\',\n           \'LotConfig_Corner\', \'LotConfig_CulDSac\', \'LotConfig_FR2\', \'LotConfig_FR3\', \'LotConfig_Inside\']]\n\n# imputate LotFrontage with the mean value (we saw low outliers ratio so we gonna use the mean value)\nencoded_df[\'LotFrontage\'].fillna(encoded_df[\'LotFrontage\'].mean(), inplace=True)\nX = encoded_df.drop(\'LotFrontage\', axis=1)\ny = encoded_df[\'LotFrontage\'].astype(\'int32\')\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nclassifier = DecisionTreeRegressor()\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\ny_test = y_test.values.reshape(-1, 1)\nclassifier.score(y_test, y_pred)\nprint("Accuracy is: ", accuracy_score(y_test, y_pred) * 100)\n\n\nwhen it\'s gets to calculating the score of the model I get the following error:\n\nValueError: Number of features of the model must match the input. Model n_features is 9 and input n_features is 1 \n\n\nNot sure as to why it happens because according sklearn docs the Test Samples are to be in the shape of (n_samples, n_features)\nand y_test is indeed in this shape:\n\ny_test.shape # (365, 1)\n\n\nand the True labels should be in the shape of (n_samples) or (n_samples, n_outputs) and y_pred is indeed in this shape:\n\ny_pred.shape # (365,)\n\n\nThe dataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n'
'All possible combinations of three columns\n\nI am not able to do it with itertools.combinations or itertools.permutations.\n\nInput dataframe :\n\na   b   c\n1   101 1001\n2   102 1002\n3   103 1003\n\n\nExpected dataframe :\n\na   b   c\n1   101 1001\n1   101 1002\n1   101 1003\n1   102 1001\n1   102 1002\n1   102 1003\n1   103 1001\n1   103 1002\n1   103 1003\n2   101 1001\n2   101 1002\n2   101 1003\n2   102 1001\n2   102 1002\n2   102 1003\n2   103 1001\n2   103 1002\n2   103 1003\n3   101 1001\n3   101 1002\n3   101 1003\n3   102 1001\n3   102 1002\n3   102 1003\n3   103 1001\n3   103 1002\n3   103 1003\n\n'
"I have two large data sets which I can't do the aggregations by combining two dataframes. I have to do the aggregation on df_train first, then map the values to the df_test.\n\ndf_train and df_test have the same exact id1 and id2, but the df_test have more samples. I'm computing the target mean on id1 and id2 and store it as a dictionary for memory issues.\n\ntarget_mean = df_train.groupby(['id1', 'id2'])['target'].mean().to_dict()\n\n\nThis is the output of the aggregation. The keys are tuple pairs with id1 as the first element and id2 as the second element, and the values are target means of the groups.\n\n{(0, 0): 146.45497131347656,\n (1, 0): 74.86539459228516,\n (2, 0): 14.551384925842285,\n (3, 0): 235.5499725341797,\n (4, 0): 976.5567626953125,\n (5, 0): 17.894445419311523,\n (6, 0): 64.06660461425781,\n (7, 0): 350.33416748046875,\n (7, 1): 3097.043701171875,\n (8, 0): 256.92779541015625,\n (9, 0): 72.7147445678711 }\n\n\nHow can I map those values to id1 and id2 columns properly?\n\n(There are 60 million rows of data, 1449 id1 and 4 id2 values, so speed is important)\n\nEDIT:\n\ndf_train[['id1', 'id2']].map(target_mean)\n\n\nI tried this, but map is only supported by pd.Series.\n"
"I am learning data science for the first time. The course I am taking recommends to use Jupyter Notebook as IDE.\n\nI can't find the feature of predetermining potential errors before running the code. \n\nI used Sublime Text when I first learned Python, and if I'm remembering correctly Sublime Text underlined typos or undefined variables etc.\n\nIs there a way to enable the feature in Jupyter Notebook?\n\nI am curious as to why such an essential feature to an IDE is missing in Jupyter Notebook.\n\n++ I also just noticed that auto-complete feature is missing as well.\n"
"A Mersenne number is any number that can be written as  2^p−1  for some  p . For example, 3 is a Mersenne number (2^2-1) as is 31 (2^5-1). \nWrite a function that accepts an exponent  p  and returns the corresponding Mersenne number.\n\ndef mersenne number(p):\n   return ((2**p)-1)\n\n\nI'm just at the start of my programming course and I get stuck in simply things. I wrote that so far and have no idea how to make it to the end, and even if that is correct so far. I''l be grateful for any help from your side.\n"
"I have a 87288-point dataset that I need to filter. The filtering fields for the dataset are a X position and a Y position, as latitude and longitude. Plotted the data looks like this:\n\n\n\nThe problem is , I only need data along a certain path, which is known in advance.  Something like this:\n\n\n\nI already know how to filter data in a Pandas DF, but given the path is not linear, I need an effective strategy to clear out all the noisy data with a certain degree of precision (since the dataset is so large, manually picking the points is not an option).\n\nHere is some sample data.The only important columns are Latitude and Longitude, Y and X respectively.\n\nSesion,Tiempo,Latitud,Longitud,PM2.5,Modo,Hora,DiaSemana\nM-O-AM-07OCT19-DMR,2019-10-01 09:48:17.625,3.3659550000000005,-76.5288288,13.0,OUTDOOR,AM,1\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:03.555,3.3661757000000003,-76.5289441,12.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:04.596,3.3661757000000003,-76.5289441,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:05.572,3.3661767,-76.5289375,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:06.614,3.3661790999999996,-76.5289188,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:07.581,3.3661814,-76.5289024,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:08.588,3.3661847999999996,-76.52889820000001,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:09.570,3.3661922,-76.52890450000001,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:10.579,3.3661922,-76.52890450000001,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:11.577,3.3662135,-76.52893370000001,12.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:12.611,3.3662227999999996,-76.5289516,12.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:13.561,3.3662227999999996,-76.5289516,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:14.631,3.3662346,-76.5289927,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:15.554,3.3662421,-76.52901440000001,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:16.623,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:17.593,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:18.617,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:19.608,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:20.605,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:21.594,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:22.608,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:23.620,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:24.611,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:25.622,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:26.590,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:27.619,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:28.595,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:29.628,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:30.621,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\n\n\nI have tried of handpicking a few points inside the route, and filtering the rest using a fixed min distance, something like this.\n\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nimport numpy as np\nfrom salem import get_demo_file, DataLevels, GoogleVisibleMap, Map\nimport geopy.distance\n\ndef get_dist(coords_1 , coords_2):\n    return geopy.distance.distance(coords_1, coords_2).meters\n\ndists=[\n    (-76.5297163,3.3665631),\n    (-76.5307019,3.3656924),\n    (-76.5314718,3.3646900),\n    (-76.5319956,3.3638394),\n    (-76.5316622,3.3621781),\n    (-76.5311999,3.3611796),\n    (-76.5308636,3.3599338),\n    (-76.5306335,3.3585191),\n    (-76.5304758,3.3577502),\n    (-76.5303957,3.3561101),\n    (-76.5302998,3.3543178),\n    (-76.5302220,3.3531897),\n    (-76.5302369,3.3515283),\n    (-76.5303363,3.3502667),\n    (-76.5305351,3.3485951),\n    (-76.5306779,3.3475220),\n    (-76.5308545,3.3456382),\n    (-76.5307738,3.3446934),\n    (-76.530618,3.3430422)\n]\ndf = pd.read_csv('movil.csv')\n\n\nfor index, row in df.iterrows():\n    if index%1000 ==0:\n        print(index)\n    mind=None\n    for i in dists:\n        if mind:\n            d=get_dist((row['Latitud'],row['Longitud']),(i[1],i[0]))\n            if d&lt;mind:\n                mind=d\n        else:\n            mind=get_dist((row['Latitud'],row['Longitud']),(i[1],i[0]))\n    if mind&gt;125:\n        df.drop(index, inplace=True)\n\nprint(df)\n\n\nUsing these approach I managed to get some cleaning, but I feel a lot of useful data is getting filtered.\n\n\n"
'I have 4 different csv files (extracted from 1 xlsm workbook). My plot function is a bit complex but works really perfect on 1 of these files each. If I try to plot all 4 files with a loop I get always an \n\n\n  ValueError: arrays must all be same length \n\n\nThen the function is plotting the first element from the list. I want to plot the same plt.figure with subplots for each file from csv list. I think the script is trying to plot all different csv files on one plot (the csv files have the same structure but different amount of rows)\n\ndef plotMSN(data):  \n\n    csv = data  \n    #csv = "LL.csv" (this is working perfect)\n    day = plotday\n    ....\n\n#this is not working\ncsvlist = ["LL.csv","UL.csv","UR.csv","LR.csv"]\n\nfor i in csvlist:\n    plotMSN(i)\n    time.sleep(5)\n\n'
"I have a 2d array. Each row of the array is a cooking recipe and each column contains the ingredients of the recipe. I want to create a normalised binary matrix of the ingredients. The normalised binary matrix will have the same number of rows as the recipe matrix ( for every recipe) and a binary vector of all the ingredients in every column. If the ingredient is present in the recipe the element will have a value of 1 if not a value of zero. \n\nRight now the binary matrix has occurrences above 1. That is happening because the count vectorizer matches more than one words in the vocabulary.\nFor example suppose my vocabulary is \n\n{'chicken': 0, 'chicken broth': 1, 'carrots': 2}\n\n\nand suppose the vector i want to transform is\n\n['chicken','carrots']\n\n\nthe binary matrix will be transformed like this\n\n[2, 0, 1]\n\n\nwhile i want it to be\n\n[1,0,1]\n\n\nthat is happening because the 'chicken' is matched with 'chicken' but also matched with 'chicken broth'. Below there is a snippet of my code that produces this. I want to match only exact occurrences of a word in the vocabulary. Are there any parameters or any way that i can use to achieve this? I tried the ngrams parameter without success.\n\ncv = CountVectorizer(vocabulary=unique_igredients,lowercase=False)\ntaggedSentences = cv.fit_transform(unique_igredients)\n\n#encode document\n\nfor i in recipes:\n    vector = cv.transform(i)\n    mylist = sum(map(numpy.array, vector.toarray()))\n    vectorized_matrix_m.append(mylist.tolist())\n\n"
"i am new in Data science and I want to make a classification from categorical data.\nI wish to do before using K-means algorithms but i got this 'error ValueError: bad input shape (2835, 18)' when i use fit_transform() and i don't know how fix it. I hope that someone could help me.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n#load my data\nmyData = pd.read_excel('panelForOneHot.xlsx')\nmyData = myData.dropna()\nmyData.reset_index(drop = True, inplace = True)\nmyData\n\nvalues = np.array(myData)\nprint(values)\n\n#integer encode\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(values)\n\n"
'I have pivot dataframe with information on what territory many travelers are on their journey to around the world. \nMy pivot df looks something like this:\n\n     Name           Anna         Robert        James\nDate               \n2018-10-01        Bulgaria       Spain         Croatia\n2018-10-02        Portugal       NaN           Portugal  \n2018-10-03        Spain          USA           Spain\n2018-10-04        USA            USA           Spain\n2018-10-05        USA            Canada        USA\n\n\nThere are 100 columns (100 travelers) and 300 days.\n\nBased on such data, how can I explore which routes are the most popular?\nIt can be seen at first glance that all of them came to the USA from Spain. Robert also flew to the USA from Spain only that his flight lasted 2 days. Two of the three presented travelers came to Spain from Portugal so this is also a popular route.\n\nIs there any way to show popular routes using ML algorithms? I will be extremely grateful for any tips.\n\nEDIT:\nWe can assume that the route has 2 nodes, so based on this df Spain-USA is a popular route\n'
'I have a time series and I would like to detect when the values are increasing rapidly increasing and pinpoint (color perhaps) the timeframe of the increase. For example in the following plot I would like to pinpoint the spikes for a specific time window in x-axis (for example 2018-05-22)\n\n\n\nI am able to find the 20% maximum values but that does not help me. I want to focus on the rapid increases instead.\n\nd = pd.Series(df[\'TS\'].values[1:] - df[\'TS].values[:-1], index=df[\'TS\'].index[:-1]).abs()\nthreshold = 0.8\nm = d.max()\nprint(d &gt; m * threshold)\n\n\nFor example, ruptures is doing something similar visually. There is an example with random data along the plot it produces:\n\nimport matplotlib.pyplot as plt\nimport ruptures as rpt\n\n# generate signal\nn_samples, dim, sigma = 1000, 3, 4\nn_bkps = 4  # number of breakpoints\n#signal, bkps = rpt.pw_constant(n_samples, dim, n_bkps, noise_std=sigma)\n\n\n# detection\nalgo = rpt.Pelt(model="rbf").fit(signal)\nresult = algo.predict(pen=10)\n\n# display\nrpt.display(signal, bkps, result)\nplt.show()\n\n\n\n'
'There are lists like [2.1, 2.01, 6, 2.2, 1.9] and [2, 7.1, 7.2, 6.9]\nIs there a function in numpy (or other library) to remove numbers which deviate more than 5% from the other numbers. In these cases it would be 6 and 2.\n\nThe list size isnt fixed. Nor the numbers range.\n\nThanks\n'
'I am new to web scraping and I\'m trying to scrape the "statistics" page of yahoo finance for AAPL. Here\'s the link: https://finance.yahoo.com/quote/AAPL/key-statistics?p=AAPL\n\nHere is the code I have so far...\n\nfrom bs4 import BeautifulSoup\nfrom requests import get\n\n\nurl = \'https://finance.yahoo.com/quote/AAPL/key-statistics?p=AAPL\'\nresponse = get(url)\nsoup = BeautifulSoup(response.text, \'html.parser\')\n\nstock_data = soup.find_all("table")\n\nfor stock in stock_data:\n    print(stock.text)\n\n\nWhen I run that, I return all of the table data on the page. However, I only want specific data from each table (e.g. "Market Cap", "Revenue", "Beta").\n\nI tried messing around with the code by doing print(stock[1].text) to see if I could limit the amount of data returned to just the second value in each table but that returned an error message. Am I on the right track by using BeautifulSoup or do I need to use a completely different library? What would I have to do in order to only return particular data and not all of the table data on the page?\n'
'there!\n\nI\'m studying the IBM Data Science course by Coursera and I\'m trying to create some snippets to practice. I\'ve created the following code:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Import and format the dataframes\nibov = pd.read_csv(\'https://raw.githubusercontent.com/thiagobodruk/datasets/master/ibov.csv\')\nifix = pd.read_csv(\'https://raw.githubusercontent.com/thiagobodruk/datasets/master/ifix.csv\')\nibov[\'DATA\'] = pd.to_datetime(ibov[\'DATA\'], format=\'%d/%m/%Y\')\nifix[\'DATA\'] = pd.to_datetime(ifix[\'DATA\'], format=\'%d/%m/%Y\')\nifix = ifix.sort_values(by=\'DATA\', ascending=False)\nibov = ibov.sort_values(by=\'DATA\', ascending=False)\nibov = ibov[[\'DATA\',\'FECHAMENTO\']]\nibov.rename(columns={\'FECHAMENTO\':\'IBOV\'}, inplace=True)\nifix = ifix[[\'DATA\',\'FECHAMENTO\']]\nifix.rename(columns={\'FECHAMENTO\':\'IFIX\'}, inplace=True)\n\n# Merge datasets \ndf_idx = ibov.merge( ifix, how=\'left\', on=\'DATA\')\ndf_idx.set_index(\'DATA\', inplace=True)\ndf_idx.head()\n\n# Split training and testing samples\nx_train, x_test, y_train, y_test = train_test_split(df_idx[\'IBOV\'], df_idx[\'IFIX\'], test_size=0.2)\n\n# Convert the samples to Numpy arrays\nregr = linear_model.LinearRegression()\nx_train = np.array([x_train])\ny_train = np.array([y_train])\nx_test = np.array([x_test])\ny_test = np.array([y_test])\n\n# Plot the result\nregr.fit(x_train, y_train)\ny_pred = regr.predict(y_train)\nplt.scatter(x_train, y_train)\nplt.plot(x_test, y_pred, color=\'blue\', linewidth=3) # This line produces no result\n\n\nI experienced some issues with the output values returned by the train_test_split() method. So I converted them to Numpy arrays, then my code worked. I can plot my scatter plot normally, but I can\'t plot my prediction line.\n\nRunning this code on my IBM Data Cloud Notebook produces the following warning:\n\n\n  /opt/conda/envs/Python36/lib/python3.6/site-packages/matplotlib/axes/_base.py:380: MatplotlibDeprecationWarning: \n  cycling among columns of inputs with non-matching shapes is deprecated.\n    cbook.warn_deprecated("2.2", "cycling among columns of inputs "\n\n\nI searched on Google and here on StackOverflow, but I can\'t figure what is wrong.\n\nI\'ll appreciate some assistance. Thanks in advance!\n'
'There are 16 categorical variables in my data set.I would like to apply a 0-1 conversion to some of them one-hot.\n\none hot code : df_one_hot =pd.get_dummies(df,columns = ["guardian"],prefix = ["guardian"])\ndf_one_hot.head()\n\n0-1 code : df["new_day"] =np.where(df["day"].str.contains("Sun"),1,0)\n\nbecause I have more than one application, I want all the conversions I do to be permanent.I applied one hot to the first data.when I applied it to the second data, I found that what I applied above was not permanent.how can I make permanent operations for both conversions? inplace=True, this function no\n\nimport pandas as pd \ndf = pd.read_csv("../data/student-mat.csv", sep=\';\')\ndf\n.\n.\n.\ndf_one_hot =pd.get_dummies(df,columns = ["reason"],prefix = ["reason"])\ndf_one_hot.head()\n#I am doing the first conversion. But when I do it again for the second data, \n#the first conversion is not permanent.\ndf_one_hot =pd.get_dummies(df,columns = ["guardian"],prefix = ["guardian"])\ndf_one_hot.head()\n\n'
"I've search this topic in several posts and I have tried all possible solutions that people give but even though still does not work for me.\n\nIm trying to replace each NaN value from each column, with it's column mean value. In other words:\n\n    A   B\n 1: 2   3\n 2: 2   1\n 3: NaN 4\n\nColumn A mean = 1.3\n\n    A   B\n 1: 2   3\n 2: 2   1\n 3: 1.3 4 \n\n\nI've tried:\n\ndf.fillna(value=0, axis=1, inplace=True)\n\n\nand works fine, but as soon as I tried:\n\ndf.fillna(value=df.mean(axis=1), inplace=True)\n\n\nI don't get any changes at all, NaN value still there.\n\nHere is the piece of code:\n\n# Drop 'station' column\ndel final_df['station']\n# Replace NaN with column mean value\nfinal_df.fillna(value=final_df.mean(axis=1), inplace=True)\nfinal_df.head()\n\n\n\n"
'Consider a Pandas dataframe with multiple columns, each column a country name, and multiple rows, each row a date. The cells are data about countries, which vary in time. This is the CSV:\n\nhttps://pastebin.com/bJbDz7ei\n\nI want to make a dynamic plot (animation) in Jupyter that shows how the data evolves in time. Out of all countries in the world, I only want to show the top 10 countries at any given time. So the countries shown in the graph may change from time to time (because the top 10 is evolving).\n\nI also want to maintain consistency in terms of colors. Only 10 countries are shown at any time, and some countries appear and disappear almost continuously, but the color for any country should not change throughout the animation. The color for any country should stick from start to finish.\n\nThis is the code I have (EDIT: now you can copy/paste the code into Jupyter and it works out of the box, so you can easily see the bug I\'m talking about):\n\nimport pandas as pd\nimport requests\nimport os\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as ani\n\nrel_big_file = \'rel_big.csv\'\nrel_big_url = \'https://pastebin.com/raw/bJbDz7ei\'\n\nif not os.path.exists(rel_big_file):\n    r = requests.get(rel_big_url)\n    with open(rel_big_file, \'wb\') as f:\n        f.write(r.content)\n\nrel_big = pd.read_csv(rel_big_file, index_col=\'Date\')\n\n# history of top N countries\nchamps = []\n# frame draw function\ndef animate_graph(i=int):\n    N = 10\n    # get current values for each country\n    last_index = rel_big.index[i]\n    # which countries are top N in last_index?\n    topN = rel_big.loc[last_index].sort_values(ascending=False).head(N).index.tolist()\n    # if country not already in champs, add it\n    for c in topN:\n        if c not in champs:\n            champs.append(c)\n    # pull a standard color map from matplotlib\n    cmap = plt.get_cmap("tab20")\n    # draw legend\n    plt.legend(topN)\n    # make a temporary dataframe with only top N countries\n    rel_plot = rel_big[topN].copy(deep=True)\n    # plot temporary dataframe\n    p = plt.plot(rel_plot[:i].index, rel_plot[:i].values)\n    # set color for each country based on index in champs\n    for i in range(0, N):\n        p[i].set_color(cmap(champs.index(topN[i]) % 20))\n\n%matplotlib notebook\nfig = plt.figure(figsize=(10, 6))\nplt.xticks(rotation=45, ha="right", rotation_mode="anchor")\n# x ticks get too crowded, limit their number\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(nbins=10))\nanimator = ani.FuncAnimation(fig, animate_graph, interval = 333)\nplt.show()\n\n\nIt does the job - somewhat. I store the top countries in the champs list, and I assign colors based on the index of each country in champs. But only the color of the plotted lines is assigned correctly, based on the index in champs.\n\nThe color in the legend is assigned in a rigid manner, first country in the legend always gets the same color, second country in the legend always gets a certain color, etc, and basically the color of each country in the legend varies throughout the animation when countries move up and down in the legend.\n\nThe colors of the plotted lines obey the index in champs. The colors of countries in the legend are based on the order within the legend. This is not what I want.\n\nHow do I assign the color for each country in the legend in a way that matches the plot lines?\n'
'I have an n x n column where two of columns as follows:\n\nheight  cost  item_x    cost2    item_y   weight\n15      10    bat        45       mitt    2\n19      12    ball       30       ball    4\n24      13    gloves     25       gloves  6\n22      14    bat        20       mitt    8\n\n\nI want to create unique columns for unique values of item_x and item_y, and fill them with appropriate values from cost and cost2 columns. So the expected output would be:\n\nheight  bat_x  ball_x  gloves_x  mitt_y  ball_y  gloves_y   weight\n15      10     0       0         45      0        0         2\n19      0      12      0         0       30       0         4\n24      0      0       13        0       0        25        6\n22      14     0       0         20      30       0         8\n\n\nAny help would be much appreciated! \n'
'A small snippet from my dataframe\n\nI have separate columns for month and date. I need to parse only month and date into a pandas datetime type(other datetime types would also help), so that I could plot a TimeSeries Line plot. \n\nI tried this piece of code, \n\ndf[\'newdate\'] = pd.to_datetime(df[[\'Days\',\'Month\']], format=\'%d%m\')\n\n\nbut I threw me an error\n\nKeyError: "[\'Days\' \'Month\'] not in index"\n\n\nHow should I approach this error?\n'
"I'm learning Pandas, specially now with Datetimes. I'm searching for days a way to select rows by their Datetime column. If the Datetime column values are on a range between the array spacex and clonx values.\n\nThe two arrays:\n\nclonx = array(['2019-08-14T23:32:00.000000000', '2019-08-14T23:35:00.000000000',\n       '2019-08-14T23:35:00.000000000', ...,\n       '2020-05-24T14:55:00.000000000', '2020-05-24T15:03:00.000000000',\n       '2020-05-25T12:09:00.000000000'], dtype='datetime64[ns]')\n\nspacex = array(['2019-08-14T23:27:00.000000000', '2019-08-14T23:30:00.000000000',\n   '2019-08-14T23:30:00.000000000', ...,\n   '2020-05-24T14:50:00.000000000', '2020-05-24T14:58:00.000000000',\n   '2020-05-25T12:04:00.000000000'], dtype='datetime64[ns]')\n\n\nthe column:\n\n    first['datim']\n\n0      2019-08-14 23:26:00\n1      2019-08-14 23:26:00\n\n2      2019-08-14 23:27:00\n3      2019-08-14 23:30:00\n4      2019-08-14 23:30:00\n               ...        \n5101   2020-05-25 20:48:00\n5102   2020-05-25 20:49:00\n5103   2020-05-26 13:52:00\n5104   2020-05-26 13:52:00\n5105   2020-05-26 14:22:00\nName: datim, Length: 3172, dtype: datetime64[ns]\n\n\nHow can I get the Datetime values from column first['datim'] that are between datetimes of spacex and clonx?\n\nSomething like this:\n\nstart_date = spacex[i]\nend_date = clonx[i]\nfor i in range:\n    final = (first['datim'] &gt;= start_date) &amp; (first['datim'] &lt;= end_date)\nresult final\n\n\nOr maybe with beween_time but can't find a way to make it work with arrays.\n\nAppreciate  your time!\n"
'So I\'m getting started with Kaggle &amp; I was doing the guided task of predicting who survived &amp; who didn\'t in the Titanic Crash. \n\nI did everything as was asked to.\n\nSo my last code cell looks something like this\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\'Survived\']\nfeatures = ["Pclass","Sex","SibSp","Parch"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(train_data[features])\nmodel = RandomForestClassifier(n_estimators=1,max_depth=5,random_state=1)\nmodel.fit(X,y)\npredictions = model.predict(X_test)\n\n\n\noutput = pd.DataFrame({\'PassengerId\': test_data.PassengerId, \'Survived\': predictions})\noutput.to_csv(\'my_submission.csv\', index=False)\nprint("Your submission was successfully saved!")\n\n\n\nAfter Compiling it shows the following Error : \n\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-24-7d2fc2ea2973&gt; in &lt;module&gt;\n     11 \n     12 \n---&gt; 13 output = pd.DataFrame({\'PassengerId\': test_data.PassengerId, \'Survived\': predictions})\n     14 output.to_csv(\'my_submission.csv\', index=False)\n     15 print("Your submission was successfully saved!")\n\n/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)\n    433             )\n    434         elif isinstance(data, dict):\n--&gt; 435             mgr = init_dict(data, index, columns, dtype=dtype)\n    436         elif isinstance(data, ma.MaskedArray):\n    437             import numpy.ma.mrecords as mrecords\n\n/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in init_dict(data, index, columns, dtype)\n    252             arr if not is_datetime64tz_dtype(arr) else arr.copy() for arr in arrays\n    253         ]\n--&gt; 254     return arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\n    255 \n    256 \n\n/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n     62     # figure out the index, if necessary\n     63     if index is None:\n---&gt; 64         index = extract_index(arrays)\n     65     else:\n     66         index = ensure_index(index)\n\n/opt/conda/lib/python3.7/site-packages/pandas/core/internals/construction.py in extract_index(data)\n    376                         f"length {len(index)}"\n    377                     )\n--&gt; 378                     raise ValueError(msg)\n    379             else:\n    380                 index = ibase.default_index(lengths[0])\n\nValueError: array length 891 does not match index length 418\n\n\nHowever, I couldn\'t debug what exactly my error was, can someone help? Thank you.\n'
"Based on this post, I was able to construct a layered histogram that is normalised. However, it seems that the normalisation is done with respect to the total number of samples instead of the total number of samples per category. I was wondering how the normalisation can be done per category using altair?\n\nExample:\n\nimport pandas as pd\nimport altair as alt\n\nsource = pd.DataFrame({'age': ['12', '32', '43', '54', '32', '32', '12','20','44','24'],'gender': ['m','m','f','f','f','m','f','m','f','m']})\n\nalt.Chart(source).transform_joinaggregate(\n    total='count(*)'\n).transform_calculate(\n    pct='1 / datum.total'\n).mark_bar().encode(\n    alt.X('age:Q', bin=True),\n    alt.Y('sum(pct):Q', axis=alt.Axis(format='%')),\n    color='gender'\n)\n\n\n\n"
'I want to train a CNN that takes as an input a numpy array of shape (1600, 800, 1) which would contain all 0s except at few pixels where I can have values from range 10 to 3100(This numpy array is not an image) and the output should be of size 310 where each element is a pair containing coordinates(x, y) positions of the points in the input that had non zero values.\n\nIs there any way of doing this? Any insight on this is greatly appreciated. Thanks in advance!\n'
"So I have been trying to use pandas to create a DataFrame that reports the number of graduates working at jobs that do require college degrees ('college_jobs'), and do not require college degrees ('non_college_jobs').\nnote: the name of the dataframe I am dealing with is recent_grads\nI tried the following code:\ndf1 = recent_grads.groupby(['major_category']).college_jobs.non_college_jobs.sum() \n\nor\ndf1 = recent_grads.groupby(['major_category']).recent_grads['college_jobs','non_college_jobs'].sum()\n\nor\ndf1 = recent_grads.groupby(['major_category']).recent_grads['college_jobs'],['non_college_jobs'].sum()\n\nnone of them worked! what am I supposed to do? can somebody give me a simple explanation regarding this? I had been trying to read through pandas documentations and did not find the explanation wanted.\nhere is the head of the dataframe:\n   rank  major_code                                      major major_category  \\\n0     1        2419                      PETROLEUM ENGINEERING    Engineering   \n1     2        2416             MINING AND MINERAL ENGINEERING    Engineering   \n2     3        2415                  METALLURGICAL ENGINEERING    Engineering   \n3     4        2417  NAVAL ARCHITECTURE AND MARINE ENGINEERING    Engineering   \n4     5        2405                       CHEMICAL ENGINEERING    Engineering   \n\n   total  sample_size    men  women  sharewomen  employed      ...        \\\n0   2339           36   2057    282    0.120564      1976      ...         \n1    756            7    679     77    0.101852       640      ...         \n2    856            3    725    131    0.153037       648      ...         \n3   1258           16   1123    135    0.107313       758      ...         \n4  32260          289  21239  11021    0.341631     25694      ...         \n\n   part_time  full_time_year_round  unemployed  unemployment_rate  median  \\\n0        270                  1207          37           0.018381  110000   \n1        170                   388          85           0.117241   75000   \n2        133                   340          16           0.024096   73000   \n3        150                   692          40           0.050125   70000   \n4       5180                 16697        1672           0.061098   65000   \n\n   p25th   p75th college_jobs  non_college_jobs  low_wage_jobs  \n0  95000  125000         1534               364            193  \n1  55000   90000          350               257             50  \n2  50000  105000          456               176              0  \n3  43000   80000          529               102              0  \n4  50000   75000        18314              4440            972  \n\n[5 rows x 21 columns]\n\n"
'I need to generate a list of color representation of numbers (kind of like a heatmap), but just the rgba values. Something that gets as input a list of numbers and return a set of rgba values representing these numbers. Is there a module that do this in python?\nthanks\n'
'I have a pandas dataframe\nCOL1 COL2 COL3        COL4 \n A    B    C    [{COL5: D1, COL6: E1, COL7: F1},\n                 {COL5: D2, COL6: E2, COL7: F2},\n                 {COL5: D3, COL6: E3, COL7: F3},\n                  ...\n\n                 {COL5: D10, COL6: E10, COL7: F10}]\n\nThe output I want will be:\nCOL1 COL2 COL3  COL5  COL6  COL7\n A    B    C     D1    E1    F1 \n A    B    C     D2    E2    F2\n A    B    C     D3    E3    F3\n...\n A    B    C     D10   E10   F10\n                  \n\nIs there any convenient way I can use?\n'
"I noticed that one can create grouped bar charts in Altair, by evaluating the column of a DataFrame. My issue is that my data doesn't have each group as a value of an specific column.\nSo, is it possible to use the column name as a group name in Altair (without evaluating the column) or modifying the DataFrame?\nBelow is the DataFrame I have and the grouped bar chart I need:\n\n"
"I am working on a dataframe of shape 146 rows x 48 columns. The columns are\n['Region','Rank 2015','Score 2015','Economy 2015','Family 2015','Health 2015','Freedom 2015','Generosity 2015','Trust 2015','Rank 2016','Score 2016','Economy 2016','Family 2016','Health 2016','Freedom 2016','Generosity 2016','Trust 2016','Rank 2017','Score 2017','Economy 2017','Family 2017','Health 2017','Freedom 2017','Generosity 2017','Trust 2017','Rank 2018','Score 2018','Economy 2018','Family 2018','Health 2018','Freedom 2018','Generosity 2018','Trust 2018','Rank 2019','Score 2019','Economy 2019','Family 2019','Health 2019','Freedom 2019','Generosity 2019','Trust 2019','Score Mean','Economy Mean','Family Mean','Health Mean','Freedom Mean','Generosity Mean','Trust Mean']\n\nI want to access a particular row and want to convert it to to the following dataframe\n    Year    Rank    Score   Family  Health  Freedom Generosity  Trust\n0   2015     NaN      NaN     NaN     NaN     NaN         NaN   NaN\n1   2016     NaN      NaN     NaN     NaN     NaN         NaN   NaN\n2   2017     NaN      NaN     NaN     NaN     NaN         NaN   NaN\n3   2018     NaN      NaN     NaN     NaN     NaN         NaN   NaN\n4   2019     NaN      NaN     NaN     NaN     NaN         NaN   NaN \n\nAny help is welcomed &amp; Thank you in advance.\n"
"I have a column in my dataset that looks like this:\ndate  \n41245.0  \n41701.0  \n36361.0\n\nI need to convert it into a date-format. When I try it in Python using this:\ndf = pd.to_datetime(df['date'])\n\nMy results are like this:\n1   1970-01-01 00:00:00.000041701\n4   1970-01-01 00:00:00.000042226\n5   1970-01-01 00:00:00.000039031\n\nThese years seem quite odd. However, when I open the my dataset(as an excel sheet) on Google Drive/Sheets, select the column, and format it using the &quot;date&quot; or &quot;date-time&quot; format, the results are quite different.\n12/2/2012\n3/3/2014\n7/20/1999\n\nMy results should be something like this. However, currently I am getting weird values. Similarly, the results on Microsoft Excel were also slightly different. Why are the dates different? What am I doing wrong?\n"
"This is the first time I'm using Pytorch and Pytorch geometric. I'm trying to create a simple Graph Neural Network with Pytorch Geometric. I'm creating a custom dataset by following the Pytorch Geometric documentations and extending the InMemoryDataset. After that I split the dataset into training, validation and test dataset which sizes (3496, 437, 439) respectively. These are the number of graphs in each dataset. Here is my simple Neural Network\nclass Net(torch.nn.Module):\ndef __init__(self):\n    super(Net, self).__init__()\n    self.conv1 = GCNConv(dataset.num_node_features, 10)\n    self.conv2 = GCNConv(10, dataset.num_classes)\n\ndef forward(self, data):\n    x, edge_index, batch = data.x, data.edge_index, data.batch\n    x = self.conv1(x, edge_index)\n    x = F.relu(x)\n    x = F.dropout(x, training=self.training)\n    x = self.conv2(x, edge_index)\n\n    return F.log_softmax(x, dim=1)\n\nI get this error while training my model, which suggest that there's some issue with my input dimensions. Maybe the reason is behind my batch sizes?\nRuntimeError: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nFile &quot;E:\\Users\\abc\\Anaconda3\\lib\\site-packages\\torch_scatter\\scatter.py&quot;, line 22, in scatter_add\n        size[dim] = int(index.max()) + 1\n    out = torch.zeros(size, dtype=src.dtype, device=src.device)\n    return out.scatter_add_(dim, index, src)\n           ~~~~~~~~~~~~~~~~ &lt;--- HERE\nelse:\n    return out.scatter_add_(dim, index, src)\nRuntimeError: index 13654 is out of bounds for dimension 0 with size 678\n\nThe error happens specifically on this line of code in the Neural Network,\nx = self.conv1(x, edge_index)\n\nEDIT: Added more information about edge_index and explained in more detail about the data that I'm using.\nHere are the shapes of the variables that I'm trying to pass\nx: torch.Size([678, 43])\nedge_index: torch.Size([2, 668])\ntorch.max(edge_index): tensor(541690)\ntorch.min(edge_index): tensor(1920)\n\nI'm using a datalist which contains Data(x=node_features, edge_index=edge_index, y=labels) objects. When I'm splitting the dataset into training, validation and test datasets, I get (3496, 437, 439) graphs in each dataset respectively. Originally I tried to create one single graph from my dataset, but I'm not sure how it would work with Dataloader and minibatches.\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nHere's the code that generates the graph from dataframe. I've tried to create an simple graph where there are just some amount of vertices with some amount of edges connecting them. I've probably overlooked something and that's why I have this issue. I've tried to follow the Pytorch geometric documentation when creating this graph (Pytorch Geometric: Creating your own dataset)\ndef process(self):\n        data_list = []\n\n        grouped = df.groupby('EntityId')\n        for id, group in grouped:\n            node_features = torch.tensor(group.drop(['Labels'], axis=1).values)\n            source_nodes = group.index[1:].values\n            target_nodes = group.index[:-1].values\n            labels = torch.tensor(group.Labels.values)\n            edge_index = torch.tensor([source_nodes, target_nodes])\n\n            data = Data(x=node_features, edge_index=edge_index, y=labels)\n            data_list.append(data)\n\n        if self.pre_filter is not None:\n            data_list = [data for data in data_list if self.pre_filter(data)]\n\n        if self.pre_transform is not None:\n            data_list = [self.pre_transform(data) for data in data_list]\n\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])\n\nIf someone could help me with the process of creating a graph on any kind of data and using it with GCNConv, I would appreciate it.\n"
"I am attempting to perform arithmetic on the 'data_d' column.\ndataframe\n\ndata_a        data_b      data_c    data_d\n60            0.30786     Discharge 2.31714    \n61            0.30792          Rest 2.34857   \n121           0.62095          Rest 2.38647    \n182           0.93398     Discharge 2.31115    \n183           0.93408          Rest 2.34550    \n243           1.24711          Rest 2.37162    \n304           1.56014     Discharge 2.30855    \n305           1.56019          Rest 2.34215    \n365           1.87322          Rest 2.36276     \n426           2.18630     Discharge 2.30591     \n\nI want to assign the variables A,B,C into a new column named 'variable'. As shown below.\ndataframe2\n\ndata_a        data_b      data_c    data_d     variable\n60            0.30786     Discharge 2.31714    A\n61            0.30792          Rest 2.34857    B\n121           0.62095          Rest 2.38647    C\n182           0.93398     Discharge 2.31115    A\n183           0.93408          Rest 2.34550    B\n243           1.24711          Rest 2.37162    C\n304           1.56014     Discharge 2.30855    A\n305           1.56019          Rest 2.34215    B\n365           1.87322          Rest 2.36276    C\n426           2.18630     Discharge 2.30591    A\n\nThe script then should perform the following operation iteratively over the entire 'data_d' column.\n(C - (B-A))\n(2.38647 - (2.34857-2.31714))\n(2.35504)\n...\n\ndataframe3\n     measurement\n0    2.35504\n1    2.33727\n2    2.32916\n...  ...\n\nAnd so on.\nThank you in advance for any insight.\n"
"As the title says, I have a dataset that has empty values. The problem is that some of them are represented by a '-' and some by NaN.\nIs it better to change all of them to NaN or to '-'?\n"
"I have a df below as:\nName | Factory | Restaurant | Store | Building\nBrian    True    False        True     False\nMike     True    True         True     True\nBrian    True    False        False    True\nSam      False   False        False    False\nSam      True    False        True     True\nMike     True    False        False    False\n\nI have code below that gives me the number of times a column like Factory is True for each name, how can I add the rest or more columns to have all the values that are true for each column like Restaurant and Store and Building and more columns too? Thanks!\ndf.groupby(['Name'])['Factory'].apply(sum).reset_index()\n\nCurrent output:\nName | Factory\nBrian    2\nMike     2\nSam      1\n\nExpected output:\nName | Factory | Restaurant | Store | Building\nBrian    2          0           1        1\nMike     2          1           1        1\nSam      1          0           1        1\n\nThanks!\n"
"I have a dataset of S&amp;P500 historical prices with the date, the price and other data that i don't need now to solve my problem.\n        Date      Price\n0     1981.01     6.19\n1     1981.02     6.17\n2     1981.03     6.24\n3     1981.04     6.25\n.       .           .\nand so on till 2020\n\nThe date is a float with the year, a dot and the month.\nI tried to plot all historical prices with matplotlib.pyplot as plt.\nplt.plot(df[&quot;Price&quot;].tail(100))\nplt.title(&quot;S&amp;P500 Composite Historical Data&quot;)\nplt.xlabel(&quot;Date&quot;)\nplt.ylabel(&quot;Price&quot;)\n\n\nThis is the result. I used df[&quot;Price&quot;].tail(100) so you can see better the difference between the first and the second graph(You are going to see in a sec).\nBut then I tried to set the index from the one before(0, 1, 2 etc..) to the df[&quot;Date&quot;] column in the DataFrame in order to see the date in the x axis.\ndf = df.set_index(&quot;Date&quot;)\nplt.plot(df[&quot;Price&quot;].tail(100))\nplt.title(&quot;S&amp;P500 Composite Historical Data&quot;)\nplt.xlabel(&quot;Date&quot;)\nplt.ylabel(&quot;Price&quot;)\n\n\nThis is the result, and it's quite disappointing.\nI have the Date where it should be in the x axis but the problem is that the graph is different from the one before which is the right one.\nIf you need the dataset to try out the problem here you can find it.\nIt is called U.S. Stock Markets 1871-Present and CAPE Ratio.\nHope you've understood everything.\nThanks in advance\nUPDATE\nI found something that could cause the problem. If you look in depth at the date you can see that in month #10 each is written as a float(in the original dataset) like this: example Year:1884 1884.1. The problem occur when you use pd.to_datetime() to transform the Date float series to a Datetime. So the problem could be that the date in the month #10, when converted into a Datetime, become: (example from before) 1884-01-01 which is the first month in the year and it has an effect on the final plot.\nSOLUTION\nFinally, I solved my problem!\nYes, the error was the one I explain in the UPDATE paragraph, so I decided to add a 0 as a String where the lenght of the Date (as a string) is 6 in order to change, for example: 1884.1 ==&gt; 1884.10\ndf[&quot;len&quot;] = df[&quot;Date&quot;].apply(len)\ndf[&quot;Date&quot;] = df[&quot;Date&quot;].where(df[&quot;len&quot;] == 7, df[&quot;Date&quot;] + &quot;0&quot;)\n\nThen i drop the len column i've just created.\ndf.drop(columns=&quot;len&quot;, inplace=True)\n\nAt the end I changed the &quot;Date&quot; to a Datetime with pd.to_datetime\ndf[&quot;Date&quot;] = pd.to_datetime(df[&quot;Date&quot;], format='%Y.%m')\ndf = df.set_index(&quot;Date&quot;)\n\nAnd then I plot\ndf[&quot;Price&quot;].tail(100).plot()\nplt.title(&quot;S&amp;P500 Composite Historical Data&quot;)\nplt.xlabel(&quot;Date&quot;)\nplt.ylabel(&quot;Price&quot;)\nplt.show()\n\n"
"Input:\n**A(Object)   B(Float)C(Object)   D(Float)**\n\nA1            NaN     String1     NaN\nB1            2.0     NaN         3.0\nC1            NaN     String2     NaN\nD1            1.0     String3     2.0\nE1            NaN     String4     3.0\nF1            2.0     String5     NaN\n\nExpected Output:\n**A(Object)   B(Float)C(Object)   D(Float)**\n\nB1            2.0     NaN         3.0\nD1            1.0     String3     2.0\nE1            NaN     String4     3.0\nF1            2.0     String5     NaN\n\nI'm trying to use the code:\ndf[df.select_dtypes(include='number').notna().all(axis=1)]\n\nbut it also removes those rows where only one numeric column is NaN, such as E1.\nHow to resolt it?\n"
"I want only the 'self.news' variable to be reset or to get a new value in the class I created below.but when I run the three function. All values \u200b\u200breturn to their original state.\nand the functions I used before are invalid.How do I change or reset only variable 'self.nexts'. very very veyr thank you.\nclass new:\n    def __init__(self, name=None, nexts=None):\n        self.nexts = nexts\n        self.name = name\n\n    def one(self):\n        self.nexts = 2\n        return self.nexts\n\n    def two(self):\n        self.nexts = 1\n        self.name = 'What'\n        return self.nexts\n\n    def three(self):\n        self.__init__()\n        print(self.name)\n        print(self.nexts)\n\n\na = new()\n\nprint(a.one())\n#output = 2\nprint(a.two())\n#output = 1, hate\na.three()\n#output = None, None\n\n"
"A Dataframe contains stock data (the data in the dataframe is irrelevant and omitted):\n      open   high   low   close\nMSFT\nAPPL\nIBM\nGM\nXP\nINTC\n\nThe problem: select combinations of 3 stocks such that\n\norder is not important: if MSFT/APPL/IBM has been calculated then IBM/MSFT/APPL is redundant\na combination cannot have duplicates: cannot have MSFT/MSFT/APPL\n\nFor example, if only selecting two stocks at a time then select the combinations with an &quot;X&quot; below. This can be extended to 3 dimensions.\n        MSFT   APPL   IBM   GM   XP   INTC\nMSFT\nAPPL    X\nIBM     X      X     \nGM      X      X     X\nXP      X      X     X      X \nINTC    X      X     X      X    X\n\nI can do this in a non-pythonic way (3 nested loops) but I'm looking for a better solution.\nA solution that doesn't hold the entire cube in memory is necessary. The dataset is large and only cumulative results are kept, not the combinations or their results.\n"
"I am trying to convert categorical value to integer using OneHotEncoder and ColumnTransformer. My understanding is it should create dummies for category columns like pd.get_dummies. My file is having ~1500 records and 10 columns.\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\ncat_features=['COMPANY_NAME', 'BRAND_NAME']\nenc=OneHotEncoder()\n\ntransformer = ColumnTransformer([(&quot;enc&quot;, \n                                  enc,\n                                  cat_features)],\n                                  remainder=&quot;passthrough&quot;)\ndf_transformed = transformer.fit_transform(df_model)\ndf_transformed)\n\nThe result is:\n&lt;1574x37 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 15513 stored elements in Compressed Sparse Row format&gt;\n\nWhen I try to look at the data after converting it into dataframe using:\n\nWhat is wrong I am doing. My data looks something like below:\n\n"
'I have a dataframe:\ndf = key1 key2 .. keyn type  val1 val2 .. valn\n      k1   k2      kn   p1    1     2      7\n      k1   k2      kn   p2    6     1      5\n      k1   k2      kn   p3    8     4      1\n      k3   k2      kn   p1    4     6      9\n      k3   k2      kn   p2    6     1      0\n      k3   k2      kn   p3    1     2      8\n\nSo , foreach set of keys key..keyn I have 3 values in the column type.\nI want to unmelt it to columns to have a column per &lt;type, val&gt; pairing so I will end up with:\ndf = key1 key2 .. keyn val1_typep1 val1_typep2 val1_typep3 ..  valn_typep1 valn_typep2 valn_typep3  \n      k1   k2      kn       1          6            8               7           5           1 \n      k3   k2      kn       4          6            1               9           0           8 \n\nWhat is the bst way to do so?\n'
'I have a dataframe:\ndf = [A   B   C   D  E_p0   E_p1   E_p2    K_p0  K_p1  K_2  \n      a   2   r   4   3       6     1       9     5     1\n      e   g   1   d   5       8     2       7     1     4]\n\nAnd I want to group columns based on the prefix and aggregate them by a function, such as mean or max or rms.\nSo, for example if my function is max, the output is:\ndf = [A   B   C   D   E    K  \n      a   2   r   4   6    9  \n      e   g   1   d   8    7   ]\n\n'
'I am having a problem on an implementation of LSTM. I am not sure if I have the right implementation or this is just an overfitting problem. I am doing essay grading using a LSTM, scoring text with score from 0 - 10 (or other range of score). I am using the ASAP kaggle competition data as one of the training data.\n\nHowever, the main goal is to achieve good performance on a private dataset, with around 500 samples. The 500 samples includes validation and training set. I have previously done some experiment and got the model to work, but after fiddling with something, the model doesn\'t fit anymore. The model does not improve at all. I have also re-implemented the code in a cleaner manner with much more obejct oriented code and still can\'t reproduce my previous result.\n\nHowever, I am getting the model to fit to my data, just there is tremendous overfitting. I am not sure if this is an implementation problem of some sort or just overfitting, but I cannot get the model to work. The maximum I can get it to is 0.35 kappa using LSTM on the ASAP data essay set 1. For some bizarre reason, I can get a single layer fully connected model to have 0.75 kappa. I think this is an implementation problem but I am not sure.\n\nHere is my old code:\n\ntrain.py\n\nimport gensim\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.metrics import cohen_kappa_score\nfrom torch import nn\nimport torch.utils.data as data_utils\nfrom torch.optim import Adam\n\nfrom dataset import AESDataset\nfrom network import Network\n\nfrom optimizer import Ranger\nfrom qwk import quadratic_weighted_kappa, kappa\n\nbatch_size = 32\n\ndevice = "cuda:0"\ntorch.manual_seed(1000)\n# Load data from csv\nfile_name = "data/data_new.csv"\ndata = pd.read_csv(file_name)\narr = data.to_numpy()\ntext = arr[:, :2]\ntext = [str(line[0]) + str(line[1]) for line in text]\ntext = [gensim.utils.simple_preprocess(line) for line in text]\n\nscore = arr[:,2]\n\nscore = [sco*6 for sco in score]\nscore = np.asarray(score, dtype=int)\n\n\ntrain_dataset = AESDataset(text_arr=text[:400], scores=score[:400])\ntest_dataset = AESDataset(text_arr=text[400:], scores=score[400:])\n\nscore = torch.tensor(score).view(-1,1).long().to(device)\n\n\ntrain_loader = data_utils.DataLoader(train_dataset,shuffle=True, batch_size=batch_size, drop_last=True)\ntest_loader = data_utils.DataLoader(test_dataset,shuffle=True,batch_size=batch_size, drop_last=True)\n\nout_class = 61\n\nepochs = 1000\n\nmodel = Network(out_class).to(device)\nmodel.load_state_dict(torch.load("model/best_model"))\ny_onehot = torch.FloatTensor(batch_size, out_class).to(device)\noptimizer = Adam(model.parameters())\ncriti = torch.nn.CrossEntropyLoss()\n# model, optimizer = amp.initialize(model, optimizer, opt_level="O2")\n\n\nstep = 0\n\nfor i in range(epochs):\n    #Testing\n    if i % 1 == 0:\n        total_loss = 0\n        total_kappa = 0\n        total_batches = 0\n        model.eval()\n        for (text, score) in test_loader:\n\n            out = model(text)\n            out_score = torch.argmax(out, 1)\n            y_onehot.zero_()\n            y_onehot.scatter_(1, score, 1)\n            kappa_l = cohen_kappa_score(score.view(batch_size).tolist(), out_score.view(batch_size).tolist())\n            score = score.view(-1)\n            loss = criti(out, score.view(-1))\n            total_loss += loss\n            total_kappa += kappa_l\n            total_batches += 1\n        print(f"Epoch {i} Testing kappa {total_kappa/total_batches} loss {total_loss/total_batches}")\n        with open(f"model/epoch_{i}", "wb") as f:\n            torch.save(model.state_dict(),f)\n        model.train()\n    #Training\n\n    for (text, score) in train_loader:\n\n        optimizer.zero_grad()\n        step += 1\n        out = model(text)\n        out_score = torch.argmax(out,1)\n        y_onehot.zero_()\n        y_onehot.scatter_(1, score, 1)\n        kappa_l = cohen_kappa_score(score.view(batch_size).tolist(),out_score.view(batch_size).tolist())\n        loss = criti(out, score.view(-1))\n        print(f"Epoch {i} step {step} kappa {kappa_l} loss {loss}")\n        loss.backward()\n        optimizer.step()\n\n\ndataset.py\n\nimport gensim\nimport torch\nimport numpy as np\n\nclass AESDataset(torch.utils.data.Dataset):\n    def __init__(self, text_arr, scores):\n        self.data = text_arr\n        self.scores = scores\n        self.w2v_model = ("w2vec_model_all")\n        self.max_len = 500\n    def __getitem__(self, item):\n        vector = []\n        essay = self.data[item]\n\n        pad_vec = [1 for i in range(300)]\n        for i in range(self.max_len - len(essay)):\n            vector.append(pad_vec)\n        for word in essay:\n            word_vec = pad_vec\n            try:\n                word_vec = self.w2v_model[word]\n            except:\n                #print(f"Skipping word as word {word} not in dictionary")\n                word_vec = pad_vec\n\n\n            vector.append(word_vec)\n        #print(len(vector))\n        vector = np.stack(vector)\n        tensor = torch.tensor(vector[:self.max_len]).float().to("cuda")\n        score = self.scores[item]\n        score = torch.tensor(score).long().to("cuda").view(1)\n\n        return tensor, score\n\n    def __len__(self):\n        return len(self.scores)\n\n\n\nnetwork.py\n\nimport torch.nn as nn\nimport torch\n\nimport torch.nn.functional as F\n\nclass Network(nn.Module):\n    def __init__(self, output_size):\n        super(Network, self).__init__()\n        self.lstm = nn.LSTM(300,500,1, batch_first=True)\n        self.dropout = nn.Dropout(p=0.5)\n        #self.l2 = nn.L2\n        self.linear = nn.Linear(500,output_size)\n\n\n\n\n\n    def forward(self,x):\n        x, _ = self.lstm(x)\n        x = x[:,-1,:]\n        x = self.dropout(x)\n        x = self.linear(x)\n\n\n        return x\n\n\nMy new code: https://github.com/Clement-Hui/EssayGrading\n'
'I\'m a beginner in Python Data Science. I\'m working on clickstream data and trying to count the consecutive clicks on an item in a given session. I\'m getting the cumulative sum in \'Block\' column. After that I\'m aggregating on Block to get the count on each block. In the end I want to groupby Session and Item and aggregate the block count since there may be cases(Sid=6 here) where an item comes consecutively m times at first and again after other items, it comes consecutively n times. So the consecutive count should be \'m+n\'.\n\nHere is the dataset-\n\n\n    Sid                    Tstamp     Itemid\n0     1  2014-04-07T10:51:09.277Z  214536502\n1     1  2014-04-07T10:54:09.868Z  214536500\n2     1  2014-04-07T10:54:46.998Z  214536506\n3     1  2014-04-07T10:57:00.306Z  214577561\n4     2  2014-04-07T13:56:37.614Z  214662742\n5     2  2014-04-07T13:57:19.373Z  214662742\n6     2  2014-04-07T13:58:37.446Z  214825110\n7     2  2014-04-07T13:59:50.710Z  214757390\n8     2  2014-04-07T14:00:38.247Z  214757407\n9     2  2014-04-07T14:02:36.889Z  214551617\n10    3  2014-04-02T13:17:46.940Z  214716935\n11    3  2014-04-02T13:26:02.515Z  214774687\n12    3  2014-04-02T13:30:12.318Z  214832672\n13    4  2014-04-07T12:09:10.948Z  214836765\n14    4  2014-04-07T12:26:25.416Z  214706482\n15    6  2014-04-03T10:44:35.672Z  214821275\n16    6  2014-04-03T10:45:01.674Z  214821275\n17    6  2014-04-03T10:45:29.873Z  214821371\n18    6  2014-04-03T10:46:12.162Z  214821371\n19    6  2014-04-03T10:46:57.355Z  214821371\n20    6  2014-04-03T10:53:22.572Z  214717089\n21    6  2014-04-03T10:53:49.875Z  214563337\n22    6  2014-04-03T10:55:19.267Z  214706462\n23    6  2014-04-03T10:55:47.327Z  214821371\n24    6  2014-04-03T10:56:30.520Z  214821371\n25    6  2014-04-03T10:57:19.331Z  214821371\n26    6  2014-04-03T10:57:39.433Z  214819762\n\n\nHere is my code-\n\nk[\'Block\'] =( k[\'Itemid\'] != k[\'Itemid\'].shift(1) ).astype(int).cumsum()\ny=k.groupby(\'Block\').count()\nz=k.groupby([\'Sid\',\'Itemid\']).agg({"y[Count]": lambda x: x.sum()})\n\n'
'I get this data frame: \n\n               Item ................. \n0              Banana (From Spain)... \n1              Chocolate ............ \n2              Apple (From USA) ..... \n               ............\n\n\nAnd I want change all Item\'s names by removing the parenthesis, getting finally \n\n               Item ................. \n0              Banana ............... \n1              Chocolate ............ \n2              Apple ................ \n               ............\n\n\nI thought, I should use replace but there are too much data so I\'m thinking in use something like \n\nimport re\n\n    for i in dataframe.index:\n       if bool(re.search(\'.*\\(.*\\).*\', dataframe.iloc[i]["Item"])):\n          dataframe.ix[i,"Item"] = dataframe.iloc[i]["Item"].split(" (")[0]\n\n\nBut I\'m not sure if is the most efficient way.\n'
'I am trying to find outliers for work schedules for individuals (mostly high variations). trying to find, if someone comes or leaves way outside the individual (8:30am to 5pm) or group normals (7am to 6pm). I tried using standard deviation but the problem with that is,\n\n\nIt gives me outliers on both sides of the mean. That is if some comes in late during work hours (say 10am)or leaves early (say 4pm).\nAnother problem is the mean itself. It takes lot of observations to bring down the mean to most frequent times if there are few extremes at the beginning of the data set. For example, one set had few in-times around 3pm, 11am, 10am, 9am but most of them were around 6am, but the mean took a lot of observations to get 6am mean. I thought of weighted averages but that would mean I will have to round up times to nearest 30 mins or so. But would like to avoid changing data points.\n\n\nIs there any known way to find outliers in work schedule? I tried to search but all I get is outliers in time-series. But I am looking for outliers in time itself. Any suggestions?\n\nNote: My data set has PersonID and multiple (swipe) times/day/PersonID. And I am using python 2.7.\n'
'The csv file was sent to me/ I can not re delimit the columns\n\n239845723,28374,2384234,AEVNE EFU 5 GN OR WNV,Owinv Vnwo Badvw 5 VIN,Ginq 2 jnwve wef evera wve 6 vwe as fgsb bfd bdfwd dsf (sdv seves 4-6), sebsbe sve(sevsev esvse 7-10) fsesef fesevsesv PaVvin (1 evesve vEV VEWee, 2 for WVEee VEWE. paper tuff as sWEFEWoon as VEWeew.).,2011-07-13 00:00:00,2011-07-13 00:00:00\n\n\nI replaced the string letters to cover sensitive info, however the problem is apparent. \n\nThis is an example "problem row" in my csv. It should be sorted into 8 columns as follows:\n\ncol1: 239845723\ncol2: 28374\ncol3: 2384234\ncol4: AEVNE EFU 5 GN OR WNV\ncol5: Owinv Vnwo Badvw 5 VIN\ncol6: Ginq 2 jnwve wef evera wve 6 vwe as fgsb bfd bdfwd dsf (sdv seves 4-6), sebsbe sve(sevsev esvse 7-10) fsesef fesevsesv PaVvin (1 evesve vEV VEWee, 2 for WVEee VEWE. paper tuff as sWEFEWoon as VEWeew.).\ncol7: 2011-07-13 00:00:00\ncol8: 2011-07-13 00:00:00\n\n\nAs you can see, column 6 is where the problem occurs as there are commas in the string that cause pandas to delimit and create new columns incorrectly. How can I solve this problem? I was thinking regex would help, perhaps with the below setup. Any help is appreciated!\n\n    csvfile = open(filetrace) \n    reader = csv.reader(csvfile)\n    new_list=[]\n    for line in reader:\n        for i in line:\n            #not sure\n\n'
'I get an error whenever I try to perform actions on U objects. I always get a KeyError. I have tried many times to plot U (last statement.)\n\nimport pandas as pd\nusers =pd.read_csv(\'./users.csv\',sep=\',\',parse_dates=[30])\nusers.head()\nusers.columns.get_loc("WhenCreated")\nusers[\'Month\'] = users[\'WhenCreated\'].dt.month\nusers.head()\nUsersCreatedin2017=users[\'WhenCreated\'] &gt; \'2017-01-01\'\nusers2017=users[UsersCreatedin2017].copy()\nusers2017[[\'Month\',\'UserPrincipalName\']].groupby(\'Month\').count()\nu=users2017[[\'Month\',\'UserPrincipalName\']].groupby(\'Month\').count().copy()\nu.plot(kind=\'line\', x=\'Month\', y=\'UserPrincipalName\')\n\n\nwhere I go the following error\n\nKeyError                                  Traceback (most recent call last)\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   2392             try:\n-&gt; 2393                 return self._engine.get_loc(key)\n   2394             except KeyError:\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5239)()\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20405)()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20359)()\n\nKeyError: \'Month\'\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-7-841cfe522e84&gt; in &lt;module&gt;()\n     13 #u.sort_values(by=\'Month\', ascending=True)\n     14 #u.plot.line(\'Month\',\'UserPrincipalName\')\n---&gt; 15 u.plot(kind=\'line\', x=\'Month\', y=\'UserPrincipalName\')\n     16 \n     17 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py in __call__(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\n   2618                           fontsize=fontsize, colormap=colormap, table=table,\n   2619                           yerr=yerr, xerr=xerr, secondary_y=secondary_y,\n-&gt; 2620                           sort_columns=sort_columns, **kwds)\n   2621     __call__.__doc__ = plot_frame.__doc__\n   2622 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py in plot_frame(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\n   1855                  yerr=yerr, xerr=xerr,\n   1856                  secondary_y=secondary_y, sort_columns=sort_columns,\n-&gt; 1857                  **kwds)\n   1858 \n   1859 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py in _plot(data, x, y, subplots, ax, kind, **kwds)\n   1660                 if is_integer(x) and not data.columns.holds_integer():\n   1661                     x = data.columns[x]\n-&gt; 1662                 data = data.set_index(x)\n   1663 \n   1664             if y is not None:\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in set_index(self, keys, drop, append, inplace, verify_integrity)\n   2926                 names.append(None)\n   2927             else:\n-&gt; 2928                 level = frame[col]._values\n   2929                 names.append(col)\n   2930                 if drop:\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __getitem__(self, key)\n   2060             return self._getitem_multilevel(key)\n   2061         else:\n-&gt; 2062             return self._getitem_column(key)\n   2063 \n   2064     def _getitem_column(self, key):\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in _getitem_column(self, key)\n   2067         # get column\n   2068         if self.columns.is_unique:\n-&gt; 2069             return self._get_item_cache(key)\n   2070 \n   2071         # duplicate columns &amp; possible reduce dimensionality\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in _get_item_cache(self, item)\n   1532         res = cache.get(item)\n   1533         if res is None:\n-&gt; 1534             values = self._data.get(item)\n   1535             res = self._box_item_values(item, values)\n   1536             cache[item] = res\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py in get(self, item, fastpath)\n   3588 \n   3589             if not isnull(item):\n-&gt; 3590                 loc = self.items.get_loc(item)\n   3591             else:\n   3592                 indexer = np.arange(len(self.items))[isnull(self.items)]\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   2393                 return self._engine.get_loc(key)\n   2394             except KeyError:\n-&gt; 2395                 return self._engine.get_loc(self._maybe_cast_indexer(key))\n   2396 \n   2397         indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5239)()\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc (pandas\\_libs\\index.c:5085)()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20405)()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item (pandas\\_libs\\hashtable.c:20359)()\n\nKeyError: \'Month\'\n\n\nI try to u.plot() but i got the following output\n\n\nwhere data I am trying to plot is :\n    UserPrincipalName\nMonth\n1   61\n2   53\n3   44\n4   31\n5   34\n6   21\n7   28\n8   196\n9   42\n'
"I have DataFrame like this:\n\n     sale_id          dt        receipts_qty \n31     196.0  2017-02-19                95.0    \n32     203.0  2017-02-20               101.0   \n33     196.0  2017-02-21               105.0            \n34     196.0  2017-02-22               112.0           \n35     196.0  2017-02-23               118.0           \n36     196.0  2017-02-24               135.0            \n37     196.0  2017-02-25               135.0           \n38     196.0  2017-02-26               124.0           \n40     203.0  2017-02-27               290.0          \n39     196.0  2017-02-27                84.0          \n42     203.0  2017-02-28               330.0            \n41     196.0  2017-02-28               124.0           \n43     196.0  2017-03-01               100.0          \n44     203.0  2017-03-01               361.0         \n\n\nI have to drop duplicates by dt and keep the rows where sale_id == 196. I have found only drop_duplicates('dt', keep='last') and drop_duplicates('dt', keep='first') but it isn't what I need.\n\nDataFrame I want to get:\n\n     sale_id          dt        receipts_qty  \n31     196.0  2017-02-19                95.0   \n32     203.0  2017-02-20               101.0       \n33     196.0  2017-02-21               105.0            \n34     196.0  2017-02-22               112.0           \n35     196.0  2017-02-23               118.0           \n36     196.0  2017-02-24               135.0            \n37     196.0  2017-02-25               135.0           \n38     196.0  2017-02-26               124.0                 \n39     196.0  2017-02-27                84.0                     \n41     196.0  2017-02-28               124.0           \n43     196.0  2017-03-01               100.0          \n\n"
'I am trying to convert my txt file to pandas dataframe.\nFirst multiple lines are like this,\n\n[\'Tue Sep 12 15:13:56 +0000 2017\', \'text. \', 0, \'en\', 390, 529, 7138, 15727, False, -84.395235, 33.771232]\n[\'Tue Sep 12 15:13:59 +0000 2017\', "text", 0, \'en\', 648, 891, 2087, 5801, False, -84.321948, 33.752879]\n[\'Tue Sep 12 15:14:01 +0000 2017\', \'text\', 0, \'en\', 217, 222, 959, 958, False, -82.849182, 27.865251]\n[\'Tue Sep 12 15:14:06 +0000 2017\', \'text\', 0, \'en\', 71, 85, 2357, 1290, False, -82.29976, 27.857254]\n\n\nExplanation for each element in each list is,\n\ntime, text, retweet_count, language, friends_count, followers_count, favourites_count, status_count, verified\n\n\nI used pandas, but it does not work what I tend to.\n\ndf = pd.read_csv("second.txt", sep=\',\')\n\n\nThen I have almost 100,000 columns, 0 rows. How can I convert this file to dataframe successfully?\nThanks!\n'
"I have a pandas data frame with the structure like this:\n\ndf = pd.DataFrame({'entry': [['A','B','C'],['A','B','E','D'],['C'],['D','A', 'B'],['D','C','E'],['E','A']]})\n\n\nwhich gives: \n\n    entry\n0   [A, B, C]\n1   [A, B, E, D]\n2   [C]\n3   [D, A, B]\n4   [D, C, E]\n5   [E, A]\n\n\nI want to convert it to the square matrix,  size of unique symbols seen in the data frame (in this case it is 5: 'A','B','C','D','E')  where every intersection is the frequency of this pair been seen together (like here the pair [A,B] is seen together 3 times, pair [D,A] - only once. If there are 3 or more symbols together, I want all the combinations to be considered.) So the output is something like this:\n\n   A B C D E\nA    3 1 1 1\nB        1 1\nC        1 1\nD          2\nE\n\n\nI am quite a beginner in the field, tried to write a loop that goes through all the combinations. Have a problem with the fact that there may be any amount of items inside the entry. \n"
"plt.figure(figsize=(15,5))\nplt.plot(data['Unemployment Rate'])\nplt.axis([1948,2017,0,15])\nplt.show()\n\n\nThese code returned an empty graph. \n\nThe table contains 2 columns, data['Year'] and data['Unemployment Rate'].\n\nThe year is between 1948 and 2017.\n\nWhat I am trying to accomplish is to generate a graph for the unemployment rate and use the values in data['Year'] as the value of the x axis. \n"
"I have a pandas groupby object that I made from a larger dataframe, in which amounts are grouped under a person ID variable as well as whether it was an ingoing or outgoing transaction. Heres an example:\n\nID In_Out Amount\n1 In 5\n1 Out 8\n2 In 4\n2 Out 2\n3 In 3\n3 Out 9\n4 Out 8\n\n\n(sorry I don't know how to put actual sample data in). Note that some folks can have one or the other (e.g., maybe they have some going out but nothing coming in).\n\nAll I want to go is get the difference in the amounts, collapsed under the person. So the ideal output would be, perhaps a dictionary or other dataframe, containing the difference in amounts under each person, like this:\n\nID Difference\n1 -3\n2 2\n3 -6\n4 -8\n\n\nI have tried a handful of different ways to do this but am not sure how to work with these nested lists in python.\n\nThanks!\n"
'I\'m pretty new to python flask, Just wanted to check my code below, where I\'m doing things wrong. \n\nAs when I\'m running when on URL like (localhost:5000/submit?name=dial&amp;id=565337) it\'s running properly, though it\'s not running when I\'m passing values on the form and producing an error.\n\nfrom flask import Flask, request, redirect, url_for\nimport Eoc_Summary\nimport Eoc_Daily\nimport Eoc_AdSize\nimport Eoc_Video\nimport Eoc_Intraction\nimport EOC_definition\nfrom config import Config\n\napp = Flask(__name__)\n\nform = \'\'\'\n&lt;html&gt;\n   &lt;body&gt;\n      &lt;form action = "http://localhost:5000" method="POST"&gt;\n         &lt;p&gt;Enter Name:&lt;/p&gt;\n         &lt;p&gt;&lt;input type = "text" name = "name" /&gt;&lt;/p&gt;\n         &lt;p&gt;Enter id:&lt;/p&gt;\n         &lt;p&gt;&lt;input type = "text" name = "id" /&gt;&lt;/p&gt;\n         &lt;p&gt;&lt;input type = "submit" value = "submit" /&gt;&lt;/p&gt;\n      &lt;/form&gt;\n   &lt;/body&gt;\n&lt;/html&gt;\n\'\'\'\n\n@app.route("/")\ndef index():\n    if request.method == \'GET\':\n        return form\n    elif request.method == \'POST\':\n        name = request.form[\'name\']\n        id = request.form[\'id\']\n        return submit(name, id)\n\n@app.route(\'/submit\')\ndef submit():\n    name = request.args.get(\'name\')\n    id = request.args.get(\'id\')\n    c = Config(name, int(id))\n\n    obj_summary=Eoc_Summary.Summary(c)\n    obj_summary.main()\n    obj_daily=Eoc_Daily.Daily(c)\n    obj_daily.main()\n    obj_adSize=Eoc_AdSize.ad_Size(c)\n    obj_adSize.main()\n    obj_Video=Eoc_Video.Video(c)\n    obj_Video.main()\n    obj_Intraction=Eoc_Intraction.Intraction(c)\n    obj_Intraction.main()\n    obj_definition=EOC_definition.definition(c)\n    obj_definition.main()\n    c.saveAndCloseWriter()\n    return \'Report Generated\'\n\n\nif __name__ == \'__main__\':\n    app.run()\n\n'
"This is a input and output from a jupyter notebook. I need help with identifying the reason why I am unable to accurately select and set the data in the 'went_out' column.\n\n\n\nBoth the red underline cells are supposed to be displaying data from the datetime column of its own row but only one is accurately displaying it. It turns out that many of the rows that matched my condition did not get selected and set.\n\n\n\nThis is a sample of the code I used:\n\n\r\n\r\n# your answer here\r\ndf.loc[(df['reading_type'] == 'motion') &amp; (df['value'] == 255), 'event'] = 'motion on'\r\ndf.loc[(df['reading_type'] == 'motion') &amp; (df['value'] == 0), 'event'] = 'motion off'\r\n\r\ndf2 = df.loc[(df['reading_type'] == 'door') | (df['event'] == 'motion on')].copy()\r\ndf2.loc[(df['event'] == 'door close') &amp; (df['event'].shift(-1) == 'door open'), 'went_out'] = df2['datetime']\r\ndf2\r\n\r\n\r\n\n\nHere are the links for the jupyter notebook file and csv file:\n\n\nJupyter Notebook:\nhttps://drive.google.com/file/d/15f6NQrM4UoAZlzRhK35TOKyhPJnmWWdU/view?usp=sharing\nCSV file:\nhttps://drive.google.com/file/d/1hZudSVbT91ESj2qkzrJ--CbVdrzVCmce/view?usp=sharing\n\n"
"I am new to Pandas. I have the following data types in my dataset. (The dataset is Indian Startup Funding downloaded from Kaggle.)\n\nDate                datetime64[ns]\nStartupName                 object\nIndustryVertical            object\nCityLocation                object\nInvestorsName               object\nInvestmentType              object\nAmountInUSD                 object\ndtype: object\n\ndata['AmountInUSD'].groupby(data['CityLocation']).describe()\n\n\nI did the above operation and found that many cities are similar for example,\n\nBangalore   \nBangalore / Palo Alto\nBangalore / SFO\nBangalore / San Mateo\nBangalore / USA\nBangalore/ Bangkok\n\n\nI want to do following operation, but I do not know the code to this. \n\nIn column CityLocation, find all cells which starts with 'Bang' and replace them all with 'Bangalore'. Help will be appreciated.\n\nI did this \n\ndata[data.CityLocation.str.startswith('Bang')] \n\n\nand I do not know what to do after this.\n"
'I would like to use Pycharm to write some data science code and I am using Visual Studio Code and run it from terminal. But I would like to know if I could do it on Pycharm? I could not find some modules such as cluster and pylab on Pycharm? Anyone knows how I could import these modules into Pycharm?\n'
'I have a dataframe with two columns: \'TotalCharges\', and \'Churn\' with 7043 rows. In 11 cells of column \'TotalCharges\' I have a missing value. What I want is to create 10 categories of TotalCharges plus one category called "MissingValues", but I can\'t find a way to do it. My DataFrame looks like this:\n\n        TotalCharges Churn\n0           29.85    No\n1          1889.5    No\n2          108.15   Yes\n3         1840.75    No\n4          151.65   Yes\n5           820.5   Yes\n6          1949.4    No\n7           301.9    No\n8         3046.05   Yes\n9         3487.95    No\n10         587.45    No\n11          326.8    No\n12         5681.1    No\n13         5036.3   Yes\n14        2686.05    No\n15        7895.15    No\n16        missing    No\n17        7382.25    No\n18         528.35   Yes\n.... ....\n.... ....\n\n\nand I want to get something like this:\n\n        TotalCharges Churn TotalChargesCategories\n0           29.85    No    (18.799, 84.61]\n1          1889.5    No    (947.38, 1400.55]\n2          108.15   Yes    (84.61, 267.37]\n3         1840.75    No    (1400.55, 2065.52]\n4          151.65   Yes    (84.61, 267.37]\n5           820.5   Yes    (552.82, 947.38]\n6          1949.4    No    (1400.55, 2065.52]\n7           301.9    No    (267.37, 552.82]\n8         3046.05   Yes    (2065.52, 3132.75]\n9         3487.95    No    (3132.75, 4471.44]\n10         587.45    No    (552.82, 947.38]\n11          326.8    No    (267.37, 552.82]\n12         5681.1    No    (4471.44, 5973.69]\n13         5036.3   Yes    (4471.44, 5973.69]\n14        2686.05    No    (2065.52, 3132.75]\n15        7895.15    No    (5973.69, 8684.8]\n16        missing    No     MissingValues\n17        7382.25    No    (5973.69, 8684.8]\n18         528.35   Yes    (267.37, 552.82]\n.... ....\n.... .... \n\n\nIf there wouldn\'t be missing values it would be easy with this code:\n\nwidth_bin = (pd.qcut(df.TotalCharges,10))\ndf = df.assign(TotalChargesCat=width_bin)\ndf\n\n\nbut since there is 11 missing values I have problems creating categories, and this code leads to error message: \n\nTypeError: unsupported operand type(s) for -: \'str\' and \'str\'\n\n'
"I have an annoying dataset I need to modify with python that is in ASCII, with a 6 line header and then a bunch of lines of data (2 million lines). The format is like this:\n\n2014     2     5  1200     0    29   349   277\n2  32463.0020  32463.0020      1.0000   -145.5000      0.0000      0.0000\n50.0000   -107.0000\n1000   975   950   925   900   875   850   825   800   775   750   725\n700   650   600   550   500   450   400   350   300   275   250   225\n200   175   150   125   100\n-6.63661    -6.63661    -6.76161    -6.76161    -6.83974    -6.55849    -6.55849    -6.12099\n-5.93349    -5.90224    -5.73036    -5.55849    -5.71474    -5.60536    -5.71474    -5.71474\n-5.76161    -5.76161    -5.83974    -5.83974    -5.83974    -5.73036    -5.60536    -5.51161\n-5.32411    -5.35536    -5.19911    -5.18349    -4.87099    -4.57411    -4.23036    -3.74599\n-3.76161    -3.76161    -3.91786    -3.91786    -4.30849    -4.43349    -5.10536    -6.37099\n\n-5.79286    -5.91786    -6.32/411    -6.82411    -6.82411    -6.71474    -6.58974    -6.58974\n-6.48036    -6.48036    -6.30849    -6.02724    -6.10536    -5.21474    -5.01161    -4.48036\n-4.60536    -4.51161    -4.44911    -4.69911    -4.77724    -4.99599    -5.43349    -5.43349\n-5.41786    -5.27724    -5.27724    -6.01161    -5.43349    -6.15224    -5.44911    -4.69911\n-3.71474    -2.40224    -3.48036    -4.12099    -4.69911    -5.16786    -6.08974    -4.74599\n\n\nThe first 6 lines are the header info, then there's a huge block of values for one variable, then a line break, and then the values for the next variable.\n\nI need to modify one of the variables in the data file, but not the other (the modification is as simple as adding an integer to each value), but I'm struggling trying to figure out how to read in this file with each block of data as one array or list of values in python. Most of the resources I've found online assumes each column is a separate variable. Is there a quick and easy way to do this in python? I figured I'd check here first before doing things the hard way myself.\n\nThanks! \n"
"I have an np.array of size 500 x 15. How can I generate a new np array with all possible pair-wise combination of 2 columns from this array? \n\narr = [ [col1],[col2],[col3],..., [col14]]\n\nI want output such that it generates combination as\n\n [[col1],[col2]]\n [[col1],[col3]]\n .\n .\n [[col13],[col14]]\n\n\nI can't find a way to select all column values in the output. For an array having 15 columns there should be N*(N-1) i.e. 15 * 14 = 210 pairs. \n"
'I have a situation :\n\npostStr = """{\n                     "zoneId":"0",\n                     "id":["a","b","c","d","f","g"],\n                     "currencycode":["USD"],\n\n                }"""\n\n\npostData = json.loads(postStr, object_pairs_hook=OrderedDict)\n\n\nAnd i have a dataframe:\n\ndf = {\n\'id\':[\'a\',\'b\',\'c\',\'d\',\'f\',\'g\',\'h\',\'i\',\'j\',\'k\'],\n\'B\':[\'c\',\'d\',\'e\',\'d\',\'d\',\'c\',\'s\',\'e\',\'s\',\'q\'],\n\'S\':[\'f\',\'g\',\'h\',\'j\',\'e\',\'j\',\'t\',\'r\',\'p\',\'p\']\n}\ndf1 = pd.DataFrame(df)\n\n\nnow i want a data frme such that if the id is in the dictionary then B corresponding column becomes XX\n\nOutput:\n\n    df = {\n\'id\':[\'a\',\'b\',\'c\',\'d\',\'f\',\'g\',\'h\',\'i\',\'j\',\'k\'],\n\'B\' :[\'XX\',\'XX\',\'XX\',\'XX\',\'XX\',\'c\',\'s\',\'e\',\'s\',\'q\'],\n\'S\' :[\'f\',\'g\',\'h\',\'j\',\'e\',\'j\',\'t\',\'r\',\'p\',\'p\']\n}\ndf1 = pd.DataFrame(df)\n\n\nPlease help\n'
"I have successfully built logistic regression model using train dataset below.\n\nX = train.drop('y', axis=1)\ny = train['y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.5)\n\nscaler = StandardScaler()  \nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nlogreg1 = LogisticRegression()\nlogreg1.fit(X_train, y_train)\n\nscore = logreg1.score(X_test, y_test)\ncvs = cross_val_score(logreg1, X_test, y_test, cv=5).mean()\n\n\nMy problem is I want to bring in the test dataset to predict the unknown y value. In the test data theres no y column. How can I predict the y value using the seperate test dataset?? \n"
'I have two dataframe df1 and df2, \n\ndf1\n\nA B\n2 6\n5 1\n7 3\n1 2\n9 7\n4 7\n3 4\n8 9\n\n\nand df2 contains\n\nA  B  A_bin  B_bin  C  D  E\n2  6  1      2      5  4  1\n5  1  2      1      2  2  4\n7  3  3      1      5  1  7\n1  2  1      1      8  4  9\n9  7  3      3      5  5  8\n4  7  2      3      1  8  5\n3  4  1      2      2  9  3\n8  9  3      3      4  6  2\n\n\nI am trying to select only those specific rows selected from df2 to a new data frame df_result_A for all the row that has A_bin = 1\nsimilarily, a separate data frame df_result_B having all those rows of df2 such that B_bin rows contain 1.\nI am finding it difficult to put my logic incorrect syntax or probably my logic is wrong,\n\nfor i in range(len(df1(df2[columns])+len(df)):\n    if(row value is 1)\nprint in df_result_A\nprint in df_result_B\n\n\nAs the challenge is to not use column name and indexing, as the code should run for other data set as well I am trying to first iterate over the first two column of df2 as len(df1) will let me know that after 2 columns A_bin and B_bin will come.\nthus, when I am on the first column of df2 then adding len(df1) will put me on A_bin and iterating over it for checking value to be 1 and storing it in a separate dataframe.\nSimilarly, when I am on 2nd column of df2 adding len(df2) will put me on B_bin and thus storing its result in df_result_B.\nexpected result in separate dataframe.\n\ndf_result_A\n\nA  B   C  D  E\n2  6   5  4  1\n1  2   8  4  9\n3  4   2  9  3\n\n\ndf_result_b\n\nA B C D E\n5 1 2 2 4\n7 3 5 1 7\n1 2 8 4 9\n\n'
'I have pandas dataframe like this. I want group by App_Name in seperate variable\n\nApp_Name    Date        Response    Gross Revenue\ncom.apple.tiles2    2018-10-13  3748.723574 24133394\ncom.orange.thescore 2018-10-13  2034.611964 8273607\ncom.number.studio   2018-10-13  1807.756545 33736740\ncom.orange.thescore 2018-10-14  4671.930435 38575556\ncom.number.studio   2018-10-14  3533.461547 38726087\ncom.banana.com      2018-10-14  2920.33747  86230313\ncom.apple.tiles2    2018-10-15  3986.434851 35928884\ncom.number.studio   2018-10-15  2044.759823 76526368\ncom.apple.tiles2    2018-10-16  2610.214035 30611434\ncom.alpha.studio    2018-10-16  1731.429858 11643154\ncom.banana.com      2018-10-16  1601.387403 13781285\ncom.alpha.studio    2018-10-17  2769.373388 13198984\ncom.banana.com      2018-10-17  2205.359489 21974901\ncom.orange.thescore 2018-10-17  1820.852862 7565015\ncom.alpha.studio    2018-10-18  2784.822039 24217875\ncom.banana.com      2018-10-18  2545.899329 28361412\ncom.orange.thescore 2018-10-18  2052.207745 7544861\n\n\nI want to group data by App_Name and stored in sepearte list or dataframe for each App_Name, something like this given below:\n\nApp_Name    Date        Response    Gross Revenue\ncom.alpha.studio    2018-10-16  1731.429858 11643154\ncom.alpha.studio    2018-10-17  2769.373388 13198984\ncom.alpha.studio    2018-10-18  2784.822039 24217875\n\nApp_Name    Date        Response    Gross Revenue\ncom.apple.tiles2    2018-10-13  3748.723574 24133394\ncom.apple.tiles2    2018-10-15  3986.434851 35928884\ncom.apple.tiles2    2018-10-16  2610.214035 30611434\n\nApp_Name    Date        Response    Gross Revenue\ncom.banana.com      2018-10-14  2920.33747  86230313\ncom.banana.com      2018-10-16  1601.387403 13781285\ncom.banana.com      2018-10-17  2205.359489 21974901\ncom.banana.com      2018-10-18  2545.899329 28361412\n\nApp_Name    Date        Response    Gross Revenue\ncom.number.studio   2018-10-14  3533.461547 38726087\ncom.number.studio   2018-10-13  1807.756545 33736740\ncom.number.studio   2018-10-15  2044.759823 76526368\n\nApp_Name    Date        Response    Gross Revenue\ncom.orange.thescore 2018-10-13  2034.611964 8273607\ncom.orange.thescore 2018-10-14  4671.930435 38575556\ncom.orange.thescore 2018-10-17  1820.852862 7565015\ncom.orange.thescore 2018-10-18  2052.207745 7544861\n\n'
'function anagrams(s1, s2) is a Boolean valued function, which returns true just in case the string s1 contains the same letters as string s2 but in a different order. The function should be case insensitive --- in other words it should return the same value if any letters in either s1 or s2 are changed from upper to lower case or from lower to upper case. You may assume that the input strings contain only letters.\n\n\nThe function find_all_anagrams(string) takes a string as input and returns a list of all words in the file english_words.txt that are anagrams of the input string.\nthe function should return a list [word1, ..., wordN] such that each word in this list is a word in the dictionary file such that the value function anagrams(string, word) are True\n\ndef anagrams( string1, string2 ):\n    str_1 = string1.lower()\n    str_2 = string2.lower()\n    if str_1 == str_2:\n        return False\n    else:\n        list_1 = list( str_1 )\n        list_1.sort()\n        list_2 = list( str_2 )\n        list_2.sort()\n        return list_1 == list_2\n\ndef find_all_anagrams( string ):\n    with open("english_words.txt") as f:\n        word_list = []\n        for line in f.readlines():\n            word_list.append(line.strip())\n        list1 = [i.split() for i in word_list]\n    for j in list1:\n        if anagrams( string, j ) == True:\n            return list1\n        else:\n            return []\n\n\nERROR kept saying this: AttributeError: \'list\' object has no attribute \'lower\'\n\nfor example,word_list contains:\n[\'pyruvates\', \'python\', \'pythoness\', \'pythonesses\', \'pythonic\', \'pythons\', \'pyuria\', \'pyurias\', \'pyx\', \'pyxes\']\n\nExpected output below\n\nPart of the txt file shown on the right:\n\n\nUpdate:\nI think I just solved it,here are my codes:\n\ndef find_all_anagrams( string ):\n    list1 = []\n    with open("english_words.txt") as f:\n        word_list = []\n        for line in f.readlines():\n            word_list.append(line.strip())\n    for i in word_list:\n            if anagrams( string, i ):\n                list1.append(i)\n    return list1\n\n'
'Is there a way to use a \'for loop\' or something similar to run through the code to create multiple DataFrames with Pandas that I can assign to separate variables, instead of hardcoding both DataFrames? \n\nIf I add new tickers it wouldn\'t be efficient to keep hardcoding them.\n\nimport pandas_datareader as pdr\nfrom datetime import datetime\n\nEquity_Tickers = ["FB", "MSFT"]\n\nstart = datetime(2018, 9, 15)\nend = datetime.today().date()\n\n# First DataFrame \ndata = pdr.DataReader(Equity_Tickers[0], \'yahoo\', start, end)\ndf = data[[\'Adj Close\']]\n\n# Second DataFrame\ndata = pdr.DataReader(Equity_Tickers[1], \'yahoo\', start, end)\ndf1 = data[[\'Adj Close\']]\n\n'
"(df.set_index('STNAME').groupby(level=0)['CENSUS2010POP']\n   .agg({'avg': np.average, 'sum': np.sum}))\n\n\nIn the above code, why is it necessary to specify the level parameter in groupby, because as per my understanding the level parameter is only required when we have multiIndex in the DataFrame.   \n"
"I am trying to create a stacked barchart from my data and keep getting an error message\n\n\n  ValueError: shape mismatch: objects cannot be broadcast to a single\n  shape\n\n\nThis is what the relevant code I have written:\n\nnum = list(yearly_posts.index)\nbarWidth = 0.50\nplt.bar(num,yearly_status.values, color='#b5ffb9',edgecolor='white',width=barWidth)\nplt.bar(num,yearly_posts.values, color='#f9bc86',edgecolor='white',width=barWidth)\n\n\nand this is a sample of my data\n\n#yearly_status table\nyear\n2009     85\n2010     86\n2011    188\n2012    274\n2013    240\n2014    171\n2015    132\n2016     22\n2017     18\n2018     13\ndtype: int64\n\n#yearly_posts table\nyear\n2009     8\n2010    19\n2013    19\n2014    40\n2015    13\n2016    20\n2017    27\n2018    17\ndtype: int64\n\n"
"Trying to create a dataframe from multiple pandas series. \n\nHere is my code\n\nseries = [pd.Series([nine, ten, eleven, twelve,thirteen], index=('link','post','shared','status','timeline'))]\nmy_names = [2009,2010,2011,2012,2013,2014,2015,2016,2017,2018]\nmy_series = [pd.Series([nine, ten, eleven, twelve,thirteen], index=('link','post','shared','status','timeline'), name=n) for n in my_names]\n\n\nHere is a sample of the data series I am interacting with:\n\n#nine\ntitle\nlink       6.060606\npost       8.080808\nstatus    85.858586\ndtype: float64\n\n#ten\ntitle\nlink         1.666667\npost        15.833333\nshared       1.666667\nstatus      71.666667\ntimeline     9.166667\ndtype: float64\n\n#eleven\ntitle\nlink        23.885350\nshared       0.318471\nstatus      59.872611\ntimeline    15.923567\ndtype: float64\n\n#twelve\ntitle\nlink        18.660287\nshared       3.588517\nstatus      65.550239\ntimeline    12.200957\ndtype: float64\n\n\nThe desired outcome is having 2009-2018 as the column titles, link, shared, status, timeline post as the row indexes and populating each row with the values from each of the series (nine =2009, ten=2010).\n\nThis is a sample of my current outcome:\n\n\n"
'How to remove \'#\' from words in a string which are followed by \'#\' and not just \'#\' if it is present by itself, in the middle of the word or even at the end.\n\nCurrently I am using the regex expression:\n\ntest = "# #DataScience"\ntest = re.sub(r\'\\b#\\w\\w*\\b\', \'\', test) \n\n\nfor removing the "#\' from the words starting with \'#\' but it does not work at all. It returns the string as it is\n\nCan anyone please tell me why the "#" is not being recognized and removed?\nExamples - \n\ntest - "# #DataScience"\n\nExpected Output - "# DataScience"\n\nTest - "kjndjk#jnjkd"\n\nExpected Output - "kjndjk#jnjkd"\n\nTest - "# #DataScience #KJSBDKJ kjndjk#jnjkd #jkzcjkh# iusadhuish#""\n\nExpected Output -"# DataScience KJSBDKJ  kjndjk#jnjkd jkzcjkh# iusadhuish#"\n'
"everyone I'm a newbie in data science. I'm working on a regression problem using support vector regression. After tunning SVM parameters using grid search I got 2.6% MAPE  but my MAE and MSE are still very high.   \n\nI have used a user-defined function for mape.\n\nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import Normalizer\nimport matplotlib.pyplot as plt\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\nimport pandas as pd\nfrom sklearn import preprocessing\n\nfeatures=pd.read_csv('selectedData.csv')\nimport numpy as np\nfrom scipy import stats\nprint(features.shape)\nfeatures=features[(np.abs(stats.zscore(features)) &lt; 3).all(axis=1)]\ntarget = features['SYSLoad']\nfeatures= features.drop('SYSLoad', axis = 1)\nnames=list(features)\n\nfor i in names:\n    x=features[[i]].values.astype(float)\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    features[i]=x_scaled\n\n\nSelecting the target Variable which want to predict and for which we are\n\nfinding feature imps \n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = \ntrain_test_split(features, target, test_size = 0.25, random_state = 42)\ntrans=Normalizer().fit(train_input);\ntrain_input=Normalizer().fit_transform(train_input);\ntest_input=trans.fit_transform(test_input);\n\nn=test_target.values;\ntest_targ=pd.DataFrame(n);\n\nfrom sklearn.svm import SVR\nsvr_rbf = SVR(kernel='poly', C=10, epsilon=10,gamma=10)\ny_rbf = svr_rbf.fit(train_input, train_target);\npredicted=y_rbf.predict(test_input);\nplt.figure\nplt.xlim(20,100);\nprint('Total Days For training',len(train_input)); print('Total Days For \nTesting',len(test_input))\nplt.ylabel('Load(MW) Prediction 3 '); plt.xlabel('Days'); \nplt.plot(test_targ,'-b',label='Actual'); plt.plot(predicted,'-r',label='RBF \nkernel ');\nplt.gca().legend(('Actual','RBF'))\nplt.title('SVM')\nplt.show();\n\n\n\nMAPE=mean_absolute_percentage_error(test_target,predicted);\nprint(MAPE);\nmae=mean_absolute_error(test_targ,predicted)\nmse=mean_squared_error(test_targ, predicted)\nprint(mae);\nprint(mse);\n\n\nI'm getting MAPE = 2.56 , MAE =400 , MSE=437696. arent mae and mse are huge. and why they are? My target variable which is sysload contains values in range of 10 thousands \n"
'I have a program that prints information from a website, but I want to put this information in a csv or excel file. This is what I did:\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nimport xlsxwriter\n\n\nworkbook  = xlsxwriter.Workbook(\'nossarede.xlsx\')\nworksheet = workbook.add_worksheet()\n\nrequest = requests.get("http://www.drogariasnossarede.com.br/nossas-lojas")\nsoup = BeautifulSoup(request.content, \'html.parser\')\ndata = soup.find_all("div", class_=\'item\')\n\nfor container in data:\n  Pharmacyname = container.find_all("h3")\n  Pharmacyadd  = container.find_all("p")\n  for pharmacy in Pharmacyname:\n      print(pharmacy.text)\n      for add in Pharmacyadd:\n          print(add.text)\n      print(\'\')\n\n\nUntil here is perfect, it prints perfectly in the way I want. Then I tried this:              \n\n    import csv\n    with open(\'names.csv\', \'wb\') as ofile:\n        writer = csv.writer(ofile)\n        for container in data:\n            Pharmacyname = container.find_all("h3")\n            Pharmacyadd  = container.find_all("p")\n            for pharmacy in Pharmacyname:\n                for add in Pharmacyadd:\n                    writer.writerow((pharmacy.text[0], add.text[1]))\n\n\nBut its not working. \nIt shows this:\n\nTypeError: a bytes-like object is required, not \'str\'\n\n\nHow can I do it??\nThanks!\n'
"I'm trying to rename two columns after using a groupby of two columns.\n\nfun = {'Age':{'mean_age':'mean', 'median_age':'median'}}\ngroupbyClass2 = mc_response.groupby(['Country','GenderSelect']).agg(fun).reset_index()\ngroupbyClass2.columns = groupbyClass2.columns.droplevel(0) \n\n\nThe dataframe looks like the following:\n\n                        mean_age    median_age\n0   Argentina   Female  33.000000   33.0\n1   Argentina   Male    33.294118   32.0\n2   Australia   Female  35.000000   34.0\n3   Australia   Male    37.158416   36.0\n\n\nNow I want to rename the first column to 'Country' and the second column to 'Gender'. I tried the following code however the two columns will both be renamed to 'Gender'. How can I fix this?\n\ngroupbyClass2.rename(columns = {groupbyClass2.columns[0]:'Country', groupbyClass2.columns[1]:'Gender'},inplace = True)\n\n"
'I extracted data from a website, I ended up with a list including span tag and data I need, I tried some tweaks but cannot find appropriate method. I want to remove the span tag and retrieve only name and info of mobile.\n\n[&lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Honor 8X (64GB + 4GB RAM) 6.5" HD 4G LTE GSM Factory Unlocked Smartphone - International Version No Warranty JSN-L23 (Black)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Honor 8X (64GB + 4GB RAM) 6.5" HD 4G LTE GSM Factory Unlocked Smartphone - International Version No Warranty JSN-L23 (Black)&lt;/span&gt;]\n\n\nI want output as following:\n\n[ Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey,Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty),Huawei Honor 8X (64GB + 4GB RAM) 6.5" HD 4G LTE GSM Factory Unlocked Smartphone - International Version No Warranty JSN-L23 (Black)]\n\n\nAbove list only contains few elements from my main list. I will remove multiple entries later.\n'
"I am calculating the value counts of usa state in a data frame . I have obtained a data frame . I have assigned a column name to the value counts i.e 'Class_0' , but how to assign column name i.e state_usa to the 1st column of data frame i.e columns of the state. \n\ndf1 = data[data['project_is_approved']==0] \n['school_state'].value_counts().rename('Class_0')\ndf1.head()\n\nActual-Output-Obtained\nCA    2183\nTX    1382\nFL    1041\nNY    1027\nNC     738\nName: Class_0, dtype: int64\n\nOutput-wanted\nState    Class_0\nCA       2183\nTX       1382\nFL       1041\nNY       1027\nNC        738\n\n"
'For Example for this data frame\n\ndf = pd.DataFrame({\'Age\':[\'12\',np.nan,\'32\',\'21\',\'55\'],\n        \'Height\':["5\'7","5\'8","5\'5",np.nan,"5\'10"],\n                  \'Weight\':[np.nan,\'160\',\'165\',\'155\',\'170\'],\n                  \'Gender\':[\'M\',\'M\',np.nan,\'F\',np.nan],\n                  \'Salary\':[2900,6550000,7840000,6550000,8950000]})\n\n\nI want output as:\n\n        Age Height  Weight  Gender  Salary\n    0   12  5\'7     NaN     M       2.9K\n    1   NaN 5\'8     160     M       6.55M\n    2   32  5\'5     165     NaN     7.84M\n    3   21  NaN     155     F       6.55M\n    4   55  5\'10    170     NaN     8.95M \n\n'
'I got a table with some columns in it and one column got values going from 0 to 400. Let\'s call it column x.\n\nNow i want to group these information in the table based on the values.\nI want a new column "valuerange" that says in which range the value of column x is. \nFor example the value of the column x is at a specific row 120, therefor i want in the new column "100-150". \n\nMaybe i need to mention that the table is a large dataframe with 210k rows.\n\nI allready tried myself but i can\'t get to the expected result since i\'m new to python and just used to java.\n\nHere is some code i tried:\n\ndf1 = df[\'valuerange\'] = [\'0-50\' if p&lt;=50 \'51-100\' elif p&lt;=100 \'101-150\' elif p&lt;=150\n                            \'151-200\' elif p&lt;=200 \'201-250\' elif p&lt;=250 \'251-300\' elif p&lt;=300\n                            \'301-350\' elif p&lt;=350 \'351-400\' elif p&lt;=400 for p in df.x]\n\n'
'I have a list of paragraphs i want to  remove stopwords from all the paragraphs.  \n\nI first splitted the words then i checked the words with stopwords if not in stopwords append that word .it working for single list of paragraph but when try for whole bunch of paragraph it create list of all words.instead of group by that list \n\ng=[]\nh=[]\nfor i in f[0:2]:\n    word_token=npl.tokenize.word_tokenize(i)\n    for j in word_token:\n        if(j not in z):\n            g.append(j)\n        h.append(g)\n\n\nExample\n\nY="\'Take a low budget, inexperienced actors doubling as production staff\\x97 as well as limited facilities\\x97and you can\\\'t expect much more than "Time Chasers" gives you, but you can absolutely expect a lot less. This film represents a bunch of good natured friends and neighbors coming together to collaborate on an interesting project. If your cousin had been one of those involved, you would probably think to yourself, "ok, this movie is terrible... but a really good effort." For all the poorly delivered dialog and ham-fisted editing, "Time Chasers" has great scope and ambition... and one can imagine it was necessary to shoot every scene in only one or two takes. So, I\\\'m suggesting people cut "Time Chasers" some slack before they cut in the jugular. That said, I\\\'m not sure I can ever forgive the pseudo-old lady from the grocery store for the worst delivery every wrenched from the jaws of a problematic script.\'"\n\nz=set(npl.corpus.stopwords.words("english"))\nx=[]\nword_token=npl.tokenize.word_tokenize(y)\nfor i in word_token:\n    if(i not in z):\n        x.append(i)\n\nprint(np.array(x))       \n\n\noutput\n\n[\'Take\' \'low\' \'budget\' \',\' \'inexperienced\' \'actors\' \'doubling\'\n \'production\' \'staff\\x97\' \'well\' \'limited\' \'facilities\\x97and\' \'ca\' "n\'t"\n \'expect\' \'much\' \'``\' \'Time\' \'Chasers\' "\'\'" \'gives\' \',\' \'absolutely\'\n \'expect\' \'lot\' \'less\' \'.\' \'This\' \'film\' \'represents\' \'bunch\' \'good\'\n \'natured\' \'friends\' \'neighbors\' \'coming\' \'together\' \'collaborate\'\n \'interesting\' \'project\' \'.\' \'If\' \'cousin\' \'one\' \'involved\' \',\' \'would\'\n \'probably\' \'think\' \',\' \'``\' \'ok\' \',\' \'movie\' \'terrible\' \'...\' \'really\'\n \'good\' \'effort\' \'.\' "\'\'" \'For\' \'poorly\' \'delivered\' \'dialog\' \'ham-fisted\'\n \'editing\' \',\' \'``\' \'Time\' \'Chasers\' "\'\'" \'great\' \'scope\' \'ambition\' \'...\'\n \'one\' \'imagine\' \'necessary\' \'shoot\' \'every\' \'scene\' \'one\' \'two\' \'takes\'\n \'.\' \'So\' \',\' \'I\' "\'m" \'suggesting\' \'people\' \'cut\' \'``\' \'Time\' \'Chasers\'\n "\'\'" \'slack\' \'cut\' \'jugular\' \'.\' \'That\' \'said\' \',\' \'I\' "\'m" \'sure\' \'I\'\n \'ever\' \'forgive\' \'pseudo-old\' \'lady\' \'grocery\' \'store\' \'worst\' \'delivery\'\n \'every\' \'wrenched\' \'jaws\' \'problematic\' \'script\' \'.\']\n\n\nLike that.i want this same output for list of paragraph\n'
"I have a pandas df, where one of my columns have faulty values. I want to clean these values\n\nThe Faulty Values are negative and end with &lt;, example '-2.44&lt;'.\nHow do I fix this without affecting other columns? My index is Date-Time\n\nI have tried to convert the column to numeric data. \n\ndf.values = pd.to_numeric(df.values, errors='coerce')\n\n\nThere are no error messages. But, I'd like to replace them with removing '&lt;'.\n"
"I have a df with DateTimeIndex (hourly readings) from multiple sensors\n\nTime                   Temp1   Temp2   Temp3  Humidity1 Humidity2 \n1/2/2017 13:00          31       23      NA     66        48\n1/2/2017 14:00           22      NA      NA      63        43\n1/2/2017 15:00           25      25      21      41        39\n\n\nI would like to replace missing values of Temperature sensor 3 (Temp3) with available data from Temp1 and Temp2. If both Temp1 and Temp2 are not null, I want to take an average. If only 1 is available, I will take that value.\n\nExpected Output:\n\nTime                      Temp3   \n1/2/2017 13:00               27     \n1/2/2017 14:00               22      \n1/2/2017 15:00               21     \n\n\nI tried to use apply with lambda, but running into issues when one of the data is missing. \n\nDf['Temp3'] = Df.apply(\n    lambda row: (row['Temp1']+row['Temp2'])/2 if np.isnan(row['Temp3']) \n    else row['Temp3'],\n    axis=1\n)\n\n"
"I'm trying to do an Exercice on Kaggle: Exercice link\n\nI made some Data processing and then tried to plot a heatmap using the seaborn Library but for some reason the heatmap shows a white row and column. here is the Image of the Heatmap. It is not clearly visible here but you can see the white row and column for the Year Feature  \n\nhere is the Code I'm using. it's actually simple that's why I don't think that this is happening because of an Error in the Code: \n\ndataset = pd.read_csv('/content/Consumo_cerveja.csv')\n# rename the columns to english\n\ndataset.columns = ['Date', 'MediumTemperature', 'MinimumTemperature', 'MaximumTemperature', 'Precipitation', 'Weekend', 'BeerConsumption' ]\ndataset['parsed_date'] = pd.to_datetime(dataset['Date'], format= '%Y-%m-%d')\ndataset.drop('Date', inplace=True, axis=1)\ndataset = dataset[0:365]   # resize the dataset to get rid of null rows\ndatetime_column = dataset.pop('parsed_date')\ndataset['Day'] = datetime_column.dt.day\ndataset['Month'] = datetime_column.dt.month\ndataset['Year'] = datetime_column.dt.year\n\n\ncor_mat = dataset.corr()   # get the correlation matrix\nh_map = sns.heatmap(data=cor_mat, annot=True, cmap='RdYlGn')\n\n\nthe original dataset can be found on Kaggle, I posted a link above, but I made some processing so that now I have those Features:\n\n Index(['MediumTemperature', 'MinimumTemperature', 'MaximumTemperature',\n       'Precipitation', 'Weekend', 'BeerConsumption', 'Day', 'Month', 'Year'],\n      dtype='object')\n\n\nI only split the Date Feature into a separate year, month and day and then I plotted the Heatmap. maybe someone had this Problem before and can tell me why is this happening, I also find a similar question\nin R but I didn't understand the answers because I have no Knowledge of R. I hope someone have an answer to this in Python.\n"
"I have a dictionary that looks like this:\n\nmy_dict = {'Community A': ['User 1', 'User 2', 'User 3'],\n           'Community B': ['User 1', 'User 2'],\n           'Community C': ['User 3', 'User 4', 'User 5'],\n           'Community D': ['User 1', 'User 3', 'User 4', 'User 5']}\n\n\nMy goal is to model the networked relations between the different communities and their sets of unique users to see which communities are most similar. Currently, I am am exploring using Jaccard similarity.\n\nI have come across answers that do similar operations, but only on exactly 2 dictionaries; in my case, I have several, and will need to calculate the similarities between each set.\n\nAlso, some of the lists are of different lengths: in other answers, I saw 0 sub in as a missing value in that case, which I think will work in my case.\n\nAny help with this issue is greatly appreciated!\n"
'I have a dataframe consisting of some Jira issues which I\'m trying to sort by week and status, and get the amount of items for each status per week. So for example, two weeks of my dataframe currently look like this: \n\n2019-11-04     Authorize Work      4\n               Await Work          1\n               Check Work          4\n               Closed              4\n               Confirm Work        3\n               Do Work             3\n2019-11-11     Authorize Work      6\n               Do Work             2\n\n\nI\'ve gotten to this point with the following: \n\n# Remove the time portion of the date\ndf[\'creation_date\'] = df[\'creation_date\'].dt.date\n# Set the date to be a week long delta\ndf[\'creation_date\'] = pd.to_datetime(df[\'creation_date\']) - pd.to_timedelta(7, unit=\'d\')\n# Sort together by creation date within the week and the status, and do a count\nendf = df.groupby([pd.Grouper(key="creation_date", freq="W-MON"), "status"]).size()\n\n\nYou\'ll notice that the second week only has two statuses, where the first has six. This is because there were zero jira issues with the missing statuses during the second week. Is there a way to make the size function include the missing statuses with a count of zero, so that the data within each week is the same set of statuses?\n'
"I tried this \n\nprint(housing.columns[housing.isnull().any()], housing.isnull().sum().sum())\n\n\nand Got the following output:\n\nIndex(['total_bedrooms'], dtype='object') 207\n\n\nBut I'm trying to get the following output:\n\n(total_bedrooms, 207 )\n\n\nI believe there's a more intuitive way to get the answer I want than the way I did. Any help would be appreciated.\n"
'For background:  I am using the Consumer Financial Protection Bureau dataset on consumer complaints.  I want to make a complaint-wise time-series plot by plotting complaint-wise counts on y and time on x getting a line for each type of complaint. I want to see how the values have changed over time. I\'ve experimented with Seaborn and Plotly so far.  Below is in Plotly.\n\ntrace1 = go.Scatter(x=df.DateReceived,\n                    y=df.Sum,\n                    name = "Time Series of Types of Complaints",\n                    line = dict(color = \'blue\'),\n                    opacity = 0.4)\n\nlayout = dict(title=\'Time Series of Types of Complaints\',)\n\nfig = dict(data=[trace1], layout=layout)\niplot(fig)\n\n\n\n\nThe dataframe looks like this:\n\ndata = {\'Date\': [\'2011-12-01\', \'2011-12-06\', \'2011-12-06\', \'2011-12-07\', \'2011-12-07\'], \'Issue\':  [\'Loan Modification\', \'Loan Servicing\', \'Loan Servicing\', \'Loan Modification\', \'Loan Servicing\'], \'Sum\': [1, 1, 2, 2, 3]}\n\ndf = pd.DataFrame(data)\n\n\nI know the issue in my plot is that it\'s connecting all of the different sums together and not separating them.\n\nI know I could separate each sum into different columns for each of the different types of complaints.  And then adding each trace on manually, doing something like this (taken from Plotly website):\n\nfig.add_trace(go.Scatter(\n                x=df.Date,\n                y=df[\'AAPL.Low\'],\n                name="AAPL Low",\n                line_color=\'dimgray\',\n                opacity=0.8))\n\n\nBut there must be an easier / less bruteforce way, where I can keep all of the sums in one column, and delineate by type of issue.\n'
"I have a data frame which contains two columns UserId and movieId. Different users have watched different movies. I want to fetch (e.g. three common movies between two common users). \n\ndf = DataFrame({'userId' : [1,2,3,1,3,6,2,4,1,2], 'movieId' : [222,222,900,555,555,888,555,222,666,666]})\n\n\n\nThe required result should be like this\n\nuserId movieId\n1       222\n1       555\n1       666\n2       222\n2       555\n2       666\n\n\n\ni do not need other entries which does not contain three common movies for  users. For example, if there is another user who watched all three movies should be considered.\n"
'I have two numpy vector arrays, one contains binary values so either 1 or 0 and the other float values so anything in between 0 and 1. \n\nI want to use the numpy.logical_and operator and have it return true if the binary value is in the range of the float plus or minus 0.2. So i.e. a float of 0.1 would return true, 0.4 false. \n\nHow would I tackle this?\n'
"As per my knowledge Python loops are slow, hence it is preferred to use pandas inbuilt functions.\n\nIn my problem, one column will have different currencies, I need to convert them to dollar. How can I detect and convert them to dollar using pandas inbuilt functions ?\n\nMy column as following:\n\n100Dollar\n200Dollar\n100Euro\n300Euro\n184pounds\n150pounds\n10rupee\n30rupee\n\n\nNote: amount and currency name is in same column.\n\nNote: conversion exchange rate w.r.t dollar {Euro: 1.2, pounds: 1.3, rupee: 0.05}\n\nNote: currency enum is ['Euro', 'Dollar', 'Pounds', 'Rupee']\n"
"I am using \n\n\n  pd.get_dummies\n\n\nto transform categorical vector with 4 labels (strings) to 2d array with 4 columns.\nHowever, after I coudln't find a way to go back to the original values afterwards.\nI also couldn't do this when using \n\n\n  sklearn.preprocessing.OneHotEncoder\n\n\nWhat is the best wat to one-hot-encode categorcal vector but have the ability to inverse the original value afterwards?\n"
"I draw the qq plot multiple regression and I got below graph. Can someone tell me why there are two points under the red line? And do these points have an effect on my model? \n\n\n\nI used below code for draw the graph.\n\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg = reg.fit(x_train,y_train)\n\npred_reg_GS = reg.predict(x_test)\ndiff= y_test-pred_reg_GS\n\nimport statsmodels.api as sm\nsm.qqplot(diff,fit=True,line='45')\nplt.show()\n\n"
'Getting started with Pandas.\n\nI have two columns:\nA                     B\nSomething             Something Else\nEverything            Evythn\nSomeone               Cat\nEveryone              Evr1\n\n\nI want to calculate fuzz ratio for each row between the two columns so the output would be something like this:\n\nA                     B                  Ratio\nSomething             Something Else     12\nEverything            Evythn             14\nSomeone               Cat                10\nEveryone              Evr1               20\n\n\nHow would I be able to accomplish this? Both the columns are in the same df.\n'
"I'd like to produce plotly plots using pandas dataframes. I am struggling on this topic.\n\nNow, I have this:\n\n           AGE_GROUP                       shop_id         count_of_member\n0                 10                             1                      40\n1                 10                            12                   57615\n2                 20                             1                     186\n4                 30                             1                     175\n5                 30                            12                  322458\n6                 40                             1                     171\n7                 40                            12                  313758\n8                 50                             1                     158\n10                60                             1                     168\n\n\nSome shop might not have a record. As an example, plotly will need x=[1,2,3], y=[4,5,6]. If my input is x=[1,2,3] and y=[4,5], then x and y is not the same size and an exception will be raised. I need to add a null value record for the missing shop_id. So, I need this:\n\n           AGE_GROUP                       shop_id         count_of_member\n0                 10                             1                      40\n1                 10                            12                   57615\n2                 20                             1                     186\n3                 20                            12                       0\n4                 30                             1                     175\n5                 30                            12                  322458\n6                 40                             1                     171\n7                 40                            12                  313758\n8                 50                             1                     158\n9                 50                            12                       0\n10                60                             1                     168\n11                60                            12                       0\n\n\nFor each age_group, I need to have 2 shop_id since the unique set of shop_id is 1 and 12\nif there are 10 age_group, 20 rows will be shown.\nFor example:\n\n           AGE_GROUP                       shop_id         count_of_member\n1                 10                            12                   57615\n2                 20                             1                     186\n3                 30                             1                     175\n4                 40                             1                     171\n5                 40                            12                  313758\n6                 50                             1                     158\n7                 60                             1                     168\n\n\nthere are 2 unique shop_id: 1 and 12 and 6 different age_group: 10,20,30,40,50,60\nin age_group 10: only shop_id 12 is exists but no shop_id 1. \nSo, I need to have a new record to show the count_of_member of age_group 10 of shop_id 1 is 0.\nThe finally dataframe i will get should be:\n\n           AGE_GROUP                       shop_id         count_of_member\n1                 10                            12                   57615\n**1                 10                             1                       0**\n2                 20                             1                     186\n**2                 20                            12                       0**\n3                 30                             1                     175\n**3                 30                            12                       0**\n4                 40                             1                     171\n5                 40                            12                  313758\n6                 50                             1                     158\n**6                 50                            12                       0**\n7                 60                            12                       0\n7                 60                             1                     168\n\n\n** are the new added rows\n\nHow can i implement this transformation?\n"
"Background\n\nI'm a data analyst setting up a new data environment to perform analysis using Python in Jupyter notebooks.\n\nI have installed miniconda on mac, and used it to create an environment called myenv. Inside there I have installed Jupyter using conda (system info below contains versions and builds). \n\nMain question\n\nWhen I launch a Jupyter notebook from terminal with jupyter notebook, and select new, I have two options to create a new notebook (in this screenshot). \n\nPython 3 and Python 3.7.4 64-bit ('base': conda)\n\nWhat is the difference between these options, and which should I use to create the notebook? \n\nExtra info\n\nMy intention is to setup a conda data environment containing all of the packages needed for data analysis (and no extra), which is easy to replicate if other analysts onboard or join in working on the same piece. \n\nI appreciate this is hopefully a very simple question - I'd be grateful of any suggested articles to help understand the setup process. \n\nThank you!\n\nJack\n\nSystem info\n\n\nmacOS Mojave - Version 10.14.6 \nminiconda installation - conda 4.8.2\nconda environment myenv (created with conda create --name myenv) containing:\n\n\npython version 3.8.1\njupyter (installed with conda install jupyter) - version 1.0.0 build py38_7\njupyter_client - version 5.3.4 build py38_0\njupyter_console - version 6.1.0 build py_0\njupyter_core - version 4.6.1 build py38_0\n\n\n"
'I am trying to do the following:\n\n\nRead through a specific portion of a text file (there is a known starting point and ending point)\nWhile reading through these lines, check to see if a word matches a word that I have included in a list \nIf a match is detected, then add that specific word to a new list\n\n\nI have been able to read through the text and grab other data from it that I need, but I\'ve been unable to do the above mentioned thus far.\n\nI have tried to implement the following example: Python - Search Text File For Any String In a List\nBut I have failed to make it read correctly.\n\nI have also tried to adapt the following: https://www.geeksforgeeks.org/python-finding-strings-with-given-substring-in-list/\nBut I was equally unsuccessful.\n\nHere is some of my code:\n\nimport re\nfrom itertools import islice\nimport os\n\n# list of all countries\noneCountries = "Afghanistan, Albania, Algeria, Andorra, Angola, Antigua &amp; Deps, Argentina, Armenia, Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bhutan, Bolivia, Bosnia Herzegovina, Botswana, Brazil, Brunei, Bulgaria, Burkina, Burma, Burundi, Cambodia, Cameroon, Canada, Cape Verde, Central African Rep, Chad, Chile, China, Republic of China, Colombia, Comoros, Democratic Republic of the Congo, Republic of the Congo, Costa Rica,, Croatia, Cuba, Cyprus, Czech Republic, Danzig, Denmark, Djibouti, Dominica, Dominican Republic, East Timor, Ecuador, Egypt, El Salvador, Equatorial Guinea, Eritrea, Estonia, Ethiopia, Fiji, Finland, France, Gabon, Gaza Strip, The Gambia, Georgia, Germany, Ghana, Greece, Grenada, Guatemala, Guinea, Guinea-Bissau, Guyana, Haiti, Holy Roman Empire, Honduras, Hungary, Iceland, India, Indonesia, Iran, Iraq, Republic of Ireland, Israel, Italy, Ivory Coast, Jamaica, Japan, Jonathanland, Jordan, Kazakhstan, Kenya, Kiribati, North Korea, South Korea, Kosovo, Kuwait, Kyrgyzstan, Laos, Latvia, Lebanon, Lesotho, Liberia, Libya, Liechtenstein, Lithuania, Luxembourg, Macedonia, Madagascar, Malawi, Malaysia, Maldives, Mali, Malta, Marshall Islands, Mauritania, Mauritius, Mexico, Micronesia, Moldova, Monaco, Mongolia, Montenegro, Morocco, Mount Athos, Mozambique, Namibia, Nauru, Nepal, Newfoundland, Netherlands, New Zealand, Nicaragua, Niger, Nigeria, Norway, Oman, Ottoman Empire, Pakistan, Palau, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Poland, Portugal, Prussia, Qatar, Romania, Rome, Russian Federation, Rwanda, St Kitts &amp; Nevis, St Lucia, Saint Vincent &amp; the Grenadines, Samoa, San Marino, Sao Tome &amp; Principe, Saudi Arabia, Senegal, Serbia, Seychelles, Sierra Leone, Singapore, Slovakia, Slovenia, Solomon Islands, Somalia, South Africa, Spain, Sri Lanka, Sudan, Suriname, Swaziland, Sweden, Switzerland, Syria, Tajikistan, Tanzania, Thailand, Togo, Tonga, Trinidad &amp; Tobago, Tunisia, Turkey, Turkmenistan, Tuvalu, Uganda, Ukraine, United Arab Emirates, United Kingdom, United States, Uruguay, Uzbekistan, Vanuatu, Vatican City, Venezuela, Vietnam, Yemen, Zambia, Zimbabwe"\ncountries = oneCountries.split(",")\n\npath = "C:/Users/me/Desktop/read.txt"\nthefile = open(path, errors=\'ignore\')\n\ncountryParsing = False\nfor line in thefile:\n    line = line.strip()\n#    if line.startswith("Submitting Author:"):\n#    if re.match(r"Submitting Author:", line):\n#        print("blahblah1")\n#        countryParsing = True\n#        if countryParsing == True:\n#            print("blahblah2")\n#            \n#            res = [x for x in line if re.search(countries, x)]\n#            print("blah blah3: " + str(res))\n#    elif re.match(r"Running Head:", line):\n#        countryParsing = False\n#    if countryParsing == True:\n#        res = [x for x in line if re.search(countries, x)]\n#        print("blah blah4: " + str(res))\n\n\n#        for x in countries:\n#            if x in thefile:\n#                print("a country is: " + x)\n#        if any(s in line for s in countries):\n#            listOfAuthorCountries = listOfAuthorCountries + s + ", "\n#    if re.match(f"Submitting Author:, line"):\n\n\nThe #commented out lines are versions of the code that I\'ve tried and failed to make work properly.\n\nAs requested, this is an example of the text file that I\'m trying to grab the data from.  I\'ve modified it to remove sensitive information, but in this particular case, the "new list" should be appended with a certain number of "France" entries:                  \n\n    txt above....\nSubmitting Author:\n\n    asdf, asdf  (proxy)\n    France\n    asdfasdf\n    blah blah\n    asdfasdf\n\n    asdf, Provence-Alpes-Côte d\'Azu 13354\n    France\n\n    blah blah\n    France\n    asdf\nRunning Head:\n    ...more text below\n\n'
'I have a requirement such that :\n\ni have a csv file with following data (this is just example, data is much larger):\n\nxxxx|A|B|C|D|E\nxxxx|P|Q|R|S|T\nDATE|L|M|N|O|P\n01/02/1997|12|4|5|0|0\n01/03/1998|71|2|4|8|0\n\n\ni want to save the data in a data frame such that the columns should be :\n\n A   01/02/1997   P   L  12\n B   01/02/1997   Q   M   4\n C   01/02/1997   R   N   5\n D   01/02/1997   S   O   0\n E   01/02/1997   T   P   0\n A   01/03/1998   P   L  71\n B   01/03/1998   Q   M   2\n C   01/03/1998   R   N   4\n D   01/03/1998   S   O   8\n E   01/03/1998   T   P   0 \n\n\nbasically first column should be pivoted . can anyone help? the names and data are just examples(dummy)\n'
'I was working with a practice data set and was told to make a new column with the sum of others like this:\n\ndf["new column"] = df["column4"] + df["column5"] + df["column6"] + df["column7"] + etc....\n\n\nI feel this is inefficient due to the fact that they are even line up (columns 4-9) and I need to speed up my coding (data sci competition coming up!).\n\nhow may i do this in a shorter / more efficient way!\n\nI\'ve tried using variations of stuff like df[\'Total\'] = sum(df.columns[4:9]) but its not working out\n\n\n  disclaimer: i am new to pandas\n\n'
"I have a dataframe with conditions like this:\n\n  condition1 Condition2 Condition3 Condition4\n0 duck&gt;1          goat&gt;2     sheep=0    chicken=0\n1 duck=0          chicken=0  donkey&gt;1   zebra&gt;0\n\n\nAnd a list with a single condition to check against the df as:\n\nlist = ['goat','goat','duck','goat','duck']\n\n\nFrom this list I have created another list which shows the number occurences of each element in list1\n\nlist2 = ['goat=3','duck=2']\n\n\nFor this example, I want to get back row '0' in the df since the conditions are all met(note: absence of sheep and chicken in my list is equal to sheep=0 and chicken=0).\n\nThe number of categories in conditions(in this case animals) are so many to define them beforehand.\n\nHow would I check and get back the rows where the conditions are met?\n\nThanks\n"
'I have a multiple types of images in one folder and i want to save each type of Images in a different variable by the name and the first 6 character of the name is same and the primly character is the 7th.\n\ni know how to import the images but my issue is how to separate it, and display them in the same time.\n\nimport cv2\nimport glop\n\nimage = [cv2.imread(img)for img in glop.glop ("c:/B/*.png")]\n\n\n\n\ne.g : \n\nsa_01_DF001 "D Type",\n\nsa_01_DF002 "D Type",\n\nsa_01_NB001 "N Type",\n\nsa_01_NB002 "N Type",\n\nsa_01_KP001 "K Type",\n\nsa_01_KP002 "K Type".\n\nTo summarize my problem :\n\n\nSeparate the image by the name.\nSave each type into variable.\nDisplay multiple image in same time.\n\n'
'I\'m new to data science and I am able to build a model and put a pipeline along with onehotencoder. However, when I call the function that I build, it gives an error. Please see below and please advice. Thanks in advance! \n\nclf = Pipeline(steps=[(\'ohe\', OneHotEncoder()),\n                  (\'rfc\', RandomForestClassifier(n_estimators=1000,criterion="entropy",max_features=None))])  \npickle.dump(clf,open(\'model.pkl\',\'wb\'))\n\n# load model\nmodel = pickle.load(open(\'model.pkl\',\'rb\'))\n\ndef predict(A,B,C,D,E,F,G):\n\n    result = model.predict(x)\n\n    # send back to browser\n    output = {\'results\': int(result[0])}\n\n\n    # return data\n    return jsonify(results=output)\n\n\nto call function: \n\npredict(\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'G\')\n\n\nError: \n\nNotFittedError: This OneHotEncoder instance is not fitted yet. Call \'fit\' with appropriate arguments before using this estimator.\n\n'
'How do I expand the time range of information in a dataframe as a new data frame.\n\n\nI have a data frame df with dates, strings and factors similar to\nthis:\n\n\n "start"     "end"      "note"   "item"\n2016-12-30  2017-01-03    Z        1\n2017-09-10  2017-09-14    W        2\n\n\n\nI want to expand it as follows.\n\n\n "start"      "note"    "item"\n2016-12-30     Z         1\n2016-12-31     Z         1\n2017-01-01     Z         1\n2017-01-02     Z         1\n2017-01-03     Z         1\n2017-09-10     W         2\n2017-09-11     W         2\n2017-09-12     W         2\n2017-09-13     W         2\n2017-09-14     W         2\n\n\nHow can I do it using python.\n'
"I'm testing the embedded methods for feature selection.\n\nI understand (maybe I misunderstand) that with embedded methods we can get best features (based on importance of the features) while training the model.\n\nIs so, I want to get the score of the trained model (which was trained to select features).\n\nI'm testing that on classification problem with Lasso method.\n\nWhen I'm trying to get the score, I'm getting error that I need to fit again the model.\n\n\nWhy do I need to do it (it seem waste of time if the model was fitted on feature selection ?)\nWhy can't we do it (select features and get model score) in one shot ?\nWhy if we are using embedded method, why do we need to do it  2 phases ? why can't we train the model while choose the best features in one fit ?\n\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nestimator = LogisticRegression(C=1, penalty='l1', solver='liblinear')\nselection = SelectFromModel(estimator)\nselection.fit(x_train, y_train)\nprint(estimator.score(x_test, y_test))\n\n\n\nError:\n\nsklearn.exceptions.NotFittedError: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n\n"
'I have a dataframe:\n\ndf = Condition A_value B_value\n       15       2       3\n       30       1       8\n       45       5       1\n       15       3       9\n       30       4       6\n       45       8       2\n       15       5       2\n       30       7       3\n       45       9       1\n       15       0       8\n       30       8       7\n       45       1       3\n\n\nI want to apply sklearn.preprocessing.StandardScaler\n\nOn each column , by the type. (meaning it will scale the column A_value for rows where condition=15, then A_value for 30, 45 , and B_value for  15, then 30, then 45.\nIs there a way to do it without iterating over all groups? (e.g one-liner?)\n\nWhat is the best way to do it?\n'
"I have a big dataset and when I try to run this code I get a memory error.\n\nuser_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n\n\nhere is the error:\n\nValueError: Unstacked DataFrame is too big, causing int32 overflow\n\n\nI have run it on another machine and it worked fine! how can I fix this error?\n"
"I'm analysing this Kaggle dataset: https://www.kaggle.com/astronasko/transport-for-london-journey-information\n\nI've created a DataFrame with all the completed journeys, where the start station ('StartStn') and end station ('EndStn') are not the same and there is information on each of them.\n\nI've created a frequency plot of Start stations and a separate frequency plot of end stations (see images below):\n\n\n\n\n\nFigure 1 code:\ncomplete['StartStn'].value_counts()[:20].plot(kind='bar')\n\nFigure 2 code:\ncomplete['EndStn'].value_counts()[:20].plot(kind='bar')\n\nHere is a sample of the dataframe, taking a subset of just these two columns:\n\nIN:\n\ncomplete[['StartStn','EndStn']].sample(10)\n\n\nOUT:\n\n        StartStn             EndStn\n102417  Leytonstone          East Ham\n995246  Walthamstow Central  Piccadilly Circus\n1102327 Earls Court          Holborn\n604323  Stratford            Shepherd's Bush Und\n481718  Warren Street        Walthamstow Central\n2344106 Marble Arch          Northolt\n1234444 Colliers Wood        Holborn\n1408620 Earls Court          Marble Arch\n465436  Tottenham Court Rd   Mile End\n1580309 Woodside Park        Hammersmith D\n\n\nAs you can see, many stations, such as 'Walthamstow Central', are in both columns.\n\nProblem:\n\nUsing seaborn, matplotlib or pandas, how do I create a frequency plot for all stations that has a hue of StartStn vs EndStn (i.e. on the same axes)?\n\nThe best I can do is to create a frequency plot with all stations, combining frequencies in 'StartStn' and 'EndStn':\n\nstations = pd.concat([complete['StartStn'],complete['EndStn']],axis=0)\nstations.value_counts()[:10].plot(kind='bar')\n\n\nWhich gives me the following output:\nMost Popular Stations (Start or End)\n\n\nWould be very grateful for any suggestions!\n\nThanks a lot,\n\nBeni\n"
"I want to change the time in the entire column to strings. Please provide the method with an example.\nGiven below is my data.\ntimeseries  CCN Number Conc SS\n0   00:00:00    15.810967   0.1\n1   00:05:00    358.401000  0.3\n2   00:10:00    797.538333  0.5\n\ngiven below is my code:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nload_file=pd.read_excel(r'E:\\CCNC\\CCNCCodes\\Modulated output\\Copy of Final_modulated_ccnc_data.xlsx',header=0)\nload_file.columns\ns=load_file.loc[0:289,['Timeseries',' CCN Number Conc','SS']]\n\nThank you.\n"
"I need to split the dataframe based on weekdays,\nThe actual dataframe looks like this,\ndf = pd.DataFrame({'values': [10,5,30,44,52,6,7,85,9,1,1,1,13,14,1,16]})\ndf['weekdays'] = ['Monday','Tuesday','Wednesay','Thursday','Friday','saturday','sunday',\n        'Tuesday','Wednesay','Thursday','Friday','saturday','sunday',\n      'Monday','Tuesday','Wednesday']\n\n\n    values   weekdays\n0       10     Monday\n1        5    Tuesday\n2       30   Wednesay\n3       44   Thursday\n4       52     Friday\n5        6   saturday\n6        7     sunday\n7       85    Tuesday\n8        9   Wednesay\n9        1   Thursday\n10       1     Friday\n11       1   saturday\n12      13     sunday\n13      14     Monday\n14       1    Tuesday\n15      16  Wednesday\n\nhow to split the dataframe based on weekdays like shown below? I Tried splitting the dataframe as 7 rows each, but this adds data from consequtive week, so how not to add data from consequtive week and split the dataframe exactly like this below? Thanks in advance for help.\nnew_df_1\n\nnew_df_2\n\nnew_df_3\n\n"
"In my df I have a column for each entity below (Grubhub, Toasttab, Tenk) and it says either yes or no in the value of that column for each row,\nI have below code such as:\ndf['Grubhub'] = df[['On GrubHub or Seamless?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\ndf['ToastTab'] = df[['On ToastTab?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\ndf['Tenk'] = df[['On Tenk?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\ndf['Udemy'] = df[['On Udmey?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\ndf['Postmates'] = df[['On Postmates?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\ndf['Doordash'] = df[['On DoorDash?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\ndf['Google'] = df[['On Goole?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\nThis gives me a new column for each entity ( Grubhub, Toasttab, Tenk ) and that column gives a true of false value, is there a more efficient method where I can do all of these in one line of code or function? Thanks for help in advance\n"
'I have the dataframe:\nvid_fn  V1  V2  V3\n a.avi  1   4   5 \n b.avi  7   8   1\n\nI want to change duplicate each row 5 times, while changing the first column:\nframe_fn  V1  V2  V3\n a1.jpg  1   4   5 \n a2.jpg  1   4   5 \n a3.jpg  1   4   5 \n a4.jpg  1   4   5 \n a5.jpg  1   4   5 \n b1.jpg  7   8   1 \n b2.jpg  7   8   1 \n b3.jpg  7   8   1 \n b4.jpg  7   8   1 \n b5.jpg  7   8   1 \n\nthe 5 values are generate by an outer function, so when calling\nmy_func(a.avi) \n\nI get:\n[a1.jpg, a2.jpg, a3.jpg, a4.jpg, a5.jpg]\n\nWhat is the best way to do so?\n'
"Given a DataFrame:\n   Number\n0    4\n1    6\n2    2\n3   -1\n\nIs there an easy and clean way to sum the first number(4) with the number after(6) and the sum(6) with(2) and so on. And is it possible to return a Pandas.series and add it as a new column of the dataframe with NaN where the number does not have a number after?.\nFor example, for the given Dataframe:\n   Number Sum\n0    4     10\n1    6     8\n2    2     1\n3   -1     NaN\n\nwhere 10=4+6, 8=6+2, and 1=2-1\nSOLUTION\n.shift() function helped me to solve this problem!\nCode:\ndf['Sum'] = df['Number'] + df['Number'].shift(-1)\n\nand the output is:\n   Number Sum\n0    4     10\n1    6     8\n2    2     1\n3   -1     NaN\n\n"
"How to map closed values from two dataframes:\nI've two dataframes in below format and looking to map values based on o_lat,o_long from data1 and near_lat,near_lon:\n\r\n\r\ndata1 ={'lat': [-0.659901, -0.659786, -0.659821], \n       'long':[2.530561, 2.530797, 2.530587], \n       'd':[0.4202, 1.0957, 0.6309],\n      'o_lat':[-37.8095,-37.8030,-37.8050],\n      'o_long':[145.0000,145.0077,145.0024]} \r\n\r\n\r\n\nWhere lat,long is coordinates of destination, d is the distance between origin and destination, o_lat,o_long is the coordinates of origin.\n\r\n\r\ndata2={'nearest_warehouse': ['Nickolson','Thompson','Bakers'], \n      'lat':[-37.8185,-37.8126,-37.8099],\n      'lon':[144.9695,144.9470,144.9952]} \r\n\r\n\r\n\nI want to produce another column in data1 which locates nearest_warehouse in the following format based on closed value:\n\r\n\r\nresult={'lat': [-0.659901, -0.659786, -0.659821], \n       'long':[2.530561, 2.530797, 2.530587], \n       'd':[0.4202, 1.0957, 0.6309],\n      'o_lat':[-37.8095,-37.8030,-37.8050],\n      'o_long':[145.0000,145.0077,145.0024],\n        'nearest_warehouse':['Bakers','Thompson','Nickolson']}\r\n\r\n\r\n\nI've tried following code:\nlat_diff=[]\nlong_diff=[]\nmin_distance=[]\nfor i in range(0,3):\n    lat_diff.append(float(warehouse.near_lat[i])-lat_long_d.o_lat[0])\nfor j in range(0,3):\n    long_diff.append(float(warehouse.near_lon[j])-lat_long_d.o_long[0])\n        long_diff.append(float(warehouse.near_lon[j])-lat_long_d.o_long[0])\nmin_distance=[min(lat_diff),min(long_diff)]\nmin_distance\n\nWhich gives the following result which is the minimum value of the difference between latitude and longitude for o_lat=-37.8095 and o_lang=145.0000:\n[-0.00897867136701791, -0.05300973586690816].\nI feel the approach is not viable to map close values over a large dataset.\nLooking for a better approach in this regard\n"
"Trying to turn the values in the Y axis into dollar amount, when using the update_layout method it only affects the first chart but not the others. I am not sure where to put the method, or how I could apply the formatting to each trace individually.\nfig = make_subplots(rows=2, cols=2, \n                    subplot_titles=(&quot;Daily&quot;, &quot;Week To Date&quot;, &quot;Month To Date&quot;, &quot;Quarter To Date&quot;),\n                    )\n    \n    fig.update_layout(yaxis_tickprefix = '$', yaxis_tickformat = ',.')\n\n    CS_df_Daily, CS_df_Weekily  = Current_Stock_Profile.Daily_DateFrame, Current_Stock_Profile.WeekToDate_DataFrame\n    CS_df_Month, CS_df_Quarter  = Current_Stock_Profile.MonthToDate_DataFrame, Current_Stock_Profile.QuarterToDate_DataFrame\n\n    fig.add_trace(go.Candlestick(x=CS_df_Daily.index,\n                open=CS_df_Daily['Open'],\n                high=CS_df_Daily['High'],\n                low=CS_df_Daily['Low'],\n                close=CS_df_Daily['Close']),\n                row = 1, col = 1)\n                \n\n    fig.add_trace(go.Candlestick(x=CS_df_Weekily.index,\n                open=CS_df_Weekily['Open'],\n                high=CS_df_Weekily['High'],\n                low=CS_df_Weekily['Low'],\n                close=CS_df_Weekily['Close']), row = 1, col = 2)\n\n\n    fig.add_trace(go.Candlestick(x=CS_df_Month.index,\n                open=CS_df_Month['Open'],\n                high=CS_df_Month['High'],\n                low=CS_df_Month['Low'],\n                close=CS_df_Month['Close']),row = 2, col = 1)\n\n    fig.add_trace(go.Candlestick(x=CS_df_Quarter.index,\n                open=CS_df_Quarter['Open'],\n                high=CS_df_Quarter['High'],\n                low=CS_df_Quarter['Low'],\n                close=CS_df_Quarter['Close']), row = 2, col = 2)\n\n    fig.update_layout(height=750, width=1200,showlegend=False,\n                  title_text=Current_Stock_Profile.shortName)\n    \n\n    fig.update_xaxes(rangeslider_visible=False)\n\nExample of the Chart I am generating\n"
'I have a dataframe which looks like this:\n**col_A col_B col_C**\nFalse True  False\nTrue  False False\nFalse False True\nFalse False False\n\nI need to collect the column name whose value is True for each row and create another dataframe:\n**col**\ncol_B\ncol_A\ncol_C\nnan\n\nNote: there is at most one True value in a row.\n'
"# Find string between two strings\ndef find_between( s, first, last ):\n     try:\n         start = s.index( first ) + len( first )\n         end = s.index( last, start )\n         return s[start:end]\n    except ValueError:\n         return &quot;&quot;\n\nfrom urllib.request import urlopen\nfrom lxml import html\nimport requests\n\nlink = &quot;https://www.calendar-12.com/catholic_holidays/2019&quot;\nresponse = urlopen(link)\ncontent = response.read().decode(&quot;utf-8&quot;) \n\n\ntable = find_between(content, &quot;&lt;tbody&gt;&quot;,&quot;&lt;/tbody&gt;&quot;);\nrows = table.split(&quot;/tr&quot;)\ncsv = &quot;Day\\n&quot;\nfor row in rows:\n    day = find_between(row, '&quot;&gt;', &quot;&lt;/t&quot;)\n    day = find_between(day, &quot;&gt; &quot;, &quot;&lt;/&quot;)\n    csv = csv + day + &quot;\\n&quot;\n\nprint(csv)\n\nThis code suppose to extract date from the website but it doesn't, can you help to solve the problem. the only output is Day\n"
"I'm attempting to select a single sample from a range of Normal distributions based upon the output of a categorical distribution, however can't seem to come up with quite the right way to do it. Using something along the lines of:\ntfp.distributions.JointDistributionSequential([\n        tfp.distributions.Categorical(probs=[0, 0, 1/2, 1/2]),\n        lambda c: tfp.distributions.Normal([0, 1, -10, 30], 1)[..., c]\n    ])\n\nReturns exactly what I want for the single case, however if I want multiple samples at once this breaks (as c becomes a numpy array rather than an integer. Is this possible and if so, how should I go about it?\n(I also attempted using OneHotCategorical and multiplying but that didn't work at all!)\n"
"I have short audio files which I'm trying to analyze using Librosa, in particular the spectral centroid function.  However, this function outputs an array of different values representing the spectral centroid at different frames within the audio file.  The documentation says that the frame size can be changed by specifying the parameter n_fft when calling the function.  It would be more beneficial to me if this function analyzed the entire audio file at once rather than outputting the result at multiple points in time.  Is there a way for me to specify that I want the function to be called with, say, a frame size of the entire audio file instead of the default time which is 2048 samples?  Is there another better way?\nCheers and thank you!\n"
"I want to convert a CSV file of time-series data with\nmultiple sensors.\nThis is what the data currently looks like:\n\nThe different sensors are described by numbers and have different numbers of axes. If a new activity is labeled, everything below belongs to this new label. The label is in the same column as the first entry of each sensor.\nThis is the way I would like the data to be:\nEach sensor axis has its own column and the according label is added in the last column.\nSo far, I have created a DataObject class to access timestamp, sensortype, sensorvalues, and the belonging parent_label for each row in the CSV.\nI thought the most convenient way to solve this would be by using pandas DataFrame but simply using pd.DataFrame(timestamp, sensortype, sensorvalues, label)\nwon't work.\nAny ideas/hints? Maybe other ways to solve this problem?\nI am fairly new to programming, especially Python, so I have already run out of ideas.\nThanks in advance\n"
"Starting with data formatted like this:\n\ndata = [[0,1],[2,3],[0,1],[0,2]]\n\n\nI would like to represent every value once with its frequency:\n\noutput = [[[0,1],2],[[2,3],1],[[0,2],1]]\n\n\nI've found many solutions to this problem for 1D lists, but they don't seem to work for 2D.\n"
"I have a pandas.Dataframe called a and the structure is as follows:\n\n\nwhile I want to get the DataFrame structure is like:\n\n\n\nwhere the b is like the transpose of a.\nBy convert a to b, I use the code :\n\nid_uni = a['id'].unique()\nb = pd.DataFrame(columns=['id']+[str(i) for i in range(1,4)])\nb['id'] = id_uni\n\nfor i in id_uni:\n    for j in range(7):\n        ind = (a['id'] == i) &amp; (a['w'] == j)\n        med = a.loc[ind, 't'].values \n        if med:  \n            b.loc[b['id'] == i, str(j)] = med[0]\n        else:\n            b.loc[b['id'] == i, str(j)] = 0\n\n\nThe method is very brutal that I just use two for-loops to get all elements from a to b. And it is very slow. Do you have an efficient way to improve it?\n"
"The question which im trying to solve is as follows: \n\nWhich country has the biggest difference between their summer gold medal counts and winter gold medal counts relative to their total gold medal count?\n\nOnly include countries that have won at least 1 gold in both summer and winter.\nThis function should return a single string value\n\nData\n\nGold: Count of summer gold medals\n\nGold.1: Count of winter gold medals\n\nGold.2: Total Gold\n\nCode\n\ndef answer_three():    \n    df1=df[(df['Gold']&gt;0) &amp; (df['Gold.1']&gt;0)]    \n    df['difference']=(df1['Gold']-df1['Gold.1']).abs()/df1['Gold.2']    \n    return df['difference'].idxmax()    \nanswer_three()\n\n\nError\n\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-7-cf807e29ca1b&gt; in &lt;module&gt;()\n      3     df['difference']=(df1['Gold']-df1['Gold.1']).abs()/df1['Gold.2']\n      4     return df['difference'].idxmax()\n----&gt; 5 answer_three()\n      6 \n\n&lt;ipython-input-7-cf807e29ca1b&gt; in answer_three()\n      1 def answer_three():\n----&gt; 2     df1=df[(df['Gold']&gt;=1) &amp; (df['Gold.1']&gt;=1)]\n      3     df['difference']=(df1['Gold']-df1['Gold.1']).abs()/df1['Gold.2']\n      4     return df['difference'].idxmax()\n      5 answer_three()\n\nNameError: name 'df' is not defined\n\n\nI am able to derive an answer without the condition that the country should have atleast 1 gold medal in both summer and winter. I am hoping someone could help me with fixing the error in line 2\n\nThanks\n"
"I have a lot of data that is barely interpretable by bare sight as an xy-scatter plot. For mit it is more intereseting where clusters where built this is why I have chosen the idea of an heatmap:\n\nheatmap, yedges, xedges = np.histogram2d(y, x, bins=(10,10))\nextent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]\n\n\ngenerates the following plot\n\n\n\nwhich is quite alright. However I'm not sure what this color even indicates but it's not the amount of datapoints between a certain range (e.g. 4&gt;x&gt;5 &amp; 11&gt;y&gt;12).\n\nThe question\n\nI know I could write a program that merges appropriate datapoints, count the instances for a cell and draw the desired heatmap by my own, but isn't there already an implementation of such a neat tool in data science?\n"
'I have the following dataframe having "Location" and "Name" as the index.\n\nLocation  Name   Cost   Item Purchased                \nStore 1   Chris  22.5   Dog Food\n          Kevyn   2.5   Kitty Litter\nStore 2   Vinod   5.0   Bird Seed\n\n\nI can access df.loc["Store 1"]\nBut df.loc["Store 1"]["Kevyn"] is giving me KeyError. What am I doing wrong?\n'
'I am dealing with a data frame with 6 columns, Below is the example df\n\n     a   b   c   d   e   f\n     1   2   3   4   5   6\n     7   8   9   10  11  12\n\n\nFollowing is the new data frame which I expect:\n\n     col1   col2   col3\n     1      2      3\n     4      5      6\n     7      8      9\n     10     11     12\n\n\nPlease note the order of the row elements, The first row from the original df, becomes the first two rows of the new df, the second row from the original df becomes the next two.\n\nPlease advice me to achieve the required new df.\nThanks in advance.\n'
'I am trying to read in the complete Titanic dataset, which can be found here:\n\nbiostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls\n\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_excel(\'titanic3.xls\')\ny = dataset.iloc[:, 1].values\nx = dataset.iloc[:, 2:14].values\n\n# Create Dataset for Men\nmen_on_board = dataset[dataset[\'sex\'] == \'male\']\nmale_fatalities = men_on_board[men_on_board[\'survived\'] ==0]\nX_male = male_fatalities.iloc[:, [4,8]].values\n\n# Taking care of missing data\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = \'NaN\', strategy = \'mean\', axis = 0)\nimputer = imputer.fit(X_male[:,0])\nX_male[:,0] = imputer.transform(X_male[:,0])\n\n\nWhen I run all but the last line, I get the following warning:\n\n/Users/&lt;username&gt;/anaconda/lib/python3.6/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n  DeprecationWarning)\n\n\nWhen I run the last line, it throws the following error:\n\nFile "&lt;ipython-input-2-07afef05ee1c&gt;", line 1, in &lt;module&gt;\n   X_male[:,0] = imputer.transform(X_male[:,0])\n   ValueError: could not broadcast input array from shape (523) into shape (682)\n\n\nI\'ve used the above code snippet for imputation on other projects, not sure why it\'s not working.\n'
'I\'m working on a project that involves having to work with preprocessed data in the following form.\n\n\n\nData explanation has been given above too. The goal is to predict whether a written digit matches the audio of said digit or not. First I transform the spoken arrays of form (N,13) to the means over the time axis as such:\n\n\n\nThis creates a consistent length of (1,13) for every array within spoken. In order to test this in a simple vanilla algorithm I zip the two arrays together such that we create an array of the form (45000, 2), when I insert this into the fit function of the LogisticRegression class it throws me the following error:\n\n\nWhat am I doing wrong?\n\nCode:\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nmatch = np.load("/srv/digits/match_train.npy")\nspoken = np.load("/srv/digits/spoken_train.npy")\nwritten = np.load("/srv/digits/written_train.npy")\nprint(match.shape, spoken.shape, written.shape)\nprint(spoken[0].shape, spoken[1].shape)\n\ndef features(signal, function):\n    new = np.copy(signal)\n    for i in range(len(signal)):\n        new[i] = function(new[i], axis=0)\n    return new\n\nspokenMean = features(spoken, np.mean)\nprint(spokenMean.shape, spokenMean[0])\n\nresult = np.array(list(zip(spokenMean,written)))\nprint(result.shape)\n\nX_train, X_val, y_train, y_val = train_test_split(result, match, test_size = \n0.33, random_state = 123)\nmodel = LogisticRegression()\nprint(X_train.shape, y_train.shape)\nmodel.fit(X_train, y_train)\nyh_val = model.predict(X_val)\n\n'
"I have OHLC data. The candle can be either 'green' (if the close is above open) or 'red' (if the open is above the close). The format is:\n\n  open close candletype\n0  542 543 GREEN\n1  543 544 GREEN \n2  544 545 GREEN\n3  545 546 GREEN\n4  546 547 GREEN\n5  547 542 RED\n6  542 543 GREEN\n\n\nWhat I would like is to count the number of consecutive green or red candles for n-previous rows. Lets say I want to identify rows preceded by 3 green candles.\n\nThat the desired output would be:\n\n  open close candletype  pattern\n0  542 543 GREEN  Toofewrows\n1  543 544 GREEN  Toofewrows\n2  544 545 GREEN  Toofewrows\n3  545 546 GREEN  3-GREEN-CANDLES-IN-A-ROW\n4  546 547 GREEN  3-GREEN-CANDLES-IN-A-ROW\n5  547 542 RED    3-GREEN-CANDLES-IN-A-ROW\n6  542 543 GREEN  No pattern\n\n\nI know how to get the solution by extracting the row number, applying a custom function to candletype series with that row number and looking at n previous rows within that custom function, creating a n-item list and checking for isAll('GREEN') but I WAS WONDERING IF THERE IS AN ELEGANT ONE LINER APPLY SOLUTION?\n"
'I am new to pandas and am struggling with to rename a column and then extracting the same.\n\nI have read an xls file into a pandas data frame object.\n\ndf = pd.read_excel("something.xls")\nbank_statement.columns.values[0] = \'Din\'\nbank_statement.columns\n\n\nThis showed the columns\n\nIndex([u\'Din\', u\'Unnamed: 1\', u\'Unnamed: 2\', u\'Unnamed: 3\', u\'Unnamed: 4\',\n       u\'Unnamed: 5\', u\'Unnamed: 6\'],\n      dtype=\'object\')\n\n\nBut this causes error.\n\nbank_statement.Din\n\n\nThe error is:\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-11-6ce73c262cd1&gt; in &lt;module&gt;()\n----&gt; 1 bank_statement.Din\n\n/Users/monideepde/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc in __getattr__(self, name)\n   3612             if name in self._info_axis:\n   3613                 return self[name]\n-&gt; 3614             return object.__getattribute__(self, name)\n   3615 \n   3616     def __setattr__(self, name, value):\n\nAttributeError: \'DataFrame\' object has no attribute \'Din\'\n\n\nContrary to this, when I had tried to do the same for a column that was named when it was imported, I did not face any issue.\n\ndata = pd.read_csv("/somepath/TestFrame.csv")\ndata\n\n\n\n\nI could access the columns\n\n\n\nCan anyone point out where I am going wrong?\n\nThanks\n'
"I want to aggregate a pandas DataFrame with following function f. The original DataFrame df has many columns and I want to exract only few of them to a new DataFrame\n\nI can not understand why I have to return a Series. I would think that I need to return a DataFrame because the output is multidimensional.\n\ndef f(x):\n    return Series(dict(Number_of_tweets = x['content'].count(),\n                       Company=x['Company'].min(),\n                       Description=x['from_user_description'].min(),\n                      ))\n\naccount_count = df.groupby('from_user_screen_name').apply(f)\nprint(len(account_count))\naccount_count\n\n"
'In below example i have data of movies :\n\nI want to split the title column values into 2 new column i.e one of the new column ( " movie title ") will take = toy story as row value and another new column ("year) will take =1995 as row value.\n\nhow to perform this operation on whole dataframe?\n\n                            title  \\\n0                    Toy Story (1995)   \n1                      Jumanji (1995)   \n2             Grumpier Old Men (1995)   \n3            Waiting to Exhale (1995)   \n4  Father of the Bride Part II (1995) \n\n'
"I struggle to implement the moving average formula in my function.\nTook me quite a while to get where the code is right now. \n\nIs there a library I could probably take?\n\nInput:\n\nma([2,3,4,3,2,6,9,3,2,1], 4)\n\n\nExpected Output:\n\n[None, None, None, 3.0, 3.0, 3.75, 5.0, 5.0, 5.0, 3.75]\n\n\nMy Output:\n\n[None, None, 0.0, 3.0, 3.75, 5.0, 5.0, None, None, None]\n\n\nI am running into the problem that the middle parts of my result is right but the rest is a mystery.\n\n\nWhy does it return None for the last three values in the list?\n\n\ndef ma(prices, n):\n\nma = []\nsums = []\ns = 0\nave = 0\n\nfor idx, i in enumerate(prices):\n    s += i\n    sums.append(s)\n    print('idx: ' + str(idx))\n    print('list of sums ' + str(sums))\n    #print('sum ' + str(s))\n\nif i &gt;= n+1:\n    print('sums[idx] ' + str(sums[idx]))\n    print('sums[idx-n] ' + str(sums[idx-n]))\n    ave = (sums[idx] - sums[idx-n]) / n\n    print('ave ' + str(ave))\n    ma.append(ave)\n    print('ma ' + str(ma))\nelse:\n    m = None\n    ma.append(m)\n    print('ma ' + str(ma))\n\n\n(Sorry for all those print function calls, but I really wanted to get to the source of the issue).\n"
"I am attempting to understand some of the example code from machinelearningmastery.com but the slice notation is throwing me off... For starters with my code I am attempting to make a simple regression type ML algorithm with some data from a CSV file:\n\nimport pandas as pd\n\nMcheLrn = pd.read_csv('C:/Users/data.csv', index_col='Date', parse_dates=True)\n\n\nMcheLrn['month'] = McheLrn.index.month\nMcheLrn['date'] = McheLrn.index.strftime('%d')\nMcheLrn['hour'] = McheLrn.index.strftime('%H')\nMcheLrn['Day_of_week'] = McheLrn.index.dayofweek\n\nMcheLrn.head()\n\n\nThis will output:\n\n    OSAT    kWh month   date    hour    Day_of_week\nDate                        \n2013-01-01 06:00:00 10.4    16.55   1   01  06  1\n2013-01-01 06:15:00 10.4    16.55   1   01  06  1\n2013-01-01 06:30:00 10.4    16.05   1   01  06  1\n2013-01-01 06:35:00 10.4    16.05   1   01  06  1\n2013-01-01 06:45:00 10.4    17.20   1   01  06  1\n\n\nNot sure if I am using the correct terminology, but the dependent variable is kWh (Y variable) and all other are independent variables for my X variables...\n\nWith the code below what is throwing me off is the slice notation X = array[:,0:2] Y = array[:,2] And I am not sure if I have my X &amp; Y variables correctly selected.\n\n# Decision Tree Regression\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\n\n\ndataframe = McheLrn\narray = dataframe.values\n\nX = array[:,0:2]\nY = array[:,2]\n\nseed = 7\nkfold = model_selection.KFold(n_splits=90, random_state=seed)\nmodel = DecisionTreeRegressor()\nscoring = 'neg_mean_squared_error'\nresults = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n\nprint(results.mean())\n\n"
'I am trying to use a deep learning model for time series prediction, and before passing the data to the model I want to scale the different variables as they have widely different ranges.\n\nI have normally done this "on the fly": load the training subset of the data set, obtain the scaler from the whole subset, store it and then load it when I want to use it for testing. \n\nNow the data is pretty big and I will not load all the training data at once for training. \n\nHow could I go to obtain the scaler? A priori I thought of doing a one-time operation of loading all the data just to calculate the scaler (normally I use the sklearn scalers, like StandardScaler), and then load it when I do my training process.\n\nIs this a common practice? If it is, how would you do if you add data to the training dataset? can scalers be combined to avoid that one-time operation and just "update" the scaler?\n'
'This is my data frame looks like:\n\nday temperature windspeed event\n0 1/1/2017 32 6 Rain\n1 1/2/2017 35 7 Sunny\n2 1/3/2017 28 2 Snow\n3 1/4/2017 24 7 Snow\n4 1/5/2017 32 4 Rain\n5 1/6/2017 31 2 Sunny\n\n\nAs per my understanding i have to take the data frame and a list of index :\n\nindex = [10 ,20,30,40,50,60]\ndf = pd.DataFrame(df , index)\ndf\n\n\nThis code is printing some rows Nan which i dont want ..\n\ni want my result should be ;\n\n10 1/1/2017 32 6 Rain\n20 1/2/2017 35 7 Sunny\n30 1/3/2017 28 2 Snow\n40 1/4/2017 24 7 Snow\n50 1/5/2017 32 4 Rain\n60 1/6/2017 31 2 Sunny\n\n'
'I have been working with relation extraction for a week. But what I need is direction between two entities, such as Company_x got bought by Company_y. So the model should predict the entities like Company_y->bought-> Company_X. Any models you guys think will be helpful for this?\n'
'for L,M in laundry1[\'latitude\'],laundry1[\'longitude\']:\n    print(\'latitude:-\')\n    print(L)\n    print(\'longitude:-\')\n    print(M)\n\n\ni am trying to iterate over the two columns of a data-frame, assigning there value to L &amp; M and printing there value but it shows error of "too many values to unpack (expected 2) " view of the dataset with error view ->enter image description here\n\nsample output:\n\nlatitude:-\n\n22.1449787\n\n18.922290399999998\n\n22.1544736\n\n22.136872\n\n22.173595499999998\n\nlongitude:-\n\n-101.0056829\n\n-99.234332\n\n-100.98580909999998\n\n-100.9345736\n\n-100.9946027\n'
"I'm a beginner in data science and I'm trying to draw a linear plot with matplotlib (and other packages like pandas and numpy) with the boston dataset.  I am asked to draw it from the [0,1] interval.  I am asked to plot the price of homes (TARGET, which is the y-axis) against the crime rate in the area (CRIM, which is the x axis).  My goal is to use Matplotlib's histogram but I couldn't find how to do that for a categorical dataset like this.  By now, this is the code I have:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\ndataFrame_x = pd.DataFrame(boston.data, columns = boston.feature_names)\ndataFrame_y = pd.DataFrame(boston.target)\n\ndataFrame_x.describe()\n\n\nAnd this is the data description (it looks too crummy on SO. I apologize for that):\n\n    CRIM    ZN  INDUS   CHAS    NOX RM  AGE DIS RAD TAX PTRATIO B   LSTAT\ncount   506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000\nmean    3.613524    11.363636   11.136779   0.069170    0.554695    6.284634    68.574901   3.795043    9.549407    408.237154  18.455534   356.674032  12.653063\nstd 8.601545    23.322453   6.860353    0.253994    0.115878    0.702617    28.148861   2.105710    8.707259    168.537116  2.164946    91.294864   7.141062\nmin 0.006320    0.000000    0.460000    0.000000    0.385000    3.561000    2.900000    1.129600    1.000000    187.000000  12.600000   0.320000    1.730000\n25% 0.082045    0.000000    5.190000    0.000000    0.449000    5.885500    45.025000   2.100175    4.000000    279.000000  17.400000   375.377500  6.950000\n50% 0.256510    0.000000    9.690000    0.000000    0.538000    6.208500    77.500000   3.207450    5.000000    330.000000  19.050000   391.440000  11.360000\n75% 3.677083    12.500000   18.100000   0.000000    0.624000    6.623500    94.075000   5.188425    24.000000   666.000000  20.200000   396.225000  16.955000\nmax 88.976200   100.000000  27.740000   1.000000    0.871000    8.780000    100.000000  12.126500   24.000000   711.000000  22.000000   396.900000  37.970000\n\n"
"import pandas as pd \n\ndfa = {'account':['a','b','a','c','a'],\n      'ret_type':['CTR','WO','T','CTR','T'],\n      'val':['0.0','0.1','0.2','0.3','0.4'],\n      'ins_date':['11','12','11','13','14']}\ndf = pd.DataFrame(dfa)\n\n    account ret_type     val    ins_date\n0     a       CTR        0.0    11\n1     b       WO         0.1    12\n2     a       T          0.2    11\n3     c       CTR        0.3    13\n4     a       T          0.4    14\n\n\nI have a requirement that I need to eliminate the duplicate row such that\n\n1 duplicate row means combination of (account,ins_dat) \n2 if duplicate found i need to keep row with ret type CTR abd drop row with T\n3 i dont want to delete T rows for which no duplicate row is there like 4\n4 in this example fr ex 2nd row is deleted as output finally\n\n\nhow should i do this?\n"
"I have a dictionary where key is model name and values are keywords. I want to filter every row in a column that string contains one of the keywords that are in the values of dictionary.\nMatching should be case insensitive. \n\nDictionary looks like this:\n\n{'J7 2017': [' J730F', 'amoled'], 'J5 2017': ['J530', 'TFT']} \n\n\ndata frame looks like:\n\n           name  \n0  SCREEN SAMSUNG FULL AMOLED  \n1  SCREEN SAMSUNG J7 J730F 2017\n2  WYŚWIETLACZ LCD + DIGITIZER SAMSUNG J5 2017 (J530)\n3  3 colors SCREEN LCD SAMSUNG Galaxy J5 TFT\n4  LG K10 K410 K420N K430\n\n\nAs a result i want to have model name [key] stored in separate data frame with with all rows that had my keyword\n\nso the output would be:\n\ndfJ72017:\n\n         name  \n0  SCREEN SAMSUNG FULL AMOLED  \n1  SCREEN SAMSUNG J7 J730F 2017\n\ndfJ52017:\n             name  \n    0  WYŚWIETLACZ LCD + DIGITIZER SAMSUNG J5 2017 (J530) \n    1  3 colors SCREEN LCD SAMSUNG Galaxy J5 TFT\n\n\nAnd do it for all keys and values in dictionary.\n"
"I have a Data Frame with some values.\n\nSuppose that does are the values of some stores, and does stores can fulfill some conditions and give them more than one 'state', but other stores can only fulfill one condition and be assigned with only one 'state'.\n\nFor example:\n\ndf = DataFrame({'one':[1,2,3,4],\n                'two';[5,6,7,8],\n                'three':[9,10,11,12]}\n\n\nand these are my conditions:\n\ndf.loc[(df.one &gt;= 1) &amp; (df.two &lt;= 7),'State'] = 1\ndf.loc[(df.one == 1) &amp; (df.two &lt;= 11),'State'] = 2\n\n\nThree rows satisfy the first condition, but only one row satisfy the second condition.\n\nThe row that satisfy the two conditions should have in the column 'State', the state 1 and 2.\n\nThe obvious problem is that when the first condition gets assign the DataFrame looks like this:\n\n    one two three   State\n0   1   5   9        1.0\n1   2   6   10       1.0\n2   3   7   11       1.0\n3   4   8   12       NaN\n\n\nand when the second condition gets assign the Data Frame looks like this:\n\n    one two three   State\n0   1   5   9        2.0\n1   2   6   10       1.0\n2   3   7   11       1.0\n3   4   8   12       NaN\n\n\nand I want something like this:\n\n    one two three   State\n0   1   5   9        [1.0,2.0]\n1   2   6   10       1.0\n2   3   7   11       1.0\n3   4   8   12       NaN\n\n\nHere I used a list, but that is the idea.\n\nAnd then If I do the store in a cell, how I call them, and how I use does cells with more than one value in the column 'State' for other conditions that depend in that column?\n\nI appreciate it\n"
"I am doing a triple for loop on a dataframe with almost 70 thousand entries. How do I optimize it?\n\nMy ultimate goal is to create a new column that has the country of a seismic event. I have a latitude, longitude and 'place' (ex: '17km N of North Nenana, Alaska') column. I tried to reverse geocode, but with 68,488 entries, there is no free service that lets me do that. And as a student, I cannot afford it. \n\nSo I am using a dataframe with a list of countries and a dataframe with a list of states to compare to USGS['place']'s values. To do that, I ultimately settled on using 3 for loops.\n\nAs you can assume, it takes a long time. I was hoping there is a way to speed things up. I am using python, but I use r as well. The for loops just run better on python.\n\nAny better options I'll take.\n\nUSGS = pd.DataFrame(data = {'latitide':[64.7385, 61.116], 'longitude':[-149.136, -138.655], 'place':['17km N of North Nenana, Alaska', '74km WNW of Haines Junction, Canada'], 'country':[NA, NA]})\n\nstates = pd.DataFrame(data = {'state':['AK', 'AL'], 'name':['Alaska', 'Alabama']})\n\ncountries = pd.DataFrame(data = {'country':['Afghanistan', 'Canada']})\n\nfor head in states:\n    for state in states[head]:\n        for p in USGS['place']:\n            if state in p:\n                USGS['country'] = USGS['country'].map({p : 'United 'States'})\n\n# I have not finished the code for the countries dataframe\n\n"
'I am building a transformer in sklearn which drops features that have a correlation coefficient lower than a specified threshold.\n\nIt works on the training set. However, when I transform the test set. All features on the test set disappear. I assume the transformer is calculating correlations between test data and training label and since those are all low, it is dropping all features. How do I make it only calculate correlations on the training set and drop those features from the test set on the transform?\n\nclass CorrelatedFeatures(BaseEstimator, TransformerMixin): #Selects only features that have a correlation coefficient higher than threshold with the response label\n    def __init__(self, response, threshold=0.1):\n        self.threshold = threshold\n        self.response = response\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        df = pd.concat([X, self.response], axis=1)\n        cols = df.columns[abs(df.corr()[df.columns[-1]]) &gt; self.threshold].drop(self.response.columns)\n        return X[cols]\n\n'
'I have two dataframes as df1 and df2.Both have the same column name as \'Accounts\'.\nI can currently access this data for comparison using the following code:\ndf1.account.isin(df2.account.values)\n\nI would like \'account\' to be accessed as a variable something like this.df1.[account].isin(df2.[account].values)After research I have discovered a possible solution as:\ndf1.loc[:, \'account\'] (I suspect this is not the correct approach.)\n\nFrom this point, I\'m not sure how to access the isin() method\nAs a result I welcome your wisdom with any alternative ways to accomplish this. Your help is very much appreciated :)\nThe full block of code is as follows:\n\nslgCSV = \'c:\\\\automation\\\\python\\\\a.csv\'\narmyCSV = \'c:\\\\automation\\\\python\\\\b.csv\'\ndf1 = pd.read_csv(slgCSV)\ndf2 = pd.read_csv(armyCSV)\nd3 = {\'Expected\': [], \'Actual\': []}\ndf3 = pd.DataFrame(data=d3)\nmatch1 = df1.account.isin(df2.account.values)\nmatch2 = df2.account.isin(df1.account.values)\nfor r1 in df1[match1].index:\n    for r2 in df2[match2].index:\n        # print("R2: " + str(r2))\n        if df1.account[r1] == df2.account[r2]:\n            idx = df1.account[r1]\n            row = {\'Expected Row ID\': r1+2, \'Actual Row ID\': r2+2}\n            print("Output: " + str(row) + ": " + str(idx))\n\n\ndf1 looks as follows:\n\nAccount\n1\n2\n3\n4\n5\n\n\ndf2 looks as follows:\n\nAccount\n3\n1\n5\n2\n4\n\n\n\nThe solution is as follows:\ncol = "account"\ndf1[col].isin(df2[col].values)\nThank you for all the help!\n'
'I have one csv file with multiple Simulations delimited by a Simulation (Index) entry. Each entry has a time line and 3 feature lines. Basically the first column just had Simulation (Index) entries and nothing else while the second column has the "header" of that simulation (Time + Features 1,n) and then only numerical values.\n\nI want to to contain this in some data frames or some sort of numpy arrays in order to plot the the graphs for each simulation and obviously to have a better grip over the data.\n\nAs someone who is fairly new to these sorts of challenges I resorted to pandas for a quick solution but I am also open to any python (numpy/other libraries) implementation.\n\nExample of Data Format:\n\n\n\nThank you\n'
'I am working on showing congestion through a freeway system.\n\nHaving difficulties so added a Bounty.\n\nI have created an example I am working through with JS bin here ( https://jsbin.com/lebeqam/edit?html,js,output )\n\nI have created a leaflet map with the detectors lat long and id numbers.\n\nI also have a csv with occupancy data, a important traffic value, over time for each detector.\n\nI am wondering how I should go about creating a heatmap with this data that shows on the map. I would like to be able to change the time and even play the time forward or backward to get an understanding of congestion and how to stop it at it\'s root.\n\nThis is html and jscript for the page currently with some parts removed\n\n    &lt;div id="mapid" style="height:1858px; border-right: 1px solid #d7d7d7; position: fixed; top: 0px;width: 67%;z-index: 0;cursor: -webkit-grab;cursor: -moz-grab;background: #fff;\n    color: #404040;color: rgba(0,0,0,.75);outline: 0;overflow: hidden;-ms-touch-action: none;\n    box-sizing: border-box;"&gt;&lt;/div&gt;\n\n  &lt;/div&gt;\n\n&lt;script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"&gt;&lt;/script&gt;\n&lt;script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js"\n        integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy"\n        crossorigin="anonymous"&gt;&lt;/script&gt;\n\n&lt;script src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"&gt;&lt;/script&gt;\n\n&lt;/body&gt;\n\n\n&lt;script&gt;\n\nvar myIcon = L.divIcon({\n    html: \'&lt;i class="fas fa-map-pin"&gt;&lt;/i&gt;\',\n    iconSize: [20, 20],\n    className: \'dummy\' // We don\'t want to use the default class\n});\n\n    var mymap = L.map(\'mapid\').setView([-37.735018, 144.894947], 13);\n\n    L.tileLayer(\'https://api.tiles.mapbox.com/v4/{id}/{z}/{x}/{y}.png?access_token=pk.eyJ1IjoibWFwYm94IiwiYSI6ImNpejY4NXVycTA2emYycXBndHRqcmZ3N3gifQ.rJcFIG214AriISLbB6B5aw\', {\n        maxZoom: 18,\n        attribution: \'Map data &amp;copy; &lt;a href="https://www.openstreetmap.org/"&gt;OpenStreetMap&lt;/a&gt; contributors, \' +\n            \'&lt;a href="https://creativecommons.org/licenses/by-sa/2.0/"&gt;CC-BY-SA&lt;/a&gt;, \' +\n            \'Imagery © &lt;a href="https://www.mapbox.com/"&gt;Mapbox&lt;/a&gt;\',\n        id: \'mapbox.streets\'\n    }).addTo(mymap);\n\n    L.geoJSON(myGeojsonData, {\n    pointToLayer: function (getJsonPoint, latlng) {\n        return L.marker(latlng, { icon: myIcon });\n    }\n}).bindPopup(function(layer) {\n    return \'ID #: \' + layer.feature.properties.IDnumber + \'&lt;br /&gt;Area: \' + layer.feature.properties.Area;\n}).addTo(mymap);\n\n    var circle = L.circle([-37.735018, 144.894947], {\n    color: \'red\',\n    fillColor: \'#f03\',\n    fillOpacity: 0.5,\n    radius: 50\n}).addTo(mymap);\n\n&lt;/script&gt;\n\n\nThis is part of the geoJson (the entire file is huge but you\'ll get the picture)\n\nvar myGeojsonData =\n{\n  "features": [\n    {\n      "geometry": {\n        "coordinates": [\n          144.829434,\n          -37.825233\n        ],\n        "type": "Point"\n      },\n      "properties": {\n        "Area": "Combined Entry MVT on Grieve Pde, West Gate Fwy North Ramps, Grieve Pde Byp Start EB between Grieve ",\n        "IDnumber": "2541EL_P0"\n      },\n      "type": "Feature"\n    },...etc\n\n\nAnd this is the CSV with the traffic data (also only a part of it for the sake of space.)\n\n\n\nI have tried to simplify this a bit by using these two json files to just get going (the time series file is in the jsbin as it\'s too large for stackoverflow.\n\nvar myGeojsonData =\n{\n  "features": [\n    {\n      "geometry": {\n        "coordinates": [\n          144.827465,\n          -37.82572\n        ],\n        "type": "Point"\n      },\n      "properties": {\n        "Area": "Freeway MVT on West Gate Fwy WB between Grieve Pde Off Ramp (ob) &amp; Split To PFW and WRR",\n        "IDnumber": "7859OB_L_P0"\n      },\n      "type": "Feature"\n    },\n    ],\n  "type": "FeatureCollection"\n}\n;\n\n\nIf anyone could show me how they would approach this that would be brilliant. \n\nThank you,\n'
"I am trying to make a dataframe using two arrays that are listing average values. The other two sets of values I would like to incorporate are extracted from another dataset. \n\nThis is what I have tried:\n\ncolumn_a = data_set['column_1']\ncolumn_b = data_set['column_2']\ndf = pd.DataFrame([column_a, column_b, average_1, average_2])\nprint(df)\n\n\nThe output was strangely formatted:\n\n                                     0            1            2   \ncolumn_a                  North Europe  East Europe  West Europe   \naverage_1                          143      100.755        175.1   \naverage_2                        297.9       171.21       227.55   \ncolumn_b                        79.265      60.8078       76.468\n\n"
'For instance, here is the dataframe\n\n1     2\nhigh  wide\nhigh  thin\nshort wide\nshort thin\n\n\nI want to select all the rows that contain value \'wide\'. And let\'s suppose that I do not know, what column contains such values. \n\nThis is just a simplified example. I need it in order to iterate through the unique table values, that are organized into an array. \n\nfor uv in uniqueValues:\n    valueObjects = """select all the rows that contain uv"""\n\n\nIs there a way to do this without determining a column for every value?\nI mean, this could work\n\nfor uv in uniqueValues:\n    for col in table.columns:\n        colValues = table[col].unique()\n        if uv in colValues:\n            valueObjects = table[table[col] == uv]\n            break\n\n\nBut I hope there is a more elegant solution.\n'
"I have a crime data set, I already calculated the crimes committed in each location. Now I want to create a new column that is the crime rate for that specific row. I already calculated the crime rate now I want to match the specific crime rate to correct row matching the same latitude value \n\nHere I have a loop that creates the crime rate per location. but now i want to get the crime rate value create a new column that matches the latitude in my for loop with my dataframe and adds to corresponding crime rate in each individual row\n\nz = ['lat'] \nfor i in z:\n    print((df[i].value_counts()/1250000)*100000)\n32.715973    112.56\n32.715738     90.32\n32.706341     83.28\n32.545300     79.52\n32.745903     78.32\n32.769389     65.52\n32.809860     63.44\n32.706287     63.04\n32.591684     55.68\n32.764136     55.44\n32.749983     52.16\n32.545291     49.04\n32.712584     47.20\n32.746868     46.32\n32.796864     44.40\n32.706287     43.76\n32.768120     42.64\n32.794497     41.52\n32.703369     40.80\n32.714797     40.40\n32.716977     39.44\n32.738989     39.04\n32.755182     37.28\n32.957955     35.52\n32.759375     35.28\n32.565237     34.72\n32.739964     34.08\n32.767116     34.00\n32.877050     32.24\n32.706559     32.24\n\n\n"
'I have this pic:\n\n\nI have the following relative coordinates:\n\n[[0.6625, 0.6035714285714285], [0.7224999999999999, 0.6035714285714285], [0.7224999999999999, 0.6571428571428571], [0.6625, 0.6571428571428571], [0.6625, 0.6035714285714285]]\n\n\n(however I don\'t understand, why are here 5 values instead of usual 4 and what they mean)\n\nMy attempt with scikit-image that shows whole pic instead of cropping:\n\nimport numpy as np\nfrom skimage import io, draw\n\nimg = io.imread(pic)\n\nvals = [[0.6625, 0.6035714285714285], [0.7224999999999999, 0.6035714285714285], [0.7224999999999999, 0.6571428571428571], [0.6625, 0.6571428571428571], [0.6625, 0.6035714285714285]]\n\nvertices = np.asarray(test_vals)\n\nrows, cols = draw.polygon(vertices[:, 0], vertices[:, 1])\n\ncrop = img.copy()\n\ncrop[:, :, -1] = 0\ncrop[rows, cols, -1] = 255\n\nio.imshow(crop)\nio.show()\n\n# shows whole pic instead of cropping\n\n\nMy attempt with opencv gives errors because coordinates are in float format:\n\nimport cv2 as cv\n\n\nvals = [[0.6625, 0.6035714285714285], [0.7224999999999999, 0.6035714285714285], [0.7224999999999999, 0.6571428571428571], [0.6625, 0.6571428571428571], [0.6625, 0.6035714285714285]]\n\nx = vals[0][0]\ny = vals[0][1]\nwidth = vals[1][0] - x\nheight = vals[2][1] - y\n\nimg = cv.imread(pic)\n\ncrop_img = img[y:y+height, x:x+width]\ncv.imshow("cropped", crop_img)\ncv.waitKey(0)\n\n#  TypeError: slice indices must be integers or None or have an __index__ method\n\n\nHow to crop car number on this pic given its relative bbox coordinates?\n\nI am not limited to any framework, so if you think that TF or anything else might help - please suggest.\n'
"I have DataFrame which looks like this - \n\n  37 days 19:07:00\n  69 days 02:32:00\n  44 days 00:38:00\n  14 days 07:30:00\n  14 days 23:03:00\n  41 days 16:41:00\n\n\nI want to convert this time into months for which I am using this code - \n\ndf_new['difference'] = df_new['time'].dt.total_seconds().div(60).astype(int)\n\n\nThis is giving me results but for days less then 30 it is returning me 0 months so I want a decimal value of this column instead of absolute number. How can I do that? My ultimate goal is to find mean of this column.\n\nMy current output looks like column one and my desired output is in column 2. \n\nCurrent \n1\n2\n2\n5\n0\n0\n\nDesired \n1\n2\n2\n5\n0.2\n0.5\n\n"
"I have a Pandas Series I created from answers to survey questions. For example\n\nAge      q1       \n10+      No           16\n         No Answer     1\n         Yes           8\n5-7      No           20\n         No Answer     1\n         Yes          22\n7-9      No           18\n         No Answer     1\n         Yes          16\nUnder 5  No           11\n\n\nNow I want to create a seaborn bar chart for this. In the bar chart I want the 'Age' to be x axis and each Age bucket should show the count for each answer in a different color as shown in here.\n\nseaborn bar chart\n"
'I am working on skin cancer classification, I have "GroundTruth.csv" file and "training data" as jpg images, the csv file in one hot encoding format and I want to put the images into folders with the columns as folders name.\n\nthis image clear the meaning\n\nhttps://user-images.githubusercontent.com/45392637/57570855-0a6cb400-7407-11e9-8eb3-adb7b1bd70b6.JPG\n\n# Create new folders in the training directory for each of the classes\nnv = os.path.join(train_dir, \'nv\')\nos.mkdir(nv)\nmel = os.path.join(train_dir, \'mel\')\nos.mkdir(mel)\nbkl = os.path.join(train_dir, \'bkl\')\nos.mkdir(bkl)\nbcc = os.path.join(train_dir, \'bcc\')\nos.mkdir(bcc)\nakiec = os.path.join(train_dir, \'akiec\')\nos.mkdir(akiec)\nvasc = os.path.join(train_dir, \'vasc\')\nos.mkdir(vasc)\ndf = os.path.join(train_dir, \'df\')\nos.mkdir(df)\n\n# Create new folders in the validation directory for each of the classes\nnv = os.path.join(val_dir, \'nv\')\nos.mkdir(nv)\nmel = os.path.join(val_dir, \'mel\')\nos.mkdir(mel)\nbkl = os.path.join(val_dir, \'bkl\')\nos.mkdir(bkl)\nbcc = os.path.join(val_dir, \'bcc\')\nos.mkdir(bcc)\nakiec = os.path.join(val_dir, \'akiec\')\nos.mkdir(akiec)\nvasc = os.path.join(val_dir, \'vasc\')\nos.mkdir(vasc)\ndf = os.path.join(val_dir, \'df\')\nos.mkdir(df)\n\n\nI want to put the images into the folders as my next step.\n'
"df = pd.DataFrame({'a':np.random.randn(1000), 'b':np.random.randn(1000), 'c':np.random.randn(1000), 'd':np.random.randn(1000),\n                  'e':np.random.randn(1000), 'f':np.random.randn(1000), 'g':np.random.randn(1000), 'h':np.random.randn(1000),\n                  'i':np.random.randn(1000), 'j':np.random.randn(1000), 'k':np.random.randn(1000), 'l':np.random.randn(1000)})\n\n\nI have a Dataframe like this with (a lot of)  columns.\n\ncol_of_interest = ['a','f', 'j', 'k', 'c']\n\n\nI want to choose only a subset which is saved in a list.\n\n%timeit df.loc[:, df.columns.isin(col_of_interest)]\n%timeit df[col_of_interest]\n%timeit df[[c for c in df.columns if c in col_of_interest]]\n%timeit df[np.intersect1d(df.columns, col_of_interest)]\n%timeit df[df.columns &amp; col_of_interest]\n\n\n803 µs ± 289 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n1.92 ms ± 324 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n2.18 ms ± 406 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n2.53 ms ± 194 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n3.39 ms ± 34.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nI was a little surprised that df[col_of_interest] is not the best solution and was interested if there is an even better/efficient way than df.loc[:, df.columns.isin(col_of_interest)].\n"
'I Am new in Data Science. I am trying to find out the feature importance ranking for my dataset. I already applied Random forest and got the output.\n\nHere is my code:\n\n# importing libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# importing dataset\n\ndataset=pd.read_csv(\'Churn_Modelling.csv\')\nX = dataset.iloc[:,3:12].values\nY = dataset.iloc[:,13].values\n\n#encoding catagorical data\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#country\nlabelencoder_X_1= LabelEncoder()\nX[:,1]=labelencoder_X_1.fit_transform(X[:,1])\n\n#gender\nlabelencoder_X_2= LabelEncoder()\nX[:,2]=labelencoder_X_2.fit_transform(X[:,2])\n\nonehotencoder = OneHotEncoder(categorical_features=[0])\nX = onehotencoder.fit_transform(X).toarray()\n\n\n#spliting dataset into test set and train set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20)\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=20, random_state=0)  \nregressor.fit(X_train, y_train) \n\n\nIn the importance part i almost copied the example shown in :\nhttps://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n\nHere is the code:\n\n#feature importance\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimportances = regressor.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in regressor.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\nprint("Feature ranking:")\n\nfor f in range(X.shape[1]):\n    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title("Feature importances")\nplt.bar(range(X.shape[1]), importances[indices],\n       color="r", yerr=std[indices], align="center")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()\n\n\n\nI am expecting the output shown in the documentation. Can Anyone Help me please ? Thanks in Advance.\n\nMy dataset is here:\n\n'
"I have a data frame like this:\n\nA1 A2 A3 ...A99 largest\n0   3  4  6      11   11\n1   1  8  2  ...  1    8\n.\n.\n.\n\n\nI created the column which contains the largest value in each row by using:\n\ndata['largest']=data.max(axis=1)\n\n\nbut I also want to get a column which contains the corresponding column name with the largest number, something like this:\n\n    A1 A2 A3 ...A99 largest name\n0   3  4  6      11   11    A99\n1   1  8  2  ...  1    8    A2\n.                            .\n.                            .\n.                            .\n\n\nI tried '.idxmax' but gave me an error'reduction operation 'argmax' not allowed for this dtype', can someone help me? Many thanks.\n"
'I have a dataframe, where I have  a ticket IDs and subsequent actions taken with the date time column a shown in the table below.\n\nticketID    ChangeDate  OldStatus   NewStatus\n0   1012327 2019-03-18 09:00:32.903 R or O  Action mail sent to client\n1   1012327 2019-03-18 09:21:34.820 Action mail sent to client  Response Client - R\n2   1012327 2019-03-18 09:34:21.890 Response Client - R Status Updated\n3   1012328 2019-03-18 07:00:09.960 R or O  ticket Closed - None\n4   1012328 2019-03-18 07:09:31.420 ticket Closed - None    Status Updated\n5   1012329 2019-03-18 06:52:03.490 R or O  ticket Closed - Satisfied\n6   1012329 2019-03-18 07:09:33.433 ticket Closed - Satisfied   Status Updated\n7   1012330 2019-03-18 10:25:13.493 R or O  Action mail sent to Service\n8   1012330 2019-03-18 10:55:20.963 Action mail sent to Service ticket Closed - Service Responded\n9   1012330 2019-03-18 11:02:05.327 ticket Closed - Service Responded   Status Updated\n10  1012332 2019-03-18 09:00:41.967 R or O  Action mail sent to client\n11  1012332 2019-03-18 10:24:20.150 Action mail sent to client  Response Client - R\n12  1012332 2019-03-18 10:32:40.717 Response Client - R Status Updated\n\n\n\n\nNow, I have certain ticket IDs and some of them are having more observations based on the statuses provided. You can also see that, for a ticket id, for next observation, new status becomes old status and a new status is provided for the same, which keeps updated, until some closing action i taken.\n\nI want to create a new dataframe/series which has the following format.\n\nticket ID  | Datetime1  | Oldest Status  | New Status | Datetime2  | New Status2| Datetime3  | New Status3 ....\n\n\nso that I have the oldest status and new statuses with dates as shown above till we do this for each of the ticket ID.\n\nMy plan is to use this dataset, to calculate the time differences later on.\n'
'I want to create a frequency table for pandas dataframes which contain missing values. Here is a sample dataframe with missing values to illustrate my problem:\n\nimport pandas as pd\nimport numpy as np\ncar_names = pd.DataFrame({\'name\' : [\'Batmobile\',\'Toyota Corolla\',\'Bike\',\n                                     \'Bike\',\'Batmobile\'],\n      \'hp\': [1000,120,np.nan,np.nan,900]})\ncar_attr = pd.DataFrame({"name": ["Bike","Toyota Corolla"],\n                         "color": ["blue","red"]})\ncars = car_names.merge(car_attr,how=\'left\',on=\'name\')\n\n\n    name            hp      color\n0   Batmobile       1000.0  NaN\n1   Toyota Corolla  120.0   red\n2   Bike            NaN     blue\n3   Bike            NaN     blue\n4   Batmobile       900.0   NaN\n\n\nMy desired output, a frequency table which shows one combination of values occurred twice:\n\n    name            hp      color    count\n0   Bike            NaN     blue     2\n1   Batmobile       1000.0  NaN      1\n2   Toyota Corolla  120.0   red      1\n3   Batmobile       900.0   NaN      1\n\n\nI have tried using groupby().size() but this method excludes rows with missing values (ie. every row in my dataframe except for the second row):\n\ncars.groupby([\'name\',\'hp\',\'color\']).size()\n\n\nname            hp     color\nToyota Corolla  120.0  red      1\n\n\nAnother method I have tried is converting the pandas dataframe to a list of lists (where each row is a list) and using the list.index() function to count occurrences of unique rows, but I run into this strange error:\n\nmy_rows = cars.values.tolist()\nmy_rows_dedup = cars.drop_duplicates().values.tolist()\n\nfor x in my_rows:\n    print(x)\n    print(\'Row index: \', my_rows.index(x),\n    \' Unique Index: \', my_rows_dedup.index(x))\n\n\n[\'Batmobile\', 1000.0, nan]\nRow index:  0  Unique Index:  0\n[\'Toyota Corolla\', 120.0, \'red\']\nRow index:  1  Unique Index:  1\n[\'Bike\', nan, \'blue\']\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-100-f17351883e95&gt; in &lt;module&gt;\n      4 for x in my_rows:\n      5     print(x)\n----&gt; 6     print(\'Row index: \', my_rows.index(x), \' Unique Index: \', my_rows_dedup.index(x))\n\nValueError: [\'Bike\', nan, \'blue\'] is not in list\n\n\nThis error doesn\'t make sense to me since [\'Bike\', nan, \'blue\'] is a value in my_rows_dedup. \n'
'My data includes invoices and customers. \nOne customer can have multiple invoices. One invoice belongs to always one customer. The invoices are updated daily (Report Date).\n\nMy goal is to calculate the age of the customer in days (see column "Age in Days"). In order to achieve this, I take the first occurrence of a customers report date and calculate the difference to the last occurrence of the report date.\n\ne.g. Customer 1 occurs from 08-14 till 08-15. Therefore he/she is 1 day old.\n\nReport Date  Invoice No   Customer No  Amount  Age in Days\n2018-08-14   A            1            50$     1\n2018-08-14   B            1            100$    1\n2018-08-14   C            2            75$     2\n\n2018-08-15   A            1            20$     1\n2018-08-15   B            1            45$     1\n2018-08-15   C            2            70$     2\n\n2018-08-16   C            2            40$     1\n2018-08-16   D            3            100$    0\n2018-08-16   E            3            60$     0\n\n\nI solved this, but however, very inefficiently and it takes too long. My data contains 26 million rows. Below I calculated the age for one customer only.\n\n# List every customer no\ncustomerNo = df["Customer No"].unique()\ncustomer_age = []\n\n# Testing for one specific customer\ntestCustomer = df.loc[df["Customer No"] == customerNo[0]]\ntestCustomer = testCustomer.sort_values(by="Report Date", ascending=True)\n\nfirst_occur = testCustomer.iloc[0][\'Report Date\']\nlast_occur = testCustomer.iloc[-1][\'Report Date\']\nage = (last_occur - first_occur).days\n\ncustomer_age.extend([age] * len(testCustomer))\ntestCustomer.loc[:,\'Customer Age\']=customer_age \n\n\nIs there a better way to solve this problem?\n'
'I have a dataset called "data" with categorical values I\'d like to encode with mean (likelihood/target) encoding rather than label encoding. \n\nMy dataset looks like:\n\ndata.head()\n\nID  X0  X1  X10 X100    X101    X102    X103    X104    X105    ... X90 X91 X92 X93 X94 X95 X96 X97 X98 X99\n0   0   k   v   0   0   0   0   0   0   0   ... 0   0   0   0   0   0   0   0   0   0\n1   6   k   t   0   1   1   0   0   0   0   ... 0   0   0   0   0   0   1   0   1   0\n2   7   az  w   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   1   0   1   0\n3   9   az  t   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   1   0   1   0\n4   13  az  v   0   0   1   0   0   0   0   ... 0   0   0   0   0   0   1   0   1   0\n5 rows × 377 columns\n\n\nI\'ve tried:\n\n# Select categorical features\ncat_features = data.dtypes == \'object\'\n\n# Define function\ndef mean_encoding(df, cols, target):\n\n    for c in cols:\n        means = df.groupby(c)[target].mean()\n        df[c].map(means)\n\n    return df\n\n# Encode\ndata = mean_encoding(data, cat_features, target)\n\n\nwhich raises:\n\n\n  KeyError: False\n\n\nI\'ve also tried:\n\n# Define function\ndef mean_encoding(df, target):\n\n    for c in df.columns:\n        if df[c].dtype == \'object\':\n            means = df.groupby(c)[target].mean()\n            df[c].map(means)\n\n    return df\n\n\nwhich raises:\n\n\n  KeyError: \'Columns not found: 87.68, 87.43, 94.38, 72.11, 73.7, 74.0,\n  74.28, 76.26,...\n\n\nI\'ve concated train and test dataset into one called "data" and saved train target before dropping in the dataset as:\n\ntarget = train.y\nsplit = len(train)\n\ndata = pd.concat(objs=[train, test])\ndata = data.drop(\'y\', axis=1)\ndata.shape\n\n\nHelp would be appreciated. Thanks.\n'
'I am trying to properly interpolate values from a circular grid onto a regular grid with Python3. The data points are sparse compared to my grid goal of 400x400. My goal is to be able to take these values and display them accurately onto an image of the earth. My input data is in the form of [x, y, value].\n\nThe following is an image of my data.\n\n\n\nI have tried using scipy griddata and several different interpolation methods in numpy but none of them produce accurate results. I believe a potential way to get accurate results is to do spherical interpolation to create a high res spherical grid, then use griddata to map it to a rectangular grid, but I have no idea as to using spherical interpolation for this. Following is a couple images, ignore the orientation of the photos as they are from different times.\n\nUsing numpy interp2d, I get this:\n\n\n\nWhat I would like to get is something similar to this, where it is very smooth as it should be:\n\n\n\nHere is code to reproduce the problem. Only numpy, matplotlib, and scipy are required. The get_rotation_array() function with no args, creates a pretty close example of what the sample data could be, for anyone testing.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom scipy import interpolate\n\n# GLOBALS\nEARTH_RADIUS = 6370997.0\nSOLAR_GRID_RES_KM = 750000\nEARTH_GRID_RES_KM = 5*100000\nCUT_OFF_VAL = 1000000\n\n# Earth Patches\nearth_circle1 = plt.Circle((EARTH_RADIUS, EARTH_RADIUS), EARTH_RADIUS, edgecolor=\'black\', fill=False, linewidth=1)\nearth_circle2 = plt.Circle((EARTH_RADIUS, EARTH_RADIUS), EARTH_RADIUS, edgecolor=\'black\', fill=False, linewidth=1)\n\n# This function is messy but it roughly simulates\n# what kind of data I am expecting\ndef get_rotation_array(steps=20, per_line=9):\n    x_vals = []\n    y_vals = []\n    z_vals = []\n    r = EARTH_RADIUS - 10000\n    for el in range(1, per_line):\n        for t in np.linspace(0, 2*np.pi, num=steps):\n            x = (el/float(per_line - 1))*r*np.cos(t) + EARTH_RADIUS\n            y = (el/float(per_line - 1))*r*np.sin(t) + EARTH_RADIUS\n            z = el - 2*(el/float(per_line - 1))*np.abs((1.5*np.pi) - t)\n            if y &lt; (EARTH_RADIUS + CUT_OFF_VAL):\n                x_vals.append(x)\n                y_vals.append(y)\n                z_vals.append(z)\n\n    x_vals.append(EARTH_RADIUS)\n    y_vals.append(EARTH_RADIUS)\n    z_vals.append(1)\n\n    return np.array(x_vals), np.array(y_vals), np.array(z_vals)\n\n# Get "Sample" Data\nx, y, z = get_rotation_array()\n\n# Create Sublots\nfig, ax = plt.subplots(1, 2)\n\n# Get Values for raw plot\ncmap = cm.get_cmap("jet", 1000)\nalpha = np.interp(z, [z.min(), z.max()], [0, 1])\ncolour = cmap(alpha)\n\n# Plot Raw Plot\nax[0].set_title("Sample Data")\nax[0].scatter(x, y, c=colour)\nax[0].add_patch(earth_circle1)\nax[0].set_xlim([0,EARTH_RADIUS*2])\nax[0].set_ylim([0,EARTH_RADIUS*2])\n\n# Use griddata interpolation\nx_solar_interp = np.arange(0, EARTH_RADIUS*2, EARTH_GRID_RES_KM)\ny_solar_interp = np.arange(0, EARTH_RADIUS + CUT_OFF_VAL, EARTH_GRID_RES_KM)\nxx_interp, yy_interp = np.meshgrid(x_solar_interp, y_solar_interp)\n\nz_interp = interpolate.griddata((x, y), z, (xx_interp, yy_interp), method=\'linear\')\n\n# Plot the Colormesh\nplt.pcolormesh(xx_interp, yy_interp, z_interp, cmap=cmap, shading=\'flat\')\n\n# Plot Interpolated Data\nax[1].set_title("Interpolated")\nax[1].add_patch(earth_circle2)\nax[1].set_xlim([0,EARTH_RADIUS*2])\nax[1].set_ylim([0,EARTH_RADIUS*2])\n\n# Show the plots\nplt.show()\n\n\nThe interpolation breaks down because the data isn\'t dependent on the x,y values and is dependent on the angle from the center of the earth.\nSo after all that it comes down to, how to do proper spherical interpolation in Python3 with data like this? Sorry if I have missed anything this is my first time posting on StackOverflow!\n'
"I cant seem to figure out what I am doing wrong,\nI have 2 columns in my df that look like below,\n\n    Date                Time \n2019-07-23 21:17:47.599  22:00:00.000\n2019-07-23 21:11:46.973  21:50:00.000\n\n\nI want to create a new column in the df that calculates the difference in minutes between these two columns. I am trying 2 methods that aren't working. \n\nFirst method is where I use pd.to_timedelta(pd.to_datetime(df3.Time)-pd.to_timedelta(df3.Date).dt.strftime('%H:%M:%S')) / np.timedelta64(1, 'm')\n\n\nIf I try this I get an error of ValueError: only leading negative signs are allowed\n\n\nSecond method I am trying is where I convert the Date column by doing\n\ndf3['new_time'] = [d.time() for d in df3['Date']]\n\n\nthat gives me a timestamp such as 22:25:03.895000\n\nI then use (pd.to_datetime(df3.Time) - pd.to_datetime(df3.new_time))\n\nThe error I am getting using that is TypeError: &lt;class 'datetime.time'&gt; is not convertible to datetime\n\n\nI am not sure what I am missing or not doing correctly, but is there a way where I can correctly have a new column that gives me the value in minutes of Time - Date? \n\nThanks so much\n"
"I'm currently counting the number of missing columns across my full dataset with:\n\nmissing_cols = X.apply(lambda x: x.shape[0] - x.dropna().shape[0], axis=1).value_counts().to_frame()\n\n\nWhen I run this, my RAM usage dramatically increases. In Kaggle, it's enough to crash the machine. After the operation and a gc.collect(), I don't seem to get all of the memory back, hinting at some sort of leak.\n\n\n\nI'm trying to get a feel for the number of rows missing 1 column of data, 2 columns of data, 3 columns of data, etc.\n\nIs there a more efficient way to perform this calculation?\n"
"I am trying to figure out a way in which I can calculate the sum of a column in a df based on filtering a different or 2 different columns.\n\nID | Shift |Valid |Amount\nB     AM      NO    23.22\nE     PM      YES   231.23\nD     AM      YES   443.12\nR     Both    NO    43.12\nT     PM      NO    111.12\n\n\nI want to filter for Shift column like \n\ndf[(df['shift'] == 'PM')] and calculate the sum of the amount column \n\nI want to try this a few times for AM, PM, Both instead of creating new dfs for each and then summing amount column. \n\nAlso if I want to also filter for the valid column and calculate sum of amount column without creating a new df is that possible as well?\n\nThanks!\n"
'Im really new to Python and Data science.\n\nI have a 100K+ CSV Dataset with 30 columns. The goal is to add some rows to the dataset if some condition are satisfied.\n\nTo make things easier, let\'s say i have only three column named "A", "B" and "C", type of A and B is integer. C is a String.\n\nLet\'s not worry about B and C, as im gonna put all B column value to 0 and im gonna calculate C later on.\n\nSo here\'s a quick view to my "Imaginary" dataset:\n\n   A\n _____\n|  1  |\n|  4  |\n|  3  |\n|  7  |\n_______\n\n\nI parsed my dataset in a Dataframe and i sorted it by value of "A".\n\nSo, now, it looks like this :\n\n   A\n _____\n|  1  |\n|  3  |\n|  4  |\n|  7  |\n_______\n\n\nNow, i want to iterate over my DataFrame and check if im missing some number between two rows and add them to the dataframe, i.e: if A[i+1]-A[i]>1, i wanna add A[i]+1 between them.\n\n   A\n _____\n|  1  |\n|  2  |\n|  3  |\n|  4  |\n|  5  |\n|  6  |\n|  7  |\n_______\n\n\nSo as far as i know i have these choices:\n\n\nAppend my new rows directly to a new Dataframe. I don\'t know why\nbut i think that it\'s not a good idea. If im right, can you guys\nexplain why ? If im wrong, can you guys explain why?\nAppend my new rows to a List, make a DataFrame from that List and "Join" my old DataFrame with my new.\nIdk, any suggestion?\n\n\nMy main problem now, is that dealing with a large DataFrame is a pain in the ass, and my script takes ages to do the job. Can you guys lead me to "Right way" of dealing with such large amount of data ?\n\nBtw, here\'s the code i made for choice n°2:\n\ndf=pd.read_csv("dataset.csv")\ndf.sort_values(by="A")\nL=[]\nfor i in range (0, len(df)-1):\n    actual=df.at[i, \'A\']\n    next=df.at[(i+1), \'A\']\n    diff=actual-next-1\n    for j in range(1, diff):\n        L.append(actual+1)\n        actual +=1\ndf=pd.DataFrame(data=L, columns=list(df))\ndf.to_csv("my_output.csv", sep=\',\')\n\n'
"I have Information Gain dataframe and tf dataframe. the data looks like this :\n\nInformation Gain\n\n    Term      IG\n0   alqur     0.641328\n1   an        0.641328\n2   ayatayat  0.641328\n3   bagai     0.641328\n4   bantai    0.641328\n5   besar     0.641328\n\n\nTerm Frequency\n\n            A   B   A+B\nahli        1   0   1\nalas        1   0   1\nalqur       0   1   1\nan          0   1   1\nayatayat    0   1   1\n...        ... ... ...\nterus       0   1   1\ntuduh       0   1   1\ntulis       1   0   1\nulama       1   0   1\nupaya       0   1   1\n\n\nlet's say table Information Gain = IG\nand table tf = TF\n\nI wanted to check if IG.Term is in TF.index then get the row values so it should be like this :\n\n    Term      A    B    A+B\n0   alqur     0    1    1\n1   an        0    1    1\n2   ayatayat  0    1    1\n3   bagai     1    0    1\n4   bantai    1    1    2\n5   besar     1    0    1\n\n\nNB : I don't need the IG value anymore\n"
"Im kinda new to Python and Datascience.\n\nI have a Dataset with two inputs x1 and x2 and one output y:\n\ndf=pd.DataFrame({'x1': [1, 2, 2, 1, 0.5, -1, -2, -2, -1, -0.5], 'x2': [1, 1, 2, 2, 0.5, -1, -1, -2, -2, -0.5], 'y': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]})\n\n    x1   x2  y\n0  1.0  1.0  0\n1  2.0  1.0  0\n2  2.0  2.0  0\n3  1.0  2.0  0\n4  0.5  0.5  0\n5 -1.0 -1.0  1\n6 -2.0 -1.0  1\n7 -2.0 -2.0  1\n8 -1.0 -2.0  1\n9 -0.5 -0.5  1\n\n\nI've plotted this dataset :\n\nplt.scatter(df.x1[df['y']==1], df.x2[df['y']==1], color='red')\nplt.scatter(df.x1[df['y']==0], df.x2[df['y']==0], color='blue')\nplt.show()\n\n\nAnd what i want to do is to have in the same plot a different background for my classes points. So the results that i want, is something like this :\n\n\n\nIm not really familiar with Matplotlib, the best i could achiev is to have two different axes for each class, but that's not what i really want to do..\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf=pd.DataFrame({'x1': [1, 2, 2, 1, 0.5, -1, -2, -2, -1, -0.5], 'x2': [1, 1, 2, 2, 0.5, -1, -1, -2, -2, -0.5], 'y': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]})\nfig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\nax0.scatter(df.x1[df1['y']==1], df.x2[df1['y']==1], color='red')\nax0.set_facecolor('xkcd:blue')\nax1.scatter(df.x1[df1['y']==0], df.x2[df1['y']==0], color='blue')\nax1.set_facecolor('xkcd:red')\nplt.show()\n\n\n\n\nI want this result but in the same axes, any solution please ?\n"
"Example of wrong figure\n\nNeed help~! I have plotted a figure for my data. But when I check the figure, the x-axis shows time, but I do not want to show the specific time but only the month (for example, I only wish to show like: 2018-01, 2018-02, but not like 2018-01-01 00:00:00). I do not know what is wrong about my code and it does not show in my terminal run result. So confused, please if someone provides some suggestion.\nMany thanks. Code is below\n\ndata = pd.read_csv('micro_null_data.csv')\n\nclean1_data = pd.value_counts(data['SAMPLE_NAME'])\n\n\nZone2 = data.loc[data['SAMPLE_NAME'] =='Zone 2', ['SAMPLED_DATE', 'FORMATTED_ENTRY']]\nvalue_map = {\n    'Not Detected': 1,\n    'Detected': 0,\n}\n\nZone2_table = pd.crosstab(pd.to_datetime(Zone2['SAMPLED_DATE']), Zone2['FORMATTED_ENTRY'])\nZone2_table.drop(Zone2_table.index[474], inplace=True)\nprint(Zone2_table)\n\nZone2_by_mongth = Zone2_table.resample('M').sum()\nprint(Zone2_by_mongth)\n\nfig = plt.figure()\nplt.style.use('seaborn')\nZone2_by_mongth.plot.bar()\n\nplt.title('Zone 2')\nplt.xticks(rotation=45)\nplt.show()\n\n"
"I have this dataframe.\n\ndf = pd.DataFrame({'userId': [10,20,10,20,10,20,60,90,60,90,60,90,30,40,30,40,30,40,50,60,50,60,50,60],\n                   'movieId': [500,500,800,800,700,700,1100,1100,1900,1900,2000,2000,1600,1600,1901,1901,3000,3000,3025,3025,4000,4000,500,500],  \n                   'ratings': [3.5,4.5,2.0,5.0,3.0,1.5,1.5,4.5,2.5,4.5,4.0,5.0,4.0,1.5,4.5,4.5,3.5,4.5,3.0,5.0,4.0,1.5,3.5,5]})\n\n\n\nI have converted this dataframe to different chunks. After every 6 rows new chunk is created.\n\nAfter creating chunks it looks like this\n\ndf\n   userId  movieId  ratings\n0      10      500      3.5\n1      20      500      4.5\n2      10      800      2.0\n3      20      800      5.0\n4      10      700      4.0\n5      20      700      1.5\n    userId  movieId  ratings\n6       60     1100      3.5\n7       90     1100      4.5\n8       60     1900      3.5\n9       90     1900      4.5\n10      60     2000      2.0\n11      90     2000      5.0\n    userId  movieId  ratings\n12      30     1600      4.0\n13      40     1600      1.5\n14      30     1901      3.5\n15      40     1901      4.5\n16      30     3000      3.5\n17      40     3000      4.5\n    userId  movieId  ratings\n18      50     3025      2.0\n19      60     3025      5.0\n20      50     4000      4.0\n21      60     4000      1.5\n22      50      500      3.5\n23      60      500      4.5\n\n\nWhat i want to do is this:\n1. Take absolute difference between two users for same movie (each pair of user have watched three movies).\n2.  Take the average of difference for first two movies then compare it with 3rd movie difference.\n   if the third movie difference is less than avg of first two movies take it positive otherwise negative. For example, in first chunk diff b/w first movie is 1 and for next is 3 (avg diff is 2). For the 3rd pair diff is 2.5 (diff > avg) it should give negative result.\n3. Do it for every chunk in dataframe.\n\npositive_counter = [] \nnegative_counter = []\n\nnumberOfUsers = 2\nnumberOfMovies = 3\nusersLength = numberOfUsers*numberOfMovies\n\ndef chunker(seq, size):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n\ndef finding_rating(df):\n     for i in chunker(data,usersLength):\n\n        df['diff'] = df.groupby('movieId')['ratings'].diff().abs()\n        #code\n\n\n\n\n\n"
'I have tables as below \n\n                   Error Code ID      Error Code              Description of  Error Code\n0                                NaN      Value                                     NaN\n1                     OE_ATO_IN_OPEN      16169      Order priced ATO cannot be entered\n2                                NaN        NaN                when a security is open.\n3                      e$dup_request      16198  Duplicate modification or cancellation\n4                                NaN        NaN     request for the same trade has been\n5                                NaN        NaN                            encountered.\n6          TRD_CONT_MOD_NOT_ALLOWED      16231     Continuous session trades \n7       STR_PRO_PARTIVIPANT_INVALID      16233     Proprietary requests cannot be made\n\n\nI want to convert it into\n\n                   Error Code ID     Error Code Value        Description of  Error Code\n1                     OE_ATO_IN_OPEN      16169      Order priced ATO cannot be entered               \n                                                               when a security is open.\n3                      e$dup_request      16198  Duplicate modification or cancellation\n                                                    request for the same trade has been\n                                                                           encountered.\n6          TRD_CONT_MOD_NOT_ALLOWED      16231     Continuous session trades \n7       STR_PRO_PARTIVIPANT_INVALID      16233     Proprietary requests cannot be made\n\n\nMissing index are just for understanding purpose.\n'
"I have a data set called test3 looking like (imported from excel): \n\n$ID  $spec    $orient $direct   $rep     $slope   $intercept\n9119    1       stance  15b     1       2859.09223  158\n9119    2       stance  15b     2       2886.53583  321\n9119    3       stance  0       1       2860.91423  21\n9119    4       fall    15f     1       2878.9364   326\n9119    5       fall    15f     2       2902.0397   45\n9120    1       stance  15b     1       1444.91347  654\n9120    2       stance  15b     2       1460.09585  23\n9120    3       stance  0       1       1470.2588   85\n9120    4       fall    15f     1       1481.6892   225\n9120    5       fall    15f     2       1475.40215  145\n\n\nand a template data frame called test3mean:\n\n$ID  $spec    $orient $direct   $slope   $intercept\n9119    1       stance  15b     nan     nan\n9119    2       stance  0       nan     nan\n9119    3       fall    15f     nan     nan\n9120    1       stance  15b     nan     nan\n9120    2       stance  0       nan     nan\n9120    3       fall    15f     nan     nan\n\n\nI am using Pandas dataframe to calculate the mean of the $slope and $intercept for repeated measurements in test3 and put them in the corresponding columns in test3mean: \n\nID = np.array([9119,9120])\norient = np.array(['stance','fall'])\ndirect = np.array(['0','15f','15b'])\n\nfor i in ID:\n    for o in orient:\n        for d in direct:\n            test3mean[test3mean['$ID']==i][test3mean['$orient']==o][test3mean['$direct']==d][['$slope','$intercept']] = test3[test3['$ID']==i][test3['$orient']==o][test3['$direct']==d][test3['$rep']!=3].mean()[['$slope','$intercept']]\n\n\nHowever, the test3mean does not change. I am aware of the copy vs view issue, and have seen the df.loc[:,()] solutions, but was not able to implement them for my specific case.\n\nThe expected output after two steps of the loop would be for the test3mean to  look like:\n\n$ID  $spec    $orient $direct   $slope       $intercept\n9119    1       stance  15b     2872.81403  239.5\n9119    2       stance  0       2860.91423  21\n9119    3       fall    15f     nan     nan\n9120    1       stance  15b     nan     nan\n9120    2       stance  0       nan     nan\n9120    3       fall    15f     nan     nan\n\n"
'I have a dataset that looks like this :\n\n    Interactor A    Interactor B    Interaction Score   score2\n0   P02574  P39205  0.928736    0.375000\n1   P02574  Q6NR18  0.297354    0.166667\n2   P02574  Q7KML4  0.297354    0.142857\n3   P02574  Q9BP34  0.297354    0.166667\n4   P02574  Q9BP35  0.297354    0.16666\n\ndata.shape = (112049, 5)\n\n\nI want to add Interactor B at the end of Interactor A column uniquely and add a column that shows their Rank. \nI did this by :\n\ncols = [data[col].squeeze() for col in data[[\'Interactor A\',\'Interactor B\']]]\nn =pd.concat(cols, ignore_index=True)\nn = pd.DataFrame(n,columns = [\'AB\'])\n\n\nto make the column unique :\n\nt = pd.unique(n[\'AB\'])\nt= pd.DataFrame(t, columns=[ "AB"])\n\n\nthen :\n\nt2 = n.groupby([\'AB\'],sort=False).size()\nt2 = pd.DataFrame(t2)\n\n\nfinally :\nby concatenating t2 and t   :\n\ndata_1 = pd.concat([t,l], axis=1)\n\n\nAB  Rank\n0   P02574  4\n\n\ndata.shape = (13631, 2)\n\n\nnow I want to add the Interaction Score   and  score2 column to DF . if there is duplicate take the mean of their Interaction Score  and delete the duplicates and replace the value of the Interaction Score by the mean.\n\nI used : \n\nscore2 = data.groupby([\'Interactor A\',\'Interactor B\'])[\'score2\'].mean()\nscore2 = pd.DataFrame(score2, columns=[\'score2\']) \n\n\nthe output in this case is like :\n\n        score2\nInteractor A    Interactor B    \nA0A023GPK8  Q9VQW1  0.200000\nA0A076NAB7  Q9VYN8  0.000000\nA0A0B4JD97  Q400N2  0.000000\nQ9VC64  0.090909\nQ9VNE4  0.307692\n\n112049 rows × 1 columns\n\n\nbut what I is to add columns with  mean of \'score2\' and \'Interaction Score\' column for 13631 unique data that I made. How can achieve this ?? please help.\nthe final df should be like :\n\nInteractor  Rank    Interaction Score   score2\n P02574      5        0.928736              0.44\n\nie: score2 is the average of all \'P0257\' score that have been in the dataset\n'
"sample data from dataframe:\n\nPairs\n(8, 8), (8, 8), (8, 8), (8, 8), (8, 8)\n(6, 7), (7, 7), (7, 7), (7, 6), (6, 7)\n(2, 12), (12, 3), (3, 4), (4, 12), (12, 12)\n\n    ```\n    \n        new_col = []\n            for e in content.Pairs:\n            new_col.append(list(dict.fromkeys(e)))\n            content['Unique'] = new_col\n    \n    ```\n\noutput expected is unique pairs from Pair column like this:\n(8, 8),(6, 7),(7, 6),(7, 7),(2, 12) so on\n\nwhat I am getting is this result when trying the above code:\nUnique\n['8', '']\n['6', '7', '']\n['2', '12', '3', '4', '']\n\nwhat is the issue with the data if I am doing with manual data then it's working why not in the data frame\n"
'Which libraries would help me read a gct file in python and edit it like removing the rows with NaN values. And how will the following code change if I apply it to a .gct file?\n\ndata = pd.read_csv(\'PAAD1.csv\')\nnew_data = data.dropna(axis = 0, how =\'any\')\nprint("Old data frame length:", len(data), "\\nNew data frame length:",  \n       len(new_data), "\\nNumber of rows with at least 1 NA value: ", \n       (len(data)-len(new_data)))\nnew_data.to_csv(\'EditedPAAD.csv\')\n\n'
'I\'m using dataset from Kaggle - Cardiovascular Disease Dataset.\nThe model has been trained and what I want to do is to label a single input(a row of 13 values)\ninserted in dynamic way.\n\nShape of Dataset is 13 Features + 1 Target, 66k rows\n\n#prepare dataset for train and test\ndfCardio = load_csv("cleanCardio.csv")\ny = dfCardio[\'cardio\']\nx = dfCardio.drop(\'cardio\',axis = 1, inplace=False)\nmodel = knn = KNeighborsClassifier()\nx_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\nmodel.fit(x_train, y_train)\n# make predictions for test data\ny_pred = model.predict(x_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint("Accuracy: %.2f%%" % (accuracy * 100.0))\n\n\nML is trained, what I want to do is to predict the label of this single row :\n\n[\'69\',\'1\',\'151\',\'22\',\'37\',\'0\',\'65\',\'140\',\'90\',\'2\',\'1\',\'0\',\'0\',\'1\']\n\n\nto return 0 or 1 for Target.\nSo I wrote this code :\n\nimport numpy as np\nimport pandas as pd\nsingle =  np.array([\'69\',\'1\',\'151\',\'22\',\'37\',\'0\',\'65\',\'140\',\'90\',\'2\',\'1\',\'0\',\'0\',\'1\'])\nsingledf = pd.DataFrame(single)\nfinal=singledf.transpose()\nprediction = model.predict(final)\nprint(prediction)\n\n\nbut it gives error : query data dimension must match training data dimension\n\nhow can I fix the labeling for single row ? why I\'m not able to predict a single case ?\n'
"Need to select only those rows where at least one value is present from the list.\n\nI have tried to use isin function as below but it's returning those rows also which don't have any value from the list:-\n\ndf[(df.loc[:,'P_1':'P_90'].isin(list))]\n\n\n\n  Note:-P_1 to P_90 indicated range of columns.\n  Sample data:-\n  dib-cust_id|p_1|p_2|.......|p_90\n  345|1950|1860|..............|675\n  \n  Note:- I can't provide exact data as it's confidential. Above is just an example how dataframe will look like with 91 columns.\n\n\nPlease let me know what is wrong in this method and is there any other way to get the desired result.\n"
'this is the code to extract the html data from the class where my data is located. but how do i extract the required data(the state wise data) as it is not given in order\nwebsite\n\nimport requests\nimport cloudscraper\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\nimport time\nimport datetime\nscraper = cloudscraper.create_scraper()\nhtml = scraper.get("https://www.mohfw.gov.in/").text\ndata = BeautifulSoup(html, \'html.parser\')\nli=data.find_all(class_=\'table-responsive\')\nli\n\n'
'I am working my way through the resource Python for Data Science For Dummies. I am currently learning about imputing missing data values using pandas. Below is my code:\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=\'NaN\',\n              strategy=\'mean\')\n# creates imputer to replace missing values.\n# missing_values parameter defines what we are looking out for to impute.\n# strategy parameter implies with what value you want to replace the missing value.\n# strategy can be either: mean, median, most_frequent\n\nimp.fit([[1, 2, 3, 4, 5, 6, 7]])\n\'\'\'\nBefore imputing, we must provide stats for the imputer to use by calling fit(). \n\'\'\'\n\ns = [[1, 2, 3, np.NaN, 5, 6, None]]\n\n\nprint(imp.transform(s))\nx = pd.Series(imp.transform(s).tolist()[0])  # .transform() fills in the missing values in s\n# we want to display the result as a series. \n# from the imputer we want to transform our imputer output to a list using .tolist()\n# after that we want to transform the list into a series by enclosing it within .Series()\nprint(x)\n\n\nHowever, when I run the code, it returns an error at the line with imp.fit():\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-38-3b624663cf89&gt; in &lt;module&gt;\n     15 # strategy can be either: mean, median, most_frequent\n     16 \n---&gt; 17 imp.fit([[1, 2, 3, 4, 5, 6, 7]])\n     18 \'\'\'\n     19 Before imputing, we must provide stats for the imputer to use by calling fit().\n\n/Applications/anaconda3/lib/python3.7/site-packages/sklearn/impute/_base.py in fit(self, X, y)\n    266         self : SimpleImputer\n    267         """\n--&gt; 268         X = self._validate_input(X)\n    269         super()._fit_indicator(X)\n    270 \n\n/Applications/anaconda3/lib/python3.7/site-packages/sklearn/impute/_base.py in _validate_input(self, X)\n    242                 raise ve\n    243 \n--&gt; 244         _check_inputs_dtype(X, self.missing_values)\n    245         if X.dtype.kind not in ("i", "u", "f", "O"):\n    246             raise ValueError("SimpleImputer does not support data with dtype "\n\n/Applications/anaconda3/lib/python3.7/site-packages/sklearn/impute/_base.py in _check_inputs_dtype(X, missing_values)\n     26                          " both numerical. Got X.dtype={} and "\n     27                          " type(missing_values)={}."\n---&gt; 28                          .format(X.dtype, type(missing_values)))\n     29 \n     30 \n\nValueError: \'X\' and \'missing_values\' types are expected to be both numerical. Got X.dtype=float64 and  type(missing_values)=&lt;class \'str\'&gt;.\n\n\nAny help on the matter is greatly appreciated!\n\nAlso, wherever you are I hope that you are coping well with the COVID-19 situation!\n'
'Problem\nI\'m trying to answer this question: Consider two documents A and B whose Euclidean distance is d and cosine similarity is c (using no normalization other than raw term frequencies). If we create a new document A\' by appending A to itself and anotherdocument B\' by appending B to itself, then: \n\na.What is the Euclidean distance between A\' and B\' (using raw term frequency)? \n\nMy solution\n\ndoc1 = "the quicker brown dogs easily jumps over the lazy dogs" \ndoc2 = "the quicker dogs pose a serious problem for lazy dogs"\n\ndef calc_term_frequency(doc : list):\n\n    dic = {}\n    for word in doc.split():\n        if word in dic:\n            dic[word] = dic[word] + 1\n        else:\n            dic[word]= 1\n\n    for word, frequency in dic.items():\n       dic[word]= frequency / len(doc.split())\n\n    return dic\n\ntfs_doc1 = calc_term_frequency(doc1)\ntfs_doc2 = calc_term_frequency(doc2)\nprint(tfs_doc1)\n\n\nOutputs tfs_doc1 as:\n{\'the\': 0.2, \'quicker\': 0.1, \'brown\': 0.1, \'dogs\': 0.2, \'easily\': 0.1, \'jumps\': 0.1, \'over\': 0.1, \'lazy\': 0.1}\nThis seems like it works at it should. I then proceed to calculate the Euclidean Distance, first between doc1 and doc1 and then doc1 and doc2, shown below. \n\nimport math\nmath.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc1.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc1.keys())))) # output: 0\nmath.sqrt(sum((tfs_doc1.get(k, 0) - tfs_doc2.get(k, 0))**2 for k in set(tfs_doc1.keys()).union(set(tfs_doc2.keys())))) # output: 0.316227766016838\n\n\nThis gives me a score of 0.316227766016838. When I try to verify that this is correct using sklearn, like below:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import euclidean_distances\n\ncorpus_vect = CountVectorizer().fit_transform(corpus).todense() \n\nprint(euclidean_distances(corpus_vect[0], corpus_vect[0])) # output: 0\nprint(euclidean_distances(corpus_vect[0], corpus_vect[1] )) # output: 3.\n\n\nI get an output of [[0.]] [[3.]], which translates to round(, 1) of my "manual" result. \n\nThe problem: when I try to answer the initial questions and "double" the strings, e.g.\n\ndoc1 = "the quicker brown dogs easily jumps over the lazy dogs the quicker brown dogs easily jumps over the lazy dogs" \ndoc2 = "the quicker dogs pose a serious problem for lazy dogs the quicker dogs pose a serious problem for lazy dogs"\n\n\nI get the same output for the manual technique (0.316227766016838) but [[0.]] [[6.]] when using the "sklearn method" / Vectorizer. So, using one method the ED stays the same and using the other it doubles!\n\nWhat is the correct solution and what causes the difference? Really stuck here. Thanks in advance. \n'
"I'm just starting out, so I try to build things that work first and then think of how can I improve the code.\n\nI've been working with CoinGecko's API to dump data like prices. The first issue I got is that query returns a list of lists. Each entry contains a UNIX timestamp and a value.\n\n\n\nFirst, I used pandas to put this data into a DataFrame.\n\ndata = cg.get_coin_market_chart_by_id('bitcoin', 'USD', 'max')\ndf = pd.DataFrame(data)\n\n\nIt returned a DataFrame with each cell containing a list with a UNIX timestamp and a value.\n\n\n\nObviously, I wasn't happy with each cell containing a UNIX timestamp. So, I made 3 DataFrames out of each Series. I also formatted UNIX timestamps in new indexes to datetime in each.\n\nprice = df['prices'].apply(pd.Series)\nprice.columns = ['date', 'price']\nprice = price.set_index(['date'])\nprice.index = pd.to_datetime(price.index, unit = 'ms')\nprice.columns = ['price']\nmarket_cap = pd.DataFrame(df.market_caps.values.tolist(), index = df.index)\nmarket_cap = market_cap.set_index(0)\nmarket_cap.index = pd.to_datetime(market_cap.index, unit = 'ms')\nmarket_cap.index.names = ['date']\nmarket_cap.columns = ['market_cap']\nvolume = pd.DataFrame(df.total_volumes.values.tolist(), index = df.index)\nvolume = volume.set_index(0)\nvolume.index = pd.to_datetime(volume.index, unit = 'ms')\nvolume.index.names = ['date']\nvolume.columns = ['volume']\n\n\nFinally, I concatenated all 3.\n\ndfs = [price, market_cap, volume]\nconc = pd.concat(dfs, axis = 1, sort = False)\n\n\n\n\nI'm not a CS guy or anything, but I want to learn how to manipulate data well. I let you, wizards of StackOverflow, use whatever unpleasant words when describing my code as long as it helps me to improve. Thanks.\n"
"I'm new to pandas and I want to clean a data frame with loads of columns. \n\nI want to keep values that fall within a range specific for each column, eg for the column named 'Age' I want to keep values larger than 5 and smaller than 25. If a value falls outside of that range I want to replace it with NaN, eg in the 'Age' column there is the value 918 that I want to replace.  \n\nIn my attempt I'm using a dictionary, because like I said I've got a lot of columns. This code doesn't work because it doesn't actually change any of the values in my original data frame (no error message).\n\nThanks for any help!  \n\n# PACKAGES \nimport pandas as pd\nimport numpy as np \n\n\n# STARTING DATA \ndata = [[1.0, 10, 0], [0.0, 12, 0.4], [2.0, 918, 0.9]]   \ndf = pd.DataFrame(data, columns = ['TriGly', 'Age', 'Chol']) \n\ndict = {\n    'Age': (5, 25),\n    'Chol': (0.2, 1.2),\n    'TriGly': (0.0, 1.0)\n}\n\n\n# CLEAN \nfor column_name in df.columns:                                             \n    if column_name in dict:                                                \n        for row in df[column_name]:                                        \n            if dict[column_name][0] &lt; row &lt; dict[column_name][1]:       \n                row = row                                                   \n            else:\n                row = np.nan                                               \n\n# DESIRED DATA \ndata2 = [[1.0, 10, np.nan], [0.0, 12, 0.4], [np.nan, np.nan, 0.9]]   \ndf2 = pd.DataFrame(data2, columns = ['TriGly', 'Age', 'Chol']) \n\n"
'I\'m trying to format the following transposed multiindex dataframe:\n\n        0   1   2   3\nB   sum 22  22  0   0\n    d   0   0   -22 -22\n    d%  0   0   -1  -1\nC   sum 30  15  30  60\n    d   0   15  0   30\n    d%  0   -0.5    1\n\n\nI\'d like to apply a style.bar and a style.format("{:.2%}" on rows with a second level index ending in %, but can\'t for the life of me figure out how to subset on rows in a MultiIndex dataframe.\n\nI\'ve tried playing with IndexSlice, and many other approches to no avail. Is there a way to do this?\n'
"This is the data in single cell of dataframe with 14 columns. Cell is the element of column. There are 45k+ this kind of cells, to do it manually is a hell.\n\none cell data\n\nI'd like to do with this cell 3 things:\n\n\nmove text part with address, state, zip - to another column;\ndelete the hooks () of cell; \nseparate for 2 columns longitude and latitude.\n\n\nHow it's possible to do?\n"
"So in order to better understand the data science topic of linear regression, I have been trying to recreate what scikitlearn's LinearRegression module does under the hood. The problem that I am having is that when I start a gradient descent of the slope and intercept using my data, I am unable to get the slope and intercept values to converge, no matter what step size I use or descent iterations. The data that I am trying to find the linear relationship between is NBA FG% and NBA W/L% which can be found here (it's only about 250 rows of data but I figured it would be easier to share in a pastebin...). You can recreate the graph the initial graph of the data by using:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndef graph1(axis = []):\n    x = FG_pct\n    y = W_L_pct\n    plt.scatter(x, y)\n\n    plt.title('NBA FG% vs. Win%')\n    plt.xlabel('FG pct (%)')\n    plt.ylabel('Win pct (%)')\n    if len(axis) &gt; 1:\n        plt.axis(axis)\n    plt.legend()\n\n\nIt will look like this (minus the color):\n\n\n\nThere is a pretty obvious relationship between the two variables and you can basically take a pretty good guess at what the line of best fit would be (my guess was a slope of 5 and an intercept of around -1.75).\n\nThe gradient descent equations I used, which are derived by taking the derivatives of the loss function with respect to both slope and intercept, are these:\n\ndef get_b_gradient(x_pts, y_pts, m, b):\n    N = len(x_pts)\n    tot = 0\n\n    for x, y in zip(x_pts, y_pts):\n        tot += y - (m*x + b)\n\n    gradient = (-2/N)*tot\n    return gradient\n\ndef get_m_gradient(x_pts, y_pts, m, b):\n    N = len(x_pts)\n    tot = 0\n\n    for x, y in zip(x_pts, y_pts):\n        tot += x * (y - (m*x + b))\n\n    gradient = (-2/N)*tot\n    return gradient\n\ndef get_step(x_pts, y_pts, m, b, learning_rate):\n    init_b = get_b_gradient(x_pts, y_pts, m, b)\n    init_m = get_m_gradient(x_pts, y_pts, m, b)\n\n    final_b = b - (init_b*learning_rate)\n    final_m = m - (init_m*learning_rate)\n\n    return final_m, final_b\n\ndef gradient_descent(x_pts, y_pts, m, b, learning_rate, num_iterations):\n    for i in range(num_iterations):\n        m, b = get_step(x_pts, y_pts, m, b, learning_rate)\n    return m, b\n\n\nAfter getting these it is just a matter of finding the right number of iterations and learning rate to get the slope and intercept to converge to the optimal value. Since I am unsure of a systematic way to find these values I simply try inputting different orders of magnitude into the gradient_descent function:\n\n# 1000 iterations, learning rate of 0.1, and initial slope and intercept guess of 0\nm, b = gradient_descent(df['FG%'], df['W/L%'], 0, 0, 0.1, 1000)\n\n\nYou can track the converge of your slope and intercept using a graph such as this:\n\ndef convergence_graph(iterations, learning_rate, m, b):\n    plt.subplot(1, 2, 1)\n    for i in range(iterations):\n        plt.scatter(i,b, color='orange')\n        plt.title('convergence of b')\n        m, b = get_step(df['FG%'], df['W/L%'], m, b, learning_rate)\n\n    plt.subplot(1, 2, 2)\n    for i in range(iterations):\n        plt.scatter(i,m, color='blue')\n        plt.title('convergence of m')\n        m, b = get_step(df['FG%'], df['W/L%'], m, b, learning_rate)\n\n\nAnd this is really where the problem is evident. Using the same iterations (1000) and learning_rate as before (0.1) you see a graph that looks like this:\n\n\n\nI would say that the linearity of those graphs mean that it is still converging at that point so the answer would be to increase the learning rate but no matter what order of magnitude I choose for the learning rate (all the way up to millions) the graphs still retain linearity and never converge. I also tried going with a smaller learning rate and messing with the # of iterations... nothing. Ultimately I decided to throw it into sklearn to see if it would have any trouble:\n\nFG_pct = np.array(FG_pct)\nFG_pct = FG_pct.reshape(-1, 1)\n\nline_fitter = LinearRegression().fit(FG_pct, W_L_pct)\n\nwin_loss_predict = line_fitter.predict(FG_pct)\n\n\nIt had no problem:\n\n\n\nSo this is getting rather long and I am sorry for that. I don't have any data sciency people to ask directly and no professors around so I figured I'd throw it up here. Ultimately, I am unsure of whether the issues arises in 1) my gradient descent equations or 2) my approach in finding a proper learning rate and # of iterations. If anyone could point out what is happening, why the slope and intercept aren't converging, and what I am doing wrong that would be much appreciated!\n"
'I spent a lot of time trying to understand how the axis argument works in the keras.utils.normalization() function. Can someone please explain it to me by using the concept of np.array by making a random (2,2) np array and explain how the normalization actually works for different axes.\n'
"In passing someone had suggested to me that I could use half normal distribution in python to set min and max points using 0 to infinity:\nhalfnorm.rvs()\n\nThe 0 seems to cut off the min, however I have no idea what to do with the infinity.\n\nI would like to do a number generator from 0 - 15 within a normal distribution, but having a hard time finding a function that doesn't go over the max or below the min due to the nature of distribution limits.\n"
"I can't plot a simple Pandas DataFrame to an Area Chart using Altair. The point is that the chart works if I import the very same DataFrame using Vega (&quot;source = data.iowa_electricity()&quot;), for example.\nsource = {\n    'year': ['2001-01-01', '2002-01-01'],\n    'source': ['Fossil Fuels', 'Nuclear Energy'],\n    'net_generation': [35361, 35991]\n}\nsource = pd.DataFrame(source) ### this DF doesn't work\n\nchart = alt.Chart(source).mark_area(opacity=0.3).encode(\n    x=&quot;year&quot;,\n    y=alt.Y(&quot;net_generation:Q&quot;, stack=None),\n    color=f&quot;source:N&quot;\n)\n\nIf I change the above &quot;source&quot; to &quot;source = data.iowa_electricity()&quot; (which contains the very same data inside), the chart works pretty fine.\nWhat am I missing here?\nPS: the same occurs using only string as labels (without dates).\n"
"I am new to python pandas and facing this issue\nYear, Category,      Law,     col_1,     col_2,   col_3,  col_4\n2015, Contraception, Law_1, 'CO MO VA',   '' ,     'XY',   ''\n\nIs there any way I can get this:\nYear, Category,      Law,    state, stage\n2015, Contraception, Law_1,   CO,   col_1\n2015, Contraception, Law_1,   MO,   col_1\n2015, Contraception, Law_1,   VA,   col_1\n2015, Contraception, Law_1,   XY,   col_3\n\n"
"I am getting an error for trying to create a column with binned values as below:\nbins = [1, 10, 20, 21, 34]\nlabels = [1-10, 11-20, 21-34, 35]\ndf['binned'] = pd.cut(df['Number of Clinics'], bins=bins, labels=labels)\n\nI am getting error: ValueError: Categorical categories must be unique \nNot sure what I am missing? The number of clinics column just contains a value between 1 and 100 that I want to bin between 1-10, 11-20, 21-34 and 35+, thanks\n"
"I'm at a loss here not sure what to do I'm hoping someone can help.\nI have a dataset with a personID, sportPlayed, and date. I need to be loop through each individual (personID) and return ONLY those that have played basketball, THEN played football. In the example below, only personID 1 and personID 3 would be returned. personID 4 played football, but they would not be returned since they played Soccer before and not Basketball.\nAny idea how to approach this? Thanks!\npersonID     sportPlayed     date\n1            Basketball      2020-01-01\n1            Basketball      2020-01-02 \n1            Football        2020-03-01\n2            Baseball        2020-02-05\n2            Baseball        2020-03-05\n3            Basketball      2020-04-01\n3            Football        2020-05-05\n4            Soccer          2020-02-01\n4            Football        2020-02-06\n\n"
'How to replace each value in the n x n array with column index if value &gt;= 1 otherwise with row index.\nEven better if the replaced value would map into other 1d array and return the value from it.\nvalue_array = np.array([200, 200, 300, 10])\n\narr = np.array(\n  [[1, 1, .66, 20],\n   [1, 1, .66, 20],\n   [1.5, 1.5, 1, 30],\n   [.05, .05, .03, 1]]\n)\n\nGoal is to get an array of the same size containing values from value_array .\nExamples:\n\nat position [0,2] the value is .66 which is less than 1 therefore row index is needed which is 0. 0 then is indexed into value_array and the answer in the result matrix position[0,2] is 200.\nat position [0,3] the value is 20 which is greater than 1 therefore column index is needed which is 3. 3 then is indexed into value_array and the answer in the result matrix position[0,3] is 10.\n\nAlso, this to be applied for a big array(1m x 1m) executions needs to be somehow split into multiple parts.\n'
"I have the following code:\nA = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=[['att1', 'att2']])\nA['idx'] = ['a', 'b', 'c']\nA\n\nwhich works fine until I do (trying to set column 'idx' as in index for the dataframe)\nA.set_index('idx', inplace=True)\n\nwhich throws an error\nTypeError: only integer scalar arrays can be converted to a scalar index\n\nWhat does this mean ?\n"
'On my way through learning ML stuff I am confused by the MinMaxScaler provided by sklearn. The goal is to normalize numerical data into a range of [0, 1].\nExample code:\nfrom sklearn.preprocessing import MinMaxScaler\n\ndata = [[1, 2], [3, 4], [4, 5]]\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaledData = scaler.fit_transform(data)\n\nGiving output:\n[[0.         0.        ]\n [0.66666667 0.66666667]\n [1.         1.        ]]\n\nThe first array [1, 2] got transformed into [0, 0] which in my eyes means:\n\nThe ratio between the numbers is gone\nNone value has any importance (anymore) as they both got set to min-value (0).\n\nExample of what I have expected:\n[[0.1, 0.2]\n [0.3, 0.4]\n [0.4, 0.5]]\n\nThis would have saved the ratios and put the numbers into the range of 0 to 1.\nWhat am I doing wrong or misunderstanding with MinMaxScaler here? Because thinking of things like training on timeseries, it makes no sense to transform important numbers like prices or temperatures etc into broken stuff like above?\n'
'I want to convert images to tensor using torchvision.transforms.ToTensor(), after processing I printed the image but the image became so weird. Here is my code:\ntrans = transforms.Compose([\n    transforms.ToTensor()])\n\ndemo = Image.open(img) \ndemo_img = trans(demo)\ndemo_array = demo_img.numpy()*255\nprint(Image.fromarray(demo_array.astype(np.uint8)))\n\nThe original image is this\nBut after processing it is showed like this\nDid I write something wrong or miss something?\n'
"I have a df with a column that I want to filter for only negative or only positve values,\nwhen I try code below as:\ndf.loc[df['values'] &gt; 0]\n\nI get error of\n`TypeError: '&gt;' not supported between instances of 'str' and 'int'\n\nI try to convert the object data type of the values column to integer:\ndf['values'].astype(str).astype(int) \n\nI get error of : ValueError: invalid literal for int() with base 10: '3.69'\nThanks!\nHow Can I convert correctly so I can then filter correctly? Thanks!\n"
"I had been trying to get rid of stopwords using spacy library.\nCode\nimport spacy\nimport pandas as pd\nimport numpy as np\n\nnlp= spacy.load('en_core_web_sm')\n\nmy_series:\nmy_series\n\n0        this laptop sits at just over 4 stars while so...\n1        i ordered this monitor because i wanted to mak...\n2        this monitor is a great deal for the price and...\n3        bought this for the height adjustment. the swi...\n4        worked for a month and then it died. after 5 c...\n                               ...                        \n30618                                           great deal\n30619                                      pour le travail\n30620                                         business use\n30621                                            good size\n30622    pour mon ordinateur.plus grande image.vraiment...\nName: text_body, Length: 30623, dtype: object\n\nTokenize\ns_tokenized=my_series.apply(lambda x: nlp(x))\n\nRemove stopwords\nall_stopwords = nlp.Defaults.stop_words\nfiltered_text=s_tokenized.apply(lambda x: [w for w in x if not w in all_stopwords])\nfiltered_text\n\n0        [this, laptop, sits, at, just, over, 4, stars,...\n1        [i, ordered, this, monitor, because, i, wanted...\n2        [this, monitor, is, a, great, deal, for, the, ...\n3        [bought, this, for, the, height, adjustment, ....\n4        [worked, for, a, month, and, then, it, died, ....\n                               ...                        \n30618                                        [great, deal]\n30619                                  [pour, le, travail]\n30620                                      [business, use]\n30621                                         [good, size]\n30622    [pour, mon, ordinateur.plus, grande, image.vra...\nName: text_body, Length: 30623, dtype: object\n\ntokenize seems to be working fine but removing stopwords does not seems to remove any word at all nor raising any errors. Is there something I miss or did wrong?\n"
'I have a clear idea of what I would like to plot, but I am not sure where to start using matplotlib/seaborn.\nI have ~999 unequal lines of 0s, 1s, and 2. Here is an example of one line:\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1]\n\nI would like to make a horizontal plot where each number maps to a color of some set unit length.\nThis looks like a stacked bar plot. But I believe the the functionality of plt.bar would pool values such that there could only be three contiguous colors. However I need to implement the graph such that there could be any number of switches between colors.\n\n'
'I have the following data frame in the picture, i want to take a Plot a histogram to show the distribution of all countries in the world for any given year (e.g. 2010). \n\nFollowing is my code table generates after the following code of cleaning:\n\ndataSheet = pd.read_excel("http://api.worldbank.org/v2/en/indicator/EN.ATM.CO2E.PC?downloadformat=excel",sheetname="Data")\ndataSheet = dataSheet.transpose()\ndataSheet = dataSheet.drop(dataSheet.columns[[0,1]], axis=1) ;\ndataSheet = dataSheet.drop([\'World Development Indicators\', \'Unnamed: 2\',\'Unnamed: 3\'])\n\n\n\n'
'I would like to have a basic GUI with two text box inputs: one for each of the arguments in my function, convert_databases, but I\'m not sure how to pass those arguments (I\'ve seen some examples using lambda, but I wasn\'t able to implement them correctly).\n\nHere my attempt so far, which mostly came from Tkinter\'s native tutorial:\n\nfrom tkinter import *\nfrom tkinter import ttk\n\ndef convert_databases(input_file, output_format):\n    #Function deleted for simplicity\n\nroot = Tk()\nroot.title("Title")\n\n#Formatting\nmainframe = ttk.Frame(root, padding="3 3 12 12")\nmainframe.grid(column=0, row=0, sticky=(N, W, E, S))\nmainframe.columnconfigure(0, weight=1)\nmainframe.rowconfigure(0, weight=1)\n\n#Setting Variables\nfile = StringVar()\nconversion = StringVar()\n\n# Places to enter variables\nfile_entry = ttk.Entry(mainframe, width=50, textvariable=file)\nfile_entry.grid(column=2, row=2, sticky=(W, E))\ntype_entry = ttk.Entry(mainframe, width=50, textvariable=conversion)\ntype_entry.grid(column=2,row=3, sticky=(W,E))\n\n# Convert Button\nttk.Button(mainframe, text="Convert", command= # Here is where I\'m having trouble#)\n\n#Label for the variable 1 input\nttk.Label(mainframe, text="Input file name: ").grid(column=1, row=2, sticky=W)\n#Label for the variable 2 input\nttk.Label(mainframe, text="Input file type conversion: ").grid(column=1, row=3, sticky=W)\n\n# This sets the window, I think?\nfor child in mainframe.winfo_children(): child.grid_configure(padx=5, pady=5)\n\n# Puts the cursor automatically in the text box\nfile_entry.focus()\n\n# Runs the thing\nroot.mainloop()\n\n\nThanks!\n'
'| 1st Most Common Value | 2nd Most Common Value | 3rd Most Common Value | 4th Most Common Value | 5th Most Common Value |\n|-----------------------|-----------------------|-----------------------|-----------------------|-----------------------|\n| Grocery Store         | Pub                   | Coffee Shop           | Clothing Store        | Park                  |\n| Pub                   | Grocery Store         | Clothing Store        | Park                  | Coffee Shop           |\n| Hotel                 | Theatre               | Bookstore             | Plaza                 | Park                  |\n| Supermarket           | Coffee Shop           | Pub                   | Park                  | Cafe                  |\n| Pub                   | Supermarket           | Coffee Shop           | Cafe                  | Park                  |\n\n\nThe name of the dataframe is df0. As you can see there are many values repeating in all the columns. So I want to create a dataframe which has all the unique values with their frequencies from all the columns. Can someone please help with the code since I want to create a Bar plot of it?\n\nThe Output should be as follows: \n\n| Venues         | Count |\n|----------------|-------|\n| Bookstore      | 1     |\n| Cafe           | 2     |\n| Coffee Shop    | 4     |\n| Clothing Store | 2     |\n| Grocery Store  | 2     |\n| Hotel          | 1     |\n| Park           | 5     |\n| Plaza          | 1     |\n| Pub            | 4     |\n| Supermarket    | 2     |\n| Theatre        | 1     |\n\n'
"import pandas as pd\n\ndf=pd.DataFrame({'Edition_TypeDate': \n                [''2016'','5 Oct 2017','2017','2 Aug 2009','Illustrated, Import','Import, 22 Feb 2018','Import, 14 Dec 2017','Import, 1 Mar 2018','Abridged, Audiobook, Box set',\n'International Edition, 26 Apr 2012','Import, 2018','Box set, 15 Jun 2014','Unabridged, 6 Jul 2007']})\n\n\n\n\nI have one of the columns in my book dataset. Now From this column, I want three New columns.\n\n1.Edition_Type -->that includes Import, Illustrated or null if nothing is mentioned\n\n2.Edition_Month--->that includes Aug, Oct or null if nothing is mentioned\n\n3.Edition _Year--->that includes 2016,2017,2018 or null if nothing is mentioned\n\nHow to do it? Help me to def a function that can I apply to this.\n"
"\n\nAs a beginner, can someone please help me with this? Is there any way to show both pictures in a single cell's output? My output could only display one picture. Thank you for your time and attention!\nHere is my code:\nfrom skimage import data\nimage_coffee = data.coffee()\nimage_horse = data.horse()\nfig = plt.figure(dpi=100)\nplt.imshow(image_coffee)\nplt.imshow(image_horse)\n\n"
'Apologies for the wording of the question but I can illustrate my problem more clearly below.\nSuppose I have the following dataframe:\n\n\n\n\nRegion\nValue\n\n\n\n\nLondon North\n8\n\n\nNorth Yorkshire\n4\n\n\nLondon South\n6\n\n\nSouth Yorkshire\n6\n\n\n\n\nHow can I combine the London rows and the Yorkshire rows and produce the mean of their values?\n(This is a simplified example of the dataframe I am working with which consists of approx. 3000 rows.)\nI would like to get the following dataframe:\n\n\n\n\nWider Region\nAvg. Value\n\n\n\n\nLondon\n7\n\n\nYorkshire\n5\n\n\n\n'
"I created a dataset with 6 clusters and visualize it with the code below, and find the cluster center points for every iteration, now i want to visualize demonstration of update of the cluster centroids in KMeans algorithm. This demonstration should include first four iterations by generating 2×2-axis figure.\nI found the points but i cant plot them, can you please check out my code and by looking that, help me write the algorithm to scatter plot?\nHere is my code so far:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.datasets import make_blobs\ndata = make_blobs(n_samples=200, n_features=8, \n                           centers=6, cluster_std=1.8,random_state=101)\ndata[0].shape\nplt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='brg')\n\nplt.show()\nfrom sklearn.cluster import KMeans\n\nprint(&quot;First iteration points:&quot;)\nkmeans = KMeans(n_clusters=6,random_state=0,max_iter=1)\nkmeans.fit(data[0])\ncentroids=kmeans.cluster_centers_\nprint(kmeans.cluster_centers_)\nprint(&quot;Second iteration points:&quot;)\nkmeans = KMeans(n_clusters=6,random_state=0,max_iter=2)\nkmeans.fit(data[0])\nprint(kmeans.cluster_centers_)\nprint(&quot;Third iteration points:&quot;)\nkmeans = KMeans(n_clusters=6,random_state=0,max_iter=3)\nkmeans.fit(data[0])\nprint(kmeans.cluster_centers_)\nprint(&quot;Forth iteration points:&quot;)\nkmeans = KMeans(n_clusters=6,random_state=0,max_iter=4)\nkmeans.fit(data[0])\nprint(kmeans.cluster_centers_)\n\n"
"I want to perform a grid search cv on my dataframe.\nIn my pipeline I use a custom transformer to format data. Though when I print the shape of the data within my custom transformer it gets printed 11 times (the transformer is called 11 times)\nI thought it should be printed 10 times since it transforms train and test dataframes, 5 times each since it is a cross validation. So 5 x 2 = 10.\nBut a 11th shape is displayed, which is actually the dimensions of the full df (not separated into train/test)\nDo you know the reason of this 11th call?\nHere is in part the code to understand the problem:\ndef binary_data(df):\n    df.gender = df.gender.map({'Female': 0, 'Male': 1})\n    print(df.shape)\n    return df\n\npipeline = ColumnTransformer([('binarydata', FunctionTransformer(binary_data), ['gender'])])\nparam_grid = {}\nsearch = GridSearchCV(pipeline, param_grid, scoring='accuracy')\nsearch.fit(X, y)\n\nEDIT: the refit=True (default) flag is actually the reason of the extra call\n"
'I got a rather large pandas dataframe (5k rows, 30 cols). I need to do  as described below. I tried\npseudocode\n\nfor i in main_df.iterrows():\n    for j in sub_df.iterrows():\n        if j == part of i:\n            i[&quot;sub_uid&quot;] = j[&quot;sub_uid&quot;]\n\n\nBut this does not seem to work, or is just too hard to debug for me. (is also absurdly time consuming)\nI am basically out of ideas and hope for your help guys :)\n    main_df:\n        v1  v2  vx3 vx4\n    1   a   b   h   j\n    2   a   b   n   p\n    3   a   c   r   g\n    4   d   e   p   j\n    \n    sub_df: take only part of main_df columns, drop duplicates. Assign uids for all combinations of v1 v2 parameters\n        v1  v2  sub_uid\n    1   a   b   01\n    2   a   c   02\n    3   d   e   03\n    \n    now back to main_df: add a column for sub_uids. For each record, determine sub_uid using sub_df\n        v1  v2  vx3 vx4 sub_uid\n    1   a   b   h   j   01\n    2   a   b   n   p   01\n    3   a   c   r   g   02\n    4   d   e   p   j   03\n\n'
"I am experimenting with Numpy features and want to know if there is a way to achieve the desired behavior as explained.\nGiven a numpy array as shown below\narray = np.array([[1,3,3],[6,7,6],[9,9,4]])\nprint(array)\n\nOutput from print(array)\n[[1 3 3]\n [6 7 6]\n [9 9 4]]\n\nI am getting the maximum in each row\nmax_array =  array.max(axis=1,keepdims=1)\nprint(max_array)\n\nOutput from print(max_array)\n[[3]\n [7]\n [9]]\n\nI am applying the mask on all the maximum elements in the array using the code below\nmasked_array = np.ma.masked_equal(array,max_array)\nprint(masked_array)\n\nOutput from print(masked_array)\n[[1 -- --]\n [6 -- 6]\n [-- -- 4]]\n\nNow that I have masked the array, I want to perform operation to the non-max elements in the array like multiplication by 3 as show in the code below\n mul_array= np.multiply(masked_array,3)\n print(mul_array)\n\nOutput from print(mul_array)\n[[3 -- --]\n [18 -- 18]\n [-- -- 12]]\n\nI want to insert the maximum elements in the max_array which were masked earlier in the same position in the mul_array, but I couldn't find anything to achieve the desired behavior. Below is my expected matrix. I wanted to ask if there are any Numpy operation to achieve the desired behavior\nDesired output\n[[3 3 3]\n [18 7 18]\n [9 9 12]]\n\nThank you for helping!\n"
"While running the below code in jupyter notebook, I am getting the value error.\n\nValueError: Number of features of the model must match the input. Model n_features is 11 and input n_features is 2\n\nHow to resolve this issue?\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n\nI am getting the below error:\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-42-bc13e66e79fe&gt; in &lt;module&gt;\n      4 X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n      5                      np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n----&gt; 6 plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n      7              alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n      8 plt.xlim(X1.min(), X1.max())\n\n~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict(self, X)\n    627             The predicted classes.\n    628         &quot;&quot;&quot;\n--&gt; 629         proba = self.predict_proba(X)\n    630 \n    631         if self.n_outputs_ == 1:\n\n~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in predict_proba(self, X)\n    671         check_is_fitted(self)\n    672         # Check data\n--&gt; 673         X = self._validate_X_predict(X)\n    674 \n    675         # Assign chunk of trees to jobs\n\n~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py in _validate_X_predict(self, X)\n    419         check_is_fitted(self)\n    420 \n--&gt; 421         return self.estimators_[0]._validate_X_predict(X, check_input=True)\n    422 \n    423     @property\n\n~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py in _validate_X_predict(self, X, check_input)\n    394         n_features = X.shape[1]\n    395         if self.n_features_ != n_features:\n--&gt; 396             raise ValueError(&quot;Number of features of the model must &quot;\n    397                              &quot;match the input. Model n_features is %s and &quot;\n    398                              &quot;input n_features is %s &quot;\n\nValueError: Number of features of the model must match the input. Model n_features is 11 and input n_features is 2 \n\nFull Model Code: https://github.com/anandsinha07/Placement-prediction-system-using-ML-algos/blob/master/PREDICTION-Random%20Forest%20Classification/random_forest_classification.py\n"
"I have a '.p' archive and I never used this kind of extension in pandas, I tried to read this archive with the 'read_csv' function but I get an error.\nI tried:\nimport pandas as pd\n\ntaxi_owners = pd.read_csv('taxi_owners.p', encoding='utf-8')\n\nBut this doesn't work, and the result was:\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n\nThanks for your help\n"
"I am in the process of refining my code for a project where I am creating shipping lanes. What I currently have is dataframe that is put together by the index values of c_match. Cool, great everything looks correct at first glance.\nA shipping lane is a group of states with the same discount and min charge. My code returns states with the same discount. Most states that have the same discount also have the same min charge. However the outliers are states with the same discount and different min charges.\nThe goal: To create shipping lanes that have the same min charges and the same discount percentages.\nMy Idea: Create a logical operation that concats the state names who have identical rates and costs and also returns their rates and costs. States with different costs for the same rate still need to be accounted for.\nDesired Output:\nShipping Lane                                 Rate  Cost\n20_21_RDWY_Purple_AL_AR_KY_LA_MS_SC_TN_PE   50.80%  120\n20_21_RDWY_Purple_AZ                        50.80%  155\n20_21_RDWY_Purple_CA                        62.40%  145\n20_21_RDWY_Purple_CO_ND_WY_MB_NF_PQ         62.40%  155\n20_21_RDWY_Purple_CT_DE_MN_NE               50.00%  145\n20_21_RDWY_Purple_DC_IA_KS_MD_MI_OH_OK_WI   49.00%  125\n20_21_RDWY_Purple_FL                        48.30%  125\n\nCurrent Code:\ndef remove_dups(input, output):\n    input.sort()\n    n_list = list(input for input, _ in itertools.groupby(input))\n    output.append(n_list)\n\n\ndef get_matches_discount(state):\n    state_groups = []\n    state_rates = []\n    state_cost = []\n    final_format = []\n\n    match = []\n    c_match = []\n   \n    for i, x in enumerate(df_d[state]):\n        #checks within the column for identical values then maps where the identical values are\n        match1 = [j for j, y in enumerate(df_d[state].isin([x])) if y is True]\n        match.append(match1)\n        remove_dups(match, c_match)\n\n\n    for list in c_match:\n\n        for elements in list:\n            r = elements[0]\n            state_g = df_d.index[elements]\n            state_groups.append(state_g)\n\n            state_r = df_d[state][r]\n            state_rates.append(state_r)\n            print(state_rates)\n            match_cost = df_m[state][r]\n            state_cost.append(match_cost)\n\n    for i in state_groups:\n        delimiter = &quot;_&quot;\n\n        join_str = delimiter.join(i)\n\n        j_str = &quot;20_21_RDWY_Purple_&quot; + join_str\n\n        final_format.append(j_str)\n\n    master_frame = pd.DataFrame(\n        {'Shipping Lane': final_format,\n         'Rate': state_rates,\n         'Cost': state_cost,\n         }\n    )\n    print(master_frame)\n    return master_frame\n\n\nm_col_names = ['AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE', 'FL', 'GA', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA',\n               'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH',\n               'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY', 'AB', 'BC',\n               'MB', 'NB', 'NF', 'NS', 'ON', 'PE', 'PQ', 'SK']\n# calls the function in a loop to process one column at a time\n# creates the master data frame outside of the function calling for loop\nmaster_dataframe0 = pd.DataFrame()\nfor state in m_col_names:\n    temp_df = get_matches_discount(state)\n    # Stores the function call as a variable\n    master_dataframe0 = master_dataframe0.append(temp_df)\n    # Creates an appended dataframe outside of the function\nprint(master_dataframe0)\nmaster_dataframe0.to_excel(&quot;shipping_lanes_revised00.xlsx&quot;)\n\nSample input:\nMinimum Charge Table\nthis is dataframe: df_m\nState   AL     AR     AZ     CA       CO      CT     DC\nAL  120.00  120.00  155.00  145.00  155.00  145.00  125.00\nAR  120.00  120.00  155.00  155.00  145.00  155.00  145.00\nAZ  155.00  155.00  120.00  120.00  125.00  185.00  185.00\nCA  145.00  164.30  120.00  120.00  170.00  185.00  185.00\nCO  155.00  145.00  125.00  145.00  120.00  155.00  155.00\nCT  145.00  155.00  185.00  185.00  155.00  120.00  120.00\nDC  125.00  155.00  185.00  185.00  155.00  120.00  185.00\nDE  145.00  155.00  185.00  185.00  155.00  120.00  120.00\nFL  125.00  145.00  145.00  185.00  145.00  155.00  145.00\nGA  120.00  120.00  155.00  145.00  155.00  145.00  120.00\nIA  125.00  125.00  155.00  145.00  125.00  155.00  145.00\nID  145.00  155.00  145.00  145.00  125.00  185.00  185.00\nIL  120.00  120.00  155.00  145.00  145.00  125.00  125.00\nIN  120.00  120.00  155.00  145.00  145.00  125.00  120.00\nKS  125.00  120.00  155.00  155.00  120.00  155.00  145.00\nKY  120.00  120.00  155.00  145.00  145.00  125.00  125.00\nLA  120.00  120.00  155.00  145.00  155.00  155.00  155.00\nMA  155.00  155.00  185.00  185.00  145.00  120.00  120.00\nMD  125.00  145.00  185.00  185.00  155.00  120.00  120.00\nME  155.00  155.00  185.00  185.00  145.00  120.00  125.00\nMI  125.00  125.00  145.00  145.00  155.00  125.00  120.00\nMN  145.00  125.00  155.00  145.00  145.00  155.00  145.00\nMO  120.00  120.00  155.00  155.00  125.00  145.00  145.00\nMS  120.00  120.00  155.00  155.00  145.00  155.00  145.00\nMT  145.00  155.00  155.00  155.00  125.00  185.00  185.00\nNC  120.00  125.00  145.00  185.00  155.00  125.00  120.00\nND  155.00  155.00  145.00  145.00  155.00  155.00  155.00\nNE  145.00  125.00  155.00  155.00  120.00  155.00  155.00\nNH  155.00  155.00  185.00  185.00  145.00  120.00  120.00\nNJ  145.00  155.00  185.00  185.00  155.00  120.00  120.00\nNM  155.00  125.00  120.00  145.00  120.00  145.00  145.00\nNV  145.00  155.00  120.00  120.00  145.00  185.00  185.00\nNY  145.00  145.00  185.00  185.00  155.00  120.00  120.00\nOH  125.00  125.00  145.00  145.00  155.00  120.00  120.00\nOK  125.00  120.00  145.00  155.00  120.00  155.00  155.00\nOR  185.00  145.00  155.00  125.00  155.00  185.00  185.00\nPA  145.00  145.00  185.00  185.00  155.00  120.00  120.00\nRI  155.00  155.00  185.00  185.00  145.00  120.00  120.00\nSC  120.00  120.00  145.00  185.00  155.00  125.00  120.00\nSD  155.00  145.00  155.00  155.00  120.00  155.00  145.00\nTN  120.00  120.00  155.00  145.00  155.00  145.00  125.00\nTX  125.00  120.00  145.00  155.00  125.00  145.00  155.00\nUT  170.00  164.30  132.50  132.50  127.20  145.00  145.00\nVA  120.00  145.00  145.00  185.00  155.00  120.00  120.00\n   \n\nDiscount Table\nthis is datatframe: df_d\nState   AL      AR      AZ      CA      CO     CT     DC\n    AL  50.80%  44.10%  54.30%  73.10%  53.90%  50.00%  49.00%\n    AR  50.80%  50.80%  53.90%  65.70%  50.00%  53.90%  50.00%\n    AZ  56.70%  55.80%  50.80%  54.10%  49.60%  59.50%  64.40%\n    CA  62.40%  61.00%  54.30%  61.40%  43.00%  52.30%  54.30%\n    CO  54.30%  67.10%  49.00%  65.70%  50.80%  54.30%  54.30%\n    CT  50.00%  53.90%  64.40%  72.50%  54.30%  50.80%  50.80%\n    DC  49.00%  53.90%  64.40%  64.40%  54.30%  50.80%  64.40%\n    DE  50.00%  53.90%  64.40%  64.40%  54.30%  50.80%  50.80%\n    FL  48.30%  35.00%  55.50%  55.50%  55.10%  66.40%  62.30%\n    GA  67.90%  44.10%  71.00%  64.60%  56.00%  50.00%  44.10%\n    IA  49.00%  49.00%  54.30%  61.80%  49.00%  53.90%  50.00%\n    ID  61.80%  54.30%  50.00%  75.90%  49.00%  64.40%  64.40%\n    IL  44.10%  44.10%  54.30%  64.00%  50.00%  49.00%  49.00%\n    IN  44.10%  1.60%   11.70%  26.10%  -0.70%  49.00%  44.10%\n    KS  49.00%  63.40%  61.00%  67.70%  72.50%  72.20%  50.00%\n    KY  50.80%  44.10%  54.30%  61.50%  50.00%  49.00%  49.00%\n    LA  50.80%  44.10%  54.30%  61.80%  53.90%  54.30%  53.90%\n    MA  63.50%  53.90%  67.70%  63.90%  53.00%  63.50%  44.10%\n    MD  49.00%  50.00%  64.40%  73.80%  54.30%  50.80%  50.80%\n    ME  53.90%  54.30%  64.40%  64.40%  61.80%  50.80%  49.00%\n    MI  49.00%  49.00%  61.80%  55.10%  53.90%  49.00%  44.10%\n    MN  50.00%  49.00%  54.30%  61.80%  50.00%  53.90%  50.00%\n    MO  44.10%  50.80%  53.90%  56.10%  49.00%  50.00%  50.00%\n    MS  50.80%  50.80%  54.30%  63.90%  50.00%  53.90%  50.00%\n    MT  61.80%  54.30%  53.90%  75.80%  49.00%  64.40%  64.40%\n    NC  44.10%  59.20%  53.50%  58.60%  57.90%  42.90%  69.60%\n    ND  54.30%  53.90%  61.80%  61.80%  54.30%  53.90%  53.90%\n    NE  50.00%  49.00%  54.30%  54.30%  44.10%  53.90%  53.90%\n    NH  53.90%  54.30%  64.40%  64.40%  61.80%  50.80%  44.10%\n    NJ  50.50%  51.50%  70.50%  66.20%  59.70%  67.10%  50.80%\n    NM  53.90%  49.00%  44.10%  68.20%  44.10%  61.80%  61.80%\n    NV  61.80%  54.30%  52.70%  73.50%  50.00%  64.40%  64.40%\n    NY  61.10%  69.00%  65.50%  68.90%  63.00%  68.40%  50.80%\n    OH  49.00%  49.00%  68.50%  71.50%  72.30%  60.70%  44.10%\n    OK  49.00%  50.80%  50.00%  54.30%  44.10%  54.30%  54.30%\n    OR  64.40%  61.80%  53.90%  64.00%  53.90%  64.40%  64.40%\n    PA  47.20%  57.00%  33.70%  51.90%  45.50%  50.80%  50.80%\n    RI  53.90%  54.30%  64.40%  64.40%  61.80%  50.80%  44.10%\n    SC  50.80%  44.10%  61.80%  58.70%  54.30%  49.00%  44.10%\n    SD  53.90%  50.00%  54.30%  54.30%  44.10%  54.30%  61.80%\n    TN  50.80%  50.80%  52.50%  62.60%  61.30%  53.30%  49.00%\n    TX  56.60%  46.00%  51.40%  58.30%  53.20%  63.10%  65.10%\n    UT  45.00%  60.60%  73.50%  73.50%  70.30%  44.40%  61.90%\n    VA  57.90%  50.00%  61.80%  72.10%  54.30%  44.10%  50.80%\n\nCurrent Output:\n                                                                      Shipping Lane     Rate   Cost\n0                                             20_21_RDWY_Purple_AL_AR_KY_LA_MS_SC_TN_PE   50.80%  120.0\n1                                                                  20_21_RDWY_Purple_AZ   56.70%  155.0\n2                                                                  20_21_RDWY_Purple_CA   62.40%  145.0\n3                                                   20_21_RDWY_Purple_CO_ND_WY_MB_NF_PQ   54.30%  155.0\n4                                                         20_21_RDWY_Purple_CT_DE_MN_NE   50.00%  145.0\n5                                             20_21_RDWY_Purple_DC_IA_KS_MD_MI_OH_OK_WI   49.00%  125.0\n6                                                                  20_21_RDWY_Purple_FL   48.30%  125.0\n7                                                                  20_21_RDWY_Purple_GA   67.90%  120.0\n8                                                      20_21_RDWY_Purple_ID_MT_NV_AB_SK   61.80%  145.0\n9                                                      20_21_RDWY_Purple_IL_IN_MO_NC_WV   44.10%  120.0\n10                                                                 20_21_RDWY_Purple_MA   63.50%  155.0\n11                                            20_21_RDWY_Purple_ME_NH_NM_RI_SD_VT_NB_NS   53.90%  155.0\n12                                                                 20_21_RDWY_Purple_NJ   50.50%  145.0\n13                                                                 20_21_RDWY_Purple_NY   61.10%  145.0\n14                                                           20_21_RDWY_Purple_OR_WA_BC   64.40%  185.0\n15                                                                 20_21_RDWY_Purple_PA   47.20%  145.0\n16                                                                 20_21_RDWY_Purple_TX   56.60%  125.0\n17                                                                 20_21_RDWY_Purple_UT   45.00%  170.0\n18                                                                 20_21_RDWY_Purple_VA   57.90%  120.0\n19                                                                 20_21_RDWY_Purple_ON   37.30%  145.0\n0                                                   20_21_RDWY_Purple_AL_GA_IL_KY_LA_SC   44.10%  120.0\n1                                          20_21_RDWY_Purple_AR_MO_MS_OK_TN_NB_NF_NS_PE   50.80%  120.0\n2                                                                  20_21_RDWY_Purple_AZ   55.80%  155.0\n3                                                                  20_21_RDWY_Purple_CA   61.00%  164.3\n4                                                                  20_21_RDWY_Purple_CO   67.10%  145.0\n5                                                   20_21_RDWY_Purple_CT_DC_DE_MA_ND_MB   53.90%  155.0\n6                                                                  20_21_RDWY_Purple_FL   35.00%  145.0\n7                                             20_21_RDWY_Purple_IA_MI_MN_NE_NM_OH_WI_WV   49.00%  125.0\n8                                          20_21_RDWY_Purple_ID_ME_MT_NH_NV_RI_VT_PQ_SK   54.30%  155.0\n9                                                                  20_21_RDWY_Purple_IN    1.60%  120.0\n10                                                                 20_21_RDWY_Purple_KS   63.40%  120.0\n11                                                        20_21_RDWY_Purple_MD_SD_VA_WY   50.00%  145.0\n12                                                                 20_21_RDWY_Purple_NC   59.20%  125.0\n13                                                                 20_21_RDWY_Purple_NJ   51.50%  155.0\n14                                                                 20_21_RDWY_Purple_NY   69.00%  145.0\n15                                                           20_21_RDWY_Purple_OR_WA_AB   61.80%  145.0\n16                                                                 20_21_RDWY_Purple_PA   57.00%  145.0\n17                                                                 20_21_RDWY_Purple_TX   46.00%  120.0\n18                                                                 20_21_RDWY_Purple_UT   60.60%  164.3\n19                                                                 20_21_RDWY_Purple_BC   64.40%  185.0\n20                                                                 20_21_RDWY_Purple_ON   32.10%  145.0\n0                              20_21_RDWY_Purple_AL_CA_IA_IL_KY_LA_MN_MS_NE_SD_WA_AB_BC   54.30%  155.0\n1                                                         20_21_RDWY_Purple_AR_MO_MT_OR   53.90%  155.0\n2                                                      20_21_RDWY_Purple_AZ_NB_NF_NS_PE   50.80%  120.0\n3                                                                  20_21_RDWY_Purple_CO   49.00%  125.0\n4                                    20_21_RDWY_Purple_CT_DC_DE_MD_ME_NH_RI_VT_ON_PQ_SK   64.40%  185.0\n5                                                                  20_21_RDWY_Purple_FL   55.50%  145.0\n6                                                                  20_21_RDWY_Purple_GA   71.00%  155.0\n7                                                            20_21_RDWY_Purple_ID_OK_WY   50.00%  145.0\n8                                                                  20_21_RDWY_Purple_IN   11.70%  155.0\n9                                                                  20_21_RDWY_Purple_KS   61.00%  155.0\n10                                                                 20_21_RDWY_Purple_MA   67.70%  185.0\n11                                                  20_21_RDWY_Purple_MI_ND_SC_VA_WV_MB   61.80%  145.0\n12                                                                 20_21_RDWY_Purple_NC   53.50%  145.0\n13                                                                 20_21_RDWY_Purple_NJ   70.50%  185.0\n14                                                                 20_21_RDWY_Purple_NM   44.10%  120.0\n15                                                                 20_21_RDWY_Purple_NV   52.70%  120.0\n16                                                                 20_21_RDWY_Purple_NY   65.50%  185.0\n17                                                                 20_21_RDWY_Purple_OH   68.50%  145.0\n18                                                                 20_21_RDWY_Purple_PA   33.70%  185.0\n19                                                                 20_21_RDWY_Purple_TN   52.50%  155.0\n20                                                                 20_21_RDWY_Purple_TX   51.40%  145.0\n\n\n  \n\n"
'I have some x &amp; y columns in a dataframe such as below:\n          X-1    X-1_y         X-2    X-2_y         X-3    X-3_y\n0   411.726266  1387.29  437.404307  3755.08  437.273585  3360.85\n1   437.692665   677.39  448.557534  1460.70  448.760155   981.45\n2   448.596937  2276.35  481.550490     0.00  481.566018     0.00\n3   481.634531     0.00  486.966310     0.00  487.208899     0.00\n4   486.971163     0.00  492.578155     0.00  492.446192     0.00\n5   492.505388     0.00  500.000000   608.22  500.153040     0.00\n6   500.030500   810.45  508.218825     0.00  508.315935     0.00\n7   508.106596     0.00  513.579177     0.00  513.620953  9582.45\n8   513.424161     0.00  515.308245     0.00  515.175867     0.00\n9   535.131828     0.00  534.346333     0.00  534.985459     0.00\n10  551.779516  3124.92  551.712654  2226.94  551.680943  2522.73\n11  559.050425  1081.89  559.084859   984.05  559.087271  1600.48\n12  562.108257  3532.11  562.253910  3686.94  562.234223  4495.73\n13  591.436797     0.00  590.659433     0.00  591.396752     0.00\n\nand I would like to align all 3 X columns and merge it to 1 X column. if the numbers in X columns are too close to each other (i.e +- 1) get a avg of the three if available but if the numbers are not close to each other, append a new row, so the final result would be a new dataframe like this :\n         avg X    X-1_y    X-2_y    X-3_y\n0   411.726266  1387.29     0.00     0.00\n1   437.456852   677.39  3755.08  3360.85\n2   448.638209  2276.35  1460.70   981.45\n3   481.583680     0.00     0.00     0.00\n4   487.048791     0.00     0.00     0.00\n5   492.509912     0.00     0.00     0.00\n6   500.061180   810.45   608.22     0.00\n7   508.213785     0.00     0.00     0.00\n8   513.541430     0.00     0.00  9582.45\n9   515.242056     0.00     0.00     0.00\n10  534.821206     0.00     0.00     0.00\n11  551.724371  3124.92  2226.94  2522.73\n12  559.074185  1081.89   984.05  1600.48\n13  562.198797  3532.11  3686.94  4495.73\n14  591.164327     0.00     0.00     0.00\n\nexample of how the result is created:\nif the numbers in a row of X are +- 1 then get a avg, if all 3 are not within +-1, then append three new rows but if 1 is not within the other two then append 2 rows (1 is the new off value and 2nd is the avg of the other 2 that are within +-1). for example , on first row of data,\n        X-1    X-1_y         X-2    X-2_y         X-3    X-3_y\n0   411.726266  1387.29  437.404307  3755.08  437.273585  3360.85\n1   437.692665   677.39  448.557534  1460.70  448.760155   981.45\n\nX1(411.72) is not within +-1 of the X2(437.4) and X3(437.2) so it will append a new line in the result but  X2(437.4) and X3(437.2) are within +-1 of each other and also within +-1 of the 2nd row of the X1(437.692) so append a avg of the 3 in the next line avg of (X1_row2 , X2_row1 , X3_row1)\nresults will be\n         avg X    X-1_y    X-2_y    X-3_y\n0   411.726266  1387.29     0.00     0.00\n1   437.456852   677.39  3755.08  3360.85\n\nthanks in advance\n'
"I am trying to compute a ratio or % that takes the number of occurrences of a grouped by column (Service Column) that has at least one of two possible values (Food or Beverage)  and then divide it over the number of unique column (Business Column) values in the df but am having trouble.\nOriginal df:\nRep      | Business | Service\nCindy    Shakeshake    Food\nCindy    Shakeshake    Outdoor\nKim      BurgerKing    Beverage\nKim      Burgerking    Phone\nKim      Burgerking    Car\nNate     Tacohouse     Food\nNate     Tacohouse     Car\nTim      Cofeeshop     Coffee\nTim      Coffeeshop    Seating\nCindy    Italia        Seating\nCindy    Italia        Coffee\n\n\n\n Desired Output:\n  Rep    | %\n  Cindy    .5\n  Kim       1\n  Nate      1\n  Tim       0\n\nWhere % is the number of Businesses cindy has with at least 1 Food or Beverage row divided by all unique Businesses in df for her.\nI am trying something like below:\n     (df.assign(Service=df.Service.isin(['Food','Beverage']).astype(int))\n       .groupby('Rep')\n       .agg({'Business':'nunique','Service':'count'}))\n\ns['Service']/s['Business']\n\nBut this doesnt give me what im looking for as the service only gives all rows in df for cindy in this case 4 and the Businees column isnt giving me an accurate # of where she has food or beverage in a grouped by business.\nThanks for looking and possible help in advance.\n"
"I have a csv file that contains data from captured network packets, including the column src_ip which stores the source IP and port numbers in the format 192.168.1.1:1234. I'm currently using the following code to split the IP and port and only save the IP:\npkt_data = pd.read_csv(&quot;sample_packets.csv&quot;, parse_dates=[&quot;Time&quot;]).set_index(&quot;Time&quot;).sort_index()\nfor index_label, row_series in pkt_data.iterrows():\n    pkt_data.at[index_label, &quot;src_ip&quot;] = row_series[&quot;src_ip].split(':')[0]\n\nThe problem is that my data set is about 200k rows and this code takes close to a minute to finish.\nIs there a faster / better way to do this?\nThanks\n"
"I am trying to filter non-English apps out of a data set for a problem I am working on.\nHow can non-English apps be removed from the data set? An initial approach will be to check whether the string can be encoded with only ASCII characters. If the string cannot be encoded with only ASCII characters, then the string has characters from some other alphabet or special characters.\nTesting this approach on some toy examples yields:\ndef is_english(app_name):\ntry:\n    app_name.encode(encoding='utf-8').decode('ascii')\nexcept UnicodeDecodeError:\n    return False\nelse:\n    return True\n\nprint(is_english('Instagram'))\nprint(is_english('爱奇艺PPS -《欢乐颂2》电视剧热播'))\nprint(is_english('Docs To Go™ Free Office Suite'))\nprint(is_english('Instachat 😜'))\n\nObviously, there is an issue with the initial approach, namely that 'Docs To Go™ Free Office Suite' and 'Instachat 😜', both English apps, are being recognized as non-English apps because they have special characters (i.e. '™' and '😜').\nAny suggestions on how to allow for special characters like '™', Emojis, etc.?\n"
"I am loading a csv file into a data frame using pandas.\nMy dataframe looks something like this:\ncol1       col2       col3         \n1           4           1 \n2           5           2\n3           6           3\n\nI wish to append 2 of the columns into a new column:\n  col1       col2        col3       col4   \n    1           4           1         1\n    2           5           2         2\n    3           6           3         3\n                                      4\n                                      5 \n                                      6\n\ncol4 needs to be created by appending the contents of col1 and col2 together.\nHow can I do this in pandas/python?\nEDIT\ndf = df.reset_index(drop=True)\n\ns = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ndf = df.join(s, how='outer')\n\ndf = df.reset_index(drop=True)\n\ns = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\ndf = df.join(s, how='outer')\n\n"
"Okay. This might be lame and silly doubt for most of you.\nI have trouble in accessing the values from column index 1 in the variable 'true_p'.This is how the variable looks like.\ntrue_p.head()\n507     80.0\n2920    44.6\n1464    74.7\n1158    74.8\n282     56.5\nName: life_exp, dtype: float64\n\nNow i need to access the values in column in 1st index(i.e, 80.0, 44.6, 74.7 etc) and add it in another dataframe.\nPS: true_p is a variable that is obtained from train_test_split() function where I've placed true prediction values in this variable.\nThank you\n"
"I'm trying to create a certain prediction model. I decided to do the entire data manipulation with\npython this time instead of with DAX (pbi) to learn python.\nI have 2 columns that are relevant for this question : Customer and Date.\nEach row is an invoice created for that customer at that certain date.\nI'm trying for each row (invoice) to get the previous invoice date. It is important to understand\nthat the new column must take in consideration the customer, that means that for each customer's invoice i want to get the previous date of invoice.\nThis is how it looks like :\ndata = [['A', 17/07/2020], ['B', 15/07/2020], ['C', 14/07/2020], ['C', 10/07/2020], ['B', 09/07/2020]]\ndf = pd.DataFrame(data, columns = ['Customer', 'Date'])\nprint(df)\n\nCustomer Date\nA        17/07/2020\nB        15/07/2020\nC        14/07/2020\nC        10/07/2020\nB        09/07/2020  \n\nThis is the result i seek :\nCustomer Date        PrevInvoiceDate\nA        17/07/2020  NaT\nB        15/07/2020  09/07/2020\nC        14/07/2020  10/07/2020\nC        10/07/2020  NaT\nB        09/07/2020  NaT\n\nI tried to use pandas' shift, loc, filter etc.. but with no luck,\nI would be grateful to learn the right way to do this kind of manipulation and learn from you all.\nThank you.\n"
"How do I merge similar data such as &quot;recommendation&quot; into one value?\ndf['Why you choose us'].str.lower().value_counts()\n\nlocation                           35\nrecommendation                     23\nrecommedation                       8\nconfort                             7\navailability                        4\nreconmmendation                     3\nfacilities                          3\n\n\n"
"I have a dataframe df:\n   Serial_no       date  Index     x    y\n           1 2014-01-01      1   2.0  3.0\n           1 2014-03-01      2   3.0  3.0\n           1 2014-04-01      3   6.0  2.0\n           2 2011-03-01      1   5.1  1.3\n           2 2011-04-01      2   5.8  0.6\n           2 2011-05-01      3   6.5 -0.1\n           2 2011-07-01      4   3.0  5.0\n           3 2019-10-01      1   7.9 -1.5\n           3 2019-11-01      2   8.6 -2.2\n           3 2020-01-01      3  10.0 -3.6\n           3 2020-02-01      4  10.7 -4.3\n           3 2020-03-01      5   4.0  3.0\n\nNotice:\nThe data is grouped by Serial_no and the date is data reported monthly (first of every month).\nThe Index column is set so each consecutive reported date is a consecutive number in the series.\nThe number of reported dates in each group Serial_no are different.\nThe interval of reported dates date are different for each group Serial_no (they don't start or end on the same date for each group).\nThe problem:\nThere is no reported data for some dates date in the time series. Notice some dates are missing in each Serial_no group.\nI want to add a row in each group for those missing dates date and have the data reported in x and y columns as 'NaN'.\nExample of the dataframe I need:\n   Serial_no       date  Index       x       y\n           1 2014-01-01      1     2.0     3.0\n           1 2014-02-01      2     NaN     NaN\n           1 2014-03-01      3     3.0     3.0\n           1 2014-04-01      4     6.0     2.0\n           2 2011-03-01      1     5.1     1.3\n           2 2011-04-01      2     5.8     0.6\n           2 2011-05-01      3     6.5    -0.1\n           2 2011-06-01      4     NaN     NaN\n           2 2011-07-01      5     3.0     5.0\n           3 2019-10-01      1     7.9    -1.5\n           3 2019-11-01      2     8.6    -2.2\n           3 2019-12-01      3     NaN     NaN\n           3 2020-01-01      4    10.0    -3.6\n           3 2020-02-01      5    10.7    -4.3\n           3 2020-03-01      6     4.0     3.0\n\nI know how to replace the blank cells with NaN once the rows with missing dates are inserted, using the following code:\nimport pandas as pd\nimport numpy as np\n\ndf['x'].replace('', np.nan, inplace=True)\ndf['y'].replace('', np.nan, inplace=True)\n\nI also know how to reset the index once the rows with missing dates are inserted, using the following code:\ndf[&quot;Index&quot;] = df.groupby(&quot;Serial_no&quot;,).cumcount('date')\n\nHowever, I'm unsure how to locate the the missing dates in each group and insert the row for those (monthly reported) dates. Any help is appreciated.\n"
'I have a dataframe like \n\n|x |y |z |\n|na|na|1 |\n|na|2 |1 |\n|2 |3 |1 |\n|na|na|1 |\n\n\nand I want to shift the NA values so the NA elements will move to the last keeping the order of the non-na values columns like\n\n|x |y |z |\n|1 |na|na|\n|2 |1 |na|\n|2 |3 |1 |\n|1 |na|na|\n\n'
'I work in data science and a typical problem I encounter while cleaning up Pandas dataframes is converting columns from one string format to another (in particular, the strings I\'m looking at are chemical identifiers and each of them represents a molecule in a obscure way, so it\'s not like the strings are easily understandable just by looking at them). I have many small functions (inherited from a chemical library called RDKit) to convert between formats, and there is roughly one function per conversion pair (i.e. input format and output format). This is too many function names to remember. I want to write a wrapper function that aggregates all of them into a single, larger one with a clean design and user interface.\n\nThe question is: given an input and output format, what would be a clean way to select from a many possible small conversion functions? Should I use a dictionary that stores the small conversion function names?\n\nFor example, let\'s say I want to convert from the format "smiles" to the format "inchi keys",  which I currently do as follows:\n\nfrom rdkit import Chem\n\n\ndef smile2inchikey(smile):\n    mol = Chem.MolFromSmiles(smile)\n    inchikey = Chem.inchi.MolToInchiKey(mol)\n    return inchikey\n\n\nInstead of manually calling smile2inchikey  (or Chem.MolFromSmiles and Chem.inchi.MolToInchiKey), I would like to write the following function:\n\ndef fancy_multiconverter(input_string, input_format, output_format):\n    pass\n\n\nwhich returns input_string (given in the format input_format) to the format output_format).\n'
'I do have a large dataset (around 8 million rows x 25 columns) in Pandas and I am struggling to do one operation in a performant manner. \n\nHere is how my dataset looks like:\n\n                   temp   size\nlocation_id hours             \n135         78     12.0  100.0\n            79      NaN    NaN\n            80      NaN    NaN\n            81     15.0  112.0\n            82      NaN    NaN\n            83      NaN    NaN\n            84     14.0   22.0\n\n\n\nI have a multi-index on [location_id, hours]. I have around 60k locations and 140 hours for each location (making up the 8 million rows).\nThe rest of the data is numeric (float). I have only included 2 columns here, normally there are around 20 columns.\nWhat I am willing to do is to fill those NaN values by using the values around it. Basically, the value of hour 79 will be derived from the values of 78 and 81. For this example, the temp value of 79 will be 13.0 (basic extrapolation).\nI always know that only the 78, 81, 84 (multiples of 3) hours will be filled and the rest will have NaN. That will always be the case. This is true for hours between 78-120.\nWith these in mind, I have implemented the following algorithm in Pandas:\n\n\ndf_relevant_data = df.loc[(df.index.get_level_values(1) &gt;= 78) &amp; (df.index.get_level_values(1) &lt;= 120), :]\n\nfor location_id, data_of_location_id in df_relevant_data.groupby("location_id"):\n\n        for hour in range(81, 123, 3):\n\n            top_hour_data = data_of_location_id.loc[(location_id, hour), [\'temp\', \'size\']] # e.g. 81\n            bottom_hour_data = data_of_location_id.loc[(location_id, (hour - 3)), [\'temp\', \'size\']] # e.g. 78\n\n            difference = top_hour_data.values - bottom_hour_data.values\n            bottom_bump = difference * (1/3) # amount to add to calculate the 79th hour\n            top_bump = difference * (2/3) # amount to add to calculate the 80th hour\n\n            df.loc[(location_id, (hour - 2)), [\'temp\', \'size\']] = bottom_hour_data.values + bottom_bump\n            df.loc[(location_id, (hour - 1)), [\'temp\', \'size\']] = bottom_hour_data.values + top_bump\n\n\n\n\nThis works really well functionally, however the performance is horrible. It is taking at least 10 minutes on my dataset and that is currently not acceptable.\nIs there a better/faster way to implement this? I am actually working only on a slice of the whole data (only hours between 78-120) so I would really expect it to work much faster.\n\n'
'I\'m currently trying to normalize a DataFrame(~600k rows) with prices (pricevalue) in different currencies(pricecurrency) so that every row has prices in EUR.\n\nI\'d like to convert them with the daily rate taken from a column date.\n\nMy current "solution" (using the CurrencyConverter package found on PyPI) looks like this:\n\nfrom currency_converter import CurrencyConverter\n\nc = CurrencyConverter(fallback_on_missing_rate=True,fallback_on_missing_rate_method="last_known")\n\ndef convert_currency(row):\n     return c.convert(row["pricevalue"], row["pricecurrency"],row["date"])\n\ndf["converted_eur"] = df.apply(lambda x: convert_currency(x),axis=1)\n\n\n\nHowever, this solution is taking forever to run.\n\nIs there a faster way to accomplish that? Any help is appreciated :)\n'
"I have got a Dataframe , i want to find which store has good quarterly growth rate in Q3\n\n  Store    Date     Weekly_Sales\n0   1   2012-03-31  18951097.69\n1   1   2012-06-30  21036965.58\n2   1   2012-09-30  18633209.98\n3   1   2012-12-31  9580784.77\n4   2   2012-03-31  22543946.63\n5   2   2012-06-30  25085123.61\n6   2   2012-09-30  22396867.61\n7   2   2012-12-31  11470757.52 \n\n\nI managed to loop through the items and got this far but after that i am unable to find any way. I think i have to go to the next value and get the sales and then add it,but i m not sure how to do that. I want to compare index 1 and 2 of Store 1 and find growth rate , again doing the same thing for Store 2 here index 5 and 6 and So on as i have total 45 Stores available.\n\nnew_df = []\nfor index, row in monthly_sales.iterrows():\n    if index == 1:  ----Not sure what condition to put here \n      q2 = row['Weekly_Sales']\n      q3 = row['Weekly_Sales']\n      growth_rate = (q3 - q2)/(q2*100)\n      new_df.append([row['Store'],growth_rate])\n      #print(index, row['Store'],row['Date'], row['Weekly_Sales'])\n      #exit;\nnew_df\n\n\nOutput can be something like this\n\n  Store Growth Rate\n0   1      6.67890\n1   2      5.54327\n\n\nI am a newbie to Python and Pandas.\n"
'I have a dask dataframe with thousands of columns and rows as follows:\n\npprint(daskdf.head())\n   grid     lat      lon  ...  2014-12-29  2014-12-30  2014-12-31\n0     0  48.125 -124.625  ...         0.0         0.0  -17.034216\n1     0  48.625 -124.625  ...         0.0         0.0  -19.904214\n4     0  42.375 -124.375  ...         0.0         0.0   -8.380443\n5     0  42.625 -124.375  ...         0.0         0.0   -8.796803\n6     0  42.875 -124.375  ...         0.0         0.0   -7.683688\n\n\nI want to count all occurrences in the entire dataframe where a certain value appears. In pandas, this can be done as follows:\n\npddf[pddf==500].count().sum()\n\n\nI\'m aware that you can\'t translate all pandas functions/syntax with dask, but how would I do this with a dask dataframe? I tried doing:\n\ndaskdf[daskdf==500].count().sum().compute()\n\n\nbut this yielded a "Not Implemented" error.\n'
"I recently got this dataset which is too large for my RAM.\nI have to read it in chunks using\n\npd.read_csv('filename.csv', chunksize=1024)\n\nAnd all the labels in the data set are continuous, i.e. all the zeros are together, and ones, and twos.\nThere are 12000 of each label, so each chunk has all zeros or ones or twos.\n\nThe problem that I have is even if I use randomize and test_train_split, I still get all same labels in my train data.\nAs a result of this, my model learns to output one value for any input.\nThe constant output depends on the random seed. I need to know how to fix this error.\n\nEDIT:\nHere is the code as requested\n\ndata_in_chunks = pd.read_csv(data_file, chunksize=4096)\ndata = next(iter(data_in_chunks)\nX = data.drop(['labels'], axis=1)\nY = data.labels\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, stratify=Y, random_state=0) # train test random state has no effect\nfor i in iter(data_in_chunks):\n    train(i) # this is just simplified i used optim in the actual code\n\n\nso to explain the problem in other words, 4096 is the highest chunksize my 16 gigs of ram can handle, and due to the sequential nature of all the labels, all my Y_train, and Y_test has only 0, or 1 or 2 (all the possible outputs)\n\nPlease help\nThanks in advance\n"
'I am new to the realm of neural networks and just going through my first actual working sample, using the hand written digits MNIST dataset. I have written a code which as far as I can think should be working (at least to some level), but I cannot figure out what makes it get stuck right after reading the first training sample. My code is the following:\n\nfrom keras.datasets import mnist\nimport numpy as np\n\ndef relu(x):\n    return (x &gt; 0) * x\n\ndef relu_deriv(x):\n    return x &gt; 0\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data();\n\nimages = x_train[0:1000].reshape(1000, 28*28)\nlabels = y_train[0:1000]\n\ntest_images = x_test[0:1000].reshape(1000, 28*28)\ntest_labels = y_test[0:1000]\n\n# converting the labels to a matrix\none_hot_labels = np.zeros((len(labels),10))\nfor i,j in enumerate(labels):\n    one_hot_labels[i][j] = 1\nlabels = one_hot_labels\n\n\nalpha = 0.005\nhidden_size = 5 # size of the hidden layer\n\n# initial weight matrixes\nw1 = .2 * np.random.random(size=[784, hidden_size]) - .1\nw2 = .2 * np.random.random(size=[hidden_size, 10]) - .1\n\nfor iteration in range(1000):\n    error = 0\n    for i in range(len(images)):\n        layer_0 = images[i:i+1]\n        layer_1 = relu(np.dot(layer_0, w1))\n        layer_2 = np.dot(layer_1, w2)\n        delta_2 = (labels[i:i+1] - layer_2)\n        error += np.sum((delta_2) ** 2)\n        delta_1 = delta_2.dot(w2.T) * relu_deriv(layer_1)\n        w2 += alpha * np.dot(layer_1.T, delta_2)\n        w1 += alpha * np.dot(layer_0.T, delta_1)\n    print("error: {0}".format(error))\n\n\n\nWhat happens is in the first iteration there is obviously a large error, and it gets corrected to 1000 after that, but then no matter how many more iterations, it just gets stuck on that forever.\n'
'In the graph, the date axis is not being displayed correctly with date formatting, the year that should be "2020" and is displayed as "51". Text text text Text text text Text text text Text text text Text text text Text text text Text text text Text text text Text text text \n\n# -*- coding: latin-1 -*-\n\nimport pandas as pd\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nhoje = datetime.datetime.now()\ndata = hoje.strftime("%Y%m%d")\ndados_url = \'https://covid.saude.gov.br/assets/files/COVID19_\'+data+\'.csv\'\n\ntry:\n    print(dados_url)\n    dados = pd.read_csv(dados_url, sep=\';\', parse_dates=[\'data\'],  dayfirst=True)\nexcept Exception as e:\n    print("Os dados para a data corrente ainda não foram atualizados, buscando dados do dia anterior...")\n    print(e)\n    DD = datetime.timedelta(days=1)\n    hoje = hoje - DD\n    data = hoje.strftime("%Y%m%d")\n    print(str(hoje))\n    dados_url = \'https://covid.saude.gov.br/assets/files/COVID19_\'+data+\'.csv\'\n    print(str(dados_url))\n    dados = pd.read_csv(dados_url, sep=\';\', parse_dates=[\'data\'],  dayfirst=True)\n\n#pd.set_option(\'display.max_rows\', None)\n\ndados_alagoas = dados[dados[\'estado\'] == \'AL\']\ndados_alagoas.set_index(\'data\',inplace=True)\n#print(dados_alagoas)\n\nplt.style.use(\'ggplot\')\n\nfig, ax = plt.subplots(figsize=(30,15))\ndados_alagoas.plot(ax=ax, rot=75)\n\n#grafico em barras\n#dados_alagoas.plot(ax=ax, rot=75, kind=\'bar\', width=2.0)\n\nax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\nax.xaxis.set_major_formatter(mdates.DateFormatter(\'%d/%m/%y\'))\nax.set_title(\'Incidência do COVID-19 em Alagoas em \'+hoje.strftime(\'%d/%m/%Y\'))\nax.set_ylabel(\'Ocorrências\')\nax.set_xlabel(\'Data\')\nplt.show()\n\n\n\n'
'\nI am using python in jupyter notebook.\n\n\nA DataFrame(df) is given : \n\ndf = pd.DataFrame({\'Grade\': [\'A\',\'B\',\'C\',\'D\',\'E\',\'A\',\'B\'],\'MArks\':[\'100\',\'\',\'\',\'70\',\'\',\'\',\'\']})\n\n\n   Grade    MArks\n0   A       100\n1   B   \n2   C   \n3   D       70\n4   E   \n5   A   \n6   B   \n\n\nGiven dictionary name(\'scaling\') is as follows : \n\nscaling = {\'A\':100,\'B\':90,\'C\':80,\'D\':75,\'E\':70}\n\n\nHow to update column "MArks" as per the given \'scaling\'? \n'
'I am trying to figure out the differences between PCA using Singular Value Decomposition as oppossed to PCA using Eigenvector-Decomposition.\n\nPicture the following matrix:\n\n B = np.array([          [1, 2],\n                         [3, 4],\n                         [5, 6] ])\n\n\nWhen computing the PCA of this matrix B using eigenvector-Decomposition, we follow these steps:\n\n\nCenter the data (entries of B) by substracting the column-mean from each column\nCompute the covariance matrix C = Cov(B) = B^T * B / (m -1), where m = # rows of B\nFind eigenvectors of C\nPCs = X * eigen_vecs\n\n\nWhen computing the PCA of matrix B using SVD, we follow these steps: \n\n\nCompute SVD of B: B = U * Sigma * V.T\nPCs = U * Sigma\n\n\nI have done both for the given matrix. \n\nWith eigenvector-Decomposition I obtain this result: \n\n[[-2.82842712  0.        ]\n [ 0.          0.        ]\n [ 2.82842712  0.        ]]\n\n\nWith SVD I obtain this result:\n\n[[-2.18941839  0.45436451]\n [-4.99846626  0.12383458]\n [-7.80751414 -0.20669536]]\n\n\nThe result obtained with eigenvector-Decomposition is the result given as solution. So, why is the result obtained with the SVD different? \n\nI know that: C = Cov(B) = V * (Sigma^2)/(m-1)) * V.T and I have a feeling this might be related to why the two results are different. Still. Can anyone help me understand better?\n'
"I have a df:\n\ndf =\n     c1  c2   c3   c4  c5\n  0  K   6    nan  Y   V\n  1  H   nan  g    5   nan\n  2  U   B    g    Y   L\n\n\nAnd a string\n\ns = 'HKg5'\n\n\nI want to return rows where s[0]=value of c1, s[1]=value of c2, .....  + in some cases where s[i]=nan.\n\nFor example, row 1 in df above matches with the string\n\n    row 1=\n           c1  c2   c3   c4  c5\n        1  H   nan  g    5   nan\n                                                match=True,   regardless of s[1,4]=nan\n     s   = H   K    g    5\n\n\nAnd also the string length is dynamic, so my df cols go above c10\n\nI am using df.apply but I can't figure it out clearly. I want to write a function to pass to df.apply, passing the string at the same time.\n\nThanks for any help!\n\nOutput from Chris's answer\n\n  df=  \n        c1  c2  c3  c4  c5 \n     0  K   6  NaN  Y   V\n     1  H  NaN  g   5  NaN\n     2  U   B   g   Y   L\n\n  s = 'HKg5'\n  s1 = pd.Series(list(s), index=[f'c{x+1}' for x in range(len(s))])\n  df.loc[((df == s1) | (df.isna())).all(1)]\n\n\nOutput \n\n  `c1  c2  c3  c4  c5`\n\n"
'I have Three data frames : london_working_hours, asian_london_table,london_table_earning which have a column common who correspond to the borough name.\n\nI would like to get the borough name which have the highest working value then get the population and the earning per hour of this borough\n\nprint("The highest borough in working hours , asian population and earning per hour is : ???")\n\n\nthis code gives me this result \n\nThe last print should give me the Borough name which have the highest working hours with asian population and earning per hour how can I do that?? \n\nEdit :\n\nYou can find in the comment the link to my github for more details\n'
"We know that the default threshold for success in a Binary Classification of the Logistic Regression model is > 0.5.\n\nI'm curious to know the output of this model if the predicted probability is exactly 0.5 for both Success and Failure. Can someone clarify me?\n"
"Sample data:\n\n\n\n\ntest_Unique is output file which is giving 8,8 2 time which I don't want, as I want only unique values and drop duplicates but 8,8 appears 2 time. Same for 6,7\n\n\n  the dtype for my sample data is object  'string' the sample data is in\n  column name final_Unique in my dataframe  final_task\n\n\n8,8\n6,7\n7,7\n7,6\n2,12\n12,3\n3,4\n4,12\n12,12\n14,14\n1,1\n1,12\n12,2\n2,2\n2,4\n6,8\n8,8\n\n\nCode I am trying is this : \n\nfinal_task['test_Unique']=final_task['final_Unique'].drop_duplicates()\n\n\nbut i am not getting the perfect output what is the issue here\n"
"sample image to check the data\n\nSAMPLE DATA\n\nActual_Data     Final_Unique\n8,8,8,8,8,8,        8,8\n6,7,7,7,6,7,        6,7\n2,12,3,4,12,12,     7,7\n14,14,14,14,14,14,  7,6\n1,1,12,2,2,4,       2,12\n6,8,8,8,8,12,       12,3\n6,6,6,6,3,14,       3,4\n1,14,14,1,1,2,      4,12\n1,1,1,1,1,14,       12,12\n\n\ni am trying this \n\nnewdf = a.pivot(index='Actual_Data', columns='Final_Unique')\n\ni have 2 column actual data and final_unique column in actual i have 44000 rows and final_unique has 266 rows i want to get the 266 column and actual data to stay same as it and count how many time actual data appears in the header \n\nerror getting \n\nduplicate data exist can't reshape \n\noutput expected\n\nACTUAL DATA          8,8  6,7  7,7   7,6  2,12\n\n8,8,8,8,8,8,          3   0    0      0    0\n\n6,7,7,7,6,7,          0    2    1     1    0\n\n2,12,3,4,12,12,       0    0    0     0    1\n\n"
'I have a json file as below\n\n{\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": null, \n   "bool": true\n}\n\n\nwhen I tried to read this json file using read_json() function of pandas as below\n\nimport pandas as pd\ndata=pd.read_json("example_1.json")\nprint(data.to_json(orient="records"))\n\n\nI am getting an error\nsaying "If using all scalar values, you must pass an index",\nbut if I enclose the above json file in square braces as below\n\n[{\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": null, \n   "bool": true\n}]\n\n\nNow if I use read_json() function of pandas my pgm works fine, I wanted to know why this square braces have such an impact\n'
"I am working with time series data, but for SO purposes Ill just make up some time series data:\n\nimport pandas as pd\nimport numpy as np\nfrom numpy.random import randint\n\n\nrng = pd.date_range('10/9/2018 00:00', periods=500, freq='6H')\ndf = pd.DataFrame({'Random_Number':randint(1, 10, 500)}, index=rng)\n\n\nIm trying to create some dummy variables which will ultimately be used for a stats regression model. Float to represent hour of the day...\n\ndf['hour'] = df.index.strftime('%H')\n\n\nThe question that I have, is why doesn't the df['hour'] populate in the summary statistics of the df.describe? different data type??\n\ndf.describe()\n\n\nReturns:\n\nRandom_Number\ncount   500.00\nmean    5.1140\nstd     2.6219\nmin     1.000\n25%     3.000\n50%     5.000\n75%     7.000\nmax     9.000\n\n\nUltimetely this is where I am stuck, I am trying to create a dummy variable that will either be a 1 or 0 depending on if the hour is between 6AM and 6PM. The code runs without any error:\n\ndf['daytime'] = np.where(df['hour'].isin([6,7,8,9,10,11,12,13,14,15,16,17,18]), 1, 0)\n\n\nBut with another df.describe() its like the hour df is not getting created properly. Is there another/better way of accomplishing this???\n\n    Random_Number   daytime\ncount   500.000000  500.0\nmean    5.128000    0.0\nstd     2.587858    0.0\nmin     1.000000    0.0\n25%     3.000000    0.0\n50%     5.000000    0.0\n75%     7.000000    0.0\nmax     9.000000    0.0\n\n"
'For this solution\nIs there an easy way to define this code within a function such that i can apply it to any dataframe column. \n'
'I was wondering if there is a one-line (or two!) solution instead of using multiple for-loops for the following problem: \n\nI want to calculate the difference between any $weight value if they are for the same $ID, $orient, and $direct:\n\n$ID $spec   $view   $orient $direct $weight\n9247    1   post    stance      0       2038.66 \n9247    2   post    stance      15b     2177.74 \n9247    4   post    stance      15f     1559.62 \n9247    5   ant     stance      15b     2271.89     \n9247    6   ant     stance      0       2075.44     \n9247    7   ant     stance      15f     1438.31     \n9247    8   post    fall        15a     1665.60     \n9247    9   post    fall        15p     1742.82     \n9119    1   ant     fall        0       994.48      \n9119    2   ant     fall        15b     1081.44     \n9119    3   post    fall        15b     1024.18 \n9119    4   post    fall        0       1093.46 \n9119    5   post    stance      15a     1220.13     \n9119    6   post    stance      15p     1089.72     \n9119    7   post    fall        15f     1056.21     \n\n\nFor example, the difference between the first and fifth line should be calculated (2038.66 - 2075.44 = −36.78) and so on, and written in such a new dataframe:\n\n$ID $orient $direct $weight-difference\n9247    stance      0       −36.78  \n9247    stance      15b     −94.15\n\n\nThanks!\n'
'I\'m cleaning a data file with some irregularities in it. I have a list of values like so:\n\nimport numpy as np\nimport pandas as pd\ndataset = pd.DataFrame.from_dict({\'data\':[\'1\',\'2\',\'3\',\'Third Street\',np.nan]})\n\n\nMy goal is to filter out the "Third Street" column while retaining the NaN value.\n\ndataset[\'data\'].astype(int)\nValueError: invalid literal for int() with base 10: \'Third Street\'\n\n\nWhich makes a lot of sense, since the last value isn\'t able to be converted to an integer. \n\nTrying to filter the non-digit column filters out the NaN value, which I want to keep:\n\ndigitFilter = dataset[\'data\'].str.isdigit()\ndataset[digitFilter]\nValueError: cannot index with vector containing NA / NaN values\n\n\nI\'ve also tried stacking filters, but the NaN seems to get in the way there, too. Sure there\'s an easy way to do this that I\'m overlooking. Appreciate any wisdom anyone can offer.\n'
'What functions do I use to convert this Dataframe:\n\nid name genre\n\n1 Fiml1 action, comedy\n2 Fiml2 animation\n3 Fiml3 comedy\n4 Fiml4 action, animation\n5 Fiml5 action\n6 Fiml6 animation, comedy\n\n\nTo:\n\nid name action animation comedy\n\n1  Fiml1   1       0       1\n2  Fiml2   0       1       0\n3  Fiml3   0       0       1\n4  Fiml4   1       1       0\n5  Fiml5   1       0       0\n6  Fiml6   0       1       1\n\n\nThis dataframe is to be used in vector space model, answer or any suggestions?\n'
'print(x)\'\nHere "x" is Independent Variables.`\n\n    Restaurant  Cuisines    Average_Cost    Rating  Votes   Reviews Area\n    0   3.526361    0.693147    5.303305    1.504077    2.564949    1.609438    7.214504\n    1   1.386294    4.127134    4.615121    1.504077    2.484907    1.609438    5.905362\n    2   2.772589    1.386294    5.017280    1.526056    4.605170    3.433987    6.131226\n    3   3.912023    2.833213    5.525453    1.547563    5.176150    4.564348    7.643483\n    4   3.526361    2.708050    5.303305    1.435085    5.948035    5.046646    6.126869\n    ... ... ... ... ... ... ... ...\n    11089   3.912023    0.693147    5.525453    1.648659    5.789960    5.046646    3.135494\n    11090   1.386294    6.028279    4.615121    1.526056    3.610918    2.833213    7.643483\n    11091   1.386294    2.397895    4.615121    1.504077    3.828641    2.944439    5.814131\n    11092   1.386294    6.028279    4.615121    1.410987    3.218876    2.302585    5.905362\n    11093   1.386294    6.028279    4.615121    1.029619    0.000000    0.000000    5.564520\n    11094 rows × 7 columns\n\n\n`print(y.value_counts()) `\n\n\nHere "y" is target variable and it has multi-class. \n\n    30 minutes     7406\n    45 minutes     2665\n    65 minutes      923\n    120 minutes      62\n    20 minutes       20\n    80 minutes       14\n    10 minutes        4\n    Name: Delivery_Time, dtype: int64\n\n\nAfter exploring the TARGET VARIABLE WE CAN SEE THAT "30 MINUTES" CLASS HAS HIGHER COUNTS AMONG OTHER CLASS.\n\nFOR FOR MAKING THINGS BALANCE I TRIED SMOTEtomek to oversamplemy data and make it balance. Below are the codes provide and got error.\n\nfrom imblearn.combine import SMOTEtomek\nsmk = SMOTEtomek(ratio = 1)\nx_res, y_res = smk.fit_sample(x,y)\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-54-426e8b86623d&gt; in &lt;module&gt;()\n      1 from imblearn.combine import SMOTETomek\n      2 smk = SMOTETomek(ratio = 1)\n----&gt; 3 x_res, y_res = smk.fit_sample(x,y)\n\n2 frames\n/usr/local/lib/python3.6/dist-packages/imblearn/utils/_validation.py in _sampling_strategy_float(sampling_strategy, y, sampling_type)\n    311     if type_y != \'binary\':\n    312         raise ValueError(\n--&gt; 313             \'"sampling_strategy" can be a float only when the type \'\n    314             \'of target is binary. For multi-class, use a dict.\')\n    315     target_stats = _count_class_sample(y)\n\nValueError: "sampling_strategy" can be a float only when the type of target is binary. For multi-class, use a dict.\n\n'
"I am using the dataset- Meteorite Landings which can be found here- https://www.kaggle.com/nasa/meteorite-landings#meteorite-landings.csv\n\nThe dataset has a lot of missing values, after filling in the values of the columns- reclat and reclong, I tried to combine those in the form of strings and put them into the GeoLocation column. \n\nreclat col:       reclong col: \n\n\n0    50.77500     0      6.08333\n1    56.18333     1     10.23333\n2    54.21667     2   -113.00000\n3    16.88333     3    -99.90000\n4   -33.16667     4    -64.95000                             \n\n\nThe problem is that the combined value does not show up in the desired coordinate format as I want. \n\nI am trying to combine the values of reclat and reclong, preferably in a coordinate format. \n\nCombining the two values in string form and inserting it into the GeoLocation column\n\ndata['GeoLocation']=str(data['reclat'])+str(data['reclong'])\n\n\nI expected the column of GeoLocation to have values of reclat and reclong appended together, but it's in a different format as shown below.\n\n0    0        50.77500\\n1        56.18333\\n2       ...\n1    0        50.77500\\n1        56.18333\\n2       ...\n2    0        50.77500\\n1        56.18333\\n2       ...\n3    0        50.77500\\n1        56.18333\\n2       ...\n4    0        50.77500\\n1        56.18333\\n2       ...\nName: GeoLocation, dtype:object \n\n"
'I am trying to create a basic item based recommender system with knn. But with the following code it always returns same distances with different k\'s with the model. Why it returns same results?\n\ndf_ratings = pd.read_csv(\'ml-1m/ratings.dat\', names=["user_id", "movie_id", "rating", "timestamp"],\n            header=None, sep=\'::\', engine=\'python\')\nmatrix_df = df_ratings.pivot(index=\'movie_id\', columns=\'user_id\', values=\'rating\').fillna(0).astype(bool).astype(int)\n\num_matrix = scipy.sparse.csr_matrix(matrix_df.values)\n\n# knn model\nmodel_knn = NearestNeighbors(metric=\'cosine\', algorithm=\'brute\', n_neighbors=17, n_jobs=-1)\nmodel_knn.fit(um_matrix)\n\ndistances, indices = model_knn.kneighbors(um_matrix[int(movie)], n_neighbors=100)\n\n'
"I'm trying to map the results of a 2 level aggregation to the original categorical feature and use it as a new feature. I created the aggregation like this.\n\ntemp_df = pd.concat([X_train[['cat1', 'cont1', 'cat2']], X_test[['cat1', 'cont1', 'cat2']]])\ntemp_df = temp_df.groupby(['cat1', 'cat2'])['cont1'].agg(['mean']).reset_index().rename(columns={'mean': 'cat1_cont1/cat2_Mean'})\n\n\nThen I made MultiIndex from the values of first and second categorical feature, and finally casted the new aggregation feature to a dict.\n\narrays = [list(temp_df['cat1']), list(temp_df['cat2'])]    \ntemp_df.index = pd.MultiIndex.from_tuples(list(zip(*arrays)), names=['cat1', 'cat2'])\ntemp_df = temp_df['cat1_cont1/cat2_Mean'].to_dict()\n\n\nThe dict keys are tuples as multi indices. The first values in the tuples are cat1's values and the second values are cat2's values.\n\n{(1000, 'C'): 23.443,\n (1001, 'H'): 50.0,\n (1001, 'W'): 69.5,\n (1002, 'H'): 60.0,\n (1003, 'W'): 42.95,\n (1004, 'H'): 51.0,\n (1004, 'R'): 150.0,\n (1004, 'W'): 226.0,\n (1005, 'H'): 50.0}\n\n\nWhen I try to map those values to the original cat1 feature, everything becomes NaN. How can I do this properly?\n\nX_train['cat1'].map(temp_df) # Produces a column of all NaNs\n\n"
"I have a dataset that holds information relating to businesses in the City of Brampton. Several variables in my dataframe show the social media accounts of the respective businesses. The null values represent businesses that don't have a social media presence on a particular platform: YouTube, LinkedIn, etc.\n\nI would like to create an additional column in my dataframe which shows the businesses that have a social media presence. A value of 0 should indicate that the business does not have a social media presence and a value of 1 should indicate that the business does have a social media presence.\n\n\n\n\n\nI've also included a link to the dataset here\n\nFrankly, I have zero clue how to tackle this problem. Any help would be much appreciated.\n"
"Pandas documentation has given following code, which works fine:\n\n frame = pd.DataFrame(np.arange(12).reshape((4, 3)),\n     index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n     columns=[['Ohio', 'Ohio', 'Colorado'],\n     ['Green', 'Red', 'Green']])\n\n\nI tried following code, based on above concept, but it does not work:\n\nhi5 = pd.DataFrame([[1,2,3],[4,5,6],[7,8,9],[10,11,12]], \n    index = [['a','a','a','b'],[1,2,3,1]], \n    columns=[['Ohio', 'Ohio', 'Colorado'], \n    ['Green', 'Red', 'Green']])\n\n\nIt is giving Following error for above code:\n\nAssertionError: 2 columns passed, passed data had 3 columns\n\n"
"Can someone explain me what's happening here? \n\nWhy is there more decimal points for 0.3 and 0.7 values. \nI just want 1 decimal point values.\n\nthreshold_range = np.arange(0.1,1,0.1)\nthreshold_range.tolist()\n\n\n[Output]: [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]\n\n"
'I have a pandas datetime column where dates are not sorted. I want to select all the dates for which the next 6 consecutive dates are available in the column without any missing day in between.\n\nMy data looks something like this and I have marked the date I want in the image.\n\n\n'
"I'm looking for a solution for the case bellow:\n\n\nHow do I pivot df such that the col values become columns when df consist only one dimension?\nAnd how to proceed in order to have in a result in each column made from those rows value 0 or 1 depending on occurrences for each of them in the previous column?\n\n\nIt'll be easier to illustrate it.\nSo from this kind of data frame:\n\ndf =\n\nDATA   \ncat1\ndog1\ncat2\ndog2\ncat3\ndog3\n...   \n\n\nto this kind of data frame:\n\ndf =\n\nAnimal   cat1   dog1   cat2   dog2   cat3   dog3    ...\ncat1    1      0      0      0      0      0        ...\ndog1    0      1      0      0      0      0        ...\ncat2    0      0      1      0      0      0        ...\ndog2    0      0      0      1      0      0        ...\ncat3    0      0      0      0      1      0        ...\ndog3    0      0      0      0      0      1        ...\n...   \ncat1    1      0      0      0      0      0        ...\ndog1    0      1      0      0      0      0        ...\n\n\nFirst I've tried to gather all unique values and then I reshaped it to pd.DataFrame because it was a np array. Then I've tried to use pivot. I know that it should have arguments like 'index', 'column' and 'values', but in my case, I have only one dimension (just one column). \n\nto_reschape = df.Animal.unique()\ntype(to_reschape)\ndataset = pd.DataFrame(to_reschape)\ndataset.pivot()\n\nKeyError: None\n\n"
'I want to plot graph with a certain condition without manipulating my data frame.\n\nFor example, I created a countplot with a data frame that has a bunch of x-values that are less than 100, and in the countplot, those less than 100 comes up as "no-bar", and it\'s taking up space. So I want to just get rid of those empty (count &lt; 100). \n\nI tried to create another data frame with only count values higher than 100, but I wanted to know if there is a simpler/cleaner way to plot a graph, rather than creating a whole data frame.\n\nplt.figure(figsize=(10,50))\nplt.ylim(100,500)\nsns.countplot(data=df, x=\'brand\')\n\n\nFrom this code, I see many empty bars caused by counting values less than 100, as xlim is set to 100-500.\n'
"I've installed fancyimpute on Anaconda with pip install fancyimpute through the Anaconda terminal but I use PyCharm with Anaconda as its interpreter.  I couldn't find a way to use fancyimpute in PyCharm.  I searched for it through Settings/Project Interpreter/Available Packages but no luck.\n"
'I have a dataframe like this\n\n   data\n0   1.5\n1   1.3\n2   1.3\n3   1.8\n4   1.3\n5   1.8\n6   1.5\n\n\nAnd I have a list of lists like this:\n\nindices = [[0, 3, 4], [0, 3], [2, 6, 4], [1, 3, 4, 5]]\n\n\nI want to produce sums of each of the groups in my dataframe using the list of lists, so \n\ngroup1 = df[0] + df[1] + df[2]\ngroup2 = df[1] + df[2] + df[3]\ngroup3 = df[2] + df[3] + df[4]\ngroup4 = df[3] + df[4] + df[5]\n\n\nso I am looking for something like df.groupby(indices).sum\n\nI know this can be done iteratively using a for loop and applying the sum to each of the df.iloc[sublist], but I am looking for a faster way.\n'
"I'm working using automobile.csv which can be found in the UCI website. I want to replace some NaNs in normalized losses attribute. I figured that a better way of doing it is by calculating the mean according to the symboling because symboling affects the value of normalized losses.\n\nSo if the NaN have a symboling of 3 I only want mean of other normalized losses that have value 3 as their symboling. How do I achieve this?\n\nexample\ntable:\n\nsymb    norm    other attrs\n1        100  8017  2\n1        90  5019  2\n-1       20   8017  1\n-1       20    8870  1\n1        NaN    8305  3\n0        10   8305  3\n3        200  8221  3\n\n\nso for NaN I only want mean from other rows with the same symboling\n\nif i use\n\nautomobile['normalizedlosses'].fillna(automobile['normalizedlosses'].mean(axis=0), inplace=True)\n\n\nThis would replace all NaN with the same value which I don't want\n"
'I am using the regression slope as follows to calculate the steepness (slope) of the trend.\n\nScenario 1:\nFor example, consider I am using sales figures (x-axis: 1, 4, 6, 8, 10, 15) for 6 days (y-axis).\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nX = [[1], [4], [6], [8], [10], [15]]\ny = [1, 2, 3, 4, 5, 6]\nregressor.fit(X, y)\nprint(regressor.coef_)\n\n\nThis gives me 0.37709497\n\nScenario 2:\nWhen I run the same program for a different sale figure (e.g., 1, 2, 3, 4, 5, 6) I get the results as 1.\n\nHowever, you can see that sales is much productive in scenario 1, but not in scenario 2. However, the slope I get for scenario 2 is higher than scenario 1.\n\nTherefore, I am not sure if the regression slope captures what I require. Is there any other approach I can use instead to calculate the sleepness of the trend slope.\n\nI am happy to provide more details if needed.\n'
'here is some part of the data table \n\nDf2\n\n   id   title   parent_id\n0   11  p1          11\n1   12  p1          11\n2   13  p2          12\n3   14  p2          12\n4   15  p2          13\n5   16  p2          13\n6   17  p3          13\n\n\nThis df2 problem should give output like\n\np1_id | no. of p1|no. of p2| no. of p3 |\n11    | 1        |4        | 1         |\n12    | 0        |2        | 0         |\n\n\ngiven that:\n1--There is no certain hierarchy. like, may be possible that p1 have another p1 under him or p3 under him. And there may be n number of branches and node possible.\n2--The p1 may have child at n number of level. how to approach to this problem.\n\nEdit - to visualize the problem\n'
'I have data with four columns, that includes: Id, CreationDate, Score and ViewCount.\n\nThe CreationDate has a next format, for example: 2011-11-30 19:41:14.960.\nI need to groupby the years of CreationDate, count them, summing Score and ViewCount also, and to add to additional columns.\n\nI want to use with pandas lib.\n\nThanks!\n\nBefore changing - sample example:\n\n     Id   CreationDate              Score   ViewCount\n0    1    2011-11-30 19:15:54.070   25      1526\n1    2    2011-11-30 19:41:14.960   20      601\n2    3    2012-11-30 19:42:45.470   36      1015\n3    4    2018-11-30 19:44:55.593   8       1941\n4    5    2011-11-30 19:53:23.387   11      5053\n5    6    2018-11-30 20:04:43.757   25      5123\n6    7    2011-11-30 20:08:23.267   53      8945\n\n\nAfter changing - present data like this:\n\n     Id   CreationDate              Score   ViewCount\n0    1    2011                      109     16125\n2    3    2012                      36      1015\n3    4    2018                      33      7064                            \n\n'
"I have two dfs in python :\n\ndf1\n\n    folder_name   name\n0   f1          aa\n1   g1          bb\n\n\ndf2   \n    name        icon\n0   aa          i1\n1   bb          i2\n2   aadoq       i3\n3   bbaddd      i4\n\n\nDesired output:\n\ndf   \n    folder_name  name    icon\n0   f1           aa      i1\n1   g1           bb      i2\n2   f1           aadoq   i3\n3   g1           bbaddd  i4\n\n\nI tried merging them but it seemed wrong \n\npd.merge(df1,df2,on='name',how='right')\n\n\ni am getting :\n\n      folder_name  name    icon\n  0   f1           aa      i1\n  1   g1           bb      i2\n  2   NAN          aadoq   i3\n  3   NAN          bbaddd  i4\n\n\nSo if the prefix string in df2 name column matches any name column itme in df1 i want that folder name to be there for that name column in output\n"
'I\'m learning pandas and when i display the data frame, it is displaying ? instead of NaN.\nWhy is it so?\n\nCODE : \n\nimport pandas as pd\n\nurl = "https://archive.ics.uci.edu/ml/machine-learning- \ndatabases/autos/imports-85.data"\n\ndf = pd.read_csv(url, header=None)\nprint(df.head())\nheaders = ["symboling", "normalized-losses", "make", "fuel-type", \n"aspiration",\n"num-of-doors", "body-style", "drive-wheels", "engine-location",\n"wheel-base", "length", "width", "height", "curb-weight",\n"engine-type", "num-of-cylinders", "engine-size", "fuel-system",\n"bore", "stroke", "compression-ratio", "hoursepower", "peak-rpm",\n"city-mpg", "highway-mpg", "price"]\n\ndf.columns=headers\n\nprint(df.head(30))\n\n'
"I want to fill missing value base on other columns in pandas.\nHere is my table:\n\nGender     Married\nMale       Yes\nMale       Yes\nFemale     No\nFemale     No\nMale       NaN\nFemale     NaN\n\n\nI to fill missing value of Married field by if Gender is Male -> Married is Yes, else Married is No: \n\ndf['Married'].fillna(df[df['Married'].isnull()].apply(lambda x: 'Yes' if (df[df['Married'].isnull()]['Gender'] is 'Male') else 'No', axis=1), inplace=True)\n\n\nBut it was fail, I try a lot of way and I get nothing as my expectation.\nI hope receive from all of you.\n"
"I want to read the following table as a pandas dataframe\n\n\n\nSay the dataframe is df, the purpose is to query \ndf['acct_id']['A']['0-3_mon] should give me 10\n\nI have done it for panel data, where everything is a column and then you create a multi-level-index for both cross-section and time-series.\n\nBut over here, the source data itself has more than two levels of columns. How do I read this csv as a multi-level index? I am stuck here, any idea. \n\nSome of the similar work if you want to look at -\nhttps://lectures.quantecon.org/py/pandas_panel.html\n\nThanks a lot.\n"
"I was working on a dataframe like this.\n\ndf = pd.DataFrame([[1, np.nan, 2],\n               [2,      3,      5],\n               [np.nan, 4,      6]],index=['a','b','c'])\ndf\n\n    0     1     2\na   1.0  NaN    2\nb   2.0  3.0    5\nc   NaN  4.0    6\n\n\nWhen I use df.isnull() it gives the output as :\n\n        0   1        2\na   False   True    False\nb   False   False   False\nc   True    False   False\n\n\nWhen I use df[df.isnull()] why does it show all elements as nan:\n\ndf[df.isnull()] \n\n    0   1   2\na   NaN NaN NaN\nb   NaN NaN NaN\nc   NaN NaN NaN\n\n\nCan somebody explain why it is happening?\n"
"I'm trying to use numpy to find configurations of rows in a matrix such that summing the columns of the rows will result in the same value. As an example, for the matrix/array\n\n[[0,0,0,1],\n [1,0,1,0],\n [1,1,0,0],\n [0,1,0,0]]\n\n\nI would like to have the first, second, and last row as output, because\n\n  0,0,0,1\n  1,0,1,0\n  0,1,0,0 +\n  -------\n= 1,1,1,1\n\n\nIs there any tools built into numpy that would assist me in acquiring this?\n"
'Recently i am working on exercise and in which i had extracted whole webpage source data . I am very much interested in area tag . In area tag i am very much interested in onclick attribute . Now how can we extract onclick attribute from particular element .\nNow our extracted data is looking like these ,\n\n&lt;area class="borderimage" coords="21.32,14.4,933.96,180.56" href="javascript:void(0);" onclick="return show_pop(\'78545\',\'51022929357\',\'1\')" onmouseover="borderit(this,\'black\',\'&lt;b&gt;इंदौर, गुरुवार, 10 मई , 2018  &lt;b&gt;&lt;br&gt;&lt;bआप पढ़ रहे हैं देश का सबसे व...\')" onmouseout="borderit(this,\'white\')" alt="&lt;b&gt;इंदौर, गुरुवार, 10 मई , 2018  &lt;b&gt;&lt;br&gt;&lt;bआप पढ़ रहे हैं देश का सबसे व..." shape="rect"&gt;\n\n\nI am very much interested in onclick attribute and my code is like these which i had already done but nothing has worked ,\n\npaper_url  = \'http://epaper.bhaskar.com/indore/129/10052018/mpcg/1/\'\n    headers = {\'User-Agent\': \'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36\'}\n\n    # Total number of pages available in these product\n    page = requests.get(paper_url,headers = headers)\n    page_response = page.text\n    parser = html.fromstring(page_response)\n    XPATH_Total_Pages = \'//div[contains(@class,"fs12 fta w100 co_wh pdt5")]//text()\'\n    raw_total_pages = parser.xpath(XPATH_Total_Pages)\n    lastpage=raw_total_pages[-1]\n    print(int(lastpage))\n    finallastpage=int(lastpage)\n    reviews_list = []\n    XPATH_PRODUCT_NAME = \'//map[contains(@name,"Mapl")]\'\n\n    #XPATH_PRODUCT_PRICE  = \'//span[@id="priceblock_ourprice"]/text()\'\n\n    #raw_product_price = parser.xpath(XPATH_PRODUCT_PRICE)\n    #product_price = raw_product_price\n    raw_product_name = parser.xpath(XPATH_PRODUCT_NAME)\n\n    XPATH_REVIEW_SECTION_2 = \'//area[@class="borderimage"]\'\n    reviews = parser.xpath(XPATH_REVIEW_SECTION_2)\n\n    product_name =raw_product_name\n    #result = product_name.find(\',\')\n    #finalproductname = slice[0:product_name]\n    print(product_name)\n    print(reviews)\n\n    for review in reviews:\n        #soup = BeautifulSoup(str(review), "html.parser")\n        #parser2.feed(str(review))\n        #allattr = [tag.attrs for tag in review.findAll(\'onclick\')]\n        #print(allattr)\n\n        XPATH_RATING  = \'.//area[@data-hook="onclick"]\'\n\n        raw_review_rating = review.xpath(XPATH_RATING)\n        #cleaning data\n        print(raw_review_rating)\n\n'
"I'm trying to transform a dataframe\n\ndf = pd.DataFrame({\n'c1': ['x','y','z'],\n'c2': [[1,2,3],[1,3],[2,4]]})\n\n\nwhich looks like\n\n    c1  c2\n0   x   [1, 2, 3]\n1   y   [1, 3]\n2   z   [2, 4]\n\n\ninto\n\np = pd.DataFrame({\n    'c1': ['x','y','z'],\n    1: [1,1,0],\n    2: [1,0,1],\n    3: [1,1,0],\n    4: [0,0,1]\n})\n\n\nwhich looks like\n\n    c1  1   2   3   4\n0   x   1   1   1   0\n1   y   1   0   1   0\n2   z   0   1   0   1\n\n\nthe value 1's and 0's are supposed to be true and false. I'm still learning pivots. Please point me in the right direction.\n"
"I am relatively new to ML and sklearn and I am trying to train a linear model for input data with 6 different features using linear_model.Lasso with different values of the regularization parameter. Given that X and y are my input parameters for the model, I can not figure out why I keep getting different values by executing these 2 expressions:\n\nsum(model.coef_*X[0])\nOut[94]: -0.4895022980752311\n\nmodel.predict(X[0])\nOut[95]: array([ 2.08767122])\n\n\nIdeally I would expect that the model coefficients would correspond to a given feature in the dataset and that both expressions would return exactly the same value.\n\nHere is the sample of the code:\n\ninput_file = 'Spud_startup_analysis.xlsx'\ndata_input_generic = pd.read_excel(input_file, skiprows = 0, sheetname='DataSet')\ndata = data_input_generic.as_matrix()\nX = data[:, 0:-1]\ny = data[:,-1]\nmodel = linear_model.Lasso(alpha = 0.1)\nmodel.fit(X, y)\n\n\ndoes it have something to do with dimensions of the input matrices?\nThanks in advance\n"
'Would anyone know how to make Python return a date format like this from the calendar function?:\n\ndate = "20170714"\n\n\nBasically I am looking for some enhancements if its possible for the API request to gather hourly weather data.. My problem is the code needs to be re-ran for each day.. Its a little clunky and time consuming to rerun the script 365 times to get a years worth of weather data, but its the only way I know how. Can a loop with the calendar function automate things?\n\nfrom urllib.request import urlopen\nfrom pandas.io.json import json_normalize\nimport json\nimport pandas as pd\n\napi_key = ""\ndate = "20170714"\nzip_code = "96345"\n\nresponse = urlopen("http://api.wunderground.com/api/%s/history_%s/q/%s.json" % (api_key, date, zip_code))\n\njson_data = response.read().decode(\'utf-8\', \'replace\')\n\ndata = json.loads(json_data)\n\nfor observation in data[\'history\'][\'observations\']:\n     print("Date/Time:    " + observation[\'date\'][\'pretty\'])\n     print("Temperature:  " + observation[\'tempi\'])\n     print("Humidity:     " + observation[\'hum\'])\n\ndf = json_normalize(data[\'history\'][\'observations\'])\n\ndf = df[[\'date.pretty\',\'tempi\',\'hum\']]\ndf[\'date.pretty\'] = pd.to_datetime(df[\'date.pretty\'])\n\nprint(df)\n\n\nThis simple calendar script can return every day in the month of December 2013, but its not in a format that weather underground API would understand... Ultimately I am hoping to create a loop where the API request can gather a months data, (or even an entire year at a time) with the calendar function Vs one day manual fashion.... Is this possible??!\n\nimport calendar\n\ntc = calendar.TextCalendar(firstweekday=0)\n\nfor i in tc.itermonthdays(2013,12):\n    print(i)\n\n\nThanks\n'
'I\'m trying to add a slider via Bokeh to my plot which is connected to a pandas dataframe. \n\nThe plot is using the datetime index to show how Air Quality Index over one year. \n\nI would like to add a slider for each month, January - December 2016.\nI\'m not able to find a clear example with code that connects the slider to a plot which is connected to a pandas dataframe. Someone help please! \n\nI was able to find the following code, but the plot was generated with random data. The output of this code is exactly what I\'m looking to do but with time series data. \n\nfrom bokeh.io import output_notebook, show, vform\nfrom bokeh.plotting import figure, Figure\nfrom bokeh.models import ColumnDataSource, Slider, CustomJS\nimport numpy as np\n\noutput_notebook()\n\nx = np.sort(np.random.uniform(0, 100, 2000))\n\ny = np.sin(x*10) + np.random.normal(scale=0.1, size=2000)\nfig = Figure(plot_height=400, x_range=(0, 2))\n\nsource = ColumnDataSource(data={"x":x, "y":y})\n\nline = fig.line(x="x", y="y", source=source)\ncallback = CustomJS(args=dict(x_range=fig.x_range), code="""\nvar start = cb_obj.get("value");\nx_range.set("start", start);\nx_range.set("end", start+2);\n""")\n\nslider = Slider(start=0, end=100, step=2, callback=callback)\nshow(vform(slider, fig))\n\n\nI also found the source code of making this type of slider (below/linked here) but I am unsure how to implement it. As you can probably tell, I\'m fairly new to Bokeh. Please help!\n\nclass DateRangeSlider(AbstractSlider):\n""" Slider-based date range selection widget. """\n\n@property\ndef value_as_datetime(self):\n    \'\'\' Convenience property to retrieve the value tuple as a tuple of\n    datetime objects.\n\n    \'\'\'\n    if self.value is None:\n        return None\n    v1, v2 = self.value\n    if isinstance(v1, numbers.Number):\n        d1 = datetime.utcfromtimestamp(v1 / 1000)\n    else:\n        d1 = v1\n    if isinstance(v2, numbers.Number):\n        d2 = datetime.utcfromtimestamp(v2 / 1000)\n    else:\n        d2 = v2\n    return d1, d2\n\nvalue = Tuple(Date, Date, help="""\nInitial or selected range.\n""")\n\nstart = Date(help="""\nThe minimum allowable value.\n""")\n\nend = Date(help="""\nThe maximum allowable value.\n""")\n\nstep = Int(default=1, help="""\nThe step between consecutive values.\n""")\n\nformat = Override(default="%d %b %G")\n\n'
"I have Transaction data with timestamps between invoices for customers. I'm trying to calcualte the average days between two unique invoices(by 'ServicedOn').Two caveats for these are: \n\n\nA unique invoice can have multiple rows with different timestamps(as they may be serviced on at different times). \nThe timestamp has time values as well, so I'm unable to use diff() method to calculate the delta days. \n\n\nThe dataframe looks as such:\n\nInvoiceNo SoldOn    ServicedOn  ItemType    ItemCode    GuestId FinalSalePrice  FirstVisit  Package BUName  SalePrice   Merchant_id\n21312   4/26/2015 12:55:12 PM   4/26/2015 8:00:00 AM    0   SER-310-008 5a531810-9413-4091-acd5-424d125b0c9e    3089.9  No  No Package  Spa 2500    POW\n31639   1153    6/12/2015 7:11:22 PM    6/12/2015 6:15:00 PM    0   SER-1257-008    5a531810-9413-4091-acd5-424d125b0c9e    5700.0  No  No Package  Spa 5000    POW\n42492   73  5/15/2015 8:05:50 PM    5/15/2015 8:05:50 PM    2   ITM-3407-001    5a531810-9413-4091-acd5-424d125b0c9e    1600.0  No  No Package  Default 1422    POW\n42493   73  5/15/2015 8:06:08 PM    5/15/2015 8:06:08 PM    2   ITM-3175-001    5a531810-9413-4091-acd5-424d125b0c9e    2750.0  No  No Package  Default 2444    POW\n42494   73  5/15/2015 8:05:38 PM    5/15/2015 8:05:38 PM    2   ITM-4340-001    5a531810-9413-4091-acd5-424d125b0c9e    575.0   No  No Package  Default 511 POW\n\n\nI'm unsure on how to proceed, so any guidance would be apprecaited!\n"
'In tensorflow, I want to evaluate the model after each epoch with the same test data set. What I did:\n\n    # Train data.\n    cTr,train_summary,_ = sess.run([loss,summary_op,optimizer], feed_dict={input_tensor: batch_xTr,output_tensor:batch_yTr})\n    # Test data.\n    batch_xTe,batch_yTe = get_batch(newsgroups_test,0,len(newsgroups_test.target)) # can also be adjusted batch size\n    cTe,test_summary, _ = sess.run([loss,summary_op, optimizer], feed_dict={input_tensor: batch_xTe,output_tensor:batch_yTe})\n\n\nThe result is that the model can reach nearly 100% accuracy at last. It\'s not reasonable and the reason might be that I\'m actually "training" while evaluating.\n\nIs there anyway that I can evaluate the model without actually "training" it with the test data?\n'
"Avg.SessionLength TimeonApp TimeonWebsite LengthofMembership Yearly Amount Spent\n0   34.497268   12.655651   39.577668   4.082621    587.951054\n1   31.926272   11.109461   37.268959   2.664034    392.204933\n2   33.000915   11.330278   37.110597   4.104543    487.547505\n3   34.305557   13.717514   36.721283   3.120179    581.852344\n4   33.330673   12.795189   37.536653   4.446308    599.406092\n5   33.871038   12.026925   34.476878   5.493507    637.102448\n6   32.021596   11.366348   36.683776   4.685017    521.572175\n\n\n\n\nI want to apply KNN: \n\nX = df[['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership']] \ny = df['Yearly Amount Spent'] \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \nfrom sklearn.neighbors import KNeighborsClassifier \nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\n\n\n\n  ValueError: Unknown label type: 'continuous'\n\n"
"I have 2 data frames like this \n\n    DF1:    Col1 Col2\n    Row1    A   30\n    Row2    B   40\n\n    DF2:  Col1 Col2\n    Row1    A   10\n    Row2    B   5\n\n\nNow i want to multiply DF1.Col2 and DF2.Col2 and get an output like this\n\n    Out:   Col1 COl3\n    Row1    A   300\n    Row2    B   200\n\n\nHow can I do this. I try bellow code but its not working\n\nDF1[:,'Col3'] = pd.Series(DF1.Col2.astype(int)*DF2.Col2.astype(int),index=DF1.index)\n\n\nThis direct to bellow error:\nTypeError: unhashable type\n"
'My pandas dataframe looks like this\n\n                   A          B          C          D          E\n(Name1, 1)         NaN        NaN        NaN        NaN        NaN  \n(Name2, 2)         NaN        NaN        NaN        NaN        NaN\n\n\nHow do I access the a particular cell or change the value of a particular cell\n\nI created the dataframe using this\n\nid=list(product(array1,array2))\ndata=pd.DataFrame(index=id ,columns=array3)\n\n'
"I have the following dataframe:\n\n     TIME   PREC    ET   PET  YIELD       date  year  intensity\n0        1   1.21  0.02  0.02   0.00 1991-01-01  1991          1\n1        2   0.00  0.03  0.04   0.00 1991-01-02  1991          1\n2        3   0.00  0.03  0.05   0.00 1991-01-03  1991          1\n3        4   0.00  0.04  0.05   0.00 1991-01-04  1991          1\n4        5   0.00  0.05  0.07   0.00 1991-01-05  1991          1\n5        6   0.00  0.03  0.05   0.00 1991-01-06  1991          1\n6        7   0.00  0.02  0.04   0.00 1991-01-07  1991          1\n7        8   1.14  0.03  0.04   0.00 1991-01-08  1991          1\n8        9   0.10  0.02  0.03   0.00 1991-01-09  1991          1\n9       10   0.00  0.03  0.04   0.00 1991-01-10  1991          1\n10      11   0.10  0.05  0.11   0.00 1991-01-11  1991          1\n11      12   0.00  0.06  0.15   0.00 1991-01-12  1991          1\n12      13   2.30  0.14  0.44   0.00 1991-01-13  1991          1\n13      14   0.17  0.09  0.29   0.00 1991-01-14  1991          1\n14      15   0.00  0.13  0.35   0.00 1991-01-15  1991          1\n15      16   0.00  0.14  0.39   0.00 1991-01-16  1991          1\n16      17   0.00  0.10  0.31   0.00 1991-01-17  1991          1\n17      18   0.00  0.15  0.51   0.00 1991-01-18  1991          1\n18      19   0.00  0.22  0.58   0.00 1991-01-19  1991          1\n19      20   0.10  0.04  0.09   0.00 1991-01-20  1991          1\n20      21   0.00  0.04  0.06   0.00 1991-01-21  1991          1\n21      22   0.27  0.13  0.43   0.00 1991-01-22  1991          1\n22      23   0.00  0.10  0.25   0.00 1991-01-23  1991          1\n23      24   0.00  0.03  0.04   0.00 1991-01-24  1991          1\n24      25   0.00  0.04  0.05   0.00 1991-01-25  1991          1\n25      26   0.43  0.04  0.15   0.00 1991-01-26  1991          1\n26      27   0.17  0.06  0.23   0.00 1991-01-27  1991          1\n27      28   0.50  0.02  0.04   0.00 1991-01-28  1991          1\n28      29   0.00  0.03  0.04   0.00 1991-01-29  1991          1\n29      30   0.00  0.04  0.08   0.00 1991-01-30  1991          1\n   ...    ...   ...   ...    ...        ...   ...        ...\n7275   336   0.00  0.04  0.06   0.03 2010-12-02  2010          1\n7276   337   0.30  0.05  0.08   0.02 2010-12-03  2010          1\n7277   338   1.62  0.08  0.12   0.02 2010-12-04  2010          1\n7278   339   0.00  0.10  0.15   0.02 2010-12-05  2010          1\n7279   340   0.00  0.09  0.15   0.02 2010-12-06  2010          1\n7280   341   0.00  0.04  0.06   0.02 2010-12-07  2010          1\n7281   342   0.00  0.04  0.06   0.02 2010-12-08  2010          1\n7282   343   0.00  0.16  0.25   0.02 2010-12-09  2010          1\n7283   344   0.00  0.22  0.35   0.02 2010-12-10  2010          1\n7284   345   0.04  0.04  0.06   0.02 2010-12-11  2010          1\n7285   346   0.01  0.02  0.03   0.02 2010-12-12  2010          1\n7286   347   0.04  0.02  0.03   0.02 2010-12-13  2010          1\n7287   348   0.00  0.05  0.08   0.02 2010-12-14  2010          1\n7288   349   0.37  0.08  0.13   0.02 2010-12-15  2010          1\n7289   350  13.19  0.08  0.13   0.02 2010-12-16  2010          1\n7290   351   1.33  0.06  0.10   0.02 2010-12-17  2010          1\n7291   352   0.00  0.08  0.12   0.02 2010-12-18  2010          1\n7292   353   0.01  0.05  0.08   0.02 2010-12-19  2010          1\n7293   354   0.00  0.03  0.04   0.02 2010-12-20  2010          1\n7294   355   9.70  0.07  0.12   0.02 2010-12-21  2010          1\n7295   356   0.00  0.06  0.11   0.02 2010-12-22  2010          1\n7296   357   0.00  0.07  0.12   0.02 2010-12-23  2010          1\n7297   358   0.13  0.08  0.13   0.02 2010-12-24  2010          1\n7298   359   0.10  0.09  0.15   0.02 2010-12-25  2010          1\n7299   360   0.00  0.07  0.12   0.02 2010-12-26  2010          1\n7300   361   0.00  0.08  0.14   0.02 2010-12-27  2010          1\n7301   362   0.61  0.18  0.32   0.02 2010-12-28  2010          1\n7302   363   0.00  0.15  0.26   0.02 2010-12-29  2010          1\n7303   364   3.95  0.10  0.18   0.02 2010-12-30  2010          1\n7304   365   0.10  0.10  0.18   0.01 2010-12-31  2010          1\n\n\nI would like to categorize the data based on the values in PREC and year columns. Here's the template of the code that I have:\n\nfor y in range(1991,2011):\n    for i in df.loc[df.year == y, 'PREC']:\n        if i &lt;= np.percentile(df.loc[df.year == y, 'PREC'], 10):\n            df.loc[df.PREC.isin([i]), 'intensity'] = 'light'\n\n\nHowever, this code categorize intensity in the whole dataframe, but I need to categorize  PREC for each year. So, how to do this?\n"
'I have a pandas series indexed by date and a pandas date range. The dates from the series are a subset of the date range.  Pandas must have a super-elegant way to join them and fill the missing values with zeroes - but I just can\'t think of it right now.\n\npandas.date_range(start, end)\n\nDatetimeIndex([\'2017-04-20\', \'2017-04-21\', \'2017-04-22\', \'2017-04-23\',\n               \'2017-04-24\', \'2017-04-25\', \'2017-04-26\', \'2017-04-27\',\n               \'2017-04-28\', \'2017-04-29\', \'2017-04-30\', \'2017-05-01\',\n               \'2017-05-02\', \'2017-05-03\', \'2017-05-04\', \'2017-05-05\',\n               \'2017-05-06\'],\n              dtype=\'datetime64[ns]\', freq=\'D\')\n\ndata.groupby("Day").size()\n\nDay\n2017-04-20    462\n2017-04-21     64\n2017-04-22     13\n2017-04-23      5\n2017-04-24      9\n2017-04-25      5\n2017-04-26      1\n2017-04-27      2\n2017-04-30      1\n2017-05-02      1\n2017-05-04      1\n2017-05-06      1\ndtype: int64\n\n\nDesired result:\n\nDay\n2017-04-20    462\n2017-04-21     64\n2017-04-22     13\n2017-04-23      5\n2017-04-24      9\n2017-04-25      5\n2017-04-26      1\n2017-04-27      2\n2017-04-28      0\n2017-04-29      0\n2017-04-30      1\n2017-05-02      1\n2017-05-03      0\n2017-05-04      1\n2017-05-05      0\n2017-05-06      1\ndtype: int64\n\n'
"I am learning Data science and while doing a problem, I came across a weird observation. The problem was to print the number of occurrences of the string 'Soup' on the Beautiful Soup home page, using python. The weird part is, the number of occurrences varies in the iPython notebook and in Python and when I ran a manual search on the webpage the result was entirely different. \n\nI'd love if someone could give a plausible explanation. I have attached along, the code snippets and the results:\n\nIn Python\n\n\n\nIn Pandas\n\n\n\nManually\n\n\n\nAs you can see the result varies in all the environments, it shows 39 occurrences in Python, 41 in Pandas and 35 via manual search. \n\nThanks \n"
"I have the following numpy array:\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nimport numpy as np\n\n# Tracking 4 associate metrics\n# Open TA's, Open SR's, Open SE's\nassociateMetrics = np.array([[111,  28,  21],\n   [ 27,  17,  20],\n   [ 79,  23,  17],\n   [185, 125,  50],\n   [155,  76,  32],\n   [ 82,  24,  17],\n   [127,  63,  33],\n   [193,  91,  63],\n   [107,  24,  17]])\n\n\nNow, I want to normalize every 'column' so that the values are between 0 and 1. What I mean is that the values in the 1st column for example should be between 0 and 1. \n\nHow do i do this?\n\nnormed_matrix = normalize(associateMetrics, axis=1, norm='l1')\n\n\nthe above gives me rowwise normalization\n"
'The issue I am having is the following: Portugal is a country next to Spain and it also has some islands. I want to select only Continental Portugal, what I mean by this is just the part of Portugal next to Spain and not include the islands.\n\nCan you help me please?\n\nThank you for the attention.\n'
"I am working with book crossing Data-set , It have a file which given user X's rating for book Y, but a lot of entries contains value 0 that means user X liked the book Y but did not give rating to it. I am using collaborative filtering hence these 0 entries are creating problems for me as if taken an 0 they decrease overall rating of the book.\n\nI am new to Data science field can some one help how to handle this ?\n\nWhat I can think of is replace 0 rating by user's average book rating but than again I don't any argument to support my Idea.\n"
'I\'m trying to write a function that runs through various iterations of classification algorithm (k-Means).\n\nIn sklearn.neighbors.KNeighborsClassifier, there are a few parameters to adjust: n_neighbors and leaf_size. I\'d like to know if there is a way to specify which parameter to adjustment during a particular iteration. \n\nfrom sklearn.neighbors import KNeighborsClassifier\ndef useNeighbors(iterations, *args):\n    print(iterations) #normal argument\n    for arg in args:\n        KNeighborsClassifier(arg=20)\n\nuseNeighbors(2, "n_neighbors", "leaf_size")\n\n\nI want this to essentially instantiate a KNeighborsClassifer instance twice- the first time with the # of neighbors at 20, and then the second time with the leaf size at 20 (default values for # of neighbors is 5, and default leaf size is 30).\n\nThis, however, unsurprisingly yields \n\n2\nTypeError: _init_params() got an unexpected keyword argument \'arg\'\n\n\nIt prints the iterations argument as expected, but then KNeighborsClassifer is not recognizing the string argument \'n_neighbors\' as my attempt to specify which parameter to adjust.\n\nHow do I switch which parameter/argument I want to adjust across many different iterations?\n\nAlso, obviously this is a toy case- I\'m asking because I\'m hoping to integrate different ML classification algorithms into an ensemble package with hyperparameters tuned through a Markov Chain Monte Carlo iterative method. But in order to do that, I need to be able to specify which parameters in each algorithm take the "steps" found in the Markov Chain across each iteration.\n'
'def read_csv(file_name):\n    f=open(file_name).read()\n    lis=f.split("\\n")\n    string_list=lis[1:len(lis)-1]\n    final_list=[]\n    for a in string_list:\n        string_fields=a.split(",")\n        int_field=[];    \n        for value in string_fields:\n            int_field.append(int(value))\n        final_list.append(int_field)\n        return(final_list)\ncdc_list=read_csv("US_births_1994-2003_CDC_NCHS.csv")\nprint(cdc_list[0:10])\n\n\nThis shows only the first element of cdc list.I am unable to find the error\n'
'I am trying to learn data analysis using the iris data set. So I just am copying the already written code for this subject and I get the following error regarding the libraries :\n\nTraceback (most recent call last):\n  File "iris.py", line 6, in &lt;module&gt;\n    from sklearn import model_selection\nImportError: cannot import name model_selection\n\n\nAnd here is how I import this module:\nfrom sklearn import model_selection\n\nI am using python 2.7,\nWhat could be the problem?\nI suspect there might be a problem with the version!right?or not?\n\nPlease don\'t suggest Anaconda, I am not willing to use it.\n\nThanks a bunch\n'
'I am trying to record summary statistics for precision and recall with tensorflow to use with tensor-board with the below code. \n\nI have added both global and local variables initializers, however this still throws an error telling me that i have an uninitialized value for \'recall\'.\n\nDoes anyone have any ideas on why this is still throwing an error?\n\nError message is below the code block\n\ndef classifier_graph(x, y, learning_rate=0.1):\n\n        with tf.name_scope(\'classifier\'):\n                with tf.name_scope(\'model\'):\n                        W = tf.Variable(tf.zeros([xdim, ydim]), name=\'W\')\n                        b = tf.Variable(tf.zeros([ydim]), name=\'b\')\n                        y_ = tf.matmul(x, W) + b\n\n                with tf.name_scope(\'cross_entropy\'):\n                        diff = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_)\n                        cross_entropy = tf.reduce_mean(diff)\n                        summary = tf.summary.scalar(\'cross_entropy\', cross_entropy)\n\n                with tf.name_scope(\'train\'):\n                        #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_), reduction_indices=[1]), name=\'cross_entropy\')\n                        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n                        # minimise cross_entropy via GD\n\n                #with tf.name_scope(\'init\'):\n                        #init = tf.global_variables_initializer()\n                        #local_init = tf.local_variables_initializer()\n                        #init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n                with tf.name_scope(\'init\'):\n                        init = tf.global_variables_initializer()\n                        init_l = tf.local_variables_initializer()\n\n\n                with tf.name_scope(\'metrics\'):\n                        recall = tf.metrics.recall(y, y_ )\n                        precision = tf.metrics.precision(y, y_)\n\n                        v_rec = tf.summary.scalar(\'recall\', recall)\n                        v_prec = tf.summary.scalar(\'precision\', precision)\n\n                        metrics = tf.summary.merge_all()\n\n        return [W, b, y_, cross_entropy, train_step, init, init_l, metrics]\n\n\n\ndef train_classifier(insamples, outsamples, batch_size, iterations, feature_set_index=1, model=None, device):\n    x = tf.placeholder(tf.float32, [None, xdim], name=\'x\') # None indications arbitrary first dimension\n    y = tf.placeholder(tf.float32, [None, ydim], name=\'y\')\n    W, b, y_, cross_entropy, train_step, init, init_l, metrics = classifier_graph(x, y)\n\n    with tf.Session(config=config) as sess, tf.device(device):\n        sess.run(init)\n        sess.run(init_l)\n        file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n\n        t = 0\n        while t &lt; iterations:\n\n            t += 1\n            _, err, metrics_str  = sess.run([train_step, cross_entropy, metrics], feed_dict={x: batch_x, y: batch_y })\n\n            all_err.append(err)\n            file_writer.add_summary(metrics_str,t)\n\n    return \'Done\'\n\n\nThe exact error message is below:\n\n    FailedPreconditionError (see above for traceback): Attempting to use uninitialized value recall/true_positives/count\n     [[Node: recall/true_positives/count/read = Identity[T=DT_FLOAT, _class=["loc:@recall/true_positives/count"], _device="/job:localhost/replica:0/task:0/gpu:0"](recall/true_positives/count)]]\n\n\nThanks!\n\nEDIT:\n\nUpon making the changes suggested by @Ishant Mrinal below, I encounter an error which i was previously hitting: \n\nInvalidArgumentError (see above for traceback): tags and values not the same shape: [] != [2] (tag \'precision_1\')\n\n\nThis suggests that the precision tensor is a different shape to the others, it does not throw this error for cross-entropy or recall.\n'
"The data is in the following link : http://www.fdic.gov/bank/individual/failed/banklist.html \n\nI want only the banks which closed in 2017. How can I do it in Pandas ?\n\nfailed_banks= pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')\nfailed_banks[0]\n\n\nWhat should I do after these lines of code to extract the desired result?\n"
"I have a super basic PYMC3 question: How do you sample from a transformed RV? I need this mostly for debugging purposes.\n\nFor example:\n\nimport pymc3 as pm\n\nwith pm.Model():\n   A = pm.Normal('A')\n\n   B = pm.Deterministic('B', A + 1)\n   # or\n   B = A + 1\n\n\nI can generate samples from A using A.random(), but that function is not defined for B. + 1 is a trivial transform but in case it were more involved I would like to draw some samples from the deformed A, i.e. B, to ensure everything is healthy.\n"
"I wrote below code to use Polynomial regression. Am able to fit the model, but not able to predict!! \n\ndef polynomial_function(power=5, random_state=9):\n    global X_train\n    global y_train\n\n    X_train =  X_train[['item_1','item_2','item_3','item_4']]\n    rng = np.random.RandomState(random_state)\n    poly = PolynomialFeatures(degree=power, include_bias=False)\n    linreg = LinearRegression(normalize=True)\n    new_X_train = poly.fit_transform(X_train)\n    linreg.fit(new_X_train, y_train)\n    new_x_test  = np.array([4, 5, 6, 7]).reshape(1, -1)\n    print linreg.predict(new_x_test)\n    return linreg\n\nlinreg = polynomial_function()\n\n\nAm getting below error message:\n\nValueError: shapes (1,4) and (125,) not aligned: 4 (dim 1) != 125 (dim 0)       \n\n\nThe error happens here, \n\nnew_x_test  = np.array([4, 5, 6, 7]).reshape(1, -1)\nprint linreg.predict(new_x_test)\n\n\nI found shape of new_X_train = (923, 125) \nand shape of new_x_test = (1, 4)\n\nHow does this matter?\n\nWhen I try to predict using a shape of (1, 4) does the algorithm try to convert it to a different shape?\n\nDoes it try to find out a polynomial of degree of 5 for test data? \n\nI am trying to Learn polynomial regression, can anyone explain what is happening?\n"
"hello I have a folder with name dict and that folder contains 4 to 6 text files, now I wanted to assign a ID docID to each text file in folder and I have used the code below \n\ndocID_list = [int(docID_string) for docID_string in os.listdir('/Users/suryavamsi/dict')]\n\n\nand I have got an error \n\ninvalid literal for int() with base 10: \n\n\nI have tried lots of ways but couldn't crack it can any one help me out \n"
"I have been using this dataset : https://www.kaggle.com/nsharan/h-1b-visa\nI have split the main dataframe into two:soc_null dataframe - where SOC_NAME column has NaN valuessoc_not_null - where SOC_NAME column has values other than NaN\nFor filling NaN values in SOC_NAME column of soc_null dataframe, I came up with this code:\n\nfor index1, row1 in soc_null.iterrows():\n    for index2, row2 in soc_not_null.iterrows():\n        if row1['JOB_TITLE'] == row2['JOB_TITLE']:\n            soc_null.set_value(index1,'SOC_NAME',row2['SOC_NAME'])\n\n\nThe problem with this code is that the length of soc_null is 17734 and the length of soc_not_null is 2984724, I ran this for a couple of hours but only a few hundred values were updated and hence it is not possible to execute this n^2 complexity code completely on a single machine.I believe there has to be a better way to do this and possibly over bigger datasets than mine, since there are several other parts following the cleaning process that will require two loops for processing.\n"
"Say the data is like\n\nd = {'col1': ['a,b', 'b', 'c,d', 'a,c'], 'col2': [3, 4, 5, 6]}\ns = pd.DataFrame(d)\n    col1    col2\n0   a,b      3\n1   b        4\n2   c,d      5\n3   a,c      6\n\n\nWould like to one hot encode the col1. As follows:\n\n    a   b   c   d\n0   1   1   0   0   \n1   0   1   0   0   \n2   0   0   1   1\n3   1   0   1   0\n\n\nThanks\n"
"I want to assess the statistical difference of male and female users group by each of their total plays (see below example):\n\nExample of female entries\n\nfemale\n\n    users   artist  plays   gender  age\n0   48591   sting   12763   f       25.0\n1   48591   stars   8192    f       25.0\n\n\nSum plays per unique female user\n\nfemale_user_plays = female.groupby('users').plays.sum()\n\nfemale_user_plays\n\nusers\n5         5479\n6         3782\n7         7521\n11        7160\n\n\nExample of male entries\n\nfemale\n    users   artist         plays    gender  age\n51  56496   iron maiden    456      m       28.0\n52  56496   elle           407      m       28.0\n\n\nSum plays per unique male user\n\nmale_user_plays = male.groupby('users').plays.sum()\nmale_user_plays\n\nusers\n0         3282\n1        25329\n2        51522\n3         1590\n\n\nAverage plays per gender\n\nAverage Total Male Plays: 11880\nAverage Total Female Plays: 13104\n\n\nBefore trying the t test, I converted each Series into value lists: \n\nfemale_plays_list = female_user_plays.values.tolist()\nmale_plays_list = male_user_plays.values.tolist()\n\n\nAnd for the t test:\n\nttest_ind(female_plays_list, male_plays_list, equal_var=False)\n\n\nThe result is what's confused me since the outputs seem very off and I'm thinking it's not due to variance of the two sample sizes....\n\nTtest_indResult(statistic=-8.9617251652001002, pvalue=3.3195063228833119e-19)\n\n\nIs there any reason outside of array length that could be causing this?\n"
'I am trying to use a log scale as the margin plots for my seaborn jointplot. I am usings set_xticks() and set_yticks(), but my changes do not appear. Here is my code below and the resulting graph:\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\ntips = sns.load_dataset(\'tips\')\nfemale_waiters = tips[tips[\'sex\']==\'Female\']\n\ndef graph_joint_histograms(df1):\n    g=sns.jointplot(x = \'total_bill\',y = \'tip\', data = tips, space = 0.3,ratio = 3)\n    g.ax_joint.cla()\n    g.ax_marg_x.cla()\n    g.ax_marg_y.cla()\n\n    for xlabel_i in g.ax_marg_x.get_xticklabels():\n        xlabel_i.set_visible(False)\n    for ylabel_i in g.ax_marg_y.get_yticklabels():\n        ylabel_i.set_visible(False)\n\n    x_labels = g.ax_joint.get_xticklabels()\n    x_labels[0].set_visible(False)\n    x_labels[-1].set_visible(False)\n\n    y_labels = g.ax_joint.get_yticklabels()\n    y_labels[0].set_visible(False)\n    y_labels[-1].set_visible(False)\n\n    g.ax_joint.set_xlim(0,200)\n    g.ax_marg_x.set_xlim(0,200)\n\n    g.ax_joint.scatter(x = df1[\'total_bill\'],y = df1[\'tip\'],data = df1,c = \'y\',edgecolors= \'#080808\',zorder = 2)\n    g.ax_joint.scatter(x = tips[\'total_bill\'],y = tips[\'tip\'],data = tips, c= \'c\',edgecolors= \'#080808\')\n\n    ax1 =g.ax_marg_x.get_axes()\n    ax2 = g.ax_marg_y.get_axes()\n    ax1.set_yscale(\'log\')\n    ax2.set_xscale(\'log\')\n\n    ax1.set_yscale(\'log\')\n    ax2.set_xscale(\'log\')\n\n    ax2.set_xlim(1e0, 1e4)\n    ax1.set_ylim(1e0, 1e3)\n    ax2.xaxis.set_ticks([1e0,1e1,1e2,1e3])\n    ax2.xaxis.set_ticklabels(("1","10","100","1000"), visible = True)\n\n\n    plt.setp(ax2.get_xticklabels(), visible = True)\n    colors = [\'y\',\'c\']\n    ax1.hist([df1[\'total_bill\'],tips[\'total_bill\']],bins = 10, stacked=True,log = True,color = colors, ec=\'black\')\n\n    ax2.hist([df1[\'tip\'],tips[\'tip\']],bins = 10,orientation = \'horizontal\', stacked=True,log = True,color = colors, ec=\'black\')\nax2.set_ylabel(\'\')\n\n\nAny ideas would be much appreciated.\n\nHere is the resulting graph: \n'
"I am trying to write a function to calculate a new variable into a new column. I have a dataset which tracks a variable for several subjects over many days.\n\nDate        Athlete  Load\n2016-01-04  Alan     180\n2016-01-04  Ben      61\n2016-01-04  David    186\n2016-01-04  Joe      99\n2016-01-04  John     131\n\n\nI have been able to filter the subjects by name and create new data frames for each. \n\nfor athlete in df['Athlete'].unique():\n    athlete = df.loc[ewma['Athlete'] == athlete]\n    print(athlete.head())\n\n\nThe part I am having an issue with is the equation to calculate the new column. The first value is calculated from the first measured variable, but each subsequent value uses the previous day's value.\n\nFor example, the first row of the new column would use:\n\nx = (df['Load'].iloc[0] * 2) - (df['Load'].iloc[0] / 2)\n\n\nx = 180 \n\nThe second row would use the previous day's value (x) in place of the second df['Load'] value. I was able to calculate the second value correctly with a basic function:\n\ny = (df['Load'].iloc[1] * 2) - (x / 2)\n\n\ny = 168\n\nI tried using an 'if/else' but it did not calculate the correct values.\n\nif df.index.name == '0':\n    (df['Load'].iloc[0] * 2) - (df['Load'].iloc[0] / 2)\nelse:\n     (df['Load'] * 2) - (df['Load'].shitf(-1) / 2)\n\n\nAny recommendations would be greatly appreciated.\n"
'I have to print out the min/max values from a specific column, which I was able to do. But I also need to show the data from all the columns of the min/max fields.\n\nbelow is the code: \n\nimport csv\n\nwith open(\'phone_data.csv\',\'r\') as p_data:\ndata = csv.reader(p_data, delimiter=\',\')\nnext(data)\nd_col = list(data)\n\nminTemp = min([float(elem[2]) for elem in d_col])\nmaxTemp = max([float(elem[2]) for elem in d_col])\nprint("min value is: ", minTemp)\nprint("max value is: ", maxTemp)\n\n'
'How to replace a part string value of a column using another column.\n\nMy DataSet here is :\n\nID          Product Name                            Size ID    Size Name\n1   24 Mantra Ancient Grains Foxtail Millet 500 gm      1       500 gm\n2   24 Mantra Ancient Grains Little Millet 500 gm       2       500 gm\n3   24 Mantra Naturals Almonds 100 gm                   3       100 gm\n4   24 Mantra Naturals Kismis 100 gm                    4       100 gm\n5   24 Mantra Organic Ajwain 100 gm                     5       100 gm\n6   24 Mantra Organic Apple Blast Drink 250 ml          6       250 ml\n7   24 Mantra Organic Apple Juice 1 Ltr Tetra Pack      7       1000 ml\n8   24 Mantra Organic Apple Juice 200 ml                8       200 ml\n9   24 Mantra Organic Assam Tea 100 gm                  9       100 gm\n\n\nRequirement here is the Product Name column value is 24 Mantra Ancient Grains Foxtail Millet 500 gm and the Size Name column has 500 Gm.  In this case my output will be 24 Mantra Ancient Grains Foxtail Millet.\nIf the Size Name contains in the Product Name string remove the size name word ignoring the case else no need to take any action.\n'
"I'm trying to figure out how to sort through rows in a spreadsheet read with pandas and save values to variables.\n\nHere is my code so far: \n\n\r\n\r\nimport pandas as pd\r\nfrom pandas import ExcelWriter\r\nfrom pandas import ExcelFile\r\n \r\ndf = pd.read_excel('data_file.xlsx', sheetname='Sheet 1')\r\n\r\n\r\nfor line in df:\r\n    if line.startswith(line):\r\n\r\n\r\n\n\nThe data is formatted the following way: \n\nColumn 1 has runner numbers, column 2 has 100 meter sprint times, Column 3 has 400 meter sprint times.\n\nHere's an example of the data:\n\n\r\n\r\nRunner  100m   400m\r\n  1     43.7   93.5\r\n  1     37.5   87.6\r\n  1     39.2   82.5\r\n  2     28.9   67.9\r\n  2     26.2   69.9\r\n  2     33.3   60.25\r\n  2     34.2   60.65\r\n  3     19.9   45.5\r\n  3     19.8   44.0\r\n  4     18.7   50.0\r\n  4     19.0   52.4\r\n\r\n\r\n\n\nHow could I store the contents of all the rows starting with 1 in a unique variable, all the rows starting with 2 in another variable, 3, etc.? I know this has to involve a loop of some sort but I'm not sure about how to approach this problem.\n"
'cpf  day  startdate              enddate\n1234  1   08/01/2018 12:50:0     08/01/2018 15:30:0\n1234  1   08/01/2018 14:30:0     08/01/2018 15:40:0\n1234  1   08/01/2018 14:50:0     08/01/2018 15:50:0\n1234  2   08/02/2018 20:20:0     08/02/2018 23:50:0\n1234  2   08/02/2018 22:50:0     08/02/2018 23:50:0\n1235  1   08/01/2018 11:50:0     08/01/2018 15:20:0\n5212  1   08/01/2018 14:50:0     08/01/2018 15:20:0\n\n\nI need to calculate conversation time of cpf column in one day. For example, the first cpf is 1234, so in day 1 this cpf initiate a conversation on 08/01/2018 12:50:0 and the end of conversation was 08/01/2018 15:50:0, what I need is exactly this substraction about enddate - startdate, but disconsidering middle of table like 1234 have in 08/01/2018 three conversations, the subtraction is about first hour of first conversation subtract last hour of last conversation. How can I do this?\n\n  cpf  day  startdate              enddate              Time_Conversation\n    1234  1   08/01/2018 12:50:0     08/01/2018 15:30:0         3:00:0\n    1234  1   08/01/2018 14:30:0     08/01/2018 15:40:0         3:00:0\n    1234  1   08/01/2018 14:50:0     08/01/2018 15:50:0         3:00:0\n    1234  2   08/02/2018 20:20:0     08/02/2018 23:50:0         3:30:0\n    1234  2   08/02/2018 22:50:0     08/02/2018 23:50:0         3:30:0\n    1235  1   08/01/2018 11:50:0     08/01/2018 15:20:0         4:30:0\n    5212  1   08/01/2018 14:50:0     08/01/2018 15:20:0         4:30:0\n\n'
'Code Snippet\n\ns = int(input())\n       # finallog.append(pd.Series([CurrentClock,timedelta(seconds=s,minutes=m),CurrentProgramCode,0,0],index=finallog.columns()), ignore_index=True)\n       # finallog.loc[j]=[CurrentClock,timedelta(seconds=s,minutes=m),CurrentProgramCode,0,0]\n       # j+=1\ndf_2 = pd.DataFrame(data={\n             \'CurrentTime\':[CurrentClock],\n             \'BookingNumber\':[""],\n             \'Duration\':[timedelta(seconds=s, minutes=m)],\n             \'BrandCode\':[""], \n             \'TapeCode\':[CurrentProgramCode]})\nfinallog = pd.concat([finallog,df_2],axis=0)\nprint(finallog)\n\n\nError message as:\n\nUnboundLocalError                         Traceback (most recent call last)\n&lt;ipython-input-45-6144a7142686&gt; in &lt;module&gt;()\n\n----&gt; 1 startup()\n\n&lt;ipython-input-25-1490643b2ba7&gt; in startup()\n\n     17        # j+=1\n\n     18 df_2=pd.DataFrame(data={\'CurrentTime\':[CurrentClock],\'BookingNumber\':[""],\'Duration\':[timedelta(seconds=s, minutes=m)],\'BrandCode\':[""],\'TapeCode\':[CurrentProgramCode]})\n\n     19 **finallog=pd.concat([finallog,df_2],axis=0)**\n\n     20 print(finallog)\n\n     21 CurrentClock=CurrentClock+timedelta(seconds=s,minutes=m)\n\n&gt; UnboundLocalError: local variable \'finallog\' referenced before assignment\n\n\nPls, suggest any workaround to make this work. Also if one of the commented out methods are better to add pls suggest because when I tried all seemed to be giving some or the other errors and all run-time ones. Thank you.\n'
'In this script below I am experimenting around with OpenCV and calculating a distance to my laptop webcam of a face detection with Haar Cascades. I am using a windows 10 laptop with a web cam, Python 3.6, and OpenCV 3.4.\n\nI am having an issue with the OpenCV.putext of displaying this calculated value on the  view of the video stream…\n\n        text = "Inches{}".format(np.int(inches))\n        cv2.putText(gray, text, (roi[0] - 10, roi[1] - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n\n\nThe code will run if this is commented out.. Any tips for what I am doing wrong would be greatly appreciated!\n\nimport numpy as np\nimport imutils\nimport cv2\nfrom imutils.video import VideoStream\nfrom imutils.video import FPS\nimport time\n\n\n\ndef distance_to_camera(knownWidth, focalLength, perWidth):\n    # compute and return the distance from the maker to the camera\n    return (knownWidth * focalLength) / perWidth\n\nface_cascade = cv2.CascadeClassifier(\'C:/Users/Haar/frontalFace10/haarcascade_frontalface_alt.xml\')\n\n#Calculated from a different script\nfocalLength = 709.0909090909091\n\n#average human head width\nknownWidth = 7\n\n\n# Initialize mutithreading the video stream.\ncamera = VideoStream(src=0).start()\n\n# Allow the camera to warm up.\ntime.sleep(2.0)\n\n#start FPS\nfps = FPS().start()\n\nroi = None\n\nwhile True:\n        image = camera.read()\n        image = imutils.resize(image, width=500)\n\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5);\n        for (x, y, w, h) in faces:\n                cv2.rectangle(gray,(x,y),(x+w,y+h),(255,255,255),2)\n                roi = gray[y:y+h, x:x+w]\n\n        if roi is None:\n                pass\n        else:\n                inches = distance_to_camera(knownWidth, focalLength, roi.shape[1])\n                print(inches)\n                text = "Inches{:.2f}".format(np.int(inches))\n                cv2.putText(gray, text, (roi[0] - 10, roi[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n\n\n        cv2.imshow("gray", gray)\n        key = cv2.waitKey(1) &amp; 0xFF\n\n        fps.update()\n\n        # if the `q` key was pressed, break from the loop\n        if key == ord("q"):\n                break\n\nfps.stop()\nprint("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))\nprint("[INFO] approx. FPS: {:.2f}".format(fps.fps()))\n\ncamera.stop()\ncv2.destroyAllWindows()\n\n\nThis is the full traceback of the error:\n\nTraceback (most recent call last):\n  File "C:\\Users\\distance-to-camera\\selectHaar3.py", line 53, in &lt;module&gt;\n    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\nTypeError: only size-1 arrays can be converted to Python scalars\n&gt;&gt;&gt; \n\n'
'I am creating a bokeh plot with a slider to refresh plot accordingly. There are 2 issues with the code posted.\n1. The plot is not refreshed as per the slider. Please help in providing a fix for this issue.\n2. Plot is not displayed with curdoc() when bokeh serve --show fn.ipynb is used\n\nI\'m trying to visualise this CSV file.\n\nimport pandas as pd\nimport numpy as np\nfrom bokeh.models import ColumnDataSource, CategoricalColorMapper, HoverTool, Slider\nfrom bokeh.plotting import figure, curdoc\nfrom bokeh.palettes import viridis\nfrom bokeh.layouts import row, widgetbox\n\n#Importing and processing data file \ncrop = pd.read_csv(\'crop_production.csv\') \n\n#Cleaning Data \ncrop.fillna(np.NaN) \ncrop[\'Season\'] = crop.Season.str.strip() \n\n#Removing Whitespace #Filtering the dataset by Season \ncrop_season = crop[crop.Season == \'Whole Year\'] \ncrop_dt = crop_season.groupby([\'State_Name\', \'District_Name\', \'Crop_Year\']).mean().round(1)\n\n#Creating Column Data Source\nsource = ColumnDataSource({\n        \'x\'        : crop_dt[crop_dt.index.get_level_values(\'Year\')==2001].loc[([\'ABC\']), :].Area,\n        \'y\'        : crop_dt[crop_dt.index.get_level_values(\'Year\')==2001].loc[([\'ABC\']), :].Production,\n        \'state\'    : crop_dt[crop_dt.index.get_level_values(\'Year\')==2001].loc[([\'ABC\']), :].index.get_level_values(\'State_Name\'),\n        \'district\' : crop_dt[crop_dt.index.get_level_values(\'Year\')==2001].loc[([\'ABC\']), :].index.get_level_values(\'District_Name\')\n})\n\n\n#Creating color palette for plot\ndistrict_list = crop_dt.loc[([\'Tamil Nadu\']), :].index.get_level_values(\'District_Name\').unique().tolist()\ncall_colors = viridis(len(district_list))\ncolor_mapper = CategoricalColorMapper(factors=district_list, palette=call_colors)\n\n\n# Creating the figure\n#xmin, xmax = min(data.Crop_Year), max(data.Crop_Year)\n#ymin, ymax = min(data.Production), max(data.Production)\np = figure(\n    title = \'Crop Area vs Production\',\n    x_axis_label = \'Area\',\n    y_axis_label = \'Production\',\n    plot_height=900, \n    plot_width=1200,\n    tools = [HoverTool(tooltips=\'@district\')]\n          )\np.circle(x=\'x\', y=\'y\', source=source, size=12, alpha=0.7, \n         color=dict(field=\'district\', transform=color_mapper),\n         legend=\'district\')\np.legend.location = \'top_right\'\n\n\ndef update_plot(attr, old, new):\n    yr = slider.value\n    new_data = {\n        \'x\'        : crop_dt[crop_dt.index.get_level_values(\'Year\')==yr].loc[([\'ABC\']), :].Area,\n        \'y\'        : crop_dt[crop_dt.index.get_level_values(\'Year\')==yr].loc[([\'ABC\']), :].Production,\n        \'state\'    : crop_dt[crop_dt.index.get_level_values(\'Year\')==yr].loc[([\'ABC\']), :].index.get_level_values(\'State_Name\'),\n        \'district\' : crop_dt[crop_dt.index.get_level_values(\'Year\')==yr].loc[([\'ABC\']), :].index.get_level_values(\'District_Name\')\n    }\n    source.data = new_data\n\n#Creating Slider for Year\nstart_yr = min(crop_dt.index.get_level_values(\'Crop_Year\'))\nend_yr = max(crop_dt.index.get_level_values(\'Crop_Year\'))\nslider = Slider(start=start_yr, end=end_yr, step=1, value=start_yr, title=\'Year\')\nslider.on_change(\'value\',update_plot)\n\nlayout = row(widgetbox(slider), p)\ncurdoc().add_root(layout)\nshow(layout)\n\n\nAlso tried a different option using CustomJS as below, but still no luck.\n\ncallback = CustomJS(args=dict(source=source), code="""\n    var data = source.data;\n    var yr = slider.value;\n    var x = data[\'x\']\n    var y = data[\'y\']\n    \'x\'        = crop_dt[crop_dt.index.get_level_values(\'Crop_Year\')==yr].loc[([\'ABC\']), :].Area;\n    \'y\'        = crop_dt[crop_dt.index.get_level_values(\'Crop_Year\')==yr].loc[([\'ABC\']), :].Production;\n    p.circle(x=\'x\', y=\'y\', source=source, size=12, alpha=0.7, \n         color=dict(field=\'district\', transform=color_mapper),\n         legend=\'district\');\n    }\n    source.change.emit();\n""")\n\n\n\n#Creating Slider for Year\nstart_yr = min(crop_dt.index.get_level_values(\'Crop_Year\'))\nend_yr = max(crop_dt.index.get_level_values(\'Crop_Year\'))\nyr_slider = Slider(start=start_yr, end=end_yr, step=1, value=start_yr, title=\'Year\', callback=callback)\ncallback.args["slider"] = yr_slider\n\n'
"I am attempting to follow this first answer of this SO post for calculating an event duration. The example in the post is rainfall duration and the person wants to know a totalization of the rainfall and a duration in hours of the rainfall event.\n\nMy scenario is a similar time series but the application is a pump and I am wanting to know a totalized duration in hours per day that the pump runs. My data is a pump speed command, and anytime the pump speed is greater than 0.0 the pump is running.\n\nTo start with, I am reading my CSV file into Pandas.\n\n#read CSV file\ndf = pd.read_csv('C:\\\\Users\\\\desktop\\\\data.csv', index_col='Date', parse_dates=True)\n\n# Converting the index as date\ndf.index = pd.to_datetime(df.index)\n\ndf\n\n\nExcept I am running into an issue when attempting to convert my Date index into date time. This returns a ValueError: day is out of range for month\n\nWould anyone know  a fix for this? Ultimetely this is the code that I am attempting to recreate from the SO post 1st answer where the author is creating help columns...\n\n# create helper columns defining contiguous blocks and day\ndf['block'] = (df['Pump4VFD'].astype(bool).shift() != df['Pump4VFD'].astype(bool)).cumsum()\ndf['day'] = df.index.dt.normalize()\n\n# group by day to get unique block count and value count\nsession_map = df[df['value'].astype(bool)].groupby('day')['block'].nunique()\nhour_map = df[df['value'].astype(bool)].groupby('day')['value'].count()\n\n# map to original dataframe\ndf['sessions'] = df['day'].map(session_map)\ndf['hours'] = df['day'].map(hour_map)\n\n# calculate result\nres = df.groupby(['day', 'hours', 'sessions'], as_index=False)['value'].sum()\nres['duration'] = res['hours'] / res['sessions']\nres['amount'] = res['value'] / res['sessions']\n\n\nMy data looks like this:\n\n                    Pump4VFD\nDate                                                                    \n1/0/00 12:45 AM          0.0\n1/0/00 12:50 AM          0.0\n1/0/00 12:55 AM          0.0\n1/0/00 12:00 AM          0.0\n1/0/00 1:05 AM           0.0\n\n"
"I am really new to python and GMM. I learned GMM recently and trying to implement the codes from here\nI met some problems when I run gmm.sample() method:\ngmm16 = GaussianMixture(n_components=16, covariance_type='full', random_state=0)    \nXnew = gmm16.sample(400,random_state=42)\nplt.scatter(Xnew[:, 0], Xnew[:, 1])\n\nthe error shows:\nTypeError: sample() got an unexpected keyword argument 'random_state'\n\nI have checked the latest document and find out the method sample should only contain n which indicates that the number of samples to generate. But when I delete 'random_state=42', new error appears:\ncodes:\nXnew = gmm16.sample(400)\nplt.scatter(Xnew[:, 0], Xnew[:, 1])\n\nerror:\nTypeError: tuple indices must be integers or slices, not tuple\n\nDoes anyone meet this problem when you implement the codes from Jake VanderPlas? How could I fix it?\nMy Jupyter:\n\nThe version of the notebook server is: 5.7.4\nPython 3.7.1 (default, Dec 14 2018, 13:28:58)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\n\n"
'First off, thanks in advance if you can help puzzle this out! I\'m trying to balance some customer data for my model. My targets are all 1s and 0s, and the 0s are overwhelmingly abundant. So I created a counter that will start to delete the 0 rows once they surpass the number of 1 rows. But at the very end of my code, when I create the np.delete to get those extra rows off my dataset I keep getting this error\n\nI don\'t really know what to try, because I don\'t even understand what the error is telling me\n\nimport pandas as pd \nimport numpy as np \nfrom sklearn import preprocessing\n#%%\n#Loading the Raw Data\nraw_csv_data= pd.read_csv(\'Audiobooks-data_raw.csv\')\nprint(display(raw_csv_data.head(20)))\n#%%\ndf=raw_csv_data.copy()\nprint(display(df.head(20)))\n#%%\nprint(df.info())\n#%%\n#Separate the Targets from the dataset\ninputs_all= df.loc[:,\'Book length (mins)_overall\':\'Last visited minus Purchase date\']\ntargets_all= df[\'Targets\']\nprint(display(inputs_all.head()))\nprint(display(targets_all.head()))\n#%%\n#Shuffling the Data to prep for balancing\nshuffled_indices= np.arange(inputs_all.shape[0])\nnp.random.shuffle(shuffled_indices)\nshuffled_inputs= inputs_all.iloc[shuffled_indices]\nshuffled_targets= targets_all[shuffled_indices]\n#%%\n#Balance the Dataset\n#There are significantly more 0\'s than 1\'s in our target.\n#We want a good accurate model\nprint(inputs_all.shape)\nprint(targets_all.shape)\n#%%\nnum_one_targets= int(np.sum(targets_all))\nzero_targets_counter= 0\nindices_to_remove= []\nprint(num_one_targets)\n#%%\nfor i in range(targets_all.shape[0]):\n    if targets_all[i]==0:\n        zero_targets_counter +=1\n        if zero_targets_counter&gt; num_one_targets:\n            indices_to_remove.append(i)\n\n#%%\n\ninputs_all_balanced= np.delete(inputs_all, indices_to_remove, axis=0)\ntargets_all_balanced= np.delete(targets_all, indices_to_remove, axis=0)\n\n\nEverything works except when I try to group my balanced datasets and delete the excess 0 rows. Here is the error:\n\nValueError                                Traceback (most recent call last)\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in create_block_manager_from_blocks(blocks, axes)\n   1652 \n-&gt; 1653         mgr = BlockManager(blocks, axes)\n   1654         mgr._consolidate_inplace()\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in __init__(self, blocks, axes, do_integrity_check)\n    113         if do_integrity_check:\n--&gt; 114             self._verify_integrity()\n    115 \n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in _verify_integrity(self)\n    310             if block._verify_integrity and block.shape[1:] != mgr_shape[1:]:\n--&gt; 311                 construction_error(tot_items, block.shape[1:], self.axes)\n    312         if len(self.items) != tot_items:\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in construction_error(tot_items, block_shape, axes, e)\n   1690     raise ValueError("Shape of passed values is {0}, indices imply {1}".format(\n-&gt; 1691         passed, implied))\n   1692 \n\nValueError: Shape of passed values is (4474, 10), indices imply (14084, 10)\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n in \n----&gt; 1 inputs_all_balanced= np.delete(inputs_all, indices_to_remove, axis=0)\n      2 targets_all_balanced= np.delete(targets_all, indices_to_remove, axis=0)\n\n~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py in delete(arr, obj, axis)\n   4419 \n   4420     if wrap:\n-&gt; 4421         return wrap(new)\n   4422     else:\n   4423         return new\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in __array_wrap__(self, result, context)\n   1907     def __array_wrap__(self, result, context=None):\n   1908         d = self._construct_axes_dict(self._AXIS_ORDERS, copy=False)\n-&gt; 1909         return self._constructor(result, **d).__finalize__(self)\n   1910 \n   1911     # ideally we would define this to avoid the getattr checks, but\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __init__(self, data, index, columns, dtype, copy)\n    422             else:\n    423                 mgr = init_ndarray(data, index, columns, dtype=dtype,\n--&gt; 424                                    copy=copy)\n    425 \n    426         # For data is list-like, or Iterable (will consume into list)\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py in init_ndarray(values, index, columns, dtype, copy)\n    165         values = maybe_infer_to_datetimelike(values)\n    166 \n--&gt; 167     return create_block_manager_from_blocks([values], [columns, index])\n    168 \n    169 \n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in create_block_manager_from_blocks(blocks, axes)\n   1658         blocks = [getattr(b, \'values\', b) for b in blocks]\n   1659         tot_items = sum(b.shape[0] for b in blocks)\n-&gt; 1660         construction_error(tot_items, blocks[0].shape[1:], axes, e)\n   1661 \n   1662 \n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in construction_error(tot_items, block_shape, axes, e)\n   1689         raise ValueError("Empty data passed with indices specified.")\n   1690     raise ValueError("Shape of passed values is {0}, indices imply {1}".format(\n-&gt; 1691         passed, implied))\n   1692 \n   1693 \n\nValueError: Shape of passed values is (4474, 10), indices imply (14084, 10)\n\n'
"I have a JSON date data set and trying to calculate the time difference between two different JSON DateTime. \n\nFor example : \n\n'2015-01-28T21:41:38.508275' - '2015-01-28T21:41:34.921589'\n\n\nPlease look at the python code below:\n\n#let's say 'time' is my data frame and JSON formatted time values are under the 'due_date' column\ntime_spent = time.iloc[2]['due_date'] - time.iloc[10]['due_date']\n\n\nThis doesn't work. I also tried to cast each operand to int, but it also didn't help. What are the different ways to perform this calculation?\n"
'I am working through this multiple regression problem with this walk through however the code that starts at  \n\nsection : #Treating categorical variables with One-hot-encoding at website: https://towardsdatascience.com/what-makes-a-movie-hit-a-jackpot-learning-from-data-with-multiple-linear-regression-339f6c1a7022\n\nI ran code up to this point but it doesn\'t work for (X)\n\nActual code:    \n\n from sklearn import preprocessing\n le = preprocessing.LabelEncoder()\n\n\n# LabelEncoder for a number of columns\nclass MultiColumnLabelEncoder:\n\n def __init__(self, columns = None):\n    self.columns = columns # list of column to encode\n    def fit(self, X, y=None):\n    return self\n    def transform(self, X):\n    \'\'\'\n    Transforms columns of X specified in self.columns using\n    LabelEncoder(). If no columns specified, transforms all\n    columns in X.\n    \'\'\'\n\n    output = X.copy()\n\n    if self.columns is not None:\n        for col in self.columns:\n            output[col] = LabelEncoder().fit_transform(output[col])\n    else:\n        for colname, col in output.iteritems():\n            output[colname] = LabelEncoder().fit_transform(col)\n\n    return output\ndef fit_transform(self, X, y=None):\n    return self.fit(X, y).transform(X)\n\n  le = MultiColumnLabelEncoder()\n  X_train_le = le.fit_transform(X)\n\n\nHere is the error that I get:    \n\n Traceback (most recent call last):\n\n  File "&lt;ipython-input-63-581cea150670&gt;", line 34, in &lt;module&gt;\n    X_train_le = le.fit_transform(X)\n\nNameError: name \'X\' is not defined\n\n'
"I have a df such as below ( 3 rows for example )\n\nID | Dollar_Value\nC       45.32\nE       5.21\nV       121.32\n\n\nWhen I view the df in my notebook such as df:\nIt shows the Dollar_value as\n\nID | Dollar_Value\nC       8.493000e+01\nE       2.720000e+01\nV       1.720000e+01\n\n\nInstead of the regular format, but when I try to filter the df for specific ID, it shows the values as they are supposed to be ( 82.23 or 2.45) \n\ndf[df['ID'] == 'E']\n\n ID | Dollar_Value\n E       45.32\n\n\nis there something I have to do formatting wise? So the df itself can display the value column as its supposed to? \n\nThanks!\n"
'I have the following simple python function that calculates the entropy of a single input X according to Shannon\'s Theory of Information:\n\nimport numpy as np\n\ndef entropy(X:\'numpy array\'):\n  _, frequencies = np.unique(X, return_counts=True)\n  probabilities  = frequencies/X.shape[0]\n  return -np.sum(probabilities*np.log2(probabilities))\n\na = np.array([1., 1., 1., 3., 3., 2.])\nb = np.array([1., 1., 1., 3., 3., 3.])\nc = np.array([1., 1., 1., 1., 1., 1.])\n\nprint(f"entropy(a): {entropy(a)}")\nprint(f"entropy(b): {entropy(b)}")\nprint(f"entropy(c): {entropy(c)}")\n\n\nWith the output being the following:\n\nentropy(a): 1.4591479170272446\nentropy(b): 1.0\nentropy(c): -0.0\n\n\nHowever, I also need to calculate the derivative over dx:\n\n\n  d entropy / dx\n\n\nThis is not an easy task since the main formula\n\n\n  -np.sum(probabilities*np.log2(probabilities))\n\n\ntakes in probabilities, not x values, therefore it is not clear how to differentiate over dx.  \n\nDoes anyone have an idea on how to do this?\n'
'I don\'t understand why the counter inside de animate function is not getting added to itself.\n\nI tried to use the animation to print new values of two lists of values, each one each axis, each second.\n\nEDITED: The code:\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [1, 12, 10, 1, 5, 3, 5, 7, 4, 4]\n\nx_vals = []\ny_vals = []\n\ncnt = 0\ndef animate(i, cnt):\n    print(cnt)\n    print(\'loop \' + str(cnt))\n\n    x_vals.append(x[cnt])\n    y_vals.append(y[cnt])\n\n    print(x_vals, y_vals)\n    cnt += 1  # this is not happening\n    plt.plot(x_vals, y_vals, label=\'Price\')\n\nani = FuncAnimation(plt.gcf(), animate, fargs=(cnt,), interval=1000)\n\n# plt.tight_layout()\nplt.show()\n\n\nI declare cnt outside the animate function and I get this output ( thank\'s PartialOrder user):\n\n0\nloop 0\n[1] [1]\n0\nloop 0\n[1, 1] [1, 1]\n0\nloop 0\n[1, 1, 1] [1, 1, 1]\n....\n\n\nWhich shows that the loop in the animate function is executed each second, but counter not getting added because reset to zero each loop.\nAnd if declare cnt counter before the function (not passed in argument), I get:\n\nFile "test3.py", line 13, in animate\n    print(\'loop \' + str(cnt))\nUnboundLocalError: local variable \'cnt\' referenced before assignment\n\n\nBoth ways I get an empty graph:\n\nenter image description here\n\nWhy the counter is not getting added, and how to pass external counter to the function?\n\nThank you!\n'
"I am trying to create a proper bin for a timestamp interval column,\n\nusing code such as \n\ndf['Bin'] = pd.cut(df['interval_length'], bins=pd.to_timedelta(['00:00:00','00:10:00','00:20:00','00:30:00','00:40:00','00:50:00','00:60:00']))\n\n\nThe Resulting df looks like:\n\ntime_interval  |           bin\n  00:17:00        (0 days 00:10:00, 0 days 00:20:00]\n  01:42:00                NaN\n  00:15:00        (0 days 00:10:00, 0 days 00:20:00]\n  00:00:00                NaN\n  00:06:00        (0 days 00:00:00, 0 days 00:10:00]\n\n\nWhich is a little off as the result I want is jjust the time value and not the days and also I want the upper limit or last bin to be 60 mins or inf ( or more)\n\nDesired Output:\n\ntime_interval  |           bin\n      00:17:00        (00:10:00,00:20:00]\n      01:42:00        (00:60:00,inf]\n      00:15:00        (00:10:00,00:20:00]\n      00:00:00        (00:00:00,00:10:00]\n      00:06:00        (00:00:00,00:10:00]\n\n\nThanks for looking!\n"
'I have a Syslog wifi log I have loaded into a dataframe as follows:\n\nwifi = pd.read_csv(\'./nov2019-wifi-stats.csv\', \n                   sep=\'\\t\', \n                   parse_dates=[2],\n                   header=None,\n                   names=[\'branch\',\'datetime\',\'ip\',\'mac\',\'status\'],\n                   index_col=1)\n\nwifi.head()\n\ndatetime            branch      ip              mac                 status          \nNov 1 2019 00:14:15 location1   10.100.14.101   98:b8:ba:01:as:66   sta_stats\nNov 1 2019 00:15:17 location2   10.100.14.101   98:b8:ba:01:sa:65   sta_stats\nNov 1 2019 00:27:17 location1   10.100.14.101   98:b8:ba:01:as:66   sta_stats\nNov 1 2019 00:37:54 location3   10.100.14.101   98:b8:ba:03:b8:66   sta_stats\nNov 1 2019 00:47:13 location2   10.100.14.101   98:b8:ba:01:b7:66   sta_stats\n\n\nI need to show the monthly stats for each branch. The problem is that we calculate this by unique mac addresses per day. \n\nThe end result would:\n\n\nGroup by branch \nGroup by month \nFor each branch, count unique values in "mac" column grouped by day [ maybe this has to occur at 2 ] \nShow totals by branch\n\n\nEx.\nNovember stats:\nLocation1 2222\nLocation2 30303\nLocation3 ...\nand so on.\n\nI can group by branch but need to then get unique mac values by day then count by month for each branch.\n\nwifi.groupby([\'branch\']).size()\n\n\nThis is more precise [ by month ] but does not look at unique mac addresses by day:\n\n wifi.index = pd.to_datetime(feet.index)\n wifi.groupby([\'branch\', feet.index.month]).count()\n\n'
'Select a number randomly with probability proportional to its magnitude from the given array of n elements\nconsider an experiment, selecting an element from the list A randomly with probability proportional to its magnitude.\nassume we are doing the same experiment for 100 times with replacement, in each experiment you will print a number that is selected randomly from A.\n\nEx 1: A = [0 5 27 6 13 28 100 45 10 79]\nlet f(x) denote the number of times x getting selected in 100 experiments.\nf(100) > f(79) > f(45) > f(28) > f(27) > f(13) > f(10) > f(6) > f(5) > f(0)\n\ndef pick_a_number_from_list(A):\n    sum=0\n    cum_sum=[]\n    for i in range(len(A)):\n        sum = sum + A[i]\n        cum_sum.append(sum)\n    #print(cum_sum)           \n    r = random.uniform(0,sum)\n    number=0\n    for index,i in enumerate(cum_sum):\n        if(r&gt;=cum_sum[index] and r&lt;cum_sum[index+1]):\n            return A[index]\n    return number\n\ndef sampling_based_on_magnitued():\n    A = [0,5,27,6,13,28,100,45,10,79]\n    for i in range(1,100):\n        number = pick_a_number_from_list(A)\n        print(number)\n\nsampling_based_on_magnitued()\n\n\nI have written the above code but not getting the correct output.\n'
'I am trying to plot a histogram for missing NaN values across all features of a dataframe\nFor that i created a dataframe for missing NaN values\n\nMissing value Dataframe\n\n   0\n-----\n0  0\n1  14\n2  800\n.\n.\n84 2344\n\n\nThen i have this master dataframe that has multiple columns i am not concerned about since i want only the row names from this dataframe\n\nMaster Dataframe\n\n     0  1\n---------\nF1   3  3\nF2   4  3\n.\n.\nF85  5  2\n\n\nHow can i merge/ concatenate these 2 dataframes where the final output should be like (Columns in master dataframe are irrelevant since i want to plot number of missing values across all features i.e. F1,F2,...F85)\n\n    F1   0  \n    F2   14 \n    F3   800\n    .\n    .\n    F85  2344\n\n'
'I want to create many index groups. My input file file.ndx\n\n[ System ]\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15\n  16   17   18   19   20   21   22   23   24   25   26   27   28   29   30\n  31   32   33   34   35   36   37   38   39   40   41   42   43   44   45\n  46   47   48   49   50   51   52   53   54   55   56   57   58   59   60\n  61   62   63   64   65   66   67   68   69   70   71   72   73   74   75\n  76   77   78   79   80   81   82   83   84   85   86   87   88   89   90\n  91   92   93   94   95   96   97   98   99  100  101  102  103  104  105\n 106  107  108  109  110  111  112  113  114  115  116  117  118  119  120\n 121  122  123  124  125  126  127  128  129  130  131  132  133  134  135\n 136  137  138  139  140  141  142  143  144  145  146  147  148  149  150\n 151  152  153  154  155  156  157  158  159  160  161  162  163  164  165\n 166  167  168  169  170  171  172  173  174  175  176  177  178  179  180\n 181  182  183  184  185  186  187  188  189  190  191  192  193  194  195\n 196  197  198  199  200  201  202  203  204  205  206  207  208  209  210\n 211  212  213  214  215  216  217  218  219  220  221  222  223  224  225\n 226  227  228  229  230  231  232  233  234  235  236  237  238  239  240\n......\n\n\n\n\n\n...... 116100\n\n\nI must create every index group of atoms, for example 1 molecule = 27 atoms, so my output will be:\n\n[ MGDG1 ]\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15\n  16   17   18   19   20   21   22   23   24   25   26   27\n[ MGDG2 ]\n  28   29   30   31   32   33   34   35   36   37   38   39   40   41   42   \n  43   44   45   46   47   48   49   50   51   52   53   54\n[ MGDG3 ]\n  55   56   57   58   59   60   61   62   63   64   65   66   67   68   69   \n  70   71   72   73   74   75   76   77   78   79   80   81  \n\netc.\n\n\nWhat is the easiest way to do this? I write bash scripts (also sed and awk) and I am beginner in python?\n'
'This is my first time using SMOTENC to upsampling my categorical data. However, I\'ve been getting error. Can you please advice what should I pass for categorical_features in SMOTENC? \n\nfrom imblearn.over_sampling import SMOTENC\n\nx=df.drop("A",1)\ny=df["A"]\n\nsmote_nc = SMOTENC(categorical_features=[\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'G\',\'H\'], random_state=0)\nX_resampled, y_resampled = smote_nc.fit_resample(x, y)\n\n\nERROR: \n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-26-f6c9d8a17967&gt; in &lt;module&gt;\n\n---&gt; 14 X_resampled, y_resampled = smote_nc.fit_resample(x, y)\n     15 #sm = SMOTE(random_state=100)\n     16 #ros = RandomOverSampler()\n\n~/.local/lib/python3.5/site-packages/imblearn/base.py in fit_resample(self, X, y)\n     82             self.sampling_strategy, y, self._sampling_type)\n     83 \n---&gt; 84         output = self._fit_resample(X, y)\n     85 \n     86         if binarize_y:\n\n~/.local/lib/python3.5/site-packages/imblearn/over_sampling/_smote.py in _fit_resample(self, X, y)\n    986     def _fit_resample(self, X, y):\n    987         self.n_features_ = X.shape[1]\n--&gt; 988         self._validate_estimator()\n    989 \n    990         # compute the median of the standard deviation of the minority class\n\n~/.local/lib/python3.5/site-packages/imblearn/over_sampling/_smote.py in _validate_estimator(self)\n    979                 raise ValueError(\n    980                     \'Some of the categorical indices are out of range. Indices\'\n--&gt; 981                     \' should be between 0 and {}\'.format(self.n_features_))\n    982             self.categorical_features_ = categorical_features\n    983         self.continuous_features_ = np.setdiff1d(np.arange(self.n_features_),\n\nValueError: Some of the categorical indices are out of range. Indices should be between 0 and 7\n\n'
"I have created a KNN model in Python (Module = Scikitlearn) by using three variables (Age, Distance, Travel Allowance) as my predictor variables, with the aim of using them to predict an outcome for the target variable (Method of Travel). \n\nWhen constructing the model, I had to normalize the data for the three predictor variables (Age, Distance, Travel Allowance). This increased the accuracy of my model compared to not normalizing the data. \n\nNow that I have constructed the model, I want to make a prediction. But how would I enter the predictor variables to make the prediction as the model has been trained on normalized data. \n\nI want to enter KNN.predict([[30,2000,40]]) to carry out a prediction where Age = 30; Distance = 2000; Allowance = 40. But as the data has been normalized, I can't think of a way on how to do this. I used the following code to normalize the data:\nX = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n"
"I have a pandas data frame which I made using an text file in Python. I was able to read the data and made the dataframe but after some processing, I am having many redundant values in my dataframe and I want to remove the repeated values.\nI tried using \n\ndf2 = df1.drop_duplicates(subset=['FROM', 'ATTENDANCE'], keep = 'last', inplace=False)\ndf2\n\n\n\n\nbut still, the repeated data is there and is not removed. I tried everything with drop_duplicates() and nothing of them worked for me.\n"
'I have a pandas Series and I have performed cumsum() on it. The resulting Series looks like:\n\nA = [10, 25, 30, 20, 27, 29]\n\n\nI want to find the range in the series in which a certain value lies.\n\nFor example, the value 28 lies between (25, 30), (30, 20) and (27,29). In this case I want to find either the last or all such ranges. How can I achieve this natively in pandas and without extra loops?\n'
'\n  I have 2 dataframe i.e df1 and df2 as follows \n\n\ndf1=pd.read_csv("abc.csv")\nprint (df1.head(10))\ndf2=pd.read_csv("xyz.csv")\nprint (df2.head(10))\n\n                     A              B\n          0  2019-01-01 03:56:29  197.199997\n          1  2019-01-01 04:02:29  197.186142\n          2  2019-01-02 06:24:29  196.857986\n          3  2019-01-02 06:42:29  196.816376\n          4  2019-01-03 11:52:29  196.100006\n          5  2019-01-03 12:00:30  196.015961\n          6  2019-01-04 14:18:30  194.566376\n          7  2019-01-04 14:38:30  194.356293\n          8  2019-01-04 19:48:30  191.100006\n          9  2019-01-05 19:56:30  191.081512\n\n                 C                  D\n          0  2019-01-1  18:00:00  1333\n          1  2019-01-2  19:00:00  1.18\n          2  2019-01-3  20:00:00  1666667\n          3  2019-01-4  21:00:00  0\n          4  2019-01-5  22:00:00  1\n          5  2019-01-6  23:00:00  1.5\n          6  2019-01-7  00:00:00  109\n          7  2019-01-8  01:00:00  200\n          8  2019-01-9  02:00:00  192\n          9  2019-01-10 03:00:00  1.700000\n\n\n\n  df2 has hourly wise average data ,Now how to select values for only date in df1 where df2 column "D" has value more than 2 i.e output will look like ,\n\n\n                     A           B\n      0  2019-01-01 03:56:29  197.199997\n      1  2019-01-01 04:02:29  197.186142\n      2  2019-01-03 11:52:29  196.100006\n      4  2019-01-03 12:00:30  196.015961\n\n\n\n  i have tried like \n\n\n,`final_data=pd.concat([df1.reset_index(drop=True),df2.reset_index(drop=True)],axis=1)\n  final_data=final_data[final_data["D"] &gt; 2]\n\n\n\n  but i didnt get the proper output , can any one please help me with the solution \n\n'
'I want exploring linear relationships among my dataa with lmplot, but it returns an error.\nCan someone help me to understand what is wrong in my code ?\nI have cleaned my data eliminating null data and converting in float format the values that were in str format. After that I try to do the lmplot(). \nThis is the link  where I found the csv file; link\n\nCODE\n\n#Analysis of obesity by country\n\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport numpy as np \n\naddress = \'C:/Users/Andre/Desktop/Python/firstMN/obesity-cleaned.csv\'\ndt = pd.read_csv(address)\n\n#eliminate superfluos data\ndt.drop(dt[\'Obesity (%)\'][dt[\'Obesity (%)\'].values == \'No data\'].index, inplace=True)  \n\nfor i in range(len(dt)):\n   dt[\'Obesity (%)\'].values[i] = float(dt[\'Obesity (%)\'].values[i].split()[0]) \n\n#print(dt[\'Country\'], \'\\n\') \n#print(dt[\'Obesity (%)\'])\n\nsb.lmplot(\'Country\', \'Obesity (%)\', dt) \nplt.show(\n\n\nHEAD DATASET\n\n   Unnamed: 0      Country  Year Obesity (%)         Sex\n0           0  Afghanistan  1975         0.5  Both sexes\n1           1  Afghanistan  1975         0.2        Male\n2           2  Afghanistan  1975         0.8      Female\n3           3  Afghanistan  1976         0.5  Both sexes\n4           4  Afghanistan  1976         0.2        Male\n5           5  Afghanistan  1976         0.8      Female\n6           6  Afghanistan  1977         0.6  Both sexes\n7           7  Afghanistan  1977         0.2        Male\n8           8  Afghanistan  1977         0.9      Female\n9           9  Afghanistan  1978         0.6  Both sexes\n\n\nOUTPUT\n\nTraceback (most recent call last):\n  File "C:\\Users\\Andre\\Desktop\\Python\\firstMN\\obesity.py", line 23, in &lt;module&gt;\n    sb.lmplot(\'Country\', \'Obesity (%)\', dt)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\regression.py", line 616, in lmplot\n    facets.map_dataframe(regplot, x, y, **regplot_kws)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\axisgrid.py", line 828, in map_dataframe\n    self._facet_plot(func, ax, args, kwargs)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\axisgrid.py", line 846, in _facet_plot\n    func(*plot_args, **plot_kwargs)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\regression.py", line 817, in regplot\n    plotter.plot(ax, scatter_kws, line_kws)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\regression.py", line 369, in plot\n    self.lineplot(ax, line_kws)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\regression.py", line 412, in lineplot\n    grid, yhat, err_bands = self.fit_regression(ax)\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\seaborn\\regression.py", line 200, in fit_regression\n    grid = np.linspace(x_min, x_max, 100)\n  File "&lt;__array_function__ internals&gt;", line 5, in linspace\n  File "C:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python38-32\\lib\\site-packages\\numpy\\core\\function_base.py", line 129, in linspace\n    start = asanyarray(start) * 1.0\nnumpy.core._exceptions.UFuncTypeError: ufunc \'multiply\' did not contain a loop with signature matching types (dtype(\'&lt;U32\'), dtype(\'&lt;U32\')) -&gt; dtype(\'&lt;U32\')\n\n'
"I'm working on taking some data from a dataset and plotting certain aspects of it. Here's my code below:\nimport matplotlib.pyplot as plt\ndf1 = pd.read_csv('dataset_1.csv')\nsoil_moisture = list(df1.Soil_Moisture)\nsoil_temperature = list(df1.Soil_Temp)\nprint(len(soil_moisture))\nprint(len(soil_temperature))\nplt.plot([soil_moisture], [soil_temperature])\nplt.show() \n\nAs you can see, it takes data from each of those columns and tries to make a line graph. However, when I run, it just displays an empty graph. This is weird since when I print the soil_moisture and soil_temperature, it tells me that there's actual data, and none of my other plots in the same notebook are experiencing this. All help is appreciated!\nHere's an image of the jupyter output\n"
'I have been trying to plot the price vs. km/100L from my dataset using seaborn regplot. I have tried converting the data type of both the columns to int64,but it doesnt work.\nautomobile_df[&quot;price&quot;].astype(&quot;int64&quot;)\nautomobile_df[&quot;km/100L&quot;].astype(&quot;int64&quot;)\n\nThen I tried plotting price vs. km/100L using regplot from the seaborn library.\nsns.regplot(x=&quot;km/100L&quot;,y=&quot;price&quot;,data=&quot;automobile_df&quot;)\n\nThe complete error message I get is,\n    TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-53-fdf8be478666&gt; in &lt;module&gt;()\n----&gt; 1 sns.regplot(x=&quot;km/100L&quot;,y=&quot;price&quot;,data=&quot;temp_df&quot;)\n\n\n/usr/local/lib/python3.6/dist-packages/seaborn/regression.py in regplot(x, y, data, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, seed, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, dropna, x_jitter, y_jitter, label, color, marker, scatter_kws, line_kws, ax)\n    807                                  order, logistic, lowess, robust, logx,\n    808                                  x_partial, y_partial, truncate, dropna,\n--&gt; 809                                  x_jitter, y_jitter, color, label)\n    810 \n    811     if ax is None:\n\n/usr/local/lib/python3.6/dist-packages/seaborn/regression.py in __init__(self, x, y, data, x_estimator, x_bins, x_ci, scatter, fit_reg, ci, n_boot, units, seed, order, logistic, lowess, robust, logx, x_partial, y_partial, truncate, dropna, x_jitter, y_jitter, color, label)\n    107         # Extract the data vals from the arguments or passed dataframe\n    108         self.establish_variables(data, x=x, y=y, units=units,\n--&gt; 109                                  x_partial=x_partial, y_partial=y_partial)\n    110 \n    111         # Drop null observations\n\n/usr/local/lib/python3.6/dist-packages/seaborn/regression.py in establish_variables(self, data, **kws)\n     43         for var, val in kws.items():\n     44             if isinstance(val, str):\n---&gt; 45                 vector = data[val]\n     46             elif isinstance(val, list):\n     47                 vector = np.asarray(val)\n\nTypeError: string indices must be integers\n\n'
"I have a dataset with one column that I want to change to date-time format. If I use this:\ndf = pd.to_datetime(df['product_first_sold_date'],unit='d',origin='1900-01-01')\n\ndf will only have this one particular column while all others are removed. Instead, I want to keep the remaining columns unchanged and just apply the to_datetime function to one column.\nI tried using loc with multiple ways, including this:\ndf.loc[df['product_first_sold_date']] = pd.to_datetime(df['product_first_sold_date'],unit='d',origin='1900-01-01')\n\nbut it throws a key error.\nHow else can I achieve this?\n"
"I have two data frames;\ndf1= {'col1': [9,2,2], 'col2': [5,1,0], 'col3': [9,3,2], 'col4': [8,3,2],  'col5': [6,0,0]}\ndf1 = pd.DataFrame(data=df1)\n\n   col1 col2 col3 col4 col5\n0   9    5    9     8   6\n1   2    1    3     3   0\n2   2    0    2     2   0\n\n\ndf2 = [[9, 8], [3, 3], [2, 2]] \ndf2 = pd.DataFrame(data=df2)\n    0   1\n0   9   8\n1   3   3\n2   2   2\n\nI want to match the columns values, in this case, 9,3,2 and 8,3,2, and output the column names, in this case, 'col3' and 'col4'\nI have tried:\ndf2[0].value_counts() == df1['col3'].value_counts()\nand\ndf1.values==df2.values but not what I am after.\n"
'i am trying to predict the next values in a time series using the ARIMA model.\nHere is my code:(sorry for the typos)\nsplit_val = floor(len(data_file)*0.8)\ntrain = data_file[[&quot;Daily Confirmed&quot;]][:split_val]\ntesst = data_file[[&quot;Daily Confirmed&quot;]][split_val:]\n\nprint(train.head())\nprint(tesst.head())\n\np = d = q = range(1, 5)\n\npdq = list(itertools.product(p, d, q))\n# print(pdq)\nbestvalues = {}\nfor i in pdq:\n    try:\n        p, d, q = i\n        moodel = ARIMA(train, order=(p, d, q))\n        trained_model = moodel.fit()\n        bestvalues[trained_model.aic] = i\n        print(trained_model.aic, &quot; &quot;, i)\n    except:\n        continue\n\nprint(bestvalues)\nminaic = min(bestvalues.keys())\n\n\nmoodel = ARIMA(train, order=bestvalues[minaic])\ntrained_model = moodel.fit()\n\npridiction = trained_model.forecast(steps=len(tesst))[0]\n\ncomparisionn = tesst.copy()\n\ncomparisionn[&quot;forcastted&quot;] = pridiction.tolist()\ncomparisionn.plot()\n\nprint(comparisionn)\nprint(trained_model.aic)\nplt.show()\n\n(the data is pre-processed)\nThe minimum aic i can get is2145.930883796257 and here are the predictions against the test data (only first 5):\n            Daily Confirmed    forcastted\nDate                                     \n2020-06-22            13560  15048.987970\n2020-06-23            15656  15349.247935\n2020-06-24            16868  15905.260648\n2020-06-25            18205  16137.086959\n2020-06-26            18255  16237.232886\n\n\nand here is the plot\n\nas you can see, the prediction is not accurate and i have brute forced all the values for p, d and q upto 4....\nWhat might be the problem?\nThanks.\n'
"Noticed something very strange in pandas. My dataframe(with 3 rows and 3 columns) looks like this:\n\nWhen I try to extract ID and Name(separated by underscore) to their own columns using command below, it gives me an error:\ndf[['ID','Name']] = df.apply(lambda x: get_first_last(x['ID_Name']), axis=1, result_type='broadcast')\n\nError is:\nValueError: cannot broadcast result\n\nHere's the interesting part though..When I delete the &quot;From_To&quot; column from the original dataframe, performing the same df.apply() to split ID_Name works perfectly fine and I get the new columns like this:\n\nI have checked a lot of SO answers but none seem to help. What did I miss here?\nP.S. get_first_last is a very simple function like this:\ndef get_first_last(s):\n    str_lis = s.split(&quot;_&quot;)\n    return [str_lis[0], str_lis[1]]\n\n"
"I want to download a dataset from the UCI repository.\nThe dataset is in the tar.Z format, and ideally I'd like to read it in as a pandas data frame.\nI've checked out uncompressing tar.Z file with python? which suggested the zgip library, so from https://docs.python.org/3/library/gzip.html I tried using the below code but I got an error message.\nThanks for any help!\nimport gzip\nwith gzip.open('https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes/diabetes-data.tar.Z', 'rb') as f:\nfile_content = f.read()  \n\nERROR MESSAGE:\nOSError: [Errno 22] Invalid argument: 'https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes/diabetes-data.tar.Z'\n\n"
"I have this df called positions:\n         Date Direction  Ticker  Price  ...  FX-rate  Comission  Short  Cost-price\n0  2020-02-11       Buy  YAR.OL  386.1  ...      1.0        0.0  False       386.1\n1  2020-06-16      Sell  YAR.OL  356.0  ...      1.0        0.0  False      -356.0\n2  2020-02-05       Buy  NHY.OL   30.0  ...      1.0        0.0  False        30.0\n\nI have a list:\nn_ticker_list = ['YAR.OL', 'NHY.OL']\n\nI'm trying to create a new DF sorting on the Ticker. If the ticker is the same then those values would go into a df, and the other would go to another.\nI have just experimented to see how this can be done, and I'm not sure if I'm on the correct path here..\n    for my_ticker in set(n_ticker_list):\n        new_df = positions[positions['Ticker'] == my_ticker]\n\n    print(new_df)\n\nHow could this be done? In the new DF I want to bring with me all the columns.\nThanks :)\n"
"I'm learning object oriented programing in a data science context.\nI want to understand what good practice is in terms of writing methods within a class that relate to one another.\nWhen I run my code:\nimport pandas as pd \npd.options.mode.chained_assignment = None  \n\nclass MyData:\n    def __init__(self, file_path):\n        self.file_path = file_path\n\ndef prepper_fun(self):\n    '''Reads in an excel sheet, gets rid of missing values and sets datatype to numerical'''\n    df = pd.read_excel(self.file_path) \n    df = df.dropna()                    \n    df = df.apply(pd.to_numeric) \n    self.df = df\n    return(df)\n\ndef quality_fun(self):\n   '''Checks if any value in any column is more than 10. If it is, the value is replaced with\n   a warning 'check the original data value'.'''\n    for col in self.df.columns:\n        for row in self.df.index:                                             \n            if self.df[col][row] &gt; 10:       \n                self.df[col][row] = str('check original data value') \n    return(self.df) \n\ndata = MyData('https://archive.ics.uci.edu/ml/machine-learning-databases/00429/Cryotherapy.xlsx')\nprint(data.prepper_fun())\nprint(data.quality_fun())\n\nI get the following output (only part of the output is shown due to space constrains):\n     sex  age   Time  \n0     1   35  12.00             \n1     1   29   7.00               \n2     1   50   8.00                \n3     1   32  11.75                \n4     1   67   9.25                         \n..  ...  ...    ...       \n\n\n     sex                        age                       Time \n0     1  check original data value  check original data value                  \n1     1  check original data value                          7                  \n2     1  check original data value                          8                  \n3     1  check original data value  check original data value               \n4     1  check original data value                       9.25 \n..  ...                        ...                        ...\n\nI am happy with the output generated by each method.\nBut if I try to call print(data.quality_fun()) without first calling print(data.prepper_fun()), I get an error AttributeError: 'MyData' object has no attribute 'df'.\nBeing new to objected oriented programming, I am wondering if it is considered good practice to structure things like this, or if there is some other way of doing it.\nThanks for any help!\n"
'I would like to convert below nested arrays into DataFrame. Specifically, I want to convert each arrays in the nested arrays as data frame rows. There are altogether 24 nested arrays but I just copied few of those to not make the post any longer\nFor example from the below array, I want to convert a single array into one row in DataFrame. Any help would be appreciated. Thank you\n[array([[[-1.36013642e-01,  1.59766637e-02,  4.49674837e-02,\n           2.87157446e-02, -9.56720412e-02, -8.88903514e-02,\n          -5.33084199e-02, -2.13210974e-02,  1.41174316e-01,\n          -4.11187410e-02,  1.70307457e-01, -2.14784797e-02,\n          -1.35124326e-01, -1.13939375e-01, -3.31919976e-02,\n           6.11963794e-02, -1.17217958e-01, -1.50817335e-01,\n          -4.65203971e-02, -1.00571744e-01,  7.85203278e-02,\n           2.94833463e-02,  1.02086905e-02,  1.93061940e-02,\n          -2.32153490e-01, -3.00551683e-01, -1.17443979e-01,\n          -1.46142572e-01,  1.07608363e-01, -9.40902233e-02,\n          -3.90545353e-02,  4.42864373e-02, -1.29430681e-01,\n          -7.02960119e-02,  8.21319446e-02,  1.80883124e-01,\n          -2.02056542e-02, -2.47046854e-02,  1.82081938e-01,\n           3.43354326e-03, -1.19226977e-01, -5.64751402e-03,\n           5.98295629e-02,  3.01894635e-01,  1.70621589e-01,\n           1.36449412e-01,  4.88062464e-02, -4.68050130e-02,\n           1.07072420e-01, -2.24809378e-01,  1.33529887e-01,\n           1.69836447e-01,  1.21353425e-01,  3.47930416e-02,\n           2.27185443e-01, -2.41639778e-01,  3.87575626e-02,\n           1.36878729e-01, -1.31427586e-01,  1.35682166e-01,\n          -8.80223699e-04,  1.59485396e-02,  2.62592174e-02,\n          -3.75868753e-02,  1.91311508e-01,  6.34296760e-02,\n          -6.77121654e-02, -9.76699442e-02,  1.88637391e-01,\n          -1.95635721e-01,  2.61483667e-03,  1.27198607e-01,\n          -7.48684406e-02, -1.59019291e-01, -2.47020170e-01,\n          -3.68376747e-02,  4.33871210e-01,  1.57070860e-01,\n          -1.62113741e-01,  3.82730290e-02, -3.92096601e-02,\n          -4.57221121e-02,  9.67647210e-02, -6.17119949e-05,\n          -1.43869311e-01, -6.41668867e-03, -1.91071168e-01,\n          -2.21797563e-02,  2.02202812e-01,  3.91443223e-02,\n           4.61347327e-02,  1.54131114e-01,  7.47928210e-03,\n           2.09267866e-02,  3.87379229e-02, -5.46494387e-02,\n          -1.59527451e-01,  7.56314816e-03, -2.13688631e-02,\n          -5.44707663e-02,  6.98875338e-02, -1.67583689e-01,\n          -1.69844925e-03,  9.92566273e-02, -1.98496655e-01,\n           6.26156628e-02, -2.95266993e-02,  5.99580631e-03,\n          -6.91434443e-02,  3.17127258e-02, -1.32520899e-01,\n           2.55816411e-02,  1.90661669e-01, -2.88114160e-01,\n           1.65751979e-01,  1.89788476e-01, -3.19299102e-02,\n           1.52912185e-01,  8.25165361e-02,  6.92607239e-02,\n          -2.91225202e-02, -3.67325470e-02, -1.58863515e-01,\n          -4.12161574e-02, -9.37818438e-02, -1.08723141e-01,\n           1.27439285e-02,  7.31237382e-02]],\n \n        [[-1.37894586e-01,  5.84010258e-02,  9.80348736e-02,\n          -9.55949910e-03, -6.22733906e-02, -5.63683398e-02,\n          -2.26371922e-02, -8.96835402e-02,  1.46475613e-01,\n          -4.40802239e-02,  1.53038278e-01, -3.29063646e-03,\n          -1.69211119e-01, -1.14617534e-01,  5.59688732e-03,\n           5.93355000e-02, -1.27704740e-01, -1.71358287e-01,\n          -4.84642014e-02, -8.19769055e-02,  2.84060407e-02,\n           6.13150261e-02, -7.79468417e-02, -8.77461210e-03,\n          -2.00844258e-01, -2.88251281e-01, -7.33745247e-02,\n          -1.42669410e-01,  1.07440352e-01, -1.39590308e-01,\n          -3.51556130e-02, -4.18033963e-03, -1.82463229e-01,\n          -4.28252965e-02,  5.28394952e-02,  1.75120696e-01,\n          -9.33501031e-03, -3.33592743e-02,  1.65851176e-01,\n           1.39791211e-02, -1.52489260e-01, -6.68389164e-03,\n           5.71185611e-02,  2.38399506e-01,  1.53594971e-01,\n           7.59008080e-02,  1.48483068e-02, -5.43878600e-02,\n           1.36847854e-01, -1.76150039e-01,  1.25447407e-01,\n           1.70887619e-01,  1.22895576e-01,  5.84644414e-02,\n           1.99920326e-01, -2.79710770e-01,  1.56115033e-02,\n           5.12041822e-02, -1.93132162e-01,  1.51428387e-01,\n           5.97179160e-02, -1.93341300e-02, -4.03094850e-03,\n           1.73912272e-02,  1.02863029e-01, -4.60668234e-03,\n          -6.12722374e-02, -1.03243664e-01,  1.96056634e-01,\n          -1.93443596e-01,  2.90273000e-02,  9.11512822e-02,\n          -3.45249921e-02, -1.79792568e-01, -2.18977749e-01,\n           2.92301551e-02,  3.94104689e-01,  1.66036054e-01,\n          -2.10731253e-01,  4.83864099e-02, -4.31987755e-02,\n          -9.82125998e-02,  1.06999256e-01,  1.90159921e-02,\n          -5.58431894e-02,  3.04446071e-02, -1.47668719e-01,\n           8.35768692e-03,  1.71920583e-01,  7.69633651e-02,\n           8.75835866e-02,  1.86722219e-01, -2.75155809e-02,\n          -1.05050392e-02,  2.00979151e-02, -7.50234211e-03,\n          -1.67125508e-01,  2.20861211e-02, -4.32664342e-03,\n          -5.50425760e-02,  8.63447115e-02, -1.67248473e-01,\n           1.38378460e-02,  1.11343339e-01, -1.99985191e-01,\n           1.35348290e-01,  1.56460218e-02,  2.73892656e-04,\n          -6.41771033e-02,  2.57192627e-02, -9.93981138e-02,\n           8.97653773e-03,  1.91183090e-01, -2.82200336e-01,\n           2.23059848e-01,  1.75120562e-01, -7.82503486e-02,\n           1.47400096e-01,  4.67997603e-02,  7.76805580e-02,\n           1.79661531e-02, -5.38391508e-02, -1.54771596e-01,\n          -9.99689624e-02, -1.09263457e-01, -9.25640911e-02,\n           1.83300804e-02,  8.41395408e-02]],\n \n        [[-1.50231764e-01,  5.30236512e-02,  5.80971278e-02,\n          -6.91052619e-03, -6.63885847e-02, -8.69466662e-02,\n          -1.51826143e-02, -5.16197905e-02,  2.19310135e-01,\n          -8.07677209e-02,  1.71911135e-01, -3.65424380e-02,\n          -1.77088380e-01, -9.78112444e-02, -1.38332583e-02,\n           8.34669024e-02, -1.31156802e-01, -1.49645716e-01,\n          -3.05147469e-02, -1.17407627e-01,  1.71565767e-02,\n           9.20448918e-03, -6.46030009e-02, -2.28046551e-02,\n          -1.94293335e-01, -2.80386955e-01, -7.64302760e-02,\n          -8.90514404e-02,  1.17872708e-01, -9.60179269e-02,\n          -3.07646766e-02,  3.37364674e-02, -1.90276399e-01,\n          -1.05832189e-01,  6.49234205e-02,  2.19782099e-01,\n          -1.96797457e-02,  1.40418569e-02,  1.81732520e-01,\n          -3.28570083e-02, -7.66380355e-02,  6.89617405e-03,\n           4.78702188e-02,  2.45228872e-01,  1.41428009e-01,\n           1.00809440e-01,  3.73133272e-03, -1.03467189e-01,\n           8.70409757e-02, -2.12297142e-01,  9.47272182e-02,\n           1.90620258e-01,  1.18681826e-01,  3.31691504e-02,\n           1.67793304e-01, -2.18925700e-01, -9.66489222e-03,\n           1.24859490e-01, -1.71614006e-01,  1.37775704e-01,\n           8.33617598e-02,  1.09298751e-02, -4.55547757e-02,\n          -2.22153030e-02,  1.64980054e-01,  1.37728015e-02,\n          -9.32593867e-02, -7.81709999e-02,  2.22985357e-01,\n          -1.69061244e-01,  4.10387591e-02,  4.30300683e-02,\n          -1.11437151e-02, -1.98694319e-01, -2.07214892e-01,\n           5.36098704e-02,  3.90068620e-01,  1.62774995e-01,\n          -1.60267770e-01,  7.80781955e-02, -5.54403551e-02,\n          -9.31668729e-02,  7.10664839e-02, -1.76445842e-02,\n          -9.73844975e-02, -3.38197052e-02, -1.21390782e-01,\n           4.05607447e-02,  1.57834575e-01,  6.25553131e-02,\n           1.10172994e-01,  1.51440471e-01, -1.40509475e-02,\n           3.16431560e-02, -2.47017704e-02, -3.98907140e-02,\n          -1.65499881e-01,  3.11357491e-02, -5.87871447e-02,\n          -4.44657058e-02,  1.02798469e-01, -1.55324146e-01,\n           4.02739719e-02,  1.22631051e-01, -1.83685839e-01,\n           1.04981773e-01, -6.72103511e-03, -2.01395378e-02,\n          -1.53558785e-02,  5.07285669e-02, -9.66893956e-02,\n          -1.21531086e-02,  2.01398820e-01, -2.65820712e-01,\n           2.13555843e-01,  1.77817732e-01, -6.56121820e-02,\n           1.58264607e-01,  8.97071287e-02,  9.25853327e-02,\n           6.48394227e-02, -1.72733609e-02, -1.74576581e-01,\n          -6.79302812e-02, -1.20070800e-01, -1.09205380e-01,\n           2.45773457e-02,  9.11332145e-02]],\n \n        [[-1.62274405e-01,  3.15038860e-02,  6.44017309e-02,\n           3.45326737e-02, -1.05685055e-01, -1.77183170e-02,\n          -8.56796950e-02, -7.91971534e-02,  1.71855256e-01,\n          -1.71298254e-02,  1.86888188e-01,  3.59892212e-02,\n          -1.92580983e-01, -6.18249550e-02,  1.21358195e-02,\n           6.60665333e-02, -1.40805155e-01, -8.28307271e-02,\n          -5.42851612e-02, -1.11442827e-01, -6.72461838e-03,\n           1.96444243e-02,  2.98103131e-02,  6.19128272e-02,\n          -1.93395317e-01, -2.87468702e-01, -8.87314901e-02,\n          -1.30469680e-01,  6.98416680e-02, -1.32677779e-01,\n           5.21699619e-03,  2.61115953e-02, -1.66991830e-01,\n          -5.70339113e-02,  6.47078753e-02,  1.88027576e-01,\n          -9.21493247e-02, -1.49603449e-02,  2.01233804e-01,\n          -8.49104673e-02, -1.06238969e-01, -3.60001624e-02,\n          -2.23363750e-03,  2.17491835e-01,  1.87389106e-01,\n           1.04524858e-01,  5.11063896e-02, -6.64142743e-02,\n           1.51605263e-01, -2.12667212e-01,  1.45079419e-01,\n           1.72637925e-01,  6.61674738e-02,  3.34243253e-02,\n           1.31693542e-01, -2.69493669e-01, -4.82268780e-02,\n           2.30443150e-01, -8.99494663e-02,  1.42259493e-01,\n           3.80099751e-03, -7.53545314e-02, -7.12354667e-03,\n          -2.95820814e-02,  1.53193980e-01,  5.97309470e-02,\n          -1.01371743e-01, -9.97453034e-02,  1.79987788e-01,\n          -1.37784496e-01,  4.48415950e-02,  1.53983220e-01,\n          -1.05956972e-01, -2.62854129e-01, -2.18802154e-01,\n           4.18419279e-02,  4.67299223e-01,  1.23464331e-01,\n          -1.62302554e-01,  3.13426740e-03, -6.86671678e-03,\n          -6.07644059e-02,  4.79412451e-02,  8.40777308e-02,\n          -6.54070973e-02, -4.18782346e-02, -1.13178916e-01,\n           5.20941988e-02,  1.85022369e-01,  5.36964908e-02,\n           3.61735113e-02,  1.85169518e-01, -1.91164780e-02,\n           6.12748265e-02,  5.99459792e-03, -2.01658588e-02,\n          -1.01727784e-01,  4.70405584e-03, -5.25830835e-02,\n          -4.94561940e-02,  4.53589484e-02, -7.16509372e-02,\n           2.04778500e-02,  5.09201735e-02, -1.25631809e-01,\n           1.64057955e-01,  9.62968543e-03,  4.30290811e-02,\n          -5.18420413e-02, -2.88595539e-02, -9.53822657e-02,\n           3.55528444e-02,  2.59569615e-01, -2.95394242e-01,\n           2.28176743e-01,  1.54641658e-01, -7.78511958e-03,\n           2.09660888e-01,  2.91666016e-02,  9.74472538e-02,\n           3.11672874e-02, -3.97840291e-02, -1.92549929e-01,\n           5.36589068e-04, -2.17817314e-02, -1.05434239e-01,\n           5.98730482e-02,  4.11710367e-02]]]),\n array([[[-1.68740377e-02,  1.52078941e-01,  8.62133503e-02,\n           3.47440667e-03, -7.50775635e-02,  8.29034671e-02,\n          -5.12160733e-02, -9.60651636e-02,  5.18606864e-02,\n           4.61538620e-02,  2.11039424e-01, -7.04053193e-02,\n          -2.30026633e-01, -2.06287913e-02,  4.42361459e-02,\n           1.13250390e-01, -1.38955295e-01, -7.81948566e-02,\n          -1.18520640e-01,  3.08102742e-03, -5.88674992e-02,\n           3.52515951e-02,  1.02329694e-01, -3.15367128e-03,\n          -1.09225258e-01, -2.74011433e-01, -7.80899003e-02,\n          -9.12662745e-02,  1.24638416e-01, -8.57445747e-02,\n          -3.88496295e-02, -1.19581092e-02, -1.75278068e-01,\n           7.70506728e-03,  7.80697837e-02,  9.88397971e-02,\n          -7.57867396e-02, -8.06069449e-02,  2.28473604e-01,\n          -2.36921087e-02, -1.76744461e-01,  3.78885008e-02,\n           8.70054811e-02,  1.42934516e-01,  2.38303959e-01,\n           6.11009710e-02,  4.36121263e-02, -3.50601189e-02,\n           6.64255023e-02, -2.49209940e-01,  1.57682329e-01,\n           1.21789858e-01,  1.16867483e-01,  8.45996961e-02,\n           5.76233957e-03, -1.76971704e-01, -4.91473451e-02,\n           1.56083614e-01, -1.41750157e-01,  7.08352476e-02,\n           9.80508104e-02, -8.59858245e-02, -8.23098868e-02,\n          -1.59299485e-02,  1.51134863e-01,  1.43824801e-01,\n          -1.24966606e-01, -1.75318450e-01,  1.13390550e-01,\n          -1.68713510e-01, -3.53813954e-02,  1.21039249e-01,\n          -8.95563290e-02, -1.01484008e-01, -2.75068283e-01,\n           5.46966009e-02,  4.39682692e-01,  3.67938466e-02,\n          -1.56368658e-01, -1.16297044e-02, -1.52784062e-03,\n          -2.47500669e-02, -1.95040870e-02,  1.20051503e-01,\n          -6.90754429e-02, -8.28582197e-02, -4.97369617e-02,\n           2.63527036e-05,  1.96313396e-01, -6.30125403e-02,\n           4.69858525e-03,  1.56594262e-01, -1.63011625e-02,\n           5.98538369e-02, -2.65194569e-04,  6.69567063e-02,\n           2.51362212e-02, -5.55414632e-02, -2.93561518e-02,\n           2.84286365e-02, -4.83711883e-02, -9.81986448e-02,\n          -3.45682539e-02,  7.68979266e-02, -1.98029101e-01,\n           1.53635189e-01,  7.00153364e-03, -3.65675725e-02,\n           7.30676875e-02, -1.04091987e-02, -5.18539827e-03,\n          -2.50395797e-02,  2.01758057e-01, -3.52356672e-01,\n           1.77500680e-01,  1.84066370e-01, -4.49734628e-02,\n           8.69778767e-02,  3.74429524e-02,  1.81517109e-01,\n           7.69926533e-02,  5.22192307e-02, -1.70259267e-01,\n          -1.13618262e-01, -1.40291126e-02, -4.68849167e-02,\n          -3.22203897e-02,  5.76205775e-02]],\n \n        [[-2.49804370e-02,  1.70671448e-01,  3.95690724e-02,\n          -3.51276388e-03, -1.98742189e-02,  7.46497139e-02,\n          -2.03474741e-02, -1.11103363e-01,  1.02668263e-01,\n           2.93022245e-02,  2.07146153e-01,  3.29615586e-02,\n          -2.63025105e-01, -2.46658288e-02,  3.71336676e-02,\n           8.23173597e-02, -1.64193496e-01, -9.91019532e-02,\n          -1.28058136e-01, -2.77458709e-02, -3.55885737e-02,\n           8.64468515e-02,  6.67003468e-02, -4.39326577e-02,\n          -1.09597728e-01, -2.84543842e-01, -7.25509301e-02,\n          -8.68509933e-02,  1.06886305e-01, -1.30941182e-01,\n          -9.13744885e-03,  4.62066382e-04, -1.28790632e-01,\n           2.69941334e-02,  8.28079656e-02,  5.13078235e-02,\n          -5.22269905e-02, -4.96672578e-02,  2.61806935e-01,\n           6.24357723e-02, -2.08110198e-01,  5.77822179e-02,\n           8.79181474e-02,  2.49593109e-01,  2.41651326e-01,\n          -8.27671867e-03, -1.22608151e-02, -7.38239959e-02,\n           8.41363668e-02, -2.08070427e-01,  1.60519928e-01,\n           1.58879861e-01,  1.78740799e-01,  1.20273851e-01,\n          -4.65768203e-03, -2.10287139e-01, -4.37513739e-02,\n           1.08855076e-01, -1.95248336e-01,  5.24971373e-02,\n           1.16677418e-01, -1.18925139e-01, -8.58063847e-02,\n           1.45739652e-02,  1.05634734e-01,  1.13258652e-01,\n          -1.43801495e-01, -1.55666292e-01,  6.84999675e-02,\n          -1.38900787e-01, -2.64124889e-02,  9.55415145e-02,\n          -1.27598137e-01, -1.43201396e-01, -2.41060451e-01,\n           6.54740036e-02,  3.65032971e-01,  4.02122661e-02,\n          -1.57019958e-01, -4.69299685e-03, -5.40541038e-02,\n          -3.91884111e-02, -9.02013388e-03,  9.22192931e-02,\n          -1.14169739e-01, -6.25471398e-02, -7.63412844e-03,\n           5.29423654e-02,  1.69518501e-01, -2.44914368e-02,\n          -4.37655076e-02,  1.66987330e-01, -3.33075225e-02,\n           4.13864255e-02,  1.90281402e-02,  9.10777077e-02,\n           2.46177800e-03, -4.55477871e-02, -6.55916557e-02,\n          -3.10473237e-03, -4.02890295e-02, -1.70472056e-01,\n          -2.12542713e-04,  7.25972354e-02, -2.65269011e-01,\n           1.94183111e-01,  8.99361670e-02, -2.56836135e-02,\n           9.89286229e-02, -2.27259118e-02, -2.12507583e-02,\n          -1.31608464e-03,  2.12979823e-01, -3.51205587e-01,\n           2.74906904e-01,  2.04230517e-01,  1.13111669e-02,\n           1.17748439e-01,  4.00286205e-02,  1.06981196e-01,\n           2.57537849e-02,  6.92378357e-03, -1.54393032e-01,\n          -6.81875497e-02,  3.07280570e-05, -6.17604423e-03,\n           1.25430766e-02,  3.37549895e-02]],\n \n        [[ 3.33601981e-02,  2.18950808e-01,  5.25589399e-02,\n          -6.06435016e-02, -8.43613148e-02,  7.20153004e-02,\n          -6.08830974e-02, -8.34409371e-02,  9.67382044e-02,\n          -3.27746384e-04,  2.91555285e-01, -4.84346934e-02,\n          -2.59748727e-01, -6.66559637e-02,  3.30047011e-02,\n           1.45614624e-01, -2.04137906e-01, -1.10366166e-01,\n          -1.42498747e-01, -3.80640477e-03, -1.71378646e-02,\n           5.47722988e-02,  6.41267151e-02,  3.91335320e-03,\n          -8.57527778e-02, -2.98490942e-01, -5.11459038e-02,\n          -1.01354338e-01,  1.27257884e-01, -1.28999814e-01,\n          -4.14374061e-02,  2.56892070e-02, -1.13893583e-01,\n          -2.45535634e-02,  6.04319125e-02,  6.74173683e-02,\n          -9.40101370e-02, -6.67680278e-02,  1.88137412e-01,\n           1.32872257e-03, -1.82188556e-01,  4.39332426e-02,\n           4.49893773e-02,  2.36567542e-01,  2.72193104e-01,\n           1.17838997e-02,  3.05325892e-02, -7.49155506e-02,\n           3.77691165e-02, -2.66298920e-01,  1.48846105e-01,\n           1.64895594e-01,  1.55318990e-01,  1.38803110e-01,\n           1.64481476e-02, -1.58437371e-01,  1.97825730e-02,\n           1.52800277e-01, -1.72297224e-01,  8.14515352e-02,\n           1.22895420e-01, -1.38855070e-01, -4.93918136e-02,\n           1.82603858e-02,  1.62638202e-01,  1.25332266e-01,\n          -1.04849547e-01, -1.97150543e-01,  5.53290471e-02,\n          -1.32151425e-01, -9.59372669e-02,  1.17293313e-01,\n          -1.40096560e-01, -9.66465995e-02, -2.62960464e-01,\n           3.64195369e-02,  3.95120800e-01,  2.55586170e-02,\n          -1.97585553e-01, -2.96364930e-02, -1.63999517e-02,\n          -4.40649725e-02,  1.55332964e-02,  6.92601651e-02,\n          -1.09604046e-01, -1.56740874e-01, -1.17389277e-01,\n           3.95389833e-02,  1.88643590e-01, -1.13158882e-01,\n          -8.29163101e-03,  1.95588186e-01, -3.58534269e-02,\n          -8.23678449e-04,  4.84100729e-02,  1.09540187e-01,\n          -3.94673832e-03,  1.27359228e-02, -1.72595084e-02,\n          -2.12953091e-02,  6.26803562e-02, -1.98052153e-01,\n          -6.99584559e-02,  7.20234066e-02, -2.17735857e-01,\n           1.60721123e-01, -1.33986981e-03,  1.67240351e-02,\n           1.30067334e-01, -5.92296720e-02, -1.77724455e-02,\n          -4.66314182e-02,  2.72649646e-01, -2.63336450e-01,\n           2.49853805e-01,  2.26347536e-01,  2.04152986e-02,\n           6.42477721e-02,  3.84325832e-02,  1.51295438e-01,\n           6.71947282e-03, -1.82412043e-02, -1.52144179e-01,\n          -6.57561570e-02, -4.76292893e-03, -1.88203622e-02,\n          -3.31684798e-02,  2.50659473e-02]],\n \n        [[ 2.82088853e-03,  1.88540086e-01,  1.68683734e-02,\n          -6.15225211e-02, -6.66730106e-02,  9.25483927e-02,\n           3.60094849e-03, -5.96660934e-02,  1.67936683e-01,\n           5.15804440e-02,  2.56727725e-01, -3.05743832e-02,\n          -2.75632173e-01, -9.82266143e-02,  4.73604612e-02,\n           1.32066280e-01, -1.86928630e-01, -1.10467285e-01,\n          -1.12709492e-01,  7.98108894e-03, -4.31619100e-02,\n           6.35776296e-02,  6.79053143e-02,  1.44015867e-02,\n          -8.84011164e-02, -2.98023641e-01, -3.90912741e-02,\n          -1.25116169e-01,  9.76984650e-02, -9.94135663e-02,\n           3.06001399e-03, -7.61403609e-03, -1.88553363e-01,\n          -1.57480519e-02,  1.00236654e-01,  7.26338178e-02,\n          -9.02482495e-02, -5.35438582e-02,  2.11272717e-01,\n           1.75018534e-02, -2.12516204e-01,  1.55753875e-03,\n           4.49219495e-02,  2.19748080e-01,  1.73106432e-01,\n           4.12785672e-02,  5.25815338e-02, -6.54551387e-02,\n           6.16361424e-02, -2.10121647e-01,  1.38409883e-01,\n           1.92241967e-01,  1.80396318e-01,  5.53400852e-02,\n           2.47791149e-02, -2.10880712e-01, -5.11872470e-02,\n           1.33346096e-01, -1.77311331e-01,  7.78129473e-02,\n           1.01757452e-01, -4.89850864e-02, -7.23917037e-02,\n          -3.58456001e-02,  1.46720067e-01,  8.36935937e-02,\n          -1.50431842e-01, -1.78618789e-01,  1.38929337e-01,\n          -1.36039600e-01, -4.11997065e-02,  1.43187419e-01,\n          -1.19545117e-01, -1.70583084e-01, -2.55052835e-01,\n           1.76979043e-02,  4.10075754e-01,  1.73812360e-02,\n          -1.80650592e-01, -5.94558157e-02, -6.30004108e-02,\n          -2.35346705e-02, -6.34292066e-02,  1.02112904e-01,\n          -8.55509266e-02, -4.29085605e-02, -7.77176172e-02,\n           1.18833035e-02,  2.08374113e-01, -9.88899767e-02,\n          -4.22141049e-03,  1.64182067e-01, -3.29365134e-02,\n          -2.23313347e-02, -3.83415073e-02,  5.85192628e-02,\n           3.04613076e-02, -6.51986338e-03, -3.01447697e-02,\n           1.43439416e-02, -2.54644640e-02, -1.97629541e-01,\n          -6.60173446e-02,  5.97969070e-02, -2.76524663e-01,\n           1.50484443e-01,  6.61896765e-02, -5.83832599e-02,\n           2.87721884e-02,  9.68062668e-04, -3.71711180e-02,\n          -2.21348181e-02,  2.51920044e-01, -3.40805888e-01,\n           2.91597039e-01,  2.26792172e-01,  1.19546121e-02,\n           1.11737035e-01, -5.84540702e-03,  1.46952957e-01,\n           3.79731879e-02, -5.13361618e-02, -1.33946911e-01,\n          -1.59345180e-01, -5.29028401e-02, -2.98120938e-02,\n          -1.22097181e-02,  2.81007364e-02]]])]\n\n\n'
'I have two dataframes A and B, I want to plot dataframe B values \u200b\u200bin dataframe A graph, both shared the same indices, but in point y two &quot;_&quot; objects will be placed one up two points and the other 2 points down the value they share in Y, how to do?\nlook like this the image=\n\ndataframes:\nDATAFRAME A\n    Date\n    2015-08-31  112.760002\n    2015-09-01  107.720001\n    2015-09-02  112.339996\n    2015-09-03  110.370003\n    2015-09-04  109.269997\n    2015-09-08  112.309998\n    2015-09-09  110.150002\n    2015-09-10  112.570000\n    2015-09-11  114.209999\n    2015-09-14  115.309998\n\nDATAFRAME B\n\n                Close\n    2015-08-31  112.760002\n    2015-09-01  107.720001\n    2015-09-02  112.339996\n    2015-09-08  112.309998\n\n'
"I am having trouble with some data formatting. I have a csv of data that looks like the data at the bottom.\nI am pretty sure something to do this exists in pandas even though I haven’t had much luck with my google search terms. I am thinking of a transpose of sorts but I haven’t had luck with that. I think for the data to be easier to access I would probably need the data in a format more like this.\n   Bed  Week_Num  Day\n0    3        27    1\n0    3        27    1\n0    3        27    2\n1    1        35    2\n1    1        35    2\n1    1        35    1\n1    1        35    1\n\nThe end goal is to have this data formatted in a way that INDEX and MATCH can work properly sum up the values based on these rows.\nHere is a link to the dataset that I am working with right now if you wanted to look this over. There are 3 sheets total, the second sheet is an index for the weeks and the 3rd sheet is the table which has the values used to get the final sum of each row.\nEDIT: New link\nhttps://docs.google.com/spreadsheets/d/1CdUzTewb8R0kcqELYkP3uanYQ0ITGVM7HlFs0A-UYKo/edit?usp=sharing\nData\n# copy the data to the clipboard and read with the following line\ndf = pd.read_clipboard(sep=',')\n\nBed,Week_Num,Day\n3,&quot;27, 27, 27&quot;,&quot;1, 1, 2&quot;\n1,&quot;35, 35, 35, 35&quot;,&quot;2, 2, 1, 1&quot;\n2,&quot;35, 35, 35, 36, 36&quot;,&quot;1, 1, 2, 2, 2&quot;\n1,&quot;35, 35, 36, 36, 36, 36, 36&quot;,&quot;1, 2, 2, 2, 2, 2, 1&quot;\n3,&quot;35, 36, 36, 36, 36, 36, 36&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;36, 36, 37&quot;,&quot;1, 2, 2&quot;\n3,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n4,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;36, 37, 37, 37, 37, 37, 37&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;37, 37, 37, 38, 38, 38, 38&quot;,&quot;1, 1, 2, 2, 2, 2, 2&quot;\n2,&quot;37, 37, 37, 38, 38, 38, 38&quot;,&quot;1, 1, 2, 2, 2, 2, 2&quot;\n2,&quot;37, 37, 38&quot;,&quot;1, 2, 2&quot;\n2,&quot;37, 37, 38, 38, 38, 38, 38, 38&quot;,&quot;1, 2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;37, 38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;38, 38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n2,&quot;38, 38, 38, 38, 38&quot;,&quot;2, 2, 2, 2, 1&quot;\n2,&quot;38, 38, 39&quot;,&quot;1, 2, 2&quot;\n3,&quot;38, 39, 39, 39, 39, 39, 39&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;38, 39, 39, 39, 39, 39, 39&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;38, 39, 39, 39, 39, 39, 39&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;38, 39, 39, 39, 39, 39, 39&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;38, 39, 39, 39, 39, 39, 39&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n4,&quot;39, 39, 39, 39, 39, 39, 40&quot;,&quot;2, 2, 2, 1, 1, 2, 2&quot;\n2,&quot;39, 39, 40&quot;,&quot;1, 2, 2&quot;\n4,&quot;39, 40, 40, 40, 40, 40, 40&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;39, 40, 40, 40, 40, 40, 40&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;39, 40, 40, 40, 40, 40, 40&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;39, 40, 40, 40, 40, 40, 40&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n4,&quot;39, 40, 40, 40, 40, 40, 40&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n1,&quot;40, 40, 40, 40&quot;,&quot;2, 2, 2, 2&quot;\n3,&quot;40, 41, 41, 41, 41, 41, 41&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n4,&quot;40, 41, 41, 41, 41, 41, 41&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;40, 41, 41, 41, 41, 41, 41&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;41, 41, 41, 42&quot;,&quot;1, 1, 2, 2&quot;\n2,&quot;41, 42, 42, 42, 42, 42, 42&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;41, 42, 42, 42, 42, 42, 42&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44&quot;,&quot;1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2&quot;\n3,&quot;42, 43, 43, 43, 43, 43, 43&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n4,&quot;43, 43, 43, 43, 43, 43&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n2,&quot;43, 44, 44, 44, 44, 44, 44&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;44, 44, 45, 45, 45, 45, 45, 45, 45&quot;,&quot;1, 2, 2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;44, 45, 45, 45, 45, 45, 45&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;45, 45, 45&quot;,&quot;1, 1, 2&quot;\n2,&quot;46, 46, 46, 46, 46, 46, 46&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;46, 46, 46, 46, 46, 46, 46&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;47, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1&quot;\n2,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;47, 48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 1&quot;\n2,&quot;48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 2, 1&quot;\n2,&quot;48, 48, 48, 48, 48, 48&quot;,&quot;2, 2, 2, 1, 1, 2&quot;\n2,&quot;48, 48, 48, 48&quot;,&quot;2, 2, 1, 1&quot;\n2,&quot;48, 48, 48, 48&quot;,&quot;2, 2, 1, 1&quot;\n2,&quot;48, 48, 48, 48&quot;,&quot;2, 1, 1, 2&quot;\n3,&quot;48, 48, 48, 48&quot;,&quot;2, 1, 1, 2&quot;\n2,&quot;48, 49, 49, 49, 49, 49, 49&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;48, 49, 49, 49, 49, 49, 49&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;50, 50, 50, 50, 50, 50&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n2,&quot;50, 50, 51, 51, 51, 51&quot;,&quot;1, 2, 2, 2, 2, 2&quot;\n3,&quot;50, 51, 51, 51, 51, 51, 51&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;51, 51, 51, 51, 51, 51, 51&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;51, 51, 52, 52, 52&quot;,&quot;1, 2, 2, 2, 2&quot;\n2,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;51, 52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;51, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1&quot;\n3,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n3,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 52, 52, 52, 52&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 52, 52, 52, 53&quot;,&quot;2, 2, 2, 1, 1, 2, 2&quot;\n3,&quot;52, 52, 52, 52, 52&quot;,&quot;2, 2, 1, 1, 2&quot;\n1,&quot;52, 52, 52, 53, 53&quot;,&quot;1, 1, 2, 2, 2&quot;\n3,&quot;52, 52, 52, 53&quot;,&quot;1, 1, 2, 2&quot;\n1,&quot;52, 52, 52, 53, 53, 53, 53, 53, 53, 53&quot;,&quot;1, 1, 2, 2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;52, 52, 52, 53, 53, 53, 53&quot;,&quot;1, 1, 2, 2, 2, 2, 2&quot;\n2,&quot;52, 52, 53, 53, 53, 53, 53&quot;,&quot;1, 2, 2, 2, 2, 2, 1&quot;\n2,&quot;52, 53, 53, 53, 53, 53, 53&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;53, 53, 53, 53, 53, 53&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n2,&quot;53, 53, 53, 53, 53, 53&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n1,&quot;53, 53, 53, 53, 53, 53&quot;,&quot;2, 2, 2, 1, 1, 2&quot;\n2,&quot;53, 53, 53, 1&quot;,&quot;1, 1, 2, 2&quot;\n2,&quot;5, 6, 6, 6, 6, 6, 6&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n1,&quot;6, 6, 7, 7, 7, 7, 7&quot;,&quot;1, 2, 2, 2, 2, 2, 1&quot;\n2,&quot;6, 7, 7, 7, 7&quot;,&quot;2, 2, 2, 2, 2&quot;\n3,&quot;8, 9, 9, 9, 9, 9, 9&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;9, 9, 10, 10, 10, 10, 10&quot;,&quot;1, 2, 2, 2, 2, 2, 1&quot;\n1,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;9, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 10, 10, 10, 10, 10, 10&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;10, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n1,&quot;11, 11, 11, 11, 11, 11, 11&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n1,&quot;11, 12, 12, 12, 12, 12, 12&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;11, 12, 12, 12, 12, 12, 12&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;11, 12, 12, 12, 12, 12, 12&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;11, 12, 12, 12, 12, 12, 12&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;11, 12, 12, 12, 12, 12, 12&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;12, 13, 13, 13, 13, 13, 13&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;12, 13, 13, 13, 13, 13, 13&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;12, 13, 13, 13, 13, 13, 13&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n1,&quot;12, 13, 13, 13, 13, 13, 13&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n3,&quot;12, 13, 13, 13, 13, 13, 13&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;12, 13, 13, 13, 13, 13, 13&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;13, 13, 13&quot;,&quot;1, 1, 2&quot;\n2,&quot;13, 14, 14, 14, 14, 14, 14&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;13, 14, 14, 14, 14, 14&quot;,&quot;2, 2, 2, 2, 2, 1&quot;\n3,&quot;13, 14, 14, 14, 14, 14, 14&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;14, 14, 14, 14, 14, 14, 14&quot;,&quot;2, 2, 2, 2, 1, 1, 2&quot;\n2,&quot;14, 15, 15, 15, 15, 15, 15&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;15, 16, 16, 16, 16, 16, 16&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;16, 17, 17, 17, 17, 17, 17&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;17, 18, 18, 18, 18, 18, 18&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;18, 19, 19, 19, 19, 19, 19&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n2,&quot;24, 24, 24, 24, 24, 24&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n2,&quot;25, 25, 25, 25, 25, 25&quot;,&quot;2, 2, 2, 2, 1, 1&quot;\n2,&quot;26, 26, 27, 27, 27, 27, 27&quot;,&quot;1, 2, 2, 2, 2, 2, 1&quot;\n2,&quot;26, 27, 27, 27, 27, 27, 27&quot;,&quot;2, 2, 2, 2, 2, 1, 1&quot;\n\n\n"
'I have created dictionary where key is actual column name and value is the name i want to change. below is what i am trying to do as for each value in data frame change name = rename value.\ndef renaming_columns(headers):\nrename_list = dict.fromkeys(headers)\nfor i in headers:\n    display(i)\n    rename_list[i] = input(&quot;Please enter relevant name: &quot;)\nreturn rename_list\n\nrenamed_column_name = renaming_columns(headers)\n\nfor i in renamed_column_name:\n    datafram = datafram.rename(columns={renamed_column_name.keys()[i]:renamed_column_name[i]})\n\n'
"I am trying to make it so that my script reads through the 'data_b' and 'data_d' columns and if it sees the conditions 'Rest' and 'True', it should save the previous row.\ndata_frame\n\nRow_ID           data_a      data_b    data_c      data_d\n59               0.30781     Discharge 2.31725     NaN\n60               0.30786     Discharge 2.31714     NaN\n61               0.30792          Rest 2.34857    True\n62               0.31313          Rest 2.38084     NaN\n181              0.93398     Discharge 2.31103     NaN\n182              0.93398     Discharge 2.31115     NaN\n183              0.93408          Rest 2.34550    True\n184              0.93930          Rest 2.36800     NaN\n\nI would like the output to be as follows:\nRow_ID           data_a      data_b    data_c      data_d\n60               0.30786     Discharge 2.31714     NaN\n182              0.93398     Discharge 2.31115     NaN\n\nAs you can see, rows 61 and 183 meet the criteria. There for it must save only rows 60 and 182.\n"
"I am trying to use pandas to create a list/array containing all the words from the &quot;review/text&quot; field of the following text file :\nproduct/productId: B001E4KFG0 review/userId: A3SGXH7AUHU8GW review/profileName: delmartian review/helpfulness: 1/1 review/score:\n5.0 review/time: 1303862400 review/summary: Good Quality Dog Food review/text: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n\nproduct/productId: B00813GRG4 review/userId: A1D87F6ZCVE5NK review/profileName: dll pa review/helpfulness: 0/0 review/score: 1.0 review/time: 1346976000 review/summary: Not as Advertised review/text: Product arrived labeled as Jumbo Salted Peanuts...\n\n(text file foods.txt available at : http://snap.stanford.edu/data/web-FineFoods.html)\nMy ultimate goal is to identify all the unique words that appear in the review/text field.\nI have written the following code :\n    import pandas as pd\n    \n    f=open(&quot;foods.txt&quot;,&quot;r&quot;)\n    df=pd.read_csv(f,names=['product/productId','review/userId','review/profileName','review/helpfulness','review/score','review/time','review/summary'])\n    selected = df[ df['review/summary'] ] \n    print(selected)\n\nselected.to_csv('result.csv', sep=' ', header=False)\n\nHowever, I am getting the error below :\nValueError: cannot index with vector containing NA / NaN values\n\nAny suggestions / comment ?\n"
'I have a pandas Series which contains the column names that I need to collect data from:\n1         col1\n3         col4\n4         col3\n5         col5\n6         col5\n\nAnd the dataframe that contains data looks like:\n   col1    col2    col3    col4    col5\n1  data1   data2   data3   data4   data5\n3  data6   data7   data8   data9   data10\n4  data11  data12  data13  data14  data15\n5  data16  data17  data18  data19  data20\n6  data21  data22  data23  data24  data25\n\nThe result should be like:\n1 data1\n3 data9\n4 data13\n5 data20\n6 data25\n\n'
'I have this dataset (screen)\n\n\nHow to draw plot with histograms of label distribution (its only 0 or 1) based on years\n\nHow to draw linear plot, where X coord is the date (year) and Y coord is the number of entries made in a given year\n\n\nThanks!\n'
"Lets say I have a dataframe A with attribute called 'score'.\nI can modify the 'score' attribute of the second row by doing:\ntmp = A.loc[2]\n\ntmp.score = some_new_value\n\nA.loc[2] = tmp\n\nBut I cant do it like this:\nA.loc[2].score = some_new_value\n\nWhy ?\n"
'I have a dataset of 400 images, 10 images of 40 different people. There are 2 NumPy arrays, &quot;olivetti_faces&quot; contains the images (400x64x64), and &quot;olivetti_faces_target&quot; contains the classes of those images (400), one class for each person.\nSo &quot;olivetti_faces&quot; is of the form: array([&lt;img1&gt;, &lt;img2&gt;, ..., &lt;img400&gt;]) where &lt;img&gt; is a 64x64 array of numbers, and &quot;olivetti_faces_target&quot; is of the form: array([0, 0, ..., 39]).\nYou can access the dataset here. You can load them after downloading as follows:\nimport numpy as np\ndata=np.load(&quot;olivetti_faces.npy&quot;)\ntarget=np.load(&quot;olivetti_faces_target.npy&quot;)\n\nI would like to randomly choose 100 of the images, with at least one image of each of the 40 people. How can I achieve this in NumPy?\nSo far I could randomly get 100 images using the following code:\nn = 100 # number of images to retrieve\nrand_indeces = np.random.choice(data.shape[0], n, replace=False)\ndata_random = data[rand_indeces]\ntarget_random = target_random[rand_indeces]\n\nBut it does not guarantee that at least one image of each of the 40 classes is included in data_random.\n'
'I have 2 dataframe, each has a single row, with the same columns:\ndf1 = feat_1 feat_2 ... feat_n a b c d ... z\n          A    B          N    1 2 3 4     9 \n\ndf2 = feat_1 feat_2 ... feat_n a b c d ... z\n          A    B          N    5 6 1 8     3\n\nfeat_1.. feat_n are the same with the 2 rows and the other columns should be concatenated such that the new row will be:\nnew_df = feat_1 feat_2 ... feat_n a_1 a_2 b_1 b_2 c_1 c_2 d_1 d_2 ... z_1 z_2\n          A    B             N     1   5   2   6   3   1   4   8       9   3\n\nWhat is the best way to do it?\n'
"I'm trying to plot a graph using matplotlib library. Here is my code.\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata = [[206.6, 735.4, 427.9, 175.2,384.4],\n[487.5, 273.7, 742.6, 159.5,144],\n[613.4, 0, 294.9, 0,0]]\nX = np.arange(5)\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(X + 0.00, data[0], color = 'b', width = 0.25)\nax.bar(X + 0.25, data[1], color = 'g', width = 0.25)\nax.bar(X + 0.50, data[2], color = 'r', width = 0.25)\nax.legend(labels=['Group-1', 'Group-2','Group-3','Group-4','Group-5'])\n\n\nI want the above graph groups to be of same color. Example, Group-1 histograms should be Red, Group-2 histograms should be Blue, Group-3 histograms should be Orange, etc.. How can I get that using the above code\n"
"By default, columns are all set to zero.\nMake entry as 1 at (row,column) where column name string present on URL column\nL # list that contains column names used to check if found on URL\nDataframe Image\ndef generate(statement,col):\n    if statement.find(col) == -1:\n      return 0\n    else:\n      return 1\n\nfor col in L:\n  df3[col].apply(generate, args=(col))\n\nI am a beginner, it throws and error:\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/series.py in f(x)\n4195     4196             def f(x):\n-&gt; 4197                 return func(x, *args, **kwds)    4198     4199         else:\nTypeError: generate() takes 2 positional arguments but 9 were given\n\nAny suggestions would be helpful\nEdit 1:\nafter,\ndf3[col].apply(generate, args=(col,))\n\ngot error:\n&gt; --------------------------------------------------------------------------- AttributeError                            Traceback (most recent call\n&gt; last) &lt;ipython-input-162-508036a6e51f&gt; in &lt;module&gt;()\n&gt;       1 for col in L:\n&gt; ----&gt; 2   df3[col].apply(generate, args=(col,))\n&gt; \n&gt; 2 frames pandas/_libs/lib.pyx in pandas._libs.lib.map_infer()\n&gt; \n&gt; &lt;ipython-input-159-9380ffd36403&gt; in generate(statement, col)\n&gt;       1 def generate(statement,col):\n&gt; ----&gt; 2     if statement.find(col) == -1:\n&gt;       3         return 0\n&gt;       4     else:\n&gt;       5         return 1\n&gt; \n&gt; AttributeError: 'int' object has no attribute 'find'\n\nEdit 2:\n&quot;I missed to emphasize on URL column in for loop code will rectify that&quot;\nEdit 3:\nUpdated and fixed to,\ndef generate(statement,col):\n    if col in str(statement):\n        return 1\n    else:\n        return 0\n\nfor col in L:\n  df3[col] = df3['url'].apply(generate, col=col)\n\nThanks for all the support!\n"
"Simple question, first time i am using pandas, but still could not find an answer googling.\nI have this dataframe info.\ninfo.head\nartist in_train new_filename\n0 Barnett Newman True 102257.jpg\n1 Barnett Newman True 75232.jpg\n2 kiri nichol False 32145.jpg\n.\n.\n.\n\nNow i want to create two new dataframes and adding rows from info to them depending on if the column in_train is True or False.\ntrain_info = pd.DataFrame(columns=('artist', 'filename'))\ntest_info = pd.DataFrame(columns=('artist', 'filename'))\n\nfor index, row in info.iterrows():\n  if row[&quot;in_train&quot;] == True:\n    train_info.append(row[[&quot;artist&quot;, &quot;new_filename&quot;]])\n  else:\n    test_info.append(row[[&quot;artist&quot;, &quot;new_filename&quot;]])\n\nFor some reason this code assigned all rows in info to test_info and nothing to train_info.\nHow do i solve this?\n"
'so I am trying to calculate the likeliness of floats on CSGO skins.\nA float is a value between 0 and 1 and they are distinguished in five sections.\nFactory New (0 to 0.07) 3%, Minimal Wear (0.07 to 0.14) 24%, Field-Tested (0.14 to 0.38) 33%, Well-Worn (0.38 to 0.45) 24% and Battle-Scarred (0.45 to 1.0) 16%.\nAs you can see the distribution among the float values is not even, but weighed. However in each section the values are then spread evenly, for example:\nhttps://blog.csgofloat.com/content/images/2020/07/image-6.png\nIt then gets tricky when you introduce float caps, meaning the float is no longer between 0 and 1, but fo example between 0.14 and 0.65.\nThe value is calculated how follows:\n\nA section is selected according to their weights.\nA float in the range of that section is randomly generated.\nThe final float is calculated according to this formula:\nfinal_float = float * (max_float - min_float) + min_float\nfloat being the randomly generated value, max and min_float the upper and lower cap (in this case 0.14 and 0.65).\n\nI now want to calculate the distribution of skins with a cap among the five sections.\nHow would I do this?\nThank you in advance.\n'
"I am looking to a data science project where I will be able to sum up the fantasy football points by the college the players went to (e.g. Alabama has 56 active players in the NFL so I will go through a database and add up all of their fantasy points to compare with other schools).\nI was looking at the website:\nhttps://fantasydata.com/nfl/fantasy-football-leaders?season=2020&amp;seasontype=1&amp;scope=1&amp;subscope=1&amp;aggregatescope=1&amp;range=3\nand I was going to use Beautiful Soup to scrape the rows of players and statistics and ultimately, fantasy football points.\nHowever, I am having trouble figuring out how to extract the players' college alma mater. To do so, I would have to:\n\nClick each &quot;players&quot; name\nScrape each and every profile of the hundreds of NFL players for one line &quot;College&quot;\nPlace all of this information into its own column.\n\nAny suggestions here?\n"
"I am trying to translate an R dplyr code into Python Pandas and I am not getting similar results when using groupby() and duplicate().\nI have a dataset of size (20000*3), as follows:\n\n\n\n\nProduct\nTrade\nCrop\n\n\n\n\nFungi\nVIC\nGrapes\n\n\nASH\nCAN\nAPPLE\n\n\nFUNGI\nCAN\nSEED\n\n\n\n\nIn R, they have written the code as follows:\nProducts_table &lt;- Products_table %&gt;% group_by(product,crop) %&gt;% filter(! duplicated(trade))}\n\nThey get a reduced dataset as output with (5000*3) size. I think the duplicated values were deleted.\nI've tried the same thing in Python Pandas:\n product_table = product_table.groupby(['product','crop']).reset_index(drop=False)\n\nBut I am getting a table of size (n*1), which has reduced the column size.\nAny suggestions to how do I get to the groupby and duplicated in Python Pandas and get the same result as in R dplyr?\n"
"I have a grouped dataframe as shown in this link:\nI want to convert it into a nested dictionary, where 'Dia' is the main key and inside contains another dictionary where the keys are the 'mac_ap' and the values are another dictionary where the key would be 'download' and 'upload' and the values would be the corresponding values to column 'bytes'\nsomething like this:\n"
"I have a pandas Data Frame df of which elements of one column col is a numpy.ndarray of str type. For example,\ncol\n['I like tea', 'cricket ']\n['basket ball', 'I like coffee', 'cricket ']\n['I like tea', 'cricket ']\n['basket ball', 'cricket ']\n\nnow I want to get number of such unique numpy.ndarray in the col to convert it into a categorical column with new column containing positive integer values for each unique numpy.ndarray. When I'm using df['col'].unique it is throwing following error\nTypeError: unhashable type: 'numpy.ndarray'\n\nHow to find the number of unique elements for this numpy.ndarray column?\n\nedit:\nThe output I'm expecting is,\n['I like tea', 'cricket '],['basket ball', 'I like coffee', 'cricket '],['basket ball', 'cricket ']\nThese are the unique lists in the column col. I want these to be outputed.\n\nedit 2:\nWhen I converted each list of the col into a tuple, I'm getting the required result. Why is this happening?\n\n\n"
"The problem:\nThe input table, let's say, is a merged table of calls and bills, having columns: TIME of the call and months of all the bills. The idea is to have a table that has the last 3 bills the person paid starting from the time of the call. That way putting the bills in context of the call.\nThe Example input and output:\n# INPUT:\n# df\n# TIME        ID   2019-08-01   2019-09-01   2019-10-01   2019-11-01   2019-12-01\n# 2019-12-01  1    1            2            3            4            5\n# 2019-11-01  2    6            7            8            9            10\n# 2019-10-01  3    11           12           13           14           15\n\n# EXPECTED OUTPUT:\n# df_context\n# TIME        ID   0     1     2\n# 2019-12-01  1    3     4     5\n# 2019-11-01  2    7     8     9\n# 2019-10-01  3    11    12    13\n\nEXAMPLE INPUT CREATION:\ndf = pd.DataFrame({\n    'TIME': ['2019-12-01','2019-11-01','2019-10-01'],\n    'ID':   [1,2,3],\n    '2019-08-01':   [1,6,11],\n    '2019-09-01':   [2,7,12],\n    '2019-10-01':   [3,8,13],\n    '2019-11-01':   [4,9,14],\n    '2019-12-01':   [5,10,15],\n})\n\nThe code I have got so far:\n# HOW DOES ONE GET THE col_to FOR EVERY ROW?\ncol_to = df.columns.get_loc(df['TIME'].astype(str).values[0])\ncol_from = col_to - 3\n\ndf_context = pd.DataFrame()\ndf_context = df_context.append(pd.DataFrame(df.iloc[:, col_from : col_to].values))\ndf_context[&quot;TIME&quot;] = df[&quot;TIME&quot;]\ncols = df_context.columns.tolist()\ndf_context = df_context[cols[-1:] + cols[:-1]]\ndf_context.head()\n\nOUTPUT of my code:\n# OUTPUTS:\n#   TIME        0   1   2\n# 0 2019-12-01  2   3   4    should be  3     4     5\n# 1 2019-11-01  7   8   9    all good\n# 2 2019-10-01  12  13  14   should be  11    12    13\n\nWhat my code seems to lack if a for loop or two, for the first two lines of code, to do waht I want it to do, but I just can't believe that there isn't a better a solution than the one I am concocting right now.\n"
"Hello guys i want to make non-linear regression in python with curve fit\nthis is my code:\n#fit a fourth degree polynomial to the economic data\nfrom numpy import arange\nfrom scipy.optimize import curve_fit\nfrom matplotlib import pyplot\nimport math\n\nx = [17.47,20.71,21.08,18.08,17.12,14.16,14.06,12.44,11.86,11.19,10.65]\ny = [5,35,65,95,125,155,185,215,245,275,305]\n\n# define the true objective function\ndef objective(x, a, b, c, d, e):\n    return ((a)-((b)*(x/3-5)))+((c)*(x/305)**2)-((d)*(math.log(305))-math.log(x))+((e)*(math.log(305)-(math.log(x))**2))\n\npopt, _ = curve_fit(objective, x, y)\n# summarize the parameter values\na, b, c, d, e = popt\n# plot input vs output\npyplot.scatter(x, y)\n# define a sequence of inputs between the smallest and largest known inputs\nx_line = arange(min(x), max(x), 1)\n# calculate the output for the range\ny_line = objective(x_line, a, b, c, d, e)\n# create a line plot for the mapping function\npyplot.plot(x_line, y_line, '--', color='red')\npyplot.show()\n\nthis is my error :\nTraceback (most recent call last):\nFile &quot;C:\\Users\\Fahmi\\PycharmProjects\\pythonProject\\main.py&quot;, line 16, in \npopt, _ = curve_fit(objective, x, y)\nFile &quot;C:\\Users\\Fahmi\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\scipy\\optimize\\minpack.py&quot;, line 784, in curve_fit\nres = leastsq(func, p0, Dfun=jac, full_output=1, **kwargs)\nFile &quot;C:\\Users\\Fahmi\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\scipy\\optimize\\minpack.py&quot;, line 410, in leastsq\nshape, dtype = _check_func('leastsq', 'func', func, x0, args, n)\nFile &quot;C:\\Users\\Fahmi\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\scipy\\optimize\\minpack.py&quot;, line 24, in _check_func\nres = atleast_1d(thefunc(((x0[:numinputs],) + args)))\nFile &quot;C:\\Users\\Fahmi\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\scipy\\optimize\\minpack.py&quot;, line 484, in func_wrapped\nreturn func(xdata, params) - ydata\nFile &quot;C:\\Users\\Fahmi\\PycharmProjects\\pythonProject\\main.py&quot;, line 13, in objective\nreturn ((a)-((b)(x/3-5)))+((c)(x/305)**2)-((d)(math.log(305))-math.log(x))+((e)(math.log(305)-(math.log(x))**2))\nTypeError: only size-1 arrays can be converted to Python scalars\nthanks before\n"
"When I change/add a variable to my config.py file and then try to import it to my Jupyter Notebook I get:\nImportError: cannot import name 'example_var' from 'config'\n\nconfig.py:\n\nexample_var = 'example'\n\n\njp_notebook.ipynb:\n\nfrom config import example_var\n\nprint(example_var)\n\nBut after I restart the Jupyter Kernel it works fine until I modify the config.py file again. I read somewhere that it's because jupyter already cached that import. Is there any other way to delete that cache so I don't have to restart the kernel every time I make a change in the config.py file. Thanks for help in advance.\n"
"I am analyzing chess games using Python. Currently I have a list of strings, containing ~400,000 elements. Each element is one of 64 possible strings. This is because each element denotes a square on the chess board, of which there are 64 ('a1', 'a2', ... , 'h7', 'h8').\nWhat is the most efficient way of finding how many times each of the 64 elements occur in the entire list?\nI know sorting the list would make such a task quicker, but since I am dealing with strings and not integers I am not sure I can sort them. I do not mind using external modules, but I am looking for the most primitive and pythonic way here.\nAny help is greatly appreciated!\n"
"I am struggling to figure this row by row iteration out in pandas.\n\nI have a dataset that contains chat conversations between 2 parties. I would like to combine the dataset to row by row conversation between Person 1 and Person 2. Sometimes people will type in multiple sentences and these will appear as multiple records within the dataframe.\n\nThis is the loop that I have come back with:\n\n\nline_text to be combined \ntimestamp to be updated with the latest time\nif the line_by show that the same person typed in multiple lines and sent through their chat\nsince there are multiple id's in this dataset signifying each conversation record between person 1 and person 2, i would like the loop to be run by each unique id.\n\nid    timestamp line_by line_text\n1234    02:54.3 Person1 Text Line 1\n1234    03:23.8 Person2 Text Line 2\n1234    03:47.0 Person2 Text Line 3\n1234    04:46.8 Person1 Text Line 4\n1234    05:46.2 Person1 Text Line 5\n9876    06:44.5 Person2 Text Line 6\n9876    07:27.6 Person1 Text Line 7\n9876    08:17.5 Person2 Text Line 8\n9876    10:20.3 Person2 Text Line 9\n\n\n\nI would like to see the data to be changed to the following:\n\nid    timestamp line_by line_text\n1234    02:54.3 Person1 Text Line 1\n1234    03:47.0 Person2 Text Line 2Text Line 3\n1234    05:46.2 Person1 Text Line 4Text Line 5\n9876    06:44.5 Person2 Text Line 6\n9876    07:27.6 Person1 Text Line 7\n9876    10:20.3 Person2 Text Line 8Text Line 9\n\n\nAny ideas are appreciated.\n"
"I'm having trouble adding a penalty to binary_crossentropy. The idea is to penalize the loss function when the mean of predefined groups of errors breaches a certain threshold. \nBelow is the helper function that takes the mask expressing the groups and the already computed crossentropy. It will simply return the number of times some threshold was breached to penalize the actual loss function calling it.\n\ndef penalty(groups_mask, binary_crossentropy):\n  errors = binary_crossentropy\n  unique_groups = set(groups_mask)\n  groups_mask = np.array(groups_mask)\n  threshold = # whatever\n  c = 0\n  for group in unique_groups:\n      error_mean = K.mean(errors[(groups_mask == group).nonzero()], axis=-1)\n      if error_mean &gt; threshold:\n        c += 1\n  return c\n\n\nThe trouble is that error_mean is not a scalar and I can't figure out a simple ways to compare it to threshold. \n"
'I have a table of counts of binary outcomes and I would like to fit a beta binomial distribution to estimate $\\alpha$ and $\\beta$ parameters, but I am getting errors when I try to fit/sample the model distribution the way I do for other cases:\n\nimport pymc3 as pm\nimport pandas as pd\n\ndf = pd.read_csv(\'~/data.csv\', low_memory=False)\ndf = df[df.Clicks &gt;= 0]\n\nC0=df.C.values\nI0=df.N.values\nN0 = C0 + I0\n\nwith pm.Model() as model:\n    C=pm.constant(C0)\n    I=pm.constant(I0)\n    C1=pm.constant(C0 + 1)\n    I1=pm.constant(I0 + 1)\n    N=pm.constant(N0)\n    alpha = pm.Exponential(\'alpha\', 1/(C0.sum()+1))\n    beta = pm.Exponential(\'beta\', 1/(I0.sum()+1))\n    obs = pm.BetaBinomial(\'obs\', alpha, beta, N, observed=C0)\n\n\nwith model:\n    advi_fit = pm.variational.advi(n=int(1e4))\n    trace1 = pm.variational.sample_vp(advi_fit, draws=int(1e4))\n\npm.traceplot(trace1[::10])\n\nwith model:\n    step = pm.NUTS()\n    #step = pm.Metropolis() # &lt;== same problem\n    trace2 = pm.sample(int(1e3), step)\n\npm.traceplot(trace2[::10])\n\n\nIn both cases the sampling fails with:\n\nMissingInputError: ("An input of the graph, used to compute Elemwise{neg,no_inplace}(P_logodds_), was not provided and not given a value.Use the Theano flag exception_verbosity=\'high\',for more information on this error.", P_logodds\n\n\nIn the advi case the full stack trace is:\n\n\n\nMissingInputError                         Traceback (most recent call last)\n&lt;ipython-input-46-8947c7c798e5&gt; in &lt;module&gt;()\n----&gt; 1 import codecs, os;__pyfile = codecs.open(\'\'\'/tmp/py7996Jip\'\'\', encoding=\'\'\'utf-8\'\'\');__code = __pyfile.read().encode(\'\'\'utf-8\'\'\');__pyfile.close();os.remove(\'\'\'/tmp/py7996Jip\'\'\');exec(compile(__code, \'\'\'/home/dmahler/Scripts/adops-bayes2.py\'\'\', \'exec\'));\n\n/home/dmahler/Scripts/adops-bayes2.py in &lt;module&gt;()\n     59     advi_fit = pm.variational.advi(n=int(J*6.4e4), learning_rate=1e-3/J, epsilon=1e-8, accurate_elbo=False)\n     60     #advi_fit = pm.variational.advi_minibatch(minibatch_RVs=[alpha, beta, p], minibatch_tensors=[C,I,N])\n---&gt; 61     trace = pm.variational.sample_vp(advi_fit, draws=int(2e4))\n     62 \n     63 pm.traceplot(trace[::10])\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/pymc3/variational/advi.pyc in sample_vp(vparams, draws, model, local_RVs, random_seed, hide_transformed)\n    317 \n    318     varnames = [str(var) for var in model.unobserved_RVs]\n--&gt; 319     trace = NDArray(model=model, vars=vars_sampled)\n    320     trace.setup(draws=draws, chain=0)\n    321 \n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/pymc3/backends/ndarray.pyc in __init__(self, name, model, vars)\n     21     """\n     22     def __init__(self, name=None, model=None, vars=None):\n---&gt; 23         super(NDArray, self).__init__(name, model, vars)\n     24         self.draw_idx = 0\n     25         self.draws = None\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/pymc3/backends/base.pyc in __init__(self, name, model, vars)\n     34         self.vars = vars\n     35         self.varnames = [var.name for var in vars]\n---&gt; 36         self.fn = model.fastfn(vars)\n     37 \n     38 \n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/pymc3/model.pyc in fastfn(self, outs, mode, *args, **kwargs)\n    374         Compiled Theano function as point function.\n    375         """\n--&gt; 376         f = self.makefn(outs, mode, *args, **kwargs)\n    377         return FastPointFunc(f)\n    378 \n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/pymc3/memoize.pyc in memoizer(*args, **kwargs)\n     12 \n     13         if key not in cache:\n---&gt; 14             cache[key] = obj(*args, **kwargs)\n     15 \n     16         return cache[key]\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/pymc3/model.pyc in makefn(self, outs, mode, *args, **kwargs)\n    344                                on_unused_input=\'ignore\',\n    345                                accept_inplace=True,\n--&gt; 346                                mode=mode, *args, **kwargs)\n    347 \n    348     def fn(self, outs, mode=None, *args, **kwargs):\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/compile/function.pyc in function(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\n    318                    on_unused_input=on_unused_input,\n    319                    profile=profile,\n--&gt; 320                    output_keys=output_keys)\n    321     # We need to add the flag check_aliased inputs if we have any mutable or\n    322     # borrowed used defined inputs\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/compile/pfunc.pyc in pfunc(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\n    477                          accept_inplace=accept_inplace, name=name,\n    478                          profile=profile, on_unused_input=on_unused_input,\n--&gt; 479                          output_keys=output_keys)\n    480 \n    481 \n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc in orig_function(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\n   1774                    profile=profile,\n   1775                    on_unused_input=on_unused_input,\n-&gt; 1776                    output_keys=output_keys).create(\n   1777             defaults)\n   1778 \n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc in __init__(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\n   1426             # OUTPUT VARIABLES)\n   1427             fgraph, additional_outputs = std_fgraph(inputs, outputs,\n-&gt; 1428                                                     accept_inplace)\n   1429             fgraph.profile = profile\n   1430         else:\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/compile/function_module.pyc in std_fgraph(input_specs, output_specs, accept_inplace)\n    175 \n    176     fgraph = gof.fg.FunctionGraph(orig_inputs, orig_outputs,\n--&gt; 177                                   update_mapping=update_mapping)\n    178 \n    179     for node in fgraph.apply_nodes:\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/gof/fg.pyc in __init__(self, inputs, outputs, features, clone, update_mapping)\n    169 \n    170         for output in outputs:\n--&gt; 171             self.__import_r__(output, reason="init")\n    172         for i, output in enumerate(outputs):\n    173             output.clients.append((\'output\', i))\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/gof/fg.pyc in __import_r__(self, variable, reason)\n    358         # Imports the owners of the variables\n    359         if variable.owner and variable.owner not in self.apply_nodes:\n--&gt; 360                 self.__import__(variable.owner, reason=reason)\n    361         if (variable.owner is None and\n    362                 not isinstance(variable, graph.Constant) and\n\n/home/dmahler/Anaconda/lib/python2.7/site-packages/theano/gof/fg.pyc in __import__(self, apply_node, check, reason)\n    472                             "for more information on this error."\n    473                             % str(node)),\n--&gt; 474                             r)\n    475 \n    476         for node in new_nodes:\n\nMissingInputError: ("An input of the graph, used to compute Elemwise{neg,no_inplace}(P_logodds_), was not provided and not given a value.Use the Theano flag exception_verbosity=\'high\',for more information on this error.", P_logodds_)\n&gt; /home/dmahler/Anaconda/lib/python2.7/site-packages/theano/gof/fg.py(474)__import__()\n    472                             "for more information on this error."\n    473                             % str(node)),\n--&gt; 474                             r)\n    475 \n    476         for node in new_nodes:\n\n\nBefore I was aware of pymc3.BetaBinomial I was fitting trying to achieve the same result using separate Beta and Binomial instances:\n\nwith pm.Model() as model:\n    C=pm.constant(C0)\n    I=pm.constant(I0)\n    C1=pm.constant(C0 + 1)\n    I1=pm.constant(I0 + 1)\n    N=pm.constant(N0)\n    alpha = pm.Exponential(\'alpha\', 1/(C0.sum()+1))\n    beta = pm.Exponential(\'beta\', 1/(I0.sum()+1))\n    p = pm.Beta(\'P\', alpha, beta,  shape=K)\n    b = pm.Binomial(\'B\', N, p, observed=C0)\n\n\nThis completes successfully but different methods produce rather different results. I thought this could be partly due to the extra level of indirection between the priors and the observations makes the search space larger. When I came across BetaBinomial I figured it would make the search easier as well as being the right thing ^TM. Otherwise I believe the to models should be logically equivalent. Unfortunatelly I cannot figure out how to make batebinomial work and I am unable to find any examples using BetaBinomial on the internet.\n\n\nHow do I make the BetaBinomial modwel work?\nAre the models really logically equivalent?\nDoes anybody have a better guess at the cause of the numerical problems with the initial hierarchical version?\n\n\nHow I could fix them?\n\n\n'
"I am working on a PySpark job with a large data in below format.\n\nID-1234567  iplong  agent   partner client  country timestamp   category    reference\n\n\nI need to find the average amount of duplicate records based on columns 2(iplong), 3(agent), 5(client), 6(country), 9(reference) within one minute time intervals for each partner.\n\nI understand that I need to\n\n\nDivide records into one-minute intervals.\nMap everything by partner\nGroup everything by partner\nReduce each interval by count of total records and by count of distinct records and take difference to get amount of duplicate records (Also need to define a function to compare two records only with the values of 2(iplong), 3(agent), 5(client), 6(country), 9(reference) columns.)\nAdd all the partner from all the intervals and their duplicate counts together. And divide by the count of their appearance.\n\n\nI understand this process but not the exact implementation in pyspark.\n\nCan someone please help me with the implementation of any of the above steps in pyspark.\n\nSample Data:\n\n9794474 1000460030  Samsung_S5233   dv4gs   dswae   in  2012-03-08 00:00:00 mg  riflql2a0yv8xoa9sq0recx4x\n9794471 3386480130  Nokia_C3-00 duq7h   dr75h   py  2012-03-08 00:00:00 co  \n9794468 1907980030  Nokia_5233  dv6i3   ds3xq   vn  2012-03-08 00:00:00 es  gp53lqr9njqd6z2ap5d364sip\n9794467 1791990020  MAUI    duxto   dvb8g   in  2012-03-08 00:00:00 ad  \n9794466 1791000060  Nokia_3110c dusg4   dvb8g   in  2012-03-08 00:00:00 ad  \n9794477 1353590020  Blackberry_9300 du6dt   dtr0u   es  2012-03-08 00:00:00 es  h5njsswvxorsau9u8fxh0e9se\n9794478 1402290050  NokiaC6-01.3    dusnc   dsgcn   ru  2012-03-08 00:00:00 mc  \n9794481 1848749950  Nokia_C3-00 dvry3   dr6sg   th  2012-03-08 00:00:01 mc  oj0rekb51pvirnjuqjt10zn4b\n\n\nUpdate:\n\nSo far I've tried putting the whole data into MySQL and reading from it. But it takes too much time in read operations.\n\nFor mapreduce approach, I've tried different smaller things. But don't understand how am I going to approach it further in code. Hence, not able to move forward with one approach.\n\nclicks_rdd = sc.parallelize(list(clicks_reader)[1:]) \nminwise_clicks = clicks_rdd.groupby(clicks_rdd.index.map(lambda t: t.minute)) # Didn't work\nclicks_mapped_publishers = clicks_rdd.map(lambda x : (x.pop(3), x)) # Works fine but need the records divided into minute intervals first.\n\n\nHave also tried some other things here and there. But nothing solid.\n\nFollowing is the first 25 records of my original dataset file.\n\nid,iplong,agent,partnerid,cid,cntr,timeat,category,referer\n9794476,1071324855,SonyEricsson_K70,dv3va,dsfag,us,2012-03-08 00:00:00.0,ad,\n9794474,1000461055,Samsung_S5233,dv4gs,dswae,in,2012-03-08 00:00:00.0,mg,riflql2a0yv8xoa9sq0recx4x\n9794471,3386484265,Nokia_C3-00,duq7h,dr75h,py,2012-03-08 00:00:00.0,co,\n9794468,1907981997,Nokia_5233,dv6i3,ds3xq,vn,2012-03-08 00:00:00.0,es,gp53lqr9njqd6z2ap5d364sip\n9794467,1791989091,MAUI,duxto,dvb8g,in,2012-03-08 00:00:00.0,ad,\n9794466,1791002478,Nokia_3110c,dusg4,dvb8g,in,2012-03-08 00:00:00.0,ad,\n9794477,1353590316,Blackberry_9300,du6dt,dtr0u,es,2012-03-08 00:00:00.0,es,h5njsswvxorsau9u8fxh0e9se\n9794478,1402285217,NokiaC6-01.3,dusnc,dsgcn,ru,2012-03-08 00:00:00.0,mc,\n9794481,1848747204,Nokia_C3-00,dvry3,dr6sg,th,2012-03-08 00:00:01.0,mc,oj0rekb51pvirnjuqjt10zn4b\n9794482,1893182670,NokiaC2-03,du77a,dr6x2,id,2012-03-08 00:00:01.0,co,r63f8uhijvr2irvka3glwyb38\n9794483,1912930086,MAUI,dvwdj,dvb8g,id,2012-03-08 00:00:01.0,ad,\n9794485,2098816838,GT-S5360B,dvjtq,dr72e,th,2012-03-08 00:00:01.0,co,\n9794486,3309473440,MAUI,dv6i3,ds3k0,za,2012-03-08 00:00:01.0,es,\n9794492,702295934,Nokia_9300,dv6i3,dtqrw,ng,2012-03-08 00:00:01.0,es,onbw7na2mi8a62g4p6y3av2qt\n9794493,694135362,Nokia_N95,dupgf,dvb8g,sd,2012-03-08 00:00:01.0,ad,hoq05psulkszxm4izlql4g962\n9794495,1791428359,Samsung_S8300,dvpo7,dvb8g,in,2012-03-08 00:00:02.0,co,im387req0zp1ucygamhgadgtm\n9794496,1783607271,GT-S5570,du56s,dsgq2,in,2012-03-08 00:00:02.0,mc,immfap8948rebeym8ri0vf5cr\n9794498,1860189232,Samsung_GT-B3313,du56s,ds22r,in,2012-03-08 00:00:02.0,mc,r81nrzjemr5jrfvjjeoxmdm4y\n9794499,1868310973,Nokia_2730c,dv3va,drvnr,au,2012-03-08 00:00:02.0,ad,\n9794500,1893182511,Nokia_5233,dv6i7,dr6tn,id,2012-03-08 00:00:02.0,co,tq09jycwii12iul7hzalucue3\n9794501,1884230403,Samsung_GT-S3653,dvjil,ds92x,in,2012-03-08 00:00:02.0,mc,h0z1j3bwiverubvwg851e9eon\n9794503,1945382244,GT-S5360,dvijt,dsgq2,in,2012-03-08 00:00:02.0,mc,fbbenjzmoe0oc7x4e2080nj8x\n9794508,2928534854,Samsung_R310,dunsq,dsg3q,us,2012-03-08 00:00:02.0,ad,kl9j183hop90uwq2p82iidjsb\n9794510,3063717709,Samsung_GT-S3653,dvjjf,dr751,in,2012-03-08 00:00:02.0,ad,rpdt9h4kpooxiedeuuxvk6gi5\n9794511,3557769762,Samsung_C3050,du53k,dr71b,hr,2012-03-08 00:00:02.0,se,\n\n\nUpdate 2\n\nSample output. This is a Tab Separated Values format. You can copy and paste it in excel to view properly. Here avg_spiky_ReAgCnIpCi is the average count of reference, Agent, Country, IP, Client combination repeating each second. Which I am interested in. And then I can make changes to derive other features.\n\npartnerid   status  avg_spiky_ReAgCnIpCi    std_spiky_ReAgCnIpCi    night_avg_spiky_ReAgCnIpCi  night_std_spiky_ReAgCnIpCi  morning_avg_spiky_ReAgCnIpCi    morning_std_spiky_ReAgCnIpCi    afternoon_avg_spiky_ReAgCnIpCi  afternoon_std_spiky_ReAgCnIpCi  evening_avg_spiky_ReAgCnIpCi    evening_std_spiky_ReAgCnIpCi    avg_spiky_ReAgCnIp  std_spiky_ReAgCnIp  avg_spiky_ReAgCn    std_spiky_ReAgCn    avg_spiky_iplong    std_spiky_iplong    avg_spiky_agent std_spiky_agent night_avg_spiky_agent   night_std_spiky_agent   morning_avg_spiky_agent morning_std_spiky_agent afternoon_avg_spiky_agent   afternoon_std_spiky_agent   evening_avg_spiky_agent evening_std_spiky_agent avg_spiky_cid   std_spiky_cid   avg_spiky_cntr  std_spiky_cntr  avg_spiky_referer   std_spiky_referer   night_avg_spiky_referer night_std_spiky_referer morning_avg_spiky_referer   morning_std_spiky_referer   afternoon_avg_spiky_referer afternoon_std_spiky_referer evening_avg_spiky_referer   evening_std_spiky_referer   category_es category_mc category_ad category_co category_se category_mg category_pp category_in category_gd category_ow total_clicks    distinct_iplong distinct_agent  distinct_cid    distinct_cntr   distinct_referer    night_click_percent morning_click_percent   afternoon_click_percent evening_click_percent   night_referer_percent   morning_referer_percent afternoon_referer_percent   evening_referer_percent night_agent_percent morning_agent_percent   afternoon_agent_percent evening_agent_percent   avg_total_clicks    std_total_clicks    avg_distinct_iplong std_distinct_iplong avg_distinct_agent  std_distinct_agent  avg_distinct_cid    std_distinct_cid    avg_distinct_cntr   std_distinct_cntr   avg_distinct_referer    std_distinct_referer    avg_null_agent  std_null_agent  avg_null_referer    std_null_referer    night_avg_null_referer  night_std_null_referer  morning_avg_null_referer    morning_std_null_referer    afternoon_avg_null_referer  afternoon_std_null_referer  evening_avg_null_referer    evening_std_null_referer    first_15_minute_percent second_15_minute_percent    third_15_minute_percent last_15_minute_percent  brand_MAUI_percent  brand_Nokia_percent brand_Generic_percent   brand_Apple_percent brand_Blackberry_percent    brand_Samsung_percent   brand_SonyEricsson_percent  brand_LG_percent    brand_other_percent avg_per_hour_density    std_per_hour_density    cntr_az_percent cntr_id_percent cntr_in_percent cntr_us_percent cntr_ng_percent cntr_tr_percent cntr_ru_percent cntr_th_percent cntr_sg_percent cntr_uk_percent cntr_other_percent\ndu3nk   0   1.23    8.47    0   0   0   0   0   0   1.23    8.47    1.24    8.48    1.27    8.61    4.14    11.73   8.73    16.06   0   0   0   0   0   0   8.73    16.06   38.18   240.99  60  248 1.8 10.35   0   0   0   0   0   0   1.8 10.35   0   1   0   0   0   0   0   0   0   0   3360    644 250 61  31  1696    0   0   0   1   0   0   0   1   0   0   0   1   3360    0   644 0   250 0   61  0   31  0   1696    0   0   0   598 0   0   0   0   0   0   0   598 0   0.16    0.17    0.33    0.35    0.01    0   0.05    0   0   0   0   0   0   2   0   0   0   0.13    0   0   0   0   0   0.01    0   0.04\ndu3nq   1   8.38    5.83    0   0   0   0   0   0   8.38    5.83    25.13   9.27    25.13   9.27    188.5   49.5    188.5   49.5    0   0   0   0   0   0   188.5   49.5    53.86   39.03   188.5   49.5    25.13   9.27    0   0   0   0   0   0   25.13   9.27    1   0   0   0   0   0   0   0   0   0   377 1   1   5   1   8   0   0   0   1   0   0   0   1   0   0   0   1   377 0   1   0   1   0   5   0   1   0   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0.09    0.14    0.33    0.44    0   0   0   1   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   1\ndu3op   0   30.43   46.87   0   0   0   0   44.67   59.63   19.75   30.19   35.5    48.84   35.5    48.84   71  52.27   71  52.27   0   0   0   0   134 0   39.5    33.5    13.31   8.24    71  52.27   35.5    48.84   0   0   0   0   67  62  19.75   30.19   0   0   1   0   0   0   0   0   0   0   213 1   1   6   1   1   0   0   0.63    0.37    0   0   1   1   0   0   1   1   213 0   1   0   1   0   6   0   1   0   1   0   0   0   205 0   0   0   0   0   129 0   76  0   0   0.09    0.25    0.66    0   1   0   0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0   0   0   1\ndu3or   0   1   0   0   0   0   0   1   0   1   0   1   0   1   0   1   0   1   0   0   0   0   0   1   0   1   0   1   0   1   0   1   0   0   0   0   0   1   0   1   0   0   1   0   0   0   0   0   0   0   0   2   2   1   1   1   1   0   0   0.5 0.5 0   0   1   1   0   0   1   1   2   0   2   0   1   0   1   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0.5 0   0   0.5 0   0   0   0   0   0   1   0   0   2   0   0   1   0   0   0   0   0   0   0   0   0\ndu3ov   0   1.01    0.11    0   0   0   0   0   0   1.01    0.11    1.01    0.11    1.01    0.11    44  30  29.33   31.63   0   0   0   0   0   0   29.33   31.63   6.29    5.59    44  30  1.02    0.21    0   0   0   0   0   0   1.02    0.21    0   0   0   0   1   0   0   0   0   0   88  1   2   10  1   86  0   0   0   1   0   0   0   1   0   0   0   1   88  0   1   0   2   0   10  0   1   0   86  0   0   0   0   0   0   0   0   0   0   0   0   0   0.84    0   0   0.16    0   0.94    0   0.06    0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   1\ndu3ox   0   1   0   0   0   0   0   0   0   1   0   1   0   1   0   1   0   1   0   0   0   0   0   0   0   1   0   1   0   1   0   1   0   0   0   0   0   0   0   1   0   0   1   0   0   0   0   0   0   0   0   1   1   1   1   1   1   0   0   0   1   0   0   0   1   0   0   0   1   1   0   1   0   1   0   1   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0   1\ndu3oy   0   1.02    0.12    0   0   0   0   0   0   1.02    0.12    1.02    0.15    1.02    0.15    64.5    31.5    32.25   35.55   0   0   0   0   0   0   32.25   35.55   7.59    6.03    64.5    31.5    1.03    0.28    0   0   0   0   0   0   1.03    0.28    0   0   0   0   1   0   0   0   0   0   129 1   3   12  1   124 0   0   0   1   0   0   0   1   0   0   0   1   129 0   1   0   3   0   12  0   1   0   124 0   0   0   0   0   0   0   0   0   0   0   0   0   0.26    0.58    0.16    0   0   0.95    0   0.04    0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   0   0   1\ndu3oz   1   1   0   0   0   0   0   1   0   0   0   1   0   33  3.35    1.01    0.08    165 0   0   0   0   0   165 0   0   0   27.5    8.18    165 0   33  3.35    0   0   0   0   33  3.35    0   0   1   0   0   0   0   0   0   0   0   0   165 164 1   6   1   5   0   0   1   0   0   0   1   0   0   0   1   0   165 0   164 0   1   0   6   0   1   0   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   1   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   1   0   0\ndu3p1   0   1   0   0   0   0   0   1   0   0   0   1   0   18.2    16.11   1.01    0.07    91  80  0   0   0   0   91  80  0   0   15.17   14.82   91  80  18.2    16.11   0   0   0   0   18.2    16.11   0   0   1   0   0   0   0   0   0   0   0   0   182 181 1   6   1   5   0   0   1   0   0   0   1   0   0   0   1   0   182 0   181 0   1   0   6   0   1   0   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0.06    0   0   0.94    0   1   0   0   0   0   0   0   0   2   0   0   0   0   0   0   0   0   0   1   0   0\ndu3r7   0   3.63    1.32    0   0   0   0   0   0   3.63    1.32    29  0   29  0   29  0   29  0   0   0   0   0   0   0   29  0   3.63    1.32    29  0   29  0   0   0   0   0   0   0   29  0   0   0   0   0   1   0   0   0   0   0   29  1   1   8   1   1   0   0   0   1   0   0   0   1   0   0   0   1   29  0   1   0   1   0   8   0   1   0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   1   0   0   1   0   0   0   0   0   0   0   0   0   0   1   0\n\n"
"I have .csv dataset that has two columns, each column has nodes and a row of this file represents a relationship between the two nodes.\nI want to use it for creating a graph using Python's networkx library but I could not figure out how to reach each cell of each column in a row. Here is my code.\n\nimport networkx as nx\nimport csv\nG = nx.Graph()\nwith open('/pathToTheFile/edges.csv', newline='') as f:\n    reader = csv.reader(f)\n    counter = 0\n    for row in reader:\n        G.add_edge()#I will pass two nodes here to add an edge between them\n        counter += 1\n        if counter &gt;= 65535:\n            break\n\n"
'I have a CSV file with a list of Towns, Countys, and Country.\n\n\n                Town           County  Country      \n            Ampthill     Bedfordshire  England      \n             Arlesey     Bedfordshire  England      \n\n\n\nI want to be able to check if a town is in the list, if a town is in the list then I want to know what the country is. \nCurrently using pandas. \n\nAny thoughts on how to check if a variable value is present in pandas.\n'
'I have a data frame like this \n CODE | TYPE  \n0001 | A\n0001 | B\n0001 | C\n0002 | A\n\n0003 | B\n....\n\nand need to transform it to the following\n \nCODE | TYPE_A  | TYPE_B  | TYPE_C\n0001 | 1       |    1    |   1   0002 | 1       |    0    |   00003 | 0       |    1    |   0    \n\nThank you in advance\n'
'I have two data sets - structured_data (having 4000 + records) and record_data (have 400 records). I am trying to compare what all records are present in record_data that matches with structured_data. \n\nI do this by using some common attributes from both the dataset by using the following condition - \n\nfilter_df = record_data[record_data.UnitNumber.isin(structured_data.UnitNumber) &amp; record_data.price.isin(structured_data.price) &amp; record_data.zipcode.astype(int).isin(structured_data.zipcode.astype(int))  &amp; record_data.bedrooms.isin(structured_data.bedrooms) &amp; record_data.bathrooms.isin(structured_data.bathrooms)]\n\nThis condition is not giving only those records that holds true for each of the above condition. While many records in the result do follow the condition but not all. I made sure that datatypes of the attributes taken above are the same in both the datasets.\n\nWhat I am trying to achieve is integrate both the datasets and eventually have one dataset giving all the unique records. \n\nWondering if there is anything wrong with the code. Will be happy to share the dataset if required. Thanks!\n'
'Assume I have a dataset below (This is a cut down version of the daily Treasury yields sourced from the Treasury website). I had artificially created some zeroes in there to illustrate the question I have. Each row corresponds to a given date.\n\nQuestion - Assume I wanted to do log or quadratic extrapolation/interpolation within each row. Is there a quick way to do it or would one have to iterate through each row to fill it in?\n\n    x = np.array([[ 1.31,  1.44,  0, 2.51,  0,  0],\n                  [ 0,  1.45,  1.63, 2.48,  0,  2.69],\n                  [ 1.31,  1.43,  1.59, 2.48,  2.55,  2.71]])\n\n'
"I have some data in a pandas dataframe and I am attempting to create another dataframe that is strictly a calculated value.\n\nThe data is from a heating plant in a CSV format that I uploaded to a Github account I created. https://github.com/bbartling/Data\n\ndata = pd.read_csv('C:\\\\Users\\\\Python Scripts\\\\SetPoint_data.csv', index_col='Date', parse_dates=True)\n\ndata.info()\n\n\noutput is:\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 69839 entries, 2017-10-01 05:00:11.205000 to 2018-01-02 05:45:14.419000\nData columns (total 3 columns):\nhwr    69839 non-null float64\nhws    69839 non-null float64\noat    69839 non-null float64\ndtypes: float64(3)\nmemory usage: 2.1 M\n\n\nB\n\nWhat I am trying to do is calculate a setpoint for a boiler system and the setpoint is based off of outdoor air temperature. data[('oat')] The other data points are hot water return and supply temperatures. Ultimately I am trying to see how close to setpoint the boiler system is performing.\n\nThe setpoint that I want to calculate is at 10F outdoor air temperature the heating water will be 170 setpoint and at 40F outdoor air temperature the heating water will be at 130 setpoint. And its really a simple linear reset calculation but I dont know how to build another pandas dataframe.\n\nX = data[('oat')]\n\ndef setpoint_calc(X):\n    A = np.matrix([[10,1],[40,1]])\n    B = np.matrix([[170],[130]])\n    A_inverse = np.linalg.inv(A)\n    X = A_inverse * B\n\n    return X\n\nsetpoint_calc(X)\n\n\noutput is:\n\nmatrix([[  -1.33333333],\n        [ 183.33333333]])\n\n\nSo how do I sweep over a range of x values to come up with my calculated setpoint? setpoint  = -1.33333333 * data[('oat')] + 183.33333333\n\nI know I need to slice/index the 2d numpy array but I am not sure how.. Is the best method in a loop?\n\nsetpoint = pd.DataFrame()\nfor X in X:\n    setpoint_calc.append(X)\n\n\nI can visually put this into a scatter plot with a some lines to represent what I am trying to calculate in a seperate dataFrame... Hopefully this makes sense:\n\nplt.scatter(data['oat'], data['hws'], color='grey', marker='+')\n\nplt.plot([10,40],[170,130], color='blue', label='Reset Range')\n\nplt.plot([40,80],[130,130], color='green', label='130F during mild weather')\nplt.plot([-25,10],[170,170], color='red', label='170F during cold weather')\n\nplt.xlabel('Outdoor Temp')\nplt.ylabel('Hot Water Temp')\nplt.title('Calculated Setpoint')\n\nplt.legend()\n\nplt.show()\n\n\n\n"
'I am pretty new with scikitlearn and right now I am struggling with the preprocessing stage. \n\nI have the following categorical features (I parsed a JSON file and place it in a dictionary) so: \n\ndct[\'alcohol\'] = ["Binge drinking",\n  "Heavy drinking",\n  "Moderate consumption",\n  "Low consumption",\n  "No consumption"]\n\n\ndct[\'tobacco\']= ["Current daily smoker - heavy",\n  "Current daily smoker",\n  "Current on-and-off smoker",\n  "Former Smoker",\n  "Never Smoked",\n  "Snuff User"]\n\ndct[\'onset\'] = "Gradual",\n  "Sudden"]\n\n\nMy first approach is to convert it first to integers with label enconder and then to the one-hot-coding method: \n\nOH_enc = sklearn.preprocessing.OneHotEncoder(n_values=[len(dct[\'alcohol\']),len(dct[\'tobacco\']),len(dct[\'onset\'])])\nle_alc = sklearn.preprocessing.LabelEncoder()\nle_tobacco = sklearn.preprocessing.LabelEncoder()\nle_onset = sklearn.preprocessing.LabelEncoder()\n\nle_alc.fit(dct[\'alcohol\'])\nle_tobacco.fit(dct[\'tobacco\'])\nle_onset.fit(dct[\'onset\'])\n\n\nlist_patient = []\nlist_patient.append(list(le_alc.transform([\'Low consumption\'])))\nlist_patient.append(list(le_tobacco.transform([\'Former Smoker\'])))\nlist_patient.append(list(le_onset.transform([\'Sudden\'])))\n\nlist1 = []\nlist1.append(np.array(list_patient).T[0][:])\nlist1.append([1,2,0])\n\nOH_enc.fit(list1)\nprint(OH_enc.transform([[4,2,0]]).toarray())\n\n\nSo eventually if you OHE (4,2,0) you get : \n\n[[0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0.]]\n\n\nWhich is what I want since the first 5 columns refers to the "alcohol" feature, the 6 next columns refers to tobacco, and the last 2 columns refers to the onset feature. \n\nHowever, let\'s assume that one example could have more than one value in one feature. Let\'s say one example gets "Binge drinking" and "Heavy drinking" from the alcohol feature. Then, if you OHE ([0,1],2,0) you would get:\n\n[[1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]]\n\n\nThis last step I do not know how to code it with sklearn.preprocessing.OneHotEncoder. I mean, how can I code 2 values in one feature per example?\n\nI know that there might be a better way to code "alcohol", "tobacco", and "onset" because they are ordinal values (and then each value in each feature correlates to the other value in the same feature. Thus I could just label them and then normalize it.But let\'s assume those are categorical variables with independent relationship. \n'
"I am trying to use Maximal information coefficient in jupyter notebook, with the Boston Housing dataset.\n\nimport numpy as np\nimport pandas as pd\nfrom minepy import MINE\n\n#Read dataset\ndf = pd.read_csv('housing.data', delim_whitespace=True, header=None);\ncol_name = ['CRIM', 'ZN' , 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf.columns = col_name;\n\n#Compute MIC\nm = MINE()\nm.compute_score(df[col_name[0:13]], df.MEDV)\nprint(m.mic())\n\n\nm.compute_score(.. is giving me a ValueError: Buffer has wrong number of dimensions (expected 1, got 2).\n\nUpdate\n\nI understand now that compute_score() is expecting a vector instead of a matrix. What would be the proper way to find MIC scores between df.MEDV and the 13 features df[col_name[0:13]]?\n"
'I am working on a Grading System ( graduation project ). I have preprocessed the data, then used TfidfVectorizer on the data and used LinearSVC to fit the model. \n\nThe System goes as follows, it has 265 definitions, of arbitrary lengths; but in total, they sum up to shape of (265, 8581 )\nso when I try to input some new random sentence to predict against it, I get this message \n\nError Message\n\nyou could have a look at the code used ( Full &amp; long ) if you want to; \n\nCode used;\n\ndef normalize(df):\n    lst = []\n    for x in range(len(df)):\n        text = re.sub(r"[,.\'!?]",\'\', df[x])\n        lst.append(text)\n    filtered_sentence = \' \'.join(lst)\n    return filtered_sentence\n\n\ndef stopWordRemove(df):\n    stop = stopwords.words("english")\n    needed_words = []\n    for x in range(len(df)):\n\n        words = word_tokenize(df)\n        for word in words:\n            if word not in stop:\n                needed_words.append(word)\n    return needed_words\n\n\ndef prepareDataSets(df):\n    sentences = []\n    for index, d in df.iterrows():\n        Definitions = stopWordRemove(d[\'Definitions\'].lower())\n        Definitions_normalized = normalize(Definitions)\n        if d[\'Results\'] == \'F\':\n            sentences.append([Definitions, \'false\'])\n        else:\n            sentences.append([Definitions, \'true\'])\n    df_sentences = DataFrame(sentences, columns=[\'Definitions\', \'Results\'])\n    for x in range(len(df_sentences)):\n        df_sentences[\'Definitions\'][x] = \' \'.join(df_sentences[\'Definitions\'][x])\n    return df_sentences\n\ndef featureExtraction(data):\n    vectorizer = TfidfVectorizer(min_df=10, max_df=0.75, ngram_range=(1,3))\n    tfidf_data = vectorizer.fit_transform(data)\n    return tfidf_data\n\ndef learning(clf, X, Y):\n    X_train, X_test,  Y_train, Y_test = \\\n    cross_validation.train_test_split(X,Y, test_size=.2,random_state=43)\n    classifier = clf()\n    classifier.fit(X_train, Y_train)\n    predict = cross_validation.cross_val_predict(classifier, X_test, Y_test, cv=5)\n    scores = cross_validation.cross_val_score(classifier, X_test, Y_test, cv=5)\n    print(scores)\n    print ("Accuracy of %s: %0.2f(+/- %0.2f)" % (classifier, scores.mean(), scores.std() *2))\n    print (classification_report(Y_test, predict))\n\n\nThen I run these scripts : which I get the mentioned error after\n\ntest = LinearSVC()\ndata, target = preprocessed_df[\'Definitions\'], preprocessed_df[\'Results\']\ntfidf_data = featureExtraction(data)\nX_train, X_test,  Y_train, Y_test = \\\ncross_validation.train_test_split(tfidf_data,target, test_size=.2,random_state=43)\ntest.fit(tfidf_data, target)\npredict = cross_validation.cross_val_predict(test, X_test, Y_test, cv=10)\nscores = cross_validation.cross_val_score(test, X_test, Y_test, cv=10)\nprint(scores)\nprint ("Accuracy of %s: %0.2f(+/- %0.2f)" % (test, scores.mean(), scores.std() *2))\nprint (classification_report(Y_test, predict))\nXnew = ["machine learning is playing games in home"]\ntvect = TfidfVectorizer(min_df=1, max_df=1.0, ngram_range=(1,3))\nX_test= tvect.fit_transform(Xnew)\nynew = test.predict(X_test)\n\n'
"I pre-processing a data-set. I binarized one of the columns. after binarization I think the values are incorrect. the data has 303 observations(rows) and 14 features(columns).and the column i am binarizing is the last column. \n\nhere is a part of my code-\n\n    import pandas as pd\n    import numpy as np\n\n    #importing the dataset\n    header_names = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n    dataset = pd.read_csv('E:/HCU proj doc/EHR dataset/cleveland_data.csv', names= header_names)\n\n\n    array = dataset.values\n\n    # binarize num\n    from sklearn.preprocessing import Binarizer\n    x = array[:,13:]\n    binarize = Binarizer(threshold=0.0).fit(x)\n    transform_binarize = binarize.transform(x)\n\n    array[:,13:]=transform_binarize\n    print(transform_binarize)\n\n\nhere is what the original data column look like-\n\n     0,2,1,0,0.........1,0,3,1,1,2\n\n\nand here is the output of the above code-\n\n         [[0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]]\n\n\nI think the last ones are incorrect. I dont understand why is that.\n"
"I am trying to make a learning curve and the algorithm I want to use is knn algorithm. for this what should be the value of estimator. Its possible values or options are not in documentation(and I am not sure if it should be there). \n\nhere is my code- \n\nfeatures = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\ntarget = 'num'\n\ntrain_size, train_scores, validation_scores = learning_curve(estimator = KNN(), x=dataset[features], y=dataset[target], train_size=train_sizes, cv=5, scoring='confusion_matrix')\n\n\nerror is - KNN() is not defined(which is obvious why). but my question is if I want to use knn algorithm what should be it value.\n"
'I have a file with several data from wifi access. The database is divided in multiple columns : user_id, start (when the device connect to the router), end (when the device disconnect from the router). \n\nExemple : \n\nuser_id   start     end \n1   15/05/16 13:51  15/05/16 14:06 \n1   15/05/16 14:06  15/05/16 14:32 \n1   15/05/16 14:32  15/05/16 14:34 \n2   15/05/16 11:14  15/05/16 11:25 \n2   15/05/16 11:25  15/05/16 12:09 \n2   15/05/16 12:14  15/05/16 12:42 \n2   15/05/16 17:33  15/05/16 17:41 \n2   15/05/16 17:41  15/05/16 18:27\n\n\nThe problem is, sometime the devices disconnect and reconnect. I would like to group the data in event like that:\n\nuser_id start   end\n1   15/05/16 13:51  15/05/16 14:34\n2   15/05/16 11:14  15/05/16 12:42\n2   15/05/16 17:33  15/05/16 18:27\n\n\nIs there an efficient way to do it with pandas ?\n'
'I have a dataset like this\n\ndataset = pd.read_csv(\'1053.csv\')\nprint dataset.head(25)\n\n                  date  power\n0    2009-7-14 0:30:00  0.039\n1    2009-7-14 1:00:00  0.147\n2    2009-7-14 1:30:00  0.134\n3    2009-7-14 2:00:00  0.131\n4    2009-7-14 2:30:00  0.076\n5    2009-7-14 3:00:00  0.039\n6    2009-7-14 3:30:00  0.039\n7    2009-7-14 4:00:00  0.052\n8    2009-7-14 4:30:00  0.148\n9    2009-7-14 5:00:00  0.136\n10   2009-7-14 5:30:00  0.132\n11   2009-7-14 6:00:00  0.060\n12   2009-7-14 6:30:00  0.034\n13   2009-7-14 7:00:00  0.034\n14   2009-7-14 7:30:00  0.033\n15   2009-7-14 8:00:00  0.326\n16   2009-7-14 8:30:00  0.140\n17   2009-7-14 9:00:00  0.133\n18   2009-7-14 9:30:00  0.107\n19  2009-7-14 10:00:00  0.161\n20  2009-7-14 10:30:00  0.042\n21  2009-7-14 11:00:00  1.259\n22  2009-7-14 11:30:00  1.227\n23  2009-7-14 12:00:00  0.167\n24  2009-7-14 12:30:00  0.518\n\n\nHow to extract rows containing exact time? for example.\nI tried to do like this\n\ndf = dataset[dataset.date.str.contains("2:00:00",regex=False)]\ndf1 = df.reset_index()\ndel df1[\'index\']\nprint df1.head(7)\n\n\nBut it is giving me this results.  \n\n                  date  power\n0    2009-7-14 2:00:00  0.131\n1   2009-7-14 12:00:00  0.167\n2   2009-7-14 22:00:00  0.208\n3    2009-7-15 2:00:00  0.085\n4   2009-7-15 12:00:00  0.097\n5   2009-7-15 22:00:00  0.203\n6    2009-7-16 2:00:00  0.038\n\n\nI want results to be like this\n\n                  date  power\n0    2009-7-14 2:00:00  0.131\n1    2009-7-15 2:00:00  0.085\n2    2009-7-16 2:00:00  0.038\n3    2009-7-17 2:00:00  0.141\n4   2009-7-18 2:00:00  0.039\n5   2009-7-19 2:00:00  0.039\n6   2009-7-20 2:00:00  0.079\n\n'
"I am working on a data frame which has 4 columns in total, i want to bin each column of that data frame iteratively in 8 equal parts. The bin number should be assigned to the data in a separate column for each column.\nThe code should work even if any different data frame is provided with different column names.\nHere, is the code i tried.\n\nfor c in df3.columns:\n    df3['bucket_' + c] = (df3.max() - df3.min()) // 2 + 1\n    buckets = pd.cut(df3['bucket_' + c], 8, labels=False) \n\n\nsample data frame\n\nexpected output\n\nThe respected bin columns display the bin number assigned to each data point according to the range in which they will (using pd.cut to cut column in 8 equal parts) fall.\nThanks in advance!!\n\nsample data\n\ngp1_min gp2 gp3 gp4\n\n17.39   23.19   28.99   44.93\n\n0.74    1.12    3.35    39.78\n\n12.63   13.16   13.68   15.26\n\n72.76   73.92   75.42   94.35\n\n77.09   84.14   74.89   89.87\n\n73.24   75.72   77.28   92.3\n\n78.63   84.35   64.89   89.31\n\n65.59   65.95   66.49   92.43\n\n76.79   83.93   75.89   89.73\n\n57.78   57.78   2.22    71.11\n\n99.9    99.1    100      100\n\n100     100    40.963855    100\n\n\nexpected output\n\ngp1_min gp2 gp3 gp4 bin_gp1 bin_gp2 bin_gp3 bin_gp4\n\n17.39   23.19   28.99   44.93   2   2   2   3\n\n0.74    1.12    3.35    39.78   1   1   1   3\n\n12.63   13.16   13.68   15.26   1   2   2   2\n\n72.76   73.92   75.42   94.35   5   6   6   7\n\n77.09   84.14   74.89   89.87   6   7   6   7\n\n73.24   75.72   77.28   92.3    6   6   6   7\n\n78.63   84.35   64.89   89.31   6   7   5   7\n\n65.59   65.95   66.49   92.43   5   6   5   7\n\n76.79   83.93   75.89   89.73   6   7   6   7\n\n57.78   57.78   2.22    71.11   4   4   1   6\n\n99.9    99.1    100      100    8   8   8   8\n\n100      100    40.96    100    8   8   3   8\n\n"
"My dataframe is like this-\n\n   Energy_MWh    Month\n0   39686.82    1979-01\n1   35388.78    1979-02\n2   50134.02    1979-03\n3   37499.22    1979-04\n4   20104.08    1979-05\n5   17440.26    1979-06\n\n\nIt goes on like this to the month 2015-12. So you can imagine all the data.\n\nI want to plot a continuous graph with the months as the x-axis and the Energy_MWh as the y-axis. How to best represent this using matplotlib?\n\nI would also like to know for my knowledge if there's a way to print 1979-01 as Jan-1979 on the x-axis and so on. Probably a lambda function or something while plotting.\n"
"guys, I'm new to Data science and Python. I'm working on a regression problem. My question is when I'm trying to plot my test part of target variable im getting strange type of plot\n\nfrom sklearn.model_selection import train_test_split\ntrain_input, test_input, train_target, test_target = \ntrain_test_split(features, target, test_size = 0.25, random_state = 42)\n# Remove the labels from the dataset\nplt.xlim(0,100)\nplt.plot(test_target , 'g');\n\n\n\n\nis it because of random indexes attached to test_target..?\n\nhow can i get continous curve like this \n"
"landData = []\nlandData = pd.read_csv('Agriculture land area.csv')\nlandData = landData.drop(landData.columns[[0]], axis=1)\n\n\nI currently have a CSV file that only have 1 column:\n\n\\\n\nI want to write my array landData to the second column after year but can't seem to find anything that works online.\n\nAnyone have any idea on how to do this?\n"
"I am request data from the Zoopla API, going through each of the page and then finally trying to append the data in a dataframe. I am using the following code: \n\nraw_data = pd.DataFrame([])\n\nfor i in range(1,5):\n    r = requests.get('http://api.zoopla.co.uk/api/v1/property_listings.json?area=Nottingham&amp;api_key=my_key&amp;page_size=20&amp;page_number='+str(i))\n    data = r.json()\n    df_temp = pd.DataFrame.from_dict(data['listing'])\n    raw_data.append(df_temp,ignore_index=True,sort=False)\n\n\nThe code above, however, doesn't append any data to raw_data at all. \n\nraw_data.shape // returns (0,0) after the for loop.\n\n\nNote: I haven't shared my API key in the url above. I can confirm that I am receiving data from the API\n"
'I am trying to return a dataframe where my rows and my columns and vice versa.\n\nThis is the code I have:\n\nmy_names = [2009,2010,2011,2012,2013,2014,2015,2016,2017,2018]\ncols = [\'link\', \'post\',\'shared\',\'timeline\',\'status\']\nser = [nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen]\ndf = pd.concat(ser, axis=1, keys=my_names)\n\n\nI have tried to us pd.pivot to do this:\n\ndf1 = df.pivot(index=df.columns, columns=df.index, values=df.values)\n\n\nBut this is the traceback I receive:\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-23-c08b1071deea&gt; in &lt;module&gt;()\n----&gt; 1 df1 = df.pivot(index=df.columns, columns=df.index, values=df.values)\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in pivot(self, index, columns, values)\n   4380         """\n   4381         from pandas.core.reshape.reshape import pivot\n-&gt; 4382         return pivot(self, index=index, columns=columns, values=values)\n   4383 \n   4384     _shared_docs[\'pivot_table\'] = """\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/reshape.py in pivot(self, index, columns, values)\n    385         else:\n    386             index = self[index]\n--&gt; 387         indexed = Series(self[values].values,\n    388                          index=MultiIndex.from_arrays([index, self[columns]]))\n    389         return indexed.unstack(columns)\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in __getitem__(self, key)\n   2131         if isinstance(key, (Series, np.ndarray, Index, list)):\n   2132             # either boolean or fancy integer index\n-&gt; 2133             return self._getitem_array(key)\n   2134         elif isinstance(key, DataFrame):\n   2135             return self._getitem_frame(key)\n\n/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py in _getitem_array(self, key)\n   2175             return self._take(indexer, axis=0, convert=False)\n   2176         else:\n-&gt; 2177             indexer = self.loc._convert_to_indexer(key, axis=1)\n   2178             return self._take(indexer, axis=1, convert=True)\n   2179 \n\n/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py in _convert_to_indexer(self, obj, axis, is_setter)\n   1267                 if mask.any():\n   1268                     raise KeyError(\'{mask} not in index\'\n-&gt; 1269                                    .format(mask=objarr[mask]))\n   1270 \n   1271                 return _values_from_object(indexer)\n\nKeyError: \'[(6.0606060606060606, 1.6666666666666667, 23.88535031847134, 18.660287081339714, 3.943661971830986, 16.666666666666664, 52.96703296703297, 72.85067873303167, 80.63380281690141, 55.69620253164557)\\n (8.080808080808081, 15.833333333333332, 0.0, 0.0, 5.352112676056338, 10.256410256410255, 2.857142857142857, 9.049773755656108, 9.507042253521126, 21.518987341772153)\\n (0.0, 1.6666666666666667, 0.3184713375796179, 3.588516746411483, 13.239436619718308, 11.282051282051283, 5.4945054945054945, 5.429864253393665, 2.464788732394366, 5.063291139240507)\\n (85.85858585858585, 71.66666666666667, 59.87261146496815, 65.55023923444976, 67.6056338028169, 43.84615384615385, 29.010989010989015, 9.95475113122172, 6.338028169014084, 16.455696202531644)\\n (0.0, 9.166666666666666, 15.92356687898089, 12.200956937799043, 9.859154929577464, 17.94871794871795, 9.670329670329672, 2.7149321266968327, 1.056338028169014, 1.2658227848101267)] not in index\'\n\n\nHere is the dataframe I am working with:\n\n'
'I need help to resolve this problem. If anyone has an idea to create a script using panda DataFrame with python3:\n\nI have two CSV dataset files too long to fit into panda DataFrame display. They have the following structure:\n\nF1: \n   gene disease\n0  g1   d1\n1  g1   d2\n2  g1   d3\n3  g2   d2\n4  g2   d3\n5  g2   d4\n6  g3   d2\n7  g3   d4\n8  g4   d1\n9  g4   d3\n\nF2:\n  gene  phenotype\n0  g1   ph1\n1  g1   ph2\n2  g2   ph2\n3  g2   ph3\n4  g2   ph4\n5  g3   ph4\n6  g4   ph1\n7  g4   ph3\n8  g5   ph4\n9  g6   ph2\n\n\ng= gene_name(strig, interger, Float), d = disease_name(string) ph= phenotype\n\nThe main goal is to compare all the genes in F2 with the genes in F1, and if they equal, append the name of gene and the phenotype associated to the disease in order to get an output file F3 containing all the diseases associated to phenotype by genes.\n\nThis is the expected output:\n\ngene disease phenotype\ng1   d1      ph2\ng1   d2      ph2\ng2   d2      ph1\ng2   d3      ph2\nect. \n\n\nCan anybody help me create the script? Any help would be much appreciated. Thanks!\n'
"So, basically what I want is to reduce the distance between two bars, but all the solutions that I read on the internet were people changing the width of the bars, and that is not what I want. \n\nThis is my chart: \n\nx = ['Goal (95%)','Achieved (100%)']\ny = [95,100]\n\nplt.bar(x,y,color = ('darkcyan','green'), width = 0.2)\nplt.ylabel('Percentage (%)')\n\n\n\n\nI expect something like this:\n\n\n\nIf I change the width I can reduce the distance, but the bars get very thick and I want them thin.\n\nplt.bar(x,y,color = ('darkcyan','green'), width = 0.8)\nplt.ylabel('Percentage (%)')\n\n\n\n"
'1st colunn : Weapon\n\n2nd column : Pepetrator_Age\n\nWhat i am trying to find is which weapon is popular in which age.\n\n\nFor example i am trying to draw a similar graph like this:\n\n\nFor example y axis should be number of cases\nx axis age of Perpetrator\n\nand lines are weapon type that Perpetrator used\n\nYou can copy paste this to jupyter to initialize dataset\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndata = pd.read_csv("hdb.csv", low_memory=False)\ncols = data.columns\ncols = cols.map(lambda x: x.replace(\' \', \'_\'))\ndata.columns = cols\n#clear the unnecessary data here\ndata = data.drop([\'Agency_Code\', \'Victim_Ethnicity\', \'Agency_Name\',\'Agency_Type\', \'Perpetrator_Ethnicity\', \'Victim_Count\', \'Perpetrator_Count\'], axis=1)\ndata = data[data.Perpetrator_Age != "0"]\ndata = data[data.Perpetrator_Age != ""]\ndata = data[data.Perpetrator_Age != " "]\ndata = data[data.Victim_Sex != "Unknown"]\ndata = data[data.Victim_Race != "Unknown"]\ndata = data[data.Perpetrator_Sex != "Unknown"]\ndata = data[data.Perpetrator_Race != "Unknown"]\ndata = data[data.Relationship != "Unknown"]\ndata = data[data.Weapon != "Unknown"]\ndata\n\n\nData set here:\nhttps://www.kaggle.com/jyzaguirre/us-homicide-reports\n'
'I need to count viewers by program for a streaming channel from a json logfile.\nI identify the programs by their starttimes, such as: \n\nSo far I have two Dataframes like this:\n\nThe first one contains all the timestamps from the logfile\n\nviewers_from_log = pd.read_json(\'sqllog.json\', encoding=\'UTF-8\')\n# Convert date string to pandas datetime object:\nviewers_from_log[\'time\'] = pd.to_datetime(viewers_from_log[\'time\'])\n\n\n\n\nSource JSON file:\n\n[\n    {\n        "logid": 191605,\n        "time": "0:00:17"\n    },\n    {\n        "logid": 191607,\n        "time": "0:00:26"\n    },\n    {\n        "logid": 191611,\n        "time": "0:01:20"\n    }\n]\n\n\nThe second contains the starting times and titles of the programs\n\nprograms_start_time = pd.DataFrame.from_dict(\'programs.json\', orient=\'index\')\n\n\n\nSource JSON file:\n\n{\n    "2019-05-29": [\n        {\n            "title": "\\"Amiről a kövek mesélnek\\"",\n            "startTime_dt": "2019-05-29T00:00:40Z"\n        },\n        {\n            "title": "Koffer - Kedvcsináló Kul(t)túrák Külföldön",\n            "startTime_dt": "2019-05-29T00:22:44Z"\n        },\n        {\n            "title": "Gubancok",\n            "startTime_dt": "2019-05-29T00:48:08Z"\n        }\n    ]\n}\n\n\nSo what I need to do is to count the entries / program in the log file and link them to the program titles.\n'
"My task:\n\nFor the next set of questions, we will be using census data from the United States Census Bureau. Counties are political and geographic subdivisions of states in the United States. This dataset contains population data for counties and states in the US from 2010 to 2015. See this document for a description of the variable names.\n\nThe census dataset (census.csv) should be loaded as census_df. Answer questions using this as appropriate.\n\nQuestion 5\n\nWhich state has the most counties in it? (hint: consider the sumlevel key carefully! You'll need this for future questions too...)\n\nThis function should return a single string value.\n\ncensus_df = pd.read_csv('census.csv')\ncensus_df = census_df[census_df['SUMLEV']==50]\ncensus_df_2 = census_df.groupby(by='STNAME',axis=0)\n\n\nThis, however, does not group the DataFrame by 'STNAME', which can be seen when executing census_df_2.head()\n\nI suppose this should work on a grouped DataFrame:\n\ndef answer_five():\n    return census_df_2[ census_df_2['COUNTY'].count() == max( census_df_2['COUNTY'].count() ) ].index().tolist()[0]\nanswer_five()\n\n\nWhy does the groupby function not work? I've tried changing the axis and using the set_index() function instead but I can't get it to work.\n\nIf someone knows another way to solve this problem I'd appreciate it.\n"
'I am completing my data science project and want to merge data using get_dummies pandas library\n\nmy df :\n\n    0_A  0_B 0_C  1_A 1_B 1_C\n0   1    0   0     0  1   0\n1   0    1   0     1  0   0\n2   0    1   0     0  0   1 \n\n\nOutput :\n\n  A    B   C  \n0   1    1   0     \n1   1    1   0     \n2   0    1   1  \n\n\ntried using if statement but no luck \n\nRegards \nSaif\n'
'I\'m training a CNN with tensorflow for recognizing handwriting digits. First code trains model with mnist dataset. After that saves the model.\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist=input_data.read_data_sets("data/mnist",one_hot=True,reshape=False)\n\nX=tf.placeholder(tf.float32,[None,28,28,1])\nY=tf.placeholder(tf.float32,[None,10])\n\nwc1=tf.Variable(tf.random.truncated_normal([6,6,1,16],stddev=0.2))\nbc1=tf.Variable(tf.random.truncated_normal([16],stddev=0.2))\n\nwc2=tf.Variable(tf.random.truncated_normal([5,5,16,32],stddev=0.2))\nbc2=tf.Variable(tf.random.truncated_normal([32],stddev=0.2))\n\nwd1=tf.Variable(tf.random.truncated_normal([1568,256],stddev=0.2))\nbd1=tf.Variable(tf.random.truncated_normal([256],stddev=0.2))\n\nwd2=tf.Variable(tf.random.truncated_normal([256,64],stddev=0.2))\nbd2=tf.Variable(tf.random.truncated_normal([64],stddev=0.2))\n\nwdo=tf.Variable(tf.random.truncated_normal([64,10],stddev=0.2))\nbdo=tf.Variable(tf.random.truncated_normal([10],stddev=0.2))\n\ny=tf.nn.relu(tf.nn.conv2d(X,wc1,strides=[1,1,1,1],padding="SAME")+bc1)\ny=tf.nn.max_pool(y,ksize=[1,2,2,1],strides=[1,2,2,1],padding="SAME")\ny=tf.nn.relu(tf.nn.conv2d(y,wc2,strides=[1,1,1,1],padding="SAME")+bc2)\ny=tf.nn.max_pool(y,ksize=[1,2,2,1],strides=[1,2,2,1],padding="SAME")\ny=tf.reshape(y,(-1,1568))\ny=tf.nn.tanh(tf.linalg.matmul(y,wd1)+bd1)\ny=tf.nn.tanh(tf.linalg.matmul(y,wd2)+bd2)\ny_pred=tf.nn.softmax(tf.linalg.matmul(y,wdo)+bdo)\n\nxent=-tf.reduce_sum(Y*tf.math.log(y_pred))\nl2=tf.reduce_sum(tf.math.square(Y-y_pred))\n\ncorrect_pred=tf.equal(tf.argmax(Y,1),tf.argmax(y_pred,1))\naccuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n\noptimizer=tf.train.AdamOptimizer(1e-3).minimize(xent)\nimages=[]\n\nsaver=tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(3001):\n        bx,by=mnist.train.next_batch(50)\n        sess.run(optimizer,feed_dict={X:bx,Y:by})\n    print("Model is trained")\n    acc,x_l,l2_l=sess.run([accuracy,xent,l2],feed_dict={X:bx,Y:by})\n    print("Iteration",i,"Accuracy="+str(acc),"Cross Entropy Loss="+str(x_l),"Mean Squared Error="+str(l2_l))\n    test_acc,test_x,test_l=sess.run([accuracy,xent,l2],feed_dict={X:mnist.test.images,Y:mnist.test.labels})\n    print("Train Accuracy="+str(acc),"Cross Entropy Loss="+str(x_l),"Mean Squared Error="+str(l2_l),"\\n\\n")\n    save_path = saver.save(sess, "tmp/model.ckpt")\n    print("Model saved in path: %s" % save_path)\n\n\nSecond code restores model. It predicts image with restored model.\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist=input_data.read_data_sets("data/mnist",one_hot=True,reshape=False)\n\nX=tf.placeholder(tf.float32,[None,28,28,1])\nY=tf.placeholder(tf.float32,[None,10])\n\nwc1=tf.Variable(tf.random.truncated_normal([6,6,1,16],stddev=0.2))\nbc1=tf.Variable(tf.random.truncated_normal([16],stddev=0.2))\n\nwc2=tf.Variable(tf.random.truncated_normal([5,5,16,32],stddev=0.2))\nbc2=tf.Variable(tf.random.truncated_normal([32],stddev=0.2))\n\nwd1=tf.Variable(tf.random.truncated_normal([1568,256],stddev=0.2))\nbd1=tf.Variable(tf.random.truncated_normal([256],stddev=0.2))\n\nwd2=tf.Variable(tf.random.truncated_normal([256,64],stddev=0.2))\nbd2=tf.Variable(tf.random.truncated_normal([64],stddev=0.2))\n\nwdo=tf.Variable(tf.random.truncated_normal([64,10],stddev=0.2))\nbdo=tf.Variable(tf.random.truncated_normal([10],stddev=0.2))\n\ny=tf.nn.relu(tf.nn.conv2d(X,wc1,strides=[1,1,1,1],padding="SAME")+bc1)\ny=tf.nn.max_pool(y,ksize=[1,2,2,1],strides=[1,2,2,1],padding="SAME")\ny=tf.nn.relu(tf.nn.conv2d(y,wc2,strides=[1,1,1,1],padding="SAME")+bc2)\ny=tf.nn.max_pool(y,ksize=[1,2,2,1],strides=[1,2,2,1],padding="SAME")\ny=tf.reshape(y,(-1,1568))\ny=tf.nn.tanh(tf.linalg.matmul(y,wd1)+bd1)\ny=tf.nn.tanh(tf.linalg.matmul(y,wd2)+bd2)\ny_pred=tf.nn.softmax(tf.linalg.matmul(y,wdo)+bdo)\n\nxent=-tf.reduce_sum(Y*tf.math.log(y_pred))\nl2=tf.reduce_sum(tf.math.square(Y-y_pred))\n\ncorrect_pred=tf.equal(tf.argmax(Y,1),tf.argmax(y_pred,1))\naccuracy=tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n\noptimizer=tf.train.AdamOptimizer(1e-3).minimize(xent)\nsaver=tf.train.Saver()\n\nwith tf.Session() as sess:\n    saver.restore(sess, "tmp/model.ckpt")\n    print("Acc is",sess.run(accuracy,feed_dict={X:mnist.test.images[0:59],Y:mnist.train.labels[0:59]}))\n    while(True):\n        path=input("file:")\n        image=cv2.imread(path,0)\n        cv2.imshow("Image",image)\n        image=cv2.resize(image,(28,28)).reshape(28,28,1)\n        val_=sess.run(y_pred,feed_dict={X:(image,)})\n        val=sess.run(tf.argmax(y_pred,1),feed_dict={X:(image,)})\n        print(val_)\n        print(val)\n        print("Value=",val)\n\n\nWhen I test the restored model, accuracy is under 0.1 .But model trained with 1.0 accuracy. How to fix this problem? \n'
'I\'m trying to split data to train/test/val sets, but I get this error:\n\n   for filename in os.listdir("Data/Descriptions"):\n        image = Image.open("Data/Images/" + filename + ".jpeg")\n        image = image.resize((new_width, new_height), PIL.Image.ANTIALIAS)\n        images.append(np.array(image))\n        #images.append(np.asarray(image))\n\n   train_images= images[:int(len(images * 0.8))]\n   labels = transfomed_labels[:int(len(transfomed_labels * 0.8))]\n\n\n\n  TypeError: can\'t multiply sequence by non-int of type \'float\'\n\n\nAnyone knows the solution?\n'
'So I have a Pandas DataFrame: \nhere\n\nAnd as you can see, I\'ve labelled it to make it a bit clearer (the book titles are simply names of the books and the numbers are their respective frequencies).\n\nTo calculate TF-IDF, I have to implement a function named "choice" with the signature as below:\n\ndef choice(term, documents):\n\n\nWhere "term" is any valid word that exists in the dataframe and "documents" is the pandas dataframe itself. The function calculates the TF-IDF for all the books in the dataframe (thus, rows, as each book is a new row) and then returns the name of the book which has the highest TF-IDF value like:\n\nchoice(\'the\', mydataframe)\n\n# output: pg16238.txt\n\n\nThe problem I am encountering is that I am unable to extract a particular frequency from the dataframe owing to having renamed the indexing from standard numbering (0, 1, 2, etc) to the book titles as you can see in the image which renders the built-in ".str.contains" function useless somehow (it doesn\'t work).\n\nI searched a lot about it and found out that it somehow became a "multi-index" dataframe? (I am a beginner so I do not have a good idea about how to tackle this).\n\nAnother problem I am having is that I do not know how to return the name of the book after the calculation is completed. Like, should I make a list of the indexes and then create a dictionary which looks something like:\n\n{\'book-title\': tf-idf-value}\n\n\nand then return the highest value? Please help me in figuring out the appropriate way to solve this.\n\nI have successfully written a working IDF calculation function:\n\ndef get_IDF(self, term):\n    N = 0\n    D = len(self.files_list)\n    for file in self.files_list:\n        with open(file, \'r\', encoding=\'utf-8-sig\', errors=\'replace\') as f:\n            temp_cleaned_data = \'\'.join(i.lower() for i in f.read() if ord(i) &lt; 128).translate(str.maketrans(\'\', \'\', string.punctuation)).replace(\'\\n\', \' \')\n            if self.contains_word(temp_cleaned_data, term):\n                N += 1\n    return 1 + (math.log(D / (1 + N)))\n\n\n"files_list" is a list of files stored in the current directory.\n\nAlso, please excuse my terrible coded "temp_cleaned_data", I will refactor it later so sorry about that. ( I know there are really simpler ways to do this using ".isalnum()" but I am trying to learn a bit of list comprehension and functional programming too so I try out different stuff and piece it together to see how it works).\n\nAnyways, I\'d really appreciate any sort of help in figuring this out, thank you. The formula for TF-IDF is:\n\n\n  term-frequency(term) x idf(term) \n\n\nNote: The DataFrame screenshot I provides is only a part of the complete dataframe. Also, the book-titles may vary and the amount of books is not just 6 and can be N in number. Thus, I intend to code the function which can factor in the above mentioned conditions.\n'
'I have a CSV file that has time stamps and the information on whether that means start (I) or end(F).\nI would like to calculate the duration between the start and end times.\n\nI\'m trying to load it to pandas, groupby the reference, \'acao\' (which states wether it\'s a start or end stamp), unstack it and then use a fillna() to be able to get a table from which I can calculate the duration.\n\ncode I am using:\n\ndata = pd.read_csv(file_path, parse_dates=[\'time_stamp\'])\ny = data.sort_values([\'referencia\',\'time_stamp\'])\n        y = y.set_index([\'referencia\',\'acao\'], append=True).time_stamp.unstack(\'acao\')\n        y = y[[\'I\',\'F\']]\n\n\nThe expected result is the following (I hope I was able to format the tables correctly):\n\n+------------+----------------------------+----------------------------+\n| referencia |             I              |             F              |\n+------------+----------------------------+----------------------------+\n|        111 | 2019-10-23 23:26:18.325750 |                            |\n|        111 |                            | 2019-10-23 23:42:45.719985 |\n|        123 | 2019-10-23 22:38:10.434322 |                            |\n|        123 |                            | 2019-10-23 22:38:19.986666 |\n|        123 | 2019-10-23 22:39:08.760218 |                            |\n|        123 |                            | 2019-10-23 22:39:42.762875 |\n|        123 | 2019-10-23 22:40:02.301749 |                            |\n|        123 |                            | 2019-10-23 22:40:24.000795 |\n|        123 | 2019-10-23 23:24:59.687386 |                            |\n|        123 |                            | 2019-10-26 11:48:07.831072 |\n|        133 | 2019-10-23 22:42:14.712779 |                            |\n|        133 |                            | 2019-10-23 22:42:20.159414 |\n|        156 | 2019-10-26 11:47:13.848750 |                            |\n|        156 |                            | 2019-10-26 11:47:21.289268 |\n|        199 | 2019-10-23 22:44:30.502311 |                            |\n|        199 |                            | 2019-10-23 22:44:38.154283 |\n|        555 | 2019-10-23 23:34:35.322073 |                            |\n|        555 |                            | 2019-10-26 11:48:13.330636 |\n+------------+----------------------------+----------------------------+\n\n\nBut unfortunately, all I can get is:\n\n+------------+----------------------------+----------------------------+\n| referencia |             I              |             F              |\n+------------+----------------------------+----------------------------+\n|        123 | 2019-10-23 22:38:10.434322 |                            |\n|        123 |                            | 2019-10-23 22:38:19.986666 |\n|        123 | 2019-10-23 22:39:08.760218 |                            |\n|        123 |                            | 2019-10-23 22:39:42.762875 |\n|        123 | 2019-10-23 22:40:02.301749 |                            |\n|        123 |                            | 2019-10-23 22:40:24.000795 |\n|        133 | 2019-10-23 22:42:14.712779 |                            |\n|        133 |                            | 2019-10-23 22:42:20.159414 |\n|        199 | 2019-10-23 22:44:30.502311 |                            |\n|        199 |                            | 2019-10-23 22:44:38.154283 |\n|        123 | 2019-10-23 23:24:59.687386 |                            |\n|        111 | 2019-10-23 23:26:18.325750 |                            |\n|        555 | 2019-10-23 23:34:35.322073 |                            |\n|        111 |                            | 2019-10-23 23:42:45.719985 |\n|        156 | 2019-10-26 11:47:13.848750 |                            |\n|        156 |                            | 2019-10-26 11:47:21.289268 |\n|        123 |                            | 2019-10-26 11:48:07.831072 |\n|        555 |                            | 2019-10-26 11:48:13.330636 |\n+------------+----------------------------+----------------------------+\n\n\nI cannot groupby because it gives me the following error when I try it:\n"ValueError: Index contains duplicate entries, cannot reshape"\n\nI forgot to attach the source data, it is not below:\n\nutilizador,referencia,time_stamp,acao\nAG,123,2019-10-23 22:38:10.434322,I\nAG,123,2019-10-23 22:38:19.986666,F\nAG,123,2019-10-23 22:39:08.760218,I\nAG,123,2019-10-23 22:39:42.762875,F\nAG,123,2019-10-23 22:40:02.301749,I\nAG,123,2019-10-23 22:40:24.000795,F\nAG,133,2019-10-23 22:42:14.712779,I\nAG,133,2019-10-23 22:42:20.159414,F\nAG,199,2019-10-23 22:44:30.502311,I\nAG,199,2019-10-23 22:44:38.154283,F\nAG,123,2019-10-23 23:24:59.687386,I\nAG,111,2019-10-23 23:26:18.325750,I\nAG,555,2019-10-23 23:34:35.322073,I\nAG,111,2019-10-23 23:42:45.719985,F\nAA,156,2019-10-26 11:47:13.848750,I\nAG,156,2019-10-26 11:47:21.289268,F\nAG,123,2019-10-26 11:48:07.831072,F\nAG,555,2019-10-26 11:48:13.330636,F\n\n'
"import pandas as pd\nimport numpy as np\n\nindex = [('California', 2000), ('California', 2010),\n         ('New York', 2000), ('New York', 2010),\n         ('Texas', 2000), ('Texas', 2010)]\n\npopulations = [33871648, 37253956,\n                18976457, 19378102,\n                20851820, 25145561]\n\npop = pd.Series(populations, index=index)\nindex = pd.MultiIndex.from_tuples(index)\npop = pop.reindex(index)\n\nd1 = pop.loc['California':'New York']\nd2 = pop['California':'New York']\n\nprint(d1)\nprint(d2)\nprint(d1 is d2)\n\n\n'''I don't understand d1 and d2 showing the same result, but why they are different? the last result shows 'False''''\n"
"I have implemented a multiple linear regression class by hand and right now I am working on the metrics methods. I have tried to calculate the AIC and BIC scores manually, but the results weren't correct. The reason is that I didn't use the Log Likelihood function but went with the SSE approach. Could you please suggest me how to change the implementation to compute the full AIC and BIC scores?\n\nHere is how my method looks like right now:\n\n  def AIC_BIC(self, actual = None, pred = None):\n    if actual is None:\n      actual = self.response\n    if pred is None:\n      pred = self.response_pred\n\n    n = len(actual)\n    k = self.num_features\n\n    residual = np.subtract(pred, actual)\n    RSS = np.sum(np.power(residual, 2))\n\n    AIC = n * np.log(RSS / n) + 2 * k\n    BIC = n * np.log(RSS / n) + k * np.log(n)\n\n    return (AIC, BIC)\n\n\nAnd please try to give me a manual approach and not a library call. Thanks!\n"
'I have pandas that looks at follows:\n\ndf = id       A_value       D_value\n      1        50            60\n      2        33            45\n\n\nAnd I want to split it to:\n\ndf = id       value       value_type\n      1        50           A\n      1        60           D \n      2        33           A\n      2        45           D\n\n\nwhat is the best way to do so?\n\nThanks!\n'
'Here\'s my sample csv file upon printing:\n\n  1556890689.332073;16384;340;48188.23529411765;[1618.6294555664062   1619.0826416015625  ...   1620.391845703125.584   1620.1904296874998]\n0  1556890689.746113;16384;341;48046.92082111437;...                         1619.082642  ...             1620.442200   1619.6868896484373]\n1  1556890690.1393259;16384;340;48188.23529411765...                         1620.643616  ...             1619.888306       1619.384765625]\n2  1556890690.536072;16384;340;48188.23529411765;...                         1619.737244  ...             1620.089722    1620.391845703125]\n\n\nI try running this:\n\nimport pandas as pd\n\ndata = pd.read_csv("project_fan.csv", error_bad_lines = False) \nprint(data.head())\ndata.columns[\'unixTime\', \'sampleAmount\',\'Time\',\'samplingRate\', \'Data\']\n\n\nand get these errors:\n\nTraceback (most recent call last):\n  File "new.py", line 6, in &lt;module&gt;\n    data.columns[\'unixTime\', \'sampleAmount\',\'Time\',\'samplingRate\', \'Data\']\n  File "/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/base.py", line 3969, in __getitem__\n    result = getitem(key)\nIndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n\n'
'I have a table like this:\n\nDate          Student    Average(for that date)\n17 Jan 2020   Alex          40  \n18 Jan 2020   Alex          50\n19 Jan 2020   Alex          80\n20 Jan 2020   Alex          70\n17 Jan 2020   Jeff          10\n18 Jan 2020   Jeff          50\n19 Jan 2020   Jeff          80\n20 Jan 2020   Jeff          60\n\n\nI want to add a column for high and low. The logic for that column should be that it is high as long as the average score for a student for today`s date is greater than the value &lt; 90% of previous days score.\nLike my comparison would look something like this:\n\navg(score)(for current date) &lt; ( avg(score)(for previous day) - (90% * avg(score)(for previous day) /100)\n\n\nI can`t figure how to incorporate the date part in my formula.That it compares averages from current day to the average of the previous date.\n\nI am working with Pandas so i was wondering if there is a way in it to incorporate this.\n'
"I have a data script application that uses dask to go trough a database and produce some intermediates, it than combines these intermediates to produce the result. Now I would like to both write out the intermediates and the result efficiently but as you can see below I have only found a very inefficient way where you compute the intermediates more than ones. \n\nimport dask.bag as db\nfrom other_functions import *\n\ninput = db.read_text(file1)\nprocessing_parameter = parse_mapping_parameters(file2)\n\nintermediates = []\nfor p in mapping_parameter:\n    intermediate = input.map(lambda x: process(x, p))\n    intermediates.append(intermediate)\n\nproducts = intermediates.pop(0)\n\nfor intermediate in intermediates:\n    products = product.products(i)\n\nresult = products.map(calc_result)\n\nfor i, intermediate in enumerate(intermediates):\n    intermediate.to_textfiles(f'./data/intermediate_{i}.*.txt')\n\nresult.to_textfiles(f'./data/result.*.txt')\n\n\nThe alternative I see is to write the intermediates to file and then use a separate script to read them into memory again and generate the results but IO wise that also feels inefficient. Is there a better way to do this in dask?\n"
'Am, trying to train a model in my training set using scikit-learn,  but getting this error:\n\n ValueError: Expected 2D array, got 1D array instead: array=[90.  4.].\n Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n\nstep 1: split x and y into training and testing set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 4)\n\n\nchecking the shape of the new splited x value(training and testing)\n\nX_train = X_train.shape\nX_test = X_test.shape\nprint(X_train)\nprint(X_test)\n\n\nchecking the shape of the new splited y value(training and testing)\n\ny_train = y_train.shape\ny_test = y_test.shape\nprint(y_train)\nprint(y_test)\n\n\nstep 2: Training our model on the training set(using logistics regression)\n\nlogR = LogisticRegression()\nlogR = logR.fit(X_train, y_train)\n\n\nRunning this code I got the error\n'
"I'm trying to use plot_confusion_matrix from sklearn 0.22\n\nBut my numbers are in format 1.3e+0.2\n\nfrom sklearn.metrics import plot_confusion_matrix\nplt.figure(figsize=(100, 230))\nplot_confusion_matrix(clf, X, y, cmap=plt.cm.Blues)\nplt.xlabel('True label')\nplt.ylabel('Predicted label')\n\n\n\n\nHow can i fix this?\n"
'Here\'s some code where that will generate some random data, and chart plus lines representing 30th &amp; 90th percentiles.\n\nimport pandas as pd \nimport numpy as np \nfrom numpy.random import randint \nimport matplotlib.pyplot as plt                                                                                                                                                       \n%matplotlib inline\n\nnp.random.seed(10)  # added for reproductibility                                                                                                                                                                 \n\nrng = pd.date_range(\'10/9/2018 00:00\', periods=10, freq=\'1H\') \ndf = pd.DataFrame({\'Random_Number\':randint(1, 100, 10)}, index=rng)                                                                                                                   \ndf.plot()    \n\nplt.axhline(df.quantile(0.3)[0], linestyle="--", color="g")                                                                                                                                                    \nplt.axhline(df.quantile(0.90)[0], linestyle="--", color="r")                                                                                                                                                    \n\nplt.show()\n\n\nOutputs: (minus the highlighted part of the chart)\n\n\n\nIm trying to figure out if its possible to calculate the time in the data it takes to reach (highlighted yellow) from green to the red line.\n\nI can manually enter in the data:\n\nminStart = df.loc[df[\'Random_Number\'] &lt; 18].index[0]\n\nmaxStart = df.loc[df[\'Random_Number\'] &gt; 90].index[0]\n\nhours = maxStart - minStart\nhours\n\n\nWhich will output:\n\nTimedelta(\'0 days 05:00:00\')\n\n\nBut if I attempt to use:\n\nminStart = df.loc[df[\'Random_Number\'] &lt; df.quantile(0.3)].index[0]\n\nmaxStart = df.loc[df[\'Random_Number\'] &gt; df.quantile(0.90)].index[0]\n\nhours = maxStart - minStart\nhours\n\n\nThis will throw an ValueError: Can only compare identically-labeled Series objects\n\nWould there be a better method to madness? Ideally it would be nice to create some sort of an algorithm that can calculate delta Time to it takes to go from 30th - 90th percentile and then delta back from 90th - 30th.. But I may have to put some thought towards how that could be accomplished..\n'
'i am trying to save a large dataframe to a database :\n\nchunk.to_sql("xxxx".lower(),engine,if_exists=\'replace\', dtype = {\'model_id\': String(80), \'EFF_DT\': DateTime(), \'FACTOR_TYP\':  String(80)\n        , \'FACTOR_ID\': String(40), \'RETURN\': String(40) })\n\n\nit gives me error :\nMODEL_ID (VARCHAR(80)) not a string\n\nas it is considering it as np.dtype. \nPlease help\n'
"\n  I have a numpy array of words that I want to delete from strings in a Pandas dataframe.\n  For example: If there a word 'the' in that array and there's a string in a column 'The cat'. So it should become ' cat'. I don't want to delete the whole string, just that words.\n\n\n# This will iterate that numpy array\ndef iterate():\n    for x in range(0, 52):\n        for y in range(0, 8):\n              return (np_array[x,y])\n\n# The code below drops that row/record\n\nfiltered = df[~df.content.str.contains(iterate())]\n\n\n\n\n  Help will be highly appreciated.\n\n\nSample data:\nnumpy array = [a, about, and, across, after, afterwards, in, on, as]\n\nOne sample cell:\ndf['content'] = Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!\n\nSample Output:\nBe sure to tune watch Donald Trump Late Night with David Letterman he presents the Top Ten List tonight!\n"
"reshape the data frame from a m rows x n columns to a m x n rows single column.\n\nYear   Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov  Dec\n2000  12.7  13.8  13.3  12.6  12.8  12.3  13.4    14    13  12.8    13 13.2\n2001  13.8  13.7  13.8  13.9  13.4  14.2  14.4    15.6  15.2    16  15.9    17\n2002  16.5    16  16.6  16.7  16.6  16.7  16.8    17    16.3  15.1  17.1  16.9\n\n\nto\n\nYear Month Value\n2000 Jan   12.7\n2000 Feb   13.8\n2000 Mar   13.3\n\n\nthen easy combine 'Year'+'Month' columns into a datefield and plot the data column.\n\nI'm rusty as heck on this. Reading the various melt, reshape, stack options is frustratingly slow.\n"
'I\'m trying to handle missing values in my dataset. \nI\'m using pandas in Python 3.\nI have a column \'name\' that should only contain names of people, but it also contains random numbers. How can I remove them or change them to NaN, so later on I can drop them or fill them with new names. \nThis is how I handled the first missing values:\n\ndf_test["name"] = df_test[\'name\'].astype(str)\ndf_test[\'name\'].replace(\'-inf\', np.nan, inplace=True)\ndf_test[\'name\'].replace(\'0\', np.nan, inplace=True)\ndf_test\n\n\nOutput:\n\n     name    \n1    NaN         \n2    NaN     \n3    29014\n...\n10   21893\n11   Amber Rose\n\n\nSo the first two values were \'-inf\' and \'0\' I replaced them with NaN but how can I replace random numbers in a string column? Do I have to do this manually? \n'
"Let's suppose I have a DataFrame :\n\nimport pandas as pd\n\ndict1 = {'Name':['John', 'Sean', 'Philip', 'John', 'Sean', 'Philip'],\n     'c_1':['a','b','c','d','f','g'],\n     'c_2':[1,2,3,4,2,3],\n     'c_3':[2,3,4,2,1,1]} \n\ndf = pd.DataFrame(dict1)\n\n\nAnd output:\n\n     Name c_1  c_2  c_3\n0    John   a    1    2\n1    Sean   b    2    3\n2  Philip   c    3    4\n3    John   d    4    2\n4    Sean   f    2    1\n5  Philip   g    3    1\n\n\nRequired output:\n\n     Name c_1  c_2  c_3\n0    John   a    1    2\n1    Sean   f    2    1\n2  Philip   g    3    1\n\n\nI need to find a row with minimum values c_1 and c_2 for each name.\nc_1 has high priority (it means that (c_1 = 1) &amp; (c_2 = 2) is more important (and required) than (c_1 = 2) &amp; (c_2 = 1)).\nI tried to use loops, but it was unsuccessfully, because I have huge DF and my PC would do that for long time.\n\nHow can I do it in the most simplest way?\n\nThank you so much for answering!\n"
"Here, I'm trying to create a new column 'new' from the sum of two columns using .loc, but I'm unable to create it, it throws an error saying 'W' in invalid key.  \n\n\n  This works\n\n\ndf['new'] = df['W'] + df['Y']\n\n\n\n  This is not working\n\n\ndf = pd.DataFrame([[1.0,5.0,1],[2,np.NaN,2],[np.NaN,np.NaN,3]], columns = ['W','Y','Z'])\ndf['new'] = df.loc['W'] + df.loc['Y'] \n\n"
"I have a dictionary of arrays (40 entries, shape of each array = (5001,)) and I want to be able to iterate through and create subplots in a 5 * 8 grid. So far I can only work out how to make a 40*1 grid: \n\nfig, axes = plt.subplots(40,sharex=True,sharey=True,figsize=(3,30))\nfor i, (key, value) in enumerate(new_dict.items()):\n    print(i, key, value)\n    axes[i].plot(value)\n    axes[i].set(title=key.upper(), xlabel='ns')\nplt.show()\n\n\n40*1 grid\n\nSomething like this will put the last plot in each graph of the 5*8 grid: \n\nfig, axes = plt.subplots(ncols=5,nrows=8,sharex=True,sharey=True,figsize=(10,30))\naxes = axes.flatten()\nfor i, ax in enumerate(axes.flatten()):\n    for a, (key, value) in enumerate(new_dict.items()):\n        print(a, key, value)\n    ax.plot(value)\n    ax.set(title=key.upper(), xlabel='ns')\nplt.show()\n\n\nSingle (final) plot in each graph\n\nAnd switching the for loops puts all the plots overlayed in each graph of the 5*8 grid:\n\nfig, axes = plt.subplots(ncols=5,nrows=8,sharex=True,sharey=True,figsize=(10,30))\nfor i, ax in enumerate(axes.flatten()):\n    for a, (key, value) in enumerate(new_dict.items()):   \n        print(a, key, value)\n        ax.plot(value)\n        ax.set(title=key.upper(), xlabel='ns')\nplt.show()\n\n\nAll plots in each graph\n\nI cannot for the life of me work out how to put a different plot in each graph. Any help would be greatly appreciated! I feel like I'm missing something really obvious here. Many thanks :-) \n\nEDIT: I've realised that the position of the for loops doesn't matter and it is the tabbing of the ax.plot... that changes whether the last plot fills the graph or all of the plots. \n"
"This is one column I have extracted from a large pandas dataframe df:\n\ndist=df['How far is your place of education/work from your residence (in km) ?']\ndist\n\n0       6-7\n1       15+\n2     14-15\n3       15+\n4       15+\n      ...  \n71      1-2\n72      0-1\n73      4-5\n74      0-1\n75      0-1\n\n\nI'd like to assign 6-7 as 6.5, 1-2 as 1.5 and so on. (15+ to 15) so that I can proceed to do calculate the mean which I need for my project. So this is how I want it as:\n\n0           6.5\n1           15\n2           14.5\n3           15\n4           15\n.\n.\n.\n71          1.5\n72          0.5\n\n\nHow do I do this?\n"
"I'm trying to replace the values in each cell with 1 if the value is equal to highest value in other columns in the row. \n\nThis is the data i have\n\nThis is where i want to get to\n\nThis is what i tried so far:\n\ndf_ref['max'] = df_ref.max(axis=1)\ndf_ref['col1'] = df_ref.col1.apply(lambda x:1 if (x==df_ref['max']) else 0)\n\n\nThanks in advance\n"
'I have a list of lists that I need to breakdown so that each list in the list is assigned to a separate variable. Further, the code should be able to take into account the addition of new lists to the list. \n\n#Eg\nlist = [[1,3,5],[2,5,6], [3,5,7]]\n# Code needs to make it so each element is assigned a variable. For example:\nl0 = [1,3,5]\nl1 = [2,5,6]\nl2 = [3,5,7]\n\n\nHere is dummy starter code:\n\nimport random\n\nrandom_pop1 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop2 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop3 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop4 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop5 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop6 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop7 = [random.randint(1000,9000) for i in range(10)]\nrandom_pop8 = [random.randint(1000,9000) for i in range(10)]\n\nrandom_pop_list = [random_pop1,random_pop2,random_pop3,random_pop4, random_pop5, random_pop6, \n                   random_pop7, random_pop8]\n\n#Each element of the list should be assigned to a variable (e.g.: r1 = random_pop1, r2 = random_pop2, r3= random_pop3 ... )\n\n'
"I want to clean a pandas data frame using a dictionary of regular expressions representing allowed data entry formats. \n\nI'm trying to iterate over the input data frame so to check every row against the allowed data entry format for a given column.  \n\nIf an entry doesn't meet the format allowed for the column, I want to replace it with NaN (see desired output below). \n\nMy current code gives me an error message: 'DataFrame' object has no attribute 'col'.\n\nMy MWE features two representative regular expressions, but for my actual data set I've got ~40. \n\nThanks for any help!\n\n# Packages \nimport pandas as pd\nimport re\nimport numpy as np \n\n\n# Input data frame \ndata = {'score': [71,72,55,'a'],\n        'bet': [0.260,0.380,'0.8dd',0.260]\n        }\ndf1 = pd.DataFrame(data, columns = ['score', 'bet'])\n\n\n# Input dictionary \ndict1 = {'score':'^\\d+$', \n         'bet': '^\\d[\\.]\\d+$'}\n\n\n# Cleaning function  \ndef cleaner(df, dict):\n    for col in df.columns:\n        if col in dict: \n            for row in df.col:\n                if re.match(dict[col], str(row)):\n                    row = row \n                else:\n                    row = np.nan\n    return(df)\n\n\ncleaned_df = cleaner(df1, dict1)\n\n\n# ERROR MESSAGE \n# 'DataFrame' object has no attribute 'col'\n\n\n# Desired output \ngoal_data = {'score': [71,72,55, np.nan],\n        'bet': [0.260,0.380, np.nan, 0.260]\n        }\ngoal_df = pd.DataFrame(goal_data, columns = ['score', 'bet'])\n\n"
"I'm learning about Multiindex, groupBy, turples, reading similar questions on Stak Overflow, on Google search and Youtube tutorials. Came into a complex point.\n\nHow can I group a number of unknown columns in groups of two. This is what I have. Header and one row:\n\npatterns    responses   patterns    responses   patterns   ...  \nhello       hi          Where?      here        When?      ...\n\n\nAnd I'm looking to create a header over the header that groups in two columns like:\n\na                       a                       a\npatterns    responses   patterns    responses   patterns   ...  \nhello       hi          Where?      here        When?      ...\n\n\nAppreciate your time!\n"
"In case of text analysis, when I apply fit() method, what exactly happens? And what does transform() do on the data?\nI can understand it for numerical data type but unable to visualize it for text data.\nI have a text array\nsents_processed[0:5]\n['so there is no way for me plug in here in us unless go by converter',\n 'good case excellent value',\n 'great for jawbone',\n 'tied charger for conversations lasting more than minutes major problems',\n 'mic is great']\n\nNow to vectorize it, I use CountVectorizer class:\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(analyzer= 'word', tokenizer= None, preprocessor= None, stop_words= None, max_features= 4500)\ndata_features = vectorizer.fit_transform(sents_processed)\nprint(data_features.toarray())\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n\nI know that I will get vectors of 4500 length. However, I am unable to visualize what exactly fit method would have done behind the scene and how exactly data would have been then transformed by tranform function? Specially that given data is text type.\n"
"Given below is some part of data, I want to export the data from output box in Jupyter Notebook to the excel file. The data is quite huge like 10k lines.\n17:38:00\n17:38:01\n17:38:02\n17:38:03\n17:38:04\n.\n.\n.\nFollowing is my code:-\nenter code here\nimport pandas as pd\nimport numpy as np\nload_var=pd.read_csv(r'path')\n\n# Select the dataframe\ncol_var=load_var['End Seconds']\n\n# Converting the dataframe to array\n\na=col_var.values.tolist()\n\n\n# define the function for time conversion\ndef convert(seconds): \nseconds = seconds % (24 * 3600) \nhour = seconds // 3600\nseconds %= 3600\nminutes = seconds // 60\nseconds %= 60\n\nreturn &quot;%d:%02d:%02d&quot; % (hour, minutes, seconds) \n\n\nfor n in a:\nprint(convert(n))\n\n1)Please suggest the addition to be made in the code.\nThank you.\n"
"I'm new to Python. Please help me solve the problem with graph construction. I have a database with the attribute &quot;Source&quot;, &quot;Interlocutor&quot; and &quot;Frequency&quot;.\nAn example of three lines:\n\nI need to build a graph based on the Source-Interlocutor, but the frequency is also taken into account.\nLike this:\n\nMy code:\ndic_values={Source:[24120.0,24120.0,24120.0], Interlocutor:[34,34,34],Frequency:[446625000, 442475000, 445300000]\nsession_graph=pd.DataFrame(dic_values)\nfriquency=session_graph['Frequency'].unique()\nplt.figure(figsize=(10,10))\nfor i in range(len(friquency)):\n   df_friq=session_subset[session_subset['Frequency']==friquency[i]]\n   G_frique=nx.from_pandas_edgelist(df_friq,source='Source',target='Interlocutor')\n   pos = nx.spring_layout(G_frique)\n   nx.draw_networkx_nodes(G_frique, pos, cmap=plt.get_cmap('jet'), node_size = 20)\n   nx.draw_networkx_edges(G_frique, pos,  arrows=True)\n   nx.draw_networkx_labels(G_frique, pos)\nplt.show()  \n\nAnd I have like this:\n\n"
"I create a pivot table and I want create a bar graph. This is my pivot_table:\n\nI don't know how to stract the values of the column 1970 and use this information to make a bar graph.\nThanks!!\n"
"I have a problem statement as follows:\nIn each examination centre, exam is to be organised in two shifts batch I &amp; batch II(reporting time 9:00 AM &amp; 2 PM). Exam can be conducted at any day in a district during December 1-30, 2020 depending upon the number of candidates in a district. Note in each district only one examination centre is possible and in one shift maximum 20 students can appear. Based on the information mentioned above complete the examination database by allocating:\n\nRollno: Roll number of the candidate will start from NL2000001 onwards(eg: NL2000001, NL2000002, NL2000003……)\ncent_allot: allocate centre by putting examination city code\ncent_add: put NL &quot;District Name&quot; as center address in each location (for eg if district name is ADI then centre add is NL ADI)\nexamDate: Allocate any exam date between December 1,2020 to December 30, 2020 keeping minimum no of examination days and not violating any conditions mentioned above\nbatch: allocate batch I or II ensuring all the conditions mentioned above\nrep_time: for batch I reporting time is 9 AM and for batch II reporting time is 2 PM.\n\nAs per the above description, I need to make a table which satisfies the above conditions. I have already made the Rollno, cent_allot and cent_add columns, but I am struggling in making the examDate column, since it should have same date for every 40 values of district.\nHere is the list of the districts and their frequency of occurrences:\nDist    Count\nWGL     299\nMAHB    289\nKUN     249\nGUN     198\nKARN    196\nKRS     171\nCTT     169\nVIZ     150\nPRA     145\nNALG    130\nMED     128\nADI     123\nKPM     119\nTRI     107\nANA     107\nKHAM    85\nNEL     85\nVIZI    84\nEGOD    84\nSOA     84\nSIR     80\nNIZA    73\nPUD     70\nKRK     69\nWGOD    56\n\nHere is the first 25 rows of the dataframe:\nRollno     cent_allot   cent_add    examDate    batch   rep_time\nNL2000001   WGL          NL WGL       NaN        NaN    NaN\nNL2000002   WGL          NL WGL       NaN        NaN    NaN\nNL2000003   WGL          NL WGL       NaN        NaN    NaN\nNL2000004   KUN          NL KUN       NaN        NaN    NaN\nNL2000005   KUN          NL KUN       NaN        NaN    NaN\nNL2000006   KUN          NL KUN       NaN        NaN    NaN\nNL2000007   GUN          NL GUN       NaN        NaN    NaN\nNL2000008   GUN          NL GUN       NaN        NaN    NaN\nNL2000009   GUN          NL GUN       NaN        NaN    NaN\nNL2000010   GUN          NL GUN       NaN        NaN    NaN\nNL2000011   VIZ          NL VIZ       NaN        NaN    NaN\nNL2000012   VIZ          NL VIZ       NaN        NaN    NaN\nNL2000013   VIZ          NL VIZ       NaN        NaN    NaN\nNL2000014   VIZ          NL VIZ       NaN        NaN    NaN\nNL2000015   MAHB         NL MAHB      NaN        NaN    NaN\nNL2000016   MAHB         NL MAHB      NaN        NaN    NaN\nNL2000017   MAHB         NL MAHB      NaN        NaN    NaN\nNL2000018   WGOD         NL WGOD      NaN        NaN    NaN\nNL2000019   WGOD         NL WGOD      NaN        NaN    NaN\nNL2000020   WGOD         NL WGOD      NaN        NaN    NaN\nNL2000021   WGOD         NL WGOD      NaN        NaN    NaN\nNL2000022   EGOD         NL EGOD      NaN        NaN    NaN\nNL2000023   EGOD         NL EGOD      NaN        NaN    NaN\nNL2000024   EGOD         NL EGOD      NaN        NaN    NaN\nNL2000025   EGOD         NL EGOD      NaN        NaN    NaN\n\nThe last 3 columns are all NaNs as these three columns are yet to be made.\nLet's take WGL for example. As per the above description, maximum 20 candidates can be allowed per shift per district, which implies that the same date is to be allotted 40 times to each district, and the same batch and the same reporting time needs to be allotted 20 times to each districs.\nDoes anyone have any idea how to do it?\n"
"when ı try to create a 2-D array using ndarray if I dont specify ddtype value it gives me error but when trying to create a 3-D one it doesn't why, I mean is there a rule like that?\nimport numpy as np\narray=np.array([[1,1,1,1,1,1],[1,1,2,1,1,1,1]])\n\n\nprint(array)\nprint(array.ravel())\n\nI receive the following error:\n\nc:/Users/fazil/Desktop/yeni metin belgesi.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray array=np.array([[1,1,1,1,1,1],[1,1,2,1,1,1,1]]) [list([1, 1, 1, 1, 1, 1]) list([1, 1, 2, 1, 1, 1, 1])] [list([1, 1, 1, 1, 1, 1]) list([1, 1, 2, 1, 1, 1, 1])]\n\n"
"I want to make the database as my all data is in the form of CSV. having 160 column and 15 rows. as the column are more so to write the name manually is very hard I want to do it like get the column names from csv file and store it in a variable I did this. My column name are now in a variable. but I'm not able to assign these value to database column.\ndef parseCSV(filePath):\n      # CVS Column Names\n      data = pd.read_csv(filePath)\n      column = data.columns\n      # Use Pandas to parse the CSV file\n      csvData = pd.read_csv(filePath,names=column, header=None)\n      # Loop through the Rows\n    #   for i,row in csvData.iterrows():\n    #          sql = &quot;INSERT INTO addresses (first_name, last_name, address, street, state, zip) VALUES (%s, %s, %s, %s, %s, %s)&quot;\n    #          value = (row['first_name'],row['last_name'],row['address'],row['street'],row['state'],str(row['zip']))\n    #          mycursor.execute(sql, value, if_exists='append')\n    #          mydb.commit()\n    #          print(i,row['first_name'],row['last_name'],row['address'],row['street'],row['state'],row['zip'])\n\nI comment those line which is hard coded. I want this column name come from CSV not to be added manually.\n"
"I am trying to write a function that does something similar to pandas's groupby().ngroups() function. The difference is that I want each subgroup count to restart at 0. So given the following data:\n| EVENT_1 | EVENT_2 |\n| ------- | ------- |\n|       0 |       3 | \n|       0 |       3 |\n|       0 |       3 |\n|       0 |       5 |\n|       0 |       5 |\n|       0 |       5 |\n|       0 |       9 |\n|       0 |       9 |\n|       0 |       9 |\n|       1 |       6 |\n|       1 |       6 |\n\nI want\n| EVENT_1 | EVENT_2 | EVENT_2A |\n| ------- | ------- | -------- |\n|       0 |       3 |        0 |\n|       0 |       3 |        0 |\n|       0 |       3 |        0 |\n|       0 |       5 |        1 |\n|       0 |       5 |        1 |\n|       0 |       5 |        1 |\n|       0 |       9 |        2 |\n|       0 |       9 |        2 |\n|       1 |       6 |        0 |\n|       1 |       6 |        0 |\n\nThe best way I can think of implementing this is by doing a groupby() on EVENT_1, within each group getting the unique values of EVENT_2, and then setting EVENT_2A as the index of the unique value. For example, in the EVENT_1 == 0 group, the unique values are [3, 5, 9] and then we set EVENT_2A to the index in the unique values list for the corresponding value in EVENT_2.\nThe code I have written is here. Note that EVENT_2 is always sorted with respect to EVENT_1 so finding the unique values like this in O(n) should work.\nimport cudf\nfrom numba import cuda\nimport numpy as np\n\ndef count(EVENT_2, EVENT_2A):\n    # Get unique values of EVENT_2\n    uq = [EVENT_2[0]] + [x for i, x in enumerate(EVENT_2) if i &gt; 0 and EVENT_2[i-1] != x]\n\n    for i in range(cuda.threadIdx.x, len(EVENT_2), cuda.blockDim.x):\n        # Get corresponding index for each value. This can probably be sped up by mapping \n        # values to indices\n        for j, v in enumerate(uq):\n            if v == EVENT_2[i]:\n                EVENT_2A[i] = j\n                break\n\n\nif __name__ == &quot;__main__&quot;:\n    data = {\n        &quot;EVENT_1&quot;:[0,0,0,0,0,0,0,0,1,1],\n        &quot;EVENT_2&quot;:[3,3,3,5,5,5,9,9,6,6]\n    }\n    df = cudf.DataFrame(data)\n    results = df.groupby([&quot;EVENT_1&quot;], method=&quot;cudf&quot;).apply_grouped(\n        count, \n        incols=[&quot;EVENT_2&quot;], \n        outcols={&quot;EVENT_2A&quot;:np.int64}\n    )\n    print(results.sort_index())\n\nThe problem with this is that there seems to be an error regarding using lists in the user defined function count(). Numba says that its JIT nopython compiler can handle list comprehension and indeed when I use the function\nfrom numba import jit\n\n@jit(nopython=True)\ndef uq_sorted(my_list):\n    return [my_list[0]] + [x for i, x in enumerate(my_list) if i &gt; 0 and my_list[i-1] != x]\n\nit works, although with a deprecation warning.\nThe error I get using cudf is\nNo implementation of function Function(&lt;numba.cuda.compiler.DeviceFunctionTemplate object at 0x7f782a179fa0&gt;) found for signature:\n \n &gt;&gt;&gt; count &lt;CUDA device function&gt;(array(int64, 1d, C), array(int64, 1d, C))\n \nThere are 2 candidate implementations:\n  - Of which 2 did not match due to:\n  Overload in function 'count &lt;CUDA device function&gt;': File: ../../../../test.py: Line 11.\n    With argument(s): '(array(int64, 1d, C), array(int64, 1d, C))':\n   Rejected as the implementation raised a specific error:\n     TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n   Unknown attribute 'append' of type list(undefined)&lt;iv=None&gt;\n   \n   File &quot;test.py&quot;, line 12:\n   def count(EVENT_2, EVENT_2A):\n       uq = [EVENT_2[0]] + [x for i, x in enumerate(EVENT_2) if i &gt; 0 and EVENT_2[i-1] != x]\n       ^\n   \n   During: typing of get attribute at test.py (12)\n   \n   File &quot;test.py&quot;, line 12:\n   def count(EVENT_2, EVENT_2A):\n       uq = [EVENT_2[0]] + [x for i, x in enumerate(EVENT_2) if i &gt; 0 and EVENT_2[i-1] != x]\n       ^\n\n  raised from /project/conda_env/lib/python3.8/site-packages/numba/core/typeinfer.py:1071\n\nDuring: resolving callee type: Function(&lt;numba.cuda.compiler.DeviceFunctionTemplate object at 0x7f782a179fa0&gt;)\nDuring: typing of call at &lt;string&gt; (10)\n\n\nFile &quot;&lt;string&gt;&quot;, line 10:\n&lt;source missing, REPL/exec in use?&gt;\n\nIs this related to the deprecation warning from numba? Even when I set uq as a static list I still get an error. Any solutions to the list comprehension issue, or to my problem as a whole are welcome. Thanks.\n"
"I am new in Data Science and have a problem.\nThis is my train and test score\nTrain Score : 99.99319245627736\nTest Score  : 94.20448487131814\n\nand this is my actual price and predict\n            Actual_price  predict_price  Error\n4928          162000         165994  -3994.343750\n11272         31000          50525   -19525.128906\n7894          110000         117209  -7209.609375\n4382          59500          75478   -15978.164062\n345           500000         482369   17630.968750\n...             ...            ...           ...\n3348          42750          38110    4639.328125\n8993          74000          96511   -22511.226562\n8270          83750          74911    8838.210938\n2757          77500          89780   -12280.585938\n6538          95000          92607    2392.765625\n\ni have high scores but predicts is bad, where i am doing wrong. This is my code\ndata_train, data_test, label_train, label_test = train_test_split(X,Y, test_size=0.3,random_state=782)\nmodel = xgb.XGBRegressor(learning_rate=0.1,max_depth=14)\nmodel.fit(data_train, label_train)\nprint(model)\nprint(&quot;Train Score:&quot;+str(model.score(data_train, label_train) * 100))\nprint(&quot;Test Score:&quot;+str(model.score(data_test, label_test) * 100))\npre = model.predict(data_test)\nout = pd.DataFrame({'Actual_price': label_test, 'predict_price': pre.astype(int), 'Error': (label_test - pre)})\nprint(out)\n\n"
"I have a dataset for banknotes wavelet data of genuine and forged banknotes with 2 features which are:\n\nX axis: Variance of Wavelet Transformed image\nY axis: Skewness of Wavelet Transformed image\n\nI run on this dataset K-means to identify 2 clusters of the data which are basically genuine and forged banknotes.\nNow I have 3 questions:\n\nHow can I count the data points of each cluster?\nHow can I set a color of each data point based on it's cluster?\nHow do I know without another feature in the data if the datapoint is genuine or forged? I know the data set has a &quot;class&quot; which shows 1 and 2 for genuine and forged but can I identify this without the &quot;class&quot; feature?\n\nMy code:\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.patches as patches\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.patches as patches\n\ndata = pd.read_csv('Banknote-authentication-dataset-all.csv')\n\nV1 = data['V1']\nV2 = data['V2']\nbn_class = data['Class']\n\n\nV1_min = np.min(V1)\nV1_max = np.max(V1)\n\nV2_min = np.min(V2)\nV2_max = np.max(V2)\n\nnormed_V1 = (V1 - V1_min)/(V1_max - V1_min)\nnormed_V2 = (V2 - V2_min)/(V2_max - V2_min)\n\nV1_mean = normed_V1.mean()\nV2_mean = normed_V2.mean()\n\nV1_std_dev = np.std(normed_V1)\nV2_std_dev = np.std(normed_V2)\n\nellipse = patches.Ellipse([V1_mean, V2_mean], V1_std_dev*2, V2_std_dev*2, alpha=0.4)\n\nV1_V2 = np.column_stack((normed_V1, normed_V2))\n\nkm_res = KMeans(n_clusters=2).fit(V1_V2)\nclusters = km_res.cluster_centers_\n\nplt.xlabel('Variance of Wavelet Transformed image')\nplt.ylabel('Skewness of Wavelet Transformed image')\nscatter = plt.scatter(normed_V1,normed_V2, s=10, c=bn_class, cmap='coolwarm')\n#plt.scatter(V1_std_dev, V2_std_dev,s=400, Alpha=0.5)\nplt.scatter(V1_mean, V2_mean, s=400, Alpha=0.8, c='lightblue')\nplt.scatter(clusters[:,0], clusters[:,1],s=3000,c='orange', Alpha=0.8)\nunique = list(set(bn_class))\n\nplt.text(1.1, 0, 'Kmeans cluster centers', bbox=dict(facecolor='orange'))\nplt.text(1.1, 0.11, 'Arithmetic Mean', bbox=dict(facecolor='lightblue'))\nplt.text(1.1, 0.33, 'Class 1 - Genuine Notes',color='white', bbox=dict(facecolor='blue'))\nplt.text(1.1, 0.22, 'Class 2 - Forged Notes', bbox=dict(facecolor='red'))\n\nplt.savefig('figure.png',bbox_inches='tight')\n\nplt.show()\n\nAppendix image for better visibility\n\n\n"
"Right now I'm trying to use statsmodels.formula.api's quantreg by putting in the formula and the dataframe by doing smf.quantreg('&lt;independant values&gt; ~ &lt;dependant values&gt;', df).fit(q=0.9) like in https://www.statology.org/quantile-regression-in-python/ However I cannot find how to structure the formula in a way to take multiple independant values, I have tried going with the structure of 'xValue1, xValue2 ~ yValue' however this causes me to get row mismatch error, making me think that statsmodels is assuming my xValues are rows and not columns.\n"
"I am new to list comprehension, and I'd like to do something with tuples. So this is the problem: \n\nGiven two vectors l1 and l2, I wish to combine them into tuples. Then I'd like to multiply them before summing all of them up. \n\nSo for example, if i have l1 = [1,2,3] and l2 = [4,5,6], I'd like to combine them with zip function into [(1,4),(2,5),(3,6)]. \n\nAfter this, I want to multiply and add 1 to the tuples. So it will be [(1*4)+1,(2*5)+1,(3*5)+1], giving [4,11,16]\n\nAfter that I want to sum the list up into 4+11+16 which should give 31. \n\nI've learnttuple(map(operator.add, a, b)) before which can add up the tupples. But since now I need to do one more calculation, I have no idea how to get started. It will be good if it can be done in a single line with list comprehension. Anyone got an idea?\n"
"I'm using itertools.product to come up with a combination of groups.I'm bad at explaining without examples, so here's code as is.\n\ngroup1=[1,2,3];group2=[4,5,6];group3=[7,8,9]\nlist(itertools.product(group1,group2,group3))\n\n\nThis gives me all combinations of 1 from each group. But how would I go about getting combinations of 2 numbers from group 1, 2 numbers from group 2, and 1 number from group 3?\n\nSo for example, I'd like the combination (1,2,5,6,9) to be in the list. Is it possible to customize this? itertools.product doesn't seem to be as flexible as I need it to be, and I've been unsuccessful in learning cartesian products enough to understand how to tweak the .product function.\n\nEDIT: I made the groups small to keep it simple, but each group will have hundreds of unique values.\n"
'the training set head looks this way\n\n    Session ID  Timestamp               Item ID     Price   Quantity\n0   420374  2014-04-06T18:44:58.314Z    214537888   12462   1\n1   420374  2014-04-06T18:44:58.325Z    214537850   10471   1\n2   281626  2014-04-06T09:40:13.032Z    214535653   1883    1\n3   420368  2014-04-04T06:13:28.848Z    214530572   6073    1\n4   420368  2014-04-04T06:13:28.858Z    214835025   2617    1\n\n\nSo I preprocess the data,make them normalized column by column and fit them to SGDClassifier.\n\nfrom sklearn import linear_model\nfrom sklearn import preprocessing as pp\n\nscaler = pp.MinMaxScaler()\ncolumns_list = list(train_data.columns)\ndel columns_list[-1]\ntrain_data[columns_list] = scaler.fit_transform(train_data[columns_list])\n\nclf = linear_model.SGDClassifier()\nclf.fit(train_data.iloc[:, :-2],train_data.iloc[:,-1])\n\n\nThen I want to predict with the model,like clf.predict() but the origin test set are supposed to be the following format.\n\n    Session ID  Timestamp               Item ID     Price   Quantity\n0   420374  2014-04-06T18:44:58.314Z    214537888   12462   1\n\n\nThen do I need to make them to normalize with the training set also?\n\nWhat test data do the model expecting?\n\nWhat prepocessing do the test data need?\n'
'I have been taking online classes at datacamp for Python data science, but when I take the same code that I use on there and run it on my computer (as opposed to their website), I am getting errors that I do not understand.  I am using Spyder and Python 3.6.\n\nThe goal of my code is to import a .csv file, extract two rows and two columns from the pandas dataframe and print out the results.  From there I can graph the data on a histogram, and then expand it.  But first, I have to get the basics to work.  The code I have been using is:\n\nimport pandas as pd\n\ndf = pd.read_csv(\'drinks.csv\')\ndf1 = df.loc[[\'USA\', \'Germany\'], [\'country\', \'beer_servings\']]\nprint(df1)\n\n\nThe error I get is:\n\nKeyError: "None of [[\'USA\', \'Germany\']] are in the [index]"\n\n\nIn case anyone wants to see the data I am using, the link I used to download it is: https://github.com/fivethirtyeight/data/blob/master/alcohol-consumption/drinks.csv\n\nEven if I go as simple as I possibly can and just extract a single row, I still get the same error (as seen below).  The same exact thing happens if I try to extract a single column.\n\nimport pandas as pd\n\ndf = pd.read_csv(\'drinks.csv\')\ndf1 = df.loc[[\'USA\']]\nprint(df1)\n\n\nThe error is: \n\nKeyError: "None of [[\'USA\']] are in the [index]"\n\n\nIs there something i\'m missing? \n\nhttps://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n\nThis is the website I was using to try and understand what I was doing wrong, but for the life of me I cannot figure out what I am missing.  I understand that this is probably a very trivial problem, but please if you have any advice I would love to hear it, thanks in advance for any help!\n'
'This is a YAML file. It contains a list of mappings from ticker to feature category.\n\nFollowing is the mapping of BANKNIFTY_O_C_0_10_W:\n\nindex: [ BANKNIFTY_O_C_0_09_W: books,BANKNIFTY_O_C_0_09_W: trends,BANKNIFTY_O_C_0_09_W: trades,BANKNIFTY_O_C_0_09_W: relations,BANKNIFTY_O_P_0_09_W: books,BANKNIFTY_O_P_0_09_W: trends,BANKNIFTY_O_P_0_09_W: trades,BANKNIFTY_O_P_0_09_W: negrelations,BANKNIFTY_O_C_0_10_W: books,BANKNIFTY_O_C_0_10_W: trends,BANKNIFTY_O_C_0_10_W: trades,BANKNIFTY_O_C_0_10_W: relations,BANKNIFTY_O_C_0_10_W: options_banknifty_weekly,BANKNIFTY_O_P_0_10_W: books,BANKNIFTY_O_P_0_10_W: trends,BANKNIFTY_O_P_0_10_W: trades,BANKNIFTY_O_P_0_10_W: negrelations,BANKNIFTY_F_0: books,BANKNIFTY_F_0: trends,BANKNIFTY_F_0: trades,BANKNIFTY_F_0: relations,NIFTY_F_0: books,NIFTY_F_0: trends,NIFTY_F_0: trades,NIFTY_F_0: relations ]\n\n\nI need the following output:\n\nindex: \n- BANKNIFTY_O_C_0_09_W: [books, trends, trades, relations]\n- BANKNIFTY_O_P_0_09_W: [books, trends, trades, negrelations]\n- BANKNIFTY_O_C_0_10_W: [books, trends, trades, relations, options_banknifty_weekly]\n- BANKNIFTY_O_P_0_09_W: [books, trends, trades, negrelations]\n- BANKNIFTY_F_0: [books, trends, trades, relations]\n- NIFTY_F_0: [books, trends, trades, relations]\n\n'
'I have a question about sklearn\'s linear regression model..\n\nI want to fit a Linear Regression using multiple features. Right now my X is a np.matrix and my Y is a np.array.\n\nMy X looks like this (printed)   X.shape -> (21, 3):\n\n[[ 8.68269590e-03 -2.83226292e-03  1.91826382e-01]\n [ 5.85903392e-03 -5.68809929e-03  2.21862758e-01]\n [ 2.90920454e-03 -1.24549359e-03  1.71619892e-01]\n [ 7.71491867e-04  1.74288704e-03  2.70315213e-03]\n [-2.44583484e-03 -4.73496469e-05 -1.25966777e-01]\n [-4.16023564e-03 -2.09644321e-03 -4.91722645e-02]\n [-3.22298365e-03 -3.55366669e-03 -1.67993225e-02]\n [-2.79712919e-03 -1.94070947e-03 -1.70873725e-01]\n [-2.76366703e-03 -4.98257755e-04 -2.52336769e-01]\n [-3.65153430e-03 -3.89128554e-03 -2.03762730e-01]\n [-6.07841812e-03 -8.89479214e-03 -1.54953118e-01]\n [-7.55809682e-03 -1.13395249e-02 -2.29260955e-01]\n [-7.46617379e-03 -5.70467322e-03 -2.01416145e-01]\n [-7.82348527e-03  3.58732358e-04 -1.47799157e-01]\n [-8.68110057e-03 -3.98060036e-05 -1.17156978e-01]\n [-9.13439934e-03  3.21795372e-03 -4.17922611e-02]\n [-6.64659597e-03  5.79326182e-03 -7.08715900e-02]\n [-3.28840696e-03  2.57177260e-03 -1.34971930e-01]\n [-1.38119572e-04  2.25318751e-03 -6.03902835e-02]\n [ 4.53278359e-03  2.40625868e-03  1.38175436e-01]\n [ 5.95225669e-03  1.00742943e-03  1.75614285e-01]]\n\n\nAnd my Y  looks like this (printed)   Y.shape -> (21, 1):\n\n[[ 0.00472189]\n [ 0.00134158]\n [-0.01183452]\n [-0.00712723]\n [ 0.01007362]\n [ 0.00373918]\n [-0.00832614]\n [-0.02623798]\n [-0.00381873]\n [ 0.0068726 ]\n [-0.01438412]\n [ 0.00898785]\n [ 0.0100893 ]\n [-0.00321919]\n [ 0.00827624]\n [ 0.00486361]\n [-0.01065365]\n [ 0.00741757]\n [ 0.01037663]\n [ 0.00230243]\n [-0.00169308]]\n\n\nHowever, sklearn is throwing the following exception:\n\nValueError: shapes (1,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)\n\n\nI have googled and tried multiple things, but no success. I come from a different programming background so python is kind of new to me.\n\nCould you guys point me in the right direction to solve this error?\n\nEDIT: My code:\n\ntraining_chart = self.prices[-self.TRAINING_DATA_SIZE:]\nx = []\ny = []\nn = 12\nfor i, price in enumerate(training_chart[n:]):\n    history = training_chart[i:i+n]\n    price_change = (price - history[-1]) / history[-1]\n\n    sma_12 = indicators.SMA(history, 12)\n    sma_8 = indicators.SMA(history, 8)\n    sma_6 = indicators.SMA(history, 6)\n    sma_4 = indicators.SMA(history, 4)\n    rsi = indicators.RSI(history, 12)[-1]\n\n    long_term_ma_index = (sma_8 - sma_12) / sma_12\n    short_term_ma_index = (sma_4 - sma_6) / sma_6\n\n    y.append(price_change)\n    x.append(long_term_ma_index)\n    x.append(short_term_ma_index)\n    x.append(rsi / 100 - 0.5)\n\nx = np.matrix(x).reshape(-1, 3)\ny = np.array(y).reshape(-1, 1)\n\nprint x\nprint x.shape\nprint y\nprint y.shape \n\nmodel = LinearRegression()\nmodel.fit(x, y)\n\n\nEDIT 2: Full traceback:\n\nTraceback (most recent call last):\n  File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py", line 162, in _run_module_as_main\n    "__main__", fname, loader, pkg_name)\n  File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py", line 72, in _run_code\n    exec code in run_globals\n  File "/Users/jessekramer/Code/trading-bot/pytrader/main.py", line 145, in &lt;module&gt;\n    main(sys.argv[1:])\n  File "/Users/jessekramer/Code/trading-bot/pytrader/main.py", line 53, in main\n    bot.run()\n  File "pytrader/runners/backtest.py", line 16, in run\n    self.strategy.tick(candlestick)\n  File "pytrader/bots/strategies/linear_regression.py", line 24, in tick\n    next_price = model.predict(self.current_price)\n  File "/Users/jessekramer/VirtualEnv/trading/lib/python2.7/site-packages/sklearn/linear_model/base.py", line 256, in predict\n    return self._decision_function(X)\n  File "/Users/jessekramer/VirtualEnv/trading/lib/python2.7/site-packages/sklearn/linear_model/base.py", line 241, in _decision_function\n    dense_output=True) + self.intercept_\n  File "/Users/jessekramer/VirtualEnv/trading/lib/python2.7/site-packages/sklearn/utils/extmath.py", line 140, in safe_sparse_dot\n    return np.dot(a, b)\nValueError: shapes (1,1) and (3,1) not aligned: 1 (dim 1) != 3 (dim 0)\n\n'
'I have a dataframe as you can see below. \n\nAs you can see the second row is not in this data frame. Also, there are a few more non-line in the whole dataframe. I want to reach a specific ID which I choose. And I have to make it with a loop.(not "loc" method or something like that) When I want to use "if loop", I get keyerror on number 2. \n\n\n\n\n\nSo How can I just pass the row 2 and keep going to search my ID?\n'
'I am wondering when I do train test split (20% test, 80% 80%) and then I apply 5 fold cross validation does that mean all data has been in the test set once? or is it randomly choosed each time that, in each fold the same events were possibly included in the test more than once and some possibly were never included in test set?\n\n#20% of the data will be used as test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed) \n\ncv_results= cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\n'
'I have a dataframe like this one. I need to replace NaN with median value, based on animal type. For example I need to calculate a median for cats and then replace only cats with NaN with this value. Is there a way to do this in one command or I need to do it manually for each type?\n\n  animal  age  weight priority\na    cat  2.5       1      yes\nb    cat  1.0       3      yes\nc    dog  0.5       6       no\nd    dog  NaN       8      yes\ne    cat  5.0       4       no\nf    cat  2.0       3       no\ng    dog  3.5      10       no\nh    cat  NaN       2      yes\ni    dog  7.0       7       no\nj    dog  3.0       3       no\n\n'
'For example the VotingClassifier expects a list of estimators, but in my case the different estimators already produced results (in the form of probabilities for each possible label e.g. [0.8, 0.2, 0.0, 0.0]) for the traing dataset as well as the result dataset. Is there a way to use this instead of the actual classifiers?\n'
'I\'m to Python and learning it by doing. I want to make two plots with matplotlib in Python. The second plot keeps the limits of first one. Wonder how I can change the limits of each next plot from previous. Any help, please. What is the recommended method? \n\nX1 = [80, 100, 120, 140, 160, 180, 200, 220, 240, 260]\nY1 = [70, 65, 90, 95, 110, 115, 120, 140, 155, 150]\n\nfrom matplotlib import pyplot as plt\nplt.plot(\n    X1\n  , Y1\n  , color = "green"\n  , marker = "o"\n  , linestyle = "solid"\n)\nplt.show()\n\n\nX2 = [80, 100, 120, 140, 160, 180, 200]\nY2 = [70, 65, 90, 95, 110, 115, 120]\n\nplt.plot(\n    X2\n  , Y2\n  , color = "green"\n  , marker = "o"\n  , linestyle = "solid"\n)\nplt.show()\n\n'
"I am working on an assignment for my Data Science class. I just need help getting started, as I'm having trouble understanding how to use pandas to group and selecting DISTINCT values.\n\nI need to find the movies with the HIGHEST RATINGS by FEMALES, my code returns me movies with ratings = 5, and gender = 'F', but it also repeats the same movie over and over again, since there are more than 1 users. I'm not sure how to just show movie, count of 5-star ratings, and gender = F. below is my code:\n\nimport pandas as pd\nimport os\nm = pd.read_csv('movies.csv')\nu = pd.read_csv('users.csv')\nr = pd.read_csv('ratings.csv')\n\nur = pd.merge(u,r)\ndata = pd.merge(m,ur)\n\ndf = pd.DataFrame(data)\n\ntop10 = df.loc[(df.gender == 'F')&amp;(df.rating == 5)]\nprint(top10)\n\n\nthe data files can be downloaded here\n\nI just need some help getting started, theres alot more to the homework, but once I figure this out I can do the rest. Just need a jump-start. thank you very much \n\nmv_id   title             genres              rating    user_id gender\n\n1       Toy Story (1995)   Animation|Children's|Comedy  5   1   F    \n2       Jumanji (1995)     Adventure|Children's|Fantasy 5   2   F        \n3       Grumpier Old Men (1995) Comedy|Romance          5   3   F            \n4       Waiting to Exhale (1995)    Comedy|Drama        5   4   F        \n5       Father of the Bride Part II (1995)  Comedy      5   5   F   \n\n"
'I\'m just getting started with data science, and I\'m planning to give the Titanic problem a shot. However, I don\'t really understand how I should import the dataset, or even where to store the downloaded dataset. Right now I created a folder in my DataScience-folder named input and stored the training set and the test-set in it. But when I\'m in pyCharm I can\'t find this folder. \n\nI tried to write the code \n\ndataset = pd.read_cvs(.../input/train.cvs)\n\n\nBut it just says \n\n\n  "Unsolved reference \'train\'\n\n\n, so where should I save my dataset and how can I access it in Python? \n'
"I have two dataframes, one with lots of rows that include a CategoryId property that repeats, and the other dataframe only has two columns: CategoryId and Category:\n\nprint(map)\n   CategoryId  Category\n1  n013523     Snake\n2  n012837     Iguana\n3  n092735     Dragon\n\nmap.shape\n(3, 2)\n\n\nprint(data)\n   CategoryId  Size\n1  n013523     0.4\n2  n013523     0.8\n3  n013523     0.15\n4  n012837     0.16\n5  n012837     0.23\n6  n012837     0.42\n...\n\ndata.shape\n(500000, 2)\n\n\nWhat I'd like to do is create a column on data that will have the value in map['Category'] where map['CategoryId'] == data['CategoryId'], such that the output is:\n\nprint(data)\n   CategoryId  Size  Category\n1  n013523     0.4   Snake\n2  n013523     0.8   Snake\n3  n013523     0.15  Snake\n4  n012837     0.16  Iguana\n5  n012837     0.23  Iguana\n6  n012837     0.42  Iguana\n...\n\n"
'Why does this piece of code create such a strange output?\n\nI want the plots to overlap so that I can see overlapping data points.\n\nIt seems that the plots are stacked on top of each other.\n\ndef read_csv(name):\n    file = open(folder+name,newline=\'\')\n    reader = csv.reader(file,delimiter=";")\n    data = []\n    for row in reader:\n        data.append(np.array(row[5:]))\n    file.close()\n    return data\n\n\ndef setup_plotting():\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.xaxis.set_major_locator(plt.MaxNLocator(10))\n    ax.yaxis.set_major_locator(plt.MaxNLocator(10))\n    return ax\n\n\nacc_x = read_csv("acc_x.csv")\n\nax=setup_plotting()\n\nfor entry in acc_x:\n    ax.plot(entry)\n\n\n\n\nPlease help me :)\n'
"I have this groupby dataframe ( I actually don't know how to call this type of table)\nA            B                C\n1            1            124284.312500\n             2             64472.187500\n             4             32048.910156\n             8             16527.763672\n             16             8841.874023\n2            1             61971.035156\n             2             31569.882812\n             4             16000.071289\n             8              7904.339844\n             16             4046.967041\n4            1             31769.435547\n             2             15804.815430\n             4              7917.609375\n             8              4081.160400\n             16             2034.404541\n8            1             15738.752930\n             2              7907.003418\n             4              3972.494385\n             8              1983.464478\n             16             1032.913574\n\nI want to plot the graph, which has A as x-axis, C as y-axis and B as different variables with legend.\nIn pandas document, I found the graph I try to have, but no luck yet.\n\n==========edited ===============\nThis is original dataframe\n    A   B        C\n0   1   1   122747.722000\n1   1   2   61839.731000\n2   1   2   61839.762000\n3   1   4   31736.405000\n4   1   4   31736.559000\n5   1   4   31787.312000\n6   1   4   31787.833000\n7   1   8   15872.596000\n8   1   8   15865.406000\n9   1   8   15891.001000\n\nI have df = df.groupby(['A', 'B']).C.mean()\nHow can I plot the graph with stacked table?\nThanks!\n"
'Suppose we have A as A=[[0,0,0],[1,1,1]] and B=[[1,1,1],[2,2,2],[3,3,3]] how can i write a vectorized implementation with no for loops in numpy to calculate matrix C as C=[[6,6,6],[7,7,7]]?\nC is created from summing each row in A with all rows in B.\n'
"I am doing analysis and I intend to restructure my scripts into a package. I have a directory structure like so:\n\nproject\n|   README.md\n|   setup.py\n|   requirements.txt\n|\n└───data\n└───notebooks\n|     notebook_A.ipynb\n|\n└───my_package\n    |   __init__.py\n    |\n    └───module_A\n    |    __init__.py\n    |    source_A.py\n    |\n    └───module_B\n        __init__.py\n        source_B.py\n\n\nFirst I will crete an environment with Conda:\n\nconda create my_environment\n\nThen, the goal is to make my_package importable in notebooks without loosing the ability to edit the source. So I will run:\n\n$ (my_environment) pip install -e .\n\n\nThis works as expected and creates:\n\n/Applications/anaconda3/envs/my_environment/lib/python3.6/site-packages/my_package.egg-link\n\n\nThen I want to check everything works:\n\n$ (my_environment) cd notebooks\n\n\nEverything works in ipython:\n\n$ (my_environment) ipython\n\n\nIn [1]: import src\nIn [2]: src.__path__\nOut[2]: ['/Users/jalmarituominen/Desktop/my_environment_project/src']\n\n\nBut when I run jupyter notebook and run it with my_environment kernel, I get\n\nimport sys\nsys.path\n[1]:\n['',\n'/Applications/anaconda3/lib/python36.zip',\n'/Applications/anaconda3/lib/python3.6',\n'/Applications/anaconda3/lib/python3.6/lib-dynload',\n'/Applications/anaconda3/lib/python3.6/site-packages',\n'/Applications/anaconda3/lib/python3.6/site-packages/aeosa',\n'/Applications/anaconda3/lib/python3.6/site-packages/IPython/extensions',\n'/Users/jalmarituominen/.ipython']\n\n\nObiviously I can't import my_package since its not in the PATH.\n\nHowever, when I change the kernel to Python 3, I get:\n\nimport sys\nsys.path\n[1]:\n['/Applications/anaconda3/envs/my_environment/lib/python36.zip',\n'/Applications/anaconda3/envs/my_environment/lib/python3.6',\n'/Applications/anaconda3/envs/my_environment/lib/python3.6/lib-dynload',\n'',\n'/Applications/anaconda3/envs/my_environment/lib/python3.6/site-packages',\n'/Users/jalmarituominen/Desktop/my_environment_project',\n'/Applications/anaconda3/envs/my_environment/lib/python3.6/site-packages/IPython/extensions',\n'/Users/jalmarituominen/.ipython']\n\n\nAnd my_package is importable.\n\nFor some reason PATHS of these two environments are mixed up. Any idea how to resolve this? Is it possible to manually change the PATH of a Kernel? \n"
'I have a dataset which has distance and weekday column.\nDistance is double value, weekday is string.(Monday,Tuesday...)\n\nHow do i show the relation between distance and weekday ? I need to check that distances are increasing in weekends or not.\n\nsome part of the data:\n\n\n\nnever mind time_of_day column\n\n distance weekday\n    1.498991 Monday\n    5.122769 Thursday\n    1.492705 Friday\n    1.972825 Monday\n    2.517838 Monday\n    1.648552 Saturday\n    2.503511 Thursday\n    1.671742 Friday\n    3.974399 Friday\n    7.616923 Wednesday\n\n'
'I have data shape of (400,93,32). Now I want to standardize the data with 0 mean and unit variance. How should I do it? \n\nIn which direction should I standardize ? should I take the mean and variance of each row or column or treat 93 x32 as matrics? I have tried using Sklearn function StandardScaler and it takes the input in the form of (n_samples,n_features) and I have (n_samples,Dim_1,Dim_2). I tried scaling it row-wise like this [n_samples,0,1,3,...n, : ] and am not sure whether am doing it the right way.\n\nyour guidance will be highly appreciated. \n'
"I have data that looks like this below. Its all in a list in an array format. Ultimately I am trying to sum all these values in the Python list. Is pandas the best option for this? I am attempting to create a dataframe named test.\n\nimport pandas as pd\nimport numpy as np\n\n\ntest = [array([[1083.8748]], dtype=float32), array([[998.98773]], dtype=float32), array([[1137.0487]], dtype=float32), array([[1077.2798]], dtype=float32), array([[926.41284]], dtype=float32),\narray([[1030.7125]], dtype=float32), array([[1028.0048]], dtype=float32), array([[523.9799]], dtype=float32), array([[1125.092]], dtype=float32), array([[1119.7738]], dtype=float32),\narray([[918.6966]], dtype=float32), array([[1112.5186]], dtype=float32), array([[555.6942]], dtype=float32), array([[1096.5643]], dtype=float32), array([[826.35657]], dtype=float32),\narray([[1014.35406]], dtype=float32), array([[1027.6962]], dtype=float32), array([[924.20087]], dtype=float32), array([[1035.217]], dtype=float32), array([[1008.9658]], dtype=float32),\narray([[970.54047]], dtype=float32), array([[847.0671]], dtype=float32), array([[913.5519]], dtype=float32), array([[1047.0747]], dtype=float32), array([[920.0606]], dtype=float32),\narray([[994.2266]], dtype=float32), array([[991.4501]], dtype=float32), array([[972.43256]], dtype=float32), array([[934.8802]], dtype=float32), array([[912.04004]], dtype=float32), array([[1131.297]], dtype=float32)]\n\n\ndf = pd.DataFrame(test)\n\nprint(test.sum())\n\n\nThis fails with the error: NameError: name 'array' is not defined\n\nCan anyone give me a tip? Thanks\n"
'I am trying to see if I can subract a number from a col, but if the number is negative after the subtraction to limit the output to 0 and nothing less or negative. \n\nFor ex. col in DF looks like\n\nMins | new_col\n 5.0    2.0\n 1.0    0.0\n 2.0    0.0\n 0.5    0.0\n 1.2    0.0\n 4.0    1.0\n\n\nif I want to create a new column that gives me the same values but subtracts 3 from each value in that column. \n'
"I have a dataframe filled with twitter data. The columns are:\n\n\nrow_id : Int\ncontent : String\nmentions : [String]\nvalue : Int\n\n\nSo for every tweet I have it's row id in the dataframe, the content of the tweet, the mentions used in it (for example: '@foo') as an array of strings and a value that I calculated based on the content of the tweet.\n\nAn example of a row would be:\n\n\nrow_id : 12\ncontent : 'Game of Thrones was awful'\nmentions : ['@hbo', '@tv', '@dissapointment', '@whatever']\nvalue: -0.71\n\n\nSo what I need is a way to do the following 3 things:\n\n\nfind all rows that contain the mention '@foo' in the mentions-field\nfind all rows that ONLY contain the mention '@foo' in the mentions-field\nabove two but checking for an array of strings instead of checking for only one handle\n\n\nIf anyone could help met with this, or even just point me in the right direction that'd be great.\n"
"I have just recently started on python data science and noticed that i can call the columns of a dataset in two ways. I was wondering if there was an advantage to using one method over the other or can they be used interchangeably?\n\nimport seaborn\niris = seaborn.load_dataset('iris')\n\nprint(iris.species)\nprint(iris['species'])\n\n\nBoth print statements give the same output in Jupyter\n"
'Good Afternoon all\n\nI have run into a small issue trying to scrape data from job posting site, I am also somewhat newer to python and scrapy as a whole. \n\nI have a script that I am running to extract data from various indeed postings. The crawler seems to complete with no errors, though will not extract data from sites that respond with either a 301 or 302 error code.\n\nI have pasted the script and log at bottom\n\nAny help would be appreciated\n\nimport scrapy\nfrom scrapy import Request\n\nclass JobsSpider(scrapy.Spider):\n    name = "jobs"\n    allowed_domains = ["indeed.com"]\n    start_urls = ["https://www.indeed.com/jobs?q=%22owner+operator%22&amp;l=dallas"]\n\n    def parse(self, response):\n        handle_httpstatus_list = [True]\n        jobs = response.xpath(\'//div[@class="title"]\')\n\n        for job in jobs:\n            title = job.xpath(\'a//@title\').extract_first()\n            posting_link = job.xpath(\'a//@href\').extract_first()\n            posting_url = "https://indeed.com" + posting_link\n            job_location = job.xpath(\'div//@data-rc-loc\').extract_first()\n\n\n            yield Request(posting_url, callback=self.parse_page, meta={\'title\': title, \'posting_url\':posting_url, \'job_location\':job_location})\n\n        relative_next_url = response.xpath(\'//link[@rel="next"]/@href\').extract_first()\n        absolute_next_url = "https://indeed.com" + relative_next_url\n\n        yield Request(absolute_next_url, callback=self.parse)\n\n\n    def parse_page(self, response):\n        posting_url = response.meta.get(\'posting_url\')\n        title = response.meta.get(\'title\')\n        job_location = response.meta.get(\'job_location\')\n\n        job_name= response.xpath(\'//*[@class="icl-u-xs-mb--xs icl-u-xs-mt--none  jobsearch-JobInfoHeader-title"]/text()\').extract_first()\n        job_description_1=response.xpath(\'//div[@class="jobsearch-jobDescriptionText"]/ul\').extract()\n        posted_on_date= response.xpath(\'//div[@class="jobsearch-JobMetadataFooter"]/text()\').extract_first()\n        job_location=response.xpath(\'//*[@class="jobsearch-InlineCompanyRating icl-u-xs-mt--xs  jobsearch-DesktopStickyContainer-companyrating"]/div[3]/text()\').extract_first()\n        job_description_2=response.xpath(\'//div[@class="jobsearch-JobComponent-description  icl-u-xs-mt--md  "]/text()\').extract_first()\n\n        yield{\'title\':title,\n            \'posting_url\':posting_url,\n            \'job_name\':job_name,\n            \'job_location\': job_location,\n            \'job_description_1\':job_description_1,\n            \'posted_on_date\':posted_on_date,\n            \'job_description_2\':job_description_2,\n            \'job_location\':job_location\n             }\n\n2019-09-29 12:37:53 [scrapy.core.engine] INFO: Closing spider (finished)\n2019-09-29 12:37:53 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{\'downloader/request_bytes\': 1860897,\n \'downloader/request_count\': 1616,\n \'downloader/request_method_count/GET\': 1616,\n \'downloader/response_bytes\': 13605809,\n \'downloader/response_count\': 1616,\n \'downloader/response_status_count/200\': 360,\n \'downloader/response_status_count/301\': 758,\n \'downloader/response_status_count/302\': 498,\n \'dupefilter/filtered\': 9,\n \'elapsed_time_seconds\': 28.657843,\n \'finish_reason\': \'finished\',\n \'finish_time\': datetime.datetime(2019, 9, 29, 19, 37, 53, 776779),\n \'item_scraped_count\': 337,\n \'log_count/DEBUG\': 1954,\n \'log_count/ERROR\': 1,\n \'log_count/INFO\': 10,\n \'memusage/max\': 54546432,\n \'memusage/startup\': 54546432,\n \'request_depth_max\': 20,\n \'response_received_count\': 360,\n \'robotstxt/request_count\': 3,\n \'robotstxt/response_count\': 3,\n \'robotstxt/response_status_count/200\': 3,\n \'scheduler/dequeued\': 1612,\n \'scheduler/dequeued/memory\': 1612,\n \'scheduler/enqueued\': 1612,\n \'scheduler/enqueued/memory\': 1612,\n \'spider_exceptions/TypeError\': 1,\n \'start_time\': datetime.datetime(2019, 9, 29, 19, 37, 25, 118936)}\n2019-09-29 12:37:53 [scrapy.core.engine] INFO: Spider closed (finished)\n\n\n  [1]: https://i.stack.imgur.com/6MOMC.png\n\n'
"I have to do a random forest classifier for an exercise and the exercise specifically says for the parameters,  and I quote from my language\n\nin-bag percentage: 25% 50% 85%\nNumber of dimensions in one node: 10%, 50%, 80%\n\nI use scikit-learn for the classifier and I don't know which are the parameters from the class to set the in-bag percentage and the number of dimensions.\n"
'I am trying to calculate avg weight under two columns , sex and type as the df is below:\n\nsex |  type     | weight\n M     Obese      305\n F     Normal     100\n M     Underweight 105\n\n'
"So, I have a keyword list lowercase. Let's say \n\nkeywords = ['machine learning', 'data science', 'artificial intelligence']\n\n\nand a list of texts in lowercase. Let's say\n\ntexts = [\n  'the new machine learning model built by google is revolutionary for the current state of artificial intelligence. it may change the way we are thinking', \n  'data science and artificial intelligence are two different fields, although they are interconnected. scientists from harvard are explaining it in a detailed presentation that could be found on our page.'\n]\n\n\nI need to transform the texts into:\n\n[[['the', 'new',\n   'machine_learning',\n   'model',\n   'built',\n   'by',\n   'google',\n   'is',\n   'revolutionary',\n   'for',\n   'the',\n   'current',\n   'state',\n   'of',\n   'artificial_intelligence'],\n  ['it', 'may', 'change', 'the', 'way', 'we', 'are', 'thinking']],\n [['data_science',\n   'and',\n   'artificial_intelligence',\n   'are',\n   'two',\n   'different',\n   'fields',\n   'although',\n   'they',\n   'are',\n   'interconnected'],\n  ['scientists',\n   'from',\n   'harvard',\n   'are',\n   'explaining',\n   'it',\n   'in',\n   'a',\n   'detailed',\n   'presentation',\n   'that',\n   'could',\n   'be',\n   'found',\n   'on',\n   'our',\n   'page']]]\n\n\nWhat I do right now is checking if the keywords are in a text and replace them with the keywords with _. But this is of complexity m*n and it is really slow when you have 700 long texts and 2M keywords as in my case.\n\nI was trying to use Phraser, but I can't manage to build one with only my keywords.\n\nCould someone suggest me a more optimized way of doing it?\n"
'I have a json array table that I am inserting into notebook such as\n\nTable\n{"id":"v100",signal:"2017-10-02 03:30:00",mode:"online"}\n{"id":"v100",signal:"2017-10-02 06:30:00",mode:"online"}\n{"id":"v400",signal:"2017-10-03 03:30:00",mode:"online"}\n{"id":"v400",signal:"2017-10-04 05:30:00",mode:"offline"}\n\n\nWithout column names and just those headers and values in each row.\n\nHow can I transform this into a dataframe that has column names for:\n\nid          | signal       | mode\nv100   2017-10-02 03:30:00   online\nv100   2017-10-02 06:30:00   online\nv400   2017-10-02 03:30:00   online\nv400   2017-10-02 05:30:00   offline\n\n\nThanks\n'
'I do know there is a word "cat" and the word "cats" in my vocabulary.\n\nExample 1:\n\nmodel.wv.most_similar("cat")\n\n\nThis returns [ ("cats", 0.83...), ("wild", 0.79...), ... ]. There is no ("cat", 1.0) on the top of result.\n\nExample 2:\n\nmodel.wv.most_similar("cats")\n\n\nThis returns [ ("cat", 0.85...), ("wild", 0.77...), ... ]. There is no ("cats", 1.0) on the top of result.\n\nQuestion: is there a way to get complete match on the top of result if it is? Or to check complete match with other method... Mayby I don\'t understand something. Anyway, help needed.\n'
"can someone help me to find the number of observations in each different category in variables using python? for that i used              \n\ndf['column name'].value_counts() for single variable.\n\n\nBut I want to know how it uses for whole categorical variables in data-set.\n"
'I need to convert a nested for loop with a function call and appending data to a list comprehension. \n\nThis is what I would like to convert to a list comprehension\n\npredictions = []\nfor train_row in test_data:\n    distances = np.argmin([dist(train_row, test_row) for test_row in train_data])\n    predictions.append(train_tgt[distances])\n\n'
'I have a data frame consists of column 1 i.e event and column 2 is Datetime:\n\nSample data \n\n Event   Time\n    0   2020-02-12 11:00:00\n    0   2020-02-12 11:30:00\n    2   2020-02-12 12:00:00\n    1   2020-02-12 12:30:00\n    0   2020-02-12 13:00:00\n    0   2020-02-12 13:30:00\n    0   2020-02-12 14:00:00\n    1   2020-02-12 14:30:00\n    0   2020-02-12 15:00:00\n    0   2020-02-12 15:30:00\n\n\nAnd I want to find start time and end time of each event:\n\nDesired Data\n\n Event  EventStartTime  EventEndTime\n    0   2020-02-12 11:00:00 2020-02-12 12:00:00\n    2   2020-02-12 12:00:00 2020-02-12 12:30:00\n    1   2020-02-12 12:30:00 2020-02-12 13:00:00\n    0   2020-02-12 13:00:00 2020-02-12 14:30:00\n    1   2020-02-12 14:30:00 2020-02-12 15:00:00\n\n\nNote: EventEndTime is time when the event changes the value say from value 1 to got change to 0 or any other value or vice versa\n'
"I want to find all rows where a certain value is present inside the column's list value.\n\nSo imagine I have a dataframe set up like this:\n\n|  placeID |                             users | \n------------------------------------------------\n|    134986|   [U1030, U1017, U1123, U1044...] |\n|    133986|   [U1034, U1011, U1133, U1044...] |\n|    134886|   [U1031, U1015, U1133, U1044...] |\n|    134976|   [U1130, U1016, U1133, U1044...] |\n\n\nHow can I get all rows where 'U1030' exists in the users column? \n\nOr... is the real problem that I should not have my data arranged like this, and I should instead explode that column to have a row for each user? \n\nWhat's the right way to approach this?\n"
"I'm working with data science and I have a pretty big script consisting of a lot of helper functions to run analysis on pandas dataframes and then one main function that utilizes my functions to get some results.\n\nThe issue is that most or the function inputs are just names of the columns in the dataframe (because they are taken as input from users) and this leads to a lot of functions have partially the same input.\n\nWhat I wonder is if it possible to have a dict containing all my parameters and simply pass the dict to every function without modifying the dict? That is, each function would have to ignore the keywords in the dict that are not an input to that particular function.\n\nTo give an example say I have this function. \n\n    def func1(a, b, c):\n        return a + b * c\n\n\nAnd I have this dict containing the inputs to all my functions. \n\n    input_dict = {'a': 1,\n                 'b': 2,\n                 'c': 3,\n                 'd': 4}\n\n\nIf I call this function with this dict as input like this:\n\n    value = func1(**input_dict)\n\n\nI will get an error because of the unexpected argument d. \n\nAny workaround here that doesnt involve altering the dict? My code works but I would love to get away from having so many repeated input arguments everywhere. \n\nThanks in advance :) \n"
"i want to write a program in python that iterate over each row of a data-matrix in a .csv file and then pass each row as an input to time-series-analysis model and the output(which is going to be a single value) of each row analysed over model will be stored in a form of coloumn.\ntill now i had tried iterating over rows ,passing it through model and printing each output:\n\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.ar_model import AR\nfrom random import random\n\n\ndata=pd.read_csv('EXAMPLEMATRIX.csv',header=None)\nfor i in data.iterrows():\n    df=np.asarray(i)\n    model=AR(df)\n    model_fit=model.fit()\n    yhat=model_fitd.predict(len(df),len(df))\n    print(yhat)\n\n\nbut the error that i'm getting is:\n\n\n  ValueError: maxlag should be &lt; nobs\n\n\nplease help me solving this problem or finding out where it is going wrong or provide me a reference for solving this problem .\n\nTHANKS in Advance \n"
'I am using MAPE metric for my linear regression model. The output is inf.\n\nThe code looks like this:\n\nimport numpy as nm\n\ndef MAPE(a, b):\n    mape = nm.mean(nm.abs((a - b) / a)) * 100\n    return mape\n\nMAPE(data.iloc[:, 1], pre)\n\n\npre is the variable storing the predicted value.\n\nCan anyone suggest what is the glitch in this ?\n'
"ImportError                               Traceback (most recent call last)\n&lt;ipython-input-13-74c9bc9e3e4a&gt; in &lt;module&gt;\n      8 from nltk.tokenize.toktok import ToktokTokenizer\n      9 #import contractions\n---&gt; 10 from contractions import CONTRACTION_MAP\n     11 import unicodedata\n     12 \n\nImportError: cannot import name 'CONTRACTION_MAP' from 'contractions' (c:\\users\\nikita\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\contractions\\__init__.py)\n\n\nOne question is: is the CONTRACTION_MAP variable deprecated from the contractions package?\n"
'I have a data-frame with n rows:\n\ndf = 1 2 3\n     4 5 6\n     4 2 3\n     3 1 9\n     6 7 0\n     9 2 5\n\n\nI want to add a columns with the same value in groups of 3. \nn (num rows) is for sure divided by 3.\n\nSo the new df will be:\n\ndf = 1 2 3 A\n     4 5 6 A\n     4 2 3 A\n     3 1 9 B\n     6 7 0 B\n     9 2 5 B\n\n\nWhat is the best way to do so?\n'
'I have a csv with two columns, one with week day names and one with time of login of the employee. How do I plot this using matplotlib or pyplot ? not able to get how to plot the time in Y coordinate. the time data is like "23/02/2017 at 11:30 PM". \n\nDo I need to make changes to the data or Can I plot it as is?\n'
'I have a dataframe with 50 numerical columns and 10 categorical columns.\n\ndf = C1 C2 .. C10 N1 N2 ... N50\n     a  b      c   2 3      1\n\n\nI want to remove all outliers, but only from columns N1,N2,N6,N8,N10.\nMeaning I wnt to keep all wors that are not outliers in any of this columns.\nWhat is the best way to do it?\n'
'Not sure how to compute IF conditionals on a dataframe as you would do in a standard python code.\n\nI have the following df:\n\n\n\nThe values in \'Label\' corresponds to the maximum value from each row. As an example, row (0) the maximum value corresponds to NO_2.\n\nI want to replace the value in \'Label\' based on the following chart:\n\n\n\nSo for example, for row (0) the \'Label\' value corresponds to NO_2 as mentioned before, so checking the chart the value of 67.120003 falls into the range of 40-100 for NO_2, hence I would like to replace the \'Label\' value for row (0) with 2.\n\nHere is a piece od data (*Note: I\'m modiying it a bit in order to obtain variablility for maximum values for each contaminant for the sake of an example):\n\n            date        O_3     PM25        PM10        CO      SO_2         NO_2       Label\n0   2001-01-01 01:00:00 7.86    12.505127   32.349998   0.45    26.459999   67.120003   67.120003\n1   2001-01-01 02:00:00 7.21    12.505127   40.709999   0.48    20.879999   70.620003   70.620003\n2   2001-01-01 03:00:00 7.11    12.505127   50.209999   0.41    21.580000   72.629997   72.629997\n3   2001-01-01 04:00:00 7.14    12.505127   54.880001   0.51    19.270000   75.029999   75.029999\n4   2001-01-01 05:00:00 8.46    12.505127   42.340000   0.19    13.640000   66.589996   66.589996\n5   2018-04-30 20:00:00 63.00   200.000000  2.000000    0.30    4.000000    58.000000   200.000000\n6   2018-04-30 21:00:00 49.00   400.000000  5.000000    0.30    4.000000    65.000000   400.000000\n7   2018-04-30 22:00:00 49.00   3.000000    125.000000  0.30    4.000000    58.000000   125.000000\n8   2018-04-30 23:00:00 48.00   7.000000    7.000000    0.30    4.000000    52.000000   52.000000\n9   2018-05-01 00:00:00 52.00   4.000000    6.000000    0.30    4.000000    43.000000   52.000000\n\n\nSo in order to get the maximum value from each row, what I\'m doing is:\n\n# Getting max values from each contaminant on each row\nmax_value = final_df.max(axis=1)\n\n\nAnd in order to obtain the maximum value column name:\n\n# Obtaining maximum value column name for each row\nlabel_max_colName = final_df.eq(final_df.max(1), \naxis=0).dot(final_df.columns)\n\n\nI have followed one of the proposed solutions from @TH14 which is:\n\nfor index, val in final_df[[x for x in final_df.columns if x != \'date\']].iterrows():\n    max_column = np.argmax(val)\n    max_column_val = np.max(val)\n\n    if max_column == "O_3":\n        if max_column_val &lt;= 80:\n            final_df.at[index, \'Label\'] = 1\n\n        if 80 &lt; max_column_val &lt; 120:\n            final_df.at[index, \'Label\'] = 2\n\n        if 120 &lt; max_column_val &lt; 180:\n            final_df.at[index, \'Label\'] = 3\n\n        if 180 &lt; max_column_val &lt; 240:\n            final_df.at[index, \'Label\'] = 4\n\n        if 240 &lt; max_column_val &lt; 600:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "NO_2":\n        if max_column_val &lt;= 40:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 100:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 200:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 400:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 1000:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "SO_2":\n        if max_column_val &lt;= 100:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 200:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 350:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 500:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 1250:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "PM10":\n        if max_column_val &lt;= 20:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 35:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 50:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 100:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 1200:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "PM25":\n        if max_column_val &lt;= 10:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 20:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 25:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 50:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 800:\n            final_df.at[index, \'Label\'] = 5\n\n\nBut does not seem to change anything in the \'Label\' column:\n\n\n'
'Here is my dataset after cleaning csv file\n\n\n\nHere is output what I want\n\n\n\nWhat I want is , I have to display years in x axis and column values in y axis.and I want to display bubbles with different colors and size with play animation button\n\nI am new to data science , can someone help me ,how can I achieve this?\n'
'input:\n\ndata["Date"] = ["2005-01-01", "2005-01-02" , ""2005-01-03" ,..., "2014-12-30","2014-12-31"]\n\n\nhow can i sort the column such that the column gives 1st date of every year, 2nd date of every and so on:\n\ni.e. \n\noutput:\n\ndata["Date"] = ["2005-01-01","2006-01-01","2007-01-01", ... "2013-12-31","2014-12-31"]\n\n\nNOTE: assuming the date column has no leap days\n'
'Problem Statement\nI have the CSV data as shown in the image. From this, I have to use only keep RegionName, State and the quarterly mean values from 2000 - 2016. Also, I want to use multi-indexing with [State, RegionName].\n \n\nI am working on a CSV file with pandas in python. As shown in the screenshot.\n\nThank you in advance.\n'
'Below I have 4 columns in my dataframe. I am interested in going through the entire "Greater_than_50" column. Upon reaching a "True" flag, I then want to take the associated  "Discharge" and "Resistance" values to make a new dataframe which contains only those values found to be "True". \n\ntime           Discharge          Resistance  Greater_than_50\n-------------------------------------------------------------\n0                  0.000                 NaN              NaN\n1                  0.005              76.373             True\n2                  0.010             -48.174            False\n3                  0.016             -37.012            False\n4                  0.021             -27.808            False\n5                  0.026             -24.674            False\n6                  0.031             -20.464            False\n7                  0.037             100.114            True\n...                  ...                 ...              ...\n\n\nI would like the new dataframe to look something like this:\n\nDischarge          Resistance  \n------------------------------\n0.005              76.373 \n0.037             100.114          \n...                   ...           \n\n\n'
'I\'m trying to use the pandas function: read_csv to obtain a dataframe containing 3 columns (the first is a string , the 2nd is a datetime64 and the third is a list of lists ). The CSV is written as shown below :\n\n**Jack**   ,   1590491881000 , [[1,0.61],[2,0.23],[3,0.89]]\n**Mark**   ,   1590407272000 , [[1,0.24],[2,0.36],[3,0.93]]\n\n\nI simplified the list because in reality there are over 1000 lists in the list of lists.\n\nAlso, the format of the timestamp is converted somehow, so I figured I could obtain the exact date using:\n\nnp.datetime64(1590491881000, \'ms\')\n\n\nThe problem is when I use read_csv, I get several columns depending on the delimiter: ",", the rows also get mixed up, but what I need is to only have 3 columns. Also, the names of the columns are given to the last "detected" columns.\n\nI tried this, but it did not work :\n\nfields = [\'Client\',\'Timestamp\',\'Measurements\']\ndf = pd.read_csv("df.csv", names = fields, dtype={"Client": str , "Timestamp": str , "Measurements" : str })\n\n\nAre there some adjustments that I can make to the read_csv function to fix this?\n'
'So I have a time series data frame which has both Patient Ids in one column and Years ( 1 - 3) in another columns. What I want to do using pandas is,  for each unique patient Id only keep rows that have entries for all the Years 1-3.\n\nIN \n\nID  Year \n111  1 \n111  2 \n111  3 \n222  1 \n222  2\n333  1\n333  2\n333  3\n\n\nOUT \n\nID  Year \n111  1 \n111  2 \n111  3 \n333  1\n333  2\n333  3\n\n'
'In this notebook the author writes the following nesterov update:\n\ndef nesterov_update(w, dw, v, lr, weight_decay, momentum):\n    dw.add_(weight_decay, w).mul_(-lr)\n    v.mul_(momentum).add_(dw)\n    w.add_(dw.add_(momentum, v))\n\n\nAs I understand it, a.add(b) in PyTorch implements a+b and a.add(b,c) implements a+(b*c), because b is in the slot of the alpha parameter. And lastly, add_ does the in-place version of add.\n\nQ: Am I right so far?\n\nThen, if I were to sketch the above nesterov update in an expanded form that illustrates the logic, I would write:\n\ndw = -lr*(dw + weight_decay*w)\nv = v*momentum + dw\nw = w + dw + momentum*v\n\n\nQ: is this correct?\n\nI\'m not planning to use the above expanded "code," I\'m just writing it this way to try communicate what I\'m understanding that it\'s doing, to check.\n'
'I would like to create a program that computes a distance matrix from the results of my calculations on sets. Data about these sets is taken from a file.\nI currently have something like the code below, but maybe my idea is quite bad and it can be improved (e.g. collections only listed, not ordered).\n\nmy_list = []\nfile = open("plik.txt","r")\n\nfor i in file:\n    my_sets = i.split(", ")\n    A = set(my_sets[0])\n    B = set(my_sets[1])\n    a = len(A.difference(B))\n    b = len(B.difference(A))\n    c = len(A.union(B))\n    metric = (a*b)/c\n    my_list.append(round(metric, 2))\n\nprint(my_list)\nfile.close()\n\n\nAs an example, imagine that I have in a file something like this (all combinations for 4 sets):\n\nset1 set2\nset1 set3\nset1 set4\nset2 set3\nset2 set4\nset3 set4\n\n\nNow, examples of values for the resulting metric could be:\n\n[0.8, 1.2, 4.3, 5, 7, 0.2]\n\nI want to get a 4x4 matrix matrix out of it, containing the metric for all pairs:\n\n[[0, 0.8, 1.2, 4.3],\n[0.8, 0, 5, 7],\n[1.2, 5, 0, 0.2],\n[4.3, 7, 0.2, 0]]\n\n\nHow should I transform the list of values of the metric obtained for the pairs so to obtain the distance matrix? \n'
"Basically for every non-nan row, I want the second part of the column name (after the comma) of that row and store it in new_df - the new dataframe (having the same index as the non-nan value in the existing dataframe) under column named after the first part (before the comma) of its column in the existing dataframe(df).\nSorry about my use of words if it gets clumsy.\n\nthe existing dataframe: https://i.stack.imgur.com/ps97u.png\n\nMy code\nnew_df = pd.DataFrame()\n\nfor i in range(0, len(df)):\n    for j in cols[:3]:\n        if df.loc[i, j] != &quot;nan&quot;:\n            col = j\n            x = col.split(',')[1]\n            y = col.split(',')[0].split(',')[0]\n            new_df[y][i] = x\n        else:\n            pass\n\nIn the code above, I was testing with just the politics section and it did not work and I am not sure how I can do that for the whole dataframe.\n"
"I am trying to read the csv files in the current directory. In-order to do that, I want to check all the files present in my current directory. I have tried doing it with check_output function. However, i received this error and I'm unable to figure out how to deal with it. This is the code I have tried:\nfrom subprocess import check_output\nprint(check_output([&quot;ls&quot;,&quot;../input&quot;]).decode('utf8'))\n\nthis is the error i have received:\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-15-c18143c64098&gt; in &lt;module&gt;\n      1 from subprocess import check_output\n----&gt; 2 print(check_output([&quot;ls&quot;,&quot;../input&quot;]).decode('utf8'))\n\n~\\Anaconda3\\lib\\subprocess.py in check_output(timeout, *popenargs, **kwargs)\n    393 \n    394     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n--&gt; 395                **kwargs).stdout\n    396 \n    397 \n\n~\\Anaconda3\\lib\\subprocess.py in run(input, capture_output, timeout, check, *popenargs, **kwargs)\n    470         kwargs['stderr'] = PIPE\n    471 \n--&gt; 472     with Popen(*popenargs, **kwargs) as process:\n    473         try:\n    474             stdout, stderr = process.communicate(input, timeout=timeout)\n\n~\\Anaconda3\\lib\\subprocess.py in __init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\n    773                                 c2pread, c2pwrite,\n    774                                 errread, errwrite,\n--&gt; 775                                 restore_signals, start_new_session)\n    776         except:\n    777             # Cleanup if the child failed starting.\n\n~\\Anaconda3\\lib\\subprocess.py in _execute_child(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\n   1176                                          env,\n   1177                                          os.fspath(cwd) if cwd is not None else None,\n-&gt; 1178                                          startupinfo)\n   1179             finally:\n   1180                 # Child is launched. Close the parent's copy of those pipe\n\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n\n\n"
"I'm digging into pandas aggregator function while working with a wine reviews dataset. To aggregate points given by wine reviewers, I noticed that, when I used mean as a standalone function in agg():\nreviewer_mean_ratings = reviews.groupby('taster_name').points.agg('mean')\n\nThe output looks like this:\n\nNoticed that the output has 2 columns(at least that's what it looks like visually). But\ntype(reviewer_mean_ratings) = pandas.core.series.Series\n\nIs that just 1 column with space between the name and mean rating? I'm confused.\nAlso noticed that, I cannot sort this output in descending order by the mean ratings. Instead if I had used mean as a list in agg() then descending order works using sort_values() method later.\nMy hypothesis is that if I want to access the mean ratings column later, the only way to do it is to use agg(['mean']) instead of agg('mean') in the original query. Am I mistaken somewhere?\n"
'A pharmacist wants to test whether a new kind of sleeping pill would be effective to increase the hours of sleep for people who take it. A random sample of 10 persons (Group A) was given the new pills and another random sample of 13 persons (Group B) was given the old pills.\nTheir sleep in hours were recorded as follows:\nMean of group A = 8.9 and Standard Deviation of group A = 0.8\nMean of Group B = 8.5 and standard Deviation of group B = 0.5\nConstruct a 95% confidence interval for the difference of the average hours of sleep that would be obtained for the people taking the new pills over the people taking the old pills. State the assumptions made.\nHow do I code this in python???\n'
"I am using TA-Lib library to calculate Bollinger Band for stock data: upperband, middleband, lowerband = BBANDS(close, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\nMy code is df['Bollinger Bands'] = talib.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0) but python gives out an error &quot;Length of values does not match length of index&quot;??\nPossibly, it could be due to three outputs that BBands produce &quot;upperband, middleband, lowerband&quot; that my code is trying to pushing into one. Is there some way to provide three columns for the output?\n"
"i have this code :\ndf = df.sort_values(['CUST','FT','DATE'])\ndf = df.groupby(['CUST', 'FT', 'DATE']).agg({'QTY': ['sum']})\ndf\n\nAfter the sort, i group by and print the table, i want to have further manipulation but python\nonly recognizes QTY inside df object. I need the object to be a DataFrame.\nThis is how it looks like :\n\nThe error i get for trying to engage CUST for example :\nAttributeError: 'DataFrame' object has no attribute 'CUST'.\nwhat is the problem ? what am i doing wrong ?\nhow do i deal with that\n"
'So my problem is that I have hundreds of min. to max. ranges. I want to find the numberrange that most of them are in.\nFor example: 0,5 - 2,5 ; 2 - 3 ; 0,2 - 4 All of these numbers have one range that covers all of them. I want to find these Ranges to cover most of the different ranges with one. Basically my program has to group them and categorise them into &quot;masterranges&quot;. Ideal solution would be that i can use only few ranges but can be sure that I also cover the ranges of others.\nIf you have any Ideas or can recommend tools to get there using python I would be happy and thankful for your help since I am new to programming.\n'
"I'm new to data science and scikit-learn so I apologize if this is a basic question. Do we need to make a new instance of a sklearn class when we want to train on a new dataset? For example, I am currently doing:\ntransformer = PowerTransformer()\ntransformed1 = transformer.fit_transform(data1.to_numpy())\n\ntransformer = PowerTransformer()\ntransformed2 = transformer.fit_transform(data2.to_numpy()) \n...\n\nI have a multiple sets of data that I want to transform so that I can run KNNImputer (again using this repeat declarative approach).\nI read that the .fit method internally stores the lambdas that it used to fit the data passed in but do the stored lambdas get overwritten with each call to .fit or do they get influenced by the fit on the new data?\nWould it be wrong to do:\ntransformer = PowerTransformer()\ntransformed1 = transformer.fit_transform(data1.to_numpy())\ntransformed2 = transformer.fit_transform(data2.to_numpy())\n...\n\nThank you in advance!\n"
"I have an excel with the column of file path. I want to change it to only the file name.\ne.g. shown as below image.\n\n\nfilename should be only Test and Test1\nJust need to write Test@test.com at twice ie once for Test.csv and another for Test1.csv.\nIn short below image is my requirement is.\n\n\nI want to do it using python and pandas / dataframe.\nI have 100 of rows in 'filename' column.\nI tried using;\nimport os\nimport glob\nmyfile=os.path.basename('C:/Users/Test.csv')\nos.path.splitext(myfile)\nprint (os.path.splitext(myfile)[0])\n\nBut it is useful for one path, how to apply it to entire column?\nThanks!\n"
'In this data set I have two categorical response values (0 and 1) and I want to fit the Logit model using statsmodels.\nX_incl_const = sm.add_constant(X)\nmodel = sm.Logit(y, X_incl_const)\nresults = model.fit()\nresults.summary()\n\nwhen I try to plot the line and points using code below:\nplt.scatter(X, y)\nplt.plot(X, model.predict(X))\n\nI get the following error:\n    ValueError                                Traceback (most recent call last)\n    &lt;ipython-input-16-d69741b1f0ad&gt; in &lt;module&gt;\n          1 plt.scatter(X, y)\n    ----&gt; 2 plt.plot(X, model.predict(X))\n    \n    ~\\Anaconda3\\lib\\site-packages\\statsmodels\\discrete\\discrete_model.py in predict(self, params, exog, linear)\n        461             exog = self.exog\n        462         if not linear:\n    --&gt; 463             return self.cdf(np.dot(exog, params))\n        464         else:\n        465             return np.dot(exog, params)\n    \n    &lt;__array_function__ internals&gt; in dot(*args, **kwargs)\n    \n    ValueError: shapes (518,2) and (518,) not aligned: 2 (dim 1) != 518 (dim 0)\n\nhow can I plot the predicted line predicted by this model?\n'
"I'm using MSE to measure the loss. In the below code, I implemented loss_mse function that should computes MSE for the input set with the given theta\ndef loss_mse(X,y,theta):\n    length = len(y)\n    predictions = X.dot(theta)\n    error = (1/2*length) * np.sum(np.square(predictions - y))\n    return error\n\nTo test the above function, I wrote the following test cases:\nX = np.array([[2.0, 1.0, 3.0], [3.0, 6.0, 2.0]])\ny =  np.array([1.0, 1.0])\ntheta = np.array([[1.0], [2.0],[1.0]])\nerror = loss_mse(X, y, theta)\nprint(error)\n\nI have to get the answer as 73 but I'm getting 584. I don't understand where I'm doing wrong.\n"
"I have some data in the following format, thousands of rows.\nI want to transpose the data and also change the format to 1 and 0's\nName Codes\nDave DSFFS\nDave SDFDF\nstu  SDFDS\nstu  DSGDSG\n\nI want to retain the Name column in row format, but have the codes column go into Column format instead and have 1 and 0's\n"
"I've a pandas dataset with two columns :\n\nPoblacion (city)\npatologie (pathologies)\n\nI would like to show per each city the most 5-6 frequent patologies\nhlt_downsampled_eng.groupby('Poblacion')['patologie'].count().nlargest(6)\n\nMy idea would be an output like this :\n    Barcellona\n         Fever 5230\n         Rheum 2000\n         headache 300\n         cough 240\n    Tessara\n         diarrhea 5230\n         flu 1000\n         headache 300\n         cough 240\n\nHow can I achieve it with pandas?\nI know that I should do :\nGroup per city &gt; group per pathology &gt; count num of pathologies &gt; aggregate\n"
"I have this following dataframe:\n\n\n\n\n#ID\nScore\n\n\n\n\n1029\n78\n\n\n1229\n89\n\n\n1929\n77\n\n\n2124\n100\n\n\n3120\n89\n\n\n4145\n84\n\n\n\n\nI want to create this following dataframe:\n\n\n\n\n#ID\nClassification\nScore\n\n\n\n\n1029\nFreshman\n78\n\n\n1229\nFreshman\n89\n\n\n1929\nFreshman\n77\n\n\n2124\nSophomore\n100\n\n\n3120\nJunior\n89\n\n\n4145\nSenior\n84\n\n\n\n\nSo the 'Classification' column assignment is dependent on the first digit of the '#ID' column. My original dataframe contains thousands of records, so creating a dictionary is not really an option. Is there a simpler way to do this?\n"
"Does the step for correcting spellings of words have to be done before Lexicon normalization(i.e. stemming, lemmatization) or after? If we do it after lexicon normalization, wouldn't the words already be reduced to their root form if we perform lemmatization?(By passing the POS i.e. Parts Of Speech tag of the word as an argument) so there wouldn't be any use for spell checking after lexicon normalization right?\n"
'For the pandas dataframe below, I want to remove all rows where video and view values are equal and two of the four columns, left, width, top, height are within +-1 of each other. So in the example below, the second row, (width, height) is (20, 14), third row, (width, height) is (21, 15), so one of these two rows should be dropped because those rows share +-1 values in width and height column.\nview     video                      left    width  top    height\nEndzone  57906_000718_Endzone.mp4   372     17      279     17\nEndzone  57906_000718_Endzone.mp4   851     20      273     14\nEndzone  57906_000718_Endzone.mp4   853     21      271     15\nEndzone  57906_000718_Endzone.mp4   855     21.     267     16\nEndzone  57906_000718_Endzone.mp4   857     21.     265     17\nSideline 57906_000718_Sideline.mp4  763     18.     98      26\n...\n\nSample Output, output can vary depending on removing method:\nview     video                      left    width  top    height\nEndzone  57906_000718_Endzone.mp4   372     17      279     17\nEndzone  57906_000718_Endzone.mp4   851     20      273     14\nEndzone  57906_000718_Endzone.mp4   857     21.     265     17\nSideline 57906_000718_Sideline.mp4  763     18.     98      26\n...\n\nThank you!\n'
"I have this csv which contains times measured for some algorithms over some matrices with a different number of cpus each time. Each cpu printed their times so the more cpus I used the more lines I had. (eg. if I used 1 cpu I had one measurement, if I used 2 i would have 2 measurements etc. going up to 64)\nWhat I'm trying to do is plot in a grouped bar chart where each bar in the group is a different 'algo' entry, the x axis is the number of cores ('numtasks' in the csv) and the y axis is the average of all the cores in that group (eg. for 2 cores the average of the times of those 2, 4 cores the average of those 4 etc.).\n(Edit: Kind of like this)\nI tried using pandas for a few hours with pivot(), mean() and concat()enating the results but to no avail.\nI'd be glad if anyone could point me to the right direction, thanks in advance!\n"
"I have a table with 501 column. The first column basically has a value of either -1 or 1. This value is based somehow on the other 500. I'm trying to find the relationship and in order to do that will need some visualization.\n\nNow there are more than 1000 rows and i decided to do a box plot. I want to take the first column and plot six random boxplots each for each values (-1 and 1)\n\nSo the plot is to search the first column, if value is -1 - take the other column and plot a boxplot of their values. But since the values are in a row that is a bit f problem. Also getting it to pick 6 random values at both -1 nd 1 is too. I recon i have to create a new list or dataframe with the 6 values for each (-1 &amp; 1) but how to do that\n\nPhoto attached \nenter image description here\n"
'I have problem with compressed counting values in one dictionary based of value of another one. \n\nI made up such a code below which represent idea of:\n\n1) Extracting the data to list\n\n2) Taking uniqe values for next proccesing\n\n3) Loop for counting the number of males and females only for "accident"\n\nProblem:\n\nWhat is the effective solution to counting the values for each category in uniqe set. I mean what if I had 1000 uniqe categories, I do not want to write 1000 "if\'s"\n\nIt\'s my first question in stackoverflow, that\'s why i\'m sorry for any mistake I\'ve done :) \n\nOriginal data (first 5 rows):\n[\n[\'\', \'year\', \'month\', \'intent\', \'police\', \'sex\', \'age\', \'race\', \'hispanic\', \'place\', \'education\'], \n[\'1\', \'2012\', \'01\', \'Suicide\', \'0\', \'M\', \'34\', \'Asian/Pacific Islander\', \'100\', \'Home\', \'4\'], \n[\'2\', \'2012\', \'01\', \'Suicide\', \'0\', \'F\', \'21\', \'White\', \'100\', \'Street\', \'3\'], \n[\'3\', \'2012\', \'01\', \'Suicide\', \'0\', \'M\', \'60\', \'White\', \'100\', \'Other specified\', \'4\'], \n[\'4\', \'2012\', \'02\', \'Suicide\', \'0\', \'M\', \'64\', \'White\', \'100\', \'Home\', \'4\']\n]\n\n\n\n# Accidents list\naccidents_list = [row[3] for row in data] # list of all accidents\n\nprint(set(accidents_list)) # unique set \n\n\n{\'Homicide\', \'NA\', \'Undetermined\', \'Accidental\', \'Suicide\'}\n\ngender_list = [row[5] for row in data]\nprint(gender_list)\n\n\n[\'M\', \'F\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'M\', \'F\', \'F\', \'M\', \'M\' ....]\n\n# Accidents dict and loop over it\naccidents_gender = {}\n\nfor i, v in enumerate(gender_list):\n    if v not in accidents_gender:\n        accidents_gender[v] = 0\n    if accidents_list[i] == \'Accidental\':\n        accidents_gender[v] += 1\n\nprint(accidents_gender) # printing only values for accidental \n\n\n{\'M\': 1421, \'F\': 218}\n'
"I am trying to figure out a way in which I can calculate quantiles in pandas or python based on a column value? Also can I calculate multiple different quantiles in one output?\n\nFor example I want to calculate the 0.25, 0.50 and 0.9 quantiles for \n\nColumn Minutes in df where it is &lt;= 5 and where it is > 5 and &lt;=10\n\ndf[df['Minutes'] &lt;=5]\n\ndf[(df['Minutes'] &gt;5) &amp; (df['Minutes']&lt;=10)]\n\n\nwhere column Minutes is just a column containing value of numerical minutes\n\nThanks!\n"
"When shell commands are run within a Jupyter Notebook Python Cell, like this:\n\noutput = ! some-shell-command\n\neach line emitted to the standard output (stdout) is captured in a list like IPython data-structure called a SList. For instance:\n\noutput = !echo -e 'line1\\nline2\\nline3'\nprint(output) # A IPython SList data-structure.\n\n\n['line1', 'line2', 'line3']\n\nSometimes, however, you want to preserve the original string output format, without tokenization into a list, like this:\n\nprint(output)\n\nline1\nline2\nline3\n\n\nExample: For structured JSON output -- which is a string containing many newline characters -- you wouldn't want this tokenization to occur.\n\nHow, then, can I execute shell commands in Jupyter Notebooks using the ! operator, and retrieve un-tokenized output (like above)?\n\nIdeally, the solution would be something native in Jupyter Notebooks.\n\nThank you!\n"
"I am learning machine learning and I came across this code.\nI am trying to run the file &quot;Recommender-Systems.py&quot; from the above source. But it throws an error \nValueError: labels ['timestamp'] not contained in axis. How can it be removed?\nHere's a dropbox link of u.data file.\n"
"When I am trying to run below code for list of values I get error:\n\n\n  -> 3088         raise ValueError('index must be monotonic increasing or decreasing')\n\n\nHowever, when I run this code for single value. It executes.\n\nDoes not run:\n\ndef block(host):\n    time_values = failedIP_df.ix[[host]].set_index(keys='index')['timestamp']\n    if (return_seconds(time_values[2:3].values[0]) \\\n      - return_seconds(time_values[0:1].values[0]))&lt;=20:\n        blocked_host.append(time_values[3:].index.tolist())\n\nlist(map(block, failedIP_list))\n\n\nRuns:\n\nhost='unicomp6.unicomp.net'\nblock(host)\n\n\nSample data:\n\nFailedIP_df:\n\n                             timestamp               index\n    host        \n    199.72.81.55              01/Jul/1995:00:00:01   0\n    unicomp6.unicomp.net      01/Jul/1995:00:00:06   1\n    freenet.edmonton.ab.ca  01/Jul/1995:00:00:12     12\n    burger.letters.com      01/Jul/1995:00:00:12     14\n    205.212.115.106         01/Jul/1995:00:00:12     15\n    129.94.144.152          01/Jul/1995:00:00:13     21\n    unicomp6.unicomp.net      01/Jul/1995:00:00:07   415\n    unicomp6.unicomp.net      01/Jul/1995:00:00:08   226\n    unicomp6.unicomp.net      01/Jul/1995:00:00:21   99\n    129.94.144.152          01/Jul/1995:00:00:14     41\n    129.94.144.152          01/Jul/1995:00:00:15     52\n    129.94.144.152          01/Jul/1995:00:00:17     55\n    129.94.144.152          01/Jul/1995:00:00:18     75\n    129.94.144.152          01/Jul/1995:00:00:21     84\n\n\n\n\nFailedIP_list = ['199.72.81.55', '129.94.144.152', 'unicomp6.unicomp.net']\n\n\nSample Output: Index of all hosts who were unssuccessful to login within 20sec after three attempts\n\nblocked_list=[99, 55, 75, 84]\n\n\nI want my code to run for all the values(i.e list of IP addresses) in the list. I would really appreciate some help on this. Thanks.\n"
"I have a pandas df which is mire or less like\n\n        ID  key dist\n   0    1   57  1\n   1    2   22  1\n   2    3   12  1\n   3    4   45  1\n   4    5   94  1\n   5    6   36  1\n   6    7   38  1\n   .....\n\n\nthis DF contains couple of millions of points. I am trying to generate some descriptors now to incorporate the time nature of the data. The idea is for each line I should create a window of lenght x going back in the data and counting the occurrences of the particular key in the window. I did a implementation, but according to my estimation for 23 different windows the calculation will run 32 days. Here is the code\n\ndef features_wind2(inp):\n   all_window = inp\n   all_window['window1'] = 0\n   for index, row in all_window.iterrows():\n      lid = index\n      lid1 = lid - 200\n      pid = row['key']\n      row['window1'] = all_window.query('index &lt; %d &amp; index &gt; %d &amp; key == %d' % (lid, lid1, key)).count()[0]     \n   return all_window\n\n\nThere are multiple different windows of different length. I however have that uneasy feeling that the iteration is probably not the smartest way to go for this data aggregation. Is there way to implement it to run faster?\n"
"I have a for loop i wrote that returns multiple tuples like this from an XML file. It looks something like this:\n\n(a, b, c)\n(a, b, c)\n(a, b, c)\n(a, b, c)\n........\n\n\nThe XML file is such that a, b &amp; c are under the same child tag. And, each of all as, bs,and cs are the same datatype. I tried getting each added to an empty list z= [], with the z.append(yyy) in the same nested for loop, i kept getting something like this:\n\n[(a, b, c), (a, b, c)] \n[(a, b, c)]\n[(a, b, c), (a, b, c) ... ]\n........\n\n\nI understand why it is like that because probably each list is is tied to same child tag in the XML tree. However, what i wanted was just to have a list of tuples like this:\n\n[(a, b, c), (a, b, c), (a, b, c), (a, b, c), ....]\n\n\nto enable me convert that to a Pandas DataFrame. What is the best way to do that? Is there a way to do that directly from the tuples without making up a list? Thank you.\nSnippet of my code is thus:\n\nfor x in root.findall('xyz'):\n    y = x.abc[-1]\n    z = []\n    for s in x.iter('xxx'):\n        yyy = s.text, s.attrib['zzz'], y\n        z.append(yyy)\n    print(z)\n\n\nNote: abc is a child element of xyz, and xxx is another child element of xyz. zzz is an attribute in the xxx element tag. Also note that xxx varies in that document: meaning you could have more than one under the xyz element. \n\nThe XML looks something like this:\n\n&lt;root&gt;\n    &lt;xyz&gt;\n       &lt;abc&gt;...&lt;/abc&gt;\n       &lt;abc&gt;...&lt;/abc&gt;\n       &lt;abc&gt;c&lt;/abc&gt;\n       &lt;xxx zzz='b'&gt;a&lt;/xxx&gt;\n    &lt;/xyz&gt;\n    &lt;xyz&gt;\n       &lt;abc&gt;...&lt;/abc&gt;\n       &lt;abc&gt;...&lt;/abc&gt;\n       &lt;abc&gt;c&lt;/abc&gt;\n       &lt;xxx zzz='b'&gt;a&lt;/xxx&gt;\n       &lt;xxx zzz='b'&gt;a&lt;/xxx&gt;\n    &lt;/xyz&gt;\n    ......\n &lt;/root&gt;\n\n"
'I am working on an unbalanced data, using undersampling, I have made the both classes in the same proportion.\n\nX_undersample dataframe (984,28)\ny_undersample dataframe(984,1)\n\n\nI am using randomforest classifier, in order to find the best parameter n_estimators I am using cross-validation. here is the code below.\n\nj_shout=range(1,300)\nj_acc=[]\nfor j in j_shout: \n   lr = RandomForestClassifier(n_estimators = j, criterion = \'entropy\', random_state = 0)\n   score=cross_val_score(lr,X_undersample,y_undersample,cv=10,scoring=\'accuracy\')\n   print (\'iteration\',j,\':cross_validation accuracy=\',score)\n   j_acc.append(score.mean())\n\n\nnow when I run this I am getting the following error.\n\nFile "&lt;ipython-input-43-954a9717dcea&gt;", line 5, in &lt;module&gt;\n    score=cross_val_score(lr,X_undersample,y_undersample,cv=10,scoring=\'accuracy\')\n\n  File "D:\\installations\\AC\\lib\\site-packages\\sklearn\\cross_validation.py", line 1562, in cross_val_score\n    cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n\n  File "D:\\installations\\AC\\lib\\site-packages\\sklearn\\cross_validation.py", line 1823, in check_cv\n    cv = StratifiedKFold(y, cv)\n\n  File "D:\\installations\\AC\\lib\\site-packages\\sklearn\\cross_validation.py", line 569, in __init__\n    label_test_folds = test_folds[y == label]\n\nIndexError: too many indices for array\n\n\nI try changing the n_estimators to smaller values but it still showing the same error \n'
"I am using a module called pyhaystack to retrieve data (rest API) from a building automation system based on 'tags.' Python will return a dictionary of the data. Im trying to use pandas with an If Else statement further below that I am having trouble with. The pyhaystack is working just fine to get the data...\n\nThis connects me to the automation system: (works just fine)\n\nfrom pyhaystack.client.niagara import NiagaraHaystackSession\nimport pandas as pd\n\nsession = NiagaraHaystackSession(uri='http://0.0.0.0', username='Z', password='z', pint=True)\n\n\nThis code finds my tags called 'znt', converts dictionary to Pandas, and filters for time: (works just fine for the two points)\n\nznt = session.find_entity(filter_expr='znt').result\nznt = session.his_read_frame(znt, rng= '2018-01-01,2018-02-12').result\nznt = pd.DataFrame.from_dict(znt)\n\n\nznt.index.names=['Date']\nznt = znt.fillna(method = 'ffill').fillna(method = 'bfill').between_time('08:00','17:00')\n\n\nWhat I am most interested in is the column name, where ultimately I want Python to return the column named based on conditions:\n\nprint(znt.columns)\nprint(znt.values)\n\n\nReturns:\n\nIndex(['C.Drivers.NiagaraNetwork.Adams_Friendship.points.A-Section.AV1.AV1ZN~2dT', 'C.Drivers.NiagaraNetwork.points.A-Section.AV2.AV2ZN~2dT'], dtype='object')\n\n[[ 65.9087  66.1592]\n [ 65.9079  66.1592]\n [ 65.9079  66.1742]\n ..., \n [ 69.6563  70.0198]\n [ 69.6563  70.2873]\n [ 69.5673  70.2873]]\n\n\nI am most interested in this name of the Pandas dataframe. C.Drivers.NiagaraNetwork.Adams_Friendship.points.A-Section.AV1.AV1ZN~2dT\n\nFor my two arrays, I am subtracting the value of 70 for the data in the data frames. (works just fine)\n\nznt_sp = 70\n\ndeviation = znt - znt_sp\n\ndeviation = deviation.abs()\n\ndeviation\n\n\nAnd this is where I am getting tripped up in Pandas. I want Python to print the name of the column if the deviation is greater than four else print this zone is Normal. Any tips would be greatly appreciated..\n\nif (deviation &gt; 4).any():\n    print('Zone %f does not make setpoint' % deviation)\n\nelse:\n    print('Zone %f is Normal' % deviation)\n\n\nThe columns names in Pandas are the:\nC.Drivers.NiagaraNetwork.Adams_Friendship.points.A-Section.AV1.AV1ZN~2dT\n\n\n"
"I'm new to data science and trying to do some data wrangling with python 2.7 in iPython notebook. A tutorial I was following for my first project asked me to replace all NaN intputs with Y or N. But I'd like to consider another approach where I can 1st look at all the rows with NaN inputs for a specific column so that I can utilize the fillna() better. \n\nIs there a code that lets me extract such rows? \n\nI have 13 rows (loan_id, gender, married, credit_history, etc.)\nMost of the rows do not have NaN values and my interest is in credit_history. How do I extract all rows have NaN values under credit history?\n\nI'd like the output to be something similar to:\n\nloan_id gender married credit_history loan_status\n1         M      Y          NaN           Y\n2         F      Y          NaN           N\n3         M      Y          NaN           Y\n4         M      Y          NaN           N\n5         F      N          NaN           Y\n\n"
'import re\n\ndef main():\n    slang_file = [line.strip().split(":") for line in open("dictionary.txt")]\n    slang = {k:v for k, v in slang_file}\n    sentence = input("enter a sentence to translate\\n")\n    print(re.sub(r"\\w+", lambda m: slang.get(m.group(0), m.group(0)), sentence))\n\nmain()\n\n\nDictionary  contain of this:\n\nmeee:me\n\nr:are\n\n\nBut I always got this type of error:\n\nslang = {k:v for k, v in slang_file}\nValueError: too many values to unpack (expected 2)\n\n\nThe output that i want is like this:\n\nr u hungry\n\nare you hungry\n\n'
'This is my Pandas Frame:\n\n       istat    cap               Comune\n0       1001  10011               AGLIE\'\n1       1002  10060              AIRASCA\n2       1003  10070         ALA DI STURA\n\n\nI want to reproduce the equivalent SQL query:\n\nSelect cap\nfrom DataFrame\nwhere Comune = \'AIRASCA\'\n\n\nObtaining:\n\ncap\n10060\n\n\nI tried to achieve this with dataframe.loc() but i cannot retrieve what i need.\n\nAnd this is my Python code:\n\nimport pandas as pd\nfrom lxml import etree\nfrom pykml import parser\n\ndef to_upper(l):\n    return l.upper()\n\n\nkml_file_path = \'../Source/Kml_Regions/Lombardia.kml\'\nexcel_file_path = \'../Source/Milk_Coverage/Milk_Milan_Coverage.xlsx\'\nzip_file_path = \'../Source/ZipCodes/italy_cap.csv\'\n\n# Read zipcode csv\nzips = pd.read_csv(zip_file_path)\nzip_df = pd.DataFrame(zips, columns=[\'cap\',  \'Comune\']).set_index(\'Comune\')\nzips_dict = zips.apply(lambda x: x.astype(str).str.upper())\n\n# Read excel file for coverage\ndf = pd.ExcelFile(excel_file_path).parse(\'Comuni\')\nx = df[\'City\'].tolist()\ncities = list(map(to_upper, x))\n\n#-----------------------------------------------------------------------------------------------#\n# Check uncovered\n# parse the input file into an object tree\nwith open(kml_file_path) as f:\n  tree = parser.parse(f)\n\n# get a reference to the "Document.Folder" node\nuncovered = tree.getroot().Document.Folder\n\n# iterate through all "Document.Folder.Placemark" nodes and find and remove all nodes\n# which contain child node "name" with content "ZONE"\n\nfor pm in uncovered.Placemark:\n        if pm.name in cities:\n            parent = pm.getparent()\n            parent.remove(pm)\n        else:\n            pass\n\n# convert the object tree into a string and write it into an output file\nwith open(\'../Output/Uncovered_Milkman_LO.kml\', \'w\') as output:\n    output.write(etree.tostring(uncovered, pretty_print=True))\n\n#---------------------------------------------------------------------------------------------#\n\n# Check covered\nwith open(kml_file_path) as f:\n    tree = parser.parse(f)\n\ncovered = tree.getroot().Document.Folder\n\nfor pmC in covered.Placemark:\n        if pmC.name in cities:\n            pass\n        else:\n            parentCovered = pmC.getparent()\n            parentCovered.remove(pmC)\n\n# convert the object tree into a string and write it into an output file\nwith open(\'../Output/Covered_Milkman_LO.kml\', \'w\') as outputs:\n    outputs.write(etree.tostring(covered, pretty_print=True))\n\n# Writing CAP\nwith open(\'../Output/Covered_Milkman_LO.kml\', \'r\') as f:\n    in_file = f.readlines()  # in_file is now a list of lines\n\n# Now we start building our output\nout_file = []\ncap = \'\'\n\n#for line in in_file:\n#    out_file.append(line)  # copy each line, one by one\n\n# Iterate through dictionary which is a list transforming it in a itemable object\nfor city in covered.Placemark:\n    print zips_dict.loc[city.name, [\'Comune\']]\n\n\nI cannot understand the errors python is giving me, what i\'m doing wrong? Technically i can look for a key by finding a value in pandas, is it correct?\n\n\n\nI think is not similar to the possible duplicate question because i\'m asking to retrieve a single value instead of a column.\n'
"I have a dataset, df.\n\nI extracted another dataset from df, df_rec, based on a certain condition.\n\nI can access the indexes of df_rec by df_rec.index.\n\nNow, I want to create a column in df, where the index in df if matches with indexes in df_rec should be populated as 1 otherwise 0.\n\nAny help, will be appreciated. \n\nI am thinking, like, which throws error.\n\ndf['reccurences'] = 0\ndf['reccurences'][df.index in df_rec.index] = 1\n\n"
"I have a list and each element of the list is a 2D matrix.\n\nnp.shape(mylist)\n&gt;&gt;(5000,)\n\nnp.shape(mylist[0])\n&gt;&gt;(62,62)\n\n\ntype(mylist)\n&gt;&gt; list\n\ntype(mylist[0])\n&gt;&gt; numpy.ndarray\n\n\nNow, I'm trying to create a list of indexes that appear in the index list:\n\ny_train = [mylist[i] for i in index]\n\n\nThe problem is that sometimes it shows a 1D shape and sometimes 3D (e.g. (nx,) or (nx,ny,nz))\n\nFor example:\n\nyy = []\nyy.append(mylist[17])\nyy.append(mylist[1381])\n\nprint(np.shape(yy))\n\n&gt;&gt; (2,)\n\nyy = []\nyy.append(mylist[17])\nyy.append(mylist[1380])\n\nprint(np.shape(yy))\n\n&gt;&gt; (2, 513, 513)\n\n\nAny idea why? maybe the fact that mylist[17] and mylist[1380] are of the same shape and mylist[17] and mylist[1381] are of different shape?\n"
"I have a 4000 record plus pandas dataframe with records for individual events by time stamp\n\nTimestamp            Date        Holiday  DayOfWeek\n2017-01-01 02:25:00  2017-01-01  True      Monday\n2017-01-01 12:25:00  2017-01-01  True      Monday\n2017-01-02 03:45:00  2017-01-02  False     Tuesday\n2017-01-02 15:55:00  2017-01-02  False     Tuesday\n2017-02-03 01:01:00  2017-02-03  False     Thursday\n2017-02-03 4:25:00   2017-02-03  False     Thursday\n2017-04-03 4:25:00   2017-04-03  True      Monday\n\n\nWhat I'm trying to do is compare the means of events per day by day of the week and if it was on a holiday.\nSo for each day of the week, compare the the average number of events per day for when that day was a holiday vs when that day was NOT a holiday.  \n\nevents.groupby(['DayOfWeek', 'Holiday']).count()\n\n\nWill get me the number of events for each day of the week by holiday\n\nDayOfWeek Holiday  Count\nMonday    True     50\n          False    34\nTuesday   True     32\n          False    23\n...\n\n\nBut I can't figure out how to combine this with the number of events per individual date\n\nevents.groupby('Date').count()\nDate        Count\n01-01-2017  2\n01-02-2017  2\n01-03-2017  4\n....\n\n\nI want a data frame more like\n\nDayOfWeek Holiday  Mean\nMonday    True     4.5\n          False    3.23\nTuesday   True     2.1\n          False    3.2\n...\n\n\nAnd then ideally make a bar chart from it.  \n\nBut can't figure out how to combine the operations to create what I want first.  \n"
'I want to add "TOTAL PLAYING TIME: 00:0:0" at the bottom,the commands I tried have been updated below:\n\ntotalmins = 0\ntotalsecs = 0\nmins, secs = map(int, record[3].split(":")) \ntotalmins = totalmins + (totalsecs // 60)\ntotalsecs = totalsecs % 60\n\n\nshows error something wrong with the indentation\n'
'I have some nodes that are being generated from a numpy adjacency matrix. I also have an array of labels for said nodes already where node 0 in the matrix corresponds to label 0 in the array. What is the easiest way to apply those labels to the nodes in the graph?\n'
"I have 10 arrays. Each of them represents one data point(input). The shape of the arrays are (16,3), (34,3) etc.. Since LSTM needs 3dim data I reshaped each of those 10 arrays. Example: if it was (16,3) now it is (1,16,3). I tried to get ((1,16,3),(1,34,3),etc..) to be my array shape, in other words 10 arrays in one numpy array with each of shape (1,something,3). When I feed the data with all 10 arrays as one I get the following error:\n\n\n  Error when checking model input: the list of Numpy arrays that you are\n  passing to your model is not the size the model expected. Expected to\n  see 1 array(s), but instead got the following list of 10 arrays.\n\n\nBut if I feed one of those arrays with one label it works and overfits(as it should).\nIf batch_size=1, shouldn't the program take one of those 10 samples to train on?\n\nHere's my code:\n\nimport os\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\ndata = []\ndirectory = 'realData'\nfor filename in os.listdir(directory):\n    data.append(np.load('realData/' + filename))\n\nfor i in range(len(data)):\n    data[i] = data[i].reshape(1,data[i].shape[0],3)\n\nsad = np.array([[0]] * 2)\nokay = np.array([[1]] * 3)\nhappy = np.array([[2]] * 2)\nperfect = np.array([[3]] * 3)\n\nlabels = np.concatenate([sad,okay,happy,perfect],axis=0)\n\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(None,3)))\nmodel.add(Dense(1))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nprint('Train...')    \nmodel.fit(data, labels,\n          batch_size=1,\n          epochs=15,\n          validation_data=(data, labels))\n\nscore, acc = model.evaluate(data, labels, batch_size=1)\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\n"
"Suppose I have a large data-set(in CSV formate) like the following :\n\n   Country  Age  Salary Purchased\n0   France   44   72000        No\n1    Spain   27   48000       Yes\n2  Germany   30   54000        No\n3    Spain   38   61000        No\n4  Germany   40   45000       Yes\n5   France   35   58000       Yes\n6    Spain   75   52000        No\n7   France   48   79000       Yes\n8  Germany   50   83000        No\n9   France   37   67000       Yes\n\n\nNow how can i swap all the values for a selected column Randomly ? For Example \ni want to swap all the values of the first column 'Country' randomly.\n\nLooking for your suggestion. Thanks in advance !\n"
"I was working on the MovieLens Dataset for recommendation-engine example. I see that we can create a user-item matrix to calculate the similarity between them where we have the the users as index (or row number) and item (movies) as columns and the ratings on each movie by each user as the data in the matrix. I believe that is what the following code is doing and it looks powerful however, it  is not clear to me how it is actually working. Is there any other method we can use than itertuples (simple pivot or transpose? Any advantage or disadvantage?)  \n\nimport pandas as pd\nimport numpy as np\n\nr_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv('ml-100k/u.data', sep='\\t', \nnames=r_cols,encoding='latin-1')\nn_users = ratings.user_id.unique().shape[0]\nn_items = ratings.movie_id.unique().shape[0]\n\ndata_matrix = np.zeros((n_users, n_items))\nfor line in ratings.itertuples():\n    data_matrix[line[1]-1, line[2]-1] = line[3]\n\n"
"My goal is to calculate the amount of purchases made by costumers before the current date.\n\nEx:\n\nID         Date           Purchases_Made      Purchase_Mades_So_Far(Result)    \nClientA    Jan/2019       5                   5\nClientA    Feb/2019       8                   13   \nClientB    Jan/2019       1                   1\nClientB    Feb/2019       3                   4\n\n\nI have tried lambda expressions and group by but none of them worked.\n\nMy best try so far is:\n\n    for index, row in df.iterrows():\n      for index2, compare in df.iterrows():\n        row['Result'] = teste[(compare['ID'] == row['ID']) &amp;\n                              (compare['Date'] &lt; row['Date'])                    \n                             ]['Purchases_Made'].sum()\n\n"
"Using machine learning I would like to identify features that influence net revenue and make conclusions from data based on that. The data set is a car sharing company data (like Turo). Data set contains ~80000 rows 14 columns.\n\nI have difficulty to build a EDA especially with ML algorithm to use to find out features that influence on net_revenue.\n\n\n\nWhat I did so far\n\n\nI did correlation matrix analysis on this data and find out 'youth\ndriver fee' has the most correlated feature to 'net_revenue' (\nI kept make and model columns out of the analysis because there are so many makes and models and its hard to predict their effect on the net_revenue)\nI wanted to see this correlation is relevant with some ML algorithms such as Logistic regression and Randomforest. To further applying RandomForest ML to verify this correlation I converted categorical variables (payment_type, returning_guest and returning_host) to the dummy variables (0's and 1's)\n\n\nSo I tried to apply these two models by following this post\n\nLogisticRegression\n\ncols=['driver_age', 'completed_trips', 'vehicle_price', 'lead_time', 'trip_length', \n              'trip_revenue', 'youth_driver_fee', 'insurance_fee', 'delivery_fee', 'returning_quest_First_time','returning_quest_Repeat','returning_host_First_time','returning_host_repeat']\n\n            X=data[cols]\n            y=data['net_revenue']\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n\n*default settings of LogisticRegression\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=’ovr’, n_jobs=1, penalty=’l2', random_state=None, solver=’liblinear’, tol=0.0001, verbose=0, warm_start=False)\n\n**The IPython notebook freezes after executing the code above and it looks like it would never output something.So I have to restart the kernel.\n\nRandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\n\n\n**Same problem!\n\nMy Questions\n\n\nHow can I a ML model for finding features that influence net revenue? Is there any resource that the same problem addressed ? Kaggle competitions definaltely fine or maybe a medium post.\n\n\nI found one dataset to predict features on target value but target value look like categorical mine is continuous. \nfrom https://www.kaggle.com/prasadkevin/prediction-of-quality-of-wine\n\n\nto use LogisticRegression and RandomForest, Is net_revenue has to be categorical variable ?\nDo you happen to know and similar dataset on Kaggle:) ? because I could not find any correlated ML flow like this one!\n\n\nAny advice would be appreciated!\n\nThx\n"
'Hello I have this pandas dataframe:\n\n\nKey      Predictions    \nC10D1   1\nC11D1   8\nC11D2   2\nC12D1   2\nC12D2   8\nC13D1   3\nC13D2   9\nC14D1   4\nC14D2   9\nC15D1   8\nC15D2   3\nC1D1    5\nC2D1    7\nC3D1    4\nC4D1    1\nC4D2    9\nC5D1    3\nC5D2    2\nC6D1    1\nC6D2    0\nC7D1    8\nC7D2    6\nC8D1    3\nC8D2    3\nC9D1    5\nC9D2    1\n\n\nI want to concatenate each cells from "Prediction" column where the "Key" matches up to 4 character.\nFor Example... in the "Key" column I have "C11D1" and "C11D2".. as they both contain "C11" i would like to concatente rows from prediction column that has "C11D1" and "C11D2" as index ..\nThus the result Should be:\n\n       Predictions\nKey \nC10     1\nC11     82\nC12     28\nand so on\n\n'
"I have some code for a school project I'm working on, and I need to create scatter plots for data on the Median household income by county and each county's investment into sustainable energy practices. For example, the first five lines of my data look like this:\n\nx = [39620, 43170, 76633, 50449, 50777]\ny = [49625, 177083, 2026280, 460648, 137874]\n\n\nThe data was gathered fine, however, when I try to create a scatter plot with the data the Y-axis compressed itself to being 1,2,3,4 instead of the actual values which are in the millions.\n\nply.scatter(x, y, norm=None)\n\n\nThe graph then appears to look like this:\n\n\n\nI've looked online and can't figure this out, any help would be greatly appreciated thank you so much! \n"
"I am trying to calculate WoE by hand but I am not able to get the same results as calculated by category_encoders WOEEncoder. Here's my dataframe for which I want to calculate scores:\n\ndf = pd.DataFrame({'cat': ['a', 'b', 'a', 'b', 'a', 'a', 'b', 'c', 'c'], 'target': [1, 0, 0, 1, 0, 0, 1, 1, 0]})\n\n\nThis is the code that I use to calculate WoE Score\n\nwoe = WOEEncoder(cols=['cat'], random_state=42)\nX = df['cat']\ny = df.target\nencoded_df = woe.fit_transform(X, y)\n\n\nThe result for the same is:\n\n0   -0.538997\n1   0.559616\n2   -0.538997\n3   0.559616\n4   -0.538997\n5   -0.538997\n6   0.559616\n7   0.154151\n8   0.154151\n\n\nSo,\n'a' is encoded as -0.538997\n'b' is encoded as 0.559616\n'c' is encoded as 0.154151\n\nWhen I calculate the scores by hand, they are differnt, I take \n\nln(% of non events / % of events).\n\n\nSay for example, for calculating WoE of a,\n\n% of non events = targets which are 0 for 'a'/ total targets for group 'a'\n\n\nSo, % of non events = 3/4 = 0.75\n\n% of events = targets which are 1 for 'a' / total targets for group 'a'\n\n\nSo, % of events = 1/4 = 0.25\n\nNow, 0.75/0.25 = 3\n\n\n\nTherefore, WoE(a) = ln(3) = 1.09 which is different from the encoder above.\n"
"I have 2 sets of latitude/longitude coordinates, 4 arrays total. My goal was to find the distance between one group (airbnb_coord) and the closest member of the second group (station_coord). I've written an nested for loop to accomplish this\n\nshortest_distance = make_array()\nfor air_lat, air_long in zip(airbnb_coord[0],airbnb_coord[1]):\n    for stat_lat, stat_long in zip(station_coord[0],station_coord[1]):\n        distances = make_array()\n        distances = np.append(distances, np.sqrt(((air_lat-stat_lat)**2)+((air_long-stat_long)**2)))\n        shortest_distance = np.append(shortest_distance, min(distances))\n\n\nThe problem is, airbnb_coord is 40,000 long, and station_coord is 500 long. This has been running for more than an hour now.\n\nCan someone tell me if there is a better way? I'm pretty weak at function application, but I'm sure there is a strategy that uses such.\n"
"Dataframe 1\n\n1  C1  C2  C3  .  .  .  C85\n2  \n3  \n4  \n.\n.\n800000  .  .  .  .  .  .\n\n\nColumns with missing values across rows\n\n0       32\n100     10\n200     7\n300     7\n400     6\n1000    5\n2000    3\n3000    3\n9000    3\n12000   2\n13000   1\n15000   1\n20000   1\n30000   1\n40000   1\n50000   1\n60000   1    \n\n\nDataframe 2\n\nattribute  missing_or_unknown\nC1         [-1,X]\nC2         [XX]\n.          .\n.          .\nC85        []\n\n\nMissing values sorted by value_Counts()\n\n[-1]        26\n[-1,9]      17\n[-1,0]      16\n[0]         12\n[]          10\n[-1,0,9]     1\n[-1,XX]      1\n[-1,X]       1\n[XX]         1\n\n\nNeed\nDataframe 1 is the master table that has many missing or unknown values that needs to be cleaned up.\nHowever that determination needs to happen by referencing dataframe 2 and using those encoded indicators in missing_or_unknown column\n\nApproach\nTo be able to do that, I was trying to concat the 2 dataframes and see if i can add that missing_or_unknown column to dataframe 1 before i could proceed and use replace function to replace those indicators with np.nan \n\nQuestion \nHow do i perform concatenation when the 2 dataframes don't have same number of rows? Basically 1st dataframe's columns are rows in 2nd dataframe?\n"
'I made a Kmeans algorithm and plot the result. Everything is going well but I want to know which individuals are in which group. is there a way (and what it is) to get individuals from a particular group?\nThank you for your answers.\n'
'The 5th column of my DataFrame is a list of floats. I want to replace the list with the maximum value from the list. How could I do so?\n\nI\'m trying this but I get an error:\n\nimport pandas as pd\nimport numpy as np\n\ncolNames = [\'unixTime\', \'sampleAmount\',\'Time\',\'samplingRate\', \'Data\']\n\ndata = pd.read_csv("project_fan.csv",  sep = \';\', error_bad_lines = False, names = colNames) \nprint(data.head())\ndata[\'Data\'] = [float(x) for x in data.Data.values]\ndata[\'Data\'] = [np.array(x).mean()for x in data.Data.values]\n\n\nTraceback (most recent call last):\n  File "new.py", line 9, in &lt;module&gt;\n    data[\'Data\'] = [float(x) for x in data.Data.values]\nValueError: could not convert string to float: [1618.6294555664062, 1619.0826416015625, 1620.0897216796875, 1620.0393676757812, 1620.0393676757812, 1620.240783691406, 1620.391845703125, 1620.0897216796875, 1619.435119628906, 1620.4925537109373, 16\n\n\nAlso tried to use astype(float).mean but doesn\'t work.\n\nSample DataFrame:\n\n       unixTime  sampleAmount  Time  samplingRate   Data\n0  1.556891e+09         16384   340  48188.235294  [1618.6294555664062,1619.0826416015625,1620.489622]\n1  1.556891e+09         16384   341  48046.920821  [1619.78759765625,1619.0826416015625,1620.49754]\n\n'
"i have this dataframe:\n\n    Timestamp   DATA0   DATA1   DATA2   DATA3   DATA4   DATA5   DATA6   DATA7\n0   1.478196e+09    219 128 220 27  141 193 95  50\n1   1.478196e+09    95  237 27  121 90  194 232 137\n2   1.478196e+09    193 22  103 217 138 195 153 172\n3   1.478196e+09    181 120 186 73  120 239 121 218\n4   1.478196e+09    70  194 36  16  81  129 95  217\n... ... ... ... ... ... ... ... ... ...\n242 1.478198e+09    15  133 112 2   236 81  94  252\n243 1.478198e+09    0   123 163 160 13  156 145 32\n244 1.478198e+09    83  147 61  61  33  199 147 110\n245 1.478198e+09    172 95  87  220 226 99  108 176\n246 1.478198e+09    123 240 180 145 132 213 47  60\n\n\nI need to create a temporal features like this:\n\n    Timestamp   DATA0   DATA1   DATA2   DATA3   DATA4   DATA5   DATA6   DATA7\n0   1.478196e+09    219 128 220 27  141 193 95  50\n1   1.478196e+09    95  237 27  121 90  194 232 137\n2   1.478196e+09    193 22  103 217 138 195 153 172\n3   1.478196e+09    181 120 186 73  120 239 121 218\n4   1.478196e+09    70  194 36  16  81  129 95  217\n\nTimestamp   DATA0   DATA1   DATA2   DATA3   DATA4   DATA5   DATA6   DATA7\n1   1.478196e+09    95  237 27  121 90  194 232 137\n2   1.478196e+09    193 22  103 217 138 195 153 172\n3   1.478196e+09    181 120 186 73  120 239 121 218\n4   1.478196e+09    70  194 36  16  81  129 95  217\n5   1.478196e+09    121 69  111 204 134 92  51  190\n\n    Timestamp   DATA0   DATA1   DATA2   DATA3   DATA4   DATA5   DATA6   DATA7\n2   1.478196e+09    193 22  103 217 138 195 153 172\n3   1.478196e+09    181 120 186 73  120 239 121 218\n4   1.478196e+09    70  194 36  16  81  129 95  217\n5   1.478196e+09    121 69  111 204 134 92  51  190\n6   1.478196e+09    199 132 39  197 159 242 153 104\n\n\nHow can I do this automatically?\nwhat structure should I use, what functions?\nI was told that the dataframe should become an array of arrays\nit's not very clear to me\n"
'I have data of electricity usage. During the power outrages the data is \'0\'. \nI want to replace those 0\'s with the data of same time during the past week. Which is 168 indexes ahead or behind in the dataset.\n\nIn the below code, I am saving the index of all the zeros. Running a loop which will place the value that lies 168 indexes ahead in the dataset at the current index.\n\nIndex_Zero = data[data["Total"]==0].index.to_list() #Output = list of indexes where all the zeros lie\n\nprint(Index_Zero[0]) #Output = 2\n\nfor i in Index_Zero:\n    data.loc[(Index_Zero[i]), \'Total\']=data.loc[(Index_Zero[i+168]), \'Total\']\n\n\nAlso, if I print \n\ndata.loc[(Index_Zero[0]), \'Total\']=data.loc[(Index_Zero[2]), \'Total\']\nprint(data.loc[(Index_Zero[0]), \'Total\'])\nOutput: 0.0\n\n\nDataSet:\n\n           Date         Time     Total\n0     23-Jan-2019  12:00:00 AM  18343.00\n1     23-Jan-2019  01:00:00 AM  18188.00\n2     23-Jan-2019  02:00:00 AM      0.00\n3     23-Jan-2019  03:00:00 AM  23394.00\n4     23-Jan-2019  04:00:00 AM  20037.00\n\n'
'I have a dictionary wherein the key is a unique name and the value is a list of non-unique names.\nFor domain knowledge sake, the keys are Tableau workbooks and the value is a list of tables the workbook connects to.\n\nWhat I am trying to do is return, for each key, every other key that has at least three matching values. Doing so will basically allow me to find workbooks that are overlapping data by using the same tables.\n\nCurrently, I am able to find all keys that match a specific value by doing the following:\n\nkeys = [key for key, value in intersect_dict.items() if \'VOLUME_DIMENSION\' in value]\nkeys\n\nvalues = [value for key, value in intersect_dict.items() if \'VOLUME_DIMENSION\' in value]\nvalues\n\n\nThe output of keys is:\n\n[\'(SAN) STORAGE GROUP INVENTORY AND CAPACITY\',\n \'(SAN) STORAGE GROUP INVENTORY AND CAPACITY V2\',\n \'SAN INVENTORY AND CAPACITY\']\n\n\nAnd the output of values is:\n\n[[\'VOLUME_DIMENSION\',\n  \'EXTENDED_DATA\',\n  \'VOLUME_HISTORY_CAPACITY_FACT\',\n  \'HOST_DIMENSION\',\n  \'STORAGE_DIMENSION\',\n  \'DATE_DIMENSION\'],\n [\'STORAGE_DIMENSION\',\n  \'DATE_DIMENSION\',\n  \'VOLUME_DIMENSION\',\n  \'HOST_DIMENSION\',\n  \'VOLUME_HISTORY_CAPACITY_FACT\',\n  \'EXTENDED_DATA\'],\n [\'VOLUME_HISTORY_CAPACITY_FACT\',\n  \'HOST_DIMENSION\',\n  \'EXTENDED_DATA\',\n  \'DATE_DIMENSION\',\n  \'STORAGE_DIMENSION\',\n  \'VOLUME_DIMENSION\']]\n\n\nIs there a possible way that I can do essentially the same thing except instead of \n\nif \'VOLUME_DIMENSION\' in value I have if values in value match 3 times or more?\n\nPlease let me know if more info is needed.\n\nEdit1: Below is the input dictionary excerpt requested:\n\n{\'(SAN) STORAGE GROUP INVENTORY AND CAPACITY\': [\'VOLUME_DIMENSION\',\n  \'EXTENDED_DATA\',\n  \'VOLUME_HISTORY_CAPACITY_FACT\',\n  \'HOST_DIMENSION\',\n  \'STORAGE_DIMENSION\',\n  \'DATE_DIMENSION\'],\n \'(SAN) STORAGE GROUP INVENTORY AND CAPACITY V2\': [\'STORAGE_DIMENSION\',\n  \'DATE_DIMENSION\',\n  \'VOLUME_DIMENSION\',\n  \'HOST_DIMENSION\',\n  \'VOLUME_HISTORY_CAPACITY_FACT\',\n  \'EXTENDED_DATA\'],\n\n\nThe requested output would be something like:\n\n{\'(SAN) STORAGE GROUP INVENTORY AND CAPACITY\': workbook1, workbook7, workbook8}\n\n\nThe "workbooks" shown as values would be the workbooks who have three or more matching values with that key.\n\nEdit2: Sorry for bad data format explanation. Attempting to clarify that here.\n\nd = { \n    \'item1\': [\'A\', \'B\', \'C\'], \n    \'item2\': [\'A\', \'B\', \'C\', \'D\'], \n    \'item3\': [\'A\', \'C\', \'D\'], \n    \'item4\': [\'B\', \'C\', \'D\', \'E\'], \n    \'item5\': [\'A\', \'B\', \'C\'], \n    \'item6\': [\'A\', \'B\', \'C\', \'E\'], \n    }\n\nResults = { \n    \'item1\': [\'item2\', \'item5\', \'item6\'] \n    \'item2\': [\'item1\', \'item5\', \'item6\'] \n    }\n\n\nIn the above example, d would be my overall dataset in dictionary form and Results are what I would like for the output to be. So it would let me target which items are sharing data. Or in this case, sharing letters.\n'
"i am trying to combine columns from excel sets and when i combine the columns it places a NaN were the column is empty. how do i get rid of the NaN without dropping the whole row?    \n\nimport os\nimport pandas as pd\nimport numpy as np\npath = os.getcwd()\nfiles = os.listdir(path)\nfiles_xls = [f for f in files if f[-4:] == 'xlsx']\ndf = pd.DataFrame()\nfor f in files_xls:\n  qw = pd.read_excel(f)\n  df = df.append(qw)\n  cf = df.iloc[:, df.columns.str.contains('address1|address2|city|state|zip|Location Address', case=False)]\n  vf = df['address1'].map(str) + '-' + df['address2'].map(str) + '-' + df['city'].map(str) + '-' + df['state'].map(str) + '-' + df['zip'].map(str)\nexport_csv = vf.to_csv('dataframe.csv', index=None, header=True)\n\n"
'I have a repository that uses python scripts and a Makefile. I want to have a setup procedure that\nallows them to easily set up an environment and copy in the necessary data files from our server.\n\nThe problem with including the source data files in the Makefile is that the company server uses \nspaces in the drive name, which make doesn\'t like very much, so I can list those files as dependencies\nfor the target output file.\n\nMy current Makefile basically does only the following:\n\n.PHONY : all\nall : output.csv\n\n.PHONY : copy_data_to_local_folder\ncopy_data_to_local_folder :\n    python copyfile.py "V:\\\\Server Path\\\\With Spaces\\\\Inputs 1.csv" local/inputs1.csv\n    python copyfile.py "V:\\\\Server Path\\\\With Spaces\\\\Inputs 2.csv" local/inputs2.csv\n\noutput.csv : combine_data.R local/inputs1.csv local/inputs2.csv\n    Rscript $^ $@\n\n\nThe copy_data_to_local_folder part is just to get the data to the local directory, but it isn\'t included \nin the DAG leading to the production of output.csv (i.e. all : output.csv copy_data_to_local_folder) or else\nthe target would need to run everytime.\n\nMy solution ideas are the following, but I\'m not sure what\'s best practice:\n\n\nUse a different make tool. I could use Luigi in Python or Drake in R, but I would prefer to keep \nthe tool somewhat more generalized.\nRun a setup script to copy in files. I assume there would be a way to run the file copying scripts\nas part of the environment setup, but I am unfamiliar with how to do this.\n\n\nI am not sure about the best way to do this. I want to be able to share the code with a co-worker and have them\nbe able to get up and running on their system without too much messing around to configure. Is there a best\npractice for this situation?\n'
'I have multiple data sets and I want to visually show "how much bigger" one is compared to the other. Therefore, I am thinking about something like planet models for kids in children\'s books where the sun (a large circle) is drawn next to the earth (smaller circle) to show how the sizes compare.\n\nIs there a simple way in python of how I can achieve this in a jupyter notebook? My inputs would just be an array of numbers e.g. [50, 100].\n'
"I would like to create a new row consisting of the percentage of the each column's ones and minus ones.\n\nalldata_2.head()\n\nConditions          -1       1\nNormalBlink1400     48      108\nNormalBlink2000     74      124\nNormalBlink3000     77      147\nNormalBlink4000     67      150\nNormalNoBlink1400   40      119\n\n\nthe two rows I want to add: perc_one = (one / one + minus_one)\n                            perc_minus_one = (minus_one / one + minus_one)\n"
'I found a lot of answers for this question but not for what I want to do specifically.\nI have a a lot of csv files, some a few lines somme more than 200mo, for a total of ~70Go of data, and I\'d like to convert them into hdf5 files.\n\nI found ways that create a big dataframe and concatenate them all together, but my data is too large to fit in a single dataframe, using the solution shown here.\nhttps://datascience.stackexchange.com/questions/53125/file-converter-from-csv-to-hdf5\n\nI\'m trying to do something like 1 dataframe per file and convert them all into hdf5 files so that I have the same amount of h5 files and csv but I don\'t know that it\'s the right solution, as I don\'t think my computer can keep all of this in memory.\n\nSomething like that I found on another SO thread to put all the csv in one dataframe before converting: \n\nfrom os import listdir\n\nfilepaths = [f for f in listdir("./data") if f.endswith(\'.csv\')]\ndf = pd.concat(map(pd.read_csv, filepaths))\n\n\ndoesn\'t work because too many files/too heavy.\n\nIf you know of another solution please help,\n\nThanks\n\nEDIT : \n\nThanks for the answers, it seems to work with this code: \n\nfor f in tqdm (listdir("E:\\\\Data\\\\Trades\\\\history")):\n    if f.endswith(\'.csv\'):\n        pd.read_csv(f, \'rb\').to_hdf(\'E:\\\\Data\\\\Trades\\\\hdf5_test.h5\', key=f)\n\n\nBut I get this error FileNotFoundError: [Errno 2] No such file or directory: \'trade_20141123.csv\'\nThat\'s the name of the first file in the list.\n\nI also get this warning in jupyter :\n\nParserWarning: Falling back to the \'python\' engine because the \'c\' engine does not support regex separators (separators &gt; 1 char and different from \'\\s+\' are interpreted as regex); you can avoid this warning by specifying engine=\'python\'.\n  pd.read_csv(f, \'rb\').to_hdf(\'E:\\\\Data\\\\Trades\\\\hdf5_test.h5\', key=f)\nC:\\Users\\Sam\\anaconda3\\envs\\vaex_env\\lib\\site-packages\\tables\\path.py:155: NaturalNameWarning: object name is not a valid Python identifier: \'trade_20141122.csv\'; it does not match the pattern ``^[a-zA-Z_][a-zA-Z0-9_]*$``; you will not be able to use natural naming to access this object; using ``getattr()`` will still work, though\n  check_attribute_name(name)\n\n\nDo I have to rename all the files ? I\'m not sure that\'s the issue, but if it is what character is the problem ?\n\nCheers\n'
'i want to get sum of each column according to group by rows. how can i do this with pandas?\n\n'
"I am trying to use a drop down and store the selected value to a local variable. Although the print statement works, it's not storing the value to a variable. Any and all help will be highly appreciated.\n\nimport ipywidgets as widgets\n\nbor = 'A'\ndrop_down = widgets.Dropdown(options=['A','B','C','D'],\n                                description='Choose',\n                                disabled=False)\n\ndef dropdown_handler(change):\n        print(change.new)\n        bor = change.new  # This line isn't working\ndrop_down.observe(dropdown_handler, names='value')\ndisplay(drop_down)\n\n\nRelevant photo\n"
"I have a series with daily data for a given year. e.g. I have data for the maximum temperature of each day for a year. Each day is represented in format 'Month_Day'. While plotting the series in matplotlib, the x-axis gets darken and nothing appears as shown in the attached figure1. I want to plot each day data bu x-axis should have the month name instead. How to set the x-axis with month name?\n"
"**I have code which throws a value error if the sample size is less than 5. i wanted to add a comment as Insufficent data in the place of value error and if the sample size is greater than or equal to 5 i want to continue with the code.This is the code that i have written and it doesnt work .\nPlease help me in modifying the above code\ndf_names = []\n\ncomp1={}\nfor j in bentonite:\n    if len(j)&lt;5:\n        print('Insufficient data')\n    else:\n        continue\n        for i in j:\n            if i!='component_id':\n                X=j.drop([i,'component_id'],axis=1) \n                y = j[i]\n                if i == 'loi_':\n                    break \n\n                sc=StandardScaler()\n                X=sc.fit_transform(X)\n                X = pd.DataFrame(X)\n\n                from sklearn.model_selection import KFold \n                from sklearn import metrics\n                n_split = 5 \n                kf=KFold(n_splits=n_split,shuffle=True,random_state=0) \n\n                for model,name in zip([lm],['lm']):\n                    rmse_test=[]\n                    r2_test=[]\n                    adj_r2_test=[]\n                    rmse_train=[]\n                    r2_train=[]\n                    adj_r2_train=[]\n                    \n                    for train,test in kf.split(X,y):\n                        X_train,X_test=X.iloc[train,:],X.iloc[test,:]\n                        y_train,y_test=y.iloc[train],y.iloc[test]\n                        model.fit(X_train,y_train) #fitting the model\n                        y_predict_test=model.predict(X_test) #predicting the test result\n                        mse_test=round(metrics.mean_squared_error(y_test,y_predict_test),3) #calculating mse\n                        rmse_test.append(np.sqrt(mse_test)) # calculating rmse for test\n                        rSquare_test = round(r2_score((y_test),(y_predict_test)),3) #calculating rsquared for test\n                        adj_rsquare_test= round(1-(1-rSquare_test)*(len(y)-1)/(len(y)-X.shape[1]-1),3) #calculating adjusted r2 for test\n                        r2_test.append(rSquare_test) #appending the result to the empty list\n                        adj_r2_test.append(adj_rsquare_test) #appending the result to the empty list\n\n                        y_predict_train=model.predict(X_train) #predicting the result for train\n                        mse_train=round(metrics.mean_squared_error(y_train,y_predict_train),3) #calculating mse for train\n                        rmse_train.append(np.sqrt(mse_train)) # calculating rmse for train\n                        rSquare_train = round(r2_score((y_train),(y_predict_train)),3) #calculating rsquared for train\n                        adj_rsquare_train= round(1-(1-rSquare_train)*(len(y)-1)/(len(y)-X.shape[1]-1),3) #calculating adjusted r2 for train\n                        r2_train.append(rSquare_train) #appending the result to the empty list\n                        adj_r2_train.append(adj_rsquare_train) #appending the result to the empty list\n                \n                \n                    exec(f&quot;df_{name}_{i} = pd.DataFrame()&quot;)\n                    exec(f&quot;df_{name}_{i}['Name'] =[name]*n_split&quot;)\n                    exec(f&quot;df_{name}_{i}['rmse_train'] =rmse_train&quot;)\n                    exec(f&quot;df_{name}_{i}['rmse_test'] =rmse_test&quot;)\n                    exec(f&quot;df_{name}_{i}['r2_train'] =r2_train&quot;)\n                    exec(f&quot;df_{name}_{i}['r2_test'] =r2_test&quot;)\n                    exec(f&quot;df_{name}_{i}['adj_r2_train'] =adj_r2_train&quot;)\n                    exec(f&quot;df_{name}_{i}['adj_r2_test'] =adj_r2_test&quot;)\n                    exec(f&quot;df_{name}_{i}['output'] = [i]*n_split&quot;)\n                    df_names.append(f&quot;df_{name}_{i}&quot;)\n            \n            \n    collection = pd.DataFrame() #collecting the results in dataframe\n\n    for nn in df_names:\n        exec(f&quot;collection=pd.concat([collection,{nn}])&quot;)\n\n        collection['new_name'] = collection['output']\n\n\n    df4 = collection.groupby(['new_name']).mean()\n\n#finding the mean of the results to find the overall best model\n#df4['Mean'] = (df4['rmse_train'] + df4['rmse_test'] + (1-df4['r_squared_train']) + (1-df4['r_squared_test']))/4\n#df4.sort_values(by='Mean',inplace=True)\n\n    abc = list(d2.columns)\n\n    new = pd.DataFrame(columns=df4.columns)\n    for names in range(len(abc)):\n        val = df4[df4.index.isin([i for i in df4.index if abc[names] in i])].iloc[0,:]\n        df5=pd.DataFrame(val.values.reshape(1,-1),columns=new.columns,index=[val.name])\n        new=pd.concat([new,df5])           \n    \n    comp=j.component_id.unique()[0]\n    comp1[comp]=new\n    print('\\n')\n    print('\\033[1m'+ comp+':')\n    print('\\n')\n    print(new)\n    print('\\n')\n    ```\n\n"
"import plotly.express as px\nimport plotly.graph_objects as go\nimport pandas as pd\n\ndf = pd.read_excel ('the sample sheet i have attached')\n\nindexNames = df[ (df['counter'] != -1) &amp; (df['counter'] != 1) &amp; (df['id'] &gt;= &quot;Fri Jul 10 19:33:12 GMT+05:30 2020&quot;)  ].index\ndff.drop(indexNames , inplace=True)\n\nv = df.employee_id.value_counts()\n\nnew_df= df[df.employee_id.isin(v.index[v.eq(1)])]\nnew_df\n\nI wanted new_df data frame to only have data of the employee who have login but not log out.\nIn attached excel data sheet if counter = -1 then i am assuming logout\nand if counter = 1 i am assuming login\nlink for the sheet &quot;https://drive.google.com/file/d/1E9hivsHzc9lc_IQG0mWf36fc8F4HgPDm/view?usp=sharing&quot;\n"
"I am trying to show on an Area Chart the column name(s) I selected from a Dataframe as label, along with its respective color using Altair.\nThe problem is that every time I do it, the chart disappear and I can't customize the colors based on a list of hex Codes.\nIs there any way to achieve this?\n\n\nimport altair as alt\nimport pandas as pd\nimport os\n\n\ndf = {\n    'Month': ['Apr', 'May'],\n    'Status': ['Working', 'Complete'],\n    'Revenue': [1000, 2000],\n    'Profit': [500, 600]\n}\n\ndf = pd.DataFrame(df)\n\nhexList = [\n    '#002664', '#72BF44', '#EED308', '#5E6A71', '#7C9DBE', '#F47920', '#1C536E', '#2D580C',\n]\n\nxSelected = 'Status'\nySelected = ['Revenue']\n\nchartsList = []\n\nchart = alt.Chart(df).mark_area().encode(\n    x=xSelected,\n    y=ySelected[0],\n    #color=alt.Color(f'{xSelected}:N'), ### **==&gt; this gives me the labels I neeed, but no chart is plotted**\n    color=alt.value(f'{hexList[0]}'), ### **==&gt; this gives me the chart with the color I want, but without the labels I need**\n    tooltip=ySelected\n)\n\nmainDir = os.path.dirname(__file__)\nfilePath = os.path.join(mainDir, 'altairChart.html')\nchart.save(filePath)\n\n"
"Image\n&lt;-How to format column 'Order Date' to DateTime?\nThis code below doesn't work :(\nIt thorws to me exception:\nValueError: time data '04/19/19 08:46' does not match format '%m/%d/%Y %X' (match)\ndates = df['Order Date'] #тут список строк с датой\n\ndates['Datka'] = pd.to_datetime(df['Order Date'], format='%m/%d/%Y %X')\n"
"This might be a very simple question, but I can't figure it out for some reason, and I need to get moving with my work.\nIf I have two arrays:\na = [3 6 4 9]\nb = [4 8 2 7]\n\nand I want to plot them in the form of a histogram, with the pillars next to eachother for each index. How would I go about that?\nThe x-axis would be like 1, 2, 3, 4 - while the y-axis would be from 0 to 10.\n"
"I am trying to take the data from direct URL in python JupyterNotebook. but the error I am getting is really frustrating me .\nHere is the Link which I am fetching:\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\nI am simply using Jupyter Notebook function as:\nfrom IPython.display import HTML\nHTML('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n\nThe error I am getting is the Following:\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-15-0a8be2c0a7c6&gt; in &lt;module&gt;\n      1 from IPython.display import HTML\n----&gt; 2 HTML('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py in __init__(self, data, url, filename, metadata)\n    693         if warn():\n    694             warnings.warn(&quot;Consider using IPython.display.IFrame instead&quot;)\n--&gt; 695         super(HTML, self).__init__(data=data, url=url, filename=filename, metadata=metadata)\n    696 \n    697     def _repr_html_(self):\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py in __init__(self, data, url, filename, metadata)\n    619 \n    620         self.reload()\n--&gt; 621         self._check_data()\n    622 \n    623     def __repr__(self):\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\display.py in _check_data(self)\n    668     def _check_data(self):\n    669         if self.data is not None and not isinstance(self.data, str):\n--&gt; 670             raise TypeError(&quot;%s expects text, not %r&quot; % (self.__class__.__name__, self.data))\n    671 \n    672 class Pretty(TextDisplayObject):\n\nTypeError: HTML expects text, not b'5.1,3.5,1.4,0.2,Iris-setosa\\n4.9,3.0,1.4,0.2,Iris-setosa\\n4.7,3.2,1.3,0.2,Iris-setosa\\n4.6, ....\n\nAny Help is really appreciated.\n"
"Im really new to python and would appreciate your help.\nI'm trying to add a column to my data frame in python. I'm using the following:\ndf['capital']=np.where(df['year']!=1960,2,df['GDP'])\n\nExcept, when I write GDP what I really want is the GDP for the year 1961. Any idea how I can include that?\n"
"from sklearn.datasets import load_svmlight_file\n\ndef get_data(dn):\n    # load_svmlight_file loads dataset into sparse CSR matrix\n    X,Y = load_svmlight_file(dn)\n    print(type(X)) # you will get numpy.ndarray\n    return X,Y\n\n\n# convert X to ndarray\nX = X.toarray()\nprint(type(X))\n    \n# As you are going to implement logistic regression, you have to convert the labels into 0 and 1 \nY = np.where(Y == -1, 0, 1)\n\nWhen running the code I get the following error X = X.toarray() NameError: name 'X' is not defined, the code is meant to convert this dataset url= 'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/diabetes'\nwget.download(url,'Assingment1')\n"
"Here is a part of the plot that I have\nI need to create TrendLine that would be extended to the 3th\nquarter of this plot... I can's think of any solution.\nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nx = [1, 8, 12, 20]\ny = [1, 8.4, 12.5, 20]\n\nfig = plt.figure(figsize=(20,20))\nax = fig.add_subplot()\nax.set_xlim(-30, 30)\nax.set_ylim(-20, 20)\n\nplt.subplot().spines['left'].set_position('center')\nplt.subplot().spines['bottom'].set_position('center')\nplt.plot(x,y, 'b.', ms=20)\nplt.minorticks_on()\nax.grid(True, which='both')\nmean_line = ax.plot()\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x,p(x),&quot;r--&quot;)\n\nplt.show()\n\n"
"I have a dataset from kaggle with medical appointment data. There is a date that the appointment is made (ScheduledDay) and a date where it is made for (AppointmentDay). There is also patientId and many patients made multiple appointments and whether they turned up or not.\nI want to do a cumcount sort of thing with same patients and where they didn't turn up (No-show == 'Yes') but ONLY for no-shows that have already happened by the time the appointment is made.\nThis is my code for counting the number of booking each patient made and is fine\ndf.sort_values(by='ScheduledDay', inplace=True)\n\ndf['book_count'] = df.groupby('PatientId').cumcount()\n\nBut for my problem I can't work out how to do make it so neat. I have done it but it's not vectorized and needs to loop through the rows. Needless to say it takes ages\nfinal_index = df.index.tolist()[-1]\n\ndf['miss_count'] = np.NaN\nfor i in df['ScheduledDay'].iteritems():\n    print(f'{final_index} -- {i[0]}')\n\n    patient = df.loc[i[0], 'PatientId']\n\n    count = df.loc[\n        (df['AppointmentDay'] &lt; i[1])\n        &amp; (df['No-show'] == 'Yes')\n        &amp; (df['PatientId'] == patient)].shape[0]\n\n    df.loc[i[0], 'miss_count']  = count\n\n    print(f'\\n{count}\\n')\n\nSo in this case I haven't used cumcount because I don't know how to filter for only dates that have happened before\nThere will then be a column with the count of how many times the patient has made an appointment and not shown up for it before the current making of an appointment.\n"
"I have a dataframe as follows:\n     A    B    C   cap\n0  482  959   67  1000\n1   79   45    2   100\n2  855  164  173  1000\n3    5    0    1    10\n4  659  831  899  1000\n\nEach number is generated by randomizing an int between 0 and df['cap']\nfor example:\nin row 0, I generate 3 random numbers between 0-1000\nin row 1, I generate 3 random numbers between 0-100\nin row 2, I generate 3 random numbers between 0-1000\nin row 3, I generate 3 random numbers between 0-10\nin row 4, I generate 3 random numbers between 0-1000\nI want to get this dataframe:\n      A       B       C  \n 0    0.482   0.959   0.067\n 1    0.790   0.450   0.020\n 2    0.855   0.164   0.173\n 3    0.500   0.000   0.100\n 4    0.659   0.831   0.899\n\n(don't mind the number of digits after the decimal point)\nI tried:\ndf['A'] / df['cap'] \n\nworked fine for a single column. but\ndf[['A','B']] / df['cap'] \n\ngot index error. Also most other tricks I've tried.\nhow do I normalize 'A' 'B' and 'C' by 'cap'?\n"
"I have very big, legacy file with ~5000 columns and very big amount of record.\nMany columns are named like a_1,a_2,...,a_200 etc.\nI want to concatenate number of columns into struct (for better data manipulation later), so instead:\n_| a_1 | a_2 | a_3 |...\n0| true | false | true |...\n1| false | true | false |...\n\nI would like to have struct a { 1: true, 2: false, ... 200: true } .\nHow to transform it using Python, probably Panda's?\nColumns have always same prefix, like a_, b_... etc.\nGreeetings\n"
"I have 3 dataframes I need joined horizontally. There are no common columns to enable me tie them together. I have tried merge, join, concat. Maybe I did something wrong while coding.\nThis is the data frame1:\ngrill_type  is_frozen   item_material\n0   Propane false   Hotdog\n0   Propane true    Hotdog\n0   Propane true    Hotdog\n0   Propane false   Hotdog\n0   Propane true    Hotdog\n\nThis is the data frame2:\nguess_grill_correct thumbs_up_score\n0   true    0.4\n0   true    1.0\n0   true    0.0\n0   true    0.0\n0   true    0.4\n\nThis is the data frame3:\nsample_item_index\n0   1\n0   10\n0   10\n0   11\n0   12\n\nAs you can see, three dataframes are all without indices.\nI want to have something like this:\nsample_item_index   item_material   is_frozen   grill_type  thumbs_up_score guess_grill_correct\n0   1   Veggie Patty    False   Propane 0.0 True\n1   1   Hotdog          False   Propane 0.4 True\n2   2   Veggie Patty    True    Propane 0.9 True\n3   3   Veggie Patty    False   Propane 0.8 True\n4   4   Veggie Patty    True    Propane 0.8 True\n\nWhen I try to use merge like for example:\ndf1.merge(df2, on='index', how = 'left')\n\nThe result will be mutiple extra rolls which is not what I want\nAny suggestion is wellcome!\n"
"I am starting to use a MultiOutputRegressor in sci-kit learn for a multi-variable target I am trying to estimate with Random Forests.\nI did start implementing manually before I came across this MultiOutputRegressor, and was trying to rotate the output for single output regressors so that a single target was used at any given time - and the other target variables used as inputs - but it was becoming computationally expensive.\nI have searched and reviewed some code, but am struggling to determine if the target output (y) is used as an input feature (X). Specifically:\n\nwhen y_1 is being predicted, are y_2 ... y_n used as input features?\nwhen y_x is being predicted, are y_1 ... y_n (excluding y_x) used as input features?\nwhen y_n is being predicted, are y_1 ... y_n-1 used as input features?\n(apologies if I'm being overly verbose)\n\nThe paper &quot;Multi-target regression via input space expansion&quot; explains what I am looking to achieve.\nSome answers have alluded to the fact that the MultiOutputRegressor algorithm may look for correlations between the target values, but I'm hoping they're actually rotated to be inputs (or effective inputs) for the algorithm in my application.\n"
"X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3)                   # Number of clusters == 3\nkmeans = kmeans.fit(X)                          # Fitting the input data\nlabels = kmeans.predict(X)                      # Getting the cluster labels\ncentroids = kmeans.cluster_centers_             # Centroid values\nprint(&quot;Centroids are:&quot;, centroids)              # From sci-kit learn\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111, projection='3d')\n\nx = np.array(labels==0)\ny = np.array(labels==1)\nz = np.array(labels==2)\nax.scatter(x,y,z, marker=&quot;s&quot;[kmeans.labels_], s=40, cmap=&quot;RdBu&quot;)\n\nI am trying to Plot the clusters in 3D by colouring all labels belonging to their class, and plot the centroids using a separate symbol. I managed to get the KMeans technique working, atleast I believe I did. But I'm stuck trying to plot it in 3D. I believe there can be a simple solution I'm just not seeing it. Does anyone have any idea what I need to change in my solution to achieve this?\n"
'I have a dataframe:\nd = [f1  f2  f3 \n     1    2   3 \n     5    1   2 \n     3    3   1 \n     2    4   7\n     ..  ..  ..]\n\nI want to add, per feature, the percentile of the value for this feature in the row (for subset of features).\nSo for subset = [f1,f2,f3] my dataframe will be\nnew_d =[f1   f2   f3  f1_per   f2_per   f3_per\n         1    2   3    0         0.25     0.5\n         5    1   2    1          0       0.25\n         3    3   1    0.5        0.5     0\n         2    4   5    0.25      0.75     1\n         4    5   4    0.75       1       0.75]\n\nWhat is the best way to do so?\n'
"def z_score(df, column, mean, std):\n    return #  ?????\n\nmean = history_df['distances'].mean()\nstd = history_df['distances'].std()\ntraining_df['distances_normal'] = z_score(training_df, 'distances', mean, std)\ntesting_df['distances_normal'] = z_score(testing_df, 'distances', mean, std)\n\nhello, any suggestions on what the z_score function should look like (after return) so that further down when I create the new columns 'distances_normal' to the training and testing dataframes based on the history dataframe column 'distances' the values are normalized?\nthx in advance\n"
'I have a dataframe that represents the location of some people.\nThis dataframe is not cleaned and the names are a mess. some rows have only the country name, others have name and city, and others have only the city. I also have sentences that are not in English.\nHow can I use python with NLP to tidy this dataset and get a homogenous dataset?\nHere is a screenshot of the dataset:\n\nThanks in advance\n'
"I've created a dummy variable (in Python), seo, which takes the value 1 if the value of another column is greater than 0, as shown in the code below.\ndf['seo'] = (df['amount'] &gt; 0).astype(int)\n\nWhat I want to do is to create a second dummy variable, past_seo, which takes the value 1 if the seo dummy for a particular firm was 1 at any historical time.\nFor reference, my dataset comprises monthly firm data and contains a firm identifier variable (6_cusip).\nWhat I tried to do was to group the dataset by 6_cusip and date, and then &quot;fill forward&quot; the seo dummy variable. However, I couldn't get this to work.\nThe code below shows an example of the first 20 observations in my dataset. As shown, the observations are all from the same firm. What I want to do is create a new column which fills that '1' in the seo column forward to all subsequent observations belonging to the same firm.\n{'date': {0: '1994-05',\n  1: '1994-06',\n  2: '1994-07',\n  3: '1994-08',\n  4: '1994-09',\n  5: '1994-10',\n  6: '1994-11',\n  7: '1994-12',\n  8: '1995-01',\n  9: '1995-02',\n  10: '1995-03',\n  11: '1995-04',\n  12: '1995-05',\n  13: '1995-06',\n  14: '1995-07',\n  15: '1995-08',\n  16: '1995-09',\n  17: '1995-10',\n  18: '1995-11',\n  19: '1995-12'},\n '6_cusip': {0: '00077R',\n  1: '00077R',\n  2: '00077R',\n  3: '00077R',\n  4: '00077R',\n  5: '00077R',\n  6: '00077R',\n  7: '00077R',\n  8: '00077R',\n  9: '00077R',\n  10: '00077R',\n  11: '00077R',\n  12: '00077R',\n  13: '00077R',\n  14: '00077R',\n  15: '00077R',\n  16: '00077R',\n  17: '00077R',\n  18: '00077R',\n  19: '00077R'},\n 'seo': {0: 0,\n  1: 0,\n  2: 0,\n  3: 0,\n  4: 0,\n  5: 0,\n  6: 0,\n  7: 0,\n  8: 0,\n  9: 0,\n  10: 0,\n  11: 0,\n  12: 0,\n  13: 0,\n  14: 0,\n  15: 1,\n  16: 0,\n  17: 0,\n  18: 0,\n  19: 0}}\n\nLet me know if you have any advice, thanks!\n"
'I have a problem filling in values in a column with pandas. I want to add strings which should describe the annual income class of a customer. I want 20% of the length of the data frame to get the value &quot;Lowest&quot;, 9% of the data frame should get &quot;Lower Middle&quot; etc... I thought of creating a list and appending the values and then set it as the value for the column but then I get a ValueError Length of values (5) does not match length of index (500)\nlist_of_lists = []\nlist_of_lists.append(int(0.2*len(df))*&quot;Lowest&quot;)\nlist_of_lists.append(int(0.09*len(df))*&quot;Lower Middle&quot;)\nlist_of_lists.append(int(0.5*len(df))*&quot;Middle&quot;)\nlist_of_lists.append(int(0.12*len(df))*&quot;Upper Middle&quot;)\nlist_of_lists.append(int(0.12*len(df))*&quot;Highest&quot;)\ndf[&quot;Annual Income&quot;] = list_of_lists\n\nDo you have an idea of what could be the best way to do this?\nThanks in advance\nBest regards\nAlina\n'
"I have a pandas DataFrame df with multiple columns. Now I want to add a new column based on other column values. I found many answers for this on stack that includes np.where and np.select. However, in my case, for every if condition (every if/elif/else block), the new column has to choose among 3 values with a specific ratio. For example,\nfor i in range(df.shape[0]):\n    if(df.iloc[i]['col1']==x):\n        df.iloc[i]['new_col']= choose one value between l=['a','b','c'] in 0.3,0.3,0.4 ratio\n\nthat is, for all rows satisfying the condition in the if statement, the elements of list l should be distributed in the above mentioned ratio to new column.\n\nThe current way I am doing is, split the df into multiple sub data frames df_sub for each if-else condtional statement. Next creating a list using np.random.choices(l,df_sub.shape[0],p=[0.3,0.3,0.4) where l=['a','b','c']. Add l to df_sub as new column and then join all those sub data frames along axis=0.\nI want to know if there is simpler way to accomplish this task instead of dividing and joining data frames?\n\n"
"I'm trying to replicate R's fitdist() results (reference, cannot modify R code) in Python using scipy.stats. The results are totally different. Does anyone know why? How can I replicate R's results in Python?\ndata = [2457.145, 1399.034, 20000.0, 476743.9, 24059.6, 28862.8]\n\nR code:\nlibrary(fitdistrplus)\nlibrary(actuar)\n\nfitdist(data, 'pareto', &quot;mle&quot;)$estimate\n\nR results:\n       shape        scale \n    0.760164 10066.274196\n\nPython code\nst.pareto.fit(data, floc=0, scale=1)\n\nPython results\n(0.4019785013487883, 0, 1399.0339889072732)\n\n"
"I have a column of value and I have to plot the value like &lt;0.99,1 to 1.99,2 to 3.99 and 4&gt;\nI tried the below code\ndata = pd.cut(TPA_Data_Details['cokumn name'], bins=[-np.inf,0.99,1,1.99,2,3.99,4,np.inf])\nplt.figure(figsize=(17,15))\nsns.countplot(data)\n\nbut it is giving the output like this \nhow to make the bar from (-inf,0.99),(1,1.99),(2,3.99) and (4,inf)?\n"
"I have a list of integers representing a time series in milliseconds. Each value was obtained by calling int(time.time() * 1000) and appending to the list.\nNow I would like to make a plot using matplotlib that will display the milliseconds time series on the x-axis in the format &quot;%H:%M:%S&quot;.\nI tried using these values along with a matplotlib data formatter by setting: plt.gca().xaxis.set_major_formatter(dates.DateFormatter(&quot;%H:%M:%S&quot;))\nbut I got the following error when attempting to plot: OverflowError: int too big to convert.\nI have also tried using already preformatted time series values as strings by calling time.strftime('%H:%M:%S', time.gmtime(time.time())) and appending to the list instead with no luck, resulting in this problem: Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nCould anyone please help me to plot milliseconds in a human readable format on the x-axis?\n"
'So i have this code writen in python. I\'m not going to explain it as it is a simple syntax fix that i can\'t seem to see so it is useless to explain what\'s for.\n\nThe problem that i have is that for a given d, for example 15 i get the value of "cuentas" right and "e" correct.\n\nWhat i want to do is to iterate through a set of d\'s and get the value of each cuentas, and each e in order to plot d vs e.\n\nMy problem is that i dont seem to get how to create a matrix in python.\n\nIn matlab i used to write two different loops like theese:\n\nfor i=1:1:N\n    for j=1:9\n\n      a[i,j]= and so on\n\n\na[i,j] would be a matrix with N rows and 9 columns that i could access and manipulate.\n\nIn my code below i will intentionaly put # comments where i want to iterate through the distances\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nN=100000\ncos=np.zeros(N)\nphi=np.zeros(N)\nteta=np.zeros(N)\na=np.zeros(N)\n\n\nxrn=np.zeros(N)\nyrn=np.zeros(N)\nzrn=np.zeros(N)\n\nx=np.zeros(N)\ny=np.zeros(N)\nz=np.zeros(N)\n\nlim1=14.7\nlim2=3.35\nlim3=-lim1\nlim4=-lim2\n\n#d=np.array([15,20,25,30,35,40,45,50,55])\nd=15\n\n    #for j in range(9):\nfor i in range(N):\n    cos[i]=np.random.uniform(-1,1)\n    teta[i]=np.random.uniform(-np.pi,np.pi)\n    phi[i]=np.random.uniform(0,2*np.pi)\n\n# a[i]=d[j]/cos[i]*np.cos(phi[i])\na[i]=d/cos[i]*np.cos(phi[i])\n\n\nxrn[i]=np.random.uniform(-1,1)*lim1\nyrn[i]=np.random.uniform(-1,1)*lim2\n\nx[i]=a[i]*np.sin(teta[i])*np.cos(phi[i])+xrn[i]\ny[i]=a[i]*np.sin(teta[i])*np.sin(phi[i])+yrn[i]\n\n#cuentas[j]=0\ncuentas=0\n\n#for j in range(9):\nfor i in range(N):\n    if a[i]&gt;0 and x[i] &lt; lim1 and x[i]&gt;lim3 and y[i] &lt; lim2 and y[i]&gt;lim4:\n        cuentas=cuentas+1\n#e[j]=cuenta[j]/N\ne=cuentas/N\n\n\nThanks so much to those even reading!!\n'
'I am working through the following machine learning tutorial:\n\nhttp://machinelearningmastery.com/machine-learning-in-python-step-by-step/\n\nHere is my (mac) development environment:\n\nPython 2.7.10 \nscipy: 0.13.0b1\nnumpy: 1.8.0rc1\nmatplotlib: 1.3.1\npandas: 0.20.2\nsklearn: 0.18.1\n\n\nWhen I try to run a script, to load the data from a URL containing the CSV data, I get the following error: \n\nTraceback (most recent call last):\n  File "load_data.py", line 4, in &lt;module&gt;\n    dataset = pandas.read_csv(url, names=names)\nNameError: name \'pandas\' is not defined\n\n\nHere\'s the script:\n\n# Load dataset\nurl = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"\nnames = [\'sepal-length\', \'sepal-width\', \'petal-length\', \'petal-width\', \'class\']\ndataset = pandas.read_csv(url, names=names)\n\n'
"As I want to count the unique number of column A in a moving time window(60 seconds):\n\nfn = lambda x: len(np.unique(x)) \ndf = pd.DataFrame({'A':['a', 'b', 'a', 'b', 'e'], 'B': [0, 1, 2, 3, 4]},\n                index = [pd.Timestamp('20130101 09:01:00'),\n                         pd.Timestamp('20130101 09:01:32'),\n                         pd.Timestamp('20130101 09:02:03'),\n                         pd.Timestamp('20130101 09:02:25'),\n                         pd.Timestamp('20130101 09:03:06')])\n\n\ndf[['A']].rolling('60s').apply(fn)\n\n\nI expect the result as \n\n2013-01-01 09:01:00 1\n2013-01-01 09:01:32 2\n2013-01-01 09:02:03 2\n2013-01-01 09:02:25 2\n2013-01-01 09:03:06 2\n\n\nhowever, the result is:\n\n2013-01-01 09:01:00 a\n2013-01-01 09:01:32 b\n2013-01-01 09:02:03 a\n2013-01-01 09:02:25 b\n2013-01-01 09:03:06 e\n\n\nwhat's the problem?\n"
'I\'m currently working on building a deep neural network using Tensorflow, and encountering some issues implementing a regularization technique called dropout (check out the original paper by Geoffrey Hinton here).\n\nTensorflow has a function to take care of this, and I\'m following a tutorial by Aurelien Geron\'s book Hands-On Machine Learning with Scikit-Learn &amp; Tensorflow (which, by the way, is incredible). In it, his sample code to implement dropout consists of declaring a training placeholder:\n\ntraining = tf.placeholder(tf.float32, shape = (), name = "training")\n\n\nand then creating the hidden layer dropout object:\n\nhidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training = training)\n\n\nHowever, when I execute this, I receive an error pointing to the above line.\n\nTypeError: Input \'pred\' of \'Switch\' Op has type float32 that does not match expected type of bool\n\n\nI looked into the Tensorflow documentation regarding dropout, tf.layers.dropout() method\'s training parameter is defined as \n\n\n  Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a\n  placeholder). Whether to return the output in training mode (apply\n  dropout) or in inference mode (return the input untouched).\n\n\nHowever, in the code above, I\'m clearly passing in tf.float32. I suspect this is the cause of my error- it\'s even stated in the error message itself. So was this simply a typo by the author, or am I not understanding what is happening behind the scenes?\n\nShould I just replace the hidden layer declaration with this line instead?\n\nhidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training = True)\n\n\nI\'ve also looked into other SO posts with similar errors, like this one, but the answers seem to suggest that the error stems from an outdated version of Tensorflow, which is not the case- I only recently installed on my machine a few weeks ago.\n'
"I have a the following dataframes in python:\n\ndataframe 1\n\n             1  2  3  4  5\ndog   dog    0  0  0  0  0\n      fox    0  0  0  0  0\n      jumps  0  0  0  0  0\n      over   0  0  0  0  0\n      the    0  0  0  0  0\nfox   dog    0  0  0  0  0\n      fox    0  0  0  0  0\n      jumps  0  0  0  0  0\n      over   0  0  0  0  0\n      the    0  0  0  0  0\njumps dog    0  0  0  0  0\n      fox    0  0  0  0  0\n      jumps  0  0  0  0  0\n      over   0  0  0  0  0\n      the    0  0  0  0  0\nover  dog    0  0  0  0  0\n      fox    0  0  0  0  0\n      jumps  0  0  0  0  0\n      over   0  0  0  0  0\n      the    0  0  0  0  0\nthe   dog    0  0  0  0  0\n      fox    0  0  0  0  0\n      jumps  0  0  0  0  0\n      over   0  0  0  0  0\n      the    0  0  0  0  0\n\n\ndataframe 2\n\n             1  2  4  5\ndog   dog    0  0  0  0\n      fox    0  0  0  0\n      jumps  0  0  0  0\n      the    0  0  0  0\n      horse  0  0  0  0\nfox   dog    0  0  0  0\n      fox    0  0  0  0\n      over   0  0  0  0\n      the    0  0  0  0\n      cat    0  0  0  0\n\n\nYou can see that dataframe2 contains multiindexes of dataframe1 but it also contains additional multiindexes like horse and cat. Dataframe 2 also doesn't contain all the columns of dataframe 1 as you can see it misses column 3.\n\nI want to compare these two dataframes in such a way that the function returns True if the values of any common first->second (multi) indices between the two dataframes also match each other, otherwise False.\n\nI could iterate over by looking over each value manually but that would increase the function's time complexity. Does any know if pandas provides a builtin way of doing this or do I need to construct a function myself. If so, can you point me in the right direction? Any suggestions are highly appreciated. Thank you.\n"
"When using Grid search for classifiers in python using this function GridSearchCV() imagine we have an interval of parameters to tune form 1 to 100 how are we able to specify that (1:100 doesn't work ) ??\n"
"I am looking for a way to get the indices of the local maxima of a tensor using TensorFlow exclusively.\n\ntl;dr\n\nI am not a data scientist.  I don't know much about the theory behind much of computer vision, but I am trying to build a computer vision app using TensorFlow.  I plan on saving my model and calling it as a service using TF Serving, so I can't depend on external libraries such as numpy, scipy, etc.  What I want to accomplish is algorithmically the same as scipy's signal.argrelextrema, but in a way that can be saved with my model and rerun.  Other algorithms for this have been shown here, but none execute within TensorFlow. Can anyone point me in the right direction?\n"
'I have the following data frame,\n\nuuid    variable    value\nAAS Highly_Active   False\nAAS Highly_Active   True\nSAP Highly_Active   False\nSAP Multiple_days   True\nYAS Highly_Active   False\nYAS Highly_Active   False\nYAS Busi_weekday    False\n\n\nAnd I need to use the values from the column variable and value to define third column Activity and I have following classic python code doing it but my primary data frame is 121Mb in size and so its taking long time. Any pandas solutions would be great\n\ndef activity(row):\n    if row[\'variable\'] == "Highly_Active" and row[\'value\'] ==True:\n        val = "Highly_Active"    \n    else:\n        val = "NO"\n    if row[\'variable\'] == "Multiple_days" and row[\'value\']==True:\n        val = "Multiple_days"    \n    else:\n        val = "NO" \n    if row[\'variable\'] == "Busi_weekday" and row[\'value\']==True:\n        val = "Busi_weekday"\n    else:\n        val="NO"\n    return val\n\n'
'I earlier worked on taking just one column(string type data) as my train set, I would like to take another corresponding column(Amount column of float type) into consideration as a train set along with the Details column.\nIn the amount column negative value indicates debit and positive value indicates credit.\nHow do I proceed with this, I tried appending two columns together but I\nhad to convert the float type amount to  string type which doesn\'t make\nany sense in my dataset.\nI want to include the Amount column to check if the machine could learn the variations, which is quite important in this case.\nThanks in advance.\n\nDetails                    |Amount               |Category\n-------------------------------------------------------------                                \nTanishq Jwellery Bangalore |-990                 |jwellery\nODESK***BAL-28APR13        |240                  |Others\nAEGON RELIGARE LIFE IN     |456                  |Others\nINTERNET PAYMENT #999999   |-250                 |Transfer in for Card Payment\nWWW.VISTAPRINT.IN          |245                  |Print\nKhazana Jwellery           |-9000                |jwellery\nINTERNET PAYMENT #999999   |785                  |Transfer in for Card Payment\nIndian Oil                 |344                  |Fuel\nTouch foot wear            |-782                 |Clothing\n\n\nPart of my script:\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nimport time\nimport matplotlib.pyplot as plt  \nfrom sklearn.model_selection import train_test_split \n\n# TRAIN DATA\ndata= pd.read_csv(\'ds1.csv\', delimiter=\',\',usecols=[\'Details\',\'Amount\',\'Category\'],encoding=\'utf-8\')\ndata=data[data.Category !="Others"]\n\ntarget_one=data[\'Category\']\ntarget_list=data[\'Category\'].unique()\n\n# TEST DATASET\ntest_data=pd.read_csv(\'ds2.csv\', delimiter=\'\\t\',usecols=[\'Details\',\'Amount\',\'Category\'],encoding=\'utf-8\')\n\nx_train, y_train = (data.Details, data.Category )\nx_test, y_test = (test_data.Details, test_data.Category)\n\nvect = CountVectorizer(ngram_range=(1,2))\nX_train = vect.fit_transform(x_train)\n\nX_test = vect.transform(x_test)\nstart = time.clock()\n\nmnb = MultinomialNB(alpha =0.13)\nmnb.fit(X_train,y_train)\n\nresult= mnb.predict(X_test)\nprint (time.clock()-start)\n\naccuracy_score(result,y_test)\n\n'
'I have a .csv file that contains my data. I would like to do Logistic Regression, Naive Bayes and Decision Trees. I already know how to implement these.\n\nHowever, my teacher wants me to split the data in my .csv file into 80% and let my algorithms predict the other 20%. I would like to know how to actually split the data in that way.\n\ndiabetes_df = pd.read_csv("diabetes.csv")\ndiabetes_df.head()\n\nwith open("diabetes.csv", "rb") as f:\n    data = f.read().split()\n    train_data = data[:80]\n    test_data = data[20:]\n\n\nI tried to split it like this (sure it isn\'t working).\n'
'I have data frame(df) consists of 47 columns and 30,000 rows, columns are belows\n\nIndex([\'Unnamed: 0\', \'CtpJobId\', \'TransformJobStateId\', \'LastError\',\n       \'PriorityDate\', \'QueuedTime\', \'AccurateAsOf\', \'SentToDevice\',\n       \'StartedAtDevice\', \'ProcessStart\', \'LastProgressAt\', \'ProcessEnd\',\n       \'OutputFileDuration\', \'Tags\', \'SegmentId\', \'VideoId\',\n       \'ClipFirstFrameNumber\', \'ClipLastFrameNumber\', \'SourceId\',\n       \'SourceNamedLocation\', \'SourceDirectory\', \'SourceFileSize\',\n       \'srcMediaFormat\', \'srcFrameRate\', \'srcWidth\', \'srcHeight\', \'srcCodec\',\n       \'srcDuration\', \'TargetId\', \'TargetNamedLocation\', \'TargetDirectory\',\n       \'TargetFilename\', \'Description\', \'TargetTags\', \'tgtFrameRate\',\n       \'tgtDropFrame\', \'tgtWidth\', \'tgtHeight\', \'tgtCodec\', \'DeviceType\',\n       \'DeviceResourceId\', \'AssignedDeviceId\', \'DeviceName\',\n       \'AssignedDeviceJobId\', \'DeviceUri\'],\n      dtype=\'object\')\n\n\nI want to apply a function for selective column or that data frame to create a new column called df[\'seg_duration\'], so my function is as below\n\ndef seq_duration(df):\n\n    if ClipFirstFrameNumber is not None and ClipLastFrameNumber is not None:\n        fn = ClipLastFrameNumber -ClipFirstFrameNumber\n        if FrameRate ==\'23.98\' and DropFrame == \'False\' :\n            fps = 24 / 1.001\n        elif FrameRate == \'24\' and DropFrame == \'False\':\n            fps = 24\n        elif FrameRate == \'25\'and DropFrame == \'False\':\n            fps = 25\n        elif  FrameRate == \'29.97\':\n            fps = 30 / 1.001\n        elif  FrameRate == \'30\' and DropFrame == \'False\':\n            fps = 30\n        elif FrameRate == \'59.94\':\n            fps = 60 / 1.001\n        Duration = fn/fps\n\n    elif srcDuration is not None:\n         Duration = srcDuration\n    else:\n        None\n\n\nThe function is actually have 3 case and in one case have many conditions, so first i have subtract the value from ClipLastFrameNumber to ClipFirstframeNumber columns and save it to fn variable. and aplly other logic, same as srcDuration is column and its value. such as below\n\nClipLastFrameNumber ClipFirstFrameNumber    tgtDropFrame    tgtFrameRate\nNaN                    NaN                    True          29.97\nNaN                    NaN                    True          29.97\nNaN                    NaN                    True          29.97\n34354.0                28892.0                True          29.97\n\n\nWhen I apply this function as below\n\ndf[\'seg_duration\']=df.apply(seq_duration)\n\n\nI am getting error NameError: ("name \'ClipFirstFrameNumber\' is not defined", \'occurred at index Unnamed: 0\')\n\nIs that right way to write function for pandas or how do I use this function to that data frame and achieve my goal to create a new column df[\'seg_dur\'] based on that function. Thanks in advance\n'
"I need to calculate something with the refractive index that looks like this ref_i=n+ik, so its a complex number. I then compare the result with my given data to compare them. But I have no clue how to test ALL possible complex number combinations in an effective way. Right now I'm only doing the real part:\n\nn = 0.1\nstep = 0.001\n\nfor i in range (1,8000):\n    calculation()\n    compare()\n    n = n + step\n\n\nwhile n can be anything between 0.1 and 8, k can be 0-20. And there is no special connection between them, so every combination is allowed. Is there a clever way to get the complex number in there? Or do I have to calculate EVERY POSSIBLE combination of the two?\n"
'I\'ve tried to use the imputer to replace all of the NaN portions of my database with the averages of its respectful column. For example, I wanted to fix a blank entry in my database under the salary column and I want that blank section to be filled with the average salary values under that column. I tried doing this by following along with a tutorial but I think the video was outdated, resulting in this error.\n Code: \n\n#Data Proccesing \n\n#Importing the Libaries \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd\n\n# Importing the dataset \ndataset = pd.read_csv("Data.csv") \nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 3].values \n\n#Taking care of Missig Data \nfrom sklearn.preprocessing import Imputer \n#The source of all the problems\nimputer = Imputer(missing_values = \'NaN\', strategy = \'mean\', axis = 0)\nimputer = imputer.fit(X[:, 1:3]) \nX[:, 1:3] = imputer.transform\n\n\nInitially, X looked like this when compiled prior to using Imputer:\n\nHowever, Once I compiled lines 16-18, I got this error and I\'m not sure what to do \n'
"I need to calculate the mean of a certain column in DataFrame, so that means for each row is calculated excluding the value of the row for which it's calculated. \n\nI know I can iterate each row by index, dropping each row by index in every iteration, and then calculating mean. I wonder if there's a more efficient way of doing it.\n"
"I am using pandas to transform a Data Frame that looks like this:\n\n        Id     Question   Answer\n54      49477  Color      Red\n60      49477  Base       Standard\n133     49204  Color      Blue\n171     49204  Base       Extended\n254     48993  Color      Blue\n292     48993  Base       Standard\n\n\nI'm trying to figure out how to aggregate the data and append data together so that the resulting Data Frame looks like this:\n\n       Id     Question    Answer\n       49477  Color,Base  Red,Standard\n       49204  Color,Base  Blue,Extended\n       48993  Color,Base  Blue,Standard\n\n\nAny guidance on how I can approach this?\n"
"I want to extract data from multiple URLs, but the URLs are in a column of a data frame.\n\nI tried data extraction with the code below but no luck.\n\nfrom urllib.request import urlopen,Request\nlink = data.column1\nf = urlopen(link)\nmyfile = f.read()\nprint(myfile)\n\n\nIt shows:\n\n\n  AttributeError: 'Series' object has no attribute 'type'.\n\n\nPlease help with the code. \nThank you\n"
"I have a dataframe where I need to  group the TX/RX column into pairs, and then put these into a new dataframe with a new index and the timedelta between them as values.\n\ndf = pd.DataFrame()\ndf['time1'] = pd.date_range('2018-01-01', periods=6, freq='H') \ndf['time2'] = pd.date_range('2018-01-01', periods=6, freq='1H1min')\ndf['id'] = ids\ndf['val'] = vals\n\n                time1               time2  id val\n0 2018-01-01 00:00:00 2018-01-01 00:00:00   1   A\n1 2018-01-01 01:00:00 2018-01-01 01:01:00   2   B\n2 2018-01-01 02:00:00 2018-01-01 02:02:00   3   A\n3 2018-01-01 03:00:00 2018-01-01 03:03:00   4   B\n4 2018-01-01 04:00:00 2018-01-01 04:04:00   5   A\n5 2018-01-01 05:00:00 2018-01-01 05:05:00   6   B\n\n\nneeds to be...\n\nindex    timedelta    A   B\n  0      1            1   2\n  1      1            3   4\n  2      1            5   6\n\n\nI think that pivot_tables or stack/unstack is probably the best way to go about this, but I'm not entirely sure how...\n"
'I have a phone directory that stores Department, Title, Email and Extension on seperate rows, the things being in common are First and Last Name. I have combined First and Last Name as a Key, and would like to merge the rows to where you would end up with a single row with the Name, Title, Department, Email and Extension.\n\nI have tried creating a dictionary for each key, but I have not had any luck with the actual merging. This is where I am on coding it. I had to clean the data first to get the appropriate columns.\n\nthe table looks like the following:\n\nLastName  FirstName  Department Title   Extension Email           Key\nDoe       Jane       HR         Officer 0000                      Jane Doe\nDoe       Jane       HR         Officer           jdoe@email.com  Jane Doe\n\n\ndf = pd.read_excel("Directory.xlsx")\ndf = df.drop(columns = ["group_name","editable","id","contact_type","id2","account_id","server_uuid","picture",\n             "dial_prefix","name","label","id3","transfer_name","value","key","primary","label4","id5",\n             "type","display","group_name6"])\n\ndf = df.rename(index = str, columns = {"last_name":"Last Name","first_name":"First Name","location":"Department",\n               "title":"Title","dial":"Extension","address":"Email"})\n\ndf["Key"] = df["First Name"].map(str) + " " + df["Last Name"].map(str)\n\n\nLastName FirstName Department Title   Extension Email          Key  \nDoe      Jane      HR         Officer 0000      jdoe@email.com Jane Doe\n\n'
'I have a df that has columns such as below:\n\n   xx_id          interval_start_time                   interval_end_time      percentage_rate  \n    bd63             2019-04-01 20:00:00.000                  2019-04-01 20:30:00.000       0.208   \n    a519             2019-04-01 22:00:00.000                  2019-04-01 22:30:00.000       0.083   \n\n\nI wanted to calc the percentiles for the percentage_rate column  ( multiple percentiles like p5 p25 p50 p75 p90 ) based on the interval_start_time column. Something maybe where i can input the day and time for the interval_start_time column ( so I can do it for the different values in that column since that column contains different days and time intervals) and it would give me the percentiles or quantiles that I wanted?  \n\nThanks for the time \n'
"I have two catogery column the values in first are client_abc, client_def,\nthe second column are F1,F2,F3 and rest are the numeric column.\n\ndata looks like \n\n date       client          facility     count     claim\n21/3/2019   'client_abc'     F1           200        1300\n22/3/2019    'client_def'    F2           400        1800\n21/3/2019    'client_abc'    F3           1000       3000\n22/3/2019    'client_def'    F1           380        3600\n21/3/2019    'client_abc'    F2           900        900\n22/3/2019    'client_def'    F3           1030       2500\n21/3/2019    'client_abc'    F1           190        1700\n22/3/2019    'client_def'    F2           100000     1560\n\n\nfor client 'abc' and 'f1'\n\n date       client          facility     count     claim\n21/3/2019   'client_abc'     F1           200        1300\n21/3/2019    'client_abc'    F1           190        1700\n\n\nsimilarly for 'abc' and 'f2' ,'abc' and 'f3', 'def' and 'f1','def' and 'f2','def' and 'f3'.\n\nMy attempt\n\ndf_fac_f1 =df[facility=='F1' &amp; client == 'client_abc' ]\ndf_fac_f1 =df[facility=='F1' &amp; client == 'client_def' ]\ndf_fac_f1 =df[facility=='F2' &amp; client == 'client_abc' ]\ndf_fac_f1 =df[facility=='F2' &amp; client == 'client_def' ]\ndf_fac_f1 =df[facility=='F3' &amp; client == 'client_abc' ]\ndf_fac_f1 =df[facility=='F3' &amp; client == 'client_def' ]\n\n\nHow ca I get the same result without knowing the facility and client column values in advance?\n"
'First off, in my real situation I handel much bigger data sets, but here for this minimal, reproducible example (reprex) let\'s assume:\n\nI have two .csv files. They look like this:\nFile 1 is called "ObjectList.csv"\n\n"Object","ProductName","ID"\n"Radio","ICF-306",1112\n"TV","Q60R",1113\n"Computer","EliteBook745",1114\n"Keyboard","LX410",1115\n"Camera","D7500",1116\n"USB-Stick","CruzerBlade",1117\n"HDMI-Cable","AmazonBasic",1118\n"Console","XBOXOne",1119\n"Controller","XBOXOne",1120\n"Antivirus","AntiVirusPlus",1121\n"Game","HaloWars2",1122\n\n\nFile 2 is called "PropertyList.csv\n\n"ID","Manufacturer","Category","Price","Release"\n1112,"SONY","Electronics",50,"1.3.2015"\n1113,"SAMSUNG","Electronics",800,"1.7.2016"\n1114,"HP","Electronics",1500,"1.3.2018"\n1115,"FUJITSU","Electronics","80","1.2.2016"\n1116,"NIKON","Electronics","250","1.8.2017"\n1117,"SANDISK","Accessories","20","1.6.2007"\n1118,"AMAZON","Cables",15,"1.8.2015"\n1119,"MICROSOFT","Entertainment",450,"22.11.2013"\n1120,"MICROSOFT","Entertainment",50,"22.11.2013"\n1121,"NORTON","Programme",100,"1.8.2016"\n1122,"MicrosoftStudios","Programme",70,"21.2.2017"\n\n\nAs you can see the two .csv files have one column with overlapping information, the "ID" column.\nWhat I would like to do is\n1. search for a specific property in file 2 (for example the most expensive product)\n2. getting back the "ID" as the search result\n3. use the computationally optained "ID" as a search term for file 1 to\n4. as a final result get the clear object data.\n\nSo far I was able to do this manually, meaning I have a Python/Pandas script that extracts the IDs, then I manually have to look up the result and hard code the result into the script:\n\nimport pandas as pd\n\nObjectList="/media/jk/DE88159688156E71/Statistik/StackOverflow/ObjectList.csv"\nPropertyList="/media/jk/DE88159688156E71/Statistik/StackOverflow/PropertyList.csv"\n\nOL = pd.read_csv(ObjectList)\nPL = pd.read_csv(PropertyList)\n\nSN = PL[PL[\'Price\']==PL[\'Price\'].max()][\'ID\']\nprint(SN)\n\nprint(OL[OL.ID == 1114])\n\n\nThis script works and gives this result:\n\njk@debian:~$ python3 "/media/jk/DE88159688156E71/Statistik/StackOverflow/DataManipulation.py" \n2    1114\nName: ID, dtype: int64\n     Object   ProductName    ID\n2  Computer  EliteBook745  1114\n\n\nHaving to do manual look ups for computational results is bad for obvious reasons. Therefore my question is:\nHow to use the search result from one .csv file as a search term for a second .csv file?\n\nI tried this:\n\nprint(OL[OL[\'ID\'] == SN])\n\n\nBut this just gives me an error.\n\nFinal remark: What I can NOT do in my real scenario is merging of the two files because they are too big for such an operation.\n'
"I am trying to calculate quantile output and am close, but not sure how to turn my output into a df that I can access as a df. \n\nx.groupby(['day'])['mins'].quantile(.5)\n\n\nThis gives me what I want,\n\nThe output isn't a dataframe and I needed the output to be a dataframe. \n\nOutput looks like:\n\n    day\n    2019-06-28    3.0\n    2019-06-30    4.0\n    2019-07-02    3.0\n    2019-07-06    3.0\n    2019-07-08    3.0\n    Name: mins, dtype: float64\n\n\nThanks!\n"
"How to extract only the pin-code and city from the address given and in particular column and assign it into two new pandas columns named as 'city' and 'address'. This is working fine with the regex in python pandas , is there any other quick way to run since it takes more than 6 min for 10000 rows.\n\nExample address:87 F/F  Place Opp. C-2, Uttam Nagar  NA  Delhi 110059 Delhi\n\npincoderegex=re.compile(r'([\\w]*)[\\s]([\\d]{6})')\npincoderegex.search(ref).group()  --- &gt; o/p : 'Delhi 110059'\npincoderegex.search(data_rnr['BORROWER ADDRESS'][80]).groups()[1] ---&gt; o/p:'700105'\ndata_rnr['BORROWER CITY_NAME']='default value'\ndata_rnr['BORROWER CITY_PINCODE']='default value'\nfor i in range(0,len(data_rnr['BORROWER ADDRESS'])):\n    try:\n        data_rnr['BORROWER CITY_NAME'][i]=pincoderegex.search(data_rnr['BORROWER ADDRESS'][i]).groups()[0]\n        data_rnr['BORROWER CITY_PINCODE'][i]=pincoderegex.search(data_rnr['BORROWER ADDRESS'][i]).groups()[1]\n    except TypeError:\n        print('TypeError')\n    except NameError:\n        print('NameError')\n    except AttributeError:\n        print('AttributeError')\n    except:\n        pass\n\n\nThe output will be added in the new Df columns data_rnr['BORROWER CITY_NAME'] and data_rnr['BORROWER CITY_PINCODE']\n"
'I am currently building a reccurent neural network model and i am currently stuck when i was about to transform my input data into a set on input/output for the RNN model.\nI have tried the windoe_tranform_series function that takes the series, window_size and the stepsize as inputs but i keep getting a KEYERROR.\ncutting our time series into sequences\nThe function below transforms the input series and window-size into a set #of input/output pairs for our RNN model.\ndef window_transform_series(series,window_size,step_size):\n    inputs = []\n    outputs = []\n    ctr = 0\n     for i in range(window_size, len(series), step_size):\n    inputs.append(series[ctr:i])\n    outputs.append(series[i])\n    ctr = ctr + step_size\nreturn inputs,outputs\n\nwindow_size = 7\nstep_size = 5\ninputs, outputs = window_transform_series(carbon_persil,window_size,step_size)\n\n\n\nKeyError                                  Traceback (most recent call last)\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   2656             try:\n-&gt; 2657                 return self._engine.get_loc(key)\n   2658             except KeyError:\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 7\n\nDuring handling of the above exception, another exception occurred:\n\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-45-9810d786d8b5&gt; in &lt;module&gt;\n      2 window_size = 7\n      3 step_size = 5\n----&gt; 4 inputs, outputs = window_transform_series(carbon_persil,window_size,step_size)\n\n&lt;ipython-input-41-82e8b484e9e9&gt; in window_transform_series(series, window_size, step_size)\n      9     for i in range(window_size, len(series), step_size):\n     10         inputs.append(series[ctr:i])\n---&gt; 11         outputs.append(series[i])\n     12         ctr = ctr + step_size\n     13     return inputs,outputs\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py in __getitem__(self, key)\n   2925             if self.columns.nlevels &gt; 1:\n   2926                 return self._getitem_multilevel(key)\n-&gt; 2927             indexer = self.columns.get_loc(key)\n   2928             if is_integer(indexer):\n   2929                 indexer = [indexer]\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py in get_loc(self, key, method, tolerance)\n   2657                 return self._engine.get_loc(key)\n   2658             except KeyError:\n-&gt; 2659                 return self._engine.get_loc(self._maybe_cast_indexer(key))\n   2660         indexer = self.get_indexer([key], method=method, tolerance=tolerance)\n   2661         if indexer.ndim &gt; 1 or indexer.size &gt; 1:\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas\\_libs\\index.pyx in pandas._libs.index.IndexEngine.get_loc()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\npandas\\_libs\\hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 7\n\n\n        \n\n'
"I am using the code below to use an if statement for a column in a df to result in a value \n\ndef time_delta(df):\n    if df['a_time'] &gt; df['b_time']:\n        res = 'Early'\n    else:\n        res = 'Late'\n    return res\n\n\nIs there a better or more efficient way to code this? \n\nBasically I want a new column with a value of either Early or Late based on the two time columns relationship. \n\nThanks!\n"
"I have a class in which I create a countVectorizer and create vectors with fit_transform. This generates a vocabulary_.\nI would like to have this CountVectorizer with the vocabulary in one file to be able to reuse it in another class.\nDoes anyone have any advice for me? I already tried to do the whole thing with save_npz. But it didn't work properly.\n\nHere's my function as I've tried to save the whole thing. I'm not sure if this is the right one.\n\n...\ncount_vect = CountVectorizer()\n...\n\ndef vectorizeData():\n\n  clean_data = pd.read_feather('../working/' + PROJECT + '_clean.feather') \n\n  word_count = count_vect.fit_transform(clean_data.text)\n\n  scipy.sparse.save_npz('../working/' + PROJECT + '_countVec.npz', word_count)\n\n\n\n\nand the load\n\n\ndef ModelData():\n  ...\n  count_vect_test = scipy.sparse.load_npz('../working/' + PROJECT + '_countVec.npz')\n  ...\n\n\n\nafter the load I have only the csr_matrix, but not the CountVectorizer object.\n"
'Im kinda new to Datascience et Python.\n\nFirst of all, do you suggest using any other Library than pandas when dealing with huge dataset (100K+ rows) ?\n\nSecond of all, lemme expose to you my current problem.\n\nI have a Dataset in wich i have a Datetime column, to make it easy to understand, let\'s say i only have a Datetime column named date_col.\n\nHere\'s what my date_col values looks like :\n\ndf=pd.DataFrame({\'dt_col\': ["2019-03-13 08:12:23", "2019-03-13 07:10:18", "2019-03-13 08:12:23", "2019-03-15 10:35:53", "2019-03-20 11:12:23", "2019-03-20 08:12:23"]})\n\n                dt_col\n0  2019-03-13 08:12:23\n1  2019-03-13 07:10:18\n2  2019-03-13 08:12:23\n3  2019-03-15 10:35:53\n4  2019-03-20 11:12:23\n5  2019-03-20 08:12:23\n\n\nI want to extract foreach day the minimum and the maximum hour or datetime, for example for 2019-03-13, i want to extract 2019-03-13 07:10:18 and 2019-03-13 08:12:23.\n\nI tought about :\n\n\nGetting distinc dates without the time from my DataFrame\nForeach of these dates, getting the min and max corresponding date from my Dataframe\n\n\nIm kinda stuck at step 2 as i don\'t know how to really achieve this in Python, i mean i can do it the "old way" with some loops but i don\'t think that it will do the job with a large Dataset.\n\nBtw, here\'s what i\'ve done for step 1:\n\ndates=pd.to_datetime(df.dt_col)\ndistinc_dates=dates.dt.strftime("%Y-%m-%d").unique()\n\n\nOnce i got those min and max, i want to generate datetime rows between each min and max datetime, for example between 2019-03-13 07:10:18 and 2019-03-13 08:12:23, i want to get 2019-03-13 07:10:18, 2019-03-13 07:10:19, 2019-03-13 07:10:20, 2019-03-13 07:10:21, 2019-03-13 07:10:22,..... until 2019-03-13 08:12:23.\n\nI think this can be achieved using pd.date_range. So once i have got my min and max, im thinking user using pd.date_tange to do something like this:\n\ndates=[]\nfor index,row in df.iterrows():\n    dates.append(pd.date_range(start=row[\'min\'], end=row[\'max\'], freq=\'1S\'))\nprint(dates)\n\n\nBut i know that iterrows is slow asf, , so im asking you guys for the best way to achieve this when having huge dataset.\n'
'I really new to Python and Datascience.\n\nI have a dataset with like 100K+ rows, i have two columns on this dataset.\n\nThe first one is a Datetime column, let\'s name it A, the last one is an Integer, let\'s name it B.\n\nMy Dataset is sorted by column A.\n\nIn my Dataset, some of B values are NaN.\n\nI want to fill my NaN Values using by doing this:\n\nFor row i with NaN B value: \n    If (Latest none NaN B value before my row i - First none NaN B value after my row i) == 0 set B value of row i to "Latest none NaN B value before my row i"\n    else set it to "Latest none NaN B value before my row i"-difference in seconds between column A of "Latest none NaN B value before my row i" and column A of my row i\n\nLet me explain myself with an example:\n\nMy Dataset looks like this :\n\n                     A     B\n0  2019-03-13 08:12:20  10.0\n1  2019-03-13 08:12:21   NaN\n2  2019-03-13 08:12:22   NaN\n3  2019-03-13 08:12:23  10.0\n4  2019-03-13 08:12:24   NaN\n5  2019-03-13 08:12:25   NaN\n6  2019-03-13 08:12:26   7.0\n\n\nAt the end i want it to look like this:\n\n                     A     B\n0  2019-03-13 08:12:20  10.0\n1  2019-03-13 08:12:21  10.0\n2  2019-03-13 08:12:22  10.0\n3  2019-03-13 08:12:23  10.0\n4  2019-03-13 08:12:24   9.0\n5  2019-03-13 08:12:25   8.0\n6  2019-03-13 08:12:26   7.0\n\n\n(Column B of row id=1) and (Column B of row id=2) are 10 because (Column B of row id=0)=(Column B of row id=3).\n(Column B of row id=4) is 9 because (Column B of row id=3)=/=(Column B of row id=6) and (Column B of row id=3)-(time_diff(Column A of row id=3, Column A of row id=4)).\n\nThe best i could do is to set the NaN to the latest known value, but that\'s really not what i want to do.\n\ndf=pd.DataFrame({\'A\': ["2019-03-13 08:12:20", "2019-03-13 08:12:21", "2019-03-13 08:12:22", "2019-03-13 08:12:23", "2019-03-13 08:12:24", "2019-03-13 08:12:25"], \'B\': [10, 10, 10, 9, 8, 7]})\ndf[\'B\'] = df[\'B\'].replace({\'B\': {0: np.nan}}).ffill()\nprint(df)\n\n\nYou guys have any clean way to achieve this ?\n'
'Im trying to implement digit classification with knn alogorithm.\n\n#pseudocode\n    note:(the order of the digits follow the order of the labels)\n    i.e \n    train={digit0,digit0,digit1,digit2...}\n\n    label={0,0,1,2...}\n    labels.shape = (10000,)\n    train.shape=(784*1000)\n\n\ni have a huge dataset of 10000 digits from 0 to 9 as images of 28 * 28 pixels along with their labels.the labels and digits are arranged in same order.\nSO i need to extract the digits 0 and 1 from the dataset and perform knn for different values of k={1,2,3,4,5} for digits 0 and 1 which are 28*28 pixels.i need help with digits extraction.\n\nany suggestions will be appreciated\n'
'So, the thing is I need to create a crosstable from string data. I mean like in excel, if You put some string data into crosstable it is going to be automatically transformed into counted values per the other factor. For instance, I have column \'A\' which contains application numbers and column \'B\' which contains dates. I need to show how many applications were placed per each day. Classic crosstable returns me an error.\n\ndata.columns = [[\'applicationnumber\', \'date\', \'param1\', \'param2\', \'param3\']] #mostly string values\n\n\nExamples of input data: \napplicationnumber = "AAA12345678"\ndate = \'YYYY-MM-DD\'\n'
"Recently I came across a code snippet. Please explain to me it's working.\n\narr = np.arange(9).reshape(3,3)\na1 = np.array([[1,2],[0,1]])\na2 = np.array([[0,2],[1,2]])\n\n#please explain this line \nout = arr[a1,a2]\n\nprint(out.sum())\n\n"
"I'm trying to create some material for introductory statistics for a seminar. The above code computes a 95% confidence interval for estimating the mean, but the result is not the same from the one implemented in Python. Is there something wrong with my math / code? Thanks.\n\nEDIT:\n\nData was sampled from here\n\nimport pandas as pd\nimport numpy as np\nx = np.random.normal(60000,15000,200)\nincome = pd.DataFrame()\nincome = pd.DataFrame()\nincome['Data Scientist'] = x\n\n# Manual Implementation\nsample_mean = income['Data Scientist'].mean()\nsample_std = income['Data Scientist'].std()\nstandard_error = sample_std / (np.sqrt(income.shape[0]))\nprint('Mean',sample_mean)\nprint('Std',sample_std)\nprint('Standard Error',standard_error)\nprint('(',sample_mean-2*standard_error,',',sample_mean+2*standard_error,')')\n\n\n# Python Library\nimport scipy.stats as st\nse = st.sem(income['Data Scientist'])\na = st.t.interval(0.95, len(income['Data Scientist'])-1, loc=sample_mean, scale=se)\nprint(a)\nprint('Standard Error from this code block',se)\n\n"
"I have numeric data within Student marks and I would like to group them into 3 categories A, B and C. \n\ndf = pd.DataFrame([('Adel',  3.5),\n                   ('Betty',  2.75),\n                   ('Djamel',  2.10),\n                   ('Ramzi',  1.75),\n                   ('Alexa', 3.15)],\n                  columns=('Name', 'GPA'))\n\n\nI tried function pd.cut() but it didn't lead to wanted result . \n"
"I have created sample graph using matplotlib-python with below code.\n\ndf =pd.DataFrame ({'City': ['London', 'Jakarta', 'Newyork', 'Mumbai'],\n                   'Staff': [1000,2000,3000,400]})\nprint (df.head())\ndf.plot(kind='line',x='City', y='Staff', color= 'Blue')\nplt.show()\n\n\nI am facing issue with the display of names when it is a line kind graph.\nBelow are the images where it shows names of the city in bar graph but not displaying in line type graph.\n\nHow do I make the city names show as labels for the X axis in the line plot?\n\nmatplotlib version used is : 3.1.3\n\n\n\n"
'I have written following Python code which should extract how many "C"s occur within the Bootstrap of votes. However,\n\nprint(Counter(bootstrap).get(\'C\'))\n\n\nreturns None\n\nimport numpy as np\nfrom datascience import *\nvotes = Table().with_column(\'vote\', np.array([\'C\']*470 + [\'T\']*380 + [\'J\']*80 + [\'S\']*30 + [\'U\']*40))\nfrom collections import Counter\ndef proportions_in_resamples():\n    prop_c = make_array()\n    for i in np.arange(5000):\n        bootstrap = votes.sample(votes.num_rows, with_replacement=False)\n        print(Counter(bootstrap).get(\'C\'))\n        single_proportion=Counter(bootstrap).get("C")/ bootstrap.num_rows\n        prop_c = np.append(prop_c, single_proportion)\n    return prop_c\n\n\nI was thinking that I could use np.count_nonzero but I am unsure how I could use this exactly to filter for "C" since\n\nsingle_proportion = np.count_nonzero(bootstrap=="C") / bootstrap.num_rows\n\n\nreturn 0.0\n\nThank you for your help!\n'
'We have the lets say fold value in KNN is N we need to divide the array in  N equal part and for each iteration of fold value we need to divide the train and test such way that\n\nexample :\nfold is 5\n1. In First iteration It Consider last means 5th part as test data and rest train data\n2. In Second iteration It Consider second last means 4th part as test data and rest train data\n3. In third iteration It Consider third last means 3rd part as test data and rest train data\n... so on\n5. In  Firth iteration It Consider first means 1st part as test data and rest train data\n\n\nHow we can achieve this in Python Can you please explain this .\n'
'I am facing a challenge finding Mean Average Error (MAE) using Pipeline and GridSearchCV\n\nBackground:\n\nI have worked on a Data Science project (MWE as below) where a MAE value would be returned of a classifier as it\'s performance metric. \n\n#Library\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n#Data import and preparation\ndata = pd.read_csv("data.csv")\ndata_features = [\'location\',\'event_type_count\',\'log_feature_count\',\'total_volume\',\'resource_type_count\',\'severity_type\']\nX = data[data_features]\ny = data.fault_severity\n\n#Train Validation Split for Cross Validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n#RandomForest Modeling\nRF_model = RandomForestClassifier(n_estimators=100, random_state=0)\nRF_model.fit(X_train, y_train)\n\n#RandomForest Prediction\ny_predict = RF_model.predict(X_valid)\n\n#MAE \nprint(mean_absolute_error(y_valid, y_predict))\n#Output:\n#   0.38727149627623564\n\n\nChallenge:\n\nNow I am trying to implement the same using Pipeline and GridSearchCV (MWE as below). The expectation is the same MAE value would be returned as above. Unfortunately I could not get it right using the 3 approaches below.\n\n#Library\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n#Data import and preparation\ndata = pd.read_csv("data.csv")\ndata_features = [\'location\',\'event_type_count\',\'log_feature_count\',\'total_volume\',\'resource_type_count\',\'severity_type\']\nX = data[data_features]\ny = data.fault_severity\n\n#Train Validation Split for Cross Validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n#RandomForest Modeling via Pipeline and Hyper-parameter tuning\nsteps = [(\'rf\', RandomForestClassifier(random_state=0))]\npipeline = Pipeline(steps) # define the pipeline object.\nparameters = {\'rf__n_estimators\':[100]}\ngrid = GridSearchCV(pipeline, param_grid=parameters, scoring=\'neg_mean_squared_error\', cv=None, refit=True)\ngrid.fit(X_train, y_train)\n\n#Approach 1:\nprint(grid.best_score_)\n# Output:\n#    -0.508130081300813\n\n#Approach 2:\ny_predict=grid.predict(X_valid)\nprint("score = %3.2f"%(grid.score(y_predict, y_valid)))\n# Output:\n#    ValueError: Expected 2D array, got 1D array instead:\n#    array=[0. 0. 0. ... 0. 1. 0.].\n#    Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n#Approach 3:\ny_predict_df = pd.DataFrame(y_predict.reshape(len(y_predict), -1),columns=[\'fault_severity\'])\nprint("score = %3.2f"%(grid.score(y_predict_df, y_valid)))\n# Output: \n#    ValueError: Number of features of the model must match the input. Model n_features is 6 and input n_features is 1 \n\n\nDiscussion:\n\nApproach 1:\n  As in GridSearchCV() the scoring variable is set to neg_mean_squared_error, tried to read the grid.best_score_. But it did not get the same MAE result.\n\nApproach 2:\n  Tried to get the y_predict values using grid.predict(X_valid). Then tried to get the MAE using grid.score(y_predict, y_valid) as the scoring variable in GridSearchCV() is set to neg_mean_squared_error. It returned a ValueError complaining "Expected 2D array, got 1D array instead".\n\nApproach 3:\n  Tried to reshape y_predict and it did not work either. This time it returned "ValueError: Number of features of the model must match the input."\n\nIt would be helpful if you can assist to point where I could have made the error?\n\nIf you need, the data.csv is available at https://www.dropbox.com/s/t1h53jg1hy4x33b/data.csv\n\nThank you very much\n'
"I am working with a large panel data of financial info, however the values are a bit spotty. I am trying to calculate the return between each year of each stock in my panel data. However, because of missing values sometimes firms have year gaps, making the: df['stock_ret'] = df.groupby(['tic'])['stock_price'].pct_change() impossible to practice as it would be wrong. The df looks something like this (just giving an example):\n\n       datadate      month     fyear    ticker    price\n0    31/12/1998         12      1998      AAPL   188.92\n1    31/12/1999         12      1999      AAPL   197.44\n2    31/12/2002         12      2002      AAPL   268.13\n3    31/12/2003         12      2003      AAPL   278.06\n4    31/12/2004         12      2004      AAPL   288.35\n5    31/12/2005         12      2005      AAPL   312.23\n6    31/05/2008          5      2008      TSLA    45.67\n7    31/05/2009          5      2009      TSLA    38.29\n8    31/05/2010          5      2010      TSLA    42.89\n9    31/05/2011          5      2011      TSLA    56.03\n10   31/05/2014          5      2014      TSLA   103.45\n..       ...            ..       ..        ..      ..\n\n\nWhat I am looking for is a piece of code that would allow me to understand (for each individual firm) if there is any gap in the data, and calculate returns for the two different series. Just like this: \n\n       datadate      month     fyear    ticker    price   return\n0    31/12/1998         12      1998      AAPL   188.92      NaN \n1    31/12/1999         12      1999      AAPL   197.44   0.0451\n2    31/12/2002         12      2002      AAPL   268.13      NaN\n3    31/12/2003         12      2003      AAPL   278.06   0.0370\n4    31/12/2004         12      2004      AAPL   288.35   0.0370\n5    31/12/2005         12      2005      AAPL   312.23   0.0828\n6    31/05/2008          5      2008      TSLA    45.67      NaN\n7    31/05/2009          5      2009      TSLA    38.29  -0.1616\n8    31/05/2010          5      2010      TSLA    42.89   0.1201\n9    31/05/2011          5      2011      TSLA    56.03   0.3063\n10   31/05/2014          5      2014      TSLA   103.45      NaN\n..       ...            ..       ..        ..      ..\n\n\nIf you have any other suggestions on how to treat this problem, please feel free to share your knowledge :) I am a bit inexperienced so I am sure that your advice could help!\n\nThank you in advance guys!\n"
"I need to fit two different regression models to the following df (e.g. curves belonging to two different families).\n\nThus, thinking about the type of regression that I know, there are : \n\n\nLinear regression (the green one in the pic below, just to show)\nPolynomial regression\nRidge regression\nLasso regression \nElasticNet regression  \n\n\nThe df is super-simple, just two columns xand y with 450 entries each.\n\nThe scatter plot is the following:\n\n\n\nBUT, when I go through the train/test plotting process I get the following one : \n\n\n\nNow, it is clear that a simple linear model can't be enough with a train/test distribution like this.\n\nBUT, when I move to investigate the MSE (mean squared error) I get something interesting :\n\nTrain Error: 0.06336815111266955\nTest Error: 0.06359148208824823\n\n\nI am sure about the code (that I not reported). I checked it with another toy dataset and worked perfectly. \n\nMay someone help me, please?\n\nMany thanks in advance! \n\nEDIT: in the model fitting process I applied MinMaxScaler() in range [0,1] function\n"
"I have a dataframe with indexes names of countries and columns medals.\nI want to get the name of the country with the most number of gold medals.\nI've tried this : \n\ndef answer_one():\n    x= df[df['Gold.2']==df['Gold.2'].max()]\n    return x.index\nanswer_one()\n\n\nI want to get just the string which is the name of the country but instead I keep getting this \n\nIndex(['United States'], dtype='object')\n\n"
"I have a dataframe (df):\n\n        date                O_3     NO_2        SO_2        PM10        PM25        CO      Label\n    0   2001-01-01 01:00:00 7.86    67.120003   26.459999   32.349998   12.505127   0.45    2.0\n    1   2001-01-01 02:00:00 7.21    70.620003   20.879999   40.709999   12.505127   0.48    2.0\n    2   2001-01-01 03:00:00 7.11    72.629997   21.580000   50.209999   12.505127   0.41    2.0\n    3   2001-01-01 04:00:00 7.14    75.029999   19.270000   54.880001   12.505127   0.51    2.0\n    4   2001-01-01 05:00:00 8.46    66.589996   13.640000   42.340000   12.505127   0.19    2.0\n    ... ... ... ... ... ... ... ... ...\n139603  2018-04-30 20:00:00 63.00   58.000000   4.000000    2.000000    2.000000    0.30    1.0\n139604  2018-04-30 21:00:00 49.00   65.000000   4.000000    5.000000    4.000000    0.30    2.0\n139605  2018-04-30 22:00:00 49.00   58.000000   4.000000    5.000000    3.000000    0.30    2.0\n139606  2018-04-30 23:00:00 48.00   52.000000   4.000000    7.000000    7.000000    0.30    2.0\n139607  2018-05-01 00:00:00 52.00   43.000000   4.000000    6.000000    4.000000    0.30    1.0\n\n\nI want to know the variability of the 'Label' values, hence I:\n\n# Variability of 'Labels' values\nreshape_df['Label'].value_counts()\n\n\nand I get:\n\n2.0    80435\n1.0    39393\n3.0    15045\n4.0     3295\n5.0     1440\nName: Label, dtype: int64\n\n\nI add a new column in order to see the maximum value column name on each row:\n\n# Create column with max pollutant name\nreshape_df['Max_pollutant'] = reshape_df.eq(reshape_df.max(1), axis=0).dot(reshape_df.columns)\n\n\nand I get:\n\ndate                        O_3     NO_2        SO_2        PM10        PM25        CO      Label       Max_pollutant\n0       2001-01-01 01:00:00 7.86    67.120003   26.459999   32.349998   12.505127   0.45    2.0         NO_2\n1       2001-01-01 02:00:00 7.21    70.620003   20.879999   40.709999   12.505127   0.48    2.0         NO_2\n2       2001-01-01 03:00:00 7.11    72.629997   21.580000   50.209999   12.505127   0.41    2.0         NO_2\n3       2001-01-01 04:00:00 7.14    75.029999   19.270000   54.880001   12.505127   0.51    2.0         NO_2\n4       2001-01-01 05:00:00 8.46    66.589996   13.640000   42.340000   12.505127   0.19    2.0         NO_2\n... ... ... ... ... ... ... ... ... ...\n139603  2018-04-30 20:00:00 63.00   58.000000   4.000000    2.000000    2.000000    0.30    1.0         O_3\n139604  2018-04-30 21:00:00 49.00   65.000000   4.000000    5.000000    4.000000    0.30    2.0         NO_2\n139605  2018-04-30 22:00:00 49.00   58.000000   4.000000    5.000000    3.000000    0.30    2.0         NO_2\n139606  2018-04-30 23:00:00 48.00   52.000000   4.000000    7.000000    7.000000    0.30    2.0         NO_2\n139607  2018-05-01 00:00:00 52.00   43.000000   4.000000    6.000000    4.000000    0.30    1.0         O_3\n\n\nIf I check the variability of 'Max_pollutant':\n\n# Variability of 'Max_pollutant' names\nreshape_df['Max_pollutant'].value_counts()\n\n\nI get the following output:\n\nNO_2           91155\nO_3            43166\nPM10            4760\nO_3NO_2          417\nNO_2PM10          48\nSO_2              23\nO_3PM10           22\nPM25              15\nO_3NO_2PM10        2\nName: Max_pollutant, dtype: int64\n\n\nI don't quite understand the value counts where two or more pollutants appear. For example, 'O_3NO_2' = 417, does that mean that the maximum value for O_3 is the same as NO_2?\n\nHow can I print those row in particularly in oprder to see the readings of each pollutant?\n"
'I am trying to fine tune Logistic Regression using Hyperopt in Python. Please find the optimization function below:\n\ndef objective(params, n_folds = N_FOLDS):\n    """Objective function for Logistic Regression Hyperparameter Tuning"""\n\n    # Perform n_fold cross validation with hyperparameters\n    # Use early stopping and evaluate based on ROC AUC\n\n    clf = LogisticRegression(**params,random_state=0,verbose =0)\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    score = cross_val_score(clf, X_train, y_train, cv=cv, scoring=\'f1_macro\')\n\n    # Extract the best score\n    best_score = max(score)\n\n    # Loss must be minimized\n    loss = 1 - best_score\n\n    # Dictionary with information for evaluation\n    return {\'loss\': loss, \'params\': params, \'status\': STATUS_OK}\n\n\nWhile running the algorithm, It is showing me the following error.\n\nTypeError: can\'t pickle module objects\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-22-934e4c484469&gt; in &lt;module&gt;\n      6 \n      7 # Optimize\n----&gt; 8 best = fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)\n\n&lt;ipython-input-20-d9ea5e676676&gt; in objective(params, n_folds)\n      7     clf = LogisticRegression(**params,random_state=0,verbose =0)\n      8     cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n----&gt; 9     score = cross_val_score(clf, X_train, y_train, cv=cv, scoring=\'f1_macro\')\n      10 \n      11     # Extract the best score\n\n\nHow can I solve this problem?\n'
'So, I have a dataframe like this\n\nIn [1]: df = pd.DataFrame([[\'indiejesse.d@gmail.com; pamelasilvera69@gmail.com; kristinestringer69@gmail.com\', \'conference meeting ...\']], columns=[\'CC\', \'Body\'])\n\n\nThe column "CC" contains a list of emails separated by ";". I want to iterate through the "CC" column, count the number of emails separated by ";" in each of the rows and add a new column let\'s say "CC_count" after CC which contains the number CC emails of each row.\n\nHow do I do this?\n'
'I have the following scenario: I have a train.csv file as the one below. Each row is mentioned 4 times with same index value.\nIndex sentence ending0 ending1 ending2 ending3 \n\n0        ABC     DEF     GHI     JKL     MNO     \n0        ABC     DEF     GHI     JKL     MNO       \n0        ABC     DEF     GHI     JKL     MNO     \n0        ABC     DEF     GHI     JKL     MNO       \n1        LKJ     KJS     AJA     QHW     IUH             \n...      ...     ...     ...     ...     ...\n...\n...  \n2 \n...\n...\n...     \n\nWhat i am wanting to get is shown below:\nIndex sentence ending-id ending \n0       ABC       0        DEF    \n0       ABC       1        GHI    \n0       ABC       2        JKL    \n0       ABC       3        MNO    \n1       LKJ       0        KJS \n...     ...      ...       ...\n...\n...   \n\n'
"I found a question that gives an equation to normalize, but doesn't address syntax:\nQuestion:\nNormalize any value in range (-inf…+inf) to (0…1). Is it possible?\nEquation I want to use:\n(1 + x / (1 + abs(x))) / 2\n\nIn my dataframe, the output is identical all the way down my new 'normalized_column', even though the input variables in my 'not_normalized_column' are unique.\nI'm accessing the values by column name: 'not_normalized_column'.\nWhat is the best way to perform the operation for all values in the column? Thanks!\nHere is my (failed) code:\nimport pandas as pd\n\nexcel_file = r&quot;C:\\Users\\kevin\\Desktop\\file.xlsx&quot;\ndf = pd.read_excel(excel_file)\n\n\nfor x in df['not_normalized_column']:\n    df['normalized_column'] = (1 + x / (1 + abs(x))) / 2 \n\noutput:\nprint(df.head(20))\n\n    not_normalized_column  normalized_column\n0                1.192575           0.866625\n1                1.517879           0.866625\n2                1.550685           0.866625\n3                1.680974           0.866625\n4                1.600331           0.866625\n5                1.600675           0.866625\n6                1.599243           0.866625\n7                1.577447           0.866625\n8                1.546771           0.866625\n9                1.513165           0.866625\n10               1.481408           0.866625\n11               1.446590           0.866625\n12               1.415659           0.866625\n13               1.386197           0.866625\n14               1.355200           0.866625\n15               1.335662           0.866625\n16               1.321336           0.866625\n17               1.316952           0.866625\n18               1.306000           0.866625\n19               1.302547           0.866625\n\n"
"I have a dataset of velocities registered by sensors on highways and I'm changing the label values for the avg5 (velocities average of 5 minutes timestamp) 2 hours in the future (the normal is 30 minutes. The label value of now is the observed avg5 of 30 minutes in the future).\nMy dataset have the following features and values:\n\n\nAnd I'm doing this switch of values by this way:\nhours_added = datetime.timedelta(hours = 2)\n\nfor index in data_copy.index:\n\n  hours_ahead = data.loc[index, &quot;timestamp5&quot;] + hours_added\n  result = data_copy[((data_copy[&quot;timestamp5&quot;] == hours_ahead) &amp; (data_copy[&quot;sensor_id&quot;] == data_copy[&quot;sensor_id&quot;].loc[index]))]\n\n  if len(result) == 1:\n    data_copy.at[index, &quot;label&quot;] = result[&quot;avg5&quot;]\n\n  if(index % 50 == 0):\n    print(f&quot;Index: {index}&quot;)\n\nThe code is querying 2 hours ahead and catching the result for the same sensor_id that I'm iterating now. I only change the value of my label if the result brings me something (len(result) == 1).\nMy dataframe has 2950521 indexes and at the moment I'm publishing this question the kernel is running for more then 24 hours and only reached the 371650 Index.\nSo I started thinking that I'm doing something wrong or if have a better way of change these values who don't take so long time.\nFor reproducing purposes\nThe desired behavior is to assign the avg5 of the respective sensor_id of 2 hours in the future for the label 2 hours before.\nFor reproducibility, take as example a sample of my dataset with the 10 first registers:\n{'sensor_id': {0: 1385001,\n  1: 1385001,\n  2: 1385001,\n  3: 1385001,\n  4: 1385001,\n  5: 1385001,\n  6: 1385001,\n  7: 1385001,\n  8: 1385001,\n  9: 1385001},\n 'label': {0: 50.79999923706055,\n  1: 52.69230651855469,\n  2: 50.0,\n  3: 48.61538314819336,\n  4: 48.0,\n  5: 47.90909194946289,\n  6: 51.41666793823242,\n  7: 48.3684196472168,\n  8: 49.8636360168457,\n  9: 48.66666793823242},\n 'avg5': {0: 49.484848,\n  1: 51.735294,\n  2: 51.59375,\n  3: 49.266666,\n  4: 50.135135999999996,\n  5: 50.5,\n  6: 50.8,\n  7: 52.69230699999999,\n  8: 50.0,\n  9: 48.615383},\n 'timestamp5': {0: Timestamp('2014-08-01 00:00:00'),\n  1: Timestamp('2014-08-01 00:05:00'),\n  2: Timestamp('2014-08-01 00:10:00'),\n  3: Timestamp('2014-08-01 00:15:00'),\n  4: Timestamp('2014-08-01 00:20:00'),\n  5: Timestamp('2014-08-01 00:25:00'),\n  6: Timestamp('2014-08-01 00:30:00'),\n  7: Timestamp('2014-08-01 00:35:00'),\n  8: Timestamp('2014-08-01 00:40:00'),\n  9: Timestamp('2014-08-01 00:45:00')}}\n\nLet's suppose I want to assign to the first label (where the timestamp is 2014-08-01 00:00:00) the value of the avg5 field of 40 minutes ahead (2014-08-01 00:40:00) for the same sensor_id (in this example all the sensors are the same, but I have a lot of more others registered at the dataset), so my label of the index 0 (50.79) should receive the value of the avg5 of index 8 (50.0) if the sensor_id are the same.\n"
"Does anyone could help with this code? I'm writing this code to compute some values over some equations. I satarted by reading the CSV values into a dictionary, but after calculating the values to reach the final set of parameters, I cannot find a way to iterate repeatedly over the same list.\nTo simply, I have two input lists dPdT and listT. I need to iterate every parameter of list dPdT over listT and produce three different lists P.\nI thank anyone willing to help. This is a study project for a course.\n# Request user's interval parameters for calculations\nprint(&quot;Inform the temperature range and interval (°C). Only integers.&quot;)\nminT = int(input(&quot;Min: &quot;))\nmaxT = int(input(&quot;Max: &quot;))\nstepT = int(input(&quot;Temperature interval: &quot;))\n\n# create a list of temperature values to compute pressure parameters\nlistT = []\n\nfor x in range(minT, (maxT+stepT), stepT):\n    listT.append(x)\n\n# Open CSV file in read mode to acces data and read it into a dictionary\nwith open(CSVfile, &quot;r&quot;) as CSV:\n    reader = csv.DictReader(CSV)\n\n    listDict = []\n\n    # Creates a list of dictionaries with the fluid inclusion parameters\n    for lines in reader:\n        listDict.append(lines)\n\n    # Define list parameters to be computated\n    a, b, c, dPdT, P = [], [], [], [], []\n\n    # Loop iterates over the dictionary list and computates parameters a,b,c stored in lists a,b,c\n    for i, rows in enumerate(listDict):\n        a.append(i)\n        b.append(i)\n        c.append(i)\n        if &quot;sal&quot; in rows:\n            a[i] = (18.28 + 1.4413 * float(rows[&quot;sal&quot;]) + 0.0047241 * float(rows[&quot;sal&quot;]) ** 2\n                + 0.0024213 * float(rows[&quot;sal&quot;]) ** 3 + 0.000038064 * float(rows[&quot;sal&quot;]) ** 4)\n            b[i] = (0.019041 - 1.4268 * 0.01 * float(rows[&quot;sal&quot;]) + 5.66012 * 0.0001 * float(rows[&quot;sal&quot;]) ** 2\n                - 4.2329 * 0.000001 * float(rows[&quot;sal&quot;]) ** 3 - 3.0354 * 0.00000001 * float(rows[&quot;sal&quot;]) ** 4)\n            c[i] = (- 1.5988 * 0.0001 + 3.6892 * (10 ** -5) * float(rows[&quot;sal&quot;]) - 1.9473 * (10 ** -6) * float(rows[&quot;sal&quot;]) ** 2\n                + 4.1674 * (10 ** -8) * float(rows[&quot;sal&quot;]) ** 3 - 3.3008 * (10 ** -10) * float(rows[&quot;sal&quot;]) ** 4)\n\n    # Loop iterates over the dictionary list and computates dPdT to store the values in list dPdT\n    for i, rows in enumerate(listDict):\n        dPdT.append(i)\n        if &quot;th&quot; in rows:\n            dPdT[i] = a[i] + b[i] * float(rows[&quot;th&quot;])  + c[i] * float(rows[&quot;th&quot;]) ** 2\n\n    # Loop populates list P (pressure values)\n    for i in range(len(listT)):\n        P.append(i)\n    \n    # problem starts here: I need to iterate over the listT or P, repeating it for every dPdT values.\n    # Loop to calculate P based on lits dPdT and listT. \n    while i in range(len(dPdT)):\n        for j in range(len(P)):\n            P[j] = dPdT[j] * listT[j]\n\n"
"I try to cluster my dataset with python and scikit-learn. It's an exercise of my University. The dataset looks like this:\nStationNr        A1        A2        A3        A4        A5        A6        A7        A8        A9        A10        A11        A12\n\n  \n    0        2.45        4.32        3.5      1.89      2.87      4.34      2.67      3.90      3.97       2.61       3.01       2.95\n  \n  1200       3.01        2.95        3.53     1.8       2.8       4.3       2.67      2.87      2.45       4.32       3.53       2.95            \n\nThe column &quot;StationNr&quot; has the following different values:\n[   0, 1200,  900,  100,  400,  300,  600,  200, 1100,  500,\n1000,  800,  700] \nIf there is a zero in &quot;StationNr&quot;, it means that the product is good. The other numbers has the meaning, on which station the product declared bad. \nThe columns &quot;A%&quot; has the following different values and the columns reprensent a processing station:  [ 2.017,  1.767,  0.987, ..., 24.083, 18.977,  4.904] \nThere are about 4000 different values for &quot;A1&quot;. This values are the duration of the processing on the station.\nNow, I want to know, is there a cluster in duration of processing on stations (A1, A2, A3 ... ,A11 ,A12), for example a single value or the combination of values, that's result is a bad or a good product. Good products has the &quot;StationNr&quot; == 0  and bad products has the &quot;StationNr&quot; != 0.\nI select the k-Means algorithm to explore the data and i don't how-to get my exercise in combination with k-Means in python code.\nMaybe k-Means is not the best algorithm for this case, then I will be happy, if you suggest a better algorithm.\nI'm very new in this topic and will be happy, if you help me to clustering my data.\nBest regards\nChristian\n"
'I have Sr.no columns in my csv file witch contain all integer values but will reading it as pandas data frame some integer values are converted into float why?\nI have Data set contain following records \nWhen i load it as Data Frame it shows like this \n These are n th records of same data set\nBut this time in Data Frame SR.NO column it is showing float values\n'
'I am new to Keras and still looking for ways for continuous training the model. Since my dataset is very large to store in memory, I am supposed to store in a DB (NoSql DB- MongoDb or HBase) and train records as batch wise. My model LSTM - multi input &amp; outputs. How my current trainings and prediction are as following.\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=in_dim, activation=&quot;relu&quot;))\nmodel.add(Dense(out_dim))\nmodel.compile(loss=&quot;mse&quot;, optimizer=&quot;adam&quot;)\nmodel.summary()\n\nmodel.fit(xtrain, ytrain, epochs=100, batch_size=12, verbose=0)    \nypred = model.predict(xtest)\n\nHowever, still I am looking for very clear and simple samples that shows how to feed batch wise records pulled from DB to train the model.\n'
"I want to use MinMaxScaler from sklearn to scale test and training data  before analyzing it.\nI've been following a tutorial (https://mc.ai/an-introduction-on-time-series-forecasting-with-simple-neura-networks-lstm/), but I get an error message ValueError: Expected 2D array, got 1D array instead.\nI tried looking at Print predict ValueError: Expected 2D array, got 1D array instead, but I get an error message if I try train = train.reshape(-1, 1) or test = test.reshape(-1, 1) because they are series (error message AttributeError: 'Series' object has no attribute 'reshape')\nHow do I best resolve this?\n# Import libraries \nimport pandas as pd \nfrom sklearn.preprocessing import MinMaxScaler \n\n# Create MWE dataset \ndata = [['1981-11-03', 510], ['1982-11-03', 540], ['1983-11-03', 480],\n   ['1984-11-03', 490], ['1985-11-03', 492], ['1986-11-03', 380],\n   ['1987-11-03', 440], ['1988-11-03', 640], ['1989-11-03', 560], \n   ['1990-11-03', 660], ['1991-11-03', 610], ['1992-11-03', 480]] \n\ndf = pd.DataFrame(data, columns = ['Date', 'Tickets']) \n\n# Set 'Date' to datetime data type \ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Set 'Date to index   \ndf = df.set_index(['Date'], drop=True)\n\n# Split dataset into train and test  \nsplit_date = pd.Timestamp('1989-11-03')\ndf =  df['Tickets']\ntrain = df.loc[:split_date]\ntest = df.loc[split_date:]\n\n# Scale train and test data \nscaler = MinMaxScaler(feature_range=(-1, 1))\ntrain_sc = scaler.fit_transform(train)\ntest_sc = scaler.transform(test)\n\nX_train = train_sc[:-1]\ny_train = train_sc[1:]\nX_test = test_sc[:-1]\ny_test = test_sc[1:]\n\n# ERROR MESSAGE \n  ValueError: Expected 2D array, got 1D array instead:\n  array=[510. 540. 480. 490. 492. 380. 440. 640. 560.].\n  Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\n"
"Until now:\n\nI load a csv file, without header. I put in new column names.\n\nI strip away all spaces\n\nI organize the data. All R150 appear.\nBut I cant do a Boxplot of my Istwert Column. Error:\n&quot;None of [Index([',Istwert'], dtype='object')] are in the [columns]&quot;\n\n\nIf I save the csv, I do not find any suspicious.\nAny ideas?\nThe code so far:\n    import matplotlib as mpl\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    df = pd.read_csv(&quot;&lt;FILELOCATION&gt;&quot;, delimiter=&quot;;&quot; , skiprows = 1, names=[&quot;BID&quot;,&quot;Testschritt&quot;,&quot;Testbeschreibung&quot;,&quot;Sollwert&quot;,&quot;Minimum&quot;,&quot;Maximum&quot;,&quot;Istwert&quot;,&quot;Einheit&quot;])\n    df = df.apply(lambda x: x.str.strip() if (x.dtype == &quot;object&quot;) | (x.dtype == &quot;float&quot;) else x)\n    result = df.loc[df[&quot;Testschritt&quot;] == &quot;R150&quot;]\n    result.boxplot(column = [&quot;Istwert&quot;])\n\nThis is my CSV data:\n\nAnd this is my result before the boxplot:\n\n"
'    Year    Week_Number DC_Zip  Asin_code\n0   2016    1   12206   NaN\n1   2016    1   29306   NaN\n2   2016    1   33426   NaN\n3   2016    1   37206   NaN\n4   2016    1   45216   NaN\n5   2016    1   60160   NaN\n6   2016    1   76110   NaN\n7   2016    1   80215   NaN\n8   2016    1   84105   NaN\n9   2016    1   85034   NaN\n10  2016    1   93711   NaN\n11  2016    1   98433   NaN\n12  2016    2   12206   21.0\n13  2016    2   29306   10.0\n14  2016    2   33426   11.0\n15  2016    2   37206   1.0\n16  2016    2   45216   5.0\n17  2016    2   60160   7.0\n18  2016    2   76110   12.0\n19  2016    2   80215   NaN\n20  2016    2   84105   2.0\n21  2016    2   85034   1.0\n22  2016    2   93711   23.0\n23  2016    2   98433   7.0\n24  2016    3   12206   95.0\n25  2016    3   29306   26.0\n26  2016    3   33426   51.0\n27  2016    3   37206   18.0\n28  2016    3   45216   34.0\n29  2016    3   60160   30.0\n... ... ... ... ...\n2778    2020    29  76110   33.0\n2779    2020    29  80215   5.0\n2780    2020    29  84105   3.0\n2781    2020    29  85034   8.0\n2782    2020    29  93711   53.0\n2783    2020    29  98433   15.0\n2784    2020    30  12206   75.0\n2785    2020    30  29306   27.0\n2786    2020    30  33426   34.0\n2787    2020    30  37206   12.0\n2788    2020    30  45216   14.0\n2789    2020    30  60160   28.0\n2790    2020    30  76110   47.0\n2791    2020    30  80215   11.0\n2792    2020    30  84105   3.0\n2793    2020    30  85034   17.0\n2794    2020    30  93711   62.0\n2795    2020    30  98433   13.0\n2796    2020    31  12206   109.0\n2797    2020    31  29306   30.0\n2798    2020    31  33426   31.0\n2799    2020    31  37206   14.0\n2800    2020    31  45216   23.0\n2801    2020    31  60160   21.0\n2802    2020    31  76110   25.0\n2803    2020    31  80215   7.0\n2804    2020    31  84105   4.0\n2805    2020    31  85034   8.0\n2806    2020    31  93711   71.0\n2807    2020    31  98433   9.0\n2808 rows × 4 columns\n\nThis is the sales data I am dealing with. I have to perform a weighted average on Asin_code with weighted rate = [5, 5, 20, 30, 40] on respective years 2016, 2017, 2018, 2019 and 2020. I have to create a function so that it will give me a column containing the weighted average of Asin_code.&quot;Nan&quot; values should be dropped. We should also change the weighted rate in the future to view more patterns with the data. Any help would be appreciated.\ni am trying the following code:\nfor i in range(len(df.Asin_code)):          \n df[&quot;Weighted_avg&quot;]=rate[0]*df.Asin_code[i]/df.Asin_code.loc[(df.Year==2016)].sum()\n\njust facing difficulties in consolidating the data for whole 5 years.\n'
"   import pandas as pd\n   data = {'Company':['GOOG','MSFT','FB','GOOG','MSFT','FB'],\n       'Dates':[&quot;1970-01-01 01:00:00&quot;,&quot;1970-01-01 01:00:02&quot;,&quot;1970-01-01 01:00:03&quot;,&quot;1970-01-01 01:00:04&quot;,&quot;1970-01-01 01:00:05&quot;,&quot;1970-01-01 01:00:06&quot;]}\n   df = pd.DataFrame(data)\n   df[&quot;Sales&quot;]=pd.to_datetime(df[&quot;Sales&quot;])\n   df.Sales.diff().dt.total_seconds()/3600\n\nThis code gives me output\n   0         NaN\n   1    0.000556\n   2    0.000278\n   3    0.000278\n   4    0.000278\n   5    0.000278\n   Name: Sales, dtype: float64\n\nand\ndf.groupby(&quot;Company&quot;).Sales.diff().dt.total_seconds()/3600\n\nthis gives me output\n   0         NaN\n   1         NaN\n   2         NaN\n   3    0.001111\n   4    0.000833\n   5    0.000833\n   Name: Sales, dtype: float64\n\nCan you explain what groupby function does here?\n"
"I'm working on some data analysis and I'm trying to make some changes to the y-axis of my plot. I'm using Seaborn and using distplot().\nCurrently this is my plot:\n\nbut I'm trying to achieve this\n\nAm wondering is there a specific argument I need to include in distplot(), or is there some formatting method I could use?\n"
"I am trying to learn about Tables and while importing datascience library as:\nfrom datascience import *\n\n\nOr by\nimport datascience as ds\n\nI get an error saying\n\nTypeError: use() got an unexpected keyword argument 'warn'\n\nI get this error in module: matplotlib.use()\nI have no any code other than that import line.\nI'm using python version 3.8.5 and pip version 20.2.1. I installed the whole datascience libraries by:\npip install datascience\nI only found this solution in this topic but the error persisted after trying it. What's going on?\n"
"I'm trying to train a Linear Regression Model with Python via using Google Stock Prices that can be found here: https://www.kaggle.com/medharawat/google-stock-price\nAnd trying to predict future stocks by given features. After that I'm planning to plot it with the values in current dataset.\nFirst, I read dataframes with date values with date parser and concatted these 2 dataframes into one in order to split it myself:\nparser = lambda date: pd.datetime.strptime(date, '%m/%d/%Y')\ndf_test=pd.read_csv(&quot;/kaggle/input/google-stock-price/Google_Stock_Price_Test.csv&quot;,parse_dates=[0], date_parser=parser)\ndf_train=pd.read_csv(&quot;/kaggle/input/google-stock-price/Google_Stock_Price_Train.csv&quot;,parse_dates=[0], date_parser=parser)\ndf=pd.concat([df_train,df_test])\n\nThen I changed the type of Close column as &quot;float64&quot; and plotted the Date-Close relation via using seaborn:\nimport seaborn as sns\nsns.relplot(x='Date', y='Close', data=df,kind=&quot;line&quot;)\n\nThe output is:\n\nI've managed the necessary column translations until this part of the code. In this part I split the data frame, create and trained model, and predicted values.\nfrom sklearn.model_selection import train_test_split\n\nX=df[[&quot;Open&quot;,&quot;High&quot;,&quot;Low&quot;,&quot;pc&quot;]]\ny=df[&quot;Close&quot;]     \nX_train,X_test,y_train,y_test = train_test_split(X,y)\n\nfrom sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X_train,y_train)\nmodel.score(X_test,y_test)\ny_pred=model.predict(X_test)\n\nWhat I want to achieve after this part is I want to set these predictions' dates for future dates in order to combine them into my data frame and plot. I managed to create 2 data frames for real and predicted data and concat and melt them into new dataframe in order to plot it.\ndates=(df[-320:][&quot;Date&quot;]).values\ndf_plot=pd.DataFrame(columns=[&quot;Date&quot;,&quot;Close&quot;])\ndf_plot[&quot;Date&quot;]=dates\ndf_plot[&quot;Close&quot;]=y_test.values.transpose()\n\ndf_predd=pd.DataFrame(columns=[&quot;Predicted&quot;,&quot;Date&quot;])\ndf_predd[&quot;Predicted&quot;]=y_pred.transpose()\ndf_predd[&quot;Date&quot;]=dates\ndf_predd[&quot;Date&quot;]=df_predd[&quot;Date&quot;]+pd.offsets.DateOffset(years=8) #I want to plot it as future predictions\n\nconcatenated = pd.concat([df_predd.assign(dataset='df_predd'), df_plot.assign(dataset='df_plot')],axis=0)\nmelted_df=pd.melt(concatenated,id_vars=[&quot;Date&quot;],value_vars=[&quot;Predicted&quot;,&quot;Close&quot;])\n\nsns.relplot(x='Date', y='value', data=melted_df,hue=&quot;variable&quot;,style='variable',kind=&quot;line&quot;,height=10)\n\nHere's the undesired output:\n\nI want an output something like that:\n\nWhat Am I Missing? I checked the Date Column's type. It's datetime. I can't spread the x-axis as the first plot shown above.\nAny helps will be appreciated. Thanks in advance.\n"
"I have a CSV file format like this:\nfilename  value  class\nfile1     55     mango\nfile1     62     orange\nfile1     74     apple\nfile2     75     mango\nfile2     42     orange\nfile3     89     apple\nfile4     54     orange\nfile4     35     apple\n\nI want to transform this on the basis of its class values, like this:\nfilename   mango  apple  orange\nfile1      55     74     62\nfile2      75     0      42\nfile3      0      89     0\nfile4      0      54     35\n\nPlease, help me with this.\nI have tried the groupby method but that didn't work.\n"
"I have code below as,\ndf['New_Column'] = df[['On Night Shift?']].apply(lambda x: any(x == 'Yes'), axis = 1)\n\nwhich gives me a True or False value in the new column if the column value of On Night Shift? is Yes, if I want to scan multiple different columns for no, how would I tweak this code?\nSomething like below code,\ndf['New_Column'] = df[['On Night Shift?']].apply(lambda x: any(x == 'No'), axis = 1)\n\nexcept I want to look if the columns On Day Shift?, On Mid Shift, On Weekend Shift? says no as well then give me True in the new column if all say no\nThanks!\n"
"I want to read in a txt file that sits in a folder within a zipped folder as a pandas data frame.\nI've looked at how to read in a txt file and how to access a file from within a zipped folder,  Load data from txt with pandas and Download Returned Zip file from URL respectively.\nThe problem is I get a KeyError message with my code.\nI think it's because my txt file sits in a folder within a folder?\nThanks for any help!\n# MWE\n\nimport requests\nimport pandas as pd\nfrom zipfile import ZipFile\nfrom io import BytesIO\n\n\ntxt_raw = 'hcc-data.txt'\nzip_raw = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00423/hcc-survival.zip'\n\nr = requests.get(zip_raw)\nfiles = ZipFile(BytesIO(r.content))\ndf_raw = pd.read_csv(files.open(txt_raw), sep=&quot;,&quot;, header=None)\n\n\n# ERROR\nKeyError: &quot;There is no item named 'hcc-data.txt' in the archive&quot;\n\n"
"screen scrape\n    `url = 'https://www.pro-football- \n    reference.com/teams/buf/2020/gamelog/'\n    BuffaloBillsO = pd.read_html(url)[0]`\n\nrenaming the unnamed level 1's\nBuffaloBillsO.rename(columns={'Unnamed: 4_level_1': 'W/L'}, inplace=True)\nBuffaloBillsO.rename(columns={'Unnamed: 3_level_1': 'Box_link'}, inplace=True)\n\nBuffaloBillsO.rename(columns={'Unnamed: 6_level_1': '@'}, inplace=True)\n\nBuffaloBillsO.rename(columns={'Unnamed: 3_level_1': 'Box_link'}, inplace=True)\n\nstrong text\nI have tried variations of the following solution but it replaces first two row and turns the first row of data into the column headers:\nBuffaloBillsO.columns = BuffaloBillsO.iloc[0]  \nBuffaloBillsO=BuffaloBillsO[1:]\nBuffaloBillsO.rename_axis(columns=None).reset_index(drop=True) \n\n"
'I have 2 tables that look like this:\nTable 1\n| ID | Tel. |   Name  |\n|:--:|:----:|:-------:|\n| 1  | 1234 | Denis   |\n| 2  | 4567 | Michael |\n| 3  | 3425 | Peter   |\n| 4  | 3242 | Mary    |\n\nTable 2\n| ID | Contact Date |\n|:--:|:------------:|\n| 1  | 2014-05-01   |\n| 2  | 2003-01-05   |\n| 3  | 2020-01-10   |\n| 4  | NULL         |\n\nNow I want to Compare the First Table with the second table with the ID column to look if contact 1 is already in the list where people were contacted. After this, I want to write the Contact Date into the first table to see the last contact date in the main table.\nHow would I do this?\nThanks for any answers!!\n'
"I'm new to bokeh, but trying desperately to apply a Hover tooptip to a Box-Whisker plot.\nI am trying to display the Q1,Q2,Q3 and IQR values when hovering over the Box glyph but have been unsuccessful in my attempts, and confused by the process of creating the components of a Box-Whisker plot\nI am using the example provided from the bokeh documentations.\nimport numpy as np\nimport pandas as pd\nfrom bokeh.models import ColumnDataSource, Grid, LinearAxis, Plot, Quad,Range1d,HoverTool, Panel, Tabs,Legend, LegendItem\nfrom bokeh.plotting import figure, output_file, show\n\n\n# generate some synthetic time series for six different categories\ncats = list(&quot;abcdef&quot;)\nyy = np.random.randn(2000)\ng = np.random.choice(cats, 2000)\nfor i, l in enumerate(cats):\n    yy[g == l] += i // 2\ndf = pd.DataFrame(dict(score=yy, group=g))\n\n# find the quartiles and IQR for each category\ngroups = df.groupby('group')\nq1 = groups.quantile(q=0.25)\nq2 = groups.quantile(q=0.5)\nq3 = groups.quantile(q=0.75)\niqr = q3 - q1\nupper = q3 + 1.5*iqr\nlower = q1 - 1.5*iqr\n\n# find the outliers for each category\ndef outliers(group):\n    cat = group.name\n    return group[(group.score &gt; upper.loc[cat]['score']) | (group.score &lt; lower.loc[cat]['score'])]['score']\nout = groups.apply(outliers).dropna()\n\n# prepare outlier data for plotting, we need coordinates for every outlier.\nif not out.empty:\n    outx = []\n    outy = []\n    for keys in out.index:\n        outx.append(keys[0])\n        outy.append(out.loc[keys[0]].loc[keys[1]])\n\np = figure(tools=&quot;&quot;, background_fill_color=&quot;#efefef&quot;, x_range=cats, toolbar_location=None)\n\n# if no outliers, shrink lengths of stems to be no longer than the minimums or maximums\nqmin = groups.quantile(q=0.00)\nqmax = groups.quantile(q=1.00)\nupper.score = [min([x,y]) for (x,y) in zip(list(qmax.loc[:,'score']),upper.score)]\nlower.score = [max([x,y]) for (x,y) in zip(list(qmin.loc[:,'score']),lower.score)]\n\n# stems\np.segment(cats, upper.score, cats, q3.score, line_color=&quot;black&quot;)\np.segment(cats, lower.score, cats, q1.score, line_color=&quot;black&quot;)\n\n# boxes\np.vbar(cats, 0.7, q2.score, q3.score, fill_color=&quot;#E08E79&quot;, line_color=&quot;black&quot;)\np.vbar(cats, 0.7, q1.score, q2.score, fill_color=&quot;#3B8686&quot;, line_color=&quot;black&quot;)\n\n# whiskers (almost-0 height rects simpler than segments)\np.rect(cats, lower.score, 0.2, 0.01, line_color=&quot;black&quot;)\np.rect(cats, upper.score, 0.2, 0.01, line_color=&quot;black&quot;)\n\n# outliers\nif not out.empty:\n    p.circle(outx, outy, size=6, color=&quot;#F38630&quot;, fill_alpha=0.6)\n\np.xgrid.grid_line_color = None\np.ygrid.grid_line_color = &quot;white&quot;\np.grid.grid_line_width = 2\np.xaxis.major_label_text_font_size=&quot;16px&quot;\n\n# Format the tooltip\ntooltips = [\n    ('q1', '@q2'),\n    ('q2', '@q1'),\n    ('q3', '@q3'),\n    ('iqr', '@iqr'),\n]\n\n# Add the HoverTool to the figure\np.add_tools(HoverTool(tooltips=tooltips))\n\noutput_file(&quot;boxplot.html&quot;, title=&quot;boxplot.py example&quot;)\n\nshow(p)\n\n\n"
'As you can see in the picture below, i cannot pass lat lon value thought the series to get time series data (date, value). How can i get it using pandas in python?\nenter image description here\n'
"I have a dataset which has a column that contains two different texts (PAIDOFF, COLLECTION) and I want to convert it to binary values so I tried the following:\ny = df['loan_status'].values\ny[0:5]\n\nOutput:\narray(['PAIDOFF', 'PAIDOFF', 'PAIDOFF', 'PAIDOFF', 'PAIDOFF'],\n  dtype=object)\n\nAfter defining the target column, tried to convert it to binary values:\n#Convert y to binary values\nle_loan_status=preprocessing.LabelEncoder()\nle_loan_status.fit(['PAIDOFF','COLLECTION'])\ny[:,0]= le_loan_status.transform(y[:,0])\n\nOutput:\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-10-917e44b54b88&gt; in &lt;module&gt;\n      2 le_loan_status=preprocessing.LabelEncoder()\n      3 le_loan_status.fit(['PAIDOFF','COLLECTION'])\n----&gt; 4 y[:,0]= le_loan_status.transform(y[:,0])\n\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n\nDo you have any idea to solve this?\n"
"Excuse my bad english.\nOn a DataFrame like the following one :\n-----------------\n|index|var1|var2|\n-----------------\n\nthere is lot of rows\nvar1 is between 0 and 4000\nvar2 is between -100 and 100\nI'm looking to create an histogram that show how many rows there is according to var1.\n\nOn the Y axis, we can see how many rows there is, for example for 0 &gt; var1 &lt; 500, there is almost 500k rows.\nNow I want to add var2, which show the quality of a row.\nI want that for example the histgram become blue from 0 to 500 and another color from 500 to 1000 according to the value of var2 (like if the bar as values where the mean of var2 is 100, make it green, if the mean is 0, make it red).\nI tried to hardcore this, but as soon as I change the bins or anything, my code break.\nI also tried to do it using plot on the top of the hist, but it doesn't work.\nMy current code for the screenshot :\nplt.hist(var1, bins=10, range=(0,4000), color='orange', alpha=0.7)\nplt.title('Var 1',weight='bold', fontsize=18)\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n\nI feel like this is simple things to do, but I'm completely stuck in my learning because of this.\nMany thanks for your help.\n"
"Following the open-ended principle i want to create my LinearRegression by inheritance for sklearn one.\nI try to create MyRegression with the same functionality as LinearRegression but add one function\nCould you help, why it's not working?\n    from sklearn.linear_model import LinearRegression\n\nclass MyRegression(LinearRegression):\n    def __init__(self):\n        super(LinearRegression, self).__init__()\n        \n    def modified_fit(self, x, y):\n        return self.fit(x, y)\n\n\nx = [\n    (1,2),\n    (2,3)\n]\ny = [1,2]\n\nregression = MyRegression()\nregression.modified_fit(x, y)\n\nI've got an error, but as I understand all parametrs and methods of original LinearRegression had to be ingereted from parent during init() process\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-29-7d9a5c74a33f&gt; in &lt;module&gt;\n     16 \n     17 regression = MyRegression()\n---&gt; 18 regression.modified_fit(x, y)\n\n&lt;ipython-input-29-7d9a5c74a33f&gt; in modified_fit(self, x, y)\n      6 \n      7     def modified_fit(self, x, y):\n----&gt; 8         return self.fit(x, y)\n      9 \n     10 \n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py in fit(self, X, y, sample_weight)\n    478         &quot;&quot;&quot;\n    479 \n--&gt; 480         n_jobs_ = self.n_jobs\n    481         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n    482                          y_numeric=True, multi_output=True)\n\nAttributeError: 'MyRegression' object has no attribute 'n_jobs'\n\n"
"I'm looking for a way to identify the local extremes in a pandas timeseries.\nA MWE would be\nimport math\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsin_list = []\nfor i in range(200):\n    sin_list.append(math.sin(i / 10) + i / 100)\n\nidx = pd.date_range('2018-01-01', periods=200, freq='H')\n\nts = pd.Series(sin_list, index=idx)\n\nts.plot(style='.')\nplt.show()\n\nand the red lines would mark the timestamps which I want to identify. Note that there are, of course, finite steps in this series.\nA possible solution could be to fit a curve to it, derive it and then identify the exact place where the gradient is 0. This does seem like a big effort to program myself, and I assume such an implementation exists somewhere.\n\n"
"I have 5 sheets in an excel workbook. I would like to export each sheet to csv using python libraries.\nThis is a sheet showing sales in 2019. I have named the seets according to the year they represent as shown here.\nI have read the excel spreadsheet using pandas. I have used the for loop since I am interested in saving the csv file like the_sheet_name.csv. This is my code in a jupyter notebook:\nimport pandas as pd\n\ndf = pd.DataFrame() \nmyfile = 'sampledata.xlsx’ \nxl = pd.ExcelFile(myfile)\n\nfor sheet in xl.sheet_names: \n     df_tmp = xl.parse(sheet) \n     print(df_tmp)\n     df = df.append(df_tmp, ignore_index=True,sort=False) \n     csvfile = f'{sheet_name}.csv' \n     df.to_csv(csvfile, index=False)\n\nExecuting the code is producing just one csv file that has the data for all the other sheets. I would like to know if there is a way to customize my code so that I can produce individual sheets e.g sales2011.csv, sales2012.csv and so on.\n"
"I have created a CSV file from the recording of different sensors using pandas DataFrame. The CSV file basically looks like this:\n\nI would like to get rid of the redundant timestamps and instead have all sensor entries that share a timestamp appear in the same row (for example x2 and x3 in the image).\nAlso, the labels that share a timestamp are always identical, but would need to be reduced as well.\nSo far, I've come across the drop_duplicate() function which only drops entire rows.\nEdit: here's a text version of the example above:\ntimestamp,sensor_a,sensor_b,sensor_c,label\n1,x1,,,0\n2,,x2,,0\n2,,,x3,0\n3,x4,,,1\n4,,,,1\n5,,x6,,1\n5,,,x7,1\n\n"
'Quick Quesiton here.\nI have to calculate the mean of a 60 days rolling window R2 time serie. My time serie has 4680 observations.\nI was wondering, is the mean of the serie of R2 the total R2 of the regression?\nExample :\nMean(series of r2) = R2 of the regression on all the sample data\n'
'Currently I am using the method specified here to generate the following plot: \n \n\nMy code is :\n\ngrouped_data = df.groupby(["ssc_b", "hsc_b"]).status.value_counts().unstack(2);\ngrouped_data.plot.bar(title = "Placements by Board of Education", rot = 45).set_xlabel("(SSC, HSC)");\n\n\nBut I would like to change the values within group to represent the proportions within ech group rather than the counts themselves, since the number of samples per group varies quite a bit. I feel that this would make it easier to compare the groups. Is there a simple way to do this? \n'
'Here is the code :-\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport matplotlib.pyplot as plt\n\npoly_reg = PolynomialFeatures(degree = 4)      \nx_poly = poly_reg.fit_transform(x)\n\nlin_reg2 = LinearRegression()\nlin_reg2.fit(x_poly, y)\n\nplt.title("Polynomial Regression")\nplt.xlabel("Position Level")\nplt.ylabel("Salary")\nplt.scatter(x, y, color =\'red\')\nplt.plot(x, lin_reg2.predict(poly_reg.fit_transform(x)), color = \'blue\')\nplt.show()\n\n\nwhy we are using this following code :-\n\nplt.plot(x, lin_reg2.predict(poly_reg.fit_transform(x)), color = \'blue\')\n\n\nInstead of using :-  \n\nplt.plot(x_poly, lin_reg2.predic(x_poly), color = "blue")\n\n'
'I have this dataframe:\n\nColumns:\n\nCountries\nIsVertical\n\n\nExample:\n\nUSA FALSE\nUSA FALSE\nPoland FALSE\nItaly TRUE\nItaly TRUE\n\n\nI want to create a bar chart that has 2 columns for each country (1 for True and one for false - From IsVertical) and I need the y axis to be the normalized count of times that the country appears.\n\nI tryed something like this:\n\ndata.groupby("Country").count().plot(kind="bar")\n\n\nThe problem is that it doesn\'t give me 2 bars of IsVertical foreach country and that it only counts the number of times that IsVertical appears and doesn\'t divide by the total count of IsVertical\n\nSOLUTION:\n\nresult = data.groupby(\'Country\').apply(\n         lambda group: (group.IsVertical.sum() / float(data.IsVertical.sum()))\n     ).to_frame(\'Vertical\')\n\n    result.plot(kind=\'bar\')\n\n'
'How can I multiply two columns of within a same dataframe? My dataframe looks like below image and I want to output like this. However, I cannot find how to multiply two columns that are dependent on first row of same dataframe. I would really appreciate some help on this.\n\nrequest                            totalbytes\n/login                              8520\n/shuttle/countdown/                 7970\n/shuttle/countdown/liftoff.html     0\n\n\n\n\nSo far my output is below, but how can I get unique rows.\n\n'
'I\'m trying to remove escape codes that are scattered throughout this one cell in my pandas column. I need to execute code to the whole column but that one cell is hindering that for all. \n\nThe code to call specific cell looks like so;\n df.topics[0]\n\nOutput \n\n\n  \'[{\\\'urlkey\\\': \\\'witi\\\', \\\'name\\\': \\\'Women in Technology\\\', \\\'id\\\': 10296}, {\\\'urlkey\\\': \\\'cross-mentoring-with-expert-ceo-business-owners\\\', \\\'name\\\': \\\'Cross Mentoring with expert CEO business owners\\\', \\\'id\\\': 15145}, {\\\'urlkey\\\': \\\'entrepreneurship\\\', \\\'name\\\': \\\'Entrepreneurship\\\', \\\'id\\\': 19882}, {\\\'urlkey\\\': \\\'womens-business-networking\\\', \\\'name\\\': "Women\\\'s Business Networking", \\\'id\\\': 21283}, {\\\'urlkey\\\': \\\'startup-businesses\\\', \\\'name\\\': \\\'Startup Businesses\\\', \\\'id\\\': 21681}, {\\\'urlkey\\\': \\\'lean-startup\\\', \\\'name\\\': \\\'Lean Startup\\\', \\\'id\\\': 38660}, {\\\'urlkey\\\': \\\'female-entrepreneurs\\\', \\\'name\\\': \\\'Female Entrepreneurs\\\', \\\'id\\\': 41905}, {\\\'urlkey\\\': \\\'founders\\\', \\\'name\\\': \\\'Founders\\\', \\\'id\\\': 46616}, {\\\'urlkey\\\': \\\'technology-startups\\\', \\\'name\\\': \\\'Technology Startups\\\', \\\'id\\\': 108403}, {\\\'urlkey\\\': \\\'ceo-2-ceo-coaching-mentoring-for-mutual-growth\\\', \\\'name\\\': \\\'CEO 2 CEO Coaching &amp; Mentoring For Mutual Growth\\\', \\\'id\\\': 133122}, {\\\'urlkey\\\': \\\'cto\\\', \\\'name\\\': \\\'CTO\\\', \\\'id\\\': 141917}, {\\\'urlkey\\\': \\\'ceo\\\', \\\'name\\\': \\\'CEO\\\', \\\'id\\\': 141921}, {\\\'urlkey\\\': \\\'c-level-tech\\\', \\\'name\\\': \\\'C-Level Tech\\\', \\\'id\\\': 816562}, {\\\'urlkey\\\': \\\'ceos-founders\\\', \\\'name\\\': \\\'CEOs &amp; Founders\\\', \\\'id\\\': 1379732}, {\\\'urlkey\\\': \\\'cio-cto\\\', \\\'name\\\': \\\'CIO / CTO\\\', \\\'id\\\': 1485582}]\'`\n\n\nWhereas the other cells look like so;\n\n\n  [{\'urlkey\': \'opensource\', \'name\': \'Open Source\', \'id\': 563}, {\'urlkey\': \'ebizowners\', \'name\': \'E-Business Owners\', \'id\': 1330}, {\'urlkey\': \'softwaredev\', \'name\': \'Software Development\', \'id\': 3833}, {\'urlkey\': \'socialnetwork\', \'name\': \'Social Networking\', \'id\': 4422}, {\'urlkey\': \'web\', \'name\': \'Web Technology\', \'id\': 10209}, {\'urlkey\': \'technology\', \'name\': \'Technology\', \'id\': 10579}, {\'urlkey\': \'online-marketing\', \'name\': \'Online Marketing\', \'id\': 15585}, {\'urlkey\': \'digital-media\', \'name\': \'Digital Media\', \'id\': 17188}]\n\n\nCould you help me with code to remove the \\ (newline break I think) so all cells are similar. \n\nNot sure if it is only for that first cell it may be in others but the rest look normal. However, code to remove for the whole column just in case would be really helpful.\n\nThanks. \n'
"The data comes in two data sets which I need to check for if a single time event in the first data set at a specific location coincides with a time range in the second dataset at the same specific location, and append the ID of the second set to the first accordingly if the conditions are met. I have a list of specific locations that I want to check.\n\nMy problem is that the first data set contains about 500,000 rows, and the second contains about 90,000 rows. Running through both data sets takes forever and my computing power is limited.\n\nHere is the Python code:\n\nimport datetime\nimport pandas as pd\n\ndef assign_tRangeID(singleEventDF, timeRangeDF):\n    margin = datetime.timedelta(minutes=15)\n    for i, single in singleEventDF.iterrows():\n        for j, timeRange in timeRangeDF.iterrows():\n           if timeRange['start_time']-margin &lt;= single['singleEvent_time'] &lt;= timeRange['end_time']\n               singleEventDF.at[i, 'tRange_ID'] = timeRangeDF['ID']\n\nfor i, location in location_list.iterrows():\n    single_subset = singleEvent['loc'].loc[[singleEvent['loc'] = location['loc']]\n    tRange_subset = timeRange['loc'].loc[[timeRange['loc'] = location['loc']]\n    assign_eventID(single_subset, tRange_subset)\n\n\nI am a beginner in Python, so I'm just wondering if I can do this in a more efficient manner without having to use a database or some big data solution. Thanks for all the help!\n"
"Here is my dataset dataset_for_this_Question\nI want to group dataset according to 'Time' and 'Type',\nSo that I can get frequency of 'Name' for each hourly basis. [Per Hour How many Types and what are their Names].\nMy first requirement is to group dataset according to 'Time' - Hourly basis.\nI am using Pandas in Python.\n"
'So I am new to pandas and am punching above my weight here. I have two csv files: one is a list of authors I am interested in (data frame 1) and the second file is a total list of authors for the publishing company and their publication date (data frame 2).\n\nI need to use data frame 1 to see if there is an exact name match in data frame 2. If there is a match (there can be more than 1 match) I want to pull the minimum date. Ex:) For Jake Smith in df 1 there may be 2 matches in df 2 and i want to add the oldest publication date to data frame 1.\n\ndf \n\nfirst name|last name |\n\ndf 2\n\nfirst name|last name| publication date\n\ndesired\n\nif author is in df1 then add the lowest publication date to df1\n\nSo heres what I did. I created the data frames from the csv files and concatenated all the author files to create df2. I then did an inner join on first and last name because I thought that would be the best way to name match. I keep getting an error. And then I used a group by to try and get the minimum date.  \n\nimport pandas as pd\n\nfiles_path= \'C:\'\ndf_1 = pd.read_csv( files_path + \'/author_desired.csv\', sep="|")\n\ndf_merged= pd.read_csv(files_path +\'/master_list.csv\', sep="|")\n\ndf_final= pd.join(df_1, df_merged, on= [\'LAST_NAME\' , \'FIRST_NAME\'], how=\'inner\')\n\ndf_final.groupby([\'FIRST_NAME\', \'LAST_NAME\']).max()[\'FIRST_PUB_DATE\']\n\ndf_final.to_csv(files_path + "/merged_file.csv")\n\n\nPLEASE HELP\n'
'This is a python problem. I am a novice to python and visualization and tried to do some research before this. But I wasn\'t able to get the right answer.\n\nI have a csv file with first column as names of countries and remaining with some numerical data. I am trying to plot a horizontal histogram with the countries on y axis and the respective first column data on x axis. However, with this code I am getting "nan" instead of country names. How can I make sure that the yticks are correctly showing country names and not nan?\n\nClick here for image of the plot diagram\n\nMy code is as such: (displaying only first 5 rows)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nmy_data = np.genfromtxt(\'c:\\drinks.csv\', delimiter=\',\')\ncountries = my_data[0:5,0]\ny_pos = np.arange(len(countries)`enter code here`)\nplt.figure()\nplt.barh(y_pos, my_data[0:5:,1])\nplt.yticks(y_pos, countries)\nplt.show()\n\n\nHere is the link to the csv file\n'
"I just wrote this function to calculated the age's person based in two columns in a Python DataFrame. Unfortunately, if a use the return the function return the same value for all rows, but if I use the print statement the function gives me the right values.\n\nHere is the code:\n\ndef calc_age(dataset):\n    index = dataset.index\n    for element in index:\n        year_nasc = train['DT_NASCIMENTO_BENEFICIARIO'][element][6:]\n        year_insc = train['ANO_CONCESSAO_BOLSA'][element]\n        age = int(year_insc) - int(year_nasc)\n        print ('Age: ', age)\n        #return age\n\n\ntrain['DT_NASCIMENTO_BENEFICIARIO'] = 03-02-1987 \n\ntrain['ANO_CONCESSAO_BOLSA'] = 2009\n\nWhat am I doing wrong?!\n"
"Consider the following DataFrame df:\n\nDate                   Kind\n2018-09-01 13:15:32    Red\n2018-09-02 16:13:26    Blue\n2018-09-04 22:10:09    Blue\n2018-09-04 09:55:30    Red\n...                    ...\n\n\nIn which you have a column with a datetime64[ns] dtype and another which contains a np.object which can assume only a finite number of values (in this case, 2).\n\nYou have to plot a date histogram in which you have:\n\n\nOn the x-axis, the dates (per-day histogram showing month and day);\nOn the y-axis, the number of items belonging to that date, showing in a stacked bar the difference between Blue and Red.\n\n\nHow is it possible to achieve this using Matplotlib?\n\nI was thinking to do a set_index and resample as follows:\n\ndf.set_index('Date', inplace=True)\ndf.resample('1d').count()\n\n\nBut I'm losing the information on the number of items per Kind. I also want to keep any missing day as zero.\n\nAny help very appreciated.\n"
'I am trying to create a graphic where I overlay multiple contour plots on a single image. So I want to have colorbars for each of the plots, as well as a legend indicating what each contour represents. However Matplotlib will not allow me to create a separate legend for my contour plots. Simple example:\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.crs as ccrs\nimport numpy as np\n\n\n\ndef create_contour(i,j):\n    colors = ["red","green","blue"]\n    hatches = [\'-\',\'+\',\'x\',\'//\',\'*\']\n    fig = plt.figure()\n    ax = plt.axes(projection=ccrs.PlateCarree())\n    ax.set_extent((-15.0,15.0,-15.0,15.0))\n    delta = 0.25\n    x = np.arange(-3.0,3.0,delta)\n    y = np.arange(-2.0,2.0,delta)\n    X, Y = np.meshgrid(x, y)\n    data = np.full(np.shape(X), 1.0)\n    plot = ax.contourf(X,Y,data, levels = [float(i),float(i+1)], hatch=[hatches[j]], colors = colors[i], label="label")\n    plt.legend(handles=[plot], labels=["label"])\n    plt.savefig("figure_"+str(i)+".png")\n\ncreate_contour(1,3)\n\n\nWhen I run this, I get the following message:\n\n\n  UserWarning: Legend does not support\n  (matplotlib.contour.QuadContourSet object at 0x7fa69df7cac8)\n  instances. A proxy artist may be used instead. See:\n  http://matplotlib.org/users/legend_guide.html#creating-artists-specifically-for-adding-to-the-legend-aka-proxy-artists\n  "aka-proxy-artists".format(orig_handle)\n\n\nBut as far as I can tell, I am following those directions as closely as possible, the only difference being that they do not use contourf in the example.\n\nAny help would be greatly appreciated.\n'
"I'm working on a script that converts ids of school names to actual school names structured in a numpy array.\n\nFor example\n\n[[1,2,3],[3,6,7]]\n\n\nbecomes\n\n[[school-a,school-b,school-c],[school-c,school-f,school-g]\n\n\nThe school and ids sit together in a python dictionary.\n\nI've tried doing this:\n\nfor x in np.nditer(finalarray, op_flags=['readwrite']):\n    x[...] = school_ids.get(int(x))\n    print(school_ids.get(int(x)))\nprint(finalarray)\n\n\nbut that gave the error:\n\nValueError: invalid literal for int() with base 10: 'school-a'\n\n\nit's important that the structure of the numpy array stays the same, because I also thought of just iterating every item, but then the structure is lost.\n"
"I am trying to calculate the weighted average of amount of times a social media post was made on a given weekday between 2009- 2018.\n\nThis is the code I have:\n\nweight = fb_posts2[fb_posts2['title']=='status'].groupby('year',as_index=False).apply(lambda x: (x.count())/x.sum())\n\n\nWhat i am trying to do is to groupby year and weekday, count the number of time each weekday has occurred in a year and divide that by the total number of posts in each year. The idea is to return a dataframe with a  weighted average of how many times each weekday occurred between 2009 and 2018.\n\nThis is a sample of the dataframe I am interacting with:\n\n"
'How to remove # from words in a string if it is the first character in a word. It should remain if it is present by itself, in the middle of a word, or at the end of a word.\n\nCurrently I am using the regex expression:\n\ntest = "# #DataScience"\ntest = re.sub(r\'\\b#\\w\\w*\\b\', \'\', test) \n\n\nfor removing the # from the words starting with # but it does not work at all. It returns the string as it is\n\nCan anyone please tell me why the # is not being recognized and removed? \n\nExamples -\n\ntest - "# #DataScience"\nExpected Output - "# DataScience"\n\nTest - "kjndjk#jnjkd"\nExpected Output - "kjndjk#jnjkd"\n\nTest - "# #DataScience #KJSBDKJ kjndjk#jnjkd #jkzcjkh# iusadhuish#""\nExpected Output -"# DataScience KJSBDKJ kjndjk#jnjkd jkzcjkh# iusadhuish#"\n\n'
'Output plot is not fitting properly into the frame while plotting hist + density plot for eda analysis in r \n\n  hist(Absenteeism_Data$Absenteeism.time.in.hours, col = "peachpuff", border = "black", prob = TRUE, xlab = "Absenteeism.time.in.hour", main = "Absenteeism_Data")\n    lines(density(Absenteeism_Data$Absenteeism.time.in.hours, na.rm = TRUE), lwd = 2, col = "chocolate3")\n\n\n\n'
'I am working on a survey data analysing project which consist 2 Excel files- in file pre-survey, it contains 800+ response records; while in post-survey file it contains 500ish responses. Both of them have (at least) one common column SID (Student ID). Something Y happened in between, and I am interested analysing the effectiveness of Y, and in what degrade Y impacts on different categories of people.\n\nWhat adds more complexity is that in each Excel file, it contains multiple tabs. Different interviewers interviewed several interviewees and documented in each tabs for different sections of survey. Columns may or may not be the same for different tabs, so it would be hard to be complied in one file. (Or does it actually make sense to combine them in one with lots of null values?)\n\nI am trying to find the students who did both pre- and post- surveys. How can I do it across sheets and files using python/pandas/other packages?\n\nBonus if you could also suggest the approach to solve the problem.\n'
"I have a dataframe consisting of 3 columns: Longitude, Latitude and area.\n\nCELL NAME   Longitude   Latitude    area\n\nLE1072_012  -0.072457   61.042381   6.170170e-01\nLE1437_011  1.711201    60.936088   5.960055e-01\nLE2614_012  0.071279    58.835267   4.412428e-01\nLE2826_013  1.558309    60.730385   2.844340e-01\nLE2346_011  -1.056118   59.646612   2.528572e-01\nLE2676_012  -0.198150   58.546112   2.395335e-01\nLE2526_012  0.594452    59.184265   2.392216e-01\n....\n\n\nI need a fourth column that depends on the value of area. If area is greater than a threshold it should say 'rural' and if it is smaller 'urban'\n\nI wanted to implement it as a function with an if\n\ndef CellType(area):\n        if area &gt; threshold\n           a='rural'\n        else\n           a='urban'\n        return a\n\n\nand then call it this way\n\ndf['CellType']=CellType(df['area'])\n\n\nThis way I get an error that says that the conditional of a series is ambiguous\n\nThen I tried this piece of code\n\nfor i in range(len(df)):\n    if df['area'][i]&lt;0.002:\n        df['CellType'][i]='urban'\n    else:\n        df['CellType'][i]='rural'\n\n\nThis works, but it takes forever when I run this on 15k cells\n\nIs there a way to make this task faster?\nThanks\n"
'when running this code I have an error \n\n\n  IndexError: only integers, slices (:), ellipsis (...),\n  numpy.newaxis (None) and integer or boolean arrays are valid indices\n\n\npython code\n\nimport random\n\ndef getsys():\n    row = \'\'\n    for i in range(0 , 8):\n        randintt = str(random.randint(0 , 4))\n        row += randintt\n    return row\n\n\ndef getx():\n    x = []\n    for i in range(0,14):\n        mysys = getsys()\n        x.append(mysys)\n\n    return x \n\ny = getx()\nprint (y)\n\n\n\n\nimport initialsys\nimport numpy as np\n\nR = np.array([[0.90 , 0.93,0.91 , 0.95],\n               [0.95 , 0.94, 0.93, 0],\n               [0.85 , 0.90 , 0.87 , 0.92],\n               [0.83 , 0.87 , 0.85 , 0 ],\n               [0.94 , 0.93 , 0.95 , 0],\n               [0.99 , 0.98 , 0.97 , 0.96],\n               [0.91 , 0.92 , 0.94 , 0],\n               [0.81 , 0.90 , 0.91 , 0],\n               [0.97 , 0.99 , 0.96 , 0.91],\n               [0.83 , 0.85 , 0.90 , 0],\n               [0.94 , 0.95 , 0.96 , 0],\n               [0.79 , 0.82 , 0.85 , 0.90],\n               [0.98 , 0.99 , 0.97 , 0],\n               [0.85 , 0.92 , 0.95 , 0.99]\n              ])\n\ndef expression(r ,possition , char ):\n    exp = 1-r[possition , char]\n\n\nx = initialsys.getx()\npossition = 1\nTotal = 1\nchar = ""\nfor row in x :\n    for char in row :\n        if char!= 0 :\n            exp = expression(R , possition , char)\n            Total = Total*exp\n    Total = 1-Total\n    possition = possition + 1\n\n'
"I have a df such as below:\n\nI am using simple code such as below: that filters columns in the df and then I calculate simple math based on value of the column,\nso if the column values is cancelled, processing, and complete; I want to calculate the % or number of rows that were cancelled of the entire df or all the rows.\n\ndf looks like:\n\n   ID |    Status    |   Color\n   555    Cancelled     Green\n   434    Processed     Red   \n   212    Cancelled     Blue\n   121    Cancelled     Green\n   242    Cancelled     Blue\n   352    Processed     Green\n   343    Processed     Blue\n\n\nThe Code Im currently using is:\n\ndf[df['Color'] == 'Green']\n\ndf[(df['Status']=='Cancelled') &amp; (df['Color']=='Green')]\n\n\nMeaning for each different type of color I manually first filter the df to get the # of rows, then double filter it below to get the number of rows or orders cancelled then manually divide that # but he # of just green rows.\n\nIf I wanted to create a function where I can insert the color name and the status and do the math that way in a simple function what would be the best approach for that?\n\nExpected Output would be something like:\n\n Status      Green\nCancelled    0.666667\nProcessed    0.333333\ndtype: float64\n\n\nThanks so much!\n"
"Please tell me type of train_images? Its nparray?\n\n    data_dir = self.data_dir\n    fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_images = loaded[16:].reshape((60000, 28, 28,1)).astype(np.float)\n\n\nOther my question is: How I can convert jpg images of one folder to format of train_images?? I want to make train and test dataset.\nThank you\n"
"In my data frame, I have column 'countries', I am trying to change that column values into 'developed countries' and 'developing countries'. My data frame is as following:\n\n   countries age gender\n1  India     21  Male\n2  China     22  Female\n3  USA       23  Male\n4  UK        25  Male\n\n\nI have following two arrays:\n\ndeveloped = ['USA','UK']\ndeveloping = ['India', 'China']\n\n\nI want to convert array into following data frame:\n\n   countries    age gender\n1  developing   21  Male\n2  developing   22  Female\n3  developed    23  Male\n4  developed    25  Male\n\n\nI tried following code, but I got 'SettingWithCopyWarning' error:\n\ndf[df['countries'].isin(developed)]['countries'] = 'developed'\n\n\nI tried following code, but I got 'SettingWithCopyWarning' error and my jupyter notebook got hanged:\n\nfor i, x in enumerate(df['countries']):\n    if x in developed:\n        df['countries'][i] = 'developed'\n\n\nIs their alternative way to change column categories??\n"
"I'm very new to pandas data frame that has a date time column, and a column that contains a string of text (headlines). Each headline will be a new row.\n\nI need to plot the date on the x-axis, and the y-axis needs to contain how many times a headline occurs on each date.\n\nSo for example, one date may contain 3 headlines.\n\nWhat's the simplest way to do this? I can't figure out how to do it at all. Maybe add another column with a '1' for each row? If so, how would you do this?\n\nPlease point me in the direction of anything that may help!\n\nThanks you!\n\nI have tried plotting the count on the y, but keep getting errors, I tried creating a variable that counts the number of rows, but that didn't return anything of use either.\n\nI tried add a column with the count of headlines\n\ndf_data['headline_count'] = df_data['headlines'].count\n\n\nand I tried the group by method\n\ndf_data['count'] = df.groupby('headlines')['headlines'].transform('count')\n\n\nWhen I use groupie, i get an error of \n\nKeyError: 'headlines'\n\n\nThe output should simply be a plot with how many times a date is repeated in the dataframe (which signals that there are multiple headlines) in the rows plotted on the y-axis. And the x-axis should be the date that the observations occurred.\n"
'Im really new to Python and Datascience.\n\nI have a Dataset with 100K+ rows, and like 30 columns(Two datetime, 27 integers and 1 string). \nI wanna create a 31th column by computing difference between my two datetimes columns and get the result in seconds. Also, i wanna make "-5000" as the minimum diffrence between my two dates.\n\nSo to make it easy, lets just focus only on the two Datetime columns.\n\nThe goal is to go from this:\n\n        first_datetime        second_datetime\n0  2019-03-13 04:35:30  2019-03-13 05:35:30\n1  2019-03-13 05:35:30  2019-03-13 06:35:30\n2  2019-03-13 05:35:30  2019-03-14 06:35:30\n\n\nTo This :\n\n   diff       first_datetime        second_datetime\n0 -3600    2019-03-13 04:35:30  2019-03-13 05:35:30\n1   -10    2019-03-13 05:35:30  2019-03-13 05:35:40\n2 -5000    2019-03-13 05:35:30  2019-03-14 05:35:40\n\n\nI\'ve been told that the .apply function is the fastest, so i used it to make this function:\n\ndef calc_diff(row):\n    diff=int((row[\'first_datetime\']-row[\'second_datetime\']).total_seconds())\n    if diff&lt;-5000:\n         return -5000\n    else:\n         return diff\n\n\nAnd im using it with the .apply function :\n\ndf = pd.DataFrame([{\'first_datetime\': "2019-03-13 04:35:30", \'second_datetime\': "2019-03-13 05:35:30"},{\'first_datetime\': "2019-03-13 05:35:30", \'second_datetime\': "2019-03-13 05:35:40"}])\n\ndf[\'diff\']=df.apply(calc_diff, axis=1) \n\n\nThe problem is that im getting a Memory Error, im i doing something wrong ? What\'s the best way to achieve this?\n'
'Im kinda new to Python and Datascience.\n\nI have a Dataset with 2 datetime columns A and B :\n\ndf=pd.DataFrame({\'A\': ["2019-03-13 08:12:20", "2019-03-15 10:02:18"], \'B\': ["2019-03-13 08:12:25", "2019-03-13 10:02:20"], \'C\': [\'C1\', \'C2\']})\n                     A                    B C\n0  2019-03-13 08:12:20  2019-03-13 08:12:25 C1\n1  2019-03-15 10:02:18  2019-03-13 10:02:20 C2\n\n\nAnd a second dataset with like 500K+ rows with and i want to add C column to this dataset\n\nTo make it easier, let\'s say my second dataset have only one column D:\n\ndf2=pd.DataFrame({\'D\': ["2019-03-13 08:12:20", "2019-03-13 08:12:23", "2019-03-13 08:12:24", "2019-03-13 08:12:25", "2019-03-15 10:02:18", "2019-03-15 10:02:19", "2019-03-16 10:02:20"]})\n                     D \n0  2019-03-13 08:12:20\n1  2019-03-13 08:12:23\n2  2019-03-13 08:12:24\n3  2019-03-13 08:12:25\n4  2019-03-15 10:02:18\n5  2019-03-15 10:02:19\n6  2019-03-16 10:02:20\n\n\nFor each row in D, i want to check the date and if its between A and B of the first dataset, i want to give the C value given in the first dataset.\nAnd i want it to set C to NaN it its not between any A and B.   \n\n                     D  C\n0  2019-03-13 08:12:20  C1\n1  2019-03-13 08:12:23  C1\n2  2019-03-13 08:12:24  C1\n3  2019-03-13 08:12:25  C1\n4  2019-03-15 10:02:18  C2\n5  2019-03-15 10:02:19  C2\n6  2019-03-16 10:02:20  NaN\n\n\nI don\'t really how i can achieve this without using a regular loop and iterating over my second dataset..Can you please guys show me what\'s the best way to achieve this?\n'
"This seems like such an easy problem but I'm stumped. I am trying to return a new dataframe (df3) containing the difference between df1 and df2. I don't care about differences in values, just column labels/indices. I've tried pd.columns.difference() but to no avail. My expected output is:\n\ndf1 \n1  2  3  4  5  6  7  8  9  10 11 12 13 14\nC  C  C  C  C  C  T  T  T  T  T  S  S  S\nG  N  D  A  G  N  D  A  G  N  D  A  G  N\nK  M  I  L  R  K  M  I  L  R  K  M  I  L\nC  C  L  C  C  C  T  T  T  T  T  S  S  S\nG  N  D  A  E  N  D  A  G  N  D  A  G  N\nY  F  V  H  Q  E  P  W  Y  F  V  H  Q  E\nG  N  D  A  G  N  F  A  G  G  D  A  G  N\nG  Y  D  A  G  N  D  A  G  N  D  A  G  N\n\ndf2 \n1  4  9\nC  C  T\nG  A  G\nK  L  L\nC  C  T\nG  A  G\nY  H  Y\nG  A  G\nG  A  G\n\nExpected df3: \n2  3  5  6  7  8  10 11 12 13 14\nC  C  C  C  T  T  T  T  S  S  S\nN  D  G  N  D  A  N  D  A  G  N\nM  I  R  K  M  I  R  K  M  I  L\nC  L  C  C  T  T  T  T  S  S  S\nN  D  E  N  D  A  N  D  A  G  N\nF  V  Q  E  P  W  F  V  H  Q  E\nN  D  G  N  F  A  G  D  A  G  N\nY  D  G  N  D  A  N  D  A  G  N\n\n\n\n\n"
'when I\'m running this code I have an error and I can\'t understand whats the problem.\nthis code work like when the (getx )create the array of systems, Cost, weight and expression functions start calculation according to (getx) array. array W is Weight , C is Cost and R is Reliability under the error.  \n\nerror is : \n\n&gt;IndexError                                Traceback (most recent call last)\n&lt;ipython-input-4-0ce5705f90fb&gt; in &lt;module&gt;\n     76             if char!= 0 :\n     77                 exp = expression(R , possition , char)\n---&gt; 78                 wei = Weight(W , possition , char)\n     79                 cost = Cost(C , possition , char)\n     80                 Total = Total* exp\n&lt;ipython-input-4-0ce5705f90fb&gt; in Weight(W, possition, char)\n     54 def Weight(W , possition , char):\n     55     expW = 0\n---&gt; 56     expW = expW + W[possition][int(char)]\n     57     return expW\n     58 \n&gt;IndexError: list index out of range\n\n\nmy code :\n\nimport random\n\ndef getsys():\n    row = \'\'\n    for i in range(0 , 8):\n        randintt = str(random.randint(0 , 3))\n        row += randintt\n    return row\n\n\ndef getx():\n    x = []\n    for i in range(0,14):\n        mysys = getsys()\n        x.append(mysys)\n    return x \n\ny = getx()\nprint (y)\n\n\n\n\nimport initialsys\nimport numpy as np\n\nR = np.array([[0.90 , 0.93,0.91 , 0.95],\n               [0.95 , 0.94, 0.93, 0],\n               [0.85 , 0.90 , 0.87 , 0.92],\n               [0.83 , 0.87 , 0.85 , 0 ],\n               [0.94 , 0.93 , 0.95 , 0],\n               [0.99 , 0.98 , 0.97 , 0.96],\n               [0.91 , 0.92 , 0.94 , 0],\n               [0.81 , 0.90 , 0.91 , 0],\n               [0.97 , 0.99 , 0.96 , 0.91],\n               [0.83 , 0.85 , 0.90 , 0],\n               [0.94 , 0.95 , 0.96 , 0],\n               [0.79 , 0.82 , 0.85 , 0.90],\n               [0.98 , 0.99 , 0.97 , 0],\n               [0.85 , 0.92 , 0.95 , 0.99]\n              ])\nC = np.array([[1, 1, 2, 2],\n               [2, 1, 1, 0],\n               [2, 3, 1, 4],\n               [3, 4, 5, 0],\n               [2, 2, 3, 0],\n               [3, 3, 2, 2],\n               [4, 4, 5, 0],\n               [3, 5, 6, 0],\n               [2, 3, 4, 3],\n               [4, 4, 5, 0],\n               [3, 4, 5, 0],\n               [2, 3, 4, 5],\n               [2, 3, 2, 0],\n               [4, 4, 5, 6]\n              ])\nW = np.array([[3,4,2,5],\n               [8,10,9,0],\n               [7,5,6,4],\n               [5,6,4,0],\n               [4,3,5,0],\n               [5,4,5,5],\n               [7,8,9,0],\n               [4,7,6,0],\n               [8,9,7,8],\n               [6,5,6,0],\n               [5,6,6,0],\n               [4,5,6,7],\n               [5,5,6,0],\n               [6,7,6,9]\n              ])\n\ndef expression(r ,possition , char ):\n    exp=1-r[possition , int(char)]\n    return exp\n\ndef Weight(W , possition , char):\n    expW = 0\n    expW = expW + W[possition][int(char)]\n    return expW\n\ndef Cost (C , possition , char):\n    expC = 0\n    expC =expC + W[possition][int(char)]\n    return expC\n\nT = []\nW = []\nC = []\nfor z in range(1000):\n    x = initialsys.getx()\n    possition = 0\n    Total = 1.0\n    char = ""\n    TotalC = 0\n    TotalW = 0\n    for row in x :\n        for char in row :\n            if char!= 0 :\n                exp = expression(R , possition , char)\n                wei = Weight(W , possition , char)\n                cost = Cost(C , possition , char)\n                Total = Total* exp\n                TotalC = TotalC + cost\n                TotalW = TotalW + wei\n            Total = 1-Total\n        end = 1\n        end = Total * end\n        possition = possition + 1 \n        T.append(end)\n        W.append(TotalC)\n        C.appaend(TotalW)\nprint(T)\n\n\nWhat can I do for this problem ??? :(\n'
"enter image description herePlease i am trying to name the index column but I can't. I want to be a able to name it such that I can reference it to view the index values which are dates. i have tried \n\ndf3.rename(columns={0:'Date'}, inplace=True) but it's not working. \n\nPlease can someone help me out? Thank you.\n"
"I have three separate DataFrames. Each DataFrame has the same columns - ['Email', 'Rating']. There are duplicate row values in all three DataFrames for the column Email. I'm trying to find those emails that appear in all three DataFrames and then create a new DataFrame based off those rows. So far I have I had all three DataFrames saved to a list like this dfs = [df1, df2, df3], and then concatenated them together using df = pd.concat(dfs). I tried using groupby from here but to no avail. Any help would be greatly appreciated  \n"
'New Plot\n\nMy code is as follows:\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndata=pd.read_csv("C:/Users/Jagdeep/Downloads/Alokji political data/political plot data parliamentary.csv")\n\n￼fig=plt.figure()\nax = fig.add_subplot(111)\nplt.plot(parl_data[\'BJP\'], marker=\'o\', label=\'BJP\', color=\'red\')\nplt.plot(parl_data[\'INC\'], marker=\'o\', label=\'INC\', color=\'blue\')\nplt.plot(parl_data[\'AAP\'], marker=\'o\', label=\'AAP\', color=\'brown\')\nplt.plot(parl_data[\'Total_Polling\'], marker=\'o\', label=\'Total \nPolling\', color=\'green\', linestyle=\'dashed\')\nplt.legend()\n\nfor i,j in parl_data.BJP.items():\n    ax.annotate(str(j), xy=(i, j))\nfor i,j in parl_data.INC.items():\n    ax.annotate(str(j), xy=(i, j))\nfor i,j in parl_data.AAP.items():\n    ax.annotate(str(j), xy=(i, j))\nfor i,j in parl_data.Total_Polling.items():\n    ax.annotate(str(j), xy=(i, j))\n\n\nax.set_alpha(0.8)\nax.set_title("Party-wise vote share in Lok Sabha Polls (Delhi) 2019", \nfontsize=15)\nax.set_ylabel("Parliament Polling Percentage", fontsize=15);\nax.set_xlabel("Election Year", fontsize=15)\n\nax.set_xticklabels([\'2004\',\'2009\',\'2014\',\'2019\'])\nplt.yticks(np.arange(0,100,10))\n\nplt.show()\n\n\nI am a new to Python and data science. How should I add Years 2004,2009,2014,2019 on x axis in the graph? I want Polling % vs Years plot. On using the code, I am not able to plot years on x-axis.\n'
"For SO purposes, this is some made up times series data:\n\nimport pandas as pd \nimport numpy as np \nfrom numpy.random import randint \n\nnp.random.seed(10)  # added for reproductibility                                                                                                                                                                 \n\nrng = pd.date_range('10/9/2018 00:00', periods=1000, freq='1H') \ndf = pd.DataFrame({'Random_Number':randint(1, 100, 1000)}, index=rng) \n\n\nQuestion, how do I create a function that can return re-sampled daily 97.5 &amp; 2.5 percentiles values for each in day in pandas data frame? I know this code below isn't even close it will just return the upper &amp; lower percentiles of the entire dataset. Ultimately Im trying to break this down per day and the dataframe returned the index would be the time stamp (date) of the day re sampled..\n\ndef createDfs(data):\n    for day in df:\n        dfDay = pd.DataFrame()\n\n        hi = df.quantile(0.975)[0]\n        low = df.quantile(0.025)[0]\n\n        data = {'upper_97.5%': [hi],\n                'lower_2.5%' : [low]}     \n\n        dfUpperLower = pd.DataFrame(data)\n        #dfUpperLower.set_index('Date')\n\n    return dfUpperLower\n\n\nAny tips greatly appreciated..\n"
'I try to use this tutorial to train my own car model recognition model: https://github.com/Helias/Car-Model-Recognition. And i want to use coda and my gpu perfomance to enhance training speed (preprocesssing step was completed without any errors).But when I try to train my model, I\'ve got the following errors:\n\n######### ERROR #######\n\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n\n            if __name__ == \'__main__\':\n                freeze_support()\n                ...\n\n        The "freeze_support()" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n\n\n######### batch #######\nTraceback (most recent call last):\n  File "D:\\Car-Model-Recognition\\main.py", line 78, in train_model\n\n\n######### ERROR #######\n[Errno 32] Broken pipe\n    for i, batch in enumerate(loaders[mode]):\n\n\n######### batch #######  File "C:\\Program Files\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 279, in __iter__\n\n    return _MultiProcessingDataLoaderIter(self)\nTraceback (most recent call last):\n  File "C:\\Program Files\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 719, in __init__\n  File "main.py", line 78, in train_model\n    w.start()\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\process.py", line 112, in start\n    for i, batch in enumerate(loaders[mode]):\n  File "C:\\Program Files\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 279, in __iter__\n    self._popen = self._Popen(self)\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\context.py", line 223, in _Popen\n    return _MultiProcessingDataLoaderIter(self)\n  File "C:\\Program Files\\Python37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py", line 719, in __init__\n    return _default_context.get_context().Process._Popen(process_obj)\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\context.py", line 322, in _Popen\n    w.start()\n    return Popen(process_obj)\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\popen_spawn_win32.py", line 46, in __init__\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\process.py", line 112, in start\n    prep_data = spawn.get_preparation_data(process_obj._name)\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\spawn.py", line 143, in get_preparation_data\n    self._popen = self._Popen(self)\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\context.py", line 223, in _Popen\n    _check_not_importing_main()\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\spawn.py", line 136, in _check_not_importing_main\n    return _default_context.get_context().Process._Popen(process_obj)\n  File "C:\\Program Files\\Python37\\lib\\multiprocessing\\context.py", line 322, in _Popen\n    is not going to be frozen to produce an executable.\'\'\')\nRuntimeError:\n        An attempt has been made to start a new process before the\n        current process has finished its bootstrapping phase.\n\n        This probably means that you are not using fork to start your\n        child processes and you have forgotten to use the proper idiom\n        in the main module:\n\n            if __name__ == \'__main__\':\n                freeze_support()\n                ...\n\n        The "freeze_support()" line can be omitted if the program\n        is not going to be frozen to produce an executable.\n    return Popen(process_obj)\n\n\nI have used the exact code from given link, and if i start my code using wsl, everything is ok, but I can\'t use my gpu from wsl. Where should I insert this name == \'main\' check to prevent such a mistake or how can i disable this multiprocessing\n'
'I\'m trying to scrape this link: \n34th government\n\n(https://knesset.gov.il/govt/eng/GovtByNumber_eng.asp)\n\nwhich has several tables, but when i perform a request using this code: \n\nimport requests\nfrom bs4 import BeautifulSoup\n\ngovts_url = r\'https://knesset.gov.il/govt/eng/GovtByNumber_eng.asp\'\nwebsite_url = requests.get(govts_url).text\nsoup = BeautifulSoup(website_url, \'lxml\')\nprint(f"HTML: \\n {soup.prettify()}")\n\n\nI get the following result:\n\n &lt;html&gt;\n &lt;head&gt;\n  &lt;meta charset="utf-8"/&gt;\n  &lt;script&gt;\n   window.rbzid="Q5gSRBmIWVopQazRgPTWKOEV0wGh1o+KvPO3KMiDuHxM9vVecPeHn4ult+Ba/KU9zInGRSRXUggEmkFs+D5NKSC/WEkCn+B4PCw9CeWkT+Q=";\n        u82222.O=function(x){return x;};u82222.E=function (){return typeof u82222.t.u===\'function\'?u82222.t.u.apply(u82222.t,arguments):u82222.t.u;};u82222.i=function(x,y){return x+y;};u82222.A=function (){return typeof u82222.u.u===\'function\'?u82222.u.u.apply(u82222.u,arguments):u82222.u.u;};u82222.Y=function (){return typeof u82222.u.u===\'function\'?u82222.u.u.apply(u82222.u,arguments):u82222.u.u;};u82222.n=function(x,y){return x+y;};u82222.f=function(x,y){return x+y;};u82222.u=function(){var M=function(K,N){var I=N&amp;0xffff;var r=N-I;return(r*K|0)+(I*K|0)|0;},Y=function(x,d,Z){var n=0xcc9e2d51,b=0x1b873593;var E=Z;var O=d&amp;~0x3;for(var w=0;w&lt;O;w+=4){var e=x.charCodeAt(w)&amp;0xff|(x.charCodeAt(w+1)&amp;0xff)&lt;&lt;8|(x.charCodeAt(w+2)&amp;0xff)&lt;&lt;16|(x.charCodeAt(w+3)&amp;0xff)&lt;&lt;24;e=M(e,n);e=(e&amp;0x1ffff)&lt;&lt;15|e&gt;&gt;&gt;17;e=M(e,b);E^=e;E=(E&amp;0x7ffff)&lt;&lt;13|E&gt;&gt;&gt;19;E=E*5+0xe6546b64|0;}e=0;switch(d%4){case 3:e=(x.charCodeAt(O+2)&amp;0xff)&lt;&lt;16;case 2:e|=(x.charCodeAt(O+1)&amp;0xff)&lt;&lt;8;case 1:e|=x.charCodeAt(O)&amp;0xff;e=M(e,n);e=(e&amp;0x1ffff)&lt;&lt;15|e&gt;&gt;&gt;17;e=M(e,b);E^=e;}E^=d;E^=E&gt;&gt;&gt;16;E=M(E,0x85ebca6b);E^=E&gt;&gt;&gt;13;E=M(E,0xc2b2ae35);E^=E&gt;&gt;&gt;16;return E;};return{u:Y};}();u82222.d=function(x,y){return x+y;};u82222.K=function (){return typeof u82222.u.u===\'function\'?u82222.u.u.apply(u82222.u,arguments):u82222.u.u;};u82222.N=function (){return typeof u82222.t.u===\'function\'?u82222.t.u.apply(u82222.t,arguments):u82222.t.u;};u82222.Z=function(x,y){return x+y;};u82222.I=function (){return typeof u82222.u.u===\'function\'?u82222.u.u.apply(u82222.u,arguments):u82222.u.u;};u82222.e=function (){return typeof u82222.t.u===\'function\'?u82222.t.u.apply(u82222.t,arguments):u82222.t.u;};u82222.t=function(){return{u:function(K){var A=\'\',I=decodeURI("1?\'%1CYH.=uVWU~%254_hW,o,WKM%22(-W%5BW,o,LU%075?\'%1CH%5D9.5LU%07#?\'%1C@W,o6LU%07%22?\'%1Ch%5C$.%25N%07D1?\'%1C%5DW,o4LU%07%124=LU%07%3C.%25N%07F=?\'%1COW,o7bAH%3E?\'%1CL%5B.=uF@F%3E%02%25N%07v%0F13SG%5D?,:AWU~13LU%07%3E?\'%1CvY8%205FFD.=uF%5BF%3C-%25N%07J1?\'%1C%5CW,o7");for(var Y=0,M=0;Y&lt;I.length;Y++,M++){if(M===K.length){M=0;}A+=String.fromCharCode(I.charCodeAt(Y)^K.charCodeAt(M));}A=A.split(\'~|.\');return function(t){return A[t];};}(\'PA[2))\')};}();u82222.o=function(x,y){return x+y;};u82222.r=function (){return typeof u82222.t.u===\'function\'?u82222.t.u.apply(u82222.t,arguments):u82222.t.u;};u82222.b=function(x,y){return x+y;};u82222.w=function(x){return x;};u82222.s=function(x,y){return x+y;};u82222.F=function(x,y){return x+y;};u82222.M=function (){return typeof u82222.u.u===\'function\'?u82222.u.u.apply(u82222.u,arguments):u82222.u.u;};u82222.T=function(x,y){return x&gt;y;};function u82222(){}u82222.x=function (){return typeof u82222.t.u===\'function\'?u82222.t.u.apply(u82222.t,arguments):u82222.t.u;};(typeof window==="object"?window:global).u82222=u82222;_=window;if(u82222.w(u82222.O(_[u82222.r(24)+u82222.e(0)+u82222.E(25)+u82222.E(14)+u82222.e(18)])||_[u82222.N(26)]||_[u82222.d(u82222.F(u82222.n(u82222.e(28),u82222.r(30))+u82222.N(20),u82222.N(14)),u82222.r(18))]||_[u82222.x(23)])||_[u82222.b(u82222.x(16),u82222.x(19))+u82222.r(6)+u82222.x(11)]||_[u82222.Z(u82222.E(6)+u82222.x(10)+u82222.e(9),u82222.x(14))]||_[u82222.s(u82222.T(975.11,476.89)?u82222.N(8):(13,105.77),u82222.E(1))+u82222.E(5)+u82222.N(25)]||_[u82222.E(4)]||_[u82222.o(u82222.x(3)+u82222.N(29)+u82222.e(14),u82222.e(15))+u82222.N(10)+u82222.x(7)]||_[u82222.i(u82222.e(2)+u82222.N(18)+u82222.N(12)+u82222.e(13)+u82222.x(22)+u82222.E(15)+u82222.E(25),u82222.e(27))+u82222.E(21)]){}else{location[u82222.f(u82222.r(11)+u82222.x(6)+u82222.e(17)+u82222.N(0),u82222.e(2))]();}\n  &lt;/script&gt;\n &lt;/head&gt;\n &lt;body&gt;\n &lt;/body&gt;\n&lt;/html&gt;\n\n\nWhich is, of course, not the content i desire. \nI guess i\'m missing some kind of "activation" to the site, to see the true content. But how can i see it?\n\nThx!\n'
'Please help me in this problem\n\nAre iterative calculations are possible in Python?\n\nI have the table which contains material number, stock and usage. I am giving input as 20\n\nMaterial   stock       usage       material availability(stock -input)\n\n100           10          2                 -10\n\n101           10          2                 -10\n\n102           10          2                 -10\n\n101            5          2                  -5\n\n\nIf I give input parameter as 20, it should subtract from stock and if you have the same material number it should hit first material number and then pass to the second material number.\n\nI have 2 of the same material numbers if I give input as 20 first it should hit the first material (101) gives as -10 and passes through the same material number(101) which has stock as 5 (-10+5) gives -5.\n\nThanks\n\nchaitanya\n'
"Hi guys I have really simple question for you, let me explain shortly, for more detailed explanation please read below.\n\nI want to order my 2D array which is an array of coordinates. However k= np.sort(k,0) is not working for me since it causes shuffling of y values. How can I solve this problem without losing my y values in an array of (x,y) ?\n\n\n\nOn the above you can see the result of np.sort(). It sorts all columns and shuffling the values, this makes the data useless. \n\nHere is a longer explanation why I need that order:\n\nI have a 2D array which are centers for K-means\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n\n\n\n\nI would like to sort this array with their x values because I want to use this in a RBF function like this:\n\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n\nrbfi = Rbf(centers[:, 0], centers[:, 1],function='gaussian')\nu = rbfi(centers[:,0])\nplt.plot(centers[:,0],u,'orange',linewidth=4)\nplt.tick_params(labelcolor = 'white')\n\n\n\n\nSince it is not ordered the function takes 2D points randomly and try to draw a line between them.\n\nThe result should look like something like this:\n\n\n"
'Using the dcc.Upload, you can build a drag and drop or button-based uploading feature in Dash Plotly dashboard. However, there is a limit in the documentation on handling a specific filetype such as .zip. Here\'s a snippet of the uploading html:\n\ndcc.Upload(\n    id=\'upload_prediction\',\n    children=html.Div([\n        \'Drag and Drop or \',\n        html.A(\'Select Files\'),\n        \' New Predictions (*.zip)\'\n    ]),\n    style={\n        \'width\': \'100%\',\n        \'height\': \'60px\',\n        \'lineHeight\': \'60px\',\n        \'borderWidth\': \'1px\',\n        \'borderStyle\': \'dashed\',\n        \'borderRadius\': \'5px\',\n        \'textAlign\': \'center\',\n        \'margin\': \'10px\'\n    },\n    accept=".zip",\n    multiple=True\n)\n\n\nThen when I try to examine the uploaded file with this snippet:\n\n@app.callback(Output(\'output_uploaded\', \'children\'),\n              [Input(\'upload_prediction\', \'contents\')],\n              [State(\'upload_prediction\', \'filename\'),\n               State(\'upload_prediction\', \'last_modified\')])\ndef test_callback(list_of_contents, list_of_names, list_of_dates):\n    for content in list_of_contents:\n        print(content)\n\n\nThe content-type after uploading is ‘data:application/x-zip-compressed;base64’. How to handle this type of file in Dash Plotly (for example to extract it somewhere)?\n\nA similar question was asked in the plotly forum without answers:\nhttps://community.plot.ly/t/dcc-upload-zip-file/33976\n'
'I am trying to convert nested JSON into CSV using pandas. I have viewed similar questions asked here but I can\'t seem apply in on my scenario.\nMy JSON is the following\n\n{\n "51% FIFTY ONE PERCENT(PWD)" : {\n "ID" : "51%1574233975114-WEBAD",\n "contactName" : "",\n "createdAt" : 1574233975,\n "debit" : 118268.19999999995,\n "defaultCompany" : "",\n "emailAddress" : "",\n "lastUpdatedAt" : "",\n "phoneNumber" : "",\n "taskNumber" : 0\n},\n "51% STORE (MUZ)" : {\n "ID" : "51%1576650784631-WEBAD",\n "contactName" : "",\n "createdAt" : 1576650784,\n "debit" : 63860,\n "defaultCompany" : "",\n "emailAddress" : "",\n "lastUpdatedAt" : "",\n "phoneNumber" : "",\n "taskNumber" : 0\n},\n "ABBOTT S" : {\n  "STORE (ABD)" : {\n   "ID" : "ABB1574833257715-WEBAD",\n   "contactName" : "",\n   "createdAt" : 1574833257,\n   "debit" : 35065,\n   "defaultCompany" : "",\n   "emailAddress" : "",\n   "lastUpdatedAt" : "",\n   "phoneNumber" : "",\n   "taskNumber" : 0\n }\n}\n}\n\n\nThis is a snippet of the JSON and as you can see some entries, not all, are nested.\nI tried using the json_normalize the following way i.e.\n\nimport json\nfrom pandas.io.json import json_normalize  \n\nwith open(\'.\\Customers\\kontrolkotlin-CUSTOMERS-export.json\') as f:\nd = json.load(f)\n\nnycphil = json_normalize(data = d)\nnycphil\n\n\nAnd got a single row dataframe as output as shown below\n\nThis doesn\'t seem to work out as I want to something readable and understandable.\n'
"from sklearn.svm import LinearSVC\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\nfrom sklearn.metrics import accuracy_score\n\nX = data['Review']\n\ny = data['Category']\n\ntfidf = TfidfVectorizer(ngram_range=(1,1))\n\nclassifier = LinearSVC()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\nclf =  Pipeline([\n    ('tfidf', tfidf),\n    ('clf', classifier)\n])\n\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n\naccuracy_score(y_test, y_pred)\n\n\nThis is the code to train a model and prediction. I need to know my model performance. so where should I change to become cross_val_score?\n"
"Lets say I have a data frame \n\npurchase_1 = pd.Series({'Name': 'Chris',\n                        'Item Purchased': 'Dog Food',\n                        'Cost': 22.50})\npurchase_2 = pd.Series({'Name': 'Kevyn',\n                        'Item Purchased': 'Kitty Litter',\n                        'Cost': 2.50})\npurchase_3 = pd.Series({'Name': 'Vinod',\n                        'Item Purchased': 'Bird Seed',\n                        'Cost': 5.00})\n\ndf = pd.DataFrame([purchase_1, purchase_2, purchase_3], index=['Store 1', 'Store 1', 'Store 2'])\n\n\nMy question is that is there any performance issue/difference between these 2 code blocks\n\n#code number 1\n%time\nprint(df[df['Cost'] &gt; 3]['Name'])\nOUT[3]:CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n      Wall time: 14.8 µs\n\n#Code number 2\n%time\nprint(df['Name'][df['Cost'] &gt; 3])\nOUT[4]:CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n       Wall time: 7.39 µs\n\n"
"I have coordinates of city: (52.2319581, 21.0067249) and Python dictionary with cities around mentioned city. How to get 3 closest cities from given coordinates: \n\n({'Brwinów': (52.133333, 20.716667), 'Warszawa Bielany': (52.283333, 20.966667), 'Legionowo': (52.4, 20.966667), 'Warszawa-Okęcie': (52.16039, 20.961674), 'Warszawa': (52.280957, 20.961348), 'Belsk Duży': (51.833333, 20.8)}, {})\n\nThanks for help.\n"
'import requests\nr=requests.get(\'https://www.crummy.com/software/BeautifulSoup/\')\nfrom bs4 import BeautifulSoup as bs\nsoup=bs(r.text,\'html.parser\')\nlinks=[x[\'href\'] for x in soup.find_all(\'a\')]\nlinks \n\n\nerror is: \n\nKeyError                                  \nTraceback (most recent call last)\n&lt;ipython-input-137-97ef77b6e69a&gt; in &lt;module&gt;\n----&gt; 1 links=[x[\'href\'] for x in soup.find_all(\'a\')]\n      2 links\n\n&lt;ipython-input-137-97ef77b6e69a&gt; in &lt;listcomp&gt;(.0)\n----&gt; 1 links=[x[\'href\'] for x in soup.find_all(\'a\')]\n      2 links\n\n~\\anaconda3\\lib\\site-packages\\bs4\\element.py in __getitem__(self, key)\n   1319         """tag[key] returns the value of the \'key\' attribute for the Tag,\n   1320         and throws an exception if it\'s not there."""\n-&gt; 1321         return self.attrs[key]\n   1322 \n   1323     def __iter__(self):\n\nKeyError: \'href\'\n\n\nbut, the following code works fine:\n\nimport requests\nr=requests.get(\'https://en.wikipedia.org/wiki/Harvard_University\')\nfrom bs4 import BeautifulSoup as bs\nsoup=bs(r.text,\'html.parser\')\nclasses=[table[\'class\'] for table in soup.find_all(\'table\')]\nclasses \n\n'
"I have 3 xlsx files named as data, vector and heading.\n\nData contains 5x5 entries ,this is data dataframe looks\nvector contains 5x1 entries ,this is vector dataframe looks\nheading contains 5x1 entries ,this is heading dataframe looks\n\nI had taken each column from data file and find its correlation with vector\nfor i,j in data.iteritems():\ncol=j\ncoef, p = spearmanr(col, vector)\nalpha = 0.05\nif p &lt; alpha:\n    print('\\n\\nSpearmans correlation coefficient: %.3f' % coef)\n    print(f' coloumn {[i]} method are correlated (reject H0) p=%.3f' % p)\n\nwhich gives me the output as\n\nSpearmans correlation coefficient: 1.000.  coloumn[0] method are correlated (reject H0) p=0.000\nSpearmans correlation coefficient: 1.000.  coloumn[2] method are correlated (reject H0) p=0.000\nSpearmans correlation coefficient: 1.000.  coloumn[3] method are correlated (reject H0) p=0.000\n\nI want to point the particular coloumn of heading so that my desired output will be\n\nSpearmans correlation coefficient: 1.000. Red method are correlated (reject H0) p=0.000\nSpearmans correlation coefficient: 1.000. Green method are correlated (reject H0) p=0.000\nSpearmans correlation coefficient: 1.000. Blue method are correlated (reject H0) p=0.000\n\nAny reference or resource will be helpful.\nThanks\n"
"Let's say the data used is something like this\n\ndf = pd.DataFrame({'Order_id': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n                   'Order_date': ['10/1/2020', '10/1/2020', '11/1/2020', '11/1/2020', '12/1/2020', '12/1/2020', '12/1/2020', '12/1/2020', '13/1/2020', '13/1/2020'],\n                   'Product_nr': [0, 2, 1, 0, 2, 0, 2, 1, 2, 0],\n                   'Quantity': [3, 1, 6, 5, 10, 1, 2, 5, 4, 3]})\n\n#transforming the date column into datetime\ndf['Order_date'] = pd.to_datetime(df['Order_date'])\n\n\nand I'm trying to plot the number of ordered products per day per product over the given time span.  \n\nMy initial idea would be something like\n\nproduct_groups = df.groupby(['Product_nr']) \nproducts_daily = pd.DataFrame()\nfor product, total_orders in product_groups:\n    products_daily[product.day] = total_orders.values\nproducts_daily.plot(subplots=True, legend=False)\npyplot.show()\n\n\nI know there must be a groupby('Product_nr') and the date should be splitted into days using Grouper(freq='D'). They should also be a for loop to combine them and then plotting them all but I really have no clue how to put those pieces together. How can I archieve this? My ultimate goal is actually to plot them per month per product for over 4 years of sales records, but given the example data here I changed it into daily.\n\nAny suggestion or link for guides, tutorials are welcome too. Thank you very much!\n"
"I am trying to run this on Amazon Sagemaker but I am getting this error while when I try to run it on my local machine, it works very fine.\nthis is my code:\nimport tensorflow as tf\n\nimport IPython.display as display\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12,12)\nmpl.rcParams['axes.grid'] = False\n\nimport numpy as np\nimport PIL.Image\nimport time\nimport functools\n    \ndef tensor_to_image(tensor):\n  tensor = tensor*255\n  tensor = np.array(tensor, dtype=np.uint8)\n  if np.ndim(tensor)&gt;3:\n    assert tensor.shape[0] == 1\n    tensor = tensor[0]\n  return PIL.Image.fromarray(tensor)\n\ncontent_path = tf.keras.utils.get_file('YellowLabradorLooking_nw4.jpg', 'https://example.com/IMG_20200216_163015.jpg')\n\n\nstyle_path = tf.keras.utils.get_file('kandinsky3.jpg','https://example.com/download+(2).png')\n\n\ndef load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim / long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n\ndef imshow(image, title=None):\n  if len(image.shape) &gt; 3:\n    image = tf.squeeze(image, axis=0)\n\n  plt.imshow(image)\n  if title:\n    plt.title(title)\n\n\ncontent_image = load_img(content_path)\nstyle_image = load_img(style_path)\n\nplt.subplot(1, 2, 1)\nimshow(content_image, 'Content Image')\n\nplt.subplot(1, 2, 2)\nimshow(style_image, 'Style Image')\n\nimport tensorflow_hub as hub\nhub_module = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/1')\nstylized_image = hub_module(tf.constant(content_image), tf.constant(style_image))[0]\ntensor_to_image(stylized_image)\n\n\nfile_name = 'stylized-image5.png'\ntensor_to_image(stylized_image).save(file_name)\n\nThis is the exact error I get:\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-24-c47a4db4880c&gt; in &lt;module&gt;()\n     53 \n     54 \n---&gt; 55 content_image = load_img(content_path)\n     56 style_image = load_img(style_path)\n     57 \n\n in load_img(path_to_img)\n     34 \n     35     shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n---&gt; 36     long_dim = max(shape)\n     37     scale = max_dim / long_dim\n     38 \n\n~/anaconda3/envs/amazonei_tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in iter(self)\n    475     if not context.executing_eagerly():\n    476       raise TypeError(\n--&gt; 477           &quot;Tensor objects are only iterable when eager execution is &quot;\n    478           &quot;enabled. To iterate over this tensor use tf.map_fn.&quot;)\n    479     shape = self._shape_tuple()\n\nTypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.\n"
"I am trying to format a tooltip using datum but, so far, without any success.\nThe tooltip I need is the something like &quot;INV: 174,000.00&quot;. How can I do it?\nThis is where I supposed to use datum:\ntext = line.mark_text(align='right', dx=-10, dy=-10).encode(\n        text=alt.condition(nearest, f'Revenue:Q', alt.value(' '))\n    ).transform_calculate(label='&quot;INV: &quot; + datum.Revenue')\n\nFull code:\nimport altair as alt\nfrom altair import datum\nimport pandas as pd\nimport numpy as np\nimport os\n\n\n\ndef areaChart():\n\n\n    df = {\n        'Stat': ['INV'],\n        'Revenue': [474147.84, 2170326.05, 2184077.88, 3957965.97]\n    }\n\n    source = pd.DataFrame(np.cumsum(df, 0),\n                        columns='Revenue', index=pd.RangeIndex(len(df['Stat']), name='Revenue'))\n    print(source)\n    source = source.reset_index().melt('Revenue', var_name='Analyzing', value_name='Revenue')\n\n    nearest = alt.selection(type='single', nearest=True, on='mouseover',\n                            fields=['Revenue'], empty='none')\n\n    line = alt.Chart(source).mark_area(opacity=0.60).encode(\n        x=alt.X(f'Stat:Q', axis=alt.AxisConfig()),\n        y=f'Revenue:Q',\n        color='Analyzing:N'\n    )\n\n    selectors = alt.Chart(source).mark_point().encode(\n        x=f'Stat:Q',\n        opacity=alt.value(0),\n    ).add_selection(\n        nearest\n    )\n\n    points = line.mark_point().encode(\n        opacity=alt.condition(nearest, alt.value(1), alt.value(0))\n    ).properties(\n        title=f'Stat     x     INV'\n    )\n\n    text = line.mark_text(align='right', dx=-10, dy=-10).encode(\n        text=alt.condition(nearest, f'Revenue:Q', alt.value(' '))\n    ).transform_calculate(label='&quot;INV: &quot; + datum.Revenue')\n\n\n    rules = alt.Chart(source).mark_rule(color='black', size=0.70).encode(\n        x=f'Stat:Q',\n    ).transform_filter(\n        nearest\n    )\n\n    chart = alt.layer(\n        line, selectors, points, rules, text\n    )\n\n    return chart\n\n"
'My X axis data is like this:\n X =[0.0, 0.9, 0.99, 0.999, 0.9999, 0.99999, 0.999999, 0.9999999, 0.99999999, 0.999999999, 0.9999999999, 0.99999999999, 0.999999999999, 0.9999999999999, 0.99999999999999, 1.000000000000001, 1.00000000000001, 1.0000000000001, 1.000000000001, 1.00000000001, 1.0000000001, 1.000000001, 1.00000001, 1.0000001, 1.000001, 1.00001, 1.0001, 1.001, 1.01, 1.1]\n\nMy Y axis data is like this:\nY = [1.0, 3.16227766016838, 9.999999999999995, 31.622776601683782, 100.00000000000551, 316.22776601755754, 999.9999999856221, 3162.277661000621, 9999.999974876204, 31622.777048860404, 99999.99586298171, 316227.7529344374, 1000011.0610435781, 3161786.1272856337, 10003998.786452563, -30011996.35935769, -10003998.786452563, -3163542.1874750517, -999955.5526723525, -316227.7529344374, -99999.99586298171, -31622.77529344374, -10000.000030387355, -3162.2776592452046, -1000.0000000411333, -316.2277660158021, -100.00000000000551, -31.622776601685537, -9.999999999999995, -3.162277660168378]\n\nI want to plot this data and I should be getting a graph with curves but all I get is this:\nplt.plot(X,Y)\nplt.show()\n\n#output:\n\n\nBut when I use this code:\n\u200bplt.plot(range(30),Y)\nplt.show()\n\n#Output:\n\n\nHow do I get Second Plot but using the X data of the List as mentioned above?\n'
"I try to prepare my dataset for cluster analyses like k-Means or BIRCH algorithm. I don't know how-to start with the preparation and find, for example, outliner or something else. I hope you can help me.\nMy dataset is a dataframe and has the following columns:\nA1 | A2 | A3 | A4 | iO\n\nWith df.describe(include = &quot;all&quot;)  I get the following datas:\n         A1             | A2            | A3            |A4             | iO\n-------------------------------------------------------------------------------\ncount    15372.000000    15372.000000    15372.000000    15372.000000    15372\nunique   NaN             NaN             NaN             NaN             2\ntop      NaN             NaN             NaN             NaN             True\nfreq     NaN             NaN             NaN             NaN             14935 \nmean     145.570554      35.750641       64.284180       49.225190       NaN\nstd      5134.725533     1709.696386     2433.688046     2380.091285     NaN\nmin      0.436000        0.353000        0.353000        -0.000000       NaN\n25%      1.000000        1.920000        2.000000        1.690000        NaN\n50%      1.970000        2.000000        3.860000        2.000000        NaN\n75%      3.984000        3.907000        6.957000        3.883000        NaN\nmax      200990.260000   200072.157000   200992.243000   200992.243000   NaN\n\nThe median of each column is:\nA2: 2.00\nA3: 3.86\nA4: 2.0\niO: 1.0\n\nIn the normal case the values of A1,A2,A3 and A4 are between 2 and 5.\nFurther I want to use this dataframe for clustering analyses. In my dataframe is &quot;iO&quot; a result of the usecase and A1, A2, A3 and A4 has an impact on the result.\nI hope you can help me to prepare my data.\nBest regards\nChristian\n"
"I have a requirement to fill out a column with values from same column but from different rows.\nExample: Below is the data structure.\n    fname   lname\n0    bob    andy\n1  manny  dorson\n2    bob     NaN \n\nNow for all &quot;lname&quot; values which are NaN, I want to fill those with &quot;lname&quot; values from the rows which share the common &quot;fname&quot;\nSo 3rd row which does not have &quot;lname&quot; I want to pick the &quot;lname&quot; from 1st row since &quot;fname&quot; for both the rows are same.\nThe result which I expect :\n    fname   lname\n0    bob    andy\n1  manny  dorson\n2    bob     andy \n\nThis is just a simple minified example.\nAnd let's say, if there are multiple rows with matching first name, we can pick up the first one.\nI tried a lot of things but not getting it to work.\nAny help is appreciated. Thanks.\n"
"I've got a pandas data frame with two columns, 'morning' and 'evening', which represent a morning and an evening pressure measurement. The values in the columns are either 'high', 'medium', or 'low'.\n    morning evening\n0   high    high\n1   high    medium\n2   high    medium\n3   high    low\n4   medium  high\n\nI want to create two lists, 'pressure_change' and 'pressure_change_likelihood'. 'pressure_change' describes what type of pressure change took place between the morning and the evening, eg 'high-low' is a change from a high morning pressure to a low evening pressure. 'pressure_change_likelihood' describes how frequent a given type of pressure change is, eg if the pressure was high in the morning it went to medium in the evening half of the time (0.50).\npressure_change = [['high-high', 'high-medium', 'high-low'],\n                  ['medium-high', 'medium-medium', 'medium-low'],\n                  ['low-high', 'low-medium', 'low-low']]\n\npressure_change_likelihood = [[0.25, 0.50, 0.25],\n                              [0.33, 0.00, 0.67],\n                             [0.00, 1.00, 0.00]]\n\nI've been able to create 'pressure_change' fine, but the problem is 'pressure_change_likelihood'. I thought I'd try to use pd.df.groupby() as a starting point and then convert the output to a list of lists, but the pandas series generated doesn't include a 0.00 value for events that never took place, and the order of the values is different from what I need it to be.\nimport pandas as pd \n\n# Data \ndata = [['high', 'high'], ['high', 'medium'], ['high', 'medium'], ['high', 'low'], ['medium', 'high'], ['medium', 'low'], ['medium', 'low'],['low', 'medium']] \ndf = pd.DataFrame(data, columns = ['morning', 'evening']) \n\n# pressure_change\nunique_array= df.morning.unique()\npressure_change = []\nfor i in unique_array:\n    sub_list = []\n    for k in unique_array:\n        sub_list.append(i+'-'+k)\n    pressure_change.append(sub_list)\n\n# pressure_change_likelihood   \nper = df.groupby(['morning', 'evening'])['evening'].size()\npressure_change_likelihood  = per.groupby(level=0).apply(lambda x: round(x / float(x.sum()), 2))\n\nprint(pressure_change_likelihood)\n\nmorning  evening\nhigh     high       0.25\n         low        0.25\n         medium     0.50\nlow      medium     1.00\nmedium   high       0.33\n         low        0.67\nName: evening, dtype: float64\n\nThanks for any help!\n"
"How do we filter the dataframe below to remove all duplicate ID rows after a certain number of ID occurrence. I.E. remove all rows of ID == 0 after the 3rd occurrence of ID == 0\nThanks\n pd.DataFrame(np.random.randint(0,10,size=(100, 2)), columns=['ID', 'Value']).sort_values('ID')\n               \n                 Output:\n                   ID   Value\n                    0   7\n                    0   8\n                    0   5\n                    0   5\n                ... ... ...\n                    9   7\n                    9   7\n                    9   1\n                    9   3\n\nDesired Output for filter_count = 3:\n\n                 Output:\n                   ID   Value\n                    0   7\n                    0   8\n                    0   5\n                    1   7\n                    1   7\n                    1   1\n                    2   3\n\n\n                \n\n"
"I have a Dataframe like this\ndata = {'Customer':['C1', 'C1', 'C1', 'C2', 'C2', 'C2', 'C3', 'C3', 'C3'],\n        'NumOfItems':[3, 2, 4, 5, 5, 6, 10, 6, 14],\n        'PurchaseTime':[&quot;2014-01-01&quot;, &quot;2014-01-02&quot;, &quot;2014-01-03&quot;,&quot;2014-01-01&quot;, &quot;2014-01-02&quot;, &quot;2014-01-03&quot;,&quot;2014-01-01&quot;, &quot;2014-01-02&quot;, &quot;2014-01-03&quot;]\n       }\ndf = pd.DataFrame(data)\ndf\n\nI want to create a Feature which is for example the max value for each customer up to this point:\n'MaxPerID(NumOfItems)':[3, 3, 4, 5, 5, 6, 10, 10, 14] #the output i want\n\nSo I set up the EntitySet and normalize it …\nes = ft.EntitySet(id=&quot;customer_data&quot;)\nes = es.entity_from_dataframe(entity_id=&quot;customer&quot;,\n                              dataframe=df,\n                              index='index',\n                              time_index=&quot;PurchaseTime&quot;,\n                             make_index=True)\n\nes = es.normalize_entity(base_entity_id=&quot;customer&quot;,\n                         new_entity_id=&quot;sessions&quot;,\n                         index=&quot;Customer&quot;)\n\nBut creating the feature matrix doesn't produce the results i want.\nfeature_matrix, features = ft.dfs(entityset=es,\n                                 target_entity=&quot;customer&quot;,\n                                 agg_primitives = [&quot;max&quot;],\n                                 max_depth = 3                                      \n                                 )\nfeature_matrix.head\n\nsessions.MAX(customer.NumOfItems)  \nindex                                                                         \n0                                      4                                    \n3                                      6                                    \n6                                     14                                    \n1                                      4                                    \n4                                      6                                    \n7                                     14                                    \n2                                      4                                    \n5                                      6                                    \n8                                     14                                    \n\nThe returned feature is the max value per day from all customers (sorted by time), however if I run the same code without the time_index = &quot;PurchaseTime&quot; the result is the max value just for the specific customer\n    sessions.MAX(customer.NumOfItems)  \\\nindex                                                                       \n0                    4   \n1                    4   \n2                    4   \n3                    6   \n4                    6   \n5                    6   \n6                   14   \n7                   14   \n8                   14   \n                             \n\nI want a combination of these two: the max value for the specific customer up to this point.\nIs this possible? I tried to work with es['customer']['Customer'].interesting_values =['C1', 'C2', 'C3'] but it didn't get me anywhere. I also tried modifying the new normalized entity and writing my own primitive for this.\nI'm new to featuretools so any help would be greatly appreciated.\nThis Question is similar to mine but the solution has no time_index and is creating the new features on the normalized entity\n"
'dataframe1\ndata_a           data_b      data_c    data_d    data_e\n61               0.30792          Rest 2.34857    True\n183              0.93408          Rest 2.34550    True\n305              1.56019          Rest 2.34215    True\n427              2.18636          Rest 2.33955    True\n549              2.81252          Rest 2.33660    True\n\ndataframe2\ndata_a           data_b      data_c    data_d    data_e\n122              0.62616     Discharge 2.32013   False\n244              1.25233     Discharge 2.31390   False\n366              1.87844     Discharge 2.31087   False\n488              2.50460     Discharge 2.30819   False\n610              3.13077     Discharge 2.30567   False\n\nI would like the out put to be as follows:\ndataframe3\ndata_a           data_b      data_c    data_d    data_e\n61               0.30792          Rest 2.34857    True\n122              0.62616     Discharge 2.32013   False\n183              0.93408          Rest 2.34550    True\n244              1.25233     Discharge 2.31390   False\n305              1.56019          Rest 2.34215    True\n366              1.87844     Discharge 2.31087   False\n427              2.18636          Rest 2.33955    True\n488              2.50460     Discharge 2.30819   False\n549              2.81252          Rest 2.33660    True\n610              3.13077     Discharge 2.30567   False\n\nAs you can see, the new dataframe should be sorted by according to the sequential order of the values in the data_a column.\n'
"I'm working with a dataset of about ~ 32.000.000 rows:\nRangeIndex: 32084542 entries, 0 to 32084541\n\ndf.head()\n\n\n        time                        device      kpi                                 value\n0   2020-10-22 00:04:03+00:00       1-xxxx  chassis.routing-engine.0.cpu-idle   100\n1   2020-10-22 00:04:06+00:00       2-yyyy  chassis.routing-engine.0.cpu-idle   97\n2   2020-10-22 00:04:07+00:00       3-zzzz  chassis.routing-engine.0.cpu-idle   100\n3   2020-10-22 00:04:10+00:00       4-dddd  chassis.routing-engine.0.cpu-idle   93\n4   2020-10-22 00:04:10+00:00       5-rrrr  chassis.routing-engine.0.cpu-idle   99\n\nMy goal is to create one aditional columns named role, filled with regard a regex\nThis is my approach\ndef router_role(row):\n    if row[&quot;device&quot;].startswith(&quot;1&quot;):\n        row[&quot;role&quot;] = '1'\n    if row[&quot;device&quot;].startswith(&quot;2&quot;):\n        row[&quot;role&quot;] = '2'\n    if row[&quot;device&quot;].startswith(&quot;3&quot;):\n        row[&quot;role&quot;] = '3'\n    if row[&quot;device&quot;].startswith(&quot;4&quot;):\n        row[&quot;role&quot;] = '4'\n    return row\n\nthen,\ndf = df.apply(router_role,axis=1)\n\nHowever it's taking a lot of time ... any idea about other possible approach ?\nThanks\n"
'I want to visualize 4 test samples of k-NN Classifier. I have searched it but I could not find anything. Can you help me with implementing the code?\nHere is my code so far,\nfrom sklearn.datasets import make_moons\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX, y = make_moons(n_samples=100, noise=0.3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.04, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nThrough a 1×4-axis figure. For each axis, I want to visualize training samples, corresponding testing sample (indicated with ‘+’ marker) as well the nearest k neighbors of that sample (indicated with green border color). The title of each axis should state predicted class.\n'
"I'm trying to change the labels in a violin plot on Seaborn. I wanna change the NU_NOTA_CN, NU_NOTA_CH, NU_NOTA_LC, NU_NOTA_MT and NU_NOTA_REDAÇÃO, and TP_ESCOLA, and the 2 and 3.\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfig_dims = (10, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.boxplot(x=&quot;DISCIPLINA&quot;, y=&quot;NOTA&quot;, hue=&quot;TP_ESCOLA&quot;, data=publica_privada_pivot)  \nplt.show()\n\nplt.clf()\nplt.close()\n\nviolin plot here\n"
"As part of assignment 4, Coursera CV TF course, my code fails in model.fit()\nmodel.compile(loss='categorical_crossentropy',metrics=\n   ['accuracy'],optimizer=tf.keras.optimizers.RMSprop(lr=0.001))\n# shuffle and create batches before training\n\nmodel.fit(train_batches,epochs=25)\n\nwith error:\nValueError: Shapes (None, 1) and (None, 2) are incompatible\n\nAny hint at where problem might come from? I suspect bad format or type for train_batches:\ntrain_data = tfds.load('cats_vs_dogs', split='train[:80%]', as_supervised=True) \naugmented_training_data = train_data.map(augmentimages)\ntrain_batches = augmented_training_data.batch(32)\n\n"
"I have an expected array [1,1,3] and a predicted array [1,2,2,4] for which I want to calculate precision_recall_fscore_support, so I need a matrix in the following format:\n&gt;&gt; mlb = MultiLabelBinarizerWithDuplicates()\n&gt;&gt; transformed = mlb.fit_transform([(1, 1, 3), (1, 2, 2, 4)])\narray([[1,1,0,0,1,0],\n       [1,0,1,1,0,1]])\n&gt;&gt; mlb.classes_\n[1,1,2,2,3,4]\n\n\nFor the duplicated values I don't care which one of them is turned on, meaning that this is also a valid result:\narray([[1,1,0,0,1,0],\n       [0,1,1,1,0,1]])\n\nMultiLabelBinarizer clearly says &quot;All entries should be unique (cannot contain duplicate classes)&quot; so it doesn't support this usecase.\n"
'I ran into this error while trying to do a project: ValueError: Found arrays with inconsistent numbers of samples: [878049 884262].\n\nI get it when I try to run my knn classifier at the bottom. I\'ve been reading about it and I know it\'s because my X and y are not the same.\nThe shape for X is (878049, 2) and y is (884262, ).\n\nHow can I fix this error so that they match?\n\nCode:\n\n# drop features that we wont be using\n# train.head()\ndf = train.drop([\'Descript\', \'Resolution\', \'Address\'], axis=1)\n\ndf2 = test.drop([\'Address\'], axis=1)\n\n# trying to see the times during a day a particular crime occurs, for example\n# rapes occur more from 12am-4am during the weekend.\n# example below\ndow = {\n    \'Monday\':0,\n    \'Tuesday\':1,\n    \'Wednesday\':2,\n    \'Thursday\':3,\n    \'Friday\':4,\n    \'Saturday\':5,\n    \'Sunday\':6\n}\ndf[\'DOW\'] = df.DayOfWeek.map(dow)\n\n# Add column containing time of day\ndf[\'Hour\'] = pd.to_datetime(df.Dates).dt.hour\n\n# making my feature column\nfeature_cols = [\'DOW\', \'Hour\']\nX = df[feature_cols] \n\ndf2[\'DOW\'] = df2.DayOfWeek.map(dow)\n\n\ny = df2[\'DOW\']\n\n# columns in X and y don\'t match\nprint(X.shape)\nprint(y.shape)\nprint(y.head())\nprint(X.head())\n\n# Knn classifier\nk = 5\nmy_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k)\nmy_knn_for_cs4661.fit(X, y)\n\n# KNN (with k=5), Decision Tree accuracy\ny_predict = my_knn_for_cs4661.predict(X)\nprint(\'\\n\')\nscore = accuracy_score(y, y_predict)\n\nprint("K=",k,"Has ",score, "Accuracy")\nresults = pd.DataFrame()\nresults[\'actual\'] = y\nresults[\'prediction\'] = y_predict \nprint(results.head(10))\n\n\nStack trace:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-11-5a002c1fd668&gt; in &lt;module&gt;()\n      7 k = 5\n      8 my_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k)\n----&gt; 9 my_knn_for_cs4661.fit(X, y)\n     10 #KNN (with k=5), Decision Tree accuracy\n     11 y_predict = my_knn_for_cs4661.predict(X)\n\nC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\base.py in fit(self, X, y)\n    776         """\n    777         if not isinstance(X, (KDTree, BallTree)):\n--&gt; 778             X, y = check_X_y(X, y, "csr", multi_output=True)\n    779 \n    780         if y.ndim == 1 or y.ndim == 2 and y.shape[1] == 1:\n\nC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\n    518         y = y.astype(np.float64)\n    519 \n--&gt; 520     check_consistent_length(X, y)\n    521 \n    522     return X, y\n\nC:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py in check_consistent_length(*arrays)\n    174     if len(uniques) &gt; 1:\n    175         raise ValueError("Found arrays with inconsistent numbers of samples: "\n--&gt; 176                          "%s" % str(uniques))\n    177 \n    178 \n\nValueError: Found arrays with inconsistent numbers of samples: [878049 884262]\n\n'
"This is my data Frame \n\nId_Student  English History Mathmatic\n\n1   66.0    NaN         80.0\n2   NaN     66.0        NaN\n3   NaN     NaN         NaN\n4   55.0    94.0        94.0\n\n\nI want to use this methode to fix missing value \n\nmdf1 = mdf.fillna(method='ffill')\n\n\nBut it looks like if first value is NaN it's not helping much. First value under History column still NaN \n\nId_Student  English History Mathmatic\n\n1       66.0        NaN      80.0\n2       66.0       66.0      80.0\n3       66.0       66.0      80.0\n4       55.0       94.0      94.0\n5       55.0       85.0      85.0\n\n\nAny idea to fix this kind of issue \nCheers mate \n"
"I have a data set of some advertise publishers. publishers earn money for each click on the advertises. data set is consist of publishers list and the corresponding number of clicks and number of transaction they caused. the problem is whether the publisher cheating and click it's own advertise to gain more money or not. but some of these publishers total click is very very small (below 10) and therefore the number of transactions are 0. \n\nmy question is that what should i do with these zero data? they actually ruin my gaussian distribution of data. what should i do with them? just eliminate them from my data set? is there any statistical approach to do such thing?\n\nby the way I'm very new to data analysis and excuse me if the answer is obvious, but i couldn't find answer on the web.\n"
"I have a dataframe with some columns that i have been adding myself. There is one specific column that gathers the max and min tide levels. \n\nPandas Column mostly empty but with some reference values\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'a':[1,2,3,4],'b':[np.nan,np.nan,3,4]},columns=['a','b']) \ndf\n\n\nThe problem is that the column is mostly empty because it only shows those peak values and not the intermediate ones. I would like to fill the missing values with a function similiar to the image shown below.\n\nI want to fill it with a function of this kind\n\nThank you in advance.\n"
'I have a dataset where I tagged the noun phrases. How to find these tags and extract the data from inside the tag. \n\nدر\nهمین\nحال\n&lt;coref coref_coref_class="set_0" coref_mentiontype="ne" markable_scheme="coref" coref_coreftype="ident"&gt;\nنجیب\nالله\nخواجه\nعمری\n&lt;/coref&gt;\n&lt;coref coref_coref_class="set_0" coref_mentiontype="np" markable_scheme="coref" coref_coreftype="ident"&gt;\nسرپرست\nوزارت\nتحصیلات\nعالی\nافغانستان\n&lt;/coref&gt;\nگفت\nکه\n\n\n def ex_feature(text):\n    for w in text:\n        if w.startswith("&lt;coref") and w.endswith("&lt;/coref&gt;"):\n            print(w)\n\n'
"I have done a ton of preprocessing and math on this data to arrive at two equally sized 3xN numpy arrays. A and B. \n\nA = integers that have been classified as labels to predict B.\nB = time series data.\n\nI also have C which is just B[1:] \n\nA and B are equal at their respective time steps and I can't figure out a better way to make a list of the next time step of B, so I have A[0:-1], B[0:-1] and C[1:])\n\nI know that A + or - B is always == to at least 1 integer in C.\n\nExample:\n\nif A = [2,3,4] and B = [5,2,1] I know that at least 1 of C will be a 7,8,1,3, or any other combination of those numbers + or - each other.\n\nHow can I make a simple function to do this operation and check if it IS == to C and then store it as a key in a dictionary? The goal would be to create a neural network once this is complete to evaluate new data.\n"
'I trained a neural network for sign language recognition. Here\'s my output layer  model.add(Dense(units=26,activation="softmax"))\nNow I\'m getting probability for all 26 alphabets. Somehow I\'m getting 99% accuracy when I test this model accuracy = model.evaluate(x=test_X,y=test_Y,batch_size=32). I\'m new at this. I can\'t understand how this code works and I\'m missing something major here. How to get a 1D list having just the predicted alphabet in it? \n'
'First, the "clicks" dataframe is passed through the function "get_data_splits" which return train, valid and test. Further, it is passed through the train model.\n\nclicks = clicks.join(interactions)\nprint("Score with interactions")\ntrain, valid, test = get_data_splits(clicks)\n_ = train_model(train, valid)\n\n\nClue: The first feature you\'ll be creating is the number of events from the same IP in the last six hours. It\'s likely that someone who is visiting often will download the app. Implement a function count_past_events that takes a Series of click times (timestamps) and returns another Series with the number of events in the last hour.\nBut I\'m unable to comprehend these lines of code.\n\ndef count_past_events(series, time_window=\'6H\'):\n    series = pd.Series(series.index, index=series)\n    past_events = series.rolling(time_window).count() - 1\n    return past_events\n\n'
"hello i am a beginner and stuck in this weird error i am getting\n\ni have 4 dataframes which i want to concatenate \n\nthese are 4 DF i have \n'beds_mntd_bystates', 'beds_mntd_mod', 'beds_mntd_gov', 'beds_mntd_insure' \n\nevery dataframe has states in it and count of total no. of hospital \nand total no.of beds there the shape of each DF is also same\n            Total_No_of_Hospital    Total_No_of_Beds\nStates  \nAndhra Pradesh   5.0                 345.0\nAssam            1.0                 75.0\nBihar            3.0                 50.0 \n\n\ni made them in a list using\nframes_collection=[]\nframes_collection.extend(values for values,name in locals().items() if values.startswith('beds_mntd'))\n\n\nthe main problem is when i use concat to join all frames\ni tried\n\nframe_df=pd.concat(frames_collection,axis=1)\n\n\ni am getting a weird error \n\nType Error:cannot concatenate object of type '&lt;class 'str'&gt;'; only Series and DataFrame objs are valid\n\n\ni dont know how to deal with this\n\nalso  datatype of each of DF is 'object' and columns are 'float' type\n"
"I have a column such as \n\n  time\n1508089361\n1508065388\n1508011482\n\n\nI want to convert this to a local timestamp setting the timezone for a particular region, so I use the code below:\n\ndf['time'] = pd.to_datetime(df['time'], unit='s').dt.tz_localize('UTC').dt.tz_convert('Europe/Berlin')\n\n\nIt returns me a column with values as below:\n\n          time\n2017-10-04 16:17:34+02:00   \n2017-10-04 14:17:34+02:00   \n2017-10-04 08:17:34+02:00   \n\n\nIS there any way can I remove the +02:00 so I can just have a normal timestamp?\nthe column type is a datetime64[ns, Europe/Berlin]\n\nI want to join two dfs on these times but cannot since the other df has these times already in normal datetime timestamp format such as:\n\n       Time\n2017-10-09 00:00:00\n2017-10-09 00:00:00\n2017-10-09 00:00:00\n2017-10-09 00:00:00\n\n\nThanks!\n"
'I\'m a beginner in Python Data Science and I’m doing clickstream analysis. My file size is too large- around 33 million rows. I\'m running this script I made to find the session duration. I\'m printing i to find the progress of the operation. However close to 12 hours have passed and i has only reached till 400 000. It has close to 9 million sessions. With this speed it will take close to 270 hours(11 days). I need to optimize this to reduce the time.\n\nHere is the dataset:\n\n\n             Sid                    Tstamp     Itemid\n0              1  2014-04-07T10:54:09.868Z  214536500\n1              1  2014-04-07T10:54:46.998Z  214536506\n2              1  2014-04-07T10:57:00.306Z  214577561\n3              2  2014-04-07T13:56:37.614Z  214662742\n4              2  2014-04-07T13:57:19.373Z  214662742\n5              2  2014-04-07T13:58:37.446Z  214825110\n6              2  2014-04-07T13:59:50.710Z  214757390\n7              2  2014-04-07T14:00:38.247Z  214757407\n8              2  2014-04-07T14:02:36.889Z  214551617\n\n\nHere is my code. I think the for loop is slowing down the operation. For session duration I\'m first finding max and min timestamps in every session. Then in for loop I\'m converting them to seconds.microseconds and then final subtraction with simultaneous update in the Tstamp column.\n\nadi = "H:/excelfiles/clicks3.csv"\nk = pandas.read_csv(adi)\n\nk.columns=[\'Sid\',\'Tstamp\',\'Itemid\']\n\n#Dropping Redundant Columns\nk.drop(k.columns[2],axis=1,inplace=True)\n\n#Stores max timestamp in amax\nidx=k.groupby([\'Sid\'])[\'Tstamp\'].transform(max) == k[\'Tstamp\']\namax=k[idx].set_index(\'Sid\')\n\n#Stores min timestamp in amin\nidy=k.groupby([\'Sid\'])[\'Tstamp\'].transform(min) == k[\'Tstamp\']\namin=k[idy].set_index(\'Sid\')\n\ni=0\nfor temp1,temp2,temp3 in zip(amax[\'Tstamp\'],amax.index,amin[\'Tstamp\']):\n    sv1= datetime.datetime.strptime(temp1, "%Y-%m-%dT%H:%M:%S.%fZ")\n    sv2= datetime.datetime.strptime(temp3, "%Y-%m-%dT%H:%M:%S.%fZ")\n    if(i%1000==0):    \n        print i\n    d1=time.mktime(sv1.timetuple()) + (sv1.microsecond / 1000000.0)\n    d2=time.mktime(sv2.timetuple()) + (sv2.microsecond / 1000000.0)\n    amax.loc[temp2,\'duration\']= (d1-d2)/60   \n    i=i+1\n\n#amax stores the final session duration\namax=amax.reset_index()\n\n\nWhat could be done to optimize this code.\n\nEDIT 1: Removed the microseconds part.\n'
"I have a for loop to iterate through every row of a data frame. Now, when certain expressions match, I want to add the matching row to a new data frame.\n\nThis is what I have done:\n\ndfWithSand3 = pd.DataFrame()\nfor index, row in df.iterrows():\n    if row['embarked'] == 'S' and row['pclass'] == 3:\n        dfWithSand3.append(row)\nprint(dfWithSand3)\n\n\nHere, pd is pandas and df is the data frame.\n\nWhenever the expression below is True I want to add the row to the data frame dfWithSand3.\n\nif row['embarked'] == 'S' and row['pclass'] == 3:\n\n\nRight now the value of the data frame dfWithSand3 is:\n\nEmpty DataFrame\nColumns: []\nIndex: []\n\n\nBelow is what a possible row value could look like\n\nUnnamed: 0     600\nsurvived         0\npclass           3\nsex           male\nage             42\nsibsp            0\nparch            0\nfare          7.55\nembarked         S\n\n\nI don't know if row represents a data frame or not. \n\nAlso, how would I add row to the new data frame I have created?\n"
'I am learning python on dataquest and trying to solve this problem.\n\nWrite a function that extracts the same values across years and calculates the differences between consecutive values to show if number of births is increasing or decreasing.\nFor example, how did the number of births on Saturday change each year between 1994 and 2003?\n\nI am trying to solve this in Jupyter. I am new to python and I am not sure how to get started on this problem.\n\nThe data input is here in CSV format : US births\n\n    # coding: utf-8\n\n# In[1]:\n\ntext_file = open("US_births_1994-2003_CDC_NCHS.csv", "r").read()\nline_split = text_file.split("\\n")\nline_split\n\n\n# In[2]:\n\ndef read_csv(filename):\n    text = open(filename, "r").read()\n    string_list = text.split(\'\\n\')[1:]\n    final_list = []\n    for row in string_list:\n        int_fields = []\n        string_fields = row.split(\',\')\n        for item in string_fields:\n            int_fields.append(int(item))\n        final_list.append(int_fields)\n    return(final_list)\n\ncdc_list = read_csv("US_births_1994-2003_CDC_NCHS.csv")\ncdc_list[0:10]\n\n\n# In[3]:\n\ndef months_births(filename):\n    births_per_month = dict()\n    for item in filename:\n        num_month = int(item[1])\n        num_births = int(item[4])\n        if num_month in births_per_month:\n            births_per_month[num_month] += num_births\n        else:\n            births_per_month[num_month] = num_births\n    return(births_per_month)\n\ncdc_month_births = months_births(cdc_list)\ncdc_month_births\n\n\n# In[4]:\n\ndef dow_births(filename):\n    sum_births = dict()\n    for item in filename:\n        day_week = int(item[3])\n        day_birth = int(item[4])\n        if day_week in sum_births:\n            sum_births[day_week] += day_birth\n        else:\n            sum_births[day_week] = day_birth\n    return(sum_births)\n\ncdc_day_births = dow_births(cdc_list)\ncdc_day_births\n\n\n# In[30]:\n\ndef calc_counts(data, column):\n    sum_dict = dict()\n    for item in data:\n        col_num = item[column]\n        birth_count = int(item[4])\n        if col_num in sum_dict:\n            sum_dict[col_num] += birth_count\n        else:\n            sum_dict[col_num] = birth_count\n    return(sum_dict)\n\ncdc_year_births = calc_counts(cdc_list, 0)\ncdc_month_births = calc_counts(cdc_list, 1)\ncdc_dom_births = calc_counts(cdc_list, 2)\ncdc_dow_births = calc_counts(cdc_list, 3)\n\n\n# In[31]:\n\ncdc_year_births\n\n\n# In[32]:\n\ncdc_month_births\n\n\n# In[33]:\n\ncdc_dom_births\n\n\n# In[34]:\n\ncdc_dow_births\n\n\n# In[6]:\n\ndef min_max_dict(filename, request):\n    if request == "max":\n        max_value = max(filename, key=filename.get)\n        return(filename[max_value])\n    else:\n        min_value = min(filename, key=filename.get)\n        return(filename[min_value])\n\nmax_value = min_max_dict(cdc_year_births, "max")\nprint("max: ",max_value)\nmin_value = min_max_dict(cdc_year_births, "min")\nprint("min: ",min_value)\n\n\n# In[36]:\n\ndef diff_in_values(filename):\n    final_dict = dict()\n    seen_set = set()\n    unique_values = list()\n    for item in filename:\n        year_count = int(item[0])\n        birth_count = int(item[4])\n        day_of_week = int(item[3])\n\n        if birth_count not in seen_set:\n            unique_values.append(birth_count)\n            seen_set.add(birth_count)\n\n    return(seen_set)\n\nresult = diff_in_values(cdc_list)\nresult\n\n'
"I am trying to add more than two timestamp values and I expect to see output in minutes/seconds. How can I add two timestamps? I basically want to do: '1995-07-01 00:00:01' + '1995-07-01 00:05:06' and see if total time&gt;=60minutes.\nI tried this code: df['timestamp'][0]+df['timestamp'][1]. I referred this post but my timestamps are coming from dataframe.\nHead of my dataframe column looks like this:\n\n\r\n\r\n0   1995-07-01 00:00:01\r\n1   1995-07-01 00:00:06\r\n2   1995-07-01 00:00:09\r\n3   1995-07-01 00:00:09\r\n4   1995-07-01 00:00:09\r\nName: timestamp, dtype: datetime64[ns]\r\n\r\n\r\n\n\nI am getting this error: \nTypeError: unsupported operand type(s) for +: 'Timestamp' and 'Timestamp'\n\n"
'I have some data that looks like this\n\nColumnName\nvalue 1, value 2\nvalue 1, value 3\n\n\nI would like to eliminate this column, and instead replace with with a column for each value, like so.\n\nvalue 1 value 2 value 3\n1       1       0\n1       0       1\n\n\nIs there a library/function call available to take care of this for me? I have seen in R there is a close equivalent from the splitstackshape library called Csplit_e\n'
'How do you convert a nested json of multiple arrays into csv tabular structure using python?\nsee the complete json here\n\nCODE : \n\nimport json\nimport csv\n\nf = open(\'cost_drilldown_data.json\')\n\ndata = json.load(f)\ns=csv.writer(open(\'costdrillwittime_storage4.csv\',\'w\'))\ns.writerow(["filter","cost","value","cost","subvalue","cost","res_id","cost","tot_cost","metdata"])\ni=0\nd = []\nfor breakdown in data[\'breakdown\']:\n    #for time in data[\'time\']:\n        for storage in data[\'storage\']:\n             if storage not in d:\n\n                for values in breakdown[\'values\']:\n\n                    if \'subvalues\' in values:\n                         for subvalues in values[\'subvalues\']:\n                    #for i in range(0,len(data)):\n                            s.writerow([breakdown[\'filter\'],breakdown["cost"],values[\'value\'],values[\'cost\'],\n                            subvalues["subvalue"],subvalues["cost"],storage[\'resource_id\'],storage[\'cost\'],\n                            storage[\'total_cost\'],storage[\'metadata\']])\n\n                    else :\n                         s.writerow([breakdown[\'filter\'],"","",values[\'value\'],values[\'cost\']])\n\n\nACTUAL OUTPUT OF THE ABOVE CODE:\n\n\n  filter,cost,value,cost,subvalue,cost,res_id,cost,tot_cost,metdata\n  \n  tags,5517.734,Name,462.62,BizOps-VM-20,227.576,i-048e0bfa74ac9cf78,25.047,25.801000000000002,{u\'name\': u\'BizOps0424001\'}\n  \n  tags,5517.734,Name,462.62,,70.358,i-048e0bfa74ac9cf78,25.047,25.801000000000002,{u\'name\':\n  u\'BizOps0424001\'}\n  \n  tags,5517.734,Name,462.62,BizOps01,60.188,i-048e0bfa74ac9cf78,25.047,25.801000000000002,{u\'name\':\n  u\'BizOps0424001\'}...\n\n\nits repeating the values of meta data again and again and also repeating the NAME after wards.\n\nhow to make print the values only once and differnt values only as per the counts.\n\nsee the full csv file here\n\n\n  TOTALLY EDITED CODE AFTER ANKUSH\'s HELP:\n\n\nimport json\nimport csv\n\nf = open(\'cost_drilldown_data.json\')\ndata = json.load(f)\n\n\ns=csv.writer(open(\'googletry1.csv\',\'w\'))\n\n\nbreakdown = data[\'breakdown\']\nstorage = data[\'storage\']\nfilter_list = []            #first column\nfilter_cost_list = []       #second column\nvalue_list = []             #third column\ncost_list = []              #fourth column\nsubValue_list = []          #fifth col\nsubvalueCost_list = []      #sixth col\nresID_list = []             #seventh col\nstorageCost_list = []       #eighth col\ntotalCost_list = []         #ninth col\nmetadata_list = []          #tenth col\n\ns.writerow(["filter_list","filter_cost_list","value_list","cost_list","subValue_list","subvalueCost_list","resID_list","storageCost_list","totalCost_list","metadata_list"])\n\nfor eachBreakdown in breakdown:\n    filter_list.append(eachBreakdown[\'filter\'])\n    filter_cost_list.append([\'cost\'])\n    valuesArr = eachBreakdown[\'values\']\n    for eachValues in valuesArr:\n        value_list.append(eachValues[\'value\'])\n        cost_list.append(eachValues[\'cost\'])\n        if \'subvalues\' in eachValues:\n            subValueArr = eachValues[\'subvalues\']\n        for eachSubValueArr in subValueArr:\n            subValue_list.append(eachSubValueArr[\'subvalue\'])\n            subvalueCost_list.append(eachSubValueArr[\'cost\'])\n\n                    #s.writerow([breakdown[\'filter\'],breakdown["cost"],values[\'value\'],values[\'cost\'],\n                    #subvalues["subvalue"],subvalues["cost"],storage[\'resource_id\'],storage[\'cost\'],\n                    #storage[\'total_cost\'],storage[\'metadata\'],storage[\'volume_cost\'],storage[\'provider\']])\n\n            s.writerow([eachBreakdown[\'filter\'],eachBreakdown[\'cost\'],eachValues[\'value\'],eachValues[\'cost\'],\n            eachSubValueArr[\'subvalue\'],eachSubValueArr[\'cost\']])\n\nfor eachStorage in storage:\n    resID_list.append(eachStorage[\'resource_id\'])\n    storageCost_list.append(eachStorage[\'cost\'])\n    totalCost_list.append(eachStorage[\'total_cost\'])\n    metadata_list.append([eachStorage[\'metadata\']])\n\n\n    s.writerow([eachStorage[\'resource_id\']])\n\n\nnow i am getting challenge in printing the resource id to the side of the subvalue cost: I am getting csv file as :\nenter code here\n\n'
'I wrote a python script which automatically categorizes set of articles. I want to integrate it into my rails app.Please let me know if anybody has the approach.Thanks in advance\n'
'How to perform standardizing on the data in GridSearchCV?\n\nHere is the code. I have no idea on how to do it.\n\nimport dataset\nimport warnings\nwarnings.filterwarnings("ignore")\n\nimport pandas as pd\ndataset = pd.read_excel(\'../dataset/dataset_experiment1.xlsx\')\nX = dataset.iloc[:,1:-1].values\ny = dataset.iloc[:,66].values\n\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nstdizer = StandardScaler()\n\nprint(\'===Grid Search===\')\n\nprint(\'logistic regression\')\nmodel = LogisticRegression()\nparameter_grid = {\'solver\': [\'newton-cg\', \'lbfgs\', \'liblinear\', \'sag\', \'saga\']}\ngrid_search = GridSearchCV(model, param_grid=parameter_grid, cv=kfold, scoring = scoring3)\ngrid_search.fit(X, y)\nprint(\'Best score: {}\'.format(grid_search.best_score_))\nprint(\'Best parameters: {}\'.format(grid_search.best_params_))\nprint(\'\\n\')\n\n\nUpdate \nThis is what I try to run but get the error:\n\nprint(\'logistic regression\')\nmodel = LogisticRegression()\npipeline = Pipeline([(\'scale\', StandardScaler()), (\'clf\', model)])\nparameter_grid = {\'solver\': [\'newton-cg\', \'lbfgs\', \'liblinear\', \'sag\', \'saga\']}\ngrid_search = GridSearchCV(pipeline, param_grid=parameter_grid, cv=kfold, scoring = scoring3)\ngrid_search.fit(X, y)\nprint(\'Best score: {}\'.format(grid_search.best_score_))\nprint(\'Best parameters: {}\'.format(grid_search.best_params_))\nprint(\'\\n\')\n\n'
"This is a sample what my dataframe looks like:\n\ncompany_name country_code state_code software finance commerce etc......\ngoogle       USA           CA          1        0          0\njimmy        GBR           unknown     0        0          1\n\n\nI would like to be able to group the industry of a company with its state code. For example I would like to have the total number of software companies in a state etc. (e.g. 200 software companies in CA, 100 finance companies in NY).\n\nI am currently just counting the number of total companies in each state using:\n\n usa_df['state_code'].value_counts()\n\n\nBut I can't figure out how to group the number of each type of industry in each individual state. \n"
'I\'m pre-processing trump-hillary debate script text to create 3 lists which will including each 3 person\'s saying.\nEntire script is 1046 lists\n\nsome of text are as following  \n\nfor i in range(len(loaded_txt)):\n    print("load_text[i]",load_text[i])\n\n\n\n\nloaded_txt[i] TRUMP: No, it\'s going to totally help you. And one thing we have to do: Repeal and replace the disaster known as Obamacare. It\'s destroying our country. It\'s destroying our businesses, our small business and our big businesses. We have to repeal and replace Obamacare.\n\nloaded_txt[i] \n\nloaded_txt[i] You take a look at the kind of numbers that that will cost us in the year \'17, it is a disaster. If we don\'t repeal and replace -- now, it\'s probably going to die of its own weight. But Obamacare has to go. It\'s -- the premiums are going up 60 percent, 70 percent, 80 percent. Next year they\'re going to go up over 100 percent.\n\nloaded_txt[i] \n\nloaded_txt[i] And I\'m really glad that the premiums have started -- at least the people see what\'s happening, because she wants to keep Obamacare and she wants to make it even worse, and it can\'t get any worse. Bad health care at the most expensive price. We have to repeal and replace Obamacare.\n\nloaded_txt[i] \n\nloaded_txt[i] WALLACE: And, Secretary Clinton, same question, because at this point, Social Security and Medicare are going to run out, the trust funds are going to run out of money. Will you as president entertain -- will you consider a grand bargain, a deal that includes both tax increases and benefit cuts to try to save both programs?\n\n\n\nI tried to append list into TRUMP_script_list=[], if it has "TRUMP:" in list like this\n\nTRUMP_script_list=[]\n\nfor i in range(len(loaded_txt)):\n    if "TRUMP:" in loaded_txt[i]:\n        TRUMP_script_list.append(loaded_txt[i])\n\n\nBut the problem is list without name.\nBut text without name should be trump\'s saying if it is under text with name of trump, UNTIL list meets texts including names not trump(wallace or clinton)\n\nI tried "while" loop which will be terminated if list would contain other names(wallace, clinton). But failed to implement.\n\nHow can I implement this algorithm or any other good idea?\n'
'I am doing sentiment analysis on tweets. Most of the tweets contains short words  and i want to replace them as original/full word.\n\nSuppose that tweet is:\n\nI was wid Ali.\n\n\nI want to convert:\n\nwid -&gt; with\n\n\nSimilarly\n\nwud -&gt; would\nu -&gt; you\nr -&gt; are\n\n\ni have 6000 tweets in which there are lots of short words.\nHow i can replace them ? is there any library available in python for this task? or any  dictionary of shorts words available online? \n\ni read answer of Replace appostrophe/short words in python Question but it provides dictionary of appostrophe only.\n\nCurrently i am using NLTK but this task is not possible with NLTK.\n'
"I have a data frame which consists many columns and one of them column called SourceTechAttributes which has valuable attributeName and attribute Value such as\n\n    df['SourceTechAttributes'][0]\n    'DropFrame: True, Duration: 4874.1359333333333333333333333, FieldDominance: Upper Field First, FrameRate: 29.97, Height: 1080, MediaFormat: 912, NumberOfAudioChannels: 8, NumberOfAudioTracks: 8, ScanType: Interlaced, StartSmpte: 00:59:59;26, ViewportDisplayFormat: Anamorphic, Width: 1920'\n0    DropFrame: True, Duration: 4874.13593333333333...\n1    ActionType: CG, DropFrame: True, Duration: 129...\n2    DropFrame: True, Duration: 4874.13593333333333...\n3    DropFrame: True, Duration: 4874.13593333333333...\n4    ActionType: CG, DropFrame: True, Duration: 129...\n5    ActionType: CG, DropFrame: True, Duration: 129...\nName: SourceTechAttributes, dtype: object\n\n\nThis column key and value also changes its position, \nI want to parse that column and create new seven column such as below\n\n\nI can do in pandas one by one such as \n\ndf['m']=df['SourceTechAttributes'][0].split(',')[0]\n\n\nwhich gives me a results of parse of first comma separated such as \n\ndf['m']\n0        DropFrame: True\n1        DropFrame: True\n2        DropFrame: True\n3        DropFrame: True\n\n\nthen again parse colon separated and take the last part and give the column name as df['DropFrame'] \n\ndf['DropFrame']=df['m'][0].split(':')[1]\ndf['DropFrame']\n\n0         True\n1         True\n2         True\n3         True\n\n\nBut this process gives wrong because sometime it does not get what i want to as because of some rows the attributes and values are many and sometime few. Can Anyone please help me on that matter to create a function that will take care all of this and I can achieve my goal. Thanks in advance. \n"
"Trying to understand the sklearn.decomposition.PCA API and it's giving me a hard time.\nI divided my data (40features x 10 samples) into training (39 samples) and testing subsets (1 sample).\n\nI commented the code with what I think/ guess is happening.\n\nX_train, X_test = X_all[ix1], X_all[ix2]\n\n# Instantiate PCA\npca = PCA(n_components=n_comps)\n\n# train the model\nX_train_reduced = pca.fit_transform(X_train)\n\n# reduce X_test\nX_test_reduced = pca.transform(X_test)\n\n# invert X_test back to original number of components\nX_test_inv = pca.inverse_transform(X_test)  # &lt;--- ERROR\n\n....\n[this would continue with checking errors bassed on n_comps]\n\n\nError on the indicated line states the following:\nshapes (1,40) and (n_comps,40) not aligned: 40 (dim 1) != n_comps (dim 0)\n\nEDIT:\nDimensions of the variables:\nX_test = 1 x 40\nX_train = 9 x 40\nX_test_reduced = 9 x n_comps  \n\nHow should this actually be done?\n"
"I have a bunch of samples with shape (1, 104). All samples are integers( +ve, -ve and 0) which are being used in the imshow function of matplotlib. Below is the function I've created to display them as images.\n\ndef show_as_image(sample):\n    bitmap = sample.reshape((13, 8))\n    plt.figure()\n    # this line needs changes.\n    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n    plt.colorbar()\n    plt.show()\n\n\nI need to color code the positive and negative values from the sample. PS: Take 0 as positive.\nHow do I change my code?\n"
"How can I insert this list of data into a pandas DataFrame\n\norgdata = ['somestring', data[2], data[3], data[4], data[8], data[9], data[10], data[14], data[15], data[16], data[20], data[21], data[22], data[26], data[27], data[28], data[32], data[33], data[34], data[38], data[39], data[40], data[44], data[45], data[46] ]\n\n\nwhere 'data' is another list of data out of which i parse specific data.\n\nI have a list of columns names which is also derived from the 'data' list\n\ncolnames = ['USN', data[0], data[6], data[12], data[18], data[24], data[30], data[36], data[42]]\n\n\nNow I need to have three subcolumns under each column, so i do this\n\ncols = pd.MultiIndex.from_product([colnames, ['IA', 'EX', 'Total']])\n\n\nBut when i try to insert this list of 'data' into a DataFrame like this\n\ndf = pd.DataFrame(orgdata, columns=cols)\n\n\nI get the following error\n\nValueError: Wrong number of items passed 1, placement implies 27\n\n\nAlso i get this error\n\nValueError: Shape of passed values is (1, 25), indices imply (27, 25)\n\n\nWhat am I doing wrong? The documentation provided online doesn't give much insight to this topic.\n\nAre there any other ways to work around this? Any help provided is Appreciated.\n\nEdit:\n\nFirst I make a list of 'data' from the response of a request I made to. Here's an instance of the data i received from the response.\n\ndata = ['15EC41', 'LIC', '40', '60', 'P']\n\n\nThis is the sort of data i'm working  with.\n"
'I\'ve been given the python script where matplotlib is used , when running the script it opens the window and display graph.\nits working perfectly on my laptop. But this error occurs when I upload the file on AWS elasticbeanstalk.\nI successfully reproduce this error in my laptop by using\n\nimport matplotlib\nmatplotlib.use(\'agg\')\n\n\nthe error raised from\n\nFile "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/dates.py", line 1001, in viewlim_to_dt\n.format(vmin))\n\n\nhere is the code in that file\n\ndef viewlim_to_dt(self):\n    """\n    Converts the view interval to datetime objects.\n    """\n    print(self.axis)\n    vmin, vmax = self.axis.get_view_interval()\n    if vmin &gt; vmax:\n        vmin, vmax = vmax, vmin\n    if vmin &lt; 1:\n        raise ValueError(\'view limit minimum {} is less than 1 and \'\n                         \'is an invalid Matplotlib date value. This \'\n                         \'often happens if you pass a non-datetime \'\n                         \'value to an axis that has datetime units\'\n                         .format(vmin))\n    return num2date(vmin, self.tz), num2date(vmax, self.tz)\n\n\nprint(self.axis) shows this and then raises error\n\nXAxis(100.000000,373.620690)\n\n\nwhen I remove the line matplotlib.use(\'agg\') , print(self.axis) shows\n\n\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(200.000000,110.000000)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(59.111111,106.555556)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(225.000000,88.000000)\n  XAxis(122.930556,106.555556)\n  XAxis(122.930556,106.555556)\n  XAxis(122.930556,106.555556)\n  XAxis(122.930556,106.555556)\n\n\nCan anyone helps me out, I don\'t know what\'s going on.\nI\'m using python3\nhere is my full import\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\nimport datetime\nfrom matplotlib.dates import DateFormatter\nfrom matplotlib.dates import HourLocator\n\n\nhere is the full error trace\n\nFile "/Users/abc/project/pythonfile.py", line 301, in plot_now\n    facecolor=\'tab:purple\', interpolate=True, alpha = 0.3, label=\'my_label\')\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/__init__.py", line 1785, in inner\n    return func(ax, *args, **kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_axes.py", line 5021, in fill_between\n    self._process_unit_info(xdata=x, ydata=y1, kwargs=kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py", line 2111, in _process_unit_info\n    kwargs = _process_single_axis(xdata, self.xaxis, \'xunits\', kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py", line 2094, in _process_single_axis\n    axis.update_units(data)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py", line 1478, in update_units\n    self._update_axisinfo()\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py", line 1496, in _update_axisinfo\n    self.set_major_locator(info.majloc)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py", line 1617, in set_major_locator\n    self.stale = True\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 236, in stale\n    self.stale_callback(self, val)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 63, in _stale_axes_callback\n    self.axes.stale = val\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 236, in stale\n    self.stale_callback(self, val)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py", line 57, in _stale_figure_callback\n    self.figure.stale = val\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 236, in stale\n    self.stale_callback(self, val)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/pyplot.py", line 568, in _auto_draw_if_interactive\n    fig.canvas.draw_idle()\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backend_bases.py", line 1899, in draw_idle\n    self.draw(*args, **kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py", line 402, in draw\n    self.figure.draw(self.renderer)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 50, in draw_wrapper\n    return draw(artist, renderer, *args, **kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/figure.py", line 1652, in draw\n    renderer, self, artists, self.suppressComposite)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py", line 138, in _draw_list_compositing_images\n    a.draw(renderer)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 50, in draw_wrapper\n    return draw(artist, renderer, *args, **kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axes/_base.py", line 2604, in draw\n    mimage._draw_list_compositing_images(renderer, self, artists)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/image.py", line 138, in _draw_list_compositing_images\n    a.draw(renderer)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/artist.py", line 50, in draw_wrapper\n    return draw(artist, renderer, *args, **kwargs)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py", line 1185, in draw\n    ticks_to_draw = self._update_ticks(renderer)\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py", line 1023, in _update_ticks\n    tick_tups = list(self.iter_ticks())  # iter_ticks calls the locator\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/axis.py", line 967, in iter_ticks\n    majorLocs = self.major.locator()\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/dates.py", line 1230, in __call__\n    self.refresh()\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/dates.py", line 1250, in refresh\n    dmin, dmax = self.viewlim_to_dt()\n  File "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/matplotlib/dates.py", line 1001, in viewlim_to_dt\n    .format(vmin))\nValueError: view limit minimum 0.0 is less than 1 and is an invalid Matplotlib date value. This often happens if you pass a non-datetime value to an axis that has datetime units\n'
'print (amazon.shape)\nprint (amazon.drop(amazon[(amazon["Id"] &gt; 150492) &amp; (amazon["Id"] &lt; 150530 )].index).shape)\nprint (amazon.shape)\n\n\noutput :\n\n(525814, 10)\n(525782, 10)\n(525814, 10)\n\n\nwhy is it not deleting 32 rows from main dataframe .?\n'
'Getting below error:\n\n\n  SyntaxError: invalid syntax error on this line of the code ->\n  parsed_json = kstream.map(lambda (k,v): json.loads()) arrow indicating\n  (>k,a), link to the code is \n  https://github.com/patilankita79/Location-based-Restaurants-Recommendation-System/blob/master/BigDataProject/Consumer.py\n\n'
'Here is the equation. Example\n\nNumbers     | Date      | Mean\n1000        |12/1/2018  | 1\n1002        |12/2/2018  | 0\n1003        |12/3/2018  | 0.5\n 0          |12/4/2018  | 0.6\n 0          |12/5/2018  | 0.4\n 0          |12/6/2018  |0.1\n 0          |12/7/2018  | -0.7\n 0          |12/8/2018  | 0.2\n 0          |12/9/2018  | -0.1\n\n\nHere what I want \n\n|Numbers | Date      | Mean | Multiplication |\n| ------ |-----------|------|----------------|\n|1000    | 12/1/2018 | 1    | 1000           |\n|1002    | 12/2/2018 | 0    | 0*1000= 0      |\n|1003    | 12/3/2018 | 0.5  | 0.5*1002=501   |\n|0       | 12/4/2018 | 0.6  | 1003*0.6=601.8 |\n|0       | 12/5/208  | 0.4  | 601.8*0.4\n|0       | 12/6/2018 | 0.1  | 601.8*0.4*0.1  |\n|0       |12/7/2018  | -0.7 |601.8*0.4*0.1*-0.7| \n 0       |12/8/2018  | 0.2  |601.8*0.4*0.1*-0.7*0.2\n 0       |12/9/2018  | -0.1 |601.8*0.4*0.1*-0.7*0.2*-0.1\n\n\nthe data is already in the data-frame and I am using pandas functions\n'
'i am trying to build an application .Its working fine to genrate csv and pdf file .Now i want to generate an zip file which will contain both pdf and csv output how to do that?\n\nmy code:\n\ndef save_analysis():\n    file = asksaveasfile(mode=\'w\', defaultextension=".csv", filetypes=[(\'CSV Files\',\'*.csv\')])\n    print(file.name, "save file name")\n    predict.generateCSV(adf,file.name)\n    root.update()\n    messagebox.showinfo("Saved Analysis", "Successfully Generated and Saved File to :\\n" + file.name)\n\ndef view_stats():\n    file = asksaveasfile(mode=\'w\', defaultextension=".pdf", filetypes=[(\'Pdf Files\',\'*.pdf\')])\n    print(file.name, "save file name")\n    visual.show_visuals(adf,file.name)\n    root.update()\n    messagebox.showinfo("Visualization Saved", "Successfully Generated and Saved File to :\\n" + file.name)\n\n'
"i'm trying to plot distribution plot using seaborn but not getting the gridlines in the plot it is plain.\n\nimport seaborn as sns\n%matplotlib inline\n\nsns.distplot(df_0['temp'], bins=20)\n\nBelow is the image for reference\n\n  [1]: https://i.stack.imgur.com/q7z4W.png\n\n"
'I have a .csv file that includes 2 columns, begin_time &amp; end_time which contain records in this format: 2013-02-12 16:21:48 .\nThey start from 2013 till 2017.\nMy goal is to convert all of them to only 16:21:48 part, so I\'ll be able to map all of the datasets into a [00:00:00 - 23:59:59] range to monitor them through 24 hours of the day to check abnormal events.\nSo I want to remove the yy:mm:dd part of the record, and only keep the hh:mm:ss part.\n\nThese are the datasets of smart home events and I\'m trying to discover the activities and the abnormal changes among them.\n\nI have tried datetime.strftime(time_records, format="%H:%M:%S") and it returns str, but when I try to convert the str to pandas.timestamps, it brings back the yy:mm:dd\n\nI expect the 2013-02-12 16:21:48 to be 16:21:48 in timestamp or datetime format, so I\'ll be able to convert them to UNIX timestamp format.\n\nThanks in advance\n'
"I am trying to figure out the code to program a function called get_cartesian.\n\nHere is a screenshot of the problem I am looking to solve:\n\nCreate get_cartesian function\n\nHere is the question copied and pasted:\n\nExercise 3: get_cartesian\n\nExample\n\nThe function get_cartesian does a cartesian product of an RDD with itself and returns an RDD with DISTINCT pairs of points.\n\nInput: An RDD containing the given list of points\n\nOutput: An RDD containing The cartesian product of the RDD with itself\n\nExample Code\n\ntest_rdd = sc.parallelize([(1,0), (2,0), (3,0)])\nget_cartesian(test_rdd).collect()\nExample Output\n\n\n[((1, 0), (2, 0)), ((1, 0), (3, 0)), ((2, 0), (1, 0)), ((2, 0), (3, 0)), ((3, 0), (1, 0)), ((3, 0), (2, 0))]\n\nI tried with code but it was not correct, and I'm stuck on how to go forward. How can I solve this problem?\n"
'I tried asking a question earlier, but deleted so I could ask clearer and show what I am trying if it is close.\n\nMy sample df is \n\n    day         a   b\n  5/11/19       3   1\n  5/11/19       4   6\n  5/12/19       1   2\n  5/12/19       5   9\n  5/13/19      11   14\n\n\nI want to group by the day column and want to have a new df that calculates for both col a and col b the number of values that are &lt; 10 / all values for that day and column ( a or b) as a %. Since there are numerous rows for each day. \n\nI am trying something like \n\ndef calc_(group_df):\nresult = dict()\nresult["x"] = group_df[(group_df.x) &lt; 10] / len(group_df.x)\nresult["y"] = group_df[(group_df.y) &lt; 10] / len(group_df.y)\nreturn pd.Series(result, index=["x", "y"])\n\n\nand then \n\ndf.groupby("day").apply(calc)\n\n\nbut I am getting error of \n\nTypeError: Could not operate 163143 with block values unsupported operand type(s) for /: \'str\' and \'int\'\n\nAm I missing something?\n\nI want my final output \n\n     day         a   b\n  5/11/19       .3  .1\n  5/12/19       .5  .9\n  5/13/19       .1  .4\n\n\nI want it grouped by the business day and want each business day to show up only once in my final output. \n'
'when running the code have an error like that \n\n\nunsupported operand type(s) for *: \'float\' and \'NoneType\'\n\n\n\nif I delete this part \n\nTotal = Total* exp\n    Total = 1-Total\n    possition = possition + 1 \n\n\nhave an error like this \n\n\nIndexError: index 4 is out of bounds for axis 1 with size 4\n\n\n\nCode :\n\nimport random\n\ndef getsys():\n    row = \'\'\n    for i in range(0 , 8):\n        randintt = str(random.randint(0 , 4))\n        row += randintt\n    return row\n\n\ndef getx():\n    x = []\n    for i in range(0,14):\n        mysys = getsys()\n        x.append(mysys)\n\n    return x \n\ny = getx()\nprint (y)\n\n\nimport initialsys\nimport numpy as np\n\nR = np.array([[0.90 , 0.93,0.91 , 0.95],\n               [0.95 , 0.94, 0.93, 0],\n               [0.85 , 0.90 , 0.87 , 0.92],\n               [0.83 , 0.87 , 0.85 , 0 ],\n               [0.94 , 0.93 , 0.95 , 0],\n               [0.99 , 0.98 , 0.97 , 0.96],\n               [0.91 , 0.92 , 0.94 , 0],\n               [0.81 , 0.90 , 0.91 , 0],\n               [0.97 , 0.99 , 0.96 , 0.91],\n               [0.83 , 0.85 , 0.90 , 0],\n               [0.94 , 0.95 , 0.96 , 0],\n               [0.79 , 0.82 , 0.85 , 0.90],\n               [0.98 , 0.99 , 0.97 , 0],\n               [0.85 , 0.92 , 0.95 , 0.99]\n              ])\n\ndef expression(r ,possition , char ):\n    exp = 1-r[possition , int(char)]\n\n\nx = initialsys.getx()\npossition = 0\nTotal = 1\nTotal = float(Total)\nchar = ""\nfor row in x :\n    for char in row :\n        if char!= 0 :\n            exp = expression(R , possition , char)\n            Total = Total* exp\n    Total = 1-Total\n    possition = possition + 1 \n\n'
"I have the following dataframe:\n\nPIC   Label   EncodedPixels\npic1  fish    True\npic1  flower  True\npic1  gravel  False\npic1  sugar   False\npic2  fish    True\npic2  flower  True\n\n\nWhat i want to do is this:\n\nfor every EQUAL value in PIC, count the labels that coexist (being the true value).\n For instance, sow the number of times that fish and flower coexist for every pic.\n\nI can count each individual value in the following way: \n\ndf.loc[ (df['Pixels'] == True ) &amp; (df['Label'] == 'Sugar') ])\n\n\nThe expected output is the number of combinations for each pic. For instance, in pic1 fish and flower are both true in EncodedPixels, so the output would be 2\nHow can i do the rest?\n"
"I have a data frame like the one below.\n\namplitude   -13.125 |-13.125 |-11.875 |-11.875 |-11.25  |-11.25\nduration -----------|--------|--------|--------|--------|--------\n1           NaN     |NaN     |NaN     |NaN     |NaN     |NaN\n2           NaN     |0.008032|NaN     |NaN     |NaN     |NaN\n3           0.004016|NaN     |NaN     |NaN     |0.004016|0.004016\n4           0.9     |NaN     |NaN     |NaN     |NaN     |NaN\n5           NaN     |NaN     |NaN     |NaN     |NaN     |NaN\n--------------------|--------|--------|--------|--------|--------\nsum         0.904016|0.008032|NaN     |NaN     |0.004016|0.004016\n\n\n\nHow do I find the value at the intersection of the row and column in the data frame? Also, I want calculate the density by dividing the value I found by the value in 'sum'.\nExample:\n\nduration        amplitude       density \n3               -13.125      0.004016/0.904016  \n2               -13.125      0.008032/0.008032\n... \n\n\n"
"I have the following pandas data set:\n\ndate, pair, value, fruit\n2019-11-15 09:35:33,EUR,10,BANANA\n2019-11-15 09:35:32,EUR,12,BANANA\n2019-11-15 09:35:31,EUR,21,APPLE\n2019-11-15 09:35:30,EUR,17,ORANGE\n2019-11-15 09:35:28,EUR,19,BANANA\n2019-11-14 09:58:05,EUR,37,APPLE\n2019-11-14 09:23:42,EUR,41,ORANGE\n2019-11-14 09:23:42,EUR,15,APPLE\n\n\nHow can I group and add the value field for the same fruit(s)?\n\nSo I get,\n\n[\n ['BANANA', 'APPLE', 'ORANGE'],\n [41, 73, 58]\n]\n\n\n41 Being the sum of all BANANA values,\n\n73 Being the sum of all APPLE values,\n\n58 Being the sum of all ORANGE values.\n\nThe intention is draw a bar chart.\n"
"I have a dataset like this:\n\nId|Sem|Grade|Rating|SUB\n1|2|A|3|sub1\n1|4|C|1|sub2\n2|2|B|2|sub1\n\n\nI want to form association rules for the above data and recommend sub1,sub2 to students. How do I do it?\nI tried:\n\nrecords=[]\nfor i in range(0,60):\n    records.append([str(df.values[i,j]) for j in range(0,5)])\nfrom apyori import apriori\nassosciation_rules=apriori(records,min_support=0.1)\nassosciation_results=list(assosciation_rules)\n\n\nThe output is not understandable.Is there any better way.The output is like this:\n\n[RelationRecord(items=frozenset({'0'}), support=0.3333333333333333, ordered_statistics=[OrderedStatistic(items_base=frozenset(), items_add=frozenset({'0'}), confidence=0.3333333333333333, lift=1.0)]),....\n\n"
"[\n\n\n\n\n  error :   oneHot = OneHotEncoder(categorical_features = [0]) \n  TypeError: init() got an unexpected keyword argument\n  'categorical_features'.\n\n\nI'm tring to encode first column\nhere's my code sample:\n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder \nlabelen_x = LabelEncoder() # string to numeric encoding object\nx[:,0]= labelen_x.fit_transform(x[:,0]) # replaces the string labels with numerics for ML algorithm to be able to work with it\noneHot = OneHotEncoder(categorical_features = [0])\nx = oneHot.fit_transform(x).toarray()\n\n"
"I'm new at NLP and I'm trying basic preprocessing steps while learning. I'm trying to separate punctuations at the start and end of words for embeddings. While doing that, I don't want to damage words like can't, I'm, etc. because I'm handling them separately.\n\ns = 'This is what I'm trying to do, but I can't figure out how.'\n\n\nDesired output:\n\ns_separated = 'This is what I'm trying to do , but I can't figure out how .'\n\n"
"I have recently been using python and I am doing a job using Foursquare and data obtained from wikipedia. I am trying to make a map with the following code:\n\n    venues_map = folium.Map(location=[latitude, longitude], zoom_start=13) # generate map centred of Ciutat Vella\n\n# add a red circle marker to represent the center of the neighborhoods \nfolium.vector_layers.CircleMarker(\n    ['lat','lng'],\n    radius=10,\n    color='red',\n    popup='Eixample',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(venues_map)\n\n# add the shops as blue circle markers\nfor lat, lng, label in zip(new_df.lat, new_df.lng, new_df.categories):\n    folium.vector_layers.CircleMarker(\n        [lat,lng],\n        radius=5,\n        color='blue',\n        popup=label,\n        fill = True,\n        fill_color='blue',\n        fill_opacity=0.6\n    ).add_to(venues_map)\n\n# display map\nvenues_map\n\n\nWhen executing the line I get the following error:\n\n    ---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/folium/utilities.py in validate_location(location)\n     58         try:\n---&gt; 59             float(coord)\n     60         except (TypeError, ValueError):\n\nValueError: could not convert string to float: 'lat'\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-27-7a61c4e1c46b&gt; in &lt;module&gt;\n     11     fill = True,\n     12     fill_color = 'red',\n---&gt; 13     fill_opacity = 0.6\n     14 ).add_to(venues_map)\n     15 \n\n/opt/conda/envs/Python36/lib/python3.6/site-packages/folium/vector_layers.py in __init__(self, location, radius, popup, tooltip, **kwargs)\n    303     def __init__(self, location, radius=10, popup=None, tooltip=None, **kwargs):\n    304         super(CircleMarker, self).__init__(location, popup=popup,\n--&gt; 305                                            tooltip=tooltip)\n    306         self._name = 'CircleMarker'\n    307         self.options = path_options(line=False, radius=radius, **kwargs)\n\n/opt/conda/envs/Python36/lib/python3.6/site-packages/folium/map.py in __init__(self, location, popup, tooltip, icon, draggable, **kwargs)\n    275         super(Marker, self).__init__()\n    276         self._name = 'Marker'\n--&gt; 277         self.location = validate_location(location)\n    278         self.options = parse_options(\n    279             draggable=draggable or None,\n\n/opt/conda/envs/Python36/lib/python3.6/site-packages/folium/utilities.py in validate_location(location)\n     61             raise ValueError('Location should consist of two numerical values, '\n     62                              'but {!r} of type {} is not convertible to float.'\n---&gt; 63                              .format(coord, type(coord)))\n     64         if math.isnan(float(coord)):\n     65             raise ValueError('Location values cannot contain NaNs.')\n\nValueError: Location should consist of two numerical values, but 'lat' of type &lt;class 'str'&gt; is not convertible to float.\n\n\nI have looked and the LAT column if it is a float, I don't know how to fix it and I can't move forward. I would appreciate any help.\n\nThe value of Lat comes from the following table, in which it shows the name of the neighborhoods of Barcelona and shows its latitude and longitude to be able to take from there the values to form the map:\n\nenter image description here\n"
'How to find final regression model equation including coefficients with all variables? is there any method? \n'
"I've been struggling for a while at a problem. Actually i want to make a new column in my dataframe which will give 0 for values of 3 columns are below a certain range and give 1 for values exceeding a threshold value. I'm attaching a image of my data head. In my data if value of Ir, Iy and Ib goes above 4 then 1 else 0.\nThanks in advance!\n\n\n"
"First I am applying the condition on a particular column and getting the desired result.\n\nFirst case\n\ndf[df['country'] == 'Zimbabwe']\n\n\nOutput\n\n    country continent   year    lifeExp pop gdpPercap\n1692    Zimbabwe    Africa  1952    48.451  3080907 406.884115\n1693    Zimbabwe    Africa  1957    50.469  3646340 518.764268\n1694    Zimbabwe    Africa  1962    52.358  4277736 527.272182\n1695    Zimbabwe    Africa  1967    53.995  4995432 569.795071\n1696    Zimbabwe    Africa  1972    55.635  5861135 799.362176\n1697    Zimbabwe    Africa  1977    57.674  6642107 685.587682\n1698    Zimbabwe    Africa  1982    60.363  7636524 788.855041\n1699    Zimbabwe    Africa  1987    62.351  9216418 706.157306\n1700    Zimbabwe    Africa  1992    60.377  10704340    693.420786\n1701    Zimbabwe    Africa  1997    46.809  11404948    792.449960\n1702    Zimbabwe    Africa  2002    39.989  11926563    672.038623\n1703    Zimbabwe    Africa  2007    43.487  12311143    469.709298\n\n\nSecond Case\n\ndf[df.iloc[:,[0]] == 'Zimbabwe']\n\n\nOutput\n\n    country continent   year    lifeExp pop gdpPercap\n0   NaN NaN NaN NaN NaN NaN\n1   NaN NaN NaN NaN NaN NaN\n2   NaN NaN NaN NaN NaN NaN\n3   NaN NaN NaN NaN NaN NaN\n4   NaN NaN NaN NaN NaN NaN\n... ... ... ... ... ... ...\n1699    Zimbabwe    NaN NaN NaN NaN NaN\n1700    Zimbabwe    NaN NaN NaN NaN NaN\n1701    Zimbabwe    NaN NaN NaN NaN NaN\n1702    Zimbabwe    NaN NaN NaN NaN NaN\n1703    Zimbabwe    NaN NaN NaN NaN NaN\n\n\nWhy am I not getting the same output in the second case as in first case?\nWhat is the difference between these two commands?\n"
'Week_number   Holiday Description  Qty\n38              1        A          5\n38              0        A          6\n38              0        B          1\n38              1        C          1\n40              0        A          1\n\n\nI want to find duplicates for same Week_number and Description. If we take example as above above Week_number 38 and Desciption A there are 2 records of it. Then I want to get sum of those 2 Qty so it will be 11. \nFinally merge those 2 records and display sum of the Qty and Holiday as 1.\n\nWeek_number   Holiday Description  Qty\n38              1        A          11\n38              0        B          1\n38              1        C          1\n40              0        A          1\n\n\nAgain check the duplicates for Week_number 38 and there will be 3 records of it. And change Holiday to 1 of all the records which has same Week_number.\n\nWeek_number   Holiday Description  Qty\n38              1        A          11\n38              1        B          1\n38              1        C          1\n40              0        A          1\n\n\nAny comments how to do that?\nThanks\n'
'I have this section in my code:\n\n# I/O files \ninp_arq = sys.argv[1]\nout_arq = sys.argv[2]\npad_ref = "pad_ref03.fasta"\ntes_mdl = "model_05_weights.best.hdf5"\n\n\nand at the end:\n\ntry:\n    results_df.to_csv(out_arq,index = False)\n    print(f"File saved as: {out_arq}")\nexcept IndexError:\n    print("No output file created")\n\n\nIf no file is passed in as out_arq (sys.argv[2]) it should run the script and print "No output file created" at the end. But I\'m getting the "IndexError: list index out of range."\n\nBut if I comment out the "out_arq = sys.argv[2]" line and change the code to:\n\ntry:\n    results_df.to_csv(sys.argv[2],index = False)\n    print(f"File saved as: {sys.argv[2]}")\nexcept IndexError:\n    print("No output file created")\n\n\nIt works and I got the message, but I\'m not sure why. I\'d like to have all my I/O file/vars at the begginig of the script, but with this one (out_arq) I can\'t.\n\nHow can I solve this? And why this happens?\n'
"I have a pandas dataset in which i have to convert the float values to integer values in one particular column. I've tried various things but keep getting errors. Any ideas?\n\nI've tried the following till now -\n\ntrain['CoapplicantIncome']=train['CoapplicantIncome'].dropna().apply(np.int64)\n\ntrain['CoapplicantIncome']=train['CoapplicantIncome'].apply(np.int64)\n\ntrain['CoapplicantIncome']=train['CoapplicantIncome'].applymap(np.int64)\n\ntrain['CoapplicantIncome']=train['CoapplicantIncome'].astype(int)\n\n"
'I have a list of tags "list_a" and a second bigger list with tags and a score "list_b". I would like to find the sorted version of list_a based on the score of the tags in list_b. The goal is the print a number of signals but sorted based on their respective scores. Currently i print them like this:\n\n    plot_tags = {\n        \'Tag_x\': 1,\n        \'Tag_y\': 2,\n        \'Tag_z\': 3,\n    }\n\n ax[1].set_ylabel(\'Tag_x\')\n ax[2].set_ylabel(\'Tag_y\')\n ax[3].set_ylabel(\'Tag_z\')\n\n\nHowever i want to plot them dynamically based on the score plot_tags have from list_b\n\nlist_b\ntag\nTag_c   1.637486\nTag_a  -1.397149\nTag_v   1.390255\nTag_b  -1.248466\nTag_f  -1.243615\n                      ...   \n\n\nP.S. list_a contains elements that list_b doesn\'t \n'
'date[\'Maturity_date\'] = data.apply(lambda data: relativedelta(months=int(data[\'TRM_LNTH_MO\'])) + data[\'POL_EFF_DT\'], axis=1)\n\n\nTried this also:\n\ndate[\'Maturity_date\'] = date[\'POL_EFF_DT\'] + date[\'TRM_LNTH_MO\'].values.astype("timedelta64[M]")\n\n\n\n  TypeError: \'type\' object does not support item assignment\n\n'
"actual data headI am stuck in a following problem,\nimage is my dataframe\n\nThe image is my dataframe in which list of state is very long which includes different states of USA as index and other 2 columns has information about counties in it and Census population 2010.\n\nMy aim is to only looking at the three most populous counties for each state, what are the three most populous states (in order of highest population to lowest population)? Use CENSUS2010POP.\nThis function should return a list of string values.\n\ndf = pd.DataFrame({'State': ['A', 'A','A','A','A','B','B','B','B','B','B','C','C','C','C','C', 'D','D', 'D', 'D'],\n               'County': ['Aa', 'Ab','Ac','Ad', 'Ae', 'Ba','Ba','Bb','Bc','Bd','Be','Ca','Cb','Cc','Cd','Ce','Da','Db','Dc','Dd'],\n               'Population': [25,35,45,60,12,80,45,60,20,30,14,65,87,65,13,29,45,60,75,80]})\n\n"
'i have a CSV file with list of texts(column with rows), and i want to extract the ages of the patients from the each row, i can\'t do with "is digit" cuz there are also some other digits in the texts. how can i do such thing? Thank You\n\nEXTRA: i want to extract the genders too - Patient sometimes is referred as male/female, sometimes as man/woman and sometimes as gentleman/lady.\n\nIs there a method to write the findall for example if the text is 17-year-old print me the number if it is followed by -year-old \n\nre.findall("[\\d].", \'-year-old\')\n\n\nSample of lines from text:\n\nThis 23-year-old white female presents with...\n\n...pleasant gentleman who is 42 years old...\n\n...The patient is a 10-1/2-year-old born with...\n\n...A 79-year-old Filipino woman...\n\nPatient, 37,...\n\n\nHow can i have a list of age/gender\n\ni.e.:\n\nAge:\n\n    [\'23\',\'42\',\'79\',\'37\'...]\n\nGender:\n\n    [\'female\',\'male\',\'male\',\'female\',\'male\'...]\n\n'
"assume I have a train dataset\n\nr1: cheap, expensive -> price\n\nr2: excited          -> entertainment\n\nr3: hot, summer      -> weather\n\nr4: money            -> price\n\nr5: rain             -> weather\n\n\n\nthen I want to display it in this pattern:\n\nprice         -> cheap, expensive, money\n\nentertainment -> excited\n\nweather       -> hot, summer, rain\n\nanyone knows? I'am doing a NLP research. thankyou. \n"
'Table with 10+ columns. Every cell in subject column has 3 elements: 1) address, 2) city-state-zip, 3) coordinates.\n\ncolumn in csv with cells\n\nHow to remove elements 1 &amp; 2 and leave coordinates only without  hooks?\n'
'Usually we have one datset and we perform train and test split, but now i already have two datasets i.e train data set and test data set. How do i pass them to the model!?\n'
"I have a dataset which contains columns: 'Month', 'Category', and 'Profitability'. I used following to find the sum of 'Profitability' for each month and category.\n\nq1=df.groupby(['Month','Category'])['Profitability'].sum()\n\n\nHere is the result I got.\n\nMonth  Category   \n1      Cosmetics       2685.9000\n       First Aid       2128.0200\n       Magazine         703.8900\n       Supplements    37005.6200\n       Toiletries      1893.0600\n2      Cosmetics       2569.0600\n       First Aid       3282.7850\n       Magazine         679.1100\n       Supplements    36647.8800\n       Toiletries      1357.7500\n3      Cosmetics       1350.7925\n       First Aid       2238.3100\n       Magazine         371.1200\n       Supplements    21444.0900\n       Toiletries      1226.1600\n\n\nI want to represent them in a bar chart. What would be the best approach to visualize these categorical data?\n"
"I want to specify that I need to get the whole row for a max value, not different max values from multiple rows, in my example this should be based on the column 'Number'. Such as this way:\n\nimport pandas as pd\n\ndata = {\n    'Number':[12,55,3,2,88,17],\n    'People':['Zack','Zack','Merry','Merry','Cross','Cross'],\n    'Random':[353,0.5454,0.5454336,32,-7,4]\n}\n\ndf = pd.DataFrame (data, columns = ['Number','People','Random'])\n\nprint(df,'\\n')\n\nmax_values = df.groupby('People').max()\n\nprint(max_values)\n\n\nHere is the result:\n\n   Number People      Random\n0      12   Zack  353.000000\n1      55   Zack    0.545400\n2       3  Merry    0.545434\n3       2  Merry   32.000000\n4      88  Cross   -7.000000\n5      17  Cross    4.000000 \n\n        Number  Random\nPeople                \nCross       88     4.0\nMerry        3    32.0\nZack        55   353.0\n\n\nHere is the expected result for max_values:\n\n        Number  Random\nPeople                \nCross       88    -7.000000\nMerry        3    0.545434\nZack        55   353.0\n\n"
'I have a Fifa 19 Dataset with data on more than 1000 players hailing from more than 100 countries. I want to make a bar chart showing the distribution of nationalities where number of players is greater than 3. Here is what I have tried:\n\nnation = fifa_19[fifa_19[\'Nationality\'].value_counts()&gt;=3]\nplt.figure(figsize=(30, 15))\nsns.set(style="whitegrid")\nsns.countplot(nation, order = fifa_19[\'Nationality\'].value_counts().index)\nplt.title(\'Distribution of Nationalities of players\')\nplt.xlabel(\'Nationality\') \nplt.ylabel(\'Frequency\')\n\n\nand I get this error:\n\nIndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).\n\n\nI am making a sample DataFrame and adding it here.\n\nfifa_19 = pd.DataFrame({\'Player\':[\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'E\',\'G\',\'H\'],\'Nationality\':[\'USA\',\'USA\',\'USA\',\'Canada\',\'Pakistan\',\'India\', \'Brazil\',\'Brazil\',\'Brazil\']})\n\n\nIf someone can help me, It will be great.\n'
"I've got data called 'Planned Leave' which includes 'Start Date', 'End Date', 'User ID' and 'Leave Type'.\n\nI want to be able to create a new data-frame which shows all days between Start and End Date, per 'User ID'.\n\nSo far, I've only been able to create a date_list which supplies a range of dates between start and end date, but I cannot find a way to include this for each 'User ID' and 'Leave Type'.\n\nHere is my current function: \n\ndef datesplit(data):\n    x = pd.DataFrame(columns=['Date'])\n    for i in plannedleave.iterrows():\n        start = data['Start Date'][i]\n        end = data['End Date'][i]\n        date_list = [start + dt.timedelta(days=x) for x in range((end-start).days)]\n    x.append(date_list)\n    return x\n\n&gt;&gt;&gt; datesplit(plannedleave)\n&gt;&gt;&gt; Value Error: Can only Tuple-index with a MultiIndex\n\n\nHere's what the data looks like: \n\n&gt;&gt;&gt; plannedleave.dtypes\n&gt;&gt;&gt;\n    Employee ID                      int64\n    First Name                      object\n    Last Name                       object\n    Leave Type                      object\n    Start Date              datetime64[ns]\n    End Date                datetime64[ns]\ndtype: object\n\n\nI'd be forever grateful if you could find a solution here! :-) \n"
'I  was learning python coding and was using a function for calculating the gc percentage in a DNA sequence with undefined character N or n (NAAATTTGGGCCCN) and this created the following problem. is there a way to overcome this ?\n\ndef gc(sequence) :\n    "This function computes the GC percentage of a dna sequence"\n    nbases=sequence.count(\'n\')+sequence.count(\'N\')\n    gc_count=sequence.count(\'c\')+sequence.count(\'C\')+sequence.count(\'g\')+sequence.count(\'G\')      #total gc count\n    gc_percent=float(gc_count)/(len(sequence-nbases))     # TOTAL GC COUNT DIVIDED BY TOTAL LEN OF THE sequence-TOTAL NO. OF N\n    return 100 * gc_percent\n\n'
"How do I create a function that can: replace  (0.0) to NaN, remove underscores, convert clean strings into a float datatype or otherwise return the converted data ?\nSo far I have tried the following:\ndef score_cleaner(underscored): \n    if underscored == '_000':\n         return np.NaN\n           \nlong_data['Numeric Score']= long_data['Score'].apply(lambda x:(float(x.replace('_',''))))\n\nlong_data ['Numeric Score']= long_data ['Score'].apply(score_cleaner) \n\nHowever this has resulted in either an output of endless &quot;NaNs&quot;, or all the numerical values rather than a combination of the two where 0.0's are converted to NaNs and the rest of the data is left alone:\nPID_Sex PID_Age ManipulationScoreFace IDCondition Numeric Score\n103 Female  18  Symmetry    _005    101 Manipulated NaN\n106 Female  19  Symmetry    _000    101 Manipulated NaN\n106 Male    22  Symmetry    _000    101 Manipulated NaN\n109 Male    20  Symmetry    _000    101 Manipulated NaN\n112 Female  18  Symmetry    _000    101 Manipulated NaN \n115 Female  18  Symmetry    _000    101 Manipulated NaN\n118 Female  19  Symmetry    _003    101 Manipulated NaN\n121 Female  18  Symmetry    _000    101 Manipulated NaN\n124 Female  19  Symmetry    _004    101 Manipulated NaN\n127 Female  19  Symmetry    _005    101 Manipulated NaN\n\nPID_Sex PID_Age ManipulationScoreFace IDConditionNumericScore\n103 Female  18  Symmetry    _005    101 Manipulated 5.0\n106 Female  19  Symmetry    _000    101 Manipulated 0.0\n106 Male    22  Symmetry    _000    101 Manipulated 0.0\n109 Male    20  Symmetry    _000    101 Manipulated 0.0\n112 Female  18  Symmetry    _000    101 Manipulated 0.0\n115 Female  18  Symmetry    _000    101 Manipulated 0.0\n118 Female  19  Symmetry    _003    101 Manipulated 3.0\n121 Female  18  Symmetry    _000    101 Manipulated 0.0\n124 Female  19  Symmetry    _004    101 Manipulated 4.0\n127 Female  19  Symmetry    _005    101 Manipulated 5.0\n\n"
"For the input dataframe below, we want to create two columns, A_count and B_count.\nThere is an input variable called surrounding_row_num. I.E. for surrounding_row_num=2, we want to look 2 rows before and 2 rows after the current row, and count the number of occurrences of 'A' in the column 'Label', that has the same 'ID' as the current row.\nInput:\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': [1, 1, 1, 1, 2, 2],'Label': ['A', 'A', 'B', 'B', 'A', 'B']})\n    \n   ID Label\n0   1     A\n1   1     A\n2   1     B\n3   1     B\n4   2     A\n5   2     B\n\nOutput for surrounding_row_num=2:\n        ID  Label A_count B_count \n        1   A     2        1\n        1   A     2        1\n        1   B     1        2\n        1   B     0        2        \n        2   A     1        1\n        2   B     1        1       \n\n\nThanks!\n"
"Following is the sample table which consists of transaction data of bank customers. I need to create a separate column as annual salary of customer taking the data from txn_description column.\nCustomer_ID txn_description Amount Type\n01           POS            345    Dr\n02           SALARY         2000   Cr\n03           INTER BANK     148    Dr\n04           SALARY         1500   Cr\n05           NEFT           289    Dr\n06           SALARY         1800   Cr\n01           NEFT           40     Dr\n02           SALARY         2000   Cr\n04           POS            69     Dr\n04           SALARY         1500   Cr\n06           SALARY         1800   Cr\n\nNote: The transaction data is of three months. So the salary is credited to a particular customer's account thrice in this table for three months.\n(Dr = Debit transaction and Cr = Credit transaction)\n"
"Im trying to remove patterns from a dataset which have a daily activity shape like the one below.  I tried seasonal_decompose on it which may not be appropriate.\nWhat I would like to do is remove the expected peak usage pattern and arrive at a trend or peak as happens when you apply the seasonal_decompose function in monthly data.\nDoes anyone know can I see trends and abnormal data in daily data like this ?\n\nEdit: Here is the code to reproduce above example.\nsample = {'EventTime': [pd.Timestamp('2020-09-21 00:00:00'), pd.Timestamp('2020-09-21 01:00:00'), pd.Timestamp('2020-09-21 02:00:00'), pd.Timestamp('2020-09-21 03:00:00'), pd.Timestamp('2020-09-21 04:00:00'), pd.Timestamp('2020-09-21 05:00:00'), pd.Timestamp('2020-09-21 06:00:00'), pd.Timestamp('2020-09-21 07:00:00'), pd.Timestamp('2020-09-21 08:00:00'), pd.Timestamp('2020-09-21 09:00:00'), pd.Timestamp('2020-09-21 10:00:00'), pd.Timestamp('2020-09-21 11:00:00'), pd.Timestamp('2020-09-21 12:00:00'), pd.Timestamp('2020-09-21 13:00:00'), pd.Timestamp('2020-09-21 14:00:00'), pd.Timestamp('2020-09-21 15:00:00'), pd.Timestamp('2020-09-21 16:00:00'), pd.Timestamp('2020-09-21 17:00:00'), pd.Timestamp('2020-09-22 01:00:00'), pd.Timestamp('2020-09-22 02:00:00'), pd.Timestamp('2020-09-22 03:00:00'), pd.Timestamp('2020-09-22 04:00:00'), pd.Timestamp('2020-09-22 05:00:00'), pd.Timestamp('2020-09-22 06:00:00'), pd.Timestamp('2020-09-22 07:00:00'), pd.Timestamp('2020-09-22 08:00:00'), pd.Timestamp('2020-09-22 09:00:00'), pd.Timestamp('2020-09-22 10:00:00'), pd.Timestamp('2020-09-22 11:00:00'), pd.Timestamp('2020-09-22 12:00:00'), pd.Timestamp('2020-09-22 13:00:00'), pd.Timestamp('2020-09-22 14:00:00'), pd.Timestamp('2020-09-22 15:00:00'), pd.Timestamp('2020-09-22 16:00:00'), pd.Timestamp('2020-09-22 17:00:00'), pd.Timestamp('2020-09-23 00:00:00'), pd.Timestamp('2020-09-23 01:00:00'), pd.Timestamp('2020-09-23 02:00:00'), pd.Timestamp('2020-09-23 03:00:00'), pd.Timestamp('2020-09-23 04:00:00'), pd.Timestamp('2020-09-23 05:00:00'), pd.Timestamp('2020-09-23 06:00:00'), pd.Timestamp('2020-09-23 07:00:00'), pd.Timestamp('2020-09-23 08:00:00'), pd.Timestamp('2020-09-23 09:00:00'), pd.Timestamp('2020-09-23 10:00:00'), pd.Timestamp('2020-09-23 11:00:00'), pd.Timestamp('2020-09-23 12:00:00'), pd.Timestamp('2020-09-23 13:00:00'), pd.Timestamp('2020-09-23 14:00:00'), pd.Timestamp('2020-09-23 15:00:00'), pd.Timestamp('2020-09-23 16:00:00'), pd.Timestamp('2020-09-23 17:00:00'), pd.Timestamp('2020-09-24 01:00:00'), pd.Timestamp('2020-09-24 02:00:00'), pd.Timestamp('2020-09-24 03:00:00'), pd.Timestamp('2020-09-24 04:00:00'), pd.Timestamp('2020-09-24 05:00:00'), pd.Timestamp('2020-09-24 06:00:00'), pd.Timestamp('2020-09-24 07:00:00'), pd.Timestamp('2020-09-24 08:00:00'), pd.Timestamp('2020-09-24 09:00:00'), pd.Timestamp('2020-09-24 10:00:00'), pd.Timestamp('2020-09-24 11:00:00'), pd.Timestamp('2020-09-24 12:00:00'), pd.Timestamp('2020-09-24 13:00:00'), pd.Timestamp('2020-09-24 14:00:00'), pd.Timestamp('2020-09-24 15:00:00'), pd.Timestamp('2020-09-24 16:00:00'), pd.Timestamp('2020-09-24 17:00:00'), pd.Timestamp('2020-09-25 00:00:00'), pd.Timestamp('2020-09-25 01:00:00'), pd.Timestamp('2020-09-25 02:00:00'), pd.Timestamp('2020-09-25 03:00:00'), pd.Timestamp('2020-09-25 04:00:00'), pd.Timestamp('2020-09-25 05:00:00'), pd.Timestamp('2020-09-25 06:00:00'), pd.Timestamp('2020-09-25 07:00:00'), pd.Timestamp('2020-09-25 08:00:00'), pd.Timestamp('2020-09-25 09:00:00'), pd.Timestamp('2020-09-25 10:00:00'), pd.Timestamp('2020-09-25 11:00:00'), pd.Timestamp('2020-09-25 12:00:00'), pd.Timestamp('2020-09-25 13:00:00'), pd.Timestamp('2020-09-25 14:00:00')],\n          'SpeedKbs': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1088.48, 58282.31, 83008.37, 58044.14, 34211.61, 27468.72, 25756.96, 14090.29, 5392.43, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1008.33, 44002.72, 47254.5, 37419.96, 23934.41, 19402.93, 18192.84, 9040.67, 3842.37, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1241.15, 43260.7, 56718.99, 41968.16, 33144.51, 22361.08, 28672.93, 21182.31, 5352.42, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 946.01, 46169.63, 51720.39, 37393.39, 27732.89, 25779.79, 24790.86, 15786.72, 4202.65, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 871.7, 37196.78, 40910.71, 26758.97, 17710.98, 16024.61, 15312.96, 9529.89]}\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nseasonal_decompose(pd.DataFrame(sample).set_index(&quot;EventTime&quot;), model='additive', period=1).plot();\n\n"
'\ni have table as shown above where there are two columns(Gender and year). i want to convert this into following format as shown below. any help on how to do this would be appreciated.\n\n'
'This is an extract from my dataframe with shape (7800, 3)\n             datetime   ui1 ui2\n0 2019-01-01 00:12:45  0.20   4\n1 2019-01-01 01:04:17  0.12  36\n2 2019-01-01 04:27:01  0.63   7\n3 2019-01-01 06:31:09  0.31  40\n4 2019-01-01 07:43:28  0.48   9\n\nI want to bin the dataframe into bins using the datetime column with interval of 3H to look like this\n             datetime  ui1 ui2  ui1a ui2a\n0 2019-01-01 00:00:00  nan nan   nan  nan\n1 2019-01-01 00:03:00 0.20   4  0.12   36\n2 2019-01-01 00:06:00 0.63   7   nan  nan\n3 2019-01-01 00:09:00 0.31  40  0.48    9\n\n'
'I have this dataframe train_info with 423 different artists and filenames corresponding to images of paintings.\n    artist             filename\n0   Hiroshige          53180.jpg\n1   Ivan Aivazovsky    99442.jpg\n2   Hiroshige          23508.jpg\n3   Hieronymus Bosch   82352.jpg\n4   Hiroshige          27254.jpg\n... ... ... ... ...\n128069  Frans Snyders   14264images161.jpg\n128070  Frans Snyders   14260images158.jpg\n128071  Frans Snyders   14274images170.jpg\n128072  Frans Snyders   14355images90.jpg\n128073  Frans Snyders   14270images167.jpg\n\nThen i have a folder - Paintings - containing all these images.\nWhat i want to do is create another folder - train - with sub-folders for each artist and each sub-folder should contain all the images corresponding to each artist.\nLike this:\n-train\n    -Hiroshige\n         -53180.jpg\n         -23508.jpg\n         -27254.jpg\n         ...\n    -Ivan Aivazovsky\n         -99442.jpg\n         ...\n    -Frans Snyders\n         -14264images161.jpg\n         -14260images158.jpg\n         -14274images170.jpg\n         -14355images90.jpg\n         -14270images167.jpg\n         ...\n\nUnfortunately, I have no idea how to solve this.\n'
'I am trying to Calculate the average distance of these points from the point (0,0).I seem to be getting errors and I am unsure what the problem is. Below is what I have done\ncoordinates = [(0,1), (3,4), (-5,12), (2,2)]\n\nprint (coordinates)\nimport math\ndef distance(x1, y1, x2, y2):\n     return math.sqrt(square(x2-x1) + square(y2-y1))\ndef square(value):\n    return value * value\n\ndistances = list(map(lambda (x,y): distance(0,0,x,y), coordinates))\naverage = reduce(lambda p,q: p + q, distances)/float(len(distances))\nprint (average)\n\nI keep getting the following error\nFile &quot;&lt;ipython-input-103-76dd44055d43&gt;&quot;, line 7\n    distances = list(map(lambda (x,y): distance(0,0,x,y), coordinates))\n                                ^\nSyntaxError: invalid syntax\n\n'
'I have a text dataset. the content of the text is looks like as follows.\n.I 1\\n.T\\nPreliminary Report-International Algebraic Language\\n.B\\nCACM December,\n .I 2\\n.T\\nExtraction of Roots by,5\\t3\\n .I 3\\n.T\\nTechniquI 4\\n.T\\nGlossary of Computer \n\nthis is the description of the dataset\n.I 1, I.2, .I 3 -&gt; are the document id and the rest of the text is the content of the document.\nthe task is: to create a list of tuples-&gt; [(doc_id, content)]. Any help or suggestion is highly appreciated!\n'
"I am a newbie in data analysis stuff. I am trying to analyse a dataset using python. \n\n\nI want to count no. of 1s in survived column\nNo. of male , female in Sex column \n\n\nPassengerId  Survived  Pclass   Sex\n    0            1         0       3   male\n    1            2         1       1   female\n    2            3         1       3   male\n    3            4         1       1   female\n    4            5         0       3   male\n\n\nI tried groupby() but its giving error.\n\nIn[88]   titanic_data.groupby('Survived') \n Out[88] &lt;pandas.core.groupby.DataFrameGroupBy object at 0x000000000BFFE588&gt;\n \n\nPlease suggest solution\n"
"I made a random data of my own, that comprises of a text file with 18 rows and 5 columns with all integer entries.\n\nI successfully managed to do PCA but now stuck. I am unable to do a scatter plot. Here is my code:\n\nf=open(r'&lt;path&gt;mydata.txt')\nprint(f.read()) #reading from a file\n\n\nwith open(r'&lt;path&gt;mydata.txt') as f:\nemp= []\nfor line in f:\n    line = line.split() \n    if line:            \n        line = [int(i) for i in line]\n        emp.append(line)\n\n\nfrom sklearn.decomposition import PCA\nimport pylab as pl\nfrom itertools import cycle\nX = emp\npca = PCA(n_components=3, whiten=True).fit(X)\nX_pca = pca.transform(X) #regular PCA\n\n\nNow, with PCA done and my variances known, how do I plot?\n\nHere is how a sample data in my data set looks:\n\n2    1    2    3    0\n2    3    2    3    0\n1    3    1    1    0\n1    5    2    1    0\n2    3    1    1    0\n3    3    0    1    0\n7    1    1    1    1\n7    2    2    1    1\n1    1    1    4    1\n3    2    3    2    1\n2    2    2    2    1\n1    3    2    3    1\n2    3    2    1    2\n2    2    1    1    2\n7    5    3    2    2\n3    4    2    4    2\n2    1    1    1    2\n7    1    3    3    2\n\n"
'enter image description here\n\n-Here is my dataset contain the special character, so my question is how to remove that A^ symbol from the Quantity column..\n- after removing that special symbol from data, how i can convert this column to float type..?\n'
'I have 100k rows and I want to group it as explained below in python. A simple python iteration takes lots of time. How to optimize it using any python ML library?\n\n    [[1,2,3,4],[2,3],[1,2,3],[2,3],[1,2,3],[1,2,3,4],[1],[2]...]\n\n    Output\n    [[0,5],[1,3]],[2,4],[6],[7]]\n\n    Explanation:  index 0,5 have same list ;\n                  index 1,3 have same list ;\n                  index 2,4 have same list ; \n                  index 6 no match\n\n\nI have 100k sub list and I want to group it as explained above in python.\n'
'I am new to data mining I was trying to implement the KNN Classifier on separate training and testing datasets. all tutorials that I see use train_test_split method to split the data set, whereas I already have the dataset split into Train and Test. How do I assign the target variable? \n'
"i have five week seasonal data in a single series with date and time, how do i separate it based on week wise,like week1,week2...week5 so that i can plot all the week data in same graph.\n\ni tried re sampling the data on day wise by finding the mean. but the data is still in single series. i just want to separate the data based on weeks like 2019-04-02 to 2019-04-08 in different dataframe,2019-04-08 to 2019-04-16 in separate df\n\ndf.open.resample('M').mean()\ndate    pageload  day\n0     2019-04-02 10:48:00  -79.002023  Tue\n1     2019-04-02 10:49:00   33.563679  Tue\n2     2019-04-02 10:50:00  -76.448319  Tue\n3     2019-04-02 10:51:00   30.974816  Tue\n4     2019-04-02 10:52:00  -68.789962  Tue\n5     2019-04-02 10:53:00   30.593374  Tue\n21    2019-04-16 11:34:00   40.333445  Fri\n\n\ndata frame separated on week wise. To plot all the week data in single graph.\n"
"I want to very simply call a column in a df based on a different columns value.\n\nBelow is what I would use, but how can I add another method on top of this that says give me all the values in a column called minutes or df['minutes'] if another called column_name is a specific value?\n\ndf.loc[df['column_name'] == some_value]\n\n\nSample Data:\n\ncolumn_name | Minutes\n   1-5           19\n   6-10          22\n   11-15          8\n   1-5           11\n   6-10          33\n\n\nI want to filter for column_name = any value in that column and return all values under the minutes column. \n\nso if column_name is 1-5 return all values in Minutes column\n"
"I have an array of values that range from 0 to 1 that relates to the output truth values for a neural network I'm building. However the distribution is very wide and uneven, so I was curious if there was a package for Python that could remove samples so that the distribution is more even across the array.\n\nHere's the distribution plot from seaborn's seaborn.distplot().\n\n\n\nWhat I'd like to do is essentially specify a value of how many 'sections' to break the array into, and to remove values from the largest sections so that the distribution is more even.\n\nThe plot from the output of this function would probably look something like this:\n\n\n\nDoes there exist any kind of built-in package for numpy, or scipy to do this?\n"
'I am learning python now and need a solution for this problem!\n\ncity_indices = list(range (0,len (cities)))\n\n\ncity_indices\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n\ncity_names = [\'Buenos Aires\',\'Toronto\',\'Pyeongchang\',\'Marakesh\', \'Albuquerque\', \'Los Cabos\', \'Greenville\', \'Archipelago Sea\', \'Walla Walla Valley\', \'Salina Island\', \'Solta\', \'Iguazu Falls\']\n\nYour task is to assign the variable names_and_ranks to a list, with each element equal to the city name and it\'s corresponding rank. For example, the first element would be, "1. Buenos Aires" and the second would be "2. Toronto". Use a for loop and the lists city_indices and city_names to accomplish this.\n\nnames_and_ranks = [\'change this to different elements\'] # make sure the list is empty\n\nnames_and_ranks[0] # \'1. Buenos Aires\'\n\nnames_and_ranks[1] # \'2. Toronto\'\n\nnames_and_ranks[-1] # \'12. Iguazu Falls\'\n\n\nThanks for help. I found the solution\n\nnames_and_ranks = []\nfor index in city_indices: \n    names_and_ranks.append(f"{index + 1}. {city_names[index]}")\n\n'
'I am using the learning curve code path that is mentioned in this link.\n\nhttps://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html\n\nI am getting the error below:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-41-f71104873302&gt; in &lt;module&gt;\n    132 cv = KFold(n_splits=15, random_state=42, shuffle=False)\n    133 plot_learning_curve(bag, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n--&gt; 134                     cv=cv, n_jobs=4)\n    135 \n    136 title = r"Learning Curves LightGBM Classifier"\n\n&lt;ipython-input-41-f71104873302&gt; in plot_learning_curve(estimator, title, X, y, axes, ylim, cv, n_jobs, train_sizes)\n     78     axes[0].set_ylabel("Score")\n     79 \n---&gt; 80     train_sizes, train_scores, test_scores, fit_times= learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,train_sizes=train_sizes)\n     81     train_scores_mean = np.mean(train_scores, axis=1)\n     82     train_scores_std = np.std(train_scores, axis=1)\n\nValueError: not enough values to unpack (expected 4, got 3)\n\n\nPlease help me solve this issue. I am using the exact same code for plotting with different models but i am still getting the same error.\n'
"I'm having some trouble trying to match user input to a list.\nI would like python to grab what the user inputs for example: travel; it matches the input with first index in the list since it contains &quot;travel&quot;. If the user types: &quot;mystery,&quot; it would grab the second link and return true.\ncode example:\nurl_links = [\n    'http://books.toscrape.com/catalogue/category/books/travel_2/index.html', \n    'http://books.toscrape.com/catalogue/category/books/mystery_3/index.html'\n]\n\nu_input = input(&quot;what genre of books do you want listed?&quot;) \n\nI've tried:\n\nif any(s in url_links for s in u_input.lower()): #this gives false\nif u_input.lower() in url_links: #this also gives a false.\n\nThank you!\n"
'I am trying to learn pandas. Currently working on a car price prediction algorithm. Registration_date data is given in year/month/day (it is a string not in date class) format like this:\nIndex Registiration_date\n0     2012-02-01\n1     2016-04-01\n2     2012-04-01\n3     2014-07-01\n4     2014-12-01\n5     2011-05-01\n6     2009-05-01\n7     2009-08-01\n8     2004-07-01\n\nI want to have the year data from this in a new column. What is the fastest way to do this?\nThe data I am working on is this\n'
"I want to find the raw data which are classified successfully and which are not classified after Multinomial Nieves Bayes Classification algorithm is applied.\nFor instance I got the accuracy as 88% after applying Multinomail Naives Bayes classification.\nI want to know the 12% of data which are not classified and also 88% of the data that is classified.\n Thanks in advance\n\nMy data set:\n\n+----------------------+------------+\n| Details              | Category   |\n+----------------------+------------+\n| Any raw text1        | cat1       |\n+----------------------+------------+\n| any raw text2        | cat1       |\n+----------------------+------------+\n| any raw text5        | cat2       |\n+----------------------+------------+\n| any raw text7        | cat1       |\n+----------------------+------------+\n| any raw text8        | cat2       |\n+----------------------+------------+\n| Any raw text4        | cat4       |\n+----------------------+------------+\n| any raw text5        | cat4       |\n+----------------------+------------+\n| any raw text6        | cat3       |\n+----------------------+------------+\n\n\nMy code:\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt  \nfrom sklearn.model_selection import train_test_split \ndata= pd.read_csv('mydat.xls', delimiter='\\t',usecols=\n['Details','Category'],encoding='utf-8')\ntarget_one=data['Category']\ntarget_list=data['Category'].unique()         \nx_train, x_test, y_train, y_test = train_test_split(data.Details, \ndata.Category, random_state=42)\nvect = CountVectorizer(ngram_range=(1,2))\n#converting traning features into numeric vector\nX_train = vect.fit_transform(x_train.values.astype('U'))\n#converting training labels into numeric vector\nX_test = vect.transform(x_test.values.astype('U'))\n# start = time.clock()\n\nmnb = MultinomialNB(alpha =0.13)\n\nmnb.fit(X_train,y_train)\n\nresult= mnb.predict(X_test)\n\n\n# mnb.predict_proba(x_test)[0:10,1]\naccuracy_score(result,y_test)\n\n"
'I have used OneVsRestClassifier of sklearn for multi-class classification. I need improve my model by adding incremental learning. I am not able to find any incremental learning that supports multi-class.\n'
'I\'m trying to find sentences having only one digit number along with.\n\nsentence="I\'m 30 years old."\nprint(re.match("[0-9]", sentence)\n\n\nthen it returns\n\n&lt;re.Match object; span=(0, 1), match=\'3\'&gt;\n\n\nbut it\'s 30 which is two digit number actually, and I don\'t want it to be matched.\nseems like each consisting number 3 and 0 is recognized as one independent number.\nAnd these numbers are two-byte number which is usually used in my country.\n\nHow can I change my regular expression?\nThanks!\n'
'In my df I have a field called day for day and month such as, 2019/07/29 if I want to create another field in the df that states the actual day of week this is such as Sunday or Monday is that possible?\n\nThanks!\n'
'I want to scrap the data from the booking.com but got some errors and couldn\'t find any similar codes.\nI want to scrap the name of the hotel,price and etc.\n\ni have tried beautifulSoup 4 and tried to get data to a csv file.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas\n\n# Replace search_url with a valid one byb visiting and searching booking.com\nsearch_url = \'https://www.booking.com/searchresults.....\'\npage = requests.get(search_url)\nsoup = BeautifulSoup(page.content, \'html.parser\')\n\nweek = soup.find(id = \'search_results_table\'  )\n#print(week)\n\nitems = week.find_all(class_=\'sr-hotel__name\')\nprint(items[0])\nprint(items[0].find(class_ = \'sr-hotel__name\').get_text())\nprint(items[0].find(class_ = \'short-desc\').get_text())\n\n\n\nHere is a sample URL that can be used in place of search_url.\n\nThis is the error msg...\n\n&lt;span class="sr-hotel__name " data-et-click="\n"&gt;\nThe Fort Printers\n&lt;/span&gt;\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-44-77b38c8546bb&gt; in &lt;module&gt;\n     11 items = week.find_all(class_=\'sr-hotel__name\')\n     12 print(items[0])\n---&gt; 13 print(items[0].find(class_ = \'sr-hotel__name\').get_text())\n     14 print(items[0].find(class_ = \'short-desc\').get_text())\n     15 \n\nAttributeError: \'NoneType\' object has no attribute \'get_text\'\n\n'
'Pandas:I have a dataframe given below which contains the same set of banks twice..I need to slice the data from 0th index that contains a bank name, upto the index that contains the same bank name..here in the problem -DEUTSCH BANK AG..I need to apply same logic to any such kind of dataframes.ty..\n\nI tried with logic:-  df25.iloc[0,1]==df25[1].any().. but it returns nly true but not the index position.\n\nDataFrame:-[1]:https://i.stack.imgur.com/iJ1hJ.png, https://i.stack.imgur.com/J2aDX.png\n'
'I have a data set in which date(yyyy/mm/dd), hour and output are the three columns. Can i model this data as a time series data to forecast the output as a function of the date and hour using ARIMA or any other algorithm?\n'
'Given a dataframe, with Age as basis, I need to get the correspondence values from other columns (Sport, Mode). Could someone help with R/python code? \n. \nIn fact, it will be helpful if I could get for age 15, 2 baseball with 2 play; age 19 1 golf and 1 play. \n\n the output should look like the below with Age as base variable\n \n\nFurther on with Sport as base variable, the mode should have a similar summary. Thank you\n'
'I have dataframe which consists with many columns. \n\ndf2\n\n\n\n   TargetDescription                               Output_media_duration\n0   VMN 4.0 16x9 25 - 1920x1080, 1280x720, 960x540...    NaN\n1   VMN 4.0 16x9 25 - 1920x1080, 1280x720, 960x540...    NaN\n2   XDCAM HD NTSC 1920x1080 MXF 8CA                      661.120000\n3   VMN 4.0 16x9 29.97 - 1920x1080, 1280x720, 960x...   285.647686\n4   VMN 4.0 16x9 29.97 - 1920x1080, 1280x720, 960x...   402.697303\n5   VMN 4.0 16x9 29.97 - 1920x1080, 1280x720, 960x...   269.597070\n6   VMN 4.0 16x9 29.97 - 1920x1080, 1280x720, 960x...   307.059607\n7   Caption QC HD MOV 2CA                               2516.096917\n8   QT Proxy 640x360 2997 12CA                          NaN\n9   XDCAM HD NTSC 1920x1080 MXF 8CA                     1414.785215\n10  Caption QC HD MOV 2CA                               1295.027067\n11  QT Proxy 640x360 2398 4CA                           2524.980792\n12  Caption QC HD MOV 2CA                               120.820700\n13  Caption QC HD MOV 2CA                               2516.096917\n\n\nNow I want to get one new dataframe which would show me like this \n\nTargetDescription                                                     format_duration\n1   VMN 4.0 16x9 25 - 1920x1080, 1280x720, 960x540...                       NaN\n2   XDCAM HD NTSC 1920x1080 MXF 8CA                                         661.120000\n3   VMN 4.0 16x9 29.97 - 1920x1080, 1280x720, 960x...                       1656.561906 \n4   Caption QC HD MOV 2CA                                                   2516.096917\n5   QT Proxy 640x360 2997 12CA                                              NaN\n6   Caption QC HD MOV 2CA                                                   2636.917\n\n\nHow can I achieve this in pandas,thanks in advance\n'
"There are four variables \n(S1, S2, S3, S4) \nwith the constraint \n(S1+S2+S3+S4=100). \nThere are four given constants (C1, C2, C3, C4). I want to maximize the value of (S1/C1 + S2/C2 + S3/C3 + S4/C4).  Here is my code in python:\n\n#!/usr/bin/env python3\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\nS0 = [25, 25, 25, 25]\nC = [89415,8991,10944,15164]\n\ndef objective(S, C):\n    total = 0\n    for index in range(4):\n        total = total + S[index]/C[index]        \n    return -total\n\ndef constraint(S):\n    return (100 - S[0] - S[1] - S[2] - S[3])\n\nb = (0.0, 100.0)\nboundaries = (b,b,b,b)\ncon = ({'type':'eq', 'fun':constraint})\n\nsolution = minimize(objective,S0,args=(C),method='SLSQP',bounds=boundaries,constraints=con)\n\nprint (solution)\n\n\nMy code is simply returning the initial guess for S as the final result\n\nfun: -0.0069931517268763755\n     jac: array([-1.11838453e-05, -1.11222384e-04, -9.13742697e-05, -6.59456709e-05])\n message: 'Optimization terminated successfully.'\n    nfev: 6\n     nit: 1\n    njev: 1\n  status: 0\n success: True\n       x: array([25., 25., 25., 25.])\n\n\nWhere am I going wrong?\n"
"I have a dataframe with one column and I need to return 3 most frequent genres.\n\n\n  INPUT\n\n\n    genres\n0   Drama\n1   Animation|Children's|Musical\n2   Musical|Romance\n3   Drama\n4   Animation|Children's|Comedy\n5   Action|Adventure|Comedy|Romance\n6   Action|Adventure|Drama\n7   Comedy|Drama\n8   Animation|Children's|Musical\n9   Adventure|Children's|Drama|Musical\n10  Animation|Children's|Musical\n11  Musical\n12  Drama\n13  Comedy\n\n\nDrama 6\nMusical 6\nChildren's 5\nAnimation 4\nComedy 4\nAdventure 3\nAction 2\n\n\n  OUTPUT - A dataframe with:\n\n\n  genres\n0 Drama\n1 Musical\n2 Children's\n\n"
"I understand that this is not the most concise block of code and looking for ways to simplify it\n\nnine = fb_posts2[fb_posts2['year']==2009].groupby('title').size()\nten = fb_posts2[fb_posts2['year']==2010].groupby('title').size()\neleven = fb_posts2[fb_posts2['year']==2011].groupby('title').size()\ntwelve = fb_posts2[fb_posts2['year']==2012].groupby('title').size()\nthirteen = fb_posts2[fb_posts2['year']==2013].groupby('title').size()\nfourteen = fb_posts2[fb_posts2['year']==2014].groupby('title').size()\nfifteen = fb_posts2[fb_posts2['year']==2015].groupby('title').size()\nsixteen = fb_posts2[fb_posts2['year']==2016].groupby('title').size()\nseventeen = fb_posts2[fb_posts2['year']==2017].groupby('title').size()\neighteen = fb_posts2[fb_posts2['year']==2018].groupby('title').size()\na1 = lambda x: x/sum(nine)*100\na2 = lambda x: x/sum(ten)*100\na3 = lambda x: x/sum(eleven)*100\na4 = lambda x: x/sum(twelve)*100\na5 = lambda x: x/sum(thirteen)*100\na6 = lambda x: x/sum(fourteen)*100\na7 = lambda x: x/sum(fifteen)*100\na8 = lambda x: x/sum(sixteen)*100\na9 = lambda x: x/sum(seventeen)*100\na10 = lambda x: x/sum(eighteen)*100\nnine = a1(nine)\nten = a2(ten)\neleven = a3(eleven) \ntwelve = a4(twelve)\nthirteen = a5(thirteen)\nfourteen = a6(fourteen)\nfifteen = a7(fifteen)\nsixteen = a8(sixteen)\nseventeen = a9(seventeen)\neighteen = a10(eighteen)\nmy_names = [2009,2010,2011,2012,2013,2014,2015,2016,2017,2018]\ncols = ['link', 'post','shared','timeline','status']\nser = [nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen]\ndf = pd.concat(ser, axis=1, keys=my_names)\ndf[2009].fillna(0, inplace=True)\ndf[2011].fillna(0, inplace=True)\ndf[2012].fillna(0, inplace=True)\ndf = df.transpose()\n\n\nThe intention of this is to return a dataframe that shows how many times each 'title' occurred in a given year as a percentage.\n\nThis is the sample input\n\n\nThis is the sample output\n"
"I am getting this error\n\n\n  ValueError: Expected 2D array, got scalar array instead: array=6.5.\n  Reshape your data either using array.reshape(-1, 1) if your data has a\n  single feature or array.reshape(1, -1) if it contains a single sample.\n\n\nwhile executing this code\n\n# SVR\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVR\n\n# Load dataset\ndataset = pd.read_csv('Position_Salaries.csv')\n\nX = dataset.iloc[:, 1:2].values\ny = dataset.iloc[:, 2].values\n\n# Fitting the SVR to the data set\nregressor = SVR(kernel = 'rbf', gamma = 'auto')\nregressor.fit(X, y)\n\n# Predicting a new result \ny_pred = regressor.predict(6.5)\n\n"
"I'm importing a csv file that contains transposed data. The data has columns in the following format: AC1,AC2,AD1,AD2,BP1,BP2,CT1,CO1,CO2,CS1,etc\n\nWhat I've been hoping to accomplish is to group together and find the SUM of each LIKE column heading along with the min and max of each of those newly grouped columns. \n\nExample would be:\n\nAC1+AC2 = AC(sum), min, max\n\nAD1+AD2 = AD(sum), min, max\n\nBP1+BP2 = BP(sum), min, max\n\n\nFormat and structure do not matter as long as the end result works.\n\nIs this feasibly possible or is there a better approach that can be taken to achieve this?\n\nI've used macros via Excel and there still requires a bit manual effort to group the data etc.\n"
"The data:\n\nI want to remove duplicates from &quot;Borough&quot; without deleting the rows, I only need only one value for each\ng2 = dfff.groupby(['Postcode'])[&quot;Borough&quot;].agg( ','.join)\ng3 = dfff.groupby(['Postcode'])[&quot;Neighbourhood&quot;].agg( ','.join)\ndf2=pd.DataFrame(g2)\ndf3=pd.DataFrame(g3)\ndf4 = pd.merge(df2, df3, on='Postcode')\n\n"
'How can I define constraints in python? I want to calculate are of parallelogram only if vertices lie between 0 to 1000 ,i.e. 0&lt;=x&lt;1000.\n'
"import cv2\nimport os\nimport glob\nimport pandas as pd\nfrom pylibdmtx import pylibdmtx\nimport xlsxwriter\n\n\n# co de for scanning\n\nimg_dir = &quot;C:\\\\images&quot; # Enter Directory of all images\ndata_path = os.path.join(img_dir,'*g')\nfiles = glob.glob(data_path)\ndata = []\nresult=[]\n\nfor f1 in files:\n    img = cv2.imread(f1,cv2.IMREAD_UNCHANGED)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n    msg = pylibdmtx.decode(thresh)\n    print(msg)\n    data.append(img)\n    result.append(msg)\n\nprint(type(result[0]))\n\nI have a lsit of 4 list inside a lists names result .The output of above code is result . The code is intended to read the barcode , but it also provides location which is not required by me .\nSO after the above code , I have a output named result , whch gives me ::\n[[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))], [Decoded(data=b'AZ:RCHKBW5WGZE98J7E9853OW4ZHE', rect=Rect(left=40, top=125, width=91, height=95))], [Decoded(data=b'AZ:5Z7HME1FRNAZFINDPTDAOTB9GQ', rect=Rect(left=27, top=112, width=88, height=88))]\n\nso NOW i want ot jsut extract or find The az aprt from the all the single lists and export it to excel.\nAZ:HP7CXNGSUFEPZCO4GS5RQPY6XY\nAZ:RCHKBW5WGZE98J7E9853OW4ZHE\nAZ:5Z7HME1FRNAZFINDPTDAOTB9GQ\n\nI want only the above output and omit all the location details .\nI have tried with indexing , but IT's saying lists out of range.\nPlease helpme.\n"
"so I have an excel file where I have information on some start-ups and I want to Display details of startups where the City Location starts with &quot;s&quot; or &quot;p&quot;. How can I do this using pandas?\nData:\nCity  Location\nPune\nMumbai\nMumbai\nHyderabad\nBurnsville\nBengaluru\nMumbai\nMenlo Park\nBengaluru\nGurgaon\nNew Delhi\nBengaluru\nMumbai\nNoida\nBengaluru\n\nThis not working:\n df[df['City Location'].str.startswith(('s','p'))]\n\n"
'\r\n\r\n\'&lt;table border="1" class="dataframe"&gt;  &lt;thead&gt;    &lt;tr style="text-align: right;"&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;CDS_CODE&lt;/th&gt;      &lt;th&gt;COUNTY&lt;/th&gt;      &lt;th&gt;DISTRICT&lt;/th&gt;      &lt;th&gt;SCHOOL&lt;/th&gt;      &lt;th&gt;KDGN&lt;/th&gt;      &lt;th&gt;GR_1&lt;/th&gt;      &lt;th&gt;GR_2&lt;/th&gt;      &lt;th&gt;GR_3&lt;/th&gt;      &lt;th&gt;GR_4&lt;/th&gt;      &lt;th&gt;GR_5&lt;/th&gt;      &lt;th&gt;GR_6&lt;/th&gt;      &lt;th&gt;GR_7&lt;/th&gt;      &lt;th&gt;GR_8&lt;/th&gt;      &lt;th&gt;UNGR_ELM&lt;/th&gt;      &lt;th&gt;GR_9&lt;/th&gt;      &lt;th&gt;GR_10&lt;/th&gt;      &lt;th&gt;GR_11&lt;/th&gt;      &lt;th&gt;GR_12&lt;/th&gt;      &lt;th&gt;UNGR_SEC&lt;/th&gt;      &lt;th&gt;ENR_TOTAL&lt;/th&gt;      &lt;th&gt;ADULT&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;1611436056865&lt;/td&gt;      &lt;td&gt;Alameda&lt;/td&gt;      &lt;td&gt;Berkeley Unified&lt;/td&gt;      &lt;td&gt;Willard Middle&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;12&lt;/td&gt;      &lt;td&gt;16&lt;/td&gt;      &lt;td&gt;6&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;34&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;1611436056865&lt;/td&gt;      &lt;td&gt;Alameda&lt;/td&gt;      &lt;td&gt;Berkeley Unified&lt;/td&gt;      &lt;td&gt;Willard Middle&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;1&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;3&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;4&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;1611436056865&lt;/td&gt;      &lt;td&gt;Alameda&lt;/td&gt;      &lt;td&gt;Berkeley Unified&lt;/td&gt;      &lt;td&gt;Willard Middle&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;23&lt;/td&gt;      &lt;td&gt;30&lt;/td&gt;      &lt;td&gt;22&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;75&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;3&lt;/th&gt;      &lt;td&gt;1611436056865&lt;/td&gt;      &lt;td&gt;Alameda&lt;/td&gt;      &lt;td&gt;Berkeley Unified&lt;/td&gt;      &lt;td&gt;Willard Middle&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;1&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;2&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;3&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;4&lt;/th&gt;      &lt;td&gt;1611436056865&lt;/td&gt;      &lt;td&gt;Alameda&lt;/td&gt;      &lt;td&gt;Berkeley Unified&lt;/td&gt;      &lt;td&gt;Willard Middle&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;5&lt;/td&gt;      &lt;td&gt;15&lt;/td&gt;      &lt;td&gt;13&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;33&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;\'\r\n\r\n\r\n\n\nThis is the head of my dataframe. I was wondering if there was a way to combine the each grade level together. That is, combine those 5 rows into 1 row. Help??\n'
"I'm preparing a database analysis from the website:\n\n https://www.kaggle.com/c/predicting-loan-default/data \n\nMy variable emp_length takes about 3000 different values. Some values are the same or have the same keyword (for example account, accountant, accounting, account specialist, acct.). Some words contain errors or are shortcuts. I want to decrease the values to simplify the names and encode as numeric values. I tried to find keywords with text mining in R, but I'm not convinced that this is the right way. Does anyone have any idea for this? \n"
'Can any one tell my what that part (town = thisLine[:thisLine.index(\'(\')-1])exactly do?\n\ndef get_list_of_university_towns():\n\'\'\'Returns a DataFrame of towns and the states they are in from the \nuniversity_towns.txt list. The format of the DataFrame should be:\nDataFrame( [ ["Michigan", "Ann Arbor"], ["Michigan", "Yipsilanti"] ], \ncolumns=["State", "RegionName"]  )\n\nThe following cleaning needs to be done:\n1. For "State", removing characters from "[" to the end.\n2. For "RegionName", when applicable, removing every character from " (" to the end.\n3. Depending on how you read the data, you may need to remove newline character \'\\n\'. \'\'\'\n\ndata = []\nstate = None\nstate_towns = []\nwith open(\'university_towns.txt\') as file:\n    for line in file:\n        thisLine = line[:-1]\n        if thisLine[-6:] == \'[edit]\':\n            state = thisLine[:-6]\n            continue\n        if \'(\' in line:\n            town = thisLine[:thisLine.index(\'(\')-1]\n            state_towns.append([state,town])\n        else:\n            town = thisLine\n            state_towns.append([state,town])\n        data.append(thisLine)\ndf = pd.DataFrame(state_towns,columns = [\'State\',\'RegionName\'])\nreturn df\n\n\nget_list_of_university_towns()\n'
"I want to create a data frame of 2 columns from a list.\nThe list contains: States and Region names by order\nThe States are the ones with 'edit' infront of their names, and the other words are region names\nfor exemple here the state is alabama and her regions names are Auburn, Florence, ... until we reach the second state which is 'Alaska'.\n\n['Alabama[edit]',\n 'Auburn',\n 'Florence',\n 'Jacksonville',\n 'Livingston',\n 'Montevallo',\n 'Troy',\n 'Tuscaloosa',\n 'Tuskegee',\n 'Alaska[edit]',\n 'Fairbanks',\n 'Arizona[edit]',\n 'Flagstaff',\n 'Tempe',\n 'Tucson',\n 'Arkansas[edit]',\n 'Arkadelphia',\n....\n\n\nAnd the data frame columns will be States and Region names.\n\n\n\nHere is my code :\n\n    for i in range(len(list)):\n    if 'edit' in list[i]:\n        university['state'][i:]=re.sub('\\[.+','',list[i])\n    else:\n        university['regionName'][i]=list[i]\n\n"
"I have a list with around 1500 number (1 to 1500) and I want to get all the possible Permutation out of it to do some calculations and choose the smallest number out of all of it.\n\nProblem is that the number of possibilites as you can figure is wayyy wayyy too big and my computer just freeze whiel running the code so I have to make a forced restart. Also my RAM is 8GB so it should be big enough (?) or so I hope.\n\nTo limit it I can specify a start point but that won't reduce it much.\n\nAlso it's a super important thing but I feel so lost. what do you think should I do to make it run ?\n"
'I have data like this:\n\n\n\nI want to remove the rows in user ID_2 column which the data is more than and less than 5 digit\n'
'Please help here as I am able to do it in Jupyter simply but not able to run this program via functions.\nMake a function simple\nuse p,r,t variables to get input from the users as principle, time and rate of interest.\nuse si variable to calculate simple interest.\ndisplay simple interest from si variable and also the current objects defined in the namespaces.\nPlease find the skeletal program below and adjust it according to my need:\ndef simple(p,r,t):\n    p=\n    r=\n    t=\n    si=\n    return si\n\n'
'I would like some help converting a tuple in format [(X, Y, Z),(..) of type  (string, string, int)]  to a JSON file in the format:  \n\n{\n    "name": "X",\n    "children": [{\n        "name": "Y",\n        "value": Z\n    }]\n}\n\n\nI have at least 1M values to convert and at the moment I have attempted using a key for the dictionary:  \n\nb = (dict(zip(keys,row)) for row in tuples)  \n\n\nusing the JSON library  \n\nprint (json.dumps(list(b)))  \n\n\nhowever this yields a JSON in the format \n\n[{"type": "y", "name": "z", "count": z},...  \n\n\nPreferably I would like the Y and Z values to be nested under children and the X value to be used once per unique string. \n\n   {\n    "name": "X",\n    "children": [{\n        "name": "Y",\n        "value": Z\n    },\n                {\n        "name": "Y2",\n        "value": Z2\n    }]\n   }\n\n'
"Trying to accomplish the below, but in Pandas and optimally as this is abysmally slow on 1,000,000 records using my current numpy-ish approach\n\nSample of data in csv:\n\n03530c9197f5845,5/9/14,EmailOpen\n03530c9197f5845,5/12/14,EmailOpen\n03530c9197f5845,5/19/14,EmailOpen\n03530c9197f5845,5/20/14,EmailOpen \n03530c9197f5845,5/27/14,EmailOpen\n03530c9197f5845,5/29/14,EmailOpen\n03530c9197f5845,6/2/14,PageView\n03530c9197f5845,6/2/14,WebVisit  \n...\n\n\nwhere first column is user hash, second column is event date and third column is event type\n\nExample output needed:\n\n03530c9197f5845, 0, 0, 0, 0, 1, 3, 5, 2, 3, 5\n89430s7897r3821, 1, 4, 3, 0, 0, 0, 2, 2, 1, 0\n...\n\n\nwhere the first column is the userid, and the proceeding columns are the counts of each unique event type (there are around 8 event types) as features for training.\n\nThe code below accomplishes what I'm looking for, but slowly:\n\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv('myfile.csv').as_matrix()\nusersData = [v[0] for v in data]\nactionsData = [v[2] for v in data]\n\nactions = set(actionsData)\nusers = set(usersData)\n\ntarget = np.zeros((len(users), len(actions)))\nfor i, user in enumerate(users):\n    for j, action in enumerate(actions):\n        val = len([d for d in data if d[0] == user and d[2] == action])\n        target[i][j] = val\n\n\nTried using groupby and count_values on a dataframe, but stuck on expanding the results to a count vector a.k.a. the count columns\n"
'How to define the value of i and j in one or two sentences.If i and j match, it should return true. The value of i and j can be a "Noun phrase", or "Pronoun. example: \n\nlist= [(\'اکلیل\', \'N\'), (\'احمد\', \'Ne\'), (\'استاد\', \'N\'), (\'پوهنتون\', \'Ne\'), (\'کابل\', \n    \'N\'), (\'است\', \'V\'), (\'.\', \'PUNC\'), (\'او\', \'PRO\'), (\'هر\', \'DET\'), (\'روز\', \'N\'), \n    (\'ساعت\', \'Ne\'), (\'۸\', \'NUM\'), (\'بجه\', \'N\'), (\'به\', \'P\'), (\'کار\', \'N\'), \n    (\'میرود\', \'V\'), (\'.\', \'PUNC\')]\n\n    for i, j in list: \n        if i == \'NP\'and j ==\'PRO\':\n           ......\n\n'
'I have an array I want to iterate through. The array consists of strings consisting of numbers and signs. \nlike this: €110.5M\nI want to loop over it and remove all Euro sign and also the M and return that array with the strings as ints.\nHow would I do this knowing that the array is a column in a table?\n'
"Already looked at Google and past questions in here and couldn't find a simple and well-explained answer.\n\nHow to loop through a large number in python?\n\ne.g. I would like to check how long it will take to loop between 1 and 1.2e+34 and print the final result.\n\nNot sure how to write for look/while loop for this and I have no idea how to write 1.2e+34 in python language (For i = 1 to i = ?).\n"
