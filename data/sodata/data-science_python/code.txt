"# import BatchNormalization\nfrom keras.layers.normalization import BatchNormalization\n\n# instantiate model\nmodel = Sequential()\n\n# we can think of this chunk as the input layer\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\n\n# we can think of this chunk as the hidden layer    \nmodel.add(Dense(64, init='uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\n\n# we can think of this chunk as the output layer\nmodel.add(Dense(2, init='uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('softmax'))\n\n# setting up the optimization of our weights \nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd)\n\n# running the fitting\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n"
"$ cat /proc/sys/vm/overcommit_memory\n0\n\n&gt;&gt;&gt; 156816 * 36 * 53806 / 1024.0**3\n282.8939827680588\n\n$ echo 1 &gt; /proc/sys/vm/overcommit_memory\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.zeros((156816, 36, 53806), dtype='uint8')\n&gt;&gt;&gt; a.nbytes\n303755101056\n"
'def create_model():\n   model = Sequential()\n   model.add(Dense(64, input_dim=14, init=\'uniform\'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5)) \n   model.add(Dense(64, init=\'uniform\'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5))\n   model.add(Dense(2, init=\'uniform\'))\n   model.add(Activation(\'softmax\'))\n   return model\n\ndef train():\n   model = create_model()\n   sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n   model.compile(loss=\'binary_crossentropy\', optimizer=sgd)\n\n   checkpointer = ModelCheckpoint(filepath="/tmp/weights.hdf5", verbose=1, save_best_only=True)\n   model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose=2, callbacks=[checkpointer])\n\ndef load_trained_model(weights_path):\n   model = create_model()\n   model.load_weights(weights_path)\n'
"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(data, labels, validation_split=0.2)\n"
"model = Sequential()\nact = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(act)\n"
'Latex subscript:\n\n$x_{2}$\n\nLatex superscript:\n\n$x^{2}$\n'
"df_grouped.reset_index(name='count')\n\nprint (df_grouped.rename('count').reset_index())\n\n   A  Amt  count\n0  1   30      4\n1  1   20      3\n2  1   40      2\n3  2   40      3\n4  2   10      2\n\ndf_grouped1 =  dftest.groupby(['A','Amt']).size().reset_index(name='count')\n\nprint (df_grouped1)\n   A  Amt  count\n0  1   20      3\n1  1   30      4\n2  1   40      2\n3  2   10      2\n4  2   40      3\n"
"scaled_features = data.copy()\n\ncol_names = ['Age', 'Weight']\nfeatures = scaled_features[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n\nscaled_features[col_names] = features\nprint(scaled_features)\n\n\n        Age  Name    Weight\n0 -1.411004     3  1.202703\n1  0.623041     4  0.042954\n2  0.787964     6 -1.245657\n"
'import nltk\nwords = set(nltk.corpus.words.words())\n\nsent = "Io andiamo to the beach with my amico."\n" ".join(w for w in nltk.wordpunct_tokenize(sent) \\\n         if w.lower() in words or not w.isalpha())\n# \'Io to the beach with my\'\n'
"df.assign(\n    timediff=df.sort_values(\n        'datetime', ascending=False\n    ).groupby(['from', 'to']).datetime.diff(-1).dt.seconds.div(60).fillna(0))\n"
"total_year.set_index('year').plot(figsize=(10,5), grid=True)\n"
'example 1: x[0] = [s0, s1, s2, ..., s6] | y[0] = s7   \nexample 2: x[1] = [s1, s2, s3, ..., s7] | y[1] = s8\n\nx.shape -&gt; (BatchSize, 7, 10) -&gt; (BatchSize, 7 step sequences, 10 features)   \ny.shape -&gt; (BatchSize, 10)\n\nx.shape -&gt; (Users, 52, 10).    \n\nx[0] = [s0, s1, s2, ......., s51] -&gt; user with the longest sequence    \nx[1] = [0 , 0 , s0, s1, ..., s49] -&gt; user with a shorter sequence\n'
"numpy.einsum('az,bz,cz,dz -&gt; abcd', A, B, C, D)\n\nnumpy.einsum('az,bz-&gt;ab', P.U[0], P.U[1])\n\nnumpy.einsum('az,bz,cz-&gt;abc', P.U[0], P.U[1], P.U[2])\n\nnp.allclose(np.einsum('az,bz-&gt;ab', P.U[0], P.U[1]), P.totensor())\n&gt;&gt;&gt; True\n"
"rng = pd.to_datetime(['2015-01-10','2015-01-12','2015-01-13'])\ndata = pd.DataFrame({'a': range(3)}, index=rng)  \nprint(data)\n             a\n 2015-01-10  0\n 2015-01-12  1\n 2015-01-13  2\n\na = data.index.to_series().diff()\nprint(a)\n\n2015-01-10      NaT\n2015-01-12   2 days\n2015-01-13   1 days\ndtype: timedelta64[ns]\n\na = pd.Series(data.index).diff()\nprint(a)\n 0      NaT\n 1   2 days\n 2   1 days\ndtype: timedelta64[ns]\n\n"
"print (df[pd.to_numeric(df.col, errors='coerce').isnull()])\n\ndf = pd.DataFrame({'B':['a','7','8'],\n                   'C':[7,8,9]})\n\nprint (df)\n   B  C\n0  a  7\n1  7  8\n2  8  9\n\nprint (df[pd.to_numeric(df.B, errors='coerce').isnull()])\n   B  C\n0  a  7\n\ndf = pd.DataFrame({'B':['a',7, 8],\n                   'C':[7,8,9]})\n\nprint (df)\n   B  C\n0  a  7\n1  7  8\n2  8  9\n\nprint (df[df.B.apply(lambda x: isinstance(x, str))])\n   B  C\n0  a  7\n"
'import ipdb\nipdb.set_trace()\n\nfrom ipdb import launch_ipdb_on_exception\n\ndef silly():\n    my_list = [1,2,3]\n    for i in xrange(4):\n        print my_list[i]\n\nif __name__ == "__main__":\n    with launch_ipdb_on_exception():\n        silly()\n\n      5         for i in xrange(4):\n----&gt; 6             print my_list[i]\n      7\n\nipdb&gt; i\n3\n'
'In [307]: df\nOut[307]:\n  sex  age     name\n0   M   40      Max\n1   F   35     Anna\n2   M   29      Joe\n3   F   18    Maria\n4   F   23  Natalie\n\nIn [308]: df.query("20 &lt;= age &lt;= 30 and sex==\'F\'")\nOut[308]:\n  sex  age     name\n4   F   23  Natalie\n\nIn [309]: df[(df[\'age\']&gt;=20) &amp; (df[\'age\']&lt;=30) &amp; (df[\'sex\']==\'F\')]\nOut[309]:\n  sex  age     name\n4   F   23  Natalie\n\nIn [315]: conditions = {\'name\':\'Joe\', \'sex\':\'M\'}\n\nIn [316]: q = \' and \'.join([\'{}=="{}"\'.format(k,v) for k,v in conditions.items()])\n\nIn [317]: q\nOut[317]: \'name=="Joe" and sex=="M"\'\n\nIn [318]: df.query(q)\nOut[318]:\n  sex  age name\n2   M   29  Joe\n'
'df = df.dropna()\n\ndf.dropna(inplace= True)\n'
'def hash_trick(features, n_features):\n     for f in features:\n         res = np.zero_like(features)\n         h = usual_hash_function(f) # just the usual hashing\n         index = h % n_features  # find the modulo to get index to place f in res\n         if single_bit_hash_function(f) == 1:  # to reduce collision\n             res[index] += 1\n         else:\n             res[index] -= 1 # &lt;--- this will make values to become negative\n\n     return res \n'
'from pygal import graph\nimport pygal\n\ndef draw(measurement_1, measurement_2 ,x_labels):\n  graph = pygal.Line()\n  graph.x_labels = x_labels\n\n  for key, value in measurement_1.iteritems():\n     ##\n     if "component1":\n        graph.add(key, value, stroke_style={\'width\': 5, \'dasharray\': \'3, 6\', \'linecap\': \'round\', \'linejoin\': \'round\'})\n     else:\n     ##\n        graph.add(key, value)\n  for key, value in measurement_2.iteritems():\n      graph.add(key, value, secondary=True)\n\n  return graph.render_data_uri()\n'
"y = [7342.1301373073857, 6881.7109460930769, 6531.1657905495022,  \n6356.2255554679778, 6209.8382535595829, 6094.9052166741121, \n5980.0191582610196, 5880.1869867848218, 5779.8957906367368, \n5691.1879324562778, 5617.5153566271356, 5532.2613232619951, \n5467.352265375117, 5395.4493783888756, 5345.3459908298091, \n5290.6769823693812, 5243.5271656371888, 5207.2501206569532, \n5164.9617535255456]\n\nx = range(1, len(y)+1)\n\nfrom kneed import KneeLocator\nkn = KneeLocator(x, y, curve='convex', direction='decreasing')\nprint(kn.knee)\n5\n\nimport matplotlib.pyplot as plt\nplt.xlabel('number of clusters k')\nplt.ylabel('Sum of squared distances')\nplt.plot(x, y, 'bx-')\nplt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n"
'train_cols = train.columns\ntest_cols = test.columns\n\ncommon_cols = train_cols.intersection(test_cols)\ntrain_not_test = train_cols.difference(test_cols)\n'
'estimator = KerasClassifier(build_fn=SS.create_model, nb_epoch=10, verbose=0)\n'
'def answer_eight():\n    counties=census_df[census_df[\'SUMLEV\']==50]\n    # this is wrong you\'re passing the df here multiple times\n    regions = counties[(counties[counties[\'REGION\']==1]) | (counties[counties[\'REGION\']==2])]\n    # here you\'re doing it again\n    washingtons = regions[regions[regions[\'COUNTY\']].str.startswith("Washington")]\n    # here you\'re doing here again also\n    grew = washingtons[washingtons[washingtons[\'POPESTIMATE2015\']]&gt;washingtons[washingtons[\'POPESTIMATES2014\']]]\n    return grew[grew[\'STNAME\'],grew[\'COUNTY\']]\n\ndef answer_eight():\n    counties=census_df[census_df[\'SUMLEV\']==50]\n    regions = counties[(counties[\'REGION\']==1]) | (counties[\'REGION\']==2])]\n    washingtons = regions[regions[\'COUNTY\'].str.startswith("Washington")]\n    grew = washingtons[washingtons[\'POPESTIMATE2015\']&gt;washingtons[\'POPESTIMATES2014\']]\n    return grew[[\'STNAME\',\'COUNTY\']]\n'
'def mape_vectorized(a, b): \n    mask = a &lt;&gt; 0\n    return (np.fabs(a[mask] - b[mask])/a[mask]).mean()\n\ndef mape_vectorized_v2(a, b): \n    mask = a &lt;&gt; 0\n    return (np.fabs(a - b)/a)[mask].mean() \n\nIn [217]: a = np.random.randint(-10,10,(10000))\n     ...: b = np.random.randint(-10,10,(10000))\n     ...: \n\nIn [218]: %timeit mape(a,b)\n100 loops, best of 3: 11.7 ms per loop\n\nIn [219]: %timeit mape_vectorized(a,b)\n1000 loops, best of 3: 273 µs per loop\n\nIn [220]: %timeit mape_vectorized_v2(a,b)\n1000 loops, best of 3: 220 µs per loop\n'
"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('C:/Dataset.csv')\ndf['split'] = np.random.randn(df.shape[0], 1)\n\nmsk = np.random.rand(len(df)) &lt;= 0.7\n\ntrain = df[msk]\ntest = df[~msk]\n"
'scale_y_continuous(labels=lambda l: ["%d%%" % (v * 100) for v in l])\n'
"import numpy as np\nfrom scipy import interpolate\nimport matplotlib.pyplot as plt\n\ndef f(x, y):\n    return x**2 + x*y + 2*y + 1\n\nNx, Ny = 21, 17\nxl = np.linspace(-3, 3, Nx)\nyl = np.linspace(-2, 2, Ny)\n\nX, Y = np.meshgrid(xl, yl)\nZ = f(X, Y)\nzl = np.arange(np.floor(Z.min()), np.ceil(Z.max())+1, 2)\n\ndZdy, dZdx = np.gradient(Z, yl, xl, edge_order=1)\nV = np.hypot(dZdx, dZdy)\n\naxe = plt.axes(projection='3d')\naxe.plot_surface(X, Y, Z, cmap='jet', alpha=0.5)\naxe.view_init(elev=25, azim=-45)\n\naxe = plt.contour(X, Y, Z, zl, cmap='jet')\naxe.axes.quiver(X, Y, dZdx, dZdy, V, units='x', pivot='tip', cmap='jet')\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n\naxe = plt.contour(X, Y, V, 10, cmap='jet')\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n\nSdZx = np.cumsum(dZdx, axis=1)*np.diff(xl)[0]\nSdZy = np.cumsum(dZdy, axis=0)*np.diff(yl)[0]\n\nZhat = np.zeros(SdZx.shape)\nfor i in range(Zhat.shape[0]):\n    for j in range(Zhat.shape[1]):\n        Zhat[i,j] += np.sum([SdZy[i,0], -SdZy[0,0], SdZx[i,j], -SdZx[i,0]])\n        \nZhat += Z[0,0] - Zhat[0,0]\n\nr = np.stack([X.ravel(), Y.ravel()]).T\nSx = interpolate.CloughTocher2DInterpolator(r, dZdx.ravel())\nSy = interpolate.CloughTocher2DInterpolator(r, dZdy.ravel())\n\nNx, Ny = 200, 200\nxli = np.linspace(xl.min(), xl.max(), Nx)\nyli = np.linspace(yl.min(), yl.max(), Nx)\nXi, Yi = np.meshgrid(xli, yli)\nri = np.stack([Xi.ravel(), Yi.ravel()]).T\n\ndZdxi = Sx(ri).reshape(Xi.shape)\ndZdyi = Sy(ri).reshape(Xi.shape)\n\nSdZxi = np.cumsum(dZdxi, axis=1)*np.diff(xli)[0]\nSdZyi = np.cumsum(dZdyi, axis=0)*np.diff(yli)[0]\n\nZhati = np.zeros(SdZxi.shape)\nfor i in range(Zhati.shape[0]):\n    for j in range(Zhati.shape[1]):\n        Zhati[i,j] += np.sum([SdZyi[i,0], -SdZyi[0,0], SdZxi[i,j], -SdZxi[i,0]])\n        \nZhati += Z[0,0] - Zhati[0,0]\n\nq = np.full(dZdx.shape, False)\nq[0:6,5:11] = True\nq[-6:,-6:] = True\ndZdx[q] = np.nan\ndZdy[q] = np.nan\n\nq2 = ~np.isnan(dZdx.ravel())\nr = np.stack([X.ravel(), Y.ravel()]).T[q2,:]\nSx = interpolate.CloughTocher2DInterpolator(r, dZdx.ravel()[q2])\nSy = interpolate.CloughTocher2DInterpolator(r, dZdy.ravel()[q2])\n\nVl = np.arange(0, 11, 1)\naxe = plt.contour(X, Y, np.hypot(dZdx, dZdy), Vl, cmap='jet')\naxe.axes.contour(Xi, Yi, np.hypot(dZdxi, dZdyi), Vl, cmap='jet', linestyles='-.')\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n\nSx = interpolate.Rbf(r[:,0], r[:,1], dZdx.ravel()[q2], function='thin_plate')\nSy = interpolate.Rbf(r[:,0], r[:,1], dZdy.ravel()[q2], function='thin_plate')\n\ndZdxi = Sx(ri[:,0], ri[:,1]).reshape(Xi.shape)\ndZdyi = Sy(ri[:,0], ri[:,1]).reshape(Xi.shape)\n\ndf = pd.read_excel('./Trial-Wireup 2.xlsx')\nx = df['X'].to_numpy()\ny = df['Y'].to_numpy()\nz = df['Delay'].to_numpy()\n\nr = np.stack([x, y]).T\n\n#S = interpolate.CloughTocher2DInterpolator(r, z)\n#S = interpolate.LinearNDInterpolator(r, z)\nS = interpolate.Rbf(x, y, z, epsilon=0.1, function='thin_plate')\n\nN = 200\nxl = np.linspace(x.min(), x.max(), N)\nyl = np.linspace(y.min(), y.max(), N)\nX, Y = np.meshgrid(xl, yl)\n\n#Zp = S(np.stack([X.ravel(), Y.ravel()]).T)\nZp = S(X.ravel(), Y.ravel())\nZ = Zp.reshape(X.shape)\n\ndZdy, dZdx = np.gradient(Z, yl, xl, edge_order=1)\n\nSdZx = np.nancumsum(dZdx, axis=1)*np.diff(xl)[0]\nSdZy = np.nancumsum(dZdy, axis=0)*np.diff(yl)[0]\n\nZhat = np.zeros(SdZx.shape)\nfor i in range(Zhat.shape[0]):\n    for j in range(Zhat.shape[1]):\n        #Zhat[i,j] += np.nansum([SdZy[i,0], -SdZy[0,0], SdZx[i,j], -SdZx[i,0]])\n        Zhat[i,j] += np.nansum([SdZx[0,N//2], SdZy[i,N//2], SdZx[i,j], -SdZx[i,N//2]])\n        \nZhat += Z[100,100] - Zhat[100,100]\n\nlz = np.linspace(0, 5000, 20)\naxe = plt.contour(X, Y, Z, lz, cmap='jet')\naxe = plt.contour(X, Y, Zhat, lz, cmap='jet', linestyles=':')\naxe.axes.plot(x, y, '.', markersize=1)\naxe.axes.set_aspect('equal')\naxe.axes.grid()\n\nZhat[i,j] += np.nansum([SdZy[N//2,0], SdZx[N//2,j], SdZy[i,j], -SdZy[N//2,j]])\n"
'pip install pyramid-arima\n'
"df = df[df['A'] &gt;= 0]\n\nselRows = df[df['A'] &lt; 0].index\ndf = df.drop(selRows, axis=0)\n\ndf['A'] &gt;= 0\n\ndf[df['A'] &gt;= 0]\n\nselRows = df[df['A'] &lt; 0].index\ndf = df.drop(selRows, axis=0)\n\ndf[df['A'] &lt; 0]\n"
"from sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\niris = datasets.load_iris()\n# Select 2 features / variable for the 2D plot that we are going to create.\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('y label here')\nax.set_xlabel('x label here')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title(title)\nax.legend()\nplt.show()\n\nfrom sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.decomposition import PCA\n\niris = datasets.load_iris()\n\nX = iris.data  \ny = iris.target\n\npca = PCA(n_components=2)\nXreduced = pca.fit_transform(X)\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(Xreduced, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = Xreduced[:, 0], Xreduced[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('PC2')\nax.set_xlabel('PC1')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title('Decison surface using the PCA transformed/projected features')\nax.legend()\nplt.show()\n\nfrom sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom mpl_toolkits.mplot3d import Axes3D\n\niris = datasets.load_iris()\nX = iris.data[:, :3]  # we only take the first three features.\nY = iris.target\n\n#make it binary classification problem\nX = X[np.logical_or(Y==0,Y==1)]\nY = Y[np.logical_or(Y==0,Y==1)]\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, Y)\n\n# The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.\n# Solve for w3 (z)\nz = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]\n\ntmp = np.linspace(-5,5,30)\nx,y = np.meshgrid(tmp,tmp)\n\nfig = plt.figure()\nax  = fig.add_subplot(111, projection='3d')\nax.plot3D(X[Y==0,0], X[Y==0,1], X[Y==0,2],'ob')\nax.plot3D(X[Y==1,0], X[Y==1,1], X[Y==1,2],'sr')\nax.plot_surface(x, y, z(x,y))\nax.view_init(30, 60)\nplt.show()\n"
"import numpy as np \nimport pandas as pd\nfrom pandas.tools.plotting import scatter_matrix\n\n#first make some fake data with same layout as yours\ndata = pd.DataFrame(np.random.randn(100, 10), columns=['x1', 'x2', 'x3',\\\n                    'x4','x5','x6','x7','x8','x9','x10'])\n\n#now plot using pandas \nscatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')\n\nimport corner_plot as cp\n\ncp.corner_plot(data.as_matrix(),axis_labels=data.columns,nbins=10,\\\n              figsize=(7,7),scatter=True,fontsize=10,tickfontsize=7)\n"
'# Train the model (a.k.a. `fit` training data to it).\nlr.fit(train_df[features_columns], train_df["target"])\n# Use the model to make predictions based on testing data.\ny_pred = lr.predict(test_df[feature_columns])\n# Compare the predicted y values to actual y values.\naccuracy = (y_pred == test_df["target"]).mean()\n'
'cols = ["workclass", "native-country"]\ndf[cols]=df[cols].fillna(df.mode().iloc[0])\n\ndf[cols]=df[cols].fillna(mode.iloc[0])\n\ndf[cols]=df.filter(cols).fillna(mode.iloc[0])\n\ndf = pd.DataFrame({\'workclass\':[\'Private\',\'Private\',np.nan, \'another\', np.nan],\n                   \'native-country\':[\'United-States\',np.nan,\'Canada\',np.nan,\'United-States\'],\n                   \'col\':[2,3,7,8,9]})\n\nprint (df)\n   col native-country workclass\n0    2  United-States   Private\n1    3            NaN   Private\n2    7         Canada       NaN\n3    8            NaN   another\n4    9  United-States       NaN\n\nmode = df.filter(["workclass", "native-country"]).mode()\nprint (mode)\n  workclass native-country\n0   Private  United-States\n\ncols = ["workclass", "native-country"]\ndf[cols]=df[cols].fillna(df.mode().iloc[0])\nprint (df)\n   col native-country workclass\n0    2  United-States   Private\n1    3  United-States   Private\n2    7         Canada   Private\n3    8  United-States   another\n4    9  United-States   Private\n'
"df.groupby(level=0).apply(max)\n\ndf = pd.DataFrame(np.random.randn(5,4), columns = l)\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps','Lower'])\ndf = pd.DataFrame(df.unstack())\n"
"df = pd.DataFrame({'val':[1,11,130,670]})\ndf.val.round(decimals=-2)\n\n0      0\n1      0\n2    100\n3    700\nName: val, dtype: int64\n\ndf = pd.DataFrame({'val':[1,11,130,670], 'x':[1,11,150,900]})\ndf.round({'val':-2})\n"
"import pandas as pd\nimport json\nfrom folium.plugins import FastMarkerCluster\n\nrome_lat, rome_lng = 41.9028, 12.4964\nwith open(&quot;file_name.json&quot;, 'r') as f:\n  # create a new DataFrame\n  samples = pd.DataFrame(json.loads(f.read()))        \n# init the folium map object\nmy_map = folium.Map(location=[rome_lat, rome_lng], zoom_start=5)\n# add all the point from the file to the map object using FastMarkerCluster\nmy_map.add_child(FastMarkerCluster(samples[['latitude', 'longitude']].values.tolist()))\n# save the map \nmy_map.save(&quot;save_file.html&quot;)\n"
"L = ['ABC', 'DEF', 'GHI', 'DEF', 'ABC']\n\nprint (np.unique(L, return_inverse=True)[1])\n[0 1 2 1 0]\n\nprint (pd.factorize(L)[0])\n[0 1 2 1 0]\n"
'def get_fake(n):\n    df = pd.DataFrame(np.random.rand(n * 2).reshape(-1, 2))\n    df.loc[:, 1] += 1\n    return df\n\ndf1 = get_fake(200)\ndf2 = get_fake(90000)\n\nfrom collections import defaultdict\nresult = defaultdict(list)\nfor index, start, stop in df1.itertuples():\n    subdf = df2[(start &lt; df2.iloc[:, 0]) &amp; (df2.iloc[:, 1] &lt; stop)]\n    result[(start, stop)] += subdf.values.tolist()\n'
'model = Sequential()\nmodel.add(Dense(60, input_dim=7, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(55, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(50, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(45, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(30, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(20, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(1, kernel_initializer=\'normal\'))\nmodel.load_weights("kwhFinal.h5")\nmodel.compile(loss=\'mse\', optimizer=\'adam\', metrics=[rmse])\n\nmodel.save("kwhFinal.h5")\n\nfrom keras.models import load_model\nmodel=load_model("kwhFinal.h5")\n'
'chart.configure_legend(\n    gradientLength=400,\n    gradientThickness=30\n) \n'
"s=df.values\nt=np.all((s==s[:,None])|np.isnan(s),-1)\nidx=pd.DataFrame(t).where(t).stack().index\n# we get the pair for each row\ndf=df.reindex(idx.get_level_values(1))\n# reorder our df to the idx we just get \ndf.groupby(level=[0]).transform('first').groupby(level=1).first()\n# using two times groupby with first , get what we need .\nOut[217]: \n   col1  col2  col3  col4\n0   3.0   4.0   7.0   7.0\n1   2.0   3.0   8.0   8.0\n2   8.0   6.0   9.0   9.0\n3   4.0   NaN   NaN   NaN\n4   2.0   3.0   8.0   8.0\n5   3.0   4.0   7.0   7.0\n6   NaN   5.0   7.0   6.0\n"
's.fillna(s.expanding(1).mean())\n\nimport numpy as np\nimport numba\nfrom numba import jit\n\n\n@jit(nopython=True)\ndef rolling_fill(a): \n    for i, e in enumerate(a):\n        if np.isnan(e):\n            a[i] = np.mean(a[:i])\n\nts_values = np.array([17.0, np.NaN, 12.0, np.NaN, 18.0])\nrolling_fill(ts_values)\nprint(ts_values)\n\n[17.         17.         12.         15.33333333 18.        ]\n\n&gt;&gt;&gt; s = pd.Series([0, 1, np.nan, 5])\n&gt;&gt;&gt; s\n0    0.0\n1    1.0\n2    NaN\n3    5.0\ndtype: float64\n&gt;&gt;&gt; s.interpolate()\n0    0.0\n1    1.0\n2    3.0\n3    5.0\ndtype: float64\n'
"model = Sequential()\nmodel.add(Dense(128, input_dim=14, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd3 = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='binary_crossentropy', optimizer=sgd3)\nmodel.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose = 2)\n"
'[285 compo.]---[neural network]---+---[neural network]---[output 285 compo.]\n                                  |\n                       [4 compo.]-+\n'
'import pandas as pd\ndf2 = pd.merge(df0,df1, left_index=True, right_index=True)\n'
'scalerY = StandardScaler().fit(trainY)  # fit y scaler\npipeline.fit(trainX, scalerY.transform(trainY))  # fit your pipeline to scaled Y\ntestY = scalerY.inverse_transform(pipeline.predict(testX))  # predict and rescale\n'
'titanic["Sex"] == "male"\n\ntitanic = pd.DataFrame({\'Sex\':[\'male\',\'female\', \'male\']})\nprint (titanic)\n      Sex\n0    male\n1  female\n2    male\n\nprint (titanic["Sex"] == "male")\n0     True\n1    False\n2     True\nName: Sex, dtype: bool\n\ntitanic.loc[titanic["Sex"] == "male", "Sex"] = 0\nprint (titanic)\n\n0       0\n1  female\n2       0\n\nprint (titanic.loc[titanic["Sex"] == "male", "Sex"])\n0    male\n2    male\nName: Sex, dtype: object\n\ntitanic = pd.DataFrame({\'Sex\':[\'male\',\'female\', \'male\']})\ntitanic["Sex"] = titanic["Sex"].map({\'male\':0, \'female\':1})\nprint (titanic)\n   Sex\n0    0\n1    1\n2    0\n\ntitanic = pd.DataFrame({\'Sex\':[\'male\',\'female\', \'male\']}, index=[\'a\',\'b\',\'c\'])\nprint (titanic)\n      Sex\na    male\nb  female\nc    male\n\ntitanic.loc["a", "Sex"] = 0\nprint (titanic)\n      Sex\na       0\nb  female\nc    male\n\ntitanic.loc[["a", "b"], "Sex"] = 0\nprint (titanic)\n    Sex\na     0\nb     0\nc  male\n'
'all_data = []\nfor f in glob.glob("output/test/*.xlsx"):\n    all_data.append(pd.read_excel(f))\n\ndf = pd.concat(all_data, ignore_index=True)\n\ng = map(pd.read_excel, glob.glob("output/test/*.xlsx"))\ndf = pd.concat(list(g), ignore_index=True)\n'
"a = defaultdict(lambda:None)\nlongest = defaultdict(int)\ncurrent = defaultdict(int)\nfor i, j, k in df.itertuples(index=False):\n    if a[(i, j)] == k - 1:\n        current[(i, j)] += 1 if current[(i, j)] else 2\n        longest[(i, j)] = max(longest[(i, j)], current[(i, j)])\n    else:\n        current[(i, j)] = 0\n        longest[(i, j)] |= 0\n    a[(i, j)] = k\n\npd.concat(\n    [pd.Series(d) for d in [longest, current]],\n    axis=1, keys=['longest_streak', 'last_streak']\n).rename_axis(['user_id', 'product_id']).reset_index()\n\n   user_id  product_id  longest_streak  last_streak\n0        1           1               3            3\n1        1           2               0            0\n2        2           1               3            3\n3        3           1               2            0\n"
'   A  B   C   D\n0  0  1   2   3\n1  4  5   6   7\n2  8  9  10  11\n\ndf.drop([0, 1]) # Here 0 and 1 are the index of the rows\n\n   A  B   C   D\n2  8  9  10  11\n'
"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n     'Actual': [18.442, 15.4233, 20.6217, 16.7, 18.185], \n     'Forecasted': [19.6377, 13.1665, 19.3992, 17.4557, 14.0053]\n})\n\narr = np.zeros(3)\ndf_arr = pd.DataFrame({'Predictors': [arr]})\n\nresult = pd.merge(\n    df,\n    df_arr,\n    how='left',\n    left_index=True, # Merge on both indexes, since right only has 0...\n    right_index=True # all the other rows will be NaN\n)\n\n&gt;&gt;&gt; print(result)\n    Actual  Forecasted       Predictors\n0  18.4420     19.6377  [0.0, 0.0, 0.0]\n1  15.4233     13.1665              NaN\n2  20.6217     19.3992              NaN\n3  16.7000     17.4557              NaN\n4  18.1850     14.0053              NaN\n\n&gt;&gt;&gt; result.loc[0, 'Predictors']\narray([0., 0., 0.])\n\n&gt;&gt;&gt; result.loc[1, 'Predictors'] # actually contains a NaN value\nnan \n"
'ValueError: For multi-metric scoring, the parameter refit must be set to a scorer \nkey to refit an estimator with the best parameter setting on the whole data and make\nthe best_* attributes available for that metric. If this is not needed, refit should \nbe set to False explicitly. True was passed.\n'
'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.utils.extmath import safe_sparse_dot\n\nnp.random.seed(20181001)\n\na, b = 1, 4\nx = np.linspace(0, 2, 100).reshape(-1, 1)\ny = a*x**2 + b*x + np.random.normal(scale=0.2, size=(100,1))\n\npoly = PolynomialFeatures(degree=2, include_bias=True)\nxp = poly.fit_transform(x)\nprint(\'We can see that the new features are now [1, x, x**2]:\')\nprint(f\'xp.shape: {xp.shape}\')\nprint(f\'xp[-5:]:\\n{xp[-5:]}\', end=\'\\n\\n\')\n# Scale the `x` columns so we obtain similar results.\nxp[:, 1] *= np.sqrt(2)\n\nridge = Ridge(alpha=0, fit_intercept=False, solver=\'cholesky\')\nridge.fit(xp, y)\n\nkrr = KernelRidge(alpha=0, kernel=\'poly\', degree=2, gamma=1, coef0=1)\nkrr.fit(x, y)\n\n# Let\'s try to reproduce some of the involved steps for the different models.\nridge_K = safe_sparse_dot(xp, xp.T)\nkrr_K = krr._get_kernel(x)\nprint(\'The computed kernels are (alomst) similar:\')\nprint(f\'Max. kernel difference: {np.abs(ridge_K - krr_K).max()}\', end=\'\\n\\n\')\nprint(\'Predictions slightly differ though:\')\nprint(f\'Max. difference: {np.abs(krr.predict(x) - ridge.predict(xp)).max()}\', end=\'\\n\\n\')\n\n# Let\'s see if the fit changes if we provide `x**2, x, 1` instead of `x**2, sqrt(2)*x, 1`.\nxp_2 = xp.copy()\nxp_2[:, 1] /= np.sqrt(2)\nridge_2 = Ridge(alpha=0, fit_intercept=False, solver=\'cholesky\')\nridge_2.fit(xp_2, y)\nprint(\'Using features "[x**2, x, 1]" instead of "[x**2, sqrt(2)*x, 1]" predictions are (almost) the same:\')\nprint(f\'Max. difference: {np.abs(ridge_2.predict(xp_2) - ridge.predict(xp)).max()}\', end=\'\\n\\n\')\nprint(\'Interpretability of the coefficients changes though:\')\nprint(f\'ridge.coef_[1:]: {ridge.coef_[0, 1:]}, ridge_2.coef_[1:]: {ridge_2.coef_[0, 1:]}\')\nprint(f\'ridge.coef_[1]*sqrt(2): {ridge.coef_[0, 1]*np.sqrt(2)}\')\nprint(f\'Compare with: a, b = ({a}, {b})\')\n\nplt.plot(x.ravel(), y.ravel(), \'o\', color=\'skyblue\', label=\'Data\')\nplt.plot(x.ravel(), ridge.predict(xp).ravel(), \'-\', label=\'Ridge\', lw=3)\nplt.plot(x.ravel(), krr.predict(x).ravel(), \'--\', label=\'KRR\', lw=3)\nplt.grid()\nplt.legend()\nplt.show()\n\nWe can see that the new features are now [x, x**2]:\nxp.shape: (100, 3)\nxp[-5:]:\n[[1.         1.91919192 3.68329762]\n [1.         1.93939394 3.76124885]\n [1.         1.95959596 3.84001632]\n [1.         1.97979798 3.91960004]\n [1.         2.         4.        ]]\n\nThe computed kernels are (alomst) similar:\nMax. kernel difference: 1.0658141036401503e-14\n\nPredictions slightly differ though:\nMax. difference: 0.04244651134471766\n\nUsing features "[x**2, x, 1]" instead of "[x**2, sqrt(2)*x, 1]" predictions are (almost) the same:\nMax. difference: 7.15642822779472e-14\n\nInterpretability of the coefficients changes though:\nridge.coef_[1:]: [2.73232239 1.08868872], ridge_2.coef_[1:]: [3.86408737 1.08868872]\nridge.coef_[1]*sqrt(2): 3.86408737392841\nCompare with: a, b = (1, 4)\n'
'!apt-get install libgeos-3.5.0\n!apt-get install libgeos-dev\n!pip install https://github.com/matplotlib/basemap/archive/master.zip\n\n!pip install pyproj==1.9.6\n\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\n%matplotlib inline\n'
'def main():\n      do_something()\n\nif __name__ == "__main__":\n      main()\n\n\n[[source]]\nurl = \'https://pypi.python.org/simple\'\nverify_ssl = true\nname = \'pypi\'\n\n[requires]\npython_version = \'2.7\'\n\n[common-packages]\nscipy &gt;= "0.17.0"\npandas \n\n[model1-packages]\nnumpy &gt;= "1.11.0"\n\n[model2-packages]\nnumpy == "1.0.0"\n\n.DEFAULT_GOAL := run\n\ninit:\n    pipenv --three install\n    pipenv shell\n\nanalyze:\n    flake8 ./src\n\nrun_tests:\n    pytest --cov=src test/jobs/\n\nrun:\n\n    # cleanup\n    find . -name \'__pycache__\' | xargs rm -rf\n\n    # run the job\n    python main.py \n\nmake run\n'
'df.books.eq(0).astype(int).groupby(df.nationality).sum()\n\nnationality\nFrance    1\nUK        0\nUSA       1\nName: books, dtype: int64\n'
"customer_id_array = []\nwith open('online_pageviews.json') as f:\n    for line in f:\n        customer_id_array.append(json.loads(line)['customer_id'])\nonline_pageviews = pd.DataFrame(customer_id_array,columns = ['customer_id'])\n"
"df_chunked[~(df_chunked['value'].ge(10))]\n#df_chunked[~(df_chunked['value']&gt;=10)] #greater or equal(the same)\n\n   index  value\n0      1    5.0\n1      2    6.0\n2      3    7.0\n3      4    NaN\n4      5    9.0\n\nprint(df.assign(greater_than_5 = df['value'].gt(5),\n          not_greater_than_5 = df['value'].le(5)))\n\n\n   index  value  greater_than_5  not_greater_than_5\n0      1    5.0           False                True\n1      2    6.0            True               False\n2      3    7.0            True               False\n3      4    NaN           False               False\n4      5    9.0            True               False\n5      6    3.0           False                True\n6      7   11.0            True               False\n7      8   34.0            True               False\n8      9   78.0            True               False\n"
'train_generator = train_datagen.flow_from_directory(\n        \'train_directory\',\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode = "categorical"\n        )\n\nclass_weights = class_weight.compute_class_weight(\n           \'balanced\',\n            np.unique(train_generator.classes), \n            train_generator.classes)\n\ncur.execute("SELECT class, count(*) FROM table group by classes order by 1")\nrows = cur.fetchall()\n\nclass_weights = {}\nfor row in rows:\n    class_weights[row[0]]=rows[0][1]/row[1] \n    #dividing the least value the current value to get the weight, \n    # so that the least value becomes 1, \n    # and other values becomes &lt; 1\n'
'import cv2\nimport imutils\n\n# edged is the edge detected image\ncnts = cv2.findContours(edged, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\ncnts = imutils.grab_contours(cnts)\ncnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n# loop over the contours\nfor c in cnts:\n    # approximate the contour\n    peri = cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n    # if our approximated contour has four points, then we\n    # can assume that we have found our screen\n    if len(approx) == 4:\n        screenCnt = approx\n        break\n'
'def request(method, url, **kwargs):\n    . . .\n    with sessions.Session() as session:\n        return session.request(method=method, url=url, **kwargs)\n\ndef request(self, method, url,\n            params=None, data=None, headers=None, cookies=None, files=None,\n            auth=None, timeout=None, allow_redirects=True, proxies=None,\n            hooks=None, stream=None, verify=None, cert=None, json=None):\n    . . .\n'
"&gt;&gt;&gt; xs = [{'Name':'A','Class':'1'},\n          {'Name':'B','Class':'1'},\n          {'Name':'C','Class':'2'}]\n&gt;&gt;&gt; sum(x.get('Class') == '1' for x in xs)\n2\n"
'xr.concat([df.to_xarray() for df in data_df], dim="dataset")\n'
"df = ohlc.assign(Close=ohlc['Close'].ffill()).bfill(axis=1)\nprint (df)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n\nohlc['Close'] = ohlc['Close'].ffill()\ndf = ohlc.bfill(axis=1)\nprint (df)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n"
"#loading my train dataset into python\ntrain = pd.read_csv('/Users/sohaib/Downloads/train.csv')\ntest = pd.read_csv('/Users/sohaib/Downloads/test.csv')\n\n#factors that will predict the price\ndesired_factors = ['OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\n\n#set my model to DecisionTree\nmodel = DecisionTreeRegressor()\n\n#set prediction data to factors that will predict, and set target to SalePrice\ntrain_data = train[desired_factors]\ntest_data = test[desired_factors]\ntarget = train.SalePrice\n\n#fitting model with prediction data and telling it my target\nmodel.fit(train_data, target)\n\nmodel.predict(test_data.head())\n"
'x_test = onehotencoder.transform(x_test)\nx_pred = regressor.predict(x_test)\n'
'def DaviesBouldin(X, labels):\n    n_cluster = len(np.bincount(labels))\n    cluster_k = [X[labels == k] for k in range(n_cluster)]\n    centroids = [np.mean(k, axis = 0) for k in cluster_k]\n    variances = [np.mean([euclidean(p, centroids[i]) for p in k]) for i, k in enumerate(cluster_k)]\n    db = []\n\n    for i in range(n_cluster):\n        for j in range(n_cluster):\n            if j != i:\n                db.append((variances[i] + variances[j]) / euclidean(centroids[i], centroids[j]))\n\n    return(np.max(db) / n_cluster)\n'
"df.loc[df.groupby('YearReleased')['count'].idxmax()]\nOut[445]: \n    id  YearReleased     Artist  count\n1  169          2015    Rihanna      3\n4  172          2016    Rihanna      3\n7  175          2017  EdSheeran      2\n"
"mydict = {80178.37739073468: (2, 5),\n81623.18006660603: (13, 14),\n82583.3021359235: (8, 16),\n83491.34222215343: (9, 8),\n83724.73005402873: (8, 14),\n83856.2891246289: (7, 8),\n83984.92825308126: (6, 5),\n84314.30519882984: (13, 16),\n84577.4110193269: (4, 11),\n86338.86146117302: (6, 20)}\n\nfrom mpl_toolkits.mplot3d import Axes3D\n# Get your x, y and z variables\nz = list(mydict.keys())\n# unpack the dictionary values into two variables, x and y\nx,y = zip(*mydict.values())\n# plot\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.scatter(x, y, z)\n"
'In [41]: np.random.seed(0)\n\nIn [42]: a = np.sort(np.random.randint(0,100,(10000)))\n\nIn [43]: bins = [20,40,60,80]\n\nIn [46]: idx = np.searchsorted(a, bins)\n\nIn [47]: np.split(a,idx)\nOut[47]: \n[array([ 0,  0,  0, ..., 19, 19, 19]),\n array([20, 20, 20, ..., 39, 39, 39]),\n array([40, 40, 40, ..., 59, 59, 59]),\n array([60, 60, 60, ..., 79, 79, 79]),\n array([80, 80, 80, ..., 99, 99, 99])]\n'
"df.groupby(['date','letter'])['value'].apply(lambda g: g[g &lt;= g.quantile(.1)].mean())\n"
"idx = np.searchsorted(B, A, side='right')\nresult = A-B[idx-1] # substract one for proper index\n"
"df = pd.read_json('data/data.json')\n\ndf['time'] = pd.to_datetime(df['time'])\n#timedelta is a more appropriate data type for session_duration\ndf['session_duration'] = pd.to_timedelta(df['session_duration'], unit='s')\n\n# Example filtering\ndf_short_duration = df[df['session_duration'].dt.total_seconds() &lt;= 60]\n\n# Example creating histogram\ndf_hist = df_short_duration.groupby(df['time'].dt.hour).count()\n# Now just plot df_hist as a bar chart using matplotlib, might be something like plt.bar(df_hist.index, df_hist['count'])\n"
"df1 = df.loc[df['y'].abs().groupby(df['x']).idxmax()]\nprint (df1)\n         x    y\n1     1510 -125\n4214  1555   69\n\ndf1 = df.loc[df.assign(y=df['y'].abs()).groupby('x')['y'].idxmax()]\n\nprint (df)\n         x    y\n1     1510 -125\n1     1510 -125\n5     1510 -124\n4210  1555   68\n4214  1555   69\n\ny = df['y'].abs()\ndf1 = df[y.groupby(df['x']).transform('max') == y]\nprint (df1)\n         x    y\n1     1510 -125\n1     1510 -125\n4214  1555   69\n"
"class ChainEstimator(BaseEstimator,ClassifierMixin):\n    def __init__(self,est1,est2):\n        self.est1 = est1\n        self.est2 = est2\n\n    def fit(self,X,y):\n        self.est1.fit(X,y)\n        self.est2.fit(X,y)\n        return self\n\n    def predict(self,X):\n        ans = np.zeros((len(X),)) - 1\n        probs = self.est1.predict_proba(X)       #averaging confidence of Ada &amp; SVC\n        conf_samples = np.any(probs&gt;=.8,axis=1)  #samples with &gt;80% confidence\n        ans[conf_samples] = np.argmax(probs[conf_samples,:],axis=1) #Predicted Classes of confident samples\n        if conf_samples.sum()&lt;len(X):            #Use est2 for non-confident samples\n            ans[~conf_samples] = self.est2.predict(X[~conf_samples])\n        return ans\n\nest1 = VotingClassifier(estimators=[('ada',AdaBoostClassifier()),('svm',SVC(probability=True))],voting='soft')\nest2 = VotingClassifier(estimators=[('dt',DecisionTreeClassifier()),('knn',KNeighborsClassifier())])\nclf = ChainEstimator(est1,est2).fit(X_train,Y_train)\nans = clf.predict(X_test)\n\ndef fit(self,X,y):\n    self.est1.fit(X,y)\n    self.est1_perf = cross_val_score(self.est1,X,y,cv=4,scoring='f1_macro')\n    self.est2.fit(X,y)\n    self.est2_perf = cross_val_score(self.est2,X,y,cv=4,scoring='f1_macro')\n    return self\n"
"new_ids_simple = {}\nnew_ids_map = {}\ni=0\nfor d_ph_nm, r_vid in df[['d_ph_nm','r_vid']].values:\n\n    if all([x not in new_ids_map.keys() for x in [d_ph_nm, r_vid]]):\n        new_ids_map[d_ph_nm] = i\n        new_ids_map[r_vid] = i\n        new_ids_simple[i] = {'d_ph_nm':[d_ph_nm],'r_vid':[r_vid]}\n        i+=1\n    else:\n        # retrieving unique value:\n        None\n        for x in [d_ph_nm, r_vid]:\n            if x in new_ids_map.keys():\n                new_val = new_ids_map.get(x)\n            else:\n                new_key = x\n        # setting unique value\n        new_ids_map[new_key] = new_val\n        new_ids_simple[new_val]['d_ph_nm'].append(d_ph_nm)\n        new_ids_simple[new_val]['r_vid'].append(r_vid)\n\n\nmap_df = pd.DataFrame.from_dict(new_ids_simple,orient='index')\nmap_df.index.names = ['ID']\nmap_df['d_ph_nm'] = map_df['d_ph_nm'].apply(pd.unique)\nmap_df['r_vid'] = map_df['r_vid'].apply(pd.unique)\n\n# To convert from an array to a string (inside the df)\nmap_df['r_vid'] = map_df['r_vid'].apply(', '.join)\n\n                                           d_ph_nm             r_vid\nID                                                                  \n0                                       6123340277           DQLA853\n1   6999045706, 6999340277, 7123104672, 9123010121  DQLA851, DQLCT41\n2                                       6999290277  DQLA852, DQLA962\n3                                       6222232026  DQLC181, DQLT381\n4                           9912332326, 9912336579           DQLC860\n"
"df['rank'] = df.groupby('type').value.rank(ascending=False).sub(1).astype(int)\n\nprint(df)\n\n  type  value rank\n0    A      5    1\n1    A      7    0\n2    B      2    1\n3    B      6    0\n4    A      1    2\n"
'df1 = df.stack().unstack()\n\n# print(df1)\n\n  Arts/Culture Politics\n0            c        c\n1            a        b\n2            a        b\n3            c        a\n'
"In [1]: import pandas as pd\n\nIn [2]: df = pd.read_csv('census.csv')\n\nIn [3]: unique_counties = df.groupby('STNAME')['COUNTY'].nunique()\n\nIn [4]: unique_counties\nOut[4]: \nSTNAME\nAlabama                  68\nAlaska                   30\nArizona                  16\nArkansas                 76\nCalifornia               59\nColorado                 65\nConnecticut               9\nDelaware                  4\nDistrict of Columbia      2\nFlorida                  68\nGeorgia                 160\nHawaii                    6\nIdaho                    45\nIllinois                103\nIndiana                  93\nIowa                    100\nKansas                  106\nKentucky                121\nLouisiana                65\nMaine                    17\nMaryland                 25\nMassachusetts            15\nMichigan                 84\nMinnesota                88\nMississippi              83\nMissouri                116\nMontana                  57\nNebraska                 94\nNevada                   18\nNew Hampshire            11\nNew Jersey               22\nNew Mexico               34\nNew York                 63\nNorth Carolina          101\nNorth Dakota             54\nOhio                     89\nOklahoma                 78\nOregon                   37\nPennsylvania             68\nRhode Island              6\nSouth Carolina           47\nSouth Dakota             67\nTennessee                96\nTexas                   255\nUtah                     30\nVermont                  15\nVirginia                134\nWashington               40\nWest Virginia            56\nWisconsin                73\nWyoming                  24\nName: COUNTY, dtype: int64\n"
'class TransformerWrapper(sklearn.base.BaseEstimator):\n\n    def __init__(self, func):\n        self._func = func\n\n    def fit(self, *args, **kwargs):\n        return self\n\n    def transform(self, X, *args, **kwargs):\n        return self._func(X, *args, **kwargs)\n\n@TransformerWrapper\ndef foo(x):\n  return x*2\n\ndef foo(x):\n  return x*2\n\nfoo = TransformerWrapper(foo)\n\nfrom sklearn.preprocessing import FunctionTransformer\n\n@FunctionTransformer\ndef foo(x):\n  return x*2\n'
'start = df.columns.get_loc(con_start())\nstop = df.columns.get_loc(con_stop())\n\ndf.iloc[:, start:stop + 1]\n\nstart = con_start()\nstop = con_stop()\n\nc = df.columns.values\nm = (start &lt;= c) &amp; (stop &gt;= c)\n\ndf.loc[:, m]\n'
"df_highest_countries[year] = pd.DataFrame(highest_countries)\n\ndf_highest_countries[continent+str(year)] = pd.DataFrame(highest_countries)\n\nfinaldf = pd.concat(df_highest_countries, join='outer').reset_index(drop=True)\n\ndf = pd.concat(continents_list)\n\n# MELT FOR YEAR VALUES IN COLUMN\ndf = pd.melt(df, id_vars=['Continent', 'Country Name', 'Country Code'], var_name='Year')\n\n# AGGREGATE HIGHEST VALUE AND MERGE BACK TO ORIGINAL SET\ndf = df.groupby(['Continent', 'Year'])['value'].max().reset_index().\\\n        merge(df, on=['Continent', 'Year', 'value'])\n\n# PIVOT BACK TO YEAR COLUMNS\npvt = df.pivot_table(index=['Continent', 'Country Name', 'Country Code'],\n                     columns='Year', values='value').reset_index()\n"
'X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\n\nrkf = RepeatedKFold(n_splits=2, n_repeats=1, random_state=2652124)\nfor train_index, test_index in rkf.split(X):\n  print("TRAIN:", train_index, "TEST:", test_index)\n\nTRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\n\nTRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\n'
'array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267f425f8&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267f1bb38&gt;],\n       [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267ec95c0&gt;,\n        &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f4267ef9080&gt;]],\n      dtype=object)\n\nfig, axes = plt.subplots(2, 2)\n\nax = sns.boxplot(x="Species", y="SepalLengthCm", data=iris, orient=\'v\', \n    ax=axes[0, 0])\nax = sns.boxplot(x="Species", y="SepalWidthCm", data=iris, orient=\'v\', \n    ax=axes[0, 1])\nax = sns.boxplot(x="Species", y="PetalLengthCm", data=iris, orient=\'v\', \n    ax=axes[1, 0])\nax = sns.boxplot(x="Species", y="PetalWidthCm", data=iris, orient=\'v\', \n    ax=axes[1, 1])\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Make some fake data:\nn_series = 3\nn_observations = 5\nx = np.arange(n_observations)\ndata = np.random.random((n_observations,n_series))\n\n\n# Plotting:\n\nfig, ax = plt.subplots(figsize=(20,5))\n\n# Determine bar widths\nwidth_cluster = 0.7\nwidth_bar = width_cluster/n_series\n\nfor n in range(n_series):\n    x_positions = x+(width_bar*n)-width_cluster/2\n    ax.bar(x_positions, data[:,n], width_bar, align='edge')\n"
"df_corr = (df.merge(df, on=['date', 'action'])\n           .query('user_x != user_y')\n           .pivot_table(index='user_x', columns='user_y', aggfunc='size'))\n\nuser_y  user_1  user_2  user_3\nuser_x                        \nuser_1     NaN     2.0     NaN\nuser_2     2.0     NaN     1.0\nuser_3     NaN     1.0     NaN\n\nmask = np.triu_indices_from(df_corr)\ndf_corr.values[mask] = np.nan\n\nuser_y  user_1  user_2  user_3\nuser_x                        \nuser_1     NaN     NaN     NaN\nuser_2     2.0     NaN     NaN\nuser_3     NaN     1.0     NaN\n"
"r = requests.get('https://coronavirus.data.gov.uk/downloads/csv/coronavirus-cases_latest.csv', stream=True)\npd.read_csv(r.raw)\n"
'# Load the "autoreload" extension\n%load_ext autoreload\n\n# always reload modules marked with "%aimport"\n%autoreload 1\n\nimport os\nimport sys\n\n# add the \'src\' directory as one where we can import modules\nsrc_dir = os.path.join(os.getcwd(), os.pardir, \'src\')\nsys.path.append(src_dir)\n\n# import my method from the source code\n%aimport preprocess.build_features\n'
'scala&gt; println(subtreeToString(model.topNode))\n\nIf (feature 2 &lt;= -0.762712) (Prob: 35.35 %)\n Predict: 1.0\nElse (feature 2 &gt; -0.762712) (Prob: 64.65 %)\n If (feature 3 &lt;= 0.333333) (Prob: 52.24 %)\n  If (feature 0 &lt;= -0.666667) (Prob: 92.11 %)\n   Predict: 3.0\n  Else (feature 0 &gt; -0.666667) (Prob: 7.89 %)\n   If (feature 2 &lt;= 0.322034) (Prob: 94.59 %)\n    Predict: 2.0\n   Else (feature 2 &gt; 0.322034) (Prob: 5.41 %)\n    If (feature 3 &lt;= 0.166667) (Prob: 50.00 %)\n     Predict: 3.0\n    Else (feature 3 &gt; 0.166667) (Prob: 50.00 %)\n     Predict: 2.0\n Else (feature 3 &gt; 0.333333) (Prob: 47.76 %)\n  Predict: 3.0\n'
'&gt;&gt;&gt; import ipaddress\n&gt;&gt;&gt; i = ipaddress.ip_address(\'10.9.8.7\')\n&gt;&gt;&gt; int(i)\n168364039\n\n&gt;&gt;&gt; int(ipaddress.ip_address("2001:db8::1"))\n42540766411282592856903984951653826561\n'
"df['dif'] = df.groupby('id')['day'].diff(-1) * (-1)\nprint (df)\n    id        day  total_amount      dif\n0    1 2015-07-09          1000 105 days\n1    1 2015-10-22           100  21 days\n2    1 2015-11-12           200  15 days\n3    1 2015-11-27          2392  19 days\n4    1 2015-12-16           123      NaT\n5    7 2015-07-09           200   0 days\n6    7 2015-07-09          1000  49 days\n7    7 2015-08-27        100018  90 days\n8    7 2015-11-25          1000      NaT\n9    8 2015-08-27          1000 102 days\n10   8 2015-12-07         10000  42 days\n11   8 2016-01-18           796  73 days\n12   8 2016-03-31         10000      NaT\n13  15 2015-09-10          1500  20 days\n14  15 2015-09-30          1000      NaT\n\ndf['diff'] = df.groupby('id')['day'].apply(lambda x: x.shift(-1) - x)\nprint (df)\n    id        day  total_amount     diff\n0    1 2015-07-09          1000 105 days\n1    1 2015-10-22           100  21 days\n2    1 2015-11-12           200  15 days\n3    1 2015-11-27          2392  19 days\n4    1 2015-12-16           123      NaT\n5    7 2015-07-09           200   0 days\n6    7 2015-07-09          1000  49 days\n7    7 2015-08-27        100018  90 days\n8    7 2015-11-25          1000      NaT\n9    8 2015-08-27          1000 102 days\n10   8 2015-12-07         10000  42 days\n11   8 2016-01-18           796  73 days\n12   8 2016-03-31         10000      NaT\n13  15 2015-09-10          1500  20 days\n14  15 2015-09-30          1000      NaT\n\ndf['diff'] = df.groupby('id')['day'].diff(-1) * (-1) / np.timedelta64(1, 'h')\nprint (df)\n    id        day  total_amount    diff\n0    1 2015-07-09          1000  2520.0\n1    1 2015-10-22           100   504.0\n2    1 2015-11-12           200   360.0\n3    1 2015-11-27          2392   456.0\n4    1 2015-12-16           123     NaN\n5    7 2015-07-09           200     0.0\n6    7 2015-07-09          1000  1176.0\n7    7 2015-08-27        100018  2160.0\n8    7 2015-11-25          1000     NaN\n9    8 2015-08-27          1000  2448.0\n10   8 2015-12-07         10000  1008.0\n11   8 2016-01-18           796  1752.0\n12   8 2016-03-31         10000     NaN\n13  15 2015-09-10          1500   480.0\n14  15 2015-09-30          1000     NaN\n\ndf['diff'] = df.groupby('id')['day'].apply(lambda x: x.shift(-1) - x) / \n                                     np.timedelta64(1, 'h')\nprint (df)\n    id        day  total_amount    diff\n0    1 2015-07-09          1000  2520.0\n1    1 2015-10-22           100   504.0\n2    1 2015-11-12           200   360.0\n3    1 2015-11-27          2392   456.0\n4    1 2015-12-16           123     NaN\n5    7 2015-07-09           200     0.0\n6    7 2015-07-09          1000  1176.0\n7    7 2015-08-27        100018  2160.0\n8    7 2015-11-25          1000     NaN\n9    8 2015-08-27          1000  2448.0\n10   8 2015-12-07         10000  1008.0\n11   8 2016-01-18           796  1752.0\n12   8 2016-03-31         10000     NaN\n13  15 2015-09-10          1500   480.0\n14  15 2015-09-30          1000     NaN\n"
" users.language = np.where( users.language !='en', 'non-english', 'english' )\n"
'import numpy_indexed as npi\nunique_training_images = npi.unique(train)\n\nindices = npi.group_by(train).split(np.arange(len(train)))\n'
'salesdata.Outlet_Size.dropna().unique()\n'
'df.apply(pd.Series.nunique)\n\nage       4\ngroup     3\nitem      3\nperson    4\ndtype: int64\n'
"df = pd.pivot(index=df.groupby('label').cumcount(), columns=df.label, values=df.a).fillna(0)\nprint (df)\nlabel    0    1\n0      2.0  1.0\n1      7.0  4.0\n2      0.0  6.0\n\ndf.plot.bar()\n\ndf = df.groupby(['label', 'a']).size().unstack(0, fill_value=0)\n\ndf.plot.bar()\n"
"def check_type(homicide_df):\n    for i, age in homicide_df['Perpetrator Age'].iteritems():\n        if type(age) is str:\n            print(i, age, type(age))\n\nhomicide_df = pd.DataFrame({'Perpetrator Age':[10, '15', 'aa']})\nprint (homicide_df)\n  Perpetrator Age\n0              10\n1              15\n2              aa\n\n\ndef check_type(homicide_df):\n    for i, age in homicide_df['Perpetrator Age'].iteritems():\n        if type(age) is str:\n            print(i, age, type(age))\n\ncheck_type(homicide_df)\n1 15 &lt;class 'str'&gt;\n2 aa &lt;class 'str'&gt;\n\ndef check_type(homicide_df):\n    return homicide_df.loc[homicide_df['Perpetrator Age'].apply(type)==str,'Perpetrator Age']\n\nprint  (check_type(homicide_df))\n1    15\n2    aa\nName: Perpetrator Age, dtype: object\n\nprint ((homicide_df['Perpetrator Age'].apply(type)==str).all())\nTrue\n\nhomicide_df = pd.DataFrame({'Perpetrator Age':['10', '15']})\n\nhomicide_df['Perpetrator Age'] = homicide_df['Perpetrator Age'].astype(int)\nprint (homicide_df)\n\n   Perpetrator Age\n0               10\n1               15\n\nprint (homicide_df['Perpetrator Age'].dtypes)\nint32\n\nhomicide_df = pd.DataFrame({'Perpetrator Age':[10, '15', 'aa']})\n\nhomicide_df['Perpetrator Age']=pd.to_numeric(homicide_df['Perpetrator Age'], errors='coerce')\nprint (homicide_df)\n   Perpetrator Age\n0             10.0\n1             15.0\n2              NaN\n\nhomicide_df['Perpetrator Age'] = homicide_df['Perpetrator Age'].fillna(0).astype(int)\nprint (homicide_df)\n   Perpetrator Age\n0               10\n1               15\n2                0\n"
'        [2.0  1.3  0.0]\nsigma = [1.3  2.0  1.3]\n        [0.0  1.3  2.0]\n\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\n\ndef f(x, a, b):\n    return a + b*x\n\n\nx = np.array([1, 2, 3])\ny = np.array([2, 0.75, 0])\nsig = np.array([[2.0, 1.3, 0.0],\n                [1.3, 2.0, 1.3],\n                [0.0, 1.3, 2.0]])\n\nparams, pcov = curve_fit(f, x, y, sigma=sig)\n\ny_errors = f(x, *params) - y\n\nplt.plot(x, y, \'ko\', label="data")\nplt.plot(x, f(x, *params), linewidth=2.5, label="fitted curve")\nplt.vlines(x, y, f(x, *params), \'r\')\n\nfor k in range(3):\n    plt.annotate(s=r"$e_{%d}$" % (k+1), xy=(x[k]-0.05, y[k]+0.5*y_errors[k]), ha=\'right\')\n\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\nplt.axis(\'equal\')\nplt.grid()\nplt.legend(framealpha=1, shadow=True)\nplt.show()\n\n        [ 2.0   1.3  -1.0]\nsigma = [ 1.3   2.0  -1.3]\n        [-1.0  -1.3   2.0]\n'
"data = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\nrdd = spark.sparkContext.parallelize(data).zipWithIndex()\n\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\ndf = spark.createDataFrame(rdd, ['letters', 'id'])\n\nsize_of_df = df.count()/4\n\ndf_part0 = df.where(col('id') &lt; size_of_df) \ndf_part1 = df.where((col('id') &gt; size_of_df) &amp; \n                    (col('id') &lt;= 2*size_of_df))\ndf_part2 = df.where((col('id') &gt; 2*size_of_df) &amp; \n                    (col('id') &lt;= 3*size_of_df))\ndf_part3 = df.where((col('id') &gt; 3*size_of_df) &amp; \n                    (col('id') &lt;= 4*size_of_df))\n"
"df.reindex(collist, axis=1)\n\ndf.reindex(rowlist, axis=0)\n\ndf.reindex(index=rowlist, columns=collist)\n\ndf = pd.DataFrame(data=np.random.rand(5,5),columns=list('ABCDE'),index=list('abcde'))\n\ndf\n          A         B         C         D         E\na  0.460537  0.174788  0.167554  0.298469  0.630961\nb  0.728094  0.275326  0.405864  0.302588  0.624046\nc  0.953253  0.682038  0.802147  0.105888  0.089966\nd  0.122748  0.954955  0.766184  0.410876  0.527166\ne  0.227185  0.449025  0.703912  0.617826  0.037297\n\ncollist = ['B','D','E']\n\nrowlist = ['a','c']\n\ndf[collist]\n\n          B         D         E\na  0.174788  0.298469  0.630961\nb  0.275326  0.302588  0.624046\nc  0.682038  0.105888  0.089966\nd  0.954955  0.410876  0.527166\ne  0.449025  0.617826  0.037297\n\ndf.loc[rowlist]\n\n          A         B         C         D         E\na  0.460537  0.174788  0.167554  0.298469  0.630961\nc  0.953253  0.682038  0.802147  0.105888  0.089966\n"
'from sklearn import datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy import interp\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n\nclassifier = DecisionTreeClassifier()\n\ny_score = classifier.fit(X_train, y_train).predict(X_test)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc["micro"] = auc(fpr["micro"], tpr["micro"])\n\n#ROC curve for a specific class here for the class 2\nroc_auc[2]\n\n0.94852941176470573\n'
'df["Cost"] = df["Cost"].str[1:].astype(float)\n'
'def get_activations(clf, X):\n        hidden_layer_sizes = clf.hidden_layer_sizes\n        if not hasattr(hidden_layer_sizes, "__iter__"):\n            hidden_layer_sizes = [hidden_layer_sizes]\n        hidden_layer_sizes = list(hidden_layer_sizes)\n        layer_units = [X.shape[1]] + hidden_layer_sizes + \\\n            [clf.n_outputs_]\n        activations = [X]\n        for i in range(clf.n_layers_ - 1):\n            activations.append(np.empty((X.shape[0],\n                                         layer_units[i + 1])))\n        clf._forward_pass(activations)\n        return activations\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef readpgm(name):\n    with open(name) as f:\n        lines = f.readlines()\n\n    # Ignores commented lines\n    for l in list(lines):\n        if l[0] == '#':\n            lines.remove(l)\n\n    # Makes sure it is ASCII format (P2)\n    assert lines[0].strip() == 'P2' \n\n    # Converts data to a list of integers\n    data = []\n    for line in lines[1:]:\n        data.extend([int(c) for c in line.split()])\n\n    return (np.array(data[3:]),(data[1],data[0]),data[2])\n\ndata = readpgm('/location/of/file.pgm')\n\nplt.imshow(np.reshape(data[0],data[1])) # Usage example\n"
"from sklearn.feature_extraction import DictVectorizer\nfrom collections import Counter, OrderedDict\n\nFile_1 = ('aaa', 'xyz', 'cccc', 'dddd', 'aaa')\nFile_2 = ('abc', 'aaa')\n\nv = DictVectorizer()\n\n# discover corpus and vectorize file word frequencies in a single pass\nX = v.fit_transform(Counter(f) for f in (File_1, File_2))\n\n# or, if you have a pre-defined corpus and/or would like to restrict the words you consider\n# in your matrix, you can do\n\n# Corpus = ('aaa', 'bbb', 'cccc', 'dddd', 'xyz')\n# v.fit([OrderedDict.fromkeys(Corpus, 1)])\n# X = v.transform(Counter(f) for f in (File_1, File_2))\n\n# X is a sparse matrix, but you can access the A property to get a dense numpy.ndarray \n# representation\nprint(X)\nprint(X.A)\n"
'http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n\nhttp://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html\n'
"df = pd.DataFrame.drop('Meter ID', axis=1)\n\ndf = pd.DataFrame.drop(['Meter ID', 'abc'], axis=1)\n\ndel df\n\ndf = None\n"
'pd.isnull(train_data).sum() &gt; 0\n\nportfolio_id      False\ndesk_id           False\noffice_id         False\npf_category       False\nstart_date        False\nsold               True\ncountry_code      False\neuribor_rate      False\ncurrency          False\nlibor_rate         True\nbought             True\ncreation_date     False\nindicator_code    False\nsell_date         False\ntype              False\nhedge_value       False\nstatus            False\nreturn            False\ndtype: bool\n'
'df.Type=df.Type.astype(\'category\', categories=[\'type1\',\'type2\',\'type3\',\'type4\'])\ndf\nOut[200]: \n    Type\n0  type1\n1  type2\n2  type3\npd.get_dummies(df["Type"], prefix="type")\nOut[201]: \n   type_type1  type_type2  type_type3  type_type4\n0           1           0           0           0\n1           0           1           0           0\n2           0           0           1           0\n'
"one = df.drop(['total_ethnicities'],1).values\n# Select the values other than total_ethnicities\ntwo = df['total_ethnicities'].values[:,None]\n# Select the values of total_ethnicities\ndf['diversity'] = 1 - pd.np.sum((one/two)**2, axis=1)\n# Divide the values of one by two, square them. Sum over the axis. Then subtract from 1. \ndf['diversity']\n\ntconst\ntt0000001    0.750000\ntt0000002    0.666667\ntt0000003    0.710744\ntt0000004    0.666667\ntt0000005    0.693878\nName: diversity, dtype: float64\n"
"def search_range(x, y, plot=False):\n    '''\n    Given a dataset with points (x, y) searches for a best guess for \n    initial values of 'a'.\n    '''\n    data_lenght = len(x)             # Total size of of the dataset\n    q_lenght = int(data_lenght / 4)  # Size of a quartile of the dataset\n\n    # Finding the max and min value for y in the first quartile\n    min_Q1 = (x[0], y[0])\n    max_Q1 = (x[0], y[0])\n\n    for i in range(q_lenght):\n        temp_point = (x[i], y[i])\n        if temp_point[1] &lt; min_Q1[1]:\n            min_Q1 = temp_point\n        if temp_point[1] &gt; max_Q1[1]:\n            max_Q1 = temp_point\n\n    # Finding the max and min value for y in the 4th quartile\n    min_Q4 = (x[data_lenght - 1], y[data_lenght - 1])\n    max_Q4 = (x[data_lenght - 1], y[data_lenght - 1])\n\n    for i in range(data_lenght - 1, data_lenght - q_lenght, -1):\n        temp_point = (x[i], y[i])\n        if temp_point[1] &lt; min_Q4[1]:\n            min_Q4 = temp_point\n        if temp_point[1] &gt; max_Q4[1]:\n            max_Q4 = temp_point\n\n    mean_Q4 = (((min_Q4[0] + max_Q4[0]) / 2), ((min_Q4[1] + max_Q4[1]) / 2))\n\n    # Finding max_y and min_y given the points found above\n    # Two lines need to be defined, L1 and L2.\n    # L1 will pass through min_Q1 and mean_Q4\n    # L2 will pass through max_Q1 and mean_Q4\n\n    # Calculatin slope for L1 and L2 given m = Delta(y) / Delta (x)\n    slope_L1 = (min_Q1[1] - mean_Q4[1]) / (min_Q1[0] - mean_Q4[0])\n    slope_L2 = (max_Q1[1] - mean_Q4[1]) / (max_Q1[0] -mean_Q4[0])\n\n    # Calculating y-intercepts for L1 and L2 given line equation in the form y = mx + b\n    # Float numbers are converted to int because they will be used as range for itaration\n    y_L1 = int(min_Q1[1] - min_Q1[0] * slope_L1)\n    y_L2 = int(max_Q1[1] - max_Q1[0] * slope_L2)\n\n    # Ploting L1 and L2\n    if plot:\n        L1 = [(y_L1 + slope_L1 * x) for x in data['x']]\n        L2 = [(y_L2 + slope_L2 * x) for x in data['x']]\n\n        plt.plot(data['x'], data['y'], '.')\n        plt.plot(data['x'], L1, '-', color='r') \n        plt.plot(data['x'], L2, '-', color='r') \n        plt.title('Scatterplot of Sample Data')\n        plt.xlabel('x',fontsize=12)\n        plt.ylabel('y',fontsize=12)\n        plt.show()\n\n    return y_L1, y_L2\n\ndef run_search_gradient_descent(x_values, y_values, alpha, precision, verbose=False):\n    '''\n    Runs the gradient_descent_step function and updates (a,b) until\n    the value of the cost function varies less than 'precision'.\n\n    x_values, y_values: points (x,y) of the dataset\n    alpha: learning rate for the algorithm\n    precision: value for the algorithm to stop calculation\n    '''    \n    from math import inf\n\n    a1, a2 = search_range(x_values, y_values)\n\n    best_guess = [inf, 0, 0]\n\n    for a in range(a1, a2):\n\n        cost, linear_coef, slope = run_gradient_descent(a, 0, x_values, y_values, alpha, precision)\n\n        # Saving value for cost_function and parameters (a,b)        \n        if cost &lt; best_guess[0]:\n            best_guess = [cost, linear_coef, slope]\n    if verbose:        \n        print('Cost Function = ' + str(best_guess[0]))\n        print('a = ' + str(best_guess[1]) + ' and b = ' + str(best_guess[2]))\n\n    return (best_guess[0], best_guess[1], best_guess[2])\n"
"def _print(x):\n    if (x &lt; 4).any():\n        print('Zone %s does not make setpoint' % x.name)\n        df[x.name].plot.box() #call x.name to retrieve the column name\n        plt.show()\n        print(df[x.name].describe())\n    else:\n        print('Zone %s is Normal' % x.name)\n        print('The average is %s' % x.mean())\n    print('---')\n\ndf.apply(lambda x: _print(x))\n\ns = df.apply(lambda x: not (x &lt; 4).any())\n\ndf[s[~s].index].boxplot()\nplt.show()\n\nfor col in s[~s].index:\n    df[col].plot.box()\n    plt.show()\n\nstatdf = df[s[~s].index].describe()\nprint(statdf)\n\n              a         b         d\ncount  3.000000  3.000000  3.000000\nmean   4.533333  5.133333  1.966667\nstd    4.178915  1.960442  0.901850\nmin    1.500000  3.300000  1.100000\n25%    2.150000  4.100000  1.500000\n50%    2.800000  4.900000  1.900000\n75%    6.050000  6.050000  2.400000\nmax    9.300000  7.200000  2.900000\n\nprint(df[s[s].index].mean())\n\nc    11.3\nName: mean, dtype: float64\n"
'from pyspark.ml.feature import SQLTransformer\n\ndef column_selector(columns):\n    return SQLTransformer(\n        statement="SELECT {} FROM __THIS__".format(", ".join(columns))\n    )\n\ndef na_dropper(columns):\n    return SQLTransformer(\n        statement="SELECT * FROM __THIS__ WHERE {}".format(\n            " AND ".join(["{} IS NOT NULL".format(x) for x in columns])\n        )\n    )\n'
'import numpy as np\nimport pandas as pd\nimport time as t\n\n# Example Functions\ndef foo(x):\n    return x + x\n\ndef bar(x):\n    return x * x\n\n# Example Functions for multiple columns\ndef foo2(x, y):\n    return x + y\n\ndef bar2(x, y):\n    return x * y\n\n# Create function dictionary\nfuncs = {\'foo\': foo, \'bar\': bar}\nfuncs2 = {\'foo\': foo2, \'bar\': bar2}\n\nn_rows = 1000000\n# Generate Sample Data\nnames = np.random.choice(list(funcs.keys()), size=n_rows)\nvalues = np.random.normal(100, 20, size=n_rows)\ndf = pd.DataFrame()\ndf[\'name\'] = names\ndf[\'value\'] = values\n\n# Create copy for comparison using different methods\ndf_copy = df.copy()\n\n# Modified original master function\ndef masterFunc(row, functs):\n    correctFunction = funcs[row[\'name\']]\n    return correctFunction(row[\'value\']) + 3*row[\'value\']\n\nt1 = t.time()\ndf[\'output\'] = df.apply(lambda x: masterFunc(x, funcs), axis=1)\nt2 = t.time()\nprint("Time for all rows/functions: ", t2 - t1)\n\n\n# For Functions that Can be vectorized using numpy\nt3 = t.time()\noutput_dataframe_list = []\nfor func_name, func in funcs.items():\n    df_subset = df_copy.loc[df_copy[\'name\'] == func_name,:]\n    df_subset[\'output\'] = func(df_subset[\'value\'].values) + 3 * df_subset[\'value\'].values\n    output_dataframe_list.append(df_subset)\n\noutput_df = pd.concat(output_dataframe_list)\n\nt4 = t.time()\nprint("Time for all rows/functions: ", t4 - t3)\n\n\n# Using a for loop over numpy array of values is still faster than dataframe apply using\nt5 = t.time()\noutput_dataframe_list2 = []\nfor func_name, func in funcs2.items():\n    df_subset = df_copy.loc[df_copy[\'name\'] == func_name,:]\n    col1_values = df_subset[\'value\'].values\n    outputs = np.zeros(len(col1_values))\n    for i, v in enumerate(col1_values):\n        outputs[i] = func(col1_values[i], col1_values[i]) + 3 * col1_values[i]\n\n    df_subset[\'output\'] = np.array(outputs)\n    output_dataframe_list2.append(df_subset)\n\noutput_df2 = pd.concat(output_dataframe_list2)\n\nt6 = t.time()\nprint("Time for all rows/functions: ", t6 - t5)\n'
"pd.concat([df,df.Colors.str.get_dummies(sep=', ')],1)\nOut[450]: \n   Item          Colors  Blue  Green  Red\n0  ID-1  Red,Blue,Green     1      1    1\n1  ID-2        Red,Blue     1      0    1\n2  ID-3      Blue,Green     1      1    0\n3  ID-4            Blue     1      0    0\n4  ID-5             Red     0      0    1\n"
"X_train = X_train_num.join(\n   pd.DataFrame(X_train_cat, X_train_num.index).add_prefix('cat_')\n)\n"
'import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame(np.random.random((6, 4)), columns=list(\'ABCD\'))\nfig, ax = plt.subplots()\nsns.heatmap(df.corr(method=\'pearson\'), annot=True, fmt=\'.4f\', \n            cmap=plt.get_cmap(\'coolwarm\'), cbar=False, ax=ax)\nax.set_yticklabels(ax.get_yticklabels(), rotation="horizontal")\nplt.savefig(\'result.png\', bbox_inches=\'tight\', pad_inches=0.0)\n\nimport colorsys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame(np.random.random((6, 4)), columns=list(\'ABCD\'))\ncorr = df.corr(method=\'pearson\')\n\nfig, ax = plt.subplots()\ndata = corr.values\nheatmap = ax.pcolor(data, cmap=plt.get_cmap(\'coolwarm\'), \n                    vmin=np.nanmin(data), vmax=np.nanmax(data))\nax.set_xticks(np.arange(data.shape[1])+0.5, minor=False)\nax.set_yticks(np.arange(data.shape[0])+0.5, minor=False)\nax.invert_yaxis()\nrow_labels = corr.index\ncolumn_labels = corr.columns\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(column_labels, minor=False)\n\ndef _annotate_heatmap(ax, mesh):\n    """\n    **Taken from seaborn/matrix.py**\n    Add textual labels with the value in each cell.\n    """\n    mesh.update_scalarmappable()\n    xpos, ypos = np.meshgrid(ax.get_xticks(), ax.get_yticks())\n    for x, y, val, color in zip(xpos.flat, ypos.flat,\n                                mesh.get_array(), mesh.get_facecolors()):\n        if val is not np.ma.masked:\n            _, l, _ = colorsys.rgb_to_hls(*color[:3])\n            text_color = ".15" if l &gt; .5 else "w"\n            val = ("{:.3f}").format(val)\n            text_kwargs = dict(color=text_color, ha="center", va="center")\n            # text_kwargs.update(self.annot_kws)\n            ax.text(x, y, val, **text_kwargs)\n\n_annotate_heatmap(ax, heatmap)\nplt.savefig(\'result.png\', bbox_inches=\'tight\', pad_inches=0.0)\n'
'from sklearn.datasets import load_breast_cancer\nimport pandas as pd\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\ndata = load_breast_cancer()\nX = pd.DataFrame(data=data.data, columns=data.feature_names)\n\ncount_class_0 = 300\ncount_class_1 = 300\npipe = make_pipeline(\n    SMOTE(sampling_strategy={0: count_class_0}),\n    NearMiss(sampling_strategy={1: count_class_1}\n)\n\nX_smt, y_smt = pipe.fit_resample(X, data.target)\n'
'summary(lm(Temp ~ . - 1, data = train[,3:NCOL(train)]))\n\nCall:\nlm(formula = Temp ~ . - 1, data = train[, 3:NCOL(train)])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.221940 -0.032347  0.002071  0.037048  0.167294 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)  \nMEI       0.036076   0.027983   1.289   0.2079  \nCO2       0.004640   0.008945   0.519   0.6080  \nCH4      -0.002328   0.002132  -1.092   0.2843  \nN2O      -0.014115   0.079452  -0.178   0.8603  \n`CFC-11` -0.031232   0.096693  -0.323   0.7491  \n`CFC-12`  0.035760   0.103574   0.345   0.7325  \nTSI      -0.003283   0.036861  -0.089   0.9297  \nAerosols 69.968040  33.093275   2.114   0.0435 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.07457 on 28 degrees of freedom\nMultiple R-squared:  0.9724,    Adjusted R-squared:  0.9645 \nF-statistic: 123.1 on 8 and 28 DF,  p-value: &lt; 2.2e-16\n\nX_with_constant = sm.add_constant(X)\n\nmodel = sm.OLS(y, X_with_constant).fit()\nmodel.summary()\n\nOLS Regression Results\nDep. Variable:  Temp    R-squared:  0.535\nModel:  OLS Adj. R-squared: 0.397\nMethod: Least Squares   F-statistic:    3.877\nDate:   Tue, 02 Oct 2018    Prob (F-statistic): 0.00372\nTime:   10:14:03    Log-Likelihood: 46.899\nNo. Observations:   36  AIC:    -75.80\nDf Residuals:   27  BIC:    -61.55\nDf Model:   8       \nCovariance Type:    nonrobust       \ncoef    std err t   P&gt;|t|   [0.025  0.975]\nconst   -17.8663    563.008 -0.032  0.975   -1173.064   1137.332\nMEI 0.0361  0.029   1.265   0.217   -0.022  0.095\nCO2 0.0048  0.011   0.451   0.656   -0.017  0.027\nCH4 -0.0024 0.002   -0.950  0.351   -0.007  0.003\nN2O -0.0130 0.088   -0.148  0.884   -0.194  0.168\nCFC-11  -0.0332 0.116   -0.285  0.777   -0.272  0.205\nCFC-12  0.0378  0.123   0.307   0.761   -0.215  0.290\nTSI 0.0091  0.392   0.023   0.982   -0.795  0.813\nAerosols    70.4633 37.139  1.897   0.069   -5.739  146.666\nOmnibus:    8.316   Durbin-Watson:  1.488\nProb(Omnibus):  0.016   Jarque-Bera (JB):   10.432\nSkew:   -0.535  Prob(JB):   0.00543\nKurtosis:   5.410   Cond. No.   1.06e+08\n'
"regressor = SVR(kernel='rbf')\nregressor.fit(X.reshape(-1, 1), y)\n"
'    for bin_name in (column_name + "_bin" for column_name in df_1_columns):\n      print(bin_name)\n      df_3_joined = pd.merge(df_3[df_3_op_columns], df_2[df_2[bin_name] == 2][df_1_columns], how=\'right\', on=df_1_columns, suffixes=[\'_l\', \'\'])\n      print(df_3_joined)\n\nimport pandas as pd\n\ndf_1 = pd.DataFrame(columns = [\'A\', \'B\'])\ndf_1.loc[len(df_1)] = [5,6]\ndf_1.loc[len(df_1)] = [8, 1]\ndf_1.loc[len(df_1)] = [6, 7]\ndf_1.loc[len(df_1)] = [4, 9]\ndf_1.loc[len(df_1)] = [1, 3]\ndf_1.loc[len(df_1)] = [9, 2]\ndf_1.loc[len(df_1)] = [2, 5]\n\ndf_2 = pd.DataFrame(columns = [\'A\', \'B\', \'A_bin\', \'B_bin\'])\ndf_2.loc[len(df_2)] = [5, 6, 2, 2]\ndf_2.loc[len(df_2)] = [8, 1, 1, 1]\ndf_2.loc[len(df_2)] = [6, 7, 3, 2]\ndf_2.loc[len(df_2)] = [4, 9, 3, 3]\ndf_2.loc[len(df_2)] = [1, 3, 1, 1]\ndf_2.loc[len(df_2)] = [9, 2, 1, 1]\ndf_2.loc[len(df_2)] = [2, 5, 2, 2]\n\ndf_3 = pd.DataFrame(columns = [\'A\', \'B\', \'C\', \'D\', \'A_bin\', \'B_bin\', \'C_bin\', \'D_bin\'])\ndf_3.loc[len(df_3)] = [5, 6, 2, 6, 2, 2, 1, 2]\ndf_3.loc[len(df_3)] = [8, 1, 6, 4, 1, 1, 2, 2]\ndf_3.loc[len(df_3)] = [6, 7, 3, 1, 3, 2, 1, 1]\ndf_3.loc[len(df_3)] = [4, 9, 1, 9, 3, 3, 1, 3]\ndf_3.loc[len(df_3)] = [1, 3, 8, 7, 1, 1, 3, 3]\ndf_3.loc[len(df_3)] = [9, 2, 4, 8, 1, 1, 2, 3]\ndf_3.loc[len(df_3)] = [2, 5, 9, 2, 2, 2, 3, 1]\n\nresults = {}\ndf_1_columns = list(df_1.columns)\ndf_3_op_columns = [cname for cname in list(df_3.columns) if not cname.endswith("_bin")]\nfor bin_name in (column_name + "_bin" for column_name in df_1_columns):\n    df_3_joined = pd.merge(df_3[df_3_op_columns], df_2[df_2[bin_name] == 2][df_1_columns], how=\'right\', on=df_1_columns)\n    results[bin_name] = df_3_joined\n\nfor binName, result in results.iteritems():\n    print(binName)\n    print(result)\n\nA_bin_df = results[\'A_bin\']\nprint(A_bin_df)\nB_bin_df = results[\'B_bin\']\nprint(B_bin_df)\n'
"from datetime import timedelta\n\ndf['startdate'] = pd.to_datetime(df['startdate']) - timedelta(hours=3)\ndf['enddate'] = pd.to_datetime(df['enddate']) - timedelta(hours=3)\n"
'df = pd.DataFrame([0,1,3,\'blah\',4,5,\'blah\'], columns = [\'pickup_latitude\'])\n# Identify the numeric observations\nnumeric = df[\'pickup_latitude\'].astype(str).str.isdigit()\n# Calculate mean\nmean = df.loc[numeric,\'pickup_latitude\'].mean()\n# Impute non numeric values\ndf.loc[~numeric,\'pickup_latitude\'] = mean\n# Impute outliers\ndf.loc[df[\'pickup_latitude\'] &gt;= mean, \'pickup_latitude\'] = mean\n\n\ndf[\'pickup_latitude\']\nOut[81]: \n0      0\n1      1\n2    2.6\n3    2.6\n4    2.6\n5    2.6\n6    2.6\nName: pickup_latitude, dtype: object\n\nimport pandas as pd\ndf = pd.DataFrame([0,1,3,4,5], columns = [\'pickup_latitude\'])\nif df[\'pickup_latitude\'] &gt;= df[\'pickup_latitude\'].mean():\n   df[\'pickup_latitude\'] = df[\'pickup_latitude\'].mean()\n\ndf[\'pickup_latitude\']\nOut[12]: \n0    0\n1    1\n2    3\n3    4\n4    5\nName: pickup_latitude, dtype: int64\n\ndf[\'pickup_latitude\'].mean()\nOut[13]: 2.6\n\nif df[\'pickup_latitude\'] &gt;= df[\'pickup_latitude\'].mean():\n   df[\'pickup_latitude\'] = df[\'pickup_latitude\'].mean()\n\n\nTraceback (most recent call last):\n\n  File "&lt;ipython-input-15-1135c8386dd6&gt;", line 1, in &lt;module&gt;\n    if df[\'pickup_latitude\'] &gt;= df[\'pickup_latitude\'].mean():\n\n  File "C:\\Users\\____\\AppData\\Local\\Continuum\\anaconda3\\envs\\DS\\lib\\site-packages\\pandas\\core\\generic.py", line 1121, in __nonzero__\n    .format(self.__class__.__name__))\n\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n'
"dataset['ver'] = dataset['ver'].str.replace('.', '')\n"
"df.agg(sum)\n\nfrom collections import Counter\n\ndf = pd.Series([[('かげ', 20), ('男', 17), ('たち', 15), ('お前', 14)],[('お前', 11), ('ゾロ', 10), ('うっ', 10), ('たち', 9)],[('おれ', 11), ('男', 6), ('てめえ', 6), ('お前', 5), ('首', 5)]])   \ndf = df.apply(lambda x: Counter({y[0]:y[1] for y in x}))\n\ndf\n# Out:\n# 0          {'かげ': 20, '男': 17, 'たち': 15, 'お前': 14}\n# 1          {'お前': 11, 'ゾロ': 10, 'うっ': 10, 'たち': 9}\n# 2    {'おれ': 11, '男': 6, 'てめえ': 6, 'お前': 5, '首': 5}\n# dtype: object\n\ndf.agg(sum)\n# Out:\n# Counter({'うっ': 10,\n#          'おれ': 11,\n#          'お前': 30,\n#          'かげ': 20,\n#          'たち': 24,\n#          'てめえ': 6,\n#          'ゾロ': 10,\n#          '男': 23,\n#          '首': 5})\n"
"df['home_drives'] = df.where(df.team == 'home').groupby('drive').ngroup()+1\ndf['hdwcl'] = df.where(df.home_comfy_lead == 1).groupby('home_drives').ngroup()+1\n\n    drive  team  home_comfy_lead  home_drives  hdwcl\n0       1  home                0            1      0\n1       1  home                0            1      0\n2       2  away                0            0      0\n3       2  away                0            0      0\n4       2  away                0            0      0\n5       3  home                0            2      0\n6       3  home                0            2      0\n7       3  home                1            2      1\n8       4  away                0            0      0\n9       4  away                0            0      0\n10      4  away                0            0      0\n11      5  home                1            3      2\n12      5  home                1            3      2\n13      6  away                0            0      0\n14      6  away                0            0      0\n15      7  home                1            4      3\n16      7  home                1            4      3\n"
"df['month'] = df['Date_Time'].dt.month\n"
"# fill in the missing values (similar to mutate_if from tidyverse)\ndf1 = df.select_dtypes(include=['double']).fillna(0)\ndf2 = df.select_dtypes(include=['object']).fillna('zz').astype('category')\n\ndf = pd.concat([df2.reset_index(drop = True), df1], axis = 1)\n\nprint(df)\n    name gender   age  height_in\n0   john      m  20.0       66.0\n1   mary      f   0.0       62.0\n2     zz      f  38.0       68.0\n3  larry     zz   0.0        0.0\n\n# check again for the data types\ndf.dtypes\nname         category\ngender       category\nage           float64\nheight_in     float64\ndtype: object\n"
"#round datetimes by 15 minutes\ndata['Time'] = pd.to_datetime(data['Time'])\nminutes = pd.to_timedelta(15*(data['Time'].dt.minute // 15), unit='min')\ndata['Time'] = data['Time'].dt.floor('H') + minutes\n\n#change date range for 4 values (to `12:45`)\nrng = pd.date_range('1/1/2019 12:00','1/1/2019 12:45',freq='15min')\n#create MultiIndex and reindex\nmux = pd.MultiIndex.from_product([data['ID'].unique(), rng], names=['ID','Time'])\ndata = data.set_index(['ID','Time']).reindex(mux).reset_index()\n#interpolate per groups\ndata['Value'] = (data.groupby('ID')['Value']\n                     .apply(lambda x: x.interpolate(method='linear', limit_direction='both')))\nprint (data)\n   ID                Time  Value\n0   1 2019-01-01 12:00:00    3.0\n1   1 2019-01-01 12:15:00    3.0\n2   1 2019-01-01 12:30:00    2.0\n3   1 2019-01-01 12:45:00    2.0\n4   2 2019-01-01 12:00:00    5.0\n5   2 2019-01-01 12:15:00    7.0\n6   2 2019-01-01 12:30:00    7.0\n7   2 2019-01-01 12:45:00    7.0\n\ndata['Time'] = pd.to_datetime(data['Time'])\nminutes = pd.to_timedelta(15*(data['Time'].dt.minute // 15), unit='min')\ndata['Time'] = data['Time'].dt.floor('H') + minutes\n\n#end in 13:00\nrng = pd.date_range('1/1/2019 12:00','1/1/2019 13:00',freq='15min')\nmux = pd.MultiIndex.from_product([data['ID'].unique(), rng], names=['ID','Time'])\ndata = data.set_index(['ID','Time']).reindex(mux).reset_index()\ndata['Value'] = (data.groupby('ID')['Value']\n                     .apply(lambda x: x.interpolate(method='linear', limit_direction='both')))\n\n#remove last row per groups\ndata = data[data['ID'].duplicated(keep='last')]\nprint (data)\n   ID                Time  Value\n0   1 2019-01-01 12:00:00    3.0\n1   1 2019-01-01 12:15:00    3.0\n2   1 2019-01-01 12:30:00    2.0\n3   1 2019-01-01 12:45:00    2.0\n5   2 2019-01-01 12:00:00    5.0\n6   2 2019-01-01 12:15:00    7.0\n7   2 2019-01-01 12:30:00    7.0\n8   2 2019-01-01 12:45:00    7.0\n\nfrom  itertools import product\n\n#round datetimes by 15 minutes\ndata['Time'] = pd.to_datetime(data['Time'])\nminutes = pd.to_timedelta(15*(data['Time'].dt.minute // 15), unit='min')\ndata['Time'] = data['Time'].dt.floor('H') + minutes\n\n#change date range for 4 values (to `12:45`)\nrng = pd.date_range('1/1/2019 12:00','1/1/2019 12:45',freq='15min')\n#create helper DataFrame and merge with left join\ndf = pd.DataFrame(list(product(data['ID'].unique(), rng)), columns=['ID','Time'])\nprint (df)\n   ID                Time\n0   1 2019-01-01 12:00:00\n1   1 2019-01-01 12:15:00\n2   1 2019-01-01 12:30:00\n3   1 2019-01-01 12:45:00\n4   2 2019-01-01 12:00:00\n5   2 2019-01-01 12:15:00\n6   2 2019-01-01 12:30:00\n7   2 2019-01-01 12:45:00\n\ndata = df.merge(data, how='left')\n##interpolate per groups\ndata['Value'] = (data.groupby('ID')['Value']\n                     .apply(lambda x: x.interpolate(method='linear', limit_direction='both')))\nprint (data)\n   ID                Time  Value\n0   1 2019-01-01 12:00:00    3.0\n1   1 2019-01-01 12:15:00    3.0\n2   1 2019-01-01 12:30:00    2.0\n3   1 2019-01-01 12:45:00    2.0\n4   2 2019-01-01 12:00:00    5.0\n5   2 2019-01-01 12:15:00    7.0\n6   2 2019-01-01 12:30:00    7.0\n7   2 2019-01-01 12:45:00    7.0\n"
"# sample data creation\ndata = pd.DataFrame(np.random.rand(4000,2),columns=['Item_Identifier','Item_Visibility'])\ndata.loc[:,'Item_Identifier']= data.loc[:,'Item_Identifier'].apply(\n        lambda x: 'id1' if x&gt; 0.4 else 'id2')\n# creating map_table so we could map values\nmap_table = data.groupby('Item_Identifier').mean()\n# mapping values\ndata.loc[:,'Item_Visibility'] = data.loc[:,'Item_Identifier'].map(\n        map_table.to_dict()['Item_Visibility'])\n"
"import math\nimport numpy as np\nimport pandas as pd\n\n# Create a dataframe to work with from the data provided in the question\ncolumns = ['id', 'half_hour_bucket', 'clock_in_time', 'clock_out_time' , 'rate']\n\ndata = [[232, '4/1/19 8:00 PM', '4/1/19 7:12 PM', '4/1/19 10:45 PM', 0.54],\n        [342, '4/1/19 8:30 PM', '4/1/19 7:12 PM', '4/1/19 07:22 PM ', 0.23],\n        [232, '4/1/19 7:00 PM', '4/1/19 7:12 PM', '4/1/19 10:45 PM', 0.54]]\n\ndf = pd.DataFrame(data, columns=columns)\n\ndef convert_cols_to_dt(df):\n    # Convert relevant columns to datetime format\n    for col in df:\n        if col not in ['id', 'rate']:\n            df[col] = pd.to_datetime(df[col])\n\n    return df\n\ndf = convert_cols_to_dt(df)\n# Create the mins column\ndf['mins'] = (df.clock_out_time - df.clock_in_time)\n\n  id  half_hour_bucket    clock_in_time       clock_out_time      rate mins\n0 232 2019-04-01 20:00:00 2019-04-01 19:12:00 2019-04-01 22:45:00 0.54 0 days 03:33:00.000000000\n1 342 2019-04-01 20:30:00 2019-04-01 19:12:00 2019-04-01 19:22:00 0.23 0 days 00:10:00.000000000\n2 232 2019-04-01 19:00:00 2019-04-01 19:12:00 2019-04-01 22:45:00 0.54 0 days 03:33:00.000000000\n\ndef upsample_list(x):\n    multiplier = math.ceil(x.total_seconds() / (60 * 30))\n\n    return list(range(multiplier))\n\ndf['samples'] = df.mins.apply(upsample_list)\n\ns = df.apply(lambda x: pd.Series(x['samples']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'sample'\n\ndf = df.drop('samples', axis=1).join(s, how='inner').drop('sample', axis=1)\n\n   id   half_hour_bucket    clock_in_time        clock_out_time       rate  mins\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n0  232  2019-04-01 20:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n1  342  2019-04-01 20:30:00 2019-04-01 19:12:00  2019-04-01 19:22:00  0.23  00:10:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n2  232  2019-04-01 19:00:00 2019-04-01 19:12:00  2019-04-01 22:45:00  0.54  03:33:00\n\ndf = df.reset_index(drop=True)\n\ndf = df.mask(df.duplicated())\n\n   id    half_hour_bucket    clock_in_time       clock_out_time      rate mins\n0  232.0 2019-04-01 20:00:00 2019-04-01 19:12:00 2019-04-01 22:45:00 0.54 03:33:00\n1  NaN   NaT                 NaT                 NaT                 NaN  NaT\n2  NaN   NaT                 NaT                 NaT                 NaN  NaT\n3  NaN   NaT                 NaT                 NaT                 NaN  NaT\n4  NaN   NaT                 NaT                 NaT                 NaN  NaT\n5  NaN   NaT                 NaT                 NaT                 NaN  NaT\n6  NaN   NaT                 NaT                 NaT                 NaN  NaT\n7  NaN   NaT                 NaT                 NaT                 NaN  NaT\n8  342.0 2019-04-01 20:30:00 2019-04-01 19:12:00 2019-04-01 19:22:00 0.23 00:10:00\n9  232.0 2019-04-01 19:00:00 2019-04-01 19:12:00 2019-04-01 22:45:00 0.54 03:33:00\n10 NaN   NaT                 NaT                 NaT                 NaN  NaT\n11 NaN   NaT                 NaT                 NaT                 NaN  NaT\n12 NaN   NaT                 NaT                 NaT                 NaN  NaT\n13 NaN   NaT                 NaT                 NaT                 NaN  NaT\n14 NaN   NaT                 NaT                 NaT                 NaN  NaT\n15 NaN   NaT                 NaT                 NaT                 NaN  NaT\n16 NaN   NaT                 NaT                 NaT                 NaN  NaT\n\ndf[['half_hour_bucket', 'rate']] = df[['half_hour_bucket', 'rate']].ffill()\n\n     id     half_hour_bucket     clock_in_time        clock_out_time       rate  mins\n0    232.0  2019-04-01 20:00:00  2019-04-01_19:12:00  2019-04-01_22:45:00  0.54  03:33:00\n1    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n2    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n3    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n4    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n5    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n6    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n7    NaN    2019-04-01 20:00:00  NaT                  NaT                  0.54  NaT\n8    342.0  2019-04-01 20:30:00  2019-04-01_19:12:00  2019-04-01_19:22:00  0.23  00:10:00\n9    232.0  2019-04-01 19:00:00  2019-04-01_19:12:00  2019-04-01_22:45:00  0.54  03:33:00\n10   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n11   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n12   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n13   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n14   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n15   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n16   NaN    2019-04-01 19:00:00  NaT                  NaT                  0.54  NaT\n"
'for tweet in k1_tweets_filtered:   \n    k1_tweets_processed.append(pre_process(tweet))\n'
"df.iloc[np.r_[176:898, 1595:2352, 2906:3409]][3].agg(['mean','std'])\n"
'def cut_points(n, already_cut=None):\n    # The first cut point is at 0 \n    if already_cut is None:\n        already_cut = [0]\n\n    # We can cut at all places between the last cut plus 3 \n    # and the length minus 3, and yield recursively the solutions for each choice\n    for i in range(already_cut[-1]+3, n-2):\n        cuts = already_cut[:] + [i]\n        yield from cut_points(n, cuts)\n\n    # When we tried all cut points and reached the total length, we yield the cut points list \n    yield already_cut[:] + [n]\n\n\ndef all_possible_sublists(data):\n    n = len(data)\n    for cut in cut_points(n):\n        yield [data[cut[i]:cut[i+1]] for i in range(len(cut)-1)]\n\nlist(all_possible_sublists([0, 1, 2, 3]))\n# [[[0, 1, 2, 3]]]\n\nlist(all_possible_sublists([0, 1, 2, 3, 4, 5, 6]))\n# [[[0, 1, 2], [3, 4, 5, 6]],\n#  [[0, 1, 2, 3], [4, 5, 6]],\n#  [[0, 1, 2, 3, 4, 5, 6]]]\n\nfor sublist in all_possible_sublists([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]):\n    print(sublist)\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]\n# [[0, 1, 2], [3, 4, 5, 6], [7, 8, 9]]\n# [[0, 1, 2], [3, 4, 5, 6, 7, 8, 9]]\n# [[0, 1, 2, 3], [4, 5, 6], [7, 8, 9]]\n# [[0, 1, 2, 3], [4, 5, 6, 7, 8, 9]]\n# [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]\n# [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9]]\n# [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n# [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]\n'
'[(7, 5, 0, 7, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n (7, 5, 5, 0, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n (7, 6, 0, 6, array([1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1])),\n (7, 6, 5, 0, array([1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1])),\n (7, 7, 0, 5, array([1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1])),\n (7, 7, 5, 0, array([1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1])),\n (7, 8, 0, 4, array([1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1])),\n (7, 8, 5, 0, array([1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1])),\n (7, 9, 0, 3, array([1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1])),\n (7, 9, 5, 0, array([1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1])),\n (8, 4, 0, 8, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n (8, 4, 4, 0, array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])),\n (8, 5, 0, 7, array([1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1])),\n (8, 5, 4, 0, array([1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1])),\n (8, 6, 0, 6, array([1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1])),\n (8, 6, 4, 0, array([1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1])),\n (8, 7, 0, 5, array([1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1])),\n (8, 7, 4, 0, array([1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1])),\n (8, 8, 0, 4, array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1])),\n (8, 8, 4, 0, array([1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1])),\n (8, 9, 0, 3, array([1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1])),\n (8, 9, 4, 0, array([1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1])),\n (9, 4, 0, 8, array([1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1])),\n (9, 4, 3, 0, array([1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1])),\n (9, 5, 0, 7, array([1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1])),\n (9, 5, 3, 0, array([1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1])),\n (9, 6, 0, 6, array([1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1])),\n (9, 6, 3, 0, array([1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1])),\n (9, 7, 0, 5, array([1, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1])),\n (9, 7, 3, 0, array([1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1, 1])),\n (9, 8, 0, 4, array([1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1])),\n (9, 8, 3, 0, array([1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1])),\n (9, 9, 0, 3, array([1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1])),\n (9, 9, 3, 0, array([1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1]))]\n'
"df.loc[:, df.nunique() &lt; 32]\n\ndf = pd.DataFrame({'A': list('abbcde'), 'B': list('ababab')})\ndf\n   A  B\n0  a  a\n1  b  b\n2  b  a\n3  c  b\n4  d  a\n5  e  b\n\ndf.nunique()\nA    5\nB    2\ndtype: int64\n\ndf.loc[:, df.nunique() &lt; 3]\n   B\n0  a\n1  b\n2  a\n3  b\n4  a\n5  b\n"
'from bs4 import BeautifulSoup as soup\nhtml = \'\'\'\n &lt;span&gt;$289&lt;span class="rightEndPrice_6y_hS"&gt;99&lt;/span&gt;&lt;/span&gt;\n\'\'\'\nresult = soup(html, \'html.parser\').find(\'span\').contents[0]\n\n\'$289\'\n\nresult = page_soup.find("div", {"class":"price_FHDfG large_3aP7Z"}).span.contents[0]\n'
'import tensorflow as tf\nimport numpy as np\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nc = np.stack([a, b])\n*d, = c\nprint(d)\n\nc_ = tf.stack([a, b])\nd_ = tf.unstack(c_)\n\nwith tf.Session() as sess:\n    print(sess.run(d_))\n'
'    -&gt; 2380                 eval_name, val, is_higher_better = feval_ret // this is the return of mlogloss\n       2381                 ret.append((data_name, eval_name, val, is_higher_better))\n       2382         return ret\nTypeError: \'numpy.float64\' object is not iterable\n\ndef mlogloss(...):\n...\nreturn "my_loss_name", loss_value, False\n'
"as_date = pd.to_datetime(df.A)\ns = np.argsort(as_date)\n\ndf['B'] = df.B.loc[s].groupby(as_date.loc[s].dt.date).ffill().loc[df.index]\n\n                     A    B\n0  2019-03-13 08:12:23   B1\n1  2019-03-13 07:10:18   B0\n2  2019-03-20 08:12:23  B13\n3  2019-03-13 08:12:23   B1\n4  2019-03-15 10:35:53  B10\n5  2019-03-20 11:12:23  B12\n"
"df1 = pd.Dataframe(df.values.reshape(-1, 4), columns=['RESP','HR','SPO2','PULSE'])\ndf1['RESP1'] = df['RESP'].shift(-1)\n\na = '46 122 0 0 46 122 0 0 45 122 0 0 45 122 0'.split()\ndf = pd.DataFrame([a]).astype(int)\nprint (df)\n    0    1  2  3   4    5  6  7   8    9  10  11  12   13  14\n0  46  122  0  0  46  122  0  0  45  122   0   0  45  122   0\n\n#flatten values\na = df.values.ravel()\n#number of new columns\nN = 4\n#array filled by NaNs for possible add NaNs to end of last row\narr = np.full(((len(a) - 1)//N + 1)*N, np.nan)\n#fill array by flatten values\narr[:len(a)] = a\n#reshape to new DataFrame (last value is NaN)\ndf1 = pd.DataFrame(arr.reshape((-1, N)), columns=['RESP','HR','SPO2','PULSE'])\n#new column with shifting first col\ndf1['RESP1'] = df1['RESP'].shift(-1)\nprint(df1)\n   RESP     HR  SPO2  PULSE  RESP1\n0  46.0  122.0   0.0    0.0   46.0\n1  46.0  122.0   0.0    0.0   45.0\n2  45.0  122.0   0.0    0.0   45.0\n3  45.0  122.0   0.0    NaN    NaN\n"
"import pandas as pd \nimport json\n\ndf = pd.read_excel(path)\n\nresult = []\nfor labels, df1 in df.groupby(['id', 'label'], sort=False):\n    id_, label = labels\n    record = {'id': int(id_), 'label': label, 'Customer': []}\n    for inner_labels, df2 in df1.groupby(['id_customer', 'label_customer'], sort=False):\n        id_, label = inner_labels\n        customer = {'id': id_, 'label': label, 'Number': []}\n        for inner_labels, df3 in df2.groupby(['part_number', 'number_customer'], sort=False):\n            p, s = inner_labels\n            number = {'part': str(p), 'number_customer': str(s), 'Products': []}\n            for inner_labels, df4 in df3.groupby(['product', 'label_product'], sort=False):\n                p, lp = inner_labels\n                product = {'product': p, 'label_product': lp, 'Order': []}\n                for k, c, v in zip(df4['key'], df4['country'], df4['value_product']):\n                    product['Order'].append({'key': k, 'country': c, 'value_product': v})\n                number['Products'].append(product)\n            customer['Number'].append(number)\n        record['Customer'].append(customer)\n    result.append(record)\n"
"sheffield = pd.read_csv('data/sheffield_weather_station.csv', skiprows=8, sep='\\s+', engine='python')\n"
"df['mean'] = df.price.expanding().mean()\n\ndf\nindex   price   mean\n0       4       4.000000\n1       6       5.000000\n2       10      6.666667\n3       12      8.000000\n"
"import pandas as pd \nimport numpy as np \nfrom numpy.random import randint \nimport matplotlib.pyplot as plt                                                                                                                                                       \n\nnp.random.seed(10)  # added for reproductibility                                                                                                                                                                 \n\nrng = pd.date_range('10/9/2018 00:00', periods=10, freq='1H') \ndf = pd.DataFrame({'Random_Number':randint(1, 100, 10)}, index=rng)                                                                                                                   \ndf.plot()    \n\nplt.axhline(df.quantile(0.025)[0])                                                                                                                                                    \nplt.axhline(df.quantile(0.975)[0])                                                                                                                                                    \n\nplt.show()\n"
"df1['locationid']=df1.locationid.fillna(df1.groupby('geo_loc')['locationid'].transform('max'))\nprint (df1)\n   locationid geo_loc\n0       111.0     G12\n1       158.0     K11\n2       145.0     B16\n3       111.0     G12\n4       189.0     B22\n5       145.0     B16\n6       158.0     K11\n7       145.0     B16\n\ndf1 = pd.DataFrame({'locationid':[111, np.nan, 145, np.nan, 189,np.nan, 158, 145],\n                     'geo_loc':['G12','K11','B16','G12','B22','B16', 'K11', 'B16']})\n\n#sample data strings with missing values\ndf1['locationid'] = df1['locationid'].dropna().astype(str) + 'a'\n\n\ndf1['locationid']= (df1.groupby('geo_loc')['locationid']\n                       .transform(lambda x: x.fillna(x.dropna().max())))\n\nprint (df1)\n  locationid geo_loc\n0     111.0a     G12\n1     158.0a     K11\n2     145.0a     B16\n3     111.0a     G12\n4     189.0a     B22\n5     145.0a     B16\n6     158.0a     K11\n7     145.0a     B16\n"
'# fill nans with zeros\nif weight_matrix is not None:\n    weight_matrix[np.isnan(weight_matrix)] = 0.0\n\ndist_pot_donors : ndarray of shape (n_receivers, n_potential_donors)\n    Distance matrix between the receivers and potential donors from\n    training set. There must be at least one non-nan distance between\n    a receiver and a potential donor.\n\nX = np.array([[np.nan,7,4,5],[2,8,4,5],[3,7,4,6],[1,np.nan,np.nan,5]])\n\nprint(X)\narray([[nan,  7.,  4.,  5.],\n       [ 2.,  8.,  4.,  5.],\n       [ 3.,  7.,  4.,  6.],\n       [ 1., nan, nan,  5.]])\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=1)\n\nimputer.fit_transform(X)\narray([[1., 7., 4., 5.],\n       [2., 8., 4., 5.],\n       [3., 7., 4., 6.],\n       [1., 7., 4., 5.]])\n'
"&gt;&gt;&gt; pd.DataFrame(np.where(registration.isna(), 'No', 'Yes'))\n     0\n0  Yes\n1  Yes\n2  Yes\n3   No\n4   No\n5  Yes\n"
'df = pd.read_clipboard(sep=\'\\s{2,}\', engine=\'python\', parse_dates=[\'Time\'])\n\nres = (df\n       #appending Event,Location and ID with current index\n       #prevents duplicate values when unstacking\n       .set_index([\'Event\',\'Location\',\'ID\'], append=True)\n       #get Event index as column\n       .unstack(\'Event\')\n       #topmost column level redundant ... remove\n       .droplevel(0,axis=1)\n       #fill upwards on the end to align the dates to \n       #the appropriate positions\n       .assign(end = lambda x: x[\'end\'].bfill())\n       .dropna()\n       .add_suffix("_time")\n       .reset_index()\n       .drop("level_0", axis=1)\n       .reindex([\'start_time\',\'end_time\',\'Location\',\'ID\'], axis=1)\n       .rename_axis(None,axis=1)\n      )\n\nres\n\n\n\n          start_time                      end_time      Location    ID\n0   2020-05-22 21:22:04.784622  2020-05-22 21:43:07.060629  UK      50\n1   2020-05-25 23:22:04.784622  2020-05-25 23:43:07.060629  UK      50\n2   2020-05-25 23:44:15.000566  2020-05-26 00:48:06.820164  US      30\n3   2020-05-25 23:48:23.416348  2020-05-26 00:48:06.820164  Italy   70\n4   2020-05-27 20:48:23.416348  2020-05-27 00:33:42.454450  Italy   30\n'
'df[\'newcol\'] = df.apply(lambda x: " ".join(x[1:]), axis=1)\ndf.groupby(\'Column1\').agg({\'newcol\': lambda x: " ".join()})\n'
"import pandas as pd\n\ndata = {\n    '': '',\n    'asd': '',\n    'bfdgfd': '',\n    'trytr': '',\n    'jlhj': '',\n    'Job': 'Revenue',\n    'abc123': 1000.00,\n    'hey098': 2000.00\n}\ndf = pd.DataFrame(data.items(),\n    columns=['Unnamed: 0', 'Unnamed: 1'])\nstartRow = 5\n\ndf.columns = df.loc[startRow].to_list()  # set the &quot;header&quot; to the values in this row\ndf = df.loc[startRow+1:].reset_index(drop=True)  # select only the rows you want\n\n      Job Revenue\n0  abc123    1000\n1  hey098    2000\n"
'    A   B   C   D\n303 0   84  13  96\n728 0   43  48  32\n558 0   35  49  49\n286 0   34  17  4\n652 0   29  53  4\n292 0   18  62  29\n139 0   17  63  99\n\n    A   B   C   D\n718 1   91  6   48\n611 1   83  19  75\n208 1   80  35  73\n'
"$ awk 'NR &gt; 1 &amp;&amp; prev != $1 { sum[prev&quot;-&gt;&quot;$1]++ }\n       { prev = $1 }\n       END { for (a in sum) { print a&quot; = &quot;sum[a] } }\n  ' input.txt\nAA-&gt;CC = 1\nBB-&gt;AA = 2\nAA-&gt;BB = 3\nBB-&gt;CC = 1\nCC-&gt;AA = 2\n"
"m = lambda x: map(str.strip, x.split(','))\n\nwith open('test.csv') as f:\n    df = pd.DataFrame(\n        [[x, y] for x, *ys in map(m, f.readlines()) for y in ys if y],\n        columns=['Example', 'Class']\n    )\n\ndf\n\n     Example    Class\n0  example 1  class 1\n1  example 2  class 1\n2  example 2  class 2\n3  example 3  class 2\n4  example 4  class 1\n5  example 4  class 2\n6  example 4  class 4\n"
"class_names = [\n        'BG', 'person'\n    ]\n\nclass_names = [\n        'BG', 'person', 'some other class', , 'some other class'\n    ]\n"
'# Setup\ndf = pd.DataFrame({\'case_text\': ["Billy was glad to see jack. Jack was estatic to play with Billy. Jack and Billy were lonely without eachother. Jack is tall and Billy is clever."]})\n\nstartwords = {"happy":["glad","estatic"],\n              "sad": ["depressed", "lonely"],\n              "big": ["tall", "fat"],\n              "smart": ["clever", "bright"]}\n\n# First you need to rearrange your startwords dict\nstartwords_map = {w: k for k, v in startwords.items() for w in v}\n\n(df[\'case_text\'].str.lower()     # casts to lower case\n .str.replace(\'[.,\\*!?:]\', \'\')   # removes punctuation and special characters\n .str.split()                    # splits the text on whitespace\n .explode()                      # expands into a single pandas.Series of words\n .map(startwords_map)            # maps the words to the startwords\n .value_counts()                 # counts word occurances\n .to_dict())                     # outputs to dict\n\n{\'happy\': 2, \'big\': 1, \'smart\': 1, \'sad\': 1}\n'
'accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n\nprint(sess.run(y, feed_dict={x: mnist.test.images}))\n'
"k.ix[(k.groupby('Block').Sid.transform('count') == 1), 'td'] = 0\n\n&gt;&gt;&gt; k\n    Sid     Itemid  Block      td\n0     1  214536502      1   0.000\n1     1  214536500      2   0.000\n2     1  214536506      3   0.000\n3     1  214577561      4   0.000\n4     2  214662742      5  41.759\n5     2  214662742      5  78.073\n6     3  214576500      6   0.000\n7     4  214821275      7  26.002\n8     4  214821275      7  28.199\n9     5  214821371      8  42.289\n10    5  214821371      8  45.193\n"
'if(classes[max_index] == y[q]):\n    corr += 1\n'
'mask = ~np.in1d(df.index.date, pd.to_datetime(removelist).date)\ndf = df.loc[mask, :]\n\n%timeit df.loc[~np.in1d(df.index.date, pd.to_datetime(removelist).date), :]\n1000 loops, best of 3: 1.42 ms per loop\n\n%timeit df[[d.date() not in pd.to_datetime(removelist) for d in df.index]]\n100 loops, best of 3: 3.25 ms per loop\n'
'GridSearchCV(..., verbose=1)\n'
"import pandas as pd\n\ndf = pd.DataFrame({'Name': ['Curly', 'Moe', 'Larry'],\n                   'Fruit': [['Apple'], ['Orange'], ['Apple', 'Banana']]},\n                  columns=['Name', 'Fruit'])\n\n# a one-liner... that's pretty long    \ndummies_df = pd.get_dummies(\n  df.join(pd.Series(df['Fruit'].apply(pd.Series).stack().reset_index(1, drop=True),\n                    name='Fruit1')).drop('Fruit', axis=1).rename(columns={'Fruit1': 'Fruit'}),\n  columns=['Fruit']).groupby('Name', as_index=False).sum()\n\nprint(dummies_df)\n\n0     Apple\n1    Orange\n2     Apple\n2    Banana\ndtype: object \n\ndf.join(* steps 1 and 2 code *).drop('Fruit', axis=1).rename(columns={'Fruit1': 'Fruit'})\n\npd.get_dummies(* steps 1, 2, and 3 here*, columns=['Fruit'])\n\n    Name  Fruit_Apple  Fruit_Banana  Fruit_Orange\n0  Curly          1.0           0.0           0.0\n1    Moe          0.0           0.0           1.0\n2  Larry          1.0           0.0           0.0\n2  Larry          0.0           1.0           0.0\n\ndummies_df = (*steps 1, 2, 3, and 4*).groupby('Name', as_index=False).sum()\n\n    Name  Fruit_Apple  Fruit_Banana  Fruit_Orange\n0  Curly          1.0           0.0           0.0\n1  Larry          1.0           1.0           0.0\n2    Moe          0.0           0.0           1.0\n"
"In [64]: pd.DataFrame([np.sort(x['C1'].values) for x in dfs], columns=d1.index)\nOut[64]:\n     0         1         2         3         4\n0  0.0  0.141421  0.509902  0.538516  0.648074\n1  0.0  0.300000  0.331662  0.538516  0.608276\n2  0.0  0.244949  0.300000  0.509902  0.509902\n\nIn [67]: d1\nOut[67]:\n   ID        C1\n0   0  0.000000\n1   1  0.538516\n2   2  0.509902\n3   3  0.648074\n4   4  0.141421\n\nIn [68]: d1['C1'].values\nOut[68]: array([ 0.      ,  0.538516,  0.509902,  0.648074,  0.141421])\n\nIn [69]: np.sort(d1['C1'].values)\nOut[69]: array([ 0.      ,  0.141421,  0.509902,  0.538516,  0.648074])\n\nIn [70]: [np.sort(x['C1'].values) for x in dfs]\nOut[70]:\n[array([ 0.      ,  0.141421,  0.509902,  0.538516,  0.648074]),\n array([ 0.      ,  0.3     ,  0.331662,  0.538516,  0.608276]),\n array([ 0.      ,  0.244949,  0.3     ,  0.509902,  0.509902])]\n\nIn [71]: pd.DataFrame([np.sort(x['C1'].values) for x in dfs], columns=d1.index)\nOut[71]:\n     0         1         2         3         4\n0  0.0  0.141421  0.509902  0.538516  0.648074\n1  0.0  0.300000  0.331662  0.538516  0.608276\n2  0.0  0.244949  0.300000  0.509902  0.509902\n"
"a = df1.columns[df1.columns.str.startswith('a')]\nb = df1.columns[df1.columns.str.startswith('b')]\nc = df1.columns[df1.columns.str.startswith('c')]\n\nfor col1 in b:\n    for col2 in a:\n        df1[col2 + '_' + col1.split('_')[1]] = df1[col1].mul(df1[col2])\n\nfor col1 in b:\n    for col2 in c:\n        df1[col2 + '_' + col1.split('_')[1]] = df1[col1].mul(df1[col2])\nprint (df1)\n\n   a_Yes  b_b  b_c  b_d  b_e  c_2  a_Yes_b  a_Yes_c  a_Yes_d  a_Yes_e  c_2_b  \\\n0      1    0    0    0    0    0        0        0        0        0      0   \n1      1    1    0    0    0    1        1        0        0        0      1   \n2      0    0    1    0    0    1        0        0        0        0      0   \n3      0    0    0    1    0    0        0        0        0        0      0   \n4      0    0    0    0    1    1        0        0        0        0      0   \n5      1    0    0    0    0    0        0        0        0        0      0   \n6      1    0    1    0    0    0        0        1        0        0      0   \n\n   c_2_c  c_2_d  c_2_e  \n0      0      0      0  \n1      0      0      0  \n2      1      0      0  \n3      0      0      0  \n4      0      0      1  \n5      0      0      0  \n6      0      0      0  \n\na = df1.filter(regex='^a')\nb = df1.filter(regex='^b')\nc = df1.filter(regex='^c')\n\ndfa = b.mul(a.squeeze(), axis=0).rename(columns=lambda x: a.columns[0] + x[1:])\ndfc = b.mul(c.squeeze(), axis=0).rename(columns=lambda x: c.columns[0] + x[1:])\n\ndf1 = pd.concat([df1, dfa, dfc], axis=1)\nprint (df1)\n   a_Yes  b_b  b_c  b_d  b_e  c_2  a_Yes_b  a_Yes_c  a_Yes_d  a_Yes_e  c_2_b  \\\n0      1    0    0    0    0    0        0        0        0        0      0   \n1      1    1    0    0    0    1        1        0        0        0      1   \n2      0    0    1    0    0    1        0        0        0        0      0   \n3      0    0    0    1    0    0        0        0        0        0      0   \n4      0    0    0    0    1    1        0        0        0        0      0   \n5      1    0    0    0    0    0        0        0        0        0      0   \n6      1    0    1    0    0    0        0        1        0        0      0   \n\n   c_2_c  c_2_d  c_2_e  \n0      0      0      0  \n1      0      0      0  \n2      1      0      0  \n3      0      0      0  \n4      0      0      1  \n5      0      0      0  \n6      0      0      0  \n"
"In [43]: df\nOut[43]:\n   User  Purchase_Count  Location_Count\n0     1               2               3\n1     2              10               5\n2     3               5               1\n3     4              20               4\n4     5               2               3\n5     6               2               3\n6     7              10               5\n\nIn [44]: total = len(df)\n\nIn [45]: df['percentage'] = df.groupby(['Purchase_Count', 'Location_Count']).transform(lambda r: r.count()/total)\n\nIn [46]: df\nOut[46]:\n   User  Purchase_Count  Location_Count  percentage\n0     1               2               3    0.428571\n1     2              10               5    0.285714\n2     3               5               1    0.142857\n3     4              20               4    0.142857\n4     5               2               3    0.428571\n5     6               2               3    0.428571\n6     7              10               5    0.285714\n\nIn [53]: df['percentage'] = (df.groupby(['Purchase_Count', 'Location_Count'])\n    ...:                     .transform(lambda r: r.count()/total))\n\nIn [54]: df\nOut[54]:\n   User  Purchase_Count  Location_Count  percentage\n0     1               2               3    0.428571\n1     2              10               5    0.285714\n2     3               5               1    0.142857\n3     4              20               4    0.142857\n4     5               2               3    0.428571\n5     6               2               3    0.428571\n6     7              10               5    0.285714\n\ndf.groupby(['Purchase_Count', 'Location_Count']).transform('count') / total\n"
'df_tr = pd.DataFrame({\'Col1\':[1,2,1,2,2],\n                      \'Col2\':[5,5,5,6,6],\n                      \'aCol\':[1,8,9,6,4]})\nprint(df_tr)\n   Col1  Col2  aCol\n0     1     5     1\n1     2     5     8\n2     1     5     9\n3     2     6     6\n4     2     6     4\n\n#your solution, only multiple 10 \ndf_tr_mod = df_tr.groupby([\'Col1\',\'Col2\']).aCol.agg([\'count\']) * 10\nprint (df_tr_mod)\n           count\nCol1 Col2       \n1    5        20\n2    5        10\n     6        20\n\nprint (type(df_tr_mod))\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\n\n#for MultiIndex add to_frame\ndf_tr_mod = df_tr.groupby([\'Col1\',\'Col2\']).size().to_frame(name=\'count\') * 10\nprint (df_tr_mod)\n           count\nCol1 Col2       \n1    5        20\n2    5        10\n     6        20\n\n#for all columns from index add reset_index() \ndf_tr_mod = df_tr.groupby([\'Col1\',\'Col2\']).size().reset_index(name=\'count\') \ndf_tr_mod["count"]= df_tr_mod["count"]*10\nprint (df_tr_mod)\n   Col1  Col2  count\n0     1     5     20\n1     2     5     10\n2     2     6     20\n\ndf_tr_mod = df_tr.groupby([\'Col1\',\'Col2\']).aCol.agg([\'size\', \'sum\', \'mean\'])\nprint (df_tr_mod)\n           size  sum  mean\nCol1 Col2                 \n1    5        2   10     5\n2    5        1    8     8\n     6        2   10     5\n'
"df.rate / df.reservation_num.map(df.reservation_num.value_counts())\n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\ndf.rate / df.groupby('reservation_num').rate.transform('size')\n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\nu, f = np.unique(df.reservation_num.values, return_inverse=True)\ndf.rate / np.bincount(f)[f]\n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\ndef factor(a):\n    if len(a) &gt; 10000:\n        return pd.factorize(a)[0]\n    else:\n        return np.unique(a, return_inverse=True)[1]\n\ndef count(a):\n    f = factor(a)\n    return np.bincount(f)[f]\n\ndf.rate / count(df.reservation_num.values)  \n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\n%timeit df.rate / df.reservation_num.map(df.reservation_num.value_counts())\n%timeit df.rate / df.groupby('reservation_num').rate.transform('size')\n\n1000 loops, best of 3: 650 µs per loop\n1000 loops, best of 3: 768 µs per loop\n\n%%timeit\nu, f = np.unique(df.reservation_num.values, return_inverse=True)\ndf.rate / np.bincount(f)[f]\n\n10000 loops, best of 3: 131 µs per loop\n"
"df = df.stack().str.split(',', expand=True).stack().reset_index(name='a')\ndf = df.groupby(['a', 'level_1'])['level_0'].apply(','.join).unstack()\nprint (df)\nlevel_1          dataset 1          dataset 2          dataset 3\na                                                               \ngene1    cell1,cell2,cell3  cell1,cell2,cell3  cell1,cell2,cell3\ngene2    cell1,cell2,cell3  cell1,cell2,cell3  cell1,cell2,cell3\ngene3    cell1,cell2,cell3               None               None\ngene4                 None  cell1,cell2,cell3               None\ngene5                 None               None  cell1,cell2,cell3\n"
"#datatable imported earlier as 'data'\n#Create a new dictionary\nplotDict = {}\n# Loop across each of the two lists that contain the items you want to compare\nfor gene1 in list_1:\n    for gene2 in list_2:\n        # Do a pearsonR comparison between the two items you want to compare\n        tempDict = {(gene1, gene2): scipy.stats.pearsonr(data[gene1],data[gene2])}\n        # Update the dictionary each time you do a comparison\n        plotDict.update(tempDict)\n# Unstack the dictionary into a DataFrame\ndfOutput = pd.Series(plotDict).unstack()\n# Optional: Take just the pearsonR value out of the output tuple\ndfOutputPearson = dfOutput.apply(lambda x: x.apply(lambda x:x[0]))\n# Optional: generate a heatmap\nsns.heatmap(dfOutputPearson)\n"
"pd.get_dummies(df.customer).T.dot(pd.get_dummies(df.visited_city)).clip(0, 1)\n\n       London  Melbourne  New_York  Paris\nJohn        1          1         1      0\nMary        1          1         0      0\nPeter       0          0         1      0\nSteve       0          0         0      1\n\ni, r = pd.factorize(df.customer.values)\nj, c = pd.factorize(df.visited_city.values)\nn, m = r.size, c.size\nb = np.zeros((n, m), dtype=int)\nb[i, j] = 1\n\npd.DataFrame(b, r, c).sort_index().sort_index(1)\n\n       London  Melbourne  New_York  Paris\nJohn        1          1         1      0\nMary        1          1         0      0\nPeter       0          0         1      0\nSteve       0          0         0      1\n\ndf.groupby(['customer', 'visited_city']).size().unstack(fill_value=0).clip(0, 1)\n\nvisited_city  London  Melbourne  New_York  Paris\ncustomer                                        \nJohn               1          1         1      0\nMary               1          1         0      0\nPeter              0          0         1      0\nSteve              0          0         0      1\n\n# Multiples of Minimum time\n#\n           pir1  pir2      pir3       wen       vai\n10     1.392237   1.0  1.521555  4.337469  5.569029\n30     1.445762   1.0  1.821047  5.977978  7.204843\n100    1.679956   1.0  1.901502  6.685429  7.296454\n300    1.568407   1.0  1.825047  5.556880  7.210672\n1000   1.622137   1.0  1.613983  5.815970  5.396008\n3000   1.808637   1.0  1.852953  4.159305  4.224724\n10000  1.654354   1.0  1.502092  3.145032  2.950560\n30000  1.555574   1.0  1.413612  2.404061  2.299856\n\nwen = lambda d: d.pivot_table(index='customer', columns='visited_city',aggfunc=len, fill_value=0)\nvai = lambda d: pd.crosstab(d.customer, d.visited_city)\npir1 = lambda d: pd.get_dummies(d.customer).T.dot(pd.get_dummies(d.visited_city)).clip(0, 1)\npir3 = lambda d: d.groupby(['customer', 'visited_city']).size().unstack(fill_value=0).clip(0, 1)\n\ndef pir2(d):\n    i, r = pd.factorize(d.customer.values)\n    j, c = pd.factorize(d.visited_city.values)\n    n, m = r.size, c.size\n    b = np.zeros((n, m), dtype=int)\n    b[i, j] = 1\n\n    return pd.DataFrame(b, r, c).sort_index().sort_index(1)\n\nresults = pd.DataFrame(\n    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    columns='pir1 pir2 pir3 wen vai'.split(),\n    dtype=float\n)\n\nfor i in results.index:\n    d = pd.concat([df] * i, ignore_index=True)\n    for j in results.columns:\n        stmt = '{}(d)'.format(j)\n        setp = 'from __main__ import d, {}'.format(j)\n        results.at[i, j] = timeit(stmt, setp, number=10)\n\nprint((lambda r: r.div(r.min(1), 0))(results))\n\nresults.plot(loglog=True)\n"
'df1 = df_1.set_index(["housenumber", "street"])\ndf2 = df_2.set_index(["housenumber", "street"])\n\ndf = df1.combine_first(df2).reset_index()\n\ndf = df1.fillna(df2).reset_index()\n'
'convert YourP5Image.pgm -compress none ActualP2Image.pgm\n'
"df.Timestamp = pd.to_datetime(df.Timestamp)\n\ndf['date'] = df.Timestamp.dt.floor('d')\n\nu = df.USER.unique()\na = df.groupby(['USER', 'date']).size().reset_index(level=1, drop=True)\na = a[a&gt;1]\ntarget_df = a[~a.index.duplicated()]\n                .astype(bool).reindex(u, fill_value=False).to_frame(name='Active')\n\na = df.groupby('USER')['Timestamp'].nunique()\ntarget_df['Multiple_days'] = a[a&gt;1].astype(bool).reindex(u, fill_value=False)\n\na = df[(df.Busi_days==True)&amp;(df.Busi_hours==True)].USER.unique()\ntarget_df['Busi_weekday'] = target_df.index.isin(a)\nprint(target_df)\n\n      Active  Multiple_days  Busi_weekday\nUSER                                     \nAAD     True           True          True\nSAP    False           True          True\nYAS    False          False         False\n\nprint (df1)\n  USER   Timestamp day_of_week  Busi_days  Busi_hours\n0  AAD  2017-07-11    09:31:44       True        True\n1  AAD  2017-07-11    23:24:43       True       False\n2  AAD  2017-07-12    13:24:43       True        True\n3  SAP  2017-07-23    14:24:34      False       False\n4  SAP  2017-07-24    16:58:49       True        True\n5  YAS  2017-07-31    21:10:35       True       False\n\ndef func(df, time_col, user_col):\n    df[time_col] = pd.to_datetime(df[time_col])\n\n    df['date'] = df[time_col].dt.floor('d')\n\n    u = df.USER.unique()\n    a = df.groupby([user_col, 'date']).size().reset_index(level=1, drop=True)\n    a = a[a&gt;1]\n    target_df = (a[~a.index.duplicated()]\n                    .astype(bool).reindex(u, fill_value=False).to_frame(name='Active'))\n\n    a = df.groupby(user_col)[time_col].nunique()\n    target_df['Multiple_days'] = a[a&gt;1].astype(bool).reindex(u, fill_value=False)\n\n    a = df.loc[(df.Busi_days==True)&amp;(df.Busi_hours==True), user_col].unique()\n    target_df['Busi_weekday'] = target_df.index.isin(a)\n    return target_df\n\n#inputs are name of DataFrame, column for timestamp and column for user    \nprint (func(df1, 'Timestamp', 'USER'))\n      Active  Multiple_days  Busi_weekday\nUSER                                     \nAAD     True           True          True\nSAP    False           True          True\nYAS    False          False         False\n"
"f = [   # declare an aggfunc list in advance, we'll need it later\n      ('Total Observations', 'size'), \n      ('Missed', 'sum')\n]\n\ng = df.groupby(['Group', 'Subgroup'])\\\n      .Obs.diff()\\\n      .sub(1)\\\n      .groupby(df.Group)\\\n      .agg(f)\n\ng['Total Observations'] += g['Missed']\ng['%'] = g['Missed'] / g['Total Observations'] * 100 \n\ng\n\n       Total Observations  Missed          %\nGroup                                       \nA                     8.0     1.0  12.500000\nB                     9.0     2.0  22.222222\n"
'#specify columns to sum\ncols = ["Delivered Impressions", "Clicks", "Conversion", "Spend"]\n#replace NaNs by forward filling\ndf[\'Placement# Name\'] = df[\'Placement# Name\'].ffill()\n#count grand total\ngrand = df[cols].sum()\ngrand.loc[\'Placement# Name\'] = \'Grand total\'\nprint (grand)\n\n#get subtotal by aggregation sum \ndf1 = df.groupby(\'Placement# Name\')[cols].sum()\n#change sum for correct order\ndf1.index = df1.index + \'____\'\n#join to original, sort by second level of MultiIndex\ndf = (pd.concat([df.set_index(\'Placement# Name\'), df1], keys=(\'a\', \'b\'))\n        .sort_index(level=1)\n        .reset_index())\n\n#change values to total\ndf[\'Placement# Name\'] = np.where(df[\'level_0\'] == \'a\', df[\'Placement# Name\'], \'Total\')\n#remove column\ndf = df.drop(\'level_0\', axis=1)\n#add grand total\ndf.loc[len(df.index)] = grand\n\nprint (df)\n                                      Placement# Name   Adsize          CTR  \\\n0   13.iab units (mobile only) + non-expanding adh...   320x50    0.0119683   \n1                                               Total      NaN          NaN   \n2   15.iab units (mobile only) + non-expanding adh...   320x50    0.0138741   \n3   15.iab units (mobile only) + non-expanding adh...   768x90    0.0271041   \n4                                               Total      NaN          NaN   \n5                     18.iab units - desktop + mobile  160x600   0.00155927   \n6                     18.iab units - desktop + mobile  300x250   0.00797965   \n7                     18.iab units - desktop + mobile  300x600   0.00275059   \n8                     18.iab units - desktop + mobile   728x90   0.00496391   \n9                                               Total      NaN          NaN   \n10  4.iab units (mobile only) + non-expanding adhe...   320x50    0.0141497   \n11                                              Total      NaN          NaN   \n12  5.iab units (mobile only) + non-expanding adhe...   320x50    0.0111654   \n13  5.iab units (mobile only) + non-expanding adhe...   768x90    0.0253428   \n14                                              Total      NaN          NaN   \n15                     6.iab units - desktop + mobile  160x600  7.41895e-05   \n16                     6.iab units - desktop + mobile  300x250     0.011838   \n17                     6.iab units - desktop + mobile  300x600  0.000259538   \n18                     6.iab units - desktop + mobile   728x90   0.00538178   \n19                                              Total      NaN          NaN   \n20                                        Grand total      NaN          NaN   \n\n   Clicks Conversion Conversion Rate Delivered Impressions    Spend     eCPA  \n0    1888          4     2.53566e-05                157750  1126.79  281.696  \n1    1888          4             NaN                157750  1126.79      NaN  \n2    2121         17     0.000111202                152875  1091.96  64.2332  \n3     152          2     0.000356633                  5608  40.0571  20.0286  \n4    2273         19             NaN                158483  1132.02      NaN  \n5      37         21     0.000884993                 23729  132.204  6.29545  \n6     684         58     0.000676637                 85718  477.572    8.234  \n7      34         13      0.00105169                 12361  68.8684  5.29757  \n8     403         80     0.000985392                 81186  452.322  5.65403  \n9    1158        172             NaN                202994  1130.97      NaN  \n10   3840         23     8.47511e-05                271383  1938.45  84.2804  \n11   3840         23             NaN                271383  1938.45      NaN  \n12   1127          4     3.96287e-05                100937  720.979  180.245  \n13    183          0               0                  7221  51.5786        0  \n14   1310          4             NaN                108158  772.557      NaN  \n15      1          0               0                 13479  75.0973        0  \n16    792          0               0                 66903  372.745        0  \n17      1          0               0                  3853  21.4667        0  \n18    266          0               0                 49426  275.373        0  \n19   1060          0             NaN                133661  744.683      NaN  \n20  11529        222             NaN           1.03243e+06  6845.46      NaN  \n\n#if necessary write to file\ndf.to_excel(\'file.xlsx\', index=False)\n\ncols = ["Delivered Impressions", "Clicks", "Conversion", "Spend"]\n\ndf[\'Placement# Name\'] = df[\'Placement# Name\'].ffill()\ngrand = df[cols].sum()\ngrand.loc[\'Placement# Name\'] = \'Grand total\'\nprint (grand)\n\ndf1 = df.groupby(\'Placement# Name\')[cols].sum()\ndf1.index = df1.index + \'____\'\n\n#create empty DataFrame\ndf2 = pd.DataFrame(index=df1.index + \'__\')\ndf = pd.concat([df.set_index(\'Placement# Name\'), df1, df2], keys=(\'a\', \'b\', \'c\')).sort_index(level=1).reset_index()\n\n#get output by 2 conditions\nm1 = df[\'level_0\'] == \'a\'\nm2 = df[\'level_0\'] == \'c\'\ndf[\'Placement# Name\'] = np.select([m1, m2], [df[\'Placement# Name\'], np.nan], default=\'Total\')\ndf = df.drop(\'level_0\', axis=1)\ndf.loc[len(df.index)] = grand\n\nprint (df)\n0   13.iab units (mobile only) + non-expanding adh...   320x50    0.0119683   \n1                                               Total      NaN          NaN   \n2                                                 NaN      NaN          NaN   \n3   15.iab units (mobile only) + non-expanding adh...   320x50    0.0138741   \n4   15.iab units (mobile only) + non-expanding adh...   768x90    0.0271041   \n5                                               Total      NaN          NaN   \n6                                                 NaN      NaN          NaN   \n7                     18.iab units - desktop + mobile  160x600   0.00155927   \n8                     18.iab units - desktop + mobile  300x250   0.00797965   \n9                     18.iab units - desktop + mobile  300x600   0.00275059   \n10                    18.iab units - desktop + mobile   728x90   0.00496391   \n11                                              Total      NaN          NaN   \n12                                                NaN      NaN          NaN   \n13  4.iab units (mobile only) + non-expanding adhe...   320x50    0.0141497   \n14                                              Total      NaN          NaN   \n15                                                NaN      NaN          NaN   \n16  5.iab units (mobile only) + non-expanding adhe...   320x50    0.0111654   \n17  5.iab units (mobile only) + non-expanding adhe...   768x90    0.0253428   \n18                                              Total      NaN          NaN   \n19                                                NaN      NaN          NaN   \n20                     6.iab units - desktop + mobile  160x600  7.41895e-05   \n21                     6.iab units - desktop + mobile  300x250     0.011838   \n22                     6.iab units - desktop + mobile  300x600  0.000259538   \n23                     6.iab units - desktop + mobile   728x90   0.00538178   \n24                                              Total      NaN          NaN   \n25                                                NaN      NaN          NaN   \n26                                        Grand total      NaN          NaN   \n\n   Clicks Conversion Conversion Rate Delivered Impressions    Spend     eCPA  \n0    1888          4     2.53566e-05                157750  1126.79  281.696  \n1    1888          4             NaN                157750  1126.79      NaN  \n2     NaN        NaN             NaN                   NaN      NaN      NaN  \n3    2121         17     0.000111202                152875  1091.96  64.2332  \n4     152          2     0.000356633                  5608  40.0571  20.0286  \n5    2273         19             NaN                158483  1132.02      NaN  \n6     NaN        NaN             NaN                   NaN      NaN      NaN  \n7      37         21     0.000884993                 23729  132.204  6.29545  \n8     684         58     0.000676637                 85718  477.572    8.234  \n9      34         13      0.00105169                 12361  68.8684  5.29757  \n10    403         80     0.000985392                 81186  452.322  5.65403  \n11   1158        172             NaN                202994  1130.97      NaN  \n12    NaN        NaN             NaN                   NaN      NaN      NaN  \n13   3840         23     8.47511e-05                271383  1938.45  84.2804  \n14   3840         23             NaN                271383  1938.45      NaN  \n15    NaN        NaN             NaN                   NaN      NaN      NaN  \n16   1127          4     3.96287e-05                100937  720.979  180.245  \n17    183          0               0                  7221  51.5786        0  \n18   1310          4             NaN                108158  772.557      NaN  \n19    NaN        NaN             NaN                   NaN      NaN      NaN  \n20      1          0               0                 13479  75.0973        0  \n21    792          0               0                 66903  372.745        0  \n22      1          0               0                  3853  21.4667        0  \n23    266          0               0                 49426  275.373        0  \n24   1060          0             NaN                133661  744.683      NaN  \n25    NaN        NaN             NaN                   NaN      NaN      NaN  \n26  11529        222             NaN           1.03243e+06  6845.46      NaN  \n'
'year_1900 = datetime_object.year - 100\ndatetime_object = datetime_object.replace(year=year_1900)\n'
"import pandas as pd\n\npd.merge_asof(df2.sort_values('time'), df1.sort_values('time'), on='time', direction='nearest')\n#          time  velocity   yawrate  velocity_x       yaw\n#0  35427009860   12.6556 -0.074351     12.5451 -0.078781\n#1  35427029728   12.6556 -0.074351     12.5451 -0.078781\n#2  35427049705   12.6444 -0.074351     12.5451 -0.078781\n#3  35427929709   12.6583 -0.075049     12.5351 -0.079489\n#4  35427949712   12.6556 -0.075049     12.5401 -0.079591\n"
'count_the_feat = StringCount(es[\'transactions\'][\'product_id\'], es[\'sessions\'], string="5")\n\nsessions.STRING_COUNT(product_id, "5")\n'
'i = np.sign(df.a)\nj = np.sign(df.b)\n\ni = i.mask(i != j).ffill()\ni &gt;= 0\n\n# for your `lst` and `lst2` input \n0    False\n1     True\n2     True\n3    False\n4     True\n5     True\n6    False\n7    False\n8    False\n9    False\nName: a, dtype: bool\n'
"new_df = df[['Image']].join(df['Bounding Boxes'].str.split(' ', expand=True))\n\n&gt;&gt;&gt; new_df\n   Image    0    1    2    3    4     5     6     7     8     9\n0  a.jpg  xyz  0.1  0.2  0.3  0.4  None  None  None  None  None\n1  b.jpg  xyz  0.1  0.2  0.3  0.4   ijk   0.4   0.3   0.2   0.1\n"
'listRules = [list(MB[i][0]) for i in range(0,len(MB))]\n'
'import ast\n\nd = {\'Answer.annotation_data\': [\'[{"left":171,"top":0,"width":163,"height":137,"label":"styrofoam container"},{"left":222,"top":42,"width":45,"height":70,"label":"chopstick"}]\',\n                                \'[{"left":170,"top":10,"width":173,"height":157,"label":"styrofoam container"},{"left":222,"top":42,"width":45,"height":70,"label":"chopstick"}]\']}\ndf = pd.DataFrame(d)\n\nprint (df)\n                              Answer.annotation_data\n0  [{"left":171,"top":0,"width":163,"height":137,...\n1  [{"left":170,"top":10,"width":173,"height":157...\n\n#convert string data to list of dicts if necessary\ndf[\'Answer.annotation_data\'] = df[\'Answer.annotation_data\'].apply(ast.literal_eval)\n\ndef get_val(val):\n    comb = [[y.get(val, np.nan) for y in x] for x in df[\'Answer.annotation_data\']]\n    return pd.DataFrame(comb).add_prefix(\'{}_\'.format(val))\n\ncols = [\'left\',\'top\',\'width\',\'height\']\ndf1 = pd.concat([get_val(x) for x in cols], axis=1)\nprint (df1)\n   left_0  left_1  top_0  top_1  width_0  width_1  height_0  height_1\n0     171     222      0     42      163       45       137        70\n1     170     222     10     42      173       45       157        70\n'
"d = {1: 'yes', 0: 'no'}\nres = df.join(df.pop('Intervention').str.get_dummies(', ').applymap(d.get))\n\nprint(res)\n\n   ID Blood Draw Blood return Verified Cap Changed Flushed Heparin-Locked  \\\n0   1        yes                    no          no     yes             no   \n1   1        yes                    no          no      no            yes   \n2   1        yes                    no          no     yes             no   \n3   2         no                   yes          no     yes             no   \n4   2         no                    no         yes      no             no   \n5   3         no                    no          no      no             no   \n\n  Locked Port De-Accessed Tubing Changed  \n0    yes               no             no  \n1     no              yes            yes  \n2     no               no             no  \n3     no               no             no  \n4     no               no             no  \n5     no              yes             no  \n\ndf = pd.DataFrame({'ID': [1, 1, 1, 2, 2, 3],\n                   'Intervention': ['Blood Draw, Flushed, Locked',\n                                    'Blood Draw, Port De-Accessed, Heparin-Locked, Tubing Changed',\n                                    'Blood Draw, Flushed', 'Blood return Verified, Flushed',\n                                    'Cap Changed', 'Port De-Accessed']})\n"
'In [483]: a = np.arange(300).reshape(100,3)\nIn [484]: b=np.array([8,9])\nIn [485]: res = np.zeros((100,5),int)\nIn [486]: res[:,:3]=a\nIn [487]: res[:,3:]=b\n\nIn [488]: %%timeit\n     ...: res = np.zeros((100,5),int)\n     ...: res[:,:3]=a\n     ...: res[:,3:]=b\n     ...: \n     ...: \n6.11 µs ± 20.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\nIn [491]: timeit np.concatenate((a, b.repeat(100).reshape(2,-1).T),1)\n7.74 µs ± 15.1 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\nIn [164]: timeit np.concatenate([a, np.ones([a.shape[0],1], dtype=int).dot(np.array([b]))], axis=1) \n8.58 µs ± 160 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n'
'fig, ax = plt.subplots(figsize=(16,9), ncols=3, nrows=2)\nfor col, elem in zip(df.columns[:-1], ax.flat):\n    sns.countplot(x="CLASS", hue=col, data=df, ax=elem)\n'
"df.boxplot(column = ['Shares'],by='Day')\n"
'contributions = [a==b for a, b in zip(l1, l2)]\nplt.plot(list(range(len(contributions)), contributions)\n'
"from __future__ import division #For decimal division.\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef nlvh(x, H, C):\n    return ((H-xi*C)/8.314)*((1/xi) - x) + (C/8.314)*np.log((1/x)/xi) + np.log(yi)\n\nxdata = np.arange(1,21) #Choose an array for x.\n\n#Choose an array for y.\nydata = np.array([-0.1404996,  -0.04353953,  0.35002257,  0.12939468, -0.34259184, -0.2906065,\n     -0.37508709, -0.41583238, -0.511851,   -0.39465581, -0.32631751, -0.34403938,\n     -0.592997,   -0.34312689, -0.4838437,  -0.19311436, -0.20962735, -0.31134191, \n     -0.09487793, -0.55578775])\n\n\nH_lst, C_lst = [], []\nfor i in range( len(xdata)-5 ):\n    #Select 5 consecutive points of xdata (from index i to i+4).\n    xnew = xdata[i: i+5]\n    globals()['xi'] = xnew[0]\n\n    #Select 5 consecutive points of ydata (from index i to i+4).\n    ynew = ydata[i: i+5]\n    globals()['yi'] = ynew[0]  \n\n    #Fit function nlvh to data using scipy.optimize.curve_fit\n    popt, pcov = curve_fit(nlvh, xnew, ynew, maxfev=100000)\n\n    #Optimal values for H from minimization of sum of the squared residuals.\n    H_lst += [popt[0]] \n\n    #Optimal values for C from minimization of sum of the squared residuals.\n    C_lst += [popt[1]] \n\nH_arr, C_arr = np.asarray(H_lst), np.asarray(C_lst) #Convert list to numpy arrays.\n\nprint H_arr \n&gt;&gt;&gt;[1.0, 1.0, -23.041138662879327, -34.58915200575536, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n\nprint C_arr\n&gt;&gt;&gt;[1.0, 1.0, -8.795855063863234, -9.271561975595562, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
"#convert to datetimes\ndf['Timestamp'] = pd.to_datetime(df['Timestamp'])\n#identify difference 5Min for each group with cumulative sum\ng = (df.groupby('user')['Timestamp'].diff() &gt; pd.Timedelta('5Min')).cumsum()\n#create counter of groups\ndf['session_id'] = df.groupby(['user', g], sort=False).ngroup() + 1\nprint (df)\n            Timestamp   user event code  session_id\n0 2019-01-07 13:30:23  user1   event123           1\n1 2019-01-07 13:30:45  user1   event123           1\n2 2019-01-07 13:30:56  user3   event123           2\n3 2019-01-07 13:40:15  user2   event123           3\n4 2019-01-07 13:55:20  user1   event123           4\n"
"import matplotlib.pyplot as plt, pandas as pd, numpy as np\n# generate dummy data.\nX  = np.random.rand(10, 2)\nX[:,1] *= 1000\nx  = np.arange(X.shape[0]) * 2 # xticks\ndf = pd.DataFrame(X, columns = 'Spend Roast'.split())\n# end dummy data\n\nfig, ax1 = plt.subplots(figsize = (9,6),sharex=True)\nax2 = ax1.twinx()\n\n# tmp axes\naxes = [ax1, ax2] # setup axes\ncolors = plt.cm.tab20(x)\nwidth = .5 # bar width\n\n# generate dummy legend\nelements = []\n# plot data\nfor idx, col in enumerate(df.columns):\n    tax = axes[idx]\n    tax.bar(x + idx * width, df[col], label = col, width = width, color = colors[idx])\n    element = tax.Line2D([0], [0], color = colors[idx], label = col) # setup dummy label\n    elements.append(element)\n# desired hline\ntax.axhline(200, color = 'red')\ntax.set(xlabel = 'Bundle FC', ylabel = 'ROAST')\naxes[0].set_ylabel('SPEND')\ntax.legend(handles = elements)\n"
"import matplotlib.pyplot as plt\nimport matplotlib\nmat = [[ 150. ,  0.    ,   0.     ,  0.   , 0.   ,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.   ,    0.    ,   0.     ,  0.   ,  0.     ,  0.     ,  0.      , 0.     ,  0.   ],\n       [ 150. , 69.388 ,  35.36   , 18.211, 7.851,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   ,  -0.03  ,  -0.047 ,  -0.044  , -0.027 ,   0.   ],\n       [ 150. , 92.192 ,  53.842  , 29.633,13.192,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   ,  -0.075 ,  -0.112 ,  -0.104  , -0.062 ,   0.   ],\n       [ 150. , 95.538 ,  58.184  , 33.287,15.285,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   ,  -0.156 ,  -0.221 ,  -0.199  , -0.116 ,   0.   ],\n       [ 150. , 81.776 ,  50.068  , 30.045,14.659,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   ,  -0.331 ,  -0.416 ,  -0.357  , -0.203 ,   0.   ],\n       [   0. , 31.498 ,  30.267  , 22.168,13.308,  5.653,    2.766  ,  1.5    ,  0.86  ,   0.492  ,  0.254  , 0.082, -0.058,   -0.182  , -0.298 ,  -0.41 ,   -0.515  , -0.617,  -0.751 ,  -0.754 ,  -0.609  , -0.34  ,   0.   ],\n       [   0. , 13.948 ,  17.335  , 15.05 ,10.751,  6.537,    3.913  ,  2.374  ,  1.447 ,   0.854  ,  0.443  , 0.13 , -0.132,   -0.372  , -0.602  , -0.826 ,  -1.034  , -1.204,  -1.301 ,  -1.241 ,  -0.985  , -0.548 ,   0.   ],\n       [   0. ,  6.958 ,  10.074  ,  9.949, 8.107,  5.832,    3.973  ,  2.635  ,  1.7   ,   1.034  ,  0.534  , 0.129, -0.229,   -0.57   , -0.913  , -1.258 ,  -1.59   , -1.864,  -2.007 ,  -1.924 ,  -1.543  , -0.866 ,   0.   ],\n       [   0. ,  3.812 ,   6.053  ,  6.564, 5.896,  4.712,    3.511  ,  2.493  ,  1.685 ,   1.048  ,  0.529  , 0.079, -0.342,   -0.768  , -1.22   , -1.705 ,  -2.203  , -2.654,  -2.941 ,  -2.904 ,  -2.395  , -1.375 ,   0.   ],\n       [   0. ,  2.237 ,   3.761  ,  4.358, 4.202,  3.609,    2.867  ,  2.142  ,  1.498 ,   0.943  ,  0.455  , 0.002, -0.452,   -0.94   , -1.495  , -2.139 ,  -2.864  , -3.607,  -4.2   ,  -4.357 ,  -3.76   , -2.237 ,   0.   ],\n       [   0. ,  1.375 ,   2.396  ,  2.906, 2.943,  2.656,    2.206  ,  1.708  ,  1.223 ,   0.771  ,  0.345  ,-0.076, -0.526,   -1.045  , -1.682  , -2.491 ,  -3.509  , -4.71 ,  -5.895 ,  -6.563 ,  -6.052  , -3.812 ,   0.   ],\n       [   0. ,  0.867 ,   1.543  ,  1.925, 2.009,  1.865,    1.592  ,  1.261  ,  0.915 ,   0.573  ,  0.232  ,-0.126, -0.531,   -1.031  , -1.698  , -2.633 ,  -3.971  , -5.831,  -8.105 ,  -9.948 , -10.073  , -6.958 ,   0.   ],\n       [   0. ,  0.548 ,   0.986  ,  1.242, 1.302,  1.205,    1.035  ,  0.828  ,  0.604 ,   0.373  ,  0.134  ,-0.128, -0.441,   -0.852  , -1.445  , -2.372 ,  -3.911  , -6.536, -10.749 , -15.049 , -17.334  ,-13.947 ,   0.   ],\n       [   0. ,  0.34  ,   0.609  ,  0.755, 0.751,  0.618,    0.516  ,  0.411  ,  0.299 ,   0.183  ,  0.059  ,-0.081, -0.253,   -0.491  , -0.859  , -1.499 ,  -2.766  , -5.652, -13.307 , -22.167 , -30.267  ,-31.497 ,   0.   ],\n       [   0. ,  0.203 ,   0.357  ,  0.416, 0.331,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.     ,  0.    ,   0.     ,  0.   , -14.659 , -30.045 , -50.068  ,-81.776 ,-150.   ],\n       [   0. ,  0.116 ,   0.2    ,  0.221, 0.157,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   , -15.284 , -33.286 , -58.184  ,-95.538 ,-150.   ],\n       [   0. ,  0.062 ,   0.104  ,  0.112, 0.075,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   , -13.192 , -29.633 , -53.842  ,-92.192 ,-150.   ],\n       [   0. ,  0.027 ,   0.044  ,  0.047, 0.03 ,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   ,  -7.851 , -18.211 , -35.36   ,-69.388 ,-150.   ],\n       [   0. ,  0.    ,   0.     ,  0.   , 0.   ,  0.   ,    0.     ,  0.     ,  0.    ,   0.    ,   0.    ,  0.  ,   0.  ,     0.    ,   0.    ,   0.   ,    0.     ,  0.   ,   0.    ,   0.    ,   0.     ,  0.    ,-150.   ]]\nplt.matshow(mat, aspect='auto', norm=matplotlib.colors.SymLogNorm(0.1))\nplt.show()\n"
'df = df.replace(0, np.nan)\n\nm = df.isnull()\ndf1 = df.mask(m.any(axis=1)).ffill()\ndf2 = df.mask(m.all(axis=1), df1, axis=1)\nprint (df2)\n       18    19    20\n197  14.0  28.0  14.0\n198  14.0   NaN  14.0\n200  14.0  28.0  14.0\n201  14.0  28.0  14.0\n202  15.0  23.0  12.0\n203  16.0   NaN  18.0\n204  15.0  23.0  12.0\n205  15.0  23.0  12.0\n'
'class_probabilitiesDec = clf.predict_proba(X_test) \n\n[[ 0.00490808  0.00765327  0.01123035  0.00332751  0.00665502  0.00357707\n   0.05182597  0.03169453  0.04267532  0.02761833  0.01988187  0.01281091\n   0.02936528  0.03934781  0.02329257  0.02961484  0.0353548   0.02503951\n   0.03577073  0.04700108  0.07661592  0.04433907  0.03019715  0.02196157\n   0.0108976   0.0074869   0.0291989   0.03951418  0.01372598  0.0176358\n   0.02345895  0.0169703   0.02487314  0.01813493  0.0482489   0.01988187\n   0.03252641  0.01572249  0.01455786  0.00457533  0.00083188]\n\nclf = DecisionTreeClassifier()\nclf.fit([[1],[2],[3],[4],[5],[6],[7]], [[11],[12],[13],[13],[12],[11],[13]])\n\nclf.predict([[5]])\n\nclf.predict_proba([[5]])\n\nprobabilities = clf.predict_proba([[5]])[0]\n{clf.classes_[i] : probabilities[i] for i in range(len(probabilities))}\n\n{11: 0.0, 12: 1.0, 13: 0.0}\n'
'def conv(s, conv_from="K", conv_to=1000):\n    return s.mask(\n        s.str.contains(f"\\d+{conv_from}", na=False),\n        pd.to_numeric(s.str.replace(conv_from,""), \n                      errors="coerce") * conv_to,\n        errors="ignore")\n\n# get rid of commas and spaces    \ndf["price"] = df["price"].str.replace(r"[\\s,]", "")\n\ndf["price"] = df["price"].pipe(conv, "[Kk]", 10**3).pipe(conv, "[Mm]", 10**6)\n\nIn [96]: df\nOut[96]:\n      price\n0        3K\n1     0.56M\n2      2050\n3      1.5K\n4  4,059.99\n\nIn [97]: df["price"] = df["price"].str.replace(r"[\\s,]", "")\n\nIn [98]: df["price"] = df["price"].pipe(conv, "[Kk]", 10**3).pipe(conv, "[Mm]", 10**6)\n\nIn [99]: df\nOut[99]:\n     price\n0     3000\n1   560000\n2     2050\n3     1500\n4  4059.99\n'
'import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\'titanic_data/train.csv\', skipinitialspace=True)\n\nmales = data[data[\'Sex\']==\'male\'][\'Survived\'].value_counts()\nfemales = data[data[\'Sex\']==\'female\'][\'Survived\'].value_counts()\n\nfigure1 = plt.bar(range(len(males)), males, align=\'edge\', width=0.4, label=\'Male\')\nfigure2 = plt.bar(range(len(females)), females, align=\'edge\', width=-0.4, label=\'Female\')\nplt.legend()\nplt.xticks(np.arange(len(males)), rotation=0)\nplt.title("Third class survivors")\n\nplt.show()\n'
'from tkinter import *\nfrome tkinter.filedialog import askopenfilename\nimport pandas as pd\n\nTk().withdraw()\nprint("Please select a csv file to load") \nfile = askopenfilename()\ndf = pd.read_csv(file, header = 0)\n'
"df['team'] = [1 if p&lt;=5 else 2 for p in df.player]\ndf.groupby(['matchid', 'team'])['visionscore', 'win'].sum()\n\nmatchid team    visionscore win\n10      1       274         3\n10      2       124         3\n11      1       416         3\n11      2       89          0\n\n"
'data = {\'labels\': ["A-F", "G-L", "M-R", "S-Z"], \'count\':[1882, 3096, 3830, 1017]}\ndf = pd.DataFrame.from_dict(data)\n\nprint(df)\n\n  labels  count\n0    A-F   1882\n1    G-L   3096\n2    M-R   3830\n3    S-Z   1017\n\ndf[\'percentage\'] = (df[\'count\'] / df[\'count\'].sum()) * 100\n\nprint(df)\n\n  labels  count  percentage\n0    A-F   1882   19.155216\n1    G-L   3096   31.511450\n2    M-R   3830   38.982188\n3    S-Z   1017   10.351145\n'
"print (df)\n                   Column1\n0                      NaN\n1                    B2-52\n2  C3-1245Â¯main_123456789\n3                       D4\n4                   Z89028\n5  F7Â¯main_123456789,Z241\n\nfor c in df.columns:\n    out = []\n    for x in df[c]:\n        if x == x:\n            p = x.find('¯')\n            if p != -1:\n                out.append(x[:p] + x[p+14:])\n            else:\n                out.append(x)\n        else:\n            out.append(x)\n    df[c] = out\n\nprint (df)\n     Column1\n0        NaN\n1      B2-52\n2  C3-1245Â9\n3         D4\n4     Z89028\n5  F7Â9,Z241\n"
"df = pd.read_csv('Store_scores.csv')\n\n#removes duplicates\nstores = set(df.index)\n\n#iterate over the stores and plot\nfor store in stores:    \n    sns.boxplot(data=df.loc[store,:], y=values, x=categories)\n\n\n"
"&gt;&gt;&gt; from collections import Counter\n&gt;&gt;&gt; name_numbers = Counter()\n&gt;&gt;&gt; for item in json_data:\n...     name_numbers[item['name']] += item['items']\n...\n&gt;&gt;&gt; name_numbers\nCounter({'Gaviscon Advance_Liq (Peppermint) S/F': 36, 'Sod Algin/Pot Bicarb_Susp S/F': 14, 'Sod Alginate/Pot Bicarb_Tab Chble 500mg': 3, 'Co-Magaldrox_Susp 195mg/220mg/5ml S/F': 2, 'Alginate_Raft-Forming Oral Susp S/F': 1, 'Alverine Cit_Cap 60mg': 1})\n&gt;&gt;&gt; name_numbers.most_common(1)\n[('Gaviscon Advance_Liq (Peppermint) S/F', 36)]\n"
"&gt;&gt;&gt; [x for sublist in master_list for x in sublist or [] if x]\n['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.3.',\n 'the supply fan is running, the VFD speed output mean value is 94.3.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.2.',\n 'the supply fan is running, the VFD speed output mean value is 94.2.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.1.',\n 'the supply fan is running, the VFD speed output mean value is 94.1.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.0.',\n 'the supply fan is running, the VFD speed output mean value is 94.0.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 93.9.',\n 'the supply fan is running, the VFD speed output mean value is 93.9.']\n"
"import numpy as np\nfrom PIL import Image\n\narr = np.load('img.npy')\nimg = Image.fromarray(arr)\nimg.resize(size=(100, 100))\n\nimport skimage.transform as st\n\nst.resize(arr, (100, 100))\n"
'import pandas as pd\nraw_data = [{\'Date\': \'1-10-19\', \'Price\': 7, \'Check\': 0},\n            {\'Date\': \'2-10-19\', \'Price\': 8.5, \'Check\': 0},\n            {\'Date\': \'3-10-19\', \'Price\': 9, \'Check\': 1},\n            {\'Date\': \'4-10-19\', \'Price\': 50, \'Check\': 1},\n            {\'Date\': \'5-10-19\', \'Price\': 80, \'Check\': 1},\n            {\'Date\': \'6-10-19\', \'Price\': 100, \'Check\': 1}]\ndf = pd.DataFrame(raw_data)\n\nnew_column = [None] * len(df["Price"])  # create new column\n\nfor i in range(len(df["Price"])):\n    if df[\'Check\'][i] == 1:\n        percent_10 = df[\'Price\'][i] * 0.1\n        for j in range(i, -1, -1):\n            print(j)\n            if df[\'Price\'][j] &lt; percent_10:\n                new_column[i] = i - j\n                break\n\n\ndf["New"] = new_column  # add new column\n\nprint(df)\n'
'img_rgb = cv2.resize(img_rgb,(224,224),3)  # resize\nimg_rgb = np.array(img_rgb).astype(np.float32)/255.0  # scaling\nimg_rgb = np.expand_dims(img_rgb, axis=0)  # expand dimension\ny_pred = model.predict(img_rgb) # prediction\ny_pred_class = y_pred.argmax(axis=1)[0]\n'
'from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(80, 155))\nX = scaler.fit_transform(X)\ny = scaler.fit_transform(y)\n'
'df1 = df[df[\'Date_1\'] == df[\'Date_2\'])\n\ndf1 = df.query("Date_1 == Date_2")\n'
"#one possible solution is generate dictionary from existing values \nd = df.dropna().set_index('currency')['country'].to_dict()\n#or use dictionary\n#d = {'GBP': 'GB', 'USD': 'US', 'AUD': 'AUD'}\ndf['country'] = df['country'].fillna(df['currency'].map(d))\nprint (df)\n  currency country\n0      GBP      GB\n1      USD      US\n2      USD      US\n3      AUD     AUD\n4      GBP      GB\n\nd = {'GBP': 'GB', 'USD': 'US', 'AUD': 'AUD'}\ndf['country'] = df['currency'].map(d)\nprint (df)\n  currency country\n0      GBP      GB\n1      USD      US\n2      USD      US\n3      AUD     AUD\n4      GBP      GB\n\nprint (df)\n  currency country\n0     GBP1     GB1\n1      USD     NaN\n2      USD      US\n3      AUD     AUD\n4      GBP     NaN\n\nd = {'GBP': 'GB', 'USD': 'US', 'AUD': 'AUD'}\ndf['country'] = df['currency'].map(d)\nprint (df)\n  currency country\n0     GBP1     NaN\n1      USD      US\n2      USD      US\n3      AUD     AUD\n4      GBP      GB\n\nd = {'GBP': 'GB', 'USD': 'US', 'AUD': 'AUD'}\ndf['country'] = df['country'].fillna(df['currency'].map(d))\nprint (df)\n  currency country\n0     GBP1     GB1\n1      USD      US\n2      USD      US\n3      AUD     AUD\n4      GBP      GB\n"
"df = df_filter['b'].explode().reset_index().merge(df)\nprint (df)\n     a  b\n0  201  1\n1  201  2\n2  201  3\n3  202  1\n4  202  2\n5  203  1\n\ndf_filter = pd.DataFrame({'a': [      201,    202, 203],\n                       'b': [[1, 2, 3], [1, 2], [1]]})\n\ndf = df_filter.explode('b').merge(df)\nprint (df)\n     a  b\n0  201  1\n1  201  2\n2  201  3\n3  202  1\n4  202  2\n5  203  1\n\ndf = df_filter.explode('b').merge(df.reset_index()).set_index('index')\nprint (df)\n         a  b\nindex        \n0      201  1\n1      201  2\n2      201  3\n4      202  1\n5      202  2\n7      203  1\n"
'import plotly.express as px\nimport pandas as pd\n\ngapminder = px.data.gapminder()\ngapminder2=gapminder.copy(deep=True)\n\ngapminder[\'planet\']=\'earth\'\ngapminder2[\'planet\']=\'mars\'\ngapminder3=pd.concat([gapminder, gapminder2])\n\nfig = px.bar(gapminder3, x="continent", y="pop", color="planet",\n  animation_frame="year", animation_group="country", range_y=[0,4000000000*2])\nfig.show()\n'
'import os\nimport  imageio\nimport pandas as pd\n\ncatimages = os.listdir("Cat")\ndogimages = os.listdir("Dog")\ncatVec = []\ndogVec = []\nfor img in catimages:\n       img = imageio.imread(f"Cat/{img}")\n       ar = img.flatten()\n       catVec.append(ar)    \ncatdf = pd.DataFrame(catVec)    \ncatdf.insert(loc=0,column ="label",value=1)\n\nfor img in dogimages:\n       img = imageio.imread(f"Dog/{img}")\n       ar = img.flatten()\n       dogVec.append(ar)    \ndogdf = pd.DataFrame(dogVec)    \ndogdf.insert(loc=0,column ="label",value=0)\n\ndata = pd.concat([catdf,dogdf])      \ndata = data.sample(frac=1)\n'
"df1 = df.groupby('Auto Center', 'Model', 'Make', 'Year', 'Color').count()\n\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import row_number, desc\n\nw1 = Window.partitionBy('Auto Center').orderBy(desc('count'))\n\ndf_new = df1.withColumn('rn', row_number().over(w1)).where('rn &lt;= 5').drop('rn')\n"
"bins = (2, 3, 5)\n\nimport pandas as pd\n\nappStore = pd.DataFrame()\nappStore['user_rating'] = [2.3, 3.3, 4, 6]\n\nbins = (2, 3, 5)\ngroup_names = ['bad', 'good']\nappStore['user_rating'] = pd.cut(appStore['user_rating'], bins=bins, labels=group_names)\nprint(appStore['user_rating'].unique())\n\nprint()\nprint(appStore)\n\n[bad, good, NaN]\nCategories (2, object): [bad &lt; good]\n\n  user_rating\n0         bad\n1        good\n2        good\n3         NaN\n\nimport pandas as pd\n\nappStore = pd.DataFrame()\nappStore['user_rating'] = [2.3, 3.3, 4, 4.5]\n\nbins = (2, 3, 5)\ngroup_names = ['bad', 'good']\nappStore['user_rating'] = pd.cut(appStore['user_rating'], bins=bins, labels=group_names)\nprint(appStore['user_rating'].unique())\n\nprint()\nprint(appStore)\n\n[bad, good]\nCategories (2, object): [bad &lt; good]\n\n  user_rating\n0         bad\n1        good\n2        good\n3        good\n"
"import pandas as pd\n\ndata = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\n\ndata\n"
'df = df.repartition(partition_size="100MB")\n\ndf.to_parquet(output_path)\n'
"X = df[['Open','Close','High','Low','Volume']].values\n\ny = df['adj close'].values\n"
'import random\n\nmaxy=max(y)\nlval=[]\nfor i in range(10000):\n    ## pick a random y belonging to selection\n    xv=random.choice(selection)\n    yv=random.random()*maxy\n    yral=rayleigh.pdf(xv, RAYLEIGH_ARG_1, RAYLEIGH_ARG_2)\n    if(yv&lt;=yral):\n       lval.append(xv)\nplt.plot(selection, y, "r-",  label="Density")\nplt.hist(lval, bins=10, density=True, label="Histogram", rwidth=0.1)\n\nr=rayleigh.rvs(loc=RAYLEIGH_ARG_1, scale=RAYLEIGH_ARG_2, size=1000)\nplt.plot(selection, y, "r-",  label="Density")\nplt.hist(r, bins=10, density=True, label="Histogram", rwidth=0.1)\n'
"import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom scipy.optimize import curve_fit\nfrom scipy.interpolate import splrep, splev\n\ndf = pd.DataFrame()\n# mdates.date2num allows functions like curve_fit and spline to digest time series data\ndf['dates'] = mdates.date2num(dates)\ndf['values'] = values \n\n# Exponential fit function\ndef exponential_func(x, a, b, c, d):\n    return a*np.exp(b*(x-c))+d\n\n# Spline interpolation\ndef spline_interp(x, y, x_new):\n    tck = splrep(x, y)\n    return splev(x_new, tck)\n\n# define forecast timerange (forecasting 5 days into future)\ndates_forecast = np.linspace(df['dates'].min(), df['dates'].max() + 5, 100)\ndd = mdates.num2date(dates_forecast)\n\n# Doing exponential fit\npopt, pcov = curve_fit(exponential_func, df['dates'], df['values'], \n                       p0=(1, 1e-2, df['dates'][0], 1))\n\n# Doing spline interpolation\nyy = spline_interp(df['dates'], df['values'], dates_forecast)\n\n# Interpolating data for exponential smoothing (no missing data in time series allowed)\ndf_interp = pd.DataFrame()\ndf_interp['dates'] = np.arange(dates[0], dates[-1] + 1, dtype='datetime64[D]')\ndf_interp['values'] = spline_interp(df['dates'], df['values'], \n                                    mdates.date2num(df_interp['dates']))\nseries_interp = pd.Series(df_interp['values'].values, \n                          pd.date_range(start='2020-01-19', end='2020-03-04', freq='D'))\n\n# Now the exponential smoothing works fine, provide the `trend` argument given your data \n# has a clear (kind of exponential) trend\nfit1 = ExponentialSmoothing(series_interp, trend='mul').fit(optimized=True)\n\n# Plot data\nplt.plot(mdates.num2date(df['dates']), df['values'], 'o')\n# Plot exponential function fit\nplt.plot(dd, exponential_func(dates_forecast, *popt))\n# Plot interpolated values\nplt.plot(dd, yy)\n# Plot Exponential smoothing prediction using function `forecast`\nplt.plot(np.concatenate([series_interp.index.values, fit1.forecast(5).index.values]),\n     np.concatenate([series_interp.values, fit1.forecast(5).values]))\n\n&gt;&gt; fit1.predict(start=np.datetime('2020-03-01'), end=np.datetime64('2020-03-09'))\n2020-03-01    4240.649526\n2020-03-02    5631.207307\n2020-03-03    5508.614325\n2020-03-04    5898.717779\n2020-03-05    6249.810230\n2020-03-06    6767.659081\n2020-03-07    7328.416024\n2020-03-08    7935.636353\n2020-03-09    8593.169945\nFreq: D, dtype: float64\n\n&gt;&gt; fit1.forecast(5)\n2020-03-05    6249.810230\n2020-03-06    6767.659081\n2020-03-07    7328.416024\n2020-03-08    7935.636353\n2020-03-09    8593.169945\nFreq: D, dtype: float64\n"
'datapoints = []\n# ...\n                            datapoints.append((key,ts,delay))\n# ...\nnpdata = np.array(datapoints, dtype=int)\n'
'books = {k: [x.split(" by ")[0] for x in v] for k, v in books.items()}\n'
"from collections import defaultdict\n\ndic = {\n    'key-1': [('blue', '-20'), ('red', '-67')], \n    'key-2': [('blue', '-77'), ('cyan', '-67'), ('white', '-57')],\n    'key-3': [('blue', '-39'), ('cyan' , '-35'), ('purple', '-60')]\n}\n\ncolor_weight_groups = defaultdict(list)\n\nfor lst in dic.values():\n    for color, weight in lst:\n        color_weight_groups[color].append(weight)\n\nmax_weights = {k: max(map(int, v)) for k, v in color_weight_groups.items()}\n\nprint(max_weights)\n\n{'blue': -20, 'red': -67, 'cyan': -35, 'white': -57, 'purple': -60}\n\nresult = {\n    k: [(color, weight) for color, weight in v if int(weight) == max_weights[color]]\n    for k, v in dic.items()\n}\n\nprint(result)\n\n{'key-1': [('blue', '-20'), ('red', '-67')], 'key-2': [('white', '-57')], 'key-3': [('cyan', '-35'), ('purple', '-60')]}\n"
'from sklearn.datasets import make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n\nclf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\nclf.fit(X_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ny_pred = clf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\n\ncm_display = ConfusionMatrixDisplay(cm, [0,1]).plot()\n'
'from io import StringIO\ntest = "Baby_names,age,country\\nsarah,4,USA\\njames,1,UK\\nsarah,2,\'UK\'\\n\'sarah,3,France\\n\'john,2,UK\\njames,6,Australia"\na = np.genfromtxt(StringIO(test), delimiter=\',\',usecols=(0), skip_header=1, dtype=str)\nprint(a)\n\n[\'sarah\' \'james\' \'sarah\' "\'sarah" "\'john" \'james\']\n\nunique, counts = np.unique(a, return_counts=True)\nx = dict(zip(unique, counts))\n\n{"\'john": 1, "\'sarah": 1, \'james\': 2, \'sarah\': 2}\n\nprint([key for key, value in x.items() if value &gt;= 2])\n\n[\'james\', \'sarah\']\n\nfor (name, count) in zip(*np.unique(a, return_counts=True)):\n    if count &gt;1:\n        print(name)\n'
'import pandas as pd\n\ndata =  {\n    \'ClientID\': {0: 123, 1: 123, 2: 123, 3: 456, 4: 456, 5: 456},\n    \'Date\': {0: \'2020-03-01\', 1: \'2020-03-05\', 2: \'2020-03-10\',\n             3: \'2020-02-22\', 4: \'2020-02-25\', 5: \'2020-02-28\'},\n \'Orders\': {0: 23, 1: 10, 2: 7, 3: 3, 4: 15, 5: 5}\n}\n\ndf = pd.DataFrame(data)\n\n# Make sure the dates are datetimes\ndf[\'Date\'] = pd.to_datetime(df[\'Date\'])\n\n# Put into index so we can smuggle them through "rolling"\ndf = df.set_index([\'ClientID\', \'Date\'])\n\n\ndef date(a):\n    # get the "Date" index-column from the dataframe \n    return a.index.get_level_values(\'Date\')\n\ndef previous_week(a):\n    # get a column of 0s and 1s identifying the previous week, \n    # (compared to the date in the last row in a).\n    return (date(a) &gt;= date(a)[-1] - pd.DateOffset(days=7)) * (date(a) &lt; date(a)[-1]) \n\ndef previous_week_order_total(a):\n    #compute the order total for the previous week\n    return sum(previous_week(a) * a)\n\ndef total_last_week(group):\n    # for a "ClientID" compute all the "previous week order totals"\n    return group.rolling(8, min_periods=1).apply(previous_week_order_total, raw=False)\n\n# Ok, actually compute this\ndf[\'Orders_Last_Week\'] = df.groupby([\'ClientID\']).transform(total_last_week)\n\n# Reset the index back so you can have the ClientID and Date columns back\ndf = df.reset_index()\n'
'import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv("test.csv")\nf,ax1 = plt.subplots(figsize=(15, 5))\nax1.set_xlabel(\'Date\')\nax1.set_ylabel(\'Price\')\nax1.set_title(\'Original Plot\')\nax1.plot(\'Date\', \'Price\', data = df)\n\nax1.set_xticks(ax1.get_xticks()[::30])\nplt.xticks(rotation=45)\n'
'texts = \'\'\'yellow color       \nyellow color looks like \nyellow color bright\nred color okay\nred color blood\'\'\'.split(\'\\n\')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import Normalizer, FunctionTransformer\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.pipeline import make_pipeline\nmodel = make_pipeline(\n    CountVectorizer(), \n    Normalizer(), \n    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n    AgglomerativeClustering(distance_threshold=1.0, n_clusters=None),\n)\nclusters = model.fit_predict(texts)\nprint(clusters)  # [0 0 0 1 1]\n\nfrom collections import defaultdict\ncluster2words = defaultdict(list)\nfor text, cluster in zip(texts, clusters):\n    for word in text.split():\n        if word not in cluster2words[cluster]:\n            cluster2words[cluster].append(word)\nresult = [\' \'.join(wordlist) for wordlist in cluster2words.values()]\nprint(result)  # [\'yellow color looks like bright\', \'red color okay blood\']\n\n# !python -m spacy download en_core_web_lg\nimport spacy\nimport numpy as np\nnlp = spacy.load("en_core_web_lg")\n\nmodel = make_pipeline(\n    FunctionTransformer(lambda x: np.stack([nlp(t).vector for t in x])),\n    Normalizer(), \n    AgglomerativeClustering(distance_threshold=0.5, n_clusters=None),\n)\nclusters = model.fit_predict(texts)\nprint(clusters)  # [2 0 2 0 1]\n\n# !pip install laserembeddings\n# !python -m laserembeddings download-models\nfrom laserembeddings import Laser\nlaser = Laser()\n\nmodel = make_pipeline(\n    FunctionTransformer(lambda x: laser.embed_sentences(x, lang=\'en\')),\n    Normalizer(), \n    AgglomerativeClustering(distance_threshold=0.8, n_clusters=None),\n)\nclusters = model.fit_predict(texts)\nprint(clusters)  # [1 1 1 0 0]\n'
"(df['% Renewable'].gt(df['% Renewable'].median())).astype(int)\n\ncountry\nAustralia             0\nBrazil                1\nCanada                1\nChina                 0\nFrance                1\nGermany               1\nIndia                 0\nIran                  0\nItaly                 1\nJapan                 0\nRussian Federation    1\nSouth Korea           0\nSpain                 1\nUnited Kingdom        0\nUnited States         0\nName: % Renewable, dtype: int64\n"
"s=df.velocity.diff()\ndf['new']=np.where(s.abs()&gt;0.005,s/value,s)\n"
'import requests\nimport pandas as pd\n\nr = requests.get(\n    "https://www.vivino.com/api/explore/explore",\n    params = {\n        "country_code": "FR",\n        "country_codes[]":"pt",\n        "currency_code":"EUR",\n        "grape_filter":"varietal",\n        "min_rating":"1",\n        "order_by":"price",\n        "order":"asc",\n        "page": 1,\n        "price_range_max":"500",\n        "price_range_min":"0",\n        "wine_type_ids[]":"1"\n    },\n    headers= {\n        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0"\n    }\n)\nresults = [\n    (\n        t["vintage"]["wine"]["winery"]["name"], \n        f\'{t["vintage"]["wine"]["name"]} {t["vintage"]["year"]}\',\n        t["vintage"]["statistics"]["ratings_average"],\n        t["vintage"]["statistics"]["ratings_count"]\n    )\n    for t in r.json()["explore_vintage"]["matches"]\n]\ndataframe = pd.DataFrame(results,columns=[\'Winery\',\'Wine\',\'Rating\',\'num_review\'])\n\nprint(dataframe)\n'
'import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\ntips = sns.load_dataset("tips")\n\nsns.catplot(x = "day",\n            y = "total_bill",\n            hue = "sex",\n            col = "smoker",\n            data = tips,\n            kind = "violin",\n            split = True)\n\nplt.show()\n'
"name,platform,year_of_release,genre,na_sales,eu_sales,jp_sales,other_sales,critic_score,user_score,rating\nWii Sports,Wii,2006.0,Sports,41.36,28.96,3.77,8.45,76.0,8.0,E\nSuper Mario Bros.,NES,1985.0,Platform,29.08,3.58,6.81,0.77,,,\nMario Kart Wii,Wii,2008.0,Racing,15.68,12.76,3.79,3.29,82.0,8.3,E\nWii Sports Resort,Wii,2009.0,Sports,15.61,10.93,3.28,2.95,80.0,,E\nPokemon Red/Pokemon Blue,GB,1996.0,Role-Playing,11.27,8.89,10.22,1.0,,,\n\ndf.iloc[3]['user_score']\n\nnan\n\ndf['user_score'] = df.groupby('genre')['user_score'].transform(lambda x: x.fillna(x.mean()))\n\ndf.iloc[3]['user_score']\n\n8.0\n"
"def tweets_per_day(df):\n    df['Date'] = pd.to_datetime(df['Date'], format='%Y%m%d')\n    return df[['Tweets']].groupby(df['Date'].dt.date).count()\n  # return df['Tweets'].groupby(df['Date'].dt.date).count() \n  # if you want output to be `Series`\n\ntweets_per_day(twitter_df)\n\nd = pd.to_datetime(['2019-11-29', '2019-11-30']).repeat(10)\ndf = pd.DataFrame({'Tweets':'anything', 'Date':d})\ndf\n#      Tweets       Date\n#0   anything 2019-11-29\n#1   anything 2019-11-30\n#2   anything 2019-11-29\n#3   anything 2019-11-30\n#4   anything 2019-11-29\n#5   anything 2019-11-30\n#6   anything 2019-11-29\n#7   anything 2019-11-30\n#8   anything 2019-11-29\n#9   anything 2019-11-30\n#10  anything 2019-11-29\n#11  anything 2019-11-30\n#12  anything 2019-11-29\n#13  anything 2019-11-30\n#14  anything 2019-11-29\n#15  anything 2019-11-30\n#16  anything 2019-11-29\n#17  anything 2019-11-30\n#18  anything 2019-11-29\n#19  anything 2019-11-30\n\ntweets_per_day(df)\n\n            Tweets\nDate\n2019-11-29      10\n2019-11-30      10\n"
"g = df['person'].ne(df['person'].shift()).cumsum()\ns = g.map(g.value_counts())\n\nprint (s)\n0    3\n1    3\n2    3\n3    4\n4    4\n5    4\n6    4\n7    2\n8    2\nName: person, dtype: int64\n\nprint (s.groupby(df['person']).transform('max'))\n0    3\n1    3\n2    3\n3    4\n4    4\n5    4\n6    4\n7    3\n8    3\nName: person, dtype: int64\n\ndf = df[s.groupby(df['person']).transform('max').eq(s)]\nprint (df)\n  gene person allele allele2\n0   A1     p1      G       C\n1   A2     p1      A       C\n2   A3     p1      A       T\n3   A1     p2      G       C\n4   A2     p2      T       T\n5   A3     p2      G       C\n6   A4     p2      A       T\n\n#added last row for another data test\nprint (df)\n  gene person allele allele2\n0   A1     p1      G       C\n1   A2     p1      A       C\n2   A3     p1      A       T\n3   A1     p2      G       C\n4   A2     p2      T       T\n5   A3     p2      G       C\n6   A4     p2      A       T\n7   A2     p1      G       C\n8   A3     p1      C       C\n9   A4     p1      C       C\n\ng = df['person'].ne(df['person'].shift()).cumsum()\nprint (g)\n0    1\n1    1\n2    1\n3    2\n4    2\n5    2\n6    2\n7    3\n8    3\n9    3\nName: person, dtype: int32\n\n#same size 3\ns = g.map(g.value_counts())\nprint (s)\n0    3\n1    3\n2    3\n3    4\n4    4\n5    4\n6    4\n7    3\n8    3\n9    3\nName: person, dtype: int64\n\n#selected first max index in s\nidx = s.groupby(df['person']).idxmax()\nprint (idx)\nperson\np1    0\np2    3\nName: person, dtype: int64\n\n#seelcted groups g\nprint (g.loc[idx])\n0    1\n3    2\nName: person, dtype: int32\n\n#selected only matched groups\nprint (g.isin(g.loc[idx]))\n0     True\n1     True\n2     True\n3     True\n4     True\n5     True\n6     True\n7    False\n8    False\n9    False\nName: person, dtype: bool\n\ndf = df[g.isin(g.loc[idx])]\nprint (df)\n  gene person allele allele2\n0   A1     p1      G       C\n1   A2     p1      A       C\n2   A3     p1      A       T\n3   A1     p2      G       C\n4   A2     p2      T       T\n5   A3     p2      G       C\n6   A4     p2      A       T\n"
"x = np.ones(shape = (1,20,6))\narray([[[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]]])\n\n\ny = np.ones(shape = (1,6))\narray([[1., 1., 1., 1., 1., 1.]])\n\n\ny-x\narray([[[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]]])\n\nx = np.ones(shape = (10,20,6))\ny = np.ones(shape = (10,6))\ny-x\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-102-4a65323a80fa&gt; in &lt;module&gt;\n      1 x = np.ones(shape = (10,20,6))\n      2 y = np.ones(shape = (10,6))\n----&gt; 3 y-x\n\nValueError: operands could not be broadcast together with shapes (10,6) (10,20,6)\n\nmodel = Sequential()\nmodel.add(LSTM(242, input_shape=Input_shape, return_sequences=True))\nmodel.add(Dropout(0.3)); model.add(BatchNormalization())  \n\nmodel.add(LSTM(242, return_sequences=True))\nmodel.add(Dropout(0.3)); model.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(labels, activation='tanh')) \n\nopt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\nmodel.compile(loss='mean_absolute_error',optimizer=opt,metrics=['mse'])\ntf.keras.utils.plot_model(model, 'my_first_model.png', show_shapes=True)\n\nmodel.fit(train_batch_gen, epochs=EPOCHS, validation_data = validation_batch_gen) \n\nEpoch 1/3\n2/2 [==============================] - 1s 708ms/step - loss: 0.2891 - mse: 0.5739 - val_loss: 0.4078 - val_mse: 0.2461\nEpoch 2/3\n2/2 [==============================] - 0s 46ms/step - loss: 0.2229 - mse: 0.3151 - val_loss: 0.3867 - val_mse: 0.2225\nEpoch 3/3\n2/2 [==============================] - 0s 49ms/step - loss: 0.2315 - mse: 0.3341 - val_loss: 0.3813 - val_mse: 0.2161\n"
"df['Price'] = np.where(df.Price.between(24000,28000, inclusive=False), np.nan,df.Price)\nprint(df)\n            Brand    Price\n0     Honda Civic  22000.0\n1  Toyota Corolla      NaN\n2      Ford Focus      NaN\n3         Audi A4  35000.0\n\ndf['Price'] = np.where(( 24000 &lt; df.Price) &amp; (df.Price &lt; 28000), np.nan,df.Price)\nprint(df)\n            Brand    Price\n0     Honda Civic  22000.0\n1  Toyota Corolla      NaN\n2      Ford Focus      NaN\n3         Audi A4  35000.0\n"
"df22 = df2.melt('Name', value_name='Email').drop('variable', axis=1)\ndf = df1.merge(df22, on='Email', how='left')\nprint (df)\n                Email          Mobile      Name\n0     test1@test1.com        98989892  x_person\n1     test4@test4.com        98989895  y_person\n2     test5@test5.com        98989894  z_person\n3  Otheruser@mail.com  98438348342343       NaN\n"
'import pandas as pd\n\ndf = pd.DataFrame({&quot;1&quot;:list(range(6))})\ndf.insert(0, &quot;Age&quot;, [21, 23, 24, 21],True )\n# Length of values does not match length of index\n\ndf[&quot;new_col&quot;] = None\n\n# or\n\ndf.insert(0,&quot;Age&quot;, None, True)  # to get a new colum at position 0 all None\n\n# extend the data you want to give by a None-List and slice the whole down to size\ndf.insert(0,&quot;Age&quot;, ([21,23,24,21] + [None]*len(df))[:len(df)], True)\n\n    Age  1\n0  21.0  0\n1  23.0  1\n2  24.0  2\n3  21.0  3\n4   NaN  4   # only 2 None appends needed \n5   NaN  5\n'
"data = data[~ data['MAKE'].isin(['DODGE','VOLVO'])]\n"
'import time\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\n\noptions = Options()\noptions.headless = False\ndriver = webdriver.Chrome(options=options)\n\ndriver.get(&quot;https://fbref.com/en/comps/9/gca/Premier-League-Stats&quot;)\ntime.sleep(2)\n\nsoup = BeautifulSoup(driver.page_source, &quot;html.parser&quot;).find(&quot;div&quot;, {&quot;id&quot;: &quot;div_stats_gca&quot;})\ndriver.close()\n\ndf = pd.read_html(str(soup), skiprows=[0, 1])\ndf = pd.concat(df)\ndf.to_csv(&quot;data.csv&quot;, index=False)\n\n'
'from scipy.stats import norm\nprint(norm.ppf(0.95))\nprint(norm.cdf(1.6448536269514722))\n'
"df1['ColF'] = df1['ColC'] / df1['ColA'].map(df2.set_index(['ColD'])['ColE'])\n\nColA   ColB  ColC       ColF\n0     1   dog    439   4.346535\n1     1   cat    932   9.227723\n2     1  frog    932   9.227723\n3     2   dog   2122   6.757962\n4     2   cat    454   1.445860\n5     2  frog    773   2.461783\n6     3   dog   9223  74.379032\n7     3   cat   3012  24.290323\n8     3  frog    898   7.241935\n"
"In [83]: df\nOut[83]:\n  AverageTotalPayments\n0             $7064.38\n1             $7455.75\n2             $6921.90\n3                  aaa\n\nIn [84]: df.AverageTotalPayments.str.extract(r'.*?(\\d+\\.*\\d*)', expand=False).astype(float) &gt; 7000\nOut[84]:\n0     True\n1     True\n2    False\n3    False\nName: AverageTotalPayments, dtype: bool\n\nIn [85]: df[df.AverageTotalPayments.str.extract(r'.*?(\\d+\\.*\\d*)', expand=False).astype(float) &gt; 7000]\nOut[85]:\n  AverageTotalPayments\n0             $7064.38\n1             $7455.75\n"
"ourdates = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S')\n"
'df.drop("Unknown", axis=1).drop("Unknown", axis=0)\n\ndf = pd.DataFrame([[1,2],[3,4]], columns=[\'A\', \'B\'], index=[\'C\',\'D\'])\nprint(df)\n   A  B\nC  1  2\nD  3  4\n\ndf.drop(\'B\', axis=1).drop(\'C\', axis=0)\n\n   A\nD  3\n'
'df1 = pd.concat([df.ffill(), df.bfill()]).groupby(level=0).mean()\nprint (df1)\n   A  B    C    D\n0  1  1  1.0  2.0\n1  1  1  2.0  4.0\n2  1  1  3.0  4.0\n3  3  3  5.0  6.0\n\nprint (df.ffill())\n   A  B    C    D\n0  1  1  1.0  2.0\n1  1  1  1.0  2.0\n2  1  1  3.0  2.0\n3  3  3  5.0  6.0\n\nprint (df.bfill())\n   A  B    C    D\n0  1  1  1.0  2.0\n1  1  1  3.0  6.0\n2  1  1  3.0  6.0\n3  3  3  5.0  6.0\n'
'step = 1.0\nxs = range(min(data), max(data) + 1, step)\nys = [normal_cdf(i + step / 2, mu, sigma) - normal_cdf(i - step / 2, mu, sigma) for i in xs]\n'
"type,time,latitude,longitude,altitude (m),speed (km/h),name,desc,currentdistance,timeelapsed\nT,2017-10-07 10:44:48,28.750766667,77.088805000,783.5,0.0,2017-10-07_10-44-48,,0.0,00:00:00\nT,2017-10-07 10:44:58,28.752345000,77.087840000,853.5,7.8,,,198.70532,00:00:10\nT,2017-10-07 10:45:00,28.752501667,77.087705000,854.5,7.7,,,220.53915,00:00:12\n\nfrom datetime import datetime    \nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('input.csv', parse_dates=['time'], dtype={'name':str, 'desc':str})\ndf['timeelapsed'] = (pd.to_datetime(df['timeelapsed'], format='%H:%M:%S') - datetime(1900, 1, 1)).dt.total_seconds()\ndf['acceleration'] = (df['speed (km/h)'] - df['speed (km/h)'].shift(1)) / (df['timeelapsed'] - df['timeelapsed'].shift(1))\n\nprint df\n\n  type                time   latitude  longitude  altitude (m)  speed (km/h)                 name desc  currentdistance  timeelapsed  acceleration\n0    T 2017-10-07 10:44:48  28.750767  77.088805         783.5           0.0  2017-10-07_10-44-48  NaN          0.00000          0.0           NaN\n1    T 2017-10-07 10:44:58  28.752345  77.087840         853.5           7.8                  NaN  NaN        198.70532         10.0          0.78\n2    T 2017-10-07 10:45:00  28.752502  77.087705         854.5           7.7                  NaN  NaN        220.53915         12.0         -0.05\n"
"mapping_dict = {'A':['a', 'b', 'c', 'd'], 'B':['aa', 'bb', 'cc']}\n\ndf = pd.DataFrame(pd.Series(mapping_dict).reset_index()).set_axis(['Key','Value'],1,inplace=False)\n\n  Key         Value\n0   A  [a, b, c, d]\n1   B  [aa, bb, cc]\n\ndf = pd.DataFrame(list(mapping_dict.items()),columns=['Key','Value'])\n"
'Sub ParaStatsCount()\nDim Para As Paragraph\nFor Each Para In ActiveDocument.Paragraphs\n  With Para.Range\n    MsgBox .Text &amp; vbCr &amp; "Line Count = " &amp; .ComputeStatistics(wdStatisticLines) &amp; vbCr _\n      &amp; "Word Count = " &amp; .ComputeStatistics(wdStatisticWords)\n  End With\nNext\nEnd Sub\n'
"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n# Create a dataframe of random ints\ndf = pd.DataFrame(np.random.randint(0, 4, size=(100, 4)),\n                  columns=['pattern_id', 'B', 'C', 'D'])\nonehotencoder = OneHotEncoder(categorical_features=[df.columns.tolist().index('pattern_id')])\ndf = onehotencoder.fit_transform(df)\n"
"train_data['NumberOfSegment'] = train_data.groupby('CtpJobId')['SegmentId'].transform('count')\n\n        CtpJobId  SegmentId  NumberOfSegment\n0   qa1-9epx-dk1     347772                1\n1   qa1-9epx-dv1     347774                7\n2   qa1-9epx-dv1     347777                7\n3   qa1-9epx-dv1     347780                7\n4   qa1-9epx-dv1     347783                7\n5   qa1-9epx-dv1     347786                7\n6   qa1-9epx-dv1     347789                7\n7   qa1-9epx-dv1     347792                7\n8   qa1-9epx-e01     347794                1\n9   qa1-9epx-eb2     347795                1\n10  qa1-9epx-ez1     347796                1\n11  qa1-9epx-f32     347797                1\n12  qa1-9epx-fi1     347798                1\n"
'from lxml import etree\nfrom pykml import parser\n\nkml_file_path = \'./input.kml\'\n\n# parse the input file into an object tree\nwith open(kml_file_path) as f:\n  tree = parser.parse(f)\n\n# get a reference to the "Document.Folder" node\nfolder = tree.getroot().Document.Folder\n\n# iterate through all "Document.Folder.Placemark" nodes and find and remove all nodes \n# which contain child node "name" with content "ZONE"\nfor pm in folder.Placemark:\n    if pm.name == \'ZONE\':\n        parent = pm.getparent()\n        parent.remove(pm)\n\n# convert the object tree into a string and write it into an output file\nwith open(\'output.kml\', \'w\') as output:\n    output.write(etree.tostring(tree, pretty_print=True))\n'
"status = np.array(('area1', 'active'), dtype=status_type)\n\nmask = (products['area_states'] == status).any(axis=1)\nactive_products_in_area1 = products[mask]\n\narray([(253, '12:00', [('area1', 'active'), ('area2', 'inactive'), ('area3', 'inactive')])],\n      dtype=[('message_counter', '&lt;i8'), ('alteration_time', '&lt;U32'), ('area_states', [('area', '&lt;U32'), ('state', '&lt;U32')], (3,))])\n"
"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier \nimport matplotlib.pyplot as plt\n\n# create a training and testing set (use your X and y)    \nX_train,X_test, y_train, y_test= train_test_split(X,y,random_state=42, test_size=.3)\n# create a set of k values and an empty list for training and testing accuracy scores\nk_values=[1,2,3,4,5,6,7,8,9,10]\ntrain_scores=[]\ntest_scores=[]\n# instantiate the model \nk_nn=KNeighborsClassifier()\n# create a for loop of models with different k's \n\nfor k in k_values: \n  k_nn.n_neighbors=k \n  k_nn.fit(X_train,y_train)\n  train_score=k_nn.score(X_train,y_train)\n  test_score=k_nn.score(X_test,y_test)\n  train_scores.append(train_score)\n  test_scores.append(test_score)\n\nplt.plot(k_values,train_scores, color='red',label='Training Accuracy')\nplt.plot(k_values,test_scores, color='blue',label='Testing Accuracy')\nplt.xlabel('K values')\nplt.ylabel('Accuracy Score')\nplt.title('Performace Under Varying K Values')    \n"
"df1.groupby((df1['Name'] != df1['Name'].shift()).cumsum()).first()\n\n         Name      City\nName                   \n1       Alice   Seattle\n2         Bob   Seattle\n3     Mallory  Portland\n4         Bob  Portland\n5     Mallory   Seattle\n6       Alice   Seattle\n\ndf1.groupby((df1['Name'] != df1['Name'].shift()).cumsum())['Name'].first().values\n\n['Alice' 'Bob' 'Mallory' 'Bob' 'Mallory' 'Alice']\n"
"dic= df.groupby('PatientId').apply(lambda x:x[['x','y','width','height']].values.tolist()).to_dict()\n\n{'Aldkd2': [[92.0, 22.0, 12.0, 30.0], [29.0, 11.0, 98.0, 34.0]], 'Alll34': [[nan, 0.0, nan, nan]], 'A12kxk': [[23.0, 45.0, 10.0, 20.0]]}\n\nprint dic['Aldkd2']\n\n[[92.0, 22.0, 12.0, 30.0], [29.0, 11.0, 98.0, 34.0]]\n"
'y_pred_prob = np.array(gaussian.predict_proba(X_test))\nmetrics.roc_auc_score(y_test, y_pred_prob[:,1])\n'
"plt.subplot(2, 5, i + 1) # &lt;-- You have put this command after imshow \nplt.imshow(plottable_image, cmap='gray_r')\n\nfig = plt.figure()\n\nfor i in range(10):\n    im_idx = np.argwhere(y == i)[0]\n    plottable_image = np.reshape(X[im_idx], (28, 28))\n    ax = fig.add_subplot(2, 5, i+1)\n    ax.imshow(plottable_image, cmap='gray_r')\n\nfig, ax = plt.subplots(2,5)\nfor i, ax in enumerate(ax.flatten()):\n    im_idx = np.argwhere(y == i)[0]\n    plottable_image = np.reshape(X[im_idx], (28, 28))\n    ax.imshow(plottable_image, cmap='gray_r')\n"
"df['b'] = df['Data'].str.findall('(\\[.*?\\])').str.join(', ')\nprint (df)\n\n   Record                                               Data  \\\n0       1            Rohan is [age:10] with [height:130 cm].   \n1       2           Girish is [age:12] with [height:140 cm].   \n2       3   Both kids live in [location:Punjab] and [Delhi].   \n3       4  They love to play [Sport:Cricket] and [Sport:F...   \n\n                                   b  \n0          [age:10], [height:130 cm]  \n1          [age:12], [height:140 cm]  \n2         [location:Punjab], [Delhi]  \n3  [Sport:Cricket], [Sport:Football] \n\ndf['b'] = df['Data'].str.findall('\\[(.*?)\\]')\nprint (df)\n\n   Record                                               Data  \\\n0       1            Rohan is [age:10] with [height:130 cm].   \n1       2           Girish is [age:12] with [height:140 cm].   \n2       3   Both kids live in [location:Punjab] and [Delhi].   \n3       4  They love to play [Sport:Cricket] and [Sport:F...   \n\n                                 b  \n0          [age:10, height:130 cm]  \n1          [age:12, height:140 cm]  \n2         [location:Punjab, Delhi]  \n3  [Sport:Cricket, Sport:Football]  \n"
"df= pd.read_csv('C:\\\\Users\\\\desktop\\\\master.csv', parse_dates=[['Date', 'Time']])\n\ndf= pd.read_csv('C:\\\\Users\\\\desktop\\\\master.csv')\ndf['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n"
"np.concatenate(o, axis=1).flatten(order='f')\n\narray([ 1,  5,  2,  6,  3,  7,  4,  8,  9, 13, 10, 14, 11, 15, 12, 16])\n"
'b, m = gradient_descent(a, b, 5e-25, 100)\nprint(b, m)\nOut: -3.7387067636195266e-13 0.13854551291084335\n'
'from pyramid.arima.stationarity import ADFTest\nadf_test = ADFTest(alpha=0.05)\nadf_test.is_stationary(series)\ntrain, test = series[1:741], series[742:927]\ntrain.shape\ntest.shape\nplt.plot(train)\nplt.plot(test)\nplt.title("Training and Test Data")\nplt.show()\n\n&gt;&gt;&gt; Arima_model=auto_arima(train, start_p=1, start_q=1, max_p=8, max_q=8, start_P=0, start_Q=0, max_P=8, max_Q=8, m=12, seasonal=True, trace=True, d=1, D=1, error_action=\'warn\', suppress_warnings=True, random_state = 20, n_fits=30)\nFit ARIMA: order=(1, 1, 1) seasonal_order=(0, 1, 0, 12); AIC=-667.202, BIC=-648.847, Fit time=3.710 seconds\nFit ARIMA: order=(0, 1, 0) seasonal_order=(0, 1, 0, 12); AIC=-270.700, BIC=-261.522, Fit time=0.354 seconds\nFit ARIMA: order=(1, 1, 0) seasonal_order=(1, 1, 0, 12); AIC=-625.446, BIC=-607.090, Fit time=2.365 seconds\nFit ARIMA: order=(0, 1, 1) seasonal_order=(0, 1, 1, 12); AIC=-1090.370, BIC=-1072.014, Fit time=7.584 seconds\nFit ARIMA: order=(0, 1, 1) seasonal_order=(1, 1, 1, 12); AIC=-1088.657, BIC=-1065.712, Fit time=10.024 seconds\nFit ARIMA: order=(0, 1, 1) seasonal_order=(0, 1, 0, 12); AIC=-653.939, BIC=-640.172, Fit time=1.733 seconds\nFit ARIMA: order=(0, 1, 1) seasonal_order=(0, 1, 2, 12); AIC=-1087.889, BIC=-1064.944, Fit time=25.853 seconds\nFit ARIMA: order=(0, 1, 1) seasonal_order=(1, 1, 2, 12); AIC=-1087.188, BIC=-1059.655, Fit time=31.205 seconds\nFit ARIMA: order=(1, 1, 1) seasonal_order=(0, 1, 1, 12); AIC=-1105.233, BIC=-1082.288, Fit time=10.266 seconds\nFit ARIMA: order=(1, 1, 0) seasonal_order=(0, 1, 1, 12); AIC=-887.349, BIC=-868.994, Fit time=9.558 seconds\nFit ARIMA: order=(1, 1, 2) seasonal_order=(0, 1, 1, 12); AIC=-1086.931, BIC=-1059.397, Fit time=11.649 seconds\nFit ARIMA: order=(0, 1, 0) seasonal_order=(0, 1, 1, 12); AIC=-724.814, BIC=-711.047, Fit time=4.372 seconds\nFit ARIMA: order=(2, 1, 2) seasonal_order=(0, 1, 1, 12); AIC=-1085.480, BIC=-1053.358, Fit time=17.619 seconds\nFit ARIMA: order=(1, 1, 1) seasonal_order=(1, 1, 1, 12); AIC=-1072.933, BIC=-1045.400, Fit time=13.924 seconds\nFit ARIMA: order=(1, 1, 1) seasonal_order=(0, 1, 2, 12); AIC=-1102.926, BIC=-1075.392, Fit time=28.082 seconds\nFit ARIMA: order=(1, 1, 1) seasonal_order=(1, 1, 2, 12); AIC=-1102.342, BIC=-1070.219, Fit time=35.426 seconds\nFit ARIMA: order=(2, 1, 1) seasonal_order=(0, 1, 1, 12); AIC=-1010.837, BIC=-983.303, Fit time=8.926 seconds\nTotal fit time: 222.656 seconds\n&gt;&gt;&gt; \n&gt;&gt;&gt; Arima_model.summary()\n&lt;class \'statsmodels.iolib.summary.Summary\'&gt;\n"""\n                                 Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                                  y   No. Observations:                  740\nModel:             SARIMAX(1, 1, 1)x(0, 1, 1, 12)   Log Likelihood                 557.617\nDate:                            Thu, 14 Mar 2019   AIC                          -1105.233\nTime:                                    16:33:59   BIC                          -1082.288\nSample:                                         0   HQIC                         -1096.379\n                                            - 740                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nintercept   1.359e-06   6.75e-06      0.201      0.840   -1.19e-05    1.46e-05\nar.L1          0.1558      0.034      4.575      0.000       0.089       0.223\nma.L1         -0.9847      0.013    -75.250      0.000      -1.010      -0.959\nma.S.L12      -0.9933      0.092    -10.837      0.000      -1.173      -0.814\nsigma2         0.0118      0.001     11.259      0.000       0.010       0.014\n===================================================================================\nLjung-Box (Q):                       54.38   Jarque-Bera (JB):              3179.66\nProb(Q):                              0.06   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.77   Skew:                            -1.46\nProb(H) (two-sided):                  0.04   Kurtosis:                        12.82\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n'
'import pandas as pd\n\n\ndf[\'date_of_admission\'] = pd.to_datetime(df[\'date_of_admission\']).dt.date\n\ndf[\'DOB\'] = pd.to_datetime(df[\'DOB\']).dt.date\n\ndf[\'age\'] = ((df[\'date_of_admission\']-df[\'DOB\']).dt.days) //365\n\n\n#Now I have use DOB AND date_of_admission data from the question and it is working fine\n\ndf = pd.DataFrame(data={"DOB":[\'2000-05-07\',\'1965-01-30\',\'NaT\'],\n                   "date_of_admission":["2019-01-19 12:26:00","2019-03-21 02:23:12", "2018-11-02 18:30:10"]})\n\ndf[\'DOB\'] = pd.to_datetime(df[\'DOB\']).dt.date\ndf[\'date_of_admission\'] = pd.to_datetime(df[\'date_of_admission\']).dt.date\ndf[\'age\'] = ((df[\'date_of_admission\']-df[\'DOB\']).dt.days) //365\n\nDOB       date_of_admission   age\n2000-05-07  2019-01-19       18.0\n1965-01-30  2019-03-21       54.0\nNaT         2018-11-02       NaN\n'
"plt.bar(females.index, females, align='edge', width=-0.4, label='Female', color='red', alpha=0.5)\n"
'import pandas as pd\ndata=[\n{\'type\': \'Feature\',\n \'properties\': {\'PRI_NEIGH\': \'Printers Row\',\n  \'SEC_NEIGH\': \'PRINTERS ROW\',\n  \'SHAPE_AREA\': 2162137.97139,\n  \'SHAPE_LEN\': 6864.247156},\n \'geometry\': {\'type\': \'Polygon\',\n  \'coordinates\': [[-87.62760697485339, 41.87437097785366],\n    [-87.6275952566332, 41.873861712441126],\n    [-87.62756611032259, 41.873091933433905],\n    [-87.62755513014902, 41.872801941012725],\n    [-87.62754038267386, 41.87230261598636],\n    [-87.62752573582432, 41.8718067089444],\n    [-87.62751740010017, 41.87152447340544],\n    [-87.62749380061304, 41.87053328991345],\n    [-87.62748640976544, 41.87022285721281],\n    [-87.62747968351987, 41.86986997314866],\n    [-87.62746758964467, 41.86923545315858],\n    [-87.62746178584428, 41.868930955522266]]\n              }}\n      ]\n\ndf = {}\nfor item in data:\n    if  item["type"] ==\'Feature\':\n        if \'properties\' in item.keys():\n            nn = item.get("properties").get("PRI_NEIGH")\n        if \'geometry\' in item:\n            coords = item.get(\'geometry\').get(\'coordinates\')\n            df[nn] = coords\ndf_n=pd.DataFrame(df)\nprint(df_n)\n\n                               Printers Row\n0    [-87.62760697485339, 41.87437097785366]\n1    [-87.6275952566332, 41.873861712441126]\n2   [-87.62756611032259, 41.873091933433905]\n3   [-87.62755513014902, 41.872801941012725]\n4    [-87.62754038267386, 41.87230261598636]\n5     [-87.62752573582432, 41.8718067089444]\n6    [-87.62751740010017, 41.87152447340544]\n7    [-87.62749380061304, 41.87053328991345]\n8    [-87.62748640976544, 41.87022285721281]\n9    [-87.62747968351987, 41.86986997314866]\n10   [-87.62746758964467, 41.86923545315858]\n11  [-87.62746178584428, 41.868930955522266]\n'
'import scipy.stats\nscipy.stats.t.sf(abs(-0.278), df=97)*2\n\n0.78160405761659357\n'
"df['gr'] = df.A.diff().gt(pd.Timedelta(hours=2)).cumsum()\ndf_output = df.set_index('A').groupby('gr', as_index=False).resample('s').sum().reset_index()[['A']]\nprint (df_output)\n                     A\n0  2019-03-13 08:12:20\n1  2019-03-13 08:12:21\n2  2019-03-13 08:12:22\n3  2019-03-13 08:12:23\n4  2019-03-13 08:12:24\n5  2019-03-13 08:12:25\n6  2019-03-20 08:17:23\n7  2019-03-22 08:17:25\n8  2019-03-22 11:12:20\n9  2019-03-22 11:12:21\n10 2019-03-22 11:12:22\n11 2019-03-22 11:12:23\n12 2019-03-24 12:33:23\n"
'import numpy as np\nfrom datascience import *\n\nmodifier = 11\npossible_rolls = np.arange(20)\nroll_result = np.random.choice(possible_rolls)\nmodified_result = roll_result + modifier\nnum_observations = 7\n\ndef simulate_observations():\n    """Produces an array of 7 simulated modified die rolls"""\n    possible_rolls = np.arange(20)\n\n    array = make_array()\n    for i in np.arange(num_observations):\n       # moved roll_result inside the loop\n       roll_result = np.random.choice(possible_rolls)\n       modified_result = roll_result + modifier\n       array = np.append(array, modified_result)\n    return array\n\nobservation_array = simulate_observations()\nprint(observation_array)\n# [20. 11. 24. 11. 15. 16. 26.]\n\ndef simulate_observations():\n    """Produces an array of 7 simulated modified die rolls"""\n    possible_rolls = np.arange(modifier,20+modifier)\n    return np.random.choice(possible_rolls,size=num_observations).tolist()\n'
"df.groupby(list(df.columns)).size().reset_index().rename(columns={0:'count'})\n\ndf['counts'] = df.groupby('id')['year'].transform('count')\n"
" l = zip((1,2),(3,4)) \n    x,y = zip(*l)\n\nl=zip([1,2],[3,4])\ntemp = list(l)\nx,y = zip(*temp)\nprint(x)\nprint(y)\n\ndef func_temp(x,y,z):\n    print('call to function success')\nif __name__ == '__main__':\n    lis= [1,2,3]\n    func_temp(lis)\n\n    func_temp(lis)\nTypeError: func_temp() missing 2 required positional arguments: 'y' and 'z'\n\nProcess finished with exit code 1\n\ndef func_temp(x,y,z):\n    print('call to function success')\n\nif __name__ == '__main__':\n    lis= [1,2,3]\n    func_temp(*lis)\n"
'import math\n\ndata_set = [\n    (2,9,8,4, "Good"),\n    (3,7,7,9, "Bad"),\n    (10,3,10,3, "Good"),\n    (2,9,6,10, "Good"),\n    (3,3,2,5, "Bad"),\n    (2,8,5,6, "Bad"),\n    (7,2,3,10, "Good"),\n    (1,10,8,10, "Bad"),\n    (2,8,1,10, "Good"),\n]\n\nA = (3,2,1,5)\nB = (8,3,1,2)\nC = (6,10,8,3)\nD = (9,6,4,1)\n\ndef calc_distance(datas, test):\n    distances = []\n    for data in datas:\n        distances.append(\n            ( round(math.sqrt(((data[0] - test[0])**2 + (data[1] - test[1])**2 + (data[2] - test[2])**2 + (data[3] - test[3])**2)), 3), data[4] ))\n    return distances\n\ndef most_frequent(list1):\n    return max(set(list1), key = list1.count)\n\ndef get_neibours(distances, k):\n    labels = []\n    distances.sort()\n    print(distances[:k])\n    for distance in distances[:k]:\n        labels.append(distance[1])\n    print("It can be classified as: ", end="")\n    print(most_frequent(labels))\n\ndistances = calc_distance(data_set,D)\nget_neibours(distances, 7)\n\ndistances = calc_distance(data_set,D)\nget_neibours(distances, 7) \n'
"df['C']=np.select([df.A=='No animal', df.A.str.match('Zoo.*') &amp; df.B.str.match('\\w{2}[.].'), df.A.str.match('\\w{2}-.+-.+') &amp; df.B.str.match('GB.+')], [df.B, df.A,df.A]) \n\n            A             B     C\n0   No animal   EE.Elephant     EE.Elephant\n1   No animal   SS.Penguin      SS.Penguin\n2   Zoo One     EE.Elephant     Zoo One\n3   Zoo Two     SS.Penguin      Zoo Two\n4   Me-Lo-N     GB One          Me-Lo-N\n5   Ap-Pl-E     GB Two          Ap-Pl-E\n"
"import pandas as pd\nimport numpy as np\n\nfrom pandas.api.indexers import BaseIndexer\n\nclass ForwardTimewindowIndexer(BaseIndexer):\n    def get_window_bounds(\n        self,\n        num_values=0,\n        min_periods=None,\n        center=None,\n        closed=None,\n    ):\n        start = np.arange(0, num_values, dtype='int64')\n        if closed not in ['left', 'both']:\n            start += 1\n        search_side = 'left'\n        if closed in ['right', 'both']:\n            search_side='right'\n        end = self.index_array.searchsorted(\n            self.index_array + self.window_size,\n            side=search_side,\n        ).astype('int64')[:num_values]\n        return (start, end)\n\nindexer = ForwardTimewindowIndexer(\n    index_array=df.index,\n    window_size=pd.to_timedelta('1h'),\n)\ndf['required_col'] = df['target'].rolling(indexer).sum().astype(bool)\n"
"df['col1'] = df.apply(lambda x: x['col3'] if x['col1'] &lt; x['col2'] else x['col1'], axis=1)\n"
"(pd.concat((df1, df2))\n  .groupby('id').head(200)\n)\n"
'params = {\n    \'max_depth\': 6,\n    \'objective\': \'multi:softprob\',\n    \'num_class\': 3,\n    \'n_gpus\': 0\n}\npipe_xgb = Pipeline([\n    (\'clf\', xgb.XGBClassifier(**params))\n])\n\n#################################################################\n# Libraries\n#################################################################\nimport time\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\n#################################################################\n# Data loading and Symlinks\n#################################################################\ntrain = pd.read_csv("https://dl.dropbox.com/s/bnomyoidkcgyb2y/data_train.csv?dl=0")\ntest = pd.read_csv("https://dl.dropbox.com/s/kn1bgde3hsf6ngy/data_test.csv?dl=0")\n\n#################################################################\n# Train Test Split\n#################################################################\n# Selected features - Training data\nX = train.drop(columns=\'fault_severity\')\n\n# Training data\ny = train.fault_severity\n\n# Test data\nx = test\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n\n#################################################################\n# Pipeline\n#################################################################\nparams = {\n    \'max_depth\': 6,\n    \'objective\': \'multi:softprob\',  # error evaluation for multiclass training\n    \'num_class\': 3,\n    \'n_gpus\': 0\n}\npipe_xgb = Pipeline([\n    (\'clf\', xgb.XGBClassifier(**params))\n    ])\n\nparameters_xgb = {\n        \'clf__n_estimators\':[30,40], \n        \'clf__criterion\':[\'entropy\'], \n        \'clf__min_samples_split\':[15,20], \n        \'clf__min_samples_leaf\':[3,4]\n    }\n\ngrid_xgb = GridSearchCV(pipe_xgb,\n    param_grid=parameters_xgb,\n    scoring=\'accuracy\',\n    cv=5,\n    refit=True)\n\n#################################################################\n# Modeling\n#################################################################\nstart_time = time.time()\n\ngrid_xgb.fit(X_train, y_train)\n\n#Calculate the score once and use when needed\nacc = grid_xgb.score(X_valid,y_valid)\n\nprint("Best params                        : %s" % grid_xgb.best_params_)\nprint("Best training data accuracy        : %s" % grid_xgb.best_score_)    \nprint("Best validation data accuracy (*)  : %s" % acc)\nprint("Modeling time                      : %s" % time.strftime("%H:%M:%S", time.gmtime(time.time() - start_time)))\n\n#################################################################\n# Prediction\n#################################################################\n#Predict using the test data with selected features\ny_pred = grid_xgb.predict(X_valid)\n\n# Transform numpy array to dataframe\ny_pred = pd.DataFrame(y_pred)\n\n# Rearrange dataframe\ny_pred.columns = [\'prediction\']\ny_pred.insert(0, \'id\', x[\'id\'])\naccuracy_score(y_valid, y_pred.prediction)\n\ny_pred = pd.DataFrame(grid_xgb.predict_proba(X_valid),\n                      columns=[\'prediction_0\', \'prediction_1\', \'prediction_2\'])\ny_pred.insert(0, \'id\', x[\'id\'])\n\n      id  prediction_0  prediction_1  prediction_2\n0  11066      0.490955      0.436085      0.072961\n1  18000      0.718351      0.236274      0.045375\n2  16964      0.920252      0.052558      0.027190\n3   4795      0.958216      0.021558      0.020226\n4   3392      0.306204      0.155550      0.538246\n'
"results = lgb.cv({'num_threads':1}, lgb.Dataset(X_train, y_train), folds=folds,metrics=['rmse'])\n"
'from flask import Flask\nfrom flask_restful import Resource, Api, reqparse\nimport werkzeug, os\nimport json\nimport numpy as np\nimport base64\n\n\nclass NumpyEncoder(json.JSONEncoder): # useful for sending numpy arrays\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return json.JSONEncoder.default(self, obj)\n\n\napp = Flask(__name__)\napi = Api(app)\nparser = reqparse.RequestParser()\nparser.add_argument(\'file\', type=werkzeug.datastructures.FileStorage, location=\'files\')\nparser.add_argument(\'imgb64\')\n# add other arguments if needed\n\n# test response, check if live\nclass Test(Resource):\n    def get(self):\n        return {\'status\': \'ok\'}\n\n\nclass PredictB64(Resource): # for detecting from base64 images\n\n\n    def post(self):\n        data = parser.parse_args()\n        if data[\'imgb64\'] == "":\n            return {\n                    \'data\':\'\',\n                    \'message\':\'No file found\',\n                    \'status\':\'error\'\n                    }\n\n        img = data[\'imgb64\']\n        #print(img)\n\n\n        br = base64.b64decode(img)\n        im = np.frombuffer(br, dtype=np.uint8). reshape(-1, 416, 3) # width will be always 416, which is generally the bigger dimension\n        # reshape with the actual dimension of your image\n        #print(im.shape)\n\n\n        #print(type(im))\n        #print(im.shape)\n\n\n        if img:\n            r = # call your model here\n            #print(r)\n\n            return json.dumps({\n                    \'data\': json.dumps(list(r)), #(images), # may change based on your output, could be a string too\n                    \'message\':\'darknet processed\',\n                    \'status\':\'success\'\n                    }, cls=NumpyEncoder)\n        return {\n                \'data\':\'\',\n                \'message\':\'Something when wrong\',\n                \'status\':\'error\'\n                }\n\n\n\napi.add_resource(Test, \'/test\')\napi.add_resource(PredictB64,\'/predict_b64\')\n\nif __name__ == \'__main__\':\n    app.run(debug=True, host = \'0.0.0.0\', port = 5000, threaded=True)\n'
'print(df.apply(lambda x: " ".join([k for k, v in x.iteritems() if v]), axis=1))\n\n0                    activerecord air ajax\n1                    activerecord air ajax\n2                    activerecord air ajax\n3    .net 2007 actionscript-3 activerecord\n4    .net 2007 actionscript-3 activerecord\n5    .net 2007 actionscript-3 activerecord\n'
'filt_1 = (census_df[\'REGION\'] == 1)  | (census_df[\'REGION\'] == 2)\n\nimport pandas as pd\n\ncols = [\'REGION\', \'CTYNAME\', \'POPESTIMATE2014\', \'POPESTIMATE2015\']\ndata = [[1, "Washington", 4846411, 4858979],\n        [3, "Autauga County", 55290, 55347]]\n\ncensus_df = pd.DataFrame(data, columns=cols)\n\nfilt_1 = (census_df[\'REGION\'] == 1)  | (census_df[\'REGION\'] == 2)\nfilt_2 = census_df[\'CTYNAME\'].str.contains("^Washington[a-z]*")\nfilt_3 = census_df[\'POPESTIMATE2015\'] &gt; census_df[\'POPESTIMATE2014\']\n\nfilt = filt_1 &amp; filt_2 &amp; filt_3\n\nnew_df = census_df.loc[filt]\n\nprint(new_df)\n\n   REGION     CTYNAME  POPESTIMATE2014  POPESTIMATE2015\n0       1  Washington          4846411          4858979\n'
"all_selected_values2=df.loc[index_list,'selected_column'].tolist()\n\nnp.random.seed(1)\nindex_list = np.random.randint(0,11000,1000) #1000 values\ndf = pd.DataFrame(np.random.randint(0,100,(11000,1)),columns=['selected_column'])\n\nall_selected_values = []\nfor idx in index_list:\n    all_selected_values.append(df.loc[df.index == idx,'selected_column'].values[0])\n\nall_selected_values2=df.loc[index_list,'selected_column'].tolist()\n\nprint(all_selected_values == all_selected_values2)\nTrue\n\n%%timeit\nall_selected_values1 = []\nfor idx in index_list:\n    all_selected_values.append(df.loc[df.index == idx,'selected_column'].values[0])\n197 ms ± 9.04 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%%timeit\nall_selected_values2=df.loc[index_list,'selected_column'].tolist()\n415 µs ± 29.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
"import numpy as np\ndf=pd.DataFrame({'Province/State':[np.nan], 'Country/region':['Afghanistan'], 'Lat':[0],'Long':[0],'1/22/20':[0],'1/23/20':[0],'6/23/20':[29640]})\nprint(df)\n\n\n\n Province/State Country/region  Lat  Long  1/22/20  1/23/20  6/23/20\n0             NaN    Afghanistan    0     0        0        0    29640\n\npd.melt(df, id_vars=['Province/State', 'Country/region'], value_vars=['1/22/20', '1/23/20','6/23/20'],var_name='Date', value_name='Confirmed')\n\n\n\n Province/State Country/region     Date  Confirmed\n0             NaN    Afghanistan  1/22/20          0\n1             NaN    Afghanistan  1/23/20          0\n2             NaN    Afghanistan  6/23/20      29640\n"
"import numpy as np\nchart_data = pd.DataFrame([[1, 2, 1], [0, 2, 1], [1, 3, 5]],\n                          columns=['a', 'b', 'c'],\n                          index=[1, 2, 3])\nst.area_chart(chart_data)\n"
"In [37]: def my_func(df): \n    ...:     if df['mixid2'].nunique() == 1: \n    ...:         return None \n    ...:     else: \n    ...:         S.append(df['mixid2']) \n    ...:         return df \n\nS = []\nobs.groupby('id').apply(my_func)  \n\nOut[38]: \n                                   id  mixid2       magh\nid                                                      \n3447001203296326 0   3447001203296326     557  14.250000\n                 1   3447001203296326     573  14.250000\n...                               ...     ...        ...\n3447001203296409 98  3447001203296409     632  15.840000\n                 99  3447001203296409     616  15.840000\n\n[97 rows x 3 columns]\n"
'       ID               Name                    Occupation            Pay    OvertimePay        BasePay           Val4    Val5\n0  148646   Carolyn A Wilson     Human Services Technician              0              0              0              0    0.00\n1  148647       Not provided                  Not provided   Not Provided   Not Provided   Not Provided   Not Provided    0.00\n2  148648     Joann Anderson   Communications Dispatcher 2              0              0              0              0    0.00\n3  148649        Leon Walker                     Custodian              0              0              0              0    0.00\n4  148650      Roy 1 Tillery                     Custodian              0              0              0              0    0.00\n5  148651       Not provided                  Not provided   Not Provided   Not Provided   Not Provided   Not Provided    0.00\n6  148652       Not provided                  Not provided   Not Provided   Not Provided   Not Provided   Not Provided    0.00\n7  148653       Not provided                  Not provided   Not Provided   Not Provided   Not Provided   Not Provided    0.00\n8  148654          Joe Lopez    Counselor, Log Cabin Ranch              0              0        -618.13              0 -618.13\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(&quot;data.csv&quot;, sep=&quot;;&quot;)\nprint(df)\n\ndf = df.replace(&quot;Not [pP]rovided&quot;, np.NaN, regex=True)\n\nBasePay_mean = df[&quot;BasePay&quot;].fillna(0).astype(float).mean()\nprint(BasePay_mean)\n\n-68.68111111111111\n'
"import numpy as np \nfrom sklearn.preprocessing import StandardScaler\n\nX_train = np.array([[1, 2], [3, 4], [5, 6]])\nX_test = np.array([[7, 8], [9, 10]])\n\nX_train:\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\nX_test:\narray([[ 7,  8],\n       [ 9, 10]])\n\nsc = StandardScaler()\n\nprint(sc.mean_)\n\nAttributeError: 'StandardScaler' object has no attribute 'mean_'\n\nsc.fit(X_train)\n\nprint(sc.mean_)\n\n[3. 4.]\n\nsc.transform(X_test)\n\narray([[2.44949 , 2.44949 ],\n       [3.674235, 3.674235]])\n\nsc.transform(X_train)\n\narray([[-1.224745, -1.224745],\n       [ 0.      ,  0.      ],\n       [ 1.224745,  1.224745]])\n\nX_train = sc.fit_transform(X_train)\n\narray([[-1.224745, -1.224745],\n       [ 0.      ,  0.      ],\n       [ 1.224745,  1.224745]])\n"
'user_id  x                time\n      0  0 2014-01-01 04:00:00\n      1  1 2014-01-01 05:00:00\n      0  2 2014-01-01 06:00:00\n      1  3 2014-01-01 08:00:00\n      0  4 2014-01-01 10:00:00\n      1  5 2014-01-01 12:00:00\n\n       DIFF(x) by user_id\nindex\n0                     NaN\n1                     NaN\n2                     2.0\n3                     2.0\n4                     2.0\n5                     2.0\n'
'CV.best_estimator_.get_params()\n'
'&gt;&gt;&gt; x3 = np.random.rand(5, 5)\n&gt;&gt;&gt; x3\narray([[0.50866152, 0.56821455, 0.88531855, 0.36596337, 0.08705278],\n       [0.96215686, 0.19553668, 0.15948972, 0.20486815, 0.74759719],\n       [0.36269356, 0.54718917, 0.66196524, 0.82380099, 0.77739482],\n       [0.0431448 , 0.47664036, 0.80188153, 0.8099637 , 0.65258638],\n       [0.84862179, 0.22976325, 0.03508076, 0.72360136, 0.76835819]])\n&gt;&gt;&gt; x3[x3 &lt; .5] = 0\n&gt;&gt;&gt; x3\narray([[0.50866152, 0.56821455, 0.88531855, 0.        , 0.        ],\n       [0.96215686, 0.        , 0.        , 0.        , 0.74759719],\n       [0.        , 0.54718917, 0.66196524, 0.82380099, 0.77739482],\n       [0.        , 0.        , 0.80188153, 0.8099637 , 0.65258638],\n       [0.84862179, 0.        , 0.        , 0.72360136, 0.76835819]])\n'
'df.loc[df[&quot;Testschritt&quot;].str.strip() == &quot;F1&quot;]\n\ndf[&quot;Testschritt&quot;] = df[&quot;Testschritt&quot;].str.strip()\ndf.loc[df[&quot;Testschritt&quot;] == &quot;F1&quot;]\n'
"import numpy as np\n\nweights = np.array([0.5, 1, 0.5, 1]))\nvalues = df.drop('ent_id', axis=1)\n\ndf['AVG_WEIGHTAGE'] = np.dot(values.fillna(0).to_numpy(), weights)/np.dot(values.notna().to_numpy(), weights)\n\n\ndf['AVG_WEIGHTAGE']\n0    0.436647\n1    0.217019\n2    0.330312\n3    0.383860\n4    0.916891\n5    0.614998\n6    0.693245\n7    0.288001\n"
"def getdata(name):\n\n    url = u'https://www.website.com/search?q=' + quote(str(name))\n    .\n    .\n    .\n    return (data)\n\nfor i, row in df.iterrows():\n\n    leaddata = getdata(df.name[i])\n\n    list1.append(leaddata)\n"
"df['cost'] = df['cost'].replace(-1, np.nan).interpolate()\n\nprint (df)\n             cost\n0     8762.000000\n1       -1.000000\n2     7276.000000\n3   957400.000000\n4       -1.000000\n59    5508.000000\n60    7193.750000\n61      59.333333\n62      -1.000000\n63    4972.000000\n\ndf['cost'] = df['cost'].replace(-1, np.nan)\n\nq_low = df[&quot;cost&quot;].quantile(0.01)\nq_hi  = df[&quot;cost&quot;].quantile(0.99)\n\nm = df[&quot;cost&quot;].between(q_low, q_hi, inclusive=False)\n\ndf['cost'] = df['cost'].where(m).interpolate()\nprint (df)\n           cost\n0   8762.000000\n1   8019.000000\n2   7276.000000\n3   6686.666667\n4   6097.333333\n59  5508.000000\n60  7193.750000\n61  6453.166667\n62  5712.583333\n63  4972.000000\n"
'&gt; parameters &lt;- fitdist(data, &quot;weibull&quot;, method=&quot;mle&quot;, control=list(reltol=1e-14))$estimate\n&gt; parameters\n    shape     scale \n   2.3079 1463.7715\n'
"look_up = {1: 'jan', 2: 'feb', 3: 'mar', 4: 'apr', 5: 'may',\n           6: 'jun', 7: 'jul', 8: 'aug', 9: 'sep',\n           10: 'oct', 11: 'nov', 12: 'dec'}\n\n\ndf1 = (pd.get_dummies(df['join_month'])\n         .reindex(range(1,13), axis=1, fill_value=0)\n         .rename(columns=look_up)\n         .add_prefix('join_month_'))\n# print (df1)\n\n\ndf = df.join(df1)\nprint (df)\n\n EmployeeId       City  join_month  join_month_jan  join_month_feb  \\\n0        001     Mumbai           1               1               0   \n1        001  Bangalore           3               0               0   \n2        002       Pune           2               0               1   \n3        002     Mumbai           6               0               0   \n4        003      Delhi           9               0               0   \n5        003     Mumbai          12               0               0   \n6        004  Bangalore          11               0               0   \n7        004       Pune          10               0               0   \n8        005     Mumbai           5               0               0   \n\n   join_month_mar  join_month_apr  join_month_may  join_month_jun  \\\n0               0               0               0               0   \n1               1               0               0               0   \n2               0               0               0               0   \n3               0               0               0               1   \n4               0               0               0               0   \n5               0               0               0               0   \n6               0               0               0               0   \n7               0               0               0               0   \n8               0               0               1               0   \n\n   join_month_jul  join_month_aug  join_month_sep  join_month_oct  \\\n0               0               0               0               0   \n1               0               0               0               0   \n2               0               0               0               0   \n3               0               0               0               0   \n4               0               0               1               0   \n5               0               0               0               0   \n6               0               0               0               0   \n7               0               0               0               1   \n8               0               0               0               0   \n\n   join_month_nov  join_month_dec  \n0               0               0  \n1               0               0  \n2               0               0  \n3               0               0  \n4               0               0  \n5               0               1  \n6               1               0  \n7               0               0  \n8               0               0  \n"
"def custom_agg(group):\n    group = group.reset_index(drop=True)\n    max_ind = group.shape[0]-1\n    current_ind = -1\n    current_val = None\n    for ind, val in group.iterrows():\n        if pd.isna(val.topic) and ind != max_ind:\n            continue\n        if current_ind == -1:\n            current_ind = ind\n            current_val = val[&quot;start_time&quot;]\n        else:\n            group.loc[current_ind,&quot;topic_length&quot;] = val[&quot;start_time&quot;] - current_val\n            current_ind = ind\n            current_val = val[&quot;start_time&quot;]\n    return group\ndf = df.sort_values(by=['interaction_id', 'start_time']).groupby(&quot;interaction_id&quot;).apply(custom_agg).reset_index(drop=True)\n\n    interaction_id  start_time  topic   topic_length\n0   1   2.00    Cost    6.83\n1   1   5.72    NaN NaN\n2   1   8.83    Billing 4.03\n3   1   12.86   NaN NaN\n4   2   2.00    Cost    6.54\n5   2   6.75    NaN NaN\n6   2   8.54    NaN NaN\n7   3   1.50    Payments    2.15\n8   3   3.65    Products    NaN\n"
"In [134]: df\nOut[134]:\n   begin  end  userA  userB\n0    130  134      1      2\n1    201  203      5      1\n2    333  334      2      5\n\ntime_range = np.arange(0, 1001)\n\ndfs = []\nfor u in df[['userA','userB']].stack().unique():\n    dfs.append(pd.DataFrame({'time':time_range,\n                             'user':[u] * len(time_range),\n                             'interaction': ['no'] * len(time_range)}))\nrep = pd.concat(dfs, ignore_index=True)\n\nfor i,r in df.iterrows():\n    qry = 'user in {} and {} &lt;= time &lt;= {}'.format([r.userA, r.userB], r.begin, r.end)\n    print('Query: [{}]'.format(qry))\n    rep.ix[rep.eval(qry), 'interaction'] = 'yes'\n\nQuery: [user in [1, 2] and 130 &lt;= time &lt;= 134]\nQuery: [user in [5, 1] and 201 &lt;= time &lt;= 203]\nQuery: [user in [2, 5] and 333 &lt;= time &lt;= 334]\n\nIn [133]: rep[rep.interaction == 'yes']\nOut[133]:\n     interaction  time  user\n130          yes   130     1\n131          yes   131     1\n132          yes   132     1\n133          yes   133     1\n134          yes   134     1\n201          yes   201     1\n202          yes   202     1\n203          yes   203     1\n1131         yes   130     2\n1132         yes   131     2\n1133         yes   132     2\n1134         yes   133     2\n1135         yes   134     2\n1334         yes   333     2\n1335         yes   334     2\n2203         yes   201     5\n2204         yes   202     5\n2205         yes   203     5\n2335         yes   333     5\n2336         yes   334     5\n"
"df['Total Score'] = .3 **df.research + \n                    .3 **df.citations + \n                    .3 **df.teaching + \n                    .075 **df.international + \n                    .025 **df.income\n\ndef cal_score(x):\n     return .3 **x.research + \n            .3 **x.citations + \n            .3 **x.teaching +\n            .075 **x.international + \n            .025 **x.income\n\ndf['Total Score'] = df.apply(cal_score, axis=1)    \n\nimport pandas as pd\n\ndf = pd.DataFrame({'citations': {510: 38.799999999999997, 832: 39.0, 856: 45.600000000000001, 959: 45.799999999999997, 1232: 84.700000000000003, 1360: 38.5, 1361: 41.799999999999997, 1362: 35.299999999999997, 1363: 53.600000000000001, 1679: 51.600000000000001}, 'country': {510: 'India', 832: 'India', 856: 'India', 959: 'India', 1232: 'India', 1360: 'India', 1361: 'India', 1362: 'India', 1363: 'India', 1679: 'India'}, 'female_male_ratio': {510: '16 : 84', 832: '15 : 85', 856: '16 : 84', 959: '17 : 83', 1232: '46 : 54', 1360: '18 : 82', 1361: '13 : 87', 1362: '15 : 85', 1363: '17 : 83', 1679: '19 : 81'}, 'income': {510: '24.2', 832: '72.4', 856: '52.7', 959: '70.4', 1232: '28.4', 1360: '-', 1361: '42.4', 1362: '-', 1363: '64.8', 1679: '37.9'}, 'international': {510: '14.3', 832: '16.1', 856: '19.9', 959: '15.6', 1232: '29.3', 1360: '15.3', 1361: '17.3', 1362: '14.7', 1363: '15.6', 1679: '18.2'}, 'international_students': {510: '1%', 832: '0%', 856: '1%', 959: '1%', 1232: '1%', 1360: '1%', 1361: '0%', 1362: '0%', 1363: '1%', 1679: '1%'}, 'num_students': {510: '8,327', 832: '9,928', 856: '8,327', 959: '8,061', 1232: '16,691', 1360: '8,371', 1361: '6,167', 1362: '9,928', 1363: '8,061', 1679: '3,318'}, 'research': {510: 15.699999999999999, 832: 45.299999999999997, 856: 33.100000000000001, 959: 13.699999999999999, 1232: 14.0, 1360: 23.0, 1361: 25.199999999999999, 1362: 30.0, 1363: 12.300000000000001, 1679: 39.5}, 'student_staff_ratio': {510: 14.9, 832: 17.5, 856: 14.9, 959: 18.699999999999999, 1232: 23.899999999999999, 1360: 17.300000000000001, 1361: 12.199999999999999, 1362: 17.5, 1363: 18.699999999999999, 1679: 8.1999999999999993}, 'teaching': {510: 43.799999999999997, 832: 44.200000000000003, 856: 47.299999999999997, 959: 30.399999999999999, 1232: 25.800000000000001, 1360: 33.799999999999997, 1361: 31.300000000000001, 1362: 39.299999999999997, 1363: 25.100000000000001, 1679: 32.600000000000001}, 'total_score': {510: 29.489999999999995, 832: 38.549999999999997, 856: 37.799999999999997, 959: 26.969999999999999, 1232: 37.350000000000001, 1360: 28.589999999999996, 1361: 29.489999999999998, 1362: 31.379999999999995, 1363: 27.299999999999997, 1679: 37.109999999999999}, 'university_name': {510: 'Indian Institute of Technology Bombay', 832: 'Indian Institute of Technology Kharagpur', 856: 'Indian Institute of Technology Bombay', 959: 'Indian Institute of Technology Roorkee', 1232: 'Panjab University', 1360: 'Indian Institute of Technology Delhi', 1361: 'Indian Institute of Technology Kanpur', 1362: 'Indian Institute of Technology Kharagpur', 1363: 'Indian Institute of Technology Roorkee', 1679: 'Indian Institute of Science'}, 'world_rank': {510: '301-350', 832: '226-250', 856: '251-275', 959: '351-400', 1232: '226-250', 1360: '351-400', 1361: '351-400', 1362: '351-400', 1363: '351-400', 1679: '276-300'}, 'year': {510: 2012, 832: 2013, 856: 2013, 959: 2013, 1232: 2014, 1360: 2014, 1361: 2014, 1362: 2014, 1363: 2014, 1679: 2015}})\n\n#replace , to empty string\ndf['num_students'] = df.num_students.str.replace(',', '')\n#replace - to '0'\ndf['income'] = df['income'].str.replace('-', '0')\n\n#convert columns to float\ndf[['teaching', 'international', 'research', 'citations', 'income']] = \ndf[['teaching', 'international', 'research', 'citations', 'income']].astype(float)\n\ndf['Total Score'] = .3 **df.research + \n                    .3 **df.citations +  \n                    .3 **df.teaching +  \n                    .075 **df.international +  \n                    .025 **df.income\n\nprint (df)\n\n      citations country female_male_ratio  income  international  \\\n510        38.8   India           16 : 84    24.2           14.3   \n832        39.0   India           15 : 85    72.4           16.1   \n856        45.6   India           16 : 84    52.7           19.9   \n959        45.8   India           17 : 83    70.4           15.6   \n1232       84.7   India           46 : 54    28.4           29.3   \n1360       38.5   India           18 : 82     0.0           15.3   \n1361       41.8   India           13 : 87    42.4           17.3   \n1362       35.3   India           15 : 85     0.0           14.7   \n1363       53.6   India           17 : 83    64.8           15.6   \n1679       51.6   India           19 : 81    37.9           18.2   \n\n     international_students num_students  research  student_staff_ratio  \\\n510                      1%         8327      15.7                 14.9   \n832                      0%         9928      45.3                 17.5   \n856                      1%         8327      33.1                 14.9   \n959                      1%         8061      13.7                 18.7   \n1232                     1%        16691      14.0                 23.9   \n1360                     1%         8371      23.0                 17.3   \n1361                     0%         6167      25.2                 12.2   \n1362                     0%         9928      30.0                 17.5   \n1363                     1%         8061      12.3                 18.7   \n1679                     1%         3318      39.5                  8.2   \n\n      teaching  total_score                           university_name  \\\n510       43.8        29.49     Indian Institute of Technology Bombay   \n832       44.2        38.55  Indian Institute of Technology Kharagpur   \n856       47.3        37.80     Indian Institute of Technology Bombay   \n959       30.4        26.97    Indian Institute of Technology Roorkee   \n1232      25.8        37.35                         Panjab University   \n1360      33.8        28.59      Indian Institute of Technology Delhi   \n1361      31.3        29.49     Indian Institute of Technology Kanpur   \n1362      39.3        31.38  Indian Institute of Technology Kharagpur   \n1363      25.1        27.30    Indian Institute of Technology Roorkee   \n1679      32.6        37.11               Indian Institute of Science   \n\n     world_rank  year   Total Score  \n510     301-350  2012  6.177371e-09  \n832     226-250  2013  7.776087e-19  \n856     251-275  2013  4.928529e-18  \n959     351-400  2013  6.863746e-08  \n1232    226-250  2014  4.782972e-08  \n1360    351-400  2014  1.000000e+00  \n1361    351-400  2014  6.664022e-14  \n1362    351-400  2014  1.000000e+00  \n1363    351-400  2014  3.703322e-07  \n1679    276-300  2015  9.003721e-18  \n"
'import numpy as np\nclasses = np.argmax(model.predict(x), axis = 1)\n\nimport numpy as np\nfrom keras.utils.np_utils import to_categorical\nclasses_one_hot = to_categorical(np.argmax(model.predict(x), axis = 1))\n'
"df['ct'] = df.groupby([1,2,3])[1].transform('size')\n#alternatively\n#df['ct'] = df.groupby([1,2,3])[1].transform(len)\nprint (df)\n   0  1  2  3  ct\n0  1  2  3  4   2\n1  1  2  3  4   2\n2  1  3  5  6   1\n3  1  4  6  7   3\n4  1  4  6  7   3\n5  1  4  6  7   3\n"
"df = df.groupby(['Id','Date'], sort=False).agg({'BuyPrice':'max','SellPrice':'min'})\ndf['NBBO'] = np.where(df[['BuyPrice', 'SellPrice']].eq(0).any(1), \n                      np.nan, \n                      df['BuyPrice'] -  df['SellPrice'])\ndf['index'] =  df.groupby(level=0).cumcount() + 1\n\nd = {'BuyPrice':'highest buy price','SellPrice':'lowest sell price'}\ndf = df.reset_index().rename(columns=d)\nprint (df)\n\n   Id        Date  highest buy price  lowest sell price  NBBO  index\n0   1  2017-10-30              94617                  0   NaN      1\n1   1  2017-09-20              99100              99059  41.0      2\n2   2  2010-11-01              99933              99848  85.0      1\n\n#comapre with 0 eq is same as ==\nprint (df[['BuyPrice', 'SellPrice']].eq(0))\n               BuyPrice  SellPrice\nId Date                           \n1  2017-10-30     False       True\n   2017-09-20     False      False\n2  2010-11-01     False      False\n\n#get at least one True per row by any(1)\nprint (df[['BuyPrice', 'SellPrice']].eq(0).any(1))\nId  Date      \n1   2017-10-30     True\n    2017-09-20    False\n2   2010-11-01    False\ndtype: bool\n"
"df[df.status == 'a'].groupby('code').size()\n\ncode\nAZ    1\nMO    1\nNV    1\nNY    2\ndtype: int64\n"
"# Exclude Sundays\ndata.loc[data.index.dayofweek != 6]\n\n# Weekends only\ndata.loc[data.index.dayofweek.isin([5, 6])]\n\n# Weekdays only\ndata.loc[~data.index.dayofweek.isin([5, 6])]\n\ndf.resample('H').mean()\n"
"# you don't need `reset_index()` in your code\nsales_std=sales_std[['WeekDay','price']].groupby(['WeekDay']).std()\nsales_std['price'].plot(kind='bar')\n"
' foldx.append(X[j:j+143,:])\n\nfoldy.append(y[foldx[j]])\n'
"df['IS_ALIVE'] = (df.DOB.notna() &amp; df.DOD.isna()).astype(int)\n#for old versions of pandas\n#df['IS_ALIVE'] = (df.DOB.notnull() &amp; df.DOD.isnull()).astype(int)\n"
'import pandas as pd\nimport dask.dataframe as dd\nimport numpy as np\nimport string\n\nN = 5\nrndm2 = lambda :"".join(np.random.choice(list(string.ascii_lowercase), 2))\ndf_sample = pd.DataFrame({"C1":np.arange(N),\n                          "C2":[rndm2() for i in range(N)],\n                          "C3":np.random.randn(N)})\n\nM = 2\ndf = pd.concat([df_sample for i in range(M)], ignore_index=True)\ndf["C4"] =  np.random.randn(N*M)\n\n   C1  C2        C3        C4\n3   3  bx  0.668654 -0.237081\n8   3  bx  0.668654  0.619883\n\nddf = dd.from_pandas(df, npartitions=4)\nddf.to_parquet("saved/", partition_on=["C2"])\n\n# You can check that the parquet files\n# are in separated folder as\n! ls saved/ # If you are on Linux\n\n\'C2=iw\'  \'C2=jl\'  \'C2=qf\'  \'C2=wy\'  \'C2=yr\'   _common_metadata\n\ndf = dd.read_parquet("saved/")\nout = df.map_partitions(lambda x: fun(x)).compute() # you should add your output meta\n'
"xTrain = pd.DataFrame({'address':['a', 'b', 'c'],'b':[1,2, np.nan]})\nprint (xTrain)\n  address    b\n0       a  1.0\n1       b  2.0\n2       c  NaN\n\ncols = xTrain.select_dtypes(include=np.number).columns\n\nxTrain[cols] = xTrain[cols].fillna(xTrain.mean())\nprint (xTrain)\n  address    b\n0       a  1.0\n1       b  2.0\n2       c  1.5\n"
'df = df.dropna(subset=["Deviation from Partisanship"])\n\ndf.dropna(subset=["Deviation from Partisanship"], inplace=True)\n'
'np.random.seed(2019)\n\nrng = (pd.date_range(\'2018-01-01\', periods=10, freq=\'H\').tolist() +\n      pd.date_range(\'2018-01-02\', periods=10, freq=\'H\').tolist())\ndf = pd.DataFrame({\'Temperature\': np.random.randint(100, size=20)}, index=rng)  \nprint (df)\n                     Temperature\n2018-01-01 00:00:00           72\n2018-01-01 01:00:00           31\n2018-01-01 02:00:00           37\n2018-01-01 03:00:00           88\n2018-01-01 04:00:00           62\n2018-01-01 05:00:00           24\n2018-01-01 06:00:00           29\n2018-01-01 07:00:00           15\n2018-01-01 08:00:00           12\n2018-01-01 09:00:00           16\n2018-01-02 00:00:00           48\n2018-01-02 01:00:00           71\n2018-01-02 02:00:00           83\n2018-01-02 03:00:00           12\n2018-01-02 04:00:00           80\n2018-01-02 05:00:00           50\n2018-01-02 06:00:00           95\n2018-01-02 07:00:00            5\n2018-01-02 08:00:00           24\n2018-01-02 09:00:00           28\n\n#if necessary create DatetimeIndex if DT is column\ndf = df.set_index("DT")\n\ndef countChange(day):\n    return (day[\'Temperature\'].diff() &gt; 3).sum()\n\nprint (countChange(df.loc[\'2018-01-01\']))\n4\n\nprint (countChange(df.loc[\'2018-01-02\']))\n9\n'
"df1 = df.resample('D').mean()\n"
'samples = [f for f in os.listdir(datasetPath) if isfile(join(datasetPath, f))]\n'
'credit_test["Class"].value_counts()\n\n1    138\n0     54\n\n[clf.intercept_,clf.coef_]\n[array([0.59140229]), array([[0.9820343]])]\n\nclf = LogisticRegression(random_state=0,fit_intercept=False).fit(X, y)\ny_pred=clf.predict(credit_test[[\'CreditHistory.Critical\']])\nconfusion_matrix(y_pred=y_pred, y_true=y_test)\n\narray([[42, 12],\n       [84, 54]])\n'
'import re\n\ndf["Match"] = [1 if re.search(fr"{pat}", data) else 0 \n               for data, pat in zip(df["Data"],df["Regex"])]\n\nprint (df)\n\n         Data        Regex  Match\n0  HU13568812  ^HU[0-9]{8}      1\n1    78567899  ^NO[0-9]{5}      0\n2   AT1234567  ^AT[0-9]{7}      1\n'
"import networkx as nx\nfrom matplotlib import pyplot as plt\n\npath_car1 = ['cityA','cityB','cityD']\npath_car2 = ['cityZ','cityA','cityK']\n\npaths = [path_car1, path_car2]\ncolors = ['Red','Blue']\n\nG = nx.DiGraph()\nfor path, color in zip(paths, colors):\n    for edge in zip(path[:-1], path[1:]):\n        G.add_edge(*edge, color=color)\n\nedge_colors = nx.get_edge_attributes(G, 'color')\n\nplt.figure(figsize=(10,7))\npos = nx.spring_layout(G, scale=20)\nnx.draw(G, pos, \n        node_color='black',\n        with_labels=True, \n        node_size=1200,\n        edgelist=G.edges(),\n        edge_color=edge_colors.values(),\n        arrowsize=15,\n        font_color='white',\n        width=3,\n        alpha=0.9)\n"
"import pandas as pd\nfrom plotly.offline import iplot\n\ndf = pd.DataFrame({'x': ['Jan', 'Feb'],\n                   'y': [28446000, 43267700],\n                   'z': [110489.0, 227900.0]})\n\nlayout = {'title': 'Secondary Y-Axis Demonstration',\n          'legend': {'orientation': 'h'}}\n\ntraces = []\ntraces.append({'x': df['x'], 'y': df['y'], 'name': 'Y Values'})\ntraces.append({'x': df['x'], 'y': df['z'], 'name': 'Z Values', 'yaxis': 'y2'})\n\n\n# Add config common to all traces.\nfor t in traces:\n    t.update({'line': {'shape': 'spline'}})\n\nlayout['yaxis1'] = {'title': 'Y Values', 'range': [0, 50000000]}\nlayout['yaxis2'] = {'title': 'Z Values', 'side': 'right', 'overlaying': 'y1', 'range': [0, 400000]}\n\n\niplot({'data': traces, 'layout': layout})\n"
'In [3831]: x = (\n      ...:     df.groupby(&quot;day&quot;)\n      ...:     .agg({&quot;hours&quot;: [&quot;first&quot;, &quot;last&quot;]})\n      ...:     .diff(axis=1)[(&quot;hours&quot;, &quot;last&quot;)]\n      ...:     .reset_index()\n      ...:     .droplevel(level=1, axis=1)\n      ...: )\n\nIn [3829]: x\nOut[3829]: \n         day  hours\n0     monday    1.0\n1    tuesday    2.0\n2  wednesday    2.0\n'
"df['col_name'] = df['col_name'].str.replace(r&quot;\\[.*\\]&quot;,&quot;&quot;)\n"
'class_accuracies = []\nfor class_ in np.unique(y_true):\n    class_acc = np.mean(y_pred[y_true == class_] == class_)\n    class_acuracies.append(class_acc)\n'
'In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({"Product Category": [[\'Category1\', \'Category2\'],\n   ...:                                         [\'Category3\'],\n   ...:                                         [\'Category1\', \'Category4\'],\n   ...:                                         [\'Category1\', \'Category3\', \'Category5\']]})\n\nIn [3]: df\nOut[3]:\n                    Product Category\n0             [Category1, Category2]\n1                        [Category3]\n2             [Category1, Category4]\n3  [Category1, Category3, Category5]\n\nIn [4]: def list_to_dict(category_list):\n   ...:         n_categories = len(category_list)\n   ...:         return dict(zip(category_list, [1]*n_categories))\n   ...:\n\nIn [5]: df_dummies = pd.DataFrame(list(df[\'Product Category\'].apply(list_to_dict).values)).fillna(0)\n\nIn [6]: df_new = df.join(df_dummies)\n\nIn [7]: df_new\nOut[7]:\n                    Product Category  Category1  Category2  Category3 Category4  Category5\n0             [Category1, Category2]          1          1          0         0          0\n1                        [Category3]          0          0          1         0          0\n2             [Category1, Category4]          1          0          0         1          0\n3  [Category1, Category3, Category5]          1          0          1         0          1\n'
"a['col3'] + a['col2'].map(b.set_index('col2')['col3'])\nOut[94]: \n0    11\n1    22\n2    13\n3    24\ndtype: int64\n\na.merge(b, on='col2', how='left')[['col3_x', 'col3_y']].sum(axis=1)\nOut[110]: \n0    11\n1    22\n2    13\n3    24\ndtype: int64\n\na['col3'] = a.merge(b, on='col2', how='left')[['col3_x', 'col3_y']].sum(axis=1)\n"
"saved_groupby = df.groupby(['Week', 'ID_1', 'ID_2'])['Target'].median()\nsaved_groupby = saved_groupby.reset_index()\nsaved_groupby['Week'] = saved_groupby['Week'] + 2\n\nresult = pd.merge(df, saved_groupby, on=['Week', 'ID_1', 'ID_2'], how='left')\n\nresult['Median'] = result['Median'].fillna(0)\n\nimport numpy as np\nimport pandas as pd\nnp.random.seed(2016)\n\ndf = pd.DataFrame(np.random.randint(5, size=(20,5)), \n                  columns=['Week', 'ID_1', 'ID_2', 'Target', 'Foo'])\n\nsaved_groupby = df.groupby(['Week', 'ID_1', 'ID_2'])['Target'].median()\nsaved_groupby = saved_groupby.reset_index()\nsaved_groupby['Week'] = saved_groupby['Week'] + 2\nsaved_groupby = saved_groupby.rename(columns={'Target':'Median'})\n\nresult = pd.merge(df, saved_groupby, on=['Week', 'ID_1', 'ID_2'], how='left')\nresult['Median'] = result['Median'].fillna(0)\nprint(result)\n\n    Week  ID_1  ID_2  Target  Foo  Median\n0      3     2     3       4    2     0.0\n1      3     3     0       3    4     0.0\n2      4     3     0       1    2     0.0\n3      3     4     1       1    1     0.0\n4      2     4     2       0    3     2.0\n5      1     0     1       4    4     0.0\n6      2     3     4       0    0     0.0\n7      4     0     0       2    3     0.0\n8      3     4     3       2    2     0.0\n9      2     2     4       0    1     0.0\n10     2     0     4       4    2     0.0\n11     1     1     3       0    0     0.0\n12     0     1     0       2    0     0.0\n13     4     0     4       0    3     4.0\n14     1     2     1       3    1     0.0\n15     3     0     1       3    4     2.0\n16     0     4     2       2    4     0.0\n17     1     1     4       4    2     0.0\n18     4     1     0       3    0     0.0\n19     1     0     1       0    0     0.0\n"
'import random\nimport numpy as np\nimport pandas as pd\nimport itertools as it\n\nnp.random.seed(0)\nnumGroups=4\nnumMembers=4\n\ndata = list(it.product(range(numGroups),range(numMembers)))\ndf = pd.DataFrame(data=data,columns=[\'group\',\'partid\'])\n\ng = np.repeat(range(numGroups),numMembers).reshape((numGroups,numMembers))\nIn [95]: g\nOut[95]: \narray([[0, 0, 0, 0],\n       [1, 1, 1, 1],\n       [2, 2, 2, 2],\n       [3, 3, 3, 3]])\n\ng = np.random.permutation(g)\nIn [102]: g\nOut[102]: \narray([[2, 2, 2, 2],\n       [3, 3, 3, 3],\n       [1, 1, 1, 1],\n       [0, 0, 0, 0]])\n\ng = np.tile(g,(2,1))\nIn [104]: g\nOut[104]: \narray([[2, 2, 2, 2],\n       [3, 3, 3, 3],\n       [1, 1, 1, 1],\n       [0, 0, 0, 0],\n       [2, 2, 2, 2],\n       [3, 3, 3, 3],\n       [1, 1, 1, 1],\n       [0, 0, 0, 0]])\n\narray([[2, -, -, -],\n       [3, 3, -, -],\n       [1, 1, 1, -],\n       [0, 0, 0, 0],\n       [-, 2, 2, 2],\n       [-, -, 3, 3],\n       [-, -, -, 1],\n       [-, -, -, -]])\n\nnewGroups = []\nfor i in range(numGroups):\n    newGroups.append(np.diagonal(g[i:i+numMembers]))\n\nIn [106]: newGroups\nOut[106]: \n[array([2, 3, 1, 0]),\n array([3, 1, 0, 2]),\n array([1, 0, 2, 3]),\n array([0, 2, 3, 1])]\n\nnewGroups = np.ravel(newGroups)\ndf["newGroups"] = newGroups\n\nIn [110]: df\nOut[110]: \n    group  partid  newGroups\n0       0       0          2\n1       0       1          3\n2       0       2          1\n3       0       3          0\n4       1       0          3\n5       1       1          1\n6       1       2          0\n7       1       3          2\n8       2       0          1\n9       2       1          0\n10      2       2          2\n11      2       3          3\n12      3       0          0\n13      3       1          2\n14      3       2          3\n15      3       3          1\n\nimport random\nimport numpy as np\nimport pandas as pd\nimport itertools as it\n\nrandom.seed(0)\nnumGroups=4\nnumMembers=4\n\ndata = list(it.product(range(numGroups),range(numMembers)))\ndf = pd.DataFrame(data=data,columns=[\'group\',\'partid\'])\n\ng = np.repeat(range(numGroups),numMembers).reshape((numGroups,numMembers))\n\nIn [4]: g\nOut[4]: \narray([[0, 0, 0, 0],\n       [1, 1, 1, 1],\n       [2, 2, 2, 2],\n       [3, 3, 3, 3]])\n\ndef reArrange(g):\n    g = np.transpose(g)\n    g = [np.random.permutation(x) for x in g]\n    return np.transpose(g)\n\n# check to see if any members in each old group have duplicate new groups\n# if so repeat\nwhile np.any(np.apply_along_axis(lambda x: len(np.unique(x))&lt;numMembers,1,g)):\n    g = reArrange(g)\n\ndf["newGroup"] = g.ravel()\n\nIn [7]: df\nOut[7]: \n    group  partid  newGroup\n0       0       0         2\n1       0       1         3\n2       0       2         1\n3       0       3         0\n4       1       0         0\n5       1       1         1\n6       1       2         2\n7       1       3         3\n8       2       0         1\n9       2       1         0\n10      2       2         3\n11      2       3         2\n12      3       0         3\n13      3       1         2\n14      3       2         0\n15      3       3         1\n'
"busiest_hosts[busiest_hosts['host'].str.contains('\\*{3}\\.novo\\.dk')==True]\n\nbusiest_hosts = pd.DataFrame(dict(host=['***.novo.dk', '007.thegap.com'], timestamp=[16, 45]))\n\nprint(busiest_hosts)\n\n             host  timestamp\n0     ***.novo.dk         16\n1  007.thegap.com         45\n\nbusiest_hosts[busiest_hosts['host'].str.contains('\\*{3}\\.novo\\.dk')==True]\n\n          host  timestamp\n0  ***.novo.dk         16\n\nbusiest_hosts[busiest_hosts['host'].str.contains('***.novo.dk', regex=False)==True]\n"
'import selenium\nfrom selenium import webdriver\ndriver = webdriver.PhantomJS()\nurl = "https://www.tripadvisor.com/Restaurant_Review-g56010-d470148-Reviews-Chez_Nous-Humble_Texas.html"\ndriver.get(url)\n\nelem = driver.get_element_by_class_name("taLnk")\n...\n'
"data['DayIndex'] = pandas.to_datetime(data['Day'])\n\ndata['DayIndex'] = pandas.to_datetime(data['Day'], format='%m/%d/%Y')\n\ndata = read_csv(..., parse_dates=['Day'],\n     date_parser=lambda s: pandas.datetime.strptime(s, '%m/%d/%y'))\n\n# let u be unique date stings.  We'll do this so that we only parse them once.\nu = pd.unique(data['Day'])\n\n# then build a dictionary of these\nm = dict(zip(u, pd.to_datetime(u, format='%m/%d/%Y')))\n\n# then use `map` to build the new column\ndata['DayIndex'] = data['Day'].map(m)\n\na = np.random.choice(\n    ['04/23/17', '04/20/17', '04/21/17', '04/24/17', '04/22/17',\n     '05/02/17', '04/27/17', '05/06/17', '04/30/17', '04/25/17',\n     '04/26/17', '05/04/17'],\n    100000)\n\ndata = pd.DataFrame(dict(Day=a))\n\n\n%%timeit\nu = pd.unique(a)\nm = dict(zip(u, pd.to_datetime(u, format='%m/%d/%y'))) \ndata['Day'].map(m)\n100 loops, best of 3: 15.4 ms per loop\n\n%timeit pd.to_datetime(data['Day'], format='%m/%d/%y')\n1 loop, best of 3: 206 ms per loop\n"
'pip install jupyter_dashboards_bundlers\njupyter bundlerextension enable --sys-prefix --py dashboards_bundlers\n'
"#! /usr/bin/python3\n\nimport pandas as pd\nimport numpy as np\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\n\n# Simulate data\ndf_subject = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD')) # This is the one we're iterating to check similarity to target.\ndf_target = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD')) # This is the one we're checking distance to\n\n# This will hold the min dstances.\ndistances=[]\n# Loop to iterate over subject DF\nfor ix1,subject in df_subject.iterrows():\n    distances_cur=[]\n    # Loop to iterate over target DF\n    for ix2,target in df_target.iterrows():\n        distances_cur.append(np.linalg.norm(target-subject))\n    # Get the minimum distance for the subject set member.\n    distances.append(min(distances_cur))\n\n# Distances to df\ndistances=pd.DataFrame(distances)\n# Normalize.\ndistances=0.5-(distances-distances.mean(axis=0))/distances.max(axis=0)\n\n# Column index joining, ordering and beautification.\nProximity_Ratings_name='Proximity Ratings'\ndistances=distances.rename(columns={0: Proximity_Ratings_name})\ndf_subject=df_subject.join(distances)\npp.pprint(df_subject.sort_values(Proximity_Ratings_name,ascending=False))\n\n     A   B   C   D  Proximity Ratings\n55  86  21  91  78           0.941537\n38  91  31  35  95           0.901638\n43  49  89  49   6           0.878030\n98  28  98  98  36           0.813685\n77  67  23  78  84           0.809324\n35  52  16  36  58           0.802223\n54   2  25  61  44           0.788591\n95  76   3  60  46           0.766896\n5   55  39  88  37           0.756049\n52  79  71  90  70           0.752520\n66  52  27  82  82           0.751353\n41  45  67  55  33           0.739919\n76  12  93  50  62           0.720323\n94  99  84  39  63           0.716123\n26  62   6  97  60           0.715081\n40  64  50  37  27           0.714042\n68  70  21   8  82           0.698824\n47  90  54  60  65           0.676680\n7   85  95  45  71           0.672036\n2   14  68  50   6           0.661113\n34  62  63  83  29           0.659322\n8   87  90  28  74           0.647873\n75  14  61  27  68           0.633370\n60   9  91  42  40           0.630030\n4   46  46  52  35           0.621792\n81  94  19  82  44           0.614510\n73  67  27  34  92           0.608137\n30  92  64  93  51           0.608137\n11  52  25  93  50           0.605770\n51  17  48  57  52           0.604984\n..  ..  ..  ..  ..                ...\n64  28  56   0   9           0.397054\n18  52  84  36  79           0.396518\n99  41   5  32  34           0.388519\n27  19  54  43  94           0.382714\n92  69  56  73  93           0.382714\n59   1  29  46  16           0.374878\n58   2  36   8  96           0.362525\n69  58  92  16  48           0.361505\n31  27  57  80  35           0.349887\n10  59  23  47  24           0.345891\n96  41  77  76  33           0.345891\n78  42  71  87  65           0.344398\n93  12  31   6  27           0.329152\n23   6   5  10  42           0.320445\n14  44   6  43  29           0.319964\n6   81  51  44  15           0.311840\n3   17  60  13  22           0.293066\n70  28  40  22  82           0.251549\n36  95  72  35   5           0.249354\n49  78  10  30  18           0.242370\n17  79  69  57  96           0.225168\n46  42  95  86  81           0.224742\n84  58  81  59  86           0.221346\n9    9  62   8  30           0.211659\n72  11  51  74   8           0.159265\n90  74  26  80   1           0.138993\n20  90   4   6   5           0.117652\n50   3  12   5  53           0.077088\n42  90  76  42   1           0.075284\n45  94  46  88  14           0.054244\n"
"def combine(row):\n    result = row['one'] + row['two']\n    return(result)\n\ndef combine(row):\n    result = row['one'] + row['two']\n    return pd.Series({'result': result})\n\ntest.apply(lambda x: combine(x), axis=1)\n       result\n0      [2, 5]\n1  [test, 10]\n"
'df = pd.DataFrame()\n\nfor url in url_baseball_players:\n    df = pd.concat([df, pd.DataFrame(scrape_baseball_data(url))])\n'
'plt.rcParams[\'patch.linewidth\'] = 0\nplt.rcParams[\'patch.edgecolor\'] = \'none\'\n\nplt.rcParams["patch.force_edgecolor"] = False\n'
'extract_patches(image,\n                patch_shape=(p_h, p_w, n_colors),\n                extraction_step=1)\n\nas_strided(arr, shape=shape, strides=strides)\n'
'  P(X&gt;=a*m)\n= P(X&gt;=c*m)\n\ndef Markov(n, p, c):\n  return 1.0/c\n\n&gt;&gt;&gt; Markov(100,0.2,1.5)\n0.6666666666666666\n\n  P(X&gt;=(1+d)*m)\n= P(X&gt;=c    *m)\n\n1+d = c\n  d = c-1\n\ndef Chernoff(n, p, c):\n  d = c-1\n  m = n*p\n  return math.exp(-d**2/(2+d)*m)\n\n&gt;&gt;&gt; Chernoff(100,0.2,1.5)\n0.1353352832366127\n\n  P(X&gt;=c*m)\n= P(X&gt;=m+k*s)\n\nc*m     = m+k*s\nm*(c-1) = k*s\nk       = m*(c-1)/s\n\ndef Chebyshev(n, p, c):\n  m = n*p\n  s = math.sqrt(n*p*(1-p))\n  k = m*(c-1)/s\n  return 1/k**2\n\n&gt;&gt;&gt; Chebyshev(100,0.2,1.5)\n0.16\n'
"df = pd.DataFrame(columns=['1999-1','2000-1','2000-10'])\n\ndf = df.loc[:, pd.to_datetime(df.columns, format='%Y-%m').year &gt;= 2000]\nprint (df)\n\nEmpty DataFrame\nColumns: [2000-1, 2000-10]\nIndex: []\n\ndf = df.loc[:, df.columns.str[0] != '1']\n\nprint (df)\nEmpty DataFrame\nColumns: [2000-1, 2000-10]\nIndex: []\n\ndef convert_housing_data_to_quarters():\n    #set index from columns in read csv\n    data_source = pd.read_csv('City_Zhvi_AllHomes.csv', index_col=['State','RegionName'])\n    data_source.drop(['Metro','CountyName','RegionID','SizeRank'],axis=1,inplace=True)\n    data_source = data_source.loc[:, data_source.columns.str[0] != '1']\n    return data_source\n\ndf = convert_housing_data_to_quarters()\nprint (df.columns)\nIndex(['2000-01', '2000-02', '2000-03', '2000-04', '2000-05', '2000-06',\n       '2000-07', '2000-08', '2000-09', '2000-10',\n       ...\n       '2017-02', '2017-03', '2017-04', '2017-05', '2017-06', '2017-07',\n       '2017-08', '2017-09', '2017-10', '2017-11'],\n      dtype='object', length=215)\n"
"df.loc[pd.isnull(df['Credit_History']), 'Self_Employed'].value_counts()\ndf.loc[pd.isnull(df['Credit_History']), 'Married'].value_counts()\n\ncols = ['Applicant_Income', 'LoanAmount', 'Loan_Amount_Term']\n\ndf.loc[pd.isnull(df['Credit_History']), cols].mean()\n"
"df.loc[(df['CoapplicantIncome'] &gt; 0) &amp; pd.isnull(df['Married']), 'Married'] = 'Yes'\n"
'from neupy import algorithms\n\ngrnn = algorithms.GRNN(std=0.1)\ngrnn.train(x_train, y_train)\n\ngrnn.train_target = modify_grnn_algorithm(grnn.train_target)\npredicted = grnn.predict(x_test)\n\nimport numpy as np\nfrom neupy import algorithms\nfrom neupy.algorithms.rbfn.utils import pdf_between_data\n\ngrnn = algorithms.GRNN(std=0.1)\ngrnn.train(x_train, y_train)\n\n# In this part of the code you can do any moifications you want\nratios = pdf_between_data(grnn.input_train, x_test, grnn.std)\npredicted = (np.dot(grnn.target_train.T, ratios) / ratios.sum(axis=0)).T\n'
'import pandas as pd\nimport numpy as np\nfrom io import StringIO\n\nmystr = StringIO("""FEATURE_1   FEATURE_2   FEATURE_3   FEATURE_4\n1               1         &lt;1.5        &gt;3.4\nNaN             2           2           4\n4            CANCELED       3          4.5\n1.34            2         &lt;1.5         &lt;2""")\n\n# replace mystr with \'file.csv\'\ndf = pd.read_csv(mystr, delim_whitespace=True)\n\n# define float converter check\ndef converter(x):\n    try:\n        x = float(x)\n        return np.nan\n    except ValueError:\n        return x\n\n# use list comprehension to apply function and clean up\nres = {col: df[col].apply(converter).dropna()\\\n                   .drop_duplicates().tolist() for col in df}\n\n{\'FEATURE_1\': [],\n \'FEATURE_2\': [\'CANCELED\'],\n \'FEATURE_3\': [\'&lt;1.5\'],\n \'FEATURE_4\': [\'&gt;3.4\', \'&lt;2\']}\n'
"df_train.loc[df_train['IsAlone'] &gt; 0, 'IsAlone'] = 1\n"
'import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.transforms as mtrans\n\nfig, (ax_image, ax) = plt.subplots(ncols=2, figsize=(8,3))\n\nax_image.imshow(mpimg.imread(\'https://matplotlib.org/_images/stinkbug.png\'))\nax_image.set_aspect("auto")\nax.plot([0,1],[2,3])\nax.set_ylabel("y")\nxlabel = ax.set_xlabel("want image flush with bottom of this label")\nfig.tight_layout()\nax_image.axis(\'off\')\n\nfig.canvas.draw()\nxlabel_bbox = ax.xaxis.get_tightbbox(fig.canvas.get_renderer())\nxlabel_bbox = xlabel_bbox.transformed(fig.transFigure.inverted())\nbbox1 = ax.get_position().union((ax.get_position(),xlabel_bbox))\nbbox2 = ax_image.get_position()\nbbox3 = mtrans.Bbox.from_bounds(bbox2.x0, bbox1.y0, bbox2.width, bbox1.height)\nax_image.set_position(bbox3)\n\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport matplotlib.transforms as mtrans\n\nfig, (ax_image, ax) = plt.subplots(ncols=2, figsize=(8,3))\n\nax_image.imshow(mpimg.imread(\'https://matplotlib.org/_images/stinkbug.png\'))\nax.plot([0,1],[2,3])\nax.set_ylabel("y")\nxlabel = ax.set_xlabel("want image flush with bottom of this label")\nfig.tight_layout()\nax_image.axis(\'off\')\n\nfig.canvas.draw()\nxlabel_bbox = ax.xaxis.get_tightbbox(fig.canvas.get_renderer())\nxlabel_bbox = xlabel_bbox.transformed(fig.transFigure.inverted())\nbbox1 = ax.get_position().union((ax.get_position(),xlabel_bbox))\nbbox2 = ax_image.get_position()\naspect=bbox2.height/bbox2.width\nbbox3 = mtrans.Bbox.from_bounds(bbox2.x0-(bbox1.height/aspect-bbox2.width)/2., \n                                bbox1.y0, bbox1.height/aspect, bbox1.height)\nax_image.set_position(bbox3)\n\nplt.show()\n'
'&gt;&gt;&gt; data.groupby(data.index.date).mean()\n2013-06-28    51.211806\n2013-06-29    50.256944\n2013-06-30    53.902778\n2013-07-01    50.555556\n2013-07-02    48.302083\n2013-07-03    48.538194\n2013-07-04    50.522059\ndtype: float64\n'
"   start end\n0   0   0\n1   0   0\n2   0   0\n3   1   0\n4   0   0\n5   0   1\n6   0   0\n7   0   0\n8   1   0\n9   0   1\n10  0   0\n\ns = df.start.nonzero()[0]\ne = df.end.nonzero()[0]\n&gt;&gt;&gt; s, e\n(array([3, 8], dtype=int64), array([5, 9], dtype=int64))\n\n&gt;&gt;&gt; index = df.index.values.reshape(-1,1)\n\narray([[ 0],\n   [ 1],\n   [ 2],\n   [ 3],\n   [ 4],\n   [ 5],\n   [ 6],\n   [ 7],\n   [ 8],\n   [ 9],\n   [10]], dtype=int64)\n\n&gt;&gt;&gt; index &lt; [1]       &gt;&gt;&gt; index &lt; [1,2,3,4,5]\narray([[ True],       array([[ True,  True,  True,  True,  True],\n       [False],             [False,  True,  True,  True,  True],\n       [False],             [False, False,  True,  True,  True],\n       [False],             [False, False, False,  True,  True],\n       [False],             [False, False, False, False,  True],\n       [False],             [False, False, False, False, False],\n       [False],             [False, False, False, False, False],\n       [False],             [False, False, False, False, False],\n       [False],             [False, False, False, False, False],\n       [False],             [False, False, False, False, False],\n       [False]])            [False, False, False, False, False]])\n\n&gt;&gt;&gt; ((s &lt;= index) &amp; (index &lt;= e))\n\narray([[False, False],\n       [False, False],\n       [False, False],\n       [ True, False],\n       [ True, False],\n       [ True, False],\n       [False, False],\n       [False, False],\n       [False,  True],\n       [False,  True],\n       [False, False]])\n\n df['Expected Flag'] = ((s &lt;= index) &amp; (index &lt;= e)).sum(axis=1)\n\n    start  end  Expected Flag\n0       0    0              0\n1       0    0              0\n2       0    0              0\n3       1    0              1\n4       0    0              1\n5       0    1              1\n6       0    0              0\n7       0    0              0\n8       1    0              1\n9       0    1              1\n10      0    0              0\n"
"x = df[['Feature 1', 'Feature 2']]\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndataset = pd.DataFrame(x_scaled)\ndataset['Feature 3'] = df['Feature 3']\n"
"cols = ['A', 'B', 'C', 'D']\nmask_1 = df['UNIX_TS'] &gt; df['UNIX_TS'].cummax().shift().fillna(0)\nmask_2 = mask_2 = (df[cols] &gt;= df[cols].cummax().shift().fillna(0)).all(1)\n\ndf[mask_1 &amp; mask_2]\n\n    UNIX_TS     A   B   C   D\n0   1515288240  100 50  90  70\n1   1515288241  101 60  95  75\n2   1515288242  110 70  100 80\n"
"&gt;&gt;&gt; df\n   0\n0  2\n1  2\n2  2\n3  1\n4  1\n5  2\n6  2\n7  1\n8  1\n9  1\n\nonehotencoder = OneHotEncoder(categorical_features= [0])\n\n&gt;&gt;&gt; onehotencoder.fit_transform(df[0].values.reshape(1,-1))\n&lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 10 stored elements in COOrdinate format&gt;\n\n&gt;&gt;&gt; onehotencoder.fit_transform(df[0].values.reshape(1,-1)).toarray()\narray([[1., 2., 2., 1., 1., 2., 2., 1., 1., 1.]])\n\nonehotencoder = OneHotEncoder(categorical_features= [0], sparse=False)\n\n&gt;&gt;&gt; onehotencoder.fit_transform(df[0].values.reshape(1,-1))\narray([[1., 2., 2., 1., 1., 2., 2., 1., 1., 1.]])\n"
'#sample data\npubg_main_df= pd.DataFrame({\'kills\':[60,  42], \'headshotKills\':[3, 26]})\n\nprint("Someone has killed {:.4f} with headshot, have {} kills, while the most kills ever recorded is {}."\n            .format(pubg_main_df[\'headshotKills\'].max(), \n                    pubg_main_df.set_index(\'kills\')[\'headshotKills\'].idxmax(), \n                    pubg_main_df[\'kills\'].max()))\n\n\nSomeone has killed 26.0000 with headshot, have 42 kills, while the most kills ever recorded is 60.\n'
"import string\ndef password_strength(string):\n    if len(string) &lt; 8 or is_english_word(string):\n        return 'WEAK'\n    elif (len(string) &gt; 11 and \n            any(ch in string.ascii_lowercase for ch in string) and\n            any(ch in string.ascii_uppercase for ch in string) and\n            any(ch.isdigit() for ch in string)):\n        return 'STRONG'\n    else:\n        return 'MEDIUM'   \n"
'import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nimport numpy as np\n\n\ndef getDataset(path, x_attr, y_attr, mapping):\n    """\n    Extract dataset from CSV file\n    :param path: location of csv file\n    :param x_attr: list of Features Names\n    :param y_attr: Y header name in CSV file\n    :param mapping: dictionary of the classes integers\n    :return: tuple, (X, Y)\n    """\n    df = pd.read_csv(path)\n    df.replace(mapping, inplace=True)\n    X = np.array(df[x_attr]).reshape(len(df), len(x_attr))\n    Y = np.array(df[y_attr])\n    return X, Y\n\n\ndef custom_recall_score(y_true, y_pred):\n    """\n    Workaround for the recall score\n    :param y_true: Ground Truth during iterations\n    :param y_pred: Y predicted during iterations\n    :return: float, recall\n    """\n    wanted_labels = [0, 1]\n    assert set(wanted_labels).issubset(y_true)\n    wanted_indices = [y_true.tolist().index(x) for x in wanted_labels]\n    wanted_y_true = [y_true[x] for x in wanted_indices]\n    wanted_y_pred = [y_pred[x] for x in wanted_indices]\n    recall_ = recall_score(wanted_y_true, wanted_y_pred,\n                           labels=wanted_labels, average=\'macro\')\n    print("Wanted Indices: {}".format(wanted_indices))\n    print("Wanted y_true: {}".format(wanted_y_true))\n    print("Wanted y_pred: {}".format(wanted_y_pred))\n    print("Recall during cross validation: {}".format(recall_))\n    return recall_\n\n\ndef run(X_data, Y_data):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n    train_index, test_index = next(sss.split(X_data, Y_data))\n    X_train, X_test = X_data[train_index], X_data[test_index]\n    Y_train, Y_test = Y_data[train_index], Y_data[test_index]\n    param_grid = {\'C\': [0.1, 1]} # or whatever parameter you want\n    # I am using LR just for example\n    model = LogisticRegression(solver=\'saga\', random_state=0)\n    clf = GridSearchCV(model, param_grid,\n                       cv=StratifiedKFold(n_splits=2),\n                       return_train_score=True,\n                       scoring=make_scorer(custom_recall_score))\n    clf.fit(X_train, Y_train)\n    print(clf.cv_results_)\n\n\nX_data, Y_data = getDataset("dataset_example.csv", [\'TSH\', \'T4\'], \'diagnosis\',\n                            {\'compensated_hypothyroid\': 0, \'primary_hypothyroid\': 1,\n                             \'hyperthyroid\': 2, \'normal\': 3})\nrun(X_data, Y_data)\n\nWanted Indices: [3, 5]\nWanted y_true: [0, 1]\nWanted y_pred: [3, 3]\nRecall during cross validation: 0.0\n...\n...\nWanted Indices: [0, 4]\nWanted y_true: [0, 1]\nWanted y_pred: [1, 1]\nRecall during cross validation: 0.5\n...\n...\n{\'param_C\': masked_array(data=[0.1, 1], mask=[False, False],\n  fill_value=\'?\', dtype=object), \n  \'mean_score_time\': array([0.00094521, 0.00086224]), \n  \'mean_fit_time\': array([0.00298035, 0.0023526 ]), \n  \'std_score_time\': array([7.02142715e-05, 1.78813934e-06]), \n  \'mean_test_score\': array([0.21428571, 0.5       ]), \n  \'std_test_score\': array([0.24743583, 0.        ]), \n  \'params\': [{\'C\': 0.1}, {\'C\': 1}], \n  \'mean_train_score\': array([0.25, 0.5 ]), \n  \'std_train_score\': array([0.25, 0.  ]), \n  ....\n  ....}\n'
'filtered = dataset[dataset[\'position\'].eq(\'data scientist\')]\nfiltered = dataset[dataset[\'position\'] == \'data scientist\']\nfiltered = dataset.query(\'position == "data scientist"\')\n'
"df.groupby(['placeID','latitude','longitude','price','Rcuisine']).mean()\n"
"membership['new'] = membership.groupby('A')['C']\\\n                              .transform(lambda x: np.where(x.index == x.index[-1], \n                                                            x.max(), 0))\n\n              A  B  C  new\n0   20000000460  4  1    0\n1   20000000460  0  1    0\n2   20000000460  5  0    0\n3   20000000460  0  0    1\n4   20000000459  6  0    0\n5   20000000461  0  1    0\n6   20000000461  2  0    1\n7   20000000462  5  1    1\n8   20000000464  6  1    0\n9   20000000464  7  1    0\n10  20000000464  4  0    0\n11  20000000464  3  0    1\n12  20000000465  2  0    0\n13  20000000465  7  0    0\n14  20000000466  9  1    1\n"
"for x in ['link', 'post', 'status', 'timeline']:\n    fb_posts.loc[fb_posts['title'].str.contains(x, regex=False), 'title'] = x\n"
"g = df.groupby('unit')['A']\ndf['Expected'] = g.cumsum().div(g.cumcount() + 1)\nprint (df)\n  unit   A  Expected\n0  T10   8       8.0\n1  T10   7       7.5\n2  T10  12       9.0\n3  T11  10      10.0\n4  T11   6       8.0\n5  T12  17      17.0\n6  T12   7      12.0\n7  T12   3       9.0\n"
"#for valid set \n\nv = valid.reshape(15150,)\n\nor_fpath = '/content/food-101/images/' #path of original folder\ncp_fpath = '/content/food101/valid/'   #path of destination folder\n\nfor y in tqdm(v):\n\n foldername = y.split('/')[0]\n\n img = y.split('/')[1] +'.jpg'\n\n ip_path = or_fpath+foldername\n op_path = cp_fpath+foldername\n\n if not os.path.exists(op_path):\n   os.mkdir(op_path)\n\n os.rename(os.path.join(ip_path, img), os.path.join(op_path, img))\n\n\n\n"
"df['business_year'] = df['year'] + df['month'].apply(lambda x: -1 if x in ('Jan', 'Feb') else 0)\n\ndf['business_year'] = df['year'] + ~df1['month'].isin(('Jan', 'Feb')) - 1\n"
"c = df.groupby('order_id').cumcount() // 3\nm = (c == 0).groupby(df.order_id).transform('all')\n\ndf['order_id2'] = (\n    np.where(m, df.order_id, df.order_id.astype(str) + '-' + c.astype(str))\n      .astype(str))\n\ndf.head(10)\n\n   order_id order_id2\n0         1       1-0\n1         1       1-0\n2         1       1-0\n3         1       1-1\n4         1       1-1\n5         1       1-1\n6         1       1-2\n7         2         2\n8         2         2\n9         2         2\n\nc = (df.groupby('order_id').cumcount() // 3).astype(str)\ndf['order_id3'] = df['order_id'].astype(str).str.cat(c, sep='-')\n\ndf.head(10)\n\n   order_id order_id2 order_id3\n0         1       1-0       1-0\n1         1       1-0       1-0\n2         1       1-0       1-0\n3         1       1-1       1-1\n4         1       1-1       1-1\n5         1       1-1       1-1\n6         1       1-2       1-2\n7         2         2       2-0\n8         2         2       2-0\n9         2         2       2-0\n"
"from collections import Counter\nsources = df['source'].unique()\noutput = {source: 0 for source in sources}\nfor image, sub_df in df.groupby('image'):\n    counts = Counter(sub_df['label'].sum())\n    for image, source, labels in sub_df.itertuples(index=False):\n        for label in labels:\n            output[source] += counts[label] - 1\nprint(output)\n"
"from bisect import bisect_left\nimport pandas as pd\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n# Create sample dataframe of 10k rows with an interval of 1-19 days.\nnp.random.seed(0)\nn = 10000  # Sample size, e.g. 10k rows.\nyears = np.random.randint(2010, 2019, n)\nmonths = np.random.randint(1, 13, n)\ndays = np.random.randint(1, 29, n)\ndf = pd.DataFrame({'start_date': [pd.Timestamp(*x) for x in zip(years, months, days)],\n                   'interval': np.random.randint(1, 20, n)})\ndf['end_date'] = df['start_date'] + pd.TimedeltaIndex(df['interval'], unit='d')\ndf = df.drop('interval', axis=1)\n\n# Get a sorted list of holidays since the fist start date.\nhols = calendar().holidays(df['start_date'].min())\n\n# Determine if there is a holiday between the start and end dates (both inclusive).\ndf['holiday_in_range'] = df['end_date'].ge(\n    df['start_date'].apply(lambda x: bisect_left(hols, x)).map(lambda x: hols[x]))\n\n&gt;&gt;&gt; df.head(6)\n  start_date   end_date  holiday_in_range\n0 2015-07-14 2015-07-31             False\n1 2010-12-18 2010-12-30              True  # 2010-12-24\n2 2013-04-06 2013-04-16             False\n3 2013-09-12 2013-09-24             False\n4 2017-10-28 2017-10-31             False\n5 2013-12-14 2013-12-29              True  # 2013-12-25\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# create test data\nx = np.arange(1,20,1)\ny = x**0.1\n\ndf = pd.DataFrame(y, columns=['data'])\n\n# find value for proportion shape\ndef proportion_value(df):\n    for i in np.arange(0.1, 100, 0.1):\n        if len(df)-1 &lt; i &lt; len(df):\n            return i\n\n\npercentage = 20\n# load proportion\ndf['proportion'] = np.arange(-percentage, percentage, (percentage*2)/proportion_value(df))/100\n\n# calculate new data\ndf['new_data'] = df['data'] + (df['data']*df['proportion'])\n\n# plot data\ndf[['data', 'new_data']].plot()\nplt.show()\n"
'classifier.score(X_test, y_test)\n'
'from  itertools import product\n#all columns\ndf = pd.DataFrame(list(product(*df.values.T)))\n#if ned specify columns\n#df = pd.DataFrame(list(product(*[df.a, df.b, df.c])))\nprint (df)\n'
"target_mean = df_train.groupby(['id1', 'id2'])['target'].mean().rename('avg')\n\ndf_test = df_test.join(target_mean, on=['id1', 'id2'])\n\ntarget_mean = df_train.groupby(['id1', 'id2'])['target'].mean().to_dict()\ndf_test['avg'] = df_test.set_index(['id1', 'id2']).index.map(target_mean)\n"
'pip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib nbextension install \n'
'def mersenne_number(p):\n   return ((2**p)-1)\n'
'import numpy\nimport pandas\n\ndef add_radians(df):\n    return df.assign(**{colname.rstrip("_deg"): numpy.radians(col) for colname, col in df.iteritems()})\n\nn_ref = 26\nref_traj = pandas.DataFrame({"lat_deg": -76 + numpy.linspace(-1, 1, n_ref),\n                             "lon_deg":   3 + numpy.linspace(-1, 1, n_ref)**2,\n                            }).pipe(add_radians)\n\nn = 500\ntraj = pandas.DataFrame({"lat_deg": -76 + numpy.cumsum(numpy.random.choice([-1, 1], size=n) * 0.05),\n                         "lon_deg":   3 + numpy.cumsum(numpy.random.choice([-1, 1], size=n) * 0.05),\n                        }).pipe(add_radians)\n\nax = traj.plot.scatter(x="lat_deg", y="lon_deg")\nax = ref_traj.plot.scatter(x="lat_deg", y="lon_deg", color="red", ax=ax)\n\ndef distance(lat1, lon1, lat2, lon2):\n    # TODO: check that shape of lat1, lon1, lat2, lon2 are all compatible.\n    R = 6371  # Radius of Earth in kilometers\n\n    # TODO: check this distance calculation\n\n    def hav(theta):\n        return numpy.sin(theta)**2\n\n    d_lat = lat2 - lat1\n    d_lon = lon2 - lon1\n    a = hav(d_lat / 2) + numpy.cos(lat1) * numpy.cos(lat2) * hav(d_lon / 2)\n    return 2 * R * numpy.sqrt(a)\n\ndef min_distance(ref_lat, ref_lon, lat, lon):\n    shape = (numpy.shape(lat)[0], numpy.shape(ref_lat)[0])\n\n    def broadcasted(a):\n        return numpy.broadcast_to(a, shape=shape)\n\n    d = distance(lat1=broadcasted(ref_lat), \n                 lon1=broadcasted(ref_lon), \n                 lat2=broadcasted(lat[:, numpy.newaxis]),\n                 lon2=broadcasted(lon[:, numpy.newaxis]))\n    return numpy.amin(d, axis=-1)\n\nd = min_distance(ref_traj[\'lat\'], ref_traj[\'lon\'], traj[\'lat\'], traj[\'lon\'])\ntolerance = 10  # in kilometers\nnear_ref = d &lt; tolerance\n\nax = ref_traj.plot.scatter(x="lat_deg", y="lon_deg", color="red")\ntraj[near_ref].plot.scatter(x="lat_deg", y="lon_deg", color="blue", ax=ax)\ntraj[~near_ref].plot.scatter(x="lat_deg", y="lon_deg", color="gray", ax=ax)\n'
"import matplotlib.pyplot as plt\n\ndef plotMSN(i):\n    #whatever you're doing in here\n    ax.plot() #plot it on its own axis, this will reference the one you're on in your loop\n\n\nfig, axis_array = plt.subplots(len(csvlist), 1, subplot_kw = {'aspect':1}) #this will set up subplots that are arranged vertically\nfor i, ax in zip(csvlist, axis_array):\n    plotMSN(i)\n\ndef plotMSN(i):\n  #determine an appropriate name either in this function or in the loop that calls it \n  #plotting stuff here\n  fig.savefig(new_filename)\n  plt.close() # this prevents it from using the same instance over and over. \n"
'cv = CountVectorizer(vocabulary=unique_ingredients, lowercase=False, ngram_range=(max_word_count))\n'
"# Import label encoder \nfrom sklearn import preprocessing \n\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n\n# Encode labels in column 'species'. \ndf['species']= label_encoder.fit_transform(df['species']) \n\ndf['species'].unique() \n\ndf.apply(LabelEncoder().fit_transform)\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_columns = ['country', 'gender']\nnumerical_columns = ['age']\ncolumn_trans = make_column_transformer(\n    (categorical_columns, OneHotEncoder(handle_unknown='ignore'),\n    (numerical_columns, RobustScaler())\ncolumn_trans.fit_transform(df)\n"
"import pandas as pd\nfrom collections import Counter\n\ndf.fillna(method='ffill', inplace=True)\n\n# Create a counter object and pass it the origin-destination tuples\ncounter = Counter()\nfor col in df.columns:\n    routes = list(zip(df[col].shift(1, fill_value=df[col][0]), df[col]))\n    routes = [(k, v) for k, v in routes if k != v]\n    counter.update(routes)\ncounter.most_common(3)\n\ncounter.most_common(3)\nOut[76]: \n[(('Spain', 'USA'), 3),\n (('Portugal', 'Spain'), 2),\n (('Bulgaria', 'Portugal'), 1)]\n"
"df = pd.DataFrame({'a': [10, 20, 30, 40, 50, 60],\n                   'b': [1, 1000, 20000, 3, 50, 80],\n                   'c': [1000, 4, 97, 16, 2500, 36]})\ndf.diff()\n\n      a        b       c\n0   NaN      NaN     NaN\n1  10.0    999.0  -996.0\n2  10.0  19000.0    93.0\n3  10.0 -19997.0   -81.0\n4  10.0     47.0  2484.0\n5  10.0     30.0 -2464.0\n\ndf['a_first_difference'] = df['a'].diff()\n\ndf.diff(periods=-1)\n\n      a        b       c\n0 -10.0   -999.0   996.0\n1 -10.0 -19000.0   -93.0\n2 -10.0  19997.0    81.0\n3 -10.0    -47.0 -2484.0\n4 -10.0    -30.0  2464.0\n5   NaN      NaN     NaN\n"
'import numpy\ndata = [2.1, 2.01, 6, 2.2, 1.9]\n\nelements = numpy.array(data)\n\nmean = numpy.mean(elements, axis=0)\nsd = numpy.std(elements, axis=0)\nfinal_list = [x for x in data if (x &gt; mean - 1 * sd)]\nfinal_list = [x for x in final_list if (x &lt; mean + 1 * sd)]\nprint(final_list)\n\n[2.1, 2.01, 2.2, 1.9]\n'
'from bs4 import BeautifulSoup\nfrom requests import get\n\n\nurl = \'https://finance.yahoo.com/quote/AAPL/key-statistics?p=AAPL\'\nresponse = get(url)\nsoup = BeautifulSoup(response.text, \'html.parser\')\n\nstock_data = soup.find_all("table")\n# stock_data will contain multiple tables, next we examine each table one by one\n\nfor table in stock_data:\n    # Scrape all table rows into variable trs\n    trs = table.find_all(\'tr\')\n    for tr in trs:\n        # Scrape all table data tags into variable tds\n        tds = tr.find_all(\'td\')\n        # Index 0 of tds will contain the measurement\n        print("Measure: {}".format(tds[0].get_text()))\n        # Index 1 of tds will contain the value\n        print("Value: {}".format(tds[1].get_text()))\n        print("")\n\n\ndef get_measurement(table_array, measurement):\n    for table in table_array:\n        trs = table.find_all(\'tr\')\n        for tr in trs:\n            tds = tr.find_all(\'td\')\n            if measurement.lower() in tds[0].get_text().lower():\n                return(tds[1].get_text())\n\n\n# print only one measurement, e.g. operating cash flow\nprint(get_measurement(stock_data, "operating cash flow"))\n'
"# Split training and testing samples\nx_train, x_test, y_train, y_test = train_test_split(df_idx['IBOV'], df_idx['IFIX'], test_size=0.2)\n\n# Convert the samples to Numpy arrays\nregr = linear_model.LinearRegression()\nx_train = x_train.values\ny_train = y_train.values\nx_test = x_test.values\ny_test = y_test.values\n\n# Plot the result\nplt.scatter(x_train, y_train)\n\nregr.fit(x_train.reshape(-1,1), y_train)\nidx = np.argsort(x_train)\ny_pred = regr.predict(x_train[idx].reshape(-1,1))\nplt.plot(x_train[idx], y_pred, color='blue', linewidth=3);\n\n# Plot the result\nplt.scatter(x_test, y_test)\nidx = np.argsort(x_test)\ny_pred = regr.predict(x_test[idx].reshape(-1,1))\nplt.plot(x_test[idx], y_pred, color='blue', linewidth=3);\n"
'import pandas as pd \ndf = pd.read_csv("../data/student-mat.csv", sep=\';\')\n\n# pass "df" to pd.get_dummies()\ndf_one_hot = pd.get_dummies(df, columns=["reason"],prefix=["reason"])\ndf_one_hot.head()\n\n# passed again "df" to pd.get_dummies()\ndf_one_hot = pd.get_dummies(df, columns=["guardian"], prefix=["guardian"])\n\n## solution =&gt; pass the df_one_hot to pd.get_dummies\n# df_one_hot = pd.get_dummies(df_one_hot, columns=["guardian"], prefix=["guardian"])\n'
'df = df.fillna(value=df.mean())\nprint (df)\n      A  B\n1:  2.0  3\n2:  2.0  1\n3:  2.0  4\n'
'# initializing fixed color to all countries\ncolorsCountries = {}\nfor country in rel_big.columns:\n    colorsCountries[country] = random.choice(list(mcd.CSS4_COLORS.keys()))\n\n# plot temporary dataframe\np = plt.plot(rel_plot[:i].index, rel_plot[:i].values)\n\n# plot temporary dataframe\nfor keyIndex in rel_plot[:i].keys() :\n    p = plt.plot(rel_plot[:i].index,rel_plot[:i][keyIndex].values,color=colorsCountries[keyIndex])\n\nleg = plt.legend(topN)\nfor line, text in zip(leg.get_lines(), leg.get_texts()):\n    line.set_color(colorsCountries[text.get_text()])\n\nimport matplotlib._color_data as mcd\nimport random\n\nimport pandas as pd\nimport requests\nimport os\nfrom matplotlib import pyplot as plt\nimport matplotlib.animation as ani\nimport matplotlib._color_data as mcd\nimport random\n\nrel_big_file = \'rel_big.csv\'\nrel_big_url = \'https://pastebin.com/raw/bJbDz7ei\'\n\nif not os.path.exists(rel_big_file):\n    r = requests.get(rel_big_url)\n    with open(rel_big_file, \'wb\') as f:\n        f.write(r.content)\n\nrel_big = pd.read_csv(rel_big_file, index_col=\'Date\')\n\n# history of top N countries\nchamps = []\n# initializing fixed color to all countries\ncolorsCountries = {}\nfor country in rel_big.columns:\n    colorsCountries[country] = random.choice(list(mcd.CSS4_COLORS.keys()))\n# frame draw function\ndef animate_graph(i=int):\n    N = 10\n    # get current values for each country\n    last_index = rel_big.index[i]\n    # which countries are top N in last_index?\n    topN = rel_big.loc[last_index].sort_values(ascending=False).head(N).index.tolist()\n    # if country not already in champs, add it\n    for c in topN:\n        if c not in champs:\n            champs.append(c)\n    # pull a standard color map from matplotlib\n    cmap = plt.get_cmap("tab20")\n    # draw legend\n    plt.legend(topN)\n    # make a temporary dataframe with only top N countries\n    rel_plot = rel_big[topN].copy(deep=True)\n    # plot temporary dataframe\n    #### Removed Code\n    #p = plt.plot(rel_plot[:i].index, rel_plot[:i].values)\n    #### Removed Code\n    for keyIndex in rel_plot[:i].keys() :\n        p = plt.plot(rel_plot[:i].index,rel_plot[:i][keyIndex].values,color=colorsCountries[keyIndex])\n    # set color for each country based on index in champs\n    #### Removed Code\n    #for i in range(0, N):\n        #p[i].set_color(cmap(champs.index(topN[i]) % 20))\n    #### Removed Code\n    leg = plt.legend(topN)\n    for line, text in zip(leg.get_lines(), leg.get_texts()):\n        line.set_color(colorsCountries[text.get_text()])\n\n%matplotlib notebook\nfig = plt.figure(figsize=(10, 6))\nplt.xticks(rotation=45, ha="right", rotation_mode="anchor")\n# x ticks get too crowded, limit their number\nplt.gca().xaxis.set_major_locator(plt.MaxNLocator(nbins=10))\nanimator = ani.FuncAnimation(fig, animate_graph, interval = 333)\nplt.show()\n'
"# extract the suffixes `_x, _y`\nsuffixes = df.columns.str.extract('(_.*)$')[0]\n\n# output\npd.concat([pd.get_dummies(df.iloc[:,i+1])\n             .add_suffix(suffixes[i+1])\n             .mul(df.iloc[:,i],axis=0) \n           for i in range(0,df.shape[1], 2)],\n          axis=1\n         )\n\n   ball_x  bat_x  gloves_x  ball_y  gloves_y  mitt_y\n0       0     10         0       0         0      45\n1      12      0         0      30         0       0\n2       0      0        13       0        25       0\n3       0     14         0       0         0      20\n"
"import pandas as pd\ndf = pd.DataFrame({'Month': [1,2,11,12], 'Days': [1,22,3,23]})\n\npd.to_datetime(df['Month'].astype(str)+' '+df['Days'].astype(str), format='%m %d')\n# 0   1900-01-01\n# 1   1900-02-22\n# 2   1900-11-03\n# 3   1900-12-23\n# dtype: datetime64[ns]\n\ndf = pd.DataFrame({'Month': [1,2,11,12], 'Days': [1,22,3,23]})\ndf['Year'] = 2020\npd.to_datetime(df[['Year', 'Month', 'Days']])\n"
"def between_any(time):\n    for s,c in zip(spacex, clonx):\n        if (time  &gt;= s) and (time &lt;= c):\n            return True\n    return False\n\ndf['Between Any'] = df['datim'].apply(between_any)\n\ndef between_index(time):\n    output = []\n    for i in range(len(spacex)):\n        if (time  &gt;= spacex[i]) and (time &lt;= clonx[i]):\n            output.append(i)\n    return output if output else np.nan\n\ndf['Between Indices'] = df['datim'].apply(between_index)\n\ndef between_values(time):\n    output = []\n    for s,c in zip(spacex, clonx):\n        if (time  &gt;= s) and (time &lt;= c):\n            output.append((s,c))\n    return output if output else np.nan\n\ndf['Between Values'] = df['datim'].apply(between_values)\n\nIn[0]: df\n\nOut[0]:\n                   datim\n0    2019-08-14 23:26:00\n1    2019-08-14 23:26:00\n2    2019-08-14 23:27:00\n3    2019-08-14 23:30:00\n4    2019-08-14 23:30:00\n5101 2020-05-25 20:48:00\n5102 2020-05-25 20:49:00\n5103 2020-05-26 13:52:00\n5104 2020-05-26 13:52:00\n5105 2020-05-26 14:22:00\n\nIn[1]:\n\nclonx = pd.Series(['2019-08-14T23:32:00.000000000', '2019-08-14T23:35:00.000000000','2019-08-14T23:35:00.000000000','2020-05-24T14:55:00.000000000', '2020-05-24T15:03:00.000000000','2020-05-25T12:09:00.000000000'])\n\nspacex = pd.Series(['2019-08-14T23:27:00.000000000', '2019-08-14T23:30:00.000000000','2019-08-14T23:30:00.000000000','2020-05-24T14:50:00.000000000', '2020-05-24T14:58:00.000000000','2020-05-25T12:04:00.000000000'])\n\nclonx = pd.to_datetime(clonx)\nspacex = pd.to_datetime(spacex)\n\ndf['Between Any'] = df['datim'].apply(between_any)\ndf['Between Indices'] = df['datim'].apply(between_index)\ndf['Between Values'] = df['datim'].apply(between_values)\n\ndf\n\nOut[1]:\n\n                   datim  Between Any Between Indices  \\\n0    2019-08-14 23:26:00        False             NaN   \n1    2019-08-14 23:26:00        False             NaN   \n2    2019-08-14 23:27:00         True             [0]   \n3    2019-08-14 23:30:00         True       [0, 1, 2]   \n4    2019-08-14 23:30:00         True       [0, 1, 2]   \n5101 2020-05-25 20:48:00        False             NaN   \n5102 2020-05-25 20:49:00        False             NaN   \n5103 2020-05-26 13:52:00        False             NaN   \n5104 2020-05-26 13:52:00        False             NaN   \n5105 2020-05-26 14:22:00        False             NaN   \n\n                                         Between Values  \n0                                                   NaN  \n1                                                   NaN  \n2          [(2019-08-14 23:27:00, 2019-08-14 23:32:00)]  \n3     [(2019-08-14 23:27:00, 2019-08-14 23:32:00), (...  \n4     [(2019-08-14 23:27:00, 2019-08-14 23:32:00), (...  \n5101                                                NaN  \n5102                                                NaN  \n5103                                                NaN  \n5104                                                NaN  \n5105                                                NaN  \n"
'X_test = pd.get_dummies(test_data[features])\n'
"import pandas as pd\nimport altair as alt\n\nsource = pd.DataFrame({\n    'age': ['12', '32', '43', '54', '32', '32', '12','20','44','24'],\n    'gender': ['m','m','f','f','f','m','f','m','f','m']\n})\n\nalt.Chart(source).transform_joinaggregate(\n    total='count(*)',\n    groupby=['gender']\n).transform_calculate(\n    pct='1 / datum.total'\n).mark_bar().encode(\n    alt.X('age:Q', bin=True),\n    alt.Y('sum(pct):Q', axis=alt.Axis(format='%')),\n    color='gender'\n)\n"
'#Tensor of size 1x1x2x3100 \n[0, ..., non_zero_val, 0, other_non_zero_val, 0, 0]\n[0,  1 , ...                                  3099]/3099 #element-wise division just to normalise\n'
"recent_grads[['major_category', 'college_jobs', 'non_college_jobs']].groupby('major_category').sum()\n"
"from matplotlib.colors import cm\n\ndef get_rgba(value, limits, cmap):\n    fac = (value-limits[0]) / (limits[1]-limits[0])\n    return cmap(fac)\n\ncmap = cm.get_cmap('jet')\nlimits = [0,10]\nvalue  = 5.292\n\nprint(get_rgba(value, limits, cmap))\n\n(0.5787476280834913, 1.0, 0.38899430740037955, 1.0)\n"
"data = [{&quot;COL5&quot;: f&quot;D{i}&quot;, &quot;COL6&quot;: f&quot;E{i}&quot;, &quot;COL7&quot;: f&quot;F{i}&quot;} for i in range(1, 4)]\ndf = pd.DataFrame({&quot;COL1&quot;: [&quot;A&quot;], &quot;COL2&quot;: [&quot;B&quot;], &quot;COL3&quot;: [&quot;C&quot;], &quot;COL4&quot;: [data]})\n\n#   COL1 COL2 COL3                                               COL4\n# 0    A    B    C  [{'COL5': 'D1', 'COL6': 'E1', 'COL7': 'F1'}, {...\n\nu = df.explode('COL4').reset_index(drop=True)\nu.iloc[:, :-1].join(pd.DataFrame(u['COL4'].tolist()))\n\n  COL1 COL2 COL3 COL5 COL6 COL7\n0    A    B    C   D1   E1   F1\n1    A    B    C   D2   E2   F2\n2    A    B    C   D3   E3   F3\n"
"import pandas as pd\nimport altair as alt\n\ndf = pd.DataFrame({\n    &quot;Job Stat&quot;: ['INV', &quot;WRK&quot;, &quot;CMP&quot;, &quot;JRB&quot;],\n    &quot;Revenue&quot;: [100, 200, 300, 400],\n    &quot;Total Income&quot;: [150, 250, 350, 450]\n})\n\n(\n  alt.Chart(df)\n    .transform_fold([&quot;Revenue&quot;, &quot;Total Income&quot;], as_=[&quot;key&quot;, &quot;value&quot;])\n    .mark_bar()\n    .encode(\n        x=&quot;key:N&quot;,\n        y=&quot;value:Q&quot;,\n        color=&quot;key:N&quot;,\n        column=&quot;Job Stat&quot;,\n    )\n)\n\n(\n  alt.Chart(df)\n    .transform_fold([&quot;Revenue&quot;, &quot;Total Income&quot;], as_=[&quot;key&quot;, &quot;value&quot;])\n    .mark_bar()\n    .encode(\n        alt.X('key:N', axis=None),\n        alt.Y(&quot;value:Q&quot;),\n        alt.Color(&quot;key:N&quot;, legend=alt.Legend(title=None, orient='bottom')),\n        alt.Column(&quot;Job Stat&quot;,\n          sort=['INV', &quot;WRK&quot;, &quot;CMP&quot;, &quot;JRB&quot;],\n          header=alt.Header(labelOrient=&quot;bottom&quot;, title=None)\n        )\n    )\n)\n"
"cols = ['Region','Rank 2015','Score 2015','Economy 2015','Family 2015','Health 2015','Freedom 2015','Generosity 2015','Trust 2015','Rank 2016','Score 2016','Economy 2016','Family 2016','Health 2016','Freedom 2016','Generosity 2016','Trust 2016','Rank 2017','Score 2017','Economy 2017','Family 2017','Health 2017','Freedom 2017','Generosity 2017','Trust 2017','Rank 2018','Score 2018','Economy 2018','Family 2018','Health 2018','Freedom 2018','Generosity 2018','Trust 2018','Rank 2019','Score 2019','Economy 2019','Family 2019','Health 2019','Freedom 2019','Generosity 2019','Trust 2019','Score Mean','Economy Mean','Family Mean','Health Mean','Freedom Mean','Generosity Mean','Trust Mean']\ndf = pd.DataFrame(np.random.randint(1,10,(3,48)))\ndf.columns = cols\nprint(df.iloc[:, :4])\n\n   Region  Rank 2015  Score 2015  Economy 2015\n0       7          9           9             9\n1       8          7           2             3\n2       3          3           4             5\n\ntarget_cols = ['Rank', 'Score', 'Family', 'Health', 'Freedom', 'Generosity', 'Trust']\nyears = ['2015', '2016', '2017', '2018', '2019']\nnewdf = pd.DataFrame([df.loc[1, [x + ' ' + year for x in target_cols]].values for year in years])\nnewdf.columns = target_cols\nnewdf['year'] = years\nprint(newdf)\n\n   Rank  Score  Family  Health  Freedom  Generosity  Trust  year\n0     7      2       6       9        3           4      9  2015\n1     2      8       1       1        7           6      1  2016\n2     7      4       2       5        1           7      4  2017\n3     9      7       1       4        7           5      2  2018\n4     5      4       4       9        1           6      2  2019\n"
"pd.to_datetime(df.date,unit='d',origin='1900-01-01')\nOut[205]: \n0   2012-12-04\n1   2014-03-05\n2   1999-07-22\nName: date, dtype: datetime64[ns]\n"
'Accuracy:  tensor(0.5059)\nAccuracy:  tensor(0.8702)\nAccuracy:  tensor(0.9159)\nAccuracy:  tensor(0.9233)\nAccuracy:  tensor(0.9336)\nAccuracy:  tensor(0.9484)\nAccuracy:  tensor(0.9602)\nAccuracy:  tensor(0.9676)\nAccuracy:  tensor(0.9705)\nAccuracy:  tensor(0.9749)\n'
"key = df['data_c'].eq('Discharge').cumsum()\ndf['variable'] = df.groupby(key).cumcount().map({0:'A',1:'B',2:'C'})\ndf\nOut[61]: \n   data_a   data_b     data_c   data_d variable\n0      60  0.30786  Discharge  2.31714        A\n1      61  0.30792       Rest  2.34857        B\n2     121  0.62095       Rest  2.38647        C\n3     182  0.93398  Discharge  2.31115        A\n4     183  0.93408       Rest  2.34550        B\n5     243  1.24711       Rest  2.37162        C\n6     304  1.56014  Discharge  2.30855        A\n7     305  1.56019       Rest  2.34215        B\n8     365  1.87322       Rest  2.36276        C\n9     426  2.18630  Discharge  2.30591        A\n\ns = pd.crosstab(index=key, columns=df['variable'], values=df['data_d'], aggfunc='sum')\ndfout = s.eval('C - (B-A)').to_frame(name = 'measurement')\ndfout\nOut[69]: \n        measurement\ndata_c             \n1           2.35504\n2           2.33727\n3           2.32916\n4               NaN\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; df = pd.DataFrame({'A':['Alpha','Beta','Gamma','-','Kappa','Delta','-','Zeta']})\n&gt;&gt;&gt; df\n       A\n0  Alpha\n1   Beta\n2  Gamma\n3      -\n4  Kappa\n5  Delta\n6      -\n7   Zeta\n\n&gt;&gt;&gt; df.replace('-',np.nan)\n       A\n0  Alpha\n1   Beta\n2  Gamma\n3    NaN\n4  Kappa\n5  Delta\n6    NaN\n7   Zeta\n"
"cols = ['Factory', 'Restaurant', 'Store', 'Building']\n\ndf.groupby('Name', as_index=False)[cols].sum()\n\n    Name  Factory  Restaurant  Store  Building\n0  Brian        2           0      1         1\n1   Mike        2           1      1         1\n2    Sam        1           0      1         1\n"
"df[&quot;Date&quot;] = pd.to_datetime(df[&quot;Date&quot;].astype(str), format='%Y.%m')\ndf = df.set_index(&quot;Date&quot;)\nplt.plot(df[&quot;Price&quot;].tail(100))\n"
"float_cols = df.select_dtypes(include='number').columns\ndf = df.dropna(subset=float_cols, how='all')\ndf\nOut[1]: \n    A    B        C    D\n1  B1  2.0  NaN      3.0\n3  D1  1.0  String3  2.0\n4  E1  NaN  String4  3.0\n5  F1  2.0  String5  NaN \n"
'def three(self):\n    self.__init__(None, self.nexts)\n    print(self.name)\n    print(self.nexts)\n\na.three()\n# Output: None, 1\n'
"from itertools import combinations\n\nstocks = ['MSFT','APPL','IBM','GM','XP','INTC']\nlist(combinations(iterable=stocks, r=2))\n"
'pd.DataFrame(df_transformed.toarray())\n\ntransformer = ColumnTransformer([(&quot;enc&quot;, \n                                  enc,\n                                  cat_features)],\n                                  remainder=&quot;passthrough&quot;,sparse_threshold=0)\n'
"# sample data\ndata = [{'key1': 'k1', 'key2': 'k2', 'keyn': 'kn', 'type': 'p1', 'val1': 1, 'val2': 2, 'valn': 7}, {'key1': 'k1', 'key2': 'k2', 'keyn': 'kn', 'type': 'p2', 'val1': 6, 'val2': 1, 'valn': 5}, {'key1': 'k1', 'key2': 'k2', 'keyn': 'kn', 'type': 'p3', 'val1': 8, 'val2': 4, 'valn': 1}, {'key1': 'k3', 'key2': 'k2', 'keyn': 'kn', 'type': 'p1', 'val1': 4, 'val2': 6, 'valn': 9}, {'key1': 'k3', 'key2': 'k2', 'keyn': 'kn', 'type': 'p2', 'val1': 6, 'val2': 1, 'valn': 0}, {'key1': 'k3', 'key2': 'k2', 'keyn': 'kn', 'type': 'p3', 'val1': 1, 'val2': 2, 'valn': 8}]\ndf = pd.DataFrame(data)\n\n# stack the df, with index `['key1', 'key2', 'keyn', 'type']`\ncols = df.columns[df.columns.str.startswith('key')].tolist()\ncols.append('type')\n# print(cols) # ['key1', 'key2', 'keyn', 'type']\ndfn = df.set_index(cols).loc[:, 'val1':'valn'].stack().reset_index()\n# print(dfn)\n\n# create col_name as `valn_typep1`\ndfn['col_name'] = dfn.iloc[:,-2] + '_type'  +  dfn.iloc[:,-3] \n\n# set index with `['key1', 'key2', 'keyn', 'col_name']`, value is column 0, and unstack, transfer index col_name to columns\ncols = df.columns[df.columns.str.startswith('key')].tolist()\ncols.append('col_name')\n# print(cols) # ['key1', 'key2', 'keyn', 'col_name']\ndf_result = dfn.set_index(cols)[0].unstack().reset_index()\n# print(df_result)\n\nprint(dfn)\n\n       key1 key2 keyn type level_4  0\n    0    k1   k2   kn   p1    val1  1\n    1    k1   k2   kn   p1    val2  2\n    2    k1   k2   kn   p1    valn  7\n    3    k1   k2   kn   p2    val1  6\n    4    k1   k2   kn   p2    val2  1\n    5    k1   k2   kn   p2    valn  5\n    6    k1   k2   kn   p3    val1  8\n    7    k1   k2   kn   p3    val2  4\n    8    k1   k2   kn   p3    valn  1\n    9    k3   k2   kn   p1    val1  4\n    10   k3   k2   kn   p1    val2  6\n    11   k3   k2   kn   p1    valn  9\n    12   k3   k2   kn   p2    val1  6\n    13   k3   k2   kn   p2    val2  1\n    14   k3   k2   kn   p2    valn  0\n    15   k3   k2   kn   p3    val1  1\n    16   k3   k2   kn   p3    val2  2\n    17   k3   k2   kn   p3    valn  8\n\nprint(df_result)\n\n    col_name key1 key2 keyn  val1_typep1  val1_typep2  val1_typep3  val2_typep1  \\\n    0          k1   k2   kn            1            6            8            2   \n    1          k3   k2   kn            4            6            1            6   \n    \n    col_name  val2_typep2  val2_typep3  valn_typep1  valn_typep2  valn_typep3  \n    0                   1            4            7            5            1  \n    1                   1            2            9            0            8 \n\n"
"m = df.columns.str.contains('_')\n\ndf = (df.set_index(df.columns[~m].tolist())\n        .groupby(lambda x: x.split('_')[0], axis=1)\n        .max()\n        .reset_index())\nprint (df)\n   A  B  C  D  E  K\n0  a  2  r  4  6  9\n1  e  g  1  d  8  7\n\ndef rms(x):\n    return np.sqrt(np.sum(x**2, axis=1)/len(x.columns))\n\nm = df.columns.str.contains('_')\n\ndf1 = (df.set_index(df.columns[~m].tolist())\n        .groupby(lambda x: x.split('_')[0], axis=1)\n        .agg(rms)\n        .reset_index())\nprint (df1)\n   A  B  C  D         E         K\n0  a  2  r  4  3.915780  5.972158\n1  e  g  1  d  5.567764  4.690416\n"
'import torch.nn as nn\nimport torch\n\nimport torch.nn.functional as F\n\nclass Network(nn.Module):\n    def __init__(self, output_size):\n        super(Network, self).__init__()\n        self.lstm = nn.LSTM(300,500,1, batch_first=True)\n        self.dropout = nn.Dropout(p=0.5)\n        #self.l2 = nn.L2\n        self.linear = nn.Linear(500,output_size)\n\n    def forward(self,x,hidden):\n        x, hidden = self.lstm(x,hidden)\n        x = x.contiguous().view(-1, 500)\n        x = self.dropout(x)\n        x = self.linear(x)\n        return x , hidden\n\n    def init_hidden(self,batch_size):\n        weights = next(self.parameters()).data\n        hidden = (weights.new(1 , batch_size,500).zero_().cuda(),\n                  weights.new(1 , batch_size,500).zero_().cuda())\n        return hidden\n\n# your code for intializing the model and data and all other stuff\nfor i in range(epochs):\n\n    #Testing\n    if i % 1 == 0:\n        total_loss = 0\n        total_kappa = 0\n        total_batches = 0\n        model.eval()\n        val_h  = model.init_hidden(batch_size) # intialize the hidden state\n        for (text, score) in test_loader:\n           # Creating new variables for the hidden state, otherwise\n           # we\'d backprop through the entire training history\n            val_h = tuple([each.data for each in val_h]) \n            out ,  val_h  = model(text,val_h)\n            out_score = torch.argmax(out, 1)\n            y_onehot.zero_()\n            y_onehot.scatter_(1, score, 1)\n            kappa_l = cohen_kappa_score(score.view(batch_size).tolist(), out_score.view(batch_size).tolist())\n            score = score.view(-1)\n            loss = criti(out, score.view(-1))\n            total_loss += loss\n            total_kappa += kappa_l\n            total_batches += 1\n        print(f"Epoch {i} Testing kappa {total_kappa/total_batches} loss {total_loss/total_batches}")\n        with open(f"model/epoch_{i}", "wb") as f:\n            torch.save(model.state_dict(),f)\n    model.train()\n\n    #Training\n    h =  model.init_hidden(batch_size) # intialize the hidden state\n    for (text, score) in train_loader:\n        optimizer.zero_grad()\n        step += 1\n        # Creating new variables for the hidden state, otherwise\n        # we\'d backprop through the entire training history\n        h = tuple([each.data for each in h])\n        out , h  = model(text,h)\n        out_score = torch.argmax(out,1)\n        y_onehot.zero_()\n        y_onehot.scatter_(1, score, 1)\n        kappa_l = cohen_kappa_score(score.view(batch_size).tolist(),out_score.view(batch_size).tolist())\n        loss = criti(out, score.view(-1))\n        print(f"Epoch {i} step {step} kappa {kappa_l} loss {loss}")\n        loss.backward()\n        optimizer.step()\n'
"k.groupby(['Sid', 'Itemid']).Block.count()\nSid  Itemid   \n1    214536500    1\n     214536502    1\n     214536506    1\n     214577561    1\n2    214551617    1\n     214662742    2\n     214757390    1\n     214757407    1\n     214825110    1\n3    214716935    1\n     214774687    1\n     214832672    1\n4    214706482    1\n     214836765    1\n6    214563337    1\n     214706462    1\n     214717089    1\n     214819762    1\n     214821275    2\n     214821371    6\nName: Block, dtype: int64\n"
'df.Item = df.Item.apply(lambda x: x.split(" (")[0])\n'
'Outlier PersonIDs based on overall data\narray([ 1.,  4.,  7.,  8.])\nOutlier PersonIDs based on each user\'s data and overall deviation\narray([ 1.,  3.,  4.,  5.,  7.,  8.,  9.])\n\n#! /usr/bin/python3\n\nimport random\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\n\n# Visualize:\nimport matplotlib.pyplot as plt\n\n#### Create Sample Data START\n# Parameters:\nTimeInExpected=8.5 # 8:30am\nTimeOutExpected=17 # 5pm\nsig=1 # 1 hour variance\nPersons=11\n# Increasing the ratio between sample size and persons will make more people outliers.\nSampleSize=20\nAccuracy=1 # Each hour is segmented by hour tenth (6 minutes)\n\n# Generate sample\nSampleDF=pd.DataFrame([\n    np.random.randint(1,Persons,size=(SampleSize)),\n    np.around(np.random.normal(TimeInExpected, sig,size=(SampleSize)),Accuracy),\n    np.around(np.random.normal(TimeOutExpected, sig,size=(SampleSize)),Accuracy)\n    ]).T\nSampleDF.columns = [\'PersonID\', \'TimeIn\',\'TimeOut\']\n\n# Visualize\nplt.hist(SampleDF[\'TimeIn\'],rwidth=0.5,range=(0,24))\nplt.hist(SampleDF[\'TimeOut\'],rwidth=0.5,range=(0,24))\nplt.xticks(np.arange(0,24, 1.0))\nplt.xlabel(\'Hour of day\')\nplt.ylabel(\'Arrival / Departure Time Frequency\')\nplt.show()\n#### Create Sample Data END\n\n\n#### Analyze data \n# Threshold distribution percentile\nOutlierSensitivity=0.05 # Will catch extreme events that happen 5% of the time. - one sided! i.e. only late arrivals and early departures.\npresetPercentile=scipy.stats.norm.ppf(1-OutlierSensitivity)\n\n# Distribution feature and threshold percentile\nargdictOverall={\n    "ExpIn":SampleDF[\'TimeIn\'].mode().mean().round(1)\n    ,"ExpOut":SampleDF[\'TimeOut\'].mode().mean().round(1)\n    ,"sigIn":SampleDF[\'TimeIn\'].var()\n    ,"sigOut":SampleDF[\'TimeOut\'].var()\n    ,"percentile":presetPercentile\n}\nOutlierIn=argdictOverall[\'ExpIn\']+argdictOverall[\'percentile\']*argdictOverall[\'sigIn\']\nOutlierOut=argdictOverall[\'ExpOut\']-argdictOverall[\'percentile\']*argdictOverall[\'sigOut\']\n\n# Overall\n# See all users with outliers - overall\nOutliers=SampleDF["PersonID"].loc[(SampleDF[\'TimeIn\']&gt;OutlierIn) | (SampleDF[\'TimeOut\']&lt;OutlierOut)]\n\n# See all observations with outliers - Overall\n# pp.pprint(SampleDF.loc[(SampleDF[\'TimeIn\']&gt;OutlierIn) | (SampleDF[\'TimeOut\']&lt;OutlierOut)].sort_values(["PersonID"]))\n\n# Sort and remove NAs\nOutliers=np.sort(np.unique(Outliers))\n# Show users with overall outliers:\nprint("Outlier PersonIDs based on overall data")\npp.pprint(Outliers)\n\n# For each\nOutliersForEach=[]\nfor Person in SampleDF[\'PersonID\'].unique():\n    # Person specific dataset\n    SampleDFCurrent=SampleDF.loc[SampleDF[\'PersonID\']==Person]\n    # Distribution feature and threshold percentile\n    argdictCurrent={\n        "ExpIn":SampleDFCurrent[\'TimeIn\'].mode().mean().round(1)\n        ,"ExpOut":SampleDFCurrent[\'TimeOut\'].mode().mean().round(1)\n        ,"sigIn":SampleDFCurrent[\'TimeIn\'].var()\n        ,"sigOut":SampleDFCurrent[\'TimeOut\'].var()\n        ,"percentile":presetPercentile\n    }\n    OutlierIn=argdictCurrent[\'ExpIn\']+argdictCurrent[\'percentile\']*argdictCurrent[\'sigIn\']\n    OutlierOut=argdictCurrent[\'ExpOut\']-argdictCurrent[\'percentile\']*argdictCurrent[\'sigOut\']\n    if SampleDFCurrent[\'TimeIn\'].max()&gt;OutlierIn or SampleDFCurrent[\'TimeOut\'].min()&lt;OutlierOut:\n        Outliers=np.append(Outliers,Person)\n\n# Sort and get unique values\nOutliers=np.sort(np.unique(Outliers))\n# Show users with overall outliers:\nprint("Outlier PersonIDs based on each user\'s data and overall deviation")\npp.pprint(Outliers)\n'
'import re\n\n# make the regex pattern here\npattern = r"([\\d\\.]*),([\\d\\.]*),([\\d\\.]*),([^,]*),([^,]*),(.*\\.?),([\\d\\-\\s:]*),([\\d\\-\\s:]*)"\n\n# open the file with \'with\' so you don\'t have to worry about closing it\nwith open(filetrace) as f:\n    for line in f:  # iterate through the lines\n        values = re.findall(pattern, line)[0]  # re.findall returns a list \n                                               # literal of a tuple\n        # record the values somewhere\n'
'In [11]: df = pd.DataFrame([[1, 2], [1, 3], [2, 4]], columns=["A", "B"])\n\nIn [12]: df\nOut[12]:\n   A  B\n0  1  2\n1  1  3\n2  2  4\n\nIn [13]: df.groupby("A").count()\nOut[13]:\n   B\nA\n1  2\n2  1\n\nIn [14]: df.groupby("A", as_index=False).count()\nOut[14]:\n   A  B\n0  1  2\n1  2  1\n'
"print (df)\n    sale_id          dt  receipts_qty\n31    196.0  2017-02-19          95.0\n32    203.0  2017-02-20         101.0\n33    196.0  2017-02-21         105.0\n34    196.0  2017-02-22         112.0\n35    196.0  2017-02-23         118.0\n36    196.0  2017-02-24         135.0\n37    196.0  2017-02-25         135.0\n38    196.0  2017-02-26         124.0\n40    203.0  2017-02-27         290.0\n39    196.0  2017-02-27          84.0\n42    103.0  2017-02-28         330.0 &lt;-changed data, value &lt; 196\n41    196.0  2017-02-28         124.0\n43    196.0  2017-03-01         100.0\n44    203.0  2017-03-01         361.0\n\n#get only values &gt; 196 \ndf['a'] = (df.sale_id == 196).astype(int)\n#sorting by new column, remove duplicates, remove helper column\ndf['a'] = (df.sale_id == 196).astype(int)\ndf = (df.sort_values(['a','dt'], ascending=[False, True])\n       .drop_duplicates('dt')\n       .drop('a', axis=1)\n       .sort_index())\nprint (df)\n    sale_id          dt  receipts_qty\n31    196.0  2017-02-19          95.0\n32    203.0  2017-02-20         101.0\n33    196.0  2017-02-21         105.0\n34    196.0  2017-02-22         112.0\n35    196.0  2017-02-23         118.0\n36    196.0  2017-02-24         135.0\n37    196.0  2017-02-25         135.0\n38    196.0  2017-02-26         124.0\n39    196.0  2017-02-27          84.0\n41    196.0  2017-02-28         124.0\n43    196.0  2017-03-01         100.0\n"
'In [11]: import ast\n\nIn [12]: pd.DataFrame([ast.literal_eval(line) for line in open("second.txt")])\nOut[12]:\n                               0       1   2   3    4    5     6      7      8          9          10\n0  Tue Sep 12 15:13:56 +0000 2017  text.    0  en  390  529  7138  15727  False -84.395235  33.771232\n1  Tue Sep 12 15:13:59 +0000 2017    text   0  en  648  891  2087   5801  False -84.321948  33.752879\n2  Tue Sep 12 15:14:01 +0000 2017    text   0  en  217  222   959    958  False -82.849182  27.865251\n3  Tue Sep 12 15:14:06 +0000 2017    text   0  en   71   85  2357   1290  False -82.299760  27.857254\n\nIn [21]: line = "[\'Tue Sep 12 15:13:56 +0000 2017\', \'text. \', 0, \'en\', 390, 529, 7138, 15727, False, -84.395235, 33.771232]"\n\nIn [22]: ast.literal_eval(line)\nOut[22]:\n[\'Tue Sep 12 15:13:56 +0000 2017\',\n \'text. \',\n 0,\n \'en\',\n 390,\n 529,\n 7138,\n 15727,\n False,\n -84.395235,\n 33.771232]\n'
"import pandas as pd\nimport itertools\n\n# First get a DataFrame (or could be a Series) of the pairwise combinations in each row\ncombinations = df['entry'].apply(lambda x: list(itertools.combinations(sorted(x), 2)))\n\n# Then get a list of unique values - A,B,C,D,E\nunique_values = sorted(list(set(\n    symbol for symbol_list in df.values.flatten() for symbol in symbol_list)))\n\n# Create empty dataframe\nresult = pd.DataFrame(columns=unique_values, index=unique_values)\n\n# Iterate through symbols and fill dataframe\nfor symbol_pair in list(itertools.combinations(unique_values, 2)):\n    result.loc[symbol_pair[0], symbol_pair[1]] = combinations.apply(lambda x: symbol_pair in x).sum()\n"
'plt.plot(x,y)\n'
"import pandas as pd\n\ns = '''\\\nID In_Out Amount\n1  In     5\n1  Out    8\n2  In     4\n2  Out    2\n3  In     3\n3  Out    9\n4  Out    8'''\n\n# Recreate dataframe\ndf = pd.read_csv(pd.compat.StringIO(s), sep='\\s+')\n\n# Select rows where In_Out == 'Out' and multiple by -1\ndf.loc[df['In_Out'] == 'Out', 'Amount'] *= -1\n\n# Convert to dict\nd = df.groupby('ID')['Amount'].sum().to_dict()\nprint(d)\n\n{1: -3, 2: 2, 3: -6, 4: -8}\n"
'@app.route("/", methods=[\'GET\', \'POST\'])\ndef index():\n   {...}\n\n@app.route(\'/submit\')\ndef submit(name, id):\n   {...}\n\nfrom flask import Flask, request, redirect, url_for, make_response\n'
"df.loc[(df['reading_type'] == 'door') &amp; (df['value'] == 255), 'event'] = 'door on'\ndf.loc[(df['reading_type'] == 'door') &amp; (df['value'] == 0), 'event'] = 'door off'\n\ndf2 = df[df['reading_type'] == 'door'].copy()\n# The line below is modified\ndf2.loc[df2['event'] == 'door off', 'went_out'] = df2[df2['event'] == 'door off']['datetime']\nprint(df2)\n\n    id  datetime    device  location    reading_type    value   event   went_out\n284 284 2018-01-01 07:57:56 Door    door    door    255.0   door on NaN\n285 285 2018-01-01 07:58:12 Door    door    door    0.0 door off    2018-01-01 07:58:12\n294 294 2018-01-01 08:29:25 Door    door    door    255.0   door on NaN\n295 295 2018-01-01 08:29:38 Door    door    door    0.0 door off    2018-01-01 08:29:38\n357 357 2018-01-01 09:16:38 Door    door    door    255.0   door on NaN\n361 361 2018-01-01 09:17:40 Door    door    door    0.0 door off    2018-01-01 09:17:40\n\ndf2.loc[((df2['event'].shift(-1) == 'door on') &amp; (df2['event']=='door off') ), 'went_out'] = df2[df2['event']=='door off']['datetime']\n\nprint(df2[df2['event'] == 'door off'])\n\n    id  datetime    device  location    reading_type    value   event   went_out\n285 285 2018-01-01 07:58:12 Door    door    door    0.0 door off    2018-01-01 07:58:12\n295 295 2018-01-01 08:29:38 Door    door    door    0.0 door off    NaN\n361 361 2018-01-01 09:17:40 Door    door    door    0.0 door off    2018-01-01 09:17:40\n509 509 2018-01-01 15:50:46 Door    door    door    0.0 door off    2018-01-01 15:50:46\n"
'import pandas as pd\ndf = pd.DataFrame({\'CityLocation\': ["Bangalore / Palo Alto", "Bangalore / SFO", "Other"]})\n\ndf[\'CityLocation\'] = df[\'CityLocation\'].str.replace("^Bang.*", "Bangalore")\n\nprint(df)\n\n  CityLocation\n0    Bangalore\n1    Bangalore\n2        Other\n'
' pip install cluster\n\npip install matplotlib\n'
"df['TotalChargesCategories'] = pd.cut(pd.to_numeric(df['TotalCharges'], errors='coerce'),10)\n\n&gt;&gt;&gt; df\n   TotalCharges Churn TotalChargesCategories\n0         29.85    No       (21.985, 816.38]\n1        1889.5    No     (1602.91, 2389.44]\n2        108.15   Yes       (21.985, 816.38]\n3       1840.75    No     (1602.91, 2389.44]\n4        151.65   Yes       (21.985, 816.38]\n5         820.5   Yes      (816.38, 1602.91]\n6        1949.4    No     (1602.91, 2389.44]\n7         301.9    No       (21.985, 816.38]\n8       3046.05   Yes     (2389.44, 3175.97]\n9       3487.95    No      (3175.97, 3962.5]\n10       587.45    No       (21.985, 816.38]\n11        326.8    No       (21.985, 816.38]\n12       5681.1    No     (5535.56, 6322.09]\n13       5036.3   Yes     (4749.03, 5535.56]\n14      2686.05    No     (2389.44, 3175.97]\n15      7895.15    No     (7108.62, 7895.15]\n16      missing    No                    NaN\n17      7382.25    No     (7108.62, 7895.15]\n18       528.35   Yes       (21.985, 816.38]\n"
"def extract(path_to_file):\n    with open(path_to_file) as f:\n        values = []\n        for idx, line in enumerate(f):\n            if idx &lt; 6:\n                # Ignore header lines\n                continue\n            if line.strip():\n                # Add the values in this line to the current\n                # values list.\n                values.extend(line.split())\n            else:\n                # Blank line, so output values and\n                # clear the list.\n                yield values\n                del values[:]\n        # Yield the final set of values, assuming\n        # the last line of the file is not blank.\n        yield values\n\n\nvalues = extract('data.dat')\nfor item in values:\n    print(item)\n\n['-6.63661', '-6.63661', '-6.76161', '-6.76161', '-6.83974', '-6.55849', '-6.55849', '-6.12099', '-5.93349', '-5.90224', '-5.73036', '-5.55849', '-5.71474', '-5.60536', '-5.71474', '-5.71474', '-5.76161', '-5.76161', '-5.83974', '-5.83974', '-5.83974', '-5.73036', '-5.60536', '-5.51161', '-5.32411', '-5.35536', '-5.19911', '-5.18349', '-4.87099', '-4.57411', '-4.23036', '-3.74599', '-3.76161', '-3.76161', '-3.91786', '-3.91786', '-4.30849', '-4.43349', '-5.10536', '-6.37099']\n['-5.79286', '-5.91786', '-6.32/411', '-6.82411', '-6.82411', '-6.71474', '-6.58974', '-6.58974', '-6.48036', '-6.48036', '-6.30849', '-6.02724', '-6.10536', '-5.21474', '-5.01161', '-4.48036', '-4.60536', '-4.51161', '-4.44911', '-4.69911', '-4.77724', '-4.99599', '-5.43349', '-5.43349', '-5.41786', '-5.27724', '-5.27724', '-6.01161', '-5.43349', '-6.15224', '-5.44911', '-4.69911', '-3.71474', '-2.40224', '-3.48036', '-4.12099', '-4.69911', '-5.16786', '-6.08974', '-4.74599']\n"
'from itertools import combinations\n\nnp.random.seed(0)\n\narr = np.array(np.random.randn(2, 3))\n&gt;&gt;&gt; arr\narray([[ 1.76405235,  0.40015721,  0.97873798],\n       [ 2.2408932 ,  1.86755799, -0.97727788]])\n\n&gt;&gt;&gt; np.array([arr[:, [i, j]] for i, j in combinations(range(arr.shape[1]), 2)])\narray([[[ 1.76405235,  0.40015721],  # First and second column.\n        [ 2.2408932 ,  1.86755799]],\n\n       [[ 1.76405235,  0.97873798],  # First and third column.\n        [ 2.2408932 , -0.97727788]],\n\n       [[ 0.40015721,  0.97873798],  # Second and third column.\n        [ 1.86755799, -0.97727788]]])\n'
'df1.loc[df1[\'id\'].isin(postData[\'id\']), \'id\'] = \'XX\'\nprint (df1)\n   id  B  S\n0  XX  c  f\n1  XX  d  g\n2  XX  e  h\n3  XX  d  j\n4  XX  d  e\n5  XX  c  j\n6   h  s  t\n7   i  e  r\n8   j  s  p\n9   k  q  p\n\npostStr = """{\n                     "S":["f","h"],\n                     "id":["a","b","c","d","f","g"],\n                     "currencycode":["USD"]\n\n                }"""\n\npostData = json.loads(postStr, object_pairs_hook=OrderedDict)\nprint (postData)\nOrderedDict([(\'S\', [\'f\', \'h\']), \n             (\'id\', [\'a\', \'b\', \'c\', \'d\', \'f\', \'g\']), \n             (\'currencycode\', [\'USD\'])])\n\ndf = {\n\'id\':[\'a\',\'b\',\'c\',\'d\',\'f\',\'g\',\'h\',\'i\',\'j\',\'k\'],\n\'B\':[\'c\',\'d\',\'e\',\'d\',\'d\',\'c\',\'s\',\'e\',\'s\',\'q\'],\n\'S\':[\'f\',\'g\',\'h\',\'j\',\'e\',\'j\',\'t\',\'r\',\'p\',\'p\']\n}\ndf1 = pd.DataFrame(df)\n\nfor col in df1.columns.intersection(postData.keys()):\n    df1.loc[df1[col].isin(postData[col]), col] = \'XX\'\nprint (df1)\n   id  B   S\n0  XX  c  XX\n1  XX  d   g\n2  XX  e  XX\n3  XX  d   j\n4  XX  d   e\n5  XX  c   j\n6   h  s   t\n7   i  e   r\n8   j  s   p\n9   k  q   p\n'
'y_pred = logreg1.predict(X_test)\nscore = logreg1.score(X_test, y_pred)\nprint(y_pred)     // see the predictions\n'
"In [31]: df1\nOut[31]: \n   A  B\n0  2  6\n1  5  1\n2  7  3\n3  1  2\n4  9  7\n5  4  7\n6  3  4\n7  8  9\n\nIn [36]: df2\nOut[36]: \n   A  B  A_bin  B_bin  C  D  E\n0  2  6      1      2  5  4  1\n1  5  1      2      1  2  2  4\n2  7  3      3      1  5  1  7\n3  1  2      1      1  8  4  9\n4  9  7      3      3  5  5  8\n5  4  7      2      3  1  8  5\n6  3  4      1      2  2  9  3\n7  8  9      3      3  4  6  2\n\ncount = 0\nd = dict()\n\nfor col in df2.columns:\n    print(col)\n    l = df1.shape[1]\n    if count &lt; l:\n        d[col] = df2[ df2.iloc[:, count + l] == 1 ]\n    count += 1\n\nIn [52]: for key in d.keys():\n    ...:     print(d[key][d[key].columns.drop(list(d[key].filter(regex='bin')))])\n\n   A  B  C  D  E\n0  2  6  5  4  1\n3  1  2  8  4  9\n6  3  4  2  9  3\n\n\n   A  B  C  D  E\n1  5  1  2  2  4\n2  7  3  5  1  7\n3  1  2  8  4  9\n"
"d = dict(tuple(df.groupby('App_Name')))\n\nprint (d['com.alpha.studio'])\n            App_Name        Date     Response     Gross  Revenue\n9   com.alpha.studio  2018-10-16  1731.429858  11643154      NaN\n11  com.alpha.studio  2018-10-17  2769.373388  13198984      NaN\n14  com.alpha.studio  2018-10-18  2784.822039  24217875      NaN\n\nd1 = {}\nfor k, v in d.items():\n     d1[k] = v['Gross Revenue'].rolling(2).mean()\n"
'list1 = [i.split() for i in word_list] \n\nword_list = ["hello darkness my", "old friend I\'ve", "come to see you", "again"]\n\nlist1 = [i.split() for i in word_list]\nprint(list1)\n\n[[\'hello\', \'darkness\', \'my\'], [\'old\', \'friend\', "I\'ve"], [\'come\', \'to\', \'see\', \'you\'], [\'again\']]\n\nfor j in list1:\n    if anagrams( string, j ) == True:\n        return list1\n    else:\n        return []\n\ndef anagrams( string1, string2 ):\n    str_1 = string1.lower()\n    str_2 = string2.lower()\n\nlist1 = []\nfor i in word_list:\n  for word in i.split():\n    list1.append(word)\n\nprint(list1)\n\n[\'hello\', \'darkness\', \'my\', \'old\', \'friend\', "I\'ve", \'come\', \'to\', \'see\', \'you\', \'again\']\n'
"ticker = Equity_Tickers[0]  # FB\ndata = pdr.DataReader(ticker, 'yahoo', start, end)\n\nfor ticker in Equity_Tickers:\n    data = pdr.DataReader(ticker, 'yahoo', start, end)\n    df = data[['Adj Close']]\n\nframes = []\nfor ticker in Equity_Tickers:\n    data = pdr.DataReader(ticker, 'yahoo', start, end)\n    frames.append(data[['Adj Close']])\n\n# eg, use frames[0] to access first ticker's DataFrame\n\nframes = {}\nfor ticker in Equity_Tickers:\n    data = pdr.DataReader(ticker, 'yahoo', start, end)\n    frames[ticker] = data[['Adj Close']]\n\n# eg, use frames['FB'] to access FB ticker's DataFrame\n\n# list comprehension\nframes = [pdr.DataReader(t, 'yahoo', start, end)[['Adj Close']] for t in Equity_Tickers]\n\n# dict comprehension\nframes = {t: pdr.DataReader(t, 'yahoo', start, end)[['Adj Close']] for t in Equity_Tickers}\n"
"df1 = (df.groupby('STNAME')['CENSUS2010POP']\n         .agg({'avg': np.average, 'sum': np.sum}))\n\ndf = pd.DataFrame({'STNAME':list('aab'),\n                   'CENSUS2010POP':[10,20,50]})\n\ndf1 = (df.groupby('STNAME')['CENSUS2010POP']\n         .agg([('avg', np.average), ('sum', np.sum)]))\nprint (df1)\n        avg  sum\nSTNAME          \na        15   30\nb        50   50\n\n(df.set_index('STNAME').groupby('STNAME')['CENSUS2010POP']\n   .agg([('avg', np.average), ('sum', np.sum)]))\n"
"num1 = list(yearly_status.index)\nnum2 = list(yearly_posts.index)\nbarWidth = 0.50\nplt.bar(num1, yearly_status.values.flatten(), color='#b5ffb9',edgecolor='white',width=barWidth)\nplt.bar(num2, yearly_posts.values.flatten(), color='#f9bc86',edgecolor='white',width=barWidth)\n"
'ser = [nine, ten, eleven, twelve,thirteen]\nmy_names = [2009,2010,2011,2012,2013]\n\ndf = pd.concat(ser, axis=1, keys=my_names)\n\nnine = [4,5,6]\nten = [9,0,3]\neleven = [7,3,0]\n\nL = [nine, ten, eleven]\nmy_names = [2009,2010,2011]\n\ndf = pd.DataFrame({k:v for k, v in zip(my_names, L)})\nprint (df)\n   2009  2010  2011\n0     4     9     7\n1     5     0     3\n2     6     3     0\n\ndf = pd.concat(my_series, axis=1)\n\ndf = pd.DataFrame(my_series).T\n'
'test ="# #DataScience #KJSBDKJ kjndjk#jnjkd #jkzcjkh# iusadhuish#"\ntest = re.sub(r\'(?&lt;!\\S)#(?=\\S)\', \'\', test)\n\n# DataScience KJSBDKJ kjndjk#jnjkd jkzcjkh# iusadhuish#\n'
'y_true = [3, -0.5, 2, 7]\ny_pred = [2.5, 0.0, 2, 8]\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n32.73809523809524\n\nmean_squared_error(y_true, y_pred)\n0.375\n'
' writer.writerow((pharmacy.text[0], add.text[0]))\n\nwriter.writerow((pharmacy.text, add.text))\n'
"print (mc_response)\n     Country GenderSelect  Age\n0  Argentina       Female   10\n1  Australia         Male   20\n2  Australia       Female   30\n3  Australia         Male   43\n\nfun = [('mean_age', 'mean'), ('median_age','median')]\ngroupbyClass2 = (mc_response.groupby(['Country','GenderSelect'])['Age']\n                            .agg(fun)\n                            .rename_axis(['Country','Gender'])\n                            .reset_index())\nprint (groupbyClass2)\n     Country  Gender  mean_age  median_age\n0  Argentina  Female      10.0        10.0\n1  Australia  Female      30.0        30.0\n2  Australia    Male      31.5        31.5\n\ndf.columns = ['Country','Gender'] + df.columns[2:].tolist()\n"
'from bs4 import BeautifulSoup as bs\n\naList = [\'&lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Mate SE Factory Unlocked 5.93” - 4GB/64GB Octa-core Processor| 16MP + 2MP Dual Camera| GSM Only |Grey (US Warranty)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Honor 8X (64GB + 4GB RAM) 6.5" HD 4G LTE GSM Factory Unlocked Smartphone - International Version No Warranty JSN-L23 (Black)&lt;/span&gt;, &lt;span class="a-size-medium a-color-base a-text-normal"&gt;Huawei Honor 8X (64GB + 4GB RAM) 6.5" HD 4G LTE GSM Factory Unlocked Smartphone - International Version No Warranty JSN-L23 (Black)&lt;/span&gt;\']\nfor i in aList:\n    soup = bs(i, \'lxml\')\n    text = [item.text for item in soup.select(\'span\')]  #list\n    print(text)\n    text = \',\'.join([item.text for item in soup.select(\'span\')]) #comma separated string\n    print(text)\n'
"df1 = data[data['project_is_approved']==0] \n['school_state'].value_counts().rename('Class_0').reset_index('State')\n"
"   Age Height Weight Gender   Salary      s\n0   12    5'7    NaN      M    29000  29.0K\n1  NaN    5'8    160      M      650    650\n2   32    5'5    165    NaN  7840000  7.84M\n3   21    NaN    155      F  6550000  6.55M\n4   55   5'10    170    NaN  8950000  8.95M\n"
"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'values' : np.random.randint(low=0, high=401, size=500)\n})\n\n# df.head():\n    values\n0   35\n1   10\n2   61\n3   19\n4   144\n\ndf['valuerange'] = pd.cut(\n    df['values'],\n    bins= [0,50,100,150,200,250,300,350,400],\n    labels=['0-50', '51-100',\n        '100-150', '151-200', '201-250', \n        '251-300', '301-350', '351-400']\n)\n\n    values  valuerange\n0   35      0-50\n1   10      0-50\n2   61      51-100\n3   19      0-50\n4   144     100-150\n"
"doc_set = ['my name is omprakash', 'my name is rajesh']\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ntokenizer = RegexpTokenizer(r'\\w+')\nen_stop = set(stopwords.words('english'))\n\ncleaned_texts = []\n\nfor i in doc_set:\n    tokens = tokenizer.tokenize(i)\n    stopped_tokens = [i for i in tokens if not i in en_stop]\n    cleaned_texts.append(stopped_tokens)\n\n[['name', 'omprakash'], ['name', 'rajesh']]\n\nimport pandas as pd\ndf = pd.DataFrame()\ndf['unclean_text'] = doc_set\ndf['clean_text'] = cleaned_texts\n\n                   text              clean\n0  my name is omprakash  [name, omprakash]\n1     my name is rajesh     [name, rajesh]\n"
"df.values = pd.to_numeric(df.values.str.rstrip('&lt;'), errors='coerce')\n\ndf.values = pd.to_numeric(df.values.str.strip('&lt;&gt;'), errors='coerce')\n"
"df.loc[df['Temp3'].isnull(), 'Temp3'] = df.loc[df['Temp3'].isnull(), ['Temp1', 'Temp2']].mean(axis=1)\n&gt;&gt;&gt; df\n   Temp1  Temp2  Temp3\n0     31   23.0    NaN\n1     22    NaN    NaN\n2     25   25.0   21.0\n&gt;&gt;&gt; df.loc[df['Temp3'].isnull(), 'Temp3'] = df.loc[df['Temp3'].isnull(), ['Temp1', 'Temp2']].mean(axis=1)\n&gt;&gt;&gt; df\n   Temp1  Temp2  Temp3\n0     31   23.0   27.0\n1     22    NaN   22.0\n2     25   25.0   21.0\n"
"color = plt.get_cmap('RdYlGn')   # default color\ncolor.set_bad('lightblue')    # if the value is bad the color would be lightblue instead of white\nh_map = sns.heatmap(data=cor_mat, annot=True, cmap=color)   # now plot the heatmap\n"
"def jaccard(first, second):\n    return len(set(first).intersection(second)) / len(set(first).union(second))\n\nkeys = list(my_dict.keys())\nresult_dict = {}\n\nfor k in keys:\n    for l in keys:\n        result_dict[(k,l)] = result_dict.get((l,k), jaccard(my_dict[k], my_dict[l]))\n\nprint(result_dict)\n{('Community A', 'Community A'): 1.0, ('Community A', 'Community B'): 0.6666666666666666, ('Community A', 'Community C'): 0.2, ('Community A', 'Community D'): 0.4, ('Community B', 'Community A'): 0.6666666666666666, ('Community B', 'Community B'): 1.0, ('Community B', 'Community C'): 0.0, ('Community B', 'Community D'): 0.2, ('Community C', 'Community A'): 0.2, ('Community C', 'Community B'): 0.0, ('Community C', 'Community C'): 1.0, ('Community C', 'Community D'): 0.75, ('Community D', 'Community A'): 0.4, ('Community D', 'Community B'): 0.2, ('Community D', 'Community C'): 0.75, ('Community D', 'Community D'): 1.0}\n"
'enddf.unstack(level=-1, fill_value=0).stack()\n\n                           2\n0          1                \n2019-11-04 Authorize Work  4\n           Await Work      1\n           Check Work      4\n           Closed          4\n           Confirm Work    3\n           Do Work         3\n2019-11-11 Authorize Work  6\n           Await Work      0\n           Check Work      0\n           Closed          0\n           Confirm Work    0\n           Do Work         2\n'
'[*df.isnull().sum().loc[lambda x : x&gt;0].items()]\n'
"# imports\nimport plotly.graph_objs as go\nimport pandas as pd\nimport numpy as np\n\n# data\ndata = {'Date': ['2011-12-01', '2011-12-06', '2011-12-06', '2011-12-07', '2011-12-07'], 'Issue':  ['Loan Modification', 'Loan Servicing', 'Loan Servicing', 'Loan Modification', 'Loan Servicing'], 'Sum': [1, 1, 2, 2, 3]}\ndf = pd.DataFrame(data)\n\n# build figure\nfig=go.Figure(data=[go.Bar(name='Modification',\n                          x=df[df['Issue']=='Loan Modification']['Date'],\n                          y=df[df['Issue']=='Loan Modification']['Sum']),\n                   \n                    go.Bar(name='Servicing',\n                          x=df[df['Issue']=='Loan Servicing']['Date'],\n                          y=df[df['Issue']=='Loan Servicing']['Sum'])])\n\n\n# Change the bar mode\nfig.update_layout(barmode='group')\n#fig.update_layout(barmode='stack')\n\n# show figure\nfig.show()\n\n# imports\nimport plotly.graph_objs as go\nimport pandas as pd\nimport numpy as np\n\n# data\ndata = {'Date': ['2011-12-01', '2011-12-06', '2011-12-06', '2011-12-07', '2011-12-07'], 'Issue':  ['Loan Modification', 'Loan Servicing', 'Loan Servicing', 'Loan Modification', 'Loan Servicing'], 'Sum': [1, 1, 2, 2, 3]}\ndf = pd.DataFrame(data)\n\n# build figure\nfig=go.Figure(data=[go.Scatter(name='Modification',\n                          x=df[df['Issue']=='Loan Modification']['Date'],\n                          y=df[df['Issue']=='Loan Modification']['Sum']),\n                   \n                    go.Scatter(name='Servicing',\n                          x=df[df['Issue']=='Loan Servicing']['Date'],\n                          y=df[df['Issue']=='Loan Servicing']['Sum'])])\n\n# show figure\nfig.show()\n"
'df1\n\n   userId           movies\n0       1  [222, 555, 666]\n1       2  [222, 555, 666]\n2       3       [900, 555]\n3       4            [222]\n4       6            [888]\n\ndf2\n\n   userId           movies\n0       1  [222, 555, 666]\n1       2  [222, 555, 666]\n\ndf3\n\n   userId movies\n0       1    222\n0       1    555\n0       1    666\n1       2    222\n1       2    555\n1       2    666\n\ndf4\n\n   movies  viewer_count\n0     222             2\n1     555             2\n2     666             2\n'
'bin_arr = np.random.randint(2, size = 100)\nfloat_arr = np.random.rand(100)\nout = np.isclose(bin_arr.astype(float), float_arr, atol = .2)\n'
"df[['Amount', 'Currency']] = df['column'].str.extract(r'(\\d+)(\\D+)')\n\nexchange_rate = {'Euro': 1.2, 'pounds': 1.3, 'rupee': 0.05}\ndf['Amount_dollar'] = pd.to_numeric(df['Amount']) * df['Currency'].map(exchange_rate).fillna(1) \n\n      column  Amount Currency  Amount_dollar\n0  100Dollar     100   Dollar         100.00\n1  200Dollar     200   Dollar         200.00\n2    100Euro     100     Euro         120.00\n3    300Euro     300     Euro         360.00\n4  184pounds     184   pounds         239.20\n5  150pounds     150   pounds         195.00\n6    10rupee      10    rupee           0.50\n7    30rupee      30    rupee           1.50\n"
"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\nX = [['Male'], ['Female'], ['Female']]\nenc.fit(X)\nenc.categories_\n\n[array(['Female', 'Male'], dtype=object)]\n\nenc.transform([['Female'], ['Male']]).toarray()\n\narray([[1., 0.],\n       [0., 1.]])\n\nenc.inverse_transform([[0, 1], [1,0], [0, 1]])\n\narray([['Male'],\n       ['Female'],\n       ['Male']], dtype=object)\n\nA = {}\nfor i in enc.categories_[0]:\n    A[i] = enc.transform([[i]]).toarray()\n"
"===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nsepal_width     1.8690      0.033     57.246      0.000       1.804       1.934\n==============================================================================\nOmnibus:                       18.144   Durbin-Watson:                   0.427\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                7.909\nSkew:                          -0.338   Prob(JB):                       0.0192\nKurtosis:                       2.101   Cond. No.                         1.00\n==============================================================================\n\n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nsepal_width       1.4512      0.015     94.613      0.000       1.420       1.482\noutlier_dummy    -6.6097      0.394    -16.791      0.000      -7.401      -5.819\n==============================================================================\nOmnibus:                        1.917   Durbin-Watson:                   2.188\nProb(Omnibus):                  0.383   Jarque-Bera (JB):                1.066\nSkew:                           0.218   Prob(JB):                        0.587\nKurtosis:                       3.558   Cond. No.                         27.0\n==============================================================================\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sample data\ndf = pd.DataFrame(sns.load_dataset('iris'))\n\n# subset of sample data\ndf=df[df['species']=='setosa']\n\n# add column for dummy variable\ndf['outlier_dummy']=0\n\n# append line with extreme value for sepal width\n# as well as a dummy variable = 1 for that row.\ndf.loc[len(df)] = [5,8,1.4, 0.3, 'setosa', 1]\n\n# define independent variables\nx=['sepal_width', 'outlier_dummy']\n\n# run regression\nmod_fit = sm.OLS(df['sepal_length'], df[x]).fit()\nres = mod_fit.resid\n\nfig = sm.qqplot(res)\nplt.show()\nmod_fit.summary()\n"
"from fuzzywuzzy import fuzz\n\ndf['Ratio'] = df.apply(lambda x: fuzz.ratio(x.A, x.B), axis=1)\n#alternative  with list comprehension\n#df['Ratio'] = [fuzz.ratio(a, b) for a,b in zip(df.A, df.B)]\nprint (df)\n            A               B  Ratio\n0   Something  Something Else     78\n1  Everything          Evythn     75\n2     Someone             Cat      0\n3    Everyone            Evr1     50\n\nprint (df)\n            A               B\n0   Something  Something Else\n1  Everything             NaN\n2     Someone             Cat\n3    Everyone            Evr1\n\nfrom fuzzywuzzy import fuzz\n\ndf['Ratio'] = df.dropna(subset=['A', 'B']).apply(lambda x: fuzz.ratio(x.A, x.B), axis=1)\nprint (df)\n            A               B  Ratio\n0   Something  Something Else   78.0\n1  Everything             NaN    NaN\n2     Someone             Cat    0.0\n3    Everyone            Evr1   50.0\n"
"           AGE_GROUP                       shop_id         count_of_member\n1                 10                            12                   57615\n2                 20                             1                     186\n3                 30                             1                     175\n4                 40                             1                     171\n5                 40                            12                  313758\n6                 50                             1                     158\n7                 60                             1                     168\n\n# imports\nimport plotly.graph_objects as go\nimport pandas as pd\n\ndata = {'AGE_GROUP': {0: 10, 1: 10, 2: 20, 4: 30, 5: 30, 6: 40, 7: 40, 8: 50, 10: 60},\n                     'shop_id': {0: 1, 1: 12, 2: 1, 4: 1, 5: 12, 6: 1, 7: 12, 8: 1, 10: 1},\n                     'count_of_member': {0: 40,\n                      1: 57615,\n                      2: 186,\n                      4: 175,\n                      5: 322458,\n                      6: 171,\n                      7: 313758,\n                      8: 158,\n                      10: 168}}\n\n## Optional dataset\n# data = {'AGE_GROUP': {0: 10, 1: 10, 2: 20, 4: 30, 5: 30, 6: 40, 7: 40, 8: 50, 10: 60},\n#                      'shop_id': {0: 1, 1: 12, 2: 1, 4: 1, 5: 12, 6: 1, 7: 12, 8: 1, 10: 1},\n#                      'count_of_member': {0: 40,\n#                       1: 57615,\n#                       2: 186000,\n#                       4: 175000,\n#                       5: 322458,\n#                       6: 171000,\n#                       7: 313758,\n#                       8: 158000,\n#                       10: 168000}}\n\n# # Create DataFrame \ndf = pd.DataFrame(data)\n\n# Manage shop_id\nshops = df['shop_id'].unique()\n\n# set up plotly figure\nfig = go.Figure()\n\n# add one trace per NAR type and show counts per hospital\nfor shop in shops:\n\n    # subset dataframe by shop_id\n    df_ply=df[df['shop_id']==shop]\n\n    # add trace\n    fig.add_trace(go.Bar(x=df_ply['AGE_GROUP'], y=df_ply['count_of_member'], name='shop_id'+str(shop)))\n\nfig.show()\n"
'conda install ipykernel\npython -m ipykernel install --user --name=env_name\n'
'# list of all countries\ncountries = "Afghanistan, Albania, Algeria, Andorra, Angola, Antigua &amp; Deps, Argentina, Armenia, Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bhutan, Bolivia, Bosnia Herzegovina, Botswana, Brazil, Brunei, Bulgaria, Burkina, Burma, Burundi, Cambodia, Cameroon, Canada, Cape Verde, Central African Rep, Chad, Chile, China, Republic of China, Colombia, Comoros, Democratic Republic of the Congo, Republic of the Congo, Costa Rica, Croatia, Cuba, Cyprus, Czech Republic, Danzig, Denmark, Djibouti, Dominica, Dominican Republic, East Timor, Ecuador, Egypt, El Salvador, Equatorial Guinea, Eritrea, Estonia, Ethiopia, Fiji, Finland, France, Gabon, Gaza Strip, The Gambia, Georgia, Germany, Ghana, Greece, Grenada, Guatemala, Guinea, Guinea-Bissau, Guyana, Haiti, Holy Roman Empire, Honduras, Hungary, Iceland, India, Indonesia, Iran, Iraq, Republic of Ireland, Israel, Italy, Ivory Coast, Jamaica, Japan, Jonathanland, Jordan, Kazakhstan, Kenya, Kiribati, North Korea, South Korea, Kosovo, Kuwait, Kyrgyzstan, Laos, Latvia, Lebanon, Lesotho, Liberia, Libya, Liechtenstein, Lithuania, Luxembourg, Macedonia, Madagascar, Malawi, Malaysia, Maldives, Mali, Malta, Marshall Islands, Mauritania, Mauritius, Mexico, Micronesia, Moldova, Monaco, Mongolia, Montenegro, Morocco, Mount Athos, Mozambique, Namibia, Nauru, Nepal, Newfoundland, Netherlands, New Zealand, Nicaragua, Niger, Nigeria, Norway, Oman, Ottoman Empire, Pakistan, Palau, Panama, Papua New Guinea, Paraguay, Peru, Philippines, Poland, Portugal, Prussia, Qatar, Romania, Rome, Russian Federation, Rwanda, St Kitts &amp; Nevis, St Lucia, Saint Vincent &amp; the Grenadines, Samoa, San Marino, Sao Tome &amp; Principe, Saudi Arabia, Senegal, Serbia, Seychelles, Sierra Leone, Singapore, Slovakia, Slovenia, Solomon Islands, Somalia, South Africa, Spain, Sri Lanka, Sudan, Suriname, Swaziland, Sweden, Switzerland, Syria, Tajikistan, Tanzania, Thailand, Togo, Tonga, Trinidad &amp; Tobago, Tunisia, Turkey, Turkmenistan, Tuvalu, Uganda, Ukraine, United Arab Emirates, United Kingdom, United States, Uruguay, Uzbekistan, Vanuatu, Vatican City, Venezuela, Vietnam, Yemen, Zambia, Zimbabwe"\ncountries = countries.split(",")\ncountries = [c.strip() for c in countries]\n\nfilename = "read.txt"\nfilehandle = open(filename, errors=\'ignore\')\nmy_other_list = []\ntoParse = False\nfor line in filehandle:\n    line = line.strip()\n    if line.startswith("Submitting Author:"):\n        toParse = True\n        continue\n    elif line.startswith("Running Head:"):\n        toParse = False\n        continue\n    elif toParse:\n        for c in countries:\n            if c in line:\n                my_other_list.append(c)\n'
'df = pd.read_csv(filename, sep="|", header=[0,1,2], index_col=[0])\nprint (df)\nxxxx         A  B  C  D  E\nxxxx         P  Q  R  S  T\nDATE         L  M  N  O  P\n01/02/1997  12  4  5  0  0\n01/03/1998  71  2  4  8  0\n\nprint (df.columns)\nMultiIndex([(\'A\', \'P\', \'L\'),\n            (\'B\', \'Q\', \'M\'),\n            (\'C\', \'R\', \'N\'),\n            (\'D\', \'S\', \'O\'),\n            (\'E\', \'T\', \'P\')],\n           names=[\'xxxx\', \'xxxx\', \'DATE\'])\n\ndf1 = (df.T\n         .stack(dropna=False)\n         .sort_index(level=3)\n         .rename_axis((\'a\',\'b\',\'c\',\'d\'))\n         .reset_index(name=\'e\'))\nprint (df1)\n   a  b  c           d   e\n0  A  P  L  01/02/1997  12\n1  B  Q  M  01/02/1997   4\n2  C  R  N  01/02/1997   5\n3  D  S  O  01/02/1997   0\n4  E  T  P  01/02/1997   0\n5  A  P  L  01/03/1998  71\n6  B  Q  M  01/03/1998   2\n7  C  R  N  01/03/1998   4\n8  D  S  O  01/03/1998   8\n9  E  T  P  01/03/1998   0\n'
"df['new column'] = df.loc[:, 'column4':'column9'].sum(axis=1)\n\ncols = ['column4', 'column5', 'column6', 'column7', 'column8', 'column9']\ndf['new column'] = df[cols].sum(axis=1)\n"
"list2 = ['goat=3','duck=2']\ndict2 = dict(item.split('=') for item in list2)\ncomparison_func = {'&gt;': lambda x,y: x &gt; y, \n                   '&lt;': lambda x,y: x &lt; y, \n                   '=': lambda x,y: x == y}\n\nvalid_rows = []\nfor row_ix, row_contents in df.iterrows():\n    conditions_met = True\n    for condition in row_contents:\n        for operator in comparison_func:\n            if operator in condition:\n                animal, value = condition.split(operator)\n                break\n\n        is_true = comparison_func[operator]\n        if not is_true(int(dict2.get(animal,0)), int(value)):\n            conditions_met = False\n            break\n    if conditions_met:\n        valid_rows.append(row_ix)\nprint(valid_rows)\n\n[0]\n\n&gt;&gt;&gt; df['conditions_met'] = df.index.isin(valid_rows)\n&gt;&gt;&gt; df\n  condition1 Condition2 Condition3 Condition4  conditions_met\n0     duck&gt;1     goat&gt;2    sheep=0  chicken=0            True\n1     duck=0  chicken=0   donkey&gt;1    zebra&gt;0           False\n\n&gt;&gt;&gt; list2 = ['goat=3','duck=2']\n&gt;&gt;&gt; dict2 = dict(item.split('=') for item in list2)\n&gt;&gt;&gt; globals().update(dict2)\n\n&gt;&gt;&gt; def is_true(x):\n...     if '&gt;' in x:\n...         animal, value = x.split('&gt;')\n...         return int(globals().get(animal, 0)) &gt; int(value)\n...     elif '&lt;' in x:\n...         animal, value = x.split('&lt;')\n...         return int(globals().get(animal, 0)) &lt; int(value)\n...     else:\n...         animal, value = x.split('=')\n...         return int(globals().get(animal, 0)) == int(value)\n\n&gt;&gt;&gt; df.applymap(is_true)\n   condition1  Condition2  Condition3  Condition4\n0        True        True        True        True\n1       False        True       False       False\n\n&gt;&gt;&gt; df[df.applymap(is_true).all(1)].index\nInt64Index([0], dtype='int64')\n\n&gt;&gt;&gt; df['conditions_met'] = df.applymap(is_true).all(1)\n\n&gt;&gt;&gt; df\n  condition1 Condition2 Condition3 Condition4  conditions_met\n0     duck&gt;1     goat&gt;2    sheep=0  chicken=0            True\n1     duck=0  chicken=0   donkey&gt;1    zebra&gt;0           False\n\n# make a dict out of list2, there are other ways  \n# to get to the next step directly from list1\ndict2 = dict(item.split('=') for item in list2)\n\n# make a mapping of checker functions wrt operators\ncomparison_func = {'&gt;': lambda x,y: x &gt; y, \n                   '&lt;': lambda x,y: x &lt; y, \n                   '=': lambda x,y: x == y}\n\n# create an empty list to keep provision for the case\n# when conditions are matched in multiple rows\nvalid_rows = []\n\n# Iterate over the dataframe\nfor row_ix, row_contents in df.iterrows():\n\n    # set a flag for each row\n    conditions_met = True\n\n    # iterate through each column in the row\n    for condition in row_contents:\n\n        # check which operator is in the current column\n        for operator in comparison_func:\n\n            # if operator found, split by it, store the (animal, value) and break\n            if operator in condition:\n                animal, value = condition.split(operator)\n                break\n        # get the comparison function for the operator, name the function is_true\n        is_true = comparison_func[operator]\n\n        # check if the function evaluates to true with given values\n        # dict2.get(animal,0) will give the value for that animal from dict2\n        # if the animal is not in dict2 it will return default value 0\n        if not is_true(int(dict2.get(animal,0)), int(value)):\n            # if condition did not meet, set conditions_met False, break\n            conditions_met = False\n            break\n\n    # outside the loop, if the conditions_met stays True throughout the previous loop\n    # append the row index to the valid_rows\n    if conditions_met:\n        valid_rows.append(row_ix)\n\nprint(valid_rows)\n"
'import cv2\nimport glob\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D\nfrom keras.layers import Activation,Dropout,Flatten,Dense\nimport numpy\nimport os\n\nseed = 10\nnumpy.random.seed(seed)\nimage_b = [cv2.imread(file_b)for file_b in glob.glob ("C:/BC/BENI/*.png")]\nlabels = [name[name.find("_") + 1 : name.find("-")] \nfor name in glob.glob ("C:/BC/BENI/*.png")]\nlabeled_images = list(zip(image_b, labels))\nall_image =(\'image_b\')\nX = all_image[:] \nY = all_image[:]  \n'
'def trainPipeline(pipeline, X, y):\n    X_transformed = X\n    for name, step in pipeline.steps[:-1]:\n        X_transformed = step.fit_transform(X_transformed, y)\n    pipeline.steps[-1][1].fit(X_transformed, y)\n'
"#convert columns to datetimes if necessary\ndf[['start','end']] = df[['start','end']].apply(pd.to_datetime)\n#repeat datetimes to Series\ns = pd.concat([pd.Series(r.Index,pd.date_range(r.start, r.end)) \n                         for r in df.itertuples()])\n\n#repoeat values, remove end column and reaasign start by index values\ndf = df.loc[s].drop(['end'], axis=1).assign(start=s.index).reset_index(drop=True)\nprint (df)\n       start note  item\n0 2016-12-30    Z     1\n1 2016-12-31    Z     1\n2 2017-01-01    Z     1\n3 2017-01-02    Z     1\n4 2017-01-03    Z     1\n5 2017-09-10    W     2\n6 2017-09-11    W     2\n7 2017-09-12    W     2\n8 2017-09-13    W     2\n9 2017-09-14    W     2\n"
'selection.estimator_.score(x_test, y_test)\n'
"from sklearn.preprocessing import StandardScaler\n\nf = lambda x: (StandardScaler().fit_transform(x.to_frame()))[:, 0]\ndf = df.set_index('Condition').groupby('Condition').transform(f).reset_index()\nprint (df)\n    Condition   A_value   B_value\n0          15 -0.277350 -0.821995\n1          30 -1.460593  1.069045\n2          45 -0.240966 -0.904534\n3          15  0.277350  1.150793\n4          30 -0.365148  0.000000\n5          45  0.722897  0.301511\n6          15  1.386750 -1.150793\n7          30  0.730297 -1.603567\n8          45  1.044185 -0.904534\n9          15 -1.386750  0.821995\n10         30  1.095445  0.534522\n11         45 -1.526117  1.507557\n"
"!pip install pandas==0.21\nimport pandas as pd\nuser_by_movie = user_items.groupby(['user_id', 'movie_id'])['rating'].max().unstack()\n!pip install pandas\n"
"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import OrderedDict\n\ncolumns = ['StartStn','EndStn']\nstartstn = ['Leytonstone','Walthamstow','Earls Court','Stratford','Warren Street','Marble Arch','Colliers Wood',\n            'Earls Court','Tottenham Court Rd','Woodside Park']\nendstn = ['East Ham','Piccadilly Circus','Holborn','Shepherds Bush Und','Walthamstow Central','Northolt',\n          'Holborn','Marble Arch','Mile End','Hammersmith D']\ndf = pd.DataFrame(data={'StartStn':startstn,'EndStn':endstn})\nprint(df)\n\ndf['hue'] = 'Start'\ndf['Stations'] = df['StartStn']\ndf_start = df[['Stations','hue']]\ndf['hue'] = 'End'\ndf['Stations'] = df['EndStn']\ndf_end = df[['Stations','hue']]\n\norderstart = df['StartStn'].value_counts()\nstartstnlist = orderstart.index.tolist()\norderend = df['EndStn'].value_counts()\nendstnlist = orderend.index.tolist()\norder = startstnlist+endstnlist\norder = list(OrderedDict.fromkeys(order))\n\ndf_concatenated = pd.concat([df_start,df_end],ignore_index=True)\nsns.countplot(data=df_concatenated,x='Stations', order=order,hue='hue')\nplt.show()\n"
"pd.read_excel(filename, dtype={'Timeseries': str}, header=0)\n"
"grp = df['weekdays'].shift().eq('sunday').cumsum()\ndfs = {f'df{k+1}': g for k, g in df.groupby(grp)}\n\nprint(dfs['df1'])\n   values  weekdays\n0      10    Monday\n1       5   Tuesday\n2      30  Wednesay\n3      44  Thursday\n4      52    Friday\n5       6  saturday\n6       7    sunday\n\nprint(dfs['df2'])\n    values  weekdays\n7       85   Tuesday\n8        9  Wednesay\n9        1  Thursday\n10       1    Friday\n11       1  saturday\n12      13    sunday\n\nprint(dfs['df3'])\n    values   weekdays\n13      14     Monday\n14       1    Tuesday\n15      16  Wednesday\n"
"columns_map = (\n    ('Grubhub', 'On GrubHub or Seamless?'),\n    ('ToastTab', 'On ToastTab?'),\n    ('Tenk', 'On Tenk?'),\n    # etc ...\n)\n\nfor new_col, alias in columns_map:\n    df[new_col] = df[alias].apply(lambda x: x == 'Yes')\n    # also you can easily remove aliases columns:\n    # df = df.drop(columns=[alias])\n\nfor new_col, alias in columns_map:\n    df[alias] = df[alias].apply(lambda x: x == 'Yes')\n\ndf.rename(\n    columns={alias: new_col for new_col, alias in columns_map},\n    inplace=True\n)\n"
"df.assign(vid_fn=df['vid_fn'].map(my_func)).explode('vid_fn')\\\n  .rename(columns={'vid_fn': 'frame_fn'}).reset_index(drop=True)\n\n  frame_fn  V1  V2  V3\n0   a1.avi   1   4   5\n1   a2.avi   1   4   5\n2   a3.avi   1   4   5\n3   a4.avi   1   4   5\n4   a5.avi   1   4   5\n5   b1.avi   7   8   1\n6   b2.avi   7   8   1\n7   b3.avi   7   8   1\n8   b4.avi   7   8   1\n9   b5.avi   7   8   1\n"
"df['Sum'] = df['Number'] + df['Number'].shift(-1)\n\n   Number   Sum\n0       4  10.0\n1       6   8.0\n2       2   1.0\n3      -1   NaN\n"
"data1 = pd.DataFrame({'lat': [-0.659901, -0.659786, -0.659821], 'long':[2.530561, 2.530797, 2.530587],\n                      'd':[0.4202, 1.0957, 0.6309],\n                      'o_lat':[-37.8095,-37.8030,-37.8050], 'o_long':[145.0000,145.0077,145.0024]})\ndata2= pd.DataFrame({'nearest_warehouse': ['Nickolson','Thompson','Bakers'], \n       'lat':[-37.818595, -37.812673, -37.809996], 'lon':[144.969551, 144.947069, 144.995232],\n      'near_lat':[-37.8185,-37.8126,-37.8099], 'near_lon':[144.9695,144.9470,144.9952]})\n\ndata1['key'] = data1.apply(lambda x: ((x['o_lat'] - data2['near_lat']).abs()\n                                      + (x['o_long'] - data2['near_lon']).abs()).idxmin(), axis=1)\ndata1 = pd.merge(data1, data2[['nearest_warehouse']], how='left', left_on='key', right_index=True).drop('key', axis=1)\ndata1\nOut[1]: \n        lat      long       d    o_lat    o_long nearest_warehouse\n0 -0.659901  2.530561  0.4202 -37.8095  145.0000  Bakers          \n1 -0.659786  2.530797  1.0957 -37.8030  145.0077  Bakers          \n2 -0.659821  2.530587  0.6309 -37.8050  145.0024  Bakers\n\nimport matplotlib.pyplot as plt\ndata1 = pd.DataFrame({'o_lat':[-37.8095,-37.8030,-37.8050], 'o_long':[145.0000,145.0077,145.0024],\n                    'nearest_warehouse': ['0','1','2']})\ndata2= pd.DataFrame({'nearest_warehouse': ['Nickolson','Thompson','Bakers'],\n      'o_lat':[-37.8185,-37.8126,-37.8099], 'o_long':[144.9695,144.9470,144.9952]})\ndf = data1.append(data2)\n\ny = df['o_lat'].to_list()\nz = df['o_long'].to_list()\nn = df['nearest_warehouse'].to_list()\n\nfig, ax = plt.subplots()\nax.scatter(z, y)\n\nfor i, txt in enumerate(n):\n    ax.annotate(txt, (z[i], y[i]))\nplt.gca().set_aspect('equal', adjustable='box')\n"
"fig.update_layout(yaxis_tickformat='$',\n                  yaxis2_tickformat='$',\n                  yaxis3_tickformat='$',\n                  yaxis4_tickformat='$',\n                  height=750,\n                  width=1200,\n                  showlegend=False,\n                  title_text=Current_Stock_Profile.shortName)\n"
'df.idxmax(1).where(df.any(1))\n\n0    col_B\n1    col_A\n2    col_C\n3      NaN\ndtype: object\n'
"import lxml.html as lh\nimport requests\n\nlink = &quot;https://www.calendar-12.com/catholic_holidays/2019&quot;\nreq = requests.get(link)\n\ndoc= lh.fromstring(req.text)\ntab = doc.xpath('//table')[0]\nrows = []\nfor t in tab.xpath('.//tr[@class]//td/a'):\n    row = []\n    row.extend(t.text.strip().replace('day,','day,xxx').split('xxx'))\n    rows.extend(row)\n    \nfor day,date,holiday in zip(rows[0::3],rows[1::3],rows[2::3]):\n    print(day,date,holiday)\n    #EDIT:\n    #or to store these in variables:\n    a,b,c = day,date,holiday\n    print(a,b,c)\n\nTuesday,  January 1, 2019 Solemnity of Mary, Mother of God\nSunday,  January 6, 2019 Epiphany\nTuesday,  March 5, 2019 Shrove Tuesday (Mardi Gras)\nWednesday,  March 6, 2019 Ash Wednesday\n"
'tfp.distributions.JointDistributionSequential([\n        tfp.distributions.Categorical(probs=[0, 0, 1/2, 1/2]),\n        lambda c: tfp.distributions.Normal(tf.gather([0., 1, -10, 30], c), 1)\n    ])\n'
"import numpy as np\nimport librosa\n\ny, sr = librosa.load(librosa.ex('trumpet'))\ncent = librosa.feature.spectral_centroid(y=y, sr=sr, center=False)\navg_cent = np.mean(cent)\nprint(avg_cent)\n\n2618.004809523263\n"
'pd.DataFrame(timestamp, sensortype, sensorvalues, label)\n'
'&gt;&gt;&gt; from collections import Counter\n&gt;&gt;&gt; \n&gt;&gt;&gt; Counter(map(tuple,data))\nCounter({(0, 1): 2, (2, 3): 1, (0, 2): 1})\n&gt;&gt;&gt; Counter(map(tuple,data)).items()\n[((0, 1), 2), ((2, 3), 1), ((0, 2), 1)]\n'
"print (df.pivot(index='id', columns='w', values='t'))\nw    1    2   3\nid             \n0   54  147  12\n1    1    0   1\n\ndf1 = df.pivot(index='id', columns='w', values='t').reset_index()\ndf1.columns.name=None\nprint (df1)\n   id   1    2   3\n0   0  54  147  12\n1   1   1    0   1\n"
"def answer_three():\n    df = {}\n    df1=df[(df['Gold']&gt;0) &amp; (df['Gold.1']&gt;0)]\n    ....\n"
'import numpy as np\nimport matplotlib.pyplot as plt\n\ndef makeOwnHeatMap(x,y,bins):\n    #shift +/- for the axes labels and \n    xMin = float(int(min(x)))-0.5\n    xMax = float(int(max(x)))+0.5\n    yMin = float(int(min(y)))-0.5\n    yMax = float(int(max(y)))+0.5\n    yStep = float(yMax-yMin)/bins[0]\n    xStep = float(xMax-xMin)/bins[1]\n\n\n    downscaledGraph = np.zeros((bins[0],bins[1]))\n\n    #make heatmap\n    for i in range(0,len(y)):\n        curY = y[i] #current y-value from data\n        curX = x[i] #current x-value from data\n\n        yetY = 0 #current y compare value within a stepsize\n        yetX = 0 #current x compare value within a stepsize\n        cntY = 0 #counter y for matrix coordinates\n        cntX = 0 #counter x for matrix coodrinates\n        while (yetY &lt; curY-yMin):\n            yetY += yStep\n            cntY += 1\n\n        while (yetX &lt; curX-xMin):\n            yetX += xStep\n            cntX += 1\n\n        #ends up with incrementing 1 x too much\n        cntY -= 1\n        cntX -= 1\n\n        downscaledGraph[cntY,cntX] += 1\n\n\n    #make axes labels\n    xbar = []\n    ybar = []\n    thisY = yMin\n    while thisY &lt;= yMax:\n        ybar.append(thisY)\n        thisY += yStep\n\n    thisX = xMin\n    while thisX &lt;= xMax:\n        xbar.append(thisX)\n        thisX += xStep\n\n    #draw heatmap\n    xbar, ybar = np.meshgrid(xbar, ybar)\n    intensity = np.array(downscaledGraph)\n    plt.pcolormesh(xbar, ybar, intensity)\n    plt.show()\n\n\n    for i in range(0,bins[0]):\n        for j in range(0, bins[1]):\n            print downscaledGraph[i,j],"\\t",\n        print "|"\n    print "_______"\n\n0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     |\n0.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0     |\n1.0     0.0     12.0    0.0     0.0     0.0     0.0     0.0     |\n18.0    0.0     7.0     0.0     0.0     16.0    0.0     0.0     |\n8.0     0.0     7.0     0.0     0.0     10.0    0.0     1.0     |\n15.0    0.0     6.0     0.0     0.0     12.0    0.0     7.0     |\n0.0     0.0     3.0     0.0     0.0     3.0     0.0     6.0     |\n0.0     0.0     4.0     0.0     0.0     1.0     0.0     0.0     |\n0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     |\n0.0     0.0     2.0     0.0     0.0     0.0     0.0     0.0     |\n'
"df.loc['Store 1', 'Kevyn']\n"
'pd.DataFrame(df.values.reshape(-1, 3), columns=["Col"+str(i) for i in range(1,4)])\n\n#Col1   Col2   Col3\n#0  1      2      3\n#1  4      5      6\n#2  7      8      9\n#3  10    11     12\n'
'X_male[:,0][~np.isnan(X_male[:,0])].reshape(1, -1)\n\nimputer = imputer.fit(X_male[:,0].reshape(-1, 1))\nX_male[:,0] = imputer.transform(X_male[:,0].reshape(-1, 1)).reshape(-1)\n'
'spokenMean = features(spoken, np.mean)\nspokenMean = np.vstack(spokenMean[:]).astype(np.float32)\n\nresult = np.concatenate([spokenMean, written], axis=1)\n'
"df = pd.read_clipboard()\ndf['code'] = df.candletype.astype('category').cat.codes\n\n    open    close   candletype  code\n0   542 543 GREEN   0\n1   543 544 GREEN   0\n2   544 545 GREEN   0\n3   545 546 GREEN   0\n4   546 547 GREEN   0\n5   547 542 RED 1\n6   542 543 GREEN   0\n\n     NaN\n1    NaN\n2    NaN\n3    1.0\n4    1.0\n5    1.0\n6    0.0\n"
"bank_statement.columns.values[0] = 'Din'\n\nbank_statement = bank_statement.rename(columns={'Unnamed: 0':'Din'})\n\nIn [216]: df = pd.DataFrame(np.random.randn(3, 3), columns=list('abc'))\n\nIn [217]: df\nOut[217]:\n          a         b         c\n0 -0.972161 -0.484091 -0.289475\n1  1.081694  1.215217  0.241532\n2 -0.581193  0.691856  0.194182\n\nIn [218]: df.columns\nOut[218]: Index(['a', 'b', 'c'], dtype='object')\n\nIn [219]: df.columns.values[0] = 'Din'\n\nIn [220]: df.columns\nOut[220]: Index(['Din', 'b', 'c'], dtype='object')\n\nIn [221]: df['Din']\n...\nskipped\n...\nKeyError: 'Din'\n\nIn [222]: df['a']\nOut[222]:\n0   -0.972161\n1    1.081694\n2   -0.581193\nName: Din, dtype: float64\n\nIn [224]: df.columns = ['Din'] + df.columns.tolist()[1:]\n\nIn [225]: df.columns\nOut[225]: Index(['Din', 'b', 'c'], dtype='object')\n\nIn [226]: df['Din']\nOut[226]:\n0   -0.972161\n1    1.081694\n2   -0.581193\nName: Din, dtype: float64\n"
"d = {'content': 'count','Company': 'min','from_user_description': 'min'}\naccount_count = df.groupby('from_user_screen_name').agg(d)\n"
"df[['movie_title', 'year']] = df.title.str.extract('(.*)\\s\\((\\d+)', expand=True)\n\ndf = pd.DataFrame({'title': \n                  ['Toy Story (1995)', 'Jumanji (1995)', 'Grumpier Old Men (1995)',\n                   'Waiting to Exhale (1995)', 'Father of the Bride Part II (1995)', \n                   'Hello (Goodbye) (1995)'\n                  ]})\n\ndf[['movie_title', 'year']] = df.title.str.extract('(.*)\\s\\((\\d+)', expand=True)\n\n                                title                  movie_title  year\n0                    Toy Story (1995)                    Toy Story  1995\n1                      Jumanji (1995)                      Jumanji  1995\n2             Grumpier Old Men (1995)             Grumpier Old Men  1995\n3            Waiting to Exhale (1995)            Waiting to Exhale  1995\n4  Father of the Bride Part II (1995)  Father of the Bride Part II  1995\n5              Hello (Goodbye) (1995)              Hello (Goodbye)  1995\n"
'for idx, i in enumerate(prices):\n    s += i\n    sums.append(s)\n    if idx == n-1: # Added\n        ave = (sums[idx]) / n  # Added \n        ma.append(ave)  # Added\n    elif idx &gt;= n: # modified\n        ave = (sums[idx] - sums[idx-n]) / n\n        ma.append(ave)\n    else:\n        ma.append(None) # removed extra variable m\n\nif i &gt;= n+1:\n\nif idx &gt;= n+1:\n\nmoving_average([2,3,4,5,8,5,4,3,2,1], 3)\n\n[None, None, 3.0, 4.0, 5.666666666666667, 6.0, 5.666666666666667, 4.0, 3.0, 2.0]\n'
"df=pd.DataFrame({'a':[1,2],'b':[2,3],'c':[1,9]})\ntarget_col=['c'] # column 'c' is the target column here\nX=df[list(set(df.columns).difference(target_col))].values # X-&gt; features\nY=df[target_col].values # Y -&gt; target\n\ndata=df.values\nX=data[:,:2] # from column 1 upto second last column including all the rows\nY=data[:,2] # only last column(target) including all the rows\n"
'from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# First pass\n# some_generator can be anything which reads the data in batches\nfor data in some_generator:\n    scaler.partial_fit(data)\n\n    # View the updated mean and std variance at each batch\n    print(scaler.mean_)\n    print(scaler.var_)\n\n\n# Second pass\nfor data in some_generator:\n    scaled_data = scaler.transform(data)\n\n    # Do whatever you want with the scaled_data\n'
'&gt;&gt;&gt; df = pd.DataFrame([[1,2], [3,4], [5,6]])\n&gt;&gt;&gt; df\n   0  1\n0  1  2\n1  3  4\n2  5  6\n&gt;&gt;&gt; df.index = (df.index + 1)*10\n&gt;&gt;&gt; df\n    0  1\n10  1  2\n20  3  4\n30  5  6\n\n&gt;&gt;&gt; df[0] = (df[0] + 1)*10\n&gt;&gt;&gt; df\n     0  1\n10  20  2\n20  40  4\n30  60  6\n'
'from nltk import pos_tag\nfrom nltk import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlmtzr = WordNetLemmatizer()\naux_verbs = [\'be\']\n\ndef detect_passive_voice(pattern):\n    passive_voice = False\n\n    if len(pattern) &gt;= 3:\n        if pattern[0][1].startswith(\'V\'):\n            verb = lmtzr.lemmatize(pattern[0][0], \'v\')\n            if verb in aux_verbs:\n                if (pattern[1][1] == \'VBN\' or pattern[1][1] == \'VBD\') and pattern[-1][0] == \'by\':\n                    passive_voice = True\n\n            # past verb + by\n            elif (pattern[-2][1] == \'VBN\' or pattern[-2][1] == \'VBD\') and pattern[-1][0] == \'by\':\n                passive_voice = True\n\n        # past verb + by\n        elif (pattern[-2][1] == \'VBN\' or pattern[-2][1] == \'VBD\') and pattern[-1][0] == \'by\':\n                passive_voice = True\n\n    # past verb + by\n    elif len(pattern) &gt;= 2:\n        if (pattern[-2][1] == \'VBN\' or pattern[-2][1] == \'VBD\') and pattern[-1][0] == \'by\':\n            passive_voice = True\n\nreturn passive_voice\n\nIn [4]: tokens = word_tokenize("was bought by")\n   ...: tags = pos_tag(tokens)\n   ...: detect_passive_voice(tags)\nOut[4]: True\n\nIn [5]: tokens = word_tokenize("mailed the letter")\n   ...: tags = pos_tag(tokens)\n   ...: detect_passive_voice(tags)\nOut[5]: False\n\nIn [7]: tokens = word_tokenize("was mailed by")\n   ...: tags = pos_tag(tokens)\n   ...: detect_passive_voice(tags)\nOut[7]: True\n'
"for L,M in zip(laundry1['latitude'],laundry1['longitude']):\n    print('latitude:-')\n    print(L)\n    print('longitude:-')\n    print(M)\n"
"xdata = dataFrame_x['CRIM'][dataFrame_x['CRIM'] &lt; 1].values\nydata = dataFrame_y[dataFrame_x['CRIM'] &lt; 1].values.flatten()\n\nxmesh = np.linspace(min(xdata), max(xdata), 50)\n\nfit = np.poly1d(np.polyfit(xdata, ydata, 1))\n\nplt.plot(xdata, ydata, 'bo', label='Data')\nplt.plot(xmesh, fit(xmesh), '-b', label='Fit')\nplt.legend(fontsize=16)\nplt.xlabel('CRIM', fontsize=18)\nplt.ylabel('Target',fontsize=18)\n"
'df["duplicated"] = df[["account", "ins_date"]].duplicated(keep=False)\ndf = df[(df.ret_type == \'CTR\') | ~df["duplicated"]]\n'
"d = {'J7 2017': [' J730F', 'AMOLED'], 'J5 2017': ['J530', 'TFT']} \n\ndfs = {k: df[df['name'].str.contains('|'.join(v))] for k, v in d.items()}\n\nprint (dfs)\n\n{'J7 2017':                            name\n0  SCREEN SAMSUNG FULL AMOLED  \n1  SCREEN SAMSUNG J7 J730F 2017, 'J5 2017':   name\n2  WYŚWIETLACZ LCD + DIGITIZER SAMSUNG J5 2017 (J...\n3          3 colors SCREEN LCD SAMSUNG Galaxy J5 TFT}\n"
"s1=(df.one &gt;= 1) &amp; (df.two &lt;= 7)\ns2=(df.one == 1) &amp; (df.two &lt;= 11)\nl=[[ z for z in [x,y] if z != 0]for x , y in zip(s1*1,s2*2)]\ndf['State']=l\ndf\nOut[21]: \n   one  two  three   State\n0    1    5      9  [1, 2]\n1    2    6     10     [1]\n2    3    7     11     [1]\n3    4    8     12      []\n"
'import pandas as pd\nimport geopy\nfrom geopy.geocoders import Nominatim\n\n\nUSGS_df = pd.DataFrame(data = {\'latitude\':[64.7385, 61.116], \'longitude\':[-149.136, -138.655], \'place\':[\'17km N of North Nenana, Alaska\', \'74km WNW of Haines Junction, Canada\'], \'country\':[None, None]})\n\ngeopy.geocoders.options.default_user_agent = "locations-application"\ngeolocator=Nominatim(timeout=10) \n\n\n\nfor i, row in USGS_df.iterrows():\n    try:\n        lat = row[\'latitude\']\n        lon = row[\'longitude\']\n\n        location = geolocator.reverse(\'%s, %s\' %(lat, lon))\n        country = location.raw[\'address\'][\'country\']\n\n        print (\'Found: \' + location.address)\n\n        USGS_df.loc[i, \'country\'] = country\n\n    except:\n        print (\'Location not identified: %s, %s\' %(lat, lon))\n\nprint (USGS_df)\n   latitude  longitude                                place country\n0   64.7385   -149.136       17km N of North Nenana, Alaska    None\n1   61.1160   -138.655  74km WNW of Haines Junction, Canada    None\n\nprint (USGS_df)\n   latitude  longitude                                place country\n0   64.7385   -149.136       17km N of North Nenana, Alaska     USA\n1   61.1160   -138.655  74km WNW of Haines Junction, Canada  Canada\n'
'....\n....\n\ndef fit(self, X, y=None):\n    df = pd.concat([X, self.response], axis=1)\n    self.cols = df.columns[abs(df.corr()[df.columns[-1]]) &gt; self.threshold].drop(self.response.columns)\n    return self\ndef transform(self, X, y=None):\n    return X[self.cols]\n'
"Usercol ='Account' #user entry\nCommon = \nlist(set(df1.loc[:Usercol]).intersect(set(df2.loc[:Usercol])))\n#fetch index of each data frame using\ndf1[df1[Usercol].isin(Common)].index\ndf2[df2[Usercol].isin(Common)].index\n"
"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf0 = pd.read_excel('testdata.xlsx',header=None)\ndf0.head()\n\n    0   1   2   3   4   5   6   7   8   9   10  11  12\n0   sim1    time    1   2   3   4   5   6   7   8   9   10  11\n1   NaN     feat1   1   0   -1  0   1   0   -1  0   1   0   -1\n2   NaN     feat2   2   0   -2  0   2   0   -2  0   2   0   -2\n3   NaN     feat3   3   0   -3  0   3   0   -3  0   3   0   -3\n4   sim2    time    1   2   3   4   5   6   7   8   9   10  11\n\ndef get_data_numpy(df,j):\n    i = j * (nFeats+1)\n    t =  np.array(df.iloc[i,2:])\n    y0 = np.array(df.iloc[i+1,2:])\n    y1 = np.array(df.iloc[i+2,2:])\n    y2 = np.array(df.iloc[i+3,2:])\n    return t,y0,y1,y2\n\ndef get_data_pandas(df,j):\n    i = j * (nFeats+1)\n    t =  np.array(df.iloc[i,2:])\n    dfy = df.iloc[i+1:i+nFeats+1,2:]\n    return t,dfy\n\nnModels = 1                                         # run for 1 model\nnFeats  = 3\nfor jModel in range(nModels):\n    tn,y0,y1,y2 = get_data_numpy(df0,jModel)\n    tp,dfy      = get_data_pandas(df0,jModel)\n\n    #--- graphics ---\n    plt.style.use('fast')  \n    fig, ax0 = plt.subplots(figsize=(20,4))\n    plt.plot(tp,dfy.T, lw=4, alpha=0.4);           # plot pandas dfy with 1 command\n\n    plt.plot(tn,-y0,lw=6,ls='--')                   # plot each numpy time series\n    plt.plot(tn,-y1,lw=6,ls=':') \n    plt.plot(tn,-y2,lw=6,ls='-')\n    plt.show() \n\nfig.savefig('plot_model_1.png', transparency=True)  \n"
'// Register an update handler on the slider which:\n//  - Updates the "timeSelected" element\n//  - Calculates the new data for the time\n//  - sets the values into the heatmap and updates the "max" value\n//\n//  NB: if there is LOTS of data, it might be too slow to recalculate these\n//      on every change, in which case perhaps building the [lat, lng, val] arrays\n//      for each time up front might be a better idea\nslider.noUiSlider.on(\'update\', function (values, handle) {\n  var index = Number(values[0])\n\n  timeSelected.value = tabular[\'Time\'][index]\n  var dataRow = tabular[\'Data\'][index]\n   // play with this val for scaling, i.e. depends how zoomed you are etc\n  var max = Math.max.apply(null, dataRow) * 1.0\n  var heatValues = tabular[\'Locations\'].map((loc, idx) =&gt; {\n    return [].concat(locationCoords[loc]).concat(dataRow[idx])\n  })\n\n  heat.setOptions({max: max})\n  heat.setLatLngs(heatValues.concat(heatValues).concat(heatValues))\n\n});\n'
"import pandas as pd\n\nd = ({\n    'column_a' : ['North Europe','East Europe','West Europe'],  \n    'average_1' : [143, 100.755, 175.1],  \n    'average_2' : [297.9, 171.21, 227.55 ],                   \n    'column_b' : [79.265, 60.8078, 76.468],                                                                         \n    })\n\ndata_set = pd.DataFrame(d)\n\ncolumn_a = data_set['column_a']\ncolumn_b = data_set['column_b']\naverage_1 = data_set['average_1']\naverage_2 = data_set['average_2']\n\ndf = pd.DataFrame([column_a, column_b, average_1, average_2])\n\ndf = df.T\nprint(df)\n\n       column_a column_b average_1 average_2\n0  North Europe   79.265       143     297.9\n1   East Europe  60.8078   100.755    171.21\n2   West Europe   76.468     175.1    227.55\n\nimport pandas as pd\n\nd = ({\n    'column_a' : ['North Europe','East Europe','West Europe'],                    \n    'column_b' : [79.265, 60.8078, 76.468],                                                                         \n     })\n\ndata_set1 = pd.DataFrame(d)\n\nd = ({  \n    'average_1' : [143, 100.755, 175.1],  \n    'average_2' : [297.9, 171.21, 227.55],                                                                                          \n     })\n\ndata_set2 = pd.DataFrame(d)\n\ncolumn_a = data_set1['column_a']\ncolumn_b = data_set1['column_b']\naverage_1 = data_set2['average_1']\naverage_2 = data_set2['average_2']\n\ndf = pd.concat([column_a, column_b, average_1, average_2], axis = 1)\n\nprint(df)\n\n       column_a column_b average_1 average_2\n0  North Europe   79.265       143     297.9\n1   East Europe  60.8078   100.755    171.21\n2   West Europe   76.468     175.1    227.55\n"
"df = df[df.eq('wide').any(axis=1)]\n#alternative\n#df = df[(df == 'wide').any(axis=1)]\nprint (df)\n       1     2\n0   high  wide\n2  short  wide\n\nprint (df.eq('wide'))\n       1      2\n0  False   True\n1  False  False\n2  False   True\n3  False  False\n\nprint(df.eq('wide').any(axis=1))\n0     True\n1    False\n2     True\n3    False\ndtype: bool\n"
"df['Newcol']=(df.groupby('lat')['lat'].transform('count')/1250000)*100000\n\nz = ['lat'] \nfor i in z:\n    df[i+'col']=(df.groupby(i)[i].transform('count')/1250000)*100000\n"
'vals = [[0.6625, 0.6035714285714285], [0.7224999999999999, 0.6035714285714285], [0.7224999999999999, 0.6571428571428571], [0.6625, 0.6571428571428571], [0.6625, 0.6035714285714285]]\n\n[top_left, bottom_left, bottom_right, top_right, top_left]\n\n# dummy img sizes:\nimage_height = 480\nimage_width = 640\n\n# rescale to img dimensions, and convert to int, to allow slicing:\nbbox_coordinates = [[int(a[0]*image_height), int(a[1]* image_width)] for a in vals]\n\ntop_left = bbox_coordinates[0]\nbottom_right = boox_coordinates[2]\n\nbbox = img[top_left[0]:bottom_right[0], top_left[1]:bottom_right[1]]\n'
"df_new['difference'] = df_new['time'].dt.total_seconds().div(60).astype(float)\n"
'import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.set(style="white", context="talk")\n\nd1 =[\'Age\',\'q1\',\'q2\']  #have taken count as q2\ndata= [\n        [\'10+\',\'No\',16],\n        [\'10+\',\'No Answer\',1],\n        [\'10+\',\'Yes\',8],\n        [\'5-7\',\'No\',20],\n        [\'5-7\',\'No Answer\',1],\n        [\'5-7\',\'Yes\',22],\n        [\'7-9\',\'No\',18],\n        [\'7-9\',\'No Answer\',1],\n        [\'7-9\',\'Yes\',16],\n        [\'Under5\',\'No\',11,]\n       ]\n\ndf =pd.DataFrame(data,columns=d1)\n\nsns.catplot(x="Age", y="q2", hue="q1", data=df,\n                height=6, kind="bar", palette="muted")\n\nplt.show()\n'
"import pandas as pd\nimport os\n\n# Read in the data\nground_truth = pd.read_csv('GroundTruth.csv')\n\n# Loop through the DataFrame created from the csv file\nfor row in ground_truth.iterrows():\n    image_name = row[1].image\n\n    # Skip the first column, which is not one hot encoded\n    target_folder = row[1].index[row[1].values[1:].argmax() + 1]\n    if not os.path.exists(target_folder):\n        # Create the folder\n        os.makedirs(target_folder)\n    # Move the file\n    os.rename(f'./{image_name}.jpg',\n              f'./{target_folder}/{image_name}.jpg')\n"
'%timeit df.loc[:, np.in1d(df.columns, col_of_interest)]\n#\xa0493 µs ± 2.59 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n%timeit df.loc[:, set(df.columns).intersection(col_of_interest)]\n# 915 µs ± 65.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n'
'# Plot the feature importances of the forest\nplt.figure(figsize=(18,9))\nplt.title("Feature importances")\nn=20\n_ = plt.bar(range(n), importances[indices][:n], color="r", yerr=std[indices][:n])\nplt.xticks(range(n), indices)\nplt.xlim([-1, n])\nplt.show()\n'
"df = data.assign(largest=data.max(axis=1), name=data.idxmax(axis=1))\nprint (df)\n   A1  A2  A3  A99  largest name\n0   3   4   6   11       11  A99\n1   1   8   2    1        8   A2\n\ndata[['largest','name']] = data.agg(['max','idxmax'], 1)\nprint (data)\n   A1  A2  A3  A99 largest name\n0   3   4   6   11      11  A99\n1   1   8   2    1       8   A2\n\ndf1 = data.select_dtypes(np.number)\n\ndf1 = data.astype(int)\n\ndf1 = data.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n\ndf = data.assign(largest=df1.max(axis=1), name=df1.idxmax(axis=1))\n"
"df['rank'] = df.groupby('ticketID').apply(lambda x:\n                                          pd.Series(range(len(x)))).values\nresul = df.pivot('ticketID', 'rank').fillna('')\nresul.columns = resul.columns.swaplevel()\nresul.sort_index(axis=1,inplace=True, level=0, sort_remaining=False)\nresul.columns = ['{1}_{0}'.format(*c) for c in resul.columns]\nresul.reset_index(inplace=True)\n\n   ticketID             ChangeDate_0 OldStatus_0                  NewStatus_0             ChangeDate_1                  OldStatus_1                        NewStatus_1             ChangeDate_2                        OldStatus_2     NewStatus_2\n0   1012327  2019-03-18 09:00:32.903      R or O   Action mail sent to client  2019-03-18 09:21:34.820   Action mail sent to client                Response Client - R  2019-03-18 09:34:21.890                Response Client - R  Status Updated\n1   1012328  2019-03-18 07:00:09.960      R or O         ticket Closed - None  2019-03-18 07:09:31.420         ticket Closed - None                     Status Updated                                                                            \n2   1012329  2019-03-18 06:52:03.490      R or O    ticket Closed - Satisfied  2019-03-18 07:09:33.433    ticket Closed - Satisfied                     Status Updated                                                                            \n3   1012330  2019-03-18 10:25:13.493      R or O  Action mail sent to Service  2019-03-18 10:55:20.963  Action mail sent to Service  ticket Closed - Service Responded  2019-03-18 11:02:05.327  ticket Closed - Service Responded  Status Updated\n4   1012332  2019-03-18 09:00:41.967      R or O   Action mail sent to client  2019-03-18 10:24:20.150   Action mail sent to client                Response Client - R  2019-03-18 10:32:40.717                Response Client - R  Status Updated\n"
"x = 'g8h.|$hTdo+jC9^@'    \n(cars.fillna(x).groupby(['name','hp','color']).size().reset_index()\n               .rename(columns={0 : 'count'}).replace(x,np.NaN))\n            name         hp     color   count\n    0   Batmobile       900.0   NaN     1\n    1   Batmobile       1000.0  NaN     1\n    2   Bike            NaN     blue    2\n    3   Toyota Corolla  120.0   red     1\n"
"grps = df.groupby('Customer No')['Report Date']    \ndf['Age in Days'] = (grps.transform('last') - grps.transform('first')).dt.days\n\n  Report Date Invoice No  Customer No Amount  Age in Days\n0  2018-08-14          A            1    50$            1\n1  2018-08-14          B            1   100$            1\n2  2018-08-14          C            2    75$            2\n3  2018-08-15          A            1    20$            1\n4  2018-08-15          B            1    45$            1\n5  2018-08-15          C            2    70$            2\n6  2018-08-16          C            2    40$            2\n7  2018-08-16          D            3   100$            0\n8  2018-08-16          E            3    60$            0\n"
"mycolumns = data.columns\nnumerical_columns = data._get_numeric_data().columns\ncat_features= list(set(mycolumns) - set(numerical_columns))\n\ncat_features = df.select_dtypes(['object']).columns\n\n  # Define function\n  def mean_encoding(df, cols, target):\n\n     for c in cols:\n        means = df.groupby(c)[target].mean()\n        df[c].map(means)\n\n    return df\n\n# Encode\ndata = mean_encoding(data, cat_features, target)\n"
"plt.figure(figsize=(8,8))\nax = plt.subplot(aspect='equal')\ncmap = cm.get_cmap('jet')\n\n\nplt.tripcolor(x, y, z, cmap=cmap, shading='flat'); # use shading='gouraud' to smooth\nax.plot(x, y, '.', color='red', label='data points');\n\nearth_circle = plt.Circle((EARTH_RADIUS, EARTH_RADIUS), EARTH_RADIUS,\n                           edgecolor='black', fill=False, linewidth=1);\n\nax.add_artist(earth_circle);\nax.set_xlabel('x (m)'); ax.set_ylabel('y (m)');\ncbar = plt.colorbar();\ncbar.set_label('z')\nax.legend();\n\n# Get Values for griddata plot\n# Use griddata interpolation\nEARTH_GRID_RES_KM = 5*100000 # changed! to emphasis what is really plotted\nx_solar_interp = np.arange(0, EARTH_RADIUS + CUT_OFF_VAL, EARTH_GRID_RES_KM)\ny_solar_interp = np.arange(0, EARTH_RADIUS*2, EARTH_GRID_RES_KM)\nxx_interp, yy_interp = np.meshgrid(x_solar_interp, y_solar_interp)\n\nz_interp = interpolate.griddata((x, y), z, (xx_interp, yy_interp),\n                                method='linear', fill_value=np.nan)\n\n# Graph\nplt.figure(figsize=(8,8))\nax = plt.subplot(aspect='equal')\n\ncmap = cm.get_cmap('jet')\ncmap.set_bad(color='white')\n\nplt.pcolormesh(xx_interp, yy_interp, z_interp, cmap=cmap,\n               shading='flat'); # try shading='gouraud'\n# note about pcolormesh dealing with NaN: https://stackoverflow.com/a/33667896/8069403\n\nearth_circle = plt.Circle((EARTH_RADIUS, EARTH_RADIUS), EARTH_RADIUS,\n                           edgecolor='black', fill=False, linewidth=1);\nax.add_artist(earth_circle);\n\nax.plot(xx_interp.flatten(), yy_interp.flatten(), '.',\n        color='black', label='data points');\n\nax.set_xlabel('x (m)'); ax.set_ylabel('y (m)');\ncbar = plt.colorbar(cmap=cmap);\ncbar.set_label('z')\nax.legend();\n"
"df3['new'] = ((pd.to_timedelta(df3.Time.astype(str))- \n               pd.to_timedelta(df3.Date.dt.strftime('%H:%M:%S'))) /\n                 np.timedelta64(1, 'm'))\nprint (df3)\n                     Date          Time        new\n0 2019-07-23 21:17:47.599  22:00:00.000  42.216667\n1 2019-07-23 21:11:46.973  21:50:00.000  38.233333\n"
"df.isnull().sum(axis=1).value_counts().to_frame()\n\ndf=pd.DataFrame()\ndf['col1']=[np.nan,1,3,5,np.nan]\ndf['col2']=[2,np.nan,np.nan,3,6]\ndf['col3']=[1,3,np.nan,4,np.nan]\nprint(df)\nprint(df.isnull().sum(axis=1))\nprint(df.isnull().sum(axis=0))\n\n\n   col1  col2  col3\n0   NaN   2.0   1.0\n1   1.0   NaN   3.0\n2   3.0   NaN   NaN\n3   5.0   3.0   4.0\n4   NaN   6.0   NaN\n0    1\n1    1\n2    2\n3    0\n4    2\ndtype: int64\ncol1    2\ncol2    2\ncol3    2\ndtype: int64\n\ndf.isnull().sum(axis=1).value_counts().to_frame()\n\n   0\n2  2\n1  2\n0  1\n"
"df = pd.DataFrame({'Shift': ['AM', 'PM', 'AM', 'PM', 'Both'], 'Valid':['YES','NO','YES','NO','YES'],'Amount': [3, 8, 4, 5, 100]})\n\n  Shift Valid  Amount\n0    AM   YES       3\n1    PM    NO       8\n2    AM   YES       4\n3    PM    NO       5\n4  Both   YES     100\n\ndf.loc[(df['Shift'] == 'AM') &amp; (df['Valid'] == 'YES')]['Amount'].sum()\n# output: 7\n\ndf.groupby(['Valid','Shift'])['Amount'].sum()\n\nValid  Shift\nNO     PM        13\nYES    AM         7\n       Both     100\nName: Amount, dtype: int64\n"
"df = pd.DataFrame({'A': [4, 3, 1, 7], \n                   'B': [10, 11, 12, 13], \n                   'C': ['B1', 'B2', 'B3', 'B4'],\n                   'D': [True, False, True, True]})\n\nidx = range(df.A.min(), df.A.max()+1)  # All 'A' values you want to represent\ndf.set_index('A').reindex(idx).reset_index()\n#   A     B    C      D\n#0  1  12.0   B3   True\n#1  2   NaN  NaN    NaN\n#2  3  11.0   B2  False\n#3  4  10.0   B1   True\n#4  5   NaN  NaN    NaN\n#5  6   NaN  NaN    NaN\n#6  7  13.0   B4   True\n\n(df.set_index('A')\n   .join(pd.DataFrame(index=pd.Index(idx, name='A')), how='outer')\n   .reset_index())\n"
"df = TF[TF.index.isin(IG['Term'])].rename_axis('Term').reset_index()\nprint (df)\n       Term  A  B  A+B\n0     alqur  0  1    1\n1        an  0  1    1\n2  ayatayat  0  1    1\n\ndf = IG[['Term']].merge(TF, left_on='Term', right_index=True)\nprint (df)\n       Term  A  B  A+B\n0     alqur  0  1    1\n1        an  0  1    1\n2  ayatayat  0  1    1\n"
"# Get X Values\nX = {x for x in df['x1']}\nX.update({x for x in df['x2']})\nX.update({min(X)-0.25, max(X)+0.25})\nX = pd.Series(list(X)).sort_values()\n\n\nfig, ax = plt.subplots(1,1, figsize=(10, 4))\n\n# Plot the backgrounds first to avoid overwriting the colors\nax.fill_between(X,-1*X,X.min(), color='blue', alpha=0.7)\nax.fill_between(-X,X.max(),(X), color='red', alpha=0.7)\n\nax.scatter(x = 'x1', y='x2', data = df[df['y']==1], color='red')\nax.scatter(x = 'x1', y='x2', data = df[df['y']==0], color='blue')\nplt.show()\n"
"Zone2_table = pd.crosstab(pd.to_datetime(Zone2['SAMPLED_DATE']).dt.strftime('%Y-%m'), Zone2['FORMATTED_ENTRY'])\n"
'dfchunk                                                                                                              \n   userId  movieId  ratings\n0      10      500      3.5\n1      20      500      4.5\n2      10      800      2.0\n3      20      800      5.0\n4      10      700      4.0\n5      20      700      1.5\n\nr=dfchunk.pivot(index="movieId",columns="userId")                                                                    \n\n        ratings     \nuserId       10   20\nmovieId             \n500         3.5  4.5\n700         4.0  1.5\n800         2.0  5.0\n\nr.columns=["u1","u2"]                                                                                                \n\nr["drate"]=r.u1.sub(r.u2).abs()                                                                                      \n\n          u1   u2  drate\nmovieId                 \n500      3.5  4.5    1.0\n700      4.0  1.5    2.5\n800      2.0  5.0    3.0\n\nv= r.drate.iloc[:-1].mean()-r.drate.iloc[-1]                                                                            \n-1.25\n\nif v&lt;0:\n   negative_counter.append(v)\nelse:\n   positive_counter.append(v)\n'
"c = ['Error Code ID', 'Error Code']\ndf[c] = df[c].ffill()\ndf = df.groupby(c).agg(' '.join).reset_index()\n\ndf = df.dropna(subset=['Description of  Error Code'])\nc = ['Error Code ID', 'Error Code']\ndf[c] = df[c].ffill()\ndf = df.groupby(c).agg(' '.join).reset_index()\n\nprint (df)\n                 Error Code ID Error Code  \\\n0               OE_ATO_IN_OPEN      16169   \n1  STR_PRO_PARTIVIPANT_INVALID      16233   \n2     TRD_CONT_MOD_NOT_ALLOWED      16231   \n3                e$dup_request      16198   \n\n                           Description of Error Code  \n0  Order priced ATO cannot be entered when a secu...  \n1                Proprietary requests cannot be made  \n2                          Continuous session trades  \n3  Duplicate modification or cancellation request...  \n"
"result = (test3[test3['$rep'].ne(3)].groupby(['$ID','$orient','$direct'],as_index=False)['$slope','$intercept']\n                                    .mean())\n\n#    $ID $orient $direct       $slope  $intercept\n#0  9119    fall     15f  2890.488050       185.5\n#1  9119  stance       0  2860.914230        21.0\n#2  9119  stance     15b  2872.814030       239.5\n#3  9120    fall     15f  1478.545675       185.0\n#4  9120  stance       0  1470.258800        85.0\n#5  9120  stance     15b  1452.504660       338.5\n\n(test3mean.drop(['$slope','$intercept'],axis=1)\n          .merge(result,on=['$ID','$orient','$direct'],how='left'))\n\n    $ID  $spec $orient $direct       $slope  $intercept\n0  9119      1  stance     15b  2872.814030       239.5\n1  9119      2  stance       0  2860.914230        21.0\n2  9119      3    fall     15f  2890.488050       185.5\n3  9120      1  stance     15b  1452.504660       338.5\n4  9120      2  stance       0  1470.258800        85.0\n5  9120      3    fall     15f  1478.545675       185.0\n"
'from io import StringIO\nimport pandas as pd    \n\ntxt = \'\'\'    "Interactor A"    "Interactor B"    "Interaction Score"   "score2"\n0   P02574  P39205  0.928736    0.375000\n1   P02574  Q6NR18  0.297354    0.166667\n2   P02574  Q7KML4  0.297354    0.142857\n3   P02574  Q9BP34  0.297354    0.166667\n4   P02574  Q9BP35  0.297354    0.16666\'\'\'\n\ndata = pd.read_csv(StringIO(txt), sep="\\s+")\n\n# FOR id FIELD\ndata["id"] = data.index\n\n# FOR STUB NAMES\ndata = data.rename(columns={"Interaction Score": "score A",\n                            "score2": "score B"})\n\ndf_long = pd.wide_to_long(data, ["Interactor", "score"], i="id", \n                           j="score_type", sep=" ", suffix="(A|B)")\n\ndf_long\n#               Interactor     score\n# id score_type                     \n# 0  A              P02574  0.928736\n# 1  A              P02574  0.297354\n# 2  A              P02574  0.297354\n# 3  A              P02574  0.297354\n# 4  A              P02574  0.297354\n# 0  B              P39205  0.375000\n# 1  B              Q6NR18  0.166667\n# 2  B              Q7KML4  0.142857\n# 3  B              Q9BP34  0.166667\n# 4  B              Q9BP35  0.166660\n\ndf_long.groupby(["Interactor"])["score"].agg(["count", "mean"])\n\n#            count      mean\n# Interactor\n# P02574         5  0.423630\n# P39205         1  0.375000\n# Q6NR18         1  0.166667\n# Q7KML4         1  0.142857\n# Q9BP34         1  0.166667\n# Q9BP35         1  0.166660\n\ndf_long.groupby(["Interactor", "score_type"])[\'score\'].agg(["count", "mean"])\n\n#                        count      mean\n# Interactor score_type                 \n# P02574     A               5  0.423630\n# P39205     B               1  0.375000\n# Q6NR18     B               1  0.166667\n# Q7KML4     B               1  0.142857\n# Q9BP34     B               1  0.166667\n# Q9BP35     B               1  0.166660\n\ndf_long.pivot_table(index="Interactor", columns="score_type", values=\'score\',\n                    aggfunc = ["count", "mean"])\n\n#            count          mean          \n# score_type     A    B        A         B\n# Interactor                              \n# P02574       5.0  NaN  0.42363       NaN\n# P39205       NaN  1.0      NaN  0.375000\n# Q6NR18       NaN  1.0      NaN  0.166667\n# Q7KML4       NaN  1.0      NaN  0.142857\n# Q9BP34       NaN  1.0      NaN  0.166667\n# Q9BP35       NaN  1.0      NaN  0.166660\n'
'data = (((8, 8), (8, 8), (8, 8), (8, 8), (8, 8)),\n  ((6, 7), (7, 7), (7, 7), (7, 6), (6, 7)),\n  ((2, 12), (12, 3), (3, 4), (4, 12), (12, 12)))\n\nuniques = []\n\nfor col in data:\n  for unique in list(set(col)):\n    uniques.append(unique)\n\nfor x in uniques:\n  print(x)\n\ndata = (((8, 8), (8, 8), (8, 8), (8, 8), (8, 8)),\n  ((6, 7), (7, 7), (7, 7), (7, 6), (6, 7)),\n  ((2, 12), (12, 3), (3, 4), (4, 12), (12, 12)))\n\nuniques = []\n\nfor col in data:\n  uniques += [unique for unique in list(set(col))]\n\nfor x in uniques:\n  print(x)\n'
'#1.2            \n22215   2       \nName    Description Tumor_One   Normal_One\n1007_s_at   na  -0.214548   -0.18069\n1053_at "RFC2 : replication factor C (activator 1) 2, 40kDa |@RFC2|"    0.868853    -1.330921\n117_at  na  1.124814    0.933021\n121_at  PAX8 : paired box gene 8 |@PAX8|    -0.825381   0.102078\n1255_g_at   GUCA1A : guanylate cyclase activator 1A (retina) |@GUCA1A|  -0.734896   -0.184104\n1294_at UBE1L : ubiquitin-activating enzyme E1-like |@UBE1L|    -0.366741   -1.209838\n1316_at "THRA : thyroid hormone receptor, alpha (erythroblastic leukemia viral (v-erb-a) oncogene homolog, avian) |@THRA|"  -0.126108   1.486972\n1320_at "PTPN21 : protein tyrosine phosphatase, non-receptor type 21 |@PTPN21|" 3.083681    -0.086705\n...\n\nfrom cmapPy.pandasGEXpress.parse_gct import parse\nfrom cmapPy.pandasGEXpress.write_gct import write\n\ndata = parse(\'example.gct\', rid=[\'1007_s_at\', \'1053_at\',\n                                 \'117_at\', \'121_at\',\n                                 \'1255_g_at\', \'1294_at  UBE1L\'])\n# remove nan values from row_metadata (description column)\ndata.row_metadata_df.dropna(inplace=True)\n# remove the entries of .data_df where nan values are in row_metadata\ndata.data_df = data.data_df.loc[data.row_metadata_df.index]\n\n# Can only write GCT version 1.3\nwrite(data, \'new_example.gct\')\n\n#1.3\n3   2   1   0\nid  Description Tumor_One   Normal_One\n\n1053_at RFC2 : replication factor C (activator 1) 2, 40kDa |@RFC2|  0.8689  -1.3309\n\n121_at  PAX8 : paired box gene 8 |@PAX8|    -0.8254 0.1021\n\n1255_g_at   GUCA1A : guanylate cyclase activator 1A (retina) |@GUCA1A|  -0.7349 -0.1841\n'
"x = dfCardio.drop('cardio',axis = 1, inplace=False)\n"
"df[df.loc[:,'P_1':'P_90'].isin(list())].dropna(how='all')\n\ndf[df[df.loc[:,'P_1':'P_90'].isin(list())].dropna(how='all').index]\n"
"import requests\nimport pandas as pd\nurl = 'https://www.mohfw.gov.in/'\nhtml = requests.get(url).content\ndf_list = pd.read_html(html)\ndf = df_list[-1]\nprint(df)\n"
'missing_values = np.nan\n'
"dist(A', B') = sqrt((2 * a1- 2 * b1)^2 + (2 * a2 - 2 *b2)^2 + ... + (2 * ad - 2 * bd)^2) = \n2 * sqrt((a1-b1)^2 + (a2-b2)^2 + ... + (ad - bd)^2) = 2 * dist(A,B)\n"
"{column0:{index0:value0, index1: value1, ...}, ...}\n\nIn [22]: import pandas as pd\n\nIn [23]: data ={\n    ...:     'prices': [[1367107200000, 135.3], [1367193600000, 141.96]],\n    ...:     'market_caps': [[1367107200000, 1500517590], [1367193600000, 1575032004.0]],\n    ...:     'total_volumes': [[1367107200000, 0], [1367193600000, 0.0]]\n    ...: }\n    ...:\n\nIn [24]: pd.DataFrame({k:dict(v) for k,v in data.items()})\nOut[24]:\n               prices   market_caps  total_volumes\n1367107200000  135.30  1.500518e+09            0.0\n1367193600000  141.96  1.575032e+09            0.0\n\nIn [26]: df.set_index(pd.to_datetime(df.index,unit='ms'))\nOut[26]:\n            prices   market_caps  total_volumes\n2013-04-28  135.30  1.500518e+09            0.0\n2013-04-29  141.96  1.575032e+09            0.0\n\nIn [28]: from datetime import datetime\n    ...: pd.DataFrame({\n    ...:     k:{datetime.fromtimestamp(x/1000): y for x,y in v}\n    ...:     for k,v in data.items()\n    ...: })\nOut[28]:\n                     prices   market_caps  total_volumes\n2013-04-27 17:00:00  135.30  1.500518e+09            0.0\n2013-04-28 17:00:00  141.96  1.575032e+09            0.0\n"
'df.apply(lambda x: x.where(x.between(*(my_dict[x.name])) ) )\n\n   TriGly   Age  Chol\n0     1.0  10.0   NaN\n1     0.0  12.0   0.4\n2     NaN   NaN   0.9\n'
'rows = [idx for idx in df.index if idx[1][-1]==\'%\']\ndf.style.format("{:.2%}",subset=(rows,pd.IndexSlice[:]))\n# equivalently\n# df.style.format("{:.2%}",subset=(rows,df.columns))\n\n                   0        1              2    3\nB   sum            0        1              2    3\n     d%      400.00%     500.00%     600.00%    700.00%\nC   sum            8           9          10    11\n     d%     1200.00%    1300.00%    1400.00%    1500.00%\n'
'text1 = """30881 EKLUTNA LAKE RD\nCHUGIAK, AK 99567\n(61.4478, -149.3136)"""\n\ntext2 = """30882 FAKE STR\nCHUGIAK, AK 98817\n(43.4478, -119.3136)"""\n\nd = {\'col1\': [text1, text2]}\n\ndf = pd.DataFrame(data=d)\n\ndef fix(row):\n  #We split the text by newline\n  address, cp, latlong =  row.col1.split(\'\\n\')\n\n  #We get the latitude and longitude by splitting by a comma\n  latlong_vec = latlong[1:-1].split(\',\')\n\n  #This part isn\'t really necessary but we create the variables for claity\n  lat = float(latlong_vec[0])\n  long = float(latlong_vec[1])\n\n  return pd.Series([address + ". " + cp, lat, long])\n\n\ndf[[\'full address\', \'lat\', \'long\']] = df.apply(fix, axis = 1)\n\ndf[\'full address\']\n0    30881 EKLUTNA LAKE RD. CHUGIAK, AK 99567\n1           30882 FAKE STR. CHUGIAK, AK 98817\n\ndf[\'lat\']\n\n0    61.4478\n1    43.4478\nName: lat, dtype: float64\n\ndf[\'long\']\n\n0   -149.3136\n1   -119.3136\nName: long, dtype: float64\n'
'gradient = (-2/N)*tot\n\ngradient = (-2)*tot\n\nm, b = gradient_descent(FG_pct, W_L_pct, 6, -1, 0.003, 10000)\nm = 6.465\nb = -2.44\n\nfrom matplotlib.pyplot import plot, scatter\nimport numpy as np\n\nY = np.array(W_L_pct)\nX = np.array([np.ones(len(FG_pct)), FG_pct]).reshape(2, 270).T\n\nA = np.linalg.inv(np.matmul(X.T, X))\nB = np.matmul(X.T, Y)\n\nbeta = np.matmul(A, B)\nm, b = beta[1], beta[0]\nprint(m, b)\nr = np.arange(0.4, 0.52, 0.01)\nscatter(FG_pct, Y)\nplot(r, m * r + b)\n'
"x = np.array([[1,2],\n              [3,4]])\n\nx_norm_rows_axis = tf.keras.utils.normalize(x, axis= 0)\nprint(x_norm_rows_axis)\n\n[[0.31622777 0.4472136 ]\n [0.9486833  0.89442719]]\n\nprint('x_norm_rows_axis[0][0] = {}'.format(1/np.sqrt(1 ** 2 + 3 ** 2)))\nprint('x_norm_rows_axis[0][1] = {}'.format(2/np.sqrt(2 ** 2 + 4 ** 2)))\nprint('x_norm_rows_axis[1][0] = {}'.format(3/np.sqrt(1 ** 2 + 3 ** 2)))\nprint('x_norm_rows_axis[1][1] = {}'.format(4/np.sqrt(2 ** 2 + 4 ** 2)))\n\nx_norm_rows_axis[0][0] = 0.31622776601683794\nx_norm_rows_axis[0][1] = 0.4472135954999579\nx_norm_rows_axis[1][0] = 0.9486832980505138\nx_norm_rows_axis[1][1] = 0.8944271909999159\n\nx_norm_col_axis = tf.keras.utils.normalize(x, axis= 1)\nprint(x_norm_col_axis)\n\n[[0.4472136  0.89442719]\n [0.6        0.8       ]]\n\nprint('x_norm_col_axis[0][0] = {}'.format(1/np.sqrt(1 ** 2 + 2 ** 2)))\nprint('x_norm_col_axis[0][1] = {}'.format(2/np.sqrt(2 ** 2 + 1 ** 2)))\nprint('x_norm_col_axis[1][0] = {}'.format(3/np.sqrt(4 ** 2 + 3 ** 2)))\nprint('x_norm_col_axis[1][1] = {}'.format(4/np.sqrt(3 ** 2 + 4 ** 2)))\n\nx_norm_col_axis[0][0] = 0.4472135954999579\nx_norm_col_axis[0][1] = 0.8944271909999159\nx_norm_col_axis[1][0] = 0.6\nx_norm_col_axis[1][1] = 0.8\n"
'from scipy.stats import beta\nfrom numpy import histogram\nimport pylab\n\nmax_time = 3\nmin_time = 0.5\n\na, b = 2, 7\ndist = beta(a, b)\n\nfor _ in range(10):\n    sample = min_time + dist.rvs(size=1000) * (max_time - min_time)\n    his, bins = histogram(sample, bins=20, density=True)\n    pylab.plot(bins[:-1], his, ".")\npylab.xlabel("Reaction time [s]")\npylab.ylabel("Probability density [1/s]")\npylab.grid()\npylab.show()\n'
"source = {\n    'year': ['2001-01-01', '2001-01-02', '2001-01-01', '2001-01-02'],\n    'source': ['Fossil Fuels', 'Fossil Fuels', 'Nuclear Energy', 'Nuclear Energy'],\n    'net_generation': [100, 500, 200, 400]\n}\nsource = pd.DataFrame(source)\n\nchart = alt.Chart(source).mark_area(opacity=0.3).encode(\n    x=&quot;year:T&quot;,\n    y=alt.Y(&quot;net_generation:Q&quot;, stack=None),\n    color=&quot;source:N&quot;\n)\n"
"df = pd.melt(frame=data, id_vars=data.columns[:3], value_name='State',var_name='Stage')  \n\ndf.State = df.State.str.split()\ndf = df.explode('State')\n\ndf.dropna(subset = [&quot;State&quot;], inplace=True)\n\n"
"bins = [1, 10, 20, 34, 100]\nlabels = ['1-10', '11-20', '21-34', '35+']\n"
"s = (df[df['sportPlayed'].eq('Basketball')].groupby('personID').date.min()\n      .lt(df[df['sportPlayed'].eq('Football')].groupby('personID').date.max()))\n\n#personID\n#1     True\n#3     True\n#4    False\n#Name: date, dtype: bool\n\ns[s].index\n#Int64Index([1, 3], dtype='int64', name='personID')\n"
'conds = [arr&gt;1, arr&lt;1]\ntarget = np.full(arr.shape, value_array)\nnp.select(conds, [target, target.T], arr)\n\narray([[  1.,   1., 200.,  10.],\n       [  1.,   1., 200.,  10.],\n       [200., 200.,   1.,  10.],\n       [ 10.,  10.,  10.,   1.]])\n'
"columns = [['att1', 'att2']]\n\nMultiIndex([('att1',),\n            ('att2',),\n            ( 'idx',)],\n           )\n\nA.set_index(('idx',))\n\n       att1 att2\n(idx,)          \na         1    2\nb         1    3\nc         4    6\n\ncolumns = ['att1', 'att2']\n"
'X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\nX_scaled = X_std * (max - min) + min\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\nscaler = MinMaxScaler()\n\ndata = [[1, 2], [3, 4], [4, 5]]\n\n# min to max range is given from the feature range you specify\nmin = 0\nmax = 1\n\nX_std = (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n\nX_scaled = X_std * (max - min) + min\n\narray([[0.        , 0.        ],\n       [0.66666667, 0.66666667],\n       [1.        , 1.        ]])\n'
'demo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)\n\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\ntrans = transforms.Compose([transforms.ToTensor()])\n\ndemo = Image.open(img) \ndemo_img = trans(demo)\ndemo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)\nprint(Image.fromarray(demo_array.astype(np.uint8)))\n'
"df.loc[df['values'].astype(float) &gt; 0]\n"
'filtered_text=s_tokenized.apply(lambda x: [w for w in x if not w in all_stopwords])\n\nfiltered_text=s_tokenized.apply(lambda x: [w for w in x if not w.text in all_stopwords])\n\nimport spacy\nnlp=spacy.load(&quot;en_core_web_sm&quot;)\ntexts = [&quot;this laptop sits at just over 4 stars while&quot;, &quot;i ordered this monitor because i wanted&quot;]\ndocs = nlp.pipe(texts)\nfiltered_text= []\nfor doc in docs:\n#     yield [tok for tok in doc if not tok.is_stop]\n    filtered_text.append([tok for tok in doc if not tok.is_stop])\nprint(filtered_text)\n\n[[laptop, sits, 4, stars], [ordered, monitor, wanted]]\n'
"import matplotlib.pyplot as plt\nimport numpy as np\n\n# generate some sample data\nn, m = 10, 20\na = np.random.randint(0, 3, (n,m))\ns = np.random.randint(int(m/2), m, n)\nfor i in range(n):\n    a[i,s[i]:] = -1\n\n# show them as image\ncmap = plt.matplotlib.colors.ListedColormap(['w', 'r', 'lime', 'b'])\nplt.imshow(a, cmap=cmap)\n"
'dataSheet = pd.read_excel("http://api.worldbank.org/v2/en/indicator/EN.ATM.CO2E.PC?    downloadformat=excel",sheetname="Data")\ndataSheet = dataSheet.transpose()\ndataSheet = dataSheet.drop(dataSheet.columns[[0,1]], axis=1)\ndataSheet = dataSheet.drop([\'World Development Indicators\', \'Unnamed: 2\',\'Unnamed: 3\'])\n\ndataSheet.columns = dataSheet.iloc[1] # here I\'m assigning the column names\ndataSheet = dataSheet.reindex(dataSheet.index.drop(\'Data Source\')) # here I\'m re-indexing and getting rid of the duplicate row\n\ndf = dataSheet.transpose()\n\ndf.columns = df.iloc[0]\ndf = df.reindex(df.index.drop(\'Country Name\'))\n\nimport matplotlib.pyplot as plt\ndf[2010].plot(kind=\'bar\', figsize=[30,10])\n'
'    from tkinter import *\n    from tkinter import ttk\n\n    def convert_databases():\n            global file\n            global convert\n            # get the values of entries\n            file = file_entry.get()\n            convert = type_entry.get()\n\n    root = Tk()\n    root.title("Title")\n\n #Formatting\n    mainframe = ttk.Frame(root, padding="3 3 12 12")\n    mainframe.grid(column=0, row=0, sticky=(N, W, E, S))\n    mainframe.columnconfigure(0, weight=1)\n    mainframe.rowconfigure(0, weight=1)\n\n\n\n   # Places to enter variables\n    file_entry = ttk.Entry(mainframe, width=50)\n    file_entry.grid(column=2, row=2, sticky=(W, E))\n    type_entry = ttk.Entry(mainframe, width=50)\n    type_entry.grid(column=2,row=3, sticky=(W,E))\n\n    # Convert Button\n    ttk.Button(mainframe, text="Convert", command= convert_databases\n    )\n\n    #Label for the variable 1 input\n    ttk.Label(mainframe, text="Input file name: ").grid(column=1, row=2, \n    sticky=W)\n    #Label for the variable 2 input\n    ttk.Label(mainframe, text="Input file type conversion: \n    ").grid(column=1, row=3, sticky=W)\n\n     # This sets the window, I think?\n     for child in mainframe.winfo_children(): child.grid_configure(padx=5, \n     pady=5)\n\n     # Puts the cursor automatically in the text box\n     file_entry.focus()\n\n     # Runs the thing\n\n\n     root.mainloop()\n'
"new_df = pd.DataFrame(df0.stack().value_counts())\n\nimport pandas as pd\nimport numpy as np\n\ndef counted_entries(df, array):\n    output = pd.DataFrame(columns=df.columns, index=array)\n    for i in array:\n        output.loc[i] = (df==i).sum()\n    return output\n\ncolumns = ['Column ' + str(i+1) for i in range(10)]\nindex = ['Row ' + str(i+1) for i in range(5)]\n\ndf = pd.DataFrame(np.random.choice(['pig','cow','sheep','horse','dog'],size=(5,10)), columns=columns, index=index)\n\nunique_vals = list(set(df.stack())) #this is all the possible entries in the df\n\ndf2 = counted_entries(df, unique_vals)\n\n      Column 1 Column 2 Column 3 Column 4  ... Column 7 Column 8 Column 9 Column 10\nRow 1      pig      pig      cow      cow  ...      cow      pig      dog       pig\nRow 2    sheep      cow      pig    sheep  ...      dog      pig      pig       cow\nRow 3      cow      cow      cow    sheep  ...    horse      dog    sheep     sheep\nRow 4    sheep      cow    sheep      cow  ...      cow    horse      pig       pig\nRow 5      dog      pig    sheep    sheep  ...    sheep    sheep    horse     horse\n\n       Column 1  Column 2  Column 3  ...  Column 8  Column 9  Column 10\npig           1         2         1  ...         2         2          2\nhorse         0         0         0  ...         1         1          1\nsheep         2         0         2  ...         1         1          1\ndog           1         0         0  ...         1         1          0\ncow           1         3         2  ...         0         0          1\n"
"df['Edition_Type'] = df['Edition_TypeDate'].str.extract(r'(Import|Illustrated)')\ndf['Edition_Month'] = df['Edition_TypeDate'].str.extract(r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)')\ndf['Edition _Year'] = df['Edition_TypeDate'].str.extract(r'(\\d{4}$)')\nprint (df)\n                      Edition_TypeDate Edition_Type Edition_Month  \\\n0                                 2016          NaN           NaN   \n1                           5 Oct 2017          NaN           Oct   \n2                                 2017          NaN           NaN   \n3                           2 Aug 2009          NaN           Aug   \n4                  Illustrated, Import  Illustrated           NaN   \n5                  Import, 22 Feb 2018       Import           Feb   \n6                  Import, 14 Dec 2017       Import           Dec   \n7                   Import, 1 Mar 2018       Import           Mar   \n8         Abridged, Audiobook, Box set          NaN           NaN   \n9   International Edition, 26 Apr 2012          NaN           Apr   \n10                        Import, 2018       Import           NaN   \n11                Box set, 15 Jun 2014          NaN           Jun   \n12              Unabridged, 6 Jul 2007          NaN           Jul   \n\n   Edition _Year  \n0           2016  \n1           2017  \n2           2017  \n3           2009  \n4            NaN  \n5           2018  \n6           2017  \n7           2018  \n8            NaN  \n9           2012  \n10          2018  \n11          2014  \n"
'from skimage import data\nimport matplotlib.pyplot as plt\nimage_coffee = data.coffee()\nimage_horse = data.horse()\nfig,ax = plt.subplots(ncols=2,nrows=1)\nax[0].imshow(image_coffee)\nax[2].imshow(image_horse)\n'
"my_territories = ['London','Yorkshire']\ndf['territories'] = df.apply(lambda x: [t for t in my_territories if t in x['Region']][0], axis=1)\n\n    Region          Value   territories\n0   London North    8       London\n1   North Yorkshire 4       Yorkshire\n2   London South    6       London\n3   South Yorkshire 6       Yorkshire\n\ndf.groupby(['territories'])['Value'].mean().reset_index()\n\n    territories Value\n0   London      7\n1   Yorkshire   5\n"
"import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\ndata = make_blobs(n_samples=200, n_features=8, \n                           centers=6, cluster_std=1.8,random_state=101)\n\nfig, ax = plt.subplots(nrows=2, ncols=2,figsize=(10,10))\n\nfrom sklearn.cluster import KMeans\nc=d=0\nfor i in range(4):\n    ax[c,d].title.set_text(f&quot;{i+1} iteration points:&quot;)\n    kmeans = KMeans(n_clusters=6,random_state=0,max_iter=i+1)\n    kmeans.fit(data[0])\n    centroids=kmeans.cluster_centers_\n    ax[c,d].scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='brg')\n    ax[c,d].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black')\n    d+=1\n    if d==2:\n        c+=1\n        d=0\n"
"import pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nvalues = [{'gender':'Female'} if i%2==0 else {'gender':'Male'} for i in range(100)]\n\nX = pd.DataFrame(values)\ny = [0 if i%2==0 else 1 for i in range(100)]\n\ndef binary_data(df):\n    df.gender = df.gender.map({'Female': 0, 'Male': 1})\n    print(df.shape)\n    return df\n\ncolumntransf = ColumnTransformer([('binarydata', FunctionTransformer(binary_data), ['gender'])])\nmodel_pipeline = Pipeline([\n    ('preprocessing', columntransf),\n    ('classifier', LogisticRegression(solver='lbfgs'))\n])\nparam_grid = {}\nsearch = GridSearchCV(model_pipeline, param_grid, scoring='accuracy')\nsearch.fit(X, y) \n\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(100, 1)\n\nsearch.best_estimator_\n"
"In [2473]: df['sub_uid'] = df.groupby(['v1', 'v2']).ngroup().add(1)\n\nIn [2474]: df\nOut[2474]: \n  v1 v2 vx3 vx4  sub_uid\n1  a  b   h   j        1\n2  a  b   n   p        1\n3  a  c   r   g        2\n4  d  e   p   j        3\n"
"import numpy as np\na = np.array([[1,3,3],[6,7,6],[9,9,4]])\n\nmax_array =  a.max(axis=1,keepdims=True)\n\nIn [19]: max_array\nOut[19]: \narray([[3],\n       [7],\n       [9]])\n\nmask = (a != max_array)\n\nIn [20]: mask\nOut[20]: \narray([[ True, False, False],\n       [ True, False,  True],\n       [False, False,  True]], dtype=bool)\n\na[mask] = a[mask] * 3\n\nIn [21]: a\nOut[21]: \narray([[ 3,  3,  3],\n       [18,  7, 18],\n       [ 9,  9, 12]])\n\nb = np.array([[1,3,3],[6,7,6],[9,9,4]])\nb_max =  b.max(axis=1,keepdims=True)\nb_ma = np.ma.masked_equal(b,b_max)\nq = b_ma * 3\nb = q.data\n\n# or\nb_ma *= 3\nb = b_ma.data\n\nIn [80]: b.flags['OWNDATA']\nOut[80]: False\n\nIn [81]: b = b_ma.data.copy()\n\nIn [82]: b.flags['OWNDATA']\nOut[82]: True\n"
"# Random Forest Classification\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('finalplacementdata3.csv')\nX = dataset.iloc[:, range(1, 12)].values\ny = dataset.iloc[:, 12].values\n\nsiX = np.lexsort((X[:, 1], X[:, 0]))\nsX, sy = X[siX], y[siX]\n\n# Splitting the dataset into the Training set and Test set\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n                     \nriX = np.minimum(sX.shape[0] - 1, np.searchsorted(sX[:, 0], X1.ravel()))\nrX = X[riX]\n\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()] + list(rX[:, 2:].T)).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Training set)')\nplt.xlabel('Quants')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n\nriX = np.minimum(sX.shape[0] - 1, np.searchsorted(sX[:, 0], X1.ravel()))\nrX = X[riX]\n\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()] + list(rX[:, 2:].T)).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\nplt.xlabel('Quants')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()\n"
"data = pd.read_pickle('taxi_owners.p')\n"
"master_dataframe0 = (pd.merge(df_d[['State', 'AL']], df_m[['State', 'AL']], how='inner', on='State')\n                    .rename({'AL_x' : 'Rate', 'AL_y' : 'Cost'}, axis=1)\n                    .groupby(['Rate', 'Cost'])['State'].apply(lambda x: '_'.join(x)).reset_index()\n                    .sort_values('State'))\nmaster_dataframe0 = master_dataframe0[['State', 'Rate', 'Cost']].assign(State='20_21_RDWY_Purple_' + master_dataframe0['State'])\nmaster_dataframe0\nOut[1]: \n                                     State    Rate   Cost\n7   20_21_RDWY_Purple_AL_AR_KY_LA_MS_SC_TN  50.80%  120.0\n11                    20_21_RDWY_Purple_AZ  56.70%  155.0\n15                    20_21_RDWY_Purple_CA  62.40%  145.0\n9                  20_21_RDWY_Purple_CO_ND  54.30%  155.0\n5            20_21_RDWY_Purple_CT_DE_MN_NE  50.00%  145.0\n4   20_21_RDWY_Purple_DC_IA_KS_MD_MI_OH_OK  49.00%  125.0\n3                     20_21_RDWY_Purple_FL  48.30%  125.0\n18                    20_21_RDWY_Purple_GA  67.90%  120.0\n14              20_21_RDWY_Purple_ID_MT_NV  61.80%  145.0\n0            20_21_RDWY_Purple_IL_IN_MO_NC  44.10%  120.0\n16                    20_21_RDWY_Purple_MA  63.50%  155.0\n8         20_21_RDWY_Purple_ME_NH_NM_RI_SD  53.90%  155.0\n6                     20_21_RDWY_Purple_NJ  50.50%  145.0\n13                    20_21_RDWY_Purple_NY  61.10%  145.0\n17                    20_21_RDWY_Purple_OR  64.40%  185.0\n2                     20_21_RDWY_Purple_PA  47.20%  145.0\n10                    20_21_RDWY_Purple_TX  56.60%  125.0\n1                     20_21_RDWY_Purple_UT  45.00%  170.0\n12                    20_21_RDWY_Purple_VA  57.90%  120.0\n"
'x=list(df[&quot;X-1&quot;])+list(df[&quot;X-2&quot;])+list(df[&quot;X-3&quot;])\n\nitems=[[row[&quot;X-1&quot;]]+[row[&quot;X-2&quot;]]+[row[&quot;X-3&quot;]]  for index,row in df.iterrows()]\n\nflat_list = [item for sublist in items for item in sublist]\n\nfinal=[]\nx=0\nwhile x&lt;len(flat_list):\n    try:\n        if (abs(flat_list[x:x+3][0]-flat_list[x:x+3][1])&lt;1)&amp;(abs(flat_list[x:x+3][0]-flat_list[x:x+3][2])&lt;1):\n            final.append(sum(flat_list[x:x+3])/3)\n            x+=3\n        else:\n            final.append(flat_list[x])\n            x+=1\n    except:\n        final.append(flat_list(x))\n\nfinal\n\n\n[411.726266,\n 437.4568523333333,\n 448.6382086666667,\n 481.5836796666667,\n 487.04879066666666,\n 492.5099116666667,\n 500.06118,\n 508.21378533333336,\n 513.5414303333333,\n 515.308245,\n 515.175867,\n 534.8212066666666,\n 551.7243709999999,\n 559.0741849999999,\n 562.1987966666667,\n 591.1643273333334]\n'
"df1 = (df.assign(Service=df.Service.isin(['Food','Beverage']).astype(int))\n       .groupby('Rep')\n       .agg({'Business':'nunique','Service':'sum'}))\nprint (df1)\n       Business  Service\nRep                     \nCindy         2        1\nKim           2        1\nNate          1        1\nTim           2        0\n\ns = df1['Service']/df1['Business']\nprint (s)\nCindy    0.5\nKim      0.5\nNate     1.0\nTim      0.0\ndtype: float64\n"
"pkt_data[&quot;src_ip&quot;] = pkt_data[&quot;src_ip&quot;].apply(lambda x: x.split(':')[0])\n"
"import re\ndef is_probably_english(app_name, threshold=0.9):\n    rx = re.compile(r'[-a-zA-Z0-9_ ]')\n    ascii = [char for char in app_name if rx.search(char)]\n    quotient = len(ascii) / len(app_name)\n    passed = True if quotient &gt;= threshold else False\n    return passed, quotient\n\n\nprint(is_probably_english('Instagram'))\nprint(is_probably_english('爱奇艺PPS -《欢乐颂2》电视剧热播'))\nprint(is_probably_english('Docs To Go™ Free Office Suite'))\nprint(is_probably_english('Instachat 😜'))\n\n(True, 1.0)\n(False, 0.3157894736842105)\n(True, 0.9655172413793104)\n(True, 0.9090909090909091)\n"
"s = df['col1'].append(df['col2'], ignore_index=True).rename('col4')\n#alternative\n#s = pd.concat([df['col1'], df['col2']], ignore_index=True).rename('col4')\n\ndf1 = df.join(s, how='outer')\n#alternative\n#df1 = pd.concat([df, s], axis=1)\n\nprint (df1)\n\n   col1  col2  col3  col4\n0   1.0   4.0   1.0     1\n1   2.0   5.0   2.0     2\n2   3.0   6.0   3.0     3\n3   NaN   NaN   NaN     4\n4   NaN   NaN   NaN     5\n5   NaN   NaN   NaN     6\n\ndf1 = df1.astype('Int64')\nprint (df1)\n   col1  col2  col3  col4\n0     1     4     1     1\n1     2     5     2     2\n2     3     6     3     3\n3  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     4\n4  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     5\n5  &lt;NA&gt;  &lt;NA&gt;  &lt;NA&gt;     6\n\ndf1 = df1.fillna('')\nprint (df1)\n\n  col1 col2 col3  col4\n0    1    4    1     1\n1    2    5    2     2\n2    3    6    3     3\n3                    4\n4                    5\n5                    6\n\ndf = df.reset_index(drop=True)\n\ns1 = df['full_name'].append(df['alt_name'], ignore_index=True).rename('combined_names')\ns2 = df['full_address'].append(df['alt_add'], ignore_index=True).rename('combined_address')\n\ndf1 = pd.concat([df, s1, s2], axis=1)\n"
'import pandas as pd\n\nindex = [\n    507,\n    2920,\n    1464,\n    1158,\n    282\n]\n\ntrue_p = pd.Series([\n    80.0,\n    44.6,\n    74.7,\n    74.8,\n    56.5,\n], index=index)\n\nprint(true_p.values)\n\n[80.  44.6 74.7 74.8 56.5]\n'
"import pandas as pd\ndata = [['A', '17/07/2020'], ['B', '15/07/2020'], ['C', '14/07/2020'], ['C', '10/07/2020'], ['B', '09/07/2020']]\ndf = pd.DataFrame(data, columns = ['Customer', 'Date'])\ndf=df.sort_values(by=['Customer','Date'])\ndf['test']=df.groupby('Customer')['Date'].shift()\nprint(df)\n  Customer        Date        test\n0        A  17/07/2020         NaN\n4        B  09/07/2020         NaN\n1        B  15/07/2020  09/07/2020\n3        C  10/07/2020         NaN\n2        C  14/07/2020  10/07/2020\n"
"            reason  count\n0         location     35\n1   recommendation     23\n2    recommedation      8\n3          confort      7\n4     availability      4\n5  reconmmendation      3\n6       facilities      3\n\ndf['groupcount']=df.groupby(df.reason.str[0:4])['count'].transform('sum')\n\n\n\n          reason  count  groupcount\n0         location     35          35\n1   recommendation     23          34\n2    recommedation      8          34\n3          confort      7           7\n4     availability      4           4\n5  reconmmendation      3          34\n6       facilities      3           3\n\ndf=df.assign(groupname=df.reason.str[0:4])\ndf['groupcount']=df.groupby(df.reason.str[0:4])['count'].transform('sum')\nprint(df)\n\n\n      reason  count groupname  groupcount\n0         location     35      loca          35\n1   recommendation     23      reco          34\n2    recommedation      8      reco          34\n3          confort      7      conf           7\n4     availability      4      avai           4\n5  reconmmendation      3      reco          34\n6       facilities      3      faci           3\n\n#Read csv\ndf=pd.read_csv(r'path')\n#Create another column which is a list of values 'Why you choose us' in each row\ndf['Why you choose us']=(df['Why you choose us'].str.lower().fillna('no comment given')).str.split(',')\n#Explode group to ensure each unique reason is int its own row but with all the otehr attrutes intact\ndf=df.explode('Why you choose us')\n#remove any white spaces before values in the column group and value_counts\ndf['Why you choose us'].str.strip().value_counts()\nprint(df['Why you choose us'].str.strip().value_counts())\n\nlocation            48\nno comment given    34\nrecommendation      25\nconfort              8\nfacilities           8\nrecommedation        8\nprice                7\navailability         6\nreputation           5\nreconmmendation      3\ninternet             3\nac                   3\nbreakfast            3\ntranquility          2\ncleanliness          2\naveilable            1\ncostumer service     1\npool                 1\ncomfort              1\nsearch engine        1\nName: group, dtype: int64\n"
"df['date'] = pd.to_datetime(df['date'])\n\ndf = (df.set_index('date')\n        .groupby('Serial_no')\n        .apply(lambda x: x.asfreq('MS'))\n        .drop('Serial_no', axis=1))\ndf = df.reset_index()\ndf[&quot;Index&quot;] = df.groupby(&quot;Serial_no&quot;).cumcount() + 1\nprint (df)\n    Serial_no       date  Index     x    y\n0           1 2014-01-01      1   2.0  3.0\n1           1 2014-02-01      2   NaN  NaN\n2           1 2014-03-01      3   3.0  3.0\n3           1 2014-04-01      4   6.0  2.0\n4           2 2011-03-01      1   5.1  1.3\n5           2 2011-04-01      2   5.8  0.6\n6           2 2011-05-01      3   6.5 -0.1\n7           2 2011-06-01      4   NaN  NaN\n8           2 2011-07-01      5   3.0  5.0\n9           3 2019-10-01      1   7.9 -1.5\n10          3 2019-11-01      2   8.6 -2.2\n11          3 2019-12-01      3   NaN  NaN\n12          3 2020-01-01      4  10.0 -3.6\n13          3 2020-02-01      5  10.7 -4.3\n14          3 2020-03-01      6   4.0  3.0\n\ndf['date'] = pd.to_datetime(df['date'])\n\nf = lambda x: x.reindex(pd.date_range(x.index.min(), x.index.max(), freq='MS', name='date'))\ndf = df.set_index('date').groupby('Serial_no').apply(f).drop('Serial_no', axis=1)\ndf = df.reset_index()\ndf[&quot;Index&quot;] = df.groupby(&quot;Serial_no&quot;).cumcount() + 1\n"
"df=df.replace('na',np.nan).transform(lambda x : sorted(x,key=pd.isnull),1)\n   x    y    z\n0  1  NaN  NaN\n1  2    1  NaN\n2  2    3    1\n3  1  NaN  NaN\n"
'convert_to_reference = {\n                         "format1": function1,\n                         "format2": function2,\n                       }\nconvert_from_reference = {\n                           ...\n                         }\nreference = convert_to_reference[input("input format: ")](input("input value: "))\noutput = convert_from_reference[input("output format: ")](reference)\nprint(output)\n'
'print (df.interpolate())\n\n                        temp   size\nlocation_id hours\n135         78     12.000000  100.0\n            79     13.000000  104.0\n            80     14.000000  108.0\n            81     15.000000  112.0\n            82     14.666667   82.0\n            83     14.333333   52.0\n            84     14.000000   22.0\n'
'from tqdm import tqdm\n\ndf["converted_eur"] = df.progress_apply(convert_currency, axis=1)\n'
'df["Date"] = pd.to_datetime(df["Date"])\ndf["Weekly_Sales"] = pd.to_numeric(df["Weekly_Sales"])\n\n\nout = df.sort_values(by=["Store", "Date"]) \\\n        .groupby(["Store"]) \\\n        .agg(growth_Q3=("Weekly_Sales", lambda x: (x.iloc[2] - x.iloc[1])/(x.iloc[1]) * 100))\n\n# Step 1 (Optionnal if types are already correct)\nprint(df.dtypes)\n# Store                    int64\n# Date                    object\n# Weekly_Sales            object\n# dtype: object\n\ndf["Date"] = pd.to_datetime(df["Date"])\ndf["Weekly_Sales"] = pd.to_numeric(df["Weekly_Sales"])\nprint(df.dtypes)\n# Store                    int64\n# Date            datetime64[ns]\n# Weekly_Sales           float64\n# dtype: object\n\n# Step 2 (Optionnal if data already sorted)\nprint(df.sort_values(by=["Store", "Date"]))\n#    Store       Date  Weekly_Sales\n# 0      1 2012-03-31   18951097.69\n# 1      1 2012-06-30   21036965.58\n# 2      1 2012-09-30   18633209.98\n# 3      1 2012-12-31    9580784.77\n# 4      2 2012-03-31   22543946.63\n# 5      2 2012-06-30   25085123.61\n# 6      2 2012-09-30   22396867.61\n# 7      2 2012-12-31   11470757.52\n\n# Step 4\nprint(df.sort_values(by=["Store", "Date"])\n        .groupby(["Store"])\n        .agg(growth_Q3=("Weekly_Sales", lambda x: (x.iloc[2] - x.iloc[1])/x.iloc[1] * 100)))\n#        growth_Q3\n# Store\n# 1     -11.426342\n# 2     -10.716535\n'
'ppdf.map_partitions(lambda df: df[df==500].count()).sum().compute()\n'
'import pandas as pd\nimport random, math\n\ndef read_shuffled_chunks(filepath: str, chunk_size: int,\n                        file_lenght: int, has_header=True):\n\n    header = 0 if has_header else None\n    first_data_idx = 1 if has_header else 0\n    # create index list\n    index_list = list(range(first_data_idx,file_lenght))\n\n    # shuffle the list in place\n    random.shuffle(index_list)\n\n    # iterate through the chunks and read them\n    n_chunks = ceil(file_lenght/chunk_size)\n    for i in range(n_chunks):\n\n        rows_to_keep = index_list[(i*chunk_size):((i+1)*chunk_size - 1)]\n        if has_header:\n            rows_to_keep += [0] # include the index row\n        # get the inverse selection\n        rows_to_skip = list(set(index_list) - set(rows_to_keep)) \n        yield pd.read_csv(filepath,skiprows=rows_to_skip, header=header)\n'
'images = x_train[0:1000].reshape(1000, 28*28)\nimages = images / 255\nlabels = y_train[0:1000]\n\ntest_images = x_test[0:1000].reshape(1000, 28*28)\ntest_images = test_images / 255\ntest_labels = y_test[0:1000]\n'
"# ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n# ax.xaxis.set_major_formatter(mdates.DateFormatter('%d/%m/%y'))\n\nax.set_xticks(dados_alagoas.index)\nax.set_xticklabels(dados_alagoas.index.strftime('%d/%m/%y'))\n"
'df.MArks = df.Grade.map(scaling)\n'
'from sklearn.decomposition import PCA\nimport numpy as np \nnp.set_printoptions(precision=3)\n\nB = np.array([[1.0,2], [3,4], [5,6]])\n\nB1 = B.copy() \nB1 -= np.mean(B1, axis=0) \nn_samples = B1.shape[0]\nprint("B1 is B after centering:")\nprint(B1)\n\ncov_mat = np.cov(B1.T)\npca = PCA(n_components=2) \nX = pca.fit_transform(B1)\nprint("X")\nprint(X)\n\neigenvecmat =   []\nprint("Eigenvectors:")\nfor eigenvector in pca.components_:\n   if eigenvecmat == []:\n        eigenvecmat = eigenvector\n   else:\n        eigenvecmat = np.vstack((eigenvecmat, eigenvector))\n   print(eigenvector)\nprint("eigenvector-matrix")\nprint(eigenvecmat)\n\nprint("CHECK FOR PCA:")\nprint("X * eigenvector-matrix (=B1)")\nprint(np.dot(PCs, eigenvecmat))\n\nB1 is B after centering:\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\nX\n[[-2.828  0.   ]\n [ 0.     0.   ]\n [ 2.828  0.   ]]\nEigenvectors:\n[0.707 0.707]\n[-0.707  0.707]\neigenvector-matrix\n[[ 0.707  0.707]\n [-0.707  0.707]]\nCHECK FOR PCA:\nX * eigenvector-matrix (=B1)\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\n\nprint("B1 is B after centering:")\nprint(B1)\n\nfrom numpy.linalg import svd \nU, S, Vt = svd(X1, full_matrices=True)\n\nprint("U:")\nprint(U)\nprint("S used for building Sigma:")\nprint(S)\nSigma = np.zeros((3, 2), dtype=float)\nSigma[:2, :2] = np.diag(S)\nprint("Sigma:")\nprint(Sigma)\nprint("V already transposed:")\nprint(Vt)\nprint("CHECK FOR SVD:")\nprint("U * Sigma * Vt (=B1)")\nprint(np.dot(U, np.dot(Sigma, Vt)))\n\nB1 is B after centering:\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\nU:\n[[-0.707  0.     0.707]\n [ 0.     1.     0.   ]\n [ 0.707  0.     0.707]]\nS used for building Sigma:\n[4. 0.]\nSigma:\n[[4. 0.]\n [0. 0.]\n [0. 0.]]\nV already transposed:\n[[ 0.707  0.707]\n [-0.707  0.707]]\nCHECK FOR SVD:\nU * Sigma * Vt (=B1)\n[[-2. -2.]\n [ 0.  0.]\n [ 2.  2.]]\n'
"s1 = pd.Series(list(s), index=[f'c{x+1}' for x in range(len(s))])\n\n# print(s1)    \n# c1    H\n# c2    K\n# c3    g\n# c4    5\n# dtype: object\n\ndf.loc[((df == s1) | (df.isna())).all(1)]\n\n  c1   c2 c3 c4   c5\n1  H  NaN  g  5  NaN\n\ndef df_match_string(frame, string):\n    s1 = pd.Series(list(string), index=[f'c{x+1}' for x in range(len(string))])\n    return ((frame == s1) | (frame.isna())).all(1)\n\ndf_match_string(df, s)\n\n0    False\n1     True\n2    False\ndtype: bool\n\nfor col in df:\n    df[col] = df[col].str.strip()\n"
"most_working_borough = london_working_hours.sort_values(by='Working Hours%', ascending=False).Boroughs.tolist()[0]\npopulation = asian_london_table.loc[asian_london_table['Area'] == most_working_borough].Area.tolist()[0]\nearning = london_table_earning.loc[london_table_earning['Area'] == most_working_borough].Area.tolist()[0]\n"
'def predict(self, X):\n    """\n    Predict class labels for samples in X.\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Samples.\n    Returns\n    -------\n    C : array, shape [n_samples]\n        Predicted class label per sample.\n    """\n    scores = self.decision_function(X)\n    if len(scores.shape) == 1:\n        indices = (scores &gt; 0).astype(np.int)\n    else:\n        indices = scores.argmax(axis=1)\n    return self.classes_[indices]\n\nscores = np.array([[0.5, 0.5]])\nscores.argmax(axis=1)\nOut[5]: array([0])\n'
'import pandas as pd\n\ndf = pd.read_csv("final_task.csv")\n\ndf[\'final_Unique\'].apply(lambda x: ",".join(map(str,sorted(map(int,x.split(\',\')))))).drop_duplicates()\n'
"newdf = a.assign(val=1).pivot_table(values='val', index='Actual_Data',\n                                    columns='Final_Unique', aggfunc=sum, fill_value=0\n                                    ).reset_index().rename_axis(None, axis=1)\n\n          Actual_Data  12,12  12,3  2,12  3,4  4,12  6,7  7,6  7,7  8,8\n0       1,1,1,1,1,14,      1     0     0    0     0    0    0    0    0\n1       1,1,12,2,2,4,      0     0     1    0     0    0    0    0    0\n2      1,14,14,1,1,2,      0     0     0    0     1    0    0    0    0\n3  14,14,14,14,14,14,      0     0     0    0     0    0    1    0    0\n4     2,12,3,4,12,12,      0     0     0    0     0    0    0    1    0\n5       6,6,6,6,3,14,      0     0     0    1     0    0    0    0    0\n6        6,7,7,7,6,7,      0     0     0    0     0    1    0    0    0\n7       6,8,8,8,8,12,      0     1     0    0     0    0    0    0    0\n8        8,8,8,8,8,8,      0     0     0    0     0    0    0    0    1\n\nnewdf = a[['Actual_Data']]\nfor col in set(a['Final_Unique'].dropna()):\n    newdf[col] = newdf.Actual_Data.str.findall('(^|,)'+col+'(?=,)').apply(len)\n\n          Actual_Data  8,8  12,3  12,12  7,7  2,12  6,7  4,12  3,4  7,6\n0        8,8,8,8,8,8,    3     0      0    0     0    0     0    0    0\n1        6,7,7,7,6,7,    0     0      0    1     0    2     0    0    1\n2     2,12,3,4,12,12,    0     1      1    0     2    0     1    1    0\n3  14,14,14,14,14,14,    0     0      0    0     0    0     0    0    0\n4       1,1,12,2,2,4,    0     0      0    0     0    0     0    0    0\n5       6,8,8,8,8,12,    2     0      0    0     0    0     0    0    0\n6       6,6,6,6,3,14,    0     0      0    0     0    0     0    0    0\n7      1,14,14,1,1,2,    0     0      0    0     0    0     0    0    0\n8       1,1,1,1,1,14,    0     0      0    0     0    0     0    0    0\n"
'pd.DataFrame({\n   "fruit": ["Apple"],\n   "size": ["Large"], \n   "color": ["Red"], \n   "grade": [None], \n   "bool": [True]\n})\n\npd.DataFrame([{\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n}])\n\npd.DataFrame({\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n})\n\npd.DataFrame({\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n}, index=[0])\n\n  fruit   size color grade  bool\n0  Apple  Large   Red  None  True\n\npd.DataFrame({\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n}, index=[0,1])\n\n   fruit   size color grade  bool\n0  Apple  Large   Red  None  True\n1  Apple  Large   Red  None  True\n'
"df['hour'] = df.index.strftime('%H').astype('int')\n\ndf['daytime'] = df['hour'].apply(lambda hr : 1 if (hr &gt; 6 and hr &lt; 18) else 0)\n\n\ndf.describe()\n\nRandom_Number   hour    daytime\ncount 500.000000 500.000000 500.000000\nmean 4.914000   9.000000    0.250000\nstd 2.539982    6.714922    0.433446\nmin 1.000000    0.000000    0.000000\n25% 3.000000    4.500000    0.000000\n50% 5.000000    9.000000    0.000000\n75% 7.000000    13.500000   0.250000\nmax 9.000000    18.000000   1.000000\n"
"def replace_thresh(df, col, thresh, new_val):\n    s = df[col].value_counts(normalize=True).mul(100)\n    df[col] = np.where(df[col].isin(s.index[s &lt; thresh]), new_val, df[col])\n    return df\n\ndf = replace_thresh(df, 'col', 1, 'Other')\n"
"df.assign(difference=df.groupby(['$ID','$orient','$direct'])['$weight'].diff().mul(-1))\n                                                                      .dropna()\n\n     $ID  $spec $view $orient $direct  $weight  difference\n3   9247      5   ant  stance     15b  2271.89      -94.15\n4   9247      6   ant  stance       0  2075.44      -36.78\n5   9247      7   ant  stance     15f  1438.31      121.31\n10  9119      3  post    fall     15b  1024.18       57.26\n11  9119      4  post    fall       0  1093.46      -98.98\n"
"digitFilter = (dataset['data'].str.isdigit()) | (dataset['data'] == 'NaN')\ndataset[digitFilter]\n"
"df = pd.concat([\n    df[['id', 'name']], df['genre'].str.get_dummies(sep=', ')\n], axis=1)\n\n   id   name  action  animation  comedy\n0   1  Fiml1       1          0       1\n1   2  Fiml2       0          1       0\n2   3  Fiml3       0          0       1\n3   4  Fiml4       1          1       0\n4   5  Fiml5       1          0       0\n5   6  Fiml6       0          1       1\n"
'from imblearn.oversampling import SMOTE\nsmote=SMOTE("minority")\nX,Y=smote.fit_sample(x_train_data,y_train_data)\n'
"data['reclat'] = data['reclat'].astype(str)\ndata['reclong'] = data['reclong'].astype(str)\n\ndata['GeoLocation'] = data['reclat'] + data['reclong']\n"
'X = [[0,0],[0,5],[5,0],[5,5][4,4]] \n\n   [0,0]:  [0.        , 5.        , 5.        , 5.65685425, 7.07106781],\n   [0,5]:  [0.        , 4.12310563, 5.        , 5.        , 7.07106781],\n   [5,0]:  [0.        , 4.12310563, 5.        , 5.        , 7.07106781],\n   [5,5]:  [0.        , 1.41421356, 5.        , 5.        , 7.07106781],\n   [4,4]:  [0.        , 1.41421356, 4.12310563, 4.12310563, 5.65685425]]\n'
"temp_df = pd.DataFrame({\n        'cat1':list('aaaabb'),\n         'cat2':[4,5,4,5,5,4],\n         'cont1':[7,8,9,4,2,3],\n\n})\n\nnew = (temp_df.groupby(['cat1', 'cat2'])['cont1'].agg(['mean'])\n             .reset_index()\n             .rename(columns={'mean': 'cat1_cont1/cat2_Mean'}))\nprint (new)\n  cat1  cat2  cat1_cont1/cat2_Mean\n0    a     4                     8\n1    a     5                     6\n2    b     4                     3\n3    b     5                     2\n\narrays = [list(new['cat1']), list(new['cat2'])]    \nnew.index = pd.MultiIndex.from_tuples(list(zip(*arrays)), names=['cat1', 'cat2'])\nd = new['cat1_cont1/cat2_Mean'].to_dict()\nprint (d)\n{('a', 4): 8, ('a', 5): 6, ('b', 4): 3, ('b', 5): 2}\n\ntemp_df['cat1_cont1/cat2_Mean'] = temp_df[['cat1', 'cat2']].apply(tuple, axis=1).map(d)\n\ntemp_df['cat1_cont1/cat2_Mean1'] = temp_df.groupby(['cat1', 'cat2'])['cont1'].transform('mean')\n\ns = temp_df.groupby(['cat1', 'cat2'])['cont1'].agg('mean').rename('cat1_cont1/cat2_Mean2')\ntemp_df = temp_df.join(s, on=['cat1', 'cat2'])\n\nprint (temp_df)\n  cat1  cat2  cont1  cat1_cont1/cat2_Mean  cat1_cont1/cat2_Mean1  \\\n0    a     4      7                     8                      8   \n1    a     5      8                     6                      6   \n2    a     4      9                     8                      8   \n3    a     5      4                     6                      6   \n4    b     5      2                     2                      2   \n5    b     4      3                     3                      3   \n\n   cat1_cont1/cat2_Mean2  \n0                      8  \n1                      6  \n2                      8  \n3                      6  \n4                      2  \n5                      3  \n"
"df['HasSocialMedia'] = df[['FACEBOOK', 'TWITTER', 'LINKEDIN', 'YOUTUBE', 'INSTAGRAM']].any(axis=1)\n"
"frame = pd.DataFrame(np.array([[ 0,  1,  2], [ 3,  4,  5], [ 6,  7,  8], [ 9, 10, 11]]),\nindex=[['a', 'a', 'a', 'b'], [1, 2, 3, 1]],\ncolumns=[['Ohio', 'Ohio', 'Colorado'],\n['Green', 'Red', 'Green']])\n\nframe = pd.DataFrame([[ 0,  1,  2], [ 3,  4,  5], [ 6,  7,  8], [ 9, 10, 11]],\nindex=[['a', 'a', 'a', 'b'], [1, 2, 3, 1]],\ncolumns=[['Ohio', 'Ohio', 'Colorado'],\n['Green', 'Red', 'Green']])\n\nAssertionError: 2 columns passed, passed data had 3 columns\n\nframe = pd.DataFrame([[ 0,  1,  2], [ 3,  4,  5], [ 6,  7,  8], [ 9, 10, 11]])\nframe.index=[['a', 'a', 'a', 'b'], [1, 2, 3, 1]]\nframe.columns=[['Ohio', 'Ohio', 'Colorado'],\n['Green', 'Red', 'Green']]\n"
'import numpy as np\n\nthreshold_range = np.arange(0.1,1,0.1)\nprint(threshold_range.tolist())\nprint(np.round(threshold_range, 2).tolist())\n\n[0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]\n[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n'
"import pandas as pd\nfrom datetime import timedelta\n\ndf  = pd.read_excel(r'C:\\Users\\me\\Desktop\\Sovrflw_data.xlsx')\ndf\n\ndf.sort_values(by='dates', inplace=True)\n\ndf[df['dates'] - df['dates'].shift(-6) == timedelta(-6)]\n\ndf.sort_index(inplace=True)\n"
"   DATA\n0  cat1\n1  dog1\n2  cat2\n3  dog2\n4  cat3\n5  dog3\n6  dog1\n\npd.get_dummies(df.set_index(df.DATA), prefix='', prefix_sep='')\n\n      cat1  cat2  cat3  dog1  dog2  dog3\nDATA                                    \ncat1     1     0     0     0     0     0\ndog1     0     0     0     1     0     0\ncat2     0     1     0     0     0     0\ndog2     0     0     0     0     1     0\ncat3     0     0     1     0     0     0\ndog3     0     0     0     0     0     1\ndog1     0     0     0     1     0     0\n"
"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplot_data = df.groupby('brand').size().reset_index(name='count').query('count&gt;=100')\nplt.figure(figsize=(10,50))\nplt.ylim(100,500)\nsns.barplot(data=plot_data, x='brand', y='count')\n"
'python -m pip install fancyimpute\n'
"a = [df.loc[x, 'data'].sum() for x in indices]\nprint (a)\n[4.6, 3.3, 4.1, 6.2]\n\narr = df['data'].values\na = [arr[x].sum() for x in indices]\nprint (a)\n[4.6, 3.3, 4.1, 6.2]\n\ndf1 = pd.DataFrame({\n    'd' : df['data'].values[np.concatenate(indices)], \n    'g' : np.arange(len(indices)).repeat([len(x) for x in indices])\n})\n\nprint (df1)\n      d  g\n0   1.5  0\n1   1.8  0\n2   1.3  0\n3   1.5  1\n4   1.8  1\n5   1.3  2\n6   1.5  2\n7   1.3  2\n8   1.3  3\n9   1.8  3\n10  1.3  3\n11  1.8  3\n\nprint(df1.groupby('g')['d'].sum())\ng\n0    4.6\n1    3.3\n2    4.1\n3    6.2\nName: d, dtype: float64\n\nIn [150]: %timeit [df.loc[x, 'data'].sum() for x in indices]\n4.84 ms ± 80.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nIn [151]: %%timeit\n     ...: df['data'].values\n     ...: [arr[x].sum() for x in indices]\n     ...: \n     ...: \n20.9 µs ± 99.3 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\nIn [152]: %timeit pd.DataFrame({'d' : df['data'].values[np.concatenate(indices)],'g' : np.arange(len(indices)).repeat([len(x) for x in indices])}).groupby('g')['d'].sum()\n1.46 ms ± 234 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\nIn [37]: %timeit [df.iloc[x, 0].sum() for x in indices]\n158 ms ± 485 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn [38]: arr = df['data'].values\n    ...: %timeit \\\n    ...: [arr[x].sum() for x in indices]\n5.99 ms ± 18 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nIn[49]: %timeit pd.DataFrame({'d' : df['last'].values[np.concatenate(sample_indices['train'])],'g' : np.arange(len(sample_indices['train'])).repeat([len(x) for x in sample_indices['train']])}).groupby('g')['d'].sum()\n   ...: \n5.97 ms ± 45.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n"
"s = automobile.groupby('symb')['norm'].transform('mean')\nautomobile['norm'] = automobile['norm'].fillna(s)\n\nprint (automobile)\n   symb   norm  other  attrs\n0     1  100.0   8017      2\n1     1   90.0   5019      2\n2    -1   20.0   8017      1\n3    -1   20.0   8870      1\n4     1   95.0   8305      3\n5     0   10.0   8305      3\n6     3  200.0   8221      3\n"
"1 / 0.377 = 2.65\n\nimport matplotlib.pyplot as plt\n\ndays = [1,2,3,4,5,6]\nsales1 = [1,4,6,8,10,15]\nsales2 = [1,2,3,4,5,6]\n\ndf = pd.DataFrame({'days': days, 'sales1': sales1, 'sales2': sales2})\ndf = df.set_index('days')\ndf.plot(marker='o', linestyle='--')\n"
"import pandas as pd\nimport numpy as np\n\ndata=np.array([['','id','title','parent_id'],\n                [0,11,'p1',11],\n                [1,12,'p1',11],\n                [2,13,'p2',12],\n                [3,14,'p2',12],\n                [4,15,'p2',13],\n                [5,16,'p3',13],\n                [6,17,'p3',13]])\n\n\ndf=pd.DataFrame(data=data[1:,1:],\n                  index=data[1:,0],\n                  columns=data[0,1:])\n\ndf2=df.pivot(index='id',columns='title',values='parent_id')\n\ndf2=df.groupby(['title', 'parent_id']).count()\ndf2=df2.reset_index(drop=False)\ndf3=df2.pivot(index='parent_id',columns='title',values='id')\ndf3=df3.fillna(0)\nprint(df3)\n\n   id title parent_id\n0  11    p1        11\n1  12    p1        11\n2  13    p2        12\n3  14    p2        12\n4  15    p2        13\n5  16    p3        13\n6  17    p3        13\n\ntitle       p1   p2   p3\nparent_id               \n11         2.0  0.0  0.0\n12         0.0  2.0  0.0\n13         0.0  1.0  2.0\n"
"#if necessary convert to datetimes\ndf['CreationDate'] = pd.to_datetime(df['CreationDate'])\n\ndf1 = (df.groupby(df['CreationDate'].dt.year)\n         .agg({'Id':'first', 'Score':'sum', 'ViewCount':'sum'})\n         .reset_index()\n         .reindex(columns=df.columns)\n       )\n\nprint (df1)\n   Id  CreationDate  Score  ViewCount\n0   1          2011    109      16125\n1   3          2012     36       1015\n2   4          2018     33       7064\n"
"from pandas import DataFrame\nimport re\n\ndf1=DataFrame({'folder_name':['f1','g1'],'name':['aa','bb']})\ndf2=DataFrame({'name':['aa','bb','aadoq','bbaddd','ding'],'icon':['i1','i2','i3','i4','i5']})\ndf1_name_list=df1['name']\ndf2_name_list=df2['name']\nMappedName=[]\nfor name2 in df2_name_list:\n    for name1 in df1_name_list:\n        if re.match(name1,name2):\n            name2=name1\n            break\n    MappedName.append(name2)\ndf2['MappedName']=MappedName\ndf3=df1.merge(df2,left_on='name',right_on='MappedName',how='right').drop(['name_x','MappedName'],axis=1)\ndf4=df1.merge(df2,left_on='name',right_on='MappedName').drop(['name_x','MappedName'],axis=1)\n\nprint ('\\ndf1\\n',df1)\nprint ('\\ndf2\\n',df2)\nprint ('\\ndf3\\n',df3)\nprint ('\\ndf4\\n',df4)\n\ndf1\n   folder_name name\n0          f1   aa\n1          g1   bb\n\ndf2\n      name icon MappedName\n0      aa   i1         aa\n1      bb   i2         bb\n2   aadoq   i3         aa\n3  bbaddd   i4         bb\n4    ding   i5       ding\n\ndf3\n   folder_name  name_y icon\n0          f1      aa   i1\n1          f1   aadoq   i3\n2          g1      bb   i2\n3          g1  bbaddd   i4\n4         NaN    ding   i5\n\ndf4\n   folder_name  name_y icon\n0          f1      aa   i1\n1          f1   aadoq   i3\n2          g1      bb   i2\n3          g1  bbaddd   i4\n"
'url = "https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data"\n\nheaders = ["symboling", "normalized-losses", "make", "fuel-type", "aspiration",\n"num-of-doors", "body-style", "drive-wheels", "engine-location",\n"wheel-base", "length", "width", "height", "curb-weight",\n"engine-type", "num-of-cylinders", "engine-size", "fuel-system",\n"bore", "stroke", "compression-ratio", "hoursepower", "peak-rpm",\n"city-mpg", "highway-mpg", "price"]\n\ndf = pd.read_csv(url, header=None, names=headers, na_values=\'?\')\n\nprint(df.head(10))\n\n   symboling  normalized-losses         make fuel-type aspiration  \\\n0          3                NaN  alfa-romero       gas        std   \n1          3                NaN  alfa-romero       gas        std   \n2          1                NaN  alfa-romero       gas        std   \n3          2              164.0         audi       gas        std   \n4          2              164.0         audi       gas        std   \n5          2                NaN         audi       gas        std   \n6          1              158.0         audi       gas        std   \n7          1                NaN         audi       gas        std   \n8          1              158.0         audi       gas      turbo   \n9          0                NaN         audi       gas      turbo   \n\n  num-of-doors   body-style drive-wheels engine-location  wheel-base   ...     \\\n0          two  convertible          rwd           front        88.6   ...      \n1          two  convertible          rwd           front        88.6   ...      \n2          two    hatchback          rwd           front        94.5   ...      \n3         four        sedan          fwd           front        99.8   ...      \n4         four        sedan          4wd           front        99.4   ...      \n5          two        sedan          fwd           front        99.8   ...      \n6         four        sedan          fwd           front       105.8   ...      \n7         four        wagon          fwd           front       105.8   ...      \n8         four        sedan          fwd           front       105.8   ...      \n9          two    hatchback          4wd           front        99.5   ...      \n\n   engine-size  fuel-system  bore  stroke compression-ratio hoursepower  \\\n0          130         mpfi  3.47    2.68               9.0       111.0   \n1          130         mpfi  3.47    2.68               9.0       111.0   \n2          152         mpfi  2.68    3.47               9.0       154.0   \n3          109         mpfi  3.19    3.40              10.0       102.0   \n4          136         mpfi  3.19    3.40               8.0       115.0   \n5          136         mpfi  3.19    3.40               8.5       110.0   \n6          136         mpfi  3.19    3.40               8.5       110.0   \n7          136         mpfi  3.19    3.40               8.5       110.0   \n8          131         mpfi  3.13    3.40               8.3       140.0   \n9          131         mpfi  3.13    3.40               7.0       160.0   \n\n   peak-rpm city-mpg  highway-mpg    price  \n0    5000.0       21           27  13495.0  \n1    5000.0       21           27  16500.0  \n2    5000.0       19           26  16500.0  \n3    5500.0       24           30  13950.0  \n4    5500.0       18           22  17450.0  \n5    5500.0       19           25  15250.0  \n6    5500.0       19           25  17710.0  \n7    5500.0       19           25  18920.0  \n8    5500.0       17           20  23875.0  \n9    5500.0       16           22      NaN  \n\n[10 rows x 26 columns]\n'
"mask = df['Married'].isnull()\ndf.loc[mask, 'Married'] = df.loc[mask, 'Gender'].map({'Male':'Yes', 'Female':'No'})\nprint (df)\n   Gender Married\n0    Male     Yes\n1    Male     Yes\n2  Female      No\n3  Female      No\n4    Male     Yes\n5  Female      No\n\nmask = df['Married'].isnull()\ndf.loc[mask, 'Married'] = np.where(df.loc[mask, 'Gender']  == 'Male', 'Yes','No')\nprint (df)\n   Gender Married\n0    Male     Yes\n1    Male     Yes\n2  Female      No\n3  Female      No\n4    Male     Yes\n5  Female      No\n\ndf['Married'] = df['Married'].fillna(df['Gender'].map({'Male':'Yes', 'Female':'No'}))\n"
'df = pd.read_csv(file, header=[0,1], index_col=[0])\n\nidx = pd.IndexSlice\nprint (df.loc[1, idx[\'A\', \'0-3_mon\']])\n\nimport pandas as pd\n\ntemp=u"""A;A;B;B\n0-3_mon;3-6_mon;0-3_mon;3-6_mon\n1;10;12;14;18\n2;11;15;17;19\n3;13;16;21;20"""\n#after testing replace \'pd.compat.StringIO(temp)\' to \'filename.csv\'\ndf = pd.read_csv(pd.compat.StringIO(temp), sep=";", header=[0,1])\nprint (df)\n        A               B        \n  0-3_mon 3-6_mon 0-3_mon 3-6_mon\n1      10      12      14      18\n2      11      15      17      19\n3      13      16      21      20\n\nprint (df.columns)\nMultiIndex(levels=[[\'A\', \'B\'], [\'0-3_mon\', \'3-6_mon\']],\n           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n\nidx = pd.IndexSlice\nprint (df.loc[1, idx[\'A\', \'0-3_mon\']])\n10\n\nimport pandas as pd\n\ntemp=u"""acct_id;A;A;B;B\nlevel;0-3_mon;3-6_mon;0-3_mon;3-6_mon\n1;10;12;14;18\n2;11;15;17;19\n3;13;16;21;20"""\n#after testing replace \'pd.compat.StringIO(temp)\' to \'filename.csv\'\ndf = pd.read_csv(pd.compat.StringIO(temp), sep=";", index_col=[0], header=[0,1])\nprint (df)\nacct_id       A               B        \nlevel   0-3_mon 3-6_mon 0-3_mon 3-6_mon\n1            10      12      14      18\n2            11      15      17      19\n3            13      16      21      20\n\nprint (df.columns)\n\nMultiIndex(levels=[[\'A\', \'B\'], [\'0-3_mon\', \'3-6_mon\']],\n           labels=[[0, 0, 1, 1], [0, 1, 0, 1]],\n           names=[\'acct_id\', \'level\'])\n\nidx = pd.IndexSlice\nprint (df.loc[1, idx[\'A\', \'0-3_mon\']])\n10\n'
'df[~df.isnull()] \nOut[342]: \n     0    1  2\na  1.0  NaN  2\nb  2.0  3.0  5\nc  NaN  4.0  6\n\ndf[df==2]\nOut[343]: \n     0   1    2\na  NaN NaN  2.0\nb  2.0 NaN  NaN\nc  NaN NaN  NaN\n\ndf[df.isnull()] \nOut[344]: \n    0                           1       2\na NaN(False mask as NaN)       NaN(True) NaN\nb NaN(True)                    NaN       NaN\nc NaN                          NaN       NaN\n'
'from itertools import chain, combinations\n\ndef powerset(iterable):\n    xs = list(iterable)\n    return chain.from_iterable(combinations(xs, n) for n in range(len(xs) + 1))\n\nIn [79]: data\nOut[79]: \narray([[0, 1, 1],\n       [0, 0, 1],\n       [1, 0, 1],\n       [0, 1, 1],\n       [0, 0, 0],\n       [0, 1, 0],\n       [1, 1, 1],\n       [1, 1, 0],\n       [1, 1, 1],\n       [0, 1, 0]], dtype=int32)\n\nIn [80]: def is_constant(array):\n    ...:     return (array == array[0]).all()\n    ...: \n\nIn [81]: solution = []\n\nIn [82]: for candidate in powerset(range(len(data))):\n    ...:     if candidate and is_constant(data[candidate, :].sum(axis=0)):\n    ...:         solution.append(candidate)\n    ...: \n\nIn [83]: solution\nOut[83]: \n[(4,),\n (6,),\n (8,),\n (1, 7),\n (2, 5),\n (2, 9),\n (4, 6),\n (4, 8),\n (6, 8),\n (0, 2, 7),\n (1, 4, 7),\n (1, 6, 7),\n (1, 7, 8),\n (2, 3, 7),\n (2, 4, 5),\n (2, 4, 9),\n (2, 5, 6),\n (2, 5, 8),\n (2, 6, 9),\n (2, 8, 9),\n (4, 6, 8),\n (0, 2, 4, 7),\n (0, 2, 6, 7),\n (0, 2, 7, 8),\n (1, 2, 5, 7),\n (1, 2, 7, 9),\n (1, 4, 6, 7),\n (1, 4, 7, 8),\n (1, 6, 7, 8),\n (2, 3, 4, 7),\n (2, 3, 6, 7),\n (2, 3, 7, 8),\n (2, 4, 5, 6),\n (2, 4, 5, 8),\n (2, 4, 6, 9),\n (2, 4, 8, 9),\n (2, 5, 6, 8),\n (2, 6, 8, 9),\n (0, 2, 4, 6, 7),\n (0, 2, 4, 7, 8),\n (0, 2, 6, 7, 8),\n (1, 2, 4, 5, 7),\n (1, 2, 4, 7, 9),\n (1, 2, 5, 6, 7),\n (1, 2, 5, 7, 8),\n (1, 2, 6, 7, 9),\n (1, 2, 7, 8, 9),\n (1, 4, 6, 7, 8),\n (2, 3, 4, 6, 7),\n (2, 3, 4, 7, 8),\n (2, 3, 6, 7, 8),\n (2, 4, 5, 6, 8),\n (2, 4, 6, 8, 9),\n (0, 2, 4, 6, 7, 8),\n (1, 2, 4, 5, 6, 7),\n (1, 2, 4, 5, 7, 8),\n (1, 2, 4, 6, 7, 9),\n (1, 2, 4, 7, 8, 9),\n (1, 2, 5, 6, 7, 8),\n (1, 2, 6, 7, 8, 9),\n (2, 3, 4, 6, 7, 8),\n (1, 2, 4, 5, 6, 7, 8),\n (1, 2, 4, 6, 7, 8, 9)]\n\nIn [84]: data[(1, 2, 4, 6, 7, 8, 9), :].sum(axis=0)\nOut[84]: array([4, 4, 4])\n\nIn [85]: data[(0, 2, 4, 6, 7), :].sum(axis=0)\nOut[85]: array([3, 3, 3])\n\nfrom itertools import chain, combinations\n\ndef powerset(N):\n    """Power set of integers {0, ..., N-1}."""\n    xs = list(range(N))\n    return chain.from_iterable(combinations(xs, n) for n in range(N + 1))\n\n...\nfor candidate in powerset(len(data)):\n    ...\n'
'import requests\nfrom bs4 import BeautifulSoup\n\nTAG_NAME = \'area\'\nATTR_NAME = \'onclick\'\n\nurl = \'http://epaper.bhaskar.com/indore/129/10052018/mpcg/1/\'\nheaders = {\'User-Agent\': \'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36\'}\npage = requests.get(url, headers=headers)\nsoup = BeautifulSoup(page.text, \'html.parser\')\n\n# there are 3 &lt;area&gt; tags on page; putting them into a list\narea_onclick_attrs = [x[ATTR_NAME] for x in soup.findAll(TAG_NAME)]\nprint(area_onclick_attrs)\n\n[\n    "return show_pophead(\'78545\',\'51022929357\',\'1\')", \n    "return show_pop(\'78545\',\'51022928950\',\'4\')", \n    "return show_pop(\'78545\',\'51022929357\',\'1\')",\n]\n'
"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\n\ndf1 = pd.DataFrame(mlb.fit_transform(df['c2']),columns=mlb.classes_, index=df.index)\n\ndf = df.drop('c2', 1).join(df1)\nprint (df)\n\n  c1  1  2  3  4\n0  x  1  1  1  0\n1  y  1  0  1  0\n2  z  0  1  0  1\n\ndf1 = df['c2'].apply(lambda x: '|'.join([str(y) for y in x])).str.get_dummies()\n\ndf = df.drop('c2', 1).join(df1)\nprint (df)\n  c1  1  2  3  4\n0  x  1  1  1  0\n1  y  1  0  1  0\n2  z  0  1  0  1\n\ndf = df.join(pd.DataFrame(mlb.fit_transform(df.pop('c2')),\n                          columns\u200c\u200b=mlb.classes_, \n                          index=df.index))\n"
'class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False,\n    precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False,\n    positive=False, random_state=None, selection=’cyclic’)[source]\n\nreturn safe_sparse_dot(X, self.coef_.T,\n                       dense_output=True) + self.intercept_\n'
'import arrow # learn more: https://python.org/pypi/arrow\nfrom WunderWeather import weather # learn more: https://python.org/pypi/WunderWeather\n\napi_key = \'\'\nextractor = weather.Extract(api_key)\nzip = \'96345\'\n\n# get 20170101 00:00\nbegin_date = arrow.get("2017","YYYY")\n# get 20171231 23:00\nend_date = arrow.get("2018","YYYY").shift(hours=-1)\nfor date in arrow.Arrow.range(\'hour\',begin_date,end_date):\n  # get date object for feature\n  # http://wunderweather.readthedocs.io/en/latest/WunderWeather.html#WunderWeather.weather.Extract.date\n  date_weather = extractor.date(zip,date.format(\'YYYYMMDD\'))\n\n  # use shortcut to get observations and data\n  # http://wunderweather.readthedocs.io/en/latest/WunderWeather.html#WunderWeather.date.Observation\n  for observation in date_weather.observations:\n    print("Date:",observation.date_pretty)\n    print("Temp:",observation.temp_f)\n'
"def callback(attrname, old, new):\n    month = slider.value\n    source.data = ColumnDataSource(data[month]).data\n\ndata_to_use = data[data['Month'] == month[slider.value]]\n"
"mean_difference = (df.sort_values(by='ServicedOn')  # Get everything in date order\n                     .groupby('InvoiceNo')  # group by invoice\n                     .head(1)  # take first of each group\n                     .ServicedOn  # only look at ServicedOn value\n                     .diff()  # take differences\n                     .mean())  # calculate mean\n"
'cTe = sess.run([loss], feed_dict={input_tensor: batch_xTe, output_tensor: batch_yTe})\n'
"X = df[['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership']] \ny = df['Yearly Amount Spent'] \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor(n_neighbors=1)\nknn.fit(X_train,y_train)\n"
"DF1['Col3'] = DF1.Col2.astype(int)*DF2.Col2.astype(int)\nprint (DF1)\n     Col1  Col2  Col3\nRow1    A    30   300\nRow2    B    40   200\n\nDF1['Col3'] = DF1.Col2.astype(int).mul(DF2.Col2.astype(int))\nprint (DF1)\n     Col1  Col2  Col3\nRow1    A    30   300\nRow2    B    40   200\n"
"np.random.seed(124)\narray1 = np.array(['Name1','Name2'])\narray2 = np.array([1,2])\narray3 = np.array(list('ABCDE'))\nidx= pd.MultiIndex.from_product([array1,array2])\ndata=pd.DataFrame(np.random.randint(10, size=[len(idx), len(array3)]),\n                  index=idx ,columns=array3)\nprint (data)\n         A  B  C  D  E\nName1 1  1  7  2  9  0\n      2  4  4  5  5  6\nName2 1  9  6  0  8  9\n      2  9  0  2  2  1\n\nprint (data.index)\nMultiIndex(levels=[['Name1', 'Name2'], [1, 2]],\n           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])\n\n\ndata.loc[('Name1', 2), 'B'] = 20\nprint (data)\n         A  B  C  D  E\nName1 1  1  7  2  9  0\n      2  4 20  5  5  6\nName2 1  9  6  0  8  9\n      2  9  0  2  2  1\n\nidx = pd.IndexSlice\ndata.loc[idx['Name1', 2], 'B'] = 20\nprint (data)\n         A  B  C  D  E\nName1 1  1  7  2  9  0\n      2  4 20  5  5  6\nName2 1  9  6  0  8  9\n      2  9  0  2  2  1\n\nidx = pd.IndexSlice\nprint (data.loc[idx['Name1', 2], 'A'])\n4\n\n#select all values with 2 of second level and column A\nidx = pd.IndexSlice\nprint (data.loc[idx[:, 2], 'A'])\nName1  2    4\nName2  2    9\nName: A, dtype: int32\n\n#select 1 form second level and slice between B and D columns\nidx = pd.IndexSlice\nprint (data.loc[idx[:, 1], idx['B':'D']])\n         B  C  D\nName1 1  7  2  9\nName2 1  6  0  8\n\nprint (data.xs('Name1', axis=0, level=0))\n   A  B  C  D  E\n1  1  7  2  9  0\n2  4  4  5  5  6\n"
"for y in range(1991,2011):\n    for i in df.loc[df.year == y, 'PREC']:\n        if i &lt;= np.percentile(df.loc[df.year == y, 'PREC'], 10):\n            df.loc[(df['year'] == y) &amp; (df['PREC'] == i), 'intensity'] = 'light'\n"
'data.groupby("Day").size().reindex(pandas.date_range(start, end), fill_value=0)\n\n# I also named the new index :-)\ndata.groupby("Day").size().reindex(\n    pd.date_range(\'2017-04-20\', \'2017-05-06\', name=\'Day\'), fill_value=0)\n\nDay\n2017-04-20    462\n2017-04-21     64\n2017-04-22     13\n2017-04-23      5\n2017-04-24      9\n2017-04-25      5\n2017-04-26      1\n2017-04-27      2\n2017-04-28      0\n2017-04-29      0\n2017-04-30      1\n2017-05-01      0\n2017-05-02      1\n2017-05-03      0\n2017-05-04      1\n2017-05-05      0\n2017-05-06      1\nFreq: D, dtype: int64\n'
'&lt;title&gt;Beautiful Soup: We called him Tortoise because he taught us.&lt;/title&gt;\n&lt;meta name="Description" content="Beautiful Soup: a library designed for screen-scraping HTML and XML."&gt;\n\n&lt;a href="http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html"&gt;Here\'s\nthe Beautiful Soup 3 documentation.&lt;/a&gt;\n&lt;a href="download/3.x/BeautifulSoup-3.2.1.tar.gz"&gt;3.2.1&lt;/a&gt; \n&lt;a href="/source/software/BeautifulSoup/index.bhtml"&gt;\n&lt;a href="http://www.crummy.com/software/BeautifulSoup/"&gt;\n'
"normalized_metrics = normalize(associateMetrics, axis=0, norm='l1')\n"
"import osmnx as ox\nimport geopandas as gpd\nregion = {'country':'Portugal'}\ngdf = ox.gdf_from_place(region)\nfig, ax = ox.plot_shape(gdf, figsize=(7,7))\n\nexploded_gdf = gdf.explode()\n\nexploded_gdf['area'] = exploded_gdf.area\nexploded_gdf.sort_values(by='area', inplace=True)\nexploded_gdf\n\n# Extract the shapely underlying geometry:\ncontinental_part = exploded_gdf.iloc[-1]['geometry']\n"
'"User-ID";"ISBN";"Book-Rating"\n"11676";" 9022906116";"7"\n"11676";"\\"0432534220\\"";"6"\n"11676";"\\"2842053052\\"";"7"\n"11676";"0 7336 1053 6";"0"\n"11676";"0=965044153";"7"\n"11676";"0000000000";"9"\n"11676";"00000000000";"8"\n"146859";"01402.9182(PB";"7"\n"158509";"0672=630155(P";"0"\n"194500";"(THEWINDMILLP";"0"\n\ndf.ISBN = df.ISBN.str.replace(r\'[^\\w\\d]+\', \'\')\n\navg_ratings = df.groupby(\'ISBN\')[\'Book-Rating\'].mean().round().astype(np.int8)\n\ndf.loc[df[\'Book-Rating\'] == 0, \'Book-Rating\'] = df.loc[df[\'Book-Rating\'] == 0, \'ISBN\'].map(avg_ratings)\n'
'from sklearn.neighbors import KNeighborsClassifier\ndef useNeighbors(iterations, *args):\n    print(iterations) #normal argument\n    for arg in args:\n        my_dict = {}\n        my_dict[arg] = 20\n        KNeighborsClassifier(**my_dict)\n\nuseNeighbors(2, "n_neighbors", "leaf_size")\n'
'def read_csv(file_name):\n    f=open(file_name).read()\n    lis=f.split("\\n")\n    string_list=lis[1:len(lis)-1]\n    final_list=[]\n    for a in string_list:\n        string_fields=a.split(",")\n        int_field=[];    \n        for value in string_fields:\n            int_field.append(int(value))\n        final_list.append(int_field)\n    return(final_list)\ncdc_list=read_csv("US_births_1994-2003_CDC_NCHS.csv")\nprint(cdc_list[0:10])\n\ndef read_csv(file_name):\n    with open(file_name) as f:\n        #read lines\n        lines = [line.strip() for line in f.readlines()]\n        #remove header\n        lines = lines[1:]\n        # split lines and cast to integer\n        lines = [[int(val) for val in line.split(\',\')] for line in lines]\n    return(lines)\n'
'Please update your sklearn by pip or other tools \n'
'def train_classifier(...):\n    ...\n    init = tf.global_variables_initializer()\n    init_l = tf.local_variables_initializer()\n    with tf.Session(config=config) as sess, tf.device(device):\n        sess.run(init)\n        sess.run(init_l)\n'
"# assuming pandas successfully parsed this column as datetime object\n# and pandas version &gt;= 0.16\nfailed_banks= pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')[0]\nfailed_banks = failed_banks[failed_banks['Closing Date'].dt.year == 2017]\n\nfailed_banks = pd.read_html('http://www.fdic.gov/bank/individual/failed/banklist.html')[0]\n\ndef parse_date_strings(date_str):\n    return int(date_str.split(', ')[-1]) == 2017\n\nfailed_banks = failed_banks[failed_banks['Closing Date'].apply(parse_date_strings)]\n"
"with pm.Model():\n    A = pm.Normal('A')\n    B = pm.Deterministic('B', A + 1)\n    trace = pm.sample(1000)\n\n(trace['B'] == trace['A'] + 1).all()  # True\n"
"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=5, include_bias=False)),\n    ('linreg', LinearRegression(normalize=True))\n    ])\n\npipeline.fit(X_train, y_train)\npipeline.predict(np.array([4, 5, 6, 7]).reshape(1, -1))\n"
"docID_list = [i for i, _ in enumerate(os.listdir('/Users/suryavamsi/dict'))]\n\ndocID_list = list(range(len(os.listdir('/Users/suryavamsi/dict'))))\n\ndocID_list = {i:doc for i, doc in enumerate(os.listdir('/Users/suryavamsi/dict'))}\n"
"import pandas as pd\nimport numpy as np\n\nvalues = [\n    {'JOB_TITLE':'secretary', 'SALARY':30000},\n    {'JOB_TITLE':'programmer', 'SALARY':60000},\n    {'JOB_TITLE':'manager', 'SALARY':None},\n    {'JOB_TITLE':'president', 'SALARY':None},\n]\n\nsecret_values = [\n    {'JOB_TITLE':'manager', 'SALARY':150000},\n    {'JOB_TITLE':'president', 'SALARY':1000000},\n]\n\ndf = pd.DataFrame(values)\ndf_secret = pd.DataFrame(secret_values)\ndf.set_index('JOB_TITLE', inplace=True)\ndf_secret.set_index('JOB_TITLE', inplace=True)\n\ndf.combine_first(df_secret).reset_index()\n"
"# 1. Create a list of lists, where each sublist contains the characters\n#    contained in the columnd    \nseparated_data = [[sub_el for sub_el in el.strip(',') if ',' not in sub_el] \n                    for el in s['col1']]\n# separated_data is [['a', 'b'], ['b'], ['c', 'd'], ['a', 'c']]\n\n\n# 2. (optional) find the set of keys contained in your dataframe,\n#        if you don't already know that\nkeys = set([key for sublist in separated_data for key in sublist])\n# keys is {'a', 'b', 'c', 'd'}\n\n\n# 3. Create a dictionary, where the each character is a key and each value\n#     is a list. The n-th value of the list says 1 if the character is\n#     contained in the n-th row, 0 otherwise\ncolumns = {key: [1 if key in sublist else 0 for sublist in separated_data] \n                for key in keys}\n              for key in keys]\n# columns is {'a': [1, 0, 0, 1], 'b': [1, 1, 0, 0], 'c': [0, 0, 1, 1], 'd': [0, 0, 1, 0]}\n\n\n# 4. Your dataframe\nonehot_dataframe = pd.Dataframe(columns)\n# onehot_dataframe is:\n#    a  b  c  d\n# 0  1  1  0  0\n# 1  0  1  0  0\n# 2  0  0  1  1\n# 3  1  0  1  0\n"
'In []: try1 = np.random.randint(1, 10000, 100000000)\n\nIn []: try2 = np.random.randint(1, 10000, 100000000)\n\nIn []: ttest_ind(try1, try2, equal_var=False)\nOut[]: Ttest_indResult(statistic=-0.67549204672468233, pvalue=0.49936320345035146)\n\nIn []: try1 = np.random.randint(1, 10000, 1000000)\n\nIn []: ttest_ind(try1, try2, equal_var=False)\nOut[]: Ttest_indResult(statistic=-0.39754328321364363, pvalue=0.6909669583715552)\n'
'ax1 =g.ax_marg_x\nax2 = g.ax_marg_y\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntips = sns.load_dataset(\'tips\')\n\ndef graph_joint_histograms(tips):\n    g=sns.jointplot(x = \'total_bill\',y = \'tip\', data = tips, space = 0.3,ratio = 3)\n    g.ax_joint.cla()\n    g.ax_marg_x.cla()\n    g.ax_marg_y.cla()\n\n    for xlabel_i in g.ax_marg_x.get_xticklabels():\n        xlabel_i.set_visible(False)\n    for ylabel_i in g.ax_marg_y.get_yticklabels():\n        ylabel_i.set_visible(False)\n\n    x_labels = g.ax_joint.get_xticklabels()\n    x_labels[0].set_visible(False)\n    x_labels[-1].set_visible(False)\n\n    y_labels = g.ax_joint.get_yticklabels()\n    y_labels[0].set_visible(False)\n    y_labels[-1].set_visible(False)\n\n    g.ax_joint.set_xlim(0,200)\n    g.ax_marg_x.set_xlim(0,200)\n\n    g.ax_joint.scatter(x = tips[\'total_bill\'],y = tips[\'tip\'],data = tips,\n                       c = \'y\',edgecolors= \'#080808\',zorder = 2)\n    g.ax_joint.scatter(x = tips[\'total_bill\'],y = tips[\'tip\'],data = tips, \n                       c= \'c\',edgecolors= \'#080808\')\n\n    ax1 =g.ax_marg_x\n    ax2 = g.ax_marg_y\n    ax1.set_yscale(\'log\')\n    ax2.set_xscale(\'log\')\n\n    ax2.set_xlim(1e0, 1e4)\n    ax1.set_ylim(1e0, 1e3)\n\n    ax2.xaxis.set_ticks([1e0,1e1,1e2,1e3])\n    ax2.xaxis.set_ticklabels(("1","10","100","1000"), visible = True)\n\n    plt.setp(ax2.get_xticklabels(), visible = True)\n    colors = [\'y\',\'c\']\n    ax1.hist([tips[\'total_bill\'],tips[\'total_bill\']],bins = 10, \n             stacked=True, color = colors, ec=\'black\')\n\n    ax2.hist([tips[\'tip\'],tips[\'tip\']],bins = 10,orientation = \'horizontal\', \n             stacked=True, color = colors, ec=\'black\')\n    ax2.set_ylabel(\'\')\n\n\ngraph_joint_histograms(tips)\nplt.show()\n'
'def update_row(df):\n    row_tmp = {"next_row": None}\n    def updater(row):\n        last_row_id = row.name - 1\n        if row.name == 0:\n            row_tmp[\'next_row\'] = (row[\'Load\'] * 2) - (row[\'Load\'] /2.0)\n            return row_tmp[\'next_row\']\n        row_tmp[\'next_row\'] = (2* row[\'Load\']) - (row_tmp[\'next_row\']/2.0)\n        return row_tmp[\'next_row\']\n    return updater\n\n\ndf\n\nDate    Athlete Load\n0   2016-01-04  Alan    180\n1   2016-01-04  Alan    0\n2   2016-01-04  Alan    123\n3   2016-01-04  Alan    71\n4   2016-01-04  Alan    137\n5   2016-01-04  Alan    0\n6   2016-01-04  Alan    0\n\n\ndf.apply(update_row(df), axis=1)\n\n0    270.00000\n1   -135.00000\n2    313.50000\n3    -14.75000\n4    281.37500\n5   -140.68750\n6     70.34375\ndtype: float64\n'
'import csv\n\nwith open(\'phone_data.csv\',\'r\') as p_data:\n    data = csv.reader(p_data, delimiter=\',\')\n    next(data)\n    d_col = list(data)\n\nmin_row = min(d_col, key=lambda row: float(row[2]))\nmax_row = max(d_col, key=lambda row: float(row[2]))\nprint("min row is: ", min_row)\nprint("max row is: ", max_row)\n'
"df['Product Name'] = df.apply(lambda x: x['Product Name'].replace(x['Size Name'], '').strip(), axis=1)\n\n   ID                                    Product Name  Size ID Size Name\n0   1         24 Mantra Ancient Grains Foxtail Millet        1    500 gm\n1   2          24 Mantra Ancient Grains Little Millet        2    500 gm\n2   3                      24 Mantra Naturals Almonds        3    100 gm\n3   4                       24 Mantra Naturals Kismis        4    100 gm\n4   5                        24 Mantra Organic Ajwain        5    100 gm\n5   6             24 Mantra Organic Apple Blast Drink        6    250 ml\n6   7  24 Mantra Organic Apple Juice 1 Ltr Tetra Pack        7   1000 ml\n7   8                   24 Mantra Organic Apple Juice        8    200 ml\n8   9                     24 Mantra Organic Assam Tea        9    100 gm\n"
"d = {}\nfor i, tdf in df.groupby('Runner'):\n    d[i] = tdf[['100m', '400m']].values\n\nd = {}\nfor i, x in df.iterrows():\n    runner = x['Runner']\n    data = x[['100m', '400m']].tolist()\n    d[runner] = d.get(runner, []).append(data)\n"
"#df[['startdate','enddate']]=df[['startdate','enddate']].apply(pd.to_datetime)\ng=df.groupby(['cpf' ,'day'])\ndf['DIFF']=g.enddate.transform('last')-g.startdate.transform('first')\n"
"finallog = pd.concat([finallog, df_2],axis=0)\n...\n&gt; UnboundLocalError: local variable 'finallog' referenced before assignment\n\nx = 10\n\ndef p():\n    print(x)\n    x += 1\n\np()\nTraceback:\nUnboundLocalError                         Traceback (most recent call last)\n&lt;ipython-input-3-d40139f363ae&gt; in &lt;module&gt;\n----&gt; 1 p()\n\n&lt;ipython-input-2-271014bcda23&gt; in p()\n      1 def p():\n----&gt; 2     print(x)\n      3     x += 1\n      4\n\nx = 10\n\ndef p():\n    global x\n    print(x)\n    x += 10\n\np()\nOut: \n10\n"
'cv2.putText(gray, text, (x - 10, y - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n'
'#!/usr/bin/python3\nimport pandas as pd\nimport numpy as np\nfrom bokeh.models import ColumnDataSource, CategoricalColorMapper, HoverTool\nfrom bokeh.plotting import figure, curdoc\nfrom bokeh.palettes import viridis\nfrom bokeh.layouts import row, widgetbox\nfrom bokeh.models.widgets import Select, Slider\n\n#Importing and processing data file \ncrop = pd.read_csv(\'crop_production.csv\') \n\n#Cleaning Data \ncrop.fillna(np.NaN) \ncrop[\'Season\'] = crop.Season.str.strip() \n\n#Removing Whitespace #Filtering the dataset by Season \ncrop_season = crop[crop.Season == \'Whole Year\'] \ncrop_dt = crop_season.groupby([\'State_Name\', \'District_Name\', \'Crop_Year\']).mean().round(1)\n\ncrop_dt_year = crop_dt[crop_dt.index.get_level_values(\'Crop_Year\')==2001]\ncrop_dt_year_state = crop_dt_year[crop_dt_year.index.get_level_values(\'State_Name\')==\'Tamil Nadu\']\n\n#Creating Column Data Source\nsource = ColumnDataSource({\n    \'x\': crop_dt_year_state.Area.tolist(), \n    \'y\': crop_dt_year_state.Production.tolist(), \n    \'state\': crop_dt_year_state.index.get_level_values(\'State_Name\').tolist(), \n    \'district\': crop_dt_year_state.index.get_level_values(\'District_Name\').tolist()\n})\n\n#Creating color palette for plot\ndistrict_list = crop_dt.loc[([\'Tamil Nadu\']), :].index.get_level_values(\'District_Name\').unique().tolist()\ncall_colors = viridis(len(district_list))\ncolor_mapper = CategoricalColorMapper(factors=district_list, palette=call_colors)\n\n# Creating the figure\np = figure(\n    title = \'Crop Area vs Production\',\n    x_axis_label = \'Area\',\n    y_axis_label = \'Production\',\n    plot_height=900, \n    plot_width=1200,\n    tools = [HoverTool(tooltips=\'@district\')]\n          )\nglyphs = p.circle(x=\'x\', y=\'y\', source=source, size=12, alpha=0.7, \n         color=dict(field=\'district\', transform=color_mapper),\n         legend=\'district\')\np.legend.location = \'top_right\'\n\ndef update_plot(attr, old, new):\n    #Update glyph locations\n    yr = slider.value\n    state  = select.value\n    crop_dt_year = crop_dt[crop_dt.index.get_level_values(\'Crop_Year\')==yr]\n    crop_dt_year_state = crop_dt_year[crop_dt_year.index.get_level_values(\'State_Name\')==state]\n    new_data = {\n        \'x\': crop_dt_year_state.Area.tolist(), \n        \'y\': crop_dt_year_state.Production.tolist(), \n        \'state\': crop_dt_year_state.index.get_level_values(\'State_Name\').tolist(), \n        \'district\': crop_dt_year_state.index.get_level_values(\'District_Name\').tolist()\n    }\n    source.data = new_data\n    #Update colors\n    district_list = crop_dt.loc[([state]), :].index.get_level_values(\'District_Name\').unique().tolist()\n    call_colors = viridis(len(district_list))\n    color_mapper = CategoricalColorMapper(factors=district_list, palette=call_colors)\n    glyphs.glyph.fill_color = dict(field=\'district\', transform=color_mapper)\n    glyphs.glyph.line_color = dict(field=\'district\', transform=color_mapper)\n\n#Creating Slider for Year\nstart_yr = min(crop_dt.index.get_level_values(\'Crop_Year\'))\nend_yr = max(crop_dt.index.get_level_values(\'Crop_Year\'))\nslider = Slider(start=start_yr, end=end_yr, step=1, value=start_yr, title=\'Year\')\nslider.on_change(\'value\',update_plot)\n\n#Creating drop down for state\noptions = list(set(crop_dt.index.get_level_values(\'State_Name\').tolist()))\noptions.sort()\nselect = Select(title="State:", value="Tamil Nadu", options=options)\nselect.on_change(\'value\', update_plot)\n\nlayout = row(widgetbox(slider, select), p)\ncurdoc().add_root(layout)\n'
'df.index = pd.to_datetime(df.index,dayfirst=True)\n\ndf.index = pd.to_datetime(df.index,dayfirst=True,infer_datetime_format=True)\n'
"Xnew, Ynew = gmm16.sample(400)  # if Ynew is valuable\nplt.scatter(Xnew[:, 0], Xnew[:, 1])\n\nXnew, _ = gmm16.sample(400)  # if Ynew isn't valuable\nplt.scatter(Xnew[:, 0], Xnew[:, 1])\n"
'inputs_all_balanced  = inputs_all.drop(indices_to_remove,axis=0)\ntargets_all_balanced = targets_all.drop(indices_to_remove,axis=0)\n'
'from dateutil.parser import parse\n\nfirst_date_obj = parse("2015-01-28T21:41:38.508275")\nsecond_date_obj = parse("2015-02-28T21:41:38.508275")\nprint(second_date_obj - first_date_obj)\n\nprint(first_date_obj.year)\nprint(first_date_obj.month)\nprint(first_date_obj.day)\n# and so on\n'
'#importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nimport statsmodels.api as sm\nimport pyreadr\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\n\nresult = pyreadr.read_r(\'Movies.RData\')# also works for Rds\nprint(result.keys())\ndf = pd.DataFrame(result[\'movies\'], columns=result[\'movies\'].keys() )\ndf.shape\n\ndf.shape[0]\ndf.set_index("title", inplace=True) #setting the index name\ndf_1 = df.loc[:, [\'imdb_rating\',\'genre\', \'runtime\', \'best_pic_nom\',\n                  \'top200_box\', \'director\', \'actor1\']]\n\n#Let\'s also check the column-wise distribution of null values\nprint(df_1.isnull().values.sum())\nprint(df_1.isnull().sum())\n\n#Dropping missing values from my dataset\ndf_1.dropna(how=\'any\', inplace=True)\nprint(df_1.isnull().values.sum()) #checking for missing values after the dropna()\n\n#Splitting for 2 matrices: independent variables used for prediction and dependent variables (that is predicted)\nX = df_1.drop(["imdb_rating", \'runtime\'], axis = 1)   #Feature Matrix\ny = df_1["imdb_rating"] #Dependent Variables\n'
"pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
"ε = 1e-12\ndef derivative(f, x):\n    return (f(x + ε) - f(x)) / ε\n\ndef inner(x):\n    return x * np.log2(x)\n\ndef numerical_dentropy(X):\n    _, frequencies = np.unique(X, return_counts=True)\n    probabilities = frequencies / X.shape[0]\n    return -np.sum([derivative(inner, p) for p in probabilities])\n\nimport math\ndef dentropy(X):\n    _, frequencies = np.unique(X, return_counts=True)\n    probabilities = frequencies / X.shape[0]\n    return -np.sum([(1/math.log(2, math.e) + np.log2(p)) for p in probabilities])\n\na = np.array([1., 1., 1., 3., 3., 2.])\nb = np.array([1., 1., 1., 3., 3., 3.])\nc = np.array([1., 1., 1., 1., 1., 1.])\n\nprint(f&quot;numerical d[entropy(a)]: {numerical_dentropy(a)}&quot;)\nprint(f&quot;numerical d[entropy(b)]: {numerical_dentropy(b)}&quot;)\nprint(f&quot;numerical d[entropy(c)]: {numerical_dentropy(c)}&quot;)\n\nprint(f&quot;analytical d[entropy(a)]: {dentropy(a)}&quot;)\nprint(f&quot;analytical d[entropy(b)]: {dentropy(b)}&quot;)\nprint(f&quot;analytical d[entropy(c)]: {dentropy(c)}&quot;)\n\nnumerical d[entropy(a)]: 0.8417710972707937\nnumerical d[entropy(b)]: -0.8854028621385623\nnumerical d[entropy(c)]: -1.4428232973189605\nanalytical d[entropy(a)]: 0.8418398787754222\nanalytical d[entropy(b)]: -0.8853900817779268\nanalytical d[entropy(c)]: -1.4426950408889634\n\nimport torch\n\na, b, c = torch.from_numpy(a), torch.from_numpy(b), torch.from_numpy(c)\n\ndef torch_entropy(X):\n    _, frequencies = torch.unique(X, return_counts=True)\n    frequencies = frequencies.type(torch.float32)\n    probabilities = frequencies / X.shape[0]\n    probabilities.requires_grad_(True)\n    return -(probabilities * torch.log2(probabilities)).sum(), probabilities\n\nfor v in a, b, c:\n    h, p = torch_entropy(v)\n    print(f'torch entropy: {h}')\n    h.backward()\n    print(f'torch derivative: {p.grad.sum()}')\n\ntorch entropy: 1.4591479301452637\ntorch derivative: 0.8418397903442383\ntorch entropy: 1.0\ntorch derivative: -0.885390043258667\ntorch entropy: -0.0\ntorch derivative: -1.4426950216293335\n"
"import matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nx = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ny = [1, 12, 10, 1, 5, 3, 5, 7, 4, 4]\n\nx_vals = []\ny_vals = []\n\ncnt = 0\ndef animate(i):\n    global cnt\n    if cnt == 10:\n        exit('Finished')\n    print('loop ' + str(cnt))\n\n    x_vals.append(x[cnt])\n    y_vals.append(y[cnt])\n\n    print(x_vals, y_vals)\n    cnt += 1  # this is not happening\n    plt.plot(x_vals, y_vals, label='Price')\n\nani = FuncAnimation(plt.gcf(), animate, interval=1000)\n\nplt.show()\n"
"b = pd.to_timedelta(['00:00:00','00:10:00','00:20:00',\n                     '00:30:00','00:40:00',\n                     '00:50:00','00:60:00'])\nb = b.append(pd.Index([pd.Timedelta.max]))\ndf['Bin'] = pd.cut(df['time_interval'],  include_lowest=True, bins=b)\nprint (df)\n  time_interval                                             Bin\n0      00:17:00              (0 days 00:10:00, 0 days 00:20:00]\n1      01:42:00  (0 days 01:00:00, 106751 days 23:47:16.854775]\n2      00:15:00              (0 days 00:10:00, 0 days 00:20:00]\n3      00:00:00     (-1 days +23:59:59.999999, 0 days 00:10:00]\n4      00:06:00     (-1 days +23:59:59.999999, 0 days 00:10:00]\n\nvals = ['00:00:00','00:10:00','00:20:00',\n        '00:30:00','00:40:00', '00:50:00','00:60:00']\n\nb = pd.to_timedelta(vals).append(pd.Index([pd.Timedelta.max]))\n\nvals.append('inf')\nlabels = ['{}-{}'.format(i, j) for i, j in zip(vals[:-1], vals[1:])] \n\ndf['Bin'] = pd.cut(df['time_interval'],  include_lowest=True, bins=b, labels=labels)\nprint (df)\n  time_interval                Bin\n0      00:17:00  00:10:00-00:20:00\n1      01:42:00       00:60:00-inf\n2      00:15:00  00:10:00-00:20:00\n3      00:00:00  00:00:00-00:10:00\n4      00:06:00  00:00:00-00:10:00\n"
'    month   mac\n0   11      4\n'
'import random\n\ndef pick_a_number_from_list(A):\n    sum=0\n    cum_sum=[]\n    for i in range(len(A)):\n        sum = sum + A[i]\n        cum_sum.append(sum)\n    #print(cum_sum) \n    r = int(random.uniform(0,sum))\n    print(r)\n    number=0\n    for index in range(len(cum_sum)):\n        if(r&gt;=cum_sum[index] and r&lt;cum_sum[index+1]):\n            return A[index+1]\n    return number\n\ndef sampling_based_on_magnitued():\n    A = [0,5,27,6,13,28,100,45,10,79]\n    a = dict()\n    #A.sort()\n    print(A,sum(A))\n    for i in range(1,100):\n        number = pick_a_number_from_list(A)\n        #print(number)\n        if number not in a:\n            a[number] = 1\n        else:\n            a[number]+=1\n    print(a)\n\nsampling_based_on_magnitued()\n\n{100: 35, 5: 1, 45: 15, 79: 20, 28: 8, 13: 8, 6: 2, 27: 9, 10: 1}\n'
"df1.columns=['X']\nres = df2.reset_index().join(df1.reset_index(), rsuffix='_r')[['index', 'X']].set_index('index')\nprint(res)\n\nindex      \nF1        0\nF2       14\nF3      800\n...\nF85    2344\n"
"import itertools\ndef grouper(iterable, n, fillvalue=None):\n    &quot;Collect data into fixed-length chunks or blocks&quot;\n    # grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx&quot;\n    args = [iter(iterable)] * n\n    return iterttools.zip_longest(*args, fillvalue=fillvalue)\n\natoms = range(1, 116101)\nit = grouper(atoms, 27)\n\n"
"categorical_features : ndarray, shape (n_cat_features,) or (n_features,)\nSpecified which features are categorical. Can either be:\n\n- array of indices specifying the categorical features;\n- mask array of shape (n_features, ) and ``bool`` dtype for which\n  ``True`` indicates the categorical features.\n\nsmote_nc = SMOTENC(categorical_features=['A','B','C','D','E','F','G','H'], random_state=0)\n\nsmote_nc = SMOTENC(categorical_features=[df.dtypes==object], random_state=0)\n"
'scaler = preprocessing.StandardScaler().fit(X)\n# scaler is an object that knows how to normalize data points\nX_normalized = scaler.transform(X.astype(float))\n# used scalar to normalize the data points in X\n# Note, this is what you have done, just in two steps. \n# I just capture the scaler object \n#\n# ... Train your model on X_normalized\n#\n# Now predict\nother_data = [[30,2000,40]]\nother_data_normalized = scaler.transform(other_data)\nKNN.predict(other_data_normalized)\n'
"df1 = df[['FROM', 'ATTENDANCE']].copy()\ndf1['FROM'] = df1['FROM'].str.strip()\n\ndf2 = df1.drop_duplicates(keep='last')\n\n                  FROM ATTENDANCE\n2           Usha Dubey    PRESENT\n9   Pranjal Srivastava    PRESENT\n11       Jagriti Gupta    PRESENT\n12         Samaksh X A    PRESENT\n13        Bhavya Malik    PRESENT\n"
"import numpy as np\n\na = np.array([10, 25, 30, 20, 27, 29])\nv = 28 # value to find\n\n# Define intervals\nintervals = {i: f'[ {min(i1, i2)}, {max(i1,i2)} ]' for i, (i1,i2) in enumerate(zip(a[:-1],a[1:]))}\nintervals\n{0: '[ 10, 25 ]', 1: '[ 25, 30 ]', 2: '[ 20, 30 ]', 3: '[ 20, 27 ]', 4: '[ 27, 29]'}\n\n# Find indices of intervals\nb = a-v\nindices = np.squeeze(np.argwhere(b[:-1] *b[1:]&lt;=0))\nindices\narray([1, 2, 4], dtype=int64)\n\n[intervals[i] for i in indices]\n['[ 25, 30 ]', '[ 20, 30 ]', '[ 27, 29 ]']\n"
'import pandas as pd\n\ndf1 = pd.read_csv("file.csv")\ndf2 = pd.read_csv("file2.csv")\n\ndf2[\'C\'] = pd.to_datetime(df2[\'C\'], format=\'%Y-%m-%d\')\ndates = []\n\nfor ind in df2.index:\n    if(df2[\'D\'][ind]&gt;2):\n         date_tup = (df2[\'C\'][ind].year,df2[\'C\'][ind].month,df2[\'C\'][ind].day)\n         dates.append(date_tup)\n\ndf1[\'A\'] = pd.to_datetime(df1[\'A\'], format=\'%Y-%m-%d\', errors=\'ignore\')\n\nfor ind in df1.index:\n    date_tup = (df1[\'A\'][ind].year,df1[\'A\'][ind].month,df1[\'A\'][ind].day)\n    if(date_tup not in dates):\n         df1 = df1.drop([ind])\n\nprint(df1)\n\nA,B\n2019-01-01 03:56:29,197.199997\n2019-01-01 04:02:29,197.186142\n2019-01-02 06:24:29,196.857986\n2019-01-02 06:42:29,196.816376\n2019-01-03 11:52:29,196.100006\n2019-01-03 12:00:30,196.015961\n2019-01-04 14:18:30,194.566376\n2019-01-04 14:38:30,194.356293\n2019-01-04 19:48:30,191.100006\n2019-01-05 19:56:30,191.081512\n\nC,D\n2019-01-01 18:00:00,1333\n2019-01-02 19:00:00,1.18\n2019-01-03 20:00:00,1666667\n2019-01-04 21:00:00,0\n2019-01-05 22:00:00,1\n2019-01-06 23:00:00,1.5\n2019-01-07 00:00:00,109\n2019-01-08 01:00:00,200\n2019-01-09 02:00:00,192\n2019-01-10 03:00:00,1.700000\n'
"sb.lmplot(x='Year', y='Obesity (%)', data=dt) \nplt.show()\n"
'plt.plot(soil_moisture, soil_temperature)\n'
'automobile_df[&quot;price&quot;] = automobile_df[&quot;price&quot;].astype(&quot;int64&quot;)\nautomobile_df[&quot;km/100L&quot;] = automobile_df[&quot;km/100L&quot;].astype(&quot;int64&quot;)\n\nsns.regplot(x=automobile_df[&quot;km/100L&quot;].astype(&quot;int64&quot;),y=automobile_df[&quot;price&quot;].astype(&quot;int64&quot;))\n'
"df['product_first_sold_date'] = pd.to_datetime(df['product_first_sold_date'],unit='d',origin='1900-01-01')\n"
"temp=df1.apply(lambda x: df2.eq(x,axis=0).all()).any()\ntemp.index[temp]\n\n['col3', 'col4']\n"
'history_endog = list(train.copy(deep=True))\ny_true = []\ny_pred = []\n\nfor obs in test: \n    model = ARIMA(endog=history_endog, order=(p,d,q))\n    model_fit = model.fit()\n    forecast = model_fit.forecast()[0]\n\n    y_true.append(obs)\n    y_pred.append(forecast)\n    history_endog.append(obs)\n'
"  table = [\n      ['1_john', 23, 'LoNDon_paris'],\n      ['2_bob', 34, 'Madrid_milan'],\n      ['3_abdellah', 26, 'Paris_Stockhom']\n  ]\n  df = pd.DataFrame(table, columns=['ID_Name', 'Score', 'From_to'])\n  df[['ID','Name']] = df.apply(lambda x: get_first_last(x['ID_Name']), axis=1, result_type='expand')\n"
"import subprocess\nfrom io import StringIO\n\ndata = subprocess.run(\n    &quot;&quot;&quot;curl https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes/diabetes-data.tar.Z | \n    tar -xOvf diabetes-data.tar.Z --wildcards 'Diabetes-Data/data-*' &quot;&quot;&quot;,\n    shell=True,\n    capture_output=True,\n    text=True,\n).stdout\n\n\ndf = pd.read_csv(StringIO(data), sep=&quot;\\t&quot;, header=None)\n\ndf.head()\n\n        0       1        2  3\n0   04-21-1991  9:09    58  100\n1   04-21-1991  9:09    33  009\n2   04-21-1991  9:09    34  013\n3   04-21-1991  17:08   62  119\n4   04-21-1991  17:08   33  007\n"
"new_df = positions[positions['Ticker'].isin(my_ticker)]\n"
'class MyData:\n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.df = None\n    def quality_fun():\n       if self.df is None:\n          self.prepper_fun()\n       # rest of the code \n'
'from numpy import array\narrays_list = [array([[[-1.36013642e-01,  1.59766637e-02,  4.49674837e-02,...,\n          -1.22097181e-02,  2.81007364e-02]]])]\n\ndf = pd.DataFrame()\nfor i in  range(len(arrays_list)):\n    for j in range(len(arrays_list[i])):\n        df = pd.concat([df , pd.DataFrame(arrays_list[i][j]) ]).reset_index(drop=True)\ndf\n\n'
"import pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndata_a={&quot;date&quot;:[&quot;2015-08-31&quot;,&quot;2015-09-01&quot;, &quot;2015-09-02&quot;,&quot;2015-09-03&quot;,\n&quot;2015-09-04&quot;,&quot;2015-09-08&quot;,&quot;2015-09-09&quot;,&quot;2015-09-10&quot;,&quot;2015-09-11&quot;,&quot;2015-09-14&quot;],\n&quot;val&quot;:[112.760002,107.720001,112.339996,110.370003,109.269997,\n112.309998,110.150002,112.570000,114.209999,115.309998]}\n\ndata_b={&quot;date&quot;:[&quot;2015-08-31&quot;,&quot;2015-09-01&quot;,&quot;2015-09-11&quot;,&quot;2015-09-14&quot;],\n&quot;val&quot;:[112.760002,107.720001,114.209999,115.309998]}\n\n\ndf_a=pd.DataFrame(data_a)\ndf_b= pd.DataFrame(data_b)\n\ndf_a['date']=pd.to_datetime(df_a['date'],format='%Y-%m-%d', errors='ignore')\ndf_b['date']=pd.to_datetime(df_b['date'],format='%Y-%m-%d', errors='ignore')\n\n\nfig, ax = plt.subplots()\n\nax.plot(df_a['date'],df_a[&quot;val&quot;],color='red')\nax.scatter(df_b['date'],df_b[&quot;val&quot;]-1,marker=&quot;_&quot;,s=30,facecolor='green')\nax.scatter(df_b['date'],df_b[&quot;val&quot;]+1,marker=&quot;_&quot;,s=30,facecolor='green')\n\nplt.show()\n"
"df = (df\n     .applymap(lambda x: str(x).split(','))\n     .apply(pd.Series.explode)\n     .apply(pd.to_numeric)\n     .reset_index(drop=True))\n\nprint(df)\n\n   Bed  Week_Num  Day\n0    3        27    1\n1    3        27    1\n2    3        27    2\n3    1        35    2\n4    1        35    2\n5    1        35    1\n6    1        35    1\n"
'headers = datafram.columns  # current column names\nrenamed_column_name = renaming_columns(headers)  # get new names\n\ndatafram = datafram.rename(columns=renamed_column_name)  # rename all columns\n'
"newdf = df[df['data_d'].shift(-1).equal('True') &amp; df['data_b'].shift(-1).eq('Rest')]\n"
"#create a dictionary to store the list of review summary values\nd = {'review summary':[]}\n\n#function to extract only the review_summary from the line\ndef split_review_summary(full_line):\n    \n    #find review/text and exclude it from the line\n    found = full_line.find('review/text:')\n    if found &gt;= 0:\n        full_line = full_line[:found]\n\n    #find review summary. All text to the right is review summary\n    #add this to the dictionary\n    found = full_line.find('review/summary:')\n    if found &gt;= 0:\n        review_summary = full_line[(found + 15):]\n        d['review summary'].append(review_summary)\n\n#open the file for reading\nwith open (&quot;xyz.txt&quot;,&quot;r&quot;) as f:\n    #read the first line\n    new_line = f.readline().rstrip('\\n')\n    #loop through the rest of the lines\n    for line in f:\n        #remove newline from the data\n        line = line.rstrip('\\n')\n        \n        #if the line starts with product/productId, then its a new entry\n        #process the previous line and strip out the review_summary\n        #to do that, call split_review_summary function\n        \n        if line[:17] == 'product/productId':\n            split_review_summary(new_line)\n            #reset new_line to the current line\n            new_line = line\n        else:\n            #append to the new_line as its part of the previous record\n            new_line += line\n\n#the last full record has not been processed\n#So send it to split_review_summary to extract review summary\nsplit_review_summary(new_line)\n\n#now dictionary d has all the review summary items\nprint (d)\n\n{'review summary': [' Good Quality Dog Food ', ' Not as Advertised ']}\n"
"print (df2.lookup(df2.index, df1))\n\n['data1' 'data9' 'data13' 'data20' 'data25']\n"
"import seaborn as sns\nyears=df['date'].dt.year\nsns.distplot(years)\n\nlabel=1#0, you choose\ndf_lab=df.loc['label'==label]\nyears_label=df_lab['date'].dt.year\nsns.distplot(years_label)\n"
"A.loc[2].score\n\nA.loc[2, &quot;score&quot;] = some_new_value\n\nA.at[2, &quot;score&quot;] = some_new_value\n\nA.loc.__setitem__((2, 'score'), some_new_value) # modifies A directly\n"
'import numpy as np\nimport random\n\ndata=np.load(&quot;olivetti_faces.npy&quot;)\ntarget=np.load(&quot;olivetti_faces_target.npy&quot;)\n\n# target is groups of 10, so select random index in each block\nfor i in range(40):  # class 0-39\n   rndindex.append(i*10 + random.randint(0,9)) # one per class\n   \nfor i in range(60):  # up to 100\n   idx = rndindex[0]\n   while idx in rndindex:  # prevent duplicates\n       idx = random.randint(0,399)  # other indexes can be anywhere\n   rndindex.append(idx)\n\nrand_indeces = []  # np array objects\nfor idx in rndindex:\n   rand_indeces.append(data[idx])\n\nprint(rndindex)\n#print(rand_indeces)\n\n[9, 17, 23, 31, 41, 52, 60, 72, 83, 95, \n 100, 119, 121, 136, 140, 150, 166, 175, 188, 198, \n 209, 211, 221, 238, 243, 250, 261, 276, 289, 290, \n 306, 315, 325, 333, 344, 351, 368, 376, 382, 391, \n 62, 296, 327, 241, 393, 215, 64, 59, 185, 286, \n 162, 163, 364, 309, 220, 273, 32, 214, 217, 182, \n 172, 98, 19, 358, 92, 322, 68, 399, 226, 285, \n 103, 155, 249, 1, 75, 303, 311, 125, 339, 106, \n 127, 94, 101, 113, 35, 20, 189, 199, 128, 30, \n 131, 317, 337, 156, 340, 99, 397, 385, 384, 193]\n'
"new_df = pd.merge(df1, df2, how=&quot;inner&quot;, on =[&quot;feat_1&quot;, 'feat_2', 'feat_n'], suffixes=('_1','_2'))\n\ndf1\n  feat_1 feat_2 feat_n  a  b\n0      A      B      N  1  2\n\ndf2\n  feat_1 feat_2 feat_n  a  b\n0      A      B      N  3  4\n\nnew_df = pd.merge(df1, df2, how=&quot;inner&quot;, on =[&quot;feat_1&quot;, 'feat_2', 'feat_n'], suffixes=('_1','_2'))\n\n  feat_1 feat_2 feat_n  a_1  b_1  a_2  b_2\n0      A      B      N    1    2    3    4\n"
"import numpy as np\nimport matplotlib.pyplot as plt\ndata = [[206.6, 735.4, 427.9, 175.2,384.4],\n[487.5, 273.7, 742.6, 159.5,144],\n[613.4, 0, 294.9, 0,0]]\nX = np.arange(5)\ncol_list = ['red','blue','orange','green','cyan']\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nhandles = ax.bar(X + 0.00, data[0], color = col_list, width = 0.25)\nax.bar(X + 0.25, data[1], color = col_list, width = 0.25)\nax.bar(X + 0.50, data[2], color = col_list, width = 0.25)\nax.legend(labels=['Group-1','Group-2','Group-3','Group-4', 'Group-5'],\n           handles=handles)\nplt.show()\n"
"df = pd.DataFrame([['xyz', 'US'],['abc', 'MX'],['xyz', 'CA']], columns = [&quot;Name&quot;, &quot;Country&quot;])\n\nprint(df)\n\nName    Country\nxyz     US\nabc     MX\nxyz     CA\n\ndef generate(statement,col):\n    if statement.find(col) == -1:\n        return 0\n    else:\n        return 1\n\nfor col in L:\n    print(df[col].apply(generate, args=(col)))\n\n\nTypeError: generate() takes 2 positional arguments but 5 were given\n\n\ndf[col].apply(generate, col=col)\n\ndf[col].apply(generate, args=(col,))\n"
"df = pd.DataFrame({'artist':np.random.randint(0,10,10),\n                   'in_train':np.random.randint(0,2,10).astype(bool),\n                   'new_filename':np.random.rand(10)})\n\ntrain_info = df[df['in_train']].drop(columns='in_train').reset_index(drop=True)\ntest_info = df[~df['in_train']].drop(columns='in_train').reset_index(drop=True)\n"
"import numpy as np\n\n# input data\nn_types = 5\ntypes_weights = np.array([0.03, 0.24, 0.33, 0.24, 0.16])\ntypes_intervals = np.array([0.0, 0.07, 0.14, 0.38, 0.45, 1.0])\n\n# simulate distribution, by generating `n_samples` random floats \nn_samples = 1000000\ntype_samples = np.random.choice(range(n_types), p=types_weights, size=n_samples, replace=True, )\nfloat_ranges_begin = types_intervals[type_samples]\nfloat_ranges_end = types_intervals[type_samples + 1]\nfloat_samples = float_ranges_begin + np.random.rand(n_samples) * (float_ranges_end - float_ranges_begin)\n\n# plot results\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nplt.hist(float_samples, bins=100, density=True, rwidth=0.8)\n# to see types names instead\n# plt.xticks(types_intervals, types + ['1.0'], rotation='vertical', fontsize=16)\nplt.xlabel('Float', fontsize=16)\nplt.ylabel('Probability density', fontsize=16);\n\n\nn_types = 5\ntypes = ['Factory New', 'Minimal Wear', 'Field-Tested', 'Well-Worn', 'Battle-Scarred']\ntypes_weights = np.array([0.03, 0.24, 0.33, 0.24, 0.16])\ntypes_intervals = np.array([-0.0001, 0.07, 0.14, 0.38, 0.45, 1.0])\n\n# corerspond to top values on my plot, approximately [0.4 3.4 1.37 3.4 0.3]\ntypes_probability_density = types_weights / (types_intervals[1:] - types_intervals[:-1])\n\ndef float_probability_density(x):\n  types = np.searchsorted(types_intervals, x) - 1\n  return types_probability_density[types]\n\nsample_floats = np.linspace(0.0, 1.0, 100)\nplt.figure(figsize=(16,8))\nplt.bar(sample_floats, float_probability_density(sample_floats), width=0.005)\n"
'def main():\n\n    import requests\n    from bs4 import BeautifulSoup\n\n    url = &quot;https://fantasydata.com/NFL_FantasyStats/FantasyStats_Read&quot;\n\n    data = {\n        &quot;sort&quot;: &quot;FantasyPoints-desc&quot;,\n        &quot;pageSize&quot;: &quot;50&quot;,\n        &quot;filters.season&quot;: &quot;2020&quot;,\n        &quot;filters.seasontype&quot;: &quot;1&quot;,\n        &quot;filters.scope&quot;: &quot;1&quot;,\n        &quot;filters.subscope&quot;: &quot;1&quot;,\n        &quot;filters.aggregatescope&quot;: &quot;1&quot;,\n        &quot;filters.range&quot;: &quot;3&quot;,\n    }\n\n    response = requests.post(url, data=data)\n    response.raise_for_status()\n\n    players = response.json()[&quot;Data&quot;]\n    for player in players:\n        url = &quot;https://fantasydata.com&quot; + player[&quot;PlayerUrlString&quot;]\n\n        response = requests.get(url)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.content, &quot;html.parser&quot;)\n\n        college = soup.find(&quot;dl&quot;, {&quot;class&quot;: &quot;dl-horizontal&quot;}).findAll(&quot;dd&quot;)[-1].text.strip()\n\n        print(player[&quot;Name&quot;] + &quot; went to &quot; + college)\n\n    return 0\n\n\nif __name__ == &quot;__main__&quot;:\n    import sys\n    sys.exit(main())\n\nPatrick Mahomes went to Texas Tech\nKyler Murray went to Oklahoma\nAaron Rodgers went to California\nRussell Wilson went to Wisconsin\nJosh Allen went to Wyoming\nDeshaun Watson went to Clemson\nRyan Tannehill went to Texas A&amp;M\nLamar Jackson went to Louisville\nDalvin Cook went to Florida State\n...\n'
"product_table.drop_duplicates(subset=['product','crop'], inplace=True, ignore_index=True)\n"
"df = pd.DataFrame([['2010-12-06', 'MAC_AP_1', 'download', 1], \n                    ['2010-12-06', 'MAC_AP_1', 'upload', 2],\n                    ['2010-12-06', 'MAC_AP_2', 'download', 3],\n                    ['2010-12-06', 'MAC_AP_2', 'upload', 4], \n                    ['2020-01-01', 'MAC_AP_3', 'download', 5],\n                    ['2020-01-01', 'MAC_AP_3', 'upload', 6],\n                    ['2020-01-01', 'MAC_AP_4', 'download', 7],\n                    ['2020-01-01', 'MAC_AP_4', 'upload', 8]]\n                    , columns=['Dia', 'macap', 'transmission', 'bytes'])\n\n    Dia         macap       transmission    bytes\n0   2010-12-06  MAC_AP_1    download    1\n1   2010-12-06  MAC_AP_1    upload  2\n2   2010-12-06  MAC_AP_2    download    3\n3   2010-12-06  MAC_AP_2    upload  4\n4   2020-01-01  MAC_AP_3    download    5\n5   2020-01-01  MAC_AP_3    upload  6\n6   2020-01-01  MAC_AP_4    download    7\n7   2020-01-01  MAC_AP_4    upload  \n\nd = df.groupby('Dia').apply(lambda a: dict(a.groupby('macap').apply(lambda x: dict(zip(x['transmission'], x['bytes'])))))\nd = d.to_dict()\n\n{'2010-12-06': {'MAC_AP_1': {'download': 1, 'upload': 2}, \n                'MAC_AP_2': {'download': 3, 'upload': 4}}, \n'2020-01-01': {'MAC_AP_3': {'download': 5, 'upload': 6}, \n               'MAC_AP_4': {'download': 7, 'upload': 8}}}\n"
"df['col'].apply(tuple).unique()\n\ndf['col'].apply(tuple).explode().unique()\n"
"# Initialize dataframe\ndf = pd.DataFrame({\n    'TIME': ['2019-12-01','2019-11-01','2019-10-01'],\n    'ID':   [1,2,3],\n    '2019-08-01':   [1,6,11],\n    '2019-09-01':   [2,7,12],\n    '2019-10-01':   [3,8,13],\n    '2019-11-01':   [4,9,14],\n    '2019-12-01':   [5,10,15],\n})\n\n# Convert the wide table to a long table by melting the date columns\n# Name the new date column as REF_TIME, and the bill column as BILL\n\ndate_cols = ['2019-08-01', '2019-09-01', '2019-10-01', '2019-11-01', '2019-12-01']\ndf = df.melt(id_vars=['TIME','ID'], value_vars=date_cols, var_name='REF_TIME', value_name='BILL')\n\n# Convert TIME and REF_TIME to datetime type\ndf['TIME'] = pd.to_datetime(df['TIME'])\ndf['REF_TIME'] = pd.to_datetime(df['REF_TIME'])\n\n# Find out difference between TIME and REF_TIME\ndf['TIME_DIFF'] = (df['TIME'] - df['REF_TIME']).dt.days\ndf['TIME_DIFF'] = (df['TIME_DIFF'] / 30).round()\n\n# Keep only the preceding 3 months (including the month = TIME)\nselection = (\n    (df['TIME_DIFF'] &lt; 3) &amp;\n    (df['TIME_DIFF'] &gt;= 0)\n)\n\n# Apply selection, sort the columns and keep only columns needed\ndf_out = (\n    df[selection]\n    .sort_values(['TIME','ID','REF_TIME'])\n    [['TIME','ID','BILL']]\n)\n\n# Add a running number, lets call this BILL_NO\ndf_out = df_out.assign(BILL_NO = df_out.groupby(['TIME','ID']).cumcount() + 1)\n\n# Pivot the output table to the format needed\ndf_out = df_out.pivot(index=['ID','TIME'], columns='BILL_NO', values='BILL')\n\nBILL_NO         1   2   3\nID  TIME            \n1   2019-12-01  3   4   5\n2   2019-11-01  7   8   9\n3   2019-10-01  11  12  13\n"
"#fit a fourth degree polynomial to the economic data\nimport numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\n\nx = [17.47,20.71,21.08,18.08,17.12,14.16,14.06,12.44,11.86,11.19,10.65]\ny = [5,35,65,95,125,155,185,215,245,275,305]\n\n# define the true objective function\ndef objective(x, a, b, c, d, e):\n    return ((a)-((b)*(x/3-5)))+((c)*(x/305)**2)-((d)*(np.log(305))-np.log(x))+((e)*(np.log(305)-(np.log(x))**2))\n\npopt, _ = curve_fit(objective, x, y)\n# summarize the parameter values\na, b, c, d, e = popt\n# plot input vs output\nplt.scatter(x, y)\n# define a sequence of inputs between the smallest and largest known inputs\nx_line = np.arange(np.min(x), np.max(x), 1)\n# calculate the output for the range\ny_line = objective(x_line, a, b, c, d, e)\n# create a line plot for the mapping function\nplt.plot(x_line, y_line, '--', color='red')\nplt.show()\n"
'%load_ext autoreload\n%autoreload 2\nfrom config import example_var\n\nprint(example_var)\n'
"&gt;&gt;&gt; from collections import Counter\n&gt;&gt;&gt; li = ['a1', 'b1', 'a1', 'c3', 'a1', 'b1', 'd5']\n&gt;&gt;&gt; Counter(li)\nCounter({'a1': 3, 'b1': 2, 'c3': 1, 'd5': 1})\n"
"In [1918]: (df.groupby((df.line_by != df.line_by.shift()).cumsum(), as_index=False)\n              .agg({'id': 'first', 'timestamp': 'last', 'line_by': 'first',\n                   'line_text': ''.join}))\nOut[1918]:\n  timestamp               line_text    id  line_by\n0   02:54.3             Text Line 1  1234  Person1\n1   03:47.0  Text Line 2Text Line 3  1234  Person2\n2   05:46.2  Text Line 4Text Line 5  1234  Person1\n3   06:44.5             Text Line 6  9876  Person2\n4   07:27.6             Text Line 7  9876  Person1\n5   10:20.3  Text Line 8Text Line 9  9876  Person2\n\nIn [1919]: (df.line_by != df.line_by.shift()).cumsum()\nOut[1919]:\n0    1\n1    2\n2    2\n3    3\n4    3\n5    4\n6    5\n7    6\n8    6\nName: line_by, dtype: int32\n\nIn [1920]: df\nOut[1920]:\n     id timestamp  line_by    line_text\n0  1234   02:54.3  Person1  Text Line 1\n1  1234   03:23.8  Person2  Text Line 2\n2  1234   03:47.0  Person2  Text Line 3\n3  1234   04:46.8  Person1  Text Line 4\n4  1234   05:46.2  Person1  Text Line 5\n5  9876   06:44.5  Person2  Text Line 6\n6  9876   07:27.6  Person1  Text Line 7\n7  9876   08:17.5  Person2  Text Line 8\n8  9876   10:20.3  Person2  Text Line 9\n"
'import keras.backend as K\n\n....\nc = K.variable([0])\n.....\n.....\n    errorGreater = K.cast(K.greater(error_mean,threshold), K.floatx())\n    c+=K.max(errorGreater) #if error_mean is 1 element only, you can just c+=errorGreater.\n'
"with pm.Model() as model:\n    alpha = pm.Exponential('alpha', 1/(C0.sum()+1))\n    beta = pm.Exponential('beta', 1/(I0.sum()+1))\n    obs = pm.BetaBinomial('obs', alpha, beta, N0, observed=C0)\n"
'from pyspark import *\nfrom pyspark.sql import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as f\n\ndf = spark.read.load(path="file:///home/zht/PycharmProjects/test/disk_file", format=\'csv\', sep=\',\', header=True)\n\ndf = df.withColumn(\'iplong\', f.substring(\'iplong\', pos=0, len=1)) \\\n    .withColumn(\'agent\', f.substring(\'agent\', pos=0, len=1)) \\\n    .withColumn(\'client\', f.substring(\'client\', pos=0, len=2)) \\\n    .withColumn(\'partner\', f.substring(\'partner\', pos=0, len=2)) \\\n    .withColumn(\'timestamp\',f.when(f.substring(\'id\', pos=6, len=1) % 2 == 1, \'2012-03-08 00:01:00.0\').otherwise(df[\'timestamp\']))\ndf.show()\n\n+-------+------+-----+-------+------+-------+--------------------+--------+--------------------+\n|     id|iplong|agent|partner|client|country|           timestamp|category|           reference|\n+-------+------+-----+-------+------+-------+--------------------+--------+--------------------+\n|9794476|     1|    S|     dv|    ds|     us|2012-03-08 00:01:...|      ad|                null|\n|9794474|     1|    S|     dv|    ds|     in|2012-03-08 00:01:...|      mg|riflql2a0yv8xoa9s...|\n|9794471|     3|    N|     du|    dr|     py|2012-03-08 00:01:...|      co|                null|\n|9794468|     1|    N|     dv|    ds|     vn|2012-03-08 00:00:...|      es|gp53lqr9njqd6z2ap...|\n|9794467|     1|    M|     du|    dv|     in|2012-03-08 00:00:...|      ad|                null|\n|9794466|     1|    N|     du|    dv|     in|2012-03-08 00:00:...|      ad|                null|\n|9794477|     1|    B|     du|    dt|     es|2012-03-08 00:01:...|      es|h5njsswvxorsau9u8...|\n|9794478|     1|    N|     du|    ds|     ru|2012-03-08 00:01:...|      mc|                null|\n|9794481|     1|    N|     dv|    dr|     th|2012-03-08 00:00:...|      mc|oj0rekb51pvirnjuq...|\n|9794482|     1|    N|     du|    dr|     id|2012-03-08 00:00:...|      co|r63f8uhijvr2irvka...|\n|9794483|     1|    M|     dv|    dv|     id|2012-03-08 00:00:...|      ad|                null|\n|9794485|     2|    G|     dv|    dr|     th|2012-03-08 00:00:...|      co|                null|\n|9794486|     3|    M|     dv|    ds|     za|2012-03-08 00:00:...|      es|                null|\n|9794492|     7|    N|     dv|    dt|     ng|2012-03-08 00:01:...|      es|onbw7na2mi8a62g4p...|\n|9794493|     6|    N|     du|    dv|     sd|2012-03-08 00:01:...|      ad|hoq05psulkszxm4iz...|\n|9794495|     1|    S|     dv|    dv|     in|2012-03-08 00:01:...|      co|im387req0zp1ucyga...|\n|9794496|     1|    G|     du|    ds|     in|2012-03-08 00:01:...|      mc|immfap8948rebeym8...|\n|9794498|     1|    S|     du|    ds|     in|2012-03-08 00:01:...|      mc|r81nrzjemr5jrfvjj...|\n|9794499|     1|    N|     dv|    dr|     au|2012-03-08 00:01:...|      ad|                null|\n|9794500|     1|    N|     dv|    dr|     id|2012-03-08 00:00:...|      co|tq09jycwii12iul7h...|\n+-------+------+-----+-------+------+-------+--------------------+--------+--------------------+\n\nres = df.groupBy([f.window(\'timestamp\', windowDuration=\'1 minutes\'),\'partner\', \'iplong\', \'agent\']).count()\nres = res.withColumn(\'total\',f.sum(\'count\').over(Window.partitionBy(["window", "partner"])))\nres.show(n=30, truncate=False)\n\n+---------------------------------------------+-------+------+-----+-----+-----+\n|window                                       |partner|iplong|agent|count|total|\n+---------------------------------------------+-------+------+-----+-----+-----+\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |1     |N    |1    |7    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |3     |N    |1    |7    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |3     |S    |1    |7    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |6     |N    |1    |7    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |1     |B    |1    |7    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |1     |G    |1    |7    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|du     |1     |S    |1    |7    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|dv     |3     |M    |1    |8    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|dv     |1     |N    |3    |8    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|dv     |2     |G    |1    |8    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|dv     |1     |G    |1    |8    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|dv     |1     |M    |1    |8    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|dv     |1     |S    |1    |8    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|dv     |3     |S    |1    |6    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|dv     |7     |N    |1    |6    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|dv     |1     |S    |3    |6    |\n|[2012-03-08 00:01:00.0,2012-03-08 00:02:00.0]|dv     |1     |N    |1    |6    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|du     |2     |S    |1    |4    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|du     |1     |M    |1    |4    |\n|[2012-03-08 00:00:00.0,2012-03-08 00:01:00.0]|du     |1     |N    |2    |4    |\n+---------------------------------------------+-------+------+-----+-----+-----+\n'
"import networkx as nx\nimport csv\nG = nx.Graph()\nwith open('csv-networkx.csv', newline='') as f:\n    reader = csv.reader(f)\n    counter = 0\n    for row in reader: # row is a list of the cells in this row.\n        G.add_edge(row[0], row[1])\n        counter += 1\n        if counter &gt;= 65535:\n            break\n"
"import pandas as pd\ndf = pd.read_csv('YOUR_PATH_HERE')\n\n# to check if a single town\ncountry = df.loc[(df['Town'] == 'Arlesey')]['Country']\n\n# using the isin() operator\ndf.loc[df['Town'].isin(['Arlesey'])]['Country']\n\n1    England\n\n     Town      County       Country\n1   Arlesey Bedfordshire    England\n\n# using values[0] because it is the first element in the array\ndf.loc[df['Town'].isin(['Arlesey'])]['Country'].values[0]\n"
"import pandas as pd\ndf = pd.DataFrame({'CODE': ['0001', '0001', '0001', '0002','0003'], \n                   'TYPE': ['A', 'B', 'C', 'A', 'B']})\npd.get_dummies(df, columns=['TYPE'])\n\n   CODE  TYPE_A  TYPE_B  TYPE_C\n0  0001       1       0       0\n1  0001       0       1       0\n2  0001       0       0       1\n3  0002       1       0       0\n4  0003       0       1       0\n"
"In [1]: import pandas as pd\n\nIn [2]: record_data = pd.DataFrame([[1,2,'a'],[2,6,'b'],[2,2,'c']], columns=['bedrooms', 'bathrooms', 'something_else'])\n\nIn [3]: record_data\nOut[3]: \n   bedrooms  bathrooms something_else\n0         1          2              a\n1         2          6              b\n2         2          2              c\n\nIn [4]: structured_data = pd.DataFrame([[1,2,'d'],[2,3,'e'],[1,3,'e']], columns=['bedrooms', 'bathrooms', 'something_else'])\n\nIn [5]: structured_data\nOut[5]: \n   bedrooms  bathrooms something_else\n0         1          2              d\n1         2          3              e\n2         1          3              e\n\nIn [6]: record_data[record_data.bedrooms.isin(structured_data.bedrooms) &amp; record_data.bathrooms.isin(structured_data.bathrooms)]\nOut[6]: \n   bedrooms  bathrooms something_else\n0         1          2              a\n2         2          2              c\n\nIn [1]: import pandas as pd\n\nIn [3]: structured_data = pd.DataFrame([[1,2,'d'],[2,3,'e'],[1,3,'e'],[1,6,'e']], columns=['bedrooms', 'bathrooms', 'something_else'])\n\nIn [4]: structured_data\nOut[4]: \n   bedrooms  bathrooms something_else\n0         1          2              d\n1         2          3              e\n2         1          3              e\n3         1          6              e\n\nIn [5]: record_data = pd.DataFrame([[1,2,'a'],[2,6,'b'],[2,2,'c'],[1,8,'g'],[4,2,'h']], columns=['bedrooms', 'bathrooms', 'something_else'])\n\nIn [6]: record_data\nOut[6]: \n   bedrooms  bathrooms something_else\n0         1          2              a\n1         2          6              b\n2         2          2              c\n3         1          8              g\n4         4          2              h\n\nIn [7]: record_data.bathrooms.isin(structured_data.bathrooms)\nOut[7]: \n0     True\n1     True\n2     True\n3    False\n4     True\nName: bathrooms, dtype: bool\n\nIn [8]: record_data.bedrooms.isin(structured_data.bedrooms)\nOut[8]: \n0     True\n1     True\n2     True\n3     True\n4    False\nName: bedrooms, dtype: bool\n\nIn [9]: record_data.bathrooms.isin(structured_data.bathrooms) &amp; record_data.bedrooms.isin(structured_data.bedrooms)\nOut[9]: \n0     True\n1     True\n2     True\n3    False\n4    False\ndtype: bool\n\nIn [10]: record_data[record_data.bathrooms.isin(structured_data.bathrooms) &amp; record_data.bedrooms.isin(structured_data.bedrooms)]\nOut[10]: \n   bedrooms  bathrooms something_else\n0         1          2              a\n1         2          6              b\n2         2          2              c\n\nIn [28]: { tuple(rec) for rec in record_data[['bedrooms', 'bathrooms']].values.tolist() }\nOut[28]: {(1, 2), (1, 8), (2, 2), (2, 6), (4, 2)}\n\nIn [12]: pd.merge(left=record_data, right=structured_data.drop('something_else', axis=1), how='inner', on=['bedrooms', 'bathrooms'])\nOut[12]: \n   bedrooms  bathrooms something_else\n0         1          2              a\n"
'import numpy as np\n\n# Interpolate using the mean of each row\n\n# Your original data\nx = np.array([[ 1.31,  1.44,  0, 2.51,  0,  0],\n              [ 0,  1.45,  1.63, 2.48,  0,  2.69],\n              [ 1.31,  1.43,  1.59, 2.48,  2.55,  2.71]])\n\nprint(x)\n\nprint()\n\n# for each row in x, set the zero values to the mean of the non zero values\nfor row in x:\n    row[row == 0] = np.mean(row[np.nonzero(row)])\n\nprint(x)\n\n[[ 1.31  1.44  0.    2.51  0.    0.  ]\n [ 0.    1.45  1.63  2.48  0.    2.69]\n [ 1.31  1.43  1.59  2.48  2.55  2.71]]\n\n[[ 1.31        1.44        1.75333333  2.51        1.75333333  1.75333333]\n [ 2.0625      1.45        1.63        2.48        2.0625      2.69      ]\n [ 1.31        1.43        1.59        2.48        2.55        2.71      ]]\n'
"data['setpoint'] = data['oat'].apply(lambda x: -1.33333333 * x + 183.33333333)\n"
'headings = dct[\'alcohol\'] + dct[\'tobacco\'] + dct[\'onset\']\n\n#print(\'my headings:\'+ str(headings))\n\nl1 = [\'Heavy drinking, Low consumption, Former Smoker, Gradual\', \'Low consumption, No consumption, Current on-and-off smoker, Sudden\', \'Heavy drinking, Current on-and-off smoker\']\n\n\nmlb = MultiLabelBinarizer()  # pass sparse_output=True if you\'d like\ndataMatrix = mlb.fit_transform(headings.split(\', \') for headings in l1)\n\nprint("My Classes: ")\nprint(mlb.classes_)\nprint(dataMatrix)\n'
'from minepy import pstats, cstats\n\n... # Load of the data\n\nmicOneVector, ticOneVector = pstats(df)            # Returns mic and tic (Arrays of 1D)\n\nmicTwoVectors, ticTwoVectors = cstats(df, df.MEDV) # Returns mic and tic (Arrays of 1D)\n'
'def featureExtraction(data):\n    vectorizer = TfidfVectorizer(min_df=10, max_df=0.75, ngram_range=(1,3))\n    tfidf_data = vectorizer.fit_transform(data)\n\n    # Here I am returning the vectorizer as well, which was used to generate the training data\n    return vectorizer, tfidf_data\n...\n...\ntfidf_vectorizer, tfidf_data = featureExtraction(data)\n...\n...\n\n# Now using the same vectorizer on test data\nX_test= tfidf_vectorizer.transform(Xnew)\n...\n'
'for idx in range(0,len(x)):\n    print idx,x[idx],transform_binarize[idx]\n\n278 [1L] [1.]\n279 [0L] [0.]\n280 [2L] [1.]\n281 [0L] [0.]\n282 [3L] [1.]\n283 [0L] [0.]\n284 [2L] [1.]\n285 [4L] [1.]\n286 [2L] [1.]\n287 [0L] [0.]\n288 [0L] [0.]\n289 [0L] [0.]\n290 [1L] [1.]\n291 [0L] [0.]\n292 [2L] [1.]\n293 [2L] [1.]\n294 [1L] [1.]\n295 [0L] [0.]\n296 [3L] [1.]\n297 [1L] [1.]\n298 [1L] [1.]\n299 [2L] [1.]\n300 [3L] [1.]\n301 [1L] [1.]\n302 [0L] [0.]     #&lt;--- I think you missed this row while reading your dataset\n'
'from sklearn.neighbors import KNeighborsRegressor\n# define the no. of nearest neighbors k\ntrain_size, train_scores, validation_scores = learning_curve(estimator = KNeighborsRegressor(n_neighbors=k), [...])\n\nfrom sklearn.neighbors import KNeighborsClassifier\n# define the no. of nearest neighbors k\ntrain_size, train_scores, validation_scores = learning_curve(estimator = KNeighborsClassifier(n_neighbors=k), [...])\n'
"df[['start']] =pd.to_datetime(df['start'])\ndf[['end']] =pd.to_datetime(df['end'])\n\ndf['id_connection'] = False\n\nindexes = df.drop_duplicates(subset='user_id', keep='first').index\ndf.loc[indexes,'id_connection'] = True\n\ndiff_ = (df['start'].values[1:] - df['end'].values[:-1]).astype('float')\ntime_criteria_mins = 5\nnew_connection = np.insert(( diff_ / (60*10**9)) &gt; time_criteria_mins, 0, 1)\n\ndf['id_connection'] = (new_connection | df['id_connection']).cumsum()\n\ngb = df.groupby('id_connection').agg({'user_id': 'first', 'start': 'first','end':'last'})\n"
'df = dataset[dataset.date.str.contains(" 2:00:00",regex=False)\n'
"import numpy as np\ndef binner(df,num_bins):\n    for c in df.columns:\n        cbins = np.linspace(min(df[c]),max(df[c]),num_bins+1)\n        df[c + '_binned'] = np.digitize(df[c],cbins)\n    return df\n"
'from datetime import datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import DateFormatter\n\ndf = &lt;set_your_data_frame_here&gt;\n\nmyDates = pd.to_datetime(df[\'Month\'])\nmyValues = df[\'Energy_MWh\']\nfig, ax = plt.subplots()\nax.plot(myDates,myValues)\n\nmyFmt = DateFormatter("%b-%Y")\nax.xaxis.set_major_formatter(myFmt)\n\n## Rotate date labels automatically\nfig.autofmt_xdate()\nplt.show()\n'
"df_train = df_train.reset_index()\n\ndf_train = df_train.reset_index().set_index('A')\n"
"landData = pd.read_csv('Agriculture land area.csv')\nlandData = landData.drop(landData.columns[[0]], axis=1)\n\nlandData_Series = landData.loc[:,landData.columns.values[0]]\n\nOtherDataFrame['NewColumnName'] = landData_Series\n"
'raw_data = raw_data.append(df_temp,ignore_index=True,sort=False)\n'
'df_transposed = df.T\n'
'data = [\n    ["g1", "d1"], \n    ["g1", "d2"],\n    ["g1", "d3"],\n    ["g2", "d2"], \n    ["g2", "d3"]\n]\nf1 = pd.DataFrame(data, columns={"gene", "disease"})\n\n\ndata = [\n    ["g1", "ph1"], \n    ["g1", "ph2"],\n    ["g2", "ph2"], \n    ["g2", "ph3"]\n]\nf2 = pd.DataFrame(data, columns={"gene", "phenotype"})\n\nf1.merge(f2)\n\n\n\nidx gene disease phenotype\n0   g1  d1  ph1\n1   g1  d1  ph2\n2   g1  d2  ph1\n3   g1  d2  ph2\n4   g1  d3  ph1\n5   g1  d3  ph2\n6   g2  d2  ph2\n7   g2  d2  ph3\n8   g2  d3  ph2\n9   g2  d3  ph3\n'
"plt.figure(figsize=(3,4))\nplt.bar(x,y,color = ('darkcyan','green'), width = 0.8)\n"
'df_grouped = df.groupby([\'Perpetrator_Age\', \'Weapon\']).count()\n\nprint(df_grouped)\n                               Perpetrator_Race  Relationship\nPerpetrator_Age Weapon                                       \n15              Blunt Object                  1             1\n27              Knife                         1             1\n36              Rifle                         1             1\n42              Strangulation                 2             2\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style="whitegrid")\n\nax = sns.countplot(x="Perpetrator_Age", hue="Weapon", data=df)\nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles=handles, labels=labels)\nax.set_ylabel("Number of cases")\n\nimport altair as alt\nalt.renderers.enable(\'notebook\')\n\nalt.Chart(df).mark_bar(size=15).encode(\n    alt.Y(\'count(Weapon):Q\', axis=alt.Axis(title=\'Number of cases\')),\n    alt.X(\'Perpetrator_Age:O\', axis=alt.Axis(labelAngle=0)),\n    color=\'Weapon:N\'\n).properties(\n    width=250,\n    height=250\n)\n'
"import pandas as pd\n\n# setup test data\nlog_data = {'Time': ['2019-05-30 00:00:26', '2019-05-30 00:00:50', '2019-05-30 00:05:50','2019-05-30 00:23:26']}\nlog_data = pd.DataFrame(data=log_data)\n\nprogram_data = {'Time': ['2019-05-30 00:00:00', '2019-05-30 00:22:44'],\n            'Program': ['Program 1', 'Program 2']}\nprogram_data = pd.DataFrame(data=program_data)\n\n\ncounts = []\nfor index, row in program_data.iterrows():\n    # get counts on selected range\n    try:\n        log_range = log_data[(log_data['Time'] &gt; program_data.loc[index].values[0]) &amp; (log_data['Time'] &lt; program_data.loc[index+1].values[0])]\n        counts.append(log_range.shape[0])\n    except:\n        log_range = log_data[log_data['Time'] &gt; program_data.loc[index].values[0]]\n        counts.append(log_range.shape[0])\n\n # add aditional column with collected counts\n program_data['Counts'] = counts\n\n                  Time    Program  Counts\n 0  2019-05-30 00:00:00  Program 1       3\n 1  2019-05-30 00:22:44  Program 2       1\n"
"df.groupby(by='STNAME').aggregate({'COUNTY': 'nunique'}).idxmax()[0]\n\n'Texas'\n"
'df = pd.DataFrame(data={"0_A":[0,1,1],\n                       "0_B":[1,0,0],\n                       "0_C":[0,1,1],\n                       "1_A":[0,0,0],\n                       "1_B":[0,1,0],\n                       "1_C":[0,0,1]})\nfinal_df = pd.DataFrame(columns=["A","B","C"])\n\nfor i in final_df:\n    final_df[i] = (df[df.columns[df.columns.str.contains(i)]].max(axis=1))\n\nprint(final_df)\n\n\n    A   B   C\n0   1   1   0\n1   1   1   0\n2   0   1   1\n'
'print("Acc is",sess.run(accuracy,feed_dict={X:mnist.test.images[0:59],Y:mnist.train.labels[0:59]}))\n\nprint("Acc is",sess.run(accuracy,feed_dict:{X:mnist.test.images[0:59],Y:mnist.test.labels[0:59]}))\n'
'len( images * 0.8 ) \n\nlen(images) * 0.8 \n'
"IDF = pd.DataFrame([1.0/len(df.index)]*len(df.index), index = df.index)\nprint(IDF)\n\n\n                0\n11-0.txt     0.166667\n1342-0.txt   0.166667\n1661-0.txt   0.166667\n1952-0.txt   0.166667\n84-0.txt     0.166667\npg16328.txt  0.166667\n\nTF = df.copy()\n\n\n\ndef choice(term, TF, impute_val=0.000001):\n    TF = TF.fillna(impute_val)\n\n    # Based on the formula provided, calculate the TFIDF score for all documents of this term\n    tfidf_score = TF[term].values.ravel() * IDF.values.ravel()\n\n    doc_names = TF.index.tolist()\n    # sort by TFIDF score and return the doc name that has max tfidf value\n    return sorted(zip(doc_names,tfidf_score),key=lambda x: x[1])[-1][0]\n\nprint(choice(term='accept', TF=TF))\n\n'1661-0.txt'\n"
"df.F = df.F.shift(-1)\n\ndf.dropna(inplace=True)\n\n    referencia                   I                   F\n0          111 2019-10-23 23:26:18 2019-10-23 23:42:45\n2          123 2019-10-23 22:38:10 2019-10-23 22:38:19\n4          123 2019-10-23 22:39:08 2019-10-23 22:39:42\n6          123 2019-10-23 22:40:02 2019-10-23 22:40:24\n8          123 2019-10-23 23:24:59 2019-10-26 11:48:07\n10         133 2019-10-23 22:42:14 2019-10-23 22:42:20\n12         156 2019-10-26 11:47:13 2019-10-26 11:47:21\n14         199 2019-10-23 22:44:30 2019-10-23 22:44:38\n16         555 2019-10-23 23:34:35 2019-10-26 11:48:13\n\ndf['duration'] = df.F - df.I\n\n    referencia                   I                   F        duration\n0          111 2019-10-23 23:26:18 2019-10-23 23:42:45 0 days 00:16:27\n2          123 2019-10-23 22:38:10 2019-10-23 22:38:19 0 days 00:00:09\n4          123 2019-10-23 22:39:08 2019-10-23 22:39:42 0 days 00:00:34\n6          123 2019-10-23 22:40:02 2019-10-23 22:40:24 0 days 00:00:22\n8          123 2019-10-23 23:24:59 2019-10-26 11:48:07 2 days 12:23:08\n10         133 2019-10-23 22:42:14 2019-10-23 22:42:20 0 days 00:00:06\n12         156 2019-10-26 11:47:13 2019-10-26 11:47:21 0 days 00:00:08\n14         199 2019-10-23 22:44:30 2019-10-23 22:44:38 0 days 00:00:08\n16         555 2019-10-23 23:34:35 2019-10-26 11:48:13 2 days 12:13:38\n\ndef reformat(grp):\n    tStart = grp.iloc[0, 2]\n    tEnd = grp.iloc[1, 2]\n    return pd.Series(grp.iloc[0, 0:3].tolist() + [tEnd, tEnd - tStart],\n        index=['utilizador', 'referencia', 'start', 'end', 'duration'])\n\ndf = df.groupby(np.arange(len(df.index)) // 2).apply(reformat)\n\n  utilizador  referencia               start                 end duration\n0         AG         123 2019-10-23 22:38:10 2019-10-23 22:38:19 00:00:09\n1         AG         123 2019-10-23 22:39:08 2019-10-23 22:39:42 00:00:34\n2         AG         123 2019-10-23 22:40:02 2019-10-23 22:40:24 00:00:22\n3         AG         133 2019-10-23 22:42:14 2019-10-23 22:42:20 00:00:06\n4         AG         199 2019-10-23 22:44:30 2019-10-23 22:44:38 00:00:08\n5         AG         123 2019-10-23 23:24:59 2019-10-23 23:26:18 00:01:19\n6         AG         555 2019-10-23 23:34:35 2019-10-23 23:42:45 00:08:10\n7         AA         156 2019-10-26 11:47:13 2019-10-26 11:47:21 00:00:08\n8         AG         123 2019-10-26 11:48:07 2019-10-26 11:48:13 00:00:06\n\ndf = df.sort_values(['utilizador', 'referencia', 'time_stamp'])\\\n    .groupby(np.arange(len(df.index)) // 2).apply(reformat)\n\n  utilizador  referencia               start                 end        duration\n0         AA         156 2019-10-26 11:47:13 2019-10-26 11:47:21 0 days 00:00:08\n1         AG         111 2019-10-23 23:26:18 2019-10-23 23:42:45 0 days 00:16:27\n2         AG         123 2019-10-23 22:38:10 2019-10-23 22:38:19 0 days 00:00:09\n3         AG         123 2019-10-23 22:39:08 2019-10-23 22:39:42 0 days 00:00:34\n4         AG         123 2019-10-23 22:40:02 2019-10-23 22:40:24 0 days 00:00:22\n5         AG         123 2019-10-23 23:24:59 2019-10-26 11:48:07 2 days 12:23:08\n6         AG         133 2019-10-23 22:42:14 2019-10-23 22:42:20 0 days 00:00:06\n7         AG         199 2019-10-23 22:44:30 2019-10-23 22:44:38 0 days 00:00:08\n8         AG         555 2019-10-23 23:34:35 2019-10-26 11:48:13 2 days 12:13:38\n"
'print(id(d1) is id(d2))\n'
' def LogLikelihood(self):\n     n = self.num_obs\n     k = self.num_features\n     residuals = self.residuals\n\n     ll = -(n * 1/2) * (1 + np.log(2 * np.pi)) - (n / 2) * np.log(residuals.dot(residuals) / n)\n\n     return ll\n\n  def AIC_BIC(self):\n    ll = self.LogLikelihood()\n    n = self.num_obs\n    k = self.num_features + 1\n\n    AIC = (-2 * ll) + (2 * k)\n    BIC = (-2 * ll) + (k * np.log(n))\n\n    return AIC, BIC\n'
"&gt;&gt;&gt; df1 = pd.melt(df, id_vars=['id'], var_name='value_type')\n&gt;&gt;&gt; df1\n   id value_type  value\n0   1    A_value     50\n1   2    A_value     33\n2   1    D_value     60\n3   2    D_value     45\n\n&gt;&gt;&gt; df1.value_type = df1.value_type.str.extract(r'(\\w)_')\n&gt;&gt;&gt; df1\n   id value_type  value\n0   1          A     50\n1   2          A     33\n2   1          D     60\n3   2          D     45\n"
'names = [\'unixTime\', \'sampleAmount\',\'Time\',\'samplingRate\', \'Data\']\ndata = pd.read_csv("project_fan.csv", sep=\';\', error_bad_lines = False, names=names) \n'
"df['Previous Day'] = df.sort_values('Date').groupby('Student')['Average'].shift()*.90\n\ndf['Indicator'] = np.where(df['Average']&gt;df['Previous Day'],'High','Low')\n\ndf\n\n        Date Student  Average  Previous Day Indicator\n0 2020-01-17    Alex       40           NaN       Low\n1 2020-01-18    Alex       50          36.0      High\n2 2020-01-19    Alex       80          45.0      High\n3 2020-01-20    Alex       70          72.0       Low\n4 2020-01-17    Jeff       10           NaN       Low\n5 2020-01-18    Jeff       50           9.0      High\n6 2020-01-19    Jeff       80          45.0      High\n7 2020-01-20    Jeff       60          72.0       Low\n"
"import dask.bag as db\nfrom other_functions import *\n\ninput = db.read_text(file1)\nprocessing_parameter = parse_mapping_parameters(file2)\n\nto_compute = []\nintermediates = []\nfor i, p in enumerate(mapping_parameter):\n    intermediate = input.map(lambda x: process(x, p))\n    to_compute.append(\n        intermediate.to_textfiles(f'./data/intermediate_{i}.*.txt', compute=False)\n    intermediates.append(intermediate)\n\nproducts = intermediates.pop(0)\nfor intermediate in intermediates:\n    products = product.products(i)\n\nresult = products.map(calc_result)\n\nto_compute.append(result.to_textfiles(f'./data/result.*.txt', compute=False))\ndask.compute(*to_compute)\n"
'X_train = X_train.shape\nX_test = X_test.shape\ny_train = y_train.shape\ny_test = y_test.shape\n'
'disp = plot_confusion_matrix(classifier, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize,\n                                   values_format="d")\n'
"minStart = df.loc[df['Random_Number'] &lt; df.quantile(0.3)[0]].index[0]\n\nmaxStart = df.loc[df['Random_Number'] &gt; df.quantile(0.90)[0]].index[0]\n\nhours = maxStart - minStart\nhours\n"
"df.to_sql('mytable', conn, if_exists='replace', index=True,dtype={'NAME': types.VARCHAR(df.NAME.str.len().max())})\n"
'stopwords = [\n    "a", "about", "and", "across", "after",\n    "afterwards", "in", "on", "as",\n]\n\n# Compile a regular expression that will match all the words in one sweep\nstopword_re = re.compile("|".join(r"\\b%s\\b" % re.escape(word) for word in stopwords))\n\n# Replace and reassign into the column\ndf["content"].replace(stopword_re, "", inplace=True)\n\n'
"df = pd.melt(df, id_vars=['Year'])\n"
"df_test = pd.DataFrame({\n    'name': [np.NaN, np.NaN, '29014', '21893', 'Amber Rose']\n})\n\ndf_test['name'] = np.where(\n    pd.to_numeric(df_test['name'], errors='coerce').isnull()==False,\n    np.NaN,\n    df_test['name']\n)\n\ndf_test\n    name\n0   NaN\n1   NaN\n2   NaN\n3   NaN\n4   Amber Rose\n"
"df.sort_values(['Name','c_2','c_3']).groupby('Name').agg('first')\n\n       c_1  c_2  c_3\nName                \nJohn     a    1    2\nPhilip   g    3    1\nSean     f    2    1\n\ndf['Name'] = pd.Categorical(df.Name, categories=df.Name.unique())\ndf.sort_values(['Name','c_2','c_3']).groupby('Name').agg('first')\n\n       c_1  c_2  c_3\nName                \nJohn     a    1    2\nSean     f    2    1\nPhilip   g    3    1\n"
"df['new'] = df.loc[:, 'W'] + df.loc[:, 'Y'] \n"
"# sample data\nnew_dict = {str(i): np.linspace(0,1,100)**i for i in range(40)}\n\nfig, axes = plt.subplots(5,8,sharex=True,sharey=True,figsize=(8,5))\n\n#flatten axes so you can access axes[i]\naxes = axes.ravel()\n\nfor i, (key, value) in enumerate(new_dict.items()):\n    print(i, key, value)\n    axes[i].plot(value)\n    axes[i].set(title=key.upper(), xlabel='ns')\nplt.show()\n"
"df.x.str.findall('\\d+').explode().astype(float).mean(level=0)\n0      6.5\n1     15.0\n2     14.5\n3     15.0\n4     15.0\n71     1.5\n72     0.5\n73     4.5\n74     0.5\n75     0.5\nName: x, dtype: float64\n"
"import pandas as pd\n\n#data\nd = {'col1': [0, 1, 0.170531, 0.170533, 0.170531],\n 'col2': [0, 0, 0.005285, 0.005285, 0.005285],\n 'col3': [0, 0, 0.047557, 0.047557, 0.047557],\n 'col4': [1, 0, 0.482381, 0.003104, 0.482381],\n 'col5': [0, 0, 0.003104, 0.482458, 0.003104],\n 'col6': [0, 0, 0.001109, 0.001108, 0.001109]}\n\n#create dataframe\ndf = pd.DataFrame(data = d)\n\n#list of columns\ncolumns = df.columns.tolist()\n\n#loop over columns\nfor col in columns:\n    #change to 1 if value equals to the max in that row\n    df[col] = df[col].apply(lambda x:1 if (x==df.max(axis=1)).any() else 0)\n\nprint(df)\n   col1  col2  col3  col4  col5  col6\n0     0     0     0     1     0     0\n1     1     0     0     0     0     0\n2     0     0     0     1     0     0\n3     0     0     0     0     1     0\n4     0     0     0     1     0     0\n"
"import random\n\nfor n in [1,2,3,4,5,6,7,8]:\n    locals()[f'random_pop{n}'] = []\n    for m in range(10):\n        locals()[f'a{n}{m}'] = random.randint(1000,9000) # Defines variable a0,a1,a2...a9\n        locals()[f'random_pop{n}'].append(locals()[f'a{n}{m}'])\n\nrandom_pop_list = [random_pop1, random_pop2, random_pop3, random_pop4, random_pop5, random_pop6, random_pop7, random_pop8]\n"
'# Cleaning function  \ndef cleaner(df, dict):\n    for col in df.columns:\n        if col in dict.keys(): \n            for row in df.index:\n                if type(re.match(dict[col], str(df[col][row]))) is re.Match:\n                    df[col][row] = df[col][row] \n                    print(df[col][row])\n                else:\n                    df[col][row] = np.nan\n    return(df)\nprint(cleaner(df1, dict1))\ncleaned_df = cleaner(df1, dict1)\n'
"  patterns responses patterns.1 responses.1 patterns.2 responses.2\n0    hello        hi     Where?        here      When?       there\n\ncols = df.columns\nnPairs = len(cols) // 2\nh1 = [ f'Q{i}' for i in range(1, nPairs + 1) ]\ndf.columns = pd.MultiIndex.from_product([h1, cols[:2]])\n\n        Q1                 Q2                 Q3          \n  patterns responses patterns responses patterns responses\n0    hello        hi   Where?      here    When?     there\n\ndf = pd.read_csv('Input.csv', index_col=[0])\n"
"from sklearn.feature_extraction.text import CountVectorizer\ntext = ['this is a sentence', 'this is another sentence', 'not a sentence']\n\nvector = CountVectorizer(analyzer= 'word', tokenizer= None, max_features= 4500)\ndt = vector.fit_transform(text)\n\nprint(vector.vocabulary_) = {'this': 4, 'is': 1, 'sentence': 3, 'another': 0, 'not': 2}\n\ndata_features = vectorizer.fit_transform(text)\nprint(data_features.toarray())\n= [[0 1 0 1 1]\n [1 1 0 1 1]\n [0 0 1 1 0]]\n\n[0, 0, 0, 0, 0].\n\n[0            1(is)       0          1(sentence)           1(this)]\n[1(another)   1(is)       0          1(sentence)           1(this)]\n[0            0           1(not)     1(sentence)           0      ]\n"
"load_var = pd.read_csv(r'path')    \nload_var['End Seconds'] = load_var['End Seconds'].apply(convert)\n"
"import networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pydot\nfrom IPython.display import Image\n\n\ndic_values = {&quot;Source&quot;:[24120.0,24120.0,24120.0], &quot;Interlocutor&quot;:[34,34,34],\n              &quot;Frequency&quot;:[446625000, 442475000, 445300000]}\n\nsession_graph = pd.DataFrame(dic_values)\nsources = session_graph['Source'].unique()\ntargets = session_graph['Interlocutor'].unique()\n\n\n#create a Multigraph and add the unique nodes\nG = nx.MultiDiGraph()\nfor n in [sources, targets]:\n    G.add_node(n[0])\n    \n#Add edges, multiple connections between the same set of nodes okay. \n# Handled by enum in Multigraph    \n\n#Itertuples() is a faster way to iterate through a Pandas dataframe. Adding one edge per row\nfor row in session_graph.itertuples():\n    #print(row[1], row[2], row[3])\n    G.add_edge(row[1], row[2], label=row[3])        \n        \n\n#Now, render it to a file...\np=nx.drawing.nx_pydot.to_pydot(G)\np.write_png('multi.png')\nImage(filename='multi.png') #optional \n"
"import pandas as pd\nimport matplotlib.pyplot as plt\n\n#converting column names to string\ndf.columns = df.columns.astype(str)\n\n#plotting a bar plot\ndf['1970'].plot.bar()\nplt.show()\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame([[1970,'a',1],\n                   [1970,'b',2],\n                   [1971,'a',2],\n                   [1971,'b',3]],\n                   columns=['year','location', 'value'])\ndf = pd.pivot_table(df, values='value', index='location', columns='year')\ndf.columns = df.columns.astype(str)\ndf['1970'].plot.bar()\n\nplt.show()\n"
"# examDate column\n\nn_stud = 20   # mention the number of students per batch here\nn_batch = 2   # mention the number of batches per day here\n\ntemp = data['TH_CENT_CH'].value_counts().sort_index().reset_index()  # storing centers and their counts in a temp variable\ncent = temp['index'].to_list()      # storing centers in a list\ncnt = temp['TH_CENT_CH'].to_list()  # storing counts in a list\ncent1 = []\ncnt1 = []\nj = 0\n\n# for loops to repeat each center by count times\nfor c in cent:\n    for i in range(1, cnt[j] + 1):\n        cent1.append(c)\n        cnt1.append(i)\n    j += 1\n\ndf1 = pd.DataFrame(list(zip(cent1, cnt1)), columns = ['cent','cnt'])  # dataframe to store the centers and new count list\n\ncounts = df1['cnt'].to_list() # storing the new counts in a list\nhelper = {}  # helper dictionary\nmax_no = max(cnt)\n\n# for-while loops to map helper number to each counts number\nfor i in counts:\n    j = 0\n    while(j &lt; (round(max_no / (n_stud * n_batch)) + 1)):\n        if((i &gt; (n_stud * n_batch * j)) &amp; (i &lt; (n_stud * n_batch * (i + 1)))):\n            helper[i] = j\n        j += 1\n\n# mapping the helper with counts\ncounts = pd.Series(counts)\nhelper = pd.Series(helper)\nhel = counts.map(helper).to_list()\ndf1['helper'] = hel\n\nexamDate = {}  # dictionary to store exam dates\n\n# for loop to map dates to each helper number\nfor i in hel:\n    examDate[i] = pd.to_datetime(date(2020, 12, 1) + timedelta(days = (2 * i)))\n\n# mapping the dates with helpers\nhel = pd.Series(hel)\nexamDate = pd.Series(examDate)\nexam = hel.map(examDate).to_list()\ndf1['examDate'] = exam\n        \n# adding the dates to the original dataframe\nexamDate = df1['examDate'].to_list()\ndata['examDate'] = examDate\ndata['examDate']\n\n        Rollno  cent_allot  cent_add  examDate   batch  rep_time\n0     NL2000001        ADI   NL ADI  2020-12-01      1  09:00:00\n1     NL2000002        ADI   NL ADI  2020-12-01      1  09:00:00\n2     NL2000003        ADI   NL ADI  2020-12-01      1  09:00:00\n3     NL2000004        ADI   NL ADI  2020-12-01      1  09:00:00\n4     NL2000005        ADI   NL ADI  2020-12-01      1  09:00:00\n         ...        ...      ...         ...    ...       ...\n3345  NL2003346        WGOD  NL WGOD 2020-12-03      1  09:00:00\n3346  NL2003347        WGOD  NL WGOD 2020-12-04      1  09:00:00\n3347  NL2003348         KRS  NL KRS  2020-12-05      1  09:00:00\n3348  NL2003349        WGOD  NL WGOD 2020-12-02      1  09:00:00\n3349  NL2003350        WGOD  NL WGOD 2020-12-04      1  09:00:00\n\n# batch column\n\ncounts = df1['cnt'].to_list()  # storing the new counts in a list\nhelper2 = {}  # helper dictionary\n\n# for-while loops to map helper number to each counts number\nfor i in counts:\n    j = 0\n    while(j &lt; (round(max_no / (n_stud)) + 1)):\n        if((i &gt; (n_stud * j)) &amp; (i &lt; (n_stud * (i + 1)))):\n            helper2[i] = j\n        j += 1\n\n# mapping the helper with counts\ncounts = pd.Series(counts)\nhelper2 = pd.Series(helper2)\nhel2 = counts.map(helper2).to_list()\ndf1['helper2'] = hel2\n\nbatch = {}   # dictionary to store batch numbers\n\n# for loop to map batch numbers to each helper number\nfor i in hel2:\n    if(i % 2 == 0):\n        batch[i] = 1\n    else:\n        batch[i] = 2\n        \n# mapping the batches with helpers\nhel2 = pd.Series(hel2)\nbatch = pd.Series(batch)\nbat = hel2.map(batch).to_list()\ndf1['batch'] = bat\n\n# adding the batches to the original dataframe\nbatch = df1['batch'].to_list()\ndata['batch'] = batch\ndata['batch'].unique()\n\n# rep_time column\ndata.loc[data['batch'] == 1, 'rep_time'] = '9:00 AM'\ndata.loc[data['batch'] == 2, 'rep_time'] = '2:00 PM'\ndata['rep_time'].unique()\n"
"In [217]: np.array([[1,1,1,1,1,1],[1,1,2,1,1,1,1]])\n     ...: \n&lt;ipython-input-217-a5da97fcce54&gt;:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  np.array([[1,1,1,1,1,1],[1,1,2,1,1,1,1]])\nOut[217]: \narray([list([1, 1, 1, 1, 1, 1]), list([1, 1, 2, 1, 1, 1, 1])],\n      dtype=object)\nIn [218]: x=np.array([[1,1,1,1,1,1],[1,1,2,1,1,1,1]], object)\n     ...: \n     ...: \nIn [219]: \nIn [219]: x\nOut[219]: \narray([list([1, 1, 1, 1, 1, 1]), list([1, 1, 2, 1, 1, 1, 1])],\n      dtype=object)\nIn [220]: len(x[0])\nOut[220]: 6\nIn [221]: len(x[1])\nOut[221]: 7\n"
"def parseCSV(filePath):\n      data = pd.read_csv(filePath)\n      data.to_sql('addresses', 'yoursqlconnection')\n"
"import cudf\nimport numpy as np #only to create a really large array to test scale\n\n### Your Original data\n# data = { \n#         &quot;EVENT_1&quot;:[0,0,0,0,0,0,0,0,1,1],\n#         &quot;EVENT_2&quot;:[3,3,3,5,5,5,9,9,6,6]\n#     }\n\n### your data at scale (10,000,000 rows)    \ndata = {\n    &quot;EVENT_1&quot;:np.random.default_rng().integers(0,10,10000000),\n    &quot;EVENT_2&quot;:np.random.default_rng().integers(12,20,10000000)\n}\ndf = cudf.DataFrame(data)\n\nfrom collections import defaultdict\n\ndef ngroup_test(df, col1, col2, col3):\n    df[col3] = df[col1].astype(str) + ',' + df[col2].astype(str)\n    mapping = {}\n    d = {}\n    last_index = {}\n    for marker in df[col3].unique().to_array():\n        first, second = marker.split(',')\n        if first not in d:\n            d[first] = {second: 0}\n            last_index[first] = 1\n        elif second not in d[first]:\n            d[first][second] = last_index[first]\n            last_index[first] += 1\n        mapping[marker] = d[first][second]\n\n    col_to_insert = list(map(lambda x: mapping[x], list(df[col3].to_array())))\n    df[col3] = col_to_insert\n    return df\n\ndf1 = ngroup_test(df, 'EVENT_1', 'EVENT_2', 'EVENT_2A')\ndf1\n"
'            Actual_price  predict_price  Error          Relative Error\n4928          162000         165994  -3994.343750         2.5%\n11272         31000          50525   -19525.128906        62,9%\n7894          110000         117209  -7209.609375         6,5%\n4382          59500          75478   -15978.164062        26,5%\n345           500000         482369   17630.968750        3,5%\n...             ...            ...           ...\n3348          42750          38110    4639.328125         10,8%\n8993          74000          96511   -22511.226562        30%\n8270          83750          74911    8838.210938         10%\n2757          77500          89780   -12280.585938        15%\n6538          95000          92607    2392.765625         2,5%\n'
"labels = kM.predict(V1_V2)\n\nplt.scatter(normed_V1, normed_V2, s=10, c=labels, cmap='coolwarm') # instead of c=bn_class\n"
"smf.quantreg('yValue ~ xValue1 + xValue2 + xValue3', df).fit(q=0.9)\n"
'sum(a * b + 1 for a, b in zip(l1, l2))\n\n&gt;&gt;&gt; l1 = [1,2,3]; l2 = [4,5,6]\n&gt;&gt;&gt; sum(a * b + 1 for a, b in zip(l1, l2))\n35\n'
'from itertools import product, chain, combinations, permutations\n\ngroups = [[1,2,3],[4,5,6],[7,8,9]]\ncounts = (2, 2, 1)\n\nselections = [combinations(g, c) for g, c in zip(groups, counts)]\n\nfor n_tuple in product(*selections):\n    print(tuple(chain.from_iterable(n_tuple)))\n\n(1, 2, 4, 5, 7)\n(1, 2, 4, 5, 8)\n(1, 2, 4, 5, 9)\n(1, 2, 4, 6, 7)\n(1, 2, 4, 6, 8)\n(1, 2, 4, 6, 9)\n(1, 2, 5, 6, 7)\n(1, 2, 5, 6, 8)\n(1, 2, 5, 6, 9)\n(1, 3, 4, 5, 7)\n(1, 3, 4, 5, 8)\n(1, 3, 4, 5, 9)\n(1, 3, 4, 6, 7)\n(1, 3, 4, 6, 8)\n(1, 3, 4, 6, 9)\n(1, 3, 5, 6, 7)\n(1, 3, 5, 6, 8)\n(1, 3, 5, 6, 9)\n(2, 3, 4, 5, 7)\n(2, 3, 4, 5, 8)\n(2, 3, 4, 5, 9)\n(2, 3, 4, 6, 7)\n(2, 3, 4, 6, 8)\n(2, 3, 4, 6, 9)\n(2, 3, 5, 6, 7)\n(2, 3, 5, 6, 8)\n(2, 3, 5, 6, 9)\n'
'test_data[columns_list] = scaler.transform(test_data[columns_list])\n\n# Predict Quantity from model\ny_pred = clf.predict(test_data.iloc[:, :-2])\n\n# Actual values of Quantity\ny_true = test_data.iloc[:,-1]\n\n# Now use y_true and y_pred to calculate the metrics.\n'
"df1 = df.loc[df['country'].isin(['USA', 'Germany']), ['country', 'beer_servings']]\n\ndf = df.set_index('country')\n"
"import sys\nimport pathlib\nfrom collections import OrderedDict\nimport ruamel.yaml\n\nyaml_file = pathlib.Path('input.yaml')\nyaml = ruamel.yaml.YAML()\nyaml.default_flow_style = None \ndata = yaml.load(yaml_file)\nindexed = OrderedDict()\nfor elem in data['index']:\n    for k in elem:  # just one each\n        single_item_map = indexed.setdefault(k, OrderedDict())\n        single_item_map[elem[k]] = None  # arbitrary value, unused\nl = []\nfor elem in indexed:\n    l.append({elem: [k for k in indexed[elem]]})\ndata['index'] = l\nyaml.dump(data, sys.stdout)\n\nindex:\n- BANKNIFTY_O_C_0_09_W: [books, trends, trades, relations]\n- BANKNIFTY_O_P_0_09_W: [books, trends, trades, negrelations]\n- BANKNIFTY_O_C_0_10_W: [books, trends, trades, relations, options_banknifty_weekly]\n- BANKNIFTY_O_P_0_10_W: [books, trends, trades, negrelations]\n- BANKNIFTY_F_0: [books, trends, trades, relations]\n- NIFTY_F_0: [books, trends, trades, relations]\n"
'next_price = model.predict(self.current_price)\n'
"for index, val in df['_id'].items():\n    print(val)\n"
'X = np.arange(10)\ny = np.concatenate((np.ones(5), np.zeros(5)))\nX\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny\narray([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\nX_train\narray([5, 0, 7, 2, 9, 4, 3, 6])\nX_test\narray([8, 1])\nkf = Kfold(n_splits=5)\nfor train, test in kf.split(X_train):\n    print(train, test)\n[2 3 4 5 6 7] [0 1]\n[0 1 4 5 6 7] [2 3]\n[0 1 2 3 6 7] [4 5]\n[0 1 2 3 4 5 7] [6]\n[0 1 2 3 4 5 6] [7]\n'
"df['age'] = df['age'].fillna(df.groupby('animal')['age'].transform('median'))\nprint (df)\n  animal   age  weight priority\na    cat  2.50       1      yes\nb    cat  1.00       3      yes\nc    dog  0.50       6       no\nd    dog  3.25       8      yes\ne    cat  5.00       4       no\nf    cat  2.00       3       no\ng    dog  3.50      10       no\nh    cat  2.25       2      yes\ni    dog  7.00       7       no\nj    dog  3.00       3       no\n\nprint (df.groupby('animal')['age'].transform('median'))\na    2.25\nb    2.25\nc    3.25\nd    3.25\ne    2.25\nf    2.25\ng    3.25\nh    2.25\ni    3.25\nj    3.25\nName: age, dtype: float64\n"
"import numpy as np\n\nclass_1 = [0.5, 0.4, 0.1, 0.0]\nclass_2 = [0.0, 0.4, 0.6, 0.0] \nclass_3 = [0.5, 0.4, 0.05, 0.05] \nclass_combined = np.array([class_1, class_2, class_3])\nclass_combined\n\n# VotingClassifier(voting='hard')\nhard_voting = np_matrix.argmax(axis=1)\nhard = np.bincount(voting).argmax()\n0\n\n# VotingClassifier(voting='soft')\nsoft_sum = class_combined.sum(axis=0)\nsoft = soft_sum.argmax()\n1\n"
"plt.xlim(60,200)\nplt.ylim(60,200)\n\n# create a figure object    \nfig = plt.figure()\n# create two axes within the figure and arrange them on the grid 1x2\nax1 = fig.add_Subplot(121)\n# ax2 is the second set of axes so it is 1x2, 2nd plot (hence 122)\n# they won't have the same limits this way because they are set up as separate objects, whereas in your example they are the same object that is being re-purposed each time!\nax2 = fig.add_Subplot(122)\n\nax1.plot(X1,Y1)\nax2.plot(X2,Y2)\n"
'# filter the data in `ur`\nf_5s_ratings = ur.loc[(ur.gender == \'F\')&amp;(ur.rating == 5)]\n\n# count rows per `movie_id`\nabs_num_f_5s_ratings = f_5s_ratings.groupby("movie_id").size()\n\nmovie_id\n1       253\n2        15\n3        14\n...\n'
"dataset = pd.read_cvs(C:/Users/'username'/Desktop/input/train.cvs)\n"
"map.set_index('CategoryId',inplace=True)\ndata['Category'] = data['CategoryId'].map(map['Category'],na_action=np.nan)\n\ndata = data.merge(map,how='left',on='CategoryId')\n\ndata['Category'] = data.CategoryId.map(dict(map.values),na_action=np.nan)\n\ndata['Category'] = data.CategoryId.replace(dict(map.values))\n\nprint(data)\n      CategoryId  Size Category\n    1    n013523  0.40    Snake\n    2    n013523  0.80    Snake\n    3    n013523  0.15    Snake\n    4    n012837  0.16   Iguana\n    5    n012837  0.23   Iguana\n    6    n012837  0.42   Iguana\n"
'for row in reader:\n        data.append(np.array([int(x) for x in row[5:]]))\n'
'df.unstack().plot()\n'
'a = np.array([[0,0,0],[1,1,1]])\nb = np.array([[1,1,1],[2,2,2],[3,3,3]])\n\nc = b.sum(axis=0) + a\n\nprint(c)\n\narray([[6, 6, 6],\n       [7, 7, 7]])\n'
'$ conda activate my_env\n$ (my_env) conda install ipykernel -y\n$ (my_env) python -m ipykernel install --user --name my_env --display-name "My Env"\n\n$ (my_env) jupyter kernelspec list | grep my_env\n'
"import pandas as pd\nimport numpy\n\nd = ({             \n   'Day' : ['Monday','Thursday','Friday','Monday','Monday','Saturday','Thursday','Friday','Friday','Wednesday'],                                                                      \n   'Distance' : [1.498991,5.122769,1.492705,1.972825,2.517838,1.648552,2.503511,1.671742,3.974399,7.616923],                                                               \n   'Group' : [0,0,0,0,0,1,0,0,0,0],                                                                              \n    })\n\ndf = pd.DataFrame(data=d)\n\n#The average distance for Weekdays\nWeekday = df.loc[df['Day'] == 0]\nWeekday_mean = Weekday['Distance'].mean()\n\n#The average distance for Weekends\nWeekend = df.loc[df['Day'] == 1]\nWeekend_mean = Weekend['Distance'].mean()\n\n#Correlation between Weekends and Weekdays\ncorr = (numpy.corrcoef(df['Distance'],df['Day']))\n\nprint(corr)\n\n[[ 1.         -0.23640194]\n [-0.23640194  1.        ]]\n"
'[20,4,5] , [200,1,5]\n\n[1,0.20,0.25] and [1,0.005,0.025]\n'
"# Your case\narray([1,2], dtype=float32)\n\n# Output\nNameError: name 'array' is not defined\n\n\n#Correct case\nnp.array([1,2], dtype='float32')\n\n# Output\narray([1., 2.], dtype=float32)\n\ntest = [np.array([[1083.8748]], dtype='float32'), np.array([[998.98773]], dtype='float32'), np.array([[1137.0487]], dtype='float32'), np.array([[1077.2798]], dtype='float32'), np.array([[926.41284]], dtype='float32'),\nnp.array([[1030.7125]], dtype='float32'), np.array([[1028.0048]], dtype='float32'), np.array([[523.9799]], dtype='float32'), np.array([[1125.092]], dtype='float32'), np.array([[1119.7738]], dtype='float32'),\nnp.array([[918.6966]], dtype='float32'), np.array([[1112.5186]], dtype='float32'), np.array([[555.6942]], dtype='float32'), np.array([[1096.5643]], dtype='float32'), np.array([[826.35657]], dtype='float32'),\nnp.array([[1014.35406]], dtype='float32'), np.array([[1027.6962]], dtype='float32'), np.array([[924.20087]], dtype='float32'), np.array([[1035.217]], dtype='float32'), np.array([[1008.9658]], dtype='float32'),\nnp.array([[970.54047]], dtype='float32'), np.array([[847.0671]], dtype='float32'), np.array([[913.5519]], dtype='float32'), np.array([[1047.0747]], dtype='float32'), np.array([[920.0606]], dtype='float32'),\nnp.array([[994.2266]], dtype='float32'), np.array([[991.4501]], dtype='float32'), np.array([[972.43256]], dtype='float32'), np.array([[934.8802]], dtype='float32'), np.array([[912.04004]], dtype='float32'), np.array([[1131.297]], dtype='float32')]\n\ndf = pd.Series(test)\nprint(df.head())\n\n0    [[1083.8748]]\n1    [[998.98773]]\n2    [[1137.0487]]\n3    [[1077.2798]]\n4    [[926.41284]]\n"
"df['new_col'] = df['Mins'].sub(3).clip(0)\n"
"result = df[(Dataframe(df['mentions'].tolist()) == '@foo').any(1)]\n"
"iris['species'] is iris.species\n# True\n"
"2019-09-30 10:37:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https://www.indeed.com/rc/clk?jk=69995bf12d9f2f9a&amp;fccid=b87e01ade6c824ee&amp;vjs=3&gt; from &lt;GET http://www.indeed.com/rc/clk?jk=69995bf12d9f2f9a&amp;fccid=b87e01ade6c824ee&amp;vjs=3&gt;\n\n2019-09-30 10:37:06 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to &lt;GET https://www.indeed.com/viewjob?jk=69995bf12d9f2f9a&amp;from=serp&amp;vjs=3&gt; from &lt;GET https://www.indeed.com/rc/clk?jk=69995bf12d9f2f9a&amp;fccid=b87e01ade6c824ee&amp;vjs=3&gt;\n\n2019-09-30 10:37:07 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://www.indeed.com/viewjob?jk=69995bf12d9f2f9a&amp;from=serp&amp;vjs=3&gt; (referer: None)\n\n2019-09-30 10:37:07 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://www.indeed.com/viewjob?jk=69995bf12d9f2f9a&amp;from=serp&amp;vjs=3&gt; \n{'title': 'General Manager', 'posting_url': 'https://indeed.com/rc/clk?jk=69995bf12d9f2f9a&amp;fccid=b87e01ade6c824ee&amp;vjs=3', 'job_name': 'General Manager', 'job_location': 'Plano, TX 75024', 'job_description_1': [], 'posted_on_date': ' - 30+ days ago', 'job_description_2': None}\n"
'rf = RandomForestClassifier(max_features=.1)\n\nbase = DecisionTreeClassifier(max_features=.1)    \nrf = BaggingClassifier(base_estimator=base, max_samples=.25)\n'
"df.groupby(['sex','type'])['weight'].mean().filter(like='Obese') \n\nsex  type \nM    Obese    305\nName: weight, dtype: int64\n"
'tokenized_texts = [text.split() for text in texts]\nretokenized_texts = [list(combining_generator(tokens, combinations_dict)) for tokens in tokenized_texts]\nretokenized_texts\n'
'import ast, json\n\nprint (df)\n                                               Table\n0  {"id":"v100","signal":"2017-10-02 03:30:00","m...\n1  {"id":"v100","signal":"2017-10-02 06:30:00","m...\n2  {"id":"v400","signal":"2017-10-03 03:30:00","m...\n3  {"id":"v400","signal":"2017-10-04 05:30:00","m...\n\n\ndf = pd.DataFrame([ast.literal_eval(x) for x in df[\'Table\']])\n#alternative\n#df = pd.DataFrame([json.loads(x) for x in df[\'Table\']])\nprint (df)\n     id               signal     mode\n0  v100  2017-10-02 03:30:00   online\n1  v100  2017-10-02 06:30:00   online\n2  v400  2017-10-03 03:30:00   online\n3  v400  2017-10-04 05:30:00  offline\n\ndf = pd.read_json(file, lines=True)\n'
'   from gensim.models import Word2Vec, KeyedVectors\n   from gensim.models.word2vec import Text8Corpus\n\n\n   params = {\n     \'alpha\': 0.05,\n     \'size\': 100,\n     \'window\': 5,\n     \'iter\': 5,\n     \'min_count\': 5,\n     \'sample\': 1e-4,\n     \'sg\': 1,\n     \'hs\': 0,\n     \'negative\': 5\n  }\n  model = Word2Vec(Text8Corpus(text8_path), **params)\n  print(model)\n  from gensim.similarities.index import AnnoyIndexer\n\n  annoy_index = AnnoyIndexer(model, 100)\n\n  vector = model.wv["cats"]\n\n  approximate_neighbors = model.wv.most_similar([vector], topn=11, \n  indexer=annoy_index)\n\n  print("Approximate Neighbors")\n  for neighbor in approximate_neighbors:\n     print(neighbor)\n\nApproximate Neighbors\n(\'cats\', 1.0)\n(\'wallabies\', 0.6341749131679535)\n(\'coyotes\', 0.6311245858669281)\n(\'kangaroos\', 0.6296325325965881)\n(\'felines\', 0.6287126243114471)\n(\'squirrels\', 0.6270308494567871)\n(\'dogs\', 0.6266725659370422)\n(\'leopards\', 0.6130028069019318)\n(\'omnivores\', 0.6129975318908691)\n(\'koalas\', 0.612080842256546)\n(\'microbats\', 0.6070675551891327)\n'
"df_ = df.select_dtypes(exclude=['int', 'float'])\nfor col in df_.columns:\n    print(df_[col].unique()) # to print categories name only\n    print(df_[col].value_counts()) # to print count of every category\n"
'predictions = []\nfor train_row in test_data:\n    distances = ...\n    predictions.append(train_tgt[distances])\n\npredictions = []\nfor train_row in test_data:\n    predictions.append(train_tgt[...])\n\npredictions = [train_tgt[...] for train_row in test_data]\n'
"current_event = None\nresult = []\nfor event, time in zip(data['Event'], data['Time']):\n    if event != current_event:\n        if current_event is not None:\n            result.append([current_event, start_time, time])\n        current_event, start_time = event, time\ndata = pandas.DataFrame(result, columns=['Event','EventStartTime','EventEndTime'])\n"
"\n# user_filter filters the dataframe to all the rows where\n# 'userID' is NOT in the 'users' column (the value of which\n# is a list type)\nuser_filter = df['users'].map(lambda u: userID not in u)\n\n# cuisine_filter filters the dataframe to only the rows\n# where 'cuisine' exists in the 'cuisines' column (the value\n# of which is a list type)\ncuisine_filter = df['cuisines'].map(lambda c: cuisine in c)\n\n# Display the result, filtering by the weight assigned\ndf[user_filter &amp; cuisine_filter]\n\n"
"from operator import attrgetter\n\n\nclass Foo:\n    def __init__(self, a, b, c, d):\n        self.a = a\n        self.b = b\n        self.c = c\n        self.d = d\n\n    def func1(self):\n        a, b, c = attrgetter('a', 'b', 'c')(self)\n        return a + b * c\n\nf = Foo(1, 2, 3, 4)\n\nvalue = f.func1()\n\ndef func1(self):\n    return self.a + self.b * self.c\n"
'import pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.ar_model import AR\nfrom random import random\n\nfor i in range(data.shape[0]):\n    row = data.iloc[i]\n    model=AR(row.values)\n    model_fit=model.fit()\n    yhat=model_fit.predict(len(row),len(row))\n    print(yhat)\n\n'
"data = pd.DataFrame(\n    {\n        'real_1': [100, 100, 100, 100, 100],\n        'real_2': [100, 100, 100, 100, pd.np.nan],\n        'real_3': [100, 100, 100, 100, 0],\n        'pred_1': [110, 130, 120, 105, 100],\n        'pred_2': [110, 130, 120, pd.np.nan, 100],\n        'pred_3': [110, 130, 120, 105, pd.np.nan],\n    }\n)\nprint(data)\n   real_1  real_2  real_3  pred_1  pred_2  pred_3\n0     100   100.0     100     110   110.0   110.0\n1     100   100.0     100     130   130.0   130.0\n2     100   100.0     100     120   120.0   120.0\n3     100   100.0     100     105     NaN   105.0\n4     100     NaN       0     100   100.0     NaN\n\nfor real_val_col in ['real_1', 'real_2', 'real_3']:\n    for pred_val_col in ['pred_1', 'pred_2', 'pred_3']:\n        real_val = data[real_val_col]\n        pred_val = data[pred_val_col]\n        print(f'- MAPE for {real_val_col} and {pred_val_col}: {MAPE(real_val, pred_val):.2f} %')\n\n- MAPE for real_1 and pred_1: 13.00 %\n- MAPE for real_1 and pred_2: 15.00 %\n- MAPE for real_1 and pred_3: 16.25 %\n- MAPE for real_2 and pred_1: 16.25 %\n- MAPE for real_2 and pred_2: 20.00 %\n- MAPE for real_2 and pred_3: 16.25 %\n- MAPE for real_3 and pred_1: inf %\n- MAPE for real_3 and pred_2: inf %\n- MAPE for real_3 and pred_3: 16.25 %\n"
'import contractions\ncontractions.fix("you\'re happy now")\n# "you are happy now"\n\nfrom contractions import contractions_dict\n\n{..., \'you’ll\': \'you will\', ...}\n'
"print (df)\n   a  b  d\n0  1  2  3\n1  4  5  6\n2  4  2  3\n3  3  1  9\n4  6  7  0\n5  9  2  5\n6  0  0  4 &lt;- removed last row\n\nN = 3\nnum = len(df) // N * N\ndf = df.iloc[:num]\ndf['groups'] = np.arange(len(df)) // N\nprint (df)\n   a  b  d  groups\n0  1  2  3       0\n1  4  5  6       0\n2  4  2  3       0\n3  3  1  9       1\n4  6  7  0       1\n5  9  2  5       1\n"
'from datetime import datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nsampleTime = "23/02/2017 at 11:30 PM"\ntimeFormatted = datetime.strptime(sampleTime, "%d/%m/%Y at %I:%M %p")\nprint(timeFormatted)\n\nplt.scatter(timeFormatted, 0)\n\n# reshape X labels\nplt.gcf().autofmt_xdate()\nshowFormat = mdates.DateFormatter(\'%Y-%m-%d %H:%M\')\nplt.gca().xaxis.set_major_formatter(showFormat)\n\nplt.show()\n'
"test_cols = ['N1','N2','N6','N8','N10']\n\nfor c in test_cols:\n    drop_rows = df[(((df[c] - df[c].mean()) / df[c].std()).abs() &lt; 3)].index\n    df = df.drop(drop_rows)\n\ndrop_set = {}\nfor c in test_cols:\n    drop_ind = df[(((df[c] - df[c].mean()) / df[c].std()).abs() &lt; 3)].index\n    drop_set = {*drop_set, *drop_ind}\ndf = df.drop(drop_set)\n\ndrop_rows = df[(((df['N1'] - df['N1'].mean()) / df['N1'].std()).abs() &lt; 3) |\n               (((df['N2'] - df['N2'].mean()) / df['N2'].std()).abs() &lt; 3) |\n               (((df['N6'] - df['N6'].mean()) / df['N6'].std()).abs() &lt; 3) |\n               (((df['N8'] - df['N8'].mean()) / df['N8'].std()).abs() &lt; 3) |\n               (((df['N10'] - df['N10'].mean()) / df['N10'].std()).abs() &lt; 3)].index\ndf = df.drop(drop_rows)\n"
'df[\'Label\'] = df.max(axis=1)\n\n\nfor index, val in final_df[[x for x in final_df.columns if x != \'date\']].iterrows():\n    max_column = np.argmax(val)\n    max_column_val = np.max(val)\n\n    if max_column == "O_3":\n        if max_column_val &lt;= 80:\n            final_df.at[index, \'Label\'] = 1\n\n        if 80 &lt; max_column_val &lt; 120:\n            final_df.at[index, \'Label\'] = 2\n\n        if 120 &lt; max_column_val &lt; 180:\n            final_df.at[index, \'Label\'] = 3\n\n        if 180 &lt; max_column_val &lt; 240:\n            final_df.at[index, \'Label\'] = 4\n\n        if 240 &lt; max_column_val &lt; 600:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "NO_2":\n        if max_column_val &lt;= 40:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 100:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 200:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 400:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 1000:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "SO_2":\n        if max_column_val &lt;= 100:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 200:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 350:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 500:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 1250:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "PM10":\n        if max_column_val &lt;= 20:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 35:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 50:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 100:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 1200:\n            final_df.at[index, \'Label\'] = 5\n\n    if max_column == "PM25":\n        if max_column_val &lt;= 10:\n            final_df.at[index, \'Label\'] = 1\n\n        if 40 &lt; max_column_val &lt; 20:\n            final_df.at[index, \'Label\'] = 2\n\n        if 100 &lt; max_column_val &lt; 25:\n            final_df.at[index, \'Label\'] = 3\n\n        if 200 &lt; max_column_val &lt; 50:\n            final_df.at[index, \'Label\'] = 4\n\n        if 400 &lt; max_column_val &lt; 800:\n            final_df.at[index, \'Label\'] = 5\n\n       date                O_3         PM25        PM10      CO    SO_2        NO_2        Label\n0   2001-01-01  01:00:00    7.86    12.505127   32.349998   0.45    26.459999   67.120003   2.0\n1   2001-01-01  02:00:00    7.21    12.505127   40.709999   0.48    20.879999   70.620003   2.0\n2   2001-01-01  03:00:00    7.11    12.505127   50.209999   0.41    21.580000   72.629997   2.0\n3   2001-01-01  04:00:00    7.14    12.505127   54.880001   0.51    19.270000   75.029999   2.0\n4   2001-01-01  05:00:00    8.46    12.505127   42.340000   0.19    13.640000   66.589996   2.0\n5   2018-04-30  20:00:00    63.00   200.000000  2.000000    0.30    4.000000    58.000000   200.0\n6   2018-04-30  21:00:00    49.00   400.000000  5.000000    0.30    4.000000    65.000000   400.0\n7   2018-04-30  22:00:00    49.00   3.000000    125.000000  0.30    4.000000    58.000000   125.0\n8   2018-04-30  23:00:00    48.00   7.000000    7.000000    0.30    4.000000    52.000000   2.0\n9   2018-05-01  00:00:00    52.00   4.000000    6.000000    0.30    4.000000    43.000000   1.0\n'
'# imports\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport pandas as pd\nimport math\nimport numpy as np\n\n# color cycle\ncolors = px.colors.qualitative.Alphabet*10\n\n# sample data with similar structure as OP\ndf = px.data.gapminder().query("continent==\'Americas\'")\ndfp=df.pivot(index=\'year\', columns=\'country\', values=\'pop\')\ndfp=dfp[[\'United States\', \'Mexico\', \'Argentina\', \'Brazil\', \'Colombia\']]\ndfp=dfp.sort_values(by=\'United States\', ascending = False)\ndfp=dfp.T\ndfp.columns = [str(yr) for yr in dfp.columns]\ndfp = dfp[dfp.columns[::-1]].T\n\n# build figure and add traces\nfig=go.Figure()\nfor col, country in enumerate(dfp):\n    vals = dfp[country].values\n    yVals = [col]*len(vals)\n    fig.add_traces(go.Scatter(\n\n        y=yVals,\n        x=dfp.index,\n        mode=\'markers\',\n        marker=dict(color=colors[col],\n            size=vals,\n            sizemode=\'area\',\n            #sizeref=2.*max(vals)/(40.**2),\n            sizeref=2.*max(dfp.max())/(40.**2),\n            sizemin=4),\n        name = country\n    ))\n\n# edit y tick layout\ntickVals = np.arange(0, len(df.columns))\nfig.update_layout(\n    yaxis = dict(tickmode = \'array\',\n                 tickvals = tickVals,\n                 ticktext = dfp.columns.tolist()))\n\n\nfig.show()\n'
"data['D'] = data['Date'].apply(lambda x : datetime.datetime.strptime(x, '%Y-%m-%d'))\ndata['Day'] = data['D'].apply(lambda x: x.day)\ndata['Month'] = data['D'].apply(lambda x: x.month)\ndata['Year'] = data['D'].apply(lambda x: x.year)\ndata.drop(columns='D', inplace=True)\n\ndata.sort_values(by=['Day','Month','Year'], inplace=True)\n\ndata.drop(columns = ['Day','Month','Year'], inplace=True)\n"
"house_data.columns = pd.to_datetime(house_data.columns).to_period('M')\n\nhouse_data[str(year)+'q2'] = house_data[[str(year)+'-04',...]].mean(axis=1)\n\nhouse_data.columns = pd.to_datetime(house_data.columns).to_period('M').strftime('%Y-%m')\n\nhouse_data.columns = pd.to_datetime(house_data.columns).to_period('Q')\n\nhouse_data.groupby(level=0, axis=1).mean()\n"
"df['Greater_than_50'] = [val.strip() for val in df['Greater_than_50'].astype(str)]\n\n# columns to keep\ncol_mask = ['Discharge', 'Resistance']\n\ndf_new = df.loc[df['Greater_than_50'] == 'True'][col_mask] \n\n'''\ntime    Discharge   Resistance  Greater_than_50\n0   0.000   NaN NaN\n1   0.005   76.373  True\n2   0.010   -48.174 False\n3   0.016   -37.012 False\n4   0.021   -27.808 False\n5   0.026   -24.674 False\n6   0.031   -20.464 False\n7   0.037   100.114 True\n'''\n\nimport pandas as pd\n\ndf = pd.read_clipboard()\n\nprint(df)\n\n   time  Discharge  Resistance Greater_than_50\n0     0      0.000         NaN             NaN\n1     1      0.005      76.373            True\n2     2      0.010     -48.174           False\n3     3      0.016     -37.012           False\n4     4      0.021     -27.808           False\n5     5      0.026     -24.674           False\n6     6      0.031     -20.464           False\n7     7      0.037     100.114            True\n\ndf['Greater_than_50'] = [val.strip() for val in df['Greater_than_50'].astype(str)]\n\n# columns to keep\ncol_mask = ['Discharge', 'Resistance']\n\ndf_new = df.loc[df['Greater_than_50'] == 'True'][col_mask] \nprint(df_new)\n\n   Discharge  Resistance\n1      0.005      76.373\n7      0.037     100.114\n"
'df = pandas.read_csv("file", names = fields, dtype={"Client": str , "Timestamp": str , "Measurements" : str }, sep=\' , \', engine=\'python\')\n'
'df = pd.DataFrame({\n    "ID": [111, 111, 111, 222, 222, 333, 333, 333],\n    "Year": [1, 2, 3, 1, 2, 1, 2, 3]})\n\n# filter unique IDs that doesn\'t contain [1, 2, 3]\ndf = df.groupby(\'ID\').filter(lambda g: (sorted(set(g["Year"].values)) == [1,2,3]))\nprint(df)\n#    ID  Year\n#0  111     1\n#1  111     2\n#2  111     3\n#5  333     1\n#6  333     2\n#7  333     3\n'
'dw.add_(weight_decay, w)\n\na = torch.FloatTensor([0, 1.0, 2.0, 3.0])\nb = torch.FloatTensor([0, 4.0, 5.0, 6.0])\nc = 1.0\nz = a.add(b, c)\n\nTypeError: add() takes 1 positional argument but 2 were given\n\nz = a.add(b, alpha=c)\n'
'my_sets\nABCD\nEFGD\nAGID\nZWHK\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv("plik.txt")\n\nmy_sets = data.my_sets.apply(set).to_list()\n\n [{\'A\', \'B\', \'C\', \'D\'},\n {\'D\', \'E\', \'F\', \'G\'},\n {\'A\', \'D\', \'G\', \'I\'},\n {\'H\', \'K\', \'W\', \'Z\'}]\n\ndef dist(a, b) : return len(a.difference(b))*len(b.difference(a))/len(a.union(b))\n\nN = len(my_sets)\n\npdist = np.zeros((N, N)) # I have imported numpy as np above!\n\nfor i in range(N):\n  for j in range(i + 1, N):\n    pdist[i,j] = dist(my_sets[i], my_sets[j])\n    pdist[j,i] = pdist[i,j]\n'
"import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'Politics, Very Interested': [np.nan, np.nan, np.nan, 1],\n                    'Politics, Not Interested': [np.nan, 1, 1, np.nan]})\n\ncol_labels = ['']*len(df.columns)\nfor c, col in enumerate(df.columns):\n    col_name, val = col.split(',')\n    df.loc[df[col].notna(), col] = val\n    col_labels[c] = col_name\n\ndf.columns = col_labels\n\nprint(df)\n\n           Politics         Politics\n0               NaN              NaN\n1               NaN   Not Interested\n2               NaN   Not Interested\n3   Very Interested              NaN\n\ndf.fillna('', inplace=True)\ndf = df.groupby(df.columns, axis=1).apply(np.max, axis=1)\n\n           Politics\n0                  \n1    Not Interested\n2    Not Interested\n3   Very Interested\n"
"import os\nfiles = os.listdir('./')\n"
"reviews.groupby('taster_name').points.agg('mean').sort_values(ascending=False)\n"
'from scipy import stats\nfrom math import sqrt\nfrom scipy.stats import t\n\nN1 = 10 #numbers of observations\nN2 = 13\ndf = (N1 + N2 - 2) #deegrees of freedom\nstd1 = 0.8 #standard deviations\nstd2 = 0.5\nstd_N1N2 = sqrt( ((N1 - 1)*(std1)**2 + (N2 - 1)*(std2)**2) / df) #average standard deviations between groups.\ndiff_mean = 8.9 - 8.5 #difference between means\nMoE = t.ppf(0.975, df) * std_N1N2 * sqrt(1/N1 + 1/N2) # margin of error \n\nprint(f&quot;Difference between means is: {diff_mean:.2f} ( {diff_mean-MoE:.2f}, {diff_mean + MoE:.2f} )&quot;)\n\n\nDifference between means is: 0.40 ( -0.16, 0.96 )\n'
"df['BB_upperband'], df['BB_middleband'], df['BB_lowerband'] = talib.BBANDS(cl, timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n"
"df = df.groupby(['CUST', 'FT', 'DATE'], as_index=False).agg({'QTY': ['sum']})\n"
"import matplotlib.pyplot as plt\n\nintervals = [[0.5, 1.5],\n             [2,3],\n             [0.2,4]]\n\nfor int in intervals:\n    plt.plot(int,[0,0], 'b', alpha = 0.2, linewidth = 100)\n\nplt.show()\n"
'a = np.array([[1, 3], \n              [np.nan, 2], \n              [5, 9]])\n\nc = np.array([[3, 4], \n              [6, 12], \n              [8, np.nan]])\n\nimp = SimpleImputer(strategy=&quot;mean&quot;)\na1 = imp.fit_transform(a)\nc1 = imp.fit_transform(c)\n\na1: array([[1., 3.],\n           [3., 2.],\n           [5., 9.]])\n\nc1: array([[ 3.,  4.],\n           [ 6., 12.],\n           [ 8.,  8.]])\n'
'df[&quot;filename&quot;] = df[&quot;filename&quot;].apply(os.path.basename)\n\ndf[&quot;filename&quot;] = df[&quot;filename&quot;].apply(lambda path: os.path.basename(path))\n\n&gt;&gt;&gt; df\n      filename\n0  a/b/c/d.txt\n1  h/g/f/e.txt\n&gt;&gt;&gt; df[&quot;filename&quot;] = df[&quot;filename&quot;].apply(os.path.basename)\n&gt;&gt;&gt; df\n  filename\n0    d.txt\n1    e.txt\n'
"from sklearn.datasets import load_breast_cancer\nimport statsmodels.api as sm\n\ndat = load_breast_cancer()\ndf = pd.DataFrame(dat.data,columns=dat.feature_names)\ndf['target'] = dat.target\nX = df['mean radius']\ny = df['target']\n\nX_incl_const = sm.add_constant(X)\nmodel = sm.Logit(y, X_incl_const)\nresults = model.fit()\nresults.summary()\n\nmodel.predict(X)\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-180-2558e7096c7c&gt; in &lt;module&gt;\n----&gt; 1 model.predict(X)\n      2 \n      3 \n\n~/anaconda2/lib/python3.7/site-packages/statsmodels/discrete/discrete_model.py in predict(self, params, exog, linear)\n    482             exog = self.exog\n    483         if not linear:\n--&gt; 484             return self.cdf(np.dot(exog, params))\n    485         else:\n    486             return np.dot(exog, params)\n\n&lt;__array_function__ internals&gt; in dot(*args, **kwargs)\n\nValueError: shapes (569,2) and (569,) not aligned: 2 (dim 1) != 569 (dim 0)\n\nplt.scatter(X,results.predict(sm.add_constant(X)))\n\nplt.scatter(X,results.predict())\n"
'1/(2*length) * np.sum(np.square(predictions - y))\n'
"df = pd.DataFrame({'Name': ['Dave','Dave','stu','stu'],\n                   'Codes': ['DSFFS','SDFDF','SDFDS','DSGDSG']})\nprint(df)\n    Codes  Name\n0   DSFFS  Dave\n1   SDFDF  Dave\n2   SDFDS   stu\n3  DSGDSG   stu\n\nprint(pd.get_dummies(df, columns=['Codes']))\n   Name  Codes_DSFFS  Codes_DSGDSG  Codes_SDFDF  Codes_SDFDS\n0  Dave            1             0            0            0\n1  Dave            0             0            1            0\n2   stu            0             0            0            1\n3   stu            0             1            0            0\n"
"# value_counts makes more sense to me\n(hlt_downsampled_eng.groupby('Poblacion')['patologie'].value_counts()\n                    .groupby('Poblacion').nlargest(6)                 # head(6) should also work\n)\n"
"classification = {\n    '1': 'Freshman',\n    '2': 'Sophomore',\n    '3': 'Junior',\n    '4': 'Senior'\n}\n\ndf['Classification'] = df['#ID'].apply(lambda id: classification[str(id)[0]])\n\n    #ID  Score Classification\n0  1029     78       Freshman\n1  1229     89       Freshman\n2  1929     77       Freshman\n3  2124    100      Sophomore\n4  3120     89         Junior\n5  4145     84         Senior\n"
'# import these modules \nfrom nltk.stem import WordNetLemmatizer \n  \nlemmatizer = WordNetLemmatizer() \n  \nprint(&quot;changing :&quot;, lemmatizer.lemmatize(&quot;changing&quot;, pos =&quot;v&quot;)) \nprint(&quot;change :&quot;, lemmatizer.lemmatize(&quot;change&quot;)) \nprint(&quot;changer :&quot;, lemmatizer.lemmatize(&quot;changer&quot;)) \n  \n# # a denotes adjective in &quot;pos&quot; \nprint(&quot;changing :&quot;, lemmatizer.lemmatize(&quot;changyng&quot;, pos =&quot;v&quot;)) \n\nchanging : change #&lt;----------\nchange : change\nchanger : changer\nchanging : changyng #&lt;-------\n'
"&gt; df[df.index.isin(df[['view', 'video']].join(df[['width', 'height']]//2).drop_duplicates().index)\n       view                      video  left  width  top  height\n0   Endzone   57906_000718_Endzone.mp4   372   17.0  279      17\n1   Endzone   57906_000718_Endzone.mp4   851   20.0  273      14\n3   Endzone   57906_000718_Endzone.mp4   855   21.0  267      16\n5  Sideline  57906_000718_Sideline.mp4   763   18.0   98      26\n"
'fig, ax = plt.subplots(1,1, figsize=[10,5],\n                      sharey=False, sharex=False, gridspec_kw={&quot;hspace&quot;:0.3})\n\n\n(pd.read_csv(&quot;EH3CzHf8.txt&quot;)\n .groupby([&quot;algo&quot;,&quot;numtasks&quot;])[&quot;total&quot;].mean()\n .to_frame()\n .unstack(0)\n .droplevel([0], axis=1)\n .plot(ax=ax, kind=&quot;bar&quot;, stacked=True)\n)\n'
"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nd = {\n    'a': pd.Series(np.random.choice([1, -1], size=10)),\n    'b': pd.Series(np.random.randn(10)),\n    'c': pd.Series(np.random.randn(10))\n}\ndf = pd.DataFrame(d)\n\n# plot for first column when value is 1\ndf[df.a == 1].sample(3).T.iloc[1:].boxplot()\n\n# plot for first column when value is -11\ndf[df.a == -1].sample(3).T.iloc[1:].boxplot()\n\ndf[df.a == 1]\n\ndf[df.a == -1].sample(3)\n\ndf[df.a == -1].sample(3).T.iloc[1:]\n\ndf[df.a == -1].sample(3).T.iloc[1:].boxplot()\n"
"from collections import Counter\n\n# Exclude header\ndata = data[1:]\n\n# Filter accidents\naccidents = filter(lambda x: x[3] == 'Accidental', data)\n\n# Count by gender\nby_gender = Counter(item[5] for item in accidents)\nprint(by_gender)\n\nimport pandas as pd\n\ndf = pd.DataFrame.from_records(data=data[1:], columns=data[0])\n\n# Filter 'Accidental', group by sex, get the size of each group\ndf = df[df['intent'] == 'Accidental'].groupby('sex').size()\n\n# Print it out\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    print(df)\n"
"df['minute'].quantile([0.25, 0.50 , 0.9])\n\ndf.loc[df['minute'] &lt;= 5, 'minute'].quantile([0.25, 0.50 , 0.9])\n"
"In [151]: ret = !ls *.json                                                                       \nIn [152]: ret                                                                                    \nOut[152]: ['foo1.json', 'foo.json', 'logins.json', 'stack56532806.json']\n\nIn [153]: ret.l                                                                                  \nOut[153]: ['foo1.json', 'foo.json', 'logins.json', 'stack56532806.json']\n\nIn [154]: ret.n                                                                                  \nOut[154]: 'foo1.json\\nfoo.json\\nlogins.json\\nstack56532806.json'\n\nIn [155]: ret.s                                                                                  \nOut[155]: 'foo1.json foo.json logins.json stack56532806.json'\nIn [156]: type(ret)                                                             \n\nIn [158]: ret?                                                                                   \nType:        SList\nString form: ['foo1.json', 'foo.json', 'logins.json', 'stack56532806.json']\nLength:      4\nFile:        /usr/local/lib/python3.6/dist-packages/IPython/utils/text.py\nDocstring:  \nList derivative with a special access attributes.\n\nThese are normal lists, but with the special attributes:\n\n* .l (or .list) : value as list (the list itself).\n* .n (or .nlstr): value as a string, joined on newlines.\n* .s (or .spstr): value as a string, joined on spaces.\n* .p (or .paths): list of path objects (requires path.py package)\n\nAny values which require transformations are computed only once and\ncached.\n"
'## Explore the data (line 27)\ndata = pd.read_table(\'u.data\', header=None)  # header=None avoid getting the columns automatically\ndata.columns = [\'userID\', \'itemID\',\n                \'rating\', \'timestamp\']       # Manually set the columns.\ndata = data.drop(\'timestamp\', axis=1)        # Continue with regular work.\n\n...\n\n## Load user information (line 75)\nusers_info = pd.read_table(\'u.user\', sep=\'|\', header=None)\nusers_info.columns = [\'useID\', \'age\', \'gender\',\n                      \'occupation\' \'zipcode\']\nusers_info = users_info.set_index(\'userID\')\n\n...\n\n## Load movie information (line 88)\nmovies_info = pd.read_table(\'u.item\', sep=\'|\', header=None)\nmovies_info.columns = [\'movieID\', \'movie title\', \'release date\',\n                       \'video release date\', \'IMDb URL\', \'unknown\',\n                       \'Action\', \'Adventure\', \'Animation\', "Children\'s",\n                       \'Comedy\', \'Crime\', \'Documentary\', \'Drama\',\n                       \'Fantasy\', \'Film-Noir\', \'Horror\', \'Musical\',\n                       \'Mystery\', \'Romance\', \'Sci-Fi\',\' Thriller\',\n                       \'War\', \'Western\']\nmovies_info = movies_info.set_index(\'movieID\')#.drop(low_count_movies)\n'
"print (df)\n                                   timestamp  index\nhost                                               \n199.72.81.55            01/Jul/1995:00:00:01      0\nunicomp6.unicomp.net    01/Jul/1995:00:00:06      1\nfreenet.edmonton.ab.ca  01/Jul/1995:00:00:12     12\nburger.letters.com      01/Jul/1995:00:00:12     14\n205.212.115.106         01/Jul/1995:00:00:12     15\n129.94.144.152          01/Jul/1995:00:00:13     21\nunicomp6.unicomp.net    01/Jul/1995:00:00:07    415\nunicomp6.unicomp.net    01/Jul/1995:00:00:08    226\nunicomp6.unicomp.net    01/Jul/1995:00:00:33     99 &lt;-change time for matching\n129.94.144.152          01/Jul/1995:00:00:14     41\n129.94.144.152          01/Jul/1995:00:00:15     52\n129.94.144.152          01/Jul/1995:00:00:17     55\n129.94.144.152          01/Jul/1995:00:00:18     75\n129.94.144.152          01/Jul/1995:00:00:21     84\n\n#convert to datetimes\ndf.timestamp = pd.to_datetime(df.timestamp, format='%d/%b/%Y:%H:%M:%S')\nfailedIP_list = ['199.72.81.55', '129.94.144.152', 'unicomp6.unicomp.net']\n\n#filter rows by failedIP_list\ndf = df[df.index.isin(failedIP_list)]\n\n#get difference and count for all values in index\ng = df.groupby(level=0)['timestamp']\nDIFF = pd.to_timedelta(g.transform(pd.Series.diff)).dt.total_seconds()\nCOUNT = g.cumcount()\n\n#filter rows\nmask = (DIFF &gt; 20) | (COUNT &gt;= 3)\nL = df.loc[mask, 'index'].tolist()\nprint (L)\n[99, 55, 75, 84]\n"
"    ID  key dist\n0    1   57  1\n1    2   22  1\n2    3   12  1\n3    4   45  1\n4    5   94  1\n5    6   36  1\n6    7   38  1\n7    8   94  1\n8    9   94  1\n9   10   38  1\n\nimport pandas as pd\ndf = pd.read_clipboard()\n\n    key  dist  window\nID                   \n1    57     1       0\n2    22     1       0\n3    12     1       0\n4    45     1       0\n5    94     1       0\n6    36     1       0\n7    38     1       0\n8    94     1       1\n9    94     1       2\n10   38     1       1\n\ndef features_wind2(inp):\n    all_window = inp\n    all_window['window1'] = 0\n    for index, row in all_window.iterrows():\n        lid = index\n        lid1 = lid - 200\n        pid = row['key']\n        row['window1'] = all_window.query('index &lt; %d &amp; index &gt; %d &amp; key == %d' % (lid, lid1, pid)).count()[0]     \n    return all_window\n\nprint('old solution: ')\n%timeit features_wind2(df) \n\nold solution: \n10 loops, best of 3: 25.6 ms per loop\n\ndef compute_window(row):\n    # when using apply(), .name gives the row index\n    # pandas indexing is inclusive, so take index-1 as cut_idx\n    cut_idx = row.name - 1 \n    key = row.key\n    # count the number of instances key appears in df, prior to this row\n    return sum(df.ix[:cut_idx,'key']==key)\n\nprint('new solution: ')\n%timeit df['window1'] = df.apply(compute_window, axis='columns')\n\nnew solution: \n100 loops, best of 3: 3.71 ms per loop\n\n# sample data\nimport numpy as np\nimport pandas as pd\n\nN = int(1e7)\nidx = np.arange(N)\nkeys = np.random.randint(1,100,size=N)\ndists = np.ones(N).astype(int)\ndf = pd.DataFrame({'ID':idx,'key':keys,'dist':dists})\ndf = df.set_index('ID')\n\n%timeit df['window'] = df.groupby('key').cumsum().subtract(1)\n\n1 loop, best of 3: 755 ms per loop\n\n    dist  key  window\nID                   \n0      1   83       0\n1      1    4       0\n2      1   87       0\n3      1   66       0\n4      1   31       0\n5      1   33       0\n6      1    1       0\n7      1   77       0\n8      1   49       0\n9      1   49       1\n10     1   97       0\n11     1   36       0\n12     1   19       0\n13     1   75       0\n14     1    4       1\n"
"z = []\nfor x in root.findall('xyz'):\n    y = x.findall('abc')[-1]\n    for s in x.iter('xxx'):\n        yyy = s.text, s.attrib['zzz'], y.text\n        z.append(yyy)\n\nprint(z)\n"
"score=cross_val_score(estimator=lr,\n                      X=X_undersample.values,\n                      y=y_undersample.values.ravel(),\n                      cv=10,\n                      scoring='accuracy')\n"
"for col in df.columns:\n    if (df[col] &gt; 4).any(): # or .all if needed \n        print('Zone %s does not make setpoint' % col)\n    else:\n        print('Zone %s is Normal' % col)\n\ndef _print(x):\n    if (x &gt; 4).any():\n        print('Zone %s does not make setpoint' % x.name)\n    else:\n        print('Zone %s is Normal' % x.name)\n\ndf.apply(lambda x: _print(x))\n# you can even do\n[_print(df[col]) for col in df.columns]\n\ndef is_normal(x):\n    return not (x &gt; 4).any()\n\ns = df.apply(lambda x: is_normal(x))\n\n# or directly\ns = df.apply(lambda x: not (x &gt; 4).any())\n\ndf = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4],c=[3,4,5])) # A sample\n\nprint(df)\n   a  b  c\n0  1  2  3\n1  2  3  4\n2  3  4  5\n\nfor col in df.columns:\nif (df[col] &lt; 4).any():\n    print('Zone %s does not make setpoint' % col)\nelse:\n    print('Zone %s is Normal' % col)\n\nZone a is Normal\nZone b is does not make setpoint\nZone c is does not make setpoint\n\ns = df.apply(lambda x: not (x &lt; 4).any()) # Build the series\nprint(s)\n\na     True\nb     False\nc     False\ndtype: bool\n\n\nprint(df[s[~s].index]) #Falsecolumns df\n   b  c\n0  2  3\n1  3  4\n2  4  5\n\n\nprint(df[s[s].index]) #Truecolumns df\n   a\n0  1\n1  2\n2  3\n"
"df2 = df[pd.isnull(df['credit_history'])]\n"
'slang_file = [line.strip().split(":", 1) for line in open("dictionary.txt")]\n'
" df.loc[df['Comune'] == 'AIRASCA', 'cap']\n"
"df = pd.DataFrame()\n\ndf['X'] = [1, 2, 3, 4, 5, 6]\ndf['Y'] = [10, 20, 30, 40, 50, 60]\n\ndf_res = df.loc[df['X'] &gt; 3]\n\ndf['C'] = df.index.map(lambda x : 1 if x in df_res.index else 0)\n\ndf['C'] = [1 if x in df_res.index else 0 for x in df.index]\n"
"In [204]: alist = [np.ones((2,3),int), np.zeros((1,3),int)]\nIn [205]: alist\nOut[205]: \n[array([[1, 1, 1],\n        [1, 1, 1]]), array([[0, 0, 0]])]\nIn [206]: len(alist)\nOut[206]: 2\nIn [207]: np.shape(alist)\nOut[207]: (2,)\nIn [208]: alist.shape\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-208-6ab8dc5f9201&gt; in &lt;module&gt;()\n----&gt; 1 alist.shape\n\nAttributeError: 'list' object has no attribute 'shape'\nIn [209]: np.array(alist)\nOut[209]: \narray([array([[1, 1, 1],\n       [1, 1, 1]]), array([[0, 0, 0]])], dtype=object)\n\nIn [210]: alist = [np.ones((2,3),int), np.zeros((2,3),int)]\nIn [211]: alist\nOut[211]: \n[array([[1, 1, 1],\n        [1, 1, 1]]), array([[0, 0, 0],\n        [0, 0, 0]])]\nIn [212]: len(alist)\nOut[212]: 2\nIn [213]: np.shape(alist)\nOut[213]: (2, 2, 3)\nIn [214]: np.array(alist)\nOut[214]: \narray([[[1, 1, 1],\n        [1, 1, 1]],\n\n       [[0, 0, 0],\n        [0, 0, 0]]])\n"
"df.groupby(['Date', 'DayOfWeek', 'Holiday']).count().reset_index().groupby(['DayOfWeek', 'Holiday']).mean()['Timestamp']\n\nDayOfWeek  Holiday\nMonday     True       1.5\nThursday   False      2.0\nTuesday    False      2.0\n"
'print("| TRACK", " " * (max_track - 5),\n      "| ARTIST", " " * (max_artist - 6),\n      "| ALBUM", " " * (max_album - 5),\n      "| TIME |")\n\nfor i in data[1:]:\n\n# Before the loop:\ntotalSeconds = 0\n\n# Print out the track information, one track per line:\nfor ...:\n    # Inside of the loop:\n\n    # Print out the track information:\n    print(...)\n\n    # Collect the run-length time by adding it to totalSeconds:\n    mins, secs = map(int, record[3].split(":"))\n    totalSeconds += mins * 60 + secs\n\n# After the loop:\nhours = totalSeconds // 3600\nminutes = (totalSeconds // 60) % 60\nseconds = totalSeconds % 60\nprint("TOTAL PLAYING TIME:  {}:{:02d}:{:02d}".format(\n          hours, minutes, seconds))\n'
'nx.draw(G, labels=labelDict)\n'
"import numpy as np\n\ndef pad_txt_data(arr):\n  paded_arr = []\n  prefered_len = len(max(arr, key=len))\n\n  for each_arr in arr:\n    if len(each_arr) &lt; prefered_len:\n      print('padding array with zero')\n      while len(each_arr) &lt; prefered_len:\n          each_arr.insert(0, np.zeros(3))\n      paded_arr.append(each_arr)\n  return np.array(paded_arr)\n\n# your_arr = [shape(16, 3), shape(32, 3), . .. .]\n# loop through your_arr and prepare a single array with all the arrays and pass this array to padding function.\n\ninterm_arr = []\ndef input_prep():\n  for each_arr in your_arr:\n    interm_arr.append(each_arr)\n  final_arr = pad_txt_data(interm_arr)\n"
"# &lt;= 0.23\n# np.random.shuffle(df['Country'].values)\n# 0.24+\nnp.random.shuffle(df['Country'].to_numpy())\n\ndf['Country'] = np.random.choice(df['Country'], len(df), replace=False)\n"
"ratings.pivot(index='user_id', columns='movie_id', values='rating')\n"
"df['Result'] = df.groupby('ID')['Purchases_Made'].cumsum()\nprint(df)\n\n        ID      Date  Purchases_Made  Purchase_Mades_So_Far(Result)  Result\n0  ClientA  Jan/2019               5                              5       5\n1  ClientA  Feb/2019               8                             13      13\n2  ClientB  Jan/2019               1                              1       1\n3  ClientB  Feb/2019               3                              4       4\n"
'logreg = LogisticRegression(verbose=1)\n\nrf = RandomForestRegressor(verbose=1)\n'
"df.groupby(df['Key'].replace(regex=True,to_replace=r'(C[0-9]+).*',value=r'\\1'))\\\n['Predictions'].apply(lambda x: ','.join(map(str,x)))\n\ndf.groupby(df['Key'].replace(regex=True,to_replace=r'(C[0-9]+).*',value=r'\\1')).sum()\n\ndf.groupby(df['Key'].replace(regex=True,to_replace=r'(C[0-9]+).*',value=r'\\1')).sum()\\\n.reset_index()\n\ndf.groupby(df['Key'].replace(regex=True,to_replace=r'(C[0-9]+).*',value=r'\\1')).sum()\n\ndf.groupby: Means use groupby for df whatever values passed to it.\ndf['Key'].replace(regex=True,to_replace=r'(C[0-9]+).*',value=r'\\1'): Means df's key column I am using regex to replace everything after Cdigits with NULL as per OP's question.\n\n.sum(): Means to get total sum of all similar 1st column as per need.\n"
"import matplotlib.pyplot as plt\n\nx = [39620, 43170, 76633, 50449, 50777]\ny = [49625, 177083, 2026280, 460648, 137874]\n\nplt.figure()\nplt.ticklabel_format(style='plain')\nplt.scatter(x, y, norm=None)\n"
"% of non events = 3/5\n% of events = 1/4\n\nln(% of events / % of non events ) = ln(5/12) = -0.8754687373538999\n\nwoe = WOEEncoder(cols=['cat'], random_state=42, regularization=0)\nX = df['cat']\ny = df.target\nencoded_df = woe.fit_transform(X, y)\n\n0   -0.875469\n1   0.916291\n2   -0.875469\n3   0.916291\n4   -0.875469\n5   -0.875469\n6   0.916291\n7   0.223144\n8   0.223144\n"
'shortest_distance = make_array()\nfor air_lat, air_long in zip(airbnb_coord[0],airbnb_coord[1]):\n    min_distance = np.sqrt(((air_lat-station_coord[0][0])**2)+((air_long-station_coord[0][0])**2))\n    for stat_lat, stat_long in zip(station_coord[0], station_coord[1]):\n        distance = np.sqrt(((air_lat-stat_lat)**2)+((air_long-stat_long)**2))\n        if distance &lt; min_distance:\n            min_distance = distance\n\n    shortest_distance = np.append(shortest_distance, min_distance)\n'
'import pandas as pd\n\ndummy_data1 = {\n        \'C1\': [\'11\', \'12\', \'13\', \'14\', \'15\', \'16\', \'17\', \'18\', \'19\', \'20\'],\n        \'C2\': [\'A\', \'E\', \'I\', \'M\', \'Q\', \'A\', \'E\', \'I\', \'M\', \'Q\', ],\n        \'C3\': [\'B\', \'F\', \'J\', \'N\', \'R\', \'B\', \'F\', \'J\', \'N\', \'R\', ],\n        \'C4\': [\'C\', \'G\', \'K\', \'O\', \'S\', \'C\', \'G\', \'K\', \'O\', \'S\', ],\n        \'C5\': [\'D\', \'H\', \'L\', \'P\', \'T\', \'D\', \'H\', \'L\', \'P\', \'T\', ]}\n\ndf1 = pd.DataFrame(dummy_data1, columns = [\'C1\', \'C2\', \'C3\', \'C4\', \'C5\'])\n\ndummy_data2 = {\n        \'attribute\': [\'C1\', \'C2\', \'C4\', \'C5\', \'C3\', ],\n        \'missing_or_unknown\': [\'X1\', \'X2\', \'X4\', \'X5\', \'X3\', ]}\n\ndf2 = pd.DataFrame(dummy_data2, columns = [\'attribute\', \'missing_or_unknown\'])\n\ndf2_transposed = df2.transpose()\nprint("df2_transposed=\\n", df2_transposed)\ndf2_transposed.columns = df2_transposed.iloc[0]\ndf2_transposed = df2_transposed.drop(df2_transposed.index[0])\nprint("df2_transposed with HEADER Replaced=\\n", df2_transposed)\n\ndf_new = pd.concat([df2_transposed, df1])\nprint("df_new=\\n", df_new)\n'
'from sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport numpy as np\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nkm = KMeans(n_clusters=3)\nkm.fit(X)\n\ndef ClusterIndices(clustNum, labels_array): #numpy \n    return np.where(labels_array == clustNum)[0]\n\nClusterIndices(1, km.labels_)\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n   17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n   34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n\nX[ClusterIndicesNumpy(1,km.labels_)]\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],...[4.8, 3. , 1.4, 0.3],\n   [5.1, 3.8, 1.6, 0.2],\n   [4.6, 3.2, 1.4, 0.2],\n   [5.3, 3.7, 1.5, 0.2],\n   [5. , 3.3, 1.4, 0.2]])\n'
"import ast\ndata['Data'] = data.Data.transform(ast.literal_eval)\n\nimport json\ndata['Data'] = data.Data.transform(json.loads)\n\ndata['Data'] = data.Data.apply(np.mean)\n"
"import pandas as pd\n\n# dummy dataframe\ndf = pd.DataFrame({'col_1': range(10), 'col_2': range(10)})\n\n# returns slices of size slice_length with step size 1\nslice_length = 5\nlst = [df.iloc[i:i+slice_length,: ] for i in range(df.shape[0] - slice_length)]\n\nslice_length = 5\n\n# loop over indices (i.e. slices)\nfor idx_from in range(df.shape[0] - slice_length):\n    # create the slice and write to file\n    df.iloc[idx_from: idx_from + slice_length, :].to_csv(f'slice_starting_idx_{idx_from}.csv', sep=';', index=False)\n"
'Index_Zero = data[data["Total"]==0].index.to_list()\nfor items in range(0, len(Index_Zero)-1):\n    data.loc[(Index_Zero[items]), \'Total\'] = data.loc[(items+168), \'Total\']\n\n'
"d = {\n    'item1': ['A', 'B', 'C'],\n    'item2': ['A', 'B', 'C', 'D'],\n    'item3': ['A', 'C', 'D'],\n    'item4': ['B', 'C', 'D', 'E'],\n}\n\nsearch_items = {'A', 'B', 'C'}\nkeys = [key for key, value in d.items() if len(search_items &amp; set(value)) &gt;= 3]\nprint(keys)\n\nvalues = [value for key, value in d.items() if len(search_items &amp; set(value)) &gt;= 3]\nprint(values)\n\n['item1', 'item2']\n[['A', 'B', 'C'], ['A', 'B', 'C', 'D']]\n\ncommon_items = [\n    (search_key, key, set(search_values) &amp; set(values))\n    for search_key, search_values in d.items()\n    for key, values in d.items()\n    if search_key != key and len(set(search_values) &amp; set(values)) &gt;= 3\n]\nprint(common_items)\n\n[('item1', 'item2', {'C', 'B', 'A'}),\n ('item2', 'item1', {'C', 'B', 'A'}),\n ('item2', 'item3', {'C', 'D', 'A'}),\n ('item2', 'item4', {'C', 'D', 'B'}),\n ('item3', 'item2', {'C', 'D', 'A'}),\n ('item4', 'item2', {'C', 'D', 'B'})]\n"
"import os\nimport pandas as pd\nimport numpy as np\npath = os.getcwd()\nfiles = os.listdir(path)\nfiles_xls = [f for f in files if f[-4:] == 'xlsx']\ndf = pd.DataFrame()\nfor f in files_xls:\n    qw = pd.read_excel(f)\n    df = df.append(qw)\n    cf = df.iloc[:, \n    df.columns.str.contains('address1|address2|city|state|zip|Location Address', case=False)]\n    vf = df['address1'].map(str) + '-' + df['address2'].map(str) + '-' + \n    df['city'].map(str) + '-' + df['state'].map(str) + '-' + df['zip'].map(str)\ndf = df.replace(np.nan, '', regex=True)\nexport_csv = vf.to_csv('dataframe.csv', index=None, header=True)\n"
'local/inputs1.csv :\n    python copyfile.py "V:\\\\Server Path\\\\With Spaces\\\\Inputs 1.csv" $@\nlocal/inputs2.csv :\n    python copyfile.py "V:\\\\Server Path\\\\With Spaces\\\\Inputs 2.csv" $@\n\noutput.csv : combine_data.R | local/inputs1.csv local/inputs2.csv\n    Rscript $^ $| $@\n'
"import numpy as np\nfrom math import pi\nfrom matplotlib.patches import Circle\nimport matplotlib.pyplot as plt\n\ndata = [50, 100]\n\nradii = np.array([(A / pi) ** .5 for A in data])\n\nradii = radii / radii.max()\nradii.sort()\nradii = radii[::-1]\n# def get_circle(A):\n#     c = Circle()\n\ncolors = ['m', 'c']\ncircles = [Circle((0, 0), r, color=color) for r, color in zip(radii, colors)]\n\nfig, ax = plt.subplots()\n\nfor c in circles:\n    ax.add_artist(c)\n\nplt.axis([-1, 1, -1, 1])\nax.set_aspect(aspect=1)\n"
'import pandas as pd \n\ndata = {\'Conditions \': [\'NormalBlink1400\', \'NormalBlink2000\', \'NormalBlink3000\', \'NormalBlink4000\',\'NormalNoBlink1400\'], \n        \'-1\': [48, 74, 77, 67,40], \n        \'1\': [108, 124, 147, 150,119]} \n\ndf = pd.DataFrame(data) \n\ndf[\'perc_one \'] = (df["1"] / (df["1"] + df["-1"]))\ndf["perc_minus_one"] = (df["-1"] / (df["1"] + df["-1"]))\n\ndf.head()\n'
'for f in listdir("./data"):\n    if f.endswith(\'.csv\'):\n        pd.read_csv(f).to_hdf(...)\n\nimport numpy as np\nimport pandas as pd\nimport os, shutil, time, h5py\n\nroot_dir = \'./data/\'  # Unique results directory\nfilepath = os.path.join(root_dir, \'file{0:03d}.csv\')\nhdfpath = os.path.join(root_dir, \'results.h5\')\n\nn_files = 10\nn_rows = 100\nn_cols = 10\n\nif True:\n    # Clear previous results\n    if os.path.isdir(root_dir):\n        shutil.rmtree(root_dir)\n        os.makedirs(root_dir)\n    for i in range(n_files):\n        print("write csv file:",i)\n        results = np.random.random((n_rows, n_cols))\n        np.savetxt(filepath.format(i), results, delimiter=\',\')\n\n# Convert the many csv files into a single hdf file\nstart_time = time.time()\n\nfor f in os.listdir("./data"):\n    if f.endswith(\'.csv\'):\n       x=\'./data/\'+f\n       y=\'./data/\'+f+\'.hd5\'\n       df=pd.read_csv(x, \'rb\',engine=\'python\')\n       df.to_hdf(y, key=f)\n\nprint(\'%s seconds\' % (time.time() - start_time))\n'
"from google.colab import files\ncompression_opts = dict(method='zip',archive_name='out.csv')  \nxyz.groupby('Country/Region').sum().to_csv('out.zip', index=True,compression=compression_opts)\nfiles.download('out.zip')\n"
"import ipywidgets as widgets\n\nbor = 'A'\ndrop_down = widgets.Dropdown(options=['A','B','C','D'],\n                                description='Choose',\n                                disabled=False)\n\ndef dropdown_handler(change):\n    global bor\n    print(change.new)\n    bor = change.new  # This line isn't working\ndrop_down.observe(dropdown_handler, names='value')\ndisplay(drop_down)\n"
"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.dates import MonthLocator, DateFormatter\n\n# Sample data\ndates = pd.date_range('2015-01-01', '2015-12-31')\nvalues = np.random.rand(len(dates))\ndf = pd.DataFrame({&quot;Date&quot;: dates,\n                   &quot;Value&quot;: values})\n\n&gt;&gt;&gt;\n    Date    Value\n0   2015-01-01  0.150016\n1   2015-01-02  0.565111\n2   2015-01-03  0.229569\n3   2015-01-04  0.952355\n4   2015-01-05  0.509594\n\nfig, ax = plt.subplots()\n\nax.plot(df.Date, df.Value)\nax.xaxis.set_major_locator(MonthLocator())  # Tick locator\nax.xaxis.set_major_formatter(DateFormatter('%B'))  # Date format\n"
"for j in bentonite:\n    if len(j)&lt;5:\n        print('Insufficient data')\n        continue\n    for i in j:\n        ...\n\nfor j in bentonite:\n    if len(j)&lt;5:\n        print('Insufficient data')\n    else:\n        for i in j:\n            ...\n"
"import pandas as pd\nlogins = {'EmployeeId': [22, 22, 22, 22, 67, 67, 67],\n    'Counter': [1, 2, 3, -1, 1, 2, 3] }\n\ndf = pd.DataFrame(logins)\ndf2 = df.loc[df['Counter'] == -1]\nprint(df[~df['EmployeeId'].isin(df2['EmployeeId'])])\n"
"chart = alt.Chart(df).mark_bar().encode(\n    x=xSelected,\n    y=ySelected[0],\n    color=alt.Color(f'{xSelected}:N'),\n    tooltip=ySelected\n)\n"
"dates['Datka'] = pd.to_datetime(df['Order Date'], format='%m/%d/%Y %H:%M')\n"
'import random\nimport numpy\nfrom matplotlib import pyplot\n\na = [3, 6, 4, 9]\nb = [4, 8, 2, 7]\n\nx = numpy.array([0,1,2,3])\n\npyplot.bar(x, a, 0.3)\npyplot.bar(x + 0.3, b, 0.3)\npyplot.show()\n'
"import pandas as pd\ndata = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data')\nprint(data)\n\n     5.1  3.5  1.4  0.2     Iris-setosa\n0    4.9  3.0  1.4  0.2     Iris-setosa\n1    4.7  3.2  1.3  0.2     Iris-setosa\n2    4.6  3.1  1.5  0.2     Iris-setosa\n3    5.0  3.6  1.4  0.2     Iris-setosa\n4    5.4  3.9  1.7  0.4     Iris-setosa\n..   ...  ...  ...  ...             ...\n144  6.7  3.0  5.2  2.3  Iris-virginica\n145  6.3  2.5  5.0  1.9  Iris-virginica\n146  6.5  3.0  5.2  2.0  Iris-virginica\n147  6.2  3.4  5.4  2.3  Iris-virginica\n148  5.9  3.0  5.1  1.8  Iris-virginica\n"
"df['capital'] = np.where((df['year'] != 1960), 2, df['GDP'].shift())\n"
'from sklearn.datasets import load_svmlight_file\ndef get_data(dn):\n    # load_svmlight_file loads dataset into sparse CSR matrix\n    X,Y = load_svmlight_file(dn)\n    print(type(X)) # you will get numpy.ndarray\n    return X,Y\n\n# X, Y = get_data(dn) uncomment this code and pass the dn parameter you want.\n# convert X to ndarray\nX = X.toarray()\nprint(type(X))\n\n# As you are going to implement logistic regression, you have to convert the \nlabels into 0 and 1 \nY = np.where(Y == -1, 0, 1)\n'
"import matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nx = [1, 8, 12, 20]\ny = [1, 8.4, 12.5, 20]\n\n# make an xx that with from -20 to 20\n#xx =np.array(x)\n#xx = sorted(np.concatenate((-xx, xx), axis=0))\nxx = [-20, 20] # also work\n\n\nfig, ax = plt.subplots(figsize=(10,10))\nax.set_xlim(-30, 30)\nax.set_ylim(-20, 20)\n\nplt.subplot().spines['left'].set_position('center')\nplt.subplot().spines['bottom'].set_position('center')\nplt.subplot().spines['right'].set_color('none')\nplt.subplot().spines['top'].set_color('none')\n\n\nplt.plot(x,y, 'b.', ms=20)\nplt.minorticks_on()\n#ax.grid(True, which='both')\nplt.subplot().grid(True, which='both')\nmean_line = ax.plot()\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\n\nplt.plot(xx,p(xx),&quot;r--&quot;)\n\nplt.show()\n"
"def count_missed_apts_before_now(row, df):\n    subdf = df.query(&quot;AppointmentDay&lt;@row.ScheduledDay and `No-show`=='Yes' and PatientId==@row.PatientId&quot;)\n    return len(subdf)\n    \nmissed_apt_counts = df.apply(count_missed_apts_before_now, axis=1, args = (df,))\n"
'df.drop(labels=&quot;cap&quot;, axis=1).div(df.cap, axis=0)\n\n       A      B      C\n0  0.482  0.959  0.067\n1  0.790  0.450  0.020\n2  0.855  0.164  0.173\n3  0.500  0.000  0.100\n4  0.659  0.831  0.899\n'
"_|a_1|a_2|a_3|a_4|b_1|b_2|b_3|b_4\n0|true|false|true|false|true|false|true|false\n1|false|true|false|true|false|true|false|true\n\n[{'a': {'1': 'true', '2': 'false', '3': 'true', '4': 'false'},\n  'b': {'1': 'true', '2': 'false', '3': 'true', '4': 'false'}},\n {'a': {'1': 'false', '2': 'true', '3': 'false', '4': 'true'},\n  'b': {'1': 'false', '2': 'true', '3': 'false', '4': 'true'}}]\n"
'pd.concat([df1,df2,df3], axis=1)\n'
'from sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.linear_model import LinearRegression\n\nnp.random.seed(111)\n\nmean = [0, 2]\ncov = [[1, 0.3], [0.3, 3]]  \n\ny = np.random.multivariate_normal(mean, cov, 100)\nX = np.random.normal(0,1,(100,2))\n\nregr_multi = MultiOutputRegressor(LinearRegression())\nregr_multi.fit(X, y)\n\nregr_list = [LinearRegression().fit(X,y[:,i]) for i in range(y.shape[1])]\n\nprint(regr_multi.estimators_[0].coef_ , regr_list[0].coef_)\n\n[-0.04355358 -0.03379101] [-0.04355358 -0.03379101]\n\nprint(regr_multi.estimators_[1].coef_ , regr_list[1].coef_)\n\n[ 0.2921806 -0.1372799] [ 0.2921806 -0.1372799]\n'
"import matplotlib.pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nfrom mpl_toolkits.mplot3d import Axes3D\nX, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3)                   # Number of clusters == 3\nkmeans = kmeans.fit(X)                          # Fitting the input data\nlabels = kmeans.predict(X)                      # Getting the cluster labels\ncentroids = kmeans.cluster_centers_             # Centroid values\n# print(&quot;Centroids are:&quot;, centroids)              # From sci-kit learn\n\n\nfig = plt.figure(figsize=(10,10))\nax = fig.gca(projection='3d')\n\nx = np.array(labels==0)\ny = np.array(labels==1)\nz = np.array(labels==2)\n\n\nax.scatter(centroids[:,0],centroids[:,1],centroids[:,2],c=&quot;black&quot;,s=150,label=&quot;Centers&quot;,alpha=1)\nax.scatter(X[x,0],X[x,1],X[x,2],c=&quot;blue&quot;,s=40,label=&quot;C1&quot;)\nax.scatter(X[y,0],X[y,1],X[y,2],c=&quot;yellow&quot;,s=40,label=&quot;C2&quot;)\nax.scatter(X[z,0],X[z,1],X[z,2],c=&quot;red&quot;,s=40,label=&quot;C3&quot;)\n\n"
"df = df.join(df.rank(pct=True).add_prefix('pct'))\nprint (df)\n  f1  f2  f3  pctf1  pctf2  pctf3\n0   1   2   3    0.2    0.4    0.6\n1   5   1   2    1.0    0.2    0.4\n2   3   3   1    0.6    0.6    0.2\n3   2   4   7    0.4    0.8    1.0\n4   4   5   4    0.8    1.0    0.8\n\ndf = df.join(df.rank().sub(1).div(len(df) - 1).add_prefix('pct'))\nprint (df)\n   f1  f2  f3  pctf1  pctf2  pctf3\n0   1   2   3   0.00   0.25   0.50\n1   5   1   2   1.00   0.00   0.25\n2   3   3   1   0.50   0.50   0.00\n3   2   4   7   0.25   0.75   1.00\n4   4   5   4   0.75   1.00   0.75\n"
"training_df['distances_normal'] = (training_df['distances'] - mean)/ std\n\ndef z_score(x, mean, std):\n    return (x - mean)/std\n\ntraining_df['distances_normal'] = training_df['distances'].apply(lambda x: z_score(x, mean, std))\n"
'import nltk\nser = #&lt;your series of strings&gt;\nlocations = df.apply(lambda x:nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(str(x)))))\n'
'df[&quot;past_seo&quot;] = df.groupby(&quot;6_cusip&quot;).seo.cumsum().gt(0).astype(int)\n\n       date 6_cusip  seo  past_seo\n0   1994-05  00077R    0         0\n1   1994-06  00077R    0         0\n2   1994-07  00077R    0         0\n3   1994-08  00077R    0         0\n4   1994-09  00077R    0         0\n5   1994-10  00077R    0         0\n6   1994-11  00077R    0         0\n7   1994-12  00077R    0         0\n8   1995-01  00077R    0         0\n9   1995-02  00077R    0         0\n10  1995-03  00077R    0         0\n11  1995-04  00077R    0         0\n12  1995-05  00077R    0         0\n13  1995-06  00077R    0         0\n14  1995-07  00077R    0         0\n15  1995-08  00077R    1         1\n16  1995-09  00077R    0         1\n17  1995-10  00077R    0         1\n18  1995-11  00077R    0         1\n19  1995-12  00077R    0         1\n'
"import pandas as pd\nfrom numpy.random import choice\nincomes = choice(['Lowest','Lower Middle','Middle','Upper Middle','Highest'], 500,\n              p=[.2,.09,.49,.11,.11])\n\ndf= pd.DataFrame({'Annual Income':incomes})\n\n\ndf.value_counts()\n\nAnnual Income\nMiddle           245\nLowest            87\nUpper Middle      66\nHighest           57\nLower Middle      45\n"
"s = df['col1'] == x\ndf.loc[s, 'new_col'] = np.random.choice(['a','b','c'], \n                                        size=s.sum(), \n                                        p=[0.3,0.3,0.4])\n"
"import scipy.stats as st\ndata = [2457.145, 1399.034, 20000.0, 476743.9, 24059.6, 28862.8]\nprint(st.pareto.fit(data, floc = 0, scale = 1))\n\n# (0.4019785013487883, 0, 1399.0339889072732)\n\nlibrary(fitdistrplus)\nlibrary(actuar)\ndata &lt;- c(2457.145, 1399.034, 20000.0, 476743.9, 24059.6, 28862.8)\nfitdist(data, 'pareto', &quot;mle&quot;)$estimate\n\n#    shape        scale \n#    0.760164 10066.274196 \n\nlibrary(fitdistrplus)\nlibrary(actuar)\ndata &lt;- c(2457.145, 1399.034, 20000.0, 476743.9, 24059.6, 28862.8)\nfitdist(data, 'pareto1', &quot;mle&quot;)$estimate\n\n#     shape          min \n#   0.4028921 1399.0284977\n\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef dpareto(x, shape, scale):\n    return shape * scale**shape / (x + scale)**(shape + 1)\n\ndef negloglik(x):\n    data = [2457.145, 1399.034, 20000.0, 476743.9, 24059.6, 28862.8]\n    return -np.sum([np.log(dpareto(i, x[0], x[1])) for i in data])\n\nres = minimize(negloglik, (1, 1), method='Nelder-Mead', tol=2.220446e-16)\nprint(res.x)\n\n# [7.60082820e-01 1.00691719e+04]\n"
'import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n#generate sample data\nnp.random.seed(123)\nn=100\ndf = pd.DataFrame({&quot;A&quot;: np.random.random(n)*50 - 5})\n\n#count the numbers per bin\nvals, bins = np.histogram(df[&quot;A&quot;], bins=[-np.inf,0.99,1,1.99,2,3.99,4,np.inf])\n\n#plot and label only every other bar\nplt.bar([f&quot;[{i}, {j})&quot; for i, j in zip(bins[::2], bins[1::2])], vals[::2])\nplt.show()\n'
'import matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport time\n\nnow_ms = int(time.time() * 1000)\nx = [now_ms+t for t in range(0,10_000,1000)]\n\nplt.plot([mpl.dates.epoch2num(t/1000) for t in x], range(10))\nplt.gca().xaxis.set_major_formatter(mpl.dates.DateFormatter(&quot;%H:%M:%S&quot;)) \n'
'a = np.zeros([N, 9])\nfor i in range(N):\n    for j in range(9):\n        a[i,j]= and so on\n\na = []\nfor i in range(N):\n    a.append([])\n    for j in range(9):\n        a[-1].append( and so on\na = np.array(a)\n'
'    dataset = pandas.read_csv(url, names=names)\nNameError: name \'pandas\' is not defined\n\nimport pandas\nurl = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"\nnames = [\'sepal-length\', \'sepal-width\', \'petal-length\', \'petal-width\', \'class\']\ndataset = pandas.read_csv(url, names=names)\n\ndataset.head(3)\n\n   sepal-length  sepal-width  petal-length  petal-width        class\n0           5.1          3.5           1.4          0.2  Iris-setosa\n1           4.9          3.0           1.4          0.2  Iris-setosa\n2           4.7          3.2           1.3          0.2  Iris-setosa\n'
"a = df[['B']].rolling('60s').apply(fn)\nprint (a)\n                       B\n2013-01-01 09:01:00  1.0\n2013-01-01 09:01:32  2.0\n2013-01-01 09:02:03  2.0\n2013-01-01 09:02:25  3.0\n2013-01-01 09:03:06  2.0\n\na = df[['B']].rolling('60s').apply(fn).astype(int)\nprint (a)\n                     B\n2013-01-01 09:01:00  1\n2013-01-01 09:01:32  2\n2013-01-01 09:02:03  2\n2013-01-01 09:02:25  3\n2013-01-01 09:03:06  2\n\na = df.assign(B=np.arange(len(df.index)))[['B']].rolling('60s').apply(fn).astype(int)\nprint (a)\n                     B\n2013-01-01 09:01:00  1\n2013-01-01 09:01:32  2\n2013-01-01 09:02:03  2\n2013-01-01 09:02:25  3\n2013-01-01 09:03:06  2\n\ndf['B'] = np.arange(len(df.index))\na = df[['B']].rolling('60s').apply(fn).astype(int)\nprint (a)\n                     B\n2013-01-01 09:01:00  1\n2013-01-01 09:01:32  2\n2013-01-01 09:02:03  2\n2013-01-01 09:02:25  3\n2013-01-01 09:03:06  2\n\ndf['B'] = np.arange(len(df.index))\na = df.groupby('A')[['B']].rolling('60s').apply(fn).astype(int)\nprint (a)\n                       B\nA                       \na 2013-01-01 09:01:00  1\n  2013-01-01 09:02:03  1\nb 2013-01-01 09:01:32  1\n  2013-01-01 09:02:25  2\ne 2013-01-01 09:03:06  1\n"
'training = tf.placeholder_with_default(False, shape=())\n'
'df1.eq(df2)\n\n                 1      2      3      4      5\ndog   dog     True   True  False   True   True\n      fox     True   True  False   True   True\n      horse  False  False  False  False  False\n      jumps   True   True  False   True   True\n      over   False  False  False  False  False\n      the     True   True  False   True   True\nfox   cat    False  False  False  False  False\n      dog     True   True  False   True   True\n      fox     True   True  False   True   True\n      jumps   True   True  False   True   True\n      over   False  False  False  False  False\n      the     True   True  False   True   True\njumps dog    False  False  False  False  False\n      fox    False  False  False  False  False\n      jumps  False  False  False  False  False\n      over   False  False  False  False  False\n      the    False  False  False  False  False\nover  dog    False  False  False  False  False\n      fox    False  False  False  False  False\n      jumps  False  False  False  False  False\n      over   False  False  False  False  False\n      the    False  False  False  False  False\nthe   dog    False  False  False  False  False\n      fox    False  False  False  False  False\n      jumps  False  False  False  False  False\n      over   False  False  False  False  False\n      the    False  False  False  False  False\n'
"pipeline = Pipeline([\n    ('clf', OneVsRestClassifier(SVC(), n_jobs=1)),\n])\n\nparameters = [\n\n    {'clf__estimator__kernel': ['rbf'],\n     'clf__estimator__gamma': [1e-3, 1e-4],\n     'clf__estimator__C': [1, 10]\n    },\n\n    {'clf__estimator__kernel': ['poly'],\n     'clf__estimator__C': [1, 10]\n    }\n     ]\n\ngrid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=3, verbose=10)\n"
'def get_slope(prev, cur):\n    # A: Ascending\n    # D: Descending\n    # P: PEAK (on previous node)\n    # V: VALLEY (on previous node)\n    return tf.cond(prev[0] &lt; cur, lambda: (cur, ascending_or_valley(prev, cur)), lambda: (cur, descending_or_peak(prev, cur)))\n\ndef ascending_or_valley(prev, cur):\n    return tf.cond(tf.logical_or(tf.equal(prev[1], \'A\'), tf.equal(prev[1], \'V\')), lambda: np.array(\'A\'), lambda: np.array(\'V\'))\n\ndef descending_or_peak(prev, cur):\n    return tf.cond(tf.logical_or(tf.equal(prev[1], \'A\'), tf.equal(prev[1], \'V\')), lambda: np.array(\'P\'), lambda: np.array(\'D\'))\n\ndef label_local_extrema(tens):\n    """Return a vector of chars indicating ascending, descending, peak, or valley slopes"""\n    # initializer element values don\'t matter, just the type.\n    initializer = (np.array(0, dtype=np.float32), np.array(\'A\'))\n    # First, get the slope for each element\n    slope = tf.scan(get_slope, tens, initializer)\n    # shift by one, since each slope indicator is the slope\n    # of the previous node (necessary to identify peaks and valleys)\n    return slope[1][1:]\n\ndef find_local_maxima(tens):\n    """Return the indices of the local maxima of the first dimension of the tensor"""\n    return tf.squeeze(tf.where(tf.equal(label_local_extrema(blur_x_tf), \'P\')))\n\ndef zero_descent(prev, cur):\n    """reduces all descent steps to zero"""\n    return tf.cond(prev[0] &lt; cur, lambda: (cur, cur), lambda: (cur, 0.0))\n\ndef skeletonize_1d(tens):\n    """reduces all point other than local maxima to zero"""\n    # initializer element values don\'t matter, just the type.\n    initializer = (np.array(0, dtype=np.float32), np.array(0, dtype=np.float32))\n    # First, zero out the trailing side\n    trail = tf.scan(zero_descent, tens, initializer)\n    # Next, let\'s make the leading side the trailing side\n    trail_rev = tf.reverse(trail[1], [0])\n    # Now zero out the leading (now trailing) side\n    lead = tf.scan(zero_descent, trail_rev, initializer)\n    # Finally, undo the reversal for the result\n    return tf.reverse(lead[1], [0])\n\ndef find_local_maxima(tens):\n    return tf.where(skeletonize_1d &gt;0)\n'
"df['Active']=(df.variable*df.value).replace('','No')\ndf\nOut[653]: \n  uuid       variable  value         Active\n0  AAS  Highly_Active  False             No\n1  AAS  Highly_Active   True  Highly_Active\n2  SAP  Highly_Active  False             No\n3  SAP  Multiple_days   True  Multiple_days\n4  YAS  Highly_Active  False             No\n5  YAS  Highly_Active  False             No\n6  YAS   Busi_weekday  False             No\n"
'import numpy as np\n\nX_amount = data["Amount"].as_matrix().reshape(-1, 1)\nX_train = X_train.toarray()\nX_train = np.hstack((X_train, X_amount))\nX_test_amount = test_data["Amount"].as_matrix().reshape(-1, 1)\nX_test = X_test.toarray()\nX_test = np.hstack((X_test, X_test_amount)) \n\nimport scipy as sp\n\nX_amount = data["Amount"].as_matrix().reshape(-1, 1)\nX_train = sp.sparse.hstack((X_train, X_amount))\nX_test_amount = test_data["Amount"].as_matrix().reshape(-1, 1)\nX_test = sp.sparse.hstack((X_test, X_test_amount)) \n'
'from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=0)\n'
"def seq_duration(row):\n    Duration = None\n    if row.ClipFirstFrameNumber is not None and row.ClipLastFrameNumber is not None:\n        fn = row.ClipLastFrameNumber -row.ClipFirstFrameNumber\n        fps = 0\n        if row.FrameRate =='23.98' and row.DropFrame == 'False' :\n            fps = 24 / 1.001\n        elif row.FrameRate == '24' and row.DropFrame == 'False':\n            fps = 24\n        elif row.FrameRate == '25'and row.DropFrame == 'False':\n            fps = 25\n        elif  row.FrameRate == '29.97':\n            fps = 30 / 1.001\n        elif  row.FrameRate == '30' and row.DropFrame == 'False':\n            fps = 30\n        elif row.FrameRate == '59.94':\n            fps = 60 / 1.001\n        if fps&gt;0:\n            Duration = fn/fps\n\n    elif row.srcDuration is not None:\n         Duration = row.srcDuration\n\n    return Duration\n\ndf['seg_duration']=df.apply(seq_duration,axis = 1)\n"
'import numpy\nlsp_re = numpy.linspace(0.1, 8, 8000)\nlsp_im = numpy.linspace(0, 20, 20001)\nre, im = numpy.meshgrid(lsp, lsp, copy=False)\nmatrix = 1j * im + re\nfor c in matrix.flat:\n    dosomething(c)\n\n&gt;&gt;&gt; numpy.linspace(0, 10, 10)\narray([  0.        ,   1.11111111,   2.22222222,   3.33333333,\n         4.44444444,   5.55555556,   6.66666667,   7.77777778,\n         8.88888889,  10.        ])\n&gt;&gt;&gt; numpy.linspace(0, 10, 11)\narray([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.])\n'
'imputer.transform\n\nimputer.transform()\n'
"df = pd.DataFrame({'a':[1,2,3,4]})\n\n#slow, working only with unique values\ndf['b'] = df['a'].apply(lambda x: df.loc[df.a != x, 'a'].mean())\n#faster\ndf['b1'] = (df['a'].sum() - df['a']) / (len(df) - 1)\nprint (df)\n   a         b        b1\n0  1  3.000000  3.000000\n1  2  2.666667  2.666667\n2  3  2.333333  2.333333\n3  4  2.000000  2.000000\n"
"df.groupby('Id').agg(','.join)\nOut[180]: \n         Question         Answer\nId                              \n48993  Color,Base  Blue,Standard\n49204  Color,Base  Blue,Extended\n49477  Color,Base   Red,Standard\n"
"from urllib.request import urlopen\n\nfor link in data['column1']:\n    with urlopen(link) as response:\n        myfile = response.read()\n        print(myfile)\n"
"df = pd.DataFrame()\ndf['time1'] = pd.date_range('2018-01-01', periods=6, freq='H') \ndf['time2'] = df['time1'] + pd.to_timedelta([60,60,120,120,180,180], 's')\ndf['id'] = range(1,7)\ndf['val'] = ['A','B'] * 3\n\ndf['t'] = df['time2'] - df['time1']\nprint (df)\n                time1               time2  id val        t\n0 2018-01-01 00:00:00 2018-01-01 00:01:00   1   A 00:01:00\n1 2018-01-01 01:00:00 2018-01-01 01:01:00   2   B 00:01:00\n2 2018-01-01 02:00:00 2018-01-01 02:02:00   3   A 00:02:00\n3 2018-01-01 03:00:00 2018-01-01 03:02:00   4   B 00:02:00\n4 2018-01-01 04:00:00 2018-01-01 04:03:00   5   A 00:03:00\n5 2018-01-01 05:00:00 2018-01-01 05:03:00   6   B 00:03:00\n\n#if necessary convert to seconds\n#df['t'] = (df['time2'] - df['time1']).dt.total_seconds()\ndf = df.pivot('t','val','id').reset_index().rename_axis(None, axis=1)\n#if necessary aggregate values\n#df = (df.pivot_table(index='t',columns='val',values='id', aggfunc='mean')\n#        .reset_index().rename_axis(None, axis=1))\nprint (df)\n         t  A  B\n0 00:01:00  1  2\n1 00:02:00  3  4\n2 00:03:00  5  6\n"
"df['Key'] = df['FirstName'] + ' ' + df['LastName']\ndf.replace('', np.NaN, inplace=True)\ndf = df.groupby('Key').apply(lambda x: x.fillna(method='ffill').fillna(method='bfill')).drop_duplicates()\n\nprint(df)\n  LastName FirstName Department    Title Extension           Email       Key\n0      Doe      Jane         HR  Officer      0000  jdoe@email.com  Jane Doe\n"
"df.groupby(df['interval_start_time'].dt.day)['percentage_rate'].apply(lambda x: np.percentile(x, [25,50,75,100]))\n"
"grouped = df.groupby(['client','facility'])\nprint(grouped.get_group(('client_abc', 'F1')))\n\n         date      client facility  count\n 0  21/3/2019  client_abc       F1    200\n 2  21/3/2019  client_abc       F1   1000\n 4  21/3/2019  client_abc       F1    900\n 6  21/3/2019  client_abc       F1    190\n"
"import pandas as pd\n\nOL = pd.read_csv('ol.csv')\nPL = pd.read_csv('pl.csv')\n\nSN = PL[PL['Price']==PL['Price'].max()]['ID']\n\nprint(OL[OL.ID == SN.values[0]])\n"
"df = pd.DataFrame({'mins': [1, 2, 2, 10, 6],\n                   'day':['2019-06-28','2019-06-28','2019-06-30','2019-06-30','2019-07-02']})\n\nres = df.groupby(['day'])['mins'].quantile(0.5).reset_index()\nres.rename(columns={'mins':'quantile_value'},inplace=True)\nprint(res)\n\n          day  quantile_value\n0  2019-06-28   1.5\n1  2019-06-30   6.0\n2  2019-07-02   6.0\n"
"pd.concat([data_rnr, data_rnr['BORROWER ADDRESS'].str.extract(r'\\s(?P&lt;BORROWER_CITY_NAME&gt;[\\w]+)\\s(?P&lt;BORROWER_CITY_PINCODE&gt;[\\d]{6})')], axis=1)\n\ndata = [\n    &quot;87 F/F Place Opp. C-2, Uttam Nagar NA Delhi 110059 Delhi&quot;,\n    &quot;87 F/F Place Opp. C-2, Uttam Nagar NA Paris 930000 Paris&quot;,\n    &quot;87 F/F Place Opp. C-2, Uttam Nagar NA Somewhere 115800 Somewhere&quot;,\n    &quot;Wrong stuff&quot;,\n    &quot;87 F/F Place Opp. C-2, Uttam Nagar NA Bombay 148444 Bombay&quot;,\n]\n\ndef regex():\n    data_rnr = pd.DataFrame(data, columns=[&quot;BORROWER ADDRESS&quot;])\n    pincoderegex=re.compile(r'\\s([\\w]+)\\s([\\d]{6})')\n    data_rnr['BORROWER CITY_NAME']='default value'\n    data_rnr['BORROWER CITY_PINCODE']='default value'\n    for i in range(0,len(data_rnr['BORROWER ADDRESS'])):\n        try:\n            data_rnr['BORROWER CITY_NAME'][i]=pincoderegex.search(data_rnr['BORROWER ADDRESS'][i]).groups()[0]\n            data_rnr['BORROWER CITY_PINCODE'][i]=pincoderegex.search(data_rnr['BORROWER ADDRESS'][i]).groups()[1]\n        except:\n            pass\n    return data_rnr\n%timeit regex()\n\n2.1 ms ± 125 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n                                    BORROWER ADDRESS BORROWER CITY_NAME BORROWER CITY_PINCODE\n0  87 F/F Place Opp. C-2, Uttam Nagar NA Delhi 11...              Delhi                110059\n1  87 F/F Place Opp. C-2, Uttam Nagar NA Paris 93...              Paris                930000\n2  87 F/F Place Opp. C-2, Uttam Nagar NA Somewher...          Somewhere                115800\n3                                        Wrong stuff      default value         default value\n4  87 F/F Place Opp. C-2, Uttam Nagar NA Bombay 1...             Bombay                148444\n\ndef pandasExtract():\n    data_rnr = pd.DataFrame(data, columns=[&quot;BORROWER ADDRESS&quot;])\n    return pd.concat([data_rnr, data_rnr['BORROWER ADDRESS'].str.extract(r'\\s(?P&lt;BORROWER_CITY_NAME&gt;[\\w]+)\\s(?P&lt;BORROWER_CITY_PINCODE&gt;[\\d]{6})')], axis=1)\n%timeit pandasExtract()\n\n1.1 ms ± 6.22 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n                                    BORROWER ADDRESS BORROWER_CITY_NAME BORROWER_CITY_PINCODE\n0  87 F/F Place Opp. C-2, Uttam Nagar NA Delhi 11...              Delhi                110059\n1  87 F/F Place Opp. C-2, Uttam Nagar NA Paris 93...              Paris                930000\n2  87 F/F Place Opp. C-2, Uttam Nagar NA Somewher...          Somewhere                115800\n3                                        Wrong stuff                NaN                   NaN\n4  87 F/F Place Opp. C-2, Uttam Nagar NA Bombay 1...             Bombay                148444\n\ndef pandasExtractWithoutNan():\n   data_rnr = pd.DataFrame(data, columns=[&quot;BORROWER ADDRESS&quot;])\n   return pd.concat([data_rnr, data_rnr['BORROWER ADDRESS'].str.extract(r'\\s(?P&lt;BORROWER_CITY_NAME&gt;[\\w]+)\\s(?P&lt;BORROWER_CITY_PINCODE&gt;[\\d]{6})').fillna('default value')], axis=1)\n%timeit pandasExtractWithoutNan()\n\n1.57 ms ± 21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n                                    BORROWER ADDRESS BORROWER_CITY_NAME BORROWER_CITY_PINCODE\n0  87 F/F Place Opp. C-2, Uttam Nagar NA Delhi 11...              Delhi                110059\n1  87 F/F Place Opp. C-2, Uttam Nagar NA Paris 93...              Paris                930000\n2  87 F/F Place Opp. C-2, Uttam Nagar NA Somewher...          Somewhere                115800\n3                                        Wrong stuff      default value         default value\n4  87 F/F Place Opp. C-2, Uttam Nagar NA Bombay 1...             Bombay                148444\n"
"import numpy as np\nimport pandas as pd\n\ndata = np.array(['a','b','c','d'])\ns = pd.Series(data)  # create dummy series\n\n1    b\n2    c\n3    d\n"
"df['new_col'] = 'Late'\ndf.loc[df['a_time'] &gt; df['b_time'], 'new_col'] = 'Early'\n\ndf['new_col'] = np.where(df['a_time'] &gt; df['b_time'], 'Early', 'Late')\n"
'import joblib\n\n# example for saving python object as pkl\njoblib.dump(vectorizer, "vectorizer.pkl")\n\n# loading pickled vectorizer\nvectorizer = joblib.load("vectorizer.pkl")\n'
"df.dt_col = pd.to_datetime(df.dt_col)\n\ndf1 = df.groupby(df.dt_col.dt.date).dt_col.agg(['min', 'max'])\n\nOut[555]:\n                           min                 max\ndt_col\n2019-03-13 2019-03-13 07:10:18 2019-03-13 08:12:23\n2019-03-15 2019-03-15 10:35:53 2019-03-15 10:35:53\n2019-03-20 2019-03-20 08:12:23 2019-03-20 11:12:23\n\ntime_arr = [pd.date_range(df1.loc[ix,'min'], df1.loc[ix,'max'], freq='S') \n                       for ix in df1.index]\n\ntime_arr = [pd.date_range(x[0], x[1], freq='S') for x in df1.values]\n\nOut[640]:\n[DatetimeIndex(['2019-03-13 07:10:18', '2019-03-13 07:10:19',\n                '2019-03-13 07:10:20', '2019-03-13 07:10:21',\n                '2019-03-13 07:10:22', '2019-03-13 07:10:23',\n                '2019-03-13 07:10:24', '2019-03-13 07:10:25',\n                '2019-03-13 07:10:26', '2019-03-13 07:10:27',\n                ...\n                '2019-03-13 08:12:14', '2019-03-13 08:12:15',\n                '2019-03-13 08:12:16', '2019-03-13 08:12:17',\n                '2019-03-13 08:12:18', '2019-03-13 08:12:19',\n                '2019-03-13 08:12:20', '2019-03-13 08:12:21',\n                '2019-03-13 08:12:22', '2019-03-13 08:12:23'],\n               dtype='datetime64[ns]', length=3726, freq='S'),\n DatetimeIndex(['2019-03-15 10:35:53'], dtype='datetime64[ns]', freq='S'),\n DatetimeIndex(['2019-03-20 08:12:23', '2019-03-20 08:12:24',\n                '2019-03-20 08:12:25', '2019-03-20 08:12:26',\n                '2019-03-20 08:12:27', '2019-03-20 08:12:28',\n                '2019-03-20 08:12:29', '2019-03-20 08:12:30',\n                '2019-03-20 08:12:31', '2019-03-20 08:12:32',\n                ...\n                '2019-03-20 11:12:14', '2019-03-20 11:12:15',\n                '2019-03-20 11:12:16', '2019-03-20 11:12:17',\n                '2019-03-20 11:12:18', '2019-03-20 11:12:19',\n                '2019-03-20 11:12:20', '2019-03-20 11:12:21',\n                '2019-03-20 11:12:22', '2019-03-20 11:12:23'],\n               dtype='datetime64[ns]', length=10801, freq='S')]\n"
"df.interpolate(method ='linear', limit_direction ='forward') \n"
"selected = train[ (label == 0) | (label == 1) ]\n\nimport numpy as np\n\ntrain = np.array(['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1'])\nlabel = np.array([0, 0, 1, 2])\n\nselected = train[ (label == 0) | (label == 1) ]\n\nprint(selected)\n\nselected = train['item'][ (label['val'] == 0) | (label['val'] == 1) ]\n\nimport pandas as pd\n\ntrain = pd.DataFrame({'item': ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1']})\nlabel = pd.DataFrame({'val': [0, 0, 1, 2]})\n\nselected = train['item'][ (label['val'] == 0) | (label['val'] == 1) ]\n\nprint(selected)\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'train': ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1'],\n    'label': [0, 0, 1, 2]\n})\n\nselected = df['train'][ (df['label'] == 0) | (df['label'] == 1) ]\n\nprint(selected)\n\ntrain = ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1']\nlabel = [0, 0, 1, 2]\n\nselected = []\n\nfor t, l in zip(train, label):\n    if l in (0, 1):\n        selected.append(t)\n\nprint(selected)\n\ntrain = ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1']\nlabel = [0, 0, 1, 2]\n\nselected = [t for t, l in zip(train, label) if l in (0, 1)]\n\nprint(selected)\n"
"df = pd.DataFrame([['app1', '01/01/2019'],\n                   ['app2', '01/02/2019'],\n                   ['app3', '01/02/2019'],\n                   ['app4', '01/02/2019'],\n                   ['app5', '01/04/2019'],\n                   ['app6', '01/04/2019']],\n                  columns=['app.no','date'])\n\nprint(pd.pivot_table(df, values='app.no', index='date', aggfunc=np.size))\n\n            app.no\ndate              \n01/01/2019       1\n01/02/2019       3\n01/04/2019       2\n"
'arr = [[0 1 2]\n       [3 4 5]\n       [6 7 8]]\n\narr[a1, a2] = [ [arr[a1[0,0], a2[0,0]] arr[a1[0,1], a2[0,1]]] ]\n                [arr[a1[1,0], a2[1,0]] arr[a1[1,1], a2[1,1]]] ]\n            = [ [arr[1,0] arr[2,2]] ]\n                [arr[0,1] arr[1,2]] ]\n\nout = [[3 8]\n      [1 5]]\n'
"In [45]: st.norm.cdf(1.96)                                                                   \nOut[45]: 0.9750021048517795\n\nIn [46]: print('(',sample_mean-1.96*standard_error,',',sample_mean+1.96*standard_error,')')  \n( 57558.007862202685 , 61510.37559873406 )\n\nIn [47]: st.norm.interval(0.95, loc=sample_mean, scale=se)                                   \nOut[47]: (57558.044175045005, 61510.33928589174)\n"
"df = df.sort_values('GPA')\n\ndf['bins'] = pd.cut(df['GPA'], bins=3, labels = ['A','B','C'])\n\n     Name   GPA bins\n3   Ramzi  1.75    A\n2  Djamel  2.10    A\n1   Betty  2.75    B\n4   Alexa  3.15    C\n0    Adel  3.50    C\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame ({'City': ['London', 'Jakarta', 'Newyork', 'Mumbai'],\n                    'Staff': [1000,2000,3000,400]})\nax = df.plot(kind='line',x='City', y='Staff', color= 'Blue')\n\n# Set axis labels as desired\nax.set_xticklabels(df['City'])\nax.set_xticks(df.index)\n\nplt.show()\n"
'138\n158\n159\n162\n155\n159\n165\n151\n159\n161\n[0.46546547 0.43843844 0.48648649 ... 0.45345345 0.47747748 0.48348348]\n'
"# you can declare number of splits here\nkfold = model_selection.KFold(n_splits=5, random_state=42)\n# your model goes here. \nmodel = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n# this will fit your model 5 times and use 1/5 as test data and 4/5 as training data\nresults = model_selection.cross_val_score(model, X_train,  y_train, cv=kfold)\n"
"grid = GridSearchCV(pipeline, param_grid=parameters,scoring='neg_mean_absolute_error', cv=None, refit=True)\n"
"df['return'] = np.nan\nmask = df.groupby('ticker')['fyear'].apply(lambda x: x.shift(1)==x-1)\ndf.loc[mask,'return'] = df.groupby('ticker')['price'].pct_change()\n"
'import numpy as np\nfrom sklearn import linear_model\nimport seaborn as sns\nfrom sklearn.preprocessing import PolynomialFeatures\n\nLR = linear_model.LinearRegression()\n\nx = np.linspace(7, 15, num=100)\ny = x + 2*x**2 + np.random.normal(0,5,size=100)\npoly = PolynomialFeatures(2)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n\nclf = linear_model.LinearRegression()\nclf.fit(poly.fit_transform(X_train.reshape(-1,1)), y_train)\npred_train = clf.predict(poly.fit_transform(X_train.reshape(-1,1)))\npred_test = clf.predict(poly.fit_transform(X_test.reshape(-1,1)))\n\ndf = pd.DataFrame({\'x\':X_train,\'y\':y_train,\'pred\':pred_train})\nsns.scatterplot(x=\'x\',y=\'y\',data=df)\nsns.lineplot(x=\'x\',y=\'pred\',data=df,color="orange")\n\ndf = pd.DataFrame({\'x\':X_test,\'y\':y_test,\'pred\':pred_test})\nsns.scatterplot(x=\'x\',y=\'y\',data=df)\nsns.lineplot(x=\'x\',y=\'pred\',data=df,color="orange")\n'
"def answer_one():\n    x= df[df['Gold.2']==df['Gold.2'].max()]\n    return x.index.values[0]\nanswer_one()\n"
"reshape_df.loc[reshape_df['Max_pollutant']=='O_3NO_2']\n"
"space = {\n\n    'warm_start' : hp.choice('warm_start', [True, False]),\n    'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n    'tol' : hp.uniform('tol', 0.00001, 0.0001),\n    'C' : hp.uniform('C', 0.05, 3),\n    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n    'max_iter' : hp.choice('max_iter', range(5,1000))\n}\n"
"df['CC_count'] = df.CC.str.count(';').add(1)\n\nprint(df)\n                                               CC                    Body  \\\n0  indiejesse.d@gmail.com; pamelasilvera69@gmail....  conference meeting ...   \n\n   CC_count  \n0         3\n"
"from itertools import cycle\ndf=df.set_index('Index').drop_duplicates()\nnewdf= pd.DataFrame(data=df.sentence, columns=['sentence'], index=df.index)\nnewdf['ending']=df[df.columns[1:]].values.tolist()\nnewdf=newdf.explode('ending')\nids = cycle([0,1,2,3])\nnewdf.insert(1, 'endingid', [next(ids) for idx in range(len(newdf))])\nprint(newdf)\n\n      sentence  endingid ending\nIndex                          \n0          ABC         0    DEF\n0          ABC         1    GHI\n0          ABC         2    JKL\n0          ABC         3    MNO\n1          LKJ         0    KJS\n1          LKJ         1    AJA\n1          LKJ         2    QHW\n1          LKJ         3    IUH\n"
"for x in range(-2000,2000, 300):\n    y=(1 + x / (1 + abs(x)))/2\n    print(y)\n\nimport pandas as pd\nd = {'not_normalized_column': [-1000, 1000], 'normalized_column': [None, None]}\ndf = pd.DataFrame(data=d)\nprint(&quot;This is the toy dataframe before running the normalization:\\n&quot;,df)\n    \ndf['normalized_column'] = df['not_normalized_column'].apply(lambda x: (1 + x / (1 + abs(x))) / 2)\n    \nprint(&quot;\\n This is the toy dataframe after running the normalization:&quot;,df)\n"
'&gt; print(data_copy)\n\n   sensor_id      label       avg5          timestamp5\n0    1385001  50.000000  49.484848 2014-08-01 00:00:00\n1    1385001  48.615383  51.735294 2014-08-01 00:05:00\n2    1385001  50.000000  51.593750 2014-08-01 00:10:00\n3    1385001  48.615383  49.266666 2014-08-01 00:15:00\n4    1385001  48.000000  50.135136 2014-08-01 00:20:00\n5    1385001  47.909092  50.500000 2014-08-01 00:25:00\n6    1385001  51.416668  50.800000 2014-08-01 00:30:00\n7    1385001  48.368420  52.692307 2014-08-01 00:35:00\n8    1385001  49.863636  50.000000 2014-08-01 00:40:00\n9    1385001  48.666668  48.615383 2014-08-01 00:45:00\n'
'P = [ [0 for i in range(len(listT)] for j in range(len(dPdT)) ]\nfor row in range(len(dPdT)):\n     for col in range(len(listT)):\n          P[row][col] = dPdT[row] * listT[col]\n'
"X = df.values\n\nfrom sklearn.cluster import Birch, KMeans, SpectralClustering, AgglomerativeClustering, DBSCAN\n\nbrc = Birch(n_clusters=None) # with int value, it'll perform Agglomerative Clustering\nbrc.fit(X)\nbrc.predict(X) # array of points belonging to unique clusters\n\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\nkmeans.labels_ # array of clusters according to indices\n\nclustering = SpectralClustering(n_clusters=2, assign_labels=&quot;discretize&quot;,random_state=0).fit(X)\nclustering.labels_  # label of each data point belonging to the cluster\n\nclustering = AgglomerativeClustering().fit(X)\nclustering.labels_\n\nclustering = DBSCAN(eps=3, min_samples=2).fit(X)\nclustering.labels_\n\n"
"df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB'))\n\n\n    A   B\n0   1   2\n1   3   4  &lt;---- ALL INTEGERS\n\ndf2 = pd.DataFrame([[np.nan, 6], [7, 8]], columns=list('AB'))\n\n    A   B\n0   NaN 6\n1   7.0 8 &lt;-- NOT INTEGER\n\ndf.append(df2, ignore_index=True)\n\n    A   B\n0   1.0 2\n1   3.0 4\n2   NaN 6\n3   7.0 8\n"
"import keras\nimport numpy as np\n\n# Dummy database class\nclass DB:\n  def get_total_records_count(self):\n    return 1e6\n  \n  def read_records_at(self, ids):\n    X = np.random.randn(len(ids), 50)\n    y = np.random.randint(0, 5, len(ids))\n    return X, y\n\n# Generator which generate a batch at a time\nclass DataGenerator(keras.utils.Sequence):\n  def __init__(self, db, batch_size=32):\n    self.db = db\n    self.n = self.db.get_total_records_count()\n    self.idx = np.arange(self.n)\n    self.batch_size = batch_size\n\n  def __len__(self):\n    return int(np.floor(self.n / self.batch_size))\n\n  # Generate a batch of (X, y)\n  def __getitem__(self, index):\n    idxs = self.idx[index*self.batch_size:(index+1)*self.batch_size]\n    return self.db.read_records_at(idxs)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(5, input_dim=(50)))\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n\ndf = DataGenerator(DB(), 4)\nmodel.fit_generator(df)\n\nEpoch 1/1\n250000/250000 [==============================] - 380s 2ms/step - loss: 7.1443\n&lt;keras.callbacks.callbacks.History at 0x7fa3ff150048&gt;\n"
"df =  df['Tickets']\n\ndf =  df[['Tickets']]\n"
"result = pd.DataFrame(\n{'idx': [226, 1070, 1914, 2758, 3602, 4446, 5290, 6134, 6978, 7822],\n 'BID': [7249, 7326, 7327, 7328, 7329, 7330, 7331, 7332, 7333, 7333],\n 'Testschritt': ['R150','R150',  'R150',  'R150',  'R150',  'R150',  'R150',  'R150',  'R150',  'R150'],\n 'Testbeschreibung': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n 'Sollwert': [22, 22, 22, 22, 22, 22, 22, 22, 22, 22],\n 'Minimum': [19.8, 19.8, 19.8, 19.8, 19.8, 19.8, 19.8, 19.8, 19.8, 19.8],\n 'Maximum': [24.2, 24.2, 24.2, 24.2, 24.2, 24.2, 24.2, 24.2, 24.2, 24.2],\n 'Istwert': [20953,  21002,  20838,  20827,  20879,  20942,  20999,  20855,  20969,  20874],\n 'Einheit': ['KOhm',  'KOhm',  'KOhm',  'KOhm',  'KOhm',  'KOhm',  'KOhm',  'KOhm',  'KOhm',  'KOhm']}\n).set_index(&quot;idx&quot;)\nprint(f&quot;pandas: {pd.__version__}\\n{result.dtypes}&quot;)\nresult.boxplot(column = [&quot;Istwert&quot;])\n\npandas: 1.1.0\nBID                   int64\nTestschritt          object\nTestbeschreibung      int64\nSollwert              int64\nMinimum             float64\nMaximum             float64\nIstwert               int64\nEinheit              object\ndtype: object\n"
'df[&quot;Weighted_avg&quot;] = df.apply(lambda r: ( (r[&quot;Asin_code&quot;] *wr[r[&quot;Year&quot;]]).sum(axis = 0)), axis=1)\n\n\n12  2016    2   12206   21.0    105.0\n13  2016    2   29306   10.0    50.0\n14  2016    2   33426   11.0    55.0\n15  2016    2   37206   1.0     5.0\n16  2016    2   45216   5.0     25.0\n17  2016    2   60160   7.0     35.0\n18  2016    2   76110   12.0    60.0\n19  2016    2   80215   NaN     NaN\n20  2016    2   84105   2.0     10.0\n21  2016    2   85034   1.0     5.0\n22  2016    2   93711   23.0    115.0\n23  2016    2   98433   7.0     35.0\n24  2016    3   12206   95.0    475.0\n25  2016    3   29306   26.0    130.0\n26  2016    3   33426   51.0    255.0\n27  2016    3   37206   18.0    90.0\n28  2016    3   45216   34.0    170.0\n29  2016    3   60160   30.0    150.0\n... ... ... ... ... ...\n2778    2020    29  76110   33.0    1320.0\n2779    2020    29  80215   5.0     200.0\n2780    2020    29  84105   3.0     120.0\n2781    2020    29  85034   8.0     320.0\n2782    2020    29  93711   53.0    2120.0\n2783    2020    29  98433   15.0    600.0\n2784    2020    30  12206   75.0    3000.0\n2785    2020    30  29306   27.0    1080.0\n2786    2020    30  33426   34.0    1360.0\n2787    2020    30  37206   12.0    480.0\n2788    2020    30  45216   14.0    560.0\n2789    2020    30  60160   28.0    1120.0\n2790    2020    30  76110   47.0    1880.0\n2791    2020    30  80215   11.0    440.0\n2792    2020    30  84105   3.0     120.0\n2793    2020    30  85034   17.0    680.0\n2794    2020    30  93711   62.0    2480.0\n2795    2020    30  98433   13.0    520.0\n2796    2020    31  12206   109.0   4360.0\n2797    2020    31  29306   30.0    1200.0\n2798    2020    31  33426   31.0    1240.0\n2799    2020    31  37206   14.0    560.0\n2800    2020    31  45216   23.0    920.0\n2801    2020    31  60160   21.0    840.0\n2802    2020    31  76110   25.0    1000.0\n2803    2020    31  80215   7.0     280.0\n2804    2020    31  84105   4.0     160.0\n2805    2020    31  85034   8.0     320.0\n2806    2020    31  93711   71.0    2840.0\n2807    2020    31  98433   9.0     360.0\n'
"df[&quot;Dates&quot;] = pd.to_datetime(df[&quot;Dates&quot;])\n    ...: \nfor x , y in df.groupby('Company'):\n    ...:     print(y)\n    ...:     print(y['Dates'].diff().dt.total_seconds())\n    ...:     \n  Company               Dates\n2      FB 1970-01-01 01:00:03\n5      FB 1970-01-01 01:00:06\n2    NaN\n5    3.0\nName: Dates, dtype: float64\n  Company               Dates\n0    GOOG 1970-01-01 01:00:00\n3    GOOG 1970-01-01 01:00:04\n0    NaN\n3    4.0\nName: Dates, dtype: float64\n  Company               Dates\n1    MSFT 1970-01-01 01:00:02\n4    MSFT 1970-01-01 01:00:05\n1    NaN\n4    3.0\nName: Dates, dtype: float64\n"
'custom_ticks = [0, 0.03, 0.1, 0.16, 0.23, 0.27, 0.32, 0.35, 0.39, 0.41]\nax.set_yticks(custom_ticks)\n'
'matplotlib.use(backend, force=True)\n'
'import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nnp.random.seed(1)\n\ndf_actual = pd.DataFrame(data={\n    &quot;date&quot;  : pd.date_range(start=&quot;2020-01-01&quot;, periods=8, freq=&quot;MS&quot;),\n    &quot;value&quot; : np.random.randint(10, 30, 8),\n})\n\ndf_forecast = pd.DataFrame(data={\n    &quot;date&quot;  : pd.date_range(start=&quot;2020-08-01&quot;, periods=4, freq=&quot;MS&quot;),\n    &quot;value&quot; : np.random.randint(10, 30, 4)\n})\n\n#first forecast value == last actual value\ndf_forecast.iloc[0, :] = df_actual.iloc[-1, :]\n\ndf_forecast[&quot;type&quot;] = &quot;forecast&quot;\ndf_actual[&quot;type&quot;] = &quot;actual&quot;\n\ndf = pd.concat([df_actual, df_forecast])\n\nplt.figure(figsize=(10,5))\nsns.lineplot(x=&quot;date&quot;, y=&quot;value&quot;, hue=&quot;type&quot;, data=df)\n'
"df.pivot(index='file', columns='class', values='value').fillna(0).reset_index()\n\nclass   file    apple   mango   orange\n0      file1    74.0    55.0    62.0\n1      file2    0.0     75.0    42.0\n2      file3    89.0    0.0     0.0\n3      file4    35.0    0.0     54.0\n"
"np.random.seed(1)\ndf = pd.DataFrame({'Number':np.arange(10), 'Day Shift':np.random.choice(['Yes', 'No'], 10),\n                  'On Mid Shift':np.random.choice(['Yes', 'No'], 10),\n                  'On Weekend Shift':np.random.choice(['Yes', 'No'], 10)})\n\ndf['No shifts'] = (df[['Day Shift', 'On Mid Shift', 'On Weekend Shift']] == 'No').all(axis=1)\nprint(df)\n\n   Number Day Shift On Mid Shift On Weekend Shift  No shifts\n0       0        No          Yes              Yes      False\n1       1        No           No               No       True\n2       2       Yes          Yes              Yes      False\n3       3       Yes           No              Yes      False\n4       4        No           No               No       True\n5       5        No          Yes              Yes      False\n6       6        No          Yes              Yes      False\n7       7        No           No              Yes      False\n8       8        No          Yes               No      False\n9       9       Yes          Yes              Yes      False\n"
"txt_raw = 'hcc-survival/hcc-data.txt'\n"
"ncols=[]\nfor i in range(len(BuffaloBillsO.columns)):\n    ncols.append(BuffaloBillsO.columns[i][1])\nncols=dict(zip(BuffaloBillsO.columns,ncols))\nBuffaloBillsO.columns =BuffaloBillsO.columns.to_series().map(ncols)\n\nIndex(['Week', 'Day', 'Date', 'Unnamed: 3_level_1', 'Unnamed: 4_level_1', 'OT',\n       'Unnamed: 6_level_1', 'Opp', 'Tm', 'Opp', 'Cmp', 'Att', 'Yds', 'TD',\n       'Int', 'Sk', 'Yds.1', 'Y/A', 'NY/A', 'Cmp%', 'Rate', 'Att', 'Yds',\n       'Y/A', 'TD', 'FGM', 'FGA', 'XPM', 'XPA', 'Pnt', 'Yds', '3DConv',\n       '3DAtt', '4DConv', '4DAtt', 'ToP'],\n      dtype='object')\n"
"create table T1(\n    id integer primary key,\n    tel integer, \n    name varchar(100)\n    );\n\ncreate table T2(\n    id integer primary key,\n    contactDate date\n    );\n\ncreate table T2(\n    id integer primary key,\n    contactDate date\n    );\n\n-- Table 'T1' Insertion\n\ninsert into T1(id, tel, name) VALUES \n    (1, 1234, &quot;Denis&quot;),\n    (2, 4567, &quot;Michael&quot;),\n    (3, 3425,&quot;Peter&quot;),\n    (4, 3242, &quot;Mary&quot;);\n\n-- Table 'T2' Insertion              \n              \ninsert into T2(id, contactDate) VALUES \n    (1, 20140105),\n    (2, 20030105),\n    (3, 20201001),\n    (4, Null);    \n\nCREATE TABLE T3 AS\n    SELECT T1.id, T1.name, T1.tel, T2.contactDate\n    FROM T1\n    INNER JOIN T2 ON T1.id=T2.id;\n\n| id |   name  | tel  | contactDate |\n|:--:|:-------:|------|-------------|\n| 1  | Denis   | 1234 | 2014-01-05  |\n| 2  | Michael | 4567 | 2003-01-05  |\n| 3  | Peter   | 3425 | 2020-10-01  |\n| 4  | Mary    | 3242 | NULL        |\n"
"# separate ColumnDataSource for boxes\nboxes_data = pd.concat([\n    q1.rename(columns={&quot;score&quot;:&quot;q1&quot;}),\n    q2.rename(columns={&quot;score&quot;:&quot;q2&quot;}),\n    q3.rename(columns={&quot;score&quot;:&quot;q3&quot;}),\n    iqr.rename(columns={&quot;score&quot;:&quot;iqr&quot;})\n], axis=1)\n\n\n# boxes\nboxes_source = ColumnDataSource(boxes_data)\ntop_box = p.vbar(\n    &quot;group&quot;, 0.7, &quot;q2&quot;, &quot;q3&quot;, fill_color=&quot;#E08E79&quot;,\n    line_color=&quot;black&quot;, source=boxes_source)\n\nbottom_box = p.vbar(\n    &quot;group&quot;, 0.7, &quot;q1&quot;, &quot;q2&quot;, fill_color=&quot;#3B8686&quot;,\n    line_color=&quot;black&quot;, source=boxes_source)\n\n# add hover just to the two box renderers\nbox_hover = HoverTool(renderers=[top_box, bottom_box],\n                         tooltips=[\n                             ('q1', '@q1'),\n                             ('q2', '@q2'),\n                             ('q3', '@q3'),\n                             ('iqr', '@iqr')\n                         ])\np.add_tools(box_hover)\n"
'df.reset_index() # this could allow your to access lat and lon as normal column\n'
'dummies = pd.get_dummies(df[&quot;loan_status&quot;],drop_first=True) \n\nnew_data = pd.concat([df,dummies],axis=1)\n\n'
"n, bins, patches = plt.hist(var1, bins=8, range=(0,4000), color=&quot;orange&quot;, alpha=0.7)\nfor i, patch in enumerate(patches):\n    plt.setp(patch, &quot;facecolor&quot;, colors[i])\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# create random values and store them in a DataFrame\ny1 = np.random.randint(0,4000, 50)\ny2 = np.random.randint(-100, 101, 50)\ny = zip(y1,y2)\ndf = pd.DataFrame(y, columns=[&quot;Var1&quot;,&quot;Var2&quot;])\n\nvar1 = df[&quot;Var1&quot;].values\n\n# pd.cut to bin the dataframe in the appropriate ranges of Var1\n# then the mean of Var2 is calculated for each bin, results are stored in a list\nmean = [df.groupby(pd.cut(df[&quot;Var1&quot;], np.arange(0, 4000+500, 500)))[&quot;Var2&quot;].mean()]\n\n# how to color the bars based on Var2:\n# -100 &lt;= mean(Var2) &lt; -33: blue\n# -33 &lt;= mean(Var2) &lt; 33: red\n# 33 &lt;= mean(Var2) &lt; 100: green\ncolor_bins = np.array([-100,-33,33,100])\ncolor_list = [&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;]\n\n# bin the means of Var2 according to the color_bins we just created\ninds = np.digitize(mean, color_bins)\n\n# list that assigns the appropriate color to each patch\ncolors = [color_list[value-1] for value in inds[0]]\n\nn, bins, patches = plt.hist(var1, bins=8, range=(0,4000), color=&quot;orange&quot;, alpha=0.7)\nfor i, patch in enumerate(patches):\n    plt.setp(patch, &quot;facecolor&quot;, colors[i])\n\nplt.title('Var 1',weight='bold', fontsize=18)\nplt.yticks(weight='bold')\nplt.xticks(weight='bold')\n\nplt.show()\n"
'from sklearn.linear_model import LinearRegression\nclass MyRegression(LinearRegression):\n    def __init__(self, *, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None):\n        super().__init__(fit_intercept=fit_intercept, normalize=normalize, copy_X=copy_X, n_jobs=n_jobs)\n\n    def modified_fit(self, x, y):\n        return self.fit(x, y)\n\n\nx = [(1, 2),(2, 3)]\ny = [1, 2]\n\nregression = MyRegression()\nregression.modified_fit(x, y)\n\nfrom sklearn.linear_model import LinearRegression\nclass MyRegression(LinearRegression):\n    def __init__(self):\n        super().__init__()\n'
"import copy\nimport math\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef get_percentile(data: list, p: float):\n    _data = copy.copy(data)\n    _data.sort()\n    result = _data[math.floor(len(_data) * p) - 1]\n    return result\n\n\nsin_list = []\nfor i in range(200):\n    sin_list.append(math.sin(i / 10) + i / 100)\n\nidx = pd.date_range('2018-01-01', periods=200, freq='H')\n\nts = pd.Series(sin_list, index=idx)\n\ngradient_ts = abs(ts.diff())\n\npercentile = get_percentile(gradient_ts.values, p=0.05)\n\nbinary_ts = gradient_ts.where(gradient_ts &gt; percentile, 1).where(gradient_ts &lt; percentile, 0)\n\nfig, ax = plt.subplots()\nbinary_ts.plot(drawstyle=&quot;steps&quot;, ax=ax)\nax.fill_between(binary_ts.index, binary_ts, facecolor='green', alpha=0.5, step='pre')\n\nts.plot(secondary_y=True, style='.')\n\nplt.show()\n"
"dfs = pd.read_excel('file.xlsx', sheet_name=None)\n\nfor sheet_name, data in dfs.items():\n    data.to_csv(f&quot;{sheet_name}.csv&quot;)\n    \n"
"import pandas as pd\n\ndf = pd.read_csv('sensors.txt', delimiter=',', header=0)\n\ndf2 = df.groupby('timestamp').ffill()\ndf2['timestamp'] = df['timestamp']\ndf2 = df2.groupby('timestamp').bfill()\ndf2['timestamp'] = df['timestamp']\ndf2 = df2.drop_duplicates()\n\ndf = df2[['timestamp', 'sensor_a', 'sensor_b', 'sensor_c', 'label']]\nprint(df)\n\n   timestamp sensor_a sensor_b sensor_c  label\n0          1       x1      NaN      NaN      0\n1          2      NaN       x2       x3      0\n3          3       x4      NaN      NaN      1\n4          4      NaN      NaN      NaN      1\n5          5      NaN       x6       x7      1\n"
'library(zoo)\nset.seed(123)\nn &lt;- 4680\nw &lt;- 60\nx &lt;- rnorm(4680)\n\nr2 &lt;- function(x) summary(lm(x ~ seq_along(x)))$r.squared\n# r2 &lt;- function(x) cor(x, seq_along(x))^2  # this is equivalent\n\nmean(rollapplyr(x, w, r2))\n## [1] 0.4992133\n\nr2(x)\n## [1] 0.0001151601\n'
'grouped_data = (df.groupby(["ssc_b", "hsc_b"]).status\n                  .value_counts(normalize=True).unstack(2)\n               )\n'
"plt.plot(x, lin_reg2.predict(poly_reg.fit_transform(x)), color = 'blue')\n"
"result = df.groupby('Countries').apply(\n             lambda group: (group.IsVertical == 'TRUE').sum() / \n                            float(group.IsVertical.count())\n         ).to_frame('Vertical')\n\nresult['NotVertical'] = 1 - result.Vertical\n&gt;&gt;&gt; result\n           Vertical  NotVertical\nCountries                       \nItaly             1            0\nPoland            0            1\nUSA               0            1\n\nresult.plot(kind='bar')\n"
"(df['bytesbytes']*df['bytesfrequency']).drop_duplicates()\n"
'import ast\n\n&gt;&gt;&gt; ast.literal_eval(s)\n\n[{\'id\': 10296, \'name\': \'Women in Technology\', \'urlkey\': \'witi\'},\n {\'id\': 15145,\n  \'name\': \'Cross Mentoring with expert CEO business owners\',\n  \'urlkey\': \'cross-mentoring-with-expert-ceo-business-owners\'},\n {\'id\': 19882, \'name\': \'Entrepreneurship\', \'urlkey\': \'entrepreneurship\'},\n {\'id\': 21283,\n  \'name\': "Women\'s Business Networking",\n  \'urlkey\': \'womens-business-networking\'},\n {\'id\': 21681, \'name\': \'Startup Businesses\', \'urlkey\': \'startup-businesses\'},\n {\'id\': 38660, \'name\': \'Lean Startup\', \'urlkey\': \'lean-startup\'},\n {\'id\': 41905,\n  \'name\': \'Female Entrepreneurs\',\n  \'urlkey\': \'female-entrepreneurs\'},\n {\'id\': 46616, \'name\': \'Founders\', \'urlkey\': \'founders\'},\n {\'id\': 108403,\n  \'name\': \'Technology Startups\',\n  \'urlkey\': \'technology-startups\'},\n {\'id\': 133122,\n  \'name\': \'CEO 2 CEO Coaching &amp; Mentoring For Mutual Growth\',\n  \'urlkey\': \'ceo-2-ceo-coaching-mentoring-for-mutual-growth\'},\n {\'id\': 141917, \'name\': \'CTO\', \'urlkey\': \'cto\'},\n {\'id\': 141921, \'name\': \'CEO\', \'urlkey\': \'ceo\'},\n {\'id\': 816562, \'name\': \'C-Level Tech\', \'urlkey\': \'c-level-tech\'},\n {\'id\': 1379732, \'name\': \'CEOs &amp; Founders\', \'urlkey\': \'ceos-founders\'},\n {\'id\': 1485582, \'name\': \'CIO / CTO\', \'urlkey\': \'cio-cto\'}]\n'
"single_times = [4, 5, 2, 3, -1]\ntime_ranges = [(1, 5), (10, 11), (2, 3)]\n\nres = [[0], [0], [0, 2], [0, 2], []]\n\nimport itertools as it\n\ndef matching_times(single_times, time_ranges):\n    single_index = sorted(xrange(len(single_times)), key=lambda i: single_times[i])\n    single_times_sorted = [single_times[i] for i in single_index]\n    time_ranges_sorted = sorted([(i, v[0], v[1]) for i, v in enumerate(time_ranges)], key=lambda w: w[1])\n\n    m = 0  # keep track of min location in time_ranges_sorted\n    res = [[]]\n\n    # Find solutions for single_times_sorted[0]\n    for i, w in enumerate(time_ranges_sorted):\n        if w[1] &gt; single_times_sorted[0]:\n            break\n        if w[2] &gt;= single_times_sorted[0]:\n            res[0].append(w)\n            m = i+1\n\n    for cur_time in it.islice(single_times_sorted, 1, len(single_times_sorted)):\n        # Keep previous solutions that don't end too soon\n        res.append([w for w in res[-1] if w[2]&gt;=cur_time])\n\n        # Strip extraneous information as soon as possible to preserve a semblance\n        # of memory efficiency\n        res[-2] = [w[0] for w in res[-2]]\n\n        for i, w in enumerate(it.islice(time_ranges_sorted, m, len(time_ranges_sorted)), m):\n            if w[1] &gt; cur_time:\n                break\n            if w[2] &gt;= cur_time:\n                res[-1].append(w)\n                m = i+1\n\n    # Strip remaining extra information from solution\n    res[-1] = [w[0] for w in res[-1]]\n\n    # Re-sort result according to original locations in single_times\n    return [v[1] for v in sorted(enumerate(res), key=lambda v: single_index[v[0]])]\n\nres = matching_times(single_times, time_ranges); res\n&gt;&gt;&gt; [[0], [0], [0, 2], [0, 2], []]\n"
"dates = pd.date_range(start='2018-04-09', end='2018-05-17', freq='H')\ndates\n\nDatetimeIndex(['2018-04-09 00:00:00', '2018-04-09 01:00:00',\n           '2018-04-09 02:00:00', '2018-04-09 03:00:00',\n           '2018-04-09 04:00:00', '2018-04-09 05:00:00',\n           '2018-04-09 06:00:00', '2018-04-09 07:00:00',\n           '2018-04-09 08:00:00', '2018-04-09 09:00:00',\n           ...\n           '2018-05-16 15:00:00', '2018-05-16 16:00:00',\n           '2018-05-16 17:00:00', '2018-05-16 18:00:00',\n           '2018-05-16 19:00:00', '2018-05-16 20:00:00',\n           '2018-05-16 21:00:00', '2018-05-16 22:00:00',\n           '2018-05-16 23:00:00', '2018-05-17 00:00:00'],\n          dtype='datetime64[ns]', length=913, freq='H')\n\ndf_new = pd.DataFrame()   \n\nfor x in range(0, len(dates) - 2, 2):\n    start_date = str(dates[x])[:13]\n    end_date = str(dates[x+1])[:13]\n    print(start_date, end_date)\n\n\ndf_temp = df[start_date:end_date]   \n# Each hour of data collected to new dataframe. \n"
'lis1=[{\'FIRST_NAME\':\'James\',\'Last_Name\':\'Cameran\',\'City\':\'NYC\'},{\'FIRST_NAME\':\'Samuel\',\'Last_Name\':\'Smith\',\'City\':\'London\'},{\'FIRST_NAME\':\'Kane\',\'Last_Name\':\'Win\',\'City\':\'NYC\'}]\nlis2=[{\'FIRST_NAME\':\'James\',\'Last_Name\':\'Cameran\',\'Pub. Year\':2011},{\'FIRST_NAME\':\'Kane\',\'Last_Name\':\'Win\',\'Pub. Year\':2010},{\'FIRST_NAME\':\'James\',\'Last_Name\':\'Cameran\',\'Pub. Year\':2018},{\'FIRST_NAME\':\'Kane\',\'Last_Name\':\'Win\',\'Pub. Year\':2014}]\nimport pandas as pd\ndf1=pd.DataFrame(lis1)\ndf2=pd.DataFrame(lis2)\n\nprint(df1)\nprint(df2)\n\ndf1[\'Full_Name\']=df1.FIRST_NAME+" "+df1.Last_Name\ndf2[\'Full_Name\']=df2.FIRST_NAME+" "+df2.Last_Name\n\nmerged=pd.merge(df1,df2)[[\'Full_Name\',\'Pub. Year\']]\n\ndf1[\'Pub. Year\']=[merged[merged.Full_Name==fullname][\'Pub. Year\'].min() for fullname in df1.Full_Name]\nprint(df1)\n\n     City FIRST_NAME Last_Name\n0     NYC      James   Cameran\n1  London     Samuel     Smith\n2     NYC       Kane       Win\n\n     FIRST_NAME Last_Name  Pub. Year\n0      James   Cameran       2011\n1       Kane       Win       2010\n2      James   Cameran       2018\n3       Kane       Win       2014\n\n     City FIRST_NAME Last_Name      Full_Name  Pub. Year\n0     NYC      James   Cameran  James Cameran     2011.0\n1  London     Samuel     Smith   Samuel Smith        NaN\n2     NYC       Kane       Win       Kane Win     2010.0\n'
'with open("drinks.csv") as file:\n    lines = file.readlines()\n    countries = [line.split(",")[0] for line in lines[0:10]] \n    my_data = [int(line.split(",")[1]) for line in lines[0:10]] \n\nplt.figure()\ny_pos = np.arange(len(countries))\nplt.barh(y_pos, my_data)\nplt.yticks(y_pos, countries)\nplt.show() \n'
'# cast to datetime\ndf["DT_NASCIMENTO_BENEFICIARIO"] = pd.to_datetime(df["DT_NASCIMENTO_BENEFICIARIO"])\ndf["age"] = df["ANO_CONCESSAO_BOLSA"] - df["DT_NASCIMENTO_BENEFICIARIO"].dt.year\n\n# print the result, or do something else with it:\nprint(df["age"])\n'
"df2 = df.groupby(['Date', 'Kind'])['Kind'].count().unstack('Kind').fillna(0)\n\ndf2 = df2.resample('D').sum()\ndf2.index = df2.index.date    \n\ndf2.plot(kind='bar', stacked=True)\n\ncols = df['Kind'].unique() # Find all original values in the column \nind = range(len(df2))\n\np1 = plt.bar(ind, df2[cols[0]])\np2 = plt.bar(ind, df2[cols[1]], bottom=df2[cols[0]])\n"
"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport numpy as np\n\n\nfig = plt.figure(figsize=(10, 10))\nax = plt.axes(projection=ccrs.PlateCarree())\nax.coastlines('10m')\n\ny = np.linspace(40.0, 60.0, 30)\nx = np.linspace(-10.0, 10.0, 40)\nX, Y = np.meshgrid(x, y)\ndata = 2*np.cos(2*X**2/Y) - np.sin(Y**X)\n\ncs = ax.contourf(X, Y, data, 3,\n                 hatches=['//','+','x','o'],\n                 alpha=0.5)\nartists, labels = cs.legend_elements()\n\nplt.legend(handles=artists, labels=labels)\n\nplt.show()\n"
"x = np.array([[1,1,3], [2,2,2]])\nd = {1: 'a', 2:'b', 3:'c'}\nnp.vectorize(d.get)(x)\n&gt;&gt; array([['a', 'a', 'c'],\n   ['b', 'b', 'b']], dtype=object)\n"
"import pandas as pd\nimport numpy as np\n\nnp.random.seed(123)\ndf = pd.DataFrame({'year': np.random.choice([2010, 2011], 1000),\n                   'weekday': np.random.choice(list('abcdefg'), 1000),\n                   'val': np.random.normal(1, 10, 1000)})\n\ndf.groupby('year').weekday.value_counts(normalize=True)\n\nyear  weekday\n2010  d          0.152083\n      f          0.147917\n      g          0.147917\n      c          0.143750\n      e          0.139583\n      b          0.137500\n      a          0.131250\n2011  d          0.182692\n      a          0.163462\n      e          0.153846\n      b          0.148077\n      c          0.128846\n      f          0.111538\n      g          0.111538\nName: weekday, dtype: float64\n"
'a = \'# #DataScience\'\nb = \'kjndjk#jnjkd\'\nc = "# #DataScience #KJSBDKJ kjndjk#jnjkd #jkzcjkh# iusadhuish#"\nregex = \'(\\s+)#(\\S)\'\n\nimport re\nprint re.sub(regex, \'\\\\1\\\\2\', a)\nprint re.sub(regex, \'\\\\1\\\\2\', b)\nprint re.sub(regex, \'\\\\1\\\\2\', c)\n'
'hist(iris$Sepal.Width, col = "peachpuff", border = "black", \n    prob = TRUE, xlab = "Sepal.Width", main = "Iris Data", breaks=5)\nlines(density(iris$Sepal.Width, na.rm = TRUE), lwd = 2, col = "chocolate3")\n\nDENS = density(iris$Sepal.Width, na.rm = TRUE)\nYMax = max(DENS$y)\nhist(iris$Sepal.Width, col = "peachpuff", border = "black", ylim=c(0,YMax),\n    prob = TRUE, xlab = "Sepal.Width", main = "Iris Data", breaks=5)\nlines(DENS, lwd = 2, col = "chocolate3")\n'
'STU-ID QUESTION 1 RESPONSE 1 QUESTION 2 RESPONSE 2\n00001  tutoring?  True       lunch a?   False\n\nSTU-ID QUESTION 1 RESPONSE 1 QUESTION 2 RESPONSE 2\n00004  tutoring?  True        lunch a?  TRUE\n\nSTU-ID QUESTION 1    RESPONSE 1 Tutorer GPA\n00001  improvement?  True       Jim     3.5\n\nSTU-ID QUESTION 1    RESPONSE 1 Tutorer  GPA\n00004  improvement?  yes        Sally    2.8\n\nimport pandas as pd\n\ndf_s1s1 = pd.read_excel(\'survey1.xlsx\', na_values="Missing", sheet_names=\'sheet 1\', usecols=cols)\ndf.head()\ndf_s1s2 = pd.read_excel(\'survey1.xlsx\', na_values="Missing", sheet_names=\'sheet 2\', usecols=cols)\ndf_s1s2.head()\n\ndf_s2s1 = pd.read_excel(\'survey2.xlsx\', na_values="Missing", sheet_names=\'sheet 1\', usecols=cols)\ndf.head()\ndf_s2s2 = pd.read_excel(\'survey2.xlsx\', na_values="Missing", sheet_names=\'sheet 2\', usecols=cols)\ndf_s1s2.head()\n\ndf_survey_1 = pd.concat([df_s1s1, df_s1s2])\ndf_survey_1.head()\n\ndf_survey_2 = pd.concat([df_s2s1, df_s2s2])\ndf_survey_2.head()\n\nmaster_df = pd.merge(df_survey_1, df_survey2, left_on=\'STU_ID\', right_on=\'STU_ID\')\n\nmaster_df = master_df.dropna(axis = 0, how =\'any\')\n'
"df['CellType'] = np.where(df['area'] &lt; 0.002, 'urban','rural')\n"
'plt.show()\nval = d["Y_prediction_test"][0,index]\nval = int(val)\nprint(classes[val])\n'
"df.groupby(by='Status').apply(lambda x: len(x)/len(df))\n\nStatus\nCancelled    0.666667\nProcessed    0.333333\ndtype: float64\n\ncc = df.groupby(by='Color').ID.count()\ndf.groupby(by=['Color', 'Status']).apply(lambda x: len(x)/cc.loc[x.Color.iloc[0]])\n\nColor  Status   \nBlue   Cancelled    0.666667\n       Processed    0.333333\nGreen  Cancelled    0.666667\n       Processed    0.333333\nRed    Processed    1.000000\ndtype: float64\n"
"print( type(train_images) ) \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfilename = 'image.jpg'\n\nimport matplotlib.pyplot\n\nimg = matplotlib.pyplot.imread(filename)\n\nprint(type(img), img.shape)\nplt.imshow(img)\nplt.show()\n\nimport cv2\n\nimg = cv2.imread(filename)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nprint(type(img), img.shape)\nplt.imshow(img)\nplt.show()\n\nimport imageio\n\nimg = np.array(imageio.imread(filename))\nimg = np.array(img)\n\nprint(type(img), img.shape)\nplt.imshow(img)\nplt.show()\n\nimport PIL.Image\n\nimg = PIL.Image.open(filename)\nimg = np.array(img)\n\nprint(type(img), img.shape)\nplt.imshow(img)\nplt.show()\n\nimport pygame\n\nimg = pygame.image.load(filename)\nimg = pygame.surfarray.array3d(img)\nimg = img.swapaxes(0, 1)\n\nprint(type(img), img.shape)\nplt.imshow(img)\nplt.show()\n\nimport skimage\n\nimg = skimage.io.imread(filename)\n\nprint(type(img), img.shape)\n\nplt.imshow(img)\nplt.show()\n\nimport scipy.misc\n\nimg = scipy.misc.imread(filename)\n\nprint('scipy:', type(img), img.shape)\n\nplt.imshow(img)\nplt.show()\n\nDeprecationWarning: `imread` is deprecated!\n`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\nUse ``imageio.imread`` instead.\n\nfrom keras.preprocessing.image import load_img\n\nimg = load_img(filename)\nimg = np.array(img)\n\nprint('keras:', type(img), img.shape)\n\nplt.imshow(img)\nplt.show()\n"
"import numpy as np \ndf['countries']=np.where(df['countries'].isin(developed),'developed','developing')\nprint(df)\n\n    countries  age  gender\n1  developing   21    Male\n2  developing   22  Female\n3   developed   23    Male\n4   developed   25    Male\n\nc=df['countries'].isin(developed)\ndf.loc[c,'countries']='developed'\ndf.loc[~c,'countries']='developing'\nprint(df)\n\n\n   countries  age  gender\n1  developing   21    Male\n2  developing   22  Female\n3   developed   23    Male\n4   developed   25    Male\n"
'import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nA = pd.DataFrame(columns=["Date", "Headlines"], data=[["01/03/2018","Cricket"],["01/03/2018","Football"],\n                                                    ["02/03/2018","Football"],["01/03/2018","Football"],\n                                                    ["02/03/2018","Cricket"],["02/03/2018","Cricket"]] )\n\nprint (A)\n\n       Date Headlines\n0   01/03/2018  Cricket\n1   01/03/2018  Football\n2   02/03/2018  Football\n3   01/03/2018  Football\n4   02/03/2018  Cricket\n5   02/03/2018  Cricket\n\ndata = A.groupby(["Date","Headlines"]).size()\nprint(data)\n\nDate        Headlines\n01/03/2018  Cricket      1\n            Football     2\n02/03/2018  Cricket      2\n            Football     1\ndtype: int64\n\n# set width of bar\nbarWidth = 0.25\n\n# set height of bar\nbars1 = data.loc[(data.index.get_level_values(\'Headlines\') =="Cricket")].values\nbars2 = data.loc[(data.index.get_level_values(\'Headlines\') =="Football")].values\n\n\n# Set position of bar on X axis\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\n\n# Make the plot\nplt.bar(r1, bars1, color=\'#7f6d5f\', width=barWidth, edgecolor=\'white\', label=\'Cricket\')\nplt.bar(r2, bars2, color=\'#557f2d\', width=barWidth, edgecolor=\'white\', label=\'Football\')\n\n# Add xticks on the middle of the group bars\nplt.xlabel(\'group\', fontweight=\'bold\')\nplt.xticks([r + barWidth for r in range(len(bars1))], data.index.get_level_values(\'Date\').unique())\n\n# Create legend &amp; Show graphic\nplt.legend()\nplt.xlabel("Date")\nplt.ylabel("Count")\nplt.show()\n'
'df["diff"]= (df["first_datetime"]-df["second_datetime"]).dt.seconds.clip(lower=-5000)\n\n(pd.Timestamp("2019-10-13 00:00:50")-pd.Timestamp("2019-10-10 00:00:00")).seconds                                    \nOut: 50\n\n(pd.Timestamp("2019-10-13 00:00:50")-pd.Timestamp("2019-10-10 00:00:00")).total_seconds()                            \nOut: 259250.0\n'
"df.A = pd.to_datetime(df.A)\ndf.B = pd.to_datetime(df.B)\ns = df.set_index(pd.IntervalIndex.from_arrays(df.A, df.B, closed='both'))['C']\n\ndf2['C'] = pd.to_datetime(df2['D']).map(s)\n\n                     D    C\n0  2019-03-13 08:12:20   C1\n1  2019-03-13 08:12:23   C1\n2  2019-03-13 08:12:24   C1\n3  2019-03-13 08:12:25   C1\n4  2019-03-15 10:02:18   C2\n5  2019-03-15 10:02:19   C2\n6  2019-03-16 10:02:20  NaN\n"
'df3 = df1[[col for col in df1.columns if col not in df2.columns]]\n\ndf3 = df1.drop(df2.columns, axis=1)\n'
'expW = expW + w[possition][int(char)]\n'
"df=df.rename_axis(index='Date')\n\ndf=df.reset_index()\n\ndf['Date']\n\ndf.index\n"
"in_common = pd.merge(df1, df2, on=['Email'], how='inner')\n\n"
'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom  matplotlib.ticker import MaxNLocator #imported this to set the tick locators correct\nimport seaborn as sns\n%matplotlib inline\n\ndic = {\'Year\': {0: 2004, 1: 2009, 2: 2014, 3: 2019}, \'BJP\': {0: 40.67, 1: 35.2, 2: 46.63, 3: 56.56}, \'INC\': {0: 54.81, 1: 57.11, 2: 15.22, 3: 22.63}, \'AAP\': {0: 0.0, 1: 0.0, 2: 33.08, 3: 18.2}, \'Total_Polling\': {0: 47.09, 1: 51.81, 2: 65.1, 3: 60.59}} \nparl_data = pd.DataFrame(dic)\n\nfig, ax = plt.subplots(figsize = (15,6)) # assigned fig and ax in one line and added size to figure for better visibility\nplt.plot(parl_data[\'BJP\'], marker=\'o\', label=\'BJP\', color=\'red\')\nplt.plot(parl_data[\'INC\'], marker=\'o\', label=\'INC\', color=\'blue\')\nplt.plot(parl_data[\'AAP\'], marker=\'o\', label=\'AAP\', color=\'brown\')\nplt.plot(parl_data[\'Total_Polling\'], marker=\'o\', label=\'Total Polling\', color=\'green\', linestyle=\'dashed\')\nplt.legend()\n\nfor i,j in parl_data.BJP.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\nfor i,j in parl_data.INC.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\nfor i,j in parl_data.AAP.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\nfor i,j in parl_data.Total_Polling.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\n\n\nax.set_alpha(0.8)\nax.set_title("\\n\\nParty-wise vote share in Lok Sabha Polls (Delhi) 2019\\n", fontsize=15)\nax.set_ylabel("Parliament Polling Percentage\\n", fontsize=15);\nax.set_xlabel("Election Year\\n", fontsize=15)\nplt.yticks(np.arange(0,100,10))\n\n##I Added from here\nax.xaxis.set_major_locator(MaxNLocator(4)) # To set the locators correct\nax.set_xticklabels([\'0\',\'2004\',\'2009\',\'2014\',\'2019\']) # To set the labels correct.\n\nplt.tick_params( left = False,bottom = False, labelsize= 13) #removed tick lines\nplt.legend(frameon = False, loc = "best", ncol=4, fontsize = 14) # removed the frame around the legend and spread them along a line\nplt.box(False) # Removed the box - 4 lines - you may keep if you like. Comment this line out\nplt.style.use(\'bmh\') #used the style bmh for better look n feel (my view) you may remove or keep as you like\n\n\nplt.show();\n\nwidth = .35\nind = np.arange(len(parl_data))\n\nfig, ax = plt.subplots(figsize = (15,6)) # assigned fig and ax in one line and added size to figure for better visibility\n\n#plot bars for bjp,inc &amp; aap\nbjp = plt.bar(ind,parl_data[\'BJP\'],width/2, label=\'BJP\', color=\'red\')\ninc = plt.bar(ind+width/2,parl_data[\'INC\'],width/2, label=\'INC\', color=\'green\')\naap = plt.bar(ind+width,parl_data[\'AAP\'],width/2, label=\'AAP\', color=\'orange\')\n\n#Make a line plot for Total_Polling\nplt.plot(parl_data[\'Total_Polling\'],\'bo-\',label=\'Total Polling\', color = \'darkred\')\n\ndef anotate_bars(bars):\n    \'\'\'\n    This function helps annotate the bars with data labels in the desired position.\n    \'\'\'\n    for bar in bars:\n        h = bar.get_height()\n        ax.text(bar.get_x()+bar.get_width()/2., 0.75*h, h,ha=\'center\', va=\'bottom\', color = \'white\', fontweight=\'bold\')\n\nanotate_bars(bjp)\nanotate_bars(inc)\nanotate_bars(aap)\n\nfor x,y in parl_data[\'Total_Polling\'].items():\n    ax.text(x, y+4, y,ha=\'center\', va=\'bottom\', color = \'black\', fontweight=\'bold\')\n\n\nax.set_title("\\n\\nParty-wise vote share in Lok Sabha Polls (Delhi) 2019\\n", fontsize=15)\nax.set_ylabel("Parliament Polling Percentage\\n", fontsize=15);\nax.set_xlabel("Election Year\\n", fontsize=15)\nplt.yticks(np.arange(0,100,10))\n\nax.xaxis.set_major_locator(MaxNLocator(4)) # To set the locators correct\nax.set_xticklabels([\'0\',\'2004\',\'2009\',\'2014\',\'2019\']) # To set the labels correct.\n\nplt.tick_params( left = False,bottom = False, labelsize= 13) #removed tick lines\nplt.legend(frameon = False, loc = "best", ncol=4, fontsize = 14) # removed the frame around the legend and spread them along a line\nplt.style.use(\'bmh\') #used the style bmh for better look n feel (my view) you may remove or keep as you like\n\n\nplt.show();\n'
"In [10]: df.resample('1D').quantile([0.025, 0.975]).unstack()\nOut[10]:\n           Random_Number\n                   0.025   0.975\n2018-10-09         5.600  91.700\n2018-10-10        12.575  94.425\n2018-10-11         5.575  92.400\n2018-10-12         9.875  97.425\n2018-10-13         2.725  87.550\n2018-10-14        10.200  96.425\n2018-10-15        10.725  96.425\n...\n"
'def main():\n    the stuff you were doing a module level\n\nif __name__ == "__main__":\n    main()\n'
'from bs4 import BeautifulSoup\nfrom selenium import webdriver\n\ngovts_url = r\'https://knesset.gov.il/govt/eng/GovtByNumber_eng.asp\'\nexe_path = r\'C:\\Users\\JRV\\Desktop\\WebCrawling/chromedriver.exe\'\n\nbrowser = webdriver.Chrome(exe_path)\nbrowser.get(govts_url)\npage = browser.page_source\nbrowser.close()\n\nsoup = BeautifulSoup(page, \'html.parser\')\nprint(f"HTML: \\n {soup}")\n'
"data = [\n    {\n        'material_number': 100,\n        'stock': 10,\n        'usage': 2\n    },\n    {\n        'material_number': 101,\n        'stock': 10,\n        'usage': 2\n    },\n    {\n        'material_number': 102,\n        'stock': 10,\n        'usage': 2\n    },\n    {\n        'material_number': 101,\n        'stock': 5,\n        'usage': 2\n    },\n]\n\nparams = 20\n\nupdate = []\nfor i in range(len(data)):\n    update_dict = {}\n    flag = None\n    for j in range(len(update)):\n        if update[j]['material_number'] == data[i]['material_number']:\n            flag = j\n    if flag != None:\n        update_dict['material_number'] = data[i]['material_number']\n        update_dict['stock'] = data[i]['stock'] + update[flag]['stock']\n        update_dict['usage'] = data[i]['usage']\n    else:\n        update_dict['material_number'] = data[i]['material_number']\n        update_dict['stock'] = data[i]['stock'] + (-params)\n        update_dict['usage'] = data[i]['usage']\n\n    update.append(update_dict)\nprint(update)\n"
"import dataframe as df\ndf = pd.DataFrame({'x': your_x_numpy_array, 'y': your_y_numpy_array})\ndf.sort_values(by=[your_index_of_choice],inplace = True)\n"
"import io\nimport base64\nfrom zipfile import ZipFile\n\n\n@app.callback(Output('output_uploaded', 'children'),\n              [Input('upload_prediction', 'contents')],\n              [State('upload_prediction', 'filename'),\n               State('upload_prediction', 'last_modified')])\ndef update_output(list_of_contents, list_of_names, list_of_dates):\n    for content, name, date in zip(list_of_contents, list_of_names, list_of_dates):\n        # the content needs to be split. It contains the type and the real content\n        content_type, content_string = content.split(',')\n        # Decode the base64 string\n        content_decoded = base64.b64decode(content_string)\n        # Use BytesIO to handle the decoded content\n        zip_str = io.BytesIO(content_decoded)\n        # Now you can use ZipFile to take the BytesIO output\n        zip_obj = ZipFile(zip_str, 'r')\n\n        # you can do what you wanna do with the zip object here\n"
'                       ID contactName   createdAt     debit defaultCompany  \\\n0  51%1574233975114-WEBAD              1574233975  118268.2                  \n1  51%1576650784631-WEBAD              1576650784   63860.0                  \n2  ABB1574833257715-WEBAD              1574833257   35065.0                  \n\n  emailAddress lastUpdatedAt phoneNumber  taskNumber  \\\n0                                                  0   \n1                                                  0   \n2                                                  0   \n\n                          path  \n0  /51% FIFTY ONE PERCENT(PWD)  \n1             /51% STORE (MUZ)  \n2        /ABBOTT S/STORE (ABD)  \n'
'import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\ndef cv_f1(model, X, y):\n  score = np.mean(cross_val_score(model, X, y,\n                                scoring="f1",\n                                cv=kfolds))\n  return (score)\n\n\nmodel = ....\n\nscore_f1 = cv_f1(model, X_train, y_train)\n'
"%%timeit -n 1000\ndf[df['Cost'] &gt; 3]['Name']\n397 µs ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n%%timeit -n 1000\ndf['Name'][df['Cost'] &gt; 3]\n306 µs ± 54.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n%%timeit -n 1000\ndf.loc[df['Cost'] &gt; 3, 'Name']\n235 µs ± 19.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
'for result in three_closest(city, cities):\n    print(result + " : " + str(cities[result]))\n\nWarszawa Bielany : (52.283333, 20.966667)\nWarszawa : (52.280957, 20.961348)\nWarszawa-Okęcie : (52.16039, 20.961674)\n'
'&lt;a name="Download"&gt;\n\nlinks=[x[\'href\'] for x in soup.select(\'a[href]\')]\n'
"import pandas as pd\nfrom scipy.stats import spearmanr\n\n\ndata = pd.DataFrame([[2, 3, 5, 10, 76],\n                     [20, 6, 10, 100, 87],\n                     [40, 30, 15, 1000, 46],\n                     [60, 9, 20, 10000, 43],\n                     [80, 12, 25, 100000, 98]])\n\nvector = pd.DataFrame([[60], [80], [100], [120], [140]])\nheading = pd.DataFrame([['red'], ['orange'], ['green'], ['blue'], ['yellow']])\n\nfor i,j in data.iteritems():\n    col=j\n    coef, p = spearmanr(col, vector)\n    alpha = 0.05\n    if p &lt; alpha:\n        print('\\n\\nSpearmans correlation coefficient: %.3f' % coef)\n        print(f' {heading.loc[i,0]} method are correlated (reject H0) p=%.3f' % p)\n\nSpearmans correlation coefficient: 1.000\n red method are correlated (reject H0) p=0.000\n\n\nSpearmans correlation coefficient: 1.000\n green method are correlated (reject H0) p=0.000\n\n\nSpearmans correlation coefficient: 1.000\n blue method are correlated (reject H0) p=0.000\n"
"(df.groupby(['Order_date', 'Product_nr'])\n   ['Quantity'].sum()\n   .unstack('Product_nr')\n   .plot(subplots=True, layout=(1,3)) # change layout to fit your data\n)\n"
'def load_img(path_to_img):\n    max_dim = 512\n    img = tf.io.read_file(path_to_img)\n    img = tf.image.decode_image(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim / long_dim\n\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]\n    return img\n\n    long_dim = max(shape)\n\n    long_dim = tf.reduce_max(shape)\n'
"text = line.mark_text(align='right', dx=-10, dy=-10).encode(\n        text=alt.condition(nearest, 'label:N', alt.value(' '))\n    ).transform_calculate(label='&quot;INV: &quot; + datum.Revenue')\n"
"import matplotlib\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(18, 16), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(range(29), Y, 'o-')\nax = plt.gca()\nfmtr = matplotlib.ticker.IndexFormatter(X)\nax.xaxis.set_major_formatter(fmtr)\n\n#Outputs: \n"
'threshold=3\nmean_1 = np.mean(column_data_array)\nstd_1 =np.std(column_data_array)\n\n\nfor y in data_1:\n    z_score= (y - mean_1)/std_1 \n    if np.abs(z_score) &gt; threshold:\n        outliers.append(y)\nreturn outliers\n'
"#To fill the NaN's with last value of group\ndf['lname'] = df.groupby('fname', as_index=False)['lname'].ffill()\n\n#To fill the NaN's with first value of group\ndf['lname'] = (df['lname'].fillna(df.groupby('fname')['lname']\n                                    .transform('first')))\n"
"s = pd.crosstab(df.morning,df.evening,normalize='index').stack()\nOut[84]: \nmorning  evening\nhigh     high       0.250000\n         low        0.250000\n         medium     0.500000\nlow      high       0.000000\n         low        0.000000\n         medium     1.000000\nmedium   high       0.333333\n         low        0.666667\n         medium     0.000000\ndtype: float64\n"
'df.groupby(&quot;ID&quot;).head(3)\n\ndf[&quot;count&quot;] = df.groupby(&quot;ID&quot;)[&quot;Value&quot;].cumcount()\n\nprint (df.loc[(df[&quot;ID&quot;].ne(0))|((df[&quot;ID&quot;].eq(0)&amp;(df[&quot;count&quot;]&lt;3)))])\n\n    ID  Value  count\n64   0      6      0\n77   0      6      1\n83   0      0      2\n44   1      7      0\n58   1      5      1\n40   1      2      2\n35   1      7      3\n89   1      9      4\n19   1      7      5\n10   1      3      6\n45   2      4      0\n68   2      1      1\n74   2      4      2\n75   2      8      3\n34   2      4      4\n60   2      6      5\n78   2      0      6\n31   2      8      7\n97   2      9      8\n2    2      6      9\n93   2      8     10\n13   2      2     11\n...\n'
'       CUM_MAX(NumOfItems) by Customer  MaxPerID(NumOfItems)\nindex\n0                                  3.0                     3\n1                                  3.0                     3\n2                                  4.0                     4\n3                                  5.0                     5\n4                                  5.0                     5\n5                                  6.0                     6\n6                                 10.0                    10\n7                                 10.0                    10\n8                                 14.0                    14\n'
"df3 = pd.concat([df1, df2]).sort_values('data_a')\n"
"df['role'] = df['device'].str[0]\n"
'def get_neighbors(xs, sample, k=5):\n    neighbors = [(x, np.sum(np.abs(x - sample))) for x in xs]\n    neighbors = sorted(neighbors, key=lambda x: x[1])\n    return np.array([x for x, _ in neighbors[:k]])\n\n_, ax = plt.subplots(nrows=1, ncols=4, figsize=(15, 5))\nfor i in range(4):\n    sample = X_test[i]\n    neighbors = get_neighbors(X_train, sample, k=5)\n    ax[i].scatter(X_train[:, 0], X_train[:, 1], c=&quot;skyblue&quot;)\n    ax[i].scatter(neighbors[:, 0], neighbors[:, 1], edgecolor=&quot;green&quot;)\n    ax[i].scatter(sample[0], sample[1], marker=&quot;+&quot;, c=&quot;red&quot;, s=100)\n    ax[i].set(xlim=(-2, 2), ylim=(-2, 2))\n\nplt.tight_layout()\n'
"f, ax = plt.subplots()\nsns.boxplot(x=&quot;DISCIPLINA&quot;, y=&quot;NOTA&quot;, hue=&quot;TP_ESCOLA&quot;, data=publica_privada_pivot, ax=ax)\nax.set_xticklabels([...]) # list of strings\n\nxticklabels = [t.get_text() for t in ax.get_xticklabels()]\nxticklabels = [t.replace('NU_', '').replace('_', ' ').title()\nax.set_xticklabels(xticklabels)\n"
"model.compile(loss='categorical_crossentropy',metrics=\n   ['accuracy'],optimizer=tf.keras.optimizers.RMSprop(lr=0.001))\n"
'&gt;&gt; mlb = MultiLabelBinarizerWithDuplicates()\n&gt;&gt; transformed = mlb.fit_transform([(1, 1, 3), (1, 2, 2, 4)])\narray([[1,1,0,0,1,0],\n       [1,0,1,1,0,1]])\n&gt;&gt; mlb.classes_\n[1,1,2,2,3,4]\n'
'"""Fit the model using X as training data and y as target values\n        Parameters\n        ----------\n        X : {array-like, sparse matrix, BallTree, KDTree}\n            Training data. If array or matrix, shape [n_samples, n_features],\n            or [n_samples, n_samples] if metric=\'precomputed\'.\n        y : {array-like, sparse matrix}\n            Target values, array of float values, shape = [n_samples]\n             or [n_samples, n_outputs]\n        """\n\nX is (878049, 2) -&gt; n_samples  = 878049 and n_features = 2\ny is (884262,)  -&gt; Here, n_samples = 884262\n\nmy_knn_for_cs4661.fit(X, y[:878049])\n'
"mdf1 = mdf.ffill().fillna(0)\n#same as\n#mdf1 = mdf.fillna(method='ffill').fillna(0)\n\nprint (mdf)\n   Id_Student  English  History  Mathmatic\n0           1     66.0      NaN        NaN\n1           2      NaN     66.0        NaN\n2           3      NaN      NaN        NaN\n3           4     55.0     94.0       94.0\n4           5      NaN     10.0        NaN\n5           6      NaN      NaN       20.0\n\nprint (mdf.ffill())\n   Id_Student  English  History  Mathmatic\n0           1     66.0      NaN        NaN\n1           2     66.0     66.0        NaN\n2           3     66.0     66.0        NaN\n3           4     55.0     94.0       94.0\n4           5     55.0     10.0       94.0\n5           6     55.0     10.0       20.0\n\nprint (mdf.bfill())\n   Id_Student  English  History  Mathmatic\n0           1     66.0     66.0       94.0\n1           2     55.0     66.0       94.0\n2           3     55.0     94.0       94.0\n3           4     55.0     94.0       94.0\n4           5      NaN     10.0       20.0\n5           6      NaN      NaN       20.0\n\nmdf1 = mdf.ffill().fillna(0)\nprint (mdf1)\n   Id_Student  English  History  Mathmatic\n0           1     66.0      0.0        0.0\n1           2     66.0     66.0        0.0\n2           3     66.0     66.0        0.0\n3           4     55.0     94.0       94.0\n4           5     55.0     10.0       94.0\n5           6     55.0     10.0       20.0\n\n\nmdf1 = mdf.bfill().fillna(0)\nprint (mdf1)\n   Id_Student  English  History  Mathmatic\n0           1     66.0     66.0       94.0\n1           2     55.0     66.0       94.0\n2           3     55.0     94.0       94.0\n3           4     55.0     94.0       94.0\n4           5      0.0     10.0       20.0\n5           6      0.0      0.0       20.0\n\nmdf1 = mdf.ffill().bfill()\nprint (mdf1)\n   Id_Student  English  History  Mathmatic\n0           1     66.0     66.0       94.0\n1           2     66.0     66.0       94.0\n2           3     66.0     66.0       94.0\n3           4     55.0     94.0       94.0\n4           5     55.0     10.0       94.0\n5           6     55.0     10.0       20.0\n\n\nmdf1 = mdf.bfill().ffill()\nprint (mdf1)\n   Id_Student  English  History  Mathmatic\n0           1     66.0     66.0       94.0\n1           2     55.0     66.0       94.0\n2           3     55.0     94.0       94.0\n3           4     55.0     94.0       94.0\n4           5     55.0     10.0       20.0\n5           6     55.0     10.0       20.0\n"
'&gt;&gt;&gt; x = [0,2,0,5,0,6,77,8,9]\n&gt;&gt;&gt; list(filter((0).__ne__, x))\n[2, 5, 6, 77, 8, 9]\n'
'import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ntide = np.asarray([-1.2,np.nan,np.nan,3.4,np.nan,np.nan,-1.6,np.nan,np.nan,3.7,np.nan,np.nan,-1.4,])\ntide_time = np.arange(len(tide))\ndf = pd.DataFrame({\'a\':tide_time,\'b\':tide}) \n\n#define your fit function with amplitude, frequence, phase and offset\ndef fit_func(x, ampl, freq, phase, offset):\n    return ampl * np.sin(freq * x + phase) + offset\n\n#extract rows that contain your values\ndf_nona = df.dropna()\n\n#perform the least square fit, get the coefficients for your fitted data\ncoeff, _mat = curve_fit(fit_func, df_nona["a"], df_nona["b"])\nprint(coeff)\n\n#append a column with fit data\ndf["fitted_b"] = fit_func(df["a"], *coeff)\n\n#amplitude    frequency   phase       offset\n[ 2.63098177  1.12805625 -2.17037976  1.0127173 ]\n\n     a    b  fitted_b\n0    0 -1.2 -1.159344\n1    1  NaN -1.259341\n2    2  NaN  1.238002\n3    3  3.4  3.477807\n4    4  NaN  2.899605\n5    5  NaN  0.164376\n6    6 -1.6 -1.601058\n7    7  NaN -0.378513\n8    8  NaN  2.434439\n9    9  3.7  3.622127\n10  10  NaN  1.826826\n11  11  NaN -0.899136\n12  12 -1.4 -1.439532\n'
"import re\nprint(re.findall(r'&lt;coref.*?&gt;(.*?)&lt;/coref&gt;', text, re.S))\n"
'a = [2,3,4]\nb = [5,2,1]\nc = set(b[1:])\n\nsums = set([ aa + bb for aa in a for bb in b ])\nsubs1 = [ (aa - bb) for aa in a for bb in b if (aa - bb) &gt; 0]\nsubs2 = [ bb - aa for bb in b for aa in a if (bb - aa) &gt; 0]\nsubs = set(subs1 + subs2)\n\nprint(sums)\nprint(subs)\nprint(c)\n\nprint(len(sums &amp; c)) #=&gt; 0 c does not contains any of sums\nprint(len(subs &amp; c)) #=&gt; 2 c contains two of subs\n'
'prediction = model.predict(test_X)\nprobs = prediction.max(1)\n'
'#Create a sample DataFrame\ndf=pd.DataFrame([["1/1/2016 12:00:20 AM", 1],\n                 ["1/2/2016 5:03:20 AM", 2],\n                 ["1/2/2016 5:06:20 AM", 3],\n                 ["1/2/2016 5:07:20 AM", 4],\n                 ["1/2/2016 6:06:20 AM", 5],\n                 ["1/3/2016 00:00:20 AM", 6]]\n        ,columns=[\'date\',\'event_id\'])\n\n#We convert the date column into datetime and set as the index\ndf[\'date\'] = pd.to_datetime(df[\'date\'])\ndf.index = df.date\ndel df[\'date\']\n\n\n#This is where the magic happens.    \ndf.resample(\'6H\', label = \'right\').count()\n\n           date     \n2016-01-01 06:00:00     1\n2016-01-01 12:00:00     0\n2016-01-01 18:00:00     0\n2016-01-02 00:00:00     0\n2016-01-02 06:00:00     3\n2016-01-02 12:00:00     1\n2016-01-02 18:00:00     0\n2016-01-03 00:00:00     0\n2016-01-03 06:00:00     1\n'
'df_list = [beds_mntd_bystates, beds_mntd_mod, beds_mntd_gov, beds_mntd_insure]\n\nframe_df = pandas.concat(df_list)\n'
"dt.strftime('%Y-%m-%d %H:%M:%S')\n"
"&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; k['Tstamp'] = pd.to_datetime( k['Tstamp'] ) \n\n&gt;&gt;&gt; duration = ( k.groupby('Sid')['Tstamp'].max() \n               - k.groupby('Sid')['Tstamp'].min() )\n\nSid\n1   00:02:50.438000\n2   00:05:59.275000\nName: Tstamp, dtype: timedelta64[ns]\n"
"dfWithSand3  = df.loc[(df.embarked == 'S') &amp; (df.pclass == 3)].copy()\n\ndfWithSand3  = df.loc[(df.embarked == 'S') &amp; (df.pclass == 3)].reset_index().copy()\n"
'def read_csv(birth_data_file):\n    raw_data = open(birth_data_file, "r").read()\n    raw_data = raw_data.split("\\n")\n    string_list = raw_data[1:]\n    final_list = []\n    for data in string_list:\n        int_fields = []\n        string_fields = data.split(",")\n        for string_field in string_fields:\n            field = int(string_field)\n            int_fields.append(field)\n        final_list.append(int_fields)\n    return(final_list)\n\n\ndef calc_counts(data, column):\n    births_counts = {}\n    if not column &gt; 0 and column &lt;= 4:\n        return("\'column\' must be either 1, 2, 3, or 4")\n    else:\n        for instance in data:\n            field = instance[column-1]\n            births  = instance[4]\n            if field in births_counts.keys():\n                births_counts[field] += births\n            else:\n                births_counts[field] = births\n        return(births_counts)\n\n\n# Write a function that extracts the same values across years and calculates the\n# differences between consecutive values to show if number of\n# births is increasing or decreasing.\n\ndef check_birth_growth(birth_data_file):\n    cdc_list = read_csv(birth_data_file)\n    cdc_year_births = calc_counts(cdc_list, 1)\n    previous_year_birth = 0\n    previous_birth_diff = 0\n    for year, total_births in cdc_year_births.items():\n        current_year_birth = int(total_births)\n        if previous_year_birth == 0:\n            growth_status = "Growth of births in {} not available.".format(year)\n            print(growth_status)\n            previous_year_birth = current_year_birth\n        else:\n            if current_year_birth &gt; previous_year_birth:\n                growth_status = "Births increased in {}.".format(year)\n                print(growth_status)\n                previous_year_birth = current_year_birth\n            elif current_year_birth &lt; previous_year_birth:\n                growth_status = "Births decreased in {}.".format(year)\n                print(growth_status)\n                previous_year_birth = current_year_birth\n            elif current_year_birth == previous_year_birth:\n                growth_status = "Births in {} was same as previous year.".format(year)\n                print(growth_status)\n                previous_year_birth = current_year_birth\n'
'#Adding two timestamps is not supported and not logical\n#Probably, you really want to add the time rather than the timestamp itself\n#This is how to extract the time from the timestamp then summing it up\n\nimport datetime\nimport time\n\nt = [\'1995-07-01 00:00:01\',\'1995-07-01 00:00:06\',\'1995-07-01 00:00:09\',\'1995-07-01 00:00:09\',\'1995-07-01 00:00:09\']\ntSum = datetime.timedelta()\ndf = pd.DataFrame(t, columns=[\'timestamp\'])\nfor i in range(len(df)):\n    df[\'timestamp\'][i] = datetime.datetime.strptime(df[\'timestamp\'][i], "%Y-%m-%d %H:%M:%S").time()\n    dt=df[\'timestamp\'][i]\n    (hr, mi, sec) = (dt.hour, dt.minute, dt.second)\n    sum = datetime.timedelta(hours=int(hr), minutes=int(mi),seconds=int(sec))\n    tSum += sum\nif tSum.seconds &gt;= 60*60:\n    print("more than 1 hour")\nelse:\n    print("less than 1 hour")\n'
"import pandas as pd\n\n\ndf = pd.DataFrame({'ColumnName': ['value 1, value 2', 'value 1, value 3']})\n\n\n#          ColumnName\n# 0  value 1, value 2\n# 1  value 1, value 3\n\npd.get_dummies(df.ColumnName.str.split(',', expand=True), prefix='', prefix_sep='')\n\n\n#    value 1   value 2   value 3\n# 0        1         1         0\n# 1        1         0         1\n"
"import json\n\nf = open('cost_drilldown_data.json')\ndata = json.load(f)\n\nbreakdown = data['breakdown']\nstorage = data['storage']\nfilter_list = []            #first column\nfilter_cost_list = []       #second column\nvalue_list = []             #third column\ncost_list = []              #fourth column\nsubValue_list = []          #fifth col\nsubvalueCost_list = []      #sixth col\nresID_list = []             #seventh col\nstorageCost_list = []       #eighth col\ntotalCost_list = []         #ninth col\nmetadata_list = []          #tenth col\n\nfor eachBreakdown in breakdown:\n    filter_list.append(eachBreakdown['filter'])\n    filter_cost_list.append(['cost'])\n    valuesArr = eachBreakdown['values']\n    for eachValues in valuesArr:\n        value_list.append(eachValues['value'])\n        cost_list.append(eachValues['cost'])\n        if 'subvalues' in eachValues:\n            subValueArr = eachValues['subvalues']\n        for eachSubValueArr in subValueArr:\n            subValue_list.append(eachSubValueArr['subvalue'])\n            subvalueCost_list.append(eachSubValueArr['cost'])\n\nfor eachStorage in storage:\n    resID_list.append(eachStorage['resource_id'])\n    storageCost_list.append(eachStorage['cost'])\n    totalCost_list.append(eachStorage['total_cost'])\n    metadata_list.append([eachStorage['metadata']])\n"
'module ArticleConcern\n  require "rubypython"\n  :\n  :\nend\n'
"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=0.33)\n\npipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('clf', LogisticRegression())\n])\n\nparam_grid = [\n    {\n        'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n        'clf__C': np.logspace(-3, 1, 5),\n    },\n]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\ngrid.fit(X_train, y_train)\n"
"df[df.country_code == 'USA'].groupby('state_code').sum().reset_index()\n\n#  state_code  commerce  finance  software\n#0         CA         0        0         1\n"
"def get_title(text, titles, previous_title):\n    for title in titles:\n        if title in text:\n            return title\n    return previous_title\n\nname_script_list = {'TRUMP:':TRUMP_script_list, 'HILLARY:':HILLARY_script_list, 'WALLACE:':WALLACE_script_list}\ntitles = set(name_script_list.keys())\ntitle = ''\n\nfor text in loaded_txt:\n    title = get_title(text, titles, title)\n    name_script_list[title].append(text)\n"
'import requests\nprefixStr = \'&lt;div class="translation-text"&gt;\'\npostfixStr = \'&lt;/div\'\n\nslangText = \'I was wid Ali.\'\n\nr = requests.post(\'https://www.noslang.com/\', {\'action\': \'translate\', \'p\': \nslangText, \'noswear\': \'noswear\', \'submit\': \'Translate\'})\nstartIndex = r.text.find(prefixStr)+len(prefixStr)\nendIndex = startIndex + r.text[startIndex:].find(postfixStr)\nprint(r.text[startIndex:endIndex])\n'
"def str2series(s):\n    pieces = [x.split(': ') for x in s.split(',')]\n    return pd.Series({k.strip(): v.strip() for k,v in pieces})\n\nnew_df = df.SourceTechAttributes.apply(str2series)\n\ndf = df.join(new_df)\n"
'X_test_inversed = pca.inverse_transform(X_test_reduced)\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef show_as_image(sample):\n    bitmap = sample.reshape((13, 8))\n    maxval = np.max(np.abs([bitmap.min(),bitmap.max()]))\n    plt.figure()\n    plt.imshow(bitmap, cmap='RdYlGn', interpolation='nearest',\n               vmin=-maxval, vmax=maxval)\n    plt.colorbar()\n    plt.show()\n\nsample=np.random.randn(1,104)\nshow_as_image(sample)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef show_as_image(sample):\n    bitmap = sample.reshape((13, 8))\n    bitmap[bitmap &gt;= 0] = 1\n    bitmap[bitmap &lt; 0] = 0\n    plt.figure()\n    plt.imshow(bitmap, cmap='RdYlGn', interpolation='nearest',\n               vmin=-.1, vmax=1.1)\n    plt.show()\n\nsample=np.random.randn(1,104)\nshow_as_image(sample)\n"
"df = pd.DataFrame([orgdata], columns=cols)\n\nimport pandas as pd\n\ndata = ['15EC41', 'LIC', '40', '60', 'P']\n\norgdata = ['somestring', data[0], data[1], data[2], data[3], data[4]]\n\ncolnames = ['USN', data[2]]\n\ncols = pd.MultiIndex.from_product([colnames, ['IA', 'EX', 'Total']])\n\ndf = pd.DataFrame([orgdata], columns=cols)\n\n          USN                40          \n           IA      EX Total  IA  EX Total\n0  somestring  15EC41   LIC  40  60     P\n\nimport pandas as pd\n\ndata1 = ['15EC41', 'LIC', '40', '60', 'P']\ndata2 = ['62F793', 'DUH', '52', '85', 'O']\ndata3 = ['9734HJ', 'IAS', '34', '94', 'D']\n\norgdata = [['somestring', i[0], i[1], i[2], i[3], i[4]] for i in [data1, data2, data3]]\n\ncolnames = [data1[0], data1[2]]\n\ncols = pd.MultiIndex.from_product([colnames, ['IA', 'EX', 'Total']])\n\ndf = pd.DataFrame(orgdata, columns=cols)\n\nUSN = [0, 1, 2]\n\ndf.index = USN; df.index.name = 'USN'\n\n         15EC41                40          \n             IA      EX Total  IA  EX Total\nUSN                                        \n0    somestring  15EC41   LIC  40  60     P\n1    somestring  62F793   DUH  52  85     O\n2    somestring  9734HJ   IAS  34  94     D\n"
'plt.ion() \n'
'amazon.drop(amazon[(amazon["Id"] &gt; 150492) &amp; (amazon["Id"] &lt; 150530 )].index, inplace=True)\n\namazon = amazon.drop(amazon[(amazon["Id"] &gt; 150492) &amp; (amazon["Id"] &lt; 150530 )].index)\n'
'kstream.map(lambda k, v: json.loads(v))\n'
"#import pandas for managing data with dataframe\nimport pandas as pd\n#import tabulate to print your data frame as table\nfrom tabulate import tabulate\n#Create a data dictionary\nmyData={'Numbers':[1000,1002,1003,0,0,0,0,0,0],'Date':['12/1/2018','12/2/2018','12/3/2018','12/4/2018','12/5/2018','12/6/2018','12/7/2018','12/8/2018','12/9/2018'],'Mean':[1,0,0.5,0.6,0.4,0.1,-0.7,0.2,-0.1]}\n#Create a data frame from the data dictionary using pandas. User mentioned that the data is already in the\n#pandas data frame\nmyDataFrame=pd.DataFrame(myData)\n#Print your final table (just pretty print)\nprint(tabulate(myDataFrame, headers='keys', tablefmt='psql'))\n#Declare a list\nMultiplicationList=[]\n#Declare a constant\nStorePreviousValue=0\nfor i in range(0,len(myDataFrame['Numbers'])):\n    #If it is the first row then use the Number\n    if i==0:\n        #Append the value to the list\n        MultiplicationList.append(myDataFrame['Numbers'][i])\n    else:\n        #If it is not the first row, and the value in the first column of the previous row is '0'\n        #multiply Mean with the previous multiplication result\n        if myDataFrame['Numbers'][i-1]==0:\n            StorePreviousValue=StorePreviousValue*myDataFrame['Mean'][i]\n        #If it is not the first row, and the value in the first column of the previous row is not '0'\n        #(should probably say greate than '0', but the question is not clear about that), then \n        #multiply Mean with the Number in the first column of the previous row\n        else:\n            StorePreviousValue=myDataFrame['Numbers'][i-1]*myDataFrame['Mean'][i]\n        #Append the value to the list\n        MultiplicationList.append(StorePreviousValue)\n#Create a new column in the data frame and pass the list as the value\nmyDataFrame['Multiplication']=MultiplicationList\n#Print your final table (just pretty print)\nprint(tabulate(myDataFrame, headers='keys', tablefmt='psql'))\n\n+----+-----------+-----------+--------+\n|    |   Numbers | Date      |   Mean |\n|----+-----------+-----------+--------|\n|  0 |      1000 | 12/1/2018 |    1   |\n|  1 |      1002 | 12/2/2018 |    0   |\n|  2 |      1003 | 12/3/2018 |    0.5 |\n|  3 |         0 | 12/4/2018 |    0.6 |\n|  4 |         0 | 12/5/2018 |    0.4 |\n|  5 |         0 | 12/6/2018 |    0.1 |\n|  6 |         0 | 12/7/2018 |   -0.7 |\n|  7 |         0 | 12/8/2018 |    0.2 |\n|  8 |         0 | 12/9/2018 |   -0.1 |\n+----+-----------+-----------+--------+\n+----+-----------+-----------+--------+------------------+\n|    |   Numbers | Date      |   Mean |   Multiplication |\n|----+-----------+-----------+--------+------------------|\n|  0 |      1000 | 12/1/2018 |    1   |      1000        |\n|  1 |      1002 | 12/2/2018 |    0   |         0        |\n|  2 |      1003 | 12/3/2018 |    0.5 |       501        |\n|  3 |         0 | 12/4/2018 |    0.6 |       601.8      |\n|  4 |         0 | 12/5/2018 |    0.4 |       240.72     |\n|  5 |         0 | 12/6/2018 |    0.1 |        24.072    |\n|  6 |         0 | 12/7/2018 |   -0.7 |       -16.8504   |\n|  7 |         0 | 12/8/2018 |    0.2 |        -3.37008  |\n|  8 |         0 | 12/9/2018 |   -0.1 |         0.337008 |\n+----+-----------+-----------+--------+------------------+\n"
"import shutil\nshutil.make_archive(output_filename, 'zip', folder_path)\n"
"ax = sns.distplot(df_0['temp'], bins=20)\nax.grid()\nax.axvline(10)\n\nseaborn.set_style('whitegrid')\nsns.distplot(df_0['temp'], bins=20)\n"
"from datetime import datetime\na='2013-02-12 16:21:48'\nc=datetime.strptime(a,'%Y-%m-%d %H:%M:%S').time()\nprint(c)\n\n16:21:48\n"
'def get_cartesian(rdd): \n    rdd  = rdd.cartesian(rdd).filter(lambda x: x[0] != x[1])\n    return rdd\n'
"       day   a   b\n0  5/11/19   3   1\n1  5/11/19  11   3\n2  5/12/19   5   9\n3  5/13/19  11  14\n\ndef calc(df):\n\n    len_a_under_10 = (df['a'] &lt; 10).sum() / len(df['a'])\n    len_b_under_10 = (df['b'] &lt; 10).sum() / len(df['b'])\n\n    df['a_under_10'] = len_a_under_10\n    df['b_under_10'] = len_b_under_10\n\nreturn df\n\ndf.groupby('day').apply(calc)\n\n       day   a   b  a_under_10  b_under_10\n0  5/11/19   3   1         0.5         1.0\n1  5/11/19  11   3         0.5         1.0\n2  5/12/19   5   9         1.0         1.0\n3  5/13/19  11  14         0.0         0.0\n"
'def expression(r ,possition , char ):\n    return 1-r[possition , int(char)]\n'
"df\n\n&gt;&gt;&gt; PIC Label   EncodedPixels\n    0   pic1    fish    True\n    1   pic1    flower  True\n    2   pic1    gravel  False\n    3   pic1    sugar   False\n    4   pic2    fish    True\n    5   pic2    flower  True\n\n\nhelper_df = df.groupby(['PIC', 'Label']).apply(lambda grp: grp['EncodedPixels'].sum()).unstack()\nhelper_df\n\n\n&gt;&gt;&gt; Label   fish    flower  gravel  sugar\n    PIC             \n    pic1    1.0 1.0 0.0 0.0\n    pic2    1.0 1.0 NaN NaN\n\noutput_df = pd.DataFrame(index = df['Label'].unique(), columns = df['Label'].unique())\n\nfor ind_x in output_df.columns:\n    for ind_y in output_df.columns:\n        output_df.loc[ind_x, ind_y] = helper_df[helper_df[ind_x] &amp; helper_df[ind_y]].sum()[ind_x]\n\noutput_df\n\n    &gt;&gt;&gt; fish    flower  gravel  sugar\n        fish    2   2   0   0\n        flower  2   2   0   0\n        gravel  0   0   0   0\n        sugar   0   0   0   0\n"
"# Divide every line except the last line by the last line\ndensity = (df.iloc[:-1] / df.iloc[-1]).stack().to_frame('density')\n\n                     density\nduration amplitude          \n2        -13.125    1.000000\n3        -13.125    0.004442\n         -11.250    1.000000\n         -11.250    1.000000\n4        -13.125    0.995558\n"
'a=your_df.groupby(["fruit"]).sum()["value"]\n'
"for i in range(0,60):\n    records.append([df.columns[j]+'='+str(df.values[i,j]) for j in range(0,5)])\n"
"from sklearn.preprocessing import OneHotEncoder \n\n# sample data\ndf = pd.DataFrame({'col': [0,1,2,3,0,1,2]})\ncolnames = ['col'] # modify this for your df\n\noneHot = OneHotEncoder()\nx_ohe = oneHot.fit_transform(df[colnames].values.reshape(-1,1))\n\nx_ohe.todense()\n"
'import re\n\nstr = "This is what I\'m trying to do, but I can\'t figure out how."\nres = re.sub(r\'(?&lt;=\\w)(?=[,.!;:])\', \' \', str)\nprint res\n'
"folium.vector_layers.CircleMarker(\n    ['lat','lng'],\n    radius=10,\n    color='red',\n    popup='Eixample',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(venues_map)\n"
'# load a dataset and regression function\nfrom sklearn import linear_model,datasets\nimport pandas as pd\n# I use boston dataset to show you \nfull_data = datasets.load_boston()\n\n# get a regressor, fit intercept\nreg = linear_model.LinearRegression(fit_intercept=True)\n# data is our explanatory, target is our response\nreg.fit(full_data[\'data\'],full_data[\'target\'])\n\n# we have 1 intercept and  11 variables\' coef\nreg.intercept_,reg.coef_\n\n# get the name of features\nfull_data.feature_names\n# append to get a new list\ncoef = np.append(reg.intercept_,reg.coef_)\nfeature_names = np.append([\'Intercept\'], full_data.feature_names)\n# output a dataframe contains coefficients you want\npd.DataFrame({"feature_names":feature_names,"coef":coef})\n\n   feature_names       coef\n0      Intercept  36.459488\n1           CRIM  -0.108011\n2             ZN   0.046420\n3          INDUS   0.020559\n4           CHAS   2.686734\n5            NOX -17.766611\n6             RM   3.809865\n7            AGE   0.000692\n8            DIS  -1.475567\n9            RAD   0.306049\n10           TAX  -0.012335\n11       PTRATIO  -0.952747\n12             B   0.009312\n13         LSTAT  -0.524758\n'
"import numpy as np\n\ndf['new_col'] = np.where(((df['lr'] &gt; 4) &amp; (df['ly'] &gt; 4) &amp; (df['lb'] &gt; 4)), 1, 0)\n"
"df[df.iloc[:,0] == 'Zimbabwe']\n\n#Series\nprint(df['country'])\n1692    Zimbabwe\n1693    Zimbabwe\n1694    Zimbabwe\nName: country, dtype: object\n\n#Series\nprint (df.iloc[:,0])\n1692    Zimbabwe\n1693    Zimbabwe\n1694    Zimbabwe\nName: country, dtype: object\n\n#one column Dataframe\nprint(df.iloc[:,[0]])\n       country\n1692  Zimbabwe\n1693  Zimbabwe\n1694  Zimbabwe\n"
"df['Qty'] = df.groupby(['Description','Week_number']).Qty.transform('sum')\n\ncond = df.query('Holiday ==1').Week_number.unique()\n\ndf['Holiday'] = np.where(df.Week_number.isin(cond),1,df.Holiday)\n\ndf = df.drop_duplicates(['Week_number','Description'])\n\n    Week_number Holiday Description  Qty\n0       38          1       A        11\n2       38          1       B         1\n3       38          1       C         1\n4       40          0       A         1\n"
'out_arq = sys.argv[2]\n\nout_arq = sys.argv[2] if len(sys.argv) &gt;= 3 else None\n\nif out_arq:\n    results_df.to_csv(out_arq,index = False)\n    print(f"File saved as: {out_arq}")\nelse:\n    print("No output file created")\n'
"train['CoapplicantIncome'] = train['CoapplicantIncome'].map(int)\n"
'sorted_list_a = sorted(list_a, key = lambda x: dict_b.get(x, 0))\n'
"import pandas as pd\nimport datetime\n\n#Convert the date column to date format\ndate['date_format'] = pd.to_datetime(date['Maturity_date'])\n\n#Add a month column\ndate['Month'] = date['date_format'].apply(lambda x: x.strftime('%b'))\n"
"state_group=df.groupby(['State'])['Population'].nlargest(3).sum(level=0)\nstate_group_largest3=state_group.nlargest(3)\n\nState\nA    140 # because 140=35+45+60, which are the 3 largest counties in A\nB    185\nC    217\nD    215\nName: Population, dtype: int64\n\nState\nC    217\nD    215\nB    185\nName: Population, dtype: int64\n"
'import re\n\n# returns all numbers\nage = re.findall("[\\d].", your_text)\n\n# returns all words related to gender\ngender = re.findall("female|gentleman|woman", your_text)\n\ngender_dict = {"male": ["gentleman", "man", "male"],\n               "female": ["female", "woman", "girl"]}\ngender_aux = []\nfor g in gender:\n    if g in gender_dict[\'male\']:\n        gender_aux.append(\'male\')\n    elif g in gender_dict[\'female\']:\n        gender_aux.append(\'female\')\n'
"import pandas as pd\n# Dictionary of items\nd = {'words' : [ [ 'cheap', 'expensive'], ['excited'], ['hot', 'summer'], ['money'], ['rain'] ], \n     'category': ['price', 'entertainment', 'weather', 'price', 'weather']}\n# Convert dictionary to dataframe\ndf = pd.DataFrame(d)\n# Unpack the list of 'words' by joining with ','\ndf.words = df.words.str.join(',')\n# Groupby and aggregate to get the unique 'words' for each 'category'\nnew_df = df.groupby('category').agg({'words':'unique'})\n# Since the groupby results in a list of items, unpack by joining with ','\nnew_df.words = new_df.words.str.join(',')\n# reset_index() to convert the groupby object to a dataframe\n# This is optional. If not used, 'category' will the index of the dataframe.\nnew_df.reset_index(inplace=True)\nnew_df\n"
"df.drop(['address', 'col1'], axis=1, inplace = True)\n\ndf = df.drop(['address', 'col1'], axis=1)\n"
'model.fit(x_train, y_train, batch_size = 64, epochs = 10)\n\nmodel.evaluate(x_test, y_test, batch_size = 128)\n'
"df2 = df.Profitability.unstack()\n\ndf2 = q1.unstack()\n\nCategory  Cosmetics  First Aid  Magazine  Supplements  Toiletries\nMonth                                                            \n1         2685.9000   2128.020    703.89     37005.62     1893.06\n2         2569.0600   3282.785    679.11     36647.88     1357.75\n3         1350.7925   2238.310    371.12     21444.09     1226.16\n\nax = df2.plot.bar(rot=0)\nax.get_figure().suptitle(t='Profitability', fontsize=20)\nax.legend(bbox_to_anchor=(1.35, 1.0));\n\nax = df2.plot.bar(rot=0, logy=True)\nax.get_figure().suptitle(t='Profitability', fontsize=20)\nax.legend(bbox_to_anchor=(1.1, 1.0))\nyTicks = [1000, 3000, 10000, 30000]\nyTickLabels = [ f'{i:,}' for i in yTicks ]\nax.set_yticks(yTicks)\nax.set_yticklabels(yTickLabels);\n"
"import pandas as pd\n\ndata = {\n    'Number':[12,55,3,2,88,17],\n    'People':['Zack','Zack','Merry','Merry','Cross','Cross'],\n    'Random':[353,0.5454,0.5454336,32,-7,4]\n}\n\ndf = pd.DataFrame (data, columns = ['Number','People','Random'])\n\nprint(df,'\\n')\n\nres = df[df.groupby(['People'])['Number'].transform(max) == df['Number']].set_index('People')\nprint(res)\n\n        Number    Random\nPeople                  \nZack        55  0.545400\nMerry        3  0.545434\nCross       88 -7.000000\n"
'nation = fifa_19[\'Nationality\'].value_counts()\nnation = nation.loc[nation &gt;= 3]\n\nplt.figure(figsize=(30, 15))\nsns.set(style="whitegrid")\nsns.barplot(data=nation, x="Country", y="Frequency", order = fifa_19[\'Nationality\'].value_counts().index)\nplt.title(\'Distribution of Nationalities of players\')\nplt.xlabel(\'Nationality\') \nplt.ylabel(\'Frequency\')\n\nsns.barplot(data=nation, x="Country", y="Frequency", order = fifa_19[\'Nationality\'].value_counts().index)\n\norder = pd.DataFrame(fifa_19[\'Nationality\'].value_counts().index).join(nation["Country"], how="inner")["Country"]\nsns.barplot(data=nation, x="Country", y="Frequency", order = order)\n'
"def datesplit(data):\n    parts = []\n    for idx, row in data.iterrows():\n        parts.append(pd.DataFrame(row['Employee ID'], columns=['Employee ID'],\n            index=pd.date_range(start=row['Start Date'], end=row['End Date'],\n                name='Date')))\n    return pd.concat(parts).reset_index()\n\nresult = datesplit(plannedleave)\n\n   Employee ID First Name Last Name Leave Type Start Date   End Date\n0         1001       John     Brown       Xxxx 2020-05-10 2020-05-15\n1         1002      Betty     Smith       Yyyy 2020-05-18 2020-05-22\n\n         Date  Employee ID\n0  2020-05-10         1001\n1  2020-05-11         1001\n2  2020-05-12         1001\n3  2020-05-13         1001\n4  2020-05-14         1001\n5  2020-05-15         1001\n6  2020-05-18         1002\n7  2020-05-19         1002\n8  2020-05-20         1002\n9  2020-05-21         1002\n10 2020-05-22         1002\n"
'def GC_content(dnaseq):\n    percent = round(((dnaseq.count("C") + dnaseq.count("G")) / len(dnaseq)) * 100, 3)\n    print(f\'GC content: {percent} %\')\n'
"long_data['Numeric Score'] = long_data['Score'].str.replace('_', '').astype(float).replace(0., np.nan)\n\ndef score_cleaner(underscore):\n    return underscore.str.replace(\n        '_', '').astype(float).replace(0., np.nan)\n\nlong_data['Numeric Score'] = score_cleaner(long_data['Score'])\n\nlong_data['Numeric Score'] = long_data['Score'].str.replace('_000', 'NaN')\n\ndef score_cleaner(underscore):\n    return underscore.str.replace('_000', 'NaN')\n\nlong_data['Numeric Score'] = score_cleaner(long_data['Score'])\n"
"df.set_index([df['ID'], 'Label'], append=True)['ID'].unstack()\\\n  .groupby('ID', as_index=False, group_keys=False)\\\n  .rolling(3, center=True).count()\n\nLabel    A    B\n  ID           \n0 1    2.0  0.0\n1 1    2.0  1.0\n2 1    1.0  2.0\n3 1    0.0  2.0\n4 2    1.0  1.0\n5 2    1.0  1.0\n\ndf.set_index('Label', append=True)['ID'].unstack().rolling(3, center=True).count()\n\nLabel    A    B\n0      2.0  0.0\n1      2.0  1.0\n2      1.0  2.0\n3      1.0  2.0\n4      1.0  2.0\n5      1.0  1.0\n"
'df= df[df[&quot;txn_description&quot;]==&quot;SALARY&quot;]\ndf[&quot;Annual&quot;] = df[&quot;Amount&quot;]*12\n\n   Customer_ID txn_description  Amount  Annual\n1            2          SALARY    2000   24000\n3            4          SALARY    1500   18000\n5            6          SALARY    1800   21600\n\ndic = df.set_index(&quot;Customer_ID&quot;)[&quot;Annual&quot;].to_dict()\n'
"seasonal_decompose(pd.DataFrame(sample).set_index(&quot;EventTime&quot;), model='additive', period=24).plot();\n"
"df = pd.DataFrame({'Gender': ['m', 'm', 'm', 'm', 'f'],\n                   'year': [2011, 2013, 2011, 2011, 2012]})\npd.crosstab(df['year'], df['Gender'])\n\nGender  f   m\nyear        \n2011    0   3\n2012    1   0\n2013    0   1\n\npd.crosstab(df['year'], df['Gender'])[['m', 'f']]\n"
"out = df.groupby(pd.Grouper(key='datetime',freq='3H')).agg(list)\nOut[386]: \n                              ui1      ui2\ndatetime                                  \n2019-01-01 00:00:00   [0.2, 0.12]  [4, 36]\n2019-01-01 03:00:00        [0.63]      [7]\n2019-01-01 06:00:00  [0.31, 0.48]  [40, 9]\n"
"import os\nimport shutil\n\nsrcdir = 'Paintings'\ndstdir = 'train'\n\nfor name, s in df.groupby('artist')['filename']:\n    artistdir = os.path.join(dstdir, name)\n    print(f'copying {s.shape[0]} images from {srcdir} to {artistdir}')\n    os.makedirs(artistdir, exist_ok=True)\n    for filename in s:\n        shutil.copy(os.path.join(srcdir, name), os.path.join(artistdir, name))\n\ncopying 1 images from Paintings to train/Hieronymus Bosch\ncopying 3 images from Paintings to train/Hiroshige\ncopying 1 images from Paintings to train/Ivan Aivazovsky\n...\n"
'import math\nimport numpy as np\n\ncoordinates = [(0,1), (3,4), (-5,12), (2,2)]\ndef distance(x1, y1, x2, y2):\n     return math.sqrt(square(x2-x1) + square(y2-y1))\ndef square(value):\n    return value * value\ndistances = list(map(lambda x: distance(0,0,x[0],x[1]), coordinates))\nprint(np.mean(distances))\n5.457106781186548\n'
'FILENAME = &quot;your filename&quot;\nf = open(FILENAME)\nlst = []\nlines = f.read().splitlines()\nfor i in range(0,len(lines),3):\n    lst.append((lines[i],lines[i+1]))\n'
"df['Survived'].value_counts()\ndf['Sex'].value_counts()\n"
"import numpy as np\nfrom matplotlib import pyplot as plt\n\n\ndata1 = [np.random.normal(0,0.1, 10), np.random.normal(0,0.1,10)]\ndata2 = [np.random.normal(1,0.2, 10), np.random.normal(2,0.3,10)]\ndata3 = [np.random.normal(-2,0.1, 10), np.random.normal(1,0.5,10)]\n\n\nplt.scatter(data1[0],data1[1])\nplt.scatter(data2[0],data2[1])\nplt.scatter(data3[0],data3[1])\n\nplt.show()\n\nimport numpy as np\nfrom matplotlib import pyplot as plt    \n\nwith open(r'mydata.txt') as f:\n    emp= []\n    for line in f:\n        line = line.split() \n        if line:            \n            line = [int(i) for i in line]\n            emp.append(line)\n\n\nfrom sklearn.decomposition import PCA\nimport pylab as pl\nfrom itertools import cycle\nX = emp\npca = PCA(n_components=3, whiten=True).fit(X)\nX_pca = pca.transform(X) #regular PCA\n\njobs = ['A', 'B', 'C']\njob_id = np.array([e[4] for e in emp])\n\nfig, axes = plt.subplots(3,3, figsize=(5,5))\n\nfor row in range(axes.shape[0]):\n    for col in range(axes.shape[1]):\n        ax = axes[row,col]\n        if row == col:\n            ax.tick_params(\n                axis='both',which='both',\n                bottom='off',top='off',\n                labelbottom='off',\n                left='off',right='off',\n                labelleft='off'\n            )\n            ax.text(0.5,0.5,jobs[row],horizontalalignment='center')\n        else:\n            ax.scatter(X_pca[:,row][job_id==0],X_pca[:,col][job_id==0],c='r')\n            ax.scatter(X_pca[:,row][job_id==1],X_pca[:,col][job_id==1],c='g')\n            ax.scatter(X_pca[:,row][job_id==2],X_pca[:,col][job_id==2],c='b')\nfig.tight_layout()\nplt.show()\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nwith open(r'mydata.txt') as f:\n    emp= []\n    for line in f:\n        line = line.split() \n        if line:            \n            line = [int(i) for i in line]\n            emp.append(line)\n\n\nfrom sklearn.decomposition import PCA\nimport pylab as pl\nfrom itertools import cycle\nX = emp\npca = PCA(n_components=3, whiten=True).fit(X)\nX_pca = pca.transform(X) #regular PCA\n\njobs = ['A', 'B', 'C']\njob_id = np.array([e[4] for e in emp])\n\nrow = 0\ncol = 1\n\nplt.scatter(X_pca[:,row][job_id==0],X_pca[:,col][job_id==0],c='r')\nplt.scatter(X_pca[:,row][job_id==1],X_pca[:,col][job_id==1],c='g')\nplt.scatter(X_pca[:,row][job_id==2],X_pca[:,col][job_id==2],c='b')\n\nplt.show()\n"
"df['Quantity'] = pd.to_numeric(df.Quantity.str.replace('Â',''))\n\n&gt;&gt;&gt; df\n  Quantity  data\n0    0.1 Â     1\n1    0.2 Â     2\n\ndf['Quantity'] = pd.to_numeric(df.Quantity.str.replace('Â',''))\n\n&gt;&gt;&gt; df\n   Quantity  data\n0       0.1     1\n1       0.2     2\n\ndf['Quantity'] = pd.to_numeric(df['Quantity'].str.split().str[0])\n"
"import pandas as pd\ndf = pd.DataFrame({'vals': [[1,2,3,4], [2,3], [1,2,3], [2,3],\n                            [1,2,3], [1,2,3,4], [1], [2], [2,2], [2,1,3]]})\n\ndf.groupby(df.vals.apply(tuple)).groups\n#{(1,): Int64Index([6], dtype='int64'),\n# (1, 2, 3): Int64Index([2, 4], dtype='int64'),\n# (1, 2, 3, 4): Int64Index([0, 5], dtype='int64'),\n# (2,): Int64Index([7], dtype='int64'),\n# (2, 1, 3): Int64Index([9], dtype='int64'),\n# (2, 2): Int64Index([8], dtype='int64'),\n# (2, 3): Int64Index([1, 3], dtype='int64')}\n\ndf.reset_index().groupby(df.vals.apply(tuple))['index'].apply(list).sort_values().tolist()\n#[[0, 5], [1, 3], [2, 4], [6], [7], [8], [9]]\n"
'from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(train_X, train_y)\n\ny_pred = model.predict(test_X)\n\nimport numpy as np\naccuracy = np.mean(test_y == y_pred)\n'
'            vals  csum  weeks\n2014-01-01     4     4      1\n2014-01-02    -5    -1      1\n...\n2014-01-30    -2    -9      5\n2014-01-31    -5   -14      5\n\n1\n            vals  csum  weeks\n2014-01-01     4     4      1\n2014-01-02    -5    -1      1\n2014-01-03    -4    -5      1\n2014-01-04     4    -1      1\n2014-01-05    -5    -6      1\n2\n            vals  csum  weeks\n2014-01-06    -5   -11      2\n2014-01-07     2    -9      2\n2014-01-08     4    -5      2\n2014-01-09    -1    -6      2\n2014-01-10    -1    -7      2\n2014-01-11    -3   -10      2\n2014-01-12    -2   -12      2\n'
"df.xx == 20\n\n(df.xx == 22).any()\n\nif (df.column_name == '1-5').any():\n    result = df.Minutes\n\nresult = df[src_col]\n"
'def reject_outliers(x_t, y_t, m):\n    mean = np.mean(y_t)\n    std = np.std(y_t)\n    x_t, y_t = zip(*[[x, y] for x, y in zip(x_t, y_t) if abs(y - mean) &lt; (m * std)])\n    return list(x_t), np.array(y_t)\n\n\ndef even_out_distribution(x_t, y_t, n_sections, reduction=0.5, reduce_min=.5, m=2):\n    x_t, y_t = reject_outliers(x_t, y_t, m)\n    linspace = np.linspace(np.min(y_t), np.max(y_t), n_sections + 1)\n    sections = [[] for i in range(n_sections)]\n    for x, y in zip(x_t, y_t):\n        where = max(np.searchsorted(linspace, y) - 1, 0)\n        sections[where].append([x, y])\n    sections = [sec for sec in sections if sec != []]\n\n    min_section = min([len(i) for i in sections])  # np.mean([len(i) for i in sections]) * reduce_min  # todo: in replace of min([len(i) for i in sections])\n    print([len(i) for i in sections])\n    new_sections = []\n    for section in sections:\n        this_section = list(section)\n        if len(section) &gt; min_section:\n            to_remove = (len(section) - min_section) * reduction\n            for i in range(int(to_remove)):\n                this_section.pop(random.randrange(len(this_section)))\n\n        new_sections.append(this_section)\n    print([len(i) for i in new_sections])\n    output = [inner for outer in new_sections for inner in outer]\n    x_t, y_t = zip(*output)\n\n    return list(x_t), np.array(y_t)\n'
'Listing_1 = [1,2,3]\nListing_2 = [4,5,6]\nListing_1.extend(Listing_2)\nListing_1\n[1,2,3,4,5,6]\n\nListing_1 = [1,2,3]\nListing_2 = [4,5,6]\nfor item in Listing_2:\n    Listing_1.append(item)\n'
'train_sizes, train_scores, test_scores, fit_times\n\ntrain_sizes, train_scores, test_scores, fit_times, _\n'
'if any(u_input.lower() in url for url in url_links):\n\nif any(s in url_links for s in u_input.lower()):\n\nif u_input.lower() in url_links:\n'
"df['year'] = df['Registiration_date'].str.split('-')[0][0]\n"
'for i in range(len(y_test)):\n    if result[i] == y_test[i]:\n        print("CORRECT: ", X_test[i])\n    else\n        print("INCORRECT: ", X_test[i])\n'
'clf.partial_fit(x1, y1)\n# get x2, y2\n# update accuracy if needed\nclf.partial_fit(x2, y2)\n'
'import re\nprint(re.search(r\'\\b\\d\\b\', "I\'m 30 years old."))\n\nNone\n'
"df = pd.DataFrame({\n    'Date': ['2019/07/29','2019/01/20','2019/09/3']\n})\n\n    Date\n0   2019/07/29\n1   2019/01/20\n2   2019/09/3\n\ndf['Weekday'] = pd.to_datetime(df['Date']).dt.day_name()\n\n         Date   Weekday\n0   2019/07/29  Monday\n1   2019/01/20  Sunday\n2   2019/09/3   Tuesday\n\n# df['Date'] = pd.to_datetime(df['Date'])\n# df['Weekday'] = df['Date'].dt.day_name()\n"
"import requests\nfrom bs4 import BeautifulSoup\nimport pandas\n\n# Replace search_url with a valid one byb visiting and searching booking.com\nsearch_url = 'https://www.booking.com/searchresults.....'\npage = requests.get(search_url)\nsoup = BeautifulSoup(page.content, 'html.parser')\n\nweek = soup.find(id = 'search_results_table'  )\n#print(week)\n\nitems = week.find_all(class_='sr-hotel__name')\n# print the whole thing\nprint(items[0])\nhotel_name = items[0].getText()\n\n# print hotel name\nprint(hotel_name)\n\n# print without newlines\nprint(hotel_name[1:-1])\n"
"df = pd.DataFrame({'Col1':list('abcdeafgbfhi')})\nsearch_str = 'b'\nidx_list = list(df[(df['Col1']==search_str)].index.values)\nprint(df[idx_list[0]:idx_list[1]])\n\n  Col1\n1    b\n2    c\n3    d\n4    e\n5    a\n6    f\n7    g\n"
'import pandas as pd\ndata = [{\'date\': "2019/12/29", \'hour\': 12, \'output\': 42},\n        {\'date\': "2019/12/30", \'hour\': 10, \'output\': 36},\n        {\'date\': "2019/12/31", \'hour\': 23, \'output\': 48}]\ndf=pd.DataFrame(data)  # build DataFrame (do this with your data).\n\ndf[\'time\'] = df.date + \' \' + df.hour.astype(str)  # create date+time string\ndf[\'time\'] = pd.to_datetime(df.time, format=\'%Y/%m/%d %H\') # convert to datetime format\n\nprint(df)\n         date  hour  output                time\n0  2019/12/29    12      42 2019-12-29 12:00:00\n1  2019/12/30    10      36 2019-12-30 10:00:00\n2  2019/12/31    23      48 2019-12-31 23:00:00\n'
'df = data.frame(Age = c(15,15,16,17,18,18,19,20),\n                Sport = c("Baseball","Baseball","Baseball","Baseball","Baseball","Golf","Golf","Golf"),\n                Mode = c("Play","Play","Play","Watch","Watch","Play","Play","Watch"),\n                stringsAsFactors = F)\n\nlibrary(dplyr)\nlibrary(tidyr)\n\ndf %&gt;%\n  count(Age, Sport) %&gt;%\n  spread(Sport, n, fill = 0)\n\n# # A tibble: 6 x 3\n#     Age Baseball  Golf\n# * &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n# 1    15        2     0\n# 2    16        1     0\n# 3    17        1     0\n# 4    18        1     1\n# 5    19        0     1\n# 6    20        0     1\n\n\ndf %&gt;%\n  count(Age, Mode) %&gt;%\n  spread(Mode, n, fill = 0)\n\n# # A tibble: 6 x 3\n#     Age  Play Watch\n# * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n# 1    15     2     0\n# 2    16     1     0\n# 3    17     0     1\n# 4    18     1     1\n# 5    19     1     0\n# 6    20     0     1\n\ndf = data.frame(Age = c(15,15,16,17,18,18,19,20),\n                Sport = c("Baseball","Baseball","Baseball","Baseball","Baseball","Golf","Golf","Golf"),\n                Mode = c("Play","Play","Play","Watch","Watch","Play","Play","Watch"),\n                stringsAsFactors = F)\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\n\n# function that reshapes data based on a column name\n# (uses Age column as an identifier/key)\nf = function(x) {\ndf %&gt;%\n  group_by_("Age",x) %&gt;%\n  summarise(n = n()) %&gt;%\n  spread_(x, "n", fill = 0) %&gt;%\n  ungroup()\n}\n\n\nnames(df)[names(df) != "Age"] %&gt;%   # get all column names (different than Age)\n  map(f) %&gt;%                        # apply function to each column name\n  reduce(left_join, by="Age")       # join datasets sequentially\n\n# # A tibble: 6 x 5\n#     Age Baseball  Golf  Play Watch\n#   &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n# 1    15        2     0     2     0\n# 2    16        1     0     1     0\n# 3    17        1     0     0     1\n# 4    18        1     1     1     1\n# 5    19        0     1     1     0\n# 6    20        0     1     0     1\n'
"df.groupby('TargetDescription')['Output_media_duration'].sum().reset_index(name ='format_duration')\n"
"solution = minimize(objective,S0,args=(C),method='SLSQP',bounds=boundaries,constraints=con, tol=1e-12)\n\n   fun: -0.01112223334445557\n     jac: array([ -1.11837871e-05,  -1.11222267e-04,  -9.13742697e-05,\n        -6.59456709e-05])\n message: 'Optimization terminated successfully.'\n    nfev: 192\n     nit: 32\n    njev: 32\n  status: 0\n success: True\n       x: array([  0.00000000e+00,   1.00000000e+02,   3.01980663e-14,\n         0.00000000e+00])\n"
"df.genres.str.split('|',expand=True).stack().value_counts().head(3)\nDrama         6\nMusical       6\nChildren's    5\ndtype: int64\n"
"ser = []\nfor year in my_names:\n    ser.append(\n        x/sum(fb_posts2[fb_posts2['year']==year].groupby('title').size()) * 100\n\nser = [x/sum(fb_posts2[fb_posts2['year']==year].groupby('title').size()) * 100\n    for year in my_names]\n"
'import numpy as np\nfrom sklearn.svm import SVR\n\n# Data: 200 instances of 5 features each\nX = randint(1, 100, size=(200, 5))\ny = randint(0, 2, size=200)\n\nreg = SVR()\nreg.fit(X, y)\n\ny_test = np.array([[0, 1, 2, 3, 4]])    # Input to .predict must be 2-dimensional\nreg.predict(y_test)\n'
'set.seed(1001)\ndf1 &lt;- data.frame(AC1 = sample(1:100, 50, replace = TRUE),\n                  AC2 = sample(1:100, 50, replace = TRUE),\n                  AD1 = sample(1:100, 50, replace = TRUE),\n                  AD2 = sample(1:100, 50, replace = TRUE),\n                  BP1 = sample(1:100, 50, replace = TRUE),\n                  BP2 = sample(1:100, 50, replace = TRUE)\n)\n\nlibrary(tidyr)\nlibrary(dplyr)\n\ndf1 %&gt;% \n  gather(Var, Val) %&gt;% \n  mutate(Var2 = gsub("\\\\d+", "", Var)) %&gt;% \n  group_by(Var2) %&gt;% \n  summarise(Sum = sum(Val, na.rm = TRUE), \n            Min = min(Val, na.rm = TRUE), \n            Max = max(Val, na.rm = TRUE))\n\n# A tibble: 3 x 4\n  Var2    Sum   Min   Max\n  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 AC     4846     1   100\n2 AD     4924     4    96\n3 BP     5000     1   100\n'
'# setup\ndf = pd.DataFrame({\n    "data": [\'scarborough, scarborough, scarborough\', \'london,london\', \'north york, north york\', \'test,test\']\n})\n\n# logic\ndef custom_dedup(s):\n    return [*set([_.strip() for _ in s.split(\',\')])][0]\n\ndf[\'data\'].apply(custom_dedup)\n\n    data\n0   scarborough, scarborough, scarborough\n1   london,london\n2   north york, north york\n3   test,test\n\n0    scarborough\n1         london\n2     north york\n3           test\n'
'def yourFunction(x):\n    if(0&lt;=x&lt;1000):\n        raise SyntaxError("Please provide a correct value")\n    pass # your code here\n'
"values = [[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))],\n          [Decoded(data=b'AZ:9475EFWZCNARPEJEZEMXDFHIBI', rect=Rect(left=32, top=191, width=90, height=88))],\n          [Decoded(data=b'AZ:6ECWZUQGEJCR5EZXDH9URCN53M', rect=Rect(left=48, top=183, width=88, height=89))],\n          [Decoded(data=b'AZ:XZ9P6KTDGREM5KIXUO9IHCTKAQ', rect=Rect(left=73, top=121, width=91, height=94))]]\n\ndatas = [value[0].data for value in values]          # list of encoded string (b'')\ndatas = [value[0].data.decode() for value in values] # list of strings\n"
"df[df['City Location'].str.startswith(('S','P'), na=False)]\n\ndf[df['City Location'].str.lower().str.startswith(('s','p'), na=False)]\n"
"df.filter(like='GR').sum()\n\nGR_1         0\nGR_2         0\nGR_3         0\nGR_4         0\nGR_5         0\nGR_6        42\nGR_7        61\nGR_8        46\nUNGR_ELM     0\nGR_9         0\nGR_10        0\nGR_11        0\nGR_12        0\nUNGR_SEC     0\n"
'emp_length&lt;-c("account","accountant","accounting","account specialist","Data Scientist","Data Science Expert")\n\ncluster&lt;-kmeans(stringdistmatrix(emp_length,emp_length,method="jw"),centers=2)\ncluster_n&lt;-cluster$cluster\n\ncbind(emp_length,cluster_n)\n     emp_length            cluster_n\n[1,] "account"             "2"      \n[2,] "accountant"          "2"      \n[3,] "accounting"          "2"      \n[4,] "account specialist"  "2"      \n[5,] "Data Scientist"      "1"      \n[6,] "Data Science Expert" "1" \n'
'line = "Michigan, (Ann Arbor"\n'
"names =  ['Alabama[edit]',\n          'Auburn',\n          'Florence',\n          'Jacksonville',\n          'Livingston',\n          'Montevallo',\n          'Troy',\n          'Tuscaloosa',\n          'Tuskegee',\n          'Alaska[edit]',\n          'Fairbanks',\n          'Arizona[edit]',\n          'Flagstaff',\n          'Tempe',\n          'Tucson',\n          'Arkansas[edit]',\n          'Arkadelphia']\n\n\ndata = []\nstate = None\nfor name in names:\n    name = name.strip()\n    if name.endswith('[edit]'):\n        state = name[:-6]\n        continue\n    if not state:     # In case the first name of the list is not a state\n        state = 'Unknown'\n    data.append((state,name))\n\ndf = pd.DataFrame(data)\n\n&gt;&gt;&gt; df\n           0             1\n0    Alabama        Auburn\n1    Alabama      Florence\n2    Alabama  Jacksonville\n3    Alabama    Livingston\n4    Alabama    Montevallo\n5    Alabama          Troy\n6    Alabama    Tuscaloosa\n7    Alabama      Tuskegee\n8     Alaska     Fairbanks\n9    Arizona     Flagstaff\n10   Arizona         Tempe\n11   Arizona        Tucson\n12  Arkansas   Arkadelphia\n"
'import itertools\n\n\narray = [ 1, 2, ... , 1500] # or other numbers\nf = sum   # or whatever function you have\n\n\ndef fmax_on_perms(array, f): \n    perms = itertools.permutations(array) # generator of all the \n           permutations rather than static list\n    fvalues = map(my_function, perms) # generator of values of your   function on different permutations\n    return max(fvalues)\n'
"df = pd.DataFrame({'userID': [20394756382,29304857203,20294857642,20293847564,20192837453],\n                   'UserID2' : [38493,2212324,30498,30928,432]})\n\ndf = df.loc[df['UserID2'].astype(str).str.len().eq(5)]\nprint(df)\n        userID  UserID2\n0  20394756382    38493\n2  20294857642    30498\n3  20293847564    30928\n"
"def simple():\n    p = float(input('Enter principal amount :'))\n    r = float(input('Enter rate of interest :'))\n    t = float(input('Enter time in years    :'))\n    return (p * t * r) / 100\n\nsi = simple()\nprint(&quot;Simple interest : {}&quot;.format(si))\n\nEnter principal amount :1000\nEnter rate of interest :2\nEnter time in years    :5\nSimple interest : 100.0\n\ndef simple(p,r,t):\n    return (p * t * r) / 100\n\nsi = simple(1000, 2, 5)\nprint(&quot;Simple interest : {}&quot;.format(si))\n\nsi = simple(2500, 2, 5)\nprint(&quot;Simple interest : {}&quot;.format(si))\n\nsi = simple(2500, 4, 5)\nprint(&quot;Simple interest : {}&quot;.format(si))\n\nSimple interest : 100.0\n\nSimple interest : 250.0\n\nSimple interest : 500.0\n"
"[{'name':i[0], 'children': [{'name': i[1], 'value': i[2]}]}  for i in tuples]\n\n# should work as long as you aren't doing any fancy sorting\nstuples = sorted(tuples) \n\nname = None\ndat = None\nfor t in stuples:\n    if name != t[0]:\n        if dat is not None:\n            writeDat(json.dumps(dat))\n        name = t[0]\n        dat = {'name': t[0], 'children': [{'name': t[1], 'value': t[2]}]}\n    else:\n        dat['children'].append({'name': t1, 'value': t[2]})\n"
"import pandas as pd\n\nframe = pd.read_csv('myfile.csv', header=None)\nframe.columns = ['user_id', 'date', 'event_type']\nframe_pivoted = frame.pivot_table(\n    index='user_id', columns='event_type', aggfunc='count'\n)\n"
"list= [('اکلیل', 'N'), ('احمد', 'Ne'), ('استاد', 'N'), ('پوهنتون', 'Ne'), ('کابل', 'N'), ('است', 'V'), ('.', 'PUNC'), ('او', 'PRO'), ('هر', 'DET'), ('روز', 'N'), ('ساعت', 'Ne'), ('۸', 'NUM'), ('بجه', 'N'), ('به', 'P'), ('کار', 'N'), ('میرود', 'V'), ('.', 'PUNC')]\n\nfor i,j in list:\n    if ( j == 'N' or j == 'PRO'):\n        return True\n        #your code goes here\n"
"numbers = [float(p.replace('€','').replace('M','')) for p in a]\n\n[110.5, 210.5, 310.5]\n"
'for i in range(1, int(1.2e34)+1):\n    print(i)   # or do whatever you want\n\ni = 1\nwhile i &lt;= 1.2e34:\n    print i    # or do whatever you want\n    i += 1\n'
