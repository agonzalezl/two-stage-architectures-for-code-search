{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [(\"sodata/tensorflow/code.txt\", \"sodata/tensorflow/title.txt\", \"sodata/tensorflow/meta.txt\"),\n",
    "         (\"sodata/machine-learning_python/code.txt\",\"sodata/machine-learning_python/title.txt\",\"sodata/machine-learning_python/meta.txt\"),\n",
    "         (\"sodata/data-science_python/code.txt\",\"sodata/data-science_python/title.txt\",\"sodata/data-science_python/meta.txt\"),\n",
    "         (\"sodata/data-cleaning_python/code.txt\",\"sodata/data-cleaning_python/title.txt\",\"sodata/data-cleaning_python/meta.txt\"),\n",
    "         (\"sodata/data-science_python/code.txt\",\"sodata/data-science_python/title.txt\",\"sodata/data-science_python/meta.txt\")\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_code_file(path):\n",
    "    file = open(path, 'r', encoding=\"utf-8\")\n",
    "    code_file = file.read()\n",
    "\n",
    "    processed_code = []\n",
    "\n",
    "    for entry in code_file.split(\"\\n\"):\n",
    "\n",
    "        filtered_lanes = \"\"\n",
    "\n",
    "        for line in entry.split(\"\\\\n\"):\n",
    "            line = line.strip()\n",
    "            line = line.replace(\"(\", \" \").replace(\")\", \" \").replace(\"\\\"\", \" \").replace(\"'\", \" \").replace(\"&quot;\", \" \").replace(\"_\", \" \").replace(\".\", \" \").replace(\",\", \" \").replace(\"=\", \" \")\n",
    "            line = line.replace(\":\", \" \").replace(\"[\", \" \").replace(\"]\", \" \").replace(\"\\\\\",\" \").replace(\"/\",\" \")\n",
    "            line = line.replace(\"+\",\" \").replace(\"-\",\" \").replace(\"_\",\" \").replace(\"&gt;\", \" \").replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "            line = line.replace(\"%\", \" \").replace(\"$\", \" \")\n",
    "            # remove numbers\n",
    "            line = res = re.sub('([0-9]+.[0-9]+|[0-9]+)', ' NUM ', line) \n",
    "            \n",
    "            # Split camelcase\n",
    "            line = re.sub('([A-Z][a-z]+)', r' \\1', re.sub('([A-Z]+)', r' \\1', line))\n",
    "            \n",
    "            line = line.lower()\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"print\") or line.startswith(\"#\") or line.startswith(\"\\\"#\") or line == \"\" or line == \"\\\"\" :\n",
    "                continue\n",
    "            \n",
    "            # remove non alphanumeric characters\n",
    "            line = re.sub(\"[^0-9a-zA-Z ]+\", ' ', line)\n",
    "            \n",
    "            filtered_lanes += line+\" \"\n",
    "\n",
    "        processed_code.append(filtered_lanes)\n",
    "\n",
    "    file.close()\n",
    "    return processed_code  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_title_file(path):\n",
    "    file = open(path, 'r', encoding=\"utf-8\")\n",
    "    tile_file = file.read()\n",
    "    tile_file = tile_file.replace(\"?\", \" \").replace(\",\", \" \").replace(\".\", \" \").replace(\")\", \" \").replace(\"(\", \" \")\n",
    "    tile_file = tile_file.replace(\":\", \" \").replace(\"`\", \"\").replace(\"[\", \" \").replace(\"]\", \" \")\n",
    "    tile_file = tile_file.split(\"\\n\")\n",
    "    return tile_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_meta_file(path):\n",
    "    file = open(path, 'r', encoding=\"utf-8\")\n",
    "    meta_file = file.read()\n",
    "    return meta_file.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "codes = []\n",
    "titles = []\n",
    "urls = []\n",
    "for file in files:\n",
    "    codes.extend(process_code_file(file[0]))\n",
    "    titles.extend(process_title_file(file[1]))\n",
    "    urls.extend(process_meta_file(file[2]))\n",
    "    \n",
    "c = list(zip(codes, titles, urls))\n",
    "\n",
    "random.shuffle(c)\n",
    "\n",
    "codes, titles, urls = zip(*c)\n",
    "\n",
    "codes = list(codes)\n",
    "titles = list(titles)\n",
    "urls = list(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_division_pos =  int(len(codes) * 0.8)\n",
    "\n",
    "training_codes = codes[:test_division_pos]\n",
    "training_titles = titles[:test_division_pos]\n",
    "\n",
    "test_codes = codes[test_division_pos:]\n",
    "test_titles = titles[test_division_pos:]\n",
    "\n",
    "print(len(training_codes), len(test_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer with code + titles\n",
    "extended_training = []\n",
    "extended_training.extend(training_codes)\n",
    "extended_training.extend(training_titles)\n",
    "len(extended_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "tokenizer.enable_padding()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]) \n",
    "\n",
    "tokenizer.train_from_iterator(extended_training, trainer=trainer)\n",
    "\n",
    "print(\"Number of tokens\", len(tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer.encode_batch(extended_training, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 0\n",
    "for output in outputs:\n",
    "    sentence_len = max(sentence_len, len(output.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max sentence lenght\", sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-friendly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "longer_input_size = sentence_len\n",
    "number_of_tokens = len(tokenizer.get_vocab())\n",
    "\n",
    "input_layer = tf.keras.Input(shape=(longer_input_size,), name=\"input\")\n",
    "embeding_layer = tf.keras.layers.Embedding(number_of_tokens, embedding_size, name=\"embeding\")(input_layer)\n",
    "\n",
    "attention_layer = tf.keras.layers.Attention(name=\"attention\")([embeding_layer, embeding_layer])\n",
    "\n",
    "print(attention_layer.shape)\n",
    "\n",
    "sum_layer = tf.keras.layers.Lambda(lambda x: K.sum(x, axis=1), name=\"sum\")( attention_layer)\n",
    "#average_layer = tf.keras.layers.Lambda(lambda x: K.mean(x, axis=1), name=\"average\")( attention_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_layer], outputs=[sum_layer], name='siamese_model')\n",
    "\n",
    "tf.keras.utils.plot_model(model, \"cos_model.png\", show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_code = tf.keras.Input(shape=(longer_input_size,), name=\"code\")\n",
    "input_desc = tf.keras.Input(shape=(longer_input_size,), name=\"desc\")\n",
    "\n",
    "output_code = model(input_code)\n",
    "output_desc = model(input_desc)\n",
    "\n",
    "cos_sim = tf.keras.layers.Dot(axes=1, normalize=True, name='cos_sim')([output_code, output_desc]) \n",
    "\n",
    "cos_model = tf.keras.Model(inputs=[input_code, input_desc], outputs=[cos_sim],name='sim_model')  \n",
    "\n",
    "tf.keras.utils.plot_model(cos_model, \"cos_model.png\", show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_codes = codes[:test_division_pos]\n",
    "training_titles = titles[:test_division_pos]\n",
    "\n",
    "\n",
    "negative_titles = training_titles\n",
    "negative_codes = training_codes\n",
    "\n",
    "random.shuffle(negative_codes)\n",
    "\n",
    "positive_results = np.ones((len(training_codes)))\n",
    "negative_results = np.zeros((len(negative_titles)))\n",
    "\n",
    "print(len(training_codes), len(negative_codes),len(positive_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-advocate",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_codes = []\n",
    "final_training_codes.extend(training_codes)\n",
    "final_training_codes.extend(negative_codes)\n",
    "\n",
    "final_training_titles = []\n",
    "final_training_titles.extend(training_titles)\n",
    "final_training_titles.extend(negative_titles)\n",
    "\n",
    "results = np.concatenate((positive_results, negative_results), axis=0)\n",
    "\n",
    "print(len(final_training_codes) , len(final_training_titles), len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, code in enumerate(final_training_codes):\n",
    "    \n",
    "    output = tokenizer.encode(code, add_special_tokens=True)\n",
    "    output.pad(sentence_len, direction=\"right\", pad_token=\"[PAD]\")\n",
    "    \n",
    "    final_training_codes[idx] = np.array(output.ids)\n",
    "    \n",
    "    if idx%5000 == 0:\n",
    "        print(idx,\"/\", len(final_training_codes))\n",
    "\n",
    "final_training_codes = np.array(final_training_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, code in enumerate(final_training_titles):\n",
    "    \n",
    "    output = tokenizer.encode(code, add_special_tokens=True)\n",
    "    output.pad(sentence_len, direction=\"right\", pad_token=\"[PAD]\")\n",
    "    \n",
    "    final_training_titles[idx] = np.array(output.ids)\n",
    "    \n",
    "    if idx%5000 == 0:\n",
    "        print(idx,\"/\",len(final_training_titles))\n",
    "\n",
    "final_training_titles = np.array(final_training_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earlystop_callback = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "cos_model.fit(x=[final_training_codes, final_training_titles], y=results, epochs=1, verbose=1, batch_size=32)\n",
    "              #callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_codes = codes[test_division_pos:]\n",
    "test_titles = titles[test_division_pos:]\n",
    "positive_test_results = np.ones((len(training_codes)))\n",
    "\n",
    "for idx, code in enumerate(test_codes):\n",
    "    \n",
    "    output = tokenizer.encode(code, add_special_tokens=True)\n",
    "    output.pad(sentence_len, direction=\"right\", pad_token=\"[PAD]\")\n",
    "    \n",
    "    test_codes[idx] = np.array(output.ids)\n",
    "    \n",
    "    if idx%2500 == 0:\n",
    "        print(idx,\"/\",len(test_codes))\n",
    "\n",
    "test_codes = np.array(test_codes)\n",
    "\n",
    "\n",
    "for idx, code in enumerate(test_titles):\n",
    "    \n",
    "    output = tokenizer.encode(code, add_special_tokens=True)\n",
    "    output.pad(sentence_len, direction=\"right\", pad_token=\"[PAD]\")\n",
    "    \n",
    "    test_titles[idx] = np.array(output.ids)\n",
    "    \n",
    "    if idx%2500 == 0:\n",
    "        print(idx,\"/\",len(test_titles))\n",
    "\n",
    "test_titles = np.array(test_titles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cos_model.metrics_names)\n",
    "cos_model.evaluate(x=[test_codes, test_titles], y=positive_test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-polymer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
