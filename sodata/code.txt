"import pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n\n10 100\n11 110\n12 120\n"
"df.loc[df['column_name'] == some_value]\n\ndf.loc[df['column_name'].isin(some_values)]\n\ndf.loc[(df['column_name'] &gt;= A) &amp; (df['column_name'] &lt;= B)]\n\ndf['column_name'] &gt;= A &amp; df['column_name'] &lt;= B\n\ndf['column_name'] &gt;= (A &amp; df['column_name']) &lt;= B\n\ndf.loc[df['column_name'] != some_value]\n\ndf.loc[~df['column_name'].isin(some_values)]\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nprint(df.loc[df['B'].isin(['one','three'])])\n\n     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n\ndf = df.set_index(['B'])\nprint(df.loc['one'])\n\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n\ndf.loc[df.index.isin(['one','two'])]\n\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n"
"&gt;&gt;&gt; df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n&gt;&gt;&gt; df\n   $a  $b\n0   1  10\n1   2  20\n\n&gt;&gt;&gt; df.columns = ['a', 'b']\n&gt;&gt;&gt; df\n   a   b\n0  1  10\n1  2  20\n"
"del df['column_name']\n"
"df1 = df[['a', 'b']]\n\ndf1 = df.iloc[:, 0:2] # Remember that Python does not slice inclusive of the ending index.\n\ndf1 = df.iloc[0, 0:2].copy() # To avoid the case where changing df1 also changes df\n\n{df.columns.get_loc(c): c for idx, c in enumerate(df.columns)}\n"
'list(my_dataframe.columns.values)\n\nlist(my_dataframe)\n'
"df1['e'] = pd.Series(np.random.randn(sLength), index=df1.index)\n\n&gt;&gt;&gt; sLength = len(df1['a'])\n&gt;&gt;&gt; df1\n          a         b         c         d\n6 -0.269221 -0.026476  0.997517  1.294385\n8  0.917438  0.847941  0.034235 -0.448948\n\n&gt;&gt;&gt; df1['e'] = pd.Series(np.random.randn(sLength), index=df1.index)\n&gt;&gt;&gt; df1\n          a         b         c         d         e\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167\n8  0.917438  0.847941  0.034235 -0.448948  2.228131\n\n&gt;&gt;&gt; p.version.short_version\n'0.16.1'\n\n&gt;&gt;&gt; df1.loc[:,'f'] = pd.Series(np.random.randn(sLength), index=df1.index)\n&gt;&gt;&gt; df1\n          a         b         c         d         e         f\n6 -0.269221 -0.026476  0.997517  1.294385  1.757167 -0.050927\n8  0.917438  0.847941  0.034235 -0.448948  2.228131  0.006109\n&gt;&gt;&gt; \n\ndf1 = df1.assign(e=pd.Series(np.random.randn(sLength)).values)\n"
"import numpy as np\nimport pandas as pd\n\n# create a store\nstore = pd.HDFStore('mystore.h5')\n\n# this is the key to your storage:\n#    this maps your fields to a specific group, and defines \n#    what you want to have as data_columns.\n#    you might want to create a nice class wrapping this\n#    (as you will want to have this map and its inversion)  \ngroup_map = dict(\n    A = dict(fields = ['field_1','field_2',.....], dc = ['field_1',....,'field_5']),\n    B = dict(fields = ['field_10',......        ], dc = ['field_10']),\n    .....\n    REPORTING_ONLY = dict(fields = ['field_1000','field_1001',...], dc = []),\n\n)\n\ngroup_map_inverted = dict()\nfor g, v in group_map.items():\n    group_map_inverted.update(dict([ (f,g) for f in v['fields'] ]))\n\nfor f in files:\n   # read in the file, additional options may be necessary here\n   # the chunksize is not strictly necessary, you may be able to slurp each \n   # file into memory in which case just eliminate this part of the loop \n   # (you can also change chunksize if necessary)\n   for chunk in pd.read_table(f, chunksize=50000):\n       # we are going to append to each table by group\n       # we are not going to create indexes at this time\n       # but we *ARE* going to create (some) data_columns\n\n       # figure out the field groupings\n       for g, v in group_map.items():\n             # create the frame for this group\n             frame = chunk.reindex(columns = v['fields'], copy = False)    \n\n             # append it\n             store.append(g, frame, index=False, data_columns = v['dc'])\n\nframe = store.select(group_that_I_want)\n# you can optionally specify:\n# columns = a list of the columns IN THAT GROUP (if you wanted to\n#     select only say 3 out of the 20 columns in this sub-table)\n# and a where clause if you want a subset of the rows\n\n# do calculations on this frame\nnew_frame = cool_function_on_frame(frame)\n\n# to 'add columns', create a new group (you probably want to\n# limit the columns in this new_group to be only NEW ones\n# (e.g. so you don't overlap from the other tables)\n# add this info to the group_map\nstore.append(new_group, new_frame.reindex(columns = new_columns_created, copy = False), data_columns = new_columns_created)\n\n# This may be a bit tricky; and depends what you are actually doing.\n# I may need to modify this function to be a bit more general:\nreport_data = store.select_as_multiple([groups_1,groups_2,.....], where =['field_1&gt;0', 'field_1000=foo'], selector = group_1)\n\nstore.select(group, where = ['field_1000=foo', 'field_1001&gt;0'])\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from numpy.random import randint\n\n&gt;&gt;&gt; df = pd.DataFrame(columns=['lib', 'qty1', 'qty2'])\n&gt;&gt;&gt; for i in range(5):\n&gt;&gt;&gt;     df.loc[i] = ['name' + str(i)] + list(randint(10, size=2))\n\n&gt;&gt;&gt; df\n     lib qty1 qty2\n0  name0    3    3\n1  name1    2    4\n2  name2    2    8\n3  name3    2    1\n4  name4    9    6\n"
"&gt;&gt;&gt; s = pd.Series([&quot;8&quot;, 6, &quot;7.5&quot;, 3, &quot;0.9&quot;]) # mixed string and numeric values\n&gt;&gt;&gt; s\n0      8\n1      6\n2    7.5\n3      3\n4    0.9\ndtype: object\n\n&gt;&gt;&gt; pd.to_numeric(s) # convert everything to float values\n0    8.0\n1    6.0\n2    7.5\n3    3.0\n4    0.9\ndtype: float64\n\n# convert Series\nmy_series = pd.to_numeric(my_series)\n\n# convert column &quot;a&quot; of a DataFrame\ndf[&quot;a&quot;] = pd.to_numeric(df[&quot;a&quot;])\n\n# convert all columns of DataFrame\ndf = df.apply(pd.to_numeric) # convert all columns of DataFrame\n\n# convert just columns &quot;a&quot; and &quot;b&quot;\ndf[[&quot;a&quot;, &quot;b&quot;]] = df[[&quot;a&quot;, &quot;b&quot;]].apply(pd.to_numeric)\n\n&gt;&gt;&gt; s = pd.Series(['1', '2', '4.7', 'pandas', '10'])\n&gt;&gt;&gt; s\n0         1\n1         2\n2       4.7\n3    pandas\n4        10\ndtype: object\n\n&gt;&gt;&gt; pd.to_numeric(s) # or pd.to_numeric(s, errors='raise')\nValueError: Unable to parse string\n\n&gt;&gt;&gt; pd.to_numeric(s, errors='coerce')\n0     1.0\n1     2.0\n2     4.7\n3     NaN\n4    10.0\ndtype: float64\n\n&gt;&gt;&gt; pd.to_numeric(s, errors='ignore')\n# the original Series is returned untouched\n\ndf.apply(pd.to_numeric, errors='ignore')\n\n&gt;&gt;&gt; s = pd.Series([1, 2, -7])\n&gt;&gt;&gt; s\n0    1\n1    2\n2   -7\ndtype: int64\n\n&gt;&gt;&gt; pd.to_numeric(s, downcast='integer')\n0    1\n1    2\n2   -7\ndtype: int8\n\n&gt;&gt;&gt; pd.to_numeric(s, downcast='float')\n0    1.0\n1    2.0\n2   -7.0\ndtype: float32\n\n# convert all DataFrame columns to the int64 dtype\ndf = df.astype(int)\n\n# convert column &quot;a&quot; to int64 dtype and &quot;b&quot; to complex type\ndf = df.astype({&quot;a&quot;: int, &quot;b&quot;: complex})\n\n# convert Series to float16 type\ns = s.astype(np.float16)\n\n# convert Series to Python strings\ns = s.astype(str)\n\n# convert Series to categorical type - see docs for more details\ns = s.astype('category')\n\n&gt;&gt;&gt; s = pd.Series([1, 2, -7])\n&gt;&gt;&gt; s\n0    1\n1    2\n2   -7\ndtype: int64\n\n&gt;&gt;&gt; s.astype(np.uint8)\n0      1\n1      2\n2    249\ndtype: uint8\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': [7, 1, 5], 'b': ['3','2','1']}, dtype='object')\n&gt;&gt;&gt; df.dtypes\na    object\nb    object\ndtype: object\n\n&gt;&gt;&gt; df = df.infer_objects()\n&gt;&gt;&gt; df.dtypes\na     int64\nb    object\ndtype: object\n\n&gt;&gt;&gt; df.convert_dtypes().dtypes                                             \na     Int64\nb    string\ndtype: object\n\n&gt;&gt;&gt; df.convert_dtypes(infer_objects=False).dtypes                          \na    object\nb    string\ndtype: object\n"
"In [1]: df = pd.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})\n\nIn [2]: df\nOut[2]:\n   A  B\n0  5  1\n1  6  2\n2  3  3\n3  4  5\n\nIn [3]: df[df['A'].isin([3, 6])]\nOut[3]:\n   A  B\n1  6  2\n2  3  3\n\nIn [4]: df[~df['A'].isin([3, 6])]\nOut[4]:\n   A  B\n0  5  1\n3  4  5\n"
"df.to_csv(file_name, sep='\\t')\n\ndf.to_csv(file_name, sep='\\t', encoding='utf-8')\n"
"df[df['A'] &gt; 2]['B'] = new_val  # new_val not set in df\n\ndf.loc[df['A'] &gt; 2, 'B'] = new_val\n\ndf = df[df['A'] &gt; 2]\ndf['B'] = new_val\n\nimport pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\n"
'df = pd.DataFrame(d)\n'
"with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n    print(df)\n"
"import pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n"
"&gt;&gt;&gt; s = pd.Series(list(&quot;abcdef&quot;), index=[49, 48, 47, 0, 1, 2]) \n49    a\n48    b\n47    c\n0     d\n1     e\n2     f\n\n&gt;&gt;&gt; s.loc[0]    # value at index label 0\n'd'\n\n&gt;&gt;&gt; s.iloc[0]   # value at index location 0\n'a'\n\n&gt;&gt;&gt; s.loc[0:1]  # rows at index labels between 0 and 1 (inclusive)\n0    d\n1    e\n\n&gt;&gt;&gt; s.iloc[0:1] # rows at index location between 0 and 1 (exclusive)\n49    a\n\n&gt;&gt;&gt; s2 = pd.Series(s.index, index=s.values)\n&gt;&gt;&gt; s2\na    49\nb    48\nc    47\nd     0\ne     1\nf     2\n\n&gt;&gt;&gt; s2.loc['c':'e']  # all rows lying between 'c' and 'e' (inclusive)\nc    47\nd     0\ne     1\n\n&gt;&gt;&gt; s3 = pd.Series(list('abcde'), pd.date_range('now', periods=5, freq='M')) \n&gt;&gt;&gt; s3\n2021-01-31 16:41:31.879768    a\n2021-02-28 16:41:31.879768    b\n2021-03-31 16:41:31.879768    c\n2021-04-30 16:41:31.879768    d\n2021-05-31 16:41:31.879768    e\n\n&gt;&gt;&gt; s3.loc['2021-03':'2021-04']\n2021-03-31 17:04:30.742316    c\n2021-04-30 17:04:30.742316    d\n\n&gt;&gt;&gt; import numpy as np \n&gt;&gt;&gt; df = pd.DataFrame(np.arange(25).reshape(5, 5),  \n                      index=list('abcde'), \n                      columns=['x','y','z', 8, 9])\n&gt;&gt;&gt; df\n    x   y   z   8   9\na   0   1   2   3   4\nb   5   6   7   8   9\nc  10  11  12  13  14\nd  15  16  17  18  19\ne  20  21  22  23  24\n\n&gt;&gt;&gt; df.loc['c': , :'z']  # rows 'c' and onwards AND columns up to 'z'\n    x   y   z\nc  10  11  12\nd  15  16  17\ne  20  21  22\n\n&gt;&gt;&gt; df.iloc[:, 3]        # all rows, but only the column at index location 3\na     3\nb     8\nc    13\nd    18\ne    23\n\n&gt;&gt;&gt; import numpy as np \n&gt;&gt;&gt; df = pd.DataFrame(np.arange(25).reshape(5, 5),  \n                      index=list('abcde'), \n                      columns=['x','y','z', 8, 9])\n&gt;&gt;&gt; df\n    x   y   z   8   9\na   0   1   2   3   4\nb   5   6   7   8   9\nc  10  11  12  13  14\nd  15  16  17  18  19\ne  20  21  22  23  24\n\n&gt;&gt;&gt; df.iloc[:df.index.get_loc('c') + 1, :4]\n    x   y   z   8\na   0   1   2   3\nb   5   6   7   8\nc  10  11  12  13\n"
'df = df[df.line_race != 0]\n'
"import datetime\nimport pandas as pd\nimport numpy as np\n\ntodays_date = datetime.datetime.now().date()\nindex = pd.date_range(todays_date-datetime.timedelta(10), periods=10, freq='D')\n\ncolumns = ['A','B', 'C']\n\ndf_ = pd.DataFrame(index=index, columns=columns)\ndf_ = df_.fillna(0) # with 0s rather than NaNs\n\ndata = np.array([np.arange(10)]*3).T\n\nIn [10]: df = pd.DataFrame(data, index=index, columns=columns)\n\nIn [11]: df\nOut[11]: \n            A  B  C\n2012-11-29  0  0  0\n2012-11-30  1  1  1\n2012-12-01  2  2  2\n2012-12-02  3  3  3\n2012-12-03  4  4  4\n2012-12-04  5  5  5\n2012-12-05  6  6  6\n2012-12-06  7  7  7\n2012-12-07  8  8  8\n2012-12-08  9  9  9\n"
"df.xs('C')['x']=10\n\ndf['x']['C'] = 10\n\ndf.at['C', 'x'] = 10\n\nIn [18]: %timeit df.set_value('C', 'x', 10)\n100000 loops, best of 3: 2.9 µs per loop\n\nIn [20]: %timeit df['x']['C'] = 10\n100000 loops, best of 3: 6.31 µs per loop\n\nIn [81]: %timeit df.at['C', 'x'] = 10\n100000 loops, best of 3: 9.2 µs per loop\n"
"df['index1'] = df.index\n\ndf.reset_index(level=0, inplace=True)\n\n&gt;&gt;&gt; df\n                       val\ntick       tag obs        \n2016-02-26 C   2    0.0139\n2016-02-27 A   2    0.5577\n2016-02-28 C   6    0.0303\n\n&gt;&gt;&gt; df.reset_index(level=['tick', 'obs'])\n          tick  obs     val\ntag                        \nC   2016-02-26    2  0.0139\nA   2016-02-27    2  0.5577\nC   2016-02-28    6  0.0303\n"
'In [19]: type(g1)\nOut[19]: pandas.core.frame.DataFrame\n\nIn [20]: g1.index\nOut[20]: \nMultiIndex([(\'Alice\', \'Seattle\'), (\'Bob\', \'Seattle\'), (\'Mallory\', \'Portland\'),\n       (\'Mallory\', \'Seattle\')], dtype=object)\n\nIn [21]: g1.add_suffix(\'_Count\').reset_index()\nOut[21]: \n      Name      City  City_Count  Name_Count\n0    Alice   Seattle           1           1\n1      Bob   Seattle           2           2\n2  Mallory  Portland           2           2\n3  Mallory   Seattle           1           1\n\nIn [36]: DataFrame({\'count\' : df1.groupby( [ "Name", "City"] ).size()}).reset_index()\nOut[36]: \n      Name      City  count\n0    Alice   Seattle      1\n1      Bob   Seattle      2\n2  Mallory  Portland      2\n3  Mallory   Seattle      1\n'
"import pandas as pd\n\n&gt;&gt;&gt; df\n  country\n0        US\n1        UK\n2   Germany\n3     China\n&gt;&gt;&gt; countries_to_keep\n['UK', 'China']\n&gt;&gt;&gt; df.country.isin(countries_to_keep)\n0    False\n1     True\n2    False\n3     True\nName: country, dtype: bool\n&gt;&gt;&gt; df[df.country.isin(countries_to_keep)]\n  country\n1        UK\n3     China\n&gt;&gt;&gt; df[~df.country.isin(countries_to_keep)]\n  country\n0        US\n2   Germany\n"
'$ python3 -m memory_profiler .\\test.py\nFilename: .\\test.py\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     5     68.5 MiB     68.5 MiB   @profile\n     6                             def shuffle():\n     7    847.8 MiB    779.3 MiB       df = pd.DataFrame(np.random.randn(100, 1000000))\n     8    847.9 MiB      0.1 MiB       df = df.sample(frac=1).reset_index(drop=True)\n\n'
"df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])\n"
'In [7]: df\nOut[7]: \n          0         1\n0       NaN       NaN\n1 -0.494375  0.570994\n2       NaN       NaN\n3  1.876360 -0.229738\n4       NaN       NaN\n\nIn [8]: df.fillna(0)\nOut[8]: \n          0         1\n0  0.000000  0.000000\n1 -0.494375  0.570994\n2  0.000000  0.000000\n3  1.876360 -0.229738\n4  0.000000  0.000000\n\nIn [12]: df[1].fillna(0, inplace=True)\nOut[12]: \n0    0.000000\n1    0.570994\n2    0.000000\n3   -0.229738\n4    0.000000\nName: 1\n\nIn [13]: df\nOut[13]: \n          0         1\n0       NaN  0.000000\n1 -0.494375  0.570994\n2       NaN  0.000000\n3  1.876360 -0.229738\n4       NaN  0.000000\n\ndf.fillna({1:0}, inplace=True)\n'
"In [116]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])\n\nIn [117]: frame\nOut[117]: \n               b         d         e\nUtah   -0.029638  1.081563  1.280300\nOhio    0.647747  0.831136 -1.549481\nTexas   0.513416 -0.884417  0.195343\nOregon -0.485454 -0.477388 -0.309548\n\nIn [118]: f = lambda x: x.max() - x.min()\n\nIn [119]: frame.apply(f)\nOut[119]: \nb    1.133201\nd    1.965980\ne    2.829781\ndtype: float64\n\nIn [120]: format = lambda x: '%.2f' % x\n\nIn [121]: frame.applymap(format)\nOut[121]: \n            b      d      e\nUtah    -0.03   1.08   1.28\nOhio     0.65   0.83  -1.55\nTexas    0.51  -0.88   0.20\nOregon  -0.49  -0.48  -0.31\n\nIn [122]: frame['e'].map(format)\nOut[122]: \nUtah       1.28\nOhio      -1.55\nTexas      0.20\nOregon    -0.31\nName: e, dtype: object\n"
'np.random.seed(0)\nleft = pd.DataFrame({\'key\': [\'A\', \'B\', \'C\', \'D\'], \'value\': np.random.randn(4)})    \nright = pd.DataFrame({\'key\': [\'B\', \'D\', \'E\', \'F\'], \'value\': np.random.randn(4)})\n  \nleft\n\n  key     value\n0   A  1.764052\n1   B  0.400157\n2   C  0.978738\n3   D  2.240893\n\nright\n\n  key     value\n0   B  1.867558\n1   D -0.977278\n2   E  0.950088\n3   F -0.151357\n\nleft.merge(right, on=\'key\')\n# Or, if you want to be explicit\n# left.merge(right, on=\'key\', how=\'inner\')\n\n  key   value_x   value_y\n0   B  0.400157  1.867558\n1   D  2.240893 -0.977278\n\nleft.merge(right, on=\'key\', how=\'left\')\n\n  key   value_x   value_y\n0   A  1.764052       NaN\n1   B  0.400157  1.867558\n2   C  0.978738       NaN\n3   D  2.240893 -0.977278\n\nleft.merge(right, on=\'key\', how=\'right\')\n\n  key   value_x   value_y\n0   B  0.400157  1.867558\n1   D  2.240893 -0.977278\n2   E       NaN  0.950088\n3   F       NaN -0.151357\n\nleft.merge(right, on=\'key\', how=\'outer\')\n\n  key   value_x   value_y\n0   A  1.764052       NaN\n1   B  0.400157  1.867558\n2   C  0.978738       NaN\n3   D  2.240893 -0.977278\n4   E       NaN  0.950088\n5   F       NaN -0.151357\n\n(left.merge(right, on=\'key\', how=\'left\', indicator=True)\n     .query(\'_merge == &quot;left_only&quot;\')\n     .drop(\'_merge\', 1))\n\n  key   value_x  value_y\n0   A  1.764052      NaN\n2   C  0.978738      NaN\n\nleft.merge(right, on=\'key\', how=\'left\', <b>indicator=True</b>)\n\n  key   value_x   value_y     _merge\n0   A  1.764052       NaN  left_only\n1   B  0.400157  1.867558       both\n2   C  0.978738       NaN  left_only\n3   D  2.240893 -0.977278       both\n(left.merge(right, on=\'key\', how=\'right\', <b>indicator=True</b>)\n     .query(\'_merge == "right_only"\')\n     .drop(\'_merge\', 1))\n\n  key  value_x   value_y\n2   E      NaN  0.950088\n3   F      NaN -0.151357\n(left.merge(right, on=\'key\', how=\'outer\', indicator=True)\n     .query(\'_merge != &quot;both&quot;\')\n     .drop(\'_merge\', 1))\n\n  key   value_x   value_y\n0   A  1.764052       NaN\n2   C  0.978738       NaN\n4   E       NaN  0.950088\n5   F       NaN -0.151357\n\nleft2 = left.rename({\'key\':\'keyLeft\'}, axis=1)\nright2 = right.rename({\'key\':\'keyRight\'}, axis=1)\n\nleft2\n \n  keyLeft     value\n0       A  1.764052\n1       B  0.400157\n2       C  0.978738\n3       D  2.240893\n\nright2\n\n  keyRight     value\n0        B  1.867558\n1        D -0.977278\n2        E  0.950088\n3        F -0.151357\n\nleft2.merge(right2, left_on=\'keyLeft\', right_on=\'keyRight\', how=\'inner\')\n\n  keyLeft   value_x keyRight   value_y\n0       B  0.400157        B  1.867558\n1       D  2.240893        D -0.977278\n\nleft3 = left2.set_index(\'keyLeft\')\nleft3.merge(right2, left_index=True, right_on=\'keyRight\')\n    \n    value_x keyRight   value_y\n0  0.400157        B  1.867558\n1  2.240893        D -0.977278\n\nright3 = right.assign(newcol=np.arange(len(right)))\nright3\n  key     value  newcol\n0   B  1.867558       0\n1   D -0.977278       1\n2   E  0.950088       2\n3   F -0.151357       3\n\nleft.merge(right3[[\'key\', \'newcol\']], on=\'key\')\n\n  key     value  newcol\n0   B  0.400157       0\n1   D  2.240893       1\n\n# left[\'newcol\'] = left[\'key\'].map(right3.set_index(\'key\')[\'newcol\']))\nleft.assign(newcol=left[\'key\'].map(right3.set_index(\'key\')[\'newcol\']))\n\n  key     value  newcol\n0   A  1.764052     NaN\n1   B  0.400157     0.0\n2   C  0.978738     NaN\n3   D  2.240893     1.0\n\nleft.merge(right3[[\'key\', \'newcol\']], on=\'key\', how=\'left\')\n\n  key     value  newcol\n0   A  1.764052     NaN\n1   B  0.400157     0.0\n2   C  0.978738     NaN\n3   D  2.240893     1.0\n\nleft.merge(right, on=[\'key1\', \'key2\'] ...)\n\nleft.merge(right, left_on=[\'lkey1\', \'lkey2\'], right_on=[\'rkey1\', \'rkey2\'])\n'
'import pandas as pd\nimport glob\n\npath = r\'C:\\DRO\\DCL_rawdata_files\' # use your path\nall_files = glob.glob(path + "/*.csv")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)\n'
"result = result[(result['var']&gt;0.25) | (result['var']&lt;-0.25)]\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; x = pd.Series([1])\n&gt;&gt;&gt; bool(x)\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n&gt;&gt;&gt; x or x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n&gt;&gt;&gt; x and x\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n&gt;&gt;&gt; if x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n&gt;&gt;&gt; while x:\n...     print('fun')\nValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.logical_or(x, y)\n\n&gt;&gt;&gt; x | y\n\n&gt;&gt;&gt; np.logical_and(x, y)\n\n&gt;&gt;&gt; x &amp; y\n\n&gt;&gt;&gt; x = pd.Series([])\n&gt;&gt;&gt; x.empty\nTrue\n&gt;&gt;&gt; x = pd.Series([1])\n&gt;&gt;&gt; x.empty\nFalse\n\n&gt;&gt;&gt; x = pd.Series([100])\n&gt;&gt;&gt; (x &gt; 50).bool()\nTrue\n&gt;&gt;&gt; (x &lt; 50).bool()\nFalse\n\n&gt;&gt;&gt; x = pd.Series([100])\n&gt;&gt;&gt; x.item()\n100\n\n&gt;&gt;&gt; x = pd.Series([0, 1, 2])\n&gt;&gt;&gt; x.all()   # because one element is zero\nFalse\n&gt;&gt;&gt; x.any()   # because one (or more) elements are non-zero\nTrue\n"
"&gt;&gt;&gt; df = pd.DataFrame({'A': [a], 'B': [b]})\n&gt;&gt;&gt; df\n   A  B\n0  2  3\n\n&gt;&gt;&gt; df = pd.DataFrame({'A': a, 'B': b}, index=[0])\n&gt;&gt;&gt; df\n   A  B\n0  2  3\n"
"In [3]: sub_df\nOut[3]:\n          A         B\n2 -0.133653 -0.030854\n\nIn [4]: sub_df.iloc[0]\nOut[4]:\nA   -0.133653\nB   -0.030854\nName: 2, dtype: float64\n\nIn [5]: sub_df.iloc[0]['A']\nOut[5]: -0.13365288513107493\n"
"In [1]: df = pd.DataFrame(np.random.rand(5,2),index=range(0,10,2),columns=list('AB'))\n\nIn [2]: df\nOut[2]: \n          A         B\n0  1.068932 -0.794307\n2 -0.470056  1.192211\n4 -0.284561  0.756029\n6  1.037563 -0.267820\n8 -0.538478 -0.800654\n\nIn [5]: df.iloc[[2]]\nOut[5]: \n          A         B\n4 -0.284561  0.756029\n\nIn [6]: df.loc[[2]]\nOut[6]: \n          A         B\n2 -0.470056  1.192211\n"
"df.duplicated(['row', 'col']).any()\n\nTrue\n\ndf.pivot(index='row', columns='col', values='val0')\n\ndf.set_index(['row', 'col'])['val0'].unstack()\n\ndf.pivot_table(\n    values='val0', index='row', columns='col',\n    fill_value=0, aggfunc='mean')\n\ncol   col0   col1   col2   col3  col4\nrow                                  \nrow0  0.77  0.605  0.000  0.860  0.65\nrow2  0.13  0.000  0.395  0.500  0.25\nrow3  0.00  0.310  0.000  0.545  0.00\nrow4  0.00  0.100  0.395  0.760  0.24\n\ndf.groupby(['row', 'col'])['val0'].mean().unstack(fill_value=0)\n\npd.crosstab(\n    index=df['row'], columns=df['col'],\n    values=df['val0'], aggfunc='mean').fillna(0)\n\ndf.pivot_table(\n    values='val0', index='row', columns='col',\n    fill_value=0, aggfunc='sum')\n\ncol   col0  col1  col2  col3  col4\nrow                               \nrow0  0.77  1.21  0.00  0.86  0.65\nrow2  0.13  0.00  0.79  0.50  0.50\nrow3  0.00  0.31  0.00  1.09  0.00\nrow4  0.00  0.10  0.79  1.52  0.24\n\ndf.groupby(['row', 'col'])['val0'].sum().unstack(fill_value=0)\n\npd.crosstab(\n    index=df['row'], columns=df['col'],\n    values=df['val0'], aggfunc='sum').fillna(0)\n\ndf.pivot_table(\n    values='val0', index='row', columns='col',\n    fill_value=0, aggfunc=[np.size, np.mean])\n\n     size                      mean                           \ncol  col0 col1 col2 col3 col4  col0   col1   col2   col3  col4\nrow                                                           \nrow0    1    2    0    1    1  0.77  0.605  0.000  0.860  0.65\nrow2    1    0    2    1    2  0.13  0.000  0.395  0.500  0.25\nrow3    0    1    0    2    0  0.00  0.310  0.000  0.545  0.00\nrow4    0    1    2    2    1  0.00  0.100  0.395  0.760  0.24\n\ndf.groupby(['row', 'col'])['val0'].agg(['size', 'mean']).unstack(fill_value=0)\n\npd.crosstab(\n    index=df['row'], columns=df['col'],\n    values=df['val0'], aggfunc=[np.size, np.mean]).fillna(0, downcast='infer')\n\ndf.pivot_table(\n    values=['val0', 'val1'], index='row', columns='col',\n    fill_value=0, aggfunc='mean')\n\n      val0                             val1                          \ncol   col0   col1   col2   col3  col4  col0   col1  col2   col3  col4\nrow                                                                  \nrow0  0.77  0.605  0.000  0.860  0.65  0.01  0.745  0.00  0.010  0.02\nrow2  0.13  0.000  0.395  0.500  0.25  0.45  0.000  0.34  0.440  0.79\nrow3  0.00  0.310  0.000  0.545  0.00  0.00  0.230  0.00  0.075  0.00\nrow4  0.00  0.100  0.395  0.760  0.24  0.00  0.070  0.42  0.300  0.46\n\ndf.groupby(['row', 'col'])['val0', 'val1'].mean().unstack(fill_value=0)\n\ndf.pivot_table(\n    values='val0', index='row', columns=['item', 'col'],\n    fill_value=0, aggfunc='mean')\n\nitem item0             item1                         item2                   \ncol   col2  col3  col4  col0  col1  col2  col3  col4  col0   col1  col3  col4\nrow                                                                          \nrow0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.605  0.86  0.65\nrow2  0.35  0.00  0.37  0.00  0.00  0.44  0.00  0.00  0.13  0.000  0.50  0.13\nrow3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.000  0.28  0.00\nrow4  0.15  0.64  0.00  0.00  0.10  0.64  0.88  0.24  0.00  0.000  0.00  0.00\n\ndf.groupby(\n    ['row', 'item', 'col']\n)['val0'].mean().unstack(['item', 'col']).fillna(0).sort_index(1)\n\ndf.pivot_table(\n    values='val0', index=['key', 'row'], columns=['item', 'col'],\n    fill_value=0, aggfunc='mean')\n\nitem      item0             item1                         item2                  \ncol        col2  col3  col4  col0  col1  col2  col3  col4  col0  col1  col3  col4\nkey  row                                                                         \nkey0 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.86  0.00\n     row2  0.00  0.00  0.37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.50  0.00\n     row3  0.00  0.00  0.00  0.00  0.31  0.00  0.81  0.00  0.00  0.00  0.00  0.00\n     row4  0.15  0.64  0.00  0.00  0.00  0.00  0.00  0.24  0.00  0.00  0.00  0.00\nkey1 row0  0.00  0.00  0.00  0.77  0.00  0.00  0.00  0.00  0.00  0.81  0.00  0.65\n     row2  0.35  0.00  0.00  0.00  0.00  0.44  0.00  0.00  0.00  0.00  0.00  0.13\n     row3  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.28  0.00\n     row4  0.00  0.00  0.00  0.00  0.10  0.00  0.00  0.00  0.00  0.00  0.00  0.00\nkey2 row0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.40  0.00  0.00\n     row2  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.13  0.00  0.00  0.00\n     row4  0.00  0.00  0.00  0.00  0.00  0.64  0.88  0.00  0.00  0.00  0.00  0.00\n\ndf.groupby(\n    ['key', 'row', 'item', 'col']\n)['val0'].mean().unstack(['item', 'col']).fillna(0).sort_index(1)\n\ndf.set_index(\n    ['key', 'row', 'item', 'col']\n)['val0'].unstack(['item', 'col']).fillna(0).sort_index(1)\n\ndf.pivot_table(index='row', columns='col', fill_value=0, aggfunc='size')\n\n    col   col0  col1  col2  col3  col4\nrow                               \nrow0     1     2     0     1     1\nrow2     1     0     2     1     2\nrow3     0     1     0     2     0\nrow4     0     1     2     2     1\n\ndf.groupby(['row', 'col'])['val0'].size().unstack(fill_value=0)\n\npd.crosstab(df['row'], df['col'])\n\n# get integer factorization `i` and unique values `r`\n# for column `'row'`\ni, r = pd.factorize(df['row'].values)\n# get integer factorization `j` and unique values `c`\n# for column `'col'`\nj, c = pd.factorize(df['col'].values)\n# `n` will be the number of rows\n# `m` will be the number of columns\nn, m = r.size, c.size\n# `i * m + j` is a clever way of counting the \n# factorization bins assuming a flat array of length\n# `n * m`.  Which is why we subsequently reshape as `(n, m)`\nb = np.bincount(i * m + j, minlength=n * m).reshape(n, m)\n# BTW, whenever I read this, I think 'Bean, Rice, and Cheese'\npd.DataFrame(b, r, c)\n\n      col3  col2  col0  col1  col4\nrow3     2     0     0     1     0\nrow2     1     2     1     0     2\nrow0     1     0     1     2     1\nrow4     2     2     0     1     1\n\npd.get_dummies(df['row']).T.dot(pd.get_dummies(df['col']))\n\n      col0  col1  col2  col3  col4\nrow0     1     2     0     1     1\nrow2     1     0     2     1     2\nrow3     0     1     0     2     0\nrow4     0     1     2     2     1\n\ndf2.insert(0, 'count', df.groupby('A').cumcount())\ndf2\n\n   count  A   B\n0      0  a   0\n1      1  a  11\n2      2  a   2\n3      3  a  11\n4      0  b  10\n5      1  b  10\n6      2  b  14\n7      0  c   7\n\ndf2.pivot(*df)\n# df.pivot(index='count', columns='A', values='B')\n\nA         a     b    c\ncount                 \n0       0.0  10.0  7.0\n1      11.0  10.0  NaN\n2       2.0  14.0  NaN\n3      11.0   NaN  NaN\n\ndf.columns = df.columns.map('|'.join)\n\ndf.columns = df.columns.map('{0[0]}|{0[1]}'.format) \n"
"data = pd.read_csv('file1.csv', error_bad_lines=False)\n"
'&gt;&gt;&gt; df = pd.DataFrame({\'col2\': {0: \'a\', 1: 2, 2: np.nan}, \'col1\': {0: \'w\', 1: 1, 2: 2}})\n&gt;&gt;&gt; di = {1: "A", 2: "B"}\n&gt;&gt;&gt; df\n  col1 col2\n0    w    a\n1    1    2\n2    2  NaN\n&gt;&gt;&gt; df.replace({"col1": di})\n  col1 col2\n0    w    a\n1    A    2\n2    B  NaN\n'
'df = df.reset_index(drop=True)\n\ndf.reset_index(drop=True, inplace=True)\n'
"def label_race (row):\n   if row['eri_hispanic'] == 1 :\n      return 'Hispanic'\n   if row['eri_afr_amer'] + row['eri_asian'] + row['eri_hawaiian'] + row['eri_nat_amer'] + row['eri_white'] &gt; 1 :\n      return 'Two Or More'\n   if row['eri_nat_amer'] == 1 :\n      return 'A/I AK Native'\n   if row['eri_asian'] == 1:\n      return 'Asian'\n   if row['eri_afr_amer']  == 1:\n      return 'Black/AA'\n   if row['eri_hawaiian'] == 1:\n      return 'Haw/Pac Isl.'\n   if row['eri_white'] == 1:\n      return 'White'\n   return 'Other'\n\ndf.apply (lambda row: label_race(row), axis=1)\n\n0           White\n1        Hispanic\n2           White\n3           White\n4           Other\n5           White\n6     Two Or More\n7           White\n8    Haw/Pac Isl.\n9           White\n\ndf['race_label'] = df.apply (lambda row: label_race(row), axis=1)\n\n      lname   fname rno_cd  eri_afr_amer  eri_asian  eri_hawaiian   eri_hispanic  eri_nat_amer  eri_white rno_defined    race_label\n0      MOST    JEFF      E             0          0             0              0             0          1       White         White\n1    CRUISE     TOM      E             0          0             0              1             0          0       White      Hispanic\n2      DEPP  JOHNNY    NaN             0          0             0              0             0          1     Unknown         White\n3     DICAP     LEO    NaN             0          0             0              0             0          1     Unknown         White\n4    BRANDO  MARLON      E             0          0             0              0             0          0       White         Other\n5     HANKS     TOM    NaN             0          0             0              0             0          1     Unknown         White\n6    DENIRO  ROBERT      E             0          1             0              0             0          1       White   Two Or More\n7    PACINO      AL      E             0          0             0              0             0          1       White         White\n8  WILLIAMS   ROBIN      E             0          0             1              0             0          0       White  Haw/Pac Isl.\n9  EASTWOOD   CLINT      E             0          0             0              0             0          1       White         White\n"
"dtype={'user_id': int}\n\nimport pandas as pd\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n\n\ncsvdata = &quot;&quot;&quot;user_id,username\n1,Alice\n3,Bob\nfoobar,Caesar&quot;&quot;&quot;\nsio = StringIO(csvdata)\npd.read_csv(sio, dtype={&quot;user_id&quot;: int, &quot;username&quot;: &quot;string&quot;})\n\nValueError: invalid literal for long() with base 10: 'foobar'\n"
"df[df['column name'].map(len) &lt; 2]\n"
"df.columns = df.columns.get_level_values(0)\n\ndf.columns = [' '.join(col).strip() for col in df.columns.values]\n\nIn [11]: [' '.join(col).strip() for col in df.columns.values]\nOut[11]: \n['USAF',\n 'WBAN',\n 'day',\n 'month',\n 's_CD sum',\n 's_CL sum',\n 's_CNT sum',\n 's_PC sum',\n 'tempf amax',\n 'tempf amin',\n 'year']\n"
'In [11]: df = pd.DataFrame(np.random.randn(100, 2))\n\nIn [12]: msk = np.random.rand(len(df)) &lt; 0.8\n\nIn [13]: train = df[msk]\n\nIn [14]: test = df[~msk]\n\nIn [15]: len(test)\nOut[15]: 21\n\nIn [16]: len(train)\nOut[16]: 79\n'
"# Using DataFrame.drop\ndf.drop(df.columns[[1, 2]], axis=1, inplace=True)\n\n# drop by Name\ndf1 = df1.drop(['B', 'C'], axis=1)\n\n# Select the ones you want\ndf1 = df[['a','d']]\n"
"In [31]: df_test.iloc[0]\nOut[31]: \nATime     1.2\nX         2.0\nY        15.0\nZ         2.0\nBtime     1.2\nC        12.0\nD        25.0\nE        12.0\nName: 0, dtype: float64\n\nIn [30]: df_test['Btime'].iloc[0]\nOut[30]: 1.2\n\ndf.iloc[0, df.columns.get_loc('Btime')] = x\n\ndf.loc[df.index[n], 'Btime'] = x\n\ndf.iloc[n, df.columns.get_loc('Btime')] = x\n\nIn [22]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\nIn [24]: df['bar'] = 100\nIn [25]: df['bar'].iloc[0] = 99\n/home/unutbu/data/binky/bin/ipython:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  self._setitem_with_indexer(indexer, value)\n\nIn [26]: df\nOut[26]: \n  foo  bar\n0   A   99  &lt;-- assignment succeeded\n2   B  100\n1   C  100\n\nIn [66]: df.iloc[0]['bar'] = 123\n/home/unutbu/data/binky/bin/ipython:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\nIn [67]: df\nOut[67]: \n  foo  bar\n0   A   99  &lt;-- assignment failed\n2   B  100\n1   C  100\n\nIn [1]: df = pd.DataFrame({'foo':list('ABC')}, index=[0,2,1])\n\nIn [2]: df\nOut[2]: \n  foo\n0   A\n2   B\n1   C\n\nIn [4]: df.ix[1, 'foo']\nOut[4]: 'C'\n"
"if df.empty:\n    print('DataFrame is empty!')\n"
"df.to_pickle(file_name)  # where to save it, usually as a .pkl\n\ndf = pd.read_pickle(file_name)\n\nimport pandas as pd\nstore = pd.HDFStore('store.h5')\n\nstore['df'] = df  # save it\nstore['df']  # load it\n"
"In [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [99]: df[(df.A == 1) &amp; (df.D == 6)]\nOut[99]:\n   A  B  C  D\nd  1  3  9  6\n\nIn [90]: def mask(df, key, value):\n   ....:     return df[df[key] == value]\n   ....:\n\nIn [92]: pandas.DataFrame.mask = mask\n\nIn [93]: df = pandas.DataFrame(np.random.randint(0, 10, (4,4)), index=list('abcd'), columns=list('ABCD'))\n\nIn [95]: df.ix['d','A'] = df.ix['a', 'A']\n\nIn [96]: df\nOut[96]:\n   A  B  C  D\na  1  4  9  1\nb  4  5  0  2\nc  5  5  1  0\nd  1  3  9  6\n\nIn [97]: df.mask('A', 1)\nOut[97]:\n   A  B  C  D\na  1  4  9  1\nd  1  3  9  6\n\nIn [98]: df.mask('A', 1).mask('D', 6)\nOut[98]:\n   A  B  C  D\nd  1  3  9  6\n"
"df['color'] = np.where(df['Set']=='Z', 'green', 'red')\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\ndf['color'] = np.where(df['Set']=='Z', 'green', 'red')\nprint(df)\n\n  Set Type  color\n0   Z    A  green\n1   Z    B  green\n2   X    B    red\n3   Y    C    red\n\ndf = pd.DataFrame({'Type':list('ABBC'), 'Set':list('ZZXY')})\nconditions = [\n    (df['Set'] == 'Z') &amp; (df['Type'] == 'A'),\n    (df['Set'] == 'Z') &amp; (df['Type'] == 'B'),\n    (df['Type'] == 'B')]\nchoices = ['yellow', 'blue', 'purple']\ndf['color'] = np.select(conditions, choices, default='black')\nprint(df)\n\n  Set Type   color\n0   Z    A  yellow\n1   Z    B    blue\n2   X    B  purple\n3   Y    C   black\n"
"df.loc[:, df.columns != 'b']\n\n          a         c         d\n0  0.561196  0.013768  0.772827\n1  0.882641  0.615396  0.075381\n2  0.368824  0.651378  0.397203\n3  0.788730  0.568099  0.869127\n"
"In [37]:\ndf = pd.DataFrame({'a':list('abssbab')})\ndf.groupby('a').count()\n\nOut[37]:\n\n   a\na   \na  2\nb  3\ns  2\n\n[3 rows x 1 columns]\n\nIn [38]:\ndf['a'].value_counts()\n\nOut[38]:\n\nb    3\na    2\ns    2\ndtype: int64\n\nIn [41]:\ndf['freq'] = df.groupby('a')['a'].transform('count')\ndf\n\nOut[41]:\n\n   a freq\n0  a    2\n1  b    3\n2  s    2\n3  s    2\n4  b    3\n5  a    2\n6  b    3\n\n[7 rows x 2 columns]\n"
"In [1]: df = pd.DataFrame( {'a':['A','A','B','B','B','C'], 'b':[1,2,5,5,4,6]})\n        df\n\nOut[1]: \n   a  b\n0  A  1\n1  A  2\n2  B  5\n3  B  5\n4  B  4\n5  C  6\n\nIn [2]: df.groupby('a')['b'].apply(list)\nOut[2]: \na\nA       [1, 2]\nB    [5, 5, 4]\nC          [6]\nName: b, dtype: object\n\nIn [3]: df1 = df.groupby('a')['b'].apply(list).reset_index(name='new')\n        df1\nOut[3]: \n   a        new\n0  A     [1, 2]\n1  B  [5, 5, 4]\n2  C        [6]\n"
"pd.DataFrame(d)\nValueError: If using all scalar values, you must must pass an index\n\nIn [11]: pd.DataFrame(d.items())  # or list(d.items()) in python 3\nOut[11]:\n             0    1\n0   2012-07-02  392\n1   2012-07-06  392\n2   2012-06-29  391\n3   2012-06-28  391\n...\n\nIn [12]: pd.DataFrame(d.items(), columns=['Date', 'DateValue'])\nOut[12]:\n          Date  DateValue\n0   2012-07-02        392\n1   2012-07-06        392\n2   2012-06-29        391\n\nIn [21]: s = pd.Series(d, name='DateValue')\nOut[21]:\n2012-06-08    388\n2012-06-09    388\n2012-06-10    388\n\nIn [22]: s.index.name = 'Date'\n\nIn [23]: s.reset_index()\nOut[23]:\n          Date  DateValue\n0   2012-06-08        388\n1   2012-06-09        388\n2   2012-06-10        388\n"
"if 'A' in df:\n\nif 'A' in df.columns:\n"
'for index, row in df.iterrows():\n\n    # do some logic here\n'
'df = df.reindex(sorted(df.columns), axis=1)\n'
'df[\'col\'] = pd.to_datetime(df[\'col\'])\n\nIn [11]: pd.to_datetime(pd.Series([\'05/23/2005\']))\nOut[11]:\n0   2005-05-23 00:00:00\ndtype: datetime64[ns]\n\nIn [12]: pd.to_datetime(pd.Series([\'05/23/2005\']), format="%m/%d/%Y")\nOut[12]:\n0   2005-05-23\ndtype: datetime64[ns]\n'
"a,b\n1,2\n2,3\n3,4\n4,5\n\ndf['a'] = df['a'].apply(lambda x: x + 1)\n\n   a  b\n0  2  2\n1  3  3\n2  4  4\n3  5  5\n"
"&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; dt = datetime.utcnow()\n&gt;&gt;&gt; dt\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n&gt;&gt;&gt; dt64 = np.datetime64(dt)\n&gt;&gt;&gt; ts = (dt64 - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n&gt;&gt;&gt; ts\n1354650685.3624549\n&gt;&gt;&gt; datetime.utcfromtimestamp(ts)\ndatetime.datetime(2012, 12, 4, 19, 51, 25, 362455)\n&gt;&gt;&gt; np.__version__\n'1.8.0.dev-7b75899'\n\n&gt;&gt;&gt; np.datetime64(datetime.utcnow()).astype(datetime)\ndatetime.datetime(2012, 12, 4, 13, 34, 52, 827542)\n\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; import numpy \n&gt;&gt;&gt; numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\ndatetime.datetime(2002, 6, 28, 0, 0)\n&gt;&gt;&gt; numpy.__version__\n'1.6.2' # current version available via pip install numpy\n\npip install git+https://github.com/numpy/numpy.git#egg=numpy-dev\n\n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; numpy.datetime64('2002-06-28T01:00:00.000000000+0100').astype(datetime)\n1025222400000000000L\n&gt;&gt;&gt; numpy.__version__\n'1.8.0.dev-7b75899'\n\n&gt;&gt;&gt; dt64.dtype\ndtype('&lt;M8[ns]')\n&gt;&gt;&gt; ns = 1e-9 # number of seconds in a nanosecond\n&gt;&gt;&gt; datetime.utcfromtimestamp(dt64.astype(int) * ns)\ndatetime.datetime(2002, 6, 28, 0, 0)\n\n&gt;&gt;&gt; dt64 = numpy.datetime64('2002-06-28T01:00:00.000000000+0100', 's')\n&gt;&gt;&gt; dt64.dtype\ndtype('&lt;M8[s]')\n&gt;&gt;&gt; datetime.utcfromtimestamp(dt64.astype(int))\ndatetime.datetime(2002, 6, 28, 0, 0)\n"
'&gt;&gt;&gt; pd.DataFrame(data=data[1:,1:],    # values\n...              index=data[1:,0],    # 1st column as index\n...              columns=data[0,1:])  # 1st row as the column names\n'
"df.index[df['BoolCol'] == True].tolist()\n\ndf.index[df['BoolCol']].tolist()\n\ndf = pd.DataFrame({'BoolCol': [True, False, False, True, True]},\n       index=[10,20,30,40,50])\n\nIn [53]: df\nOut[53]: \n   BoolCol\n10    True\n20   False\n30   False\n40    True\n50    True\n\n[5 rows x 1 columns]\n\nIn [54]: df.index[df['BoolCol']].tolist()\nOut[54]: [10, 40, 50]\n\nIn [56]: idx = df.index[df['BoolCol']]\n\nIn [57]: idx\nOut[57]: Int64Index([10, 40, 50], dtype='int64')\n\nIn [58]: df.loc[idx]\nOut[58]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n\nIn [55]: df.loc[df['BoolCol']]\nOut[55]: \n   BoolCol\n10    True\n40    True\n50    True\n\n[3 rows x 1 columns]\n\nIn [110]: np.flatnonzero(df['BoolCol'])\nOut[112]: array([0, 3, 4])\n\nIn [113]: df.iloc[np.flatnonzero(df['BoolCol'])]\nOut[113]: \n   BoolCol\n10    True\n40    True\n50    True\n"
"table.groupby('YEARMONTH').CLIENTCODE.nunique()\n\nIn [2]: table\nOut[2]: \n   CLIENTCODE  YEARMONTH\n0           1     201301\n1           1     201301\n2           2     201301\n3           1     201302\n4           2     201302\n5           2     201302\n6           3     201302\n\nIn [3]: table.groupby('YEARMONTH').CLIENTCODE.nunique()\nOut[3]: \nYEARMONTH\n201301       2\n201302       3\n"
"In [18]:\ndf.sort_values('2')\n\nOut[18]:\n        0          1     2\n4    85.6    January   1.0\n3    95.5   February   2.0\n7   104.8      March   3.0\n0   354.7      April   4.0\n8   283.5        May   5.0\n6   238.7       June   6.0\n5   152.0       July   7.0\n1    55.4     August   8.0\n11  212.7  September   9.0\n10  249.6    October  10.0\n9   278.8   November  11.0\n2   176.5   December  12.0\n"
'import sys\nif sys.version_info[0] &lt; 3: \n    from StringIO import StringIO\nelse:\n    from io import StringIO\n\nimport pandas as pd\n\nTESTDATA = StringIO("""col1;col2;col3\n    1;4.4;99\n    2;4.5;200\n    3;4.7;65\n    4;3.2;140\n    """)\n\ndf = pd.read_csv(TESTDATA, sep=";")\n'
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({"A": [1,2,3], "B": [2,3,4]})\n&gt;&gt;&gt; df\n   A  B\n0  1  2\n1  2  3\n2  3  4\n&gt;&gt;&gt; df["C"] = ""\n&gt;&gt;&gt; df["D"] = np.nan\n&gt;&gt;&gt; df\n   A  B C   D\n0  1  2   NaN\n1  2  3   NaN\n2  3  4   NaN\n'
"df.to_csv('my_csv.csv', mode='a', header=False)\n"
"In [1]: s1 = pd.Series([1, 2], index=['A', 'B'], name='s1')\n\nIn [2]: s2 = pd.Series([3, 4], index=['A', 'B'], name='s2')\n\nIn [3]: pd.concat([s1, s2], axis=1)\nOut[3]:\n   s1  s2\nA   1   3\nB   2   4\n\nIn [4]: pd.concat([s1, s2], axis=1).reset_index()\nOut[4]:\n  index  s1  s2\n0     A   1   3\n1     B   2   4\n"
"df.sort_values(['a', 'b'], ascending=[True, False])\n\ndf.sort(['a', 'b'], ascending=[True, False])\n\nIn [11]: df1 = pd.DataFrame(np.random.randint(1, 5, (10,2)), columns=['a','b'])\n\nIn [12]: df1.sort(['a', 'b'], ascending=[True, False])\nOut[12]:\n   a  b\n2  1  4\n7  1  3\n1  1  2\n3  1  2\n4  3  2\n6  4  4\n0  4  3\n9  4  3\n5  4  1\n8  4  1\n\ndf1 = df1.sort(['a', 'b'], ascending=[True, False])\n\ndf1.sort(['a', 'b'], ascending=[True, False], inplace=True)\n"
'+------------+---------+--------+\n|            |  A      |  B     |\n+------------+---------+---------\n|      0     | 0.626386| 1.52325|----axis=1-----&gt;\n+------------+---------+--------+\n             |         |\n             | axis=0  |\n             ↓         ↓\n'
"In [7]: df.index.name\nOut[7]: 'Index Title'\n\nIn [8]: df.index.name = 'foo'\n\nIn [9]: df.index.name\nOut[9]: 'foo'\n\nIn [10]: df\nOut[10]: \n         Column 1\nfoo              \nApples          1\nOranges         2\nPuppies         3\nDucks           4\n"
"df3 = df3[~df3.index.duplicated(keep='first')]\n\n&gt;&gt;&gt; %timeit df3.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n1000 loops, best of 3: 1.54 ms per loop\n\n&gt;&gt;&gt; %timeit df3.groupby(df3.index).first()\n1000 loops, best of 3: 580 µs per loop\n\n&gt;&gt;&gt; %timeit df3[~df3.index.duplicated(keep='first')]\n1000 loops, best of 3: 307 µs per loop\n\n&gt;&gt;&gt; %timeit df1.groupby(level=df1.index.names).last()\n1000 loops, best of 3: 771 µs per loop\n\n&gt;&gt;&gt; %timeit df1[~df1.index.duplicated(keep='last')]\n1000 loops, best of 3: 365 µs per loop\n"
"raw_data['Mycol'] =  pd.to_datetime(raw_data['Mycol'], format='%d%b%Y:%H:%M:%S.%f')\n"
'import pandas as pd\nfrom sklearn import preprocessing\n\nx = df.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)\n'
"# selects all rows and all columns beginning at 'foo' up to and including 'sat'\ndf.loc[:, 'foo':'sat']\n# foo bar quz ant cat sat\n\n# slice from 'foo' to 'cat' by every 2nd column\ndf.loc[:, 'foo':'cat':2]\n# foo quz cat\n\n# slice from the beginning to 'bar'\ndf.loc[:, :'bar']\n# foo bar\n\n# slice from 'quz' to the end by 3\ndf.loc[:, 'quz'::3]\n# quz sat\n\n# attempt from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar']\n# no columns returned\n\n# slice from 'sat' to 'bar'\ndf.loc[:, 'sat':'bar':-1]\nsat cat ant quz bar\n\n# slice notation is syntatic sugar for the slice function\n# slice from 'quz' to the end by 2 with slice function\ndf.loc[:, slice('quz',None, 2)]\n# quz cat dat\n\n# select specific columns with a list\n# select columns foo, bar and dat\ndf.loc[:, ['foo','bar','dat']]\n# foo bar dat\n\n# slice from 'w' to 'y' and 'foo' to 'ant' by 3\ndf.loc['w':'y', 'foo':'ant':3]\n#    foo ant\n# w\n# x\n# y\n"
'In [65]: df\nOut[65]: \n       one  two\none      1    4\ntwo      2    3\nthree    3    2\nfour     4    1\n\n\nIn [66]: df.drop(df.index[[1,3]])\nOut[66]: \n       one  two\none      1    4\nthree    3    2\n'
"In [1]: df\nOut[1]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n1  MM1  S1     n      2\n2  MM1  S3    cb      5\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n5  MM2  S4   dgd      1\n6  MM4  S2    rd      2\n7  MM4  S2    cb      2\n8  MM4  S2   uyi      7\n\nIn [2]: df.groupby(['Mt'], sort=False)['count'].max()\nOut[2]:\nMt\nS1     3\nS3     8\nS4    10\nS2     7\nName: count\n\nIn [3]: idx = df.groupby(['Mt'])['count'].transform(max) == df['count']\n\nIn [4]: df[idx]\nOut[4]:\n    Sp  Mt Value  count\n0  MM1  S1     a      3\n3  MM2  S3    mk      8\n4  MM2  S4    bg     10\n8  MM4  S2   uyi      7\n\nIn [5]: df['count_max'] = df.groupby(['Mt'])['count'].transform(max)\n\nIn [6]: df\nOut[6]:\n    Sp  Mt Value  count  count_max\n0  MM1  S1     a      3          3\n1  MM1  S1     n      2          3\n2  MM1  S3    cb      5          8\n3  MM2  S3    mk      8          8\n4  MM2  S4    bg     10         10\n5  MM2  S4   dgd      1         10\n6  MM4  S2    rd      2          7\n7  MM4  S2    cb      2          7\n8  MM4  S2   uyi      7          7\n"
'In [45]: df = DataFrame({"pear": [1,2,3], "apple": [2,3,4], "orange": [3,4,5]})\n\nIn [46]: df.columns\nOut[46]: Index([apple, orange, pear], dtype=object)\n\nIn [47]: df.columns.get_loc("pear")\nOut[47]: 2\n'
"In [76]: import pandas as pd\n\nIn [77]: pd.__version__\nOut[77]: '0.12.0-933-g281dc4e'\n\nIn [53]: pd.show_versions(as_json=False)\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 2.7.6.final.0\npython-bits: 64\nOS: Linux\nOS-release: 3.13.0-45-generic\nmachine: x86_64\nprocessor: x86_64\nbyteorder: little\nLC_ALL: None\nLANG: en_US.UTF-8\n\npandas: 0.15.2-113-g5531341\nnose: 1.3.1\nCython: 0.21.1\nnumpy: 1.8.2\nscipy: 0.14.0.dev-371b4ff\nstatsmodels: 0.6.0.dev-a738b4f\nIPython: 2.0.0-dev\nsphinx: 1.2.2\npatsy: 0.3.0\ndateutil: 1.5\npytz: 2012c\nbottleneck: None\ntables: 3.1.1\nnumexpr: 2.2.2\nmatplotlib: 1.4.2\nopenpyxl: None\nxlrd: 0.9.3\nxlwt: 0.7.5\nxlsxwriter: None\nlxml: 3.3.3\nbs4: 4.3.2\nhtml5lib: 0.999\nhttplib2: 0.8\napiclient: None\nrpy2: 2.5.5\nsqlalchemy: 0.9.8\npymysql: None\npsycopg2: 2.4.5 (dt dec mx pq3 ext)\n"
"In [1]: df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=['a', 'b', 'c']); df\n   A  B\na  1  4\nb  2  5\nc  3  6\n\nIn [2]: df.index.values\nOut[2]: array(['a', 'b', 'c'], dtype=object)\n\nIn [3]: df['A'].values\nOut[3]: Out[16]: array([1, 2, 3])\n\nIn [4]: df.index.tolist()\nOut[4]: ['a', 'b', 'c']\n"
'df = df.groupby(\'domain\')[\'ID\'].nunique()\n\nprint (df)\ndomain\n\'facebook.com\'    1\n\'google.com\'      1\n\'twitter.com\'     2\n\'vk.com\'          3\nName: ID, dtype: int64\n\ndf = df.ID.groupby([df.domain.str.strip("\'")]).nunique()\nprint (df)\ndomain\nfacebook.com    1\ngoogle.com      1\ntwitter.com     2\nvk.com          3\nName: ID, dtype: int64\n\ndf.groupby(df.domain.str.strip("\'"))[\'ID\'].nunique()\n\ndf = df.groupby(by=\'domain\', as_index=False).agg({\'ID\': pd.Series.nunique})\nprint(df)\n    domain  ID\n0       fb   1\n1      ggl   1\n2  twitter   2\n3       vk   3\n'
'df = pd.read_csv(file_path, header=None, usecols=[3,6])\n'
"In [119]:\n\ncommon = df1.merge(df2,on=['col1','col2'])\nprint(common)\ndf1[(~df1.col1.isin(common.col1))&amp;(~df1.col2.isin(common.col2))]\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\nOut[119]:\n   col1  col2\n3     4    13\n4     5    14\n\nIn [138]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[138]:\n   col1  col2\n3     4    13\n4     5    14\n\ndf2 = pd.DataFrame(data = {'col1' : [2, 3,4], 'col2' : [11, 12,13]})\n\nIn [140]:\n\ndf1[~df1.isin(df2)].dropna()\nOut[140]:\n   col1  col2\n0     1    10\n1     2    11\n2     3    12\n3     4    13\n4     5    14\n"
"pd.set_option('display.max_colwidth', -1)\n"
"df.groupby(['Fruit','Name']).sum()\n\nOut[31]: \n               Number\nFruit   Name         \nApples  Bob        16\n        Mike        9\n        Steve      10\nGrapes  Bob        35\n        Tom        87\n        Tony       15\nOranges Bob        67\n        Mike       57\n        Tom        15\n        Tony        1\n"
"df = pd.DataFrame(np.random.rand(4,4), columns=list('abcd'))\ndf['group'] = [0, 0, 1, 1]\ndf\n\n          a         b         c         d  group\n0  0.418500  0.030955  0.874869  0.145641      0\n1  0.446069  0.901153  0.095052  0.487040      0\n2  0.843026  0.936169  0.926090  0.041722      1\n3  0.635846  0.439175  0.828787  0.714123      1\n\ndf.groupby('group').agg({'a':['sum', 'max'], \n                         'b':'mean', \n                         'c':'sum', \n                         'd': lambda x: x.max() - x.min()})\n\n              a                   b         c         d\n            sum       max      mean       sum  &lt;lambda&gt;\ngroup                                                  \n0      0.864569  0.446069  0.466054  0.969921  0.341399\n1      1.478872  0.843026  0.687672  1.754877  0.672401\n\ndef max_min(x):\n    return x.max() - x.min()\n\nmax_min.__name__ = 'Max minus Min'\n\ndf.groupby('group').agg({'a':['sum', 'max'], \n                         'b':'mean', \n                         'c':'sum', \n                         'd': max_min})\n\n              a                   b         c             d\n            sum       max      mean       sum Max minus Min\ngroup                                                      \n0      0.864569  0.446069  0.466054  0.969921      0.341399\n1      1.478872  0.843026  0.687672  1.754877      0.672401\n\ndef f(x):\n    d = {}\n    d['a_sum'] = x['a'].sum()\n    d['a_max'] = x['a'].max()\n    d['b_mean'] = x['b'].mean()\n    d['c_d_prodsum'] = (x['c'] * x['d']).sum()\n    return pd.Series(d, index=['a_sum', 'a_max', 'b_mean', 'c_d_prodsum'])\n\ndf.groupby('group').apply(f)\n\n         a_sum     a_max    b_mean  c_d_prodsum\ngroup                                           \n0      0.864569  0.446069  0.466054     0.173711\n1      1.478872  0.843026  0.687672     0.630494\n\n    def f_mi(x):\n        d = []\n        d.append(x['a'].sum())\n        d.append(x['a'].max())\n        d.append(x['b'].mean())\n        d.append((x['c'] * x['d']).sum())\n        return pd.Series(d, index=[['a', 'a', 'b', 'c_d'], \n                                   ['sum', 'max', 'mean', 'prodsum']])\n\ndf.groupby('group').apply(f_mi)\n\n              a                   b       c_d\n            sum       max      mean   prodsum\ngroup                                        \n0      0.864569  0.446069  0.466054  0.173711\n1      1.478872  0.843026  0.687672  0.630494\n"
'In [7]: s = pd.Series([True, True, False, True])\n\nIn [8]: ~s\nOut[8]: \n0    False\n1    False\n2     True\n3    False\ndtype: bool\n\nIn [119]: s = pd.Series([True, True, False, True]*10000)\n\nIn [10]:  %timeit np.invert(s)\n10000 loops, best of 3: 91.8 µs per loop\n\nIn [11]: %timeit ~s\n10000 loops, best of 3: 73.5 µs per loop\n\nIn [12]: %timeit (-s)\n10000 loops, best of 3: 73.5 µs per loop\n'
"df= pd.DataFrame(range(5), columns=['a'])\ndf.a = df.a.astype(float)\ndf\n\nOut[33]:\n\n          a\n0 0.0000000\n1 1.0000000\n2 2.0000000\n3 3.0000000\n4 4.0000000\n\npd.options.display.float_format = '{:,.0f}'.format\ndf\n\nOut[35]:\n\n   a\n0  0\n1  1\n2  2\n3  3\n4  4\n"
'df.replace([np.inf, -np.inf], np.nan)\n\ndf.replace([np.inf, -np.inf], np.nan).dropna(subset=["col1", "col2"], how="all")\n\nIn [11]: df = pd.DataFrame([1, 2, np.inf, -np.inf])\n\nIn [12]: df.replace([np.inf, -np.inf], np.nan)\nOut[12]:\n    0\n0   1\n1   2\n2 NaN\n3 NaN\n'
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from random import randint\n&gt;&gt;&gt; df = pd.DataFrame({\'A\': [randint(1, 9) for x in range(10)],\n                   \'B\': [randint(1, 9)*10 for x in range(10)],\n                   \'C\': [randint(1, 9)*100 for x in range(10)]})\n&gt;&gt;&gt; df\n   A   B    C\n0  9  40  300\n1  9  70  700\n2  5  70  900\n3  8  80  900\n4  7  50  200\n5  9  30  900\n6  2  80  700\n7  2  80  400\n8  5  80  300\n9  7  70  800\n\n&gt;&gt;&gt; df["B"] &gt; 50\n0    False\n1     True\n2     True\n3     True\n4    False\n5    False\n6     True\n7     True\n8     True\n9     True\nName: B\n&gt;&gt;&gt; (df["B"] &gt; 50) &amp; (df["C"] == 900)\n0    False\n1    False\n2     True\n3     True\n4    False\n5    False\n6    False\n7    False\n8    False\n9    False\n\n&gt;&gt;&gt; df["A"][(df["B"] &gt; 50) &amp; (df["C"] == 900)]\n2    5\n3    8\nName: A, dtype: int64\n\n&gt;&gt;&gt; df.loc[(df["B"] &gt; 50) &amp; (df["C"] == 900), "A"]\n2    5\n3    8\nName: A, dtype: int64\n&gt;&gt;&gt; df.loc[(df["B"] &gt; 50) &amp; (df["C"] == 900), "A"].values\narray([5, 8], dtype=int64)\n&gt;&gt;&gt; df.loc[(df["B"] &gt; 50) &amp; (df["C"] == 900), "A"] *= 1000\n&gt;&gt;&gt; df\n      A   B    C\n0     9  40  300\n1     9  70  700\n2  5000  70  900\n3  8000  80  900\n4     7  50  200\n5     9  30  900\n6     2  80  700\n7     2  80  400\n8     5  80  300\n9     7  70  800\n'
"df['year'] = pd.DatetimeIndex(df['ArrivalDate']).year\ndf['month'] = pd.DatetimeIndex(df['ArrivalDate']).month\n\ndf['year'] = df['ArrivalDate'].dt.year\ndf['month'] = df['ArrivalDate'].dt.month\n"
"import numpy as np\ndf1 = df.replace(np.nan, '', regex=True)\n"
"df.merge(df.textcol.apply(lambda s: pd.Series({'feature1':s+1, 'feature2':s-1})), \n    left_index=True, right_index=True)\n\n    textcol  feature1  feature2\n0  0.772692  1.772692 -0.227308\n1  0.857210  1.857210 -0.142790\n2  0.065639  1.065639 -0.934361\n3  0.819160  1.819160 -0.180840\n4  0.088212  1.088212 -0.911788\n"
"In [55]: pd.concat([Series(row['var2'], row['var1'].split(','))              \n                    for _, row in a.iterrows()]).reset_index()\nOut[55]: \n  index  0\n0     a  1\n1     b  1\n2     c  1\n3     d  2\n4     e  2\n5     f  2\n"
"In [43]: df['Value'] = df.apply(lambda row: my_test(row['a'], row['c']), axis=1)\n\nIn [44]: df\nOut[44]:\n                    a    b         c     Value\n          0 -1.674308  foo  0.343801  0.044698\n          1 -2.163236  bar -2.046438 -0.116798\n          2 -0.199115  foo -0.458050 -0.199115\n          3  0.918646  bar -0.007185 -0.001006\n          4  1.336830  foo  0.534292  0.268245\n          5  0.976844  bar -0.773630 -0.570417\n\nIn [53]: def my_test2(row):\n....:     return row['a'] % row['c']\n....:     \n\nIn [54]: df['Value'] = df.apply(my_test2, axis=1)\n"
"import matplotlib.pyplot as plt\n\nplt.matshow(dataframe.corr())\nplt.show()\n\nf = plt.figure(figsize=(19, 15))\nplt.matshow(df.corr(), fignum=f.number)\nplt.xticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14, rotation=45)\nplt.yticks(range(df.select_dtypes(['number']).shape[1]), df.select_dtypes(['number']).columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16);\n"
"df['date'] = pd.to_datetime(df['date'])  \n\n#greater than the start date and smaller than the end date\nmask = (df['date'] &gt; start_date) &amp; (df['date'] &lt;= end_date)\n\ndf.loc[mask]\n\ndf = df.loc[mask]\n\n            0         1         2       date\n153  0.208875  0.727656  0.037787 2000-06-02\n154  0.750800  0.776498  0.237716 2000-06-03\n155  0.812008  0.127338  0.397240 2000-06-04\n156  0.639937  0.207359  0.533527 2000-06-05\n157  0.416998  0.845658  0.872826 2000-06-06\n158  0.440069  0.338690  0.847545 2000-06-07\n159  0.202354  0.624833  0.740254 2000-06-08\n160  0.465746  0.080888  0.155452 2000-06-09\n161  0.858232  0.190321  0.432574 2000-06-10\n\n                   0         1         2\ndate                                    \n2000-06-01  0.040457  0.326594  0.492136    # &lt;- includes start_date\n2000-06-02  0.279323  0.877446  0.464523\n2000-06-03  0.328068  0.837669  0.608559\n2000-06-04  0.107959  0.678297  0.517435\n2000-06-05  0.131555  0.418380  0.025725\n2000-06-06  0.999961  0.619517  0.206108\n2000-06-07  0.129270  0.024533  0.154769\n2000-06-08  0.441010  0.741781  0.470402\n2000-06-09  0.682101  0.375660  0.009916\n2000-06-10  0.754488  0.352293  0.339337\n"
'&gt;&gt;&gt; df = pd.DataFrame()\n&gt;&gt;&gt; data = pd.DataFrame({"A": range(3)})\n&gt;&gt;&gt; df.append(data)\n   A\n0  0\n1  1\n2  2\n\n&gt;&gt;&gt; df\nEmpty DataFrame\nColumns: []\nIndex: []\n&gt;&gt;&gt; df = df.append(data)\n&gt;&gt;&gt; df\n   A\n0  0\n1  1\n2  2\n'
"import pandas as pd\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]}).set_index('key')\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]}).set_index('key')\nleft.join(right, lsuffix='_l', rsuffix='_r')\n\n     val_l  val_r\nkey            \nfoo      1      4\nbar      2      5\n\nleft = pd.DataFrame({'key': ['foo', 'bar'], 'val': [1, 2]})\nright = pd.DataFrame({'key': ['foo', 'bar'], 'val': [4, 5]})\nleft.merge(right, on=('key'), suffixes=('_l', '_r'))\n\n   key  val_l  val_r\n0  foo      1      4\n1  bar      2      5\n"
"&gt;&gt;&gt; import pandas\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; df = pandas.DataFrame(np.random.randn(5,3),columns=['A','B','C'])\n&gt;&gt;&gt; df\n          A         B         C\n0  1.232853 -1.979459 -0.573626\n1  0.140767  0.394940  1.068890\n2  0.742023  1.343977 -0.579745\n3  2.125299 -0.649328 -0.211692\n4 -0.187253  1.908618 -1.862934\n&gt;&gt;&gt; df['A'].argmax()\n3\n&gt;&gt;&gt; df['B'].argmax()\n4\n&gt;&gt;&gt; df['C'].argmax()\n1\n\nIn [19]: dfrm\nOut[19]: \n          A         B         C\na  0.143693  0.653810  0.586007\nb  0.623582  0.312903  0.919076\nc  0.165438  0.889809  0.000967\nd  0.308245  0.787776  0.571195\ne  0.870068  0.935626  0.606911\nf  0.037602  0.855193  0.728495\ng  0.605366  0.338105  0.696460\nh  0.000000  0.090814  0.963927\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n\nIn [20]: dfrm['A'].idxmax()\nOut[20]: 'i'\n\nIn [21]: dfrm.iloc[dfrm['A'].idxmax()]  # .ix instead of .iloc in older versions of pandas\nOut[21]: \n          A         B         C\ni  0.688343  0.188468  0.352213\ni  0.879000  0.105039  0.900260\n"
"new = old[['A', 'C', 'D']].copy()\n\nnew = old.filter(['A','B','D'], axis=1)\n\nnew = old.drop('B', axis=1)\n"
"df = DataFrame({'x': [1,2]})\ndf_sub = df[0:1]\ndf_sub.x = -1\nprint(df)\n\nx\n0 -1\n1  2\n\ndf_sub_copy = df[0:1].copy()\ndf_sub_copy.x = -1\n"
"In [79]:\n\ndf\nOut[79]:\n         Date, Open, High,  Low,  Close\n0  01-01-2015,  565,  600,  400,    450\nIn [80]:\n\ndf['Name'] = 'abc'\ndf\nOut[80]:\n         Date, Open, High,  Low,  Close Name\n0  01-01-2015,  565,  600,  400,    450  abc\n"
"df.insert(loc, column, value)\n\ndf = pd.DataFrame({'B': [1, 2, 3], 'C': [4, 5, 6]})\n\ndf\nOut: \n   B  C\n0  1  4\n1  2  5\n2  3  6\n\nidx = 0\nnew_col = [7, 8, 9]  # can be a list, a Series, an array or a scalar   \ndf.insert(loc=idx, column='A', value=new_col)\n\ndf\nOut: \n   A  B  C\n0  7  1  4\n1  8  2  5\n2  9  3  6\n"
"                            row\n    0       00000 UNITED STATES\n    1             01000 ALABAMA\n    2  01001 Autauga County, AL\n    3  01003 Baldwin County, AL\n    4  01005 Barbour County, AL\n\ndf = pd.DataFrame(df.row.str.split(' ',1).tolist(),\n                                 columns = ['flips','row'])\n\n   flips                 row\n0  00000       UNITED STATES\n1  01000             ALABAMA\n2  01001  Autauga County, AL\n3  01003  Baldwin County, AL\n4  01005  Barbour County, AL\n"
"In [1]: df = pd.DataFrame([[1, 2], [1, 3], [4, 6]], columns=['A', 'B'])\n\nIn [2]: df\nOut[2]: \n   A  B\n0  1  2\n1  1  3\n2  4  6\n\nIn [3]: iwantthis\nOut[3]: \n   A  B\n0  1  5\n1  4  6\n\nIn [4]: df.groupby('A').sum()\nOut[4]: \n   B\nA   \n1  5\n4  6\n\ndf['date'] = pd.to_datetime(df['date']) # this column ought to be date..\n\nIn [11]: df\nOut[11]:\n     C\nA B   \n1 2  3\n  2  6\n\nIn [12]: df = pd.DataFrame([[1, 2, 3], [1, 2, 6]], columns=['A', 'B', 'C']).set_index(['A', 'B'])\n\nIn [13]: df\nOut[13]: \n     C\nA B   \n1 2  3\n  2  6\n\n   B\nA   \n1  1\n5  0\n\ndf = pd.read_csv('my_secret_file.csv')  # ideally with lots of parsing options\n"
'ax = df2.plot(lw=2, colormap=\'jet\', marker=\'.\', markersize=10, title=\'Video streaming dropout by category\')\nax.set_xlabel("x label")\nax.set_ylabel("y label")\n'
"xls = pd.ExcelFile('path_to_file.xls')\ndf1 = pd.read_excel(xls, 'Sheet1')\ndf2 = pd.read_excel(xls, 'Sheet2')\n"
'total_rows[\'ColumnID\'] = total_rows[\'ColumnID\'].astype(str)\n\nIn [11]: df = pd.DataFrame([[\'A\', 2], [\'A\', 4], [\'B\', 6]])\n\nIn [12]: df.to_json()\nOut[12]: \'{"0":{"0":"A","1":"A","2":"B"},"1":{"0":2,"1":4,"2":6}}\'\n\nIn [13]: df[0].to_json()\nOut[13]: \'{"0":"A","1":"A","2":"B"}\'\n'
"&gt;&gt;&gt; df.set_index('ID').T.to_dict('list')\n{'p': [1, 3, 2], 'q': [4, 3, 2], 'r': [4, 0, 9]}\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': ['red', 'yellow', 'blue'], 'b': [0.5, 0.25, 0.125]})\n&gt;&gt;&gt; df\n        a      b\n0     red  0.500\n1  yellow  0.250\n2    blue  0.125\n\n&gt;&gt;&gt; df.to_dict('dict')\n{'a': {0: 'red', 1: 'yellow', 2: 'blue'}, \n 'b': {0: 0.5, 1: 0.25, 2: 0.125}}\n\n&gt;&gt;&gt; df.to_dict('list')\n{'a': ['red', 'yellow', 'blue'], \n 'b': [0.5, 0.25, 0.125]}\n\n&gt;&gt;&gt; df.to_dict('series')\n{'a': 0       red\n      1    yellow\n      2      blue\n      Name: a, dtype: object, \n\n 'b': 0    0.500\n      1    0.250\n      2    0.125\n      Name: b, dtype: float64}\n\n&gt;&gt;&gt; df.to_dict('split')\n{'columns': ['a', 'b'],\n 'data': [['red', 0.5], ['yellow', 0.25], ['blue', 0.125]],\n 'index': [0, 1, 2]}\n\n&gt;&gt;&gt; df.to_dict('records')\n[{'a': 'red', 'b': 0.5}, \n {'a': 'yellow', 'b': 0.25}, \n {'a': 'blue', 'b': 0.125}]\n\n&gt;&gt;&gt; df.to_dict('index')\n{0: {'a': 'red', 'b': 0.5},\n 1: {'a': 'yellow', 'b': 0.25},\n 2: {'a': 'blue', 'b': 0.125}}\n"
'nms.dropna(thresh=2)\n\nIn [87]:\n\nnms\nOut[87]:\n  movie    name  rating\n0   thg    John       3\n1   thg     NaN       4\n3   mol  Graham     NaN\n4   lob     NaN     NaN\n5   lob     NaN     NaN\n\n[5 rows x 3 columns]\nIn [89]:\n\nnms = nms.dropna(thresh=2)\nIn [90]:\n\nnms[nms.name.notnull()]\nOut[90]:\n  movie    name  rating\n0   thg    John       3\n3   mol  Graham     NaN\n\n[2 rows x 3 columns]\n\nnms[nms.name.notnull()]\n\nIn [4]:\nnms.dropna(thresh=2)\n\nOut[4]:\n  movie    name  rating\n0   thg    John     3.0\n1   thg     NaN     4.0\n3   mol  Graham     NaN\n'
"import pandas as pd\n\ndfs = [df0, df1, df2, dfN]\n\ndf_final = reduce(lambda left,right: pd.merge(left,right,on='name'), dfs)\n\nfrom functools import reduce\n"
"new_df = pd.merge(A_df, B_df,  how='left', left_on=['A_c1','c2'], right_on = ['B_c1','c2'])\n"
"data.rename(columns={'gdp':'log(gdp)'}, inplace=True)\n"
"In [27]: df=df.rename(columns = {'two':'new_name'})\n\nIn [28]: df\nOut[28]: \n  one three  new_name\n0    1     a         9\n1    2     b         8\n2    3     c         7\n3    4     d         6\n4    5     e         5\n"
'In [27]: df \nOut[27]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3       NaN -2.027325  1.533582\n4       NaN       NaN  0.461821\n5 -0.788073       NaN       NaN\n6 -0.916080 -0.612343       NaN\n7 -0.887858  1.033826       NaN\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n\nIn [28]: df.mean()\nOut[28]: \nA   -0.151121\nB   -0.231291\nC   -0.530307\ndtype: float64\n\nIn [29]: df.fillna(df.mean())\nOut[29]: \n          A         B         C\n0 -0.166919  0.979728 -0.632955\n1 -0.297953 -0.912674 -1.365463\n2 -0.120211 -0.540679 -0.680481\n3 -0.151121 -2.027325  1.533582\n4 -0.151121 -0.231291  0.461821\n5 -0.788073 -0.231291 -0.530307\n6 -0.916080 -0.612343 -0.530307\n7 -0.887858  1.033826 -0.530307\n8  1.948430  1.025011 -2.982224\n9  0.019698 -0.795876 -0.046431\n'
'df = df.iloc[3:]\n'
"In [60]: df_agg = df.groupby(['job','source']).agg({'count':sum})\n\nIn [63]: g = df_agg['count'].groupby('job', group_keys=False)\n\nIn [64]: res = g.apply(lambda x: x.sort_values(ascending=False).head(3))\n\nIn [65]: g.nlargest(3)\nOut[65]:\njob     source\nmarket  A         5\n        D         4\n        B         3\nsales   E         7\n        C         6\n        B         4\ndtype: int64\n\ndf_agg['count'].groupby('job', group_keys=False).nlargest(3)\n"
"In [11]: s = pd.Series(list('abc'))\n\nIn [12]: s\nOut[12]: \n0    a\n1    b\n2    c\ndtype: object\n\nIn [13]: 1 in s\nOut[13]: True\n\nIn [14]: 'a' in s\nOut[14]: False\n\nIn [21]: s.unique()\nOut[21]: array(['a', 'b', 'c'], dtype=object)\n\nIn [22]: 'a' in s.unique()\nOut[22]: True\n\nIn [23]: set(s)\nOut[23]: {'a', 'b', 'c'}\n\nIn [24]: 'a' in set(s)\nOut[24]: True\n\nIn [31]: s.values\nOut[31]: array(['a', 'b', 'c'], dtype=object)\n\nIn [32]: 'a' in s.values\nOut[32]: True\n"
'import pandas as pd\ndf = pd.DataFrame({"pear": [1,2,3], "apple": [2,3,4], "orange": [3,4,5]})\n\nlen(df.columns)\n3\n'
'for column in df:\n    print(df[column])\n'
"In [20]: timeit df.T.to_dict().values()\n1000 loops, best of 3: 395 µs per loop\n\nIn [21]: timeit df.to_dict('records')\n10000 loops, best of 3: 53 µs per loop\n\nIn [1]: df\nOut[1]:\n   customer  item1   item2   item3\n0         1  apple    milk  tomato\n1         2  water  orange  potato\n2         3  juice   mango   chips\n\nIn [2]: df.T.to_dict().values()\nOut[2]:\n[{'customer': 1.0, 'item1': 'apple', 'item2': 'milk', 'item3': 'tomato'},\n {'customer': 2.0, 'item1': 'water', 'item2': 'orange', 'item3': 'potato'},\n {'customer': 3.0, 'item1': 'juice', 'item2': 'mango', 'item3': 'chips'}]\n"
'&gt;&gt;&gt; df = pd.DataFrame([[1, 2.3456, \'c\', \'d\', 78]], columns=list("ABCDE"))\n&gt;&gt;&gt; df\n   A       B  C  D   E\n0  1  2.3456  c  d  78\n\n[1 rows x 5 columns]\n&gt;&gt;&gt; df.dtypes\nA      int64\nB    float64\nC     object\nD     object\nE      int64\ndtype: object\n&gt;&gt;&gt; g = df.columns.to_series().groupby(df.dtypes).groups\n&gt;&gt;&gt; g\n{dtype(\'int64\'): [\'A\', \'E\'], dtype(\'float64\'): [\'B\'], dtype(\'O\'): [\'C\', \'D\']}\n&gt;&gt;&gt; {k.name: v for k, v in g.items()}\n{\'object\': [\'C\', \'D\'], \'int64\': [\'A\', \'E\'], \'float64\': [\'B\']}\n'
"import pandas as pd\nlst1 = range(100)\nlst2 = range(100)\nlst3 = range(100)\npercentile_list = pd.DataFrame(\n    {'lst1Title': lst1,\n     'lst2Title': lst2,\n     'lst3Title': lst3\n    })\n\npercentile_list\n    lst1Title  lst2Title  lst3Title\n0          0         0         0\n1          1         1         1\n2          2         2         2\n3          3         3         3\n4          4         4         4\n5          5         5         5\n6          6         6         6\n...\n\nimport numpy as np\npercentile_list = pd.DataFrame(np.column_stack([lst1, lst2, lst3]), \n                               columns=['lst1Title', 'lst2Title', 'lst3Title'])\n"
"pd.merge(df1, df2, left_index=True, right_index=True)\n\ndf1.join(df2)\n\npd.concat([df1, df2], axis=1)\n\ndf1 = pd.DataFrame({'a':range(6),\n                    'b':[5,3,6,9,2,4]}, index=list('abcdef'))\n\nprint (df1)\n   a  b\na  0  5\nb  1  3\nc  2  6\nd  3  9\ne  4  2\nf  5  4\n\ndf2 = pd.DataFrame({'c':range(4),\n                    'd':[10,20,30, 40]}, index=list('abhi'))\n\nprint (df2)\n   c   d\na  0  10\nb  1  20\nh  2  30\ni  3  40\n\n#default inner join\ndf3 = pd.merge(df1, df2, left_index=True, right_index=True)\nprint (df3)\n   a  b  c   d\na  0  5  0  10\nb  1  3  1  20\n\n#default left join\ndf4 = df1.join(df2)\nprint (df4)\n   a  b    c     d\na  0  5  0.0  10.0\nb  1  3  1.0  20.0\nc  2  6  NaN   NaN\nd  3  9  NaN   NaN\ne  4  2  NaN   NaN\nf  5  4  NaN   NaN\n\n#default outer join\ndf5 = pd.concat([df1, df2], axis=1)\nprint (df5)\n     a    b    c     d\na  0.0  5.0  0.0  10.0\nb  1.0  3.0  1.0  20.0\nc  2.0  6.0  NaN   NaN\nd  3.0  9.0  NaN   NaN\ne  4.0  2.0  NaN   NaN\nf  5.0  4.0  NaN   NaN\nh  NaN  NaN  2.0  30.0\ni  NaN  NaN  3.0  40.0\n"
'In [10]: df.drop_duplicates(subset=\'A\', keep="last")\nOut[10]: \n   A   B\n1  1  20\n3  2  40\n4  3  10\n\nIn [12]: df.groupby(\'A\', group_keys=False).apply(lambda x: x.loc[x.B.idxmax()])\nOut[12]: \n   A   B\nA       \n1  1  20\n2  2  40\n3  3  10\n'
'B         business day frequency\nC         custom business day frequency (experimental)\nD         calendar day frequency\nW         weekly frequency\nM         month end frequency\nSM        semi-month end frequency (15th and end of month)\nBM        business month end frequency\nCBM       custom business month end frequency\nMS        month start frequency\nSMS       semi-month start frequency (1st and 15th)\nBMS       business month start frequency\nCBMS      custom business month start frequency\nQ         quarter end frequency\nBQ        business quarter endfrequency\nQS        quarter start frequency\nBQS       business quarter start frequency\nA         year end frequency\nBA, BY    business year end frequency\nAS, YS    year start frequency\nBAS, BYS  business year start frequency\nBH        business hour frequency\nH         hourly frequency\nT, min    minutely frequency\nS         secondly frequency\nL, ms     milliseconds\nU, us     microseconds\nN         nanoseconds\n'
'Cov = pd.read_csv("path/to/file.txt", \n                  sep=\'\\t\', \n                  names=["Sequence", "Start", "End", "Coverage"])\n'
"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n# from tqdm.auto import tqdm  # for notebooks\n\ndf = pd.DataFrame(np.random.randint(0, int(1e8), (10000, 1000)))\n\n# Create and register a new `tqdm` instance with `pandas`\n# (can use tqdm_gui, optional kwargs, etc.)\ntqdm.pandas()\n\n# Now you can use `progress_apply` instead of `apply`\ndf.groupby(0).progress_apply(lambda x: x**2)\n\ndf_users.groupby(['userID', 'requestDate']).apply(feature_rollup)\n\nfrom tqdm import tqdm\ntqdm.pandas()\ndf_users.groupby(['userID', 'requestDate']).progress_apply(feature_rollup)\n\nfrom tqdm import tqdm, tqdm_pandas\ntqdm_pandas(tqdm())\n"
"         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n\ndf.loc[['a']]\n\nTypeError: Expected tuple, got str\n\ndf.xs('a', level=0, axis=0, drop_level=False)\n# df.xs('a', drop_level=False)\n\ndf.query(&quot;one == 'a'&quot;)\n\ndf[df.index.get_level_values('one') == 'a']\n# If your levels are unnamed, or if you need to select by position (not label),\n# df[df.index.get_level_values(0) == 'a']\n\n     col\ntwo     \nt      0\nu      1\nv      2\nw      3\n\ndf.loc['a'] # Notice the single string argument instead the list.\n\ndf.xs('a', level=0, axis=0, drop_level=True)\n# df.xs('a')\n\nv = df.loc[['a']]\nprint(v)\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n\nprint(v.index)\nMultiIndex(levels=[['a', 'b', 'c', 'd'], ['t', 'u', 'v', 'w']],\n           labels=[[0, 0, 0, 0], [0, 1, 2, 3]],\n           names=['one', 'two'])\n\nv.index = v.index.remove_unused_levels()\n\nprint(v.index)\nMultiIndex(levels=[['a'], ['t', 'u', 'v', 'w']],\n           labels=[[0, 0, 0, 0], [0, 1, 2, 3]],\n           names=['one', 'two'])\n\n         col\none two     \na   t      0\nb   t      4\n    t      8\nd   t     12\n\ndf.loc[(slice(None), 't'), :]\n\nidx = pd.IndexSlice\ndf.loc[idx[:, 't'], :]\n\ndf.loc(axis=0)[pd.IndexSlice[:, 't']]\n\ndf.xs('t', axis=0, level=1, drop_level=False)\n\ndf.query(&quot;two == 't'&quot;)\n# Or, if the first level has no name, \n# df.query(&quot;ilevel_1 == 't'&quot;) \n\ndf[df.index.get_level_values('two') == 't']\n# Or, to perform selection by position/integer,\n# df[df.index.get_level_values(1) == 't']\n\n         col\none two     \nb   t      4\n    u      5\n    v      6\n    w      7\n    t      8\nd   w     11\n    t     12\n    u     13\n    v     14\n    w     15\n\ndf.loc[['b', 'd']]\n\nitems = ['b', 'd']\ndf.query(&quot;one in @items&quot;)\n# df.query(&quot;one == @items&quot;, parser='pandas')\n# df.query(&quot;one in ['b', 'd']&quot;)\n# df.query(&quot;one == ['b', 'd']&quot;, parser='pandas')\n\ndf[df.index.get_level_values(&quot;one&quot;).isin(['b', 'd'])]\n\n         col\none two     \na   t      0\n    w      3\nb   t      4\n    w      7\n    t      8\nd   w     11\n    t     12\n    w     15\n\ndf.loc[pd.IndexSlice[:, ['t', 'w']], :] \n\nitems = ['t', 'w']\ndf.query(&quot;two in @items&quot;)\n# df.query(&quot;two == @items&quot;, parser='pandas') \n# df.query(&quot;two in ['t', 'w']&quot;)\n# df.query(&quot;two == ['t', 'w']&quot;, parser='pandas')\n\ndf[df.index.get_level_values('two').isin(['t', 'w'])]\n\n         col\none two     \nc   u      9\n\ndf.loc[('c', 'u'), :]\n\ndf.loc[pd.IndexSlice[('c', 'u')]]\n\nPerformanceWarning: indexing past lexsort depth may impact performance.\n\ndf_sort = df.sort_index()\ndf_sort.loc[('c', 'u')]\n\ndf.xs(('c', 'u'))\n\ndf.query(&quot;one == 'c' and two == 'u'&quot;)\n\nm1 = (df.index.get_level_values('one') == 'c')\nm2 = (df.index.get_level_values('two') == 'u')\ndf[m1 &amp; m2]\n\n         col\none two     \nc   u      9\na   w      3\n\ndf.loc[[('c', 'u'), ('a', 'w')]]\n# df.loc[pd.IndexSlice[[('c', 'u'), ('a', 'w')]]]\n\ncses = [('c', 'u'), ('a', 'w')]\nlevels = ['one', 'two']\n# This is a useful check to make in advance.\nassert all(len(levels) == len(cs) for cs in cses) \n\nquery = '(' + ') or ('.join([\n    ' and '.join([f&quot;({l} == {repr(c)})&quot; for l, c in zip(levels, cs)]) \n    for cs in cses\n]) + ')'\n\nprint(query)\n# ((one == 'c') and (two == 'u')) or ((one == 'a') and (two == 'w'))\n\ndf.query(query)\n\ndf[df.index.droplevel(unused_level).isin([('c', 'u'), ('a', 'w')])]\n\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\nb   t      4\n    t      8\nd   t     12\n\npd.concat([\n    df.loc[['a'],:], df.loc[pd.IndexSlice[:, 't'],:]\n])\n\n         col\none two     \na   t      0\n    u      1\n    v      2\n    w      3\n    t      0   # Does this look right to you? No, it isn't!\nb   t      4\n    t      8\nd   t     12\n\nv = pd.concat([\n        df.loc[['a'],:], df.loc[pd.IndexSlice[:, 't'],:]\n])\nv[~v.index.duplicated()]\n\ndf.query(&quot;one == 'a' or two == 't'&quot;)\n\nm1 = (df.index.get_level_values('one') == 'a')\nm2 = (df.index.get_level_values('two') == 't')\ndf[m1 | m2] \n\n         col\none two     \na   u      1\n    v      2\nb   u      5\n    v      6\nd   w     11\n    w     15\n\nkeys = [('a', 'u'), ('a', 'v'), ('b', 'u'), ('b', 'v'), ('d', 'w')]\ndf.loc[keys, :]\n\npd.concat([\n     df.loc[(('a', 'b'), ('u', 'v')), :], \n     df.loc[('d', 'w'), :]\n   ], axis=0)\n\n         col\none two     \nb   7      4\n    9      5\nc   7     10\nd   6     11\n    8     12\n    8     13\n    6     15\n\ndf2.query(&quot;two &gt; 5&quot;)\n\ndf2[df2.index.get_level_values('two') &gt; 5]\n\nnp.random.seed(0)\nmux3 = pd.MultiIndex.from_product([\n        list('ABCD'), list('efgh')\n], names=['one','two'])\n\ndf3 = pd.DataFrame(np.random.choice(10, (3, len(mux))), columns=mux3)\nprint(df3)\n\none  A           B           C           D         \ntwo  e  f  g  h  e  f  g  h  e  f  g  h  e  f  g  h\n0    5  0  3  3  7  9  3  5  2  4  7  6  8  8  1  6\n1    7  7  8  1  5  9  8  9  4  3  0  3  5  0  2  3\n2    8  1  3  3  3  7  0  1  9  9  0  4  7  3  2  7\n\n df3.loc[:, ....] # Notice how we slice across the index with `:`. \n\n df3.loc[:, pd.IndexSlice[...]]\n\n df.loc[:, {condition}] \n\n df3.T.query(...).T\n"
"In [56]: df.groupby(['col5','col2']).size().reset_index().groupby('col2')[[0]].max()\nOut[56]: \n      0\ncol2   \nA     3\nB     2\nC     1\nD     3\n"
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; df = pd.DataFrame({"A": [10,20,30], "B": [20, 30, 10]})\n&gt;&gt;&gt; df[\'new_column\'] = np.multiply(df[\'A\'], df[\'B\'])\n&gt;&gt;&gt; df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n\n&gt;&gt;&gt; def fx(x, y):\n...     return x*y\n...\n&gt;&gt;&gt; df[\'new_column\'] = np.vectorize(fx)(df[\'A\'], df[\'B\'])\n&gt;&gt;&gt; df\n    A   B  new_column\n0  10  20         200\n1  20  30         600\n2  30  10         300\n'
"import pandas as pd\npd.options.display.float_format = '${:,.2f}'.format\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\nprint(df)\n\n        cost\nfoo  $123.46\nbar  $234.57\nbaz  $345.68\nquux $456.79\n\nimport pandas as pd\ndf = pd.DataFrame([123.4567, 234.5678, 345.6789, 456.7890],\n                  index=['foo','bar','baz','quux'],\n                  columns=['cost'])\ndf['foo'] = df['cost']\ndf['cost'] = df['cost'].map('${:,.2f}'.format)\nprint(df)\n\n         cost       foo\nfoo   $123.46  123.4567\nbar   $234.57  234.5678\nbaz   $345.68  345.6789\nquux  $456.79  456.7890\n"
'data = pd.read_csv(\'output_list.txt\', sep=" ", header=None)\ndata.columns = ["a", "b", "c", "etc."]\n'
"lst_col = 'samples'\n\nr = pd.DataFrame({\n      col:np.repeat(df[col].values, df[lst_col].str.len())\n      for col in df.columns.drop(lst_col)}\n    ).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns]\n\nIn [103]: r\nOut[103]:\n    samples  subject  trial_num\n0      0.10        1          1\n1     -0.20        1          1\n2      0.05        1          1\n3      0.25        1          2\n4      1.32        1          2\n5     -0.17        1          2\n6      0.64        1          3\n7     -0.22        1          3\n8     -0.71        1          3\n9     -0.03        2          1\n10    -0.65        2          1\n11     0.76        2          1\n12     1.77        2          2\n13     0.89        2          2\n14     0.65        2          2\n15    -0.98        2          3\n16     0.65        2          3\n17    -0.30        2          3\n\nIn [10]: np.repeat(df['trial_num'].values, df[lst_col].str.len())\nOut[10]: array([1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 2, 2, 2, 3, 3, 3], dtype=int64)\n\nIn [11]: pd.DataFrame({\n    ...:           col:np.repeat(df[col].values, df[lst_col].str.len())\n    ...:           for col in df.columns.drop(lst_col)}\n    ...:         )\nOut[11]:\n    trial_num  subject\n0           1        1\n1           1        1\n2           1        1\n3           2        1\n4           2        1\n5           2        1\n6           3        1\n..        ...      ...\n11          1        2\n12          2        2\n13          2        2\n14          2        2\n15          3        2\n16          3        2\n17          3        2\n\n[18 rows x 2 columns]\n\nIn [12]: np.concatenate(df[lst_col].values)\nOut[12]: array([-1.04, -0.58, -1.32,  0.82, -0.59, -0.34,  0.25,  2.09,  0.12,  0.83, -0.88,  0.68,  0.55, -0.56,  0.65, -0.04,  0.36, -0.31])\n\nIn [13]: pd.DataFrame({\n    ...:           col:np.repeat(df[col].values, df[lst_col].str.len())\n    ...:           for col in df.columns.drop(lst_col)}\n    ...:         ).assign(**{lst_col:np.concatenate(df[lst_col].values)})\nOut[13]:\n    trial_num  subject  samples\n0           1        1    -1.04\n1           1        1    -0.58\n2           1        1    -1.32\n3           2        1     0.82\n4           2        1    -0.59\n5           2        1    -0.34\n6           3        1     0.25\n..        ...      ...      ...\n11          1        2     0.68\n12          2        2     0.55\n13          2        2    -0.56\n14          2        2     0.65\n15          3        2    -0.04\n16          3        2     0.36\n17          3        2    -0.31\n\n[18 rows x 3 columns]\n"
"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'State':['Texas', 'Texas', 'Florida', 'Florida'], \n                   'a':[4,5,1,3], 'b':[6,10,3,11]})\n\n     State  a   b\n0    Texas  4   6\n1    Texas  5  10\n2  Florida  1   3\n3  Florida  3  11\n\ndef inspect(x):\n    print(type(x))\n    raise\n\ndf.groupby('State').apply(inspect)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRuntimeError\n\ndf.groupby('State').transform(inspect)\n&lt;class 'pandas.core.series.Series'&gt;\n&lt;class 'pandas.core.series.Series'&gt;\nRuntimeError\n\ndef subtract_two(x):\n    return x['a'] - x['b']\n\ndf.groupby('State').transform(subtract_two)\nKeyError: ('a', 'occurred at index a')\n\ndf.groupby('State').apply(subtract_two)\n\nState     \nFlorida  2   -2\n         3   -8\nTexas    0   -2\n         1   -5\ndtype: int64\n\nfrom IPython.display import display\ndef subtract_two(x):\n    display(x)\n    return x['a'] - x['b']\n\ndef return_three(x):\n    return np.array([1, 2, 3])\n\ndf.groupby('State').transform(return_three)\nValueError: transform must return a scalar value for each group\n\ndef rand_group_len(x):\n    return np.random.rand(len(x))\n\ndf.groupby('State').transform(rand_group_len)\n\n          a         b\n0  0.962070  0.151440\n1  0.440956  0.782176\n2  0.642218  0.483257\n3  0.056047  0.238208\n\ndef group_sum(x):\n    return x.sum()\n\ndf.groupby('State').transform(group_sum)\n\n   a   b\n0  9  16\n1  9  16\n2  4  14\n3  4  14\n"
'df = pd.DataFrame(table, columns=headers)\ndf\n\n   Heading1  Heading2\n0         1         2\n1         3         4\n'
'In [3]: df[df[\'ids\'].str.contains("ball")]\nOut[3]:\n     ids  vals\n0  aball     1\n1  bball     2\n3  fball     4\n'
"In [37]:\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))\npd.read_csv(io.StringIO(df.to_csv()))\n\nOut[37]:\n   Unnamed: 0         a         b         c\n0           0  0.109066 -1.112704 -0.545209\n1           1  0.447114  1.525341  0.317252\n2           2  0.507495  0.137863  0.886283\n3           3  1.452867  1.888363  1.168101\n4           4  0.901371 -0.704805  0.088335\n\nIn [38]:\npd.read_csv(io.StringIO(df.to_csv(index=False)))\n\nOut[38]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n\nIn [40]:\npd.read_csv(io.StringIO(df.to_csv()), index_col=0)\n\nOut[40]:\n          a         b         c\n0  0.109066 -1.112704 -0.545209\n1  0.447114  1.525341  0.317252\n2  0.507495  0.137863  0.886283\n3  1.452867  1.888363  1.168101\n4  0.901371 -0.704805  0.088335\n"
'In [4]: import pandas as pd\nIn [5]: df = pd.DataFrame(columns=[\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'G\'])\nIn [6]: df\nOut[6]:\nEmpty DataFrame\nColumns: [A, B, C, D, E, F, G]\nIndex: []\n\nIn [7]: df = pd.DataFrame(index=range(1,10))\nIn [8]: df\nOut[8]:\nEmpty DataFrame\nColumns: []\nIndex: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ndf = pd.DataFrame(columns=[\'A\',\'B\',\'C\',\'D\',\'E\',\'F\',\'G\'])\ndf.to_html(\'test.html\')\n\n&lt;table border="1" class="dataframe"&gt;\n  &lt;thead&gt;\n    &lt;tr style="text-align: right;"&gt;\n      &lt;th&gt;&lt;/th&gt;\n      &lt;th&gt;A&lt;/th&gt;\n      &lt;th&gt;B&lt;/th&gt;\n      &lt;th&gt;C&lt;/th&gt;\n      &lt;th&gt;D&lt;/th&gt;\n      &lt;th&gt;E&lt;/th&gt;\n      &lt;th&gt;F&lt;/th&gt;\n      &lt;th&gt;G&lt;/th&gt;\n    &lt;/tr&gt;\n  &lt;/thead&gt;\n  &lt;tbody&gt;\n  &lt;/tbody&gt;\n&lt;/table&gt;\n'
"&gt;&gt;&gt; df.groupby('id').head(2)\n       id  value\nid             \n1  0   1      1\n   1   1      2 \n2  3   2      1\n   4   2      2\n3  7   3      1\n4  8   4      1\n\n&gt;&gt;&gt; df.groupby('id').head(2).reset_index(drop=True)\n    id  value\n0   1      1\n1   1      2\n2   2      1\n3   2      2\n4   3      1\n5   4      1\n"
"In [21]: ne = (df1 != df2).any(1)\n\nIn [22]: ne\nOut[22]:\n0    False\n1     True\n2     True\ndtype: bool\n\nIn [23]: ne_stacked = (df1 != df2).stack()\n\nIn [24]: changed = ne_stacked[ne_stacked]\n\nIn [25]: changed.index.names = ['id', 'col']\n\nIn [26]: changed\nOut[26]:\nid  col\n1   score         True\n2   isEnrolled    True\n    Comment       True\ndtype: bool\n\nIn [27]: difference_locations = np.where(df1 != df2)\n\nIn [28]: changed_from = df1.values[difference_locations]\n\nIn [29]: changed_to = df2.values[difference_locations]\n\nIn [30]: pd.DataFrame({'from': changed_from, 'to': changed_to}, index=changed.index)\nOut[30]:\n               from           to\nid col\n1  score       1.11         1.21\n2  isEnrolled  True        False\n   Comment     None  On vacation\n"
"In [25]: pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nIn [28]: Series(np.random.randn(3))*1000000000\nOut[28]: \n0    -757322420.605\n1   -1436160588.997\n2   -1235116117.064\ndtype: float64\n\nIn [6]: Series(np.random.randn(3)).apply(lambda x: '%.3f' % x)\nOut[6]: \n0     0.026\n1    -0.482\n2    -0.694\ndtype: object\n"
'In [479]: df\nOut[479]: \n         ID  birthyear    weight\n0    619040       1962  0.123123\n1    600161       1963  0.981742\n2  25602033       1963  1.312312\n3    624870       1987  0.942120\n\nIn [480]: df["weight"].mean()\nOut[480]: 0.83982437500000007\n'
"# From Paul H\nimport numpy as np\nimport pandas as pd\nnp.random.seed(0)\ndf = pd.DataFrame({'state': ['CA', 'WA', 'CO', 'AZ'] * 3,\n                   'office_id': list(range(1, 7)) * 2,\n                   'sales': [np.random.randint(100000, 999999)\n                             for _ in range(12)]})\nstate_office = df.groupby(['state', 'office_id']).agg({'sales': 'sum'})\n# Change: groupby state_office and divide by sum\nstate_pcts = state_office.groupby(level=0).apply(lambda x:\n                                                 100 * x / float(x.sum()))\n\n                     sales\nstate office_id           \nAZ    2          16.981365\n      4          19.250033\n      6          63.768601\nCA    1          19.331879\n      3          33.858747\n      5          46.809373\nCO    1          36.851857\n      3          19.874290\n      5          43.273852\nWA    2          34.707233\n      4          35.511259\n      6          29.781508\n"
'import pandas as pd\ndf = pd.DataFrame({"A":["foo", "foo", "foo", "bar"], "B":[0,1,1,1], "C":["A","A","B","A"]})\ndf.drop_duplicates(subset=[\'A\', \'C\'], keep=False)\n'
"import pandas as pd\n\nd1 = {'teams': [['SF', 'NYG'],['SF', 'NYG'],['SF', 'NYG'],\n                ['SF', 'NYG'],['SF', 'NYG'],['SF', 'NYG'],['SF', 'NYG']]}\ndf2 = pd.DataFrame(d1)\nprint (df2)\n       teams\n0  [SF, NYG]\n1  [SF, NYG]\n2  [SF, NYG]\n3  [SF, NYG]\n4  [SF, NYG]\n5  [SF, NYG]\n6  [SF, NYG]\n\ndf2[['team1','team2']] = pd.DataFrame(df2.teams.tolist(), index= df2.index)\nprint (df2)\n       teams team1 team2\n0  [SF, NYG]    SF   NYG\n1  [SF, NYG]    SF   NYG\n2  [SF, NYG]    SF   NYG\n3  [SF, NYG]    SF   NYG\n4  [SF, NYG]    SF   NYG\n5  [SF, NYG]    SF   NYG\n6  [SF, NYG]    SF   NYG\n\ndf3 = pd.DataFrame(df2['teams'].to_list(), columns=['team1','team2'])\nprint (df3)\n  team1 team2\n0    SF   NYG\n1    SF   NYG\n2    SF   NYG\n3    SF   NYG\n4    SF   NYG\n5    SF   NYG\n6    SF   NYG\n\n#7k rows\ndf2 = pd.concat([df2]*1000).reset_index(drop=True)\n\nIn [121]: %timeit df2['teams'].apply(pd.Series)\n1.79 s ± 52.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [122]: %timeit pd.DataFrame(df2['teams'].to_list(), columns=['team1','team2'])\n1.63 ms ± 54.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
"xl = pd.ExcelFile('foo.xls')\n\nxl.sheet_names  # see all sheet names\n\nxl.parse(sheet_name)  # read a specific sheet to DataFrame\n"
'&gt;&gt;&gt; myseries[myseries == 7]\n3    7\ndtype: int64\n&gt;&gt;&gt; myseries[myseries == 7].index[0]\n3\n'
"(a['x']==1) and (a['y']==10)\n\nValueError: The truth value of an array is ambiguous. Use a.empty, a.any() or a.all().\n\n(a['x']==1) &amp; (a['y']==10)\n"
"grouped = df.groupby('A')\n\nfor name, group in grouped:\n    ...\n"
"In [71]: df\nOut[71]:\n     a    b  c\n0  NaN  7.0  0\n1  0.0  NaN  4\n2  2.0  NaN  4\n3  1.0  7.0  0\n4  1.0  3.0  9\n5  7.0  4.0  9\n6  2.0  6.0  9\n7  9.0  6.0  4\n8  3.0  0.0  9\n9  9.0  0.0  1\n\nIn [72]: df.isna().any()\nOut[72]:\na     True\nb     True\nc    False\ndtype: bool\n\nIn [74]: df.columns[df.isna().any()].tolist()\nOut[74]: ['a', 'b']\n\nIn [73]: df.loc[:, df.isna().any()]\nOut[73]:\n     a    b\n0  NaN  7.0\n1  0.0  NaN\n2  2.0  NaN\n3  1.0  7.0\n4  1.0  3.0\n5  7.0  4.0\n6  2.0  6.0\n7  9.0  6.0\n8  3.0  0.0\n9  9.0  0.0\n\nIn [97]: df\nOut[97]:\n     a    b  c\n0  NaN  7.0  0\n1  0.0  NaN  4\n2  2.0  NaN  4\n3  1.0  7.0  0\n4  1.0  3.0  9\n5  7.0  4.0  9\n6  2.0  6.0  9\n7  9.0  6.0  4\n8  3.0  0.0  9\n9  9.0  0.0  1\n\nIn [98]: pd.isnull(df).sum() &gt; 0\nOut[98]:\na     True\nb     True\nc    False\ndtype: bool\n\nIn [5]: df.isnull().any()\nOut[5]:\na     True\nb     True\nc    False\ndtype: bool\n\nIn [7]: df.columns[df.isnull().any()].tolist()\nOut[7]: ['a', 'b']\n\nIn [31]: df.loc[:, df.isnull().any()]\nOut[31]:\n     a    b\n0  NaN  7.0\n1  0.0  NaN\n2  2.0  NaN\n3  1.0  7.0\n4  1.0  3.0\n5  7.0  4.0\n6  2.0  6.0\n7  9.0  6.0\n8  3.0  0.0\n9  9.0  0.0\n"
"df = pd.DataFrame([\n    [-0.532681, 'foo', 0],\n    [1.490752, 'bar', 1],\n    [-1.387326, 'foo', 2],\n    [0.814772, 'baz', ' '],     \n    [-0.222552, '   ', 4],\n    [-1.176781,  'qux', '  '],         \n], columns='A B C'.split(), index=pd.date_range('2000-01-01','2000-01-06'))\n\n# replace field that's entirely space (or empty) with NaN\nprint(df.replace(r'^\\s*$', np.nan, regex=True))\n"
'new_df = df[~df["col"].str.contains(word)]\n\nnew_df = df[~df["col"].str.contains(word, na=False)]\n\nnew_df = df[df["col"].str.contains(word) == False]\n'
"In [21]: gb.get_group('foo')\nOut[21]: \n     A         B   C\n0  foo  1.624345   5\n2  foo -0.528172  11\n4  foo  0.865408  14\n\nIn [22]: gb[[&quot;A&quot;, &quot;B&quot;]].get_group(&quot;foo&quot;)\nOut[22]:\n     A         B\n0  foo  1.624345\n2  foo -0.528172\n4  foo  0.865408\n\nIn [23]: gb[&quot;C&quot;].get_group(&quot;foo&quot;)\nOut[23]:\n0     5\n2    11\n4    14\nName: C, dtype: int64\n"
"from datetime import datetime\ndateparse = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n\ndateparse = lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n\ndf = pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)\n"
'df.reset_index(inplace=True)  \n'
"&gt;&gt;&gt; df = pd.DataFrame([[1, 2, 3], [4, None, None], [None, None, 9]])\n&gt;&gt;&gt; df.fillna(method='ffill')\n   0  1  2\n0  1  2  3\n1  4  2  3\n2  4  2  9\n\ndf.fillna(method='ffill', inplace=True)\n"
'In [305]: train, validate, test = \\\n              np.split(df.sample(frac=1, random_state=42), \n                       [int(.6*len(df)), int(.8*len(df))])\n\nIn [306]: train\nOut[306]:\n          A         B         C         D         E\n0  0.046919  0.792216  0.206294  0.440346  0.038960\n2  0.301010  0.625697  0.604724  0.936968  0.870064\n1  0.642237  0.690403  0.813658  0.525379  0.396053\n9  0.488484  0.389640  0.599637  0.122919  0.106505\n8  0.842717  0.793315  0.554084  0.100361  0.367465\n7  0.185214  0.603661  0.217677  0.281780  0.938540\n\nIn [307]: validate\nOut[307]:\n          A         B         C         D         E\n5  0.806176  0.008896  0.362878  0.058903  0.026328\n6  0.145777  0.485765  0.589272  0.806329  0.703479\n\nIn [308]: test\nOut[308]:\n          A         B         C         D         E\n4  0.521640  0.332210  0.370177  0.859169  0.401087\n3  0.333348  0.964011  0.083498  0.670386  0.169619\n\nIn [45]: a = np.arange(1, 21)\n\nIn [46]: a\nOut[46]: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n\nIn [47]: np.split(a, [int(.8 * len(a)), int(.9 * len(a))])\nOut[47]:\n[array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n array([17, 18]),\n array([19, 20])]\n'
'import random\n\ndef some(x, n):\n    return x.ix[random.sample(x.index, n)]\n'
"from urllib2 import Request, urlopen\nimport json\n\nimport pandas as pd    \n\npath1 = '42.974049,-81.205203|42.974298,-81.195755'\nrequest=Request('http://maps.googleapis.com/maps/api/elevation/json?locations='+path1+'&amp;sensor=false')\nresponse = urlopen(request)\nelevations = response.read()\ndata = json.loads(elevations)\ndf = pd.json_normalize(data['results'])\n"
"In [91]:\n\ndf = pd.DataFrame({'a': [1,2,3], 'b': [2,3,4], 'c':['dd','ee','ff'], 'd':[5,9,1]})\ndf['e'] = df.sum(axis=1)\ndf\nOut[91]:\n   a  b   c  d   e\n0  1  2  dd  5   8\n1  2  3  ee  9  14\n2  3  4  ff  1   8\n\nIn [98]:\n\ncol_list= list(df)\ncol_list.remove('d')\ncol_list\nOut[98]:\n['a', 'b', 'c']\nIn [99]:\n\ndf['e'] = df[col_list].sum(axis=1)\ndf\nOut[99]:\n   a  b   c  d  e\n0  1  2  dd  5  3\n1  2  3  ee  9  5\n2  3  4  ff  1  7\n"
"In [41]:\ndf.loc[df['First Season'] &gt; 1990, 'First Season'] = 1\ndf\n\nOut[41]:\n                 Team  First Season  Total Games\n0      Dallas Cowboys          1960          894\n1       Chicago Bears          1920         1357\n2   Green Bay Packers          1921         1339\n3      Miami Dolphins          1966          792\n4    Baltimore Ravens             1          326\n5  San Franciso 49ers          1950         1003\n\ndf.loc[&lt;mask&gt;(here mask is generating the labels to index) , &lt;optional column(s)&gt; ]\n\nIn [43]:\ndf['First Season'] = (df['First Season'] &gt; 1990).astype(int)\ndf\n\nOut[43]:\n                 Team  First Season  Total Games\n0      Dallas Cowboys             0          894\n1       Chicago Bears             0         1357\n2   Green Bay Packers             0         1339\n3      Miami Dolphins             0          792\n4    Baltimore Ravens             1          326\n5  San Franciso 49ers             0         1003\n"
"In [11]: df.loc[df['col1'] &gt;= 1, 'col1']\nOut[11]: \n1    1\n2    2\nName: col1\n\nIn [12]: df[df['col1'] &gt;= 1]\nOut[12]: \n   col1  col2\n1     1    11\n2     2    12\n\nIn [13]: df[(df['col1'] &gt;= 1) &amp; (df['col1'] &lt;=1 )]\nOut[13]: \n   col1  col2\n1     1    11\n\nIn [14]: def b(x, col, op, n): \n             return op(x[col],n)\n\nIn [15]: def f(x, *b):\n             return x[(np.logical_and(*b))]\n\nIn [16]: b1 = b(df, 'col1', ge, 1)\n\nIn [17]: b2 = b(df, 'col1', le, 1)\n\nIn [18]: f(df, b1, b2)\nOut[18]: \n   col1  col2\n1     1    11\n\nIn [21]: df.query('col1 &lt;= 1 &amp; 1 &lt;= col1')\nOut[21]:\n   col1  col2\n1     1    11\n"
"pd.set_option('display.max_rows', 500)\n\npd.set_option('display.height', 500)\npd.set_option('display.max_rows', 500)\n\nfrom IPython.display import display\nwith pd.option_context('display.max_rows', 100, 'display.max_columns', 10):\n    display(df) #need display to show the dataframe when using with in jupyter\n    #some pandas stuff\n"
"In [9]: pd.Series(df.Letter.values,index=df.Position).to_dict()\nOut[9]: {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e'}\n\nIn [6]: df = pd.DataFrame(randint(0,10,10000).reshape(5000,2),columns=list('AB'))\n\nIn [7]: %timeit dict(zip(df.A,df.B))\n1000 loops, best of 3: 1.27 ms per loop\n\nIn [8]: %timeit pd.Series(df.A.values,index=df.B).to_dict()\n1000 loops, best of 3: 987 us per loop\n"
"&gt;&gt;&gt; df.groupby('id').first()\n     value\nid        \n1    first\n2    first\n3    first\n4   second\n5    first\n6    first\n7   fourth\n\n&gt;&gt;&gt; df.groupby('id').first().reset_index()\n   id   value\n0   1   first\n1   2   first\n2   3   first\n3   4  second\n4   5   first\n5   6   first\n6   7  fourth\n\n&gt;&gt;&gt; df.groupby('id').head(2).reset_index(drop=True)\n    id   value\n0    1   first\n1    1  second\n2    2   first\n3    2  second\n4    3   first\n5    3   third\n6    4  second\n7    4   fifth\n8    5   first\n9    6   first\n10   6  second\n11   7  fourth\n12   7   fifth\n"
"df[['a', 'b']] = df[['a','b']].fillna(value=0)\n\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  5.0  NaN\n2  3.0  0.0  7.0\n3  0.0  6.0  8.0\n"
"df.index.names = ['Date']\n\nIn [1]: df = pd.DataFrame([[1, 2, 3], [4, 5 ,6]], columns=list('ABC'))\n\nIn [2]: df\nOut[2]: \n   A  B  C\n0  1  2  3\n1  4  5  6\n\nIn [3]: df1 = df.set_index('A')\n\nIn [4]: df1\nOut[4]: \n   B  C\nA      \n1  2  3\n4  5  6\n\nIn [5]: df1.rename(index={1: 'a'})\nOut[5]: \n   B  C\nA      \na  2  3\n4  5  6\n\nIn [6]: df1.rename(columns={'B': 'BB'})\nOut[6]: \n   BB  C\nA       \n1   2  3\n4   5  6\n\nIn [7]: df1.index.names = ['index']\n        df1.columns.names = ['column']\n\nIn [8]: df1\nOut[8]: \ncolumn  B  C\nindex       \n1       2  3\n4       5  6\n"
"import pandas as pd\n\ndata = {'spike-2': [1,2,3], 'hey spke': [4,5,6], 'spiked-in': [7,8,9], 'no': [10,11,12]}\ndf = pd.DataFrame(data)\n\nspike_cols = [col for col in df.columns if 'spike' in col]\nprint(list(df.columns))\nprint(spike_cols)\n\n['hey spke', 'no', 'spike-2', 'spiked-in']\n['spike-2', 'spiked-in']\n\ndf2 = df.filter(regex='spike')\nprint(df2)\n\n   spike-2  spiked-in\n0        1          7\n1        2          8\n2        3          9\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from sklearn.preprocessing import MinMaxScaler\n\n\n&gt;&gt;&gt; scaler = MinMaxScaler()\n\n&gt;&gt;&gt; dfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],\n                           'B':[103.02,107.26,110.35,114.23,114.68],\n                           'C':['big','small','big','small','small']})\n\n&gt;&gt;&gt; dfTest[['A', 'B']] = scaler.fit_transform(dfTest[['A', 'B']])\n\n&gt;&gt;&gt; dfTest\n          A         B      C\n0  0.000000  0.000000    big\n1  0.926219  0.363636  small\n2  0.935335  0.628645    big\n3  1.000000  0.961407  small\n4  0.938495  1.000000  small\n"
"import pandas as pd\nimport io\nimport requests\nurl=&quot;https://raw.githubusercontent.com/cs109/2014_data/master/countries.csv&quot;\ns=requests.get(url).content\nc=pd.read_csv(io.StringIO(s.decode('utf-8')))\n"
'df = df.loc[:,~df.columns.duplicated()]\n'
"subset = data_set[['data_date', 'data_1', 'data_2']]\ntuples = [tuple(x) for x in subset.to_numpy()]\n\ntuples = [tuple(x) for x in subset.values]\n"
'In [20]: df.groupby("dummy").agg({"returns": [np.mean, np.sum]})\nOut[20]:         \n           mean       sum\ndummy                    \n1      0.036901  0.369012\n\nIn [21]: df.groupby(\'dummy\').agg({\'returns\':\n                                  {\'Mean\': np.mean, \'Sum\': np.sum}})\nOut[21]: \n        returns          \n           Mean       Sum\ndummy                    \n1      0.036901  0.369012\n'
'df1 = df[(df.a != -1) &amp; (df.b != -1)]\n\ndf2 = df[(df.a != -1) | (df.b != -1)]\n'
"if isinstance(x, pd.DataFrame):\n    ... # do something\n\nNo:  type(x) is pd.DataFrame\nNo:  type(x) == pd.DataFrame\nYes: isinstance(x, pd.DataFrame)\n\nif obj.__class__.__name__ = 'DataFrame':\n    expect_problems_some_day()\n\nif isinstance(obj, basestring):\n    i_am_string(obj)\n\nimport pandas as pd\nisinstance(var, pd.DataFrame)\n"
"w['female'] = w['female'].map({'female': 1, 'male': 0})\n"
'my_series.apply(your_function, args=(2,3,4), extra_kw=1)\n\n&gt;&gt;&gt; import functools\n&gt;&gt;&gt; import operator\n&gt;&gt;&gt; add_3 = functools.partial(operator.add,3)\n&gt;&gt;&gt; add_3(2)\n5\n&gt;&gt;&gt; add_3(7)\n10\n\nmy_series.apply((lambda x: your_func(a,b,c,d,...,x)))\n'
"df1 = df.where(pd.notnull(df), None)\n\nIn [1]: df = pd.DataFrame([1, np.nan])\n\nIn [2]: df\nOut[2]: \n    0\n0   1\n1 NaN\n\nIn [3]: df1 = df.where(pd.notnull(df), None)\n\nIn [4]: df1\nOut[4]: \n      0\n0     1\n1  None\n\ndf1 = df.astype(object).replace(np.nan, 'None')\n"
"import pandas as pd\n\nidx = pd.date_range('09-01-2013', '09-30-2013')\n\ns = pd.Series({'09-02-2013': 2,\n               '09-03-2013': 10,\n               '09-06-2013': 5,\n               '09-07-2013': 1})\ns.index = pd.DatetimeIndex(s.index)\n\ns = s.reindex(idx, fill_value=0)\nprint(s)\n\n2013-09-01     0\n2013-09-02     2\n2013-09-03    10\n2013-09-04     0\n2013-09-05     0\n2013-09-06     5\n2013-09-07     1\n2013-09-08     0\n...\n"
"In [173]:\n\ndf_a.join(df_b, on='mukey', how='left', lsuffix='_left', rsuffix='_right')\nOut[173]:\n       mukey_left  DI  PI  mukey_right  niccdcd\nindex                                          \n0          100000  35  14          NaN      NaN\n1         1000005  44  14          NaN      NaN\n2         1000006  44  14          NaN      NaN\n3         1000007  43  13          NaN      NaN\n4         1000008  43  13          NaN      NaN\n\nIn [176]:\n\ndf_a.merge(df_b, on='mukey', how='left')\nOut[176]:\n     mukey  DI  PI  niccdcd\n0   100000  35  14      NaN\n1  1000005  44  14      NaN\n2  1000006  44  14      NaN\n3  1000007  43  13      NaN\n4  1000008  43  13      NaN\n"
'import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=2, ncols=2)\n\ndf1.plot(ax=axes[0,0])\ndf2.plot(ax=axes[0,1])\n...\n'
'import pandas as pd\ndf = pd.concat(list_of_dataframes)\n'
"df.loc[df.A==0, 'B'] = np.nan\n\ndf.loc[df.A==0, 'B'] = df.loc[df.A==0, 'B'] / 2\n"
"data['result'] = data['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))\n"
'&gt;&gt;&gt; xl = pd.ExcelFile("dummydata.xlsx")\n&gt;&gt;&gt; xl.sheet_names\n[u\'Sheet1\', u\'Sheet2\', u\'Sheet3\']\n&gt;&gt;&gt; df = xl.parse("Sheet1")\n&gt;&gt;&gt; df.head()\n                  Tid  dummy1    dummy2    dummy3    dummy4    dummy5  \\\n0 2006-09-01 00:00:00       0  5.894611  0.605211  3.842871  8.265307   \n1 2006-09-01 01:00:00       0  5.712107  0.605211  3.416617  8.301360   \n2 2006-09-01 02:00:00       0  5.105300  0.605211  3.090865  8.335395   \n3 2006-09-01 03:00:00       0  4.098209  0.605211  3.198452  8.170187   \n4 2006-09-01 04:00:00       0  3.338196  0.605211  2.970015  7.765058   \n\n     dummy6  dummy7    dummy8    dummy9  \n0  0.623354       0  2.579108  2.681728  \n1  0.554211       0  7.210000  3.028614  \n2  0.567841       0  6.940000  3.644147  \n3  0.581470       0  6.630000  4.016155  \n4  0.595100       0  6.350000  3.974442  \n\n&gt;&gt;&gt; parsed = pd.io.parsers.ExcelFile.parse(xl, "Sheet1")\n&gt;&gt;&gt; parsed.columns\nIndex([u\'Tid\', u\'dummy1\', u\'dummy2\', u\'dummy3\', u\'dummy4\', u\'dummy5\', u\'dummy6\', u\'dummy7\', u\'dummy8\', u\'dummy9\'], dtype=object)\n'
"'g' in df.index\n"
"DataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t')\n\nDataFrame.from_csv('c:/~/trainSetRel3.txt', sep='\\t', header=0)\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'x' : [1, 2, 3, 4], 'y' : [4, 5, 6, 7]})\n&gt;&gt;&gt; df\n   x  y\n0  1  4\n1  2  5\n2  3  6\n3  4  7\n&gt;&gt;&gt; s = df.ix[:,0]\n&gt;&gt;&gt; type(s)\n&lt;class 'pandas.core.series.Series'&gt;\n&gt;&gt;&gt;\n"
"&gt;&gt;&gt; pd.unique(df[['Col1', 'Col2']].values.ravel('K'))\narray(['Bob', 'Joe', 'Bill', 'Mary', 'Steve'], dtype=object)\n\n&gt;&gt;&gt; np.unique(df[['Col1', 'Col2']].values)\narray(['Bill', 'Bob', 'Joe', 'Mary', 'Steve'], dtype=object)\n\n&gt;&gt;&gt; df1 = pd.concat([df]*100000, ignore_index=True) # DataFrame with 500000 rows\n&gt;&gt;&gt; %timeit np.unique(df1[['Col1', 'Col2']].values)\n1 loop, best of 3: 1.12 s per loop\n\n&gt;&gt;&gt; %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel('K'))\n10 loops, best of 3: 38.9 ms per loop\n\n&gt;&gt;&gt; %timeit pd.unique(df1[['Col1', 'Col2']].values.ravel()) # ravel using C order\n10 loops, best of 3: 49.9 ms per loop\n"
"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    'col_1': [0, 1, 2, 3],\n    'col_2': [4, 5, 6, 7]\n})\n\ndf['column_new_1'], df['column_new_2'], df['column_new_3'] = [np.nan, 'dogs', 3]\n\ndf[['column_new_1', 'column_new_2', 'column_new_3']] = pd.DataFrame([[np.nan, 'dogs', 3]], index=df.index)\n\ndf = pd.concat(\n    [\n        df,\n        pd.DataFrame(\n            [[np.nan, 'dogs', 3]], \n            index=df.index, \n            columns=['column_new_1', 'column_new_2', 'column_new_3']\n        )\n    ], axis=1\n)\n\ndf = df.join(pd.DataFrame(\n    [[np.nan, 'dogs', 3]], \n    index=df.index, \n    columns=['column_new_1', 'column_new_2', 'column_new_3']\n))\n\ndf = df.join(pd.DataFrame(\n    {\n        'column_new_1': np.nan,\n        'column_new_2': 'dogs',\n        'column_new_3': 3\n    }, index=df.index\n))\n\ndf = df.assign(column_new_1=np.nan, column_new_2='dogs', column_new_3=3)\n\nnew_cols = ['column_new_1', 'column_new_2', 'column_new_3']\nnew_vals = [np.nan, 'dogs', 3]\ndf = df.reindex(columns=df.columns.tolist() + new_cols)   # add empty cols\ndf[new_cols] = new_vals  # multi-column assignment works for existing cols\n\ndf['column_new_1'] = np.nan\ndf['column_new_2'] = 'dogs'\ndf['column_new_3'] = 3\n"
"df.explode('B')\n\n       A  B\n    0  1  1\n    1  1  2\n    0  2  1\n    1  2  2\n\ndf = pd.DataFrame({'A': [1, 2, 3, 4],'B': [[1, 2], [1, 2], [], np.nan]})\ndf.B = df.B.fillna({i: [] for i in df.index})  # replace NaN with []\ndf.explode('B')\n\n   A    B\n0  1    1\n0  1    2\n1  2    1\n1  2    2\n2  3  NaN\n3  4  NaN\n\ndf.set_index('A').B.apply(pd.Series).stack().reset_index(level=0).rename(columns={0:'B'})\nOut[463]: \n   A  B\n0  1  1\n1  1  2\n0  2  1\n1  2  2\n\ndf=pd.DataFrame({'A':df.A.repeat(df.B.str.len()),'B':np.concatenate(df.B.values)})\ndf\nOut[465]: \n   A  B\n0  1  1\n0  1  2\n1  2  1\n1  2  2\n\ns=pd.DataFrame({'B':np.concatenate(df.B.values)},index=df.index.repeat(df.B.str.len()))\ns.join(df.drop('B',1),how='left')\nOut[477]: \n   B  A\n0  1  1\n0  2  1\n1  1  2\n1  2  2\n\ns.join(df.drop('B',1),how='left').reindex(columns=df.columns)\n\npd.DataFrame([[x] + [z] for x, y in df.values for z in y],columns=df.columns)\nOut[488]: \n   A  B\n0  1  1\n1  1  2\n2  2  1\n3  2  2\n\ns=pd.DataFrame([[x] + [z] for x, y in zip(df.index,df.B) for z in y])\ns.merge(df,left_on=0,right_index=True)\nOut[491]: \n   0  1  A       B\n0  0  1  1  [1, 2]\n1  0  2  1  [1, 2]\n2  1  1  2  [1, 2]\n3  1  2  2  [1, 2]\n\ndf.reindex(df.index.repeat(df.B.str.len())).assign(B=np.concatenate(df.B.values))\nOut[554]: \n   A  B\n0  1  1\n0  1  2\n1  2  1\n1  2  2\n\n#df.loc[df.index.repeat(df.B.str.len())].assign(B=np.concatenate(df.B.values))\n\ndf=pd.DataFrame({'A':[1,2],'B':[[1,2],[3,4]]})\nfrom collections import ChainMap\nd = dict(ChainMap(*map(dict.fromkeys, df['B'], df['A'])))\npd.DataFrame(list(d.items()),columns=df.columns[::-1])\nOut[574]: \n   B  A\n0  1  1\n1  2  1\n2  3  2\n3  4  2\n\nnewvalues=np.dstack((np.repeat(df.A.values,list(map(len,df.B.values))),np.concatenate(df.B.values)))\npd.DataFrame(data=newvalues[0],columns=df.columns)\n   A  B\n0  1  1\n1  1  2\n2  2  1\n3  2  2\n\nfrom itertools import cycle,chain\nl=df.values.tolist()\nl1=[list(zip([x[0]], cycle(x[1])) if len([x[0]]) &gt; len(x[1]) else list(zip(cycle([x[0]]), x[1]))) for x in l]\npd.DataFrame(list(chain.from_iterable(l1)),columns=df.columns)\n   A  B\n0  1  1\n1  1  2\n2  2  1\n3  2  2\n\ndf=pd.DataFrame({'A':[1,2],'B':[[1,2],[3,4]],'C':[[1,2],[3,4]]})\ndf\nOut[592]: \n   A       B       C\n0  1  [1, 2]  [1, 2]\n1  2  [3, 4]  [3, 4]\n\ndef unnesting(df, explode):\n    idx = df.index.repeat(df[explode[0]].str.len())\n    df1 = pd.concat([\n        pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode], axis=1)\n    df1.index = idx\n\n    return df1.join(df.drop(explode, 1), how='left')\n\n        \nunnesting(df,['B','C'])\nOut[609]: \n   B  C  A\n0  1  1  1\n0  2  2  1\n1  3  3  2\n1  4  4  2\n\ndf.join(pd.DataFrame(df.B.tolist(),index=df.index).add_prefix('B_'))\nOut[33]: \n   A       B       C  B_0  B_1\n0  1  [1, 2]  [1, 2]    1    2\n1  2  [3, 4]  [3, 4]    3    4\n\ndef unnesting(df, explode, axis):\n    if axis==1:\n        idx = df.index.repeat(df[explode[0]].str.len())\n        df1 = pd.concat([\n            pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode], axis=1)\n        df1.index = idx\n\n        return df1.join(df.drop(explode, 1), how='left')\n    else :\n        df1 = pd.concat([\n                         pd.DataFrame(df[x].tolist(), index=df.index).add_prefix(x) for x in explode], axis=1)\n        return df1.join(df.drop(explode, 1), how='left')\n\nunnesting(df, ['B','C'], axis=0)\nOut[36]: \n   B0  B1  C0  C1  A\n0   1   2   1   2  1\n1   3   4   3   4  2\n"
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_csv("dup.csv")\n&gt;&gt;&gt; ids = df["ID"]\n&gt;&gt;&gt; df[ids.isin(ids[ids.duplicated()])].sort("ID")\n       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE\n24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12\n6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12\n18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12\n2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12\n12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN\n3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12\n26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12\n\n&gt;&gt;&gt; pd.concat(g for _, g in df.groupby("ID") if len(g) &gt; 1)\n       ID ENROLLMENT_DATE        TRAINER_MANAGING        TRAINER_OPERATOR FIRST_VISIT_DATE\n6   11795        3-Jul-12  0649597-White River VT  0649597-White River VT        30-Mar-12\n24  11795       27-Feb-12      0643D38-Hanover NH      0643D38-Hanover NH        19-Jun-12\n2    8096        8-Aug-12      0643D38-Hanover NH      0643D38-Hanover NH        25-Jun-12\n18   8096       19-Dec-11  0649597-White River VT  0649597-White River VT         9-Apr-12\n3    A036        1-Apr-12      06CB8CF-Hanover NH      06CB8CF-Hanover NH         9-Aug-12\n12   A036       30-Nov-11     063B208-Randolph VT     063B208-Randolph VT              NaN\n26   A036       11-Aug-12      06D3206-Hanover NH                     NaN        19-Jun-12\n'
"df['new_col'] = list(zip(df.lat, df.long))\n"
'sudo apt-get install python-pip\n\nsudo pip install python-dateutil\n'
"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nprint(df.columns.tolist())\n"
"data.reindex(index=data.index[::-1])\n\ndata.iloc[::-1]\n\nfor idx in reversed(data.index):\n    print(idx, data.loc[idx, 'Even'], data.loc[idx, 'Odd'])\n\nfor idx in reversed(data.index):\n    print(idx, data.Even[idx], data.Odd[idx])\n"
'for y in agg.columns:\n    if(agg[y].dtype == np.float64 or agg[y].dtype == np.int64):\n          treat_numeric(agg[y])\n    else:\n          treat_str(agg[y])\n'
"In [43]: df\nOut[43]: \n   CustNum     CustomerName  ItemQty Item                 Seatblocks  ItemExt\n0    32363  McCartney, Paul        3  F04               2:218:10:4,6       60\n1    31316     Lennon, John       25  F01  1:13:36:1,12 1:13:37:1,13      300\n\nIn [44]: s = df['Seatblocks'].str.split(' ').apply(Series, 1).stack()\n\nIn [45]: s.index = s.index.droplevel(-1) # to line up with df's index\n\nIn [46]: s.name = 'Seatblocks' # needs a name to join\n\nIn [47]: s\nOut[47]: \n0    2:218:10:4,6\n1    1:13:36:1,12\n1    1:13:37:1,13\nName: Seatblocks, dtype: object\n\nIn [48]: del df['Seatblocks']\n\nIn [49]: df.join(s)\nOut[49]: \n   CustNum     CustomerName  ItemQty Item  ItemExt    Seatblocks\n0    32363  McCartney, Paul        3  F04       60  2:218:10:4,6\n1    31316     Lennon, John       25  F01      300  1:13:36:1,12\n1    31316     Lennon, John       25  F01      300  1:13:37:1,13\n\nIn [50]: df.join(s.apply(lambda x: Series(x.split(':'))))\nOut[50]: \n   CustNum     CustomerName  ItemQty Item  ItemExt  0    1   2     3\n0    32363  McCartney, Paul        3  F04       60  2  218  10   4,6\n1    31316     Lennon, John       25  F01      300  1   13  36  1,12\n1    31316     Lennon, John       25  F01      300  1   13  37  1,13\n"
"Top15['Citable docs per Capita'].corr(Top15['Energy Supply per Capita'])\n\nimport pandas as pd\n\ndf = pd.DataFrame({'A': range(4), 'B': [2*i for i in range(4)]})\n\n   A  B\n0  0  0\n1  1  2\n2  2  4\n3  3  6\n\ndf['A'].corr(df['B'])\n\ndf.loc[2, 'B'] = 4.5\n\n   A    B\n0  0  0.0\n1  1  2.0\n2  2  4.5\n3  3  6.0\n\ndf['A'].corr(df['B'])\n\n0.99586\n\ndf.corr()\n\n          A         B\nA  1.000000  0.995862\nB  0.995862  1.000000\n"
"In [5]: a.reset_index().merge(b, how=&quot;left&quot;).set_index('index')\nOut[5]:\n       col1  to_merge_on  col2\nindex\na         1            1     1\nb         2            3     2\nc         3            4   NaN\n"
"In [7]: df = pandas.DataFrame(columns=['a','b','c','d'], index=['x','y','z'])\n\nIn [8]: df.loc['y'] = pandas.Series({'a':1, 'b':5, 'c':2, 'd':3})\n\nIn [9]: df\nOut[9]: \n     a    b    c    d\nx  NaN  NaN  NaN  NaN\ny    1    5    2    3\nz  NaN  NaN  NaN  NaN\n"
"&gt;&gt;&gt; searchfor = ['og', 'at']\n&gt;&gt;&gt; s[s.str.contains('|'.join(searchfor))]\n0    cat\n1    hat\n2    dog\n3    fog\ndtype: object\n\n&gt;&gt;&gt; import re\n&gt;&gt;&gt; matches = ['$money', 'x^y']\n&gt;&gt;&gt; safe_matches = [re.escape(m) for m in matches]\n&gt;&gt;&gt; safe_matches\n['\\\\$money', 'x\\\\^y']\n"
'df.melt(id_vars=["location", "name"], \n        var_name="Date", \n        value_name="Value")\n\n  location    name        Date  Value\n0        A  "test"    Jan-2010     12\n1        B   "foo"    Jan-2010     18\n2        A  "test"    Feb-2010     20\n3        B   "foo"    Feb-2010     20\n4        A  "test"  March-2010     30\n5        B   "foo"  March-2010     25\n\n&gt;&gt;&gt; df\n  location  name  Jan-2010  Feb-2010  March-2010\n0        A  test        12        20          30\n1        B   foo        18        20          25\n&gt;&gt;&gt; df2 = pd.melt(df, id_vars=["location", "name"], \n                  var_name="Date", value_name="Value")\n&gt;&gt;&gt; df2\n  location  name        Date  Value\n0        A  test    Jan-2010     12\n1        B   foo    Jan-2010     18\n2        A  test    Feb-2010     20\n3        B   foo    Feb-2010     20\n4        A  test  March-2010     30\n5        B   foo  March-2010     25\n&gt;&gt;&gt; df2 = df2.sort(["location", "name"])\n&gt;&gt;&gt; df2\n  location  name        Date  Value\n0        A  test    Jan-2010     12\n2        A  test    Feb-2010     20\n4        A  test  March-2010     30\n1        B   foo    Jan-2010     18\n3        B   foo    Feb-2010     20\n5        B   foo  March-2010     25\n'
"In [34]: df.loc[df['Value'].idxmax()]\nOut[34]: \nCountry        US\nPlace      Kansas\nValue         894\nName: 7\n\ndf = df.reset_index()\n"
"df['col'] = 'str' + df['col'].astype(str)\n\n&gt;&gt;&gt; df = pd.DataFrame({'col':['a',0]})\n&gt;&gt;&gt; df\n  col\n0   a\n1   0\n&gt;&gt;&gt; df['col'] = 'str' + df['col'].astype(str)\n&gt;&gt;&gt; df\n    col\n0  stra\n1  str0\n"
"In [2]: df\nOut[2]:\n                                                 one\n0                                                one\n1                                                two\n2  This is very long string very long string very...\n\nIn [3]: pd.options.display.max_colwidth\nOut[3]: 50\n\nIn [4]: pd.options.display.max_colwidth = 100\n\nIn [5]: df\nOut[5]:\n                                                                               one\n0                                                                              one\n1                                                                              two\n2  This is very long string very long string very long string veryvery long string\n\nIn [7]: df.iloc[2,0]    # or df.loc[2,'one']\nOut[7]: 'This is very long string very long string very long string veryvery long string'\n"
"df = pd.DataFrame({&quot;A&quot;: [9, 4, 2, 1], &quot;B&quot;: [12, 7, 5, 4]})\ndf\n\n   A   B\n0  9  12\n1  4   7\n2  2   5\n3  1   4\n\ndf.apply(np.sum)\n\nA    16\nB    28\ndtype: int64\n\ndf.sum()\n\nA    16\nB    28\ndtype: int64\n\n%timeit df.apply(np.sum)\n%timeit df.sum()\n2.22 ms ± 41.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n471 µs ± 8.16 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n%timeit df.apply(np.sum, raw=True)\n840 µs ± 691 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\ndf.apply(lambda x: x.max() - x.min())\n\nA    8\nB    8\ndtype: int64\n\ndf.max() - df.min()\n\nA    8\nB    8\ndtype: int64\n\n%timeit df.apply(lambda x: x.max() - x.min())\n%timeit df.max() - df.min()\n\n2.43 ms ± 450 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n1.23 ms ± 14.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\ndf = pd.DataFrame({\n    'Name': ['mickey', 'donald', 'minnie'],\n    'Title': ['wonderland', &quot;welcome to donald's castle&quot;, 'Minnie mouse clubhouse'],\n    'Value': [20, 10, 86]})\ndf\n\n     Name  Value                       Title\n0  mickey     20                  wonderland\n1  donald     10  welcome to donald's castle\n2  minnie     86      Minnie mouse clubhouse\n\ndf.apply(lambda x: x['Name'].lower() in x['Title'].lower(), axis=1)\n\n0    False\n1     True\n2     True\ndtype: bool\n \ndf[df.apply(lambda x: x['Name'].lower() in x['Title'].lower(), axis=1)]\n\n     Name                       Title  Value\n1  donald  welcome to donald's castle     10\n2  minnie      Minnie mouse clubhouse     86\n\ndf[[y.lower() in x.lower() for x, y in zip(df['Title'], df['Name'])]]\n\n     Name                       Title  Value\n1  donald  welcome to donald's castle     10\n2  minnie      Minnie mouse clubhouse     86\n\n%timeit df[df.apply(lambda x: x['Name'].lower() in x['Title'].lower(), axis=1)]\n%timeit df[[y.lower() in x.lower() for x, y in zip(df['Title'], df['Name'])]]\n\n2.85 ms ± 38.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n788 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\ns = pd.Series([[1, 2]] * 3)\ns\n\n0    [1, 2]\n1    [1, 2]\n2    [1, 2]\ndtype: object\n\ns.apply(pd.Series)\n\n   0  1\n0  1  2\n1  1  2\n2  1  2\n\npd.DataFrame(s.tolist())\n\n   0  1\n0  1  2\n1  1  2\n2  1  2\n\n%timeit s.apply(pd.Series)\n%timeit pd.DataFrame(s.tolist())\n\n2.65 ms ± 294 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n816 µs ± 40.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\ndf = pd.DataFrame(\n         pd.date_range('2018-12-31','2019-01-31', freq='2D').date.astype(str).reshape(-1, 2), \n         columns=['date1', 'date2'])\ndf\n\n       date1      date2\n0 2018-12-31 2019-01-02\n1 2019-01-04 2019-01-06\n2 2019-01-08 2019-01-10\n3 2019-01-12 2019-01-14\n4 2019-01-16 2019-01-18\n5 2019-01-20 2019-01-22\n6 2019-01-24 2019-01-26\n7 2019-01-28 2019-01-30\n\ndf.dtypes\n\ndate1    object\ndate2    object\ndtype: object\n    \n\ndf.apply(pd.to_datetime, errors='coerce').dtypes\n\ndate1    datetime64[ns]\ndate2    datetime64[ns]\ndtype: object\n\n%timeit df.apply(pd.to_datetime, errors='coerce')\n%timeit pd.to_datetime(df.stack(), errors='coerce').unstack()\n%timeit pd.concat([pd.to_datetime(df[c], errors='coerce') for c in df], axis=1)\n%timeit for c in df.columns: df[c] = pd.to_datetime(df[c], errors='coerce')\n\n5.49 ms ± 247 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n3.94 ms ± 48.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n3.16 ms ± 216 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n2.41 ms ± 1.71 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nu = df.apply(lambda x: x.str.contains(...))\nv = df.apply(lambda x: x.astype(category))\n\nu = pd.concat([df[c].str.contains(...) for c in df], axis=1)\nv = df.copy()\nfor c in df:\n    v[c] = df[c].astype(category)\n\nimport perfplot\n\nperfplot.show(\n    setup=lambda n: pd.Series(np.random.randint(0, n, n)),\n    kernels=[\n        lambda s: s.astype(str),\n        lambda s: s.apply(str)\n    ],\n    labels=['astype', 'apply'],\n    n_range=[2**k for k in range(1, 20)],\n    xlabel='N',\n    logx=True,\n    logy=True,\n    equality_check=lambda x, y: (x == y).all())\n\ndf = pd.DataFrame({&quot;A&quot;: list('aabcccddee'), &quot;B&quot;: [12, 7, 5, 4, 5, 4, 3, 2, 1, 10]})\ndf\n\n   A   B\n0  a  12\n1  a   7\n2  b   5\n3  c   4\n4  c   5\n5  c   4\n6  d   3\n7  d   2\n8  e   1\n9  e  10\n\ndf.groupby('A').B.cumsum().groupby(df.A).shift()\n \n0     NaN\n1    12.0\n2     NaN\n3     NaN\n4     4.0\n5     9.0\n6     NaN\n7     3.0\n8     NaN\n9     1.0\nName: B, dtype: float64\n\ndf.groupby('A').B.apply(lambda x: x.cumsum().shift())\n\n0     NaN\n1    12.0\n2     NaN\n3     NaN\n4     4.0\n5     9.0\n6     NaN\n7     3.0\n8     NaN\n9     1.0\nName: B, dtype: float64\n\ndf = pd.DataFrame({\n    'A': [1, 2],\n    'B': ['x', 'y']\n})\n\ndef func(x):\n    print(x['A'])\n    return x\n\ndf.apply(func, axis=1)\n\n# 1\n# 1\n# 2\n   A  B\n0  1  x\n1  2  y\n"
'In [11]: df = pd.DataFrame([["foo1"], ["foo2"], ["bar"], [np.nan]], columns=[\'a\'])\n\nIn [12]: df.a.str.contains("foo")\nOut[12]:\n0     True\n1     True\n2    False\n3      NaN\nName: a, dtype: object\n\nIn [13]: df.a.str.contains("foo", na=False)\nOut[13]:\n0     True\n1     True\n2    False\n3    False\nName: a, dtype: bool\n\nIn [21]: df.loc[df.a.str.contains("foo", na=False)]\nOut[21]:\n      a\n0  foo1\n1  foo2\n'
"Total = df['MyColumn'].sum()\nprint (Total)\n319\n\ndf.loc['Total'] = pd.Series(df['MyColumn'].sum(), index = ['MyColumn'])\nprint (df)\n         X  MyColumn      Y      Z\n0        A      84.0   13.0   69.0\n1        B      76.0   77.0  127.0\n2        C      28.0   69.0   16.0\n3        D      28.0   28.0   31.0\n4        E      19.0   20.0   85.0\n5        F      84.0  193.0   70.0\nTotal  NaN     319.0    NaN    NaN\n\ndf.loc['Total'] = df['MyColumn'].sum()\nprint (df)\n         X  MyColumn      Y      Z\n0        A        84   13.0   69.0\n1        B        76   77.0  127.0\n2        C        28   69.0   16.0\n3        D        28   28.0   31.0\n4        E        19   20.0   85.0\n5        F        84  193.0   70.0\nTotal  319       319  319.0  319.0\n\ndf.at['Total', 'MyColumn'] = df['MyColumn'].sum()\nprint (df)\n         X  MyColumn      Y      Z\n0        A      84.0   13.0   69.0\n1        B      76.0   77.0  127.0\n2        C      28.0   69.0   16.0\n3        D      28.0   28.0   31.0\n4        E      19.0   20.0   85.0\n5        F      84.0  193.0   70.0\nTotal  NaN     319.0    NaN    NaN\n\ndf.ix['Total', 'MyColumn'] = df['MyColumn'].sum()\nprint (df)\n         X  MyColumn      Y      Z\n0        A      84.0   13.0   69.0\n1        B      76.0   77.0  127.0\n2        C      28.0   69.0   16.0\n3        D      28.0   28.0   31.0\n4        E      19.0   20.0   85.0\n5        F      84.0  193.0   70.0\nTotal  NaN     319.0    NaN    NaN\n"
"mask = df.my_channel &gt; 20000\ncolumn_name = 'my_channel'\ndf.loc[mask, column_name] = 0\n\ndf.loc[df.my_channel &gt; 20000, 'my_channel'] = 0\n"
'&gt;&gt;&gt; df = pd.DataFrame([[1,2,3],[3,4,5]])\n&gt;&gt;&gt; lol = df.values.tolist()\n&gt;&gt;&gt; lol\n[[1L, 2L, 3L], [3L, 4L, 5L]]\n'
"import ...\nimport matplotlib.ticker as mtick\n\nax = df['myvar'].plot(kind='bar')\nax.yaxis.set_major_formatter(mtick.PercentFormatter())\n"
'In [92]: df\nOut[92]:\n           a         b          c         d\nA  -0.488816  0.863769   4.325608 -4.721202\nB -11.937097  2.993993 -12.916784 -1.086236\nC  -5.569493  4.672679  -2.168464 -9.315900\nD   8.892368  0.932785   4.535396  0.598124\n\nIn [93]: df_norm = (df - df.mean()) / (df.max() - df.min())\n\nIn [94]: df_norm\nOut[94]:\n          a         b         c         d\nA  0.085789 -0.394348  0.337016 -0.109935\nB -0.463830  0.164926 -0.650963  0.256714\nC -0.158129  0.605652 -0.035090 -0.573389\nD  0.536170 -0.376229  0.349037  0.426611\n\nIn [95]: df_norm.mean()\nOut[95]:\na   -2.081668e-17\nb    4.857226e-17\nc    1.734723e-17\nd   -1.040834e-17\n\nIn [96]: df_norm.max() - df_norm.min()\nOut[96]:\na    1\nb    1\nc    1\nd    1\n'
"In [182]:\n\ndf = pd.DataFrame([[1,2,3],[4,5,6]], columns=['a','b','c'])\ndef rowFunc(row):\n    return row['a'] + row['b'] * row['c']\n\ndef rowIndex(row):\n    return row.name\ndf['d'] = df.apply(rowFunc, axis=1)\ndf['rowIndex'] = df.apply(rowIndex, axis=1)\ndf\nOut[182]:\n   a  b  c   d  rowIndex\n0  1  2  3   7         0\n1  4  5  6  34         1\n\nIn [198]:\n\ndf['d'] = df['a'] + df['b'] * df['c']\ndf\nOut[198]:\n   a  b  c   d\n0  1  2  3   7\n1  4  5  6  34\n\nIn [199]:\n\n%timeit df['a'] + df['b'] * df['c']\n%timeit df.apply(rowIndex, axis=1)\n10000 loops, best of 3: 163 µs per loop\n1000 loops, best of 3: 286 µs per loop\n\nIn[15]:\ndf['d'],df['rowIndex'] = df['a'] + df['b'] * df['c'], df.index\ndf\n\nOut[15]: \n   a  b  c   d  rowIndex\n0  1  2  3   7         0\n1  4  5  6  34         1\n\nIn[16]:\ndf['newCol'] = df['a'] + df['b'] + df['c'] + df.index\ndf\n\nOut[16]: \n   a  b  c   d  rowIndex  newCol\n0  1  2  3   7         0       6\n1  4  5  6  34         1      16\n"
'&gt;&gt;&gt; df.idxmax(axis=1)\n0    Communications\n1          Business\n2    Communications\n3    Communications\n4          Business\ndtype: object\n'
"df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))\n"
"import pandas as pd\n\ndf = pd.DataFrame({'DOB': {0: '26/1/2016', 1: '26/1/2016'}})\nprint (df)\n         DOB\n0  26/1/2016 \n1  26/1/2016\n\ndf['DOB'] = pd.to_datetime(df.DOB)\nprint (df)\n         DOB\n0 2016-01-26\n1 2016-01-26\n\ndf['DOB1'] = df['DOB'].dt.strftime('%m/%d/%Y')\nprint (df)\n         DOB        DOB1\n0 2016-01-26  01/26/2016\n1 2016-01-26  01/26/2016\n"
"data.set_index('Locality', inplace=True)\n\n&gt; import pandas as pd\n&gt; df = pd.DataFrame([['ABBOTSFORD', 427000, 448000],\n                     ['ABERFELDIE', 534000, 600000]],\n                    columns=['Locality', 2005, 2006])\n\n&gt; df\n     Locality    2005    2006\n0  ABBOTSFORD  427000  448000\n1  ABERFELDIE  534000  600000\n\n&gt; df.set_index('Locality', inplace=True)\n&gt; df\n              2005    2006\nLocality                  \nABBOTSFORD  427000  448000\nABERFELDIE  534000  600000\n\n&gt; df.loc['ABBOTSFORD']\n2005    427000\n2006    448000\nName: ABBOTSFORD, dtype: int64\n\n&gt; df.loc['ABBOTSFORD'][2005]\n427000\n\n&gt; df.loc['ABBOTSFORD'].values\narray([427000, 448000])\n\n&gt; df.loc['ABBOTSFORD'].tolist()\n[427000, 448000]\n"
"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas\n"
'import pandas\nfrom openpyxl import load_workbook\n\nbook = load_workbook(\'Masterfile.xlsx\')\nwriter = pandas.ExcelWriter(\'Masterfile.xlsx\', engine=\'openpyxl\') \nwriter.book = book\n\n## ExcelWriter for some reason uses writer.sheets to access the sheet.\n## If you leave it empty it will not know that sheet Main is already there\n## and will create a new sheet.\n\nwriter.sheets = dict((ws.title, ws) for ws in book.worksheets)\n\ndata_filtered.to_excel(writer, "Main", cols=[\'Diff1\', \'Diff2\'])\n\nwriter.save()\n'
"df[df.C &lt;= df.B].loc[:,'B':'E']\n\ndf.loc[df.C &lt;= df.B, 'B':'E']\n"
'def sizes(s):\n    s[\'size_kb\'] = locale.format("%.1f", s[\'size\'] / 1024.0, grouping=True) + \' KB\'\n    s[\'size_mb\'] = locale.format("%.1f", s[\'size\'] / 1024.0 ** 2, grouping=True) + \' MB\'\n    s[\'size_gb\'] = locale.format("%.1f", s[\'size\'] / 1024.0 ** 3, grouping=True) + \' GB\'\n    return s\n\ndf_test = df_test.append(rows_list)\ndf_test = df_test.apply(sizes, axis=1)\n'
"In [2]: df\nOut[2]:\n    A  B\n0  p1  1\n1  p1  2\n2  p3  3\n3  p2  4\n\nIn [3]: df.loc[df['B'] == 3, 'A']\nOut[3]:\n2    p3\nName: A, dtype: object\n\nIn [4]: df.loc[df['B'] == 3, 'A'].iloc[0]\nOut[4]: 'p3'\n"
'from pandas import DataFrame\ndf = DataFrame(resoverall.fetchall())\ndf.columns = resoverall.keys()\n'
"df2[list('xab')]  # df2 but only with columns x, a, and b\n\ndf1.merge(df2[list('xab')])\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; simple_list=[['a','b']]\n&gt;&gt;&gt; simple_list.append(['e','f'])\n&gt;&gt;&gt; df=pd.DataFrame(simple_list,columns=['col1','col2'])\n   col1 col2\n0    a    b\n1    e    f\n"
'In [20]: df = DataFrame(data[\'values\'])\n\nIn [21]: df.columns = ["date","price"]\n\nIn [22]: df\nOut[22]: \n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 358 entries, 0 to 357\nData columns (total 2 columns):\ndate     358  non-null values\nprice    358  non-null values\ndtypes: float64(1), int64(1)\n\nIn [23]: df.head()\nOut[23]: \n         date  price\n0  1349720105  12.08\n1  1349806505  12.35\n2  1349892905  12.15\n3  1349979305  12.19\n4  1350065705  12.15\nIn [25]: df[\'date\'] = pd.to_datetime(df[\'date\'],unit=\'s\')\n\nIn [26]: df.head()\nOut[26]: \n                 date  price\n0 2012-10-08 18:15:05  12.08\n1 2012-10-09 18:15:05  12.35\n2 2012-10-10 18:15:05  12.15\n3 2012-10-11 18:15:05  12.19\n4 2012-10-12 18:15:05  12.15\n\nIn [27]: df.dtypes\nOut[27]: \ndate     datetime64[ns]\nprice           float64\ndtype: object\n'
'df.iloc[::5, :]\n'
'In [4]: df = read_csv(StringIO(data),sep=\'\\s+\')\n\nIn [5]: df\nOut[5]: \n   A         B       C\n0  1  0.749065    This\n1  2  0.301084      is\n2  3  0.463468       a\n3  4  0.643961  random\n4  1  0.866521  string\n5  2  0.120737       !\n\nIn [6]: df.dtypes\nOut[6]: \nA      int64\nB    float64\nC     object\ndtype: object\n\nIn [8]: df.groupby(\'A\').apply(lambda x: x.sum())\nOut[8]: \n   A         B           C\nA                         \n1  2  1.615586  Thisstring\n2  4  0.421821         is!\n3  3  0.463468           a\n4  4  0.643961      random\n\nIn [9]: df.groupby(\'A\')[\'C\'].apply(lambda x: x.sum())\nOut[9]: \nA\n1    Thisstring\n2           is!\n3             a\n4        random\ndtype: object\n\nIn [11]: df.groupby(\'A\')[\'C\'].apply(lambda x: "{%s}" % \', \'.join(x))\nOut[11]: \nA\n1    {This, string}\n2           {is, !}\n3               {a}\n4          {random}\ndtype: object\n\ndef f(x):\n     return Series(dict(A = x[\'A\'].sum(), \n                        B = x[\'B\'].sum(), \n                        C = "{%s}" % \', \'.join(x[\'C\'])))\n\nIn [14]: df.groupby(\'A\').apply(f)\nOut[14]: \n   A         B               C\nA                             \n1  2  1.615586  {This, string}\n2  4  0.421821         {is, !}\n3  3  0.463468             {a}\n4  4  0.643961        {random}\n'
"In [11]: df['Date'] + ' ' + df['Time']\nOut[11]:\n0    01-06-2013 23:00:00\n1    02-06-2013 01:00:00\n2    02-06-2013 21:00:00\n3    02-06-2013 22:00:00\n4    02-06-2013 23:00:00\n5    03-06-2013 01:00:00\n6    03-06-2013 21:00:00\n7    03-06-2013 22:00:00\n8    03-06-2013 23:00:00\n9    04-06-2013 01:00:00\ndtype: object\n\nIn [12]: pd.to_datetime(df['Date'] + ' ' + df['Time'])\nOut[12]:\n0   2013-01-06 23:00:00\n1   2013-02-06 01:00:00\n2   2013-02-06 21:00:00\n3   2013-02-06 22:00:00\n4   2013-02-06 23:00:00\n5   2013-03-06 01:00:00\n6   2013-03-06 21:00:00\n7   2013-03-06 22:00:00\n8   2013-03-06 23:00:00\n9   2013-04-06 01:00:00\ndtype: datetime64[ns]\n"
'import pandas\ndf = pandas.read_csv("test.csv")\ndf.loc[df.ID == 103, \'FirstName\'] = "Matt"\ndf.loc[df.ID == 103, \'LastName\'] = "Jones"\n\ndf.loc[df.ID == 103, [\'FirstName\', \'LastName\']] = \'Matt\', \'Jones\'\n\nimport pandas\ndf = pandas.read_csv("test.csv")\ndf[\'FirstName\'][df.ID == 103] = "Matt"\ndf[\'LastName\'][df.ID == 103] = "Jones"\n'
"import numpy as np \nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\n\nindex = ['aaa', 'bbb', 'ccc', 'ddd', 'eee']\ncolumns = ['A', 'B', 'C', 'D']\ndf = DataFrame(abs(np.random.randn(5, 4)), index=index, columns=columns)\n\nplt.pcolor(df)\nplt.yticks(np.arange(0.5, len(df.index), 1), df.index)\nplt.xticks(np.arange(0.5, len(df.columns), 1), df.columns)\nplt.show()\n"
'xl_file = pd.ExcelFile(file_name)\n\ndfs = {sheet_name: xl_file.parse(sheet_name) \n          for sheet_name in xl_file.sheet_names}\n\ndfs = pd.read_excel(file_name, sheet_name=None)\n\ndfs = pd.read_excel(file_name, sheetname=None)\n'
"import pandas as pd\n\nimport numpy as np\n\narray=np.random.random((2,4))\n\ndf=pd.DataFrame(array, columns=('Test1', 'toto', 'test2', 'riri'))\n\nprint df\n\n      Test1      toto     test2      riri\n0  0.923249  0.572528  0.845464  0.144891\n1  0.020438  0.332540  0.144455  0.741412\n\ncols = [c for c in df.columns if c.lower()[:4] != 'test']\n\ndf=df[cols]\n\nprint df\n       toto      riri\n0  0.572528  0.144891\n1  0.332540  0.741412\n"
'In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=["foo","bar","fooXYZbar", "bat"]))\n\nIn [92]: df\nOut[92]:\n   A          C\n0  5        foo\n1  3        bar\n2  5  fooXYZbar\n3  6        bat\n\nIn [93]: df[~df.C.str.contains("XYZ")]\nOut[93]:\n   A    C\n0  5  foo\n1  3  bar\n3  6  bat\n'
"'O'     (Python) objects\n\n'b'       boolean\n'i'       (signed) integer\n'u'       unsigned integer\n'f'       floating-point\n'c'       complex-floating point\n'O'       (Python) objects\n'S', 'a'  (byte-)string\n'U'       Unicode\n'V'       raw data (void)\n"
'bigdata = data1.append(data2, ignore_index=True)\n'
"In [28]:\n\nfilter_col = [col for col in df if col.startswith('foo')]\nfilter_col\nOut[28]:\n['foo.aa', 'foo.bars', 'foo.fighters', 'foo.fox', 'foo.manchu']\nIn [29]:\n\ndf[filter_col]\nOut[29]:\n   foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu\n0     1.0         0             0        2         NA\n1     2.1         0             1        4          0\n2     NaN         0           NaN        1          0\n3     4.7         0             0        0          0\n4     5.6         0             0        0          0\n5     6.8         1             0        5          0\n\nIn [33]:\n\ndf[df.columns[pd.Series(df.columns).str.startswith('foo')]]\nOut[33]:\n   foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu\n0     1.0         0             0        2         NA\n1     2.1         0             1        4          0\n2     NaN         0           NaN        1          0\n3     4.7         0             0        0          0\n4     5.6         0             0        0          0\n5     6.8         1             0        5          0\n\nIn [36]:\n\ndf[df[df.columns[pd.Series(df.columns).str.startswith('foo')]]==1]\nOut[36]:\n   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo\n0      NaN       1       NaN           NaN      NaN        NaN     NaN\n1      NaN     NaN       NaN             1      NaN        NaN     NaN\n2      NaN     NaN       NaN           NaN        1        NaN     NaN\n3      NaN     NaN       NaN           NaN      NaN        NaN     NaN\n4      NaN     NaN       NaN           NaN      NaN        NaN     NaN\n5      NaN     NaN         1           NaN      NaN        NaN     NaN\n\nIn [72]:\n\ndf.loc[df[df[df.columns[pd.Series(df.columns).str.startswith('foo')]] == 1].dropna(how='all', axis=0).index]\nOut[72]:\n   bar.baz  foo.aa  foo.bars  foo.fighters  foo.fox foo.manchu nas.foo\n0      5.0     1.0         0             0        2         NA      NA\n1      5.0     2.1         0             1        4          0       0\n2      6.0     NaN         0           NaN        1          0       1\n5      6.8     6.8         1             0        5          0       0\n"
"df[df.Letters=='C'].Letters.item()\n"
"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\n# save load_iris() sklearn dataset to iris\n# if you'd like to check dataset type use: type(load_iris())\n# if you'd like to view list of attributes use: dir(load_iris())\niris = load_iris()\n\n# np.c_ is the numpy concatenate function\n# which is used to concat iris['data'] and iris['target'] arrays \n# for pandas column argument: concat iris['feature_names'] list\n# and string list (in this case one string); you can make this anything you'd like..  \n# the original dataset would probably call this ['Species']\ndata1 = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n                     columns= iris['feature_names'] + ['target'])\n"
'df = pd.read_sql(query.statement, query.session.bind)\n'
"frame = frame[['column I want first', 'column I want second'...etc.]]\n"
"In [29]: df = pd.DataFrame({'a':[1,2,1,2], 'b':[3,4,3,5]})\n\nIn [30]: df\nOut[30]:\n   a  b\n0  1  3\n1  2  4\n2  1  3\n3  2  5\n\nIn [32]: df.drop_duplicates()\nOut[32]:\n   a  b\n0  1  3\n1  2  4\n3  2  5\n"
"In [21]: df = pd.DataFrame([(1,2,3), ('foo','bar','baz'), (4,5,6)])\n\nIn [22]: df\nOut[22]: \n     0    1    2\n0    1    2    3\n1  foo  bar  baz\n2    4    5    6\n\nIn [23]: df.columns = df.iloc[1]\n\nIn [24]: df.drop(df.index[1])\nOut[24]: \n1 foo bar baz\n0   1   2   3\n2   4   5   6\n\nIn [133]: df.iloc[pd.RangeIndex(len(df)).drop(1)]\nOut[133]: \n1 foo bar baz\n0   1   2   3\n2   4   5   6\n"
'pd.to_datetime, pd.to_timedelta and pd.to_numeric\n\ndf = pd.DataFrame({\'x\': {0: \'a\', 1: \'b\'}, \'y\': {0: \'1\', 1: \'2\'}, \'z\': {0: \'2018-05-01\', 1: \'2018-05-02\'}})\n\ndf.dtypes\n\nx    object\ny    object\nz    object\ndtype: object\n\ndf\n\n   x  y           z\n0  a  1  2018-05-01\n1  b  2  2018-05-02\n\ndf["y"] = pd.to_numeric(df["y"])\ndf["z"] = pd.to_datetime(df["z"])    \ndf\n\n   x  y          z\n0  a  1 2018-05-01\n1  b  2 2018-05-02\n\ndf.dtypes\n\nx            object\ny             int64\nz    datetime64[ns]\ndtype: object\n\nIn [21]: df\nOut[21]: \n   x  y\n0  a  1\n1  b  2\n\nIn [22]: df.dtypes\nOut[22]: \nx    object\ny    object\ndtype: object\n\nIn [23]: df.convert_objects(convert_numeric=True)\nOut[23]: \n   x  y\n0  a  1\n1  b  2\n\nIn [24]: df.convert_objects(convert_numeric=True).dtypes\nOut[24]: \nx    object\ny     int64\ndtype: object\n'
"from sqlalchemy import create_engine\nengine = create_engine('postgresql://username:password@localhost:5432/mydatabase')\ndf.to_sql('table_name', engine)\n\nimport sql  # the patched version (file is named sql.py)\nsql.write_frame(df, 'table_name', con, flavor='postgresql')\n"
"In [26]:\n\ndf1.groupby(['A','B']).size().reset_index().rename(columns={0:'count'})\nOut[26]:\n     A    B  count\n0   no   no      1\n1   no  yes      2\n2  yes   no      4\n3  yes  yes      3\n\nIn[202]:\ndf1.groupby(['A','B']).size()\n\nOut[202]: \nA    B  \nno   no     1\n     yes    2\nyes  no     4\n     yes    3\ndtype: int64\n\nIn[203]:\ndf1.groupby(['A','B']).size().reset_index()\n\nOut[203]: \n     A    B  0\n0   no   no  1\n1   no  yes  2\n2  yes   no  4\n3  yes  yes  3\n\nIn[204]:\ndf1.groupby(['A','B']).size().reset_index().rename(columns={0:'count'})\n\nOut[204]: \n     A    B  count\n0   no   no      1\n1   no  yes      2\n2  yes   no      4\n3  yes  yes      3\n\nIn[205]:\ndf1.groupby(['A','B'], as_index=False).size()\n\nOut[205]: \nA    B  \nno   no     1\n     yes    2\nyes  no     4\n     yes    3\ndtype: int64\n"
"from pandas import DataFrame, merge\ndf1 = DataFrame({'key':[1,1], 'col1':[1,2],'col2':[3,4]})\ndf2 = DataFrame({'key':[1,1], 'col3':[5,6]})\n\nmerge(df1, df2,on='key')[['col1', 'col2', 'col3']]\n\n   col1  col2  col3\n0     1     3     5\n1     1     3     6\n2     2     4     5\n3     2     4     6\n"
'df.an_operation(inplace=True)\n\ndf = df.an_operation(inplace=False) \n'
"bins = [0, 1, 5, 10, 25, 50, 100]\ndf['binned'] = pd.cut(df['percentage'], bins)\nprint (df)\n   percentage     binned\n0       46.50   (25, 50]\n1       44.20   (25, 50]\n2      100.00  (50, 100]\n3       42.12   (25, 50]\n\nbins = [0, 1, 5, 10, 25, 50, 100]\nlabels = [1,2,3,4,5,6]\ndf['binned'] = pd.cut(df['percentage'], bins=bins, labels=labels)\nprint (df)\n   percentage binned\n0       46.50      5\n1       44.20      5\n2      100.00      6\n3       42.12      5\n\nbins = [0, 1, 5, 10, 25, 50, 100]\ndf['binned'] = np.searchsorted(bins, df['percentage'].values)\nprint (df)\n   percentage  binned\n0       46.50       5\n1       44.20       5\n2      100.00       6\n3       42.12       5\n\ns = pd.cut(df['percentage'], bins=bins).value_counts()\nprint (s)\n(25, 50]     3\n(50, 100]    1\n(10, 25]     0\n(5, 10]      0\n(1, 5]       0\n(0, 1]       0\nName: percentage, dtype: int64\n\ns = df.groupby(pd.cut(df['percentage'], bins=bins)).size()\nprint (s)\npercentage\n(0, 1]       0\n(1, 5]       0\n(5, 10]      0\n(10, 25]     0\n(25, 50]     3\n(50, 100]    1\ndtype: int64\n"
"import pandas as pd\n\nsource = pd.DataFrame({'Country' : ['USA', 'USA', 'Russia','USA'], \n                  'City' : ['New-York', 'New-York', 'Sankt-Petersburg', 'New-York'],\n                  'Short name' : ['NY','New','Spb','NY']})\n\nsource.groupby(['Country','City']).agg(lambda x:x.value_counts().index[0])\n\n# Let's add a new col,  account\nsource['account'] = [1,2,3,3]\n\nsource.groupby(['Country','City']).agg(mod  = ('Short name', \\\n                                        lambda x: x.value_counts().index[0]),\n                                        avg = ('account', 'mean') \\\n                                      )\n"
"&gt; dat1 = pd.DataFrame({'dat1': [9,5]})\n&gt; dat2 = pd.DataFrame({'dat2': [7,6]})\n&gt; dat1.join(dat2)\n   dat1  dat2\n0     9     7\n1     5     6\n"
"C = np.where(cond, A, B)\n\nimport numpy as np\nimport pandas as pd\n\na = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\ndf['que'] = np.where((df['one'] &gt;= df['two']) &amp; (df['one'] &lt;= df['three'])\n                     , df['one'], np.nan)\n\n  one  two three  que\n0  10  1.2   4.2   10\n1  15   70  0.03  NaN\n2   8    5     0  NaN\n\nconditions = [\n    (df['one'] &gt;= df['two']) &amp; (df['one'] &lt;= df['three']), \n    df['one'] &lt; df['two']]\n\nchoices = [df['one'], df['two']]\n\ndf['que'] = np.select(conditions, choices, default=np.nan)\n\n  one  two three  que\n0  10  1.2   4.2   10\n1  15   70  0.03   70\n2   8    5     0  NaN\n\nconditions = [\n    df['one'] &lt; df['two'],\n    df['one'] &lt;= df['three']]\n\nchoices = [df['two'], df['one']]\n\na = [['10', '1.2', '4.2'], ['15', '70', '0.03'], ['8', '5', '0']]\ndf = pd.DataFrame(a, columns=['one', 'two', 'three'])\n\ndf2 = df.astype(float)\n\nIn [61]: '10' &lt;= '4.2'\nOut[61]: True\n\nIn [62]: 10 &lt;= 4.2\nOut[62]: False\n"
"[f(x) for x in seq]\n\n[f(x, y) for x, y in zip(seq1, seq2)]\n\n# Boolean indexing with Numeric value comparison.\ndf[df.A != df.B]                            # vectorized !=\ndf.query('A != B')                          # query (numexpr)\ndf[[x != y for x, y in zip(df.A, df.B)]]    # list comp\n\ndf[df.A.values != df.B.values]\n\n# Value Counts comparison.\nser.value_counts(sort=False).to_dict()           # value_counts\ndict(zip(*np.unique(ser, return_counts=True)))   # np.unique\nCounter(ser)                                     # Counter\n\nfrom numba import njit, prange\n\n@njit(parallel=True)\ndef get_mask(x, y):\n    result = [False] * len(x)\n    for i in prange(len(x)):\n        result[i] = x[i] != y[i]\n\n    return np.array(result)\n\ndf[get_mask(df.A.values, df.B.values)] # numba\n\n# Boolean indexing with string value comparison.\ndf[df.A != df.B]                            # vectorized !=\ndf.query('A != B')                          # query (numexpr)\ndf[[x != y for x, y in zip(df.A, df.B)]]    # list comp\n\n# Dictionary value extraction.\nser.map(operator.itemgetter('value'))     # map\npd.Series([x.get('value') for x in ser])  # list comprehension\n\n# List positional indexing. \ndef get_0th(lst):\n    try:\n        return lst[0]\n    # Handle empty lists and NaNs gracefully.\n    except (IndexError, TypeError):\n        return np.nan\n\nser.map(get_0th)                                          # map\nser.str[0]                                                # str accessor\npd.Series([x[0] if len(x) &gt; 0 else np.nan for x in ser])  # list comp\npd.Series([get_0th(x) for x in ser])                      # list comp safe\n\npd.Series([...], index=ser.index)\n\n# Nested list flattening.\npd.DataFrame(ser.tolist()).stack().reset_index(drop=True)  # stack\npd.Series(list(chain.from_iterable(ser.tolist())))         # itertools.chain\npd.Series([y for x in ser for y in x])                     # nested list comp\n\np = re.compile(...)\nser2 = pd.Series([x for x in ser if p.search(x)])\n\nser2 = ser[[bool(p.search(x)) for x in ser]]\n\nser[[bool(p.search(x)) if pd.notnull(x) else False for x in ser]]\n\ndf['col2'] = [p.search(x).group(0) for x in df['col']]\n\ndef matcher(x):\n    m = p.search(str(x))\n    if m:\n        return m.group(0)\n    return np.nan\n\ndf['col2'] = [matcher(x) for x in df['col']]\n\n# Extracting strings.\np = re.compile(r'(?&lt;=[A-Z])(\\d{4})')\ndef matcher(x):\n    m = p.search(x)\n    if m:\n        return m.group(0)\n    return np.nan\n\nser.str.extract(r'(?&lt;=[A-Z])(\\d{4})', expand=False)   #  str.extract\npd.Series([matcher(x) for x in ser])                  #  list comprehension\n\nimport perfplot  \nimport operator \nimport pandas as pd\nimport numpy as np\nimport re\n\nfrom collections import Counter\nfrom itertools import chain\n\n# Boolean indexing with Numeric value comparison.\nperfplot.show(\n    setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=['A','B']),\n    kernels=[\n        lambda df: df[df.A != df.B],\n        lambda df: df.query('A != B'),\n        lambda df: df[[x != y for x, y in zip(df.A, df.B)]],\n        lambda df: df[get_mask(df.A.values, df.B.values)]\n    ],\n    labels=['vectorized !=', 'query (numexpr)', 'list comp', 'numba'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N'\n)\n\n# Value Counts comparison.\nperfplot.show(\n    setup=lambda n: pd.Series(np.random.choice(1000, n)),\n    kernels=[\n        lambda ser: ser.value_counts(sort=False).to_dict(),\n        lambda ser: dict(zip(*np.unique(ser, return_counts=True))),\n        lambda ser: Counter(ser),\n    ],\n    labels=['value_counts', 'np.unique', 'Counter'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N',\n    equality_check=lambda x, y: dict(x) == dict(y)\n)\n\n# Boolean indexing with string value comparison.\nperfplot.show(\n    setup=lambda n: pd.DataFrame(np.random.choice(1000, (n, 2)), columns=['A','B'], dtype=str),\n    kernels=[\n        lambda df: df[df.A != df.B],\n        lambda df: df.query('A != B'),\n        lambda df: df[[x != y for x, y in zip(df.A, df.B)]],\n    ],\n    labels=['vectorized !=', 'query (numexpr)', 'list comp'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N',\n    equality_check=None\n)\n\n# Dictionary value extraction.\nser1 = pd.Series([{'key': 'abc', 'value': 123}, {'key': 'xyz', 'value': 456}])\nperfplot.show(\n    setup=lambda n: pd.concat([ser1] * n, ignore_index=True),\n    kernels=[\n        lambda ser: ser.map(operator.itemgetter('value')),\n        lambda ser: pd.Series([x.get('value') for x in ser]),\n    ],\n    labels=['map', 'list comprehension'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N',\n    equality_check=None\n)\n\n# List positional indexing. \nser2 = pd.Series([['a', 'b', 'c'], [1, 2], []])        \nperfplot.show(\n    setup=lambda n: pd.concat([ser2] * n, ignore_index=True),\n    kernels=[\n        lambda ser: ser.map(get_0th),\n        lambda ser: ser.str[0],\n        lambda ser: pd.Series([x[0] if len(x) &gt; 0 else np.nan for x in ser]),\n        lambda ser: pd.Series([get_0th(x) for x in ser]),\n    ],\n    labels=['map', 'str accessor', 'list comprehension', 'list comp safe'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N',\n    equality_check=None\n)\n\n# Nested list flattening.\nser3 = pd.Series([['a', 'b', 'c'], ['d', 'e'], ['f', 'g']])\nperfplot.show(\n    setup=lambda n: pd.concat([ser2] * n, ignore_index=True),\n    kernels=[\n        lambda ser: pd.DataFrame(ser.tolist()).stack().reset_index(drop=True),\n        lambda ser: pd.Series(list(chain.from_iterable(ser.tolist()))),\n        lambda ser: pd.Series([y for x in ser for y in x]),\n    ],\n    labels=['stack', 'itertools.chain', 'nested list comp'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N',    \n    equality_check=None\n\n)\n\n# Extracting strings.\nser4 = pd.Series(['foo xyz', 'test A1234', 'D3345 xtz'])\nperfplot.show(\n    setup=lambda n: pd.concat([ser4] * n, ignore_index=True),\n    kernels=[\n        lambda ser: ser.str.extract(r'(?&lt;=[A-Z])(\\d{4})', expand=False),\n        lambda ser: pd.Series([matcher(x) for x in ser])\n    ],\n    labels=['str.extract', 'list comprehension'],\n    n_range=[2**k for k in range(0, 15)],\n    xlabel='N',\n    equality_check=None\n)\n"
"In [30]:\n\ndf['range'] = df['range'].str.replace(',','-')\ndf\nOut[30]:\n      range\n0    (2-30)\n1  (50-290)\n\ndf['range'].replace(',','-',inplace=True)\n\nIn [43]:\n\ndf = pd.DataFrame({'range':['(2,30)',',']})\ndf['range'].replace(',','-', inplace=True)\ndf['range']\nOut[43]:\n0    (2,30)\n1         -\nName: range, dtype: object\n"
"pd.concat([df1,df2]).drop_duplicates(keep=False)\n\ndf1=pd.DataFrame({'A':[1,2,3,3],'B':[2,3,4,4]})\ndf2=pd.DataFrame({'A':[1],'B':[2]})\n\npd.concat([df1, df2]).drop_duplicates(keep=False)\nOut[655]: \n   A  B\n1  2  3\n\nOut[656]: \n   A  B\n1  2  3\n2  3  4\n3  3  4\n\ndf1[~df1.apply(tuple,1).isin(df2.apply(tuple,1))]\nOut[657]: \n   A  B\n1  2  3\n2  3  4\n3  3  4\n\ndf1.merge(df2,indicator = True, how='left').loc[lambda x : x['_merge']!='both']\nOut[421]: \n   A  B     _merge\n1  2  3  left_only\n2  3  4  left_only\n3  3  4  left_only\n"
'print paramdata.values\n\nparamdata.columns\n\nparamdata.index\n'
'&gt;&gt;&gt; df\n          A         B         C         D\n0  0.424634  1.716633  0.282734  2.086944\n1 -1.325816  2.056277  2.583704 -0.776403\n2  1.457809 -0.407279 -1.560583 -1.316246\n3 -0.757134 -1.321025  1.325853 -2.513373\n4  1.366180 -1.265185 -2.184617  0.881514\n&gt;&gt;&gt; df.iloc[:, 2]\n0    0.282734\n1    2.583704\n2   -1.560583\n3    1.325853\n4   -2.184617\nName: C\n&gt;&gt;&gt; df[df.columns[2]]\n0    0.282734\n1    2.583704\n2   -1.560583\n3    1.325853\n4   -2.184617\nName: C\n'
"&gt; df = pd.DataFrame({'a':[0,0,1,1], 'b':[0,1,0,1]})\n&gt; df = df[(df.T != 0).any()]\n&gt; df\n   a  b\n1  0  1\n2  1  0\n3  1  1\n"
"In [75]: df = pd.DataFrame({'col1':[1,2,3,4,5], 'col2':list('abcab'),  'col3':list('ababb')})\n\nIn [76]: df['col2'] = df['col2'].astype('category')\n\nIn [77]: df['col3'] = df['col3'].astype('category')\n\nIn [78]: df.dtypes\nOut[78]:\ncol1       int64\ncol2    category\ncol3    category\ndtype: object\n\nIn [80]: cat_columns = df.select_dtypes(['category']).columns\n\nIn [81]: cat_columns\nOut[81]: Index([u'col2', u'col3'], dtype='object')\n\nIn [83]: df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n\nIn [84]: df\nOut[84]:\n   col1  col2  col3\n0     1     0     0\n1     2     1     1\n2     3     2     0\n3     4     0     1\n4     5     1     1\n"
"In [99]: np.isnan(np.array([np.nan, 0], dtype=np.float64))\nOut[99]: array([ True, False], dtype=bool)\n\nIn [96]: np.isnan(np.array([np.nan, 0], dtype=object))\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\nIn [97]: pd.isnull(np.array([np.nan, 0], dtype=float))\nOut[97]: array([ True, False], dtype=bool)\n\nIn [98]: pd.isnull(np.array([np.nan, 0], dtype=object))\nOut[98]: array([ True, False], dtype=bool)\n"
"pip install swifter\n\nimport swifter\n\ndef some_function(data):\n    return data * 10\n\ndata['out'] = data['in'].swifter.apply(some_function)\n"
"df['hID'].nunique()\n5\n\ndf['hID'].count()\n8\n\ndf['hID'].size\n8\n\ndf.loc[df['mID']=='A','hID'].agg(['nunique','count','size'])\n\ndf.query('mID == &quot;A&quot;')['hID'].agg(['nunique','count','size'])\n\nnunique    5\ncount      5\nsize       5\nName: hID, dtype: int64\n"
'        date\n0 2001-08-10\n1 2002-08-31\n2 2003-08-29\n3 2006-06-21\n4 2002-03-27\n5 2003-07-14\n6 2004-06-15\n7 2003-08-14\n8 2003-07-29\n\ndf["date"] = df["date"].astype("datetime64")\n\ndf.groupby(df["date"].dt.month).count().plot(kind="bar")\n\ndf.groupby([df["date"].dt.year, df["date"].dt.month]).count().plot(kind="bar")\n'
"import pandas as pd\n\npd.concat([df], keys=['Foo'], names=['Firstlevel'])\n\npd.concat({'Foo': df}, names=['Firstlevel'])\n"
"males = df[(df[Gender]=='Male') &amp; (df[Year]==2014)]\n\nfrom collections import defaultdict\ndic={}\nfor g in ['male', 'female']:\n  dic[g]=defaultdict(dict)\n  for y in [2013, 2014]:\n    dic[g][y]=df[(df[Gender]==g) &amp; (df[Year]==y)] #store the DataFrames to a dict of dict\n\ndef getDF(dic, gender, year):\n  return dic[gender][year]\n\nprint genDF(dic, 'male', 2014)\n"
'd = pd.DataFrame(0, index=np.arange(len(data)), columns=feature_list)\n'
"df[df['Col2'].isnull()]\n"
"  Downloading pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl (26.2MB)\n  Downloading numpy-1.14.1-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n\n  Downloading pandas-0.22.0.tar.gz (11.3MB)\n  Downloading numpy-1.14.1.zip (4.9MB)\n\n/ # ps aux\nPID   USER     TIME   COMMAND\n    1 root       0:00 /bin/sh -c pip install pandas\n    7 root       0:04 {pip} /usr/local/bin/python /usr/local/bin/pip install pandas\n   21 root       0:07 /usr/local/bin/python -c import setuptools, tokenize;__file__='/tmp/pip-build-en29h0ak/pandas/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n\n  496 root       0:00 sh\n  660 root       0:00 /bin/sh -c gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -DTHREAD_STACK_SIZE=0x100000 -fPIC -Ibuild/src.linux-x86_64-3.6/numpy/core/src/pri\n  661 root       0:00 gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -DTHREAD_STACK_SIZE=0x100000 -fPIC -Ibuild/src.linux-x86_64-3.6/numpy/core/src/private -Inump\n  662 root       0:00 /usr/libexec/gcc/x86_64-alpine-linux-musl/6.4.0/cc1 -quiet -I build/src.linux-x86_64-3.6/numpy/core/src/private -I numpy/core/include -I build/src.linux-x86_64-3.6/numpy/core/includ\n  663 root       0:00 ps aux\n\nFROM python:3.6.4-alpine3.7\nRUN apk add --no-cache g++ wget\nRUN wget https://pypi.python.org/packages/da/c6/0936bc5814b429fddb5d6252566fe73a3e40372e6ceaf87de3dec1326f28/pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl\nRUN pip install pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl\n\nStep 4/4 : RUN pip install pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl\n ---&gt; Running in 0faea63e2bda\npandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl is not a supported wheel on this platform.\nThe command '/bin/sh -c pip install pandas-0.22.0-cp36-cp36m-manylinux1_x86_64.whl' returned a non-zero code: 1\n"
"In [2]: read_csv('sample.csv', dtype={'ID': object})\nOut[2]: \n                           ID\n0  00013007854817840016671868\n1  00013007854817840016749251\n2  00013007854817840016754630\n3  00013007854817840016781876\n4  00013007854817840017028824\n5  00013007854817840017963235\n6  00013007854817840018860166\n\npd.read_csv('sample.csv', dtype = str)\n\n# lst of column names which needs to be string\nlst_str_cols = ['prefix', 'serial']\n# use dictionary comprehension to make dict of dtypes\ndict_dtypes = {x : 'str'  for x in lst_str_cols}\n# use dict on dtypes\npd.read_csv('sample.csv', dtype=dict_dtypes)\n"
"import pandas as pd\niter_csv = pd.read_csv('file.csv', iterator=True, chunksize=1000)\ndf = pd.concat([chunk[chunk['field'] &gt; constant] for chunk in iter_csv])\n"
'nrows : int, default None\n\n    Number of rows of file to read. Useful for reading pieces of large files\n\nIn [1]: import pandas as pd\n\nIn [2]: time z = pd.read_csv("P00000001-ALL.csv", nrows=20)\nCPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s\nWall time: 0.00 s\n\nIn [3]: len(z)\nOut[3]: 20\n\nIn [4]: time z = pd.read_csv("P00000001-ALL.csv")\nCPU times: user 27.63 s, sys: 1.92 s, total: 29.55 s\nWall time: 30.23 s\n'
"In [31]: df\nOut[31]: \n   a        time\n0  1  2013-01-01\n1  2  2013-01-02\n2  3  2013-01-03\n\nIn [32]: df['time'] = df['time'].astype('datetime64[ns]')\n\nIn [33]: df\nOut[33]: \n   a                time\n0  1 2013-01-01 00:00:00\n1  2 2013-01-02 00:00:00\n2  3 2013-01-03 00:00:00\n"
'df = data.groupby(...).agg(...)\ndf.columns = df.columns.droplevel(0)\n\ndf.columns = ["_".join(x) for x in df.columns.ravel()]\n\nimport pandas as pd\nimport pandas.rpy.common as com\nimport numpy as np\n\ndata = com.load_data(\'Loblolly\')\nprint(data.head())\n#     height  age Seed\n# 1     4.51    3  301\n# 15   10.89    5  301\n# 29   28.72   10  301\n# 43   41.74   15  301\n# 57   52.70   20  301\n\ndf = data.groupby(\'Seed\').agg(\n    {\'age\':[\'sum\'],\n     \'height\':[\'mean\', \'std\']})\nprint(df.head())\n#       age     height           \n#       sum        std       mean\n# Seed                           \n# 301    78  22.638417  33.246667\n# 303    78  23.499706  34.106667\n# 305    78  23.927090  35.115000\n# 307    78  22.222266  31.328333\n# 309    78  23.132574  33.781667\n\ndf.columns = df.columns.droplevel(0)\nprint(df.head())\n\n      sum        std       mean\nSeed                           \n301    78  22.638417  33.246667\n303    78  23.499706  34.106667\n305    78  23.927090  35.115000\n307    78  22.222266  31.328333\n309    78  23.132574  33.781667\n\ndf = data.groupby(\'Seed\').agg(\n    {\'age\':[\'sum\'],\n     \'height\':[\'mean\', \'std\']})\ndf.columns = ["_".join(x) for x in df.columns.ravel()]\n\n      age_sum   height_std  height_mean\nSeed                           \n301        78    22.638417    33.246667\n303        78    23.499706    34.106667\n305        78    23.927090    35.115000\n307        78    22.222266    31.328333\n309        78    23.132574    33.781667\n'
"import pandas as pd\n\na = ['2015-01-01' , '2015-02-01']\n\ndf = pd.DataFrame(data={'date':['2015-01-01' , '2015-02-01', '2015-03-01' , '2015-04-01', '2015-05-01' , '2015-06-01']})\n\nprint(df)\n#         date\n#0  2015-01-01\n#1  2015-02-01\n#2  2015-03-01\n#3  2015-04-01\n#4  2015-05-01\n#5  2015-06-01\n\ndf = df[~df['date'].isin(a)]\n\nprint(df)\n#         date\n#2  2015-03-01\n#3  2015-04-01\n#4  2015-05-01\n#5  2015-06-01\n"
"df = df[(df['closing_price'] &gt;= 99) &amp; (df['closing_price'] &lt;= 101)]\n"
"In [106]:\ndf.replace('N/A',np.NaN)\n\nOut[106]:\n    x    y\n0  10   12\n1  50   11\n2  18  NaN\n3  32   13\n4  47   15\n5  20  NaN\n\nIn [108]:\ndf.loc[df['y'] == 'N/A','y'] = np.nan\ndf\n\nOut[108]:\n    x    y\n0  10   12\n1  50   11\n2  18  NaN\n3  32   13\n4  47   15\n5  20  NaN\n"
"In [18]: a\nOut[18]: \n   x1  x2\n0   0   5\n1   1   6\n2   2   7\n3   3   8\n4   4   9\n\nIn [19]: a['x2'] = a.x2.shift(1)\n\nIn [20]: a\nOut[20]: \n   x1  x2\n0   0 NaN\n1   1   5\n2   2   6\n3   3   7\n4   4   8\n"
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({"A": [1,2,3], "B": [-2, 8, 1]})\n&gt;&gt;&gt; df\n   A  B\n0  1 -2\n1  2  8\n2  3  1\n&gt;&gt;&gt; df[["A", "B"]]\n   A  B\n0  1 -2\n1  2  8\n2  3  1\n&gt;&gt;&gt; df[["A", "B"]].max(axis=1)\n0    1\n1    8\n2    3\n\n&gt;&gt;&gt; df["C"] = df[["A", "B"]].max(axis=1)\n&gt;&gt;&gt; df\n   A  B  C\n0  1 -2  1\n1  2  8  8\n2  3  1  3\n\n&gt;&gt;&gt; df["C"] = df.max(axis=1)\n'
"data.columns = map(str.lower, data.columns)\n\ndata.columns = [x.lower() for x in data.columns]\n\n&gt;&gt;&gt; data = pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})\n&gt;&gt;&gt; data\n   A  B  C\n0  0  3  a\n1  1  2  b\n2  2  1  c\n&gt;&gt;&gt; data.columns = map(str.lower, data.columns)\n&gt;&gt;&gt; data\n   a  b  c\n0  0  3  a\n1  1  2  b\n2  2  1  c\n"
"In [7]: from pandas import DataFrame\n\nIn [8]: from numpy.random import randint\n\nIn [9]: df = DataFrame({'a': randint(3, size=10)})\n\nIn [10]:\n\nIn [10]: df\nOut[10]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [11]: s = df.a[:5]\n\nIn [12]: dfa, sa = df.align(s, axis=0)\n\nIn [13]: dfa\nOut[13]:\n   a\n0  0\n1  2\n2  0\n3  1\n4  0\n5  0\n6  0\n7  0\n8  0\n9  0\n\nIn [14]: sa\nOut[14]:\n0     0\n1     2\n2     0\n3     1\n4     0\n5   NaN\n6   NaN\n7   NaN\n8   NaN\n9   NaN\nName: a, dtype: float64\n"
'In [4]: t = pd.date_range(start="2013-05-18 12:00:00", periods=2, freq=\'H\',\n                          tz= "Europe/Brussels")\n\nIn [5]: t\nOut[5]: DatetimeIndex([\'2013-05-18 12:00:00+02:00\', \'2013-05-18 13:00:00+02:00\'],\n                       dtype=\'datetime64[ns, Europe/Brussels]\', freq=\'H\')\n\nIn [6]: t.tz_localize(None)\nOut[6]: DatetimeIndex([\'2013-05-18 12:00:00\', \'2013-05-18 13:00:00\'], \n                      dtype=\'datetime64[ns]\', freq=\'H\')\n\nIn [7]: t.tz_convert(None)\nOut[7]: DatetimeIndex([\'2013-05-18 10:00:00\', \'2013-05-18 11:00:00\'], \n                      dtype=\'datetime64[ns]\', freq=\'H\')\n\nIn [31]: t = pd.date_range(start="2013-05-18 12:00:00", periods=10000, freq=\'H\',\n                           tz="Europe/Brussels")\n\nIn [32]: %timeit t.tz_localize(None)\n1000 loops, best of 3: 233 µs per loop\n\nIn [33]: %timeit pd.DatetimeIndex([i.replace(tzinfo=None) for i in t])\n10 loops, best of 3: 99.7 ms per loop\n'
'import pandas as pd\nimport numpy as np\n\nshape = (50, 4460)\n\ndata = np.random.normal(size=shape)\n\ndata[:, 1000] += data[:, 2000]\n\ndf = pd.DataFrame(data)\n\nc = df.corr().abs()\n\ns = c.unstack()\nso = s.sort_values(kind="quicksort")\n\nprint so[-4470:-4460]\n\n2192  1522    0.636198\n1522  2192    0.636198\n3677  2027    0.641817\n2027  3677    0.641817\n242   130     0.646760\n130   242     0.646760\n1171  2733    0.670048\n2733  1171    0.670048\n1000  2000    0.742340\n2000  1000    0.742340\ndtype: float64\n'
"df.Temp_Rating.fillna(df.Farheit, inplace=True)\ndel df['Farheit']\ndf.columns = 'File heat Observations'.split()\n"
'In [19]: df2 = df2.reset_index(drop=True)\n\nIn [20]: df2\nOut[20]:\n   x  y\n0  0  0\n1  0  1\n2  0  2\n3  1  0\n4  1  1\n5  1  2\n6  2  0\n7  2  1\n8  2  2\n'
"def f(row):\n    if row['A'] == row['B']:\n        val = 0\n    elif row['A'] &gt; row['B']:\n        val = 1\n    else:\n        val = -1\n    return val\n\nIn [1]: df['C'] = df.apply(f, axis=1)\n\nIn [2]: df\nOut[2]:\n   A  B  C\na  2  2  0\nb  3  1  1\nc  1  3 -1\n\ndf['C'] = np.where(\n    df['A'] == df['B'], 0, np.where(\n    df['A'] &gt;  df['B'], 1, -1)) \n"
'import pandas as pd\nimport numpy as np\n\ndf = df.fillna(value=np.nan)\n\ndf.mycol.fillna(value=np.nan, inplace=True)\n'
"import matplotlib.pyplot as plt\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nfig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\nax.scatter(df['carat'], df['price'], c=df['color'].apply(lambda x: colors[x]))\n\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport pandas as pd\n\ncarat = [5, 10, 20, 30, 5, 10, 20, 30, 5, 10, 20, 30]\nprice = [100, 100, 200, 200, 300, 300, 400, 400, 500, 500, 600, 600]\ncolor =['D', 'D', 'D', 'E', 'E', 'E', 'F', 'F', 'F', 'G', 'G', 'G',]\n\ndf = pd.DataFrame(dict(carat=carat, price=price, color=color))\n\nsns.lmplot('carat', 'price', data=df, hue='color', fit_reg=False)\n\nplt.show()\n\nfig, ax = plt.subplots()\n\ncolors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n\ngrouped = df.groupby('color')\nfor key, group in grouped:\n    group.plot(ax=ax, kind='scatter', x='carat', y='price', label=key, color=colors[key])\n\nplt.show()\n"
"In [119]:\n\ndf['text'] = df[['name','text','month']].groupby(['name','month'])['text'].transform(lambda x: ','.join(x))\ndf[['name','text','month']].drop_duplicates()\nOut[119]:\n    name         text  month\n0  name1       hej,du     11\n2  name1        aj,oj     12\n4  name2     fin,katt     11\n6  name2  mycket,lite     12\n\nIn [124]:\n\ndf.groupby(['name','month'])['text'].apply(lambda x: ','.join(x)).reset_index()\n\nOut[124]:\n    name  month         text\n0  name1     11       hej,du\n1  name1     12        aj,oj\n2  name2     11     fin,katt\n3  name2     12  mycket,lite\n\nIn[38]:\ndf.groupby(['name','month'])['text'].apply(','.join).reset_index()\n\nOut[38]: \n    name  month         text\n0  name1     11           du\n1  name1     12        aj,oj\n2  name2     11     fin,katt\n3  name2     12  mycket,lite\n"
"import pandas as pd\nimport numpy as np\ns = pd.Series(['apple', np.nan, 'banana'])\npd.isnull(s)\nOut[9]: \n0    False\n1     True\n2    False\ndtype: bool\n\nIn [24]: s = Series([Timestamp('20130101'),np.nan,Timestamp('20130102 9:30')],dtype='M8[ns]')\n\nIn [25]: s\nOut[25]: \n0   2013-01-01 00:00:00\n1                   NaT\n2   2013-01-02 09:30:00\ndtype: datetime64[ns]``\n\nIn [26]: pd.isnull(s)\nOut[26]: \n0    False\n1     True\n2    False\ndtype: bool\n"
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from StringIO import StringIO\n&gt;&gt;&gt; s = """1, 2\n... 3, 4\n... 5, 6"""\n&gt;&gt;&gt; pd.read_csv(StringIO(s), skiprows=[1], header=None)\n   0  1\n0  1  2\n1  5  6\n&gt;&gt;&gt; pd.read_csv(StringIO(s), skiprows=1, header=None)\n   0  1\n0  3  4\n1  5  6\n'
"In [27]:\n# get a list of columns\ncols = list(df)\n# move the column to head of list using index, pop and insert\ncols.insert(0, cols.pop(cols.index('Mid')))\ncols\nOut[27]:\n['Mid', 'Net', 'Upper', 'Lower', 'Zsore']\nIn [28]:\n# use ix to reorder\ndf = df.ix[:, cols]\ndf\nOut[28]:\n                      Mid Net  Upper   Lower  Zsore\nAnswer_option                                      \nMore_than_once_a_day    2  0%  0.22%  -0.12%     65\nOnce_a_day              3  0%  0.32%  -0.19%     45\nSeveral_times_a_week    4  2%  2.45%   1.10%     78\nOnce_a_week             6  1%  1.63%  -0.40%     65\n\nIn [39]:\nmid = df['Mid']\ndf.drop(labels=['Mid'], axis=1,inplace = True)\ndf.insert(0, 'Mid', mid)\ndf\nOut[39]:\n                      Mid Net  Upper   Lower  Zsore\nAnswer_option                                      \nMore_than_once_a_day    2  0%  0.22%  -0.12%     65\nOnce_a_day              3  0%  0.32%  -0.19%     45\nSeveral_times_a_week    4  2%  2.45%   1.10%     78\nOnce_a_week             6  1%  1.63%  -0.40%     65\n\ndf = df.loc[:, cols]\n"
'%matplotlib inline\nimport matplotlib.pyplot as plt\n'
"df['A'] = pd.to_datetime(df['A'])\ndf['B'] = pd.to_datetime(df['B'])\n\nIn [11]: df.dtypes  # if already datetime64 you don't need to use to_datetime\nOut[11]:\nA    datetime64[ns]\nB    datetime64[ns]\ndtype: object\n\nIn [12]: df['A'] - df['B']\nOut[12]:\none   -58 days\ntwo   -26 days\ndtype: timedelta64[ns]\n\nIn [13]: df['C'] = df['A'] - df['B']\n\nIn [14]: df\nOut[14]:\n             A          B        C\none 2014-01-01 2014-02-28 -58 days\ntwo 2014-02-03 2014-03-01 -26 days\n"
'&gt;&gt;&gt; df = pd.DataFrame([list(range(5))], columns=["a{}".format(i) for i in range(5)])\n&gt;&gt;&gt; df\n   a0  a1  a2  a3  a4\n0   0   1   2   3   4\n&gt;&gt;&gt; df.iloc[0]\na0    0\na1    1\na2    2\na3    3\na4    4\nName: 0, dtype: int64\n&gt;&gt;&gt; type(_)\n&lt;class \'pandas.core.series.Series\'&gt;\n'
'import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r&quot;&quot;&quot;dummy,date,loc,x\nbar,20090101,a,1\nbar,20090102,a,3\nbar,20090103,a,5\nbar,20090101,b,1\nbar,20090102,b,3\nbar,20090103,b,5&quot;&quot;&quot;\n\ndf = pd.read_csv(StringIO(csv),\n        header=0,\n        index_col=[&quot;date&quot;, &quot;loc&quot;], \n        usecols=[&quot;date&quot;, &quot;loc&quot;, &quot;x&quot;],\n        parse_dates=[&quot;date&quot;])\n\n                x\ndate       loc\n2009-01-01 a    1\n2009-01-02 a    3\n2009-01-03 a    5\n2009-01-01 b    1\n2009-01-02 b    3\n2009-01-03 b    5\n'
'area_dict = dict(zip(lakes.area, lakes.count))\n'
'read_csv(..., nrows=999999)\n\nread_csv(..., skiprows=1000000, nrows=999999)\n'
"import numpy as np\nindex = df['b'].index[df['b'].apply(np.isnan)]\n\ndf['a'].ix[index[0]]\n&gt;&gt;&gt; 1.452354\n\ndf_index = df.index.values.tolist()\n[df_index.index(i) for i in index]\n&gt;&gt;&gt; [3, 6]\n"
"import pandas as pd\nprint pd.read_csv('value.txt')\n\n        Date    price  factor_1  factor_2\n0  2012-06-11  1600.20     1.255     1.548\n1  2012-06-12  1610.02     1.258     1.554\n2  2012-06-13  1618.07     1.249     1.552\n3  2012-06-14  1624.40     1.253     1.556\n4  2012-06-15  1626.15     1.258     1.552\n5  2012-06-16  1626.15     1.263     1.558\n6  2012-06-17  1626.15     1.264     1.572\n"
"#  without forcing dtype it changes None to NaN!\ns_bad = pd.Series([1, None], dtype=object)\ns_good = pd.Series([1, np.nan])\n\nIn [13]: s_bad.dtype\nOut[13]: dtype('O')\n\nIn [14]: s_good.dtype\nOut[14]: dtype('float64')\n\nIn [15]: s_bad.sum()\nOut[15]: 1\n\nIn [16]: s_good.sum()\nOut[16]: 1.0\n"
'In [5]: df\nOut[5]: \n   Year  Month   Value\n0     1       2      3\n\n[1 rows x 3 columns]\n\nIn [6]: df.rename(columns=lambda x: x.strip())\nOut[6]: \n   Year  Month  Value\n0     1      2      3\n\n[1 rows x 3 columns]\n\ndf.rename(columns=lambda x: x.strip(), inplace=True)\n\ndf = df.rename(columns=lambda x: x.strip())\n'
'1) vectorization\n2) using a custom cython routine\n3) apply\n    a) reductions that can be performed in cython\n    b) iteration in python space\n4) itertuples\n5) iterrows\n6) updating an empty frame (e.g. using loc one-row-at-a-time)\n'
"sns.heatmap(table2,annot=True,cmap='Blues', fmt='g')\n"
"df1.loc[df1['stream'] == 2, ['feat','another_feat']] = 'aaaa'\nprint df1\n   stream        feat another_feat\na       1  some_value   some_value\nb       2        aaaa         aaaa\nc       2        aaaa         aaaa\nd       3  some_value   some_value\n\ndf1.loc[df1['stream'] == 2, 'feat'] = 10\nprint df1\n   stream        feat another_feat\na       1  some_value   some_value\nb       2          10   some_value\nc       2          10   some_value\nd       3  some_value   some_value\n\ndf1['feat'] = np.where(df1['stream'] == 2, 10,20)\nprint df1\n   stream  feat another_feat\na       1    20   some_value\nb       2    10   some_value\nc       2    10   some_value\nd       3    20   some_value\n\nprint df1\n   stream  feat  another_feat\na       1     4             5\nb       2     4             5\nc       2     2             9\nd       3     1             7\n\n#filter columns all without stream\ncols = [col for col in df1.columns if col != 'stream']\nprint cols\n['feat', 'another_feat']\n\ndf1.loc[df1['stream'] == 2, cols ] = df1 / 2\nprint df1\n   stream  feat  another_feat\na       1   4.0           5.0\nb       2   2.0           2.5\nc       2   1.0           4.5\nd       3   1.0           7.0\n\ndf0 = pd.DataFrame({'Col':[5,0,-6]})\n\ndf0['New Col1'] = np.where((df0['Col'] &gt; 0), 'Increasing', \n                          np.where((df0['Col'] &lt; 0), 'Decreasing', 'No Change'))\n\ndf0['New Col2'] = np.select([df0['Col'] &gt; 0, df0['Col'] &lt; 0],\n                            ['Increasing',  'Decreasing'], \n                            default='No Change')\n\nprint (df0)\n   Col    New Col1    New Col2\n0    5  Increasing  Increasing\n1    0   No Change   No Change\n2   -6  Decreasing  Decreasing\n"
'import pandas as pd\nfrom pymongo import MongoClient\n\n\ndef _connect_mongo(host, port, username, password, db):\n    """ A util for making a connection to mongo """\n\n    if username and password:\n        mongo_uri = \'mongodb://%s:%s@%s:%s/%s\' % (username, password, host, port, db)\n        conn = MongoClient(mongo_uri)\n    else:\n        conn = MongoClient(host, port)\n\n\n    return conn[db]\n\n\ndef read_mongo(db, collection, query={}, host=\'localhost\', port=27017, username=None, password=None, no_id=True):\n    """ Read from Mongo and Store into DataFrame """\n\n    # Connect to MongoDB\n    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n\n    # Make a query to the specific DB and Collection\n    cursor = db[collection].find(query)\n\n    # Expand the cursor and construct the DataFrame\n    df =  pd.DataFrame(list(cursor))\n\n    # Delete the _id\n    if no_id:\n        del df[\'_id\']\n\n    return df\n'
"b = pd.read_csv('b.dat')\nb.index = pd.to_datetime(b['date'],format='%m/%d/%y %I:%M%p')\nb.groupby(by=[b.index.month, b.index.year])\n\nb.groupby(pd.Grouper(freq='M'))  # update for v0.21+\n"
'In [5]:\n\nimport pandas as pd\n\ntest = {\n383:    3.000000,\n663:    1.000000,\n726:    1.000000,\n737:    9.000000,\n833:    8.166667\n}\n\ns = pd.Series(test)\ns = s[s != 1]\ns\nOut[0]:\n383    3.000000\n737    9.000000\n833    8.166667\ndtype: float64\n'
'import matplotlib\nmatplotlib.style.use(\'ggplot\')\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndf = pd.DataFrame({ \'celltype\':["foo","bar","qux","woz"], \'s1\':[5,9,1,7], \'s2\':[12,90,13,87]})\ndf = df[["celltype","s1","s2"]]\ndf.set_index(["celltype"],inplace=True)\ndf.plot(kind=\'bar\',alpha=0.75, rot=0)\nplt.xlabel("")\nplt.show()\n'
"import pandas as pd\n\nd1 = pd.DataFrame(dict(A=.1, B=.2, C=.3), index=[2, 3])\nd2 = pd.DataFrame(dict(B=.4, C=.5, D=.6), index=[1, 2])\nd3 = pd.DataFrame(dict(A=.7, B=.8, D=.9), index=[1, 3])\n\ns1 = pd.Series([1, 2], index=[2, 3])\ns2 = pd.Series([3, 4], index=[1, 2])\ns3 = pd.Series([5, 6], index=[1, 3])\n\npd.concat([d1, d2])\n\n     A    B    C    D\n2  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6\n\npd.concat([d1, d2], axis=0)\n\n     A    B    C    D\n2  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6\n\npd.concat([d1, d2], axis=1)\n\n     A    B    C    B    C    D\n1  NaN  NaN  NaN  0.4  0.5  0.6\n2  0.1  0.2  0.3  0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN\n\n               |                       |                        |  pd.concat(\n               |  pd.concat(           |  pd.concat(            |      [s1.rename('A'),\n pd.concat(    |      [s1.rename('A'), |      [s1.rename('A'),  |       s2.rename('B'),\n     [s1, s2]) |       s2])            |       s2.rename('A')]) |       s3.rename('A')])\n-------------- | --------------------- | ---------------------- | ----------------------\n2    1         | 2    1                | 2    1                 | 2    1\n3    2         | 3    2                | 3    2                 | 3    2\n1    3         | 1    3                | 1    3                 | 1    3\n2    4         | 2    4                | 2    4                 | 2    4\ndtype: int64   | dtype: int64          | Name: A, dtype: int64  | 1    5\n               |                       |                        | 3    6\n               |                       |                        | dtype: int64\n\n                       |                       |  pd.concat(\n                       |  pd.concat(           |      [s1.rename('X'),\n pd.concat(            |      [s1.rename('X'), |       s2.rename('Y'),\n     [s1, s2], axis=1) |       s2], axis=1)    |       s3.rename('Z')], axis=1)\n---------------------- | --------------------- | ------------------------------\n     0    1            |      X    0           |      X    Y    Z\n1  NaN  3.0            | 1  NaN  3.0           | 1  NaN  3.0  5.0\n2  1.0  4.0            | 2  1.0  4.0           | 2  1.0  4.0  NaN\n3  2.0  NaN            | 3  2.0  NaN           | 3  2.0  NaN  6.0\n\n pd.concat(               |\n     [s1.to_frame(), d1]) |  pd.concat([s1, d1])\n------------------------- | ---------------------\n     0    A    B    C     |      0    A    B    C\n2  1.0  NaN  NaN  NaN     | 2  1.0  NaN  NaN  NaN\n3  2.0  NaN  NaN  NaN     | 3  2.0  NaN  NaN  NaN\n2  NaN  0.1  0.2  0.3     | 2  NaN  0.1  0.2  0.3\n3  NaN  0.1  0.2  0.3     | 3  NaN  0.1  0.2  0.3\n\n # Effectively renames       |                            |\n # `s1` but does not align   |  # Does not rename.  So    |  # Renames to something\n # with columns in `d1`      |  # Pandas defaults to `0`  |  # that does align with `d1`\n pd.concat(                  |  pd.concat(                |  pd.concat(\n     [s1.to_frame('X'), d1]) |      [s1.rename('X'), d1]) |      [s1.to_frame('B'), d1])\n---------------------------- | -------------------------- | ----------------------------\n     A    B    C    X        |      0    A    B    C      |      A    B    C\n2  NaN  NaN  NaN  1.0        | 2  1.0  NaN  NaN  NaN      | 2  NaN  1.0  NaN\n3  NaN  NaN  NaN  2.0        | 3  2.0  NaN  NaN  NaN      | 3  NaN  2.0  NaN\n2  0.1  0.2  0.3  NaN        | 2  NaN  0.1  0.2  0.3      | 2  0.1  0.2  0.3\n3  0.1  0.2  0.3  NaN        | 3  NaN  0.1  0.2  0.3      | 3  0.1  0.2  0.3\n\n                    |  pd.concat(\n pd.concat(         |      [s1.rename('X'),\n     [s1, d1],      |       s2, s3, d1],\n     axis=1)        |      axis=1)\n------------------- | -------------------------------\n   0    A    B    C |      X    0    1    A    B    C\n2  1  0.1  0.2  0.3 | 1  NaN  3.0  5.0  NaN  NaN  NaN\n3  2  0.1  0.2  0.3 | 2  1.0  4.0  NaN  0.1  0.2  0.3\n                    | 3  2.0  NaN  6.0  0.1  0.2  0.3\n\npd.concat([d1, d2], axis=1, join='outer')\n\n     A    B    C    B    C    D\n1  NaN  NaN  NaN  0.4  0.5  0.6\n2  0.1  0.2  0.3  0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN\n\npd.concat([d1, d2], axis=1, join='inner')\n\n     A    B    C    B    C    D\n2  0.1  0.2  0.3  0.4  0.5  0.6\n\npd.concat([d1, d2, d3], axis=1, join_axes=[d1.index])\n\n     A    B    C    B    C    D    A    B    D\n2  0.1  0.2  0.3  0.4  0.5  0.6  NaN  NaN  NaN\n3  0.1  0.2  0.3  NaN  NaN  NaN  0.7  0.8  0.9\n\npd.concat([d1, d2, d3], axis=1, join_axes=[d3.index])\n\n     A    B    C    B    C    D    A    B    D\n1  NaN  NaN  NaN  0.4  0.5  0.6  0.7  0.8  0.9\n3  0.1  0.2  0.3  NaN  NaN  NaN  0.7  0.8  0.9\n\n                      |  pd.concat(             |  pd.concat(\n                      |      [d1, d2],          |      [d1, d2]\n pd.concat([d1, d2])  |      ignore_index=True) |  ).reset_index(drop=True)\n--------------------- | ----------------------- | -------------------------\n     A    B    C    D |      A    B    C    D   |      A    B    C    D\n2  0.1  0.2  0.3  NaN | 0  0.1  0.2  0.3  NaN   | 0  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN | 1  0.1  0.2  0.3  NaN   | 1  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6 | 2  NaN  0.4  0.5  0.6   | 2  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6 | 3  NaN  0.4  0.5  0.6   | 3  NaN  0.4  0.5  0.6\n\n                                   |     pd.concat(\n                                   |         [d1, d2], axis=1,\n pd.concat([d1, d2], axis=1)       |         ignore_index=True)\n-------------------------------    |    -------------------------------\n     A    B    C    B    C    D    |         0    1    2    3    4    5\n1  NaN  NaN  NaN  0.4  0.5  0.6    |    1  NaN  NaN  NaN  0.4  0.5  0.6\n2  0.1  0.2  0.3  0.4  0.5  0.6    |    2  0.1  0.2  0.3  0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN    |    3  0.1  0.2  0.3  NaN  NaN  NaN\n\n #           length 3             length 3           #         length 2        length 2\n #          /--------\\         /-----------\\         #          /----\\         /------\\\n pd.concat([s1, s2, s3], keys=['A', 'B', 'C'])       pd.concat([s1, s2], keys=['A', 'B'])\n----------------------------------------------      -------------------------------------\nA  2    1                                           A  2    1\n   3    2                                              3    2\nB  1    3                                           B  1    3\n   2    4                                              2    4\nC  1    5                                           dtype: int64\n   3    6\ndtype: int64\n\n pd.concat(\n     [s1, s2, s3],\n     keys=[('A', 'X'), ('A', 'Y'), ('B', 'X')])\n-----------------------------------------------\nA  X  2    1\n      3    2\n   Y  1    3\n      2    4\nB  X  1    5\n      3    6\ndtype: int64\n\n               |                       |                        |  pd.concat(\n               |  pd.concat(           |  pd.concat(            |      [s1.rename('U'),\n pd.concat(    |      [s1, s2],        |      [s1.rename('U'),  |       s2.rename('V')],\n     [s1, s2], |      axis=1,          |       s2.rename('V')], |       axis=1,\n     axis=1)   |      keys=['X', 'Y']) |       axis=1)          |       keys=['X', 'Y'])\n-------------- | --------------------- | ---------------------- | ----------------------\n     0    1    |      X    Y           |      U    V            |      X    Y\n1  NaN  3.0    | 1  NaN  3.0           | 1  NaN  3.0            | 1  NaN  3.0\n2  1.0  4.0    | 2  1.0  4.0           | 2  1.0  4.0            | 2  1.0  4.0\n3  2.0  NaN    | 3  2.0  NaN           | 3  2.0  NaN            | 3  2.0  NaN\n\n pd.concat(\n     [s1, s2],\n     axis=1,\n     keys=[('W', 'X'), ('W', 'Y')])\n-----------------------------------\n     W\n     X    Y\n1  NaN  3.0\n2  1.0  4.0\n3  2.0  NaN\n\n pd.concat(                     |  pd.concat(\n     [d1, d2],                  |      [d1, d2],\n     axis=1,                    |      axis=1,\n     keys=['X', 'Y'])           |      keys=[('First', 'X'), ('Second', 'X')])\n------------------------------- | --------------------------------------------\n     X              Y           |   First           Second\n     A    B    C    B    C    D |       X                X\n1  NaN  NaN  NaN  0.4  0.5  0.6 |       A    B    C      B    C    D\n2  0.1  0.2  0.3  0.4  0.5  0.6 | 1   NaN  NaN  NaN    0.4  0.5  0.6\n3  0.1  0.2  0.3  NaN  NaN  NaN | 2   0.1  0.2  0.3    0.4  0.5  0.6\n                                | 3   0.1  0.2  0.3    NaN  NaN  NaN\n\n pd.concat(           |  pd.concat(\n     [s1, d1],        |      [s1.rename('Z'), d1],\n     axis=1,          |      axis=1,\n     keys=['X', 'Y']) |      keys=['X', 'Y'])\n--------------------- | --------------------------\n   X    Y             |    X    Y\n   0    A    B    C   |    Z    A    B    C\n2  1  0.1  0.2  0.3   | 2  1  0.1  0.2  0.3\n3  2  0.1  0.2  0.3   | 3  2  0.1  0.2  0.3\n\nd1_ = pd.concat(\n    [d1], axis=1,\n    keys=['One'])\nd1_\n\n   One\n     A    B    C\n2  0.1  0.2  0.3\n3  0.1  0.2  0.3\n\npd.concat([d1_, d2], axis=1)\n\n   (One, A)  (One, B)  (One, C)    B    C    D\n1       NaN       NaN       NaN  0.4  0.5  0.6\n2       0.1       0.2       0.3  0.4  0.5  0.6\n3       0.1       0.2       0.3  NaN  NaN  NaN\n\n # axis=0               |  # axis=1\n pd.concat(             |  pd.concat(\n     {0: d1, 1: d2})    |      {0: d1, 1: d2}, axis=1)\n----------------------- | -------------------------------\n       A    B    C    D |      0              1\n0 2  0.1  0.2  0.3  NaN |      A    B    C    B    C    D\n  3  0.1  0.2  0.3  NaN | 1  NaN  NaN  NaN  0.4  0.5  0.6\n1 1  NaN  0.4  0.5  0.6 | 2  0.1  0.2  0.3  0.4  0.5  0.6\n  2  NaN  0.4  0.5  0.6 | 3  0.1  0.2  0.3  NaN  NaN  NaN\n\ndf = pd.concat(\n    [d1, d2, d3], axis=1,\n    keys=['First', 'Second', 'Fourth'])\n\ndf\n\n  First           Second           Fourth\n      A    B    C      B    C    D      A    B    D\n1   NaN  NaN  NaN    0.4  0.5  0.6    0.7  0.8  0.9\n2   0.1  0.2  0.3    0.4  0.5  0.6    NaN  NaN  NaN\n3   0.1  0.2  0.3    NaN  NaN  NaN    0.7  0.8  0.9\n\nprint(df, *df.columns.levels, sep='\\n')\n\nIndex(['First', 'Second', 'Fourth'], dtype='object')\nIndex(['A', 'B', 'C', 'D'], dtype='object')\n\ndf.groupby(axis=1, level=0).sum()\n\n   First  Fourth  Second\n1    0.0     2.4     1.5\n2    0.6     0.0     1.5\n3    0.6     2.4     0.0\n\ncats = ['First', 'Second', 'Third', 'Fourth', 'Fifth']\nlvl = pd.CategoricalIndex(cats, categories=cats, ordered=True)\n\ndf = pd.concat(\n    [d1, d2, d3], axis=1,\n    keys=['First', 'Second', 'Fourth'],\n    levels=[lvl]\n)\n\ndf\n\n   First  Fourth  Second\n1    0.0     2.4     1.5\n2    0.6     0.0     1.5\n3    0.6     2.4     0.0\n\ndf.columns.levels[0]\n\nCategoricalIndex(\n    ['First', 'Second', 'Third', 'Fourth', 'Fifth'],\n    categories=['First', 'Second', 'Third', 'Fourth', 'Fifth'],\n    ordered=True, dtype='category')\n\ndf.groupby(axis=1, level=0).sum()\n\n   First  Second  Third  Fourth  Fifth\n1    0.0     1.5    0.0     2.4    0.0\n2    0.6     1.5    0.0     0.0    0.0\n3    0.6     0.0    0.0     2.4    0.0\n\n # axis=0                     |  # axis=1\n pd.concat(                   |  pd.concat(\n     [d1, d2],                |      [d1, d2],\n     keys=[0, 1],             |      axis=1, keys=[0, 1],\n     names=['lvl0', 'lvl1'])  |      names=['lvl0', 'lvl1'])\n----------------------------- | ----------------------------------\n             A    B    C    D | lvl0    0              1\nlvl0 lvl1                     | lvl1    A    B    C    B    C    D\n0    2     0.1  0.2  0.3  NaN | 1     NaN  NaN  NaN  0.4  0.5  0.6\n     3     0.1  0.2  0.3  NaN | 2     0.1  0.2  0.3  0.4  0.5  0.6\n1    1     NaN  0.4  0.5  0.6 | 3     0.1  0.2  0.3  NaN  NaN  NaN\n     2     NaN  0.4  0.5  0.6 |\n\npd.concat([d1, d2])\n\n     A    B    C    D\n2  0.1  0.2  0.3  NaN\n3  0.1  0.2  0.3  NaN\n1  NaN  0.4  0.5  0.6\n2  NaN  0.4  0.5  0.6\n\npd.concat([d1, d2], verify_integrity=True)\n"
'In [1]: from pandas import *\n\nIn [2]: def calculate(x):\n   ...:     return x*2, x*3\n   ...: \n\nIn [3]: df = DataFrame({\'a\': [1,2,3], \'b\': [2,3,4]})\n\nIn [4]: df\nOut[4]: \n   a  b\n0  1  2\n1  2  3\n2  3  4\n\nIn [5]: df["A1"], df["A2"] = zip(*df["a"].map(calculate))\n\nIn [6]: df\nOut[6]: \n   a  b  A1  A2\n0  1  2   2   3\n1  2  3   4   6\n2  3  4   6   9\n'
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; m = np.arange(16)*10\n&gt;&gt;&gt; m[df.A]\narray([  0,  40,  50,  60, 150, 150, 140, 130])\n&gt;&gt;&gt; df["D"] = m[df.A]\n&gt;&gt;&gt; df\n    A   B   C    D\n0   0 NaN NaN    0\n1   4 NaN NaN   40\n2   5 NaN NaN   50\n3   6 NaN NaN   60\n4  15 NaN NaN  150\n5  15 NaN NaN  150\n6  14 NaN NaN  140\n7  13 NaN NaN  130\n'
'pandas.read_csv(filename, sep=\'\\t\', lineterminator=\'\\r\')\n\nimport codecs\n\ndoc = codecs.open(\'document\',\'rU\',\'UTF-16\') #open for reading with "universal" type set\n\ndf = pandas.read_csv(doc, sep=\'\\t\')\n'
"import pandas as pd\n\ndata = pd.read_json('/path/to/file.json', lines=True)\n"
"In [17]: df['Cat1'].fillna(df['Cat2'])\nOut[17]:\n0    cat\n1    dog\n2    cat\n3    ant\nName: Cat1, dtype: object\n"
"df.loc[0, 'C'] = df.loc[0, 'D']\n\nfor i in range(1, len(df)):\n    df.loc[i, 'C'] = df.loc[i-1, 'C'] * df.loc[i, 'A'] + df.loc[i, 'B']\n\n\n  Index_Date   A   B    C    D\n0 2015-01-31  10  10   10   10\n1 2015-02-01   2   3   23   22\n2 2015-02-02  10  60  290  280\n"
'Alias    Description\nB        business day frequency\nC        custom business day frequency\nD        calendar day frequency\nW        weekly frequency\nM        month end frequency\nSM       semi-month end frequency (15th and end of month)\nBM       business month end frequency\nCBM      custom business month end frequency\nMS       month start frequency\nSMS      semi-month start frequency (1st and 15th)\nBMS      business month start frequency\nCBMS     custom business month start frequency\nQ        quarter end frequency\nBQ       business quarter end frequency\nQS       quarter start frequency\nBQS      business quarter start frequency\nA, Y     year end frequency\nBA, BY   business year end frequency\nAS, YS   year start frequency\nBAS, BYS business year start frequency\nBH       business hour frequency\nH        hourly frequency\nT, min   minutely frequency\nS        secondly frequency\nL, ms    milliseconds\nU, us    microseconds\nN        nanoseconds\n'
"In [10]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])\n\nIn [11]: df\nOut[11]:\n   A  B\n0  1  2\n1  3  4\n\nIn [12]: df[['A']]\n\nIn [13]: df[[0]]\n\nIn [14]: df.loc[:, ['A']]\n\nIn [15]: df.iloc[:, [0]]\n\nOut[12-15]:  # they all return the same thing:\n   A\n0  1\n1  3\n\nIn [16]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 0])\n\nIn [17]: df\nOut[17]:\n   A  0\n0  1  2\n1  3  4\n\nIn [18]: df[[0]]  # ambiguous\nOut[18]:\n   A\n0  1\n1  3\n"
"df.plot(x='col_name_1', y='col_name_2', style='o')\n\nimport numpy as np\nimport pandas as pd\n\nd = {'one' : np.random.rand(10),\n     'two' : np.random.rand(10)}\n\ndf = pd.DataFrame(d)\n\ndf.plot(style=['o','rx'])\n"
'&gt;&gt;&gt; df\n         date  duration user_id\n0  2013-04-01        30    0001\n1  2013-04-01        15    0001\n2  2013-04-01        20    0002\n3  2013-04-02        15    0002\n4  2013-04-02        30    0002\n&gt;&gt;&gt; df.groupby("date").agg({"duration": np.sum, "user_id": pd.Series.nunique})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n&gt;&gt;&gt; df.groupby("date").agg({"duration": np.sum, "user_id": lambda x: x.nunique()})\n            duration  user_id\ndate                         \n2013-04-01        65        2\n2013-04-02        45        1\n'
'In [2]: type(df.loc[[3]])\nOut[2]: pandas.core.frame.DataFrame\n\nIn [3]: type(df.loc[[1]])\nOut[3]: pandas.core.frame.DataFrame\n'
"df = df[cols_of_interest]\n\ndf.drop(df.ix[:,'Unnamed: 24':'Unnamed: 60'].head(0).columns, axis=1)\n\nIn [2]:\ndf = pd.DataFrame(columns=['a','Unnamed: 1', 'Unnamed: 1','foo'])\ndf\n\nOut[2]:\nEmpty DataFrame\nColumns: [a, Unnamed: 1, Unnamed: 1, foo]\nIndex: []\n\nIn [4]:\n~df.columns.str.contains('Unnamed:')\n\nOut[4]:\narray([ True, False, False,  True], dtype=bool)\n\nIn [5]:\ndf[df.columns[~df.columns.str.contains('Unnamed:')]]\n\nOut[5]:\nEmpty DataFrame\nColumns: [a, foo]\nIndex: []\n"
'pd.qcut(factors, 5).value_counts()\n\n[-2.578, -0.829]    6\n(-0.829, -0.36]     6\n(-0.36, 0.366]      6\n(0.366, 0.868]      6\n(0.868, 2.617]      6\n\npd.cut(factors, 5).value_counts()\n\n(-2.583, -1.539]    5\n(-1.539, -0.5]      5\n(-0.5, 0.539]       9\n(0.539, 1.578]      9\n(1.578, 2.617]      2\n'
"In [59]: (df.groupby(['cluster', 'org'], as_index=False).mean()\n            .groupby('cluster')['time'].mean())\nOut[59]:\ncluster\n1          15\n2          54\n3           6\nName: time, dtype: int64\n\nIn [58]: df.groupby(['cluster']).mean()\nOut[58]:\n              time\ncluster\n1        12.333333\n2        54.000000\n3         6.000000\n\nIn [57]: df.groupby(['cluster', 'org']).mean()\nOut[57]:\n               time\ncluster org\n1       a    438886\n        c        23\n2       d      9874\n        h        34\n3       w         6\n"
'&gt;&gt;&gt; import pandas\n&gt;&gt;&gt; my_series = pandas.Series([1,2,2,3,3,3, "fred", 1.8, 1.8])\n&gt;&gt;&gt; my_series\n0       1\n1       2\n2       2\n3       3\n4       3\n5       3\n6    fred\n7     1.8\n8     1.8\n&gt;&gt;&gt; counts = my_series.value_counts()\n&gt;&gt;&gt; counts\n3       3\n2       2\n1.8     2\nfred    1\n1       1\n&gt;&gt;&gt; len(counts)\n5\n&gt;&gt;&gt; sum(counts)\n9\n&gt;&gt;&gt; counts["fred"]\n1\n&gt;&gt;&gt; dict(counts)\n{1.8: 2, 2: 2, 3: 3, 1: 1, \'fred\': 1}\n'
"df.index.values.tolist()  # an ndarray method, you probably shouldn't depend on this\n\nlist(df.index.values)  # this will always work in pandas\n"
"cols_to_use = df2.columns.difference(df.columns)\n\ndfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')\n"
'%matplotlib inline\nimport time\nimport pylab as pl\nfrom IPython import display\nfor i in range(10):\n    pl.plot(pl.randn(100))\n    display.clear_output(wait=True)\n    display.display(pl.gcf())\n    time.sleep(1.0)\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame( {'Symbol':['A','A','A'] ,\n    'Date':['02/20/2015','01/15/2016','08/21/2015']})\n&gt;&gt;&gt; df\n         Date Symbol\n0  02/20/2015      A\n1  01/15/2016      A\n2  08/21/2015      A\n&gt;&gt;&gt; df['Date'] =pd.to_datetime(df.Date)\n&gt;&gt;&gt; df.sort('Date') # This now sorts in date order\n        Date Symbol\n0 2015-02-20      A\n2 2015-08-21      A\n1 2016-01-15      A\n\n&gt;&gt;&gt; df.sort_values(by='Date') # This now sorts in date order\n        Date Symbol\n0 2015-02-20      A\n2 2015-08-21      A\n1 2016-01-15      A\n"
"df['colour'].value_counts().plot(kind='bar')\n"
'In [11]: df.groupby(["Group", "Size"]).size()\nOut[11]:\nGroup     Size\nModerate  Medium    1\n          Small     1\nShort     Small     2\nTall      Large     1\ndtype: int64\n\nIn [12]: df.groupby(["Group", "Size"]).size().reset_index(name="Time")\nOut[12]:\n      Group    Size  Time\n0  Moderate  Medium     1\n1  Moderate   Small     1\n2     Short   Small     2\n3      Tall   Large     1\n'
"In [46]:\ndf = pd.DataFrame({'a':[0,0,1,2,2,2], 'b':[1,2,3,4,np.NaN,4], 'c':np.random.randn(6)})\ndf\n\nOut[46]:\n   a   b         c\n0  0   1  1.067627\n1  0   2  0.554691\n2  1   3  0.458084\n3  2   4  0.426635\n4  2 NaN -2.238091\n5  2   4  1.256943\n\nIn [48]:\nprint(df.groupby(['a'])['b'].count())\nprint(df.groupby(['a'])['b'].size())\n\na\n0    2\n1    1\n2    2\nName: b, dtype: int64\n\na\n0    2\n1    1\n2    3\ndtype: int64 \n"
"In [1]: df\nOut[1]:\n            value  \ndatetime                         \n2010-01-01      1  \n2010-02-01      1  \n2009-01-01      1  \n\n# create additional month and year columns for convenience\ndf['Month'] = map(lambda x: x.month, df.index)\ndf['Year'] = map(lambda x: x.year, df.index)    \n\nIn [5]: df.groupby(['Month','Year']).mean().unstack()\nOut[5]:\n       value      \nYear    2009  2010\nMonth             \n1          1     1\n2        NaN     1\n\ndf.groupby(['Month','Year']).mean().unstack().plot()\n"
'ax = plt.gca()\nax.grid(True)\n'
"# sort the dataframe\ndf.sort_values(by='name', axis=1, inplace=True)\n\n# set the index to be this and don't drop\ndf.set_index(keys=['name'], drop=False,inplace=True)\n\n# get a list of names\nnames=df['name'].unique().tolist()\n\n# now we can perform a lookup on a 'view' of the dataframe\njoe = df.loc[df.name=='joe']\n\n# now you can query all 'joes'\n"
'&gt;&gt;&gt; df.groupby(pd.cut(df["B"], np.arange(0, 1.0+0.155, 0.155))).sum()\n                      A         B\nB                                \n(0, 0.155]     2.775458  0.246394\n(0.155, 0.31]  1.123989  0.471618\n(0.31, 0.465]  2.051814  1.882763\n(0.465, 0.62]  2.277960  1.528492\n(0.62, 0.775]  1.577419  2.810723\n(0.775, 0.93]  0.535100  1.694955\n(0.93, 1.085]       NaN       NaN\n\n[7 rows x 2 columns]\n'
"pd.DataFrame({'email':sf.index, 'list':sf.values})\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; \n&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(10, 2), columns=list('AB'))\n&gt;&gt;&gt; df['Tenant'] = np.random.choice(['Babar', 'Rataxes', ''], 10)\n&gt;&gt;&gt; print df\n\n          A         B   Tenant\n0 -0.588412 -1.179306    Babar\n1 -0.008562  0.725239         \n2  0.282146  0.421721  Rataxes\n3  0.627611 -0.661126    Babar\n4  0.805304 -0.834214         \n5 -0.514568  1.890647    Babar\n6 -1.188436  0.294792  Rataxes\n7  1.471766 -0.267807    Babar\n8 -1.730745  1.358165  Rataxes\n9  0.066946  0.375640         \n\n&gt;&gt;&gt; df['Tenant'].replace('', np.nan, inplace=True)\n&gt;&gt;&gt; print df\n\n          A         B   Tenant\n0 -0.588412 -1.179306    Babar\n1 -0.008562  0.725239      NaN\n2  0.282146  0.421721  Rataxes\n3  0.627611 -0.661126    Babar\n4  0.805304 -0.834214      NaN\n5 -0.514568  1.890647    Babar\n6 -1.188436  0.294792  Rataxes\n7  1.471766 -0.267807    Babar\n8 -1.730745  1.358165  Rataxes\n9  0.066946  0.375640      NaN\n\n&gt;&gt;&gt; df.dropna(subset=['Tenant'], inplace=True)\n&gt;&gt;&gt; print df\n\n          A         B   Tenant\n0 -0.588412 -1.179306    Babar\n2  0.282146  0.421721  Rataxes\n3  0.627611 -0.661126    Babar\n5 -0.514568  1.890647    Babar\n6 -1.188436  0.294792  Rataxes\n7  1.471766 -0.267807    Babar\n8 -1.730745  1.358165  Rataxes\n"
'import pandas as pd\nimport numpy as np\nfrom IPython.display import display, HTML\n\nCSS = """\n.output {\n    flex-direction: row;\n}\n"""\n\nHTML(\'&lt;style&gt;{}&lt;/style&gt;\'.format(CSS))\n\nCSS = """\ndiv.cell:nth-child(5) .output {\n    flex-direction: row;\n}\n"""\n'
"In [11]: df\nOut[11]:\n     0\nA B\n1 4  1\n2 5  2\n3 6  3\n\nIn [12]: df.iloc[df.index.get_level_values('A') == 1]\nOut[12]:\n     0\nA B\n1 4  1\n\ndf.xs(1, level='A', drop_level=False) # axis=1 if columns\n\nIn [21]: df1 = df.T\n\nIn [22]: df1.iloc[:, df1.columns.get_level_values('A') == 1]\nOut[22]:\nA  1\nB  4\n0  1\n"
"import pandas as pd\ndf = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], columns=['A'])\ndf.sort_index(inplace=True)\nprint(df.to_string())\n\n     A\n1    4\n29   2\n100  1\n150  5\n234  3\n"
'for p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))\n'
"In [2]: df = pd.DataFrame({'a':[1,2], 'b':[(1,2), (3,4)]})                                                                                                                      \n\nIn [3]: df                                                                                                                                                                      \nOut[3]: \n   a       b\n0  1  (1, 2)\n1  2  (3, 4)\n\nIn [4]: df['b'].tolist()                                                                                                                                                        \nOut[4]: [(1, 2), (3, 4)]\n\nIn [5]: pd.DataFrame(df['b'].tolist(), index=df.index)                                                                                                                                          \nOut[5]: \n   0  1\n0  1  2\n1  3  4\n\nIn [6]: df[['b1', 'b2']] = pd.DataFrame(df['b'].tolist(), index=df.index)                                                                                                                       \n\nIn [7]: df                                                                                                                                                                      \nOut[7]: \n   a       b  b1  b2\n0  1  (1, 2)   1   2\n1  2  (3, 4)   3   4\n"
"df = (pd.DataFrame({'name': ['A.J. Price'] * 3, \n                    'opponent': ['76ers', 'blazers', 'bobcats'], \n                    'nearest_neighbors': [['Zach LaVine', 'Jeremy Lin', 'Nate Robinson', 'Isaia']] * 3})\n      .set_index(['name', 'opponent']))\n\n&gt;&gt;&gt; df\n                                                    nearest_neighbors\nname       opponent                                                  \nA.J. Price 76ers     [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           blazers   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n           bobcats   [Zach LaVine, Jeremy Lin, Nate Robinson, Isaia]\n\ndf.reset_index(inplace=True)\nrows = []\n_ = df.apply(lambda row: [rows.append([row['name'], row['opponent'], nn]) \n                         for nn in row.nearest_neighbors], axis=1)\ndf_new = pd.DataFrame(rows, columns=df.columns).set_index(['name', 'opponent'])\n\n&gt;&gt;&gt; df_new\n                    nearest_neighbors\nname       opponent                  \nA.J. Price 76ers          Zach LaVine\n           76ers           Jeremy Lin\n           76ers        Nate Robinson\n           76ers                Isaia\n           blazers        Zach LaVine\n           blazers         Jeremy Lin\n           blazers      Nate Robinson\n           blazers              Isaia\n           bobcats        Zach LaVine\n           bobcats         Jeremy Lin\n           bobcats      Nate Robinson\n           bobcats              Isaia\n\n&gt;&gt;&gt; (pd.melt(df.nearest_neighbors.apply(pd.Series).reset_index(), \n             id_vars=['name', 'opponent'],\n             value_name='nearest_neighbors')\n     .set_index(['name', 'opponent'])\n     .drop('variable', axis=1)\n     .dropna()\n     .sort_index()\n     )\n"
'df1 = pd.DataFrame({"a": [1, 2], "b": [0, 8]}, columns=[\'a\', \'b\'])\ndf2 = pd.DataFrame({"a": [4, 5], "b": [7, 3]}, columns=[\'a\', \'b\'])\n\nprint (pd.concat([df1, df2]))\n   a  b\n0  1  0\n1  2  8\n0  4  7\n1  5  3\n\ndf1 = pd.DataFrame({"a": [1, 2], "b": [0, 8]}, columns=[\'b\', \'a\'])\ndf2 = pd.DataFrame({"a": [4, 5], "b": [7, 3]}, columns=[\'b\', \'a\'])\n\nprint (pd.concat([df1, df2]))\n   b  a\n0  0  1\n1  8  2\n0  7  4\n1  3  5\n\ndf1 = pd.DataFrame({"a": [1, 2], "b": [0, 8]}, columns=[\'b\', \'a\'])\ndf2 = pd.DataFrame({"a": [4, 5], "b": [7, 3]}, columns=[\'a\', \'b\'])\n\nprint (pd.concat([df1, df2]))\n\n   a  b\n0  1  0\n1  2  8\n0  4  7\n1  5  3\n\nprint (pd.concat([df1, df2], sort=True))\n   a  b\n0  1  0\n1  2  8\n0  4  7\n1  5  3\n\nprint (pd.concat([df1, df2], sort=False))\n   b  a\n0  0  1\n1  8  2\n0  7  4\n1  3  5\n\ndf1 = pd.DataFrame({"a": [1, 2], "b": [0, 8], \'e\':[5, 0]}, \n                    columns=[\'b\', \'a\',\'e\'])\ndf2 = pd.DataFrame({"a": [4, 5], "b": [7, 3], \'c\':[2, 8], \'d\':[7, 0]}, \n                    columns=[\'c\',\'b\',\'a\',\'d\'])\n\nprint (pd.concat([df1, df2]))\n\n   a  b    c    d    e\n0  1  0  NaN  NaN  5.0\n1  2  8  NaN  NaN  0.0\n0  4  7  2.0  7.0  NaN\n1  5  3  8.0  0.0  NaN\n\nprint (pd.concat([df1, df2], sort=True))\n   a  b    c    d    e\n0  1  0  NaN  NaN  5.0\n1  2  8  NaN  NaN  0.0\n0  4  7  2.0  7.0  NaN\n1  5  3  8.0  0.0  NaN\n\nprint (pd.concat([df1, df2], sort=False))\n\n   b  a    e    c    d\n0  0  1  5.0  NaN  NaN\n1  8  2  0.0  NaN  NaN\n0  7  4  NaN  2.0  7.0\n1  3  5  NaN  8.0  0.0\n\nplacement_by_video_summary = placement_by_video_summary.drop(placement_by_video_summary_new.index)\n                                                       .append(placement_by_video_summary_new, sort=True)\n                                                       .sort_index()\n'
'In [21]: df[\'m\'] = pd.Categorical(df[\'m\'], ["March", "April", "Dec"])\n\nIn [22]: df  # looks the same!\nOut[22]:\n   a  b      m\n0  1  2  March\n1  5  6    Dec\n2  3  4  April\n\nIn [23]: df.sort_values("m")\nOut[23]:\n   a  b      m\n0  1  2  March\n2  3  4  April\n1  5  6    Dec\n\ndf = pd.DataFrame([[1, 2, \'March\'],[5, 6, \'Dec\'],[3, 4, \'April\']], columns=[\'a\',\'b\',\'m\'])\ns = df[\'m\'].apply(lambda x: {\'March\':0, \'April\':1, \'Dec\':3}[x])\ns.sort_values()\n\nIn [4]: df.set_index(s.index).sort()\nOut[4]: \n   a  b      m\n0  1  2  March\n1  3  4  April\n2  5  6    Dec\n\ns = df[\'m\'].replace({\'March\':0, \'April\':1, \'Dec\':3})\n'
"import pandas as pd\ndf = pd.DataFrame([])\ndf.instrument_name = 'Binky'\n"
"Docstring:\nSplit an array into multiple sub-arrays.\n\nPlease refer to the ``split`` documentation.  The only difference\nbetween these functions is that ``array_split`` allows\n`indices_or_sections` to be an integer that does *not* equally\ndivide the axis.\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n   ...:                           'foo', 'bar', 'foo', 'foo'],\n   ...:                    'B' : ['one', 'one', 'two', 'three',\n   ...:                           'two', 'two', 'one', 'three'],\n   ...:                    'C' : randn(8), 'D' : randn(8)})\n\nIn [3]: print df\n     A      B         C         D\n0  foo    one -0.174067 -0.608579\n1  bar    one -0.860386 -1.210518\n2  foo    two  0.614102  1.689837\n3  bar  three -0.284792 -1.071160\n4  foo    two  0.843610  0.803712\n5  bar    two -1.514722  0.870861\n6  foo    one  0.131529 -0.968151\n7  foo  three -1.002946 -0.257468\n\nIn [4]: import numpy as np\nIn [5]: np.array_split(df, 3)\nOut[5]: \n[     A    B         C         D\n0  foo  one -0.174067 -0.608579\n1  bar  one -0.860386 -1.210518\n2  foo  two  0.614102  1.689837,\n      A      B         C         D\n3  bar  three -0.284792 -1.071160\n4  foo    two  0.843610  0.803712\n5  bar    two -1.514722  0.870861,\n      A      B         C         D\n6  foo    one  0.131529 -0.968151\n7  foo  three -1.002946 -0.257468]\n"
'import pandas as pd\nimport numpy as np\n\npath = r"C:\\Users\\fedel\\Desktop\\excelData\\PhD_data.xlsx"\n\nx1 = np.random.randn(100, 2)\ndf1 = pd.DataFrame(x1)\n\nx2 = np.random.randn(100, 2)\ndf2 = pd.DataFrame(x2)\n\nwriter = pd.ExcelWriter(path, engine = \'xlsxwriter\')\ndf1.to_excel(writer, sheet_name = \'x1\')\ndf2.to_excel(writer, sheet_name = \'x2\')\nwriter.save()\nwriter.close()\n\nimport pandas as pd\nimport numpy as np\nfrom openpyxl import load_workbook\n\npath = r"C:\\Users\\fedel\\Desktop\\excelData\\PhD_data.xlsx"\n\nbook = load_workbook(path)\nwriter = pd.ExcelWriter(path, engine = \'openpyxl\')\nwriter.book = book\n\nx3 = np.random.randn(100, 2)\ndf3 = pd.DataFrame(x3)\n\nx4 = np.random.randn(100, 2)\ndf4 = pd.DataFrame(x4)\n\ndf3.to_excel(writer, sheet_name = \'x3\')\ndf4.to_excel(writer, sheet_name = \'x4\')\nwriter.save()\nwriter.close()\n'
"df.to_csv('pandasfile.csv', float_format='%.3f')\n\ndf.to_csv('pandasfile.csv', float_format='%g')\n\nBob,0.085\nAlice,0.005\n"
"In [44]: df['gdp'] = df['gdp'].shift(-1)\n\nIn [45]: df\nOut[45]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n4  6  NaN    7\n\nIn [46]: df[:-1]                                                                                                                                                                                                                                                                                                               \nOut[46]: \n   y  gdp  cap\n0  1    3    5\n1  2    7    9\n2  8    4    2\n3  3    7    7\n"
"df.columns = [str(col) + '_x' for col in df.columns]\n"
"user_dict = {12: {'Category 1': {'att_1': 1, 'att_2': 'whatever'},\n                  'Category 2': {'att_1': 23, 'att_2': 'another'}},\n             15: {'Category 1': {'att_1': 10, 'att_2': 'foo'},\n                  'Category 2': {'att_1': 30, 'att_2': 'bar'}}}\n\npd.DataFrame.from_dict({(i,j): user_dict[i][j] \n                           for i in user_dict.keys() \n                           for j in user_dict[i].keys()},\n                       orient='index')\n\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n\nuser_ids = []\nframes = []\n\nfor user_id, d in user_dict.iteritems():\n    user_ids.append(user_id)\n    frames.append(pd.DataFrame.from_dict(d, orient='index'))\n\npd.concat(frames, keys=user_ids)\n\n               att_1     att_2\n12 Category 1      1  whatever\n   Category 2     23   another\n15 Category 1     10       foo\n   Category 2     30       bar\n"
"pd.merge(frame_1, frame_2, left_on='county_ID', right_on='countyid')\n\npd.merge(frame_1, frame_2, how='left', left_on='county_ID', right_on='countyid')\n"
"df.read_csv('foo.tsv', sep='\\t', thousands=',')\n\nIn [ 9]: import locale\n\nIn [10]: from locale import atof\n\nIn [11]: locale.setlocale(locale.LC_NUMERIC, '')\nOut[11]: 'en_GB.UTF-8'\n\nIn [12]: df.applymap(atof)\nOut[12]:\n      0        1\n0  1200  4200.00\n1  7000    -0.03\n2     5     0.00\n"
"&gt;&gt;&gt; df['BrandName'].replace(['ABC', 'AB'], 'A')\n0    A\n1    B\n2    A\n3    D\n4    A\n\ndf['BrandName'] = df['BrandName'].replace(['ABC', 'AB'], 'A')\n"
"import seaborn as sns\n\niris = sns.load_dataset('iris')\niris.head()\n\niris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n\nimport statsmodels.api as sm\n\niris = sm.datasets.get_rdataset('iris').data\n\nfrom pydataset import data\n\niris = data('iris')\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\n# `iris.data` holds the numerical values\n# `iris.feature_names` holds the numerical column names\n# `iris.target` holds the categorical (species) values (as ints)\n# `iris.target_names` holds the unique categorical names\n\n# In your terminal\n$ pip install quilt\n$ quilt install uciml/iris\n\nimport quilt.data.uciml.iris as ir\n\niris = ir.tables.iris()\n"
"import pandas as pd\ndf = pd.DataFrame({'A':[1,1,3,2,6,2,8]})\na = df['A'].unique()\nprint sorted(a)\n\n[1, 2, 3, 6, 8]\n"
"from pandas import ExcelWriter\n# from pandas.io.parsers import ExcelWriter\n\ndef save_xls(list_dfs, xls_path):\n    with ExcelWriter(xls_path) as writer:\n        for n, df in enumerate(list_dfs):\n            df.to_excel(writer,'sheet%s' % n)\n        writer.save()\n"
'df.loc[:, (df != 0).any(axis=0)]\n\nIn [74]: import pandas as pd\n\nIn [75]: df = pd.DataFrame([[1,0,0,0], [0,0,1,0]])\n\nIn [76]: df\nOut[76]: \n   0  1  2  3\n0  1  0  0  0\n1  0  0  1  0\n\n[2 rows x 4 columns]\n\nIn [77]: df != 0\nOut[77]: \n       0      1      2      3\n0   True  False  False  False\n1  False  False   True  False\n\n[2 rows x 4 columns]\n\nIn [78]: (df != 0).any(axis=0)\nOut[78]: \n0     True\n1    False\n2     True\n3    False\ndtype: bool\n\nIn [79]: df.loc[:, (df != 0).any(axis=0)]\nOut[79]: \n   0  2\n0  1  0\n1  0  1\n\n[2 rows x 2 columns]\n\ndf = df.loc[:, (df != 0).any(axis=0)]\n'
"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nfig, ax = plt.subplots()\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend()\n\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1974)\n\n# Generate Data\nnum = 20\nx, y = np.random.random((2, num))\nlabels = np.random.choice(['a', 'b', 'c'], num)\ndf = pd.DataFrame(dict(x=x, y=y, label=labels))\n\ngroups = df.groupby('label')\n\n# Plot\nplt.rcParams.update(pd.tools.plotting.mpl_stylesheet)\ncolors = pd.tools.plotting._get_standard_colors(len(groups), color_type='random')\n\nfig, ax = plt.subplots()\nax.set_color_cycle(colors)\nax.margins(0.05)\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=name)\nax.legend(numpoints=1, loc='upper left')\n\nplt.show()\n"
'&gt;&gt;&gt; df = pd.concat([df1, df2])\n&gt;&gt;&gt; df = df.reset_index(drop=True)\n\n&gt;&gt;&gt; df_gpby = df.groupby(list(df.columns))\n\n&gt;&gt;&gt; idx = [x[0] for x in df_gpby.groups.values() if len(x) == 1]\n\n&gt;&gt;&gt; df.reindex(idx)\n         Date   Fruit   Num   Color\n9  2013-11-25  Orange   8.6  Orange\n8  2013-11-25   Apple  22.1     Red\n'
"&gt;&gt;&gt; df['x'].str.lower()\n0    one\n1    two\n2    NaN\nName: x, dtype: object\n"
'df.applymap(foo_bar)\n\n#     A       B       C\n#0  wow     bar wow bar\n#1  bar wow wow     bar\n\nimport numpy as np\ndf.apply(np.vectorize(foo_bar))\n#     A       B       C\n#0  wow     bar wow bar\n#1  bar wow wow     bar\n'
"def letgo(df):\n    df.drop('b', axis=1, inplace=True)\n\na = pd.DataFrame({'a':[1,2], 'b':[3,4]})\nletgo(a)  # will alter a\n\ndef letgo3(x):\n    x[:] = np.array([[3,3],[3,3]])\n\nv = np.empty((2, 2))\nletgo3(v)   # will alter v\n\ndef letgo(df):\n    df = df.drop('b',axis=1)\n    return df\n\na = pd.DataFrame({'a':[1,2], 'b':[3,4]})\na = letgo(a)\n\ndef letgo():\n    global a\n    a = a.drop('b',axis=1)\n\na = pd.DataFrame({'a':[1,2], 'b':[3,4]})\nletgo()   # will alter a!\n"
"dataframe.column.dtype\n\ndataframe.dtypes\n\nimport pandas as pd\ndf = pd.DataFrame({'A': [1,2,3], 'B': [True, False, False], 'C': ['a', 'b', 'c']})\n\ndf.A.dtype\n# dtype('int64')\ndf.B.dtype\n# dtype('bool')\ndf.C.dtype\n# dtype('O')\n\ndf.dtypes\n#A     int64\n#B      bool\n#C    object\n#dtype: object\n"
"fig, axs = plt.subplots(1,2)\n\ndf['korisnika'].plot(ax=axs[0])\ndf['osiguranika'].plot(ax=axs[1])\n"
"import pandas\ndf = pandas.DataFrame(columns=['to','fr','ans'])\ndf.to = [pandas.Timestamp('2014-01-24 13:03:12.050000'), pandas.Timestamp('2014-01-27 11:57:18.240000'), pandas.Timestamp('2014-01-23 10:07:47.660000')]\ndf.fr = [pandas.Timestamp('2014-01-26 23:41:21.870000'), pandas.Timestamp('2014-01-27 15:38:22.540000'), pandas.Timestamp('2014-01-23 18:50:41.420000')]\n(df.fr-df.to).astype('timedelta64[h]')\n\n0    58\n1     3\n2     8\ndtype: float64\n"
"g = sns.lmplot('X', 'Y', df, col='Z', sharex=False, sharey=False)\ng.set(ylim=(0, None))\n"
'new_header = df.iloc[0] #grab the first row for the header\ndf = df[1:] #take the data less the header row\ndf.columns = new_header #set the header row as the df header\n'
"from matplotlib import pyplot as plt\nfrom itertools import cycle, islice\nimport pandas, numpy as np  # I find np.random.randint to be better\n\n# Make the data\nx = [{i:np.random.randint(1,5)} for i in range(10)]\ndf = pandas.DataFrame(x)\n\n# Make a list by cycling through the colors you care about\n# to match the length of your data.\nmy_colors = list(islice(cycle(['b', 'r', 'g', 'y', 'k']), None, len(df)))\n\n# Specify this list of colors as the `color` option to `plot`.\ndf.plot(kind='bar', stacked=True, color=my_colors)\n\nmy_colors = ['g', 'b']*5 # &lt;-- this concatenates the list to itself 5 times.\nmy_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5 # &lt;-- make two custom RGBs and repeat/alternate them over all the bar elements.\nmy_colors = [(x/10.0, x/20.0, 0.75) for x in range(len(df))] # &lt;-- Quick gradient example along the Red/Green dimensions.\n"
"In [11]: df.sort_values([('Group1', 'C')], ascending=False)\nOut[11]: \n  Group1       Group2      \n       A  B  C      A  B  C\n2      5  6  9      1  0  0\n1      1  0  3      2  5  7\n3      7  0  2      0  3  5\n"
"In [1]: df = DataFrame({'B': range(5)})\n\nIn [2]: df.index = [Timestamp('20130101 09:00:00'),\n   ...:             Timestamp('20130101 09:00:02'),\n   ...:             Timestamp('20130101 09:00:03'),\n   ...:             Timestamp('20130101 09:00:05'),\n   ...:             Timestamp('20130101 09:00:06')]\n\nIn [3]: df\nOut[3]: \n                     B\n2013-01-01 09:00:00  0\n2013-01-01 09:00:02  1\n2013-01-01 09:00:03  2\n2013-01-01 09:00:05  3\n2013-01-01 09:00:06  4\n\nIn [4]: df.rolling(2, min_periods=1).sum()\nOut[4]: \n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  5.0\n2013-01-01 09:00:06  7.0\n\nIn [5]: df.rolling('2s', min_periods=1).sum()\nOut[5]: \n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:02  1.0\n2013-01-01 09:00:03  3.0\n2013-01-01 09:00:05  3.0\n2013-01-01 09:00:06  7.0\n"
"&gt;&gt;&gt; x = np.timedelta64(2069211000000000, 'ns')\n&gt;&gt;&gt; days = x.astype('timedelta64[D]')\n&gt;&gt;&gt; days / np.timedelta64(1, 'D')\n23\n"
"ax = s.hist()  # s is an instance of Series\nfig = ax.get_figure()\nfig.savefig('/path/to/figure.pdf')\n\nimport matplotlib.pyplot as plt\ns.hist()\nplt.savefig('path/to/figure.pdf')  # saves the current figure\n"
"df.cc = pd.Categorical(df.cc)\n\ndf['code'] = df.cc.cat.codes\n\n   cc  temp  code\n0  US  37.0     2\n1  CA  12.0     1\n2  US  35.0     2\n3  AU  20.0     0\n\ndf.cc.astype('category').cat.codes\n\ndf2 = pd.DataFrame(df.temp)\ndf2.index = pd.CategoricalIndex(df.cc)\n"
"In [3]: df = pandas.DataFrame(np.random.randn(5, 3), columns=['a', 'b', 'c'])\n\nIn [4]: df\nOut[4]: \n          a         b         c\n0 -0.001968 -1.877945 -1.515674\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n\nIn [6]: df[df.apply(lambda x: x['b'] &gt; x['c'], axis=1)]\nOut[6]: \n          a         b         c\n1 -0.540628  0.793913 -0.983315\n2 -1.313574  1.946410  0.826350\n3  0.015763 -0.267860 -2.228350\n4  0.563111  1.195459  0.343168\n"
'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as dates\n\nidx = pd.date_range(\'2011-05-01\', \'2011-07-01\')\ns = pd.Series(np.random.randn(len(idx)), index=idx)\n\nfig, ax = plt.subplots()\nax.plot_date(idx.to_pydatetime(), s, \'v-\')\nax.xaxis.set_minor_locator(dates.WeekdayLocator(byweekday=(1),\n                                                interval=1))\nax.xaxis.set_minor_formatter(dates.DateFormatter(\'%d\\n%a\'))\nax.xaxis.grid(True, which="minor")\nax.yaxis.grid()\nax.xaxis.set_major_locator(dates.MonthLocator())\nax.xaxis.set_major_formatter(dates.DateFormatter(\'\\n\\n\\n%b\\n%Y\'))\nplt.tight_layout()\nplt.show()\n'
'&gt;&gt;&gt; df = pd.DataFrame({"date": range(10, 64, 8)})\n&gt;&gt;&gt; df.index += 17\n&gt;&gt;&gt; df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n&gt;&gt;&gt; df["date"].iloc[0]\n10\n&gt;&gt;&gt; df["date"].iloc[-1]\n58\n\n&gt;&gt;&gt; df = pd.DataFrame({"date": range(10, 64, 8)})\n&gt;&gt;&gt; df.index += 17\n&gt;&gt;&gt; df\n    date\n17    10\n18    18\n19    26\n20    34\n21    42\n22    50\n23    58\n&gt;&gt;&gt; df[\'date\'].iget(0)\n10\n&gt;&gt;&gt; df[\'date\'].iget(-1)\n58\n\n&gt;&gt;&gt; df[\'date\'][df.index[0]]\n10\n&gt;&gt;&gt; df[\'date\'][df.index[-1]]\n58\n\n&gt;&gt;&gt; df.ix[0]\nTraceback (most recent call last):\n  File "&lt;ipython-input-489-494245247e87&gt;", line 1, in &lt;module&gt;\n    df.ix[0]\n[...]\nKeyError: 0\n'
"import numpy as np\nimport pandas as pd\nimport scipy.sparse as sparse\n\ndf = pd.DataFrame(np.arange(1,10).reshape(3,3))\narr = sparse.coo_matrix(([1,1,1], ([0,1,2], [1,2,0])), shape=(3,3))\ndf['newcol'] = arr.toarray().tolist()\nprint(df)\n\n   0  1  2     newcol\n0  1  2  3  [0, 1, 0]\n1  4  5  6  [0, 0, 1]\n2  7  8  9  [1, 0, 0]\n"
"import pandas as pd\npd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer')\n\n pd.merge(restaurant_ids_dataframe, restaurant_review_frame, on='business_id', how='outer', suffixes=('_restaurant_id', '_restaurant_review'))\n"
'df.drop(column_name, axis=1)\n'
"L = ['Thanks You', 'Its fine no problem', 'Are you sure']\n\n#create new df \ndf = pd.DataFrame({'col':L})\nprint (df)\n\n                   col\n0           Thanks You\n1  Its fine no problem\n2         Are you sure\n\ndf = pd.DataFrame({'oldcol':[1,2,3]})\n\n#add column to existing df \ndf['col'] = L\nprint (df)\n   oldcol                  col\n0       1           Thanks You\n1       2  Its fine no problem\n2       3         Are you sure\n\n#default column name 0\ndf = pd.DataFrame(L)\nprint (df)\n                     0\n0           Thanks You\n1  Its fine no problem\n2         Are you sure\n"
"df[1:3]['A'] = 5\n\ndf.loc[1:3, 'A'] = 5\n"
'In [27]:\n\ndf.merge(pd.DataFrame(data = [s.values] * len(s), columns = s.index), left_index=True, right_index=True)\nOut[27]:\n   a  b  s1  s2\n0  1  3   5   6\n1  2  4   5   6\n\ndf.merge(pd.DataFrame(data = [s.values] * len(df), columns = s.index, index=df.index), left_index=True, right_index=True)\n'
"np.savetxt(r'c:\\data\\np.txt', df.values, fmt='%d')\n\n18 55 1 70\n18 55 2 67\n18 57 2 75\n18 58 1 35\n19 54 2 70\n\ndf.to_csv(r'c:\\data\\pandas.txt', header=None, index=None, sep=' ', mode='a')\n"
"df = pd.DataFrame({'Sales':[10,20,30,40,50], 'A':[3,4,7,6,1]})\nprint (df)\n   A  Sales\n0  3     10\n1  4     20\n2  7     30\n3  6     40\n4  1     50\n\ns = 30\n\ndf1 = df[df['Sales'] &gt;= s]\nprint (df1)\n   A  Sales\n2  7     30\n3  6     40\n4  1     50\n\ndf2 = df[df['Sales'] &lt; s]\nprint (df2)\n   A  Sales\n0  3     10\n1  4     20\n\nmask = df['Sales'] &gt;= s\ndf1 = df[mask]\ndf2 = df[~mask]\nprint (df1)\n   A  Sales\n2  7     30\n3  6     40\n4  1     50\n\nprint (df2)\n   A  Sales\n0  3     10\n1  4     20\n\nprint (mask)\n0    False\n1    False\n2     True\n3     True\n4     True\nName: Sales, dtype: bool\n\nprint (~mask)\n0     True\n1     True\n2    False\n3    False\n4    False\nName: Sales, dtype: bool\n"
"In : col = ['a','b','c']\n\nIn : data = DataFrame([[1,2,3],[10,11,12],[20,21,22]],columns=col)\n\nIn : data\nOut:\n    a   b   c\n0   1   2   3\n1  10  11  12\n2  20  21  22\n\nIn : data2 = data.set_index('a')\n\nIn : data2\nOut:\n     b   c\na\n1    2   3\n10  11  12\n20  21  22\n"
'data.groupby(data.date.dt.year)\n'
"In [97]: df = pandas.DataFrame({'month': np.random.randint(0,11, 100), 'A': np.random.randn(100), 'B': np.random.randn(100)})\n\nIn [98]: df.join(df.groupby('month')['A'].sum(), on='month', rsuffix='_r')\nOut[98]:\n           A         B  month       A_r\n0  -0.040710  0.182269      0 -0.331816\n1  -0.004867  0.642243      1  2.448232\n2  -0.162191  0.442338      4  2.045909\n3  -0.979875  1.367018      5 -2.736399\n4  -1.126198  0.338946      5 -2.736399\n5  -0.992209 -1.343258      1  2.448232\n6  -1.450310  0.021290      0 -0.331816\n7  -0.675345 -1.359915      9  2.722156\n"
'appended_data = []\nfor infile in glob.glob("*.xlsx"):\n    data = pandas.read_excel(infile)\n    # store DataFrame in list\n    appended_data.append(data)\n# see pd.concat documentation for more info\nappended_data = pd.concat(appended_data)\n# write DataFrame to an excel sheet \nappended_data.to_excel(\'appended.xlsx\')\n'
'In [16]: df\nOut[16]:\n         Column 1\nfoo\nApples          1\nOranges         2\nPuppies         3\nDucks           4\n\nIn [17]: del df.index.name\n\nIn [18]: df\nOut[18]:\n         Column 1\nApples          1\nOranges         2\nPuppies         3\nDucks           4\n'
'import pandas as pd\nfrom io import StringIO\n\ndata = """\nPDB CHAIN SP_PRIMARY RES_BEG RES_END PDB_BEG PDB_END SP_BEG SP_END\n5d8b N P60490 1 146 1 146 1 146\n5d8b NA P80377 _ 126 1 126 1 126\n5d8b O P60491 1 118 1 118 1 118\n"""\n\ndf = pd.read_csv(StringIO(data), sep=\' \', keep_default_na=False, na_values=[\'_\'])\n\nIn [130]: df\nOut[130]:\n    PDB CHAIN SP_PRIMARY  RES_BEG  RES_END  PDB_BEG  PDB_END  SP_BEG  SP_END\n0  5d8b     N     P60490        1      146        1      146       1     146\n1  5d8b    NA     P80377      NaN      126        1      126       1     126\n2  5d8b     O     P60491        1      118        1      118       1     118\n\nIn [144]: df.CHAIN.apply(type)\nOut[144]:\n0    &lt;class \'str\'&gt;\n1    &lt;class \'str\'&gt;\n2    &lt;class \'str\'&gt;\nName: CHAIN, dtype: object\n'
'df.iloc[:,-1:]\n'
"df1 = df[df.isna().any(axis=1)]\n\nd = {'filename': ['M66_MI_NSRh35d32kpoints.dat', 'F71_sMI_DMRI51d.dat', 'F62_sMI_St22d7.dat', 'F41_Car_HOC498d.dat', 'F78_MI_547d.dat'], 'alpha1': [0.8016, 0.0, 1.721, 1.167, 1.897], 'alpha2': [0.9283, 0.0, 3.833, 2.809, 5.459], 'gamma1': [1.0, np.nan, 0.23748000000000002, 0.36419, 0.095319], 'gamma2': [0.074804, 0.0, 0.15, 0.3, np.nan], 'chi2min': [39.855990000000006, 1e+25, 10.91832, 7.966335000000001, 25.93468]}\ndf = pd.DataFrame(d).set_index('filename')\n\nprint (df)\n                             alpha1  alpha2    gamma1    gamma2       chi2min\nfilename                                                                     \nM66_MI_NSRh35d32kpoints.dat  0.8016  0.9283  1.000000  0.074804  3.985599e+01\nF71_sMI_DMRI51d.dat          0.0000  0.0000       NaN  0.000000  1.000000e+25\nF62_sMI_St22d7.dat           1.7210  3.8330  0.237480  0.150000  1.091832e+01\nF41_Car_HOC498d.dat          1.1670  2.8090  0.364190  0.300000  7.966335e+00\nF78_MI_547d.dat              1.8970  5.4590  0.095319       NaN  2.593468e+01\n\nprint (df.isna())\n                            alpha1 alpha2 gamma1 gamma2 chi2min\nfilename                                                       \nM66_MI_NSRh35d32kpoints.dat  False  False  False  False   False\nF71_sMI_DMRI51d.dat          False  False   True  False   False\nF62_sMI_St22d7.dat           False  False  False  False   False\nF41_Car_HOC498d.dat          False  False  False  False   False\nF78_MI_547d.dat              False  False  False   True   False\n\nprint (df.isna().any(axis=1))\nfilename\nM66_MI_NSRh35d32kpoints.dat    False\nF71_sMI_DMRI51d.dat             True\nF62_sMI_St22d7.dat             False\nF41_Car_HOC498d.dat            False\nF78_MI_547d.dat                 True\ndtype: bool\n\ndf1 = df[df.isna().any(axis=1)]\nprint (df1)\n                     alpha1  alpha2    gamma1  gamma2       chi2min\nfilename                                                           \nF71_sMI_DMRI51d.dat   0.000   0.000       NaN     0.0  1.000000e+25\nF78_MI_547d.dat       1.897   5.459  0.095319     NaN  2.593468e+01\n"
"import pandas as pd\nfrom functools import reduce\n\ndf1 = pd.read_table('file1.csv', sep=',')\ndf2 = pd.read_table('file2.csv', sep=',')\ndf3 = pd.read_table('file3.csv', sep=',')\n\n# compile the list of dataframes you want to merge\ndata_frames = [df1, df2, df3]\n\ndf_merged = reduce(lambda  left,right: pd.merge(left,right,on=['DATE'],\n                                            how='outer'), data_frames)\n\n# if you want to fill the values that don't exist in the lines of merged dataframe simply fill with required strings as\n\ndf_merged = reduce(lambda  left,right: pd.merge(left,right,on=['DATE'],\n                                            how='outer'), data_frames).fillna('void')\n\npd.DataFrame.to_csv(df_merged, 'merged.txt', sep=',', na_rep='.', index=False)\n"
"In [43]: temp2.str[-1]\nOut[43]: \n0    p500\n1    p600\n2    p700\nName: ticker\n\n&gt;&gt;&gt; temp = pd.DataFrame({'ticker' : ['spx 5/25/2001 p500', 'spx 5/25/2001 p600', 'spx 5/25/2001 p700']})\n&gt;&gt;&gt; temp['ticker'].str.split(' ').str[-1]\n0    p500\n1    p600\n2    p700\nName: ticker, dtype: object\n"
'df.replace(\'-\', None)\nTypeError: If "to_replace" and "value" are both None then regex must be a mapping\n\nIn [11]: df.replace(\'-\', df.replace([\'-\'], [None]) # or .replace(\'-\', {0: None})\nOut[11]:\n      0\n0  None\n1     3\n2     2\n3     5\n4     1\n5    -5\n6    -1\n7  None\n8     9\n\nIn [12]: df.replace(\'-\', np.nan)\nOut[12]:\n     0\n0  NaN\n1    3\n2    2\n3    5\n4    1\n5   -5\n6   -1\n7  NaN\n8    9\n'
"&gt;&gt;&gt; barlist=plt.bar([1,2,3,4], [1,2,3,4])\n&gt;&gt;&gt; barlist[0].set_color('r')\n&gt;&gt;&gt; plt.show()\n\n&gt;&gt;&gt; f=plt.figure()\n&gt;&gt;&gt; ax=f.add_subplot(1,1,1)\n&gt;&gt;&gt; ax.bar([1,2,3,4], [1,2,3,4])\n&lt;Container object of 4 artists&gt;\n&gt;&gt;&gt; ax.get_children()\n[&lt;matplotlib.axis.XAxis object at 0x6529850&gt;, \n &lt;matplotlib.axis.YAxis object at 0x78460d0&gt;,  \n &lt;matplotlib.patches.Rectangle object at 0x733cc50&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x733cdd0&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x777f290&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x777f710&gt;, \n &lt;matplotlib.text.Text object at 0x7836450&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x7836390&gt;, \n &lt;matplotlib.spines.Spine object at 0x6529950&gt;, \n &lt;matplotlib.spines.Spine object at 0x69aef50&gt;,\n &lt;matplotlib.spines.Spine object at 0x69ae310&gt;, \n &lt;matplotlib.spines.Spine object at 0x69aea50&gt;]\n&gt;&gt;&gt; ax.get_children()[2].set_color('r') \n #You can also try to locate the first patches.Rectangle object \n #instead of direct calling the index.\n\n&gt;&gt;&gt; import matplotlib\n&gt;&gt;&gt; childrenLS=ax.get_children()\n&gt;&gt;&gt; barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)\n[&lt;matplotlib.patches.Rectangle object at 0x3103650&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x3103810&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x3129850&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x3129cd0&gt;, \n &lt;matplotlib.patches.Rectangle object at 0x3112ad0&gt;]\n"
"In [11]: df = pd.DataFrame([[1, 3], [2, 4]], columns=['A', 'B'])\n\nIn [12]: df2 = pd.DataFrame([[1, 5], [1, 6]], columns=['A', 'C'])\n\nIn [13]: df.merge(df2, how='left')  # merges on columns A\nOut[13]: \n   A  B   C\n0  1  3   5\n1  1  3   6\n2  2  4 NaN\n\nIn [21]: df2.drop_duplicates(subset=['A'])  # you can use take_last=True\nOut[21]: \n   A  C\n0  1  5\n\nIn [22]: df.merge(df2.drop_duplicates(subset=['A']), how='left')\nOut[22]: \n   A  B   C\n0  1  3   5\n1  2  4 NaN\n"
"In [11]: df = pd.DataFrame([[1, 2], [3, 4]], ['a', 'b'], ['A', 'B'])\n\nIn [12]: df\nOut[12]: \n   A  B\na  1  2\nb  3  4\n\nIn [13]: df.iloc[0]  # first row in a DataFrame\nOut[13]: \nA    1\nB    2\nName: a, dtype: int64\n\nIn [14]: df['A'].iloc[0]  # first item in a Series (Column)\nOut[14]: 1\n"
"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\nts.plot()\nplt.show()\n\nimport numpy as np\nimport pandas as pd\nts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\nts.plot()\npd.tseries.plotting.pylab.show()\n\npython -mtimeit -s 'import pandas as pd'\n100000000 loops, best of 3: 0.0122 usec per loop\n\npython -mtimeit -s 'import pandas as pd; import matplotlib.pyplot as plt'\n100000000 loops, best of 3: 0.0125 usec per loop\n"
'from IPython.display import display\ndisplay(salaries.head())\ndisplay(teams.head())\n'
"In [110]: (df.groupby('Company Name')\n   .....:    .agg({'Organisation Name':'count', 'Amount': 'sum'})\n   .....:    .reset_index()\n   .....:    .rename(columns={'Organisation Name':'Organisation Count'})\n   .....: )\nOut[110]:\n          Company Name   Amount  Organisation Count\n0  Vifor Pharma UK Ltd  4207.93                   5\n\ndf.groupby('Company Name')['Amount'].agg(['sum','count'])\n\ndf.groupby('Company Name').agg({'Amount': ['sum','count']})\n\nIn [98]: df.groupby('Company Name')['Amount'].agg(['sum','count'])\nOut[98]:\n                         sum  count\nCompany Name\nVifor Pharma UK Ltd  4207.93      5\n\nIn [99]: df.groupby('Company Name').agg({'Amount': ['sum','count']})\nOut[99]:\n                      Amount\n                         sum count\nCompany Name\nVifor Pharma UK Ltd  4207.93     5\n"
"pd.read_parquet('example_pa.parquet', engine='pyarrow')\n\npd.read_parquet('example_fp.parquet', engine='fastparquet')\n"
'import pandas as pd\nimport io\n\ndata = """value          \n"2015-09-25 00:46"    71.925000\n"2015-09-25 00:47"    71.625000\n"2015-09-25 00:48"    71.333333\n"2015-09-25 00:49"    64.571429\n"2015-09-25 00:50"    72.285714"""\n\ndf = pd.read_table(io.StringIO(data), delim_whitespace=True)\n\n# Converting the index as date\ndf.index = pd.to_datetime(df.index)\n\n# Extracting hour &amp; minute\ndf[\'A\'] = df.index.hour\ndf[\'B\'] = df.index.minute\ndf\n\n#                          value  A   B\n# 2015-09-25 00:46:00  71.925000  0  46\n# 2015-09-25 00:47:00  71.625000  0  47\n# 2015-09-25 00:48:00  71.333333  0  48\n# 2015-09-25 00:49:00  64.571429  0  49\n# 2015-09-25 00:50:00  72.285714  0  50\n'
"df.loc[df.groupby('id').date.idxmax()]\n\n    id  product       date\n2  220     6647 2014-10-16\n5  826     3380 2015-05-19\n8  901     4555 2014-11-01\n"
"# Python 3.6.5, NumPy 1.14.3, Pandas 0.23.0\n\nnp.random.seed(0)\nN = 10**5\n\n%timeit list(map(divide, df['A'], df['B']))                                   # 43.9 ms\n%timeit np.vectorize(divide)(df['A'], df['B'])                                # 48.1 ms\n%timeit [divide(a, b) for a, b in zip(df['A'], df['B'])]                      # 49.4 ms\n%timeit [divide(a, b) for a, b in df[['A', 'B']].itertuples(index=False)]     # 112 ms\n%timeit df.apply(lambda row: divide(*row), axis=1, raw=True)                  # 760 ms\n%timeit df.apply(lambda row: divide(row['A'], row['B']), axis=1)              # 4.83 s\n%timeit [divide(row['A'], row['B']) for _, row in df[['A', 'B']].iterrows()]  # 11.6 s\n\ndef foo(row):\n    print(type(row))\n    assert False  # because you only need to see this once\ndf.apply(lambda row: foo(row), axis=1)\n\n%timeit np.where(df['B'] == 0, 0, df['A'] / df['B'])       # 1.17 ms\n%timeit (df['A'] / df['B']).replace([np.inf, -np.inf], 0)  # 1.96 ms\n\nfrom numba import njit\n\n@njit\ndef divide(a, b):\n    res = np.empty(a.shape)\n    for i in range(len(a)):\n        if b[i] != 0:\n            res[i] = a[i] / b[i]\n        else:\n            res[i] = 0\n    return res\n\n%timeit divide(df['A'].values, df['B'].values)  # 717 µs\n"
'&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; df.pivot(index=0, columns=1, values=2)\n# avg DataFrame\n1      c1     c2\n0               \nr1  avg11  avg12\nr2  avg21  avg22\n&gt;&gt;&gt; df.pivot(index=0, columns=1, values=3)\n# stdev DataFrame\n1        c1       c2\n0                   \nr1  stdev11  stdev12\nr2  stdev21  stdev22\n'
' df[(df &lt; 3) | (df == 5)]\n\ndf[np.logical_or(df&lt;3, df==5)]\n\ndf[np.logical_or.reduce([df&lt;3, df==5])]\n'
"pd.set_option('display.max_colwidth', -1)\n"
"In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a': [0, -1, 2], 'b': [-3, 2, 1]})\n\nIn [3]: df\nOut[3]: \n   a  b\n0  0 -3\n1 -1  2\n2  2  1\n\nIn [4]: df[df &lt; 0] = 0\n\nIn [5]: df\nOut[5]: \n   a  b\n0  0  0\n1  0  2\n2  2  1\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a': [0, -1, 2], 'b': [-3, 2, 1],\n                           'c': ['foo', 'goo', 'bar']})\n\nIn [3]: df\nOut[3]: \n   a  b    c\n0  0 -3  foo\n1 -1  2  goo\n2  2  1  bar\n\nIn [4]: num = df._get_numeric_data()\n\nIn [5]: num[num &lt; 0] = 0\n\nIn [6]: df\nOut[6]: \n   a  b    c\n0  0  0  foo\n1  0  2  goo\n2  2  1  bar\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a': pd.to_timedelta([0, -1, 2], 'd'),\n   ...:                    'b': pd.to_timedelta([-3, 2, 1], 'd')})\n\nIn [3]: df\nOut[3]: \n        a       b\n0  0 days -3 days\n1 -1 days  2 days\n2  2 days  1 days\n\nIn [4]: for k, v in df.iteritems():\n   ...:     v[v &lt; 0] = 0\n   ...:     \n\nIn [5]: df\nOut[5]: \n       a      b\n0 0 days 0 days\n1 0 days 2 days\n2 2 days 1 days\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'a': pd.to_timedelta([0, -1, 2], 'd'),\n   ...:                    'b': pd.to_timedelta([-3, 2, 1], 'd')})\n\nIn [3]: df[df &lt; pd.Timedelta(0)] = 0\n\nIn [4]: df\nOut[4]: \n       a      b\n0 0 days 0 days\n1 0 days 2 days\n2 2 days 1 days\n"
"In [248]:\n\nd = {112: 'en', 113: 'es', 114: 'es', 111: 'en'}\ndf['D'] = df['U'].map(d)\ndf\nOut[248]:\n     U   L   D\n0  111  en  en\n1  112  en  en\n2  112  es  en\n3  113  es  es\n4  113  ja  es\n5  113  zh  es\n6  114  es  es\n"
"import pandas as pd\nimport numpy as np #for the random integer example\ndf = pd.DataFrame(np.random.randint(0.0,100.0,size=(10,4)),\n              index=range(10,20),\n              columns=['col1','col2','col3','col4'],\n              dtype='float64')\n\nIn [14]: df.head(3)\nOut[14]:\n    col1    col2    col3    col4\n    10  3   38  86  65\n    11  98  3   66  68\n    12  88  46  35  68\n\nfrom sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(df.values)\n\nIn [15]: scaled_features[:3,:] #lost the indices\nOut[15]:\narray([[-1.89007341,  0.05636005,  1.74514417,  0.46669562],\n       [ 1.26558518, -1.35264122,  0.82178747,  0.59282958],\n       [ 0.93341059,  0.37841748, -0.60941542,  0.59282958]])\n\nscaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n\nIn [17]:  scaled_features_df.head(3)\nOut[17]:\n    col1    col2    col3    col4\n10  -1.890073   0.056360    1.745144    0.466696\n11  1.265585    -1.352641   0.821787    0.592830\n12  0.933411    0.378417    -0.609415   0.592830\n\nfrom sklearn_pandas import DataFrameMapper\n\nmapper = DataFrameMapper([(df.columns, StandardScaler())])\nscaled_features = mapper.fit_transform(df.copy(), 4)\nscaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)\n"
"df_obj = df.select_dtypes(['object'])\nprint (df_obj)\n0    a  \n1    c  \n\ndf[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\nprint (df)\n\n   0   1\n0  a  10\n1  c   5\n\ndf[0] = df[0].str.strip()\n"
"&gt;&gt;&gt; d\n    A   B  C\n0  11  13  5\n1   6   7  4\n2   8   3  6\n3   4   8  7\n4   0   1  7\n&gt;&gt;&gt; (d.A + d.B) / d.C\n0    4.800000\n1    3.250000\n2    1.833333\n3    1.714286\n4    0.142857\n&gt;&gt;&gt; d.A &gt; d.C\n0     True\n1     True\n2     True\n3    False\n4    False\n\n&gt;&gt;&gt; d.apply(lambda row: min([row['A'], row['B']])-row['C'], axis=1)\n0    6\n1    2\n2   -3\n3   -3\n4   -7\n\nd['A'][:-1] &lt; d['C'][1:]\n\nd['A'][1:] &lt; d['C'][:-1]\n"
"In [23]: import difflib \n\nIn [24]: difflib.get_close_matches\nOut[24]: &lt;function difflib.get_close_matches&gt;\n\nIn [25]: df2.index = df2.index.map(lambda x: difflib.get_close_matches(x, df1.index)[0])\n\nIn [26]: df2\nOut[26]: \n      letter\none        a\ntwo        b\nthree      c\nfour       d\nfive       e\n\nIn [31]: df1.join(df2)\nOut[31]: \n       number letter\none         1      a\ntwo         2      b\nthree       3      c\nfour        4      d\nfive        5      e\n\ndf1 = DataFrame([[1,'one'],[2,'two'],[3,'three'],[4,'four'],[5,'five']], columns=['number', 'name'])\ndf2 = DataFrame([['a','one'],['b','too'],['c','three'],['d','fours'],['e','five']], columns=['letter', 'name'])\n\ndf2['name'] = df2['name'].apply(lambda x: difflib.get_close_matches(x, df1['name'])[0])\ndf1.merge(df2)\n"
"df = pd.DataFrame(np.random.randn(10,2), columns=['col1','col2'])\ndf['col3'] = np.arange(len(df))**2 * 100 + 100\n\nIn [5]: df\nOut[5]: \n       col1      col2  col3\n0 -1.000075 -0.759910   100\n1  0.510382  0.972615   200\n2  1.872067 -0.731010   500\n3  0.131612  1.075142  1000\n4  1.497820  0.237024  1700\n\nplt.scatter(df.col1, df.col2, s=df.col3)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=df.col3)\n\ncolors = np.where(df.col3 &gt; 300, 'r', 'k')\nplt.scatter(df.col1, df.col2, s=120, c=colors)\n# OR (with pandas 0.13 and up)\ndf.plot(kind='scatter', x='col1', y='col2', s=120, c=colors)\n\ncond = df.col3 &gt; 300\nsubset_a = df[cond].dropna()\nsubset_b = df[~cond].dropna()\nplt.scatter(subset_a.col1, subset_a.col2, s=120, c='b', label='col3 &gt; 300')\nplt.scatter(subset_b.col1, subset_b.col2, s=60, c='r', label='col3 &lt;= 300') \nplt.legend()\n\ndf['subset'] = np.select([df.col3 &lt; 150, df.col3 &lt; 400, df.col3 &lt; 600],\n                         [0, 1, 2], -1)\nfor color, label in zip('bgrm', [0, 1, 2, -1]):\n    subset = df[df.subset == label]\n    plt.scatter(subset.col1, subset.col2, s=120, c=color, label=str(label))\nplt.legend()\n"
"&gt;&gt;&gt; frame['HighScore'] = frame[['test1','test2','test3']].max(axis=1)\n&gt;&gt;&gt; frame\n    name  test1  test2  test3  HighScore\n0   bill     85     35     51         85\n1    joe     75     45     61         75\n2  steve     85     83     45         85\n"
"In [23]:\ndf = pd.DataFrame(columns=['A'])\ndf\n\nOut[23]:\nEmpty DataFrame\nColumns: [A]\nIndex: []\n\nIn [24]:    \npd.concat([df,pd.DataFrame(columns=list('BCD'))])\n\nOut[24]:\nEmpty DataFrame\nColumns: [A, B, C, D]\nIndex: []\n"
"dataframe[column].value_counts().index.tolist()\n['apple', 'sausage', 'banana', 'cheese']\n"
"In [16]: def shuffle(df, n=1, axis=0):     \n    ...:     df = df.copy()\n    ...:     for _ in range(n):\n    ...:         df.apply(np.random.shuffle, axis=axis)\n    ...:     return df\n    ...:     \n\nIn [17]: df = pd.DataFrame({'A':range(10), 'B':range(10)})\n\nIn [18]: shuffle(df)\n\nIn [19]: df\nOut[19]: \n   A  B\n0  8  5\n1  1  7\n2  7  3\n3  6  2\n4  3  4\n5  0  1\n6  9  0\n7  4  6\n8  2  8\n9  5  9\n"
"df.id.apply(str)\n\n0        123\n1        512\n2      zhub1\n3    12354.3\n4        129\n5        753\n6        295\n7        610\n\ndf['id'].astype(basestring)\n0        123\n1        512\n2      zhub1\n3    12354.3\n4        129\n5        753\n6        295\n7        610\nName: id, dtype: object\n"
"&gt;&gt;&gt; from ast import literal_eval\n&gt;&gt;&gt; literal_eval('[1.23, 2.34]')\n[1.23, 2.34]\n"
"pd.read_csv('test.csv', sep='|', skiprows=range(1, 10))\n\na\nb\nc\nd\ne\nf\n\n&gt;&gt;&gt; pd.read_csv(f, header=None)\n   0\n0  a\n1  b\n2  c\n3  d\n4  e\n5  f\n\n&gt;&gt;&gt; pd.read_csv(f, header=3)\n   d\n0  e\n1  f\n\n&gt;&gt;&gt; pd.read_csv(f, header=[2, 4])                                                                                                                                                                        \n   c\n   e\n0  f\n\n&gt;&gt;&gt; pd.read_csv(f, skiprows=3)                                                                                                                                                                      \n   d\n0  e\n1  f\n\n&gt;&gt;&gt; pd.read_csv(f, skiprows=[2, 4])                                                                                                                                                                      \n   a\n0  b\n1  d\n2  f\n"
"df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n\nIn [162]: df\nOut[162]:\n   colA  ColB  colC  colD  colE  colF  colG\n0    44    45    26    26    40    26    46\n1    47    16    38    47    48    22    37\n2    19    28    36    18    40    18    46\n3    50    14    12    33    12    44    23\n4    39    47    16    42    33    48    38\n\ndf = pd.read_csv('data.csv', index_col=0)\n"
'x = p.Series()\nN = 4\nfor i in xrange(N):\n   x = x.set_value(i, i**2)\n\n0    0\n1    1\n2    4\n3    9\n'
'Frame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData))\n\nFrame = Frame.append(pandas.DataFrame(data = SomeNewLineOfData), ignore_index=True)\n'
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; for line in open("whitespace.csv"):\n...     print repr(line)\n...     \n\'a\\t  b\\tc 1 2\\n\'\n\'d\\t  e\\tf 3 4\\n\'\n&gt;&gt;&gt; pd.read_csv("whitespace.csv", header=None, delimiter=r"\\s+")\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n&gt;&gt;&gt; pd.read_csv("whitespace.csv", header=None, delim_whitespace=True)\n   0  1  2  3  4\n0  a  b  c  1  2\n1  d  e  f  3  4\n'
'In [14]: df.reindex(["Z", "C", "A"])\nOut[14]:\ncompany  Amazon  Apple  Yahoo\nZ             0      0    150\nC           173      0      0\nA             0    130      0\n\nIn [12]: df.sort_index(ascending=False)\nOut[12]:\ncompany  Amazon  Apple  Yahoo\nname\nZ             0      0    150\nC           173      0      0\nA             0    130      0\n\nIn [13]: df = df.sort_index(ascending=False)\n'
"&gt;&gt;&gt; df1.join(df2, how='outer')\n            V1  V2\nA 1/1/2012  12  15\n  2/1/2012  14 NaN\n  3/1/2012 NaN  21\nB 1/1/2012  15  24\n  2/1/2012   8   9\nC 1/1/2012  17 NaN\n  2/1/2012   9 NaN\nD 1/1/2012 NaN   7\n  2/1/2012 NaN  16\n"
'&gt;&gt;&gt; !cat ragged.csv\n1,2,3\n1,2,3,4\n1,2,3,4,5\n1,2\n1,2,3,4\n&gt;&gt;&gt; my_cols = ["A", "B", "C", "D", "E"]\n&gt;&gt;&gt; pd.read_csv("ragged.csv", names=my_cols, engine=\'python\')\n   A  B   C   D   E\n0  1  2   3 NaN NaN\n1  1  2   3   4 NaN\n2  1  2   3   4   5\n3  1  2 NaN NaN NaN\n4  1  2   3   4 NaN\n'
'df2 = pd.DataFrame(index=df1.index)\n'
'&gt;&gt;&gt; df.append([df_try]*5,ignore_index=True)\n\n    Store  Dept       Date  Weekly_Sales IsHoliday\n0       1     1 2010-02-05      24924.50     False\n1       1     1 2010-02-12      46039.49      True\n2       1     1 2010-02-19      41595.55     False\n3       1     1 2010-02-26      19403.54     False\n4       1     1 2010-03-05      21827.90     False\n5       1     1 2010-03-12      21043.39     False\n6       1     1 2010-03-19      22136.64     False\n7       1     1 2010-03-26      26229.21     False\n8       1     1 2010-04-02      57258.43     False\n9       1     1 2010-02-12      46039.49      True\n10      1     1 2010-02-12      46039.49      True\n11      1     1 2010-02-12      46039.49      True\n12      1     1 2010-02-12      46039.49      True\n13      1     1 2010-02-12      46039.49      True\n'
'&gt;&gt;&gt; df.loc[df[\'a\'] == 1, \'b\'].sum()\n15\n\ndf.loc[(df[\'a\'] == 1) &amp; (df[\'c\'] == 2), \'b\'].sum()\n\n&gt;&gt;&gt; df.query("a == 1")[\'b\'].sum()\n15\n\ndf.query("a == 1 and c == 2")[\'b\'].sum()\n\n&gt;&gt;&gt; df.groupby(\'a\')[\'b\'].sum()[1]\n15\n\n&gt;&gt;&gt; df.groupby(\'a\')[\'b\'].sum()\na\n1    15\n2     8\n'
"df['bar', 'three'] = [0, 1, 2]\ndf = df.sort_index(axis=1)\nprint(df)\n\n        bar                        baz          \n        one       two  three       one       two\nA -0.212901  0.503615      0 -1.660945  0.446778\nB -0.803926 -0.417570      1 -0.336827  0.989343\nC  3.400885 -0.214245      2  0.895745  1.011671\n"
'In [1]: df1 = pd.DataFrame([[1, 2], [3, 4]])\n\nIn [2]: df2 = pd.DataFrame([[3, 4], [1, 2]], index=[1, 0])\n\nIn [3]: df1 == df2\nException: Can only compare identically-labeled DataFrame objects\n\nIn [4]: df2.sort_index(inplace=True)\n\nIn [5]: df1 == df2\nOut[5]: \n      0     1\n0  True  True\n1  True  True\n\nIn [11]: df1.sort_index().sort_index(axis=1) == df2.sort_index().sort_index(axis=1)\nOut[11]: \n      0     1\n0  True  True\n1  True  True\n'
"&gt;&gt; df = pd.DataFrame({'a': [1, 2, 3]})\n&gt;&gt; df[df.a &gt; 1].sum()   \na    5\ndtype: int64\n\n&gt;&gt; df[(df.a &gt; 1) &amp; (df.a &lt; 3)].sum()\na    2\ndtype: int64\n"
"import timeit\n\nsetup = '''\nimport numpy, pandas\ndf = pandas.DataFrame(numpy.zeros(shape=[10, 1000]))\ndictionary = df.to_dict()\n'''\n\n# f = ['value = dictionary[5][5]', 'value = df.loc[5, 5]', 'value = df.iloc[5, 5]']\nf = ['value = [val[5] for col,val in dictionary.items()]', 'value = df.loc[5]', 'value = df.iloc[5]']\n\nfor func in f:\n    print(func)\n    print(min(timeit.Timer(func, setup).repeat(3, 100000)))\n\nvalue = [val[5] for col,val in dictionary.iteritems()]\n25.5416321754\nvalue = df.loc[5]\n5.68071913719\nvalue = df.iloc[5]\n4.56006002426\n\ndf.loc['2000-1-1':'2000-3-31']\n"
"In [6]: np.dtype('datetime64[ns]') == np.dtype('&lt;M8[ns]')\nOut[6]: True\n"
"names   ages\nbob     05\ntom     4\nsuzy    3\n\nimport pandas as pd\n\ndf = pd.read_excel('Book1.xlsx',sheetname='Sheet1',header=0,converters={'names':str,'ages':str})\n&gt;&gt;&gt; df\n       names ages\n   0   bob   05\n   1   tom   4\n   2   suzy  3\n"
"df.loc[:,'quantity'] *= -1 #seems to prevent SettingWithCopyWarning \n"
'letters_only = re.sub("[^a-zA-Z]",  # Search for all non-letters\n                          " ",          # Replace all non-letters with spaces\n                          str(location))\n'
"import matplotlib.pyplot as plt\nimport numpy as np\nfrom pandas import DataFrame\ndf = DataFrame(np.random.randn(5, 3), columns=['A', 'B', 'C'])\n\nfig, ax = plt.subplots()\nax3 = ax.twinx()\nrspine = ax3.spines['right']\nrspine.set_position(('axes', 1.15))\nax3.set_frame_on(True)\nax3.patch.set_visible(False)\nfig.subplots_adjust(right=0.7)\n\ndf.A.plot(ax=ax, style='b-')\n# same ax as above since it's automatically added on the right\ndf.B.plot(ax=ax, style='r-', secondary_y=True)\ndf.C.plot(ax=ax3, style='g-')\n\n# add legend --&gt; take advantage of pandas providing us access\n# to the line associated with the right part of the axis\nax3.legend([ax.get_lines()[0], ax.right_ax.get_lines()[0], ax3.get_lines()[0]],\\\n           ['A','B','C'], bbox_to_anchor=(1.5, 0.5))\n"
"s = pd.Series([1,2,3,2,2,3,5,2,3,2,np.nan])\nfig, ax = plt.subplots()\nax.hist(s, alpha=0.9, color='blue')\n\nax.hist(s.dropna(), alpha=0.9, color='blue')\n\ndfj2_MARKET1['VSPD1_perc'].hist(ax=axes[0], alpha=0.9, color='blue')\n"
"In [20]:\ndf.groupby(['Name','Type','ID']).count().reset_index()\n\nOut[20]:\n    Name   Type  ID  Count\n0  Book1  ebook   1      2\n1  Book2  paper   2      2\n2  Book3  paper   3      1\n\nIn [25]:\ndf['Count'] = df.groupby(['Name'])['ID'].transform('count')\ndf.drop_duplicates()\n\nOut[25]:\n    Name   Type  ID  Count\n0  Book1  ebook   1      2\n1  Book2  paper   2      2\n2  Book3  paper   3      1\n"
"import pandas as pd\nimport numpy as np\n\nnp.random.seed(2015)\ndf = pd.DataFrame([])\nfor i in range(5):\n    data = dict(zip(np.random.choice(10, replace=False, size=5),\n                    np.random.randint(10, size=5)))\n    data = pd.DataFrame(data.items())\n    data = data.transpose()\n    data.columns = data.iloc[0]\n    data = data.drop(data.index[[0]])\n    df = df.append(data)\nprint('{}\\n'.format(df))\n# 0   0   1   2   3   4   5   6   7   8   9\n# 1   6 NaN NaN   8   5 NaN NaN   7   0 NaN\n# 1 NaN   9   6 NaN   2 NaN   1 NaN NaN   2\n# 1 NaN   2   2   1   2 NaN   1 NaN NaN NaN\n# 1   6 NaN   6 NaN   4   4   0 NaN NaN NaN\n# 1 NaN   9 NaN   9 NaN   7   1   9 NaN NaN\n\nnp.random.seed(2015)\ndata = []\nfor i in range(5):\n    data.append(dict(zip(np.random.choice(10, replace=False, size=5),\n                         np.random.randint(10, size=5))))\ndf = pd.DataFrame(data)\nprint(df)\n"
"df = pd.DataFrame({'a':range(1,5), 'b':['a','b','c','d']})\ndf2 = df.iloc[[0, -1]]\n\nprint df2\n\n   a  b\n0  1  a\n3  4  d\n"
'a.loc[a.shift(-1) != a]\n\nOut[3]:\n\n1    1\n3    2\n4    3\n5    2\ndtype: int64\n\nIn [82]:\n\na.loc[a.diff() != 0]\nOut[82]:\n1    1\n2    2\n4    3\n5    2\ndtype: int64\n\nIn [87]:\n\na.loc[a.shift() != a]\nOut[87]:\n1    1\n2    2\n4    3\n5    2\ndtype: int64\n'
"df.to_csv('filename.csv', header=False)\n\ndf.to_csv('filename.tsv', sep='\\t', index=False)\n"
"import pandas as pd\n\ndf = pd.read_csv('filex.csv')\ndf['A'] = df['A'].astype('str')\ndf['B'] = df['B'].astype('str')\nmask = (df['A'].str.len() == 10) &amp; (df['B'].str.len() == 10)\ndf = df.loc[mask]\nprint(df)\n\nA,B\n123,abc\n1234,abcd\n1234567890,abcdefghij\n\n            A           B\n2  1234567890  abcdefghij\n"
"df.append(df.sum(numeric_only=True), ignore_index=True)\n\nbaz = 2*df['qux'].sum() + 3*df['bar'].sum()\n"
'import pandas as pd\n\ndf = pd.DataFrame(\n    [p, p.team, p.passing_att, p.passer_rating()] for p in game.players.passing()\n)\n'
"from sqlalchemy import create_engine\nimport pymysql\n\ndb_connection_str = 'mysql+pymysql://mysql_user:mysql_password@mysql_host/mysql_db'\ndb_connection = create_engine(db_connection_str)\n\ndf = pd.read_sql('SELECT * FROM table_name', con=db_connection)\n"
"In [13]:\n\ndf = pd.DataFrame(np.random.random((4,4)))\ndf.columns = pd.MultiIndex.from_product([[1,2],['A','B']])\nprint df\n          1                   2          \n          A         B         A         B\n0  0.543980  0.628078  0.756941  0.698824\n1  0.633005  0.089604  0.198510  0.783556\n2  0.662391  0.541182  0.544060  0.059381\n3  0.841242  0.634603  0.815334  0.848120\nIn [14]:\n\nprint df.iloc[:, df.columns.get_level_values(1)=='A']\n          1         2\n          A         A\n0  0.543980  0.756941\n1  0.633005  0.198510\n2  0.662391  0.544060\n3  0.841242  0.815334\n"
"for index, row in rche_df.iterrows():\n    if isinstance(row.wgs1984_latitude, float):\n        row = row.copy()\n        target = row.address_chi        \n        dict_temp = geocoding(target)\n        rche_df.loc[index, 'wgs1984_latitude'] = dict_temp['lat']\n        rche_df.loc[index, 'wgs1984_longitude'] = dict_temp['long']\n"
'df.drop(df.columns[[1, 69]], axis=1, inplace=True)\n'
"import numpy as np\ndf1['randNumCol'] = np.random.randint(1, 6, df1.shape[0])\n\ndf1['randNumCol'] = np.random.choice([1, 9, 20], df1.shape[0])\n"
"In [165]: df=pd.read_csv(url, index_col=0, na_values=['(NA)']).fillna(0)\n\nIn [166]: df.dtypes\nOut[166]:\nGeoName                    object\nComponentName              object\nIndustryId                  int64\nIndustryClassification     object\nDescription                object\n2004                        int64\n2005                        int64\n2006                        int64\n2007                        int64\n2008                        int64\n2009                        int64\n2010                        int64\n2011                        int64\n2012                        int64\n2013                        int64\n2014                      float64\ndtype: object\n\nIn [271]: df\nOut[271]:\n     id    a  b  c  d  e    f\n0  id_3  AAA  6  3  5  8    1\n1  id_9    3  7  5  7  3  BBB\n2  id_7    4  2  3  5  4    2\n3  id_0    7  3  5  7  9    4\n4  id_0    2  4  6  4  0    2\n\nIn [272]: df.dtypes\nOut[272]:\nid    object\na     object\nb      int64\nc      int64\nd      int64\ne      int64\nf     object\ndtype: object\n\nIn [273]: cols = df.columns.drop('id')\n\nIn [274]: df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n\nIn [275]: df\nOut[275]:\n     id    a  b  c  d  e    f\n0  id_3  NaN  6  3  5  8  1.0\n1  id_9  3.0  7  5  7  3  NaN\n2  id_7  4.0  2  3  5  4  2.0\n3  id_0  7.0  3  5  7  9  4.0\n4  id_0  2.0  4  6  4  0  2.0\n\nIn [276]: df.dtypes\nOut[276]:\nid     object\na     float64\nb       int64\nc       int64\nd       int64\ne       int64\nf     float64\ndtype: object\n\ncols = df.columns[df.dtypes.eq('object')]\n"
"In [229]: df = pd.DataFrame({True:[1,2,3],False:[3,4,5]}); df\nOut[229]: \n   False  True \n0      3      1\n1      4      2\n2      5      3\n\nIn [230]: df[[True]]\nValueError: Item wrong length 1 instead of 3.\n\nIn [231]: df.loc[[True]]\nOut[231]: \n   False  True \n0      3      1\n\nIn [258]: df2 = pd.DataFrame({'A':[1,2,3],'B':[3,4,5]}); df2\nOut[258]: \n   A  B\n0  1  3\n1  2  4\n2  3  5\n\nIn [259]: df2[['B']]\nOut[259]: \n   B\n0  3\n1  4\n2  5\n\nIn [237]: df2.loc[[True,False,True], 'B']\nOut[237]: \n0    3\n2    5\nName: B, dtype: int64\n\nIn [239]: df2.loc[1:2]\nOut[239]: \n   A  B\n1  2  4\n2  3  5\n\nIn [271]: df2[1:2]\nOut[271]: \n   A  B\n1  2  4\n"
'&gt;&gt;&gt; df.loc[df.groupby("item")["diff"].idxmin()]\n   item  diff  otherstuff\n1     1     1           2\n6     2    -6           2\n7     3     0           0\n\n[3 rows x 3 columns]\n\n&gt;&gt;&gt; df.sort_values("diff").groupby("item", as_index=False).first()\n   item  diff  otherstuff\n0     1     1           2\n1     2    -6           2\n2     3     0           0\n\n[3 rows x 3 columns]\n'
'import pandas as pd\nimport numpy as np\n\nfrom sklearn.base import TransformerMixin\n\nclass DataFrameImputer(TransformerMixin):\n\n    def __init__(self):\n        """Impute missing values.\n\n        Columns of dtype object are imputed with the most frequent value \n        in column.\n\n        Columns of other types are imputed with mean of column.\n\n        """\n    def fit(self, X, y=None):\n\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype(\'O\') else X[c].mean() for c in X],\n            index=X.columns)\n\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\ndata = [\n    [\'a\', 1, 2],\n    [\'b\', 1, 1],\n    [\'b\', 2, 2],\n    [np.nan, np.nan, np.nan]\n]\n\nX = pd.DataFrame(data)\nxt = DataFrameImputer().fit_transform(X)\n\nprint(\'before...\')\nprint(X)\nprint(\'after...\')\nprint(xt)\n\nbefore...\n     0   1   2\n0    a   1   2\n1    b   1   1\n2    b   2   2\n3  NaN NaN NaN\nafter...\n   0         1         2\n0  a  1.000000  2.000000\n1  b  1.000000  1.000000\n2  b  2.000000  2.000000\n3  b  1.333333  1.666667\n'
"In [17]: df1 = pd.DataFrame(dict(A = range(10000)),index=pd.date_range('20130101',periods=10000,freq='s'))\n\nIn [18]: df1\nOut[18]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 10000 entries, 2013-01-01 00:00:00 to 2013-01-01 02:46:39\nFreq: S\nData columns (total 1 columns):\nA    10000  non-null values\ndtypes: int64(1)\n\nIn [19]: df4 = pd.DataFrame()\n\nThe concat\n\nIn [20]: %timeit pd.concat([df1,df2,df3])\n1000 loops, best of 3: 270 us per loop\n\nThis is equavalent of your append\n\nIn [21]: %timeit pd.concat([df4,df1,df2,df3])\n10 loops, best of \n\n 3: 56.8 ms per loop\n"
'&gt;&gt;&gt; result.index\nInt64Index([0, 1, 2], dtype=int64)\n\n&gt;&gt;&gt; result.index += 1 \n&gt;&gt;&gt; result.index\nInt64Index([1, 2, 3], dtype=int64)\n'
"import pandas as pd\nfields = ['star_name', 'ra']\n\ndf = pd.read_csv('data.csv', skipinitialspace=True, usecols=fields)\n# See the keys\nprint df.keys()\n# See content in 'star_name'\nprint df.star_name\n"
'import pandas as pd\nimport xml.etree.ElementTree as ET\nimport io\n\ndef iter_docs(author):\n    author_attr = author.attrib\n    for doc in author.iter(\'document\'):\n        doc_dict = author_attr.copy()\n        doc_dict.update(doc.attrib)\n        doc_dict[\'data\'] = doc.text\n        yield doc_dict\n\nxml_data = io.StringIO(u\'\'\'\\\n&lt;author type="XXX" language="EN" gender="xx" feature="xx" web="foobar.com"&gt;\n    &lt;documents count="N"&gt;\n        &lt;document KEY="e95a9a6c790ecb95e46cf15bee517651" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="bc360cfbafc39970587547215162f0db" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="19e71144c50a8b9160b3f0955e906fce" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="21d4af9021a174f61b884606c74d9e42" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="28a45eb2460899763d709ca00ddbb665" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="a0c0712a6a351f85d9f5757e9fff8946" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="626726ba8d34d15d02b6d043c55fe691" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...]\n]]&gt;\n        &lt;/document&gt;\n        &lt;document KEY="2cb473e0f102e2e4a40aa3006e412ae4" web="www.foo_bar_exmaple.com"&gt;&lt;![CDATA[A large text with lots of strings and punctuations symbols [...] [...]\n]]&gt;\n        &lt;/document&gt;\n    &lt;/documents&gt;\n&lt;/author&gt;\n\'\'\')\n\netree = ET.parse(xml_data) #create an ElementTree object \ndoc_df = pd.DataFrame(list(iter_docs(etree.getroot())))\n\ndef iter_author(etree):\n    for author in etree.iter(\'author\'):\n        for row in iter_docs(author):\n            yield row\n'
"In [99]: df = pd.DataFrame(np.random.randn(8, 4), columns=['A','B','C','D'])\n\nIn [100]: s = df.xs(3)\n\nIn [101]: s.name = 10\n\nIn [102]: df.append(s)\nOut[102]: \n           A         B         C         D\n0  -2.083321 -0.153749  0.174436  1.081056\n1  -1.026692  1.495850 -0.025245 -0.171046\n2   0.072272  1.218376  1.433281  0.747815\n3  -0.940552  0.853073 -0.134842 -0.277135\n4   0.478302 -0.599752 -0.080577  0.468618\n5   2.609004 -1.679299 -1.593016  1.172298\n6  -0.201605  0.406925  1.983177  0.012030\n7   1.158530 -2.240124  0.851323 -0.240378\n10 -0.940552  0.853073 -0.134842 -0.277135\n"
"In [11]: ts = pd.Timestamp('2014-01-23 00:00:00', tz=None)\n\nIn [12]: ts.to_pydatetime()\nOut[12]: datetime.datetime(2014, 1, 23, 0, 0)\n\nIn [13]: rng = pd.date_range('1/10/2011', periods=3, freq='D')\n\nIn [14]: rng.to_pydatetime()\nOut[14]:\narray([datetime.datetime(2011, 1, 10, 0, 0),\n       datetime.datetime(2011, 1, 11, 0, 0),\n       datetime.datetime(2011, 1, 12, 0, 0)], dtype=object)\n"
"df['dA'] = df['A'] - df['A'].shift(-1)\n"
'df[\'var2\'] = pd.Series([round(val, 2) for val in df[\'var2\']], index = df.index)\ndf[\'var3\'] = pd.Series(["{0:.2f}%".format(val * 100) for val in df[\'var3\']], index = df.index)\n'
"In [3]: df.index.unique(level='co')\nOut[3]: Index(['DE', 'FR'], dtype='object', name='co')\n"
'In [5]:\n\npd.concat([df_a,df_b], axis=1)\nOut[5]:\n        AAseq Biorep  Techrep Treatment     mz      AAseq1 Biorep1  Techrep1  \\\n0  ELVISLIVES      A        1         C  500.0  ELVISLIVES       A         1   \n1  ELVISLIVES      A        1         C  500.5  ELVISLIVES       A         1   \n2  ELVISLIVES      A        1         C  501.0  ELVISLIVES       A         1   \n\n  Treatment1  inte1  \n0          C   1100  \n1          C   1050  \n2          C   1010  \n\nIn [6]:\n\ndf_a.merge(df_b, left_index=True, right_index=True)\nOut[6]:\n        AAseq Biorep  Techrep Treatment     mz      AAseq1 Biorep1  Techrep1  \\\n0  ELVISLIVES      A        1         C  500.0  ELVISLIVES       A         1   \n1  ELVISLIVES      A        1         C  500.5  ELVISLIVES       A         1   \n2  ELVISLIVES      A        1         C  501.0  ELVISLIVES       A         1   \n\n  Treatment1  inte1  \n0          C   1100  \n1          C   1050  \n2          C   1010  \n\nIn [7]:\n\ndf_a.join(df_b)\nOut[7]:\n        AAseq Biorep  Techrep Treatment     mz      AAseq1 Biorep1  Techrep1  \\\n0  ELVISLIVES      A        1         C  500.0  ELVISLIVES       A         1   \n1  ELVISLIVES      A        1         C  500.5  ELVISLIVES       A         1   \n2  ELVISLIVES      A        1         C  501.0  ELVISLIVES       A         1   \n\n  Treatment1  inte1  \n0          C   1100  \n1          C   1050  \n2          C   1010  \n'
"df = pd.DataFrame([\n    ['red', 0, 0],\n    ['red', 1, 1],\n    ['red', 2, 2],\n    ['red', 3, 3],\n    ['red', 4, 4],\n    ['red', 5, 5],\n    ['red', 6, 6],\n    ['red', 7, 7],\n    ['red', 8, 8],\n    ['red', 9, 9],\n    ['blue', 0, 0],\n    ['blue', 1, 1],\n    ['blue', 2, 4],\n    ['blue', 3, 9],\n    ['blue', 4, 16],\n    ['blue', 5, 25],\n    ['blue', 6, 36],\n    ['blue', 7, 49],\n    ['blue', 8, 64],\n    ['blue', 9, 81],\n], columns=['color', 'x', 'y'])\n\ndf = df.pivot(index='x', columns='color', values='y')\n\ndf.plot()\n"
"colnames=['TIME', 'X', 'Y', 'Z'] \nuser1 = pd.read_csv('dataset/1.csv', names=colnames, header=None)\n"
"import matplotlib.pyplot as plt\nimport pandas as pd\nfrom pandas.table.plotting import table # EDIT: see deprecation warnings below\n\nax = plt.subplot(111, frame_on=False) # no visible frame\nax.xaxis.set_visible(False)  # hide the x axis\nax.yaxis.set_visible(False)  # hide the y axis\n\ntable(ax, df)  # where df is your data frame\n\nplt.savefig('mytable.png')\n\nfirst  second\nbar    one       1.991802\n       two       0.403415\nbaz    one      -1.024986\n       two      -0.522366\nfoo    one       0.350297\n       two      -0.444106\nqux    one      -0.472536\n       two       0.999393\ndtype: float64\n\ndf = df.reset_index() \ndf\n    first second       0\n0   bar    one  1.991802\n1   bar    two  0.403415\n2   baz    one -1.024986\n3   baz    two -0.522366\n4   foo    one  0.350297\n5   foo    two -0.444106\n6   qux    one -0.472536\n7   qux    two  0.999393\n\ndf.ix[df.duplicated('first') , 'first'] = '' # see deprecation warnings below\ndf\n  first second         0\n0   bar    one  1.991802\n1          two  0.403415\n2   baz    one -1.024986\n3          two -0.522366\n4   foo    one  0.350297\n5          two -0.444106\n6   qux    one -0.472536\n7          two  0.999393\n\nnew_cols = df.columns.values\nnew_cols[:2] = '',''  # since my index columns are the two left-most on the table\ndf.columns = new_cols \n\ntable(ax, df, rowLabels=['']*df.shape[0], loc='center')\n\nfrom pandas.tools.plotting import table\n\nfrom pandas.plotting import table \n\ndf.ix[df.duplicated('first') , 'first'] = ''\n\ndf.loc[df.duplicated('first') , 'first'] = ''\n"
"import pandas as pd\ndef csv_to_df(path: str) -&gt; pd.DataFrame:\n    return pd.read_csv(path, skiprows=1, sep='\\t', comment='#')\n\n&gt; help(csv_to_df)\nHelp on function csv_to_df in module __main__:\ncsv_to_df(path:str) -&gt; pandas.core.frame.DataFrame\n"
"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer(sparse_output=True)\n\ndf = df.join(\n            pd.DataFrame.sparse.from_spmatrix(\n                mlb.fit_transform(df.pop('Col3')),\n                index=df.index,\n                columns=mlb.classes_))\n\nIn [38]: df\nOut[38]:\n  Col1  Col2  Apple  Banana  Grape  Orange\n0    C  33.0      1       1      0       1\n1    A   2.5      1       0      1       0\n2    B  42.0      0       1      0       0\n\nIn [39]: df.dtypes\nOut[39]:\nCol1                object\nCol2               float64\nApple     Sparse[int32, 0]\nBanana    Sparse[int32, 0]\nGrape     Sparse[int32, 0]\nOrange    Sparse[int32, 0]\ndtype: object\n\nIn [40]: df.memory_usage()\nOut[40]:\nIndex     128\nCol1       24\nCol2       24\nApple      16    #  &lt;--- NOTE!\nBanana     16    #  &lt;--- NOTE!\nGrape       8    #  &lt;--- NOTE!\nOrange      8    #  &lt;--- NOTE!\ndtype: int64\n\nmlb = MultiLabelBinarizer()\ndf = df.join(pd.DataFrame(mlb.fit_transform(df.pop('Col3')),\n                          columns=mlb.classes_,\n                          index=df.index))\n\nIn [77]: df\nOut[77]:\n  Col1  Col2  Apple  Banana  Grape  Orange\n0    C  33.0      1       1      0       1\n1    A   2.5      1       0      1       0\n2    B  42.0      0       1      0       0\n"
"import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': list('abcdefg')})\ndf.loc[5, 'c1'] = 'Value'\n\n&gt;&gt;&gt; df\n      c1\n0      a\n1      b\n2      c\n3      d\n4      e\n5  Value\n6      g\n\ndf = df.assign(c2 = df['c1'])\n# OR:\ndf['c2'] = df['c1']\n\ndf.loc[df['c1'] == 'Value', 'c2'] = 10\n\n&gt;&gt;&gt; df\n      c1  c2\n0      a   a\n1      b   b\n2      c   c\n3      d   d\n4      e   e\n5  Value  10\n6      g   g\n\ndf['c1'].loc[df['c1'] == 'Value'] = 10\n# or:\ndf.loc[df['c1'] == 'Value', 'c1'] = 10\n\n&gt;&gt;&gt; df\n      c1\n0      a\n1      b\n2      c\n3      d\n4      e\n5     10\n6      g\n"
"In [117]: import pandas\n\nIn [118]: import random\n\nIn [119]: df = pandas.DataFrame(np.random.randn(100, 4), columns=list('ABCD'))\n\nIn [120]: rows = random.sample(df.index, 10)\n\nIn [121]: df_10 = df.ix[rows]\n\nIn [122]: df_90 = df.drop(rows)\n"
'import numpy as np\ncount, division = np.histogram(series)\n\ncount, division = np.histogram(series, bins = [-201,-149,949,1001])\n\nseries.hist(bins=division)\n'
"In [22]: orders_df['C'] = orders_df.Action.apply(\n               lambda x: (1 if x == 'Sell' else -1))\n\nIn [23]: orders_df   # New column C represents the sign of the transaction\nOut[23]:\n   Prices  Amount Action  C\n0       3      57   Sell  1\n1      89      42   Sell  1\n2      45      70    Buy -1\n3       6      43   Sell  1\n4      60      47   Sell  1\n5      19      16    Buy -1\n6      56      89   Sell  1\n7       3      28    Buy -1\n8      56      69   Sell  1\n9      90      49    Buy -1\n\nIn [24]: orders_df['Value'] = orders_df.Prices * orders_df.Amount * orders_df.C\n\nIn [25]: orders_df   # The resulting dataframe\nOut[25]:\n   Prices  Amount Action  C  Value\n0       3      57   Sell  1    171\n1      89      42   Sell  1   3738\n2      45      70    Buy -1  -3150\n3       6      43   Sell  1    258\n4      60      47   Sell  1   2820\n5      19      16    Buy -1   -304\n6      56      89   Sell  1   4984\n7       3      28    Buy -1    -84\n8      56      69   Sell  1   3864\n9      90      49    Buy -1  -4410\n"
"df['size'] = df.groupby(['A','B']).transform(np.size)\n"
"df.unstack(level=0).plot(kind='bar', subplots=True)\n"
'df = pd.read_sql_query(\'select * from "Stat_Table"\',con=engine)\n'
"df.groupby(['col2','col3'], as_index=False).sum()\n"
"In [25]: %timeit df.loc[('a', 'A'), ('c', 'C')]\n10000 loops, best of 3: 187 µs per loop\n\nIn [26]: %timeit df.at[('a', 'A'), ('c', 'C')]\n100000 loops, best of 3: 8.33 µs per loop\n\nIn [35]: %timeit df.get_value(('a', 'A'), ('c', 'C'))\n100000 loops, best of 3: 3.62 µs per loop\n"
'df1 = datasX.iloc[:, :72]\ndf2 = datasX.iloc[:, 72:]\n'
"df = pd.DataFrame({'A': [1,2,3,4]})\n\ndf['B'] = [3,4]   # or df['B'] = np.array([3,4])\n\ndf['B'] = pd.Series([3,4])\n\ndf\n#   A     B\n#0  1   3.0\n#1  2   4.0\n#2  3   NaN          # NaN because the value at index 2 and 3 doesn't exist in the Series\n#3  4   NaN\n\ndf.apply(lambda col: col.drop_duplicates().reset_index(drop=True))\n\n#   A     B\n#0  1   1.0\n#1  2   5.0\n#2  7   9.0\n#3  8   NaN\n"
'df  = read_csv(filename, index_col = 0,header = 0)\nself.datatable = QtGui.QTableWidget(parent=self)\nself.datatable.setColumnCount(len(df.columns))\nself.datatable.setRowCount(len(df.index))\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        self.datatable.setItem(i,j,QtGui.QTableWidgetItem(str(df.iget_value(i, j))))\n\n%gui qt5 \nfrom PyQt5.QtWidgets import QWidget,QScrollArea, QTableWidget, QVBoxLayout,QTableWidgetItem\nimport pandas as pd\n\nwin = QWidget()\nscroll = QScrollArea()\nlayout = QVBoxLayout()\ntable = QTableWidget()\nscroll.setWidget(table)\nlayout.addWidget(table)\nwin.setLayout(layout)    \n\n\ndf = pd.DataFrame({"a" : [4 ,5, 6],"b" : [7, 8, 9],"c" : [10, 11, 12]},index = [1, 2, 3])\ntable.setColumnCount(len(df.columns))\ntable.setRowCount(len(df.index))\nfor i in range(len(df.index)):\n    for j in range(len(df.columns)):\n        table.setItem(i,j,QTableWidgetItem(str(df.iloc[i, j])))\n\nwin.show()\n'
"In [17]: df = DataFrame(randn(20,4),columns=list('ABCD'))\n\nIn [18]: df[(df['A']&gt;0) &amp; (df['B']&gt;0) &amp; (df['C']&gt;0)]\nOut[18]: \n           A         B         C         D\n12  0.491683  0.137766  0.859753 -1.041487\n13  0.376200  0.575667  1.534179  1.247358\n14  0.428739  1.539973  1.057848 -1.254489\n\nIn [19]: df[(df['A']&gt;0) &amp; (df['B']&gt;0) &amp; (df['C']&gt;0)].count()\nOut[19]: \nA    3\nB    3\nC    3\nD    3\ndtype: int64\n\nIn [20]: len(df[(df['A']&gt;0) &amp; (df['B']&gt;0) &amp; (df['C']&gt;0)])\nOut[20]: 3\n"
"from sklearn.metrics.pairwise import cosine_similarity\nfrom scipy import sparse\n\nA =  np.array([[0, 1, 0, 0, 1], [0, 0, 1, 1, 1],[1, 1, 0, 1, 0]])\nA_sparse = sparse.csr_matrix(A)\n\nsimilarities = cosine_similarity(A_sparse)\nprint('pairwise dense output:\\n {}\\n'.format(similarities))\n\n#also can output sparse matrices\nsimilarities_sparse = cosine_similarity(A_sparse,dense_output=False)\nprint('pairwise sparse output:\\n {}\\n'.format(similarities_sparse))\n\npairwise dense output:\n[[ 1.          0.40824829  0.40824829]\n[ 0.40824829  1.          0.33333333]\n[ 0.40824829  0.33333333  1.        ]]\n\npairwise sparse output:\n(0, 1)  0.408248290464\n(0, 2)  0.408248290464\n(0, 0)  1.0\n(1, 0)  0.408248290464\n(1, 2)  0.333333333333\n(1, 1)  1.0\n(2, 1)  0.333333333333\n(2, 0)  0.408248290464\n(2, 2)  1.0\n\nA_sparse.transpose()\n"
"In [3]: df\nOut[3]: \n   M  D     Y  Apples  Oranges\n0  5  6  1990      12        3\n1  5  7  1990      14        4\n2  5  8  1990      15       34\n3  5  9  1990      23       21\n\nIn [4]: df.dtypes\nOut[4]: \nM          int64\nD          int64\nY          int64\nApples     int64\nOranges    int64\ndtype: object\n\n# in 0.12, use this\nIn [5]: pd.to_datetime((df.Y*10000+df.M*100+df.D).apply(str),format='%Y%m%d')\n\n# in 0.13 the above or this will work\nIn [5]: pd.to_datetime(df.Y*10000+df.M*100+df.D,format='%Y%m%d')\nOut[5]: \n0   1990-05-06 00:00:00\n1   1990-05-07 00:00:00\n2   1990-05-08 00:00:00\n3   1990-05-09 00:00:00\ndtype: datetime64[ns]\n"
"&gt;&gt;&gt; origin.pivot(index='label', columns='type')['value']\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n\n&gt;&gt;&gt; origin.pivot_table(values='value', index='label', columns='type')\n       value      \ntype       a  b  c\nlabel             \nx          1  2  3\ny          4  5  6\nz          7  8  9\n\n[3 rows x 3 columns]\n\n&gt;&gt;&gt; origin.groupby(['label', 'type'])['value'].aggregate('mean').unstack()\ntype   a  b  c\nlabel         \nx      1  2  3\ny      4  5  6\nz      7  8  9\n\n[3 rows x 3 columns]\n"
'&gt;&gt;&gt; df = pd.DataFrame(np.random.rand(15, 5), index=[0]*15)\n&gt;&gt;&gt; df[0] = range(15)\n&gt;&gt;&gt; df\n    0         1         2         3         4\n0   0  0.746300  0.346277  0.220362  0.172680\n0   1  0.657324  0.687169  0.384196  0.214118\n0   2  0.016062  0.858784  0.236364  0.963389\n[...]\n0  13  0.510273  0.051608  0.230402  0.756921\n0  14  0.950544  0.576539  0.642602  0.907850\n\n[15 rows x 5 columns]\n\n&gt;&gt;&gt; df.groupby(np.arange(len(df))//10)\n&lt;pandas.core.groupby.DataFrameGroupBy object at 0xb208492c&gt;\n&gt;&gt;&gt; for k,g in df.groupby(np.arange(len(df))//10):\n...     print(k,g)\n...     \n0    0         1         2         3         4\n0  0  0.746300  0.346277  0.220362  0.172680\n0  1  0.657324  0.687169  0.384196  0.214118\n0  2  0.016062  0.858784  0.236364  0.963389\n[...]\n0  8  0.241049  0.246149  0.241935  0.563428\n0  9  0.493819  0.918858  0.193236  0.266257\n\n[10 rows x 5 columns]\n1     0         1         2         3         4\n0  10  0.037693  0.370789  0.369117  0.401041\n0  11  0.721843  0.862295  0.671733  0.605006\n[...]\n0  14  0.950544  0.576539  0.642602  0.907850\n\n[5 rows x 5 columns]\n'
"pd.date_range('2014-01-01', periods=10) + pd.Timedelta(days=1)\n"
'alldata_balance = alldata[(alldata[IBRD] !=0) | (alldata[IMF] !=0)]\n'
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(np.nan, index=[0, 1, 2, 3], columns=['A', 'B'])\n\n&gt;&gt;&gt; df.dtypes\nA    float64\nB    float64\ndtype: object\n\n&gt;&gt;&gt; df.values\narray([[nan, nan],\n       [nan, nan],\n       [nan, nan],\n       [nan, nan]])\n"
'In [6]:\n\ndf[\'date\'] = pd.to_datetime(df[\'date\'])\ndf[\'year\'], df[\'month\'] = df[\'date\'].dt.year, df[\'date\'].dt.month\ndf\nOut[6]:\n        date  Count  year  month\n0 2010-06-30    525  2010      6\n1 2010-07-30    136  2010      7\n2 2010-08-31    125  2010      8\n3 2010-09-30     84  2010      9\n4 2010-10-29   4469  2010     10\n\nIn [18]:\n\ndf[\'date\'] = pd.to_datetime(df[\'date\'])\ndf[\'year\'], df[\'month\'] = df[\'date\'].apply(lambda x: x.year), df[\'date\'].apply(lambda x: x.month)\ndf\nOut[18]:\n        date  Count  year  month\n0 2010-06-30    525  2010      6\n1 2010-07-30    136  2010      7\n2 2010-08-31    125  2010      8\n3 2010-09-30     84  2010      9\n4 2010-10-29   4469  2010     10\n\nIn [20]:\n\nt="""date   Count\n6/30/2010   525\n7/30/2010   136\n8/31/2010   125\n9/30/2010   84\n10/29/2010  4469"""\ndf = pd.read_csv(io.StringIO(t), sep=\'\\s+\', parse_dates=[0])\ndf.info()\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 5 entries, 0 to 4\nData columns (total 2 columns):\ndate     5 non-null datetime64[ns]\nCount    5 non-null int64\ndtypes: datetime64[ns](1), int64(1)\nmemory usage: 120.0 bytes\n'
"In [212]:\ndf = pd.DataFrame(np.random.randint(0, 2, (10, 4)), columns=list('abcd'))\ndf.apply(pd.Series.value_counts)\nOut[212]:\n   a  b  c  d\n0  4  6  4  3\n1  6  4  6  7\n"
'df = pd.DataFrame({"a": [1,2,3,1,2,3], "b":[1,1,1,2,2,2], "c":np.random.rand(6)})\npd.pivot_table(df, index=["a"], columns=["b"], values=["c"], aggfunc=np.sum)\n\nb         1         2\na                    \n1  0.528470  0.484766\n2  0.187277  0.144326\n3  0.866832  0.650100\n\ndf.groupby([\'a\',\'b\'])[\'c\'].sum()\n\na  b\n1  1    0.528470\n   2    0.484766\n2  1    0.187277\n   2    0.144326\n3  1    0.866832\n   2    0.650100\nName: c, dtype: float64\n\nprint df.groupby(["a","b"]).sum()\n            c\na b          \n1 1  0.528470\n  2  0.484766\n2 1  0.187277\n  2  0.144326\n3 1  0.866832\n  2  0.650100\n'
'In [14]: %timeit test_sql_write(df)\n1 loops, best of 3: 6.24 s per loop\n\nIn [15]: %timeit test_hdf_fixed_write(df)\n1 loops, best of 3: 237 ms per loop\n\nIn [16]: %timeit test_hdf_table_write(df)\n1 loops, best of 3: 901 ms per loop\n\nIn [17]: %timeit test_csv_write(df)\n1 loops, best of 3: 3.44 s per loop\n\nIn [18]: %timeit test_sql_read()\n1 loops, best of 3: 766 ms per loop\n\nIn [19]: %timeit test_hdf_fixed_read()\n10 loops, best of 3: 19.1 ms per loop\n\nIn [20]: %timeit test_hdf_table_read()\n10 loops, best of 3: 39 ms per loop\n\nIn [22]: %timeit test_csv_read()\n1 loops, best of 3: 620 ms per loop\n\nimport sqlite3\nimport os\nfrom pandas.io import sql\n\nIn [3]: df = DataFrame(randn(1000000,2),columns=list(\'AB\'))\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nData columns (total 2 columns):\nA    1000000  non-null values\nB    1000000  non-null values\ndtypes: float64(2)\n\ndef test_sql_write(df):\n    if os.path.exists(\'test.sql\'):\n        os.remove(\'test.sql\')\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.write_frame(df, name=\'test_table\', con=sql_db)\n    sql_db.close()\n\ndef test_sql_read():\n    sql_db = sqlite3.connect(\'test.sql\')\n    sql.read_frame("select * from test_table", sql_db)\n    sql_db.close()\n\ndef test_hdf_fixed_write(df):\n    df.to_hdf(\'test_fixed.hdf\',\'test\',mode=\'w\')\n\ndef test_csv_read():\n    pd.read_csv(\'test.csv\',index_col=0)\n\ndef test_csv_write(df):\n    df.to_csv(\'test.csv\',mode=\'w\')    \n\ndef test_hdf_fixed_read():\n    pd.read_hdf(\'test_fixed.hdf\',\'test\')\n\ndef test_hdf_table_write(df):\n    df.to_hdf(\'test_table.hdf\',\'test\',format=\'table\',mode=\'w\')\n\ndef test_hdf_table_read():\n    pd.read_hdf(\'test_table.hdf\',\'test\')\n'
'apt-get install python-numpy\n\n--system-site-packages\n'
'In [11]: df1\nOut[11]:\n            abc  xyz\nDate\n2013-06-01  100  200\n2013-06-03  -20   50\n2013-08-15   40   -5\n2014-01-20   25   15\n2014-02-21   60   80\n\nIn [12]: g = df1.groupby(pd.Grouper(freq="M")) \xa0# DataFrameGroupBy (grouped by Month)\n\nIn [13]: g.sum()\nOut[13]:\n            abc  xyz\nDate\n2013-06-30   80  250\n2013-07-31  NaN  NaN\n2013-08-31   40   -5\n2013-09-30  NaN  NaN\n2013-10-31  NaN  NaN\n2013-11-30  NaN  NaN\n2013-12-31  NaN  NaN\n2014-01-31   25   15\n2014-02-28   60   80\n\nIn [14]: df1.resample("M", how=\'sum\') \xa0# the same\nOut[14]:\n            abc  xyz\nDate\n2013-06-30   40  125\n2013-07-31  NaN  NaN\n2013-08-31   40   -5\n2013-09-30  NaN  NaN\n2013-10-31  NaN  NaN\n2013-11-30  NaN  NaN\n2013-12-31  NaN  NaN\n2014-01-31   25   15\n2014-02-28   60   80\n\nIn [21]: df\nOut[21]:\n        Date  abc  xyz\n0 2013-06-01  100  200\n1 2013-06-03  -20   50\n2 2013-08-15   40   -5\n3 2014-01-20   25   15\n4 2014-02-21   60   80\n\nIn [22]: pd.DatetimeIndex(df.Date).to_period("M")  # old way\nOut[22]:\n&lt;class \'pandas.tseries.period.PeriodIndex\'&gt;\n[2013-06, ..., 2014-02]\nLength: 5, Freq: M\n\nIn [23]: per = df.Date.dt.to_period("M")  # new way to get the same\n\nIn [24]: g = df.groupby(per)\n\nIn [25]: g.sum()  # dang not quite what we want (doesn\'t fill in the gaps)\nOut[25]:\n         abc  xyz\n2013-06   80  250\n2013-08   40   -5\n2014-01   25   15\n2014-02   60   80\n'
"In [11]: df\nOut[11]:\n   0  1  2      3\n0  1  2  a  16.86\n1  1  2  a  17.18\n2  1  4  a  17.03\n3  2  5  b  17.28\n\nIn [12]: df.pivot_table(values=3, index=[0, 1], columns=2, aggfunc='mean')  # desired?\nOut[12]:\n2        a      b\n0 1\n1 2  17.02    NaN\n  4  17.03    NaN\n2 5    NaN  17.28\n\nIn [13]: df1 = df.set_index([0, 1, 2])\n\nIn [14]: df1\nOut[14]:\n           3\n0 1 2\n1 2 a  16.86\n    a  17.18\n  4 a  17.03\n2 5 b  17.28\n\nIn [15]: df1.unstack(2)\nValueError: Index contains duplicate entries, cannot reshape\n\nIn [16]: df1.reset_index().pivot_table(values=3, index=[0, 1], columns=2, aggfunc='mean')\nOut[16]:\n2        a      b\n0 1\n1 2  17.02    NaN\n  4  17.03    NaN\n2 5    NaN  17.28\n"
"d = pandas.date_range(start='1/1/1980', end='11/1/1990', freq='MS')    \nprint(d)\n\nDatetimeIndex(['1980-01-01', '1980-02-01', '1980-03-01', '1980-04-01',\n               '1980-05-01', '1980-06-01', '1980-07-01', '1980-08-01',\n               '1980-09-01', '1980-10-01', \n               ...\n               '1990-02-01', '1990-03-01', '1990-04-01', '1990-05-01',\n               '1990-06-01', '1990-07-01', '1990-08-01', '1990-09-01',\n               '1990-10-01', '1990-11-01'],\n              dtype='datetime64[ns]', length=131, freq='MS', tz=None)\n"
"In [145]:\n# create a df\ndf = pd.DataFrame(np.random.randn(5,3))\ndf\n\nOut[145]:\n          0         1         2\n0 -2.845811 -0.182439 -0.526785\n1 -0.112547  0.661461  0.558452\n2  0.587060 -1.232262 -0.997973\n3 -1.009378 -0.062442  0.125875\n4 -1.129376  3.282447 -0.403731\n\nIn [146]:    \ndf.index = df.index.set_names(['foo'])\ndf\n\nOut[146]:\n            0         1         2\nfoo                              \n0   -2.845811 -0.182439 -0.526785\n1   -0.112547  0.661461  0.558452\n2    0.587060 -1.232262 -0.997973\n3   -1.009378 -0.062442  0.125875\n4   -1.129376  3.282447 -0.403731\n\nIn [147]:\ndf.reset_index().rename(columns={df.index.name:'bar'})\n\nOut[147]:\n   bar         0         1         2\n0    0 -2.845811 -0.182439 -0.526785\n1    1 -0.112547  0.661461  0.558452\n2    2  0.587060 -1.232262 -0.997973\n3    3 -1.009378 -0.062442  0.125875\n4    4 -1.129376  3.282447 -0.403731\n\nIn [149]:\ndf.rename_axis('bar').reset_index()\n\nOut[149]:\n   bar         0         1         2\n0    0 -2.845811 -0.182439 -0.526785\n1    1 -0.112547  0.661461  0.558452\n2    2  0.587060 -1.232262 -0.997973\n3    3 -1.009378 -0.062442  0.125875\n4    4 -1.129376  3.282447 -0.403731\n\ndf.index.name = 'bar'\n"
"from scipy.stats import ttest_ind\n\ncat1 = my_data[my_data['Category']=='cat1']\ncat2 = my_data[my_data['Category']=='cat2']\n\nttest_ind(cat1['values'], cat2['values'])\n&gt;&gt;&gt; (1.4927289925706944, 0.16970867501294376)\n"
"In [536]: result_df = df.loc[(df.index.get_level_values('A') &gt; 1.7) &amp; (df.index.get_level_values('B') &lt; 666)]\n\nIn [537]: result_df\nOut[537]: \n          C\nA   B      \n3.3 222  43\n    333  59\n5.5 333  56\n\nIn [538]: result_df.index.get_level_values('A')\nOut[538]: Index([3.3, 3.3, 5.5], dtype=object)\n\nIn [558]: df = store.select(STORE_EXTENT_BURSTS_DF_KEY)\n\nIn [559]: len(df)\nOut[559]: 12857\n\nIn [560]: df.sort(inplace=True)\n\nIn [561]: df_without_index = df.reset_index()\n\nIn [562]: %timeit df.loc[(df.index.get_level_values('END_TIME') &gt; 358200) &amp; (df.index.get_level_values('START_TIME') &lt; 361680)]\n1000 loops, best of 3: 562 µs per loop\n\nIn [563]: %timeit df_without_index[(df_without_index.END_TIME &gt; 358200) &amp; (df_without_index.START_TIME &lt; 361680)]\n1000 loops, best of 3: 507 µs per loop\n"
'import pandas as pd\nimport matplotlib.cm as cm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_clustered_stacked(dfall, labels=None, title=&quot;multiple stacked bar plot&quot;,  H=&quot;/&quot;, **kwargs):\n    &quot;&quot;&quot;Given a list of dataframes, with identical columns and index, create a clustered stacked bar plot. \nlabels is a list of the names of the dataframe, used for the legend\ntitle is a string for the title of the plot\nH is the hatch used for identification of the different dataframe&quot;&quot;&quot;\n\n    n_df = len(dfall)\n    n_col = len(dfall[0].columns) \n    n_ind = len(dfall[0].index)\n    axe = plt.subplot(111)\n\n    for df in dfall : # for each data frame\n        axe = df.plot(kind=&quot;bar&quot;,\n                      linewidth=0,\n                      stacked=True,\n                      ax=axe,\n                      legend=False,\n                      grid=False,\n                      **kwargs)  # make bar plots\n\n    h,l = axe.get_legend_handles_labels() # get the handles we want to modify\n    for i in range(0, n_df * n_col, n_col): # len(h) = n_col * n_df\n        for j, pa in enumerate(h[i:i+n_col]):\n            for rect in pa.patches: # for each index\n                rect.set_x(rect.get_x() + 1 / float(n_df + 1) * i / float(n_col))\n                rect.set_hatch(H * int(i / n_col)) #edited part     \n                rect.set_width(1 / float(n_df + 1))\n\n    axe.set_xticks((np.arange(0, 2 * n_ind, 2) + 1 / float(n_df + 1)) / 2.)\n    axe.set_xticklabels(df.index, rotation = 0)\n    axe.set_title(title)\n\n    # Add invisible data to add another legend\n    n=[]        \n    for i in range(n_df):\n        n.append(axe.bar(0, 0, color=&quot;gray&quot;, hatch=H * i))\n\n    l1 = axe.legend(h[:n_col], l[:n_col], loc=[1.01, 0.5])\n    if labels is not None:\n        l2 = plt.legend(n, labels, loc=[1.01, 0.1]) \n    axe.add_artist(l1)\n    return axe\n\n# create fake dataframes\ndf1 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;],\n                   columns=[&quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;, &quot;M&quot;])\ndf2 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;],\n                   columns=[&quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;, &quot;M&quot;])\ndf3 = pd.DataFrame(np.random.rand(4, 5),\n                   index=[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;], \n                   columns=[&quot;I&quot;, &quot;J&quot;, &quot;K&quot;, &quot;L&quot;, &quot;M&quot;])\n\n# Then, just call :\nplot_clustered_stacked([df1, df2, df3],[&quot;df1&quot;, &quot;df2&quot;, &quot;df3&quot;])\n    \n\nplot_clustered_stacked([df1, df2, df3],\n                       [&quot;df1&quot;, &quot;df2&quot;, &quot;df3&quot;],\n                       cmap=plt.cm.viridis)\n\ndf1[&quot;Name&quot;] = &quot;df1&quot;\ndf2[&quot;Name&quot;] = &quot;df2&quot;\ndf3[&quot;Name&quot;] = &quot;df3&quot;\ndfall = pd.concat([pd.melt(i.reset_index(),\n                           id_vars=[&quot;Name&quot;, &quot;index&quot;]) # transform in tidy format each df\n                   for i in [df1, df2, df3]],\n                   ignore_index=True)\n\ndfall.set_index([&quot;Name&quot;, &quot;index&quot;, &quot;variable&quot;], inplace=1)\ndfall[&quot;vcs&quot;] = dfall.groupby(level=[&quot;Name&quot;, &quot;index&quot;]).cumsum()\ndfall.reset_index(inplace=True) \n\n&gt;&gt;&gt; dfall.head(6)\n  Name index variable     value       vcs\n0  df1     A        I  0.717286  0.717286\n1  df1     B        I  0.236867  0.236867\n2  df1     C        I  0.952557  0.952557\n3  df1     D        I  0.487995  0.487995\n4  df1     A        J  0.174489  0.891775\n5  df1     B        J  0.332001  0.568868\n\nc = [&quot;blue&quot;, &quot;purple&quot;, &quot;red&quot;, &quot;green&quot;, &quot;pink&quot;]\nfor i, g in enumerate(dfall.groupby(&quot;variable&quot;)):\n    ax = sns.barplot(data=g[1],\n                     x=&quot;index&quot;,\n                     y=&quot;vcs&quot;,\n                     hue=&quot;Name&quot;,\n                     color=c[i],\n                     zorder=-i, # so first bars stay on top\n                     edgecolor=&quot;k&quot;)\nax.legend_.remove() # remove the redundant legends \n'
'print (df.index.min())\nprint (df.index.max())\n\n2014-03-13 00:00:00\n2014-03-31 00:00:00\n'
"df = pd.DataFrame({'foo':np.random.random(), 'index':range(10000)})\ndf_with_index = df.set_index(['index'])\n\ndf[df['index'] == 999]\n\n#           foo  index\n# 999  0.375489    999\n\ndf_with_index.loc[999]\n# foo        0.375489\n# index    999.000000\n# Name: 999, dtype: float64\n\nIn [254]: %timeit df[df['index'] == 999]\n1000 loops, best of 3: 368 µs per loop\n\nIn [255]: %timeit df_with_index.loc[999]\n10000 loops, best of 3: 57.7 µs per loop\n\nIn [220]: %timeit df.set_index(['index'])\n1000 loops, best of 3: 330 µs per loop\n"
"import pandas as pd\n\n\ndf = pd.DataFrame({'my_dates':['2015-01-01','2015-01-02','2015-01-03'],'myvals':[1,2,3]})\ndf['my_dates'] = pd.to_datetime(df['my_dates'])\n\ndf['day_of_week'] = df['my_dates'].dt.day_name()\n\n    my_dates  myvals day_of_week\n0 2015-01-01       1    Thursday\n1 2015-01-02       2      Friday\n2 2015-01-03       3    Saturday\n\nimport pandas as pd\n\ndf = pd.DataFrame({'my_dates':['2015-01-01','2015-01-02','2015-01-03'],'myvals':[1,2,3]})\ndf['my_dates'] = pd.to_datetime(df['my_dates'])\ndf['day_of_week'] = df['my_dates'].dt.weekday_name\n\n    my_dates  myvals day_of_week\n0 2015-01-01       1    Thursday\n1 2015-01-02       2      Friday\n2 2015-01-03       3    Saturday\n\nimport pandas as pd\n\ndf = pd.DataFrame({'my_dates':['2015-01-01','2015-01-02','2015-01-03'],'myvals':[1,2,3]})\ndf['my_dates'] = pd.to_datetime(df['my_dates'])\ndf['day_of_week'] = df['my_dates'].dt.dayofweek\n\ndays = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}\n\ndf['day_of_week'] = df['day_of_week'].apply(lambda x: days[x])\n\n    my_dates  myvals day_of_week\n0 2015-01-01       1       Thurs\n1 2015-01-02       2         Fri\n2 2015-01-01       3       Thurs\n"
"df['col'] = df['col'].str[:9]\n\ndf['col'] = df['col'].str.slice(0, 9)\n"
"&gt;&gt;&gt; df['purchase'].astype(str).astype(int)\n"
'print(df.loc[[159220]])\n'
'lambda x: x*10 if x&lt;2 else (x**2 if x&lt;4 else x+10)\n'
"df = df.value_counts().rename_axis('unique_values').reset_index(name='counts')\nprint (df)\n   unique_values  counts\n0              2       3\n1              1       2\n\ndf = df.value_counts().rename_axis('unique_values').to_frame('counts')\nprint (df)\n               counts\nunique_values        \n2                   3\n1                   2\n"
"In [39]: df2.pivot_table(values='X', rows='Y', cols='Z', \n                         aggfunc=lambda x: len(x.unique()))\nOut[39]: \nZ   Z1  Z2  Z3\nY             \nY1   1   1 NaN\nY2 NaN NaN   1\n"
'df.sex.value_counts(normalize=True)\n'
"x = np.random.randn(30)\n\nfig, ax = plt.subplots(1,2, figsize=(10,4))\n\nax[0].hist(x, density=True, color='grey')\n\nhist, bins = np.histogram(x)\nax[1].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=(bins[1]-bins[0]), color='grey')\n\nax[0].set_title('normed=True')\nax[1].set_title('hist = hist / hist.sum()')\n"
'k1 = df.loc[(df.Product == p_id) &amp; (df.Time &gt;= start_time) &amp; (df.Time &lt; end_time), [\'Time\', \'Product\']]\n\ndf[[\'Time\', \'Product\']].query(\'Product == p_id and Month &lt; mn and Year == yr\')\n\nIn [9]: df = DataFrame({\'gender\': np.random.choice([\'m\', \'f\'], size=10), \'price\': poisson(100, size=10)})\n\nIn [10]: df\nOut[10]:\n  gender  price\n0      m     89\n1      f    123\n2      f    100\n3      m    104\n4      m     98\n5      m    103\n6      f    100\n7      f    109\n8      f     95\n9      m     87\n\nIn [11]: df.query(\'gender == "m" and price &lt; 100\')\nOut[11]:\n  gender  price\n0      m     89\n4      m     98\n9      m     87\n\nk1 = df[[\'Time\', \'Product\']].query(\'Product == p_id and start_time &lt;= Time &lt; end_time\')\n'
'In [136]: for date, new_df in df.groupby(level=0):\n     ...:     print(new_df)\n     ...:     \n                    observation1  observation2\ndate       Time                               \n2012-11-02 9:15:00     79.373668           224\n           9:16:00    130.841316           477\n\n                    observation1  observation2\ndate       Time                               \n2012-11-03 9:15:00     45.312814           835\n           9:16:00    123.776946           623\n           9:17:00    153.766460           624\n           9:18:00    463.276946           626\n           9:19:00    663.176934           622\n           9:20:00    763.773330           621\n\n                    observation1  observation2\ndate       Time                               \n2012-11-04 9:15:00    115.449437           122\n           9:16:00    123.776946           555\n           9:17:00    153.766460           344\n           9:18:00    463.276946           212\n'
'In[2]: \n    df.value1 = df.value1.round()\n    print df\n\nOut[2]:\n    item  value1  value2\n    0    a       1     1.3\n    1    a       2     2.5\n    2    a       0     0.0\n    3    b       3    -1.0\n    4    b       5    -1.0\n'
"df1 = pd.DataFrame([[11, 12], [21, 22]], columns=['c1', 'c2'], index=['i1', 'i2'])\n\npd.DataFrame().reindex_like(df1)\nOut: \n    c1  c2\ni1 NaN NaN\ni2 NaN NaN   \n"
'print df\n  col1 education\n0    a       9th\n1    b       9th\n2    c       8th\n\nprint df.education == \'9th\'\n0     True\n1     True\n2    False\nName: education, dtype: bool\n\nprint df[df.education == \'9th\']\n  col1 education\n0    a       9th\n1    b       9th\n\nprint df[df.education == \'9th\'].shape[0]\n2\nprint len(df[df[\'education\'] == \'9th\'])\n2\n\nimport perfplot, string\nnp.random.seed(123)\n\n\ndef shape(df):\n    return df[df.education == \'a\'].shape[0]\n\ndef len_df(df):\n    return len(df[df[\'education\'] == \'a\'])\n\ndef query_count(df):\n    return df.query(\'education == "a"\').education.count()\n\ndef sum_mask(df):\n    return (df.education == \'a\').sum()\n\ndef sum_mask_numpy(df):\n    return (df.education.values == \'a\').sum()\n\ndef make_df(n):\n    L = list(string.ascii_letters)\n    df = pd.DataFrame(np.random.choice(L, size=n), columns=[\'education\'])\n    return df\n\nperfplot.show(\n    setup=make_df,\n    kernels=[shape, len_df, query_count, sum_mask, sum_mask_numpy],\n    n_range=[2**k for k in range(2, 25)],\n    logx=True,\n    logy=True,\n    equality_check=False, \n    xlabel=\'len(df)\')\n'
'In [28]:\ndf.groupby(df.columns.tolist(),as_index=False).size()\n\nOut[28]:\none    three  two  \nFalse  False  True     1\nTrue   False  False    2\n       True   True     1\ndtype: int64\n'
"df[df.index.isin(a_list) &amp; df.a_col.isnull()]\n\ndf2 = df.loc[a_list]\ndf2[df2.a_col.isnull()]\n\ndf.loc[a_list].query('a_col != a_col')\n"
"In [11]: medals = df.pivot_table('no of medals', ['Year', 'Country'], 'medal')\n\nIn [12]: medals\nOut[12]:\nmedal             Bronze  Gold  Silver\nYear Country\n1896 Afghanistan       3     5       4\n     Algeria           3     1       2\n\nIn [12]: medals.reindex_axis(['Gold', 'Silver', 'Bronze'], axis=1)\nOut[12]:\nmedal             Gold  Silver  Bronze\nYear Country\n1896 Afghanistan     5       4       3\n     Algeria         1       2       3\n"
"df.loc[df['line_race'] == 0, 'rating'] = 0\n"
"In [10]: df = pd.DataFrame({'A' : [0, 1], 'B' : [1, 6]})\n\nIn [11]: import io\n\nIn [12]: s = io.StringIO()\n\nIn [13]: df.to_csv(s)\n\nIn [14]: s.getvalue()\nOut[14]: ',A,B\\n0,0,1\\n1,1,6\\n'\n"
"df['ID'] = df['ID'].apply(lambda x: '{0:0&gt;15}'.format(x))\n\ndf['ID'] = df['ID'].apply(lambda x: x.zfill(15))\n"
'def set_color_cycle(self, clist=None):\n    if clist is None:\n        clist = rcParams[\'axes.color_cycle\']\n    self.color_cycle = itertools.cycle(clist\n\ndef set_color_cycle(self, clist):\n    """\n    Set the color cycle for any future plot commands on this Axes.\n\n    *clist* is a list of mpl color specifiers.\n    """\n    self._get_lines.set_color_cycle(clist)\n    self._get_patches_for_fill.set_color_cycle(clist)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfor i in range(3):\n    plt.plot(np.arange(10) + i)\n\n# for Matplotlib version &lt; 1.5\nplt.gca().set_color_cycle(None)\n# for Matplotlib version &gt;= 1.5\nplt.gca().set_prop_cycle(None)\n\nfor i in range(3):\n    plt.plot(np.arange(10, 1, -1) + i)\n\nplt.show()\n'
"import pandas as pd\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\ndef tmpFunc(df):\n    df['c'] = df.a + df.b\n    return df\n\ndef applyParallel(dfGrouped, func):\n    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n    return pd.concat(retLst)\n\nif __name__ == '__main__':\n    df = pd.DataFrame({'a': [6, 2, 2], 'b': [4, 5, 6]},index= ['g1', 'g1', 'g2'])\n    print 'parallel version: '\n    print applyParallel(df.groupby(df.index), tmpFunc)\n\n    print 'regular version: '\n    print df.groupby(df.index).apply(tmpFunc)\n\n    print 'ideal version (does not work): '\n    print df.groupby(df.index).applyParallel(tmpFunc)\n"
'In [12]:\n\npd.concat([df,df1], axis=0, ignore_index=True)\nOut[12]:\n   attr_1  attr_2  attr_3  id  quantity\n0       0       1     NaN   1        20\n1       1       1     NaN   2        23\n2       1       1     NaN   3        19\n3       0       0     NaN   4        19\n4       1     NaN       0   5         8\n5       0     NaN       1   6        13\n6       1     NaN       1   7        20\n7       1     NaN       1   8        25\n'
"import pandas as pd\n\ndf1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                    'B': ['B0', 'B1', 'B2', 'B3'],\n                    'D': ['D0', 'D1', 'D2', 'D3']},\n                    index=[0, 2, 3,4])\n\ndf2 = pd.DataFrame({'A1': ['A4', 'A5', 'A6', 'A7'],\n                    'C': ['C4', 'C5', 'C6', 'C7'],\n                    'D2': ['D4', 'D5', 'D6', 'D7']},\n                    index=[ 4, 5, 6 ,7])\n\n\ndf1.reset_index(drop=True, inplace=True)\ndf2.reset_index(drop=True, inplace=True)\n\ndf = pd.concat( [df1, df2], axis=1) \n\n    A   B   D   A1  C   D2\n0   A0  B0  D0  A4  C4  D4\n1   A1  B1  D1  A5  C5  D5\n2   A2  B2  D2  A6  C6  D6\n3   A3  B3  D3  A7  C7  D7\n"
"In [194]:\ndf['A'] = 'foo'\ndf\n\nOut[194]:\n     A\n0  foo\n1  foo\n2  foo\n3  foo\n"
"df['CreationDate'].str.len()\n\ndf['Length'] = df['CreationDate'].str.len()\ndf\nOut: \n                                                    CreationDate  Length\n2013-12-22 15:25:02                  [ubuntu, mac-osx, syslinux]       3\n2009-12-14 14:29:32  [ubuntu, mod-rewrite, laconica, apache-2.2]       4\n2013-12-22 15:42:00               [ubuntu, nat, squid, mikrotik]       4\n\nser = pd.Series([random.sample(string.ascii_letters, \n                               random.randint(1, 20)) for _ in range(10**6)])\n\n%timeit ser.apply(lambda x: len(x))\n1 loop, best of 3: 425 ms per loop\n\n%timeit ser.str.len()\n1 loop, best of 3: 248 ms per loop\n\n%timeit [len(x) for x in ser]\n10 loops, best of 3: 84 ms per loop\n\n%timeit pd.Series([len(x) for x in ser], index=ser.index)\n1 loop, best of 3: 236 ms per loop\n"
"all_df[all_df['City'].isnull()]\n"
'df = pd.DataFrame({\'text\':[\'a..b?!??\', \'%hgh&amp;12\',\'abc123!!!\', \'$$$1234\']})\ndf\n        text\n0   a..b?!??\n1    %hgh&amp;12\n2  abc123!!!\n3    $$$1234\n\ndf[\'text\'] = df[\'text\'].str.replace(r\'[^\\w\\s]+\', \'\')\n\ndf\n     text\n0      ab\n1   hgh12\n2  abc123\n3    1234\n\nimport re\np = re.compile(r\'[^\\w\\s]+\')\ndf[\'text\'] = [p.sub(\'\', x) for x in df[\'text\'].tolist()]\n\ndf\n     text\n0      ab\n1   hgh12\n2  abc123\n3    1234\n\nimport string\n\npunct = \'!"#$%&amp;\\\'()*+,-./:;&lt;=&gt;?@[\\\\]^_`{}~\'   # `|` is not present here\ntranstab = str.maketrans(dict.fromkeys(punct, \'\'))\n\ndf[\'text\'] = \'|\'.join(df[\'text\'].tolist()).translate(transtab).split(\'|\')\n\ndf\n     text\n0      ab\n1   hgh12\n2  abc123\n3    1234\n\ndf = pd.DataFrame({\'text\': [\n    \'a..b?!??\', np.nan, \'%hgh&amp;12\',\'abc123!!!\', \'$$$1234\', np.nan]})\n\nidx = np.flatnonzero(df[\'text\'].notna())\ncol_idx = df.columns.get_loc(\'text\')\ndf.iloc[idx,col_idx] = [\n    p.sub(\'\', x) for x in df.iloc[idx,col_idx].tolist()]\n\ndf\n     text\n0      ab\n1     NaN\n2   hgh12\n3  abc123\n4    1234\n5     NaN\n\nv = pd.Series(df.values.ravel())\ndf[:] = translate(v).values.reshape(df.shape)\n\nv = df.stack()\nv[:] = translate(v)\ndf = v.unstack()\n\ndef pd_replace(df):\n    return df.assign(text=df[\'text\'].str.replace(r\'[^\\w\\s]+\', \'\'))\n\n\ndef re_sub(df):\n    p = re.compile(r\'[^\\w\\s]+\')\n    return df.assign(text=[p.sub(\'\', x) for x in df[\'text\'].tolist()])\n\ndef translate(df):\n    punct = string.punctuation.replace(\'|\', \'\')\n    transtab = str.maketrans(dict.fromkeys(punct, \'\'))\n\n    return df.assign(\n        text=\'|\'.join(df[\'text\'].tolist()).translate(transtab).split(\'|\')\n    )\n\n# MaxU\'s version (https://stackoverflow.com/a/50444659/4909087)\ndef pd_translate(df):\n    punct = string.punctuation.replace(\'|\', \'\')\n    transtab = str.maketrans(dict.fromkeys(punct, \'\'))\n\n    return df.assign(text=df[\'text\'].str.translate(transtab))\n\nfrom timeit import timeit\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nres = pd.DataFrame(\n       index=[\'pd_replace\', \'re_sub\', \'translate\', \'pd_translate\'],\n       columns=[10, 50, 100, 500, 1000, 5000, 10000, 50000],\n       dtype=float\n)\n\nfor f in res.index: \n    for c in res.columns:\n        l = [\'a..b?!??\', \'%hgh&amp;12\',\'abc123!!!\', \'$$$1234\'] * c\n        df = pd.DataFrame({\'text\' : l})\n        stmt = \'{}(df)\'.format(f)\n        setp = \'from __main__ import df, {}\'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=30)\n\nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel("N"); \nax.set_ylabel("time (relative)");\n\nplt.show()\n'
'line = DataFrame({"onset": 30.0, "length": 1.3}, index=[3])\ndf2 = concat([df.iloc[:2], line, df.iloc[2:]]).reset_index(drop=True)\n'
"In [27]: s.apply(lambda x: type(x))\nOut[27]: \na  b\n1  2    &lt;type 'numpy.float64'&gt;\n3  6    &lt;type 'numpy.float64'&gt;\n4  4    &lt;type 'numpy.float64'&gt;\n\nSeries(s.reset_index().apply(f, axis=1).values, index=s.index)\n"
"s1 = pd.merge(df1, df2, how='inner', on=['user_id'])\n"
'df.plot(legend=False)\n'
"In [11]: df.applymap(np.isreal)\nOut[11]:\n          a     b\nitem\na      True  True\nb      True  True\nc      True  True\nd     False  True\ne      True  True\n\nIn [12]: df.applymap(np.isreal).all(1)\nOut[12]:\nitem\na        True\nb        True\nc        True\nd       False\ne        True\ndtype: bool\n\nIn [13]: df[~df.applymap(np.isreal).all(1)]\nOut[13]:\n        a    b\nitem\nd     bad  0.4\n\nIn [14]: np.argmin(df.applymap(np.isreal).all(1))\nOut[14]: 'd'\n\ndf.applymap(lambda x: isinstance(x, (int, float)))\n"
"In [33]: s1 = pd.merge(df1, df2, how='left', on=['Year', 'Week', 'Colour'])\n\nIn [39]: df = pd.merge(s1, df3[['Week', 'Colour', 'Val3']],\n                       how='left', on=['Week', 'Colour'])\n\nIn [40]: df\nOut[40]: \n   Year Week Colour  Val1  Val2 Val3\n0  2014    A    Red    50   NaN  NaN\n1  2014    B    Red    60   NaN   60\n2  2014    B  Black    70   100   10\n3  2014    C    Red    10    20  NaN\n4  2014    D  Green    20   NaN   20\n\n[5 rows x 6 columns]\n"
'In [81]: df\nOut[81]: \n                            x         y         z\nts                                               \n2014-05-15 10:38:00  0.120117  0.987305  0.116211\n2014-05-15 10:39:00  0.117188  0.984375  0.122070\n2014-05-15 10:40:00  0.119141  0.987305  0.119141\n2014-05-15 10:41:00  0.116211  0.984375  0.120117\n2014-05-15 10:42:00  0.119141  0.983398  0.118164\n\n[5 rows x 3 columns]\n\nIn [82]: def myfunc(args):\n   ....:        e=args[0] + 2*args[1]\n   ....:        f=args[1]*args[2] +1\n   ....:        g=args[2] + args[0] * args[1]\n   ....:        return [e,f,g]\n   ....: \n\nIn [83]: df.apply(myfunc ,axis=1)\nOut[83]: \n                            x         y         z\nts                                               \n2014-05-15 10:38:00  2.094727  1.114736  0.234803\n2014-05-15 10:39:00  2.085938  1.120163  0.237427\n2014-05-15 10:40:00  2.093751  1.117629  0.236770\n2014-05-15 10:41:00  2.084961  1.118240  0.234512\n2014-05-15 10:42:00  2.085937  1.116202  0.235327\n'
'import pandas as pd\nimport sqlite3\n\n#We\'ll use firelynx\'s tables:\npresidents = pd.DataFrame({"name": ["Bush", "Obama", "Trump"],\n                           "president_id":[43, 44, 45]})\nterms = pd.DataFrame({\'start_date\': pd.date_range(\'2001-01-20\', periods=5, freq=\'48M\'),\n                      \'end_date\': pd.date_range(\'2005-01-21\', periods=5, freq=\'48M\'),\n                      \'president_id\': [43, 43, 44, 44, 45]})\nwar_declarations = pd.DataFrame({"date": [datetime(2001, 9, 14), datetime(2003, 3, 3)],\n                                 "name": ["War in Afghanistan", "Iraq War"]})\n#Make the db in memory\nconn = sqlite3.connect(\':memory:\')\n#write the tables\nterms.to_sql(\'terms\', conn, index=False)\npresidents.to_sql(\'presidents\', conn, index=False)\nwar_declarations.to_sql(\'wars\', conn, index=False)\n\nqry = \'\'\'\n    select  \n        start_date PresTermStart,\n        end_date PresTermEnd,\n        wars.date WarStart,\n        presidents.name Pres\n    from\n        terms join wars on\n        date between start_date and end_date join presidents on\n        terms.president_id = presidents.president_id\n    \'\'\'\ndf = pd.read_sql_query(qry, conn)\n\n         PresTermStart          PresTermEnd             WarStart  Pres\n0  2001-01-31 00:00:00  2005-01-31 00:00:00  2001-09-14 00:00:00  Bush\n1  2001-01-31 00:00:00  2005-01-31 00:00:00  2003-03-03 00:00:00  Bush\n'
"df1 = pd.DataFrame({'c': ['A', 'A', 'B', 'C', 'C'],\n                    'k': [1, 2, 2, 2, 2],\n                    'l': ['a', 'b', 'a', 'a', 'd']})\ndf2 = pd.DataFrame({'c': ['A', 'C'],\n                    'l': ['b', 'a']})\nkeys = list(df2.columns.values)\ni1 = df1.set_index(keys).index\ni2 = df2.set_index(keys).index\ndf1[~i1.isin(i2)]\n\ndf1 = pd.DataFrame({'c': ['A', 'A', 'B', 'C', 'C'],\n                    'k': [1, 2, 2, 2, 2],\n                    'l': ['a', 'b', 'a', 'a', 'd']})\ndf2 = pd.DataFrame({'c': ['A', 'C'],\n                    'l': ['b', 'a']})\n\n# create a column marking df2 values\ndf2['marker'] = 1\n\n# join the two, keeping all of df1's indices\njoined = pd.merge(df1, df2, on=['c', 'l'], how='left')\njoined\n\n# extract desired columns where marker is NaN\njoined[pd.isnull(joined['marker'])][df1.columns]\n"
'# for Python 2\ndf.index = df.index.map(unicode) \n\n# for Python 3 (the unicode type does not exist and is replaced by str)\ndf.index = df.index.map(str)\n'
"In [25]: df['C'] = df.groupby(['A','B']).cumcount()+1; df\nOut[25]: \n   A  B  C\n0  A  a  1\n1  A  a  2\n2  A  b  1\n3  B  a  1\n4  B  a  2\n5  B  a  3\n"
"df = df.reset_index()\ndf.columns[0] = 'New_ID'\ndf['New_ID'] = df.index + 880\n"
"import pandas as pd\nimport random\n\nA = [ random.randint(0,100) for i in range(10) ]\nB = [ random.randint(0,100) for i in range(10) ]\n\ndf = pd.DataFrame({ 'field_A': A, 'field_B': B })\ndf\n#    field_A  field_B\n# 0       90       72\n# 1       63       84\n# 2       11       74\n# 3       61       66\n# 4       78       80\n# 5       67       75\n# 6       89       47\n# 7       12       22\n# 8       43        5\n# 9       30       64\n\ndf.field_A.mean()   # Same as df['field_A'].mean()\n# 54.399999999999999\n\ndf.field_A.median() \n# 62.0\n\n# You can call `quantile(i)` to get the i'th quantile,\n# where `i` should be a fractional number.\n\ndf.field_A.quantile(0.1) # 10th percentile\n# 11.9\n\ndf.field_A.quantile(0.5) # same as median\n# 62.0\n\ndf.field_A.quantile(0.9) # 90th percentile\n# 89.10000000000001\n"
"dfb = df[df['A']==5].index.values.astype(int)[0]\ndfbb = df[df['A']==8].index.values.astype(int)[0]\n\ndfb = int(df[df['A']==5].index[0])\ndfbb = int(df[df['A']==8].index[0])\n\ndfb = next(iter(df[df['A']==5].index), 'no match')\nprint (dfb)\n4\n\ndfb = next(iter(df[df['A']==50].index), 'no match')\nprint (dfb)\nno match\n\nprint (df.loc[dfb:dfbb-1,'B'])\n4    0.894525\n5    0.978174\n6    0.859449\nName: B, dtype: float64\n\nprint (df[(df['A'] &gt;= 5) &amp; (df['A'] &lt; 8)])\n   A         B\n4  5  0.894525\n5  6  0.978174\n6  7  0.859449\n\nprint (df.loc[(df['A'] &gt;= 5) &amp; (df['A'] &lt; 8), 'B'])\n4    0.894525\n5    0.978174\n6    0.859449\nName: B, dtype: float64\n\nprint (df.query('A &gt;= 5 and A &lt; 8'))\n   A         B\n4  5  0.894525\n5  6  0.978174\n6  7  0.859449\n"
"df['Date'] = pd.to_datetime(df['Date']) - pd.to_timedelta(7, unit='d')\ndf = df.groupby(['Name', pd.Grouper(key='Date', freq='W-MON')])['Quantity']\n       .sum()\n       .reset_index()\n       .sort_values('Date')\nprint (df)\n     Name       Date  Quantity\n0   Apple 2017-07-10        90\n3  orange 2017-07-10        20\n1   Apple 2017-07-17        30\n2  Orange 2017-07-24        40\n"
"df['C'] = df['A'] + df['B']\n"
"In [16]: df.groupby('id')['x'].apply(pd.rolling_mean, 2, min_periods=1)\nOut[16]: \n0    0.0\n1    0.5\n2    1.5\n3    3.0\n4    3.5\n5    4.5\n\nIn [17]: df.groupby('id')['x'].cumsum()\nOut[17]: \n0     0\n1     1\n2     3\n3     3\n4     7\n5    12\n"
'df.reset_index(level=2, drop=True)\nOut[29]: \n     A\n1 1  8\n  3  9\n'
"In [11]: s = pd.Series(['a', 'ab', 'c', 11, np.nan])\n\nIn [12]: s\nOut[12]:\n0      a\n1     ab\n2      c\n3     11\n4    NaN\ndtype: object\n\nIn [13]: s.str.startswith('a', na=False)\nOut[13]:\n0     True\n1     True\n2    False\n3    False\n4    False\ndtype: bool\n\nIn [14]: s.loc[s.str.startswith('a', na=False)]\nOut[14]:\n0     a\n1    ab\ndtype: object\n"
"df['N'].hist(by=df['Letter'])\n"
"df['cum_sum'] = df['val1'].cumsum()\ndf['cum_perc'] = 100*df['cum_sum']/df['val1'].sum()\n"
'def balanced_subsample(x,y,subsample_size=1.0):\n\n    class_xs = []\n    min_elems = None\n\n    for yi in np.unique(y):\n        elems = x[(y == yi)]\n        class_xs.append((yi, elems))\n        if min_elems == None or elems.shape[0] &lt; min_elems:\n            min_elems = elems.shape[0]\n\n    use_elems = min_elems\n    if subsample_size &lt; 1:\n        use_elems = int(min_elems*subsample_size)\n\n    xs = []\n    ys = []\n\n    for ci,this_xs in class_xs:\n        if len(this_xs) &gt; use_elems:\n            np.random.shuffle(this_xs)\n\n        x_ = this_xs[:use_elems]\n        y_ = np.empty(use_elems)\n        y_.fill(ci)\n\n        xs.append(x_)\n        ys.append(y_)\n\n    xs = np.concatenate(xs)\n    ys = np.concatenate(ys)\n\n    return xs,ys\n'
"df_csv = df0_fa.to_csv('revenue/data/test.csv',mode = 'w', index=False)\n"
"for col in ['parks', 'playgrounds', 'sports', 'roading']:\n    public[col] = public[col].astype('category')\n"
'ax.xaxis.set_major_formatter(formatter)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.ticker as ticker\n\nstart = pd.to_datetime("5-1-2012")\nidx = pd.date_range(start, periods= 365)\ndf = pd.DataFrame({\'A\':np.random.random(365), \'B\':np.random.random(365)})\ndf.index = idx\ndf_ts = df.resample(\'W\', how= \'max\')\n\nax = df_ts.plot(kind=\'bar\', x=df_ts.index, stacked=True)\n\n# Make most of the ticklabels empty so the labels don\'t get too crowded\nticklabels = [\'\']*len(df_ts.index)\n# Every 4th ticklable shows the month and day\nticklabels[::4] = [item.strftime(\'%b %d\') for item in df_ts.index[::4]]\n# Every 12th ticklabel includes the year\nticklabels[::12] = [item.strftime(\'%b %d\\n%Y\') for item in df_ts.index[::12]]\nax.xaxis.set_major_formatter(ticker.FixedFormatter(ticklabels))\nplt.gcf().autofmt_xdate()\n\nplt.show()\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\n\ndates = pd.date_range(\'2012-1-1\', \'2017-1-1\', freq=\'M\')\ndf = pd.DataFrame({\'A\':np.random.random(len(dates)), \'Date\':dates})\nfig, ax = plt.subplots()\ndf.plot.bar(x=\'Date\', y=\'A\', ax=ax)\nticklabels = [\'\']*len(df)\nskip = len(df)//12\nticklabels[::skip] = df[\'Date\'].iloc[::skip].dt.strftime(\'%Y-%m-%d\')\nax.xaxis.set_major_formatter(mticker.FixedFormatter(ticklabels))\nfig.autofmt_xdate()\n\n# fixes the tracker\n# https://matplotlib.org/users/recipes.html\ndef fmt(x, pos=0, max_i=len(ticklabels)-1):\n    i = int(x) \n    i = 0 if i &lt; 0 else max_i if i &gt; max_i else i\n    return dates[i]\nax.fmt_xdata = fmt\nplt.show()\n'
"c_maxes = df.groupby(['A', 'B']).C.transform(max)\ndf = df.loc[df.C == c_maxes]\n\ndf.sort('C').drop_duplicates(subset=['A', 'B'], take_last=True)\n\ndf.sort_values('C').drop_duplicates(subset=['A', 'B'], keep='last')\n\ndf.sort_values('C', ascending=False).drop_duplicates(subset=['A', 'B'])\n\n%timeit -n 10 df.loc[df.groupby(['A', 'B']).C.max == df.C]\n10 loops, best of 3: 25.7 ms per loop\n\n%timeit -n 10 df.sort_values('C').drop_duplicates(subset=['A', 'B'], keep='last')\n10 loops, best of 3: 101 ms per loop\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\ndf = pd.DataFrame({'A':26, 'B':20}, index=['N'])\ndf.plot(kind='bar', ax=ax)\n#ax = df.plot(kind='bar') # &quot;same&quot; as above\nax.legend([&quot;AAA&quot;, &quot;BBB&quot;]);\n\nimport matplotlib.pyplot as plt\ndf.plot(kind='bar')\nplt.legend([&quot;AAA&quot;, &quot;BBB&quot;]);\n"
"In [11]: t = pd.Timestamp('2013-12-25 00:00:00')\n\nIn [12]: t.date()\nOut[12]: datetime.date(2013, 12, 25)\n\nIn [13]: t.date() == datetime.date(2013, 12, 25)\nOut[13]: True\n\nIn [21]: pd.Timestamp(datetime.date(2013, 12, 25))\nOut[21]: Timestamp('2013-12-25 00:00:00')\n\nIn [22]: ts = pd.DatetimeIndex([t])\n\nIn [23]: ts == pd.Timestamp(datetime.date(2013, 12, 25))\nOut[23]: array([ True], dtype=bool)\n"
"import sqlite3\nimport pandas as pd\n# Create your connection.\ncnx = sqlite3.connect('file.db')\n\ndf = pd.read_sql_query(&quot;SELECT * FROM table_name&quot;, cnx)\n"
"import numpy as np\nimport pandas as pd\n\ndates = np.asarray(pd.date_range('1/1/2000', periods=8))\ndf1 = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])\ndf2 = df1.copy()\ndf3 = df1.copy()\ndf = df1.append([df2, df3])\n\nprint df\n"
'df.rename(columns={ df.columns[1]: "your value" }, inplace = True)\n'
'%timeit -n 100 df.col1.str.len().max()\n100 loops, best of 3: 11.7 ms per loop\n\n%timeit -n 100 df.col1.map(lambda x: len(x)).max()\n100 loops, best of 3: 16.4 ms per loop\n\n%timeit -n 100 df.col1.map(len).max()\n100 loops, best of 3: 10.1 ms per loop\n'
'df = psql.read_sql((\'select "Timestamp","Value" from "MyTable" \'\n                     \'where "Timestamp" BETWEEN %(dstart)s AND %(dfinish)s\'),\n                   db,params={"dstart":datetime(2014,6,24,16,0),"dfinish":datetime(2014,6,24,17,0)},\n                   index_col=[\'Timestamp\'])\n'
"merged = pd.merge(DataFrameA,DataFrameB, on=['Code','Date'])\n\nimport pandas as pd\n# create some timestamps for date column\ni = pd.to_datetime(pd.date_range('20140601',periods=2))\n\n#create two dataframes to merge\ndf = pd.DataFrame({'code': ['ABC','EFG'], 'date':i,'col1': [10,100]})\ndf2 = pd.DataFrame({'code': ['ABC','EFG'], 'date':i,'col2': [10,200]})\n\n#merge on columns (default join is inner)\npd.merge(df, df2, on =['code','date'])\n\n    code    col1    date    col2\n0   ABC     10      2014-06-01  10\n1   EFG     100     2014-06-02  200\n"
"pd.Timestamp.min\nOut[54]: Timestamp('1677-09-22 00:12:43.145225')\n\nIn [55]: pd.Timestamp.max\nOut[55]: Timestamp('2262-04-11 23:47:16.854775807')\n"
"In [16]: df = pd.DataFrame(np.random.randint(0, 10, size=(10, 2)), columns=list('ab'))\n\nIn [17]: df.loc[::2, 'a'] = np.nan\n\nIn [18]: df\nOut[18]:\n     a  b\n0  NaN  0\n1  5.0  5\n2  NaN  8\n3  2.0  8\n4  NaN  3\n5  9.0  4\n6  NaN  7\n7  2.0  0\n8  NaN  6\n9  2.0  5\n\nIn [19]: df['c'] = df.a.combine_first(df.b)\n\nIn [20]: df\nOut[20]:\n     a  b    c\n0  NaN  0  0.0\n1  5.0  5  5.0\n2  NaN  8  8.0\n3  2.0  8  2.0\n4  NaN  3  3.0\n5  9.0  4  9.0\n6  NaN  7  7.0\n7  2.0  0  2.0\n8  NaN  6  6.0\n9  2.0  5  2.0\n"
"df.groupby(['A','C'])['B'].sum()\n\ndf.groupby(['A','C'], as_index=False)['B'].sum()\n"
"In [21]: pandas.isnull(pandas.NaT)\nOut[21]: True\n\nIn [29]: x = pandas.NaT\n\nIn [30]: y = numpy.datetime64('NaT')\n\nIn [31]: x != x\nOut[31]: True\n\nIn [32]: y != y\n/home/i850228/.local/lib/python3.6/site-packages/IPython/__main__.py:1: FutureWarning: In the future, NAT != NAT will be True rather than False.\n  # encoding: utf-8\nOut[32]: False\n\nIn [33]: numpy.isnat(pandas.NaT)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-33-39a66bbf6513&gt; in &lt;module&gt;()\n----&gt; 1 numpy.isnat(pandas.NaT)\n\nTypeError: ufunc 'isnat' is only defined for datetime and timedelta.\n\nIn [34]: pandas.isnull(pandas.NaT)\nOut[34]: True\n\nIn [35]: pandas.isnull(numpy.datetime64('NaT'))\nOut[35]: True\n"
's1.intersection(s2)\n\npd.Series(list(set(s1).intersection(set(s2))))\n\nSeries(list(set(s1) &amp; set(s2)))\n'
'g = df.groupby("A").apply(lambda x: pd.concat((x["B"], x["C"])))\nk = g.reset_index()\nk["i"] = k1.index\nk["rn"] = k1.groupby("A")["i"].rank()\nk.pivot_table(rows="A", cols="rn", values=0)\n\n# output\n# rn   1   2   3   4   5   6\n# A                         \n# 1   10  12  11  22  20   8\n# 2   10  11  10  13 NaN NaN\n# 3   14  10 NaN NaN NaN NaN\n\nA   \n1  0    10\n   1    12\n   2    11\n   0    22\n   1    20\n   2     8\n2  3    10\n   4    11\n   3    10\n   4    13\n3  5    14\n   5    10\n\n    A  level_1   0\n0   1        0  10\n1   1        1  12\n2   1        2  11\n3   1        0  22\n4   1        1  20\n5   1        2   8\n6   2        3  10\n7   2        4  11\n8   2        3  10\n9   2        4  13\n10  3        5  14\n11  3        5  10\n\n    A  level_1   0   i\n0   1        0  10   0\n1   1        1  12   1\n2   1        2  11   2\n3   1        0  22   3\n4   1        1  20   4\n5   1        2   8   5\n6   2        3  10   6\n7   2        4  11   7\n8   2        3  10   8\n9   2        4  13   9\n10  3        5  14  10\n11  3        5  10  11\n\n    A  level_1   0   i  rn\n0   1        0  10   0   1\n1   1        1  12   1   2\n2   1        2  11   2   3\n3   1        0  22   3   4\n4   1        1  20   4   5\n5   1        2   8   5   6\n6   2        3  10   6   1\n7   2        4  11   7   2\n8   2        3  10   8   3\n9   2        4  13   9   4\n10  3        5  14  10   1\n11  3        5  10  11   2\n\nrn   1   2   3   4   5   6\nA                         \n1   10  12  11  22  20   8\n2   10  11  10  13 NaN NaN\n3   14  10 NaN NaN NaN NaN\n'
'pd.crosstab(df.A, df.B).apply(lambda r: r/r.sum(), axis=1)\n'
"%pylab inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf2 = df.groupby(['Name', 'Abuse/NFF'])['Name'].count().unstack('Abuse/NFF').fillna(0)\ndf2[['abuse','nff']].plot(kind='bar', stacked=True)\n"
"df.ix[0, 'COL_NAME'] = x\n\ndf.iloc[0, df.columns.get_loc('COL_NAME')] = x\n\nimport pandas as pd\nimport numpy as np\n\n# your data\n# ========================\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randn(10, 2), columns=['col1', 'col2'], index=np.random.randint(1,100,10)).sort_index()\n\nprint(df)\n\n\n      col1    col2\n10  1.7641  0.4002\n24  0.1440  1.4543\n29  0.3131 -0.8541\n32  0.9501 -0.1514\n33  1.8676 -0.9773\n36  0.7610  0.1217\n56  1.4941 -0.2052\n58  0.9787  2.2409\n75 -0.1032  0.4106\n76  0.4439  0.3337\n\n# .iloc with get_loc\n# ===================================\ndf.iloc[0, df.columns.get_loc('col2')] = 100\n\ndf\n\n      col1      col2\n10  1.7641  100.0000\n24  0.1440    1.4543\n29  0.3131   -0.8541\n32  0.9501   -0.1514\n33  1.8676   -0.9773\n36  0.7610    0.1217\n56  1.4941   -0.2052\n58  0.9787    2.2409\n75 -0.1032    0.4106\n76  0.4439    0.3337\n"
"df = df[['a', 'y', 'b', 'x']]\n\n&gt;&gt;&gt; df[[c for c in df if c not in ['b', 'x']] \n       + ['b', 'x']]\n   a  y  b   x\n0  1 -1  2   3\n1  2 -2  4   6\n2  3 -3  6   9\n3  4 -4  8  12\n\ncols_at_end = ['b', 'x']\ndf = df[[c for c in df if c not in cols_at_end] \n        + [c for c in cols_at_end if c in df]]\n"
"df.groupby(['id', 'group', 'term']).size().unstack(fill_value=0)\n\ndf = pd.DataFrame(dict(id=np.random.choice(100, 1000000),\n                       group=np.random.choice(20, 1000000),\n                       term=np.random.choice(10, 1000000)))\n"
"np.random.seed(0)\ndf1 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\ndf2 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\ndf3 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\ndf4 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\n\nx = 5\npd.eval(&quot;df1.A + (df1.B * x)&quot;)  \n\npd.eval(&quot;df1.A + df2.A&quot;)   # Valid, returns a pd.Series object\npd.eval(&quot;abs(df1) ** .5&quot;)  # Valid, returns a pd.DataFrame object\n\npd.eval(&quot;df1 &gt; df2&quot;)        \npd.eval(&quot;df1 &gt; 5&quot;)    \npd.eval(&quot;df1 &lt; df2 and df3 &lt; df4&quot;)      \npd.eval(&quot;df1 in [1, 2, 3]&quot;)\npd.eval(&quot;1 &lt; 2 &lt; 3&quot;)\n\npd.eval('df1.A * (df1.index &gt; 1)')\n\npd.eval(&quot;(df1 &gt; df2) &amp; (df3 &lt; df4)&quot;)\n\npd.eval(&quot;df1 &gt; df2 &amp; df3 &lt; df4&quot;)\n# pd.eval(&quot;df1 &gt; df2 &amp; df3 &lt; df4&quot;, parser='pandas')\n\npd.eval(&quot;df1 &gt; df2 and df3 &lt; df4&quot;)\n\n(df1 &gt; df2) &amp; (df3 &lt; df4)\n\ndf1 &gt; df2 &amp; df3 &lt; df4\n\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n\npd.eval(&quot;(df1 &gt; df2) &amp; (df3 &lt; df4)&quot;, parser='python')\n\npd.eval(&quot;df1 == [1, 2, 3]&quot;)\n\npd.eval(&quot;df1 in [1, 2, 3]&quot;)\n\ndf = pd.DataFrame({'A': ['abc', 'def', 'abacus']})\npd.eval('df.A.str.contains(&quot;ab&quot;)', engine='python')\n\n0     True\n1    False\n2     True\nName: A, dtype: bool\n\npd.eval(&quot;df1 &gt; thresh&quot;)\n\nUndefinedVariableError: name 'thresh' is not defined\n\npd.eval(&quot;df1 &gt; thresh&quot;, local_dict={'thresh': 10})\n    \n\nmydict = {'thresh': 5}\n# Dictionary values with *string* keys cannot be accessed without \n# using the 'python' engine.\npd.eval('df1 &gt; mydict[&quot;thresh&quot;]', engine='python')\n\nx = 5\ndf2['D'] = df1['A'] + (df1['B'] * x)\n\npd.eval('D = df1.A + (df1.B * x)', target=df2)\n\n   A  B  C   D\n0  5  9  8   5\n1  4  3  0  52\n2  5  0  2  22\n3  8  1  3  48\n4  3  7  0  42\n\npd.eval('df1.A + df2.A')\n\n0    10\n1    11\n2     7\n3    16\n4    10\ndtype: int32\n\ndf = pd.DataFrame(columns=list('FBGH'), index=df1.index)\ndf\n     F    B    G    H\n0  NaN  NaN  NaN  NaN\n1  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  NaN\n3  NaN  NaN  NaN  NaN\n4  NaN  NaN  NaN  NaN\n\ndf = pd.eval('B = df1.A + df2.A', target=df)\n# Similar to \n# df = df.assign(B=pd.eval('df1.A + df2.A'))\n\ndf\n     F   B    G    H\n0  NaN  10  NaN  NaN\n1  NaN  11  NaN  NaN\n2  NaN   7  NaN  NaN\n3  NaN  16  NaN  NaN\n4  NaN  10  NaN  NaN\n\npd.eval('B = df1.A + df2.A', target=df, inplace=True)\n# Similar to \n# df['B'] = pd.eval('df1.A + df2.A')\n\ndf\n     F   B    G    H\n0  NaN  10  NaN  NaN\n1  NaN  11  NaN  NaN\n2  NaN   7  NaN  NaN\n3  NaN  16  NaN  NaN\n4  NaN  10  NaN  NaN\n\ndf = df.eval(&quot;B = @df1.A + @df2.A&quot;)\n# df.eval(&quot;B = @df1.A + @df2.A&quot;, inplace=True)\ndf\n\n     F   B    G    H\n0  NaN  10  NaN  NaN\n1  NaN  11  NaN  NaN\n2  NaN   7  NaN  NaN\n3  NaN  16  NaN  NaN\n4  NaN  10  NaN  NaN\n\npd.eval(&quot;[1, 2, 3]&quot;)\narray([1, 2, 3], dtype=object)\n\npd.eval(&quot;[[1, 2, 3], [4, 5], [10]]&quot;, engine='python')\n[[1, 2, 3], [4, 5], [10]]\n\npd.eval([&quot;[1, 2, 3]&quot;, &quot;[4, 5]&quot;, &quot;[10]&quot;], engine='python')\n[[1, 2, 3], [4, 5], [10]]\n\npd.eval([&quot;[1]&quot;] * 100, engine='python') # Works\npd.eval([&quot;[1]&quot;] * 101, engine='python') \n\nAttributeError: 'PandasExprVisitor' object has no attribute 'visit_Ellipsis'\n\ndef eval(self, expr, inplace=False, **kwargs):\n\n    from pandas.core.computation.eval import eval as _eval\n\n    inplace = validate_bool_kwarg(inplace, 'inplace')\n    resolvers = kwargs.pop('resolvers', None)\n    kwargs['level'] = kwargs.pop('level', 0) + 1\n    if resolvers is None:\n        index_resolvers = self._get_index_resolvers()\n        resolvers = dict(self.iteritems()), index_resolvers\n    if 'target' not in kwargs:\n        kwargs['target'] = self\n    kwargs['resolvers'] = kwargs.get('resolvers', ()) + tuple(resolvers)\n    return <b>_eval(expr, inplace=inplace, **kwargs)</b>\npd.eval(&quot;df1.A + df1.B&quot;)\n\ndf1.eval(&quot;A + B&quot;)\n\ndf1.eval(&quot;A + index&quot;)\n\nA = 5\ndf1.eval(&quot;A &gt; @A&quot;) \n\ndf1.eval(&quot;&quot;&quot;\nE = A + B\nF = @df2.A + @df2.B\nG = E &gt;= F\n&quot;&quot;&quot;)\n\n   A  B  C  D   E   F      G\n0  5  0  3  3   5  14  False\n1  7  9  3  5  16   7   True\n2  2  4  7  6   6   5   True\n3  8  8  1  6  16   9   True\n4  7  7  8  1  14  10   True\n\ndf1.A\n\n0    5\n1    7\n2    2\n3    8\n4    7\nName: A, dtype: int32\n\ndf1.B\n\n0    9\n1    3\n2    0\n3    1\n4    7\nName: B, dtype: int32\n\nm = df1.eval(&quot;A &gt;= B&quot;)\nm\n0     True\n1    False\n2    False\n3     True\n4     True\ndtype: bool\n\ndf1[m]\n# df1.loc[m]\n\n   A  B  C  D\n0  5  0  3  3\n3  8  8  1  6\n4  7  7  8  1\n\ndf1.query(&quot;A &gt;= B&quot;)\n\n   A  B  C  D\n0  5  0  3  3\n3  8  8  1  6\n4  7  7  8  1\n\ndf1_big = pd.concat([df1] * 100000, ignore_index=True)\n\n%timeit df1_big[df1_big.eval(&quot;A &gt;= B&quot;)]\n%timeit df1_big.query(&quot;A &gt;= B&quot;)\n\n14.7 ms ± 33.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n14.7 ms ± 24.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\ndf1.query(&quot;index&quot;)\n# Same as df1.loc[df1.index] # Pointless,... I know\n\n   A  B  C  D\n0  5  0  3  3\n1  7  9  3  5\n2  2  4  7  6\n3  8  8  1  6\n4  7  7  8  1\n"
"Definition: DataFrame.dropna(self, axis=0, how='any', thresh=None, subset=None)\nDocstring:\nReturn object with labels on given axis omitted where alternately any\nor all of the data are missing\n\nParameters\n----------\naxis : {0, 1}\nhow : {'any', 'all'}\n    any : if any NA values are present, drop that label\n    all : if all values are NA, drop that label\nthresh : int, default None\n    int value : require that many non-NA values\nsubset : array-like\n    Labels along other axis to consider, e.g. if you are dropping rows\n    these would be a list of columns to include\n\nReturns\n-------\ndropped : DataFrame\n\ndf=df.dropna(axis=1,how='all')\n"
"In [31]: data\nOut[31]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2557 entries, 2004-01-01 00:00:00 to 2010-12-31 00:00:00\nFreq: &lt;1 DateOffset&gt;\nColumns: 360 entries, -89.75 to 89.75\ndtypes: object(360)\n"
"In [0]: grp = df.groupby('A')\n\nIn [1]: grp[['B']].transform(sum).sort('B')\nOut[1]:\n          B\n2 -2.829710\n5 -2.829710\n1  0.253651\n4  0.253651\n0  0.551377\n3  0.551377\n\nIn [2]: sort1 = df.ix[grp[['B']].transform(sum).sort('B').index]\n\nIn [3]: sort1\nOut[3]:\n     A         B      C\n2  baz -0.528172  False\n5  baz -2.301539   True\n1  bar -0.611756   True\n4  bar  0.865408  False\n0  foo  1.624345  False\n3  foo -1.072969   True\n\nIn [4]: f = lambda x: x.sort('C', ascending=False)\n\nIn [5]: sort2 = sort1.groupby('A', sort=False).apply(f)\n\nIn [6]: sort2\nOut[6]:\n         A         B      C\nA\nbaz 5  baz -2.301539   True\n    2  baz -0.528172  False\nbar 1  bar -0.611756   True\n    4  bar  0.865408  False\nfoo 3  foo -1.072969   True\n    0  foo  1.624345  False\n\nIn [7]: sort2.reset_index(0, drop=True)\nOut[7]:\n     A         B      C\n5  baz -2.301539   True\n2  baz -0.528172  False\n1  bar -0.611756   True\n4  bar  0.865408  False\n3  foo -1.072969   True\n0  foo  1.624345  False\n"
"df.groupby(pd.Grouper(freq='2D', level=-1))\n\nlevel_values = df.index.get_level_values\nresult = (df.groupby([level_values(i) for i in [0,1]]\n                      +[pd.Grouper(freq='2D', level=-1)]).sum())\n\nimport numpy as np\nimport pandas as pd\nimport datetime as DT\n\ndef using_Grouper(df):\n    level_values = df.index.get_level_values\n    return (df.groupby([level_values(i) for i in [0,1]]\n                       +[pd.Grouper(freq='2D', level=-1)]).sum())\n\ndef using_reset_index(df):\n    df = df.reset_index(level=[0, 1])\n    return df.groupby(['State','City']).resample('2D').sum()\n\ndef using_stack(df):\n    # http://stackoverflow.com/a/15813787/190597\n    return (df.unstack(level=[0,1])\n              .resample('2D').sum()\n              .stack(level=[2,1])\n              .swaplevel(2,0))\n\ndef make_orig():\n    values_a = range(16)\n    values_b = range(10, 26)\n    states = ['Georgia']*8 + ['Alabama']*8\n    cities = ['Atlanta']*4 + ['Savanna']*4 + ['Mobile']*4 + ['Montgomery']*4\n    dates = pd.DatetimeIndex([DT.date(2012,1,1)+DT.timedelta(days = i) for i in range(4)]*4)\n    df = pd.DataFrame(\n        {'value_a': values_a, 'value_b': values_b},\n        index = [states, cities, dates])\n    df.index.names = ['State', 'City', 'Date']\n    return df\n\ndef make_df(N):\n    dates = pd.date_range('2000-1-1', periods=N)\n    states = np.arange(50)\n    cities = np.arange(10)\n    index = pd.MultiIndex.from_product([states, cities, dates], \n                                       names=['State', 'City', 'Date'])\n    df = pd.DataFrame(np.random.randint(10, size=(len(index),2)), index=index,\n                      columns=['value_a', 'value_b'])\n    return df\n\ndf = make_orig()\nprint(using_Grouper(df))\n\n                               value_a  value_b\nState   City       Date                        \nAlabama Mobile     2012-01-01       17       37\n                   2012-01-03       21       41\n        Montgomery 2012-01-01       25       45\n                   2012-01-03       29       49\nGeorgia Atlanta    2012-01-01        1       21\n                   2012-01-03        5       25\n        Savanna    2012-01-01        9       29\n                   2012-01-03       13       33\n\nIn [30]: df = make_df(10)\n\nIn [34]: len(df)\nOut[34]: 5000\n\nIn [32]: %timeit using_Grouper(df)\n100 loops, best of 3: 6.03 ms per loop\n\nIn [33]: %timeit using_stack(df)\n10 loops, best of 3: 22.3 ms per loop\n\nIn [31]: %timeit using_reset_index(df)\n1 loop, best of 3: 659 ms per loop\n"
'&gt;&gt;&gt; pi = np.pi; nan = np.nan\n&gt;&gt;&gt; df = pd.DataFrame({"value": [3,4,9,10,11,np.nan,12]})\n&gt;&gt;&gt; df.query("(value &lt; 10) and (value &gt; @pi)")\n   value\n1      4\n2      9\n\n&gt;&gt;&gt; df.query("(value &lt; 10) or (value == @nan)")\n   value\n0      3\n1      4\n2      9\n\n&gt;&gt;&gt; df.query("(value &lt; 10) or (value != value)")\n   value\n0      3\n1      4\n2      9\n5    NaN\n'
"In [239]:\n\ndf = pd.DataFrame({'a':np.random.randn(5)})\ndf\nOut[239]:\n          a\n0 -0.548275\n1 -0.411741\n2 -1.187369\n3  1.028967\n4 -2.755030\nIn [240]:\n\nt = [2,4]\ndf.loc[~df.index.isin(t)]\nOut[240]:\n          a\n0 -0.548275\n1 -0.411741\n3  1.028967\n"
"import pandas as pd\n\n# your df\n# =========================\nprint(df)\n\n   id  score1  score2  score3  score4  score5\n0   1  0.0000  0.1087  0.0000  0.0786       1\n1   2  0.0532  0.3083  0.2864  0.4464       1\n2   3  0.0000  0.0840  0.8090  0.2331       1\n\n# to_dict\n# =========================\ndf.to_dict(orient='records')\n\nOut[318]: \n[{'id': 1.0,\n  'score1': 0.0,\n  'score2': 0.10865899999999999,\n  'score3': 0.0,\n  'score4': 0.078597,\n  'score5': 1.0},\n {'id': 2.0,\n  'score1': 0.053238000000000001,\n  'score2': 0.308253,\n  'score3': 0.28635300000000002,\n  'score4': 0.44643299999999997,\n  'score5': 1.0},\n {'id': 3.0,\n  'score1': 0.0,\n  'score2': 0.083978999999999998,\n  'score3': 0.80898300000000001,\n  'score4': 0.23305200000000001,\n  'score5': 1.0}]\n"
"df['D'] = df['B']\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([['a.1','b.1','c.1'],['a.2','b.2','c.2'],['a.3','b.3','c.3']],columns=['A','B','C'])\n\nIn [3]: df\nOut[3]:\n     A    B    C\n0  a.1  b.1  c.1\n1  a.2  b.2  c.2\n2  a.3  b.3  c.3\n\nIn [4]: df['D'] = df['B']                  #&lt;---What you want.\n\nIn [5]: df\nOut[5]:\n     A    B    C    D\n0  a.1  b.1  c.1  b.1\n1  a.2  b.2  c.2  b.2\n2  a.3  b.3  c.3  b.3\n\nIn [6]: df.loc[0,'D'] = 'd.1'\n\nIn [7]: df\nOut[7]:\n     A    B    C    D\n0  a.1  b.1  c.1  d.1\n1  a.2  b.2  c.2  b.2\n2  a.3  b.3  c.3  b.3\n"
"data.loc[data['name'] == 'fred', 'A'] = 0\n"
"mobile = pd.DataFrame({'PattLen':[1,1,2,6,6,7,7,7,7,8]})\nprint (mobile)\n   PattLen\n0        1\n1        1\n2        2\n3        6\n4        6\n5        7\n6        7\n7        7\n8        7\n9        8\n\nprint (mobile.PattLen.value_counts())\n7    4\n6    2\n1    2\n8    1\n2    1\nName: PattLen, dtype: int64\n\n\nmt = mobile.PattLen.value_counts().sort_index()\nprint (mt)\n1    2\n2    1\n6    2\n7    4\n8    1\nName: PattLen, dtype: int64\n"
'output = pd.DataFrame()\noutput = output.append(dictionary, ignore_index=True)\nprint(output.head())\n'
'import matplotlib.pyplot as plt\n\nplt.show()\n'
"&lt;your DataFrame&gt;.rename(columns={'count':'Total_Numbers'})\n\n&lt;your DataFrame&gt;.columns = ['ID', 'Region', 'Total_Numbers']\n"
"from io import BytesIO\n\nimport requests\nimport pandas as pd\n\nr = requests.get('https://docs.google.com/spreadsheet/ccc?key=0Ak1ecr7i0wotdGJmTURJRnZLYlV3M2daNTRubTdwTXc&amp;output=csv')\ndata = r.content\n    \nIn [10]: df = pd.read_csv(BytesIO(data), index_col=0,parse_dates=['Quradate'])\n\nIn [11]: df.head()\nOut[11]: \n          City                                            region     Res_Comm  \\\n0       Dothan  South_Central-Montgomery-Auburn-Wiregrass-Dothan  Residential   \n10       Foley                              South_Mobile-Baldwin  Residential   \n12  Birmingham      North_Central-Birmingham-Tuscaloosa-Anniston   Commercial   \n38       Brent      North_Central-Birmingham-Tuscaloosa-Anniston  Residential   \n44      Athens                 North_Huntsville-Decatur-Florence  Residential   \n\n          mkt_type            Quradate  National_exp  Alabama_exp  Sales_exp  \\\n0            Rural 2010-01-15 00:00:00             2            2          3   \n10  Suburban_Urban 2010-01-15 00:00:00             4            4          4   \n12  Suburban_Urban 2010-01-15 00:00:00             2            2          3   \n38           Rural 2010-01-15 00:00:00             3            3          3   \n44  Suburban_Urban 2010-01-15 00:00:00             4            5          4   \n\n    Inventory_exp  Price_exp  Credit_exp  \n0               2          3           3  \n10              4          4           3  \n12              2          2           3  \n38              3          3           2  \n44              4          4           4  \n"
"d = {'ABC' : df1, 'XYZ' : df2}\nprint pd.concat(d.values(), axis=1, keys=d.keys())\n\n\n                XYZ                                          ABC           \\\n               Open     High      Low    Close   Volume     Open     High   \nDate                                                                        \n2002-01-17  0.18077  0.18800  0.16993  0.18439  1720833  0.18077  0.18800   \n2002-01-18  0.18439  0.21331  0.18077  0.19523  2027866  0.18439  0.21331   \n2002-01-21  0.19523  0.20970  0.19162  0.20608   771149  0.19523  0.20970   \n\n\n                Low    Close   Volume  \nDate                                   \n2002-01-17  0.16993  0.18439  1720833  \n2002-01-18  0.18077  0.19523  2027866  \n2002-01-21  0.19162  0.20608   771149\n\nprint(pd.concat([df1, df2], axis=1, keys=['ABC', 'XYZ']))\n"
"# Creating Excel Writer Object from Pandas  \nwriter = pd.ExcelWriter('test.xlsx',engine='xlsxwriter')   \nworkbook=writer.book\nworksheet=workbook.add_worksheet('Validation')\nwriter.sheets['Validation'] = worksheet\ndf.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   \nanother_df.to_excel(writer,sheet_name='Validation',startrow=20, startcol=0) \n\nexcel_writer.write_cells(formatted_cells, sheet_name, startrow=startrow, startcol=startcol)\n\ndef write_cells(self, cells, sheet_name=None, startrow=0, startcol=0):\n    # Write the frame cells using xlsxwriter.\n    sheet_name = self._get_sheet_name(sheet_name)\n    if sheet_name in self.sheets:\n        wks = self.sheets[sheet_name]\n    else:\n        wks = self.book.add_worksheet(sheet_name)\n        self.sheets[sheet_name] = wks\n"
'df_c = pd.concat([df_a.reset_index(drop=True), df_b], axis=1)\n'
"df['avg'] = df.mean(axis=1)\n\n       Monday  Tuesday  Wednesday        avg\nMike       42      NaN         12  27.000000\nJenna     NaN      NaN         15  15.000000\nJon        21        4          1   8.666667\n\ndf['avg'] = df[['Monday', 'Tuesday']].mean(axis=1)\n\n       Monday  Tuesday  Wednesday   avg\nMike       42      NaN         12  42.0\nJenna     NaN      NaN         15   NaN\nJon        21        4          1  12.5\n"
"(df['A'] + df['B']).where((df['A'] &lt; 0) | (df['B'] &gt; 0), df['A'] / df['B'])\n\nnp.where(m, A, B)\n\nA.where(m, B)\n\npd.DataFrame.where(cond=(df['A'] &lt; 0) | (df['B'] &gt; 0), self=df['A'] + df['B'], other=df['A'] / df['B'])\n\npd.DataFrame.where(df['A'] + df['B'], (df['A'] &lt; 0) | (df['B'] &gt; 0), df['A'] / df['B'])\n"
'import pandas as pd\n\ndef strip(text):\n    try:\n        return text.strip()\n    except AttributeError:\n        return text\n\ndef make_int(text):\n    return int(text.strip(\'" \'))\n\ntable = pd.read_table("data.csv", sep=r\',\',\n                      names=["Year", "Make", "Model", "Description"],\n                      converters = {\'Description\' : strip,\n                                    \'Model\' : strip,\n                                    \'Make\' : strip,\n                                    \'Year\' : make_int})\nprint(table)\n\n   Year     Make   Model              Description\n0  1997     Ford    E350                     None\n1  1997     Ford    E350                     None\n2  1997     Ford    E350   Super, luxurious truck\n3  1997     Ford    E350  Super "luxurious" truck\n4  1997     Ford    E350    Super luxurious truck\n5  1997     Ford    E350                     None\n6  1997     Ford    E350                     None\n7  2000  Mercury  Cougar                     None\n'
'&gt;&gt;&gt; df.to_csv(\'foo.txt\',index=False,header=False)\n&gt;&gt;&gt; !cat foo.txt\n123,"this is ""out text"""\n&gt;&gt;&gt; import csv\n&gt;&gt;&gt; df.to_csv(\'foo.txt\',index=False,header=False, quoting=csv.QUOTE_NONE)\n&gt;&gt;&gt; !cat foo.txt\n123,this is "out text"\n'
"df['idx'] = df.groupby('Salesman').cumcount()\n\nprint df.pivot(index='Salesman',columns='idx')[['product','price']]\n\n        product              price        \nidx            0     1     2      0   1   2\nSalesman                                   \nKnut         bat  ball  wand      5   1   3\nSteve        pen   NaN   NaN      2 NaN NaN\n\ndf['prod_idx'] = 'product_' + df.idx.astype(str)\ndf['prc_idx'] = 'price_' + df.idx.astype(str)\n\nproduct = df.pivot(index='Salesman',columns='prod_idx',values='product')\nprc = df.pivot(index='Salesman',columns='prc_idx',values='price')\n\nreshape = pd.concat([product,prc],axis=1)\nreshape['Height'] = df.set_index('Salesman')['Height'].drop_duplicates()\nprint reshape\n\n         product_0 product_1 product_2  price_0  price_1  price_2  Height\nSalesman                                                                 \nKnut           bat      ball      wand        5        1        3       6\nSteve          pen       NaN       NaN        2      NaN      NaN       5\n\ndf['idx'] = df.groupby('Salesman').cumcount()\n\ntmp = []\nfor var in ['product','price']:\n    df['tmp_idx'] = var + '_' + df.idx.astype(str)\n    tmp.append(df.pivot(index='Salesman',columns='tmp_idx',values=var))\n\nreshape = pd.concat(tmp,axis=1)\n\n     +-------------------------------------------+\n     | salesman   idx   height   product   price |\n     |-------------------------------------------|\n  1. |     Knut     0        6       bat       5 |\n  2. |     Knut     1        6      ball       1 |\n  3. |     Knut     2        6      wand       3 |\n  4. |    Steve     0        5       pen       2 |\n     +-------------------------------------------+\n\nreshape wide product price, i(salesman) j(idx)\n"
"In [132]: df[:5]['duration'] / np.timedelta64(1, 's')\nOut[132]: \n0    1232\n1    1390\n2    1495\n3     797\n4    1132\nName: duration, dtype: float64\n\nIn [131]: df[:5]['duration'].values.view('&lt;i8')/10**9\nOut[131]: array([1232, 1390, 1495,  797, 1132], dtype=int64)\n"
'In [195]: df.isnull().sum(axis=1)\nOut[195]:\n0    0\n1    0\n2    0\n3    3\n4    0\n5    0\ndtype: int64\n\nIn [196]: df.isnull().sum(axis=1).tolist()\nOut[196]: [0, 0, 0, 3, 0, 0]\n\nIn [130]: df.shape[1] - df.count(axis=1)\nOut[130]:\n0    0\n1    0\n2    0\n3    3\n4    0\n5    0\ndtype: int64\n'
"from pandas.tseries.offsets import MonthEnd\n\ndf['Date'] = pd.to_datetime(df['Date'], format=&quot;%Y%m&quot;) + MonthEnd(1)\n\ndf = pd.DataFrame({'Date': [200104, 200508, 201002, 201602, 199912, 200611]})\ndf['EndOfMonth'] = pd.to_datetime(df['Date'], format=&quot;%Y%m&quot;) + MonthEnd(1)\n\n     Date EndOfMonth\n0  200104 2001-04-30\n1  200508 2005-08-31\n2  201002 2010-02-28\n3  201602 2016-02-29\n4  199912 1999-12-31\n5  200611 2006-11-30\n"
"df_2.index = pd.IntervalIndex.from_arrays(df_2['start'],df_2['end'],closed='both')\ndf_1['event'] = df_1['timestamp'].apply(lambda x : df_2.iloc[df_2.index.get_loc(x)]['event'])\n"
'Python 3.6.5 (default, Apr  1 2018, 05:46:30)\n[GCC 7.3.0] on linux\nType "help", "copyright", "credits" or "license" for more information.\n&gt;&gt;&gt; import pandas_datareader\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/usr/local/lib/python3.6/dist-packages/pandas_datareader/__init__.py", line 2, in &lt;module&gt;\n    from .data import (DataReader, Options, get_components_yahoo,\n  File "/usr/local/lib/python3.6/dist-packages/pandas_datareader/data.py", line 14, in &lt;module&gt;\n    from pandas_datareader.fred import FredReader\n  File "/usr/local/lib/python3.6/dist-packages/pandas_datareader/fred.py", line 1, in &lt;module&gt;\n    from pandas.core.common import is_list_like\nImportError: cannot import name \'is_list_like\'\n'
"In [26]: data\nOut[26]: \n           Date   Close  Adj Close\n251  2011-01-03  147.48     143.25\n250  2011-01-04  147.64     143.41\n249  2011-01-05  147.05     142.83\n248  2011-01-06  148.66     144.40\n247  2011-01-07  147.93     143.69\n\nIn [27]: data.set_index('Date').diff()\nOut[27]: \n            Close  Adj Close\nDate                        \n2011-01-03    NaN        NaN\n2011-01-04   0.16       0.16\n2011-01-05  -0.59      -0.58\n2011-01-06   1.61       1.57\n2011-01-07  -0.73      -0.71\n"
"df['col_name'] = df['col_name'].astype(object)\n\ndf = df.astype(object)\n\ndf['col_name'] = df['col_name'].astype('category')\n"
"import pandas as pd\ndesired_width = 320    \npd.set_option('display.width', desired_width)\n"
"In [2]:\n\ndf['weekday'] = df['Timestamp'].dt.dayofweek\ndf\nOut[2]:\n            Timestamp  Value  weekday\n0 2012-06-01 00:00:00    100        4\n1 2012-06-01 00:15:00    150        4\n2 2012-06-01 00:30:00    120        4\n3 2012-06-01 01:00:00    220        4\n4 2012-06-01 01:15:00     80        4\n\nIn [14]:\n\ndf = df.reset_index()\ndf['weekday'] = df['Timestamp'].dt.dayofweek\ndf\nOut[14]:\n            Timestamp  Value  weekday\n0 2012-06-01 00:00:00    100        4\n1 2012-06-01 00:15:00    150        4\n2 2012-06-01 00:30:00    120        4\n3 2012-06-01 01:00:00    220        4\n4 2012-06-01 01:15:00     80        4\n\nIn [16]:\n\ndf['weekday'] = pd.Series(df.index).dt.dayofweek\ndf\nOut[16]:\n                     Value  weekday\nTimestamp                          \n2012-06-01 00:00:00    100      NaN\n2012-06-01 00:15:00    150      NaN\n2012-06-01 00:30:00    120      NaN\n2012-06-01 01:00:00    220      NaN\n2012-06-01 01:15:00     80      NaN\nIn [17]:\n\ndf['weekday'] = df.reset_index()['Timestamp'].dt.dayofweek\ndf\nOut[17]:\n                     Value  weekday\nTimestamp                          \n2012-06-01 00:00:00    100      NaN\n2012-06-01 00:15:00    150      NaN\n2012-06-01 00:30:00    120      NaN\n2012-06-01 01:00:00    220      NaN\n2012-06-01 01:15:00     80      NaN\n"
"&gt;&gt;&gt; [df[1:]]\n[  viz  a1_count  a1_mean  a1_std\n1   n         0      NaN     NaN\n2   n         2       51      50]\n&gt;&gt;&gt; df.as_matrix(columns=[df[1:]])\narray([[ nan,  nan],\n       [ nan,  nan],\n       [ nan,  nan]])\n\n&gt;&gt;&gt; df.columns[1:]\nIndex(['a1_count', 'a1_mean', 'a1_std'], dtype='object')\n&gt;&gt;&gt; df.as_matrix(columns=df.columns[1:])\narray([[  3.      ,   2.      ,   0.816497],\n       [  0.      ,        nan,        nan],\n       [  2.      ,  51.      ,  50.      ]])\n"
"In [51]:\npd.to_datetime(df['I_DATE'])\n\nOut[51]:\n0   2012-03-28 14:15:00\n1   2012-03-28 14:17:28\n2   2012-03-28 14:50:50\nName: I_DATE, dtype: datetime64[ns]\n\nIn [54]:\ndf['I_DATE'].dt.date\n\nOut[54]:\n0    2012-03-28\n1    2012-03-28\n2    2012-03-28\ndtype: object\n\nIn [56]:    \ndf['I_DATE'].dt.time\n\nOut[56]:\n0    14:15:00\n1    14:17:28\n2    14:50:50\ndtype: object\n\nIn [59]:\ndf = pd.DataFrame({'date':pd.date_range(start = dt.datetime(2015,1,1), end = dt.datetime.now())})\ndf[(df['date'] &gt; '2015-02-04') &amp; (df['date'] &lt; '2015-02-10')]\n\nOut[59]:\n         date\n35 2015-02-05\n36 2015-02-06\n37 2015-02-07\n38 2015-02-08\n39 2015-02-09\n"
"In [153]:\ndf['ColumnA'] = df[df.columns[1:]].apply(\n    lambda x: ','.join(x.dropna().astype(str)),\n    axis=1\n)\ndf\n\nOut[153]:\n  Column1  Column2  Column3  Column4  Column5  ColumnA\n0       a        1        2        3        4  1,2,3,4\n1       a        3        4        5      NaN    3,4,5\n2       b        6        7        8      NaN    6,7,8\n3       c        7        7      NaN      NaN      7,7\n"
"import numpy as np\n\nmask = (z['b'] != 0)\nz_valid = z[mask]\n\nz['c'] = 0\nz.loc[mask, 'c'] = z_valid['a'] / np.log(z_valid['b'])\n"
"import pandas as pd\n\ncats = ['a', 'b', 'c']\ndf = pd.DataFrame({'cat': ['a', 'b', 'a']})\n\ndummies = pd.get_dummies(df, prefix='', prefix_sep='')\ndummies = dummies.T.reindex(cats).T.fillna(0)\n\nprint dummies\n\n    a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  1.0  0.0  0.0\n"
"df.columns = pd.MultiIndex.from_product([df.columns, ['C']])\n\nprint(df)\n#    A  B\n#    C  C\n# a  0  0\n# b  1  1\n# c  2  2\n# d  3  3\n# e  4  4\n"
'df.name.mode()\nOut[712]: \n0     alex\n1    helen\ndtype: object\n'
"df = pd.read_csv('/path/to/file.csv', dtype=str)\n# example df; yours will be from pd.read_csv() above\ndf = pd.DataFrame({'A': ['1', '3', '5'], 'B': ['2', '4', '6'], 'C': ['x', 'y', 'z']})\ntypes_dict = {'A': int, 'B': float}\nfor col, col_type in types_dict.items():\n    df[col] = df[col].astype(col_type)\n\ncol_names = pd.read_csv('file.csv', nrows=0).columns\ntypes_dict = {'A': int, 'B': float}\ntypes_dict.update({col: str for col in col_names if col not in types_dict})\npd.read_csv('file.csv', dtype=types_dict)\n"
"def percentile(n):\n    def percentile_(x):\n        return np.percentile(x, n)\n    percentile_.__name__ = 'percentile_%s' % n\n    return percentile_\n\nIn [11]: column.agg([np.sum, np.mean, np.std, np.median,\n                     np.var, np.min, np.max, percentile(50), percentile(95)])\nOut[11]:\n           sum       mean        std  median          var  amin  amax  percentile_50  percentile_95\nAGGREGATE\nA          106  35.333333  42.158431      12  1777.333333    10    84             12           76.8\nB           36  12.000000   8.888194       9    79.000000     5    22             12           76.8\n"
"csvdata_old = csvdata.copy()\n\nfrom pandas.util.testing import assert_frame_equal\nassert_frame_equal(csvdata, csvdata_old)\n\ntry:\n    assert_frame_equal(csvdata, csvdata_old)\n    return True\nexcept:  # appeantly AssertionError doesn't catch all\n    return False\n"
'In [31]: pd.read_json(\'{"a":1,"b":2}\\n{"a":3,"b":4}\', lines=True)\nOut[31]:\n   a  b\n0  1  2\n1  3  4\n\npd.read_json(json_file, lines=True)\n\nIn [11]: \'[%s]\' % \',\'.join(test.splitlines())\nOut[11]: \'[{"a":1,"b":2},{"a":3,"b":4}]\'\n\nIn [21]: %timeit pd.read_json(\'[%s]\' % \',\'.join(test.splitlines()))\n1000 loops, best of 3: 977 µs per loop\n\nIn [22]: %timeit l=[ json.loads(l) for l in test.splitlines()]; df = pd.DataFrame(l)\n1000 loops, best of 3: 282 µs per loop\n\nIn [23]: test_100 = \'\\n\'.join([test] * 100)\n\nIn [24]: %timeit pd.read_json(\'[%s]\' % \',\'.join(test_100.splitlines()))\n1000 loops, best of 3: 1.25 ms per loop\n\nIn [25]: %timeit l = [json.loads(l) for l in test_100.splitlines()]; df = pd.DataFrame(l)\n1000 loops, best of 3: 1.25 ms per loop\n\nIn [26]: test_1000 = \'\\n\'.join([test] * 1000)\n\nIn [27]: %timeit l = [json.loads(l) for l in test_1000.splitlines()]; df = pd.DataFrame(l)\n100 loops, best of 3: 9.78 ms per loop\n\nIn [28]: %timeit pd.read_json(\'[%s]\' % \',\'.join(test_1000.splitlines()))\n100 loops, best of 3: 3.36 ms per loop\n'
'value = re.sub(r"[^0-9]+", "", value)\n'
"In [2]: pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')\nOut[2]: \n0   0 days, 00:00:01\n1   1 days, 00:00:01\n2   2 days, 00:00:01\n3   3 days, 00:00:01\n4   4 days, 00:00:01\ndtype: timedelta64[ns]\n\nIn [3]: (pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')).astype('timedelta64[s]')\nOut[3]: \n0         1\n1     86401\n2    172801\n3    259201\n4    345601\ndtype: float64\n\nIn [4]: (pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')).astype('timedelta64[D]')\nOut[4]: \n0    0\n1    1\n2    2\n3    3\n4    4\ndtype: float64\n\nIn [5]: (pd.to_timedelta(np.arange(5),unit='d')+pd.to_timedelta(1,unit='s')) / np.timedelta64(1,'D')\nOut[5]: \n0    0.000012\n1    1.000012\n2    2.000012\n3    3.000012\n4    4.000012\ndtype: float64\n"
"pd.get_dummies(data=df, columns=['A', 'B'])\n\n&gt;&gt;&gt; df\n   A  B  C\n0  a  c  1\n1  b  c  2\n2  a  b  3\n\n&gt;&gt;&gt; pd.get_dummies(data=df, columns=['A', 'B'])\n   C  A_a  A_b  B_b  B_c\n0  1  1.0  0.0  0.0  1.0\n1  2  0.0  1.0  0.0  1.0\n2  3  1.0  0.0  1.0  0.0\n"
"In[1]:\n\ndf = pd.DataFrame({'$a':['a', 'b', 'c', 'd', 'a'], '$b': np.arange(5)})\ndf.describe(include = 'all')\n\nOut[1]:\n\n        $a    $b\ncount   5   5.000000\nunique  4   NaN\ntop     a   NaN\nfreq    2   NaN\nmean    NaN 2.000000\nstd     NaN 1.581139\nmin     NaN 0.000000\n25%     NaN 1.000000\n50%     NaN 2.000000\n75%     NaN 3.000000\nmax     NaN 4.000000\n\nIn[2]:\n\ndf.describe(include = [np.number])\n\nOut[3]:\n\n         $b\ncount   5.000000\nmean    2.000000\nstd     1.581139\nmin     0.000000\n25%     1.000000\n50%     2.000000\n75%     3.000000\nmax     4.000000\n\nIn[3]:\n\ndf.describe(include = ['O'])\n\nOut[3]:\n\n    $a\ncount   5\nunique  4\ntop     a\nfreq    2\n"
'In [10]: import matplotlib.pyplot as plt\n\nIn [11]: plt.hist?\n...\nPlot a histogram.\n\nCompute and draw the histogram of *x*. The return value is a\ntuple (*n*, *bins*, *patches*) or ([*n0*, *n1*, ...], *bins*,\n[*patches0*, *patches1*,...]) if the input contains multiple\ndata.\n...\ncumulative : boolean, optional, default : True\n    If `True`, then a histogram is computed where each bin gives the\n    counts in that bin plus all bins for smaller values. The last bin\n    gives the total number of datapoints.  If `normed` is also `True`\n    then the histogram is normalized such that the last bin equals 1.\n    If `cumulative` evaluates to less than 0 (e.g., -1), the direction\n    of accumulation is reversed.  In this case, if `normed` is also\n    `True`, then the histogram is normalized such that the first bin\n    equals 1.\n\n...\n\nIn [12]: import pandas as pd\n\nIn [13]: import numpy as np\n\nIn [14]: ser = pd.Series(np.random.normal(size=1000))\n\nIn [15]: ser.hist(cumulative=True, density=1, bins=100)\nOut[15]: &lt;matplotlib.axes.AxesSubplot at 0x11469a590&gt;\n\nIn [16]: plt.show()\n'
"In [31]: d = {'a':[1,2,3,4,5,6], 'b':[1,2,3,4,5,6]}\n\nIn [32]: df = pd.DataFrame(d)\n\nIn [33]: bad_df = df.index.isin([3,5])\n\nIn [34]: df[~bad_df]\nOut[34]: \n   a  b\n0  1  1\n1  2  2\n2  3  3\n4  5  5\n"
"import pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\ndf = pd.DataFrame(100 + np.random.randn(100).cumsum(), columns=['price'])\ndf['pct_change'] = df.price.pct_change()\ndf['log_ret'] = np.log(df.price) - np.log(df.price.shift(1))\n\nOut[56]: \n       price  pct_change  log_ret\n0   101.7641         NaN      NaN\n1   102.1642      0.0039   0.0039\n2   103.1429      0.0096   0.0095\n3   105.3838      0.0217   0.0215\n4   107.2514      0.0177   0.0176\n5   106.2741     -0.0091  -0.0092\n6   107.2242      0.0089   0.0089\n7   107.0729     -0.0014  -0.0014\n..       ...         ...      ...\n92  101.6160      0.0021   0.0021\n93  102.5926      0.0096   0.0096\n94  102.9490      0.0035   0.0035\n95  103.6555      0.0069   0.0068\n96  103.6660      0.0001   0.0001\n97  105.4519      0.0172   0.0171\n98  105.5788      0.0012   0.0012\n99  105.9808      0.0038   0.0038\n\n[100 rows x 3 columns]\n"
"# Define a lambda function to compute the weighted mean:\nwm = lambda x: np.average(x, weights=df.loc[x.index, &quot;adjusted_lots&quot;])\n\n# Define a dictionary with the functions to apply for a given column:\n# the following is deprecated since pandas 0.20:\n# f = {'adjusted_lots': ['sum'], 'price': {'weighted_mean' : wm} }\n# df.groupby([&quot;contract&quot;, &quot;month&quot;, &quot;year&quot;, &quot;buys&quot;]).agg(f)\n\n# Groupby and aggregate with namedAgg [1]:\ndf.groupby([&quot;contract&quot;, &quot;month&quot;, &quot;year&quot;, &quot;buys&quot;]).agg(adjusted_lots=(&quot;adjusted_lots&quot;, &quot;sum&quot;),  \n                                                      price_weighted_mean=(&quot;price&quot;, wm))\n\n                          adjusted_lots  price_weighted_mean\ncontract month year buys                                    \nC        Z     5    Sell            -19           424.828947\nCC       U     5    Buy               5          3328.000000\nSB       V     5    Buy              12            11.637500\nW        Z     5    Sell             -5           554.850000\n"
'In [53]: import pandas as pd\nIn [54]: df = pd.read_clipboard()\n\nIn [55]: df\nOut[55]: \n   bar  foo\n0    4    1\n1    5    2\n2    6    3\n\n                      0         1         2\nlevel1 level2                              \nfoo    a       0.518444  0.239354  0.364764\n       b       0.377863  0.912586  0.760612\nbar    a       0.086825  0.118280  0.592211\n\n              0         1         2\nfoo a  0.859630  0.399901  0.052504\n    b  0.231838  0.863228  0.017451\nbar a  0.422231  0.307960  0.801993\n'
"In [11]: d = {'Jill': {'Django Unchained': 6.5, 'Gone Girl': 9.0, 'Kill the Messenger': 8.0, 'Avenger: Age of Ultron': 7.0}, 'Toby': {'Django Unchained': 9.0, 'Zoolander': 2.0, 'Avenger: Age of Ultron': 8.5}}\n\nIn [12]: pd.DataFrame(d)\nOut[12]:\n                        Jill  Toby\nAvenger: Age of Ultron   7.0   8.5\nDjango Unchained         6.5   9.0\nGone Girl                9.0   NaN\nKill the Messenger       8.0   NaN\nZoolander                NaN   2.0\n\nIn [13]: pd.DataFrame.from_dict(d)\nOut[13]:\n                        Jill  Toby\nAvenger: Age of Ultron   7.0   8.5\nDjango Unchained         6.5   9.0\nGone Girl                9.0   NaN\nKill the Messenger       8.0   NaN\nZoolander                NaN   2.0\n\nIn [14]: pd.DataFrame.from_dict(d, orient='index')\nOut[14]:\n      Django Unchained  Gone Girl  Kill the Messenger  Avenger: Age of Ultron  Zoolander\nJill               6.5          9                   8                     7.0        NaN\nToby               9.0        NaN                 NaN                     8.5          2\n"
"In [42]:\ndf[['V','allele']] = df['V'].str.split('-',expand=True)\ndf\n\nOut[42]:\n      ID    Prob      V allele\n0   3009  1.0000  IGHV7   B*01\n1    129  1.0000  IGHV7   B*01\n2    119  0.8000  IGHV6   A*01\n3    120  0.8056   GHV6   A*01\n4    121  0.9000  IGHV6   A*01\n5    122  0.8050  IGHV6   A*01\n6    130  1.0000  IGHV4   L*03\n7   3014  1.0000  IGHV4   L*03\n8    266  0.9970  IGHV5   A*01\n9    849  0.4010  IGHV5   A*04\n10   174  1.0000  IGHV6   A*02\n11   844  1.0000  IGHV6   A*02\n"
'import pandas\ndfinal = df1.merge(df2, on="movie_title", how = \'inner\')\n\ndfinal = df1.merge(df2, how=\'inner\', left_on=\'movie_title\', right_on=\'movie_name\')\n'
"from copy import deepcopy\ndf = deepcopy(df_all.loc[df_all['issueid']==specific_id,:])\ndf['industry'] = 'yyy'\n\ndf = df_all.loc[df_all['issueid']==specific_id,:].copy()\ndf['industry'] = 'yyy'\n"
"df\n               0\nstock1 price   1\n       volume  2\nstock2 price   3\n       volume  4\nstock3 price   5\n       volume  6\n\ndf.xs('price', level=1, drop_level=False)\n              0\nstock1 price  1\nstock2 price  3\nstock3 price  5\n\ndf\n  stock1        stock2        stock3       \n   price volume  price volume  price volume\n0      1      2      3      4      5      6\n\ndf.xs('price', axis=1, level=1, drop_level=False)\n  stock1 stock2 stock3\n   price  price  price\n0      1      3      5\n"
"df.groupby(df.index.map(lambda t: t.minute))\n\ndf.groupby([df.index.map(lambda t: t.minute), 'Source'])\n\ndf.groupby([df['Source'],pd.TimeGrouper(freq='Min')])\n"
"df['date'] = df['datetime'].apply(lambda x: x.strftime('%d%m%Y'))\ndf['time'] = df['datetime'].apply(lambda x: x.strftime('%H%M%S'))\n\ndf[['date', 'time', ... ]].to_csv('df.csv')\n"
"In [66]:\ncols = list(df.columns)\ncols.remove('ID')\ndf[cols]\n\nOut[66]:\n   Age  BMI  Risk  Factor\n0    6   48  19.3       4\n1    8   43  20.9     NaN\n2    2   39  18.1       3\n3    9   41  19.5     NaN\nIn [68]:\n# now iterate over the remaining columns and create a new zscore column\nfor col in cols:\n    col_zscore = col + '_zscore'\n    df[col_zscore] = (df[col] - df[col].mean())/df[col].std(ddof=0)\ndf\nOut[68]:\n   ID  Age  BMI  Risk  Factor  Age_zscore  BMI_zscore  Risk_zscore  \\\n0  PT    6   48  19.3       4   -0.093250    1.569614    -0.150946   \n1  PT    8   43  20.9     NaN    0.652753    0.074744     1.459148   \n2  PT    2   39  18.1       3   -1.585258   -1.121153    -1.358517   \n3  PT    9   41  19.5     NaN    1.025755   -0.523205     0.050315   \n\n   Factor_zscore  \n0              1  \n1            NaN  \n2             -1  \n3            NaN  \n"
"&gt;&gt;&gt; reform = {(outerKey, innerKey): values for outerKey, innerDict in dictionary.iteritems() for innerKey, values in innerDict.iteritems()}\n&gt;&gt;&gt; reform\n{('A', 'a'): [1, 2, 3, 4, 5],\n ('A', 'b'): [6, 7, 8, 9, 1],\n ('B', 'a'): [2, 3, 4, 5, 6],\n ('B', 'b'): [7, 8, 9, 1, 2]}\n&gt;&gt;&gt; pandas.DataFrame(reform)\n   A     B   \n   a  b  a  b\n0  1  6  2  7\n1  2  7  3  8\n2  3  8  4  9\n3  4  9  5  1\n4  5  1  6  2\n\n[5 rows x 4 columns]\n"
"from __future__ import print_function\n\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n\n\nX_train = [[0, 0, np.nan], [np.nan, 1, 1]]\nY_train = [0, 1]\nX_test_1 = [0, 0, np.nan]\nX_test_2 = [0, np.nan, np.nan]\nX_test_3 = [np.nan, 1, 1]\n\n# Create our imputer to replace missing values with the mean e.g.\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp = imp.fit(X_train)\n\n# Impute our data, then train\nX_train_imp = imp.transform(X_train)\nclf = RandomForestClassifier(n_estimators=10)\nclf = clf.fit(X_train_imp, Y_train)\n\nfor X_test in [X_test_1, X_test_2, X_test_3]:\n    # Impute each test item, then predict\n    X_test_imp = imp.transform(X_test)\n    print(X_test, '-&gt;', clf.predict(X_test_imp))\n\n# Results\n[0, 0, nan] -&gt; [0]\n[0, nan, nan] -&gt; [0]\n[nan, 1, 1] -&gt; [1]\n"
'df[df["location"] == "c"].squeeze()\nOut[5]: \ndate        20130102\nlocation           c\nName: 2, dtype: object\n'
"pd.DatetimeIndex(montdist['date']) + pd.DateOffset(1)\n\nIn [11]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['A', 'B'])\n\nIn [12]: df['date'] = pd.to_datetime(['21-11-2013', '22-11-2013'])\n\nIn [13]: pd.DatetimeIndex(df.date) + pd.DateOffset(1)\nOut[13]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2013-11-22 00:00:00, 2013-11-23 00:00:00]\nLength: 2, Freq: None, Timezone: None\n\nIn [14]: pd.DatetimeIndex(df.date) + pd.offsets.Hour(1)\nOut[14]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2013-11-21 01:00:00, 2013-11-22 01:00:00]\nLength: 2, Freq: None, Timezone: Non\n"
"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nseries = pandas.Series(np.random.normal(size=2000))\nfig, ax = plt.subplots()\nseries.hist(ax=ax, bins=100, bottom=0.1)\nax.set_yscale('log')\n"
"foo = lambda x: pd.Series([i for i in reversed(x.split(','))])\nrev = df['City, State, Country'].apply(foo)\nprint rev\n\n      0    1        2\n0   HUN  NaN      NaN\n1   ESP  NaN      NaN\n2   GBR  NaN      NaN\n3   ESP  NaN      NaN\n4   FRA  NaN      NaN\n5   USA   ID      NaN\n6   USA   GA      NaN\n7   USA   NJ  Hoboken\n8   USA   NJ      NaN\n9   AUS  NaN      NaN\n\nrev.rename(columns={0:'Country',1:'State',2:'City'},inplace=True)\nrev = rev[['City','State','Country']]\nprint rev\n\n     City State Country\n0      NaN   NaN     HUN\n1      NaN   NaN     ESP\n2      NaN   NaN     GBR\n3      NaN   NaN     ESP\n4      NaN   NaN     FRA\n5      NaN    ID     USA\n6      NaN    GA     USA\n7  Hoboken    NJ     USA\n8      NaN    NJ     USA\n9      NaN   NaN     AUS\n"
'In [13]: pd.concat([x]*5)\nOut[13]: \n   a  b\n0  1  2\n0  1  2\n0  1  2\n0  1  2\n0  1  2\n\nIn [14]: pd.concat([x]*5, ignore_index=True)\nOut[14]: \n   a  b\n0  1  2\n1  1  2\n2  1  2\n3  1  2\n4  1  2\n'
"In [50]:\n\nprint df[np.in1d(df.index.get_level_values(1), ['Lake', 'River', 'Upland'])]\n                          Area\nNSRCODE PBL_AWI               \nCM      Lake      57124.819333\n        River      1603.906642\nLBH     Lake     258046.508310\n        River     44262.807900\n"
"&gt;&gt;&gt; df.pivot_table(values='val', index=df.index, columns='key', aggfunc='first')\nkey      bar     baz      foo\nid                           \n2    bananas  apples  oranges\n3      kiwis     NaN   grapes\n\n&gt;&gt;&gt; df.pivot(index=df.index, columns='key')['val']\nkey      bar     baz      foo\nid                           \n2    bananas  apples  oranges\n3      kiwis     NaN   grapes\n\n&gt;&gt;&gt; df.reset_index().groupby(['id', 'key'])['val'].aggregate('first').unstack()\nkey      bar     baz      foo\nid                           \n2    bananas  apples  oranges\n3      kiwis     NaN   grapes\n"
"In [21]: s = pd.Series({0: ['a', 'b', 'c'], 1:['c'], 2: ['b', 'c', 'e'], 3: ['a', 'c'], 4: ['b', 'e'] })\n\nIn [22]: s\nOut[22]:\n0    [a, b, c]\n1          [c]\n2    [b, c, e]\n3       [a, c]\n4       [b, e]\ndtype: object\n\nIn [23]: pd.get_dummies(s.apply(pd.Series).stack()).sum(level=0)\nOut[23]:\n   a  b  c  e\n0  1  1  1  0\n1  0  0  1  0\n2  0  1  1  1\n3  1  0  1  0\n4  0  1  0  1\n"
"dates.dt.strftime('%Y-%m-%d')\n"
'In [28]: d = \'2014-12-24 01:02:03\'\n\nIn [29]: c = re.sub(\'-\', \'/\', d)\n\nIn [30]: s_c = pd.Series([c]*10000)\n\nIn [31]: %timeit pd.to_datetime(s_c)\n1 loops, best of 3: 1.14 s per loop\n\nIn [32]: %timeit pd.to_datetime(s_c, infer_datetime_format=True)\n10 loops, best of 3: 105 ms per loop\n\nIn [33]: %timeit pd.to_datetime(s_c, format="%Y/%m/%d %H:%M:%S")\n10 loops, best of 3: 99.5 ms per loop\n'
"DF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value)\n    DF = DF.append(SR_row,ignore_index=True)\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([[1,2],[3,4]],columns=['A','B'])\n\nIn [3]: df\nOut[3]:\n   A  B\n0  1  2\n1  3  4\n\nIn [5]: s = pd.Series([5,6],index=['A','B'])\n\nIn [6]: s\nOut[6]:\nA    5\nB    6\ndtype: int64\n\nIn [36]: df.append(s,ignore_index=True)\nOut[36]:\n   A  B\n0  1  2\n1  3  4\n2  5  6\n\nDF = DF.append(SR_row,ignore_index=True)\n\nDF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value,name=sample)\n    DF = DF.append(SR_row)\nDF.head()\n"
'import pandas as pd\nfrom io import StringIO\n\ndata = """\nid,name\n1,A\n2,B\n3,C\ntt,D\n4,E\n5,F\nde,G\n"""\n\ndf = pd.read_csv(StringIO(data))\n\nIn [55]: df\nOut[55]: \n   id name\n0   1    A\n1   2    B\n2   3    C\n3  tt    D\n4   4    E\n5   5    F\n6  de    G\n\nIn [56]: df[df.id.apply(lambda x: x.isnumeric())]\nOut[56]: \n  id name\n0  1    A\n1  2    B\n2  3    C\n4  4    E\n5  5    F\n\nIn [61]: df[df.id.apply(lambda x: x.isnumeric())].set_index(\'id\')\nOut[61]: \n   name\nid     \n1     A\n2     B\n3     C\n4     E\n5     F\n\ndf_big = pd.concat([df]*10000)\n\nIn [3]: df_big = pd.concat([df]*10000)\n\nIn [4]: df_big.shape\nOut[4]: (70000, 2)\n\nIn [5]: %timeit df_big[df_big.id.apply(lambda x: x.isnumeric())]\n15.3 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn [6]: %timeit df_big[df_big.id.str.isnumeric()]\n20.3 ms ± 171 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn [7]: %timeit df_big[pd.to_numeric(df_big[\'id\'], errors=\'coerce\').notnull()]\n29.9 ms ± 682 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n'
'variables = arr[0].keys()\ndf = pd.DataFrame([[getattr(i,j) for j in variables] for i in arr], columns = variables)\n'
"from shapely.ops import cascaded_union\npolygons = [poly1[0], poly1[1], poly2[0], poly2[1]]\nboundary = gpd.GeoSeries(cascaded_union(polygons))\nboundary.plot(color = 'red')\nplt.show()\n\n&gt;&gt;&gt; shapely.geos.geos_version\n(3, 5, 1)\n"
'df2 = df.head(10)\n'
"df.to_sql(con=con, name='table_name_for_df', if_exists='replace', flavor='mysql')\n"
'In [11]: df\nOut[11]:\n   1  2  3  4  5  6\nA  x  x  x  x  x  x\nB  x  x  x  x  x  x\nC  x  x  x  x  x  x\n\nIn [12]: col_list = [3, 5]\n\nIn [13]: df = df[col_list]\n\nIn [14]: df\nOut[14]:\n   3  5\nA  x  x\nB  x  x\nC  x  x\n'
"In [11]: df = pd.DataFrame([[1, 'a', 2.]])\n\nIn [12]: df\nOut[12]: \n   0  1  2\n0  1  a  2\n\nIn [13]: df.dtypes\nOut[13]: \n0      int64\n1     object\n2    float64\ndtype: object\n\nIn [14]: df.dtypes == object\nOut[14]: \n0    False\n1     True\n2    False\ndtype: bool\n\nIn [15]: df.loc[:, df.dtypes == object]\nOut[15]: \n   1\n0  a\n\nIn [16]: df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].fillna('')\n"
"In [11]: df = pd.DataFrame([[1, 2], [1, 3], [5, 6]], columns=['A', 'B'])\n\nIn [12]: df\nOut[12]:\n   A  B\n0  1  2\n1  1  3\n2  5  6\n\nIn [13]: g = df.groupby('A')  #  GROUP BY A\n\nIn [14]: g.filter(lambda x: len(x) &gt; 1)  #  HAVING COUNT(*) &gt; 1\nOut[14]:\n   A  B\n0  1  2\n1  1  3\n\nIn [15]: g.filter(lambda x: x['B'].sum() == 5)\nOut[15]:\n   A  B\n0  1  2\n1  1  3\n"
'&gt;&gt;&gt; df\n\n   a          b  c\n0  1        NaT  w\n1  2 2014-02-01  g\n2  3        NaT  x\n\n&gt;&gt;&gt; df.dtypes\n\na             int64\nb    datetime64[ns]\nc            object\n\ndf[df.b.isnull()]\n\n   a   b  c\n0  1 NaT  w\n2  3 NaT  x\n'
'# DF TO EXCEL\nfrom pandas import ExcelWriter\n\nwriter = ExcelWriter(\'PythonExport.xlsx\')\nyourdf.to_excel(writer,\'Sheet5\')\nwriter.save()\n\n# DF TO CSV\nyourdf.to_csv(\'PythonExport.csv\', sep=\',\')\n\nSub DataFrameImport()\n  \'RUN PYTHON TO EXPORT DATA FRAME\n  Shell "C:\\pathTo\\python.exe fullpathOfPythonScript.py", vbNormalFocus\n\n  \'CLEAR EXISTING CONTENT\n  ThisWorkbook.Worksheets(5).Cells.Clear\n\n  \'COPY AND PASTE TO WORKBOOK\n  Workbooks("PythonExport").Worksheets(1).Cells.Copy\n  ThisWorkbook.Worksheets(5).Range("A1").Select\n  ThisWorkbook.Worksheets(5).Paste\nEnd Sub\n\nimport os\nimport win32com.client\nfrom pandas import ExcelWriter\n\nif os.path.exists("C:\\Full Location\\To\\excelsheet.xlsm"):\n  xlApp=win32com.client.Dispatch("Excel.Application")\n  wb = xlApp.Workbooks.Open(Filename="C:\\Full Location\\To\\excelsheet.xlsm")\n\n  # MACRO TO CLEAR SHEET 5 CONTENT\n  xlApp.Run("ClearExistingContent")\n  wb.Save() \n  xlApp.Quit()\n  del xl\n\n  # WRITE IN DATA FRAME TO SHEET 5\n  writer = ExcelWriter(\'C:\\Full Location\\To\\excelsheet.xlsm\')\n  yourdf.to_excel(writer,\'Sheet5\')\n  writer.save() \n'
"In [51]:\n\ndf['hour'] = pd.to_datetime(df['time'], format='%H:%M').dt.hour\ndf\nOut[51]:\n        time  hour\nindex             \n1      10:53    10\n2      12:17    12\n3      14:46    14\n4      16:36    16\n5      18:39    18\n6      20:31    20\n7      22:28    22\n\nIn [53]:\ndf['time'].iloc[0]\n\nOut[53]:\ndatetime.time(10, 53)\n"
"In [2]:\ndf = pd.DataFrame({'a': [1,2,3,4,5,6,7],\n                   'b': [1,1,1,0,0,0,0]})\n\u200b\ngrouped = df.groupby('b')\ngrouped.apply(lambda x: x.sample(frac=0.3))\n\nOut[2]:\n     a  b\nb        \n0 6  7  0\n1 2  3  1\n"
"x = v.fit_transform(df['Review'].values.astype('U'))  ## Even astype(str) would work\n"
'mask = select_k_best_classifier.get_support() #list of booleans\nnew_features = [] # The list of your K best features\n\nfor bool, feature in zip(mask, feature_names):\n    if bool:\n        new_features.append(feature)\n\ndataframe = pd.DataFrame(fit_transofrmed_features, columns=new_features)\n'
"df['Fruit Total']= df.iloc[:, -4:-1].sum(axis=1)\nprint (df)\n   Apples  Bananas  Grapes  Kiwis  Fruit Total\n0     2.0      3.0     NaN    1.0          5.0\n1     1.0      3.0     7.0    NaN         11.0\n2     NaN      NaN     2.0    3.0          2.0\n\ndf['Fruit Total']= df.sum(axis=1)\n"
'df.plot(x="X", y=["A", "B", "C"], kind="bar")\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ny = np.random.rand(10,4)\ny[:,0]= np.arange(10)\ndf = pd.DataFrame(y, columns=["X", "A", "B", "C"])\n\nax = df.plot(x="X", y="A", kind="bar")\ndf.plot(x="X", y="B", kind="bar", ax=ax, color="C2")\ndf.plot(x="X", y="C", kind="bar", ax=ax, color="C3")\n\nplt.show()\n'
"# pandas &lt;= 1.1.X\ndef cartesian_product_basic(left, right):\n    return (\n       left.assign(key=1).merge(right.assign(key=1), on='key').drop('key', 1))\n\ncartesian_product_basic(left, right)\n\n# pandas &gt;= 1.2 (est)\nleft.merge(right, how=&quot;cross&quot;)\n\n  col1_x  col2_x col1_y  col2_y\n0      A       1      X      20\n1      A       1      Y      30\n2      A       1      Z      50\n3      B       2      X      20\n4      B       2      Y      30\n5      B       2      Z      50\n6      C       3      X      20\n7      C       3      Y      30\n8      C       3      Z      50\n\ndef cartesian_product(*arrays):\n    la = len(arrays)\n    dtype = np.result_type(*arrays)\n    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n    for i, a in enumerate(np.ix_(*arrays)):\n        arr[...,i] = a\n    return arr.reshape(-1, la)  \n\ndef cartesian_product_generalized(left, right):\n    la, lb = len(left), len(right)\n    idx = cartesian_product(np.ogrid[:la], np.ogrid[:lb])\n    return pd.DataFrame(\n        np.column_stack([left.values[idx[:,0]], right.values[idx[:,1]]]))\n\ncartesian_product_generalized(left, right)\n\n   0  1  2   3\n0  A  1  X  20\n1  A  1  Y  30\n2  A  1  Z  50\n3  B  2  X  20\n4  B  2  Y  30\n5  B  2  Z  50\n6  C  3  X  20\n7  C  3  Y  30\n8  C  3  Z  50\n\nnp.array_equal(cartesian_product_generalized(left, right),\n               cartesian_product_basic(left, right))\nTrue\n\nleft2 = left.copy()\nleft2.index = ['s1', 's2', 's1']\n\nright2 = right.copy()\nright2.index = ['x', 'y', 'y']\n    \n\nleft2\n   col1  col2\ns1    A     1\ns2    B     2\ns1    C     3\n\nright2\n  col1  col2\nx    X    20\ny    Y    30\ny    Z    50\n\nnp.array_equal(cartesian_product_generalized(left, right),\n               cartesian_product_basic(left2, right2))\nTrue\n\ndef cartesian_product_multi(*dfs):\n    idx = cartesian_product(*[np.ogrid[:len(df)] for df in dfs])\n    return pd.DataFrame(\n        np.column_stack([df.values[idx[:,i]] for i,df in enumerate(dfs)]))\n\ncartesian_product_multi(*[left, right, left]).head()\n\n   0  1  2   3  4  5\n0  A  1  X  20  A  1\n1  A  1  X  20  B  2\n2  A  1  X  20  C  3\n3  A  1  X  20  D  4\n4  A  1  Y  30  A  1\n\ndef cartesian_product_simplified(left, right):\n    la, lb = len(left), len(right)\n    ia2, ib2 = np.broadcast_arrays(*np.ogrid[:la,:lb])\n\n    return pd.DataFrame(\n        np.column_stack([left.values[ia2.ravel()], right.values[ib2.ravel()]]))\n\nnp.array_equal(cartesian_product_simplified(left, right),\n               cartesian_product_basic(left2, right2))\nTrue\n\nfrom timeit import timeit\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nres = pd.DataFrame(\n       index=['cartesian_product_basic', 'cartesian_product_generalized', \n              'cartesian_product_multi', 'cartesian_product_simplified'],\n       columns=[1, 10, 50, 100, 200, 300, 400, 500, 600, 800, 1000, 2000],\n       dtype=float\n)\n\nfor f in res.index: \n    for c in res.columns:\n        # print(f,c)\n        left2 = pd.concat([left] * c, ignore_index=True)\n        right2 = pd.concat([right] * c, ignore_index=True)\n        stmt = '{}(left2, right2)'.format(f)\n        setp = 'from __main__ import left2, right2, {}'.format(f)\n        res.at[f, c] = timeit(stmt, setp, number=5)\n\nax = res.div(res.min()).T.plot(loglog=True) \nax.set_xlabel(&quot;N&quot;); \nax.set_ylabel(&quot;time (relative)&quot;);\n\nplt.show()\n"
"df1 = df.apply(pd.to_numeric, args=('coerce',))\n\ndf1 = df.apply(pd.to_numeric, errors='coerce')\n"
'&gt;&gt;&gt; df1\n   A  B\n0  1  2\n1  3  1\n&gt;&gt;&gt; df2\n   A  B\n0  5  6\n1  3  1\n&gt;&gt;&gt; pandas.concat([df1,df2]).drop_duplicates().reset_index(drop=True)\n   A  B\n0  1  2\n1  3  1\n2  5  6\n'
"&gt;&gt;&gt; g\n              uses  books\n               sum    sum\ntoken   year             \nxanthos 1830     3      3\n        1840     3      3\n        1868     2      2\n        1875     1      1\n\n[4 rows x 2 columns]\n&gt;&gt;&gt; g = g.reset_index()\n&gt;&gt;&gt; g\n     token  year  uses  books\n                   sum    sum\n0  xanthos  1830     3      3\n1  xanthos  1840     3      3\n2  xanthos  1868     2      2\n3  xanthos  1875     1      1\n\n[4 rows x 4 columns]\n\n&gt;&gt;&gt; g = dfalph[['token', 'year', 'uses', 'books']].groupby(['token', 'year'], as_index=False).sum()\n&gt;&gt;&gt; g\n     token  year  uses  books\n0  xanthos  1830     3      3\n1  xanthos  1840     3      3\n2  xanthos  1868     2      2\n3  xanthos  1875     1      1\n\n[4 rows x 4 columns]\n"
"In [11]: df['Name'].isin(['Alice', 'Bob'])\nOut[11]: \n0     True\n1     True\n2    False\n3     True\n4    False\nName: Name, dtype: bool\n\nIn [12]: df[df.Name.isin(['Alice', 'Bob'])]\nOut[12]: \n    Name  Amount\n0  Alice     100\n1    Bob      50\n3  Alice      30\n"
"gb = df.groupby('ZZ')    \n[gb.get_group(x) for x in gb.groups]\n"
"hours = sales.index.hour\n\nimport pandas as pd\npd.concat([sales, pd.DataFrame(hours, index=sales.index)], axis = 1)\n\nhours = sales['date'].hour\n\nsales['datehour'] = sales['date'].dt.hour\n\n"
"pandas.merge(df1, df2, how='left', left_on=['id_key'], right_on=['fk_key'])\n"
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; arr = np.array([1, 2, 3, 4])\n&gt;&gt;&gt; arr.dtype.name\n'int64'\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; cat = pd.Categorical(['a', 'b', 'c'])\n&gt;&gt;&gt; cat.dtype.name\n'category'\n\ndef is_categorical(array_like):\n    return array_like.dtype.name == 'category'\n"
'import pandas\nfrom io import StringIO\n\ncsv = StringIO("""index,A,B\n0,1,0.0\n1,1,3.0\n2,1,6.0\n3,2,0.0\n4,2,5.0\n5,2,7.0""")\n\ndf = pandas.read_csv(csv, index_col=\'index\')\ngroups = df.groupby(by=[\'A\'])\nprint(groups.apply(lambda g: g[g[\'B\'] == g[\'B\'].max()]))\n\n         A  B\nA index      \n1 2      1  6\n2 4      2  7\n'
"import os\n# if file does not exist write header \nif not os.path.isfile('filename.csv'):\n   df.to_csv('filename.csv', header='column_names')\nelse: # else it exists so append without writing the header\n   df.to_csv('filename.csv', mode='a', header=False)\n"
"In [40]:\nfrom collections import Counter\nd = Counter({'fb_view_listing': 76, 'fb_homescreen': 63, 'rt_view_listing': 50, 'rt_home_start_app': 46, 'fb_view_wishlist': 39, 'fb_view_product': 37, 'fb_search': 29, 'rt_view_product': 23, 'fb_view_cart': 22, 'rt_search': 12, 'rt_view_cart': 12, 'add_to_cart': 2, 'create_campaign': 1, 'fb_connect': 1, 'sale': 1, 'guest_sale': 1, 'remove_from_cart': 1, 'rt_transaction_confirmation': 1, 'login': 1})\ndf = pd.DataFrame.from_dict(d, orient='index').reset_index()\ndf\n\nOut[40]:\n                          index   0\n0                         login   1\n1   rt_transaction_confirmation   1\n2                  fb_view_cart  22\n3                    fb_connect   1\n4               rt_view_product  23\n5                     fb_search  29\n6                          sale   1\n7               fb_view_listing  76\n8                   add_to_cart   2\n9                  rt_view_cart  12\n10                fb_homescreen  63\n11              fb_view_product  37\n12            rt_home_start_app  46\n13             fb_view_wishlist  39\n14              create_campaign   1\n15                    rt_search  12\n16                   guest_sale   1\n17             remove_from_cart   1\n18              rt_view_listing  50\n\nIn [43]:\ndf = df.rename(columns={'index':'event', 0:'count'})\ndf\n\nOut[43]:\n                          event  count\n0                         login      1\n1   rt_transaction_confirmation      1\n2                  fb_view_cart     22\n3                    fb_connect      1\n4               rt_view_product     23\n5                     fb_search     29\n6                          sale      1\n7               fb_view_listing     76\n8                   add_to_cart      2\n9                  rt_view_cart     12\n10                fb_homescreen     63\n11              fb_view_product     37\n12            rt_home_start_app     46\n13             fb_view_wishlist     39\n14              create_campaign      1\n15                    rt_search     12\n16                   guest_sale      1\n17             remove_from_cart      1\n18              rt_view_listing     50\n"
"pd.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5062 entries, 0 to 5061\nData columns (total 51 columns):\nSomeCol                    5062 non-null object\nCol2                       5062 non-null object\n\ndf[['SomeCol', 'Col2']] = df[['SomeCol', 'Col2']].astype(str)\n"
"df_test['Difference'] = (df_test['First_Date'] - df_test['Second Date']).dt.days\n"
"as_list = df.index.tolist()\nidx = as_list.index('Republic of Korea')\nas_list[idx] = 'South Korea'\ndf.index = as_list\n"
"df.plot(style='.-')\n\ndf.plot(style=['+-','o-','.--','s:'])\n\ndf.plot(style='.-', markevery=5)\n"
'df.time_diff.quantile([0.25,0.5,0.75])\n\n\nOut[793]: \n0.25    0.483333\n0.50    0.500000\n0.75    0.516667\nName: time_diff, dtype: float64\n'
"df['gender'].value_counts(normalize=True) * 100\n"
"np.random.seed(123)\n\ndf = pd.DataFrame({'A' : ['foo', 'foo', 'bar', 'foo', 'bar', 'foo'],\n                   'B' : ['one', 'two', 'three','two', 'two', 'one'],\n                   'C' : np.random.randint(5, size=6),\n                   'D' : np.random.randint(5, size=6),\n                   'E' : np.random.randint(5, size=6)})\nprint (df)\n     A      B  C  D  E\n0  foo    one  2  3  0\n1  foo    two  4  1  0\n2  bar  three  2  1  1\n3  foo    two  1  0  3\n4  bar    two  3  1  4\n5  foo    one  2  1  0\n\ndf1 = df.groupby(['A', 'B'], as_index=False)['C'].sum()\nprint (df1)\n     A      B  C\n0  bar  three  2\n1  bar    two  3\n2  foo    one  4\n3  foo    two  5\n\ndf2 = df.groupby(['A', 'B'], as_index=False).sum()\nprint (df2)\n     A      B  C  D  E\n0  bar  three  2  1  1\n1  bar    two  3  1  4\n2  foo    one  4  4  0\n3  foo    two  5  1  3\n\ndf3 = df.groupby(['A', 'B'], as_index=False)['C','D'].sum()\nprint (df3)\n     A      B  C  D\n0  bar  three  2  1\n1  bar    two  3  1\n2  foo    one  4  4\n3  foo    two  5  1\n\ndf1 = df.groupby(['A', 'B'], as_index=False)['C'].agg('sum')\nprint (df1)\n     A      B  C\n0  bar  three  2\n1  bar    two  3\n2  foo    one  4\n3  foo    two  5\n\ndf2 = df.groupby(['A', 'B'], as_index=False).agg('sum')\nprint (df2)\n     A      B  C  D  E\n0  bar  three  2  1  1\n1  bar    two  3  1  4\n2  foo    one  4  4  0\n3  foo    two  5  1  3\n\ndf4 = (df.groupby(['A', 'B'])['C']\n         .agg([('average','mean'),('total','sum')])\n         .reset_index())\nprint (df4)\n     A      B  average  total\n0  bar  three      2.0      2\n1  bar    two      3.0      3\n2  foo    one      2.0      4\n3  foo    two      2.5      5\n\ndf5 = (df.groupby(['A', 'B'])\n         .agg([('average','mean'),('total','sum')]))\n\nprint (df5)\n                C             D             E      \n          average total average total average total\nA   B                                              \nbar three     2.0     2     1.0     1     1.0     1\n    two       3.0     3     1.0     1     4.0     4\nfoo one       2.0     4     2.0     4     0.0     0\n    two       2.5     5     0.5     1     1.5     3\n\n    \n\nprint (df5.columns)\nMultiIndex(levels=[['C', 'D', 'E'], ['average', 'total']],\n           labels=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]])\n           \n\ndf5.columns = df5.columns.map('_'.join)\ndf5 = df5.reset_index()\nprint (df5)\n     A      B  C_average  C_total  D_average  D_total  E_average  E_total\n0  bar  three        2.0        2        1.0        1        1.0        1\n1  bar    two        3.0        3        1.0        1        4.0        4\n2  foo    one        2.0        4        2.0        4        0.0        0\n3  foo    two        2.5        5        0.5        1        1.5        3\n\ndf5 = df.groupby(['A', 'B']).agg(['mean','sum'])\n    \ndf5.columns = (df5.columns.map('_'.join)\n                  .str.replace('sum','total')\n                  .str.replace('mean','average'))\ndf5 = df5.reset_index()\nprint (df5)\n     A      B  C_average  C_total  D_average  D_total  E_average  E_total\n0  bar  three        2.0        2        1.0        1        1.0        1\n1  bar    two        3.0        3        1.0        1        4.0        4\n2  foo    one        2.0        4        2.0        4        0.0        0\n3  foo    two        2.5        5        0.5        1        1.5        3\n\ndf6 = (df.groupby(['A', 'B'], as_index=False)\n         .agg({'C':'sum','D':'mean'})\n         .rename(columns={'C':'C_total', 'D':'D_average'}))\nprint (df6)\n     A      B  C_total  D_average\n0  bar  three        2        1.0\n1  bar    two        3        1.0\n2  foo    one        4        2.0\n3  foo    two        5        0.5\n\ndef func(x):\n    return x.iat[0] + x.iat[-1]\n\ndf7 = (df.groupby(['A', 'B'], as_index=False)\n         .agg({'C':'sum','D': func})\n         .rename(columns={'C':'C_total', 'D':'D_sum_first_and_last'}))\nprint (df7)\n     A      B  C_total  D_sum_first_and_last\n0  bar  three        2                     2\n1  bar    two        3                     2\n2  foo    one        4                     4\n3  foo    two        5                     1\n\ndf1 = df.groupby(['A', 'B'])['C'].sum()\nprint (df1)\nA    B    \nbar  three    2\n     two      3\nfoo  one      4\n     two      5\nName: C, dtype: int32\n\nprint (df1.index)\nMultiIndex(levels=[['bar', 'foo'], ['one', 'three', 'two']],\n           labels=[[0, 0, 1, 1], [1, 2, 0, 2]],\n           names=['A', 'B'])\n\nprint (type(df1))\n&lt;class 'pandas.core.series.Series'&gt;\n\ndf1 = df.groupby(['A', 'B'], as_index=False)['C'].sum()\nprint (df1)\n     A      B  C\n0  bar  three  2\n1  bar    two  3\n2  foo    one  4\n3  foo    two  5\n\ndf1 = df.groupby(['A', 'B'])['C'].sum().reset_index()\nprint (df1)\n     A      B  C\n0  bar  three  2\n1  bar    two  3\n2  foo    one  4\n3  foo    two  5\n\ndf2 = df.groupby('A')['C'].sum()\nprint (df2)\nA\nbar    5\nfoo    9\nName: C, dtype: int32\n\nprint (df2.index)\nIndex(['bar', 'foo'], dtype='object', name='A')\n\nprint (type(df2))\n&lt;class 'pandas.core.series.Series'&gt;\n\ndf2 = df.groupby('A', as_index=False)['C'].sum()\nprint (df2)\n     A  C\n0  bar  5\n1  foo  9\n\ndf2 = df.groupby('A')['C'].sum().reset_index()\nprint (df2)\n     A  C\n0  bar  5\n1  foo  9\n\ndf = pd.DataFrame({'A' : ['a', 'c', 'b', 'b', 'a', 'c', 'b'],\n                   'B' : ['one', 'two', 'three','two', 'two', 'one', 'three'],\n                   'C' : ['three', 'one', 'two', 'two', 'three','two', 'one'],\n                   'D' : [1,2,3,2,3,1,2]})\nprint (df)\n   A      B      C  D\n0  a    one  three  1\n1  c    two    one  2\n2  b  three    two  3\n3  b    two    two  2\n4  a    two  three  3\n5  c    one    two  1\n6  b  three    one  2\n\ndf1 = df.groupby('A')['B'].agg(list).reset_index()\nprint (df1)\n   A                    B\n0  a           [one, two]\n1  b  [three, two, three]\n2  c           [two, one]\n\ndf1 = df.groupby('A')['B'].apply(list).reset_index()\nprint (df1)\n   A                    B\n0  a           [one, two]\n1  b  [three, two, three]\n2  c           [two, one]\n\ndf2 = df.groupby('A')['B'].agg(','.join).reset_index()\nprint (df2)\n   A                B\n0  a          one,two\n1  b  three,two,three\n2  c          two,one\n\ndf3 = (df.groupby('A')['D']\n         .agg(lambda x: ','.join(x.astype(str)))\n         .reset_index())\nprint (df3)\n   A      D\n0  a    1,3\n1  b  3,2,2\n2  c    2,1\n\ndf3 = (df.assign(D = df['D'].astype(str))\n         .groupby('A')['D']\n         .agg(','.join).reset_index())\nprint (df3)\n   A      D\n0  a    1,3\n1  b  3,2,2\n2  c    2,1\n\ndf4 = df.groupby('A').agg(','.join).reset_index()\nprint (df4)\n   A                B            C\n0  a          one,two  three,three\n1  b  three,two,three  two,two,one\n2  c          two,one      one,two\n\ndf5 = (df.groupby('A')\n         .agg(lambda x: ','.join(x.astype(str)))\n         .reset_index())\nprint (df5)\n   A                B            C      D\n0  a          one,two  three,three    1,3\n1  b  three,two,three  two,two,one  3,2,2\n2  c          two,one      one,two    2,1\n\ndf = pd.DataFrame({'A' : ['a', 'c', 'b', 'b', 'a', 'c', 'b'],\n                   'B' : ['one', 'two', 'three','two', 'two', 'one', 'three'],\n                   'C' : ['three', np.nan, np.nan, 'two', 'three','two', 'one'],\n                   'D' : [np.nan,2,3,2,3,np.nan,2]})\nprint (df)\n   A      B      C    D\n0  a    one  three  NaN\n1  c    two    NaN  2.0\n2  b  three    NaN  3.0\n3  b    two    two  2.0\n4  a    two  three  3.0\n5  c    one    two  NaN\n6  b  three    one  2.0\n\ndf1 = df.groupby('A').size().reset_index(name='COUNT')\nprint (df1)\n   A  COUNT\n0  a      2\n1  b      3\n2  c      2\n\ndf2 = df.groupby('A')['C'].count().reset_index(name='COUNT')\nprint (df2)\n   A  COUNT\n0  a      2\n1  b      2\n2  c      1\n\ndf3 = df.groupby('A').count().add_suffix('_COUNT').reset_index()\nprint (df3)\n   A  B_COUNT  C_COUNT  D_COUNT\n0  a        2        2        1\n1  b        3        2        3\n2  c        2        1        1\n\ndf4 = (df['A'].value_counts()\n              .rename_axis('A')\n              .reset_index(name='COUNT'))\nprint (df4)\n   A  COUNT\n0  b      3\n1  a      2\n2  c      2\n\ndf5 = (df['A'].value_counts()\n              .sort_index()\n              .rename_axis('A')\n              .reset_index(name='COUNT'))\nprint (df5)\n   A  COUNT\n0  a      2\n1  b      3\n2  c      2\n\nnp.random.seed(123)\n\ndf = pd.DataFrame({'A' : ['foo', 'foo', 'bar', 'foo', 'bar', 'foo'],\n                    'B' : ['one', 'two', 'three','two', 'two', 'one'],\n                    'C' : np.random.randint(5, size=6),\n                    'D' : np.random.randint(5, size=6)})\nprint (df)\n     A      B  C  D\n0  foo    one  2  3\n1  foo    two  4  1\n2  bar  three  2  1\n3  foo    two  1  0\n4  bar    two  3  1\n5  foo    one  2  1\n\n\ndf['C1'] = df.groupby('A')['C'].transform('sum')\ndf['C2'] = df.groupby(['A','B'])['C'].transform('sum')\n\n\ndf[['C3','D3']] = df.groupby('A')['C','D'].transform('sum')\ndf[['C4','D4']] = df.groupby(['A','B'])['C','D'].transform('sum')\n\nprint (df)\n\n     A      B  C  D  C1  C2  C3  D3  C4  D4\n0  foo    one  2  3   9   4   9   5   4   4\n1  foo    two  4  1   9   5   9   5   5   1\n2  bar  three  2  1   5   2   5   2   2   1\n3  foo    two  1  0   9   5   9   5   5   1\n4  bar    two  3  1   5   3   5   2   3   1\n5  foo    one  2  1   9   4   9   5   4   4\n"
"df1 = pandas.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\n             'dat a1': range(7)})\n\ndf1['dat a1']\n"
"from pandas import  DataFrame\n\ndf1 = DataFrame({'col1':[1,2,3], 'col2':[2,3,4]})\ndf2 = DataFrame({'col1':[4,2,5], 'col2':[6,3,5]})\n\n\nprint(df2[~df2.isin(df1).all(1)])\nprint(df2[(df2!=df1)].dropna(how='all'))\nprint(df2[~(df2==df1)].dropna(how='all'))\n"
"df['Col3'] = (df['Col2'] &lt;= 1).astype(int)\n\ndf['Col3'] = df['Col2'].map(lambda x: 42 if x &gt; 1 else 55)\n\ndf['Col3'] = 0\ncondition = df['Col2'] &gt; 1\ndf.loc[condition, 'Col3'] = 42\ndf.loc[~condition, 'Col3'] = 55\n"
"monthly_mean.reset_index().plot(x='index', y='A')\n"
'&gt;&gt;&gt; df_asint = df.astype(int)\n&gt;&gt;&gt; coocc = df_asint.T.dot(df_asint)\n&gt;&gt;&gt; coocc\n       Dop  Snack  Trans\nDop      4      2      3\nSnack    2      3      2\nTrans    3      2      4\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.fill_diagonal(coocc.values, 0)\n&gt;&gt;&gt; coocc\n       Dop  Snack  Trans\nDop      0      2      3\nSnack    2      0      2\nTrans    3      2      0\n'
'df[["A", "B"]].multiply(df["C"], axis="index")\n'
'df.values.flatten()\n'
"df.replace({'\\n': '&lt;br&gt;'}, regex=True)\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': ['1\\n', '2\\n', '3'], 'b': ['4\\n', '5', '6\\n']})\n&gt;&gt;&gt; df\n   a    b\n0  1\\n  4\\n\n1  2\\n  5\n2  3    6\\n\n\n&gt;&gt;&gt; df.replace({'\\n': '&lt;br&gt;'}, regex=True)\n   a      b\n0  1&lt;br&gt;  4&lt;br&gt;\n1  2&lt;br&gt;  5\n2  3      6&lt;br&gt;\n"
"df['Datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\ndf = df.set_index('Datetime')\n\ndf = df.drop(['date','time'], axis=1)\n"
"In [4]: df.groupby('StationID', as_index=False)['BiasTemp'].mean()\nOut[4]:\n  StationID  BiasTemp\n0        BB       5.0\n1     KEOPS       2.5\n2    SS0279      15.0\n\nIn [5]: df.groupby('StationID')['BiasTemp'].mean()\nOut[5]:\nStationID\nBB            5.0\nKEOPS         2.5\nSS0279       15.0\nName: BiasTemp, dtype: float64\n"
"grouper = df.groupby([pd.Grouper(freq='1H'), 'Location'])\n\ngrouper['Event'].count()\n#                      Location\n# 2014-08-25 21:00:00  HK          1\n#                      LDN         1\n# 2014-08-25 22:00:00  LDN         2\n# Name: Event, dtype: int64\n\ngrouper['Event'].count().unstack()\n# Out[49]: \n# Location             HK  LDN\n# 2014-08-25 21:00:00   1    1\n# 2014-08-25 22:00:00 NaN    2\n\ngrouper = df.groupby([pd.Grouper(freq='1H'), 'Location'])\nresult = grouper['Event'].count().unstack('Location').fillna(0)\n\nLocation             HK  LDN\n2014-08-25 21:00:00   1    1\n2014-08-25 22:00:00   0    2\n"
"df['mean'] = df.mean(axis=1)\n&gt;&gt;&gt; df\n       Y1961      Y1962      Y1963      Y1964      Y1965 Region       mean\n0  82.567307  83.104757  83.183700  83.030338  82.831958     US  82.943612\n1   2.699372   2.610110   2.587919   2.696451   2.846247     US   2.688020\n2  14.131355  13.690028  13.599516  13.649176  13.649046     US  13.743824\n3   0.048589   0.046982   0.046583   0.046225   0.051750     US   0.048026\n4   0.553377   0.548123   0.582282   0.577811   0.620999     US   0.576518\n"
"df.apply(pd.to_numeric)\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': ['1', '2'], \n                       'b': ['45.8', '73.9'],\n                       'c': [10.5, 3.7]})\n\n&gt;&gt;&gt; df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2 entries, 0 to 1\nData columns (total 3 columns):\na    2 non-null object\nb    2 non-null object\nc    2 non-null float64\ndtypes: float64(1), object(2)\nmemory usage: 64.0+ bytes\n\n&gt;&gt;&gt; df.apply(pd.to_numeric).info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2 entries, 0 to 1\nData columns (total 3 columns):\na    2 non-null int64\nb    2 non-null float64\nc    2 non-null float64\ndtypes: float64(2), int64(1)\nmemory usage: 64.0 bytes\n\n  Signature: pd.to_numeric(arg, errors='raise')\n  Docstring:\n  Convert argument to a numeric type.\n\nParameters\n----------\narg : list, tuple or array of objects, or Series\nerrors : {'ignore', 'raise', 'coerce'}, default 'raise'\n    - If 'raise', then invalid parsing will raise an exception\n    - If 'coerce', then invalid parsing will be set as NaN\n    - If 'ignore', then invalid parsing will return the input\n\n&gt;&gt;&gt; df = pd.DataFrame({'ints': ['3', '5'], 'Words': ['Kobe', 'Bryant']})\n&gt;&gt;&gt; df.apply(pd.to_numeric, errors='ignore').info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2 entries, 0 to 1\nData columns (total 2 columns):\nWords    2 non-null object\nints     2 non-null int64\ndtypes: int64(1), object(1)\nmemory usage: 48.0+ bytes\n\n&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; df = pd.DataFrame({'ints': ['3', '5'], \n                       'Words': ['Kobe', 'Bryant']})\n&gt;&gt;&gt; df.apply(partial(pd.to_numeric, errors='ignore')).info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2 entries, 0 to 1\nData columns (total 2 columns):\nWords    2 non-null object\nints     2 non-null int64\ndtypes: int64(1), object(1)\nmemory usage: 48.0+ bytes\n"
"aaa  = pd.DataFrame([0,1,0,1,0,0], columns=['prediction'], index=[4,5,8,7,10,12])\nprint(aaa)\n    prediction\n4            0\n5            1\n8            0\n7            1\n10           0\n12           0\n\nbbb  = pd.DataFrame([0,0,1,0,1,1], columns=['groundTruth'])\nprint(bbb)\n   groundTruth\n0            0\n1            0\n2            1\n3            0\n4            1\n5            1\n\nprint (pd.concat([aaa, bbb], axis=1))\n    prediction  groundTruth\n0          NaN          0.0\n1          NaN          0.0\n2          NaN          1.0\n3          NaN          0.0\n4          0.0          1.0\n5          1.0          1.0\n7          1.0          NaN\n8          0.0          NaN\n10         0.0          NaN\n12         0.0          NaN\n\naaa.reset_index(drop=True, inplace=True)\nbbb.reset_index(drop=True, inplace=True)\n\nprint(aaa)\n   prediction\n0           0\n1           1\n2           0\n3           1\n4           0\n5           0\n\nprint(bbb)\n   groundTruth\n0            0\n1            0\n2            1\n3            0\n4            1\n5            1\n\nprint (pd.concat([aaa, bbb], axis=1))\n   prediction  groundTruth\n0           0            0\n1           1            0\n2           0            1\n3           1            0\n4           0            1\n5           0            1\n"
'df = pd.DataFrame({"User": ["user1", "user2", "user2", "user3", "user2", "user1", "user3"],\n                  "Amount": [10.0, 5.0, 8.0, 10.5, 7.5, 8.0, 9],\n                  \'Score\': [9, 1, 8, 7, 7, 6, 9]})\n\ndef my_agg(x):\n    names = {\n        \'Amount mean\': x[\'Amount\'].mean(),\n        \'Amount std\':  x[\'Amount\'].std(),\n        \'Amount range\': x[\'Amount\'].max() - x[\'Amount\'].min(),\n        \'Score Max\':  x[\'Score\'].max(),\n        \'Score Sum\': x[\'Score\'].sum(),\n        \'Amount Score Sum\': (x[\'Amount\'] * x[\'Score\']).sum()}\n\n    return pd.Series(names, index=[\'Amount range\', \'Amount std\', \'Amount mean\',\n                                   \'Score Sum\', \'Score Max\', \'Amount Score Sum\'])\n\ndf.groupby(\'User\').apply(my_agg)\n\ndf.groupby(\'User\')[\'Amount\'].agg([\'sum\', \'count\'])\n\n       sum  count\nUser              \nuser1  18.0      2\nuser2  20.5      3\nuser3  10.5      1\n\ndf = pd.DataFrame({"User": ["user1", "user2", "user2", "user3", "user2", "user1"],\n              "Amount": [10.0, 5.0, 8.0, 10.5, 7.5, 8.0],\n              \'Other\': [1,2,3,4,5,6]})\n\ndf.groupby(\'User\').agg({\'Amount\' : [\'sum\', \'count\'], \'Other\':[\'max\', \'std\']})\n\n      Amount       Other          \n         sum count   max       std\nUser                              \nuser1   18.0     2     6  3.535534\nuser2   20.5     3     5  1.527525\nuser3   10.5     1     4       NaN\n'
'In [42]: df = pd.DataFrame(np.random.random((5, 5)))\n\nIn [43]: df\nOut[43]: \n          0         1         2         3         4\n0  0.886864  0.518538  0.359964  0.167291  0.940414\n1  0.834130  0.022920  0.265131  0.059002  0.530584\n2  0.648019  0.953043  0.263551  0.595798  0.153969\n3  0.207003  0.015721  0.931170  0.045044  0.432870\n4  0.039886  0.898780  0.728195  0.112069  0.468485\n\nIn [44]: print df.to_latex()\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n{} &amp;         0 &amp;         1 &amp;         2 &amp;         3 &amp;         4 \\\\\n\\hline\n0 &amp;  0.886864 &amp;  0.518538 &amp;  0.359964 &amp;  0.167291 &amp;  0.940414 \\\\\n1 &amp;  0.834130 &amp;  0.022920 &amp;  0.265131 &amp;  0.059002 &amp;  0.530584 \\\\\n2 &amp;  0.648019 &amp;  0.953043 &amp;  0.263551 &amp;  0.595798 &amp;  0.153969 \\\\\n3 &amp;  0.207003 &amp;  0.015721 &amp;  0.931170 &amp;  0.045044 &amp;  0.432870 \\\\\n4 &amp;  0.039886 &amp;  0.898780 &amp;  0.728195 &amp;  0.112069 &amp;  0.468485 \\\\\n\\hline\n\\end{tabular}\n'
'import numpy \nimport pandas\nfrom  matplotlib import pyplot\nimport seaborn\nseaborn.set(style=\'ticks\')\n\nnumpy.random.seed(0)\nN = 37\n_genders= [\'Female\', \'Male\', \'Non-binary\', \'No Response\']\ndf = pandas.DataFrame({\n    \'Height (cm)\': numpy.random.uniform(low=130, high=200, size=N),\n    \'Weight (kg)\': numpy.random.uniform(low=30, high=100, size=N),\n    \'Gender\': numpy.random.choice(_genders, size=N)\n})\n\nfg = seaborn.FacetGrid(data=df, hue=\'Gender\', hue_order=_genders, aspect=1.61)\nfg.map(pyplot.scatter, \'Weight (kg)\', \'Height (cm)\').add_legend()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ndef dfScatter(df, xcol=\'Height\', ycol=\'Weight\', catcol=\'Gender\'):\n    fig, ax = plt.subplots()\n    categories = np.unique(df[catcol])\n    colors = np.linspace(0, 1, len(categories))\n    colordict = dict(zip(categories, colors))  \n\n    df["Color"] = df[catcol].apply(lambda x: colordict[x])\n    ax.scatter(df[xcol], df[ycol], c=df.Color)\n    return fig\n\nif 1:\n    df = pd.DataFrame({\'Height\':np.random.normal(size=10),\n                       \'Weight\':np.random.normal(size=10),\n                       \'Gender\': ["Male","Male","Unknown","Male","Male",\n                                  "Female","Did not respond","Unknown","Female","Female"]})    \n    fig = dfScatter(df)\n    fig.savefig(\'fig1.png\')\n'
'In [15]: df = pd.DataFrame([1, 2, 3], index=[dt.datetime(2013, 1, 1), dt.datetime(2013, 1, 3), dt.datetime(2013, 1, 5)])\n\nIn [16]: df\nOut[16]: \n            0\n2013-01-01  1\n2013-01-03  2\n2013-01-05  3\n\nIn [22]: start = df.index.searchsorted(dt.datetime(2013, 1, 2))\n\nIn [23]: end = df.index.searchsorted(dt.datetime(2013, 1, 4))\n\nIn [24]: df.iloc[start:end]\nOut[24]: \n            0\n2013-01-03  2\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; date_stngs = ('2008-12-20','2008-12-21','2008-12-22','2008-12-23')\n&gt;&gt;&gt; a = pd.Series([pd.to_datetime(date) for date in date_stngs])\n&gt;&gt;&gt; a\n0    2008-12-20 00:00:00\n1    2008-12-21 00:00:00\n2    2008-12-22 00:00:00\n3    2008-12-23 00:00:00\n\n&gt;&gt;&gt; pd.to_datetime(pd.Series(date_stngs))\n0   2008-12-20 00:00:00\n1   2008-12-21 00:00:00\n2   2008-12-22 00:00:00\n3   2008-12-23 00:00:00\n"
"In [5]: df = read_csv(StringIO(data),sep='\\s+')\n\nIn [6]: df\nOut[6]: \n           id     value\nid       1.00 -0.422000\nvalue   -0.42  1.000000\npercent -0.72  0.100000\nplayed   0.03 -0.043500\nmoney   -0.22  0.337000\nother     NaN       NaN\nsy      -0.03  0.000219\nsz      -0.33  0.383000\n\nIn [7]: df.dtypes\nOut[7]: \nid       float64\nvalue    float64\ndtype: object\n\nIn [8]: df.astype(object)\nOut[8]: \n           id     value\nid          1    -0.422\nvalue   -0.42         1\npercent -0.72       0.1\nplayed   0.03   -0.0435\nmoney   -0.22     0.337\nother     NaN       NaN\nsy      -0.03  0.000219\nsz      -0.33     0.383\n\nIn [9]: df.astype(object).convert_objects()\nOut[9]: \n           id     value\nid       1.00 -0.422000\nvalue   -0.42  1.000000\npercent -0.72  0.100000\nplayed   0.03 -0.043500\nmoney   -0.22  0.337000\nother     NaN       NaN\nsy      -0.03  0.000219\nsz      -0.33  0.383000\n\nIn [10]: df.astype(object).dtypes\nOut[10]: \nid       object\nvalue    object\ndtype: object\n"
'In [1]: df = DataFrame(np.random.randint(0,10,size=100).reshape(10,10))\n\nIn [2]: df\nOut[2]: \n   0  1  2  3  4  5  6  7  8  9\n0  2  2  3  2  6  1  9  9  3  3\n1  1  2  5  8  5  2  5  0  6  3\n2  0  7  0  7  5  5  9  1  0  3\n3  5  3  2  3  7  6  8  3  8  4\n4  8  0  2  2  3  9  7  1  2  7\n5  3  2  8  5  6  4  3  7  0  8\n6  4  2  6  5  3  3  4  5  3  2\n7  7  6  0  6  6  7  1  7  5  1\n8  7  4  3  1  0  6  9  7  7  3\n9  5  3  4  5  2  0  8  6  4  7\n\nIn [13]: Series(df.values.ravel()).unique()\nOut[13]: array([9, 1, 4, 6, 0, 7, 5, 8, 3, 2])\n\nIn [14]: df = DataFrame(np.random.randint(0,10,size=10000).reshape(100,100))\n\nIn [15]: %timeit Series(df.values.ravel()).unique()\n10000 loops, best of 3: 137 ﾵs per loop\n\nIn [16]: %timeit np.unique(df.values.ravel())\n1000 loops, best of 3: 270 ﾵs per loop\n'
"converters : dict. optional\n\nDict of functions for converting values in certain columns. Keys can either be integers or column labels\n\ndef CustomParser(data):\n    import json\n    j1 = json.loads(data)\n    return j1\n\ndf = pandas.read_csv(f1, converters={'stats':CustomParser},header=0)\n\ndf[sorted(df['stats'][0].keys())] = df['stats'].apply(pandas.Series)\n"
"&gt;&gt;&gt; df['StateInitial'] = df['state'].str[:2]\n&gt;&gt;&gt; df\n   pop       state  year StateInitial\n0  1.5    Auckland  2000           Au\n1  1.7       Otago  2001           Ot\n2  3.6  Wellington  2002           We\n3  2.4     Dunedin  2001           Du\n4  2.9    Hamilton  2002           Ha\n\n&gt;&gt;&gt; df['state'].apply(lambda x: x[len(x)/2-1:len(x)/2+1])\n0    kl\n1    ta\n2    in\n3    ne\n4    il\n"
"In [11]: a = b = 1\n\nIn [12]: a\nOut[12]: 1\n\nIn [13]: b\nOut[13]: 1\n\nsum = df['budget'] + df['actual'] \xa0# a Series\n# and\ndf['variance'] = df['budget'] + df['actual']  # assigned to a column\n\nIn [21]: df\nOut[21]:\n  cluster                 date  budget  actual\n0       a  2014-01-01 00:00:00   11000   10000\n1       a  2014-02-01 00:00:00    1200    1000\n2       a  2014-03-01 00:00:00     200     100\n3       b  2014-04-01 00:00:00     200     300\n4       b  2014-05-01 00:00:00     400     450\n5       c  2014-06-01 00:00:00     700    1000\n6       c  2014-07-01 00:00:00    1200    1000\n7       c  2014-08-01 00:00:00     200     100\n8       c  2014-09-01 00:00:00     200     300\n\nIn [22]: df['variance'] = df['budget'] + df['actual']\n\nIn [23]: df\nOut[23]:\n  cluster                 date  budget  actual  variance\n0       a  2014-01-01 00:00:00   11000   10000     21000\n1       a  2014-02-01 00:00:00    1200    1000      2200\n2       a  2014-03-01 00:00:00     200     100       300\n3       b  2014-04-01 00:00:00     200     300       500\n4       b  2014-05-01 00:00:00     400     450       850\n5       c  2014-06-01 00:00:00     700    1000      1700\n6       c  2014-07-01 00:00:00    1200    1000      2200\n7       c  2014-08-01 00:00:00     200     100       300\n8       c  2014-09-01 00:00:00     200     300       500\n"
"is_none = df.set_index(['Company', 'date'], inplace=True)\ndf  # the dataframe you want\nis_none # has the value None\n\ndf = df.set_index(['Company', 'date'], inplace=True)\n\ndf.set_index(['Company', 'date'], inplace=True)\n"
"grouped.columns = ['%s%s' % (a, '|%s' % b if b else '') for a, b in grouped.columns]\n"
"df.filter(regex='[A-CEG-I]')   # does NOT depend on the column order\n\ndf[ list(df.loc[:,'A':'C']) + ['E'] + list(df.loc[:,'G':'I']) ]\n\ndf[['A','B','C','E','G','H','I']]   # does NOT depend on the column order\n\n          A         B         C         E         G         H         I\n0 -0.814688 -1.060864 -0.008088  2.697203 -0.763874  1.793213 -0.019520\n1  0.549824  0.269340  0.405570 -0.406695 -0.536304 -1.231051  0.058018\n2  0.879230 -0.666814  1.305835  0.167621 -1.100355  0.391133  0.317467\n"
"import pandas as pd\nimport mysql.connector\nfrom sqlalchemy import create_engine\n\nengine = create_engine('mysql+mysqlconnector://[user]:[pass]@[host]:[port]/[schema]', echo=False)\ndata.to_sql(name='sample_table2', con=engine, if_exists = 'append', index=False)\n"
"In [286]:\ndf['Date'].dt.week\n\nOut[286]:\n0    25\ndtype: int64\n\nIn [287]:\ndf['Week_Number'] = df['Date'].dt.week\ndf\n\nOut[287]:\n        Date  Week_Number\n0 2015-06-17           25\n"
"&gt;&gt;&gt; df.groupby(['year', 'month', 'item'])['value'].sum().unstack('item')\nitem        item 1  item 2\nyear month                \n2004 1          33     250\n     2          44     224\n     3          41     268\n     4          29     232\n     5          57     252\n     6          61     255\n     7          28     254\n     8          15     229\n     9          29     258\n     10         49     207\n     11         36     254\n     12         23     209\n\n&gt;&gt;&gt; df.pivot_table(\n        values='value', \n        index=['year', 'month'], \n        columns='item', \n        aggfunc=np.sum)\nitem        item 1  item 2\nyear month                \n2004 1          33     250\n     2          44     224\n     3          41     268\n     4          29     232\n     5          57     252\n     6          61     255\n     7          28     254\n     8          15     229\n     9          29     258\n     10         49     207\n     11         36     254\n     12         23     209\n"
"s = pd.Series([1,2,3,4], index=['a', 'b', 'c', 'd'])\n\ns\n#a    1\n#b    2\n#c    3\n#d    4\n#dtype: int64\n\nfor i in s:\n    print(i)\n1\n2\n3\n4\n\nfor i, v in s.items():\n    print('index: ', i, 'value: ', v)\n#index:  a value:  1\n#index:  b value:  2\n#index:  c value:  3\n#index:  d value:  4\n\nfor i, v in s.iteritems():\n    print('index: ', i, 'value: ', v)\n#index:  a value:  1\n#index:  b value:  2\n#index:  c value:  3\n#index:  d value:  4\n\nfor i, row in df.groupby('a').size().iteritems():\n    print(i, row)\n\n# 12 4\n# 14 2\n"
'df = sqlContext.createDataFrame([("foo", 1), ("bar", 2), ("baz", 3)], (\'k\', \'v\'))\ndf.show(n=2)\n\n+---+---+\n|  k|  v|\n+---+---+\n|foo|  1|\n|bar|  2|\n+---+---+\nonly showing top 2 rows\n'
'mask = np.isnan(arr)\nidx = np.where(~mask,np.arange(mask.shape[1]),0)\nnp.maximum.accumulate(idx,axis=1, out=idx)\nout = arr[np.arange(idx.shape[0])[:,None], idx]\n\narr[mask] = arr[np.nonzero(mask)[0], idx[mask]]\n\nIn [179]: arr\nOut[179]: \narray([[  5.,  nan,  nan,   7.,   2.,   6.,   5.],\n       [  3.,  nan,   1.,   8.,  nan,   5.,  nan],\n       [  4.,   9.,   6.,  nan,  nan,  nan,   7.]])\n\nIn [180]: out\nOut[180]: \narray([[ 5.,  5.,  5.,  7.,  2.,  6.,  5.],\n       [ 3.,  3.,  1.,  8.,  8.,  5.,  5.],\n       [ 4.,  9.,  6.,  6.,  6.,  6.,  7.]])\n'
"print (df.groupby('param')['group'].nunique())\nparam\na    2\nb    1\nName: group, dtype: int64\n\na = df[df.param.notnull()].groupby('group')['param'].unique()\nprint (pd.DataFrame.from_records(a.values.tolist()).stack().value_counts())\na    2\nb    1\ndtype: int64\n"
"df.plot(x='Date',y='adj_close')\n\ndf.set_index('Date', inplace=True)\ndf['adj_close'].plot()\n\ndf.set_index('Date', inplace=True)\ndf.groupby('ticker')['adj_close'].plot(legend=True)\n\ngrouped = df.groupby('ticker')\n\nncols=2\nnrows = int(np.ceil(grouped.ngroups/ncols))\n\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(12,4), sharey=True)\n\nfor (key, ax) in zip(grouped.groups.keys(), axes.flatten()):\n    grouped.get_group(key).plot(ax=ax)\n\nax.legend()\nplt.show()\n"
"df['new_col'] = data['column'].rolling(5).mean()\n\n%timeit df['ma'] = data['pop'].rolling(5).mean()\n%timeit df['ma_2'] = data.rolling(5).mean()['pop']\n\n1000 loops, best of 3: 497 µs per loop\n100 loops, best of 3: 2.6 ms per loop\n"
"idx_lower = pd.Index([*'abcde'], name='lower')\nidx_range = pd.RangeIndex(5, name='range')\n\ns0 = pd.Series(range(10, 15), idx_lower)\ns1 = pd.Series(range(30, 40, 2), idx_lower)\ns2 = pd.Series(range(50, 10, -8), idx_range)\n\ndf0 = pd.DataFrame(100, index=idx_range, columns=idx_lower)\ndf1 = pd.DataFrame(\n    np.arange(np.product(df0.shape)).reshape(df0.shape),\n    index=idx_range, columns=idx_lower\n)\n\ns1 + s0\n\nlower\na    40\nb    43\nc    46\nd    49\ne    52\ndtype: int64\n\ns1 + s0.sample(frac=1)\n\nlower\na    40\nb    43\nc    46\nd    49\ne    52\ndtype: int64\n\ns1 + s0.sample(frac=1).values\n\nlower\na    42\nb    42\nc    47\nd    50\ne    49\ndtype: int64\n\ns1 + 1\n\nlower\na    31\nb    33\nc    35\nd    37\ne    39\ndtype: int64\n\ndf0 + df1\n\nlower    a    b    c    d    e\nrange                         \n0      100  101  102  103  104\n1      105  106  107  108  109\n2      110  111  112  113  114\n3      115  116  117  118  119\n4      120  121  122  123  124\n\ndf0 + df1.sample(frac=1).sample(frac=1, axis=1)\n\nlower    a    b    c    d    e\nrange                         \n0      100  101  102  103  104\n1      105  106  107  108  109\n2      110  111  112  113  114\n3      115  116  117  118  119\n4      120  121  122  123  124\n\ndf0 + df1.sample(frac=1).sample(frac=1, axis=1).values\n\nlower    a    b    c    d    e\nrange                         \n0      123  124  121  122  120\n1      118  119  116  117  115\n2      108  109  106  107  105\n3      103  104  101  102  100\n4      113  114  111  112  110\n\ndf0 + [*range(2, df0.shape[1] + 2)]\n\nlower    a    b    c    d    e\nrange                         \n0      102  103  104  105  106\n1      102  103  104  105  106\n2      102  103  104  105  106\n3      102  103  104  105  106\n4      102  103  104  105  106\n\ndf0 + 1\n\nlower    a    b    c    d    e\nrange                         \n0      101  101  101  101  101\n1      101  101  101  101  101\n2      101  101  101  101  101\n3      101  101  101  101  101\n4      101  101  101  101  101\n\ns0:\nlower    a    b    c    d    e\n        10   11   12   13   14\n\ndf0:\nlower    a    b    c    d    e\nrange                         \n0      100  100  100  100  100\n1      100  100  100  100  100\n2      100  100  100  100  100\n3      100  100  100  100  100\n4      100  100  100  100  100\n\ndf0 + s0\n\nlower    a    b    c    d    e\nrange                         \n0      110  111  112  113  114\n1      110  111  112  113  114\n2      110  111  112  113  114\n3      110  111  112  113  114\n4      110  111  112  113  114\n\ns2:               df0:\n\n             |    lower    a    b    c    d    e\nrange        |    range                         \n0      50    |    0      100  100  100  100  100\n1      42    |    1      100  100  100  100  100\n2      34    |    2      100  100  100  100  100\n3      26    |    3      100  100  100  100  100\n4      18    |    4      100  100  100  100  100\n\ndf0 + s2\n\n        a   b   c   d   e   0   1   2   3   4\nrange                                        \n0     NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n1     NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n2     NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n3     NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n4     NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN\n\n(df0.T + s2).T\n\nlower    a    b    c    d    e\nrange                         \n0      150  150  150  150  150\n1      142  142  142  142  142\n2      134  134  134  134  134\n3      126  126  126  126  126\n4      118  118  118  118  118\n\ndf0.add(s2, axis='index')\n\nlower    a    b    c    d    e\nrange                         \n0      150  150  150  150  150\n1      142  142  142  142  142\n2      134  134  134  134  134\n3      126  126  126  126  126\n4      118  118  118  118  118\n\ndf0.add(s2, axis=0)\n\nlower    a    b    c    d    e\nrange                         \n0      150  150  150  150  150\n1      142  142  142  142  142\n2      134  134  134  134  134\n3      126  126  126  126  126\n4      118  118  118  118  118\n\ndf0.sub(s2, axis=0)\n\nlower   a   b   c   d   e\nrange                    \n0      50  50  50  50  50\n1      58  58  58  58  58\n2      66  66  66  66  66\n3      74  74  74  74  74\n4      82  82  82  82  82\n\ndf0.mul(s2, axis=0)\n\nlower     a     b     c     d     e\nrange                              \n0      5000  5000  5000  5000  5000\n1      4200  4200  4200  4200  4200\n2      3400  3400  3400  3400  3400\n3      2600  2600  2600  2600  2600\n4      1800  1800  1800  1800  1800\n\ndf0.div(s2, axis=0)\n\nlower         a         b         c         d         e\nrange                                                  \n0      2.000000  2.000000  2.000000  2.000000  2.000000\n1      2.380952  2.380952  2.380952  2.380952  2.380952\n2      2.941176  2.941176  2.941176  2.941176  2.941176\n3      3.846154  3.846154  3.846154  3.846154  3.846154\n4      5.555556  5.555556  5.555556  5.555556  5.555556\n\ndf0.pow(1 / s2, axis=0)\n\nlower         a         b         c         d         e\nrange                                                  \n0      1.096478  1.096478  1.096478  1.096478  1.096478\n1      1.115884  1.115884  1.115884  1.115884  1.115884\n2      1.145048  1.145048  1.145048  1.145048  1.145048\n3      1.193777  1.193777  1.193777  1.193777  1.193777\n4      1.291550  1.291550  1.291550  1.291550  1.291550\n"
"import pandas as pd\n\ndf1 = pd.DataFrame([(1,2),(3,4),(5,6)], columns=['a','b'])\nOut: \n   a  b\n0  1  2\n1  3  4\n2  5  6\n\ndf2 = pd.DataFrame([(100,200),(300,400),(500,600)], columns=['a','b'])\nOut: \n     a    b\n0  100  200\n1  300  400\n2  500  600\n\ndf_add = df1.add(df2, fill_value=0)\nOut: \n     a    b\n0  101  202\n1  303  404\n2  505  606\n"
'In [11]: df\nOut[11]: \n       C\nA B     \n0 one  3\n1 one  2\n2 two  1\n\nIn [12]: df.index.levels[1]\nOut[12]: Index([one, two], dtype=object)\n'
'import pandas as pd\nimport pandas.io.sql as psql\nchunk_size = 10000\noffset = 0\ndfs = []\nwhile True:\n  sql = "SELECT * FROM MyTable limit %d offset %d order by ID" % (chunk_size,offset) \n  dfs.append(psql.read_frame(sql, cnxn))\n  offset += chunk_size\n  if len(dfs[-1]) &lt; chunk_size:\n    break\nfull_df = pd.concat(dfs)\n'
"size = 2        # sample size\nreplace = True  # with replacement\nfn = lambda obj: obj.loc[np.random.choice(obj.index, size, replace),:]\ndf.groupby('Group_Id', as_index=False).apply(fn)\n"
"&gt;&gt;&gt; ((df['A'] == 2) &amp; (df['B'] == 3)).any()\nTrue\n&gt;&gt;&gt; ((df['A'] == 1) &amp; (df['B'] == 2)).any()\nFalse\n"
"In [20]: df.sort_index(ascending=False).groupby('A').agg([np.mean, lambda x: x.iloc[1] ])\nOut[20]: \n           B             C         \n        mean &lt;lambda&gt; mean &lt;lambda&gt;\nA                                  \ngroup1  11.0       10  101      100\ngroup2  17.5       10  175      100\ngroup3  11.0       10  101      100\n"
'In [11]: df = pd.DataFrame([[1, 2], [1, 4], [5, 6]], columns=[\'A\', \'B\'])\n\nIn [12]: df\nOut[12]:\n   A  B\n0  1  2\n1  1  4\n2  5  6\n\nIn [13]: df.groupby("A").filter(lambda x: len(x) &gt; 1)\nOut[13]:\n   A  B\n0  1  2\n1  1  4\n'
"import numpy as np\nimport seaborn as sns\n\nx = np.random.randn(200)\nkwargs = {'cumulative': True}\nsns.distplot(x, hist_kws=kwargs, kde_kws=kwargs)\n"
"df1 = None\n\nif df1 is not None:\n    print df1.head()\n\ndf1 = pd.DataFrame()\n\nif not df1.empty:\n    print df1.head()\n\ntry:\n    print df1.head()\n# catch when df1 is None\nexcept AttributeError:\n    pass\n# catch when it hasn't even been defined\nexcept NameError:\n    pass\n\ndf1 = pd.DataFrame(np.arange(25).reshape(5, 5), list('ABCDE'), list('abcde'))\ndf1\n"
"import xlrd\nxls = xlrd.open_workbook(r'&lt;path_to_your_excel_file&gt;', on_demand=True)\nprint xls.sheet_names() # &lt;- remeber: xlrd sheet_names is a function, not a property\n"
'In [1]: import pandas as pd\n\nIn [2]: df = pd.read_csv("test.csv")\n\nIn [3]: df\nOut[3]: \n  id  value1  value2  value3\n0  A       1       2       3\n1  B       4       5       6\n2  C       7       8       9\n\nIn [4]: df["sum"] = df.sum(axis=1)\n\nIn [5]: df\nOut[5]: \n  id  value1  value2  value3  sum\n0  A       1       2       3    6\n1  B       4       5       6   15\n2  C       7       8       9   24\n\nIn [6]: df_new = df.loc[:,"value1":"value3"].div(df["sum"], axis=0)\n\nIn [7]: df_new\nOut[7]: \n     value1    value2  value3\n0  0.166667  0.333333   0.500\n1  0.266667  0.333333   0.400\n2  0.291667  0.333333   0.375\n\nIn [8]: df.loc[:,"value1":"value3"] = df.loc[:,"value1":"value3"].div(df["sum"], axis=0)\n\nIn [9]: df\nOut[9]: \n  id    value1    value2  value3  sum\n0  A  0.166667  0.333333   0.500    6\n1  B  0.266667  0.333333   0.400   15\n2  C  0.291667  0.333333   0.375   24\n\nIn [10]: df = pd.read_csv("test.csv")\n\nIn [11]: df\nOut[11]: \n  id  value1  value2  value3\n0  A       1       2       3\n1  B       4       5       6\n2  C       7       8       9\n\nIn [12]: df.loc[:,"value1":"value3"] = df.loc[:,"value1":"value3"].div(df.sum(axis=1), axis=0)\n\nIn [13]: df\nOut[13]: \n  id    value1    value2  value3\n0  A  0.166667  0.333333   0.500\n1  B  0.266667  0.333333   0.400\n2  C  0.291667  0.333333   0.375\n'
"In [152]: import numpy as np\nIn [153]: import pandas as pd\nIn [154]: np.where(pd.isnull(df))\nOut[154]: (array([2, 5, 6, 6, 7, 7]), array([7, 7, 6, 7, 6, 7]))\n\nIn [155]: df.iloc[2,7]\nOut[155]: nan\n\nIn [160]: [df.iloc[i,j] for i,j in zip(*np.where(pd.isnull(df)))]\nOut[160]: [nan, nan, nan, nan, nan, nan]\n\nIn [182]: np.where(df.applymap(lambda x: x == ''))\nOut[182]: (array([5]), array([7]))\n"
'df.columns = df.columns.str.replace("[()]", "_")\n\ndf = pd.DataFrame({\'(A)\':[1,2,3],\n                   \'(B)\':[4,5,6],\n                   \'C)\':[7,8,9]})\n\nprint (df)\n   (A)  (B)  C)\n0    1    4   7\n1    2    5   8\n2    3    6   9\n\ndf.columns = df.columns.str.replace(r"[()]", "_")\nprint (df)\n   _A_  _B_  C_\n0    1    4   7\n1    2    5   8\n2    3    6   9\n'
"In [3]: pd.DataFrame(q_list, columns=['q_data'])\nOut[3]:\n      q_data\n0  112354401\n1  116115526\n2  114909312\n3  122425491\n4  131957025\n5  111373473\n"
'from pandas import *\n\nd = {"my_label": Series([\'A\',\'B\',\'A\',\'C\',\'D\',\'D\',\'E\'])}\ndf = DataFrame(d)\n\n\ndef as_perc(value, total):\n    return value/float(total)\n\ndef get_count(values):\n    return len(values)\n\ngrouped_count = df.groupby("my_label").my_label.agg(get_count)\ndata = grouped_count.apply(as_perc, total=df.my_label.count())\n'
"In [126]: df.replace(['very bad', 'bad', 'poor', 'good', 'very good'], \n                     [1, 2, 3, 4, 5]) \nOut[126]: \n      resp  A  B  C\n   0     1  3  3  4\n   1     2  4  3  4\n   2     3  5  5  5\n   3     4  2  3  2\n   4     5  1  1  1\n   5     6  3  4  1\n   6     7  4  4  4\n   7     8  5  5  5\n   8     9  2  2  1\n   9    10  1  1  1\n"
"In [7]: d.sales[d.sales==24] = 100\n\nIn [8]: d\nOut[8]: \n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     12  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana    100  2009\n\nIn [26]: d.loc[d.sales == 12, 'sales'] = 99\n\nIn [27]: d\nOut[27]: \n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     99  2008\n2  sat      banana     22  2008\n3  sun      banana     23  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     23  2009\n7  sun      banana    100  2009\n\nIn [28]: d.sales = d.sales.replace(23, 24)\n\nIn [29]: d\nOut[29]: \n   day     flavour  sales  year\n0  sat  strawberry     10  2008\n1  sun  strawberry     99  2008\n2  sat      banana     22  2008\n3  sun      banana     24  2008\n4  sat  strawberry     11  2009\n5  sun  strawberry     13  2009\n6  sat      banana     24  2009\n7  sun      banana    100  2009\n"
"df_new[df_new['l_ext'].isin([31, 22, 30, 25, 64])]\n"
"In [15]: df['Data_lagged'] = df.groupby(['Group'])['Data'].shift(1)\n\nIn [16]: df\nOut[16]: \n                Date Group  Data  Data_lagged\n2014-05-14  09:10:00     A     1          NaN\n2014-05-14  09:20:00     A     2            1\n2014-05-14  09:30:00     A     3            2\n2014-05-14  09:40:00     A     4            3\n2014-05-14  09:50:00     A     5            4\n2014-05-14  10:00:00     B     1          NaN\n2014-05-14  10:10:00     B     2            1\n2014-05-14  10:20:00     B     3            2\n2014-05-14  10:30:00     B     4            3\n\n[9 rows x 4 columns]\n\ndf['Data_lagged'] = (df.sort_values(by=['Date'], ascending=True)\n                       .groupby(['Group'])['Data'].shift(1))\n"
"In [10]: df.to_numpy().max()\nOut[10]: 'f'\n\nIn [11]: 'f' &gt; 43.0\nOut[11]: True\n\ndf.select_dtypes(include=[np.number]).max()\n"
"DF3 = DF[(DF['a'] == 0) | (DF['b'] == 0)]\n"
'super_x = pd.concat([super_x, x], axis=0)\n\n| iteration | size of old super_x | size of x | copying required |\n|         0 |                   0 |         1 |                1 |\n|         1 |                   1 |         1 |                2 |\n|         2 |                   2 |         1 |                3 |\n|       ... |                     |           |                  |\n|       N-1 |                 N-1 |         1 |                N |\n\nsuper_x = []\nfor i, df_chunk in enumerate(df_list):\n    [x, y] = preprocess_data(df_chunk)\n    super_x.append(x)\nsuper_x = pd.concat(super_x, axis=0)\n'
'import pyodbc\nimport pandas\ncnxn = pyodbc.connect(r\'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};\'\n                      r\'DBQ=C:\\users\\bartogre\\desktop\\data.mdb;\')\nsql = "Select sum(CYTM), sum(PYTM), BRAND From data Group By BRAND"\ndata = pandas.read_sql(sql,cnxn)\n'
'In [22]:\ndf.columns.set_levels([\'b1\',\'c1\',\'f1\'],level=1,inplace=True)\ndf\n\nOut[22]:\n     a         d\n    b1   c1   f1\n0    1    2    3\n1   10   20   30\n2  100  200  300\n\nIn [26]:\ndf.columns = df.columns.rename("b1", level=1)\ndf\n\nOut[26]:\n      a         d\nb1    b    c    f\n0     1    2    3\n1    10   20   30\n2   100  200  300\n'
"print (type(np.nan))\n&lt;class 'float'&gt;\n\ndf['A'] = df['A'].str.extract('(\\d+)', expand=False)\ndf['B'] = df['B'].str.extract('(\\d+)', expand=False)\nprint (df)\n     A    B\n0   10   20\n1   20  NaN\n2  NaN   30\n3   40   40\n\ndf1 = df.fillna(0).astype(int)\nprint (df1)\n    A   B\n0  10  20\n1  20   0\n2   0  30\n3  40  40\n\nprint (df1.dtypes)\nA    int32\nB    int32\ndtype: object\n"
"df.set_index(['d'], append=True)\n"
"In [4]: df.pivot('name', 'id', 'x')\nOut[4]: \nid    1  2\nname      \njohn  0  0\nmike  1  0\n"
'import urllib\n\nbase_url = "http://ichart.finance.yahoo.com/table.csv?s="\ndef make_url(ticker_symbol):\n    return base_url + ticker_symbol\n\noutput_path = "C:/path/to/output/directory"\ndef make_filename(ticker_symbol, directory="S&amp;P"):\n    return output_path + "/" + directory + "/" + ticker_symbol + ".csv"\n\ndef pull_historical_data(ticker_symbol, directory="S&amp;P"):\n    try:\n        urllib.urlretrieve(make_url(ticker_symbol), make_filename(ticker_symbol, directory))\n    except urllib.ContentTooShortError as e:\n        outfile = open(make_filename(ticker_symbol, directory), "w")\n        outfile.write(e.content)\n        outfile.close()\n'
"In [77]: df = pd.concat([df, pd.get_dummies(df['YEAR'])], axis=1); df\nOut[77]: \n      JOINED_CO GENDER    EXEC_FULLNAME  GVKEY  YEAR    CONAME  BECAMECEO  \\\n5622        NaN   MALE   Ira A. Eichner   1004  1992  AAR CORP   19550101   \n5622        NaN   MALE   Ira A. Eichner   1004  1993  AAR CORP   19550101   \n5622        NaN   MALE   Ira A. Eichner   1004  1994  AAR CORP   19550101   \n5622        NaN   MALE   Ira A. Eichner   1004  1995  AAR CORP   19550101   \n5622        NaN   MALE   Ira A. Eichner   1004  1996  AAR CORP   19550101   \n5622        NaN   MALE   Ira A. Eichner   1004  1997  AAR CORP   19550101   \n5622        NaN   MALE   Ira A. Eichner   1004  1998  AAR CORP   19550101   \n5623        NaN   MALE  David P. Storch   1004  1992  AAR CORP   19961009   \n5623        NaN   MALE  David P. Storch   1004  1993  AAR CORP   19961009   \n5623        NaN   MALE  David P. Storch   1004  1994  AAR CORP   19961009   \n5623        NaN   MALE  David P. Storch   1004  1995  AAR CORP   19961009   \n5623        NaN   MALE  David P. Storch   1004  1996  AAR CORP   19961009   \n\n      REJOIN   LEFTOFC    LEFTCO  RELEFT    REASON  PAGE  1992  1993  1994  \\\n5622     NaN  19961001  19990531     NaN  RESIGNED    79     1     0     0   \n5622     NaN  19961001  19990531     NaN  RESIGNED    79     0     1     0   \n5622     NaN  19961001  19990531     NaN  RESIGNED    79     0     0     1   \n5622     NaN  19961001  19990531     NaN  RESIGNED    79     0     0     0   \n5622     NaN  19961001  19990531     NaN  RESIGNED    79     0     0     0   \n5622     NaN  19961001  19990531     NaN  RESIGNED    79     0     0     0   \n5622     NaN  19961001  19990531     NaN  RESIGNED    79     0     0     0   \n5623     NaN       NaN       NaN     NaN       NaN    57     1     0     0   \n5623     NaN       NaN       NaN     NaN       NaN    57     0     1     0   \n5623     NaN       NaN       NaN     NaN       NaN    57     0     0     1   \n5623     NaN       NaN       NaN     NaN       NaN    57     0     0     0   \n5623     NaN       NaN       NaN     NaN       NaN    57     0     0     0   \n\n      1995  1996  1997  1998  \n5622     0     0     0     0  \n5622     0     0     0     0  \n5622     0     0     0     0  \n5622     1     0     0     0  \n5622     0     1     0     0  \n5622     0     0     1     0  \n5622     0     0     0     1  \n5623     0     0     0     0  \n5623     0     0     0     0  \n5623     0     0     0     0  \n5623     1     0     0     0  \n5623     0     1     0     0  \n\ndf = pd.concat([df.drop('YEAR', axis=1), pd.get_dummies(df['YEAR'])], axis=1)\n"
"In [38]:\n\nimport numpy as np\ndf = pd.DataFrame({'a':np.arange(5)})\ndf1 = pd.DataFrame({'b':np.arange(4)})\nprint(df1)\ndf\n   b\n0  0\n1  1\n2  2\n3  3\nOut[38]:\n   a\n0  0\n1  1\n2  2\n3  3\n4  4\nIn [39]:\n\npd.concat([df,df1], ignore_index=True, axis=1)\nOut[39]:\n   0   1\n0  0   0\n1  1   1\n2  2   2\n3  3   3\n4  4 NaN\n"
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from collections import OrderedDict\n&gt;&gt;&gt;\n&gt;&gt;&gt; foo = np.array( [ 1, 2, 3 ] )\n&gt;&gt;&gt; bar = np.array( [ 4, 5, 6 ] )\n&gt;&gt;&gt;\n&gt;&gt;&gt; pd.DataFrame( OrderedDict( { 'foo': pd.Series(foo), 'bar': pd.Series(bar) } ) )\n\n   foo  bar\n0    1    4\n1    2    5\n2    3    6\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from collections import OrderedDict\n&gt;&gt;&gt;\n&gt;&gt;&gt; a = np.array( [ 1, 2, 3 ] )\n&gt;&gt;&gt; b = np.array( [ 4, 5, 6 ] )\n&gt;&gt;&gt; c = np.array( [ 7, 8, 9 ] )\n&gt;&gt;&gt;\n&gt;&gt;&gt; pd.DataFrame( OrderedDict( { 'a': pd.Series(a), 'b': pd.Series(b), 'c': pd.Series(c) } ) )\n\n   a  c  b\n0  1  7  4\n1  2  8  5\n2  3  9  6\n\n&gt;&gt;&gt; pd.DataFrame( OrderedDict( (('a', pd.Series(a)), ('b', pd.Series(b)), ('c', pd.Series(c))) ) )\n\n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n"
"df = pd.DataFrame({'column name':[7500000.0,7500000.0, np.nan]})\nprint (df['column name'])\n0    7500000.0\n1    7500000.0\n2          NaN\nName: column name, dtype: float64\n\ndf['column name'] = df['column name'].astype(np.int64)\n\n#http://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\ndf['column name'] = df['column name'].astype('Int64')\nprint (df['column name'])\n0    7500000\n1    7500000\n2        NaN\nName: column name, dtype: Int64\n\ndf['column name'].astype(np.int64)\n\ndf = pd.DataFrame({'column name':[7500000.0,7500000.0]})\nprint (df['column name'])\n0    7500000.0\n1    7500000.0\nName: column name, dtype: float64\n\ndf['column name'] = df['column name'].astype(np.int64)\n#same as\n#df['column name'] = df['column name'].astype(pd.np.int64)\nprint (df['column name'])\n0    7500000\n1    7500000\nName: column name, dtype: int64\n\ndf = pd.DataFrame({'column name':[7500000.0,np.nan]})\n\ndf['column name'] = df['column name'].fillna(0).astype(np.int64)\nprint (df['column name'])\n0    7500000\n1          0\nName: column name, dtype: int64\n\ndf = pd.DataFrame({'column name':[7500000.0,np.nan]})\n\ndf['column name'] = df['column name'].values.astype(np.int64)\nprint (df['column name'])\n0                7500000\n1   -9223372036854775808\nName: column name, dtype: int64\n"
'In [107]: pd.isnull(df.iloc[1,0])\nOut[107]: True\n\nIn [7]: pd.isna(df.iloc[1,0])\nOut[7]: True\n'
'Y.values.reshape(-1,1)\n'
"idx = df.groupby('word')['count'].idxmax()\nprint(idx)\n\nword\na       2\nan      3\nthe     1\nName: count\n\nprint(df.loc[idx, ['word', 'tag']])\n\n  word tag\n2    a   T\n3   an   T\n1  the   S\n\nimport pandas as pd\ndf = pd.DataFrame({'word':'a the a an the'.split(),\n                   'tag': list('SSTTT'),\n                   'count': [30, 20, 60, 5, 10]})\n\nprint(df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n\nword\na       T\nan      T\nthe     S\n\nN = 10000\ndf = pd.DataFrame({'word':'a the a an the'.split()*N,\n                   'tag': list('SSTTT')*N,\n                   'count': [30, 20, 60, 5, 10]*N})\ndef using_apply(df):\n    return (df.groupby('word').apply(lambda subf: subf['tag'][subf['count'].idxmax()]))\n\ndef using_idxmax_loc(df):\n    idx = df.groupby('word')['count'].idxmax()\n    return df.loc[idx, ['word', 'tag']]\n\nIn [22]: %timeit using_apply(df)\n100 loops, best of 3: 7.68 ms per loop\n\nIn [23]: %timeit using_idxmax_loc(df)\n100 loops, best of 3: 5.43 ms per loop\n\nIn [36]: df2 = df.loc[idx, ['word', 'tag']].set_index('word')\n\nIn [37]: df2\nOut[37]: \n     tag\nword    \na      T\nan     T\nthe    S\n\nIn [38]: df2.to_dict()['tag']\nOut[38]: {'a': 'T', 'an': 'T', 'the': 'S'}\n"
"In [9]: mapping = {'set': 1, 'test': 2}\n\nIn [10]: df.replace({'set': mapping, 'tesst': mapping})\nOut[10]: \n   Unnamed: 0 respondent  brand engine  country  aware  aware_2  aware_3  age  \\\n0           0          a  volvo      p      swe      1        0        1   23   \n1           1          b  volvo   None      swe      0        0        1   45   \n2           2          c    bmw      p       us      0        0        1   56   \n3           3          d    bmw      p       us      0        1        1   43   \n4           4          e    bmw      d  germany      1        0        1   34   \n5           5          f   audi      d  germany      1        0        1   59   \n6           6          g  volvo      d      swe      1        0        0   65   \n7           7          h   audi      d      swe      1        0        0   78   \n8           8          i  volvo      d       us      1        1        1   32   \n\n  tesst set  \n0     2   1  \n1     1   2  \n2     2   1  \n3     1   2  \n4     2   1  \n5     1   2  \n6     2   1  \n7     1   2  \n8     2   1  \n"
"frame = pd.DataFrame({'a' : ['the cat is blue', 'the sky is green', 'the dog is black']})\n\nframe\n                  a\n0   the cat is blue\n1  the sky is green\n2  the dog is black\n\nmylist = ['dog', 'cat', 'fish']\npattern = '|'.join(mylist)\n\npattern\n'dog|cat|fish'\n\nframe.a.str.contains(pattern)\n0     True\n1    False\n2     True\nName: a, dtype: bool\n\nframe = pd.DataFrame({'a' : ['Cat Mr. Nibbles is blue', 'the sky is green', 'the dog is black']})\n\nframe\n                     a\n0  Cat Mr. Nibbles is blue\n1         the sky is green\n2         the dog is black\n\npattern = '|'.join([f'(?i){animal}' for animal in mylist])  # python 3.6+\n\npattern\n'(?i)dog|(?i)cat|(?i)fish'\n\nframe.a.str.contains(pattern)\n0     True  # Because of the (?i) flag, 'Cat' is also matched to 'cat'\n1    False\n2     True\n"
'In [11]: pd.concat([s, s.shift(), s.shift(2)], axis=1)\nOut[11]: \n   0   1   2\n1  5 NaN NaN\n2  4   5 NaN\n3  3   4   5\n4  2   3   4\n5  1   2   3\n\nIn [12]: pd.concat([s, s.shift(), s.shift(2)], axis=1).dropna()\nOut[12]: \n   0  1  2\n3  3  4  5\n4  2  3  4\n5  1  2  3\n'
"print df\n\n    company       date  measure\n0         0 2010-01-01       10\n1         0 2010-01-15       10\n2         0 2010-02-01       10\n3         0 2010-02-15       10\n4         0 2010-03-01       10\n5         0 2010-03-15       10\n6         0 2010-04-01       10\n7         1 2010-03-01        5\n8         1 2010-03-15        5\n9         1 2010-04-01        5\n10        1 2010-04-15        5\n11        1 2010-05-01        5\n12        1 2010-05-15        5\n\nprint windows\n\n   company   end_date\n0        0 2010-02-01\n1        0 2010-03-15\n2        1 2010-04-01\n3        1 2010-05-15\n\nwindows['beg_date'] = (windows['end_date'].values.astype('datetime64[D]') -\n                       np.timedelta64(30,'D'))\nprint windows\n\n   company   end_date   beg_date\n0        0 2010-02-01 2010-01-02\n1        0 2010-03-15 2010-02-13\n2        1 2010-04-01 2010-03-02\n3        1 2010-05-15 2010-04-15\n\ndf = df.merge(windows,on='company',how='left')\ndf = df[(df.date &gt;= df.beg_date) &amp; (df.date &lt;= df.end_date)]\nprint df\n\n    company       date  measure   end_date   beg_date\n2         0 2010-01-15       10 2010-02-01 2010-01-02\n4         0 2010-02-01       10 2010-02-01 2010-01-02\n7         0 2010-02-15       10 2010-03-15 2010-02-13\n9         0 2010-03-01       10 2010-03-15 2010-02-13\n11        0 2010-03-15       10 2010-03-15 2010-02-13\n16        1 2010-03-15        5 2010-04-01 2010-03-02\n18        1 2010-04-01        5 2010-04-01 2010-03-02\n21        1 2010-04-15        5 2010-05-15 2010-04-15\n23        1 2010-05-01        5 2010-05-15 2010-04-15\n25        1 2010-05-15        5 2010-05-15 2010-04-15\n\nprint df.groupby(['company','end_date']).sum()\n\n                    measure\ncompany end_date           \n0       2010-02-01       20\n        2010-03-15       30\n1       2010-04-01       10\n        2010-05-15       15\n\nwindows['beg_date'] = (windows['end_date'].values.astype('datetime64[D]') -\n                       np.timedelta64(30,'D'))\n\ndef cond_merge(g,windows):\n    g = g.merge(windows,on='company',how='left')\n    g = g[(g.date &gt;= g.beg_date) &amp; (g.date &lt;= g.end_date)]\n    return g.groupby('end_date')['measure'].sum()\n\nprint df.groupby('company').apply(cond_merge,windows)\n\ncompany  end_date  \n0        2010-02-01    20\n         2010-03-15    30\n1        2010-04-01    10\n         2010-05-15    15\n\nwindows['date'] = windows['end_date']\n\ndf = df.merge(windows,on=['company','date'],how='outer')\nprint df\n\n    company       date  measure   end_date\n0         0 2010-01-01       10        NaT\n1         0 2010-01-15       10        NaT\n2         0 2010-02-01       10 2010-02-01\n3         0 2010-02-15       10        NaT\n4         0 2010-03-01       10        NaT\n5         0 2010-03-15       10 2010-03-15\n6         0 2010-04-01       10        NaT\n7         1 2010-03-01        5        NaT\n8         1 2010-03-15        5        NaT\n9         1 2010-04-01        5 2010-04-01\n10        1 2010-04-15        5        NaT\n11        1 2010-05-01        5        NaT\n12        1 2010-05-15        5 2010-05-15\n\ndf['end_date'] = df.groupby('company')['end_date'].apply(lambda x: x.bfill())\n\nprint df\n\n    company       date  measure   end_date\n0         0 2010-01-01       10 2010-02-01\n1         0 2010-01-15       10 2010-02-01\n2         0 2010-02-01       10 2010-02-01\n3         0 2010-02-15       10 2010-03-15\n4         0 2010-03-01       10 2010-03-15\n5         0 2010-03-15       10 2010-03-15\n6         0 2010-04-01       10        NaT\n7         1 2010-03-01        5 2010-04-01\n8         1 2010-03-15        5 2010-04-01\n9         1 2010-04-01        5 2010-04-01\n10        1 2010-04-15        5 2010-05-15\n11        1 2010-05-01        5 2010-05-15\n12        1 2010-05-15        5 2010-05-15\n\ndf = df[df.end_date.notnull()]\ndf['beg_date'] = (df['end_date'].values.astype('datetime64[D]') -\n                   np.timedelta64(30,'D'))\n\nprint df\n\n   company       date  measure   end_date   beg_date\n0         0 2010-01-01       10 2010-02-01 2010-01-02\n1         0 2010-01-15       10 2010-02-01 2010-01-02\n2         0 2010-02-01       10 2010-02-01 2010-01-02\n3         0 2010-02-15       10 2010-03-15 2010-02-13\n4         0 2010-03-01       10 2010-03-15 2010-02-13\n5         0 2010-03-15       10 2010-03-15 2010-02-13\n7         1 2010-03-01        5 2010-04-01 2010-03-02\n8         1 2010-03-15        5 2010-04-01 2010-03-02\n9         1 2010-04-01        5 2010-04-01 2010-03-02\n10        1 2010-04-15        5 2010-05-15 2010-04-15\n11        1 2010-05-01        5 2010-05-15 2010-04-15\n12        1 2010-05-15        5 2010-05-15 2010-04-15\n\ndf = df[(df.date &gt;= df.beg_date) &amp; (df.date &lt;= df.end_date)]\nprint df.groupby(['company','end_date']).sum()\n\n                    measure\ncompany end_date           \n0       2010-02-01       20\n        2010-03-15       30\n1       2010-04-01       10\n        2010-05-15       15\n"
"&gt;&gt;&gt; df.loc[df.filename == 'test2.dat', 'n'] = df2[df2.filename == 'test2.dat'].loc[0]['n']\n&gt;&gt;&gt; df\nOut[331]: \n    filename   m     n\n0  test0.dat  12  None\n1  test2.dat  13    16\n\n&gt;&gt;&gt; df.set_index('filename', inplace=True)\n&gt;&gt;&gt; df2.set_index('filename', inplace=True)\n&gt;&gt;&gt; df.update(df2)\n&gt;&gt;&gt; df\nOut[292]: \n            m     n\nfilename           \ntest0.dat  12  None\ntest2.dat  13    16\n"
'null_data = df[df.isnull().any(axis=1)]\n'
"data.index = pd.to_datetime(data.index, unit='s')\n\ndata = pd.DataFrame(\n    {'Timestamp':[1313331280, 1313334917, 1313334917, 1313340309, 1313340309], \n     'Price': [10.4]*3 + [10.5]*2, 'Volume': [0.779, 0.101, 0.316, 0.150, 1.8]})\ndata = data.set_index(['Timestamp'])\n#             Price  Volume\n# Timestamp                \n# 1313331280   10.4   0.779\n# 1313334917   10.4   0.101\n# 1313334917   10.4   0.316\n# 1313340309   10.5   0.150\n# 1313340309   10.5   1.800\n\ndata.index = pd.to_datetime(data.index, unit='s')\n\n                     Price  Volume\n2011-08-14 14:14:40   10.4   0.779\n2011-08-14 15:15:17   10.4   0.101\n2011-08-14 15:15:17   10.4   0.316\n2011-08-14 16:45:09   10.5   0.150\n2011-08-14 16:45:09   10.5   1.800\n\nticks = data.ix[:, ['Price', 'Volume']]\nbars = ticks.Price.resample('30min').ohlc()\nvolumes = ticks.Volume.resample('30min').sum()\n\nIn [368]: bars\nOut[368]: \n                     open  high   low  close\n2011-08-14 14:00:00  10.4  10.4  10.4   10.4\n2011-08-14 14:30:00   NaN   NaN   NaN    NaN\n2011-08-14 15:00:00  10.4  10.4  10.4   10.4\n2011-08-14 15:30:00   NaN   NaN   NaN    NaN\n2011-08-14 16:00:00   NaN   NaN   NaN    NaN\n2011-08-14 16:30:00  10.5  10.5  10.5   10.5\n\nIn [369]: volumes\nOut[369]: \n2011-08-14 14:00:00    0.779\n2011-08-14 14:30:00      NaN\n2011-08-14 15:00:00    0.417\n2011-08-14 15:30:00      NaN\n2011-08-14 16:00:00      NaN\n2011-08-14 16:30:00    1.950\nFreq: 30T, Name: Volume, dtype: float64\n"
'df = pd.read_csv(Input, delimiter=";", decimal=",")\n'
"df.index = np.arange(1, len(df) + 1)\n\nIn [151]:\n\ndf = pd.DataFrame({'a':np.random.randn(5)})\ndf\nOut[151]:\n          a\n0  0.443638\n1  0.037882\n2 -0.210275\n3 -0.344092\n4  0.997045\nIn [152]:\n\ndf.index = np.arange(1,len(df)+1)\ndf\nOut[152]:\n          a\n1  0.443638\n2  0.037882\n3 -0.210275\n4 -0.344092\n5  0.997045\n\ndf.index = df.index + 1\n\nIn [160]:\n\n%timeit df.index = df.index + 1\nThe slowest run took 6.45 times longer than the fastest. This could mean that an intermediate result is being cached \n10000 loops, best of 3: 107 µs per loop\n\n\nIn [161]:\n\n%timeit df.index = np.arange(1, len(df) + 1)\n10000 loops, best of 3: 154 µs per loop\n"
"import datetime\ndf['&lt;column&gt;'] = df['&lt;column&gt;'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour,15*(dt.minute // 15)))\n\nimport datetime\ndf['&lt;column&gt;'] = df['&lt;column&gt;'].apply(lambda dt: datetime.datetime(dt.year, dt.month, dt.day, dt.hour,15*round((float(dt.minute) + float(dt.second)/60) / 15)))\n"
"%matplotlib inline\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndata = np.random.randn(10,12)\n\nax = plt.axes()\nsns.heatmap(data, ax = ax)\n\nax.set_title('lalala')\nplt.show()\n"
"for row in df.loc[df.ids.isnull(), 'ids'].index:\n    df.at[row, 'ids'] = []\n\n&gt;&gt;&gt; df\n        date                                             ids\n0 2011-04-23  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n1 2011-04-24  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n2 2011-04-25  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n3 2011-04-26                                              []\n4 2011-04-27  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n5 2011-04-28  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
"res = df.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' else x.fillna('.'))\n\nprint(res)\n     A      B     City   Name\n0  1.0   0.25  Seattle   Jack\n1  2.1   0.00       SF    Sue\n2  0.0   0.00       LA      .\n3  4.7   4.00       OC    Bob\n4  5.6  12.20        .  Alice\n5  6.8  14.40        .   John\n"
"print df['status'].value_counts()\nN    14\nS     4\nC     2\nName: status, dtype: int64\n\ncounts = df['status'].value_counts().to_dict()\nprint counts\n{'S': 4, 'C': 2, 'N': 14}\n"
"dat.dropna()\n\ndat.dropna(how='any')    #to drop if any value in the row has a nan\ndat.dropna(how='all')    #to drop if all values in the row are nan\n\ndat.dropna(subset=[col_list])  # col_list is a list of column names to consider for nan values.\n"
'df.drop(df.index, inplace=True)\n\ndf.iloc[0:0]\n'
"df.resample('D', on='Date_Time').mean()\n\n              B\nDate_Time      \n2001-10-01  4.5\n2001-10-02  6.0\n\ndf.set_index('Date_Time').groupby(pd.Grouper(freq='D')).mean()\n\n              B\nDate_Time      \n2001-10-01  4.5\n2001-10-02  6.0\n\ndf.set_index('Date_Time').groupby(pd.TimeGrouper('D')).mean().dropna()\n\n              B\nDate_Time      \n2001-10-01  4.5\n2001-10-02  6.0\n"
'kwargs = {"ln(A)" : lambda x: np.log(x.A)}\ndf.assign(**kwargs)\n\n    A         B     ln(A)\n0   1  0.500033  0.000000\n1   2 -0.392229  0.693147\n2   3  0.385512  1.098612\n3   4 -0.029816  1.386294\n4   5 -2.386748  1.609438\n5   6 -1.828487  1.791759\n6   7  0.096117  1.945910\n7   8 -2.867469  2.079442\n8   9 -0.731787  2.197225\n9  10 -0.686110  2.302585\n'
"    df = pd.DataFrame([[1, 2], [3, 4]], index=['A', 'B'], columns=['X', 'Y'])\n\ndf.loc['A', 'Y']\n\n2\n\ndf.loc[['B', 'A'], 'X']\n\nB    3\nA    1\nName: X, dtype: int64\n\ndf.loc[['B', 'A'], ['X']]\n\n   X\nB  3\nA  1\n\ndf.loc[[True, False], ['X']]\n\n   X\nA  1\n\niris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n\niris_data.loc[iris_data['class'] == 'versicolor', 'class'] = 'Iris-versicolor'\n"
"import pyarrow.parquet as pq\nimport s3fs\ns3 = s3fs.S3FileSystem()\n\npandas_dataframe = pq.ParquetDataset('s3://your-bucket/', filesystem=s3).read_pandas().to_pandas()\n"
'import numpy as np\nimport pandas\n\ndf = pandas.DataFrame({"a": np.random.random(100),\n                       "b": np.random.random(100),\n                       "id": np.arange(100)})\n\n# Bin the data frame by "a" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(np.digitize(df.a, bins))\n\n# Get the mean of each bin:\nprint groups.mean() # Also could do "groups.aggregate(np.mean)"\n\n# Similarly, the median:\nprint groups.median()\n\n# Apply some arbitrary function to aggregate binned data\nprint groups.aggregate(lambda x: np.mean(x[x &gt; 0.5]))\n\ngroups.mean().b\n\nimport numpy as np\nimport pandas\n\ndf = pandas.DataFrame({"a": np.random.random(100), \n                       "b": np.random.random(100) + 10})\n\n# Bin the data frame by "a" with 10 bins...\nbins = np.linspace(df.a.min(), df.a.max(), 10)\ngroups = df.groupby(pandas.cut(df.a, bins))\n\n# Get the mean of b, binned by the values in a\nprint groups.mean().b\n\na\n(0.00186, 0.111]    10.421839\n(0.111, 0.22]       10.427540\n(0.22, 0.33]        10.538932\n(0.33, 0.439]       10.445085\n(0.439, 0.548]      10.313612\n(0.548, 0.658]      10.319387\n(0.658, 0.767]      10.367444\n(0.767, 0.876]      10.469655\n(0.876, 0.986]      10.571008\nName: b\n'
"import pandas as pd\nimport numpy as np\ndf = pd.DataFrame(np.arange(1,7).reshape(2,3),\n                  columns = list('abc'), \n                  index=pd.Series([2,5], name='b'))\nprint(df)\n#    a  b  c\n# b         \n# 2  1  2  3\n# 5  4  5  6\nprint(np.where(df.index==5)[0])\n# [1]\nprint(np.where(df['c']==6)[0])\n# [1]\n"
"In [48]: cols = list('abc')\n\nIn [49]: df = DataFrame(randn(10, len(cols)), columns=cols)\n\nIn [50]: df.a.quantile(0.95)\nOut[50]: 1.5776961953820687\n\nIn [72]: df[df.a &lt; df.a.quantile(.95)]\nOut[72]:\n       a      b      c\n0 -1.044 -0.247 -1.149\n2  0.395  0.591  0.764\n3 -0.564 -2.059  0.232\n4 -0.707 -0.736 -1.345\n5  0.978 -0.099  0.521\n6 -0.974  0.272 -0.649\n7  1.228  0.619 -0.849\n8 -0.170  0.458 -0.515\n9  1.465  1.019  0.966\n"
"&gt;&gt;&gt; import json\n&gt;&gt;&gt; df = pd.DataFrame.from_dict({'A': {1: datetime.datetime.now()}})\n&gt;&gt;&gt; df\n                           A\n1 2013-11-23 21:14:34.118531\n\n&gt;&gt;&gt; records = json.loads(df.T.to_json()).values()\n&gt;&gt;&gt; db.myCollection.insert(records)\n\n&gt;&gt;&gt; df = read_mongo(db, 'myCollection')\n&gt;&gt;&gt; df\n                     A\n0  1385241274118531000\n&gt;&gt;&gt; df.dtypes\nA    int64\ndtype: object\n\n&gt;&gt;&gt; df['A'] = pd.to_datetime(df['A'])\n&gt;&gt;&gt; df\n                           A\n0 2013-11-23 21:14:34.118531\n"
'my_df.ix[(my_df.CHUNK_NAME==chunks[0])&amp;(my_df.LAMBDA==lam_beta[0][0])]\n         ^                           ^ ^                            ^\n'
'In [43]:\n\ndf.describe()\n\nOut[43]:\n\n       shopper_num is_martian  number_of_items  count_pineapples\ncount      14.0000         14        14.000000                14\nmean        7.5000          0         3.357143                 0\nstd         4.1833          0         6.452276                 0\nmin         1.0000      False         0.000000                 0\n25%         4.2500          0         0.000000                 0\n50%         7.5000          0         0.000000                 0\n75%        10.7500          0         3.500000                 0\nmax        14.0000      False        22.000000                 0\n\n[8 rows x 4 columns]\n\nIn [47]:\n\ndf.describe().transpose()\n\nOut[47]:\n\n                 count      mean       std    min   25%  50%    75%    max\nshopper_num         14       7.5    4.1833      1  4.25  7.5  10.75     14\nis_martian          14         0         0  False     0    0      0  False\nnumber_of_items     14  3.357143  6.452276      0     0    0    3.5     22\ncount_pineapples    14         0         0      0     0    0      0      0\n\n[4 rows x 8 columns]\n'
'import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom io import StringIO\n\ns = StringIO("""     amount     price\nA     40929   4066443\nB     93904   9611272\nC    188349  19360005\nD    248438  24335536\nE    205622  18888604\nF    140173  12580900\nG     76243   6751731\nH     36859   3418329\nI     29304   2758928\nJ     39768   3201269\nK     30350   2867059""")\n\ndf = pd.read_csv(s, index_col=0, delimiter=\' \', skipinitialspace=True)\n\nfig = plt.figure() # Create matplotlib figure\n\nax = fig.add_subplot(111) # Create matplotlib axes\nax2 = ax.twinx() # Create another axes that shares the same x-axis as ax.\n\nwidth = 0.4\n\ndf.amount.plot(kind=\'bar\', color=\'red\', ax=ax, width=width, position=1)\ndf.price.plot(kind=\'bar\', color=\'blue\', ax=ax2, width=width, position=0)\n\nax.set_ylabel(\'Amount\')\nax2.set_ylabel(\'Price\')\n\nplt.show()\n'
"test['tweet'].apply(lambda x: [item for item in x if item not in stop])\n\n0               [love, car]\n1           [view, amazing]\n2    [feel, great, morning]\n3        [excited, concert]\n4            [best, friend]\n"
'In[16]:\n\ndf.reindex(df.b.abs().sort_values().index)\nOut[16]: \n   a  b\n2  3 -1\n3  4  2\n0  1 -3\n1  2  5\n4  5 -9\n'
"df.nunique()\na    4\nb    5\nc    1\ndtype: int64\n\nIn [205]:\ndf = pd.DataFrame({'a':[0,1,1,2,3],'b':[1,2,3,4,5],'c':[1,1,1,1,1]})\ndf\n\nOut[205]:\n   a  b  c\n0  0  1  1\n1  1  2  1\n2  1  3  1\n3  2  4  1\n4  3  5  1\n\nIn [206]:\ndf.T.apply(lambda x: x.nunique(), axis=1)\n\nOut[206]:\na    4\nb    5\nc    1\ndtype: int64\n\nIn [208]:\ndf.apply(pd.Series.nunique)\n\nOut[208]:\na    4\nb    5\nc    1\ndtype: int64\n"
"In [29]:\ndf['new_col'] = df['First'].astype(str).str[0]\ndf\n\nOut[29]:\n   First  Second new_col\n0    123     234       1\n1     22    4353       2\n2     32     355       3\n3    453     453       4\n4     45     345       4\n5    453     453       4\n6     56      56       5\n"
"print (pd.merge(df1, df2, on='company'))\n\nprint (df1)\n  company standard\n0    tata       A1\n1     cts       A2\n2    dell       A3\n\nprint (df2)\n  company  return\n0    tata      71\n1    dell      78\n2     cts      27\n3     hcl      23\n\nprint (pd.merge(df1, df2, on='company'))\n  company standard  return\n0    tata       A1      71\n1     cts       A2      27\n2    dell       A3      78\n"
"# Setup\nA = pd.DataFrame(index=['a', 'b', 'c']) \nB = pd.DataFrame(index=['b', 'c', 'd', 'f'])                                  \nC = pd.DataFrame(index=[1, 2, 3])\n\n# Example of alignable indexes - A &amp; B (complete or partial overlap of indexes)\nA.index B.index\n      a        \n      b       b   (overlap)\n      c       c   (overlap)\n              d\n              f\n\n# Example of unalignable indexes - A &amp; C (no overlap at all)\nA.index C.index\n      a        \n      b        \n      c        \n              1\n              2\n              3\n\ndf1.index.equals(df2.index)\n# False\ndf1.index.intersection(df2.index).empty\n# True\n\n# Optional, if you want a RangeIndex =&gt; [0, 1, 2, ...]\n# df1.index = pd.RangeIndex(len(df))\n# Homogenize the index values,\ndf2.index = df1.index\n# Assign the columns.\ndf2[['date', 'hour']] = df1[['date', 'hour']]\n\n# pandas &gt;= 0.24\ndf2['date'] = df1['date'].to_numpy()\n# pandas &lt; 0.24\ndf2['date'] = df1['date'].values\n\ndf2[['date', 'hour']] = df1[['date', 'hour']].to_numpy()\n"
'In [33]: df2\nOut[33]: \n                   A         B         C         D\n2000-01-03  0.638998  1.277361  0.193649  0.345063\n2000-01-04 -0.816756 -1.711666 -1.155077 -0.678726\n2000-01-05  0.435507 -0.025162 -1.112890  0.324111\n2000-01-06 -0.210756 -1.027164  0.036664  0.884715\n2000-01-07 -0.821631 -0.700394 -0.706505  1.193341\n2000-01-10  1.015447 -0.909930  0.027548  0.258471\n2000-01-11 -0.497239 -0.979071 -0.461560  0.447598\n\nIn [34]: df1\nOut[34]: \n                   A         B         C\n2000-01-03  2.288863  0.188175 -0.040928\n2000-01-04  0.159107 -0.666861 -0.551628\n2000-01-05 -0.356838 -0.231036 -1.211446\n2000-01-06 -0.866475  1.113018 -0.001483\n2000-01-07  0.303269  0.021034  0.471715\n2000-01-10  1.149815  0.686696 -1.230991\n2000-01-11 -1.296118 -0.172950 -0.603887\n2000-01-12 -1.034574 -0.523238  0.626968\n2000-01-13 -0.193280  1.857499 -0.046383\n2000-01-14 -1.043492 -0.820525  0.868685\n\nIn [35]: df2.comb\ndf2.combine        df2.combineAdd     df2.combine_first  df2.combineMult    \n\nIn [35]: df2.combine_first(df1)\nOut[35]: \n                   A         B         C         D\n2000-01-03  0.638998  1.277361  0.193649  0.345063\n2000-01-04 -0.816756 -1.711666 -1.155077 -0.678726\n2000-01-05  0.435507 -0.025162 -1.112890  0.324111\n2000-01-06 -0.210756 -1.027164  0.036664  0.884715\n2000-01-07 -0.821631 -0.700394 -0.706505  1.193341\n2000-01-10  1.015447 -0.909930  0.027548  0.258471\n2000-01-11 -0.497239 -0.979071 -0.461560  0.447598\n2000-01-12 -1.034574 -0.523238  0.626968       NaN\n2000-01-13 -0.193280  1.857499 -0.046383       NaN\n2000-01-14 -1.043492 -0.820525  0.868685       NaN\n'
'from pandas import *\n\nidx = Int64Index([171, 174, 173])\ndf = DataFrame(index = idx, data =([1,2,3]))\nprint df\n\n     0\n171  1\n174  2\n173  3\n'
"import numpy as np\nimport pandas as pd\nx = np.random.randint(0, 200, 10**6)\ndf1 = pd.DataFrame({'x':x})\ndf2 = df1.set_index('x', drop=False)\ndf3 = df2.sort_index()\n%timeit df1.loc[100]\n%timeit df2.loc[100]\n%timeit df3.loc[100]\n\n10000 loops, best of 3: 71.2 µs per loop\n10 loops, best of 3: 38.9 ms per loop\n10000 loops, best of 3: 134 µs per loop\n"
'In [1]: pandas.date_range("11:00", "21:30", freq="30min")\nOut[1]: \n&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2013-07-14 11:00:00, ..., 2013-07-14 21:30:00]\nLength: 22, Freq: 30T, Timezone: None\n\nIn [2]: pandas.date_range("11:00", "21:30", freq="30min").time\nOut[2]: \narray([datetime.time(11, 0), datetime.time(11, 30), datetime.time(12, 0),\n       datetime.time(12, 30), datetime.time(13, 0), datetime.time(13, 30),\n       datetime.time(14, 0), datetime.time(14, 30), datetime.time(15, 0),\n       datetime.time(15, 30), datetime.time(16, 0), datetime.time(16, 30),\n       datetime.time(17, 0), datetime.time(17, 30), datetime.time(18, 0),\n       datetime.time(18, 30), datetime.time(19, 0), datetime.time(19, 30),\n       datetime.time(20, 0), datetime.time(20, 30), datetime.time(21, 0),\n       datetime.time(21, 30)], dtype=object)\n'
'In [11]: from pandas.testing import assert_frame_equal\n\nIn [12]: assert_frame_equal(df, expected, check_names=False)\n\ntry:\n    assert_frame_equal(df, expected, check_names=False)\n    return True\nexcept AssertionError:\n    return False\n\ndf.equals(expected)\n'
'import pylab as pl\nfrom pandas import *\ndata = DataFrame(np.random.randn(500).reshape(100,5), columns=list(\'abcde\'))\naxes = data.hist(sharey=True, sharex=True)\npl.suptitle("This is Figure title")\n'
"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame()\ndf['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']\ndf['calories'] = [2200, 2100, 1500]\ndf['sleep hours'] = [8, 7.5, 8.2]\ndf['gym'] = [True, False, False]\n\ndef render_mpl_table(data, col_width=3.0, row_height=0.625, font_size=14,\n                     header_color='#40466e', row_colors=['#f1f1f2', 'w'], edge_color='w',\n                     bbox=[0, 0, 1, 1], header_columns=0,\n                     ax=None, **kwargs):\n    if ax is None:\n        size = (np.array(data.shape[::-1]) + np.array([0, 1])) * np.array([col_width, row_height])\n        fig, ax = plt.subplots(figsize=size)\n        ax.axis('off')\n    mpl_table = ax.table(cellText=data.values, bbox=bbox, colLabels=data.columns, **kwargs)\n    mpl_table.auto_set_font_size(False)\n    mpl_table.set_fontsize(font_size)\n\n    for k, cell in mpl_table._cells.items():\n        cell.set_edgecolor(edge_color)\n        if k[0] == 0 or k[1] &lt; header_columns:\n            cell.set_text_props(weight='bold', color='w')\n            cell.set_facecolor(header_color)\n        else:\n            cell.set_facecolor(row_colors[k[0]%len(row_colors) ])\n    return ax.get_figure(), ax\n\nfig,ax = render_mpl_table(df, header_columns=0, col_width=2.0)\nfig.savefig(&quot;table_mpl.png&quot;)\n\nimport plotly.figure_factory as ff\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['date'] = ['2016-04-01', '2016-04-02', '2016-04-03']\ndf['calories'] = [2200, 2100, 1500]\ndf['sleep hours'] = [8, 7.5, 8.2]\ndf['gym'] = [True, False, False]\n\nfig =  ff.create_table(df)\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=200,\n)\nfig.write_image(&quot;table_plotly.png&quot;, scale=2)\nfig.show()\n"
"df.sort(['ticker', 'date'], inplace=True)\ndf['diffs'] = df['value'].diff()\n\nmask = df.ticker != df.ticker.shift(1)\ndf['diffs'][mask] = np.nan\n\ndf.filter(['ticker', 'date', 'value'])\n\ndf.set_index(['ticker','date'], inplace=True)\ndf.sort_index(inplace=True)\ndf['diffs'] = np.nan \n\nfor idx in df.index.levels[0]:\n    df.diffs[idx] = df.value[idx].diff()\n\n   date ticker  value\n0    63      C   1.65\n1    88      C  -1.93\n2    22      C  -1.29\n3    76      A  -0.79\n4    72      B  -1.24\n5    34      A  -0.23\n6    92      B   2.43\n7    22      A   0.55\n8    32      A  -2.50\n9    59      B  -1.01\n\n             value  diffs\nticker date              \nA      22     0.55    NaN\n       32    -2.50  -3.05\n       34    -0.23   2.27\n       76    -0.79  -0.56\nB      59    -1.01    NaN\n       72    -1.24  -0.23\n       92     2.43   3.67\nC      22    -1.29    NaN\n       63     1.65   2.94\n       88    -1.93  -3.58\n"
"In [11]: pd.concat([df1['c'], df2['c']], axis=1, keys=['df1', 'df2'])\nOut[11]: \n                 df1       df2\n2014-01-01       NaN -0.978535\n2014-01-02 -0.106510 -0.519239\n2014-01-03 -0.846100 -0.313153\n2014-01-04 -0.014253 -1.040702\n2014-01-05  0.315156 -0.329967\n2014-01-06 -0.510577 -0.940901\n2014-01-07       NaN -0.024608\n2014-01-08       NaN -1.791899\n\n[8 rows x 2 columns]\n\ndf1 = pd.DataFrame([1, 2, 3])\ndf2 = pd.DataFrame(['a', 'b', 'c'])\n\npd.concat([df1, df2], axis=0)\n   0\n0  1\n1  2\n2  3\n0  a\n1  b\n2  c\n\npd.concat([df1, df2], axis=1)\n\n   0  0\n0  1  a\n1  2  b\n2  3  c\n"
'df.loc[:, df.dtypes == np.float64]\n'
"In [11]: df1.change.shift(1)\nOut[11]:\n0          NaT\n1   2014-03-08\n2   2014-04-08\n3   2014-05-08\n4   2014-06-08\nName: change, dtype: datetime64[ns]\n\nIn [12]: df1.change.shift(1) - df1.change\nOut[12]:\n0        NaT\n1   -31 days\n2   -30 days\n3   -31 days\n4     0 days\nName: change, dtype: timedelta64[ns]\n\nIn [13]: df.groupby('case')['change'].apply(lambda x: x.shift(1) - x)\nOut[13]:\n0        NaT\n1   -31 days\n2   -30 days\n3   -31 days\n4        NaT\ndtype: timedelta64[ns]\n"
'import numpy as np\nimport pandas as pd\n\n# some random data frames\ndf1 = pd.DataFrame(dict(x=np.random.randn(100), y=np.random.randint(0, 5, 100)))\ndf2 = pd.DataFrame(dict(x=np.random.randn(100), y=np.random.randint(0, 5, 100)))\n\n# concatenate them\ndf_concat = pd.concat((df1, df2))\n\nprint df_concat.mean()\n# x   -0.163044\n# y    2.120000\n# dtype: float64\n\nprint df_concat.median()\n# x   -0.192037\n# y    2.000000\n# dtype: float64\n\nby_row_index = df_concat.groupby(df_concat.index)\ndf_means = by_row_index.mean()\n\nprint df_means.head()\n#           x    y\n# 0 -0.850794  1.5\n# 1  0.159038  1.5\n# 2  0.083278  1.0\n# 3 -0.540336  0.5\n# 4  0.390954  3.5\n'
"cols = ['X', 'Y']\ndf.loc[:,cols] = df.loc[:,cols].ffill()\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; \n&gt;&gt;&gt; ## create dataframe\n... ts1 = [0, 1, np.nan, np.nan, np.nan, np.nan]\n&gt;&gt;&gt; ts2 = [0, 2, np.nan, 3, np.nan, np.nan]\n&gt;&gt;&gt; d =  {'X': ts1, 'Y': ts2, 'Z': ts2}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; print(df.head())\n    X   Y   Z\n0   0   0   0\n1   1   2   2\n2 NaN NaN NaN\n3 NaN   3   3\n4 NaN NaN NaN\n&gt;&gt;&gt; \n&gt;&gt;&gt; ## apply forward fill\n... cols = ['X', 'Y']\n&gt;&gt;&gt; df.loc[:,cols] = df.loc[:,cols].ffill()\n&gt;&gt;&gt; print(df.head())\n   X  Y   Z\n0  0  0   0\n1  1  2   2\n2  1  2 NaN\n3  1  3   3\n4  1  3 NaN\n"
"for df in pd.read_csv('matrix.txt',sep=',', header = None, chunksize=1):\n    #do something\n\ndf = pd.read_csv('matrix.txt',sep=',', header = None, skiprows=1000, chunksize=1000)\n"
"merged = pd.merge(type_df, time_df, left_index=True, right_on='Project')\n"
"import numpy as np\n\nprint df\n     a    b    c\na  1.0  0.5  0.3\nb  0.5  1.0  0.4\nc  0.3  0.4  1.0\n\nprint np.triu(np.ones(df.shape)).astype(np.bool)\n[[ True  True  True]\n [False  True  True]\n [False False  True]]\n\ndf = df.where(np.triu(np.ones(df.shape)).astype(np.bool))\nprint df\n    a    b    c\na   1  0.5  0.3\nb NaN  1.0  0.4\nc NaN  NaN  1.0\n\ndf = df.stack().reset_index()\ndf.columns = ['Row','Column','Value']\nprint df\n\n  Row Column  Value\n0   a      a    1.0\n1   a      b    0.5\n2   a      c    0.3\n3   b      b    1.0\n4   b      c    0.4\n5   c      c    1.0\n"
'df.select_dtypes(include=[np.datetime64])\n\n    date_col\n0   2017-02-01\n1   2017-03-01\n2   2017-04-01\n3   2017-05-01\n\ndf.select_dtypes(include=[np.number])\n\n    col1    col2\n0   1       2\n1   1       2\n2   1       2\n3   1       2\n'
"import pandas as pd\nimport pandasql as ps\n\ndf = pd.DataFrame([[1234, 'Customer A', '123 Street', np.nan],\n               [1234, 'Customer A', np.nan, '333 Street'],\n               [1233, 'Customer B', '444 Street', '333 Street'],\n              [1233, 'Customer B', '444 Street', '666 Street']], columns=\n['ID', 'Customer', 'Billing Address', 'Shipping Address'])\n\nq1 = &quot;&quot;&quot;SELECT ID FROM df &quot;&quot;&quot;\n\nprint(ps.sqldf(q1, locals()))\n\n     ID\n0  1234\n1  1234\n2  1233\n3  1233\n\nps.sqldf(&quot;select * from df&quot;)\n"
"import os\n\noutname = 'name.csv'\n\noutdir = './dir'\nif not os.path.exists(outdir):\n    os.mkdir(outdir)\n\nfullname = os.path.join(outdir, outname)    \n\ndf.to_csv(fullname)\n"
"print(df[df['x'].isnull()])\n\ndf['x'] = pd.to_numeric(df['x'], errors='coerce')\n\ndf = df.dropna(subset=['x'])\n\ndf['x'] = df['x'].astype(int)\n"
'In [68]: data = read_table(\'sample.txt\', skiprows=3, header=None, sep=r"\\s*")\n\nIn [69]: data\nOut[69]: \n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 7 entries, 0 to 6\nData columns:\nX.1     7  non-null values\nX.2     7  non-null values\nX.3     7  non-null values\nX.4     7  non-null values\nX.5     7  non-null values\nX.6     7  non-null values\n[...]\nX.23    7  non-null values\nX.24    7  non-null values\nX.25    5  non-null values\nX.26    3  non-null values\ndtypes: float64(8), int64(10), object(8)\n\nIn [73]: data.ix[:,20:]\nOut[73]: \n   X.21  X.22           X.23                   X.24            X.25    X.26\n0   315  0.95            ABC            transporter   transmembrane  region\n1   527  0.93            ABC            transporter            None    None\n2   408  0.86  RecF/RecN/SMC                      N        terminal  domain\n3   575  0.85  RecF/RecN/SMC                      N        terminal  domain\n4   556  0.72            AAA                 ATPase          domain    None\n5   275  0.85      YceG-like                 family            None    None\n6   200  0.85       Pyridine  nucleotide-disulphide  oxidoreductase    None\n'
'import pandas\ndfdict={}\ndfdict["a"]=[1,2,3,4]\ndfdict["b"]=[5,6,7,8]\ndfdict["c"]=[9,10,11,12]\ndf=pandas.DataFrame(dfdict)\ndf.to_csv("dfTest.txt","\\t",header=True,cols=["b","a","c"])\n\n    b   a   c\n0   1   5   9\n1   2   6   10\n2   3   7   11\n3   4   8   12\n\npandas.version.version\n\ndf.to_csv("dfTest.txt","\\t",header=True,cols=["b","a","c"], engine=\'python\')\n'
"In [1]: df = DataFrame(np.random.rand(4,5), columns = list('abcde'))\n\nIn [2]: df\nOut[2]: \n          a         b         c         d         e\n0  0.669701  0.780497  0.955690  0.451573  0.232194\n1  0.952762  0.585579  0.890801  0.643251  0.556220\n2  0.900713  0.790938  0.952628  0.505775  0.582365\n3  0.994205  0.330560  0.286694  0.125061  0.575153\n\nIn [5]: df.loc[df['c']&gt;0.5,['a','d']]\nOut[5]: \n          a         d\n0  0.669701  0.451573\n1  0.952762  0.643251\n2  0.900713  0.505775\n\nIn [6]: df.loc[df['c']&gt;0.5,['a','d']].values\nOut[6]: \narray([[ 0.66970138,  0.45157274],\n       [ 0.95276167,  0.64325143],\n       [ 0.90071271,  0.50577509]])\n"
"# generate a series of 365 days\n# index = 20190101, 20190102, ... 20191231\n# values = [0,1,...364]\nts = pd.Series(range(365), index = pd.date_range(start='20190101', \n                                                end='20191231',\n                                                freq = 'D'))\nts.head()\n\noutput:\n2019-01-01    0\n2019-01-02    1\n2019-01-03    2\n2019-01-04    3\n2019-01-05    4\nFreq: D, dtype: int64\n\nts.asfreq(freq='Q')\n\noutput:\n2019-03-31     89\n2019-06-30    180\n2019-09-30    272\n2019-12-31    364\nFreq: Q-DEC, dtype: int64\n\nts.resample('Q')\n\noutput:\nDatetimeIndexResampler [freq=&lt;QuarterEnd: startingMonth=12&gt;, axis=0, closed=right, label=right, convention=start, base=0]\n\nbins = ts.resample('Q')\nbin.groups\n\noutput:\n {Timestamp('2019-03-31 00:00:00', freq='Q-DEC'): 90,\n Timestamp('2019-06-30 00:00:00', freq='Q-DEC'): 181,\n Timestamp('2019-09-30 00:00:00', freq='Q-DEC'): 273,\n Timestamp('2019-12-31 00:00:00', freq='Q-DEC'): 365}\n\n# (89+180+272+364)/4 = 226.25\nts.asfreq(freq='Q').mean()\n\noutput:\n226.25\n\nts.resample('Q').mean()\n\noutput:\n2019-03-31     44.5\n2019-06-30    135.0\n2019-09-30    226.5\n2019-12-31    318.5\n"
"df.map_partitions(func, columns=...)\n\ndf.mycolumn.map(func)\n\ndf.apply(func, axis=1)\n\ndf = dd.read_csv(...)\n\ndf.map_partitions(func, columns=...).compute(scheduler='processes')\n\nIn [1]: import numpy as np\nIn [2]: import pandas as pd\nIn [3]: s = pd.Series([10000]*120)\n\nIn [4]: %paste\ndef slow_func(k):\n    A = np.random.normal(size = k) # k = 10000\n    s = 0\n    for a in A:\n        if a &gt; 0:\n            s += 1\n        else:\n            s -= 1\n    return s\n## -- End pasted text --\n\nIn [5]: %time _ = s.apply(slow_func)\nCPU times: user 345 ms, sys: 3.28 ms, total: 348 ms\nWall time: 347 ms\n\nIn [6]: import numba\nIn [7]: fast_func = numba.jit(slow_func)\n\nIn [8]: %time _ = s.apply(fast_func)  # First time incurs compilation overhead\nCPU times: user 179 ms, sys: 0 ns, total: 179 ms\nWall time: 175 ms\n\nIn [9]: %time _ = s.apply(fast_func)  # Subsequent times are all gain\nCPU times: user 68.8 ms, sys: 27 µs, total: 68.8 ms\nWall time: 68.7 ms\n"
"df.append(pandas.Series(name='NameOfNewRow'))\n\ndf.append(pandas.Series(), ignore_index=True)\n"
"df['match'] = df.col1.eq(df.col1.shift())\nprint (df)\n   col1  match\n0     1  False\n1     3  False\n2     3   True\n3     1  False\n4     2  False\n5     3  False\n6     2  False\n7     2   True\n\ndf['match'] = df.col1 == df.col1.shift()\nprint (df)\n   col1  match\n0     1  False\n1     3  False\n2     3   True\n3     1  False\n4     2  False\n5     3  False\n6     2  False\n7     2   True\n\nimport pandas as pd\ndata={'col1':[1,3,3,1,2,3,2,2]}\ndf=pd.DataFrame(data,columns=['col1'])\nprint (df)\n#[80000 rows x 1 columns]\ndf = pd.concat([df]*10000).reset_index(drop=True)\n\ndf['match'] = df.col1 == df.col1.shift()\ndf['match1'] = df.col1.eq(df.col1.shift())\nprint (df)\n\nIn [208]: %timeit df.col1.eq(df.col1.shift())\nThe slowest run took 4.83 times longer than the fastest. This could mean that an intermediate result is being cached.\n1000 loops, best of 3: 933 µs per loop\n\nIn [209]: %timeit df.col1 == df.col1.shift()\n1000 loops, best of 3: 1 ms per loop\n"
"df.set_index('Attribute',inplace=True)\ndf.transpose()\n\ndf.set_index('Attribute').T\n"
"df = df.drop_duplicates('COL2')\n#same as\n#df = df.drop_duplicates('COL2', keep='first')\nprint (df)\n    COL1  COL2\n0  a.com    22\n1  b.com    45\n2  c.com    34\n4  f.com    56\n\ndf = df.drop_duplicates('COL2', keep='last')\nprint (df)\n    COL1  COL2\n2  c.com    34\n4  f.com    56\n5  g.com    22\n6  h.com    45\n\ndf = df.drop_duplicates('COL2', keep=False)\nprint (df)\n    COL1  COL2\n2  c.com    34\n4  f.com    56\n"
'a = np.array([5, 6, 7, 8])\ndf = pd.DataFrame({"a": [a]})\n\ndf = pd.DataFrame({\'id\': [1, 2, 3, 4],\n                   \'a\': [\'on\', \'on\', \'off\', \'off\'],\n                   \'b\': [\'on\', \'off\', \'on\', \'off\']})\n\ndf[\'new\'] = df.apply(lambda r: tuple(r), axis=1).apply(np.array)\n\ndf[\'new\'][0]\n\narray([\'on\', \'on\', \'1\'], dtype=\'&lt;U2\')\n'
'grouped.agg(lambda x: x.iloc[0])\n\nIn [1]: df = pd.DataFrame([[1, 2], [3, 4]])\n\nIn [2]: g = df.groupby(0)\n\nIn [3]: g.first()\nOut[3]: \n   1\n0   \n1  2\n3  4\n\nIn [4]: g.agg(lambda x: x.iloc[0])\nOut[4]: \n   1\n0   \n1  2\n3  4\n\ng.agg({1: lambda x: x.iloc[0]})\n\ng.nth(0)  # first\ng.nth(-1)  # last\n\ngrouped[\'D\'].agg({\'result1\' : "sum", \'result2\' : "mean"})\n'
'&gt;&gt;&gt; df = pd.DataFrame({"A": range(1000), "B": range(1000)})\n&gt;&gt;&gt; df\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 1000 entries, 0 to 999\nData columns:\nA    1000  non-null values\nB    1000  non-null values\ndtypes: int64(2)\n&gt;&gt;&gt; df[:5]\n   A  B\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n4  4  4\n\n&gt;&gt;&gt; df = pd.DataFrame({i: range(1000) for i in range(100)})\n&gt;&gt;&gt; df.ix[:5, :10]\n   0   1   2   3   4   5   6   7   8   9   10\n0   0   0   0   0   0   0   0   0   0   0   0\n1   1   1   1   1   1   1   1   1   1   1   1\n2   2   2   2   2   2   2   2   2   2   2   2\n3   3   3   3   3   3   3   3   3   3   3   3\n4   4   4   4   4   4   4   4   4   4   4   4\n5   5   5   5   5   5   5   5   5   5   5   5\n'
"# dont' use dtype converters explicity for the columns you care about\n# they will be converted to float64 if possible, or object if they cannot\ndf = pd.read_csv('test.csv'.....)\n\n#### this is optional and related to the issue you posted ####\n# force anything that is not a numeric to nan\n# columns are the list of columns that you are interesetd in\ndf[columns] = df[columns].convert_objects(convert_numeric=True)\n\n\n    # astype\n    df[columns] = df[columns].astype('float32')\n\nsee http://pandas.pydata.org/pandas-docs/dev/basics.html#object-conversion\n\nIts not as efficient as doing it directly in read_csv (but that requires\n some low-level changes)\n\nIn [5]: x = pd.read_csv(StringIO.StringIO(data), dtype={'a': np.float32}, delim_whitespace=True)\n\nIn [6]: x\nOut[6]: \n         a        b\n0  0.76398  0.81394\n1  0.32136  0.91063\n\nIn [7]: x.dtypes\nOut[7]: \na    float32\nb    float64\ndtype: object\n\nIn [8]: pd.__version__\nOut[8]: '0.11.0.dev-385ff82'\n\nIn [9]: quit()\nvagrant@precise32:~/pandas$ uname -a\nLinux precise32 3.2.0-23-generic-pae #36-Ubuntu SMP Tue Apr 10 22:19:09 UTC 2012 i686 i686 i386 GNU/Linux\n"
"In [11]: pd.read_csv('a', dtype=object, index_col=0)\nOut[11]:\n                      A                     B\n1A  0.35633069074776547     0.745585398803751\n1B  0.20037376323337375  0.013921830784260236\n\nIn [12]: pd.read_csv('a', index_col=0)\nOut[12]:\n           A         B\n1A  0.356331  0.745585\n1B  0.200374  0.013922\n\nIn [13]: pd.read_csv('a', converters={i: str for i in range(100)})\nOut[13]:\n                      A                     B\n1A  0.35633069074776547     0.745585398803751\n1B  0.20037376323337375  0.013921830784260236\n"
"In [11]: g = df.groupby('key1')\n\nIn [12]: g['data1'].rank(method='min')\nOut[12]:\n0    1\n1    2\n2    2\n3    1\n4    4\ndtype: float64\n\nIn [13]: df['RN'] = g['data1'].rank(method='min')\n\nIn [14]: g1 = df.groupby(['key1', 'RN'])\n\nIn [15]: g1['data2'].rank(ascending=False) - 1\nOut[15]:\n0    0\n1    0\n2    1\n3    0\n4    0\ndtype: float64\n\nIn [16]: df['RN'] += g1['data2'].rank(ascending=False) - 1\n\nIn [17]: df\nOut[17]:\n   data1  data2 key1  RN\n0      1      1    a   1\n1      2     10    a   2\n2      2      2    a   3\n3      3      3    b   1\n4      3     30    a   4\n"
"df['New_Sample'] = df.Sample.str[:1]\n\nIn [187]:\n\ndf['New_Sample'] = df.Sample.apply(lambda x: x[:1])\ndf\nOut[187]:\n  Sample  Value New_Sample\n0    AAB     23          A\n1    BAB     25          B\n"
"&gt;&gt;&gt; df = pandas.DataFrame(numpy.random.randint(1,3, (10,5)), columns=['a','b','c','d','e'])\n&gt;&gt;&gt; df\n   a  b  c  d  e\n0  2  1  2  1  1\n1  2  1  1  1  1\n2  1  2  2  1  2\n3  1  2  1  1  2\n4  2  2  1  1  2\n5  2  2  2  2  1\n6  2  2  1  1  1\n7  2  2  2  1  1\n8  2  2  2  2  1\n9  2  2  2  2  1\n\n\nfor k, v in df.groupby('a'):\n    print k\n    print 'normal'\n    print v\n    print 'shifted'\n    print v.shift(1)\n\n1\nnormal\n   a  b  c  d  e\n2  1  2  2  1  2\n3  1  2  1  1  2\nshifted\n    a   b   c   d   e\n2 NaN NaN NaN NaN NaN\n3   1   2   2   1   2\n2\nnormal\n   a  b  c  d  e\n0  2  1  2  1  1\n1  2  1  1  1  1\n4  2  2  1  1  2\n5  2  2  2  2  1\n6  2  2  1  1  1\n7  2  2  2  1  1\n8  2  2  2  2  1\n9  2  2  2  2  1\nshifted\n    a   b   c   d   e\n0 NaN NaN NaN NaN NaN\n1   2   1   2   1   1\n4   2   1   1   1   1\n5   2   2   1   1   2\n6   2   2   2   2   1\n7   2   2   1   1   1\n8   2   2   2   1   1\n9   2   2   2   2   1\n"
"for i, d in enumerate([360, 30, 7, 1]):\n    ax = axes.flatten()[i]\n    earlycut = now - relativedelta(days=d)\n    data = df.loc[df.index&gt;=earlycut, :]\n    ax.plot(data.index, data['value'])\n\n    ax.get_xaxis().set_minor_locator(mpl.ticker.AutoMinorLocator())\n    ax.get_yaxis().set_minor_locator(mpl.ticker.AutoMinorLocator())\n\n    ax.grid(b=True, which='major', color='w', linewidth=1.5)\n    ax.grid(b=True, which='minor', color='w', linewidth=0.75)\n\n    plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\n\nfig.tight_layout()\n"
'In [54]:\n\ndf[\'text\']\nOut[54]:\n0    text1\n1    text2\n2    textn\nName: text, dtype: object\n\nIn [53]:\nimport io\ntemp="""id    text\n363.327    text1\n366.356    text2\n37782    textn"""\ndf = pd.read_csv(io.StringIO(temp), delimiter=\'\\s+\', usecols=[\'text\'])\ndf\nOut[53]:\n    text\n0  text1\n1  text2\n2  textn\n'
'func : function\nconvert_dtype : boolean, default True\nTry to find better dtype for elementwise function results. If False, leave as dtype=object\nargs : tuple\nPositional arguments to pass to function in addition to the value\n'
"x.loc[x['A'] == 2, 'B']\n\nIn : x.loc[x['A'] == 2, 'B'].values[0]\nOut: 6\n"
"import numpy as np\n\n&gt;&gt; td / np.timedelta64(1, 'h')\n26.0\n"
'# Enforce UTF-8 encoding\nimport sys\nstdin, stdout = sys.stdin, sys.stdout\nreload(sys)\nsys.stdin, sys.stdout = stdin, stdout\nsys.setdefaultencoding(\'UTF-8\')\n\n# SQLite3 database\nimport sqlite3\n# Pandas: Data structures and data analysis tools\nimport pandas as pd\n\n# Read database, attach as Pandas dataframe\ndb = sqlite3.connect("Applications.db")\ndf = pd.read_sql_query("SELECT path, language, date, shortest_sentence, longest_sentence, number_words, readability_consensus FROM applications ORDER BY date(date) DESC", db)\ndb.close()\ndf.columns = [\'Path\', \'Language\', \'Date\', \'Shortest Sentence\', \'Longest Sentence\', \'Words\', \'Readability Consensus\']\n\n# Parse Dataframe and apply Markdown, then save as \'table.md\'\ncols = df.columns\ndf2 = pd.DataFrame([[\'---\',\'---\',\'---\',\'---\',\'---\',\'---\',\'---\']], columns=cols)\ndf3 = pd.concat([df2, df])\ndf3.to_csv("table.md", sep="|", index=False)\n\n| Path                    | Language | Date       | Shortest Sentence                                                                            | Longest Sentence                                                                                                                                                                                                                                         | Words | Readability Consensus |\n|-------------------------|----------|------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|-----------------------|\n| data/Eng/Something1.txt | Eng      | 2015-09-17 | I am able to relocate to London on short notice.                                             | With my administrative experience in the preparation of the structure and content of seminars in various courses, and critiquing academic papers on various levels, I am confident that I can execute the work required as an editorial assistant.       | 306   | 11th and 12th grade   |\n| data/Nor/NoeNorrønt.txt | Nor      | 2015-09-17 | Jeg har grundig kjennskap til Microsoft Office og Adobe.                                     | I løpet av studiene har jeg vært salgsmedarbeider for et større konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Trønderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 205   | 18th and 19th grade   |\n| data/Nor/Ørret.txt.txt  | Nor      | 2015-09-17 | Jeg håper på positiv tilbakemelding, og møter naturligvis til intervju hvis det er ønskelig. | I løpet av studiene har jeg vært salgsmedarbeider for et større konsern, hvor jeg solgte forsikring til studentene og de faglige ansatte ved universitetet i Trønderlag, samt renholdsarbeider i et annet, hvor jeg i en periode var avdelingsansvarlig. | 160   | 18th and 19th grade   |\n'
"train_df.reset_index(drop=True)\n\nimport pandas as pd\n\n&gt;&gt;&gt; pd.concat([\n    pd.DataFrame({'a': [1, 2]}), \n    pd.DataFrame({'a': [1, 2]})]).reset_index(drop=True)\n    a\n0   1\n1   2\n2   1\n3   2\n"
"&gt;&gt;&gt; df[condition]\n\n&gt;&gt;&gt; df[df.A &gt; 3].iloc[0]\nA    4\nB    6\nC    3\nName: 2, dtype: int64\n\n&gt;&gt;&gt; df[(df.A &gt; 4) &amp; (df.B &gt; 3)].iloc[0]\nA    5\nB    4\nC    5\nName: 4, dtype: int64\n\n&gt;&gt;&gt; df[(df.A &gt; 3) &amp; ((df.B &gt; 3) | (df.C &gt; 2))].iloc[0]\nA    4\nB    6\nC    3\nName: 2, dtype: int64\n\n&gt;&gt;&gt; def series_or_default(X, condition, default_col, ascending=False):\n...     sliced = X[condition]\n...     if sliced.shape[0] == 0:\n...         return X.sort_values(default_col, ascending=ascending).iloc[0]\n...     return sliced.iloc[0]\n&gt;&gt;&gt; \n&gt;&gt;&gt; series_or_default(df, df.A &gt; 6, 'A')\nA    5\nB    4\nC    5\nName: 4, dtype: int64\n"
"ID = pd.to_numeric(ID, errors='coerce')\n\ndf.ID = pd.to_numeric(df.ID, errors='coerce')\n\ndf.ID = pd.to_numeric(df.ID, errors='coerce').fillna(0).astype(np.int64)\n\ndf = pd.DataFrame({'ID':['4806105017087','4806105017087','CN414149']})\nprint (df)\n              ID\n0  4806105017087\n1  4806105017087\n2       CN414149\n\nprint (pd.to_numeric(df.ID, errors='coerce'))\n0    4.806105e+12\n1    4.806105e+12\n2             NaN\nName: ID, dtype: float64\n\ndf.ID = pd.to_numeric(df.ID, errors='coerce').fillna(0).astype(np.int64)\nprint (df)\n              ID\n0  4806105017087\n1  4806105017087\n2              0\n\ndf.ID = pd.to_numeric(df.ID, errors='coerce').astype('Int64')\nprint (df)\n              ID\n0  4806105017087\n1  4806105017087\n2            NaN\n"
"df['Normalized'] = np.where(df['Currency'] == '$', df['Budget'] * 0.78125, df['Budget'])\n"
'scipy.sparse.csr_matrix(df.values)\n'
'@app.route(\'/analysis/&lt;filename&gt;\')\ndef analysis(filename):\n    x = pd.DataFrame(np.random.randn(20, 5))\n    return render_template("analysis.html", name=filename, data=x.to_html())\n                                                                # ^^^^^^^^^\n'
"plt.plot(range(10), label='Some very long label')\nplt.plot(range(1,11), label='Short label')\nL=plt.legend()\nL.get_texts()[0].set_text('make it short')\nplt.savefig('temp.png')\n"
"In [4]: df.groupby(['c1', 'c2']).cumcount()\nOut[4]: \n0     0\n1     1\n2     0\n3     1\n4     0\n5     1\n6     2\n7     0\n8     0\n9     0\n10    1\n11    2\ndtype: int64\n\nIn [5]: df.groupby(['c1', 'c2']).cumcount()+1\nOut[5]: \n0     1\n1     2\n2     1\n3     2\n4     1\n5     2\n6     3\n7     1\n8     1\n9     1\n10    2\n11    3\ndtype: int64\n"
"In [116]: gb.agg({'sum_col' : np.sum,\n     ...:         'date' : [np.min, np.max]})\nOut[116]: \n                      date             sum_col\n                      amin       amax      sum\ntype weekofyear                               \nA    25         2014-06-22 2014-06-22        1\n     26         2014-06-25 2014-06-25        1\n     27         2014-07-05 2014-07-05        2\nB    26         2014-06-24 2014-06-24        2\n     27         2014-07-02 2014-07-02        1\nC    26         2014-06-25 2014-06-25        3\n     27         2014-07-06 2014-07-06        3\n     30         2014-07-27 2014-07-27        1\n"
'df.stack().std()         # pandas default degrees of freedom is one\n\ndf.values.std(ddof=1)    # numpy default degrees of freedom is zero\n'
"In [26]:\n\ndf.merge(df1, on='sku', how='left')\nOut[26]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  122   62   True    b\n2  122   63  False    b\n3  123   61   True    b\n4  123   62  False    b\n5  113   62   True    a\n6  301   63   True    c\n\nIn [28]:\n\ndf.merge(df1, left_index=True, right_index=True, how='left')\nOut[28]:\n     loc   flag dept\nsku                 \n113   62   True    a\n122   61   True    b\n122   62   True    b\n122   63  False    b\n123   61   True    b\n123   62  False    b\n301   63   True    c\n\nIn [19]:\n\ndf['dept']=df.sku.map(df1.dept)\ndf\nOut[19]:\n   sku  loc   flag dept\n0  122   61   True    b\n1  123   61   True    b\n2  113   62   True    a\n3  122   62   True    b\n4  123   62  False    b\n5  122   63  False    b\n6  301   63   True    c\n"
"dtype : dtype, default None\n\n    Data type to force. Only a single dtype is allowed. If None, infer\n\n&gt;&gt;&gt; myarray = np.random.randint(0,5,size=(2,2))\n&gt;&gt;&gt; record = np.array(map(tuple,myarray),dtype=[('a',np.float),('b',np.int)])\n&gt;&gt;&gt; mydf = pd.DataFrame.from_records(record)\n&gt;&gt;&gt; mydf.dtypes\na    float64\nb      int64\ndtype: object\n"
'mask = np.column_stack([df[col].str.contains(r"\\^", na=False) for col in df])\ndf.loc[mask.any(axis=1)]\n'
'&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; import json\n&gt;&gt;&gt; json.dumps(numpy.int32(685))\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/usr/lib/python2.7/json/__init__.py", line 243, in dumps\n    return _default_encoder.encode(obj)\n  File "/usr/lib/python2.7/json/encoder.py", line 207, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File "/usr/lib/python2.7/json/encoder.py", line 270, in iterencode\n    return _iterencode(o, 0)\n  File "/usr/lib/python2.7/json/encoder.py", line 184, in default\n    raise TypeError(repr(o) + " is not JSON serializable")\nTypeError: 685 is not JSON serializable\n\nclass MyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, numpy.integer):\n            return int(obj)\n        elif isinstance(obj, numpy.floating):\n            return float(obj)\n        elif isinstance(obj, numpy.ndarray):\n            return obj.tolist()\n        else:\n            return super(MyEncoder, self).default(obj)\n\njson.dumps(numpy.float32(1.2), cls=MyEncoder)\njson.dumps(numpy.arange(12), cls=MyEncoder)\njson.dumps({\'a\': numpy.int32(42)}, cls=MyEncoder)\n'
'&gt;&gt;&gt; df["TimeReviewed"] = pd.to_datetime(df["TimeReviewed"])\n&gt;&gt;&gt; df["TimeReviewed"]\n205  76032930   2015-01-24 00:05:27.513000\n232  76032930   2015-01-24 00:06:46.703000\n233  76032930   2015-01-24 00:06:56.707000\n413  76032930   2015-01-24 00:14:24.957000\n565  76032930   2015-01-24 00:23:07.220000\nName: TimeReviewed, dtype: datetime64[ns]\n&gt;&gt;&gt; df["TimeReviewed"].dt\n&lt;pandas.tseries.common.DatetimeProperties object at 0xb10da60c&gt;\n&gt;&gt;&gt; df["TimeReviewed"].dt.year\n205  76032930    2015\n232  76032930    2015\n233  76032930    2015\n413  76032930    2015\n565  76032930    2015\ndtype: int64\n&gt;&gt;&gt; df["TimeReviewed"].dt.month\n205  76032930    1\n232  76032930    1\n233  76032930    1\n413  76032930    1\n565  76032930    1\ndtype: int64\n&gt;&gt;&gt; df["TimeReviewed"].dt.minute\n205  76032930     5\n232  76032930     6\n233  76032930     6\n413  76032930    14\n565  76032930    23\ndtype: int64\n\n&gt;&gt;&gt; df["TimeReviewed"].apply(lambda x: x.year)\n205  76032930    2015\n232  76032930    2015\n233  76032930    2015\n413  76032930    2015\n565  76032930    2015\nName: TimeReviewed, dtype: int64\n'
"df.index = df.index.get_level_values('first')\n\ndf.index = df.index.get_level_values(0)\n"
"&gt;&gt;&gt; df['dt2'] = df['dt'].dt.floor('h')\n&gt;&gt;&gt; df\n                      dt                     dt2\n0    2014-10-01 10:02:45     2014-10-01 10:00:00\n1    2014-10-01 13:08:17     2014-10-01 13:00:00\n2    2014-10-01 17:39:24     2014-10-01 17:00:00\n\ndf['dt'].values.astype('&lt;M8[h]')\n\n&gt;&gt;&gt; df\n                       dt\n0     2014-10-01 10:02:45\n1     2014-10-01 13:08:17\n2     2014-10-01 17:39:24\n\n&gt;&gt;&gt; df['dt2'] = df['dt'].values.astype('&lt;M8[h]')\n&gt;&gt;&gt; df\n                      dt                     dt2\n0    2014-10-01 10:02:45     2014-10-01 10:00:00\n1    2014-10-01 13:08:17     2014-10-01 13:00:00\n2    2014-10-01 17:39:24     2014-10-01 17:00:00\n\n&gt;&gt;&gt; df.dtypes\ndt     datetime64[ns]\ndt2    datetime64[ns]\n"
"dist_df = dist_df.reset_index(level=[0,1])\n\nIn [28]:\ndf.reset_index(level=[0,1])\n\nOut[28]:\n            YEAR  MONTH  NI\ndatetime                     \n2000-01-01  2000      1   NaN\n2000-01-02  2000      1   NaN\n2000-01-03  2000      1   NaN\n2000-01-04  2000      1   NaN\n2000-01-05  2000      1   NaN\n\ndf.reset_index(level=['YEAR','MONTH'])\n"
"out = df.to_json(orient='records')[1:-1].replace('},{', '} {')\n\nwith open('file_name.txt', 'w') as f:\n    f.write(out)\n"
'import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndf = sns.load_dataset("tips")\ngroupedvalues=df.groupby(\'day\').sum().reset_index()\n\npal = sns.color_palette("Greens_d", len(groupedvalues))\nrank = groupedvalues["total_bill"].argsort().argsort() \ng=sns.barplot(x=\'day\',y=\'tip\',data=groupedvalues, palette=np.array(pal[::-1])[rank])\n\nfor index, row in groupedvalues.iterrows():\n    g.text(row.name,row.tip, round(row.total_bill,2), color=\'black\', ha="center")\n\nplt.show()\n\nrank = groupedvalues[\'total_bill\'].rank(ascending=True).values\nrank = (rank-1).astype(np.int)\n'
'tensor([ 0.5827,  0.5881,  0.1543,  0.6815,  0.9400,  0.8683,  0.4289,\n         0.5940,  0.6438,  0.7514], dtype=torch.float64)\n'
'moving_avg = pd.rolling_mean(ts_log,12)\n\nmoving_avg = ts_log.rolling(12).mean()\n'
'some_df = sc.parallelize([\n ("A", "no"),\n ("B", "yes"),\n ("B", "yes"),\n ("B", "no")]\n ).toDF(["user_id", "phone_number"])\npandas_df = some_df.toPandas()\n'
'isnull = isna\n\n&gt;&gt;&gt; pd.isnull\n&lt;function isna at 0x7fb4c5cefc80&gt;\n'
"In [31]: frame.resample('1H').agg({'radiation': np.sum, 'tamb': np.mean})\nOut[31]: \n                         tamb   radiation\n2012-04-05 08:00:00  5.161235  279.507182\n2012-04-05 09:00:00  4.968145  290.941073\n2012-04-05 10:00:00  4.478531  317.678285\n2012-04-05 11:00:00  4.706206  335.258633\n2012-04-05 12:00:00  2.457873    8.655838\n\nIn [30]: frame.resample('1H', how={'radiation': np.sum, 'tamb': np.mean})\nOut[30]: \n                         tamb   radiation\n2012-04-05 08:00:00  5.161235  279.507182\n2012-04-05 09:00:00  4.968145  290.941073\n2012-04-05 10:00:00  4.478531  317.678285\n2012-04-05 11:00:00  4.706206  335.258633\n2012-04-05 12:00:00  2.457873    8.655838\n"
"import pandas\nA = pandas.DataFrame({\n    'val' :  ['aaaaa', 'acaca', 'ddddd', 'zzzzz'],\n    'extra' : range(10,14),\n})\nA = A.reset_index(drop=True)\nA = A.reset_index(drop=True)\nA = A.reset_index(drop=True)\n"
"df.plot(kind='bar', stacked=True, width=1)\n"
"In [10]: table = pivot_table(df, values=['SalesToday', 'SalesMTD','SalesYTD'],\\\n                     rows=['State'], cols=['City'], aggfunc=np.sum, margins=True)\n\n\nIn [11]: table.stack('City')\nOut[11]: \n            SalesMTD  SalesToday  SalesYTD\nState City                                \nstA   All        900          50      2100\n      ctA        400          20      1000\n      ctB        500          30      1100\nstB   All        700          50      2200\n      ctC        500          10       900\n      ctD        200          40      1300\nstC   All        300          30       800\n      ctF        300          30       800\nAll   All       1900         130      5100\n      ctA        400          20      1000\n      ctB        500          30      1100\n      ctC        500          10       900\n      ctD        200          40      1300\n      ctF        300          30       800\n"
"with open(file_name, 'a') as f:\n    df.to_csv(f, header=False)\n\ndf.to_csv(f, mode='a', header=False)\n"
"df['text'].str.lower().str.split()\nOut[43]: \n0             [my, nickname, is, ft.jgt]\n1    [someone, is, going, to, my, place]\n\nresults = set()\ndf['text'].str.lower().str.split().apply(results.update)\nprint(results)\n\nset(['someone', 'ft.jgt', 'my', 'is', 'to', 'going', 'place', 'nickname'])\n\nfrom collections import Counter\nresults = Counter()\ndf['text'].str.lower().str.split().apply(results.update)\nprint(results)\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame([[1,2],[3,4]], columns=list('ab'))\n&gt;&gt;&gt; df\n   a  b\n0  1  2\n1  3  4\n&gt;&gt;&gt; df['c'] = df['b']**2\n&gt;&gt;&gt; df\n   a  b   c\n0  1  2   4\n1  3  4  16\n"
"In [11]: df\nOut[11]:\n    a    b\nA  14  103\nB  90  107\nC  90  110\nD  96  114\nE  91  114\n\nIn [12]: df -= df.min()  # equivalent to df = df - df.min()\n\nIn [13]: df /= df.max()  # equivalent to df = df / df.max()\n\nIn [14]: df\nOut[14]:\n          a         b\nA  0.000000  0.000000\nB  0.926829  0.363636\nC  0.926829  0.636364\nD  1.000000  1.000000\nE  0.939024  1.000000\n\nIn [15]: df['b'] = 1 - df['b']\n"
"#libs required\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\n\n#generate ramdom data with same seed (to be reproducible)\nnp.random.seed(seed=1)\ndf = pd.DataFrame(np.random.uniform(0,1,(10)), columns=['a'])\n\n#quantile function\nx = df.quantile(0.5)[0]\n\n#inverse of quantile\nstats.percentileofscore(df['a'],x)\n"
'&gt;&gt;&gt; df1.loc[(df1[list(filter_v)] == pd.Series(filter_v)).all(axis=1)]\n   A  B      C  D\n3  1  0  right  3\n\n&gt;&gt;&gt; pd.Series(filter_v)\nA        1\nB        0\nC    right\ndtype: object\n\n&gt;&gt;&gt; df1[list(filter_v)]\n    A      C  B\n0   1  right  1\n1   0  right  1\n2   1  wrong  1\n3   1  right  0\n4 NaN  right  1\n\n&gt;&gt;&gt; df1[list(filter_v)] == pd.Series(filter_v)\n       A      B      C\n0   True  False   True\n1  False  False   True\n2   True  False  False\n3   True   True   True\n4  False  False   True\n\n&gt;&gt;&gt; (df1[list(filter_v)] == pd.Series(filter_v)).all(axis=1)\n0    False\n1    False\n2    False\n3     True\n4    False\ndtype: bool\n\n&gt;&gt;&gt; df1.loc[(df1[list(filter_v)] == pd.Series(filter_v)).all(axis=1)]\n   A  B      C  D\n3  1  0  right  3\n'
"df['NewCol'] = df.apply(lambda x: segmentMatch(x['TimeCol'], x['ResponseCol']), axis=1)\n"
'                  read_s  write_s  size_ratio_to_CSV\nstorage\nCSV               17.900    69.00              1.000\nCSV.gzip          18.900   186.00              0.047\nPickle             0.173     1.77              0.374\nHDF_fixed          0.196     2.03              0.435\nHDF_tab            0.230     2.60              0.437\nHDF_tab_zlib_c5    0.845     5.44              0.035\nHDF_tab_zlib_c9    0.860     5.95              0.035\nHDF_tab_bzip2_c5   2.500    36.50              0.011\nHDF_tab_bzip2_c9   2.500    36.50              0.011\n'
"df1['Avg_Annual'] = df1['Avg_Annual'].str.replace(',', '')\ndf1['Avg_Annual'] = df1['Avg_Annual'].str.replace('$', '')\ndf1['Avg_Annual'] = df1['Avg_Annual'].astype(int)\n\ndf1['Avg_Annual'] = df1['Avg_Annual'].str.replace(',', '').str.replace('$', '').astype(int)\n"
"df %&gt;% groupby(col1) %&gt;% summarize(col2_agg=max(col2), col3_agg=min(col3))\n\ndf.groupby('col1').agg({'col2': 'max', 'col3': 'min'})\n\n      col2  col3\ncol1            \n1        5    -5\n2        9    -9\n\ndf.groupby('col1', as_index=False).agg({'col2': 'max', 'col3': 'min'})\n\ndf.groupby('col1').agg({'col2': 'max', 'col3': 'min'}).reset_index()\n\ncol1  col2  col3           \n   1     5    -5\n   2     9    -9\n\nagg_df = df.groupby('col1').agg({'col2': ['max', 'min', 'std'], \n                                 'col3': ['size', 'std', 'mean', 'max']})\n\n     col2               col3                   \n      max min       std size       std mean max\ncol1                                           \n1       5   1  1.581139    5  1.581139   -3  -1\n2       9   0  3.535534    5  3.535534   -6   0\n\nagg_df['col2']  # select the second column\n      max  min       std\ncol1                    \n1       5    1  1.581139\n2       9    0  3.535534\n\nagg_df[('col2', 'max')]  # select the maximum of the second column\nOut: \ncol1\n1    5\n2    9\nName: (col2, max), dtype: int64\n\nagg_df.xs('max', axis=1, level=1)  # select the maximum of all columns\nOut: \n      col2  col3\ncol1            \n1        5    -1\n2        9     0\n\ndf.groupby('col1')['col2'].agg({'max_col2': 'max'})\n\n      max_col2\ncol1          \n1            5\n2            9\n\ndf.groupby('col1')['col2'].agg(['max']).rename(columns={'max': 'col2_max'})\n\n      col2_max\ncol1          \n1            5\n2            9\n\nagg_df.columns = ['_'.join(col) for col in agg_df.columns]\n\n      col2_max  col2_min  col2_std  col3_size  col3_std  col3_mean  col3_max\ncol1                                                                        \n1            5         1  1.581139          5  1.581139         -3        -1\n2            9         0  3.535534          5  3.535534         -6         0\n\ndf.assign(new_col=df.eval('col2 * col3')).groupby('col1').agg('max') \n\n      col2  col3  new_col\ncol1                     \n1        5    -1       -1\n2        9     0        0\n\ndf.assign(new_col=df.eval('col2 * col3')).groupby('col1')['new_col'].agg('max')\n\ncol1\n1   -1\n2    0\nName: new_col, dtype: int64\n\ndf.groupby('col1').apply(lambda x: (x.col2 * x.col3).max())\n\ncol1\n1   -1\n2    0\ndtype: int64\n"
'# tested with python3 &amp; numpy 1.15.2\nimport numpy as np\n\ndef ewma_vectorized_safe(data, alpha, row_size=None, dtype=None, order=\'C\', out=None):\n    """\n    Reshapes data before calculating EWMA, then iterates once over the rows\n    to calculate the offset without precision issues\n    :param data: Input data, will be flattened.\n    :param alpha: scalar float in range (0,1)\n        The alpha parameter for the moving average.\n    :param row_size: int, optional\n        The row size to use in the computation. High row sizes need higher precision,\n        low values will impact performance. The optimal value depends on the\n        platform and the alpha being used. Higher alpha values require lower\n        row size. Default depends on dtype.\n    :param dtype: optional\n        Data type used for calculations. Defaults to float64 unless\n        data.dtype is float32, then it will use float32.\n    :param order: {\'C\', \'F\', \'A\'}, optional\n        Order to use when flattening the data. Defaults to \'C\'.\n    :param out: ndarray, or None, optional\n        A location into which the result is stored. If provided, it must have\n        the same shape as the desired output. If not provided or `None`,\n        a freshly-allocated array is returned.\n    :return: The flattened result.\n    """\n    data = np.array(data, copy=False)\n\n    if dtype is None:\n        if data.dtype == np.float32:\n            dtype = np.float32\n        else:\n            dtype = np.float\n    else:\n        dtype = np.dtype(dtype)\n\n    row_size = int(row_size) if row_size is not None \n               else get_max_row_size(alpha, dtype)\n\n    if data.size &lt;= row_size:\n        # The normal function can handle this input, use that\n        return ewma_vectorized(data, alpha, dtype=dtype, order=order, out=out)\n\n    if data.ndim &gt; 1:\n        # flatten input\n        data = np.reshape(data, -1, order=order)\n\n    if out is None:\n        out = np.empty_like(data, dtype=dtype)\n    else:\n        assert out.shape == data.shape\n        assert out.dtype == dtype\n\n    row_n = int(data.size // row_size)  # the number of rows to use\n    trailing_n = int(data.size % row_size)  # the amount of data leftover\n    first_offset = data[0]\n\n    if trailing_n &gt; 0:\n        # set temporary results to slice view of out parameter\n        out_main_view = np.reshape(out[:-trailing_n], (row_n, row_size))\n        data_main_view = np.reshape(data[:-trailing_n], (row_n, row_size))\n    else:\n        out_main_view = out\n        data_main_view = data\n\n    # get all the scaled cumulative sums with 0 offset\n    ewma_vectorized_2d(data_main_view, alpha, axis=1, offset=0, dtype=dtype,\n                       order=\'C\', out=out_main_view)\n\n    scaling_factors = (1 - alpha) ** np.arange(1, row_size + 1)\n    last_scaling_factor = scaling_factors[-1]\n\n    # create offset array\n    offsets = np.empty(out_main_view.shape[0], dtype=dtype)\n    offsets[0] = first_offset\n    # iteratively calculate offset for each row\n    for i in range(1, out_main_view.shape[0]):\n        offsets[i] = offsets[i - 1] * last_scaling_factor + out_main_view[i - 1, -1]\n\n    # add the offsets to the result\n    out_main_view += offsets[:, np.newaxis] * scaling_factors[np.newaxis, :]\n\n    if trailing_n &gt; 0:\n        # process trailing data in the 2nd slice of the out parameter\n        ewma_vectorized(data[-trailing_n:], alpha, offset=out_main_view[-1, -1],\n                        dtype=dtype, order=\'C\', out=out[-trailing_n:])\n    return out\n\ndef get_max_row_size(alpha, dtype=float):\n    assert 0. &lt;= alpha &lt; 1.\n    # This will return the maximum row size possible on \n    # your platform for the given dtype. I can find no impact on accuracy\n    # at this value on my machine.\n    # Might not be the optimal value for speed, which is hard to predict\n    # due to numpy\'s optimizations\n    # Use np.finfo(dtype).eps if you  are worried about accuracy\n    # and want to be extra safe.\n    epsilon = np.finfo(dtype).tiny\n    # If this produces an OverflowError, make epsilon larger\n    return int(np.log(epsilon)/np.log(1-alpha)) + 1\n\ndef ewma_vectorized(data, alpha, offset=None, dtype=None, order=\'C\', out=None):\n    """\n    Calculates the exponential moving average over a vector.\n    Will fail for large inputs.\n    :param data: Input data\n    :param alpha: scalar float in range (0,1)\n        The alpha parameter for the moving average.\n    :param offset: optional\n        The offset for the moving average, scalar. Defaults to data[0].\n    :param dtype: optional\n        Data type used for calculations. Defaults to float64 unless\n        data.dtype is float32, then it will use float32.\n    :param order: {\'C\', \'F\', \'A\'}, optional\n        Order to use when flattening the data. Defaults to \'C\'.\n    :param out: ndarray, or None, optional\n        A location into which the result is stored. If provided, it must have\n        the same shape as the input. If not provided or `None`,\n        a freshly-allocated array is returned.\n    """\n    data = np.array(data, copy=False)\n\n    if dtype is None:\n        if data.dtype == np.float32:\n            dtype = np.float32\n        else:\n            dtype = np.float64\n    else:\n        dtype = np.dtype(dtype)\n\n    if data.ndim &gt; 1:\n        # flatten input\n        data = data.reshape(-1, order)\n\n    if out is None:\n        out = np.empty_like(data, dtype=dtype)\n    else:\n        assert out.shape == data.shape\n        assert out.dtype == dtype\n\n    if data.size &lt; 1:\n        # empty input, return empty array\n        return out\n\n    if offset is None:\n        offset = data[0]\n\n    alpha = np.array(alpha, copy=False).astype(dtype, copy=False)\n\n    # scaling_factors -&gt; 0 as len(data) gets large\n    # this leads to divide-by-zeros below\n    scaling_factors = np.power(1. - alpha, np.arange(data.size + 1, dtype=dtype),\n                               dtype=dtype)\n    # create cumulative sum array\n    np.multiply(data, (alpha * scaling_factors[-2]) / scaling_factors[:-1],\n                dtype=dtype, out=out)\n    np.cumsum(out, dtype=dtype, out=out)\n\n    # cumsums / scaling\n    out /= scaling_factors[-2::-1]\n\n    if offset != 0:\n        offset = np.array(offset, copy=False).astype(dtype, copy=False)\n        # add offsets\n        out += offset * scaling_factors[1:]\n\n    return out\n\ndef ewma_vectorized_2d(data, alpha, axis=None, offset=None, dtype=None, order=\'C\', out=None):\n    """\n    Calculates the exponential moving average over a given axis.\n    :param data: Input data, must be 1D or 2D array.\n    :param alpha: scalar float in range (0,1)\n        The alpha parameter for the moving average.\n    :param axis: The axis to apply the moving average on.\n        If axis==None, the data is flattened.\n    :param offset: optional\n        The offset for the moving average. Must be scalar or a\n        vector with one element for each row of data. If set to None,\n        defaults to the first value of each row.\n    :param dtype: optional\n        Data type used for calculations. Defaults to float64 unless\n        data.dtype is float32, then it will use float32.\n    :param order: {\'C\', \'F\', \'A\'}, optional\n        Order to use when flattening the data. Ignored if axis is not None.\n    :param out: ndarray, or None, optional\n        A location into which the result is stored. If provided, it must have\n        the same shape as the desired output. If not provided or `None`,\n        a freshly-allocated array is returned.\n    """\n    data = np.array(data, copy=False)\n\n    assert data.ndim &lt;= 2\n\n    if dtype is None:\n        if data.dtype == np.float32:\n            dtype = np.float32\n        else:\n            dtype = np.float64\n    else:\n        dtype = np.dtype(dtype)\n\n    if out is None:\n        out = np.empty_like(data, dtype=dtype)\n    else:\n        assert out.shape == data.shape\n        assert out.dtype == dtype\n\n    if data.size &lt; 1:\n        # empty input, return empty array\n        return out\n\n    if axis is None or data.ndim &lt; 2:\n        # use 1D version\n        if isinstance(offset, np.ndarray):\n            offset = offset[0]\n        return ewma_vectorized(data, alpha, offset, dtype=dtype, order=order,\n                               out=out)\n\n    assert -data.ndim &lt;= axis &lt; data.ndim\n\n    # create reshaped data views\n    out_view = out\n    if axis &lt; 0:\n        axis = data.ndim - int(axis)\n\n    if axis == 0:\n        # transpose data views so columns are treated as rows\n        data = data.T\n        out_view = out_view.T\n\n    if offset is None:\n        # use the first element of each row as the offset\n        offset = np.copy(data[:, 0])\n    elif np.size(offset) == 1:\n        offset = np.reshape(offset, (1,))\n\n    alpha = np.array(alpha, copy=False).astype(dtype, copy=False)\n\n    # calculate the moving average\n    row_size = data.shape[1]\n    row_n = data.shape[0]\n    scaling_factors = np.power(1. - alpha, np.arange(row_size + 1, dtype=dtype),\n                               dtype=dtype)\n    # create a scaled cumulative sum array\n    np.multiply(\n        data,\n        np.multiply(alpha * scaling_factors[-2], np.ones((row_n, 1), dtype=dtype),\n                    dtype=dtype)\n        / scaling_factors[np.newaxis, :-1],\n        dtype=dtype, out=out_view\n    )\n    np.cumsum(out_view, axis=1, dtype=dtype, out=out_view)\n    out_view /= scaling_factors[np.newaxis, -2::-1]\n\n    if not (np.size(offset) == 1 and offset == 0):\n        offset = offset.astype(dtype, copy=False)\n        # add the offsets to the scaled cumulative sums\n        out_view += offset[:, np.newaxis] * scaling_factors[np.newaxis, 1:]\n\n    return out\n\ndata_n = 100000000\ndata = ((0.5*np.random.randn(data_n)+0.5) % 1) * 100\n\nspan = 5000  # span &gt;= 1\nalpha = 2/(span+1)  # for pandas` span parameter\n\n# com = 1000  # com &gt;= 0\n# alpha = 1/(1+com)  # for pandas` center-of-mass parameter\n\n# halflife = 100  # halflife &gt; 0\n# alpha = 1 - np.exp(np.log(0.5)/halflife)  # for pandas` half-life parameter\n\nresult = ewma_vectorized_safe(data, alpha)\n\ndef window_size(alpha, sum_proportion):\n    # Increases with increased sum_proportion and decreased alpha\n    # solve (1-alpha)**window_size = (1-sum_proportion) for window_size        \n    return int(np.log(1-sum_proportion) / np.log(1-alpha))\n\nalpha = 0.02\nsum_proportion = .99  # window covers 99% of contribution to the moving average\nwindow = window_size(alpha, sum_proportion)  # = 227\nsum_proportion = .75  # window covers 75% of contribution to the moving average\nwindow = window_size(alpha, sum_proportion)  # = 68\n\nresult = ewma_vectorized_safe(data, alpha, chunk_size)\nsum_proportion = .99\ncutoff_idx = window_size(alpha, sum_proportion)\nresult = result[cutoff_idx:]\n\ndata_n = 100000\ndata = np.random.rand(data_n) * 100\nwindow = 1000\nsum_proportion = .99\nalpha = 1 - np.exp(np.log(1-sum_proportion)/window)\n\nresult = ewma_vectorized_safe(data, alpha)\n\ncutoff_idx = window_size(alpha, sum_proportion)\nx = np.arange(start=0, stop=result.size)\n\nimport matplotlib.pyplot as plt\nplt.plot(x[:cutoff_idx+1], result[:cutoff_idx+1], \'-r\',\n         x[cutoff_idx:], result[cutoff_idx:], \'-b\')\nplt.show()\n'
"for row in df.itertuples():\n    print(row.A)\n    print(row.Index)\n\ndf = pd.DataFrame([x for x in range(1000*1000)], columns=['A'])\nst=time.time()\nfor index, row in df.iterrows():\n    row.A\nprint(time.time()-st)\n45.05799984931946\n\nst=time.time()\nfor row in df.itertuples():\n    row.A\nprint(time.time() - st)\n0.48400020599365234\n"
'rowIndex = df.index[someRowNumber]\n\ndf.loc[rowIndex, \'New Column Title\'] = "some value"\n\ndf.loc[df.index[someRowNumber], \'New Column Title\'] = "some value"\n'
"def pp(start, end, n):\n    start_u = start.value//10**9\n    end_u = end.value//10**9\n\n    return pd.DatetimeIndex((10**9*np.random.randint(start_u, end_u, n, dtype=np.int64)).view('M8[ns]'))\n"
"for elem in dfrm['Category'].unique():\n    dfrm[str(elem)] = dfrm['Category'] == elem\n\ncat_names = {1:'Some_Treatment', 2:'Full_Treatment', 3:'Control'}\nfor elem in dfrm['Category'].unique():\n    dfrm[cat_names[elem]] = dfrm['Category'] == elem\n"
"def calculate(s):\n    a = s['path'] + 2*s['row'] # Simple calc for example\n    b = s['path'] * 0.153\n    return pd.Series(dict(col1=a, col2=b))\n\nst.ix[i]['a'] = a\n\nst.ix[i, 'a'] = a\n"
"import warnings\nwarnings.filterwarnings('error')\n\nwarnings.filterwarnings('error', category=UnicodeWarning)\nwarnings.filterwarnings('error', message='*equal comparison failed*')\n"
'&gt;&gt;&gt; import numpy as np, pandas as pd\n&gt;&gt;&gt; import io, pkgutil\n&gt;&gt;&gt; wells = pkgutil.get_data(\'pymc.examples\', \'data/wells.dat\')\n&gt;&gt;&gt; type(wells)\n&lt;class \'bytes\'&gt;\n&gt;&gt;&gt; df = pd.read_csv(io.BytesIO(wells), encoding=\'utf8\', sep=" ", index_col="id", dtype={"switch": np.int8})\n&gt;&gt;&gt; df.head()\n    switch  arsenic       dist  assoc  educ\nid                                         \n1        1     2.36  16.826000      0     0\n2        1     0.71  47.321999      0     0\n3        0     2.07  20.966999      0    10\n4        1     1.15  21.486000      0    12\n5        1     1.10  40.874001      1    14\n\n[5 rows x 5 columns]\n'
'X = sm.add_constant(X)\nsm.OLS(y,X)\n'
'df = (df1-df2).dropna().copy()\n'
"DF.dtypes\n\n[key for key in dict(DF.dtypes) if dict(DF.dtypes)[key] in ['float64', 'int64']]\n"
'df_new1, df_new2 = df[:10, :], df[10:, :] if len(df) &gt; 10 else df, None\n'
'In [11]: g = df.groupby(\'Date\')\n\nIn [12]: df.value / g.value.transform("sum") * df.wt\nOut[12]:\n0    0.125000\n1    0.250000\n2    0.416667\n3    0.277778\n4    0.444444\ndtype: float64\n\nIn [13]: df[\'wa\'] = df.value / g.value.transform("sum") * df.wt\n\nIn [14]: g.wa.sum()\nOut[14]:\nDate\n01/01/2012    0.791667\n01/02/2012    0.722222\nName: wa, dtype: float64\n\nIn [15]: g.wa.transform("sum")\nOut[15]:\n0    0.791667\n1    0.791667\n2    0.791667\n3    0.722222\n4    0.722222\nName: wa, dtype: float64\n'
'ax = df[[\'V1\',\'V2\']].plot(kind=\'bar\', title ="V comp", figsize=(15, 10), legend=True, fontsize=12)\n\nimport matplotlib.pyplot as plt\nax = df[[\'V1\',\'V2\']].plot(kind=\'bar\', title ="V comp", figsize=(15, 10), legend=True, fontsize=12)\nax.set_xlabel("Hour", fontsize=12)\nax.set_ylabel("V", fontsize=12)\nplt.show()\n'
"In [5]:\ndf['UNIXTIME'] = pd.to_datetime(df['UNIXTIME'], unit='ms')\ndf\n\nOut[5]:\n   RUN                UNIXTIME  VALUE\n0    1 2015-11-10 13:05:02.320     10\n1    2 2015-11-10 13:05:02.364     20\n2    3 2015-11-10 13:05:22.364     42\n"
"df = df.groupby('Name').agg({'Sid':'first', \n                             'Use_Case': ', '.join, \n                             'Revenue':'first' }).reset_index()\n\n#change column order                           \nprint df[['Name','Sid','Use_Case','Revenue']]                              \n  Name   Sid           Use_Case Revenue\n0    A  xx01         Voice, SMS  $10.00\n1    B  xx02              Voice   $5.00\n2    C  xx03  Voice, SMS, Video  $15.00\n\ndf = df.groupby(['Name','Sid','Revenue'])['Use_Case'].apply(', '.join).reset_index()\n\n#change column order                           \nprint df[['Name','Sid','Use_Case','Revenue']]                              \n  Name   Sid           Use_Case Revenue\n0    A  xx01         Voice, SMS  $10.00\n1    B  xx02              Voice   $5.00\n2    C  xx03  Voice, SMS, Video  $15.00\n"
"In [71]:\ndf.loc[df['sport'].str.contains('ball'), 'sport'] = 'ball sport'\ndf\n\nOut[71]:\n    name       sport\n0    Bob      tennis\n1   Jane  ball sport\n2  Alice  ball sport\n\ndf.loc[df['sport'].str.contains('ball', case=False), 'sport'] = 'ball sport'\n"
"import pandas as pd\n\n# Example DataFrame\ndf = pd.DataFrame.from_dict({'Name'  : ['May21', 'James', 'Adi22', 'Hello', 'Girl90'],\n                             'Volume': [23, 12, 11, 34, 56],\n                             'Value' : [21321, 12311, 4435, 32454, 654654]})\n\ndf['Name'] = df['Name'].str.replace('\\d+', '')\n\nprint(df)\n\n    Name   Value  Volume\n0    May   21321      23\n1  James   12311      12\n2    Adi    4435      11\n3  Hello   32454      34\n4   Girl  654654      56\n"
"df['month'] = df['purchase_date'].values.astype('datetime64[M]')\nprint (df)\n   user_id       purchase_date      month\n0        1 2015-01-23 14:05:21 2015-01-01\n1        2 2015-02-05 05:07:30 2015-02-01\n2        3 2015-02-18 17:08:51 2015-02-01\n3        4 2015-03-21 17:07:30 2015-03-01\n4        5 2015-03-11 18:32:56 2015-03-01\n5        6 2015-03-03 11:02:30 2015-03-01\n\ndf['month'] = df['purchase_date'].dt.floor('d') - pd.offsets.MonthBegin(1)\nprint (df)\n   user_id       purchase_date      month\n0        1 2015-01-23 14:05:21 2015-01-01\n1        2 2015-02-05 05:07:30 2015-02-01\n2        3 2015-02-18 17:08:51 2015-02-01\n3        4 2015-03-21 17:07:30 2015-03-01\n4        5 2015-03-11 18:32:56 2015-03-01\n5        6 2015-03-03 11:02:30 2015-03-01\n\ndf['month'] = (df['purchase_date'] - pd.offsets.MonthBegin(1)).dt.floor('d')\nprint (df)\n   user_id       purchase_date      month\n0        1 2015-01-23 14:05:21 2015-01-01\n1        2 2015-02-05 05:07:30 2015-02-01\n2        3 2015-02-18 17:08:51 2015-02-01\n3        4 2015-03-21 17:07:30 2015-03-01\n4        5 2015-03-11 18:32:56 2015-03-01\n5        6 2015-03-03 11:02:30 2015-03-01\n\ndf['month'] = df['purchase_date'].dt.to_period('M')\nprint (df)\n   user_id       purchase_date   month\n0        1 2015-01-23 14:05:21 2015-01\n1        2 2015-02-05 05:07:30 2015-02\n2        3 2015-02-18 17:08:51 2015-02\n3        4 2015-03-21 17:07:30 2015-03\n4        5 2015-03-11 18:32:56 2015-03\n5        6 2015-03-03 11:02:30 2015-03\n\ndf['month'] = df['purchase_date'].dt.to_period('M').dt.to_timestamp()\nprint (df)\n   user_id       purchase_date      month\n0        1 2015-01-23 14:05:21 2015-01-01\n1        2 2015-02-05 05:07:30 2015-02-01\n2        3 2015-02-18 17:08:51 2015-02-01\n3        4 2015-03-21 17:07:30 2015-03-01\n4        5 2015-03-11 18:32:56 2015-03-01\n5        6 2015-03-03 11:02:30 2015-03-01\n\nrng = pd.date_range('1980-04-03 15:41:12', periods=100000, freq='20H')\ndf = pd.DataFrame({'purchase_date': rng})  \nprint (df.head())\n\nIn [300]: %timeit df['month1'] = df['purchase_date'].values.astype('datetime64[M]')\n100 loops, best of 3: 9.2 ms per loop\n\nIn [301]: %timeit df['month2'] = df['purchase_date'].dt.floor('d') - pd.offsets.MonthBegin(1)\n100 loops, best of 3: 15.9 ms per loop\n\nIn [302]: %timeit df['month3'] = (df['purchase_date'] - pd.offsets.MonthBegin(1)).dt.floor('d')\n100 loops, best of 3: 12.8 ms per loop\n\nIn [303]: %timeit df['month4'] = df['purchase_date'].dt.to_period('M').dt.to_timestamp()\n1 loop, best of 3: 399 ms per loop\n\n#MaxU solution\nIn [304]: %timeit df['month5'] = df['purchase_date'].dt.normalize() - pd.offsets.MonthBegin(1)\n10 loops, best of 3: 24.9 ms per loop\n\n#MaxU solution 2\nIn [305]: %timeit df['month'] = df['purchase_date'] - pd.offsets.MonthBegin(1, normalize=True)\n10 loops, best of 3: 28.9 ms per loop\n\n#Wen solution\nIn [306]: %timeit df['month6']= pd.to_datetime(df.purchase_date.astype(str).str[0:7]+'-01')\n1 loop, best of 3: 214 ms per loop\n"
"df = pd.DataFrame({'InvoiceNo': ['aaC','ff','lC'],\n                   'a':[1,2,5]})\nprint (df)\n  InvoiceNo  a\n0       aaC  1\n1        ff  2\n2        lC  5\n\n#check if column contains C\nprint (df['InvoiceNo'].str.contains('C'))\n0     True\n1    False\n2     True\nName: InvoiceNo, dtype: bool\n\n#inversing mask\nprint (~df['InvoiceNo'].str.contains('C'))\n0    False\n1     True\n2    False\nName: InvoiceNo, dtype: bool\n\ndf = df[~df['InvoiceNo'].str.contains('C')]\nprint (df)\n  InvoiceNo  a\n1        ff  2\n"
'# Setup\n\nimport pandas as pd\nimport numpy as np\n\ndef on_series(s):\n    return np.clip(s, a_min=None, a_max=1)\n\ndef on_values_of_series(s):\n    return np.clip(s.values, a_min=None, a_max=1)\n\n# Timing setup\ntimings = {on_series: [], on_values_of_series: []}\nsizes = [2**i for i in range(1, 26, 2)]\n\n# Timing\nfor size in sizes:\n    func_input = pd.Series(np.random.randint(0, 30, size=size))\n    for func in timings:\n        res = %timeit -o func(func_input)\n        timings[func].append(res)\n\n%matplotlib notebook\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor func in timings:\n    ax1.plot(sizes, \n             [time.best for time in timings[func]], \n             label=str(func.__name__))\nax1.set_xscale(\'log\')\nax1.set_yscale(\'log\')\nax1.set_xlabel(\'size\')\nax1.set_ylabel(\'time [seconds]\')\nax1.grid(which=\'both\')\nax1.legend()\n\nbaseline = on_values_of_series # choose one function as baseline\nfor func in timings:\n    ax2.plot(sizes, \n             [time.best / ref.best for time, ref in zip(timings[func], timings[baseline])], \n             label=str(func.__name__))\nax2.set_yscale(\'log\')\nax2.set_xscale(\'log\')\nax2.set_xlabel(\'size\')\nax2.set_ylabel(\'time relative to {}\'.format(baseline.__name__))\nax2.grid(which=\'both\')\nax2.legend()\n\nplt.tight_layout()\n\n&gt;&gt;&gt; np.clip??\nSource:   \ndef clip(a, a_min, a_max, out=None):\n    """\n    ...\n    """\n    return _wrapfunc(a, \'clip\', a_min, a_max, out=out)\n\n&gt;&gt;&gt; np.core.fromnumeric._wrapfunc??\nSource:   \ndef _wrapfunc(obj, method, *args, **kwds):\n    try:\n        return getattr(obj, method)(*args, **kwds)\n    # ...\n    except (AttributeError, TypeError):\n        return _wrapit(obj, method, *args, **kwds)\n\n&gt;&gt;&gt; np.ndarray.clip\n&lt;method \'clip\' of \'numpy.ndarray\' objects&gt;\n&gt;&gt;&gt; pd.Series.clip\n&lt;function pandas.core.generic.NDFrame.clip&gt;\n\ns = pd.Series(np.random.randint(0, 100, 5000))\n\n%load_ext line_profiler\n%lprun -f np.clip -f np.core.fromnumeric._wrapfunc np.clip(s.values, a_min=None, a_max=1)\n\nTimer unit: 4.10256e-07 s\n\nTotal time: 2.25641e-05 s\nFile: numpy\\core\\fromnumeric.py\nFunction: clip at line 1673\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  1673                                           def clip(a, a_min, a_max, out=None):\n  1674                                               """\n  ...\n  1726                                               """\n  1727         1           55     55.0    100.0      return _wrapfunc(a, \'clip\', a_min, a_max, out=out)\n\nTotal time: 1.51795e-05 s\nFile: numpy\\core\\fromnumeric.py\nFunction: _wrapfunc at line 55\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    55                                           def _wrapfunc(obj, method, *args, **kwds):\n    56         1            2      2.0      5.4      try:\n    57         1           35     35.0     94.6          return getattr(obj, method)(*args, **kwds)\n    58                                           \n    59                                               # An AttributeError occurs if the object does not have\n    60                                               # such a method in its class.\n    61                                           \n    62                                               # A TypeError occurs if the object does have such a method\n    63                                               # in its class, but its signature is not identical to that\n    64                                               # of NumPy\'s. This situation has occurred in the case of\n    65                                               # a downstream library like \'pandas\'.\n    66                                               except (AttributeError, TypeError):\n    67                                                   return _wrapit(obj, method, *args, **kwds)\n\n%lprun -f np.clip -f np.core.fromnumeric._wrapfunc -f pd.Series.clip -f pd.Series._clip_with_scalar np.clip(s, a_min=None, a_max=1)\n\nTimer unit: 4.10256e-07 s\n\nTotal time: 0.000823794 s\nFile: numpy\\core\\fromnumeric.py\nFunction: clip at line 1673\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  1673                                           def clip(a, a_min, a_max, out=None):\n  1674                                               """\n  ...\n  1726                                               """\n  1727         1         2008   2008.0    100.0      return _wrapfunc(a, \'clip\', a_min, a_max, out=out)\n\nTotal time: 0.00081846 s\nFile: numpy\\core\\fromnumeric.py\nFunction: _wrapfunc at line 55\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    55                                           def _wrapfunc(obj, method, *args, **kwds):\n    56         1            2      2.0      0.1      try:\n    57         1         1993   1993.0     99.9          return getattr(obj, method)(*args, **kwds)\n    58                                           \n    59                                               # An AttributeError occurs if the object does not have\n    60                                               # such a method in its class.\n    61                                           \n    62                                               # A TypeError occurs if the object does have such a method\n    63                                               # in its class, but its signature is not identical to that\n    64                                               # of NumPy\'s. This situation has occurred in the case of\n    65                                               # a downstream library like \'pandas\'.\n    66                                               except (AttributeError, TypeError):\n    67                                                   return _wrapit(obj, method, *args, **kwds)\n\nTotal time: 0.000804922 s\nFile: pandas\\core\\generic.py\nFunction: clip at line 4969\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4969                                               def clip(self, lower=None, upper=None, axis=None, inplace=False,\n  4970                                                        *args, **kwargs):\n  4971                                                   """\n  ...\n  5021                                                   """\n  5022         1           12     12.0      0.6          if isinstance(self, ABCPanel):\n  5023                                                       raise NotImplementedError("clip is not supported yet for panels")\n  5024                                           \n  5025         1           10     10.0      0.5          inplace = validate_bool_kwarg(inplace, \'inplace\')\n  5026                                           \n  5027         1           69     69.0      3.5          axis = nv.validate_clip_with_axis(axis, args, kwargs)\n  5028                                           \n  5029                                                   # GH 17276\n  5030                                                   # numpy doesn\'t like NaN as a clip value\n  5031                                                   # so ignore\n  5032         1          158    158.0      8.1          if np.any(pd.isnull(lower)):\n  5033         1            3      3.0      0.2              lower = None\n  5034         1           26     26.0      1.3          if np.any(pd.isnull(upper)):\n  5035                                                       upper = None\n  5036                                           \n  5037                                                   # GH 2747 (arguments were reversed)\n  5038         1            1      1.0      0.1          if lower is not None and upper is not None:\n  5039                                                       if is_scalar(lower) and is_scalar(upper):\n  5040                                                           lower, upper = min(lower, upper), max(lower, upper)\n  5041                                           \n  5042                                                   # fast-path for scalars\n  5043         1            1      1.0      0.1          if ((lower is None or (is_scalar(lower) and is_number(lower))) and\n  5044         1           28     28.0      1.4                  (upper is None or (is_scalar(upper) and is_number(upper)))):\n  5045         1         1654   1654.0     84.3              return self._clip_with_scalar(lower, upper, inplace=inplace)\n  5046                                           \n  5047                                                   result = self\n  5048                                                   if lower is not None:\n  5049                                                       result = result.clip_lower(lower, axis, inplace=inplace)\n  5050                                                   if upper is not None:\n  5051                                                       if inplace:\n  5052                                                           result = self\n  5053                                                       result = result.clip_upper(upper, axis, inplace=inplace)\n  5054                                           \n  5055                                                   return result\n\nTotal time: 0.000662153 s\nFile: pandas\\core\\generic.py\nFunction: _clip_with_scalar at line 4920\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4920                                               def _clip_with_scalar(self, lower, upper, inplace=False):\n  4921         1            2      2.0      0.1          if ((lower is not None and np.any(isna(lower))) or\n  4922         1           25     25.0      1.5                  (upper is not None and np.any(isna(upper)))):\n  4923                                                       raise ValueError("Cannot use an NA value as a clip threshold")\n  4924                                           \n  4925         1           22     22.0      1.4          result = self.values\n  4926         1          571    571.0     35.4          mask = isna(result)\n  4927                                           \n  4928         1           95     95.0      5.9          with np.errstate(all=\'ignore\'):\n  4929         1            1      1.0      0.1              if upper is not None:\n  4930         1          141    141.0      8.7                  result = np.where(result &gt;= upper, upper, result)\n  4931         1           33     33.0      2.0              if lower is not None:\n  4932                                                           result = np.where(result &lt;= lower, lower, result)\n  4933         1           73     73.0      4.5          if np.any(mask):\n  4934                                                       result[mask] = np.nan\n  4935                                           \n  4936         1           90     90.0      5.6          axes_dict = self._construct_axes_dict()\n  4937         1          558    558.0     34.6          result = self._constructor(result, **axes_dict).__finalize__(self)\n  4938                                           \n  4939         1            2      2.0      0.1          if inplace:\n  4940                                                       self._update_inplace(result)\n  4941                                                   else:\n  4942         1            1      1.0      0.1              return result\n\ns = pd.Series(np.random.randint(0, 100, 1000000))\n\n%lprun -f np.clip -f np.core.fromnumeric._wrapfunc -f pd.Series.clip -f pd.Series._clip_with_scalar np.clip(s, a_min=None, a_max=1)\n\nTimer unit: 4.10256e-07 s\n\nTotal time: 0.00593476 s\nFile: numpy\\core\\fromnumeric.py\nFunction: clip at line 1673\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  1673                                           def clip(a, a_min, a_max, out=None):\n  1674                                               """\n  ...\n  1726                                               """\n  1727         1        14466  14466.0    100.0      return _wrapfunc(a, \'clip\', a_min, a_max, out=out)\n\nTotal time: 0.00592779 s\nFile: numpy\\core\\fromnumeric.py\nFunction: _wrapfunc at line 55\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    55                                           def _wrapfunc(obj, method, *args, **kwds):\n    56         1            1      1.0      0.0      try:\n    57         1        14448  14448.0    100.0          return getattr(obj, method)(*args, **kwds)\n    58                                           \n    59                                               # An AttributeError occurs if the object does not have\n    60                                               # such a method in its class.\n    61                                           \n    62                                               # A TypeError occurs if the object does have such a method\n    63                                               # in its class, but its signature is not identical to that\n    64                                               # of NumPy\'s. This situation has occurred in the case of\n    65                                               # a downstream library like \'pandas\'.\n    66                                               except (AttributeError, TypeError):\n    67                                                   return _wrapit(obj, method, *args, **kwds)\n\nTotal time: 0.00591302 s\nFile: pandas\\core\\generic.py\nFunction: clip at line 4969\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4969                                               def clip(self, lower=None, upper=None, axis=None, inplace=False,\n  4970                                                        *args, **kwargs):\n  4971                                                   """\n  ...\n  5021                                                   """\n  5022         1           17     17.0      0.1          if isinstance(self, ABCPanel):\n  5023                                                       raise NotImplementedError("clip is not supported yet for panels")\n  5024                                           \n  5025         1           14     14.0      0.1          inplace = validate_bool_kwarg(inplace, \'inplace\')\n  5026                                           \n  5027         1           97     97.0      0.7          axis = nv.validate_clip_with_axis(axis, args, kwargs)\n  5028                                           \n  5029                                                   # GH 17276\n  5030                                                   # numpy doesn\'t like NaN as a clip value\n  5031                                                   # so ignore\n  5032         1          125    125.0      0.9          if np.any(pd.isnull(lower)):\n  5033         1            2      2.0      0.0              lower = None\n  5034         1           30     30.0      0.2          if np.any(pd.isnull(upper)):\n  5035                                                       upper = None\n  5036                                           \n  5037                                                   # GH 2747 (arguments were reversed)\n  5038         1            2      2.0      0.0          if lower is not None and upper is not None:\n  5039                                                       if is_scalar(lower) and is_scalar(upper):\n  5040                                                           lower, upper = min(lower, upper), max(lower, upper)\n  5041                                           \n  5042                                                   # fast-path for scalars\n  5043         1            2      2.0      0.0          if ((lower is None or (is_scalar(lower) and is_number(lower))) and\n  5044         1           32     32.0      0.2                  (upper is None or (is_scalar(upper) and is_number(upper)))):\n  5045         1        14092  14092.0     97.8              return self._clip_with_scalar(lower, upper, inplace=inplace)\n  5046                                           \n  5047                                                   result = self\n  5048                                                   if lower is not None:\n  5049                                                       result = result.clip_lower(lower, axis, inplace=inplace)\n  5050                                                   if upper is not None:\n  5051                                                       if inplace:\n  5052                                                           result = self\n  5053                                                       result = result.clip_upper(upper, axis, inplace=inplace)\n  5054                                           \n  5055                                                   return result\n\nTotal time: 0.00575753 s\nFile: pandas\\core\\generic.py\nFunction: _clip_with_scalar at line 4920\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n  4920                                               def _clip_with_scalar(self, lower, upper, inplace=False):\n  4921         1            2      2.0      0.0          if ((lower is not None and np.any(isna(lower))) or\n  4922         1           28     28.0      0.2                  (upper is not None and np.any(isna(upper)))):\n  4923                                                       raise ValueError("Cannot use an NA value as a clip threshold")\n  4924                                           \n  4925         1          120    120.0      0.9          result = self.values\n  4926         1         3525   3525.0     25.1          mask = isna(result)\n  4927                                           \n  4928         1           86     86.0      0.6          with np.errstate(all=\'ignore\'):\n  4929         1            2      2.0      0.0              if upper is not None:\n  4930         1         9314   9314.0     66.4                  result = np.where(result &gt;= upper, upper, result)\n  4931         1           61     61.0      0.4              if lower is not None:\n  4932                                                           result = np.where(result &lt;= lower, lower, result)\n  4933         1          283    283.0      2.0          if np.any(mask):\n  4934                                                       result[mask] = np.nan\n  4935                                           \n  4936         1           78     78.0      0.6          axes_dict = self._construct_axes_dict()\n  4937         1          532    532.0      3.8          result = self._constructor(result, **axes_dict).__finalize__(self)\n  4938                                           \n  4939         1            2      2.0      0.0          if inplace:\n  4940                                                       self._update_inplace(result)\n  4941                                                   else:\n  4942         1            1      1.0      0.0              return result\n\nPython 3.6.3 64-bit on Windows 10\nNumpy 1.13.3\nPandas 0.21.1\n'
'import pandas as pd\nimport numpy as np\n\npd.set_option(\'display.width\', 1000)\npd.set_option(\'colheader_justify\', \'center\')\n\nnp.random.seed(6182018)\ndemo_df = pd.DataFrame({\'date\': np.random.choice(pd.date_range(\'2018-01-01\', \'2018-06-18\', freq=\'D\'), 50),\n                        \'analysis_tool\': np.random.choice([\'pandas\', \'r\', \'julia\', \'sas\', \'stata\', \'spss\'],50),              \n                        \'database\': np.random.choice([\'postgres\', \'mysql\', \'sqlite\', \'oracle\', \'sql server\', \'db2\'],50), \n                        \'os\': np.random.choice([\'windows 10\', \'ubuntu\', \'mac os\', \'android\', \'ios\', \'windows 7\', \'debian\'],50), \n                        \'num1\': np.random.randn(50)*100,\n                        \'num2\': np.random.uniform(0,1,50),                   \n                        \'num3\': np.random.randint(100, size=50),\n                        \'bool\': np.random.choice([True, False], 50)\n                       },\n                        columns=[\'date\', \'analysis_tool\', \'num1\', \'database\', \'num2\', \'os\', \'num3\', \'bool\']\n          )\n\n\nprint(demo_df.head(10))\n#      date    analysis_tool     num1      database     num2        os      num3  bool \n# 0 2018-04-21     pandas     153.474246       mysql  0.658533         ios   74    True\n# 1 2018-04-13        sas     199.461669      sqlite  0.656985   windows 7   11   False\n# 2 2018-06-09      stata      12.918608      oracle  0.495707     android   25   False\n# 3 2018-04-24       spss      88.562111  sql server  0.113580   windows 7   42   False\n# 4 2018-05-05       spss     110.231277      oracle  0.660977  windows 10   76    True\n# 5 2018-04-05        sas     -68.140295  sql server  0.346894  windows 10    0    True\n# 6 2018-05-07      julia      12.874660    postgres  0.195217         ios   79    True\n# 7 2018-01-22          r     189.410928       mysql  0.234815  windows 10   56   False\n# 8 2018-01-12     pandas    -111.412564  sql server  0.580253      debian   30   False\n# 9 2018-04-12          r      38.963967    postgres  0.266604   windows 7   46   False\n\n/* includes alternating gray and white with on-hover color */\n\n.mystyle {\n    font-size: 11pt; \n    font-family: Arial;\n    border-collapse: collapse; \n    border: 1px solid silver;\n\n}\n\n.mystyle td, th {\n    padding: 5px;\n}\n\n.mystyle tr:nth-child(even) {\n    background: #E0E0E0;\n}\n\n.mystyle tr:hover {\n    background: silver;\n    cursor: pointer;\n}\n\npd.set_option(\'colheader_justify\', \'center\')   # FOR TABLE &lt;th&gt;\n\nhtml_string = \'\'\'\n&lt;html&gt;\n  &lt;head&gt;&lt;title&gt;HTML Pandas Dataframe with CSS&lt;/title&gt;&lt;/head&gt;\n  &lt;link rel="stylesheet" type="text/css" href="df_style.css"/&gt;\n  &lt;body&gt;\n    {table}\n  &lt;/body&gt;\n&lt;/html&gt;.\n\'\'\'\n\n# OUTPUT AN HTML FILE\nwith open(\'myhtml.html\', \'w\') as f:\n    f.write(html_string.format(table=demo_df.to_html(classes=\'mystyle\')))\n'
"df['A_perc'] = df['A']/df['sum']\n\nds.div(ds['sum'], axis=0)\n\n&gt;&gt;&gt; ds.join(ds.div(ds['sum'], axis=0), rsuffix='_perc')\n          A         B         C         D       sum    A_perc    B_perc  \\\n1  0.151722  0.935917  1.033526  0.941962  3.063127  0.049532  0.305543   \n2  0.033761  1.087302  1.110695  1.401260  3.633017  0.009293  0.299283   \n3  0.761368  0.484268  0.026837  1.276130  2.548603  0.298739  0.190013   \n\n     C_perc    D_perc  sum_perc  \n1  0.337409  0.307517         1  \n2  0.305722  0.385701         1  \n3  0.010530  0.500718         1  \n"
"import matplotlib.pyplot as plt\nx=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\ny=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\nfor i in range(len(x)):\n    plt.figure()\n    plt.plot(x[i],y[i])\n    # Show/save figure as desired.\n    plt.show()\n# Can show all four figures at once by calling plt.show() here, outside the loop.\n#plt.show()\n\nimport matplotlib.pyplot as plt\nplt.figure()\nx=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\ny=[[1,2,3,4],[2,3,4,5],[3,4,5,6],[7,8,9,10]]\nplt.plot(x[0],y[0],'r',x[1],y[1],'g',x[2],y[2],'b',x[3],y[3],'k')\n\nimport matplotlib.pyplot as plt\nx=[[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]]\ny=[[1,2,3,4],[2,3,4,5],[3,4,5,6],[7,8,9,10]]\ncolours=['r','g','b','k']\nplt.figure() # In this example, all the plots will be in one figure.    \nfor i in range(len(x)):\n    plt.plot(x[i],y[i],colours[i])\nplt.show()\n"
'import pandas as pd\nimport numpy as np\nfrom pandas import *\n\n&gt;&gt;&gt; L = [4, nan ,6]\n&gt;&gt;&gt; df = Series(L)\n\n&gt;&gt;&gt; df\n0     4\n1   NaN\n2     6\n\n&gt;&gt;&gt; if(pd.isnull(df[1])):\n        print "Found"\n\nFound\n\n&gt;&gt;&gt; if(np.isnan(df[1])):\n        print "Found"\n\nFound\n'
"In [67]: import pandas.tseries.converter as converter\n\nIn [68]: c = converter.DatetimeConverter()\n\nIn [69]: type(c.convert(df['Date'].values, None, None))\nOut[69]: numpy.ndarray              # converted (good)\n\nIn [70]: type(c.convert(df['Date'], None, None))\nOut[70]: pandas.core.series.Series  # left unchanged\n\nd = data['Date'].values\nplt.fill_between(d, data['A'], data['B'],\n                where=data['A'] &gt;= data['B'],\n                facecolor='green', alpha=0.2, interpolate=True)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nN = 300\ndates = pd.date_range('2000-1-1', periods=N, freq='D')\nx = np.linspace(0, 2*np.pi, N)\ndata = pd.DataFrame({'A': np.sin(x), 'B': np.cos(x),\n               'Date': dates})\nplt.plot_date(data['Date'], data['A'], '-')\nplt.plot_date(data['Date'], data['B'], '-')\n\nd = data['Date'].values\nplt.fill_between(d, data['A'], data['B'],\n                where=data['A'] &gt;= data['B'],\n                facecolor='green', alpha=0.2, interpolate=True)\nplt.xticks(rotation=25)\nplt.show()\n"
"In [223]:\n\ndf['count'] = df.groupby('group')['group'].transform('count')\ndf\nOut[223]:\n    org  group  count\n0  org1      1      2\n1  org2      1      2\n2  org3      2      1\n3  org4      3      3\n4  org5      3      3\n5  org6      3      3\n"
"data = 'col1,col2,col3\\na,b,1\\na,b,2\\nc,d,3'\ndf = pd.read_csv(pd.compat.StringIO(data), dtype='category')\nprint (df)\n  col1 col2 col3\n0    a    b    1\n1    a    b    2\n2    c    d    3\n\nprint (df.dtypes)\ncol1    category\ncol2    category\ncol3    category\ndtype: object\n\ndf = pd.read_csv(pd.compat.StringIO(data), dtype={'col1':'category'})\nprint (df)\n  col1 col2  col3\n0    a    b     1\n1    a    b     2\n2    c    d     3\n\nprint (df.dtypes)\ncol1    category\ncol2      object\ncol3       int64\ndtype: object\n"
'import os, json\nimport pandas as pd\n\npath_to_json = \'somedir/\'\njson_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith(\'.json\')]\nprint(json_files)  # for me this prints [\'foo.json\']\n\nmontreal_json = pd.DataFrame.from_dict(many_jsons[0])\nprint montreal_json[\'features\'][0][\'geometry\']\n\n{u\'type\': u\'Point\', u\'coordinates\': [-73.6051013, 45.5115944]}\n\nimport os, json\nimport pandas as pd\n\n# this finds our json files\npath_to_json = \'json/\'\njson_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith(\'.json\')]\n\n# here I define my pandas Dataframe with the columns I want to get from the json\njsons_data = pd.DataFrame(columns=[\'country\', \'city\', \'long/lat\'])\n\n# we need both the json and an index number so use enumerate()\nfor index, js in enumerate(json_files):\n    with open(os.path.join(path_to_json, js)) as json_file:\n        json_text = json.load(json_file)\n\n        # here you need to know the layout of your json and each json has to have\n        # the same structure (obviously not the structure I have here)\n        country = json_text[\'features\'][0][\'properties\'][\'country\']\n        city = json_text[\'features\'][0][\'properties\'][\'name\']\n        lonlat = json_text[\'features\'][0][\'geometry\'][\'coordinates\']\n        # here I push a list of data into a pandas DataFrame at row given by \'index\'\n        jsons_data.loc[index] = [country, city, lonlat]\n\n# now that we have the pertinent json data in our DataFrame let\'s look at it\nprint(jsons_data)\n\n  country           city                   long/lat\n0  Canada  Montreal city  [-73.6051013, 45.5115944]\n1  Canada        Toronto  [-79.3849008, 43.6529206]\n\n{"features":\n[{"properties":\n{"osm_key":"boundary","extent":\n[-73.9729016,45.7047897,-73.4734865,45.4100756],\n"name":"Montreal city","state":"Quebec","osm_id":1634158,\n"osm_type":"R","osm_value":"administrative","country":"Canada"},\n"type":"Feature","geometry":\n{"type":"Point","coordinates":\n[-73.6051013,45.5115944]}}],\n"type":"FeatureCollection"}\n'
"df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n"
"df = df.assign(B=df1['E'])\n"
"                  read_s  write_s  size_ratio_to_CSV\nstorage\nCSV               17.900    69.00              1.000\nCSV.gzip          18.900   186.00              0.047\nPickle             0.173     1.77              0.374\nHDF_fixed          0.196     2.03              0.435\nHDF_tab            0.230     2.60              0.437\nHDF_tab_zlib_c5    0.845     5.44              0.035\nHDF_tab_zlib_c9    0.860     5.95              0.035\nHDF_tab_bzip2_c5   2.500    36.50              0.011\nHDF_tab_bzip2_c9   2.500    36.50              0.011\n\nIn [68]: %timeit df.to_csv(fcsv)\n1 loop, best of 3: 1min 9s per loop\n\nIn [74]: %timeit pd.read_csv(fcsv)\n1 loop, best of 3: 17.9 s per loop\n\nIn [70]: %timeit df.to_csv(fcsv_gz, compression='gzip')\n1 loop, best of 3: 3min 6s per loop\n\nIn [75]: %timeit pd.read_csv(fcsv_gz)\n1 loop, best of 3: 18.9 s per loop\n\nIn [66]: %timeit df.to_pickle(fpckl)\n1 loop, best of 3: 1.77 s per loop\n\nIn [72]: %timeit pd.read_pickle(fpckl)\n10 loops, best of 3: 173 ms per loop\n\nIn [67]: %timeit df.to_hdf(fh5, 'df')\n1 loop, best of 3: 2.03 s per loop\n\nIn [73]: %timeit pd.read_hdf(fh5, 'df')\n10 loops, best of 3: 196 ms per loop\n\nIn [37]: %timeit df.to_hdf('D:\\\\temp\\\\.data\\\\37010212_tab.h5', 'df', format='t')\n1 loop, best of 3: 2.6 s per loop\n\nIn [38]: %timeit pd.read_hdf('D:\\\\temp\\\\.data\\\\37010212_tab.h5', 'df')\n1 loop, best of 3: 230 ms per loop\n\nIn [40]: %timeit df.to_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_zlib5.h5', 'df', format='t', complevel=5, complib='zlib')\n1 loop, best of 3: 5.44 s per loop\n\nIn [41]: %timeit pd.read_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_zlib5.h5', 'df')\n1 loop, best of 3: 854 ms per loop\n\nIn [36]: %timeit df.to_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_zlib9.h5', 'df', format='t', complevel=9, complib='zlib')\n1 loop, best of 3: 5.95 s per loop\n\nIn [39]: %timeit pd.read_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_zlib9.h5', 'df')\n1 loop, best of 3: 860 ms per loop\n\nIn [42]: %timeit df.to_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_bzip2_l5.h5', 'df', format='t', complevel=5, complib='bzip2')\n1 loop, best of 3: 36.5 s per loop\n\nIn [43]: %timeit pd.read_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_bzip2_l5.h5', 'df')\n1 loop, best of 3: 2.5 s per loop\n\nIn [42]: %timeit df.to_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_bzip2_l9.h5', 'df', format='t', complevel=9, complib='bzip2')\n1 loop, best of 3: 36.5 s per loop\n\nIn [43]: %timeit pd.read_hdf('D:\\\\temp\\\\.data\\\\37010212_tab_compress_bzip2_l9.h5', 'df')\n1 loop, best of 3: 2.5 s per loop\n\nIn [49]: df.shape\nOut[49]: (4000000, 6)\n\nIn [50]: df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4000000 entries, 0 to 3999999\nData columns (total 6 columns):\na    datetime64[ns]\nb    datetime64[ns]\nc    datetime64[ns]\nd    datetime64[ns]\ne    datetime64[ns]\nf    datetime64[ns]\ndtypes: datetime64[ns](6)\nmemory usage: 183.1 MB\n\nIn [41]: df.head()\nOut[41]:\n                    a                   b                   c  \\\n0 1970-01-01 00:00:00 1970-01-01 00:01:00 1970-01-01 00:02:00\n1 1970-01-01 00:01:00 1970-01-01 00:02:00 1970-01-01 00:03:00\n2 1970-01-01 00:02:00 1970-01-01 00:03:00 1970-01-01 00:04:00\n3 1970-01-01 00:03:00 1970-01-01 00:04:00 1970-01-01 00:05:00\n4 1970-01-01 00:04:00 1970-01-01 00:05:00 1970-01-01 00:06:00\n\n                    d                   e                   f\n0 1970-01-01 00:03:00 1970-01-01 00:04:00 1970-01-01 00:05:00\n1 1970-01-01 00:04:00 1970-01-01 00:05:00 1970-01-01 00:06:00\n2 1970-01-01 00:05:00 1970-01-01 00:06:00 1970-01-01 00:07:00\n3 1970-01-01 00:06:00 1970-01-01 00:07:00 1970-01-01 00:08:00\n4 1970-01-01 00:07:00 1970-01-01 00:08:00 1970-01-01 00:09:00\n\n{ .data }  » ls -lh 37010212.*                                                                          /d/temp/.data\n-rw-r--r-- 1 Max None 492M May  3 22:21 37010212.csv\n-rw-r--r-- 1 Max None  23M May  3 22:19 37010212.csv.gz\n-rw-r--r-- 1 Max None 214M May  3 22:02 37010212.h5\n-rw-r--r-- 1 Max None 184M May  3 22:02 37010212.pickle\n-rw-r--r-- 1 Max None 215M May  4 10:39 37010212_tab.h5\n-rw-r--r-- 1 Max None 5.4M May  4 10:46 37010212_tab_compress_bzip2_l5.h5\n-rw-r--r-- 1 Max None 5.4M May  4 10:51 37010212_tab_compress_bzip2_l9.h5\n-rw-r--r-- 1 Max None  17M May  4 10:42 37010212_tab_compress_zlib5.h5\n-rw-r--r-- 1 Max None  17M May  4 10:36 37010212_tab_compress_zlib9.h5\n"
"TableA = pd.DataFrame(np.random.rand(4, 3),\n                      pd.Index(list('abcd'), name='Key'),\n                      ['A', 'B', 'C']).reset_index()\nTableB = pd.DataFrame(np.random.rand(4, 3),\n                      pd.Index(list('aecf'), name='Key'),\n                      ['A', 'B', 'C']).reset_index()\n\nTableA\n\nTableB\n\n# Identify what values are in TableB and not in TableA\nkey_diff = set(TableB.Key).difference(TableA.Key)\nwhere_diff = TableB.Key.isin(key_diff)\n\n# Slice TableB accordingly and append to TableA\nTableA.append(TableB[where_diff], ignore_index=True)\n\nrows = []\nfor i, row in TableB.iterrows():\n    if row.Key not in TableA.Key.values:\n        rows.append(row)\n\npd.concat([TableA.T] + rows, axis=1).T\n"
"df = pd.concat((df, s.rename('col')), axis=1)\n"
"from dask import dataframe as dd \nsd = dd.from_pandas(df, npartitions=3)\nprint (sd)\ndd.DataFrame&lt;from_pa..., npartitions=2, divisions=(0, 1, 2)&gt;\n\nimport pandas as pd\nimport dask.dataframe as dd\nfrom dask.dataframe.utils import make_meta\n\ndf=pd.DataFrame({'a':[1,2,3],'b':[4,5,6]})\n\ndsk = {('x', 0): df}\n\nmeta = make_meta({'a': 'i8', 'b': 'i8'}, index=pd.Index([], 'i8'))\nd = dd.DataFrame(dsk, name='x', meta=meta, divisions=[0, 1, 2])\nprint (d)\ndd.DataFrame&lt;x, npartitions=2, divisions=(0, 1, 2)&gt;\n"
"In [90]: df = pd.DataFrame(np.random.randint(10**5,10**7,(5,3)),columns=list('abc'), dtype=np.int64)\n\nIn [91]: df\nOut[91]:\n         a        b        c\n0  9059440  9590567  2076918\n1  5861102  4566089  1947323\n2  6636568   162770  2487991\n3  6794572  5236903  5628779\n4   470121  4044395  4546794\n\nIn [92]: df.dtypes\nOut[92]:\na    int64\nb    int64\nc    int64\ndtype: object\n\nIn [93]: df['a'] = df['a'].astype(float)\n\nIn [94]: df.dtypes\nOut[94]:\na    float64\nb      int64\nc      int64\ndtype: object\n\nIn [95]: df.loc[1, 'b'] = 'XXXXXX'\n\nIn [96]: df\nOut[96]:\n           a        b        c\n0  9059440.0  9590567  2076918\n1  5861102.0   XXXXXX  1947323\n2  6636568.0   162770  2487991\n3  6794572.0  5236903  5628779\n4   470121.0  4044395  4546794\n\nIn [97]: df.dtypes\nOut[97]:\na    float64\nb     object\nc      int64\ndtype: object\n\nIn [98]: df['b'].astype(float)\n...\nskipped\n...\nValueError: could not convert string to float: 'XXXXXX'\n\nIn [99]: df['b'] = pd.to_numeric(df['b'], errors='coerce')\n\nIn [100]: df\nOut[100]:\n           a          b        c\n0  9059440.0  9590567.0  2076918\n1  5861102.0        NaN  1947323\n2  6636568.0   162770.0  2487991\n3  6794572.0  5236903.0  5628779\n4   470121.0  4044395.0  4546794\n\nIn [101]: df.dtypes\nOut[101]:\na    float64\nb    float64\nc      int64\ndtype: object\n"
'netc.loc[:,"DeltaAMPP"] = netc.LOAD_AM - netc.VPP12_AM.\n\nnetc["DeltaAMPP"] = netc.LOAD_AM - netc.VPP12_AM\n\nnetc.__setitem__(\'DeltaAMPP\', netc.LOAD_AM - netc.VPP12_AM)\n'
"start = pd.to_datetime('2015-02-24')\nrng = pd.date_range(start, periods=10)\n\ndf = pd.DataFrame({'Date': rng, 'a': range(10)})  \n\ndf.Date = df.Date.dt.tz_localize('UTC').dt.tz_convert('Asia/Kolkata')\nprint (df)\n                       Date  a\n0 2015-02-24 05:30:00+05:30  0\n1 2015-02-25 05:30:00+05:30  1\n2 2015-02-26 05:30:00+05:30  2\n3 2015-02-27 05:30:00+05:30  3\n4 2015-02-28 05:30:00+05:30  4\n5 2015-03-01 05:30:00+05:30  5\n6 2015-03-02 05:30:00+05:30  6\n7 2015-03-03 05:30:00+05:30  7\n8 2015-03-04 05:30:00+05:30  8\n9 2015-03-05 05:30:00+05:30  9\n\ndf.Date = df.Date + pd.Timedelta('05:30:00')\nprint (df)\n                 Date  a\n0 2015-02-24 05:30:00  0\n1 2015-02-25 05:30:00  1\n2 2015-02-26 05:30:00  2\n3 2015-02-27 05:30:00  3\n4 2015-02-28 05:30:00  4\n5 2015-03-01 05:30:00  5\n6 2015-03-02 05:30:00  6\n7 2015-03-03 05:30:00  7\n8 2015-03-04 05:30:00  8\n9 2015-03-05 05:30:00  9\n"
'Y.reshape((2,1))\n'
'df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n\n             time    X    Y  X_t0     X_tp0   X_t1     X_tp1   X_t2     X_tp2\n4        0.037389    3   10     3  0.333333    2.0  0.500000    1.0  1.000000\n5        0.037393    4   10     4  0.250000    3.0  0.333333    2.0  0.500000\n1030308  9.962213  256  268   256  0.000000  256.0  0.003906  255.0  0.003922\n'
"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style='darkgrid')\n\ntitanic = sns.load_dataset('titanic')\nsns.countplot(x = 'class',\n              data = titanic,\n              order = titanic['class'].value_counts().index)\nplt.show()\n"
"df\n\n     A    B  C\n0  7.0  NaN  8\n1  3.0  3.0  5\n2  8.0  1.0  7\n3  NaN  0.0  3\n4  8.0  2.0  7\n\ndf.loc[df.A.gt(6), ['A', 'C']]\n\n     A  C\n0  7.0  8\n2  8.0  7\n4  8.0  7\n\ndf.loc[df.A.gt(6), ['A', 'D']]\nFutureWarning: Passing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n     \n     A   D\n0  7.0 NaN\n2  8.0 NaN\n4  8.0 NaN\n"
"&gt;&gt;&gt; import re\n&gt;&gt;&gt; esc_lst = [re.escape(s) for s in lst]\n\n&gt;&gt;&gt; pattern = '|'.join(esc_lst)\n\ndf[col].str.contains(pattern, case=False)\n\nfrom random import randint, seed\n\nseed(321)\n\n# 100 substrings of 5 characters\nlst = [''.join([chr(randint(0, 256)) for _ in range(5)]) for _ in range(100)]\n\n# 50000 strings of 20 characters\nstrings = [''.join([chr(randint(0, 256)) for _ in range(20)]) for _ in range(50000)]\n\ncol = pd.Series(strings)\nesc_lst = [re.escape(s) for s in lst]\npattern = '|'.join(esc_lst)\n\n%timeit col.str.contains(pattern, case=False)\n1 loop, best of 3: 981 ms per loop\n"
"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsin = np.sin\ncos = np.cos\npi = np.pi\nN = 100\n\nx = np.linspace(0, pi, N)\na = sin(x)\nb = cos(x)\n\ndf = pd.DataFrame({\n    'A': [True]*N + [False]*N,\n    'B': np.hstack((a,b))\n    })\n\nfor key, grp in df.groupby(['A']):\n    plt.plot(grp['B'], label=key)\n    grp['D'] = pd.rolling_mean(grp['B'], window=5)    \n    plt.plot(grp['D'], label='rolling ({k})'.format(k=key))\nplt.legend(loc='best')    \nplt.show()\n"
"In [9]: df = DataFrame([ Timestamp('20010101'), Timestamp('20040601') ])\n\nIn [10]: df\nOut[10]: \n                    0\n0 2001-01-01 00:00:00\n1 2004-06-01 00:00:00\n\nIn [11]: df = DataFrame([ Timestamp('20010101'), \n                          Timestamp('20040601') ],columns=['age'])\n\nIn [12]: df\nOut[12]: \n                  age\n0 2001-01-01 00:00:00\n1 2004-06-01 00:00:00\n\nIn [13]: df['today'] = Timestamp('20130419')\n\nIn [14]: df['diff'] = df['today']-df['age']\n\nIn [16]: df['years'] = df['diff'].apply(lambda x: float(x.item().days)/365)\n\nIn [17]: df\nOut[17]: \n                  age               today                diff      years\n0 2001-01-01 00:00:00 2013-04-19 00:00:00 4491 days, 00:00:00  12.304110\n1 2004-06-01 00:00:00 2013-04-19 00:00:00 3244 days, 00:00:00   8.887671\n"
'import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nfig = plt.figure()\nax = fig.add_subplot(111)\nbar_0_10 = ax.bar(np.arange(0,10), np.arange(1,11), color="k")\nbar_10_100 = ax.bar(np.arange(0,10), np.arange(30,40), bottom=np.arange(1,11), color="g")\n# create blank rectangle\nextra = Rectangle((0, 0), 1, 1, fc="w", fill=False, edgecolor=\'none\', linewidth=0)\nax.legend([extra, bar_0_10, bar_10_100], ("My explanatory text", "0-10", "10-100"))\nplt.show()\n'
"df.loc[('at', [1,3,4]), 'Dwell']\n"
"test = df.sort('one', ascending=False)\n"
"In [4]:\ns = pd.Series(np.arange(5))\ns\n\nOut[4]:\n0    0\n1    1\n2    2\n3    3\n4    4\ndtype: int32\n\nIn [5]:    \ns * 10\n\nOut[5]:\n0     0\n1    10\n2    20\n3    30\n4    40\ndtype: int32\n\nIn [6]:    \ndf = pd.DataFrame({'a':np.random.randn(4), 'b':np.random.randn(4)})\ndf\n\nOut[6]:\n          a         b\n0  0.216920  0.652193\n1  0.968969  0.033369\n2  0.637784  0.856836\n3 -2.303556  0.426238\n\nIn [7]:    \ndf * 10\n\nOut[7]:\n           a         b\n0   2.169204  6.521925\n1   9.689690  0.333695\n2   6.377839  8.568362\n3 -23.035557  4.262381\n\nIn [8]:\ndf = pd.DataFrame({'a':np.random.randn(4), 'b':np.random.randn(4), 'c':np.random.randn(4)})\ndf\n\nOut[8]:\n          a         b         c\n0  0.122073 -1.178127 -1.531254\n1  0.011346 -0.747583 -1.967079\n2 -0.019716 -0.235676  1.419547\n3  0.215847  1.112350  0.659432\n\nIn [26]:    \ndf.iloc[0]\n\nOut[26]:\na    0.122073\nb   -1.178127\nc   -1.531254\nName: 0, dtype: float64\n\nIn [27]:    \ndf + df.iloc[0]\n\nOut[27]:\n          a         b         c\n0  0.244146 -2.356254 -3.062507\n1  0.133419 -1.925710 -3.498333\n2  0.102357 -1.413803 -0.111707\n3  0.337920 -0.065777 -0.871822\n\nIn [30]:\ndf + pd.Series(np.arange(4))\n\nOut[30]:\n    a   b   c   0   1   2   3\n0 NaN NaN NaN NaN NaN NaN NaN\n1 NaN NaN NaN NaN NaN NaN NaN\n2 NaN NaN NaN NaN NaN NaN NaN\n3 NaN NaN NaN NaN NaN NaN NaN\n\nIn [55]:\ndf[['a']] + df.iloc[0]\n\nOut[55]:\n          a   b   c\n0  0.244146 NaN NaN\n1  0.133419 NaN NaN\n2  0.102357 NaN NaN\n3  0.337920 NaN NaN\n\nIn [56]:\ndf[['a']].values + df.iloc[0].values\n\nOut[56]:\narray([[ 0.24414608, -1.05605392, -1.4091805 ],\n       [ 0.13341899, -1.166781  , -1.51990758],\n       [ 0.10235701, -1.19784299, -1.55096957],\n       [ 0.33792013, -0.96227987, -1.31540645]])\n\nIn[42]:\ndf[['a']].values + df.iloc[0].values\n\nOut[42]: \narray([[ 0.244146, -1.056054, -1.409181],\n       [ 0.133419, -1.166781, -1.519908],\n       [ 0.102357, -1.197843, -1.55097 ],\n       [ 0.33792 , -0.96228 , -1.315407]])\n\nIn[43]:\npd.DataFrame(df[['a']].values + df.iloc[0].values, columns=df.columns)\n\nOut[43]: \n          a         b         c\n0  0.244146 -1.056054 -1.409181\n1  0.133419 -1.166781 -1.519908\n2  0.102357 -1.197843 -1.550970\n3  0.337920 -0.962280 -1.315407\n"
"pool.map(calc_dist, ['lat','lon'])\n\ngrp_lst_args = list(df.groupby('co_nm').groups.items())\n\nprint(grp_lst_args)\n[('aa', [0, 1, 2]), ('cc', [7, 8, 9]), ('bb', [3, 4, 5, 6])]\n\ndef calc_dist2(arg):\n    grp, lst = arg\n    return pd.DataFrame(\n               [ [grp,\n                  df.loc[c[0]].ser_no,\n                  df.loc[c[1]].ser_no,\n                  vincenty(df.loc[c[0], ['lat','lon']], \n                           df.loc[c[1], ['lat','lon']])\n                 ]\n                 for c in combinations(lst, 2)\n               ],\n               columns=['co_nm','machineA','machineB','distance'])\n\npool = mp.Pool(processes = (mp.cpu_count() - 1))\nresults = pool.map(calc_dist2, grp_lst_args)\npool.close()\npool.join()\n\nresults_df = pd.concat(results)\n\nprint(results_df)\n  co_nm  machineA  machineB          distance\n0    aa         1         2  156.876149391 km\n1    aa         1         3  313.705445447 km\n2    aa         2         3  156.829329105 km\n0    cc         8         9  156.060165391 km\n1    cc         8         0  311.910998169 km\n2    cc         9         0  155.851498134 km\n0    bb         4         5  156.665641837 km\n1    bb         4         6  313.214333025 km\n2    bb         4         7  469.622535339 km\n3    bb         5         6  156.548897414 km\n4    bb         5         7  312.957597466 km\n5    bb         6         7   156.40899677 km\n\nwith mp.Pool() as pool:\n    results = pool.map(calc_dist2, grp_lst_args)\n"
"df['ts'] = df.datetime.values.astype(np.int64) // 10 ** 9\nprint (df)\n              datetime          ts\n0  2016-01-01 00:00:01  1451606401\n1  2016-01-01 01:00:01  1451610001\n2  2016-01-01 02:00:01  1451613601\n3  2016-01-01 03:00:01  1451617201\n4  2016-01-01 04:00:01  1451620801\n5  2016-01-01 05:00:01  1451624401\n6  2016-01-01 06:00:01  1451628001\n7  2016-01-01 07:00:01  1451631601\n8  2016-01-01 08:00:01  1451635201\n9  2016-01-01 09:00:01  1451638801\n10 2016-01-01 10:00:01  1451642401\n11 2016-01-01 11:00:01  1451646001\n12 2016-01-01 12:00:01  1451649601\n13 2016-01-01 13:00:01  1451653201\n14 2016-01-01 14:00:01  1451656801\n15 2016-01-01 15:00:01  1451660401\n16 2016-01-01 16:00:01  1451664001\n17 2016-01-01 17:00:01  1451667601\n18 2016-01-01 18:00:01  1451671201\n19 2016-01-01 19:00:01  1451674801\n20 2016-01-01 20:00:01  1451678401\n21 2016-01-01 21:00:01  1451682001\n22 2016-01-01 22:00:01  1451685601\n23 2016-01-01 23:00:01  1451689201\n24 2016-01-02 00:00:01  1451692801\n"
"df = pd.DataFrame(dict(\n        A=[1, 1, 1, 2, 2, 2, 2, 3, 4, 4],\n        B=range(10)\n    ))\n\ndf.groupby('A', group_keys=False).apply(lambda x: x.sample(min(len(x), 2)))\n\n   A  B\n1  1  1\n2  1  2\n3  2  3\n6  2  6\n7  3  7\n9  4  9\n8  4  8\n"
"In [11]: df\nOut[11]: \n            A        B        C        D      \n2000-01-03 -0.59885 -0.18141 -0.68828 -0.77572\n2000-01-04  0.83935  0.15993  0.95911 -1.12959\n2000-01-05  2.80215 -0.10858 -1.62114 -0.20170\n2000-01-06  0.71670 -0.26707  1.36029  1.74254\n2000-01-07 -0.45749  0.22750  0.46291 -0.58431\n2000-01-10 -0.78702  0.44006 -0.36881 -0.13884\n2000-01-11  0.79577 -0.09198  0.14119  0.02668\n2000-01-12 -0.32297  0.62332  1.93595  0.78024\n2000-01-13  1.74683 -1.57738 -0.02134  0.11596\n2000-01-14 -0.55613  0.92145 -0.22832  1.56631\n2000-01-17 -0.55233 -0.28859 -1.18190 -0.80723\n2000-01-18  0.73274  0.24387  0.88146 -0.94490\n2000-01-19  0.56644 -0.49321  1.17584 -0.17585\n2000-01-20  1.56441  0.62331 -0.26904  0.11952\n2000-01-21  0.61834  0.17463 -1.62439  0.99103\n2000-01-24  0.86378 -0.68111 -0.15788 -0.16670\n2000-01-25 -1.12230 -0.16128  1.20401  1.08945\n2000-01-26 -0.63115  0.76077 -0.92795 -2.17118\n2000-01-27  1.37620 -1.10618 -0.37411  0.73780\n2000-01-28 -1.40276  1.98372  1.47096 -1.38043\n2000-01-31  0.54769  0.44100 -0.52775  0.84497\n2000-02-01  0.12443  0.32880 -0.71361  1.31778\n2000-02-02 -0.28986 -0.63931  0.88333 -2.58943\n2000-02-03  0.54408  1.17928 -0.26795 -0.51681\n2000-02-04 -0.07068 -1.29168 -0.59877 -1.45639\n2000-02-07 -0.65483 -0.29584 -0.02722  0.31270\n2000-02-08 -0.18529 -0.18701 -0.59132 -1.15239\n2000-02-09 -2.28496  0.36352  1.11596  0.02293\n2000-02-10  0.51054  0.97249  1.74501  0.20525\n2000-02-11  0.10100  0.27722  0.65843  1.73591\n\nIn [12]: df[(df.values &gt; 1.5).any(1)]\nOut[12]: \n            A       B       C        D     \n2000-01-05  2.8021 -0.1086 -1.62114 -0.2017\n2000-01-06  0.7167 -0.2671  1.36029  1.7425\n2000-01-12 -0.3230  0.6233  1.93595  0.7802\n2000-01-13  1.7468 -1.5774 -0.02134  0.1160\n2000-01-14 -0.5561  0.9215 -0.22832  1.5663\n2000-01-20  1.5644  0.6233 -0.26904  0.1195\n2000-01-28 -1.4028  1.9837  1.47096 -1.3804\n2000-02-10  0.5105  0.9725  1.74501  0.2052\n2000-02-11  0.1010  0.2772  0.65843  1.7359\n\nIn [13]: df[(df['A'] &gt; 1) | (df['B'] &lt; -1)]\nOut[13]: \n            A        B       C        D     \n2000-01-05  2.80215 -0.1086 -1.62114 -0.2017\n2000-01-13  1.74683 -1.5774 -0.02134  0.1160\n2000-01-20  1.56441  0.6233 -0.26904  0.1195\n2000-01-27  1.37620 -1.1062 -0.37411  0.7378\n2000-02-04 -0.07068 -1.2917 -0.59877 -1.4564\n"
'In [29]: df.drop_duplicates()\nOut[29]: \n   b  c\n1  2  3\n3  4  0\n7  5  9\n'
"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ntestdataframe = pd.DataFrame(np.arange(12).reshape(4,3), columns=['A', 'B', 'C'])\nstyles = ['bs-','ro-','y^-']\nlinewidths = [2, 1, 4]\nfig, ax = plt.subplots()\nfor col, style, lw in zip(testdataframe.columns, styles, linewidths):\n    testdataframe[col].plot(style=style, lw=lw, ax=ax)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\ntestdataframe1 = pd.DataFrame(np.arange(12).reshape(4,3), columns=['A', 'B', 'C'])\ntestdataframe2 = pd.DataFrame(np.random.normal(size=(4,3)), columns=['D', 'E', 'F'])\nstyles1 = ['bs-','ro-','y^-']\nstyles2 = ['rs-','go-','b^-']\nfig, ax = plt.subplots()\ntestdataframe1.plot(style=styles1, ax=ax)\ntestdataframe2.plot(style=styles2, ax=ax)\n"
'&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; df.head()\n                                 ID                DATE\n8  0103bd73af66e5a44f7867c0bb2203cc 2001-02-01 00:00:00\n0  002691c9cec109e64558848f1358ac16 2003-08-13 00:00:00\n1  002691c9cec109e64558848f1358ac16 2003-08-13 00:00:00\n5  00d34668025906d55ae2e529615f530a 2006-03-09 00:00:00\n4  00d34668025906d55ae2e529615f530a 2006-03-09 00:00:00\n&gt;&gt;&gt; df["X_DATE"] = df["DATE"] + timedelta(days=180)\n&gt;&gt;&gt; df.head()\n                                 ID                DATE              X_DATE\n8  0103bd73af66e5a44f7867c0bb2203cc 2001-02-01 00:00:00 2001-07-31 00:00:00\n0  002691c9cec109e64558848f1358ac16 2003-08-13 00:00:00 2004-02-09 00:00:00\n1  002691c9cec109e64558848f1358ac16 2003-08-13 00:00:00 2004-02-09 00:00:00\n5  00d34668025906d55ae2e529615f530a 2006-03-09 00:00:00 2006-09-05 00:00:00\n4  00d34668025906d55ae2e529615f530a 2006-03-09 00:00:00 2006-09-05 00:00:00\n'
"dataframe.tz_localize('UTC', level=0)\n"
"d[(d['x']&gt;2) &amp; (d['y']&gt;7)]\n"
'df.div(df.sum(axis=1), axis=0)\n'
"def gen():\n    lines = [\n        'col1,col2\\n',\n        'foo,bar\\n',\n        'foo,baz\\n',\n        'bar,baz\\n'\n    ]\n    for line in lines:\n        yield line\n\nclass Reader(object):\n    def __init__(self, g):\n        self.g = g\n    def read(self, n=0):\n        try:\n            return next(self.g)\n        except StopIteration:\n            return ''\n\n&gt;&gt;&gt; pd.read_csv(Reader(gen()))\n  col1 col2\n0  foo  bar\n1  foo  baz\n2  bar  baz\n"
'&gt;&gt;&gt; df\n\n   0  1  2\n0  1  9  0\n1  2  7  0\n2  3  7  0\n\n&gt;&gt;&gt; df.loc[:, (df != df.iloc[0]).any()] \n\n   0  1\n0  1  9\n1  2  7\n2  3  7\n'
"&gt;&gt;&gt; ld = pd.DataFrame(labeldict).T\n&gt;&gt;&gt; ld.columns = ['color', 'size']\n&gt;&gt;&gt; ld.index.name = 'label'\n&gt;&gt;&gt; df.merge(ld.reset_index(), on='label')\n  label  somedata  color    size\n0     b  1.462108    red  medium\n1     c -2.060141  green   small\n2     c  1.133769  green   small\n3     c  0.042214  green   small\n4     e -0.322417    red  medium\n5     e -1.099891    red  medium\n6     e -0.877858    red  medium\n7     e  0.582815    red  medium\n8     f -0.384054    red   large\n9     d -0.172428    red  medium\n"
"df.mycol.get(myIndex, NaN)\n\nIn [117]:\n\ndf = pd.DataFrame({'mycol':arange(5), 'dummy':arange(5)})\ndf\nOut[117]:\n   dummy  mycol\n0      0      0\n1      1      1\n2      2      2\n3      3      3\n4      4      4\n\n[5 rows x 2 columns]\nIn [118]:\n\nprint(df.mycol.get(2, NaN))\nprint(df.mycol.get(5, NaN))\n2\nnan\n"
"In [46]: s = Series(list('aaabbbccddefgh')).astype('category')\n\nIn [47]: s\nOut[47]: \n0     a\n1     a\n2     a\n3     b\n4     b\n5     b\n6     c\n7     c\n8     d\n9     d\n10    e\n11    f\n12    g\n13    h\ndtype: category\nCategories (8, object): [a &lt; b &lt; c &lt; d &lt; e &lt; f &lt; g &lt; h]\n\nIn [48]: df = pd.get_dummies(s)\n\nIn [49]: df\nOut[49]: \n    a  b  c  d  e  f  g  h\n0   1  0  0  0  0  0  0  0\n1   1  0  0  0  0  0  0  0\n2   1  0  0  0  0  0  0  0\n3   0  1  0  0  0  0  0  0\n4   0  1  0  0  0  0  0  0\n5   0  1  0  0  0  0  0  0\n6   0  0  1  0  0  0  0  0\n7   0  0  1  0  0  0  0  0\n8   0  0  0  1  0  0  0  0\n9   0  0  0  1  0  0  0  0\n10  0  0  0  0  1  0  0  0\n11  0  0  0  0  0  1  0  0\n12  0  0  0  0  0  0  1  0\n13  0  0  0  0  0  0  0  1\n\nIn [50]: x = df.stack()\n\n# I don't think you actually need to specify ALL of the categories here, as by definition\n# they are in the dummy matrix to start (and hence the column index)\nIn [51]: Series(pd.Categorical(x[x!=0].index.get_level_values(1)))\nOut[51]: \n0     a\n1     a\n2     a\n3     b\n4     b\n5     b\n6     c\n7     c\n8     d\n9     d\n10    e\n11    f\n12    g\n13    h\nName: level_1, dtype: category\nCategories (8, object): [a &lt; b &lt; c &lt; d &lt; e &lt; f &lt; g &lt; h]\n"
"ax = point.plot(x='x', y='y', ax=ax, style='r-', label='point')\n\nax = point.plot(x='x', y='y', ax=ax, style='bx', label='point')\n"
'[\n{"ID":"12345","Timestamp":"20140101", "Usefulness":"Yes",\n  "Code":[{"event1":"A","result":"1"},…]},\n{"ID":"1A35B","Timestamp":"20140102", "Usefulness":"No",\n  "Code":[{"event1":"B","result":"1"},…]},\n{"ID":"AA356","Timestamp":"20140103", "Usefulness":"No",\n  "Code":[{"event1":"B","result":"0"},…]},\n...\n]\n\nimport json\n\nwith open(\'file.json\') as json_file:\n\n    data = json.load(json_file)\n\ndata[0]["ID"]\n'
"test3 = pd.concat([test1, test2], axis=1)\ntest3.columns = ['a','b']\n"
"In [66]: cols = df.columns\n\nIn [67]: num_cols = df._get_numeric_data().columns\n\nIn [68]: num_cols\nOut[68]: Index([u'0', u'1', u'2'], dtype='object')\n\nIn [69]: list(set(cols) - set(num_cols))\nOut[69]: ['3', '4']\n"
'In [11]: df.groupby(["item", "color"]).count()\nOut[11]:\n             id\nitem  color\ncar   black   2\ntruck blue    1\n      red     2\n\nIn [12]: df.groupby(["item", "color"])["id"].count().reset_index(name="count")\nOut[12]:\n    item  color  count\n0    car  black      2\n1  truck   blue      1\n2  truck    red      2\n\nIn [13]: df.groupby(["item", "color"])["id"].transform("count")\nOut[13]:\n0    2\n1    2\n2    2\n3    1\n4    2\ndtype: int64\n'
"In [81]: df.groupby('Column1')['Column3'].apply(list).to_dict()\nOut[81]: {0: [1], 1: [2, 3, 5], 2: [1, 2], 3: [4, 5], 4: [1], 5: [1, 2, 3]}\n\nIn [433]: {k: list(v) for k, v in df.groupby('Column1')['Column3']}\nOut[433]: {0: [1], 1: [2, 3, 5], 2: [1, 2], 3: [4, 5], 4: [1], 5: [1, 2, 3]}\n"
"df['1/2 ID'] = map(lambda x: str(x).upper(), df['1/2 ID'])\n\ndf.columns = map(lambda x: str(x).upper(), df.columns)\n"
'In [11]: d = {"response": {"body": {"contact": {"email": "mr@abc.com", "mobile_number": "0123456789"}, "personal": {"last_name": "Muster", "gender": "m", "first_name": "Max", "dob": "1985-12-23", "family_status": "single", "title": "Dr."}, "customer": {"verified": "true", "customer_id": "1234567"}}, "token": "dsfgf", "version": "1.1"}}\n\nIn [12]: df = pd.json_normalize(d)\n\nIn [13]: df.columns = df.columns.map(lambda x: x.split(".")[-1])\n\nIn [14]: df\nOut[14]:\n        email mobile_number customer_id verified         dob family_status first_name gender last_name title  token version\n0  mr@abc.com    0123456789     1234567     true  1985-12-23        single        Max      m    Muster   Dr.  dsfgf     1.1\n'
"print df['feedback_id'].combine_first(df['_id'])\n0    568a8c25cac4991645c287ac\n1    568df45b177e30c6487d3603\n2    568df434832b090048f34974\n3    568cd22e9e82dfc166d7dff1\n4    568df3f0832b090048f34711\n5    568e5a38b4a797c664143dda\nName: feedback_id, dtype: object\n\nprint df['feedback_id'].fillna(df['_id'])\n0    568a8c25cac4991645c287ac\n1    568df45b177e30c6487d3603\n2    568df434832b090048f34974\n3    568cd22e9e82dfc166d7dff1\n4    568df3f0832b090048f34711\n5    568e5a38b4a797c664143dda\nName: feedback_id, dtype: object\n"
'df2.plot(figsize=(20,10))\n'
'In [1]: s = pd.Series([1, 2, 3, 1, 1, 4])\n\nIn [2]: s.unique()\nOut[2]: array([1, 2, 3, 4])\n\nIn [3]: set(s)\nOut[3]: {1, 2, 3, 4}\n'
'import pandas as pd\nimport numpy as np\n# Get some time series data\ndf = pd.read_csv("https://raw.githubusercontent.com/plotly/datasets/master/timeseries.csv")\ndf.head()\n\nDate      A       B       C      D      E      F      G\n0   2008-03-18  24.68  164.93  114.73  26.27  19.21  28.87  63.44\n1   2008-03-19  24.18  164.89  114.75  26.22  19.07  27.76  59.98\n2   2008-03-20  23.99  164.63  115.04  25.78  19.01  27.04  59.61\n3   2008-03-25  24.14  163.92  114.85  27.41  19.61  27.84  59.41\n4   2008-03-26  24.44  163.45  114.84  26.86  19.53  28.02  60.09\n\n# Put your inputs into a single list\ndf[\'single_input_vector\'] = df[input_cols].apply(tuple, axis=1).apply(list)\n# Double-encapsulate list so that you can sum it in the next step and keep time steps as separate elements\ndf[\'single_input_vector\'] = df.single_input_vector.apply(lambda x: [list(x)])\n# Use .cumsum() to include previous row vectors in the current row list of vectors\ndf[\'cumulative_input_vectors\'] = df.single_input_vector.cumsum()\n\n# If your output is multi-dimensional, you need to capture those dimensions in one object\n# If your output is a single dimension, this step may be unnecessary\ndf[\'output_vector\'] = df[output_cols].apply(tuple, axis=1).apply(list)\n\n# Pad your sequences so they are the same length\nfrom keras.preprocessing.sequence import pad_sequences\n\nmax_sequence_length = df.cumulative_input_vectors.apply(len).max()\n# Save it as a list   \npadded_sequences = pad_sequences(df.cumulative_input_vectors.tolist(), max_sequence_length).tolist()\ndf[\'padded_input_vectors\'] = pd.Series(padded_sequences).apply(np.asarray)\n\n# Extract your training data\nX_train_init = np.asarray(df.padded_input_vectors)\n# Use hstack to and reshape to make the inputs a 3d vector\nX_train = np.hstack(X_train_init).reshape(len(df),max_sequence_length,len(input_cols))\ny_train = np.hstack(np.asarray(df.output_vector)).reshape(len(df),len(output_cols))\n\n&gt;&gt;&gt; print(X_train_init.shape)\n(11,)\n&gt;&gt;&gt; print(X_train.shape)\n(11, 11, 6)\n&gt;&gt;&gt; print(X_train == X_train_init)\nFalse\n\n# Get your input dimensions\n# Input length is the length for one input sequence (i.e. the number of rows for your sample)\n# Input dim is the number of dimensions in one input vector (i.e. number of input columns)\ninput_length = X_train.shape[1]\ninput_dim = X_train.shape[2]\n# Output dimensions is the shape of a single output vector\n# In this case it\'s just 1, but it could be more\noutput_dim = len(y_train[0])\n\nfrom keras.models import Model, Sequential\nfrom keras.layers import LSTM, Dense\n\n# Build the model\nmodel = Sequential()\n\n# I arbitrarily picked the output dimensions as 4\nmodel.add(LSTM(4, input_dim = input_dim, input_length = input_length))\n# The max output value is &gt; 1 so relu is used as final activation.\nmodel.add(Dense(output_dim, activation=\'relu\'))\n\nmodel.compile(loss=\'mean_squared_error\',\n              optimizer=\'sgd\',\n              metrics=[\'accuracy\'])\n\n# Set batch_size to 7 to show that it doesn\'t have to be a factor or multiple of your sample size\nhistory = model.fit(X_train, y_train,\n              batch_size=7, nb_epoch=3,\n              verbose = 1)\n\nEpoch 1/3\n11/11 [==============================] - 0s - loss: 3498.5756 - acc: 0.0000e+00     \nEpoch 2/3\n11/11 [==============================] - 0s - loss: 3498.5755 - acc: 0.0000e+00     \nEpoch 3/3\n11/11 [==============================] - 0s - loss: 3498.5757 - acc: 0.0000e+00 \n'
"contrib_df[&quot;AMNT&quot;].describe().apply(lambda x: format(x, 'f'))\n\ndf.describe().apply(lambda s: s.apply('{0:.5f}'.format))\n\ncontrib_df.describe().apply(lambda s: s.apply(lambda x: format(x, 'g')))\n\ncount    9.500000e+01\nmean     5.621943e+05\nstd      2.716369e+06\nmin      4.770000e+02\n25%      2.118160e+05\n50%      2.599960e+05\n75%      3.121170e+05\nmax      2.670423e+07\nName: salary, dtype: float64\n\ncount          95.000000\nmean       562194.294737\nstd       2716369.154553\nmin           477.000000\n25%        211816.000000\n50%        259996.000000\n75%        312117.000000\nmax      26704229.000000\nName: salary, dtype: object\n"
"In[33]: import pandas as pd\nIn[34]: df1 = pd.DataFrame([1,2,3,4,5])\nIn[35]: id(df1)\nOut[35]: 4541269200\n\nIn[36]: df2 = df1\nIn[37]: id(df2)\nOut[37]: 4541269200  # Same id as df1\n\nIn[38]: df3 = df1.copy()\nIn[39]: id(df3)\nOut[39]: 4541269584  # New object, new id.\n\nIn[40]: df4 = df1.copy(deep=False)\nIn[41]: id(df4)\nOut[41]: 4541269072  # New object, new id.\n\nIn[42]: df1 = pd.DataFrame([9, 9, 9])\nIn[43]: id(df1)\nOut[43]: 4541271120  # New object created and bound to name 'df1'.\n\nIn[44]: id(df2)\nOut[44]: 4541269200  # Old object's id not impacted.\n\nIn[10]: arr1 = [1, 2, 3]\nIn[11]: arr2 = [1, 2, 3, 4]\nIn[12]: df1 = pd.DataFrame([[arr1], [arr2]], columns=['A'])\nIn[13]: df1.applymap(id)\nOut[13]: \n            A\n0  4515714832\n1  4515734952\n\nIn[14]: df2 = df1.copy(deep=True)\nIn[15]: df2.applymap(id)\nOut[15]: \n            A\n0  4515714832\n1  4515734952\n\nIn[16]: df2.loc[0, 'A'].append(55)\nIn[17]: df2\nOut[17]: \n               A\n0  [1, 2, 3, 55]\n1   [1, 2, 3, 4]\nIn[18]: df1\nOut[18]: \n               A\n0  [1, 2, 3, 55]\n1   [1, 2, 3, 4]\n"
'from pandas import *\n\nP1Channels = data.filter(regex="P1")\nP1Sum = P1Channels.sum(axis=1)\n'
'from pandas.tseries.offsets import *\n\nIn [185]: s\nOut[185]: \n2011-01-01   -0.011629\n2011-01-02   -0.089666\n2011-01-03   -1.314430\n2011-01-04   -1.867307\n2011-01-05    0.779609\n2011-01-06    0.588950\n2011-01-07   -2.505803\n2011-01-08    0.800262\n2011-01-09    0.376406\n2011-01-10   -0.469988\nFreq: D\n\nIn [186]: s.asfreq(BDay())\nOut[186]: \n2011-01-03   -1.314430\n2011-01-04   -1.867307\n2011-01-05    0.779609\n2011-01-06    0.588950\n2011-01-07   -2.505803\n2011-01-10   -0.469988\nFreq: B\n\nIn [187]: x=datetime(2011, 1, 5)\n\nIn [188]: y=datetime(2011, 1, 9)\n\nIn [189]: s.ix[x:y]\nOut[189]: \n2011-01-05    0.779609\n2011-01-06    0.588950\n2011-01-07   -2.505803\n2011-01-08    0.800262\n2011-01-09    0.376406\nFreq: D\n\nIn [190]: s.ix[x:y].asfreq(BDay())\nOut[190]: \n2011-01-05    0.779609\n2011-01-06    0.588950\n2011-01-07   -2.505803\nFreq: B\n\nIn [191]: s.ix[x:y].asfreq(BDay()).count()\nOut[191]: 3\n'
'&gt;&gt;&gt; df\n              val1  val2  val3\ncity_id                       \nhouston,tx       1     2     0\nhouston,tx       0     0     1\nhouston,tx       2     1     1\nsomewhere,ew     4     3     7\n\n&gt;&gt;&gt; df.groupby(df.index).sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n\n&gt;&gt;&gt; df.reset_index().groupby("city_id").sum()\n              val1  val2  val3\ncity_id                       \nhouston,tx       3     3     2\nsomewhere,ew     4     3     7\n\n&gt;&gt;&gt; df.groupby(df.index)\n&lt;pandas.core.groupby.DataFrameGroupBy object at 0x1045a1790&gt;\n&gt;&gt;&gt; df.groupby(df.index).max()\n              val1  val2  val3\ncity_id                       \nhouston,tx       2     2     1\nsomewhere,ew     4     3     7\n&gt;&gt;&gt; df.groupby(df.index).mean()\n              val1  val2      val3\ncity_id                           \nhouston,tx       1     1  0.666667\nsomewhere,ew     4     3  7.000000\n'
"df_data['vals'] = df_data['vals'].map(lambda x: '%2.1f' % x)\n\ndf_data.to_csv(outfile, index=False, header=False, float_format='%11.6f')\n"
'In [36]: pd.concat([noclickDF, clickDF], ignore_index=True)\nOut[36]: \n   click    id  location\n0      0   123       321\n1      0  1543       432\n2      1   421       123\n3      1   436      1543\n'
'header = ["InviteTime (Oracle)", "Orig Number", "Orig IP Address", "Dest Number"]\ndf.to_csv(\'output.csv\', columns = header)\n'
"class DenseTransformer(TransformerMixin):\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\npipeline = Pipeline([\n     ('vectorizer', CountVectorizer()), \n     ('to_dense', DenseTransformer()), \n     ('classifier', RandomForestClassifier())\n])\n\nfrom sklearn.svm import LinearSVC\npipeline = Pipeline([('vectorizer', CountVectorizer()), ('classifier', LinearSVC())])\n"
"from pandas import Series\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nys = [[0,1,2,3,4],[4,3,2,1,0]]\nx_ax = [0,1,2,3,4]\n\nfor y_ax in ys:\n    ts = Series(y_ax,index=x_ax)\n    ts.plot(kind='bar', figsize=(15,5))\n    plt.show()\n"
"import numpy as np\nimport pandas as pd\nx = np.random.randn(5)\ny = np.sin(x)\ndf = pd.DataFrame({'x':x, 'y':y})\ndf.plot('x', 'y', kind='scatter')\n"
"np.where(consumption_energy &gt; 400, 'high', \n         (np.where(consumption_energy &lt; 200, 'low', 'medium')))\n"
"In [84]: df\nOut[84]:\n               A               B\n0     some value      [[L1, L2]]\n1  another value  [[L3, L4, L5]]\n\nIn [85]: (df['B'].apply(lambda x: pd.Series(x[0]))\n   ....:         .stack()\n   ....:         .reset_index(level=1, drop=True)\n   ....:         .to_frame('B')\n   ....:         .join(df[['A']], how='left')\n   ....: )\nOut[85]:\n    B              A\n0  L1     some value\n0  L2     some value\n1  L3  another value\n1  L4  another value\n1  L5  another value\n"
'df = DataFrame(series).transpose()\n\n    a   b   c\n0   1   2   3\n'
'df_all.dropna(inplace=True)\ndf_all.reset_index(drop=True, inplace=True)\n\ndf_all = df_all.dropna()\ndf_all = df_all.reset_index(drop=True)\n'
'C:\\&gt; py      -m pip install pandas  %= one of Python on the system =%\nC:\\&gt; py -2   -m pip install pandas  %= one of Python 2 on the system =%\nC:\\&gt; py -2.7 -m pip install pandas  %= only for Python 2.7 =%\nC:\\&gt; py -3   -m pip install pandas  %= one of Python 3 on the system =%\nC:\\&gt; py -3.6 -m pip install pandas  %= only for Python 3.6 =%\n\nC:\\&gt; setx PATH "%PATH%;C:\\&lt;path\\to\\python\\folder&gt;\\Scripts"\n\nC:\\&gt; pip install pandas\n\nconnection error: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed\n\nC:\\&gt; py -m pip install --trusted-host pypi.python.org pip pandas\n\nPermissionError: [WinError 5] Access is denied\n\nC:\\&gt; py -m pip install --user pandas\n\nC:\\&gt; py -m venv c:\\path\\to\\new\\venv\nC:\\&gt; &lt;path\\to\\the\\new\\venv&gt;\\Scripts\\activate.bat\n'
"ax = sns.boxplot(x='categories', y='oxygen', hue='target', data=df)\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\n\nax = sns.boxplot(x='categories', y='oxygen', hue='target', data=df)\nplt.setp(ax.get_xticklabels(), rotation=45)\n"
"╔═════════╦═══════════════╗\n║ Student ║ Date          ║\n╠═════════╬═══════════════╣\n║ Joe     ║ December 2017 ║\n╠═════════╬═══════════════╣\n║ Bob     ║ April 2018    ║\n╠═════════╬═══════════════╣\n║ Joe     ║ December 2018 ║\n╚═════════╩═══════════════╝\n\nboolean = not df[&quot;Student&quot;].is_unique      # True (credit to @Carsten)\nboolean = df['Student'].duplicated().any() # True\n\nboolean = df.duplicated(subset=['Student']).any() # True\n# We were expecting True, as Joe can be seen twice.\n\nboolean = df.duplicated().any() # False\nboolean = df.duplicated(subset=['Student','Date']).any() # False\n# We were expecting False here - no duplicates row-wise \n# ie. Joe Dec 2017, Joe Dec 2018\n\nimport pandas as pd\nimport io\n\ndata = '''\\\nStudent,Date\nJoe,December 2017\nBob,April 2018\nJoe,December 2018'''\n\ndf = pd.read_csv(io.StringIO(data), sep=',')\n\n# Approach 1: Simple True/False\nboolean = df.duplicated(subset=['Student']).any()\nprint(boolean, end='\\n\\n') # True\n\n# Approach 2: First store boolean array, check then remove\nduplicate_in_student = df.duplicated(subset=['Student'])\nif duplicate_in_student.any():\n    print(df.loc[~duplicate_in_student], end='\\n\\n')\n\n# Approach 3: Use drop_duplicates method\ndf.drop_duplicates(subset=['Student'], inplace=True)\nprint(df)\n\nTrue\n\n  Student           Date\n0     Joe  December 2017\n1     Bob     April 2018\n\n  Student           Date\n0     Joe  December 2017\n1     Bob     April 2018\n"
'd1 = pd.DataFrame([[1,2], [10, 20]], index=[0,2])\nd2 = pd.DataFrame([[1, 2], [10, 20]], index=[0, 1])\nnp.array_equal(d1.values,d2.values)\nOut[759]: True\n'
"import pandas as pd\n\ndf = pd.DataFrame({\n    'name': ['Jason'],\n    'coverage': [25.1]\n})\n\nprint(df.to_dict())\nprint(df.set_index('name')['coverage'].to_dict())\nprint(df.to_dict('r'))\n\n{'name': {0: 'Jason'}, 'coverage': {0: 25.1}}\n{'Jason': 25.1}\n[{'name': 'Jason', 'coverage': 25.1}]\n"
'import pandas as pd\nimport numpy as np\n\nnp.arrays = [[\'one\',\'one\',\'one\',\'two\',\'two\',\'two\'],[1,2,3,1,2,3]]\n\ndf = pd.DataFrame(np.random.randn(6,2),index=pd.MultiIndex.from_tuples(list(zip(*np.arrays))),columns=[\'A\',\'B\'])\n\ndf  # This is the dataframe we have generated\n\n          A         B\none 1 -0.732470 -0.313871\n    2 -0.031109 -2.068794\n    3  1.520652  0.471764\ntwo 1 -0.101713 -1.204458\n    2  0.958008 -0.455419\n    3 -0.191702 -0.915983\n\ndf.ndim\n\n2\n\nIn [44]: df.ix["one"]\nOut[44]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n\nIn [45]: df.ix["one"].ix[1]\nOut[45]: \nA   -0.732470\nB   -0.313871\nName: 1\n\nIn [46]: df.ix["one"].ix[1]["A"]\nOut[46]: -0.73247029752040727\n\nIn [47]: df.xs(\'one\')\nOut[47]: \n          A         B\n1 -0.732470 -0.313871\n2 -0.031109 -2.068794\n3  1.520652  0.471764\n\nIn [48]: df.xs(\'B\', axis=1)\nOut[48]: \none  1   -0.313871\n     2   -2.068794\n     3    0.471764\ntwo  1   -1.204458\n     2   -0.455419\n     3   -0.915983\nName: B\n'
"&gt;&gt;&gt; df = DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n&gt;&gt;&gt;                         'foo', 'bar', 'foo', 'foo'],\n...                  'B' : ['one', 'one', 'two', 'three',\n...                         'two', 'two', 'one', 'three'],\n...                  'C' : randn(8), 'D' : randn(8)})\n&gt;&gt;&gt; grouped = df.groupby('A')\n&gt;&gt;&gt; grouped\n&lt;pandas.core.groupby.DataFrameGroupBy object at 0x04E2F630&gt;\n&gt;&gt;&gt; test = grouped.aggregate(np.sum)\n&gt;&gt;&gt; test\n            C         D\nA                      \nbar -1.852376  2.204224\nfoo -3.398196 -0.045082\n"
'In [9]: df = pandas.DataFrame([1,2,3,4], columns=["data"])\n\nIn [10]: df\nOut[10]: \n   data\n0     1\n1     2\n2     3\n3     4\n\nIn [11]: df["desired"] = df["data"] &gt; 2.5\nIn [11]: df\nOut[12]: \n   data desired\n0     1   False\n1     2   False\n2     3    True\n3     4    True\n'
'df.values.T.tolist()\n\n[list(l) for l in zip(*df.values)]\n\n[[0, 0, 1, 1, 1, 0],\n [1, 1, 0, 0, 0, 1],\n [1, 0, 0, 0, 1, 1],\n [0, 1, 1, 0, 0, 0],\n [0, 0, 0, 1, 0, 0],\n [0, 0, 1, 1, 1, 0],\n [1, 1, 0, 0, 0, 1]]\n'
"    import pandas\n    df = pandas.DataFrame(data, columns=['a','b','c','d'], index=['x','y','z'])\n    df = df.fillna(0)\n    df = df.astype(int)\n    df.to_csv('test.csv', sep='\\t')\n"
"In [1]: df = pd.DataFrame([[1,2], [3,4]], columns=['a', 'b'])\n\nIn [2]: df\nOut[2]:\n   a  b\n0  1  2\n1  3  4\n\nIn [3]: df['b']\nOut[3]:\n0    2\n1    4\nName: b, dtype: int64\n\nIn [4]: df[[1]]\nOut[4]:\n   b\n0  2\n1  4\n\nIn [5]: df.iloc[:, [1]]\nOut[5]:\n   b\n0  2\n1  4\n\nIn [6]: df.loc[:, ['b']]\nOut[6]:\n   b\n0  2\n1  4\n\nIn [7]: df.loc[:, 'b']\nOut[7]:\n0    2\n1    4\nName: b, dtype: int64\n"
"ax1 = df2250.plot()\nax2 = df2260.plot()\nax3 = df5.plot()\n\nax1.set_ylim(100000,500000)\nax2.set_ylim(100000,500000)\netc...\n\nax1 = df2250.plot()\ndf2260.plot(ax=ax1)\netc...\n\nfig, axs = plt.subplots(1,3,figsize=(10,4), subplot_kw={'ylim': (100000,500000)})\n\ndf2260.plot(ax=axs[0])\ndf2260.plot(ax=axs[1])\netc...\n"
'"nt|nv"  # rather than "nt" | " nv"\nf_recs[f_recs[\'Behavior\'].str.contains("nt|nv", na=False)]\n\nIn [1]: "nt" | "nv"\nTypeError: unsupported operand type(s) for |: \'str\' and \'str\'\n'
"In [6]: df = DataFrame(dict(A = date_range('20130101',periods=10)))\n\nIn [7]: df\nOut[7]: \n                    A\n0 2013-01-01 00:00:00\n1 2013-01-02 00:00:00\n2 2013-01-03 00:00:00\n3 2013-01-04 00:00:00\n4 2013-01-05 00:00:00\n5 2013-01-06 00:00:00\n6 2013-01-07 00:00:00\n7 2013-01-08 00:00:00\n8 2013-01-09 00:00:00\n9 2013-01-10 00:00:00\n\nIn [8]: df['A'].apply(lambda x: x.strftime('%d%m%Y'))\nOut[8]: \n0    01012013\n1    02012013\n2    03012013\n3    04012013\n4    05012013\n5    06012013\n6    07012013\n7    08012013\n8    09012013\n9    10012013\nName: A, dtype: object\n"
'In [1]: from IPython.display import HTML\n\nIn [2]: df = pd.DataFrame(list(range(5)), columns=[\'a\'])\n\nIn [3]: df[\'a\'] = df[\'a\'].apply(lambda x: \'&lt;a href="http://example.com/{0}"&gt;link&lt;/a&gt;\'.format(x))\n\nIn [4]: HTML(df.to_html(escape=False))\n'
"out.apply(pd.Series)\n\nout.columns=['Kstats','Pvalue']\n\nout.index.name=None\n"
'In [21]: df.values[[np.arange(df.shape[0])]*2] = 0\n\nIn [22]: df\nOut[22]: \n          0         1         2         3         4\n0  0.000000  0.931374  0.604412  0.863842  0.280339\n1  0.531528  0.000000  0.641094  0.204686  0.997020\n2  0.137725  0.037867  0.000000  0.983432  0.458053\n3  0.594542  0.943542  0.826738  0.000000  0.753240\n4  0.357736  0.689262  0.014773  0.446046  0.000000\n\nIn [36]: np.fill_diagonal(df.values, 0)\n'
"In [62]:\n\ndf['b'].notnull()\n\nOut[62]:\n0     True\n1    False\n2     True\n3     True\n4     True\nName: b, dtype: bool\nIn [63]:\n\ndf[df['b'].notnull()]\nOut[63]:\n   A   b   c\n0  1  q1   1\n2  3  q2   3\n3  4  q1 NaN\n4  5  q2   7\n"
'In [27]: data.groupby(level=[0, 1]).sum()\nOut[27]:\nA  B    277\n   b     37\na  B    159\n   b     16\ndtype: int64\n'
"In [92]:\n\nt = 20070530\npd.to_datetime(str(t), format='%Y%m%d')\nOut[92]:\nTimestamp('2007-05-30 00:00:00')\n\nIn [94]:\n\nt = 20070530\ndf = pd.DataFrame({'date':[t]*10})\ndf\nOut[94]:\n       date\n0  20070530\n1  20070530\n2  20070530\n3  20070530\n4  20070530\n5  20070530\n6  20070530\n7  20070530\n8  20070530\n9  20070530\nIn [98]:\n\ndf['DateTime'] = df['date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\ndf\nOut[98]:\n       date   DateTime\n0  20070530 2007-05-30\n1  20070530 2007-05-30\n2  20070530 2007-05-30\n3  20070530 2007-05-30\n4  20070530 2007-05-30\n5  20070530 2007-05-30\n6  20070530 2007-05-30\n7  20070530 2007-05-30\n8  20070530 2007-05-30\n9  20070530 2007-05-30\nIn [99]:\n\ndf.dtypes\nOut[99]:\ndate                 int64\nDateTime    datetime64[ns]\ndtype: object\n\nIn [102]:\n\ndf['DateTime'] = pd.to_datetime(df['date'].astype(str), format='%Y%m%d')\ndf\nOut[102]:\n       date   DateTime\n0  20070530 2007-05-30\n1  20070530 2007-05-30\n2  20070530 2007-05-30\n3  20070530 2007-05-30\n4  20070530 2007-05-30\n5  20070530 2007-05-30\n6  20070530 2007-05-30\n7  20070530 2007-05-30\n8  20070530 2007-05-30\n9  20070530 2007-05-30\n\nIn [104]:\n\n%timeit df['date'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n\n100 loops, best of 3: 2.55 ms per loop\nIn [105]:\n\n%timeit pd.to_datetime(df['date'].astype(str), format='%Y%m%d')\n1000 loops, best of 3: 396 µs per loop\n"
"In [64]: df\nOut[64]:\n       mydate     mytime\n0  2011-01-01 2011-11-14\n1  2011-01-02 2011-11-15\n2  2011-01-03 2011-11-16\n3  2011-01-04 2011-11-17\n4  2011-01-05 2011-11-18\n5  2011-01-06 2011-11-19\n6  2011-01-07 2011-11-20\n7  2011-01-08 2011-11-21\n8  2011-01-09 2011-11-22\n9  2011-01-10 2011-11-23\n10 2011-01-11 2011-11-24\n11 2011-01-12 2011-11-25\n\nIn [65]: lambdafunc = lambda x: pd.Series([x['mytime'].hour,\n                                           x['mydate'].isocalendar()[1],\n                                           x['mydate'].weekday()])\n\nIn [66]: df[['hour', 'weekday', 'weeknum']] = df.apply(lambdafunc, axis=1)\n\nIn [67]: df\nOut[67]:\n       mydate     mytime  hour  weekday  weeknum\n0  2011-01-01 2011-11-14     0       52        5\n1  2011-01-02 2011-11-15     0       52        6\n2  2011-01-03 2011-11-16     0        1        0\n3  2011-01-04 2011-11-17     0        1        1\n4  2011-01-05 2011-11-18     0        1        2\n5  2011-01-06 2011-11-19     0        1        3\n6  2011-01-07 2011-11-20     0        1        4\n7  2011-01-08 2011-11-21     0        1        5\n8  2011-01-09 2011-11-22     0        1        6\n9  2011-01-10 2011-11-23     0        2        0\n10 2011-01-11 2011-11-24     0        2        1\n11 2011-01-12 2011-11-25     0        2        2\n"
"In[88]:\ndf['NEW_DATE'] = df['ACC_DATE'] - pd.DateOffset(years=1)\ndf\n\nOut[88]: \n        ACC_DATE   NEW_DATE\nindex                      \n538   2006-04-07 2005-04-07\n550   2006-04-12 2005-04-12\n"
"x = x.copy()\n\nx.loc[:,'Mass32s'] = pandas.rolling_mean(x.Mass32, 5).shift(-2)\n"
"In [97]: df = DataFrame(np.random.randn(100000,20))\n\nIn [98]: df['B'] = 'foo'\n\nIn [99]: df['C'] = pd.Timestamp('20130101')\n\nIn [103]: df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 100000 entries, 0 to 99999\nData columns (total 22 columns):\n0     100000 non-null float64\n1     100000 non-null float64\n2     100000 non-null float64\n3     100000 non-null float64\n4     100000 non-null float64\n5     100000 non-null float64\n6     100000 non-null float64\n7     100000 non-null float64\n8     100000 non-null float64\n9     100000 non-null float64\n10    100000 non-null float64\n11    100000 non-null float64\n12    100000 non-null float64\n13    100000 non-null float64\n14    100000 non-null float64\n15    100000 non-null float64\n16    100000 non-null float64\n17    100000 non-null float64\n18    100000 non-null float64\n19    100000 non-null float64\nB     100000 non-null object\nC     100000 non-null datetime64[ns]\ndtypes: datetime64[ns](1), float64(20), object(1)\nmemory usage: 17.5+ MB\n\nIn [85]: def f1():\n   ....:     result = df\n   ....:     for i in range(9):\n   ....:         result = result.append(df)\n   ....:     return result\n   ....: \n\nIn [86]: def f2():\n   ....:     result = []\n   ....:     for i in range(10):\n   ....:         result.append(df)\n   ....:     return pd.concat(result)\n   ....: \n\nIn [100]: f1().equals(f2())\nOut[100]: True\n\nIn [101]: %timeit f1()\n1 loops, best of 3: 1.66 s per loop\n\nIn [102]: %timeit f2()\n1 loops, best of 3: 220 ms per loop\n\nIn [104]: df = DataFrame(np.random.randn(2500,40))\n\nIn [105]: %timeit f1()\n10 loops, best of 3: 33.1 ms per loop\n\nIn [106]: %timeit f2()\n100 loops, best of 3: 4.23 ms per loop\n"
'import csv\nwith open(r"C:\\work\\DATA\\Raw_data\\store.csv", \'rb\') as f:\n    reader = csv.reader(f)\n    linenumber = 1\n    try:\n        for row in reader:\n            linenumber += 1\n    except Exception as e:\n        print (("Error line %d: %s %s" % (linenumber, str(type(e)), e.message)))\n'
"s = df['Neighborhood'].groupby(df['Borough']).value_counts()\nprint s\nBorough                      \nBronx          Melrose            7\nManhattan      Midtown           12\n               Lincoln Square     2\nStaten Island  Grant City        11\ndtype: int64\n\nprint s.groupby(level=[0,1]).nlargest(1)\nBronx          Bronx          Melrose        7\nManhattan      Manhattan      Midtown       12\nStaten Island  Staten Island  Grant City    11\ndtype: int64\n"
"In [101]: df.resample('1H').agg({'openbid': 'first', \n                                 'highbid': 'max', \n                                 'lowbid': 'min', \n                                 'closebid': 'last'})\nOut[101]: \n                      lowbid  highbid  closebid  openbid\nctime                                                   \n2015-09-30 23:00:00  1.11687  1.11712   1.11708    1.117\n"
"df = pd.DataFrame(A.toarray())\n\nA = csr_matrix([[1, 0, 2], [0, 3, 0]])\n\n  (0, 0)    1\n  (0, 2)    2\n  (1, 1)    3\n\n&lt;class 'scipy.sparse.csr.csr_matrix'&gt;\n\npd.DataFrame(A.todense())\n\n   0  1  2\n0  1  0  2\n1  0  3  0\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null int64\n1    2 non-null int64\n2    2 non-null int64\n"
"import pandas as pd\nimport numpy as np\n\ndata = pd.DataFrame([])\n\nfor i in np.arange(0, 4):\n    if i % 2 == 0:\n        data = data.append(pd.DataFrame({'A': i, 'B': i + 1}, index=[0]), ignore_index=True)\n    else:\n        data = data.append(pd.DataFrame({'A': i}, index=[0]), ignore_index=True)\n\nprint(data.head())\n\n   A    B\n0  0  1.0\n1  2  3.0\n2  3  NaN\n"
'np.r_[1:10, 15, 17, 50:100]\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 15, 17, 50, 51, 52, 53, 54, 55,\n       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n       90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\ndf.iloc[:, np.r_[1:10, 15, 17, 50:100]]\n'
'from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(["paris", "paris", "tokyo", "amsterdam"])\nlist(le.classes_)\nle.transform(["tokyo", "tokyo", "paris"])\nlist(le.inverse_transform([2, 2, 1]))\n'
"In [40]:\ndf = pd.DataFrame({'group':[0,1,1,1,2,2,3,3,3], 'val':np.arange(9)})\ngp = df.groupby('group')\ngp.groups.keys()\n\nOut[40]:\ndict_keys([0, 1, 2, 3])\n\nIn [41]:\ngp.groups\n\nOut[41]:\n{0: Int64Index([0], dtype='int64'),\n 1: Int64Index([1, 2, 3], dtype='int64'),\n 2: Int64Index([4, 5], dtype='int64'),\n 3: Int64Index([6, 7, 8], dtype='int64')}\n\nIn [65]:\ndf = pd.DataFrame({'group':list('bgaaabxeb'), 'val':np.arange(9)})\ngp = df.groupby('group')\ngp.groups.keys()\n\nOut[65]:\ndict_keys(['b', 'e', 'g', 'a', 'x'])\n\nIn [79]:\ngp.groups\n\nOut[79]:\n{'a': Int64Index([2, 3, 4], dtype='int64'),\n 'b': Int64Index([0, 5, 8], dtype='int64'),\n 'e': Int64Index([7], dtype='int64'),\n 'g': Int64Index([1], dtype='int64'),\n 'x': Int64Index([6], dtype='int64')}\n\nIn [78]:\ngp.apply(lambda x: x.name)\n\nOut[78]:\ngroup\na    a\nb    b\ne    e\ng    g\nx    x\ndtype: object\n\nIn [81]:\nagg = gp.sum()\nagg\n\nOut[81]:\n       val\ngroup     \na        9\nb       13\ne        7\ng        1\nx        6\n\nIn [83]:    \nagg.index.get_level_values(0)\n\nOut[83]:\nIndex(['a', 'b', 'e', 'g', 'x'], dtype='object', name='group')\n"
"df.groupby('columnName').apply(lambda x: myFunction(x, arg1))\n\ndf.groupby('columnName').apply(myFunction, ('arg1'))\n\nIn [82]: df = pd.DataFrame(np.random.randint(5,size=(5,3)), columns=list('abc'))\n\nIn [83]: df\nOut[83]:\n   a  b  c\n0  0  3  1\n1  0  3  4\n2  3  0  4\n3  4  2  3\n4  3  4  1\n\nIn [84]: def f(ser, n):\n    ...:     return ser.max() * n\n    ...:\n\nIn [85]: df.apply(f, args=(10,))\nOut[85]:\na    40\nb    40\nc    40\ndtype: int64\n\nIn [86]: df.groupby('a').apply(f, n=10)\nOut[86]:\n    a   b   c\na\n0   0  30  40\n3  30  40  40\n4  40  20  30\n\nIn [87]: df.groupby('a').apply(f, (10))\nOut[87]:\n    a   b   c\na\n0   0  30  40\n3  30  40  40\n4  40  20  30\n"
"            dt  user    val\n0   2016-01-01     a      1\n1   2016-01-02     a     33\n2   2016-01-05     b      2\n3   2016-01-06     b      1\n\nx['dt'] = pd.to_datetime(x['dt'])\n\ndates = x.set_index('dt').resample('D').asfreq().index\n\n&gt;&gt; DatetimeIndex(['2016-01-01', '2016-01-02', '2016-01-03', '2016-01-04',\n               '2016-01-05', '2016-01-06'],\n              dtype='datetime64[ns]', name='dt', freq='D')\n\nusers = x['user'].unique()\n\n&gt;&gt; array(['a', 'b'], dtype=object)\n\nidx = pd.MultiIndex.from_product((dates, users), names=['dt', 'user'])\n\n&gt;&gt; MultiIndex(levels=[[2016-01-01 00:00:00, 2016-01-02 00:00:00, 2016-01-03 00:00:00, 2016-01-04 00:00:00, 2016-01-05 00:00:00, 2016-01-06 00:00:00], ['a', 'b']],\n           labels=[[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1]],\n           names=['dt', 'user'])\n\nx.set_index(['dt', 'user']).reindex(idx, fill_value=0).reset_index()\nOut: \n           dt user  val\n0  2016-01-01    a    1\n1  2016-01-01    b    0\n2  2016-01-02    a   33\n3  2016-01-02    b    0\n4  2016-01-03    a    0\n5  2016-01-03    b    0\n6  2016-01-04    a    0\n7  2016-01-04    b    0\n8  2016-01-05    a    0\n9  2016-01-05    b    2\n10 2016-01-06    a    0\n11 2016-01-06    b    1\n\nx.set_index(['dt', 'user']).reindex(idx, fill_value=0).reset_index().sort_values(by='user')\nOut: \n           dt user  val\n0  2016-01-01    a    1\n2  2016-01-02    a   33\n4  2016-01-03    a    0\n6  2016-01-04    a    0\n8  2016-01-05    a    0\n10 2016-01-06    a    0\n1  2016-01-01    b    0\n3  2016-01-02    b    0\n5  2016-01-03    b    0\n7  2016-01-04    b    0\n9  2016-01-05    b    2\n11 2016-01-06    b    1\n"
"    # drop the columns where all elements are NaN:\n\n    &gt;&gt;&gt; df.dropna(axis=1, how='all')\n         A    B  D\n    0  NaN  2.0  0\n    1  3.0  4.0  1\n    2  NaN  NaN  5\n"
"In [399]: df = pd.DataFrame(columns=list('ABC'))\n\nIn [400]: df.loc[0] = [1,1.23,'Hello']\n\nIn [401]: df\nOut[401]:\n   A     B      C\n0  1  1.23  Hello\n\nIn [395]: df = pd.DataFrame([[1,1.23,'Hello']], columns=list('ABC'))\n\nIn [396]: df\nOut[396]:\n   A     B      C\n0  1  1.23  Hello\n"
"def explode(df, lst_cols, fill_value=''):\n    # make sure `lst_cols` is a list\n    if lst_cols and not isinstance(lst_cols, list):\n        lst_cols = [lst_cols]\n    # all columns except `lst_cols`\n    idx_cols = df.columns.difference(lst_cols)\n\n    # calculate lengths of lists\n    lens = df[lst_cols[0]].str.len()\n\n    if (lens &gt; 0).all():\n        # ALL lists in cells aren't empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .loc[:, df.columns]\n    else:\n        # at least one list in cells is empty\n        return pd.DataFrame({\n            col:np.repeat(df[col].values, df[lst_cols[0]].str.len())\n            for col in idx_cols\n        }).assign(**{col:np.concatenate(df[col].values) for col in lst_cols}) \\\n          .append(df.loc[lens==0, idx_cols]).fillna(fill_value) \\\n          .loc[:, df.columns]\n\nIn [82]: explode(df, lst_cols=list('BCDE'))\nOut[82]:\n    A   B   C   D   E\n0  x1  v1  c1  d1  e1\n1  x1  v2  c2  d2  e2\n2  x2  v3  c3  d3  e3\n3  x2  v4  c4  d4  e4\n4  x3  v5  c5  d5  e5\n5  x3  v6  c6  d6  e6\n6  x4  v7  c7  d7  e7\n7  x4  v8  c8  d8  e8\n"
'import seaborn as sns\nsns.set()\ndf.set_index(\'App\').T.plot(kind=\'bar\', stacked=True)\n\nfrom matplotlib.colors import ListedColormap\n\ndf.set_index(\'App\')\\\n  .reindex(df.set_index(\'App\').sum().sort_values().index, axis=1)\\\n  .T.plot(kind=\'bar\', stacked=True,\n          colormap=ListedColormap(sns.color_palette("GnBu", 10)), \n          figsize=(12,6))\n'
"In [37]: col = np.array([0,0,1,2,2,2])\n\nIn [38]: data = np.array([1,2,3,4,5,6],dtype='float64')\n\nIn [39]: m = csc_matrix( (data,(row,col)), shape=(3,3) )\n\nIn [40]: m\nOut[40]: \n&lt;3x3 sparse matrix of type '&lt;type 'numpy.float64'&gt;'\n        with 6 stored elements in Compressed Sparse Column format&gt;\n\nIn [46]: pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                              for i in np.arange(m.shape[0]) ])\nOut[46]: \n   0  1  2\n0  1  0  4\n1  0  0  5\n2  2  3  6\n\nIn [47]: df = pd.SparseDataFrame([ pd.SparseSeries(m[i].toarray().ravel()) \n                                   for i in np.arange(m.shape[0]) ])\n\nIn [48]: type(df)\nOut[48]: pandas.sparse.frame.SparseDataFrame\n"
"a = pandas.DataFrame.from_csv('st1.csv', sep=' ')\n\na = pandas.DataFrame.from_csv('st1.csv', index_col=None)\n"
"import pandas as pd\n\ndf = pd.DataFrame(data={'count':[1, -1, 2, -2, 3, -3]})\n\ndf['count'] = df['count'].abs()\n\nprint(df)\n   count\n#0      1\n#1      1\n#2      2\n#3      2\n#4      3\n#5      3\n"
"In [228]:\n\ndf['first'] = df['construct_name'].str.split('_').str[0]\ndf\nOut[228]:\n  construct_name first\n0      aaaa_t1_2  aaaa\n1     cccc_t4_10  cccc\n2      bbbb_g3_3  bbbb\n"
"df.loc[0:15,'A'] = 16\nprint (df)\n     A   B\n0   16  45\n1   16   5\n2   16  97\n3   16  58\n4   16  26\n5   16  87\n6   16  51\n7   16  17\n8   16  39\n9   16  73\n10  16  94\n11  16  69\n12  16  57\n13  16  24\n14  16  43\n15  16  77\n16  41   0\n17   3  21\n18   0  98\n19  45  39\n20  66  62\n21   8  53\n22  69  47\n23  48  53\n"
"ser = pd.read_json('people_wiki_map_index_to_word.json', typ='series')\n\nimport json\nwith open('people_wiki_map_index_to_word.json', 'r') as f:\n    data = json.load(f)\n\ndf = pd.DataFrame({'count': data})\n"
"import ast\nfrom pandas.io.json import json_normalize\n\ndef only_dict(d):\n    '''\n    Convert json string representation of dictionary to a python dict\n    '''\n    return ast.literal_eval(d)\n\ndef list_of_dicts(ld):\n    '''\n    Create a mapping of the tuples formed after \n    converting json strings of list to a python list   \n    '''\n    return dict([(list(d.values())[1], list(d.values())[0]) for d in ast.literal_eval(ld)])\n\nA = json_normalize(df['columnA'].apply(only_dict).tolist()).add_prefix('columnA.')\nB = json_normalize(df['columnB'].apply(list_of_dicts).tolist()).add_prefix('columnB.pos.') \n\ndf[['id', 'name']].join([A, B])\n"
"df1 = df.sort_values('score',ascending = False).groupby('pidx').head(2)\nprint (df1)\n\n    mainid pidx pidy  score\n8        2    x    w     12\n4        1    a    e      8\n2        1    c    a      7\n10       2    y    x      6\n1        1    a    c      5\n7        2    z    y      5\n6        2    y    z      3\n3        1    c    b      2\n5        2    x    y      1\n\ndf = df.set_index(['mainid','pidy']).groupby('pidx')['score'].nlargest(2).reset_index() \nprint (df)\n  pidx  mainid pidy  score\n0    a       1    e      8\n1    a       1    c      5\n2    c       1    a      7\n3    c       1    b      2\n4    x       2    w     12\n5    x       2    y      1\n6    y       2    x      6\n7    y       2    z      3\n8    z       2    y      5\n\nnp.random.seed(123)\nN = 1000000\n\nL1 = list('abcdefghijklmnopqrstu')\nL2 = list('efghijklmnopqrstuvwxyz')\ndf = pd.DataFrame({'mainid':np.random.randint(1000, size=N),\n                   'pidx': np.random.randint(10000, size=N),\n                   'pidy': np.random.choice(L2, N),\n                   'score':np.random.randint(1000, size=N)})\n#print (df)\n\ndef epat(df):\n    grouped = df.groupby('pidx')\n    new_df = pd.DataFrame([], columns = df.columns)\n    for key, values in grouped:\n        new_df = pd.concat([new_df, grouped.get_group(key).sort_values('score', ascending=True)[:2]], 0)\n    return (new_df)\n\nprint (epat(df))\n\nIn [133]: %timeit (df.sort_values('score',ascending = False).groupby('pidx').head(2))\n1 loop, best of 3: 309 ms per loop\n\nIn [134]: %timeit (df.set_index(['mainid','pidy']).groupby('pidx')['score'].nlargest(2).reset_index())\n1 loop, best of 3: 7.11 s per loop\n\nIn [147]: %timeit (epat(df))\n1 loop, best of 3: 22 s per loop\n"
'In [119]: dfi\nOut[119]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n\nIn [120]: dfi.loc[3] = 5\n\nIn [121]: dfi\nOut[121]: \n   A  B  C\n0  0  1  0\n1  2  3  2\n2  4  5  4\n3  5  5  5\n\n%%timeit df = pd.DataFrame(columns=["A", "B", "C"]); new_row = pd.Series({"A": 4, "B": 4, "C": 4})\nfor i in range(8000):\n    df = df.append(new_row, ignore_index=True)\n\n# 6.59 s ± 53.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%%timeit df = pd.DataFrame(columns=["A", "B", "C"]); new_row = pd.Series({"A": 4, "B": 4, "C": 4})\nfor i in range(8000):\n    df.loc[i] = new_row\n\n# 10.2 s ± 148 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%%timeit df = pd.DataFrame(columns=["A", "B", "C"]); new_row = pd.Series({"A": 4, "B": 4, "C": 4})\nfor i in range(16000):\n    df.loc[i] = new_row\n\n# 23.7 s ± 286 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n'
'df = df.applymap(str)\n'
"import pandas as pd\n\ncols = pd.MultiIndex.from_arrays([['basic_amt']*4,\n                                     ['NSW','QLD','VIC','All']], \n                                     names = [None, 'Faculty'])\nidx = pd.Index(['All', 'Full Time', 'Part Time'])\n\ndf = pd.DataFrame([(1,1,2,4),\n                   (0,1,0,1),\n                   (1,0,2,3)], index = idx, columns=cols)\n                   \nprint (df)\n          basic_amt            \nFaculty         NSW QLD VIC All\nAll               1   1   2   4\nFull Time         0   1   0   1\nPart Time         1   0   2   3\n\ndf.columns = df.columns.droplevel(0)\n#pandas 0.18.0 and higher\ndf = df.rename_axis(None, axis=1)\n#pandas bellow 0.18.0\n#df.columns.name = None\n\nprint (df)\n           NSW  QLD  VIC  All\nAll          1    1    2    4\nFull Time    0    1    0    1\nPart Time    1    0    2    3\n\nprint (df.columns)\nIndex(['NSW', 'QLD', 'VIC', 'All'], dtype='object')\n\ndf.columns = ['_'.join(col) for col in df.columns]\nprint (df)\n           basic_amt_NSW  basic_amt_QLD  basic_amt_VIC  basic_amt_All\nAll                    1              1              2              4\nFull Time              0              1              0              1\nPart Time              1              0              2              3\n\nprint (df.columns)\nIndex(['basic_amt_NSW', 'basic_amt_QLD', 'basic_amt_VIC', 'basic_amt_All'], dtype='object')\n"
"In [1]: df\nOut[1]: \n           x         y val\n 0 -1.015235  0.840049   a\n 1 -0.427016  0.880745   b\n 2  0.744470 -0.401485   c\n 3  1.334952 -0.708141   d\n 4  0.127634 -1.335107   e\n\nax = df.set_index('x')['y'].plot(style='o')\n\ndef label_point(x, y, val, ax):\n    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n    for i, point in a.iterrows():\n        ax.text(point['x'], point['y'], str(point['val']))\n\nlabel_point(df.x, df.y, df.val, ax)\n\ndraw()\n"
"df1.join(df2, how='inner') # how='outer' keeps all records from both data frames\n\nmerge(df1.reset_index(),\n      df2.reset_index(),\n      on=['index1'],\n      how='inner'\n     ).set_index(['index1','index2'])\n\nindex_left = pd.MultiIndex.from_tuples([('K0', 'X0'), ('K0', 'X1'),\n                                        ('K1', 'X2')],\n                                        names=['key', 'X'])\n\nleft = pd.DataFrame({'A': ['A0', 'A1', 'A2'],\n                     'B': ['B0', 'B1', 'B2']}, index=index_left)\n\nindex_right = pd.MultiIndex.from_tuples([('K0', 'Y0'), ('K1', 'Y1'),\n                                        ('K2', 'Y2'), ('K2', 'Y3')],\n                                        names=['key', 'Y'])\n\nright = pd.DataFrame({'C': ['C0', 'C1', 'C2', 'C3'],\n                      'D': ['D0', 'D1', 'D2', 'D3']}, index=index_right)\n\nleft.join(right)\n\n\n            A   B   C   D\nkey X  Y                 \nK0  X0 Y0  A0  B0  C0  D0\n    X1 Y0  A1  B1  C0  D0\nK1  X2 Y1  A2  B2  C1  D1\n\n[3 rows x 4 columns]\n"
"In [7]: df = pd.DataFrame({'Status':['Delivered', 'Delivered', 'Undelivered',\n                                     'SomethingElse']})\n\nIn [8]: df\nOut[8]:\n          Status\n0      Delivered\n1      Delivered\n2    Undelivered\n3  SomethingElse\n\nIn [9]: d = {'Delivered': True, 'Undelivered': False}\n\nIn [10]: df['Status'].map(d)\nOut[10]:\n0     True\n1     True\n2    False\n3      NaN\nName: Status, dtype: object\n"
"In [63]: df\nOut[63]: \n   a          b    c\n0  1  [1, 2, 3]  foo\n1  1     [2, 5]  bar\n2  2     [5, 6]  baz\n\n\nIn [64]: df.groupby('a').agg({'b': 'sum', 'c': lambda x: ' '.join(x)})\nOut[64]: \n         c                b\na                          \n1  foo bar  [1, 2, 3, 2, 5]\n2      baz           [5, 6]\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; t = pd.tslib.Timestamp('2016-03-03 00:00:00')\n&gt;&gt;&gt; type(t)\npandas.tslib.Timestamp\n&gt;&gt;&gt; t.to_datetime()  #Warning deprecated!\ndatetime.datetime(2016, 3, 3, 0, 0)\n&gt;&gt;&gt; t.to_pydatetime()\ndatetime.datetime(2016, 3, 3, 0, 0)\n\n&gt;&gt;&gt; t.date()\ndatetime.date(2016, 3, 3)\n\nIn [4]: t.to_datetime()\n/Users/qiuwei/Library/Python/2.7/lib/python/site-packages/IPython/core/interactiveshell.py:2881: FutureWarning: to_datetime is deprecated. Use self.to_pydatetime()\n  exec(code_obj, self.user_global_ns, self.user_ns)\nOut[4]: datetime.datetime(2016, 3, 3, 0, 0)\n"
"# Setup\ns = pd.Series(['1', '2', '3', '4', '.'])\ns\n\n0    1\n1    2\n2    3\n3    4\n4    .\ndtype: object\n\npd.to_numeric(s, errors='coerce')\n\n0    1.0\n1    2.0\n2    3.0\n3    4.0\n4    NaN\ndtype: float64\n\npd.to_numeric(s, errors='coerce').fillna(0, downcast='infer')\n\n0    1\n1    2\n2    3\n3    4\n4    0\ndtype: float64\n\npd.__version__\n# '0.24.1'\n\npd.to_numeric(s, errors='coerce').astype('Int32')\n\n0      1\n1      2\n2      3\n3      4\n4    NaN\ndtype: Int32\n\n# Setup.\nnp.random.seed(0)\ndf = pd.DataFrame({\n    'A' : np.random.choice(10, 5), \n    'C' : np.random.choice(10, 5), \n    'B' : ['1', '###', '...', 50, '234'], \n    'D' : ['23', '1', '...', '268', '$$']}\n)[list('ABCD')]\ndf\n\n   A    B  C    D\n0  5    1  9   23\n1  0  ###  3    1\n2  3  ...  5  ...\n3  3   50  2  268\n4  7  234  4   $$\n\ndf.dtypes\n\nA     int64\nB    object\nC     int64\nD    object\ndtype: object\n\ndf2 = df.apply(pd.to_numeric, errors='coerce')\ndf2\n\n   A      B  C      D\n0  5    1.0  9   23.0\n1  0    NaN  3    1.0\n2  3    NaN  5    NaN\n3  3   50.0  2  268.0\n4  7  234.0  4    NaN\n\ndf2.dtypes\n\nA      int64\nB    float64\nC      int64\nD    float64\ndtype: object\n\ndf.transform(pd.to_numeric, errors='coerce')\n\n   A      B  C      D\n0  5    1.0  9   23.0\n1  0    NaN  3    1.0\n2  3    NaN  5    NaN\n3  3   50.0  2  268.0\n4  7  234.0  4    NaN\n\ndf.dtypes.eq(object)\n\nA    False\nB     True\nC    False\nD     True\ndtype: bool\n\ncols = df.columns[df.dtypes.eq(object)]\n# Actually, `cols` can be any list of columns you need to convert.\ncols\n# Index(['B', 'D'], dtype='object')\n\ndf[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n# Alternatively,\n# for c in cols:\n#     df[c] = pd.to_numeric(df[c], errors='coerce')\n\ndf\n\n   A      B  C      D\n0  5    1.0  9   23.0\n1  0    NaN  3    1.0\n2  3    NaN  5    NaN\n3  3   50.0  2  268.0\n4  7  234.0  4    NaN\n"
'&gt;&gt;&gt; df2 = df.loc[np.repeat(df.index.values,df.n)]\n&gt;&gt;&gt; df2\n  id  n   v\n0  A  1  10\n1  B  2  13\n1  B  2  13\n2  C  3   8\n2  C  3   8\n2  C  3   8\n\n&gt;&gt;&gt; df2 = df2.drop("n",axis=1).reset_index(drop=True)\n&gt;&gt;&gt; df2\n  id   v\n0  A  10\n1  B  13\n2  B  13\n3  C   8\n4  C   8\n5  C   8\n\nIn [86]: df.iloc[np.repeat(np.arange(len(df)), df["n"])].drop("n", axis=1).reset_index(drop=True)\nOut[86]: \n  id   v\n0  A  10\n1  B  13\n2  B  13\n3  C   8\n4  C   8\n5  C   8\n'
'print df.reset_index().to_json(orient=\'records\')\n\n[\n     {"id":0,"location":"[50, 50]"},\n     {"id":1,"location":"[60, 60]"},\n     {"id":2,"location":"[70, 70]"},\n     {"id":3,"location":"[80, 80]"}\n]\n'
'pandas.concat([df1, df2], axis=1)\n'
"pd.merge(df1, df2, on=['A', 'B', 'C', 'D'], how='inner')\n\n    A           B            C          D\n0   0.403846    0.312230    0.209882    0.397923\n1   0.934957    0.731730    0.484712    0.734747\n2   0.588245    0.961589    0.910292    0.382072\n3   0.534226    0.276908    0.323282    0.629398\n4   0.259533    0.277465    0.043652    0.925743\n5   0.667415    0.051182    0.928655    0.737673\n6   0.217923    0.665446    0.224268    0.772592\n7   0.023578    0.561884    0.615515    0.362084\n8   0.346373    0.375366    0.083003    0.663622\n9   0.352584    0.103263    0.661686    0.246862\n\n     A          B            C           D\n0   0.259533    0.277465    0.043652    0.925743\n1   0.667415    0.051182    0.928655    0.737673\n2   0.217923    0.665446    0.224268    0.772592\n3   0.023578    0.561884    0.615515    0.362084\n4   0.346373    0.375366    0.083003    0.663622\n5   2.000000    3.000000    4.000000    5.000000\n6   14.000000   15.000000   16.000000   17.000000\n\n     A           B           C           D\n0   0.259533    0.277465    0.043652    0.925743\n1   0.667415    0.051182    0.928655    0.737673\n2   0.217923    0.665446    0.224268    0.772592\n3   0.023578    0.561884    0.615515    0.362084\n4   0.346373    0.375366    0.083003    0.663622\n\ndf1 = pd.DataFrame(np.random.rand(10,4),columns=list('ABCD'))\ndf2 = df1.ix[4:8]\ndf2.reset_index(drop=True,inplace=True)\ndf2.loc[-1] = [2, 3, 4, 5]\ndf2.loc[-2] = [14, 15, 16, 17]\ndf2.reset_index(drop=True,inplace=True)\ndf2 = df2[['A', 'B', 'C']] # df2 has only columns A B C\n\npd.merge(df1, df2, on=common_cols, how='inner')\n\nds1 = set(tuple(line) for line in df1.values)\nds2 = set(tuple(line) for line in df2.values)\ndf = pd.DataFrame(list(ds2.difference(ds1)), columns=df2.columns)\n\n   A  B  C  D\n0  6  4  1  6\n1  7  6  6  8\n2  1  6  2  7\n3  8  0  4  1\n4  1  0  2  3\n5  8  4  7  5\n6  4  7  1  1\n7  3  7  3  4\n8  5  2  8  8\n9  3  2  8  4\n\n   A  B  C  D\n0  1  0  2  3\n1  8  4  7  5\n2  4  7  1  1\n3  3  7  3  4\n4  5  2  8  8\n5  1  1  1  1\n6  2  2  2  2\n\n   A  B  C  D\n0  1  0  2  3\n1  8  4  7  5\n2  4  7  1  1\n3  3  7  3  4\n4  5  2  8  8\n\n   A  B  C  D\n0  1  1  1  1\n1  2  2  2  2\n\n   A  B  C  D\n6  2  2  2  2\n\ndf12['key'] = 'x'\ntemp_df = pd.merge(df2, df12, on=df2.columns.tolist(), how='left')\ntemp_df[temp_df['key'].isnull()].drop('key', axis=1)\n"
"df['empty_list'] = np.empty((len(df), 0)).tolist()\n\ndf['empty_list'] = [[] for _ in range(len(df))]\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame(pd.np.random.rand(1000000, 5))\n\nIn [3]: timeit df['empty1'] = pd.np.empty((len(df), 0)).tolist()\n10 loops, best of 3: 127 ms per loop\n\nIn [4]: timeit df['empty2'] = [[] for _ in range(len(df))]\n10 loops, best of 3: 193 ms per loop\n\nIn [5]: timeit df['empty3'] = df.apply(lambda x: [], axis=1)\n1 loops, best of 3: 5.89 s per loop\n"
"df[df.columns[1:]] = df[df.columns[1:]].replace('[\\$,]', '', regex=True).astype(float)\n"
'In [11]: df = pd.DataFrame([[2, 3], [5, 6]], pd.Index([1, 4], name="A"), columns=["B", "C"])\n\nIn [12]: df\nOut[12]:\n   B  C\nA\n1  2  3\n4  5  6\n\nIn [13]: df.reset_index()\nOut[13]:\n   A  B  C\n0  1  2  3\n1  4  5  6\n\nstep1 = step3.groupby([\'Id\', \'interestingtabsplittest2__grp\'], as_index=False)[\'applications\'].sum()\n'
'import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.ticker as ticker\n\n# Some random data\ndfWIM = pd.DataFrame({\'AXLES\': np.random.normal(8, 2, 5000).astype(int)})\nncount = len(dfWIM)\n\nplt.figure(figsize=(12,8))\nax = sns.countplot(x="AXLES", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\nplt.title(\'Distribution of Truck Configurations\')\nplt.xlabel(\'Number of Axles\')\n\n# Make twin axis\nax2=ax.twinx()\n\n# Switch so count axis is on right, frequency on left\nax2.yaxis.tick_left()\nax.yaxis.tick_right()\n\n# Also switch the labels over\nax.yaxis.set_label_position(\'right\')\nax2.yaxis.set_label_position(\'left\')\n\nax2.set_ylabel(\'Frequency [%]\')\n\nfor p in ax.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    ax.annotate(\'{:.1f}%\'.format(100.*y/ncount), (x.mean(), y), \n            ha=\'center\', va=\'bottom\') # set the alignment of the text\n\n# Use a LinearLocator to ensure the correct number of ticks\nax.yaxis.set_major_locator(ticker.LinearLocator(11))\n\n# Fix the frequency range to 0-100\nax2.set_ylim(0,100)\nax.set_ylim(0,ncount)\n\n# And use a MultipleLocator to ensure a tick spacing of 10\nax2.yaxis.set_major_locator(ticker.MultipleLocator(10))\n\n# Need to turn the grid on ax2 off, otherwise the gridlines end up on top of the bars\nax2.grid(None)\n\nplt.savefig(\'snscounter.pdf\')\n'
"def EOQ(D,p,ck,ch):\n    Q = math.sqrt((2*D*ck)/(ch*p))\n    return Q\nch=0.2\nck=5\ndf['Q'] = df.apply(lambda row: EOQ(row['D'], row['p'], ck, ch), axis=1)\ndf\n\nIn [80]:\ndf['Q'] = np.sqrt((2*df['D']*ck)/(ch*df['p']))\n\ndf\nOut[80]:\n    D   p          Q\n0  10  20   5.000000\n1  20  30   5.773503\n2  30  10  12.247449\n\nIn [92]:\n\nimport math\nch=0.2\nck=5\ndef EOQ(D,p,ck,ch):\n    Q = math.sqrt((2*D*ck)/(ch*p))\n    return Q\n\n%timeit np.sqrt((2*df['D']*ck)/(ch*df['p']))\n%timeit df.apply(lambda row: EOQ(row['D'], row['p'], ck, ch), axis=1)\n1000 loops, best of 3: 622 µs per loop\n1 loops, best of 3: 1.19 s per loop\n"
"&gt;&gt;&gt; groups = df.groupby(['username', pd.cut(df.views, bins)])\n&gt;&gt;&gt; groups.size().unstack()\nviews     (1, 10]  (10, 25]  (25, 50]  (50, 100]\nusername\njane            1         1         1          1\njohn            1         1         1          1\n"
"df = pd.DataFrame({'my_timestamp': pd.date_range('2016-1-1 15:00', periods=5)})\n\n&gt;&gt;&gt; df\n         my_timestamp\n0 2016-01-01 15:00:00\n1 2016-01-02 15:00:00\n2 2016-01-03 15:00:00\n3 2016-01-04 15:00:00\n4 2016-01-05 15:00:00\n\ndf['new_date'] = [d.date() for d in df['my_timestamp']]\ndf['new_time'] = [d.time() for d in df['my_timestamp']]\n\n&gt;&gt;&gt; df\n         my_timestamp    new_date  new_time\n0 2016-01-01 15:00:00  2016-01-01  15:00:00\n1 2016-01-02 15:00:00  2016-01-02  15:00:00\n2 2016-01-03 15:00:00  2016-01-03  15:00:00\n3 2016-01-04 15:00:00  2016-01-04  15:00:00\n4 2016-01-05 15:00:00  2016-01-05  15:00:00\n\nnew_dates, new_times = zip(*[(d.date(), d.time()) for d in df['my_timestamp']])\ndf = df.assign(new_date=new_dates, new_time=new_times)\n"
"Index([u'regiment', u'company',  u'name',u'postTestScore'], dtype='object')\n\ndata = data.rename(columns={'Number ': 'Number'})\n"
'df = pd.DataFrame([A])\nprint (df)\n   0  1  2    3 4\n0  1  d  p  bab  \n\ndf = pd.DataFrame(np.array(A).reshape(-1,len(A)))\nprint (df)\n\n   0  1  2    3 4\n0  1  d  p  bab  \n\ndf = pd.DataFrame(A).T\nprint (df)\n   0  1  2    3 4\n0  1  d  p  bab  \n'
'&gt;&gt;&gt; print df\n   A  B  C\n0 -1  0  0\n1 -4  3 -1\n2 -1  0  2\n3  0  3  2\n4  1 -1  0\n&gt;&gt;&gt; print df.applymap(lambda x: x&gt;1)\n       A      B      C\n0  False  False  False\n1  False   True  False\n2  False  False   True\n3  False   True   True\n4  False  False  False\n'
"import sqlite3\n# Create your connection.\ncnx = sqlite3.connect(':memory:')\n\nprice2.to_sql(name='price2', con=cnx)\n\np2 = pd.read_sql('select * from price2', cnx)\n\np2.Date = pd.to_datetime(p2.Date)\np = p2.set_index('Date')\n\nIn [11]: p2\nOut[11]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1006 entries, 2009-01-02 00:00:00 to 2012-12-31 00:00:00\nData columns:\nAAPL    1006  non-null values\nGE      1006  non-null values\ndtypes: float64(2)\n\nfrom sqlalchemy import create_engine\ne = create_engine('sqlite://')  # pass your db url\n\nprice2.to_sql(name='price2', con=cnx)\n\npd.read_sql_table(table_name='price2', con=e)\n#         Date   AAPL     GE\n# 0 2009-01-02  89.95  14.76\n# 1 2009-01-05  93.75  14.38\n# 2 2009-01-06  92.20  14.58\n# 3 2009-01-07  90.21  13.93\n# 4 2009-01-08  91.88  13.95\n"
"df2.dropna(subset=['three', 'four', 'five'], how='all')\n\nsubset=df2.columns[k:]\n\nsubset=filter(lambda x: len(x) &gt; 3, df2.columns)\n"
'In [1]: s = Series(["a","b","c","a"], dtype="category")\n\nIn [2]: s\nOut[2]: \n0    a\n1    b\n2    c\n3    a\ndtype: category\nCategories (3, object): [a &lt; b &lt; c]\n'
"df_masked = df[(df.date &gt; '2012-04-01') &amp; (df.date &lt; '2012-04-04')]\n\ndf_masked = df[(df.date &gt; datetime.date(2012,4,1)) &amp; (df.date &lt; datetime.date(2012,4,4))]\n"
'&gt;&gt;&gt; data\n        one                           two                    \n          a         b         c         a         b         c\n0 -0.927134 -1.204302  0.711426  0.854065 -0.608661  1.140052\n1 -0.690745  0.517359 -0.631856  0.178464 -0.312543 -0.418541\n2  1.086432  0.194193  0.808235 -0.418109  1.055057  1.886883\n3 -0.373822 -0.012812  1.329105  1.774723 -2.229428 -0.617690\n&gt;&gt;&gt; data.loc[:,data.columns.get_level_values(1).isin({"a", "c"})]\n        one                 two          \n          a         c         a         c\n0 -0.927134  0.711426  0.854065  1.140052\n1 -0.690745 -0.631856  0.178464 -0.418541\n2  1.086432  0.808235 -0.418109  1.886883\n3 -0.373822  1.329105  1.774723 -0.617690\n'
'followers_df.index = range(20)\n'
"dUp= delta[delta &gt; 0]\ndDown= delta[delta &lt; 0]\n\nRolUp = RolUp.reindex_like(delta, method='ffill')\nRolDown = RolDown.reindex_like(delta, method='ffill')\n\n# dUp= delta[delta &gt; 0]\n# dDown= delta[delta &lt; 0]\n\n# dUp = dUp.reindex_like(delta, fill_value=0)\n# dDown = dDown.reindex_like(delta, fill_value=0)\n\ndUp, dDown = delta.copy(), delta.copy()\ndUp[dUp &lt; 0] = 0\ndDown[dDown &gt; 0] = 0\n\nRolUp = pd.rolling_mean(dUp, n)\nRolDown = pd.rolling_mean(dDown, n).abs()\n\nRS = RolUp / RolDown\n"
"In [32]: data3['diffs'] = data3.groupby('ticker')['value'].transform(Series.diff)\n\nIn [34]: data3.sort_index(inplace=True)\n\nIn [25]: data3\nOut[25]: \n         date    ticker     value     diffs\n0  2013-10-03  ticker_2  0.435995  0.015627\n1  2013-10-04  ticker_2  0.025926 -0.410069\n2  2013-10-02  ticker_1  0.549662       NaN\n3  2013-10-01  ticker_0  0.435322       NaN\n4  2013-10-02  ticker_2  0.420368  0.120713\n5  2013-10-03  ticker_0  0.330335 -0.288936\n6  2013-10-04  ticker_1  0.204649 -0.345014\n7  2013-10-02  ticker_0  0.619271  0.183949\n8  2013-10-01  ticker_2  0.299655       NaN\n\n[9 rows x 4 columns]\n"
'import pandas as pd\nimport numpy as np\n\ndf = pd.Series(np.random.rand(3), index=[&quot;a&quot;,&quot;b&quot;,&quot;c&quot;]).to_frame().T\ndf.columns = pd.MultiIndex.from_product([[&quot;new_label&quot;], df.columns])\n\n  new_label                    \n          a         b         c\n0   0.25999  0.337535  0.333568\n'
"pd.__version__ \n# '1.0.4'\n\ndf.apply(mul2)\nhello\nhello\n\n      a\n0  2.00\n1  4.00\n2  1.34\n3  2.68\n\npd.__version__\n# '1.1.0.dev0+2004.g8d10bfb6f'\n\ndf.apply(mul2)\nhello\n\n      a\n0  2.00\n1  4.00\n2  1.34\n3  2.68\n"
"import pandas as pd\n\nthelist = [ ['sentence 1'], ['sentence 2'], ['sentence 3'] ]\ndf = pd.Series( (v[0] for v in thelist) )\n"
"for index, row in df.iterrows():\n    print row['Date']\n"
'import sys\nprint(sys.getsizeof(OBEJCT_NAME_HERE))\n\n%mprun?\n%memit?\n\n%load_ext memory_profiler\ndef lol(x):\n    return x\n%memit lol(500)\n#output --- peak memory: 48.31 MiB, increment: 0.00 MiB\n'
"pd.pivot_table(df, values = 'Value', index=['Country','Year'], columns = 'Indicator').reset_index()\n\n Indicator  Country     Year    1   2   3   4   5\n 0          Angola      2005    6   13  10  11  5\n 1          Angola      2006    3   2   7   3   6\n"
"In [235]:\n\ndf = pd.DataFrame({'Gene':s.index, 'count':s.values})\ndf\nOut[235]:\n   Gene  count\n0  Ezh2      2\n1  Hmgb      7\n2  Irf1      1\n\nIn [237]:\n\ndf = pd.DataFrame(s).reset_index()\ndf.columns = ['Gene', 'count']\ndf\nOut[237]:\n   Gene  count\n0  Ezh2      2\n1  Hmgb      7\n2  Irf1      1\n"
'# create a list with sheet numbers you want to process\nsheets = map(str,range(1,6))\n\n# convert each sheet to csv and then read it using read_csv\ndf={}\nfrom subprocess import call\nexcel=\'C:\\\\Users\\\\rsignell\\\\OTT_Data_All_stations.xlsx\'\nfor sheet in sheets:\n    csv = \'C:\\\\Users\\\\rsignell\\\\test\' + sheet + \'.csv\' \n    call([\'cscript.exe\', \'C:\\\\Users\\\\rsignell\\\\ExcelToCsv.vbs\', excel, csv, sheet])\n    df[sheet]=pd.read_csv(csv)\n\n#write vbscript to file\nvbscript="""if WScript.Arguments.Count &lt; 3 Then\n    WScript.Echo "Please specify the source and the destination files. Usage: ExcelToCsv &lt;xls/xlsx source file&gt; &lt;csv destination file&gt; &lt;worksheet number (starts at 1)&gt;"\n    Wscript.Quit\nEnd If\n\ncsv_format = 6\n\nSet objFSO = CreateObject("Scripting.FileSystemObject")\n\nsrc_file = objFSO.GetAbsolutePathName(Wscript.Arguments.Item(0))\ndest_file = objFSO.GetAbsolutePathName(WScript.Arguments.Item(1))\nworksheet_number = CInt(WScript.Arguments.Item(2))\n\nDim oExcel\nSet oExcel = CreateObject("Excel.Application")\n\nDim oBook\nSet oBook = oExcel.Workbooks.Open(src_file)\noBook.Worksheets(worksheet_number).Activate\n\noBook.SaveAs dest_file, csv_format\n\noBook.Close False\noExcel.Quit\n""";\n\nf = open(\'ExcelToCsv.vbs\',\'w\')\nf.write(vbscript.encode(\'utf-8\'))\nf.close()\n'
"In [277]:\n\ndf = pd.DataFrame({'a':np.arange(10), 'b':np.random.randn(10)})\ndf\nOut[277]:\n   a         b\n0  0  0.293422\n1  1 -1.631018\n2  2  0.065344\n3  3 -0.417926\n4  4  1.925325\n5  5  0.167545\n6  6 -0.988941\n7  7 -0.277446\n8  8  1.426912\n9  9 -0.114189\nIn [278]:\n\ndf.index\nOut[278]:\nInt64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n"
"df.loc(axis=0)[:, :, 'C1', :]\n"
"engine = sa.create_engine('postgresql:///somedb')\npandas_sql = pd.io.sql.pandasSQL_builder(engine, schema=None, flavor=None)\n\ndef to_sql_k(self, frame, name, if_exists='fail', index=True,\n           index_label=None, schema=None, chunksize=None, dtype=None, **kwargs):\n    if dtype is not None:\n        from sqlalchemy.types import to_instance, TypeEngine\n        for col, my_type in dtype.items():\n            if not isinstance(to_instance(my_type), TypeEngine):\n                raise ValueError('The type of %s is not a SQLAlchemy '\n                                 'type ' % col)\n\n    table = pd.io.sql.SQLTable(name, self, frame=frame, index=index,\n                     if_exists=if_exists, index_label=index_label,\n                     schema=schema, dtype=dtype, **kwargs)\n    table.create()\n    table.insert(chunksize)\n\nto_sql_k(pandas_sql, df2save, 'tmp',\n        index=True, index_label='id', keys='id', if_exists='replace')\n\nCREATE TABLE public.tmp\n(\n  id bigint NOT NULL DEFAULT nextval('tmp_id_seq'::regclass),\n...\n)\n"
"df1.merge(df2,how='left', left_on='Column1', right_on='ColumnA')\n"
'generated-members=pandas.*\n'
"transactions = pd.read_csv('transactions.csv', sep=r'\\s*,\\s*',\n                           header=0, encoding='ascii', engine='python')\n\nprint(transactions.columns.tolist())\n\n['product_id', 'customer_id', 'store_id', 'promotion_id', 'month_of_year', 'quarter', 'the_year', 'store_sales', 'store_cost', 'unit_sales', 'fact_count']\n"
'import numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import acf\nimport matplotlib.pyplot as plt\nplt.style.use("seaborn-colorblind")\n\ndef autocorr_by_hand(x, lag):\n    # Slice the relevant subseries based on the lag\n    y1 = x[:(len(x)-lag)]\n    y2 = x[lag:]\n    # Subtract the subseries means\n    sum_product = np.sum((y1-np.mean(y1))*(y2-np.mean(y2)))\n    # Normalize with the subseries stds\n    return sum_product / ((len(x) - lag) * np.std(y1) * np.std(y2))\n\ndef acf_by_hand(x, lag):\n    # Slice the relevant subseries based on the lag\n    y1 = x[:(len(x)-lag)]\n    y2 = x[lag:]\n    # Subtract the mean of the whole series x to calculate Cov\n    sum_product = np.sum((y1-np.mean(x))*(y2-np.mean(x)))\n    # Normalize with var of whole series\n    return sum_product / ((len(x) - lag) * np.var(x))\n\nx = np.linspace(0,100,101)\n\nresults = {}\nnlags=10\nresults["acf_by_hand"] = [acf_by_hand(x, lag) for lag in range(nlags)]\nresults["autocorr_by_hand"] = [autocorr_by_hand(x, lag) for lag in range(nlags)]\nresults["autocorr"] = [pd.Series(x).autocorr(lag) for lag in range(nlags)]\nresults["acf"] = acf(x, unbiased=True, nlags=nlags-1)\n\npd.DataFrame(results).plot(kind="bar", figsize=(10,5), grid=True)\nplt.xlabel("lag")\nplt.ylim([-1.2, 1.2])\nplt.ylabel("value")\nplt.show()\n'
"In [36]:\ndf['Group'] = df['Group'].map(df1.set_index('Group')['Hotel'])\ndf\n\nOut[36]:\n         Date  Group  Family  Bonus\n0  2011-06-09  Jamel  Laavin    456\n1  2011-07-09  Frank  Grendy    679\n2  2011-09-10   Luxy  Fantol    431\n3  2011-11-02  Frank  Gondow    569\n"
'In [2]: animals = pd.DataFrame({"monkey":[0,1,0,0,0],"rabbit":[1,0,0,0,0],"fox":[0,0,1,0,0]})\n\nIn [3]: def get_animal(row):\n   ...:     for c in animals.columns:\n   ...:         if row[c]==1:\n   ...:             return c\n\nIn [4]: animals.apply(get_animal, axis=1)\nOut[4]: \n0    rabbit\n1    monkey\n2       fox\n3      None\n4      None\ndtype: object\n'
"mask = df.genre.apply(lambda x: 'comedy' in x)\ndf1 = df[mask]\nprint (df1)\n                       genre\n0           [comedy, sci-fi]\n1  [action, romance, comedy]\n"
"df1['A'] = df1['A'].apply(lambda x: [y if y &lt;= 9 else 11 for y in x])\nprint (df1)\n                                A\n2017-01-01 02:00:00  [11, 11, 11]\n2017-01-01 03:00:00    [3, 11, 9]\n\na = np.array(df1['A'].values.tolist())\nprint (a)\n[[33 34 39]\n [ 3 43  9]]\n\ndf1['A'] = np.where(a &gt; 9, 11, a).tolist()\nprint (df1)\n                                A\n2017-01-01 02:00:00  [11, 11, 11]\n2017-01-01 03:00:00    [3, 11, 9]\n"
'import urllib.parse\nimport pandas as pd\nfrom pyfinance.ols import PandasRollingOLS\n\n# You can also do this with pandas-datareader; here\'s the hard way\nurl = "https://fred.stlouisfed.org/graph/fredgraph.csv"\n\nsyms = {\n    "TWEXBMTH" : "usd", \n    "T10Y2YM" : "term_spread", \n    "GOLDAMGBD228NLBM" : "gold",\n}\n\nparams = {\n    "fq": "Monthly,Monthly,Monthly",\n    "id": ",".join(syms.keys()),\n    "cosd": "2000-01-01",\n    "coed": "2019-02-01",\n}\n\ndata = pd.read_csv(\n    url + "?" + urllib.parse.urlencode(params, safe=","),\n    na_values={"."},\n    parse_dates=["DATE"],\n    index_col=0\n).pct_change().dropna().rename(columns=syms)\nprint(data.head())\n#                  usd  term_spread      gold\n# DATE                                       \n# 2000-02-01  0.012580    -1.409091  0.057152\n# 2000-03-01 -0.000113     2.000000 -0.047034\n# 2000-04-01  0.005634     0.518519 -0.023520\n# 2000-05-01  0.022017    -0.097561 -0.016675\n# 2000-06-01 -0.010116     0.027027  0.036599\n\ny = data.usd\nx = data.drop(\'usd\', axis=1)\n\nwindow = 12  # months\nmodel = PandasRollingOLS(y=y, x=x, window=window)\n\nprint(model.beta.head())  # Coefficients excluding the intercept\n#             term_spread      gold\n# DATE                             \n# 2001-01-01     0.000033 -0.054261\n# 2001-02-01     0.000277 -0.188556\n# 2001-03-01     0.002432 -0.294865\n# 2001-04-01     0.002796 -0.334880\n# 2001-05-01     0.002448 -0.241902\n\nprint(model.fstat.head())\n# DATE\n# 2001-01-01    0.136991\n# 2001-02-01    1.233794\n# 2001-03-01    3.053000\n# 2001-04-01    3.997486\n# 2001-05-01    3.855118\n# Name: fstat, dtype: float64\n\nprint(model.rsq.head())  # R-squared\n# DATE\n# 2001-01-01    0.029543\n# 2001-02-01    0.215179\n# 2001-03-01    0.404210\n# 2001-04-01    0.470432\n# 2001-05-01    0.461408\n# Name: rsq, dtype: float64\n'
'In [96]: df.rolling(window=len(df), min_periods=1).mean()[:5]\nOut[96]: \n                   A         B         C         D\n2000-01-01  0.314226 -0.001675  0.071823  0.892566\n2000-01-02  0.654522 -0.171495  0.179278  0.853361\n2000-01-03  0.708733 -0.064489 -0.238271  1.371111\n2000-01-04  0.987613  0.163472 -0.919693  1.566485\n2000-01-05  1.426971  0.288267 -1.358877  1.808650\n\nIn [97]: df.expanding(min_periods=1).mean()[:5]\nOut[97]: \n                   A         B         C         D\n2000-01-01  0.314226 -0.001675  0.071823  0.892566\n2000-01-02  0.654522 -0.171495  0.179278  0.853361\n2000-01-03  0.708733 -0.064489 -0.238271  1.371111\n2000-01-04  0.987613  0.163472 -0.919693  1.566485\n2000-01-05  1.426971  0.288267 -1.358877  1.808650\n'
"#if need also category c with no values of 'one'\ndf11=df.groupby('key1')['key2'].apply(lambda x: (x=='one').sum()).reset_index(name='count')\nprint (df11)\n  key1  count\n0    a      2\n1    b      1\n2    c      0\n\ndf['key1'] = df['key1'].astype('category')\ndf1 = df[df['key2'] == 'one'].groupby(['key1']).size().reset_index(name='count') \nprint (df1)\n  key1  count\n0    a      2\n1    b      1\n2    c      0\n\ndf2 = df.groupby(['key1', 'key2']).size().reset_index(name='count') \nprint (df2)\n  key1 key2  count\n0    a  one      2\n1    a  two      1\n2    b  one      1\n3    b  two      1\n4    c  two      1\n\ndf3 = df.groupby(['key1', 'key2']).size().unstack(fill_value=0)\nprint (df3)\nkey2  one  two\nkey1          \na       2    1\nb       1    1\nc       0    1\n"
'df.groupby([\'col1\', \'col2\'])["col3", "col4"].apply(lambda x : x.astype(int).sum())\nOut[1257]: \n           col3  col4\ncol1 col2            \na    c        2     4\n     d        1     2\nb    d        1     2\n     e        2     4\n\ndf.groupby([\'col1\', \'col2\']).agg({\'col3\':\'sum\',\'col4\':\'sum\'})\n'
'In [1756]: df.date + pd.DateOffset(months=plus_month_period)\nOut[1756]:\n0   2017-01-11\n1   2017-02-01\nName: date, dtype: datetime64[ns]\n\nIn [1785]: df.date + pd.offsets.MonthOffset(plus_month_period)\nOut[1785]:\n0   2016-10-14\n1   2016-11-04\nName: date, dtype: datetime64[ns]\n\nIn [1757]: df\nOut[1757]:\n        date\n0 2016-10-11\n1 2016-11-01\n\nIn [1758]: plus_month_period\nOut[1758]: 3\n'
"In [3361]: df.Firm_Name.str.split(expand=True).stack().value_counts()\nOut[3361]:\nSociety       3\nLtd           2\nJames's       1\nR.X.          1\nYah           1\nAssociates    1\nSt            1\nKensington    1\nMMV           1\nBig           1\n&amp;             1\nThe           1\nCo            1\nOil           1\nBuilding      1\ndtype: int64\n\npd.Series(np.concatenate([x.split() for x in df.Firm_Name])).value_counts()\n\npd.Series(' '.join(df.Firm_Name).split()).value_counts()\n\nIn [3379]: pd.Series(' '.join(df.Firm_Name).split()).value_counts()[:3]\nOut[3379]:\nSociety    3\nLtd        2\nJames's    1\ndtype: int64\n\nIn [3380]: df\nOut[3380]:\n      URN                   Firm_Name\n0  104472               R.X. Yah &amp; Co\n1  104873        Big Building Society\n2  109986          St James's Society\n3  114058  The Kensington Society Ltd\n4  113438      MMV Oil Associates Ltd\n"
'df.boxplot()\n\nsns.boxplot(x="variable", y="value", data=pd.melt(df))\n\nimport numpy as np; np.random.seed(42)\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.DataFrame(data = np.random.random(size=(4,4)), columns = [\'A\',\'B\',\'C\',\'D\'])\n\nsns.boxplot(x="variable", y="value", data=pd.melt(df))\n\nplt.show()\n\n          A         B         C         D\n0  0.374540  0.950714  0.731994  0.598658\n1  0.156019  0.155995  0.058084  0.866176\n2  0.601115  0.708073  0.020584  0.969910\n3  0.832443  0.212339  0.181825  0.183405\n\n   variable     value\n0         A  0.374540\n1         A  0.156019\n2         A  0.601115\n3         A  0.832443\n4         B  0.950714\n5         B  0.155995\n6         B  0.708073\n7         B  0.212339\n8         C  0.731994\n9         C  0.058084\n10        C  0.020584\n11        C  0.181825\n12        D  0.598658\n13        D  0.866176\n14        D  0.969910\n15        D  0.183405\n'
"In [36]: df\nOut[36]:\n   A  B  C  D\na  0  2  6  0\nb  6  1  5  2\nc  0  2  6  0\nd  9  3  2  2\n\nIn [37]: rows\nOut[37]: ['a', 'c']\n\nIn [38]: df.drop(rows)\nOut[38]:\n   A  B  C  D\nb  6  1  5  2\nd  9  3  2  2\n\nIn [39]: df[~((df.A == 0) &amp; (df.B == 2) &amp; (df.C == 6) &amp; (df.D == 0))]\nOut[39]:\n   A  B  C  D\nb  6  1  5  2\nd  9  3  2  2\n\nIn [40]: df.ix[rows]\nOut[40]:\n   A  B  C  D\na  0  2  6  0\nc  0  2  6  0\n\nIn [41]: df[((df.A == 0) &amp; (df.B == 2) &amp; (df.C == 6) &amp; (df.D == 0))]\nOut[41]:\n   A  B  C  D\na  0  2  6  0\nc  0  2  6  0\n"
"In [5]: df.xs('a', level=0)\nOut[5]: \n        value1  value2\ngroup2                \nc          1.1     7.1\nc          2.0     8.0\nd          3.0     9.0\n\nIn [6]: df.xs('c', level='group2')\nOut[6]: \n        value1  value2\ngroup1                \na          1.1     7.1\na          2.0     8.0\n"
"In [82]: df\nOut[82]: \n   X         Y\n0  0 -0.631214\n1  0  0.783142\n2  0  0.526045\n3  1 -1.750058\n4  1  1.163868\n5  1  1.625538\n6  1  0.076105\n7  2  0.183492\n8  2  0.541400\n9  2 -0.672809\n\nIn [83]: def func(x):\n   ....:     x['NewCol'] = np.nan\n   ....:     return x\n   ....: \n\nIn [84]: df.groupby('X').apply(func)\nOut[84]: \n   X         Y  NewCol\n0  0 -0.631214     NaN\n1  0  0.783142     NaN\n2  0  0.526045     NaN\n3  1 -1.750058     NaN\n4  1  1.163868     NaN\n5  1  1.625538     NaN\n6  1  0.076105     NaN\n7  2  0.183492     NaN\n8  2  0.541400     NaN\n9  2 -0.672809     NaN\n"
"-rw-rw-r--  1 jreback users 203200986 May 19 20:58 test.csv\n-rw-rw-r--  1 jreback users  88007312 May 19 20:59 test.h5\n\nIn [1]: df = DataFrame(randn(1000000,10))\n\nIn [9]: df\nOut[9]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 1000000 entries, 0 to 999999\nData columns (total 10 columns):\n0    1000000  non-null values\n1    1000000  non-null values\n2    1000000  non-null values\n3    1000000  non-null values\n4    1000000  non-null values\n5    1000000  non-null values\n6    1000000  non-null values\n7    1000000  non-null values\n8    1000000  non-null values\n9    1000000  non-null values\ndtypes: float64(10)\n\nIn [5]: %timeit df.to_csv('test.csv',mode='w')\n1 loops, best of 3: 12.7 s per loop\n\nIn [6]: %timeit df.to_hdf('test.h5','df',mode='w')\n1 loops, best of 3: 825 ms per loop\n\nIn [7]: %timeit pd.read_csv('test.csv',index_col=0)\n1 loops, best of 3: 2.35 s per loop\n\nIn [8]: %timeit pd.read_hdf('test.h5','df')\n10 loops, best of 3: 38 ms per loop\n"
"In [11]: g = df.groupby('key1')\n\nIn [12]: g.apply(lambda x: x[x['key2'] == 'one']['data1'].sum())\nOut[12]:\nkey1\na       0.093391\nb       1.468194\ndtype: float64\n\nIn [21]: a = g.get_group('a')\n\nIn [22]: a\nOut[22]:\n      data1     data2 key1 key2\n0  0.361601  0.375297    a  one\n1  0.069889  0.809772    a  two\n4 -0.268210  1.250340    a  one\n\nIn [23]: a[a['key2'] == 'one']\nOut[23]:\n      data1     data2 key1 key2\n0  0.361601  0.375297    a  one\n4 -0.268210  1.250340    a  one\n\nIn [24]: a[a['key2'] == 'one']['data1']\nOut[24]:\n0    0.361601\n4   -0.268210\nName: data1, dtype: float64\n\nIn [25]: a[a['key2'] == 'one']['data1'].sum()\nOut[25]: 0.093391000000000002\n\nIn [31]: df1 = df[df['key2'] == 'one']\n\nIn [32]: df1\nOut[32]:\n      data1     data2 key1 key2\n0  0.361601  0.375297    a  one\n2  1.468194  0.272929    b  one\n4 -0.268210  1.250340    a  one\n\nIn [33]: df1.groupby('key1')['data1'].sum()\nOut[33]:\nkey1\na       0.093391\nb       1.468194\nName: data1, dtype: float64\n"
"In [7]: df = df.set_index(pd.DatetimeIndex(df['b']))\n\nIn [8]: df\nOut[8]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 100 entries, 2013-06-14 09:10:23.523845 to 2013-06-14 10:12:51.650043\nData columns (total 2 columns):\nb    100  non-null values\nc    100  non-null values\ndtypes: datetime64[ns](1), int64(1)\n\nIn [13]: pd.to_datetime(a,unit='us')\nOut[13]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2013-06-14 13:10:23.523845, ..., 2013-06-14 14:12:51.650043]\nLength: 100, Freq: None, Timezone: None\n"
'The character to used to denote the start and end of a quoted item. Quoted items \ncan include the delimiter and it will be ignored.\n\nIn [1]: import pandas as pd\n\nIn [2]: from StringIO import StringIO\n\nIn [3]: s="""year, city, value\n   ...: 2012, "Louisville KY", 3.5\n   ...: 2011, "Lexington, KY", 4.0"""\n\nIn [4]: pd.read_csv(StringIO(s), quotechar=\'"\', skipinitialspace=True)\nOut[4]:\n   year           city  value\n0  2012  Louisville KY    3.5\n1  2011  Lexington, KY    4.0\n'
"In [4]: df\nOut[4]:\n      label\n0  (a, c, e)\n1     (a, d)\n2       (b,)\n3     (d, e)\n\nIn [5]: df['label'].str.join(sep='*').str.get_dummies(sep='*')\nOut[5]:\n   a  b  c  d  e\n0  1  0  1  0  1\n1  1  0  0  1  0\n2  0  1  0  0  0\n3  0  0  0  1  1\n"
'print df.to_string(index=False)\n\ndf.to_latex(index=False)\n'
"In [11]: df.pivot_table(index='Account_number', columns='Product', \n                        aggfunc=len, fill_value=0)\nOut[11]:\nProduct         A  B\nAccount_number\n1               2  0\n2               1  2\n3               1  1\n"
"po_grouped_df = poagg_df.groupby(['EID','PCODE'], as_index=False)\n\nIn [356]: pd.merge(acc_df, pol_df, on=['EID','PCODE'], how='inner',suffixes=('_Acc','_Po'))\nOut[356]: \n   EID PCODE  SC_Acc  EE_Acc        SI_Acc  PVALUE_Acc  EE_Po  PVALUE_Po  \\\n0  123    GR     236   40000  1.805222e+31         350  10000         50   \n1  123    GR     236   40000  1.805222e+31         350  30000        300   \n2  123    GU     443   12000  8.765549e+87         250  10000        100   \n3  123    GU     443   12000  8.765549e+87         250   2000        150   \n\n   SC_Po  SI_Po  \n0     23     40  \n1    213    140  \n2    230    400  \n3    213    140  \n"
"if 'Met' not in df:\n    df['Met'] = df['freqC'] * df['coverage'] \n"
"In [68]: df['Auction_Rank'] = df.groupby('Auction_ID')['Bid_Price'].rank(ascending=False)\n\nIn [69]: df\nOut[69]:\n   Auction_ID  Bid_Price  Auction_Rank\n0         123          9             1\n1         123          7             2\n2         123          6             3\n3         123          2             4\n4         124          3             1\n5         124          2             2\n6         124          1             3\n7         125          1             1\n"
'slist =[]\nfor x in series:\n    slist.extend(x)\n\nslist = [st for row in s for st in row]\n'
"In [26]: df\nOut[26]: \n          a         b         c         d         e\n0 -1.079547 -0.722903  0.457495 -0.687271 -0.787058\n1  1.326133  1.359255 -0.964076 -1.280502  1.460792\n2  0.479599 -1.465210 -0.058247 -0.984733 -0.348068\n3 -0.608238 -1.238068 -0.126889  0.572662 -1.489641\n4 -1.533707 -0.218298 -0.877619  0.679370  0.485987\n5 -0.864651 -0.180165 -0.528939  0.270885  1.313946\n6  0.747612 -1.206509  0.616815 -1.758354 -0.158203\n7 -2.309582 -0.739730 -0.004303  0.125640 -0.973230\n8  1.735822 -0.750698  1.225104  0.431583 -1.483274\n9 -0.374557 -1.132354  0.875028  0.032615 -1.131971\n\nIn [27]: df['e'].iloc[-1]\nOut[27]: -1.1319705662711321\n\nIn [28]: df.e.iat[-1]\nOut[28]: -1.1319705662711321\n\nIn [31]: %timeit df.e.iat[-1]\n100000 loops, best of 3: 18 µs per loop\n\nIn [32]: %timeit df.e.iloc[-1]\n10000 loops, best of 3: 24 µs per loop\n"
'd.bar &amp; d.foo\n\n0     True\n1    False\n2    False\ndtype: bool\n'
"In [158]:\ndf['Result'] = df['Column A']/df['Column B']\ndf\n\nOut[158]:\n   Column A  Column B  Result\n0        12         2     6.0\n1        14         7     2.0\n2        16         8     2.0\n3        20         5     4.0\n"
'pd.Series(np.diag(df), index=[df.index, df.columns])\n\nA  a    2\nB  b    2\nC  c    3\ndtype: int64\n'
"sample['PR'] = sample['PR'].mask(sample['PR'] &lt; 90, np.nan)\n\nsample.loc[sample['PR'] &lt; 90, 'PR'] = np.nan\n\nimport pandas as pd\nimport numpy as np\n\nsample = pd.DataFrame({'PR':[10,100,40] })\nprint (sample)\n    PR\n0   10\n1  100\n2   40\n\nsample['PR'] = sample['PR'].mask(sample['PR'] &lt; 90, np.nan)\nprint (sample)\n      PR\n0    NaN\n1  100.0\n2    NaN\n\nsample.loc[sample['PR'] &lt; 90, 'PR'] = np.nan\nprint (sample)\n      PR\n0    NaN\n1  100.0\n2    NaN\n\nsample['PR'] = sample['PR'].apply(lambda x: np.nan if x &lt; 90 else x)\n\nsample = pd.concat([sample]*100000).reset_index(drop=True)\n\nIn [853]: %timeit sample['PR'].apply(lambda x: np.nan if x &lt; 90 else x)\n10 loops, best of 3: 102 ms per loop\n\nIn [854]: %timeit sample['PR'].mask(sample['PR'] &lt; 90, np.nan)\nThe slowest run took 4.28 times longer than the fastest. This could mean that an intermediate result is being cached.\n100 loops, best of 3: 3.71 ms per loop\n"
'df.to_sql(\'db_table2\', engine, if_exists=\'replace\')\n\ndf.to_sql(\'db_table2\', engine, if_exists=\'append\')\n\n"""\nif_exists : {\'fail\', \'replace\', \'append\'}, default \'fail\'\n    - fail: If table exists, do nothing.\n    - replace: If table exists, drop it, recreate it, and insert data.\n    - append: If table exists, insert data. Create if does not exist.\n"""\n'
'df.to_numpy().sum()\n\ndf.values\n\ndf.values.sum()\n'
'df = pd.melt(df, id_vars="class", var_name="sex", value_name="survival rate")\ndf\nOut: \n    class       sex  survival rate\n0   first       men       0.914680\n1  second       men       0.300120\n2   third       men       0.118990\n3   first     woman       0.667971\n4  second     woman       0.329380\n5   third     woman       0.189747\n6   first  children       0.660562\n7  second  children       0.882608\n8   third  children       0.121259\n\nsns.factorplot(x=\'class\', y=\'survival rate\', hue=\'sex\', data=df, kind=\'bar\')\n\nsns.catplot(x=\'class\', y=\'survival rate\', hue=\'sex\', data=df, kind=\'bar\')\n'
"data = []\n\n# always inserting new rows at the first position - last row will be always on top    \ndata.insert(0, {'name': 'dean', 'age': 45, 'sex': 'male'})\ndata.insert(0, {'name': 'joe', 'age': 33, 'sex': 'male'})\n#...\n\npd.concat([pd.DataFrame(data), df], ignore_index=True)\n\nIn [56]: pd.concat([pd.DataFrame(data), df], ignore_index=True)\nOut[56]:\n   age  name     sex\n0   33   joe    male\n1   45  dean    male\n2   30   jon    male\n3   25   sam    male\n4   18  jane  female\n5   26   bob    male\n"
"import pandas as pd\ndf = pd.read_excel('path_to_file.xlsb', engine='pyxlsb')\n"
"Past=pd.read_csv(&quot;C:/Users/Admin/Desktop/Python/Past.csv&quot;,encoding='utf-8') \n\nPast=pd.read_csv(&quot;C:/Users/Admin/Desktop/Python/Past.csv&quot;,encoding='cp1252')\n"
"out = (df1.merge(df2, left_on='store', right_on='store_code')\n          .reindex(columns=['id', 'store', 'address', 'warehouse']))\nprint(out)\n\n   id  store address warehouse\n0   1    100     xyz      Land\n1   2    200     qwe       Sea\n2   3    300     asd      Land\n3   4    400     zxc      Land\n4   5    500     bnm       Sea\n\nu = df1.sort_values('store')\nv = df2.sort_values('store_code')[['warehouse']].reset_index(drop=1)\nout = pd.concat([u, v], 1)\n\nprint(out)\n\n   id  store address warehouse\n0   1    100     xyz      Land\n1   2    200     qwe       Sea\n2   3    300     asd      Land\n3   4    400     zxc      Land\n4   5    500     bnm       Sea\n    \n\ns = df1.store.replace(df2.set_index('store_code')['warehouse'])\nprint(s) \n0    Land\n1     Sea\n2    Land\n3    Land\n4     Sea\n\ndf1['warehouse'] = s\nprint(df1)\n\n   id  store address warehouse\n0   1    100     xyz      Land\n1   2    200     qwe       Sea\n2   3    300     asd      Land\n3   4    400     zxc      Land\n4   5    500     bnm       Sea\n\nmapping = dict(df2[['store_code', 'warehouse']].values)\ndf1['warehouse'] = df1.store.map(mapping)\nprint(df1)\n\n   id  store address warehouse\n0   1    100     xyz      Land\n1   2    200     qwe       Sea\n2   3    300     asd      Land\n3   4    400     zxc      Land\n4   5    500     bnm       Sea\n"
"In [3]: df.loc['e'] = [1.0, 'hotel', 'true']\n\nIn [4]: df\nOut[4]:\n   number    variable values\na     NaN        bank   True\nb     3.0        shop  False\nc     0.5      market   True\nd     NaN  government   True\ne     1.0       hotel   true\n\nIn [17]: ix = pd.date_range('2018-11-10 00:00:00', periods=4, freq='30min')\n\nIn [18]: df = pd.DataFrame(np.random.randint(100, size=(4,3)), columns=list('abc'), index=ix)\n\nIn [19]: df\nOut[19]:\n                      a   b   c\n2018-11-10 00:00:00  77  64  90\n2018-11-10 00:30:00   9  39  26\n2018-11-10 01:00:00  63  93  72\n2018-11-10 01:30:00  59  75  37\n\nIn [20]: df.loc[pd.to_datetime('2018-11-10 02:00:00')] = [100,100,100]\n\nIn [21]: df\nOut[21]:\n                       a    b    c\n2018-11-10 00:00:00   77   64   90\n2018-11-10 00:30:00    9   39   26\n2018-11-10 01:00:00   63   93   72\n2018-11-10 01:30:00   59   75   37\n2018-11-10 02:00:00  100  100  100\n\nIn [22]: df.index\nOut[22]: DatetimeIndex(['2018-11-10 00:00:00', '2018-11-10 00:30:00', '2018-11-10 01:00:00', '2018-11-10 01:30:00', '2018-11-10 02:00:00'], dtype='da\ntetime64[ns]', freq=None)\n"
"result = json_normalize(data, 'counties', ['state', 'shortname', ['info', 'governor']])\n\n&gt;&gt;&gt; data = [{'state': 'Florida',\n...          'shortname': 'FL',\n...         'info': {'governor': 'Rick Scott'},\n...         'counties': [{'name': 'Dade', 'population': 12345},\n...                      {'name': 'Broward', 'population': 40000},\n...                      {'name': 'Palm Beach', 'population': 60000}]},\n...         {'state': 'Ohio',\n...          'shortname': 'OH',\n...          'info': {'governor': 'John Kasich'},\n...          'counties': [{'name': 'Summit', 'population': 1234},\n...                       {'name': 'Cuyahoga', 'population': 1337}]}]\n&gt;&gt;&gt; pprint(data[0]['counties'])\n[{'name': 'Dade', 'population': 12345},\n {'name': 'Broward', 'population': 40000},\n {'name': 'Palm Beach', 'population': 60000}]\n&gt;&gt;&gt; pprint(data[1]['counties'])\n[{'name': 'Summit', 'population': 1234},\n {'name': 'Cuyahoga', 'population': 1337}]\n\n&gt;&gt;&gt; json_normalize(data, 'counties')\n         name  population\n0        Dade       12345\n1     Broward       40000\n2  Palm Beach       60000\n3      Summit        1234\n4    Cuyahoga        1337\n\n&gt;&gt;&gt; data[0]['state'], data[0]['shortname'], data[0]['info']['governor']\n('Florida', 'FL', 'Rick Scott')\n&gt;&gt;&gt; data[1]['state'], data[1]['shortname'], data[1]['info']['governor']\n('Ohio', 'OH', 'John Kasich')\n&gt;&gt;&gt; json_normalize(data, 'counties', ['state', 'shortname', ['info', 'governor']])\n         name  population    state shortname info.governor\n0        Dade       12345  Florida        FL    Rick Scott\n1     Broward       40000  Florida        FL    Rick Scott\n2  Palm Beach       60000  Florida        FL    Rick Scott\n3      Summit        1234     Ohio        OH   John Kasich\n4    Cuyahoga        1337     Ohio        OH   John Kasich\n\n&gt;&gt;&gt; d['hits']['hits'][0]['_source']['authors']   # this value is None, and is skipped\n&gt;&gt;&gt; d['hits']['hits'][1]['_source']['authors']\n[{'affiliations': ['Punjabi University'],\n  'author_id': '780E3459',\n  'author_name': 'munish puri'},\n {'affiliations': ['Punjabi University'],\n  'author_id': '48D92C79',\n  'author_name': 'rajesh dhaliwal'},\n {'affiliations': ['Punjabi University'],\n  'author_id': '7D9BD37C',\n  'author_name': 'r s singh'}]\n&gt;&gt;&gt; d['hits']['hits'][2]['_source']['authors']\n[{'author_id': '7FF872BC',\n  'author_name': 'barbara eileen ryan'}]\n&gt;&gt;&gt; # etc.\n\n&gt;&gt;&gt; json_normalize(d['hits']['hits'], ['_source', 'authors'])\n           affiliations author_id          author_name\n0  [Punjabi University]  780E3459          munish puri\n1  [Punjabi University]  48D92C79      rajesh dhaliwal\n2  [Punjabi University]  7D9BD37C            r s singh\n3                   NaN  7FF872BC  barbara eileen ryan\n4                   NaN  0299B8E9     fraser j harbutt\n5                   NaN  7DAB7B72   richard m freeland\n\n&gt;&gt;&gt; json_normalize(\n...     data['hits']['hits'],\n...     ['_source', 'authors'],\n...     ['_id', ['_source', 'journal'], ['_source', 'title']]\n... )\n           affiliations author_id          author_name       _id   \\\n0  [Punjabi University]  780E3459          munish puri  7AF8EBC3  \n1  [Punjabi University]  48D92C79      rajesh dhaliwal  7AF8EBC3\n2  [Punjabi University]  7D9BD37C            r s singh  7AF8EBC3\n3                   NaN  7FF872BC  barbara eileen ryan  7521A721\n4                   NaN  0299B8E9     fraser j harbutt  7DAEB9A4\n5                   NaN  7DAB7B72   richard m freeland  7B3236C5\n\n                                     _source.journal\n0  Journal of Industrial Microbiology &amp; Biotechno...\n1  Journal of Industrial Microbiology &amp; Biotechno...\n2  Journal of Industrial Microbiology &amp; Biotechno...\n3                     The American Historical Review\n4                     The American Historical Review\n5                     The American Historical Review\n\n                                       _source.title  \\\n0  Development of a stable continuous flow immobi...\n1  Development of a stable continuous flow immobi...\n2  Development of a stable continuous flow immobi...\n3  Feminism and the women's movement : dynamics o...\n4  The iron curtain : Churchill, America, and the...\n5  The Truman Doctrine and the origins of McCarth...\n"
"import numpy as np\nfrom itertools import chain\n\n# return list from series of comma-separated strings\ndef chainer(s):\n    return list(chain.from_iterable(s.str.split(',')))\n\n# calculate lengths of splits\nlens = df['package'].str.split(',').map(len)\n\n# create new dataframe, repeating or chaining as appropriate\nres = pd.DataFrame({'order_id': np.repeat(df['order_id'], lens),\n                    'order_date': np.repeat(df['order_date'], lens),\n                    'package': chainer(df['package']),\n                    'package_code': chainer(df['package_code'])})\n\nprint(res)\n\n   order_id order_date package package_code\n0         1  20/5/2018      p1         #111\n0         1  20/5/2018      p2         #222\n0         1  20/5/2018      p3         #333\n1         3  22/5/2018      p4         #444\n2         7  23/5/2018      p5         #555\n2         7  23/5/2018      p6         #666\n"
'from flask import Flask, request, render_template, session, redirect\nimport numpy as np\nimport pandas as pd\n\n\napp = Flask(__name__)\n\ndf = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                   \'B\': [5, 6, 7, 8, 9],\n                   \'C\': [\'a\', \'b\', \'c--\', \'d\', \'e\']})\n\n\n@app.route(\'/\', methods=("POST", "GET"))\ndef html_table():\n\n    return render_template(\'simple.html\',  tables=[df.to_html(classes=\'data\')], titles=df.columns.values)\n\n\n\nif __name__ == \'__main__\':\n    app.run(host=\'0.0.0.0\')\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang="en"&gt;\n&lt;head&gt;\n    &lt;meta charset="UTF-8"&gt;\n    &lt;title&gt;Title&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n{% for table in tables %}\n            {{titles[loop.index]}}\n            {{ table|safe }}\n{% endfor %}\n&lt;/body&gt;\n&lt;/html&gt;\n\nreturn render_template(\'simple.html\',  tables=[df.to_html(classes=\'data\', header="true")])\n'
"df = male_trips.groupby('start_station_id').size()\n"
'In [149]:\n# dummy data\ntemp1 = """index col \n113 34%\n122 50%\n123 32%\n301 12%"""\n# custom function taken from https://stackoverflow.com/questions/12432663/what-is-a-clean-way-to-convert-a-string-percent-to-a-float\ndef p2f(x):\n    return float(x.strip(\'%\'))/100\n# pass to convertes param as a dict\ndf = pd.read_csv(io.StringIO(temp1), sep=\'\\s+\',index_col=[0], converters={\'col\':p2f})\ndf\nOut[149]:\n        col\nindex      \n113    0.34\n122    0.50\n123    0.32\n301    0.12\nIn [150]:\n# check that dtypes really are floats\ndf.dtypes\nOut[150]:\ncol    float64\ndtype: object\n'
"&gt;&gt;&gt; my_index = pd.MultiIndex(levels=[[],[],[]],\n                             labels=[[],[],[]],\n                             names=[u'one', u'two', u'three'])\n&gt;&gt;&gt; my_index\nMultiIndex(levels=[[], [], []],\n           labels=[[], [], []],\n           names=[u'one', u'two', u'three'])\n&gt;&gt;&gt; my_columns = [u'alpha', u'beta']\n&gt;&gt;&gt; df = pd.DataFrame(index=my_index, columns=my_columns)\n&gt;&gt;&gt; df\nEmpty DataFrame\nColumns: [alpha, beta]\nIndex: []\n&gt;&gt;&gt; df.loc[('apple','banana','cherry'),:] = [0.1, 0.2]\n&gt;&gt;&gt; df\n                    alpha beta\none   two    three            \napple banana cherry   0.1  0.2\n"
"df.columns = df.columns.swaplevel(0, 1)\ndf.sortlevel(0, axis=1, inplace=True)\n&gt;&gt;&gt; df\n\nmonth   '1Jan'                 'Feb'                 'Mar'              \n        weight  extent  rank  weight  extent  rank  weight  extent  rank\nyear                                                                    \n2000      45.1  13.442    13    46.1   14.94    17    25.1   15.02    14\n2001      85.0  13.380    12    16.0   14.81    15    49.0   15.14    17\n2002      90.0  13.590    15    33.0   15.13    22    82.0   14.88    10\n2003      47.0  13.640    17    34.0   14.83    16    78.0   15.27    22\n\ndf.to_csv(filename)\n\ndf.sort_index(axis=1, level=0, inplace=True)\n"
'df = df.fillna(df.mode().iloc[0])\n'
"df = pd.get_dummies(df, columns=['type'])\n"
'$python\n&gt;&gt;import numpy\n&gt;&gt;print(numpy)\n'
'In [11]: s.loc[(\'b\', slice(2, 10))]\nOut[11]:\nb  2   -0.65394\n   4    0.08227\ndtype: float64\n\nIn [12]: s.loc[(slice(\'a\', \'b\'), slice(2, 10))]\nOut[12]:\na  5    0.27919\nb  2   -0.65394\n   4    0.08227\ndtype: float64\n\ns.ix[1:10, "b"]\n\ns["b"].iloc[1:10]\n'
"d.groupby(['ip', 'useragent']).size()\n\nip          useragent               \n192.168.0.1 a           2\n            b           1\n192.168.0.2 b           1\n"
'z[k] = sum_n a[n] * conj(v[n+k])\n\ncov = np.cov(a, b)\nbeta = cov[1, 0] / cov[0, 0]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(100)\n\n\ndef geometric_brownian_motion(T=1, N=100, mu=0.1, sigma=0.01, S0=20):\n    """\n    http://stackoverflow.com/a/13203189/190597 (unutbu)\n    """\n    dt = float(T) / N\n    t = np.linspace(0, T, N)\n    W = np.random.standard_normal(size=N)\n    W = np.cumsum(W) * np.sqrt(dt)  # standard brownian motion ###\n    X = (mu - 0.5 * sigma ** 2) * t + sigma * W\n    S = S0 * np.exp(X)  # geometric brownian motion ###\n    return S\n\nN = 10 ** 6\na = geometric_brownian_motion(T=1, mu=0.1, sigma=0.01, N=N)\nb = geometric_brownian_motion(T=1, mu=0.2, sigma=0.01, N=N)\n\ncov = np.cov(a, b)\nprint(cov)\n# [[ 0.38234755  0.80525967]\n#  [ 0.80525967  1.73517501]]\nbeta = cov[1, 0] / cov[0, 0]\nprint(beta)\n# 2.10609347015\n\nplt.plot(a)\nplt.plot(b)\nplt.show()\n\nimport pandas as pd\ndf = pd.DataFrame({\'a\': a, \'b\': b})\nbeta2 = (df.corr() * df[\'b\'].std() * df[\'a\'].std() / df[\'a\'].var()).ix[0, 1]\nprint(beta2)\n# 2.10609347015\nassert np.allclose(beta, beta2)\n'
"In [130]: s\nOut[130]:\n0           1\n1999-03-31  SOLD_PRICE   NaN\n1999-06-30  SOLD_PRICE   NaN\n1999-09-30  SOLD_PRICE   NaN\n1999-12-31  SOLD_PRICE     3\n2000-03-31  SOLD_PRICE     3\nName: 2, dtype: float64\n\nIn [131]: s.reset_index()\nOut[131]:\n            0           1   2\n0  1999-03-31  SOLD_PRICE NaN\n1  1999-06-30  SOLD_PRICE NaN\n2  1999-09-30  SOLD_PRICE NaN\n3  1999-12-31  SOLD_PRICE   3\n4  2000-03-31  SOLD_PRICE   3\n\nIn [136]: s.reset_index(0).reset_index(drop=True)\nOut[136]:\n            0   2\n0  1999-03-31 NaN\n1  1999-06-30 NaN\n2  1999-09-30 NaN\n3  1999-12-31   3\n4  2000-03-31   3\n\nIn [137]: df = s.reset_index()\n\nIn [138]: df\nOut[138]:\n            0           1   2\n0  1999-03-31  SOLD_PRICE NaN\n1  1999-06-30  SOLD_PRICE NaN\n2  1999-09-30  SOLD_PRICE NaN\n3  1999-12-31  SOLD_PRICE   3\n4  2000-03-31  SOLD_PRICE   3\n\nIn [139]: del df[1]\n\nIn [140]: df\nOut[140]:\n            0   2\n0  1999-03-31 NaN\n1  1999-06-30 NaN\n2  1999-09-30 NaN\n3  1999-12-31   3\n4  2000-03-31   3\n\nIn [144]: s.reset_index().drop(1, axis=1)\nOut[144]:\n            0   2\n0  1999-03-31 NaN\n1  1999-06-30 NaN\n2  1999-09-30 NaN\n3  1999-12-31   3\n4  2000-03-31   3\n\nIn [146]: df.columns = ['Date', 'Sales']\n\nIn [147]: df\nOut[147]:\n         Date  Sales\n0  1999-03-31    NaN\n1  1999-06-30    NaN\n2  1999-09-30    NaN\n3  1999-12-31      3\n4  2000-03-31      3\n"
'&gt;&gt;&gt; df = pd.read_csv("test_data2.csv", index_col=[0,1], skipinitialspace=True)\n&gt;&gt;&gt; df\n                       dep  freq   arr   code  mode\nfrom       to                                      \nRGBOXFD    RGBPADTON   127     0    27  99999     2\n           RGBPADTON   127     0    33  99999     2\n           RGBRDLEY    127     0  1425  99999     2\n           RGBCHOLSEY  127     0    52  99999     2\n           RGBMDNHEAD  127     0    91  99999     2\nRGBDIDCOTP RGBPADTON   127     0    46  99999     2\n           RGBPADTON   127     0     3  99999     2\n           RGBCHOLSEY  127     0    61  99999     2\n           RGBRDLEY    127     0  1430  99999     2\n           RGBPADTON   127     0   115  99999     2\n'
"&gt;&gt;&gt; def GroupColFunc(df, ind, col):\n...     if df[col].loc[ind] &gt; 1:\n...         return 'Group1'\n...     else:\n...         return 'Group2'\n... \n\n&gt;&gt;&gt; people.groupby(lambda x: GroupColFunc(people, x, 'a')).sum()\n               a         b         c         d        e\nGroup2 -2.384614 -0.762208  3.359299 -1.574938 -2.65963\n\n&gt;&gt;&gt; people.groupby(lambda x: 'Group1' if people['b'].loc[x] &gt; people['a'].loc[x] else 'Group2').sum()\n               a         b         c         d         e\nGroup1 -3.280319 -0.007196  1.525356  0.324154 -1.002439\nGroup2  0.895705 -0.755012  1.833943 -1.899092 -1.657191\n\n&gt;&gt;&gt; mapping = np.where(people['b'] &gt; people['a'], 'Group1', 'Group2')\n&gt;&gt;&gt; mapping\nJoe       Group2\nSteve     Group1\nWes       Group2\nJim       Group1\nTravis    Group1\ndtype: string48\n&gt;&gt;&gt; people.groupby(mapping).sum()\n               a         b         c         d         e\nGroup1 -3.280319 -0.007196  1.525356  0.324154 -1.002439\nGroup2  0.895705 -0.755012  1.833943 -1.899092 -1.657191\n"
"In [1]: df = DataFrame(np.random.randn(10,2))\n\nIn [2]: df.iloc[3:6,0] = np.nan\n\nIn [3]: df\nOut[3]: \n          0         1\n0 -0.560342  1.862640\n1 -1.237742  0.596384\n2  0.603539 -1.561594\n3       NaN  3.018954\n4       NaN -0.046759\n5       NaN  0.480158\n6  0.113200 -0.911159\n7  0.990895  0.612990\n8  0.668534 -0.701769\n9 -0.607247 -0.489427\n\n[10 rows x 2 columns]\n\nIn [4]: df.describe()\nOut[4]: \n              0          1\ncount  7.000000  10.000000\nmean  -0.004166   0.286042\nstd    0.818586   1.363422\nmin   -1.237742  -1.561594\n25%   -0.583795  -0.648684\n50%    0.113200   0.216699\n75%    0.636036   0.608839\nmax    0.990895   3.018954\n\n[8 rows x 2 columns]\n\n\nIn [5]: df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 10 entries, 0 to 9\nData columns (total 2 columns):\n0    7 non-null float64\n1    10 non-null float64\ndtypes: float64(2)\n\nIn [20]: len(df.index)-df.count()\nOut[20]: \n0    3\n1    0\ndtype: int64\n\nIn [23]: df.isnull().sum()\nOut[23]: \n0    3\n1    0\ndtype: int64\n"
"df.index = pd.Series(df.index).fillna(method='ffill')\n\nIn [42]: df\nOut[42]: \n       Sample    CD4   CD8\nDay 1    8311  17.30  6.44\nNaN      8312  13.60  3.50\nNaN      8321  19.80  5.88\nNaN      8322  13.50  4.09\nDay 2    8311  16.00  4.92\nNaN      8312   5.67  2.28\nNaN      8321  13.00  4.34\nNaN      8322  10.60  1.95\n\n[8 rows x 3 columns]\n\nIn [43]: df.index = pd.Series(df.index).fillna(method='ffill')\n\nIn [44]: df\nOut[44]: \n       Sample    CD4   CD8\nDay 1    8311  17.30  6.44\nDay 1    8312  13.60  3.50\nDay 1    8321  19.80  5.88\nDay 1    8322  13.50  4.09\nDay 2    8311  16.00  4.92\nDay 2    8312   5.67  2.28\nDay 2    8321  13.00  4.34\nDay 2    8322  10.60  1.95\n\n[8 rows x 3 columns]\n"
"frame['c'] = frame.fillna(0)['a'] + frame.fillna(0)['b']\n\nframe['c'] = frame.a.fillna(0) + frame.b.fillna(0)\n\n    a   b  c\n0   1   3  4\n1   2 NaN  2\n2 NaN   4  4\n"
'In [11]: df = pd.DataFrame({"t": pd.date_range(\'2014-01-01\', periods=5, freq=\'H\')})\n\nIn [12]: df\nOut[12]:\n                    t\n0 2014-01-01 00:00:00\n1 2014-01-01 01:00:00\n2 2014-01-01 02:00:00\n3 2014-01-01 03:00:00\n4 2014-01-01 04:00:00\n\nIn [13]: pd.DatetimeIndex(df.t).normalize()\nOut[13]:\n&lt;class \'pandas.tseries.index.DatetimeIndex\'&gt;\n[2014-01-01, ..., 2014-01-01]\nLength: 5, Freq: None, Timezone: None\n\nIn [14]: df[\'date\'] = pd.DatetimeIndex(df.t).normalize()\n\nIn [15]: df\nOut[15]:\n                    t       date\n0 2014-01-01 00:00:00 2014-01-01\n1 2014-01-01 01:00:00 2014-01-01\n2 2014-01-01 02:00:00 2014-01-01\n3 2014-01-01 03:00:00 2014-01-01\n4 2014-01-01 04:00:00 2014-01-01\n\ndf.t.dt.normalize()\n# equivalent to\npd.DatetimeIndex(df.t).normalize()\n'
'df = DataFrame({"A":[0,0.5,1.0,3.5,4.0,4.5], "B":[1,4,6,2,4,3], "C":[3,2,1,0,5,3]})\n\nIn [64]: df.set_index("A")\nOut[64]: \n     B  C\n A        \n0.0  1  3\n0.5  4  2\n1.0  6  1\n3.5  2  0\n4.0  4  5\n4.5  3  3\n\nIn [66]: new_index = Index(arange(0,5,0.5), name="A")\nIn [67]: df.set_index("A").reindex(new_index)\nOut[67]: \n      B   C\n0.0   1   3\n0.5   4   2\n1.0   6   1\n1.5 NaN NaN\n2.0 NaN NaN\n2.5 NaN NaN\n3.0 NaN NaN\n3.5   2   0\n4.0   4   5\n4.5   3   3\n\nIn [69]: df.set_index("A").reindex(new_index).reset_index()\nOut[69]: \n       A   B   C\n0    0.0   1   3\n1    0.5   4   2\n2    1.0   6   1\n3    1.5 NaN NaN\n4    2.0 NaN NaN\n5    2.5 NaN NaN\n6    3.0 NaN NaN\n7    3.5   2   0\n8    4.0   4   5\n9    4.5   3   3\n'
'df[\'qty_s\'] = df[\'qty\'].shift(-1)\ndf[\'t_s\'] = df[\'t\'].shift(-1)\ndf[\'z_s\'] = df[\'z\'].shift(-1)\n\ndf[\'is_something\'] = (df[\'qty\'] == df[\'qty_s\']) &amp; (df[\'t\'] &lt; df[\'t_s\']) &amp; (df[\'z\'] == df[\'z_s\'])\n\ndf = pd.DataFrame({"temp_celcius":pd.np.random.choice(10, 10) + 20}, index=pd.date_range("2015-05-15", "2015-05-24")) \ndf\n            temp_celcius\n\n2015-05-15            21\n2015-05-16            28\n2015-05-17            27\n2015-05-18            21\n2015-05-19            25\n2015-05-20            28\n2015-05-21            25\n2015-05-22            22\n2015-05-23            29\n2015-05-24            25\n\ndf["temp_c_yesterday"] = df["temp_celcius"].shift(1)\ndf\n            temp_celcius  temp_c_yesterday\n2015-05-15            21               NaN\n2015-05-16            28                21\n2015-05-17            27                28\n2015-05-18            21                27\n2015-05-19            25                21\n2015-05-20            28                25\n2015-05-21            25                28\n2015-05-22            22                25\n2015-05-23            29                22\n2015-05-24            25                29\n\ndf["warmer_than_yesterday"] = df["temp_celcius"] &gt; df["temp_c_yesterday"]\n            temp_celcius  temp_c_yesterday warmer_than_yesterday\n2015-05-15            21               NaN                 False\n2015-05-16            28                21                  True\n2015-05-17            27                28                 False\n2015-05-18            21                27                 False\n2015-05-19            25                21                  True\n2015-05-20            28                25                  True\n2015-05-21            25                28                 False\n2015-05-22            22                25                 False\n2015-05-23            29                22                  True\n2015-05-24            25                29                 False\n'
"In [156]:\ndf['cumsum'] = df.groupby('id')['val'].transform(pd.Series.cumsum)\ndf\n\nOut[156]:\n  id   stuff  val  cumsum\n0  A      12    1       1\n1  B   23232    2       2\n2  A      13   -3      -2\n3  C    1234    1       1\n4  D    3235    5       5\n5  B    3236    6       8\n6  C  732323   -2      -1\n\nIn [159]:\ndf.groupby('id')['val'].cumsum()\n\nOut[159]:\n0    1\n1    2\n2   -2\n3    1\n4    5\n5    8\n6   -1\ndtype: int64\n"
"df.div(df.sum(axis=1), axis=0)\n\nimport pandas as pd\n\ndf = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n&gt;&gt;&gt; df.div(df.sum(axis=1), axis=0)\n    a   b\n0   0.250000    0.750000\n1   0.333333    0.666667\n"
"df[(df['col_name'].str.contains('apple')) &amp; (df['col_name'].str.contains('banana'))]\n"
'p = p.reindex(columns=[\'1Sun\', \'2Mon\', \'3Tue\', \'4Wed\', \'5Thu\', \'6Fri\', \'7Sat\'])\n\ndf = pd.read_csv(CsvFileName)\n\np = df.pivot_table(index=[\'Hour\'], columns=\'DOW\', values=\'Changes\', aggfunc=np.mean).round(0)\np.fillna(0, inplace=True)\n\ncolumns = ["1Sun", "2Mon", "3Tue", "4Wed", "5Thu", "6Fri", "7Sat"]\np = p.reindex(columns=columns)\np[columns] = p[columns].astype(int)\n'
'In [26]: df = pd.DataFrame(np.random.random((3,6)), columns=[1,2,3,4,5,\'Class\'])\n\nIn [27]: df\nOut[27]: \n          1         2         3         4         5     Class\n0  0.773423  0.865091  0.614956  0.219458  0.837748  0.862177\n1  0.544805  0.535341  0.323215  0.929041  0.042705  0.759294\n2  0.215638  0.251063  0.648350  0.353999  0.986773  0.483313\n\nIn [28]: df.columns.map(type)\nOut[28]: \narray([&lt;class \'int\'&gt;, &lt;class \'int\'&gt;, &lt;class \'int\'&gt;, &lt;class \'int\'&gt;,\n       &lt;class \'int\'&gt;, &lt;class \'str\'&gt;], dtype=object)\n\nIn [29]: df.to_hdf("out.h5", "d1")\nC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:260: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type-&gt;mixed-integer,key-&gt;axis0] [items-&gt;None]\n\n  f(store)\nC:\\Anaconda3\\lib\\site-packages\\pandas\\io\\pytables.py:260: PerformanceWarning: \nyour performance may suffer as PyTables will pickle object types that it cannot\nmap directly to c-types [inferred_type-&gt;mixed-integer,key-&gt;block0_items] [items-&gt;None]\n\n  f(store)\n\nIn [30]: df.columns = df.columns.astype(str)\n\nIn [31]: df.columns.map(type)\nOut[31]: \narray([&lt;class \'str\'&gt;, &lt;class \'str\'&gt;, &lt;class \'str\'&gt;, &lt;class \'str\'&gt;,\n       &lt;class \'str\'&gt;, &lt;class \'str\'&gt;], dtype=object)\n\nIn [32]: df.to_hdf("out.h5", "d1")\n\nIn [33]:\n'
"df = pd.merge(df1, df2, on=['User','Movie'], how='left', indicator='Exist')\ndf.drop('Rating', inplace=True, axis=1)\ndf['Exist'] = np.where(df.Exist == 'both', True, False)\nprint (df)\n   User  Movie  Exist\n0     1    333  False\n1     1   1193   True\n2     1      3  False\n3     2    433  False\n4     3     54   True\n5     3    343  False\n6     3     76   True\n"
"In [17]: df\nOut[17]: \n     regiment company deaths battles size\n0  Nighthawks     1st    kkk       5    l\n1  Nighthawks     1st     52      42   ll\n2  Nighthawks     2nd     25       2    l\n3  Nighthawks     2nd    616       2    m\n\nIn [18]: df.apply(lambda x: x.astype(str).str.upper())\nOut[18]: \n     regiment company deaths battles size\n0  NIGHTHAWKS     1ST    KKK       5    L\n1  NIGHTHAWKS     1ST     52      42   LL\n2  NIGHTHAWKS     2ND     25       2    L\n3  NIGHTHAWKS     2ND    616       2    M\n\nIn [42]: df2 = df.apply(lambda x: x.astype(str).str.upper())\n\nIn [43]: df2['battles'] = pd.to_numeric(df2['battles'])\n\nIn [44]: df2\nOut[44]: \n     regiment company deaths  battles size\n0  NIGHTHAWKS     1ST    KKK        5    L\n1  NIGHTHAWKS     1ST     52       42   LL\n2  NIGHTHAWKS     2ND     25        2    L\n3  NIGHTHAWKS     2ND    616        2    M\n\nIn [45]: df2.dtypes\nOut[45]: \nregiment    object\ncompany     object\ndeaths      object\nbattles      int64\nsize        object\ndtype: object\n"
"df['changed'] = df['ColumnB'].ne(df['ColumnB'].shift().bfill()).astype(int)\n\ndf = pd.concat([df]*10**5, ignore_index=True) \n\n%timeit df['ColumnB'].ne(df['ColumnB'].shift().bfill()).astype(int)\n10 loops, best of 3: 38.1 ms per loop\n\n%timeit (df.ColumnB != df.ColumnB.shift()).astype(int)\n10 loops, best of 3: 77.7 ms per loop\n\n%timeit df['ColumnB'] == df['ColumnB'].shift(1).fillna(df['ColumnB'])\n10 loops, best of 3: 99.6 ms per loop\n\n%timeit (df.ColumnB.ne(df.ColumnB.shift())).astype(int)\n10 loops, best of 3: 19.3 ms per loop\n"
"import json\n\nwith open('myJson.json') as data_file:    \n    data = json.load(data_file)  \n\ndf = pd.json_normalize(data, 'locations', ['date', 'number', 'name'], \n                    record_prefix='locations_')\nprint (df)\n  locations_arrTime locations_arrTimeDiffMin locations_depTime  \\\n0                                                        06:32   \n1             06:37                        1             06:40   \n2             08:24                        1                     \n\n  locations_depTimeDiffMin           locations_name locations_platform  \\\n0                        0  Spital am Pyhrn Bahnhof                  2   \n1                        0  Windischgarsten Bahnhof                  2   \n2                                    Linz/Donau Hbf               1A-B   \n\n  locations_stationIdx locations_track number    name        date  \n0                    0          R 3932         R 3932  01.10.2016  \n1                    1                         R 3932  01.10.2016  \n2                   22                         R 3932  01.10.2016 \n\ndf = pd.read_json(&quot;myJson.json&quot;)\ndf.locations = pd.DataFrame(df.locations.values.tolist())['name']\ndf = df.groupby(['date','name','number'])['locations'].apply(','.join).reset_index()\nprint (df)\n        date    name number                                          locations\n0 2016-01-10  R 3932         Spital am Pyhrn Bahnhof,Windischgarsten Bahnho... \n"
"df.A.ne('a').idxmax()\n\n3\n\n(df.A.values != 'a').argmax()\n\n3\n\ndf.A.searchsorted('a', side='right')\n\narray([3])\n\ndf.A.values.searchsorted('a', side='right')\n\n3\n"
"import pandas as pd\nimport numpy as np\ndef f(l):\n    if isinstance(l,(list,pd.core.series.Series,np.ndarray)):\n        print(5)\n    else:\n        raise Exception('wrong type')\n"
"col = pd.DataFrame(data, columns=['runs','balls', 'wickets', 'ground_average', 'pp_balls_left', 'total_overs'])\n\ncol = pd.DataFrame([data], columns=['runs','balls', 'wickets', 'ground_average', 'pp_balls_left', 'total_overs'])\n\na = [1, 2, 3]\n&gt;&gt;&gt; pd.DataFrame(a)\n   0\n0  1\n1  2\n2  3\n\n&gt;&gt;&gt; pd.DataFrame([a])\n   0  1  2\n0  1  2  3\n"
"In [2]: a = np.random.randint(10**4, size=10**6)\n   ...: b = np.random.randint(10**4, size=10**6)\n   ...: a_list = a.tolist()\n   ...: b_list = b.tolist()\n\nIn [3]: %timeit Counter(zip(a, b))\n455 ms ± 4.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [4]: %timeit Counter(zip(a_list, b_list))\n334 ms ± 4.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\ndef grouper(df):\n    return df.groupby(['A', 'B'], sort=False).size()\n\ndef count(df):\n    return Counter(zip(df.A.values, df.B.values))\n\nfor m, n in [(10, 10**6), (10**3, 10**6), (10**7, 10**6)]:\n\n    df = pd.DataFrame({'A': np.random.randint(0, m, n),\n                       'B': np.random.randint(0, m, n)})\n\n    print(m, n)\n\n    %timeit grouper(df)\n    %timeit count(df)\n\nm       grouper   counter\n10      62.9 ms    315 ms\n10**3    191 ms    535 ms\n10**7    514 ms    459 ms\n"
"percent_missing = df.isnull().sum() * 100 / len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\n\nmissing_value_df.sort_values('percent_missing', inplace=True)\n\npercent_missing = df.isnull().sum() * 100 / len(df)\n"
"In [1]: import pandas as pd\n\nIn [2]: pd.to_datetime('2008-02-27')\nOut[2]: datetime.datetime(2008, 2, 27, 0, 0)\n\ndf.index = pd.to_datetime(df.index)\n\ndf['date_col'] = df['date_col'].apply(pd.to_datetime)\n"
"In [14]: df.groupby('Mt').first()\nOut[14]: \n   Sp  Value  count\nMt                 \ns1  a      1      3\ns2  c      3      5\ns3  f      6      6\n\nIn [28]: df.groupby('Mt', as_index=False).first()\nOut[28]: \n   Mt Sp  Value  count\n0  s1  a      1      3\n1  s2  c      3      5\n2  s3  f      6      6 \n\nIn [196]: df.sort('count', ascending=False).groupby('Mt', as_index=False).first()\nOut[196]: \n   Mt Sp  Value  count\n0  s1  a      1      3\n1  s2  e      5     10\n2  s3  f      6      6\n"
'df = pd.DataFrame.from_records(\n   [namedtuple_instance1, namedtuple_instance2],\n   columns=namedtuple_type._fields\n)\n\ndf = pd.DataFrame.from_records([dict(a=1, b=2), dict(a=2, b=3)])\n'
"df.groupby(level=0).filter(lambda x: len(x) &gt; 1)['type']\n"
'&gt;&gt;&gt; df = pd.DataFrame({"A": ["Hello", "this", "World", "apple"]})\n&gt;&gt;&gt; df.A.str.contains("Hello|World")\n0     True\n1    False\n2     True\n3    False\nName: A, dtype: bool\n&gt;&gt;&gt; ~df.A.str.contains("Hello|World")\n0    False\n1     True\n2    False\n3     True\nName: A, dtype: bool\n&gt;&gt;&gt; df[~df.A.str.contains("Hello|World")]\n       A\n1   this\n3  apple\n\n[2 rows x 1 columns]\n'
"In [11]: df_a = pd.DataFrame([[1], [2]], columns=['A'])\n\nIn [12]: df_b = pd.DataFrame([[3], [4]], columns=['B'])\n\nIn [13]: df_a.join(df_b)\nOut[13]: \n   A  B\n0  1  3\n1  2  4\n\nIn [14]: df_a.columns = df_a.columns.map(lambda x: str(x) + '_a')\n\nIn [15]: df_a\nOut[15]: \n   A_a\n0    1\n1    2\n\nIn [16]: df_b.columns = df_b.columns.map(lambda x: str(x) + '_b')\n\nIn [17]: df_a.join(df_b)\nOut[17]: \n   A_a  B_b\n0    1    3\n1    2    4\n"
"import pytz\neastern = pytz.timezone('US/Eastern')\ndf.index = df.index.tz_localize(pytz.utc).tz_convert(eastern)\n\nimport pandas as pd\nimport pytz\n\nindex = pd.date_range('20140101 21:55', freq='15S', periods=5)\ndf = pd.DataFrame(1, index=index, columns=['X'])\nprint(df)\n#                      X\n# 2014-01-01 21:55:00  1\n# 2014-01-01 21:55:15  1\n# 2014-01-01 21:55:30  1\n# 2014-01-01 21:55:45  1\n# 2014-01-01 21:56:00  1\n\n# [5 rows x 1 columns]\nprint(df.index)\n# &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n# [2014-01-01 21:55:00, ..., 2014-01-01 21:56:00]\n# Length: 5, Freq: 15S, Timezone: None\n\neastern = pytz.timezone('US/Eastern')\ndf.index = df.index.tz_localize(pytz.utc).tz_convert(eastern)\nprint(df)\n#                            X\n# 2014-01-01 16:55:00-05:00  1\n# 2014-01-01 16:55:15-05:00  1\n# 2014-01-01 16:55:30-05:00  1\n# 2014-01-01 16:55:45-05:00  1\n# 2014-01-01 16:56:00-05:00  1\n\n# [5 rows x 1 columns]\n\nprint(df.index)\n# &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n# [2014-01-01 16:55:00-05:00, ..., 2014-01-01 16:56:00-05:00]\n# Length: 5, Freq: 15S, Timezone: US/Eastern\n"
"row_iterator = df.iterrows()\n_, last = row_iterator.next()  # take first item from row_iterator\nfor i, row in row_iterator:\n    print(row['value'])\n    print(last['value'])\n    last = row\n\nlast = df.irow(0)\nfor i in range(1, df.shape[0]):\n    print(last)\n    print(df.irow(i))\n    last = df.irow(i)\n"
"In [34]:\n\nData = [{u'_id': u'a1XHMhdHQB2uV7oq6dUldg',\n      u'_index': u'logstash-2014.08.07',\n      u'_score': 1.0,\n      u'_type': u'logs',\n      u'fields': {u'@timestamp': u'2014-08-07T12:36:00.086Z',\n       u'path': u'app2.log'}},\n     {u'_id': u'TcBvro_1QMqF4ORC-XlAPQ',\n      u'_index': u'logstash-2014.08.07',\n      u'_score': 1.0,\n      u'_type': u'logs',\n      u'fields': {u'@timestamp': u'2014-08-07T12:36:00.200Z',\n       u'path': u'app1.log'}}]\nIn [35]:\n\ndf = pd.concat(map(pd.DataFrame.from_dict, Data), axis=1)['fields'].T\nIn [36]:\n\nprint df.reset_index(drop=True)\n                 @timestamp      path\n0  2014-08-07T12:36:00.086Z  app2.log\n1  2014-08-07T12:36:00.200Z  app1.log\n"
"df[['A','C']].apply(lambda x: my_func(x) if(np.all(pd.notnull(x[1]))) else x, axis = 1)\n"
"# the dictionary to pass to pandas dataframe\nd = {}\n\n# a counter to use to add entries to &quot;dict&quot;\ni = 0 \n\n# Example data to loop and append to a dataframe\ndata = [{&quot;foo&quot;: &quot;foo_val_1&quot;, &quot;bar&quot;: &quot;bar_val_1&quot;}, \n       {&quot;foo&quot;: &quot;foo_val_2&quot;, &quot;bar&quot;: &quot;bar_val_2&quot;}]\n\n# the loop\nfor entry in data:\n\n    # add a dictionary entry to the final dictionary\n    d[i] = {&quot;col_1_title&quot;: entry['foo'], &quot;col_2_title&quot;: entry['bar']}\n    \n    # increment the counter\n    i = i + 1\n\n# create the dataframe using 'from_dict'\n# important to set the 'orient' parameter to &quot;index&quot; to make the keys as rows\ndf = DataFrame.from_dict(d, &quot;index&quot;)\n"
'dataset_array = dataset.values\nprint(dataset_array.dtype)\nprint(dataset_array)\n'
"In [159]:\n\ns = pd.Series(['race','gender'],index=[311,317])\ns\nOut[159]:\n311      race\n317    gender\ndtype: object\nIn [162]:\n\ns.values\nOut[162]:\narray(['race', 'gender'], dtype=object)\n\nIn [163]:\n\nlist(s)\nOut[163]:\n['race', 'gender']\n\nIn [164]:\n\nfor val in s:\n    print(val)\nrace\ngender\n"
"import datetime as dt\n\nfrom pandas.tseries.holiday import AbstractHolidayCalendar, Holiday, nearest_workday, \\\n    USMartinLutherKingJr, USPresidentsDay, GoodFriday, USMemorialDay, \\\n    USLaborDay, USThanksgivingDay\n\n\nclass USTradingCalendar(AbstractHolidayCalendar):\n    rules = [\n        Holiday('NewYearsDay', month=1, day=1, observance=nearest_workday),\n        USMartinLutherKingJr,\n        USPresidentsDay,\n        GoodFriday,\n        USMemorialDay,\n        Holiday('USIndependenceDay', month=7, day=4, observance=nearest_workday),\n        USLaborDay,\n        USThanksgivingDay,\n        Holiday('Christmas', month=12, day=25, observance=nearest_workday)\n    ]\n\n\ndef get_trading_close_holidays(year):\n    inst = USTradingCalendar()\n\n    return inst.holidays(dt.datetime(year-1, 12, 31), dt.datetime(year, 12, 31))\n\n\nif __name__ == '__main__':\n    print(get_trading_close_holidays(2016))\n    #    DatetimeIndex(['2016-01-01', '2016-01-18', '2016-02-15', '2016-03-25',\n    #                   '2016-05-30', '2016-07-04', '2016-09-05', '2016-11-24',\n    #                   '2016-12-26'],\n    #                  dtype='datetime64[ns]', freq=None)\n"
'#!/usr/bin/python\nimport pandas as pd\nimport glob\n\n# Grab all the csv files in the folder to a list.\nfileList = glob.glob(\'input_folder/*.csv\')\n\n#Initialize an empty dataframe to grab the csv content.\nall_data = pd.DataFrame()\n\n#Initialize an empty list to grab the dataframes.\ndfList= []\n\nfor files in  fileList:\n    df =  pd.read_csv(files, index_col = None, header= False)\n    dfList.append(df)\n\n#The frames will be in reverse order i.e last read file\'s content in the begining. So reverse it again\nReversed_dfList =  dfList[::-1]\nCombinedFrame =  pd.concat(Reversed_dfList)\n\n# The "Combined.csv" file will have combination of all the files.\nCombinedFrame.to_csv(\'output_folder/Combined.csv\', index=False)\n'
'occupation_counts = (df.groupby([\'income\'])[\'occupation\']\n                     .value_counts(normalize=True)\n                     .rename(\'percentage\')\n                     .mul(100)\n                     .reset_index()\n                     .sort_values(\'occupation\'))\np = sns.barplot(x="occupation", y="percentage", hue="income", data=occupation_counts)\n_ = plt.setp(p.get_xticklabels(), rotation=90)  # Rotate labels\n'
"import boto3\nimport pandas as pd\n\ns3 = boto3.client('s3')\nobj = s3.get_object(Bucket='bucket', Key='key')\ndf = pd.read_csv(obj['Body'])\n"
"pd.read_pickle('foo.pkl')\n"
"list(zip(s,s.index))\n\nIn [8]: s\nOut[8]: \na    1\nb    2\nc    3\ndtype: int64\n\nIn [9]: list(zip(s,s.index))\nOut[9]: [(1, 'a'), (2, 'b'), (3, 'c')]\n\nIn [10]: tuple(zip(s,s.index))\nOut[10]: ((1, 'a'), (2, 'b'), (3, 'c'))\n"
'In [285]:\nnunique = df.apply(pd.Series.nunique)\ncols_to_drop = nunique[nunique == 1].index\ndf.drop(cols_to_drop, axis=1)\n\nOut[285]:\n   index   id   name  data1\n0      0  345  name1      3\n1      1   12  name2      2\n2      5    2  name6      7\n\nIn [298]:\ncols = df.select_dtypes([np.number]).columns\ndiff = df[cols].diff().abs().sum()\ndf.drop(diff[diff== 0].index, axis=1)\n\u200b\nOut[298]:\n   index   id   name  data1\n0      0  345  name1      3\n1      1   12  name2      2\n2      5    2  name6      7\n\nIn [300]:\ncols = df.select_dtypes([np.number]).columns\nstd = df[cols].std()\ncols_to_drop = std[std==0].index\ndf.drop(cols_to_drop, axis=1)\n\nOut[300]:\n   index   id   name  data1\n0      0  345  name1      3\n1      1   12  name2      2\n2      5    2  name6      7\n\nIn [306]:\ndf.drop(df.std()[(df.std() == 0)].index, axis=1)\n\nOut[306]:\n   index   id   name  data1\n0      0  345  name1      3\n1      1   12  name2      2\n2      5    2  name6      7\n'
"df = df[['STNAME','CTYNAME']].groupby(['STNAME'])['CTYNAME'] \\\n                             .count() \\\n                             .reset_index(name='count') \\\n                             .sort_values(['count'], ascending=False) \\\n                             .head(5)\n\ndf = pd.DataFrame({'STNAME':list('abscscbcdbcsscae'),\n                   'CTYNAME':[4,5,6,5,6,2,3,4,5,6,4,5,4,3,6,5]})\n\nprint (df)\n    CTYNAME STNAME\n0         4      a\n1         5      b\n2         6      s\n3         5      c\n4         6      s\n5         2      c\n6         3      b\n7         4      c\n8         5      d\n9         6      b\n10        4      c\n11        5      s\n12        4      s\n13        3      c\n14        6      a\n15        5      e\n\ndf = df[['STNAME','CTYNAME']].groupby(['STNAME'])['CTYNAME'] \\\n                             .count() \\\n                             .reset_index(name='count') \\\n                             .sort_values(['count'], ascending=False) \\\n                             .head(5)\n\nprint (df)\n  STNAME  count\n2      c      5\n5      s      4\n1      b      3\n0      a      2\n3      d      1\n\ndf = df[['STNAME','CTYNAME']].groupby(['STNAME'])['CTYNAME'].count().nlargest(5)\n\ndf = df[['STNAME','CTYNAME']].groupby(['STNAME'])['CTYNAME'].size().nlargest(5)\n\ndf = pd.DataFrame({'STNAME':list('abscscbcdbcsscae'),\n                   'CTYNAME':[4,5,6,5,6,2,3,4,5,6,4,5,4,3,6,5]})\n\nprint (df)\n    CTYNAME STNAME\n0         4      a\n1         5      b\n2         6      s\n3         5      c\n4         6      s\n5         2      c\n6         3      b\n7         4      c\n8         5      d\n9         6      b\n10        4      c\n11        5      s\n12        4      s\n13        3      c\n14        6      a\n15        5      e\n\ndf = df[['STNAME','CTYNAME']].groupby(['STNAME'])['CTYNAME']\n                             .size()\n                             .nlargest(5)\n                             .reset_index(name='top5')\nprint (df)\n  STNAME  top5\n0      c     5\n1      s     4\n2      b     3\n3      a     2\n4      d     1\n"
'pip install --upgrade pip\npip install jupyter\n\n!pip install pandas\n\nsudo apt-get install python3-setuptools\nsudo easy_install3 pip\n'
"In [11]:\nd['Report Number'] = d['Report Number'].str[3:]\nd\n\nOut[11]:\n     Name Report Number\n0  George       1234567\n1    Bill       9876543\n2   Sally       4434555\n"
"df['nb_months'] = ((df.date2 - df.date1)/np.timedelta64(1, 'M'))\n\ndf['nb_months'] = df['nb_months'].astype(int)\n"
"rnm_cols = dict(size='Size', sum='Sum', mean='Mean', std='Std')\ndf.set_index(['Category', 'Item']).stack().groupby('Category') \\\n  .agg(rnm_cols.keys()).rename(columns=rnm_cols)\n\n            Size   Sum        Mean        Std\nCategory                                     \nBooks          3    58   19.333333   2.081666\nClothes        3   148   49.333333   4.041452\nTechnology     6  1800  300.000000  70.710678\n\nagg_funcs = dict(Size='size', Sum='sum', Mean='mean', Std='std')\ndf.set_index(['Category', 'Item']).stack().groupby(level=0).agg(agg_funcs)\n\n                  Std   Sum        Mean  Size\nCategory                                     \nBooks        2.081666    58   19.333333     3\nClothes      4.041452   148   49.333333     3\nTechnology  70.710678  1800  300.000000     6\n\ndf.set_index(['Category', 'Item']).stack().groupby(level=0).describe().unstack()\n\n            count        mean        std    min    25%    50%    75%    max\nCategory                                                                   \nBooks         3.0   19.333333   2.081666   17.0   18.5   20.0   20.5   21.0\nClothes       3.0   49.333333   4.041452   45.0   47.5   50.0   51.5   53.0\nTechnology    6.0  300.000000  70.710678  200.0  262.5  300.0  337.5  400.0\n"
"df['C'] = df.apply(lambda x: x.A in x.B, axis=1)\nprint (df)\n   RecID  A    B      C\n0      1  a  abc   True\n1      2  b  cba   True\n2      3  c  bca   True\n3      4  d  bac  False\n4      5  e  abc  False\n\ndf['C'] = [x[0] in x[1] for x in zip(df['A'], df['B'])]\nprint (df)\n   RecID  A    B      C\n0      1  a  abc   True\n1      2  b  cba   True\n2      3  c  bca   True\n3      4  d  bac  False\n4      5  e  abc  False\n"
"pd.concat([df1,df2]).drop_duplicates(['Code','Name'],keep='last').sort_values('Code')\nOut[1280]: \n   Code      Name  Value\n0     1  Company1    200\n0     2  Company2   1000\n2     3  Company3    400\n"
'twice = df[0]*2\nmask = df[0] &gt; 0.5\n%timeit np.where(mask, twice, df[0])  \n# 61.4 ms ± 1.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n%timeit df[0].mask(mask, twice)\n# 143 ms ± 5.27 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n&gt;&gt;&gt; perf record python np_where.py\n&gt;&gt;&gt; perf report\n\nOverhead  Command  Shared Object                                Symbol                              \n  68,50%  python   multiarray.cpython-36m-x86_64-linux-gnu.so   [.] PyArray_Where\n   8,96%  python   [unknown]                                    [k] 0xffffffff8140290c\n   1,57%  python   mtrand.cpython-36m-x86_64-linux-gnu.so       [.] rk_random\n\n&gt;&gt;&gt; perf record python pd_mask.py\n&gt;&gt;&gt; perf report\n\nOverhead  Command  Shared Object                                Symbol                                                                                               \n  37,12%  python   interpreter.cpython-36m-x86_64-linux-gnu.so  [.] vm_engine_iter_task\n  23,36%  python   libc-2.23.so                                 [.] __memmove_ssse3_back\n  19,78%  python   [unknown]                                    [k] 0xffffffff8140290c\n   3,32%  python   umath.cpython-36m-x86_64-linux-gnu.so        [.] DOUBLE_isnan\n   1,48%  python   umath.cpython-36m-x86_64-linux-gnu.so        [.] BOOL_logical_not\n\nOverhead  Command        Shared Object                     Symbol                                                                                                     \n  32,42%  python         multiarray.so                     [.] PyArray_Where\n  30,25%  python         libc-2.23.so                      [.] __memmove_ssse3_back\n  21,31%  python         [kernel.kallsyms]                 [k] clear_page\n   1,72%  python         [kernel.kallsyms]                 [k] __schedule\n\n%timeit df[0].mask(mask.values &gt; 0.5, twice.values)\n# 75.7 ms ± 1.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nOverhead  Command  Shared Object                                Symbol                                                                                                \n  50,81%  python   interpreter.cpython-36m-x86_64-linux-gnu.so  [.] vm_engine_iter_task\n  14,12%  python   [unknown]                                    [k] 0xffffffff8140290c\n   9,93%  python   libc-2.23.so                                 [.] __memmove_ssse3_back\n   4,61%  python   umath.cpython-36m-x86_64-linux-gnu.so        [.] DOUBLE_isnan\n   2,01%  python   umath.cpython-36m-x86_64-linux-gnu.so        [.] BOOL_logical_not\n\nnp.where(df[0] &gt; 0.5, df[0]*2, df[0])\n\nimport numba as nb\n@nb.njit\ndef nb_where(df):\n    n = len(df)\n    output = np.empty(n, dtype=np.float64)\n    for i in range(n):\n        if df[i]&gt;0.5:\n            output[i] = 2.0*df[i]\n        else:\n            output[i] = df[i]\n    return output\n\nassert(np.where(df[0] &gt; 0.5, twice, df[0])==nb_where(df[0].values)).all()\n%timeit np.where(df[0] &gt; 0.5, df[0]*2, df[0])\n# 85.1 ms ± 1.61 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n%timeit nb_where(df[0].values)\n# 17.4 ms ± 673 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n%%cython -a\ncimport numpy as np\nimport numpy as np\ncimport cython\n\n@cython.boundscheck(False)\n@cython.wraparound(False)\ndef cy_where(double[::1] df):\n    cdef int i\n    cdef int n = len(df)\n    cdef np.ndarray[np.float64_t] output = np.empty(n, dtype=np.float64)\n    for i in range(n):\n        if df[i]&gt;0.5:\n            output[i] = 2.0*df[i]\n        else:\n            output[i] = df[i]\n    return output\n\nassert (df[0].mask(df[0] &gt; 0.5, 2*df[0]).values == cy_where(df[0].values)).all()\n\n%timeit cy_where(df[0].values)\n# 66.7± 753 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\n\nn = 10000000\ndf = pd.DataFrame(np.random.random(n))\n\ntwice = df[0]*2\nfor _ in range(50):\n      np.where(df[0] &gt; 0.5, twice, df[0])  \n\nimport pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\n\nn = 10000000\ndf = pd.DataFrame(np.random.random(n))\n\ntwice = df[0]*2\nmask = df[0] &gt; 0.5\nfor _ in range(50):\n      df[0].mask(mask, twice)\n'
"In [17]: ax = x.plot(kind='bar', legend=False)\n\nIn [18]: patches, labels = ax.get_legend_handles_labels()\n\nIn [19]: ax.legend(patches, labels, loc='best')\nOut[19]: &lt;matplotlib.legend.Legend at 0x10b292ad0&gt;\n"
"style = '&lt;style&gt;.dataframe td { text-align: right; }&lt;/style&gt;'\nHTML( style + df.to_html( formatters=frmt ) )\n\nstyle = '&lt;style&gt;.right_aligned_df td { text-align: right; }&lt;/style&gt;'\nHTML(style + df.to_html(formatters=frmt, classes='right_aligned_df'))\n\n# Some cell at the begining of the notebook\nIn [2]: HTML('''&lt;style&gt;\n                    .right_aligned_df td { text-align: right; }\n                    .left_aligned_df td { text-align: right; }\n                    .pink_df { background-color: pink; }\n                &lt;/style&gt;''')\n\n...\n\n# Much later in your notebook\nIn [66]: HTML(df.to_html(classes='pink_df'))\n"
'df.iloc[3,4]\n'
'In [11]: df\nOut[11]: \n   a  b  c  d\n0  C  C  C  C\n1  C  C  A  A\n2  A  A  A  A\n\nIn [12]: df.iloc[:, 0]\nOut[12]: \n0    C\n1    C\n2    A\nName: a, dtype: object\n\nIn [13]: df.eq(df.iloc[:, 0], axis=0)\nOut[13]: \n      a     b      c      d\n0  True  True   True   True\n1  True  True  False  False\n2  True  True   True   True\n\nIn [14]: df.eq(df.iloc[:, 0], axis=0).all(1)\nOut[14]: \n0     True\n1    False\n2     True\ndtype: bool\n'
"In [41]: pd.merge(df1, df2, on=['id', 'name']).set_index(['id', 'name']).sum(axis=1)\nOut[41]: \nid  name\n2   B       25\n3   C       20\ndtype: int64\n"
"In [27]:\n\ndf.loc[df.Name.isin(df1.Name), ['Nonprofit', 'Education']] = df1[['Nonprofit', 'Education']]\ndf\nOut[27]:\n  Name  Nonprofit  Business  Education\n0    X          1         1          0\n1    Y          1         1          1\n2    Z          1         0          1\n3    Y          1         1          1\n\n[4 rows x 4 columns]\n"
"g = x.groupby('Color')\n\ng.groups.keys()\n"
"In [74]: df['stats'].str[1:-1].str.split(',', expand=True).astype(float)\nOut[74]:\n          0         1         2         3         4\n0 -0.009242  0.410000 -0.742016  0.003683  0.002517\n1  0.041154  0.318231  0.758717  0.002640  0.010654\n2 -0.014435  0.168438 -0.808703  0.000817  0.003166\n3  0.034346  0.288731  0.950845  0.000001  0.003373\n4  0.009052  0.151031  0.670257  0.012179  0.003022\n5 -0.004797  0.171615 -0.552879  0.050032  0.002180\n\npd.DataFrame(df['stats'].tolist(), index=df.index)\n"
"In [11]: left_a = left.set_index('a')\n\nIn [12]: right_a = right.set_index('a')\n\nIn [13]: res = left_a.reindex(columns=left_a.columns.union(right_a.columns))\n\nIn [14]: res.update(right_a)\n\nIn [15]: res.reset_index(inplace=True)\n\nIn [16]: res\nOut[16]:\n   a   b   c   d\n0  1   4   7  13\n1  2   5   8  14\n2  3   6   9  15\n3  4   7  12 NaN\n"
"In [27]: df\nOut[27]:\n     Col X  Col Y\n0  class 1  cat 1\n1  class 2  cat 1\n2  class 3  cat 2\n3  class 2  cat 3\n\nIn [28]: pd.crosstab(df['Col X'], df['Col Y'])\nOut[28]:\nCol Y    cat 1  cat 2  cat 3\nCol X\nclass 1      1      0      0\nclass 2      1      0      1\nclass 3      0      1      0\n\nIn [29]: df.groupby(['Col X','Col Y']).size().unstack('Col Y', fill_value=0)\nOut[29]:\nCol Y    cat 1  cat 2  cat 3\nCol X\nclass 1      1      0      0\nclass 2      1      0      1\nclass 3      0      1      0\n\nIn [30]: pd.pivot_table(df, index=['Col X'], columns=['Col Y'], aggfunc=len, fill_value=0)\nOut[30]:\nCol Y    cat 1  cat 2  cat 3\nCol X\nclass 1      1      0      0\nclass 2      1      0      1\nclass 3      0      1      0\n\nIn [492]: df.assign(v=1).set_index(['Col X', 'Col Y'])['v'].unstack(fill_value=0)\nOut[492]:\nCol Y    cat 1  cat 2  cat 3\nCol X\nclass 1      1      0      0\nclass 2      1      0      1\nclass 3      0      1      0\n"
"df.to_csv('path', header=True, index=False, encoding='utf-8')\n"
'from scipy.sparse import csr_matrix\nfrom pandas.api.types import CategoricalDtype\n\nperson_c = CategoricalDtype(sorted(frame.person.unique()), ordered=True)\nthing_c = CategoricalDtype(sorted(frame.thing.unique()), ordered=True)\n\nrow = frame.person.astype(person_c).cat.codes\ncol = frame.thing.astype(thing_c).cat.codes\nsparse_matrix = csr_matrix((frame["count"], (row, col)), \\\n                           shape=(person_c.categories.size, thing_c.categories.size))\n\n&gt;&gt;&gt; sparse_matrix\n&lt;3x4 sparse matrix of type \'&lt;class \'numpy.int64\'&gt;\'\n     with 6 stored elements in Compressed Sparse Row format&gt;\n\n&gt;&gt;&gt; sparse_matrix.todense()\nmatrix([[0, 1, 0, 1],\n        [1, 0, 0, 1],\n        [1, 0, 1, 0]], dtype=int64)\n\n\ndfs = pd.SparseDataFrame(sparse_matrix, \\\n                         index=person_c.categories, \\\n                         columns=thing_c.categories, \\\n                         default_fill_value=0)\n&gt;&gt;&gt; dfs\n        a   b   c   d\n him    0   1   0   1\n  me    1   0   0   1\n you    1   0   1   0\n'
'import warnings\nwarnings.filterwarnings("ignore")\n\nimport pandas\n'
"In [38]: df.query('a == @id_list')\nOut[38]:\n   a  b  c  d\n0  a  a  3  4\n1  a  a  4  5\n2  b  a  2  3\n3  b  a  1  5\n4  c  b  2  4\n5  c  b  1  2\n"
"&gt;&gt;&gt; output = io.StringIO()\n&gt;&gt;&gt; output.write('x,y\\n')\n4\n&gt;&gt;&gt; output.write('1,2\\n')\n4\n&gt;&gt;&gt; output.seek(0)\n0\n&gt;&gt;&gt; pd.read_csv(output)\n   x  y\n0  1  2\n"
'import pandas as pd\n\ndf = pd.read_excel(\'test/ipsos_excel_tables_type_2_trimed_nosig.xlsx\', \n                   header=[0,1], \n                   index_col=[0,1], \n                   sheetname="0001")\nprint df\n\n                                                                       T  \\\n                                                                   Total   \nQ1. Do you have a social media account (such as... Unweighted base  2019   \n                                                   Weighted base    2019   \n                                                   Yes              1519   \n                                                   Yes                75   \n                                                   No                494   \n                                                   No                 24   \n                                                   Don’t know          5   \n\n                                                                   Gender  \\\n                                                                     Male   \nQ1. Do you have a social media account (such as... Unweighted base   1011   \n                                                   Weighted base     1000   \n                                                   Yes                705   \n                                                   Yes                 70   \n                                                   No                 291   \n                                                   No                  29   \n                                                   Don’t know           4   \n\n                                                                           \\\n                                                                   Female   \nQ1. Do you have a social media account (such as... Unweighted base   1008   \n                                                   Weighted base     1019   \n                                                   Yes                814   \n                                                   Yes                 80   \n                                                   No                 204   \n                                                   No                  20   \n                                                   Don’t know           1   \n\n                                                                     Age  \\\n                                                                   16-24   \nQ1. Do you have a social media account (such as... Unweighted base   321   \n                                                   Weighted base     323   \n                                                   Yes               293   \n                                                   Yes                91   \n                                                   No                 28   \n                                                   No                  9   \n                                                   Don’t know          1   \n\n                                                                          \\\n                                                                   25-34   \nQ1. Do you have a social media account (such as... Unweighted base   361   \n                                                   Weighted base     361   \n                                                   Yes               318   \n                                                   Yes                88   \n                                                   No                 41   \n                                                   No                 11   \n                                                   Don’t know          2   \n\n                                                                          \\\n                                                                   35-44   \nQ1. Do you have a social media account (such as... Unweighted base   372   \n                                                   Weighted base     370   \n                                                   Yes               289   \n                                                   Yes                78   \n                                                   No                 81   \n                                                   No                 22   \n                                                   Don’t know          -   \n\n                                                                          \\\n                                                                   45-54   \nQ1. Do you have a social media account (such as... Unweighted base   376   \n                                                   Weighted base     376   \n                                                   Yes               258   \n                                                   Yes                69   \n                                                   No                118   \n                                                   No                 31   \n                                                   Don’t know          -   \n\n                                                                          \\\n                                                                   55-75   \nQ1. Do you have a social media account (such as... Unweighted base   589   \n                                                   Weighted base     589   \n                                                   Yes               361   \n                                                   Yes                61   \n                                                   No                227   \n                                                   No                 38   \n                                                   Don’t know          2   \n\n                                                                   Social grade  \\\n                                                                             AB   \nQ1. Do you have a social media account (such as... Unweighted base          593   \n                                                   Weighted base            533   \n                                                   Yes                      416   \n                                                   Yes                       78   \n                                                   No                       116   \n                                                   No                        22   \n                                                   Don’t know                 1   \n\n                                                                         \\\n                                                                     C1   \nQ1. Do you have a social media account (such as... Unweighted base  588   \n                                                   Weighted base    563   \n                                                   Yes              417   \n                                                   Yes               74   \n                                                   No               147   \n                                                   No                26   \n                                                   Don’t know         -   \n\n                                                                         ...        \\\n                                                                         ...         \nQ1. Do you have a social media account (such as... Unweighted base       ...         \n                                                   Weighted base         ...         \n                                                   Yes                   ...         \n                                                   Yes                   ...         \n                                                   No                    ...         \n                                                   No                    ...         \n                                                   Don’t know            ...         \n\n                                                                   Region (4 code scale)  \\\n                                                                                    East   \nQ1. Do you have a social media account (such as... Unweighted base                   341   \n                                                   Weighted base                     342   \n                                                   Yes                               259   \n                                                   Yes                                76   \n                                                   No                                 83   \n                                                   No                                 24   \n                                                   Don’t know                          -   \n\n                                                                            Education  \\\n                                                                   GCSE/O Level/NVQ12   \nQ1. Do you have a social media account (such as... Unweighted base                503   \n                                                   Weighted base                  520   \n                                                   Yes                            370   \n                                                   Yes                             71   \n                                                   No                             147   \n                                                   No                              28   \n                                                   Don’t know                       4   \n\n                                                                                          \\\n                                                                   A Level or equivalent   \nQ1. Do you have a social media account (such as... Unweighted base                   454   \n                                                   Weighted base                     461   \n                                                   Yes                               359   \n                                                   Yes                                78   \n                                                   No                                101   \n                                                   No                                 22   \n                                                   Don’t know                          1   \n\n                                                                                       \\\n                                                                   Degree/Masters/PhD   \nQ1. Do you have a social media account (such as... Unweighted base                914   \n                                                   Weighted base                  886   \n                                                   Yes                            697   \n                                                   Yes                             79   \n                                                   No                             189   \n                                                   No                              21   \n                                                   Don’t know                       -   \n\n                                                                                             \\\n                                                                   No formal qualifications   \nQ1. Do you have a social media account (such as... Unweighted base                      148   \n                                                   Weighted base                        152   \n                                                   Yes                                   93   \n                                                   Yes                                   61   \n                                                   No                                    58   \n                                                   No                                    38   \n                                                   Don’t know                             1   \n\n                                                                   Employment status  \\\n                                                                           Full-time   \nQ1. Do you have a social media account (such as... Unweighted base               774   \n                                                   Weighted base                 763   \n                                                   Yes                           598   \n                                                   Yes                            78   \n                                                   No                            162   \n                                                   No                             21   \n                                                   Don’t know                      3   \n\n                                                                              \\\n                                                                   Part-time   \nQ1. Do you have a social media account (such as... Unweighted base       272   \n                                                   Weighted base         274   \n                                                   Yes                   195   \n                                                   Yes                    71   \n                                                   No                     79   \n                                                   No                     29   \n                                                   Don’t know              -   \n\n                                                                                  \\\n                                                                   Self-Employed   \nQ1. Do you have a social media account (such as... Unweighted base           166   \n                                                   Weighted base             162   \n                                                   Yes                       108   \n                                                   Yes                        67   \n                                                   No                         54   \n                                                   No                         33   \n                                                   Don’t know                  -   \n\n                                                                                \\\n                                                                   ANY WORKING   \nQ1. Do you have a social media account (such as... Unweighted base        1212   \n                                                   Weighted base          1200   \n                                                   Yes                     901   \n                                                   Yes                      75   \n                                                   No                      295   \n                                                   No                       25   \n                                                   Don’t know                3   \n\n\n                                                                   ANY NOT WORKING  \nQ1. Do you have a social media account (such as... Unweighted base             625  \n                                                   Weighted base               645  \n                                                   Yes                         460  \n                                                   Yes                          71  \n                                                   No                          183  \n                                                   No                           28  \n                                                   Don’t know                    2  \n\n[7 rows x 25 columns]\n'
"pd.core.format.header_style = None\n\nimport pandas as pd\n\ndata = pd.DataFrame({'test_data': [1,2,3,4,5]})\nwriter = pd.ExcelWriter('test.xlsx', engine='xlsxwriter')\n\npd.core.format.header_style = None\n\ndata.to_excel(writer, sheet_name='test', index=False)\n\nworkbook  = writer.book\nworksheet = writer.sheets['test']\n\nfont_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10})\nheader_fmt = workbook.add_format({'font_name': 'Arial', 'font_size': 10, 'bold': True})\n\nworksheet.set_column('A:A', None, font_fmt)\nworksheet.set_row(0, None, header_fmt)\n\nwriter.save()\n\npd.core.format.header_style = None\n\npd.formats.format.header_style = None\n\nimport pandas.io.formats.excel\npandas.io.formats.excel.header_style = None\n\nimport pandas.io.formats.excel\npandas.io.formats.excel.ExcelFormatter.header_style = None\n"
"df['2008-01-01':'2008-01-01']\n\ndf.loc['2008-01-01']\n"
'df = pd.read_csv(\'sample.tar.gz\', compression=\'gzip\', header=0, sep=\' \', quotechar=\'"\', error_bad_lines=False)\n'
'df.where(df.notnull(), None)\n'
"df_grouped.reset_index(name='count')\n\nprint (df_grouped.rename('count').reset_index())\n\n   A  Amt  count\n0  1   30      4\n1  1   20      3\n2  1   40      2\n3  2   40      3\n4  2   10      2\n\ndf_grouped1 =  dftest.groupby(['A','Amt']).size().reset_index(name='count')\n\nprint (df_grouped1)\n   A  Amt  count\n0  1   20      3\n1  1   30      4\n2  1   40      2\n3  2   10      2\n4  2   40      3\n"
"df.loc[df['favcount'].idxmax(), 'sn']\n"
'df["int"] = pd.Series([], dtype=object)\ndf["str"] = pd.Series([], dtype=str)\ndf.loc[0] = [0, "zero"]\nprint(df)\nprint()\ndf.loc[1] = [1, None]\nprint(df)\n\n   int   str\n0    0  zero\n1  NaN   NaN\n\n  int   str\n0   0  zero\n1   1  None\n'
"import pandas as pd\n\ndf = pd.DataFrame(data={'books':['bk1','bk1','bk1','bk2','bk2','bk3'], 'price': [12,12,12,15,15,17]})\n\nprint(df)\n\nprint(df.groupby('books', as_index=True).sum())\n\nprint(df.groupby('books', as_index=False).sum())\n\n  books  price\n0   bk1     12\n1   bk1     12\n2   bk1     12\n3   bk2     15\n4   bk2     15\n5   bk3     17\n\n       price\nbooks       \nbk1       36\nbk2       30\nbk3       17\n\n  books  price\n0   bk1     36\n1   bk2     30\n2   bk3     17\n"
"df['foodstuff'].fillna(df['type'])\n\n0      apple-martini\n1          apple-pie\n2    strawberry-tart\n3            dessert\n4               None\n"
"df3 = result[result['Value'] ! &lt;= 10]  \n\ndf3 = result[~(result['Value'] &lt;= 10)]  \n\ndf3 = result[result['Value'] &gt; 10]  \n"
"s = pd.Series([np.nan,2,np.nan])\nprint (s)\n0    NaN\n1    2.0\n2    NaN\ndtype: float64\n\nprint (s.first_valid_index())\n1\n\nprint (s.loc[s.first_valid_index()])\n2.0\n\n# If your Series contains ALL NaNs, you'll need to check as follows:\n\ns = pd.Series([np.nan, np.nan, np.nan])\nidx = s.first_valid_index()  # Will return None\nfirst_valid_value = s.loc[idx] if idx is not None else None\nprint(first_valid_value)\nNone\n"
"colname = df.columns[pos]\n\ndf = pd.DataFrame({'A':[1,2,3],\n                   'B':[4,5,6],\n                   'C':[7,8,9],\n                   'D':[1,3,5],\n                   'E':[5,3,6],\n                   'F':[7,4,3]})\n\nprint (df)\n   A  B  C  D  E  F\n0  1  4  7  1  5  7\n1  2  5  8  3  3  4\n2  3  6  9  5  6  3\n\npos = 3\ncolname = df.columns[pos]\nprint (colname)\nD\n\npos = [3,5]\ncolname = df.columns[pos]\nprint (colname)\nIndex(['D', 'F'], dtype='object')\n"
'df1[\'employee_id\'] = df1[\'employee_id\'].str.strip()\ndf2[\'employee_id\'] = df2[\'employee_id\'].str.strip()\n\ndf1 = pd.read_csv(\'input1.csv\', sep=\',\\s+\', delimiter=\',\', encoding="utf-8", skipinitialspace=True)\ndf2 = pd.read_csv(\'input2.csv\', sep=\',\\s,\', delimiter=\',\', encoding="utf-8", skipinitialspace=True)\n\ndf1[\'employee_id\'] = df1[\'employee_id\'].str.replace(" ","")\ndf2[\'employee_id\'] = df2[\'employee_id\'].str.replace(" ","")\n'
"n = 200000  #chunk row size\nlist_df = [df[i:i+n] for i in range(0,df.shape[0],n)]\n\nlist_df[0]\nlist_df[1]\netc...\n\nlist_df = []\n\nfor n,g in df.groupby('AcctName'):\n    list_df.append(g)\n"
'cols = ["Weight","Height","BootSize","SuitSize","Type"]\ndf2[cols] = df2[cols].replace({\'0\':np.nan, 0:np.nan})\n'
"df.loc[df['channel'].isin(['sale','fullprice'])]\n"
"&gt;&gt;&gt; from pandas.core.tools.datetimes import _guess_datetime_format_for_array\n&gt;&gt;&gt; array = np.array(['2016-05-01T00:00:59.3+10:00'])\n&gt;&gt;&gt; _guess_datetime_format_for_array(array)\n# returns None\n\n&gt;&gt;&gt; array = np.array(['2016-05-01T00:00:59.300000']) # six digits, no tz\n&gt;&gt;&gt; _guess_datetime_format_for_array(array)\n'%Y-%m-%dT%H:%M:%S.%f'\n\nfrom pandas._libs.tslib import _test_parse_iso8601\n\ndef is_iso8601(string):\n    try:\n        _test_parse_iso8601(string)\n        return True\n    except ValueError:\n        return False\n\n&gt;&gt;&gt; is_iso8601('2016-05-01T00:00:59.3+10:00')\nTrue\n"
"import pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\nchunksize=10000 # this is the number of lines\n\npqwriter = None\nfor i, df in enumerate(pd.read_csv('sample.csv', chunksize=chunksize)):\n    table = pa.Table.from_pandas(df)\n    # for the first chunk of records\n    if i == 0:\n        # create a parquet write object giving it an output file\n        pqwriter = pq.ParquetWriter('sample.parquet', table.schema)            \n    pqwriter.write_table(table)\n\n# close the parquet writer\nif pqwriter:\n    pqwriter.close()\n"
'df1 = pd.read_csv("yourdata.csv")\ndf2 = pd.read_csv("yourdata2.csv")\ndf2_key = df2.Colname2\n\n# creating a empty bucket to save result\ndf_result = pd.DataFrame(columns=(df1.columns.append(df2.columns)).unique())\ndf_result.to_csv("df3.csv",index_label=False)\n\n# save data which only appear in df1 # sorry I was doing left join here. no need to run below two line.\n# df_result = df1[df1.Colname1.isin(df2.Colname2)!=True]\n# df_result.to_csv("df3.csv",index_label=False, mode="a")\n\n# deleting df2 to save memory\ndel(df2)\n\ndef preprocess(x):\n    df2=pd.merge(df1,x, left_on = "Colname1", right_on = "Colname2")\n    df2.to_csv("df3.csv",mode="a",header=False,index=False)\n\nreader = pd.read_csv("yourdata2.csv", chunksize=1000) # chunksize depends with you colsize\n\n[preprocess(r) for r in reader]\n'
"import pandas as pd\n\ngroup_cols = ['Group1', 'Group2', 'Group3']\n\ndf = pd.DataFrame([['A', 'B', 'C', 54.34],\n                   ['A', 'B', 'D', 61.34],\n                   ['B', 'A', 'C', 514.5],\n                   ['B', 'A', 'A', 765.4],\n                   ['A', 'B', 'D', 765.4]],\n                  columns=(group_cols+['Value']))\n\nfor col in group_cols:\n    df[col] = df[col].astype('category')\n\ndf.groupby(group_cols, as_index=False, observed=True).sum()\n"
" T(n,m)=T_preprocess(n)+m*T_lookup(n)\n\n%%cython\nimport numpy as np\ncimport numpy as np\n\nfrom cykhash.khashsets cimport Int64Set\n\ndef isin_khash(np.ndarray[np.int64_t, ndim=1] a, Int64Set b):\n    cdef np.ndarray[np.uint8_t,ndim=1, cast=True] res=np.empty(a.shape[0],dtype=np.bool)\n    cdef int i\n    for i in range(a.size):\n        res[i]=b.contains(a[i])\n    return res\n\n#isin.py\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(0)\n\nx_set = {i for i in range(2*10**6)}\nx_arr = np.array(list(x_set))\n\n\narr = np.random.randint(0, 20000, 10000)\nser = pd.Series(arr)\n\n\nfor _ in range(10):\n   ser.isin(x_arr)\n\n&gt;&gt;&gt; valgrind --tool=callgrind python isin.py\n&gt;&gt;&gt; kcachegrind\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\nx_set = {i for i in range(10**2)}\nx_arr = np.array(list(x_set))\nx_list = list(x_set)\n\narr = np.random.randint(0, 20000, 10000)\nser = pd.Series(arr)\nlst = arr.tolist()\n\nn=10**3\nresult=[]\nwhile n&lt;3*10**6:\n    x_set = {i for i in range(n)}\n    x_arr = np.array(list(x_set))\n    x_list = list(x_set)\n\n    t1=%timeit -o  ser.isin(x_arr) \n    t2=%timeit -o  [i in x_set for i in lst]\n    t3=%timeit -o  [i in x_set for i in ser.values]\n\n    result.append([n, t1.average, t2.average, t3.average])\n    n*=2\n\n#plotting result:\nfor_plot=np.array(result)\nplt.plot(for_plot[:,0], for_plot[:,1], label='numpy')\nplt.plot(for_plot[:,0], for_plot[:,2], label='python')\nplt.plot(for_plot[:,0], for_plot[:,3], label='numpy-&gt;python')\nplt.xlabel('n')\nplt.ylabel('running time')\nplt.legend()\nplt.show()\n\n%%cython --cplus -c=-std=c++11 -a\n\nfrom libcpp.unordered_set cimport unordered_set\n\ncdef class HashSet:\n    cdef unordered_set[long long int] s\n    cpdef add(self, long long int z):\n        self.s.insert(z)\n    cpdef bint contains(self, long long int z):\n        return self.s.count(z)&gt;0\n\nimport numpy as np\ncimport numpy as np\n\ncimport cython\n@cython.boundscheck(False)\n@cython.wraparound(False)\n\ndef isin_cpp(np.ndarray[np.int64_t, ndim=1] a, HashSet b):\n    cdef np.ndarray[np.uint8_t,ndim=1, cast=True] res=np.empty(a.shape[0],dtype=np.bool)\n    cdef int i\n    for i in range(a.size):\n        res[i]=b.contains(a[i])\n    return res\n\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom cykhash import Int64Set\n\nnp.random.seed(0)\n\nx_set = {i for i in range(10**2)}\nx_arr = np.array(list(x_set))\nx_list = list(x_set)\n\n\narr = np.random.randint(0, 20000, 10000)\nser = pd.Series(arr)\nlst = arr.tolist()\n\nn=10**3\nresult=[]\nwhile n&lt;3*10**6:\n    x_set = {i for i in range(n)}\n    x_arr = np.array(list(x_set))\n    cpp_set=HashSet()\n    khash_set=Int64Set()\n\n    for i in x_set:\n        cpp_set.add(i)\n        khash_set.add(i)\n\n\n    assert((ser.isin(x_arr).values==isin_cpp(ser.values, cpp_set)).all())\n    assert((ser.isin(x_arr).values==isin_khash(ser.values, khash_set)).all())\n\n\n    t1=%timeit -o  isin_khash(ser.values, khash_set)\n    t2=%timeit -o  isin_cpp(ser.values, cpp_set) \n    t3=%timeit -o  [i in x_set for i in lst]\n    t4=%timeit -o  [i in x_set for i in ser.values]\n\n    result.append([n, t1.average, t2.average, t3.average, t4.average])\n    n*=2\n\n#ploting result:\nfor_plot=np.array(result)\nplt.plot(for_plot[:,0], for_plot[:,1], label='khash')\nplt.plot(for_plot[:,0], for_plot[:,2], label='cpp')\nplt.plot(for_plot[:,0], for_plot[:,3], label='pure python')\nplt.plot(for_plot[:,0], for_plot[:,4], label='numpy-&gt;python')\nplt.xlabel('n')\nplt.ylabel('running time')\nymin, ymax = plt.ylim()\nplt.ylim(0,ymax)\nplt.legend()\nplt.show()\n"
"firsts = df1.index.get_level_values('first')\ndf1['value2'] = df2.loc[firsts].values\n\ndf1 = pd.DataFrame([['a', 'x', 0.123], ['a','x', 0.234],\n                    ['a', 'y', 0.451], ['b', 'x', 0.453]],\n                   columns=['first', 'second', 'value1']\n                   ).set_index(['first', 'second'])\ndf2 = pd.DataFrame([['a', 10],['b', 20]],\n                   columns=['first', 'value']).set_index(['first'])\n\nfirsts = df1.index.get_level_values('first')\ndf1['value2'] = df2.loc[firsts].values\n\nIn [5]: df1\nOut[5]: \n              value1  value2\nfirst second                \na     x        0.123      10\n      x        0.234      10\n      y        0.451      10\nb     x        0.453      20\n"
'df[\'Season2\'] = df[\'Season\'].apply(split_it)\n\ndf[\'Season2\'] = df[\'Season\'].apply(lambda x: split_it(x))\n\n&gt;&gt;&gt; df["Season"].apply(split_it)\n74     [1982]\n84     [1982]\n176    [1982]\n177    [1983]\n243    [1982]\nName: Season, dtype: object\n\n&gt;&gt;&gt; df["Season"].str[:4].astype(int)\n74     1982\n84     1982\n176    1982\n177    1983\n243    1982\nName: Season, dtype: int64\n\n&gt;&gt;&gt; df["Season"].str.split("-").str[0].astype(int)\n74     1982\n84     1982\n176    1982\n177    1983\n243    1982\nName: Season, dtype: int64\n'
"In [66]:\n\ndf = pd.DataFrame({'x':[0,-3,5,-1,1]})\ndf\nOut[66]:\n   x\n0  0\n1 -3\n2  5\n3 -1\n4  1\n\nIn [69]:\n\ndf['y'] = 0\ndf.loc[df['x'] &lt; -2, 'y'] = 1\ndf.loc[df['x'] &gt; 2, 'y'] = -1\ndf\nOut[69]:\n   x  y\n0  0  0\n1 -3  1\n2  5 -1\n3 -1  0\n4  1  0\n\nIn [77]:\n\ndf['y'] = np.where(df['x'] &lt; -2 , 1, np.where(df['x'] &gt; 2, -1, 0))\ndf\nOut[77]:\n   x  y\n0  0  0\n1 -3  1\n2  5 -1\n3 -1  0\n4  1  0\n\nIn [79]:\n\n%timeit df['y'] = np.where(df['x'] &lt; -2 , 1, np.where(df['x'] &gt; 2, -1, 0))\n\n1000 loops, best of 3: 1.79 ms per loop\n\nIn [81]:\n\n%%timeit\ndf['y'] = 0\ndf.loc[df['x'] &lt; -2, 'y'] = 1\ndf.loc[df['x'] &gt; 2, 'y'] = -1\n\n100 loops, best of 3: 3.27 ms per loop\n"
'In [160]:\ndef func(x):\n    if x.values[0] is None:\n        return None\n    else:\n        return df.loc[x.name, x.values[0]]\npd.DataFrame(df.apply(lambda x: x.first_valid_index(), axis=1)).apply(func,axis=1)\n\u200b\nOut[160]:\n0     1\n1     3\n2     4\n3   NaN\ndtype: float64\n\nIn [12]:\ndef func(x):\n    if x.first_valid_index() is None:\n        return None\n    else:\n        return x[x.first_valid_index()]\ndf.apply(func, axis=1)\n\nOut[12]:\n0     1\n1     3\n2     4\n3   NaN\ndtype: float64\n'
"import pandas\nimport datetime\ndef dateparse (time_in_secs):    \n    return datetime.datetime.fromtimestamp(float(time_in_secs))\n\nx = pandas.read_csv('data.csv',delimiter=';', parse_dates=True,date_parser=dateparse, index_col='DateTime', names=['DateTime', 'X'], header=None)\n\nout = x.truncate(before=datetime.datetime(2015,12,2,12,2,18))\n"
"df['two'].between(-0.5, 0.5, inclusive=False)\n\n(df['two'] &gt;= -0.5) &amp; (df['two'] &lt; 0.5)\n"
"import networkx as nx\n\nG = nx.from_edgelist(pair_array, create_using=nx.MultiGraph)\nnx.to_pandas_adjacency(G, nodelist=sorted(G.nodes()), dtype='int')\n\n      18   31   69   183  205  254  267  382\n18     0    0    1    0    0    0    0    0\n31     0    0    0    1    0    0    1    1\n69     1    0    0    0    0    0    0    0\n183    0    1    0    0    0    0    1    1\n205    0    0    0    0    0    1    0    2\n254    0    0    0    0    1    0    0    1\n267    0    1    0    1    0    0    0    0\n382    0    1    0    1    2    1    0    0\n"
"In : df = pandas.DataFrame(numpy.random.randn(5,3), index=['a','c','d','e','g'])\n\nIn : df\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\ng -1.632493  0.938456  0.492695\n\nIn : df2 = df.reindex(['a','b','c','d','e','f','g'])\n\nIn : df2\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb       NaN       NaN       NaN\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf       NaN       NaN       NaN\ng -1.632493  0.938456  0.492695\n\nIn : df2.interpolate()\nOut:\n          0         1         2\na -1.987879 -2.028572  0.024493\nb  0.052363 -1.729055  0.114652\nc  2.092605 -1.429537  0.204811\nd  0.767215  1.077814  0.565666\ne -1.027733  1.330702 -0.490780\nf -1.330113  1.134579  0.000958\ng -1.632493  0.938456  0.492695\n"
"import io\nimport pandas as pd\ncontent = '''\\\ntimestamp  score\n2013-06-29 00:52:28+00:00        -0.420070\n2013-06-29 00:51:53+00:00        -0.445720\n2013-06-28 16:40:43+00:00         0.508161\n2013-06-28 15:10:30+00:00         0.921474\n2013-06-28 15:10:17+00:00         0.876710\n'''\n\ndf = pd.read_table(io.BytesIO(content), sep='\\s{2,}', parse_dates=[0], index_col=[0])\n\nprint(df)\n\n                        score\ntimestamp                    \n2013-06-29 00:52:28 -0.420070\n2013-06-29 00:51:53 -0.445720\n2013-06-28 16:40:43  0.508161\n2013-06-28 15:10:30  0.921474\n2013-06-28 15:10:17  0.876710\n\nprint(df.index)\n# &lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n\nprint(df.groupby(df.index.date).count())\n\n            score\n2013-06-28      3\n2013-06-29      2\n"
"grouped.agg({'numberA':'sum', 'numberB':'min'})\n\nimport numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n                         'foo', 'bar', 'foo', 'foo'],\n                   'B': ['one', 'one', 'two', 'three',\n                         'two', 'two', 'one', 'three'],\n                   'number A': np.arange(8),\n                   'number B': np.arange(8) * 2})\ngrouped = df.groupby('A')\n\nprint(grouped.agg({\n    'number A': 'sum',\n    'number B': 'min'}))\n\n     number B  number A\nA                      \nbar         2         9\nfoo         0        19\n\nprint(df.columns)\n"
"import pandas as pd\ndf = pd.DataFrame({'A':'foo foo foo bar bar bar'.split(),\n                   'B':[0.1, 0.5, 1.0]*2})\n\ndf['C'] = df.groupby(['A'])['B'].transform(\n                     lambda x: pd.qcut(x, 3, labels=range(1,4)))\nprint(df)\n\n     A    B  C\n0  foo  0.1  1\n1  foo  0.5  2\n2  foo  1.0  3\n3  bar  0.1  1\n4  bar  0.5  2\n5  bar  1.0  3\n"
"In [11]: pd.DataFrame(list(my_dict.iteritems()),\n                      columns=['business_id','business_code'])\nOut[11]: \n  business_id business_code\n0         id2          val2\n1         id3          val3\n2         id1          val1\n"
"cols_to_use = ['col1', 'col2'] # or [0,1,2,3]\ndf = pd.read_csv('mycsv.csv', usecols= cols_to_use)\n\ndf = df.drop(labels='column_to_delete', axis=1) # axis 1 drops columns, 0 will drop rows that match index value in labels\n\ndef df_from_csv(path):\n    df = read_csv(path, nrows=1) # read just first line for columns\n    columns = df.columns.tolist() # get the columns\n    cols_to_use = columns[:len(columns)-1] # drop the last one\n    df = read_csv(path, usecols=cols_to_use)\n    return df\n"
"In [11]: s = pd.Series(['aa', 'ab', 'ca', np.nan])\n\nIn [12]: s.str.startswith('a', na=False)\nOut[12]: \n0     True\n1     True\n2    False\n3    False\ndtype: bool\n\nIn [13]: s.str.contains('^a', na=False)\nOut[13]: \n0     True\n1     True\n2    False\n3    False\ndtype: bool\n\nIn [14]: s.str.startswith('a')  # can't use as boolean mask\nOut[14]:\n0     True\n1     True\n2    False\n3      NaN\ndtype: object\n"
'private["ISH"] = private.HolidayName.str.contains("(?i)holiday|recess")\n\nprivate["ISH"] = private.HolidayName.str.lower().str.contains("holiday|recess")\n'
"In [60]: df['beyer_shifted'] = df.groupby(level=0)['beyer'].shift(1); df\nOut[61]: \n                  line_date  line_race  beyer  beyer_shifted\nLast Gunfighter  2013-09-28         10     99            NaN\nLast Gunfighter  2013-08-18         10    102             99\nLast Gunfighter  2013-07-06          8    103            102\nPaynter          2013-09-28         10    103            NaN\nPaynter          2013-08-31         10     88            103\nPaynter          2013-07-27          8    100             88\n"
'import sqlalchemy\nimport pyodbc\nengine = sqlalchemy.create_engine("mssql+pyodbc://&lt;username&gt;:&lt;password&gt;@&lt;dsnname&gt;")\n\n# write the DataFrame to a table in the sql database\ndf.to_sql("table_name", engine)\n'
"In [14]:\n\nfrom numpy.random import randn\ndf = pd.DataFrame(randn(5, 3), index=['a', 'c', 'e', 'f', 'h'],\n               columns=['one', 'two', 'three'])\ndf = df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])\ndf\nOut[14]:\n        one       two     three\na -0.209453 -0.881878  3.146375\nb       NaN       NaN       NaN\nc  0.049383 -0.698410 -0.482013\nd       NaN       NaN       NaN\ne -0.140198 -1.285411  0.547451\nf -0.219877  0.022055 -2.116037\ng       NaN       NaN       NaN\nh -0.224695 -0.025628 -0.703680\nIn [18]:\n\ndf.shape[0] - df.dropna().shape[0]\nOut[18]:\n3\n\nIn [30]:\n\ndf.isnull().values.ravel().sum()\nOut[30]:\n9\n\nIn [34]:\n\n%timeit sum([True for idx,row in df.iterrows() if any(row.isnull())])\n%timeit df.shape[0] - df.dropna().shape[0]\n%timeit sum(map(any, df.apply(pd.isnull)))\n1000 loops, best of 3: 1.55 ms per loop\n1000 loops, best of 3: 1.11 ms per loop\n1000 loops, best of 3: 1.82 ms per loop\nIn [33]:\n\n%timeit sum(df.isnull().values.ravel())\n%timeit df.isnull().values.ravel().sum()\n%timeit df.isnull().sum().sum()\n1000 loops, best of 3: 215 µs per loop\n1000 loops, best of 3: 210 µs per loop\n1000 loops, best of 3: 605 µs per loop\n\nIn [39]:\n\n%timeit sum([True for idx,row in df.iterrows() if any(row.isnull())])\n%timeit df.shape[0] - df.dropna().shape[0]\n%timeit sum(map(any, df.apply(pd.isnull)))\n%timeit np.count_nonzero(df.isnull())\n1 loops, best of 3: 9.33 s per loop\n100 loops, best of 3: 6.61 ms per loop\n100 loops, best of 3: 3.84 ms per loop\n1000 loops, best of 3: 395 µs per loop\nIn [40]:\n\n%timeit sum(df.isnull().values.ravel())\n%timeit df.isnull().values.ravel().sum()\n%timeit df.isnull().sum().sum()\n%timeit np.count_nonzero(df.isnull().values.ravel())\n1000 loops, best of 3: 675 µs per loop\n1000 loops, best of 3: 679 µs per loop\n100 loops, best of 3: 6.56 ms per loop\n1000 loops, best of 3: 368 µs per loop\n"
"df_grp.apply(lambda x: pd.Series({'new_name':\n                    x['C'].sum() * x['D'].mean() / x['E'].max()}))\n# or df_grp.apply(lambda x: x['C'].sum() * x['D'].mean() / x['E'].max()).to_frame('new_name')\n\n     new_name\nA B          \nX N  5.583333\nY M  2.975000\n  N  3.845455\n"
"def lookup(date_pd_series, format=None):\n    &quot;&quot;&quot;\n    This is an extremely fast approach to datetime parsing.\n    For large data, the same dates are often repeated. Rather than\n    re-parse these, we store all unique dates, parse them, and\n    use a lookup to convert all dates.\n    &quot;&quot;&quot;\n    dates = {date:pd.to_datetime(date, format=format) for date in date_pd_series.unique()}\n    return date_pd_series.map(dates)\n\ndf['date-column'] = lookup(df['date-column'], format='%Y%m%d')\n\n$ python date-parse.py\nto_datetime: 5799 ms\ndateutil:    5162 ms\nstrptime:    1651 ms\nmanual:       242 ms\nlookup:        32 ms\n"
'n = 400\nfor g, df in test.groupby(np.arange(len(test)) // n):\n    print(df.shape)\n# (400, 2)\n# (400, 2)\n# (311, 2)\n'
"print maupayment\n  log_month  user_id install_month  payment\n1   2013-06        1       2013-06        0\n2   2013-06        2       2013-04        0\n3   2013-06        3       2013-04    14994\n\nprint np.where(maupayment['log_month'] == maupayment['install_month'], 'install', 'existing')\n['install' 'existing' 'existing']\n"
"if self.pandas_df is not None:\n    print 'Do stuff'\n"
"print df.pivot_table(index='Symbol', \n                     columns='Year', \n                     values='Action',\n                     fill_value=0, \n                     aggfunc='count').unstack()\n\nYear  Symbol\n2001  AAPL      2\n      BAC       0\n2002  AAPL      0\n      BAC       2\ndtype: int64\n\nprint df.pivot_table(index='Symbol', \n                     columns='Year', \n                     values='Action',\n                     fill_value=0, \n                     aggfunc='count').unstack()\n                                     .to_frame()\n                                     .rename(columns={0:'Action'})\n\n             Action\nYear Symbol        \n2001 AAPL         2\n     BAC          0\n2002 AAPL         0\n     BAC          2\n"
"In [294]: pd.get_dummies(df, prefix=['A', 'D'], columns=['A', 'D'])\nOut[294]: \n   B  C  A_x  A_y  D_j  D_l\n0  z  1  1.0  0.0  1.0  0.0\n1  u  2  0.0  1.0  0.0  1.0\n2  z  3  1.0  0.0  1.0  0.0\n"
"df.columns[df.isnull().any()]\n\ndf = pd.DataFrame({'A': [1, 2, 3], \n                   'B': [1, 2, np.nan], \n                   'C': [4, 5, 6], \n                   'D': [np.nan, np.nan, np.nan]})\n\ndf\nOut: \n   A    B  C   D\n0  1  1.0  4 NaN\n1  2  2.0  5 NaN\n2  3  NaN  6 NaN\n\ndf.columns[df.isnull().any()]\nOut: Index(['B', 'D'], dtype='object')\n\ndf.columns[df.isnull().any()].tolist()  # to get a list instead of an Index object\nOut: ['B', 'D']\n"
"In [75]:\n# convert to datetime\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23 entries, 0 to 22\nData columns (total 2 columns):\nDate     23 non-null datetime64[ns]\nValue    23 non-null float64\ndtypes: datetime64[ns](1), float64(1)\nmemory usage: 448.0 bytes\n\nIn [76]:\n# set the index\ndf.set_index('Date', inplace=True)\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 23 entries, 1988-01-01 to 1988-01-23\nData columns (total 1 columns):\nValue    23 non-null float64\ndtypes: float64(1)\nmemory usage: 368.0 bytes\n"
"import pandas as pd\nfrom functools import reduce\nreduce(lambda x, y: pd.merge(x, y, on = 'Date'), dfList)\n\ndf = pd.DataFrame({'Date': [1,2,3,4], 'Value': [2,3,3,4]})\ndfList = [df, df, df]\ndfList\n\n# [   Date  Value\n#  0     1      2\n#  1     2      3\n#  2     3      3\n#  3     4      4,    Date  Value\n#  0     1      2\n#  1     2      3\n#  2     3      3\n#  3     4      4,    Date  Value\n#  0     1      2\n#  1     2      3\n#  2     3      3\n#  3     4      4]\n\nreduce(lambda x, y: pd.merge(x, y, on = 'Date'), dfList)\n#   Date  Value_x  Value_y  Value\n# 0    1        2        2      2\n# 1    2        3        3      3\n# 2    3        3        3      3\n# 3    4        4        4      4\n"
'print (reshaped_df)\nsale_product_id  1    8    52   312  315\nsale_user_id                            \n1                  1    1    1    5    1\n\nprint (reshaped_df.index.name)\nsale_user_id\n\nprint (reshaped_df.rename_axis(None))\nsale_product_id  1    8    52   312  315\n1                  1    1    1    5    1\n\nreshaped_df.index.name = None\nprint (reshaped_df)\n\nsale_product_id  1    8    52   312  315\n1                  1    1    1    5    1\n\nprint (reshaped_df.columns.name)\nsale_product_id\n\nprint (reshaped_df.rename_axis(None).rename_axis(None, axis=1))\n   1    8    52   312  315\n1    1    1    1    5    1\n\nreshaped_df.columns.name = None\nreshaped_df.index.name = None\nprint (reshaped_df)\n   1    8    52   312  315\n1    1    1    1    5    1\n\nreshaped_df = reshaped_df.reset_index(drop=True)\nprint (reshaped_df)\nsale_product_id  1    8    52   312  315\n0                  1    1    1    5    1\n\n#if need reset index nad remove column name\nreshaped_df = reshaped_df.reset_index(drop=True).rename_axis(None, axis=1)\nprint (reshaped_df)\n   1    8    52   312  315\n0    1    1    1    5    1\n\nreshaped_df = reshaped_df.rename_axis(None, axis=1)\nprint (reshaped_df)\n              1    8    52   312  315\nsale_user_id                         \n1               1    1    1    5    1\n\nreshaped_df =  reshaped_df.rename_axis(None, axis=1).reset_index() \nprint (reshaped_df)\n   sale_user_id  1  8  52  312  315\n0             1  1  1   1    5    1\n'
"# don't forget to import\nimport pandas as pd\nimport multiprocessing\n\n# create as many processes as there are CPUs on your machine\nnum_processes = multiprocessing.cpu_count()\n\n# calculate the chunk size as an integer\nchunk_size = int(df.shape[0]/num_processes)\n\n# this solution was reworked from the above link.\n# will work even if the length of the dataframe is not evenly divisible by num_processes\nchunks = [df.ix[df.index[i:i + chunk_size]] for i in range(0, df.shape[0], chunk_size)]\n\ndef func(d):\n   # let's create a function that squares every value in the dataframe\n   return d * d\n\n# create our pool with `num_processes` processes\npool = multiprocessing.Pool(processes=num_processes)\n\n# apply our function to each chunk in the list\nresult = pool.map(func, chunks)\n\nfor i in range(len(result)):\n   # since result[i] is just a dataframe\n   # we can reassign the original dataframe based on the index of each chunk\n   df.ix[result[i].index] = result[i]\n\ndef func(d):\n   for row in d.iterrow():\n      idx = row[0]\n      k = row[1]['Chromosome']\n      start,end = row[1]['Bin'].split('-')\n\n      sequence = sequence_from_coordinates(k,1,start,end) #slow download form http\n      d.set_value(idx,'GC%',gc_content(sequence,percent=False,verbose=False))\n      d.set_value(idx,'G4 repeats', sum([len(list(i)) for i in g4_scanner(sequence)]))\n      d.set_value(idx,'max flexibility',max([item[1] for item in dna_flex(sequence,verbose=False)]))\n   # return the chunk!\n   return d\n"
'p.xaxis.major_label_orientation = math.pi/2\n\n# or alternatively:\np.xaxis.major_label_orientation = &quot;vertical&quot;\n'
'for col in df.columns:\n    df[col].values[:] = 0\n\nfor col in df.columns:\n    if np.issubdtype(df[col].dtype, np.number):\n        df[col].values[:] = 0\n\nimport pandas as pd\nimport numpy as np\n\ndef make_df(n, only_numeric):\n    series = [\n        pd.Series(range(n), name="int", dtype=int),\n        pd.Series(range(n), name="float", dtype=float),\n    ]\n    if only_numeric:\n        series.extend(\n            [\n                pd.Series(range(n, 2 * n), name="int2", dtype=int),\n                pd.Series(range(n, 2 * n), name="float2", dtype=float),\n            ]\n        )\n    else:\n        series.extend(\n            [\n                pd.date_range(start="1970-1-1", freq="T", periods=n, name="dt")\n                .to_series()\n                .reset_index(drop=True),\n                pd.Series(\n                    [chr((i % 26) + 65) for i in range(n)],\n                    name="string",\n                    dtype="object",\n                ),\n            ]\n        )\n\n    return pd.concat(series, axis=1)\n\n&gt;&gt;&gt; make_df(5, True)\n   int  float  int2  float2\n0    0    0.0     5     5.0\n1    1    1.0     6     6.0\n2    2    2.0     7     7.0\n3    3    3.0     8     8.0\n4    4    4.0     9     9.0\n\n&gt;&gt;&gt; make_df(5, False)\n   int  float                  dt string\n0    0    0.0 1970-01-01 00:00:00      A\n1    1    1.0 1970-01-01 00:01:00      B\n2    2    2.0 1970-01-01 00:02:00      C\n3    3    3.0 1970-01-01 00:03:00      D\n4    4    4.0 1970-01-01 00:04:00      E\n\nn = 10_000                                                                                  \n\n# Numeric df, no issubdtype check\n%%timeit df = make_df(n, True)\nfor col in df.columns:\n    df[col].values[:] = 0\n36.1 µs ± 510 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n# Numeric df, yes issubdtype check\n%%timeit df = make_df(n, True)\nfor col in df.columns:\n    if np.issubdtype(df[col].dtype, np.number):\n        df[col].values[:] = 0\n53 µs ± 645 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n# Non-numeric df, no issubdtype check\n%%timeit df = make_df(n, False)\nfor col in df.columns:\n    df[col].values[:] = 0\n113 µs ± 391 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n# Non-numeric df, yes issubdtype check\n%%timeit df = make_df(n, False)\nfor col in df.columns:\n    if np.issubdtype(df[col].dtype, np.number):\n        df[col].values[:] = 0\n39.4 µs ± 1.91 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\nn = 10_000_000                                                                             \n\n# Numeric df, no issubdtype check\n%%timeit df = make_df(n, True)\nfor col in df.columns:\n    df[col].values[:] = 0\n38.7 ms ± 151 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n# Numeric df, yes issubdtype check\n%%timeit df = make_df(n, True)\nfor col in df.columns:\n    if np.issubdtype(df[col].dtype, np.number):\n        df[col].values[:] = 0\n39.1 ms ± 556 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n# Non-numeric df, no issubdtype check\n%%timeit df = make_df(n, False)\nfor col in df.columns:\n    df[col].values[:] = 0\n99.5 ms ± 748 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n# Non-numeric df, yes issubdtype check\n%%timeit df = make_df(n, False)\nfor col in df.columns:\n    if np.issubdtype(df[col].dtype, np.number):\n        df[col].values[:] = 0\n17.8 ms ± 228 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\ndf[:] = 0\n'
"d={\n    'key1': [10, 100.1, 0.98, 1.2],\n    'key2': [72.5],\n    'key3': [1, 5.2, 71.2, 9, 10.11, 12.21, 65, 7]\n}\n\ndf=pd.DataFrame.from_dict(d,orient='index').transpose()\n\n    key3    key2    key1\n0   1.00    72.5    10.00\n1   5.20    NaN     100.10\n2   71.20   NaN     0.98\n3   9.00    NaN     1.20\n4   10.11   NaN     NaN\n\nimport numpy as np\nnp.nanmean(df[['key1']])\n28.07\n"
"test['range'] = pd.cut(test.days, [0,30,60], include_lowest=True)\nprint (test)\n   days           range\n0     0  (-0.001, 30.0]\n1    31    (30.0, 60.0]\n2    45    (30.0, 60.0]\n\ntest = pd.DataFrame({'days': [0,20,30,31,45,60]})\n\ntest['range1'] = pd.cut(test.days, [0,30,60], include_lowest=True)\n#30 value is in [30, 60) group\ntest['range2'] = pd.cut(test.days, [0,30,60], right=False)\n#30 value is in (0, 30] group\ntest['range3'] = pd.cut(test.days, [0,30,60])\nprint (test)\n   days          range1    range2    range3\n0     0  (-0.001, 30.0]   [0, 30)       NaN\n1    20  (-0.001, 30.0]   [0, 30)   (0, 30]\n2    30  (-0.001, 30.0]  [30, 60)   (0, 30]\n3    31    (30.0, 60.0]  [30, 60)  (30, 60]\n4    45    (30.0, 60.0]  [30, 60)  (30, 60]\n5    60    (30.0, 60.0]       NaN  (30, 60]\n\narr = np.array([0,30,60])\ntest['range1'] = arr.searchsorted(test.days)\ntest['range2'] = arr.searchsorted(test.days, side='right') - 1\nprint (test)\n   days  range1  range2\n0     0       0       0\n1    20       1       0\n2    30       1       1\n3    31       2       1\n4    45       2       1\n5    60       2       2\n"
"data = {'pk' : np.random.choice(10, 1000)} \ndata.update({'Val{}'.format(i) : np.random.randn(1000) for i in range(100)})\n\ndf = pd.DataFrame(data)\n\ng = df.groupby('pk')\nc = ['Val{}'.format(i) for i in range(100)]\n\nv1 = df.groupby('pk')[c].diff().fillna(0)\n\nv2 = df.groupby('pk')[c].transform(lambda x: x - x.shift(1)).fillna(0)\n\nnp.allclose(v1, v2)\nTrue\n\n%timeit df.groupby('pk')[c].transform(lambda x: x - x.shift(1)).fillna(0)\n10 loops, best of 3: 44.3 ms per loop\n\n%timeit df.groupby('pk')[c].diff(-1).fillna(0)\n100 loops, best of 3: 9.63 ms per loop\n\ng = df.groupby('pk', as_index=False)\n\nv3 = g[c].rolling(4).mean().shift(1)\n\ng2 = df.groupby('pk')\nv4 = g2[c].rolling(4).mean().shift(1).reset_index()[c]\n\nnp.allclose(v3.fillna(0), v4.fillna(0))\nTrue\n\n%timeit df.groupby('pk')[c].rolling(4).mean().shift(1).reset_index()[c]\n10 loops, best of 3: 46.5 ms per loop\n\n%timeit df.groupby('pk', as_index=False)[c].rolling(4).mean().shift(1)\n10 loops, best of 3: 41.7 ms per loop\n"
"df1 = pd.DataFrame([[1,2,3,4], [6,7,8,9]], columns=['D', 'B', 'E', 'A'], index=[1,2])\ndf2 = pd.DataFrame([[10,20,30,40], [60,70,80,90], [600,700,800,900]], columns=['A', 'B', 'C', 'D'], index=[2,3,4])\n\nprint(df1)\n\n   D  B  E  A\n1  1  2  3  4\n2  6  7  8  9\n\nprint(df2)\n\n     A    B    C    D\n2   10   20   30   40\n3   60   70   80   90\n4  600  700  800  900\n\na1, a2 = df1.align(df2, join='outer', axis=1)\nprint(a1)\nprint(a2)\n\n   A  B   C  D  E\n1  4  2 NaN  1  3\n2  9  7 NaN  6  8\n     A    B    C    D   E\n2   10   20   30   40 NaN\n3   60   70   80   90 NaN\n4  600  700  800  900 NaN\n\na1, a2 = df1.align(df2, join='right', axis=None)\nprint(a1)\nprint(a2)\n\n     A    B   C    D\n2  9.0  7.0 NaN  6.0\n3  NaN  NaN NaN  NaN\n4  NaN  NaN NaN  NaN\n     A    B    C    D\n2   10   20   30   40\n3   60   70   80   90\n4  600  700  800  900\n\na1, a2 = df1.align(df2, join='inner', axis=1)\nprint(a1)\nprint(a2)\n\n   D  B  A\n1  1  2  4\n2  6  7  9\n     D    B    A\n2   40   20   10\n3   90   70   60\n4  900  700  600\n"
'   df = df.to_json()\n'
"In [20]: df = pd.DataFrame({'A':[1,1,2,2],'B':[1,2,1,2],'values':np.arange(10,30,5)})\n\nIn [21]: df\nOut[21]:\n   A  B  values\n0  1  1      10\n1  1  2      15\n2  2  1      20\n3  2  2      25\n\nIn [22]: df['sum_values_A'] = df.groupby('A')['values'].transform(np.sum)\n\nIn [23]: df\nOut[23]:\n   A  B  values  sum_values_A\n0  1  1      10            25\n1  1  2      15            25\n2  2  1      20            45\n3  2  2      25            45\n"
'&gt;&gt;&gt; df\n                 A      B\nDATE                     \n2013-05-01  473077  71333\n2013-05-02   35131  62441\n2013-05-03     727  27381\n2013-05-04     481   1206\n2013-05-05     226   1733\n2013-05-06     NaN   4064\n2013-05-07     NaN  41151\n2013-05-08     NaN   8144\n2013-05-09     NaN     23\n2013-05-10     NaN     10\n&gt;&gt;&gt; df.mean(axis=1)\nDATE\n2013-05-01    272205.0\n2013-05-02     48786.0\n2013-05-03     14054.0\n2013-05-04       843.5\n2013-05-05       979.5\n2013-05-06      4064.0\n2013-05-07     41151.0\n2013-05-08      8144.0\n2013-05-09        23.0\n2013-05-10        10.0\ndtype: float64\n'
'import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(np.random.normal(0, 1, (5, 2)), columns=["A", "B"])\n\ndf.A.count()\n#or\ndf[\'A\'].count()\n\ndf[\'A\'][1::2] = np.NAN\ndf.count()\n\n A    3\n B    5\n'
'import pandas as pd\ndf = pd.DataFrame(list(tweets.find()))\n'
"pd.merge(df1, df2, on=['A', 'B'])\n\npd.merge(df1, df2, on=['A', 'B'], how='outer')\n\nmerged_df.drop_duplicates(cols=['A', 'B'],inplace=True)\n"
"# value_name is 'value' by default, but setting it here to make it clear\npd.melt(x, id_vars=['farm', 'fruit'], var_name='year', value_name='value')\n\n  farm  fruit  year  value\n0    A  apple  2014     10\n1    B  apple  2014     12\n2    A   pear  2014      6\n3    B   pear  2014      8\n4    A  apple  2015     11\n5    B  apple  2015     13\n6    A   pear  2015      7\n7    B   pear  2015      9\n\n[8 rows x 4 columns]\n"
'In [341]: max_len = max(len(sublist) for sublist in C)\nIn [344]: for sublist in C:\n     ...:     sublist.extend([np.nan] * (max_len - len(sublist)))\n\nIn [345]: C\nOut[345]: \n[[7, 11, 56, 45],\n [20, 21, 74, 12],\n [42, nan, nan, nan],\n [52, nan, nan, nan],\n [90, 213, 9, nan],\n [101, 34, 45, nan]]\n\nIn [288]: C = np.array(C)\nIn [289]: df = pd.DataFrame(data=C.T, columns=pd.MultiIndex.from_tuples(zip(A,B)))\n\nIn [349]: df\nOut[349]: \n     one         two       three     \n   start  end  start  end  start  end\n0      7   20     42   52     90  101\n1     11   21    NaN  NaN    213   34\n2     56   74    NaN  NaN      9   45\n3     45   12    NaN  NaN    NaN  NaN\n'
'df = DataFrame(eval(data))\n\ndata = []\nfor row in result_set:\n    data.append({\'value\': row["tag_expression"], \'key\': row["tag_name"]})\n'
'df[(df != 0).all(1)]\n'
"import numpy as np\nimport pandas as pd\n\nfilename = '/tmp/test.h5'\n\ndf = pd.DataFrame(np.arange(10).reshape((5,2)), columns=['A', 'B'])\nprint(df)\n#    A  B\n# 0  0  1\n# 1  2  3\n# 2  4  5\n# 3  6  7\n# 4  8  9\n\n# Save to HDF5\ndf.to_hdf(filename, 'data', mode='w', format='table')\ndel df    # allow df to be garbage collected\n\n# Append more data\ndf2 = pd.DataFrame(np.arange(10).reshape((5,2))*10, columns=['A', 'B'])\ndf2.to_hdf(filename, 'data', append=True)\n\nprint(pd.read_hdf(filename, 'data'))\n\n    A   B\n0   0   1\n1   2   3\n2   4   5\n3   6   7\n4   8   9\n0   0  10\n1  20  30\n2  40  50\n3  60  70\n4  80  90\n\nimport numpy as np\nimport pandas as pd\n\nfilename = '/tmp/test.h5'\nstore = pd.HDFStore(filename)\n\nfor i in range(2):\n    df = pd.DataFrame(np.arange(10).reshape((5,2)) * 10**i, columns=['A', 'B'])\n    store.append('data', df)\n\nstore.close()\n\nstore = pd.HDFStore(filename)\ndata = store['data']\nprint(data)\nstore.close()\n\n    A   B\n0   0   1\n1   2   3\n2   4   5\n3   6   7\n4   8   9\n0   0  10\n1  20  30\n2  40  50\n3  60  70\n4  80  90\n"
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from pandas import *\n&gt;&gt;&gt; df = DataFrame({'foo1' : np.random.randn(2),\n                    'foo2' : np.random.randn(2)})\n&gt;&gt;&gt; df.to_html('filename.html')\n"
'&gt;&gt;&gt; df = pd.DataFrame({"body": ["ball", "red BALL", "round sphere"]})\n&gt;&gt;&gt; df[df["body"].str.contains("ball")]\n   body\n0  ball\n&gt;&gt;&gt; df[df["body"].str.lower().str.contains("ball")]\n       body\n0      ball\n1  red BALL\n&gt;&gt;&gt; df[df["body"].str.contains("ball", case=False)]\n       body\n0      ball\n1  red BALL\n&gt;&gt;&gt; df[df["body"].str.contains("ball", case=True)]\n   body\n0  ball\n'
"writer = pandas.ExcelWriter(r'file.xlsx', engine='xlsxwriter',options={'strings_to_urls': False})\ndf.to_excel(writer)\nwriter.close()\n"
"a[pd.isnull(a)]  = 'NaN'\n"
"import calendar\ndf['Month'] = df['Month'].apply(lambda x: calendar.month_abbr[x])\n"
'import pandas as pd\nimport numpy as np\nprng = np.random.RandomState(0)\ndf = pd.DataFrame(prng.randn(100, 2), columns = ["A", "B"])\n\nser, bins = pd.qcut(df["A"], 20, retbins=True, labels=False)\n\npd.cut(df["B"], bins=bins, labels=False, include_lowest=True)\nOut[38]: \n0     13\n1     19\n2      3\n3      9\n4     13\n5     17\n...\n'
"In [2]:\nA = pd.DataFrame({'x':np.arange(5)})\nB = pd.DataFrame({'x':np.arange(3,8)})\nprint(A)\nprint(B)\n   x\n0  0\n1  1\n2  2\n3  3\n4  4\n   x\n0  3\n1  4\n2  5\n3  6\n4  7\n\nIn [3]:\npd.merge(A,B, how='outer', indicator=True)\n\nOut[3]:\n     x      _merge\n0  0.0   left_only\n1  1.0   left_only\n2  2.0   left_only\n3  3.0        both\n4  4.0        both\n5  5.0  right_only\n6  6.0  right_only\n7  7.0  right_only\n\nIn [4]:\nmerged = pd.merge(A,B, how='outer', indicator=True)\nmerged[merged['_merge'] == 'left_only']\n\nOut[4]:\n     x     _merge\n0  0.0  left_only\n1  1.0  left_only\n2  2.0  left_only\n\nIn [5]:\nA[~A['x'].isin(B['x'])]\n\nOut[5]:\n   x\n0  0\n1  1\n2  2\n"
'&gt;&gt;&gt; df1.col1 = df1.col1.astype(str)\n&gt;&gt;&gt; df2.col2 = df2.col2.astype(str)\n'
"### Test data\ndf = DataFrame({'text': ['foo foo foo foo foo foo foo foo', 'bar bar bar bar bar'],\n                 'number': [1, 2]})\n\ndf.style.set_properties(subset=['text'], **{'width': '300px'})\n"
"import pandas as pd\n\nUSERS = pd.DataFrame({'email':['a@g.com','b@g.com','b@g.com','c@g.com','d@g.com']})\nprint (USERS)\n     email\n0  a@g.com\n1  b@g.com\n2  b@g.com\n3  c@g.com\n4  d@g.com\n\nEXCLUDE = pd.DataFrame({'email':['a@g.com','d@g.com']})\nprint (EXCLUDE)\n     email\n0  a@g.com\n1  d@g.com\n\nprint (USERS.email.isin(EXCLUDE.email))\n0     True\n1    False\n2    False\n3    False\n4     True\nName: email, dtype: bool\n\nprint (~USERS.email.isin(EXCLUDE.email))\n0    False\n1     True\n2     True\n3     True\n4    False\nName: email, dtype: bool\n\nprint (USERS[~USERS.email.isin(EXCLUDE.email)])\n     email\n1  b@g.com\n2  b@g.com\n3  c@g.com\n\ndf = pd.merge(USERS, EXCLUDE, how='outer', indicator=True)\nprint (df)\n     email     _merge\n0  a@g.com       both\n1  b@g.com  left_only\n2  b@g.com  left_only\n3  c@g.com  left_only\n4  d@g.com       both\n\nprint (df.loc[df._merge == 'left_only', ['email']])\n     email\n1  b@g.com\n2  b@g.com\n3  c@g.com\n"
'df[\'COLUMN1\'].str.decode("utf-8")\n\nstr_df = df.select_dtypes([np.object])\n\nstr_df = str_df.stack().str.decode(\'utf-8\').unstack()\n\nfor col in str_df:\n    df[col] = str_df[col]\n'
'j = (df.groupby([\'ID\',\'Location\',\'Country\',\'Latitude\',\'Longitude\'], as_index=False)\n             .apply(lambda x: x[[\'timestamp\',\'tide\']].to_dict(\'r\'))\n             .reset_index()\n             .rename(columns={0:\'Tide-Data\'})\n             .to_json(orient=\'records\'))\n\nIn [103]: print(json.dumps(json.loads(j), indent=2, sort_keys=True))\n[\n  {\n    "Country": "FRA",\n    "ID": 1,\n    "Latitude": 48.383,\n    "Location": "BREST",\n    "Longitude": -4.495,\n    "Tide-Data": [\n      {\n        "tide": 6905.0,\n        "timestamp": "1807-01-01"\n      },\n      {\n        "tide": 6931.0,\n        "timestamp": "1807-02-01"\n      },\n      {\n        "tide": 6896.0,\n        "timestamp": "1807-03-01"\n      },\n      {\n        "tide": 6953.0,\n        "timestamp": "1807-04-01"\n      },\n      {\n        "tide": 7043.0,\n        "timestamp": "1807-05-01"\n      }\n    ]\n  },\n  {\n    "Country": "DEU",\n    "ID": 7,\n    "Latitude": 53.867,\n    "Location": "CUXHAVEN 2",\n    "Longitude": 8.717,\n    "Tide-Data": [\n      {\n        "tide": 7093.0,\n        "timestamp": "1843-01-01"\n      },\n      {\n        "tide": 6688.0,\n        "timestamp": "1843-02-01"\n      },\n      {\n        "tide": 6493.0,\n        "timestamp": "1843-03-01"\n      },\n      {\n        "tide": 6723.0,\n        "timestamp": "1843-04-01"\n      },\n      {\n        "tide": 6533.0,\n        "timestamp": "1843-05-01"\n      }\n    ]\n  },\n  {\n    "Country": "DEU",\n    "ID": 8,\n    "Latitude": 53.899,\n    "Location": "WISMAR 2",\n    "Longitude": 11.458,\n    "Tide-Data": [\n      {\n        "tide": 6957.0,\n        "timestamp": "1848-07-01"\n      },\n      {\n        "tide": 6944.0,\n        "timestamp": "1848-08-01"\n      },\n      {\n        "tide": 7084.0,\n        "timestamp": "1848-09-01"\n      },\n      {\n        "tide": 6898.0,\n        "timestamp": "1848-10-01"\n      },\n      {\n        "tide": 6859.0,\n        "timestamp": "1848-11-01"\n      }\n    ]\n  },\n  {\n    "Country": "NLD",\n    "ID": 9,\n    "Latitude": 51.918,\n    "Location": "MAASSLUIS",\n    "Longitude": 4.25,\n    "Tide-Data": [\n      {\n        "tide": 6880.0,\n        "timestamp": "1848-02-01"\n      },\n      {\n        "tide": 6700.0,\n        "timestamp": "1848-03-01"\n      },\n      {\n        "tide": 6775.0,\n        "timestamp": "1848-04-01"\n      },\n      {\n        "tide": 6580.0,\n        "timestamp": "1848-05-01"\n      },\n      {\n        "tide": 6685.0,\n        "timestamp": "1848-06-01"\n      }\n    ]\n  },\n  {\n    "Country": "USA",\n    "ID": 10,\n    "Latitude": 37.807,\n    "Location": "SAN FRANCISCO",\n    "Longitude": -122.465,\n    "Tide-Data": [\n      {\n        "tide": 6909.0,\n        "timestamp": "1854-07-01"\n      },\n      {\n        "tide": 6940.0,\n        "timestamp": "1854-08-01"\n      },\n      {\n        "tide": 6961.0,\n        "timestamp": "1854-09-01"\n      },\n      {\n        "tide": 6952.0,\n        "timestamp": "1854-10-01"\n      },\n      {\n        "tide": 6952.0,\n        "timestamp": "1854-11-01"\n      }\n    ]\n  }\n]\n\nj = (df.groupby([\'ID\',\'Location\',\'Country\',\'Latitude\',\'Longitude\'], as_index=False)\n       .apply(lambda x: dict(zip(x.timestamp,x.tide)))\n       .reset_index()\n       .rename(columns={0:\'Tide-Data\'})\n       .to_json(orient=\'records\'))\n\nIn [112]: print(json.dumps(json.loads(j), indent=2, sort_keys=True))\n[\n  {\n    "Country": "FRA",\n    "ID": 1,\n    "Latitude": 48.383,\n    "Location": "BREST",\n    "Longitude": -4.495,\n    "Tide-Data": {\n      "1807-01-01": 6905.0,\n      "1807-02-01": 6931.0,\n      "1807-03-01": 6896.0,\n      "1807-04-01": 6953.0,\n      "1807-05-01": 7043.0\n    }\n  },\n  {\n    "Country": "DEU",\n    "ID": 7,\n    "Latitude": 53.867,\n    "Location": "CUXHAVEN 2",\n    "Longitude": 8.717,\n    "Tide-Data": {\n      "1843-01-01": 7093.0,\n      "1843-02-01": 6688.0,\n      "1843-03-01": 6493.0,\n      "1843-04-01": 6723.0,\n      "1843-05-01": 6533.0\n    }\n  },\n  {\n    "Country": "DEU",\n    "ID": 8,\n    "Latitude": 53.899,\n    "Location": "WISMAR 2",\n    "Longitude": 11.458,\n    "Tide-Data": {\n      "1848-07-01": 6957.0,\n      "1848-08-01": 6944.0,\n      "1848-09-01": 7084.0,\n      "1848-10-01": 6898.0,\n      "1848-11-01": 6859.0\n    }\n  },\n  {\n    "Country": "NLD",\n    "ID": 9,\n    "Latitude": 51.918,\n    "Location": "MAASSLUIS",\n    "Longitude": 4.25,\n    "Tide-Data": {\n      "1848-02-01": 6880.0,\n      "1848-03-01": 6700.0,\n      "1848-04-01": 6775.0,\n      "1848-05-01": 6580.0,\n      "1848-06-01": 6685.0\n    }\n  },\n  {\n    "Country": "USA",\n    "ID": 10,\n    "Latitude": 37.807,\n    "Location": "SAN FRANCISCO",\n    "Longitude": -122.465,\n    "Tide-Data": {\n      "1854-07-01": 6909.0,\n      "1854-08-01": 6940.0,\n      "1854-09-01": 6961.0,\n      "1854-10-01": 6952.0,\n      "1854-11-01": 6952.0\n    }\n  }\n]\n\n(df.groupby([\'ID\',\'Location\',\'Country\',\'Latitude\',\'Longitude\'], as_index=False)\n   .apply(lambda x: dict(zip(x.timestamp,x.tide)))\n   .reset_index()\n   .rename(columns={0:\'Tide-Data\'})\n   .to_json(\'/path/to/file_name.json\', orient=\'records\'))\n'
'tqdm_notebook().pandas(*args, **kwargs)\n\ntqdm_pandas(tqdm_notebook, *args, **kwargs)\n'
"data.iloc[:, 7:12] = data.iloc[:, 7:12].apply(pd.to_datetime, errors='coerce')\n"
"df.set_index('Timestamp', inplace=True, drop=True)\n"
"import seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nbatData = ['a','b','c','a','c']\nbowlData = ['b','a','d','d','a']\n\ndf=pd.DataFrame()\ndf['batting']=batData\ndf['bowling']=bowlData\n\n\nfig, ax =plt.subplots(1,2)\nsns.countplot(df['batting'], ax=ax[0])\nsns.countplot(df['bowling'], ax=ax[1])\nfig.show()\n"
"t['combined']= t.values.tolist()\n\nt\nOut[50]: \n         A         B     C        D                       combined\n0    hello         1  GOOD  long.kw      [hello, 1, GOOD, long.kw]\n1     1.20  chipotle   NaN    bingo    [1.2, chipotle, nan, bingo]\n2  various       NaN  3000   123.46  [various, nan, 3000, 123.456]\n"
'A = pd.DataFrame(dict(\n        A_id=range(10),\n        A_value=range(5, 105, 10)\n    ))\nB = pd.DataFrame(dict(\n        B_id=range(5),\n        B_low=[0, 30, 30, 46, 84],\n        B_high=[10, 40, 50, 54, 84]\n    ))\n\nA\n\n   A_id  A_value\n0     0        5\n1     1       15\n2     2       25\n3     3       35\n4     4       45\n5     5       55\n6     6       65\n7     7       75\n8     8       85\n9     9       95\n\nB\n\n   B_high  B_id  B_low\n0      10     0      0\n1      40     1     30\n2      50     2     30\n3      54     3     46\n4      84     4     84\n\na = A.A_value.values\nbh = B.B_high.values\nbl = B.B_low.values\n\ni, j = np.where((a[:, None] &gt;= bl) &amp; (a[:, None] &lt;= bh))\n\npd.DataFrame(\n    np.column_stack([A.values[i], B.values[j]]),\n    columns=A.columns.append(B.columns)\n)\n\n   A_id  A_value  B_high  B_id  B_low\n0     0        5      10     0      0\n1     3       35      40     1     30\n2     3       35      50     2     30\n3     4       45      50     2     30\n\npd.DataFrame(\n    np.column_stack([A.values[i], B.values[j]]),\n    columns=A.columns.append(B.columns)\n).append(\n    A[~np.in1d(np.arange(len(A)), np.unique(i))],\n    ignore_index=True, sort=False\n)\n\n    A_id  A_value  B_id  B_low  B_high\n0      0        5   0.0    0.0    10.0\n1      3       35   1.0   30.0    40.0\n2      3       35   2.0   30.0    50.0\n3      4       45   2.0   30.0    50.0\n4      1       15   NaN    NaN     NaN\n5      2       25   NaN    NaN     NaN\n6      5       55   NaN    NaN     NaN\n7      6       65   NaN    NaN     NaN\n8      7       75   NaN    NaN     NaN\n9      8       85   NaN    NaN     NaN\n10     9       95   NaN    NaN     NaN\n'
"df['min'] = df.data[(df.data.shift(1) &gt; df.data) &amp; (df.data.shift(-1) &gt; df.data)]\ndf['max'] = df.data[(df.data.shift(1) &lt; df.data) &amp; (df.data.shift(-1) &lt; df.data)]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Generate a noisy AR(1) sample\nnp.random.seed(0)\nrs = np.random.randn(200)\nxs = [0]\nfor r in rs:\n    xs.append(xs[-1]*0.9 + r)\ndf = pd.DataFrame(xs, columns=['data'])\n\n# Find local peaks\ndf['min'] = df.data[(df.data.shift(1) &gt; df.data) &amp; (df.data.shift(-1) &gt; df.data)]\ndf['max'] = df.data[(df.data.shift(1) &lt; df.data) &amp; (df.data.shift(-1) &lt; df.data)]\n\n# Plot results\nplt.scatter(df.index, df['min'], c='r')\nplt.scatter(df.index, df['max'], c='g')\ndf.data.plot()\n"
'col = df.loc[: , "salary_1":"salary_3"]\n\ndf[\'salary_mean\'] = col.mean(axis=1)\ndf\n'
"pd.read_csv(StringIO(data), converters={'strings' : str})\n\n  strings  numbers\n0     foo        1\n1     bar        2\n2    null        3\n\npd.read_csv(StringIO(data), na_filter=False)\n\n  strings  numbers\n0     foo        1\n1     bar        2\n2    null        3\n"
'def _compute_qth_percentile(sorted, per, interpolation_method, axis):\n    ....\n    indexer = [slice(None)] * sorted.ndim\n    ...\n    indexer[axis] = slice(i, i + 2)\n    ...\n    return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n\nIn [81]: x = np.arange(12).reshape(3,4)\nIn [83]: indexer = [slice(None), slice(None,2)]\nIn [84]: x[indexer]\n/usr/local/bin/ipython3:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n  #!/usr/bin/python3\nOut[84]: \narray([[0, 1],\n       [4, 5],\n       [8, 9]])\nIn [85]: x[tuple(indexer)]\nOut[85]: \narray([[0, 1],\n       [4, 5],\n       [8, 9]])\n'
"Collecting matplotlib\n  Downloading https://files.pythonhosted.org/packages/26/04/8b381d5b166508cc258632b225adbafec49bbe69aa9a4fa1f1b461428313/matplotlib-3.0.3.tar.gz (36.6MB)\n    Complete output from command python setup.py egg_info:\n    Download error on https://pypi.org/simple/numpy/: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833) -- Some packages may not be found!\n    Couldn't find index page for 'numpy' (maybe misspelled?)\n    Download error on https://pypi.org/simple/: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:833) -- Some packages may not be found!\n    No local packages or working download links found for numpy&gt;=1.10.0\n\npostgresql-dev libffi-dev libressl-dev libxml2 libxml2-dev libxslt libxslt-dev libjpeg-turbo-dev zlib-dev\n\nRUN apk del build-runtime &amp;&amp; \\\n    find -type d -name __pycache__ -prune -exec rm -rf {} \\; &amp;&amp; \\\n    rm -rf ~/.cache/pip\n"
'from more_itertools import consecutive_groups\nfinal=pd.concat([df.loc[i].reset_index(drop=True) \n                    for i in consecutive_groups(df.index)],axis=1)\nfinal.columns=range(len(final.columns))\nprint(final)\n\n              0             1\n0  19218.965703  19279.216956\n1  19247.621650  19330.087371\n2  19232.651322  19304.316973\n'
"df1 = df.stack().reset_index().drop(columns='level_1').drop_duplicates()\n\ndf1['col'] = df1.groupby('level_0').cumcount()\ndf1 = (df1.pivot(index='level_0', columns='col', values=0)\n          .rename_axis(index=None, columns=None))\n\n   0  1    2    3\n0  A  B    C    D\n1  A  D    C  NaN\n2  C  B  NaN  NaN\n3  B  A  NaN  NaN\n\nimport perfplot\nimport pandas as pd\nimport numpy as np\n\ndef stack(df):\n    df1 = df.stack().reset_index().drop(columns='level_1').drop_duplicates()\n\n    df1['col'] = df1.groupby('level_0').cumcount()\n    df1 = (df1.pivot(index='level_0', columns='col', values=0)\n              .rename_axis(index=None, columns=None))\n    return df1\n\ndef apply_drop_dup(df):\n    return pd.DataFrame.from_dict(df.apply(lambda x: x.drop_duplicates().tolist(),\n                                           axis=1).to_dict(), orient='index')\n\ndef apply_unique(df):\n    return pd.DataFrame(df.apply(pd.Series.unique, axis=1).tolist())\n\n\ndef list_map(df):\n    return pd.DataFrame(list(map(pd.unique, df.values)))\n\n\nperfplot.show(\n    setup=lambda n: pd.DataFrame(np.random.choice(list('ABCD'), (n, 4)),\n                                 columns=list('abcd')), \n    kernels=[\n        lambda df: stack(df),\n        lambda df: apply_drop_dup(df),\n        lambda df: apply_unique(df),\n        lambda df: list_map(df),\n    ],\n    labels=['stack', 'apply_drop_dup', 'apply_unique', 'list_map'],\n    n_range=[2 ** k for k in range(18)],\n    equality_check=lambda x,y: x.compare(y).empty,  \n    xlabel='~len(df)'\n)\n\ndef with_numpy(df):\n    arr = np.sort(df.to_numpy(), axis=1)\n    r = np.roll(arr, 1, axis=1)\n    r[:, 0] = np.NaN\n    \n    arr = np.where((arr != r), arr, np.NaN)\n    \n    # Move all NaN to the right. Credit @Divakar\n    mask = pd.notnull(arr)\n    justified_mask = np.flip(np.sort(mask, axis=1), 1)\n    out = np.full(arr.shape, np.NaN, dtype=object) \n    out[justified_mask] = arr[mask]\n    \n    return pd.DataFrame(out, index=df.index).dropna(how='all', axis='columns')\n\nwith_numpy(df)\n#   0  1    2    3\n#0  A  B    C    D\n#1  A  C    D  NaN\n#2  B  C  NaN  NaN     # B/c this method sorts, B before C\n#3  A  B  NaN  NaN\n\nperfplot.show(\n    setup=lambda n: pd.DataFrame(np.random.choice(list('ABCD'), (n, 4)),\n                                 columns=list('abcd')), \n    kernels=[\n        lambda df: stack(df),\n        lambda df: with_numpy(df),\n    ],\n    labels=['stack', 'with_numpy'],\n    n_range=[2 ** k for k in range(3, 22)],\n    # Lazy check to deal with string/NaN and irrespective of sort order. \n    equality_check=lambda x, y: (np.sort(x.fillna('ZZ').to_numpy(), 1) \n                                 == np.sort(y.fillna('ZZ').to_numpy(), 1)).all(),\n    xlabel='len(df)'\n)\n"
'In [55]: df = pd.DataFrame({\'A\': [\'loc_a\'] * 12 + [\'loc_b\'],\n   ....:                    \'B\': [\'group_a\'] * 7 + [\'group_b\'] * 3 + [\'group_c\'] * 2 + [\'group_a\'],\n   ....:                    \'Date\': ["2013-06-11",\n   ....:                            "2013-07-02",\n   ....:                            "2013-07-09",\n   ....:                            "2013-07-30",\n   ....:                            "2013-08-06",\n   ....:                            "2013-09-03",\n   ....:                            "2013-10-01",\n   ....:                            "2013-07-09",\n   ....:                            "2013-08-06",\n   ....:                            "2013-09-03",\n   ....:                            "2013-07-09",\n   ....:                            "2013-09-03",\n   ....:                            "2013-10-01"],\n   ....:                     \'Value\': [22, 35, 14,  9,  4, 40, 18, 4, 2, 5, 1, 2, 3]})\n\nIn [56]: \n\nIn [56]: df.Date = pd.to_datetime(df.Date)\n\nIn [57]: df = df.set_index([\'A\', \'B\', \'Date\'])\n\nIn [58]: \n\nIn [58]: print(df)\n                          Value\nA     B       Date             \nloc_a group_a 2013-06-11     22\n              2013-07-02     35\n              2013-07-09     14\n              2013-07-30      9\n              2013-08-06      4\n              2013-09-03     40\n              2013-10-01     18\n      group_b 2013-07-09      4\n              2013-08-06      2\n              2013-09-03      5\n      group_c 2013-07-09      1\n              2013-09-03      2\nloc_b group_a 2013-10-01      3\n\nIn [71]: df.unstack([\'A\', \'B\'])\nOut[71]: \n              Value                           \nA             loc_a                      loc_b\nB           group_a  group_b  group_c  group_a\nDate                                          \n2013-06-11       22      NaN      NaN      NaN\n2013-07-02       35      NaN      NaN      NaN\n2013-07-09       14        4        1      NaN\n2013-07-30        9      NaN      NaN      NaN\n2013-08-06        4        2      NaN      NaN\n2013-09-03       40        5        2      NaN\n2013-10-01       18      NaN      NaN        3\n\n\nIn [59]: df.unstack([\'A\', \'B\']).fillna(0).stack([\'A\', \'B\'])\nOut[59]: \n                          Value\nDate       A     B             \n2013-06-11 loc_a group_a     22\n                 group_b      0\n                 group_c      0\n           loc_b group_a      0\n2013-07-02 loc_a group_a     35\n                 group_b      0\n                 group_c      0\n           loc_b group_a      0\n2013-07-09 loc_a group_a     14\n                 group_b      4\n                 group_c      1\n           loc_b group_a      0\n2013-07-30 loc_a group_a      9\n                 group_b      0\n                 group_c      0\n           loc_b group_a      0\n2013-08-06 loc_a group_a      4\n                 group_b      2\n                 group_c      0\n           loc_b group_a      0\n2013-09-03 loc_a group_a     40\n                 group_b      5\n                 group_c      2\n           loc_b group_a      0\n2013-10-01 loc_a group_a     18\n                 group_b      0\n                 group_c      0\n           loc_b group_a      3\n'
'clean[column_name].value_counts()\n\npd.value_counts(df.values.flatten())\n'
'from pandas.util.testing import assert_frame_equal\n'
"In [39]: data['positive'] = data['values'] &gt; 0\n\nIn [40]: data\nOut[40]: \n   values positive\na   -15.0    False\nb    10.0     True\nc     8.0     True\nd    -4.5    False\n\n[4 rows x 2 columns]\n\nIn [41]: data['values'].plot(kind='barh',\n                             color=data.positive.map({True: 'g', False: 'r'}))\n"
"In [47]: pd.DatetimeIndex(pd.to_datetime(s,unit='ms')).tz_localize('UTC').tz_convert('US/Eastern')\nOut[47]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2014-09-19 17:18:27.178000-04:00, ..., 2014-09-19 10:32:40.544000-04:00]\nLength: 10, Freq: None, Timezone: US/Eastern\n"
'a = [0,0,1,1,1,0,0,1,0,1,1]\nb = [0,0,0,0,0,0,0,0,0,0,0]\n\nfor i in range(len(a)):\n    if a[i] == 1:\n        b[i] = b[i-1] + 1\n    else:\n        b[i] = 0\n'
'comb[criteria.index[criteria]]\n'
"import contextlib\nimport numpy as np\nimport pandas as pd\nimport pandas.io.formats.format as pf\nnp.random.seed(2015)\n\n@contextlib.contextmanager\ndef custom_formatting():\n    orig_float_format = pd.options.display.float_format\n    orig_int_format = pf.IntArrayFormatter\n\n    pd.options.display.float_format = '{:0,.2f}'.format\n    class IntArrayFormatter(pf.GenericArrayFormatter):\n        def _format_strings(self):\n            formatter = self.formatter or '{:,d}'.format\n            fmt_values = [formatter(x) for x in self.values]\n            return fmt_values\n    pf.IntArrayFormatter = IntArrayFormatter\n    yield\n    pd.options.display.float_format = orig_float_format\n    pf.IntArrayFormatter = orig_int_format\n\n\ndf = pd.DataFrame(np.random.randint(10000, size=(5,3)), columns=list('ABC'))\ndf['D'] = np.random.random(df.shape[0])*10000\n\nwith custom_formatting():\n    print(df)\n\n      A     B     C        D\n0 2,658 2,828 4,540 8,961.77\n1 9,506 2,734 9,805 2,221.86\n2 3,765 4,152 4,583 2,011.82\n3 5,244 5,395 7,485 8,656.08\n4 9,107 6,033 5,998 2,942.53\n\nprint(df)\n\n      A     B     C            D\n0  2658  2828  4540  8961.765260\n1  9506  2734  9805  2221.864779\n2  3765  4152  4583  2011.823701\n3  5244  5395  7485  8656.075610\n4  9107  6033  5998  2942.530551\n"
'dates_input = df["month_15"].values.astype(\'datetime64[D]\')\n\nIn [29]: df[\'month_15\'].astype(\'datetime64[D]\').dtype\nOut[29]: dtype(\'&lt;M8[ns]\')\n\nIn [28]: df[\'month_15\'].astype(\'datetime64[D]\').tolist()\nOut[28]: [Timestamp(\'2010-01-15 00:00:00\'), Timestamp(\'2011-01-15 00:00:00\')]\n\ntestf(df[\'month_15\'].astype(\'datetime64[D]\').values)\n\nIn [31]: df[\'month_15\'].astype(\'datetime64[D]\').values.dtype\nOut[31]: dtype(\'&lt;M8[ns]\')\n'
'&gt;&gt;&gt; df["rank"] = df.groupby("group_ID")["value"].rank("dense", ascending=False)\n&gt;&gt;&gt; df\n     group_ID item_ID  value  rank\n0  0S00A1HZEy      AB     10     2\n1  0S00A1HZEy      AY      4     3\n2  0S00A1HZEy      AC     35     1\n3  0S03jpFRaS      AY     90     1\n4  0S03jpFRaS      A5      3     5\n5  0S03jpFRaS      A3     10     2\n6  0S03jpFRaS      A2      8     4\n7  0S03jpFRaS      A4      9     3\n8  0S03jpFRaS      A6      2     6\n9  0S03jpFRaS      AX      0     7\n'
"print df[df['date'].astype(str).str.contains('07311954')]\n\nprint df[df['date'].str.contains('07311954')]\n\nprint df[df['date'].astype(str).str[-4:].str.contains('1954')]\n\nprint df['date']\n0    8152007\n1    9262007\n2    7311954\n3    2252011\n4    2012011\n5    2012011\n6    2222011\n7    2282011\nName: date, dtype: int64\n\nprint df['date'].astype(str).str[-4:].str.contains('1954')\n0    False\n1    False\n2     True\n3    False\n4    False\n5    False\n6    False\n7    False\nName: date, dtype: bool\n\nprint df[df['date'].astype(str).str[-4:].str.contains('1954')]\n     cmte_id trans_typ entity_typ state  employer  occupation     date  \\\n2  C00119040       24K        CCM    MD       NaN         NaN  7311954   \n\n   amount     fec_id    cand_id  \n2    1000  C00140715  H2MD05155  \n"
"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.tools.plotting import table\n\n# sample data\nraw_data = {'officer_name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'],\n        'jan_arrests': [4, 24, 31, 2, 3],\n        'feb_arrests': [25, 94, 57, 62, 70],\n        'march_arrests': [5, 43, 23, 23, 51]}\ndf = pd.DataFrame(raw_data, columns = ['officer_name', 'jan_arrests', 'feb_arrests', 'march_arrests'])\ndf['total_arrests'] = df['jan_arrests'] + df['feb_arrests'] + df['march_arrests']\n\nplt.figure(figsize=(16,8))\n# plot chart\nax1 = plt.subplot(121, aspect='equal')\ndf.plot(kind='pie', y = 'total_arrests', ax=ax1, autopct='%1.1f%%', \n startangle=90, shadow=False, labels=df['officer_name'], legend = False, fontsize=14)\n\n# plot table\nax2 = plt.subplot(122)\nplt.axis('off')\ntbl = table(ax2, df, loc='center')\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(14)\nplt.show()\n"
"def first_last(df):\n    return df.ix[[0, -1]]\n\ndf.groupby(level=0, group_keys=False).apply(first_last)\n\nidx = df.index.to_series().groupby(level=0).agg(['first', 'last']).stack()\ndf.loc[idx]\n\ndf.reset_index(1).groupby(level=0).agg(['first', 'last']).stack() \\\n    .set_index('level_1', append=True).reset_index(1, drop=True) \\\n    .rename_axis([None, None])\n\ndf = pd.DataFrame(np.arange(20).reshape(10, -1),\n                  [list('aaaabbbccd'),\n                   list('abcdefghij')],\n                  list('XY'))\n\ndf.loc[tuple('aa'), 'X'] = np.nan\n\ndef first_last(df):\n    return df.ix[[0, -1]]\n\ndf.groupby(level=0, group_keys=False).apply(first_last)\n\ndf.reset_index(1).groupby(level=0).agg(['first', 'last']).stack() \\\n    .set_index('level_1', append=True).reset_index(1, drop=True) \\\n    .rename_axis([None, None])\n"
'export LC_ALL=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\n$ source ~/.bash_profile\n'
"s1 = pd.Series(dict((v,k) for k,v in s.iteritems()))\nprint (s1)\na    A\nb    B\nc    C\nd    D\ne    E\nf    F\ng    G\nh    H\ni    I\nj    J\ndtype: object\n\nprint (pd.Series(s.index.values, index=s ))\na    A\nb    B\nc    C\nd    D\ne    E\nf    F\ng    G\nh    H\ni    I\nj    J\ndtype: object\n\nIn [63]: %timeit pd.Series(dict((v,k) for k,v in s.iteritems()))\nThe slowest run took 6.55 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 146 µs per loop\n\nIn [71]: %timeit (pd.Series(s.index.values, index=s ))\nThe slowest run took 7.42 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000 loops, best of 3: 102 µs per loop\n\ns = pd.Series(list('abcdefghij'), list('ABCDEFGHIJ'))\ns = pd.concat([s]*1000000).reset_index(drop=True)\nprint (s)\n\nIn [72]: %timeit (pd.Series(s.index, index=s ))\n10000 loops, best of 3: 106 µs per loop\n\nIn [229]: %timeit pd.Series(dict((v,k) for k,v in s.iteritems()))\n1 loop, best of 3: 1.77 s per loop\n\nIn [230]: %timeit (pd.Series(s.index, index=s ))\n10 loops, best of 3: 130 ms per loop\n\nIn [231]: %timeit (pd.Series(s.index.values, index=s ))\n10 loops, best of 3: 26.5 ms per loop\n"
"12 * (df.today.dt.year - df.date.dt.year) + (df.today.dt.month - df.date.dt.month)\n\n# 0    2\n# 1    1\n# dtype: int64\n\ndef month_diff(a, b):\n    return 12 * (a.dt.year - b.dt.year) + (a.dt.month - b.dt.month)\n\nmonth_diff(df.today, df.date)\n# 0    2\n# 1    1\n# dtype: int64\n\ndf['elapased_months'] = df.today.dt.to_period('M') - df.date.dt.to_period('M')\n\ndf\n#         date       today  elapased_months\n#0  2016-10-11  2016-12-02                2\n#1  2016-11-01  2016-12-02                1\n"
'dt = pd.to_datetime("2016-11-13 22:01:25.450")\nprint (dt)\n2016-11-13 22:01:25.450000\n\nprint (df.index.get_loc(dt, method=\'nearest\'))\n2\n\nidx = df.index[df.index.get_loc(dt, method=\'nearest\')]\nprint (idx)\n2016-11-13 22:00:28.417561344\n\n#if need select row to Series use iloc\ns = df.iloc[df.index.get_loc(dt, method=\'nearest\')]\nprint (s)\nb      1.0\nc    132.0\nName: 2016-11-13 22:00:28.417561344, dtype: float64\n'
'import pandas as pd, numpy as np\n\n### Versions: Pandas 0.20.3, Numpy 1.13.1, Python 3.6.2 ###\n\nx = pd.Series(np.random.randint(0, 100, 100000))\n\n%timeit x.apply(str)          # 42ms   (1)\n%timeit x.map(str)            # 42ms   (2)\n%timeit x.astype(str)         # 559ms  (3)\n%timeit [str(i) for i in x]   # 566ms  (4)\n%timeit list(map(str, x))     # 536ms  (5)\n%timeit x.values.astype(str)  # 25ms   (6)\n\ncpdef ndarray[object] astype_str(ndarray arr):\n    cdef:\n        Py_ssize_t i, n = arr.size\n        ndarray[object] result = np.empty(n, dtype=object)\n\n    for i in range(n):\n        # we can use the unsafe version because we know `result` is mutable\n        # since it was created from `np.empty`\n        util.set_value_at_unsafe(result, i, str(arr[i]))\n\n    return result\n'
'df = pd.DataFrame({\n    \'column\': [\'Total\',\'a\',np.nan],\n    \'B\': list(range(3))\n})\nprint (df)\n  column  B\n0  Total  0\n1      a  1\n2    NaN  2\n\ndf = df[~df["column"].str.contains("Total", na=False)]\nprint (df)\n  column  B\n1      a  1\n2    NaN  2\n'
'In [17]: prices\nOut[17]: \n              AAPL    GOOG     IBM    XOM\n2011-01-10  339.44  614.21  142.78  71.57\n2011-01-13  342.64  616.69  143.92  73.08\n2011-01-26  340.82  616.50  155.74  75.89\n2011-02-02  341.29  612.00  157.93  79.46\n2011-02-10  351.42  616.44  159.32  79.68\n2011-03-03  356.40  609.56  158.73  82.19\n2011-05-03  345.14  533.89  167.84  82.00\n2011-06-03  340.42  523.08  160.97  78.19\n2011-06-10  323.03  509.51  159.14  76.84\n2011-08-01  393.26  606.77  176.28  76.67\n2011-12-20  392.46  630.37  184.14  79.97\n\nIn [18]: orders\nOut[18]: \n                  Date direction  size ticker  prices\n0  2011-01-10 00:00:00       Buy  1500   AAPL  339.44\n1  2011-01-13 00:00:00      Sell  1500   AAPL  342.64\n2  2011-01-13 00:00:00       Buy  4000    IBM  143.92\n3  2011-01-26 00:00:00       Buy  1000   GOOG  616.50\n4  2011-02-02 00:00:00      Sell  4000    XOM   79.46\n5  2011-02-10 00:00:00       Buy  4000    XOM   79.68\n6  2011-03-03 00:00:00      Sell  1000   GOOG  609.56\n7  2011-03-03 00:00:00      Sell  2200    IBM  158.73\n8  2011-06-03 00:00:00      Sell  3300    IBM  160.97\n9  2011-05-03 00:00:00       Buy  1500    IBM  167.84\n10 2011-06-10 00:00:00       Buy  1200   AAPL  323.03\n11 2011-08-01 00:00:00       Buy    55   GOOG  606.77\n12 2011-08-01 00:00:00      Sell    55   GOOG  606.77\n13 2011-12-20 00:00:00      Sell  1200   AAPL  392.46\n\nIn [19]: prices.lookup(orders.Date, orders.ticker)\nOut[19]: \narray([ 339.44,  342.64,  143.92,  616.5 ,   79.46,   79.68,  609.56,\n        158.73,  160.97,  167.84,  323.03,  606.77,  606.77,  392.46])\n'
"df.reset_index().groupby('A')['index'].apply(np.array)\n\nIn [1]: import numpy as np\n\nIn [2]: from pandas import *\n\nIn [3]: df = DataFrame([3]*4+[4]*4+[1]*4, columns=['A'])\nIn [4]: df\nOut[4]:\n    A\n0   3\n1   3\n2   3\n3   3\n4   4\n5   4\n6   4\n7   4\n8   1\n9   1\n10  1\n11  1\n\nIn [5]: df.reset_index().groupby('A')['index'].apply(np.array)\nOut[5]:\nA\n1    [8, 9, 10, 11]\n3      [0, 1, 2, 3]\n4      [4, 5, 6, 7]\n\nIn [1]: grp = df.groupby('A')\n\nIn [2]: grp.indices\nOut[2]:\n{1L: array([ 8,  9, 10, 11], dtype=int64),\n 3L: array([0, 1, 2, 3], dtype=int64),\n 4L: array([4, 5, 6, 7], dtype=int64)}\n\nIn [3]: grp.indices[3]\nOut[3]: array([0, 1, 2, 3], dtype=int64)\n\nIn [1]: df['block'] = (df.A.shift(1) != df.A).astype(int).cumsum()\n\nIn [2]: df\nOut[2]:\n    A  block\n0   3      1\n1   3      1\n2   3      1\n3   3      1\n4   4      2\n5   4      2\n6   4      2\n7   4      2\n8   1      3\n9   1      3\n10  1      3\n11  1      3\n12  3      4\n13  3      4\n14  3      4\n15  3      4\n\nIn [77]: df.reset_index().groupby(['A','block'])['index'].apply(np.array)\nOut[77]:\nA  block\n1  3          [8, 9, 10, 11]\n3  1            [0, 1, 2, 3]\n   4        [12, 13, 14, 15]\n4  2            [4, 5, 6, 7]\n"
"In [9]: df1 = pd.DataFrame(np.random.randn(3,2),columns=list('AB'),index=pd.date_range('20000101',periods=3))\n\nIn [10]: df2 = pd.DataFrame(np.random.randn(3,2),columns=list('AB'),index=pd.date_range('20000101',periods=3))\n\nIn [11]: df1\nOut[11]: \n                   A         B\n2000-01-01  0.129994  1.189608\n2000-01-02 -1.126812  1.087617\n2000-01-03 -0.930070  0.253098\n\nIn [12]: df2\nOut[12]: \n                   A         B\n2000-01-01  0.535700 -0.769533\n2000-01-02 -1.698531 -0.456667\n2000-01-03  0.451622 -1.500175\n\nIn [13]: pd.concat(dict(df1 = df1, df2 = df2),axis=1)\nOut[13]: \n                 df1                 df2          \n                   A         B         A         B\n2000-01-01  0.129994  1.189608  0.535700 -0.769533\n2000-01-02 -1.126812  1.087617 -1.698531 -0.456667\n2000-01-03 -0.930070  0.253098  0.451622 -1.500175\n"
"In [39]: df = DataFrame(randn(5,1),columns=['value'])\n\nIn [40]: df\nOut[40]: \n      value\n0  0.092232\n1 -0.472784\n2 -1.857964\n3 -0.014385\n4  0.301531\n\nIn [41]: df.loc[df['value']&lt;0,'value'] = 0\n\nIn [42]: df\nOut[42]: \n      value\n0  0.092232\n1  0.000000\n2  0.000000\n3  0.000000\n4  0.301531\n"
"In [26]: dfA.reset_index()\nOut[26]: \n  index special_name\n0   NaN        Apple\n1   OMG       Banana\n\nIn [30]: df = dfA.reset_index().dropna().set_index('index')\n\nIn [31]: df\nOut[31]: \n      special_name\nindex             \nOMG         Banana\n"
"import pandas\npandas.set_option('expand_frame_repr', False)\n\nmydf = pandas.DataFrame.from_csv('myfile.csv', header=1)\nprint mydf.head(20) # Prints first 20 lines\n\npandas.set_option('display.max_columns', 0) # Display any number of columns\npandas.set_option('display.max_rows', 0) # Display any number of rows\n"
"ax = var.total.plot(label='Variance')\nax = shares.average.plot(secondary_y=True, label='Average Age')\n\nax.set_ylabel('Variance of log wages')\nax.right_ax.set_ylabel('Average age')\n\nlines = ax.get_lines() + ax.right_ax.get_lines()\n\nax.legend(lines, [l.get_label() for l in lines], loc='upper center')\n\nax.set_title('Wage Variance and Mean Age')\nplt.show()\n\nfig, ax = plt.subplots()\n\nax.plot(var.index.to_datetime(), var.total, 'b', label='Variance')\nax.set_ylabel('Variance of log wages')\n\nax2 = ax.twinx()\nax2.plot(shares.index.to_datetime(), shares.average, 'g' , label='Average Age')\nax2.set_ylabel('Average age')\n\nlines = ax.get_lines() + ax2.get_lines()\nax.legend(lines, [line.get_label() for line in lines], loc='upper center')\n\nax.set_title('Wage Variance and Mean Age')\nplt.show()\n"
'import pandas as pd\n\ndata1 = {"a":[1.,3.,5.,2.],\n         "b":[4.,8.,3.,7.],\n         "c":[5.,45.,67.,34]}\ndata2 = {"a":[4.],\n         "b":[2.],\n         "c":[11.]}\n\ndf1 = pd.DataFrame(data1)\ndf2 = pd.DataFrame(data2) \n\ndf1.div(df2.iloc[0], axis=\'columns\')\n'
"df.drop('c', axis=1, level=1)\n"
's &lt;- 0:4\ns %in% c(2,4)\n\nIn [13]: s = pd.Series(np.arange(5),dtype=np.float32)\n\nIn [14]: s.isin([2, 4])\nOut[14]: \n0    False\n1    False\n2     True\n3    False\n4     True\ndtype: bool\n'
"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\n#import seaborn\n#seaborn.set(style='ticks')\n\nnp.random.seed(0)\ndf = pandas.DataFrame(np.random.normal(size=(37,2)), columns=['A', 'B'])\nfig, ax = plt.subplots()\n\na_heights, a_bins = np.histogram(df['A'])\nb_heights, b_bins = np.histogram(df['B'], bins=a_bins)\n\nwidth = (a_bins[1] - a_bins[0])/3\n\nax.bar(a_bins[:-1], a_heights, width=width, facecolor='cornflowerblue')\nax.bar(b_bins[:-1]+width, b_heights, width=width, facecolor='seagreen')\n#seaborn.despine(ax=ax, offset=10)\n"
"test[1].index + pd.DateOffset(hours=16)\n\nIn [242]: pd.to_timedelta(16, unit='h')\nOut[242]: numpy.timedelta64(16,'ns')\n"
"In [36]:\n\ndf = df.convert_objects(convert_numeric=True)\ndf.dtypes\nOut[36]:\nDate         object\nWD            int64\nManpower    float64\n2nd          object\nCTR          object\n2ndU        float64\nT1            int64\nT2          int64\nT3           int64\nT4        float64\ndtype: object\n\nIn [39]:\n\ndf['2nd'] = df['2nd'].str.replace(',','').astype(int)\ndf['CTR'] = df['CTR'].str.replace('%','').astype(np.float64)\ndf.dtypes\nOut[39]:\nDate         object\nWD            int64\nManpower    float64\n2nd           int32\nCTR         float64\n2ndU        float64\nT1            int64\nT2            int64\nT3            int64\nT4           object\ndtype: object\nIn [40]:\n\ndf.head()\nOut[40]:\n        Date  WD  Manpower   2nd   CTR  2ndU   T1  \u3000\u3000T2   T3     T4\n0   2013/4/6   6       NaN  2645  5.27  0.29  407   533  454    368\n1   2013/4/7   7       NaN  2118  5.89  0.31  257   659  583    369\n2  2013/4/13   6       NaN  2470  5.38  0.29  354   531  473  \u3000\u3000383\n3  2013/4/14   7       NaN  2033  6.77  0.37  396   748  681    458\n4  2013/4/20   6       NaN  2690  5.38  0.29  361   528  541    381\n"
"cleaned = df[~df['stn'].isin(remove_list)]\n\nIn [7]:\n\nremove_list = ['Arbutus','Bayside']\ndf[~df['stn'].isin(remove_list)]\nOut[7]:\n                          stn  years_of_data  total_minutes  avg_daily  \\\ndate                                                                     \n1900-01-14  AlberniElementary              4           5745       34.1   \n1900-01-14     AlberniWeather              6           7129       29.5   \n1900-01-14          Arrowview              7          10080       27.6   \n\n            TOA_daily  K_daily  \ndate                            \n1900-01-14      114.6    0.298  \n1900-01-14      114.6    0.257  \n1900-01-14      114.6    0.241  \n"
'conda install pandas\n'
"In [5]:\nmerged = df.merge(other, how='left', indicator=True)\nmerged\n\nOut[5]:\n   col1 col2  extra_col     _merge\n0     0    a       this  left_only\n1     1    b         is       both\n2     1    c       just  left_only\n3     2    b  something  left_only\n\nIn [6]:    \nmerged[merged['_merge']=='left_only']\n\nOut[6]:\n   col1 col2  extra_col     _merge\n0     0    a       this  left_only\n2     1    c       just  left_only\n3     2    b  something  left_only\n"
"tdf['s1']\n\nIn [24]: tdf =  pd.DataFrame({'s1' : [0,1,23.4,10,23]})\n\nIn [25]: tdf['s1']\nOut[25]:\n0     0.0\n1     1.0\n2    23.4\n3    10.0\n4    23.0\nName: s1, dtype: float64\n\nIn [26]: tdf['s1'].shape\nOut[26]: (5,)\n\nIn [27]: tdf['s1'].values\nOut[27]: array([  0. ,   1. ,  23.4,  10. ,  23. ])\n"
'import numpy as np\nimport matplotlib.pyplot as plt\n\nx=range(5)\ny=np.random.randn(5)\n\n#plot1: bar\nplt.figure()\nplt.bar(x,y)\n\n#plot2: barh, wrong order\nplt.figure()\nplt.barh(x,y)\n\n#plot3: barh with correct order: top-down y axis\nplt.figure()\nplt.barh(x,y)\nplt.gca().invert_yaxis()\n\nax = df.plot.barh()  # or df.plot(), or similar\nax.invert_yaxis()\n'
"pylab.ylabel('')\n\npylab.axes().set_ylabel('')\n\nax=groups.plot(kind='pie', shadow=True)\nax.set_ylabel('')\n"
'print df.shape[1]\n2\n\nprint range(df.shape[1])\n[0, 1]\n\ndf.columns = range(df.shape[1])\nprint df\n    0   1\n0  23  12\n1  21  44\n2  98  21\n\nprint df.to_csv(header=None,index=False)\n23,12\n21,44\n98,21\n\nprint pd.read_csv(io.StringIO(u""+df.to_csv(header=None,index=False)), header=None)\n    0   1\n0  23  12\n1  21  44\n2  98  21\n\nprint df.to_csv(index=False)\nA,B\n23,12\n21,44\n98,21\n\nprint pd.read_csv(io.StringIO(u""+df.to_csv(index=False)), header=None, skiprows=1)\n    0   1\n0  23  12\n1  21  44\n2  98  21\n'
"df.loc[:, 'new_col'] = df.A.map(lambda x: x[0])\n"
"dfc['Time_of_Sail'] = pd.to_datetime(dfc['Time_of_Sail'])\ndfc['Time_of_Sail'] = [time.time() for time in dfc['Time_of_Sail']]\n\ndfc['Time_of_Sail'] = pd.to_datetime(dfc['Time_of_Sail'],format= '%H:%M:%S' ).dt.time\n"
'redisConn.set("key", df.to_msgpack(compress=\'zlib\'))\n\npd.read_msgpack(redisConn.get("key"))\n'
"from django.db import models\nfrom django_pandas.managers import DataFrameManager\n\nclass Product(models.Model):\n  product_name=models.TextField()\n  objects = models.Manager()\n  pdobjects = DataFrameManager()  # Pandas-Enabled Manager \n\nfrom models import Product\ndef ProductView(request):\n  qs = Product.pdobjects.all()  # Use the Pandas Manager\n  df = qs.to_dataframe()\n  template = 'product.html'\n\n  #Format the column headers for the Bootstrap table, they're just a list of field names, \n  #duplicated and turned into dicts like this: {'field': 'foo', 'title: 'foo'}\n  columns = [{'field': f, 'title': f} for f in Product._Meta.fields]\n  #Write the DataFrame to JSON (as easy as can be)\n  json = df.to_json(orient='records')  # output just the records (no fieldnames) as a collection of tuples\n  #Proceed to create your context object containing the columns and the data\n  context = {\n             'data': json,\n             'columns': columns\n            }\n  #And render it!\n  return render(request, template, context)\n\n&lt;script src='/path/to/bootstrap.js'&gt;\n&lt;script src='/path/to/jquery.js'&gt;\n&lt;script src='/path/to/bootstrap-table.js'&gt;\n&lt;script src='/path/to/pandas_bootstrap_table.js'&gt;\n&lt;table id='datatable'&gt;&lt;/table&gt;\n&lt;!-- Yep, all you need is a properly identified\n     but otherwise empty, table tag! --&gt;\n\n$(function() {\n  $('#datatable')({\n    striped: true,\n    pagination: true,\n    showColumns: true,\n    showToggle: true,\n    showExport: true,\n    sortable: true,\n    paginationVAlign: 'both',\n    pageSize: 25,\n    pageList: [10, 25, 50, 100, 'ALL'],\n    columns: {{ columns|safe }},  // here is where we use the column content from our Django View\n    data: {{ data|safe }}, // here is where we use the data content from our Django View. we escape the content with the safe tag so the raw JSON isn't shown.\n  });\n});\n"
'df = df.compute()\n'
"df = pd.DataFrame({'A':[1,2,3],\n                   'B':[4,5,6],\n                   'C':[7,8,9],\n                   'D':[1,3,5],\n                   'E':[5,3,6],\n                   'F':[7,4,3]})\n\nprint (df)\n   A  B  C  D  E  F\n0  1  4  7  1  5  7\n1  2  5  8  3  3  4\n2  3  6  9  5  6  3\n\nlst = ['A','R','B']\n\nprint (df.columns.intersection(lst))\nIndex(['A', 'B'], dtype='object')\n\ndata = df[df.columns.intersection(lst)]\nprint (data)\n   A  B\n0  1  4\n1  2  5\n2  3  6\n\ndata = df[np.intersect1d(df.columns, lst)]\nprint (data)\n   A  B\n0  1  4\n1  2  5\n2  3  6\n"
'&gt;&gt;&gt; df = pd.DataFrame({\'text\': ["vendor a::ProductA", "vendor b::ProductA", "vendor a::Productb"]})\n&gt;&gt;&gt; df\n                 text\n0  vendor a::ProductA\n1  vendor b::ProductA\n2  vendor a::Productb\n&gt;&gt;&gt; df[\'text_new\'] = df[\'text\'].str.split(\'::\').str[0]\n&gt;&gt;&gt; df\n                 text  text_new\n0  vendor a::ProductA  vendor a\n1  vendor b::ProductA  vendor b\n2  vendor a::Productb  vendor a\n\n&gt;&gt;&gt; df[\'text_new1\'] = [x.split(\'::\')[0] for x in df[\'text\']]\n&gt;&gt;&gt; df\n                 text  text_new text_new1\n0  vendor a::ProductA  vendor a  vendor a\n1  vendor b::ProductA  vendor b  vendor b\n2  vendor a::Productb  vendor a  vendor a\n\n# Select the pandas.Series object you want\n&gt;&gt;&gt; df[\'text\']\n0    vendor a::ProductA\n1    vendor b::ProductA\n2    vendor a::Productb\nName: text, dtype: object\n\n# using pandas.Series.str allows us to implement "normal" string methods \n# (like split) on a Series\n&gt;&gt;&gt; df[\'text\'].str\n&lt;pandas.core.strings.StringMethods object at 0x110af4e48&gt;\n\n# Now we can use the split method to split on our \'::\' string. You\'ll see that\n# a Series of lists is returned (just like what you\'d see outside of pandas)\n&gt;&gt;&gt; df[\'text\'].str.split(\'::\')\n0    [vendor a, ProductA]\n1    [vendor b, ProductA]\n2    [vendor a, Productb]\nName: text, dtype: object\n\n# using the pandas.Series.str method, again, we will be able to index through\n# the lists returned in the previous step\n&gt;&gt;&gt; df[\'text\'].str.split(\'::\').str\n&lt;pandas.core.strings.StringMethods object at 0x110b254a8&gt;\n\n# now we can grab the first item in each list above for our desired output\n&gt;&gt;&gt; df[\'text\'].str.split(\'::\').str[0]\n0    vendor a\n1    vendor b\n2    vendor a\nName: text, dtype: object\n'
"df.plot()\nplt.axis('off')\nplt.show()\nplt.close()\n\ndf.plot()\nax1 = plt.axes()\nx_axis = ax1.axes.get_xaxis()\nx_axis.set_visible(False)\nplt.show()\nplt.close()\n\ndf.plot()\nax1 = plt.axes()\nx_axis = ax1.axes.get_xaxis()\nx_axis.set_label_text('foo')\nx_label = x_axis.get_label()\n##print isinstance(x_label, matplotlib.artist.Artist)\nx_label.set_visible(False)\nplt.show()\nplt.close()\n\nax1 = plt.axes()\nx_axis = ax1.xaxis\nx_axis.set_label_text('foo')\nx_axis.label.set_visible(False)\n\nax1 = plt.axes()\nax1.xaxis.set_label_text('foo')\nax1.xaxis.label.set_visible(False)\n\naxs = df.plot()\n"
'df = pd.DataFrame([\'http://google.com\', \'http://duckduckgo.com\'])\n\ndef make_clickable(val):\n    return \'&lt;a href="{}"&gt;{}&lt;/a&gt;\'.format(val,val)\n\ndf.style.format(make_clickable)\n'
"#keep first duplicate value\ndf = df.drop_duplicates(subset=['Id'])\nprint (df)\n       Id Type\nIndex         \n0      a1    A\n1      a2    A\n2      b1    B\n3      b3    B\n\n#keep last duplicate value\ndf = df.drop_duplicates(subset=['Id'], keep='last')\nprint (df)\n       Id Type\nIndex         \n1      a2    A\n2      b1    B\n3      b3    B\n4      a1    A\n\n#remove all duplicate values\ndf = df.drop_duplicates(subset=['Id'], keep=False)\nprint (df)\n       Id Type\nIndex         \n1      a2    A\n2      b1    B\n3      b3    B\n"
'for i, row in enumerate(df.itertuples(), 1):\n    print(i, row.name)\n'
"df = pd.DataFrame({'one': {'A': 10, 'B': 20, 'C': 30, 'D': 40, 'E': 50}})\nmap_dict = {'A': 'every', 'B': 'good', 'C': 'boy', 'D': 'does', 'E': 'fine'}\n\ndf['two'] = df.index.to_series().map(map_dict)\n\ndf\n\n   one    two\nA   10  every\nB   20   good\nC   30    boy\nD   40   does\nE   50   fine\n"
'df = df.rolling("30D").sum()\n'
"df = pd.DataFrame({'Keyword': {0: 'apply', 1: 'apply', 2: 'apply', 3: 'terms', 4: 'terms'},\n 'X': {0: [1, 2], 1: [1, 2], 2: 'xy', 3: 'xx', 4: 'yy'},\n 'Y': {0: 'yy', 1: 'yy', 2: 'yx', 3: 'ix', 4: 'xi'}})\n\n#Drop directly causes the same error\ndf.drop_duplicates()\nTraceback (most recent call last):\n...\nTypeError: unhashable type: 'list'\n\n#convert hte df to str type, drop duplicates and then select the rows from original df.\n\ndf.loc[df.astype(str).drop_duplicates().index]\nOut[205]: \n  Keyword       X   Y\n0   apply  [1, 2]  yy\n2   apply      xy  yx\n3   terms      xx  ix\n4   terms      yy  xi\n\n#the list elements are still list in the final results.\ndf.loc[df.astype(str).drop_duplicates().index].loc[0,'X']\nOut[207]: [1, 2]\n"
'df = df.melt(\'X_Axis\', var_name=\'cols\', value_name=\'vals\')\n#alternative for pandas &lt; 0.20.0\n#df = pd.melt(df, \'X_Axis\', var_name=\'cols\',  value_name=\'vals\')\ng = sns.factorplot(x="X_Axis", y="vals", hue=\'cols\', data=df)\n\ndf = pd.DataFrame({\'X_Axis\':[1,3,5,7,10,20],\n                   \'col_2\':[.4,.5,.4,.5,.5,.4],\n                   \'col_3\':[.7,.8,.9,.4,.2,.3],\n                   \'col_4\':[.1,.3,.5,.7,.1,.0],\n                   \'col_5\':[.5,.3,.6,.9,.2,.4]})\n\nprint (df)\n   X_Axis  col_2  col_3  col_4  col_5\n0       1    0.4    0.7    0.1    0.5\n1       3    0.5    0.8    0.3    0.3\n2       5    0.4    0.9    0.5    0.6\n3       7    0.5    0.4    0.7    0.9\n4      10    0.5    0.2    0.1    0.2\n5      20    0.4    0.3    0.0    0.4\n\ndf = df.melt(\'X_Axis\', var_name=\'cols\',  value_name=\'vals\')\ng = sns.factorplot(x="X_Axis", y="vals", hue=\'cols\', data=df)\n\ndf = df.melt(\'X_Axis\', var_name=\'cols\',  value_name=\'vals\')\ng = sns.catplot(x="X_Axis", y="vals", hue=\'cols\', data=df, kind=\'point\')\n'
"fig = class_counts.plot(kind='bar',  figsize=(20, 16), fontsize=26).get_figure()\n\nfig.savefig('test.pdf')\n"
"query = ' &amp; '.join(['{}&gt;{}'.format(k, v) for k, v in limits_dic.items()])\n\nquery = ' &amp; '.join([f'{k}&gt;{v}' for k, v in limits_dic.items()])\n\nprint(query)\n\n'A&gt;0 &amp; C&gt;-1 &amp; B&gt;2'\n\nout = df.query(query)\nprint(out)\n\n    A  B  C\n1   2  5  2\n2  10  3  1\n4   3  6  2\n\nquery = ' &amp; '.join([f'`{k}`&gt;{v}' for k, v in limits_dic.items()])\n\nmask = df.eval(query)\nprint(mask)\n\n0    False\n1     True\n2     True\n3    False\n4     True\ndtype: bool\n\nout = df[mask]\nprint(out)\n\n    A  B  C\n1   2  5  2\n2  10  3  1\n4   3  6  2\n\ndf = pd.DataFrame({'gender':list('MMMFFF'),\n                   'height':[4,5,4,5,5,4],\n                   'age':[70,80,90,40,2,3]})\n\nprint (df)\n  gender  height  age\n0      M       4   70\n1      M       5   80\n2      M       4   90\n3      F       5   40\n4      F       5    2\n5      F       4    3\n\ncolumn = ['height', 'age', 'gender']\nequal = ['&gt;', '&gt;', '==']\ncondition = [1.68, 20, 'F']\n\nquery = ' &amp; '.join(f'{i} {j} {repr(k)}' for i, j, k in zip(column, equal, condition))\ndf.query(query)\n\n   age gender  height\n3   40      F       5\n"
'data = data.dropna(subset=[\'sms\'])\nprint (data)\n   id city department   sms  category\n1   2  lhr    revenue  good         1\n\ndata = data[data[\'sms\'].notnull()]\nprint (data)\n   id city department   sms  category\n1   2  lhr    revenue  good         1\n\nprint (data.query("sms == sms"))\n   id city department   sms  category\n1   2  lhr    revenue  good         1\n\n#[300000 rows x 5 columns]\ndata = pd.concat([data]*100000).reset_index(drop=True)\n\nIn [123]: %timeit (data.dropna(subset=[\'sms\']))\n100 loops, best of 3: 19.5 ms per loop\n\nIn [124]: %timeit (data[data[\'sms\'].notnull()])\n100 loops, best of 3: 13.8 ms per loop\n\nIn [125]: %timeit (data.query("sms == sms"))\n10 loops, best of 3: 23.6 ms per loop\n'
"df.groupby(['A','B']).size()\nOut[590]: \nA  B\nx  p    2\ny  q    1\nz  r    2\ndtype: int64\n\ndf.groupby(['A','B']).B.agg('count')\nOut[591]: \nA  B\nx  p    2\ny  q    1\nz  r    2\nName: B, dtype: int64\n\ndf.groupby(['A','B']).B.agg('count').to_frame('c').reset_index()\n\n#df.groupby(['A','B']).size().to_frame('c').reset_index()\nOut[593]: \n   A  B  c\n0  x  p  2\n1  y  q  1\n2  z  r  2\n"
'from datetime import datetime\nimport pandas as pd\nparse = lambda x: datetime.strptime(x, \'%Y%m%d %H\')\npd.read_csv("..\\\\file.csv",  parse_dates = [[\'YYYYMMDD\', \'HH\']], \n            index_col = 0, \n            date_parser=parse)\n'
'df1.sort_index(axis=1) == df2.sort_index(axis=1)\n\ndef my_equal(df1, df2):\n    from pandas.util.testing import assert_frame_equal\n    try:\n        assert_frame_equal(df1.sort_index(axis=1), df2.sort_index(axis=1), check_names=True)\n        return True\n    except (AssertionError, ValueError, TypeError):  perhaps something else?\n        return False\n'
"misc['product_desc'] = misc['product_desc'].str.replace('\\n', '')\n"
"In [104]: df = DataFrame(dict(date = [Timestamp('20130101'),Timestamp('20130131'),Timestamp('20130331'),Timestamp('20130330')],value=randn(4))).set_index('date')\n\nIn [105]: df\nOut[105]: \n               value\ndate                \n2013-01-01 -0.346980\n2013-01-31  1.954909\n2013-03-31 -0.505037\n2013-03-30  2.545073\n\nIn [106]: df.index = df.index.to_period('M').to_timestamp('M')\n\nIn [107]: df\nOut[107]: \n               value\n2013-01-31 -0.346980\n2013-01-31  1.954909\n2013-03-31 -0.505037\n2013-03-31  2.545073\n\nIn [85]: df.index + pd.offsets.MonthEnd(0) \nOut[85]: DatetimeIndex(['2013-01-31', '2013-01-31', '2013-03-31', '2013-03-31'], dtype='datetime64[ns]', name=u'date', freq=None, tz=None)\n"
"import pandas\ndf = pandas.DataFrame.from_dict(\n    {\n     'category': {0: 'Love', 1: 'Love', 2: 'Fashion', 3: 'Fashion', 4: 'Hair', 5: 'Movies', 6: 'Movies', 7: 'Health', 8: 'Health', 9: 'Celebs', 10: 'Celebs', 11: 'Travel', 12: 'Weightloss', 13: 'Diet', 14: 'Bags'}, \n     'impressions': {0: 380, 1: 374242, 2: 197, 3: 13363, 4: 4, 5: 189, 6: 60632, 7: 269, 8: 40189, 9: 138, 10: 66590, 11: 2227, 12: 22668, 13: 21707, 14: 229}, \n     'date': {0: '2013-11-04', 1: '2013-11-04', 2: '2013-11-04', 3: '2013-11-04', 4: '2013-11-04', 5: '2013-11-04', 6: '2013-11-04', 7: '2013-11-04', 8: '2013-11-04', 9: '2013-11-04', 10: '2013-11-04', 11: '2013-11-04', 12: '2013-11-04', 13: '2013-11-04', 14: '2013-11-04'}, 'cpc_cpm_revenue': {0: 0.36823, 1: 474.81522000000001, 2: 0.19434000000000001, 3: 18.264220000000002, 4: 0.00080000000000000004, 5: 0.23613000000000001, 6: 81.391139999999993, 7: 0.27171000000000001, 8: 51.258200000000002, 9: 0.11536, 10: 83.966859999999997, 11: 3.43248, 12: 31.695889999999999, 13: 28.459320000000002, 14: 0.43524000000000002}, 'clicks': {0: 0, 1: 183, 2: 0, 3: 9, 4: 0, 5: 1, 6: 20, 7: 0, 8: 21, 9: 0, 10: 32, 11: 1, 12: 12, 13: 9, 14: 2}, 'size': {0: '300x250', 1: '300x250', 2: '300x250', 3: '300x250', 4: '300x250', 5: '300x250', 6: '300x250', 7: '300x250', 8: '300x250', 9: '300x250', 10: '300x250', 11: '300x250', 12: '300x250', 13: '300x250', 14: '300x250'}\n    }\n)\ndf.set_index(['date', 'category'], inplace=True)\ndf.groupby(level=[0,1]).sum()\n"
"In [47]: df\nOut[47]: \n           A         B    C\n0   0.719391  0.091693  one\n1   0.951499  0.837160  one\n2   0.975212  0.224855  one\n3   0.807620  0.031284  one\n4   0.633190  0.342889  one\n5   0.075102  0.899291  one\n6   0.502843  0.773424  one\n7   0.032285  0.242476  one\n8   0.794938  0.607745  one\n9   0.620387  0.574222  one\n10  0.446639  0.549749  two\n11  0.664324  0.134041  two\n12  0.622217  0.505057  two\n13  0.670338  0.990870  two\n14  0.281431  0.016245  two\n15  0.675756  0.185967  two\n16  0.145147  0.045686  two\n17  0.404413  0.191482  two\n18  0.949130  0.943509  two\n19  0.164642  0.157013  two\n\nIn [48]: df.groupby('C').quantile(.95)\nOut[48]: \n            A         B\nC                      \none  0.964541  0.871332\ntwo  0.826112  0.969558\n"
"1,2,1\n2,3,4,2,3\n1,2,3,3\n1,2,3,4,5,6\n\n&gt;&gt;&gt; pd.read_csv(r'D:/Temp/tt.csv')\nTraceback (most recent call last):\n...\nExpected 5 fields in line 4, saw 6\n\n&gt;&gt;&gt; pd.read_csv(r'D:/Temp/tt.csv', names=list('abcdef'))\n   a  b  c   d   e   f\n0  1  2  1 NaN NaN NaN\n1  2  3  4   2   3 NaN\n2  1  2  3   3 NaN NaN\n3  1  2  3   4   5   6\n"
"import os\nimport pandas as pd\n\npath = os.getcwd()\nfiles = os.listdir(path)\nfiles\n\n['.DS_Store',\n '.ipynb_checkpoints',\n '.localized',\n 'Screen Shot 2013-12-28 at 7.15.45 PM.png',\n 'test1 2.xls',\n 'test1 3.xls',\n 'test1 4.xls',\n 'test1 5.xls',\n 'test1.xls',\n 'Untitled0.ipynb',\n 'Werewolf Modelling',\n '~$Random Numbers.xlsx']\n\nfiles_xls = [f for f in files if f[-3:] == 'xls']\nfiles_xls\n\n['test1 2.xls', 'test1 3.xls', 'test1 4.xls', 'test1 5.xls', 'test1.xls']\n\ndf = pd.DataFrame()\n\nfor f in files_xls:\n    data = pd.read_excel(f, 'Sheet1')\n    df = df.append(data)\n\ndf\n\n  Result  Sample\n0      a       1\n1      b       2\n2      c       3\n3      d       4\n4      e       5\n5      f       6\n6      g       7\n7      h       8\n8      i       9\n9      j      10\n0      a       1\n1      b       2\n2      c       3\n3      d       4\n4      e       5\n5      f       6\n6      g       7\n7      h       8\n8      i       9\n9      j      10\n0      a       1\n1      b       2\n2      c       3\n3      d       4\n4      e       5\n5      f       6\n6      g       7\n7      h       8\n8      i       9\n9      j      10\n0      a       1\n1      b       2\n2      c       3\n3      d       4\n4      e       5\n5      f       6\n6      g       7\n7      h       8\n8      i       9\n9      j      10\n0      a       1\n1      b       2\n2      c       3\n3      d       4\n4      e       5\n5      f       6\n6      g       7\n7      h       8\n8      i       9\n9      j      10\n"
"from matplotlib import pyplot as plt\nplt.style.use('ggplot')\n\nplt.style.use('seaborn-white')\n\nimport seaborn as sns\nsns.set()\n"
"Index([u'?Date', u'Open', u'High', u'Low', u'Close', u'Volume'], dtype='object')\n"
"In [11]: import datetime as dt\n\nIn [12]: dt.datetime.strptime('30MAR1990', '%d%b%Y')\nOut[12]: datetime.datetime(1990, 3, 30, 0, 0)\n\nIn [13]: parser = lambda date: pd.datetime.strptime(date, '%d%b%Y')\n\nIn [14]: pd.read_csv(StringIO(s), parse_dates=[0], date_parser=parser)\nOut[14]:\n        date  value\n0 1990-03-30  140000\n1 1990-06-30   30000\n2 1990-09-30  120000\n3 1990-12-30   34555\n\ndf['date'] = pd.to_datetime(df['date'], format='%d%b%Y')\n"
"In [38]: result = df.groupby(['a'], as_index=False).agg(\n                      {'c':['mean','std'],'b':'first', 'd':'first'})\n\nIn [39]: result.columns = ['a','c','e','b','d']\n\nIn [40]: result.reindex(columns=sorted(result.columns))\nOut[40]: \n        a  b    c  d         e\n0   Apple  3  4.5  7  0.707107\n1  Banana  4  4.0  8       NaN\n2  Cherry  7  1.0  3       NaN\n\ndef pop_std(x):\n    return x.std(ddof=0)\n\nresult = df.groupby(['a'], as_index=False).agg({'c':['mean',pop_std],'b':'first', 'd':'first'})\n\nresult.columns = ['a','c','e','b','d']\nresult.reindex(columns=sorted(result.columns))\n\n        a  b    c  d    e\n0   Apple  3  4.5  7  0.5\n1  Banana  4  4.0  8  0.0\n2  Cherry  7  1.0  3  0.0\n"
'test = test.reset_index(drop=True)\n'
'&gt;&gt;&gt; df = pd.DataFrame({\'Timestamp\': s.index, \'Category\': s.values})\n&gt;&gt;&gt; df\n       Category           Timestamp\n0      Facebook 2014-10-16 15:05:17\n1         Vimeo 2014-10-16 14:56:37\n2      Facebook 2014-10-16 14:25:16\n...\n\n&gt;&gt;&gt; df[\'Week/Year\'] = df[\'Timestamp\'].apply(lambda x: "%d/%d" % (x.week, x.year))\n&gt;&gt;&gt; df\n             Timestamp     Category Week/Year\n0  2014-10-16 15:05:17     Facebook   42/2014\n1  2014-10-16 14:56:37        Vimeo   42/2014\n2  2014-10-16 14:25:16     Facebook   42/2014\n...\n\n&gt;&gt;&gt; df.groupby([\'Week/Year\', \'Category\']).size()\nWeek/Year  Category   \n41/2014    DailyMotion    1\n           Facebook       3\n           Vimeo          2\n           Youtube        3\n42/2014    Facebook       7\n           Orkut          1\n           Vimeo          1\n'
"import datetime as dt\n# Change 'myday' to contains dates as datetime objects\ndf['myday'] = pd.to_datetime(df['myday'])  \n# 'daysoffset' will container the weekday, as integers\ndf['daysoffset'] = df['myday'].apply(lambda x: x.weekday())\n# We apply, row by row (axis=1) a timedelta operation\ndf['week_start'] = df.apply(lambda x: x['myday'] - dt.TimeDelta(days=x['daysoffset']), axis=1)\n"
"&gt;&gt;&gt; df.groupby('ID')[['Val1','Val2']].corr()\n\n             Val1      Val2\nID                         \nA  Val1  1.000000  0.500000\n   Val2  0.500000  1.000000\nB  Val1  1.000000  0.385727\n   Val2  0.385727  1.000000\n\n&gt;&gt;&gt; df.groupby('ID')[['Val1','Val2']].corr().iloc[0::2,-1]\n\nID       \nA   Val1    0.500000\nB   Val1    0.385727\n\ngroups = list('Val1', 'Val2', 'Val3', 'Val4')\ndf2 = pd.DataFrame()\nfor i in range( len(groups)-1): \n    df2 = df2.append( df.groupby('ID')[groups].corr().stack()\n                        .loc[:,groups[i],groups[i+1]:].reset_index() )\n\ndf2.columns = ['ID', 'v1', 'v2', 'corr']\ndf2.set_index(['ID','v1','v2']).sort_index()\n"
"for c in companies:\n     exec('{} = pd.DataFrame()'.format(c))\n"
"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(0, high=9, size=(100,2)),\n         columns = ['A', 'B'])\n\nthreshold = 10 # Anything that occurs less than this will be removed.\nvalue_counts = df.stack().value_counts() # Entire DataFrame \nto_remove = value_counts[value_counts &lt;= threshold].index\ndf.replace(to_remove, np.nan, inplace=True)\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randint(0, high=9, size=(100,2)),\n         columns = ['A', 'B'])\n\nthreshold = 10 # Anything that occurs less than this will be removed.\nfor col in df.columns:\n    value_counts = df[col].value_counts() # Specific column \n    to_remove = value_counts[value_counts &lt;= threshold].index\n    df[col].replace(to_remove, np.nan, inplace=True)\n"
"#   reset  val  desired_col\n#0      0    1            1\n#1      0    5            6\n#2      0    4           10\n#3      1    2            2\n#4      1   -1           -1\n#5      0    6            5\n#6      0    4            9\n#7      1    2            2\ndf['cumsum'] = df['reset'].cumsum()\n#cumulative sums of groups to column des\ndf['des']= df.groupby(['cumsum'])['val'].cumsum()\nprint df\n#   reset  val  desired_col  cumsum  des\n#0      0    1            1       0    1\n#1      0    5            6       0    6\n#2      0    4           10       0   10\n#3      1    2            2       1    2\n#4      1   -1           -1       2   -1\n#5      0    6            5       2    5\n#6      0    4            9       2    9\n#7      1    2            2       3    2\n#remove columns desired_col and cumsum\ndf = df.drop(['desired_col', 'cumsum'], axis=1)\nprint df\n#   reset  val  des\n#0      0    1    1\n#1      0    5    6\n#2      0    4   10\n#3      1    2    2\n#4      1   -1   -1\n#5      0    6    5\n#6      0    4    9\n#7      1    2    2\n"
"cols = ['ID_number','TimeOfDay','TypeOfCargo','TrackStart']\ngroups = df.groupby(cols)\nprint(len(groups))\n# 5353\n\n%timeit df.groupby(cols).apply(lambda group: group)\n# 1 loops, best of 3: 2.41 s per loop\n\n%timeit df.groupby(cols).apply(lambda group: 0)\n# 10 loops, best of 3: 64.3 ms per loop\n\ndef Full_coverage(group):\n    if len(group) &gt; 1:\n        group = group.sort('SectionStart', ascending=True)\n\n        # this condition is sufficient to find when the loop\n        # will add to the list\n        if np.any(group.values[1:, 4] != group.values[:-1, 5]):\n            start_km = group.iloc[0,4]\n            end_km = group.iloc[0,5]\n            end_km_index = group.index[0]\n\n            for index, (i, j) in group.iloc[1:,[4,5]].iterrows():\n                if i != end_km:\n                    incomplete_coverage.append(('Expected startpoint: '+str(end_km)+' (row '+str(end_km_index)+')', \\\n                                        'Found startpoint: '+str(i)+' (row '+str(index)+')'))                \n                start_km = i\n                end_km = j\n                end_km_index = index\n\n    return 0\n\ncols = ['ID_number','TimeOfDay','TypeOfCargo','TrackStart']\n%timeit df.groupby(cols).apply(Full_coverage)\n# 1 loops, best of 3: 1.74 s per loop\n\ndef Full_coverage_new(group):\n    if len(group) &gt; 1:\n        mask = group.values[1:, 4] != group.values[:-1, 5]\n        if np.any(mask):\n            err = ('Expected startpoint: {0} (row {1}) '\n                   'Found startpoint: {2} (row {3})')\n            incomplete_coverage.extend([err.format(group.iloc[i, 5],\n                                                   group.index[i],\n                                                   group.iloc[i + 1, 4],\n                                                   group.index[i + 1])\n                                        for i in np.where(mask)[0]])\n    return 0\n\nincomplete_coverage = []\ncols = ['ID_number','TimeOfDay','TypeOfCargo','TrackStart']\ndf_s = df.sort_values(['SectionStart','SectionStop'])\ndf_s.groupby(cols).apply(Full_coverage_nosort)\n"
"df['result'] = df['actual_credit'].ge(df['min_required_credit']) | np.isclose(df['actual_credit'], df['min_required_credit'])\n"
"import pandas as pd\ndf = pd.DataFrame({\n'donation_orgs' : [[], ['the research of Dr.']],\n'donation_context': [[], ['In lieu of flowers , memorial donations']]})\n\ndf[df.astype(str)['donation_orgs'] != '[]']\n\nOut[9]: \n                            donation_context          donation_orgs\n1  [In lieu of flowers , memorial donations]  [the research of Dr.]\n"
"from itertools import chain\ncategories = list(chain.from_iterable(categories.values))\n\nfrom functools import reduce\nfrom itertools import chain\n\ncategories = pd.Series([['a', 'b'], ['c', 'd', 'e']] * 1000)\n\n%timeit list(chain.from_iterable(categories.values))\n1000 loops, best of 3: 231 µs per loop\n\n%timeit list(chain(*categories.values.flat))\n1000 loops, best of 3: 237 µs per loop\n\n%timeit reduce(lambda l1, l2: l1 + l2, categories)\n100 loops, best of 3: 15.8 ms per loop\n"
"df['A'] = df['A'].map(addOne)\n\ndef addOne(v):\n    v['A'] += 1\n    return v\n\ndf.apply(addOne, axis=1)\n"
"df.astype(int)\n          &lt;=35  &gt;35\nCut-off            \nCalcium      0    1\nCopper       1    0\nHelium       0    8\nHydrogen     0    1\n\n&gt;&gt;&gt; df.round()\n          &lt;=35  &gt;35\nCut-off            \nCalcium      0    1\nCopper       1    0\nHelium       0    8\nHydrogen     0    1\n\n&gt;&gt;&gt; (df - .2).round()\n          &lt;=35  &gt;35\nCut-off            \nCalcium     -0    1\nCopper       1   -0\nHelium      -0    8\nHydrogen    -0    1\n\npd.set_option('precision', 0)\n\n&gt;&gt;&gt; df\n          &lt;=35  &gt;35\nCut-off            \nCalcium      0    1\nCopper       1    0\nHelium       0    8\nHydrogen     0    1 \n"
"In [79]: df = pd.DataFrame(np.random.randint(5, 15, (10, 3)), columns=list('abc'))\n\nIn [80]: df\nOut[80]:\n    a   b   c\n0   6  11  11\n1  14   7   8\n2  13   5  11\n3  13   7  11\n4  13   5   9\n5   5  11   9\n6   9   8   6\n7   5  11  10\n8   8  10  14\n9   7  14  13\n\nIn [81]: df[df.b &gt; 10]\nOut[81]:\n   a   b   c\n0  6  11  11\n5  5  11   9\n7  5  11  10\n9  7  14  13\n\nIn [82]: df[df.b &gt; 10].min()\nOut[82]:\na     5\nb    11\nc     9\ndtype: int32\n\nIn [84]: df.loc[df.b &gt; 10, 'b'].min()\nOut[84]: 11\n"
"df.index.name='ingredient'\n\ndf['ingredient']=df.index\ndf = df.reset_index(drop=True)\n"
'from itertools import islice\n\nfor index, row in islice(df.iterrows(), 1, None):\n'
"&gt;&gt;&gt; data = pd.DataFrame({'user_id' : ['a1', 'a1', 'a1', 'a2','a2','a2','a3','a3','a3'], 'product_id' : ['p1','p1','p2','p1','p1','p1','p2','p2','p3']})\n&gt;&gt;&gt; count_series = data.groupby(['user_id', 'product_id']).size()\n&gt;&gt;&gt; count_series\nuser_id  product_id\na1       p1            2\n         p2            1\na2       p1            3\na3       p2            2\n         p3            1\ndtype: int64\n&gt;&gt;&gt; new_df = count_series.to_frame(name = 'size').reset_index()\n&gt;&gt;&gt; new_df\n  user_id product_id  size\n0      a1         p1     2\n1      a1         p2     1\n2      a2         p1     3\n3      a3         p2     2\n4      a3         p3     1\n&gt;&gt;&gt; new_df['size']\n0    2\n1    1\n2    3\n3    2\n4    1\nName: size, dtype: int64\n"
"dfs = [df.set_index('id') for df in dfList]\nprint pd.concat(dfs, axis=1)\n\nfrom functools import reduce\ndf = reduce(lambda df1,df2: pd.merge(df1,df2,on='id'), dfList)\n"
"df.dropna(subset=[1, 2], how='all')\n\ndf.dropna(subset=[1, 2], thresh=1)\n"
"In [63]: df\nOut[63]:\n   1996-04  1996-05  2000-07  2000-08  2010-10  2010-11  2010-12\n0        1        2        3        4        1        1        1\n1       25       19       37       40        1        2        3\n2       10       20       30       40        4        4        5\n\nIn [64]: df.groupby(pd.PeriodIndex(df.columns, freq='Q'), axis=1).mean()\nOut[64]:\n   1996Q2  2000Q3    2010Q4\n0     1.5     3.5  1.000000\n1    22.0    38.5  2.000000\n2    15.0    35.0  4.333333\n\nIn [66]: res = (df.groupby(pd.PeriodIndex(df.columns, freq='Q'), axis=1)\n                  .mean()\n                  .rename(columns=lambda c: str(c).lower()))\n\nIn [67]: res\nOut[67]:\n   1996q2  2000q3    2010q4\n0     1.5     3.5  1.000000\n1    22.0    38.5  2.000000\n2    15.0    35.0  4.333333\n\nIn [68]: res.columns.dtype\nOut[68]: dtype('O')\n"
"df = df.groupby('source') \\\n       .agg({'text':'size', 'sent':'mean'}) \\\n       .rename(columns={'text':'count','sent':'mean_sent'}) \\\n       .reset_index()\nprint (df)\n  source  count  mean_sent\n0    bar      2      0.415\n1    foo      3     -0.500\n"
's.index[5]\n\ns.index.values[5]\n\ns.index.values[s.values.argsort()[5]]\n\ns.sort_values().index[5]\n\ns.nsmallest(6).idxmax()\n'
"df = df.sort_values(['Total Due'])\n\ndf.sort_values(['Total Due'], inplace=True)\n"
"&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; pd.Series(['abc', 'def']) + pd.Series([1, 2])\nTypeError: ufunc 'add' did not contain a loop with signature matching types dtype('&lt;U21') dtype('&lt;U21') dtype('&lt;U21')\n\ndf1['key'] = df1['Order_ID'] + '_' + df1['Date'].apply(str)  # .apply(str) is new\n"
"import pandas as pd\n\ndf1 = pd.DataFrame({\n    'A': [1,2,3,4,5],\n    'B': [1,2,3,4,5]\n})\n\ndf2 = pd.DataFrame({\n    'C': [1,2,3,4,5],\n    'D': [1,2,3,4,5]\n})\n\ndf_concat = pd.concat([df1, df2], axis=1)\n\nprint(df_concat)\n"
"aggregation_functions = {'price': 'sum', 'amount': 'sum', 'name': 'first'}\ndf_new = df.groupby(df['id']).aggregate(aggregation_functions)\n\n    price     name  amount\nid                        \n1     130     anna       3\n2      42      bob      30\n3       3  charlie     110\n"
"df = pd.DataFrame(dict(\n    A=list('XXXXYYYYYY'),\n    B=range(10)\n))\n\n   A  B\n0  X  0\n1  X  1\n2  X  2\n3  X  3\n4  Y  4\n5  Y  5\n6  Y  6\n7  Y  7\n8  Y  8\n9  Y  9\n\ns = df.groupby('A').B.pipe(lambda g: df.B / g.transform('sum') / g.ngroups)\ns\n\n0    0.000000\n1    0.083333\n2    0.166667\n3    0.250000\n4    0.051282\n5    0.064103\n6    0.076923\n7    0.089744\n8    0.102564\n9    0.115385\nName: B, dtype: float64\n\ns.sum()\n\n0.99999999999999989\n\ns.groupby(df.A).sum()\n\nA\nX    0.5\nY    0.5\nName: B, dtype: float64\n\ndf.groupby('A').B.pipe(\n    lambda g: (\n        g.get_group('X') - g.get_group('Y').mean()\n    ).append(\n        g.get_group('Y') - g.get_group('X').mean()\n    )\n)\n\n0   -6.5\n1   -5.5\n2   -4.5\n3   -3.5\n4    2.5\n5    3.5\n6    4.5\n7    5.5\n8    6.5\n9    7.5\nName: B, dtype: float64\n"
"In [4]: df\nOut[4]:\n          age   name\nstudent1   21  Marry\nstudent2   24   John\n\nIn [5]: df.dtypes\nOut[5]:\nage      int64\nname    object\ndtype: object\n\nIn [6]: df.loc['student3'] = ['old','Tom']\n   ...:\n\nIn [7]: df.dtypes\nOut[7]:\nage     object\nname    object\ndtype: object\n\nIn [11]: vals = df.values\n\nIn [12]: vals\nOut[12]:\narray([[21, 'Marry'],\n       [24, 'John'],\n       ['old', 'Tom']], dtype=object)\n\nIn [13]: vals[0,0] = 'foo'\n\nIn [14]: vals\nOut[14]:\narray([['foo', 'Marry'],\n       [24, 'John'],\n       ['old', 'Tom']], dtype=object)\n\nIn [15]: df\nOut[15]:\n          age   name\nstudent1  foo  Marry\nstudent2   24   John\nstudent3  old    Tom\n\nIn [26]: df = pd.DataFrame([{'name':'Marry', 'age':21},{'name':'John','age':24}]\n    ...: ,index=['student1','student2'])\n    ...:\n\nIn [27]: vals = df.values\n\nIn [28]: vals\nOut[28]:\narray([[21, 'Marry'],\n       [24, 'John']], dtype=object)\n\nIn [29]: vals[0,0] = 'foo'\n\nIn [30]: vals\nOut[30]:\narray([['foo', 'Marry'],\n       [24, 'John']], dtype=object)\n\nIn [31]: df\nOut[31]:\n          age   name\nstudent1   21  Marry\nstudent2   24   John\n\nIn [39]: df.loc['student3'] = ['old','Tom']\n\n\nIn [40]: df2\nOut[40]:\n          name\nstudent3   Tom\nstudent2  John\n\nIn [41]: df2.loc[:] = 'foo'\n\nIn [42]: df2\nOut[42]:\n         name\nstudent3  foo\nstudent2  foo\n\nIn [43]: df\nOut[43]:\n          age   name\nstudent1   21  Marry\nstudent2   24   John\nstudent3  old    Tom\n"
'&gt;&gt;&gt; df = pd.DataFrame(data=data, columns=[\'id\', \'birth_year\'])\n&gt;&gt;&gt; df\n   id  birth_year\n0   1      1989.0\n1   2      1990.0\n2   3         NaN\n&gt;&gt;&gt; df.birth_year\n0    1989.0\n1    1990.0\n2       NaN\nName: birth_year, dtype: float64\n&gt;&gt;&gt; df.birth_year.astype(int)\nERROR   |2018.01.29T18:14:04|default:183: Unhandled Terminal Exception\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/util/_decorators.py", line 91, in wrapper\n    return func(*args, **kwargs)\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/core/generic.py", line 3410, in astype\n    **kwargs)\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/core/internals.py", line 3224, in astype\n    return self.apply(\'astype\', dtype=dtype, **kwargs)\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/core/internals.py", line 3091, in apply\n    applied = getattr(b, f)(**kwargs)\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/core/internals.py", line 471, in astype\n    **kwargs)\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/core/internals.py", line 521, in _astype\n    values = astype_nansafe(values.ravel(), dtype, copy=True)\n  File "/usr/local/devtools/uat/anaconda4321/lib/python3.6/site-\npackages/pandas/core/dtypes/cast.py", line 620, in astype_nansafe\n    raise ValueError(\'Cannot convert non-finite values (NA or inf) to \'\nValueError: Cannot convert non-finite values (NA or inf) to integer\n\n&gt;&gt;&gt; df = df.fillna(0)\n&gt;&gt;&gt; df.birth_year.astype(int)\n0    1989\n1    1990\n2       0\nName: birth_year, dtype: int64\n'
"df.reset_index()\n\ndf.reset_index().plot(kind='scatter', x='index', y='columnA')\n"
'import pandas as pd\nimport matplotlib\nfrom pylab import title, figure, xlabel, ylabel, xticks, bar, legend, axis, savefig\nfrom fpdf import FPDF\n\n\ndf = pd.DataFrame()\ndf[\'Question\'] = ["Q1", "Q2", "Q3", "Q4"]\ndf[\'Charles\'] = [3, 4, 5, 3]\ndf[\'Mike\'] = [3, 3, 4, 4]\n\ntitle("Professor Criss\'s Ratings by Users")\nxlabel(\'Question Number\')\nylabel(\'Score\')\n\nc = [2.0, 4.0, 6.0, 8.0]\nm = [x - 0.5 for x in c]\n\nxticks(c, df[\'Question\'])\n\nbar(m, df[\'Mike\'], width=0.5, color="#91eb87", label="Mike")\nbar(c, df[\'Charles\'], width=0.5, color="#eb879c", label="Charles")\n\nlegend()\naxis([0, 10, 0, 8])\nsavefig(\'barchart.png\')\n\npdf = FPDF()\npdf.add_page()\npdf.set_xy(0, 0)\npdf.set_font(\'arial\', \'B\', 12)\npdf.cell(60)\npdf.cell(75, 10, "A Tabular and Graphical Report of Professor Criss\'s Ratings by Users Charles and Mike", 0, 2, \'C\')\npdf.cell(90, 10, " ", 0, 2, \'C\')\npdf.cell(-40)\npdf.cell(50, 10, \'Question\', 1, 0, \'C\')\npdf.cell(40, 10, \'Charles\', 1, 0, \'C\')\npdf.cell(40, 10, \'Mike\', 1, 2, \'C\')\npdf.cell(-90)\npdf.set_font(\'arial\', \'\', 12)\nfor i in range(0, len(df)):\n    pdf.cell(50, 10, \'%s\' % (df[\'Question\'].iloc[i]), 1, 0, \'C\')\n    pdf.cell(40, 10, \'%s\' % (str(df.Mike.iloc[i])), 1, 0, \'C\')\n    pdf.cell(40, 10, \'%s\' % (str(df.Charles.iloc[i])), 1, 2, \'C\')\n    pdf.cell(-90)\npdf.cell(90, 10, " ", 0, 2, \'C\')\npdf.cell(-30)\npdf.image(\'barchart.png\', x = None, y = None, w = 0, h = 0, type = \'\', link = \'\')\npdf.output(\'test.pdf\', \'F\')\n'
'df[df.eq(var1).any(1)]\n'
"def is_unique(s):\n    a = s.to_numpy() # s.values (pandas&lt;0.24)\n    return (a[0] == a).all()\n\nis_unique(df['counts'])\n# False\n\ndef unique_cols(df):\n    a = df.to_numpy() # df.values (pandas&lt;0.24)\n    return (a[0] == a).all(0)\n\nunique_cols(df)\n# array([False, False])\n\ns_num = pd.Series(np.random.randint(0, 1_000, 1_100_000))\n\nperfplot.show(\n    setup=lambda n: s_num.iloc[:int(n)], \n\n    kernels=[\n        lambda s: s.nunique() == 1,\n        lambda s: is_unique(s)\n    ],\n\n    labels=['nunique', 'first_vs_rest'],\n    n_range=[2**k for k in range(0, 20)],\n    xlabel='N'\n)\n\nfrom numba import njit\n\n@njit\ndef unique_cols_nb(a):\n    n_cols = a.shape[1]\n    out = np.zeros(n_cols, dtype=np.int32)\n    for i in range(n_cols):\n        init = a[0, i]\n        for j in a[1:, i]:\n            if j != init:\n                break\n        else:\n            out[i] = 1\n    return out\n\ndf = pd.DataFrame(np.concatenate([np.random.randint(0, 1_000, (500_000, 200)), \n                                  np.zeros((500_000, 10))], axis=1))\n\nperfplot.show(\n    setup=lambda n: df.iloc[:int(n),:], \n\n    kernels=[\n        lambda df: (df.nunique(0) == 1).values,\n        lambda df: unique_cols_nb(df.values).astype(bool),\n        lambda df: unique_cols(df) \n    ],\n\n    labels=['nunique', 'unique_cols_nb', 'unique_cols'],\n    n_range=[2**k for k in range(0, 20)],\n    xlabel='N'\n)\n"
'In [1]: import pandas as pd\nIn [2]: x = pd.Series([1, 2, 3], index=list(\'abc\'))\nIn [3]: y = pd.Series([2, 3, 3], index=list(\'bca\'))\nIn [4]: x == y\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-73b2790c1e5e&gt; in &lt;module&gt;()\n----&gt; 1 x == y\n/usr/lib/python3.7/site-packages/pandas/core/ops.py in wrapper(self, other, axis)\n   1188 \n   1189         elif isinstance(other, ABCSeries) and not self._indexed_same(othe\nr):\n-&gt; 1190             raise ValueError("Can only compare identically-labeled "\n   1191                              "Series objects")\n   1192 \nValueError: Can only compare identically-labeled Series objects\n'
'df.iloc[:,::2] = df.iloc[:,::2].ffill(1)\ndf.iloc[:,1::2] = df.iloc[:,1::2].ffill(1)\ndf\n\n   A  B  C  D  E  F\n0  3  4  3  4  3  4\n1  9  8  9  8  9  8\n2  5  9  4  7  4  7\n3  5  7  6  3  6  3\n4  2  6  4  3  4  3\n'
'df.groupby(lambda x:x, axis=1).sum()\n\ndf.groupby(df.columns, axis=1).sum()\n\ndf.groupby(df.columns, axis=1).agg(numpy.max)\n'
"In [1]: import pandas as pd\n   ...: import numpy as np\n   ...: foo = pd.DataFrame(np.random.random((10,5)))\n   ...: \n\nIn [2]: pd.__version__\nOut[2]: '0.12.0.dev-35312e4'\n\nIn [3]: np.__version__\nOut[3]: '1.7.1'\n\nIn [4]: # DataFrame has copy method\n   ...: foo_copy = foo.copy()\n\nIn [5]: bar = foo.iloc[3:5,1:4]\n\nIn [6]: bar == foo.iloc[3:5,1:4] == foo_copy.iloc[3:5,1:4]\nOut[6]: \n      1     2     3\n3  True  True  True\n4  True  True  True\n\nIn [7]: # Changing the view\n   ...: bar.ix[3,1] = 5\n\nIn [8]: # View and DataFrame still equal\n   ...: bar == foo.iloc[3:5,1:4]\nOut[8]: \n      1     2     3\n3  True  True  True\n4  True  True  True\n\nIn [9]: # It is now different from a copy of original\n   ...: bar == foo_copy.iloc[3:5,1:4]\nOut[9]: \n       1     2     3\n3  False  True  True\n4   True  True  True\n"
"df['value'].expanding().mean()\n\ns.expanding().mean()\n"
'&gt;&gt;&gt; football[football.columns[::-1]]\n   losses  wins     team  year\n0       5    11    Bears  2010\n1       8     8    Bears  2011\n2       6    10    Bears  2012\n3       1    15  Packers  2011\n4       5    11  Packers  2012\n5      10     6    Lions  2010\n6       6    10    Lions  2011\n7      12     4    Lions  2012\n\nfootball.iloc[:, ::-1]\n'
"seaborn.countplot(x='reputation', data=df)\n\nseaborn.barplot(x=df.reputation.value_counts().index, y=df.reputation.value_counts())\n"
"In [1]: import numpy as np\nIn [2]: x = np.array(['Testing', 'a', 'string'], dtype='|S7')\nIn [3]: y = np.array(['Testing', 'a', 'string'], dtype=object)\n\nIn [4]: x[1] = 'a really really really long'\nIn [5]: x\nOut[5]:\narray(['Testing', 'a reall', 'string'],\n      dtype='|S7')\n\nIn [6]: y[1] = 'a really really really long'\n\nIn [7]: y\nOut[7]: array(['Testing', 'a really really really long', 'string'], dtype=object)\n\nIn [8]: z = x.view(np.uint8)\nIn [9]: z += 1\nIn [10]: x\nOut[10]:\narray(['Uftujoh', 'b!sfbmm', 'tusjoh\\x01'],\n      dtype='|S7')\n"
'def append_df_to_excel(filename, df, sheet_name=\'Sheet1\', startrow=None,\n                       truncate_sheet=False, \n                       **to_excel_kwargs):\n    """\n    Append a DataFrame [df] to existing Excel file [filename]\n    into [sheet_name] Sheet.\n    If [filename] doesn\'t exist, then this function will create it.\n\n    Parameters:\n      filename : File path or existing ExcelWriter\n                 (Example: \'/path/to/file.xlsx\')\n      df : dataframe to save to workbook\n      sheet_name : Name of sheet which will contain DataFrame.\n                   (default: \'Sheet1\')\n      startrow : upper left cell row to dump data frame.\n                 Per default (startrow=None) calculate the last row\n                 in the existing DF and write to the next row...\n      truncate_sheet : truncate (remove and recreate) [sheet_name]\n                       before writing DataFrame to Excel file\n      to_excel_kwargs : arguments which will be passed to `DataFrame.to_excel()`\n                        [can be dictionary]\n\n    Returns: None\n    """\n    from openpyxl import load_workbook\n\n    import pandas as pd\n\n    # ignore [engine] parameter if it was passed\n    if \'engine\' in to_excel_kwargs:\n        to_excel_kwargs.pop(\'engine\')\n\n    writer = pd.ExcelWriter(filename, engine=\'openpyxl\')\n\n    # Python 2.x: define [FileNotFoundError] exception if it doesn\'t exist \n    try:\n        FileNotFoundError\n    except NameError:\n        FileNotFoundError = IOError\n\n\n    try:\n        # try to open an existing workbook\n        writer.book = load_workbook(filename)\n\n        # get the last row in the existing Excel sheet\n        # if it was not specified explicitly\n        if startrow is None and sheet_name in writer.book.sheetnames:\n            startrow = writer.book[sheet_name].max_row\n\n        # truncate sheet\n        if truncate_sheet and sheet_name in writer.book.sheetnames:\n            # index of [sheet_name] sheet\n            idx = writer.book.sheetnames.index(sheet_name)\n            # remove [sheet_name]\n            writer.book.remove(writer.book.worksheets[idx])\n            # create an empty sheet [sheet_name] using old index\n            writer.book.create_sheet(sheet_name, idx)\n\n        # copy existing sheets\n        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n    except FileNotFoundError:\n        # file does not exist yet, we will create it\n        pass\n\n    if startrow is None:\n        startrow = 0\n\n    # write out the new sheet\n    df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n\n    # save the workbook\n    writer.save()\n\nIn [48]: writer = pd.ExcelWriter(\'c:/temp/test.xlsx\', engine=\'openpyxl\')\n\nIn [49]: df.to_excel(writer, index=False)\n\nIn [50]: df.to_excel(writer, startrow=len(df)+2, index=False)\n\nIn [51]: writer.save()\n'
"In [50]: df.groupby(dr5minute.asof).agg({'Low': lambda s: s.min(), \n                                         'High': lambda s: s.max(),\n                                         'Open': lambda s: s[0],\n                                         'Close': lambda s: s[-1],\n                                         'Volume': lambda s: s.sum()})\nOut[50]: \n                      Close    High     Low    Open  Volume\nkey_0                                                      \n1999-01-04 10:20:00  1.1806  1.1819  1.1801  1.1801      34\n1999-01-04 10:25:00  1.1789  1.1815  1.1776  1.1807      91\n1999-01-04 10:30:00  1.1791  1.1792  1.1776  1.1780      16\n\n                  agg() method     agg func    agg func          agg()\n                  input type       accepts     returns           result\nGroupBy Object\nSeriesGroupBy     function         Series      value             Series\n                  dict-of-funcs    Series      value             DataFrame, columns match dict keys\n                  list-of-funcs    Series      value             DataFrame, columns match func names\nDataFrameGroupBy  function         DataFrame   Series/dict/ary   DataFrame, columns match original DataFrame\n                  dict-of-funcs    Series      value             DataFrame, columns match dict keys, where dict keys must be columns in original DataFrame\n                  list-of-funcs    Series      value             DataFrame, MultiIndex columns (original cols x func names)\n\ndef ohlcsum(df):\n    df = df.sort()\n    return {\n       'Open': df['Open'][0],\n       'High': df['High'].max(),\n       'Low': df['Low'].min(),\n       'Close': df['Close'][-1],\n       'Volume': df['Volume'].sum()\n      }\n\nIn [30]: df.groupby(dr5minute.asof).agg(ohlcsum)\nOut[30]: \n                       Open    High     Low   Close  Volume\nkey_0                                                      \n1999-01-04 10:20:00  1.1801  1.1819  1.1801  1.1806      34\n1999-01-04 10:25:00  1.1807  1.1815  1.1776  1.1789      91\n1999-01-04 10:30:00  1.1780  1.1792  1.1776  1.1791      16\n"
"data[data.groupby('tag').pid.transform(len) &gt; 1]\n\nimport pandas\nimport numpy as np\ndata = pandas.DataFrame(\n    {'pid' : [1,1,1,2,2,3,3,3],\n     'tag' : [23,45,62,24,45,34,25,62],\n     })\n\nbytag = data.groupby('tag').aggregate(np.count_nonzero)\ntags = bytag[bytag.pid &gt;= 2].index\nprint(data[data['tag'].isin(tags)])\n\n   pid  tag\n1    1   45\n2    1   62\n4    2   45\n7    3   62\n"
"sample.resample('60Min', how=conversion, base=30)\n"
"In [63]: df.xs(('B',), level='Alpha')\nOut[63]:\n                  I        II       III        IV         V        VI       VII\nInt Bool                                                                       \n0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n    False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   True  -0.450375  1.237018  0.398290  0.246182 -0.237919  1.372239 -0.805403\n    False -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n    False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494\n\nIn [64]: df.xs(('B', False), level=('Alpha', 'Bool'))\nOut[64]:\n            I        II       III        IV         V        VI       VII\nInt                                                                      \n0    0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n1   -0.064493  0.967132 -0.674451  0.666691 -0.350378  1.721682 -0.791897\n2   -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n\nIn [87]: ix_vals = set(i for _, i, _ in df.index if i % 2 == 0)\n         ix_vals\n\nOut[87]: set([0L, 2L])\n\nIn [89]: ix = df.index.get_level_values('Int').isin(ix_vals)\nIn [90]: df[ix]\nOut[90]:                I        II       III        IV         V        VI       VII\nAlpha Int Bool                                                                       \nA     0   True  -1.315409  1.203800  0.330372 -0.295718 -0.679039  1.402114  0.778572\n          False  0.008189 -0.104372  0.419110  0.302978 -0.880262 -1.037645 -0.264265\n      2   True  -2.414290  0.896990  0.986167 -0.527074  0.550753 -0.302920  0.228165\n          False  1.275831  0.448089 -0.635874 -0.733855 -0.747774 -1.108976  0.151474\nB     0   True  -0.430563  0.139969 -0.356883 -0.574463 -0.107693 -1.030063  0.271250\n          False  0.334960 -0.640764 -0.515756 -0.327806 -0.006574  0.183520  1.397951\n      2   True   0.143154 -0.061543 -1.157361  0.864847 -0.379616 -0.762626  0.645582\n          False -3.253589  0.729562 -0.839622 -1.088309  0.039522  0.980831 -0.113494 \n"
"&gt;&gt;&gt; df\n   x   y\n0  4  GE\n1  1  RE\n2  1  AE\n3  4  CD\n&gt;&gt;&gt; df.values\narray([[4, 'GE'],\n       [1, 'RE'],\n       [1, 'AE'],\n       [4, 'CD']], dtype=object)\n\n&gt;&gt;&gt; df.values is df.values\nFalse\n\n&gt;&gt;&gt; df1 = pd.DataFrame([[1, 2], [3, 4]])\n&gt;&gt;&gt; df2 = pd.DataFrame(df1)\n&gt;&gt;&gt; df2.iloc[0,0] = 42\n&gt;&gt;&gt; df1\n    0  1\n0  42  2\n1   3  4\n\n&gt;&gt;&gt; df1 = pd.DataFrame([[1, 2], [3, 4]])\n&gt;&gt;&gt; df2 = pd.DataFrame(df1, copy=True)\n&gt;&gt;&gt; df2.iloc[0,0] = 42\n&gt;&gt;&gt; df1\n   0  1\n0  1  2\n1  3  4\n"
'df.groupby(["a", "name"]).median().index.get_level_values(1)\n\nOut[2]:\n\nIndex([u\'hello\', u\'foo\'], dtype=object)\n\ndf.groupby(["a", "name"]).median().index.get_level_values(\'name\')\n\ndf.groupby(["a", "name"]).median().index.get_level_values(1).tolist()\n\nOut[5]:\n\n[\'hello\', \'foo\']\n'
"In [11]: df.loc[df.cherry == 'bad', ['apple', 'banana']] = np.nan\n\nIn [12]: df\nOut[12]: \n   apple  banana cherry\n0      0       3   good\n1    NaN     NaN    bad\n2      2       5   good\n"
"# I'm ignoring that the index is defaulting to a sequential number. You\n# would need to explicitly assign your IDs to the index here, e.g.:\n# &gt;&gt;&gt; l_series.index = ID_list\nmil = range(1000000)\nl = mil\nl_series = pd.Series(l)\n\ndf = pd.DataFrame(l_series, columns=['ID'])\n\n\nIn [247]: %timeit df[df.index.isin(l)]\n1 loops, best of 3: 1.12 s per loop\n\nIn [248]: %timeit df[df.index.isin(l_series)]\n1 loops, best of 3: 549 ms per loop\n\n# index vs column doesn't make a difference here\nIn [304]: %timeit df[df.ID.isin(l_series)]\n1 loops, best of 3: 541 ms per loop\n\nIn [305]: %timeit df[df.index.isin(l_series)]\n1 loops, best of 3: 529 ms per loop\n\n# query 'in' syntax has the same performance as 'isin'\nIn [249]: %timeit df.query('index in @l')\n1 loops, best of 3: 1.14 s per loop\n\nIn [250]: %timeit df.query('index in @l_series')\n1 loops, best of 3: 564 ms per loop\n\n# ID must be the index for DataFrame.join and l_series must have a name.\n# join defaults to a left join so we need to specify inner for existence.\nIn [251]: %timeit df.join(l_series, how='inner')\n10 loops, best of 3: 93.3 ms per loop\n\n# Smaller datasets.\ndf = pd.DataFrame([1,2,3,4], columns=['ID'])\nl = range(10000)\nl_dict = dict(zip(l, l))\nl_series = pd.Series(l)\nl_series.name = 'ID_list'\n\n\nIn [363]: %timeit df.join(l_series, how='inner')\n1000 loops, best of 3: 733 µs per loop\n\nIn [291]: %timeit df[df.ID.isin(l_dict)]\n1000 loops, best of 3: 742 µs per loop\n\nIn [292]: %timeit df[df.ID.isin(l)]\n1000 loops, best of 3: 771 µs per loop\n\nIn [294]: %timeit df[df.ID.isin(l_series)]\n100 loops, best of 3: 2 ms per loop\n\n# It's actually faster to use apply or a list comprehension for these small cases.\nIn [296]: %timeit df[[x in l_dict for x in df.ID]]\n1000 loops, best of 3: 203 µs per loop\n\nIn [299]: %timeit df[df.ID.apply(lambda x: x in l_dict)]\n1000 loops, best of 3: 297 µs per loop\n"
"In [12]: df.groupby('group')['value'].rank(ascending=False)\nOut[12]: \n0    4\n1    1\n2    3\n3    2\n4    3\n5    2\n6    1\n7    4\ndtype: float64\n"
" df.sort_index(by='col1', ascending=False)\n\n             col1\nidx1    idx2    \n1       C    105\n        B    102\n        A    99\n        D    97\n2       A    19\n        D    17\n        B    14\n        C    10\n"
'def foo():\n    f = open("myfile.csv", "w")\n    ...\n    f.close()  # isn\'t actually needed\n\ndef foo():\n    with open("myfile.csv", "w") as f:\n        ...\n'
"new_df = pd.DataFrame(list(original['user']))\n"
"import pandas as pd\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\ndr = pd.date_range(start='2015-07-01', end='2015-07-31')\ndf = pd.DataFrame()\ndf['Date'] = dr\n\ncal = calendar()\nholidays = cal.holidays(start=dr.min(), end=dr.max())\n\ndf['Holiday'] = df['Date'].isin(holidays)\nprint df\n\n         Date Holiday\n0  2015-07-01   False\n1  2015-07-02   False\n2  2015-07-03    True\n3  2015-07-04   False\n4  2015-07-05   False\n5  2015-07-06   False\n6  2015-07-07   False\n7  2015-07-08   False\n8  2015-07-09   False\n9  2015-07-10   False\n10 2015-07-11   False\n11 2015-07-12   False\n12 2015-07-13   False\n13 2015-07-14   False\n14 2015-07-15   False\n15 2015-07-16   False\n16 2015-07-17   False\n17 2015-07-18   False\n18 2015-07-19   False\n19 2015-07-20   False\n20 2015-07-21   False\n21 2015-07-22   False\n22 2015-07-23   False\n23 2015-07-24   False\n24 2015-07-25   False\n25 2015-07-26   False\n26 2015-07-27   False\n27 2015-07-28   False\n28 2015-07-29   False\n29 2015-07-30   False\n30 2015-07-31   False\n"
"wanted_data.loc[:, 'age'] = wanted_data.age.apply(lambda x: x + 1)\n"
"In [5]:\ndf.sort_values('B').groupby('A').first()\n\nOut[5]:\n     B\nA     \nbar  1\nfoo  1\n"
'data[\'sex\'].replace(0, \'Female\',inplace=True)\ndata[\'sex\'].replace(1, \'Male\',inplace=True)\n\ndata[\'sex\'].replace([0,1],[\'Female\',\'Male\'],inplace=True)\n\nIn [10]: data = pd.DataFrame([[1,0],[0,1],[1,0],[0,1]], columns=["sex", "split"])\n\nIn [11]: data[\'sex\'].replace([0,1],[\'Female\',\'Male\'],inplace=True)\n\nIn [12]: data\nOut[12]:\n      sex  split\n0    Male      0\n1  Female      1\n2    Male      0\n3  Female      1\n\nIn [15]: data = pd.DataFrame([[1,0],[0,1],[1,0],[0,1]], columns=["sex", "split"])\n\nIn [16]: data[\'sex\'].replace({0:\'Female\',1:\'Male\'},inplace=True)\n\nIn [17]: data\nOut[17]:\n      sex  split\n0    Male      0\n1  Female      1\n2    Male      0\n3  Female      1\n'
"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot([1,2], [1,2])\n\n# Option 1\n# QT backend\nmanager = plt.get_current_fig_manager()\nmanager.window.showMaximized()\n\n# Option 2\n# TkAgg backend\nmanager = plt.get_current_fig_manager()\nmanager.resize(*manager.window.maxsize())\n\n# Option 3\n# WX backend\nmanager = plt.get_current_fig_manager()\nmanager.frame.Maximize(True)\n\nplt.show()\nplt.savefig('sampleFileName.png')\n"
'df = pd.concat([df1, df2[~df2.index.isin(df1.index)]])\ndf.update(df2)\n\n&gt;&gt;&gt; df\n             A   B\n2015-10-01  A1  B1\n2015-10-02  a1  b1\n2015-10-03  a2  b2\n2015-10-04  a3  b3\n\npd.concat([df1[~df1.index.isin(df2.index)], df2])\n'
"&gt;&gt;&gt; keys = [c for c in df if c.startswith('key.')]\n&gt;&gt;&gt; pd.melt(df, id_vars='topic', value_vars=keys, value_name='key')\n\n   topic variable  key\n0      8    key.0  abc\n1      9    key.0  xab\n2      8    key.1  def\n3      9    key.1  xcd\n4      8    key.2  ghi\n5      9    key.2  xef\n\n&gt;&gt;&gt; df.melt('topic', value_name='key').drop('variable', 1)\n\n   topic  key\n0      8  abc\n1      9  xab\n2      8  def\n3      9  xcd\n4      8  ghi\n5      9  xef\n"
"import pandas as pd\n\ndf = pd.read_csv('http://pastebin.com/raw/6xbjvEL0')\n\ndf.month = pd.to_datetime(df.month, unit='s')\ndf = df.set_index('month')\n\ndf = df.sort_index()\n\ndf['percentile'] = df.groupby(df.index)['ratio_cost'].rank(pct=True)\nprint df['percentile'].head()\n\nmonth\n2010-08-01    0.2500\n2010-08-01    0.6875\n2010-08-01    0.6250\n2010-08-01    0.9375\n2010-08-01    0.7500\nName: percentile, dtype: float64\n"
"print df\n    hours  $\n0       0  8\n1       0  9\n2       0  9\n3       3  6\n4       6  4\n5       3  7\n6       5  5\n7      10  1\n8       9  3\n9       3  6\n10      5  4\n11      5  7\n\ndf['$/hour'] = np.where(df['hours'] &lt; 1, df['hours'], df['$']/df['hours'])\nprint df\n    hours  $    $/hour\n0       0  8  0.000000\n1       0  9  0.000000\n2       0  9  0.000000\n3       3  6  2.000000\n4       6  4  0.666667\n5       3  7  2.333333\n6       5  5  1.000000\n7      10  1  0.100000\n8       9  3  0.333333\n9       3  6  2.000000\n10      5  4  0.800000\n11      5  7  1.400000\n"
'In [8]: df\nOut[8]:\n  COL1 COL2\n0    A  NaN\n1  NaN    B\n2    A    B\n\nIn [9]: df["COL3"] = df["COL1"].fillna(df["COL2"])\n\nIn [10]: df\nOut[10]:\n  COL1 COL2 COL3\n0    A  NaN    A\n1  NaN    B    B\n2    A    B    A\n'
"In [123]:\ng = df.groupby('c')['type'].value_counts().reset_index(name='t')\ng['size'] = df.groupby('c')['type'].transform('size')\ng\n\nOut[123]:\n   c type  t  size\n0  1    m  1     3\n1  1    n  1     3\n2  1    o  1     3\n3  2    m  2     4\n4  2    n  2     4\n"
"print df.loc[df['Col1'].isnull(),['Col1','Col2', 'Col3']]\n\n  Col1 Col2 Col3\n2  NaN  NaN  NaN\n3  NaN  NaN  NaN\n\nreplace_with_this = df.loc[df['Col1'].isnull(),['col1_v2','col2_v2', 'col3_v2']]\nprint replace_with_this\n\n  col1_v2 col2_v2 col3_v2\n2       a       b       d\n3       d       e       f\n\ndf.loc[df['Col1'].isnull(),['Col1','Col2', 'Col3']] = replace_with_this.values\n\nprint df\n\n  Col1 Col2 Col3 col1_v2 col2_v2 col3_v2\n0    A    B    C     NaN     NaN     NaN\n1    D    E    F     NaN     NaN     NaN\n2    a    b    d       a       b       d\n3    d    e    f       d       e       f\n"
'from IPython.display import display\ndisplay(d)\n'
'import pandas as pd\n\ndf = pd.DataFrame( {"name" : ["John", "Eric"], \n               "days" : [[1, 3, 5, 7], [2,4]]})\nresult = pd.DataFrame([(d, tup.name) for tup in df.itertuples() for d in tup.days])\nprint(result)\n\n   0     1\n0  1  John\n1  3  John\n2  5  John\n3  7  John\n4  2  Eric\n5  4  Eric\n\nIn [48]: %timeit using_repeat(df)\n1000 loops, best of 3: 834 µs per loop\n\nIn [5]: %timeit using_itertuples(df)\n100 loops, best of 3: 3.43 ms per loop\n\nIn [7]: %timeit using_apply(df)\n1 loop, best of 3: 379 ms per loop\n\nIn [8]: %timeit using_append(df)\n1 loop, best of 3: 3.59 s per loop\n\nimport numpy as np\nimport pandas as pd\n\nN = 10**3\ndf = pd.DataFrame( {"name" : np.random.choice(list(\'ABCD\'), size=N), \n                    "days" : [np.random.randint(10, size=np.random.randint(5))\n                              for i in range(N)]})\n\ndef using_itertuples(df):\n    return  pd.DataFrame([(d, tup.name) for tup in df.itertuples() for d in tup.days])\n\ndef using_repeat(df):\n    lens = [len(item) for item in df[\'days\']]\n    return pd.DataFrame( {"name" : np.repeat(df[\'name\'].values,lens), \n                          "days" : np.concatenate(df[\'days\'].values)})\n\ndef using_apply(df):\n    return (df.apply(lambda x: pd.Series(x.days), axis=1)\n            .stack()\n            .reset_index(level=1, drop=1)\n            .to_frame(\'day\')\n            .join(df[\'name\']))\n\ndef using_append(df):\n    df2 = pd.DataFrame(columns = df.columns)\n    for i,r in df.iterrows():\n        for e in r.days:\n            new_r = r.copy()\n            new_r.days = e\n            df2 = df2.append(new_r)\n    return df2\n'
"scaled_features = data.copy()\n\ncol_names = ['Age', 'Weight']\nfeatures = scaled_features[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n\nscaled_features[col_names] = features\nprint(scaled_features)\n\n\n        Age  Name    Weight\n0 -1.411004     3  1.202703\n1  0.623041     4  0.042954\n2  0.787964     6 -1.245657\n"
'In [138]: df.Close.pct_change() * 100\nOut[138]:\n0         NaN\n1    0.469484\n2    0.467290\n3   -0.930233\n4    0.469484\n5    0.467290\n6    0.000000\n7   -3.255814\n8   -3.365385\n9   -0.497512\nName: Close, dtype: float64\n\nIn [139]: df.Close.diff()\nOut[139]:\n0      NaN\n1    0.125\n2    0.125\n3   -0.250\n4    0.125\n5    0.125\n6    0.000\n7   -0.875\n8   -0.875\n9   -0.125\nName: Close, dtype: float64\n'
'import warnings\nwarnings.filterwarnings("ignore", \'This pattern has match groups\')\n\n# import warnings\n# warnings.filterwarnings("ignore", \'This pattern has match groups\') # uncomment to suppress the UserWarning\n\nimport pandas as pd\n\ndf = pd.DataFrame({ \'event_time\': [\'gouda\', \'stilton\', \'gruyere\']})\n\nurls = pd.DataFrame({\'url\': [\'g(.*)\']})   # With a capturing group, there is a UserWarning\n# urls = pd.DataFrame({\'url\': [\'g.*\']})   # Without a capturing group, there is no UserWarning. Uncommenting this line avoids the UserWarning.\n\nsubstr = urls.url.values.tolist()\ndf[df[\'event_time\'].str.contains(\'|\'.join(substr), regex=True)]\n\n  script.py:10: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n  df[df[\'event_time\'].str.contains(\'|\'.join(substr), regex=True)]\n\nurls = pd.DataFrame({\'url\': [\'g.*\']})   \n'
"df_dict = pandas.read_excel('ExcelFile.xlsx', header=[0, 1], sheetname=None)\n\ndf = pandas.concat(df_dict.values(), axis=0)\n"
'In [13]: df\nOut[13]: \n              a             b             c\n0  4.405544e+08  1.425305e+08  6.387200e+08\n1  8.792502e+08  7.135909e+08  4.652605e+07\n2  5.074937e+08  3.008761e+08  1.781351e+08\n3  1.188494e+07  7.926714e+08  9.485948e+08\n4  6.071372e+08  3.236949e+08  4.464244e+08\n5  1.744240e+08  4.062852e+08  4.456160e+08\n6  7.622656e+07  9.790510e+08  7.587101e+08\n7  8.762620e+08  1.298574e+08  4.487193e+08\n8  6.262644e+08  4.648143e+08  5.947500e+08\n9  5.951188e+08  9.744804e+08  8.572475e+08\n\nIn [14]: pd.set_option(\'float_format\', \'{:f}\'.format)\n\nIn [15]: df\nOut[15]: \n                 a                b                c\n0 440554429.333866 142530512.999182 638719977.824965\n1 879250168.522411 713590875.479215  46526045.819487\n2 507493741.709532 300876106.387427 178135140.583541\n3  11884941.851962 792671390.499431 948594814.816647\n4 607137206.305609 323694879.619369 446424361.522071\n5 174424035.448168 406285189.907148 445616045.754137\n6  76226556.685384 979050957.963583 758710090.127867\n7 876261954.607558 129857447.076183 448719292.453509\n8 626264394.999419 464814260.796770 594750038.747595\n9 595118819.308896 974480400.272515 857247528.610996\n\nIn [16]: df.describe()\nOut[16]: \n                     a                b                c\ncount        10.000000        10.000000        10.000000\nmean  479461624.877280 522785202.100082 536344333.626082\nstd   306428177.277935 320806568.078629 284507176.411675\nmin    11884941.851962 129857447.076183  46526045.819487\n25%   240956633.919592 306580799.695412 445818124.696121\n50%   551306280.509214 435549725.351959 521734665.600552\n75%   621482597.825966 772901261.744377 728712562.052142\nmax   879250168.522411 979050957.963583 948594814.816647\n\nIn [7]: df\nOut[7]: \n              a             b             c\n0  4.405544e+08  1.425305e+08  6.387200e+08\n1  8.792502e+08  7.135909e+08  4.652605e+07\n2  5.074937e+08  3.008761e+08  1.781351e+08\n3  1.188494e+07  7.926714e+08  9.485948e+08\n4  6.071372e+08  3.236949e+08  4.464244e+08\n5  1.744240e+08  4.062852e+08  4.456160e+08\n6  7.622656e+07  9.790510e+08  7.587101e+08\n7  8.762620e+08  1.298574e+08  4.487193e+08\n8  6.262644e+08  4.648143e+08  5.947500e+08\n9  5.951188e+08  9.744804e+08  8.572475e+08\n\nIn [8]: df.describe()\nOut[8]: \n                  a             b             c\ncount  1.000000e+01  1.000000e+01  1.000000e+01\nmean   4.794616e+08  5.227852e+08  5.363443e+08\nstd    3.064282e+08  3.208066e+08  2.845072e+08\nmin    1.188494e+07  1.298574e+08  4.652605e+07\n25%    2.409566e+08  3.065808e+08  4.458181e+08\n50%    5.513063e+08  4.355497e+08  5.217347e+08\n75%    6.214826e+08  7.729013e+08  7.287126e+08\nmax    8.792502e+08  9.790510e+08  9.485948e+08\n\nIn [29]: pd.options.display.float_format = "{:.2f}".format\n\nIn [10]: df\nOut[10]: \n             a            b            c\n0 440554429.33 142530513.00 638719977.82\n1 879250168.52 713590875.48  46526045.82\n2 507493741.71 300876106.39 178135140.58\n3  11884941.85 792671390.50 948594814.82\n4 607137206.31 323694879.62 446424361.52\n5 174424035.45 406285189.91 445616045.75\n6  76226556.69 979050957.96 758710090.13\n7 876261954.61 129857447.08 448719292.45\n8 626264395.00 464814260.80 594750038.75\n9 595118819.31 974480400.27 857247528.61\n\nIn [11]: df.describe()\nOut[11]: \n                 a            b            c\ncount        10.00        10.00        10.00\nmean  479461624.88 522785202.10 536344333.63\nstd   306428177.28 320806568.08 284507176.41\nmin    11884941.85 129857447.08  46526045.82\n25%   240956633.92 306580799.70 445818124.70\n50%   551306280.51 435549725.35 521734665.60\n75%   621482597.83 772901261.74 728712562.05\nmax   879250168.52 979050957.96 948594814.82\n'
"print (pd.merge(df1, df2, left_on='id', right_on='id1', how='left').drop('id1', axis=1))\n   id name  count  price  rating\n0   1    a     10  100.0     1.0\n1   2    b     20  200.0     2.0\n2   3    c     30  300.0     3.0\n3   4    d     40    NaN     NaN\n4   5    e     50  500.0     5.0\n\nprint (pd.merge(df1, df2.rename(columns={'id1':'id'}), on='id',  how='left'))\n   id name  count  price  rating\n0   1    a     10  100.0     1.0\n1   2    b     20  200.0     2.0\n2   3    c     30  300.0     3.0\n3   4    d     40    NaN     NaN\n4   5    e     50  500.0     5.0\n\ndf1['price'] = df1.id.map(df2.set_index('id1')['price'])\nprint (df1)\n   id name  count  price\n0   1    a     10  100.0\n1   2    b     20  200.0\n2   3    c     30  300.0\n3   4    d     40    NaN\n4   5    e     50  500.0\n\nprint (pd.merge(df1, df2, left_on='id', right_on='id1', how='left')\n         .drop(['id1', 'rating'], axis=1))\n   id name  count  price\n0   1    a     10  100.0\n1   2    b     20  200.0\n2   3    c     30  300.0\n3   4    d     40    NaN\n4   5    e     50  500.0\n\nprint (pd.merge(df1, df2[['id1','price']], left_on='id', right_on='id1', how='left')\n         .drop('id1', axis=1))\n   id name  count  price\n0   1    a     10  100.0\n1   2    b     20  200.0\n2   3    c     30  300.0\n3   4    d     40    NaN\n4   5    e     50  500.0\n"
"result = df.groupby(['ID']).agg({'TIME': 'mean', 'ID': 'count'}).reset_index(drop=True)\nprint (result)\n   ID      TIME\n0   3  2.666667\n1   1  3.000000\n\nresult = df.groupby(['ID']).agg({'TIME': 'mean', 'ID': 'count'})\n           .rename(columns={'ID':'COUNT','TIME':'MEAN_TIME'})\n           .reset_index()\nprint (result)\n   ID  COUNT  MEAN_TIME\n0   1      3   2.666667\n1   2      1   3.000000\n\nresult = df.groupby(['ID']).agg({'TIME':{'MEAN_TIME': 'mean'}, 'ID': {'COUNT': 'count'}})\nresult.columns = result.columns.droplevel(0)\nprint (result.reset_index())\n   ID  COUNT  MEAN_TIME\n0   1      3   2.666667\n1   2      1   3.000000\n"
"group_vars = ['a','b']\ndf.merge( df.drop_duplicates( group_vars ).reset_index(), on=group_vars )\n\n   a  b  index\n0  1  1      0\n1  1  1      0\n2  1  2      2\n3  2  1      3\n4  2  1      3\n5  2  2      5\n"
"print (df.set_index('fruits').T)\nfruits     apples  grapes  figs\nnumFruits      10      20    15\n\nprint (df.rename(columns={'numFruits':'Market 1 Order'})\n         .set_index('fruits')\n         .rename_axis(None).T)\n                apples  grapes  figs\nMarket 1 Order      10      20    15\n\nprint (pd.DataFrame(df.numFruits.values.reshape(1,-1), \n                    index=['Market 1 Order'], \n                    columns=df.fruits.values))\n\n                apples  grapes  figs\nMarket 1 Order      10      20    15\n\n#[30000 rows x 2 columns] \ndf = pd.concat([df]*10000).reset_index(drop=True)    \nprint (df)\n\n\nIn [55]: %timeit (pd.DataFrame([df.numFruits.values], ['Market 1 Order'], df.fruits.values))\n1 loop, best of 3: 2.4 s per loop\n\nIn [56]: %timeit (pd.DataFrame(df.numFruits.values.reshape(1,-1), index=['Market 1 Order'], columns=df.fruits.values))\nThe slowest run took 5.64 times longer than the fastest. This could mean that an intermediate result is being cached.\n1000 loops, best of 3: 424 µs per loop\n\nIn [57]: %timeit (df.rename(columns={'numFruits':'Market 1 Order'}).set_index('fruits').rename_axis(None).T)\n100 loops, best of 3: 1.94 ms per loop\n"
"df.groupby(pd.Grouper(freq='60Min', base=30, label='right')).first()\n# same thing using resample - df.resample('60Min', base=30, label='right').first()\n\n                           data\nindex                          \n2017-02-14 06:30:00  11198648.0\n2017-02-14 07:30:00  11198650.0\n2017-02-14 08:30:00         NaN\n2017-02-14 09:30:00         NaN\n2017-02-14 10:30:00         NaN\n2017-02-14 11:30:00         NaN\n2017-02-14 12:30:00         NaN\n2017-02-14 13:30:00         NaN\n2017-02-14 14:30:00         NaN\n2017-02-14 15:30:00         NaN\n2017-02-14 16:30:00         NaN\n2017-02-14 17:30:00         NaN\n2017-02-14 18:30:00         NaN\n2017-02-14 19:30:00         NaN\n2017-02-14 20:30:00         NaN\n2017-02-14 21:30:00         NaN\n2017-02-14 22:30:00         NaN\n2017-02-14 23:30:00  11207728.0\n"
'df = pd.DataFrame(np.array(my_list).reshape(3,3), columns = list("abc"))\nprint (df)\n   a  b  c\n0  1  2  3\n1  4  5  6\n2  7  8  9\n'
"x = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n\n q = x.loc[:, 'a']\n\nq += 2\nprint(x)  # checking x again, wow! it changed!\n   a  b\n0  2  0\n1  3  1\n2  4  2\n3  5  3\n\nx = pd.DataFrame(list(zip(range(4), range(4))), columns=['a', 'b'])\nprint(x)\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n\nq = x.loc[:, 'a'].copy()\nq += 2\nprint(x)  # oh, x did not change because q is a copy now\n   a  b\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n"
'pool.submit(myfunc(folder), 1000)\n'
'In [202]: df.dropna(subset=[\'Col2\'])\nOut[202]:\n   Col1  Col2  Col3\n1     2   5.0   4.0\n2     3   3.0   NaN\n\nIn [204]: df.loc[df.Col2.notnull()]\nOut[204]:\n   Col1  Col2  Col3\n1     2   5.0   4.0\n2     3   3.0   NaN\n\nIn [205]: df.query("Col2 == Col2")\nOut[205]:\n   Col1  Col2  Col3\n1     2   5.0   4.0\n2     3   3.0   NaN\n\nIn [241]: import numexpr as ne\n\nIn [242]: col = df.Col2\n\nIn [243]: df[ne.evaluate("col == col")]\nOut[243]:\n   Col1  Col2  Col3\n1     2   5.0   4.0\n2     3   3.0   NaN\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.random.seed(444)\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': np.random.choice(['x', 'y'], size=50),\n                       'b': np.random.rand(50)},\n                      index=pd.date_range('2010', periods=50))\n&gt;&gt;&gt; df.head()\n            a         b\n2010-01-01  y  0.959568\n2010-01-02  x  0.784837\n2010-01-03  y  0.745148\n2010-01-04  x  0.965686\n2010-01-05  y  0.654552\n\n&gt;&gt;&gt; # `a` is dropped because it is non-numeric\n&gt;&gt;&gt; df.groupby(pd.Grouper(freq='M')).sum()\n                  b\n2010-01-31  18.5123\n2010-02-28   7.7670\n\n&gt;&gt;&gt; df.resample('M').sum()\n                    b\n2010-01-31  16.168086\n2010-02-28   9.433712\n\n&gt;&gt;&gt; df.groupby([pd.Grouper(freq='M'), 'a']).sum()\n                   b\n           a        \n2010-01-31 x  8.9452\n           y  9.5671\n2010-02-28 x  4.2522\n           y  3.5148\n"
"print (df.notnull().any(axis = 0))\na     True\nb     True\nc     True\nd    False\ndtype: bool\n\ndf = df.loc[:, df.notnull().any(axis = 0)]\nprint (df)\n\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  NaN  8.0\n2  NaN  6.0  9.0\n3  NaN  NaN  NaN\n\nprint (df.columns[df.notnull().any(axis = 0)])\nIndex(['a', 'b', 'c'], dtype='object')\n\ndf = df[df.columns[df.notnull().any(axis = 0)]]\nprint (df)\n\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  NaN  8.0\n2  NaN  6.0  9.0\n3  NaN  NaN  NaN\n\nprint (df.dropna(axis=1, how='all'))\n     a    b    c\n0  1.0  4.0  NaN\n1  2.0  NaN  8.0\n2  NaN  6.0  9.0\n3  NaN  NaN  NaN\n"
"df = (pd.to_datetime(df['date &amp; time of connection'])\n       .dt.floor('d')\n       .value_counts()\n       .rename_axis('date')\n       .reset_index(name='count'))\nprint (df)\n        date  count\n0 2017-06-23      6\n1 2017-06-21      5\n2 2017-06-19      3\n3 2017-06-22      3\n4 2017-06-20      2\n\ns = pd.to_datetime(df['date &amp; time of connection'])\ndf = s.groupby(s.dt.floor('d')).size().reset_index(name='count')\nprint (df)\n  date &amp; time of connection  count\n0                2017-06-19      3\n1                2017-06-20      2\n2                2017-06-21      5\n3                2017-06-22      3\n4                2017-06-23      6\n\nnp.random.seed(1542)\n\nN = 220000\na = np.unique(np.random.randint(N, size=int(N/2)))\ndf = pd.DataFrame(pd.date_range('2000-01-01', freq='37T', periods=N)).drop(a)\ndf.columns = ['date &amp; time of connection']\ndf['date &amp; time of connection'] = df['date &amp; time of connection'].dt.strftime('%d/%m/%Y %H:%M:%S')\nprint (df.head()) \n\nIn [193]: %%timeit\n     ...: df['date &amp; time of connection']=pd.to_datetime(df['date &amp; time of connection'])\n     ...: df1 = df.groupby(by=df['date &amp; time of connection'].dt.date).count()\n     ...: \n539 ms ± 45.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [194]: %%timeit\n     ...: df1 = (pd.to_datetime(df['date &amp; time of connection'])\n     ...:        .dt.floor('d')\n     ...:        .value_counts()\n     ...:        .rename_axis('date')\n     ...:        .reset_index(name='count'))\n     ...: \n12.4 ms ± 350 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nIn [195]: %%timeit\n     ...: s = pd.to_datetime(df['date &amp; time of connection'])\n     ...: df2 = s.groupby(s.dt.floor('d')).size().reset_index(name='count')\n     ...: \n17.7 ms ± 140 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
'test.loc[test.index.repeat(test.times)]\n\n  id  times\n0  a      2\n0  a      2\n1  b      3\n1  b      3\n1  b      3\n2  c      1\n3  d      5\n3  d      5\n3  d      5\n3  d      5\n3  d      5\n\ntest.loc[test.index.repeat(test.times)].reset_index(drop=True)\n\n   id  times\n0   a      2\n1   a      2\n2   b      3\n3   b      3\n4   b      3\n5   c      1\n6   d      5\n7   d      5\n8   d      5\n9   d      5\n10  d      5\n'
'In [302]: arr = np.array([np.arange(3), np.arange(1,4), np.arange(10,13)])\nIn [303]: arr\nOut[303]: \narray([[ 0,  1,  2],\n       [ 1,  2,  3],\n       [10, 11, 12]])\n\nIn [304]: arr = np.empty(3,object)\nIn [305]: arr[:] = [np.arange(3), np.arange(1,4), np.arange(10,13)]\nIn [306]: arr\nOut[306]: \narray([array([0, 1, 2]), array([1, 2, 3]), array([10, 11, 12])],\n      dtype=object)\n\nIn [307]: np.array(arr)\nOut[307]: \narray([array([0, 1, 2]), array([1, 2, 3]), array([10, 11, 12])],\n      dtype=object)\n\nIn [308]: np.stack(arr)\nOut[308]: \narray([[ 0,  1,  2],\n       [ 1,  2,  3],\n       [10, 11, 12]])\n'
'# The default for VML is 1 thread (see #39)\nset_vml_num_threads(1)\n\nfrom pandas.core.computation.check import _NUMEXPR_INSTALLED\n\nif _NUMEXPR_INSTALLED:\n   import numexpr as ne\n\n&gt;&gt;&gt; gdb --args python slow.py\n(gdb) b mkl_serv_domain_set_num_threads\nfunction "mkl_serv_domain_set_num_threads" not defined.\nMake breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 1 (mkl_serv_domain_set_num_threads) pending.\n(gbd) run\nThread 1 "python" hit Breakpoint 1, 0x00007fffee65cd70 in mkl_serv_domain_set_num_threads () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n(gdb) bt \n#0  0x00007fffee65cd70 in mkl_serv_domain_set_num_threads () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n#1  0x00007fffe978026c in _set_vml_num_threads(_object*, _object*) () from /home/ed/anaconda37/lib/python3.7/site-packages/numexpr/interpreter.cpython-37m-x86_64-linux-gnu.so\n#2  0x00005555556cd660 in _PyMethodDef_RawFastCallKeywords () at /tmp/build/80754af9/python_1553721932202/work/Objects/call.c:694\n...\n(gdb) print $rdi\n$1 = 1\n\n(gbd) b mkl_serv_domain_get_max_threads\nBreakpoint 2 at 0x7fffee65a900\n(gdb) (gdb) c\nContinuing.\n\nThread 1 "python" hit Breakpoint 2, 0x00007fffee65a900 in mkl_serv_domain_get_max_threads () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n(gdb) bt\n#0  0x00007fffee65a900 in mkl_serv_domain_get_max_threads () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n#1  0x00007ffff01fcea9 in mkl_vml_serv_threader_d_1i_1o () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n#2  0x00007fffedf78563 in vdSqrt () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_lp64.so\n#3  0x00007ffff5ac04ac in trivial_two_operand_loop () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so\n\n(gdb) fin\nRun till exit from #0  0x00007fffee65a900 in mkl_serv_domain_get_max_threads () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n0x00007ffff01fcea9 in mkl_vml_serv_threader_d_1i_1o () from /home/ed/anaconda37/lib/python3.7/site-packages/numpy/../../../libmkl_intel_thread.so\n(gdb) print $rax\n$2 = 1\n\nimport numpy as np\nimport numexpr as ne\nimport pandas as pd\ndf=pd.DataFrame(np.random.random((10,10)))\ndf+df\n\n#HERE: reset number of vml-threads\nne.set_vml_num_threads(8)\n\nx=np.random.random(1000000)\nfor i in range(10000):\n    np.sqrt(x)     # now in parallel\n'
'import requests\nimport json\n\ncities_url = "https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json"\ncities_json = json.loads(requests.get(cities_url).content.decode(\'utf8\'))\n\ncountries = set([city[\'country\'] for city in cities_json])\ncities = set([city[\'name\'] for city in cities_json])\n\nimport requests\nimport json\nfrom flashtext import KeywordProcessor\n\ncities_url = "https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json"\ncities_json = json.loads(requests.get(cities_url).content.decode(\'utf8\'))\n\ncountries = set([city[\'country\'] for city in cities_json])\ncities = set([city[\'name\'] for city in cities_json])\n\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\n\ntexts = [\'new york to venice, italy for usd271\',\n\'return flights from brussels to bangkok with etihad from â‚¬407\',\n\'from los angeles to guadalajara, mexico for usd191\',\n\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\']\nkeyword_processor.extract_keywords(texts[0])\n\n[\'York\', \'Venice\', \'Italy\']\n\n&gt;&gt;&gt; "New York" in cities\nFalse\n\n&gt;&gt;&gt; len(countries)\n244\n&gt;&gt;&gt; len(cities)\n21940\n\nimport requests\nimport json\n\ncities_url = "https://pkgstore.datahub.io/core/world-cities/world-cities_json/data/5b3dd46ad10990bca47b04b4739a02ba/world-cities_json.json"\ncities1_json = json.loads(requests.get(cities_url).content.decode(\'utf8\'))\n\ncountries1 = set([city[\'country\'] for city in cities1_json])\ncities1 = set([city[\'name\'] for city in cities1_json])\n\ndr5hn_cities_url = "https://raw.githubusercontent.com/dr5hn/countries-states-cities-database/master/cities.json"\ndr5hn_countries_url = "https://raw.githubusercontent.com/dr5hn/countries-states-cities-database/master/countries.json"\n\ncities2_json = json.loads(requests.get(dr5hn_cities_url).content.decode(\'utf8\'))\ncountries2_json = json.loads(requests.get(dr5hn_countries_url).content.decode(\'utf8\'))\n\ncountries2 = set([c[\'name\'] for c in countries2_json])\ncities2 = set([c[\'name\'] for c in cities2_json])\n\ncountries = countries2.union(countries1)\ncities = cities2.union(cities1)\n\n&gt;&gt;&gt; len(countries)\n282\n&gt;&gt;&gt; len(cities)\n127793\n\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\ntexts = [\'new york to venice, italy for usd271\',\n\'return flights from brussels to bangkok with etihad from â‚¬407\',\n\'from los angeles to guadalajara, mexico for usd191\',\n\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\']\n\nkeyword_processor.extract_keywords(texts[0])\n\n[\'York\', \'Venice\', \'Italy\']\n\n&gt;&gt;&gt; [c for c in cities if \'york\' in c.lower()]\n[\'Yorklyn\',\n \'West York\',\n \'West New York\',\n \'Yorktown Heights\',\n \'East Riding of Yorkshire\',\n \'Yorke Peninsula\',\n \'Yorke Hill\',\n \'Yorktown\',\n \'Jefferson Valley-Yorktown\',\n \'New York Mills\',\n \'City of York\',\n \'Yorkville\',\n \'Yorkton\',\n \'New York County\',\n \'East York\',\n \'East New York\',\n \'York Castle\',\n \'York County\',\n \'Yorketown\',\n \'New York City\',\n \'York Beach\',\n \'Yorkshire\',\n \'North Yorkshire\',\n \'Yorkeys Knob\',\n \'York\',\n \'York Town\',\n \'York Harbor\',\n \'North York\']\n\nfrom itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\ntexts_labels = [(\'new york to venice, italy for usd271\', (\'New York\', \'Venice\', \'Italy\')),\n(\'return flights from brussels to bangkok with etihad from â‚¬407\', (\'Brussels\', \'Bangkok\')),\n(\'from los angeles to guadalajara, mexico for usd191\', (\'Los Angeles\', \'Guadalajara\')),\n(\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\', (\'Australia\', \'New Zealand\', \'Paris\'))]\n\n# No. of correctly extracted terms.\ntrue_positives = 0\nfalse_positives = 0\ntotal_truth = 0\n\nfor text, label in texts_labels:\n    extracted = keyword_processor.extract_keywords(text)\n\n    # We\'re making some assumptions here that the order of \n    # extracted and the truth must be the same.\n    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)\n    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)\n    total_truth += len(label)\n\n    # Just visualization candies.\n    print(text)\n    print(extracted)\n    print(label)\n    print()\n\n&gt;&gt;&gt; true_positives / total_truth\n0.9\n\nkeyword_processor.add_keyword(\'New York\')\n\nprint(texts[0])\nprint(keyword_processor.extract_keywords(texts[0]))\n\n[\'New York\', \'Venice\', \'Italy\']\n\n&gt;&gt;&gt; keyword_processor.extract_keywords(\'I live in Marawi\')\n[]\n\n&gt;&gt;&gt; keyword_processor.extract_keywords(\'I live in Jeju\')\n[]\n\nfor c in cities:\n    if \'city\' in c.lower() and c.endswith(\'City\') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n            print(c[:-5])\n\nfrom itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\nfor c in cities:\n    if \'city\' in c.lower() and c.endswith(\'City\') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n\ntexts_labels = [(\'new york to venice, italy for usd271\', (\'New York\', \'Venice\', \'Italy\')),\n(\'return flights from brussels to bangkok with etihad from â‚¬407\', (\'Brussels\', \'Bangkok\')),\n(\'from los angeles to guadalajara, mexico for usd191\', (\'Los Angeles\', \'Guadalajara\')),\n(\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\', (\'Australia\', \'New Zealand\', \'Paris\')),\n(\'I live in Florida\', (\'Florida\')), \n(\'I live in Marawi\', (\'Marawi\')), \n(\'I live in jeju\', (\'Jeju\'))]\n\n# No. of correctly extracted terms.\ntrue_positives = 0\nfalse_positives = 0\ntotal_truth = 0\n\nfor text, label in texts_labels:\n    extracted = keyword_processor.extract_keywords(text)\n\n    # We\'re making some assumptions here that the order of \n    # extracted and the truth must be the same.\n    true_positives += sum(1 for e, l in zip_longest(extracted, label) if e == l)\n    false_positives += sum(1 for e, l in zip_longest(extracted, label) if e != l)\n    total_truth += len(label)\n\n    # Just visualization candies.\n    print(text)\n    print(extracted)\n    print(label)\n    print()\n\nnew york to venice, italy for usd271\n[\'New York\', \'Venice\', \'Italy\']\n(\'New York\', \'Venice\', \'Italy\')\n\nreturn flights from brussels to bangkok with etihad from â‚¬407\n[\'Brussels\', \'Bangkok\']\n(\'Brussels\', \'Bangkok\')\n\nfrom los angeles to guadalajara, mexico for usd191\n[\'Los Angeles\', \'Guadalajara\', \'Mexico\']\n(\'Los Angeles\', \'Guadalajara\')\n\nfly to australia new zealand from paris from â‚¬422 return including 2 checked bags\n[\'Australia\', \'New Zealand\', \'Paris\']\n(\'Australia\', \'New Zealand\', \'Paris\')\n\nI live in Florida\n[\'Florida\']\nFlorida\n\nI live in Marawi\n[\'Marawi\']\nMarawi\n\nI live in jeju\n[\'Jeju\']\nJeju\n\n&gt;&gt;&gt; keyword_processor.extract_keywords(\'Adam flew to Bangkok from Singapore and then to China\')\n[\'Adam\', \'Bangkok\', \'Singapore\', \'China\']\n\n&gt;&gt;&gt; \'Adam\' in cities\nAdam\n\n[\'new york to venice, italy for usd271\',\n\'return flights from brussels to bangkok with etihad from â‚¬407\',\n\'from los angeles to guadalajara, mexico for usd191\',\n\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\'\n]\n\nOrigin: New York, USA; Destination: Venice, Italy\nOrigin: Brussels, BEL; Destination: Bangkok, Thailand\nOrigin: Los Angeles, USA; Destination: Guadalajara, Mexico\nOrigin: Paris, France; Destination: Australia / New Zealand (this is a complicated case given two countries)\n\n&gt;&gt;&gt; keyword_processor.extract_keywords(\'Adam flew to Bangkok from Singapore and then to China\')\n\n&gt; Adam flew to Bangkok from Singapore and then to China\n\n&gt; Origin: Singapore\n&gt; Departure: Bangkok\n&gt; Departure: China\n\nfrom itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\nfor c in cities:\n    if \'city\' in c.lower() and c.endswith(\'City\') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n\nkeyword_processor.add_keyword(\'to\')\nkeyword_processor.add_keyword(\'from\')\n\ntexts = [\'new york to venice, italy for usd271\',\n\'return flights from brussels to bangkok with etihad from â‚¬407\',\n\'from los angeles to guadalajara, mexico for usd191\',\n\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\']\n\n\nfor text in texts:\n    extracted = keyword_processor.extract_keywords(text)\n    print(text)\n    print(extracted)\n    print()\n\nnew york to venice, italy for usd271\n[\'New York\', \'to\', \'Venice\', \'Italy\']\n\nreturn flights from brussels to bangkok with etihad from â‚¬407\n[\'from\', \'Brussels\', \'to\', \'Bangkok\', \'from\']\n\nfrom los angeles to guadalajara, mexico for usd191\n[\'from\', \'Los Angeles\', \'to\', \'Guadalajara\', \'Mexico\']\n\nfly to australia new zealand from paris from â‚¬422 return including 2 checked bags\n[\'to\', \'Australia\', \'New Zealand\', \'from\', \'Paris\', \'from\']\n\nfrom itertools import zip_longest\nfrom flashtext import KeywordProcessor\n\nkeyword_processor = KeywordProcessor(case_sensitive=False)\nkeyword_processor.add_keywords_from_list(sorted(countries))\nkeyword_processor.add_keywords_from_list(sorted(cities))\n\nfor c in cities:\n    if \'city\' in c.lower() and c.endswith(\'City\') and c[:-5] not in cities:\n        if c[:-5].strip():\n            keyword_processor.add_keyword(c[:-5])\n\nkeyword_processor.add_keyword(\'to\')\nkeyword_processor.add_keyword(\'from\')\n\ntexts = [\'new york to venice, italy for usd271\',\n\'return flights from brussels to bangkok with etihad from â‚¬407\',\n\'from los angeles to guadalajara, mexico for usd191\',\n\'fly to australia new zealand from paris from â‚¬422 return including 2 checked bags\']\n\n\nfor text in texts:\n    extracted = keyword_processor.extract_keywords(text)\n    print(text)\n\n    new_extracted = []\n    extracted_next = extracted[1:]\n    for e_i, e_iplus1 in zip_longest(extracted, extracted_next):\n        if e_i == \'from\' and e_iplus1 not in cities and e_iplus1 not in countries:\n            print(e_i, e_iplus1)\n            continue\n        elif e_i == \'from\' and e_iplus1 == None: # last word in the list.\n            continue\n        else:\n            new_extracted.append(e_i)\n\n    print(new_extracted)\n    print()\n\nnew york to venice, italy for usd271\n[\'New York\', \'to\', \'Venice\', \'Italy\']\n\nreturn flights from brussels to bangkok with etihad from â‚¬407\nfrom None\n[\'from\', \'Brussels\', \'to\', \'Bangkok\']\n\nfrom los angeles to guadalajara, mexico for usd191\n[\'from\', \'Los Angeles\', \'to\', \'Guadalajara\', \'Mexico\']\n\nfly to australia new zealand from paris from â‚¬422 return including 2 checked bags\nfrom None\n[\'to\', \'Australia\', \'New Zealand\', \'from\', \'Paris\']\n'
'category,value\nAB,100.00\nAB,200.00\nAC,150.00\nAD,500.00\n\nimport pandas\ndata_2010 = pandas.read_csv("/path/to/2010.csv")\ndata_2010.groupby("category").agg([len, sum])\n\n          value     \n            len  sum\ncategory            \nAB            2  300\nAC            1  150\nAD            1  500\n'
"def _col_seq_set(df, col_list, seq_list):\n    ''' set dataframe 'df' col_list's sequence by seq_list '''\n    col_not_in_col_list = [x for x in list(df.columns) if x not in col_list]\n    for i in range(len(col_list)):\n        col_not_in_col_list.insert(seq_list[i], col_list[i])\n\n    return df[col_not_in_col_list]\nDataFrame.col_seq_set = _col_seq_set\n"
'import numpy as np\nax0.yaxis.set_ticks(np.arange(70000,80000,2500))\n'
"import pandas as pd\nfrom pandas.util.testing import rands\n\ndata = [pd.Series([rands(4) for j in range(6)],\n                  index=pd.date_range('1/1/2000', periods=6),\n                  name='col'+str(i)) for i in range(4)]\n\ndf = pd.concat(data, axis=1, keys=[s.name for s in data])\nprint(df)\n\n            col0  col1  col2  col3\n2000-01-01  GqcN  Lwlj  Km7b  XfaA\n2000-01-02  lhNC  nlSm  jCYu  XLVb\n2000-01-03  sSRz  PFby  C1o5  0BJe\n2000-01-04  khZb  Ny9p  crUY  LNmc\n2000-01-05  hmLp  4rVp  xF2P  OmD9\n2000-01-06  giah  psQb  T5RJ  oLSh\n"
"for f in files:\n  df = pd.read_csv(f)\n  df.to_hdf('file.h5',f,df)\n\npd.read_hdf('my_store.h5','a_table_node',['index&gt;100'])\n"
"xls = pd.ExcelFile('C:\\Users\\cb\\Machine_Learning\\cMap_Joins.xlsm')\n\ndf = xls.parse('Sheet1', skiprows=4, index_col=None, na_values=['NA'])\n"
'In [21]: df = pd.DataFrame([\'hello\', \'world\', \'hehe\'], columns=[\'words\'])\n\nIn [22]: df.words.str.count("he|wo")\nOut[22]:\n0    1\n1    1\n2    2\nName: words, dtype: int64\n\nIn [23]: df.words.str.count("he|wo").sum()\nOut[23]: 4\n\nDefinition: df.words.str.contains(self, pat, case=True, flags=0, na=nan)\nDocstring:\nCheck whether given pattern is contained in each string in the array\n\nParameters\n----------\npat : string\n    Character sequence or regular expression\ncase : boolean, default True\n    If True, case sensitive\nflags : int, default 0 (no flags)\n    re module flags, e.g. re.IGNORECASE\nna : default NaN, fill value for missing values.\n\nIn [11]: df = pd.DataFrame([\'hello\', \'world\'], columns=[\'words\'])\n\nIn [12]: df\nOut[12]:\n   words\n0  hello\n1  world\n\nIn [13]: df.words.str.contains(r\'[hw]\')\nOut[13]:\n0    True\n1    True\nName: words, dtype: bool\n\nIn [14]: df.words.str.contains(r\'he|wo\')\nOut[14]:\n0    True\n1    True\nName: words, dtype: bool\n\nIn [15]: df.words.str.contains(r\'he|wo\').sum()\nOut[15]: 2\n\nIn [16]: df.words.str.contains(r\'he\').sum()\nOut[16]: 1\n'
'In [14]: df.stack().value_counts()\nOut[14]: \n192.248.8.183    3   \n192.168.2.85     3   \n66.249.74.52     2   \n192.168.2.161    2   \n124.43.113.22    1   \ndtype: int64\n'
"grouped = data['2013-08-17'].groupby(axis=1, level='SPECIES').T\ngrouped.boxplot()\n"
"df = df[['bob']]\n"
"image_name_data['id'] = image_name_data['id'].astype(int).astype('str')\n\nimage_name_data['id'] = image_name_data['id'].map('{:.0f}'.format)\n"
"In [23]: axes = df.boxplot(by='g')\n\nIn [24]: fig = axes[0][0].get_figure()\n\nIn [25]: fig.suptitle('')\nOut[25]: &lt;matplotlib.text.Text at 0x109496090&gt;\n"
"#! /usr/bin/env python\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndata = pd.read_fwf('myfile.log',header=None,names=['time','amount'],widths=[27,5])\ndata.time = pd.to_datetime(data['time'], format='%Y-%m-%d %H:%M:%S.%f')\ndata.set_index(['time'],inplace=True)\ndata.plot()\n\n#OR \nplt.plot(data.index, data.amount)\n"
"&gt;&gt;&gt; from itertools import product\n&gt;&gt;&gt; pd.DataFrame(list(product(l1, l2)), columns=['l1', 'l2'])\n  l1  l2\n0  A   1\n1  A   2\n2  B   1\n3  B   2\n"
"import pandas as pd\n\n# Create a random time series with values over 100 days\n# starting from 1st March.\nN = 100\ndates = pd.date_range(start='2015-03-01', periods=N, freq='D')\nts = pd.DataFrame({'date': dates,\n                   'values': np.random.randn(N)}).set_index('date')\n\n# Create the plot and adjust x/y limits. The new x-axis\n# ranges from mid-February till 1st July.\nax = ts.plot()\nax.set_xlim(pd.Timestamp('2015-02-15'), pd.Timestamp('2015-07-01'))\nax.set_ylim(-5, 5)\n"
"pd.read_csv(..., header=None)\n\nimport io\nimport sys\nimport pandas as pd\nif sys.version_info.major == 3:\n    # Python3\n    StringIO = io.StringIO \nelse:\n    # Python2\n    StringIO = io.BytesIO\n\ntext = '''\\\n1 2 3\n4 5 6\n'''\n\nprint(pd.read_csv(StringIO(text), sep=' '))\n\n   1  2  3\n0  4  5  6\n\nprint(pd.read_csv(StringIO(text), sep=' ', header=None))\n\n   0  1  2\n0  1  2  3\n1  4  5  6\n"
"# Assuming same lines from your example\ncols_to_norm = ['Age','Height']\nsurvey_data[cols_to_norm] = survey_data[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n"
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(some_list, columns=["colummn"])\n&gt;&gt;&gt; df.to_csv(\'list.csv\', index=False)\n'
'df[df.isnull()] = d2\n'
"import igraph\nimport pandas as pd\n\nnode_names = ['A', 'B', 'C']\na = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]], index=node_names, columns=node_names)\n\n# Get the values as np.array, it's more convenenient.\nA = a.values\n\n# Create graph, A.astype(bool).tolist() or (A / A).tolist() can also be used.\ng = igraph.Graph.Adjacency((A &gt; 0).tolist())\n\n# Add edge weights and node labels.\ng.es['weight'] = A[A.nonzero()]\ng.vs['label'] = node_names  # or a.index/a.columns\n\ndf_from_g = pd.DataFrame(g.get_adjacency(attribute='weight').data,\n                         columns=g.vs['label'], index=g.vs['label'])\n(df_from_g == a).all().all()  # --&gt; True\n"
"a = data['A']&gt;0\nb = data['B']&gt;0\ndata.groupby([a,b]).count() \n"
'data_array = data.values\nfor train_index, test_index in sss:\n    xtrain, xtest = data_array[train_index], data_array[test_index]\n    ytrain, ytest = target[train_index], target[test_index]\n\nfor train_index, test_index in sss:\n    xtrain, xtest = data.iloc[train_index], data.iloc[test_index]\n    ytrain, ytest = target[train_index], target[test_index]\n'
'import pandas as pd\ndf = pd.DataFrame(["BULL","BEAR","BULL"], columns=[\'A\'])\ndf["B"] = ["Long" if ele  == "BULL" else "Short" for ele in df["A"]]\n\nprint(df)\n\n    A      B\n0  BULL   Long\n1  BEAR  Short\n2  BULL   Long\n\nimport pandas as pd\ndata = ["BULL","BEAR","BULL"]\ndata2 = ["Long" if ele  == "BULL" else "Short" for ele in data]\ndf = pd.DataFrame(list(zip(data, data2)), columns=[\'A\',\'B\'])\n\nprint(df)\n      A      B\n 0  BULL   Long\n 1  BEAR  Short\n 2  BULL   Long\n\ndf = pd.DataFrame([[\'BULL APPLE X5\',\'\'],[\'BEAR APPLE X5\',\'\'],[\'BULL APPLE X5\',\'\']], columns=[\'A\',\'B\'])\n\ndf["B"] = df["A"].map(lambda x: "Long" if "BULL" in x else "Short" if "BEAR" in x else "")\n\nprint(df)\n\n            A      B\n0  BULL APPLE X5   Long\n1  BEAR APPLE X5  Short\n2  BULL APPLE X5   Long\n\ndf = pd.DataFrame([\'BULL APPLE X5\',\'BEAR APPLE X5\',\'BLL APPLE X5\'], columns=[\'A\'])\n\ndf["B"] = df["A"].map(lambda x: "Long" if "BULL" in x else "Short" if "BEAR" in x else "")\n\nprint(df)\n\ndf = pd.DataFrame([[\'BULL APPLE X5\',\'\'],[\'BEAR APPLE X5\',\'\'],[\'BULL APPLE X5\',\'\']], columns=[\'A\',\'B\'])\n\n\ndf["B"][df[\'A\'].str.contains("BULL")] = "Long"\ndf["B"][df[\'A\'].str.contains("BEAR")] = "Short"\n\nprint(df)\n0  BULL APPLE X5   Long\n1  BEAR APPLE X5  Short\n2  BULL APPLE X5   Long\n'
"In [84]:\ndf['viz'] = (df['viz'] !='n').astype(int)\ndf\n\nOut[84]:\n   viz  a1_count  a1_mean     a1_std\n0    0         3        2   0.816497\n1    1         0      NaN        NaN\n2    0         2       51  50.000000\n\nIn [86]:\ndf['viz'] = np.where(df['viz'] == 'n', 0, 1)\ndf\n\nOut[86]:\n   viz  a1_count  a1_mean     a1_std\n0    0         3        2   0.816497\n1    1         0      NaN        NaN\n2    0         2       51  50.000000\n\nIn [89]:\ndf['viz'] !='n'\n\nOut[89]:\n0    False\n1     True\n2    False\nName: viz, dtype: bool\n\nIn [90]:\n(df['viz'] !='n').astype(int)\n\nOut[90]:\n0    0\n1    1\n2    0\nName: viz, dtype: int32\n"
'clf.score(X_test, y_test)\n'
"In [87]: df['A'].replace(to_replace=0, method='ffill')\nOut[87]:\n0     1\n1     1\n2     1\n3     2\n4     2\n5     4\n6     6\n7     8\n8     8\n9     8\n10    8\n11    8\n12    2\n13    1\nName: A, dtype: int64\n\nIn [88]: df['A'].replace(to_replace=0, method='ffill').values\nOut[88]: array([1, 1, 1, 2, 2, 4, 6, 8, 8, 8, 8, 8, 2, 1], dtype=int64)\n"
'import pandas as pd\n\ns = ((1,0,0,0,),(2,3,0,0,),(4,5,6,0,),(7,8,9,10,))\n\nprint pd.DataFrame(list(s))\n\n#    0  1  2   3\n# 0  1  0  0   0\n# 1  2  3  0   0\n# 2  4  5  6   0\n# 3  7  8  9  10\n\nprint pd.DataFrame(list(s), columns=[1,2,3,4], index=[1,2,3,4])  \n\n#    1  2  3   4\n# 1  1  0  0   0\n# 2  2  3  0   0\n# 3  4  5  6   0\n# 4  7  8  9  10\n'
'# ... all your data collection\ndf = pd.DataFrame(data, columns=data.keys())\n'
'%matplotlib inline\n'
'In [11]: s[s.index.dayofweek &lt; 5]\nOut[11]:\n2016-05-02 00:00:00    4.780\n2016-05-02 00:01:00    4.777\n2016-05-02 00:02:00    4.780\n2016-05-02 00:03:00    4.780\n2016-05-02 00:04:00    4.780\nName: closeAsk, dtype: float64\n'
"df.rename(columns={'old_col':'new_col', 'old_col_2':'new_col_2'}, inplace=True)\n\ncolumn_indices = [1,4,5,6]\nnew_names = ['a','b','c','d']\nold_names = df.columns[column_indices]\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n"
'df = df.drop(790)\n\ndf.drop(790, inplace=True)\n'
"d1 = {'key_a': 'val_a', 'key_b': 'val_b'}\n\nd2 = {'key_b': 'val_b', 'key_a': 'val_a'}\n\nimport pandas as pd\n\ndata1 = pd.DataFrame({ 'b' : [1, 1, 1], 'a' : [2, 2, 2]})\ndata2 = pd.DataFrame({ 'b' : [1, 1, 1], 'a' : [2, 2, 2]})\nframes = [data1, data2]\ndata = pd.concat(frames)\n\nprint(data)\n\ncols = ['b' , 'a']\ndata = data[cols]\n\nprint(data)\n"
"df[df['A'].apply(lambda x: isinstance(x, str))]\n"
"In [3]: units = {}\n\nIn [5]: newcols = []\n   ...: for col in df:\n   ...:     name, unit = col.split(' ')\n   ...:     units[name] = unit\n   ...:     newcols.append(name)\n\nIn [6]: df.columns = newcols\n\nIn [7]: df\nOut[7]:\n   length  width  thickness\n0     1.2    3.4        5.6\n1     7.8    9.0        1.2\n2     3.4    5.6        7.8\n\nIn [8]: units['length']\nOut[8]: '(m)'\n"
"In [5]:\ndf.apply(lambda x: x.str.replace(',','.'))\n\nOut[5]:\n          1-8        1-7\nH0   0.140711   0.140711\nH1     0.0999     0.0999\nH2      0.001      0.001\nH3   0.140711   0.140711\nH4   0.140711   0.140711\nH5   0.140711   0.140711\nH6          0          0\nH7          0          0\nH8   0.140711   0.140711\nH9   0.140711   0.140711\nH10  0.140711  0.1125688\nH11  0.140711  0.1125688\nH12  0.140711  0.1125688\nH13  0.140711  0.1125688\nH14  0.140711   0.140711\nH15  0.140711   0.140711\nH16  0.140711   0.140711\nH17  0.140711   0.140711\nH18  0.140711   0.140711\nH19  0.140711   0.140711\nH20  0.140711   0.140711\nH21  0.140711   0.140711\nH22  0.140711   0.140711\nH23  0.140711   0.140711\n\nIn [4]:    \ndf.stack().str.replace(',','.').unstack()\n\nOut[4]:\n          1-8        1-7\nH0   0.140711   0.140711\nH1     0.0999     0.0999\nH2      0.001      0.001\nH3   0.140711   0.140711\nH4   0.140711   0.140711\nH5   0.140711   0.140711\nH6          0          0\nH7          0          0\nH8   0.140711   0.140711\nH9   0.140711   0.140711\nH10  0.140711  0.1125688\nH11  0.140711  0.1125688\nH12  0.140711  0.1125688\nH13  0.140711  0.1125688\nH14  0.140711   0.140711\nH15  0.140711   0.140711\nH16  0.140711   0.140711\nH17  0.140711   0.140711\nH18  0.140711   0.140711\nH19  0.140711   0.140711\nH20  0.140711   0.140711\nH21  0.140711   0.140711\nH22  0.140711   0.140711\nH23  0.140711   0.140711\n"
"df = pd.DataFrame({'a': [1, 1, -1, 1, -1, -1]})\nprint (df)\n   a\n0  1\n1  1\n2 -1\n3  1\n4 -1\n5 -1\n\nprint ((df.a != df.a.shift()).cumsum())\n0    1\n1    1\n2    2\n3    3\n4    4\n5    4\nName: a, dtype: int32\n\nfor i, g in df.groupby([(df.a != df.a.shift()).cumsum()]):\n    print (i)\n    print (g)\n    print (g.a.tolist())\n\n   a\n0  1\n1  1\n[1, 1]\n2\n   a\n2 -1\n[-1]\n3\n   a\n3  1\n[1]\n4\n   a\n4 -1\n5 -1\n[-1, -1]\n"
"df['ratio'] = df.groupby(['a','b'], group_keys=False).apply(lambda g: g.c/(g.c * g.d).sum())\n"
"import pandas as pd\ndata = [{'name': 'vikash', 'age': 27}, {'name': 'Satyam', 'age': 14}]\ndf = pd.DataFrame.from_dict(data, orient='columns')\n\ndf\nOut[4]:\n   age  name\n0   27  vikash\n1   14  Satyam\n\ndata = [\n  {\n    'name': {\n      'first': 'vikash',\n      'last': 'singh'\n    },\n    'age': 27\n  },\n  {\n    'name': {\n      'first': 'satyam',\n      'last': 'singh'\n    },\n    'age': 14\n  }\n]\n\ndf = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n\ndf    \nOut[8]:\nage name.first  name.last\n0   27  vikash  singh\n1   14  satyam  singh\n"
'In [159]: df\nOut[159]:\n   a          b  c\n0  1     [1, 2]  5\n1  2  [2, 3, 4]  6\n2  3        [5]  7\n\nIn [160]: lst_col = \'b\'\n\nIn [161]: pd.DataFrame({\n     ...:     col:np.repeat(df[col].values, df[lst_col].str.len())\n     ...:     for col in df.columns.difference([lst_col])\n     ...: }).assign(**{lst_col:np.concatenate(df[lst_col].values)})[df.columns.tolist()]\n     ...:\nOut[161]:\n   a  b  c\n0  1  1  5\n1  1  2  5\n2  2  2  6\n3  2  3  6\n4  2  4  6\n5  3  5  7\n\ndf = pd.DataFrame({\n    "a" : [1,2,3],\n    "b" : [[1,2],[2,3,4],[5]],\n    "c" : [5,6,7]\n})\n\nIn [124]: pd.DataFrame({\'a\':np.repeat(df.a.values, df.b.str.len()),\n                        \'b\':np.concatenate(df.b.values)})\nOut[124]:\n   a  b\n0  1  1\n1  1  2\n2  2  2\n3  2  3\n4  2  4\n5  3  5\n\nIn [89]: df.set_index(\'a\', append=True).b.apply(pd.Series).stack().reset_index(level=[0, 2], drop=True).reset_index()\nOut[89]:\n   a    0\n0  1  1.0\n1  1  2.0\n2  2  2.0\n3  2  3.0\n4  2  4.0\n5  3  5.0\n\nIn [110]: df.set_index(\'a\').b.apply(pd.Series).stack().reset_index(level=-1, drop=True).astype(int).reset_index()\nOut[110]:\n   a  0\n0  1  1\n1  1  2\n2  2  2\n3  2  3\n4  2  4\n5  3  5\n'
"df = df.loc[:, df.isnull().mean() &lt; .8]\n\nnp.random.seed(100)\ndf = pd.DataFrame(np.random.random((100,5)), columns=list('ABCDE'))\ndf.loc[:80, 'A'] = np.nan\ndf.loc[:5, 'C'] = np.nan\ndf.loc[20:, 'D'] = np.nan\n\nprint (df.isnull().mean())\nA    0.81\nB    0.00\nC    0.06\nD    0.80\nE    0.00\ndtype: float64\n\ndf = df.loc[:, df.isnull().mean() &lt; .8]\nprint (df.head())\n         B   C         E\n0  0.278369 NaN  0.004719\n1  0.670749 NaN  0.575093\n2  0.209202 NaN  0.219697\n3  0.811683 NaN  0.274074\n4  0.940030 NaN  0.175410\n\nnp.random.seed(1997)\ndf = pd.DataFrame(np.random.choice([np.nan,1], p=(0.8,0.2),size=(10,10)))\nprint (df)\n     0   1    2    3    4    5    6    7   8    9\n0  NaN NaN  NaN  1.0  1.0  NaN  NaN  NaN NaN  NaN\n1  1.0 NaN  1.0  NaN  NaN  NaN  NaN  NaN NaN  NaN\n2  NaN NaN  NaN  NaN  NaN  1.0  1.0  NaN NaN  NaN\n3  NaN NaN  NaN  NaN  1.0  NaN  NaN  NaN NaN  NaN\n4  NaN NaN  NaN  NaN  NaN  1.0  NaN  NaN NaN  1.0\n5  NaN NaN  NaN  1.0  1.0  NaN  NaN  1.0 NaN  1.0\n6  NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN  NaN\n7  NaN NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN  NaN\n8  NaN NaN  NaN  NaN  NaN  NaN  NaN  1.0 NaN  NaN\n9  1.0 NaN  NaN  NaN  1.0  NaN  NaN  1.0 NaN  NaN\n\ndf1 = df.dropna(thresh=2, axis=1)\nprint (df1)\n     0    3    4    5    7    9\n0  NaN  1.0  1.0  NaN  NaN  NaN\n1  1.0  NaN  NaN  NaN  NaN  NaN\n2  NaN  NaN  NaN  1.0  NaN  NaN\n3  NaN  NaN  1.0  NaN  NaN  NaN\n4  NaN  NaN  NaN  1.0  NaN  1.0\n5  NaN  1.0  1.0  NaN  1.0  1.0\n6  NaN  NaN  NaN  NaN  NaN  NaN\n7  NaN  NaN  NaN  NaN  NaN  NaN\n8  NaN  NaN  NaN  NaN  1.0  NaN\n9  1.0  NaN  1.0  NaN  1.0  NaN\n\n df = df.loc[:, df.isnull().sum() &lt; 0.8*df.shape[0]]\n"
'import matplotlib.pylab as plt\nimport pandas as pd\nimport seaborn as sns\n\ndicti=({\'37\':99943,\'25\':47228,\'36\':16933,\'40\':14996,\'35\':11791,\'34\':8030,\'24\' : 6319 ,\'2\'  :5055 ,\'39\' :4758 ,\'38\' :4611  })\npd_df = pd.DataFrame(list(dicti.items()))\npd_df.columns =["Dim","Count"]\nprint (pd_df)\n# sort df by Count column\npd_df = pd_df.sort_values([\'Count\']).reset_index(drop=True)\nprint (pd_df)\n\nplt.figure(figsize=(12,8))\n# plot barh chart with index as x values\nax = sns.barplot(pd_df.index, pd_df.Count)\nax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))\nax.set(xlabel="Dim", ylabel=\'Count\')\n# add proper Dim values as x labels\nax.set_xticklabels(pd_df.Dim)\nfor item in ax.get_xticklabels(): item.set_rotation(90)\nfor i, v in enumerate(pd_df["Count"].iteritems()):        \n    ax.text(i ,v[1], "{:,}".format(v[1]), color=\'m\', va =\'bottom\', rotation=45)\nplt.tight_layout()\nplt.show()\n'
'def add_freq(idx, freq=None):\n    """Add a frequency attribute to idx, through inference or directly.\n\n    Returns a copy.  If `freq` is None, it is inferred.\n    """\n\n    idx = idx.copy()\n    if freq is None:\n        if idx.freq is None:\n            freq = pd.infer_freq(idx)\n        else:\n            return idx\n    idx.freq = pd.tseries.frequencies.to_offset(freq)\n    if idx.freq is None:\n        raise AttributeError(\'no discernible frequency found to `idx`.  Specify\'\n                             \' a frequency string with `freq`.\')\n    return idx\n\nidx=pd.to_datetime([\'2003-01-02\', \'2003-01-03\', \'2003-01-06\'])  # freq=None\n\nprint(add_freq(idx))  # inferred\nDatetimeIndex([\'2003-01-02\', \'2003-01-03\', \'2003-01-06\'], dtype=\'datetime64[ns]\', freq=\'B\')\n\nprint(add_freq(idx, freq=\'D\'))  # explicit\nDatetimeIndex([\'2003-01-02\', \'2003-01-03\', \'2003-01-06\'], dtype=\'datetime64[ns]\', freq=\'D\')\n'
'dtrain = xgb.DMatrix(Xtrain, label=ytrain, feature_names=feature_names)\n'
'from parsimonious.grammar import Grammar\nfrom parsimonious.nodes import NodeVisitor\nimport pandas as pd\ngrammar = Grammar(\n    r"""\n    schools         = (school_block / ws)+\n\n    school_block    = school_header ws grade_block+ \n    grade_block     = grade_header ws name_header ws (number_name)+ ws score_header ws (number_score)+ ws? \n\n    school_header   = ~"^School = (.*)"m\n    grade_header    = ~"^Grade = (\\d+)"m\n    name_header     = "Student number, Name"\n    score_header    = "Student number, Score"\n\n    number_name     = index comma name ws\n    number_score    = index comma score ws\n\n    comma           = ws? "," ws?\n\n    index           = number+\n    score           = number+\n\n    number          = ~"\\d+"\n    name            = ~"[A-Z]\\w+"\n    ws              = ~"\\s*"\n    """\n)\n\ntree = grammar.parse(data)\n\nclass SchoolVisitor(NodeVisitor):\n    output, names = ([], [])\n    current_school, current_grade = None, None\n\n    def _getName(self, idx):\n        for index, name in self.names:\n            if index == idx:\n                return name\n\n    def generic_visit(self, node, visited_children):\n        return node.text or visited_children\n\n    def visit_school_header(self, node, children):\n        self.current_school = node.match.group(1)\n\n    def visit_grade_header(self, node, children):\n        self.current_grade = node.match.group(1)\n        self.names = []\n\n    def visit_number_name(self, node, children):\n        index, name = None, None\n        for child in node.children:\n            if child.expr.name == \'name\':\n                name = child.text\n            elif child.expr.name == \'index\':\n                index = child.text\n\n        self.names.append((index, name))\n\n    def visit_number_score(self, node, children):\n        index, score = None, None\n        for child in node.children:\n            if child.expr.name == \'index\':\n                index = child.text\n            elif child.expr.name == \'score\':\n                score = child.text\n\n        name = self._getName(index)\n\n        # build the entire entry\n        entry = (self.current_school, self.current_grade, index, name, score)\n        self.output.append(entry)\n\nsv = SchoolVisitor()\nsv.visit(tree)\n\ndf = pd.DataFrame.from_records(sv.output, columns = [\'School\', \'Grade\', \'Student number\', \'Name\', \'Score\'])\nprint(df)\n\n^\nSchool\\s*=\\s*(?P&lt;school_name&gt;.+)\n(?P&lt;school_content&gt;[\\s\\S]+?)\n(?=^School|\\Z)\n\n^\nGrade\\s*=\\s*(?P&lt;grade&gt;.+)\n(?P&lt;students&gt;[\\s\\S]+?)\n(?=^Grade|\\Z)\n\n^\nStudent\\ number,\\ Name[\\n\\r]\n(?P&lt;student_names&gt;(?:^\\d+.+[\\n\\r])+)\n\\s*\n^\nStudent\\ number,\\ Score[\\n\\r]\n(?P&lt;student_scores&gt;(?:^\\d+.+[\\n\\r])+)\n\nimport pandas as pd, re\n\nrx_school = re.compile(r\'\'\'\n    ^\n    School\\s*=\\s*(?P&lt;school_name&gt;.+)\n    (?P&lt;school_content&gt;[\\s\\S]+?)\n    (?=^School|\\Z)\n\'\'\', re.MULTILINE | re.VERBOSE)\n\nrx_grade = re.compile(r\'\'\'\n    ^\n    Grade\\s*=\\s*(?P&lt;grade&gt;.+)\n    (?P&lt;students&gt;[\\s\\S]+?)\n    (?=^Grade|\\Z)\n\'\'\', re.MULTILINE | re.VERBOSE)\n\nrx_student_score = re.compile(r\'\'\'\n    ^\n    Student\\ number,\\ Name[\\n\\r]\n    (?P&lt;student_names&gt;(?:^\\d+.+[\\n\\r])+)\n    \\s*\n    ^\n    Student\\ number,\\ Score[\\n\\r]\n    (?P&lt;student_scores&gt;(?:^\\d+.+[\\n\\r])+)\n\'\'\', re.MULTILINE | re.VERBOSE)\n\n\nresult = ((school.group(\'school_name\'), grade.group(\'grade\'), student_number, name, score)\n    for school in rx_school.finditer(string)\n    for grade in rx_grade.finditer(school.group(\'school_content\'))\n    for student_score in rx_student_score.finditer(grade.group(\'students\'))\n    for student in zip(student_score.group(\'student_names\')[:-1].split("\\n"), student_score.group(\'student_scores\')[:-1].split("\\n"))\n    for student_number in [student[0].split(", ")[0]]\n    for name in [student[0].split(", ")[1]]\n    for score in [student[1].split(", ")[1]]\n)\n\ndf = pd.DataFrame(result, columns = [\'School\', \'Grade\', \'Student number\', \'Name\', \'Score\'])\nprint(df)\n\nrx_school = re.compile(r\'^School\\s*=\\s*(?P&lt;school_name&gt;.+)(?P&lt;school_content&gt;[\\s\\S]+?)(?=^School|\\Z)\', re.MULTILINE)\nrx_grade = re.compile(r\'^Grade\\s*=\\s*(?P&lt;grade&gt;.+)(?P&lt;students&gt;[\\s\\S]+?)(?=^Grade|\\Z)\', re.MULTILINE)\nrx_student_score = re.compile(r\'^Student number, Name[\\n\\r](?P&lt;student_names&gt;(?:^\\d+.+[\\n\\r])+)\\s*^Student number, Score[\\n\\r](?P&lt;student_scores&gt;(?:^\\d+.+[\\n\\r])+)\', re.MULTILINE)\n\n            School Grade Student number      Name Score\n0   Riverdale High     1              0    Phoebe     3\n1   Riverdale High     1              1    Rachel     7\n2   Riverdale High     2              0    Angela     6\n3   Riverdale High     2              1   Tristan     3\n4   Riverdale High     2              2    Aurora     9\n5         Hogwarts     1              0     Ginny     8\n6         Hogwarts     1              1      Luna     7\n7         Hogwarts     2              0     Harry     5\n8         Hogwarts     2              1  Hermione    10\n9         Hogwarts     3              0      Fred     0\n10        Hogwarts     3              1    George     0\n\nimport timeit\nprint(timeit.timeit(makedf, number=10**4))\n# 11.918397722000009 s\n'
'&gt;&gt;&gt; import sys\n&gt;&gt;&gt; sys.getsizeof(42)\n28\n&gt;&gt;&gt; sys.getsizeof(\'T\')\n50\n\n#!/usr/bin/env python3\n\nimport pandas as pd\nfrom memory_profiler import profile\n\n@profile\ndef main():\n    with open(\'genome_matrix_header.txt\') as header:\n        header = header.read().rstrip(\'\\n\').split(\'\\t\')\n\n    gen_matrix_df = pd.read_csv(\n        \'genome_matrix_final-chr1234-1mb.txt\', sep=\'\\t\', names=header)\n\n    gen_matrix_df.info()\n    gen_matrix_df.info(memory_usage=\'deep\')\n\nif __name__ == \'__main__\':\n    main()\n\nmprof run justpd.py\nmprof plot\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     6     54.3 MiB     54.3 MiB   @profile\n     7                             def main():\n     8     54.3 MiB      0.0 MiB       with open(\'genome_matrix_header.txt\') as header:\n     9     54.3 MiB      0.0 MiB           header = header.read().rstrip(\'\\n\').split(\'\\t\')\n    10                             \n    11   2072.0 MiB   2017.7 MiB       gen_matrix_df = pd.read_csv(\'genome_matrix_final-chr1234-1mb.txt\', sep=\'\\t\', names=header)\n    12                                 \n    13   2072.0 MiB      0.0 MiB       gen_matrix_df.info()\n    14   2072.0 MiB      0.0 MiB       gen_matrix_df.info(memory_usage=\'deep\')\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nRangeIndex: 4000000 entries, 0 to 3999999\nData columns (total 34 columns):\n...\ndtypes: int64(2), object(32)\nmemory usage: 1.0+ GB\n\nmemory usage: 7.9 GB\n\n# added after read_csv\nfrom pympler import tracker\ntr = tracker.SummaryTracker()\ntr.print_diff()   \n\n                                             types |   # objects |   total size\n================================================== | =========== | ============\n                 &lt;class \'pandas.core.series.Series |          34 |      7.93 GB\n                                      &lt;class \'list |        7839 |    732.38 KB\n                                       &lt;class \'str |        7741 |    550.10 KB\n                                       &lt;class \'int |        1810 |     49.66 KB\n                                      &lt;class \'dict |          38 |      7.43 KB\n  &lt;class \'pandas.core.internals.SingleBlockManager |          34 |      3.98 KB\n                             &lt;class \'numpy.ndarray |          34 |      3.19 KB\n\nimport ctypes\n\nclass PyASCIIObject(ctypes.Structure):\n     _fields_ = [\n         (\'ob_refcnt\', ctypes.c_size_t),\n         (\'ob_type\', ctypes.py_object),\n         (\'length\', ctypes.c_ssize_t),\n         (\'hash\', ctypes.c_int64),\n         (\'state\', ctypes.c_int32),\n         (\'wstr\', ctypes.c_wchar_p)\n    ]\n\n&gt;&gt;&gt; a = \'name\'\n&gt;&gt;&gt; b = \'!@#$\'\n&gt;&gt;&gt; a_struct = PyASCIIObject.from_address(id(a))\n&gt;&gt;&gt; a_struct.state &amp; 0b11\n1\n&gt;&gt;&gt; b_struct = PyASCIIObject.from_address(id(b))\n&gt;&gt;&gt; b_struct.state &amp; 0b11\n0\n\n&gt;&gt;&gt; a = \'foo\'\n&gt;&gt;&gt; b = \'foo\'\n&gt;&gt;&gt; a is b\nTrue\n&gt;&gt; gen_matrix_df.REF[0] is gen_matrix_df.REF[6]\nTrue\n\n&gt;&gt;&gt; rows = 4 * 10 ** 6\n&gt;&gt;&gt; int_cols = 2\n&gt;&gt;&gt; str_cols = 32\n&gt;&gt;&gt; int_size = 8\n&gt;&gt;&gt; str_size = 58  \n&gt;&gt;&gt; ptr_size = 8\n&gt;&gt;&gt; (int_cols * int_size + str_cols * (str_size + ptr_size)) * rows / 2 ** 30\n7.927417755126953\n\nLine #    Mem usage    Increment   Line Contents\n================================================\n     8     53.1 MiB     53.1 MiB   @profile\n     9                             def main():\n    10     53.1 MiB      0.0 MiB       with open("genome_matrix_header.txt") as header:\n    11     53.1 MiB      0.0 MiB           header = header.read().rstrip(\'\\n\').split(\'\\t\')\n    12                             \n    13   2070.9 MiB   2017.8 MiB       gen_matrix_df = pd.read_csv(\'genome_matrix_final-chr1234-1mb.txt\', sep=\'\\t\', names=header)\n    14   2071.2 MiB      0.4 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[gen_matrix_df.keys()[0]])\n    15   2071.2 MiB      0.0 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[gen_matrix_df.keys()[0]])\n    16   2040.7 MiB    -30.5 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    ...\n    23   1827.1 MiB    -30.5 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    24   1094.7 MiB   -732.4 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    25   1765.9 MiB    671.3 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    26   1094.7 MiB   -671.3 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    27   1704.8 MiB    610.2 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    28   1094.7 MiB   -610.2 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    29   1643.9 MiB    549.2 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    30   1094.7 MiB   -549.2 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    31   1582.8 MiB    488.1 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    32   1094.7 MiB   -488.1 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])    \n    33   1521.9 MiB    427.2 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])    \n    34   1094.7 MiB   -427.2 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    35   1460.8 MiB    366.1 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    36   1094.7 MiB   -366.1 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    37   1094.7 MiB      0.0 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n    ...\n    47   1094.7 MiB      0.0 MiB       gen_matrix_df = gen_matrix_df.drop(columns=[random.choice(gen_matrix_df.keys())])\n\n#!/usr/bin/env python3\n\nimport csv\nimport sys\nimport json\n\ndef smemstat():\n  with open(\'smemstat.json\') as f:\n    smem = json.load(f)\n\n  rows = []\n  fieldnames = set()    \n  for s in smem[\'smemstat\'][\'periodic-samples\']:\n    row = {}\n    for ps in s[\'smem-per-process\']:\n      if \'script.py\' in ps[\'command\']:\n        for k in (\'uss\', \'pss\', \'rss\'):\n          row[\'{}-{}\'.format(ps[\'pid\'], k)] = ps[k] // 2 ** 20\n\n    # smemstat produces empty samples, backfill from previous\n    if rows:            \n      for k, v in rows[-1].items():\n        row.setdefault(k, v)\n\n    rows.append(row)\n    fieldnames.update(row.keys())\n\n  with open(\'smemstat.csv\', \'w\') as out:\n    dw = csv.DictWriter(out, fieldnames=sorted(fieldnames))\n    dw.writeheader()\n    list(map(dw.writerow, rows))\n\ndef glances():\n  rows = []\n  fieldnames = [\'available\', \'used\', \'cached\', \'mem_careful\', \'percent\',\n    \'free\', \'mem_critical\', \'inactive\', \'shared\', \'history_size\',\n    \'mem_warning\', \'total\', \'active\', \'buffers\']\n  with open(\'glances.csv\', \'w\') as out:\n    dw = csv.DictWriter(out, fieldnames=fieldnames)\n    dw.writeheader()\n    with open(\'glances.json\') as f:\n      for l in f:\n        d = json.loads(l)\n        dw.writerow(d[\'mem\'])\n\nif __name__ == \'__main__\':\n  globals()[sys.argv[1]]()\n\n...\nglobal global_gen_matrix_df_values\nglobal_gen_matrix_df_values = list(gen_matrix_df_list.values())\ndel gen_matrix_df_list\n\np = Pool(2)\nresult = p.map(matrix_to_vcf, range(len(global_gen_matrix_df_values)))\n...\n\ndef matrix_to_vcf(i):\n    matrix_df = global_gen_matrix_df_values[i]\n'
'from sqlalchemy import create_engine\n# see sqlalchemy docs for how to write this url for your database type:\nengine = create_engine(\'mysql://scott:tiger@localhost/foo\')\n\ntable_name = \'my_prices\'\ndf = pd.read_sql_table(table_name, engine)\n\ndf = pd.read_sql_query("SELECT instrument, price, date FROM my_prices;", engine)\n\ndf.reset_index().pivot(\'date\', \'instrument\', \'price\')\n'
"import matplotlib.pyplot as plt\nimport pandas as pd\nx = [[1.2, 2.3, 3.0, 4.5],\n     [1.1, 2.2, 2.9, 5.0]]\ndf = pd.DataFrame(x, index=['Age of pregnant women', 'Age of pregnant men'])\n\ndf.T.boxplot(vert=False)\nplt.subplots_adjust(left=0.25)\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nx = [[1.2, 2.3, 3.0, 4.5],\n     [1.1, 2.2, 2.9, 5.0]]\ndf = pd.DataFrame(x, index=['Age of pregnant women', 'Age of pregnant men'])\n\ndf.T.boxplot()\nplt.subplots_adjust(bottom=0.25)\nplt.xticks(rotation=25)\nplt.show()\n"
"In [36]: df = pd.DataFrame({'a':[1,2,3], 'b':[1,2,3]})\n\nIn [37]: f = open('foo', 'a')\n\nIn [38]: f.write('# My awesome comment\\n')\n\nIn [39]: f.write('# Here is another one\\n')\n\nIn [40]: df.to_csv(f)\n\nIn [41]: f.close()\n\nIn [42]: more foo\n# My awesome comment\n# Here is another one\n,a,b\n0,1,1\n1,2,2\n2,3,3\n"
"In [29]:\ndf['points'] = np.where( ( (df['gender'] == 'male') &amp; (df['pet1'] == df['pet2'] ) ) | ( (df['gender'] == 'female') &amp; (df['pet1'].isin(['cat','dog'] ) ) ), 5, 0)\ndf\n\nOut[29]:\n     gender      pet1      pet2  points\n0      male       dog       dog       5\n1      male       cat       cat       5\n2      male       dog       cat       0\n3    female       cat  squirrel       5\n4    female       dog       dog       5\n5    female  squirrel       cat       0\n6  squirrel       dog       cat       0\n"
'fig = pie[0].get_figure()\nfig.savefig("~/Desktop/myplot.pdf")\n'
"df.A.str.extract('(\\d+)')\n\n0      1\n1    NaN\n2     10\n3    100\n4      0\nName: A, dtype: object\n"
"&gt;&gt;&gt; dmaster = dd.from_pandas(master, npartitions=4)\n&gt;&gt;&gt; dmaster['my_value'] = dmaster.original.apply(lambda x: helper(x, slave), name='my_value'))\n&gt;&gt;&gt; dmaster.compute()\n                  original  my_value\n0  this is a nice sentence         2\n1      this is another one         3\n2    stackoverflow is nice         1\n\nimport dask.multiprocessing\nimport dask.threaded\n\n&gt;&gt;&gt; dmaster.compute(get=dask.threaded.get)  # this is default for dask.dataframe\n&gt;&gt;&gt; dmaster.compute(get=dask.multiprocessing.get)  # try processes instead\n"
"df.drop_duplicates(subset=['City', 'State', 'Zip', 'Date']) \n\ndf.drop_duplicates(subset=df.columns.difference(['Description']))\n"
"df['price'] = df['Symbol'].apply(getquotetoday)\n\ndf['new_column_name'] = df.apply(lambda x: my_function(x['value_1'], x['value_2']), axis=1)\n"
'print "df = pd.DataFrame( %s )" % (str(df.to_dict()))\n\nprint "df = pd.DataFrame( %s )" % (str(df.to_dict()).replace(" nan"," float(\'nan\')"))\n'
"print (df.name.str.len())\n0    5\n1    2\n2    6\n3    4\nName: name, dtype: int64\n\nprint (df.name.str.len().sort_values())\n1    2\n3    4\n0    5\n2    6\nName: name, dtype: int64\n\ns = df.name.str.len().sort_values().index\nprint (s)\nInt64Index([1, 3, 0, 2], dtype='int64')\n\nprint (df.reindex(s))\n     name  score\n1      Al      4\n3    Greg      3\n0   Steve      2\n2  Markus      2\n\ndf1 = df.reindex(s)\ndf1 = df1.reset_index(drop=True)\nprint (df1)\n     name  score\n0      Al      4\n1    Greg      3\n2   Steve      2\n3  Markus      2\n"
"dropped['new'] = dropped['diff'].values.astype(np.int64)\n\nmeans = dropped.groupby('bank').mean()\nmeans['new'] = pd.to_timedelta(means['new'])\n\nstd = dropped.groupby('bank').std()\nstd['new'] = pd.to_timedelta(std['new'])\n\ndropped['new'] = dropped['diff'].dt.total_seconds()\n\nmeans = dropped.groupby('bank').mean()\n"
"# pre 0.24\nfeature_file_df['RESULT'] = RESULT_df['RESULT'].values\n# &gt;= 0.24\nfeature_file_df['RESULT'] = RESULT_df['RESULT'].to_numpy()\n\ndf\n          A         B\n0 -1.202564  2.786483\n1  0.180380  0.259736\n2 -0.295206  1.175316\n3  1.683482  0.927719\n4 -0.199904  1.077655\n\ndf2\n\n           C\n11 -0.140670\n12  1.496007\n13  0.263425\n14 -0.557958\n15 -0.018375\n\ndf['C'] = df2['C']\ndf\n\n          A         B   C\n0 -1.202564  2.786483 NaN\n1  0.180380  0.259736 NaN\n2 -0.295206  1.175316 NaN\n3  1.683482  0.927719 NaN\n4 -0.199904  1.077655 NaN\n\ndf2['C'].values \narray([-0.141,  1.496,  0.263, -0.558, -0.018])\n\ndf['C'] = df2['C'].values\ndf\n\n          A         B         C\n0 -1.202564  2.786483 -0.140670\n1  0.180380  0.259736  1.496007\n2 -0.295206  1.175316  0.263425\n3  1.683482  0.927719 -0.557958\n4 -0.199904  1.077655 -0.018375\n"
'temp2 = df[~df["Def"] &amp; (df["days since"] &gt; 7) &amp; (df["bin"] == 3)]\n\ncond1 = df["bin"] == 3    \ncond2 = df["days since"] &gt; 7\ncond3 = ~df["Def"]\n\ntemp2 = df[cond1 &amp; cond2 &amp; cond3]\n\ndf = pd.DataFrame({\'Def\':[True] *2 + [False]*4,\n                   \'days since\':[7,8,9,14,2,13],\n                   \'bin\':[1,3,5,3,3,3]})\n\nprint (df)\n     Def  bin  days since\n0   True    1           7\n1   True    3           8\n2  False    5           9\n3  False    3          14\n4  False    3           2\n5  False    3          13\n\n\ntemp2 = df[~df["Def"] &amp; (df["days since"] &gt; 7) &amp; (df["bin"] == 3)]\nprint (temp2)\n     Def  bin  days since\n3  False    3          14\n5  False    3          13\n'
"pandas.concat([df['foo'].dropna(), df['bar'].dropna()]).reindex_like(df)\n"
".plot(marker='o')\n\nplt.plot(df.R2.index.to_pydatetime(), df.R2, 'o-')\n"
'&gt;&gt;&gt; print d\n   A  B  C\n0 -2  5  4\n1  1 -1  2\n2  0  2  1\n3 -3  1  2\n4  5  0  2\n&gt;&gt;&gt; def foo(df):\n...     print "&gt;&gt;&gt;"\n...     print df\n...     print "&lt;&lt;&lt;"\n...     return df\n&gt;&gt;&gt; print d.groupby(\'C\').transform(foo)\n&gt;&gt;&gt;\n2    0\nName: A\n&lt;&lt;&lt;\n&gt;&gt;&gt;\n2    2\nName: B\n&lt;&lt;&lt;\n&gt;&gt;&gt;\n1    1\n3   -3\n4    5\nName: A\n&lt;&lt;&lt;\n&gt;&gt;&gt;\n1   -1\n3    1\n4    0\nName: B\n# etc.\n'
"df_result = pd.DataFrame(ts, columns=['value'])\n\ndef get_col_name(row):    \n    b = (df.ix[row.name] == row['value'])\n    return b.index[b.argmax()]\n\nIn [3]: df_result.apply(get_col_name, axis=1)\nOut[3]: \n1979-01-01 00:00:00    col5\n1979-01-01 06:00:00    col3\n1979-01-01 12:00:00    col1\n1979-01-01 18:00:00    col1\n\nIn [4]: row = df_result.irow(0) # an example row to pass to get_col_name\n\nIn [5]: row\nOut[5]: \nvalue    1181.220328\nName: 1979-01-01 00:00:00\n\nIn [6]: row.name # use to get rows of df\nOut[6]: &lt;Timestamp: 1979-01-01 00:00:00&gt;\n\nIn [7]: df.ix[row.name]\nOut[7]: \ncol5    1181.220328\ncol4     912.154923\ncol3     648.848635\ncol2     390.986156\ncol1     138.185861\nName: 1979-01-01 00:00:00\n\nIn [8]: b = (df.ix[row.name] == row['value'])\n        #checks whether each elements equal row['value'] = 1181.220328  \n\nIn [9]: b\nOut[9]: \ncol5     True\ncol4    False\ncol3    False\ncol2    False\ncol1    False\nName: 1979-01-01 00:00:00\n\nIn [10]: b.argmax() # index of a True value\nOut[10]: 0\n\nIn [11]: b.index[b.argmax()] # the index value (column name)\nOut[11]: 'col5'\n"
'&gt;&gt;&gt; df = pd.DataFrame({\'Name\': [\'foo\', \'bar\'] * 3,\n                   \'Rank\': np.random.randint(0,3,6),\n                   \'Val\': np.random.rand(6)})\n&gt;&gt;&gt; grouped = df.groupby(["Name", "Rank"])\n&gt;&gt;&gt; grouped.grouper.\ngrouped.grouper.agg_series        grouped.grouper.indices\ngrouped.grouper.aggregate         grouped.grouper.labels\ngrouped.grouper.apply             grouped.grouper.levels\ngrouped.grouper.axis              grouped.grouper.names\ngrouped.grouper.compressed        grouped.grouper.ngroups\ngrouped.grouper.get_group_levels  grouped.grouper.nkeys\ngrouped.grouper.get_iterator      grouped.grouper.result_index\ngrouped.grouper.group_info        grouped.grouper.shape\ngrouped.grouper.group_keys        grouped.grouper.size\ngrouped.grouper.groupings         grouped.grouper.sort\ngrouped.grouper.groups            \n\n&gt;&gt;&gt; df["GroupId"] = df.groupby(["Name", "Rank"]).grouper.group_info[0]\n&gt;&gt;&gt; df\n  Name  Rank       Val  GroupId\n0  foo     0  0.302482        2\n1  bar     0  0.375193        0\n2  foo     2  0.965763        4\n3  bar     2  0.166417        1\n4  foo     1  0.495124        3\n5  bar     2  0.728776        1\n'
'&gt;&gt;&gt; df = pd.DataFrame(range(5))\n&gt;&gt;&gt; eq2 = df[0] == 2\n&gt;&gt;&gt; df_no_2 = df[~eq2]\n&gt;&gt;&gt; df_no_2\n   0\n0  0\n1  1\n3  3\n4  4\n&gt;&gt;&gt; df_no_2.reset_index(drop=True)\n   0\n0  0\n1  1\n2  3\n3  4\n'
"&gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0, 2, (2, 8)))\n&gt;&gt;&gt; df\n   0  1  2  3  4  5  6  7\n0  0  0  0  1  0  0  1  0\n1  1  1  0  0  0  1  1  1\n&gt;&gt;&gt; df == 0\n       0      1     2      3     4      5      6      7\n0   True   True  True  False  True   True  False   True\n1  False  False  True   True  True  False  False  False\n&gt;&gt;&gt; (df == 0).all()\n0    False\n1    False\n2     True\n3    False\n4     True\n5    False\n6    False\n7    False\ndtype: bool\n&gt;&gt;&gt; df.columns[(df == 0).all()]\nInt64Index([u'2', u'4'], dtype=int64)\n&gt;&gt;&gt; df.loc[:, (df == 0).all()]\n   2  4\n0  0  0\n1  0  0\n"
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from StringIO import StringIO\n&gt;&gt;&gt; s = StringIO("""date,value\n... 12/01/2012,1\n... 12/01/2012,2\n... 30/01/2012,3""")\n&gt;&gt;&gt; \n&gt;&gt;&gt; pd.read_csv(s, index_col=0, parse_dates=True, dayfirst=True)\n            value\ndate             \n2012-01-12      1\n2012-01-12      2\n2012-01-30      3\n\n&gt;&gt;&gt; s = StringIO("""date\n... 12/01/2012\n... 12/01/2012\n... 30/01/2012""")\n&gt;&gt;&gt; \n&gt;&gt;&gt; pd.read_csv(s, parse_dates=[0], dayfirst=True)\n                 date\n0 2012-01-12 00:00:00\n1 2012-01-12 00:00:00\n2 2012-01-30 00:00:00\n'
"In [11]: df = pd.DataFrame(np.random.randn(4,4), columns=list('ABCD'))\n\nIn [12]: df\nOut[12]:\n          A         B         C         D\n0  0.933069  1.432486  0.288637 -1.867853\n1 -0.455952 -0.725268  0.339908  1.318175\n2 -0.894331  0.573868  1.116137  0.508845\n3  0.661572  0.819360 -0.527327 -0.925478\n\nIn [13]: df.mean()\nOut[13]:\nA    0.061089\nB    0.525112\nC    0.304339\nD   -0.241578\ndtype: float64\n\nIn [14]: df.mean().sort_values()\nOut[14]:\nD   -0.241578\nA    0.061089\nC    0.304339\nB    0.525112\ndtype: float64\n\nIn [15]: df.reindex(df.mean().sort_values().index, axis=1)\nOut[15]:\n          D         A         C         B\n0 -1.867853  0.933069  0.288637  1.432486\n1  1.318175 -0.455952  0.339908 -0.725268\n2  0.508845 -0.894331  1.116137  0.573868\n3 -0.925478  0.661572 -0.527327  0.819360\n"
'&gt;&gt;&gt; person1 = {\'type\': 01, \'name\': \'Jhon\', \'surname\': \'Smith\', \'phone\': \'555-1234\'}\n&gt;&gt;&gt; person2 = {\'type\': 01, \'name\': \'Jannette\', \'surname\': \'Jhonson\', \'credit\': 1000000.00}\n&gt;&gt;&gt; animal1 = {\'type\': 03, \'cname\': \'cow\', \'sciname\': \'Bos....\', \'legs\': 4, \'tails\': 1 }\n&gt;&gt;&gt; pd.DataFrame([person1])\n   name     phone surname  type\n0  Jhon  555-1234   Smith     1\n&gt;&gt;&gt; pd.DataFrame([person1, person2])\n    credit      name     phone  surname  type\n0      NaN      Jhon  555-1234    Smith     1\n1  1000000  Jannette       NaN  Jhonson     1\n&gt;&gt;&gt; pd.DataFrame.from_dict([person1, person2])\n    credit      name     phone  surname  type\n0      NaN      Jhon  555-1234    Smith     1\n1  1000000  Jannette       NaN  Jhonson     1\n\nfrom StringIO import StringIO\n\ndef get_filelike_object(filename, line_prefix):\n    s = StringIO()\n    with open(filename, "r") as fp:\n        for line in fp:\n            if line.startswith(line_prefix):\n                s.write(line)\n    s.seek(0)\n    return s\n\n&gt;&gt;&gt; type01 = get_filelike_object("animal.dat", "01")\n&gt;&gt;&gt; df = pd.read_fwf(type01, names="type name surname phone credit".split(), \n                     widths=[2, 10, 10, 8, 11], header=None)\n&gt;&gt;&gt; df\n   type      name  surname     phone     credit\n0     1      Jhon    Smith  555-1234        NaN\n1     1  Jannette  Jhonson       NaN  100000000\n'
"df.groupby(['col1','col2'])['col3'].nunique().reset_index()\n"
"def recur_dictify(frame):\n    if len(frame.columns) == 1:\n        if frame.values.size == 1: return frame.values[0][0]\n        return frame.values.squeeze()\n    grouped = frame.groupby(frame.columns[0])\n    d = {k: recur_dictify(g.ix[:,1:]) for k,g in grouped}\n    return d\n\n&gt;&gt;&gt; df\n  name  v1   v2  v3\n0    A  A1  A11   1\n1    A  A2  A12   2\n2    B  B1  B12   3\n3    C  C1  C11   4\n4    B  B2  B21   5\n5    A  A2  A21   6\n&gt;&gt;&gt; pprint.pprint(recur_dictify(df))\n{'A': {'A1': {'A11': 1}, 'A2': {'A12': 2, 'A21': 6}},\n 'B': {'B1': {'B12': 3}, 'B2': {'B21': 5}},\n 'C': {'C1': {'C11': 4}}}\n\ndef retro_dictify(frame):\n    d = {}\n    for row in frame.values:\n        here = d\n        for elem in row[:-2]:\n            if elem not in here:\n                here[elem] = {}\n            here = here[elem]\n        here[row[-2]] = row[-1]\n    return d\n"
'In [1506]: df1.assign(**df2)\nOut[1506]:\n   col_1  col_2  col_3  col_4\n0      0      4      8     12\n1      1      5      9     13\n2      2      6     10     14\n3      3      7     11     15\n\nIn [1507]: df1.assign(**additional_data)\nOut[1507]:\n   col_1  col_2  col_3  col_4\n0      0      4      8     12\n1      1      5      9     13\n2      2      6     10     14\n3      3      7     11     15\n'
'prices[:-1].values / prices[1:] - 1\n\nprices[:-1] / prices[1:].values - 1\n\nprices.shift(1) / prices - 1\n\nprices / prices.shift(1) - 1\n'
'pd.options.display.max_colwidth  # default is 50\n\npd.options.display.&lt;tab&gt;\n'
"df.groupby('col1').size().sort_values(ascending=False)\n\ndf.groupby('col1').size().order(ascending=False)\n"
"weekdays = ['Mon', 'Tues', 'Weds', 'Thurs', 'Fri', 'Sat', 'Sun']\nmapping = {day: i for i, day in enumerate(weekdays)}\nkey = df['day'].map(mapping)\n\ndf.iloc[key.argsort()]\n"
"In [96]:\n\ndf = pd.DataFrame({'a':randn(10), 'b':randn(10), 'c':randn(10)})\ndf\nOut[96]:\n          a         b         c\n0 -0.849903  0.944912  1.285790\n1 -1.038706  1.445381  0.251002\n2  0.683135 -0.539052 -0.622439\n3 -1.224699 -0.358541  1.361618\n4 -0.087021  0.041524  0.151286\n5 -0.114031 -0.201018 -0.030050\n6  0.001891  1.601687 -0.040442\n7  0.024954 -1.839793  0.917328\n8 -1.480281  0.079342 -0.405370\n9  0.167295 -1.723555 -0.033937\n\n[10 rows x 3 columns]\nIn [97]:\n\ndf[df &gt; 1.0].count()\n\nOut[97]:\na    0\nb    2\nc    2\ndtype: int64\n\ndf[df &lt; 2.0 ].count() \n\nIn [3]:\n\n%timeit df[df &lt; 1.0 ].count() \n%timeit (df &lt; 1.0).sum()\n%timeit (df &lt; 1.0).apply(np.count_nonzero)\n1000 loops, best of 3: 1.47 ms per loop\n1000 loops, best of 3: 560 us per loop\n1000 loops, best of 3: 529 us per loop\n"
"import pandas as pd\n\ndata = pd.DataFrame({'ticker': ['aapl', 'msft', 'goog'],\n                     'opinion': ['GC', nan, 'GC'],\n                     'x1': [100, 50, 40]})\n\ndata = data[data['opinion'].isnull()]\n"
'import pandas as pd\nfrom pandas.io.data import DataReader\nfrom datetime import datetime\nimport scipy.stats  as stats\n\n\ngdp = pd.DataFrame(DataReader("GDP", "fred", start=datetime(1990, 1, 1)))\nvix = pd.DataFrame(DataReader("VIXCLS", "fred", start=datetime(1990, 1, 1)))\n\n#Do it with a pandas regression to get the p value from the F-test\ndf = gdp.merge(vix,left_index=True, right_index=True, how=\'left\')\nvix_on_gdp = pd.ols(y=df[\'VIXCLS\'], x=df[\'GDP\'], intercept=True)\nprint(df[\'VIXCLS\'].corr(df[\'GDP\']), vix_on_gdp.f_stat[\'p-value\'])\n\n-0.0422917932738 0.851762475093\n\n#Do it with stats functions. \ndf_clean = df.dropna()\nstats.pearsonr(df_clean[\'VIXCLS\'], df_clean[\'GDP\'])\n\n  (-0.042291793273791969, 0.85176247509284908)\n\n#Add a third field\noil = pd.DataFrame(DataReader("DCOILWTICO", "fred", start=datetime(1990, 1, 1))) \ndf = df.merge(oil,left_index=True, right_index=True, how=\'left\')\n\n#construct two arrays, one of the correlation and the other of the p-vals\nrho = df.corr()\npval = np.zeros([df.shape[1],df.shape[1]])\nfor i in range(df.shape[1]): # rows are the number of rows in the matrix.\n    for j in range(df.shape[1]):\n        JonI        = pd.ols(y=df.icol(i), x=df.icol(j), intercept=True)\n        pval[i,j]  = JonI.f_stat[\'p-value\']\n\n             GDP    VIXCLS  DCOILWTICO\n GDP         1.000000 -0.042292    0.870251\n VIXCLS     -0.042292  1.000000   -0.004612\n DCOILWTICO  0.870251 -0.004612    1.000000\n\n [[  0.00000000e+00   8.51762475e-01   1.11022302e-16]\n  [  8.51762475e-01   0.00000000e+00   9.83747425e-01]\n  [  1.11022302e-16   9.83747425e-01   0.00000000e+00]]\n'
"In [33]: df\nOut[33]: \n       L      R  VALUE\n0   left  right     -1\n1  right   left      1\n2   left  right     -1\n3  right   left      1\n4   left  right     -1\n5  right   left      1\n\nIn [34]: df.loc[idx,['L','R']] = df.loc[idx,['R','L']].values\n\nIn [35]: df\nOut[35]: \n      L      R  VALUE\n0  left  right     -1\n1  left  right      1\n2  left  right     -1\n3  left  right      1\n4  left  right     -1\n5  left  right      1\n"
"mydata = mydata.set_index(DWDATA.index)\n\nDWDATA['MXX'] = mydata.iloc[:,0].values\n"
"row_index = df.col1 == 10\n# then with the form .loc[row_indexer,col_indexer]\ndf.loc[row_index, 'col1'] = 100\n"
"In [321]:\n\ndf['Date'] = pd.to_datetime(df['Date'], errors='coerce')\ndf\nOut[321]:\n                 Date\n0 2014-10-20 10:44:31\n1 2014-10-23 09:33:46\n2                 NaT\n3 2014-10-01 09:38:45\n\nIn [322]:\n\ndf.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 4 entries, 0 to 3\nData columns (total 1 columns):\nDate    3 non-null datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 64.0 bytes\n\nIn [324]:\n\ndef func(x):\n    try:\n        return dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n    except:\n        return pd.NaT\n\ndf['Date'].apply(func)\nOut[324]:\n0   2014-10-20 10:44:31\n1   2014-10-23 09:33:46\n2                   NaT\n3   2014-10-01 09:38:45\nName: Date, dtype: datetime64[ns]\n\nIn [326]:\n\n%timeit pd.to_datetime(df['Date'], errors='coerce')\n%timeit df['Date'].apply(func)\n10000 loops, best of 3: 65.8 µs per loop\n10000 loops, best of 3: 186 µs per loop\n"
"In [5]: a = pd.DataFrame(data=np.random.randint(0,100,(2,5)),columns=list('ABCDE'))\n\nIn [6]: b = pd.DataFrame(data=np.random.randint(0,100,(2,5)),columns=list('ABCDE'))\n\nIn [7]: c = pd.concat([a,b],ignore_index=True)\n\nIn [8]: c\nOut[8]: \n    A   B   C   D   E\n0  12  56  62  35  20\n1  10  71  63   0  70\n2  61  72  29  10  71\n3  88  82  39  73  94\n"
"&gt; df['C'] = df['B'].diff()\n&gt; print df\n   #  A  B   C\n0  1  2  3 NaN\n1  2  3  3   0\n2  3  4  4   1\n3  4  5  4   0\n4  5  5  4   0\n\n&gt; df_filtered = df[df['C'] != 0]\n&gt; print df_filtered\n   #  A  B  C\n2  3  4  4  1\n"
"In [29]:\n\ndf = pd.DataFrame(columns=list('abc'), data = np.random.randn(5,3))\ndf\nOut[29]:\n          a         b         c\n0 -1.525011  0.778190 -1.010391\n1  0.619824  0.790439 -0.692568\n2  1.272323  1.620728  0.192169\n3  0.193523  0.070921  1.067544\n4  0.057110 -1.007442  1.706704\nIn [30]:\n\nfor index,row in df.iterrows():\n    df.loc[index,'d'] = np.random.randint(0, 10)\ndf\nOut[30]:\n          a         b         c  d\n0 -1.525011  0.778190 -1.010391  9\n1  0.619824  0.790439 -0.692568  9\n2  1.272323  1.620728  0.192169  1\n3  0.193523  0.070921  1.067544  0\n4  0.057110 -1.007442  1.706704  9\n\nIn [31]:\n# reset the df by slicing\ndf = df[list('abc')]\nfor index,row in df.iterrows():\n    row['b'] = np.random.randint(0, 10)\ndf\nOut[31]:\n          a  b         c\n0 -1.525011  8 -1.010391\n1  0.619824  2 -0.692568\n2  1.272323  8  0.192169\n3  0.193523  2  1.067544\n4  0.057110  3  1.706704\n\nIn [35]:\n\ndf = df[list('abc')]\nfor index,row in df.iterrows():\n    row['d'] = np.random.randint(0,10)\ndf\nOut[35]:\n          a  b         c\n0 -1.525011  8 -1.010391\n1  0.619824  2 -0.692568\n2  1.272323  8  0.192169\n3  0.193523  2  1.067544\n4  0.057110  3  1.706704\n"
'p.multi_line(ts_list_of_list, vals_list_of_list, line_color=[\'red\', \'green\', \'blue\'])\n\nimport pandas as pd\nimport numpy as np\nfrom bokeh.palettes import Spectral11\nfrom bokeh.plotting import figure, show, output_file\noutput_file(\'temp.html\')\n\ntoy_df = pd.DataFrame(data=np.random.rand(5,3), columns = (\'a\', \'b\' ,\'c\'), index = pd.DatetimeIndex(start=\'01-01-2015\',periods=5, freq=\'d\'))   \n\nnumlines=len(toy_df.columns)\nmypalette=Spectral11[0:numlines]\n\np = figure(width=500, height=300, x_axis_type="datetime") \np.multi_line(xs=[toy_df.index.values]*numlines,\n                ys=[toy_df[name].values for name in toy_df],\n                line_color=mypalette,\n                line_width=5)\nshow(p)\n'
'c, r = labels.shape\nlabels = labels.reshape(c,)\n'
"In [20]: df.loc[df.reset_index().groupby(['F_Type'])['to_date'].idxmax()]\nOut[20]: \n                       start    end\nF_Type to_date                     \nA      20150908143000    345    316\nB      20150908143000  10743   8803\nC      20150908143000  19522  16659\nD      20150908143000    433     65\nE      20150908143000   7290   7375\nF      20150908143000      0      0\nG      20150908143000   1796    340\n"
"In [11]: df['Time'] = df['Time'].astype('datetime64[s]')\n\nIn [12]: df\nOut[12]: \n   Record_ID                Time\n0      94704 2014-03-10 07:19:19\n1      94705 2014-03-10 07:21:44\n2      94706 2014-03-10 07:21:45\n3      94707 2014-03-10 07:21:54\n4      94708 2014-03-10 07:21:55\n"
'df.fillna(df.mean(axis=1), axis=1)\n\nIn [11]: m = df.mean(axis=1)\n         for i, col in enumerate(df):\n             # using i allows for duplicate columns\n             # inplace *may* not always work here, so IMO the next line is preferred\n             # df.iloc[:, i].fillna(m, inplace=True)\n             df.iloc[:, i] = df.iloc[:, i].fillna(m)\n\nIn [12]: df\nOut[12]:\n   c1  c2   c3\n0   1   4  7.0\n1   2   5  3.5\n2   3   6  9.0\n\ndf.T.fillna(df.mean(axis=1)).T\n'
"&gt;&gt;&gt; s.str.cat(sep=', ')\n'I, will, hereby, am, gonna, going, far, to, do, this'\n"
"df = pd.read_csv('Check1_900.csv', sep='\\t', iterator=True, chunksize=1000)\n\ntp = pd.read_csv('Check1_900.csv', sep='\\t', iterator=True, chunksize=1000)\nprint tp\n#&lt;pandas.io.parsers.TextFileReader object at 0x00000000150E0048&gt;\ndf = pd.concat(tp, ignore_index=True)\n"
"df.groupby[['x','y']] \\\n.apply(lambda x: (np.max(x['z'])-np.min(x['z']))) \\\n.sort_values(ascending=False)\n\n(df.groupby[['x','y']]\n.apply(lambda x: (np.max(x['z'])-np.min(x['z'])))\n.sort_values(ascending=False))\n"
"pip install pandas\npip install lxml\npip install html5lib\npip install BeautifulSoup4\n\nimport pandas as pd\nimport html5lib\nf_states=   pd.read_html('https://simple.wikipedia.org/wiki/List_of_U.S._states') \n"
"In [131]:\ndates = pd.date_range('1/1/2000', periods=8)\ndf = pd.DataFrame(np.random.randn(8, 4), index=dates, columns=['A', 'B', 'C', 'D'])\ndf\n\nOut[131]:\n                   A         B         C         D\n2000-01-01  0.095234 -1.000863  0.899732 -1.742152\n2000-01-02 -0.517544 -1.274137  1.734024 -1.369487\n2000-01-03  0.134112  1.964386 -0.120282  0.573676\n2000-01-04 -0.737499 -0.581444  0.528500 -0.737697\n2000-01-05 -1.777800  0.795093  0.120681  0.524045\n2000-01-06 -0.048432 -0.751365 -0.760417 -0.181658\n2000-01-07 -0.570800  0.248608 -1.428998 -0.662014\n2000-01-08 -0.147326  0.717392  3.138620  1.208639\n\nIn [133]:    \nwindow_stop_row = df[df.index &lt; '2000-01-04'].iloc[-1]\nwindow_stop_row.name\n\nOut[133]:\nTimestamp('2000-01-03 00:00:00', offset='D')\n\nIn [134]:\ndf.index.get_loc(window_stop_row.name)\n\nOut[134]:\n2\n\nIn [135]:    \ndf.iloc[df.index.get_loc(window_stop_row.name)]\n\nOut[135]:\nA    0.134112\nB    1.964386\nC   -0.120282\nD    0.573676\nName: 2000-01-03 00:00:00, dtype: float64\n\nIn [142]:\ndf.index.searchsorted('2000-01-04') - 1\n\nOut[142]:\n2\n"
"mean_ratings = data.pivot_table('rating', rows='title', cols='gender', aggfunc='mean')\n\nmean_ratings = data.pivot_table('rating', index='title', columns='gender', aggfunc='mean')\n"
"df['B'].str.extract('(\\d+)').astype(int)\n"
"merge=pd.merge(df,d, how='inner', left_index=True, right_index=True)\n\nprint df\n           catcode_amt type feccandid_amt  amount\ndate                                             \n1915-12-31       A5000  24K     H6TX08100    1000\n1916-12-31       T6100  24K     H8CA52052     500\n1954-12-31       H3100  24K     S8AK00090    1000\n1985-12-31       J7120  24E     H8OH18088      36\n1997-12-31       z9600  24K     S6ND00058    2000\n\nprint d\n           catcode_disp disposition            feccandid_disp  bills\ndate                                                                \n1997-12-31        A0000     support                 S4HI00011    1.0\n2007-12-31        A1000      oppose  S4IA00020', 'P20000741 1    NaN\n2007-12-31        A1000     support                 S8MT00010    1.0\n2007-12-31        A1500     support                 S6WI00061    2.0\n2007-12-31        A1600     support  S4IA00020', 'P20000741 3    NaN\n\nmerge=pd.merge(df,d, how='inner', left_index=True, right_index=True)\nprint merge\n           catcode_amt type feccandid_amt  amount catcode_disp disposition  \\\ndate                                                                         \n1997-12-31       z9600  24K     S6ND00058    2000        A0000     support   \n\n           feccandid_disp  bills  \ndate                              \n1997-12-31      S4HI00011    1.0  \n\nprint pd.concat([df,d], join='inner', axis=1)\n\ndate                                                                         \n1997-12-31       z9600  24K     S6ND00058    2000        A0000     support   \n\n           feccandid_disp  bills  \ndate                              \n1997-12-31      S4HI00011    1.0  \n\nprint df\n           catcode_amt type feccandid_amt  amount\ndate                                             \n1915-12-31       A5000  24K     H6TX08100    1000\n1916-12-31       T6100  24K     H8CA52052     500\n1954-12-31       H3100  24K     S8AK00090    1000\n2007-12-31       J7120  24E     H8OH18088      36\n2007-12-31       z9600  24K     S6ND00058    2000\n\nprint d\n           catcode_disp disposition            feccandid_disp  bills\ndate                                                                \n1997-12-31        A0000     support                 S4HI00011    1.0\n2007-12-31        A1000      oppose  S4IA00020', 'P20000741 1    NaN\n2007-12-31        A1000     support                 S8MT00010    1.0\n2007-12-31        A1500     support                 S6WI00061    2.0\n2007-12-31        A1600     support  S4IA00020', 'P20000741 3    NaN\n\nmerge=pd.merge(df,d, how='inner', left_index=True, right_index=True)\n\nprint merge\n           catcode_amt type feccandid_amt  amount catcode_disp disposition  \\\ndate                                                                         \n2007-12-31       J7120  24E     H8OH18088      36        A1000      oppose   \n2007-12-31       J7120  24E     H8OH18088      36        A1000     support   \n2007-12-31       J7120  24E     H8OH18088      36        A1500     support   \n2007-12-31       J7120  24E     H8OH18088      36        A1600     support   \n2007-12-31       z9600  24K     S6ND00058    2000        A1000      oppose   \n2007-12-31       z9600  24K     S6ND00058    2000        A1000     support   \n2007-12-31       z9600  24K     S6ND00058    2000        A1500     support   \n2007-12-31       z9600  24K     S6ND00058    2000        A1600     support   \n\n                      feccandid_disp  bills  \ndate                                         \n2007-12-31  S4IA00020', 'P20000741 1    NaN  \n2007-12-31                 S8MT00010    1.0  \n2007-12-31                 S6WI00061    2.0  \n2007-12-31  S4IA00020', 'P20000741 3    NaN  \n2007-12-31  S4IA00020', 'P20000741 1    NaN  \n2007-12-31                 S8MT00010    1.0  \n2007-12-31                 S6WI00061    2.0  \n2007-12-31  S4IA00020', 'P20000741 3    NaN  \n"
"print df.groupby('value')['tempx'].apply(' '.join).reset_index()\n   value                                              tempx\n0    1.5  picture1 picture555 picture255 picture365 pict...\n"
"df['C']=df.A[::-1].cumsum()\n"
'# Pre v. 0.20.3\n# from pandas.util.testing import assert_frame_equal\n\nfrom pandas.testing import assert_frame_equal\n\nassert_frame_equal(df1, df2, check_dtype=False)\n'
"df.astype(float).sum().astype(int).astype(str)\n\n0     7\n1     4\n2    11\ndtype: object\n\ndf = pd.DataFrame([\n        ['2', '3', 'nan', None],\n        [None, None, None, None],\n        ['0', '1', '4', None],\n        ['5', 'nan', '7', None]\n    ])\n\ndf\n\n      0     1     2     3\n0     2     3   nan  None\n1  None  None  None  None\n2     0     1     4  None\n3     5   nan     7  None\n\ndf.astype(float).sum().astype(int).astype(str)\n\n0     7\n1     4\n2    11\n3     0\ndtype: object\n\ndf.dropna(1, 'all').astype(float).sum().astype(int).astype(str)\n\n0     7\n1     4\n2    11\ndtype: object\n"
"df['birthdate'].groupby([df.birthdate.dt.year, df.birthdate.dt.month]).agg('count')\n\nIn [165]:\ndf = pd.DataFrame({'birthdate':pd.date_range(start=dt.datetime(2015,12,20),end=dt.datetime(2016,3,1))})\ndf.groupby([df['birthdate'].dt.year, df['birthdate'].dt.month]).agg({'count'})\n\nOut[165]:\n                    birthdate\n                        count\nbirthdate birthdate          \n2015      12               12\n2016      1                31\n          2                29\n          3                 1\n\nIn[107]:\ndf.groupby([df['birthdate'].dt.year.rename('year'), df['birthdate'].dt.month.rename('month')]).agg({'count'})\n\nOut[107]: \n           birthdate\n               count\nyear month          \n2015 12           12\n2016 1            31\n     2            29\n     3             1\n"
" izmir = pd.read_excel(filepath)\n izmir_lim = izmir[['Gender','Age','MC_OLD_M&gt;=60','MC_OLD_F&gt;=60',\n                    'MC_OLD_M&gt;18','MC_OLD_F&gt;18','MC_OLD_18&gt;M&gt;5',\n                    'MC_OLD_18&gt;F&gt;5','MC_OLD_M_Child&lt;5','MC_OLD_F_Child&lt;5',\n                    'MC_OLD_M&gt;0&lt;=1','MC_OLD_F&gt;0&lt;=1','Date to Delivery',\n                    'Date to insert','Date of Entery']]\n\n izmir_lim = izmir[['Gender','Age','MC_OLD_M&gt;=60','MC_OLD_F&gt;=60',\n                    'MC_OLD_M&gt;18','MC_OLD_F&gt;18','MC_OLD_18&gt;M&gt;5',\n                    'MC_OLD_18&gt;F&gt;5','MC_OLD_M_Child&lt;5','MC_OLD_F_Child&lt;5',\n                    'MC_OLD_M&gt;0&lt;=1','MC_OLD_F&gt;0&lt;=1','Date to Delivery',\n                    'Date to insert','Date of Entery']].copy()\n\nnew_df = old_df[list_of_columns_names]\n\nnew_df.iloc[0, 0] = 1  # Should throw an error\n\nnew_df = old_df[list_of_columns_names].copy()\n\nnew_df = old_df[list_of_columns_names]\nnew_df.is_copy = None\n\nnew_df = old_df.loc[:, list_of_columns_names]\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\n\ndef background_gradient(s, m, M, cmap='PuBu', low=0, high=0):\n    rng = M - m\n    norm = colors.Normalize(m - (rng * low),\n                            M + (rng * high))\n    normed = norm(s.values)\n    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n    return ['background-color: %s' % color for color in c]\n\ndf = pd.DataFrame([[3,2,10,4],[20,1,3,2],[5,4,6,1]])\ndf.style.apply(background_gradient,\n               cmap='PuBu',\n               m=df.min().min(),\n               M=df.max().max(),\n               low=0,\n               high=0.2)\n"
"list_2_nodups = list_2.drop_duplicates()\npd.merge(list_1 , list_2_nodups , on=['email_address'])\n"
's = pd.Series(np.random.choice([3, 4, 5, 6, np.nan], 100))\ns.hist()\n\nnew = s.sub(s.min()).div((s.max() - s.min()))\nnew.hist()\n\nsigmoid = lambda x: 1 / (1 + np.exp(-x))\n\nnew = sigmoid(s.sub(s.mean()))\nnew.hist()\n\nnew = np.tanh(s.sub(s.mean())).add(1).div(2)\nnew.hist()\n'
'def final_pop(row):\n    return row.initial_pop*math.e**(row.growth_rate*35)\n'
"print (df['col'].dt.total_seconds())\n\ndf = pd.DataFrame({'date1':pd.date_range('2015-01-01', periods=3),\n                   'date2':pd.date_range('2015-01-01 02:00:00', periods=3, freq='23H')})\n\nprint (df)\n       date1               date2\n0 2015-01-01 2015-01-01 02:00:00\n1 2015-01-02 2015-01-02 01:00:00\n2 2015-01-03 2015-01-03 00:00:00\n\ndf['diff'] = df['date2'] - df['date1']\ndf['seconds'] = df['diff'].dt.total_seconds()\n\nprint (df)\n       date1               date2     diff  seconds\n0 2015-01-01 2015-01-01 02:00:00 02:00:00   7200.0\n1 2015-01-02 2015-01-02 01:00:00 01:00:00   3600.0\n2 2015-01-03 2015-01-03 00:00:00 00:00:00      0.0\n\ndf['diff'] = df['date2'] - df['date1']\ndf['diff'] = df['diff'].dt.total_seconds()\n\nprint (df)\n       date1               date2    diff\n0 2015-01-01 2015-01-01 02:00:00  7200.0\n1 2015-01-02 2015-01-02 01:00:00  3600.0\n2 2015-01-03 2015-01-03 00:00:00     0.0\n\ndf['diff'] = df['date2'] - df['date1']\ndf['diff'] = df['diff'].dt.total_seconds().astype(int)\n\nprint (df)\n       date1               date2  diff\n0 2015-01-01 2015-01-01 02:00:00  7200\n1 2015-01-02 2015-01-02 01:00:00  3600\n2 2015-01-03 2015-01-03 00:00:00     0\n"
"df[~df.col.str.get(0).isin(['t', 'c'])]\n\n     col\n1  mext1\n3   okl1\n\ndf[~df.col.str.startswith(('t', 'c'))]\n"
'flattened = pd.DataFrame(pivoted.to_records())\n#   subject        date  (\'pills\', 250)  (\'pills\', 500)  (\'pills\', 1000)\n#0        1  10/10/2012             4.0             NaN              NaN\n#1        1  10/11/2012             4.0             NaN              NaN\n#2        1  10/12/2012             NaN             2.0              NaN\n#3        2    1/6/2014             NaN             NaN              1.0\n#4        2    1/7/2014             1.0             1.0              NaN\n#5        2    1/8/2014             3.0             NaN              NaN\n\nflattened.columns = [hdr.replace("(\'pills\', ", "strength.").replace(")", "") \\\n                     for hdr in flattened.columns]\nflattened\n#   subject        date  strength.250  strength.500  strength.1000\n#0        1  10/10/2012           4.0           NaN            NaN\n#1        1  10/11/2012           4.0           NaN            NaN\n#2        1  10/12/2012           NaN           2.0            NaN\n#3        2    1/6/2014           NaN           NaN            1.0\n#4        2    1/7/2014           1.0           1.0            NaN\n#5        2    1/8/2014           3.0           NaN            NaN\n'
"In [6]: df['date'] = pd.to_datetime(df['date'])\n\nIn [7]: df\nOut[7]: \n        date  Revenue\n0 2017-06-02      100\n1 2017-05-23      200\n2 2017-05-20      300\n3 2017-06-22      400\n4 2017-06-21      500\n\n\n\nIn [59]: df.groupby(df['date'].dt.strftime('%B'))['Revenue'].sum().sort_values()\nOut[59]: \ndate\nMay      500\nJune    1000\n"
'In [82]: d\nOut[82]:\n             A   B      C      D\n0     John Doe  45   True  False\n1   Jane Smith  32  False  False\n2  Alan Holmes  55  False   True\n3   Eric Lamar  29   True   True\n\nIn [83]: d.loc[d.C | d.D]\nOut[83]:\n             A   B      C      D\n0     John Doe  45   True  False\n2  Alan Holmes  55  False   True\n3   Eric Lamar  29   True   True\n\nIn [94]: d[d[[\'C\',\'D\']].any(1)]\nOut[94]:\n             A   B      C      D\n0     John Doe  45   True  False\n2  Alan Holmes  55  False   True\n3   Eric Lamar  29   True   True\n\nIn [95]: d.query("C or D")\nOut[95]:\n             A   B      C      D\n0     John Doe  45   True  False\n2  Alan Holmes  55  False   True\n3   Eric Lamar  29   True   True\n\ndf[(df[\'C\']==True) | (df[\'D\']==True)]\n\nIn [11]: df = pd.DataFrame({"col":[True, True, True]})\n\nIn [12]: df\nOut[12]:\n    col\n0  True\n1  True\n2  True\n\nIn [13]: df["col"] is True\nOut[13]: False               # &lt;----- oops, that\'s not exactly what we wanted\n'
'df.plot(ax = ax, color = \'black\', linewidth = 0.4, x_compat=True)\n\nax.plot(df.index, df.values, color = \'black\', linewidth = 0.4)\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nidx = pd.date_range(\'2017-01-01 05:03\', \'2017-01-01 18:03\', freq = \'min\')\ndf = pd.Series(np.random.randn(len(idx)),  index = idx)\n\nfig, ax = plt.subplots()\nhours = mdates.HourLocator(interval = 1)\nh_fmt = mdates.DateFormatter(\'%H:%M:%S\')\n\nax.plot(df.index, df.values, color = \'black\', linewidth = 0.4)\n#or use\ndf.plot(ax = ax, color = \'black\', linewidth = 0.4, x_compat=True)\n#Then tick and format with matplotlib:\nax.xaxis.set_major_locator(hours)\nax.xaxis.set_major_formatter(h_fmt)\n\nfig.autofmt_xdate()\nplt.show()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nidx = pd.date_range(\'2017-01-01 05:03\', \'2017-01-01 18:03\', freq = \'min\')\n\ndf = pd.DataFrame(np.cumsum(np.random.randn(len(idx), 2),0), \n                  index = idx, columns=list("AB"))\n\nfig, ax = plt.subplots()\nax.plot(df.index, df["A"], color = \'black\')\nax2 = ax.twinx()\nax2.plot(df.index, df["B"], color = \'indigo\')\n\nhours = mdates.HourLocator(interval = 1)\nh_fmt = mdates.DateFormatter(\'%H:%M:%S\')\nax.xaxis.set_major_locator(hours)\nax.xaxis.set_major_formatter(h_fmt)\n\nfig.autofmt_xdate()\nplt.show()\n'
'import pandas as pd\ndf = pd.read_csv("sample.csv", usecols = [\'name\',\'last_name\'])\n\nimport pandas as pd\ndf = pd.read_csv("sample.csv", usecols = [i for i in range(n)])\n\n# Read column names from file\ncols = list(pd.read_csv("sample_data.csv", nrows =1))\nprint(cols)\n\n# Use list comprehension to remove the unwanted column in **usecol**\ndf= pd.read_csv("sample_data.csv", usecols =[i for i in cols if i != \'name\'])\n'
"df = df.dropna(axis=0, subset=['Charge_Per_Line'])\n\nimport numpy as np\n\ndf['Charge_Per_Line'] = df['Charge_Per_Line'].replace('-', np.nan)\ndf = df.dropna(axis=0, subset=['Charge_Per_Line'])\n"
"df.groupby('city').filter(lambda x : len(x)&gt;3)\nOut[1743]: \n  city\n0  NYC\n1  NYC\n2  NYC\n3  NYC\n\nsub_df = df[df.groupby('city').city.transform('count')&gt;3].copy() \n# add copy for future warning when you need to modify the sub df\n"
"df2 = pd.melt(df.reset_index(), id_vars='index',value_vars=['asset1','asset2'])\nprint (df2)\n    index variable  value\n0  coper1   asset1      1\n1  coper2   asset1      3\n2  coper3   asset1      5\n3  coper1   asset2      2\n4  coper2   asset2      4\n5  coper3   asset2      6\n\nprint(df2.pivot(index='index',columns = 'variable', values = 'value'))\nvariable  asset1  asset2\nindex                   \ncoper1         1       2\ncoper2         3       4\ncoper3         5       6\n\ndf2 = df.stack().reset_index()\ndf2.columns = list('abc')\nprint (df2)\n        a       b  c\n0  coper1  asset1  1\n1  coper1  asset2  2\n2  coper2  asset1  3\n3  coper2  asset2  4\n4  coper3  asset1  5\n5  coper3  asset2  6\n\nprint(df2.pivot(index='a',columns = 'b', values = 'c'))\nb       asset1  asset2\na                     \ncoper1       1       2\ncoper2       3       4\ncoper3       5       6\n"
"contains = [frame['a'].str.contains(i) for i in letters]\nresul = frame[np.all(contains, axis=0)]\n\n       a\n0  a,b,c\n1  a,c,f\n3  a,z,c\n"
"idx = df.groupby('ID')['year'].apply(lambda x: pd.Series(np.arange(x.iloc[0], x.iloc[-1]+1))).reset_index()\ndf.set_index(['ID','year']).reindex(pd.MultiIndex.from_arrays([idx['ID'], idx['year']]), fill_value=0).reset_index()\n\n  ID  year  number\n0  A  2017       1\n1  A  2018       0\n2  A  2019       1\n3  B  2017       1\n4  B  2018       1\n5  C  2016       1\n6  C  2017       0\n7  C  2018       0\n8  C  2019       1\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; Sr1 = pd.Series([1,2,3,4], index = ['A', 'B', 'C', 'D'])\n&gt;&gt;&gt; Sr2 = pd.Series([5,6], index = ['A', 'C'])\n&gt;&gt;&gt; Sr1+Sr2\nA     6\nB   NaN\nC     9\nD   NaN\n&gt;&gt;&gt; Sr1.add(Sr2, fill_value=0)\nA    6\nB    2\nC    9\nD    4\n"
'def rolling_max_dd(x, window_size, min_periods=1):\n    """Compute the rolling maximum drawdown of `x`.\n\n    `x` must be a 1d numpy array.\n    `min_periods` should satisfy `1 &lt;= min_periods &lt;= window_size`.\n\n    Returns an 1d array with length `len(x) - min_periods + 1`.\n    """\n    if min_periods &lt; window_size:\n        pad = np.empty(window_size - min_periods)\n        pad.fill(x[0])\n        x = np.concatenate((pad, x))\n    y = windowed_view(x, window_size)\n    running_max_y = np.maximum.accumulate(y, axis=1)\n    dd = y - running_max_y\n    return dd.min(axis=1)\n\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef windowed_view(x, window_size):\n    """Creat a 2d windowed view of a 1d array.\n\n    `x` must be a 1d numpy array.\n\n    `numpy.lib.stride_tricks.as_strided` is used to create the view.\n    The data is not copied.\n\n    Example:\n\n    &gt;&gt;&gt; x = np.array([1, 2, 3, 4, 5, 6])\n    &gt;&gt;&gt; windowed_view(x, 3)\n    array([[1, 2, 3],\n           [2, 3, 4],\n           [3, 4, 5],\n           [4, 5, 6]])\n    """\n    y = as_strided(x, shape=(x.size - window_size + 1, window_size),\n                   strides=(x.strides[0], x.strides[0]))\n    return y\n\n\ndef rolling_max_dd(x, window_size, min_periods=1):\n    """Compute the rolling maximum drawdown of `x`.\n\n    `x` must be a 1d numpy array.\n    `min_periods` should satisfy `1 &lt;= min_periods &lt;= window_size`.\n\n    Returns an 1d array with length `len(x) - min_periods + 1`.\n    """\n    if min_periods &lt; window_size:\n        pad = np.empty(window_size - min_periods)\n        pad.fill(x[0])\n        x = np.concatenate((pad, x))\n    y = windowed_view(x, window_size)\n    running_max_y = np.maximum.accumulate(y, axis=1)\n    dd = y - running_max_y\n    return dd.min(axis=1)\n\n\ndef max_dd(ser):\n    max2here = pd.expanding_max(ser)\n    dd2here = ser - max2here\n    return dd2here.min()\n\n\nif __name__ == "__main__":\n    np.random.seed(0)\n    n = 100\n    s = pd.Series(np.random.randn(n).cumsum())\n\n    window_length = 10\n\n    rolling_dd = pd.rolling_apply(s, window_length, max_dd, min_periods=0)\n    df = pd.concat([s, rolling_dd], axis=1)\n    df.columns = [\'s\', \'rol_dd_%d\' % window_length]\n    df.plot(linewidth=3, alpha=0.4)\n\n    my_rmdd = rolling_max_dd(s.values, window_length, min_periods=1)\n    plt.plot(my_rmdd, \'g.\')\n\n    plt.show()\n\nIn [2]: %timeit rolling_dd = pd.rolling_apply(s, window_length, max_dd, min_periods=0)\n1 loops, best of 3: 247 ms per loop\n\nIn [3]: %timeit my_rmdd = rolling_max_dd(s.values, window_length, min_periods=1)\n10 loops, best of 3: 38.2 ms per loop\n'
"In [56]:\n\ndf = pd.DataFrame({'a':[1,0,0,1,3], 'b':[0,0,1,0,1], 'c':[0,0,0,0,0]})\ndf\nOut[56]:\n   a  b  c\n0  1  0  0\n1  0  0  0\n2  0  1  0\n3  1  0  0\n4  3  1  0\nIn [64]:\n\n(df == 0).astype(int).sum(axis=1)\nOut[64]:\n0    2\n1    3\n2    2\n3    2\n4    1\ndtype: int64\n\nIn [65]:\n\n(df == 0)\nOut[65]:\n       a      b     c\n0  False   True  True\n1   True   True  True\n2   True  False  True\n3  False   True  True\n4  False  False  True\nIn [66]:\n\n(df == 0).astype(int)\nOut[66]:\n   a  b  c\n0  0  1  1\n1  1  1  1\n2  1  0  1\n3  0  1  1\n4  0  0  1\n\n(df == 0).sum(axis=1)\n"
"&gt;&gt;&gt; s\nhigh        3909\naverage     3688\nless         182\nveryless      62\ndtype: int64\n&gt;&gt;&gt; type(s)\n&lt;class 'pandas.core.series.Series'&gt;\n&gt;&gt;&gt; dict(s)\n{'high': 3909, 'average': 3688, 'veryless': 62, 'less': 182}\n&gt;&gt;&gt; s.to_dict()\n{'high': 3909, 'average': 3688, 'veryless': 62, 'less': 182}\n"
'def my_cut (x, bins,\n            lower_infinite=True, upper_infinite=True,\n            **kwargs):\n    r"""Wrapper around pandas cut() to create infinite lower/upper bounds with proper labeling.\n\n    Takes all the same arguments as pandas cut(), plus two more.\n\n    Args :\n        lower_infinite (bool, optional) : set whether the lower bound is infinite\n            Default is True. If true, and your first bin element is something like 20, the\n            first bin label will be \'&lt;= 20\' (depending on other cut() parameters)\n        upper_infinite (bool, optional) : set whether the upper bound is infinite\n            Default is True. If true, and your last bin element is something like 20, the\n            first bin label will be \'&gt; 20\' (depending on other cut() parameters)\n        **kwargs : any standard pandas cut() labeled parameters\n\n    Returns :\n        out : same as pandas cut() return value\n        bins : same as pandas cut() return value\n    """\n\n    # Quick passthru if no infinite bounds\n    if not lower_infinite and not upper_infinite:\n        return pd.cut(x, bins, **kwargs)\n\n    # Setup\n    num_labels      = len(bins) - 1\n    include_lowest  = kwargs.get("include_lowest", False)\n    right           = kwargs.get("right", True)\n\n    # Prepend/Append infinities where indiciated\n    bins_final = bins.copy()\n    if upper_infinite:\n        bins_final.insert(len(bins),float("inf"))\n        num_labels += 1\n    if lower_infinite:\n        bins_final.insert(0,float("-inf"))\n        num_labels += 1\n\n    # Decide all boundary symbols based on traditional cut() parameters\n    symbol_lower  = "&lt;=" if include_lowest and right else "&lt;"\n    left_bracket  = "(" if right else "["\n    right_bracket = "]" if right else ")"\n    symbol_upper  = "&gt;" if right else "&gt;="\n\n    # Inner function reused in multiple clauses for labeling\n    def make_label(i, lb=left_bracket, rb=right_bracket):\n        return "{0}{1}, {2}{3}".format(lb, bins_final[i], bins_final[i+1], rb)\n\n    # Create custom labels\n    labels=[]\n    for i in range(0,num_labels):\n        new_label = None\n\n        if i == 0:\n            if lower_infinite:\n                new_label = "{0} {1}".format(symbol_lower, bins_final[i+1])\n            elif include_lowest:\n                new_label = make_label(i, lb="[")\n            else:\n                new_label = make_label(i)\n        elif upper_infinite and i == (num_labels - 1):\n            new_label = "{0} {1}".format(symbol_upper, bins_final[i])\n        else:\n            new_label = make_label(i)\n\n        labels.append(new_label)\n\n    # Pass thru to pandas cut()\n    return pd.cut(x, bins_final, labels=labels, **kwargs)\n'
"df['hb'] - 5 #Where `df` is your dataframe.\n\nIn [43]: df\nOut[43]:\n  name  age  hb\n0  ali   34  14\n1  jex   16  13\n2  aja   24  16\n3  joy   23  12\n\nIn [44]: df['hb'] - 5\nOut[44]:\n0     9\n1     8\n2    11\n3     7\nName: hb, dtype: int64\n"
"df = pd.DataFrame([1, 0, 2, 3, 0], columns=['a'])\ndf = df.replace(0, np.NaN)\ndf.mean()\n"
'import seaborn as sns\nimport matplotlib.pylab as plt\nsns.set_style("whitegrid")\ntips = sns.load_dataset("tips")\nax = sns.stripplot(x="sex", y="total_bill", hue="day", data=tips, jitter=True)\n\n# remove legend from axis \'ax\'\nax.legend_.remove()\n\nplt.show()\n'
"df2 = pd.read_csv('et_users.csv', header=None, names=names2, chunksize=100000)\nchunks=[]\nfor chunk in df2:\n    chunk['ID'] = chunk.ID.map(rep.set_index('member_id')['panel_mm_id'])\n    chunks.append(chunk)\n\ndf2 = pd.concat(chunks, ignore_index=True)\n"
'from dask.distributed import Client\nclient = Client()\n\ndf = dask.dataframe.read_csv(...)\ndf.to_parquet(...)\n\nimport dask.dataframe as ddf\nimport dask.multiprocessing\n\ndf = ddf.read_csv("data/Measurements*.csv",  # read in parallel\n             sep=\';\', \n             parse_dates=["DATETIME"], \n             blocksize=1000000,\n             )\n\ndf = df.compute(get=dask.multiprocessing.get)     # convert to pandas\n\ndf[\'Type\'] = df[\'Type\'].astype(\'category\')\ndf[\'Condition\'] = df[\'Condition\'].astype(\'category\')\n\ndf.to_hdf(\'data/data.hdf\', \'Measurements\', format=\'table\', mode=\'w\')\n'
"y_hats2 = model.predict(X)\n\ndf['y_hats'] = y_hats2\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport pandas as pd\nimport numpy as np\n\ndata = load_iris()\n\n# bear with me for the next few steps... I'm trying to walk you through\n# how my data object landscape looks... i.e. how I get from raw data \n# to matrices with the actual data I have, not the iris dataset\n# put feature matrix into columnar format in dataframe\ndf = pd.DataFrame(data = data.data)\n\n# add outcome variable\ndf_class = pd.DataFrame(data = data.target)\n\n# finally, split into train-test\nX_train, X_test, y_train, y_test = train_test_split(df,df_class, train_size = 0.8)\n\nmodel = DecisionTreeClassifier()\n\nmodel.fit(X_train, y_train)\n\n# I've got my predictions now\ny_hats = model.predict(X_test)\n\ny_test['preds'] = y_hats\n\ndf_out = pd.merge(df,y_test[['preds']],how = 'left',left_index = True, right_index = True)\n"
"df['diff'] = df.sort_values(['id','time']).groupby('id')['time'].diff()\nprint (df)\n  id                time     diff\n0  A 2016-11-25 16:32:17      NaT\n1  A 2016-11-25 16:36:04 00:00:35\n2  A 2016-11-25 16:35:29 00:03:12\n3  B 2016-11-25 16:35:24      NaT\n4  B 2016-11-25 16:35:46 00:00:22\n\ndf = df.dropna(subset=['diff'])\nprint (df)\n  id                time     diff\n2  A 2016-11-25 16:35:29 00:03:12\n1  A 2016-11-25 16:36:04 00:00:35\n4  B 2016-11-25 16:35:46 00:00:22\n\ndf.time = df.sort_values(['id','time']).groupby('id')['time'].diff()\nprint (df)\n  id     time\n0  A      NaT\n1  A 00:00:35\n2  A 00:03:12\n3  B      NaT\n4  B 00:00:22\n\ndf.time = df.sort_values(['id','time']).groupby('id')['time'].diff()\ndf = df.dropna(subset=['time'])\nprint (df)\n  id     time\n1  A 00:00:35\n2  A 00:03:12\n4  B 00:00:22\n"
"In [193]: df\nOut[193]:\n              name\n0        John, Doe\n1  Max, Mustermann\n\nIn [194]: df.name.replace({r'(\\w+),\\s+(\\w+)' : r'\\2 \\1'}, regex=True)\nOut[194]:\n0          Doe John\n1    Mustermann Max\nName: name, dtype: object\n\nIn [195]: df.name.replace({r'(\\w+),\\s+(\\w+)' : r'\\2 \\1', 'Max':'Fritz'}, regex=True)\nOut[195]:\n0            Doe John\n1    Mustermann Fritz\nName: name, dtype: object\n"
"In [142]:\ndf['A'].rolling(min_periods=1, window=11).sum()\n\nOut[142]:\n0       NaN\n1      0.00\n2      0.00\n3      3.33\n4     13.54\n5     20.21\n6     27.21\n7     35.48\n8     41.55\n9     43.72\n10    47.10\n11    49.58\n12    51.66\n13    58.61\n14    55.28\n15    46.82\n16    46.81\n17    49.50\n18    47.96\n19    48.09\n20    48.93\n21    45.87\n22    43.91\nName: A, dtype: float64\n"
'from pandas.tools.plotting import scatter_matrix\n\nfrom pandas.plotting import scatter_matrix\n'
"df = df.dropna(how='any',axis=0) \n\n#Recreate random DataFrame with Nan values\ndf = pd.DataFrame(index = pd.date_range('2017-01-01', '2017-01-10', freq='1d'))\n# Average speed in miles per hour\ndf['A'] = np.random.randint(low=198, high=205, size=len(df.index))\ndf['B'] = np.random.random(size=len(df.index))*2\n\n#Create dummy NaN value on 2 cells\ndf.iloc[2,1]=None\ndf.iloc[5,0]=None\n\nprint(df)\n                A         B\n2017-01-01  203.0  1.175224\n2017-01-02  199.0  1.338474\n2017-01-03  198.0       NaN\n2017-01-04  198.0  0.652318\n2017-01-05  199.0  1.577577\n2017-01-06    NaN  0.234882\n2017-01-07  203.0  1.732908\n2017-01-08  204.0  1.473146\n2017-01-09  198.0  1.109261\n2017-01-10  202.0  1.745309\n\n#Delete row with dummy value\ndf = df.dropna(how='any',axis=0)\n\nprint(df)\n\n                A         B\n2017-01-01  203.0  1.175224\n2017-01-02  199.0  1.338474\n2017-01-04  198.0  0.652318\n2017-01-05  199.0  1.577577\n2017-01-07  203.0  1.732908\n2017-01-08  204.0  1.473146\n2017-01-09  198.0  1.109261\n2017-01-10  202.0  1.745309\n"
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n&gt;&gt;&gt; df\n   col1  col2\n0     1     3\n1     2     4\n&gt;&gt;&gt; df.iloc[[1]]  # DataFrame result\n   col1  col2\n1     2     4\n&gt;&gt;&gt; df.iloc[1]  # Series result\ncol1    2\ncol2    4\nName: 1, dtype: int64\n\n&gt;&gt;&gt; df.loc[:, ['col2']]\n   col2\n0     3\n1     4\n\n&gt;&gt;&gt; df[['col2']]\n   col2\n0     3\n1     4\n"
'N = 5\ns.groupby(s.index // N).sum()\n     \n0     10\n1     35\n2     60\n3     85\n4    110\n5    135\n6    160\n7    185\n8    210\n9    235\ndtype: int64\n\ns.values.reshape(-1, N).sum(1)\n# array([ 10,  35,  60,  85, 110, 135, 160, 185, 210, 235])\n\nb = np.zeros(len(s) // N)\nnp.add.at(b, s.index // N, s.values)\nb\n# array([ 10.,  35.,  60.,  85., 110., 135., 160., 185., 210., 235.])\n'
'df.reindex(df.index.repeat(df.persons))\nOut[951]: \n   code  .     role ..1  persons\n0   123  .  Janitor   .        3\n0   123  .  Janitor   .        3\n0   123  .  Janitor   .        3\n1   123  .  Analyst   .        2\n1   123  .  Analyst   .        2\n2   321  .   Vallet   .        2\n2   321  .   Vallet   .        2\n3   321  .  Auditor   .        5\n3   321  .  Auditor   .        5\n3   321  .  Auditor   .        5\n3   321  .  Auditor   .        5\n3   321  .  Auditor   .        5\n'
'lm( y ~ x - 1, data)\n\nlm(num_rx ~ ridageyr - 1, data=demoq)\n'
'In [25]: df = DataFrame({1: [2,3,4], 2: [3,4,5]})\n\nIn [26]: df\nOut[26]: \n   1  2\n0  2  3\n1  3  4\n2  4  5\n\nIn [27]: df[2]\nOut[27]: \n0    3\n1    4\n2    5\nName: 2\n\nIn [28]: df[2].replace(4, 17)\nOut[28]: \n0     3\n1    17\n2     5\nName: 2\n\nIn [29]: df[2].replace(4, 17, inplace=True)\nOut[29]: \n0     3\n1    17\n2     5\nName: 2\n\nIn [30]: df\nOut[30]: \n   1   2\n0  2   3\n1  3  17\n2  4   5\n\nIn [47]: df[1]\nOut[47]: \n0    2\n1    3\n2    4\nName: 1\n\nIn [48]: df[1] == 4\nOut[48]: \n0    False\n1    False\n2     True\nName: 1\n\nIn [49]: df[1][df[1] == 4]\nOut[49]: \n2    4\nName: 1\n\nIn [50]: df[1][df[1] == 4] = 19\n\nIn [51]: df\nOut[51]: \n    1   2\n0   2   3\n1   3  17\n2  19   5\n'
"In [157]: plot(test['x'][5:10].values)\nOut[157]: [&lt;matplotlib.lines.Line2D at 0xc38348c&gt;]\n\nIn [158]: plot(test['x'][5:10].reset_index(drop=True))\nOut[158]: [&lt;matplotlib.lines.Line2D at 0xc37e3cc&gt;]\n\nIn [161]: test[5:10].set_index('x')['y'].plot()\nOut[161]: &lt;matplotlib.axes.AxesSubplot at 0xc48b1cc&gt;\n"
'In [116]: df["Date"]\nOut[116]: \n0           2012-10-08 07:12:22\n1           2012-10-08 09:14:00\n2           2012-10-08 09:15:00\n3           2012-10-08 09:15:01\n4    2012-10-08 09:15:01.500000\n5           2012-10-08 09:15:02\n6    2012-10-08 09:15:02.500000\n7           2012-10-10 07:19:30\n8           2012-10-10 09:14:00\n9           2012-10-10 09:15:00\n10          2012-10-10 09:15:01\n11   2012-10-10 09:15:01.500000\n12          2012-10-10 09:15:02\nName: Date\n\nIn [117]: df["Date"][0]\nOut[117]: &lt;Timestamp: 2012-10-08 07:12:22&gt;\n\nIn [118]: df["Date"][0].date()\nOut[118]: datetime.date(2012, 10, 8)\n\nIn [126]: df["Date"].map(lambda t: t.date()).unique()\nOut[126]: array([2012-10-08, 2012-10-10], dtype=object)\n\nIn [127]: df["Date"].map(pd.Timestamp.date).unique()\nOut[127]: array([2012-10-08, 2012-10-10], dtype=object)\n'
"&gt;&gt;&gt; df.groupby('A_id').apply(lambda x: pd.Series(dict(\n    sum_up=(x.B == 'up').sum(),\n    sum_down=(x.B == 'down').sum(),\n    over_200_up=((x.B == 'up') &amp; (x.C &gt; 200)).sum()\n)))\n      over_200_up  sum_down  sum_up\nA_id                               \na1              0         0       1\na2              0         1       0\na3              1         0       2\na4              0         0       0\na5              0         0       0\n"
"import matplotlib.ticker as ticker\n\n# a is an axes object, e.g. from figure.get_axes()\n\n# Hide major tick labels\na.xaxis.set_major_formatter(ticker.NullFormatter())\n\n# Customize minor tick labels\na.xaxis.set_minor_locator(ticker.FixedLocator([1.5,2.5,3.5,4.5,5.5]))\na.xaxis.set_minor_formatter(ticker.FixedFormatter(['1','2','3','4','5']))\n\n# Hide major tick labels\na.set_xticklabels('')\n\n# Customize minor tick labels\na.set_xticks([1.5,2.5,3.5,4.5,5.5],      minor=True)\na.set_xticklabels(['1','2','3','4','5'], minor=True)\n"
"In [11]: g = df.groupby(['A', 'B'])\n\nIn [12]: df1 = df.set_index(['A', 'B'])\n\nIn [13]: df1['D'] = g.size()  # unfortunately this doesn't play nice with as_index=False\n# Same would work with g['C'].sum()\n\nIn [14]: df1.reset_index()\nOut[14]:\n    A  B  C  D\n0   1  5  1  2\n1   1  5  1  2\n2   1  6  1  1\n3   1  7  1  1\n4   2  5  1  1\n5   2  6  1  2\n6   2  6  1  2\n7   3  7  1  2\n8   3  7  1  2\n9   4  6  1  1\n10  4  7  1  2\n11  4  7  1  2\n"
"def func(row):\n    if row['mobile'] == 'mobile':\n        return 'mobile'\n    elif row['tablet'] =='tablet':\n        return 'tablet' \n    else:\n        return 'other'\n\ndf['combo'] = df.apply(func, axis=1)\n"
'mydf[\'Cigarettes\'] = mydf[\'Cigarettes\'].str.replace(\' \', \'\')\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].apply(numcigar.get).astype(float)\n\nmydf[\'CigarNum\'] = mydf[\'Cigarettes\'].replace(numcigar)\n# now convert the types\nmydf[\'CigarNum\'] = mydf[\'CigarNum\'].convert_objects(convert_numeric=True)\n\nnumcigar = {"Never":0.0 ,"1-5 Cigarettes/day" :1.0,"10-20 Cigarettes/day":4.0}\n\nmydf[\'CigarNum\'] = pd.to_numeric(mydf[\'CigarNum\'], errors=\'coerce\')\n'
'In  [99]: from collections import defaultdict\n\nIn [100]: results = defaultdict(lambda: defaultdict(dict))\n\nIn [101]: for index, value in grouped.itertuples():\n     ...:     for i, key in enumerate(index):\n     ...:         if i == 0:\n     ...:             nested = results[key]\n     ...:         elif i == len(index) - 1:\n     ...:             nested[key] = value\n     ...:         else:\n     ...:             nested = nested[key]\n\nIn [102]: results\nOut[102]: defaultdict(&lt;function &lt;lambda&gt; at 0x7ff17c76d1b8&gt;, {2010: defaultdict(&lt;type \'dict\'&gt;, {\'govnr\': {\'pati mara\': 500.0, \'jess rapp\': 80.0}, \'mayor\': {\'joe smith\': 100.0, \'jay gould\': 12.0}})})\n\nIn [106]: print json.dumps(results, indent=4)\n{\n    "2010": {\n        "govnr": {\n            "pati mara": 500.0, \n            "jess rapp": 80.0\n        }, \n        "mayor": {\n            "joe smith": 100.0, \n            "jay gould": 12.0\n        }\n    }\n}\n'
'data(iris)\nhead(iris, 10)\ntail(iris, 10)\n\nimport pandas as pd\nfrom sklearn import datasets\niris = pd.DataFrame(datasets.load_iris().data)\niris.head(10)\niris.tail(10)\n\niris.ix[:,1:2].head(10)\n'
"# empty frame with desired index\nrs = pd.DataFrame(index=df.resample('15min').iloc[1:].index)\n\n# array of indexes corresponding with closest timestamp after resample\nidx_after = np.searchsorted(df.index.values, rs.index.values)\n\n# values and timestamp before/after resample\nrs['after'] = df.loc[df.index[idx_after], 'Values'].values\nrs['before'] = df.loc[df.index[idx_after - 1], 'Values'].values\nrs['after_time'] = df.index[idx_after]\nrs['before_time'] = df.index[idx_after - 1]\n\n#calculate new weighted value\nrs['span'] = (rs['after_time'] - rs['before_time'])\nrs['after_weight'] = (rs['after_time'] - rs.index) / rs['span']\n# I got errors here unless I turn the index to a series\nrs['before_weight'] = (pd.Series(data=rs.index, index=rs.index) - rs['before_time']) / rs['span']\n\nrs['Values'] = rs.eval('before * before_weight + after * after_weight')\n\nIn [161]: rs['Values']\nOut[161]: \n1992-08-27 08:00:00    28.011429\n1992-08-27 08:15:00    28.313939\n1992-08-27 08:30:00    28.223030\n1992-08-27 08:45:00    28.952000\n1992-08-27 09:00:00    29.908571\nFreq: 15T, Name: Values, dtype: float64\n"
"In [10]: df.groupby('indx').transform(lambda x: (x - x.mean()) / x.std())\n"
"In [81]: frame[frame.duplicated(['key1', 'key2'], keep=False)].groupby(('key1', 'key2')).min()\nOut[81]: \n           data\nkey1 key2      \n1    2        5\n2    2        1\n\n[2 rows x 1 columns]\n\nIn [76]: frame.duplicated(['key1', 'key2'])\nOut[76]: \n0    False\n1    False\n2    False\n3     True\n4     True\n5    False\n6     True\n7     True\ndtype: bool\n\nIn [77]: frame.duplicated(['key1', 'key2'], take_last=True)\nOut[77]: \n0     True\n1     True\n2    False\n3    False\n4     True\n5    False\n6     True\n7    False\ndtype: bool\n\nIn [78]: frame.duplicated(['key1', 'key2'], take_last=True) | frame.duplicated(['key1', 'key2'])\nOut[78]: \n0     True\n1     True\n2    False\n3     True\n4     True\n5    False\n6     True\n7     True\ndtype: bool\n\nIn [79]: frame[frame.duplicated(['key1', 'key2'], take_last=True) | frame.duplicated(['key1', 'key2'])]\nOut[79]: \n   key1  key2  data\n0     1     2     5\n1     2     2     6\n3     1     2     6\n4     2     2     1\n6     2     2     2\n7     2     2     8\n\n[6 rows x 3 columns]\n\nIn [81]: frame[frame.duplicated(['key1', 'key2'], take_last=True) | frame.duplicated(['key1', 'key2'])].groupby(('key1', 'key2')).min()\nOut[81]: \n           data\nkey1 key2      \n1    2        5\n2    2        1\n\n[2 rows x 1 columns]\n"
'&gt;&gt;&gt; pd.algos.is[TAB]\npd.algos.is_lexsorted          pd.algos.is_monotonic_float64  pd.algos.is_monotonic_object\npd.algos.is_monotonic_bool     pd.algos.is_monotonic_int32\npd.algos.is_monotonic_float32  pd.algos.is_monotonic_int64    \n\n&gt;&gt;&gt; df = pd.DataFrame({"A": [1,2,2], "B": [2,3,1]})\n&gt;&gt;&gt; pd.algos.is_monotonic_int64(df.A.values, False)[0]\nTrue\n&gt;&gt;&gt; pd.algos.is_monotonic_int64(df.B.values, False)[0]\nFalse\n\nIn [32]: pandas.algos.is_lexsorted([np.array([-2, -1], dtype=np.int64)])\nOut[32]: True\nIn [33]: pandas.algos.is_lexsorted([np.array([-2, -1], dtype=float)])\nOut[33]: False\nIn [34]: pandas.algos.is_lexsorted([np.array([-1, -2, 0], dtype=float)])\nOut[34]: True\n'
"df['d'] = df.apply(lambda x: some_func(a = x['a'], b = x['b'], c = x['c']), axis=1)\n"
"all_data['Order Day new'] = all_data['Order Day new'].dt.strftime('%Y-%m-%d')\n\nIn [111]:\n\nall_data = pd.DataFrame({'Order Day new':[dt.datetime(2014,5,9), dt.datetime(2012,6,19)]})\nprint(all_data)\nall_data.info()\n  Order Day new\n0    2014-05-09\n1    2012-06-19\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2 entries, 0 to 1\nData columns (total 1 columns):\nOrder Day new    2 non-null datetime64[ns]\ndtypes: datetime64[ns](1)\nmemory usage: 32.0 bytes\n\nIn [108]:\n\nall_data['Order Day new'] = all_data['Order Day new'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d'))\nall_data\nOut[108]:\n  Order Day new\n0    2014-05-09\n1    2012-06-19\nIn [109]:\n\nall_data.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 2 entries, 0 to 1\nData columns (total 1 columns):\nOrder Day new    2 non-null object\ndtypes: object(1)\nmemory usage: 32.0+ bytes\n"
'df = df[(df.one &gt; 0) | (df.two &gt; 0) | (df.three &gt; 0) &amp; (df.four &lt; 1)]\n'
"new_df = old_df[((old_df['C1'] &gt; 0) &amp; (old_df['C1'] &lt; 20)) &amp; ((old_df['C2'] &gt; 0) &amp; (old_df['C2'] &lt; 20)) &amp; ((old_df['C3'] &gt; 0) &amp; (old_df['C3'] &lt; 20))]\n"
"In [37]:\n\ndf = pd.DataFrame({'date':['2015-02-21 12:08:51']})\ndf\nOut[37]:\n                  date\n0  2015-02-21 12:08:51\nIn [39]:\n\ndf['date'] = pd.to_datetime(df['date']).dt.date\ndf\nOut[39]:\n         date\n0  2015-02-21\n\nIn[10]:\ndf['date'] = pd.to_datetime(df['date']).dt.normalize()\ndf\n\nOut[10]: \n        date\n0 2015-02-21\n\nIn[11]:\ndf.dtypes\n\nOut[11]: \ndate    datetime64[ns]\ndtype: object\n"
"In [125]:\ndf.groupby('positions')['r vals'].filter(lambda x: len(x) &gt;= 3)\n\nOut[125]:\n0    1.2\n2    2.3\n3    1.8\n6    1.9\nName: r vals, dtype: float64\n\nIn [129]:\nfiltered = df.groupby('positions')['r vals'].filter(lambda x: len(x) &gt;= 3)\ndf[df['r vals'].isin(filtered)]\n\nOut[129]:\n   r vals  positions\n0     1.2          1\n1     1.8          2\n2     2.3          1\n3     1.8          1\n6     1.9          1\n\nIn [136]:\ncounts = df['positions'].value_counts()\ncounts\n\nOut[136]:\n1    4\n3    2\n2    1\ndtype: int64\n\nIn [137]:\ncounts[counts &gt; 3]\n\nOut[137]:\n1    4\ndtype: int64\n\nIn [135]:\ndf[df['positions'].isin(counts[counts &gt; 3].index)]\n\nOut[135]:\n   r vals  positions\n0     1.2          1\n2     2.3          1\n3     1.8          1\n6     1.9          1\n\nIn [139]:\nfiltered = df.groupby('positions').filter(lambda x: len(x) &gt;= 3)\nfiltered\n\nOut[139]:\n   r vals  positions\n0     1.2          1\n2     2.3          1\n3     1.8          1\n6     1.9          1\n"
'In [10]: print pd.io.sql.get_schema(df.reset_index(), \'data\')\nCREATE TABLE "data" (\n  "index" TIMESTAMP,\n  "A" REAL,\n  "B" REAL,\n  "C" REAL,\n  "D" REAL\n)\n'
'df2 = df.pivot_table(index=[\'id\',\'num\'], columns=\'q\')\ndf2.columns = df2.columns.droplevel().rename(None)\ndf2.reset_index().fillna("null").to_csv("test.csv", sep="\\t", index=None)\n\nDataError: No numeric types to aggregate\n\ndf2 = df.pivot_table(index=[\'id\',\'num\'], columns=\'q\', aggfunc= lambda x: x)\n'
"(df['Currency'].replace( '[\\$,)]','', regex=True )\n               .replace( '[(]','-',   regex=True ).astype(float))\n\n   Currency\n0         1\n1      2000\n2     -3000\n"
"df['bin'] = pd.cut(df['1'], [0, 50, 100,200])\n\n         0    1        file         bin\n0  person1   24     age.csv     (0, 50]\n1  person2   17     age.csv     (0, 50]\n2  person3   98     age.csv   (50, 100]\n3  person4    6     age.csv     (0, 50]\n4  person2  166  Height.csv  (100, 200]\n5  person3  125  Height.csv  (100, 200]\n6  person5  172  Height.csv  (100, 200]\n\ndf['bin'] = pd.cut(df['1'], [0, 50, 100,200], labels=['0-50', '50-100', '100-200'])\n\n         0    1        file      bin\n0  person1   24     age.csv     0-50\n1  person2   17     age.csv     0-50\n2  person3   98     age.csv   50-100\n3  person4    6     age.csv     0-50\n4  person2  166  Height.csv  100-200\n5  person3  125  Height.csv  100-200\n6  person5  172  Height.csv  100-200\n"
"import matplotlib.pyplot as plt\nplt.scatter(ser.index, ser)\nplt.show()\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\n\nyear = [1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014]\nvalue = np.random.rand(23)\n\nser =  pd.Series(index = year,data=value)\ndf =ser.to_frame()\n\ndf.reset_index(inplace=True)\ndf.columns = ['year','value']\ndf.plot(kind='scatter',x='year',y='value')\nplt.show()\n"
"In [56]: pd.melt(df.reset_index(), id_vars=['id', 'date'], value_vars=['value', 'value2'], var_name='var_name', value_name='value')\nOut[56]: \n   id        date var_name  value\n0   1  2015-09-31    value    100\n1   2  2015-09-31    value     95\n2   3  2015-09-31    value     42\n3   1  2015-09-31   value2    200\n4   2  2015-09-31   value2     57\n5   3  2015-09-31   value2     27\n"
'In [36]: (s &gt; 1).any()\nOut[36]: True\n'
"len(df) - df['a'].count()\n\nlen(df) - df.count()\n\ndfv = dfd['a'].value_counts(dropna=False)\n\n 3     3\nNaN    2\n 1     1\nName: a, dtype: int64\n"
"df = df.loc[~df.index.duplicated(keep='first')]\n\nimport pandas as pd\nfrom pandas import Timestamp\n\ndf1 = pd.DataFrame(\n    {'price': [0.7286, 0.7286, 0.7286, 0.7286],\n     'side': [2, 2, 2, 2],\n     'timestamp': [1451865675631331, 1451865675631400,\n                  1451865675631861, 1451865675631866]},\n    index=pd.DatetimeIndex(['2000-1-1', '2000-1-1', '2001-1-1', '2002-1-1']))\n\n\ndf2 = pd.DataFrame(\n    {'bid': [0.7284, 0.7284, 0.7284, 0.7285, 0.7285],\n     'bid_size': [4000000, 4000000, 5000000, 1000000, 4000000],\n     'offer': [0.7285, 0.729, 0.7286, 0.7286, 0.729],\n     'offer_size': [1000000, 4000000, 4000000, 4000000, 4000000]},\n    index=pd.DatetimeIndex(['2000-1-1', '2001-1-1', '2002-1-1', '2003-1-1', '2004-1-1']))\n\n\ndf1 = df1.loc[~df1.index.duplicated(keep='first')]\n#              price  side         timestamp\n# 2000-01-01  0.7286     2  1451865675631331\n# 2001-01-01  0.7286     2  1451865675631861\n# 2002-01-01  0.7286     2  1451865675631866\n\ndf2 = df2.loc[~df2.index.duplicated(keep='first')]\n#                bid  bid_size   offer  offer_size\n# 2000-01-01  0.7284   4000000  0.7285     1000000\n# 2001-01-01  0.7284   4000000  0.7290     4000000\n# 2002-01-01  0.7284   5000000  0.7286     4000000\n# 2003-01-01  0.7285   1000000  0.7286     4000000\n# 2004-01-01  0.7285   4000000  0.7290     4000000\n\nresult = pd.concat([df1, df2], axis=0)\nprint(result)\n               bid  bid_size   offer  offer_size   price  side     timestamp\n2000-01-01     NaN       NaN     NaN         NaN  0.7286     2  1.451866e+15\n2001-01-01     NaN       NaN     NaN         NaN  0.7286     2  1.451866e+15\n2002-01-01     NaN       NaN     NaN         NaN  0.7286     2  1.451866e+15\n2000-01-01  0.7284   4000000  0.7285     1000000     NaN   NaN           NaN\n2001-01-01  0.7284   4000000  0.7290     4000000     NaN   NaN           NaN\n2002-01-01  0.7284   5000000  0.7286     4000000     NaN   NaN           NaN\n2003-01-01  0.7285   1000000  0.7286     4000000     NaN   NaN           NaN\n2004-01-01  0.7285   4000000  0.7290     4000000     NaN   NaN           NaN\n\nIn [94]: df1.join(df2)\nOut[94]: \n             price  side         timestamp     bid  bid_size   offer  \\\n2000-01-01  0.7286     2  1451865675631331  0.7284   4000000  0.7285   \n2000-01-01  0.7286     2  1451865675631400  0.7284   4000000  0.7285   \n2001-01-01  0.7286     2  1451865675631861  0.7284   4000000  0.7290   \n2002-01-01  0.7286     2  1451865675631866  0.7284   5000000  0.7286   \n\n            offer_size  \n2000-01-01     1000000  \n2000-01-01     1000000  \n2001-01-01     4000000  \n2002-01-01     4000000  \n\nIn [95]: df1.join(df2, how='outer')\nOut[95]: \n             price  side     timestamp     bid  bid_size   offer  offer_size\n2000-01-01  0.7286     2  1.451866e+15  0.7284   4000000  0.7285     1000000\n2000-01-01  0.7286     2  1.451866e+15  0.7284   4000000  0.7285     1000000\n2001-01-01  0.7286     2  1.451866e+15  0.7284   4000000  0.7290     4000000\n2002-01-01  0.7286     2  1.451866e+15  0.7284   5000000  0.7286     4000000\n2003-01-01     NaN   NaN           NaN  0.7285   1000000  0.7286     4000000\n2004-01-01     NaN   NaN           NaN  0.7285   4000000  0.7290     4000000\n"
"import numpy as np\ng = df.groupby('c')['l1','l2'].apply(lambda x: list(np.unique(x)))\n"
'    In [38]: df.unstack()\n    Out[38]: \n        date \n    AA  05/03    1\n        06/03    4\n        07/03    7\n        08/03    5\n    BB  05/03    2\n        06/03    5\n        07/03    8\n        08/03    7\n    CC  05/03    3\n        06/03    6\n        07/03    9\n        08/03    1\n    dtype: int64\n\nIn [39]: df.unstack().reset_index() \nOut[39]:        \n\n    level_0 date    0\n0   AA      05-03   1\n1   AA      06-03   4\n2   AA      07-03   7\n3   AA      08-03   5\n4   BB      05-03   2\n5   BB      06-03   5\n6   BB      07-03   8\n7   BB      08-03   7\n8   CC      05-03   3\n9   CC      06-03   6\n10  CC      07-03   9\n11  CC      08-03   1\n\nIn [40]: pd.DataFrame(df.unstack())     \nOut[40]:        \n\n            0\n    date    \nAA  05-03   1\n    06-03   4\n    07-03   7\n    08-03   5\nBB  05-03   2\n    06-03   5\n    07-03   8\n    08-03   7\nCC  05-03   3\n    06-03   6\n    07-03   9\n    08-03   1\n'
'df[\'EventCount\'] = df[\'Event\'].str.split("/").str.len()\n\ndf[\'EventCount\'] = df[\'Event\'].str.count("/") + 1\n\n         Event  EventCount\n0      abc/def           2\n1          abc           1\n2  abc/def/hij           3\n\n%timeit df[\'Event\'].str.count("/") + 1\n100 loops, best of 3: 3.18 ms per loop\n\n%timeit df[\'Event\'].str.split("/").str.len()\n100 loops, best of 3: 4.28 ms per loop\n\n%timeit df[\'Event\'].str.split("/").apply(len)\n100 loops, best of 3: 4.08 ms per loop\n'
"In [89]:\ncount = df['fruits'].str.split().apply(len).value_counts()\ncount.index = count.index.astype(str) + ' words:'\ncount.sort_index(inplace=True)\ncount\n\nOut[89]:\n1 words:    2\n2 words:    2\n3 words:    1\n4 words:    1\nName: fruits, dtype: int64\n\nIn [41]:\ncount = df['fruits'].str.split().str.len()\ncount.index = count.index.astype(str) + ' words:'\ncount.sort_index(inplace=True)\ncount\n\nOut[41]:\n0 words:    2\n1 words:    1\n2 words:    3\n3 words:    4\n4 words:    2\n5 words:    1\nName: fruits, dtype: int64\n\nIn [42]:\n%timeit df['fruits'].str.split().apply(len).value_counts()\n%timeit df['fruits'].str.split().str.len()\n\n1000 loops, best of 3: 799 µs per loop\n1000 loops, best of 3: 347 µs per loop\n\nIn [51]:\n%timeit df['fruits'].str.split().apply(len).value_counts()\n%timeit df['fruits'].str.split().str.len()\n\n100 loops, best of 3: 6.3 ms per loop\n100 loops, best of 3: 6 ms per loop\n"
"#pandas 0.17.0 and above\ns.plot.bar()\n#pandas below 0.17.0\ns.plot('bar')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ns = pd.Series({16976: 2, 1: 39, 2: 49, 3: 187, 4: 159, \n               5: 158, 16947: 14, 16977: 1, 16948: 7, 16978: 1, 16980: 1},\n               name='article_id')\nprint (s)\n1         39\n2         49\n3        187\n4        159\n5        158\n16947     14\n16948      7\n16976      2\n16977      1\n16978      1\n16980      1\nName: article_id, dtype: int64\n\n\ns.plot.bar()\n\nplt.show()\n"
'a = df.rolling(window=3).mean().shift(-2)\nprint (a)\n           A\n0   3.666667\n1   5.666667\n2  11.333333\n3  18.333333\n4        NaN\n5        NaN\n'
"print (pd.Series(df.values.tolist(), index=df.index))\na    [1, 2, 3, 4]\nb    [5, 6, 7, 8]\ndtype: object\n\nIn [76]: %timeit (pd.Series(df.values.tolist(), index=df.index))\n1000 loops, best of 3: 295 µs per loop\n\nIn [77]: %timeit pd.Series(df.T.to_dict('list'))\n1000 loops, best of 3: 685 µs per loop\n\nIn [78]: %timeit df.T.apply(tuple).apply(list)\n1000 loops, best of 3: 958 µs per loop\n\nfrom string import ascii_letters\nletters = list(ascii_letters)\ndf = pd.DataFrame(np.random.choice(range(10), (52 ** 2, 52)),\n                  pd.MultiIndex.from_product([letters, letters]),\n                  letters)\n\nIn [71]: %timeit (pd.Series(df.values.tolist(), index=df.index))\n100 loops, best of 3: 2.06 ms per loop\n\nIn [72]: %timeit pd.Series(df.T.to_dict('list'))\n1 loop, best of 3: 203 ms per loop\n\nIn [73]: %timeit df.T.apply(tuple).apply(list)\n1 loop, best of 3: 506 ms per loop\n"
"df['High'].nlargest(2)\n"
'print (df.Column1.str.title())\n0    The Apple\n1     The Pear\n2    Green Tea\nName: Column1, dtype: object\n\nprint (df.Column1.str.capitalize())\n0    The apple\n1     The pear\n2    Green tea\nName: Column1, dtype: object\n'
"df = pd.DataFrame({'a':range(10,100)})\ndf.iloc[pd.np.r_[10:12, 25:28]]\n\n     a\n10  20\n11  21\n25  35\n26  36\n27  37\n"
'df.drop(571, inplace=True)\n'
"df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\ndf['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n\ndf['col1'] = np.where(df['col1'] == 0, \n                      np.where(df['col2'] == 0, df['col3'], df['col2']),\n                      df['col1'])\n\ndf = pd.concat([df]*10**4, ignore_index=True)\n\ndef root_nested(df):\n    df['col1'] = np.where(df['col1'] == 0, np.where(df['col2'] == 0, df['col3'], df['col2']), df['col1'])\n    return df\n\ndef root_split(df):\n    df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\n    df['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\n    return df\n\ndef pir2(df):\n    df['col1'] = df.where(df.ne(0), np.nan).bfill(axis=1).col1.fillna(0)\n    return df\n\ndef pir2_2(df):\n    slc = (df.values != 0).argmax(axis=1)\n    return df.values[np.arange(slc.shape[0]), slc]\n\ndef andrew(df):\n    df.col1[df.col1 == 0] = df.col2\n    df.col1[df.col1 == 0] = df.col3\n    return df\n\ndef pablo(df):\n    df['col1'] = df['col1'].replace(0,df['col2'])\n    df['col1'] = df['col1'].replace(0,df['col3'])\n    return df\n\n%timeit root_nested(df.copy())\n100 loops, best of 3: 2.25 ms per loop\n\n%timeit root_split(df.copy())\n100 loops, best of 3: 2.62 ms per loop\n\n%timeit pir2(df.copy())\n100 loops, best of 3: 6.25 ms per loop\n\n%timeit pir2_2(df.copy())\n1 loop, best of 3: 2.4 ms per loop\n\n%timeit andrew(df.copy())\n100 loops, best of 3: 8.55 ms per loop\n"
"In [2]:\n# make some data\ndf = pd.DataFrame(np.random.randn(5,7), columns= ['Survive', 'Age','Fare', 'Group_Size','deck', 'Pclass', 'Title' ])\ndf['Survive'].iloc[2] = np.NaN\ndf\nOut[2]:\n    Survive       Age      Fare  Group_Size      deck    Pclass     Title\n0  1.174206 -0.056846  0.454437    0.496695  1.401509 -2.078731 -1.024832\n1  0.036843  1.060134  0.770625   -0.114912  0.118991 -0.317909  0.061022\n2       NaN -0.132394 -0.236904   -0.324087  0.570660  0.758084 -0.176421\n3 -2.145934 -0.020003 -0.777785    0.835467  1.498284 -1.371325  0.661991\n4 -0.197144 -0.089806 -0.706548    1.621260  1.754292  0.725897  0.860482\n\nIn [3]:\nxtrain = df.loc[df['Survive'].notnull(), ['Age','Fare', 'Group_Size','deck', 'Pclass', 'Title' ]]\nxtrain\n\nOut[3]:\n        Age      Fare  Group_Size      deck    Pclass     Title\n0 -0.056846  0.454437    0.496695  1.401509 -2.078731 -1.024832\n1  1.060134  0.770625   -0.114912  0.118991 -0.317909  0.061022\n3 -0.020003 -0.777785    0.835467  1.498284 -1.371325  0.661991\n4 -0.089806 -0.706548    1.621260  1.754292  0.725897  0.860482\n"
"print (df)\n                         A         B\nDateTime                            \n01-01-2017 03:27       NaN       NaN\n01-01-2017 03:28       NaN       NaN\n01-01-2017 03:29  0.181277 -0.178836\n01-01-2017 03:30  0.186923 -0.183261\n01-01-2017 03:31       NaN       NaN\n01-01-2017 03:32       NaN       NaN\n01-01-2017 03:33  0.181277 -0.178836\n\ndata = df.ffill().bfill()\nprint (data)\n                         A         B\nDateTime                            \n01-01-2017 03:27  0.181277 -0.178836\n01-01-2017 03:28  0.181277 -0.178836\n01-01-2017 03:29  0.181277 -0.178836\n01-01-2017 03:30  0.186923 -0.183261\n01-01-2017 03:31  0.186923 -0.183261\n01-01-2017 03:32  0.186923 -0.183261\n01-01-2017 03:33  0.181277 -0.178836\n\ndata = df.fillna(method='ffill').fillna(method='bfill')\n"
"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.random.randn(100, 6), columns=['a', 'b', 'c', 'd', 'e', 'f'])\n\n\nax1 = df.plot(kind='scatter', x='a', y='b', color='r')    \nax2 = df.plot(kind='scatter', x='c', y='d', color='g', ax=ax1)    \nax3 = df.plot(kind='scatter', x='e', y='f', color='b', ax=ax1)\n\nprint(ax1 == ax2 == ax3)  # True\n"
'from simple_benchmark import BenchmarkBuilder\nb = BenchmarkBuilder()\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n@b.add_function()\ndef sum_pd(df):\n    return df.groupby(\'Group\').Value.sum()\n\n@b.add_function()\ndef sum_fc(df):\n    f, u = pd.factorize(df.Group.values)\n    v = df.Value.values\n    return pd.Series(np.bincount(f, weights=v).astype(int), pd.Index(u, name=\'Group\'), name=\'Value\').sort_index()\n\n@njit\ndef wbcnt(b, w, k):\n    bins = np.arange(k)\n    bins = bins * 0\n    for i in range(len(b)):\n        bins[b[i]] += w[i]\n    return bins\n\n@b.add_function()\ndef sum_nb(df):\n    b, u = pd.factorize(df.Group.values)\n    w = df.Value.values\n    bins = wbcnt(b, w, u.size)\n    return pd.Series(bins, pd.Index(u, name=\'Group\'), name=\'Value\').sort_index()\n\nfrom string import ascii_uppercase\n\ndef creator(n):  # taken from another answer here\n    letters = list(ascii_uppercase)\n    np.random.seed([3,1415])\n    df = pd.DataFrame(dict(\n            Group=np.random.choice(letters, n),\n            Value=np.random.randint(100, size=n)\n        ))\n    return df\n\n@b.add_arguments(\'Rows in DataFrame\')\ndef argument_provider():\n    for exponent in range(4, 22):\n        size = 2**exponent\n        yield size, creator(size)\n\nr = b.run()\n\nr.plot()\n\nr.plot_difference_percentage(relative_to=sum_nb) \n\nr.to_pandas_dataframe()\n\nfrom simple_benchmark import benchmark\nr = benchmark([sum_pd, sum_fc, sum_nb], {2**i: creator(2**i) for i in range(4, 22)}, "Rows in DataFrame")\n\nimport perfplot\nr = perfplot.bench(\n    setup=creator,\n    kernels=[sum_pd, sum_fc, sum_nb],\n    n_range=[2**k for k in range(4, 22)],\n    xlabel=\'Rows in DataFrame\',\n    )\nimport matplotlib.pyplot as plt\nplt.loglog()\nr.plot()\n'
"df = pd.DataFrame(dict(timestamp=pd.to_datetime(['2000-01-01'])))\n\ndf\n\n   timestamp\n0 2000-01-01\n\ndf.timestamp.dt.strftime('%Y-%m-%d')\n\n0    2000-01-01\nName: timestamp, dtype: object\n"
"df = df.groupby(['Col1', 'Col2']).size().reset_index(name='Freq')\nprint (df)\n   Col1  Col2  Freq\n0     1     1     1\n1     1     2     3\n2     3     4     2\n"
"In [79]: df\nOut[79]:\n   Brains  Bodies\n0      42      34\n1      32      23\n\nIn [80]: df['Brains']\nOut[80]:\n0    42\n1    32\nName: Brains, dtype: int64\n\nIn [81]: type(df['Brains'])\nOut[81]: pandas.core.series.Series\n\nIn [82]: df[['Brains']]\nOut[82]:\n   Brains\n0      42\n1      32\n\nIn [83]: type(df[['Brains']])\nOut[83]: pandas.core.frame.DataFrame\n\nIn [84]: df = pd.DataFrame(np.random.rand(5,6), columns=list('abcdef'))\n\nIn [85]: df\nOut[85]:\n          a         b         c         d         e         f\n0  0.065196  0.257422  0.273534  0.831993  0.487693  0.660252\n1  0.641677  0.462979  0.207757  0.597599  0.117029  0.429324\n2  0.345314  0.053551  0.634602  0.143417  0.946373  0.770590\n3  0.860276  0.223166  0.001615  0.212880  0.907163  0.437295\n4  0.670969  0.218909  0.382810  0.275696  0.012626  0.347549\n\nIn [86]: df[['e','a','c']]\nOut[86]:\n          e         a         c\n0  0.487693  0.065196  0.273534\n1  0.117029  0.641677  0.207757\n2  0.946373  0.345314  0.634602\n3  0.907163  0.860276  0.001615\n4  0.012626  0.670969  0.382810\n\nIn [87]: df[['e']]\nOut[87]:\n          e\n0  0.487693\n1  0.117029\n2  0.946373\n3  0.907163\n4  0.012626\n"
"&gt;&gt;&gt; df = DataFrame({'A': range(1, 11), 'B': np.random.randn(10)})\n\n&gt;&gt;&gt; new_df = df.assign(A=1)\n"
"df = df.sort_values(by=['site', 'country', 'date'])\n\ndf['diff'] = df.groupby(['site', 'country'])['score'].diff().fillna(0)\n\ndf\nOut: \n         date    site country  score  diff\n8  2018-01-01      fb      es    100   0.0\n9  2018-01-02      fb      gb    100   0.0\n5  2018-01-01      fb      us     50   0.0\n6  2018-01-02      fb      us     55   5.0\n7  2018-01-03      fb      us    100  45.0\n1  2018-01-01  google      ch     50   0.0\n4  2018-01-02  google      ch     10 -40.0\n0  2018-01-01  google      us    100   0.0\n2  2018-01-02  google      us     70 -30.0\n3  2018-01-03  google      us     60 -10.0\n"
"import io\nimport boto3\n\npickle_buffer = io.BytesIO()\ns3_resource = boto3.resource('s3')\n\nnew_df.to_pickle(pickle_buffer)\ns3_resource.Object(bucket, key).put(Body=pickle_buffer.getvalue())\n"
"df['totalwords'] = df['col'].str.split().str.len()\n\ndf['totalwords'] = df['col'].str.count(' ') + 1\n\ndf['totalwords'] = [len(x.split()) for x in df['col'].tolist()]\n"
'Df1.name.isin(Df2.IDs).astype(int)\n\n0    1\n1    1\n2    0\n3    0\nName: name, dtype: int32\n\nDf1.assign(InDf2=Df1.name.isin(Df2.IDs).astype(int))\n\n   name  InDf2\n0  Marc      1\n1  Jake      1\n2   Sam      0\n3  Brad      0\n\npd.Series(Df1.name.isin(Df2.IDs).values.astype(int), Df1.name.values)\n\nMarc    1\nJake    1\nSam     0\nBrad    0\ndtype: int32\n'
'l = []\nfor tr in table_rows:\n    td = tr.find_all(\'td\')\n    row = [tr.text for tr in td]\n    l.append(row)\npd.DataFrame(l, columns=["A", "B", ...])\n'
'newdf = pd.DataFrame(np.repeat(df.values,3,axis=0))\nnewdf.columns = df.columns\nprint(newdf)\n\n  Person   ID ZipCode  Gender\n0  12345  882   38182  Female\n1  12345  882   38182  Female\n2  12345  882   38182  Female\n3  32917  271   88172    Male\n4  32917  271   88172    Male\n5  32917  271   88172    Male\n6  18273  552   90291  Female\n7  18273  552   90291  Female\n8  18273  552   90291  Female\n\nnewdf = pd.DataFrame(np.repeat(df.values, 3, axis=0), columns=df.columns)\nprint(newdf)\n\n  Person   ID ZipCode  Gender\n0  12345  882   38182  Female\n1  12345  882   38182  Female\n2  12345  882   38182  Female\n3  32917  271   88172    Male\n4  32917  271   88172    Male\n5  32917  271   88172    Male\n6  18273  552   90291  Female\n7  18273  552   90291  Female\n8  18273  552   90291  Female\n'
'df = pd.DataFrame({\'Column1\': ["\'cat\'", "\'toy\'", "\'cat\'"],\n                   \'Column2\': ["\'bat\'", "\'flower\'", "\'bat\'"],\n                   \'Column3\': ["\'xyz\'", "\'abc\'", "\'lmn\'"]})\nprint(df)\n\n  Column1   Column2 Column3\n0   \'cat\'     \'bat\'   \'xyz\'\n1   \'toy\'  \'flower\'   \'abc\'\n2   \'cat\'     \'bat\'   \'lmn\'\n\nresult_df = df.drop_duplicates(subset=[\'Column1\', \'Column2\'], keep=\'first\')\nprint(result_df)\n\n  Column1   Column2 Column3\n0   \'cat\'     \'bat\'   \'xyz\'\n1   \'toy\'  \'flower\'   \'abc\'\n'
"# This will return a NEW DataFrame object, leave the original `df` untouched.\ndf.drop('a', axis=1)  \n# This will modify the `df` inplace. **And return a `None`**.\ndf.drop('a', axis=1, inplace=True)  \n"
'from pandas.plotting import scatter_matrix\n\nscatter_matrix(iris_df, alpha=0.2, figsize=(10, 10))\n'
"df.groupby('group').agg(lambda x : x.head(1) if x.dtype=='object' else x.mean())\nOut[63]: \n      group_color      val1      val2\ngroup                                \nA           green  3.333333  4.666667\nB            blue  4.500000  6.000000\n"
'In [1]: df\nOut[1]:\n   RollBasis  ToRoll\n0          1       1\n1          1       4\n2          1      -5\n3          2       2\n4          3      -4\n5          5      -2\n6          8       0\n7         10     -13\n8         12      -2\n9         13      -5\n\nIn [2]: def f(x):\n   ...:     ser = df.ToRoll[(df.RollBasis &gt;= x) &amp; (df.RollBasis &lt; x+5)]\n   ...:     return ser.sum()\n\nIn [3]: df[\'Rolled\'] = df.RollBasis.apply(f)\n\nIn [4]: df\nOut[4]:\n   RollBasis  ToRoll  Rolled\n0          1       1      -4\n1          1       4      -4\n2          1      -5      -4\n3          2       2      -4\n4          3      -4      -6\n5          5      -2      -2\n6          8       0     -15\n7         10     -13     -20\n8         12      -2      -7\n9         13      -5      -5\n\nIn [1]: from pandas import *\n\nIn [2]: import io\n\nIn [3]: text = """\\\n   ...:    RollBasis  ToRoll\n   ...: 0          1       1\n   ...: 1          1       4\n   ...: 2          1      -5\n   ...: 3          2       2\n   ...: 4          3      -4\n   ...: 5          5      -2\n   ...: 6          8       0\n   ...: 7         10     -13\n   ...: 8         12      -2\n   ...: 9         13      -5\n   ...: """\n\nIn [4]: df = read_csv(io.BytesIO(text), header=0, index_col=0, sep=\'\\s+\')\n'
'_ = plt.hist(...)\n'
'import numpy as np\nimport pandas as pd\nimport os\n\nfname = \'groupby.h5\'\n\n# create a frame\ndf = pd.DataFrame({\'A\': [\'foo\', \'foo\', \'foo\', \'foo\',\n                         \'bar\', \'bar\', \'bar\', \'bar\',\n                         \'foo\', \'foo\', \'foo\'],\n                   \'B\': [\'one\', \'one\', \'one\', \'two\',\n                         \'one\', \'one\', \'one\', \'two\',\n                         \'two\', \'two\', \'one\'],\n                   \'C\': [\'dull\', \'dull\', \'shiny\', \'dull\',\n                         \'dull\', \'shiny\', \'shiny\', \'dull\',\n                         \'shiny\', \'shiny\', \'shiny\'],\n                   \'D\': np.random.randn(11),\n                   \'E\': np.random.randn(11),\n                   \'F\': np.random.randn(11)})\n\n\n# create the store and append, using data_columns where I possibily\n# could aggregate\nwith pd.get_store(fname) as store:\n    store.append(\'df\',df,data_columns=[\'A\',\'B\',\'C\'])\n    print "store:\\n%s" % store\n\n    print "\\ndf:\\n%s" % store[\'df\']\n\n    # get the groups\n    groups = store.select_column(\'df\',\'A\').unique()\n    print "\\ngroups:%s" % groups\n\n    # iterate over the groups and apply my operations\n    l = []\n    for g in groups:\n\n        grp = store.select(\'df\',where = [ \'A=%s\' % g ])\n\n        # this is a regular frame, aggregate however you would like\n        l.append(grp[[\'D\',\'E\',\'F\']].sum())\n\n\n    print "\\nresult:\\n%s" % pd.concat(l, keys = groups)\n\nos.remove(fname)\n\nstore:\n&lt;class \'pandas.io.pytables.HDFStore\'&gt;\nFile path: groupby.h5\n/df            frame_table  (typ-&gt;appendable,nrows-&gt;11,ncols-&gt;6,indexers-&gt;[index],dc-&gt;[A,B,C])\n\ndf:\n      A    B      C         D         E         F\n0   foo  one   dull -0.815212 -1.195488 -1.346980\n1   foo  one   dull -1.111686 -1.814385 -0.974327\n2   foo  one  shiny -1.069152 -1.926265  0.360318\n3   foo  two   dull -0.472180  0.698369 -1.007010\n4   bar  one   dull  1.329867  0.709621  1.877898\n5   bar  one  shiny -0.962906  0.489594 -0.663068\n6   bar  one  shiny -0.657922 -0.377705  0.065790\n7   bar  two   dull -0.172245  1.694245  1.374189\n8   foo  two  shiny -0.780877 -2.334895 -2.747404\n9   foo  two  shiny -0.257413  0.577804 -0.159316\n10  foo  one  shiny  0.737597  1.979373 -0.236070\n\ngroups:Index([bar, foo], dtype=object)\n\nresult:\nbar  D   -0.463206\n     E    2.515754\n     F    2.654810\nfoo  D   -3.768923\n     E   -4.015488\n     F   -6.110789\ndtype: float64\n'
'In [161]: pd.DataFrame(df.values*df2.values, columns=df.columns, index=df.index)\nOut[161]: \n   col1  col2  col3\n1    10   200  3000\n2    10   200  3000\n3    10   200  3000\n4    10   200  3000\n5    10   200  3000\n'
'import pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\nbase = np.array(["2013-01-01 00:00:00"], "datetime64[ns]")\n\na = (np.random.rand(30)*1000000*1000).astype(np.int64)*1000000\nt1 = base + a\nt1.sort()\n\nb = (np.random.rand(10)*1000000*1000).astype(np.int64)*1000000\nt2 = base + b\nt2.sort()\n\nidx = np.searchsorted(t1, t2) - 1\nmask = idx &gt;= 0\n\ndf = pd.DataFrame({"t1":t1[idx][mask], "t2":t2[mask]})\n\n                         t1                         t2\n0 2013-01-02 06:49:13.287000 2013-01-03 16:29:15.612000\n1 2013-01-05 16:33:07.211000 2013-01-05 21:42:30.332000\n2 2013-01-07 04:47:24.561000 2013-01-07 04:53:53.948000\n3 2013-01-07 14:26:03.376000 2013-01-07 17:01:35.722000\n4 2013-01-07 14:26:03.376000 2013-01-07 18:22:13.996000\n5 2013-01-07 14:26:03.376000 2013-01-07 18:33:55.497000\n6 2013-01-08 02:24:54.113000 2013-01-08 12:23:40.299000\n7 2013-01-08 21:39:49.366000 2013-01-09 14:03:53.689000\n8 2013-01-11 08:06:36.638000 2013-01-11 13:09:08.078000\n\nimport pylab as pl\npl.figure(figsize=(18, 4))\npl.vlines(pd.Series(t1), 0, 1, colors="g", lw=1)\npl.vlines(df.t1, 0.3, 0.7, colors="r", lw=2)\npl.vlines(df.t2, 0.3, 0.7, colors="b", lw=2)\npl.margins(0.02)\n'
"In [1]: myDF = DataFrame(data=[[11,11],[22,'2A'],[33,33]], columns = ['A','B'])\n\nIn [2]: myDF.convert_objects(convert_numeric=True)\nOut[2]: \n    A   B\n0  11  11\n1  22 NaN\n2  33  33\n\n[3 rows x 2 columns]\n\nIn [3]: myDF.convert_objects(convert_numeric=True).dtypes\nOut[3]: \nA      int64\nB    float64\ndtype: object\n"
'import pandas as pd\ndf_copy = pd.DataFrame().reindex_like(df_original)\n'
"df1 = df1.join(df2, on=['Body','Season'])\n\n"
"data = read_csv(csv_path, sep=';')\n"
"import datetime as DT\nimport io\nimport numpy as np\nimport pandas as pd\n\npd.options.mode.chained_assignment = 'warn'\n\ncontent = '''     ssno        lname         fname    pos_title             ser  gender  dob \n0    23456789    PLILEY     JODY        BUDG ANAL             0560  F      031871 \n1    987654321   NOEL       HEATHER     PRTG SRVCS SPECLST    1654  F      120852\n2    234567891   SONJU      LAURIE      SUPVY CONTR SPECLST   1102  F      010999\n3    345678912   MANNING    CYNTHIA     SOC SCNTST            0101  F      081692\n4    456789123   NAUERTZ    ELIZABETH   OFF AUTOMATION ASST   0326  F      031387'''\n\ndf = pd.read_csv(io.StringIO(content), sep='\\s{2,}')\ndf['dob'] = df['dob'].apply('{:06}'.format)\n\nnow = pd.Timestamp('now')\ndf['dob'] = pd.to_datetime(df['dob'], format='%m%d%y')    # 1\ndf['dob'] = df['dob'].where(df['dob'] &lt; now, df['dob'] -  np.timedelta64(100, 'Y'))   # 2\ndf['age'] = (now - df['dob']).astype('&lt;m8[Y]')    # 3\nprint(df)\n\n        ssno    lname      fname            pos_title   ser gender  \\\n0   23456789   PLILEY       JODY            BUDG ANAL   560      F   \n1  987654321     NOEL    HEATHER   PRTG SRVCS SPECLST  1654      F   \n2  234567891    SONJU     LAURIE  SUPVY CONTR SPECLST  1102      F   \n3  345678912  MANNING    CYNTHIA           SOC SCNTST   101      F   \n4  456789123  NAUERTZ  ELIZABETH  OFF AUTOMATION ASST   326      F   \n\n                  dob  age  \n0 1971-03-18 00:00:00   43  \n1 1952-12-08 18:00:00   61  \n2 1999-01-09 00:00:00   15  \n3 1992-08-16 00:00:00   22  \n4 1987-03-13 00:00:00   27  \n"
'import unittest\nimport pandas as pd\nfrom pandas.util.testing import assert_frame_equal # &lt;-- for testing dataframes\n\nclass DFTests(unittest.TestCase):\n\n    """ class for running unittests """\n\n    def setUp(self):\n        """ Your setUp """\n        TEST_INPUT_DIR = \'data/\'\n        test_file_name =  \'testdata.csv\'\n        try:\n            data = pd.read_csv(INPUT_DIR + test_file_name,\n                sep = \',\',\n                header = 0)\n        except IOError:\n            print \'cannot open file\'\n        self.fixture = data\n\n    def test_dataFrame_constructedAsExpected(self):\n        """ Test that the dataframe read in equals what you expect"""\n        foo = pd.DataFrame()\n        assert_frame_equal(self.fixture, foo)\n'
'df[\'workclass\'].replace(\'?\', np.NaN)\n\ndf.replace(\'?\', np.NaN)\n\n54, ?, 180211, Some-college, 10, Married-civ-spouse, ?, Husband, Asian-Pac-Islander, Male, 0, 0, 60, South, &gt;50K\n\nrawfile = pd.read_csv(filename, header=None, names=DataLabels, sep=\',\\s\', na_values=["?"])\n\n27      54               NaN  180211  Some-college             10 \n'
"df[col].plot(kind='pie', autopct='%.2f', labels=['','','',''],  ax=ax, title=col, fontsize=10)\nax.legend(loc=3, labels=df.index)\n\n... labels=None ...\n"
'df.head(50)\n\ndf.iloc[:, : 50]\n'
"with open('foo.csv', 'w') as f:\n    data.to_csv(f, index=True, header=True, decimal=',', sep=' ', float_format='%.3f')\n\ndata.reset_index().to_csv('foo.csv', index=False, header=True, decimal=',', sep=' ', float_format='%.3f')\n"
"for col in df.columns:\n    if len(df[col].unique()) == 1:\n        df.drop(col,inplace=True,axis=1)\n\nres = df\nfor col in df.columns:\n    if len(df[col].unique()) == 1:\n        res = res.drop(col,axis=1)\n\nIn [154]: df = pd.DataFrame([[1,2,3],[1,3,3],[1,2,3]])\n\nIn [155]: for col in df.columns:\n   .....:     if len(df[col].unique()) == 1:\n   .....:         df.drop(col,inplace=True,axis=1)\n   .....:\n\nIn [156]: df\nOut[156]:\n   1\n0  2\n1  3\n2  2\n\nIn [166]: %paste\ndef func1(df):\n        res = df\n        for col in df.columns:\n                if len(df[col].unique()) == 1:\n                        res = res.drop(col,axis=1)\n        return res\n\n## -- End pasted text --\n\nIn [172]: df = pd.DataFrame({'a':1, 'b':np.arange(5), 'c':[0,0,2,2,2]})\n\nIn [178]: %timeit func1(df)\n1000 loops, best of 3: 1.05 ms per loop\n\nIn [180]: %timeit df[df.apply(pd.Series.value_counts).dropna(thresh=2, axis=1).columns]\n100 loops, best of 3: 8.81 ms per loop\n\nIn [181]: %timeit df.apply(pd.Series.value_counts).dropna(thresh=2, axis=1)\n100 loops, best of 3: 5.81 ms per loop\n"
"In [3]:\ndf = pd.DataFrame({'a':['asd',np.NaN,'asdsa'], 'b':['asdas','asdas',np.NaN]})\ndf\n\nOut[3]:\n       a      b\n0    asd  asdas\n1    NaN  asdas\n2  asdsa    NaN\n\nIn [7]:\ndf['a+b'] = df.fillna('').sum(axis=1)\ndf\n\nOut[7]:\n       a      b       a+b\n0    asd  asdas  asdasdas\n1    NaN  asdas     asdas\n2  asdsa    NaN     asdsa\n"
"In [32]:\ntest3.index  = ['f','g','z']\n\ntest3\nOut[32]:\nf    1\ng    2\nz    3\ndtype: int64\n"
"import pandas as pd\nimport numpy as np\n\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randn(8, 4),columns=['A', 'B', 'C', 'D'])\n\ndf = df.set_index('A')\nprint df\n\n\n                  B         C         D\nA                                      \n 1.764052  0.400157  0.978738  2.240893\n 1.867558 -0.977278  0.950088 -0.151357\n-0.103219  0.410599  0.144044  1.454274\n 0.761038  0.121675  0.443863  0.333674\n 1.494079 -0.205158  0.313068 -0.854096\n-2.552990  0.653619  0.864436 -0.742165\n 2.269755 -1.454366  0.045759 -0.187184\n 1.532779  1.469359  0.154947  0.378163\n\n\ndf.loc[1.764052345967664]\nOut[32]: \nB    0.400157\nC    0.978738\nD    2.240893\nName: 1.76405234597, dtype: float64\n"
"df['col'].apply(', '.join)\n"
'resp = make_response(df.to_csv())\nresp.headers["Content-Disposition"] = "attachment; filename=export.csv"\nresp.headers["Content-Type"] = "text/csv"\nreturn resp\n'
"df.groupby('Id', as_index=False).agg(lambda x: set(x))\n"
"pd.concat([r, s], axis=1)\nOut: \n   rrr  sss\n0    0    0\n1    3    5\n2    6   10\n3    9   15\n4   12   20\n5   15   25\n6   18   30\n7   21   35\n8   24   40\n9   27   45\n\npd.DataFrame({'r': r, 's': s})\n\nOut: \n    r   s\n0   0   0\n1   3   5\n2   6  10\n3   9  15\n4  12  20\n5  15  25\n6  18  30\n7  21  35\n8  24  40\n9  27  45\n"
'df = pd.DataFrame({\'Points\':[50,20,38,90,0, np.Inf],\n                   \'Player\':[\'a\',\'a\',\'a\',\'s\',\'s\',\'s\']})\n\nprint (df)\n  Player     Points\n0      a  50.000000\n1      a  20.000000\n2      a  38.000000\n3      s  90.000000\n4      s   0.000000\n5      s        inf\n\npoints_series = df.query("Points &lt; inf").groupby("Player").agg({"Points": "sum"})[\'Points\']\nprint (points_series)     \na = points_series[points_series &gt; 100]\nprint (a)     \nPlayer\na    108.0\nName: Points, dtype: float64\n\n\npoints_series = df.query("Points &lt; inf")\n                  .groupby("Player")\n                  .agg({"Points": "sum"})\n                  .query("Points &gt; 100")\n\nprint (points_series)     \n        Points\nPlayer        \na        108.0\n\npoints_series = df.query("Points &lt; inf")\n                  .groupby("Player")\n                  .agg({"Points": "sum"})[\'Points\']\n                  .loc[lambda x: x &gt; 100]\n\nprint (points_series)     \nPlayer\na    108.0\nName: Points, dtype: float64\n\nnp.random.seed(1234)\ndf = pd.DataFrame({\n    \'Points\': [np.random.choice([1,3]) for x in range(100)], \n    \'Player\': [np.random.choice(["A","B","C"]) for x in range(100)]})\n\nprint (df.query("Points == 3").Player.value_counts().loc[lambda x: x &gt; 15])\nC    19\nB    16\nName: Player, dtype: int64\n\nprint (df.query("Points == 3").groupby("Player").size().loc[lambda x: x &gt; 15])\nPlayer\nB    16\nC    19\ndtype: int64\n'
"ABB = ABB.asfreq('d')\n\nfor index, row in ABB.iterrows():\n    print(ABB.loc[[index + pd.Timedelta(days = 1)]])\n"
'recv.diff() / recv.index.to_series().diff().dt.total_seconds()\n\n2017-01-20 20:00:00            NaN\n2017-01-20 20:05:00    4521.493333\n2017-01-20 20:10:00    4533.760000\n2017-01-20 20:15:00    4557.493333\n2017-01-20 20:20:00    4536.053333\n2017-01-20 20:25:00    4567.813333\n2017-01-20 20:30:00    4406.160000\n2017-01-20 20:35:00    4366.720000\n2017-01-20 20:40:00    4407.520000\n2017-01-20 20:45:00    4421.173333\nFreq: 300S, dtype: float64\n\nnp.gradient(bytes_in, 300) * 8\n\narray([ 4521.49333333,  4527.62666667,  4545.62666667,  4546.77333333,\n        4551.93333333,  4486.98666667,  4386.44      ,  4387.12      ,\n        4414.34666667,  4421.17333333])\n'
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; my_data = [[500292014600, .00, .00, .00, np.nan],\n              [500292014600, 100.00, .00, .00, np.nan], \n              [500292014600, 11202.00, .00, .00, np.nan]]\n&gt;&gt;&gt; df = pd.DataFrame(my_data, columns=[0,2,3,4,20])\n&gt;&gt;&gt; df\n              0        2    3    4  20\n0  500292014600      0.0  0.0  0.0 NaN\n1  500292014600    100.0  0.0  0.0 NaN\n2  500292014600  11202.0  0.0  0.0 NaN\n\n&gt;&gt;&gt; df.columns = range(df.shape[1])\n&gt;&gt;&gt; df\n              0        1    2    3   4\n0  500292014600      0.0  0.0  0.0 NaN\n1  500292014600    100.0  0.0  0.0 NaN\n2  500292014600  11202.0  0.0  0.0 NaN\n'
"df.loc['mean'] = df.mean()\n\n       diode1  diode2  diode3  diode4\nTime                                 \n0.53      7.0     0.0    10.0    16.0\n1.218    17.0     7.0    14.0    19.0\n1.895    13.0     8.0    16.0    17.0\n2.57      8.0     2.0    16.0    17.0\n3.24     14.0     8.0    17.0    19.0\n3.91     13.0     6.0    17.0    18.0\n4.594    13.0     5.0    16.0    19.0\n5.265     9.0     0.0    12.0    16.0\n5.948    12.0     3.0    16.0    17.0\n6.632    10.0     2.0    15.0    17.0\nmean     11.6     4.1    14.9    17.5\n"
'plt.figure(figsize=(10,5))\nplt.matshow(d.corr(), fignum=1)\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.matshow(d.corr())\n'
'new_cols = {x: y for x, y in zip(df_uk.columns, df_ger.columns)}\ndf_out = df_ger.append(df_uk.rename(columns=new_cols))\n\ndf_ger = pd.read_fwf(StringIO(\n    u"""\n        index  Datum   Zahl1   Zahl2\n        0      1-1-17  1       2\n        1      2-1-17  3       4"""),\n    header=1).set_index(\'index\')\n\ndf_uk = pd.read_fwf(StringIO(\n    u"""\n        index  Date    No1     No2\n        0      1-1-17  5       6\n        1      2-1-17  7       8"""),\n    header=1).set_index(\'index\')\n\nprint(df_uk)\nprint(df_ger)\n\nnew_cols = {x: y for x, y in zip(df_uk.columns, df_ger.columns)}\ndf_out = df_ger.append(df_uk.rename(columns=new_cols))\n\nprint(df_out)\n\n         Date  No1  No2\nindex                  \n0      1-1-17    5    6\n1      2-1-17    7    8\n\n        Datum  Zahl1  Zahl2\nindex                      \n0      1-1-17      1      2\n1      2-1-17      3      4\n\n        Datum  Zahl1  Zahl2\nindex                      \n0      1-1-17      1      2\n1      2-1-17      3      4\n0      1-1-17      5      6\n1      2-1-17      7      8\n'
"import datetime as dt\ndf['Date'] = pd.to_datetime(df['Date'])\n\ninclude = df[df['Date'].dt.year == year]\nexclude = df[df['Date'].dt.year != year]\n"
'pd.Series([1, 2, 3]).is_unique\nTrue\n\npd.Series([1, 2, 2]).is_unique\nFalse\n'
"df = df.reset_index().set_index('age')\nprint (df)\n        name  weight\nage                 \n45   Bertram      65\n75    Donald      85\n21      Hugo      75\n"
"dataframe = dataframe.applymap(lambda x: x.encode('unicode_escape').\n                 decode('utf-8') if isinstance(x, str) else x)\n"
'In [16]: left\nOut[16]: \n            a\n2000-01-01  1\n2000-01-01  1\n2000-01-01  1\n2000-01-02  2\n2000-01-02  2\n2000-01-02  2\n\nIn [17]: right\nOut[17]: \n            b\n2000-01-01  3\n2000-01-01  3\n2000-01-01  3\n2000-01-02  4\n2000-01-02  4\n2000-01-02  4\n\nIn [18]: left.join(right)\nOut[18]: \n            a  b\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-01  1  3\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n2000-01-02  2  4\n'
'returns = (vfiax_monthly.open - vfiax_monthly.open.shift(1))/vfiax_monthly.open.shift(1)\n\n(3*vfiax_monthly.open + 2*vfiax_monthly.open.shift(1))/5\n'
'In [15]: t = pd.Timestamp("2013-02-27 00:00:00+00:00")\n\nIn [16]: df1.index.get_loc(t)\nOut[16]: 3\n\nIn [17]: loc = df1.index.get_loc(t)\n\nIn [18]: df.iloc[loc - 1]\nOut[18]: \nDate    2013-02-26 00:00:00\n                      -0.15\nName: 2, Dtype: object\n\nIn [19]: df1.iloc[slice(max(0, loc-3), min(loc, len(df)))]\n        # the min and max feel slightly hacky (!) but needed incase it\'s within top or bottom 3\nOut[19]:                         \nDate                    \n2013-02-22  0.280001\n2013-02-25  0.109999\n2013-02-26 -0.150000\n\nIn [11]: df = pd.read_clipboard(sep=\'\\s\\s+\', header=None, parse_dates=[0], names=[\'Date\', None])\n\nIn [12]: df\nOut[12]: \n                 Date          \n0 2013-02-22 00:00:00  0.280001\n1 2013-02-25 00:00:00  0.109999\n2 2013-02-26 00:00:00 -0.150000\n3 2013-02-27 00:00:00  0.130001\n4 2013-02-28 00:00:00  0.139999\n\nIn [13]: df1 = df.set_index(\'Date\')\n\nIn [14]: df1\nOut[14]:                \nDate                \n2013-02-22  0.280001\n2013-02-25  0.109999\n2013-02-26 -0.150000\n2013-02-27  0.130001\n2013-02-28  0.139999\n'
"In [16]: import pandas as pd\nIn [17]: p1 = {'name': 'willy', 'age': 10}\nIn [18]: p2 = {'name': 'willy', 'age': 11}\nIn [19]: p3 = {'name': 'zoe', 'age': 10}\nIn [20]: df = pd.DataFrame([p1, p2, p3])\n\nIn [21]: df\nOut[21]: \n   age   name\n0   10  willy\n1   11  willy\n2   10    zoe\n\nIn [22]: df.duplicated('name')\nOut[22]: \n0    False\n1     True\n2    False\n"
"import pandas as pd\n\nIn [1]: grouper = pd.Grouper(freq=&quot;1M&quot;)\n\nIn [2]: df['normed'] = df.groupby(grouper).transform(lambda x: x/x.mean())\n"
"In [20]: df = pd.DataFrame(dict(time = pd.Timestamp('20130102'), \n                                A = np.random.rand(3), \n                 ticker=['SPY','SLV','GLD'])).set_index(['time','ticker'])\n\nIn [21]: df\nOut[21]: \n                          A\ntime       ticker          \n2013-01-02 SPY     0.347209\n           SLV     0.034832\n           GLD     0.280951\n\nIn [22]: p = df.to_panel()\n\nIn [23]: p\nOut[23]: \n&lt;class 'pandas.core.panel.Panel'&gt;\nDimensions: 1 (items) x 1 (major_axis) x 3 (minor_axis)\nItems axis: A to A\nMajor_axis axis: 2013-01-02 00:00:00 to 2013-01-02 00:00:00\nMinor_axis axis: GLD to SPY\n\nIn [24]: p.ix[:,:,['SPY','GLD']]\nOut[24]: \n&lt;class 'pandas.core.panel.Panel'&gt;\nDimensions: 1 (items) x 1 (major_axis) x 2 (minor_axis)\nItems axis: A to A\nMajor_axis axis: 2013-01-02 00:00:00 to 2013-01-02 00:00:00\nMinor_axis axis: SPY to GLD\n"
"df = df.div(df.QT, axis='index')\n\ndf = (df.T / df.QT).T\n"
'np.sqrt(np.square(df).sum(axis=1))\n'
"In [26]: s\n\nfirst  second  third\nbar    doo     one      0.404705\n               two      0.577046\nbaz    bee     one     -1.715002\n               two     -1.039268\nfoo    bop     one     -0.370647\n               two     -1.157892\nqux    bop     one     -1.344312\n               two      0.844885\ndtype: float64\n\nIn [27]: s.groupby(level=['first','second']).sum()\n\nfirst  second\nbar    doo       0.981751\nbaz    bee      -2.754270\nfoo    bop      -1.528539\nqux    bop      -0.499427\ndtype: float64\n"
"&gt;&gt;&gt; df1 = pd.pivot_table(df, values=['D'], rows=['B'], aggfunc=np.sum)\n&gt;&gt;&gt; df2 = pd.pivot_table(df, values=['E'], rows=['B'], aggfunc=np.mean)\n&gt;&gt;&gt; pd.concat((df1, df2), axis=1)\n          D         E\nB                    \nA  1.810847 -0.524178\nB  2.762190 -0.443031\nC  0.867519  0.078460\n\n&gt;&gt;&gt; df3 = pd.pivot_table(df, values=['D','E'], rows=['B'], aggfunc=[np.sum, np.mean])\n&gt;&gt;&gt; df3\n        sum                mean          \n          D         E         D         E\nB                                        \nA  1.810847 -4.193425  0.226356 -0.524178\nB  2.762190 -3.544245  0.345274 -0.443031\nC  0.867519  0.627677  0.108440  0.078460\n&gt;&gt;&gt; df3 = df3.ix[:, [('sum', 'D'), ('mean','E')]]\n&gt;&gt;&gt; df3.columns = ['D', 'E']\n&gt;&gt;&gt; df3\n          D         E\nB                    \nA  1.810847 -0.524178\nB  2.762190 -0.443031\nC  0.867519  0.078460\n\n&gt;&gt;&gt; df.groupby('B').aggregate({'D':np.sum, 'E':np.mean})\n          E         D\nB                    \nA -0.524178  1.810847\nB -0.443031  2.762190\nC  0.078460  0.867519\n"
'from numpy.random import randn\nfrom pandas import DataFrame\nfrom datetime import timedelta as td\nimport dateutil.parser\n\nd = dateutil.parser.parse("2014-01-01")\ndf = DataFrame(randn(6,2), columns=list(\'AB\'), index=[d + td(days=x) for x in range(1,7)])\n\nIn [1]: df\nOut[1]:\n                   A         B\n2014-01-02 -1.172285  1.706200\n2014-01-03  0.039511 -0.320798\n2014-01-04 -0.192179 -0.539397\n2014-01-05 -0.475917 -0.280055\n2014-01-06  0.163376  1.124602\n2014-01-07 -2.477812  0.656750\n\nIn [2]: df[df.index &gt; dateutil.parser.parse("2014-01-04")]\nOut[2]:\n                   A         B\n2014-01-05 -0.475917 -0.280055\n2014-01-06  0.163376  1.124602\n2014-01-07 -2.477812  0.656750\n'
"cols=pd.Series(df.columns)\nfor dup in df.columns[df.columns.duplicated(keep=False)]: \n    cols[df.columns.get_loc(dup)] = ([dup + '.' + str(d_idx) \n                                     if d_idx != 0 \n                                     else dup \n                                     for d_idx in range(df.columns.get_loc(dup).sum())]\n                                    )\ndf.columns=cols\n\n    blah    blah2   blah3   blah.1  blah.2\n 0     0        1       2        3       4\n 1     5        6       7        8       9\n\n#sample df with duplicate blah column\ndf=pd.DataFrame(np.arange(2*5).reshape(2,5))\ndf.columns=['blah','blah2','blah3','blah','blah']\ndf\n\n# you just need the following 4 lines to rename duplicates\n# df is the dataframe that you want to rename duplicated columns\n\ncols=pd.Series(df.columns)\n\nfor dup in cols[cols.duplicated()].unique(): \n    cols[cols[cols == dup].index.values.tolist()] = [dup + '.' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n\n# rename the columns with the cols list.\ndf.columns=cols\n\ndf\n\n    blah    blah2   blah3   blah.1  blah.2\n0   0   1   2   3   4\n1   5   6   7   8   9\n"
'df_long = pd.melt(df, "b", var_name="a", value_name="c")\n\nsns.boxplot(x="a", hue="b", y="c", data=df_long)\n'
"In [32]: s = pd.Series(['a','b','c'])\n\nIn [33]: labels, levels = pd.factorize(s)\n\nIn [35]: labels\nOut[35]: array([0, 1, 2])\n"
'BabyDataSet = zip(names,births)\n\nBabyDataSet = list(zip(names,births))\n'
"&gt;&gt;&gt; !cat castle.dat\nc stuff\nc more header\nc begin data         \n 1 1:.5\n 1 2:6.5\n 1 3:5.3\n&gt;&gt;&gt; df = pd.read_csv('castle.dat', skiprows=3, names=['a', 'b', 'c'], \n                     sep=' |:', engine='python')\n&gt;&gt;&gt; df\n   a  b    c\n0  1  1  0.5\n1  1  2  6.5\n2  1  3  5.3\n"
"import pandas as pd\ndf = pd.DataFrame({'cat':['a','b','c','d'],'val':[1,2,5,10]})\ndf1 = pd.get_dummies(pd.DataFrame({'cat':['a'],'val':[1]}))\ndummies_frame = pd.get_dummies(df)\ndf1.reindex(columns = dummies_frame.columns, fill_value=0)\n\n        val     cat_a   cat_b   cat_c   cat_d\n  0     1       1       0       0       0\n"
"store = pd.HDFStore('store.h5')\nfor ...:\n    ...\n    chunk  # the chunk of the DataFrame (which you want to append)\n    store.append('df', chunk)\n\ndf = store['df']\n\n# note: this doesn't work, see below\nsum(df.groupby().sum() for df in store.select('df', chunksize=50000))\n# equivalent to (but doesn't read in the entire frame)\nstore['df'].groupby().sum()\n\nreduce(lambda x, y: x.add(y, fill_value=0),\n       (df.groupby().sum() for df in store.select('df', chunksize=50000)))\n\nchunks = (df.groupby().sum() for df in store.select('df', chunksize=50000))\nres = next(chunks)  # will raise if there are no chunks!\nfor c in chunks:\n    res = res.add(c, fill_value=0)\n"
"In [233]:\n\nd\nOut[233]:\n{'df1':     name         color    type\n 0  Apple        Yellow   Fruit, 'df2':     name         color    type\n 0  Banana       Red      Fruit, 'df3':     name         color    type\n 0  Chocolate    Brown    Sweet}\nIn [234]:\n\npd.concat(d.values(), ignore_index=True)\nOut[234]:\n    name         color    type\n0  Banana       Red      Fruit\n1  Apple        Yellow   Fruit\n2  Chocolate    Brown    Sweet\n"
"In [1]: import pandas as pd\n\nIn [2]: pd.__version__\nOut[2]: u'0.18.1'\n\nIn [3]: s = pd.Series(list('abcbacb'))\n\nIn [4]: pd.get_dummies(s, drop_first=True)\nOut[4]: \n     b    c\n0  0.0  0.0\n1  1.0  0.0\n2  0.0  1.0\n3  1.0  0.0\n4  0.0  0.0\n5  0.0  1.0\n6  1.0  0.0\n"
"temp_dataframe.groupby(level=0,axis=0).apply(lambda x: foo(x.name, x))\n\nIn [132]:\ndf = pd.DataFrame({'a':list('aabccc'), 'b':np.arange(6)})\ndf\n\nOut[132]:\n   a  b\n0  a  0\n1  a  1\n2  b  2\n3  c  3\n4  c  4\n5  c  5\n\nIn [134]:\ndf.groupby('a').apply(lambda x: print('name:', x.name, '\\nsubdf:',x))\n\nname: a \nsubdf:    a  b\n0  a  0\n1  a  1\nname: b \nsubdf:    a  b\n2  b  2\nname: c \nsubdf:    a  b\n3  c  3\n4  c  4\n5  c  5\nOut[134]:\nEmpty DataFrame\nColumns: []\nIndex: []\n"
"print df\n#     a     b\n#0  aaa  rrrr\n#1   bb     k\n#2  ccc     e\n#condition if condition is True then len column a else column b\ndf['c'] = np.where(df['a'].map(len) &gt; df['b'].map(len), df['a'].map(len), df['b'].map(len))\nprint df\n#     a     b  c\n#0  aaa  rrrr  4\n#1   bb     k  2\n#2  ccc     e  3\n\ndf['c'] = df.apply(lambda x: max(len(x['a']), len(x['b'])), axis=1)\n"
"df1.index.levels[-1].astype(str)\n\nIn [584]: df1.index.levels[-1].astype(str)\nOut[584]: Index(['1', '2', '3', '4', '96', '99'], dtype='object', name='Values')\n\nidx = df1.index\ndf1.index = df1.index.set_levels([idx.levels[:-1], idx.levels[-1].astype(str)])\n"
"print df\n                    datetime\n0  2015-12-01 00:00:00-06:00\n1  2015-12-01 00:00:00-06:00\n2  2015-12-01 00:00:00-06:00\n\ndf['datetime'] = df['datetime'].astype(str).str[:-6]\nprint df\n              datetime\n0  2015-12-01 00:00:00\n1  2015-12-01 00:00:00\n2  2015-12-01 00:00:00\n"
"In [29]: m = pd.melt(df, id_vars=['Year'], var_name='Name')\n\nIn [30]: d2 = {}\n\nIn [31]: for k, v in d.items():\n    for item in v:\n        d2[item] = k\n   ....:\n\nIn [32]: d2\nOut[32]: {'Amy': 'A', 'Ben': 'B', 'Bob': 'B', 'Carl': 'C', 'Chris': 'C'}\n\nIn [34]: m['Group'] = m['Name'].map(d2)\n\nIn [35]: m\nOut[35]:\n    Year   Name  value Group\n0   2013    Amy      2     A\n1   2014    Amy      9     A\n2   2013    Bob      4     B\n3   2014    Bob      2     B\n4   2013   Carl      7     C\n..   ...    ...    ...   ...\n7   2014  Chris      5     C\n8   2013    Ben      1     B\n9   2014    Ben      5     B\n10  2013  Other      3   NaN\n11  2014  Other      6   NaN\n\n[12 rows x 4 columns]\n\nIn [8]: mask = m['Name'] == 'Other'\n\nIn [9]: m.loc[mask, 'Name'] = ''\n\nIn [10]: m.loc[mask, 'Group'] = 'Other'\n\nIn [11]: m\nOut[11]:\n    Year   Name  value  Group\n0   2013    Amy      2      A\n1   2014    Amy      9      A\n2   2013    Bob      4      B\n3   2014    Bob      2      B\n4   2013   Carl      7      C\n..   ...    ...    ...    ...\n7   2014  Chris      5      C\n8   2013    Ben      1      B\n9   2014    Ben      5      B\n10  2013             3  Other\n11  2014             6  Other\n\n[12 rows x 4 columns]\n"
"&gt;&gt;&gt; s = pd.Series(['one','two','three','four'])\n&gt;&gt;&gt; recodes = {'one':'A', 'two':'B', 'three':'C'}\n&gt;&gt;&gt; s.map(recodes)\n0      A\n1      B\n2      C\n3    NaN\ndtype: object\n&gt;&gt;&gt; s.replace(recodes)\n0       A\n1       B\n2       C\n3    four\ndtype: object\n"
"df = pd.read_csv('1459966468_324.csv', encoding='utf8')\n\ndf.apply(lambda x: pd.lib.infer_dtype(x.values))\n\nargs            unicode\ndate         datetime64\nhost            unicode\nkwargs          unicode\noperation       unicode\n"
"x = df3.to_string(header=False,\n                  index=False,\n                  index_names=False).split('\\n')\nvals = [','.join(ele.split()) for ele in x]\nprint(vals)\n\n['1.221365,0.923175,-1.286149,-0.153414,-0.005078', '-0.231824,-1.131186,0.853728,0.160349,1.000170', '-0.147145,0.310587,-0.388535,0.957730,-0.185315', '-1.658463,-1.114204,0.760424,-1.504126,0.206909', '-0.734571,0.908569,-0.698583,-0.692417,-0.768087', '0.000029,0.204140,-0.483123,-1.064851,-0.835931', '-0.108869,0.426260,0.107286,-1.184402,0.434607', '-0.692160,-0.376433,0.567188,-0.171867,-0.822502', '-0.564726,-1.084698,-1.065283,-2.335092,-0.083357', '-1.429049,0.790535,-0.547701,-0.684346,2.048081']\n"
"In [273]: %timeit df1[df1['letter'] == 'ben']\n10 loops, best of 3: 36.1 ms per loop\n\nIn [274]: %timeit df2[df2['letter'] == 'ben']\n10 loops, best of 3: 108 ms per loop\n\nIn [275]: %timeit df1['letter'].values == 'ben'\n10 loops, best of 3: 24.1 ms per loop\n\nIn [276]: %timeit df2['letter'].values == 'ben'\n10 loops, best of 3: 96.5 ms per loop\n\nIn [11]: %timeit [item for item in df1['letter']]\n10 loops, best of 3: 49.4 ms per loop\n\nIn [12]: %timeit [item for item in df2['letter']]\n10 loops, best of 3: 124 ms per loop\n\nmemloc = pd.DataFrame({'df1': list(map(id, df1['letter'])),\n                       'df2': list(map(id, df2['letter'])), })\n\n               df1              df2\n0  140226328244040  140226299303840\n1  140226328243088  140226308389048\n2  140226328243872  140226317328936\n3  140226328243760  140226230086600\n4  140226328243368  140226285885624\n\nIn [272]: diffs = memloc.diff(); diffs.head(30)\nOut[272]: \n         df1         df2\n0        NaN         NaN\n1     -952.0   9085208.0\n2      784.0   8939888.0\n3     -112.0 -87242336.0\n4     -392.0  55799024.0\n5     -392.0   5436736.0\n6      952.0  22687184.0\n7       56.0 -26436984.0\n8     -448.0  24264592.0\n9      -56.0  -4092072.0\n10    -168.0 -10421232.0\n11 -363584.0   5512088.0\n12      56.0 -17433416.0\n13      56.0  40042552.0\n14      56.0 -18859440.0\n15      56.0 -76535224.0\n16      56.0  94092360.0\n17      56.0  -4189368.0\n18      56.0     73840.0\n19      56.0  -5807616.0\n20      56.0  -9211680.0\n21      56.0  20571736.0\n22      56.0 -27142288.0\n23      56.0   5615112.0\n24      56.0  -5616568.0\n25      56.0   5743152.0\n26      56.0 -73057432.0\n27      56.0  -4988200.0\n28      56.0  85630584.0\n29      56.0  -4706136.0\n\nIn [14]: \nIn [16]: diffs['df1'].value_counts()\nOut[16]: \n 56.0           986109\n 120.0           13671\n-524168.0          215\n-56.0                1\n-12664712.0          1\n 41136.0             1\n-231731080.0         1\nName: df1, dtype: int64\n\nIn [20]: len(diffs['df1'].value_counts())\nOut[20]: 7\n\nIn [17]: diffs['df2'].value_counts().head()\nOut[17]: \n-56.0     46\n 56.0     44\n 168.0    39\n-112.0    37\n-392.0    35\nName: df2, dtype: int64\n\nIn [19]: len(diffs['df2'].value_counts())\nOut[19]: 837764\n\nIn [5]: %timeit df1[df1['value'] == 0]\n1000 loops, best of 3: 1.8 ms per loop\n\nIn [6]: %timeit df2[df2['value'] == 0]\n1000 loops, best of 3: 1.78 ms per loop\n"
"mask = (data['value2'] == 'A') &amp; (data['value'] &gt; 4)\n\ndata[mask]\n"
"In [177]: df = pd.DataFrame({\n     ...:   'a': pd.util.testing.rands_array(30, 10),\n     ...:   'b': pd.util.testing.rands_array(30, 10),\n     ...: })\n     ...:\n\nIn [178]: df\nOut[178]:\n                                a                               b\n0  Mlf6nOsC8S6vv8OxW5ZOWifg3EoqAb  XSGLdkaewwZlNeZ4uTTivi2nMQFc6S\n1  0E4XCBaYFBTSalUMPGpXmke6dQGbkW  KlHuVhbNgQL9HLHYQq3fEdqEIciOhX\n2  URODJeLA0uLvcKBEXPyrmnnNU40MDl  NaY8LURHjgmT1pRrDnbPAeLZq3ANaL\n3  OYA1ahlwVtEVnDOAkZgxNkbvZ7W8Rf  mIzkeLhM7SqYH17vGDzL6DJjSYftGs\n4  uFC1shE02UfxS0VhDASmF8vh9XxFYX  fQOxjDjFehTNT27seOtCAAPW0as9Up\n5  Ja33vQym6L0Ko2Kcf8cg7OMBKMitg5  iGdCvYTyZlR23NeeTAjG1PoL8mWm3j\n6  iNZdXaVpB4zXClxTLt738DY7i6xs6p  q9VKg5fZdItmUpZiQrR6XW5WHmd33l\n7  WWnViRRMPkbXNQOHeqGmzETDpGPRl9  t3I8Ve3ybCJcXajF8pydnwNZQWslTN\n8  5oMFy2PBe1zUIE3XdraMwlrd5MKcx2  gSLtgXJwiS1HugLORXherFT4l1k5QV\n9  weV8BlyJrtRbWpSCxSbj8cSyZxusFR  ylLWort9o8mHWQQ3JB1Twb0xRbLhot\n\nIn [179]: df.apply(lambda x: x.str.slice(0, 20))\nOut[179]:\n                      a                     b\n0  Mlf6nOsC8S6vv8OxW5ZO  XSGLdkaewwZlNeZ4uTTi\n1  0E4XCBaYFBTSalUMPGpX  KlHuVhbNgQL9HLHYQq3f\n2  URODJeLA0uLvcKBEXPyr  NaY8LURHjgmT1pRrDnbP\n3  OYA1ahlwVtEVnDOAkZgx  mIzkeLhM7SqYH17vGDzL\n4  uFC1shE02UfxS0VhDASm  fQOxjDjFehTNT27seOtC\n5  Ja33vQym6L0Ko2Kcf8cg  iGdCvYTyZlR23NeeTAjG\n6  iNZdXaVpB4zXClxTLt73  q9VKg5fZdItmUpZiQrR6\n7  WWnViRRMPkbXNQOHeqGm  t3I8Ve3ybCJcXajF8pyd\n8  5oMFy2PBe1zUIE3XdraM  gSLtgXJwiS1HugLORXhe\n9  weV8BlyJrtRbWpSCxSbj  ylLWort9o8mHWQQ3JB1T\n"
'spark_df.limit(5).toPandas().head()\n'
"df = df.replace(r'\\\\n',' ', regex=True) \n\ndf = df.replace('\\n','', regex=True)\n\ndf = df.replace('\\n',' ', regex=True)\n\ndf = df.replace(r'\\\\n',' ', regex=True)\n\ntext = '''hands-on\\ndev nologies\\nrelevant scripting\\nlang\n'''\ndf = pd.DataFrame({'A':[text]})\nprint (df)\n                                                   A\n0  hands-on\\ndev nologies\\nrelevant scripting\\nla...\n\ndf = df.replace('\\n',' ', regex=True)\nprint (df)\n                                                A\n0  hands-on dev nologies relevant scripting lang \n"
"df = df.max(axis=1)\nprint (df)\n0    2.0\n1    3.2\n2    8.8\n3    7.8\ndtype: float64\n\ndf['max_value'] = df.max(axis=1)\nprint (df)\n     a    b     c  max_value\n0  1.2  2.0  0.10        2.0\n1  2.1  1.1  3.20        3.2\n2  0.2  1.9  8.80        8.8\n3  3.3  7.8  0.12        7.8\n"
"df = df.assign(id=(df['LastName'] + '_' + df['FirstName']).astype('category').cat.codes)\n&gt;&gt;&gt; df\n  FirstName  LastName  id\n0       Tom     Jones   0\n1       Tom     Jones   0\n2     David     Smith   1\n3      Alex  Thompson   2\n4      Alex  Thompson   2\n"
'In [11]: idx = pd.period_range(min(df.date), max(df.date))\n    ...: results.reindex(idx, fill_value=0)\n    ...:\nOut[11]:\n                  f1        f2        f3        f4\n2000-01-01  2.049157  1.962635  2.756154  2.224751\n2000-01-02  2.675899  2.587217  1.540823  1.606150\n2000-01-03  0.000000  0.000000  0.000000  0.000000\n2000-01-04  0.000000  0.000000  0.000000  0.000000\n2000-01-05  0.000000  0.000000  0.000000  0.000000\n2000-01-06  0.000000  0.000000  0.000000  0.000000\n2000-01-07  0.000000  0.000000  0.000000  0.000000\n2000-01-08  0.000000  0.000000  0.000000  0.000000\n2000-01-09  0.000000  0.000000  0.000000  0.000000\n2000-01-10  0.000000  0.000000  0.000000  0.000000\n2000-01-11  0.000000  0.000000  0.000000  0.000000\n2000-01-12  0.000000  0.000000  0.000000  0.000000\n2000-01-13  0.000000  0.000000  0.000000  0.000000\n2000-01-14  0.000000  0.000000  0.000000  0.000000\n2000-01-15  0.000000  0.000000  0.000000  0.000000\n2000-01-16  0.000000  0.000000  0.000000  0.000000\n2000-01-17  0.000000  0.000000  0.000000  0.000000\n2000-01-18  0.000000  0.000000  0.000000  0.000000\n2000-01-19  0.000000  0.000000  0.000000  0.000000\n2000-01-20  0.000000  0.000000  0.000000  0.000000\n2000-01-21  0.000000  0.000000  0.000000  0.000000\n2000-01-22  0.000000  0.000000  0.000000  0.000000\n2000-01-23  0.000000  0.000000  0.000000  0.000000\n2000-01-24  0.000000  0.000000  0.000000  0.000000\n2000-01-25  0.000000  0.000000  0.000000  0.000000\n2000-01-26  0.000000  0.000000  0.000000  0.000000\n2000-01-27  0.000000  0.000000  0.000000  0.000000\n2000-01-28  0.000000  0.000000  0.000000  0.000000\n2000-01-29  0.000000  0.000000  0.000000  0.000000\n2000-01-30  0.000000  0.000000  0.000000  0.000000\n2000-01-31  0.000000  0.000000  0.000000  0.000000\n2000-02-01  0.000000  0.000000  0.000000  0.000000\n2000-02-02  0.000000  0.000000  0.000000  0.000000\n2000-02-03  0.000000  0.000000  0.000000  0.000000\n2000-02-04  1.856158  2.892620  2.986166  2.793448\n\ndf.groupby(pd.PeriodIndex(data=df.date, freq=\'D\'))\n\ndf.groupby(pd.Grouper(key="date", freq=\'D\'))\n'
"df.groupby(['revenue','session','user_id'])['user_id'].count()\n"
'rawText = StringIO("""\n A         B         C\n0  100.1396  1.343921  Medium\n1  105.3268  1.786945  Medium\n2  200.3766  9.628746  High\n3  150.2400  4.225647  Medium-High\n""")\nmyData = pd.read_csv(rawText, sep = "\\s+")\n\nIn[226]: myData.assign(C=myData.C.astype(\'category\').cat.codes).corr()\nOut[226]: \n          A         B         C\nA  1.000000  0.986493 -0.438466\nB  0.986493  1.000000 -0.579650\nC -0.438466 -0.579650  1.000000\n\nIn[227]: myData[\'C\'] = myData[\'C\'].astype(\'category\')\nmyData[\'C\'].cat.categories = [2,0,1]\nmyData[\'C\'] = myData[\'C\'].astype(\'float\')\nmyData.corr()\nOut[227]: \n          A         B         C\nA  1.000000  0.986493  0.998874\nB  0.986493  1.000000  0.982982\nC  0.998874  0.982982  1.000000\n'
'np.array_equal(df.v, df.v.astype(int))\nTrue\n\ndf.v.apply(float.is_integer).all()\nTrue\n\nall(x.is_integer() for x in df.v)\nTrue\n'
"bins = [0, 2, 18, 35, 65, np.inf]\nnames = ['&lt;2', '2-18', '18-35', '35-65', '65+']\n\ndf['AgeRange'] = pd.cut(df['Age'], bins, labels=names)\n\nprint(df.dtypes)\n\n# Age             int64\n# Age_units      object\n# AgeRange     category\n# dtype: object\n\nimport pandas as pd, numpy as np\n\ndf = pd.DataFrame({'Age': [99, 53, 71, 84, 84],\n                   'Age_units': ['Y', 'Y', 'Y', 'Y', 'Y']})\n\nbins = [0, 2, 18, 35, 65]\nnames = ['&lt;2', '2-18', '18-35', '35-65', '65+']\n\nd = dict(enumerate(names, 1))\n\ndf['AgeRange'] = np.vectorize(d.get)(np.digitize(df['Age'], bins))\n\n   Age Age_units AgeRange\n0   99         Y      65+\n1   53         Y    35-65\n2   71         Y      65+\n3   84         Y      65+\n4   84         Y      65+\n"
"temp['total'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({'total':'count'})).reset_index()['total']\n\ntemp['Avg'] = pd.DataFrame(project_data.groupby(col1)[col2].agg({'Avg':'mean'})).reset_index()['Avg']\n\ntemp['total'] = pd.DataFrame(project_data.groupby(col1)[col2].agg(total='count')).reset_index()['total']\ntemp['Avg'] = pd.DataFrame(project_data.groupby(col1)[col2].agg(Avg='mean')).reset_index()['Avg']\n"
'for a, b in test.itertuples(index=False):\n    print a, b\n'
'In [12]: df = pandas.DataFrame({"a": [1, 2, 0, 1, 5], \n                                "b": [0, 10, 20, 30, 50]}).astype(\'float64\')\n\nIn [13]: df\nOut[13]: \n   a   b\n0  1   0\n1  2  10\n2  0  20\n3  1  30\n4  5  50\n\nIn [14]: df.dtypes\nOut[14]: \na    float64\nb    float64\ndtype: object\n\nIn [15]: x = df.a/df.b\n\nIn [16]: x\nOut[16]: \n0         inf\n1    0.200000\n2    0.000000\n3    0.033333\n4    0.100000\ndtype: float64\n\nIn [17]: x[np.isinf(x)] = np.nan\n\nIn [18]: x\nOut[18]: \n0         NaN\n1    0.200000\n2    0.000000\n3    0.033333\n4    0.100000\ndtype: float64\n\nIn [20]: df.a/df.b.replace({ 0 : np.nan })\nOut[20]: \n0         NaN\n1    0.200000\n2    0.000000\n3    0.033333\n4    0.100000\ndtype: float64\n'
"import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'id': [1,1,2,2,1,2,1,1], 'x':[10,20,100,200,np.nan,np.nan,300,np.nan]})\ndf['x'] = df.groupby(['id'])['x'].ffill()\nprint(df)\n\n   id      x\n0   1   10.0\n1   1   20.0\n2   2  100.0\n3   2  200.0\n4   1   20.0\n5   2  200.0\n6   1  300.0\n7   1  300.0\n"
"import numpy as np\nimport pandas as pd\n\n# Create a dtype with the binary data format and the desired column names\ndt = np.dtype([('a', 'i4'), ('b', 'i4'), ('c', 'i4'), ('d', 'f4'), ('e', 'i4'),\n               ('f', 'i4', (256,))])\ndata = np.fromfile(file, dtype=dt)\ndf = pd.DataFrame(data)\n\n# Or if you want to explicitly set the column names\ndf = pd.DataFrame(data, columns=data.dtype.names)\n"
"In [28]: df = df.reset_index()\n\nIn [29]: df['b'] = df['b'].fillna('dummy')\n\nIn [30]: df['dummy'] = np.nan\n\nIn [31]: df\nOut[31]: \n   a      b       c    d   e  dummy\n0  a      b   12.00   12  12    NaN\n1  a  dummy   12.30  233  12    NaN\n2  b      a  123.23  123   1    NaN\n3  a      b    1.00    1   1    NaN\n\nIn [32]: df.pivot_table(rows=['a', 'b'], values=['c', 'd', 'e'], aggfunc=sum)\nOut[32]: \n              c    d   e\na b                     \na b       13.00   13  13\n  dummy   12.30  233  12\nb a      123.23  123   1\n\nIn [33]: df.pivot_table(rows=['a', 'b'], values=['c', 'd', 'e'], aggfunc=sum).reset_index().replace('dummy',np.nan).set_index(['a','b'])\nOut[33]: \n            c    d   e\na b                   \na b     13.00   13  13\n  NaN   12.30  233  12\nb a    123.23  123   1\n"
"In [11]: pd.Series(a, a._fields)\nOut[11]:\nticker            GE\ndate      2010-01-01\nprice             30\ndtype: object\n\nIn [12]: df = pd.DataFrame(l, columns=l[0]._fields)\n\nIn [13]: df\nOut[13]:\n  ticker        date  price\n0     GE  2010-01-01     30\n1     GE  2010-01-02     31\n\nIn [14]: df.set_index(['ticker', 'date'], inplace=True)\n\nIn [15]: df\nOut[15]:\n                   price\nticker date\nGE     2010-01-01     30\n       2010-01-02     31\n"
'In [11]: s = pd.Series([1, 1, 2, 1, 2, 2, 3])\n\nIn [12]: s.value_counts()\nOut[12]:\n2    3\n1    3\n3    1\ndtype: int64\n\ns.groupby(lambda i: np.floor(2*s[i]) / 2).count()\n'
" $('table tbody tr').filter(':last').css('background-color', '#FF0000')\n\n $('table.dataframe tbody tr').filter(':last').css('background-color', '#FF0000')\n\ndf.to_html(classes='my_class')\n\ndf.to_html(classes=['my_class', 'my_other_class'])\n\nIn [1]: import numpy as np\n        import pandas as pd\n        from IPython.display import HTML, Javascript\nIn [2]: df = pd.DataFrame({'a': np.arange(10), 'b': np.random.randn(10)})\nIn [3]: HTML(df.to_html(classes='my_class'))\nIn [4]: Javascript('''$('.my_class tbody tr').filter(':last')\n                                             .css('background-color', '#FF0000');\n                   ''')\n\nIn [5]: HTML('''\n        &lt;style&gt;\n            .df tbody tr:last-child { background-color: #FF0000; }\n        &lt;/style&gt;\n        ''' + df.to_html(classes='df'))\n\nimport numpy as np\nimport pandas as pd\n\nHEADER = '''\n&lt;html&gt;\n    &lt;head&gt;\n        &lt;style&gt;\n            .df tbody tr:last-child { background-color: #FF0000; }\n        &lt;/style&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n'''\nFOOTER = '''\n    &lt;/body&gt;\n&lt;/html&gt;\n'''\n\ndf = pd.DataFrame({'a': np.arange(10), 'b': np.random.randn(10)})\nwith open('test.html', 'w') as f:\n    f.write(HEADER)\n    f.write(df.to_html(classes='df'))\n    f.write(FOOTER)\n"
'def func(row):\n    xml = [\'&lt;item&gt;\']\n    for field in row.index:\n        xml.append(\'  &lt;field name="{0}"&gt;{1}&lt;/field&gt;\'.format(field, row[field]))\n    xml.append(\'&lt;/item&gt;\')\n    return \'\\n\'.join(xml)\n\n&gt;&gt;&gt; print \'\\n\'.join(df.apply(func, axis=1))\n&lt;item&gt;\n  &lt;field name="field_1"&gt;cat&lt;/field&gt;\n  &lt;field name="field_2"&gt;15,263&lt;/field&gt;\n  &lt;field name="field_3"&gt;2.52&lt;/field&gt;\n  &lt;field name="field_4"&gt;00:03:00&lt;/field&gt;\n&lt;/item&gt;\n&lt;item&gt;\n  &lt;field name="field_1"&gt;dog&lt;/field&gt;\n  &lt;field name="field_2"&gt;1,652&lt;/field&gt;\n  &lt;field name="field_3"&gt;3.71&lt;/field&gt;\n  &lt;field name="field_4"&gt;00:03:47&lt;/field&gt;\n&lt;/item&gt;\n...\n'
'In [11]: df2 = pd.concat([df, df.T]).fillna(0)\n\nIn [12]: df2 = df2.reindex(df2.columns)\n\nIn [13]: df2\nOut[13]: \n       Bar  Bat  Baz  Foo  Loc 1  Loc 2  Loc 3  Loc 4  Loc 5  Loc 6  Loc 7  Quux\nBar      0    0    0    0      0      0      1      1      0      1      1     0\nBat      0    0    0    0      0      0      1      0      0      1      0     0\nBaz      0    0    0    0      0      0      1      0      0      0      0     0\nFoo      0    0    0    0      0      0      1      1      0      0      0     0\nLoc 1    0    0    0    0      0      0      0      0      0      0      0     1\nLoc 2    0    0    0    0      0      0      0      0      0      0      0     0\nLoc 3    1    1    1    1      0      0      0      0      0      0      0     0\nLoc 4    1    0    0    1      0      0      0      0      0      0      0     0\nLoc 5    0    0    0    0      0      0      0      0      0      0      0     0\nLoc 6    1    1    0    0      0      0      0      0      0      0      0     0\nLoc 7    1    0    0    0      0      0      0      0      0      0      0     0\nQuux     0    0    0    0      1      0      0      0      0      0      0     0\n\nIn[14]: graph = nx.from_numpy_matrix(df2.values)\n\nIn [15]: graph = nx.relabel_nodes(graph, dict(enumerate(df2.columns))) # is there nicer  way than dict . enumerate ?\n'
'iris.groupby("Name").PetalWidth.plot(kind=\'kde\', ax=axs[1])\niris.groupby("Name").PetalWidth.hist(alpha=0.4, ax=axs[0])\n'
'import numpy as np\n\na = np.array([\n    [1, 0, 0],\n    [0, np.nan, 0],\n    [0, 0, 0],\n    [np.nan, np.nan, np.nan],\n    [2, 3, 4]\n])\n\nmask = np.all(np.isnan(a) | np.equal(a, 0), axis=1)\na[~mask]\n'
"In [11]: df.agg(['mean', 'std'])\nOut[11]: \n           one       two\nmean  5.147471  4.964100\nstd   2.971106  2.753578\n\nIn [61]: df.groupby(lambda idx: 0).agg(['mean','std'])\nOut[61]: \n        one               two          \n       mean       std    mean       std\n0  5.147471  2.971106  4.9641  2.753578\n\nIn [68]: pd.DataFrame({col: [getattr(df[col], func)() for func in ('mean', 'std')] for col in df}, index=('mean', 'std'))\nOut[68]: \n           one       two\nmean  5.147471  4.964100\nstd   2.971106  2.753578\n"
"&gt;&gt;&gt; df.groupby(pd.TimeGrouper('5Min'))['val'].mean()\n\ntime\n2014-04-03 16:00:00    14390.000000\n2014-04-03 16:05:00    14394.333333\n2014-04-03 16:10:00    14396.500000\n\n&gt;&gt;&gt; df.groupby(pd.TimeGrouper('5Min'))['val'].apply(lambda x: len(x) &gt; 3)\n\ntime\n2014-04-03 16:00:00    False\n2014-04-03 16:05:00    False\n2014-04-03 16:10:00     True\n\nDocstring for resample:class TimeGrouper@21\n\nTimeGrouper(self, freq = 'Min', closed = None, label = None,\nhow = 'mean', nperiods = None, axis = 0, fill_method = None,\nlimit = None, loffset = None, kind = None, convention = None, base = 0,\n**kwargs)\n\nCustom groupby class for time-interval grouping\n\nParameters\n----------\nfreq : pandas date offset or offset alias for identifying bin edges\nclosed : closed end of interval; left or right\nlabel : interval boundary to use for labeling; left or right\nnperiods : optional, integer\nconvention : {'start', 'end', 'e', 's'}\n    If axis is PeriodIndex\n\nNotes\n-----\nUse begin, end, nperiods to generate intervals that cannot be derived\ndirectly from the associated object\n\n&gt;&gt;&gt; new = df.groupby(pd.TimeGrouper('5Min'),as_index=False).apply(lambda x: x['val'])\n&gt;&gt;&gt; df['period'] = new.index.get_level_values(0)\n&gt;&gt;&gt; df\n\n                     id    val  period\ntime\n2014-04-03 16:01:53  23  14389       0\n2014-04-03 16:01:54  28  14391       0 \n2014-04-03 16:05:55  24  14393       1\n2014-04-03 16:06:25  23  14395       1\n2014-04-03 16:07:01  23  14395       1\n2014-04-03 16:10:09  23  14395       2\n2014-04-03 16:10:23  26  14397       2\n2014-04-03 16:10:57  26  14397       2\n2014-04-03 16:11:10  26  14397       2\n\n&gt;&gt;&gt; new\n\n   time\n0  2014-04-03 16:01:53    14389\n   2014-04-03 16:01:54    14391\n1  2014-04-03 16:05:55    14393\n   2014-04-03 16:06:25    14395\n   2014-04-03 16:07:01    14395\n2  2014-04-03 16:10:09    14395\n   2014-04-03 16:10:23    14397\n   2014-04-03 16:10:57    14397\n   2014-04-03 16:11:10    14397\n\n&gt;&gt;&gt;  new.index.get_level_values(0)\n\nInt64Index([0, 0, 1, 1, 1, 2, 2, 2, 2], dtype='int64')\n"
'from pandas import *\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport datetime as dt\n\n#The following part is just for generating something similar to your dataframe\ndate1 = "20140605"\ndate2 = "20140606"\n\nd = {\'date\': Series([date1]*5 + [date2]*5), \'template\': Series(range(5)*2),\n\'score\': Series([random() for i in range(10)]) } \n\ndata = DataFrame(d)\n#end of dataset generation\n\nfig, ax = plt.subplots()\n\nfor temp in range(5):\n    dat = data[data[\'template\']==temp]\n    dates =  dat[\'date\']\n    dates_f = [dt.datetime.strptime(date,\'%Y%m%d\') for date in dates]\n    ax.plot(dates_f, dat[\'score\'], label = "Template: {0}".format(temp))\n\nplt.xlabel("Date")\nplt.ylabel("Score")\nax.legend()\nplt.show()\n'
'B = lfilter([a], [1.0, -b], A)\n\nimport numpy as np\nfrom scipy.signal import lfilter\n\n\nnp.random.seed(123)\n\nA = np.random.randn(10)\na = 2.0\nb = 3.0\n\n# Compute the recursion using lfilter.\n# [a] and [1, -b] are the coefficients of the numerator and\n# denominator, resp., of the filter\'s transfer function.\nB = lfilter([a], [1, -b], A)\n\nprint B\n\n# Compare to a simple loop.\nB2 = np.empty(len(A))\nfor k in range(0, len(B2)):\n    if k == 0:\n        B2[k] = a*A[k]\n    else:\n        B2[k] = a*A[k] + b*B2[k-1]\n\nprint B2\n\nprint "max difference:", np.max(np.abs(B2 - B))\n\n[ -2.17126121e+00  -4.51909273e+00  -1.29913212e+01  -4.19865530e+01\n  -1.27116859e+02  -3.78047705e+02  -1.13899647e+03  -3.41784725e+03\n  -1.02510099e+04  -3.07547631e+04]\n[ -2.17126121e+00  -4.51909273e+00  -1.29913212e+01  -4.19865530e+01\n  -1.27116859e+02  -3.78047705e+02  -1.13899647e+03  -3.41784725e+03\n  -1.02510099e+04  -3.07547631e+04]\nmax difference: 0.0\n\nIn [12]: df = pd.DataFrame([1, 7, 9, 5], columns=[\'A\'])\n\nIn [13]: df\nOut[13]: \n   A\n0  1\n1  7\n2  9\n3  5\n\nIn [14]: df[\'B\'] = lfilter([1], [1, -2], df[\'A\'].astype(float))\n\nIn [15]: df\nOut[15]: \n   A   B\n0  1   1\n1  7   9\n2  9  27\n3  5  59\n'
'In [1]: import csv\n\nIn [2]: !cat test.csv\n"column1","column2", "column3", "column4", "column5", "column6"\n"AM", "07", "1", "SD", "SD", "CR"\n"AM", "08", "1,2,3", "PR,SD,SD", "PR,SD,SD", "PR,SD,SD"\n"AM", "01", "2", "SD", "SD", "SD"\n\nIn [3]: pd.read_csv(\'test.csv\',sep=\',\\s+\',quoting=csv.QUOTE_ALL)\npandas/io/parsers.py:637: ParserWarning: Falling back to the \'python\' engine because the \'c\' engine does not support regex separators; you can avoid this warning by specifying engine=\'python\'.\n  ParserWarning)\nOut[3]: \n     "column1","column2" "column3"   "column4"   "column5"   "column6"\n"AM"                "07"       "1"        "SD"        "SD"        "CR"\n"AM"                "08"   "1,2,3"  "PR,SD,SD"  "PR,SD,SD"  "PR,SD,SD"\n"AM"                "01"       "2"        "SD"        "SD"        "SD"\n'
'import pandas as pd\nimport numpy as np\nnp.random.seed(10)\n\nremove_n = 1\ndf = pd.DataFrame({&quot;a&quot;:[1,2,3,4], &quot;b&quot;:[5,6,7,8]})\ndrop_indices = np.random.choice(df.index, remove_n, replace=False)\ndf_subset = df.drop(drop_indices)\n\n    a   b\n0   1   5\n1   2   6\n2   3   7\n3   4   8\n\n    a   b\n0   1   5\n1   2   6\n3   4   8\n'
'In [3]:\n\nimport io\nimport pandas as pd\nt=&quot;&quot;&quot;index,a,b\n0,hello,pandas&quot;&quot;&quot;\npd.read_csv(io.StringIO(t))\n\u200b\nOut[3]:\n   index      a       b\n0      0  hello  pandas\n\nIn [4]:\n\npd.read_csv(io.StringIO(t), index_col=0)\nOut[4]:\n           a       b\nindex               \n0      hello  pandas\n\nIn [5]:\n\npd.read_csv(io.StringIO(t), index_col=False)\nOut[5]:\n   index      a       b\n0      0  hello  pandas\n\nIn [6]:\n\npd.read_csv(io.StringIO(t), index_col=None)\nOut[6]:\n   index      a       b\n0      0  hello  pandas\n\nIn [6]:\n\npd.read_csv(io.StringIO(t), index_col=True)\nOut[6]:\n       index       b\na               \n0      hello  pandas\n\nIn [7]:\n\nimport io\nimport pandas as pd\nt=&quot;&quot;&quot;,a,b\n0,hello,pandas&quot;&quot;&quot;\npd.read_csv(io.StringIO(t))\n\u200b\nOut[7]:\n   Unnamed: 0      a       b\n0           0  hello  pandas\nIn [8]:\n\npd.read_csv(io.StringIO(t), index_col=0)\nOut[8]:\n       a       b\n0  hello  pandas\nIn [9]:\n\npd.read_csv(io.StringIO(t), index_col=False)\nOut[9]:\n   Unnamed: 0      a       b\n0           0  hello  pandas\nIn [10]:\n\npd.read_csv(io.StringIO(t), index_col=None)\nOut[10]:\n   Unnamed: 0      a       b\n0           0  hello  pandas\n'
'my_list = map(lambda x: x[0], z_r)\nser = pd.Series(my_list)\nIn [86]:\nser\nOut[86]:\n0      0.009093\n1      0.023903\n2      0.029988\n'
"from rpy2.robjects import pandas2ri\npandas2ri.activate()\nrobjects.globalenv['dataframe'] = dataframe\nM = stats.lm('y~x', data=base.as_symbol('dataframe'))\n\n&gt;&gt;&gt; print(base.summary(M).rx2('coefficients'))\n            Estimate Std. Error  t value  Pr(&gt;|t|)\n(Intercept)      0.6  1.1489125 0.522233 0.6376181\nx                0.8  0.3464102 2.309401 0.1040880\n"
'In [1]: s = pd.Series(["A","B","C"], name="foo")\n\nIn [2]: s\nOut[2]: \n0    A\n1    B\n2    C\nName: foo, dtype: object\n\nIn [3]: pd.DataFrame(s)\nOut[4]: \n  foo\n0   A\n1   B\n2   C\n\n   0\n0  A\n1  B\n2  C\n'
'[y[1] for y in y_test]\n#  ^ this is the problem\n\ny_test = [1, 2, 3]\ny = y_test[0] # y = 1\nprint(y[0]) # this line will fail\n\n[results.append(..., y) for y in y_test]\n\nfor y in y_test:\n    results.append(..., y)\n'
"In [341]:\ndf['MinNote'] = np.minimum(1,df['note'])\ndf\n\nOut[341]:\n   session      note  minValue   MinNote\n0        1  0.726841  0.726841  0.726841\n1        2  3.163402  3.163402  1.000000\n2        3  2.844161  2.844161  1.000000\n3        4       NaN       NaN       NaN\n"
'&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; d = datetime.now()\n&gt;&gt;&gt; only_date, only_time = d.date(), d.time()\n&gt;&gt;&gt; only_date\ndatetime.date(2015, 11, 20)\n&gt;&gt;&gt; only_time\ndatetime.time(20, 39, 13, 105773)\n\n&gt;&gt;&gt; milestone["only_date"] = [d.date() for d in milestone["datetime"]]\n&gt;&gt;&gt; milestone["only_time"] = [d.time() for d in milestone["datetime"]]\n'
"df = pd.DataFrame({'a':[1,2,3,4,5,6],\n                   'b':[1,2,3,4,5,6],\n                   'c':['q', 'q', 'q', 'q', 'w', 'w'],  \n                   'd':['z','z','z','o','o','o']})\ndf['e'] = df['a'] + df['b']\ndf['e'] = (df.groupby(['c', 'd'])['e'].transform('sum'))\nprint(df)\n\n   a  b  c  d   e\n0  1  1  q  z  12\n1  2  2  q  z  12\n2  3  3  q  z  12\n3  4  4  q  o   8\n4  5  5  w  o  22\n5  6  6  w  o  22\n\nTypeError: cannot concatenate a non-NDFrame object\n\nIn [99]: df.groupby(['c', 'd']).transform(lambda x: pd.Series(np.sum(x['a']+x['b'])))\nOut[99]: \n    a   b\n0  12  12\n1  12  12\n2  12  12\n3   8   8\n4  22  22\n5  22  22\n"
'In [2]:\ndf = pd.read_csv(io.StringIO(t))\nt="""int,float,date,str\n001,3.31,2015/01/01,005"""\ndf = pd.read_csv(io.StringIO(t))\ndf.info()\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 1 entries, 0 to 0\nData columns (total 4 columns):\nint      1 non-null int64\nfloat    1 non-null float64\ndate     1 non-null object\nstr      1 non-null int64\ndtypes: float64(1), int64(2), object(1)\nmemory usage: 40.0+ bytes\n\nIn [3]:    \ndf = pd.read_csv(io.StringIO(t), dtype=object).info()\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 1 entries, 0 to 0\nData columns (total 4 columns):\nint      1 non-null object\nfloat    1 non-null object\ndate     1 non-null object\nstr      1 non-null object\ndtypes: object(4)\nmemory usage: 40.0+ bytes\n\nIn [6]:\npd.read_csv(io.StringIO(t), dtype={\'int\':\'object\'}, parse_dates=[\'date\']).info()\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 1 entries, 0 to 0\nData columns (total 4 columns):\nint      1 non-null object\nfloat    1 non-null float64\ndate     1 non-null datetime64[ns]\nstr      1 non-null int64\ndtypes: datetime64[ns](1), float64(1), int64(1), object(1)\nmemory usage: 40.0+ bytes\n\nIn [5]:\npd.read_csv(io.StringIO(t), converters={\'date\':pd.to_datetime}).info()\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 1 entries, 0 to 0\nData columns (total 4 columns):\nint      1 non-null int64\nfloat    1 non-null float64\ndate     1 non-null datetime64[ns]\nstr      1 non-null int64\ndtypes: datetime64[ns](1), float64(1), int64(2)\nmemory usage: 40.0 bytes\n'
'from sqlalchemy.types import NVARCHAR\ndf.to_sql(...., dtype={col_name: NVARCHAR for col_name in df})\n'
"class MyModel(Model):\n    class Meta:\n        db_table = 'mytable' # This tells Django where the SQL table is\n        managed = False # Use this if table already exists\n                        # and doesn't need to be managed by Django\n\n    field_1 = ...\n    field_2 = ...\n\nfrom django.conf import settings\n\nuser = settings.DATABASES['default']['USER']\npassword = settings.DATABASES['default']['PASSWORD']\ndatabase_name = settings.DATABASES['default']['NAME']\n# host = settings.DATABASES['default']['HOST']\n# port = settings.DATABASES['default']['PORT']\n\ndatabase_url = 'postgresql://{user}:{password}@localhost:5432/{database_name}'.format(\n    user=user,\n    password=password,\n    database_name=database_name,\n)\n\nengine = create_engine(database_url, echo=False)\n\n# Doing it like this is slow\nfor index, row in df.iterrows():\n     model = MyModel()\n     model.field_1 = row['field_1']\n     model.save()\n"
" df = pd.DataFrame(data=[[1,2,3]]*5, index=range(3, 8), columns = ['a','b','c'])\n\n   a  b  c\n3  1  2  3\n4  1  2  3\n5  1  2  3\n6  1  2  3\n7  1  2  3\n\ndf.iloc[[2,4]]\n\n   a  b  c\n5  1  2  3\n7  1  2  3\n\ndf[['b', 'c']].iloc[[2,4]]\n\n   b  c\n5  2  3\n7  2  3\n\ndf[['b', 'c']].iloc[[2,4]].mean(axis=0)\n\nb    2\nc    3\n\n df[column_list].iloc[row_index_list].mean(axis=0)\n\ndfs = list()\nfor l in L:\n    dfs.append(df[['a', 'b']].iloc[l].mean(axis=0))\n\nmean_matrix = pd.concat(dfs, axis=1).T\n"
'df[\'TIME\'] = pd.to_datetime(df[\'TIME\'], format="%m/%d/%Y %I:%M:%S %p")\n\npd.read_csv(\'testresult.csv\', parse_dates=[\'TIME\'], \n    date_parser=lambda x: pd.to_datetime(x, format=\'%m/%d/%Y %I:%M:%S %p\'))\n'
"html = (\n    df.style\n    .format(percent)\n    .applymap(color_negative_red, subset=['col1', 'col2'])\n    .set_properties(**{'font-size': '9pt', 'font-family': 'Calibri'})\n    .bar(subset=['col4', 'col5'], color='lightblue')\n    .render()\n)\n"
"df = pd.DataFrame({'C': {0: -0.91985400000000006, 1: -0.042379, 2: 1.2476419999999999, 3: -0.00992, 4: 0.290213, 5: 0.49576700000000001, 6: 0.36294899999999997, 7: 1.548106}, 'A': {0: 'foo', 1: 'bar', 2: 'foo', 3: 'bar', 4: 'foo', 5: 'bar', 6: 'foo', 7: 'foo'}, 'B': {0: 'one', 1: 'one', 2: 'two', 3: 'three', 4: 'two', 5: 'two', 6: 'one', 7: 'three'}, 'D': {0: -1.131345, 1: -0.089328999999999992, 2: 0.33786300000000002, 3: -0.94586700000000001, 4: -0.93213199999999996, 5: 1.9560299999999999, 6: 0.017587000000000002, 7: -0.016691999999999999}})\n\nprint (df)\n     A      B         C         D\n0  foo    one -0.919854 -1.131345\n1  bar    one -0.042379 -0.089329\n2  foo    two  1.247642  0.337863\n3  bar  three -0.009920 -0.945867\n4  foo    two  0.290213 -0.932132\n5  bar    two  0.495767  1.956030\n6  foo    one  0.362949  0.017587\n7  foo  three  1.548106 -0.016692\n\nprint( df.groupby('A').mean())\n            C         D\nA                      \nbar  0.147823  0.306945\nfoo  0.505811 -0.344944\n"
'data = loadmat(\'datafile.mat\')\n\nimport numpy as np\nfrom scipy.io import loadmat  # this is the SciPy module that loads mat-files\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, date, time\nimport pandas as pd\n\nmat = loadmat(\'measured_data.mat\')  # load mat-file\nmdata = mat[\'measuredData\']  # variable in mat file\nmdtype = mdata.dtype  # dtypes of structures are "unsized objects"\n# * SciPy reads in structures as structured NumPy arrays of dtype object\n# * The size of the array is the size of the structure array, not the number\n#   elements in any particular field. The shape defaults to 2-dimensional.\n# * For convenience make a dictionary of the data using the names from dtypes\n# * Since the structure has only one element, but is 2-D, index it at [0, 0]\nndata = {n: mdata[n][0, 0] for n in mdtype.names}\n# Reconstruct the columns of the data table from just the time series\n# Use the number of intervals to test if a field is a column or metadata\ncolumns = [n for n, v in ndata.iteritems() if v.size == ndata[\'numIntervals\']]\n# now make a data frame, setting the time stamps as the index\ndf = pd.DataFrame(np.concatenate([ndata[c] for c in columns], axis=1),\n                  index=[datetime(*ts) for ts in ndata[\'timestamps\']],\n                  columns=columns)\n'
'ax.set_xticklabels(&lt;your labels&gt;, rotation=0)\n'
"df = pd.DataFrame({'Traded Value':[67867869890077.96,78973434444543.44],\n                   'Deals':[789797, 789878]})\ndf\n\n    Deals   Traded Value\n0   789797  6.786787e+13\n1   789878  7.897343e+13\n\n\ndf['Deals'] = df['Deals'].apply(lambda x: '{:d}'.format(x))\ndf['Traded Value'] = df['Traded Value'].apply(lambda x: '{:.2f}'.format(x))\ndf    \n\n     Deals       Traded Value\n0   789797  67867869890077.96\n1   789878  78973434444543.44\n\npd.options.display.float_format = '{:.2f}'.format\n"
'df = (spark.read.format("csv").options(header="true")\n    .load("/path/tp/demo2016q1.csv"))\n\n## root\n##  |-- primaryid: string (nullable = true)\n##  |-- caseid: string (nullable = true)\n##  |-- caseversion: string (nullable = true)\n##  |-- i_f_code: string (nullable = true)\n##  |-- i_f_code_num: string (nullable = true)\n##   ...\n##  |-- to_mfr: string (nullable = true)\n##  |-- occp_cod: string (nullable = true)\n##  |-- reporter_country: string (nullable = true)\n##  |-- occr_country: string (nullable = true)\n##  |-- occp_cod_num: string (nullable = true)\n\nfrom pyspark.sql.types import StructType\n\nschema = StructType.fromJson({\'fields\': [{\'metadata\': {},\n   \'name\': \'primaryid\',\n   \'nullable\': True,\n   \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'caseid\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'caseversion\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'i_f_code\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {},\n   \'name\': \'i_f_code_num\',\n   \'nullable\': True,\n   \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'event_dt\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'event_dt_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'mfr_dt\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'mfr_dt_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'init_fda_dt\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {},\n   \'name\': \'init_fda_dt_num\',\n   \'nullable\': True,\n   \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'fda_dt\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'fda_dt_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'rept_cod\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {},\n   \'name\': \'rept_cod_num\',\n   \'nullable\': True,\n   \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'auth_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'mfr_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'mfr_sndr\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'lit_ref\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'age\', \'nullable\': True, \'type\': \'double\'},\n  {\'metadata\': {}, \'name\': \'age_cod\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'age_grp\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'age_grp_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'sex\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'e_sub\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'wt\', \'nullable\': True, \'type\': \'double\'},\n  {\'metadata\': {}, \'name\': \'wt_cod\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'rept_dt\', \'nullable\': True, \'type\': \'integer\'},\n  {\'metadata\': {}, \'name\': \'rept_dt_num\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'to_mfr\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'occp_cod\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {},\n   \'name\': \'reporter_country\',\n   \'nullable\': True,\n   \'type\': \'string\'},\n  {\'metadata\': {}, \'name\': \'occr_country\', \'nullable\': True, \'type\': \'string\'},\n  {\'metadata\': {},\n   \'name\': \'occp_cod_num\',\n   \'nullable\': True,\n   \'type\': \'integer\'}],\n \'type\': \'struct\'})\n\n(spark.read.schema(schema).format("csv").options(header="true")\n    .load("/path/to/demo2016q1.csv"))\n'
"pd.options.mode.chained_assignment = None\n\nIn [220]: complete.is_copy\nOut[220]: &lt;weakref at 0x7f7f0b295b38; to 'DataFrame' at 0x7f7eee6fe668&gt;\n\ncomplete.is_copy = False       # deprecated as of version 0.24\n\ndef _check_setitem_copy(self, stacklevel=4, t='setting', force=False):\n    if force or self.is_copy:\n        ...\n\npd.options.mode.chained_assignment = None # None|'warn'|'raise'\n\ncomplete = complete.copy()\n"
"conda install python-docx --channel conda-forge\n\npip install python-docx\n\nimport docx\nimport pandas as pd\n\n# i am not sure how you are getting your data, but you said it is a\n# pandas data frame\ndf = pd.DataFrame(data)\n\n# open an existing document\ndoc = docx.Document('./test.docx')\n\n# add a table to the end and create a reference variable\n# extra row is so we can add the header row\nt = doc.add_table(df.shape[0]+1, df.shape[1])\n\n# add the header rows.\nfor j in range(df.shape[-1]):\n    t.cell(0,j).text = df.columns[j]\n\n# add the rest of the data frame\nfor i in range(df.shape[0]):\n    for j in range(df.shape[-1]):\n        t.cell(i+1,j).text = str(df.values[i,j])\n\n# save the doc\ndoc.save('./test.docx')\n"
"maxVal = 15\ndf['a'].where(df['a'] &lt;= maxVal, maxVal)      # where replace values with other when the \n                                              # condition is not satisfied\n\n#0    10\n#1    12\n#2    15\n#3    15\n#4    15\n#5    15\n#Name: a, dtype: int64\n\ndf['a'][df['a'] &gt;= maxVal] = maxVal\n"
"dfa = df_A.drop_duplicates(subset=['my_icon_number'])\ndfb = df_B.drop_duplicates(subset=['my_icon_number'])\n\nnew_df = pd.merge(dfa, dfb, how='inner', on='my_icon_number')\n\ndf_A = pd.DataFrame(dict(my_icon_number=[1, 2, 3, 4, 4, 4], other_column1=range(6)))\ndf_B = pd.DataFrame(dict(my_icon_number=[4, 4, 4, 5, 6, 7], other_column2=range(6)))\n\npd.merge(df_A, df_B,  how='inner', on='my_icon_number')\n\n   my_icon_number  other_column1  other_column2\n0               4              3              0\n1               4              3              1\n2               4              3              2\n3               4              4              0\n4               4              4              1\n5               4              4              2\n6               4              5              0\n7               4              5              1\n8               4              5              2\n"
"def coskew(df, bias=False):\n    v = df.values\n    s1 = sigma = v.std(0, keepdims=True)\n    means = v.mean(0, keepdims=True)\n\n    # means is 1 x n (n is number of columns\n    # this difference broacasts appropriately\n    v1 = v - means\n\n    s2 = sigma ** 2\n\n    v2 = v1 ** 2\n\n    m = v.shape[0]\n\n    skew = pd.DataFrame(v2.T.dot(v1) / s2.T.dot(s1) / m, df.columns, df.columns)\n\n    if not bias:\n        skew *= ((m - 1) * m) ** .5 / (m - 2)\n\n    return skew\n\ncoskew(df)\n\n          a         b\na -0.369380  0.096974\nb  0.325311  0.067020\n\ndf.skew()\n\na   -0.36938\nb    0.06702\ndtype: float64\n\ndef cokurt(df, bias=False, fisher=True, variant='middle'):\n    v = df.values\n    s1 = sigma = v.std(0, keepdims=True)\n    means = v.mean(0, keepdims=True)\n\n    # means is 1 x n (n is number of columns\n    # this difference broacasts appropriately\n    v1 = v - means\n\n    s2 = sigma ** 2\n    s3 = sigma ** 3\n\n    v2 = v1 ** 2\n    v3 = v1 ** 3\n\n    m = v.shape[0]\n\n    if variant in ['left', 'right']:\n        kurt = pd.DataFrame(v3.T.dot(v1) / s3.T.dot(s1) / m, df.columns, df.columns)\n        if variant == 'right':\n            kurt = kurt.T\n    elif variant == 'middle':\n        kurt = pd.DataFrame(v2.T.dot(v2) / s2.T.dot(s2) / m, df.columns, df.columns)\n\n    if not bias:\n        kurt = kurt * (m ** 2 - 1) / (m - 2) / (m - 3) - 3 * (m - 1) ** 2 / (m - 2) / (m - 3)\n    if not fisher:\n        kurt += 3\n\n    return kurt\n\ncokurt(df, variant='middle', bias=False, fisher=False)\n\n          a        b\na  1.882817  0.86649\nb  0.866490  1.63200\n\ncokurt(df, variant='left', bias=False, fisher=False)\n\n          a        b\na  1.882817  0.19175\nb -0.020567  1.63200\n\ndf.kurtosis() + 3\n\na    1.882817\nb    1.632000\ndtype: float64\n"
"df = df.assign(C=df['A']**2, D=df.B*2)\n\ndf = df.assign(**{'C': df.A.apply(lambda x: x ** 2), 'D': df.B * 2})\n\n   A   B   C   D\n0  1  11   1  22\n1  2  12   4  24\n2  3  13   9  26\n3  4  14  16  28\n"
"print (df)\n              val\n0  HF - Antartica\n1    HF - America\n2       HF - Asia\n\nprint (df.val.replace({'HF -':'Hi'}, regex=True))\n0    Hi Antartica\n1      Hi America\n2         Hi Asia\nName: val, dtype: object\n\nprint (df.val.str.replace('HF -', 'Hi'))\n0    Hi Antartica\n1      Hi America\n2         Hi Asia\nName: val, dtype: object\n"
"import math\n\nfor a in 'ABCD':\n    for b in 'ABCD':\n        count = 0\n\n        for x in document:\n            if a != b:\n                if a in x and b in x:\n                    count += 1\n\n            else:\n                n = x.count(a)\n                if n &gt;= 2:\n                    count += math.factorial(n)/math.factorial(n - 2)/2\n\n        print '{} x {} = {}'.format(a, b, count)\n"
'df2 = pd.DataFrame({ \n    \'A\' : 1.,\n    \'B\' : pd.Timestamp(\'20130102\'),\n    \'C\' : pd.Series(1,index=list(range(4)),dtype=\'float32\'),\n    \'D\' : np.array([3] * 4,dtype=\'int32\'),\n    \'E\' : pd.Categorical(["test","train","test","train"]),\n    \'F\' : \'foo\' })\n\nprint (df2)\n     A          B    C  D      E    F\n0  1.0 2013-01-02  1.0  3   test  foo\n1  1.0 2013-01-02  1.0  3  train  foo\n2  1.0 2013-01-02  1.0  3   test  foo\n3  1.0 2013-01-02  1.0  3  train  foo\n\nprint (df2.dtypes)\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n\ndf = pd.DataFrame({\'strings\':[\'a\',\'d\',\'f\'],\n                   \'dicts\':[{\'a\':4}, {\'c\':8}, {\'e\':9}],\n                   \'lists\':[[4,8],[7,8],[3]],\n                   \'tuples\':[(4,8),(7,8),(3,)],\n                   \'sets\':[set([1,8]), set([7,3]), set([0,1])] })\n\nprint (df)\n      dicts   lists    sets strings  tuples\n0  {\'a\': 4}  [4, 8]  {8, 1}       a  (4, 8)\n1  {\'c\': 8}  [7, 8]  {3, 7}       d  (7, 8)\n2  {\'e\': 9}     [3]  {0, 1}       f    (3,)\n\nprint (df.dtypes)\ndicts      object\nlists      object\nsets       object\nstrings    object\ntuples     object\ndtype: object\n\nfor col in df:\n    print (df[col].apply(type))\n\n0    &lt;class \'dict\'&gt;\n1    &lt;class \'dict\'&gt;\n2    &lt;class \'dict\'&gt;\nName: dicts, dtype: object\n0    &lt;class \'list\'&gt;\n1    &lt;class \'list\'&gt;\n2    &lt;class \'list\'&gt;\nName: lists, dtype: object\n0    &lt;class \'set\'&gt;\n1    &lt;class \'set\'&gt;\n2    &lt;class \'set\'&gt;\nName: sets, dtype: object\n0    &lt;class \'str\'&gt;\n1    &lt;class \'str\'&gt;\n2    &lt;class \'str\'&gt;\nName: strings, dtype: object\n0    &lt;class \'tuple\'&gt;\n1    &lt;class \'tuple\'&gt;\n2    &lt;class \'tuple\'&gt;\nName: tuples, dtype: object\n\nprint (type(df[\'strings\'].iat[0]))\n&lt;class \'str\'&gt;\n\nprint (type(df[\'dicts\'].iat[0]))\n&lt;class \'dict\'&gt;\n\nprint (type(df[\'lists\'].iat[0]))\n&lt;class \'list\'&gt;\n\nprint (type(df[\'tuples\'].iat[0]))\n&lt;class \'tuple\'&gt;\n\nprint (type(df[\'sets\'].iat[0]))\n&lt;class \'set\'&gt;\n'
"report = pd.DataFrame([\n        [1, 10, 'John'],\n        [1, 20, 'John'],\n        [1, 30, 'Tom'],\n        [1, 10, 'Bob'],\n        [2, 25, 'John'],\n        [2, 15, 'Bob']], columns = ['IssueKey','TimeSpent','User'])\n\n   IssueKey  TimeSpent  User\n0         1         10  John\n1         1         20  John\n2         1         30   Tom\n3         1         10   Bob\n4         2         25  John\n5         2         15   Bob\n\ntime_logged_by_user = report.groupby(['IssueKey', 'User']).TimeSpent.sum()\n\nIssueKey  User\n1         Bob     10\n          John    30\n          Tom     30\n2         Bob     15\n          John    25\n\nmax_time_logged_to_an_issue = time_logged_by_user.groupby(level='IssueKey').transform('max')\n\nIssueKey  User\n1         Bob     30\n          John    30\n          Tom     30\n2         Bob     25\n          John    25\n\nissue_owners = time_logged_by_user[time_logged_by_user == max_time_logged_to_an_issue]\n\nIssueKey  User\n1         John    30\n          Tom     30\n2         John    25\n"
"import matplotlib.pyplot as plt\nfrom matplotlib import six\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame()\ndf['x'] = np.arange(0,11)\ndf['y'] = df['x']*2\n\nfig = plt.figure(figsize=(8,5))\n\nax1 = fig.add_subplot(121)\nax1.scatter(x=df['x'],y=df['y'])\n\nax2 = fig.add_subplot(122)\nfont_size=14\nbbox=[0, 0, 1, 1]\nax2.axis('off')\nmpl_table = ax2.table(cellText = df.values, rowLabels = df.index, bbox=bbox, colLabels=df.columns)\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(font_size)\n"
'df = pd.DataFrame([["Australia", 1, 3, 5],\n                   ["Bambua", 12, 33, 56],\n                   ["Tambua", 14, 34, 58]\n                  ], columns=["Country", "Val1", "Val2", "Val10"]\n                 )\n\n&gt;&gt;&gt; val1_minus_val10 = df["Val1"] - df["Val10"]\n&gt;&gt;&gt; print(val1_minus_val10)\n0    -4\n1   -44\n2   -44\ndtype: int64\n'
'import seaborn as sns \nsns.heatmap(dataframe, xticklabels=True, yticklabels=True)\n'
'pip install --user tables\n'
"In [14]: right\nOut[14]: \n    ST_NAME  value2\n0    Oregon   6.218\n1  Nebraska   0.001\n\nIn [15]: merge(left, right)\nOut[15]: \n    ST_NAME  value  value2\n0  Nebraska  2.491   0.001\n1    Oregon  4.685   6.218\n\nIn [18]: merge(left, right, on='ST_NAME', sort=False)\nOut[18]: \n    ST_NAME  value  value2\n0    Oregon  4.685   6.218\n1  Nebraska  2.491   0.001\n"
"ts.resample('W-MON')\n"
'In [5]: row = np.random.randn(100)\n\nIn [6]: def method1():\n   ...:     df = DataFrame(columns=range(100),index=range(1000))\n   ...:     for i in xrange(len(df)):\n   ...:         df.iloc[i] = row\n   ...:     return df\n   ...: \n\nIn [9]: def method2():\n   ...:     return DataFrame([ row for i in range(1000) ])\n   ...: \n\nIn [13]: def method3():\n   ....:     df = DataFrame(columns=range(100),index=range(1000)).T\n   ....:     for i in xrange(1000):\n   ....:         df[i] = row\n   ....:     return df.T\n   ....: \n\nIn [22]: (method2() == method1()).all().all()\nOut[22]: True\n\nIn [23]: (method2() == method3()).all().all()\nOut[23]: True\n\n\nIn [8]: %timeit method1()\n1 loops, best of 3: 1.76 s per loop\n\nIn [10]: %timeit method2()\n1000 loops, best of 3: 7.79 ms per loop\n\nIn [14]: %timeit method3()\n1 loops, best of 3: 1.33 s per loop\n'
"In [36]: df = DataFrame(date_range('20000101',periods=150000,freq='H'),columns=['Date'])\n\nIn [37]: df.head(5)\nOut[37]: \n                 Date\n0 2000-01-01 00:00:00\n1 2000-01-01 01:00:00\n2 2000-01-01 02:00:00\n3 2000-01-01 03:00:00\n4 2000-01-01 04:00:00\n\n[5 rows x 1 columns]\n\nIn [38]: %timeit f(df)\n10 loops, best of 3: 22 ms per loop\n\nIn [39]: def f(df):\n    df = df.copy()\n    df['Year'] = DatetimeIndex(df['Date']).year\n    df['Month'] = DatetimeIndex(df['Date']).month\n    df['Day'] = DatetimeIndex(df['Date']).day\n    return df\n   ....: \n\nIn [40]: f(df).head()\nOut[40]: \n                 Date  Year  Month  Day\n0 2000-01-01 00:00:00  2000      1    1\n1 2000-01-01 01:00:00  2000      1    1\n2 2000-01-01 02:00:00  2000      1    1\n3 2000-01-01 03:00:00  2000      1    1\n4 2000-01-01 04:00:00  2000      1    1\n\n[5 rows x 4 columns]\n\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month\ndf['Day'] = df['Date'].dt.day\n"
"import numpy as np\nfrom scipy import sparse\n\nX = sparse.rand(10, 10000)\nxt = np.random.random((10, 1))\nprint 'X shape:', X.shape\nprint 'xt shape:', xt.shape\nprint 'Stacked shape:', np.hstack((X,xt)).shape\n#print 'Stacked shape:', sparse.hstack((X,xt)).shape #This works\n\nX shape: (10, 10000)\nxt shape: (10, 1)\n\nValueError: all the input arrays must have same number of dimensions\n\nTypeError: no supported conversion for types: (dtype('float64'), dtype('O'))\n\nX = sparse.rand(100, 10000)\nxt = np.random.random((100, 1))\nxt = xt.astype('object') # Comment this to fix the error\nprint 'X:', X.shape, X.dtype\nprint 'xt:', xt.shape, xt.dtype\nprint 'Stacked shape:', sparse.hstack((X,xt)).shape\n\nTypeError: no supported conversion for types: (dtype('float64'), dtype('O'))\n"
'df_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'],index_col=\'Date\')\n\nstart = datetime(2010, 2, 5)\nend = datetime(2012, 10, 26)\n\ndf_train_fly = pd.date_range(start, end, freq="W-FRI")\ndf_train_fly = pd.DataFrame(pd.Series(df_train_fly), columns=[\'Date\'])\n\nmerged = df_train_csv.join(df_train_fly.set_index([\'Date\']), on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\n\n# remove the index_col param\ndf_train_csv = pd.read_csv(\'./train.csv\',parse_dates=[\'Date\'])\n# don\'t set the index on df_train_fly\nmerged = df_train_csv.join(df_train_fly, on = [\'Date\'], how = \'right\', lsuffix=\'_x\')\n\nmerged = df_train_csv.join(df_train_fly, how = \'right\', lsuffix=\'_x\')\n\nmerged = df_train_csv.merge(df_train_fly.set_index([\'Date\']), left_index=True, right_index=True, how = \'right\', lsuffix=\'_x\')\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt;\n&gt;&gt;&gt; foo = pd.DataFrame([\n...     ['USA',1,2],\n...     ['Canada',3,4],\n...     ['Canada',5,6]\n... ], columns=('Country', 'x', 'y'))\n&gt;&gt;&gt;\n&gt;&gt;&gt; z = foo['x'].where(foo['Country'] == 'USA', foo['y'])\n&gt;&gt;&gt; pd.concat([foo['Country'], z], axis=1)\n  Country  x\n0     USA  1\n1  Canada  4\n2  Canada  6\n\n&gt;&gt;&gt; pd.concat([foo['Country'], z], keys=['Country', 'z'], axis=1)\n  Country  z\n0     USA  1\n1  Canada  4\n2  Canada  6\n"
"df['LSE_cat'] = pd.Categorical(\n    df['LSE'], \n    categories=['Oands','Wetnds','Develd','Cn','Soys','Otherg','Wht'], \n    ordered=True\n)\ndf.sort('LSE_cat')\nOut[5]: \n   Region     LSE       North      South LSE_cat\n3       3   Oands  -47.986764 -32.324991   Oands\n2       2  Wetnds  -38.480206 -46.089908  Wetnds\n1       1  Develd  -36.157025 -27.669988  Develd\n0       0      Cn   33.330367   9.178917      Cn\n5       5    Soys   34.936147   4.072872    Soys\n4       4  Otherg  323.209834  28.486310  Otherg\n6       6     Wht    0.983977 -14.972555     Wht\n\ndf.sort_values('LSE_cat')\n"
"pandas.set_option('display.max_columns', None)\n\ndf.to_csv('myfile.csv')\n\ndf.head(1000).to_csv('myfile.csv')\n"
"import numpy as np\nimport pandas as pd\n\ndef h5store(filename, df, **kwargs):\n    store = pd.HDFStore(filename)\n    store.put('mydata', df)\n    store.get_storer('mydata').attrs.metadata = kwargs\n    store.close()\n\ndef h5load(store):\n    data = store['mydata']\n    metadata = store.get_storer('mydata').attrs.metadata\n    return data, metadata\n\na = pd.DataFrame(\n    data=pd.np.random.randint(0, 100, (10, 5)), columns=list('ABCED'))\n\nfilename = '/tmp/data.h5'\nmetadata = dict(local_tz='US/Eastern')\nh5store(filename, a, **metadata)\nwith pd.HDFStore(filename) as store:\n    data, metadata = h5load(store)\n\nprint(data)\n#     A   B   C   E   D\n# 0   9  20  92  43  25\n# 1   2  64  54   0  63\n# 2  22  42   3  83  81\n# 3   3  71  17  64  53\n# 4  52  10  41  22  43\n# 5  48  85  96  72  88\n# 6  10  47   2  10  78\n# 7  30  80   3  59  16\n# 8  13  52  98  79  65\n# 9   6  93  55  40   3\n\nprint(metadata)\n\n{'local_tz': 'US/Eastern'}\n"
'import numpy as np\nimport pandas as pd\n\n# Sample 100 rows of data to determine dtypes.\ndf_test = pd.read_csv(filename, nrows=100)\n\nfloat_cols = [c for c in df_test if df_test[c].dtype == "float64"]\nfloat32_cols = {c: np.float32 for c in float_cols}\n\ndf = pd.read_csv(filename, engine=\'c\', dtype=float32_cols)\n\ndf = pd.read_csv(filename, nrows=100)\n&gt;&gt;&gt; df\n   int_col  float1 string_col  float2\n0        1     1.2          a     2.2\n1        2     1.3          b     3.3\n2        3     1.4          c     4.4\n\n&gt;&gt;&gt; df.info()\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 3 entries, 0 to 2\nData columns (total 4 columns):\nint_col       3 non-null int64\nfloat1        3 non-null float64\nstring_col    3 non-null object\nfloat2        3 non-null float64\ndtypes: float64(2), int64(1), object(1)\n\ndf32 = pd.read_csv(filename, engine=\'c\', dtype={c: np.float32 for c in float_cols})\n&gt;&gt;&gt; df32.info()\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nInt64Index: 3 entries, 0 to 2\nData columns (total 4 columns):\nint_col       3 non-null int64\nfloat1        3 non-null float32\nstring_col    3 non-null object\nfloat2        3 non-null float32\ndtypes: float32(2), int64(1), object(1)\n'
"In [68]:\n\nsummary_ave_data = df.copy()\nsummary_ave_data['average'] = summary_ave_data.mean(numeric_only=True, axis=1)\nsummary_ave_data\nOut[68]:\n                 Time         F7         F8         F9    average\n0 2015-07-29 00:00:00  43.005593 -56.509746  25.271271   3.922373\n1 2015-07-29 01:00:00  55.114918 -59.173852  31.849262   9.263443\n2 2015-07-29 02:00:00  63.990762 -64.699492  52.426017  17.239096\n"
'In[1]:\nimport pandas as pd\nimport io\n\ndata = """\nname score\nA      1\nA      2\nA      3\nA      4\nA      5\nB      2\nB      4\nB      6\nB      8\n    """\n\ndf = pd.read_csv(io.StringIO(data), delimiter=\'\\s+\')\nprint(df)\n\nOut[1]:\n  name  score\n0    A      1\n1    A      2\n2    A      3\n3    A      4\n4    A      5\n5    B      2\n6    B      4\n7    B      6\n8    B      8\n\nIn[2]:\ndf2 = pd.DataFrame(group.describe().rename(columns={\'score\':name}).squeeze()\n                         for name, group in df.groupby(\'name\'))\n\nprint(df2)\n\nOut[2]:\n   count  mean       std  min  25%  50%  75%  max\nA      5     3  1.581139    1  2.0    3  4.0    5\nB      4     5  2.581989    2  3.5    5  6.5    8\n'
"import numpy\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport pandas\n\nN = 20\nnumpy.random.seed(N)\n\ndates = pandas.date_range('1/1/2014', periods=N, freq='m')\ndf = pandas.DataFrame(\n    data=numpy.random.randn(N), \n    index=dates,\n    columns=['A']\n)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\nax.bar(df.index, df['A'], width=25, align='center')\n"
"print df\n  Month LSOA       code  Longitude   Latitude             Crime type\n0    2015-01  E01000916  -0.106453  51.518207          Bicycle theft\n1    2015-01  E01000914  -0.111497  51.518226               Burglary\n2    2015-01  E01000914  -0.111497  51.518226               Burglary\n3    2015-01  E01000914  -0.111497  51.518226            Other theft\n4    2015-01  E01000914  -0.113767  51.517372  Theft from the person\n\ndf = df.groupby(['Longitude', 'Latitude', 'Crime type']).size().reset_index(name='count')\nprint df\n   Longitude   Latitude             Crime type  count\n0  -0.113767  51.517372  Theft from the person      1\n1  -0.111497  51.518226               Burglary      2\n2  -0.111497  51.518226            Other theft      1\n3  -0.106453  51.518207          Bicycle theft      1\n\nprint df['count']\n0    1\n1    2\n2    1\n3    1\nName: count, dtype: int64\n"
"a = df[df['newest_date_available'] &lt; date_before]\n\n        id  code newest_date_available\n0  9793708  3514            2015-12-24\n1  9792282  2399            2015-12-25\n2  9797602  7452            2015-12-25\n"
"In [92]:\npd.concat([df1,df2.rename(columns={'b':'a'})], ignore_index=True)\n\nOut[92]:\n    a   x   y\n0   1   4   7\n1   2   5   8\n2   3   6   9\n3  10  13  16\n4  11  14  17\n5  12  15  18\n\nIn [103]:\ndf1.merge(df2.rename(columns={'b':'a'}),how='outer')\n\nOut[103]:\n    a   x   y\n0   1   4   7\n1   2   5   8\n2   3   6   9\n3  10  13  16\n4  11  14  17\n5  12  15  18\n"
"In [359]: df_1\nOut[359]: \n   A    B\n0  a  AAA\n1  b  BBA\n2  c  CCF\n\nIn [360]: df_3\nOut[360]: \n  key  value\n0   a      1\n1   a      2\n2   b      3\n3   a      4\n\nIn [361]: df_1.merge(df_3, how='left', left_on='A', right_on='key')\nOut[361]: \n   A    B  key  value\n0  a  AAA    a    1.0\n1  a  AAA    a    2.0\n2  a  AAA    a    4.0\n3  b  BBA    b    3.0\n4  c  CCF  NaN    NaN\n"
"df = df.replace('white', np.nan)\n\nIn [50]:\nd = {'color' : pd.Series(['white', 'blue', 'orange']),\n   'second_color': pd.Series(['white', 'black', 'blue']),\n   'value' : pd.Series([1., 2., 3.])}\ndf = pd.DataFrame(d)\ndf.replace('white', np.nan, inplace=True)\ndf\n\nOut[50]:\n    color second_color  value\n0     NaN          NaN    1.0\n1    blue        black    2.0\n2  orange         blue    3.0\n"
'df[\'Country\'] = df[\'Country\'].str.replace(u"Å", "A")\ndf[\'City\'] = df[\'City\'].str.replace(u"ë", "e")\n'
"df['value'] = df['value'].str[0]\n\ndf['value'] = df['value'].str.get(0)\n\ndf = pd.DataFrame({'value':[[63],[65],[64]]})\nprint (df)\n  value\n0  [63]\n1  [65]\n2  [64]\n\n#check type if index 0 exist\nprint (type(df.loc[0, 'value']))\n&lt;class 'list'&gt;\n\n#check type generally, index can be `DatetimeIndex`, `FloatIndex`...\nprint (type(df.loc[df.index[0], 'value']))\n&lt;class 'list'&gt;\n\ndf['value'] = df['value'].str.get(0)\nprint (df)\n   value\n0     63\n1     65\n2     64\n\ndf['value'] = df['value'].str.strip('[]').astype(int)\n\ndf = pd.DataFrame({'value':['[63]','[65]','[64]']})\nprint (df)\n  value\n0  [63]\n1  [65]\n2  [64]\n\n#check type if index 0 exist\nprint (type(df.loc[0, 'value']))\n&lt;class 'str'&gt;\n\n#check type generally, index can be `DatetimeIndex`, `FloatIndex`...\nprint (type(df.loc[df.index[0], 'value']))\n&lt;class 'str'&gt;\n\n\ndf['value'] = df['value'].str.strip('[]').astype(int)\nprint (df)\n  value\n0    63\n1    65\n2    64\n"
"import pandas as pd\nfrom datetime import timedelta\n\ndf = pd.DataFrame([['2016-01-10',28],['2016-05-11',28],['2016-02-23',15],['2015-12-08',30]], \n                      columns = ['ship_string','days_supply'])\n\ndf['ship_date'] = pd.to_datetime(df['ship_string'])\n\ndf['time_added'] = pd.to_timedelta(df['days_supply'],'d')\ndf['supply_ended'] = df['ship_date'] + df['time_added']\n\nprint df\n\n  ship_string  days_supply  ship_date  time_added supply_ended\n0  2016-01-10           28 2016-01-10     28 days   2016-02-07\n1  2016-05-11           28 2016-05-11     28 days   2016-06-08\n2  2016-02-23           15 2016-02-23     15 days   2016-03-09\n3  2015-12-08           30 2015-12-08     30 days   2016-01-07\n"
"df = pd.DataFrame({'A':['type1','type2','type2'],\n                   'B':['type1','type2','type3'],\n                   'C':['type1','type3','type3']})\n\nprint (df)\n       A      B      C\n0  type1  type1  type1\n1  type2  type2  type3\n2  type2  type3  type3\n\nprint (df.apply(lambda x: pd.factorize(x)[0]))\n   A  B  C\n0  0  0  0\n1  1  1  1\n2  1  2  1\n\nprint (df.stack().rank(method='dense').unstack())\n     A    B    C\n0  1.0  1.0  1.0\n1  2.0  2.0  3.0\n2  2.0  3.0  3.0\n\ndf[['B','C']] = df[['B','C']].stack().rank(method='dense').unstack()\nprint (df)\n       A    B    C\n0  type1  1.0  1.0\n1  type2  2.0  3.0\n2  type2  3.0  3.0\n\nstacked = df[['B','C']].stack()\ndf[['B','C']] = pd.Series(stacked.factorize()[0], index=stacked.index).unstack()\nprint (df)\n       A  B  C\n0  type1  0  0\n1  type2  1  2\n2  type2  2  2\n\nvals = df.stack().drop_duplicates().values\nb = [x for x in df.stack().drop_duplicates().rank(method='dense')]\n\nd1 = dict(zip(b, vals))\nprint (d1)\n{1.0: 'type1', 2.0: 'type2', 3.0: 'type3'}\n\ndf1 = df.stack().rank(method='dense').unstack()\nprint (df1)\n     A    B    C\n0  1.0  1.0  1.0\n1  2.0  2.0  3.0\n2  2.0  3.0  3.0\n\nprint (df1.stack().map(d1).unstack())\n       A      B      C\n0  type1  type1  type1\n1  type2  type2  type3\n2  type2  type3  type3\n"
'with open(\'df.json\', \'w\', encoding=\'utf-8\') as file:\n    df.to_json(file, force_ascii=False)\n\n{"0":{"0":"τ","1":"π"},"1":{"0":"a","1":"b"},"2":{"0":1,"1":2}}\n'
"from sklearn import preprocessing    \n# Test data\ndf = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])    \ndf['Fact'] = pd.factorize(df['Col'])[0]\nle = preprocessing.LabelEncoder()\ndf['Lab'] = le.fit_transform(df['Col'])\n\nprint(df)\n#   Col  Fact  Lab\n# 0   A     0    0\n# 1   B     1    1\n# 2   B     1    1\n# 3   C     2    2\n\ndf = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])\ndf = pd.get_dummies(df)\n\nprint(df)\n#    Col_A  Col_B  Col_C\n# 0    1.0    0.0    0.0\n# 1    0.0    1.0    0.0\n# 2    0.0    1.0    0.0\n# 3    0.0    0.0    1.0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ndf = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])\n# We need to transform first character into integer in order to use the OneHotEncoder\nle = preprocessing.LabelEncoder()\ndf['Col'] = le.fit_transform(df['Col'])\nenc = OneHotEncoder()\ndf = DataFrame(enc.fit_transform(df).toarray())\n\nprint(df)\n#      0    1    2\n# 0  1.0  0.0  0.0\n# 1  0.0  1.0  0.0\n# 2  0.0  1.0  0.0\n# 3  0.0  0.0  1.0\n"
"import glob\nimport pandas as pd\n\n# Give the filename you wish to save the file to\nfilename = 'Your_filename.csv'\n\n# Use this function to search for any files which match your filename\nfiles_present = glob.glob(filename)\n\n\n# if no matching files, write to csv, if there are matching files, print statement\nif not files_present:\n    pd.to_csv(filename)\nelse:\n    print 'WARNING: This file already exists!' \n"
"df.rename(index={'alpha': 'mu'})\n"
"df = pd.DataFrame(dictionary, columns=['Date', 'Open', 'Close'])  \ndf = df.set_index('Date')       \nprint (df)\n                      Open  Close\nDate                             \n2016/11/22 07:00:00  47.47  47.48\n2016/11/22 06:59:00  47.46  47.45\n2016/11/22 06:58:00  47.38  47.40\n\ndf = pd.DataFrame(dictionary, columns=['Date', 'Open', 'Close'])  \ndf.set_index('Date', inplace=True)       \nprint (df)\n                      Open  Close\nDate                             \n2016/11/22 07:00:00  47.47  47.48\n2016/11/22 06:59:00  47.46  47.45\n2016/11/22 06:58:00  47.38  47.40\n\ndf = pd.DataFrame({k: v for k, v in dictionary.items() if not k == 'Date'}, \n                   index=dictionary['Date'], \n                   columns=['Open','Close'])  \ndf.index.name = 'Date'\nprint (df)\n                      Open  Close\nDate                             \n2016/11/22 07:00:00  47.47  47.48\n2016/11/22 06:59:00  47.46  47.45\n2016/11/22 06:58:00  47.38  47.40\n"
'import seaborn as sns\n\nsns.regplot(x=\'motifScore\', y=\'expression\', data=motif)\n\nimport statsmodels.api as sm\n\n# regress "expression" onto "motifScore" (plus an intercept)\nmodel = sm.OLS(motif.expression, sm.add_constant(motif.motifScore))\np = model.fit().params\n\n# generate x-values for your regression line (two is sufficient)\nx = np.arange(1, 3)\n\n# scatter-plot data\nax = motif.plot(x=\'motifScore\', y=\'expression\', kind=\'scatter\')\n\n# plot regression line on the same axes, set x-axis limits\nax.plot(x, p.const + p.motifScore * x)\nax.set_xlim([1, 2])\n\nimport statsmodels.api as sm\nfrom statsmodels.graphics.regressionplots import abline_plot\n\n# regress "expression" onto "motifScore" (plus an intercept)\nmodel = sm.OLS(motif.expression, sm.add_constant(motif.motifScore))\n\n# scatter-plot data\nax = motif.plot(x=\'motifScore\', y=\'expression\', kind=\'scatter\')\n\n# plot regression line\nabline_plot(model_results=model.fit(), ax=ax)\n'
"dfTest2 = pd.DataFrame(dict(InvoiceDate=pd.to_datetime(['2017-06-01', pd.NaT])))\n\ndfTest2.InvoiceDate.astype(object).where(dfTest2.InvoiceDate.notnull(), None)\n\n0    2017-06-01 00:00:00\n1                   None\nName: InvoiceDate, dtype: object\n"
'out = pd.cut(s, bins=[0, 0.35, 0.7, 1], include_lowest=True)\nax = out.value_counts(sort=False).plot.bar(rot=0, color="b", figsize=(6,4))\nax.set_xticklabels([c[1:-1].replace(","," to") for c in out.cat.categories])\nplt.show()\n\nout = pd.cut(s, bins=[0, 0.35, 0.7, 1], include_lowest=True)\nout_norm = out.value_counts(sort=False, normalize=True).mul(100)\nax = out_norm.plot.bar(rot=0, color="b", figsize=(6,4))\nax.set_xticklabels([c[1:-1].replace(","," to") for c in out.cat.categories])\nplt.ylabel("pct")\nplt.show()\n'
"for idx, row in df.iterrows():\n    if  df.loc[idx,'Qty'] == 1 and df.loc[idx,'Price'] == 10:\n        df.loc[idx,'Buy'] = 1\n\nmask = (df['Qty'] == 1) &amp; (df['Price'] == 10)\ndf.loc[mask, 'Buy'] = 1\n\ndf['Buy'] = df['Buy'].mask(mask, 1)\n\ndf['Buy'] = np.where(mask, 1, 0)\n\ndf = pd.DataFrame({'Buy': [100, 200, 50], \n                   'Qty': [5, 1, 1], \n                   'Name': ['apple', 'pear', 'banana'], \n                   'Price': [1, 10, 10]})\n\nprint (df)\n   Buy    Name  Price  Qty\n0  100   apple      1    5\n1  200    pear     10    1\n2   50  banana     10    1\n\nmask = (df['Qty'] == 1) &amp; (df['Price'] == 10)\n\n\ndf['Buy'] = df['Buy'].mask(mask, 1)\nprint (df)\n   Buy    Name  Price  Qty\n0  100   apple      1    5\n1    1    pear     10    1\n2    1  banana     10    1\n\ndf['Buy'] = np.where(mask, 1, 0)\nprint (df)\n   Buy    Name  Price  Qty\n0    0   apple      1    5\n1    1    pear     10    1\n2    1  banana     10    1\n"
'df = pd.DataFrame(np.random.rand(6, 4),\n                 index=[\'one\', \'two\', \'three\', \'four\', \'five\', \'six\'],\n                 columns=pd.Index([\'A\', \'B\', \'C\', \'D\'],\n                 name=\'Genus\')).round(2)\n\nax = df.plot(kind=\'bar\',figsize=(10,4), rot = 0)\n\n# "Activate" minor ticks\nax.minorticks_on()\n\n# Get location of the center of each rectangle\nrects_locs = map(lambda x: x.get_x() +x.get_width()/2., ax.patches)\n# Set minor ticks there\nax.set_xticks(rects_locs, minor = True)\n\n\n# Labels for the rectangles\nnew_ticks = reduce(lambda x, y: x + y, map(lambda x: [x] * df.shape[0], df.columns.tolist()))\n# Set the labels\nfrom matplotlib import ticker\nax.xaxis.set_minor_formatter(ticker.FixedFormatter(new_ticks))  #add the custom ticks\n\n# Move the category label further from x-axis\nax.tick_params(axis=\'x\', which=\'major\', pad=15)\n\n# Remove minor ticks where not necessary\nax.tick_params(axis=\'x\',which=\'both\', top=\'off\')\nax.tick_params(axis=\'y\',which=\'both\', left=\'off\', right = \'off\')\n'
'# sample data\nimport numpy as np\nimport pandas as pd\n\nN = 30\ndrange = pd.date_range("2014-01", periods=N, freq="MS")\nvalues = {\'values\':np.random.randint(1,20,size=N)}\ndf = pd.DataFrame(values, index=drange)\n\n# use formatters to specify major and minor ticks\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nfig, ax = plt.subplots()\nax.plot(df.index, df.values)\nax.set_xticks(df.index)\nax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m"))\nax.xaxis.set_minor_formatter(mdates.DateFormatter("%Y-%m"))\n_=plt.xticks(rotation=90)    \n'
"df['liststring'] = [','.join(map(str, l)) for l in df['lists']]\ndf\n\n                lists    liststring\n0  [1, 2, 12, 6, ABC]  1,2,12,6,ABC\n1     [1000, 4, z, a]    1000,4,z,a\n\ndef try_join(l):\n    try:\n        return ','.join(map(str, l))\n    except TypeError:\n        return np.nan\n\ndf['liststring'] = [try_join(l) for l in df['lists']]\n\ndf['liststring'] = df['lists'].apply(lambda x: ','.join(map(str, x)))\n\ndf['liststring'] = df['lists'].agg(lambda x: ','.join(map(str, x)))\n\ndf\n                lists    liststring\n0  [1, 2, 12, 6, ABC]  1,2,12,6,ABC\n1     [1000, 4, z, a]    1000,4,z,a\n\ndf['liststring'] = (pd.DataFrame(df.lists.tolist())\n                      .fillna('')\n                      .astype(str)\n                      .agg(','.join, 1)\n                      .str.strip(','))\n\ndf\n                lists    liststring\n0  [1, 2, 12, 6, ABC]  1,2,12,6,ABC\n1     [1000, 4, z, a]    1000,4,z,a\n"
"g = df.groupby('class')\ng.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))\n\n  class  val\n0    c1    1\n1    c1    1\n2    c2    2\n3    c2    2\n4    c3    3\n5    c3    3\n"
"dfST['timestamp'] = pd.to_datetime(dfST['timestamp'])\n\ndfST['new_date_column'] = dfST['timestamp'].dt.date\n"
"pd['irr'] = np.where(pd['cs']*0.63 &gt; pd['irr'], 1.0, 0.0)\n\npd['irr'] = (pd['cs']*0.63 &gt; pd['irr']).astype(float)\n\npd = pd.DataFrame({'cs':[1,2,5],\n                   'irr':[0,100,0.04]})\n\nprint (pd)\n   cs     irr\n0   1    0.00\n1   2  100.00\n2   5    0.04\n\npd['irr'] = (pd['cs']*0.63 &gt; pd['irr']).astype(float)\nprint (pd)\n   cs  irr\n0   1  1.0\n1   2  0.0\n2   5  1.0\n"
"df.groupby('year').case_status.value_counts().unstack().plot.barh()\n\ndf.groupby('year').case_status.value_counts().unstack(0).plot.barh()\n"
"with open('C:/Users/Alberto/nutrients.json', 'rU') as f:\n\ndf = pd.read_json('C:/Users/Alberto/nutrients.json', lines=True)\n"
"df['moving'] = df.groupby('object')['value'].transform(lambda x: x.rolling(10, 1).mean())\n"
'apt-get install python3 python3-pip redis-server\npip3 install pandas pyarrow redis\n\nimport pandas as pd\nimport pyarrow as pa\nimport redis\n\ndf=pd.DataFrame({\'A\':[1,2,3]})\nr = redis.Redis(host=\'localhost\', port=6379, db=0)\n\ncontext = pa.default_serialization_context()\nr.set("key", context.serialize(df).to_buffer().to_pybytes())\ncontext.deserialize(r.get("key"))\n   A\n0  1\n1  2\n2  3\n'
"In [46]: df\nOut[46]:\n     A   B\n0  foo  10\n1  bar  20\n2  baz  30\n\nIn [47]: cond = df['A'].str.contains('a') &amp; (df['B'] == 20)\n\nIn [48]: df.drop(df[cond].index.values)\nOut[48]:\n     A   B\n0  foo  10\n2  baz  30\n"
"In [10]: ser.value_counts()\nOut[10]: \ntwo      3\none      1\nthree    1\n\nser.value_counts().plot(kind='bar')\n\nIn [12]: ser.value_counts().reindex(ser[:3])\nOut[12]: \none      1\ntwo      3\nthree    1\n"
"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame(np.arange(50), columns=['filtercol'])\nfilter_values = [0, 5, 17, 33]   \nout = pd.cut(df.filtercol, bins=filter_values)\ncounts = pd.value_counts(out)\n# counts is a Series\nprint(counts)\n\n(17, 33]    16\n(5, 17]     12\n(0, 5]       5\n\ncounts.sort_index()\n\n(0, 5]       5\n(5, 17]     12\n(17, 33]    16\n"
's = df.groupby("keys").ids.agg(lambda x:len(x.unique()))\npd.value_counts(s).plot(kind="bar")\n'
'df[:-m]\n\ndf.drop(df.index[3:5])\n'
"import matplotlib.pyplot as plt\n\nfor title, group in df.groupby('ModelID'):\n    group.plot(x='saleDate', y='MeanToDate', title=title)\n"
"def changeencode(data, cols):\n    for col in cols:\n        data[col] = data[col].str.decode('iso-8859-1').str.encode('utf-8')\n    return data   \n"
'parse_dates : boolean, list of ints or names, list of lists, or dict\nkeep_date_col : boolean, default False\ndate_parser : function\n\ntrue_values  : list  Values to consider as True\nfalse_values : list  Values to consider as False\n'
'import pandas.io.data as web\nts = web.DataReader("^GSPC", "yahoo", start=dt.date( 2013, 6, 1 ))[ \'Adj Close\' ]\n\nax = ts.plot()\nxtick = pd.date_range( start=ts.index.min( ), end=ts.index.max( ), freq=\'W\' )\nax.set_xticks( xtick, minor=True )\nax.grid(\'on\', which=\'minor\', axis=\'x\' )\nax.grid(\'off\', which=\'major\', axis=\'x\' )\n'
'In [8]: df.index\nOut[8]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8], dtype=\'int64\')\n\nIn [15]: new_column.index\nOut[15]: \nMultiIndex\n[(u\'X\', 3), (u\'X\', 1), (u\'X\', 0), (u\'Y\', 8), (u\'Y\', 7), (u\'Y\', 5), (u\'Z\', 6), (u\'Z\', 2), (u\'Z\', 4)]\n\nIn [13]: new_column = df.groupby(\'L1\', as_index=False).apply(lambda x : pd.expanding_sum(x.sort(\'L3\', ascending=False)[\'L3\'])/x[\'L3\'].sum())\nIn [14]: df["new"] = new_column.reset_index(level=0, drop=True)\n'
"In [11]: dates = pd.date_range('20130101', periods=6)\n\nIn [12]: dates\nOut[12]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2013-01-01 00:00:00, ..., 2013-01-06 00:00:00]\nLength: 6, Freq: D, Timezone: None\n\nIn [13]: isinstance(dates, pd.DatetimeIndex)\nOut[13]: True\n"
"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport patsy\n\ndef areg(formula,data=None,absorb=None,cluster=None): \n\n    y,X = patsy.dmatrices(formula,data,return_type='dataframe')\n\n    ybar = y.mean()\n    y = y -  y.groupby(data[absorb]).transform('mean') + ybar\n\n    Xbar = X.mean()\n    X = X - X.groupby(data[absorb]).transform('mean') + Xbar\n\n    reg = sm.OLS(y,X)\n    # Account for df loss from FE transform\n    reg.df_resid -= (data[absorb].nunique() - 1)\n\n    return reg.fit(cov_type='cluster',cov_kwds={'groups':data[cluster].values})\n\nreg0 = areg('ret~retlag',data=df,absorb='caldt',cluster='caldt')\n\n&gt;&gt;&gt; from pandas.stats.plm import PanelOLS\n&gt;&gt;&gt; df\n\n                y    x\ndate       id\n2012-01-01 1   0.1  0.2\n           2   0.3  0.5\n           3   0.4  0.8\n           4   0.0  0.2\n2012-02-01 1   0.2  0.7 \n           2   0.4  0.5\n           3   0.2  0.3\n           4   0.1  0.1\n2012-03-01 1   0.6  0.9\n           2   0.7  0.5\n           3   0.9  0.6\n           4   0.4  0.5\n\n&gt;&gt;&gt; reg  = PanelOLS(y=df['y'],x=df[['x']],time_effects=True)\n&gt;&gt;&gt; reg\n\n-------------------------Summary of Regression Analysis-------------------------\n\nFormula: Y ~ &lt;x&gt;\n\nNumber of Observations:         12\nNumber of Degrees of Freedom:   4\n\nR-squared:         0.2729\nAdj R-squared:     0.0002\n\nRmse:              0.1588\n\nF-stat (1, 8):     1.0007, p-value:     0.3464\n\nDegrees of Freedom: model 3, resid 8\n\n-----------------------Summary of Estimated Coefficients------------------------\n      Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%\n--------------------------------------------------------------------------------\n             x     0.3694     0.2132       1.73     0.1214    -0.0485     0.7872\n---------------------------------End of Summary--------------------------------- \n\nPanelOLS(self, y, x, weights = None, intercept = True, nw_lags = None,\nentity_effects = False, time_effects = False, x_effects = None,\ncluster = None, dropped_dummies = None, verbose = False,\nnw_overlap = False)\n\nImplements panel OLS.\n\nSee ols function docs\n"
'In [11]: a = df.values\n\nIn [12]: a.sort(axis=1)  # no ascending argument\n\nIn [13]: a = a[:, ::-1]  # so reverse\n\nIn [14]: a\nOut[14]:\narray([[8, 4, 3, 1],\n       [9, 7, 2, 2]])\n\nIn [15]: pd.DataFrame(a, df.index, df.columns)\nOut[15]:\n   A  B  C  D\n0  8  4  3  1\n1  9  7  2  2\n\nIn [21]: df.sort(axis=1, ascending=False)\nOut[21]:\n   D  C  B  A\n0  1  8  4  3\n1  2  7  2  9\n\nIn [22]: df.sort(df.columns, axis=1, ascending=False)\n'
"&gt;&gt;&gt; t1 = np.array([0, 0.5, 1.0, 1.5, 2.0])\n&gt;&gt;&gt; y1 = pd.Series(t1, index=t1)\n\n&gt;&gt;&gt; t2 = np.array([0, 0.34, 1.01, 1.4, 1.6, 1.7, 2.01])\n&gt;&gt;&gt; y2 = pd.Series(3*t2, index=t2)\n\n&gt;&gt;&gt; df = pd.DataFrame({'y1': y1, 'y2': y2})\n&gt;&gt;&gt; df\n       y1    y2\n0.00  0.0  0.00\n0.34  NaN  1.02\n0.50  0.5   NaN\n1.00  1.0   NaN\n1.01  NaN  3.03\n1.40  NaN  4.20\n1.50  1.5   NaN\n1.60  NaN  4.80\n1.70  NaN  5.10\n2.00  2.0   NaN\n2.01  NaN  6.03\n\n&gt;&gt;&gt; df.interpolate('index').reindex(y1)\n      y1   y2\n0.0  0.0  0.0\n0.5  0.5  1.5\n1.0  1.0  3.0\n1.5  1.5  4.5\n2.0  2.0  6.0\n"
"In [48]: grouped = df.groupby('A')\n\nIn [49]: grouped['C'].agg([np.sum, np.mean, np.std])\nOut[49]: \n          sum      mean       std\nA                                \nbar  0.443469  0.147823  0.301765\nfoo  2.529056  0.505811  0.96\n"
"yes_records_sample['name'].isnull()\n"
"s1 = pd.merge(dfA, dfB, how='inner', on=['S', 'T'])\n\ns1.dropna(inplace=True)\n"
"In [20]: df = pd.DataFrame({('A','a'): [-1,-1,0,10,12],\n                   ('A','b'): [0,1,2,3,-1],\n                   ('B','a'): [-20,-10,0,10,20],\n                   ('B','b'): [-200,-100,0,100,200]})\n\nIn [21]: df\nOut[21]: \n    A      B     \n    a  b   a    b\n0  -1  0 -20 -200\n1  -1  1 -10 -100\n2   0  2   0    0\n3  10  3  10  100\n4  12 -1  20  200\n\nIn [22]: idx = pd.IndexSlice\n\nIn [23]: mask = df.loc[:,idx['A',:]]&lt;0\n\nIn [24]: mask\nOut[24]: \n       A       \n       a      b\n0   True  False\n1   True  False\n2  False  False\n3  False  False\n4  False   True\n\nIn [25]: df[mask] = 0\n\nIn [26]: df\nOut[26]: \n    A      B     \n    a  b   a    b\n0   0  0 -20 -200\n1   0  1 -10 -100\n2   0  2   0    0\n3  10  3  10  100\n4  12  0  20  200\n\nIn [30]: df[df[['A']]&lt;0] = 0\n\nIn [31]: df\nOut[31]: \n    A      B     \n    a  b   a    b\n0   0  0 -20 -200\n1   0  1 -10 -100\n2   0  2   0    0\n3  10  3  10  100\n4  12  0  20  200\n"
'with warnings.catch_warnings():\n    warnings.simplefilter("ignore")\n    # Do stuff here\n'
'packages:\n   yum:\n      gcc-c++: []\n      python-devel: []\n'
" import pandas as pd\n\n df = pd.DataFrame({'a': ['a', 'b', 'a', 'a', 'b', 'c', 'd']})\n after = df.groupby('a').size()\n &gt;&gt; after\n a\n a    3\n b    2\n c    1\n d    1\n dtype: int64\n\n &gt;&gt; after[after &gt; 2]\n a\n a    3\n dtype: int64\n"
"import pandas as pd\nimport numpy as np\n\n# data\n# ==============================\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randn(100,5), columns=list('ABCDE'))\ndf[df &lt; 0] = np.nan\ndf\n\n         A       B       C       D       E\n0   1.7641  0.4002  0.9787  2.2409  1.8676\n1      NaN  0.9501     NaN     NaN  0.4106\n2   0.1440  1.4543  0.7610  0.1217  0.4439\n3   0.3337  1.4941     NaN  0.3131     NaN\n4      NaN  0.6536  0.8644     NaN  2.2698\n5      NaN  0.0458     NaN  1.5328  1.4694\n6   0.1549  0.3782     NaN     NaN     NaN\n7   0.1563  1.2303  1.2024     NaN     NaN\n8      NaN     NaN     NaN  1.9508     NaN\n9      NaN     NaN  0.7775     NaN     NaN\n..     ...     ...     ...     ...     ...\n90     NaN  0.8202  0.4631  0.2791  0.3389\n91  2.0210     NaN     NaN  0.1993     NaN\n92     NaN     NaN     NaN  0.1813     NaN\n93  2.4125     NaN     NaN     NaN  0.2515\n94     NaN     NaN     NaN     NaN  1.7389\n95  0.9944  1.3191     NaN  1.1286  0.4960\n96  0.7714  1.0294     NaN     NaN  0.8626\n97     NaN  1.5133  0.5531     NaN  0.2205\n98     NaN     NaN  1.1003  1.2980  2.6962\n99     NaN     NaN     NaN     NaN     NaN\n\n[100 rows x 5 columns]\n\n# calculations\n# ================================\ndf.corr()\n\n        A       B       C       D       E\nA  1.0000  0.2718  0.2678  0.2822  0.1016\nB  0.2718  1.0000 -0.0692  0.1736 -0.1432\nC  0.2678 -0.0692  1.0000 -0.3392  0.0012\nD  0.2822  0.1736 -0.3392  1.0000  0.1562\nE  0.1016 -0.1432  0.0012  0.1562  1.0000\n\n\nnp.corrcoef(df, rowvar=False)\n\narray([[ nan,  nan,  nan,  nan,  nan],\n       [ nan,  nan,  nan,  nan,  nan],\n       [ nan,  nan,  nan,  nan,  nan],\n       [ nan,  nan,  nan,  nan,  nan],\n       [ nan,  nan,  nan,  nan,  nan]])\n"
'import  pandas as pd\n\ndf = pd.DataFrame([sub.split(",") for sub in l])\nprint(df)\n\n   0         1   2               3         4               5         6\n0  AN  2__AS000  26  20150826113000  -283.000  20150826120000  -283.000\n1  AN   2__A000  26  20150826113000     0.000  20150826120000     0.000\n2  AN  2__AE000  26  20150826113000  -269.000  20150826120000  -269.000\n3  AN  2__AE000  26  20150826113000  -255.000  20150826120000  -255.000\n4  AN   2__AE00  26  20150826113000  -254.000  20150826120000  -254.000\n\nimport  pandas as pd\n\ndf = pd.read_csv("in.csv",skiprows=3,header=None)\nprint(df)\n\ndf = pd.read_csv("in.csv",header=None,comment="#")  \n\nimport pandas as pd\nfrom itertools import dropwhile\nimport csv\nwith open("in.csv") as f:\n    f = dropwhile(lambda x: x.startswith("#!!"), f)\n    r = csv.reader(f)\n    df = pd.DataFrame().from_records(r)\n\n#!! various\n#!! metadata\n#!! lines\nAN,2__AS000,26,20150826113000,-283.000,20150826120000,-283.000\nAN,2__A000,26,20150826113000,0.000,20150826120000,0.000\nAN,2__AE000,26,20150826113000,-269.000,20150826120000,-269.000\nAN,2__AE000,26,20150826113000,-255.000,20150826120000,-255.000\nAN,2__AE00,26,20150826113000,-254.000,20150826120000,-254.000\n\n    0         1   2               3         4               5         6\n0  AN  2__AS000  26  20150826113000  -283.000  20150826120000  -283.000\n1  AN   2__A000  26  20150826113000     0.000  20150826120000     0.000\n2  AN  2__AE000  26  20150826113000  -269.000  20150826120000  -269.000\n3  AN  2__AE000  26  20150826113000  -255.000  20150826120000  -255.000\n4  AN   2__AE00  26  20150826113000  -254.000  20150826120000  -254.000\n'
"t1 = pd.to_datetime('2015-11-01 00:00:00')\nt2 = pd.to_datetime('2015-11-02 00:00:00')\nt3 = pd.to_datetime('2015-11-03 00:00:00')\n\nTime = pd.Series([t1, t2, t3])\nr = pd.Series([-1, 1, 0.5])\n\ndf = pd.DataFrame({'Time': Time, 'Value': r})\n\nfig = plt.figure(figsize=(x_size,y_size))\nax = fig.add_subplot(111)\nax.plot_date(x=df.Time, y=df.Value, marker='o')\n"
'df = pd.read_csv(filename, error_bad_lines=False)\n'
"new_df = pd.merge(df1, df2, on=key)\nnew_df.new_col = newdf.apply(lambda row: myfunc(row['A_x'], row['A_y']), axis=1)\n"
'na_free = df.dropna()\nonly_na = df[~df.index.isin(na_free.index)]\n\nonly_na = df[np.invert(df.index.isin(na_free.index))]\n'
"df[~df['Train'].isin(['DeutscheBahn', 'SNCF'])]\n\ndf[(df['Train'] != 'DeutscheBahn') &amp; (df['Train'] != 'SNCF')]\n"
"In [2]:\nimport pandas as pd\nimport datetime as dt\ndf = pd.DataFrame({'date':['2011-04-24 01:30:00.000']})\ndf\n\nOut[2]:\n                      date\n0  2011-04-24 01:30:00.000\n\nIn [3]:\ndf['date'] = pd.to_datetime(df['date'])\ndf\n\nOut[3]:\n                 date\n0 2011-04-24 01:30:00\n\nIn [6]:    \n(df['date'] - dt.datetime(1970,1,1)).dt.total_seconds()\n\nOut[6]:\n0    1303608600\nName: date, dtype: float64\n\nIn [8]:\npd.to_datetime(1303608600, unit='s')\n\nOut[8]:\nTimestamp('2011-04-24 01:30:00')\n\nIn [9]:\ndf['epoch'] = (df['date'] - dt.datetime(1970,1,1)).dt.total_seconds()\ndf\n\nOut[9]:\n                 date       epoch\n0 2011-04-24 01:30:00  1303608600\n\nIn [3]:\ndf['date'].astype('int64')//1e9\n\nOut[3]:\n0    1303608600\nName: date, dtype: float64\n\nIn [4]:\n%timeit (df['date'] - dt.datetime(1970,1,1)).dt.total_seconds()\n%timeit df['date'].astype('int64')//1e9\n\n100 loops, best of 3: 1.72 ms per loop\n1000 loops, best of 3: 275 µs per loop\n"
'import matplotlib.pyplot as plt\nimport pandas\nimport seaborn.apionly as sns\nfrom matplotlib.colors import LinearSegmentedColormap\n\nsns.set(font_scale=0.8)\ndataFrame = pandas.read_csv(\'LUH2_trans_matrix.csv\').set_index([\'Unnamed: 0\'])\n\n# For only three colors, it\'s easier to choose them yourself.\n# If you still really want to generate a colormap with cubehelix_palette instead,\n# add a cbar_kws={"boundaries": linspace(-1, 1, 4)} to the heatmap invocation\n# to have it generate a discrete colorbar instead of a continous one.\nmyColors = ((0.8, 0.0, 0.0, 1.0), (0.0, 0.8, 0.0, 1.0), (0.0, 0.0, 0.8, 1.0))\ncmap = LinearSegmentedColormap.from_list(\'Custom\', myColors, len(myColors))\n\nax = sns.heatmap(dataFrame, cmap=cmap, linewidths=.5, linecolor=\'lightgray\')\n\n# Manually specify colorbar labelling after it\'s been generated\ncolorbar = ax.collections[0].colorbar\ncolorbar.set_ticks([-0.667, 0, 0.667])\ncolorbar.set_ticklabels([\'B\', \'A\', \'C\'])\n\n# X - Y axis labels\nax.set_ylabel(\'FROM\')\nax.set_xlabel(\'TO\')\n\n# Only y-axis labels need their rotation set, x-axis labels already have a rotation of 0\n_, labels = plt.yticks()\nplt.setp(labels, rotation=0)\n\nplt.show()\n'
'df.isnull().all(1)\n\ndf.index[df.isnull().all(1)]\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.choice((1, np.nan), (10, 2)))\ndf\n\nidx = df.index[df.isnull().all(1)]\nnans = df.ix[idx]\nnans\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.choice((1, np.nan), (10000, 5)))\n'
'df_new = df[list]\n\ndf_new = df[L]\n\nL = []\nfor x in df.columns: \n    if not "_" in x[-3:]: \n        L.append(x) \nprint (L)\n\nprint ([x for x in df.columns if not "_" in x[-3:]])\n'
'spark.createDataFrame(...)\n\nspark.sparkContext\n\nSQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n'
"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport matplotlib.dates as mdates\n\ndf = pd.DataFrame(np.random.rand(100,2), index=pd.date_range('1-1-2018', periods=100))\nax = df.plot(x_compat=True)\nax.xaxis.set_major_locator(mdates.MonthLocator())\nplt.show()\n"
'pd.to_datetime(dte.stack()).unstack()\n'
'In [6]: df.groupby(\'B\').agg({\'A\':\'sum\', \'C\':\'first\'})\nOut[6]:\n       C  A\nB\nPA  West  5\nPO   Est  1\n\nIn [8]: df = pd.DataFrame({"A": range(4), "B": ["PO", "PO", "PA", "PA"], "C": ["Est1", "Est2", "West1", "West2"]})\n\nIn [9]: df.groupby(\'B\').agg({\'A\':\'sum\', \'C\':\'first\'})\nOut[9]:\n        C  A\nB\nPA  West1  5\nPO   Est1  1\n\nIn [10]: df[\'sum_A\'] = df.groupby(\'B\')[\'A\'].transform(\'sum\')\n\nIn [11]: df\nOut[11]:\n   A   B      C  sum_A\n0  0  PO   Est1      1\n1  1  PO   Est2      1\n2  2  PA  West1      5\n3  3  PA  West2      5\n'
'In [95]:\nimport io\nimport pandas as pd\nt="""a,b,c\n0,1,2\n3,4,5"""\npd.read_csv(io.StringIO(t), header=0)\n\nOut[95]:\n   a  b  c\n0  0  1  2\n1  3  4  5\n\nIn [96]:\npd.read_csv(io.StringIO(t), header=None)\n\nOut[96]:\n   0  1  2\n0  a  b  c\n1  0  1  2\n2  3  4  5\n\nIn [98]:\npd.read_csv(io.StringIO(t), header=False)\n'
'np.nan == np.nan\n\nFalse\n\nnp.isnan(np.nan)\n\nTrue\n\npd.isnull(np.nan)\n\nTrue\n\ns = pd.Series([1., np.nan, 2.])\ns[s != np.nan]\n\n0    1.0\n1    NaN\n2    2.0\ndtype: float64\n\ns = pd.Series([1., np.nan, 2.])\ns[s.notnull()]\n\n0    1.0\n2    2.0\ndtype: float64\n\ns = pd.Series([1., np.nan, 2.])\ns[s == s]\n\n0    1.0\n2    2.0\ndtype: float64\n\ns = pd.Series([1., np.nan, 2.])\ns.dropna()\n\n0    1.0\n2    2.0\ndtype: float64\n'
'import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({"A":[2,3,1], "B":[1,2,2]})\ndf[[\'A\', \'B\']].plot(figsize=(10,4))\n\nplt.show()\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({"A":[2,3,1], "B":[1,2,2]})\nplt.figure(figsize=(10,4))\ndf[[\'A\', \'B\']].plot(ax = plt.gca())\n\nplt.show()\n'
"print (pd.to_numeric(df['num'], errors='coerce'))\n0   -1.48\n1    1.70\n2   -6.18\n3    0.25\n4     NaN\n5    0.25\nName: num, dtype: float64\n\nprint (pd.to_numeric(df['num'], errors='coerce').isnull())\n0    False\n1    False\n2    False\n3    False\n4     True\n5    False\nName: num, dtype: bool\n\nprint (df[pd.to_numeric(df['num'], errors='coerce').isnull()])\n  N-D     num unit\n4  Q5  sum(d)   UD\n\nprint (df[df['num'].apply(lambda x: isinstance(x, str))])\n  N-D     num unit\n4  Q5  sum(d)   UD\n"
'df = pd.DataFrame(orderedDictList, columns=orderedDictList.keys())\n'
"import pandas as pd\n\nd = pd.Series({'first_name': 'Andrii', 'last_name':'Furmanets', 'created_at':None})\nd = d.fillna('DATE')\n"
"from io import StringIO\n\ns=str(bytes_data,'utf-8')\n\ndata = StringIO(s) \n\ndf=pd.read_csv(data)\n"
"df.groupby('colA').count() \n\ndf.groupby('colA')['colA'].count() \n\ndf = pd.DataFrame({'colB':list('abcdefg'),\n                   'colC':[1,3,5,7,np.nan,np.nan,4],\n                   'colD':[np.nan,3,6,9,2,4,np.nan],\n                   'colA':['c','c','b','a',np.nan,'b','b']})\n\nprint (df)\n  colA colB  colC  colD\n0    c    a   1.0   NaN\n1    c    b   3.0   3.0\n2    b    c   5.0   6.0\n3    a    d   7.0   9.0\n4  NaN    e   NaN   2.0\n5    b    f   NaN   4.0\n6    b    g   4.0   NaN\n\nprint (df['colA'].value_counts())\nb    3\nc    2\na    1\nName: colA, dtype: int64\n\nprint (df.groupby('colA').count())\n      colB  colC  colD\ncolA                  \na        1     1     1\nb        3     2     2\nc        2     2     1\n\nprint (df.groupby('colA')['colA'].count())\ncolA\na    1\nb    3\nc    2\nName: colA, dtype: int64\n"
"from shapely.geometry import Point, Polygon\nimport geopandas\n\npolys = geopandas.GeoSeries({\n    'foo': Polygon([(5, 5), (5, 13), (13, 13), (13, 5)]),\n    'bar': Polygon([(10, 10), (10, 15), (15, 15), (15, 10)]),\n})\n\n_pnts = [Point(3, 3), Point(8, 8), Point(11, 11)]\npnts = geopandas.GeoDataFrame(geometry=_pnts, index=['A', 'B', 'C'])\npnts = pnts.assign(**{key: pnts.within(geom) for key, geom in polys.items()})\n\nprint(pnts)\n\n        geometry    bar    foo\nA    POINT (3 3)  False  False\nB    POINT (8 8)  False   True\nC  POINT (11 11)   True   True\n"
'df.round(0).astype(int)\n'
"df2 = df[['c', 'd']].copy()\ndf = df.drop(['c', 'd'], axis=1)\n\ndf2 = pd.concat([df.pop(x) for x in ['c', 'd']], axis=1)\n\ndf\n\n   a  b\n0  0  0\n1  1  1\n\ndf2\n\n   c  d\n0  0  0\n1  1  1\n"
"def mapper(d):\n\n    def contraster(x, DF=d):\n        matches = DF.apply(lambda row: fuzz.partial_ratio(row['last_name'], x) &gt;= 50, axis = 1)\n        return [d.ID.iloc[i] for i, x in enumerate(matches) if x]\n    d['out'] = d.apply(lambda row: \n        contraster(row['last_name']), axis =1)\n    return d\n\ndf.groupby('first_name').apply(mapper).compute()\n\n   ID first_name  last_name   out\n2   X      Danae      Smith   [X]\n4  12      Jacke       Toro  [12]\n0   X       Jake   Del Toro   [X]\n1   U       John     Foster   [U]\n5  13        Jon    Froster  [13]\n3   Y    Beatriz  Patterson   [Y]\n\nentities = pd.DataFrame(\n    {'first_name':['Jake','Jake', 'Jake', 'John'],\n     'last_name': ['Del Toro', 'Toro', 'Smith'\n                   'Froster'],\n     'ID':['Z','U','X','Y']})\n\n  ID first_name last_name     out\n0  Z       Jake  Del Toro  [Z, U]\n1  U       Jake      Toro  [Z, U]\n2  X       Jake     Smith     [X]\n3  Y       John   Froster     [Y]\n"
'import json\nimport pandas as pd\ndata = json.load(open(\'json_file.json\'))\n\ndf = pd.DataFrame(data["result"])\n'
"df1 = pd.DataFrame(dict(X=range(4)), index=['a','b','c','d'])\ndf1 = pd.DataFrame(dict(X=range(4)), index=['b','c','z'])\n\n   X\nY\na  0\nb  1\nc  2\nd  3\n\n   X\nY\nb  0\nc  1\nz  2\n\ndf.loc[start:end].iloc[:-1]\n"
"df.set_index('ID',inplace=True)\n\ndf[df==1].stack().reset_index().drop(0,1)\nOut[363]: \n     ID level_1\n0  1002       2\n1  1002       4\n2  1004       1\n3  1004       2\n4  1005       5\n5  1006       6\n6  1007       1\n7  1007       3\n8  1009       3\n9  1009       7\n"
"(df.loc[:, 'Python':]\n .apply(lambda x: '|'.join(x.dropna()), axis=1)\n .str.get_dummies('|')\n .groupby(df['Gender']).sum())\n\n                   Bash  C++  JavaScript  Python  R\nGender                                             \nFemale                0    1           1       0  1\nMale                  0    0           1       1  0\nPrefer not to say     1    0           0       1  0\n"
'df.to_pickle(file_name)\n'
"df = pd.DataFrame(prcpSeries, columns=['prcp'])\ndf['tmax'] = tmaxSeries\n...\n\ndf = prcpSeries.to_frame(name='prcp')\n\ndf1 = pd.DataFrame(prcpSeries, columns=['prcp'])\ndf2 = pd.DataFrame(tmaxSeries, columns=['tmax'])\n...\n\ndf = pd.concat([df1, df2, ...], join='outer', axis=1)\n\nIn [21]: dfA = pd.DataFrame([1,2], columns=['A'])\n\nIn [22]: dfB = pd.DataFrame([1], columns=['B'])\n\nIn [23]: pd.concat([dfA, dfB], join='outer', axis=1)\nOut[23]:\n   A   B\n0  1   1\n1  2 NaN\n"
'In [11]: df.index.get_loc(ds)\nOut[11]: 1\n'
"import pandas\n\nsample={'user1': {'item1': 2.5, 'item2': 3.5, 'item3': 3.0, 'item4': 3.5, 'item5': 2.5, 'item6': 3.0},\n        'user2': {'item1': 2.5, 'item2': 3.0, 'item3': 3.5, 'item4': 4.0},\n        'user3': {'item2':4.5,'item5':1.0,'item6':4.0}}\n\ndf = pandas.DataFrame([\n    [col1,col2,col3] for col1, d in sample.items() for col2, col3 in d.items()\n])\n"
'DataFrame(...).ix[row_indexer,column_indexer]\nSeries(...).ix[row_indexer]\n'
'df[df.apply(lambda x: min(x) == max(x), 1)]\n'
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; index = np.arange(0, 10)\n&gt;&gt;&gt; df = pd.DataFrame(index=index, columns=['foo', 'bar'])\n&gt;&gt;&gt; df\nOut[268]: \n   foo  bar\n0  NaN  NaN\n1  NaN  NaN\n2  NaN  NaN\n3  NaN  NaN\n4  NaN  NaN\n5  NaN  NaN\n6  NaN  NaN\n7  NaN  NaN\n8  NaN  NaN\n9  NaN  NaN\n"
'In [26]: df.ix[(dt.datetime(2013,2,3,9,0,2),0),:] = 5\n\nIn [27]: df\nOut[27]: \n                          vals\nTime                hsec      \n2013-02-03 09:00:01 1       45\n                    25      46\n2013-02-03 09:00:02 0        5\n'
"drugInfo.rename(columns = {list(drugInfo)[1]: 'col_1_new_name'}, inplace = True)\n\nIn [18]:\n\ndf = pd.DataFrame({'a':randn(5), 'b':randn(5), 'c':randn(5)})\ndf\nOut[18]:\n          a         b         c\n0 -1.429509 -0.652116  0.515545\n1  0.563148 -0.536554 -1.316155\n2  1.310768 -3.041681 -0.704776\n3 -1.403204  1.083727 -0.117787\n4 -0.040952  0.108155 -0.092292\nIn [19]:\n\ndf.rename(columns={list(df)[1]:'col1_new_name'}, inplace=True)\ndf\nOut[19]:\n          a  col1_new_name         c\n0 -1.429509      -0.652116  0.515545\n1  0.563148      -0.536554 -1.316155\n2  1.310768      -3.041681 -0.704776\n3 -1.403204       1.083727 -0.117787\n4 -0.040952       0.108155 -0.092292\n\ndf.rename(columns={df.columns[1]:'col1_new_name'}, inplace=True)\n\ndrugInfo.rename(columns = {drugInfo.columns[1]: 'col_1_new_name'}, inplace = True)\n"
"pd.options.display.max_columns = None\ndisplay(data)\n\npd.set_option('display.max_columns', None)\ndisplay(data)\n\nwith pd.option_context('display.max_columns', None):\n    display(data)\n"
'df.index.get_level_values(&lt;levelname&gt;)\n'
"df['time_diff'] = df.groupby('User')['Time'].diff()\n\ndf['Col1_0'] = df['Col1'].apply( lambda x: x[0] )\n\ndf['Col1_0_prev'] = df.groupby('User')['Col1_0'].shift()\n\n   User  Time                 Col1  time_diff Col1_0 Col1_0_prev\n0     1     6     [cat, dog, goat]        NaN    cat         NaN\n1     1     6         [cat, sheep]          0    cat         cat\n2     1    12        [sheep, goat]          6  sheep         cat\n3     2     3          [cat, lion]        NaN    cat         NaN\n4     2     5  [fish, goat, lemur]          2   fish         cat\n5     3     9           [cat, dog]        NaN    cat         NaN\n6     4     4          [dog, goat]        NaN    dog         NaN\n7     4    11                [cat]          7    cat         dog\n"
"In [7]:\n\ndf.index = natsorted(a)\ndf.index\nOut[7]:\nIndex(['0hr', '48hr', '72hr', '96hr', '128hr'], dtype='object')\n\nIn [10]:\n\nnatsorted(df)\nOut[10]:\n[]\n\nIn [13]:\n\ndf=pd.DataFrame(index=a, data=np.arange(5))\ndf\nOut[13]:\n       0\n0hr    0\n128hr  1\n72hr   2\n48hr   3\n96hr   4\nIn [14]:\n\ndf = df*2\ndf\nOut[14]:\n       0\n0hr    0\n128hr  2\n72hr   4\n48hr   6\n96hr   8\nIn [15]:\n\ndf.reindex(index=natsorted(df.index))\nOut[15]:\n       0\n0hr    0\n48hr   6\n72hr   4\n96hr   8\n128hr  2\n"
'from collections import Counter\nCounter(" ".join(df["text"]).split()).most_common(100)\n'
'def numpy_foo(arr):\n    vals = {i: (arr[i, :] + arr[i:, :]).sum(axis=1).tolist()\n            for i in range(arr.shape[0])}   \n    return vals\n\n%timeit foo(df)\n100 loops, best of 3: 7.2 ms per loop\n\n%timeit numpy_foo(df.values)\n10000 loops, best of 3: 144 µs per loop\n\nfoo(df) == numpy_foo(df.values)\nOut[586]: True\n'
"1*|*2*|*3*|*4*|*5\n12*|*12*|*13*|*14*|*15\n21*|*22*|*23*|*24*|*25\n\npd.read_table('file.csv', header=None, sep='\\*\\|\\*')\n"
'import pandas as pd\nimport datetime\n\ndef parse_date(td):\n    resYear = float(td.days)/364.0                   # get the number of years including the the numbers after the dot\n    resMonth = int((resYear - int(resYear))*364/30)  # get the number of months, by multiply the number after the dot by 364 and divide by 30.\n    resYear = int(resYear)\n    return str(resYear) + "Y" + str(resMonth) + "m"\n\ndf = pd.DataFrame([("2000-01-10", "1970-04-29")], columns=["start", "end"])\ndf["delta"] = [parse_date(datetime.datetime.strptime(start, \'%Y-%m-%d\') - datetime.datetime.strptime(end, \'%Y-%m-%d\')) for start, end in zip(df["start"], df["end"])]\nprint df\n\n        start         end  delta\n0  2000-01-10  1970-04-29  29Y9m\n'
"price = purchase_group['Column_name'].values[0]\n"
'import numpy as np\nimport seaborn as sns\nimport pandas as pd\nfrom scipy import stats\nimport scipy.cluster.hierarchy as hac\nimport matplotlib.pyplot as plt\n\n#\n# build 6 time series groups for testing, called: a, b, c, d, e, f\n#\n\nnum_samples = 61\ngroup_size = 10\n\n#\n# create the main time series for each group\n#\n\nx = np.linspace(0, 5, num_samples)\nscale = 4\n\na = scale * np.sin(x)\nb = scale * (np.cos(1+x*3) + np.linspace(0, 1, num_samples))\nc = scale * (np.sin(2+x*6) + np.linspace(0, -1, num_samples))\nd = scale * (np.cos(3+x*9) + np.linspace(0, 4, num_samples))\ne = scale * (np.sin(4+x*12) + np.linspace(0, -4, num_samples))\nf = scale * np.cos(x)\n\n#\n# from each main series build \'group_size\' series\n#\n\ntimeSeries = pd.DataFrame()\nax = None\nfor arr in [a,b,c,d,e,f]:\n    arr = arr + np.random.rand(group_size, num_samples) + np.random.randn(group_size, 1)\n    df = pd.DataFrame(arr)\n    timeSeries = timeSeries.append(df)\n\n    # We use seaborn to plot what we have\n    #ax = sns.tsplot(ax=ax, data=df.values, ci=[68, 95])\n    ax = sns.tsplot(ax=ax, data=df.values, err_style="unit_traces")\n\nplt.show()\n\n# Do the clustering\nZ = hac.linkage(timeSeries, method=\'single\', metric=\'correlation\')\n\n# Plot dendogram\nplt.figure(figsize=(25, 10))\nplt.title(\'Hierarchical Clustering Dendrogram\')\nplt.xlabel(\'sample index\')\nplt.ylabel(\'distance\')\nhac.dendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()\n\n# Here we use spearman correlation\ndef my_metric(x, y):\n    r = stats.pearsonr(x, y)[0]\n    return 1 - r # correlation to distance: range 0 to 2\n\n# Do the clustering    \nZ = hac.linkage(timeSeries,  method=\'single\', metric=my_metric)\n\n# Plot dendogram\nplt.figure(figsize=(25, 10))\nplt.title(\'Hierarchical Clustering Dendrogram\')\nplt.xlabel(\'sample index\')\nplt.ylabel(\'distance\')\nhac.dendrogram(\n    Z,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()\n\nfrom scipy.cluster.hierarchy import fcluster\n\ndef print_clusters(timeSeries, Z, k, plot=False):\n    # k Number of clusters I\'d like to extract\n    results = fcluster(Z, k, criterion=\'maxclust\')\n\n    # check the results\n    s = pd.Series(results)\n    clusters = s.unique()\n\n    for c in clusters:\n        cluster_indeces = s[s==c].index\n        print("Cluster %d number of entries %d" % (c, len(cluster_indeces)))\n        if plot:\n            timeSeries.T.iloc[:,cluster_indeces].plot()\n            plt.show()\n\nprint_clusters(timeSeries, Z, 6, plot=False)\n\nCluster 2 number of entries 10\nCluster 5 number of entries 10\nCluster 3 number of entries 10\nCluster 6 number of entries 10\nCluster 1 number of entries 10\nCluster 4 number of entries 10\n'
'print df\n    f1 f2 f3\nidx         \n1    a  a  b\n1    b  a  c\n1    a  b  c\n87   e  e  e\n\nprint df.groupby(level=0).apply(lambda x: x.to_json(orient=\'records\'))\n\n1     [{"f1":"a","f2":"a","f3":"b"},{"f1":"b","f2":"...\n87                       [{"f1":"e","f2":"e","f3":"e"}]\ndtype: object\n\nprint df\n   idx f1 f2 f3\n0    1  a  a  b\n1    1  b  a  c\n2    1  a  b  c\n3   87  e  e  e\n\nprint df.groupby(\'idx\').apply(lambda x: x.to_json(orient=\'records\'))\nidx\n1     [{"idx":1,"f1":"a","f2":"a","f3":"b"},{"idx":1...\n87              [{"idx":87,"f1":"e","f2":"e","f3":"e"}]\ndtype: object\n'
"    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; Meta = pd.read_csv('C:\\\\Users\\\\*****\\\\Downloads\\\\so\\\\Book1.csv')\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; from sklearn.model_selection import train_test_split\n    &gt;&gt;&gt; y = Meta.pop('Categories')\n    &gt;&gt;&gt; Meta\n        ReviewerID      ReviewText  ProductId\n        0        1212    good product   14444425\n        1        1233  will buy again     324532\n        2        5432  not recomended  789654123\n    &gt;&gt;&gt; y\n        0    Mobile\n        1     drugs\n        2       dvd\n        Name: Categories, dtype: object\n    &gt;&gt;&gt; X = Meta\n    &gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42, stratify=y)\n    &gt;&gt;&gt; X_test\n        ReviewerID    ReviewText  ProductId\n        0        1212  good product   14444425\n"
"&gt;&gt;&gt; df.groupby('filename').apply(lambda group: group.interpolate(method='index'))\n    filename  val1  val2\nt                       \n1  file1.csv     5    10\n2  file1.csv    10    15\n3  file1.csv    15    20\n6  file2.csv   NaN   NaN\n7  file2.csv    10    20\n8  file2.csv    12    15\n"
'pd.read_csv(r"C:\\test.csv",usecols=[2])\n'
"df = pd.DataFrame(dict(\n        AUTHOR_NAME=list('AAABBCCCCDEEFGG'),\n        title=      list('zyxwvutsrqponml')\n    ))\n\ndf2 = pd.DataFrame(dict(\n        AUTHOR_NAME=list('AABCCEGG'),\n        title      =list('zwvtrpml'),\n        CATEGORY   =list('11223344')\n    ))\n\ndf.merge(df2, how='left')\n\ncols = ['AUTHOR_NAME', 'title']\ndf.join(df2.set_index(cols), on=cols)\n"
"In [2]: import pandas as pd\n\nIn [3]: from simpledbf import Dbf5\n\nIn [4]: dbf = Dbf5('test.dbf')\n\nIn [5]: df = dbf.to_dataframe()\n"
'   fig = plt.figure(figsize = (15,20))\n   ax = fig.gca()\n   df.hist(ax = ax)\n'
'import seaborn as sns\nimport matplotlib.pylab as plt\nimport pandas\nimport numpy as np\n\ndf = pandas.DataFrame({"a": np.arange(1001, 1001 + 30),\n                       "l": ["A"] * 15 + ["B"] * 15,\n                       "v": np.random.rand(30)})\ng = sns.FacetGrid(row="l", data=df)\ng.map(plt.plot, "a", "v", marker="o")\ng.set(xticks=df.a[2::8])\n'
"class MyPoint:\n    def __init__(self, x, y):\n        self._x = x\n        self._y = y\n\n    @property\n    def x(self):\n        return self._x\n\n    @property\n    def y(self):\n        return self._y\n\nmy_list = [MyPoint(1, 1), MyPoint(2, 2)]\nprint(my_list)\n\nplane_pd = pd.DataFrame([[p.x, p.y, p] for p in my_list],\n                        columns=list('XYO'))\nprint(plane_pd.dtypes)\nprint(plane_pd)\n\n[&lt;__main__.MyPoint object at 0x033D2AF0&gt;, &lt;__main__.MyPoint object at 0x033D2B10&gt;]\n\nX     int64\nY     int64\nO    object\ndtype: object\n\n   X  Y                                        O\n0  1  1  &lt;__main__.MyPoint object at 0x033D2AF0&gt;\n1  2  2  &lt;__main__.MyPoint object at 0x033D2B10&gt;\n"
'df[\'d\'] = np.where(df.a.isnull(),\n         np.nan,\n         np.where((df.b == "N")&amp;(~df.c.isnull()),\n                  df.a*df.c,\n                  df.a))\n\n      a  b    c     d\n0   NaN  Y  NaN   NaN\n1  23.0  N  3.0  69.0\n2   NaN  N  2.0   NaN\n3  44.0  Y  NaN  44.0\n'
"%timeit -n 5000\ndf = pd.DataFrame({'High':[1,4,8,4,0]})\n\n5000 loops, best of 3: 592 µs per loop\n"
"df.groupby('user')[['time', 'amount']].apply(lambda x: x.values.tolist())\n"
'df = df[df.index.isin(df1.index)]\n\ndf = df.loc[df.index &amp; df1.index]\ndf = df.loc[np.intersect1d(df.index, df1.index)]\ndf = df.loc[df.index.intersection(df1.index)]\n\nprint (df)\n    A  B  C  D\n1   1  4  9  1\n3   5  5  1  0\n22  1  3  9  6\n\ndf = df.loc[df1.index]\nprint (df)\n\n      A    B    C    D\n1   1.0  4.0  9.0  1.0\n3   5.0  5.0  1.0  0.0\n4   NaN  NaN  NaN  NaN\n5   NaN  NaN  NaN  NaN\n6   NaN  NaN  NaN  NaN\n7   NaN  NaN  NaN  NaN\n22  1.0  3.0  9.0  6.0\n28  NaN  NaN  NaN  NaN\n29  NaN  NaN  NaN  NaN\n32  NaN  NaN  NaN  NaN\nC:/Dropbox/work-joy/so/_t/t.py:23: FutureWarning: \nPassing list-likes to .loc or [] with any missing label will raise\nKeyError in the future, you can use .reindex() as an alternative.\n\nSee the documentation here:\nhttp://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n  print (df)\n'
"d = {'Missed':'Sum1', 'Credit':'Sum2','Grade':'Average'}\ndf=df.groupby('Name').agg({'Missed':'sum', 'Credit':'sum','Grade':'mean'}).rename(columns=d)\nprint (df)\n      Sum1  Sum2  Average\nName                     \nA        2     4       11\nB        3     5       15\n\ndf = (df.groupby('Name', as_index=False)\n       .agg({'Missed':'sum', 'Credit':'sum','Grade':'mean'})\n       .rename(columns={'Missed':'Sum1', 'Credit':'Sum2','Grade':'Average'}))\nprint (df)\n  Name  Sum1  Sum2  Average\n0    A     2     4       11\n1    B     3     5       15\n\ndf = df.groupby('Name', as_index=False).agg(Sum1=('Missed','sum'), \n                                            Sum2= ('Credit','sum'),\n                                            Average=('Grade','mean'))\nprint (df)\n  Name  Sum1  Sum2  Average\n0    A     2     4       11\n1    B     3     5       15\n"
"df1 = pd.DataFrame(gdf)\n\ndf1 = pd.DataFrame(gdf.drop(columns='geometry'))\n# for older versions of pandas (&lt; 0.21), the drop part: gdf.drop('geometry', axis=1)\n"
"AM_train['product_category_2'] = AM_train['product_category_2'].cat.add_categories('Unknown')\nAM_train['product_category_2'].fillna('Unknown', inplace =True) \n\nAM_train['city_development_index'] = AM_train['city_development_index'].cat.add_categories('Missing')\nAM_train['city_development_index'].fillna('Missing', inplace =True)\n\nAM_train = pd.DataFrame({'product_category_2': pd.Categorical(['a','b',np.nan])})\nAM_train['product_category_2'] = AM_train['product_category_2'].cat.add_categories('Unknown')\nAM_train['product_category_2'].fillna('Unknown', inplace =True) \n\nprint (AM_train)\n  product_category_2\n0                  a\n1                  b\n2            Unknown\n"
'def ismixed(a):\n    try:\n        max(a)\n        return False\n    except TypeError as e: # we take this to imply mixed type\n        msg, fst, and_, snd = str(e).rsplit(\' \', 3)\n        assert msg=="\'&gt;\' not supported between instances of"\n        assert and_=="and"\n        assert fst!=snd\n        return True\n    except ValueError as e: # catch empty arrays\n        assert str(e)=="max() arg is an empty sequence"\n        return False\n\nv = df.values\n\nlist(map(is_mixed, v.T))\n# [True, False, False]\ntimeit(lambda: list(map(ismixed, v.T)), number=1000)\n# 0.008936170022934675\n\ntimeit(lambda: list(map(infer_dtype, v.T)), number=1000)\n# 0.02499613002873957\n'
"pd.DataFrame([a, b])\nOut[610]: \n      0     1     2     3     4      5      6      7      8      9\n0     a     a     a     a     b      a      b      b      b      b\n1  True  True  True  True  True  False  False  False  False  False\n\nc.dtypes\nOut[608]: \nClassification    object\nBoolean           object\n\nc=pd.concat([a,b],1)\nc.columns = ['Classification', 'Boolean']\n~c.Boolean\nOut[616]: \n0    False\n1    False\n2    False\n3    False\n4    False\n5     True\n6     True\n7     True\n8     True\n9     True\nName: Boolean, dtype: bool\n"
'n_nodes = clf.tree_.node_count\nchildren_left = clf.tree_.children_left\nchildren_right = clf.tree_.children_right\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\n\ndef find_path(node_numb, path, x):\n        path.append(node_numb)\n        if node_numb == x:\n            return True\n        left = False\n        right = False\n        if (children_left[node_numb] !=-1):\n            left = find_path(children_left[node_numb], path, x)\n        if (children_right[node_numb] !=-1):\n            right = find_path(children_right[node_numb], path, x)\n        if left or right :\n            return True\n        path.remove(node_numb)\n        return False\n\n\ndef get_rule(path, column_names):\n    mask = \'\'\n    for index, node in enumerate(path):\n        #We check if we are not in the leaf\n        if index!=len(path)-1:\n            # Do we go under or over the threshold ?\n            if (children_left[node] == path[index+1]):\n                mask += "(df[\'{}\']&lt;= {}) \\t ".format(column_names[feature[node]], threshold[node])\n            else:\n                mask += "(df[\'{}\']&gt; {}) \\t ".format(column_names[feature[node]], threshold[node])\n    # We insert the &amp; at the right places\n    mask = mask.replace("\\t", "&amp;", mask.count("\\t") - 1)\n    mask = mask.replace("\\t", "")\n    return mask\n\n# Leaves\nleave_id = clf.apply(X_test)\n\npaths ={}\nfor leaf in np.unique(leave_id):\n    path_leaf = []\n    find_path(0, path_leaf, leaf)\n    paths[leaf] = np.unique(np.sort(path_leaf))\n\nrules = {}\nfor key in paths:\n    rules[key] = get_rule(paths[key], pima.columns)\n\nrules =\n{3: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&lt;= 26.450000762939453) &amp; (df[\'bp\']&lt;= 9.100000381469727)  ",\n 4: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&lt;= 26.450000762939453) &amp; (df[\'bp\']&gt; 9.100000381469727)  ",\n 6: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&gt; 26.450000762939453) &amp; (df[\'skin\']&lt;= 27.5)  ",\n 7: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&gt; 26.450000762939453) &amp; (df[\'skin\']&gt; 27.5)  ",\n 10: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&lt;= 28.149999618530273) &amp; (df[\'insulin\']&lt;= 145.5)  ",\n 11: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&lt;= 28.149999618530273) &amp; (df[\'insulin\']&gt; 145.5)  ",\n 13: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&gt; 28.149999618530273) &amp; (df[\'insulin\']&lt;= 158.5)  ",\n 14: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&gt; 28.149999618530273) &amp; (df[\'insulin\']&gt; 158.5)  "}\n'
"# Sample data\nx = 100*np.random.random(15)\ny = 100*np.random.random(15)\n\nbins = np.linspace(0, 100, 5+1)\n\n# bins = array([  0.,  20.,  40.,  60.,  80., 100.])\n\nbinned, binx, biny = np.histogram2d(x, y, bins = [bins, bins])\n\n# To get the result you desire, transpose\nobjmat = binned.T\n\nfig, ax = plt.subplots()\nax.grid()\nax.set_xlim(0, 100)\nax.set_ylim(0, 100)\n\nax.scatter(x, y)\nfor i in range(objmat.shape[0]):\n    for j in range(objmat.shape[1]):\n        c = int(objmat[::-1][j,i])\n        ax.text((bins[i]+bins[i+1])/2, (bins[j]+bins[j+1])/2, str(c), fontdict={'fontsize' : 16, 'ha' : 'center', 'va' : 'center'})\n"
"import matplotlib.pyplot as plt\nplt.plot(range(10))\nplt.show()\n\nimport matplotlib            \nprint matplotlib.rcParams['backend']\n"
'In [4]: a.values.reshape(2,2)\nOut[4]: \narray([[1, 2],\n       [3, 4]], dtype=int64)\n'
"if isinstance(other, (Series, DataFrame)):\n    common = self.columns.union(other.index)\n    if (len(common) &gt; len(self.columns) or\n        len(common) &gt; len(other.index)):\n        raise ValueError('matrices are not aligned')\n\nnp.dot(x, y)\n\nleft = self.reindex(columns=common, copy=False)\nright = other.reindex(index=common, copy=False)\n\nimport pandas as pd\nimport numpy as np\n\ncolumns = ['col{}'.format(i) for i in range(36)]\nx = pd.DataFrame(np.random.random((1062, 36)), columns=columns)\ny = pd.DataFrame(np.random.random((36, 36)))\n\nprint(np.dot(x, y).shape)\n# (1062, 36)\n\nprint(x.dot(y).shape)\n# ValueError: matrices are not aligned\n"
"df.ix[df.type==7, ['var1001', 'var1002']] = 0\n\ncolumnsToReplace = ['var1001', 'var1002', ...]\ndf.ix[df.type==8, columnsToReplace] = 0\n"
"In [11]: g\nOut[11]:\n                                               Sales\nManufacturer Product Name Product Launch Date\nApple        iPad         2010-04-03              30\n             iPod         2001-10-23              34\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n\nIn [12]: g.index = g.index.swaplevel(1, 2)\n\nIn [13]: g = g.sortlevel()\n\nIn [14]: g.index = g.index.swaplevel(1, 2)\n\nIn [15]: g\nOut[15]:\n                                               Sales\nManufacturer Product Name Product Launch Date\nApple        iPod         2001-10-23              34\n             iPad         2010-04-03              30\nSamsung      Galaxy       2009-04-27              24\n             Galaxy Tab   2010-09-02              22\n\ng = df.groupby(['Manufacturer', 'Product Launch Date', 'Product Name']).sum()\n"
'import numpy as np\n\nv = np.array([1., 1., 1., np.nan, 1., 1., 1., 1., np.nan, 1.])\nn = np.isnan(v)\na = ~n\nc = np.cumsum(a)\nd = np.diff(np.concatenate(([0.], c[n])))\nv[n] = -d\nnp.cumsum(v)\n'
"In [239]: from operator import methodcaller\n\nIn [240]: s = Series(date_range(Timestamp('now'), periods=2))\n\nIn [241]: s\nOut[241]:\n0   2013-10-01 00:24:16\n1   2013-10-02 00:24:16\ndtype: datetime64[ns]\n\nIn [238]: s.map(lambda x: x.strftime('%d-%m-%Y'))\nOut[238]:\n0    01-10-2013\n1    02-10-2013\ndtype: object\n\nIn [242]: s.map(methodcaller('strftime', '%d-%m-%Y'))\nOut[242]:\n0    01-10-2013\n1    02-10-2013\ndtype: object\n\nIn [249]: s.map(methodcaller('date'))\n\nOut[249]:\n0    2013-10-01\n1    2013-10-02\ndtype: object\n\nIn [250]: s.map(methodcaller('date')).values\n\nOut[250]:\narray([datetime.date(2013, 10, 1), datetime.date(2013, 10, 2)], dtype=object)\n\nIn [273]: s.map(Timestamp.date)\nOut[273]:\n0    2013-10-01\n1    2013-10-02\ndtype: object\n\nIn [243]: index = DatetimeIndex(s)\n\nIn [244]: index\nOut[244]:\n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2013-10-01 00:24:16, 2013-10-02 00:24:16]\nLength: 2, Freq: None, Timezone: None\n\nIn [246]: index.date\nOut[246]:\narray([datetime.date(2013, 10, 1), datetime.date(2013, 10, 2)], dtype=object)\n\nIn [263]: f = methodcaller('date')\n\nIn [264]: flam = lambda x: x.date()\n\nIn [265]: fmeth = Timestamp.date\n\nIn [266]: s2 = Series(date_range('20010101', periods=1000000, freq='T'))\n\nIn [267]: s2\nOut[267]:\n0    2001-01-01 00:00:00\n1    2001-01-01 00:01:00\n2    2001-01-01 00:02:00\n3    2001-01-01 00:03:00\n4    2001-01-01 00:04:00\n5    2001-01-01 00:05:00\n6    2001-01-01 00:06:00\n7    2001-01-01 00:07:00\n8    2001-01-01 00:08:00\n9    2001-01-01 00:09:00\n10   2001-01-01 00:10:00\n11   2001-01-01 00:11:00\n12   2001-01-01 00:12:00\n13   2001-01-01 00:13:00\n14   2001-01-01 00:14:00\n...\n999985   2002-11-26 10:25:00\n999986   2002-11-26 10:26:00\n999987   2002-11-26 10:27:00\n999988   2002-11-26 10:28:00\n999989   2002-11-26 10:29:00\n999990   2002-11-26 10:30:00\n999991   2002-11-26 10:31:00\n999992   2002-11-26 10:32:00\n999993   2002-11-26 10:33:00\n999994   2002-11-26 10:34:00\n999995   2002-11-26 10:35:00\n999996   2002-11-26 10:36:00\n999997   2002-11-26 10:37:00\n999998   2002-11-26 10:38:00\n999999   2002-11-26 10:39:00\nLength: 1000000, dtype: datetime64[ns]\n\nIn [269]: timeit s2.map(f)\n1 loops, best of 3: 1.04 s per loop\n\nIn [270]: timeit s2.map(flam)\n1 loops, best of 3: 1.1 s per loop\n\nIn [271]: timeit s2.map(fmeth)\n1 loops, best of 3: 968 ms per loop\n"
'In [11]: pd.rolling_sum(ts[::-1], window=3, min_periods=0)[::-1]\nOut[11]:\n2011-01-10     3\n2011-01-11     6\n2011-01-12     9\n2011-01-13    12\n2011-01-14    15\n2011-01-15    18\n2011-01-16    21\n2011-01-17    24\n2011-01-18    17\n2011-01-19     9\nFreq: D, dtype: float64\n'
"sve2_all.loc[sve2_all['Hgtot ng/l'] &gt; 100] = np.nan\n"
"df1['date'] = [d.strftime('%Y-%m-%d') if not pd.isnull(d) else '' for d in df1['date']]\n\nimport numpy as np\nimport pandas as pd\nTimestamp = pd.Timestamp\nnan = np.nan\nNaT = pd.NaT\ndf1 = pd.DataFrame({\n    'col1': list('ac'),\n    'col2': ['b', nan],\n    'date': (Timestamp('2014-08-14'), NaT)\n    })\n\ndf1['col2'] = df1['col2'].fillna('')\ndf1['date'] = [d.strftime('%Y-%m-%d') if not pd.isnull(d) else '' for d in df1['date']]\n\nprint(df1)\n\n  col1 col2        date\n0    a    b  2014-08-14\n1    c                 \n"
"df=pd.read_csv(StringIO('\\n'.join([str(x) for x in range(1000000)] + ['a string'])))\nDtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n\ntype(df.loc[524287,'0'])\nOut[50]: int\n\ntype(df.loc[524288,'0'])\nOut[51]: str\n"
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; a = np.timedelta64(1620000000000,'ns')\n&gt;&gt;&gt; a.astype('timedelta64[m]')\nnumpy.timedelta64(27,'m')\n"
"In [24]:\n\ndf = pd.DataFrame({'a': [0.1,0.5,'jasdh', 9.0]})\ndf\nOut[24]:\n       a\n0    0.1\n1    0.5\n2  jasdh\n3      9\nIn [27]:\n\ndf.convert_objects(convert_numeric=True)\nOut[27]:\n     a\n0  0.1\n1  0.5\n2  NaN\n3  9.0\nIn [29]:\n\ndf.convert_objects(convert_numeric=True).dropna()\nOut[29]:\n     a\n0  0.1\n1  0.5\n3  9.0\n\ndf.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n"
"In [8]: df = pd.DataFrame(np.arange(12).reshape(4,3), columns=list('ABC'))\n\nIn [9]: df.loc[:, ['A','B']]\nOut[9]: \n   A   B\n0  0   1\n1  3   4\n2  6   7\n3  9  10\n\nIn [10]: df.loc[:, ('A','B')]\nOut[10]: \n   A   B\n0  0   1\n1  3   4\n2  6   7\n3  9  10\n\ndf = pd.DataFrame(np.random.randint(10, size=(5,4)),\n                  columns=pd.MultiIndex.from_arrays([['foo']*2+['bar']*2,\n                                                     list('ABAB')]),\n                  index=pd.MultiIndex.from_arrays([['baz']*2+['qux']*3,\n                                                   list('CDCDC')]))\n\n#       foo    bar   \n#         A  B   A  B\n# baz C   7  9   9  9\n#     D   7  5   5  4\n# qux C   5  0   5  1\n#     D   1  7   7  4\n#     C   6  4   3  5\n\nIn [27]: df.loc[:, ('foo','B')]\nOut[27]: \nbaz  C    9\n     D    5\nqux  C    0\n     D    7\n     C    4\nName: (foo, B), dtype: int64\n\nIn [28]: df.loc[:, ['foo','B']]\nKeyError: 'MultiIndex Slicing requires the index to be fully lexsorted tuple len (1), lexsort depth (0)'\n\nIn [29]: df.sortlevel(axis=1).loc[:, ('foo','B')]\nOut[29]: \nbaz  C    9\n     D    5\nqux  C    0\n     D    7\n     C    4\nName: (foo, B), dtype: int64\n\nIn [30]: df.sortlevel(axis=1).loc[:, ['foo','B']]\nOut[30]: \n      foo   \n        A  B\nbaz C   7  9\n    D   7  5\nqux C   5  0\n    D   1  7\n    C   6  4\n\ndf.loc[...] = value\n\ndf.loc[...][...] = value\n\ndf.loc[...][...] = value\n"
'import pandas as pd\nfrom datetime import datetime\n\nps = pd.Series([datetime(2014, 1, 7), datetime(2014, 3, 13), datetime(2014, 6, 12)])\nnew = ps.apply(lambda dt: dt.replace(day=1))\n\n0   2014-01-01\n1   2014-03-01\n2   2014-06-01\ndtype: datetime64[ns]\n'
'&gt;&gt;&gt; from sklearn import cross_validation\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n&gt;&gt;&gt; y = np.array([0, 0, 1, 1])\n&gt;&gt;&gt; skf = cross_validation.StratifiedKFold(y, n_folds=2)\n&gt;&gt;&gt; len(skf)\n2\n&gt;&gt;&gt; print(skf)  \nsklearn.cross_validation.StratifiedKFold(labels=[0 0 1 1], n_folds=2,\n                                         shuffle=False, random_state=None)\n&gt;&gt;&gt; for train_index, test_index in skf:\n...    print("TRAIN:", train_index, "TEST:", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [1 3] TEST: [0 2]\nTRAIN: [0 2] TEST: [1 3]\n'
'import pandas as pd\nfrom StringIO import StringIO\n\ncsv = r"""Date,Open Price,High Price,Low Price,Close Price,WAP,No.of Shares,No. of Trades,Total Turnover (Rs.),Deliverable Quantity,% Deli. Qty to Traded Qty,Spread High-Low,Spread Close-Open\n28-February-2015,2270.00,2310.00,2258.00,2294.85,2279.192067772602217319,73422,8043,167342840.00,11556,15.74,52.00,24.85\n27-February-2015,2267.25,2280.85,2258.00,2266.35,2269.239841485775122730,50721,4938,115098114.00,12297,24.24,22.85,-0.90\n26-February-2015,2314.90,2314.90,2250.00,2259.50,2277.198324862194860047,69845,8403,159050917.00,22046,31.56,64.90,-55.40\n25-February-2015,2290.00,2332.00,2278.35,2318.05,2315.100614216488163214,161995,10174,375034724.00,102972,63.56,53.65,28.05\n24-February-2015,2276.05,2295.00,2258.00,2278.15,2281.058946240263344242,52251,7726,119187611.00,13292,25.44,37.00,2.10\n23-February-2015,2303.95,2311.00,2253.25,2270.70,2281.912259219760108491,75951,7344,173313518.00,24969,32.88,57.75,-33.25\n20-February-2015,2324.00,2335.20,2277.00,2284.30,2301.631421152326354478,79717,10233,183479152.00,23045,28.91,58.20,-39.70\n19-February-2015,2304.00,2333.90,2292.00,2326.60,2321.485466301625211160,85835,8847,199264705.00,29728,34.63,41.90,22.60\n18-February-2015,2284.00,2305.00,2261.10,2295.75,2282.060986778089405300,69884,6639,159479550.00,26665,38.16,43.90,11.75\n16-February-2015,2281.00,2305.85,2266.00,2278.50,2284.961866239581019628,85541,10149,195457923.00,22164,25.91,39.85,-2.50\n13-February-2015,2311.00,2324.90,2286.95,2296.40,2311.371235111317676864,109731,5570,253629077.00,69039,62.92,37.95,-14.60\n12-February-2015,2280.00,2322.85,2275.00,2315.45,2301.372038211769425569,79766,9095,183571242.00,33981,42.60,47.85,35.45\n    11-February-2015,2275.00,2295.00,2258.25,2287.20,2279.587966250020639664,60563,7467,138058686.00,20058,33.12,36.75,12.20\n    10-February-2015,2244.90,2297.40,2225.00,2280.30,2269.562228214830293104,141656,13026,321497107.00,55577,39.23,72.40,35.40"""\n\ndf = pd.read_csv(StringIO(csv), \n        usecols=["Date", "Open Price", "Close Price"],\n        header=0)\n\ndf.columns = [\'Date\', \'O\', \'C\']\n\ndf\n\n                Date        O        C\n0   28-February-2015  2270.00  2294.85\n1   27-February-2015  2267.25  2266.35\n2   26-February-2015  2314.90  2259.50\n3   25-February-2015  2290.00  2318.05\n4   24-February-2015  2276.05  2278.15\n5   23-February-2015  2303.95  2270.70\n6   20-February-2015  2324.00  2284.30\n7   19-February-2015  2304.00  2326.60\n8   18-February-2015  2284.00  2295.75\n9   16-February-2015  2281.00  2278.50\n10  13-February-2015  2311.00  2296.40\n11  12-February-2015  2280.00  2315.45\n12  11-February-2015  2275.00  2287.20\n13  10-February-2015  2244.90  2280.30\n'
"df['Total'] = df.groupby(['Fullname', 'Zip'])['Amount'].transform('sum')\n\nnew_df = df.drop_duplicates(subset=['Fullname', 'Zip'])\n"
"In [363]:\n\ndf.loc[:'2015-04-25']\nOut[363]:\n                   A\n2015-04-25  0.141787\n2015-04-26  0.598237\n2015-04-27  0.106461\n2015-04-28  0.297159\n2015-04-29  0.058392\n2015-04-30  0.621325\nIn [364]:\n\ndf.loc['2015-04-25':]\nOut[364]:\n                   A\n2015-04-25  0.141787\n2015-04-26  0.598237\n2015-04-27  0.106461\n2015-04-28  0.297159\n2015-04-29  0.058392\n2015-04-30  0.621325\n\nIn [378]:\n\ndf.loc[:'2015-04-25'].head(3)\nOut[378]:\n                   A\n2015-04-20  0.827699\n2015-04-21  0.901140\n2015-04-22  0.427304\n\nIn [377]:\n\ndf.loc[:'2015-04-25'].tail(3)\nOut[377]:\n                   A\n2015-04-23  0.002189\n2015-04-24  0.041965\n2015-04-25  0.141787\n\nIn [388]:\n\ndf.index.get_loc('2015-04-25')\nOut[388]:\n5\nIn [391]:\n\ndf.iloc[df.index.get_loc('2015-04-25')-1]\nOut[391]:\nA    0.041965\nName: 2015-04-24 00:00:00, dtype: float64\nIn [392]:\n\ndf.iloc[df.index.get_loc('2015-04-25')+1]\nOut[392]:\nA    0.598237\nName: 2015-04-26 00:00:00, dtype: float64\n"
"from datetime import datetime\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as dt\n\ndf = pd.read_csv('data.csv')\ndf.amin = pd.to_datetime(df.amin).astype(datetime)\ndf.amax = pd.to_datetime(df.amax).astype(datetime)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax = ax.xaxis_date()\nax = plt.hlines(df.index, dt.date2num(df.amin), dt.date2num(df.amax))\n"
"In [8]:\ndf['OVER 5 MINS'] = (df['TIMESTAMP'].diff()).dt.seconds &gt; 300\ndf\n\nOut[8]:\n                    TIMESTAMP  EVENT_COUNT OVER 5 MINS\nINDEX                                                 \n0     2014-07-23 04:28:23.000            1       False\n1     2014-07-23 04:28:24.000            1       False\n2     2014-07-23 04:28:25.999            4       False\n3     2014-07-23 04:28:27.000            1       False\n4     2014-07-23 04:28:28.999            2       False\n5     2014-07-23 04:28:30.000            1       False\n6     2014-07-23 04:29:31.000            7       False\n7     2014-07-23 04:29:33.000            1       False\n8     2014-07-23 04:29:34.000            1       False\n9     2014-07-23 04:29:36.000            1       False\n10    2014-07-23 04:40:37.000            2        True\n11    2014-07-23 04:40:39.000            1       False\n12    2014-07-23 04:40:40.000            1       False\n13    2014-07-23 04:40:42.000            1       False\n14    2014-07-23 04:40:43.000            1       False\n15    2014-07-23 04:40:44.999            4       False\n16    2014-07-23 04:41:46.000            1       False\n17    2014-07-23 04:41:47.000            1       False\n18    2014-07-23 04:41:49.000            1       False\n19    2014-07-23 04:41:50.000            1       False\n20    2014-07-23 04:50:52.000            9        True\n21    2014-07-23 04:50:53.000            4       False\n22    2014-07-23 04:50:55.000            6       False\n23    2014-07-27 01:12:13.000            1        True\n"
'In [11]: pd.DataFrame(data)\nOut[11]:\n    DC?     building occupants\n0  True  White House    Barack\n1  True  White House  Michelle\n2  True  White House     Sasha\n3  True  White House     Malia\n\nIn [12]: pd.DataFrame([data])\nOut[12]:\n    DC?     building                         occupants\n0  True  White House  [Barack, Michelle, Sasha, Malia]\n'
"import pandas as pd\n\ndata = pd.DataFrame([\n        ('Q1','Blue',100),\n        ('Q1','Green',300),\n        ('Q2','Blue',200),\n        ('Q2','Green',350),\n        ('Q3','Blue',300),\n        ('Q3','Green',400),\n        ('Q4','Blue',400),\n        ('Q4','Green',450),\n    ], \n    columns=['quarter', 'company', 'value']\n)\ndata = data.set_index(['quarter', 'company']).value\n\ndata.unstack().plot(kind='bar', stacked=True)\n\ndata.unstack().plot(kind='bar')\n"
"entries=entries.sort(['i','j','ColumnA','ColumnB'])\n"
"In [15]: data[data.columns[1:]].corr()['special_col'][:-1]\nOut[15]: \nstem1    0.500000\nstem2   -0.500000\nstem3   -0.999945\nb1       0.500000\nb2       0.500000\nb3      -0.500000\nName: special_col, dtype: float64\n\nIn [33]: np.corrcoef(data[data.columns[1:]].T)[-1][:-1]\nOut[33]: \narray([ 0.5       , -0.5       , -0.99994535,  0.5       ,  0.5       ,\n       -0.5       ])\n\nIn [34]: %timeit np.corrcoef(data[data.columns[1:]].T)[-1][:-1]\n1000 loops, best of 3: 437 µs per loop\n\nIn [35]: %timeit data[data.columns[1:]].corr()['special_col']\n1000 loops, best of 3: 526 µs per loop\n"
"sio = StringIO()\nPandasDataFrame = pandas.DataFrame(self.csvdict)\nPandasWriter = pandas.ExcelWriter(sio, engine='xlsxwriter')\nPandasDataFrame.to_excel(PandasWriter, sheet_name=sheetname)\nPandasWriter.save()\n\nsio.seek(0)\nworkbook = sio.getvalue()\n\nresponse = StreamingHttpResponse(workbook, content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')\nresponse['Content-Disposition'] = 'attachment; filename=%s' % filename\n"
"In [12]:\ndf = pd.DataFrame(columns=['Size','COLOUR','caTegory'])\ndf.columns\n\nOut[12]:\nIndex(['Size', 'COLOUR', 'caTegory'], dtype='object')\n\nIn [14]:\ndf.columns = df.columns.str.lower()\ndf.columns\n\nOut[14]:\nIndex(['size', 'colour', 'category'], dtype='object')\n"
'import io\n\n# text buffer\ns_buf = io.StringIO()\n\n# saving a data frame to a buffer (same as with a regular file):\ndf.to_csv(s_buf)\n\ns_buf.seek(0)\n\ncur.copy_from(s_buf, table)\n'
"pd.DataFrame(reweightTarget, columns=['t'])\n"
'&gt;&gt;&gt; a = np.zeros((5,2))\n&gt;&gt;&gt; a\narray([[ 0.,  0.],\n       [ 0.,  0.],\n       [ 0.,  0.],\n       [ 0.,  0.],\n       [ 0.,  0.]])\n&gt;&gt;&gt; a.tolist()\n[[0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]\n&gt;&gt;&gt; pd.Series(a.tolist())\n0    [0.0, 0.0]\n1    [0.0, 0.0]\n2    [0.0, 0.0]\n3    [0.0, 0.0]\n4    [0.0, 0.0]\ndtype: object\n'
"dates = pd.date_range('1995-12-31', periods=480, freq='M', name='Date')\nstoks = pd.Index(['s{:04d}'.format(i) for i in range(4000)])\ndf = pd.DataFrame(np.random.rand(480, 4000), dates, stoks)\n\ndf.iloc[:5, :5]\n\ndef roll(df, w):\n    # stack df.values w-times shifted once at each stack\n    roll_array = np.dstack([df.values[i:i+w, :] for i in range(len(df.index) - w + 1)]).T\n    # roll_array is now a 3-D array and can be read into\n    # a pandas panel object\n    panel = pd.Panel(roll_array, \n                     items=df.index[w-1:],\n                     major_axis=df.columns,\n                     minor_axis=pd.Index(range(w), name='roll'))\n    # convert to dataframe and pivot + groupby\n    # is now ready for any action normally performed\n    # on a groupby object\n    return panel.to_frame().unstack().T.groupby(level=0)\n\ndef beta(df):\n    # first column is the market\n    X = df.values[:, [0]]\n    # prepend a column of ones for the intercept\n    X = np.concatenate([np.ones_like(X), X], axis=1)\n    # matrix algebra\n    b = np.linalg.pinv(X.T.dot(X)).dot(X.T).dot(df.values[:, 1:])\n    return pd.Series(b[1], df.columns[1:], name='Beta')\n\nrdf = roll(df, 12)\nbetas = rdf.apply(beta)\n\ndef calc_beta(df):\n    np_array = df.values\n    m = np_array[:,0] # market returns are column zero from numpy array\n    s = np_array[:,1] # stock returns are column one from numpy array\n    covariance = np.cov(s,m) # Calculate covariance between stock and market\n    beta = covariance[0,1]/covariance[1,1]\n    return beta\n\nprint(calc_beta(df.iloc[:12, :2]))\n\n-0.311757542437\n\nprint(beta(df.iloc[:12, :2]))\n\ns0001   -0.311758\nName: Beta, dtype: float64\n\nbetas = rdf.apply(beta)\nbetas.iloc[:5, :5]\n\nnum_sec_dfs = 4000\n\ncols = ['Open', 'High', 'Low', 'Close']\ndfs = {'s{:04d}'.format(i): pd.DataFrame(np.random.rand(480, 4), dates, cols) for i in range(num_sec_dfs)}\n\nmarket = pd.Series(np.random.rand(480), dates, name='Market')\n\ndf = pd.concat([market] + [dfs[k].Close.rename(k) for k in dfs.keys()], axis=1).sort_index(1)\n\nbetas = roll(df.pct_change().dropna(), 12).apply(beta)\n\nfor c, col in betas.iteritems():\n    dfs[c]['Beta'] = col\n\ndfs['s0001'].head(20)\n"
"pd.io.json.json_normalize(df.data.apply(json.loads))\n\nimport pandas as pd\nimport json\n\ndf = pd.read_csv('http://pastebin.com/raw/7L86m9R2', \\\n                 header=None, index_col=0, names=['data'])\n"
"df['Title'] = df['Title'].str.replace('\\(\\(\\(', '&gt;&gt;')\n"
"df['CUMSUM_C'] = df['SUM_C'].cumsum()\n\ndf\nOut[34]: \n   A  B  SUM_C  CUMSUM_C\n0  1  1     10       10\n1  1  2     20       30\n"
"data_xls.to_csv('csvfile.csv', encoding='utf-8', index=False)\n"
"In [13]: df.loc[:, df.columns.str.startswith('alp')]\nOut[13]:\n       alp1      alp2\n0  0.357564  0.108907\n1  0.341087  0.198098\n2  0.416215  0.644166\n3  0.814056  0.121044\n4  0.382681  0.110829\n5  0.130343  0.219829\n6  0.110049  0.681618\n7  0.949599  0.089632\n8  0.047945  0.855116\n9  0.561441  0.291182\n\nIn [14]: df.loc[:, df.columns.str.contains('alp')]\nOut[14]:\n       alp1      alp2\n0  0.357564  0.108907\n1  0.341087  0.198098\n2  0.416215  0.644166\n3  0.814056  0.121044\n4  0.382681  0.110829\n5  0.130343  0.219829\n6  0.110049  0.681618\n7  0.949599  0.089632\n8  0.047945  0.855116\n9  0.561441  0.291182\n"
"pd.DataFrame(df.codes.values.tolist()).add_prefix('code_')\n\n   code_0   code_1   code_2\n0   71020      NaN      NaN\n1   77085      NaN      NaN\n2   36415      NaN      NaN\n3   99213  99287.0      NaN\n4   99233  99233.0  99233.0\n\npd.DataFrame(df.codes.values.tolist(), df.index).add_prefix('code_')\n\n   code_0   code_1   code_2\n1   71020      NaN      NaN\n2   77085      NaN      NaN\n3   36415      NaN      NaN\n4   99213  99287.0      NaN\n5   99233  99233.0  99233.0\n\nf = lambda x: 'code_{}'.format(x + 1)\npd.DataFrame(\n    df.codes.values.tolist(),\n    df.index, dtype=object\n).fillna('').rename(columns=f)\n\n   code_1 code_2 code_3\n1   71020              \n2   77085              \n3   36415              \n4   99213  99287       \n5   99233  99233  99233\n"
'print (pd.merge(a,b, indicator=True, how=\'outer\')\n         .query(\'_merge=="left_only"\')\n         .drop(\'_merge\', axis=1))\n   0   1\n0  1  10\n2  3  30\n'
"df.columns = [''] * len(df.columns)\n\ndf.to_csv('file.csv', header=False, index=False)\n\ndf.to_excel('file.xlsx', header=False, index=False)\n"
"np.random.seed(100)\ndf = pd.DataFrame(np.random.random((10,5)), columns=list('ABCDE'))\ndf.index = df.index * 10\nprint (df)\n           A         B         C         D         E\n0   0.543405  0.278369  0.424518  0.844776  0.004719\n10  0.121569  0.670749  0.825853  0.136707  0.575093\n20  0.891322  0.209202  0.185328  0.108377  0.219697\n30  0.978624  0.811683  0.171941  0.816225  0.274074\n40  0.431704  0.940030  0.817649  0.336112  0.175410\n50  0.372832  0.005689  0.252426  0.795663  0.015255\n60  0.598843  0.603805  0.105148  0.381943  0.036476\n70  0.890412  0.980921  0.059942  0.890546  0.576901\n80  0.742480  0.630184  0.581842  0.020439  0.210027\n90  0.544685  0.769115  0.250695  0.285896  0.852395\n\nfrom sklearn.model_selection import KFold\n\n#added some parameters\nkf = KFold(n_splits = 5, shuffle = True, random_state = 2)\nresult = next(kf.split(df), None)\nprint (result)\n(array([0, 2, 3, 5, 6, 7, 8, 9]), array([1, 4]))\n\ntrain = df.iloc[result[0]]\ntest =  df.iloc[result[1]]\n\nprint (train)\n           A         B         C         D         E\n0   0.543405  0.278369  0.424518  0.844776  0.004719\n20  0.891322  0.209202  0.185328  0.108377  0.219697\n30  0.978624  0.811683  0.171941  0.816225  0.274074\n50  0.372832  0.005689  0.252426  0.795663  0.015255\n60  0.598843  0.603805  0.105148  0.381943  0.036476\n70  0.890412  0.980921  0.059942  0.890546  0.576901\n80  0.742480  0.630184  0.581842  0.020439  0.210027\n90  0.544685  0.769115  0.250695  0.285896  0.852395\n\nprint (test)\n           A         B         C         D         E\n10  0.121569  0.670749  0.825853  0.136707  0.575093\n40  0.431704  0.940030  0.817649  0.336112  0.175410\n"
'fish_frame = fish_frame.iloc[:, :-1]\n\n                              0        1      2\n0                         #0721      NaN    NaN\n1                       GBE COD      746  $2.00\n2                       GBW COD   13,894  $0.50\n3                       GOM COD       60  $2.00\n4            GB WINTER FLOUNDER   94,158  $0.25\n5           GOM WINTER FLOUNDER    3,030  $0.50\n6                   GBE HADDOCK   18,479  $0.02\n7                   GOM HADDOCK        0  $0.02\n8                   GBW HADDOCK  110,470  $0.02\n9                          HAKE      259  $1.30\n10                       PLAICE    3,738  $0.40\n11                      POLLOCK    3,265  $0.02\n12               WITCH FLOUNDER    1,134  $1.30\n13                       SNE YT    1,458  $0.65\n14                        GB YT    4,499  $0.70\n15                      REDFISH      841  $0.02\n16  54 DAS @ $8.00/DAY = 432.00      NaN    NaN\n'
"In [29]: idx = pd.IntervalIndex.from_tuples(data*10000)\n\nIn [30]: %timeit -n 1 -r 1 idx.map(lambda x: 900 in x)\n92.8 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\nIn [40]: %timeit -n 1 -r 1 idx.map(lambda x: 900 in x)\n42.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n# construct tree and search\nIn [31]: %timeit -n 1 -r 1 idx.get_loc(900)\n4.55 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n# subsequently\nIn [32]: %timeit -n 1 -r 1 idx.get_loc(900)\n137 µs ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n# for a single indexer you can do even better (note that this is\n# dipping into the impl a bit\nIn [27]: %timeit np.arange(len(idx))[(900 &gt; idx.left) &amp; (900 &lt;= idx.right)]\n203 µs ± 1.55 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\nIn [38]: idx.map(lambda x: 900 in x)\n    ...: \nOut[38]: \nIndex([ True, False, False,  True, False, False,  True, False, False,  True,\n       ...\n       False,  True, False, False,  True, False, False,  True, False, False], dtype='object', length=30000)\n\nIn [39]: idx.get_loc(900)\n    ...: \nOut[39]: array([29997,  9987, 10008, ..., 19992, 19989,     0])\n\nIn [5]: np.arange(len(idx))[idx.map(lambda x: 900 in x).values.astype(bool)]\nOut[5]: array([    0,     3,     6, ..., 29991, 29994, 29997])\n\nIn [6]: np.sort(idx.get_loc(900))\nOut[6]: array([    0,     3,     6, ..., 29991, 29994, 29997])\n"
"df['col'].str.get_dummies(sep=',')\n\n    a   b   c   d\n0   1   0   0   0\n1   1   1   1   0\n2   1   1   0   1\n3   0   0   0   1\n4   0   0   1   1\n\ndf['col'].str.get_dummies(sep=',').add_prefix('col_')\n\ndf = pd.DataFrame({'other':['x','y','x','x','q'],'col':['a','a,b,c','a,b,d','d','c,d']})\ndf = pd.concat([df, df['col'].str.get_dummies(sep=',')], axis = 1).drop('col', 1)\n\n  other a   b   c   d\n0   x   1   0   0   0\n1   y   1   1   1   0\n2   x   1   1   0   1\n3   x   0   0   0   1\n4   q   0   0   1   1\n"
"df.groupby(['col_a', 'col_b']).ngroups\nOut[101]: 6\n\nlen(set(zip(df['col_a'],df['col_b'])))\nOut[106]: 6\n"
'import pandas as pd\n\ndf_in  = pd.read_csv(source_file, float_precision=\'round_trip\')\ndf_out = ... # some processing of df_in\ndf_out.to_csv(target_file, float_format="%.3f") # for 3 decimal places\n'
'import statsmodels.api as sm\nimport numpy as np\nnp.random.seed(1)\nX = sm.add_constant(np.arange(100))\ny = np.dot(X, [1,2]) + np.random.normal(size=100)\nresult = sm.OLS(y, X).fit()\nprint(result.params)\n'
"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = [2, 1, 76, 140, 286, 267, 60, 271, 5, 13, 9, 76, 77, 6, 2, 27, 22, 1, 12, 7, \n     19, 81, 11, 173, 13, 7, 16, 19, 23, 197, 167, 1]\nx = pd.Series(x)\n\n# histogram on linear scale\nplt.subplot(211)\nhist, bins, _ = plt.hist(x, bins=8)\n\n# histogram on log scale. \n# Use non-equal bin sizes, such that they look equal on log scale.\nlogbins = np.logspace(np.log10(bins[0]),np.log10(bins[-1]),len(bins))\nplt.subplot(212)\nplt.hist(x, bins=logbins)\nplt.xscale('log')\nplt.show()\n"
"df=pd.DataFrame({'x':range(4)}, \n    index=pd.to_datetime(['1-1-2018','1-2-2018','1-4-2018','1-5-2018']))\n\n            x\n2018-01-01  0\n2018-01-02  1\n2018-01-04  2\n2018-01-05  3\n\ndf.rolling(2).count()\n\n              x\n2018-01-01  1.0\n2018-01-02  2.0\n2018-01-04  2.0\n2018-01-05  2.0\n\ndf.rolling('2D').count()\n\n              x\n2018-01-01  1.0\n2018-01-02  2.0\n2018-01-04  1.0\n2018-01-05  2.0\n"
"df = pd.DataFrame({'author':['a', 'a', 'b'], 'subreddit':['sr1', 'sr2', 'sr2']})\n\n&gt;&gt;&gt; df\n  author subreddit\n0      a       sr1\n1      a       sr2\n2      b       sr2\n...\n\ngroup = df.groupby('author')\n\ndf2 = group.apply(lambda x: x['subreddit'].unique())\n\n# Alternatively, same thing as a one liner:\n# df2 = df.groupby('author').apply(lambda x: x['subreddit'].unique())\n\n&gt;&gt;&gt; df2\nauthor\na    [sr1, sr2]\nb         [sr2]\n\ndf2 = df2.apply(pd.Series)\n\n&gt;&gt;&gt; df2\n          0    1\nauthor          \na       sr1  sr2\nb       sr2  NaN\n\ndf2 = pd.DataFrame({'author':df.author.unique()})\n\ndf2['subreddits'] = [list(set(df['subreddit'].loc[df['author'] == x['author']])) \n    for _, x in df2.iterrows()]\n\n&gt;&gt;&gt; df2\n  author  subreddits\n0      a  [sr2, sr1]\n1      b       [sr2]\n"
"pd_df['difficulty'] = np.where(\n     pd_df['Time'].between(0, 30, inclusive=False), \n    'Easy', \n     np.where(\n        pd_df['Time'].between(0, 30, inclusive=False), 'Medium', 'Unknown'\n     )\n)\n\npd_df['difficulty'] = np.select(\n    [\n        pd_df['Time'].between(0, 30, inclusive=False), \n        pd_df['Time'].between(30, 60, inclusive=True)\n    ], \n    [\n        'Easy', \n        'Medium'\n    ], \n    default='Unknown'\n)\n\npd_df['difficulty'] = 'Unknown'\npd_df.loc[pd_df['Time'].between(0, 30, inclusive=False), 'difficulty'] = 'Easy'\npd_df.loc[pd_df['Time'].between(30, 60, inclusive=True), 'difficulty'] = 'Medium'\n"
"df=pd.DataFrame(columns=['a'])\ndf['b'] = None\ndf = df.assign(c=None)\ndf = df.assign(d=df['a'])\ndf['e'] = pd.Series(index=df.index)   \ndf = pd.concat([df,pd.DataFrame(columns=list('f'))])\nprint(df)\n\nEmpty DataFrame\nColumns: [a, b, c, d, e, f]\nIndex: []\n"
"m = (df['Name'] == 'Alisa') &amp; (df['Age'] &gt; 24)\nprint(m)\n0      True\n1     False\n2     False\n3     False\n4     False\n5     False\n6      True\n7     False\n8     False\n9     False\n10    False\n11    False\ndtype: bool\n\n#invert mask by ~\ndf1 = df[~m]\n\ndef filter_fn(row):\n    if row['Name'] == 'Alisa' and row['Age'] &gt; 24:\n        return False\n    else:\n        return True\n\ndf = pd.DataFrame(d, columns=['Name', 'Age', 'Score'])\nm = df.apply(filter_fn, axis=1)\nprint(m)\n0     False\n1      True\n2      True\n3      True\n4      True\n5      True\n6     False\n7      True\n8      True\n9      True\n10     True\n11     True\ndtype: bool\n\ndf1 = df[m]\n"
'import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,10)) \nplt.bar(x[\'user\'], x[\'number\'], color="blue")\n\nimport matplotlib.pyplot as plt\n\nplt.bar(x[\'user\'], x[\'number\'], color="blue")\nplt.gcf().set_size_inches(20, 10)\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20, 10)).gca().bar(x[\'user\'], x[\'number\'], color="blue")\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 10))\nax = fig.add_subplot(111)\nax.bar(x[\'user\'], x[\'number\'], color="blue")\n'
'# NA group\nlmask = llab == -1\nlany = lmask.any()\nrmask = rlab == -1\nrany = rmask.any()\n\nif lany or rany:\n    if lany:\n        np.putmask(llab, lmask, count)\n    if rany:\n        np.putmask(rlab, rmask, count)\n    count += 1\n'
"csub = inv.query('County == @county')\n"
"ts.resample(rule='24H', closed='left', label='left', base=17).sum()\n\n2012-01-01 17:00:00    24\n2012-01-02 17:00:00    24\n2012-01-03 17:00:00    12\nFreq: 24H\n"
"store.select('df', [ Term('index', '&gt;', Timestamp('20010105')), \n                     Term('columns', '=', ['A','B']) ])\n\ndf = store.select('df', [ Term('index', '&gt;', Timestamp('20010105') ])\ndf.reindex(columns = ['A','B'])\n\n store.append('df', columns = ['A','B','C'])\n store.select('df', [ 'A &gt; 0', Term('index', '&gt;', Timestamp(2000105)) ])\n"
'import pandas as pd\nfrom tabulate import tabulate\n\ndef to_fwf(df, fname):\n    content = tabulate(df.values.tolist(), list(df.columns), tablefmt="plain")\n    open(fname, "w").write(content)\n\npd.DataFrame.to_fwf = to_fwf\n'
"In [11]: df[abc_columns].applymap(categories.get)\nOut[11]:\n   abc1  abc2  abc3\n0  Good   Bad   Bad\n1   Bad  Good  Good\n2   Bad   Bad  Good\n3  Good   Bad  Good\n4  Good  Good   Bad\n\nIn [12]: abc_categories = map(lambda x: x + '_category', abc_columns)\n\nIn [13]: abc_categories\nOut[13]: ['abc1_category', 'abc2_category', 'abc3_category']\n\nIn [14]: df[abc_categories] = df[abc_columns].applymap(categories.get)\n\nabc_columns = [col for col in df.columns if str(col).startswith('abc')]\n"
'pd.options.display.max_columns = 50\n\ndf.head(5)[df.columns[0:4]]\n# alternatively\ndf.iloc[:5, :4]\n'
"&gt;&gt;&gt; def random(row):\n...     return row.mean()\n\n&gt;&gt;&gt; df['new'] = df.apply(func = random, axis = 1)\n&gt;&gt;&gt; df\n          A         B         C         D       new\n0  0.201143 -2.345828 -2.186106 -0.784721 -1.278878\n1 -0.198460  0.544879  0.554407 -0.161357  0.184867\n2  0.269807  1.132344  0.120303 -0.116843  0.351403\n3 -1.131396  1.278477  1.567599  0.483912  0.549648\n4  0.288147  0.382764 -0.840972  0.838950  0.167222\n\n&gt;&gt;&gt; def random(row):\n...    return (1,2,3,4,5)\n...\n&gt;&gt;&gt; df['new'] = df.apply(func = random, axis = 1)\n&gt;&gt;&gt; df\n          A         B         C         D              new\n0  0.201143 -2.345828 -2.186106 -0.784721  (1, 2, 3, 4, 5)\n1 -0.198460  0.544879  0.554407 -0.161357  (1, 2, 3, 4, 5)\n2  0.269807  1.132344  0.120303 -0.116843  (1, 2, 3, 4, 5)\n3 -1.131396  1.278477  1.567599  0.483912  (1, 2, 3, 4, 5)\n4  0.288147  0.382764 -0.840972  0.838950  (1, 2, 3, 4, 5)\n"
'import pandas as pd\nimport random\nimport matplotlib.pyplot as plt\n\nn = 100\n# this is probably a strange way to generate random data; please feel free to correct it\ndf = pd.DataFrame({"X": [random.choice(["A","B","C"]) for i in range(n)], \n                   "Y": [random.choice(["a","b","c"]) for i in range(n)],\n                   "Z": [random.gauss(0,1) for i in range(n)]})\ngrouped = df.groupby(["X", "Y"])\n\ndf2 = pd.DataFrame({col:vals[\'Z\'] for col,vals in grouped})\n\nmeds = df2.median()\nmeds.sort_values(ascending=False, inplace=True)\ndf2 = df2[meds.index]\ndf2.boxplot()\n\nplt.show()\n'
"import pandas as pd\ndf = pd.DataFrame({'SpT': ['string1', 'string2', 'string3'],\n                   'num': ['0.1', '0.2', '0.3'],\n                   'strange': ['0.1', '0.2', 0.3]})\nprint df.dtypes\n#SpT        object\n#num        object\n#strange    object\n#dtype: object\n\nprint df['num'].apply(lambda x: len(x))\n#0    3\n#1    3\n#2    3\n\nprint df['strange'].apply(lambda x: len(x))\n# TypeError: object of type 'float' has no len()\n\ndf['strange'] = df['strange'].astype(str)\nprint df['strange'].apply(lambda x: len(x))\n#0    3\n#1    3\n#2    3\n"
'df = pd.DataFrame(data=[[34, \'null\', \'mark\'], [22, \'null\', \'mark\'], [34, \'null\', \'mark\']], columns=[\'id\', \'temp\', \'name\'], index=[1, 2, 3]) \n\nfor c in df.columns:\n    print "---- %s ---" % c\n    print df[c].value_counts()\n\n---- id ---\n34    2\n22    1\ndtype: int64\n---- temp ---\nnull    3\ndtype: int64\n---- name ---\nmark    3\ndtype: int64\n'
"print df1\n\n    Team  Year  foo\n0   Hawks  2001    5\n1   Hawks  2004    4\n2    Nets  1987    3\n3    Nets  1988    6\n4    Nets  2001    8\n5    Nets  2000   10\n6    Heat  2004    6\n7  Pacers  2003   12\n\nprint df2\n\n    Team  Year  foo\n0  Pacers  2003   12\n1    Heat  2004    6\n2    Nets  1988    6\n\nnew = df1.merge(df2,on=['Team','Year'],how='left')\nprint new[new.foo_y.isnull()]\n\n     Team  Year  foo_x  foo_y\n0  Hawks  2001      5    NaN\n1  Hawks  2004      4    NaN\n2   Nets  1987      3    NaN\n4   Nets  2001      8    NaN\n5   Nets  2000     10    NaN\n\ndf1['key'] = df1['Team'] + df1['Year'].astype(str)\ndf2['key'] = df1['Team'] + df2['Year'].astype(str)\nprint df1[~df1.key.isin(df2.key)]\n\n     Team  Year  foo         key\n0   Hawks  2001    5   Hawks2001\n2    Nets  1987    3    Nets1987\n4    Nets  2001    8    Nets2001\n5    Nets  2000   10    Nets2000\n6    Heat  2004    6    Heat2004\n7  Pacers  2003   12  Pacers2003\n"
"df['zscore'] = (df.a - df.a.mean())/df.a.std(ddof=0)\nprint df\n\n        a    zscore\n0     NaN       NaN\n1  0.0767 -1.148329\n2  0.4383  0.071478\n3  0.7866  1.246419\n4  0.8091  1.322320\n5  0.1954 -0.747912\n6  0.6307  0.720512\n7  0.6599  0.819014\n8  0.1065 -1.047803\n9  0.0508 -1.235699\n"
"ax = df_13_14_target[['month','2014_target_val']].plot(x='month',linestyle='-', marker='o')\ndf_13_14_target[['month','2013_val','2014_val']].plot(x='month', kind='bar', \n   ax=ax)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nleft_2013 = pd.DataFrame(\n    {'month': ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep',\n               'oct', 'nov', 'dec'],\n     '2013_val': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 6]})\n\nright_2014 = pd.DataFrame({'month': ['jan', 'feb'], '2014_val': [4, 5]})\n\nright_2014_target = pd.DataFrame(\n    {'month': ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep',\n               'oct', 'nov', 'dec'],\n     '2014_target_val': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]})\n\ndf_13_14 = pd.merge(left_2013, right_2014, how='outer')\ndf_13_14_target = pd.merge(df_13_14, right_2014_target, how='outer')\n\nax = df_13_14_target[['month', '2014_target_val']].plot(\n    x='month', linestyle='-', marker='o')\ndf_13_14_target[['month', '2013_val', '2014_val']].plot(x='month', kind='bar',\n                                                        ax=ax)\n\nplt.show()\n"
'import numpy as np\nimport pandas as pd\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport matplotlib.pyplot as plt\n\nfrom PySide.QtGui import QImage\nfrom PySide.QtGui import QPainter\nfrom PySide.QtCore import QSize\nfrom PySide.QtWebKit import QWebPage\n\narrays = [np.hstack([ [\'one\']*3, [\'two\']*3]), [\'Dog\', \'Bird\', \'Cat\']*2]\ncolumns = pd.MultiIndex.from_arrays(arrays, names=[\'foo\', \'bar\'])\ndf =pd.DataFrame(np.zeros((3,6)),columns=columns,index=pd.date_range(\'20000103\',periods=3))\n\nh = "&lt;!DOCTYPE html&gt; &lt;html&gt; &lt;body&gt; &lt;p&gt; " + df.to_html() + " &lt;/p&gt; &lt;/body&gt; &lt;/html&gt;";\npage = QWebPage()\npage.setViewportSize(QSize(5000,5000))\n\nframe = page.mainFrame()\nframe.setHtml(h, "text/html")\n\nimg = QImage(1000,700, QImage.Format(5))\npainter = QPainter(img)\nframe.render(painter)\npainter.end()\na = img.save("html.png")\n\npp = PdfPages(\'html.pdf\')\nfig = plt.figure(figsize=(8,6),dpi=1080) \nax = fig.add_subplot(1, 1, 1)\nimg2 = plt.imread("html.png")\nplt.axis(\'off\')\nax.imshow(img2)\npp.savefig()\npp.close()\n'
'&gt;&gt;&gt; df = pd.read_excel("out.xlsx", header=None)\n&gt;&gt;&gt; df\n          0         1         2         3         4         5         6\n0  0.619159  0.264191  0.438849  0.465287  0.445819  0.412582  0.397366\n1  0.601379  0.303953  0.457524  0.432335  0.415333  0.382093  0.382361\n2  0.579914  0.343715  0.418294  0.401129  0.385508  0.355392  0.355123\n\n&gt;&gt;&gt; names = [20140109, 20140213, 20140313, 20140410, 20140508, 20140612, 20140714]\n&gt;&gt;&gt; df = pd.read_excel("out.xlsx", header=None, names=names)\n&gt;&gt;&gt; df\n   20140109  20140213  20140313  20140410  20140508  20140612  20140714\n0  0.619159  0.264191  0.438849  0.465287  0.445819  0.412582  0.397366\n1  0.601379  0.303953  0.457524  0.432335  0.415333  0.382093  0.382361\n2  0.579914  0.343715  0.418294  0.401129  0.385508  0.355392  0.355123\n'
"import csv\nfrom glob import iglob\n\nunique_headers = set()\nfor filename in iglob('*.csv'):\n    with open(filename, 'rb') as fin:\n        csvin = csv.reader(fin)\n        unique_headers.update(next(csvin, []))\n"
"In [15]: df.reset_index().groupby('X')['Y'].nunique()\nOut[15]: \nX\nbar    1\nbaz    3\nfoo    1\nqux    2\nName: Y, dtype: int64\n"
"In [22]: start = 1406507532491431\n\nIn [23]: end = 1406535228420914\n\n[26]: dti = pd.to_datetime([start,end],unit='us')\n\nIn [27]: dti\nOut[27]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2014-07-28 00:32:12.491431, 2014-07-28 08:13:48.420914]\nLength: 2, Freq: None, Timezone: None\n\nIn [29]: pd.DatetimeIndex(((dti.asi8/(1e9*60)).round()*1e9*60).astype(np.int64))\nOut[29]: \n&lt;class 'pandas.tseries.index.DatetimeIndex'&gt;\n[2014-07-28 00:32:00, 2014-07-28 08:14:00]\nLength: 2, Freq: None, Timezone: None\n"
"import pandas as pd\nimport json\nimport os\n\nos.chdir('/Users/nicolas/Downloads')\n\n# Reading the json as a dict\nwith open('json_example.json') as json_data:\n    data = json.load(json_data)\n\n# using the from_dict load function. Note that the 'orient' parameter \n#is not using the default value (or it will give the same error that you got before)\n# We transpose the resulting df and set index column as its index to get this result\npd.DataFrame.from_dict(data, orient='index').T.set_index('index')   \n\n                                                                 data columns\nindex                                                                        \n311210177061863424  [25-34\\n, FEMALE, @bikewa absolutely the best....     age\n310912785183813632  [25-34\\n, FEMALE, Photo: I love the Burke-Gilm...  gender\n311290293871849472  [25-34\\n, FEMALE, Photo: Inhaled! #fitfoodie h...    text\n309386414548717569  [25-34\\n, FEMALE, Facebook Is Making The Most ...    None\n312327801187495936  [25-34\\n, FEMALE, Still upset about this &amp;gt;&amp;...    None\n312249421079400449  [25-34\\n, FEMALE, @JoeM_PM_UK @JonAntoine I've...    None\n308692673194246145  [25-34\\n, FEMALE, @Social_Freedom_ actually, t...    None\n308995226633129984  [25-34\\n, FEMALE, @seattleweekly that's more t...    None\n308660851219501056  [25-34\\n, FEMALE, @adamholdenbache I noticed 1...    None\n308658690528014337  [25-34\\n, FEMALE, @CEM_Social I am waiting pat...    None\n309719798001070080  [25-34\\n, FEMALE, Going to be watching Faceboo...    None\n312349448049152002  [25-34\\n, FEMALE, @anikamarketer I applied for...    None\n312325152698404864  [25-34\\n, FEMALE, @_chrisrojas_ wow, that's so...    None\n310546490844135425  [25-34\\n, FEMALE, Photo: Feeling like a bit of...    None\n"
"df.loc['A', :] = df_\n\nix_ = pd.MultiIndex.from_product([['A'], ['a', 'b', 'c', 'd']])\ndf_.index = ix_\ndf.loc['A', :] = df_\nprint(df)\n\nA a  0.229970  0.730824  0.784356\n  b  0.584390  0.628337  0.318222\n  c  0.257192  0.624273  0.221279\n  d  0.787023  0.056342  0.240735\nB a       NaN       NaN       NaN\n  b       NaN       NaN       NaN\n  c       NaN       NaN       NaN\n  d       NaN       NaN       NaN\n\ndf.loc['A', :] = df_.values\n\nidx = pd.IndexSlice\ndf.loc[idx[:,('a','b')], :] = df_.values\n\nIn [85]: df\nOut[85]: \n          1st       2nd       3rd\nA a  0.229970  0.730824  0.784356\n  b  0.584390  0.628337  0.318222\n  c       NaN       NaN       NaN\n  d       NaN       NaN       NaN\nB a  0.257192  0.624273  0.221279\n  b  0.787023  0.056342  0.240735\n  c       NaN       NaN       NaN\n  d       NaN       NaN       NaN\n"
"df.sort_index(inplace=True)\nidx = pd.IndexSlice\ndf.loc[idx[:, ('foo','bar'), 'can'], :]\n\n           hi\na b   c      \n1 bar can   3\n  foo can   1\n2 bar can   7\n  foo can   5\n3 bar can  11\n  foo can   9\n\nKeyError: 'MultiIndex Slicing requires the index to be fully lexsorted tuple len (3), lexsort depth (1)'\n\ndf[df.index.get_level_values('b').isin(ix_use.get_level_values(0)) &amp; df.index.get_level_values('c').isin(ix_use.get_level_values(1))]\n\ncond1 = (df.index.get_level_values('b').isin(['foo'])) &amp; (df.index.get_level_values('c').isin(['can']))\ncond2 = (df.index.get_level_values('b').isin(['bar'])) &amp; (df.index.get_level_values('c').isin(['baz']))\ndf[cond1 | cond2]\n\n           hi\na b   c      \n1 foo can   1\n  bar baz   2\n2 foo can   5\n  bar baz   6\n3 foo can   9\n  bar baz  10\n"
"In [59]:\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})\nmask = df.isin([1, 3, 12, 'a'])\ndf = df.where(mask, other=30)\ndf\n\nOut[59]:\n    A   B\n0   1   a\n1  30  30\n2   3  30\n\nIn [2]:    \ndf = pd.DataFrame({'A': [1, 2, 3], 'B': ['a', 'b', 'f']})\nmask = df.isin([1, 3, 12, 'a'])\ndf.where(~mask, other=30)\n\nOut[2]:\n    A   B\n0  30  30\n1   2   b\n2  30   f\n"
"ts[ts==-1] = np.nan\n\nts = ts.resample('5T')\n\nts = ts.interpolate(method='time')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nvalues = [271238, 329285, -1, 260260, 263711]\ntimestamps = pd.to_datetime(['2015-01-04 08:29:05',\n                             '2015-01-04 08:34:05',\n                             '2015-01-04 08:39:05',\n                             '2015-01-04 08:44:05',\n                             '2015-01-04 08:49:05'])\n\nts = pd.Series(values, index=timestamps)\nts[ts==-1] = np.nan\nts = ts.resample('T').mean()\n\nts.interpolate(method='spline', order=3).plot()\nts.interpolate(method='time').plot()\nlines, labels = plt.gca().get_legend_handles_labels()\nlabels = ['spline', 'time']\nplt.legend(lines, labels, loc='best')\nplt.show()\n"
'import pandas as pd\ndef front(self, n):\n    return self.iloc[:, :n]\n\ndef back(self, n):\n    return self.iloc[:, -n:]\n\npd.DataFrame.front = front\npd.DataFrame.back = back\n\ndf = pd.DataFrame(np.random.randint(10, size=(4,10)))\n\nIn [272]: df.front(4)\nOut[272]: \n   0  1  2  3\n0  2  5  2  8\n1  9  9  1  3\n2  7  0  7  4\n3  8  3  9  2\n\nIn [273]: df.back(3)\nOut[273]: \n   7  8  9\n0  3  2  7\n1  9  9  4\n2  5  7  1\n3  3  2  5\n\nIn [274]: df.front(4).back(2)\nOut[274]: \n   2  3\n0  2  8\n1  1  3\n2  7  4\n3  9  2\n\nimport utils_pandas\n'
"df = pd.DataFrame({ 'itemID': np.random.randint(1,4,100) })\n\npd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_')], axis=1).info()\n\nitemID       100 non-null int32\nitemID__1    100 non-null float64\nitemID__2    100 non-null float64\nitemID__3    100 non-null float64\n\nmemory usage: 3.5 KB\n\npd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_').astype(np.int8)], \n                              axis=1).info()\n\nitemID       100 non-null int32\nitemID__1    100 non-null int8\nitemID__2    100 non-null int8\nitemID__3    100 non-null int8\n\nmemory usage: 1.5 KB\n\npd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_',sparse=True)], \n                              axis=1).info()\n\nitemID       100 non-null int32\nitemID__1    100 non-null float64\nitemID__2    100 non-null float64\nitemID__3    100 non-null float64\n\nmemory usage: 2.0 KB\n"
'In [5]:\n\nfirst_idx = df.first_valid_index()\nlast_idx = df.last_valid_index()\nprint(first_idx, last_idx)\ndf.loc[first_idx:last_idx]\n1950 1954\nOut[5]:\n      sum\n1950    5\n1951    3\n1952  NaN\n1953    4\n1954    8\n'
'In [18]: df2.reindex(df.index)\nOut[18]: \n                 D\nArizona     Orange\nNew Mexico   Green\nColorado      Blue\n'
"df.index[-1]\n\nIn [37]:\n\ndf.index[-1]\nOut[37]:\nTimestamp('2015-03-25 00:00:00')\n\nIn [40]:\n\ndf.tail(1).index[0]\nOut[40]:\nTimestamp('2015-03-25 00:00:00')\n"
"df2['FieldName'] = df2['FieldName'].astype(int)\n"
'&gt;&gt;&gt; df2[~df2.text.isin(df1.text.values)]\nC   D   text\n0   0.5 2   shot\n1   0.3 2   shot\n'
'import pandas as pd\nimport io\n\ntemp=u"""Start Date\n1/7/13\n1/7/1\n1/7/13 12 17\n16/7/13\n16/7/13"""\n\ndata = pd.read_csv(io.StringIO(temp), sep=";", parse_dates=False)\n\n#data[\'Start Date\']= pd.to_datetime(data[\'Start Date\'],dayfirst=True)\nprint data\n\n     Start Date\n0        1/7/13\n1         1/7/1\n2  1/7/13 12 17\n3       16/7/13\n4       16/7/13\n\n#check, if length is more as 7\nprint data[data[\'Start Date\'].str.len() &gt; 7]\n\n     Start Date\n2  1/7/13 12 17\n\n#read first 3 rows\ndata= data.iloc[:3]\n\ndata[\'Start Date\']= pd.to_datetime(data[\'Start Date\'],dayfirst=True)\n\ntemp=u"""Start Date\n1/7/13\n1/7/1\n1/7/13 12 17\n16/7/13\n16/7/13 12 04"""\n\ndata = pd.read_csv(io.StringIO(temp), sep=";")\n#add parameter errors coerce\ndata[\'Start Date\']= pd.to_datetime(data[\'Start Date\'], dayfirst=True, errors=\'coerce\')\nprint data\n\n  Start Date\n0 2013-07-01\n1 2001-07-01\n2        NaT\n3 2013-07-16\n4        NaT\n\n#index of data with null - NaT to variable idx\nidx = data[data[\'Start Date\'].isnull()].index\nprint idx\n\nInt64Index([2, 4], dtype=\'int64\')\n\n#read csv again\ndata = pd.read_csv(io.StringIO(temp), sep=";")\n\n#find problematic rows, where datetime is not parsed\nprint data.iloc[idx]\n\n      Start Date\n2   1/7/13 12 17\n4  16/7/13 12 04\n'
'df3 = df3.reset_index()\n\ndf3.reset_index(inplace=True)\n\ndf3[\'new\'] = df3.index\n\ndf = pd.read_csv(\'university2.csv\', \n                 sep=";", \n                 skiprows=1,\n                 index_col=\'YYYY-MO-DD HH-MI-SS_SSS\',\n                 parse_dates=\'YYYY-MO-DD HH-MI-SS_SSS\') #if doesnt work, use pd.to_datetime\n\n#Changing datetime\ndf[\'YYYY-MO-DD HH-MI-SS_SSS\'] = pd.to_datetime(df[\'YYYY-MO-DD HH-MI-SS_SSS\'], \n                                               format=\'%Y-%m-%d %H:%M:%S:%f\')\n#Set index from column\ndf = df.set_index(\'YYYY-MO-DD HH-MI-SS_SSS\')\n'
"df1 = pd.DataFrame({'val':{'a': 1, 'b':2, 'c':3}})\ndf2 = pd.DataFrame({'val':{'a': 1, 'b':2, 'd':3}})\n\ndf1.add(df2, fill_value=0)\n\nidx1 = pd.MultiIndex.from_tuples([('a', 'A'), ('a', 'B'), ('b', 'A'), ('b', 'D')])\nidx2 = pd.MultiIndex.from_tuples([('a', 'A'), ('a', 'C'), ('b', 'A'), ('b', 'C')])\n\nnp.random.seed([3,1415])\ndf1 = pd.DataFrame(np.random.randn(4, 1), idx1, ['val'])\ndf2 = pd.DataFrame(np.random.randn(4, 1), idx2, ['val'])\n\ndf1\n\ndf2\n\ndf1.add(df2, fill_value=0)\n"
"pandas.options.display.float_format = '{:,.2f}'.format\n\n%precision %.2f\n"
"In [30]: df\nOut[30]:\n         Date      Val\n0  2016-09-23      100\n1  2016-09-22    9.60M\n2  2016-09-21   54.20K\n3  2016-09-20  115.30K\n4  2016-09-19   18.90K\n5  2016-09-16  176.10K\n6  2016-09-15   31.60K\n7  2016-09-14   10.00K\n8  2016-09-13    3.20M\n\nIn [31]: df.Val = (df.Val.replace(r'[KM]+$', '', regex=True).astype(float) * \\\n   ....:           df.Val.str.extract(r'[\\d\\.]+([KM]+)', expand=False)\n   ....:             .fillna(1)\n   ....:             .replace(['K','M'], [10**3, 10**6]).astype(int))\n\nIn [32]: df\nOut[32]:\n         Date        Val\n0  2016-09-23      100.0\n1  2016-09-22  9600000.0\n2  2016-09-21    54200.0\n3  2016-09-20   115300.0\n4  2016-09-19    18900.0\n5  2016-09-16   176100.0\n6  2016-09-15    31600.0\n7  2016-09-14    10000.0\n8  2016-09-13  3200000.0\n\nIn [36]: df.Val.replace(r'[KM]+$', '', regex=True).astype(float)\nOut[36]:\n0    100.0\n1      9.6\n2     54.2\n3    115.3\n4     18.9\n5    176.1\n6     31.6\n7     10.0\n8      3.2\nName: Val, dtype: float64\n\nIn [37]: df.Val.str.extract(r'[\\d\\.]+([KM]+)', expand=False)\nOut[37]:\n0    NaN\n1      M\n2      K\n3      K\n4      K\n5      K\n6      K\n7      K\n8      M\nName: Val, dtype: object\n\nIn [38]: df.Val.str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1)\nOut[38]:\n0    1\n1    M\n2    K\n3    K\n4    K\n5    K\n6    K\n7    K\n8    M\nName: Val, dtype: object\n\nIn [39]: df.Val.str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6]).astype(int)\nOut[39]:\n0          1\n1    1000000\n2       1000\n3       1000\n4       1000\n5       1000\n6       1000\n7       1000\n8    1000000\nName: Val, dtype: int32\n"
'df_ret = pd.read_csv(filepath, index_col=False, usecols=cols_to_use)[cols_to_use]\n'
"x_prev = x.shift(1)\nx_next = x.shift(-1)\n((x_prev != 1) &amp; (x == 1) &amp; (x_next == 1))\n\n((x == 1) &amp; (x_next == 0) &amp; (x_next2 == 0))\n\nimport numpy as np\nimport pandas as pd\n\nx = pd.Series([0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0 , 0 , 1])\nx_prev = x.shift(1)\nx_next = x.shift(-1)\nx_next2 = x.shift(-2)\ndf = pd.DataFrame(\n    dict(start = np.flatnonzero((x_prev != 1) &amp; (x == 1) &amp; (x_next == 1)),\n         end = np.flatnonzero((x == 1) &amp; (x_next == 0) &amp; (x_next2 == 0))))\nprint(df[['start', 'end']])\n\n   start  end\n0      3    5\n1      8   11\n"
"reviews[reviews['stars'] &gt; 3].groupby('business_id')['stars'].count()\n"
'title = ax.set_title(...) \nbb = title.get_bbox_patch() \n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib.path import Path\nfrom matplotlib.patches import BoxStyle\n\n\nclass ExtendedTextBox(BoxStyle._Base):\n    """\n    An Extended Text Box that expands to the axes limits \n                        if set in the middle of the axes\n    """\n\n    def __init__(self, pad=0.3, width=500.):\n        """\n        width: \n            width of the textbox. \n            Use `ax.get_window_extent().width` \n                   to get the width of the axes.\n        pad: \n            amount of padding (in vertical direction only)\n        """\n        self.width=width\n        self.pad = pad\n        super(ExtendedTextBox, self).__init__()\n\n    def transmute(self, x0, y0, width, height, mutation_size):\n        """\n        x0 and y0 are the lower left corner of original text box\n        They are set automatically by matplotlib\n        """\n        # padding\n        pad = mutation_size * self.pad\n\n        # we add the padding only to the box height\n        height = height + 2.*pad\n        # boundary of the padded box\n        y0 = y0 - pad\n        y1 = y0 + height\n        _x0 = x0\n        x0 = _x0 +width /2. - self.width/2.\n        x1 = _x0 +width /2. + self.width/2.\n\n        cp = [(x0, y0),\n              (x1, y0), (x1, y1), (x0, y1),\n              (x0, y0)]\n\n        com = [Path.MOVETO,\n               Path.LINETO, Path.LINETO, Path.LINETO,\n               Path.CLOSEPOLY]\n\n        path = Path(cp, com)\n\n        return path\n\ndpi = 80\n\n# register the custom style\nBoxStyle._style_list["ext"] = ExtendedTextBox\n\nplt.figure(dpi=dpi)\ns = pd.Series(np.random.lognormal(.001, .01, 100))\nax = s.cumprod().plot()\n# set the title position to the horizontal center (0.5) of the axes\ntitle = ax.set_title(\'My Log Normal Example\', position=(.5, 1.02), \n             backgroundcolor=\'black\', color=\'white\')\n# set the box style of the title text box toour custom box\nbb = title.get_bbox_patch()\n# use the axes\' width as width of the text box\nbb.set_boxstyle("ext", pad=0.4, width=ax.get_window_extent().width )\n\n\n# Optionally: use eventhandler to resize the title box, in case the window is resized\ndef on_resize(event):\n    print "resize"\n    bb.set_boxstyle("ext", pad=0.4, width=ax.get_window_extent().width )\n\ncid = plt.gcf().canvas.mpl_connect(\'resize_event\', on_resize)\n\n# use the same dpi for saving to file as for plotting on screen\nplt.savefig(__file__+".png", dpi=dpi)\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\ns = pd.Series(np.random.lognormal(.001, .01, 100))\nax = s.cumprod().plot()\ntitle = ax.set_title(\'My Log Normal Example\', position=(.5, 1.02),\n             backgroundcolor=\'black\', color=\'white\',\n             verticalalignment="bottom", horizontalalignment="center")\ntitle._bbox_patch._mutation_aspect = 0.04\ntitle.get_bbox_patch().set_boxstyle("square", pad=11.9)\nplt.tight_layout()\nplt.savefig(__file__+".png")\nplt.show()\n'
'&gt;&gt;&gt; dft.rolling(2, min_periods=1).sum()\n                       B\n2013-01-01 09:00:00  0.0\n2013-01-01 09:00:01  1.0\n2013-01-01 09:00:02  3.0\n2013-01-01 09:00:03  2.0\n2013-01-01 09:00:04  4.0\n'
'%matplotlib inline\n\nplt.show()\n'
"df.assign(\n    timediff=df.sort_values(\n        'datetime', ascending=False\n    ).groupby(['from', 'to']).datetime.diff(-1).dt.seconds.div(60).fillna(0))\n"
"pd.concat([pd.DataFrame({'Start': pd.date_range(row.Start, row.End, freq='W-SAT'),\n               'Note': row.Note,\n               'Item': row.Item}, columns=['Start', 'Note', 'Item']) \n           for i, row in df.iterrows()], ignore_index=True)\n\n       Start Note Item\n0 2016-10-22    Z    A\n1 2016-10-29    Z    A\n2 2016-11-05    Z    A\n3 2017-02-11    W    B\n4 2017-02-18    W    B\n5 2017-02-25    W    B\n"
"df = pd.DataFrame({'col1':pd.date_range('2015-01-02 15:00:07', periods=3),\n                   'col2':pd.date_range('2015-05-02 15:00:07', periods=3),\n                   'col3':pd.date_range('2015-04-02 15:00:07', periods=3),\n                   'col4':pd.date_range('2015-09-02 15:00:07', periods=3),\n                   'col5':[5,3,6],\n                   'col6':[7,4,3]})\n\nprint (df)\n                 col1                col2                col3  \\\n0 2015-01-02 15:00:07 2015-05-02 15:00:07 2015-04-02 15:00:07   \n1 2015-01-03 15:00:07 2015-05-03 15:00:07 2015-04-03 15:00:07   \n2 2015-01-04 15:00:07 2015-05-04 15:00:07 2015-04-04 15:00:07   \n\n                 col4  col5  col6  \n0 2015-09-02 15:00:07     5     7  \n1 2015-09-03 15:00:07     3     4  \n2 2015-09-04 15:00:07     6     3  \n\nlist_of_cols_to_change = ['col1','col2','col3','col4']\ndf[list_of_cols_to_change] = df[list_of_cols_to_change].apply(lambda x: x.dt.date)\nprint (df)\n         col1        col2        col3        col4  col5  col6\n0  2015-01-02  2015-05-02  2015-04-02  2015-09-02     5     7\n1  2015-01-03  2015-05-03  2015-04-03  2015-09-03     3     4\n2  2015-01-04  2015-05-04  2015-04-04  2015-09-04     6     3\n"
"pd.Series(d).reset_index()\nOut: \n  level_0 level_1  0\n0   first     row  3\n1  second     row  1\n\ndf = pd.Series(d).reset_index()   \ndf.columns = ['Col1', 'Col2', 'Col3']   \ndf\nOut: \n     Col1 Col2  Col3\n0   first  row     3\n1  second  row     1\n\npd.Series(d).rename_axis(['Col1', 'Col2']).reset_index(name='Col3')\nOut[7]: \n     Col1 Col2  Col3\n0   first  row     3\n1  second  row     1\n"
"In [19]: pd.to_datetime(df.Year.astype(str), format='%Y') + \\\n             pd.to_timedelta(df.Week.mul(7).astype(str) + ' days')\nOut[19]:\n0   2016-10-28\n1   2016-11-04\n2   2016-12-23\n3   2017-01-15\n4   2017-02-05\n5   2017-03-26\ndtype: datetime64[ns]\n\ndf['Date'] = pd.to_datetime(df['UNIX_Time'], unit='s')\n\nIn [26]: df = pd.DataFrame(pd.date_range('1970-01-01', freq='1T', periods=10**7), columns=['date'])\n\nIn [27]: df.shape\nOut[27]: (10000000, 1)\n\nIn [28]: df['unix_ts'] = df['date'].astype(np.int64)//10**9\n\nIn [30]: df\nOut[30]:\n                       date    unix_ts\n0       1970-01-01 00:00:00          0\n1       1970-01-01 00:01:00         60\n2       1970-01-01 00:02:00        120\n3       1970-01-01 00:03:00        180\n4       1970-01-01 00:04:00        240\n5       1970-01-01 00:05:00        300\n6       1970-01-01 00:06:00        360\n7       1970-01-01 00:07:00        420\n8       1970-01-01 00:08:00        480\n9       1970-01-01 00:09:00        540\n...                     ...        ...\n9999990 1989-01-05 10:30:00  599999400\n9999991 1989-01-05 10:31:00  599999460\n9999992 1989-01-05 10:32:00  599999520\n9999993 1989-01-05 10:33:00  599999580\n9999994 1989-01-05 10:34:00  599999640\n9999995 1989-01-05 10:35:00  599999700\n9999996 1989-01-05 10:36:00  599999760\n9999997 1989-01-05 10:37:00  599999820\n9999998 1989-01-05 10:38:00  599999880\n9999999 1989-01-05 10:39:00  599999940\n\n[10000000 rows x 2 columns]\n\nIn [31]: pd.to_datetime(df.unix_ts, unit='s')\nOut[31]:\n0         1970-01-01 00:00:00\n1         1970-01-01 00:01:00\n2         1970-01-01 00:02:00\n3         1970-01-01 00:03:00\n4         1970-01-01 00:04:00\n5         1970-01-01 00:05:00\n6         1970-01-01 00:06:00\n7         1970-01-01 00:07:00\n8         1970-01-01 00:08:00\n9         1970-01-01 00:09:00\n                  ...\n9999990   1989-01-05 10:30:00\n9999991   1989-01-05 10:31:00\n9999992   1989-01-05 10:32:00\n9999993   1989-01-05 10:33:00\n9999994   1989-01-05 10:34:00\n9999995   1989-01-05 10:35:00\n9999996   1989-01-05 10:36:00\n9999997   1989-01-05 10:37:00\n9999998   1989-01-05 10:38:00\n9999999   1989-01-05 10:39:00\nName: unix_ts, Length: 10000000, dtype: datetime64[ns]\n\nIn [32]: %timeit pd.to_datetime(df.unix_ts, unit='s')\n10 loops, best of 3: 156 ms per loop\n"
"td = {'q1':(111,222), 'q2':(333,444)}\n\npd.DataFrame(td).T.rename_axis('Query').add_prefix('Value').reset_index()\n\n  Query  Value0  Value1\n0    q1     111     222\n1    q2     333     444\n\nfrom cytoolz.dicttoolz import merge\n\npd.DataFrame(\n    [merge(\n        {'Query': k},\n        {'Value{}'.format(i): x for i, x in enumerate(v, 1)}\n     ) for k, v in td.items()]\n)\n\n  Query  Value1  Value2\n0    q1     111     222\n1    q2     333     444\n\ndf = pd.DataFrame(td).T.rename_axis('Query').add_prefix('Value')\ndf.assign(PctChg=df.pct_change(axis=1).iloc[:, -1]).reset_index()\n\n  Query  Value0  Value1    PctChg\n0    q1     111     222  1.000000\n1    q2     333     444  0.333333\n\ndf = pd.DataFrame(td).T.rename_axis('Query').add_prefix('Value')\ndf.eval('PctChg = Value1 / Value0 - 1', inplace=False).reset_index()\n\n  Query  Value0  Value1    PctChg\n0    q1     111     222  1.000000\n1    q2     333     444  0.333333\n"
'from datalab.context import Context\nimport datalab.storage as storage\nimport datalab.bigquery as bq\nimport pandas as pd\nfrom pandas import DataFrame\nimport time\n\n# Dataframe to write\nmy_data = [{1,2,3}]\nfor i in range(0,100000):\n    my_data.append({1,2,3})\nnot_so_simple_dataframe = pd.DataFrame(data=my_data,columns=[\'a\',\'b\',\'c\'])\n\n#Alternative 1\nstart = time.time()\nnot_so_simple_dataframe.to_gbq(\'TestDataSet.TestTable\', \n                 Context.default().project_id,\n                 chunksize=10000, \n                 if_exists=\'append\',\n                 verbose=False\n                 )\nend = time.time()\nprint("time alternative 1 " + str(end - start))\n\n#Alternative 3\nstart = time.time()\nsample_bucket_name = Context.default().project_id + \'-datalab-example\'\nsample_bucket_path = \'gs://\' + sample_bucket_name\nsample_bucket_object = sample_bucket_path + \'/Hello.txt\'\nbigquery_dataset_name = \'TestDataSet\'\nbigquery_table_name = \'TestTable\'\n\n# Define storage bucket\nsample_bucket = storage.Bucket(sample_bucket_name)\n\n# Create or overwrite the existing table if it exists\ntable_schema = bq.Schema.from_dataframe(not_so_simple_dataframe)\n\n# Write the DataFrame to GCS (Google Cloud Storage)\n%storage write --variable not_so_simple_dataframe --object $sample_bucket_object\n\n# Write the DataFrame to a BigQuery table\ntable.insert_data(not_so_simple_dataframe)\nend = time.time()\nprint("time alternative 3 " + str(end - start))\n\nn       alternative_1  alternative_3\n10000   30.72s         8.14s\n100000  162.43s        70.64s\n1000000 1473.57s       688.59s\n'
"df = df[df['my_col'].notnull()].copy()\ndf['my_col'] = df['my_col'].astype(int).astype(str)\n"
'import statsmodels.api as sm\n\nmodel = sm.OLS(y,x)\nresults = model.fit()\nresults_summary = results.summary()\n\n# Note that tables is a list. The table at index 1 is the "core" table. Additionally, read_html puts dfs in a list, so we want index 0\nresults_as_html = results_summary.tables[1].as_html()\npd.read_html(results_as_html, header=0, index_col=0)[0]\n'
'&gt;&gt;&gt; df\n   a  b\n0  5  5\n1  0  7\n2  1  0\n3  0  4\n4  6  4\n\n&gt;&gt;&gt; mat\narray([[0.44926098, 0.29567859, 0.60728561],\n       [0.32180566, 0.32499134, 0.94950085],\n       [0.64958125, 0.00566706, 0.56473627],\n       [0.17357589, 0.71053224, 0.17854188],\n       [0.38348102, 0.12440952, 0.90359566]])\n\n&gt;&gt;&gt; pd.concat([df, pd.DataFrame(mat)], axis=1)\n   a  b         0         1         2\n0  5  5  0.449261  0.295679  0.607286\n1  0  7  0.321806  0.324991  0.949501\n2  1  0  0.649581  0.005667  0.564736\n3  0  4  0.173576  0.710532  0.178542\n4  6  4  0.383481  0.124410  0.903596\n'
'def apply_do_g(it_row):\n    """\n    This is your function, but using isin and apply\n    """\n\n    keep = {\'Operator\': [it_row.Operator], \'Terminal\': [it_row.Terminal]}  # dict for isin combined mask\n\n    holder1 = arr[list(keep)].isin(keep).all(axis=1)  # create boolean mask\n    holder2 = arr.Already_linked.isin([0])  # create boolean mask\n    holder3 = arr.hour &lt; it_row.hour_aux  # create boolean mask\n\n    holder = holder1 &amp; holder2 &amp; holder3  # combine the masks\n\n    holder = arr.loc[holder]\n\n    if not holder.empty:\n\n        aux = np.absolute(holder.START - it_row.x).idxmin()\n\n        c.loc[it_row.name, \'a\'] = holder.loc[aux].FlightID  # use with apply \'it_row.name\'\n\n        arr.loc[aux, \'Already_linked\'] = 1\n\n\ndef new_way_2():\n    keep = {\'A_D\': [\'D\'], \'Already_linked\': [0]}\n    df_test = c[c[list(keep)].isin(keep).all(axis=1)].copy()  # returns the resultant df\n    df_test.apply(lambda row: apply_do_g(row), axis=1)  # g is multiple DataFrames"\n\n\n#call the function\nnew_way_2()\n'
"n = 3\ndf['complete'] = df.Person.apply(lambda x: 1 if df.Person.tolist().count(x) == n else 0)\ndf['num'] = df.Person.str.replace('Person ','')\ndf.sort_values(by=['num','complete'],ascending=True,inplace=True) #get all persons that are complete to the top\n\nc = 0\nperson_numbers = []\nfor x in range(0,999): #Create the numbering [1,1,1,2,2,2,3,3,3,...] with n defining how often a person is 'repeated'\n    if x % n == 0:\n        c += 1        \n    person_numbers.append(c) \n\ndf['Person_new'] = person_numbers[0:len(df)] #Add the numbering to the df\ndf.Person = 'Person ' + df.Person_new.astype(str) #Fill the person column with the new numbering\ndf.drop(['complete','Person_new','num'],axis=1,inplace=True)\n"
'Filter_df  = df[df.index.isin(my_list)]\n'
"pd.to_datetime(1.547559e+09, unit='s', origin='unix') \n# Timestamp('2019-01-15 13:30:00')\n\npd.to_datetime(['2019-01-15 13:30:00']).astype(int) / 10**9\n# Float64Index([1547559000.0], dtype='float64')\n\n# create test data\ndates = pd.to_datetime(['2019-01-15 13:30:00'])\n\n# calculate unix datetime\n(dates - pd.Timestamp(&quot;1970-01-01&quot;)) // pd.Timedelta('1s')\n\n[out]:\nInt64Index([1547559000], dtype='int64')\n"
'vv = df.iloc[:, 1::2].values\niRow, iCol = np.unravel_index(vv.argmax(), vv.shape)\niCol = iCol * 2 + 1\nresult = df.iloc[iRow, [0, iCol, iCol + 1]]\n\nSequence     1008\nDuration3     981\nValue3         82\nName: 7, dtype: int64\n\npd.DataFrame([result.values], columns=result.index)\n'
"i, j = np.where((a.values[:,None] == b.values[:,:,None]).all(axis=0))\ndict(zip(a.columns[j], b.columns[i]))\n# {'a7': 'b2', 'a6': 'b3', 'a4': 'b4', 'a2': 'b7'}\n"
"def f(group):\n    row = group.irow(0)\n    return DataFrame({'class': [row['class']] * row['count']})\ndf.groupby('class', group_keys=False).apply(f)\n\nIn [25]: df.groupby('class', group_keys=False).apply(f)\nOut[25]: \n  class\n0     A\n0     C\n1     C\n"
'In [10]: np.abs(df.time - image_time)\nOut[10]: \n0    27 days, 13:39:02\n1    26 days, 13:39:02\n2    25 days, 13:39:02\n3    24 days, 13:39:02\n4    23 days, 13:39:02\n5    22 days, 13:39:02\n'
"df = pd.DataFrame({'ID': ('STRSUB BOTDWG'.split())*4,\n                   'Days Late': [60, 60, 50, 50, 20, 20, 10, 10],\n                   'quantity': [56, 20, 60, 67, 74, 87, 40, 34]})\n#    Days Late      ID  quantity\n# 0         60  STRSUB        56\n# 1         60  BOTDWG        20\n# 2         50  STRSUB        60\n# 3         50  BOTDWG        67\n# 4         20  STRSUB        74\n# 5         20  BOTDWG        87\n# 6         10  STRSUB        40\n# 7         10  BOTDWG        34\n\ndf['status'] = pd.cut(df['Days Late'], bins=[-1, 14, 35, 56, 365], labels=False)\nlabels = np.array('White Yellow Amber Red'.split())\ndf['status'] = labels[df['status']]\ndel df['Days Late']\nprint(df)\n#        ID  quantity  status\n# 0  STRSUB        56     Red\n# 1  BOTDWG        20     Red\n# 2  STRSUB        60   Amber\n# 3  BOTDWG        67   Amber\n# 4  STRSUB        74  Yellow\n# 5  BOTDWG        87  Yellow\n# 6  STRSUB        40   White\n# 7  BOTDWG        34   White\n\ndf = df.pivot(index='ID', columns='status', values='quantity')\n\ndf = df.reindex(columns=labels[::-1], index=df.index[::-1])\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'ID': ('STRSUB BOTDWG'.split())*4,\n                   'Days Late': [60, 60, 50, 50, 20, 20, 10, 10],\n                   'quantity': [56, 20, 60, 67, 74, 87, 40, 34]})\ndf['status'] = pd.cut(df['Days Late'], bins=[-1, 14, 35, 56, 365], labels=False)\nlabels = np.array('White Yellow Amber Red'.split())\ndf['status'] = labels[df['status']]\ndel df['Days Late']\ndf = df.pivot(index='ID', columns='status', values='quantity')\ndf = df.reindex(columns=labels[::-1], index=df.index[::-1])\nprint(df)\n\n        Red  Amber  Yellow  White\nID                               \nSTRSUB   56     60      74     40\nBOTDWG   20     67      87     34\n"
"In [13]: df[df &gt; df.quantile(0.8)].dropna()\nOut[13]: \n       data\nc  0.860467\nj  1.887577\n\nIn [14]: list(df[df &gt; df.quantile(0.8)].dropna().index)\nOut[14]: ['c', 'j']\n"
"In [8]: df\nOut[8]: \n   one  two  three\nA    0    1      2\nB    3    4      5\n\nIn [10]: df.columns = [['odd','even','odd'],df.columns]\n\nIn [11]: df\nOut[11]: \n   odd  even    odd\n   one   two  three\nA    0     1      2\nB    3     4      5\n"
"In [6]: s = Series(np.random.rand(10))\n\nIn [7]: s\nOut[7]: \n0    0.302041\n1    0.353838\n2    0.421416\n3    0.174497\n4    0.600932\n5    0.871461\n6    0.116874\n7    0.233738\n8    0.859147\n9    0.145515\ndtype: float64\n\nIn [8]: s.describe()\nOut[8]: \ncount    10.000000\nmean      0.407946\nstd       0.280562\nmin       0.116874\n25%       0.189307\n50%       0.327940\n75%       0.556053\nmax       0.871461\ndtype: float64\n\nIn [9]: s.describe()[['count','mean']]\nOut[9]: \ncount    10.000000\nmean      0.407946\ndtype: float64\n"
"n = 10\n\nax = locks.plot(kind='bar', y='SUM')\nticks = ax.xaxis.get_ticklocs()\nticklabels = [l.get_text() for l in ax.xaxis.get_ticklabels()]\nax.xaxis.set_ticks(ticks[::n])\nax.xaxis.set_ticklabels(ticklabels[::n])\n\nax.figure.show()\n"
"df['date_int'] = df.date.astype(np.int64)\n\ncolor_d = {1: 'k', 2: 'b', 3: 'r'}\n\ntraining.plot(kind='scatter',x='date',y='rate', color=df.account.map(color_d))\n"
"df['colname'] &gt; somenumberIchoose\n\ndf[df['colname'] &gt; somenumberIchoose]\n"
"In [51]: import numpy as np\n\nIn [52]: import pandas as pd\n\nIn [53]: investment_df = pd.DataFrame(np.arange(10), columns=['investment'])\n\nIn [54]: investment_df['decile'] = pd.qcut(investment_df['investment'], 10, labels=False)\n\nIn [55]: investment_df['quintile'] = pd.qcut(investment_df['investment'], 5, labels=False)\n\nIn [56]: investment_df\nOut[56]: \n   investment  decile  quintile\n0           0       0         0\n1           1       1         0\n2           2       2         1\n3           3       3         1\n4           4       4         2\n5           5       5         2\n6           6       6         3\n7           7       7         3\n8           8       8         4\n9           9       9         4   \n\nIn [60]: investment_df['quintile'] = pd.qcut(investment_df['investment'], 5, labels=np.arange(5, 0, -1))\n\nIn [61]: investment_df['decile'] = pd.qcut(investment_df['investment'], 10, labels=np.arange(10, 0, -1))\n\nIn [62]: investment_df\nOut[62]: \n   investment decile quintile\n0           0     10        5\n1           1      9        5\n2           2      8        4\n3           3      7        4\n4           4      6        3\n5           5      5        3\n6           6      4        2\n7           7      3        2\n8           8      2        1\n9           9      1        1\n"
"df['value_grp'] = (df.Values.diff(1) != 0).astype('int').cumsum()\n\npd.DataFrame({'BeginDate' : df.groupby('value_grp').Date.first(), \n              'EndDate' : df.groupby('value_grp').Date.last(),\n              'Consecutive' : df.groupby('value_grp').size(), \n              'No' : df.groupby('value_grp').No.first()}).reset_index(drop=True)\n"
'In [13]:\n\ndff.dropna(thresh=len(dff) - 2, axis=1)\nOut[13]:\n          A         B\n0  0.517199 -0.806304\n1 -0.643074  0.229602\n2  0.656728  0.535155\n3       NaN -0.162345\n4 -0.309663 -0.783539\n5  1.244725 -0.274514\n6 -0.254232       NaN\n7 -1.242430  0.228660\n8 -0.311874 -0.448886\n9 -0.984453 -0.755416\n'
"df['Age_fill'][(df.Age.isnull()) &amp; (df.Gender == i) &amp; (df.Pclass == j+1)] \\\n                                                          = median_ages[i,j]\n"
'one_to_hundred = pd.Series(range(1,101))\n'
"data.iloc[499:999].plot(y='value')\n\nIn [35]:\ndf = pd.DataFrame(np.random.randn(10,2), columns=list('ab'))\ndf.iloc[2:6]\n\nOut[35]:\n          a         b\n2  0.672884  0.202798\n3  0.514998  1.744821\n4 -1.982109 -0.770861\n5  1.364567  0.341882\n\ndf.iloc[2:6].plot(y='b')\n"
'In [96]: result\nOut[96]: \nMutProb               0.001      0.005      0.010     0.050     0.100\nSymmetricDivision                                                    \n0.2               -6.146121  -8.571063  -9.784686 -6.051482 -0.964818\n0.4               -6.473629  -8.936463  -9.455776 -6.885229 -0.652147\n0.6               -6.760559  -9.292469  -9.551801 -6.621639 -0.392256\n0.8               -7.196407  -9.544065 -10.536340 -6.996394 -0.722602\n1.0               -8.027475 -10.502450 -11.408114 -9.175349 -4.180864\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame({\'MutProb\': [0.1,\n  0.05, 0.01, 0.005, 0.001, 0.1, 0.05, 0.01, 0.005, 0.001, 0.1, 0.05, 0.01, 0.005, 0.001, 0.1, 0.05, 0.01, 0.005, 0.001, 0.1, 0.05, 0.01, 0.005, 0.001], \'SymmetricDivision\': [1.0, 1.0, 1.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.8, 0.8, 0.6, 0.6, 0.6, 0.6, 0.6, 0.4, 0.4, 0.4, 0.4, 0.4, 0.2, 0.2, 0.2, 0.2, 0.2], \'test\': [\'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\', \'sackin_yule\'], \'value\': [-4.1808639999999997, -9.1753490000000006, -11.408113999999999, -10.50245, -8.0274750000000008, -0.72260200000000008, -6.9963940000000004, -10.536339999999999, -9.5440649999999998, -7.1964070000000007, -0.39225599999999999, -6.6216390000000001, -9.5518009999999993, -9.2924690000000005, -6.7605589999999998, -0.65214700000000003, -6.8852289999999989, -9.4557760000000002, -8.9364629999999998, -6.4736289999999999, -0.96481800000000006, -6.051482, -9.7846860000000007, -8.5710630000000005, -6.1461209999999999]})\nresult = df.pivot(index=\'SymmetricDivision\', columns=\'MutProb\', values=\'value\')\nsns.heatmap(result, annot=True, fmt="g", cmap=\'viridis\')\nplt.show()\n'
'header = True\nfor chunk in chunks:\n\n    chunk.to_csv(os.path.join(folder, new_folder, "new_file_" + filename),\n        header=header, cols=[[\'TIME\',\'STUFF\']], mode=\'a\')\n\n    header = False\n'
"pd.set_option('expand_frame_repr', False)\n\n#temporaly set expand_frame_repr\nwith pd.option_context('expand_frame_repr', False):\n    print (df)\n"
'# vectorized haversine function\ndef haversine(lat1, lon1, lat2, lon2, to_radians=True, earth_radius=6371):\n    """\n    slightly modified version: of http://stackoverflow.com/a/29546836/2901002\n\n    Calculate the great circle distance between two points\n    on the earth (specified in decimal degrees or in radians)\n\n    All (lat, lon) coordinates must have numeric dtypes and be of equal length.\n\n    """\n    if to_radians:\n        lat1, lon1, lat2, lon2 = np.radians([lat1, lon1, lat2, lon2])\n\n    a = np.sin((lat2-lat1)/2.0)**2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin((lon2-lon1)/2.0)**2\n\n    return earth_radius * 2 * np.arcsin(np.sqrt(a))\n\n\ndf[\'dist\'] = \\\n    haversine(df.LAT.shift(), df.LONG.shift(),\n                 df.loc[1:, \'LAT\'], df.loc[1:, \'LONG\'])\n\nIn [566]: df\nOut[566]:\n   Ser_Numb        LAT       LONG         dist\n0         1  74.166061  30.512811          NaN\n1         2  72.249672  33.427724   232.549785\n2         3  67.499828  37.937264   554.905446\n3         4  84.253715  69.328767  1981.896491\n4         5  72.104828  33.823462  1513.397997\n5         6  63.989462  51.918173  1164.481327\n6         7  80.209112  33.530778  1887.256899\n7         8  68.954132  35.981256  1252.531365\n8         9  83.378214  40.619652  1606.340727\n9        10  68.778571   6.607066  1793.921854\n\nIn [573]: pd.concat([df[\'LAT\'].shift(), df.loc[1:, \'LAT\']], axis=1, ignore_index=True)\nOut[573]:\n           0          1\n0        NaN        NaN\n1  74.166061  72.249672\n2  72.249672  67.499828\n3  67.499828  84.253715\n4  84.253715  72.104828\n5  72.104828  63.989462\n6  63.989462  80.209112\n7  80.209112  68.954132\n8  68.954132  83.378214\n9  83.378214  68.778571\n'
"df['b'] = df['a'].shift(1, fill_value=0)\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import datasets\n\niris_dataset = datasets.load_iris()\nX = iris_dataset.data\nY = iris_dataset.target\n\niris_dataframe = pd.DataFrame(X, columns=iris_dataset.feature_names)\n\n# Create a scatter matrix from the dataframe, color by y_train\ngrr = pd.plotting.scatter_matrix(iris_dataframe, c=Y, figsize=(15, 15), marker='o',\n                                 hist_kwds={'bins': 20}, s=60, alpha=.8)\n\ngrr = pd.scatter_matrix(iris_dataframe, c=Y, figsize=(15, 15), marker='o',\n                        hist_kwds={'bins': 20}, s=60, alpha=.8)\n\nplt.savefig('foo.png')\n\n# %matplotlib inline\n"
"df.columns = df.columns.map(''.join)\n\ndf.columns = df.columns.droplevel(0)\n\ndf = df.xs('CID', axis=1, level=1)\n\ndf = df.groupby(by=['CID','FE'])['FID'].count().unstack().reset_index()\n\ndf = pd.DataFrame({'CID':[2,2,3],\n                   'FE':[5,5,6],\n                   'FID':[1,7,9]})\n\nprint (df)\n   CID  FE  FID\n0    2   5    1\n1    2   5    7\n2    3   6    9\n\ndf = df.groupby(by=['CID','FE'])['FID']\n       .count()\n       .unstack()\n       .reset_index()\n       .rename_axis(None, axis=1)\n\nprint (df)    \n   CID    5    6\n0    2  2.0  NaN\n1    3  NaN  1.0\n"
"class PandasModel(QtCore.QAbstractTableModel): \n    def __init__(self, df = pd.DataFrame(), parent=None): \n        QtCore.QAbstractTableModel.__init__(self, parent=parent)\n        self._df = df.copy()\n\n    def toDataFrame(self):\n        return self._df.copy()\n\n    def headerData(self, section, orientation, role=QtCore.Qt.DisplayRole):\n        if role != QtCore.Qt.DisplayRole:\n            return QtCore.QVariant()\n\n        if orientation == QtCore.Qt.Horizontal:\n            try:\n                return self._df.columns.tolist()[section]\n            except (IndexError, ):\n                return QtCore.QVariant()\n        elif orientation == QtCore.Qt.Vertical:\n            try:\n                # return self.df.index.tolist()\n                return self._df.index.tolist()[section]\n            except (IndexError, ):\n                return QtCore.QVariant()\n\n    def data(self, index, role=QtCore.Qt.DisplayRole):\n        if role != QtCore.Qt.DisplayRole:\n            return QtCore.QVariant()\n\n        if not index.isValid():\n            return QtCore.QVariant()\n\n        return QtCore.QVariant(str(self._df.ix[index.row(), index.column()]))\n\n    def setData(self, index, value, role):\n        row = self._df.index[index.row()]\n        col = self._df.columns[index.column()]\n        if hasattr(value, 'toPyObject'):\n            # PyQt4 gets a QVariant\n            value = value.toPyObject()\n        else:\n            # PySide gets an unicode\n            dtype = self._df[col].dtype\n            if dtype != object:\n                value = None if value == '' else dtype.type(value)\n        self._df.set_value(row, col, value)\n        return True\n\n    def rowCount(self, parent=QtCore.QModelIndex()): \n        return len(self._df.index)\n\n    def columnCount(self, parent=QtCore.QModelIndex()): \n        return len(self._df.columns)\n\n    def sort(self, column, order):\n        colname = self._df.columns.tolist()[column]\n        self.layoutAboutToBeChanged.emit()\n        self._df.sort_values(colname, ascending= order == QtCore.Qt.AscendingOrder, inplace=True)\n        self._df.reset_index(inplace=True, drop=True)\n        self.layoutChanged.emit()\n\ndef btn_clk(self):\n    path = self.lineEdit.text()\n    df = pd.read_csv(path)\n    model = PandasModel(df)\n    self.tableView.setModel(model)\n"
"for col in df.columns():\n   df.loc[df[col] == 'n', col] = 0\n   df.loc[df[col] == 'y', col] = 1\n   df.loc[df[col] == '?', col] = 1\n"
"mybins=np.logspace(0, np.log(100), 100)\n\ng = sns.JointGrid(data1, data2, data, xlim=[.5, 1000000],\n                  ylim=[.1, 10000000])\ng.plot_marginals(sns.distplot, color='blue', bins=mybins)\ng = g.plot(sns.regplot, sns.distplot)\ng = g.annotate(stats.pearsonr)\n\nax = g.ax_joint\nax.set_xscale('log')\nax.set_yscale('log')\n\ng.ax_marg_x.set_xscale('log')\ng.ax_marg_y.set_yscale('log')\n"
"def sephist(col):\n    yes = df[df['group'] == 'yes'][col]\n    no = df[df['group'] == 'no'][col]\n    return yes, no\n\nfor num, alpha in enumerate('abcd'):\n    plt.subplot(2, 2, num)\n    plt.hist(sephist(alpha)[0], bins=25, alpha=0.5, label='yes', color='b')\n    plt.hist(sephist(alpha)[1], bins=25, alpha=0.5, label='no', color='r')\n    plt.legend(loc='upper right')\n    plt.title(alpha)\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n\nfor num, alpha in enumerate('abcd'):\n    plt.subplot(2, 2, num)\n    plt.hist((sephist(alpha)[0], sephist(alpha)[1]), bins=25, alpha=0.5, label=['yes', 'no'], color=['r', 'b'])\n    plt.legend(loc='upper right')\n    plt.title(alpha)\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n"
's[s.apply(lambda x: isinstance(x, str))]\n\ns[s.apply(isinstance, args=(str,))]\n\ns[[isinstance(x, str) for x in s]]\n\n0    foo\n1    bar\ndtype: object\n\ns[s.apply(type) == str]\n'
"a = pd.Series([item for sublist in din.x for item in sublist])\n\na = pd.Series(np.concatenate(din.x))\n\ndf = a.value_counts().sort_index().rename_axis('x').reset_index(name='f')\n\ndf = a.groupby(a).size().rename_axis('x').reset_index(name='f')\n\nfrom collections import Counter\nfrom  itertools import chain\n\ndf = pd.Series(Counter(chain(*din.x))).sort_index().rename_axis('x').reset_index(name='f')\n\nprint (df)\n   x  f\n0  a  2\n1  b  1\n2  c  2\n3  d  1\n4  e  1\n"
'elif is_list_like(arg) and arg not in compat.string_types:\n\nresults.append(colg.aggregate(a))\n\nif not len(results):\n    raise ValueError("no results")\n\nresult[name] = self._try_cast(func(data, *args, **kwargs)\n\n(Pdb) n\n&gt; c:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\groupby.py(3779)_aggregate_generic()\n-&gt; return self._wrap_generic_output(result, obj)\n\n(Pdb) result\n{1: {\'user_id\', \'instructor\', \'class_type\'}, 2: {\'user_id\', \'instructor\', \'class_type\'}, 3: {\'user_id\', \'instructor\', \'class_type\'}, 4: {\'user_id\', \'instructor\', \'class_type\'}}\n\nIn [8]:\n\ndf.groupby(\'user_id\').agg(lambda x: print(set(x.columns)))\n{\'class_type\', \'instructor\', \'user_id\'}\n{\'class_type\', \'instructor\', \'user_id\'}\n{\'class_type\', \'instructor\', \'user_id\'}\n{\'class_type\', \'instructor\', \'user_id\'}\nOut[8]: \n        class_type instructor\nuser_id                      \n1             None       None\n2             None       None\n3             None       None\n4             None       None\n'
'df = pd.merge(dfA, dfB, on=[\'a\',\'b\'], how="outer", indicator=True)\ndf = df[df[\'_merge\'] == \'left_only\']\n\ndf = pd.merge(dfA, dfB, on=[\'a\',\'b\'], how="outer", indicator=True\n              ).query(\'_merge=="left_only"\')\n'
"res = df[~df[['Name1', 'Name2']].apply(frozenset, axis=1).duplicated()]\n\nprint(res)\n\n  Name1 Name2  Value\n0  Juan   Ale      1\n"
"import pandas as pd\n\ndf[['Date','ClosingPrice']].plot('Date', figsize=(15,8))\n\ndf.set_index('Date', inplace=True, drop=True)\n\ndf.index = pd.to_datetime(df.index)\n\ndf.plot()\n\n# single [ ] transforms the column into series, double [[ ]] into DataFrame\ndf[['pct_change_1']].plot(figsize=(15,8)) \n"
"np.random.seed(1245)\n\na = ['No', 'Yes', 'Maybe']\ndf = pd.DataFrame(np.random.choice(a, size=(10, 3)), columns=['Col1','Col2','Col3'])\ndf['Col1'] = pd.Categorical(df['Col1'])\n\nprint (df.dtypes)\nCol1    category\nCol2      object\nCol3      object\ndtype: object\n\nprint (df['Col1'].cat.categories)\nIndex(['Maybe', 'No', 'Yes'], dtype='object')\n\nprint (df['Col2'].unique())\n['Yes' 'Maybe' 'No']\n\nprint (df['Col1'].unique())\n[Maybe, No, Yes]\nCategories (3, object): [Maybe, No, Yes]\n"
"from collections import Counter\nimport numpy as np\nimport pandas as pd\n\ndef getAssignedPeople(df, areasPerPerson):\n    areas = df['Area'].values\n    places = df['Place'].values\n    times = pd.to_datetime(df['Time']).values\n    maxPerson = np.ceil(areas.size / float(areasPerPerson)) - 1\n    assignmentCount = Counter()\n    assignedPeople = []\n    assignedPlaces = {}\n    heldPeople = {}\n    heldAreas = {}\n    holdAvailable = True\n    person = 0\n\n    # search for repeated areas. Mark them if the next repeat occurs within an hour\n    ixrep = np.argmax(np.triu(areas.reshape(-1, 1)==areas, k=1), axis=1)\n    holds = np.zeros(areas.size, dtype=bool)\n    holds[ixrep.nonzero()] = (times[ixrep[ixrep.nonzero()]] - times[ixrep.nonzero()]) &lt; np.timedelta64(1, 'h')\n\n    for area,place,hold in zip(areas, places, holds):\n        if (area, place) in assignedPlaces:\n            # this unique (area, place) has already been assigned to someone\n            assignedPeople.append(assignedPlaces[(area, place)])\n            continue\n\n        if assignmentCount[person] &gt;= areasPerPerson:\n            # the current person is already assigned to enough areas, move on to the next\n            a = heldPeople.pop(person, None)\n            heldAreas.pop(a, None)\n            person += 1\n\n        if area in heldAreas:\n            # assign to the person held in this area\n            p = heldAreas.pop(area)\n            heldPeople.pop(p)\n        else:\n            # get the first non-held person. If we need to hold in this area, \n            # also make sure the person has at least 2 free assignment slots,\n            # though if it's the last person assign to them anyway \n            p = person\n            while p in heldPeople or (hold and holdAvailable and (areasPerPerson - assignmentCount[p] &lt; 2)) and not p==maxPerson:\n                p += 1\n\n        assignmentCount.update([p])\n        assignedPlaces[(area, place)] = p\n        assignedPeople.append(p)\n\n        if hold:\n            if p==maxPerson:\n                # mark that there are no more people available to perform holds\n                holdAvailable = False\n\n            # this area recurrs in an hour, mark that the person should be held here\n            heldPeople[p] = area\n            heldAreas[area] = p\n\n    return assignedPeople\n\ndef allocatePeople(df, areasPerPerson=3):\n    assignedPeople = getAssignedPeople(df, areasPerPerson=areasPerPerson)\n    df = df.copy()\n    df.loc[:,'Person'] = df['Person'].unique()[assignedPeople]\n    return df\n\nds = dict(\nexample1 = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','08:40:00','08:42:00','08:45:00','08:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 5','House 1','House 2','House 3','House 2'],                 \n    'Area' : ['A','B','C','D','E','D','E','F','G'],     \n    'On' : ['1','2','3','4','5','6','7','8','9'], \n    'Person' : ['Person 1','Person 2','Person 3','Person 4','Person 5','Person 4','Person 5','Person 6','Person 7'],   \n    }),\nexample2 = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','8:40:00','8:42:00','8:45:00','8:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 1','House 2','House 3','House 1','House 2','House 3'],                 \n    'Area' : ['X','X','X','X','X','X','X','X','X'],     \n    'On' : ['1','2','3','3','3','3','3','3','3'], \n    'Person' : ['Person 1','Person 1','Person 1','Person 1','Person 1','Person 1','Person 1','Person 1','Person 1'],   \n    }),\n\nlong_repeats = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:25:00','8:30:00','8:31:00','8:35:00','8:45:00','8:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 1','House 1','House 2','House 3','House 2'],                 \n    'Area' : ['A','A','A','A','B','C','C','C','B'],  \n    'Person' : ['Person 1','Person 1','Person 1','Person 2','Person 3','Person 4','Person 4','Person 4','Person 3'],   \n    'On' : ['1','2','3','4','5','6','7','8','9'],                      \n    }),\nmany_repeats = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','08:40:00','08:42:00','08:45:00','08:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 1','House 1','House 2','House 1','House 2'],                 \n    'Area' : ['A', 'B', 'C', 'D', 'D', 'E', 'E', 'F', 'F'],     \n    'On' : ['1','2','3','4','5','6','7','8','9'], \n    'Person' : ['Person 1','Person 1','Person 1','Person 2','Person 3','Person 4','Person 3','Person 5','Person 6'],   \n    }),\nlarge_gap = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','08:40:00','08:42:00','08:45:00','08:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 1','House 1','House 2','House 1','House 3'],                 \n    'Area' : ['A', 'B', 'C', 'D', 'E', 'F', 'D', 'D', 'D'],     \n    'On' : ['1','2','3','4','5','6','7','8','9'], \n    'Person' : ['Person 1','Person 1','Person 1','Person 2','Person 3','Person 4','Person 3','Person 5','Person 6'],   \n    }),\ndifferent_times = ({\n    'Time' : ['8:03:00','8:17:00','8:20:00','8:28:00','8:35:00','08:40:00','09:42:00','09:45:00','09:50:00'],                 \n    'Place' : ['House 1','House 2','House 3','House 4','House 1','House 1','House 2','House 1','House 1'],                 \n    'Area' : ['A', 'B', 'C', 'D', 'D', 'E', 'E', 'F', 'G'],     \n    'On' : ['1','2','3','4','5','6','7','8','9'], \n    'Person' : ['Person 1','Person 1','Person 1','Person 2','Person 3','Person 4','Person 3','Person 5','Person 6'],   \n    })\n)\n\nexpectedPeoples = dict(\n    example1 = [1,1,1,2,3,2,3,2,3],\n    example2 = [1,1,1,1,1,1,1,1,1],\n    long_repeats = [1,1,1,2,2,3,3,3,2],\n    many_repeats = [1,1,1,2,2,3,3,2,3],\n    large_gap = [1,1,1,2,3,3,2,2,3],\n    different_times = [1,1,1,2,2,2,3,3,3],\n)\n\nfor name,d in ds.items():\n    df = pd.DataFrame(d)\n    expected = ['Person %d' % i for i in expectedPeoples[name]]\n    ap = allocatePeople(df)\n\n    print(name, ap, sep='\\n', end='\\n\\n')\n    np.testing.assert_array_equal(ap['Person'], expected)\n\nexample1\n       Time    Place Area On    Person\n0   8:03:00  House 1    A  1  Person 1\n1   8:17:00  House 2    B  2  Person 1\n2   8:20:00  House 3    C  3  Person 1\n3   8:28:00  House 4    D  4  Person 2\n4   8:35:00  House 5    E  5  Person 3\n5  08:40:00  House 1    D  6  Person 2\n6  08:42:00  House 2    E  7  Person 3\n7  08:45:00  House 3    F  8  Person 2\n8  08:50:00  House 2    G  9  Person 3\n\nexample2\n      Time    Place Area On    Person\n0  8:03:00  House 1    X  1  Person 1\n1  8:17:00  House 2    X  2  Person 1\n2  8:20:00  House 3    X  3  Person 1\n3  8:28:00  House 1    X  3  Person 1\n4  8:35:00  House 2    X  3  Person 1\n5  8:40:00  House 3    X  3  Person 1\n6  8:42:00  House 1    X  3  Person 1\n7  8:45:00  House 2    X  3  Person 1\n8  8:50:00  House 3    X  3  Person 1\n\nlong_repeats\n      Time    Place Area    Person On\n0  8:03:00  House 1    A  Person 1  1\n1  8:17:00  House 2    A  Person 1  2\n2  8:20:00  House 3    A  Person 1  3\n3  8:25:00  House 4    A  Person 2  4\n4  8:30:00  House 1    B  Person 2  5\n5  8:31:00  House 1    C  Person 3  6\n6  8:35:00  House 2    C  Person 3  7\n7  8:45:00  House 3    C  Person 3  8\n8  8:50:00  House 2    B  Person 2  9\n\nmany_repeats\n       Time    Place Area On    Person\n0   8:03:00  House 1    A  1  Person 1\n1   8:17:00  House 2    B  2  Person 1\n2   8:20:00  House 3    C  3  Person 1\n3   8:28:00  House 4    D  4  Person 2\n4   8:35:00  House 1    D  5  Person 2\n5  08:40:00  House 1    E  6  Person 3\n6  08:42:00  House 2    E  7  Person 3\n7  08:45:00  House 1    F  8  Person 2\n8  08:50:00  House 2    F  9  Person 3\n\nlarge_gap\n       Time    Place Area On    Person\n0   8:03:00  House 1    A  1  Person 1\n1   8:17:00  House 2    B  2  Person 1\n2   8:20:00  House 3    C  3  Person 1\n3   8:28:00  House 4    D  4  Person 2\n4   8:35:00  House 1    E  5  Person 3\n5  08:40:00  House 1    F  6  Person 3\n6  08:42:00  House 2    D  7  Person 2\n7  08:45:00  House 1    D  8  Person 2\n8  08:50:00  House 3    D  9  Person 3\n\ndifferent_times\n       Time    Place Area On    Person\n0   8:03:00  House 1    A  1  Person 1\n1   8:17:00  House 2    B  2  Person 1\n2   8:20:00  House 3    C  3  Person 1\n3   8:28:00  House 4    D  4  Person 2\n4   8:35:00  House 1    D  5  Person 2\n5  08:40:00  House 1    E  6  Person 2\n6  09:42:00  House 2    E  7  Person 3\n7  09:45:00  House 1    F  8  Person 3\n8  09:50:00  House 1    G  9  Person 3\n"
"dfs = [df.set_index(['profile', 'depth']) for df in [df1, df2, df3]]\n\nprint(pd.concat(dfs, axis=1).reset_index())\n#      profile  depth       VAR1     VAR2    VAR3\n# 0  profile_1    0.5  38.198002      NaN     NaN\n# 1  profile_1    0.6  38.198002  0.20440     NaN\n# 2  profile_1    1.1        NaN  0.20442     NaN\n# 3  profile_1    1.2        NaN  0.20446  15.188\n# 4  profile_1    1.3  38.200001      NaN  15.182\n# 5  profile_1    1.4        NaN      NaN  15.182\n"
'pandas.DataFrame(initialload, columns=list_of_column_names)\n'
'date_index = pd.DatetimeIndex([pd.datetime(2003,6,24), pd.datetime(2003,8,13),\n        pd.datetime(2003,8,19), pd.datetime(2003,8,22), pd.datetime(2003,8,24)])\n\nts = pd.Series([2,1,2,1,5], index=date_index)\n\n2003-06-24    2\n2003-08-13    1\n2003-08-19    2\n2003-08-22    1\n2003-08-24    5\n\nts.reindex(pd.date_range(min(date_index), max(date_index)))\n\n2003-06-24     2\n2003-06-25   NaN\n2003-06-26   NaN\n2003-06-27   NaN\n2003-06-28   NaN\n2003-06-29   NaN\n2003-06-30   NaN\n2003-07-01   NaN\n2003-07-02   NaN\n2003-07-03   NaN\n2003-07-04   NaN\n2003-07-05   NaN\n2003-07-06   NaN\n2003-07-07   NaN\n2003-07-08   NaN\n2003-07-09   NaN\n2003-07-10   NaN\n2003-07-11   NaN\n2003-07-12   NaN\n2003-07-13   NaN\n2003-07-14   NaN\n2003-07-15   NaN\n2003-07-16   NaN\n2003-07-17   NaN\n2003-07-18   NaN\n2003-07-19   NaN\n2003-07-20   NaN\n2003-07-21   NaN\n2003-07-22   NaN\n2003-07-23   NaN\n2003-07-24   NaN\n2003-07-25   NaN\n2003-07-26   NaN\n2003-07-27   NaN\n2003-07-28   NaN\n2003-07-29   NaN\n2003-07-30   NaN\n2003-07-31   NaN\n2003-08-01   NaN\n2003-08-02   NaN\n2003-08-03   NaN\n2003-08-04   NaN\n2003-08-05   NaN\n2003-08-06   NaN\n2003-08-07   NaN\n2003-08-08   NaN\n2003-08-09   NaN\n2003-08-10   NaN\n2003-08-11   NaN\n2003-08-12   NaN\n2003-08-13     1\n2003-08-14   NaN\n2003-08-15   NaN\n2003-08-16   NaN\n2003-08-17   NaN\n2003-08-18   NaN\n2003-08-19     2\n2003-08-20   NaN\n2003-08-21   NaN\n2003-08-22     1\n2003-08-23   NaN\n2003-08-24     5\nFreq: D, Length: 62\n'
"from pandas import DataFrame\ndf = DataFrame({0: {'10/10/2012': 50, '10/11/2012': -10, '10/12/2012': 100}, 1: {'10/10/2012': 0, '10/11/2012': 90, '10/12/2012': -5}})\n\nIn [3]: df\nOut[3]: \n              0   1\n10/10/2012   50   0\n10/11/2012  -10  90\n10/12/2012  100  -5\n\nIn [4]: df.cumsum()\nOut[4]: \n              0   1\n10/10/2012   50   0\n10/11/2012   40  90\n10/12/2012  140  85\n"
"import pandas as pd\nimport io\n\ntext = '''\\\nSTK_ID RPT_Date sales cash\n000568 20120930 80.093 57.488\n000596 20120930 32.585 26.177\n000799 20120930 14.784 8.157\n'''\n\ndf = pd.read_csv(io.BytesIO(text), delimiter = ' ', \n                 converters = {0:str})\ndf.set_index(['STK_ID','RPT_Date'], inplace = True)\n\nindex = df.index\nnames = index.names\nindex = [('000999','20121231')] + df.index.tolist()[1:]\ndf.index = pd.MultiIndex.from_tuples(index, names = names)\nprint(df)\n#                   sales    cash\n# STK_ID RPT_Date                \n# 000999 20121231  80.093  57.488\n# 000596 20120930  32.585  26.177\n# 000799 20120930  14.784   8.157\n\ndf.reset_index(inplace = True)\ndf.ix[0, ['STK_ID', 'RPT_Date']] = ('000999','20121231')\ndf = df.set_index(['STK_ID','RPT_Date'])\nprint(df)\n\n#                   sales    cash\n# STK_ID RPT_Date                \n# 000999 20121231  80.093  57.488\n# 000596 20120930  32.585  26.177\n# 000799 20120930  14.784   8.157\n\nIn [2]: %timeit reassign_index(df)\n10000 loops, best of 3: 158 us per loop\n\nIn [3]: %timeit reassign_columns(df)\n1000 loops, best of 3: 843 us per loop\n"
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; df = pd.DataFrame(np.random.randn(5, 10))\n&gt;&gt;&gt; df[list(df.columns[:2]) + [7]]\n          0         1         7\n0  0.210139  0.533249  1.780426\n1  0.382136  0.083999 -0.392809\n2 -0.237868  0.493646 -1.208330\n3  1.242077 -0.781558  2.369851\n4  1.910740 -0.643370  0.982876\n\ndf[col_[:2, "col5", 3:6]]\n'
"In [12]: df = pd.DataFrame(randn(6, 3), index=arrays, columns=['A', 'B', 'C'])\n\nIn [13]: df\nOut[13]: \n                  A         B         C\nbar one 0 -0.694240  0.725163  0.131891\n    two 1 -0.729186  0.244860  0.530870\nbaz one 2  0.757816  1.129989  0.893080\nqux one 3 -2.275694  0.680023 -1.054816\n    two 4  0.291889 -0.409024 -0.307302\nbar one 5  1.697974 -1.828872 -1.004187\n\nIn [14]: df = df.sortlevel(0)\n\nIn [15]: df\nOut[15]: \n                  A         B         C\nbar one 0 -0.694240  0.725163  0.131891\n        5  1.697974 -1.828872 -1.004187\n    two 1 -0.729186  0.244860  0.530870\nbaz one 2  0.757816  1.129989  0.893080\nqux one 3 -2.275694  0.680023 -1.054816\n    two 4  0.291889 -0.409024 -0.307302\n\nIn [16]: df.loc[('bar','two'),'A'] = 9999\n\nIn [17]: df\nOut[17]: \n                     A         B         C\nbar one 0    -0.694240  0.725163  0.131891\n        5     1.697974 -1.828872 -1.004187\n    two 1  9999.000000  0.244860  0.530870\nbaz one 2     0.757816  1.129989  0.893080\nqux one 3    -2.275694  0.680023 -1.054816\n    two 4     0.291889 -0.409024 -0.307302\n\nIn [23]: df.loc[('bar','two',1),'A'] = 999\n\nIn [24]: df\nOut[24]: \n                    A         B         C\nbar one 0   -0.113216  0.878715 -0.183941\n    two 1  999.000000 -1.405693  0.253388\nbaz one 2    0.441543  0.470768  1.155103\nqux one 3   -0.008763  0.917800 -0.699279\n    two 4    0.061586  0.537913  0.380175\nbar one 5    0.857231  1.144246 -2.369694\n\nIn [27]: df.index.lexsort_depth\nOut[27]: 0\n\nIn [28]: df.sortlevel(0).index.lexsort_depth\nOut[28]: 3\n\nIn [12]: df.loc[('bar','one'),'A'] = [999,888]\n\nIn [13]: df\nOut[13]:\xa0\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 A \xa0 \xa0 \xa0 \xa0 B \xa0 \xa0 \xa0 \xa0 C\nbar one 0 \xa0999.000000 -0.645641 \xa00.369443\n\xa0 \xa0 \xa0 \xa0 5 \xa0888.000000 -0.990632 -0.577401\n\xa0 \xa0 two 1 \xa0 -1.071410 \xa02.308711 \xa02.018476\nbaz one 2 \xa0 \xa01.211887 \xa01.516925 \xa00.064023\nqux one 3 \xa0 -0.862670 -0.770585 -0.843773\n\xa0 \xa0 two 4 \xa0 -0.644855 -1.431962 \xa00.232528\n"
"from collections import deque\nfrom StringIO import StringIO\n\nwith open(fname, 'r') as f:\n    q = deque(f, 2)  # replace 2 with n (lines read at the end)\n\nIn [12]: q\nOut[12]: deque(['7,8,9\\n', '10,11,12'], maxlen=2)\n         # these are the last two lines of my csv\n\nIn [13]: pd.read_csv(StringIO(''.join(q)), header=None)\n"
"import pandas as pd\nimport numpy as np\ndata = np.random.random((8,3))*10000\ndf = pd.DataFrame (data)\npd.options.display.float_format = '{:20,.2f}'.format\nprint(df)\n\n                     0                    1                    2\n0             4,839.01             6,170.02               301.63\n1             4,411.23             8,374.36             7,336.41\n2             4,193.40             2,741.63             7,834.42\n3             3,888.27             3,441.57             9,288.64\n4               220.13             6,646.20             3,274.39\n5             3,885.71             9,942.91             2,265.95\n6             3,448.75             3,900.28             6,053.93\n\ndisplay.float_format: [default: None] [currently: None] : callable\n        The callable should accept a floating point number and return\n        a string with the desired format of the number. This is used\n        in some places like SeriesFormatter.\n        See core.format.EngFormatter for an example.\n"
'&gt;&gt;&gt; bin_avg[index] = np.average(items_in_bin, weights=my_weights)\n'
'dfm.index = range(1,len(dfm) + 1)\n'
"In [11]: df.loc[3] = ['V1', 4.3, 2.2, 2.2, 20.2]\n\n In [12]: df.loc[3, list('ABCDE')] = ['V1', 4.3, 2.2, 2.2, 20.2]\n"
"In [1]: df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b'])\n\nIn [2]: df\nOut[2]: \n   a  b\n0  1  2\n1  3  4\n\nIn [3]: from collections import OrderedDict\n\nIn [4]: df.apply(OrderedDict)\nOut[4]: \n   a  b\n0  1  2\n1  3  4\n\nIn [5]: [OrderedDict(row) for i, row in df.iterrows()]\nOut[5]: [OrderedDict([('a', 1), ('b', 2)]), OrderedDict([('a', 3), ('b', 4)])]\n\nIn [6]: (OrderedDict(row) for i, row in df.iterrows())\nOut[6]: &lt;generator object &lt;genexpr&gt; at 0x10466da50&gt;\n"
'df3 = pd.concat((df1, df2))\ndf3.groupby(df3.index).mean()\n\n#    col\n# a    1\n# b    3\n# c    4\n# d    6\n\npd.concat((df1, df2), axis=1).mean(axis=1)\n'
"ax2.plot(ax.get_xticks(),\n         df[['sales_gr','net_pft_gr']].values,\n         linestyle='-',\n         marker='o', linewidth=2.0)\n\nax.set_ylim((-10, 80.))\n"
"In [33]: df = pd.read_json(s)\n\nIn [25]: df\nOut[25]: \n  args                date            host kwargs     operation  status   thingy      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf    -101  a13yy38  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}     x_initobj       1  a19kyy1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}  x_gobjParams    -101  a14yy10  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull    -101  a14yy24  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf    -101  a14yy24  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}       gxyzinf    -101  a12yy34  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}     deletemfg       1  a15yy15  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}       gxyzinf    -101  a15yy15  0.000602\n\n[8 rows x 8 columns]\n\nIn [26]: df.dtypes\nOut[26]: \nargs                 object\ndate         datetime64[ns]\nhost                 object\nkwargs               object\noperation            object\nstatus                int64\nthingy               object\ntime                float64\ndtype: object\n\nIn [27]: df.apply(lambda x: pd.lib.infer_dtype(x.values))\nOut[27]: \nargs            unicode\ndate         datetime64\nhost            unicode\nkwargs          unicode\noperation       unicode\nstatus          integer\nthingy          unicode\ntime           floating\ndtype: object\n\nIn [28]: types = df.apply(lambda x: pd.lib.infer_dtype(x.values))\n\nIn [29]: types[types=='unicode']\nOut[29]: \nargs         unicode\nhost         unicode\nkwargs       unicode\noperation    unicode\nthingy       unicode\ndtype: object\n\nIn [30]: for col in types[types=='unicode'].index:\n   ....:     df[col] = df[col].astype(str)\n   ....:     \n\nIn [31]: df\nOut[31]: \n  args                date            host kwargs     operation  status   thingy      time\n0   [] 2013-12-02 00:33:59  yy38.segm1.org     {}       x_gbinf    -101  a13yy38  0.000801\n1   [] 2013-12-02 00:33:59  kyy1.segm1.org     {}     x_initobj       1  a19kyy1  0.003244\n2   [] 2013-12-02 00:34:00  yy10.segm1.org     {}  x_gobjParams    -101  a14yy10  0.002247\n3   [] 2013-12-02 00:34:00  yy24.segm1.org     {}        gtfull    -101  a14yy24  0.002787\n4   [] 2013-12-02 00:34:00  yy24.segm1.org     {}       x_gbinf    -101  a14yy24  0.001067\n5   [] 2013-12-02 00:34:00  yy34.segm1.org     {}       gxyzinf    -101  a12yy34  0.002652\n6   [] 2013-12-02 00:34:00  yy15.segm1.org     {}     deletemfg       1  a15yy15  0.004371\n7   [] 2013-12-02 00:34:00  yy15.segm1.org     {}       gxyzinf    -101  a15yy15  0.000602\n\n[8 rows x 8 columns]\n\nIn [32]: df.apply(lambda x: pd.lib.infer_dtype(x.values))\nOut[32]: \nargs             string\ndate         datetime64\nhost             string\nkwargs           string\noperation        string\nstatus          integer\nthingy           string\ntime           floating\ndtype: object\n"
"# Write data to store (broadcasting works)\narray1[:]  = 3\n\n# Read data from store\nin_ram_array = array1[:]\n\ntable_uint8 = np.dtype([('field1', 'u1')])\ntable_1d = data_file.create_table('/', 'array_1d', description=table_uint8)\n\nindex = table_1d.get_where_list('field1 &gt; 3')\n"
'DFList = []\nfor group in df.groupby(df.index.day):\n    DFList.append(group[1])\n\nDFList = [group[1] for group in df.groupby(df.index.day)]\n'
"df = pd.DataFrame(np.random.rand(10, 4), columns=['a', 'b', 'c', 'd'])\nax = plt.figure(figsize=(10, 6)).add_subplot(111)\ndf.plot(ax=ax, kind='bar', legend=False)\n\nbars = ax.patches\nhatches = ''.join(h*len(df) for h in 'x/O.')\n\nfor bar, hatch in zip(bars, hatches):\n    bar.set_hatch(hatch)\n\nax.legend(loc='center right', bbox_to_anchor=(1, 1), ncol=4)\n"
"df.to_sql('test', engine, schema='a_schema')\n\nmeta = sqlalchemy.MetaData(engine, schema='a_schema')\nmeta.reflect()\npdsql = pd.io.sql.PandasSQLAlchemy(engine, meta=meta)\npdsql.to_sql(df, 'test')\n"
"codes = list('ABCDEFGH'); \ndates = pd.Series(pd.date_range('2013-11-01', '2014-01-31')); \ndates = dates.append(dates)\ndates.sort()\ndf = pd.DataFrame({'amount': np.random.randint(1, 10, dates.size), 'col1': np.random.choice(codes, dates.size), 'col2': np.random.choice(codes, dates.size), 'date': dates})\n\nkw = lambda x: x.isocalendar()[1]; \nkw_year = lambda x: str(x.year) + ' - ' + str(x.isocalendar()[1])\ngrouped = df.groupby([df['date'].map(kw_year), 'col1'], sort=False, as_index=False).agg({'amount': 'sum'})\nA = grouped.pivot(index='date', columns='col1', values='amount').fillna(0).reset_index()\n\nticks = A.date.values.tolist()\ndel A['date']\nax = A.plot(kind='bar')\nax.set_xticklabels(ticks)\n"
'#!/usr/bin/env python\n# -*- coding: utf-8 -*- \nimport numpy as np\nimport pandas as pd\nfrom pandas.io.parsers import StringIO\n\na = """timepoint,measure\n2014-01-01 00:00:00,78\n2014-01-02 00:00:00,29\n2014-01-03 00:00:00,5\n2014-01-04 00:00:00,73\n2014-01-05 00:00:00,40\n2014-01-06 00:00:00,45\n2014-01-07 00:00:00,48\n2014-01-08 00:00:00,2\n2014-01-09 00:00:00,96\n2014-01-10 00:00:00,82\n2014-01-11 00:00:00,61\n2014-01-12 00:00:00,68\n2014-01-13 00:00:00,8\n2014-01-14 00:00:00,94\n2014-01-15 00:00:00,16\n2014-01-16 00:00:00,31\n2014-01-17 00:00:00,10\n2014-01-18 00:00:00,34\n2014-01-19 00:00:00,27\n2014-01-20 00:00:00,58\n2014-01-21 00:00:00,90\n2014-01-22 00:00:00,41\n2014-01-23 00:00:00,97\n2014-01-24 00:00:00,7\n2014-01-25 00:00:00,86\n2014-01-26 00:00:00,62\n2014-01-27 00:00:00,91\n2014-01-28 00:00:00,0\n2014-01-29 00:00:00,73\n2014-01-30 00:00:00,22\n2014-01-31 00:00:00,43\n2014-02-01 00:00:00,87\n2014-02-02 00:00:00,56\n2014-02-03 00:00:00,45\n2014-02-04 00:00:00,25\n2014-02-05 00:00:00,92\n2014-02-06 00:00:00,83\n2014-02-07 00:00:00,13\n2014-02-08 00:00:00,50\n2014-02-09 00:00:00,48\n2014-02-10 00:00:00,78"""\n\nb = """timepoint,measure\n2014-01-01 00:00:00,78\n2014-01-08 00:00:00,29\n2014-01-15 00:00:00,5\n2014-01-22 00:00:00,73\n2014-01-29 00:00:00,40\n2014-02-05 00:00:00,45\n2014-02-12 00:00:00,48\n2014-02-19 00:00:00,2\n2014-02-26 00:00:00,96\n2014-03-05 00:00:00,82\n2014-03-12 00:00:00,61\n2014-03-19 00:00:00,68\n2014-03-26 00:00:00,8\n2014-04-02 00:00:00,94\n"""\n\ndf1 = pd.read_csv(StringIO(a), parse_dates=[\'timepoint\'])\ndf1.head()\n\n   timepoint  measure\n0 2014-01-01       78\n1 2014-01-02       29\n2 2014-01-03        5\n3 2014-01-04       73\n4 2014-01-05       40\n\ndf2 = pd.read_csv(StringIO(b), parse_dates=[\'timepoint\'])\ndf2.head()\n\n   timepoint  measure\n0 2014-01-01       78\n1 2014-01-08       29\n2 2014-01-15        5\n3 2014-01-22       73\n4 2014-01-29       40\n\ndef find_closest_date(timepoint, time_series, add_time_delta_column=True):\n    # takes a pd.Timestamp() instance and a pd.Series with dates in it\n    # calcs the delta between `timepoint` and each date in `time_series`\n    # returns the closest date and optionally the number of days in its time delta\n    deltas = np.abs(time_series - timepoint)\n    idx_closest_date = np.argmin(deltas)\n    res = {"closest_date": time_series.ix[idx_closest_date]}\n    idx = [\'closest_date\']\n    if add_time_delta_column:\n        res["closest_delta"] = deltas[idx_closest_date]\n        idx.append(\'closest_delta\')\n    return pd.Series(res, index=idx)\n\ndf1[[\'closest\', \'days_bt_x_and_y\']] = df1.timepoint.apply(\n                                          find_closest_date, args=[df2.timepoint])\ndf1.head(10)\n\n   timepoint  measure    closest  days_bt_x_and_y\n0 2014-01-01       78 2014-01-01           0 days\n1 2014-01-02       29 2014-01-01           1 days\n2 2014-01-03        5 2014-01-01           2 days\n3 2014-01-04       73 2014-01-01           3 days\n4 2014-01-05       40 2014-01-08           3 days\n5 2014-01-06       45 2014-01-08           2 days\n6 2014-01-07       48 2014-01-08           1 days\n7 2014-01-08        2 2014-01-08           0 days\n8 2014-01-09       96 2014-01-08           1 days\n9 2014-01-10       82 2014-01-08           2 days\n\ndf3 = pd.merge(df1, df2, left_on=[\'closest\'], right_on=[\'timepoint\'])\n\ncolorder = [\n    \'timepoint_x\',\n    \'closest\',\n    \'timepoint_y\',\n    \'days_bt_x_and_y\',\n    \'measure_x\',\n    \'measure_y\'\n]\n\ndf3 = df3.ix[:, colorder]\ndf3\n\n   timepoint_x    closest timepoint_y  days_bt_x_and_y  measure_x  measure_y\n0   2014-01-01 2014-01-01  2014-01-01           0 days         78         78\n1   2014-01-02 2014-01-01  2014-01-01           1 days         29         78\n2   2014-01-03 2014-01-01  2014-01-01           2 days          5         78\n3   2014-01-04 2014-01-01  2014-01-01           3 days         73         78\n4   2014-01-05 2014-01-08  2014-01-08           3 days         40         29\n5   2014-01-06 2014-01-08  2014-01-08           2 days         45         29\n6   2014-01-07 2014-01-08  2014-01-08           1 days         48         29\n7   2014-01-08 2014-01-08  2014-01-08           0 days          2         29\n8   2014-01-09 2014-01-08  2014-01-08           1 days         96         29\n9   2014-01-10 2014-01-08  2014-01-08           2 days         82         29\n10  2014-01-11 2014-01-08  2014-01-08           3 days         61         29\n11  2014-01-12 2014-01-15  2014-01-15           3 days         68          5\n12  2014-01-13 2014-01-15  2014-01-15           2 days          8          5\n13  2014-01-14 2014-01-15  2014-01-15           1 days         94          5\n14  2014-01-15 2014-01-15  2014-01-15           0 days         16          5\n15  2014-01-16 2014-01-15  2014-01-15           1 days         31          5\n16  2014-01-17 2014-01-15  2014-01-15           2 days         10          5\n17  2014-01-18 2014-01-15  2014-01-15           3 days         34          5\n18  2014-01-19 2014-01-22  2014-01-22           3 days         27         73\n19  2014-01-20 2014-01-22  2014-01-22           2 days         58         73\n20  2014-01-21 2014-01-22  2014-01-22           1 days         90         73\n21  2014-01-22 2014-01-22  2014-01-22           0 days         41         73\n22  2014-01-23 2014-01-22  2014-01-22           1 days         97         73\n23  2014-01-24 2014-01-22  2014-01-22           2 days          7         73\n24  2014-01-25 2014-01-22  2014-01-22           3 days         86         73\n25  2014-01-26 2014-01-29  2014-01-29           3 days         62         40\n26  2014-01-27 2014-01-29  2014-01-29           2 days         91         40\n27  2014-01-28 2014-01-29  2014-01-29           1 days          0         40\n28  2014-01-29 2014-01-29  2014-01-29           0 days         73         40\n29  2014-01-30 2014-01-29  2014-01-29           1 days         22         40\n30  2014-01-31 2014-01-29  2014-01-29           2 days         43         40\n31  2014-02-01 2014-01-29  2014-01-29           3 days         87         40\n32  2014-02-02 2014-02-05  2014-02-05           3 days         56         45\n33  2014-02-03 2014-02-05  2014-02-05           2 days         45         45\n34  2014-02-04 2014-02-05  2014-02-05           1 days         25         45\n35  2014-02-05 2014-02-05  2014-02-05           0 days         92         45\n36  2014-02-06 2014-02-05  2014-02-05           1 days         83         45\n37  2014-02-07 2014-02-05  2014-02-05           2 days         13         45\n38  2014-02-08 2014-02-05  2014-02-05           3 days         50         45\n39  2014-02-09 2014-02-12  2014-02-12           3 days         48         48\n40  2014-02-10 2014-02-12  2014-02-12           2 days         78         48\n'
"a.iloc[-1, a.columns.get_loc('a')] = 77\n&gt;&gt;&gt; a\n   a  b  c\n0  1  2  3\n1  4  5  6\n2 77  8  9\n"
'import numpy as np\nnp.round(p_table, decimals=2)\n'
"a={'b':[100,],'c':[300,]}\npd.DataFrame(a)\n\n     b    c\n0  100  300\n\na={'b':100,'c':300}\npd.DataFrame(a, index=['i',])\n\n     b    c\ni  100  300\n"
"In [218]: df = pd.DataFrame({'Date': [0, 1, 2, 0, 1, 2], 'Name': ['A', 'B', 'C', 'A', 'B', 'C'], 'val': [0, 1, 2, 3, 4, 5]}, index=['DC', np.nan, 'BS', 'AB', 'OA', np.nan]); df\nOut[218]: \n     Date Name  val\nDC      0    A    0\nNaN     1    B    1\nBS      2    C    2\nAB      0    A    3\nOA      1    B    4\nNaN     2    C    5\n\nIn [219]: df.index.isnull()\nOut[219]: array([False,  True, False, False, False,  True], dtype=bool)\n\nIn [220]: df.loc[df.index.isnull()]\nOut[220]: \n     Date Name  val\nNaN     1    B    1\nNaN     2    C    5\n"
"df.sort_values('impwealth', inplace=True)\ncumsum = df.indweight.cumsum()\ncutoff = df.indweight.sum() / 2.0\nmedian = df.impwealth[cumsum &gt;= cutoff].iloc[0]\n"
"In [425]:\n\ndf = pd.DataFrame({'a':np.random.randn(5), 'b':np.random.randn(5)})\ndf\nOut[425]:\n          a         b\n0 -1.348112  0.583603\n1  0.174836  1.211774\n2 -2.054173  0.148201\n3 -0.589193 -0.369813\n4 -1.156423 -0.967516\nIn [426]:\n\nfor index, row in df.iterrows():\n    if row['a'] &gt; 0:\n        df.drop(index, inplace=True)\nIn [427]:\n\ndf\nOut[427]:\n          a         b\n0 -1.348112  0.583603\n2 -2.054173  0.148201\n3 -0.589193 -0.369813\n4 -1.156423 -0.967516\n\ndf[df['a'] &lt;=0]\n"
"In [220]: x\nOut[220]: numpy.datetime64('2012-06-17T23:00:05.453000000-0700')\n\nIn [221]: datetime.datetime.utcfromtimestamp(x.tolist()/1e9)\nOut[221]: datetime.datetime(2012, 6, 18, 6, 0, 5, 452999)\n\nIn [294]: datetime.datetime.utcfromtimestamp(x.astype('O')/1e9)\nOut[294]: datetime.datetime(2012, 6, 18, 6, 0, 5, 452999)\n\nIn [295]: datetime.datetime.fromtimestamp(x.astype('O')/1e9)\n\nIn [296]: x.astype('M8[D]').astype('O')\nOut[296]: datetime.date(2012, 6, 18)\n\nIn [297]: x.astype('M8[ms]').astype('O')\nOut[297]: datetime.datetime(2012, 6, 18, 6, 0, 5, 453000)\n\nIn [303]: np.array([[x,x],[x,x]],dtype='M8[ms]').astype('O')[0,1]\nOut[303]: datetime.datetime(2012, 6, 18, 6, 0, 5, 453000)\n"
"def most_informative_feature_for_class_svm(vectorizer, classifier,  classlabel, n=10):\n    labelid = ?? # this is the coef we're interested in. \n    feature_names = vectorizer.get_feature_names()\n    svm_coef = classifier.coef_.toarray() \n    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print feat, coef\n\nimport codecs, re, time\nfrom itertools import chain\n\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntrainfile = 'train.txt'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = CountVectorizer(analyzer='word')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,'r','utf8'))\ntags = ['bs','pt','es','sr']\n\n# Training NB\nmnb = MultinomialNB()\nmnb.fit(trainset, tags)\n\nfrom sklearn.svm import SVC\nsvcc = SVC(kernel='linear', C=1)\nsvcc.fit(trainset, tags)\n\ndef most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\n\ndef most_informative_feature_for_class_svm(vectorizer, classifier,  n=10):\n    labelid = 3 # this is the coef we're interested in. \n    feature_names = vectorizer.get_feature_names()\n    svm_coef = classifier.coef_.toarray() \n    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print feat, coef\n\nmost_informative_feature_for_class(word_vectorizer, mnb, 'pt')\nprint \nmost_informative_feature_for_class_svm(word_vectorizer, svcc)\n\npt teve -4.63472898823\npt tive -4.63472898823\npt todas -4.63472898823\npt vida -4.63472898823\npt de -4.22926388012\npt foi -4.22926388012\npt mais -4.22926388012\npt me -4.22926388012\npt as -3.94158180767\npt que -3.94158180767\n\nno 0.0204081632653\nparecer 0.0204081632653\npone 0.0204081632653\npor 0.0204081632653\nrelación 0.0204081632653\nuna 0.0204081632653\nvisto 0.0204081632653\nya 0.0204081632653\nes 0.0408163265306\nlo 0.0408163265306\n"
'grep error_push h5checker.c -A1\n'
"def test(df):\n    df = df.copy(deep=True)\n    df['tt'] = np.nan\n    return df\n"
"df2.reset_index(inplace=True)\n\nid     Cost1    Cost2     Cost3 Value1   Value2   Value3   \n1       124      214      1234    12        23       15\n2      1324       0       234     45         0       34\n\ndf2.columns = [' '.join(col).strip() for col in df2.columns.values]\n"
'# Plotting\ndf.plot(kind=\'line\', subplots=True, grid=True, title="Sample Data (Unit)",\n    layout=(4, 3), sharex=True, sharey=False, legend=False,    \n    style=[\'r\', \'r\', \'r\', \'g\', \'g\', \'g\', \'b\', \'b\', \'b\', \'r\', \'r\', \'r\'],\n    xticks=np.arange(0, len(df), 16))\n\n[ax.legend(loc=1) for ax in plt.gcf().axes]\n'
'  pos    bidder\n0   1          \n1   2          \n2   3  &lt;- alice\n3        &lt;- bob\n4   5   \n\nimport pandas as pd\n\nn = 5\noutput = pd.DataFrame({\'pos\': range(1, n + 1),\n                      \'bidder\': [\'\'] * n},\n                      columns=[\'pos\', \'bidder\'])\nbids = {\'alice\': 3, \'bob\': 3}\nused_pos = []\nfor bidder, pos in bids.items():\n    if pos in used_pos:\n        output.ix[pos, \'bidder\'] = "&lt;- %s" % bidder\n        output.ix[pos, \'pos\'] = \'\'\n    else:\n        output.ix[pos - 1, \'bidder\'] = "&lt;- %s" % bidder\n        used_pos.append(pos)\nprint(output)\n\nimport pandas as pd\n\ndata = {\'johnny\\nnewline\': 2, \'alice\': 3, \'bob\': 3,\n        \'frank\': 4, \'lisa\': 1, \'tom\': 8}\nn = range(1, max(data.values()) + 1)\n\n# Create DataFrame with columns = pos\noutput = pd.DataFrame(columns=n, index=[])\n\n# Populate DataFrame with rows\nfor index, (bidder, pos) in enumerate(data.items()):\n    output.loc[index, pos] = bidder\n\n# Print the DataFrame and remove NaN to make it easier to read.\nprint(output.fillna(\'\'))\n\n# Fetch and print every element in column 2\nfor index in range(1, 5):\n    print(output.loc[index, 2])\n'
"# The default parameters in Matplotlib\nwith plt.style.context('classic'):\n    plt.plot([1, 2, 3, 4])\n\n# Similar to ggplot from R\nwith plt.style.context('ggplot'):\n    plt.plot([1, 2, 3, 4])\n\nwith plt.style.context('/path/to/stylesheet'):\n    plt.plot([1, 2, 3, 4])\n\nwith plt.rc_context({'lines.linewidth': 5}):\n    plt.plot([1, 2, 3, 4])\n"
'In [183]:\ndf.ix[:,~df.columns.duplicated()]\n\nOut[183]:\n   a\n0  0\n1  1\n2  2\n3  3\n4  4\n\nIn [184]:\ndf.columns.duplicated()\n\nOut[184]:\narray([False,  True], dtype=bool)\n\ndf.iloc[:,~df.columns.duplicated()]\n\ndf.loc[:,~df.columns.duplicated()]\n'
"#convert column to string\ndf['movie_title'] = df['movie_title'].astype(str)\n\n#but it remove numbers in names of movies too\ndf['titles'] = df['movie_title'].str.extract('([a-zA-Z ]+)', expand=False).str.strip()\ndf['titles1'] = df['movie_title'].str.split('(', 1).str[0].str.strip()\ndf['titles2'] = df['movie_title'].str.replace(r'\\([^)]*\\)', '').str.strip()\nprint df\n          movie_title      titles      titles1      titles2\n0  Toy Story 2 (1995)   Toy Story  Toy Story 2  Toy Story 2\n1    GoldenEye (1995)   GoldenEye    GoldenEye    GoldenEye\n2   Four Rooms (1995)  Four Rooms   Four Rooms   Four Rooms\n3   Get Shorty (1995)  Get Shorty   Get Shorty   Get Shorty\n4      Copycat (1995)     Copycat      Copycat      Copycat\n"
'  df[\'DB_user\'] = df["DB_user"].apply(lambda x: \'\'.join([" " if ord(i) &lt; 32 or ord(i) &gt; 126 else i for i in x]))\n\n   \'\'.join([i if 32 &lt; ord(i) &lt; 126 else " " for i in x])\n\nfrom string import printable\nst = set(printable)\ndf["DB_user"] = df["DB_user"].apply(lambda x: \'\'.join([" " if  i not in  st else i for i in x]))\n\nfrom string import maketrans\n\ndel_chars =  " ".join(chr(i) for i in range(32) + range(127, 256))\ntrans = maketrans(t, " "*len(del_chars))\n\ndf[\'DB_user\'] = df["DB_user"].apply(lambda s: s.translate(trans))\n\n  df[\'DB_user\'] = df["DB_user"].str.translate(trans)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nfrom mpl_toolkits.mplot3d import Axes3D\n\n## Matplotlib Sample Code using 2D arrays via meshgrid\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y)\nR = np.sqrt(X ** 2 + Y ** 2)\nZ = np.sin(R)\nfig = plt.figure()\nax = Axes3D(fig)\nsurf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\nax.set_zlim(-1.01, 1.01)\n\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter(\'%.02f\'))\n\nfig.colorbar(surf, shrink=0.5, aspect=5)\nplt.title(\'Original Code\')\nplt.show()\n\n## DataFrame from 2D-arrays\nx = X.reshape(1600)\ny = Y.reshape(1600)\nz = Z.reshape(1600)\ndf = pd.DataFrame({\'x\': x, \'y\': y, \'z\': z}, index=range(len(x)))\n\n# Plot using `.trisurf()`:\n\nax.plot_trisurf(df.x, df.y, df.z, cmap=cm.jet, linewidth=0.2)\nplt.show()\n\n# 2D-arrays from DataFrame\nx1 = np.linspace(df[\'x\'].min(), df[\'x\'].max(), len(df[\'x\'].unique()))\ny1 = np.linspace(df[\'y\'].min(), df[\'y\'].max(), len(df[\'y\'].unique()))\n\n"""\nx, y via meshgrid for vectorized evaluation of\n2 scalar/vector fields over 2-D grids, given\none-dimensional coordinate arrays x1, x2,..., xn.\n"""\n\nx2, y2 = np.meshgrid(x1, y1)\n\n# Interpolate unstructured D-dimensional data.\nz2 = griddata((df[\'x\'], df[\'y\']), df[\'z\'], (x2, y2), method=\'cubic\')\n\n# Ready to plot\nfig = plt.figure()\nax = fig.gca(projection=\'3d\')\nsurf = ax.plot_surface(x2, y2, z2, rstride=1, cstride=1, cmap=cm.coolwarm,\n                       linewidth=0, antialiased=False)\nax.set_zlim(-1.01, 1.01)\n\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter(\'%.02f\'))\n\nfig.colorbar(surf, shrink=0.5, aspect=5)\nplt.title(\'Meshgrid Created from 3 1D Arrays\')\n\nplt.show()\n'
'cum_returns = (1 + df_returns).cumprod()\ndrawdown =  1 - cum_returns.div(cum_returns.cummax())\n'
"print df.index.tolist()\n\nprint list(df.index)\n\nprint df.index.values.tolist()\n\nimport pandas as pd\n\nidx = pd.Index([u'Newal', u'Saraswati Khera', u'Tohana'])\nprint idx\nIndex([u'Newal', u'Saraswati Khera', u'Tohana'], dtype='object')\n\nprint idx.tolist()\n[u'Newal', u'Saraswati Khera', u'Tohana']\n\nprint list(idx)\n[u'Newal', u'Saraswati Khera', u'Tohana']\n\nprint [x.encode('UTF8') for x in idx.tolist()]\n['Newal', 'Saraswati Khera', 'Tohana']\n\nprint [str(x) for x in idx.tolist()]\n['Newal', 'Saraswati Khera', 'Tohana']\n\nimport pandas as pd\nimport numpy as np\n\n#random dataframe\nnp.random.seed(1)\ndf = pd.DataFrame(np.random.randint(10, size=(3,3)))\ndf.columns = list('ABC')\ndf.index = [u'Newal', u'Saraswati Khera', u'Tohana']\nprint df\n\nprint df.index\nIndex([u'Newal', u'Saraswati Khera', u'Tohana'], dtype='object')\n\nprint df.index.tolist()\n[u'Newal', u'Saraswati Khera', u'Tohana']\n\nprint list(df.index)\n[u'Newal', u'Saraswati Khera', u'Tohana']\n\nprint df.index.values.tolist()\n[u'Newal', u'Saraswati Khera', u'Tohana']\n\n\nIn [90]: %timeit list(df.index)\nThe slowest run took 37.42 times longer than the fastest. This could mean that an intermediate result is being cached \n100000 loops, best of 3: 2.18 µs per loop\n\nIn [91]: %timeit df.index.tolist()\nThe slowest run took 22.33 times longer than the fastest. This could mean that an intermediate result is being cached \n1000000 loops, best of 3: 1.75 µs per loop\n\nIn [92]: %timeit df.index.values.tolist()\nThe slowest run took 62.72 times longer than the fastest. This could mean that an intermediate result is being cached \n1000000 loops, best of 3: 787 ns per loop\n"
"get_sliding_window(df, 2).dot(X) # window size = 2\n\nIn [101]: df = pd.DataFrame(np.random.rand(5, 2).round(2), columns=['A', 'B'])\n\nIn [102]: X = np.array([2, 3])\n\nIn [103]: rolled_df = roll(df, 2)\n\nIn [104]: %timeit rolled_df.apply(lambda df: pd.Series(df.values.dot(X)))\n100 loops, best of 3: 5.51 ms per loop\n\nIn [105]: %timeit get_sliding_window(df, 2).dot(X)\n10000 loops, best of 3: 43.7 µs per loop\n\nIn [106]: rolled_df.apply(lambda df: pd.Series(df.values.dot(X)))\nOut[106]: \n      0     1\n1  2.70  4.09\n2  4.09  2.52\n3  2.52  1.78\n4  1.78  3.50\n\nIn [107]: get_sliding_window(df, 2).dot(X)\nOut[107]: \narray([[ 2.7 ,  4.09],\n       [ 4.09,  2.52],\n       [ 2.52,  1.78],\n       [ 1.78,  3.5 ]])\n"
'import pandas as pd\n\ntemp=u"""0 5\n1 10\n2 15\n3 20\n4 25"""\n#after testing replace io.StringIO(temp) to filename\ncolumn_names = [\'x\',\'y\']\ndf = pd.read_csv(pd.compat.StringIO(temp), sep="\\s+", header = None, names = column_names)\n\nprint (df)\n   x   y\n0  0   5\n1  1  10\n2  2  15\n3  3  20\n4  4  25\n\ncolumn_names = [\'x\',\'y\']\ndf = pd.read_csv(pd.compat.StringIO(temp),\n                 delim_whitespace=True, \n                 header = None, \n                 names = column_names)\n\nprint (df)\n   x   y\n0  0   5\n1  1  10\n2  2  15\n3  3  20\n4  4  25\n'
"df.to_csv('my_file.gz', compression='gzip')\n"
'Typeclass   Promotion dtype for storing NAs\nfloating    no change\nobject      no change\ninteger     cast to float64\nboolean     cast to object\n'
'df.mul([1, 100], axis=0)\nOut[17]: \n            0          1         2\n0   -1.198766  -1.340028  1.990843\n1  113.890468 -68.177755 -9.060228\n'
"df['new_col'] = [my_list] * len(df)\n\nIn [13]:\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))\ndf\n\nOut[13]:\n          a         b         c\n0 -0.010414  1.859791  0.184692\n1 -0.818050 -0.287306 -1.390080\n2 -0.054434  0.106212  1.542137\n3 -0.226433  0.390355  0.437592\n4 -0.204653 -2.388690  0.106218\n\nIn [17]:\ndf['b'] = [[234]] * len(df)\ndf\n\nOut[17]:\n          a      b         c\n0 -0.010414  [234]  0.184692\n1 -0.818050  [234] -1.390080\n2 -0.054434  [234]  1.542137\n3 -0.226433  [234]  0.437592\n4 -0.204653  [234]  0.106218\n"
"In [85]:\nimport datetime as dt\nimport pandas as pd\ndf = pd.DataFrame({'date':[42580.3333333333, 10023]})\ndf\n\nOut[85]:\n           date\n0  42580.333333\n1  10023.000000\n\nIn [86]:\ndf['real_date'] = pd.TimedeltaIndex(df['date'], unit='d') + dt.datetime(1900,1,1)\ndf\n\nOut[86]:\n           date                  real_date\n0  42580.333333 2016-07-31 07:59:59.971200\n1  10023.000000 1927-06-12 00:00:00.000000\n\nIn [89]:\ndf['real_date'] = pd.TimedeltaIndex(df['date'], unit='d') + dt.datetime(1899, 12, 30)\ndf\n\nOut[89]:\n           date                  real_date\n0  42580.333333 2016-07-29 07:59:59.971200\n1  10023.000000 1927-06-10 00:00:00.000000\n"
'cls.sum = _make_stat_function(\n            \'sum\', name, name2, axis_descr,\n            \'Return the sum of the values for the requested axis\',\n            nanops.nansum)\n\ndef _make_stat_function(name, name1, name2, axis_descr, desc, f):\n    @Substitution(outname=name, desc=desc, name1=name1, name2=name2,\n                  axis_descr=axis_descr)\n    @Appender(_num_doc)\n    def stat_func(self, axis=None, skipna=None, level=None, numeric_only=None,\n                  **kwargs):\n        _validate_kwargs(name, kwargs, \'out\', \'dtype\')\n\n        if skipna is None:\n            skipna = True\n        if axis is None:\n            axis = self._stat_axis_number\n        if level is not None:\n            return self._agg_by_level(name, axis=axis, level=level,\n                                      skipna=skipna)\n        return self._reduce(f, name, axis=axis, skipna=skipna,\n                            numeric_only=numeric_only)\n\ndef _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n            filter_type=None, **kwds):\n    axis = self._get_axis_number(axis)\n\n    def f(x):\n        return op(x, axis=axis, skipna=skipna, **kwds)\n\n    labels = self._get_agg_axis(axis)\n\n    # exclude timedelta/datetime unless we are uniform types\n    if axis == 1 and self._is_mixed_type and self._is_datelike_mixed_type:\n        numeric_only = True\n\n    if numeric_only is None:\n        try:\n            values = self.values\n            result = f(values)\n        except Exception as e:\n\n            # try by-column first\n            if filter_type is None and axis == 0:\n                try:\n\n                    # this can end up with a non-reduction\n                    # but not always. if the types are mixed\n                    # with datelike then need to make sure a series\n                    result = self.apply(f, reduce=False)\n                    if result.ndim == self.ndim:\n                        result = result.iloc[0]\n                    return result\n                except:\n                    pass\n\n            if filter_type is None or filter_type == \'numeric\':\n                data = self._get_numeric_data()\n            elif filter_type == \'bool\':\n                data = self._get_bool_data()\n            else:  # pragma: no cover\n                e = NotImplementedError("Handling exception with filter_"\n                                        "type %s not implemented." %\n                                        filter_type)\n                raise_with_traceback(e)\n            result = f(data.values)\n            labels = data._get_agg_axis(axis)\n    else:\n        if numeric_only:\n            if filter_type is None or filter_type == \'numeric\':\n                data = self._get_numeric_data()\n            elif filter_type == \'bool\':\n                data = self._get_bool_data()\n            else:  # pragma: no cover\n                msg = ("Generating numeric_only data with filter_type %s"\n                       "not supported." % filter_type)\n                raise NotImplementedError(msg)\n            values = data.values\n            labels = data._get_agg_axis(axis)\n        else:\n            values = self.values\n        result = f(values)\n\n    if hasattr(result, \'dtype\') and is_object_dtype(result.dtype):\n        try:\n            if filter_type is None or filter_type == \'numeric\':\n                result = result.astype(np.float64)\n            elif filter_type == \'bool\' and notnull(result).all():\n                result = result.astype(np.bool_)\n        except (ValueError, TypeError):\n\n            # try to coerce to the original dtypes item by item if we can\n            if axis == 0:\n                result = com._coerce_to_dtypes(result, self.dtypes)\n\n    return Series(result, index=labels)\n\nif hasattr(result, \'dtype\') and is_object_dtype(result.dtype):\n    try:\n        if filter_type is None or filter_type == \'numeric\':\n            result = result.astype(np.float64)\n\n&gt; c:\\users\\matthew\\anaconda2\\lib\\site-packages\\pandas\\core\\frame.py(4801)_reduce()\n-&gt; result = result.astype(np.float64)\n(Pdb) l\n4796                result = f(values)\n4797    \n4798            if hasattr(result, \'dtype\') and is_object_dtype(result.dtype):\n4799                try:\n4800                    if filter_type is None or filter_type == \'numeric\':\n4801 -&gt;                     result = result.astype(np.float64)\n4802                    elif filter_type == \'bool\' and notnull(result).all():\n4803                        result = result.astype(np.bool_)\n4804                except (ValueError, TypeError):\n4805    \n4806                    # try to coerce to the original dtypes item by item if we can\n'
"In [162]: df\nOut[162]:\n  col1  col2  col3       date\n0    A     1    30 2016-01-01\n1    B     2    10 2016-01-02\n2    C     3    20 2016-01-03\n\nIn [163]: df.dtypes\nOut[163]:\ncol1            object\ncol2             int64\ncol3             int64\ndate    datetime64[ns]\ndtype: object\n\nIn [164]: df.select_dtypes(exclude=['object', 'datetime']) * 3\nOut[164]:\n   col2  col3\n0     3    90\n1     6    30\n2     9    60\n\ndf[df.select_dtypes(include=['number']).columns] *= 3\n"
"from openpyxl import load_workbook\n\nwb = load_workbook(filename='data.xlsx', \n                   read_only=True)\n\nws = wb['Sheet2']\n\n# Read the cell values into a list of lists\ndata_rows = []\nfor row in ws['A3':'D20']:\n    data_cols = []\n    for cell in row:\n        data_cols.append(cell.value)\n    data_rows.append(data_cols)\n\n# Transform into dataframe\nimport pandas as pd\ndf = pd.DataFrame(data_rows)\n"
'df.iloc[(df.x ** 2 + df.y **2).sort_values().index]\n'
"import statsmodels.api as sm\nendog = Sorted_Data3['net_realization_rate']\nexog = sm.add_constant(Sorted_Data3[['Cohort_2','Cohort_3']])\n\n# Fit and summarize OLS model\nmod = sm.OLS(endog, exog)\nresults = mod.fit()\nprint results.summary()\n\nexog = np.concatenate((np.repeat(1, len(Sorted_Data3))[:, None], \n                       Sorted_Data3[['Cohort_2','Cohort_3']].values),\n                       axis = 1)\n"
"import requests\nimport pandas as pd\nimport io\n\nurlData = requests.get(url).content\nrawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))\n"
"import pandas as pd\nimport numpy as np\nmeta = pd.DataFrame(columns=['a', 'b', 'c'])\nmeta.a = meta.a.astype(np.int64)\nmeta.b = meta.b.astype(np.datetime64)\n\nimport pandas as pd\n\nclass Foo:\n    def __init__(self, foo):\n        self.bar = foo\n\ndf = pd.DataFrame(data=[Foo(1), Foo(2)], columns=['a'], dtype='object')\ndf.a\n# 0    &lt;__main__.Foo object at 0x00000000058AC550&gt;\n# 1    &lt;__main__.Foo object at 0x00000000058AC358&gt;\n"
"df_res = df_res.append(res)\n\nall_res = []\nfor df in df_all:\n    for i in substr:\n        res = df[df['url'].str.contains(i)]\n        all_res.append(res)\n\ndf_res = pd.concat(all_res)\n"
'print(df.head(10))\n'
'patternDel = "( \\\\((MoM|QoQ)\\\\))"\nfilter = df[\'Event Name\'].str.contains(patternDel)\n\ndf = df[~filter]\n'
"import datetime as dt\ndata_df['Date'] = pd.to_datetime(data_df['Date'])\ndata_df['Date']=data_df['Date'].map(dt.datetime.toordinal)\n"
'def boolean_indexing(v, fillval=np.nan):\n    lens = np.array([len(item) for item in v])\n    mask = lens[:,None] &gt; np.arange(lens.max())\n    out = np.full(mask.shape,fillval)\n    out[mask] = np.concatenate(v)\n    return out\n\nIn [32]: l\nOut[32]: [[1, 2, 3], [1, 2], [3, 8, 9, 7, 3]]\n\nIn [33]: boolean_indexing(l)\nOut[33]: \narray([[  1.,   2.,   3.,  nan,  nan],\n       [  1.,   2.,  nan,  nan,  nan],\n       [  3.,   8.,   9.,   7.,   3.]])\n\nIn [34]: boolean_indexing(l,-1)\nOut[34]: \narray([[ 1,  2,  3, -1, -1],\n       [ 1,  2, -1, -1, -1],\n       [ 3,  8,  9,  7,  3]])\n'
'In [16]: s\nOut[16]:\n0     [1, 2, 3]\n1     [2, 3, 4]\n2     [3, 4, 5]\n3     [2, 3, 4]\n4     [3, 4, 5]\n5     [2, 3, 4]\n6     [3, 4, 5]\n7     [2, 3, 4]\n8     [3, 4, 5]\n9     [2, 3, 4]\n10    [3, 4, 5]\ndtype: object\n\nIn [17]: sm = np.matrix(s.tolist())\n\nIn [18]: sm\nOut[18]:\nmatrix([[1, 2, 3],\n        [2, 3, 4],\n        [3, 4, 5],\n        [2, 3, 4],\n        [3, 4, 5],\n        [2, 3, 4],\n        [3, 4, 5],\n        [2, 3, 4],\n        [3, 4, 5],\n        [2, 3, 4],\n        [3, 4, 5]])\n\nIn [19]: sm.shape\nOut[19]: (11, 3)\n'
'{% for key,value in x.iterrows() %}\n      &lt;option value="{{ value[\'id\'] }}"&gt;{{ value[\'text\'] }}&lt;/option&gt;\n{% endfor %}\n'
'df[0].apply(pd.Series)\n\n#   0    1   2\n#0  8   10  12\n#1  7    9  11\n\npd.concat([df[0].apply(pd.Series), df[1]], axis = 1)\n\n#   0    1   2  1\n#0  8   10  12  A\n#1  7    9  11  B\n'
"new_time = dfs['XYF']['TimeUS'].astype(float)\nnew_time_F = new_time / 1000000\n"
'q = q.append(a)\n'
'import pandas as pd\nimport numpy as np\n\n# Set up a DataFrame\nnp.random.seed(24)\ndf = pd.DataFrame({\'A\': np.linspace(1, 10, 10)})\ndf = pd.concat([df, pd.DataFrame(np.random.randn(10, 4), columns=list(\'BCDE\'))],\n               axis=1)\ndf.iloc[0, 2] = np.nan\n\n\ndf_html_output = df.style.set_table_styles(\n    [{\'selector\': \'thead th\',\n    \'props\': [(\'background-color\', \'red\')]},\n    {\'selector\': \'thead th:first-child\',\n    \'props\': [(\'display\',\'none\')]},\n    {\'selector\': \'tbody th:first-child\',\n    \'props\': [(\'display\',\'none\')]}]\n).render()\n\nhtml.append(df_html_output)\nbody = \'\\r\\n\\n&lt;br&gt;\'.join(\'%s\'%item for item in html)\nmsg.attach(MIMEText(body, \'html\'))\n\ndf_html_output = df.to_html(na_rep = "", index = False).replace(\'&lt;th&gt;\',\'&lt;th style = "background-color: red"&gt;\')\n\nhtml.append(df_html_output)\nbody = \'\\r\\n\\n&lt;br&gt;\'.join(\'%s\'%item for item in html)\nmsg.attach(MIMEText(body, \'html\'))\n'
'df.iloc[df.Single.str.lower().argsort()]\n'
"cols = ['&lt;80%', '80-90', '&gt;90']\ndf[cols] = df[cols].div(df[cols].sum(axis=1), axis=0).multiply(100)\n"
'train = df1.iloc[:,[4,6]]\n\ndef train(classifier, X, y):\n\narray(&lt;function train at 0x7f3a311320d0&gt;, dtype=object)\n'
"df_A = pd.DataFrame({'start_date':['2017-03-27','2017-01-10'],'end_date':['2017-04-20','2017-02-01']})\ndf_B = pd.DataFrame({'event_date':['2017-01-20','2017-01-27'],'price':[100,200]})\n\ndf_A['end_date'] = pd.to_datetime(df_A.end_date)\ndf_A['start_date'] = pd.to_datetime(df_A.start_date)\ndf_B['event_date'] = pd.to_datetime(df_B.event_date)\n\ndf_A = df_A.assign(key=1)\ndf_B = df_B.assign(key=1)\ndf_merge = pd.merge(df_A, df_B, on='key').drop('key',axis=1)\n\ndf_merge = df_merge.query('event_date &gt;= start_date and event_date &lt;= end_date')\n\ndf_out = df_A.merge(df_merge, on=['start_date','end_date'], how='left').fillna('').drop('key', axis=1)\n\nprint(df_out)\n\n              end_date           start_date           event_date price\n0  2017-04-20 00:00:00  2017-03-27 00:00:00                           \n1  2017-02-01 00:00:00  2017-01-10 00:00:00  2017-01-20 00:00:00   100\n2  2017-02-01 00:00:00  2017-01-10 00:00:00  2017-01-27 00:00:00   200\n"
"In [261]: d = pd.to_datetime(['2011-09-30', '2012-02-28'])\n\nIn [262]: d\nOut[262]: DatetimeIndex(['2011-09-30', '2012-02-28'], dtype='datetime64[ns]', freq=None)\n\nIn [263]: d + pd.offsets.MonthBegin(1)\nOut[263]: DatetimeIndex(['2011-10-01', '2012-03-01'], dtype='datetime64[ns]', freq=None)\n"
"dataset = pandas.read_csv('file.csv', names=names)\n\nnew_dataset = dataset[['A','D']]\n\nnew_dataset = dataset.loc[:, ['A','D']]\n\nnew_dataset = pandas.read_csv('file.csv', names=names, usecols=['A','D'])\n\nnew_dataset = dataset[['A','D']]\n\nnew_dataset = dataset[['A','D']].copy()\n"
'In [49]: df = pd.DataFrame(np.random.rand(10**5,6), columns=list(\'abcdef\'))\n\nIn [50]: %timeit df.loc[random.randint(0, 10**4)]\nThe slowest run took 27.65 times longer than the fastest. This could mean that an intermediate result is being cached.\n1000 loops, best of 3: 331 µs per loop\n\nIn [51]: %timeit df.iloc[random.randint(0, 10**4)]\n1000 loops, best of 3: 275 µs per loop\n\nIn [52]: %timeit df.query("a &gt; 0.9")\n100 loops, best of 3: 7.84 ms per loop\n\nIn [53]: %timeit df.loc[df.a &gt; 0.9]\n100 loops, best of 3: 2.96 ms per loop\n\nIn [54]: df = pd.DataFrame(np.random.rand(10**5,6), columns=list(\'abcdef\'), index=np.random.randint(0, 10000, 10**5))\n\nIn [55]: %timeit df.loc[random.randint(0, 10**4)]\n100 loops, best of 3: 12.3 ms per loop\n\nIn [56]: %timeit df.iloc[random.randint(0, 10**4)]\n1000 loops, best of 3: 262 µs per loop\n\nIn [57]: %timeit df.query("a &gt; 0.9")\n100 loops, best of 3: 7.78 ms per loop\n\nIn [58]: %timeit df.loc[df.a &gt; 0.9]\n100 loops, best of 3: 2.93 ms per loop\n\nIn [64]: df = pd.DataFrame(np.random.rand(10**5,6), columns=list(\'abcdef\'), index=np.random.randint(0, 10000, 10**5)).sort_index()\n\nIn [65]: df.index.is_monotonic_increasing\nOut[65]: True\n\nIn [66]: %timeit df.loc[random.randint(0, 10**4)]\nThe slowest run took 9.70 times longer than the fastest. This could mean that an intermediate result is being cached.\n1000 loops, best of 3: 478 µs per loop\n\nIn [67]: %timeit df.iloc[random.randint(0, 10**4)]\n1000 loops, best of 3: 262 µs per loop\n\nIn [68]: %timeit df.query("a &gt; 0.9")\n100 loops, best of 3: 7.81 ms per loop\n\nIn [69]: %timeit df.loc[df.a &gt; 0.9]\n100 loops, best of 3: 2.95 ms per loop\n'
"&gt;&gt;&gt; df['Tags'] = df.Tags.apply(lambda x: x[1:-1].split(','))\n&gt;&gt;&gt; df.Tags[0]\n['Tag1', 'Tag2']\n"
"a = df_data.query('a.isnull()')\nprint (a)\n    a\n2 NaN\n\nb = df_data.query('a.notnull()')\nprint (b)\n      a\n0   1.0\n1  20.0\n3  40.0\n4  50.0\n\na = df_data.query('a != a')\nprint (a)\n    a\n 2 NaN\n\nb = df_data.query('a == a')\nprint (b)\n      a\n0   1.0\n1  20.0\n3  40.0\n4  50.0\n"
"total_year.set_index('year').plot(figsize=(10,5), grid=True)\n"
"result.to_sql('ds_attribution_probabilities', con=engine, \n              schema='online', index=False, if_exists='append')\n"
"import pandas as pd, numpy as np\n\ndf = pd.DataFrame({'A': np.random.randint(0, 1000, 1000000)})\nlst = df['A'].values.tolist()\n\n##### TEST 1 - Full Map #####\n\nd = {i: i+1 for i in range(1000)}\n\n%timeit df['A'].replace(d)                          # 1.98s\n%timeit df['A'].map(d)                              # 84.3ms\n%timeit [d[i] for i in lst]                         # 134ms\n\n##### TEST 2 - Partial Map #####\n\nd = {i: i+1 for i in range(10)}\n\n%timeit df['A'].replace(d)                          # 20.1ms\n%timeit df['A'].map(d).fillna(df['A']).astype(int)  # 111ms\n%timeit [d.get(i, i) for i in lst]                  # 243ms\n\nitems = list(compat.iteritems(to_replace))\nkeys, values = zip(*items)\nare_mappings = [is_dict_like(v) for v in values]\n\nif any(are_mappings):\n    # handling of nested dictionaries\nelse:\n    to_replace, value = keys, values\n\nreturn self.replace(to_replace, value, inplace=inplace,\n                    limit=limit, regex=regex)\n\nif isinstance(arg, (dict, Series)):\n    if isinstance(arg, dict):\n        arg = self._constructor(arg, index=arg.keys())\n\n    indexer = arg.index.get_indexer(values)\n    new_values = algos.take_1d(arg._values, indexer)\n"
"pd.read_csv('source.txt',header=0, delim_whitespace=True)\n"
"houseitems = []\nfor data in datum:\n    print(data.text)\n    print(data.get('href'))\n    df = {'Title': data.text, 'Url': data.get('href')}\n    houseitems.append(df)\n\nhouseitems = [{'Title': data.text, 'Url': data.get('href')} for data in datum]\n\ndf1 = pd.DataFrame(houseitems)\n"
"x = pd.DataFrame(np.zeros((4, 2)), columns=['A', 'B'])\ny = np.random.randn(4, 2)\nx[['A', 'B']] = y\n\nx = pd.DataFrame(np.zeros((4, 2)), columns=['A', 'B'])\ny = np.random.randn(4, 1)\nx[['A', 'B']] = y\n"
"s1.merge(s2, left_index=True, right_on=['third', 'fourth'])\n#s1.merge(s2, right_index=True, left_on=['first', 'second'])\n\n               s1        s2\nbar one  0.765385 -0.365508\n    two  1.462860  0.751862\nbaz one  0.304163  0.761663\n    two -0.816658 -1.810634\nfoo one  1.891434  1.450081\n    two  0.571294  1.116862\nqux one  1.056516 -0.052927\n    two -0.574916 -1.197596\n"
'df.columns.name = None\n\ndel df.columns.name\n'
'import pandas as pd\n\ndef getlocals(obj, lcls=None):\n    if lcls is None: lcls = dict(locals().items())\n\n    objlcls = {k:v for k,v in obj.__dict__.items() if not k.startswith(\'_\')}\n    lcls.update(objlcls)\n\n    return lcls\n\nx = "[123,DatetimeIndex([\'2018-12-04\',\'2018-12-05\', \'2018-12-06\'],dtype=\'datetime64[ns]\', freq=\'D\')]"\nlcls = getlocals(pd)\n\nresult = eval(x, globals(), lcls)\nprint(result)\n\n[123, DatetimeIndex([\'2018-12-04\', \'2018-12-05\', \'2018-12-06\'], dtype=\'datetime64[ns]\', freq=\'D\')]\n'
'df.columns\n# result:\nIndex([\'LABEL\', \' F1\', \' F2\', \' F3\', \' F4\', \' F5\', \' X\', \' Y\', \' Z\', \' C1\',\n       \' C2\'],\n      dtype=\'object\')\n\ntrain_features = train[[\' F1\',\' F2\',\' F3\',\' F4\',\' F5\',\' X\',\' Y\',\' Z\',\' C1\',\' C2\']] # works OK\n\nimport pandas as pd\ndf= pd.read_csv("lettera.csv", delimiter=\',\', header=None, skiprows=1, names=[\'LABEL\',\'F1\',\'F2\',\'F3\',\'F4\',\'F5\',\'X\',\'Y\',\'Z\',\'C1\',\'C2\'])\n\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, test_size = 0.2)\ntrain_features = train[[\'F1\',\'F2\',\'F3\',\'F4\',\'F5\',\'X\',\'Y\',\'Z\',\'C1\',\'C2\']] # works OK\n'
'idx = df.groupby(&quot;Item&quot;)[&quot;Count&quot;].idxmax()\ndf[&quot;New_Count&quot;] = df[&quot;Count&quot;]\ndf.loc[idx, &quot;New_Count&quot;] += 1\n\nidx = df.groupby(&quot;Item&quot;)[&quot;Count&quot;].transform(max) == df[&quot;Count&quot;]\n'
"df.groupby([df.index.year, df.index.month, df.index.day]).transform(np.cumsum).resample('B', how='ohlc')\n\ndf.groupby(pd.TimeGrouper('D')).transform(np.cumsum).resample('D', how='ohlc')\n\ndf.groupby(pd.Grouper(freq='D')).transform(np.cumsum).resample('D', how='ohlc')\n"
'&gt;&gt;&gt; df.groupby(["score", "type"]).sum()\n                        count\nscore    type                \n9.397000 advanced  537.331573\n9.397995 advanced    9.641728\n9.397996 newbie      0.100000\n9.399900 expert     19.6541374\n&gt;&gt;&gt; df.groupby(["score", "type"], as_index=False).sum()\n      score      type       count\n0  9.397000  advanced  537.331573\n1  9.397995  advanced    9.641728\n2  9.397996    newbie    0.100000\n3  9.399900    expert   19.654137\n'
'pip uninstall numpy\npip uninstall pandas\npip install pandas==0.13.1\n'
"d = {1: [20, 'NYC', 1], 2: [30, 'NYC', 2], 3: [5, 'SF', 3],      \n     4: [300, 'LA', 1], 5: [30, 'LA', 2],  6: [100, 'SF', 3]}\n\ncolumns=['Price', 'City', 'Quantity'] \n# create dataframe and rename columns\n\ndf = pd.DataFrame.from_dict(data=d, orient='index').sort_index()\ndf.columns = columns\n\n&gt;&gt;&gt; df\n   Price City  Quantity\n1     20  NYC         1\n2     30  NYC         2\n3      5   SF         3\n4    300   LA         1\n5     30   LA         2\n6    100   SF         3\n\ndf_new = pd.DataFrame([df.ix[idx] \n                       for idx in df.index \n                       for _ in range(df.ix[idx]['Quantity'])]).reset_index(drop=True)\n&gt;&gt;&gt; df_new\n    Price City  Quantity\n0      20  NYC         1\n1      30  NYC         2\n2      30  NYC         2\n3       5   SF         3\n4       5   SF         3\n5       5   SF         3\n6     300   LA         1\n7      30   LA         2\n8      30   LA         2\n9     100   SF         3\n10    100   SF         3\n11    100   SF         3\n"
'def seasonal_decompose(x, model="additive", filt=None, freq=None):\n    """\n    Parameters\n    ----------\n    x : array-like\n        Time series\n    model : str {"additive", "multiplicative"}\n        Type of seasonal component. Abbreviations are accepted.\n    filt : array-like\n        The filter coefficients for filtering out the seasonal component.\n        The default is a symmetric moving average.\n    freq : int, optional\n        Frequency of the series. Must be used if x is not a pandas\n        object with a timeseries index.\n\nlength = 400\nx = np.sin(np.arange(length)) * 10 + np.random.randn(length)\ndf = pd.DataFrame(data=x, index=pd.date_range(start=datetime(2015, 1, 1), periods=length, freq=\'w\'), columns=[\'value\'])\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 400 entries, 2015-01-04 to 2022-08-28\nFreq: W-SUN\n\ndecomp = sm.tsa.seasonal_decompose(df)\ndata = pd.concat([df, decomp.trend, decomp.seasonal, decomp.resid], axis=1)\ndata.columns = [\'series\', \'trend\', \'seasonal\', \'resid\']\n\nData columns (total 4 columns):\nseries      400 non-null float64\ntrend       348 non-null float64\nseasonal    400 non-null float64\nresid       348 non-null float64\ndtypes: float64(4)\nmemory usage: 15.6 KB\n\ndf = df.iloc[np.unique(np.random.randint(low=0, high=length, size=length * .8))]\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 222 entries, 2015-01-11 to 2022-08-21\nData columns (total 1 columns):\nvalue    222 non-null float64\ndtypes: float64(1)\nmemory usage: 3.5 KB\n\ndf.index.freq\n\nNone\n\ndf.index.inferred_freq\n\nNone\n\ndecomp = sm.tsa.seasonal_decompose(df, freq=52)\n\ndata = pd.concat([df, decomp.trend, decomp.seasonal, decomp.resid], axis=1)\ndata.columns = [\'series\', \'trend\', \'seasonal\', \'resid\']\n\nDatetimeIndex: 224 entries, 2015-01-04 to 2022-08-07\nData columns (total 4 columns):\nseries      224 non-null float64\ntrend       172 non-null float64\nseasonal    224 non-null float64\nresid       172 non-null float64\ndtypes: float64(4)\nmemory usage: 8.8 KB\n\nNotes\n-----\nThis is a naive decomposition. More sophisticated methods should\nbe preferred.\n\nThe additive model is Y[t] = T[t] + S[t] + e[t]\n\nThe multiplicative model is Y[t] = T[t] * S[t] * e[t]\n\nThe seasonal component is first removed by applying a convolution\nfilter to the data. The average of this smoothed series for each\nperiod is the returned seasonal component.\n'
"df[df.columns[::2]]\n\nIn [2]:\ncols = ['a1','b1','c1','a2','b2','c2','a3']\ndf = pd.DataFrame(columns=cols)\ndf\n\nOut[2]:\nEmpty DataFrame\nColumns: [a1, b1, c1, a2, b2, c2, a3]\nIndex: []\n\nIn [3]:\ndf[df.columns[::3]]\nOut[3]:\n\nEmpty DataFrame\nColumns: [a1, a2, a3]\nIndex: []\n\nIn [5]:\na = df.columns[df.columns.str.startswith('a')]\ndf[a]\n\nOut[5]:\nEmpty DataFrame\nColumns: [a1, a2, a3]\nIndex: []\n\nIn [19]:\ndf.columns.str.extract(r'([a-zA-Z])').unique()\n\nOut[19]:\narray(['a', 'b', 'c'], dtype=object)\n"
"In [67]:\ndf = pd.DataFrame(np.random.randn(5,3), columns=list('ABC'))\ndf\n\nOut[67]:\n          A         B         C\n0  0.197334  0.707852 -0.443475\n1 -1.063765 -0.914877  1.585882\n2  0.899477  1.064308  1.426789\n3 -0.556486 -0.150080 -0.149494\n4 -0.035858  0.777523 -0.453747\n\nIn [73]:    \ndf['total'] = df.loc[df['A'] &gt; 0,['A','B']].sum(axis=1)\ndf['total'].fillna(0, inplace=True)\ndf\n\nOut[73]:\n          A         B         C     total\n0  0.197334  0.707852 -0.443475  0.905186\n1 -1.063765 -0.914877  1.585882  0.000000\n2  0.899477  1.064308  1.426789  1.963785\n3 -0.556486 -0.150080 -0.149494  0.000000\n4 -0.035858  0.777523 -0.453747  0.000000\n\nIn [75]:\ndf['total'] = df[['A','B']].sum(axis=1).where(df['A'] &gt; 0, 0)\ndf\n\nOut[75]:\n          A         B         C     total\n0  0.197334  0.707852 -0.443475  0.905186\n1 -1.063765 -0.914877  1.585882  0.000000\n2  0.899477  1.064308  1.426789  1.963785\n3 -0.556486 -0.150080 -0.149494  0.000000\n4 -0.035858  0.777523 -0.453747  0.000000\n"
'self.assert_frame_equal(pd.DataFrame([0,0,0,0]),pd.DataFrame([0,0,0,0]))\n\nassert_frame_equal(pd.DataFrame([0,0,0,0]), pd.DataFrame([0,0,0,0]))\n'
"def group_duplicate_cols(df):\n    a = df.values\n    sidx = np.lexsort(a)\n    b = a[:,sidx]\n\n    m = np.concatenate(([False], (b[:,1:] == b[:,:-1]).all(0), [False] ))\n    idx = np.flatnonzero(m[1:] != m[:-1])\n    C = df.columns[sidx].tolist()\n    return [C[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]\n\nIn [100]: df\nOut[100]: \n    A  B  C  D  E  F\na1  1  2  1  2  3  1\na2  2  4  2  4  4  1\na3  3  2  3  2  2  1\na4  4  1  4  1  1  1\na5  5  9  5  9  2  1\n\nIn [101]: group_duplicate_cols(df)\nOut[101]: [['A', 'C'], ['B', 'D']]\n\n# Let's add one more duplicate into group containing 'A'\nIn [102]: df.F = df.A\n\nIn [103]: group_duplicate_cols(df)\nOut[103]: [['A', 'C', 'F'], ['B', 'D']]\n\ndef group_duplicate_rows(df):\n    a = df.values\n    sidx = np.lexsort(a.T)\n    b = a[sidx]\n\n    m = np.concatenate(([False], (b[1:] == b[:-1]).all(1), [False] ))\n    idx = np.flatnonzero(m[1:] != m[:-1])\n    C = df.index[sidx].tolist()\n    return [C[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]\n\nIn [260]: df2\nOut[260]: \n   a1  a2  a3  a4  a5\nA   3   5   3   4   5\nB   1   1   1   1   1\nC   3   5   3   4   5\nD   2   9   2   1   9\nE   2   2   2   1   2\nF   1   1   1   1   1\n\nIn [261]: group_duplicate_rows(df2)\nOut[261]: [['B', 'F'], ['A', 'C']]\n\n# @John Galt's soln-1\nfrom itertools import combinations\ndef combinations_app(df):\n    return[x for x in combinations(df.columns, 2) if (df[x[0]] == df[x[-1]]).all()]\n\n# @Abdou's soln\ndef pandas_groupby_app(df):\n    return [tuple(d.index) for _,d in df.T.groupby(list(df.T.columns)) if len(d) &gt; 1]                        \n\n# @COLDSPEED's soln\ndef triu_app(df):\n    c = df.columns.tolist()\n    i, j = np.triu_indices(len(c), 1)\n    x = [(c[_i], c[_j]) for _i, _j in zip(i, j) if (df[c[_i]] == df[c[_j]]).all()]\n    return x\n\n# @cmaher's soln\ndef lambda_set_app(df):\n    return list(filter(lambda x: len(x) &gt; 1, list(set([tuple([x for x in df.columns if all(df[x] == df[y])]) for y in df.columns]))))\n\nIn [179]: # Setup inputs with sizes as mentioned in the question\n     ...: df = pd.DataFrame(np.random.randint(0,10,(8000,500)))\n     ...: df.columns = ['C'+str(i) for i in range(df.shape[1])]\n     ...: idx0 = np.random.choice(df.shape[1], df.shape[1]//2,replace=0)\n     ...: idx1 = np.random.choice(df.shape[1], df.shape[1]//2,replace=0)\n     ...: df.iloc[:,idx0] = df.iloc[:,idx1].values\n     ...: \n\n# @John Galt's soln-1\nIn [180]: %timeit combinations_app(df)\n1 loops, best of 3: 24.6 s per loop\n\n# @Abdou's soln\nIn [181]: %timeit pandas_groupby_app(df)\n1 loops, best of 3: 3.81 s per loop\n\n# @COLDSPEED's soln\nIn [182]: %timeit triu_app(df)\n1 loops, best of 3: 25.5 s per loop\n\n# @cmaher's soln\nIn [183]: %timeit lambda_set_app(df)\n1 loops, best of 3: 27.1 s per loop\n\n# Proposed in this post\nIn [184]: %timeit group_duplicate_cols(df)\n10 loops, best of 3: 188 ms per loop\n\ndef view1D(a): # a is array\n    a = np.ascontiguousarray(a)\n    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\n    return a.view(void_dt).ravel()\n\ndef group_duplicate_cols_v2(df):\n    a = df.values\n    sidx = view1D(a.T).argsort()\n    b = a[:,sidx]\n\n    m = np.concatenate(([False], (b[:,1:] == b[:,:-1]).all(0), [False] ))\n    idx = np.flatnonzero(m[1:] != m[:-1])\n    C = df.columns[sidx].tolist()\n    return [C[i:j] for i,j in zip(idx[::2],idx[1::2]+1)]\n\nIn [322]: %timeit group_duplicate_cols(df)\n10 loops, best of 3: 185 ms per loop\n\nIn [323]: %timeit group_duplicate_cols_v2(df)\n10 loops, best of 3: 69.3 ms per loop\n"
"In [32]: from datetime import datetime as dt\n\nIn [33]: dr = p.DateRange(dt(2009,1,1),dt(2010,12,31), offset=p.datetools.Hour())\n\nIn [34]: hr = dr.map(lambda x: x.hour)\n\nIn [35]: dt = p.DataFrame(rand(len(dr),2), dr)\n\nIn [36]: dt \n\nOut[36]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDateRange: 17497 entries, 2009-01-01 00:00:00 to 2010-12-31 00:00:00\noffset: &lt;1 Hour&gt;\nData columns:\n0    17497  non-null values\n1    17497  non-null values\ndtypes: float64(2)\n\nIn [37]: dt[(hr &gt;= 10) &amp; (hr &lt;=16)]\n\nOut[37]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 5103 entries, 2009-01-01 10:00:00 to 2010-12-30 16:00:00\nData columns:\n0    5103  non-null values\n1    5103  non-null values\ndtypes: float64(2)\n"
"In [3]: s.to_csv('/home/wesm/tmp/sfoo.csv')\n\nIn [4]: Series.from_csv('/home/wesm/tmp/sfoo.csv')\nOut[4]: \na    1\nb    2\n"
"In [2]: df.set_index(['Name', 'Destination'])\nOut[2]: \n                   Length\nName  Destination        \nBob   Athens            3\n      Rome              5\n      Athens            2\nAlice Rome              1\n      Athens            3\n      Rome              5\n"
"ts.between_time(datetime.time(18), datetime.time(9), include_start=False, include_end=False)\n\nts.ix[ts.index.indexer_between_time(datetime.time(9), datetime.time(18))]\n\nts.ix[ts.index.indexer_between_time(datetime.time(18), datetime.time(9),\n                                    include_start=False, include_end=False)]\n\nIn [1]: rng = pd.date_range('1/1/2000', periods=24, freq='H')\n\nIn [2]: ts = pd.Series(pd.np.random.randn(len(rng)), index=rng)\n\nIn [3]: ts.ix[ts.index.indexer_between_time(datetime.time(10), datetime.time(14))] \nOut[3]: \n2000-01-01 10:00:00    1.312561\n2000-01-01 11:00:00   -1.308502\n2000-01-01 12:00:00   -0.515339\n2000-01-01 13:00:00    1.536540\n2000-01-01 14:00:00    0.108617\n\nIn [4]: df = pd.DataFrame(ts)\n\nIn [5]: df.ix[df.index.indexer_between_time(datetime.time(10), datetime.time(14))]\nOut[5]: \n                            0\n2000-01-03 10:00:00  1.312561\n2000-01-03 11:00:00 -1.308502\n2000-01-03 12:00:00 -0.515339\n2000-01-03 13:00:00  1.536540\n2000-01-03 14:00:00  0.108617\n"
"In [11]: df\nOut[11]: \n              c1        c2\nfirst   0.821354  0.936703\nsecond  0.138376  0.482180\n\nIn [12]: print df.to_latex()\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n{} &amp;        c1 &amp;        c2 \\\\\n\\hline\nfirst  &amp;  0.821354 &amp;  0.936703 \\\\\nsecond &amp;  0.138376 &amp;  0.482180 \\\\\n\\hline\n\\end{tabular}\n\ndef f1(x):\n    return 'blah_%1.2f' % x\n\ndef f2(x):\n    return 'f2_%1.2f' % x\n\nIn [15]: print df.to_latex(formatters=[f1, f2])\n\\begin{tabular}{|l|c|c|c|}\n\\hline\n{} &amp;        c1 &amp;      c2 \\\\\n\\hline\nfirst  &amp; blah\\_0.82 &amp; f2\\_0.94 \\\\\nsecond &amp; blah\\_0.14 &amp; f2\\_0.48 \\\\\n\\hline\n\\end{tabular}\n"
"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer \n\ndef fn_tdm_df(docs, xColNames = None, **kwargs):\n    ''' create a term document matrix as pandas DataFrame\n    with **kwargs you can pass arguments of CountVectorizer\n    if xColNames is given the dataframe gets columns Names'''\n\n    #initialize the  vectorizer\n    vectorizer = CountVectorizer(**kwargs)\n    x1 = vectorizer.fit_transform(docs)\n    #create dataFrame\n    df = pd.DataFrame(x1.toarray().transpose(), index = vectorizer.get_feature_names())\n    if xColNames is not None:\n        df.columns = xColNames\n\n    return df\n\nDIR = 'C:/Data/'\n\ndef fn_CorpusFromDIR(xDIR):\n    ''' functions to create corpus from a Directories\n    Input: Directory\n    Output: A dictionary with \n             Names of files ['ColNames']\n             the text in corpus ['docs']'''\n    import os\n    Res = dict(docs = [open(os.path.join(xDIR,f)).read() for f in os.listdir(xDIR)],\n               ColNames = map(lambda x: 'P_' + x[0:6], os.listdir(xDIR)))\n    return Res\n\nd1 = fn_tdm_df(docs = fn_CorpusFromDIR(DIR)['docs'],\n          xColNames = fn_CorpusFromDIR(DIR)['ColNames'], \n          stop_words=None, charset_error = 'replace')  \n"
"from datetime import datetime\nimport traces\n\nts = traces.TimeSeries(data=[\n    (datetime(2016, 9, 27, 23, 0, 0, 100000), 10),\n    (datetime(2016, 9, 27, 23, 0, 1, 200000), 8),\n    (datetime(2016, 9, 27, 23, 0, 1, 600000), 0),\n    (datetime(2016, 9, 27, 23, 0, 6, 300000), 4),\n])\n\nregularized = ts.moving_average(\n    start=datetime(2016, 9, 27, 23, 0, 1),\n    sampling_period=1,\n    placement='left',\n)\n\n[(datetime(2016, 9, 27, 23, 0, 1), 5.2),\n (datetime(2016, 9, 27, 23, 0, 2), 0.0),\n (datetime(2016, 9, 27, 23, 0, 3), 0.0),\n (datetime(2016, 9, 27, 23, 0, 4), 0.0),\n (datetime(2016, 9, 27, 23, 0, 5), 0.0),\n (datetime(2016, 9, 27, 23, 0, 6), 2.8)]\n"
"In [30]: pandas.to_datetime(data['date'])\nOut[30]: \n0   2012-02-13 00:00:00\n1   2012-02-14 00:00:00\nName: date, dtype: datetime64[ns]\n"
'In [90]: s[s==12]\nOut[90]: \nd    12\ndtype: int64\n\nIn [91]: s[s==12].index\nOut[91]: Index([d], dtype=object)\n'
"g = data.groupby(...)\nsize = g.size()\nsize[size &gt; 3]\n\nIn [11]: df = pd.DataFrame([[1, 2], [3, 4], [1,6]], columns=['A', 'B'])\n\nIn [12]: df\nOut[12]:\n   A  B\n0  1  2\n1  3  4\n2  1  6 \n\nIn [13]: g = df.groupby('A')\n\nIn [14]: size = g.size()\n\nIn [15]: size[size &gt; 1]\nOut[15]:\nA\n1    2\ndtype: int64\n\nIn [21]: g.filter(lambda x: len(x) &gt; 1)\nOut[21]:\n   A  B\n0  1  2\n2  1  6\n"
'df = pd.DataFrame(np.random.rand(10))\n\ndf.loc[np.random.choice(df.index, 5, replace=False)]\n\ndf.loc[np.random.permutation(df.index)[:5]]\n'
"df['group'].plot(kind='bar', color=['r', 'g', 'b', 'r', 'g', 'b', 'r'])\n\ncolors = {1: 'r', 2: 'b', 3: 'g'}\ndf['value'].plot(kind='bar', color=[colors[i] for i in df['group']])\n\nlist(df['group'].map(colors))\n"
" In [7]: index = date_range('20131009 08:30','20131010 10:05',freq='5T')\n\nIn [8]: df = DataFrame(randn(len(index),2),columns=list('AB'),index=index)\n\nIn [9]: df\nOut[9]: \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 308 entries, 2013-10-09 08:30:00 to 2013-10-10 10:05:00\nFreq: 5T\nData columns (total 2 columns):\nA    308  non-null values\nB    308  non-null values\ndtypes: float64(2)\n\nIn [10]: df.between_time('9:00','10:00')\nOut[10]: \n                            A         B\n2013-10-09 09:00:00 -0.664639  1.597453\n2013-10-09 09:05:00  1.197290 -0.500621\n2013-10-09 09:10:00  1.470186 -0.963553\n2013-10-09 09:15:00  0.181314 -0.242415\n2013-10-09 09:20:00  0.969427 -1.156609\n2013-10-09 09:25:00  0.261473  0.413926\n2013-10-09 09:30:00 -0.003698  0.054953\n2013-10-09 09:35:00  0.418147 -0.417291\n2013-10-09 09:40:00  0.413565 -1.096234\n2013-10-09 09:45:00  0.460293  1.200277\n2013-10-09 09:50:00 -0.702444 -0.041597\n2013-10-09 09:55:00  0.548385 -0.832382\n2013-10-09 10:00:00 -0.526582  0.758378\n2013-10-10 09:00:00  0.926738  0.178204\n2013-10-10 09:05:00 -1.178534  0.184205\n2013-10-10 09:10:00  1.408258  0.948526\n2013-10-10 09:15:00  0.523318  0.327390\n2013-10-10 09:20:00 -0.193174  0.863294\n2013-10-10 09:25:00  1.355610 -2.160864\n2013-10-10 09:30:00  1.930622  0.174683\n2013-10-10 09:35:00  0.273551  0.870682\n2013-10-10 09:40:00  0.974756 -0.327763\n2013-10-10 09:45:00  1.808285  0.080267\n2013-10-10 09:50:00  0.842119  0.368689\n2013-10-10 09:55:00  1.065585  0.802003\n2013-10-10 10:00:00 -0.324894  0.781885\n"
'from io import StringIO\nimport pandas as pd\ndata = StringIO(u"""\n121301234\n121300123\n121300012\n""")\n\npd.read_fwf(data, colspecs=[(0,3),(4,8)], converters = {1: str})\n\n    \\n Unnamed: 1\n0  121       0123\n1  121       0012\n2  121       0001\n'
"import numpy as np\nimport pandas as pd\na = np.arange(10).reshape(2,5)\nb = np.arange(10, 20).reshape(2,5)\npd.DataFrame({'foo':[42,51], 'arr':[a,b]})\nOut[10]: \n                                            arr  foo\n0            [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]   42\n1  [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]   51\n"
"In [19]: df = DataFrame(randn(5, 2), columns=['a', 'b'])\n\nIn [20]: df\nOut[20]: \n          a         b\n0 -1.949107 -0.763762\n1 -0.382173 -0.970349\n2  0.202116  0.094344\n3 -1.225579 -0.447545\n4  1.739508 -0.400829\n\nIn [21]: formulas = [ ('c','a+b'), ('d', 'a*c')]\n\nIn [22]: def lazy(x, formulas):\n   ....:     for col, f in formulas:\n   ....:         x[col] = x.eval(f)\n   ....:         yield x\n   ....:         \n\nIn [23]: gen = lazy(df,formulas)\n\nIn [24]: gen.next()\nOut[24]: \n          a         b         c\n0 -1.949107 -0.763762 -2.712869\n1 -0.382173 -0.970349 -1.352522\n2  0.202116  0.094344  0.296459\n3 -1.225579 -0.447545 -1.673123\n4  1.739508 -0.400829  1.338679\n\nIn [25]: gen.next()\nOut[25]: \n          a         b         c         d\n0 -1.949107 -0.763762 -2.712869  5.287670\n1 -0.382173 -0.970349 -1.352522  0.516897\n2  0.202116  0.094344  0.296459  0.059919\n3 -1.225579 -0.447545 -1.673123  2.050545\n4  1.739508 -0.400829  1.338679  2.328644\n"
"encode cat, generate(cat2)\n\nx['cat2'] = x['cat'].astype('category')\n\n  cat  val cat2\n0   A   10    A\n1   A   20    A\n2   B   30    B\n\nx['cat2'].cat.codes\n\n0    0\n1    0\n2    1\n"
"import numpy as np\nimport pandas as pd\n\nindex = ['x', 'y']\ncolumns = ['a','b','c']\n\n# Option 1: Set the column names in the structured array's dtype \ndtype = [('a','int32'), ('b','float32'), ('c','float32')]\nvalues = np.zeros(2, dtype=dtype)\ndf = pd.DataFrame(values, index=index)\n\n# Option 2: Alter the structured array's column names after it has been created\nvalues = np.zeros(2, dtype='int32, float32, float32')\nvalues.dtype.names = columns\ndf2 = pd.DataFrame(values, index=index, columns=columns)\n\n# Option 3: Alter the DataFrame's column names after it has been created\nvalues = np.zeros(2, dtype='int32, float32, float32')\ndf3 = pd.DataFrame(values, index=index)\ndf3.columns = columns\n\n# Option 4: Use a dict of arrays, each of the right dtype:\ndf4 = pd.DataFrame(\n    {'a': np.zeros(2, dtype='int32'),\n     'b': np.zeros(2, dtype='float32'),\n     'c': np.zeros(2, dtype='float32')}, index=index, columns=columns)\n\n# Option 5: Concatenate DataFrames of the simple dtypes:\ndf5 = pd.concat([\n    pd.DataFrame(np.zeros((2,), dtype='int32'), columns=['a']), \n    pd.DataFrame(np.zeros((2,2), dtype='float32'), columns=['b','c'])], axis=1)\n\n# Option 6: Alter the dtypes after the DataFrame has been formed. (This is not very efficient)\nvalues2 = np.zeros((2, 3))\ndf6 = pd.DataFrame(values2, index=index, columns=columns)\nfor col, dtype in zip(df6.columns, 'int32 float32 float32'.split()):\n    df6[col] = df6[col].astype(dtype)\n\n   a  b  c\nx  0  0  0\ny  0  0  0\n\na      int32\nb    float32\nc    float32\ndtype: object\n\nIn [171]:  values\nOut[172]: \narray([(0, 0.0, 0.0), (0, 0.0, 0.0)], \n      dtype=[('f0', '&lt;i4'), ('f1', '&lt;f4'), ('f2', '&lt;f4')])\n"
"px2 = px.reshape((-1,3))\ndf = pd.DataFrame({'R':px2[:,0],'G':px2[:,1],'B':px2[:,2]})\n"
"from rpy2.robjects import pandas2ri\npandas2ri.activate()\n\nfrom rpy2.robjects.packages import importr\n\nbase = importr('base')\n# call an R function on a Pandas DataFrame\nbase.summary(my_pandas_dataframe)\n"
'from sklearn import datasets\nimport pandas\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import cross_validation\nfrom sklearn.metrics import confusion_matrix\n\n\ndef get_enhanced_confusion_matrix(actuals, predictions, labels):\n    """"enhances confusion_matrix by adding sensivity and specificity metrics"""\n    cm = confusion_matrix(actuals, predictions, labels = labels)\n    sensitivity = float(cm[1][1]) / float(cm[1][0]+cm[1][1])\n    specificity = float(cm[0][0]) / float(cm[0][0]+cm[0][1])\n    weightedAccuracy = (sensitivity * 0.9) + (specificity * 0.1)\n    return cm, sensitivity, specificity, weightedAccuracy\n\niris = datasets.load_iris()\nx=pandas.DataFrame(iris.data, columns=[\'var1\',\'var2\',\'var3\', \'var4\'])\ny=pandas.Series(iris.target, name=\'target\')\n\nresponse, _  = pandas.factorize(y)\n\nxTrain, xTest, yTrain, yTest = cross_validation.train_test_split(x, response, test_size = .25, random_state = 36583)\nprint "building the first forest"\nrf = RandomForestClassifier(n_estimators = 500, min_samples_split = 2, n_jobs = -1, verbose = 1)\nrf.fit(xTrain, yTrain)\nimportances = pandas.DataFrame({\'name\':x.columns,\'imp\':rf.feature_importances_\n                                }).sort([\'imp\'], ascending = False).reset_index(drop = True)\n\ncm, sensitivity, specificity, weightedAccuracy = get_enhanced_confusion_matrix(yTest, rf.predict(xTest), [0,1])\nnumFeatures = len(x.columns)\n\nrfeMatrix = pandas.DataFrame({\'numFeatures\':[numFeatures], \n                              \'weightedAccuracy\':[weightedAccuracy], \n                              \'sensitivity\':[sensitivity], \n                              \'specificity\':[specificity]})\n\nprint "running RFE on  %d features"%numFeatures\n\nfor i in range(1,numFeatures,1):\n    varsUsed = importances[\'name\'][0:i]\n    print "now using %d of %s features"%(len(varsUsed), numFeatures)\n    xTrain, xTest, yTrain, yTest = cross_validation.train_test_split(x[varsUsed], response, test_size = .25)\n    rf = RandomForestClassifier(n_estimators = 500, min_samples_split = 2,\n                                n_jobs = -1, verbose = 1)\n    rf.fit(xTrain, yTrain)\n    cm, sensitivity, specificity, weightedAccuracy = get_enhanced_confusion_matrix(yTest, rf.predict(xTest), [0,1])\n    print("\\n"+str(cm))\n    print(\'the sensitivity is %d percent\'%(sensitivity * 100))\n    print(\'the specificity is %d percent\'%(specificity * 100))\n    print(\'the weighted accuracy is %d percent\'%(weightedAccuracy * 100))\n    rfeMatrix = rfeMatrix.append(\n                                pandas.DataFrame({\'numFeatures\':[len(varsUsed)], \n                                \'weightedAccuracy\':[weightedAccuracy], \n                                \'sensitivity\':[sensitivity], \n                                \'specificity\':[specificity]}), ignore_index = True)    \nprint("\\n"+str(rfeMatrix))    \nmaxAccuracy = rfeMatrix.weightedAccuracy.max()\nmaxAccuracyFeatures = min(rfeMatrix.numFeatures[rfeMatrix.weightedAccuracy == maxAccuracy])\nfeaturesUsed = importances[\'name\'][0:maxAccuracyFeatures].tolist()\n\nprint "the final features used are %s"%featuresUsed\n'
'for cols in data.columns.tolist()[1:]:\n    data = data.ix[data[cols] &gt; 0]\n'
"df = pd.read_csv('test.csv')\ndf['date'] = pd.to_datetime(df['date'])    \ndf['date_delta'] = (df['date'] - df['date'].min())  / np.timedelta64(1,'D')\ncity_data = df[df['city'] == 'London']\nresult = sm.ols(formula = 'sales ~ date_delta', data = city_data).fit()\n"
"    b   c   d  e\na               \n2   2   6   1  3\n2   4   8 NaN  7\n2   4   4   6  3\n3   5 NaN   2  6\n4 NaN NaN   4  1\n5   6   2   1  8\n7   3   2   4  7\n9   6   1 NaN  1\n9 NaN NaN   9  3\n9   3   4   6  1\n\nIn[335]: df.groupby('a').mean()\nOut[333]: \n          b    c    d         e\na                              \n2  3.333333  6.0  3.5  4.333333\n3  5.000000  NaN  2.0  6.000000\n4       NaN  NaN  4.0  1.000000\n5  6.000000  2.0  1.0  8.000000\n7  3.000000  2.0  4.0  7.000000\n9  4.500000  2.5  7.5  1.666667\n\nIn[340]: df.groupby('a')['b'].agg({'foo': np.mean})\nOut[338]: \n        foo\na          \n2  3.333333\n3  5.000000\n4       NaN\n5  6.000000\n7  3.000000\n9  4.500000\n"
"$ pip install tabulate\n\nfrom tabulate import tabulate\ndf = pd.DataFrame ({'Text': ['abcdef', 'x'], 'Value': [12.34, 4.2]})\nprint(tabulate(df, showindex=False, headers=df.columns))\n\nText      Value\n------  -------\nabcdef    12.34\nx          4.2\n"
'pip install numpy==1.9.2\nRequirement already satisfied (use --upgrade to upgrade): numpy==1.9.2 in /Library/Python/2.7/site-packages\nCleaning up...\n\n$ python\nPython 2.7.6 (default, Sep  9 2014, 15:04:36) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType "help", "copyright", "credits" or "license" for more information.\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; numpy.__file__\n\'/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy/__init__.pyc\'\n&gt;&gt;&gt; numpy.version.version\n\'1.8.0rc1\'\n\nmv /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/numpy_bak\n\n$ python\nPython 2.7.6 (default, Sep  9 2014, 15:04:36) \n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType "help", "copyright", "credits" or "license" for more information.\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; numpy.__file__\n\'/Library/Python/2.7/site-packages/numpy/__init__.pyc\'\n&gt;&gt;&gt; numpy.version.version\n\'1.9.2\'\n'
"df.sub(df.mean(axis=1), axis=0)\n\n&gt;&gt;&gt; df = pd.DataFrame({'a': [1.5, 2.5], 'b': [0.25, 2.75], 'c': [1.25, 0.75]})\n&gt;&gt;&gt; df\n     a     b     c\n0  1.5  0.25  1.25\n1  2.5  2.75  0.75\n\n&gt;&gt;&gt; df.mean(axis=1)\n0    1\n1    2\ndtype: float64\n\n&gt;&gt;&gt; df.sub(df.mean(axis=1), axis=0)\n     a     b     c\n0  0.5 -0.75  0.25\n1  0.5  0.75 -1.25\n"
"import pandas as pd\n\n# Some sample data to plot.\nlist_data = [30, 40, 50, 40, 20, 10, 5]\n\n# Create a Pandas dataframe from the data.\ndf = pd.DataFrame(list_data)\n\n# Create a Pandas Excel writer using XlsxWriter as the engine.\nexcel_file = 'testfile.xlsx'\nsheet_name = 'Sheet1'\n\nwriter = pd.ExcelWriter(excel_file, engine='xlsxwriter')\ndf.to_excel(writer, sheet_name=sheet_name)\n\n# Access the XlsxWriter workbook and worksheet objects from the dataframe.\n# This is equivalent to the following using XlsxWriter on its own:\n#\n#    workbook = xlsxwriter.Workbook('filename.xlsx')\n#    worksheet = workbook.add_worksheet()\n#\nworkbook = writer.book\nworksheet = writer.sheets[sheet_name]\n\n# Apply a conditional format to the cell range.\nworksheet.conditional_format('B2:B8', {'type': '3_color_scale'})\n\n# Close the Pandas Excel writer and output the Excel file.\nwriter.save()\n"
'&gt;&gt;&gt; pd.io.parsers.read_csv("tmp.csv",sep="\\t",index_col=0)\n        x     y     z\nbar  0.55  0.55  0.00\nfoo  0.30  0.40  0.10\nqux  0.00  0.30  5.55\n'
"df[(df['Delivery Date'].dt.year == 1970) | (df['Delivery Date'] &gt;= sixmonthago)]\n"
'df2 = df.pivot(columns=df.columns[0], index=df.index)\ndf2.columns = df2.columns.droplevel()\n\n&gt;&gt;&gt; df2\n0          0         1         2\n0   0.040158       NaN       NaN\n1        NaN       NaN  0.500642\n2   0.005694       NaN       NaN\n3        NaN  0.065052       NaN\n4   0.034789       NaN       NaN\n5        NaN       NaN  0.128495\n6        NaN  0.088816       NaN\n7        NaN  0.056725       NaN\n8  -0.000193       NaN       NaN\n9        NaN       NaN -0.070252\n10       NaN       NaN  0.138282\n11       NaN       NaN  0.054638\n12       NaN       NaN  0.039994\n13       NaN       NaN  0.060659\n14  0.038562       NaN       NaN\n\ndf2.boxplot()\n'
'for r in all_data.columns.values:\n    all_data[r] = all_data[r].map(str)\n    all_data[r] = all_data[r].map(str.strip)   \ntuples = [tuple(x) for x in all_data.values]\n\nstring_list = [\'NaT\', \'nan\', \'NaN\', \'None\']\n\ndef remove_wrong_nulls(x):\n    for r in range(len(x)):\n        for i,e in enumerate(tuples):\n            for j,k in enumerate(e):\n                if k == x[r]:\n                    temp=list(tuples[i])\n                    temp[j]=None\n                    tuples[i]=tuple(temp)\n\nremove_wrong_nulls(string_list)\n\ncnxn=ceODBC.connect(\'DRIVER={SOMEODBCDRIVER};DBCName=XXXXXXXXXXX;UID=XXXXXXX;PWD=XXXXXXX;QUIETMODE=YES;\', autocommit=False)\ncursor = cnxn.cursor()\n\ndef chunks(l, n):\n    n = max(1, n)\n    return [l[i:i + n] for i in range(0, len(l), n)]\n\nnew_list = chunks(tuples, 1000)\n\nquery = """insert into XXXXXXXXXXXX("XXXXXXXXXX", "XXXXXXXXX", "XXXXXXXXXXX") values(?,?,?)"""\n\nfor i in range(len(new_list)):\n    cursor.executemany(query, new_list[i])\ncnxn.commit()\ncnxn.close()\n'
'In [54]: df.to_hdf(\'test.h5\',\'df\',mode=\'w\',format=\'table\',data_columns=True)\n\nIn [55]: h = h5py.File(\'test.h5\')\n\nIn [56]: h[\'df\'][\'table\']\nOut[56]: &lt;HDF5 dataset "table": shape (7,), type "|V32"&gt;\n\nIn [64]: h[\'df\'][\'table\'][:]\nOut[64]: \narray([(1, nan, 0.2, nan), (2, nan, nan, 0.5), (3, nan, 0.2, 0.5),\n       (4, 0.1, 0.2, nan), (5, 0.1, 0.2, 0.5), (6, 0.1, nan, 0.5),\n       (7, 0.1, nan, nan)], \n      dtype=[(\'index\', \'&lt;i8\'), (\'A\', \'&lt;f8\'), (\'B\', \'&lt;f8\'), (\'C\', \'&lt;f8\')])\n\n\nIn [57]: h[\'df\'][\'table\'].attrs.items()\nOut[57]: \n[(u\'CLASS\', \'TABLE\'),\n (u\'VERSION\', \'2.7\'),\n (u\'TITLE\', \'\'),\n (u\'FIELD_0_NAME\', \'index\'),\n (u\'FIELD_1_NAME\', \'A\'),\n (u\'FIELD_2_NAME\', \'B\'),\n (u\'FIELD_3_NAME\', \'C\'),\n (u\'FIELD_0_FILL\', 0),\n (u\'FIELD_1_FILL\', 0.0),\n (u\'FIELD_2_FILL\', 0.0),\n (u\'FIELD_3_FILL\', 0.0),\n (u\'index_kind\', \'integer\'),\n (u\'A_kind\', "(lp1\\nS\'A\'\\na."),\n (u\'A_meta\', \'N.\'),\n (u\'A_dtype\', \'float64\'),\n (u\'B_kind\', "(lp1\\nS\'B\'\\na."),\n (u\'B_meta\', \'N.\'),\n (u\'B_dtype\', \'float64\'),\n (u\'C_kind\', "(lp1\\nS\'C\'\\na."),\n (u\'C_meta\', \'N.\'),\n (u\'C_dtype\', \'float64\'),\n (u\'NROWS\', 7)]\n\nIn [58]: h.close()\n'
'from IPython.display import display\n\ndef show_more(df, lines):\n    foo = 1\n    display(df)\n    foo = 2\n\n&gt;&gt; show_more(df, 1000)\n... # &lt;- Shows here the DF\n'
'In [136]:\ndf.apply(lambda x: [x.dropna()], axis=1).to_json()\n\nOut[136]:\n\'{"0":[{"a":1.0,"b":4.0,"c":7.0}],"1":[{"b":5.0}],"2":[{"a":3.0}]}\'\n\nIn [138]:\ndf.apply(lambda x: pd.Series(x.dropna()), axis=1).to_json()\n\nOut[138]:\n\'{"a":{"0":1.0,"1":null,"2":3.0},"b":{"0":4.0,"1":5.0,"2":null},"c":{"0":7.0,"1":null,"2":null}}\'\n\nIn [137]:\ndf.apply(lambda x: list(x.dropna()), axis=1).to_json()\n\nOut[137]:\n\'{"a":{"0":1.0,"1":5.0,"2":3.0},"b":{"0":4.0,"1":5.0,"2":3.0},"c":{"0":7.0,"1":5.0,"2":3.0}}\'\n'
"import pandas as pd\nimport numpy as np\n\ndate_rng = pd.date_range('2015-01-01', periods=200, freq='D')\n\ndf1 = pd.DataFrame(np.random.randn(100, 3), columns='A B C'.split(), index=date_rng[:100])\nOut[410]: \n                 A       B       C\n2015-01-01  0.2799  0.4416 -0.7474\n2015-01-02 -0.4983  0.1490 -0.2599\n2015-01-03  0.4101  1.2622 -1.8081\n2015-01-04  1.1976 -0.7410  0.4221\n2015-01-05  1.3311  1.0399  2.2701\n...            ...     ...     ...\n2015-04-06 -0.0432  0.6131 -0.0216\n2015-04-07  0.4224 -1.1565  2.2285\n2015-04-08  0.0663  1.2994  2.0322\n2015-04-09  0.1958 -0.4412  0.3924\n2015-04-10  0.1622  1.7603  1.4525\n\n[100 rows x 3 columns]\n\n\ndf2 = pd.DataFrame(np.random.randn(100, 3), columns='A B C'.split(), index=date_rng[100:])\nOut[411]: \n                 A       B       C\n2015-04-11  1.1196 -1.9627  0.6615\n2015-04-12 -0.0098  1.7655  0.0447\n2015-04-13 -1.7318 -2.0296  0.8384\n2015-04-14 -1.5472 -1.7220 -0.3166\n2015-04-15  2.5058  0.6487  1.0994\n...            ...     ...     ...\n2015-07-15 -1.4803  2.1703 -1.9391\n2015-07-16 -1.7595 -1.7647 -1.0622\n2015-07-17  1.7900  0.2280 -1.8797\n2015-07-18  0.7909 -0.4999  0.3848\n2015-07-19  1.2243  0.4681 -1.2323\n\n[100 rows x 3 columns]\n\n# to move one row from df2 to df1, use .loc to enlarge df1\n# this is far more efficient than pd.concat and pd.append\ndf1.loc[df2.index[0]] = df2.iloc[0]\n\nOut[413]: \n                 A       B       C\n2015-01-01  0.2799  0.4416 -0.7474\n2015-01-02 -0.4983  0.1490 -0.2599\n2015-01-03  0.4101  1.2622 -1.8081\n2015-01-04  1.1976 -0.7410  0.4221\n2015-01-05  1.3311  1.0399  2.2701\n...            ...     ...     ...\n2015-04-07  0.4224 -1.1565  2.2285\n2015-04-08  0.0663  1.2994  2.0322\n2015-04-09  0.1958 -0.4412  0.3924\n2015-04-10  0.1622  1.7603  1.4525\n2015-04-11  1.1196 -1.9627  0.6615\n\n[101 rows x 3 columns]\n"
"import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame(\n    {'ID': [10001, 10001, 10001, 10002, 10002, 10002, 10003, 10003, 10003],\n     'PRICE': [14.5, 14.5, 14.5, 15.125, 14.5, 14.5, 14.5, 14.5, 15.0],\n     'date': [19920103, 19920106, 19920107, 19920108, 19920109, 19920110,\n              19920113, 19920114, 19920115]},\n    index = range(1,10)) \n\ndef mask_first(x):\n    result = np.ones_like(x)\n    result[0] = 0\n    return result\n\nmask = df.groupby(['ID'])['ID'].transform(mask_first).astype(bool)\nprint(df.loc[mask])\n\n      ID  PRICE      date\n2  10001   14.5  19920106\n3  10001   14.5  19920107\n5  10002   14.5  19920109\n6  10002   14.5  19920110\n8  10003   14.5  19920114\n9  10003   15.0  19920115\n\nimport timeit\nimport operator\nimport numpy as np\nimport pandas as pd\n\nN = 10000\ndf = pd.DataFrame(\n    {'ID': np.random.randint(100, size=(N,)),\n     'PRICE': np.random.random(N),\n     'date': np.random.random(N)}) \n\ndef using_mask(df):\n    def mask_first(x):\n        result = np.ones_like(x)\n        result[0] = 0\n        return result\n\n    mask = df.groupby(['ID'])['ID'].transform(mask_first).astype(bool)\n    return df.loc[mask]\n\ndef using_apply(df):\n    return df.groupby('ID').apply(lambda group: group.iloc[1:, 1:])\n\ndef using_apply_alt(df):\n    return df.groupby('ID', group_keys=False).apply(lambda x: x[1:])\n\ntiming = dict()\nfor func in (using_mask, using_apply, using_apply_alt):\n    timing[func] = timeit.timeit(\n        '{}(df)'.format(func.__name__), \n        'from __main__ import df, {}'.format(func.__name__), number=100)\n\nfor func, t in sorted(timing.items(), key=operator.itemgetter(1)):\n    print('{:16}: {:.2f}'.format(func.__name__, t))\n\nusing_mask      : 0.85\nusing_apply_alt : 2.04\nusing_apply     : 3.70\n"
"In [9]:\n\ngeolocator = Nominatim()\ndf['city_coord'] = df['state_name'].apply(geolocator.geocode)\ndf\nOut[9]:\n    city_name state_name       county_name  \\\n0  WASHINGTON         DC  DIST OF COLUMBIA   \n1  WASHINGTON         DC  DIST OF COLUMBIA   \n\n                                          city_coord  \n0  (District of Columbia, United States of Americ...  \n1  (District of Columbia, United States of Americ...  \n\nIn [16]:\n\ndf['city_coord'] = df['city_coord'].apply(lambda x: (x.latitude, x.longitude))\ndf\nOut[16]:\n    city_name state_name       county_name                       city_coord\n0  WASHINGTON         DC  DIST OF COLUMBIA  (38.8937154, -76.9877934586326)\n1  WASHINGTON         DC  DIST OF COLUMBIA  (38.8937154, -76.9877934586326)\n\nIn [17]:\ndf['city_coord'] = df['state_name'].apply(geolocator.geocode).apply(lambda x: (x.latitude, x.longitude))\ndf\n\nOut[17]:\n    city_name state_name       county_name                       city_coord\n0  WASHINGTON         DC  DIST OF COLUMBIA  (38.8937154, -76.9877934586326)\n1  WASHINGTON         DC  DIST OF COLUMBIA  (38.8937154, -76.9877934586326)\n\nIn [38]:\nstates = df['state_name'].unique()\nd = dict(zip(states, pd.Series(states).apply(geolocator.geocode).apply(lambda x: (x.latitude, x.longitude))))\nd\n\nOut[38]:\n{'DC': (38.8937154, -76.9877934586326)}\n\nIn [40]:    \ndf['city_coord'] = df['state_name'].map(d)\ndf\n\nOut[40]:\n    city_name state_name       county_name                       city_coord\n0  WASHINGTON         DC  DIST OF COLUMBIA  (38.8937154, -76.9877934586326)\n1  WASHINGTON         DC  DIST OF COLUMBIA  (38.8937154, -76.9877934586326)\n"
"import sys\nfrom PyQt4 import QtCore, QtGui\nQt = QtCore.Qt\n\nclass PandasModel(QtCore.QAbstractTableModel):\n    def __init__(self, data, parent=None):\n        QtCore.QAbstractTableModel.__init__(self, parent)\n        self._data = data\n\n    def rowCount(self, parent=None):\n        return len(self._data.values)\n\n    def columnCount(self, parent=None):\n        return self._data.columns.size\n\n    def data(self, index, role=Qt.DisplayRole):\n        if index.isValid():\n            if role == Qt.DisplayRole:\n                return QtCore.QVariant(str(\n                    self._data.values[index.row()][index.column()]))\n        return QtCore.QVariant()\n\n\nif __name__ == '__main__':\n    application = QtGui.QApplication(sys.argv)\n    view = QtGui.QTableView()\n    model = PandasModel(your_pandas_data)\n    view.setModel(model)\n\n    view.show()\n    sys.exit(application.exec_())\n"
"In [90]:\nd.loc[d.isnull()] = d.loc[d.isnull()].apply(lambda x: [])\nd\n\nOut[90]:\n0    [1, 2, 3]\n1       [1, 2]\n2           []\n3           []\ndtype: object\n\nIn [91]:\nd.apply(len)\n\nOut[91]:\n0    3\n1    2\n2    0\n3    0\ndtype: int64\n\nIn [100]:\nd.loc[d['x'].isnull(),['x']] = d.loc[d['x'].isnull(),'x'].apply(lambda x: [])\nd\n\nOut[100]:\n           x  y\n0  [1, 2, 3]  1\n1     [1, 2]  2\n2         []  3\n3         []  4\n\nIn [102]:    \nd['x'].apply(len)\n\nOut[102]:\n0    3\n1    2\n2    0\n3    0\nName: x, dtype: int64\n"
"In [6]:\n\nto_drop = ['Clerk', 'Bagger']\ndf[~df['title'].isin(to_drop)]\nOut[6]:\n  fName  lName             email title\n0  John  Smith  jsmith@gmail.com   CEO\n\nIn [8]:\n\ndf[~df['title'].str.contains('|'.join(to_drop))]\nOut[8]:\n  fName  lName             email title\n0  John  Smith  jsmith@gmail.com   CEO\n"
'Series.str.contains(pat, case=True, flags=0, na=nan, regex=True)\n\ndf_Anomalous_Vendor_Reasons[~df_Anomalous_Vendor_Reasons[\'V\'].str.contains("File*|registry*", na=False)]\n'
'df[df.columns.difference(["T1_V6"])]\n'
"import pandas as pd\nimport nltk\n\ndf = pd.DataFrame({'sentences': ['This is a very good site. I will recommend it to others.', 'Can you please give me a call at 9983938428. have issues with the listings.', 'good work! keep it up']})\ndf['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['sentences']), axis=1)\n\n&gt;&gt;&gt; df\n                                           sentences  \\\n0  This is a very good site. I will recommend it ...   \n1  Can you please give me a call at 9983938428. h...   \n2                              good work! keep it up   \n\n                                     tokenized_sents  \n0  [This, is, a, very, good, site, ., I, will, re...  \n1  [Can, you, please, give, me, a, call, at, 9983...  \n2                      [good, work, !, keep, it, up]\n\ndf['sents_length'] = df.apply(lambda row: len(row['tokenized_sents']), axis=1)\n\n&gt;&gt;&gt; df\n                                           sentences  \\\n0  This is a very good site. I will recommend it ...   \n1  Can you please give me a call at 9983938428. h...   \n2                              good work! keep it up   \n\n                                     tokenized_sents  sents_length  \n0  [This, is, a, very, good, site, ., I, will, re...            14  \n1  [Can, you, please, give, me, a, call, at, 9983...            15  \n2                      [good, work, !, keep, it, up]             6  \n"
'&gt;&gt;&gt; x = pandas.Series(np.random.randn(10))\n&gt;&gt;&gt; stats.skew(x)\n-0.17644348972413657\n&gt;&gt;&gt; x.skew()\n-0.20923623968879457\n&gt;&gt;&gt; stats.skew(x, bias=False)\n-0.2092362396887948\n&gt;&gt;&gt; stats.kurtosis(x)\n0.6362620964462327\n&gt;&gt;&gt; x.kurtosis()\n2.0891062062174464\n&gt;&gt;&gt; stats.kurtosis(x, bias=False)\n2.089106206217446\n'
'df.loc[df.n == "d", [\'a\',\'b\']].values.flatten().tolist()\n#[4, 6]\n'
"df.reset_index(inplace=True)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.set_index('Date')\ns=sm.tsa.seasonal_decompose(df.divida)\n\n&lt;statsmodels.tsa.seasonal.DecomposeResult object at 0x110ec3710&gt;\n\ns.resid\ns.seasonal\ns.trend\n"
's.iloc[1:]\n\ns.drop(s.index[0])\n\ns.drop(s.index[[0, 2, 4]])\n\ns.drop(s.index[1: 4])\n'
"print df\n\n#        Seasonal\n#Date             \n#2014-12 -1.089744\n#2015-01 -0.283654\n#2015-02  0.158974\n#2015-03  0.461538\n\nprint df.index\n\n#PeriodIndex(['2014-12', '2015-01', '2015-02', '2015-03'],\n#              dtype='int64', name=u'Date', freq='M')\n\ndf.index=df.index.to_series().astype(str)\nprint df\n\n#         Seasonal\n#Date             \n#2014-12 -1.089744\n#2015-01 -0.283654\n#2015-02  0.158974\n#2015-03  0.461538\n\nprint df.index\n\n#Index([u'2014-12', u'2015-01', u'2015-02', u'2015-03'], dtype='object', name=u'Date')\n"
'from dateutil.tz import tzlocal\ndatetime.now(tzlocal())\n\nfrom tzlocal import get_localzone\nlocal_tz = get_localzone()\n'
'df = pd.DataFrame(np.random.randn(36, 3))\ndf.plot(drawstyle="steps", linewidth=2)\ndf.plot(drawstyle="steps-post", linewidth=2)\n'
'In [5]:\np.to_series().diff()\n\nOut[5]:\n1985-11-14       NaT\n1985-11-28   14 days\n1985-12-14   16 days\n1985-12-28   14 days\ndtype: timedelta64[ns]\n'
"member_list = (1,2,3)\nsql = &quot;&quot;&quot;select member_id, yearmonth\n         from queried_table\n         where yearmonth between {0} and {0}\n         and member_id in ({1})&quot;&quot;&quot;\nsql = sql.format('?', ','.join('?' * len(member_list)))\nprint(sql)\n\nselect member_id, yearmonth\nfrom queried_table\nwhere yearmonth between ? and ?\nand member_id in (?,?,?)\n\n# generator to flatten values of irregular nested sequences,\n# modified from answers http://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python\ndef flatten(l):\n    for el in l:\n        try:\n            yield from flatten(el)\n        except TypeError:\n            yield el\n\nparams = tuple(flatten((201601, 201603, member_list)))\nprint(params)\n\n(201601, 201603, 1, 2, 3)\n\nquery = pd.read_sql_query(sql, db2conn, params)\n"
"#temporaly set display precision\nwith pd.option_context('display.precision', 10):\n    print df\n\n     0          1   2      3   4             5            6             7   \\\n0  895  2015-4-23  19  10000  LA  0.4677978806  0.477346934  0.4089938425   \n\n             8             9            10            11  12  \n0  0.8224291972  0.8652525793  0.682994286  0.5139162227 NaN    \n"
"import pandas as pd\nimport numpy as np\n\nnp.random.seed([3,1415])\ndf = pd.DataFrame(np.random.rand(20, 2), columns=['A', 'B'])\n\n(df.index.to_series() / 5).astype(int)\n\ndf.index[4::5]\n\n# assign as variable because I'm going to use it more than once.\ns = (df.index.to_series() / 5).astype(int)\n\ndf.groupby(s).std().set_index(s.index[4::5])\n\n           A         B\n4   0.198019  0.320451\n9   0.329750  0.408232\n14  0.293297  0.223991\n19  0.095633  0.376390\n\n# assign what we've done above to df_down\ndf_down = df.groupby(s).std().set_index(s.index[4::5])\n\ndf_up = df_down.reindex(range(20)).bfill()\n\n           A         B\n0   0.198019  0.320451\n1   0.198019  0.320451\n2   0.198019  0.320451\n3   0.198019  0.320451\n4   0.198019  0.320451\n5   0.329750  0.408232\n6   0.329750  0.408232\n7   0.329750  0.408232\n8   0.329750  0.408232\n9   0.329750  0.408232\n10  0.293297  0.223991\n11  0.293297  0.223991\n12  0.293297  0.223991\n13  0.293297  0.223991\n14  0.293297  0.223991\n15  0.095633  0.376390\n16  0.095633  0.376390\n17  0.095633  0.376390\n18  0.095633  0.376390\n19  0.095633  0.376390\n"
'.dataframe td {\n    white-space: nowrap;\n}\n\ndiv.output_subarea {\n    overflow-x: inherit;\n}\n'
"from sqlalchemy import create_engine\n\nsql_engine = create_engine('sqlite:///test.db', echo=False)\nconnection = sql_engine.raw_connection()\nworking_df.to_sql('data', connection,index=False, if_exists='append')\n\nfrom sqlalchemy import create_engine\n\nsql_engine = create_engine('sqlite:///test.db', echo=False)\nconnection = sql_engine.raw_connection()\nworking_df.to_sql('data', connection,index=False, if_exists='append')\n"
'def to_csv(self, path, index=True, sep=",", na_rep=\'\', float_format=None,\n           header=False, index_label=None, mode=\'w\', nanRep=None,\n           encoding=None, date_format=None, decimal=\'.\'):\n    """\n    Write Series to a comma-separated values (csv) file\n    ...\n    """\n    from pandas.core.frame import DataFrame\n    df = DataFrame(self)\n    # result is only a string if no path provided, otherwise None\n    result = df.to_csv(path, index=index, sep=sep, na_rep=na_rep,\n                       float_format=float_format, header=header,\n                       index_label=index_label, mode=mode, nanRep=nanRep,\n                       encoding=encoding, date_format=date_format,\n                       decimal=decimal)\n    if path is None:\n        return result\n\npd.DataFrame(your_series_obj).to_csv(..., quoting=csv.QUOTE_NONE)\n\nyour_series_obj.to_frame().to_csv(..., quoting=csv.QUOTE_NONE)\n'
'In [28]: df\nOut[28]: \n   A   B  C\n0  7 NaN  8\n1  3   3  5\n2  8   1  7\n3  3   0  3\n4  8   2  7\n\nIn [29]: np.nanmax(df.iloc[:, 1].values)\nOut[29]: 3.0\n\nIn [30]: np.nanmin(df.iloc[:, 1].values)\nOut[30]: 0.0\n'
"df['B'] = df.index.get_level_values(level=1)  # Zero based indexing.\n# df['B'] = df.index.get_level_values(level='second')  # This also works.\n&gt;&gt;&gt; df\n               A      B\nfirst second           \nfoo   one     12    one\n      two     11    two\nbar   one     16    one\n      two     12    two\n      three   11  three\n"
'df["new_column"] = df[\'review\'].str.replace(\'[^\\w\\s]\',\'\')\n'
"import pandas as pd\n\ngroups = [[23,135,3], [123,500,1]]\ngroup_labels = ['views', 'orders']\n\n# Convert data to pandas DataFrame.\ndf = pd.DataFrame(groups, index=group_labels).T\n\n# Plot.\npd.concat(\n    [df.mean().rename('average'), df.min().rename('min'), \n     df.max().rename('max')],\n    axis=1).plot.bar()\n"
'df = pd.DataFrame([[100,200,300],[400,500,600]])\nfor index, row in df.iterrows():\n    print(row.to_frame().T)\n\n     0    1    2\n0  100  200  300\n     0    1    2\n1  400  500  600\n\ndef to_frame(self, name=None):\n    """\n    Convert Series to DataFrame\n    Parameters\n    ----------\n    name : object, default None\n        The passed name should substitute for the series name (if it has\n        one).\n    Returns\n    -------\n    data_frame : DataFrame\n    """\n    if name is None:\n        df = self._constructor_expanddim(self)\n    else:\n        df = self._constructor_expanddim({name: self})\n\n    return df\n\n@property\ndef _constructor_expanddim(self):\n    from pandas.core.frame import DataFrame\n    return DataFrame\n'
"In [15]:\nold = pd.DataFrame(index = ['A', 'B', 'C'],\n                   columns = ['k', 'l', 'm'],\n                   data = abs(np.floor(np.random.rand(3, 3)*10)))\n\u200b\nnew = pd.DataFrame(index = ['A', 'B', 'C', 'D'],\n                   columns = ['k', 'l', 'm', 'n'],\n                   data = abs(np.floor(np.random.rand(4, 4)*10)))\ndelta = new.sub(old, fill_value=0)\ndelta\n\nOut[15]:\n   k  l  m  n\nA  0  3 -9  7\nB  0 -2  1  8\nC -4  1  1  7\nD  8  6  0  6\n"
'covMat = np.array(covMat, dtype=float)\n'
"df1 = df.set_index(['number','class']).unstack().swaplevel(0,1,1).sort_index(1)\n\nprint (df1)\nclass        A            B     \n       english math english math\nnumber                          \n1           40   90      87   67\n2           21   20      89   89\n3           68   50      54   79\n4           89   30      21   45\n5           90   57      23   23\n\nprint (df.set_index(['number','class']).stack().unstack([1,2]))\nclass        A            B     \n       english math english math\nnumber                          \n1           40   90      87   67\n2           21   20      89   89\n3           68   50      54   79\n4           89   30      21   45\n5           90   57      23   23\n"
"np.savetxt('xgboost.txt', a.values, fmt='%d', delimiter=&quot;\\t&quot;, header=&quot;X\\tY\\tZ\\tValue&quot;)  \n\na.to_csv('xgboost.txt', header=True, index=False, sep='\\t', mode='a')\n"
'%%javascript\nIPython.OutputArea.prototype._should_scroll = function(lines) {\n    return false;\n}\n'
"data['Native Country'].fillna(data['Native Country'].mode()[0], inplace=True)\n\ndata['Native Country'] = data['Native Country'].fillna(data['Native Country'].mode()[0])\n"
"df['gene'] = df.index #you get the index as tuple\ndf['gene'] = df['gene'].map(gene_d)\ndf = df.set_index('gene', append=True)\n\n                                A   B   C\nchrom   strand  abs_pos gene            \nchrom1  -       1234    geneA   1   1   1\n        +       5678    geneB   2   2   2\n                9876    geneC   3   3   3\nchrom2  +       13579   geneD   4   4   4\n                8497    geneE   5   5   5\n        -       98765   geneF   6   6   6\n                76856   geneG   7   7   7\n"
"df2 = df.C.isnull().groupby([df['A'],df['B']]).sum().astype(int).reset_index(name='count')\nprint (df2)\n     A      B  count\n0  bar    one      0\n1  bar  three      0\n2  bar    two      1\n3  foo    one      2\n4  foo  three      1\n5  foo    two      2\n\ndf = df[df['A'] == 'foo']\ndf2 = df.C.isnull().groupby([df['A'],df['B']]).sum().astype(int)\nprint (df2)\nA    B    \nfoo  one      2\n     three    1\n     two      2\n\ndf = df[df['A'] == 'foo']\ndf2 = df['B'].value_counts()\nprint (df2)\none      2\ntwo      2\nthree    1\nName: B, dtype: int64\n\ndf['D'] = df.C.isnull().groupby([df['A'],df['B']]).transform('sum').astype(int)\nprint (df)\n     A      B     C  D\n0  foo    one   NaN  2\n1  bar    one  bla2  0\n2  foo    two   NaN  2\n3  bar  three  bla3  0\n4  foo    two   NaN  2\n5  bar    two   NaN  1\n6  foo    one   NaN  2\n7  foo  three   NaN  1\n\ndf['D'] = df.C.isnull()\ndf['D'] = df.groupby(['A','B'])['D'].transform('sum').astype(int)\nprint (df)\n     A      B     C  D\n0  foo    one   NaN  2\n1  bar    one  bla2  0\n2  foo    two   NaN  2\n3  bar  three  bla3  0\n4  foo    two   NaN  2\n5  bar    two   NaN  1\n6  foo    one   NaN  2\n7  foo  three   NaN  1\n"
'import numba\n\ndef psi(A):\n    a_cummax = np.maximum.accumulate(A)\n    a_new, idx = np.unique(a_cummax, return_index=True)\n    return idx\n\ndef foo(arr):\n    aux=np.maximum.accumulate(arr)\n    flag = np.concatenate(([True], aux[1:] != aux[:-1]))\n    return np.nonzero(flag)[0]\n\n@numba.jit\ndef f(A):\n    m = A[0]\n    a_new, idx = [m], [0]\n    for i, a in enumerate(A[1:], 1):\n        if a &gt; m:\n            m = a\n            a_new.append(a)\n            idx.append(i)\n    return idx\n\n%timeit f(a)\nThe slowest run took 5.37 times longer than the fastest. This could mean that an intermediate result is being cached.\n1000000 loops, best of 3: 1.83 µs per loop\n\n%timeit foo(a)\nThe slowest run took 9.41 times longer than the fastest. This could mean that an intermediate result is being cached.\n100000 loops, best of 3: 6.35 µs per loop\n\n%timeit psi(a)\nThe slowest run took 9.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n100000 loops, best of 3: 9.95 µs per loop\n'
"#df = df[['gene_symbol', 'sample_id', 'fc']]\ndf = df.pivot(index='gene_symbol',columns='sample_id',values='fc')\nprint (df)\nsample_id       S1     S2\ngene_symbol              \na            100.0    1.3\nb            100.0   14.0\nc            112.0  125.0\n\ndf = df.set_index(['gene_symbol','sample_id'])['fc'].unstack(fill_value=0)\nprint (df)\nsample_id       S1     S2\ngene_symbol              \na            100.0    1.3\nb            100.0   14.0\nc            112.0  125.0\n\ndf = pd.DataFrame({\n               'fc': [100,100,112,1.3,14,125, 100],\n               'sample_id': ['S1','S1','S1','S2','S2','S2', 'S2'],\n               'gene_symbol': ['a', 'b', 'c', 'a', 'b', 'c', 'c'],\n               })\nprint (df)\n      fc gene_symbol sample_id\n0  100.0           a        S1\n1  100.0           b        S1\n2  112.0           c        S1\n3    1.3           a        S2\n4   14.0           b        S2\n5  125.0           c        S2 &lt;- same c, S2, different fc\n6  100.0           c        S2 &lt;- same c, S2, different fc\n\ndf = df.pivot(index='gene_symbol',columns='sample_id',values='fc')\n\ndf = df.pivot_table(index='gene_symbol',columns='sample_id',values='fc', aggfunc='mean')\nprint (df)\nsample_id       S1     S2\ngene_symbol              \na            100.0    1.3\nb            100.0   14.0\nc            112.0  112.5\n\ndf = df.groupby(['gene_symbol','sample_id'])['fc'].mean().unstack(fill_value=0)\nprint (df)\nsample_id       S1     S2\ngene_symbol              \na            100.0    1.3\nb            100.0   14.0\nc            112.0  112.5\n\ndf.columns.name = None\ndf = df.reset_index()\nprint (df)\n  gene_symbol     S1     S2\n0           a  100.0    1.3\n1           b  100.0   14.0\n2           c  112.0  112.5\n"
'&gt;&gt;&gt;print df_cut.iloc[89]\n...\nName: 99, dtype: float64\n\n&gt;&gt;&gt;print df_cut.loc[89]\n...\nName: 89, dtype: float64\n'
'&gt;&gt;&gt; s.unstack(level=1)\nsecond       one       two\nfirst                     \nbar    -0.713374  0.556993\nbaz     0.523611  0.328348\nfoo     0.338351 -0.571854\nqux     0.036694 -0.161852\n'
"(dask_df.groupby('Column B')\n     .apply(len, meta=('int'))).compute()\n\n(dask_df.groupby('Column B')\n     .apply(len, meta=pd.Series(dtype='int', name='Column B')))\n"
"print (~df.columns.duplicated())\n[ True  True  True False]\n\ndf = df.loc[:, ~df.columns.duplicated()]\nprint (df)\n   A  B  C\n0  0  1  2\n1  4  5  6\n\ndf = df.iloc[:, ~df.columns.duplicated()]\nprint (df)\n   A  B  C\n0  0  1  2\n1  4  5  6\n\nnp.random.seed(123)\ncols = ['A','B','C','B']\n#[1000 rows x 30 columns]\ndf = pd.DataFrame(np.random.randint(10, size=(1000,30)),columns = np.random.choice(cols, 30))\nprint (df)\n\nIn [115]: %timeit (df.groupby(level=0, axis=1).first())\n1000 loops, best of 3: 1.48 ms per loop\n\nIn [116]: %timeit (df.groupby(level=0, axis=1).mean())\n1000 loops, best of 3: 1.58 ms per loop\n\nIn [117]: %timeit (df.iloc[:, ~df.columns.duplicated()])\n1000 loops, best of 3: 338 µs per loop\n\nIn [118]: %timeit (df.loc[:, ~df.columns.duplicated()])\n1000 loops, best of 3: 346 µs per loop\n"
'df = df.drop(\'Max\',axis=1)\n\ndf = pd.read_csv(\'newdata.csv\')\ndf = df.drop(\'Max\')\n\ndf = pd.read_csv("newdata.csv",index_col=0)\ndf = df.drop("Max",axis=0)\n'
'df=pd.DataFrame(data=mice.complete(d), columns=d.columns, index=d.index)\n'
"import pandas as pd\n\ndf1 = pd.DataFrame([['a','b','c'], ['d','e','f']])  \ndf2 = pd.DataFrame([['A','B','C'], ['D','E','F']])\n\nconcat_df = pd.concat([df1,df2]).sort_index().reset_index(drop=True)\n\nimport pandas as pd\ndf1 = pd.DataFrame([['a','b','c'], ['d','e','f']]).reset_index()  \ndf2 = pd.DataFrame([['A','B','C'], ['D','E','F']]).reset_index()\n\nconcat_df = pd.concat([df1,df2]).sort_index().set_index('index')\n"
'from tqdm._tqdm_notebook import tqdm_notebook\n\ntqdm_notebook.pandas(...\n'
"ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))\ndf = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=list('ABCD'))\n\nplt.figure(figsize=(12,5))\nplt.xlabel('Number of requests every 10 minutes')\n\nax1 = df.A.plot(color='blue', grid=True, label='Count')\nax2 = df.B.plot(color='red', grid=True, secondary_y=True, label='Sum')\n\nh1, l1 = ax1.get_legend_handles_labels()\nh2, l2 = ax2.get_legend_handles_labels()\n\n\nplt.legend(h1+h2, l1+l2, loc=2)\nplt.show()\n\nplt.figure(figsize=(12,5))\nplt.xlabel('Number of requests every 10 minutes')\n\nax1 = df.A.plot(color='blue', grid=True, label='Count')\nax2 = df.B.plot(color='red', grid=True, secondary_y=True, label='Sum')\n\nax1.legend(loc=1)\nax2.legend(loc=2)\n\nplt.show()\n"
'import numpy as np\nimport pandas as pd  # only used to return a dataframe\n\n\ndef list_ancestors(edges):\n    """\n    Take edge list of a rooted tree as a numpy array with shape (E, 2),\n    child nodes in edges[:, 0], parent nodes in edges[:, 1]\n    Return pandas dataframe of all descendant/ancestor node pairs\n\n    Ex:\n        df = pd.DataFrame({\'child\': [200, 201, 300, 301, 302, 400],\n                           \'parent\': [100, 100, 200, 200, 201, 300]})\n\n        df\n           child  parent\n        0    200     100\n        1    201     100\n        2    300     200\n        3    301     200\n        4    302     201\n        5    400     300\n\n        list_ancestors(df.values)\n\n        returns\n\n            descendant  ancestor\n        0          200       100\n        1          201       100\n        2          300       200\n        3          300       100\n        4          301       200\n        5          301       100\n        6          302       201\n        7          302       100\n        8          400       300\n        9          400       200\n        10         400       100\n    """\n    ancestors = []\n    for ar in trace_nodes(edges):\n        ancestors.append(np.c_[np.repeat(ar[:, 0], ar.shape[1]-1),\n                               ar[:, 1:].flatten()])\n    return pd.DataFrame(np.concatenate(ancestors),\n                        columns=[\'descendant\', \'ancestor\'])\n\n\ndef trace_nodes(edges):\n    """\n    Take edge list of a rooted tree as a numpy array with shape (E, 2),\n    child nodes in edges[:, 0], parent nodes in edges[:, 1]\n    Yield numpy array with cross-section of tree and associated\n    ancestor nodes\n\n    Ex:\n        df = pd.DataFrame({\'child\': [200, 201, 300, 301, 302, 400],\n                           \'parent\': [100, 100, 200, 200, 201, 300]})\n\n        df\n           child  parent\n        0    200     100\n        1    201     100\n        2    300     200\n        3    301     200\n        4    302     201\n        5    400     300\n\n        trace_nodes(df.values)\n\n        yields\n\n        array([[200, 100],\n               [201, 100]])\n\n        array([[300, 200, 100],\n               [301, 200, 100],\n               [302, 201, 100]])\n\n        array([[400, 300, 200, 100]])\n    """\n    mask = np.in1d(edges[:, 1], edges[:, 0])\n    gen_branches = edges[~mask]\n    edges = edges[mask]\n    yield gen_branches\n    while edges.size != 0:\n        mask = np.in1d(edges[:, 1], edges[:, 0])\n        next_gen = edges[~mask]\n        gen_branches = numpy_col_inner_many_to_one_join(next_gen, gen_branches)\n        edges = edges[mask]\n        yield gen_branches\n\n\ndef numpy_col_inner_many_to_one_join(ar1, ar2):\n    """\n    Take two 2-d numpy arrays ar1 and ar2,\n    with no duplicate values in first column of ar2\n    Return inner join of ar1 and ar2 on\n    last column of ar1, first column of ar2\n\n    Ex:\n\n        ar1 = np.array([[1,  2,  3],\n                        [4,  5,  3],\n                        [6,  7,  8],\n                        [9, 10, 11]])\n\n        ar2 = np.array([[ 1,  2],\n                        [ 3,  4],\n                        [ 5,  6],\n                        [ 7,  8],\n                        [ 9, 10],\n                        [11, 12]])\n\n        numpy_col_inner_many_to_one_join(ar1, ar2)\n\n        returns\n\n        array([[ 1,  2,  3,  4],\n               [ 4,  5,  3,  4],\n               [ 9, 10, 11, 12]])\n    """\n    ar1 = ar1[np.in1d(ar1[:, -1], ar2[:, 0])]\n    ar2 = ar2[np.in1d(ar2[:, 0], ar1[:, -1])]\n    if \'int\' in ar1.dtype.name and ar1[:, -1].min() &gt;= 0:\n        bins = np.bincount(ar1[:, -1])\n        counts = bins[bins.nonzero()[0]]\n    else:\n        counts = np.unique(ar1[:, -1], False, False, True)[1]\n    left = ar1[ar1[:, -1].argsort()]\n    right = ar2[ar2[:, 0].argsort()]\n    return np.concatenate([left[:, :-1],\n                           right[np.repeat(np.arange(right.shape[0]),\n                                           counts)]], 1)\n\ndf = pd.DataFrame(\n    {\n        \'child\': [3102, 2010, 3011, 3000, 3033, 2110, 3111, 2100],\n        \'parent\': [2010, 1000, 2010, 2110, 2100, 1000, 2110, 1000]\n    }\n)\n\ndf2 = pd.DataFrame(\n    {\n        \'child\': [4321, 3102, 4023, 2010, 5321, 4200, 4113, 6525, 4010, 4001,\n                  3011, 5010, 3000, 3033, 2110, 6100, 3111, 2100, 6016, 4311],\n        \'parent\': [3111, 2010, 3000, 1000, 4023, 3011, 3033, 5010, 3011, 3102,\n                   2010, 4023, 2110, 2100, 1000, 5010, 2110, 1000, 5010, 3033]\n    }\n)\n\ndf3 = pd.DataFrame(np.r_[np.c_[np.arange(1, 501), np.arange(500)],\n                         np.c_[np.arange(501, 1001), np.arange(500)]],\n                   columns=[\'child\', \'parent\'])\n\ndf4 = pd.DataFrame(np.r_[np.c_[np.arange(1, 101), np.repeat(0, 100)],\n                         np.c_[np.arange(1001, 11001),\n                               np.repeat(np.arange(1, 101), 100)]],\n                   columns=[\'child\', \'parent\'])\n\n%timeit get_ancestry_dataframe_flat(df)\n10 loops, best of 3: 53.4 ms per loop\n\n%timeit add_children_of_children(df)\n1000 loops, best of 3: 1.13 ms per loop\n\n%timeit all_descendants_nx(df)\n1000 loops, best of 3: 675 µs per loop\n\n%timeit list_ancestors(df.values)\n1000 loops, best of 3: 391 µs per loop\n\n%timeit get_ancestry_dataframe_flat(df2)\n10 loops, best of 3: 168 ms per loop\n\n%timeit add_children_of_children(df2)\n1000 loops, best of 3: 1.8 ms per loop\n\n%timeit all_descendants_nx(df2)\n1000 loops, best of 3: 1.06 ms per loop\n\n%timeit list_ancestors(df2.values)\n1000 loops, best of 3: 933 µs per loop\n\n%timeit add_children_of_children(df3)\n10 loops, best of 3: 156 ms per loop\n\n%timeit all_descendants_nx(df3)\n1 loop, best of 3: 952 ms per loop\n\n%timeit list_ancestors(df3.values)\n10 loops, best of 3: 104 ms per loop\n\n%timeit add_children_of_children(df4)\n1 loop, best of 3: 503 ms per loop\n\n%timeit all_descendants_nx(df4)\n1 loop, best of 3: 238 ms per loop\n\n%timeit list_ancestors(df4.values)\n100 loops, best of 3: 2.96 ms per loop\n\nnp.all(get_ancestry_dataframe_flat(df2).sort_values([\'descendant\', \'ancestor\'])\\\n                                       .reset_index(drop=True) ==\\\n       list_ancestors(df2.values).sort_values([\'descendant\', \'ancestor\'])\\\n                                 .reset_index(drop=True))\nOut[20]: True\n'
"import pandas as pd\ndf = pd.DataFrame.from_dict({'a': {0: 1, 1: 2}, 'b': {0: 3, 1: 3}})\nprint(df.to_dict())\n"
'df.pivot("column", "group", "val")\n\ngroup   g1  g2\ncolumn        \nc1      10   8\nc2      12  10\nc3      13  12\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame([[\'g1\',\'c1\',10],[\'g1\',\'c2\',12],[\'g1\',\'c3\',13],[\'g2\',\'c1\',8],\n                   [\'g2\',\'c2\',10],[\'g2\',\'c3\',12]],columns=[\'group\',\'column\',\'val\'])\n\ndf.pivot("column", "group", "val").plot(kind=\'bar\')\n\nplt.show()\n'
"df.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\nOut[32]: \n     a    c    d\na  1.0  1.0  1.0\nc  1.0  1.0  1.0\nd  1.0  1.0  1.0\n\ndf=pd.DataFrame({'a':['a','b','c'],'c':['a','b','c'],'d':['a','b','c']})\n\nfrom scipy.stats import chisquare\n\ndf=df.apply(lambda x : pd.factorize(x)[0])+1\n\npd.DataFrame([chisquare(df[x].values,f_exp=df.values.T,axis=1)[0] for x in df])\n\nOut[123]: \n     0    1    2    3\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n\ndf=pd.DataFrame({'a':['a','d','c'],'c':['a','b','c'],'d':['a','b','c'],'e':['a','b','c']})\n"
"data = np.random.randint(1, 10, (5, 3, 2))\npnl = pd.Panel(\n    data, \n    items=['item {}'.format(i) for i in range(1, 6)], \n    major_axis=[2015, 2016, 2017], \n    minor_axis=['US', 'UK']\n)\n\n             item 1  item 2  item 3  item 4  item 5\nmajor minor                                        \n2015  US          9       6       3       2       5\n      UK          8       3       7       7       9\n2016  US          7       7       8       7       5\n      UK          9       1       9       9       1\n2017  US          1       8       1       3       1\n      UK          6       8       8       1       6\n\ndata = data.reshape(5, 6).T\ndf = pd.DataFrame(\n    data=data,\n    index=pd.MultiIndex.from_product([[2015, 2016, 2017], ['US', 'UK']]),\n    columns=['item {}'.format(i) for i in range(1, 6)]\n)\n\n         item 1  item 2  item 3  item 4  item 5\n2015 US       9       6       3       2       5\n     UK       8       3       7       7       9\n2016 US       7       7       8       7       5\n     UK       9       1       9       9       1\n2017 US       1       8       1       3       1\n     UK       6       8       8       1       6\n"
'import pandas as pd\nimport numpy as np\ndf = pd.read_csv("cond_shift.csv")\ndf\n\n   user open_time   close_time  value\n0   1   12/30/2016  12/31/2016  1\n1   1   1/1/2017    3/1/2017    5\n2   1   1/2/2017    2/1/2017    6\n3   1   2/3/2017    2/5/2017    7\n4   1   2/7/2017    4/1/2017    3\n5   1   9/7/2017    9/11/2017   1\n6   2   1/1/2018    2/1/2018    15\n7   2   3/1/2018    4/1/2018    3\n\ndf["open_time"] = pd.to_datetime(df["open_time"])\ndf["close_time"] = pd.to_datetime(df["close_time"])\ndf.sort_values([\'user\',\'close_time\'],inplace=True)\ndf[\'close_cumsum\']=df.groupby(\'user\')[\'value\'].cumsum()\ndf.sort_values([\'user\',\'open_time\'],inplace=True)\ndf\n\n\n   user open_time   close_time  value   close_cumsum\n0   1   2016-12-30  2016-12-31  1       1\n1   1   2017-01-01  2017-03-01  5       19\n2   1   2017-01-02  2017-02-01  6       7\n3   1   2017-02-03  2017-02-05  7       14\n4   1   2017-02-07  2017-04-01  3       22\n5   1   2017-09-07  2017-09-11  1       23\n6   2   2018-01-01  2018-02-01  15      15\n7   2   2018-03-01  2018-04-01  3       18\n\ndf["cumulated_closed_value"] = df.groupby("user")["close_cumsum"].transform("shift")\ncondition = ~(df.groupby("user")[\'close_time\'].transform("shift") &lt; df["open_time"])\ndf.loc[ condition,"cumulated_closed_value" ] = None\ndf["cumulated_closed_value"] =df.groupby("user")["cumulated_closed_value"].fillna(method="ffill").fillna(0)\ndf\n\n\nuser    open_time   close_time  value   close_cumsum    cumulated_closed_value\n0   1   2016-12-30  2016-12-31  1       1               0.0\n1   1   2017-01-01  2017-03-01  5       19              1.0\n2   1   2017-01-02  2017-02-01  6       7               1.0\n3   1   2017-02-03  2017-02-05  7       14              7.0\n4   1   2017-02-07  2017-04-01  3       22              14.0\n5   1   2017-09-07  2017-09-11  1       23              22.0\n6   2   2018-01-01  2018-02-01  15      15              0.0\n7   2   2018-03-01  2018-04-01  3       18              15.0\n'
"df = pd.DataFrame({'a': [1, 2],'b': [3, 4]})\nprint (df)\n   a  b\n0  1  3\n1  2  4\n\ndf.set_index('a')\n   b\na   \n1  3\n2  4\n\ndf.reindex(df.a.values).drop('a',1) # equivalent to df.reindex(df.a.values).drop('a',1)\n     b\n1  4.0\n2  NaN\n# drop('a',1) is just to not care about column a in my example\n"
"def flatten_json(y):\n    out = {}\n\n    def flatten(x, name=''):\n        if type(x) is dict:\n            for a in x:\n                flatten(x[a], name + a + '_')\n        elif type(x) is list:\n            i = 0\n            for a in x:\n                flatten(a, name + str(i) + '_')\n                i += 1\n        else:\n            out[name[:-1]] = x\n\n    flatten(y)\n    return out\n"
"df = pd.DataFrame({'label': np.random.randint(, 4, size=1000000, dtype='i8')})\n\n%timeit df['output'] = df.label.map(lookup_dict.get)\n261 ms ± 12.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n%timeit df['output'] = df.label.map(lookup_dict)\n69.6 ms ± 3.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n%%timeit\nlookup_arr = np.array(list(lookup_dict.values()))\ndf['output'] = lookup_arr.take(df['label'] - 1)\n8.68 ms ± 332 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
'%matplotlib inline\n\n%matplotlib notebook\n'
"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame({'lab':['A', 'B', 'C'], 'val':[10, 30, 20]})\n\nfig, ax2 = plt.subplots()\n\ndf.plot(kind='bar',ax=ax2, fontsize=10, sort_columns=True)\nax2.grid(axis='y')\nplt.show()\n\nfig = plt.figure()\n\nax2 = df.plot(kind='bar', fontsize=10, sort_columns=True)\nax2.grid(axis='y')\n\ndf.plot(kind='bar',ax=ax2, fontsize=10, sort_columns=True).grid(axis='y')\n"
"df['country']=df.loc[df.city.str.endswith('(c)'),'city']\ndf.country=df.country.ffill()\ndf=df[df.city.ne(df.country)]\ndf.country=df.country.str.strip('(c)')\n"
"%%timeit -r 3 -n 1 -t\nglobal df2\ndf2 = mpd.read_csv('15mil.csv', header=None)\n    3.07 s ± 685 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n\n(df2.values == df1.values).all()\n    True\n\n!sed 's/.\\{4\\}/&amp;)/g' 15mil.csv &gt; 15mil_dirty.csv\n\nwith open('15mil_dirty.csv') as file:\n    df2 = pd.read_csv(FilterFile(file))\n"
"temp = df[['today', 'yesterday']].applymap(set)\nremovals = temp.diff(periods=1, axis=1).dropna(axis=1)\nadditions = temp.diff(periods=-1, axis=1).dropna(axis=1) \n\n  yesterday\n0        {}\n1        {}\n2       {a}\n\n  today\n0   {c}\n1   {b}\n2   {b}\n"
'i = frame.index.searchsorted(date)\nframe.ix[frame.index[i]]\n'
"data['weekday'] = data['my_dt'].apply(lambda x: x.weekday())\n\nweekdays_only = data[data['weekday'] &lt; 5 ]\n"
'$ cat test.py\nimport sys\nimport pandas as pd\n\ndf = pd.read_json(sys.stdin)\nprint df\n\n$ cat data.json\n{"a": [1,2,3,4], "b":[3,4,5,6]}\n\n$ python test.py &lt; data.json\n   a  b\n0  1  3\n1  2  4\n2  3  5\n3  4  6\n'
"df.set_index(['fileName','phrase'],inplace=True)\ndf.sortlevel(inplace=True)\n\ndf.ix[('somePath','somePhrase')]\n\nIn [1]: arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'])\n   ...:    .....: ,\n   ...:    .....:           np.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])\n   ...:    .....:           ]\n\nIn [2]: df = DataFrame(randn(8, 4), index=arrays)\n\nIn [3]: df\nOut[3]: \n                0         1         2         3\nbar one  1.654436  0.184326 -2.337694  0.625120\n    two  0.308995  1.219156 -0.906315  1.555925\nbaz one -0.180826 -1.951569  1.617950 -1.401658\n    two  0.399151 -1.305852  1.530370 -0.132802\nfoo one  1.097562  0.097126  0.387418  0.106769\n    two  0.465681  0.270120 -0.387639 -0.142705\nqux one -0.656487 -0.154881  0.495044 -1.380583\n    two  0.274045 -0.070566  1.274355  1.172247\n\nIn [4]: df.index.lexsort_depth\nOut[4]: 2\n\nIn [5]: df.ix[('foo','one')]\nOut[5]: \n0    1.097562\n1    0.097126\n2    0.387418\n3    0.106769\nName: (foo, one), dtype: float64\n\nIn [6]: df.ix['foo']\nOut[6]: \n            0         1         2         3\none  1.097562  0.097126  0.387418  0.106769\ntwo  0.465681  0.270120 -0.387639 -0.142705\n\nIn [7]: df.ix[['foo']]\nOut[7]: \n                0         1         2         3\nfoo one  1.097562  0.097126  0.387418  0.106769\n    two  0.465681  0.270120 -0.387639 -0.142705\n\nIn [8]: df.sortlevel(level=1)\nOut[8]: \n                0         1         2         3\nbar one  1.654436  0.184326 -2.337694  0.625120\nbaz one -0.180826 -1.951569  1.617950 -1.401658\nfoo one  1.097562  0.097126  0.387418  0.106769\nqux one -0.656487 -0.154881  0.495044 -1.380583\nbar two  0.308995  1.219156 -0.906315  1.555925\nbaz two  0.399151 -1.305852  1.530370 -0.132802\nfoo two  0.465681  0.270120 -0.387639 -0.142705\nqux two  0.274045 -0.070566  1.274355  1.172247\n\nIn [10]: df.sortlevel(level=1).index.lexsort_depth\nOut[10]: 0\n"
'data[\'amount\'] = data["amount"].fillna(data.groupby("num")["amount"].transform("mean"))\n\ndata["amount"] = data[\'amount\'].fillna(mean_avg)\n\ndata[\'amount\'] = data[\'amount\'].fillna(mean_avg)*2\n\npd.set_option(\'chained_assignment\',None)\n'
'# Read in the data from the stackoverflow question\ndf = pd.read_clipboard().iloc[1:]\n\n# Convert it to "long-form" or "tidy" representation\ndf = pd.melt(df, id_vars=["date"], var_name="condition")\n\n# Plot the average value by condition and date\nax = df.groupby(["condition", "date"]).mean().unstack("condition").plot()\n\n# Get a reference to the x-points corresponding to the dates and the the colors\nx = np.arange(len(df.date.unique()))\npalette = sns.color_palette()\n\n# Calculate the 25th and 75th percentiles of the data\n# and plot a translucent band between them\nfor cond, cond_df in df.groupby("condition"):\n    low = cond_df.groupby("date").value.apply(np.percentile, 25)\n    high = cond_df.groupby("date").value.apply(np.percentile, 75)\n    ax.fill_between(x, low, high, alpha=.2, color=palette.pop(0))\n'
"df.index.name = 'newhead'\ndf.reset_index(inplace=True)\n\n  newhead  head1  head2  head3\n0     bar     32      3    100\n1     bix     22    NaN    NaN\n2     foo     11      1    NaN\n3     qux    NaN     10    NaN\n4     xoo    NaN      2     20\n"
'x[x &gt; 0].stack().index.tolist()\n'
'rows = ...# your source data\n\ndef date_to_sortable_string(date):\n  # use datetime package to convert string to sortable date.\n  pass\n\n# Assume x[0] === patient_id and x[1] === encounter date\n\n# Sort by patient_id and date\nrows_sorted = sorted(rows, key=lambda x: "%0.5d-%s" % (x[0], date_to_sortable_string(x[1])))\n\nfor row in rows_sorted:\n  print row\n'
'&gt;&gt;&gt; df = pd.DataFrame({"A": np.arange(500), "B": np.arange(500.0)})\n&gt;&gt;&gt; df.loc[321, "A"] = "Fred"\n&gt;&gt;&gt; df.loc[325, "B"] = True\n&gt;&gt;&gt; weird = (df.applymap(type) != df.iloc[0].apply(type)).any(axis=1)\n&gt;&gt;&gt; df[weird]\n        A     B\n321  Fred   321\n325   325  True\n'
"In [10]:\n# create the merged df\nmerged = dfA.merge(dfB, on='date')\nmerged\n\nOut[10]:\n        date  impressions  spend      col_x      col_y\n0 2015-01-01       100000      3  ABC123456        NaN\n1 2015-01-02       145000      5  ABCD00000        NaN\n2 2015-01-03       300000     15        NaN  DEF123456\n\nIn [11]:\n# now create col_z using where\nmerged['col_z'] = merged['col_x'].where(merged['col_x'].notnull(), merged['col_y'])\nmerged\n\nOut[11]:\n        date  impressions  spend      col_x      col_y      col_z\n0 2015-01-01       100000      3  ABC123456        NaN  ABC123456\n1 2015-01-02       145000      5  ABCD00000        NaN  ABCD00000\n2 2015-01-03       300000     15        NaN  DEF123456  DEF123456\n\nIn [13]:\n\nmerged = merged.drop(['col_x','col_y'],axis=1)\nmerged\n\nOut[13]:\n        date  impressions  spend      col_z\n0 2015-01-01       100000      3  ABC123456\n1 2015-01-02       145000      5  ABCD00000\n2 2015-01-03       300000     15  DEF123456\n"
"In [23]:\ndf.reindex(index=np.roll(df.index,1))\n\nOut[23]:\n         vRatio\nindex          \n45     0.981553\n5      0.995232\n15     0.999794\n25     1.006853\n35     0.997781\n\nIn [25]:\ndf['vRatio'] = np.roll(df['vRatio'],1)\ndf\n\nOut[25]:\n         vRatio\nindex          \n5      0.981553\n15     0.995232\n25     0.999794\n35     1.006853\n45     0.997781\n"
'from sklearn.metrics.pairwise import pairwise_distances\njac_sim = 1 - pairwise_distances(df.T, metric = "hamming")\n# optionally convert it to a DataFrame\njac_sim = pd.DataFrame(jac_sim, index=df.columns, columns=df.columns)\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.binomial(1, 0.5, size=(100, 5)), columns=list(\'ABCDE\'))\nprint(df.head())\n\n   A  B  C  D  E\n0  1  1  1  1  0\n1  1  0  1  1  0\n2  1  1  1  1  0\n3  0  0  1  1  1\n4  1  1  0  1  0\n\nfrom sklearn.metrics import jaccard_similarity_score\nprint(jaccard_similarity_score(df[\'A\'], df[\'B\']))\n0.43\n\nfrom sklearn.metrics.pairwise import pairwise_distances\nprint(1 - pairwise_distances(df.T, metric = "hamming"))\n\narray([[ 1.  ,  0.43,  0.61,  0.55,  0.46],\n       [ 0.43,  1.  ,  0.52,  0.56,  0.49],\n       [ 0.61,  0.52,  1.  ,  0.48,  0.53],\n       [ 0.55,  0.56,  0.48,  1.  ,  0.49],\n       [ 0.46,  0.49,  0.53,  0.49,  1.  ]])\n\njac_sim = 1 - pairwise_distances(df.T, metric = "hamming")\njac_sim = pd.DataFrame(jac_sim, index=df.columns, columns=df.columns)\n# jac_sim = np.triu(jac_sim) to set the lower diagonal to zero\n# jac_sim = np.tril(jac_sim) to set the upper diagonal to zero\n\n      A     B     C     D     E\nA  1.00  0.43  0.61  0.55  0.46\nB  0.43  1.00  0.52  0.56  0.49\nC  0.61  0.52  1.00  0.48  0.53\nD  0.55  0.56  0.48  1.00  0.49\nE  0.46  0.49  0.53  0.49  1.00\n\nimport itertools\nsim_df = pd.DataFrame(np.ones((5, 5)), index=df.columns, columns=df.columns)\nfor col_pair in itertools.combinations(df.columns, 2):\n    sim_df.loc[col_pair] = sim_df.loc[tuple(reversed(col_pair))] = jaccard_similarity_score(df[col_pair[0]], df[col_pair[1]])\nprint(sim_df)\n      A     B     C     D     E\nA  1.00  0.43  0.61  0.55  0.46\nB  0.43  1.00  0.52  0.56  0.49\nC  0.61  0.52  1.00  0.48  0.53\nD  0.55  0.56  0.48  1.00  0.49\nE  0.46  0.49  0.53  0.49  1.00\n'
'import urllib.parse\nimport pandas as pd\nfrom pyfinance.ols import PandasRollingOLS\n\n# You can also do this with pandas-datareader; here\'s the hard way\nurl = "https://fred.stlouisfed.org/graph/fredgraph.csv"\n\nsyms = {\n    "TWEXBMTH" : "usd", \n    "T10Y2YM" : "term_spread", \n    "GOLDAMGBD228NLBM" : "gold",\n}\n\nparams = {\n    "fq": "Monthly,Monthly,Monthly",\n    "id": ",".join(syms.keys()),\n    "cosd": "2000-01-01",\n    "coed": "2019-02-01",\n}\n\ndata = pd.read_csv(\n    url + "?" + urllib.parse.urlencode(params, safe=","),\n    na_values={"."},\n    parse_dates=["DATE"],\n    index_col=0\n).pct_change().dropna().rename(columns=syms)\nprint(data.head())\n#                  usd  term_spread      gold\n# DATE                                       \n# 2000-02-01  0.012580    -1.409091  0.057152\n# 2000-03-01 -0.000113     2.000000 -0.047034\n# 2000-04-01  0.005634     0.518519 -0.023520\n# 2000-05-01  0.022017    -0.097561 -0.016675\n# 2000-06-01 -0.010116     0.027027  0.036599\n\ny = data.usd\nx = data.drop(\'usd\', axis=1)\n\nwindow = 12  # months\nmodel = PandasRollingOLS(y=y, x=x, window=window)\n\nprint(model.beta.head())  # Coefficients excluding the intercept\n#             term_spread      gold\n# DATE                             \n# 2001-01-01     0.000033 -0.054261\n# 2001-02-01     0.000277 -0.188556\n# 2001-03-01     0.002432 -0.294865\n# 2001-04-01     0.002796 -0.334880\n# 2001-05-01     0.002448 -0.241902\n\nprint(model.fstat.head())\n# DATE\n# 2001-01-01    0.136991\n# 2001-02-01    1.233794\n# 2001-03-01    3.053000\n# 2001-04-01    3.997486\n# 2001-05-01    3.855118\n# Name: fstat, dtype: float64\n\nprint(model.rsq.head())  # R-squared\n# DATE\n# 2001-01-01    0.029543\n# 2001-02-01    0.215179\n# 2001-03-01    0.404210\n# 2001-04-01    0.470432\n# 2001-05-01    0.461408\n# Name: rsq, dtype: float64\n'
"df1.groupby('User').apply(lambda df: df.sample(1))\n\ndf1.groupby('User', group_keys=False).apply(lambda df: df.sample(1))\n"
"df.time.astype('M8[us]')\n\nmeta = pd.Series([], name='time', dtype=pd.Timestamp)\n\nmeta = ('time', pd.Timestamp)\n\ndf.time.map_partitions(pd.to_datetime, meta=meta)\n"
"df['prod_type'] = df['prod_type'].replace({'respon':'responsive', 'r':'responsive'})\nprint (df)\n    prod_type\n0  responsive\n1  responsive\n2  responsive\n3  responsive\n4  responsive\n5  responsive\n6  responsive\n\ndf['prod_type'] = 'responsive' \n"
"&gt;&gt;&gt; df.drop_duplicates()\n   Col1 Col2  Col3\n0    12   AB    13\n1    11   AB    13\n3    12   AC    14\n\n&gt;&gt;&gt; df.drop_duplicates(inplace=True)\n&gt;&gt;&gt; df\n   Col1 Col2  Col3\n0    12   AB    13\n1    11   AB    13\n3    12   AC    14\n\n&gt;&gt;&gt; df[['Col2','Col3']].drop_duplicates()\n  Col2  Col3\n0   AB    13\n3   AC    14\n\n&gt;&gt;&gt; df.drop_duplicates(subset=['Col2','Col3'])\n   Col1 Col2  Col3\n0    12   AB    13\n3    12   AC    14\n"
"In [139]: df.groupby('id')['cat'].transform(lambda x: (x == 1).any())\nOut[139]:\n0    1\n1    1\n2    1\n3    1\n4    1\n5    1\n6    1\n7    0\n8    0\n9    1\nName: cat, dtype: int64\n\n#                         v       v\nIn [140]: df.groupby('id')[['cat']].transform(lambda x: (x == 1).any())\nOut[140]:\n     cat\n0   True\n1   True\n2   True\n3   True\n4   True\n5   True\n6   True\n7  False\n8  False\n9   True\n\nIn [141]: df.dtypes\nOut[141]:\ncat    int64\nid     int64\ndtype: object\n"
'import shutil\nimport glob\n\n\n#import csv files from folder\npath = r\'data/US/market/merged_data\'\nallFiles = glob.glob(path + "/*.csv")\nallFiles.sort()  # glob lacks reliable ordering, so impose your own if output order matters\nwith open(\'someoutputfile.csv\', \'wb\') as outfile:\n    for i, fname in enumerate(allFiles):\n        with open(fname, \'rb\') as infile:\n            if i != 0:\n                infile.readline()  # Throw away header on all but first file\n            # Block copy rest of file from input to output without parsing\n            shutil.copyfileobj(infile, outfile)\n            print(fname + " has been imported.")\n'
'from toolz import interleave\n\npd.concat([d1, d2], axis=1)[list(interleave([d1, d2]))]\n\n   0  3  1  4  2\na  1  0  1  0  1\nb  1  0  1  0  1\nc  1  0  1  0  1\n'
"s1 = pd.Series([1,2,3], index=['a','b','c'])\ns2 = pd.Series([2,4,6], index=['a','b','c'])\ns1 + s2\n#Ouput as expected:\na    3\nb    6\nc    9\ndtype: int64\n\ns2 = pd.Series([2,4,6], index=['a','a','c'])\ns1 + s2\n#Ouput\na    3.0\na    5.0\nb    NaN\nc    9.0\ndtype: float64\n\ns2 = pd.Series([2,4,6], index=['e','f','g'])\ns1 + s2\n#Output\na   NaN\nb   NaN\nc   NaN\ne   NaN\nf   NaN\ng   NaN\ndtype: float64\n"
"from itertools import chain\n\ndataframe['features'] = dataframe.apply(lambda x: ''.join([*chain.from_iterable((v, f' &lt;{i}&gt; ') for i, v in enumerate(x))][:-1]), axis=1)\n\nprint(dataframe)\n\n  col_1     col_2    col_3                      features\n0   aaa  name_aaa  job_aaa  aaa &lt;0&gt; name_aaa &lt;1&gt; job_aaa\n1   bbb  name_bbb  job_bbb  bbb &lt;0&gt; name_bbb &lt;1&gt; job_bbb\n2   ccc  name_ccc  job_ccc  ccc &lt;0&gt; name_ccc &lt;1&gt; job_ccc\n3   ddd  name_ddd  job_ddd  ddd &lt;0&gt; name_ddd &lt;1&gt; job_ddd\n"
'import pandas\nprint pandas.__version__\n'
'import os\nimport pandas as pd\n\n#list the files\nfilelist = os.listdir(targetdir) \n#read them into pandas\ndf_list = [pd.read_table(file) for file in filelist]\n#concatenate them together\nbig_df = pd.concat(df_list)\n'
"# what you described:\nIn [15]: import numpy as np\nIn [16]: import pandas\nIn [17]: x = pandas.read_csv('weird.csv')\n\nIn [19]: x.dtypes\nOut[19]: \nint_field            int64\nfloatlike_field    float64  # what you don't want?\nstr_field           object\n\nIn [20]: datatypes = [('int_field','i4'),('floatlike','S10'),('strfield','S10')]\n\nIn [21]: y_np = np.loadtxt('weird.csv', dtype=datatypes, delimiter=',', skiprows=1)\n\nIn [22]: y_np\nOut[22]: \narray([(1, '2.31', 'one'), (2, '3.12', 'two'), (3, '1.32', 'three ')], \n      dtype=[('int_field', '&lt;i4'), ('floatlike', '|S10'), ('strfield', '|S10')])\n\nIn [23]: y_pandas = pandas.DataFrame.from_records(y_np)\n\nIn [25]: y_pandas.dtypes\nOut[25]: \nint_field     int64\nfloatlike    object  # better?\nstrfield     object\n"
"import pandas\nimport numpy as np\n\ndef tocontainer(func):\n    def wrapper(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return Container(result)\n    return wrapper\n\nclass Container(object):\n   def __init__(self, df):\n       self.contained = df\n   def __getitem__(self, item):\n       result = self.contained[item]\n       if isinstance(result, type(self.contained)):\n           result = Container(result)\n       return result\n   def __getattr__(self, item):\n       result = getattr(self.contained, item)\n       if callable(result):\n           result = tocontainer(result)\n       return result\n   def __repr__(self):\n       return repr(self.contained)\n\ndf = pandas.DataFrame(\n    [(1, 2), (1, 3), (1, 4), (2, 1),(2,2,)], columns=['col1', 'col2'])\ndf = Container(df)\ndf['col1'][3] = 0\nprint(df)\n#    col1  col2\n# 0     1     2\n# 1     1     3\n# 2     1     4\n# 3     2     1\n# 4     2     2\ngp = df.groupby('col1').aggregate(np.count_nonzero)\nprint(gp)\n#       col2\n# col1      \n# 1        3\n# 2        2\nprint(type(gp))\n# &lt;class '__main__.Container'&gt;\n\nprint(type(gp[gp.col2 &gt; 2]))\n# &lt;class '__main__.Container'&gt;\n\ntf = gp[gp.col2 &gt; 2].reset_index()\nprint(type(tf))\n# &lt;class '__main__.Container'&gt;\n\nresult = df[df.col1 == tf.col1]\nprint(type(result))\n# &lt;class '__main__.Container'&gt;\n"
'In [6]: df[(df.one == 1) | (df.two == 7)]\nOut[6]: \n   one  three  two\n0    1      9    5\n2    3     17    7\n\nIn [7]: df[(df.one.isin(checkList)) | (df.two.isin(checkList))]\nOut[7]: \n   one  three  two\n0    1      9    5\n2    3     17    7\n'
'In [1]: s = Series(range(10), index=[1,2,2,2,5,6,7,7,7,8])\n\nIn [2]: s\nOut[2]:\n1    0\n2    1\n2    2\n2    3\n5    4\n6    5\n7    6\n7    7\n7    8\n8    9\n\nIn [3]: s.groupby(s.index).first()\nOut[3]:\n1    0\n2    1\n5    4\n6    5\n7    6\n8    9\n\nIn [1]: s\nOut[1]:\nSTK_ID  RPT_Date\n600809  20061231    demo\n        20070331    demo\n        20070630    demo\n        20070331    demo\n\nIn [2]: s.reset_index().groupby(s.index.names).first()\nOut[2]:\n                    0\nSTK_ID RPT_Date\n600809 20061231  demo\n       20070331  demo\n       20070630  demo\n'
"In [11]: d.name.where(d.name == 'World', np.nan)\nOut[11]: \n0      NaN\n1    World\nName: name, dtype: object\n\nIn [12]: d.name.where(d.name == 'World', np.nan).max()\nOut[12]: 'World'\n"
"In [1]: df = DataFrame([[5,3,6,7],[6,3,5,2]],index=[0,1],columns=list('ABCD'))\n\nIn [2]: df\nOut[2]: \n   A  B  C  D\n0  5  3  6  7\n1  6  3  5  2\n\nIn [3]: df.T\nOut[3]: \n   0  1\nA  5  6\nB  3  3\nC  6  5\nD  7  2\n\nIn [7]: df.T.reset_index().reindex(columns=[1,0,'index'])\nOut[7]: \n   1  0 index\n0  6  5     A\n1  3  3     B\n2  5  6     C\n3  2  7     D\n"
"grouped = data.groupby(['date', 'name'])\n\nresult = grouped.agg(combine_it)\n"
"&gt;&gt;&gt; df.index = pd.MultiIndex.from_tuples(df.index)\n&gt;&gt;&gt; df\n                     bar__sum  foo__sum\n100000550 ActivityA        14        12\n100001799 ActivityB         7         3\n\n&gt;&gt;&gt; df.index.names = ['id', 'act_type']\n&gt;&gt;&gt; df\n                     bar__sum  foo__sum\nid        act_type                     \n100000550 ActivityA        14        12\n100001799 ActivityB         7         3\n\n&gt;&gt;&gt; pd.DataFrame(d.values(), index=pd.MultiIndex.from_tuples(d.keys(), names=['id', 'act_type']))\n                     bar__sum  foo__sum\nid        act_type                     \n100001799 ActivityB         7         3\n100000550 ActivityA        14        12\n"
'&gt;&gt;&gt; for i, trial in dfTrials.iterrows():\n...     dfTrials.loc[i, "response"] = "answer {}".format(trial["no"])\n...     \n&gt;&gt;&gt; dfTrials\n   condition  no  response\n0          2   1  answer 1\n1          1   2  answer 2\n2          1   3  answer 3\n\n[3 rows x 3 columns]\n\n&gt;&gt;&gt; dfTrials["response 2"] = dfTrials["condition"] + dfTrials["no"]\n&gt;&gt;&gt; dfTrials\n   condition  no  response  response 2\n0          2   1  answer 1           3\n1          1   2  answer 2           3\n2          1   3  answer 3           4\n\n[3 rows x 4 columns]\n\n&gt;&gt;&gt; def f(row):\n...     return "c{}n{}".format(row["condition"], row["no"])\n... \n&gt;&gt;&gt; dfTrials["r3"] = dfTrials.apply(f, axis=1)\n&gt;&gt;&gt; dfTrials\n   condition  no  response  response 2    r3\n0          2   1  answer 1           3  c2n1\n1          1   2  answer 2           3  c1n2\n2          1   3  answer 3           4  c1n3\n\n[3 rows x 5 columns]\n'
'&gt;&gt;&gt; import ast\n&gt;&gt;&gt; df = pd.read_clipboard(header=None, quotechar=\'"\', sep=\',\', \n...                   converters={1:ast.literal_eval})\n&gt;&gt;&gt; df\n    0                                             1\n0  HK  [5328.1, 5329.3, 2013-12-27 13:58:57.973614]\n1  HK  [5328.1, 5329.3, 2013-12-27 13:58:59.237387]\n2  HK  [5328.1, 5329.3, 2013-12-27 13:59:00.346325]\n\n&gt;&gt;&gt; df = pd.DataFrame.from_records(df[1].tolist(), index=df[0],\n...                           columns=list(\'ABC\')).reset_index()\n&gt;&gt;&gt; df[\'C\'] = pd.to_datetime(df[\'C\'])\n&gt;&gt;&gt; df\n    0       A       B                          C\n0  HK  5328.1  5329.3 2013-12-27 13:58:57.973614\n1  HK  5328.1  5329.3 2013-12-27 13:58:59.237387\n2  HK  5328.1  5329.3 2013-12-27 13:59:00.346325\n'
"In [4]:\n\ndata = pd.Series(['a', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'])\n\npd.set_option('display.max_colwidth',1000)\n\ndata\n\nOut[4]:\n\n0                                                                         a\n1    aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n2         aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n3                                                          aaaaaaaaaaaaaaaa\n4        aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\ndtype: object\n\npd.set_printoptions(max_colwidth, 1000)\n"
'if isinstance(result.index, pandas.MultiIndex):\n'
"with pd.option_context('mode.use_inf_as_null', True):\n   df = df.dropna()\n"
"In [395]: df['count'] = df.groupby('digits')['fsq'].transform(len)\n\nIn [396]: df\nOut[396]: \n   fsq  digits digits_type  count\n0    1       1         odd      3\n1    2       1         odd      3\n2    3       1         odd      3\n3   11       2        even      2\n4   22       2        even      2\n5  101       3         odd      2\n6  111       3         odd      2\n\n[7 rows x 4 columns]\n"
'# Setting the palette using defaults only finds 6 colors\nsb.set_palette(cmap)\nsb.palplot(sb.color_palette() )\nsb.palplot(sb.color_palette(n_colors=8) )\n\n# but setting the number of colors explicitly allows it to use them all\nsb.set_palette(cmap, n_colors=8)\n# Even though unless you explicitly request all the colors it only shows 6\nsb.palplot(sb.color_palette() )\nsb.palplot(sb.color_palette(n_colors=8) )\n\n# In a chart, the palette now has access to all 8 \nfig, ax1 = plt.subplots(1,1,figsize=(4,3)) \ndf.plot(ax=ax1) \nax1.legend(bbox_to_anchor=(1.2, 1)) ;\n'
"table = pd.pivot_table(df, index=['item_id', 'hour', 'date'], columns='when', values='quantity')\n\n    item_id  hour  when      date     quantity\n0       a     1  before  2015-01-26        25\n1       b     1  before  2015-01-26        14\n2       a     1   after  2015-01-26         4\n3       d     1  before  2015-01-26        43\n4       b     1   after  2015-01-26        30\n5       d     1   after  2015-01-26        12\n\nwhen                     after  before\nitem_id hour date                     \na       1    2015-01-26      4      25\nb       1    2015-01-26     30      14\nd       1    2015-01-26     12      43\n"
'In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({"a":[1,3,5], "b":[1.1,1.2,1.2]})\n\nIn [3]: df\nOut[3]: \n   a    b\n0  1  1.1\n1  3  1.2\n2  5  1.2\n\nIn [4]: f = open("temp.txt", "w")\n\nIn [5]: for row in df.iterrows():\n    row[1].to_json(f)\n    f.write("\\n")\n   ...:     \n\nIn [6]: f.close()\n\nIn [7]: open("temp.txt").read()\nOut[7]: \'{"a":1.0,"b":1.1}\\n{"a":3.0,"b":1.2}\\n{"a":5.0,"b":1.2}\\n\'\n'
"In [3]:\n\ndf = pd.DataFrame({'a':[1,3,np.NaN, np.NaN, 4, np.NaN, 6,7,8]})\ndf\nOut[3]:\n    a\n0   1\n1   3\n2 NaN\n3 NaN\n4   4\n5 NaN\n6   6\n7   7\n8   8\nIn [6]:\n\ndf[(df.a.isnull()) &amp; (df.a.shift().isnull())]\nOut[6]:\n    a\n3 NaN\n\nIn [38]:\n\ndf = pd.DataFrame({'a':[1,2,np.NaN, np.NaN, np.NaN, 6,7,8,9,10,np.NaN,np.NaN,13,14]})\ndf\nOut[38]:\n     a\n0    1\n1    2\n2  NaN\n3  NaN\n4  NaN\n5    6\n6    7\n7    8\n8    9\n9   10\n10 NaN\n11 NaN\n12  13\n13  14\n\nIn [41]:\n\ndf.a.isnull().astype(int).groupby(df.a.notnull().astype(int).cumsum()).sum()\nOut[41]:\na\n1    0\n2    3\n3    0\n4    0\n5    0\n6    0\n7    2\n8    0\n9    0\nName: a, dtype: int32\n"
'import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import sem\ntips = sns.load_dataset("tips")\n\ntip_sumstats = (tips.groupby(["day", "sex", "smoker"])\n                     .total_bill\n                     .agg(["mean", sem])\n                     .reset_index())\n\ndef errplot(x, y, yerr, **kwargs):\n    ax = plt.gca()\n    data = kwargs.pop("data")\n    data.plot(x=x, y=y, yerr=yerr, kind="bar", ax=ax, **kwargs)\n\ng = sns.FacetGrid(tip_sumstats, col="sex", row="smoker")\ng.map_dataframe(errplot, "day", "mean", "sem")\n'
"pd.concat([df1.set_index('A'),df2.set_index('A')], axis=1, join='inner')\n\npd.concat([df1.set_index('A'),df2.set_index('A')], axis=1, join='inner').reset_index()\n"
'result = [g[1] for g in list(grouped)[:3]]\n\n# 1st\nresult[0]\n\n  item_id  user_id\n0       a        1\n1       a        2\n\n# 2nd\nresult[1]\n\n  item_id  user_id\n2       b        1\n3       b        1\n4       b        3\n'
"header = pd.MultiIndex.from_product([['location1','location2'],\n                                     ['S1','S2','S3']],\n                                    names=['loc','S'])\ndf = pd.DataFrame(np.random.randn(5, 6), \n                  index=['a','b','c','d','e'], \n                  columns=header)\n\ndf\nloc location1                     location2                    \nS          S1        S2        S3        S1        S2        S3\na   -1.245988  0.858071 -1.433669  0.105300 -0.630531 -0.148113\nb    1.132016  0.318813  0.949564 -0.349722 -0.904325  0.443206\nc   -0.017991  0.032925  0.274248  0.326454 -0.108982  0.567472\nd    2.363533 -1.676141  0.562893  0.967338 -1.071719 -0.321113\ne    1.921324  0.110705  0.023244 -0.432196  0.172972 -0.50368\n\ndf.xs('location1',level='loc',axis=1)\n\nS        S1        S2        S3\na -1.245988  0.858071 -1.433669\nb  1.132016  0.318813  0.949564\nc -0.017991  0.032925  0.274248\nd  2.363533 -1.676141  0.562893\ne  1.921324  0.110705  0.02324\n\ndf.xs('S1',level='S',axis=1)\n\nloc  location1  location2\na    -1.245988   0.105300\nb     1.132016  -0.349722\nc    -0.017991   0.326454\nd     2.363533   0.967338\ne     1.921324  -0.43219\n"
"frame.loc[frame['DESIGN_VALUE'] &gt; 20,['mycol3', 'mycol6']]\n\nIn [184]:\ndf = pd.DataFrame(columns = list('abc'), data = np.random.randn(5,3))\ndf\n\nOut[184]:\n          a         b         c\n0 -0.628354  0.833663  0.658212\n1  0.032443  1.062135 -0.335318\n2 -0.450620 -0.906486  0.015565\n3  0.280459 -0.375468 -1.603993\n4  0.463750 -0.638107 -1.598261\n\nIn [187]:\ndf.loc[df['a']&gt;0, ['b','c']]\n\nOut[187]:\n          b         c\n1  1.062135 -0.335318\n3 -0.375468 -1.603993\n4 -0.638107 -1.598261\n\nframe[(frame.DESIGN_VALUE &gt; 20) &amp; (frame['mycol3','mycol6'])]\n"
'In [417]:\nt="""\'Name\',97.7,0A,0A,65M,0A,100M,5M,75M,100M,90M,90M,99M,90M,0#,0N#,"""\ndf = pd.read_csv(io.StringIO(t), header=None)\ndf\n\nOut[417]:\n       0     1   2   3    4   5     6   7    8     9    10   11   12   13  14  \\\n0  \'Name\'  97.7  0A  0A  65M  0A  100M  5M  75M  100M  90M  90M  99M  90M  0#   \n\n    15  16  \n0  0N# NaN  \n\nIn [421]:\nfor col in df.columns[2:-1]:\n    df[col] = df[col].str.extract(r\'(\\d+\\.*\\d*)\').astype(np.float)\ndf\n\nOut[421]:\n       0     1   2   3   4   5    6   7   8    9   10  11  12  13  14  15  16\n0  \'Name\'  97.7   0   0  65   0  100   5  75  100  90  90  99  90   0   0 NaN\n\nIn [428]:\nfor col in df.select_dtypes([np.object]).columns[1:]:\n    df[col] = df[col].str.extract(r\'(\\d+\\.*\\d*)\').astype(np.float)\ndf\n\nOut[428]:\n       0     1   2   3   4   5    6   7   8    9   10  11  12  13  14  15  16\n0  \'Name\'  97.7   0   0  65   0  100   5  75  100  90  90  99  90   0   0 NaN\n'
"df.apply(test, axis=1)\n\ndf['A-B'] = df.A - df.B\n"
"n_page = (pd.pivot_table(Main_DF, \n                         values='SPC_RAW_VALUE',  \n                         index=['ALIAS', 'SPC_PRODUCT', 'LABLE', 'RAW_PARAMETER_NAME'], \n                         columns=['LOT_VIRTUAL_LINE'],\n                         aggfunc=[len, np.mean, np.std])\n          .reset_index()\n         )\n"
"dfnew = df.groupby('Groups').filter(lambda x: x['Count'].min()&gt;8 )\ndfnew.reset_index(drop=True, inplace=True) # reset index\ndfnew = dfnew[['Groups','Count']] # rearrange the column sequence\nprint(dfnew)\n\nOutput:\n   Groups  Count\n0       2     12\n1       2     15\n2       2     21\n"
"aht['call_start'] = aht['start'].dt.tz_localize('US/Eastern').dt.tz_convert('US/Central')\n"
'times = df.interval.unique()\ng = sns.FacetGrid(df, row="variable", hue="segment",\n                  palette="Set3", size=4, aspect=2)\ng.map(sns.barplot, \'interval\', \'value\', order=times)\n'
'reversed_df = df.iloc[::-1]\n'
'for i in df.index:\n    df.loc[i].to_json("row{}.json".format(i))\n'
"test['Date'] = pd.to_datetime(test['Date'])\n\ndf = test.groupby('User').apply(lambda x: x.set_index('Date').resample('1D').first())\nprint df\n                 User  Value\nUser Date                   \nJohn 2016-04-01  John    2.0\n     2016-04-02  John    3.0\n     2016-04-03   NaN    NaN\n     2016-04-04   NaN    NaN\n     2016-04-05   NaN    NaN\n     2016-04-06  John    6.0\nMike 2016-04-01  Mike    1.0\n     2016-04-02  Mike    1.0\n     2016-04-03  Mike    4.5\n     2016-04-04  Mike    1.0\n     2016-04-05  Mike    2.0\n\ndf1 = df.groupby(level=0)['Value']\n        .apply(lambda x: x.shift().rolling(min_periods=1,window=2).mean())\n        .reset_index(name='Value_Average_Past_2_days')\n\nprint df1\n    User       Date  Value_Average_Past_2_days\n0   John 2016-04-01                        NaN\n1   John 2016-04-02                       2.00\n2   John 2016-04-03                       2.50\n3   John 2016-04-04                       3.00\n4   John 2016-04-05                        NaN\n5   John 2016-04-06                        NaN\n6   Mike 2016-04-01                        NaN\n7   Mike 2016-04-02                       1.00\n8   Mike 2016-04-03                       1.00\n9   Mike 2016-04-04                       2.75\n10  Mike 2016-04-05                       2.75\n11  Mike 2016-04-06                       1.50\n\nprint pd.merge(test, df1, on=['Date', 'User'], how='left')\n        Date  User  Value  Value_Average_Past_2_days\n0 2016-04-01  Mike    1.0                        NaN\n1 2016-04-01  John    2.0                        NaN\n2 2016-04-02  Mike    1.0                       1.00\n3 2016-04-02  John    3.0                       2.00\n4 2016-04-03  Mike    4.5                       1.00\n5 2016-04-04  Mike    1.0                       2.75\n6 2016-04-05  Mike    2.0                       2.75\n7 2016-04-06  Mike    3.0                       1.50\n8 2016-04-06  John    6.0                        NaN\n"
'import pandas as pd\n\ndf = pd.DataFrame({\'A\': {0: 2, 1: 1}, \n                   \'C\': {0: 5, 1: 1}, \n                   \'B\': {0: 4, 1: 2}, \n                   \'filename\': {0: "txt.txt", 1: "x.txt"}}, \n                columns=[\'filename\',\'A\',\'B\', \'C\'])\n\nprint df\n  filename  A  B  C\n0  txt.txt  2  4  5\n1    x.txt  1  2  1\n\ndf[\'filename\'] = df[\'filename\'].str.replace(r\'.txt$\', \'\')\nprint df\n  filename  A  B  C\n0      txt  2  4  5\n1        x  1  2  1\n\ndf[\'filename\'] = df[\'filename\'].map(lambda x: str(x)[:-4])\nprint df\n  filename  A  B  C\n0      txt  2  4  5\n1        x  1  2  1\n\ndf[\'filename\'] = df[\'filename\'].str[:-4]\nprint df\n  filename  A  B  C\n0      txt  2  4  5\n1        x  1  2  1\n\nprint df\n  filename  A  B  C\n0  txt.txt  2  4  5\n1    x.txt  1  2  1\n\ndf[\'filename\'] = df[\'filename\'].str.rstrip(\'.txt\')\n\nprint df\n  filename  A  B  C\n0           2  4  5\n1           1  2  1\n'
"import pandas as pd\n\nlist1 = [1,2]\nlist2 = [2,5]\ndf=pd.DataFrame({'Name' : list1,'Probability' : list2})\nprint (df)\n   Name  Probability\n0     1            2\n1     2            5\n\ndf.set_index('Name', inplace=True)\nprint (df)\n      Probability\nName             \n1               2\n2               5\n\ndf.set_index('Name', inplace=True)\n#pandas 0.18.0 and higher\ndf = df.rename_axis(None)\n#pandas bellow 0.18.0\n#df.index.name = None\nprint (df)\n   Probability\n1            2\n2            5\n"
"In [42]: df\nOut[42]: \n        A       B      C\n1   apple  banana   pear\n2    pear    pear  apple\n3  banana    pear   pear\n4   apple   apple   pear\n\nIn [42]: df == 'banana'\nOut[42]: \n       A      B      C\n1  False   True  False\n2  False  False  False\n3   True  False  False\n4  False  False  False\n\nIn [43]: (df == 'banana').any(axis=1)\nOut[43]: \n1     True\n2    False\n3     True\n4    False\ndtype: bool\n\nIn [44]: df[(df == 'banana').any(axis=1)]\nOut[44]: \n        A       B     C\n1   apple  banana  pear\n3  banana    pear  pear\n\nIn [42]: df\nOut[42]: \n        A       B      C\n1   apple  banana   pear\n2    pear    pear  apple\n3  banana    pear   pear\n4   apple   apple   pear\n\nIn [51]: np.isin(df, ['pear','apple'])\nOut[51]: \narray([[ True, False,  True],\n       [ True,  True,  True],\n       [False,  True,  True],\n       [ True,  True,  True]])\n\n# ANY match along each row\nIn [52]: np.isin(df, ['pear','apple']).any(axis=1)\nOut[52]: array([ True,  True,  True,  True])\n\n# Select corresponding rows with masking\nIn [56]: df[np.isin(df, ['pear','apple']).any(axis=1)]\nOut[56]: \n        A       B      C\n1   apple  banana   pear\n2    pear    pear  apple\n3  banana    pear   pear\n4   apple   apple   pear\n\nIn [42]: df\nOut[42]: \n        A       B      C\n1   apple  banana   pear\n2    pear    pear  apple\n3  banana    pear   pear\n4   apple   apple   pear\n\nIn [66]: np.equal.outer(df.to_numpy(copy=False),  ['pear','apple']).any(axis=1)\nOut[66]: \narray([[ True,  True],\n       [ True,  True],\n       [ True, False],\n       [ True,  True]])\n\nIn [62]: np.equal.outer(df.to_numpy(copy=False),  ['apple','banana', 'pear']).any(axis=1)\nOut[62]: \narray([[ True,  True,  True],\n       [ True, False,  True],\n       [False,  True,  True],\n       [ True, False,  True]])\n\nIn [66]: np.equal.outer(df.to_numpy(copy=False),  ['pear','apple']).any(axis=1)\nOut[66]: \narray([[ True,  True],\n       [ True,  True],\n       [ True, False],\n       [ True,  True]])\n\nIn [67]: np.equal.outer(df.to_numpy(copy=False),  ['pear','apple']).any(axis=1).all(axis=1)\nOut[67]: array([ True,  True, False,  True])\n\nIn [70]: df[np.equal.outer(df.to_numpy(copy=False),  ['pear','apple']).any(axis=1).all(axis=1)]\nOut[70]: \n       A       B      C\n1  apple  banana   pear\n2   pear    pear  apple\n4  apple   apple   pear\n"
"df.groupby('userid')['name'].apply(lambda df: df.reset_index(drop=True)).unstack()\n\ndf = pd.DataFrame([\n        [123, 'abc'],\n        [123, 'abc'],\n        [456, 'def'],\n        [123, 'abc'],\n        [123, 'abc'],\n        [456, 'def'],\n        [456, 'def'],\n        [456, 'def'],\n    ], columns=['userid', 'name'])\n\ndf.sort_values('userid').groupby('userid')['name'].apply(lambda df: df.reset_index(drop=True)).unstack()\n\ndf.sort_values('userid').groupby('userid')['name'].apply(lambda df: df.reset_index(drop=True)).unstack().reset_index()\n"
'""" \n*rows*, *cols*, *num* are arguments where\nthe array of subplots in the figure has dimensions *rows*,\n*cols*, and where *num* is the number of the subplot\nbeing created. *num* starts at 1 in the upper left\ncorner and increases to the right.\n"""\nrows, cols, num = args\nrows = int(rows)\ncols = int(cols)\nif isinstance(num, tuple) and len(num) == 2:\n    num = [int(n) for n in num]\n    self._subplotspec = GridSpec(rows, cols)[num[0] - 1:num[1]]\nelse:\n    if num &lt; 1 or num &gt; rows*cols:\n        raise ValueError(      \n            "num must be 1 &lt;= num &lt;= {maxn}, not {num}".format(\n                maxn=rows*cols, num=num))\n\n    if num &lt; 1 or num &gt; rows*cols:\n     # maxN is the number of rows*cols and since this is showing 0 for you (in your error stacktrace), \n     # it means the number of cols being passed into your histogram is 0. Don\'t know why though :P\n        raise ValueError(      \n            "num must be 1 &lt;= num &lt;= {maxn}, not {num}".format(\n                maxn=rows*cols, num=num))\n\n    x_train =   [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n\n                [1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n\n                [0.3333333333333333, 0.3333333333333333, 2.0, 2.0, 2.0, 2.0],\n\n                [0.0, 0.0, 3.0, 3.0, 3.0, 3.0]]\n\nx_train = list([list(x) for x in x_train])\n'
'&gt;&gt;&gt; df\n   Survived  Pclass  Sex  Age  SibSp  Parch  Nonsense\n0         0       3    1   22      1      0         0\n1         1       1    2   38      1      0         0\n2         1       3    2   26      0      0         0\n\n&gt;&gt;&gt; from sklearn.feature_selection import VarianceThreshold\n&gt;&gt;&gt; def variance_threshold_selector(data, threshold=0.5):\n    selector = VarianceThreshold(threshold)\n    selector.fit(data)\n    return data[data.columns[selector.get_support(indices=True)]]\n\n&gt;&gt;&gt; variance_threshold_selector(df, 0.5)\n   Pclass  Age\n0       3   22\n1       1   38\n2       3   26\n&gt;&gt;&gt; variance_threshold_selector(df, 0.9)\n   Age\n0   22\n1   38\n2   26\n&gt;&gt;&gt; variance_threshold_selector(df, 0.1)\n   Survived  Pclass  Sex  Age  SibSp\n0         0       3    1   22      1\n1         1       1    2   38      1\n2         1       3    2   26      0\n'
'df[df[0]==""] = np.NaN\n\ndf.fillna(method=\'ffill\')\n#       0\n#0  Text\n#1    30\n#2    30\n#3    30\n#4    31\n#5  Text\n#6    31\n#7    31\n#8    31\n#9    32\n'
"dtypeCount =[df.iloc[:,i].apply(type).value_counts() for i in range(df.shape[1])]\n\ndtypeCount\n\n[&lt;class 'numpy.int32'&gt;    4\n Name: a, dtype: int64,\n &lt;class 'int'&gt;    2\n &lt;class 'str'&gt;    2\n Name: b, dtype: int64,\n &lt;class 'numpy.int32'&gt;    4\n Name: c, dtype: int64]\n\ndtypeCount[1]\n\n&lt;class 'int'&gt;    2\n&lt;class 'str'&gt;    2\nName: b, dtype: int64\n\ndf[df.iloc[:,1].map(lambda x: type(x) == str)]\n\n   a  b  c\n1  1  n  4\n3  3  g  6\n\ndf = DataFrame({'a': range(4),\n                'b': [6, 'n', 7, 'g'],\n                'c': range(3, 7)})\n"
'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\ncsv = pd.read_csv(\'/tmp/test.csv\')\ndata = csv[[\'fee\', \'time\']]\nx = data[\'fee\']\ny = data[\'time\']\nplt.scatter(x, y)\n\nz = np.polyfit(x, y, 1)\np = np.poly1d(z)\nplt.plot(x,p(x),"r--")\n\nplt.show()\n'
"def get_rolling_amount(grp, freq):\n    return grp.rolling(freq, on='date')['amount'].sum()\n\ndf['rolling_sales_180'] = df.groupby('name', as_index=False, group_keys=False) \\\n                            .apply(get_rolling_amount, '180D')\n\n    name       date  amount  rolling_sales_180\n0  David 2015-01-01     100              100.0\n1  David 2015-01-05     500              600.0\n2  David 2015-05-30      50              650.0\n3  David 2015-07-25      50              100.0\n4   Ryan 2014-01-04     100              100.0\n5   Ryan 2015-01-19     500              500.0\n6   Ryan 2016-03-31      50               50.0\n7    Joe 2015-07-01     100              100.0\n8    Joe 2015-09-09     500              600.0\n9    Joe 2015-10-15      50              650.0\n"
"import os\n\nfor csv in globbed_files:\n    frame = pd.read_csv(csv)\n    frame['filename'] = os.path.basename(csv)\n    data.append(frame)\n"
'for row in ws.iter_rows():\n    for cell in row:\n        cell.style.alignment.wrap_text=True\n'
"import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame( [1,2,3,4,1], columns = ['Var'] )\n&gt;&gt;&gt; df\n   Var\n0    1\n1    2\n2    3\n3    4\n4    1\n&gt;&gt;&gt; dict = {1:2, 2:3, 3:1, 4:3}\n&gt;&gt;&gt; df.Var.map( dict )\n0    2\n1    3\n2    1\n3    3\n4    2\nName: Var, dtype: int64\n\n&gt;&gt;&gt; df = pd.DataFrame( [1,2,3,4,1], columns = ['Var'] )\n&gt;&gt;&gt; dict = {1:2, 2:3, 3:1}\n&gt;&gt;&gt; df.Var.map( dict )\n0    2.0\n1    3.0\n2    1.0\n3    NaN\n4    2.0\nName: Var, dtype: float64\n"
"df['new'] = df.sum(axis=1).astype(int).astype(str)\n\ndf['new'] = df.apply(''.join, axis=1)\n\ndf['new'] = df.values.sum(axis=1)\n\ndf = pd.DataFrame({'A': ['1', '2', '3'], 'B': ['4', '5', '6'], 'C': ['7', '8', '9']})\n#[30000 rows x 3 columns]\ndf = pd.concat([df]*10000).reset_index(drop=True)\n#print (df)\n\ncols = list('ABC')\n\n#not_a_robot solution\nIn [259]: %timeit df['concat'] = pd.Series(df[cols].fillna('').values.tolist()).str.join('')\n100 loops, best of 3: 17.4 ms per loop\n\nIn [260]: %timeit df['new'] = df[cols].astype(str).apply(''.join, axis=1)\n1 loop, best of 3: 386 ms per loop\n\nIn [261]: %timeit df['new1'] = df[cols].values.sum(axis=1)\n100 loops, best of 3: 6.5 ms per loop\n\nIn [262]: %timeit df['new2'] = df[cols].astype(str).sum(axis=1).astype(int).astype(str)\n10 loops, best of 3: 68.6 ms per loop\n\ndf['new'] = df.astype(str).values.sum(axis=1)\n"
"df['name_length']  = df['seller_name'].str.len()\n\n  seller_name  name_length\n0        Rick            4\n1      Hannah            6\n"
"np.random.seed(100)\ndf = pd.DataFrame(np.random.randn(5,4))\ndf.iloc[1,2] = np.NaN\ndf.iloc[0,1] = np.NaN\ndf.iloc[2,1] = np.NaN\ndf.iloc[2,0] = np.NaN\nprint (df)\n          0         1         2         3\n0 -1.749765       NaN  1.153036 -0.252436\n1  0.981321  0.514219       NaN -1.070043\n2       NaN       NaN -0.458027  0.435163\n3 -0.583595  0.816847  0.672721 -0.104411\n4 -0.531280  1.029733 -0.438136 -1.118318\n\ndf1 = df.apply(lambda x: pd.Series(x.dropna().values))\nprint (df1)\n          0         1         2         3\n0 -1.749765  0.514219  1.153036 -0.252436\n1  0.981321  0.816847 -0.458027 -1.070043\n2 -0.583595  1.029733  0.672721  0.435163\n3 -0.531280       NaN -0.438136 -0.104411\n4       NaN       NaN       NaN -1.118318\n\ndf1 = df.apply(lambda x: pd.Series(x.dropna().values)).fillna('')\nprint (df1)\n          0         1         2         3\n0  -1.74977  0.514219   1.15304 -0.252436\n1  0.981321  0.816847 -0.458027 -1.070043\n2 -0.583595   1.02973  0.672721  0.435163\n3  -0.53128           -0.438136 -0.104411\n4                               -1.118318\n"
"from datetime import datetime\na = '20160228'\ndate = datetime.strptime(a, '%Y%m%d').strftime('%m/%d/%Y')\n"
'new_df = sales[~sales.CustomerID.isin(badcu)]\n'
"from IPython.display import display\nimport pandas as pd \n\nmy_df = pd.DataFrame({'foo':[1,2,3],'bar':[7,8,9]})\ndisplay(my_df)\n\n$ python test.py \n   bar  foo\n0    7    1\n1    8    2\n2    9    3\n\nfrom IPython.display import display, HTML\nimport pandas as pd \nmy_df = pd.DataFrame({'foo':[1,2,3],'bar':[7,8,9]})\n\ntry:\n    get_ipython\n    display(my_df)\nexcept:\n    print(my_df)\n"
'&gt;&gt;&gt; {[1, 2]: 3}\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nTypeError: unhashable type: \'list\'\n&gt;&gt;&gt; {(1, 2): 3}\n{(1, 2): 3}\n\ndata = pd.Series(np.random.randn(100), index=pd.date_range(\'01/01/2001\', periods=100))\nkeys = lambda x: (x.year,x.month)  # &lt;----\ndata.groupby(keys).mean()\n'
"df = pd.DataFrame({'Col1' : ['nan', 'foo', 'bar', 'baz', 'nan', 'test']})\ndf.replace('nan', '')\n\n   Col1\n0      \n1   foo\n2   bar\n3   baz\n4      \n5  test\n\ndf = df.replace('nan', '')\n\ndf = df.fillna('')    \n"
"f = {'NET_AMT': 'sum', 'QTY_SOLD': 'sum', 'UPC_DSC': 'first'}\ndf.groupby(['month', 'UPC_ID'], as_index=False).agg(f)\n\n     month  UPC_ID UPC_DSC  NET_AMT  QTY_SOLD\n0  2017.02     111   desc1       10         2\n1  2017.02     222   desc2       15         3\n2  2017.02     333   desc3        4         1\n3  2017.03     111   desc1       25         5\n\ng = df.groupby(['month', 'UPC_ID'])\ni = g.sum()\nj = g[['UPC_DSC']].first()\n\npd.concat([i, j], 1).reset_index()\n\n     month  UPC_ID  QTY_SOLD  NET_AMT UPC_DSC\n0  2017.02     111         2       10   desc1\n1  2017.02     222         3       15   desc2\n2  2017.02     333         1        4   desc3\n3  2017.03     111         5       25   desc1\n"
"df = df.drop_duplicates(subset=['C1','C2','C3'])\n\ndf = df.groupby(by=['C1','C2','C3'], as_index=False).first()\n"
'#!/usr/bin/python\nfrom    multiprocessing.managers import SyncManager\nimport  numpy\n\n# Global for storing the data to be served\ngData = {}\n\n# Proxy class to be shared with different processes\n# Don\'t put big data in here since that will force it to be piped to the\n# other process when instantiated there, instead just return a portion of\n# the global data when requested.\nclass DataProxy(object):\n    def __init__(self):\n        pass\n\n    def getData(self, key, default=None):\n        global gData\n        return gData.get(key, None)\n\nif __name__ == \'__main__\':\n    port  = 5000\n\n    print \'Simulate loading some data\'\n    for i in xrange(1000):\n        gData[i] = numpy.random.rand(1000)\n\n    # Start the server on address(host,port)\n    print \'Serving data. Press &lt;ctrl&gt;-c to stop.\'\n    class myManager(SyncManager): pass\n    myManager.register(\'DataProxy\', DataProxy)\n    mgr = myManager(address=(\'\', port), authkey=\'DataProxy01\')\n    server = mgr.get_server()\n    server.serve_forever()\n\nfrom   multiprocessing.managers import BaseManager\nimport psutil   #3rd party module for process info (not strictly required)\n\n# Grab the shared proxy class.  All methods in that class will be availble here\nclass DataClient(object):\n    def __init__(self, port):\n        assert self._checkForProcess(\'DataServer.py\'), \'Must have DataServer running\'\n        class myManager(BaseManager): pass\n        myManager.register(\'DataProxy\')\n        self.mgr = myManager(address=(\'localhost\', port), authkey=\'DataProxy01\')\n        self.mgr.connect()\n        self.proxy = self.mgr.DataProxy()\n\n    # Verify the server is running (not required)\n    @staticmethod\n    def _checkForProcess(name):\n        for proc in psutil.process_iter():\n            if proc.name() == name:\n                return True\n        return False\n\n#!/usr/bin/python\nimport time\nimport multiprocessing as mp\nimport numpy\nfrom   DataClient import *    \n\n# Confusing, but the "proxy" will be global to each subprocess, \n# it\'s not shared across all processes.\ngProxy = None\ngMode  = None\ngDummy = None\ndef init(port, mode):\n    global gProxy, gMode, gDummy\n    gProxy  = DataClient(port).proxy\n    gMode  = mode\n    gDummy = numpy.random.rand(1000)  # Same as the dummy in the server\n    #print \'Init proxy \', id(gProxy), \'in \', mp.current_process()\n\ndef worker(key):\n    global gProxy, gMode, gDummy\n    if 0 == gMode:   # get from proxy\n        array = gProxy.getData(key)\n    elif 1 == gMode: # bypass retrieve to test difference\n        array = gDummy\n    else: assert 0, \'unknown mode: %s\' % gMode\n    for i in range(1000):\n        x = sum(array)\n    return x    \n\nif __name__ == \'__main__\':\n    port   = 5000\n    maxkey = 1000\n    numpts = 100\n\n    for mode in [1, 0]:\n        for nprocs in [16, 1]:\n            if 0==mode: print \'Using client/server and %d processes\' % nprocs\n            if 1==mode: print \'Using local data and %d processes\' % nprocs                \n            keys = [numpy.random.randint(0,maxkey) for k in xrange(numpts)]\n            pool = mp.Pool(nprocs, initializer=init, initargs=(port,mode))\n            start = time.time()\n            ret_data = pool.map(worker, keys, chunksize=1)\n            print \'   took %4.3f seconds\' % (time.time()-start)\n            pool.close()\n\nUsing local data and 16 processes\n   took 0.695 seconds\nUsing local data and 1 processes\n   took 5.849 seconds\nUsing client/server and 16 processes\n   took 0.811 seconds\nUsing client/server and 1 processes\n   took 5.956 seconds\n'
"df_new = df[df.groupby('dealer').date.transform('max') == df['date']]\n\n    invoice_no  dealer  billing_change_previous_month   date\n1   100         1       -41981                          2017-01-30\n2   5505        2       0                               2017-01-30\n\ndf = pd.DataFrame({'invoice_no':[110,100,5505,5635,10000,10001], 'dealer':[1,1,2,2,3,3],'billing_change_previous_month':[0,-41981,0,58730,9000,100], 'date':['2016-12-31','2017-01-30','2017-01-30','2016-12-31', '2019-12-31', '2020-01-31']})\n\ndf['date'] = pd.to_datetime(df['date'])\ndf[df.groupby('dealer').date.transform('max') == df['date']]\n\n\n    invoice_no  dealer  billing_change_previous_month   date\n1   100         1       -41981                          2017-01-30\n2   5505        2       0                               2017-01-30\n5   10001       3       100                             2020-01-31\n"
"df['uuid'] = [uuid.uuid4() for _ in range(len(df.index))]\n"
'In [21]: df\nOut[21]:\n                   col1           col2\n0  04-APR-2018 11:04:29  2018040415203\n\nIn [22]: pd.to_datetime(df.col1)\nOut[22]:\n0   2018-04-04 11:04:29\nName: col1, dtype: datetime64[ns]\n\nIn [23]: pd.to_datetime(df.col2, format="%Y%m%d%H%M%S")\nOut[23]:\n0   2018-04-04 15:20:03\nName: col2, dtype: datetime64[ns]\n'
"df['Date'] = df['Date'].astype(str)\n\ndf['Date'] = df['Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n\nprint (df.dtypes)\nId      object\nName    object\nDate    object\nType    object\ndtype: object\n"
'pd.show_versions()\n\nINSTALLED VERSIONS\n------------------\ncommit: None\npython: 3.6.0.final.0\npython-bits: 64\nOS: Windows\nOS-release: 7\nmachine: AMD64\nprocessor: Intel64 Family 6 Model 42 Stepping 7, GenuineIntel\nbyteorder: little\nLC_ALL: None\nLANG: None\nLOCALE: None.None\n\npandas: 0.23.0\npytest: 3.0.5\npip: 9.0.3\nsetuptools: 27.2.0\nCython: 0.25.2\nnumpy: 1.14.3\nscipy: 1.1.0\npyarrow: None\nxarray: None\nIPython: 5.1.0\nsphinx: 1.5.1\npatsy: 0.4.1\ndateutil: 2.6.0\npytz: 2016.10\nblosc: None\nbottleneck: 1.2.1\ntables: 3.4.3\nnumexpr: 2.6.5\nfeather: None\nmatplotlib: 2.2.2\nopenpyxl: 2.4.1\nxlrd: 1.0.0\nxlwt: 1.2.0\nxlsxwriter: 0.9.6\nlxml: 3.7.2\nbs4: 4.5.3\nhtml5lib: 0.9999999\nsqlalchemy: 1.1.5\npymysql: None\npsycopg2: None\njinja2: 2.9.4\ns3fs: None\nfastparquet: None\npandas_gbq: None\npandas_datareader: None\n\nIn[193]:\npd.io.parquet.get_engine(\'auto\')\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-193-929185e5aca8&gt; in &lt;module&gt;()\n----&gt; 1 pd.io.parquet.get_engine(\'auto\')\n\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parquet.py in get_engine(engine)\n     27             pass\n     28 \n---&gt; 29         raise ImportError("Unable to find a usable engine; "\n     30                           "tried using: \'pyarrow\', \'fastparquet\'.\\n"\n     31                           "pyarrow or fastparquet is required for parquet "\n\nImportError: Unable to find a usable engine; tried using: \'pyarrow\', \'fastparquet\'.\npyarrow or fastparquet is required for parquet support\n\nIn[194]:\npd.io.parquet.get_engine(\'auto\')\n\nOut[194]: &lt;pandas.io.parquet.FastParquetImpl at 0xf5582b0&gt;\n\nIn[202]:\nimpl = pd.io.parquet.get_engine(\'auto\')\nimpl.__class__\n\nOut[202]: pandas.io.parquet.FastParquetImpl\n\n&gt;&gt;&gt; pd.io.parquet.get_engine(\'auto\')\n&lt;pandas.io.parquet.PyArrowImpl object at 0xa13fb1ef0&gt;\n&gt;&gt;&gt; pd.io.parquet.get_engine(\'auto\').__class__\n&lt;class \'pandas.io.parquet.PyArrowImpl\'&gt;\n'
'&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; df = pd.DataFrame([[0, 3, 0, 1, np.nan]], columns=[\'v\', \'y\', \'v\', \'x\', \'z\'])\n&gt;&gt;&gt; np.shares_memory(df[\'z\'].values, df[\'z\'])\nFalse\n\n&gt;&gt;&gt; df = pd.DataFrame([[0, 3, 1, np.nan]], columns=[\'v\', \'y\', \'x\', \'z\'])\n&gt;&gt;&gt; np.shares_memory(df[\'z\'].values, df[\'z\'])\nTrue\n\nimport numba as nb\nimport numpy as np\nimport pandas as pd\n\n@nb.njit\ndef f_(n, x, y, z):\n    for i in range(n):\n        z[i] = x[i] * y[i] \n    return z  # this is new\n\n&gt;&gt;&gt; df = pd.DataFrame([[0, 3, 0, 1, np.nan]], columns=[\'v\', \'y\', \'v\', \'x\', \'z\'])\n&gt;&gt;&gt; df[\'z\'] = f_(df.shape[0], df["x"].values, df["y"].values, df["z"].values)\n&gt;&gt;&gt; df\n   v  y  v  x    z\n0  0  3  0  1  3.0\n\n&gt;&gt;&gt; df = pd.DataFrame([[0, 3, 1, np.nan]], columns=[\'v\', \'y\', \'x\', \'z\'])\n&gt;&gt;&gt; df[\'z\'] = f_(df.shape[0], df["x"].values, df["y"].values, df["z"].values)\n&gt;&gt;&gt; df\n   v  y  x    z\n0  0  3  1  3.0\n\n&gt;&gt;&gt; df = pd.DataFrame([[0, 3, 0, 1, np.nan]], columns=[\'v\', \'y\', \'v\', \'x\', \'z\'])\n&gt;&gt;&gt; df.blocks\n{\'float64\':\n     z\n  0  NaN\n , \n \'int64\':\n     v  y  v  x\n  0  0  3  0  1}\n\n&gt;&gt;&gt; df = pd.DataFrame([[0, 3, 0, 1, np.nan]], columns=[\'v\', \'y\', \'v\', \'x\', \'z\'])\n&gt;&gt;&gt; df[\'v\']\n   v  v\n0  0  0\n\n    def _getitem_column(self, key):\n        """ return the actual column """\n\n        # get column\n        if self.columns.is_unique:\n            return self._get_item_cache(key)\n\n        # duplicate columns &amp; possible reduce dimensionality\n        result = self._constructor(self._data.get(key))\n        if result.columns.is_unique:\n            result = result[key]\n\n    return result\n'
'client.get_query_results(QueryExecutionId=res[\'QueryExecutionId\'], MaxResults=2000)\n\ndef obtain_data_from_s3(self):\n    self.resource = boto3.resource(\'s3\', \n                          region_name = self.region_name, \n                          aws_access_key_id = self.aws_access_key_id,\n                          aws_secret_access_key= self.aws_secret_access_key)\n\n    response = self.resource \\\n    .Bucket(self.bucket) \\\n    .Object(key= self.folder + self.filename + \'.csv\') \\\n    .get()\n\n    return pd.read_csv(io.BytesIO(response[\'Body\'].read()), encoding=\'utf8\')   \n\nself.filename = response[\'QueryExecutionId\'] + ".csv"\n\nimport time\nimport boto3\nimport pandas as pd\nimport io\n\nclass QueryAthena:\n\n    def __init__(self, query, database):\n        self.database = database\n        self.folder = \'my_folder/\'\n        self.bucket = \'my_bucket\'\n        self.s3_input = \'s3://\' + self.bucket + \'/my_folder_input\'\n        self.s3_output =  \'s3://\' + self.bucket + \'/\' + self.folder\n        self.region_name = \'us-east-1\'\n        self.aws_access_key_id = "my_aws_access_key_id"\n        self.aws_secret_access_key = "my_aws_secret_access_key"\n        self.query = query\n\n    def load_conf(self, q):\n        try:\n            self.client = boto3.client(\'athena\', \n                              region_name = self.region_name, \n                              aws_access_key_id = self.aws_access_key_id,\n                              aws_secret_access_key= self.aws_secret_access_key)\n            response = self.client.start_query_execution(\n                QueryString = q,\n                    QueryExecutionContext={\n                    \'Database\': self.database\n                    },\n                    ResultConfiguration={\n                    \'OutputLocation\': self.s3_output,\n                    }\n            )\n            self.filename = response[\'QueryExecutionId\']\n            print(\'Execution ID: \' + response[\'QueryExecutionId\'])\n\n        except Exception as e:\n            print(e)\n        return response                \n\n    def run_query(self):\n        queries = [self.query]\n        for q in queries:\n            res = self.load_conf(q)\n        try:              \n            query_status = None\n            while query_status == \'QUEUED\' or query_status == \'RUNNING\' or query_status is None:\n                query_status = self.client.get_query_execution(QueryExecutionId=res["QueryExecutionId"])[\'QueryExecution\'][\'Status\'][\'State\']\n                print(query_status)\n                if query_status == \'FAILED\' or query_status == \'CANCELLED\':\n                    raise Exception(\'Athena query with the string "{}" failed or was cancelled\'.format(self.query))\n                time.sleep(10)\n            print(\'Query "{}" finished.\'.format(self.query))\n\n            df = self.obtain_data()\n            return df\n\n        except Exception as e:\n            print(e)      \n\n    def obtain_data(self):\n        try:\n            self.resource = boto3.resource(\'s3\', \n                                  region_name = self.region_name, \n                                  aws_access_key_id = self.aws_access_key_id,\n                                  aws_secret_access_key= self.aws_secret_access_key)\n\n            response = self.resource \\\n            .Bucket(self.bucket) \\\n            .Object(key= self.folder + self.filename + \'.csv\') \\\n            .get()\n\n            return pd.read_csv(io.BytesIO(response[\'Body\'].read()), encoding=\'utf8\')   \n        except Exception as e:\n            print(e)  \n\n\nif __name__ == "__main__":       \n    query = "SELECT * FROM bucket.folder"\n    qa = QueryAthena(query=query, database=\'myAthenaDb\')\n    dataframe = qa.run_query()\n'
"df.loc[(df.index &lt; 6) &amp; (df.A == 0), 'C'] = 99\n"
"df=pd.DataFrame({'test':range(9),'test2':range(9)})\nsns.lineplot(x=df.index, y='test', data=df)\n"
"count_id = df.id.value_counts().sort_index().values\nmask = count_id[:,None] &gt; np.arange(count_id.max())\nvals = df.loc[:, 'date':'value2'].values\nout_shp = mask.shape + (vals.shape[1],)\nout = np.full(out_shp, np.nan)\nout[mask] = vals\n\nx = df.id.factorize()[0]   \ny = df.groupby(x).cumcount().values\nvals = df.loc[:, 'date':'value2'].values\nout_shp = (x.max()+1, y.max()+1, vals.shape[1])\nout = np.full(out_shp, np.nan)\nout[x,y] = vals\n"
"df[['add', 'multiply']]=df.apply(lambda x: add_multiply(x['col1'], x['col2']),axis=1,\n                             result_type='expand')\n\ndf[['add', 'multiply']]=pd.DataFrame(df.apply(lambda x: add_multiply(x['col1'], \n                                    x['col2']), axis=1).tolist())\n\n   col1  col2  add  multiply\n0     1     3    4         3\n1     2     4    6         8\n"
"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\ndf1 = pd.DataFrame(mlb.fit_transform(df['Month_List']),columns=mlb.classes_)\ndf['Binary_Month_List'] = df1.reindex(columns=ref, fill_value=0).values.tolist()\n\ndf['Binary_Month_List'] = (df['Month_List'].str.join('|')\n                                           .str.get_dummies()\n                                           .reindex(columns=ref, fill_value=0)\n                                           .values\n                                           .tolist())\nprint (df)\n            Month_List      Binary_Month_List\n0               [July]  [0, 0, 1, 0, 0, 0, 0]\n1             [August]  [0, 1, 0, 0, 0, 0, 0]\n2         [July, June]  [0, 0, 1, 1, 0, 0, 0]\n3  [May, April, March]  [0, 0, 0, 0, 1, 1, 1]\n\ndf = pd.concat([df] * 1000, ignore_index=True)\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\n\nIn [338]: %timeit (df['Month_List'].str.join('|').str.get_dummies().reindex(columns=ref, fill_value=0).values.tolist())\n31.4 ms ± 1.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\nIn [339]: %timeit pd.DataFrame(mlb.fit_transform(df['Month_List']),columns=mlb.classes_).reindex(columns=ref, fill_value=0).values.tolist()\n5.57 ms ± 94.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\nIn [340]: %timeit df['Binary_Month_List2'] =df.Month_List.explode().str.get_dummies().sum(level=0).reindex(columns=ref, fill_value=0).values.tolist()\n58.6 ms ± 461 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
'df = pd.Panel.from_dict(d).to_frame()\n\n                   col1        col2\nmajor minor                        \ndata1 row1         0.87        0.87\n      row2      15352.3     15352.3\n      row3            0           0\ndata2 row1   Title col1  Title col2\n      row2   Title col1  Title col2\n      row3   Title col1  Title col2\ndata3 row1      14.4878     24.4878\n      row2      14.9561     24.9561\n      row3      16.8293     26.8293\ndata4 row1   Title row1  Title row1\n      row2   Title row2  Title row2\n      row3   Title row3  Title row3\n\npd.concat(map(pd.DataFrame, d.itervalues()), keys=d.keys()).stack().unstack(0)\n'
"In [9]: s = Series([list('ABC'),list('DEF'),list('ABEF')])\n\nIn [10]: s\nOut[10]: \n0       [A, B, C]\n1       [D, E, F]\n2    [A, B, E, F]\ndtype: object\n\nIn [11]: s.apply(lambda x: Series(1,index=x)).fillna(0)\nOut[11]: \n   A  B  C  D  E  F\n0  1  1  1  0  0  0\n1  0  0  0  1  1  1\n2  1  1  0  0  1  1\n"
"df.drop(grouped.get_group(group_name).index)\n\ndf[grouped[0].transform(lambda x: x.name != group_name).astype('bool')]\n"
'pip install --upgrade pandas\n'
"In [1]: df = DataFrame(np.random.randn(8,3))\n\nIn [2]: store = HDFStore('test.h5')\n\nIn [3]: store['df'] = df\n\n# you can store an arbitrary python object via pickle\nIn [4]: store.get_storer('df').attrs.my_attribute = dict(A = 10)\n\nIn [5]: store.get_storer('df').attrs.my_attribute\n{'A': 10}\n"
"df.groupby('team').apply(lambda x: ','.join(x.user))\n\ndf.groupby('team').apply(lambda x: list(x.user))\n\nIn [33]: df.groupby('team').apply(lambda x: ', '.join(x.user))\nOut[33]:\nteam\na       elmer, daffy, bugs, foghorn, goofy, marvin\nb                               dawg, speedy, pepe\nc                                   petunia, porky\ndtype: object\n\nIn [34]: df.groupby('team').apply(lambda x: list(x.user))\nOut[34]:\nteam\na       [elmer, daffy, bugs, foghorn, goofy, marvin]\nb                               [dawg, speedy, pepe]\nc                                   [petunia, porky]\ndtype: object\n"
"In [179]: string = Series(np.random.choice(df.string.values, size=100), name='string')\n\nIn [180]: visits = Series(poisson(1000, size=100), name='date')\n\nIn [181]: date = Series(np.random.choice([df.date[0], now(), Timestamp('1/1/2001'), Timestamp('11/15/2001'), Timestamp('12/1/01'), Timestamp('5/1/01')], size=100), dtype='datetime64[ns]', name='date')\n\nIn [182]: df = DataFrame({'string': string, 'visits': visits, 'date': date})\n\nIn [183]: df.head()\nOut[183]:\n                 date   string  visits\n0 2001-11-15 00:00:00  current     997\n1 2001-11-15 00:00:00  current     974\n2 2012-10-02 00:00:00     stem     982\n3 2001-12-01 00:00:00     stem     984\n4 2001-01-01 00:00:00  current     989\n\nIn [186]: resamp = df.set_index('date').groupby('string').resample('M', how='sum')\n\nIn [187]: resamp.head()\nOut[187]:\n                    visits\nstring  date\ncurrent 2001-01-31    2996\n        2001-02-28     NaN\n        2001-03-31     NaN\n        2001-04-30     NaN\n        2001-05-31    3016\n\nIn [188]: g = resamp.groupby(level='date').apply(lambda x: x / x.sum())\n\nIn [189]: g.head()\nOut[189]:\n                    visits\nstring  date\ncurrent 2001-01-31   0.177\n        2001-02-28     NaN\n        2001-03-31     NaN\n        2001-04-30     NaN\n        2001-05-31   0.188\n\nIn [176]: h = g.sortlevel('date').head()\n\nIn [177]: h\nOut[177]:\n                      visits\nstring    date\ncurrent   2001-01-31   0.077\nmolecular 2001-01-31   0.228\nneuron    2001-01-31   0.073\nnucleus   2001-01-31   0.234\nstem      2001-01-31   0.388\n\nIn [178]: h.sum()\nOut[178]:\nvisits    1\ndtype: float64\n\nIn [196]: resamp.dropna()\nOut[196]:\n                      visits\nstring    date\ncurrent   2001-01-31    2996\n          2001-05-31    3016\n          2001-11-30    5959\n          2001-12-31    3998\n          2013-09-30    1077\nmolecular 2001-01-31    3984\n          2001-05-31    1911\n          2001-11-30    3054\n          2001-12-31    1020\n          2012-10-31     977\n          2013-09-30    1947\nneuron    2001-01-31    3961\n          2001-05-31    2069\n          2001-11-30    5010\n          2001-12-31    2065\n          2012-10-31    6973\n          2013-09-30     994\nnucleus   2001-01-31    3060\n          2001-05-31    3035\n          2001-11-30    2924\n          2001-12-31    4144\n          2012-10-31    2004\n          2013-09-30    7881\nstem      2001-01-31    2911\n          2001-05-31    5994\n          2001-11-30    6072\n          2001-12-31    4916\n          2012-10-31    1991\n          2013-09-30    3977\n\nIn [197]: resamp.dropna().reset_index()\nOut[197]:\n       string                date  visits\n0     current 2001-01-31 00:00:00    2996\n1     current 2001-05-31 00:00:00    3016\n2     current 2001-11-30 00:00:00    5959\n3     current 2001-12-31 00:00:00    3998\n4     current 2013-09-30 00:00:00    1077\n5   molecular 2001-01-31 00:00:00    3984\n6   molecular 2001-05-31 00:00:00    1911\n7   molecular 2001-11-30 00:00:00    3054\n8   molecular 2001-12-31 00:00:00    1020\n9   molecular 2012-10-31 00:00:00     977\n10  molecular 2013-09-30 00:00:00    1947\n11     neuron 2001-01-31 00:00:00    3961\n12     neuron 2001-05-31 00:00:00    2069\n13     neuron 2001-11-30 00:00:00    5010\n14     neuron 2001-12-31 00:00:00    2065\n15     neuron 2012-10-31 00:00:00    6973\n16     neuron 2013-09-30 00:00:00     994\n17    nucleus 2001-01-31 00:00:00    3060\n18    nucleus 2001-05-31 00:00:00    3035\n19    nucleus 2001-11-30 00:00:00    2924\n20    nucleus 2001-12-31 00:00:00    4144\n21    nucleus 2012-10-31 00:00:00    2004\n22    nucleus 2013-09-30 00:00:00    7881\n23       stem 2001-01-31 00:00:00    2911\n24       stem 2001-05-31 00:00:00    5994\n25       stem 2001-11-30 00:00:00    6072\n26       stem 2001-12-31 00:00:00    4916\n27       stem 2012-10-31 00:00:00    1991\n28       stem 2013-09-30 00:00:00    3977\n\nIn [198]: g.dropna()\nOut[198]:\n                      visits\nstring    date\ncurrent   2001-01-31   0.177\n          2001-05-31   0.188\n          2001-11-30   0.259\n          2001-12-31   0.248\n          2013-09-30   0.068\nmolecular 2001-01-31   0.236\n          2001-05-31   0.119\n          2001-11-30   0.133\n          2001-12-31   0.063\n          2012-10-31   0.082\n          2013-09-30   0.123\nneuron    2001-01-31   0.234\n          2001-05-31   0.129\n          2001-11-30   0.218\n          2001-12-31   0.128\n          2012-10-31   0.584\n          2013-09-30   0.063\nnucleus   2001-01-31   0.181\n          2001-05-31   0.189\n          2001-11-30   0.127\n          2001-12-31   0.257\n          2012-10-31   0.168\n          2013-09-30   0.496\nstem      2001-01-31   0.172\n          2001-05-31   0.374\n          2001-11-30   0.264\n          2001-12-31   0.305\n          2012-10-31   0.167\n          2013-09-30   0.251\n\nIn [199]: g.dropna().reset_index()\nOut[199]:\n       string                date  visits\n0     current 2001-01-31 00:00:00   0.177\n1     current 2001-05-31 00:00:00   0.188\n2     current 2001-11-30 00:00:00   0.259\n3     current 2001-12-31 00:00:00   0.248\n4     current 2013-09-30 00:00:00   0.068\n5   molecular 2001-01-31 00:00:00   0.236\n6   molecular 2001-05-31 00:00:00   0.119\n7   molecular 2001-11-30 00:00:00   0.133\n8   molecular 2001-12-31 00:00:00   0.063\n9   molecular 2012-10-31 00:00:00   0.082\n10  molecular 2013-09-30 00:00:00   0.123\n11     neuron 2001-01-31 00:00:00   0.234\n12     neuron 2001-05-31 00:00:00   0.129\n13     neuron 2001-11-30 00:00:00   0.218\n14     neuron 2001-12-31 00:00:00   0.128\n15     neuron 2012-10-31 00:00:00   0.584\n16     neuron 2013-09-30 00:00:00   0.063\n17    nucleus 2001-01-31 00:00:00   0.181\n18    nucleus 2001-05-31 00:00:00   0.189\n19    nucleus 2001-11-30 00:00:00   0.127\n20    nucleus 2001-12-31 00:00:00   0.257\n21    nucleus 2012-10-31 00:00:00   0.168\n22    nucleus 2013-09-30 00:00:00   0.496\n23       stem 2001-01-31 00:00:00   0.172\n24       stem 2001-05-31 00:00:00   0.374\n25       stem 2001-11-30 00:00:00   0.264\n26       stem 2001-12-31 00:00:00   0.305\n27       stem 2012-10-31 00:00:00   0.167\n28       stem 2013-09-30 00:00:00   0.251\n\nIn [210]: g.dropna().reset_index().reindex(columns=['visits', 'string', 'date'])\nOut[210]:\n    visits     string                date\n0    0.177    current 2001-01-31 00:00:00\n1    0.188    current 2001-05-31 00:00:00\n2    0.259    current 2001-11-30 00:00:00\n3    0.248    current 2001-12-31 00:00:00\n4    0.068    current 2013-09-30 00:00:00\n5    0.236  molecular 2001-01-31 00:00:00\n6    0.119  molecular 2001-05-31 00:00:00\n7    0.133  molecular 2001-11-30 00:00:00\n8    0.063  molecular 2001-12-31 00:00:00\n9    0.082  molecular 2012-10-31 00:00:00\n10   0.123  molecular 2013-09-30 00:00:00\n11   0.234     neuron 2001-01-31 00:00:00\n12   0.129     neuron 2001-05-31 00:00:00\n13   0.218     neuron 2001-11-30 00:00:00\n14   0.128     neuron 2001-12-31 00:00:00\n15   0.584     neuron 2012-10-31 00:00:00\n16   0.063     neuron 2013-09-30 00:00:00\n17   0.181    nucleus 2001-01-31 00:00:00\n18   0.189    nucleus 2001-05-31 00:00:00\n19   0.127    nucleus 2001-11-30 00:00:00\n20   0.257    nucleus 2001-12-31 00:00:00\n21   0.168    nucleus 2012-10-31 00:00:00\n22   0.496    nucleus 2013-09-30 00:00:00\n23   0.172       stem 2001-01-31 00:00:00\n24   0.374       stem 2001-05-31 00:00:00\n25   0.264       stem 2001-11-30 00:00:00\n26   0.305       stem 2001-12-31 00:00:00\n27   0.167       stem 2012-10-31 00:00:00\n28   0.251       stem 2013-09-30 00:00:00\n"
'idx = pd.date_range("2013-01-01", periods=1000)\nval = np.random.rand(1000)\ns = pd.Series(val, idx)\n\ng = s.groupby([s.index.year, s.index.month]).mean()\n\nax = g.plot()\nax.set_xticks(range(len(g)));\nax.set_xticklabels(["%s-%02d" % item for item in g.index.tolist()], rotation=90);\n'
"pd.read_csv(filename, sep=' ',header=None)\n\npd.read_csv(filename, sep='\\s+',header=None)\n"
"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn\nseaborn.set(rc={'axes.facecolor':'cornflowerblue', 'figure.facecolor':'cornflowerblue'})\n\nfig, ax = plt.subplots()\n"
"new_col_list = ['city','state','regions','country']\nfor n,col in enumerate(new_col_list):\n    df[col] = df['location'].apply(lambda location: location[n])\n\ndf = df.drop('location',axis=1)\n"
"In [4]: df\nOut[4]: \n     a    b\ni_1  1    2.0\ni_2  3    NaN\ni_3  5.9  6.0\n\nIn [5]: df.to_csv('file.csv', float_format = '%.12g')\n\n   , a,  b\ni_1, 1,  2\ni_2, 3, \ni_3, 5.9, 6\n"
"In [15]: df = pd.DataFrame({'x':['1.0692e+06']})\nIn [16]: df['x'].astype('int')\nValueError: invalid literal for long() with base 10: '1.0692e+06'\n\nimport ast\ndf = df.applymap(ast.literal_eval).astype('int')\n\ndf = pd.DataFrame({'x':['1.0692e+06']})\nfor i, item in enumerate(df['x']):\n   try:\n      int(item)\n   except ValueError:\n      print('ERROR at index {}: {!r}'.format(i, item))\n\nERROR at index 0: '1.0692e+06'\n"
"df['col2'] = df['col']\ndf.loc[df['col'] != 'pre', 'col2'] = 'nonpre'\n\ndf\nOut[7]: \n    col    col2\n0   pre     pre\n1  post  nonpre\n2     a  nonpre\n3     b  nonpre\n4  post  nonpre\n5   pre     pre\n6   pre     pre\n"
"import pandas as pd\nimport StringIO\n\nio = StringIO.StringIO()\n\n# Use a temp filename to keep pandas happy.\nwriter = pd.ExcelWriter('temp.xlsx', engine='xlsxwriter')\n\n# Set the filename/file handle in the xlsxwriter.workbook object.\nwriter.book.filename = io\n\n# Write the data frame to the StringIO object.\npd.DataFrame().to_excel(writer, sheet_name='Sheet1')\nwriter.save()\nxlsx_data = io.getvalue()\n\n# Note, Python 2 example. For Python 3 use: output = io.BytesIO().\noutput = StringIO.StringIO()\n\n# Use the StringIO object as the filehandle.\nwriter = pd.ExcelWriter(output, engine='xlsxwriter')\n"
"In [41]: df.columns.levels[0]\nOut[41]: Index([u'1', u'2'], dtype='object')\n\nIn [43]: df.columns = df.columns.set_levels(['one', 'two'], level=0)\n\nIn [44]: df\nOut[44]:\n        one                 two\n          A         B         A         B\n0  0.899686  0.466577  0.867268  0.064329\n1  0.162480  0.455039  0.736870  0.759595\n2  0.620960  0.922119  0.060141  0.669997\n3  0.871107  0.043799  0.080080  0.577421\n\nIn [45]: df.columns.levels[0]\nOut[45]: Index([u'one', u'two'], dtype='object')\n"
'DF = DF[DF.columns].astype(float)  # or int\n'
"df = pd.DataFrame({'a':np.arange(5), 'b':np.arange(5)})\ndf.to_csv(r'c:\\data\\t.csv')\n\n,a,b\n0,0,0\n1,1,1\n2,2,2\n3,3,3\n4,4,4\n\ndf.to_csv(r'c:\\data\\t.csv', index=False)\n\na,b\n0,0\n1,1\n2,2\n3,3\n4,4\n"
"In [4]:\ndf['col'] = pd.to_datetime(df['col'])\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 4 entries, 0 to 3\nData columns (total 2 columns):\ncol    4 non-null datetime64[ns]\nval    4 non-null object\ndtypes: datetime64[ns](1), object(1)\nmemory usage: 96.0+ bytes\n\nIn [5]:\ndf\n\nOut[5]:\n                  col     val\n0 2013-12-01 00:00:00  value1\n1 2014-01-22 00:00:01  value2\n2 2013-12-10 00:00:00  value3\n3 2013-12-31 00:00:00  value4\n"
"In [9]: pd.to_timedelta('30min')\nOut[9]: Timedelta('0 days 00:30:00')\n\nIn [10]: pd.to_timedelta('2S')\nOut[10]: Timedelta('0 days 00:00:02')\n\nIn [19]: from pandas.tseries.frequencies import to_offset\n\nIn [20]: pd.to_timedelta(to_offset('30T'))\nOut[20]: Timedelta('0 days 00:30:00')\n\nIn [34]: dtidx = pd.date_range('2012-01-01', periods=5, freq='30min')\n\nIn [35]: pd.to_timedelta(dtidx.freq)\nOut[35]: Timedelta('0 days 00:30:00')\n"
"In [6]: final_df = pd.DataFrame.from_csv('test.csv')\n\nIn [7]: final_df\nOut[7]: \n   AAA  BBB  CCC\n0    4   10  100\n1    5   20   50\n2    6   30  -30\n3    7   40  -50\n\n[4 rows x 3 columns]\n"
'users[users!=0]\n\nusers[users &gt; 0]\n\nIn [38]:\ns[s&gt;0]\n\nOut[38]:\n2015-08-17 18:29:59    18\n2015-08-16 18:29:59     3\n2015-08-15 18:29:59    11\n2015-08-14 18:29:59    12\n2015-08-13 18:29:59     8\n2015-08-12 18:29:59    10\n2015-08-11 18:29:59     6\n2015-08-10 18:29:59     6\n2015-08-09 18:29:59     7\n2015-08-08 18:29:59     7\n2015-08-07 18:29:59    13\n2015-08-06 18:29:59    16\n2015-08-05 18:29:59    12\n2015-08-04 18:29:59    14\n2015-08-03 18:29:59     5\n2015-08-02 18:29:59     5\n2015-08-01 18:29:59     8\n2015-07-31 18:29:59     6\n2015-07-30 18:29:59     7\n2015-07-29 18:29:59     9\n2015-07-28 18:29:59     7\n2015-07-27 18:29:59     5\n2015-07-26 18:29:59     4\n2015-07-25 18:29:59     8\n2015-07-24 18:29:59     8\n2015-07-23 18:29:59     8\n2015-07-22 18:29:59     9\n2015-07-21 18:29:59     5\n2015-07-20 18:29:59     7\n2015-07-19 18:29:59     6\nName: 1, dtype: int64\n'
"In [3]: df\nOut[3]:\n   A  B  C\n0  1  2  3\n1  3  4  5\n2  3  1  4\n\nIn [6]: df[(df &lt;= 2).any(axis=1)]\nOut[6]:\n   A  B  C\n0  1  2  3\n2  3  1  4\n\nIn [8]: df = pd.DataFrame([[1,2,3],[3,4,5],[3,1,4],[1,2,1]],columns=['A','B','C'])\n\nIn [9]: df\nOut[9]:\n   A  B  C\n0  1  2  3\n1  3  4  5\n2  3  1  4\n3  1  2  1\n\nIn [11]: df[(df &lt;= 2).all(axis=1)]\nOut[11]:\n   A  B  C\n3  1  2  1\n"
"hdf = hdf.groupby('group_id').filter(lambda group: group['criteria1'].max() != 0)\n\ndef filter_group(group):\n    if group['criteria1'].max() != 0:\n        return group\n    else:\n        return group.loc[other criteria here]\n\nhdf = hdf.groupby('group_id').apply(filter_group)\n"
'In [274]:\ndf = pd.DataFrame(np.random.randn(4,5))\ndf\n\nOut[274]:\n          0         1         2         3         4\n0  0.651863  0.738034 -0.477668 -0.561699  0.047500\n1 -1.565093 -0.671551  0.537272 -0.956520  0.301156\n2 -0.951549  2.177592  0.059961 -1.631530 -0.620173\n3  0.277796  0.169365  1.657189  0.713522  1.649386\n\nIn [276]:\ndf.apply(pd.DataFrame.describe, axis=1)\n\nOut[276]:\n   count      mean       std       min       25%       50%       75%       max\n0      5  0.079606  0.609069 -0.561699 -0.477668  0.047500  0.651863  0.738034\n1      5 -0.470947  0.878326 -1.565093 -0.956520 -0.671551  0.301156  0.537272\n2      5 -0.193140  1.458676 -1.631530 -0.951549 -0.620173  0.059961  2.177592\n3      5  0.893451  0.722917  0.169365  0.277796  0.713522  1.649386  1.657189\n'
"pivot_table = unified_df.pivot_table(index=['id', 'contact_id'],\n                                     columns='question', \n                                     values='response_answer',\n                                     aggfunc=lambda x: ' '.join(x))\n\nIn [24]: import pandas as pd\n\nIn [25]: import random\n\nIn [26]: df = pd.DataFrame({'id':[100*random.randint(10, 50) for _ in range(100)], 'question': [str(random.randint(0,3)) for _ in range(100)], 'response': [str(random.randint(100,120)) for _ in range(100)]})\n\nIn [27]: df.head()\nOut[27]:\n     id question response\n0  3100        1      116\n1  4500        2      113\n2  5000        1      120\n3  3900        2      103\n4  4300        0      117\n\nIn [28]: df.info()\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 100 entries, 0 to 99\nData columns (total 3 columns):\nid          100 non-null int64\nquestion    100 non-null object\nresponse    100 non-null object\ndtypes: int64(1), object(2)\nmemory usage: 3.1+ KB\n\nIn [29]: df.pivot_table(index='id', columns='question', values='response', aggfunc=lambda x: ' '.join(x)).head()\nOut[29]:\nquestion        0        1    2        3\nid\n1000      110 120      NaN  100      NaN\n1100          NaN  106 108  104      NaN\n1200      104 113      119  NaN      101\n1300          102      NaN  116  108 120\n1400          NaN      NaN  116      NaN\n"
'from pyspark.sql.functions import col\n\nmeans = df.groupBy("Category").mean("Values").alias("means")\ndf.alias("df").join(means, col("df.Category") == col("means.Category"))\n\ndf.alias("df").join(\n    broadcast(means), col("df.Category") == col("means.Category"))\n'
"some_date = df.iloc[1:2]['Date']  # gives 2016-01-01\n\n&gt;&gt;&gt; some_date\n1   2016-01-01\nName: Date, dtype: datetime64[ns]\n\n&gt;&gt;&gt; some_date = df.iloc[1]['Date']\n&gt;&gt;&gt; some_date\nTimestamp('2016-01-01 00:00:00')\n\n&gt;&gt;&gt; df[(df['ID']==some_id) &amp; (df['Date'] == some_date)] \n   ID       Date\n0   1 2016-01-01\n"
"logic = {'Open'  : 'first',\n         'High'  : 'max',\n         'Low'   : 'min',\n         'Close' : 'last',\n         'Volume': 'sum'}\n\noffset = pd.offsets.timedelta(days=-6)\n\nf = pd.read_clipboard(parse_dates=['Date'], index_col=['Date'])\nf.resample('W', loffset=offset).apply(logic)\n\n                 Open       High        Low      Close   Volume\nDate                                                           \n2010-01-04  38.660000  40.700001  38.509998  40.290001  5925600\n2010-01-11  40.209999  40.970001  39.279999  40.450001  6234600\n"
'&gt;&gt;&gt; df.diff()\ncount_a  count_b\n2015-01-01      NaN      NaN\n2015-01-02    38465      NaN\n2015-01-03    36714      NaN\n2015-01-04    35137      NaN\n2015-01-05    35864      NaN\n....\n2015-02-07   142390    25552\n2015-02-08   126768    22835\n2015-02-09   122324    21485\n'
'df2 = df[~(df.A &gt; 0) | ~(df.B &gt; 0)]\n'
'import pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pylab as plt\nfrom pandas.tools.plotting import andrews_curves\n\ndata = sm.datasets.get_rdataset(\'airquality\').data\nfig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1)\ndata = data[data.columns.tolist()[3:]] # use only Temp, Month, Day\n\n# Andrews\' curves\nandrews_curves(data, \'Month\', ax=ax1)\n\n# multiline plot with group by\nfor key, grp in data.groupby([\'Month\']): \n    ax2.plot(grp[\'Day\'], grp[\'Temp\'], label = "Temp in {0:02d}".format(key))\nplt.legend(loc=\'best\')    \nplt.show()\n'
'In [1]: import pandas as pd\ndf_1 = pd.DataFrame({"A":["foo", "foo", "foo", "bar"], "B":[0,1,1,1], "C":["A","A","B","A"]})\ndf_2 = pd.DataFrame({"A":["foo", "bar", "foo", "bar"], "B":[1,0,1,0], "C":["A","B","A","B"]})\n\nIn [2]: df = pd.concat([df_1, df_2])\n\nIn [3]: df\nOut[3]: \n     A  B  C\n0  foo  0  A\n1  foo  1  A\n2  foo  1  B\n3  bar  1  A\n0  foo  1  A\n1  bar  0  B\n2  foo  1  A\n3  bar  0  B\n\nIn [4]: df.drop_duplicates(keep=False)\nOut[4]: \n     A  B  C\n0  foo  0  A\n2  foo  1  B\n3  bar  1  A\n'
"df = pd.read_csv(fn, header=None, skiprows=512*n, nrows=512)\n\nfor chunk in pd.read_csv(f, sep = ' ', header = None, chunksize = 512):\n    # process your chunk here\n\nIn [61]: fn = 'd:/temp/a.csv'\n\nIn [62]: pd.DataFrame(np.random.randn(30, 3), columns=list('abc')).to_csv(fn, index=False)\n\nIn [63]: for chunk in pd.read_csv(fn, chunksize=10):\n   ....:     print(chunk)\n   ....:\n          a         b         c\n0  2.229657 -1.040086  1.295774\n1  0.358098 -1.080557 -0.396338\n2  0.731741 -0.690453  0.126648\n3 -0.009388 -1.549381  0.913128\n4 -0.256654 -0.073549 -0.171606\n5  0.849934  0.305337  2.360101\n6 -1.472184  0.641512 -1.301492\n7 -2.302152  0.417787  0.485958\n8  0.492314  0.603309  0.890524\n9 -0.730400  0.835873  1.313114\n          a         b         c\n0  1.393865 -1.115267  1.194747\n1  3.038719 -0.343875 -1.410834\n2 -1.510598  0.664154 -0.996762\n3 -0.528211  1.269363  0.506728\n4  0.043785 -0.786499 -1.073502\n5  1.096647 -1.127002  0.918172\n6 -0.792251 -0.652996 -1.000921\n7  1.582166 -0.819374  0.247077\n8 -1.022418 -0.577469  0.097406\n9 -0.274233 -0.244890 -0.352108\n          a         b         c\n0 -0.317418  0.774854 -0.203939\n1  0.205443  0.820302 -2.637387\n2  0.332696 -0.655431 -0.089120\n3 -0.884916  0.274854  1.074991\n4  0.412295 -1.561943 -0.850376\n5 -1.933529 -1.346236 -1.789500\n6  1.652446 -0.800644 -0.126594\n7  0.520916 -0.825257 -0.475727\n8 -2.261692  2.827894 -0.439698\n9 -0.424714  1.862145  1.103926\n\nIn [66]: reader = pd.read_csv(fn, iterator=True)\n\nIn [67]: reader.get_chunk(3)\nOut[67]:\n          a         b         c\n0  2.229657 -1.040086  1.295774\n1  0.358098 -1.080557 -0.396338\n2  0.731741 -0.690453  0.126648\n\nIn [68]: reader.get_chunk(5)\nOut[68]:\n          a         b         c\n0 -0.009388 -1.549381  0.913128\n1 -0.256654 -0.073549 -0.171606\n2  0.849934  0.305337  2.360101\n3 -1.472184  0.641512 -1.301492\n4 -2.302152  0.417787  0.485958\n\nIn [69]: reader.get_chunk(7)\nOut[69]:\n          a         b         c\n0  0.492314  0.603309  0.890524\n1 -0.730400  0.835873  1.313114\n2  1.393865 -1.115267  1.194747\n3  3.038719 -0.343875 -1.410834\n4 -1.510598  0.664154 -0.996762\n5 -0.528211  1.269363  0.506728\n6  0.043785 -0.786499 -1.073502\n"
"&gt;&gt;&gt; import io\n&gt;&gt;&gt; buf = io.StringIO()\n&gt;&gt;&gt; df.info(buf=buf)\n&gt;&gt;&gt; s = buf.getvalue()\n&gt;&gt;&gt; type(s)\n&lt;class 'str'&gt;\n&gt;&gt;&gt; print(s)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\na    5 non-null float64\nb    5 non-null float64\ndtypes: float64(2)\nmemory usage: 160.0 bytes\n"
'all_data = all_data.sort_index()\n'
"df['D'] = (df.apply(lambda x: myfunc(x[colNames[0]], x[colNames[1]]), axis=1)) \n"
"skipinitialspace=True\n\npd.read_csv('Sample.csv', skipinitialspace=True)\n"
"s = pd.Series([1159730, 1], index=['product_id_y','count'], name=6159402)\nprint (s)\nproduct_id_y    1159730\ncount                 1\nName: 6159402, dtype: int64\n\ndf = s.to_frame().T\nprint (df)\n         product_id_y  count\n6159402       1159730      1\n\ndf = s.rename(None).to_frame().T\nprint (df)\n   product_id_y  count\n0       1159730      1\n\ndf = pd.DataFrame([s])\nprint (df)\n         product_id_y  count\n6159402       1159730      1\n\ndf = pd.DataFrame([s.rename(None)])\nprint (df)\n   product_id_y  count\n0       1159730      1\n"
"def model(df, delta):\n    y = df[['value']].values\n    X = df[['date_delta']].values\n    return np.squeeze(LinearRegression().fit(X, y).predict(delta))\n\ndef group_predictions(df, date):\n    date = pd.to_datetime(date)\n    df.date = pd.to_datetime(df.date)\n\n    day = np.timedelta64(1, 'D')\n    mn = df.date.min()\n    df['date_delta'] = df.date.sub(mn).div(day)\n\n    dd = (date - mn) / day\n\n    return df.groupby('group').apply(model, delta=dd)\n\ngroup_predictions(df, '01-10-2016')\n\ngroup\nA    22.333333333333332\nB     3.500000000000007\nC                  16.0\ndtype: object\n\nfrom statsmodels.formula.api import ols\n\nfor k, g in df_group:\n    model = ols('value ~ date_delta', g)\n    results = model.fit()\n    print(results.summary())\n\n                        OLS Regression Results                            \n==============================================================================\nDep. Variable:                  value   R-squared:                       0.652\nModel:                            OLS   Adj. R-squared:                  0.565\nMethod:                 Least Squares   F-statistic:                     7.500\nDate:                Fri, 06 Jan 2017   Prob (F-statistic):             0.0520\nTime:                        10:48:17   Log-Likelihood:                -9.8391\nNo. Observations:                   6   AIC:                             23.68\nDf Residuals:                       4   BIC:                             23.26\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept     14.3333      1.106     12.965      0.000        11.264    17.403\ndate_delta     1.0000      0.365      2.739      0.052        -0.014     2.014\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   1.393\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.461\nSkew:                          -0.649   Prob(JB):                        0.794\nKurtosis:                       2.602   Cond. No.                         5.78\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  value   R-squared:                       0.750\nModel:                            OLS   Adj. R-squared:                  0.500\nMethod:                 Least Squares   F-statistic:                     3.000\nDate:                Fri, 06 Jan 2017   Prob (F-statistic):              0.333\nTime:                        10:48:17   Log-Likelihood:                -3.2171\nNo. Observations:                   3   AIC:                             10.43\nDf Residuals:                       1   BIC:                             8.631\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept     15.5000      1.118     13.864      0.046         1.294    29.706\ndate_delta    -1.5000      0.866     -1.732      0.333       -12.504     9.504\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   3.000\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.531\nSkew:                          -0.707   Prob(JB):                        0.767\nKurtosis:                       1.500   Cond. No.                         2.92\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  value   R-squared:                        -inf\nModel:                            OLS   Adj. R-squared:                   -inf\nMethod:                 Least Squares   F-statistic:                    -0.000\nDate:                Fri, 06 Jan 2017   Prob (F-statistic):                nan\nTime:                        10:48:17   Log-Likelihood:                 63.481\nNo. Observations:                   2   AIC:                            -123.0\nDf Residuals:                       0   BIC:                            -125.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nIntercept     16.0000        inf          0        nan           nan       nan\ndate_delta -3.553e-15        inf         -0        nan           nan       nan\n==============================================================================\nOmnibus:                          nan   Durbin-Watson:                   0.400\nProb(Omnibus):                    nan   Jarque-Bera (JB):                0.333\nSkew:                           0.000   Prob(JB):                        0.846\nKurtosis:                       1.000   Cond. No.                         2.62\n==============================================================================\n"
'df.rename(columns={&quot;col1&quot;: &quot;New name&quot;})\n'
"In [321]:\nus_zips['zipcode'].astype(str).values.tolist()\n\nOut[321]:\n['10601', '60047', '50301', '10606']\n"
"from sqlalchemy.orm import sessionmaker\nSession = sessionmaker(bind=engine)\nsession = Session()\nsession.execute('''TRUNCATE TABLE tempy''')\nsession.commit()\nsession.close()\n"
'fig, ax = plt.subplots()\ndf.plot(kind=\'scatter\',x=\'X\', y=\'Y\', c=\'C\', ax=ax)\nax.set_xlabel("X")\nplt.show()\n'
"pd.read_csv(zip_file.open('file3.txt'))\n\nfrom zipfile import ZipFile\n\nzip_file = ZipFile('textfile.zip')\ndfs = {text_file.filename: pd.read_csv(zip_file.open(text_file.filename))\n       for text_file in zip_file.infolist()\n       if text_file.filename.endswith('.csv')}\n"
'Ordinary: double_converter_nogil = xstrtod\nHigh: double_converter_nogil = precise_xstrtod\nRound-Trip: double_converter_withgil = round_trip\n'
"In [11]: s = pd.Series([0, 1, np.nan, np.nan, np.nan, np.nan, 3])\n\nIn [12]: s.interpolate(method='nearest')\nOut[12]: \n0    0.0\n1    1.0\n2    1.0\n3    1.0\n4    3.0\n5    3.0\n6    3.0\ndtype: float64\n\nIn [13]: s = pd.Series([0, 1, np.nan, np.nan, 2, np.nan, np.nan, 3])\n\nIn [14]: s.interpolate(method='nearest')\nOut[14]: \n0    0.0\n1    1.0\n2    1.0\n3    2.0\n4    2.0\n5    2.0\n6    3.0\n7    3.0\ndtype: float64\n"
'df[np.array([0,1,0,0,1,1,0,0,0,1],dtype=bool)]\n\ndf2 = df[np.array([0,1,0,0,1,1,0,0,0,1],dtype=bool)]\n'
"(df.index == 'entry').any()\n\n(df.index == 'entry').all()\n\n'entry' in df.index\n\ndf.index.str.contains('en').any()\n\ndf = pd.DataFrame({'Apr 2013':[1,2,3]}, index=['entry','pdf','sum'])\nprint(df)\n       Apr 2013\nentry         1\npdf           2\nsum           3\n\nprint (df.index == 'entry')\n[ True False False]\n\nprint ((df.index == 'entry').any())\nTrue\nprint ((df.index == 'entry').all())\nFalse\n\n#check columns values\nprint ('entry' in df)\nFalse\n#same as explicitely call columns (better readability)\nprint ('entry' in df.columns)\nFalse\n#check index values\nprint ('entry' in df.index)\nTrue\n#check columns values\nprint ('Apr 2013' in df)\nTrue\n#check columns values\nprint ('Apr 2013' in df.columns)\nTrue\n\ndf = pd.DataFrame({'Apr 2013':[1,2,3]}, index=['entry','entry','entry'])\nprint(df)\n       Apr 2013\nentry         1\nentry         2\nentry         3\n\nprint (df.index == 'entry')\n[ True  True  True]\n\nprint ((df.index == 'entry').any())\nTrue\nprint ((df.index == 'entry').all())\nTrue\n"
"def read_clipboard_mi(index_names_row=None, **kwargs):\n    encoding = kwargs.pop('encoding', 'utf-8')\n\n    # only utf-8 is valid for passed value because that's what clipboard\n    # supports\n    if encoding is not None and encoding.lower().replace('-', '') != 'utf8':\n        raise NotImplementedError(\n            'reading from clipboard only supports utf-8 encoding')\n\n    from pandas import compat, read_fwf\n    from pandas.io.clipboard import clipboard_get\n    from pandas.io.common import StringIO\n    data = clipboard_get()\n\n    # try to decode (if needed on PY3)\n    # Strange. linux py33 doesn't complain, win py33 does\n    if compat.PY3:\n        try:\n            text = compat.bytes_to_str(\n                text, encoding=(kwargs.get('encoding') or\n                                get_option('display.encoding'))\n            )\n        except:\n            pass\n\n    index_names = None\n    if index_names_row:\n        if isinstance(index_names_row, int):\n            index_names = data.splitlines()[index_names_row].split()\n            skiprows = [index_names_row]\n            kwargs.update({'skiprows': skiprows})\n        else:\n            raise Exception('[index_names_row] must be of [int] data type')\n\n    df = read_fwf(StringIO(data), **kwargs)\n    unnamed_cols = df.columns[df.columns.str.contains(r'Unnamed:')].tolist()\n\n    if index_names:\n        idx_cols = df.columns[range(len(index_names))].tolist()\n    elif unnamed_cols:\n        idx_cols = df.columns[range(len(unnamed_cols))].tolist()\n        index_names = [None] * len(idx_cols)\n\n    df[idx_cols] = df[idx_cols].ffill()\n    df = df.set_index(idx_cols).rename_axis(index_names)\n\n    return df\n\nIn [231]: read_clipboard_mi()\nOut[231]:\n          C\n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74\n\nIn [232]: read_clipboard_mi(index_names_row=1)\nOut[232]:\n          C\nA   B\n1.1 111  20\n    222  31\n3.3 222  24\n    333  65\n5.5 333  22\n6.6 777  74\n"
'cum_r = (1 + r).cumprod()\nresult = cum_r * v0\nfor date in r.index[r.index.is_quarter_end]:\n     result[date:] -= cum_r[date:] * (dist / cum_r.loc[date])\n\ncum_r = (1 + r).cumprod()\nt = (r.index.is_quarter_end / cum_r).cumsum()\nresult = cum_r * (v0 - dist * t)\n'
"df1 = pd.DataFrame({'A':[3,2,1,4], 'B':[7,8,9,5]})\ndf2 = pd.DataFrame({'A':[1,5,6,4], 'B':[4,1,8,5]})\n\ndf1         df2\n   A  B        A  B\n0  1  7     0  1  4\n1  2  8     1  5  1\n2  3  9     2  6  8\n3  4  5     3  4  5\n\nsuff_A = ['_on_A_match_1', '_on_A_match_2']\nsuff_B = ['_on_B_match_1', '_on_B_match_2']\n\npd.concat([df1.merge(df2, on='A', suffixes=suff_A), \n           df1.merge(df2, on='B', suffixes=suff_B)])\n\n     A  A_on_B_match_1  A_on_B_match_2    B  B_on_A_match_1  B_on_A_match_2\n0  1.0             NaN             NaN  NaN             9.0             4.0\n1  4.0             NaN             NaN  NaN             5.0             5.0\n0  NaN             2.0             6.0  8.0             NaN             NaN\n1  NaN             4.0             4.0  5.0             NaN             NaN\n\ndupes = (df.B_on_A_match_1 == df.B_on_A_match_2) # also could remove A_on_B_match\ndf.loc[~dupes]\n\n     A  A_on_B_match_1  A_on_B_match_2    B  B_on_A_match_1  B_on_A_match_2\n0  1.0             NaN             NaN  NaN             9.0             4.0\n0  NaN             2.0             6.0  8.0             NaN             NaN\n1  NaN             4.0             4.0  5.0             NaN             NaN\n"
'In[278]:\ns = s - pd.datetime.now()\ns\n\nOut[278]: \n0   -1 days +23:59:46.389639\n1   -1 days +23:59:46.389639\n2   -1 days +23:59:46.389639\n3   -1 days +23:59:46.389639\n4   -1 days +23:59:46.389639\n5   -1 days +23:59:46.389639\n6   -1 days +23:59:46.389639\n7   -1 days +23:59:46.389639\n8   -1 days +23:59:46.389639\n9   -1 days +23:59:46.389639\ndtype: timedelta64[ns]\n\nIn[279]:\ns.dt.total_seconds()\n\nOut[279]: \n0   -13.610361\n1   -13.610361\n2   -13.610361\n3   -13.610361\n4   -13.610361\n5   -13.610361\n6   -13.610361\n7   -13.610361\n8   -13.610361\n9   -13.610361\ndtype: float64\n'
"df['flag'] = df.value.groupby([df.id, df.value.diff().ne(0).cumsum()]).transform('size').ge(3).astype(int) \ndf\n\ndf.value.diff().ne(0)\n#0      True\n#1     False\n#2      True\n#3      True\n#4     False\n#5     False\n#6      True\n#7     False\n#8     False\n#9     False\n#10     True\n#11     True\n#12     True\n#13    False\n#14    False\n#15     True\n#16    False\n#17     True\n#18    False\n#19    False\n#20    False\n#21    False\n#Name: value, dtype: bool\n\ndf.value.diff().ne(0).cumsum()\n#0     1\n#1     1\n#2     2\n#3     3\n#4     3\n#5     3\n#6     4\n#7     4\n#8     4\n#9     4\n#10    5\n#11    6\n#12    7\n#13    7\n#14    7\n#15    8\n#16    8\n#17    9\n#18    9\n#19    9\n#20    9\n#21    9\n#Name: value, dtype: int64\n"
'pd.DataFrame.from_dict(dict(zip(s.index, s.values)))\n\npd.DataFrame.from_items(zip(s.index, s.values))\n\n   0  1\n0  1  4\n1  2  5\n2  3  6\n\npd.DataFrame.from_items(zip(s.index, s.values)).T\n\n   0  1  2\n0  1  2  3\n1  4  5  6\n\n%timeit pd.DataFrame.from_items(zip(s.index, s.values))\n1000 loops, best of 3: 669 µs per loop\n\n%timeit s.apply(lambda x:pd.Series(x)).T\n1000 loops, best of 3: 1.37 ms per loop\n\n%timeit pd.DataFrame.from_items(zip(s.index, s.values)).T\n1000 loops, best of 3: 919 µs per loop\n\n%timeit s.apply(lambda x:pd.Series(x))\n1000 loops, best of 3: 1.26 ms per loop\n\n%timeit pd.DataFrame(item for item in s)\n1000 loops, best of 3: 636 µs per loop\n\n%timeit pd.DataFrame(item for item in s).T\n1000 loops, best of 3: 884 µs per loop\n\n%timeit pd.DataFrame.from_records(izip_longest(*s.values))\n1000 loops, best of 3: 529 µs per loop\n\npd.DataFrame(dict(zip(s.index, s.values)))\n\n   0  1\n0  1  4\n1  2  5\n2  3  6\n'
'print(df.loc[[df.index.get_level_values(0)[-1]]])\n                    0        1        2        3\nfirst second                                    \nqux   one    -1.25388 -0.63775  0.90711 -1.42868\n      two    -0.14007 -0.86175 -0.25562 -2.79859\n'
"In [63]: df.groupby(['Parent Name']).apply(lambda x: x[x['Type'] == 'object_a']['Ticks'].sum())\nOut[63]: \nParent Name\n3217863     2\n4556421    34\ndtype: int64\n\nIn [64]: sumATicks = df.groupby(['Parent Name']).apply(lambda x: x[x['Type'] == 'object_a']['Ticks'].sum())\n\nIn [65]: merged = df.merge(pd.DataFrame(sumATicks).rename(columns={0: 'nbATicks'}), left_on='Parent Name', right_index=True)\n\nIn [66]: merged\nOut[66]: \n       Type  Parent Name  Ticks  nbATicks\n0  object_a      4556421     34        34\n1  object_a      4556421      0        34\n2  object_b      4556421      0        34\n3  object_a      3217863      2         2\n4  object_b      3217863      1         2\n\nIn [67]: merged[(merged['nbATicks'] &gt; 0) &amp; (merged['Ticks'] == 0)]\nOut[67]: \n       Type  Parent Name  Ticks  nbATicks\n1  object_a      4556421      0        34\n2  object_b      4556421      0        34\n"
'In [218]: df = df.loc[:, df.columns.notnull()]\n\nIn [219]: df\nOut[219]:\n      x   y\n0  this NaN\n1  that NaN\n2  this NaN\n3  that NaN\n4  this NaN\n'
'#convert first or second to str or int\nMergeDat[\'Motor\'] = MergeDat[\'Motor\'].astype(str)\n#Motor[\'Motor\'] = Motor[\'Motor\'].astype(str)\n\n#MergeDat[\'Motor\'] = MergeDat[\'Motor\'].astype(int)\nMotor[\'Motor\'] = Motor[\'Motor\'].astype(int)\n\n#convert first or second to str or int\n#MergeDat[\'Motor\'] = MergeDat[\'Motor\'].astype(str)\nMotor[\'Motor\'] = Motor[\'Motor\'].astype(str)\n\nMergeDat[\'Motor\'] = MergeDat[\'Motor\'].astype(int)\n#Motor[\'Motor\'] = Motor[\'Motor\'].astype(int)\n\n\nMergeDat=MergeDat.merge(Motor,how="left")\n'
'df = pd.DataFrame([["dec", 12], ["jan", 40], ["mar", 11], ["aug", 21],\n                  ["aug", 11], ["jan", 11], ["jan", 1]], \n                   columns=["Month", "Price"])\n# Preprocessing: capitalize `jan`, `dec` to `Jan` and `Dec`\ndf["Month"] = df["Month"].str.capitalize()\n\n# Now the dataset should look like\n#   Month Price\n#   -----------\n#    Dec    XX\n#    Jan    XX\n#    Apr    XX\n\n# make it a datetime so that we can sort it: \n# use %b because the data use the abbriviation of month\ndf["Month"] = pd.to_datetime(df.Month, format=\'%b\', errors=\'coerce\').dt.month\ndf = df.sort_values(by="Month")\n\ntotal = (df.groupby(df[\'Month"])[\'Price\'].mean())\n\n# total \nMonth\n1     17.333333\n3     11.000000\n8     16.000000\n12    12.000000\n'
"df.plot('a', 'b', kind='scatter', color=df['c'], colormap='YlOrRd', sharex=False)\n"
"pd.DataFrame(\n{\n     'col1' : list(''.join(df.col1)), \n     'col2' : df.col2.values.repeat(df.col1.str.len(), axis=0)\n})\n\n  col1  col2\n0    a     1\n1    s     1\n2    d     1\n3    f     1\n4    x     2\n5    y     2\n6    q     3\n\ni = list(''.join(df.col1))\nj = df.drop('col1', 1).values.repeat(df.col1.str.len(), axis=0)\n\ndf = pd.DataFrame(j, columns=df.columns.difference(['col1']))\ndf.insert(0, 'col1', i)\n\ndf\n\n  col1 col2\n0    a    1\n1    s    1\n2    d    1\n3    f    1\n4    x    2\n5    y    2\n6    q    3\n\ndf = pd.concat([df] * 100000, ignore_index=True)\n\n# MaxU's solution\n\n%%timeit\ndf.col1.str.extractall(r'(.)') \\\n           .reset_index(level=1, drop=True) \\\n           .join(df['col2']) \\\n           .reset_index(drop=True)\n\n1 loop, best of 3: 1.98 s per loop\n\n# piRSquared's solution\n\n%%timeit\npd.DataFrame(\n     [[x] + b for a, *b in df.values for x in a],\n     columns=df.columns\n)\n\n1 loop, best of 3: 1.68 s per loop\n\n# Wen's solution\n\n%%timeit\nv = df.col1.apply(list)\npd.DataFrame({'col1':np.concatenate(v.values),'col2':df.col2.repeat(v.apply(len))})\n\n1 loop, best of 3: 835 ms per loop\n\n# Alexander's solution\n\n%%timeit\npd.DataFrame([(letter, i) \n              for letters, i in zip(df['col1'], df['col2']) \n              for letter in letters],\n             columns=df.columns)\n\n1 loop, best of 3: 316 ms per loop\n\n%%timeit\npd.DataFrame(\n{\n     'col1' : list(''.join(df.col1)), \n     'col2' : df.col2.values.repeat(df.col1.str.len(), axis=0)\n})\n\n10 loops, best of 3: 124 ms per loop\n"
"df[['nnn', 'mmm', 'yyy']]\n\n   nnn  mmm  yyy\n0    5    5   10\n1    3    4    9\n2    7    0    8\n\ndf.loc[:, df.columns.isin(['nnn', 'mmm', 'yyy', 'zzzzzz'])]\n\n   yyy  nnn  mmm\n0   10    5    5\n1    9    3    4\n2    8    7    0\n"
"df['count'] = df.groupby(['col1', 'col2'])['col3'].transform('count')\n"
"In [24]: df = pd.DataFrame(data)\n\nIn [25]: df.groupby(df.columns[:-1].tolist(), as_index=False).agg(lambda x: x.astype(int).sum()).values.tolist()\nOut[25]: [['Andrew', '1', 17], ['Andrew', '2', 2], ['Peter', '1', 21], ['Sam', '4', 9]]\n"
"df.groupby(['rule_id', 'ordering'])['sequence_id'].apply(list).groupby(level=0).apply(list)\n"
'df2 = df1[\'company_name\'].str.contains("apple", na=False, case=False)\n'
"df.groupby(['a', 'b'])['type'].transform(some_func)\n\n0    1\n1    2\n2    3\ndtype: int64\n\n[1,2,3]\n\ndf.groupby(['a', 'b'])['type'].transform(lambda k: 50)\n"
"c = language.lang.astype('category')\n\nd = dict(enumerate(c.cat.categories))\nprint (d)\n{0: 'english', 1: 'spanish'}\n\nlanguage['code'] = language.lang.astype('category').cat.codes\n\nlanguage['level_back'] = language['code'].map(d)\nprint (language)\n      lang         level  code level_back\n0  english  intermediate     0    english\n1  spanish  intermediate     1    spanish\n2  spanish         basic     1    spanish\n3  english         basic     0    english\n4  english      advanced     0    english\n5  spanish  intermediate     1    spanish\n6  spanish         basic     1    spanish\n7  spanish      advanced     1    spanish\n"
"df.loc[df.CAR == 'BMW', 'DATE'].value_counts().reindex(\n    df.DATE.unique(), fill_value=0)\n\n2012/01/01    2\n2012/01/02    1\n2012/01/03    0\n2012/09/01    1\n2012/09/02    0\nName: DATE, dtype: int64\n\ndf['CAR'].eq('BMW').astype(int).groupby(df['DATE']).sum()\n\nDATE\n2012/01/01    2\n2012/01/02    1\n2012/01/03    0\n2012/09/01    1\n2012/09/02    0\nName: CAR, dtype: int32\n"
"In [86]: from functools import reduce\n\nIn [87]: reduce(lambda x,y: pd.merge(x,y, on='Col1', how='outer'), [df1, df2, df3])\nOut[87]:\n    Col1  Col2  Col3  Col4  Col5  Col6  Col7\n0  data1     3     4   7.0   4.0   NaN   NaN\n1  data2     4     3   6.0   9.0   5.0   8.0\n2  data3     2     3   1.0   4.0   2.0   7.0\n3  data4     2     4   NaN   NaN   NaN   NaN\n4  data5     1     4   NaN   NaN   5.0   3.0\n\nIn [88]: df1\nOut[88]:\n    Col1  Col2  Col3\n0  data1     3     4\n1  data2     4     3\n2  data3     2     3\n3  data4     2     4\n4  data5     1     4\n\nIn [89]: df2\nOut[89]:\n    Col1  Col4  Col5\n0  data1     7     4\n1  data2     6     9\n2  data3     1     4\n\nIn [90]: df3\nOut[90]:\n    Col1  Col6  Col7\n0  data2     5     8\n1  data3     2     7\n2  data5     5     3\n"
"df.sort_values('DATE_CHANGED').drop_duplicates('STATION_ID',keep='last')\n"
"cols = list(df.columns)\na, b = cols.index('LastName'), cols.index('MiddleName')\ncols[b], cols[a] = cols[a], cols[b]\ndf = df[cols]\n\ncols = list(df.columns)\na, b, c, d = cols.index('LastName'), cols.index('MiddleName'), cols.index('Contact'), cols.index('EmployeeID')\ncols[a], cols[b], cols[c], cols[d] = cols[b], cols[a], cols[d], cols[c]\ndf = df[cols]\n\ncols = list(df.columns)\ncols = cols[1::2] + cols[::2]\ndf = df[cols]\n"
"B.reindex( pd.MultiIndex.from_product([B.index.levels[0], \nA.index], names=['Bank', 'Curency']),fill_value=0)\n\nOut[62]: \n                Notional\nBank   Curency          \nBank_1 AUD            16\n       BRL             0\n       CAD            13\n       EUR            22\n       INR             0\nBank_2 AUD            24\n       BRL             0\n       CAD            20\n       EUR            17\n       INR             0\n\npd.concat([A]*2,keys=B.index.levels[0])\nOut[69]: \n            AUD  BRL  CAD  EUR  INR\nBank                               \nBank_1 AUD   10    5   10   14    1\n       BRL   17    1   14   10    8\n       CAD    3    7    3   15    2\n       EUR   17    1   15    2   16\n       INR    7   15    6    7    4\nBank_2 AUD   10    5   10   14    1\n       BRL   17    1   14   10    8\n       CAD    3    7    3   15    2\n       EUR   17    1   15    2   16\n       INR    7   15    6    7    4\n"
'df[pd.DataFrame(df.species.tolist()).isin(selection).any(1).values]\nOut[64]: \n  molecule            species\n0        a              [dog]\n2        c         [cat, dog]\n3        d  [cat, horse, pig]\n'
"import pandas as pd\ndf = pd.DataFrame({'time': [pd.to_datetime('2019-01-15 13:25:43')]})\ndf_unix_sec = pd.to_datetime(df['time']).astype(int)/ 10**9\nprint(df_unix_sec)\n"
"df.loc[df.groupby('A').B.idxmin()]\n\n   A  B   C\n2  1  2  10\n4  2  4   4\n\ndf.loc[df.groupby('A').B.idxmin()].reset_index(drop=True)\n\n   A  B   C\n0  1  2  10\n1  2  4   4\n"
'import numpy as np\n\nnp.log10(df)\n'
'import pandas as pd\nimport numpy as np\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nnodes = [\n    [\'ID\', \'Label\', \'Color\'],\n    [0,\'Remain+No – 28\',\'#F27420\'],\n    [1,\'Leave+No – 16\',\'#4994CE\'],\n    [2,\'Remain+Yes – 21\',\'#FABC13\'],\n    [3,\'Leave+Yes – 14\',\'#7FC241\'],\n    [4,\'Didn’t vote in at least one referendum – 21\',\'#D3D3D3\'],\n    [5,\'46 – No\',\'#8A5988\']\n]\nlinks = [\n    [\'Source\',\'Target\',\'Value\',\'Link Color\'],\n    [0,3,20,\'rgba(253, 227, 212, 0.5)\'],\n    [0,4,3,\'rgba(242, 116, 32, 1)\'],\n    [0,2,5,\'rgba(253, 227, 212, 0.5)\'],\n    [1,5,14,\'rgba(219, 233, 246, 0.5)\'],\n    [1,3,1,\'rgba(73, 148, 206, 1)\'],\n    [1,4,1,\'rgba(219, 233, 246,0.5)\'],\n    [1,2,10,\'rgba(8, 233, 246,0.5)\'],\n    [1,3,5,\'rgba(219, 77, 246,0.5)\'],\n    [1,5,12,\'rgba(219, 4, 246,0.5)\']\n]\n\nnodes_headers = nodes.pop(0)\nnodes_df = pd.DataFrame(nodes, columns = nodes_headers)\nlinks_headers = links.pop(0)\nlinks_df = pd.DataFrame(links, columns = links_headers)\n\ndata_trace = dict(\n    type=\'sankey\',\n    domain = dict(\n      x =  [0,1],\n      y =  [0,1]\n    ),\n    orientation = "h",\n    valueformat = ".0f",\n    node = dict(\n      pad = 10,\n      thickness = 30,\n      line = dict(\n        color = "black",\n        width = 0\n      ),\n      label =  nodes_df[\'Label\'].dropna(axis=0, how=\'any\'),\n      color = nodes_df[\'Color\']\n    ),\n    link = dict(\n      source = links_df[\'Source\'].dropna(axis=0, how=\'any\'),\n      target = links_df[\'Target\'].dropna(axis=0, how=\'any\'),\n      value = links_df[\'Value\'].dropna(axis=0, how=\'any\'),\n      color = links_df[\'Link Color\'].dropna(axis=0, how=\'any\'),\n  )\n)\n\nlayout =  dict(\n    title = "Scottish Referendum Voters who now want Independence",\n    height = 772,\n    font = dict(\n      size = 10\n    ),    \n)\n\nfig = dict(data=[data_trace], layout=layout)\niplot(fig, validate=False)\n\nnodes = [\n    [\'ID\', \'Label\', \'Color\'],\n    [0,\'Remain+No – 28\',\'#F27420\'],\n    [1,\'Leave+No – 16\',\'#4994CE\'],\n    [2,\'Remain+Yes – 21\',\'#FABC13\'],\n    [3,\'Leave+Yes – 14\',\'#7FC241\'],\n    [4,\'Didn’t vote in at least one referendum – 21\',\'#D3D3D3\'],\n    [5,\'46 – No\',\'#8A5988\'],\n    [6,\'WAKA1\',\'#8A5988\'],\n    [7,\'WAKA2\',\'#8A5988\'],\n    [8,\'WAKA3\',\'#8A5988\'],\n    [9,\'WAKA4\',\'#8A5988\'],\n    [10,\'WAKA5\',\'#8A5988\'],\n    [11,\'WAKA6\',\'#8A5988\'],\n\n]\nlinks = [\n    [\'Source\',\'Target\',\'Value\',\'Link Color\'],\n    [0,3,20,\'rgba(253, 227, 212, 0.5)\'],\n    [0,4,3,\'rgba(242, 116, 32, 1)\'],\n    [0,2,5,\'rgba(253, 227, 212, 0.5)\'],\n    [1,5,14,\'rgba(219, 233, 246, 0.5)\'],\n    [1,3,1,\'rgba(73, 148, 206, 1)\'],\n    [1,4,1,\'rgba(219, 233, 246,0.5)\'],\n    [1,2,10,\'rgba(8, 233, 246,0.5)\'],\n    [1,3,5,\'rgba(219, 77, 246,0.5)\'],\n    [1,5,12,\'rgba(219, 4, 246,0.5)\']\n]\n\nnodes = [\n    [\'ID\', \'Label\', \'Color\'],\n]\nlinks = [\n    [\'Source\',\'Target\',\'Value\',\'Link Color\'],\n    [0,3,20,\'rgba(253, 227, 212, 0.5)\'],\n    [0,4,3,\'rgba(242, 116, 32, 1)\'],\n    [0,2,5,\'rgba(253, 227, 212, 0.5)\'],\n    [1,5,14,\'rgba(219, 233, 246, 0.5)\'],\n    [1,3,1,\'rgba(73, 148, 206, 1)\'],\n    [1,4,1,\'rgba(219, 233, 246,0.5)\'],\n    [1,2,10,\'rgba(8, 233, 246,0.5)\'],\n    [1,3,5,\'rgba(219, 77, 246,0.5)\'],\n    [1,5,12,\'rgba(219, 4, 246,0.5)\']\n]\n\n    [1,100500,10,\'rgba(219, 233, 246,0.5)\'],\n    [1,100501,10,\'rgba(8, 233, 246,0.5)\'],\n    [1,100502,10,\'rgba(219, 77, 246,0.5)\'],\n    [1,100503,10,\'rgba(219, 4, 246,0.5)\']\n\nnodes = [\n    [\'ID\', \'Label\', \'Color\'],\n    [0,\'WAKA WANNA BE SOURCE\',\'#F27420\'],\n    [1,\'WAKA WANNA BE TARGET\',\'#4994CE\'],\n    [2,\'WAKA DON\\\'T KNOW WHO WANNA BE\',\'#FABC13\'],\n\n]\nlinks = [\n    [\'Source\',\'Target\',\'Value\',\'Link Color\'],\n    [0,1,10,\'rgba(253, 227, 212, 1)\'],\n    [0,2,10,\'rgba(242, 116, 32, 1)\'],\n    [2,1,10,\'rgba(253, 227, 212, 1)\'],\n]\n'
'import re\nimport pandas as pd\n\nSEP_RE = re.compile(r":\\s+")\nDATA_RE = re.compile(r"(?P&lt;term&gt;[a-z]+)\\s+(?P&lt;weight&gt;\\d+\\.\\d+)", re.I)\n\n\ndef parse(filepath: str):\n    def _parse(filepath):\n        with open(filepath) as f:\n            for line in f:\n                id, rest = SEP_RE.split(line, maxsplit=1)\n                for match in DATA_RE.finditer(rest):\n                    yield [int(id), match["term"], float(match["weight"])]\n    return list(_parse(filepath))\n\n&gt;&gt;&gt; df = pd.DataFrame(parse("/Users/bradsolomon/Downloads/doc.txt"),\n...                   columns=["Id", "Term", "weight"])\n&gt;&gt;&gt; \n&gt;&gt;&gt; df\n   Id     Term  weight\n0   1    frack   0.733\n1   1    shale   0.700\n2  10    space   0.645\n3  10  station   0.327\n4  10     nasa   0.258\n5   4   celebr   0.262\n6   4    bahar   0.345\n\n&gt;&gt;&gt; df.dtypes\nId          int64\nTerm       object\nweight    float64\ndtype: object\n\n&gt;&gt;&gt; line = "1: frack 0.733, shale 0.700,\\n"\n&gt;&gt;&gt; SEP_RE.split(line, maxsplit=1)\n[\'1\', \'frack 0.733, shale 0.700,\\n\']\n\n&gt;&gt;&gt; id, rest = SEP_RE.split(line, maxsplit=1)\n&gt;&gt;&gt; it = DATA_RE.finditer(rest)\n&gt;&gt;&gt; match = next(it)\n&gt;&gt;&gt; match\n&lt;re.Match object; span=(0, 11), match=\'frack 0.733\'&gt;\n&gt;&gt;&gt; match["term"]\n\'frack\'\n&gt;&gt;&gt; match["weight"]\n\'0.733\'\n'
"s = event_data.loc[event_data.event_id == event_id, 'max_total_gross']\n\nout = next(iter(s), 'no match')\nprint (out)\n"
'df.value_counts()\n\ncol_1  col_2  col_3\n1      1      A        2\n       0      C        1\n              B        1\n              A        1\n0      1      A        1\n\ndf.value_counts().head(1).index.to_frame(index=False)\n\n   col_1  col_2 col_3\n0      1      1     A\n'
"s = df.groupby('ID')['value'].apply(lambda x : x.ne(x.shift()).cumsum())\nd = {x : y for x ,y in df.groupby(s)}\nd[2]\n     ID  value\n2   100   True\n5   200   True\n6   200   True\n7   200   True\n11  300   True\n12  300   True\nd[1]\n     ID  value\n0   100  False\n1   100  False\n4   200  False\n9   300  False\n10  300  False\nd[3]\n     ID  value\n3   100  False\n8   200  False\n13  300  False\n"
"np.round(dtindex_or_datetime_col.astype(np.int64), -9).astype('datetime64[ns]')\n\nfrom pandas.lib import Timestamp\n\nt1 = Timestamp('2012-1-1 00:00:00')\nt2 = Timestamp('2012-1-1 00:00:00.000333')\n\nIn [4]: t1\nOut[4]: &lt;Timestamp: 2012-01-01 00:00:00&gt;\n\nIn [5]: t2\nOut[5]: &lt;Timestamp: 2012-01-01 00:00:00.000333&gt;\n\nIn [6]: t2.microsecond\nOut[6]: 333\n\nIn [7]: t1.value\nOut[7]: 1325376000000000000L\n\nIn [8]: t2.value\nOut[8]: 1325376000000333000L\n\n# Alternatively: t2.value - t2.value % 1000000000\nIn [9]: long(round(t2.value, -9)) # round milli-, micro- and nano-seconds\nOut[9]: 1325376000000000000L\n\nIn [10]: Timestamp(long(round(t2.value, -9)))\nOut[10]: &lt;Timestamp: 2012-01-01 00:00:00&gt;\n\ndef to_the_second(ts):\n    return Timestamp(long(round(ts.value, -9)))\n\ndtindex.map(to_the_second)\n"
"df['date'].apply(lambda x: x.toordinal())\n"
"In [40]: fills\nOut[40]: \n    Price  Symbol\n0  109.70  BUD US\n1  109.72  BUD US\n2  183.30  IBM US\n3  183.35  IBM US\n\nIn [41]: my_symbol = ['BUD US']\n\nIn [42]: fills.query('Symbol==@my_symbol')\nOut[42]: \n    Price  Symbol\n0  109.70  BUD US\n1  109.72  BUD US\n"
'import matplotlib.pyplot as plt\n\ny = range(20)\nx1 = range(20)\nx2 = range(0, 200, 10)\n\nfig, axes = plt.subplots(ncols=2, sharey=True)\naxes[0].barh(y, x1, align=\'center\', color=\'gray\')\naxes[1].barh(y, x2, align=\'center\', color=\'gray\')\naxes[0].invert_xaxis()\nplt.show()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Data\nstates = ["AK", "TX", "CA", "MT", "NM", "AZ", "NV", "CO", "OR", "WY", "MI",\n          "MN", "UT", "ID", "KS", "NE", "SD", "WA", "ND", "OK"]\nstaff = np.array([20, 30, 40, 10, 15, 35, 18, 25, 22, 7, 12, 22, 3, 4, 5, 8,\n                  14, 28, 24, 32])\nsales = staff * (20 + 10 * np.random.random(staff.size))\n\n# Sort by number of sales staff\nidx = staff.argsort()\nstates, staff, sales = [np.take(x, idx) for x in [states, staff, sales]]\n\ny = np.arange(sales.size)\n\nfig, axes = plt.subplots(ncols=2, sharey=True)\naxes[0].barh(y, staff, align=\'center\', color=\'gray\', zorder=10)\naxes[0].set(title=\'Number of sales staff\')\naxes[1].barh(y, sales, align=\'center\', color=\'gray\', zorder=10)\naxes[1].set(title=\'Sales (x $1000)\')\n\naxes[0].invert_xaxis()\naxes[0].set(yticks=y, yticklabels=states)\naxes[0].yaxis.tick_right()\n\nfor ax in axes.flat:\n    ax.margins(0.03)\n    ax.grid(True)\n\nfig.tight_layout()\nfig.subplots_adjust(wspace=0.09)\nplt.show()\n\naxes[0].set(yticks=y, yticklabels=[])\nfor yloc, state in zip(y, states):\n    axes[0].annotate(state, (0.5, yloc), xycoords=(\'figure fraction\', \'data\'),\n                     ha=\'center\', va=\'center\')\n'
"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\nenc.fit(train)\n\nenc.transform(train).toarray()\n\nfrom sklearn import preprocessing\nimport numpy as np\nimport pandas as pd\n\ntrain = {'city': ['Buenos Aires', 'New York', 'Istambul', 'Buenos Aires', 'Paris', 'Paris'],\n        'letters': ['a', 'b', 'c', 'd', 'a', 'b']}\ntrain = pd.DataFrame(train)\n\ntest = {'city': ['Buenos Aires', 'New York', 'Istambul', 'Buenos Aires', 'Paris', 'Utila'],\n        'letters': ['a', 'b', 'c', 'a', 'b', 'b']}\ntest = pd.DataFrame(test)\n\nc = 'city'\nle = preprocessing.LabelEncoder()\ntrain[c] = le.fit_transform(train[c])\ntest[c] = test[c].map(lambda s: 'other' if s not in le.classes_ else s)\nle_classes = le.classes_.tolist()\nbisect.insort_left(le_classes, 'other')\nle.classes_ = le_classes\ntest[c] = le.transform(test[c])\ntest\n\n  city  letters\n0   1   a\n1   3   b\n2   2   c\n3   1   a\n4   4   b\n5   0   b\n"
"mux = pd.MultiIndex.from_product([list('ab'), [2014, 2015], range(1, 3)])\n\ndf = pd.DataFrame(dict(A=1), mux)\n\nprint(df)\n\n          A\na 2014 1  1\n       2  1\n  2015 1  1\n       2  1\nb 2014 1  1\n       2  1\n  2015 1  1\n       2  1\n\ndf.index.get_level_values(0)\n\ndf.index.map('{0[2]}/{0[1]}'.format)\n\ndf.index = [df.index.get_level_values(0), df.index.map('{0[2]}/{0[1]}'.format)]\n\nprint(df)\n\n          A\na 1/2014  1\n  2/2014  1\n  1/2015  1\n  2/2015  1\nb 1/2014  1\n  2/2014  1\n  1/2015  1\n  2/2015  1\n"
"df\n                      a\n2015-07-16 07:14:41  12\n2015-07-16 07:14:48  34\n2015-07-16 07:14:54  65\n2015-07-16 07:15:01  34\n2015-07-16 07:15:07  23\n2015-07-16 07:15:14   1\n\ndf.loc['2015-07-16 07:00:00':'2015-07-16 23:00:00']\n                      a\n2015-07-16 07:14:41  12\n2015-07-16 07:14:48  34\n2015-07-16 07:14:54  65\n2015-07-16 07:15:01  34\n2015-07-16 07:15:07  23\n2015-07-16 07:15:14   1\n\ndf[(df.index.get_level_values(0) &gt;= '2015-07-16 07:00:00') &amp; (df.index.get_level_values(0) &lt;= '2015-07-16 23:00:00')]\n"
"df.columns = ['Leader', 'Time', 'Score']\n"
"df['b_new'] = df.groupby('a')['b'].transform('last')\n\ndf['b_new'] = df.groupby('a')['b'].transform(lambda x: x.iat[-1])\n\nprint(df)\n   a   b  b_new\n0  1  20     21\n1  1  21     21\n2  2  30     30\n3  3  40     41\n4  3  41     41\n\ndf = df.join(df.groupby('a')['b'].nth(-1).rename('b_new'), 'a')\nprint(df)\n   a   b  b_new\n0  1  20     21\n1  1  21     21\n2  2  30     30\n3  3  40     41\n4  3  41     41\n\nN = 10000\n\ndf = pd.DataFrame({'a':np.random.randint(1000,size=N),\n                   'b':np.random.randint(10000,size=N)})\n\n#print (df)\n\n\ndef f(df):\n    return df.join(df.groupby('a')['b'].nth(-1).rename('b_new'), 'a')\n\n#cᴏʟᴅsᴘᴇᴇᴅ1\nIn [211]: %timeit df['b_new'] = df.a.map(df.groupby('a').b.nth(-1))\n100 loops, best of 3: 3.57 ms per loop\n\n#cᴏʟᴅsᴘᴇᴇᴅ2\nIn [212]: %timeit df['b_new'] = df.a.replace(df.groupby('a').b.nth(-1))\n10 loops, best of 3: 71.3 ms per loop\n\n#jezrael1\nIn [213]: %timeit df['b_new'] = df.groupby('a')['b'].transform('last')\n1000 loops, best of 3: 1.82 ms per loop\n\n#jezrael2\nIn [214]: %timeit df['b_new'] = df.groupby('a')['b'].transform(lambda x: x.iat[-1])\n10 loops, best of 3: 178 ms per loop\n\n#jezrael3\nIn [219]: %timeit f(df)\n100 loops, best of 3: 3.63 ms per loop\n"
