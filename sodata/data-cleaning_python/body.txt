'I have a dataframe, df, that has some columns of type float64, while the others are of object. Due to the mixed nature, I cannot use \n\ndf.fillna(\'unknown\') #getting error "ValueError: could not convert string to float:"\n\n\nas the error happened with the columns whose type is float64 (what a misleading error message!)\n\nso I\'d wish that I could do something like \n\nfor col in df.columns[&lt;dtype == object&gt;]:\n    df[col] = df[col].fillna("unknown")\n\n\nSo my question is if there is any such filter expression that I can use with df.columns?\n\nI guess alternatively, less elegantly, I could do:  \n\n for col in df.columns:\n        if (df[col].dtype == dtype(\'O\')): # for object type\n            df[col] = df[col].fillna(\'\') \n            # still puzzled, only empty string works as replacement, \'unknown\' would not work for certain value leading to error of "ValueError: Error parsing datetime string "unknown" at position 0" \n\n\nI also would like to know why in the above code replacing \'\' with \'unknown\' the code would work for certain cells but failed with a cell with the error of "ValueError: Error parsing datetime string "unknown" at position 0" \n\nThanks a lot!\n\nYu\n'
"In Python, I have a pandas DataFrame similar to the following:\n\nItem | shop1 | shop2 | shop3 | Category\n------------------------------------\nShoes| 45    | 50    | 53    | Clothes\nTV   | 200   | 300   | 250   | Technology\nBook | 20    | 17    | 21    | Books\nphone| 300   | 350   | 400   | Technology\n\n\nWhere shop1, shop2 and shop3 are the costs of every item in different shops.\nNow, I need to  return a DataFrame, after some data cleaning, like this one:\n\nCategory (index)| size| sum| mean | std\n----------------------------------------\n\n\nwhere size is the number of items in each Category and sum, mean and std are related to the same functions applied to the 3 shops. How can I do these operations with the split-apply-combine pattern (groupby, aggregate, apply,...) ?\n\nCan someone help me out? I'm going crazy with this one...thank you!\n"
'List with attributes of persons loaded into pandas dataframe df2. For cleanup I want to replace value zero (0 or \'0\') by np.nan. \n\ndf2.dtypes\n\nID                   object\nName                 object\nWeight              float64\nHeight              float64\nBootSize             object\nSuitSize             object\nType                 object\ndtype: object\n\n\nWorking code to set value zero to np.nan:\n\ndf2.loc[df2[\'Weight\'] == 0,\'Weight\'] = np.nan\ndf2.loc[df2[\'Height\'] == 0,\'Height\'] = np.nan\ndf2.loc[df2[\'BootSize\'] == \'0\',\'BootSize\'] = np.nan\ndf2.loc[df2[\'SuitSize\'] == \'0\',\'SuitSize\'] = np.nan\n\n\nBelieve this can be done in a similar/shorter way:\n\ndf2[["Weight","Height","BootSize","SuitSize"]].astype(str).replace(\'0\',np.nan)\n\n\nHowever the above does not work. The zero\'s remain in df2. How to tackle this?\n'
"I am dealing with pandas DataFrames like this:\n\n   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1  NaN\n5   2  NaN\n6   1  300\n7   1  NaN\n\n\nI would like to replace each NAN 'x' with the previous non-NAN 'x' from a row with the same 'id' value:\n\n   id    x\n0   1   10\n1   1   20\n2   2  100\n3   2  200\n4   1   20\n5   2  200\n6   1  300\n7   1  300\n\n\nIs there some slick way to do this without manually looping over rows?\n"
'I am doing a data cleaning exercise on python and the text that I am cleaning contains Italian words which I would like to remove. I have been searching online whether I would be able to do this on Python using a tool kit like nltk. \n\nFor example given some text : \n\n"Io andiamo to the beach with my amico."\n\n\nI would like to be left with : \n\n"to the beach with my" \n\n\nDoes anyone know of a way as to how this could be done?\nAny help would be much appreciated. \n'
"The pandas factorize function assigns each unique value in a series to a sequential, 0-based index, and calculates which index each series entry belongs to.\n\nI'd like to accomplish the equivalent of pandas.factorize on multiple columns:\n\nimport pandas as pd\ndf = pd.DataFrame({'x': [1, 1, 2, 2, 1, 1], 'y':[1, 2, 2, 2, 2, 1]})\npd.factorize(df)[0] # would like [0, 1, 2, 2, 1, 0]\n\n\nThat is, I want to determine each unique tuple of values in several columns of a data frame, assign a sequential index to each, and compute which index each row in the data frame belongs to.\n\nFactorize only works on single columns. Is there a multi-column equivalent function in pandas?\n"
"Applying pandas.to_numeric to a dataframe column which contains strings that represent numbers (and possibly other unparsable strings) results in an error message like this:\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-66-07383316d7b6&gt; in &lt;module&gt;()\n      1 for column in shouldBeNumericColumns:\n----&gt; 2     trainData[column] = pandas.to_numeric(trainData[column])\n\n/usr/local/lib/python3.5/site-packages/pandas/tools/util.py in to_numeric(arg, errors)\n    113         try:\n    114             values = lib.maybe_convert_numeric(values, set(),\n--&gt; 115                                                coerce_numeric=coerce_numeric)\n    116         except:\n    117             if errors == 'raise':\n\npandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:53558)()\n\npandas/src/inference.pyx in pandas.lib.maybe_convert_numeric (pandas/lib.c:53344)()\n\nValueError: Unable to parse string\n\n\nWouldn't it be helpful to see which value failed to parse? \n"
"As per application requirement, I need to show all the data which is part of group by in comma separated format so the admin can take decision, I am new to Python and not sure how to do it.\n\nSample reproducible data \n\nimport pandas as pd\n\ncompnaies = ['Microsoft', 'Google', 'Amazon', 'Microsoft', 'Facebook', 'Google']\nproducts = ['OS', 'Search', 'E-comm', 'X-box', 'Social Media', 'Android']\n\ndf = pd.DataFrame({'company' : compnaies, 'product':products })\n-----------------------------------------------------------------   \n    company     product\n0   Microsoft   OS\n1   Google      Search\n2   Amazon      E-comm\n3   Microsoft   X-box\n4   Facebook    Social Media\n5   Google      Android\n\n\nNow I getting count of company group by this code\n\ndf.groupby(['company']).count()\n\n\nI need data in below mentioned format but not sure how to get it \n\nDesired output\n\ncompany    count product\nAmazon      1    E-comm\nFacebook    1    Social Media\nGoogle      2    Search, Android\nMicrosoft   2    OS, X-box\n\n"
'I am having a dataframe that contains columns named id, country_name, location and total_deaths. While doing data cleaning process, I came across a value in a row that has \'\\r\' attached. Once I complete cleaning process, I store the resulting dataframe in destination.csv file. Since the above particular row has \\r attached, it always creates a new row.\n\nid                               29\nlocation            Uttar Pradesh\\r\ncountry_name                  India\ntotal_deaths                     20\n\n\nI want to remove \\r. I tried df.replace({\'\\r\': \'\'}, regex=True). It isn\'t working for me.\n\nIs there any other solution. Can somebody help?\n\nEdit:\n\nIn the above process, I am iterating over df to see if \\r is present. If present, then need to replace. Here row.replace() or row.str.strip() doesn\'t seem to be working or I could be doing it in a wrong way.\n\nI don\'t want specify the column name or row number while using replace(). Because I can\'t be certain that only \'location\' column will be having \\r. Please find the code below.\n\ncount = 0\nfor row_index, row in df.iterrows():\n    if re.search(r"\\\\r", str(row)):\n        print type(row)               #Return type is pandas.Series\n        row.replace({r\'\\\\r\': \'\'} , regex=True)\n        print row\n        count += 1\n\n'
'I want to remove hashtag symbol (\'#\') and underscore that separate between words (\'_\')\n\nExample: "this tweet is example #key1_key2_key3"\n\nthe result I want: "this tweet is example key1 key2 key3"\n\nMy code  using string :\n\n#Remove punctuation , # Hashtag Symbol \ntranslate_table = dict((ord(char), None) for char in string.punctuation)   \ncleaned_combined_tweets.translate(translate_table)\n\n\nwhich gives the result: "this tweet is example key1key2key3"\n'
'I have a dataframe like this:\n\ndata = np.array([["userA","event2, event3"],\n            [\'userB\',"event3, event4"],\n            [\'userC\',"event2"]])\n\ndata = pd.DataFrame(data)\n\n        0         1\n0   userA   "event2, event3"\n1   userB   "event3, event4"\n2   userC   "event2"\n\n\nnow I would like to get a dataframe like this:\n\n       0    event2      event3      event4\n0   userA     1           1\n1   userB                 1           1\n2   userC     1\n\n\ncan anybody help please?\n'
'I have a large data set which some users put in data on an csv. I converted the CSV into a dataframe with panda. The column is over 1000 entries here is a sample\n\ndatestart\n5/5/2013\n6/12/2013\n11/9/2011\n4/11/2013\n10/16/2011\n6/15/2013\n6/19/2013\n6/16/2013\n10/1/2011\n1/8/2013\n7/15/2013\n7/22/2013\n7/22/2013\n5/5/2013\n7/12/2013\n7/29/2013\n8/1/2013\n7/22/2013\n3/15/2013\n6/17/2013\n7/9/2013\n3/5/2013\n5/10/2013\n5/15/2013\n6/30/2013\n6/30/2013\n1/1/2006\n00/00/0000\n7/1/2013\n12/21/2009\n8/14/2013\nFeb 1 2013\n\n\nThen I tried converting the dates into years  using\n\ndf[\'year\']=df[\'datestart\'].astype(\'timedelta64[Y]\')\n\n\nBut it gave me an error:\n\nValueError: Value cannot be converted into object Numpy Time delta\n\n\nUsing Datetime64\n\ndf[\'year\']=pd.to_datetime(df[\'datestart\']).astype(\'datetime64[Y]\')\n\n\nit gave:\n\n"ValueError: Error parsing datetime string ""03/13/2014"" at position 2"\n\n\nSince that column was filled in by users, the majority was in this format MM/DD/YYYY but some data was put in like this: Feb 10 2013 and there was one entry like this 00/00/0000. I am guessing the different formats screwed up the processing. \n\nIs there a try loop, if statement, or something that I can skip over problems like these?\n\nIf date time fails I will be force to use a str.extract script which also works:\n\nyear=df[\'datestart\'].str.extract("(?P&lt;month&gt;[0-9]+)(-|\\/)(?P&lt;day&gt;[0-9]+)(-|\\/)(?P&lt;year&gt;[0-9]+)")\n\n\ndel df[\'month\'], df[\'day\']  \n\n\nand use concat to take the year out.\n\nWith df[\'year\']=pd.to_datetime(df[\'datestart\'],coerce=True, errors =\'ignore\').astype(\'datetime64[Y]\') The error message is:\n\nMessage File Name   Line    Position    \nTraceback               \n    &lt;module&gt;    C:\\Users\\0\\Desktop\\python\\Example.py    23      \n    astype  C:\\Python33\\lib\\site-packages\\pandas\\core\\generic.py    2062        \n    astype  C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py  2491        \n    apply   C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py  3728        \n    astype  C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py  1746        \n    _astype C:\\Python33\\lib\\site-packages\\pandas\\core\\internals.py  470     \n    _astype_nansafe C:\\Python33\\lib\\site-packages\\pandas\\core\\common.py 2222        \nTypeError: cannot astype a datetimelike from [datetime64[ns]] to [datetime64[Y]]        \n\n'
"I have a stock data grabbed from Yahoo finance, adjusted close data is wrong somehow.\n\n            adj_close    close     ratio\ndate                                    \n2014-10-16   240.4076  2466.40  0.097473\n2014-10-17   245.8173  2521.90  0.097473\n2014-10-20   250.4522  2569.45  0.097473\n2014-10-21   251.8850  2584.15  0.097473\n2014-10-22   251.0175  2575.25  0.097473\n2014-10-23   251.3392  2578.55  0.097473\n2014-10-27   253.2155  2597.80  0.097473\n2014-10-28   258.9616  2656.75  0.097473\n2014-10-29   257.6944  2643.75  0.097473\n2014-10-30   257.1339  2638.00  0.097473\n2014-10-31    26.3450  2702.80  0.009747\n2014-11-03    26.5463  2723.45  0.009747\n2014-11-05    27.1160  2781.90  0.009747\n2014-11-07    26.7320  2742.50  0.009747\n2014-11-10    26.7027  2739.50  0.009747\n\n\nHere's a plot of the adjusted close data:\n\n\n\nHow can I replace data like this using any methods like interpolation or something?\n"
"I have a table that looks like this:\n\ncompany_id,company_name\n1,Amazon\n1,Amazon Ltd\n2,Google\n1,Amazon\n2,Gogle\n3,Facebook Ltd\n3,Facebook LTD\n1,AMAZON\n1,AMAZON LTD\n2,GOOGLE\n3,Facebook\n3,Face book\n\n\nSo I have a unique identifier for each company, but their textual representation differs. What I strive for is to eliminate these inconsistencies and have something along the lines of:\n\ncompany_id,company_name\n1,Amazon\n1,Amazon\n2,Google\n1,Amazon\n2,Google\n3,Facebook\n3,Facebook\n1,Amazon\n1,Amazon\n2,Google\n3,Facebook\n3,Facebook\n\n\nI'm not dead set on the selection criterion - it can be the most common value within said group, it can be a random one. But what I need is something efficient, because my table has grown to contain millions of rows.\n\nMy solution was to create a hash map of unique combos of id -&gt; name and then replacing based on these. Something along the lines of:\n\ndd = df.drop_duplicates().set_index('company_id').to_dict()['company_name']\ndf.company_name = df.company_id\ndf.company_name = df.company_name.replace(dd)\n\n\nWhich works fine on smaller sets, but it gets fairly slow and memory inefficient due to the large hash map it creates.\n\nI've also tried a groupby based on company_id and replacing all the company_names within each group by a random value, but I couldn't get to modify the underlying dataframe (without .locing it, which is doubly inefficient).\n\nOne last option that springs to mind is to create a separate frame with unique values (df.drop_duplicates('company_id')) and merge this with the original frame based on company_id, but it doesn't sound terribly efficient either.\n"
'New to pandas development. How do I forward fill a DataFrame with the value contained in one previously seen column?\n\nSelf-contained example:\n\nimport pandas as pd\nimport numpy as np\nO = [1, np.nan, 5, np.nan]\nH = [5, np.nan, 5, np.nan]\nL = [1, np.nan, 2, np.nan]\nC = [5, np.nan, 2, np.nan]\ntimestamps = ["2017-07-23 03:13:00", "2017-07-23 03:14:00", "2017-07-23 03:15:00", "2017-07-23 03:16:00"]\ndict = {\'Open\': O, \'High\': H, \'Low\': L, \'Close\': C}\ndf = pd.DataFrame(index=timestamps, data=dict)\nohlc = df[[\'Open\', \'High\', \'Low\', \'Close\']]\n\n\nThis yields the following DataFrame:\n\nprint(ohlc)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   NaN   NaN  NaN    NaN\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   NaN   NaN  NaN    NaN\n\n\nI want to go from that last DataFrame to something like this:\n\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n\n\nSo that the previously-seen value in \'Close\' forward fills entire rows until there\'s a new populated row seen. It\'s simple enough to fill column \'Close\' like so: \n\ncolumn2fill = \'Close\'\nohlc[column2fill] = ohlc[column2fill].ffill()\nprint(ohlc)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   NaN   NaN  NaN    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   NaN   NaN  NaN    2.0\n\n\nBut is there a way to fill across the  03:14:00 and 03:16:00 rows with the \'Close\' value of those rows? And is there a way to do it in one step using one forward fill instead of filling the \'Close\' column first?\n'
"I want to clean one column of my df['emp_length']\n[shown in the screen shot]1\n\nbut when I use \n\ndf_10v['emp_length'] = df_10v['emp_length'].map(lambda x: x.lstrip('&lt;').rstrip('+'))\n\n\nto remove thing i dont want. It gave me an error:\n\n'float' object has no attribute 'lstrip'\n\n\nHowever, the type shows object instead of float.\nI tried .remove too but gave me the same error. \nI also tried \n\ndf['column'] = df['column'].astype('str') \n\n\nto change df_10v['emp_length'] in to string and then strip, but it does not work either. Anybody know how to solve this? Thank you!\n"
'I would like to keep only the rows of a Dataframe with the following condition: the intervals(included) in which the beginning condition is col1 = 0, col2 = 1 and the interval end col1 = 0, col2 = 2.\n\nSample data\n\nimport pandas as pd\n\npd.DataFrame({\'id\':[\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id1\',\'id2\',\'id2\',\'id2\',\'id2\',\'id2\']\n                  ,\'col1\':[0,1,1,0,1,0,0,1,1,0,0,1,0,0,1,1],\'col2\':[1,2,2,1,2,2,1,2,2,2,1,2,2,1,2,2]})\n\n\nThis would look like this:\n\n    col1 col2 id\n0   0   1   id1\n1   1   2   id1\n2   1   2   id1\n3   0   1   id1\n4   1   2   id1\n5   0   2   id1\n6   0   1   id1\n7   1   2   id1\n8   1   2   id1\n9   0   2   id1\n10  0   1   id1\n11  1   2   id2\n12  0   2   id2\n13  0   1   id2\n14  1   2   id2\n15  1   2   id2\n\n\nOutput Sample\n\nWe can realise that there are only "blocks" or intervals with 0-1,0-2 in col1,col2.\n\n   col1 col2 id\n3   0   1   id1\n4   1   2   id1\n5   0   2   id1\n6   0   1   id1\n7   1   2   id1\n8   1   2   id1\n9   0   2   id1\n10  0   1   id1\n11  1   2   id2\n12  0   2   id2\n\n\nAs a result rows 0,1,2,13,14,15 were erased because they weren\'t a in a 0-1 , 0-2 interval.\n'
'I`m cleaning a messy data frame where some of the information needed appear in the column names. This information should melt to a single column that would be created.\n\nindex    name       animal    fruit    veg\n--------------------------------------------------\n0        cow        animal    NaN      NaN\n1        apple      NaN       fruit    NaN\n2        carrot     NaN       NaN      veg\n3        dog        animal    NaN      NaN\n4        horse      animal    NaN      NaN\n5        car        NaN       NaN      NaN\n6        pear       NaN       fruit    NaN\n7        pepper     NaN       NaN      veg\n8        cucumber   NaN       NaN      veg\n9        house      NaN       NaN      NaN\n\n\n\nI\'ve tried using the pandas.melt() function, however it returns a lot of rows with "wrong" NaN values and duplicates. \n\nSome of the rows are supposed to show NaN, but only the ones who don\'t fit into the categories specified in the column names, so I can\'t use the pandas.dropna(). \n\nAlso I can\'t be sure that removing the duplicates wouldn\'t remove important data.\n\nThis is the code I used:\n\nimport pandas as pd\n\npd.melt(df, id_vars=[\'index\', \'name\'],\n        value_vars=[\'animal\', \'fruit\', \'veg\'],\n        var_name=\'type\')\n\n\nThe result I need should look something like this:\n\nindex    name       type\n--------------------------------------------------\n0        cow        animal\n1        apple      fruit\n2        carrot     veg\n3        dog        animal\n4        horse      animal\n5        car        NaN\n6        pear       fruit\n7        pepper     veg\n8        cucumber   veg\n9        house      NaN\n\n\n'
'I am cleaning the monolingual corpus of Europarl for French (http://data.statmt.org/wmt19/translation-task/fr-de/monolingual/europarl-v7.fr.gz). The original raw data in .gzfile (I downloaded using wget). I want to extract the text and see how it looks like in order to further process the corpus.\n\nUsing the following code to extract the text from gzip, I obtained data with the class being bytes.\n\nwith gzip.open(file_path, \'rb\') as f_in:\n    print(\'type(f_in)=\', type(f_in))\n    text = f_in.read()\n    print(\'type(text)=\', type(text))\n\n\nThe printed results for several first lines are as follows:\n\n\n  type(f_in) = class \'gzip.GzipFile\'\n  \n  type(text)= class \'bytes\'\n  \n  b\'Reprise de la session\\nJe d\\xc3\\xa9clare reprise la session du Parlement europ\\xc3\\xa9en qui avait \\xc3\\xa9t\\xc3\\xa9 interrompue le vendredi 17 d\\xc3\\xa9cembre dernier et je vous renouvelle tous mes vux en esp\\xc3\\xa9rant que vous avez pass\\xc3\\xa9 de bonnes vacances.\\nComme vous avez pu le constater, le grand "bogue de l\\\'an 2000" ne s\\\'est pas produit.\\n\n\n\nI tried to decode the binary data with utf8 and ascii with the following code:\n\nwith gzip.open(file_path, \'rb\') as f_in:\n    print(\'type(f_in)=\', type(f_in))\n    text = f_in.read().decode(\'utf8\')\n    print(\'type(text)=\', type(text))\n\n\nAnd it returned errors like this:\n\n\n  UnicodeEncodeError: \'ascii\' codec can\'t encode character \'\\xe9\' in position 26: ordinal not in range(128)\n\n\nI also tried using codecs and unicodedata packages to open the file but it returned encoding error as well.\n\nCould you please help me explain what I should do to get the French text in the correct format like this for example?\n\n\n  Reprise de la session\\nJe déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\\nComme vous avez pu le constater, le grand "bogue de l\'an 2000" ne s\'est pas produit.\\n\n\n\nThank you a ton for your help!\n'
'I have a large dataset with more than 1000 columns, the dataset is messy with mixed dtypes. There are 2 int64 columns, 119 float columns and 1266 object columns.\n\nI would like to begin data cleaning but realised there are several issues. As there are too many columns, visual inspection of the data to locate errors is too tedious. An sample of the dataset is below \n\nCompany ID  Year    Date         Actual Loan Loss  Depreciation          Accounts Payable\n001         2001    19 Oct 2001  100000.00         40000                 $$ER: 4540,NO DATA VALUES FOUND\n002         2002    18 Sept 2001 NaN               $$ER: E100,NO WORLDSCOPE DATA FOR THIS CODE\n003         2004    01 Aug 2000  145000.00         5000                  Finance Dept\n\n\nI would like to remove all the error variables before dropping the null rows. The error variables typically start with "$$ER:"\n\nI\'ve tried the following\n\n#load the dataset\ndf = pd.read_excel("path/file1.xlsx", sheet_name = "DATA_TS")\n#examine the data\ndf.head(20)\n#check number of rows, cols and dtypes\ndf.info()\n\n#create a function to replace the error values\n\ndef convert_datatypes(val):\n    new_val = val.replace(\'$$ER: 4540,NO DATA VALUES FOUND\',\'\').replace(\'$$ER: E100,NO WORLDSCOPE DATA FOR THIS CODE\', \'\')\n    return new_val\n\ndf.apply(convert_datatypes)\n\n\nThe code worked but I checked again and realised that there were other error values such as "$$ER: E100,INVALID CODE OR EXPRESSION ENTERED".\n\nI am pretty sure there are other error values as well, would like to find out if there are any other ways to efficiently remove the error values AND AT THE SAME TIME, change the dtype of the columns to the supposedly correct dtype (i.e., from object to either int or str)? \n\nAppreciate any form of help, thank you in advance!\n'
"I have to columns in pandas dataframe, one with keys second with values, where both are list of lists.\nLike this:\nimport pandas as pd \nexample = pd.DataFrame( {'col1': [['key1','key2','key3'],['key1','key4'],['key1', 'key3', 'key4','key5']], 'col2': [['value1','value2','value3'], ['value1','value4'], ['value1', 'value3', 'value4','value5']]  }) \nprint(example)\n\n    col1    col2\n0   [key1, key2, key3]  [value1, value2, value3]\n1   [key1, key4]    [value1, value4]\n2   [key1, key3, key4, key5]    [value1, value3, value4, value5]\n\nFirst i want to convert all possible keys to columns, the append values to them.\nFinal result should look like this\n    key1      key2    key3     key4    key5\n0   value1    value2  value3   NaN     NaN\n1   value1    NaN     NaN      value4  NaN\n2   value1    NaN     value3   value4  value5\n        \n\n"
"This maybe a simple solution, but I am finding it hard to make this function work for my dataset.\nI have a salary column with variety of data in it. Example dataframe below:\nID   Income                              desired Output         \n1    26000                               26000\n2    45K                                 45000\n3    -                                   NaN\n4    0                                   NaN\n5    N/A                                 NaN\n6    2000                                2000   \n7    30000 - 45000                       37500 (30000+45000/2)   \n8    21000 per Annum                     21000                \n9    50000 per annum                     50000\n10   21000 to 30000                      25500 (21000+30000/2)\n11                                       NaN\n12   21000 To 50000                      35500 (21000+50000/2)\n13   43000/year                          43000\n14                                       NaN\n15   80000/Year                          80000\n16   12.40 p/h                           12896 (12.40 x 20 x 52)\n17   12.40 per hour                      12896 (12.40 x 20 x 52)\n18   45000.0 (this is a float value)     45000       \n\n@user34974 - has been very helpful in providing the workable solution (below). However, the solution provides me with an error because the dataframe column also consists of float values. Can anyone help in catering for float values in the function that can be taken care of in dataframe column? In the end the output in updated column should be float values.\nNormrep = ['N/A','per Annum','per annum','/year','/Year','p/h','per hour',35000.0]\n\ndef clean_income(value):\n    for i in Normrep:\n        value = value.replace(i,&quot;&quot;)\n\n\n\n    if len(value) == 0 or value.isspace() or value == '-': #- cannot be clubbed to array as used else where in data\n        return np.nan\n    elif value == '0':\n        return np.nan\n\n    # now there should not be any extra letters with K hence can be done below step\n    if value.endswith('K'):\n        value = value.replace('K','000')\n    \n    # for to and -\n    vals = value.split(' to ')\n    if len(vals) != 2:\n        vals = value.split(' To ')\n        if len(vals) != 2:\n            vals = value.split(' - ')\n\n    if len(vals) == 2:\n        return (float(vals[0]) + float(vals[1]))/2\n\n    try:\n        a = float(value)\n        return a\n    except:\n        return np.nan    # Either not proper data or need to still handle some fromat of inputs.\n\n\ntestData = ['26000','45K','-','0','N/A','2000','30000 - 45000','21000 per Annum','','21000 to 30000','21000 To 50000','43000/year', 35000.0]\n\n\ndf = pd.DataFrame(testData)\nprint(df)\n\ndf[0] = df[0].apply(lambda x: clean_income(x))\n\nprint(df)\n\n"
'I am trying to pull out months within certain years with pandas. I have the constraints returned as such {month: year}.\n\n [{1: 2003},\n {2: 2008},\n {3: 2011},\n {4: 2012},\n {5: 2008},\n {6: 2008},\n {7: 2002},\n {8: 2006},\n {9: 2005},\n {10: 2013},\n {11: 2005},\n {12: 2001}]\n\n\nMeans I want January 2003, February 2008, etc. from the data frame. I have "Month" and "Year" as two columns in the data frame. \n\nI want something that executes this incorrect code (but the idea is clear):\n\ndf[(df[\'Month\'] == key for key in dict) &amp; (df[\'Year\'] == dict[key])]\n\n'
'I have the following data frame:\n\nid        day           total_amount\n 1       2015-07-09         1000\n 1       2015-10-22          100\n 1       2015-11-12          200\n 1       2015-11-27         2392\n 1       2015-12-16          123\n 7       2015-07-09          200\n 7       2015-07-09         1000\n 7       2015-08-27       100018\n 7       2015-11-25         1000\n 8       2015-08-27         1000\n 8       2015-12-07        10000\n 8       2016-01-18          796\n 8       2016-03-31        10000\n15       2015-09-10         1500\n15       2015-09-30         1000\n\n\nI need to subtract every two successive time in day column if they have the same id until reaching the last row of that id then start subtracting times in day column this time for new id, something similar to following lines in output is expected: \n\n 1       2015-08-09         1000 2015-11-22 - 2015-08-09\n 1       2015-11-22          100 2015-12-12 - 2015-11-22\n 1       2015-12-12          200 2015-12-16 - 2015-12-12\n 1       2015-12-16         2392 2015-12-27 - 2015-12-27\n 1       2015-12-27          123         NA\n 7       2015-08-09          200 2015-09-09 - 2015-08-09\n 7       2015-09-09         1000 2015-09-27 - 2015-09-09\n 7       2015-09-27       100018 2015-12-25 - 2015-09-27\n 7       2015-12-25         1000         NA\n 8       2015-08-27         1000  2015-12-07 - 2015-08-27\n 8       2015-12-07        10000  2016-02-18 - 2015-12-07\n 8       2016-02-18          796   2016-04-31- 2016-02-18     \n 8       2016-04-31        10000         NA\n15       2015-10-10         1500  2015-10-30 - 2015-10-10\n15       2015-10-30         1000         NA\n\n'
'I have a DataFrame where the column \'Name\' has some errors in it.  I have created a dictionary with the incorrect spellings at the key and the values as the correct spelling.  What is the best way to replace the the incorrect spellings with the correct spellings?  This is what I did.\n\nfor incorrect, correct in incorrect_to_correct.items():\n    mask = s_df[\'Name\'] == incorrect\n    s_df.loc[mask, \'Name\'] = correct\n\n\nIs there a better way of doing this?  I was told that generally if you are using a for loop with pandas that you should rethink what you are doing?  Is there a better way to clean up the data?  Is this dictionary method "wrong"?  I am new to pandas and any help would be appreciated.  Thanks!\n'
"I am new to data science and currently I'm exploring a bit further. I have over 600,000 columns of a data set and I'm currently cleaning and checking it for inconsistency or outliers. I came across a problem which I am not sure how to solve it. I have some solutions in mind but I am not sure how to do it with pandas.\n\nI have converted the data types of some columns from object to int. I got no errors and checked whether it's in int and it was. I checked the values of one column to check for the factual data. This involves age and I got an error saying my column has a string. so I checked it using this method:\n\nprint('if there is string in numeric column',np.any([isinstance(val, str) for val in homicide_df['Perpetrator Age']])\n\nNow, I wanted to print all indices and with their values and type only on this column which has the string data type.\n\ncurrently I came up with this solution that works fine:\n\ndef check_type(homicide_df):\n    for age in homicide_df['Perpetrator Age']:\n        if type(age) is str:\n            print(age, type(age))\ncheck_type(homicide_df)\n\n\nHere are some of the questions I have:\n\n\nis there a pandas way to do the same thing?\nhow should I convert these elements to int?\nwhy were some elements on the columns did not convert to int?\n\n\nI would appreciate any help. Thank you very much\n"
'I am cleaning a data set with fraudulent email addresses that I am removing. \n\nI established multiple rules for catching duplicates and fraudulent domains. But there is one screnario, where I can\'t think of how to code a rule in python to flag them. \n\nSo I have for example rules like this:\n\n#delete punction\ndf[\'email\'].apply(lambda x:\'\'.join([i for i in x if i not in string.punctuation]))    \n\n#flag yopmail\npattern = "yopmail"\nmatch = df[\'email\'].str.contains(pattern)\ndf[\'yopmail\'] = np.where(match, \'Y\', \'0\')\n\n#flag duplicates\ndf[\'duplicate\']=df.email.duplicated(keep=False)\n\n\nThis is the data where I can\'t figure out a rule to catch it. Basically I am looking for a way to flag addresses that start the same way, but then have consecutive numbers in the end. \n\nabc7020@gmail.com\nabc7020.1@gmail.com\nabc7020.10@gmail.com\nabc7020.11@gmail.com\nabc7020.12@gmail.com\nabc7020.13@gmail.com\nabc7020.14@gmail.com\nabc7020.15@gmail.com\nattn1@gmail.com\nattn12@gmail.com\nattn123@gmail.com\nattn1234@gmail.com\nattn12345@gmail.com\nattn123456@gmail.com\nattn1234567@gmail.com\nattn12345678@gmail.com\n\n'
"I am reading in a text file, on each line there are multiple values. I am parsing them based on requirements using function parse.\n\ndef parse(line):\n    ......\n    ......\n    return line[0],line[2],line[5]\n\n\nI want to create a dataframe, with each line as a row and the three returened values as columns\n\ndf = pd.DataFrame()\n\nwith open('data.txt') as f:\n    for line in f:\n       df.append(line(parse(line)))\n\n\nWhen I run the above code, I get all values as a single column. Is it possible to get it in proper tabular format. \n"
"I have a dataframe in the following format: \n\nimport pandas as pd\nd1 = {'ID': ['A','A','A','B','B','B','B','B','C'], \n'Time': \n['1/18/2016','2/17/2016','2/16/2016','1/15/2016','2/14/2016','2/13/2016',\n'1/12/2016','2/9/2016','1/11/2016'],\n'Product_ID': ['2','1','1','1','1','2','1','2','2'], \n'Var_1': [0.11,0.22,0.09,0.07,0.4,0.51,0.36,0.54,0.19],\n'Var_2': [1,0,1,0,1,0,1,0,1],\n'Var_3': ['1','1','1','1','0','1','1','0','0']}\ndf1 = pd.DataFrame(d1)\n\n\nWhere df1 is of the form:\n\nID  Time        Product_ID  Var_1   Var_2   Var_3\nA   1/18/2016   2           0.11    1       1\nA   2/17/2016   1           0.22    0       1\nA   2/16/2016   1           0.09    1       1\nB   1/15/2016   1           0.07    0       1\nB   2/14/2016   1           0.4     1       0\nB   2/13/2016   2           0.51    0       1\nB   1/12/2016   1           0.36    1       1\nB   2/9/2016    2           0.54    0       0\nC   1/11/2016   2           0.19    1       0\n\n\nwhere time is in 'MM/DD/YY' format.\n\nThis is what I have to do:\n\n1) I would like to do is to group ID's and Product ID's by Time (Specifically by each Month).  \n2) I want to then carry out the following column operations.  \n   a) First, I would like to find the sum of the columns of Var_2 and Var_3 and \n   b) find the mean of the column Var_1.  \n3) Then, I would like to create a column of count of each ID and Product_ID for each month. \n\n4) And finally, I would also like to input items ID and Product ID for which there is no entries. \n\nFor example, for ID = A and Product ID = 1 in Time = 2016-1 (January 2016), there are no observations and thus all variables take the value of 0.  \nAgain, For ID = A and Product ID = 1 in Time = 2016-2 (January 2016),  Var_1 = (.22+.09)/2 = 0.155 Var_2 = 1,  Var_3 = 1+1=2  and finally Count = 2.\n\nThis is the output that I would like.\n\nID  Product_ID  Time    Var_1   Var_2   Var_3   Count\nA   1           2016-1  0       0       0       0\nA   1           2016-2  0.155   1       2       2\nB   1           2016-1  0.215   1       1       2\nB   1           2016-2  1       0.4     0       1\nC   1           2016-1  0       0       0       0\nC   1           2016-2  0       0       0       0\nA   2           2016-1  0.11    1       1       1\nA   2           2016-2  0       0       0       0\nB   2           2016-1  0       0       0       0\nB   2           2016-2  0.455   1       2       2\nC   2           2016-1  0.19    1       0       1\nC   2           2016-2  0       0       0       0\n\n\nThis is a little more than my programming capabilities (I know the groupby function exits but I could not figure out how to incorporate the rest of the changes). Please let me know if you have questions. \n\nAny help will be appreciated. Thanks.\n"
"I am trying to clean the data using pandas. When I execute df.datatypes it shows that the columns are of type objects. I wish to convert them into numeric types.\nI tried various ways of doing so like;\n\ndata[['a','b']] = data[['a','b']].apply(pd.to_numeric, errors ='ignore')\n\n\nThen,\n\ndata['c'] = data['c'].infer_objects()\n\n\nBut nothing seems to be working. The interpreter does not throw any error but at the same time, it does not performs the desired conversion.\n\nAny help will be greatly appreciated.\n\nThanking in advance.\n"
'How can I convert the dataset\n\na    |    a b c d \ns    |    e f g h\nf    |    i j k l\n\n\nto \n\na | a | b | c | d\ns | e | f | g | h\nf | i | j | k | l\n\n'
"I know I can find duplicate columns using:\n\ndf.T.duplicated()\n\n\nwhat I'd like to know the index that a duplicate column is a duplicate of.  For example, both C and D are duplicates of a A below:\n\ndf = pd.DataFrame([[1,0,1,1], [2,0,2,2]], columns=['A', 'B', 'C', 'D'])\n\n   A  B  C  D\n0  1  0  1  1\n1  2  0  2  2\n\n\nI'd like something like:\n\nduplicate_index = pd.Series([None, None, 'A', 'A'], ['A', 'B', 'C', 'D'])\n\n"
'I have a dataframe:\n\n    Name    Section\n1   James   P3\n2   Sam     2.5C\n3   Billy   T35\n4   Sarah   A85\n5   Felix   5I\n\n\nHow do I split numeric values into a separate column called Section_Number and also split alphabetic values to Section_Letter.\nDesired results\n\n    Name    Section Section_Number  Section_Letter\n1   James   P3               3          P\n2   Sam     2.5C           2.5          C\n3   Billy   T35             35          T\n4   Sarah   A85             85          A\n5   Felix   5L               5          L\n\n'
'I often come across the following common problem when cleaning the data\nthere are some more common categories (let\'s say top 10 movie genres) and lots and lots of others which are sparse. Usual practice here would be to combine sparse genres into "Other" for example.\n\nEasily done when there are not many sparse categories:\n\n# Join bungalows as they are sparse classes into 1\ndf.property_type.replace([\'Terraced bungalow\',\'Detached bungalow\', \'Semi-detached bungalow\'], \'Bungalow\', inplace=True)\n\n\nbut if for example I have a movies dataset with majority of the movies produced by let\'s say 8 big studios and I would like to combine everything else under "other" studio, it makes sense to get top 8 studios:\n\ntop_8_list = []\ntop_8 = df.studio.value_counts().head(8)\nfor key, value in top_8.iteritems():\n    top_8_list.append(key)\n\ntop_8_list\ntop_8_list\n[\'Universal Pictures\',\n \'Warner Bros.\',\n \'Paramount Pictures\',\n \'Twentieth Century Fox Film Corporation\',\n \'New Line Cinema\',\n \'Columbia Pictures Corporation\',\n \'Touchstone Pictures\',\n \'Columbia Pictures\']\n\n\nand then do something like\n\nreplace studio where studio is not in the top 8 list with "other"\n\nso the question, if anyone knows any elegant solution in pandas for this? This is very common data cleaning task\n'
"I have some pricing data for parts that updates monthly. It has been pulled into a pandas dataframe. Occasionally, a part won't get a price for a certain month, in which case I would like to replace it with that part's price from the previous month. \n\nIn the event that the previous month also has a missing price for that part, I want to continue searching backwards until a valid price is found, in which case this price should propagate forward until a valid price is found. \n\nIf no valid prices are found for that part, then I want this part to be dropped from the dataframe entirely. \n\nIf the first number of months contain missing prices for a certain part, I would like to delete these rows so that the first record is always a valid price.\n\nEssentially I want to do a forward fill on the price column but taking part numbers into account.\n\nAs an example, I would start with something like this:\n\npart   price      date\n1      NaN        2018-12-01 00:00:00.000\n2      NaN        2018-12-01 00:00:00.000\n3      99.16      2018-12-01 00:00:00.000\n1      NaN        2018-11-01 00:00:00.000\n2      NaN        2018-11-01 00:00:00.000\n3      NaN        2018-11-01 00:00:00.000\n1      67.32      2018-10-01 00:00:00.000\n2      NaN        2018-10-01 00:00:00.000\n3      167.34     2018-10-01 00:00:00.000\n1      88.37      2018-09-01 00:00:00.000\n2      NaN        2018-09-01 00:00:00.000\n3      212.70     2018-09-01 00:00:00.000\n1      88.37      2018-08-01 00:00:00.000\n2      NaN        2018-08-01 00:00:00.000\n3      NaN        2018-08-01 00:00:00.000\n1      88.37      2018-07-01 00:00:00.000\n2      NaN        2018-07-01 00:00:00.000\n3      264.02     2018-07-01 00:00:00.000\n1      NaN        2018-06-01 00:00:00.000\n\n\nAnd end with this:\n\npart   price      date\n1      67.32      2018-12-01 00:00:00.000\n3      99.16      2018-12-01 00:00:00.000\n1      67.32      2018-11-01 00:00:00.000\n3      167.34     2018-11-01 00:00:00.000\n1      67.32      2018-10-01 00:00:00.000\n3      167.34     2018-10-01 00:00:00.000\n1      88.37      2018-09-01 00:00:00.000\n3      212.70     2018-09-01 00:00:00.000\n1      88.37      2018-08-01 00:00:00.000\n3      264.02     2018-08-01 00:00:00.000\n1      88.37      2018-07-01 00:00:00.000\n3      264.02     2018-07-01 00:00:00.000\n\n"
'I want to sum up rows in a dataframe which have the same row key.\n\nThe purpose will be to shrink the data set size down.\n\nFor example if the data frame looks like this.\n\nFruit       Count\n\nApple         10\n\nPear          20\n\nApple          5\n\nBanana         7\n\nBanana         12\n\nPear           8  \n\nApple          10\n\n\nI want the final dataframe to look like this.\n\nFruit       Count\n\nApple         25\n\nPear          28\n\nBanana        19\n\n\nI am using Python\'s pandas, numpy, matplotlib and other data analysis packages. Is there a way to do this in python using functions in these packages?\n\nHere is the code to create the example dataframe.\n\ndf = pd.DataFrame([["Apple", 10], ["Pear", 20], ["Apple", 5], ["Banana", 7], ["Banana", 12], ["Pear", 8], ["Apple", 10]], columns=["Fruit", "Count"])\n\n'
"So basically, I've been trying to fill a column's nan values based on another column.\n\nLet's say, I have a column that's called ''accommodates'' (meaning how many people a certain house can accommodate) and another column called bedrooms.\n\nTo fill these nan values, I found, for example, what's the most common value for accommodates when a house has 1 bedroom. It returned that the most common value is 2. What I wanted to do now is to fill the nan values in the column accommodates, that correspond to a 1 bedroom house, with 2.\n\nAn example of the data is below:\n\n accommodates bathrooms  bedrooms\n    nan         2.0       1.0\n    nan         2.0       1.0\n    nan         2.0       1.0\n    nan         2.0       1.0\n    nan         2.0       1.0\n    nan         2.0       1.0\n    ...         ...       ...\n\n\nI've done similar things for other attributes, so I tried the following code:\n\naccom_cond=((house.bedrooms==1) &amp; (house.accommodates.isna()))\naccom_val= [2,2,2,2,2,2,2,2,2,2,2,2,2,2]\n\nhouse.accommodates= np.select(accom_cond,accom_val,house.accommodates)\n\n\nThis is assuming that there are 14 NaN values under these circumstances (also, if you know a better way than to repeat the value 2 14times, I'd appreciate it :D)\n\nHowever, it doesn't not work. It returns the error: \n\nValueError: list of cases must be same length as list of conditions\n\n\nI tried to print accom_cond to see what is going on and it returned this:\n\naccom_cond\nOut[156]: \n0       False\n1       False\n2       False\n3       False\n4       False\n5       False\n6       False\n7       False\n8       False\n9       False\n10      False\n11      False\n12      False\n13      False\n14      False\n15      False\n16      False\n17      False\n18      False\n19      False\n20      False\n21      False\n22      False\n23      False\n24      False\n25      False\n26      False\n27      False\n28      False\n29      False\n        ...\n\n\nI don't get why it's not returning just the 14 null values that follow the conditions I defined.\n\nCan anyone help me with this? \n(Thank you in advance for taking the time to read this!!)\n"
'I have a list of strings associated with twitter hashtags. I want to remove entire strings that begin with certain prefixes.\n\nFor example:\n\ntestlist = [\'Just caught up with #FlirtyDancing. Just so cute! Loved it. \', \'After work drinks with this one @MrLukeBenjamin no dancing tonight though @flirtydancing @AshleyBanjo #FlirtyDancing pic.twitter.com/GJpRUZxUe8\', \'Only just catching up and @AshleyBanjo you are gorgeous #FlirtyDancing\', \'Loved working on this. Always a pleasure getting to assist the wonderful @kendrahorsburgh on @ashleybanjogram wonderful new show !! #flirtydancing pic.twitter.com/URMjUcgmyi\', \'Just watching #FlirtyDancing &amp; \\n@AshleyBanjo what an amazing way to meet someone.. It made my heart all warm &amp; fuzzy for these people! both couples meet back up.. pic.twitter.com/iwCLRmAi5n\',]\n\n\nI would like to remove the picture URL\'s, the hashtags, and the @\'s\n\nI have tried a few things so far, namely using the startswith() method and the replace() method.\n\nFor example:\n\nprefixes = [\'pic.twitter.com\', \'#\', \'@\']\nbestlist = []\n\nfor line in testlist:\n    for word in prefixes:\n        line = line.replace(word,"")\n        bestlist.append(line)\n\n\nThis seems to get rid of the \'pic.twitter.com\', but not the series of letters and numbers at the end of the URL. These strings are dynamic and will have a different end URL each time...which is why I want to get rid of the entire string if they begin with that prefix.\n\nI also tried tokenizing everything, but replace() still won\'t get rid of the entire word:\n\nimport nltk \n\nfor line in testlist:\ntokens = nltk.tokenize.word_tokenize(line)\nfor token in tokens:\n    for word in prefixes:\n        if token.startswith(word):\n            token = token.replace(word,"")\n            print(token)\n\n\nI am starting to lose hope in the startswith() method and the replace() method, and feel I might be barking up the wrong tree with these two.\n\nIs there a better way to go about this? How can I achieve the desired result of removing all strings beginning with #, @, and pic.twitter?\n'
"I have a dataframe in which the columns are supposed to be dummy columns (for each row only one column should be populated). However, the data has some 'noise' in it: some rows have more than one column populated. I want to drop these rows.\n\nSuppose the DataFrame looks like the below example:\n\n  a       b        c        d  \n0 NaN     1        NaN      NaN\n1 1       2        3        4  \n2 1       1        NaN      NaN \n3 NaN     NaN      1        NaN\n4 1       NaN      1        NaN\n\n\nSo my expected result is that rows [1,2,4] get dropped. You may say that I only accept rows where the number of NaN values is equal to the number_of_columns - 1.\n\nIs there any way to do this in pandas?\n"
"The scenario is: VendorID column contains '#' in all rows of pandas dataframe.\nI have been trying to substitute the value of '#' in VendorID column to the auto increment row number value.\n\nI was trying str.replace() function : \n\ndata['VendorID'].str.replace(r'[#]', replacing_value)\n\n\nI am trying to figure out what should i write in place of replacing_value, in the above line\n\nCurrently its showing like:\n\nVendorID\n#\n#\n#\n.\n.\n\n\nExpected:\n\nVendorID\n0\n1\n2\n3\n.\n.\n.\n\n"
"I am trying to create tags out of dummy variables in my dataset. I have created a column &quot;Tags_col&quot; and everytime my nested for-loop iterates over every row, if there is a 1 for a certain category, I would like that category to be included in a list in the tags_col for every row.\nSomething like this:\nDog   Cat   Rabbit   Tags_col\n 0     1      1      ['Cat','Rabbit']\n 1     0      0      ['Dog']\n\nSo far I have this:\nfor x in range(len(df)):\n   for col in df.columns:\n       if df.loc[x,col] == 1:\n           df.loc[x, &quot;Tags_col&quot;] = col\n\nHowever, this is only attaching the first category the for-loop finds in the Tags_col.\nThank you.\n"
'I have a pandas dataframe with dimensions 89 rows by 13 columns. I want to remove an entire row if NaN appears within the first five columns. Here is an example.\nLotName     C15   C16  C17  C18  C19 Spots15 Spots16 ...\nCherry St   439   464  555  239  420     101     101 ...\nSpringhurst NaN   NaN  NaN  NaN  NaN      12      12\nBarton Lot   34    24   43   45   39      10       9 ...\n\nIn the above example, I would want to remove the Springhurst observation, as it contains NaN within the first five columns. How would I be able to do this in Python?\n'
"I have 32 separate html files with data in a table like format containing 8 columns of data.  Each file is for a certain species of fungi.\nI need to convert the 32 html files into 32 csv files with the data. I have the script for a single file, but can't figure out how to systematically do this with a few commands for all 32 files, instead of running the command I have 32 times.\nHere is the script I am using in an attempt to make it loop through all 32 files:\ndirectory = r'../html/species'\ndata = []\nfor filename in os.listdir(directory):\n    if filename.endswith('.html'):\n        fname = os.path.join(directory,filename)\n        with open(fname, 'r') as f:\n            soup = BeautifulSoup(f.read(),'html.parser')\n            HTML_data = soup.find_all(&quot;table&quot;)[0].find_all(&quot;tr&quot;)[1:] \n            for element in HTML_data: \n                sub_data = [] \n                for sub_element in element: \n                    try: \n                        sub_data.append(sub_element.get_text())\n                    except: \n                        continue\n                data.append(sub_data) \ndata\n\nHere is some output data from the script above simplified for replication purposes:\n[['\\n\\t\\t\\t\\t\\t\\tAfrica\\n\\t\\t\\t\\t\\t'],\n ['Kenya',\n  'Present',\n  '',\n  'Introduced',\n  '',\n  '',\n  'Shomari (1996); Ohler (1979); Mniu (1998); Nayar (1998)',\n  ''],\n ['Malawi',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Malawi, Ministry of Agriculture (1990)',\n  ''],\n ['Mozambique',\n  'Present',\n  '',\n  'Introduced',\n  '',\n  '',\n  'Ohler (1979); Shomari (1996); Mniu (1998); CABI (Undated)',\n  ''],\n ['Nigeria',\n  'Present',\n  '',\n  'Introduced',\n  '',\n  '',\n  'Ohler (1979); Shomari (1996); Nayar (1998); CABI (Undated)',\n  ''],\n ['South Africa', 'Present', '', '', '', '', 'Swart (2004)', ''],\n ['Tanzania',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Casulli (1979); Martin et al. (1997)',\n  ''],\n ['Zambia',\n  'Present',\n  '',\n  'Introduced',\n  '',\n  '',\n  'Ohler (1979); Shomari (1996); Mniu (1998); Nayar (1998)',\n  ''],\n ['\\n\\t\\t\\t\\t\\t\\tAsia\\n\\t\\t\\t\\t\\t'],\n ['India', 'Present', '', 'Introduced', '', '', 'Intini (1987)', ''],\n ['\\n\\t\\t\\t\\t\\t\\tSouth America\\n\\t\\t\\t\\t\\t'],\n ['Brazil', 'Present', '', '', '', '', 'Ponte (1986)', ''],\n ['-Sao Paulo',\n  'Present',\n  '',\n  'Native',\n  '',\n  '',\n  'Waller et al. (1992); Shomari (1996)',\n  ''],\n ['\\n\\t\\t\\t\\t\\t\\tAfrica\\n\\t\\t\\t\\t\\t'],\n ['Egypt',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Ethiopia',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Libya',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Malawi',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Morocco',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Mozambique',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['South Africa',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Sudan',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Tanzania',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Tunisia', 'Present', '', '', '', '', 'Djébali et al. (2009)', ''],\n ['Uganda',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['\\n\\t\\t\\t\\t\\t\\tAsia\\n\\t\\t\\t\\t\\t'],\n ['Afghanistan',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Armenia',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Azerbaijan',\n  'Present',\n  '',\n  '',\n  '',\n  '',\n  'Amano (1986); Braun (1995); Shin HyeonDong (2000); CABI and EPPO (2010)',\n  ''],\n ['Bhutan', 'Present', '', '', '', '', 'CABI and EPPO (2010)', '']]\n\nWhat I think I need is every species to be formatted more like this.. [[info_species1],[info_species1],[info_species1]], [[info_species2],[info_species2],[info_species2]]\nor in my output I need:\n['-Sao Paulo',\n  'Present',\n  '',\n  'Native',\n  '',\n  '',\n  'Waller et al. (1992); Shomari (1996)',\n  '']], # AN EXTRA SQUARE BRACKET RIGHT HERE\n ['\\n\\t\\t\\t\\t\\t\\tAfrica\\n\\t\\t\\t\\t\\t'],\n ['Egypt',\n  'Present',\n\n"
'I have a tab-delimited file where one of the columns has occasional newlines that haven\'t been escaped (enclosed in quotes):\n\n   JOB  REF Comment V2  Other\n1   3   45  This was a small job    NULL    sdnsdf\n2   4   456 This was a large job and I have to go onto a new line, \n    but I didn\'t properly escape so it\'s on the next row whoops!    NULL    NULL        \n3   7   354 NULL    NULL    NULL\n\n# dat &lt;- readLines("the-Dirty-Tab-Delimited-File.txt")\ndat &lt;- c("\\tJOB\\tREF\\tComment\\tV2\\tOther", "1\\t3\\t45\\tThis was a small job\\tNULL\\tsdnsdf", \n"2\\t4\\t456\\tThis was a large job and I have\\t\\t", "\\t\\"to go onto a new line, but I didn\'t properly escape so it\'s on the next row whoops!\\"\\tNULL\\tNULL\\t\\t", \n"3\\t7\\t354\\tNULL\\tNULL\\tNULL")\n\n\nI understand that this might not be possible, but these bad newlines only occur in the one field (the 10thcolumn). I\'m interested in solutions in R (preferable) or python.\n\nMy thoughts were to introduce a regular expression looking for a newline after 10 and only 10 tabs. I started off by using readLines and trying to remove all newlines that occur at the end of a space + word:\n\ndat &lt;- gsub("( [a-zA-Z]*)\\t\\n", "\\\\1", dat)\n\n\nbut it seems difficult to reverse the line structure of readLines. What should I be doing?\n\nEdit: Sometimes two newlines occurr (i.e. where the user has put a blank line between paragraphs in a comment field. An example is below (the desired result is that this should be made into a single row)\n\n140338  28855   WA  2   NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    1   NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    1000    NULL    NULL    NULL    NULL    NULL    NULL    YNNNNNNN    (Some text with two newlines)\n\nThe remainder of the text beneath two newlines  NULL    NULL    NULL    3534a   NULL    email   NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL    NULL\n\n'
"I have a dataframe (df) (orginally from a excel file) and the first 9 rows are like this:\n\n      Control      Recd_Date/Due_Date                Action        Signature/Requester\n0     2000-1703   2000-01-31 00:00:00           OC/OER/OPA/PMS/                 M WEBB\n1           NaN   2000-02-29 00:00:00                       NaN              DATA CORP\n2     2000-1776   2000-01-02 00:00:00            OC/ORA/OE/DCP/                  G KAN\n3           NaN   2000-01-03 00:00:00           OC/ORA/ORO/PNC/              PALM POST\n4           NaN                   NaN  FDA/OGROP/ORA/SE-FO/FLA-                    NaN\n5           NaN                   NaN                DO/FLA-CB/                    NaN\n6     2000-1983   2000-02-02 00:00:00  FDA/OGROP/ORA/CE-FO/CHI-                 M EGAN\n7           NaN   2000-02-03 00:00:00                DO/CHI-CB/   BERNSTEIN LIEBHARD &amp;\n8           NaN                   NaN                       NaN             LONDON LLP\n\n\n\nType(df['Control'][1])=float;\nType(df['Recd_Date/Due_Date'][1])=datetime.datetime;\ntype(df['Action_Office'][1])=float;\nType(df['Signature/Requester'][1])=unicode\n\n\nI want to transform this dataframe (e.g. first 9 rows) to this:\n\n      Control            Recd_Date/Due_Date                           Action                                                            Signature/Requester\n0     2000-1703   2000-01-31 00:00:00,2000-02-29 00:00:00           OC/OER/OPA/PMS/                                                      M WEBB,DATA CORP\n1     2000-1776   2000-01-02 00:00:00,2000-01-03 00:00:00           OC/ORA/OE/DCP/OC/ORA/ORO/PNC/FDA/OGROP/ORA/SE-FO/FLA-DO/FLA-CB/      G KAN,PALM POST\n2     2000-1983   2000-02-02 00:00:00,2000-02-03 00:00:00           FDA/OGROP/ORA/CE-FO/CHI-DO/CHI-CB/                                   M EGAN,BERNSTEIN LIEBHARD &amp; LONDON LLP\n\n\nSo basically:\n\n\nEverytime pd.isnull(row['Control']) (This should be the only if condition) is true then merge this row with the previous row (whose 'control' value is not null). \nAnd for 'Recd_Date/Due_Date' and 'Signature/Requester', add ',' (or '/') between each two values (from two merged rows) (e.g. '2000-01-31 00:00:00,2000-02-29 00:00:00' and 'G KAN,PALM POST')\nFor 'Action', simply merge them without any punctuations added (e.g. FDA/OGROP/ORA/CE-FO/CHI-DO/CHI-CB/)\n\n\nCan anyone help me out pls? This is the code im trying to get it to work:\n\nfor i, row in df.iterrows():\n    if pd.isnull(df.ix[i]['Control_#']):\n       df.ix[i-1]['Recd_Date/Due_Date'] = str(df.ix[i-1]['Recd_Date/Due_Date'])+'/'+str(df.ix[i]['Recd_Date/Due_Date'])\n       df.ix[i-1]['Subject'] = str(df.ix[i-1]['Subject'])+' '+str(df.ix[i]['Subject'])\n       if str(df.ix[i-1]['Action_Office'])[-1] == '-':\n           df.ix[i-1]['Action_Office'] = str(df.ix[i-1]['Action_Office'])+str(df.ix[i]['Action_Office'])\n       else:\n           df.ix[i-1]['Action_Office'] = str(df.ix[i-1]['Action_Office'])+','+str(df.ix[i]['Action_Office'])\n       if pd.isnull(df.ix[i-1]['Signature/Requester']):\n           df.ix[i-1]['Signature/Requester'] = str(df.ix[i-1]['Signature/Requester'])+str(df.ix[i]['Signature/Requester'])\n       elif str(df.ix[i-1]['Signature/Requester'])[-1] == '&amp;':\n           df.ix[i-1]['Signature/Requester'] = str(df.ix[i-1]['Signature/Requester'])+' '+str(df.ix[i]['Signature/Requester'])\n       else:\n           df.ix[i-1]['Signature/Requester'] = str(df.ix[i-1]['Signature/Requester'])+','+str(df.ix[i]['Signature/Requester'])\n       df.drop(df.index[i])\n\n\nHow come the drop() doesn't work? I am trying drop the current row (if its ['Control_#'] is null) so the next row (whose ['Control_#'] is null) can be added to the previous row (whose ['Control_#'] is NOT null) iteratively..\n\nMuch appreciated!! \n"
'I have a bunch of lines with random text, and at the end of each line, a timestamp. I am trying to split these lines right before the timestamp. \n\nCurrent output:\n\nYes, I\'d say so. Nov 08, 2014 UTC\nHell yes! Oct 01, 2014 UTC \nAnbefalt som bare det, løp og kjøp. Sep 16, 2014 UTC\nEtc.\n\n\nDesired output (by "tab" I mean the actual whitespace):\n\nYes, I\'d say so. &lt;tab&gt; Nov 08, 2014 UTC\nHell yes! &lt;tab&gt; Oct 01, 2014 UTC\nAnbefalt som bare det, løp og kjøp. &lt;tab&gt; Sep 16, 2014 UTC\nEtc.\n\n\nSo far I have used "replace" to place a tab character right before the month. Like this:\n\nmy_string.replace("May ", "\\tMay ").replace("Apr ", "\\tApr ").replace("Mar ", "\\tMar ").replace("Feb ", "\\tFeb ") etc. (incomplete code)\n\n\nThis works fairly well, except when the random text involves the name of a month, e.g. "I bought it last may, great stuff". As the date is formatted in such a specific way I\'d like to improve on this with regex and wildcards, if possible. Is there a way to place a tab before these dates? As you can see above, the dates are formatted as follows:\n\n[Three-letter abbreviation of the month] [two-digit day] [,] [four-digit year] [UTC]\n\n\nE.g.\n\nOct 31, 2014 UTC\n\n\nPardon the amateurish code and approach, I am an absolute regex n00b. I have looked around for answers here on SO, but I\'ve come short. I hope someone can help!\n'
'I\'ve pulled some stock data from Quandl for both Crude Oil prices (WTI) and Caterpillar (CAT) price.    When I concatenate the two dataframes together I\'m left with some NaNs.  My ultimate goal is to run a .Pearsonr() to assess the correlation (along with p-values), however I can\'t get Pearsonr() to work because of all the Nan\'s.  So I\'m trying to clean them up.  When I use the .fillNA() function it doesn\'t seem to be working.  I\'ve even tried .interpolate() as well as .dropna().  None of them appear to work. Here is my working code.\n\nimport Quandl\nimport pandas as pd\nimport numpy as np\n\n\n#WTI Data#\nWTI_daily = Quandl.get("DOE/RWTC", collapse="daily",trim_start="1986-10-10", trim_end="1986-10-15")\nWTI_daily.columns = [\'WTI\']\n\n#CAT Data\nCAT_daily = Quandl.get("YAHOO/CAT.6", collapse = "daily",trim_start="1986-10-10", trim_end="1986-10-15")\nCAT_daily.columns = [\'CAT\']  \n\n#Combine Data Frames\ndaily_price_df = pd.concat([CAT_daily, WTI_daily], axis=1)\nprint daily_price_df\n\n#Verify they are dataFrames:\ndef really_a_df(var):\n    if isinstance(var, pd.DataFrame):\n        print "DATAFRAME SUCCESS"\n    else:\n        print "Wahh Wahh"\n    return \'done\'\n\nprint really_a_df(daily_price_df)\n\n#Fill NAs \n#CAN\'T GET THIS TO WORK!!\ndaily_price_df.fillna(method=\'pad\', limit=8)\nprint daily_price_df\n\n# Try to interpolate\n#CAN\'T GET THIS TO WORK!!\ndaily_price_df.interpolate()\nprint daily_price_df\n\n#Drop NAs\n#CAN\'T GET THIS TO WORK!!\ndaily_price_df.dropna(axis=1)\nprint daily_price_df\n\n\nFor what it\'s worth I\'ve managed to get the function working when I create a dataframe from scratch using this code:\n\nimport pandas as pd\nimport numpy as np\n\nd = {\'a\' : 0., \'b\' : 1., \'c\' : 2.,\'d\':None,\'e\':6}\nd_series = pd.Series(d, index=[\'a\', \'b\', \'c\', \'d\',\'e\'])\nd_df =  pd.DataFrame(d_series)\nd_df = d_df.fillna(method=\'pad\')\n\nprint d_df\n\n\nInitially I was thinking that perhaps my data wasn\'t in dataframe form, but I used a simple test to confirm they are in fact dataframe.  The only conclusion I that remains (in my opinion) is that it is something about the structure of the Quandl dataframe, or possibly the TimeSeries nature.  Please know I\'m somewhat new to python so structure answers for a begginner/novice.  Any help is much appreciated!\n'
"I have a dataset with historical data and I want to break it into two sets:\n\n\nThe set of IDs that I have their data for at least two consecutive years.\nIt's complement, i.e, the set of IDs that I have one or more year of data from them but in nonconsecutive years.\n\n\nFor example, let's take data set A:\n\nA =\nID    Year    X   Y\n1     2010    2   3\n1     2012    4   0\n2     2011    4   3\n2     2012    2   2\n3     2010    3   1\n3     2012    2   1\n3     2013    0   3\n\n\nI want to get the set B:\n\nB = \nID    Year    X   Y\n2     2011    4   3\n2     2012    2   2\n3     2012    2   1\n3     2013    0   3\n\nB'=\nID    Year    X   Y\n1     2010    2   3\n1     2012    4   0\n3     2010    3   1\n\n\nNote that ID 3 is shown in both B and B' because it has records of consecutive years and a single year.\n\nI do not have to do this in R, I can use Python as well. Any help would be appreciated.\n"
"suppose i have a dataframe like this :\n\nlst1 = [[1,3,4,5],[1,2,3,3],[2,3,4,5],[3,4,5,5]]\nlst2 = [[1,2,3,1],[1,4,1,2],[3,3,1,5],[2,4,1,5]]\nlst3 = [[1,2,3,3],[3,2,1,2],[1,3,1,4],[2,4,3,5]]\npercentile_list = pd.DataFrame({'lst1Tite' : lst1,\n 'lst2Tite' : lst2,\n 'lst3Tite':lst3\n})\n\n&gt; precentile_list    \n        lst1Tite    lst2Tite    lst3Tite\n0   [1, 3, 4, 5]    [1, 2, 3, 1]    [1, 2, 3, 3]\n1   [1, 2, 3, 3]    [1, 4, 1, 2]    [3, 2, 1, 2]\n2   [2, 3, 4, 5]    [3, 3, 1, 5]    [1, 3, 1, 4]\n3   [3, 4, 5, 5]    [2, 4, 1, 5]    [2, 4, 3, 5]\n\n\nNow I want to extract row 0, and turn row 0 as a dataframe like this:\n\n&gt; percentile_0\ncol1    col2    col3    col4\n0   1   3   4   5\n1   1   2   3   1\n2   1   2   3   3\n\n\nHow can i do that?\n\nAnd what if I want to turn precentile_list to a dataframe like percentile_0 ?\n"
"Given the following code:\n\nimport numpy as np\nimport pandas as pd\n\narr = np.array([\n    [1,2,9,1,1,1],\n    [2,3,3,1,0,1],\n    [1,4,2,1,2,1],\n    [2,3,1,1,2,1],\n    [1,2,3,1,8,1],\n    [2,2,5,1,1,1],\n    [1,3,8,7,4,1],\n    [2,4,7,8,3,3]\n    ])\n#    1,2,3,4,5,6 &lt;- Number of the columns.\ndf = pd.DataFrame(arr)\n\nfor _ in df.columns.values:\n    print {x: list(df[_]).count(x) for x in set(df[_])}\n\n\nI want to delete from the dataframe all the columns in which one value occurs more often than all the other values of the column together. In this case I would like to drop the columns 4 and 6 (see comment) since the number 1 occurs more often than all the other numbers in these columns together (6 > 2 in column 4 and 7 > 1 in column 6). I don't want to drop the first column (4 = 4). How would I do that?\n"
"I have a column in my dataframe that is formatted like an index: \n\n0      [u'Basketball', u'Swimming', u'Gym']\n1      [u'Gym', u'Soccer', u'Football']\n2      [u'Ballet', u'Basketball', u'Volleyball']\n\n\nIs there an easy way for me to clean this up (remove the u, and the square brackets) then  split them by (',') such that sports are grouped to three columns?\n"
'Following is my data log\n\n30/10/2016 17:18:51 [13] 10-Full: L 1490; A 31; F 31; S 31; DL 0; SL 0; DT 5678\n30/10/2016 17:18:51 [13] 00-Always: Returning 31 matches\n30/10/2016 17:18:51 [13] 30-Normal: Query complete\n30/10/2016 17:18:51 [13] 30-Normal: Request completed in 120 ms.\n30/10/2016 17:19:12 [15] 00-Always: Request from 120.0.0.1\n30/10/2016 17:19:12 [15] 00-Always: action=Query&amp;Text=(("XXXXXX":*/DOCUMENT/DRECONTENT/ObjectInfo/type+OR+"XXXXXX":*/DOCUMENT/.....\n30/10/2016 17:19:12 [15] 10-Full: L 2; A 1; F 1; S 0; DL 0; SL 0; DT 5373\n30/10/2016 17:19:12 [15] 00-Always: Returning 0 matches\n30/10/2016 17:19:12 [15] 30-Normal: Query complete\n30/10/2016 17:19:12 [15] 30-Normal: Request completed in 93 ms.\n30/10/2016 17:19:20 [17] 00-Always: Request from 120.0.0.1\n30/10/2016 17:19:20 [17] 00-Always: action=Query&amp;Text=((PDF:*/DOCUMENT/DRECONTENT/XXXXX/type+AND+XXXXXX.......\n30/10/2016 17:19:51 [19] 10-Full: L 255; A 0; F 0; S 0; DL 0; SL 0; DT 5021\n30/10/2016 17:19:51 [19] 00-Always: Returning 0 matches\n30/10/2016 17:19:51 [19] 30-Normal: Query complete\n30/10/2016 17:19:51 [19] 30-Normal: Request completed in 29 ms.\n30/10/2016 17:20:44 [27] 00-Always: Request from 120.0.0.1\n30/10/2016 17:20:44 [27] 00-Always: action=Query&amp;Tex(Image:*/DOCUMENT/DRECONTENT/ObjectInfo/type+AND+(\n30/10/2016 17:20:44 [27] 10-Full: L 13; A 0; F 0; S 0; DL 0; SL 0; DT 5235\n30/10/2016 17:20:44 [27] 00-Always: Returning 0 matches\n30/10/2016 17:20:44 [27] 30-Normal: Query complete\n30/10/2016 17:20:44 [27] 30-Normal: Request completed in 27 ms.\n30/10/2016 17:21:09 [25] 00-Always: Request from 120.0.0.1\n30/10/2016 17:21:09 [25] 00-Always: action=Query&amp;Text=XXXXXX:*/DOCUMENT/DRECONTENT/ObjectIn\n\n\nThis is my data set. There are millions of them. I would like to analyze how long a query took, by whom it came from and how the request looks. The rest I want to hide. \n\nMy Expected Output:\n\n30/10/2016;17:19:12;Request completed in 93 ms.;Request from 120.0.0.1;action=Query&amp;Text=((PDF:*/DOCUMENT/DRECONTENT/XXXXX....\n30/10/2016;17:18:51;Request completed in 120 ms.;Request from 120.0.0.1;action=Query&amp;Text=(("EOM.CompoundStory":*/DOCUMENT/DRECONTE....\n30/10/2016;17:19:51;Request completed in 29 ms.;Request from 120.0.0.1;action=Query&amp;Text=(Image:*/DOCUMENT/DRECONTENT/ObjectInfo/type+AND+((.....\n30/10/2016;17:20:44;Request completed in 27 ms.;Request from 120.0.0.1;action=Query&amp;Text=XXXXX:*/DOCUMENT/DRECONT....\n\n\nI\'d like to solve it in python with pandas if possible. I already have one approach:\n\nimport csv\nimport pandas\nwith open(\'query.csv\', \'rt\') as f, open(\'leertest.csv\', \'w\') as outf:\n    reader = csv.reader(f, delimiter=\' \')\n    writer = csv.writer(outf, delimiter=\';\', quoting=csv.QUOTE_MINIMAL)\n    for row in reader:\n        for field in row:\n            if field == "Request":\n                    print row\n\n\nBut unfortunately without success. Maybe you have a better approach.\n\nI also like to look at new technologies, which do not take long to be learned.\n'
'Suppose I have the followin dataframe:\n\nKey | Amount | Term   | Other | Other_2\n----+--------+--------+-------+--------\nA   |   9999 | Short  | ABC   | 100\nA   |    261 | Short  | ABC   | 100\nB   |    281 | Long   | CDE   | 200\nC   |    140 | Long   | EFG   | 300\nC   |   9999 | Long   | EFG   | 300\n\n\nThe desired output should be:\n\nKey | Amount | Term   | Other | Other_2\n----+--------+--------+-------+--------\nA   |    261 | Short  | ABC   | 100\nB   |    281 | Long   | CDE   | 200\nC   |    140 | Long   | EFG   | 300\n\n\nThat is, to take the min of the "Amount" column while retaining the rest of the values in the row with the min value.\n\nI think this can be done with a groupby() but I don\'t visualize how.\n\nEDIT: I removed the commas, my data is numeric\n'
"I have a pandas dataframe in the following format:\n\nimport pandas as pd\n\nd1 = {'Product ID': ['A','B','C','D','A','D','E','A','B','C','B','C','E'], \n'Buyer ID': [1,1,1,1,2,2,2,3,3,3,4,5,5]}\ndf1 = pd.DataFrame(d1)\n\n\nwhich is of the format: \n\nProduct ID  Buyer ID\nA             1\nB             1\nC             1\nD             1\nA             2\nD             2\nE             2\nA             3\nB             3\nC             3\nB             4\nC             5\nE             5\n\n\nThe dataframe shows the products purchased by an individual over time. \n\nWhat I would like to do is to obtain the first and last products purchased by an individual and a count of the number of products bought between the first and last purchase. In my example Buyer 1, purchased 4 products in all and his first purchase was Product A and last purchase was product D (Full expected results table provided at the end). In case an individual purchased 1 product only, the resultant count would be 1 with the product listed.\n\nThe result I want to obtain is of this format:\n\nProduct ID  Buyer ID    Count\n    A           1         4\n    D           1         4\n    A           2         3\n    E           2         3\n    A           3         3\n    C           3         3\n    B           4         1\n    C           5         2\n    E           5         2\n\n\nI could not get my head around to solving this. Could someone help?\n"
"My timestamp looks like below in the dataframe of my column but it is in 'object'. I want to convert this into 'timestamp'. How can I convert all values such in my dataframe column into timestamp?\n\n0    01/Jul/1995:00:00:01\n1    01/Jul/1995:00:00:06\n2    01/Jul/1995:00:00:09\n3    01/Jul/1995:00:00:09\n4    01/Jul/1995:00:00:09\nName: timestamp, dtype: object\n\n\nI tried below code referring this stackoverflow post but it gives me error:\n\npd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S.%f')\n\n\nBelow is the error:\n\nValueError: time data '01/Jul/1995:00:00:01' does not match format '%d/%b/%Y:%H:%M:%S.%f' (match)\n\n"
"I have a large dataset with some x rows and y number of columns. one of the columns as words and some unwanted data. That unwanted data is has no specific pattern hence I am finding it difficult to remove that from the dataframe. \n\nnonhashtag\n['want', 'better', 'than', 'Dhabi,', 'United', 'Arab', 'Emirates']\n['Just', 'posted', 'photo', 'Rasim', 'Villa']\n['Dhabi', 'International', 'Airport', '(AUH)', '\\xd9\\x85\\xd8\\xb7\\xd8\\xa7\\xd8\\xb1', '\\xd8\\xa3\\xd8\\xa8\\xd9\\x88', '\\xd8\\xb8\\xd8\\xa8\\xd9\\x8a', '\\xd8\\xa7\\xd9\\x84\\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd9\\x8a', 'Dhabi']\n['just', 'shrug', 'off!', 'Dubai', 'Mall', 'Burj', 'Khalifa']\n['out!', 'Cowboy', 'steppin', 'Notorious', 'going', 'sleep!', 'Make', 'happen']\n['Buona', 'notte', '\\xd1\\x81\\xd0\\xbf\\xd0\\xbe\\xd0\\xba\\xd0\\xbe\\xd0\\xb9\\xd0\\xbd\\xd0\\xbe\\xd0\\xb9', '\\xd0\\xbd\\xd0\\xbe\\xd1\\x87\\xd0\\xb8', '\\xd9\\x84\\xd9\\x8a\\xd9\\x84\\xd8\\xa9', '\\xd8\\xb3\\xd8\\xb9\\xd9\\x8a\\xd8\\xaf\\xd8\\xa9!', '\\xd8\\xa3\\xd8\\xa8\\xd9\\x88', '\\xd8\\xb8\\xd8\\xa8\\xd9\\x8a', 'Viceroy', 'Hotel,', 'Yas\\xe2\\x80\\xa6']\n\n\nEvery character which is not a word is to be removed this is only one column in the large dataset. Column name is nonhashtag\n\nWhat is the simple way to clean the column. straight away remove them or replace with NAN\n\nExpected output \n\nnonhashtag\n    ['want', 'better', 'than', 'Dhabi,', 'United', 'Arab', 'Emirates']\n    ['Just', 'posted', 'photo', 'Rasim', 'Villa']\n    ['Dhabi', 'International', 'Airport', '(AUH)', 'Dhabi']\n    ['just', 'shrug', 'off!', 'Dubai', 'Mall', 'Burj', 'Khalifa']\n    ['out!', 'Cowboy', 'steppin', 'Notorious', 'going', 'sleep!', 'Make', 'happen']\n    ['Buona', 'notte', 'Viceroy', 'Hotel,']\n\n\nEvery [] is one row in that particular column so removing of only the \\x and remaining characters is needed the empty [] should be left in the row. Keeping the row is important as other column's that row is filled with needed information. \n\nTo write a proper code I couldn't get pass through the input read as I am not able to find a pattern in the dataset to write a regex. \n\nThanks in advance for the help \n"
'I\'ve written a script in python to scrape some text out of some html elements. The script I\'ve written can parse it. However, the problem is the data are getting parsed with a huge spaces between them. I tried with .strip() method but it didn\'t have any effect on the result. How can I fix it?\n\nThe html elements:\n\nhtml="""\n&lt;div class="organisation-details"&gt;\n\n    &lt;div class="personnel shaded"&gt;\n                        &lt;h3&gt;KEY PERSONNEL&lt;/h3&gt;\n                        &lt;p&gt;\n                                Director: Andrew Bickerton&lt;br&gt;\n                                Director: Andrew Connor&lt;br&gt;\n                                Office Manager: Tom Marchant&lt;br&gt;\n                        &lt;/p&gt;\n                    &lt;/div&gt;\n\n    &lt;div class="company-type shaded"&gt;\n                        &lt;h3&gt;COMPANY TYPE&lt;/h3&gt;\n                        &lt;p&gt;\n                                                        Importer\n                        &lt;/p&gt;\n                    &lt;/div&gt;\n\n    &lt;div class="company-details shaded"&gt;\n                        &lt;h3&gt;COMPANY DETAILS&lt;/h3&gt;\n                        &lt;p&gt;\n                                Year Established: 1984 &lt;br&gt;\n                                                        VAT No: GB 413 3611 93&lt;br&gt;\n                                                        No of Employees: 1-20&lt;br&gt;\n                        &lt;/p&gt;\n                    &lt;/div&gt;\n\n\n&lt;/div&gt;\n"""\n\n\nThis script:\n\nfrom lxml.html import fromstring\n\ntree = fromstring(html)\nfor title in tree.cssselect(".organisation-details"):\n    key = title.cssselect("h3:contains(\'KEY PERSONNEL\')+p")[0].text_content().strip()\n    details = title.cssselect("h3:contains(\'COMPANY DETAILS\')+p")[0].text_content().strip()\n    ctype = title.cssselect("h3:contains(\'COMPANY TYPE\')+p")[0].text_content().strip()\n    print(key,details,ctype)\n\n\nThe output I\'m having:\n\nDirector: Andrew Bickerton\n                                Director: Andrew Connor\n                                Office Manager: Tom Marchant Year Established: 1984 \n                                                        VAT No: GB 413 3611 93\n                                                        No of Employees: 1-20 Importer\n\n\nThe result I\'m after (or anything closer):\n\nDirector: Andrew Bickerton\nDirector: Andrew Connor\nOffice Manager: Tom Marchant \nYear Established: 1984 \nVAT No: GB 413 3611 93\nNo of Employees: 1-20\nImporter\n\n'
'I have a column in my DataFrame called "amenities" and this is what 1 records looks like:\n\nprint(df["amenities"][0])\n\n{"Wireless Internet",Kitchen,"Free parking on premises",Breakfast,Heating,Washer,Dryer,"Smoke detector","Carbon monoxide detector",Essentials,Shampoo,Hangers,"Hair dryer","Laptop friendly workspace","translation missing: en.hosting_amenity_49","translation missing: en.hosting_amenity_50"}\n\n\nWhat I\'m trying to do is remove the special characters and then I want to separate them so that every amenity has its own column\n\nRoom       Amenity1     Amenity2    Amenity3    Amenity4\n\n  1 Wireless Internet   Kitchen   Free Parking  Breakfast\n\n\nWhat I did is:\n\nimport re\n\ndf[\'amenities\'] = df[\'amenities\'].map(lambda x:re.sub(\'\\W+\',\' \', x))\n\n\n\n\nWireless Internet Air conditioning Pool Kitchen Free parking on premises Gym Hot tub Indoor fireplace Heating Family kid friendly Suitable for events Washer Dryer Smoke detector Carbon monoxide detector First aid kit Fire extinguisher Essentials Shampoo Lock on bedroom door 24 hour check in Hangers Hair dryer Iron Laptop friendly workspace\n.\n\n\nThis cleans the string but now I do not know how to separate them into its own columns because Wireless Internet should be onle column, not two.\n'
"here is my first question: I have a pandas DataFrame that looks like the following:\n\nPandas Dataframe\n\nThe DataFrame shown in the picture is simplified (its original shape is [1195674 x 11]) but it doesn't matter.\n\nI am trying to set the left hand side (columns A, B, C, D) as index and the right hand side (columns E, F, G) as corresponding values (look at the colors).\n\nDo you have any hint?\n\nP.S. another way could be to replicate the left hand side (rows) in order to have something like the following (the red rows should be cancelled out):\n\nSecond Solution\n\nCan you propose any other way to accomplish this task?\n\nMany thanks in advance!\n\nFranco\n"
"I have a dataframe like the following:\n\nplan type  hour status     code\nA    cont   0    ok       010.0\nA    cont   2    ok      025GWA\nA    cont   0    notok   010VVT\nA    cont   0    other     6.05\nA    vend   1    ok        6.01\n\n\nThe column code has a few string characters with different letters. In the end I would like to transform the 'code' column to float.\nI tried: \n\ndf['code'] = df['code'].str.extract('(\\d+)').astype(float)\n\n\nbut with this I got: \n\nplan type  hour status     code\nA    cont   0    ok        10.0\nA    cont   2    ok        25.0 \nA    cont   0    notok     10.0\nA    cont   0    other      6.0\nA    vend   1    ok         6.0\n\n\nHow can I get a result like the following?\n\nplan type  hour status     code\nA    cont   0    ok       10.00\nA    cont   2    ok       25.00\nA    cont   0    notok    10.00\nA    cont   0    other     6.05\nA    vend   1    ok        6.01\n\n"
'I\'m a complete beginner at programming and to StackOverflow and I just need to do some basic web-scraping from a TripAdvisor page and clean some useful information from it. Display it nicely etc. I\'m trying to isolate the title of the cafe, the number of ratings and the rating itself. I\'m thinking I might need to convert it to text and use regex or something? I really don\'t know. An example of what I mean would be:\n\nOutput:\n\nCoffee Cafe, 4 out of 5 bubbles, 201 reviews.\n\nSomething like that. I will put my code so far below, any help I could get would amazing and I would be infinitely grateful. Cheers.\n\nfrom bs4 import BeautifulSoup\n\ndef get_HTML(url):\n    response = urllib.request.urlopen(url)\n    html = response.read()\n    return html\n\n\nTripadvisor_reviews_HTML=get_HTML(\n\'https://www.tripadvisor.com.au/Restaurants- \n g255068-c8-Brisbane_Brisbane_Region_Queensland.html\')\n\n\ndef get_review_count(HTML):\n    soup = BeautifulSoup(Tripadvisor_reviews_HTML, "lxml")\n    for element in soup(attrs={\'class\' : \'reviewCount\'}):\n        print(element)\n\nget_review_count(Tripadvisor_reviews_HTML)\n\ndef get_review_score(HTML):\n    soup = BeautifulSoup(Tripadvisor_reviews_HTML, "lxml")\n    for four_point_five_score in soup(attrs={\'alt\' : \'4.5 of 5 bubbles\'}):\n        print(four_point_five_score)\n\n\nget_review_score(Tripadvisor_reviews_HTML)\n\ndef get_cafe_name(HTML):\n    soup = BeautifulSoup(Tripadvisor_reviews_HTML, "lxml")\n    for name in soup(attrs={\'class\' : "property_title"}):\n        print(name)\n\n\n\nget_cafe_name(Tripadvisor_reviews_HTML)\n\n'
"I would like to replace nan's in a list with values from a second list which has exactly same number of elements as the number of nan's in the first list.\n\nmylist1 = list([1, 2, 3, np.nan, np.nan, 4, np.nan, 5])\n\nmylist2 = list([-10, -11, -12])\n\n\nWhat I want is:\n\nmylist1 = [1, 2, 3, -10, -11, 4, -12, 5] \n\n"
'I am newbie here so please forgive for any mistakes. \nI am trying to work on adult census dataset. I am finding it hard to remove the question marks in the dataset. \n\nLink to the dataset :- https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n\nI have also tried the 1st answer in the given link:- Drop rows with a &#39;question mark&#39; value in any column in a pandas dataframe\n\nBut I am getting an error \n\n~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py in wrapper(self, other, axis)\n   1251 \n   1252             with np.errstate(all=\'ignore\'):\n-&gt; 1253                 res = na_op(values, other)\n   1254             if is_scalar(res):\n   1255                 raise TypeError(\'Could not compare {typ} type with Series\'\n\n~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py in na_op(x, y)\n   1164                     result = method(y)\n   1165                 if result is NotImplemented:\n-&gt; 1166                     raise TypeError("invalid type comparison")\n   1167             else:\n   1168                 result = op(x, y)\n\nTypeError: invalid type comparison\n\n\nPlease tell me how to solve this issue. I am using Python 3.6\n\nThank You!!\n\nEdit 1:- This is also called Census Income Dataset.\n'
'i am currently learning pandas and have an issue cleaning my Dataframe:\n\n"TIMESTAMP","RECORD","WM1_u_ms","WM1_v_ms","WM1_w_ms","WM2_u_ms","WM2_v_ms","WM2_w_ms","WS1_u_ms","WS1_v_ms"\n"2018-04-06 14:31:11.5",29699805,2.628,4.629,0.599,3.908,7.971,0.47,2.51,7.18\n"2018-04-06 14:31:11.75",29699806,3.264,4.755,-0.095,2.961,6.094,-0.504,2.47,7.18\n"2018-04-06 14:31:12",29699807,1.542,5.793,0.698,4.95,4.91,0.845,2.18,7.5\n"2018-04-06 14:31:12.25",29699808,2.527,5.207,0.012,4.843,6.285,0.924,2.15,7.4\n"2018-04-06 14:31:12.5",29699809,3.511,4.528,1.059,2.986,5.636,0.949,3.29,5.54\n"2018-04-06 14:31:12.75",29699810,3.445,3.957,-0.075,3.127,6.561,0.259,3.85,5.45\n"2018-04-06 14:31:13",29699811,2.624,5.238,-0.166,3.451,7.199,0.242,3.94,6.24\n\ndf = pd.read_csv(FilePath,parse_dates=True)  #read the csv file and save it into a variable\ndf = df.drop([\'RECORD\'],axis=1)\n\n\n \n\nI do not understand why pandas recognizes parts as float64 and others as object. Do you guys have any clue?\nBecause of this, i started trying to convert the columns on my own:\n\ndf[\'TIMESTAMP\'] = pd.to_datetime(df[\'TIMESTAMP\'])\ndf[\'WM1_u_ms\':] = df.iloc[:, df.columns != \'TIMESTAMP\'].values.astype(float)\n\n\nBut i get an error: \n\ncannot do slice indexing on &lt;class \'pandas.core.indexes.range.RangeIndex\'&gt; with these indexers [WM1_u_ms] of &lt;class \'str\'&gt;\n\n\nWhy does pandas cant read the .dat file correct from the start and what is my fault converting it. In the next stemp i want to interpolate via df.interpolate() to clear the nan\'s\n\nthanks for any help!\n'
"I have a pandas dataframe with the following categorical variables as columns on the left, and their specific realizations on the right,\n\n\n\n(apologies for low-res).\n\nFor a statistical regression, I want to label all of these categorical variables, so, for example, in LotShape, Reg becomes 0, IR1 becomes 1, IR2 2, and IR3 3. I found that scikit-learn's LabelEncoder can do the job, but there's a problem. Some of these categorical variables are implicitly ordinal, and 0, 1, ... need to be assigned to the right labels, and LotShape only happens to be in order there. \n\nSo my question is, how would I efficiently, in some order that I specify, label a large number of categorical variables?\n"
'I know this may seem stupid but I\'ve been looking everywhere and trying with regex and split in vain. My script never works for all type of string I have on my data set.\n\nI have this column that contains raw data that look like (three cases):\n\n20181223-FB-BOOST-AAAA-CC Auchy-Les-Mines - Père Noel\n20161224-FB-BOOST-SSSS-CC LeMarine - XXX XXX\n20161223-FB-BOOST-XXXX-CC Bonjour le monde - Blah blah\n\n\nSo what I want to do is to get the strings in the middle after CC and right before "-". I wrote a script that did work for the 2nd case but never the other two :\n\n1st case: Auchy-Les-Mines\n2nd case: LeMarine\n3rd case: Bonjour le monde\n\n\nHere is the regex that I used but never works for all cases: regex = r"\\s\\b.*-."\n\nThanks in advance !\n'
'I have a long block of text that contains a subtext that I want to remove based on a partial match (90%). \n\nstring = "Adam is a boy who lives in Michigan.  \n        He loves to eat apples and oranges. \n        He also enjoys playing with his dog and cat. \n        Adam is a happy boy."\n\nsubstring = "He loves to apple oranges"\n\n\nAnd I want to return\n\n"Adam is a boy who lives in Michigan.  \n He also enjoys playing with his dog and cat. \n Adam is a happy boy."\n\n\nThe words "eat" and "and" don\'t appear in the substring, but I want to remove the whole sentence "He loves to eat apples and oranges." I\'m not really sure how to do this. Thanks!\n'
'I have a dataframe called df\n\nGender  Country      Comments\nmale    USA        machine learning and fraud detection are a must learn\nmale    Canada     monte carlo method is great and so is hmm,pca, svm and neural net\nfemale  USA        clustering and cloud computing\nfemale  Germany    logistical regression and data management and fraud detection\nfemale  Nigeria    nltk and supervised machine learning\nmale    Ghana      financial engineering and cross validation and time series\n\n\nand a list called algorithms\n\nalgorithms = [\'machine learning\',\'fraud detection\', \'monte carlo method\', \'time series\', \'cross validation\', \'supervised machine learning\', \'logistical regression\', \'nltk\',\'clustering\', \'data management\',\'cloud computing\',\'financial engineering\']\n\n\nSo technically, for each row of the Comments column, I\'m trying to extract words that appear in the algorithms list. \nThis is what I\'m trying to achieve\n\nGender  Country      algorithms\nmale    USA        machine learning, fraud detection \nmale    Canada     monte carlo method, hmm,pca, svm, neural net\nfemale  USA        clustering, cloud computing\nfemale  Germany    logistical regression, data management, fraud detection\nfemale  Nigeria    nltk, supervised machine learning\nmale    Ghana      financial engineering, cross validation, time series\n\n\nHowever, this is what I\'m getting\n\nGender  Country      algorithms\nmale    USA         \nmale    Canada     hmm pca svm  \nfemale  USA        clustering\nfemale  Germany    \nfemale  Nigeria    nltk\nmale    Ghana      \n\n\nwords like machine learning and fraud detection don\'t appear. basically, all 2 grams words\n\nThis is the code I used\n\ndf[\'algorithms\'] = df[\'Comments\'].apply(lambda x: " ".join(x for x in x.split() if x in algorithms)) \n\n'
"I'm trying to add 10 rows based on a merchant_id in a table. This is the original table - \n\nid    email    trend_type\n1    abc@xyz.com \n2    cdsm@kcmd.com\n\n\nAnd this is what I'm trying to create - \n\nid    email    trend_type\n1    abc@xyz.com   Bill \n1    abc@xyz.com   Visits\n1    abc@xyz.com   Avg. Visits\n1    abc@xyz.com   abc \n1    abc@xyz.com   mcd        \n1    abc@xyz.com   mckfd      \n1    abc@xyz.com   mfd        \n1    abc@xyz.com   aps        \n1    abc@xyz.com   mvmv       \n1    abc@xyz.com   dep  \n2    cdsm@kcmd.com Bill\n2    cdsm@kcmd.com Visits    \n.    .....         ...\n.    .....         ...\n\n\nI have 10 different trend types that I want to add to one id and email combination. I've created an array of all the trend types and I've tried using a nested for loop but I haven't been successful. Could really use some help.\n"
"Currently cleaning data from a csv file. Successfully mad everything lowercase, removed stopwords and punctuation etc. But need to remove special characters. For example, the csv file contains things such as 'CÃ©sar' 'â€˜disgraceâ€™'. If there is a way to replace these characters then even better but I am fine with removing them. Below is the code I have so far.\n\nimport pandas as pd\nfrom nltk.corpus import stopwords\nimport string\nfrom nltk.stem import WordNetLemmatizer\n\nlemma = WordNetLemmatizer()\n\npd.read_csv('soccer.csv', encoding='utf-8')\ndf = pd.read_csv('soccer.csv')\n\ndf.columns = ['post_id', 'post_title', 'subreddit']\ndf['post_title'] = df['post_title'].str.lower().str.replace(r'[^\\w\\s]+', '').str.split()\n\n\nstop = stopwords.words('english')\n\ndf['post_title'] = df['post_title'].apply(lambda x: [item for item in x if item not in stop])\n\ndf['post_title']= df['post_title'].apply(lambda x : [lemma.lemmatize(y) for y in x])\n\n\ndf.to_csv('clean_soccer.csv')\n\n"
'I have a number of .xls datasheets which I am looking to clean and merge.\nEach data sheet is generated by a larger system which cannot be changed.\nThe method that generates the data sets displays the selected parameters for the data set. (E.G 1) I am looking to automate the removal of these. \n\nThe number of rows that this takes up varies, so I am unable to blanket remove x rows from each sheet. Furthermore, the system that generates the report arbitrarily merges cells in the blank sections to the right of the information.\n\nCurrently I am attempting what feels like a very inelegant solution where I convert the file to a CSV, read it as a string and remove everything before the first column. \n\ndata_xls = pd.read_excel(\'InputFile.xls\', index_col=None)\ndata_xls.to_csv(\'Change1.csv\', encoding=\'utf-8\')\n\nwith open("Change1.csv") as f:\n    s = f.read() + \'\\n\'\n\na=(s[s.index("Col1"):])\ndf = pd.DataFrame([x.split(\',\') for x in a.split(\'\\n\')])\n\n\nThis works but it seems wildly inefficient:\n\n\nMultiple format conversions\nReading every line in the file when the only rows being altered occur within first ~20\nDataframe ends up with column headers shifted over by one and must be re-aligned (Less concern)\n\n\nWith some of the files being around 20mb, merging a batch of 8 can take close to 10 minutes.\n'
'Given a large text file in French (>200GB) encoded in UTF-8 and normalised by unicode NFC, I want to remove all special characters except accented/unaccented alphabetical letters, numbers and punctuations using Python or Bash or whichever method that is faster. Previously, I do this task manually by scanning the text to identify if there is any special characters that I don\'t want and remove them using character codes like this:\n\ndef remove_special_chars(text):\n\n    text = re.sub(chr(65533), \'\', text)\n    text = re.sub(chr(9658), \'\', text) \n    text = re.sub(chr(9660), \'\', text)\n    text = re.sub(chr(169), \'\', text)  \n\n    return text\n\n\n\n\n  � (char code 65533) ► (char code 9658) ▼ (char code 9660) © (char code 169) etc.\n\n\nHowever, for a large text file, it is not possible to do it that way anymore. Therefore, I am thinking of removing all of the special characters by checking if a character is an (accented/unaccented) alphabetical letter or a number or a punctuation and removing if it is not. I tried the following but the command line does not execute.\n\ngrep -P -v \'[^a-zA-Z0-9 àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔŒÙÛÜŸÇ!"#\\$%&amp;\\\'\\(\\)\\*\\+,\\\\-\\./:;&lt;=&gt;\\?@\\[\\]\\^_`\\{\\|\\}\\~]\' file\n\n\nCould you please help me on this problem? Thank you in advance for your help!\n'
"In my dataframe, I get a '2' written over my index column's name.\nwhen I check for the columns name it doesn't show up there but as\ndf.columns give this as output. I don't know how to remove that '2' from my dataset.\n\n\nI have tried removing index name but it hasn't solved my issue.\n\n df.columns ==&gt; Output\n     Index(['name', 'census 1981', 'census 1998', 'estimate 2000',\n       'calculation 2010', 'annual growth', 'latitude', 'longitude',\n       'parent division', 'name variants'],\n      dtype='object', name=2)\n\n\nI expect only the index with its name...not including that freaking '2' over it\n"
"Would anyone have any tips to clean text data? The data I have is in a list (master_list) and I am trying to create a loop or function that would remove extra [] symbols as well as a None, or None so basically the data in master_list would just be strings separated by a ,\n\nAny help greatly appreciated..\n\nmaster_list = [['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.3.', 'the supply fan is running, the VFD speed output mean value is 94.3.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.2.', 'the supply fan is running, the VFD speed output mean value is 94.2.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.1.', 'the supply fan is running, the VFD speed output mean value is 94.1.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.0.', 'the supply fan is running, the VFD speed output mean value is 94.0.'], None, ['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 93.9.', 'the supply fan is running, the VFD speed output mean value is 93.9.'], None]\n\n"
"I have a file that lists deposit balances as strings. IN order to plot these numbers, I'm trying to convert the Objects to a float. So I wrote code to remove the $ and to take out spaces before and after the values.\n\nmember_clean.TotalDepositBalances = member_clean.TotalDepositBalances.str.replace('$', '')\n\nmember_clean['TotalDepositBalances'] = member_clean['TotalDepositBalances'].str.strip()\n\nmember_clean['TotalDepositBalances'] = member_clean['TotalDepositBalances'].astype(float)\n\n\nWhen I run the code, I get an error message that says\n\nValueError: could not convert string to float: \n\nThat's it. Before I added the str.strip, the error message showed me that some values had spaces before and after, so I knew to remove those. But I'm a little confused what else is causing it,\n\nI looked at the values of the column after I removed the spaces and $, and everything looks normal. Here's a sample.\n\n\n309.00\n38.00\n12,486.00\n6,108.00\n2,537.00\n\n\nAny ideas of what I could check for in the columns that may be causing this error\n"
'I have a csv file looks like this\n\nyear,gender,age,country\n2002,F,9-10,CO\n2002,F,9-10,CO\n2002,M,9-10,CO\n2002,F,9-10,BR\n2002,M,11-15,BR\n2002,F,11-15,CO\n2003,F,9-10,CO\n2003,M,9-10,CO\n2003,F,9-10,BR\n2003,M,9-10,CO\n2004,F,11-15,BR\n2004,F,11-15,CO\n2004,F,9-10,BR\n2004,F,9-10,CO\n\n\nAnd I want to get a output file like this:\n\nyear,gender,age,country,population\n2002,F,9-10,CO,2\n2002,M,9-10,CO,1\n2002,F,9-10,BR,1\n2002,M,9-10,BR,0\n2002,F,11-15,CO,1\n2002,M,11-15,CO,0\n2002,F,11-15,BR,0\n2002,M,11-15,BR,1\n2003,F,9-10,CO,1\n2003,M,9-10,CO,1\n2003,F,9-10,BR,1\n2003,M,9-10,BR,0\n2003,F,11-15,CO,0\n2003,M,11-15,CO,0\n2004,F,9-10,CO,1\n2004,M,9-10,CO,0\n2004,F,9-10,BR,1\n2004,M,9-10,BR,0\n2004,F,11-15,CO,1\n2004,M,11-15,CO,0\n2004,F,11-15,BR,1\n2004,M,11-15,BR,0\n\n\nBasically I want to print out the number of female for each year,each age and each country, so year,gender,age and country will be the key of the dictionary. Moreover, some year do not have the data of a specific country or some year do not have a specific age for a specific country. For example, year 2003,female do not have data for 11-15 age group in country CO. In this situation, the population will be 0. Moreover, some year do not have a specific gender data at all. For example, for year 2004, there is no male data for all the age and country, but I still want to print it out in the output file with population 0. \n\nBelow are some python code I wrote but it doesn\'t work and I don\'t know how to deal with the missing data and print it out as 0 in the population field. \n\nimport csv\nimport os\nimport sys\nfrom operator import itemgetter, attrgetter\nimport math\nfrom collections import Counter\n\n# Create dictionary to hold the data\nvalDic = {}\n\n# Read data into dictionary\nwith open(sys.argv[1], "r",) as inputfile:\n    readcsv = csv.reader(inputfile, delimiter = \',\')    \n    next(readcsv)\n    for line in readcsv:\n        key = line[0] + line[1] + line[2] + line[3]\n        year = line[0]\n        gender = line[1]\n        age = line[2]\n        country = line[3]\n        if key in valDic:\n            key = key + 1\n        else:\n            valDic[key] = [year, gender, age, country, 0] # 0s are placeholder for running sum and itemCount\n    inputfile.close()  \n\nnewcsvfile = []\n\nfor key in valDic:\n    newcsvfile.append([valDic[key][0], valDic[key][1], valDic[key][2], valDic[key][3], len(valDic[key])])\n\nnewcsvfile = sorted(newcsvfile)\nnewcsvfile = [["year", "gender", "age", "country", "population"]] \n\nwith open(sys.argv[2], "w") as outputfile:\n    writer = csv.writer(outputfile)\n    writer.writerows(newcsvfile)        \n\n'
'I have a csv with 70 columns. The 60th column contains a value which decides wether the record is valid or invalid. If the 60th column has 0, 1, 6 or 7 it\'s valid. If it contains any other value then its invalid.\n\nI realised that this functionality wasn\'t possible relying completely on changing property\'s of processors in Apache NiFi. Therfore I decided to use the executeScript processor and added this python code as the text body.\n\nimport csv\n\nvalid =0\ninvalid =0\ntotal =0\nfile2 = open("invalid.csv","w")\nfile1 = open("valid.csv","w")\n\nwith  open(\'/Users/himsaragallage/Desktop/redder/Regexo_2019101812750.dat.csv\') as f:\n    r = csv.reader(f)\n    for row in f:\n        # print row[1]\n        total +=1\n\n        if row[59] == "0" or row[59] == "1" or row[59] == "6" or row[59] == "7":\n            valid +=1\n            file1.write(row)\n        else:\n            invalid += 1\n            file2.write(row)\nfile1.close()\nfile2.close()\nprint("Total : " + str(total))\nprint("Valid : " + str(valid))\nprint("Invalid : " + str(invalid))\n\n\nI have no idea how to use a session and code within the executeScript processor as shown in this question. So I just wrote a simple python code and directed the valid and invalid data to different files. This approach I have used has many limitations.\n\n\nI want to be able to dynamically process csv\'s with different filenames.\nThe csv which the invalid data is sent to, must also have the same filename as the input csv. \nThere would be around 20 csv\'s in my redder folder. All of them must be processed in one go.\n\n\nHope you could suggest a method for me to do the following. Feel free to provide me with a solution by editing the python code I have used or even completely using a different set of processors and totally excluding the use of ExecuteScript Processer\n'
"I have two pandas dataframes. They have the same winner and loser name pairs but their are in different orders in the two dataframes. \n\nDF1 \n\n     Winner       Loser         RankW   RankL\n\n0    Fleishman Z.  Calleri A.   170.0   26.0\n1    Roddick A.   Tsonga J.W.   7.0     212.0\n2    Gasquet R.   Volandri F.   17.0    45.0\n\n\nDF2\n\n     Winner       Loser         WHand   LHand\n\n0    Gasquet R.   Volandri F.   R       R  \n1    Fleishman Z.  Calleri A.   L       R\n2    Roddick A.   Tsonga J.W.   R       R\n\n\n\nI want to merge them in one single Dataframe, however, whenever I try it I get additional rows.\nWhat I want to get is:\n\n     Winner       Loser         RankW   RankL    WHand   LHand      \n\n0    Fleishman Z.  Calleri A.   170.0   26.0     L       R\n1    Roddick A.   Tsonga J.W.   7.0     212.0    R      R\n2    Gasquet R.   Volandri F.   17.0    45.0     R      R\n\n\n\nThus, I want to merge them following the order of the pairs in DF1 but adding the corrisponing values of WHand and LHand in DF2.\n\nI know that all pairs correspond because I tried to determine the rows in DF1 which were not in DF2 but there are none.\n\nnames = DF2[['Winner','Loser']]\n\ndf = DF1.merge(names, on=['Winner','Loser'],how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']\n\nlen(df)\nOut: 0 \n\n"
'I\'m looking at a dataset where there\'s "City" columns and values in it are formatted like, for example,  Entity["City", {"Hefei", "Anhui", "China"}]. I want to clean up this column so that only the name of the city remains. In the case of the above example, I want everything removed except for Hefei. I\'m trying to use rstrip and lstrip with regex. Even though my regex seems to be correct based on online regex debugger websites, but the code doesn\'t work.\nHere\'s the code I\'m using:\n\ndf[\'City\'] = df[\'City\'].map(lambda x: x.lstrip(r\'(Entity["City", {")\').rstrip(r\'(",\\s"\\w+"}])\'))\n\n\nI want the result to be just Hefei. But the result that I\'m getting is :\n\nHefei", "Anhui", "China\n\n\nlstrip seems to be working, but rstrip only removes "}] and not the rest of the characters that I need removed.\n\nI was hoping someone could tell where I\'m making a mistake, or show me a better way to get this done. \n'
'I am new to ML and Data Science (recently graduated from Master\'s in Business Analytics) and learning as much as I can by myself now while looking for positions in Data Science / Business Analytics.\n\nI am working on a practice dataset with a goal of predicting which customers are likely to miss their scheduled appointment. One of the columns in my dataset is "Neighbourhood", which contains names of over 30 different neighborhoods. My dataset has 10,000 observations, and some neighborhood names only appear less than 50 times. I think that neighborhoods that appear less than 50 times in the dataset are too rare to be analyzed properly by machine learning models. Therefore, I want to remove the names of the neighborhoods from the "Neighborhood" column which appear in that column less than 50 times. \n\nI have been trying to write a code for this for several hours, but struggle to get it right. So far, I have gotten to the version below:\n\nmy_df = my_df.drop(my_df["Neighbourhood"].value_counts() &lt; 50, axis = 0)\n\n\nI have also tried other versions of code to get rid of the rows in that categorical column, but I keep getting a similar error:\n\nKeyError: \'[False False ...  True  True] not found in axis\'\n\n\nI appreciate your help in advance, and thank you for sharing your knowledge and insights with me!\n'
'I have a dataframe which contains a column with strings of the form XXX/XX/XXX. I want to remove all rows for which the length of the string between the \'/\'s is not equal to two.\n\nI\'m getting a "key error: True" with the following code:\n\ndf_issues = df_new[len(df_new[\'Job\'].str.split(\'/\')[1]) != 2 ]\n\n\nMy approach was to create a series with all rows for which the string length after the first \'/\' was not equal to 2.\n\nThanks for any help.\n'
"I have a column in a DataFrame named fatalities in which few of the values are like below:\ndata[''fatalities']= [1, 4,  , 10, 1+8, 5, 2+9,  , 16, 4+5]\n\nI want the values of like '1+8', '2+9', etc to be converted to its aggregated value i.e, \ndata[''fatalities']= [1, 4,  , 10, 9, 5, 11,  , 16, 9]\n\nI not sure how to write a code to perform above aggregation for one of the column in pandas DataFrame in Python. But when I tried with the below code its throwing an error.\n\ndef addition(col):\n  col= col.split('+')\n  col= int(col[0]) + int(col[1])\n  return col\n\ndata['fatalities']= [addition(row) for row in data['fatalities']]\n\n\nError:\n\nIndexError: list index out of range\n\n"
"I need to create an ID that has increases by one according to some specific conditions:\n\n\nRespondentID on row above was not the same as RespondentID on this row\nWhen last row had Purpose == 1\nWhen last row had Purpose == 7 AND TripNumber == 1\n\n\nIf more than one of these conditions apply, it still should only increase by 1. \n\ndata = {'RespondentID': [101, 101, 101, 101, 102, 102, 102, 103, 103, 103, 106, 106, 106, 107, 108, 108, 109, 109, 109, 109, 109, 110], \n        'TripNumber':   [1, 2, 3, 4, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 1, 2, 3, 4, 5, 1], \n        'Purpose':      [4, 1, 6, 1, 7, 4, 1, 4, 6, 8, 6, 5, 1, 7, 4, 4, 7, 5, 6, 5, 1, 4]} \n\ndf = pd.DataFrame(data=data)\n\n\nAnd I need to create\n\ndf['JourneyID'] = [1, 1, 2, 2, 3, 4, 4, 5, 5, 5, 6, 6, 6, 7, 8, 8, 9, 10, 10, 10, 10, 11] \n\n\nTo break it down. \n\n\n1: new (first) RespondentID. \n2: last row had 1 in Purpose. \n3, 5, 7, and 11: new RespondentID (and last row had 1 in Purpose). \n4 and 10: last row had 7 in Purpose AND TripNumber was 1. \n6 and 9: new RespondentID\n8: new RespondentID (and last row had 7 in Purpose while TripNumber\nwas 1)\n\n\nThis solution worked well for condition 2:\n\ntemp = ((df['purpose']==1).cumsum()+1).shift(1) \n#Makes it go up by one each time the previous row had purpose == 1\ntemp[0]=1\ndf['JourneyID'] = temp\n\n\nbut I also need it to increase by one with the other 2 conditions. \n"
"I'm trying to clean up the below dataframe so that there is only one value for each date. \nThe data changes daily based on the date. So if today was 01/01/2020 the data would show the following;\n\n            Value   Type\n01/01/2010  38.3    Forecast\n01/01/2020  31.85   Actual\n01/01/2020  6.45    Delta\n02/01/2010  31.08   Actual\n03/01/2020  29      Forecast\n04/01/2020  23.4    Forecast\n05/01/2020  24.5    Forecast\n06/01/2020  19.4    Forecast\n07/01/2020  21.1    Forecast\n08/01/2020  22.3    Forecast\n09/01/2020  25.6    Forecast\n\n\n\nI need to try and clean this dataframe so that if a date has an 'actual' value that value is used and if not than the forecasted value is used. \n\nI have been using the below to pull out the forecast but than I am missing the accuracy of having the 'actual' value where it is available and then this would also exclude D+1 as there is no forecasted value. \n\nsel = ['Forecast'] \ndf = df.loc[df['Type'].isin(sel)]\n\n\nThe end results would go something like;\n\n\n            Value   Type\n01/01/2020  31.85   Actual\n02/01/2010  31.08   Actual\n03/01/2020  29      Forecast\n04/01/2020  23.4    Forecast\n05/01/2020  24.5    Forecast\n06/01/2020  19.4    Forecast\n07/01/2020  21.1    Forecast\n08/01/2020  22.3    Forecast\n09/01/2020  25.6    Forecast\n\n\n\nAny help much appreciated!\n"
"df['Android Ver'].fillna(str(df.groupby('Category')['Android Ver'].mode()), inplace=True)\n\nThis piece of code is giving an error!\nI want to fill the NaN values in the column 'Android Ver' with the mode of the column 'Android Ver' from the particular 'Category' of apps such that the column 'Android Ver' of a Beauty app gets the mode of Android Version of the Beauty apps only in the dataset.\nLink to Jupyter Notebook\n"
"import pandas as pd\nfrom io import StringIO\n\ncsv_one = &quot;&quot;&quot;\\\none;two;three\n1;2;3;\n4;5;6;\n&quot;&quot;&quot;\ndf1 = pd.read_csv(StringIO(csv), sep=&quot;;&quot;)\n\nThis data looks as:\n   one  two  three\n1    2    3    NaN\n4    5    6    NaN\n\n\nThe desired result is:\n   one  two  three\n    1    2    3    \n    4    5    6    \n\n\nI could manually edit the csv, but I don't really want to have to do that if possible.\nIn R the function read_delim can manage this with something along the lines of read_delim( &lt;path&gt;, &quot;;&quot;, escape_double = FALSE, trim_ws = TRUE)\n"
"I need to clean data by this specific rule (efficitently):\nif there are 3 or fewer consecutive NaNs in a column, fill this NaN &quot;chain&quot; in df column by .fillna(method='ffill').\nOtherwise leave it (for another method)\nExample:\ndf = pd.DataFrame({&quot;A&quot;:[8001, 7999, 7998, np.NaN, 9900, 9342, 9324, 8534, 8358, 9457, np.nan, 8999, 8492, np.nan, np.nan],\n                   &quot;B&quot;:[201, 209, 298, 300,np.nan, 342, 324, 854, 858, 457, 145, 189, 192, 134, 135],\n                   &quot;C&quot;:[11991, 15631, 47998, 38030, 19900, 29342, np.nan, np.nan, np.nan,np.nan, 27245, 28999, 28492, 29334, 28234]}, \n                   index=pd.Index(['2019-06-17 00:00:00','2019-06-17 00:01:01', '2019-06-17 00:02:00', '2019-06-17 00:03:04', \n                                   '2020-06-17 00:04:00', '2020-06-17 00:05:00', '2020-06-17 00:06:00', '2020-06-17 00:07:00',\n                                   '2020-06-17 00:08:00','2020-06-17 00:09:00','2020-06-17 00:10:00','2020-06-17 00:11:00',\n                                   '2020-06-17 00:12:00','2020-06-17 00:13:00', '2020-06-17 00:14:00']))\n\ndf\n\n                 Time     A     B       C\n'2019-06-17 00:00:00'  8001   201   11991\n'2019-06-17 00:01:01'  7999   209   15631\n'2019-06-17 00:02:00'  7998   298   47998\n'2019-06-17 00:03:04'  NaN    300   38030\n'2020-06-17 00:04:00'  9900   NaN   19900\n'2020-06-17 00:05:00'  9342   342   29342\n'2020-06-17 00:06:00'  9324   324     NaN\n'2020-06-17 00:07:00'  8534   854     NaN\n'2020-06-17 00:08:00'  8358   858     NaN\n'2020-06-17 00:09:00'  9457   457     NaN\n'2020-06-17 00:10:00'   NaN   145   27245\n'2020-06-17 00:11:00'  8999   189   28999\n'2020-06-17 00:12:00'  8492   192   28492\n'2020-06-17 00:13:00'   NaN   134   29334\n'2020-06-17 00:14:00'   NaN   135   28234\n\nExpected Result:\n                 Time     A     B       C\n'2019-06-17 00:00:00'  8001   201   11991\n'2019-06-17 00:01:01'  7999   209   15631\n'2019-06-17 00:02:00'  7998   298   47998\n'2019-06-17 00:03:04'  7998   300   38030\n'2020-06-17 00:04:00'  9900   300   19900\n'2020-06-17 00:05:00'  9342   342   29342\n'2020-06-17 00:06:00'  9324   324     NaN\n'2020-06-17 00:07:00'  8534   854     NaN\n'2020-06-17 00:08:00'  8358   858     NaN\n'2020-06-17 00:09:00'  9457   457     NaN\n'2020-06-17 00:10:00'  9457   145   27245\n'2020-06-17 00:11:00'  8999   189   28999\n'2020-06-17 00:12:00'  8492   192   28492\n'2020-06-17 00:13:00'  8492   134   29334\n'2020-06-17 00:14:00'  8492   135   28234\n\n"
'In pandas data frame there are multiple binary features columns with binary values, and the challenge is to identify which column has one-hot labels/values(which column can be a part of the one-hot encoded vector) and which column is an independent feature and not a part of one-hot encoded labels/vector.\nThe data that I need to clean and preprocess somehow looks like this:\nRows   v1  v2  v3  v4  v5  v6  v7  v8  v9  v10 Label\n\n0      1   1   0   0   0   0   0   0   0   0     0\n1      0   0   0   0   0   0   1   0   0   0     0\n2      0   1   0   1   0   0   0   1   0.5 0     0\n3      0   0   0   0   0   1   0   0   0   1     0\n4      0   0   0   0   1   0   0   0   0   0     1\n5      0   0   0   0   0   0   1   0   0   0     1\n6      0   0   0   1   0   0   0   0   0   1     1\n7      0   0   1   0   1   0   0   0   0.2 0     0\n8      0   0   0   0   0   1   0   0   0   1     0\n\nNote:  Need to find out a specific combination of columns in which we have one 1 and other zeros in a row which is as there can be some non-hotEncoded/independent binary columns.\nBy specific combination of columns in which we have one 1 and other zeros in a row, I mean a result/final combination of columns like this, where we have one 1 in a row(by excluding the other binary columns):\nv1  v4  v5  v6  v7\n\n1   0   0   0   0  \n0   0   0   0   1   \n0   1   0   0   0   \n0   0   0   1   0 \n0   0   1   0   0 \n0   0   0   0   1  \n0   1   0   0   0  \n0   0   1   0   0 \n0   0   0   1   0  \n\n'
"I have a large dataframe in the following form:\nFundManager | AsOfDate | InvestmentDesc | Amount  \nManagerA | 6/1/2020 | Four Seasons 1st lien TL | 25500.86  \nManagerA | 6/1/2020 | Arden Group First Lien TL | 28731.00  \nManagerB | 6/1/2020 | Four Seasons 1L Term Loan | 16853.00  \nManagerB | 6/1/2020 | Arden 1st Lien Term Loan | 50254.30\n\nThe same underlying financial instrument will often be held by multiple asset managers but described in a slightly different manner as shown in the table above (e.g. &quot;Four Seasons 1st Lien TL&quot; vs. &quot;Four Seasons 1L Term Loan&quot;).\nI am trying to figure out the best way to make certain replacements to the &quot;InvestmentDesc&quot; column in the pandas dataframe (e.g. replace &quot;Term Loan&quot; with &quot;TL&quot;, replace &quot;1st Lien&quot; with &quot;1L&quot;), ideally (i) in a way that doesn't have to repeatedly loop through several million rows for each term, and (ii) in a way that can be used on a handful of other columns in the dataframe but using a separate list of terms to replace.\nI currently have the following function:\ndef replace_all(dictReplace, text):\n    rep = dict((re.escape(k), v) for k, v in dictReplace.items())\n    pattern = re.compile(&quot;|&quot;.join(rep.keys()))\n    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n    return text\n\nInto which I am then trying to pass a list of terms to replace (what I assume is a dictionary?) and the existing dataframe column to be modified:\ndictRepStrings = {&quot;1st lien&quot;: &quot;1l&quot;, &quot;first lien&quot;: &quot;1l&quot;, &quot;2nd lien&quot;: &quot;2l&quot;, &quot;second lien&quot;: &quot;2l&quot;, &quot;term loan&quot;: &quot;tl&quot;}\ndf['NewCol'] = df['InvestmentDesc'].apply(lambda x: replace_all(dictRepStrings, df['InvestmentDesc']))\ndf\n\nHowever upon doing so, I end up with &quot;TypeError: expected string or bytes-like object&quot; as shown below:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-10-a37630bdc5a4&gt; in &lt;module&gt;\n     23 \n     24 dictRepStrings = {&quot;1st lien&quot;: &quot;1l&quot;, &quot;first lien&quot;: &quot;1l&quot;, &quot;2nd lien&quot;: &quot;2l&quot;, &quot;second lien&quot;: &quot;2l&quot;, &quot;term loan&quot;: &quot;tl&quot;}\n---&gt; 25 df['NewCol'] = df['InvestmentDesc'].apply(lambda x: replace_all(dictRepStrings, df['InvestmentDesc']))\n     26 df\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py in apply(self, func, convert_dtype, args, **kwds)\n   3846             else:\n   3847                 values = self.astype(object).values\n-&gt; 3848                 mapped = lib.map_infer(values, f, convert=convert_dtype)\n   3849 \n   3850         if len(mapped) and isinstance(mapped[0], Series):\n\npandas\\_libs\\lib.pyx in pandas._libs.lib.map_infer()\n\n&lt;ipython-input-10-a37630bdc5a4&gt; in &lt;lambda&gt;(x)\n     23 \n     24 dictRepStrings = {&quot;1st lien&quot;: &quot;1l&quot;, &quot;first lien&quot;: &quot;1l&quot;, &quot;2nd lien&quot;: &quot;2l&quot;, &quot;second lien&quot;: &quot;2l&quot;, &quot;term loan&quot;: &quot;tl&quot;}\n---&gt; 25 df['NewCol'] = df['InvestmentDesc'].apply(lambda x: replace_all(dictRepStrings, df['InvestmentDesc']))\n     26 df\n\n&lt;ipython-input-10-a37630bdc5a4&gt; in replace_all(dictReplace, text)\n     18     rep = dict((re.escape(k), v) for k, v in dictReplace.items())\n     19     pattern = re.compile(&quot;|&quot;.join(rep.keys()))\n---&gt; 20     text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\n     21     return text\n     22 \n\nTypeError: expected string or bytes-like object\n\nAfter two days of googling-until-frustration I simply cannot figure out what I am doing wrong or how to fix. Any advice or thoughts as to how I should proceed would be greatly appreciated!\n"
"I am using df[''].isnull() on a data frame I am currently using pulled from a CSV. It is practice data that has lots of missing values and errors intentionally, however, pandas cannot identify the missing values in the DOB column, having verified there are missing values based on opening the CSV and seeing the empty cells under the DOB column.\ndf['DOB'].isnull().sum()\nOutput: 0\n\n"
'Hi I am new to pandas and struggling with a manipulation.\nI have a dataframe df with a huge number of columns, and I only want to keep the number of columns that have a count of above 5000 values.\nI tried the loop below but it does not work. Is there any easy way to do this? Also is there a function I could create to apply this to any dataframe where I want to keep columns with only n values or more?\nfor column in df.columns: \n   if df[column].count() &gt; 5000: \n      column = column\n   else: \n      df[column].drop()\n\nThanks\n'
"The input column has a variable number of dictionary lists, it is not fixed.\nINPUT column:\n\nFacilities\n[{'name': 'Work from home', 'icon': 'WFH.svg'}]\n[{'name': 'Gymnasium', 'icon': 'Gym.svg'}, {'name': 'Cafeteria', 'icon': 'Cafeteria.svg'}, {'name': 'Work from home', 'icon': 'WFH.svg'}]\n[{'name': 'Free food', 'icon': 'FreeFood.svg'}, {'name': 'Team outings', 'icon': 'TeamOuting.svg'}, {'name': 'Education assistance', 'icon': 'Education.svg'}]\n[{'name': 'Soft skill training', 'icon': 'SoftSkillsTraining.svg'}, {'name': 'Job training', 'icon': 'JobTraining.svg'}]\n[{'name': 'Free transport', 'icon': 'Transportation.svg'}, {'name': 'Work from home', 'icon': 'WFH.svg'}, {'name': 'Team outings', 'icon': 'TeamOuting.svg'}, {'name': 'Soft skill training', 'icon': 'SoftSkillsTraining.svg'}]\n\nThis above input should be filtered so that the column will only have a single list with all the values of keys &quot;name&quot; collected from different dictionaries in the list.\nDesired Output column:\n\nFacilities\n['Work from home']\n['Gymnasium', 'Cafeteria', 'Work from home']\n['Free food','Team outings','Education assistance']\n['Soft skill training','Job training']\n['Free transport', 'Work from home','Team outings','Soft skill training']\n\n"
"So I am trying to use Pandas to replace all NaN values in a table with the median across a particular range. I am working with a larger dataset but for example\nnp.random.seed(0)\nrng = pd.date_range('2020-09-24', periods=20, freq='0.2H')\ndf = pd.DataFrame({ 'Date': rng, 'Val': np.random.randn(len(rng)), 'Dist' :np.random.randn(len(rng)) }) \ndf.Dist[df.Dist&lt;=-0.6] = np.nan\ndf.Val[df.Val&lt;=-0.5] = np.nan\n\n\nWhat I want to do is replace the NaN values for Val and Dist with the median value for each hour for that column. I have managed to get the median values in a separate reference table:\ndf.set_index('Date', inplace=True)\ndf = df.assign(Hour = lambda x : x.index.hour)\ndf_val = df[[&quot;Val&quot;, &quot;Hour&quot;]].groupby(&quot;Hour&quot;).median()\ndf_dist = df[[&quot;Dist&quot;, &quot;Hour&quot;]].groupby(&quot;Hour&quot;).median()\n\n\nBut now I have tried all of the below commands in various forms and cannot work out how to fill NaN values.\ndf[[&quot;Val&quot;,&quot;Hour&quot;]].mask(df['Val'].isna(), df_val.iloc[df.Hour], inplace=True)\n\ndf.where(df['Val'].notna(), other=df_val[df.Hour],axis = 0)\n\ndf[&quot;Val&quot;] = np.where(df['Val'].notna(), df['Val'], df_val(df.Hour))\n\ndf.replace({&quot;Val&quot;:{np.nan:df_val[df.Hour]}, &quot;Dist&quot;:{np.nan:df_dist[df.Hour]}})\n\n"
"I hope you are doing well.\nI need help to perform a complex &quot;NaN replace&quot; on my dataframe.\nWhat is the best way to replace NaN values in a pandas column, based on a mode of other column values filtered by other columns?\nLet me illustrate my problem:\nimport random\nimport numpy as np\nimport pandas as pd\ndata = {'Region': [1,1,1,2,2,2,1,2,2,2,2,1,1,1,2,1], 'Country': ['a','a', 'a', 'a', 'a','a', 'a', 'a', 'b', 'b', 'b', 'b','b','b','b','b'], 'GDP' : [100,100,101,105,105,110,np.nan,np.nan,200,200,100,150,100,150,np.nan,np.nan]}\ndf = pd.DataFrame.from_dict(data)\n\ndf:\n     Region Country GDP\n0        1       a  100.0\n1        1       a  100.0\n2        1       a  101.0\n3        2       a  105.0\n4        2       a  105.0\n5        2       a  110.0\n6        1       a    NaN\n7        2       a    NaN\n8        2       b  200.0\n9        2       b  200.0\n10       2       b  100.0\n11       1       b  150.0\n12       1       b  100.0\n13       1       b  150.0\n14       2       b    NaN\n15       1       b    NaN\n\n\nI would like to replace the nan values of the GDP column with the mode of other GDP values for the same country and region.\n\nIn the case of the NaN value of the GDP column of index 6, I wish to replace it with 100 (as it is the mode for GDP values for Region 1 &amp; Country a)\nThe desired output should look like this:\n    Region Country  GDP\n0        1       a  100\n1        1       a  100\n2        1       a  101\n3        2       a  105\n4        2       a  105\n5        2       a  110\n6        1       a  100\n7        2       a  105\n8        2       b  200\n9        2       b  200\n10       2       b  100\n11       1       b  150\n12       1       b  100\n13       1       b  150\n14       2       b  200\n15       1       b  150 \n\nThank you for your help, I hope you have an excellent day!\n"
"I am cleaning a dataset and I need to remove formatting errors in column A if the value in column B matches a specific string.\nA       B\nfoo//,  cherry\nbar//,  orange\nbar//,  cherry\nbar     apple\n\nSo in this situation if column B is 'cherry' I want to replace &quot;//,&quot; with &quot;,&quot; in column A. The final result would look like this.\nA       B\nfoo,    cherry\nbar//,  orange\nbar,    cherry\nbar     apple\n\nAny advice is much appreciated\n"
"I'm working through the DataQuest Data Analyst path and am on the Laptops dataset. I'm trying to convert a column from string to float (the column contains cpu processor speeds in GHz).\nlaptops[&quot;processor_speed_ghz&quot;] = laptops[&quot;cpu&quot;].str.split().str[-1]\nlaptops[&quot;processor_speed_ghz&quot;] = laptops[&quot;processor_speed_ghz&quot;].str.replace(&quot;GHz&quot;, &quot;&quot;)\nlaptops[&quot;processor_speed_ghz&quot;] = laptops[&quot;processor_speed_ghz&quot;].astype(float)\nprint(laptops[&quot;processor_speed_ghz&quot;].value_counts())\n\nThe conversion works perfectly, except that when I inspect the new column, it says the type is int64 instead of float64. Not sure what I'm doing wrong.\n"
"I have the below python code which goes through my csv file, removes all percentage signs (%), and rewrites the file back into a new csv file. \n\nBefore I run my code, the data looks as such:\n\n| Column 1 | Column 2 | Column 3 | Column 4 | etc |\n\nNow, it looks like this:\n\n| ['Column 1' | 'Column 2' | 'Column 3' | 'Column 4' | 'etc'] |\n\nI don't want my data to include the brackets/apostrophes that are created by python's list functionality. I am afraid it will either, a) be included in my database when uploaded, or b) bomb out the import to the mySQL database. Any way around this? \n\nThanks in advance,\nDan\n\nHere is my code:\n\nimport csv\nimport string\n\ninput_file = open('Data.csv', 'r')\noutput_file = open('NewData.csv', 'w')\ndata = csv.reader(input_file)\nwriter = csv.writer(output_file)\nspecials = '%'\n\nfor line in data:\n    line = str(line)\n    new_line = str.replace(line,specials,'')\n    writer.writerow(new_line.split(','))\n\ninput_file.close()\noutput_file.close()\n\n"
'I\'m trying to create a script to remove all docstrings inside a folder. To do so, I\'d like to make a regex as efficient as possible.\n\nI\'ve started with this one:\n\nimport re\n\ndoc_reg = r\'(class|def)(.+)\\s+("""[\\w\\s\\(\\)\\-\\,\\;\\:]+""")\'\n\nfile_content = \'\'\'\n"""\nMycopyright (c)\n"""\n\nfrom abc import d\n\nclass MyClass(MotherClass):\n    """\n    Some;\n    Multi-\n    Line Docstring:\n    """\n\n    def __init__(self, my_param):\n        """Docstring"""\n        self.my_param = my_param\n\ndef test_fctn():\n    """\n    Some Docstring\n    """\n\n    return True\n\ndef test_fctn():\n    some_string = """\n    Some Docstring\n    """\n\n    return some_string\n\'\'\'\n\nprint(re.sub(doc_reg, r\'\\1\\2\', file_content))\n\n\nIt works quite well but I\'m pretty sure it\'s possible to make this regex more efficient.\n\nThanks\n'
"I am new to using Python and Pandas, but have been trying to automate some of the data cleaning/merging for reports of mine. \nSo far I've had success in building up the combined file of all information I need to feed into my reporting summary but have \ngotten stuck with grouping and merging data with matching prefixes.\n\nI have a data set that is structured similar to this in a pandas dataframe:\n\nCompany_Num     Company_Name                2019_Amt    2020_Amt    Code    Flag    Manager\n\n1               ABC Company Ltd             2000        400         A       Y       John\n1               ABC Company Ltd             2000        400         A       Y       John\n2               DEFGHIJ Company (London)    480         100         B       N       James\n3               DEFGHIJ Company (Bristol)   600         700         B       N       James\n4               DEFGHIJ Company (York)      1500        1000        B       N       James       \n5               KLM Services                9000        7000        A       Y       Jane   \n6               NOPQ Industries             300         400         C       Y       Jen   \n7               NOPQ Industries - London    7000        8000        C       Y       Jen      \n\n\nI'm wanting to get a summary set of data where there are no duplicates in my data and \ninstead of having rows for each office I have one summarised value for each company. Ultimately\nwith a dataframe like:\n\nCompany_Name            2019_Amt    2020_Amt    Code    Flag\nABC Company Ltd         2000        400         A       Y   \nDEFGHIJ Company         2580        1800        B       N   \nKLM Services            9000        7000        A       Y   \nNOPQ Industries         7300        8400        C       Y \n\n\nSo far I have managed to drop the duplicates using:\n\ndf.drop_duplicates(subset=['Company_Num', 'Company_Name', 'Code', '2019_Amt', '2020_Amt'])\n\nWith the resulting table:\n\nCompany_Num     Company_Name                2019_Amt    2020_Amt    Code    Flag    Manager\n1               ABC Company Ltd             2000        400         A       Y       John\n2               DEFGHIJ Company (London)    480         100         B       N       James\n3               DEFGHIJ Company (Bristol)   600         700         B       N       James\n4               DEFGHIJ Company (York)      1500        1000        B       N       James       \n5               KLM Services                9000        7000        A       Y       Jane\n6               NOPQ Industries             300         400         C       Y       Jen   \n7               NOPQ Industries - London    7000        8000        C       Y       Jen              \n\n\nThe solution that I have tried is to substring the first 9 characters of each company name and use a groupby \nand sum on those, but that leaves me with the column being saved as the substring. This has also dropped the \ncolumns Code and Flag from my dataframe, leaving me with table like this:\n\ndf['SubString_Company_Name'] = df['Company_Name'].str.slice(0,9)\ndf.groupby([df.SubString_Company_Name]).sum().reset_index()\n\n\nSubString_Company_Name    2019_Amt    2020_Amt    \nABC Compa                 2000        400\nDEFGHIJ C                 2580        1800   \nKLM Servi                 9000        7000\nNOPQ Indu                 7300        8400\n\n\nI have tried to use the os.path.commonprefix function to get the company names, but can't find a way to use it in a dataframe, \nand for multiple values. My understanding is it will look at the list as a whole and return the longest common prefix of the \nwhole list which wouldn't work. I have also considered extracting all duplicate substrings into new dataframes and summing \nand renaming there before merging back into one data set, but I'm not sure if that would work. The solutions I've found online \nhave been centred around uniform data where lambda can be used with a delimiter or the prefix is always the same size, whereas \nmy data is not uniform and the prefixes are varying sizes.\n\nMy data is changed every month and so I want to design a dynamic solution that isn't relying on substrings since I could run into \nissues with only taking 9 characters. My final consideration is to extract the SubString_Company_Name\ninto a list, convert that to the os.path.commonprefix of the Company_Name and then save the unique commonprefix value of each\nCompany_Name into a new list and for each item in that list create a new summary table. But I don't know if this would work and \nI want to know if there's a better or more efficient way of doing this before trying.\n"
"I have a master list of all words used in a set of articles, and now I'm trying to count the occurrence of each word in the master list within each article.  I'm then going to try and build some association rules on the data.  For example,  My data might look like this:\n\nmaster_wordlist = ['dog', 'cat', 'hat', 'bat', 'big']\narticle_a = ['dog', 'cat', 'dog','big']\narticle_b = ['dog', 'hat', 'big', 'big', 'big']\n\n\nI need to get my data into this format:\n\nArticle        dog    cat    hat    bat    big\narticle_a      2      1      0      0      1\narticle_b      1      0      1      0      3\n\n\nI'm struggling to make this transformation, I've been playing around with the nltk, but I can't figure out how to get a count where it includes the words that don't exist.  Any help would be greatly appreciated!\n"
"I have a data frame , and I want to create a new data frame based on the values of two columns. The pair of values alwyas would be: 'x' and 'x' or 'x' and NaN or NaN and 'x' or NaN and NaN. So for the first three examples the values of the new varabiel would b 'x' and for the last one would be NaN. Nan is missing value.\n\nThe pandas data frame is:\n\nName    Company nameC.antiguo   Company nameC.completado\n ssd         X                           X\n adf         B                          NaN\n dsf         C                           C\n eee         NaN                        NaN\n wqe         NaN                        C\n\n\nI tried the following code but it doen´t work at all. \n\npruebaofempat['bn']= pruebaofempat['Company nameC.antiguo'] + pruebaofempat['Company nameC.completado']\n\n\nSo, How Can I create the new variable correctly?\n"
"I am using pandas to read in a csv column where every row has the following format:\n\nIP: XXX:XX:XX:XXX\n\nTo get rid of the IP: prefix, I am editing the column after the fact:\n\nlogs['ip'] = logs['ip'].str[4:]\n\nIs there a way to perform this operation within read_csv, maybe with regex, to avoid the post-computation?\n\nUpdate |\nConsider this scenario where there are multiple columns that have these prefixes –\xa0is there a better way?\n\nlogs['mac'] = logs['mac'].str[5:]\nlogs['id'] = logs['id'].str[4:]\nlogs['lan'] = logs['lan'].str[5:]\nlogs['ip'] = logs['ip'].str[4:]\n"
"I'm parsing my data from JSON to following DataFrame, but I'm not able to remove the extra stuff from readingtime column &amp; convert it to datetime format\n\n                        readingtime                      deviceId  \n0  {u'$date': u'2014-11-04T17:27:50.000+0000'}           1224EG12\n\n\nI tried using replace, lstring-rstring but I'm not able to replace the extra characters from thr readingtime column \n\nda2['readingtime2'] = da2['readingtime'].str.replace('date', '') \n\n\ndata['readingtime'] = data['readingtime'].map(lambda x: str(x)[13:])\n\n\nTried loc as well but not getting errors \n\nEDITED :\n\nI want final readingtime to be\n'2014-11-04 17:27:50.000 +000'\nwhich I want to convert to datetime - yyyy-mm-dd hh:mm:ss.mils +UTC \n"
'I am trying to remove some hidden enters using my pdfform-scraper-script before I write it into a csv file. But I keep receiving the error mentioned in the title. The relevant piece of code is: \n\nimport glob\nimport os\nimport sys\nimport csv\nfrom pdfminer.pdfparser import PDFParser\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdftypes import resolve1\n\npath = \'C:\\Users\\Wonen\\Downloads\\Test\'\nfor filename in glob.glob(os.path.join(path, \'*.pdf\')):\n    fp = open(filename, \'rb\')\n    #read pdf\'s\n    parser = PDFParser(fp)\n    doc = PDFDocument(parser)\n    #doc.initialize()    # &lt;&lt;if password is required\n    fields = resolve1(doc.catalog[\'AcroForm\'])[\'Fields\']\n    row = []\n    for i in fields:\n        field = resolve1(i)\n        name, value = field.get(\'T\'), field.get(\'V\')\n        #removing \'hidden enter\'\n        if value == None:\n           print \'ok\'\n        elif value == NotImplementedError:\n            print \'ok\'\n        elif \'\\n\' in value:    \n           value.replace(\'\\n\',\' \')\n        elif \'\\r\' in value:    \n           value.replace(\'\\r\',\' \')\n        row.append(value)\n    writer.writerow(list(reversed(row)))\n\n\nThe complete error (+output) is:\nok\nok\n\n\n  Traceback (most recent call last):   File\n  "C:\\Python27\\Scripts\\test3.py", line 37, in \n      elif \'\\n\' in value:     TypeError: argument of type \'PSLiteral\' is not iterable\n\n\nDoes anyone know how to solve this?\n'
'(for pythonistas, the below code is in R\'s format before I get some #hatehard)\n\nThis one has been frustrating me for a way too long.\n\nI have 2 datasets\n\ndf1 &lt;- data.frame(ID = c("Person.A", "Person.B", "Person.C", "Person.D", "Person.E", "Person.F"),\n                  Aa = c(0,1,2,NA,1,1),\n                  Ab = c(0,NA,2,1,1,1),\n                  Ac = c(NA,NA,2,2,1,1),\n                  no.match = c(0,1,2,2,1,2))\n\ndf2 &lt;- data.frame(ID = c("Person.A", "Person.B", "Person.C", "Person.D", "Person.E"),\n                  Ba = c(0,NA,2,1,1),\n                  Bb = c(NA,1,2,2,1),\n                  Bc = c(0,1,2,2,1))\n\n\nI then merge these 2 datasets using merge(df1, df2, all.x = T, by = "ID" to get:\n\n         ID Aa Ab Ac no.match Ba Bb Bc\n1 Person.A  0  0 NA        0  0 NA  0\n2 Person.B  1 NA NA        1 NA  1  1\n3 Person.C  2  2  2        2  2  2  2\n4 Person.D NA  1  2        2  1  2  2\n5 Person.E  1  1  1        1  1  1  1\n6 Person.F  1  1  1        2 NA NA NA\n\n\nThe actual datasets are much more complicated with lots of columns that have no matches in other columns. So I don\'t think I could do something that depends on the arrangement of the columns.\n\nColumns Aa and Ba contain the same information; and columns Ab and Bb do as well, and so on, but column no.match does not contain a matching column. \n\nI want to "map" the values from from the same row of Ba to Aa if Aa is NA and do the same for Ab and Bb, Ac and Bc, etc.\n\nThe result DF in this case would look like:\n\n        ID Aa Ab Ac no.match Ba Bb Bc\n1 Person.A  0  0  0      0    0 NA  0\n2 Person.B  1  1  1      1    NA  1  1\n3 Person.C  2  2  2      2    2  2  2\n4 Person.D  1  1  2      2    1  2 NA\n5 Person.E  1  1  1      1    1  1  1\n6 Person.F  1  1  1      2    NA NA NA\n\n\nWhere element [4,2] was replaced by element [4,6]\nThe rows and the columns need to match up.\n\nI\'ve tried an embarrassingly large number of things: apply, ifelse, iterating through a list of columns l1 = c(\'Aa\',\'Ab\',\'Ac\'), l2 = c(\'Ba\', \'Bb\', \'Bc\')\n\nI can do the one-off: which(is.na(mdf$Aa)) &lt;- mdf[which(is.na(mdf$Aa)), c("Ba")]\n\nBut how can I do this iteratively?\n\nThank you! (sorry for the long-windedness)\n'
"In the following data, date and time are in separate columns and I combing them to get a full date-time, so that the resultant column is of type 'datetime64[ns]'. However at times there are records with blank date and time, and in such cases the resultant column is of type 'object', essentially a string object. How can I deal with this when all the records are present and when it is not?\n\n\n  SAMPLE DATA\n\n\nCARD,IN Date,IN Time,OUT Date,OUT Time\n100001,30-04-2015,14:19:18,01-05-2015,00:10:56\n100002,30-04-2015,11:27:52,,\n100003,30-04-2015,17:59:47,01-05-2015,04:51:52\n100004,30-04-2015,16:15:25,,\n100005,30-04-2015,10:25:13,01-05-2015,01:25:13\n100006,30-04-2015,16:59:10,,\n100007,30-04-2015,13:22:06,,\n100008,30-04-2015,09:15:29,,\n100009,30-04-2015,17:01:10,01-05-2015,01:51:01\n100010,30-04-2015,13:13:30,01-05-2015,01:37:28\n100011,30-04-2015,09:37:28,01-05-2015,00:37:28\n100012,30-04-2015,18:55:44,01-05-2015,03:22:22\n100013,30-04-2015,14:28:16,01-05-2015,01:27:18\n100014,30-04-2015,09:02:13,01-05-2015,00:02:13\n100015,30-04-2015,09:04:10,01-05-2015,00:04:10\n100016,30-04-2015,18:51:56,01-05-2015,09:51:56\n100017,30-04-2015,09:12:51,01-05-2015,00:12:51\n100018,30-04-2015,10:40:31,01-05-2015,01:40:31\n100019,30-04-2015,10:35:56,01-05-2015,01:35:56\n100020,30-04-2015,17:50:03,01-05-2015,03:54:54\n100021,30-04-2015,17:00:16,01-05-2015,02:45:35\n100022,30-04-2015,11:18:41,01-05-2015,01:15:52\n\n\n\n  Following is the code I've for now:\n\n\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\n#CARD,IN Date,IN Time,OUT Date,OUT Time     \ndata = pd.read_csv('DATA.csv', parse_dates=[['IN Date','IN Time'],['OUT Date','OUT Time'],'IN Date','OUT Date'], keep_date_col=True)\ndata.rename(columns={'IN Date_IN Time':'IN','OUT Date_OUT Time':'OUT'}, inplace=True)\ndata = data[['CARD','IN Date', 'IN', 'OUT Date', 'OUT']]\n#This line will fail when all the records are present\ndata.ix[(data.OUT == 'nan nan'), 'OUT'] = np.nan\n\n\n\n"
"I need to winsorize two columns in my dataframe of 12 columns.\n\nSay, I have columns 'A', 'B', 'C', and 'D', each with a series of values. Given that I cleaned some NaN columns, the number of columns was reduced from 100 to 80, but they are still indexed to 100 with gaps (e.g. row 5 is missing).\n\nI want to transform only columns 'A' and 'B' via winsorize method. To do this, I must convert my columns to a np.array.\n\nimport scipy.stats\ndf['A','B','C','D'] = #some values per each column\nab_df = df['A','B']\nX = scipy.stats.mstats.winsorize(ab_df.values, limits=0.01)\nnew_ab_df = pd.DataFrame(X, columns = ['A','B'])\ndf = pd.concat([df['C','D'], new_ab_df], axis=1, join='inner', join_axes=[df.index])\n\n\nWhen I convert to a np.array, then back to a pd.DataFrame, it's len() is correct at 80 but my indexes have been reset to be 0->80. How can I ensure that my transform 'A' and 'B' columns are indexed correctly? I don't think I can use the apply(), which would preserve index order and simply swap out the values instead of my approach, which creates a transformed copy of my df with only 2 columns, then concats them to the rest of my non-transformed columns.\n"
'I have a .csv that I need to make look like a specifically formatted dictionary and I cannot wrap my head around the loop necessary to do this the way I need. For an example, these three rows (row[0] is the header with my labels) that currently look like:\n\n0 outlook,temperature,humidity,windy,result\n1 overcast,hot,high,FALSE,yes\n2 overcast,cool,normal,TRUE,yes\n...\n\n\nShould look like:\n\n {"outlook":"overcast", "temperature":"hot", "humidity":"high","windy":"FALSE","result":"yes"},\n {"outlook":"overcast", "temperature":"cool", "humidity":"normal","windy":"TRUE","result":"yes"},\n ...\n\n\nOnce they\'ve been processed and cleaned up. I have it close but not quite right using dictionaries. I\'ve been messing with pandas and import csv and I\'m starting to lose track of what the best way to do this in Py3 is.\n\nThanks for any help. It\'s much appreciated.\n'
"I am on the process of cleaning a dataframe, and I want to check if there are any values from a list of words in a dataframe. If it is present, the value should be replaced by NA values. For example,\n\nMy dataframe is like.\n\np['title']\n\n1                                             Forest\n2                                            [VIDEO_TITLE]\n3                                            [VIDEO_TITLE]\n4                                            [VIDEO_TITLE]\n5                                [${title}url=${videourl}]\n\n\np.dtypes\ntitle    object\ndtype: object\n\n\nand \n\nc= ('${title}', '[VIDEO_TITLE]')\n\n\nSince the rows 2,3,4,5 have the words in c, I want that to be replaced by NA values.\n\nI'm trying the following,\n\np['title'].replace('|'.join(c),np.NAN,regex=True).fillna('NA')\n\n\nThis one runs without error, but I am getting the same input as output. There are no changes at all.\n\nMy next try is,\n\np['title'].apply(lambda x: 'NA' if any(s in x for s in c) else x)\n\n\nwhich is throwing an error,\n\n\n  TypeError: argument of type 'float' is not iterable\n\n\nI am trying several other things without much success. I am not sure what mistake I am doing. \n\nMy ideal output would be,\n\np['title']\n\n1     Forest\n2        NA\n3        NA\n4        NA\n5        NA\n\n\nCan anybody help me in solving this?\n"
"I need to clean a lot of text files from useless code or exception, in order to make some  text analysis, for example:\nstart-text: 7001\n\nAdd a working set\nSearch for something in that working set\nRemove the working set\nSearch via context menu\n\n==&gt;\nLog: Mon Dec 17 17:23:54 GMT+01:00 2001\n4 org.eclipse.ui 0 java.util.ConcurrentModificationException\njava.util.ConcurrentModificationException\n    at java.util.AbstractList$Itr.checkForComodification(AbstractList.java(Compiled\nCode))\n    at java.util.AbstractList$Itr.next(AbstractList.java(Compiled Code))\n    at\n\norg.eclipse.jdt.internal.ui.search.JavaSearchSubGroup.fill(JavaSearchSubGroup.java:30)\n    at org.eclipse.jdt.internal.ui.search.JavaSearchGroup.fill(JavaSearchGroup.java:51)\n    at org.eclipse.jdt.internal.ui.actions.ContextMenuGroup.add(ContextMenuGroup.java:25)\n    at\norg.eclipse.jdt.internal.ui.packageview.PackageExplorerPart.menuAboutToShow(PackageExplorerPart.java:498)\n    at org.eclipse.jface.action.MenuManager.fireAboutToShow(MenuManager.java:220)\n    at org.eclipse.jface.action.MenuManager.handleAboutToShow(MenuManager.java:253)\n    at org.eclipse.jface.action.MenuManager.access$0(MenuManager.java:250)\n    at org.eclipse.jface.action.MenuManager$1.menuShown(MenuManager.java:280)\n\n&lt;==\nend-text: 7001\nor:\nstart-text: 7019\n20011211\nRan the following compilation unit under the debugger with the breakpoint\nindicated. To get Windows to hit the breakpoint, you have to have the right dl\nand run an accessibility client. If you cannot replicate this problem with a\nsimpler little example, I can walk you through the steps to do this.\nThe only thing different about this CU is that it contains a non-public class\nas well as a public class. When I hit the breakpoint in the debugger, I got a\ndialog that told me that it can't find the source for the non-public class. The\ndialog is very persistent - I have told it OK and Cancel, but it keeps coming\nback. Even if I switch to the Java perspective, I still get the nagging dialog\n. If I kill the process, the dialog does not come back. But the point is\nthat the debugger should be able to see the source for this class - it is right\nin my eclipse workspace. It isn't even hidden in some jar somewhere - it's very\nvisible. I suspect that it's the non-public class thing that is confusing the\nsource lookup. If it helps any, I will attach the dialog. Here's the code:\n==&gt;\npackage test;\n\nimport org.eclipse.swt.*;\nimport org.eclipse.swt.graphics.*;\nimport org.eclipse.swt.widgets.*;\nimport org.eclipse.swt.layout.*;\nimport org.eclipse.swt.events.*;\nimport org.eclipse.swt.internal.win32.*;\nimport org.eclipse.swt.internal.ole.win32.*;\nimport org.eclipse.swt.ole.win32.*;\n\npublic class AccessibilityTest {\n    static Display display;\n    static Shell shell;\n    static FakeWidget fakeWidget;\n\n    public static void main(String[] args) {\n        display = new Display();\n        shell = new Shell(display);\n        shell.setLayout(new GridLayout());\n        shell.setText(&quot;Accessibility Test&quot;);\n\n        fakeWidget = new FakeWidget(shell, SWT.MULTI);\n        fakeWidget.setLayoutData(new GridData(GridData.FILL_BOTH));\n        shell.setSize(140, 110);\n        shell.open();\n        while (!shell.isDisposed()) {\n            if (!display.readAndDispatch())\n                display.sleep();\n        }\n    }\n}\n\n\n\nprivate static GUID IIDFromString(String lpsz) {\n    char[] buffer = (lpsz + &quot;\\0&quot;).toCharArray();\n    GUID lpiid = new GUID();\n    if (COM.IIDFromString(buffer, lpiid) == COM.S_OK)\n        return lpiid;\n    return null;\n}\n\n&lt;==\nend-text: 7019\nThe results must be:\nstart-text: 7001\n\nAdd a working set\nSearch for something in that working set\nRemove the working set\nSearch via context menu\n\nend-text: 7001\nand\nstart-text: 7019\n20011211\nRan the following compilation unit under the debugger with the breakpoint\nindicated. To get Windows to hit the breakpoint, you have to have the right dl\nand run an accessibility client. If you cannot replicate this problem with a\nsimpler little example, I can walk you through the steps to do this.\nThe only thing different about this CU is that it contains a non-public class\nas well as a public class. When I hit the breakpoint in the debugger, I got a\ndialog that told me that it can't find the source for the non-public class. The\ndialog is very persistent - I have told it OK and Cancel, but it keeps coming\nback. Even if I switch to the Java perspective, I still get the nagging dialog\n. If I kill the process, the dialog does not come back. But the point is\nthat the debugger should be able to see the source for this class - it is right\nin my eclipse workspace. It isn't even hidden in some jar somewhere - it's very\nvisible. I suspect that it's the non-public class thing that is confusing the\nsource lookup. If it helps any, I will attach the dialog. Here's the code:\nend-text: 7019\nin above cases the useless text is between &quot;==&gt;&quot; code &quot;&lt;==&quot; (the arrows aren't in the text)\n...I'm using python now... But I need a tool that clean all the text from code or exceptions... does it exist? because I think that could be useless and wrong to make nlp in these dirty texts...\n"
'Suppose I have a dataframe like below: \n\n        FDT_DATE    FFLT_LATITUDE   FFLT_LONGITUDE  FINT_STAT   FSTR_ID\n51307   1417390467000   31.2899     121.4845    0   112609\n51308   1417390428000   31.2910     121.4859    0   112609\n51309   1417390608000   31.2944     121.4857    1   112609\n51310   1417390548000   31.2940     121.4850    1   112609\n51313   1417390668000   31.2954     121.4886    1   112609\n51314   1417390717000   31.2965     121.4937    1   112609\n53593   1417390758000   31.2946     121.4940    0   112609\n63586   1417390798000   31.2932     121.4960    1   112609\n63587   1417390818000   31.2940     121.4966    1   112609\n63588   1417390827000   31.2946     121.4974    1   112609\n63589   1417390907000   31.2952     121.4986    0   112609\n\n\nI want to extract the location records in a polyline list, means to extract location of the records which have the same FSTR_ID and with the FINT_STAT equals to 1 :\n\n    FSTR_ID    FDT_DATE    POLYLINE\n0   112609    1417390608000 [[31.2944,121.4857],[31.2940,121.4850],[31.2954,121.4886],[31.2965,121.4937]]\n1   112609    1417390798000 [[31.2932,121.4960],[31.2940,121.4966],[31.2946, 121.4974]]\n\n\nHow can I do that?\n\nThe orginal dataset can be generated by this code:\n\nimport pandas as pd\ndf = pd.DataFrame({"FDT_DATE":{"0":1417390467000,"1":1417390428000,"2":1417390608000,"3":1417390548000,"4":1417390668000,"5":1417390717000,"6":1417390758000,"7":1417390798000,"8":1417390818000,"9":1417390827000,"10":1417390907000},"FFLT_LATITUDE":{"0":31.2899,"1":31.291,"2":31.2944,"3":31.294,"4":31.2954,"5":31.2965,"6":31.2946,"7":31.2932,"8":31.294,"9":31.2946,"10":31.2952},"FFLT_LONGITUDE":{"0":121.4845,"1":121.4859,"2":121.4857,"3":121.485,"4":121.4886,"5":121.4937,"6":121.494,"7":121.496,"8":121.4966,"9":121.4974,"10":121.4986},"FINT_STAT":{"0":0,"1":0,"2":1,"3":1,"4":1,"5":1,"6":0,"7":1,"8":1,"9":1,"10":0},"FSTR_ID":{"0":112609,"1":112609,"2":112609,"3":112609,"4":112609,"5":112609,"6":112609,"7":112609,"8":112609,"9":112609,"10":112609}})\ndf = df.sort([\'FDT_DATE\'])\n\n'
'so I\'ve got a very large dataframe of mostly floats (read from a csv) but every now and then, I get a string, or nan\n\n                         date load\n0  2016-07-12 19:04:31.604999    0\n...\n10 2016-07-12 19:04:31.634999    nan\n...\n50 2016-07-12 19:04:31.664999    ".942.197"\n...\n\n\nI can deal with nans (interpolate), but can\'t figure out how to use replace in order to catch strings, and not numbers\n\ndf.replace(to_replace=\'^[a-zA-Z0-9_.-]*$\',regex=True,value = float(\'nan\'))\n\n\nreturns all nans.  I wan\'t nans for only when it\'s actually a string\n'
'I am trying to format one column data. I can find options to split the columns as it has , in between but I am not able to format it as shown in output. \n\nInput \n\n    TITLE,Issn\nNATURE REVIEWS MOLECULAR CELL BIOLOGY,"ISSN 14710072, 14710080"\nANNUAL REVIEW OF IMMUNOLOGY,"ISSN 07320582, 15453278"\nNATURE REVIEWS GENETICS,"ISSN 14710056, 14710064"\nCA - A CANCER JOURNAL FOR CLINICIANS,"ISSN 15424863, 00079235"\nCELL,"ISSN 00928674, 10974172"\nANNUAL REVIEW OF ASTRONOMY AND ASTROPHYSICS,"ISSN 15454282, 00664146"\nNATURE REVIEWS IMMUNOLOGY,"ISSN 14741741, 14741733"\nNATURE REVIEWS CANCER,ISSN 1474175X\nANNUAL REVIEW OF BIOCHEMISTRY,"ISSN 15454509, 00664154"\nREVIEWS OF MODERN PHYSICS,"ISSN 00346861, 15390756"\nNATURE GENETICS,ISSN 10614036\n\n\n\nSplit the issn column to two columns as it has ,\nDelete the word ISSN from column only\nleave behind numbers  After 4 digits put a -\n\n\nExpected output is \n\n    TITLE,Issn\nNATURE REVIEWS MOLECULAR CELL BIOLOGY,1471-0072, 1471-0080\nANNUAL REVIEW OF IMMUNOLOGY,0732-0582, 1545-3278\nNATURE REVIEWS GENETICS,1471-0056, 1471-0064\nCA - A CANCER JOURNAL FOR CLINICIANS,1542-4863, 0007-9235\nCELL,0092-8674, 1097-4172\nANNUAL REVIEW OF ASTRONOMY AND ASTROPHYSICS,1545-4282, 0066-4146\nNATURE REVIEWS IMMUNOLOGY,1474-1741, 1474-1733\nNATURE REVIEWS CANCER, 1474-175X\nANNUAL REVIEW OF BIOCHEMISTRY,1545-4509, 0066-4154\nREVIEWS OF MODERN PHYSICS,0034-6861, 1539-0756\nNATURE GENETICS,1061-4036\n\n\nAny suggestion with pandas are appreciated .. Thanks in advance \n\nUpdate:\nWhen trying to run both the programs as mentioned in answer \n\nimport pandas as pd\nimport re\n\ndf = pd.read_csv(\'new_journal_list.csv\', header=\'TITLE,Issn\')\n\n\'\'\'\ndf_split_num = df[\'Issn\'].map(lambda x: x.split(\'ISSN \')[1].split(\', \'))\ndf_dash_num = df_split_num.map(lambda x: [num[:4] + \'-\' + num[4:] for num in x])\n\ndf_split_issn = pd.DataFrame(data=list(df_dash_num), columns=[\'Issn1\', \'Issn2\'])\ndf[[\'Issn1\', \'Issn2\']] = df_split_issn\ndel df[\'Issn\']\n\nprint df\n\n\'\'\'\n\ndf[[\'Issn1\',\'Issn2\']] = (df.pop(\'Issn\').str.extract(\'ISSN\\s+([^,]+),?\\s?(.*)\', expand=True)\n                   .apply(lambda x: x.str[:4]+\'-\'+x.str[4:]).replace(r\'^-$\', \'\', regex=True))\n\nprint df\n\n\nEither cases when run in default python 2.7 I am getting following error \n\nTraceback (most recent call last):\n  File "clean_journal_list.py", line 1, in &lt;module&gt;\n    import pandas as pd\n  File "/usr/local/lib/python2.7/dist-packages/pandas/__init__.py", line 25, in &lt;module&gt;\n    from pandas import hashtable, tslib, lib\n  File "pandas/src/numpy.pxd", line 157, in init pandas.hashtable (pandas/hashtable.c:38364)\n\n\nWhen run in python 3.4 the below given error is seen \n\nFile "clean_journal_list.py", line 21\n    print df\n           ^\nSyntaxError: invalid syntax\n\n'
'I am doing 2 things.\n1) filter a dataframe in pandas\n2) clean unicode text in a specific column in the filtered dataframe.\n\nimport pandas as pd\nimport probablepeople\nfrom unidecode import unidecode\nimport re\n\n#read data\ndf1 = pd.read_csv("H:\\\\data.csv")\n#filter\ndf1=df1[(df1.gender=="female")]\n#reset index because otherwise indexes will be as per original dataframe\ndf1=df1.reset_index()\n\n\nNow i am trying to clean unicode text in the address column\n\n#clean unicode text\nfor i in range(10):\n    df1.loc[i][16] = re.sub(r"[^a-zA-Z.,\' ]",r\' \',df1.address[i])\n\n\nHowever, i am unable to do so and below is the error i am getting.\n\nc:\\python27\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n\n'
"I have a Pandas dataframe that looks like:\n\nID, Customer, Status,  Score, Size\n01, Cust-A,   NaN,     100,   A\n01, Cust-A,   Valid,   100,   A\n02, Cust-B,   Invalid, 80,    B\n02, Cust-B,   Invalid, NaN,   B\n03, Cust-C,   Valid,   95,    C\n04, Cust-D,   Invalid, 76,    NaN\n04, Cust-D,   NaN,     76,    NaN\n...\n\n\nAnd so on.\n\nHow can I drop the correct row? \n\nI'd like to drop the first row in the case of ID-01, and the second of ID-02, in the case of ID-04 I'd like to keep the first one since its the one with less NaNs\n"
"I am trying to create a choice model dataset from multiple csv (Think of whether individuals buy a brand of a product at a given price). \n\nA small representation of my data:\n\nimport pandas as pd\n\nd1 = {'Product': [1,1,2,2,3], 'Price': [25, 25, 22, 22,35], 'Buyer ID': ['A','B','C','D','E']}\ndf1 = pd.DataFrame(d1)  \n\n\nWhere df1 contains information relating to products that a buyer considered purchasing. Note that all three products (1,2 &amp;3) are available to the consumer when they are making their purchase decision.\n\nd2 = {'Buyer Num': ['A','B','E'], 'Product': [1,1,3,], 'Purchase Decision': ['Yes','Yes','Yes']}\ndf2 = pd.DataFrame(d2)\n\n\ndf2 contains information on which product a consumer finally purchased. Consumers A, B and E bought products 1,1 &amp; 3 respectively.\n\nI tried merging the two datasets using outer and inner join. For eg: \n\ndf3 = df1.merge(df2,left_on='Buyer ID', right_on='Buyer Num', how='outer')\n\n\nWhat I get from an outer join is :\n\n   Buyer ID  Price  Product_x Buyer Num  Product_y Purchase Decision\n     A        25          1         A        1.0               Yes\n     B        25          1         B        1.0               Yes\n     C        22          2       NaN        NaN               NaN\n     D        22          2       NaN        NaN               NaN\n     E        35          3         E        3.0               Yes\n\n\nHowever what I optimally want is something like this-\n\nBuyer ID    Price   Product  Purchase Decision\nA             25        1      Yes\nB             25        1      Yes\nC             25        1      No\nD             25        1      No\nE             25        1      No\nA             22        2      No\nB             22        2      No\nC             22        2      No\nD             22        2      No\nE             22        2      No\nA             35        3      No\nB             35        3      No\nC             35        3      No\nD             35        3      No\nE             35        3      Yes\n\n\nCould someone tell me to how to do this on Python?\n"
'I have dataset, in which I read the data, df.dir.value_counts() returns\n\n169      23042\n170      22934\n168      22873\n316      22872\n315      22809\n171      22731\n317      22586\n323      22561\n318      22530\n\n         ...  \n0.069        1\n0.167        1\n0557         1\n0.093        1\n1455         1\n0.130        1\n0.683        1\n2211         1\n3.714        1\n1.093        1\n0819         1\n0.183        1\n0.110        1\n2241         1\n0.34         1\n0.330        1\n0.563        1\n60+9         1\n0.910        1\n0.232        1\n1410         1\n0.490        1\n0.107        1\n1.257        1\n1704         1\n0.491        1\n1.180        1\n5-230        1\n1735         1\n1.384        1\n\n\nThe dir column is about direction, and the data should be integer, ranging from (0,361). As you can see, there are a lot of errones data at the end of the value_counts() list.\n\nI want to know, how can I drop the non-integer data?\n\n\n\nThere are some possible ways\n\n1.read_csv as integer and throw all non-integer data\n\ndf = pd.read_csv("/data.dat", names = [\'time\', \'dir\'], dtype={\'dir\': int}})\n\n\nHowever, there some string like error data, such as 60+9, which would cause error. I don\'t know how to handle it.\n\n2.Select by isdigit(), and then do a downcast\n\ndf = df[df[\'dir\'].apply(lambda x: str(x).isdigit())]\ndf[\'dir\']=pd.to_numeric(df[\'dir\'], downcast=\'integer\', errors=\'coerce\')\n\n\nThis is from Drop rows if value in a specific column is not an integer in pandas dataframe, and works fine for me, but it feels a little bit too much. I\'m wondering if there are better approaches?\n'
'I have recently started using Scrapy and am trying to clean some data I have scraped and want to export to CSV, namely the following three examples:\n\n\nExample 1 – removing certain text  \nExample 2 – removing/replacing unwanted characters   \nExample 3 –splitting comma separated text\n\n\nExample 1 data looks like: \n\n\n  Text I want,Text I don’t want\n\n\nUsing the following code: \n\n\'Scraped 1\': response.xpath(\'//div/div/div/h1/span/text()\').extract()\n\nExample 2 data looks like:\n\n\n  Â - but I want to change this to £\n\n\nUsing the following code: \n\n\' Scraped 2\': response.xpath(\'//html/body/div/div/section/div/form/div/div/em/text()\').extract()\n\n\nExample 3 data looks like:\n\n\n  Item 1,Item 2,Item 3,Item 4,Item 4,Item5 – ultimately I want to split\n  this into separate columns in a CSV file\n\n\nUsing the following code: \n\n\' Scraped 3\': response.xpath(\'//div/div/div/ul/li/p/text()\').extract()\n\n\nI have tried using str.replace(), but can’t seem to get that to work, e.g:\n\'Scraped 1\': response.xpath(\'//div/div/div/h1/span/text()\').extract((str.replace(",Text I don\'t want",""))\n\nI am looking into this but what appreciate if anyone could point me in the right direction! \n\nCode below: \n\nimport scrapy\nfrom scrapy.loader import ItemLoader\nfrom tutorial.items import Product\n\n\nclass QuotesSpider(scrapy.Spider):\n    name = "quotes_product"\n    start_urls = [\n        \'http://www.unitestudents.com/\',\n            ]\n\n    # Step 1\n    def parse(self, response):\n        for city in response.xpath(\'//select[@id="frm_homeSelect_city"]/option[not(contains(text(),"Select your city"))]/text()\').extract(): # Select all cities listed in the select (exclude the "Select your city" option)\n            yield scrapy.Request(response.urljoin("/"+city), callback=self.parse_citypage)\n\n    # Step 2\n    def parse_citypage(self, response):\n        for url in response.xpath(\'//div[@class="property-header"]/h3/span/a/@href\').extract(): #Select for each property the url\n            yield scrapy.Request(response.urljoin(url), callback=self.parse_unitpage)\n\n\n    # Step 3\n    def parse_unitpage(self, response):\n        for final in response.xpath(\'//div/div/div[@class="content__btn"]/a/@href\').extract(): #Select final page for data scrape\n            yield scrapy.Request(response.urljoin(final), callback=self.parse_final)\n\n    #Step 4 \n    def parse_final(self, response):\n        unitTypes = response.xpath(\'//html/body/div\').extract()\n        for unitType in unitTypes: # There can be multiple unit types so we yield an item for each unit type we can find.\n            l = ItemLoader(item=Product(), response=response)\n            l.add_xpath(\'area_name\', \'//div/ul/li/a/span/text()\')\n            l.add_xpath(\'type\', \'//div/div/div/h1/span/text()\')\n            l.add_xpath(\'period\', \'/html/body/div/div/section/div/form/h4/span/text()\')\n            l.add_xpath(\'duration_weekly\', \'//html/body/div/div/section/div/form/div/div/em/text()\')\n            l.add_xpath(\'guide_total\', \'//html/body/div/div/section/div/form/div/div/p/text()\')\n            l.add_xpath(\'amenities\',\'//div/div/div/ul/li/p/text()\')\n            return l.load_item()\n\n\nHowever, I\'m getting the following? \n\nvalue = self.item.fields[field_name].get(key, default)\nKeyError: \'type\'\n\n'
'i am trying to break the name into two parts and keeping first name last name and finally replacing the common part in all of them such that first name is must then last name then if middle name remain it is added to column\n\ndf[\'owner1_first_name\'] = df[\'owner1_name\'].str.split().str[0].astype(str, \nerrors=\'ignore\')\ndf[\'owner1_last_name\'] = \ndf[\'owner1_name\'].str.split().str[-1].str.replace(df[\'owner1_first_name\'], \n"").astype(str, errors=\'ignore\')\n[\'owner1_middle_name\'] = \ndf[\'owner1_name\'].str.replace(df[\'owner1_first_name\'], \n"").str.replace(df[\'owner1_last_name\'], "").astype(str, errors=\'ignore\')\n\n\nthe problem is i am not able to use \n    .str.replace(df[\'owner1_name\'], "")\nas  i am getting an error \n    "TypeError: \'Series\' objects are mutable, thus they cannot be hashed"\n\nis there any replacement sytax in pandas for what i am tryin to achieve\n\nmy desired output is \n\nfull name = THOMAS MARY D which is in column owner1_name\n\nI want \n\nowner1_first_name = THOMAS\nowner1_middle_name = MARY\nowner1_last_name = D\n\n'
"Dears, \n\nrecently I use crawler to fetch information from website, and get a column of data like this:\n\n|               **Hotel Info**           |  \n| 2014 open    2016 retrofit    50 rooms |  \n| 60 rooms                               |       \n| 2012 open    100 rooms                 |\n| 80 rooms                               |\n| 2010 open                              |\n\n\nI want it to be like this finally:\n\n| **Hotel Open** | **Hotel Retrofit** | **Hotel Rooms** |\n|   2014         |   2016             |   50            |\n|   null         |   null             |   60            |\n|   2012         |   null             |   100           |\n|   null         |   null             |   80            |\n|   2010         |   null             |   null          |\n\n\nNOTE:\nThe original website doesn't split these 3 'information blocks' separately. They are all under a &lt;p&gt;...&lt;/p&gt;  block. Therefore I cannot avoid this issue.  \n\nI am using Python, and totally new in it. Please help me and THANK YOU VERY MUCH!!!\n"
'I have some data where every second column corresponds to a specific time, each time period have respectively \'buy\' and \'sell\' positions, and each of these position have two factors (as seen below). However the columns have unequal lengths, and so the \'sell\' options start at different rows (buried among the values).\n\ntime, time1, time,  time2,  time, time3\nbuy,       , buy,        ,  buy,    \nfactor1,  1, factor1,    2, factor1,  3\nfactor2,  4, factor2,    5, factor2,  6\nfactor1,  7, factor1,    8, factor1,  9\nfactor2, 10, factor2,   11, factor2, 12\nfactor1, 13, sell,        , factor1, 14\nfactor2, 15, factor1,   16, factor2, 17\nsell,      , factor2,   18, factor1, 19\nfactor1, 20, ,            , factor2, 21,\nfactor2, 22, ,            , sell,     \n,          , ,            , factor1, 23\n,          , ,            , factor2, 24\n,          , ,            , factor1, 25\n,          , ,            , factor2, 26\n\n\nUltimately, I would like I table structured like below.\n\ntime,   position,   factor,     value\ntime1,  buy,        factor1,    1\ntime1,  buy,        factor2,    4\ntime1,  buy,        factor1,    7\ntime1,  buy,        factor2,    10\ntime1,  buy,        factor1,    13\ntime1,  buy,        factor2,    15\ntime1,  sell,       factor1,    20\ntime1,  sell,       factor2,    22\ntime2,  buy,        factor1,    2\ntime2,  buy,        factor2,    5\ntime2,  buy,        factor1,    8\ntime2,  buy,        factor2,    11\ntime2,  sell,       factor1,    16\ntime2,  sell,       factor2,    18\ntime3,  buy,        factor1,    3\ntime3,  buy,        factor2,    6\ntime3,  buy,        factor1,    9\ntime3,  buy,        factor2,    12\ntime3,  buy,        factor1,    14\ntime3,  buy,        factor2,    17\ntime3,  buy,        factor1,    19\ntime3,  buy,        factor2,    21\ntime3,  sell,       factor1,    23\ntime3,  sell,       factor2,    24\ntime3,  sell,       factor1,    25\ntime3,  sell,       factor2,    26\n\n\nI am able to extract the indices and then create respectively \'buy\' and \'sell\' list in R. But I am unsure whether this is the easiest approach (I have many such files, and would prefer a fast automatic method). I am also open to making the transformation i Python, rather than R.\n\n# For each column find the index of buy, sell (and the corresponding empty cell)\nidx = apply(data, 2, function(x) which(x %in% c("buy","sell",""))[1:3] )\n# NA indicates that the empty cell is the last\nidx[is.na(idx)] = nrow(data)\n\ni = 0\nbuy = list( apply(idx, 2, function(x) {\n  i &lt;&lt;- i+1\n  data[seq(x[1]+1,x[2]),i]\n}) )\ni = 0\nsell = list( apply(idx, 2, function(x) {\n  i &lt;&lt;- i+1\n  data[seq(x[2]+1,x[3]),i]\n}) )\n\n'
"Pandas Manipulation DF Question Here\n\nI want to create a new column in my original DF (df) that is a value at a specific index from another DF (dfKey). \n\nI am a bit stuck (I'm sure I'm missing something obvious but I can't decode the current error message 'KeyError: 'Name').\n\nData:\n\nimport numpy as np\nimport pandas as pd\nraw_data = {'Code': [250, 200, 875, 1200],\n    'Metric1': [1.4, 350, 0.2, 500],\n    'Metric999': [1.2, 375, 0.22, 505],} \ndf = pd.DataFrame(raw_data, columns = ['Code','Metric1', 'Metric999',])\n\ndf.set_index('Code', inplace=True) #Set Code as Row Index\nprint(df)\n\nraw_dataKey = {'Code': [250, 1200, 205, 2899, 875, 5005],\n    'Ticker': ['NVID', 'ATVI', 'CRM', 'GOOGL', 'TSLA','GE', ],       \n    'Name': ['NVIDA Corp', 'Activision', 'SalesForce', 'Googlyness', 'Tesla Company','General Electric']} \ndfKey = pd.DataFrame(raw_dataKey , columns = ['Code','Ticker', 'Name'])\ndfKey.set_index('Code', inplace=True) #Set Code as Row Index\nprint(dfKey)\n\n\nDesired Output (df.head()):\n\n      Ticker           Name  Code  Metric1  Metric999\nCode  \n250     NVID     NVIDA Corp   250      1.4       1.20\n200      NaN            NaN   200    350.0     375.00\n875     TSLA  Tesla Company   875      0.2       0.22\n1200    ATVI     Activision  1200    500.0     505.00\n\n\nI think the best way to do this is a for loop, as all the other methods I've tried (such as df['Name']=np.where(df['Code']==dfKey['Code'], dfKey['Name'])) only compare/test each row at the same index; no searching. \n\nMy latest attempt:\n\ncodes=df.index.tolist()\ncodes\n\nfor code in codes:\n    #1. Find Name and Ticker in Key\n    name = dfKey['Name'].loc[code]\n    ticker = dfKey['Ticker'].loc[code]\n    #2. Put Name and Ticker back in original\n    df['Name'].loc[code] = name \n    df['Ticker'].loc[code] = ticker \n\n"
'If I have a file of inputs with loose form (when I say loose form I mean that not all lines contain all information as is explained later on): \n\n23 1990-10-10 Clark Kent\n\n\nAnd I want to define a group for age, date, and name, how do I go about extracting these into a named groupdict() such as\n\n{ age: 23, date: \'1990-10-10\', name: \'Clark Kent\' }\n\n\nIf fields age or date are missing, such as:\n\n1990-10-10 Clark Kent\n\n\nor\n\n23 Clark Kent\n\n\nThe groups should still be able to be parsed and return None for the fields that it couldn\'t find.\n\n{ age: 23, date: None, name: \'Clark Kent\' }\n\n\nNow:\n\nre.match(r\'(?P&lt;age&gt;[0-9]+)?\\s*(?P&lt;birthday&gt;\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d)?\\s*(?P&lt;name&gt;(\\w|\\s)+)\',\n "23 1990-10-10 Clark Kent")\n\n\nReturns the desired output.\n\nWhen however the testing string is:\n\n"1990-10-10 Clark Kent"\n\n\nThen the age parameter grabs the initial 199 greedily and the birthday fails to be parsed correctly.\n\nHow would you go about parsing this file to permissively grab whatever fields can be grabbed?\n'
"Current dataframe\n\nid, date, quantity\n1,2017-08-01,22\n2,1900-01-01,31\n3,2017-08-01,44\n4,2017-08-02,12\n5,1900-01-01,22\n6,1900-01-01,31\n7,2017-08-02,44\n8,2017-08-03,12\n\n\nDesired output\n\nid, date, quantity\n1,2017-08-01,22\n2,2017-08-01,31\n3,2017-08-01,44\n4,2017-08-02,12\n5,2017-08-02,22\n6,2017-08-02,31\n7,2017-08-02,44\n8,2017-08-03,12\n\n\nThere's only a few in the data that I just used set_value and did it manually, but I was wondering if there's a way to do it with a method.\nThanks in advance!\n"
"I have a pandas data frame that contains 2 columns W(number of Wins) and L(number of Losses).\nI would like to eliminate all rows of data that have a value of 0 for both W and L.\n\npitching_df.groupby('playerID')['W', 'L'].sum()\n\nplayerID    W   L\naardsda01   2   5\naasedo01    3   8\nabbotpa01   0   0\nabernte02   8   19\n\n"
'I have a data set which contains state code and its status.\n\n  code  status\n1   AZ  a\n2   CA  b\n3   KS  c\n4   MO  c\n5   NY  d\n6   AZ  d\n7   MO  a\n8   MO  b\n9   MN  b\n10  NV  a\n11  NV  e\n12  MO  f\n13  NY  a\n14  NY  a\n15  NY  b\n\n\nI want to filter out this data set which code contains only a status and count how many they have. Example output will be,\n\n  code  status  \n1   AZ  a   \n2   MO  a   \n3   NY  a   \n\n    AZ =1   MO = 1  NY =2\n\n\nI used df.groupyby("code").loc[df.status == \'a\'] but didn\'t have any luck.\nAny help appreciated!\n'
"Hi I have a dataset in the following format:\n\n\n\nCode for replicating the data:\n\nimport pandas as pd\nd1 = {'Year': \n['2008','2008','2008','2008','2008','2008','2008','2008','2008','2008'],\n  'Month':['1','1','2','6','7','8','8','11','12','12'],\n'Day':['6','22','6','18','3','10','14','6','16','24'],\n'Subject_A':['','30','','','','35','','','',''],\n'Subject_B':['','','','','','','','40','',''],\n'Subject_C': ['','','','','','65','','50','','']}\nd1 = pd.DataFrame(d1)\n\n\nI input the numbers as a string to show blank cells\n\nWhere the first three columns denotes date (Year, Month and Day) and the following columns represent individuals (My actual data file consists of about 300 such rows and about 1000 subjects. I presented a subset of the data here).  \n\nWhere the column value refers to expenditure on FMCG products.\nWhat I would like to do is the following:\n\nPart 1 (Beginning and end points)\n\na) For each individual locate the first observation and duplicate the value of the first observation for atleast the previous six months. For example: Subject C's 1st observation is on the 10th of August 2008. In that case I would want all the rows from June 10, 2008 to be equal to 65 for Subject C (Roughly 2/12/2008\nis the cutoff date. SO we leave the 3rd cell from the top for Subject_C's column blank).\n\nb) Locate last observation and repeat the last observation for the following 3 months. For example for Subject_A, we repeat 35 twice (till 6th November 2008).\n\nPlease refer to the following diagram for the highlighted cell with the solutions.\n\n\n\nPart II - (Rows in between)\n\nNext I would like to do two things (I would need to do the following three steps separately, not all at one time):\n\nFor individuals like Subject_A, locate two observations that come one after the other (30 and 35).\n\ni)  Use the average of the two observations. In this case we would have 32.5 in the four rows without caring about time.\n\nfor eg:\n\n\n\nii) Find the total time between two observations and take the mean of the time. For the 1st half of the time period assign the first value and for the 2nd half assign the second value. For example - for subject 1, the total days between 01/22/208 and 08/10/2008 is 201 days. For the first 201/2 = 100.5 days assign the value of 30 to Subject_A and for the remaining value assign 35. In this case the columns for Subject_A and Subject_C will look like:\n\n\n\nThe final dataset will use (a), (b) &amp; (i) or (a), (b) &amp; (ii)\n\nFinal data I [using a,b and i]\n\n\n\nFinal data II [using a,b and ii]\n\n\n\nI would appreciate any help with this. Thanks in advance. Please let me know if the steps are unclear.\n\nFollow  up question and Issues\n\nThanks @Juan for the initial answer. Here's my follow up question. Suppose that Subject_A has more than 2 observations (code for the example data below). Would we be able to extend this code to incorporate more than 2 observations?\n\nimport pandas as pd\nd1 = {'Year': \n['2008','2008','2008','2008','2008','2008','2008','2008','2008','2008'],\n  'Month':['1','1','2','6','7','8','8','11','12','12'],\n'Day':['6','22','6','18','3','10','14','6','16','24'],\n'Subject_A':['','30','','45','','35','','','',''],\n'Subject_B':['','','','','','','','40','',''],\n'Subject_C': ['','','','','','65','','50','','']}\nd1 = pd.DataFrame(d1)\n\n\nIssues\nFor the current code, I found an issue for part II (ii). This is the output that I get:\n\n\n\nThis is actually on the right track. The two cells above 35 does not seem to get updated. Is there something wrong on my end? Also the same question as before, would we be able to extend it to the case of >2 observations?\n"
'I have a regular expression that I want to apply to each line of a CSV file.\n\nHere\'s the function which basically removes all comma\'s encountered before a single digit number. The function is working perfectly fine for the string.\n\nInput : text = "52A, XYZ Street, ABC District, 2, M, Brown\nFunction : re.sub(\'(?&lt;!\\s[\\dA-Z]),(?!\\s+\\d,?)\', \'\', text)\nOutput : \'52A XYZ Street ABC District, 2, M, Brown\'\n\n\nHowever, I have a CSV file containing hundreds of such lines. For instance\n\n1, 5273249, 1061/72, 150-CF, S/O:XVZ, 1, ABX, 45, 0, Husband, 9213\n1, 5272849, 1063/36, 150-AS, S/O:XVZ, 1, ABX, 45, 0, Wife, 9253\n1, 5274549, 10626/12, 150-RT, S/O:XVZ, 1, ABX, 45, 0, Son, 9214\n\n\nI tried to read it using CSV reader and apply the function but unfortunately, it\'s not producing any output. What did I do wrong here:\n\ndef myFunction(text):\n    return re.sub(\'(?&lt;!\\s[\\dA-Z]),(?!\\s+\\d,?)\', \'\', text)\n\nimport csv\nwith open(\'temp1.csv\', \'r\') as csvfile:\n    spamreader = csv.reader(csvfile, delimiter=\',\')\n    for row in spamreader:\n        l = \',\'.join(row)    \n        myFunction(l)\n\n'
"Can someone please help me understand the steps to convert a Python pandas DataFrame that is in record form (data set A), into one that is pivoted with nested columns (as shown in data set B)?\n\nFor this question the underlying schema has the following rules:\n\n\nEach ProjectID appears once \nEach ProjectID is associated to a single PM\nEach ProjectID is associated to a single Category  \nMultiple ProjectIDs can be associated with a single Category \nMultiple ProjectIDs can be associated with a single PM\n\n\nInput Data Set A\n\ndf_A = pd.DataFrame({'ProjectID':[1,2,3,4,5,6,7,8],\n          'PM':['Bob','Jill','Jack','Jack','Jill','Amy','Jill','Jack'],\n          'Category':['Category A','Category B','Category C','Category B','Category A','Category D','Category B','Category B'],\n          'Comments':['Justification 1','Justification 2','Justification 3','Justification 4','Justification 5','Justification 6','Justification 7','Justification 8'],\n          'Score':[10,7,10,5,15,10,0,2]})\n\n\n\n\nDesired Output \n\nNotice above the addition of a nested index across the columns.  Also notice that 'Comments' and 'Score' both appear at the same level beneath 'ProjectID'.  Finally see how the desired output does NOT aggregate any data, but groups/merges the category data into one row per category value.\n\nI have tried so far:\n\n\ndf_A.set_index(['Category','ProjectID'],append=True).unstack() - This would only work if I first create a nested index of ['Category','ProjectID] and ADD that to the original numerical index created with a standard dataframe, however it repeats each instance of a Category/ProjectID match as its own row (because of the original index).\ndf_A.groupby() - I wasn't able to use this because it appears to force aggregation of some sort in order to get all of the values of a single category on a single row.\ndf_A.pivot('Category','ProjectID',values='Comments') - I can perform a pivot to avoid unwanted aggregation and it starts to look similar to my intended output, but can only see the 'Comments' field and also cannot set nested columns this way.  I receive an error when trying to set values=['Comments','Score'] in the pivot statement.\n\n\nI think the answer is somewhere between pivot, unstack, set_index, or groupby, but I don't know how to complete the pivot, and then add the appropriate nested column index.\n\nI'd appreciate any thoughts you all have.\nQuestion updated based on Mr. T's comments. Thank you.\n"
"Say I have this DataFrame:\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(['eggs, spam', 'lobster', 'eggs, lobster'], columns=['ingredients'])\n&gt;&gt;&gt; df\n     ingredients\n0     eggs, spam\n1        lobster\n2  eggs, lobster\n\n\nI would like to split the column 'ingredients' based on the delimiter ', '. I would like to end up with a DataFrame that looks like this:\n\n     ingredients\n0     eggs\n1     spam\n2     lobster\n3     eggs\n4     lobster\n\n\nHow can I achieve this?\n"
'i have a data column like this:\n\ndf[\'zone\'].unique()\n\nout[4]: \n\narray([\'BROOKLYN\', \'BRONX\', \'07 BRONX\', \'Unspecified\', \'05 BRONX\',\n       \'QUEENS\', \'MANHATTAN\', \'07 MANHATTAN\', \'STATEN ISLAND\',\n       \'17 BROOKLYN\', \'0 Unspecified\', \'Unspecified MANHATTAN\',\n       \'12 BROOKLYN\', \'07 BROOKLYN\', \'09 MANHATTAN\', \'01 STATEN ISLAND\',\n       \'12 MANHATTAN\', \'04 QUEENS\', \'06 BROOKLYN\',\n       \'01/04/2016 01:45:00 PM\', \'01/02/2016 05:43:34 AM\', \'07 QUEENS\',\n       \'11 BRONX\', \'01/04/2016 03:45:00 PM\', \'10 MANHATTAN\', \'03 BRONX\',\n       \'04 BRONX\', \' or 311 Online."\', \'01/13/2016 12:00:00 AM\',\n       \'04 BROOKLYN\', \'03 BROOKLYN\', \'01 QUEENS\',\n       \'01/04/2016 03:34:55 PM\', \'08 MANHATTAN\', \'14 BROOKLYN\',\n       \'10 QUEENS\', \'Unspecified STATEN ISLAND\', \'02 BRONX\', \'09 BRONX\',\n       \'08 QUEENS\', \'10 BRONX\', \'03 MANHATTAN\', \'12 QUEENS\',\n       \' please call (212) NEW-YORK (212-639-9675)."\',\n       \'Unspecified BROOKLYN\', \'01/11/2016 04:45:00 PM\', \'04 MANHATTAN\',\n       \'01 BRONX\', \'09 BROOKLYN\', \'01/05/2016 07:00:00 AM\', \'18 BROOKLYN\',\n       \'01/08/2016 09:00:00 AM\', \'01 BROOKLYN\', \'06 BRONX\',\n       \'01 MANHATTAN\', \'01/06/2016 12:15:00 PM\', \'02/04/2016 08:45:00 PM\',\n       \'01/05/2016 12:45:00 PM\', \' no action was taken."\', \'05 BROOKLYN\',\n       \'08 BROOKLYN\', \'Unspecified QUEENS\', \'01/08/2016 03:00:00 PM\',\n       \'08/22/2016 12:00:00 AM\', \'13 BROOKLYN\', \'02 QUEENS\', \'14 QUEENS\',\n       \'01/05/2016 08:45:00 AM\', \'11 QUEENS\', \'02 MANHATTAN\',\n       \'01/08/2016 10:05:00 AM\', \'01/05/2016 01:05:00 PM\',\n       \'Unspecified BRONX\', \'06 QUEENS\', \'09 QUEENS\', \'15 BROOKLYN\',\n       \'01/07/2016 09:25:00 AM\', \'02 STATEN ISLAND\',\n       \'01/02/2016 12:00:00 PM\', \'01/06/2016 08:45:00 PM\',\n       \'04/04/2016 12:00:00 AM\', \'01/06/2016 08:30:00 AM\'])\n\n\nas you can see, i have a lot of mixed types there, everything is being categorized by pandas as string object. I have tried already some parameters in the pd.read_csv command like low_memory = False, chunksize, etc... without any success.\n\nWhat i really need to do here though is to kind of map this column into the following format:\n\n(Manhattan -&gt; 1, Brooklyn -&gt; 2, Queens -&gt; 3, Staten Island -&gt; 4, Bronx -&gt; 5, Other -&gt; 0)\n\n\ni also need to include the string \'07 BRONX\' as bronx and not as other or unknown. \n\nI have thought about the .map() method as the way to go, but since the column is a real mess of mixed types, i\'m not sure anymore about what my options are.\n\nI will appreciate any suggestions here.\n\nThanks a lot in advance\n'
"\n\nHow to find a smart solution to turn Year_Q to datetime? I tried to use \n\npd.to_datetime(working_visa_nationality['Year_Q'])\n\n\nbut got error says that this cannot be recognized. So I tried a stupid way as:\n\nworking_visa_nationality['Year'] = working_visa_nationality.Year_Q.str.slice(0,4)\nworking_visa_nationality['Quarter'] = working_visa_nationality.Year_Q.str.slice(6,8)\n\n\n\n\nAnd now I found a problem: it is true that I can groupby data by the year, but it is difficult to include the quarter to my line plot.\n\nSo how to make 2010 Q1 like 2010-3-31?\n"
"I have a Data(csv format) where the first column is an epoch timestamp(strictly increasing) and the other columns are cumulative rows(just increasing or equal).\nSample is as below:\n\ndf = pandas.DataFrame([[1515288240, 100, 50, 90, 70],[1515288241, 101, 60, 95, 75],[1515288242, 110, 70, 100, 80],[1515288239, 110, 70, 110, 85],[1515288241, 110, 75, 110, 85],[1515288243,110,70,110,85]],columns =['UNIX_TS','A','B','C','D'])\ndf =\nid    UNIX_TS  A   B   C  D\n 0 1515288240 100 50  90 70\n 1 1515288241 101 60  95 75\n 2 1515288242 110 70 100 80\n 3 1515288239 110 70 110 85\n 4 1515288241 110 75 110 85\n 5 1515288243 110 70 110 85\n\nimport pandas as pd\ndef clean(df,column_name,equl):\n    i=0\n    while(df.shape[0]-2&gt;=i):\n        if df[column_name].iloc[i]&gt;df[column_name].iloc[i+1]:\n            df.drop(df[column_name].iloc[[i+1]].index,inplace=True)\n            continue\n        elif df[column_name].iloc[i]==df[column_name].iloc[i+1] and equl==1:\n            df.drop(df[column_name].iloc[[i+1]].index,inplace=True)\n            continue\n        i+=1\n\nclean(df,'UNIX_TS',1)\nfor col in df.columns[1:]:\n    clean(df,col,0)\n\ndf =\n    id    UNIX_TS  A   B   C  D\n     0 1515288240 100 50  90 70\n     1 1515288241 101 60  95 75\n     2 1515288242 110 70 100 80\n\n\nMy script works as intended but its too slow, anybody has ideas about how to improve its speed.\n\nI wrote a script to remove all the invalid data based on 2 rules:\n\n\nUnix_TS must be strictly increasing(because its a time, it cannot flow back or  pause),\nother columns are increasing and can be constant for example is in one row it is 100 and the next row it can be >=100 but not less. \n\n\nBased on the rules the index 3 &amp; 4 are invalid because unix_ts 1515288239 is 1515288241 are less than the index 2.\nindex 5 is wrong because the value B decreased   \n"
'Hoping someone can help me with the logic of transforming this Excel logic to python\n\n=IF(LEFT(A8,5)="Total",A9,I8)\n\n\nSo I am looking to find everything in a range and then creating a new column with the first element in the range. The problem is that the names of the ranges can change.\n\nA current solution I have implemented is converting a column to the index, and by manually selecting by index names by doing the following:\n\nSales = df.loc[\'1000 - Cash and Equivalents\':\'Total - 1000 - Cash and Equivalents\']\n\n\nThe problem this name may change and may contain fewer or more rows, and need to make this more versatile, so I can not specify a numbered range.\n\nThis is an example of the data:\n\n\n\nand Post transformation I have the data looking like the following:\n\n\n'
'I am working on a problem in which I have a very large dataset in the form of a csv file. This csv file has various columns, one of which is state code. The dataset is significantly larger than I need it to be- I just need the values from one state. \n\nA solution I was thinking of using would be to read in the csv file using Python and then write to a new file with just the rows that I need- removing 49 of the 50 states (U.S). \n\nThe csv file has 3million+ rows. I am new to Python and I am not sure how I can effectively do this, what are the best ways to complete this task?\n\nThank you for your help and I apologize if this seems like a simple question- I am new to Python. \n'
"I have the following dataframe:\n\nIndex   Recipe_ID   order   content\n0       1285        1       Heat oil in a large frypan with lid over mediu...\n1       1285        2       Meanwhile, add cauliflower to a pot of boiling...\n2       1285        3       Remove lid from chicken and let simmer uncover... \n3       1289        1       To make the dressing, whisk oil, vinegar and m...\n4       1289        2       Cook potatoes in a large saucepan of boiling w..\n\n\nTask: I need to get the contents in one cell:\n\ndf = df.groupby('recipe_variation_part_id', as_index=False).agg(lambda x: x.tolist())\n\n\nThis returns the following:\n\nIndex   Recipe_ID   order         content\n0       1285        [1, 2, 3]     [Heat oil in a large frypan with lid over medi...\n1       1289        [1, 2, 3]     [To make the dressing, whisk oil, vinegar and ...\n2       1297        [1, 2, 4, 3]  [Place egg in saucepan of cold water and bring...\n3       1301        [1, 2]        [Preheat a non-stick frying pan and pan fry th...\n4       1309        [2, 3, 4, 1]  [Meanwhile, cook noodles according to package ...\n\n\nIf you look at the first recipe entry, you get the following:\n\n['Heat oil in a large frypan with lid over medium-high heat. Cook onions, garlic and rosemary for a couple of minutes until soft. Add chicken and brown on both sides for a few minutes, then add in tomatoes and olives. Season with salt and pepper and allow to simmer with lid on for 20-25 minutes. ',\n 'Meanwhile, add cauliflower to a pot of boiling water and cook for 10 minutes or until soft. Drain and then mash and gently fold in olive oil, parmesan, salt and pepper. ',\n 'Remove lid from chicken and let simmer uncovered for five minutes more. Sprinkle with parsley then serve with cauliflower mash. ']\n\n\nThis is what I want, but I need to remove the square brackets\n\ndtype = list\n\nI've tried:\n\ndf.applymap(lambda x: x[0] if isinstance(x, list) else x)\n\n\nOnly returns the first entry, not every step\n\nI've tried:\n\ndf['content'].str.replace(']', '')\n\n\nOnly returns NANs\n\nI've tried:\n\ndf['content'].str.replace(r'(\\[\\[(?:[^\\]|]*\\|)?([^\\]|]*)\\]\\])', '')\n\n\nOnly returns NANs\n\nI've tried:\n\ndf['content'].str.get(0)\n\n\nOnly returns the first entry\n\nAny help would be greatly appreciated.\n\nIf you need any further information, please let me know.\n"
"I'm working on a ML project for a class. I'm currently cleaning the data and I encountered a problem. I basically have a column (which is identified as dtype object) that has ratings about a certain aspect in a hotel. When i checked what the values of this column were and in what frequency they appeared, I noticed that there are some wrong values in it (as you can see below, instead of ratings, some rows have a date as a value)\n\nrating       value_counts()      \n100           527\n98            229\n97            172\n99            163\n96            150\n95            127\n93            100\n90             94\n94             93\n80             65\n92             55\n91             39\n88             35\n89             32\n87             31\n85             25\n86             17\n84             12\n60             12\n83              8\n70              5\n73              5\n82              4\n78              3\n67              3\n2018-11-11      3\n20              2\n81              2\n2018-11-03      2\n40              2\n79              2\n75              2\n2018-10-26      2\n2               1\n2018-08-30      1\n2018-09-03      1\n2015-09-05      1\n55              1\n2018-10-12      1\n2018-05-11      1\n2018-11-14      1\n2018-09-15      1\n2018-04-07      1\n2018-08-16      1\n71              1\n2018-09-18      1\n2018-11-05      1\n2018-02-04      1\nNaN             1 \n\n\nWhat I wanted to do was to replace all the values that look like dates with NaN so I can later fill them with appropriate values. Is there a good way to do this other than selecting each different date one by one and replacing it with a NaN? Is there a way to select similar values (in this case all the dates that start in the same way, 2018) and replace them all?\n\nThank you for taking the time to read this!!\n"
'I basically have a column with postcodes and another one with neighbourhoods and I have some null values to fill on the postcode column. So first I found what neighbourhood corresponds to that postcode that is missing. Second, I found out what is the most common postcode in that neighbourhood.\n\nBelow are some of the postcodes of neighbourhood X. The mode for this specific neighbourhood is, let\'s say Y. What I\'m trying to do is, taking the rows that have the neighbourhood X under the neighbourhood column, fill the corresponding column of postcodes where the values are null.\n\nThis is the mode for neighbourhood X. It returns the actual mode (BS8) and the full list with all the postcodes regarding neighbourhood X\n\n &lt;bound method Series.mode of 25      BS8 \n\n    1904    BS1 \n    1919    BS8 \n    2070    BS1 \n    2083    BS1 \n    2099     NaN\n    2105    BS1 \n    2228     NaN\n    2256    BS1 \n    2265    BS8 \n    2285    BS8 \n    2298    BS8\n\n\nSo in this case, I\'d want to fill the nan value under postcode with the most common postcode type for HH.\n\nneighbourhood      Postcode\nWH                 BS9 \nSB                 BS9 \nHF                 BS9 \nWH                 BS9 \nWH                 BS9 \nSB                 BS9 \nHH                 nan\nSGTH               nan\n\n\nIf the most common postcode for HH was, let\'s say Z, if want to fill it in that corresponding postcode, like this:\n\nneighbourhood      Postcode\n    WH                 BS9 \n    SB                 BS9 \n    HF                 BS9 \n    WH                 BS9 \n    WH                 BS9 \n    SB                 BS9 \n    HH                 Z\n    SGTH               nan\n\n\nAfter looking online I tried something like the code below, but it didn\'t work.\n\nairbnb.postcode = airbnb.apply( \n        lambda row: "BS8 " if (airbnb.neighbourhood=="HH" &amp; airbnb.postcode== np.NaN) else row.postcode )\n\n'
"I have two dataframe, say df1 and df2, both of these dataframe are very large, having 1 million+ rows and 1000 columns. \nNow, df1 has a column, say X which has the characters in it (as shown below). And df2 has 900+ columns and each of which needs to be changed based on df1.\n\ndf1:\nIndex   ColX ColY\n 100     C    R\n 101     T    Z\n 102     A    Y\n ...    ..   ..\n\ndf2:\nIndex    ColA   ColB   ColC   ColD   ...  ...\n 100     0.033  0.10   0.22   1.22   ...  ...\n 101     1.77   1.34   0.45   1.90   ...  ...\n 102     0.88   1.56   1.99   0.99   ...  ...\n ...     ...    ...    ...    ...    ...  ...\n\n\nCondition to be applied is that:\n\nIf columns in df2 >= 0 and &lt; 1.5, then replace those values with Col X values corresponding to that index. \n\nElif columns in df2 >= 1.5 and &lt;= 2 then replace those values with Col Y values corresponding to that index\n\nExpected Output:\n\ndf2:\nIndex    ColA   ColB   ColC   ColD   ...  ...\n 100      C      C       C      C    ...  ...\n 101      Z      T       T      Z    ...  ...\n 102      A      Y       Y      A    ...  ...\n ...     ...    ...    ...    ...    ...  ...\n\n\nI tried this way:\n\nfor v in df2.columns.tolist():\n    df2 = df2.loc[(df2[v] &gt;= 0) &amp; (df2[v] &lt; 1.5) , v] = df1['ColX']\n\n\nSometimes this is working, sometimes it is not (for the first case) but this method is very slow. I have a very big file.\n\nPlease someone can tell me any efficient way to do this.\nThankx in Advance.\n"
"I have this data frame\n\n    A\n0   -2\n1   0\n2   2\n3   2\n4   0\n5   0\n6   0\n7   0\n8   0\n9   0\n10  0\n11  0\n12  2\n13  2\n14  2\n15  2\n16  2\n17  3\n18  2\n19  0\n20  2\n21  2\n22  2\n\n\nand it's plot is like this\n\n\n\nI want to threshold data based on the length of the sequence for the above example flattening the B part because it's length is less than 3 like below\n\n\n"
"I have a dataframe called passenger_details which is shown below\n\nPassenger     Age  Gender   Commute_to_work    Commute_mode    Commute_time ...\nPassenger1    32   Male      I drive to work      car              1 hour\nPassenger2    26   Female    I take the metro     train            NaN    ...\nPassenger3    33   Female      NaN                 NaN             30 mins      ...\nPassenger4    29   Female    I take the metro     train            NaN     ...\n...\n\n\nI want to apply an if function that will turn missing values(NaN values) to 0 and present values to 1, to column headings that have the string 'Commute' in them. \n\nThis is basically what I'm trying to achieve\n\nPassenger     Age  Gender   Commute_to_work    Commute_mode    Commute_time ...\nPassenger1    32   Male         1                 1              1\nPassenger2    26   Female       1                 1              0    ...\nPassenger3    33   Female       0                 0              1      ...\nPassenger4    29   Female       1                 1              0     ...\n...\n\n\nHowever, I'm struggling with how to phrase my code.  This is what I have done \n\npassenger_details = passenger_details.filter(regex = 'Location_', axis = 1).apply(lambda value: str(value).replace('value', '1', 'NaN','0'))\n\n\nBut I get a Type Error of\n\n'replace() takes at most 3 arguments (4 given)'\n\n\nAny help would be appreciated\n"
'I am trying to remove duplicate words in strings in my data frame per row. \n\nSay my data frame looks like this:\n\nIn:\nYes Yes Absolutely\nNo No Nope   \nWin Win Lose\n\n\n\n  for row in df.iterrows():\n        row["Sentence"] = (list(set(row["Sentence"])))\n\nDesired Out:\nYes Absolutely\nNo Nope\nWin Lose\n\n\nHow can I clean, each row to remove the duplicate strings. I have tried the above code.\n\nAny links to any docs or sources would be greatly appreciated if they can lead me in the right direction. Thank you.\n'
'Trying to remove stopwords from csv file that has 3 columns and creates a new csv file with the removed stopwords. This is successful however, the data in the new file appears across the top row rather than the columns in the original file.\n\n    import io \n    import codecs\n    import csv\n    from nltk.corpus import stopwords \n    from nltk.tokenize import word_tokenize \n\n    stop_words = set(stopwords.words(\'english\')) \n    file1 = codecs.open(\'soccer.csv\',\'r\',\'utf-8\') \n    line = file1.read() \n    words = line.split()\n    for r in words: \n        if not r in stop_words: \n            appendFile = open(\'stopwords_soccer.csv\',\'a\', encoding=\'utf-8\') \n            appendFile.write(" "+r)\n            appendFile.close()\n\n'
"I have a df with DateTimeIndex (hourly readings) from multiple sensors\n\nTime                   Temp1   Temp2   Temp3  Humidity1 Humidity2 \n1/2/2017 13:00          31       23      NA     66        48\n1/2/2017 14:00           22      NA      NA      63        43\n1/2/2017 15:00           25      25      21      41        39\n\n\nI would like to replace missing values of Temperature sensor 3 (Temp3) with available data from Temp1 and Temp2. If both Temp1 and Temp2 are not null, I want to take an average. If only 1 is available, I will take that value.\n\nExpected Output:\n\nTime                      Temp3   \n1/2/2017 13:00               27     \n1/2/2017 14:00               22      \n1/2/2017 15:00               21     \n\n\nI tried to use apply with lambda, but running into issues when one of the data is missing. \n\nDf['Temp3'] = Df.apply(\n    lambda row: (row['Temp1']+row['Temp2'])/2 if np.isnan(row['Temp3']) \n    else row['Temp3'],\n    axis=1\n)\n\n"
'I am new to programming and am trying to code things to make simple tasks easier. So right now I am having trouble with a csv file which lists out database fields and the table that field is associated with. The end state I want to have is a dictionary that successfully maps out every single table in which a field appears in. So for example: {firstname;[cust_table,supplier_table,dealer_table]} that has a lot of unnecessary white space in some items. A problem I do keep running into is that some of the database field are given extra white space some of the other aren\'t, for example: \'CUSTID\' , \'CUSTID         \'.  What can I do to get rid of this while still keeping the relationship there between db field and table?\n\nI have tried using the reader object in the csv module and was able to create a clean list of all of the fields that I needed and removed all duplicates. I want to be able to loop through this list and use it as a filter for the new dictionary that I will create; adding values of tables to each unique db field (key). But right now I am not returning anything (empty dictionary)\n\nimport csv\n\nf=open("Data Mapping Information.csv","r")\n\nreader=csv.reader(f)\n\nfield_list=[]\nfor row in reader:\n    field_list.append(row[1])\n\nmylist=list(dict.fromkeys(field_list))\n\ncleanList=[]\nfor item in mylist:\n    clean_item=item.strip()\n    cleanList.append(clean_item)\n\nreader2=csv.reader(f)\n\nmapping={}\n\nfor row in reader2:\n    if row[1] in mylist:\n        mapping[row[1]]=row[0]\n\n\nHere is a sample of some of the lines in the csv file:\n\nADS,CUSTID             \nADS,ROLEID         \nADS_PARTY,CUSTID     \nCUST_TABLE,CUSTID\n\nCUST_TABLE,FULLNAME\n\nDEALER_TABLE,FULLNAME         \n\nSUPPLIER_TABLE,FULLNAME\n\nSUPPLIER_TABLE,ROLEID\n\n'
"I got a dirty data set like this, for example, if email is filled in phone column, I need to move the email to email column and leave the phone column blank, if email and phone are filled in the wrong place like A03, I need to swap them to the right column.     \n\nID             Phone            Email\nA01            111111           abc@mail.com\nA02            bcd@mail.com     NaN\nA03            def@mail.com     222222222\n\n\ncurrently, I can remove all cells in phone column with email address, but I don't know how to compare two column and swap them. \n\neduDup['phone'] = eduDup.phone.str.replace(r'(^.*@.*$)', 'aaaaaaaaaaaaaaaa sport')\n\n"
"My data is in below format\n\n\r\n\r\n&lt;table&gt;\r\n&lt;tbody&gt;\r\n    &lt;tr&gt;&lt;th&gt;A&lt;/th&gt;&lt;th&gt;B&lt;/th&gt;&lt;th&gt;C&lt;/th&gt;&lt;th&gt;D&lt;/th&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;3&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;&lt;/tr&gt;\r\n&lt;/tbody&gt;\r\n&lt;/table&gt;\r\n\r\n\r\n\n\nI want to transform it in below format:\n\n\r\n\r\n&lt;table&gt;\r\n&lt;tbody&gt;\r\n    &lt;tr&gt;&lt;th&gt;1&lt;/th&gt;&lt;th&gt;2&lt;/th&gt;&lt;th&gt;3&lt;/th&gt;&lt;th&gt;4&lt;/th&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;A&lt;/td&gt;&lt;td&gt;A&lt;/td&gt;&lt;td&gt;A&lt;/td&gt;&lt;td&gt;A&lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;B&lt;/td&gt;&lt;td&gt;B&lt;/td&gt;&lt;td&gt;C&lt;/td&gt;&lt;td&gt;B&lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;C&lt;/td&gt;&lt;td&gt;B&lt;/td&gt;&lt;td&gt;C&lt;/td&gt;&lt;td&gt;D&lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt;D&lt;/td&gt;&lt;td&gt;C&lt;/td&gt;&lt;td&gt;D&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;\r\n    &lt;tr&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt;D&lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;td&gt; &lt;/td&gt;&lt;/tr&gt;\r\n&lt;/tbody&gt;\r\n&lt;/table&gt;\r\n\r\n\r\n\n\nI have tried using Excel pivot, Python pivot table but i'm not able to get desired result.\n\nCan you help me please?\n\nThanks,\nNaseer\n"
'I\'ve got a text file with >1 million observations in it that I\'m trying to process into a dataframe. The problem is it looks like this: \n\nproduct/productId: blah blah\nproduct/title: blue shirt\nproduct/price: unknown\nreview/userId: blah blah\nreview/text: blah blah\n\nproduct/productId: blah blah\nproduct/title: pair of jeans\nproduct/price: unknown\nreview/userId: blah blah\nreview/text: blah blah\n\n\nEvery block of text is a unique observation, and I have to group them and make them into rows in a neat dataframe. So all in all, this is over 5 million lines that need to be processed. \n\nI\'m fairly new to Python so I\'m not quite sure what the best way of cleaning this up would be. I started by reading the file into a Pandas df:\n\ninitialData = pd.read_csv(args["data_file"], sep="\\n", header=None, dtype=str)\ninitialData.columns = [ "data" ]\n\nprint(initialData.head(5), "\\n\\n", initialData.shape)\n\n\nOutput:\n\n                                                data\n0                      product/productId:  blah blah\n1   product/title: blah blah\n2                             product/price: unknown\n3                      review/userId: blah blah\n4   review/profileName: blah blah\n\n (5819330, 1)\n\n\nThen I try using the following function to organize the data in each line into its respective row with named columns:\n\ndef organize_data(df):\n    df["col"] = 0\n    # group lines by observation represented\n    for line_count in range(0, len(df), 10):\n        indices = [ line_count, line_count + 1, line_count + 2,\n                    line_count + 3, line_count + 4, line_count + 5,\n                    line_count + 6, line_count + 7, line_count + 8, line_count + 9 ]\n        # iterate through grouped lines\n        for index in indices:\n            row = df.iloc[index]\n            # split inputs, assign one to "col" column\n            # that\'ll be used to assign each value to its\n            # respective column\n            split_row = row["data"].split(" ", 1)\n            new_label = split_row[0]\n            last_split = new_label.split("/")\n            future_col_name = last_split[1]\n            row["col"] = future_col_name\n    organized_df = df.pivot(columns="col", values="data")\n\n    return organized_df\n\n\nAs you can imagine given that it iterates through literally every line in the file, it\'s unbelievably slow. And it gives me a SettingWithCopyWarning to boot, so it doesn\'t even do what I want it to when it\'s finished. How can I deal with these issues?\n'
'I am quite new with Python that I try to learn for basic text analysis, topic modelling etc.\n\nI wrote the following code for cleaning my text file. I prefer pywsed.utils lemmatize.sentence() function to NLTK\'s WordNetLemmatizer() because it produces cleaner texts. The following code works fine with sentences:\n\nfrom nltk.corpus import stopwords\nfrom pywsd.utils import lemmatize_sentence\nimport string\n\ns = "Dew drops fall from the leaves. Mary leaves the room. It\'s completed. Hello. This is trial. We went home. It was easier. We drank tea. These are Demo Texts. Right?"\n\nlemm = lemmatize_sentence(s)\nprint (lemm)\n\nstopword = stopwords.words(\'english\') + list(string.punctuation)\nremovingstopwords = [word for word in lemm if word not in stopword]\nprint (removingstopwords, file=open("cleaned.txt","a"))\n\n\nBut what I fail to do is lemmatizing a raw text file in a directory. I guess lemmatize.sentence() only requires strings?\n\nI manage to read contents of a file with\n\nwith open (\'a.txt\',"r+", encoding="utf-8") as fin:\n    lemm = lemmatize_sentence(fin.read())\nprint (lemm)\n\n\n\nbut this time the code fails to remove some keywords like "n\'t", "\'ll", "\'s", or "‘" and punctuations which result in an uncleaned text. \n\n1) What do I do wrong? Should I tokenize first? (I also failed to feed lemmatize.sentence() with its results). \n\n2) How do I get the output file content without any formatting (words without single quotes and bracket)?\n\nAny help is greatly appreciated. Thanks in advance.\n'
'I am trying to create a new column in my data set that is dependent on two separate columns in my data (country and zip code).  If the country is "USA", I want the new column to take the data in the zip code column and remove everything after the "-" (leaving only the first five numbers).  If the country is "Canada" I want the new column to take the data in the zip code column, remove all spaces and input the data into the new column.  See example below.  \n\n\n\nI have tried several different things including the following but none of them have worked\n\n(1) df[\'new column\'] = [df[\'Zip Code\'].str[:5] if x == \'USA\' else \'no\' for x in df[\'Country\']]\n\n(2) usa = df[\'Country\'].str.contains(\'USA\')\n\ncanada = df[\'Country\'].str.contains(\'Canada\')\n\ndf[\'PYZipCleaned\'] = np.where(USA, \'USA\', zipclean.str.replace(\'-\',""))\n\n\nPlease help\n'
"I have a 87288-point dataset that I need to filter. The filtering fields for the dataset are a X position and a Y position, as latitude and longitude. Plotted the data looks like this:\n\n\n\nThe problem is , I only need data along a certain path, which is known in advance.  Something like this:\n\n\n\nI already know how to filter data in a Pandas DF, but given the path is not linear, I need an effective strategy to clear out all the noisy data with a certain degree of precision (since the dataset is so large, manually picking the points is not an option).\n\nHere is some sample data.The only important columns are Latitude and Longitude, Y and X respectively.\n\nSesion,Tiempo,Latitud,Longitud,PM2.5,Modo,Hora,DiaSemana\nM-O-AM-07OCT19-DMR,2019-10-01 09:48:17.625,3.3659550000000005,-76.5288288,13.0,OUTDOOR,AM,1\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:03.555,3.3661757000000003,-76.5289441,12.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:04.596,3.3661757000000003,-76.5289441,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:05.572,3.3661767,-76.5289375,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:06.614,3.3661790999999996,-76.5289188,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:07.581,3.3661814,-76.5289024,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:08.588,3.3661847999999996,-76.52889820000001,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:09.570,3.3661922,-76.52890450000001,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:10.579,3.3661922,-76.52890450000001,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:11.577,3.3662135,-76.52893370000001,12.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:12.611,3.3662227999999996,-76.5289516,12.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:13.561,3.3662227999999996,-76.5289516,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:14.631,3.3662346,-76.5289927,11.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:15.554,3.3662421,-76.52901440000001,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:16.623,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:17.593,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:18.617,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:19.608,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:20.605,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:21.594,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:22.608,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:23.620,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:24.611,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:25.622,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:26.590,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:27.619,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:28.595,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:29.628,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\nM-O-AM-07OCT19-DMR,2019-10-07 08:18:30.621,3.3662523999999996,-76.5290363,10.0,OUTDOOR,AM,0\n\n\nI have tried of handpicking a few points inside the route, and filtering the rest using a fixed min distance, something like this.\n\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom cycler import cycler\nimport numpy as np\nfrom salem import get_demo_file, DataLevels, GoogleVisibleMap, Map\nimport geopy.distance\n\ndef get_dist(coords_1 , coords_2):\n    return geopy.distance.distance(coords_1, coords_2).meters\n\ndists=[\n    (-76.5297163,3.3665631),\n    (-76.5307019,3.3656924),\n    (-76.5314718,3.3646900),\n    (-76.5319956,3.3638394),\n    (-76.5316622,3.3621781),\n    (-76.5311999,3.3611796),\n    (-76.5308636,3.3599338),\n    (-76.5306335,3.3585191),\n    (-76.5304758,3.3577502),\n    (-76.5303957,3.3561101),\n    (-76.5302998,3.3543178),\n    (-76.5302220,3.3531897),\n    (-76.5302369,3.3515283),\n    (-76.5303363,3.3502667),\n    (-76.5305351,3.3485951),\n    (-76.5306779,3.3475220),\n    (-76.5308545,3.3456382),\n    (-76.5307738,3.3446934),\n    (-76.530618,3.3430422)\n]\ndf = pd.read_csv('movil.csv')\n\n\nfor index, row in df.iterrows():\n    if index%1000 ==0:\n        print(index)\n    mind=None\n    for i in dists:\n        if mind:\n            d=get_dist((row['Latitud'],row['Longitud']),(i[1],i[0]))\n            if d&lt;mind:\n                mind=d\n        else:\n            mind=get_dist((row['Latitud'],row['Longitud']),(i[1],i[0]))\n    if mind&gt;125:\n        df.drop(index, inplace=True)\n\nprint(df)\n\n\nUsing these approach I managed to get some cleaning, but I feel a lot of useful data is getting filtered.\n\n\n"
"I am working with some messy data and I'm trying to figure out how to merge multiple columns with similar information onto one column. For example I have a dataframe that looks like this and i want to know how to condense all three columns into one:\n\nCountry ------------State  ------  Temp ------ Temperature ------ Degrees\n\nUnited States -----Kentucky --- $76  ------   76    --------------------    N/A  \n\nUnited States -----Arizona ----- 92\\n   -------   N/A       ------------------         N/A\n\nUnited States   -----   Michigan     --    45     -----------           45@    -----------------  60\n"
'I want to create a new dataframe by merging two seperate dataframes. The data frames share a common key and some common columns. This common columns also contain some but not all of the same values. I would like to remove the duplicate values and keep both values are that different in each cell. My data looks something like this:\n\nLeft:\n\nkey1  valueZ  valueX  valueY\n A    bob     1       4\n B    jes     8       5\n C    joe     3       6\n\n\nRight:\n\nkey1  valueZ  valueX  valueY  valueK\n A    sam     7       4       hill town\n B    beth    8       11      market\n C    joe     9       12      mall\n\n\nThe expected output would be:\n\nkey1    valueZ     valueX     valueY   valueK \nA       bob/sam      1/7       4       hill town  \nB       jes/beth     8         5/11    market       \nC       joe          3/9       6/12    mall     \n\n\n'
'Another simple question. I have to clean up some data, and a few of the columns need to be in int64 format instead of the objects that they are now (example provided). how would I go about uniformly re-formatting these columns. \n\nprint(data.Result)\n0    98.8 PG/ML   H\n1           8.20000\n2    26.8 PG/ML   H\n3    40.8 PG/ML   H\n4            CREDIT\n5          15.30000\n\n'
"As per my knowledge Python loops are slow, hence it is preferred to use pandas inbuilt functions.\n\nIn my problem, one column will have different currencies, I need to convert them to dollar. How can I detect and convert them to dollar using pandas inbuilt functions ?\n\nMy column as following:\n\n100Dollar\n200Dollar\n100Euro\n300Euro\n184pounds\n150pounds\n10rupee\n30rupee\n\n\nNote: amount and currency name is in same column.\n\nNote: conversion exchange rate w.r.t dollar {Euro: 1.2, pounds: 1.3, rupee: 0.05}\n\nNote: currency enum is ['Euro', 'Dollar', 'Pounds', 'Rupee']\n"
'I would like to replace some values in my dataframe that were entered in the wrong format. For example, 850/07-498745 should be 07-498745. Now, I used string split successfully to do so. However, it turns all previously correctly formatted strings into NaNs. I tried to base it on a condition, but still I have the same problem. How can I fix it ?\n\nExample Input:\n\nmylist = [\'850/07-498745\', \'850/07-148465\', \'07-499015\']\ndf = pd.DataFrame(mylist)\ndf.rename(columns={ df.columns[0]: "mycolumn" }, inplace = True)\n\n\nMy Attempt:\n\ndf[\'mycolumn\'] = df[df.mycolumn.str.contains(\'/\') == True].mycolumn.str.split(\'/\', 1).str[1]\ndf\n\n\nOutput:\n\n\n\nWhat I wanted:\n\n\n'
'I have a larger data-set in PySpark and want to calculate the percentage of None/NaN values per column and store it in another dataframe called percentage_missing. For example if the following were the input dataframe: \n\ndf = sc.parallelize([\n    (0.4, 0.3),\n    (None, None),\n    (9.7, None), \n    (None, None)\n]).toDF(["A", "B"])\n\n\nI would like the output to be a dataframe where column \'A\' contains the value 0.5 and column \'B\' contains the value 0.75.\n\nI am looking for something like this:\n\nfor column_ in my_columns:\n  amount_missing = df[df[column_] == None].count().div(len(df)) * 100\n\n\nIf there is a library with a function that does this I would also be happy to use it. \n'
"Is there a best practice to remove weird whitespace unicode characters from strings in Python?\n\nFor example if a string contains one of the following unicodes in this table I would like to remove it.\n\nI was thinking of putting the unicodes into a list then doing a loop using replace but I'm sure there is a more pythonic way of doing so.\n"
'I have time series data from "5 Jan 2015" to "28 Dec 2018". I observed some of the working days\' dates and their values are missing. How to check how many weekdays are missing in my time range? and what are those dates so that i can extrapolate the values for those dates.\n\nExample:\n\nDate    Price    Volume\n2018-12-28  172.0   800\n2018-12-27  173.6   400\n2018-12-26  170.4   500\n2018-12-25  171.0   2200\n2018-12-21  172.8   800\n\n\nOn observing calendar, 21st Dec, 2018 was Friday. Then excluding Saturday and Sunday, the dataset should be having "24th Dec 2018" in the list, but its missing. I need to identify such missing dates from range.\n\nMy approach till now:\nI tried using\n\npd.date_range(\'2015-01-05\',\'2018-12-28\',freq=\'W\')\n\n\nto identify the number of weeks and calculate the no. of weekdays from them manually, to identify number of missing dates.\nBut it dint solved purpose as I need to identify missing dates from range.\n'
"I am currently working on cleaning up a car emissions data set. This is what the data set looks like (only included first 10 rows):\n\nimport pandas as pd\n\ncars_em_df = pd.DataFrame({'manufacturer_name_mapped': ['FIAT', 'FIAT','FIAT','FIAT','FIAT'],\n'commercial_name':['124 gt multiair auto', '500l wagon pop star t-jet', \n'doblo combi 1.4 95', 'panda  0.9t sge 85 natural power', 'punto 1.4  77 lpg'],\n'fuel_type_mapped':['Petrol', 'Petrol', 'Petrol', 'NG-Biomethane', 'LPG'],\n'file_year':[2018, 2018, 2018, 2018, 2018], 'emissions': [153,158,165,86,114]})\n\n\nI am mostly interested in column 'commercial_name'. The end-goal is to add another column to this dataframe that shows the 'cleaned up' version of 'commercial_name'. I have a separate pandas series that contains the 'correct' names that should be used instead of these 'messy' names.\n\nreal_model_names = pd.Series(['uno', '147', 'panda', 'punto', '166', '4c', 'brera', 'giulia',\n'giulietta', 'gtv'])\n\n\nThese are all strings as well. So as an example, I would like to look up in every row of 'commercial_name' whether it contains any of the names from the 'real_model_names series'. E.g. 'punto' from 'real_model_names' can be found in the entry 'punto 1.4 77 lpg' from the 'commercial_name' column. So then I would like (in a new column in car_em_df) to have 'punto' next to it. If it cannot be found, I would like the original 'messy' name to be shown.\n\nI tried to define a function that I would then apply along the 'commercial_name' column. I tried this:\n\ndef str_ops(series):\n   for i in real_model_names:\n      if i in series:\n         return series.replace(series, i)\n      else:\n         return series\n\n\nAnd as a next step I would apply this function and add it to the dataframe as a new column:\n\ncommercial_name_cleaned = cars_em_df.commercial_name.apply(str_ops)\ncars_em_df.insert(3,value=commercial_name_cleaned,column='commercial_name_cleaned') \n\n\nHowever, this just doesn't do anything. The new column just shows the exact same entries as 'commercial_name'. \n\nDoes anyone know how to solve this problem? Is there a better way to do this? \n\nThanks a lot in advance!\n"
"I am currently working on a car emissions data set where I want to clean/standardise car model names. The data set is quite large, but here are the first 10 rows:\n\ncars_em_df = pd.DataFrame({'manufacturer_name_mapped': ['FIAT', 'FIAT','FIAT','FIAT','FIAT','BMW AG','BMW AG','BMW AG','BMW AG','BMW AG'],\n'commercial_name':['124 gt multiair auto', '500l wagon pop star t-jet', \n'doblo combi 1.4 95', 'panda  0.9t sge 85 natural power', 'punto 1.4  77 lpg', 'x4 xdrive20d se auto', '216d active tourer b37 f45','220d gran tourer b47 f46','x1 xdrive18d sport','320i xdrive m sport gt auto'],\n'fuel_type_mapped':['Petrol', 'Petrol', 'Petrol', 'NG-Biomethane', 'LPG','Diesel','Diesel','Diesel','Diesel','Petrol'],\n'file_year':[2018, 2018, 2018, 2018, 2018,2018, 2018, 2018, 2018, 2018], 'emissions': [153,158,165,86,114,131,166,200,151,149], 'commercial_name_cleaned':['124','500',None,'panda','punto','x4',None,None,'x1',None]})  \n\n\nThe right-hand column 'commercial_name_cleaned' is the result of my first cleansing exercise in which I have matched the names in column 'commercial_name' to a list of standardised names from a different source. As you can see, these are quite simple and short names. Whenever I couldn't match a model name, my function returned 'None'.\n\nAs a second step, I would now like to do the following: If it's 'None', search for a specific string inside the adjacent 'commercial_name' column and replace it with the model name I specified. I tried this:\n\n    def str_ops(commercial_name_cleaned,commercial_name):\n          if commercial_name_cleaned == None:\n             if '216' in commercial_name:\n                return '2-series'\n             elif '220' in commercial_name:\n                return '2-series'\n             elif '320' in commercial_name:\n                return '3-series'\n\n\nThen I would apply this function to the dataframe:\n\ncars_em_df['commercial_name_cleaned'] = cars_em_df.apply(lambda x: str_ops(str(x.commercial_name_cleaned), str(x.commercial_name)), axis=1)\n\n\nIt's important to note that if '320' or '220' etc. are not found in 'commercial_name', the function should not change anything and just return the value that was already in 'commercial_name_cleaned'. However, when I apply the function, the entire 'commercial_name_cleaned' column just becomes 'None' values. So there must be something wrong with the function. Does anyone have an idea how to solve this problem?\n\nHelp is much appreciated, thanks!\n"
'Suppose I have a text file that goes like this:\n\nAAAAAAAAAAAAAAAAAAAAA              #&lt;--- line 1\nBBBBBBBBBBBBBBBBBBBBB              #&lt;--- line 2\nCCCCCCCCCCCCCCCCCCCCC              #&lt;--- line 3\nDDDDDDDDDDDDDDDDDDDDD              #&lt;--- line 4\nEEEEEEEEEEEEEEEEEEEEE              #&lt;--- line 5\nFFFFFFFFFFFFFFFFFFFFF              #&lt;--- line 6\nGGGGGGGGGGGGGGGGGGGGG              #&lt;--- line 7\nHHHHHHHHHHHHHHHHHHHHH              #&lt;--- line 8\n\n\n\nIgnore "#&lt;--- line...", it\'s just for demonstration\n\n\n\nAssumptions \n\n\nI don\'t know what line 3 is going to contain (because it changes\nall the time)...\nThe first 2 lines have to be deleted...\nAfter the first 2 lines, I want to keep 3 lines...\nThen, I want to delete all lines after the 3rd line.\n\n\n\nEnd Result\n\nThe end result should look like this:\n\nCCCCCCCCCCCCCCCCCCCCC              #&lt;--- line 3\nDDDDDDDDDDDDDDDDDDDDD              #&lt;--- line 4\nEEEEEEEEEEEEEEEEEEEEE              #&lt;--- line 5\n\n\n\nLines deleted: First 2 + Everything after the next 3 (i.e. after line 5)\n\n\nRequired\n\nAll Pythonic suggestions are welcome! Thanks!\n\n\n\n\nReference Material\n\nhttps://thispointer.com/python-how-to-delete-specific-lines-in-a-file-in-a-memory-efficient-way/\n\n\ndef delete_multiple_lines(original_file, line_numbers):\n    """In a file, delete the lines at line number in given list"""\n    is_skipped = False\n    counter = 0\n    # Create name of dummy / temporary file\n    dummy_file = original_file + \'.bak\'\n    # Open original file in read only mode and dummy file in write mode\n    with open(original_file, \'r\') as read_obj, open(dummy_file, \'w\') as write_obj:\n        # Line by line copy data from original file to dummy file\n        for line in read_obj:\n            # If current line number exist in list then skip copying that line\n            if counter not in line_numbers:\n                write_obj.write(line)\n            else:\n                is_skipped = True\n            counter += 1\n\n    # If any line is skipped then rename dummy file as original file\n    if is_skipped:\n        os.remove(original_file)\n        os.rename(dummy_file, original_file)\n    else:\n        os.remove(dummy_file)\n\n\n\nThen...\n\n\ndelete_multiple_lines(\'sample.txt\', [0,1,2])\n\n\n\nThe problem with this method might be that, if your file had 1-100 lines on top to delete, you\'ll have to specify [0,1,2...100]. Right?\n\n\n\nAnswer\n\nCourtesy of @sandes\n\n\nThe following code will:\n\n\ndelete the first 63\nget you the next 95\nignore the rest\ncreate a new file\n\n\n\n\nwith open("sample.txt", "r") as f:\n    lines = f.readlines()\n    new_lines = []\n    idx_lines_wanted = [x for x in range(63,((63*2)+95))]\n    # delete first 63, then get the next 95\n    for i, line in enumerate(lines):\n        if i &gt; len(idx_lines_wanted) -1:\n            break\n        if i in idx_lines_wanted:\n             new_lines.append(line)\n\nwith open("sample2.txt", "w") as f:\n    for line in new_lines:\n        f.write(line)\n\n'
'I have two question about correlation between Categorical variables from my dataset for predicting models. \nUsing both Cramers V and TheilU to double check the correlation.\n\n\nI got 1.0 from Cramers V for two of my variable, however, I only got 0.2 when I used TheilU method, I am not sure how to interpret the relationship between the two variables? \nAlso for those that are experienced, if I got a 0.73 for a correlation of 2 variables, should I remove one of the variable for the predicting model? \n\n\nThanks you so much in advance!\n'
"I'm new to pandas and I want to clean a data frame with loads of columns. \n\nI want to keep values that fall within a range specific for each column, eg for the column named 'Age' I want to keep values larger than 5 and smaller than 25. If a value falls outside of that range I want to replace it with NaN, eg in the 'Age' column there is the value 918 that I want to replace.  \n\nIn my attempt I'm using a dictionary, because like I said I've got a lot of columns. This code doesn't work because it doesn't actually change any of the values in my original data frame (no error message).\n\nThanks for any help!  \n\n# PACKAGES \nimport pandas as pd\nimport numpy as np \n\n\n# STARTING DATA \ndata = [[1.0, 10, 0], [0.0, 12, 0.4], [2.0, 918, 0.9]]   \ndf = pd.DataFrame(data, columns = ['TriGly', 'Age', 'Chol']) \n\ndict = {\n    'Age': (5, 25),\n    'Chol': (0.2, 1.2),\n    'TriGly': (0.0, 1.0)\n}\n\n\n# CLEAN \nfor column_name in df.columns:                                             \n    if column_name in dict:                                                \n        for row in df[column_name]:                                        \n            if dict[column_name][0] &lt; row &lt; dict[column_name][1]:       \n                row = row                                                   \n            else:\n                row = np.nan                                               \n\n# DESIRED DATA \ndata2 = [[1.0, 10, np.nan], [0.0, 12, 0.4], [np.nan, np.nan, 0.9]]   \ndf2 = pd.DataFrame(data2, columns = ['TriGly', 'Age', 'Chol']) \n\n"
'CSV File\n\nIn the df a column in there has some rows which do not start with digit, i want them to delete, i tried some code below but they dont work\n\nimport re\ndf = sqlContext.read.csv("/FileStore/tables/mtmedical_V6-16623.csv", header=\'true\', inferSchema="true")\n\ndf.show()\n\nimport pyspark.sql.functions as f\nw=df.filter(df[\'_c0\'].isdigit()) #error1\nw=df.filter(df[\'_c0\'].startswith((\'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\'))) #error2\nw.show()\n\n\nerrors:\n\n\'Column\' object is not callable #no1\npy4j.Py4JException: Method startsWith([class java.util.ArrayList]) does not exist #no2\n\n\nhere is the table, you can  see that the row below row 7 in the column \'_c0\' does not start with digit, how can i delete such rows?\n\n+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------------------------------------------+--------------------+--------------------+\n|                 _c0|         description|   medical_specialty|                 age|              gender|sample_name (What has been done to patient = Treatment)|       transcription|            keywords|\n+--------------------+--------------------+--------------------+--------------------+--------------------+-------------------------------------------------------+--------------------+--------------------+\n|                   1| A 23-year-old wh...| Allergy / Immuno...|                  23|              female|                                     Allergic Rhinitis |SUBJECTIVE:,  Thi...|allergy / immunol...|\n|                   2| Consult for lapa...|          Bariatrics|                null|                male|                                    Laparoscopic Gas...|PAST MEDICAL HIST...|bariatrics, lapar...|\n|                   3| Consult for lapa...|          Bariatrics|                  42|                male|                                    Laparoscopic Gas...|"HISTORY OF PRESE...| at his highest h...|\n|                   4| 2-D M-Mode. Dopp...| Cardiovascular /...|                null|                null|                                    2-D Echocardiogr...|2-D M-MODE: , ,1....|cardiovascular / ...|\n|                   5|  2-D Echocardiogram| Cardiovascular /...|                null|                male|                                    2-D Echocardiogr...|1.  The left vent...|cardiovascular / ...|\n|                   6| Morbid obesity. ...|          Bariatrics|                  30|                male|                                    Laparoscopic Gas...|PREOPERATIVE DIAG...|bariatrics, gastr...|\n|                   7| Liposuction of t...|                null|                null|                null|                                                   null|                null|                null|\n|", Bariatrics,31,...|       1.  Deformity| right breast rec...|2.  Excess soft t...| anterior abdomen...|                                   3.  Lipodystrophy...|POSTOPERATIVE DIA...|       1.  Deformity|\n|                   8|  2-D Echocardiogram| Cardiovascular /...|                null|                male|                                    2-D Echocardiogr...|2-D ECHOCARDIOGRA...|cardiovascular / ...|\n\n'
'I have pandas DataFrame where groups of columns belong together. Now I would like to test per row the following:\n\n\nIf all rows of a specific group of columns are nan, do nothing.\nIf only some of the columns in that group of columns (per row) are nan, change it to another value (e.g. to 999).\n\n\nSo if this would be my initial DataFrame (with the following groups of columns belonging together: Group 1: Q1_1, Q1_2, Q1_3; Group 2: Q2_1, Q2_2)\n\nID  Q1_1  Q1_2  Q1_3  Q2_1  Q2_2 \n 1  nan   nan   nan    2    nan\n 2  nan    3    nan    3     1\n 3   5     4     4     5     5\n 4   2     4     3     5     4\n\n\nThis should be the outcome:\n\nID  Q1_1  Q1_2  Q1_3  Q2_1  Q2_2 \n 1  nan   nan   nan    2    999\n 2  999    3    999    3     1\n 3   5     4     4     5     5\n 4   2     4     3     5     4\n\n'
'Hi StackOverflow community,\n\nI have a data frame consisting of datetime dates from 2010-01-04 to 2020-01-01. Here is a small random snippet:\n\n28  2010-01-04  0.70930     LVL\n29  2010-01-04  1.58850     AUD\n30  2010-01-04  26.28500    CZK\n31  2010-01-04  1.49530     CAD\n32  2010-01-04  11.16080    HKD\n33  2010-01-05  1645.74000  KRW\n34  2010-01-05  0.90045     GBP\n35  2010-01-05  15.64660    EEK\n36  2010-01-05  18.48330    MXN\n37  2010-01-05  1.44420     USD\n38  2010-01-05  10.50690    ZAR\n\n\nHow can I create a new "Day of the Week" column for each observation? For example,\n\n28  2010-01-04  0.70930     LVL    Monday\n29  2010-01-04  1.58850     AUD    Monday\n30  2010-01-04  26.28500    CZK    Monday\n31  2010-01-04  1.49530     CAD    Monday\n32  2010-01-04  11.16080    HKD    Monday\n33  2010-01-05  1645.74000  KRW    Tuesday\n34  2010-01-05  0.90045     GBP    Tuesday\n35  2010-01-05  15.64660    EEK    Tuesday\n36  2010-01-05  18.48330    MXN    Tuesday\n37  2010-01-05  1.44420     USD    Tuesday\n38  2010-01-05  10.50690    ZA     Tuesday\n\n\nThank you all for any guidance.\n\nTony\n\nHere is the code to my project:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.stats as sp\n\nstart_date = "2010-01-01"\nend_date = "2020-01-01"\nsymbol = "GBP"\n\nresp = requests.get(\'https://api.exchangeratesapi.io/history?start_at={}&amp;end_at={}&amp;base=EUR\'.format(start_date, end_date))\n\nif resp.status_code != 200:\n    # This means something went wrong.\n    raise ApiError(\'GET /tasks/ {}\'.format(resp.status_code))\n\ndata = resp.json()\ndf2 = json_normalize(data)\ndf2.drop(["start_at","base","end_at"],axis=1,inplace=True)\ndf2.columns = [col.replace(\'rates.\', \'\') for col in df2.columns]\ndf3 = df2.stack().reset_index().drop(\'level_0\',axis=1)\ndf3.columns = [\'Date\', \'FXRateEUR\']\ndf3[\'Date\'], df3[\'Symbol\'] = df3[\'Date\'].str.split(\'.\', 1).str\ndf3[\'Date\'] = df3[\'Date\'].astype(str)  + " " +  \'00:00:00\'\ndf3[\'Date\'] =  pd.to_datetime(df3[\'Date\'])\ndf3 = df3.sort_values(by=\'Date\').reset_index(drop=True)\ndf3.head(50)\n\n'
'I have a dataframe dftest which has these columns: ADDRESS1, ADDRESS2,   ADDRESS3,   POSTCODE. I\'m trying to clean the data in each column and subsequently merge them into column FULL ADDRESS with a space between the data in each column.\n\nThis is what I\'d like to do:\nreplace(\',\', \'\').replace("\'", \'\').replace(\'.\', \'\').upper().strip() but I can\'t figure out how to loop through the columns. Sometimes ADDRESS2 and ADDRESS3 have NaN entries given there is no data there, but when they do have values, I\'d like them to be used in the merged final string. \n'
'I have an n x n column where two of columns as follows:\n\nheight  cost  item_x    cost2    item_y   weight\n15      10    bat        45       mitt    2\n19      12    ball       30       ball    4\n24      13    gloves     25       gloves  6\n22      14    bat        20       mitt    8\n\n\nI want to create unique columns for unique values of item_x and item_y, and fill them with appropriate values from cost and cost2 columns. So the expected output would be:\n\nheight  bat_x  ball_x  gloves_x  mitt_y  ball_y  gloves_y   weight\n15      10     0       0         45      0        0         2\n19      0      12      0         0       30       0         4\n24      0      0       13        0       0        25        6\n22      14     0       0         20      30       0         8\n\n\nAny help would be much appreciated! \n'
"I am trying to clean a column called 'historical_rank' in a pandas dataframe. It contains string data. Here is a sample of the content:\n\n       historical_rank\n...    ...\n122    1908\n123    O'   \n124 \n125    1911  \n126    1912  \n127    1913 * * * 2010 * * *  \n128\n129    1914  \n130    1915\n131  \n132\n133    1918  \n134    (First served 1989 to 1999)\n...    ...\n\n\nThe data I want to retain are the four-digit numbers in rows 122, 125, 126, 127, 129, 130, and 133. Elsewhere in the series that number (the historical rank) may be one, two, or three digits. It always begins the string, and there is always a space after it. I want to use regex to keep the desired pattern -- r'\\d{1,4}(?=\\s)' -- and remove everything else throughout the series. What is the correct code to achieve this? Thank you.  \n"
'I have a DataFrame which looks like this:\n\n&gt;&gt; pd.DataFrame([["Anne", True, 1, "A"],["Bert", True, None, "B"],["Conan", False, 0, None],["Bert", None, None, None],["Conan", None, None, "C"],["Bert",None,2,None]], columns = ["Name", "Bool", "Int", "Char"])\n\n    Name   Bool  Int  Char\n0   Anne   True  1.0     A\n1   Bert   True  NaN     B\n2  Conan  False  0.0  None\n3   Bert   None  NaN  None\n4  Conan   None  NaN     C\n5   Bert   None  2.0  None\n\n\nWhat I want is to\n\n\nRemove duplicates based on a column\nIn the removal process, keep the rows with less NaNs\nIf possible, fill the Nan values using a set of Rows (given a condition)\n\n\nI can do (1) and (2), I cannot understand how to do the (3)\n\nPart 1 &amp; 2\n\n&gt;&gt;&gt; def remove_duplicates_smartly(df, columns):\n        df.assign(nan_count= df.isna().sum(axis=1), inplace=True) \n        df.sort_values([\'nan_count\'], inplace=True).drop_duplicates(columns, inplace=True)\n        df.drop(columns=["nan_count"], inplace=True)\n        return df\n\n&gt;&gt;&gt; my_df = pd.DataFrame([["Anne", True, 1, "A"],["Bert", True, None, "B"],["Conan", False, 0, None],["Bert", None, None, None],["Conan", None, None, "C"],["Bert",None,2,None]], columns = ["Name", "Bool", "Int", "Char"])\n&gt;&gt;&gt; remove_duplicates_smartly(my_df)\n\n&gt;&gt;&gt; remove_duplicates_smartly(my_df, ["Name"])\n\n    Name   Bool  Int  Char\n0   Anne   True  1.0     A\n1   Bert   True  NaN     B\n2  Conan  False  0.0  None\n\n\nDesired output\n\nThe current missing values can be filled using the soon-to-be-deleted rows. The new values should be taken from the soon-to-be-deleted rows who have a value (selected by the user) in common (in this case, the Name)\n\n    Name   Bool  Int  Char\n0   Anne   True  1.0     A\n1   Bert   True  2.0     B\n2  Conan  False  0.0     C\n\n'
"I have a csv file that I am trying to convert into a data frame. But the data has some extra heading material that gets repeated. For example:\n\nResults Generated Date Time  \nSampling Info  \nTime; Data  \n1; 4.0  \n2; 5.2  \n3; 6.1  \n\nResults Generated Date Time  \nSampling Info   \nTime; Data  \n6; 3.2   \n7; 4.1   \n8; 9.7    \n\n\nIf it is a clean csv file without the extra heading material, I am using   \n\ndf = pd.read_csv(r'Filelocation', sep=';', skiprows=2)  \n\n\nBut I can't figure out how to remove the second set of header info. I don't want to lose the data below the second header set. Is there a way to remove it so the data is clean? The second header set is not always in the same location (basically a data acquisition mistake).\nThank you!\n"
"I would like to import .txt file into a Pandas Dataframe, my .txt file:\n\nAnn   Gosh  1234567892008-12-15Irvine                CA45678A9Z5Steve        Ryan      \nYosh   Dave    9876543212009-04-18St. Elf              NY12345P8G0Brad      Tuck     \nClair   Simon    3245674572008-12-29New Jersey             NJ56789R9B3Dan     John\n\n\nThe dataframe should look like this:\n\nFirstN    LastN       SID        Birth        City     States    Postal    TeacherFirstN  TeacherLastN\n   Ann     Gosh   123456789  2008-12-15     Irvine       CA        A9Z5           Steve           Ryan \n  Yosh     Dave   987654321  2009-04-18    St. Elf       NY        P8G0            Brad           Tuck\n Clair    Simon   324567457  2008-12-29   New Jersey     NJ        R9B3             Dan           John\n\n\nI tried multiple ways including this:\n\ndf =  pd.read_csv('student.txt',  sep='\\s+', engine='python', header=None, index_col=False)\n\n\nto import the raw file into the dataframe, then plan to clean data for each column but it's too complicated. Could you please help me? (the Postal here is just the 4 char before TeacherFirstN)\n"
"How can you group by one column, and then inside each group apply multiple fillna strategies at once on the other columns? Multiple meaning:\n\nif first in group, replace by zero, then ffill until first datapoint is reached\ntrailing NaN's are ffilled\nfor all NaN's between datapoints, bfill\nif it's all-NaN, leave the group alone\n\nBasically, I have the following dataframe:\n    A    B     C\n0   A  NaN   NaN\n1   A  NaN   NaN\n2   A  1.0  10.0\n3   A  NaN   NaN\n4   B  NaN   NaN\n5   B  2.0  20.0\n6   B  NaN  20.0\n7   B  NaN   NaN\n8   C  NaN   NaN\n9   C  NaN   NaN\n10  C  NaN   NaN\n11  C  NaN  30.0\n\nAnd I'd like it to turn into:\n    A    B     C\n0   A    0     0\n1   A    0     0\n2   A  1.0  10.0\n3   A  1.0  10.0\n4   B    0     0\n5   B  2.0  20.0\n6   B  2.0  20.0\n7   B  2.0  20.0\n8   C  NaN     0\n9   C  NaN     0\n10  C  NaN     0\n11  C  NaN  30.0\n\nI've tried getting the first element with df.groupby('A').nth(1) and to continue conditionally but the new index created by the groupby is not the original one (i.e. 0,4,8), regardless whether I pass the .reset_index() option or not.\nCode for dataframe recreation:\n\ndf = pd.DataFrame({'A' : [&quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;,\n                          &quot;B&quot;, &quot;B&quot;, &quot;B&quot;, &quot;B&quot;,&quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;],\n                   'B' : [np.nan, np.nan, 1, np.nan,\n                          np.nan, 2, np.nan, np.nan,\n                          np.nan, np.nan, np.nan, np.nan],\n                   'C' : [np.nan, np.nan, 10, np.nan,\n                          np.nan, 20, 20, np.nan,\n                          np.nan, np.nan, np.nan, 30]})\n\n"
"I have two dataframes that I want to merge. One contains data on &quot;assessments&quot; done on particular dates for particular clients. The second contains data on different categories of &quot;services&quot; performed for clients on particular dates. See sample code below:\nassessments = pd.DataFrame({'ClientID' : ['212','212','212','292','292'],\n           'AssessmentDate' : ['2018-01-04', '2018-07-03', '2019-06-10', '2017-08-08', '2017-12-21'],\n           'Program' : ['Case Mgmt','Case Mgmt','Case Mgmt','Coordinated Access','Coordinated Access']})\n\nClientID    AssessmentDate  Program\n212         2018-01-04      Case Mgmt\n212         2018-07-03      Case Mgmt\n212         2019-06-10      Case Mgmt\n292         2017-08-08      Coordinated Access\n292         2017-12-21      Coordinated Access\n\n\nservices = pd.DataFrame({'ClientID' : ['212','212','212','292','292'],\n           'ServiceDate' : ['2018-01-02', '2018-04-08', '2018-05-23', '2017-09-08', '2017-12-03'],\n           'Assistance Navigation' : ['0','1','1','0','1'],\n           'Basic Needs' : ['1','0','0','1','2']})\n\nClientID    ServiceDate Assistance Navigation   Basic Needs\n212         2018-01-02  0                       1\n212         2018-04-08  1                       0\n212         2018-05-23  1                       0\n292         2017-09-08  0                       1\n292         2017-12-03  1                       2\n\nI want to know how many services of each service type (Assistance Navigation and Basic Needs) occur between consecutive assessments of the same program. In other words, I want to append two columns to the assessments dataframe named 'Assistance Navigation' and 'Basic Needs' that tell me how many Assistance Navigation services and how many Basic Needs services have occurred since the last assessment of the same program. The resulting dataframe would look like this:\nassessmentsFinal = pd.DataFrame({'ClientID' : ['212','212','212','292','292'],\n           'AssessmentDate' : ['2018-01-04', '2018-07-03', '2019-06-10', '2017-08-08', '2017-12-21'],\n           'Program' : ['Case Mgmt','Case Mgmt','Case Mgmt','Coordinated Access','Coordinated Access'],\n           'Assistance Navigation' : ['0','2','0','0','1'],\n           'Basic Needs' : ['0','0','0','0','3']})\n\nClientID    AssessmentDate  Program             Assistance Navigation   Basic Needs\n212         2018-01-04      Case Mgmt           0                       0\n212         2018-07-03      Case Mgmt           2                       0\n212         2019-06-10      Case Mgmt           0                       0\n292         2017-08-08      Coordinated Access  0                       0\n292         2017-12-21      Coordinated Access  1                       3\n\nOf course, the real data has many more service categories than just 'Assistance Navigation' and 'Basic Needs' and the number of services and assessments is huge. My current attempt uses loops (which I know is a Pandas sin) and takes a couple of minutes to run, which may pose problems when our dataset gets even larger. Below is the current code for reference. Basically we loop through the assessments dataframe to get the ClientID and the date range and then we go into the services sheet and tally up the service type occurrences. There's got to be a quick and easy way to do this in Pandas but I'm new to the game. Thanks in advance.\nservicesDict = {}\nprevClient = -1\nprevDate = &quot;&quot;\nprevProg = &quot;&quot;\ncategories = [&quot;ClientID&quot;,&quot;ServiceDate&quot;,&quot;Family Needs&quot;,&quot;Housing Navigation&quot;,&quot;Housing Assistance&quot;,&quot;Basic Needs&quot;,&quot;Professional&quot;,&quot;Education&quot;,&quot;Financial Aid&quot;,&quot;Healthcare&quot;,&quot;Counseling&quot;,&quot;Contact&quot;,&quot;Assistance Navigation&quot;,&quot;Referral&quot;,&quot;Misc&quot;]\n\nfor index, row in assessmentDF.iterrows():\n    curClient = row[0]\n    curDate = datetime.strptime(row[1], '%m/%d/%y')\n    curProg = row[7] \n    curKey = (curClient, curDate)\n\n    if curKey not in servicesDict:\n        services = [curClient, curDate, 0,0,0,0,0,0,0,0,0,0,0,0,0]\n        servicesDict.update({curKey : services})\n    services = servicesDict[curKey]\n\n\n    #if curDate and prevDate equal each other action required\n    if curClient == prevClient and curProg == prevProg:\n        boundary = serviceDF[serviceDF['ClientID'] == curClient].index\n    \n        for x in boundary:\n            curRowSer = serviceDF.iloc[x]\n            curDateSer = datetime.strptime(curRowSer[1], '%m/%d/%y')\n        \n            if curDateSer&gt;=prevDate and curDateSer&lt;curDate:\n                serviceCategory = curRowSer[5]\n                i = categories.index(serviceCategory)\n                services[i] = services[i] + 1\n                servicesDict.update({curKey : services})\n            \n                          \n    prevClient = curClient\n    prevDate = curDate\n    prevProg = curProg\n\nservicesCleaned = pd.DataFrame.from_dict(servicesDict, orient = 'index',columns=categories)\n#then merge into assessments on clientID and AssessmentDate\n\n"
"I have two dataframes df1 and df2. I'm searching df1['comment'] for the partial strings in df2['label'] using the following line of code, which returns a new column df1['match'] with True/False values.\ndf1['match'] = df1['comment'].str.contains('|'.join(df2['label'].values), na=False)\nNow I don't only want the the True/False df1['match'] column but also a column that shows which of the partial strings from df2['label'] was found in df1['match']. I tried using something like\nif df1['comment'].str.contains('|'.join(df2['label'].values), na=False) == True:\ndf1['label_item'] = df2['label'].values\n\nBut it doesn't seem to work. I also feel like writing an if condition is probably not the right thing to do and that there is probably some kind of shortcut to solve this question.\nThanks a lot for your help!\n"
"Okay, go easy on me here. I've been coding about for about 48 hours cumulatively (ᵔᴥᵔ)\nI have a folder full of identical .txt files (as in, the values vary, but the format is exactly the same). The text in them looks like this:\n&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;{&quot;Flashpoint Swindon&quot;:{&quot;count&quot;:3,&quot;capacity&quot;:88,&quot;lastUpdated&quot;:&quot;Last updated: now (4:49 PM)&quot;},&quot;Oakwood&quot;:{&quot;count&quot;:45,&quot;capacity&quot;:75,&quot;lastUpdated&quot;:&quot;Last updated: now (4:49 PM)&quot;},&quot;Big Depot Leeds&quot;:{&quot;count&quot;:32,&quot;capacity&quot;:105,&quot;lastUpdated&quot;:&quot;Last updated: now (4:49 PM)&quot;},&quot;Depot Birmingham&quot;:{&quot;count&quot;:45,&quot;capacity&quot;:180,&quot;lastUpdated&quot;:&quot;Last updated: now (4:49 PM)&quot;},&quot;Depot Climbing Sheffield&quot;:...}}&lt;/body&gt;&lt;/html&gt;\n\nThere are 43 records in each file. The text files also have HTML tags at the start and end.\nMy end goal is a csv file with information for all dates that looks like this:\n+----------+--------------------+-------+-----------+--------+\n| Date     | Centre             | Count | Capacity  | Time   |\n| 20200822 | Flashpoint Swindon | 3     | 88        | 19:07  |\n| 20200822 | Oakwood            | 45    | 75        | 18:11  |\n| 20200822 | Big Depot Leeds    | 32    | 105       | 20:20  |\n+----------+--------------------+-------+-----------+--------+\n\nThe file name provides the date information (e.g. 2020-08-22-17-49-40_capacity.txt). So all of the information is there, I just don't know how to finish getting from A to B.\nSo far I've written this to clean the text file into something that can be worked with as a csv:\nimport re\n\nmy_file = open(&quot;2020-08-22-17-49-40_capacity&quot;, &quot;r+&quot;)\ntext = my_file.read()\ntext = re.sub('&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;', '', text)\ntext = re.sub('&lt;/body&gt;&lt;/html&gt;', '', text)\ntext = re.sub('},', '\\n', text)\ntext = re.sub('{', '', text)\ntext = re.sub('}}', '', text)\ntext = re.sub(':', ',', text)\n\nprint(text)\n\nimport sys \n\nstdoutOrigin=sys.stdout \nsys.stdout = open(&quot;cleaned.txt&quot;, &quot;w&quot;)\n\nThis seems to be working okay - here's what the text ends up looking like:\n&quot;Flashpoint Swindon&quot;,&quot;count&quot;,0,&quot;capacity&quot;,88,&quot;lastUpdated&quot;,&quot;Last updated, 1 hour ago (7,07 PM)&quot;\n&quot;Oakwood&quot;,&quot;count&quot;,0,&quot;capacity&quot;,75,&quot;lastUpdated&quot;,&quot;Last updated, 2 hours ago (6,11 PM)&quot;\n&quot;Big Depot Leeds&quot;,&quot;count&quot;,11,&quot;capacity&quot;,105,&quot;lastUpdated&quot;,&quot;Last updated, 1 min ago (8,20 PM)&quot;\n&quot;Depot Birmingham&quot;,&quot;count&quot;,8,&quot;capacity&quot;,180,&quot;lastUpdated&quot;,&quot;Last updated, 1 min ago (8,20 PM)&quot;\n&quot;Depot Climbing Sheffield&quot;...\n\nThe output is a little temperamental - sometimes it works and sometimes it spits out a blank .txt file. I've not figured out why.\nI realise I'm asking for a HUGE amount of help here, but if anyone can offer help for even parts of what I'm doing, that would be amazing.\nThanks so much in advance.\n"
"I have a super dirty text-heavy dataset. While the various column values are tab-separated but there are many line breaks within the desired row of data.\nAll data entries are separated by a hard '\\n' notation.\nI tried Setting the lineterminator argument as '\\n', but it is still reading the line breaks as a new row.\nPerforming any sort of regex or related operation is most likely resulting in loss of tab separations, which I need to load my data into a dataframe. Also doing a word-wise of line-wise operation is not exactly feasible owing to the size of the dataset.\nIs there a way I can get the Pandas not to read the line breaks as a new row, and go to the new line only when it sees a '\\n'?\nSnapshot of my data:\nThe unprocessed dataset\nBelow is a quick look at the current state:\ncurrent output\nThe highlighted red box should be one entry.\n"
"In the following dataframe, I would like to delete pairs of rows that have the same values for ITEM_ID and VALUE, but where one has TYPE == 'O' (for 'outbound') and the other has TYPE == 'I' (for 'inbound', comes later):\n        Date        ITEM_ID TYPE VALUE\n236656  2012-02-28  ECE240  O    1.0\n242962  2012-03-02  ECE240  O    1.0\n248720  2012-03-06  ECE240  O    1.0 (remove - out)\n226194  2012-03-19  ECE240  I    1.0 (remove - in)\n263320  2012-03-20  ECE240  O    1.0 (remove - out)\n242977  2012-03-24  ECE240  I    1.0 (remove - in)\n209713  2012-03-31  ECE240  O    1.0\n279806  2012-04-06  ECE240  O    1.0\n277213  2012-04-08  ECE240  O    1.0\n288865  2012-04-17  ECE240  O    3.0\n290041  2012-04-20  ECE240  O    2.0 (remove - out)\n136730  2012-04-22  ECE240  I    2.0 (remove - in)\n295236  2012-04-24  ECE240  O    1.0\n292597  2012-04-30  ECE240  O    1.0\n313503  2012-05-14  ECE240  O    1.0\n314786  2012-05-15  ECE240  O    2.0\n318277  2012-05-20  ECE240  O    1.0 (remove - out)\n328787  2012-06-01  ECE240  O    2.0\n2134    2012-06-16  ECE240  I    1.0 (remove - in)\n343138  2012-06-17  ECE240  O    2.0\n343139  2012-06-22  ECE240  O    1.0\n346935  2012-06-29  ECE240  O    1.0\n215777  2012-07-06  ECE240  O    1.0\n356292  2012-07-06  ECE240  O    2.0\n261989  2012-07-21  ECE240  O    2.0\n\nCode:\ndf  = df.sort_values(by = ['ITEM_ID ', 'Date'])\ndf1 = df.groupby(['ITEM_ID ','VALUE']).filter(lambda x : ~(x['TYPE'].eq('I') &amp; x['TYPE'].shift().eq('O')).any())\ndf1\n\nwhich returned:\n        Date        ITEM_ID TYPE    VALUE\n288865  2012-04-17  ECE240  O       3.0\n\nThis is not what I wanted, as I was expecting only 4 pairs to be removed (labelled in df above).\nThe expected output:\n        Date        ITEM_ID TYPE VALUE\n236656  2012-02-28  ECE240  O    1.0\n242962  2012-03-02  ECE240  O    1.0\n\n\n209713  2012-03-31  ECE240  O    1.0\n279806  2012-04-06  ECE240  O    1.0\n277213  2012-04-08  ECE240  O    1.0\n288865  2012-04-17  ECE240  O    3.0\n \n\n295236  2012-04-24  ECE240  O    1.0\n292597  2012-04-30  ECE240  O    1.0\n313503  2012-05-14  ECE240  O    1.0\n314786  2012-05-15  ECE240  O    2.0\n\n328787  2012-06-01  ECE240  O    2.0\n\n343138  2012-06-17  ECE240  O    2.0\n343139  2012-06-22  ECE240  O    1.0\n346935  2012-06-29  ECE240  O    1.0\n215777  2012-07-06  ECE240  O    1.0\n356292  2012-07-06  ECE240  O    2.0\n261989  2012-07-21  ECE240  O    2.0\n\nIn Python docs it states for any():\n\nReturn True if any element of the iterable is true. If the iterable is empty, return False.\n\nI think it removed all &quot;groups&quot; of rows where TYPE==I for a row and all other rows where VALUE is the same and TYPE==O. How can I remove only one pair for each &quot;group&quot; (i.e., for each row with TYPE==I, only one row in front with TYPE==O)?\n\n[EDIT 1]\nI also tried:\ndf  = df.sort_values(by = ['ITEM_ID', 'Date'])\ndf1 = df.groupby(['ITEM_ID','VALUE']).filter(lambda x : ~(x['TYPE'].eq('I') &amp; (x['TYPE'].shift().eq('O'))))\ndf1\n\nwhich caught error:\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-935-65eda184ce24&gt; in &lt;module&gt;\n      1 df= df.sort_values(by = ['ITEM_ID', 'Date'])\n----&gt; 2 df1= df.groupby(['ITEM_ID','VALUE']).filter(lambda x : ~(x['TYPE'].eq('I') &amp; (x['TYPE'].shift().eq('O'))))\n      3 df1\n\n~\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\generic.py in filter(self, func, dropna, *args, **kwargs)\n   1594                 # non scalars aren't allowed\n   1595                 raise TypeError(\n-&gt; 1596                     f&quot;filter function returned a {type(res).__name__}, &quot;\n   1597                     &quot;but expected a scalar bool&quot;\n   1598                 )\n\nTypeError: filter function returned a Series, but expected a scalar bool\n\n\n[EDIT 2]\nFor the following dataframe:\n        Date        ITEM_ID TYPE    VALUE\n342874  2012-06-18  ECE240  O       1.0 (not removed - out)\n342415  2012-06-18  ECE240  O       25.0\n325718  2012-06-18  ECE240  O       1.0 (not removed - out)\n334488  2012-06-18  ECE240  O       1.0 (not removed - out)\n342412  2012-06-18  ECE240  O       25.0\n341634  2012-06-18  ECE240  O       9.0\n341996  2012-06-19  ECE240  O       2.0 (remove - out)\n341747  2012-06-19  ECE240  O       1.0 (remove - out)\n272185  2012-06-24  ECE240  I       1.0 (remove - in)\n219     2012-06-24  ECE240  I       1.0 (not removed - in)\n6896    2012-06-24  ECE240  I       2.0 (remove - in)\n351560  2012-06-24  ECE240  O       1.0 (remove - out)\n312636  2012-06-26  ECE240  I       1.0 (remove - in)\n2376    2012-06-30  ECE240  I       1.0 (not removed - in)\n350922  2012-07-02  ECE240  O       1.0 (remove - out)\n270589  2012-07-09  ECE240  I       4.0\n331689  2012-07-15  ECE240  I       1.0 (remove - in)\n299912  2012-07-23  ECE240  I       1.0 (not removed - in) \n212418  2012-07-23  ECE240  I       3.0\n3992    2012-07-24  ECE240  I       2.0\n388937  2012-08-10  ECE240  O       10.0\n124596  2012-08-18  ECE240  I       1.0 \n368945  2012-08-19  ECE240  O       12.0\n368944  2012-08-19  ECE240  O       6.0\n239581  2012-08-24  ECE240  I       4.0\n\nsome rows that also satisfy the conditions are not removed(see dataframe above), because they are not immediately in front of the TYPE==I row. To clear all the rows including these ones, I think I could run the code repeatedly until these rows are exhausted. I'm wondering if there is another way to do it?\n"
"I have questionairre data that has been exported to excel. i'm currently trying to clean the data but have a problem. It is numbered from 1.1 to 1.12 in the survey software however when it is exported to excel it changes 1.10 to 1.1.SO i end up with two questions both numbered 1.1. I'd like to be able to renumber the questions that should be 1.10\nThe data looks like this\ndata = [[1, '1.1', 'first'], \n       [1, '1.2', 'yes'], \n       [1, '1.1', 'daily'],\n       [2, '1.1', 'last'], \n       [2, '1.2', 'yes'], \n       [2, '1.1', 'weekly']]\n\ndf = pd.DataFrame(data, columns = ['user_id', 'question', 'answer'])\n\nI'd like it to look like this\ndata2 = [[1, '1.1', 'first'], \n        [1, '1.2', 'yes'], \n        [1, '1.10', 'daily'],\n        [2, '1.1', 'first'], \n        [2, '1.2', 'yes'], \n        [2, '1.10', 'weekly']]\ndf2 = pd.DataFrame(data2, columns = ['user_id', 'question', 'answer'])\n\nI've tried this\nif df[(df.question == '1.1')]:\n    if df[(df.answer == 'first')]:\n        df.question == '1.1'\n    else:\n        df.question == '1.10'\n\nbut get this error\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\nany help?\n"
'Here is what I currently have:\n    print(df)\n\n    10   25  26\n10  530  1   46  \n25  1    61  61\n26  46   61  330\n\nHow can i transform this to df1 so that we divide each element in the row by the sum of the index columns? The output of df1 should look like this:\ndf1:\n\n\n    10             25               26\n10  530/(530)     1/(530+61)       46/(530+330)  \n25  1/(61+530)    61/(61)          61/(61+330)\n26  46/(330+530)  61/(330+61)      330/(330)\n\n    print(df1)\n\n    10      25        26\n10  1       0.0016    0.0534\n25  0.0016  1         0.1560\n26  0.0534  0.1560    1\n\n'
"I have a string which looks like this:\nstreet.Random, street.Randomer: 2, 4\n\nI need to split it into:\nstreet.Random \nstreet.Randomer: 2, 4\n\nI can't do on the first apparence of , since the number of them can be random.\nI looked at a way of detective if : appeared recently (or at all) in the string and then somehow split it based on that, but not sure how.\n"
"suppose we have columns of\n+------+-----+-----+\n|  a+b | b+c | c+d |\n+------+-----+-----+\n| No   | yes | No  |\n| Yes  | No  | No  |\n| No   | No  | Yes |\n+------+-----+-----+\n\nhow to obtain it's individual from the combination info as\n    +-----+-----+-----+-----+\n    |  a  |  b  |  c  |  d  |\n    +-----+-----+-----+-----+\n    | no  | yes | yes | no  |\n    | yes | yes | no  | no  |\n    | no  | no  | yes | yes |\n    +-----+-----+-----+-----+\n\nI have humongous data and I can't keep using &quot;or&quot;  condition for each and everything.\n"
"suppose i have the following text:\n'Reuters - Life is beautiful.'\n'agency.com - China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made - Chinese export prices highly competitive.'\n'AP - The number of days that beaches closed or posted warnings because of pollution rose sharply in 2003 due to more rainfall, increased monitoring and tougher -standards, an environmental group said on Thursday.'\n'CNN - Warming water temperatures - in the central equatorial Pacific last month may indicate the start of a new El Nino.'\n\nI want to remove all text only at the beginning of the text that's before a &quot;-&quot; and only if the &quot;-&quot; is followed by a whitespace.\nI would like something like:\nif line.startswith(code_to_match_my_condition):\n      strip_matched_text_from_line\n\nSo the results would be:\n'Life is beautiful.'\n'China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made - Chinese export prices highly competitive.'\n'The number of days that beaches closed or posted warnings because of pollution rose sharply in 2003 due to more rainfall, increased monitoring and tougher -standards, an environmental group said on Thursday.'\n'Warming water temperatures - in the central equatorial Pacific last month may indicate the start of a new El Nino.'\n\nI actually don't know how to code this. I would appreciate any help on this.\nThank you very much in advance\n"
"I have a DataFrame where all of the values in the column show the Date of Publication\n\nI need to remove an additional text to have the following result:\nFrom:\n\n1879 [1879]\n\nTo\n\n1879\n\nI have checked the datatype and god the following result:\nIdentifier               int64\nEdition Statement       object\nPlace of Publication    object\nDate of Publication     object\nPublisher               object\nTitle                   object\nAuthor                  object\nContributors            object\nFlickr URL              object\ndtype: object\n\nFinally, I have tried the following method but it did not work for me.\nnew_books = books['Date of Publication'].astype(object).apply(lambda x: x.str.slice(0, 3))\n\nWhat am I doing wrong?\n"
'I have used the following method to remove outliers in my X variables prior to modelling:\nz = np.abs(stats.zscore(X))\nX = X[(z &lt; 3).all(axis=1)]\n\nHow can I make it so the corresponding values in the Y column are deleted so that I can continue with my modelling?\n'
"I have a dataframe like this\ndf = {'ID': ['A', 'B', 'C'], '2': ['colname1', 'colname2', 'colname1'], '3': [3, 4, 0], '4':['colname3', 'colname3', 'colname3'], '5':[0, 2, 1]}\nold = pd.DataFrame(data=df)\nold\n\n    ID  2           3       4       5\n0   A   colname1    3   colname3    0\n1   B   colname2    4   colname3    2\n2   C   colname1    0   colname3    1\n\nWhere ID A has a value 3 for colname1 and ID B has a value 4 for colname2.\nI am trying to clean it so that it looks like\ndf = {'ID': ['A', 'B', 'C'], 'colname1': [3, 'None', 0], 'colname2': ['None', 4, 'None'], 'colname3':[0, 2, 1]}\nnew = pd.DataFrame(data=df)\nnew\n\n    ID  colname1    colname2    colname3\n0   A   3           None        0\n1   B   None        4           2\n2   C   0           None        1\n\nPlease note this is a simple example. The actual dataset is a lot larger than this.\nMy thought was to build another dataframe, extracting all the distinct column names (which appears at the even column) first.\ndf.iloc[:,1::2].T.apply(lambda x: x.unique(), axis=1)\n\nThen, write a loop to extract the values from the old dataframe to the new dataframe.\nBut I am not sure how to proceed. Is there a better way of doing this?\n"
"in Pandas, I have a column col_one that originally contained comma-seperated values in each cell.\n['a, b, e, g, o', 'a, b, d', 'a, b, c, f, g', 'a, b, c, f', 'a, c, e', 'a, b, c, o', 'b, c, h, n', 'a, b, c, g, o', 'a, b, c, f', 'a, b, c, g, h, o', 'b', 'a, b, f, m', 'a, b, c, g, h', 'a, b, d, f, g', 'a, c, n', 'j', 'b, c, f', 'a, b, g, l', 'b', 'b', 'a, b, d, e ', 'a, b, c', 'a, b, e, g', 'a, b, c, d, f, g', 'd, k, l', 'a, b, c, f, g ', 'a, b, c, f', 'a, b, c, d,  g', 'b, d, e', 'b, d', 'a', 'b, o', 'c, o', 'b, c, o', 'c', 'a, g, i', 'b, c, n', 'a, b', 'b, c, o, n', 'b, c, h', 'a, b, c, f, g, h', 'a, b, c, d', 'a, b, d', 'a, e, g', 'a, b, c, e, g, k, m', 'b, c, o', 'a, b, f, k', 'd, l', 'a, b, l', 'a, b, c', 'a', 'c, d, g, l', 'b, d, e, o', 'b, d', 'a, b, c, d, e, f, o', 'b', 'a, b, c, f', 'b, c, g', 'b, c, g, k', 'a', 'c', 'b, c, o', 'b, c, n, o']\n\nI used str.split(', ').explode().value_counts() .reset_index() to get an count of the individual letters. But in the resulting table, some letters appear twice, presumably because the string contains trailing spaces. Unfortunately these are not visible in the the Jupyter Notebook display of the resulting table as they are just blanks.\nUsing this\ncol_one_list = df[&quot;letter&quot;].tolist()\nprint (col_one_list)\n\ngave me a list of all values counted. In this list, I was able to spot a trailing space (&quot;g &quot;). But how could I have done this better?\n['b', 'a', 'c', 'g', 'd', 'f', 'o', 'e', 'n', 'h', 'l', 'k', 'm', 'j', 'g ', ' g', 'e ', 'i']\n\n"
"I have 2 columns with list values in my data frame as shown below:\nsalary.labels   salary.percentages\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [29, 0.9, 2.2, 11.3, 56.6]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [74.5, 1.1, 1.4, 12, 11]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [63.4, 1.9, 2.2, 11.2, 21.3]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [58.3, 0.6, 1.9, 7.9, 31.3]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [80.4, 1.4, 2.2, 4.7, 11.3]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [71.2, 0.9, 1.2, 6.3, 20.4]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [39.9, 1.6, 5.8, 15.8, 36.9]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [56.5, 0.8, 2.3, 9.8, 30.6]\n['Not Impacted', 'Salary Not Paid', 'Salary Cut', 'Variables Impacted', 'Appraisal Delayed']    [42.9, 2.3, 5.1, 14.1, 35.6]\n\nI wish to create new columns such that the column labels will take values of salary.labels column &amp; the column values in each row will take the corresponding value from the salary.percentages column.\nThe anticipated output data frame looks like this:\n'Not Impacted' 'Salary Not Paid' 'Salary Cut' 'Variables Impacted' 'Appraisal Delayed'\n29, 0.9, 2.2, 11.3, 56.6\n74.5, 1.1, 1.4, 12, 11\n63.4, 1.9, 2.2, 11.2, 21.3\n58.3, 0.6, 1.9, 7.9, 31.3\n80.4, 1.4, 2.2, 4.7, 11.3\n71.2, 0.9, 1.2, 6.3, 20.4\n39.9, 1.6, 5.8, 15.8, 36.9\n56.5, 0.8, 2.3, 9.8, 30.6\n42.9, 2.3, 5.1, 14.1, 35.6\n\nHow to perform this with pandas operations?\n"
"I'm new to text-cleaning in python but I currently created a dictionary with various slang words/acronyms/contractions that looks something like this:\n\nfulltext = {'BYOB': 'bring your own beer', 'couldn't': 'could not', 'finna': 'going to'}... etc.\n\nand I have another large corpus of text data:\n\nuncleaned_text = ['This is finna be crazy', 'I don't know why we couldn't be there', 'I should have known when the event was BYOB that it would be terrible']\n\nFor which I am trying to 'clean' by replacing those words inside the list of strings that match the dictionary keys with their corresponding values. So, my ideal output would be:\n\ncleaned text = ['This is going to be crazy', 'I don't know why we could not be there', 'I should have known when the event was bring your own beer that it would be terrible']\n\nI know I should be using  REGEX in some way and I know I should be using loops, but I am definitely not even close to what I should be doing I think, because the error I get is builtin function not iterable...\nAny suggestions?\nfor sentence in uncleaned_text:\nfor word in sentence:\nif word in fulltext.keys:\nword.replace(word, fulltext.key)\n"
"I am trying to find the best way to remove twitter usernames from a users tweet if there is a username present in the tweet. For example I have an array of stored tweets and I would like to return the tweet with the username taken out like this\ntweets = ['@joe123 thank you', 'this reminds me of @john12', 'this tweet has no username tag in it']\n\nclean_tweets = ['thank you', 'this reminds me of', 'this tweet has no username tag in it']\n\nHere is what I have so far:\ntweets = ['@joe123 thank you', 'this reminds me of @john12', 'this tweet has no username tag in it']\n\nclean_tweets = [word for tweet in tweets for word in tweet.split() if not word.startswith('@')]\n\nHowever output looks like this:\n['thank',\n 'you',\n 'this',\n 'reminds',\n 'me',\n 'of',\n 'this',\n 'tweet',\n 'has',\n 'no',\n 'username',\n 'tag',\n 'in',\n 'it']\n\nI am hoping for a better way to solve this other than using nested list comprehension. Maybe an apply function with lambda will work better? Anything helps thanks\n"
"in my data cleaning process i found some strings with inhbit a single char that might bias my analysis\ni.e. 'hello please help r me with this s question'.\nUntil now i only found tools to remove specific chars , like\nchar= 's'\ndef char_remover(text: \n    spec_char = ''.join (i for i in text if i not in s text)\n    return spec_char\n\nor the rsplit(), split() functions, which are good for deleting first /last char of a string.\nIn the end, I want to code a function that removes all single chars (whitespace char whitespace) from my string/dataframe.\nMy own thoughts on that question:\ndef spec_char_remover(text):\n    spec_char_rem= ''.join(i for i in text if i not len(i) &lt;= 1) \n    return spec_char_rem\n\nBut that obviously didn´t work.\nThanks in advance.\n"
"I extracted a list of words from a text, but during text preprocessing I have lowercased everything for easier comparison.\nMy question is how to make the extracted words in the list appear as they exactly appeared in the original text?\nI have tried to first tokenize the original text, and then find the closest matches in thise tokenized list to the word list I have extracted from the text. I used the each of the following for finding the closest matches:\n\nnltk.edit_distance\ndifflib.get_close_matches\n\nBut neither of them worked as I wanted. They extract somehow similar words but not exactly as they appear in the original text. I think the problem is that these methods treat lowercased, and uppercased words differently.\n\nWords extracted can be unigram, bigram up to 5-gram.\n\nExample:\nI have extracted the following bigram from text [rfid alert], but in original text it appeared like this [RFID alert].\nAfter using\ndifflib.get_close_matches('rfid alert', original_text_unigram_tokens_list)\nit's output was [profile Caller]  and not [RFID alert]. That is because python is case-sensitive. I think it found that the bigram in original_text_unigram_tokens_list with the least number of different characters from [rfid alert] is [profile Caller] so it returned [profile Caller].\nTherefore my question is: Is there any ready method or any workaround I could do to return the original form of the ngram as it appeared in text exactly? For instance, I want to get [RFID alert] instead of [profile Caller] in the above example, and so on.\nI appreciate any help. Thank you in advance.\n"
"I'm working on harmonizing/cleaning a rather &quot;dirty&quot; dataset in Pandas. For the sake of demonstration, let's pretend it's this one:\ndf = pd.DataFrame({&quot;colname&quot;: [&quot;ÀÁÂÃÄÅÆ&quot;, &quot;Ç&quot;, &quot;ÈÉÊË&quot;, &quot;ÌÍÎÏ&quot;, &quot;Ð&quot;, &quot;Ñ&quot;, &quot;ÒÓÔÕÖ&quot;, &quot;×&quot;, &quot;Ø&quot;, &quot;ÙÚÛÜ&quot;, &quot;Ý&quot;, &quot;Þ&quot;, &quot;ß&quot;]})\n\nprint(df)\n\n    colname\n0   ÀÁÂÃÄÅÆ\n1         Ç\n2      ÈÉÊË\n3      ÌÍÎÏ\n4         Ð\n5         Ñ\n6     ÒÓÔÕÖ\n7         ×\n8         Ø\n9      ÙÚÛÜ\n10        Ý\n11        Þ\n12        ß\n\n\nMy goal is to clean the above column by applying the following mapping to the characters:\nBefore: ÀÁÂÃÄÅÆÇÈÉÊËÌÍÎÏÐÑÒÓÔÕÖ×ØÙÚÛÜÝÞß\nAfter:  AAAAAAACEEEEIIIIDNOOOOO OUUUUY S\n\nAlthough chaining 32 str.replace() should do the job, it doesn't seem like a particularly readable solution to me. E.g., for the first two:\ndf[&quot;colname&quot;] = df[&quot;colname&quot;].str.replace(&quot;À&quot;, &quot;A&quot;).str.replace(&quot;Á&quot;, &quot;A&quot;)\nInstead, I was wondering if one could define the following mapping dictionary\nsub_dict = {\n    &quot;À&quot;: &quot;A&quot;, &quot;Á&quot;: &quot;A&quot;, &quot;Â&quot;: &quot;A&quot;, &quot;Ã&quot;: &quot;A&quot;, &quot;Ä&quot;: &quot;A&quot;, &quot;Å&quot;: &quot;A&quot;, &quot;Æ&quot;: &quot;A&quot;, &quot;Ç&quot;: &quot;C&quot;, \n    &quot;È&quot;: &quot;E&quot;, &quot;É&quot;: &quot;E&quot;, &quot;Ê&quot;: &quot;E&quot;, &quot;Ë&quot;: &quot;E&quot;, &quot;Ì&quot;: &quot;I&quot;, &quot;Í&quot;: &quot;I&quot;, &quot;Î&quot;: &quot;I&quot;, &quot;Ï&quot;: &quot;I&quot;,\n    &quot;Ð&quot;: &quot;D&quot;, &quot;Ñ&quot;: &quot;N&quot;, &quot;Ò&quot;: &quot;O&quot;, &quot;Ó&quot;: &quot;O&quot;, &quot;Ô&quot;: &quot;O&quot;, &quot;Õ&quot;: &quot;O&quot;, &quot;Ö&quot;: &quot;O&quot;, &quot;×&quot;: &quot; &quot;,\n    &quot;Ø&quot;: &quot;O&quot;, &quot;Ù&quot;: &quot;U&quot;, &quot;Ú&quot;: &quot;U&quot;, &quot;Û&quot;: &quot;U&quot;, &quot;Ü&quot;: &quot;U&quot;, &quot;Ý&quot;: &quot;Y&quot;, &quot;Þ&quot;: &quot; &quot;, &quot;ß&quot;: &quot;S&quot;\n}\n\nand apply that mapping to the column of the DataFrame?\nThe cleaned output DataFrame should be the following:\n    colname\n0   AAAAAAA\n1         C\n2      EEEE\n3      IIII\n4         D\n5         N\n6     OOOOO\n7          \n8         O\n9      UUUU\n10        Y\n11         \n12        S\n\n"
'Say I have a list in Python:\n\nl = [1, 2, 3, [a, [[9, 10, 11]], c, d], 5, [e, f, [6, 7, [8]]]]\n\n\nI would like to clean all the nested lists so that, if they are of length 1 (just one item), the are "pulled up" out of the list, such that the revised list would look like:\n\nl = [1, 2, 3, [a, [9, 10, 11], c, d], 5, [e, f, [6, 7, 8]]]\n\n\nNaturally I can just check if len == 1 and replace that index with its contents, but... Is there a built-in way to do this?\n'
"I want to clean a JSON file of incorrectly extracted HTML content by throwing away all the text which is enclosed in HTML tags, including the tags themselves.\n\nI tried this function: \n\ndef stripIt(s):\n    txt = re.sub('&lt;/?[^&lt;]+?&gt;.*?&lt;/[^&lt;]+?&gt;', '', s)\n    return re.sub('\\s+', ' ', txt)\n\n\nbut when I applied it to the JSON file, it probably breaks the JSON file, giving some errors.\n\nThe HTML content is also broken with missing tags, only closing tags, and so on.\n\nSo how can I strip all the HTML content from a JSON file, without breaking the file?\n"
"I'm trying to write unit tests for a small script which deletes all entries older than 6 months from a PostgreSQL database table.  The script executes the following query.\n\nDELETE FROM some_table WHERE tstamp &lt; (CURRENT_DATE - INTERVAL '180 days');\n\n\nThe script is in Python and I'm using testing.postgresql to create a local temporary database instance that I can freely manipulate.  The issue I'm having is that I cannot find a way to force CURRENT_DATE to be a constant value so that I can create test entries that are before and after a certain date, and will not change if the test are run later in time.\n\nIs it possible to override CURRENT_DATE in a PostgreSQL database to always return a pre-defined time?\n"
"I am in the proces of preparing a corpus of textfiles, consisting of 170 Dutch novels. I am a literary scholar and relatively new to Python, and also to programming in general. What I am trying to do is writing a Python script for removing everything from each .txt file that does NOT belong to the actual content of the novel (i.e. the story). Things I want to remove are: added biographies of the author, blurbs, and other pieces of information that comes with converting an ePub to .txt. \n\nMy idea is to manually decide for every .txt file at which line the actual content of the novel begins and where it ends. I am using the following block of code for the purpose of removing every information in the .txt file that is not contained between those two line numbers:\n\ndef removeparatext(inputFilename, outputFilename):\n    inputfile = open(inputFilename,'rt', encoding='utf-8')\n    outputfile = open(outputFilename, 'w', encoding='utf-8')\n\n    for line_number, line in enumerate(inputfile, 1):\n        if line_number &gt;= 80 and line_number &lt;= 2741: \n            outputfile.write(inputfile.readline())\n\n    inputfile.close()\n    outputfile.close()\n\nremoveparatext(inputFilename, outputFilename)\n\n\nThe numbers 80 and 2741 are the start and end numbers for the actual content of one specific novel. However, the outputfile only outputs a .txt file with the text removed BEFORE linenumber 80, it still contains everyhing AFTER line number 2741. I do not seem to understand why. Perhaps I am not using the enumerate() function in the right way. \n\nAnother thing is that I would like to get rid of all unnecessary spaces in the .txt-file. But the .strip() method does not seem to work when I implement it in this block of code. \n\nCould anyone give me a suggestion as to how to solve this problem? Many thanks in advance!\n"
"I just started learning Python so any help is much appreciated. \n\nSo the overarching purpose here is for data exploration + data cleaning.\n\nThe function I've written below outputs a dataframe that shows the percentage of missing values from every column. \n\ndef missing_values_table(df):\n    missing_vals = df.isnull().sum()\n    # Boolean check of all value to True for all null values, then sums for total count.\n    percent_conversion = 100 * df.isnull().sum()/len(df)\n    # Percent conversion.\n    combined_table = pd.concat([missing_vals, percent_conversion], axis=1)\n    # Merging dataframes.\n    table_renamed = combined_table.rename(columns = \n        {0:'Missing Values', 1:'Percentage'})\n    # Giving column labels.\n    table_renamed.sort_values(['Percentage'], ascending=False, inplace=True)\n    # Sort descending.\n    return table_renamed\n\n\nProblematic output (it's missing index that will show me where they are in the original dataframe...which is massive):\n\n                          Missing Values  Percentage\nEngine_Horsepower                 375906   93.712932\nPushblock                         375906   93.712932\nEnclosure_Type                    375906   93.712932\nBlade_Width                       375906   93.712932\n[...]\n\n\nDesired output:\n\n                          Missing Values  Percentage\n32 Engine_Horsepower                 375906   93.712932\n15 Pushblock                         375906   93.712932\n3  Enclosure_Type                    375906   93.712932\n17 Blade_Width                       375906   93.712932\n[...]\n\n\nThe numbers correspond to the column number from the original dataframe, pre-sorted.\n\nOnly after investigating these individually to confirm that these columns can be deleted, I'll delete the columns based on a threshold (50%+ null values, delete).\n"
'I am new to pandas and am struggling with to rename a column and then extracting the same.\n\nI have read an xls file into a pandas data frame object.\n\ndf = pd.read_excel("something.xls")\nbank_statement.columns.values[0] = \'Din\'\nbank_statement.columns\n\n\nThis showed the columns\n\nIndex([u\'Din\', u\'Unnamed: 1\', u\'Unnamed: 2\', u\'Unnamed: 3\', u\'Unnamed: 4\',\n       u\'Unnamed: 5\', u\'Unnamed: 6\'],\n      dtype=\'object\')\n\n\nBut this causes error.\n\nbank_statement.Din\n\n\nThe error is:\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-11-6ce73c262cd1&gt; in &lt;module&gt;()\n----&gt; 1 bank_statement.Din\n\n/Users/monideepde/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc in __getattr__(self, name)\n   3612             if name in self._info_axis:\n   3613                 return self[name]\n-&gt; 3614             return object.__getattribute__(self, name)\n   3615 \n   3616     def __setattr__(self, name, value):\n\nAttributeError: \'DataFrame\' object has no attribute \'Din\'\n\n\nContrary to this, when I had tried to do the same for a column that was named when it was imported, I did not face any issue.\n\ndata = pd.read_csv("/somepath/TestFrame.csv")\ndata\n\n\n\n\nI could access the columns\n\n\n\nCan anyone point out where I am going wrong?\n\nThanks\n'
'I just started with Python and wanted to do data preparation with the numpy/pandas package on the Movielens dataset (especially the the file with MovieID, Movie Name and Year as well as Genre).\n\nScreenshot: movielens - movie dataset\n\nThe column Genre is a multi-value column which is a problem for me since I want to try using machine learning algorithms on the datasets.\n\nAim: I want to have a yes/no or 0/1 information about which genre the movie falls in and which not.\n\nIdea: Check if the \'Genre\' column contains the column name of the appended columns (single genre names). If so, write yes, otherwise write now in the cell. And this iterate over all the new columns and all the rows.\n\nDone so far: I appended empty/NaN columns to the dataframe for each Genre. And I also tried with dataframe.iloc[\'Genre\'].str.contains(list(dataframe)[4]) which gave me the result TRUE or FALSE if the names matched or not. But how can I iterate and write in the cells in an elegant way?\n\nMany thanks in advance.\nBest,\nMarcel\n\nEDIT: Here you will find what I achieved so far. I split the data in the Genre column with the pipe separator, renamed the columns and appended the new columns and deleted the old column. If I now use the get_dummies function on all the columns, it creates e.g. a \'Genre1_Action\', \'Genre1_Adventure\', ..., \'Genre3Thriller\', according to the text values displayed in the cell of the Genre cells.\nWhat I want to achieve is that each Genre gets its single columns for each movie. \n\n# create a small test subset\nsubset1 = movie_data [0:9]\nprint("Original Dataset")\nprint(subset1)\n# Split movie year and year in separate values -&gt; append them to the df -&gt; clean the Year column\ntempY = subset1[\'MovieNameYear\'].str.split(\'(\').apply(pd.Series)\ntempY.columns = [\'MovieName\',\'Year\']\nsubset1 = pd.concat([subset1,tempY], axis=1, join=\'inner\')\nsubset1[\'Year\'] = subset1[\'Year\'].str.replace(\')\',\'\')\ndel subset1[\'MovieNameYear\']\n\n# split the column \'Genre\' with the with the pipe separator in seperate columns\n# name the columns of the temp value with the splitted values\n# join the through split created columns to the existing subset and delete the original multi value column\ntempG = subset1[\'Genre\'].str.split(\'|\').apply(pd.Series)\ntempG.columns = [\'Genre1\',\'Genre2\',\'Genre3\']\nsubset1 = pd.concat([subset1, tempG], axis=1, join=\'inner\')\ndel subset1[\'Genre\']\nprint("Cleaned Dataset")\nprint(subset1)\n\ndummiesTemp = pd.get_dummies(data=subset1, columns=[\'Genre1\',\'Genre2\',\'Genre3\'])\nprint(dummiesTemp)\n\n'
"I'm working with a file using commas as a delimiter. However, it has a field, address in it where the address is of form x,y,z which causes a problem as each part of the address gets a new column entry. The address is immediately followed by member_no a 1 digit number like 2 etc.\nCol1 (Address), Col2(1 Digit number)\n\ntext = '52A, XYZ Street, ABC District, 2'\n\n\nI basically want to remove all commas before that number from the address field. \n\nThe output should be like\n\n52A XYZ Street ABC District, 2'\n\n\nI tried \n\nre.sub(r',', ' ', text)\n\n\nbut it's replacing all instances of commas.\n"
"I have the following text :\n\nAdam, 30 M, Husband\n\n\nExpected Output : \n\nAdam, 30, M, Husband,\n\n\nMy Approach : \n\nre.sub(r'\\b(\\d{1,2}\\s\\w{1},)\\b', r'\\1,', text)\n\n\nHow can I get a comma between 30 and M as shown in the output above? \n"
"I have a dataset with a column 'Self_Employed'. In these columns are values 'Yes', 'No' and 'NaN. I want to replace the NaN values with a value that is calculated in calc(). I've tried some methods I found on here, but I couldn't find one that was applicable to me.\nHere is my code, I put the things i've tried in comments.:\n\n    # Handling missing data - Self_employed\nSEyes = (df['Self_Employed']=='Yes').sum()\nSEno = (df['Self_Employed']=='No').sum()\n\ndef calc():\n    rand_SE = randint(0,(SEno+SEyes))\n    if rand_SE &gt; 81:\n        return 'No'\n    else:\n        return 'Yes'\n\n\n&gt; # df['Self_Employed'] = df['Self_Employed'].fillna(randint(0,100))\n&gt; #df['Self_Employed'].isnull().apply(lambda v: calc())\n&gt; \n&gt; \n&gt; # df[df['Self_Employed'].isnull()] = df[df['Self_Employed'].isnull()].apply(lambda v: calc())  \n&gt; # df[df['Self_Employed']]\n&gt; \n&gt; # df_nan['Self_Employed'] = df_nan['Self_Employed'].isnull().apply(lambda v: calc())\n&gt; # df_nan\n&gt; \n&gt; #  for i in range(df['Self_Employed'].isnull().sum()):\n&gt; #      print(df.Self_Employed[i]\n\n\ndf[df['Self_Employed'].isnull()] = df[df['Self_Employed'].isnull()].apply(lambda v: calc())\ndf\n\n\nnow the line where i tried it with df_nan seems to work, but then I have a separate set with only the former missing values, but I want to fill the missing values in the whole dataset. For the last row I'm getting an error, i linked to a screenshot of it.\nDo you understand my problem and if so, can you help?\n\nThis is the set with only the rows where Self_Employed is NaN\n\nThis is the original dataset\n\nThis is the error\n"
"I have a Pandas dataframe with the following columns\n\ngame_id, date, country, winner_name, winner_age, ... winner_ranking, loser_name, loser_age, ... loser_ranking\n1        1/2/10  UK .     Ben          21               12            Michael     22 .    13\n\n\nI want to reshape it to have the following format\n\ngame_id, date, country, competitor, name, age, ranking \n 1       1/2/10 UK       winner    Ben    21   12\n 1       1/2/10 UK       loser     Michael 22   13\n\n\nI.e. for every column starting with a prefix 'winner_' or 'loser_', remove this prefix, and split the winner and loser into different rows. The list of winner and loser variables is quite long, so it's not that helpful if I have to hardcode.\n\nHere's how I'm currently doing it, I'm wondering if there is a neater approach, for example using melt?\n\nwinner_df = combined_df.loc[:,[x for x in colnames if 'loser_' not in x]]\nwinner_df.columns = [c.replace('winner_','') for c in winner_df.columns]\nwinner_df['competitor'] = 'winner'\nloser_df = combined_df.loc[:,[x for x in colnames if 'winner_' not in x]]\nloser_df.columns = [c.replace('loser_','') for c in loser_df.columns]\nloser_df['competitor'] = 'loser'\nlong_df = winner_df.append(loser_df,sort=False)\n\n"
'I am trying to iterate through a list of data to clean it up. \n\nHere\'s a small part of the list:\n\nlines =[\'Wirkstoffliste 1 –  \',\'\',\'  \', \'Gaschromatographie (GC) \', \'LOQ \', \'[mg/kg] \', \'Acibenzolar-S-methyl\', \'Aclonifen\', \'Acrinathrin\', \'Alachlor\', \'Aldrin\', \'Allethrin\', \'Ametryn\', \'Antrachinon\', \'Atrazin\', \'Atrazin-desethyl\', \'Atrazin-desisopropyl\', \'Azinphos (-ethyl)\', \'Azinphos-methyl\', \'Benalaxyl\', \'Benfluralin\', \'Benzoylprop-ethyl\',\' Seite 13 von 14 \', \'   \', \' \', \' \', \'Wirkstoffliste 4 - \',\'Version 7.2 \']\n\n\nI want to remove any list item that contains the words "Version", "Seite" and "Wirkstoffliste". You will also see there are some strings that are either blank or contain just white space (of various lengths). \n\nI have already cleaned this data up quite a lot with regex, but now I just want the chemical names. There are some other items that keep coming up that I don\'t want, e.g. "Version" but they are never quite the same, so it might be "Version 7. 2" or "Version 8.1". Therefore I thought if I tried "If \'Version\' in string", this would find it within the string, then I could choose to delete it. However this doesn\'t seem to work. \n\nDo I really need to use regex with this too?\n\nHere\'s a bunch of stuff I tried. \n\nI have tried if string in item. \n\nif "Wirkstoffliste" in item:\n    lines.remove(item)\n\n\nI have tried using OR logic so I could put more search strings in there. e.g.\n\nif "Seite" or "Wirkstoffliste" or "Version" in item:\n    lines.remove(item)\n\n\nI used both enumerate with del and and if in statement, e.g.\n\nfor n,item in enumerate(lines):\n    if "Wirkstoffliste" in item:\n        del lines[n]\n\n\nAnd finally I tried using a list of search strings:\n\nremovables=["Seite","Version","Wirkstoffliste","Gaschromatographie","LOQ"]\n\nfor line in lines:\n    for r in removables:\n        if r in line:\n            lines.remove(line)\n\n\nTo delete the blanks and white spaces I have tried:\n\n"""delete empty items"""\nlines = list(filter(None, lines))\nlines = list(filter(bool,lines))\n\n\nand \n\nfor item in lines:\n    if item=="" or " ":\n        lines.remove(item)\n\n\nI have found none of the above works, so I am a little confused what I am doing wrong.\n'
"I am pre-processing a large dataframe for analysis.\nBasically, I am trying to find the largest number or close to largest number ('close' is defined as more than 0.9*largest number) in a column and label it with 1 while keep the other positions as 0,\ni.e. if a columns contains [25, 3, 5, 24, 0] it should be converted into [1,0,0,1,0]. Somehow the code I've written is taking forever to run.\n\nI have written a simple list comprehension to clean the data column by column. The code took less than 1 second to run for the first 2,000 columns. However, it became very slow and took more than half an hour when I increase the number of columns to 10,000. Eventually I want to run this code on a 5-million-row dataset, is there something wrong that I should be changing to make it more efficient?\n\ntic = time.time()\n\nfor col in temp_dataset_1.iloc[:,:10000]:\n    temp_dataset_1[col] = [1 if i &gt;= i.max()*.9 else 0 for i in temp_dataset_1[col]]\n\ntoc = time.time() - tic\nprint('Calculating 10,000 out of 5,810,172 rows took %d seconds' %toc)\n#temp_dataset_1.iloc[:,:10000].head(n=5)\n\n\nMy data structure knowledge is limited, is there something obvious that I am missing out?\n"
'I am trying explore the data I have, But I found lot of anamolies in my data. The date column of the dataframe has date like "12012-09-14" and "2500-09-28". I would like to replace them with "2250-05-05". \n\nI would like to retain valid dates in df1 and those invalid dates into a list\n\ndf1:\n\ncol col2        date \n1   b1a2         NaN \n2   bal2  12012-09-14 \n3   a3l2  12017-09-14 \n4   a5l2  2019-09-24 \n5   a8l2  2012-09-28 \n6   a1l2  12113-09-14 \n7   a0l2  12012-09-24 \n8   a2l2  2500-09-28 \n9   a6l2  2500-09-14 \n10  a5l2  2012-09-24 \n\n\nCould someone help me how to extract those invalid dates?\n\nExpected Output:\n\n    col col2    date\n0    1  b1a2 2250-05-05\n1    2  bal2 2250-05-05\n2    3  a3l2 2250-05-05\n3    4  a5l2 2019-09-24\n4    5  a8l2 2012-09-28\n5    6  a1l2 2250-05-05\n6    7  a0l2 2250-05-05\n7    8  a2l2 2250-05-05\n8    9  a6l2 2250-05-05\n9   10  a5l2 2012-09-24\n\n\nunique list of invalid dates:\n\ninvalid_list = [\'12012-09-14\',\'12017-09-14\',\'12113-09-14\',\'12012-09-24\',\'2500-09-28\']\n\n'
'I have a data set of the format key1=value1, key2=value2, key3=value3... where each key-value pair is separated from the others by a ", ".\n\nHowever, some values are long strings that contain ", " as part of the value.\n\nHow can I correctly go through this data and convert it to csv?\n\nI\'ve tried using a csv.reader, but it doesn\'t work.\n\n data = row.lstrip(\'(\').rstrip(\')\\n\')                               \n reader = csv.reader(StringIO(data))                                \n for row2 in reader:                                                \n     my_dict = {}                                                   \n     for d in row2:                                                 \n         my_dict[d.split(\'=\')[0].lstrip()] = d.split(\'=\', 1)[1]                                               \n\n'
'Given a sample data frame:\n\ndf = \n\n                               multi\n0 MULTIPOLYGON(((1.1 1.2, 2.1 2.2)))\n1 MULTIPOLYGON(((3.1 3.2, 4.1 4.2)))\n2 MULTIPOLYGON(((5.1 5.2, 6.1 6.1)))\n\n\nDisred output:\n\ndf = \n\n a_1 b_1 a_2 b_2 a_3 b_3\n 1.1 1.2 3.1 3.2 5.1 5.2\n 2.1 2.2 4.1 4.2 6.1 6.2\n\n\n\n\nI did very long process, which is very bad even to put here. \n\n\nStep one:\n\n\nDelete MULTIPOLYGON, then (((, then )))\n\n\nStep two:\n\n\nSplit on comma\n\n\nStep three:\n\n\nTranspose and then split on space\n\nI am sure there should be nicer, wiser way to do that (it is easy to do on excel, but I need to do with python)\n'
"I have the following DataFrame from which I'm trying to obtain multiple aggregations via a dictionary argument into groupby().agg() \n\n&gt;&gt;&gt; apply_dict\n{'Amount': ['sum', 'std'], 'TransactionDate': ['min']}\n&gt;&gt;&gt; df\n  Account  Amount TransactionDate\n0       A      10      10-20-2018\n1       A      20      10-21-2018\n&gt;&gt;&gt; df.groupby('Account').agg(apply_dict)\n        Amount           TransactionDate\n           sum       std             min\nAccount                                 \nA           30  7.071068      10-20-2018\n\n\nHowever, the standard deviation is reported as sample stdev, but I would like to obtain the population stdev. Is it possible to supply a kwarg to agg() when using a dictionary? Trying the below call returns the same DataFrame as above:\n\n&gt;&gt;&gt; df.groupby('Account').agg(apply_dict, ddof=0)\n        Amount           TransactionDate\n           sum       std             min\nAccount                                 \nA           30  7.071068      10-20-2018\n\n\nThanks!\n"
"I have a dataset with mulitple rows. I want to create a new dataset based on number of duplicate rows for a column. For first dataset, i want a dataset of  no duplicate rows, meaning only the row with one value. For second dataset, i want two duplicate rows and three duplicate rows,but only up to the second one. For third dataset, i want a dataset with just three duplicate rows. So as an example, i wrote codes to describe this situation. Let's Say i have a dataframe as such\n\nx = {'column1': ['a','a','b','b','b','c','c','c','d'],\n    'column2': [22000,25000,27000,350,0,3,5,4,312]\n    }\ndf = pd.DataFrame(x, columns = ['column1', 'column2'])\nprint (df)\n\n\nThe first dataset should look like this: \n\nx = {'column1': ['d'],\n    'column2': [312]\n    }\ndf = pd.DataFrame(x, columns = ['column1', 'column2'])\nprint (df)\n\n\nSecond dataset should look like this:\n\nx = {'column1': ['a','a','b','b','c','c'],\n    'column2': [22000,25000,27000,350,3,5]\n    }\ndf = pd.DataFrame(x, columns = ['column1', 'column2'])\nprint (df)\n\n\nThird dataset should look like this:\n\nx = {'column1': ['b','b','b','c','c','c'],\n    'column2': [27000,350,0,3,5,4]\n    }\ndf = pd.DataFrame(x, columns = ['column1', 'column2'])\nprint (df)\n\n\nHow would i do this not manually?\n"
"My goal is to take a dataframe composed of words and tags, and collapse it into a dataframe composed of sentences and a list of tags.\n\nSample input:  \n\ndf = pd.DataFrame([('Effect', 'O'),\n               ('of', 'O'),\n               ('ginseng', 'i'),\n               ('extract', 'i'),\n               ('supplementation', 'i'),\n               ('on', 'O'),\n               ('testicular', 'o'),\n               ('functions', 'o'),\n               ('in', 'O'),\n               ('diabetic', 'p'),\n               ('rats', 'p'),\n               ('.', 'p'),\n               ('OBJECTIVE', 'O'),\n               ('It', 'O'),\n               ('was', 'O')],\n               columns=('token', 'annotation'))\n\n\nGoal output:\n\ndf = pd.DataFrame([('Effect of ginseng extract supplementation on testicular functions in diabetic rats.', \\ \n                     ['O','O','i','i','i','O','o','o','O','p','p','p','O','O','O']),\n                   ('OBJECTIVE It was', ['O','O','O'])],\n                   columns=('token', 'annotation'))\n\n\nSorry for the goofy example - that really is the first 15 rows of this dataset!!\n\nAny ideas of how to compress the rows of words into rows of sentences would be much appreciated.\n"
"Can anyone help, i'm new to Python so bear with me. \n\nMy data looks like this but has all the region information available. I'm trying to create a new column 'actual price' that works out the price based on the region. as for every entry I have each price for every region. is this possible.\n\ndata = [[1, 'EDF', 'Eastern', 400, 500, 300], [2, 'EDF', 'Southern', 200, 100, 300], [3, 'NPower', \n        'Eastern', 600, 500, 700]] \n\n\ndf = pd.DataFrame(data, columns = ['ID', 'Supplier', 'Region', 'Av Price', 'Eastern Price',  \n'Southern Price']) \n\ndf\n\n"
"So I scraped data and saved it as I liked, now I'm in process of cleaning. But having  issue to save changes despite I creating new file with pandas.\nWhat I am missing, why it's not assigning value which I iter to original?\nI found some info about apply() but it haven't worked for me. Instead I used to get empty rows.\nA\nMy code:\nimport pandas as pd\n\nneedtoclean = pd.read_excel(r'C:\\Users\\sound\\Desktop\\data.xlsx')\n\n#making all into str, due its mixed with int,float,str\nneedtoclean['Salary'] = needtoclean['Salary'].astype(str)\ncolumnSeriesObj = needtoclean['Salary']\n\n\ncolumnSeriesObj = needtoclean['Salary']\nfor value in columnSeriesObj.values:\n    #print(value)\n    value = value.replace('Nuo',&quot;&quot;).replace('Iki','')\n    value = value.strip()\n    value = re.split(r'[\\s,-]+', value)\n    if len(value) &gt; 1:  #\n        value = (int(value[0])+int(value[1]))/2\n    else:\n        value = float(value[0])\n    value = round(value)\n    #print(value)\n\n\ncolumnTitle = needtoclean['Job Title']\nfor value in columnTitle.values:\n    #print(value)\n    value = value.title()\n    value = value.replace('/', ' ').replace('(-Ė)','').replace('(-A)','').replace('(-As)', '')\n    value = value.strip()\n    value = value.split()\n    #print(value)\n\nneedtoclean.to_excel(&quot;datatest.xlsx&quot;)\n\nSalary original values:\nNuo 2600\n2000-2500\n1487-2479\nIki 3636\n1600-5700\n1200-2000\n\nAnd I excepted:\n2600\n2250\n1983\n3636\n3650\n1600\n\nJob title:\nIT infrastruktūros palaikymo skyriaus vadovas (-ė)\nBankinių kortelių skaitytuvų programuotojas (-a)\nIT sistemų priežiūros skyriaus vadovas (-ė)\nProduktų palaikymo skyriaus vadovas (ė) (programinė įranga)\n\nExpected to get:\n['It', 'Infrastruktūros', 'Palaikymo', 'Skyriaus', 'Vadovas']\n['Bankinių', 'Kortelių', 'Skaitytuvų', 'Programuotojas']\n['It', 'Sistemų', 'Priežiūros', 'Skyriaus', 'Vadovas']\n['Produktų', 'Palaikymo', 'Skyriaus', 'Vadovas', '(Ė)', '(Programinė', 'Įranga)']\n\n"
'Here is what I currently have:\nprint(df)\n\n   id  event_id  people_id\n0   1   45430.0         30\n1   2   45430.0        130\n2   3   45430.0         96\n3   4   45430.0         77\n4   5   45431.0        130\n5   6   45432.0         92\n6   7   45433.0         77\n\nHow can i transform this to df1 so that we are counting the number of different events each people_id has with eachother?  The output of df1 should look like this:\nprint(df1)\n\n   30  77  96  130\n30   1  1   1   1\n77   1  2   1   2\n96   1  1   1   1\n130  1  2   1   2\n\n'
"I have trouble using dict to replace values in a dataframe.\n    m= { 'january': 1, 'february': 2, 'march': 3, 'april': 4, 'may': 5,'june': 6,'july': 7,\n'august': 8, 'september': 9, 'october': 10, 'november': 11, 'december': 12}\n\ndf = pd.DataFrame({'place': {0: 'canada', 1: 'canada', 2: 'the united states'}, 'time': {0:  'february 11, 2018', 1: 'december 9, 2017', 2: 'january 18, 2018'}})\n\n   place                      time\n0  canada              february 11, 2018\n1  canada              december 9, 2017\n2  the united states   january 18, 2018\n\nI want to only change the month names to the  and I had tried .replace and .map methods but not thing changes\ndftp2.replace({'time': m })\ndftp2['time'].map(m)\n\ndid I do something wrong or are there any other ways to do this?\n"
"Here's my series which had been tokenized and removed stop words:\n0        [laptop, sits, 4, stars, similarly, priced, co...\n1        [ordered, monitor, wanted, makeshift, area, po...\n2        [monitor, great, deal, price, size, ., use, of...\n3        [bought, height, adjustment, ., swivel, abilit...\n4        [worked, month, died, ., 5, calls, hp, support...\n                               ...                        \n30618                                        [great, deal]\n30619                                  [pour, le, travail]\n30620                                      [business, use]\n30621                                         [good, size]\n30622    [pour, mon, ordinateur.plus, grande, image.vra...\nName: text_body, Length: 30623, dtype: object\n\nI want to remove punctuation from the above series. I had tried something like this\nfiltered_text = re.sub(r'[^\\w\\s]','',str(series))\n\nthe result comes out as a string.\n2 questions I have.\n\nis there a way to convert the filtered_text string to back to list or series?\nare there better ways to Remove punctuation from the original series?\n\n"
'I\'m trying to create a script that essentially will allow me to create a list with specific items from the lines that can be inserted into an SQL DB. I have multiple lines like the following in a text file "addresses.txt":\n\n{"status":"OK","message":"OK","data":[{"type":"addressAccessType","addressAccessId":"0a3f508f-e7c8-32b8-e044-0003ba298018","municipalityCode":"0766","municipalityName":"Hedensted","streetCode":"0072","streetName":"Værnegården","streetBuildingIdentifier":"13","mailDeliverySublocationIdentifier":"","districtSubDivisionIdentifier":"","postCodeIdentifier":"8000","districtName":"Århus","presentationString":"Værnegården 13, 8000 Århus","addressSpecificCount":1,"validCoordinates":true,"geometryWkt":"POINT(553564 6179299)","x":553564,"y":6179299}]}\n\n\nFor example I want to remove \n\n"type":"addressAccessType","addressAccessId":"0a3f508f-e7c8-32b8-e044-0003ba298018"\n\n\nAnd in the end up with a column list and a value list that can be written to a file_output.txt like:\n\nINSERT INTO ADDRESSES (%s) VALUES (%s)\n\n\nThis is what I have so far\n\n# Writes %s into the file output_data.txt\naddress_line = """INSERT INTO ADDRESSES (%s) VALUES (%s)"""\n\n# Reads every line from the file messy_data.txt\nmessy_string = file("addresses.txt").readlines()\n\ncols = messy_string[0].split(",")  #Defines each word in the first line separated by , as a column name\ncolstr = \',\'.join(cols) # formatted string that will plug in nicely\noutput_data = file("output_data.txt", \'w\') # Creates the output file: output_data.txt\nfor r in messy_string[0:]: # loop through everything after first line\n    #r = r.replace(\':\',\',\')\n    #temp_replace = r.translate(None,\'"{}[]()\')\n    #address_list = temp_replace.split(",")\n    #address_list = [x.encode(\'utf-8\') for x in address_list]\n    vals = r.split(",") # split at ,\n    valstr = \',\'.join(vals) # join with commas for sql\n    output_data.write(address_line % (colstr, valstr))  # write to file\n\noutput_data.close()\n\n\nIf included some of my out commented attempts, maybe it can help. Also I noticed that when ever I use #address_list = temp_replace.split(","), all of my utf-8 characters is screwed uo, and I do not know why or how to correct this.\n\nUPDATE\nLooking at this example How can I convert JSON to CSV?\nI have come up with this code to fix my problem:\n\n# Reads every line from the file coordinates.txt\nmessy_string = file("coordinates.txt").readlines()\n\n# Reads with the json module\nx = json.loads(messy_string\n\nx = json.loads(x)\nf = csv.writer(open(\'test.csv\', \'wb+\'))\n\nfor x in x:\nf.writerow([x[\'status\'], \n            x[\'message\'], \n            x[\'data\'][\'type\'], \n            x[\'data\'][\'addressAccessId\'],\n            x[\'data\'][\'municipalityCode\'],\n            x[\'data\'][\'municipalityName\'],\n            x[\'data\'][\'streetCode\'],\n            x[\'data\'][\'streetName\'],\n            x[\'data\'][\'streetBuildingIdentifier\'],\n            x[\'data\'][\'mailDeliverySublocationIdentifier\'],\n            x[\'data\'][\'districtSubDivisionIdentifier\'],\n            x[\'data\'][\'postCodeIdentifier\'],\n            x[\'data\'][\'districtName\'],\n            x[\'data\'][\'presentationString\'],\n            x[\'data\'][\'addressSpecificCount\'],\n            x[\'data\'][\'validCoordinates\'],\n            x[\'data\'][\'geometryWkt\'],\n            x[\'data\'][\'x\'],\n            x[\'data\'][\'y\']])\n\n\nHowever, this does not fix my problem, now I get the following error\n\nTraceback (most recent call last):\n  File "test2.py", line 10, in &lt;module&gt;\n    x = json.loads(messy_string)\n  File "C:\\Python27\\lib\\json\\__init__.py", line 338, in loads\n    return _default_decoder.decode(s)\n  File "C:\\Python27\\lib\\json\\decoder.py", line 365, in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\nTypeError: expected string or buffer\n\n\nCan anyone help? Thanks in advance.\n'
'Seems totally easy but having a braincramp trying to get this done. I have an API request the receives the following as a response. \n\n[{u\'text\': u\'test ASDF\', u\'ts\': u\'1453578721.000005\', u\'user\': u\'U0K7P5EBG\', u\'team\': u\'T0K7K1KEH\', u\'type\': u\'message\', u\'channel\': u\'D0K7P9MCJ\'}]\n\n\nOR\n\n[{u\'type\': u\'hello\'}]\n\n\nI want to process that response, checking for keys, data, etc. I\'ve attempted to create dict() from the list, attempted to load as json() and I can\'t seem to get the data into a format that is easy to process. \n\n&gt;&gt;&gt; a = [{u\'text\': u\'test ASDF\', u\'ts\': u\'1453578721.000005\', u\'user\': u\'U0K7P5EBG\', u\'team\': u\'T0K7K1KEH\', u\'type\': u\'message\', u\'channel\': u\'D0K7P9MCJ\'}]\n&gt;&gt;&gt; dict(a)\nTraceback (most recent call last):\nFile "&lt;stdin&gt;", line 1, in &lt;module&gt;\nValueError: dictionary update sequence element #0 has length 6; 2 is required\n&gt;&gt;&gt; a = [{u\'text\': u\'test ASDF\', u\'ts\': u\'1453578721.000005\', u\'user\': u\'U0K7P5EBG\', u\'team\': u\'T0K7K1KEH\', u\'type\': u\'message\', u\'channel\': u\'D0K7P9MCJ\'}]\n&gt;&gt;&gt; type(a)\n&lt;type \'list\'&gt;\n&gt;&gt;&gt; a[0]\n{u\'text\': u\'test ASDF\', u\'ts\': u\'1453578721.000005\', u\'user\': u\'U0K7P5EBG\', u\'team\': u\'T0K7K1KEH\', u\'type\': u\'message\', u\'channel\': u\'D0K7P9MCJ\'}\n\n\nI want the data in a simple format like d[k] where I can check for existing of keys and get their values. A dictionary. How can I get this data formatted properly?\n\nd["text"] -&gt; would yield \'test ASDF\'\netc\n\n\nTIA\n'
'Following is the code and corresponding output to extract data of a particular job from Indeed.com. Alongwith data I have lot of junk and I want to separate out Title, location, job description and other important features. How can I convert it into dictionaries?\n\nfrom bs4 import BeautifulSoup \nimport urllib2 \nfinal_site = \'http://www.indeed.com/cmp/Pullskill-techonoligies/jobs/Data-Scientist-229a6b09c5eb6b44?q=%22data+scientist%22\'\nhtml = urllib2.urlopen(final_site).read()\nsoup = BeautifulSoup(html)\ndeep = soup.find("td","snip")\ndeep.get("p","ul")\ndeep.get_text(strip= True)\n\n\nOutput:\n\nu\'Title : Data ScientistLocation : Seattle WADuration : Fulltime / PermanentJob Responsibilities:Implement advanced and predictive analytics models usingJava,R, and Pythonetc.Develop deep expertise with Company\\u2019s data warehouse, systems, product and other resources.Extract, collate and analyze data from a variety of sources to provide insights to customersCollaborate with the research team to incorporate qualitative insights into projects where appropriateKnowledge, Skills and Experience:Exceptional problem solving skillsExperience withJava,R, and PythonAdvanced data mining and predictive modeling (especially Machine learning techniques) skillsMust have statistics orientation (Theory and applied)3+ years of business experience in an advanced analytics roleStrong Python and R programming skills are required. SAS, MATLAB will be plusStrong SQL skills are looked for.Analytical and decisive strategic thinker, flexible problem solver, great team player;Able to effectively communicate to all levelsImpeccable attention to detail and very strong ability to convert complex data into insights and action planThanksNick ArthurLead Recruiternick(at)pullskill(dot)com201-497-1010 Ext: 106Salary: $120,000.00 /yearRequired experience:Java And Python And R And PHD Level Education: 4 years5 days ago-save jobwindow[\\\'result_229a6b09c5eb6b44\\\'] = {"showSource": false, "source": "Indeed", "loggedIn": false, "showMyJobsLinks": true,"undoAction": "unsave","relativeJobAge": "5 days ago","jobKey": "229a6b09c5eb6b44", "myIndeedAvailable": true, "tellAFriendEnabled": false, "showMoreActionsLink": false, "resultNumber": 0, "jobStateChangedToSaved": false, "searchState": "", "basicPermaLink": "http://www.indeed.com", "saveJobFailed": false, "removeJobFailed": false, "requestPending": false, "notesEnabled": true, "currentPage" : "viewjob", "sponsored" : false, "reportJobButtonEnabled": false};\\xbbApply NowPlease review all application instructions before applying to Pullskill Technologies.(function(d, s, id){var js, iajs = d.getElementsByTagName(s)[0], iaqs = \\\'vjtk=1aa24enhqagvcdj7&amp;hl=en_US&amp;co=US\\\'; if (d.getElementById(id)){return;}js = d.createElement(s); js.id = id; js.async = true; js.src = \\\'https://apply.indeed.com/indeedapply/static/scripts/app/bootstrap.js\\\'; js.setAttribute(\\\'data-indeed-apply-qs\\\', iaqs); iajs.parentNode.insertBefore(js, iajs);}(document, \\\'script\\\', \\\'indeed-apply-js\\\'));Recommended JobsData Scientist, Energy AnalyticsRenew Financial-Oakland, CARenew Financial-5 days agoData ScientistePrize-Seattle, WAePrize-7 days agoData ScientistDocuSign-Seattle, WADocuSign-12 days agoEasily applyEngineer - US Citizen or Permanent ResidentVoxel Innovations-Raleigh, NCIndeed-8 days agoEasily applyData ScientistUnity Technologies-San Francisco, CAUnity Technologies-22 days agoEasily apply\'\n\n'
'I try to use R load data from mongodb with package "mongolite", and the code like this:\n\ndf &lt;- db$find(\'{}\', \'{"CurrentId":1,"_id":0}\')\n\n\nWhich I want to extract the "CurrentId" of collection, and variable "CurrentId" is ObjectId in mongodb, which may contains several ObjectId.\n\nand the df look like this:\n\n[[1]]\nlist()\n\n[[2]]\nlist()\n\n[[3]]\nlist()\n\n[[4]]\nlist()\n\n[[5]]\nlist()\n\n[[6]]\n[[6]][[1]]\n[1] 56 cd 5f 02 b8 9b 5b d0 26 cb 39 c9\n\n[[6]][[2]]\n[1] 56 cd 6c 13 b8 9b 5b d0 26 cb 39 d5\n\n[[6]][[3]]\n[1] 56 cd 6f c6 b8 9b 5b d0 26 cb 39 de\n\n\nAnd df[[6]][[1]]  is :\n\n [1] 56 cd 5f 02 b8 9b 5b d0 26 cb 39 c9\n\n\nthe type of typeof(df[[6]][[1]]) is :\n\n [1] "raw"\n\n\nI use paste(dc3[[6]][[1]],collapse = \'\') convert the raw type to string, just like mongodb ObjectId format:\n\n [1] "56cd5f02b89b5bd026cb39c9"\n\n\nAnd then I try to convert all the raw data in df to string like above. So I use sapply function:\n\nsapply(df, function(x) paste(as.character(x),collapse = \'\'))\n\nand got this :\n\n[1] ""                                                                                                                                                                                                                                                   \n[2] ""                                                                                                                                                                                                                                                   \n[3] ""                                                                                                                                                                                                                                                   \n[4] ""                                                                                                                                                                                                                                                   \n[5] ""                                                                                                                                                                                                                                                   \n[6] "as.raw(c(0x56, 0xcd, 0x5f, 0x02, 0xb8, 0x9b, 0x5b, 0xd0, 0x26, 0xcb, 0x39, 0xc9))as.raw(c(0x56, 0xcd, 0x6c, 0x13, 0xb8, 0x9b, 0x5b, 0xd0, 0x26, 0xcb, 0x39, 0xd5))as.raw(c(0x56, 0xcd, 0x6f, 0xc6, 0xb8, 0x9b, 0x5b, 0xd0, 0x26, 0xcb, 0x39, 0xde))"\n\n\nBut I want to get something like this:\n\n[[1]]\nlist()\n\n[[2]]\nlist()\n\n[[3]]\nlist()\n\n[[4]]\nlist()\n\n[[5]]\nlist()\n\n[[6]]\n[[6]][[1]]\n[1] "56cd5f02b89b5bd026cb39c9"\n\n[[6]][[2]]\n[1] "56cd6c13b89b5bd026cb39d5"\n\n[[6]][[3]]\n[1] "56cd6fc6b89b5bd026cb39de"\n\n\nCould anyone know how to handle this? And could there be more efficient way to do the whole work?\n\nUpdate:\n\nI should give some code to reproduce my origin dataset:\n\ntest = as.raw(as.hexmode(x = c("56","cd","5f","02","b8","9b","5b","d0","26","cb","39","c9")))\ndf = lapply(1:10,function(x) test)\n\n\nalthough this code produce this:\n\n[[1]]\nlist()\n\n[[2]]\n[[2]][[1]]\n[1] 5f\n\n[[2]][[2]]\n[1] d0\n\n\n[[3]]\n[[3]][[1]]\n[1] 26\n\n[[3]][[2]]\n[1] 56\n\n\n[[4]]\nlist()\n\n[[5]]\n[[5]][[1]]\n[1] cb\n\n\n[[6]]\nlist()\n\n\nIt\'s not like original df, but I really don\'t know how to paste Raw data in nested list, hope this may help you!\n\nthe result of  sapply(df, function(x) paste(x,collapse = \'\')) is just like this:\n\n[1] ""                                                                                                                                                                                                                                                   \n[2] ""                                                                                                                                                                                                                                                   \n[3] ""                                                                                                                                                                                                                                                   \n[4] ""                                                                                                                                                                                                                                                   \n[5] ""                                                                                                                                                                                                                                                   \n[6] "as.raw(c(0x56, 0xcd, 0x5f, 0x02, 0xb8, 0x9b, 0x5b, 0xd0, 0x26, 0xcb, 0x39, 0xc9))as.raw(c(0x56, 0xcd, 0x6c, 0x13, 0xb8, 0x9b, 0x5b, 0xd0, 0x26, 0xcb, 0x39, 0xd5))as.raw(c(0x56, 0xcd, 0x6f, 0xc6, 0xb8, 0x9b, 0x5b, 0xd0, 0x26, 0xcb, 0x39, 0xde))"\n\n'
'First, I want to grab this kind of string from a text file \n\n{kevin.knerr, sam.mcgettrick, mike.grahs}@google.com.au\n\nAnd then convert it to separate strings such as\n\nkevin.knerr@google.com.au \n\nsam.mcgettrick@google.com.au\n\nmike.grahs@google.com.au\n\nFor example text file can be as:\n\nSome gibberish words\n\n{kevin.knerr, sam.mcgettrick, mike.grahs}@google.com.au\n\nSome Gibberish words\n'
"I am trying to clean only one column from the long and big data sets. The data has 18 columns, more than 10k+ rows about 100s of csv files, Of which I want to clean only one column. \n\nInput fields only few from the long list\n\nuserLocation,   userTimezone,   Coordinates,\nIndia,          Hawaii,    {u'type': u'Point', u'coordinates': [73.8567, 18.5203]}\nCalifornia,     USA     \n          ,     New Delhi,  \nFt. Sam Houston,Mountain Time (US &amp; Canada),{u'type': u'Point', u'coordinates': [86.99643, 23.68088]}\nKathmandu,Nepal, Kathmandu, {u'type': u'Point', u'coordinates': [85.3248024, 27.69765658]}\n\n\nFull input file: Dropbox link\n\nCode: \n\n    import pandas as pd\n\n    data = pandas.read_cvs('input.csv')\n\n    df =  ['tweetID', 'tweetText', 'tweetRetweetCt', 'tweetFavoriteCt',       \n           'tweetSource', 'tweetCreated', 'userID', 'userScreen',\n           'userName', 'userCreateDt', 'userDesc', 'userFollowerCt', \n           'userFriendsCt', 'userLocation', 'userTimezone', 'Coordinates',\n           'GeoEnabled', 'Language']\n\n    df0 = ['Coordinates'] \n\n\nOther columns are to written as it is in output. After this how to go about ?\n\nOutput:\n\nuserLocation,   userTimezone, Coordinate_one, Coordinate_one,\nIndia,          Hawaii,         73.8567, 18.5203\nCalifornia,     USA     \n          ,     New Delhi,  \nFt. Sam Houston,Mountain Time (US &amp; Canada),86.99643, 23.68088\nKathmandu,Nepal, Kathmandu, 85.3248024, 27.69765658\n\n\nThe possible easiest suggestion or direct me to some example will be a lot helpful. \n"
"I have a Data frame something like this.\n\nAman Aggarwal      Amar Jannela   Vipin Kumar       Roshan Pati\nBlackBuck          DJ CHETAS      WOW Editions      MensXP\nTransport/Freight  Musician/Band  Furniture         News/Media Website\nLike               Like           Like              Like\nNaN                NaN            NaN               NaN   \nGiveMeSport        NaN            500 Startups      No Abuse KG\nNews/Media Website Celina Jaitly  Internet/Software Community\nLike               Actor/Director Like              Liked\nNaN                Like           NaN               NaN\nNaN                NaN            Jitendra Kumar    Monogatari Series\nAnushka Sharma     Durjoy Datta   Actor/Director    TV Show\nActor/Director     Author         Liked             Like\nLike               Like           NaN               NaN\nNaN                NaN            NaN               NaN\n\n\nObviously NaN are empty rows in original csv file. I have to extract two data frames from this.Column name as first element of every row in new datafra and page_name(BlackBuck) element of that column as further element of respective rows. Something like this.\n\nAman Aggarwal     BlackBuck        GiveMeSport    Anushka Sharma \nAmar Jannela      DJ CHETAS        Celina Jaitly  Durjoy Datta \nVipin Kumar       WOW Editions     500 Startups   Jitendra Kumar\nRoshan Pati       MensXP           No Abuse KGP   Monogatari Series\n\n\n2nd dataframe also similar like this \n\nAman Aggarwal   Transport/Freight  News/Media Website  Actor/Director\nAmar Jannela       Musician/Band      Actor/Director          Author\nVipin Kumar           Furniture   Internet/Software  Actor/Director\nRoshan Pati  News/Media Website           Community         TV Show\n\n\nThe real issue is that there is arbitrary NaN values some where ank may be like/liked too but only thing is that name(BlackBuck) and category(Transport/Freight) are together.Since my coe cant identify which is page_name and which is category. So probably I have to remove NaN value and 'Like' and 'Liked'  separately for each column first and then aligned accordingly and transpose. How to this efficiently in python2.7.  \n"
'I want to split the following strings to get each attribute seperately:\n\nString_one: \'archived\': True, \'id\': \'30znq1\', \'_has_fetched\': True\nString_two: \\\\\'hidden\\\\\': False, \\\\\'user_reports\\\\\': [], \\\\\'num_reports\\\\\': None\n\n\nI know that all of the attributes end with either ", \'" or ", \\\\\'"\nInitially I was just using two string.spilt()\n\n1) line.split(\', \\\'\')\n2) line.split(\', \\\\\\\'\')\n\n\nThese both did the job, but I am working with a large amount of data and both of these are very general and the pattern was matched somewhere in the data it wasn\'t supposed to.\n\nSo I tried a re.split instead:\n\nreg_split_no_esc = re.compile(\', \\\'(.*\\\': .*)\')\nreg_split_esc    = re.compile(\', \\\\\\\'(.*\\\\\\\': .*)\')\n\nline = re.split(reg_split_esc, line)\nline = re.split(reg_split_no_esc, line)\n\n\nThe regular expressions are meant to find the first ", \\\'" and the split on this if it is followed by characters a single quote and a colon, its meant to catch what follows the ", \\\'".\n\nreg_split_no_esc split on the first split condition but not for anymore, the other pattern is just not working.\n\nAny help would be really appreciated\n\nEDIT:\nMy first regular expression works but it only splits the first element and leaves the rest un-split so it looks like this:\n\n_uniq\': None\n\nsuggested_sort\': None, \'secure_media_embed\': {} \'report_reasons\': None, \'_params\': {}\n\n'
'I have a dataframe df in python\n\nAge     product\n------------------\n21          apple\n11          orange\neighteen    mango\n35          pineapple\n35          122\nNA          apple\n30          -1\n\n\nI only want numeric columns in age, how would I drop the rows which are not integers.\n\nSimilarly in product, I need only strings, how would I drop the values which are not strings. \n'
'My List:\n\n\n  [\'\\n\\r\\n\\tThis article is about sweet bananas. For the genus to which\n  banana plants belong, see Musa (genus).\\n\\r\\n\\tFor starchier bananas\n  used in cooking, see Cooking banana. For other uses, see Banana\n  (disambiguation)\\n\\r\\n\\tMusa species are native to tropical Indomalaya\n  and Australia, and are likely to have been first domesticated in Papua\n  New Guinea.\\n\\r\\n\\tThey are grown in 135\n  countries.\\n\\n\\n\\r\\n\\tWorldwide, there is no sharp distinction between\n  "bananas" and "plantains".\\n\\nDescription\\n\\r\\n\\tThe banana plant is\n  the largest herbaceous flowering plant.\\n\\r\\n\\tAll the above-ground\n  parts of a banana plant grow from a structure usually called a\n  "corm".\\n\\nEtymology\\n\\r\\n\\tThe word banana is thought to be of West\n  African origin, possibly from the Wolof word banaana, and passed into\n  English via Spanish or Portuguese.\\n\']\n\n\nExample code:\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\nre=requests.get(\'http://www.abcde.com/banana\')\nsoup=BeautifulSoup(re.text.encode(\'utf-8\'), "html.parser")\ntitle_tag = soup.select_one(\'.page_article_title\')\nprint(title_tag.text)\nlist=[]\nfor tag in soup.select(\'.page_article_content\'):\n    list.append(tag.text)\n#list=([c.replace(\'\\n\', \'\') for c in list])\n#list=([c.replace(\'\\r\', \'\') for c in list])\n#list=([c.replace(\'\\t\', \'\') for c in list])\nprint(list)\n\n\nAfter I scraping a web page, I need to do data cleansing. I want to replace all the "\\r", "\\n", "\\t" as "", but I found that I have subtitle in this, if I do this, subtitles and sentences are going to mix together. \n\nEvery subtitle always starts with \\n\\n and ends with \\n\\r\\n\\t, is it possible that I can do something to distinguish them in this list like \\aEtymology\\a. It\'s not going to work if I replace \\n\\n and \\n\\r\\n\\t separately to \\a first cause other parts might have the same elements like this \\n\\n\\r and it will become like \\a\\r. Thanks in advance!\n'
"I have multi-index column in my df. In my df, all the values are 1 or 0 that represent boolean. My task is to replace values with '1' with value from my another df_test dataframe. See below.\n\nIn [221]: df\nOut[221]:\nfirst        bar                 baz\nsecond       one       two       one       two\n0            0         1         0         0\n1            1         0         1         1\n2            0         0         0         1\n3            0         0         0         0\n4            1         1         1         1\n..............(continues)\n\n\nMy df_test has regular columns (not multi index) and values that should go in to df. \n\nIn [222]: df_test\nOut[222]:\n        amount\n0            38\n1            2179   \n2            191     \n3            4     \n4            19823    \n..............(continues)\n\n\nThe index of two dataframe matches and my output should be:\n\nIn [223]: df\nOut[223]:\nfirst        bar                 baz\nsecond       one       two       one       two\n0            0         38        0         0\n1            2179      0         2179      2179      \n2            0         0         0         191     \n3            0         0         0         0\n4            19823     19823     19823     19823    \n..............(continues)\n\n\nNotice that my df can have no '1' value like index = 3, or all '1' value like index = 4. If there are efficient way to set my dataframe \n"
"numeric_cols = ['temp', 'windchill', 'dewpoint', 'humidity', 'pressure', 'visibility', 'wind_speed', 'gust_speed', 'precip']\nweather_list[numeric_cols] = weather_list[numeric_cols].apply(lambda x: re.sub('[^0-9]', '', str(x)))\nweather_list[numeric_cols] = pd.to_numeric(weather_list[numeric_cols], errors='coerce')\nweather_list[numeric_cols] = weather_list[numeric_cols] / 10\n\n\nI'm trying to do a bit of cleaning of a data set but I'm getting the shape mismatch error. Going by the error, its saying I'm matching 30 columns, 0 rows with 9 columns, 30 rows... I've clearly done something wrong! I've used this method a few times before but never got error - anyone have any suggestions as to what I'm doing wrong? Data pulled from html to pandas df 'weather_list'.\n\ntime     temp windchill dewpoint humidity  pressure visibility  \\\n0   12:53 AM  21.0 °F         -  19.0 °F      92%  30.47 in    10.0 mi   \n1    1:53 AM  21.9 °F         -  19.9 °F      92%  30.48 in    10.0 mi   \n2    2:53 AM  21.9 °F         -  19.0 °F      89%  30.50 in    10.0 mi   \n3    3:53 AM  21.0 °F         -  19.0 °F      92%  30.50 in    10.0 mi   \n4    4:53 AM  19.9 °F         -  18.0 °F      92%  30.51 in    10.0 mi   \n5    5:53 AM  21.0 °F         -  18.0 °F      88%  30.51 in    10.0 mi   \n\n\n   wind_direction wind_speed gust_speed  precip  events conditions  \n0            Calm       Calm          -     NaN     NaN      Clear  \n1            Calm       Calm          -     NaN     NaN      Clear  \n2            Calm       Calm          -     NaN     NaN      Clear  \n3            Calm       Calm          -     NaN     NaN      Clear  \n4            Calm       Calm          -     NaN     NaN      Clear  \n5            Calm       Calm          -     NaN     NaN      Clear\n\n\nThanks!\n"
'I have a pandas data frame that has two columns both of them in object format. They contain year (4 means 2004) and month. I want to subtract them.\n\nstart     end\n4-oct     12-nov\ndec-3     11-oct\njan-5     16-dec\n12-oct    17-apr\n\n\nI tried:\n\ndata[\'end\'].apply(lambda x: datetime.strptime(repr(x), "\'%y-%b\'"))\ndata[\'end\'].apply(lambda x: datetime.strptime(repr(x), "b\'%y-%b\'"))\n\n\nBut they did not work.\n\n\nHow can I deal with the different format and non zero padding in the first column (\'%y-%b\' and \'%b-%y\') \nHow can I apply strptime() to object format? (can repr() convert them to string)?\n\n'
"I need solution to the following string in my data set. Need to be splitted into various words to get meaningful insights.\n\na='(Barbecue)Cheese(earthyCamembert,Fontina,nuttyAsiago,Colby,Parmesan)General(Chocolate)Meat(Beef)\n\nHere the first words (Barbecue) - represent cusine\nsecond word - Cheese(earthyCamembert,Fontina,nuttyAsiago,Colby,Parmesan)\nthird word - General(Chocolate)\nfourth word - Meat(Beef)\n\nLike this above example in need to split it into 4 categories. can anyone help me out to code it python. I am new to this. Thanks.\n"
'Parsing a large data set of poor quality data converted from pysical form using OCR and using PostgreSQL COPY to insert .csv files into psql.  Some records have ASCII bytes that are causing errors to import into postgres since I want the data in UTF-8 varchar(), as I believe that using a TEXT type column would not produce this error.  \n\nDataError: invalid byte sequence for encoding "UTF8": 0xd6 0x53\nCONTEXT:  COPY table_name, line 112809\n\n\nI want to filter all these bytes before writing to the csv file.\n\nI believe something like PHP\'s FILTER_FLAG_STRIP_HIGH (http://php.net/manual/en/filter.filters.sanitize.php) would work since it can remove all  high ASCII value > 127.\n\nIs there such a function in python?  \n'
"\ndata.csv\n1, 22, 3432\n1, 23, \\N\n2, 24, 54335\n2, 25, 3928\n\nI have a csv file of data that is collected from a device. Every now and then the device doesn't relay information and outputs '\\N'. I want to treat these as NaN and did this by doing\nread_csv(data.csv, na_values=['\\\\N']) \n\nwhich worked fine. However, I would prefer to have not only this string turned to NaN but any string that is in the csv file just in case the data I get in the future has a different string.\nIs it possible to me to make any changes in the argument so it covers all strings?\n"
'I have a dataset like this\n\ndataset = pd.read_csv(\'1053.csv\')\nprint dataset.head(25)\n\n                  date  power\n0    2009-7-14 0:30:00  0.039\n1    2009-7-14 1:00:00  0.147\n2    2009-7-14 1:30:00  0.134\n3    2009-7-14 2:00:00  0.131\n4    2009-7-14 2:30:00  0.076\n5    2009-7-14 3:00:00  0.039\n6    2009-7-14 3:30:00  0.039\n7    2009-7-14 4:00:00  0.052\n8    2009-7-14 4:30:00  0.148\n9    2009-7-14 5:00:00  0.136\n10   2009-7-14 5:30:00  0.132\n11   2009-7-14 6:00:00  0.060\n12   2009-7-14 6:30:00  0.034\n13   2009-7-14 7:00:00  0.034\n14   2009-7-14 7:30:00  0.033\n15   2009-7-14 8:00:00  0.326\n16   2009-7-14 8:30:00  0.140\n17   2009-7-14 9:00:00  0.133\n18   2009-7-14 9:30:00  0.107\n19  2009-7-14 10:00:00  0.161\n20  2009-7-14 10:30:00  0.042\n21  2009-7-14 11:00:00  1.259\n22  2009-7-14 11:30:00  1.227\n23  2009-7-14 12:00:00  0.167\n24  2009-7-14 12:30:00  0.518\n\n\nHow to extract rows containing exact time? for example.\nI tried to do like this\n\ndf = dataset[dataset.date.str.contains("2:00:00",regex=False)]\ndf1 = df.reset_index()\ndel df1[\'index\']\nprint df1.head(7)\n\n\nBut it is giving me this results.  \n\n                  date  power\n0    2009-7-14 2:00:00  0.131\n1   2009-7-14 12:00:00  0.167\n2   2009-7-14 22:00:00  0.208\n3    2009-7-15 2:00:00  0.085\n4   2009-7-15 12:00:00  0.097\n5   2009-7-15 22:00:00  0.203\n6    2009-7-16 2:00:00  0.038\n\n\nI want results to be like this\n\n                  date  power\n0    2009-7-14 2:00:00  0.131\n1    2009-7-15 2:00:00  0.085\n2    2009-7-16 2:00:00  0.038\n3    2009-7-17 2:00:00  0.141\n4   2009-7-18 2:00:00  0.039\n5   2009-7-19 2:00:00  0.039\n6   2009-7-20 2:00:00  0.079\n\n'
'I am working with pandas and have a csv file that looks like this \n\n  ID                Name        Store      Price           \nMelbourne           \n    1               aaaa        bbbb        570\n    2               cccc        dddd        236\n    3               eeee        ffff        230\nSydney\n    1               hhhh        gggg        2300\n    2               kkkk        llll        266\n\n\nI want the it in this shape\n\nCity            ID               Name        Store       Price      \nMelbourne        1               aaaa        bbbb        570\nMelbourne        2               cccc        dddd        236\nMelbourne        3               eeee        ffff        230\nSydney           1               hhhh        gggg        23\nSydney           2               kkkk        llll        266\n\n\nWhat I am thinking is \n1. adding a new column\n\n  ID        New               Name        Store      Price           \nMelbourne   NaN  \n    1       NaN               aaaa        bbbb        570\n    2       NaN               cccc        dddd        236\n    3       NaN               eeee        ffff        230\nSydney \n    1       NaN               hhhh        gggg        2300\n    2       NaN               kkkk        llll        266\n\n\n\nthen change the index to ID. So it would look like this\n\n  ID       New      Name        Store      Price           \n Melbourne NaN  \n 1         NaN      aaaa        bbbb        570\n 2         NaN      cccc        dddd        236\n 3         NaN      eeee        ffff        230\n Sydney    NaN\n 1         NaN      hhhh        gggg        2300\n 2         NaN      kkkk        llll        266\n\nand then something like this\n\n  ID         New              Name        Store      Price           \n Melbourne   NaN  \n Melbourne    1               aaaa        bbbb        570\n Melbourne    2               cccc        dddd        236\n Melbourne    3               eeee        ffff        230\n Sydney       NaN\n Sydney       1               hhhh        gggg        2300\n Sydney       2               kkkk        llll        266\n\nFinally change the column name and delete the rows without values\n\nCity            ID              Name        Store      Price           \nMelbourne       1               aaaa        bbbb        570\nMelbourne       2               cccc        dddd        236\nMelbourne       3               eeee        ffff        230\nSydney          1               hhhh        gggg        2300\nSydney          2               kkkk        llll        266\n\n\nI am not sure if it can be implemented or not. Please give me some idea about how can I implement this.\n\n'
"I am working on a data frame which has 4 columns in total, i want to bin each column of that data frame iteratively in 8 equal parts. The bin number should be assigned to the data in a separate column for each column.\nThe code should work even if any different data frame is provided with different column names.\nHere, is the code i tried.\n\nfor c in df3.columns:\n    df3['bucket_' + c] = (df3.max() - df3.min()) // 2 + 1\n    buckets = pd.cut(df3['bucket_' + c], 8, labels=False) \n\n\nsample data frame\n\nexpected output\n\nThe respected bin columns display the bin number assigned to each data point according to the range in which they will (using pd.cut to cut column in 8 equal parts) fall.\nThanks in advance!!\n\nsample data\n\ngp1_min gp2 gp3 gp4\n\n17.39   23.19   28.99   44.93\n\n0.74    1.12    3.35    39.78\n\n12.63   13.16   13.68   15.26\n\n72.76   73.92   75.42   94.35\n\n77.09   84.14   74.89   89.87\n\n73.24   75.72   77.28   92.3\n\n78.63   84.35   64.89   89.31\n\n65.59   65.95   66.49   92.43\n\n76.79   83.93   75.89   89.73\n\n57.78   57.78   2.22    71.11\n\n99.9    99.1    100      100\n\n100     100    40.963855    100\n\n\nexpected output\n\ngp1_min gp2 gp3 gp4 bin_gp1 bin_gp2 bin_gp3 bin_gp4\n\n17.39   23.19   28.99   44.93   2   2   2   3\n\n0.74    1.12    3.35    39.78   1   1   1   3\n\n12.63   13.16   13.68   15.26   1   2   2   2\n\n72.76   73.92   75.42   94.35   5   6   6   7\n\n77.09   84.14   74.89   89.87   6   7   6   7\n\n73.24   75.72   77.28   92.3    6   6   6   7\n\n78.63   84.35   64.89   89.31   6   7   5   7\n\n65.59   65.95   66.49   92.43   5   6   5   7\n\n76.79   83.93   75.89   89.73   6   7   6   7\n\n57.78   57.78   2.22    71.11   4   4   1   6\n\n99.9    99.1    100      100    8   8   8   8\n\n100      100    40.96    100    8   8   3   8\n\n"
'1How do I create another csv file from yelp_academic_dataset_business.json that include only business of Hotels, Restaurant, or both categories?\n\nThe original yelp business dataset contains rows that have only Dentists, Hair Salons, etc. I want to select business of hotels, restaurants, and both only.\n\nI just started learning python and is following a tutorial of a machine learning experiment. The below code is giving me an error. I have googled and read a lot but still don\'t understand. \nAny help will be appreciated. \n\nImage of what the yelp_academic_dataset_business.csv look like\n\nimage of code and error message\n\ndata2 = []\nwith open(\'yelp_academic_dataset_business.json\') as f:\n    for line in f:\n        data2.append(json.loads(line))\nlen(data2)\n\nbusiness_id = []\ncity = []\nstate = []\nstars = []\nreview_count = []\ncategories = []\npostal_code = []\nlatitude = []\nlongitude = []\npricerange = []\nis_open = []\nname = []\n\n\nfor entry in range(0, len(data2)): \n    if "Restaurants" in data2[entry]["categories"]:\n        business_id.append(data2[entry][\'business_id\'])\n        name.append(data2[entry][\'name\'])\n        city.append(data2[entry][\'city\'])\n        state.append(data2[entry][\'state\'])\n        stars.append(data2[entry][\'stars\'])\n        postal_code.append(data2[entry][\'postal_code\'])\n        review_count.append(data2[entry][\'review_count\'])\n        categories.append(data2[entry][\'categories\'])\n        latitude.append(data2[entry][\'latitude\'])\n        longitude.append(data2[entry][\'longitude\'])\n        is_open.append(data2[entry][\'is_open\'])\n        if \'RestaurantsPriceRange2\'in data2[entry][\'attributes\']:\n            pricerange.append(data2[entry][\'attributes\'][\'RestaurantsPriceRange2\'])\n        else:\n            pricerange.append(0)\n\ndata2 = {\'business_id \':business_id,\'name\':name,\'city\':city,\'state\':state,\'stars\':stars,\'review_count\':review_count,\n    \'categories\':categories,\'latitude\':latitude,\'longitude\':longitude,\'is_open\':is_open,\'pricerange\':pricerange,\'postal_code\':postal_code}\n\n\nbusiness_data  = pd.DataFrame(data2)\n'
'I am trying to create a dataframe of states and cities.\n\nEach state name in the table I am reading from ends with the letters [edit],city on the other hand either end with (text)[number]\n\nI have used regex to remove the text within the parentheses and square brackets, saved states in a list for states and cities in another list for cities.\n\nI then converted these two lists into a dictionary with the state as the key and city as the value.\n\nHowever there are 517 cities and when I do this I lose 467 cities. I\'m guessing because as it currently stands I am not allowing my dictionary to handle multiple values. My goal is to create a dataframe of 517x2 dimensions with a state column and city column (city matching it\'s state). If I create a dataframe from this dictionary I would therefore only get 50x2 as opposed to 512x2 dimensions.\n\nMy question is; i.) is my reasoning correct,  ii.) how should i think about solving this problem/how should I solve it, iii.) is the code that I have written the most efficient way of reaching my end goal\n\nimport pandas as pd\nimport numpy as np\nimport re\nstate = []\ncity = []\nwith open("university_towns.txt","r") as i:\n    uni = i.readlines()\nfor st in uni:\n    if "[edit]"in st:\n        state.append(re.sub("[\\\\[].*?[\\\\]]\\s", "", st))\n    else:\n        city.append(re.sub("[\\(\\[].*?[\\)\\]]\\s", "", st))\ncity_st = dict(zip(state,city))\n#need to take the key-value pairs/items from the dictionary\ns = pd.Series(city_st, name =\'RegionName\')\ns.index.name = \'State\'\ns = s.reset_index()\ns\n\n\nADD: not quite sure how to add the relevant data for this question\n'
'I am having trouble to remove a space at the beginning of string in pandas dataframe cells. If you look at the dataframe cells, it seems like that there is a space at the start of string however it prints "\\x95 12345" when you output one of cells which has set of chars at the beginning, so as you can see it is not a normal space char but it is rather "\\x95"\n\nI already tried to use strip() - But it didn\'t help.\n\nDataframe was produced after the use of str.split(pat=\',\').tolist() expression which basically split the strings into different cells by \',\' so now my strings have this char added.\n'
"I am trying to get video subtitle data, for that I am using downsub.\n\nThere is no way I have figured out to get clean subtitle, without HTML tags and timestamps, without taking it to notepad and doing replace procedure as appropriate.\n\nThis is cumbersome and I want to automate the cleaning process using python.\n\nhttps://colab.research.google.com/drive/1bbgbjbGF9bjzz3FISMfycSW4iHCj3pxk\n\nI am looking for a really really simple solution that anyone can comprehend with just basic knowledge of Python. I'm open to using the API if need be, but if this takes long, manual cleaning is still only taking a few minutes if done quickly. Automation would be nice though; would relieve a headache. With this considered, please propose a good and nice solution.\n"
'I have a list that has text and numbers as well as empty values. I\'m looking to take:\n\nproducts = [[], [], [], [], [], [], [], [], [], [], [\'productid="6836518"\', \'productid="5965878"\', \'productid="3851171"\'], [\'productid="6455623"\'], [], [\'productid="8024914"\', \'productid="2871360"\', \'productid="6694729"\', \'productid="6760262"\'], [], [], [\'productid="6466698"\', \'productid="5340641"\', \'productid="6071996"\', \'productid="5379225"\'], [\'productid="6683916"\', \'productid="6690577"\', \'productid="7117851"\'], [\'productid="7094467"\'], [\'productid="6628351"\'], [\'productid="5897930"\'], [\'productid="6812437"\', \'productid="5379225"\'], [\'productid="7918467"\', \'productid="7918466"\'], []]\n\n\nAnd return something like:\n\nproducts2 =  [6836518, 5965878, 3851171, 6455623, 8024914, 2871360, 6694729, 6760262, 6466698, 5340641, 6071996, 5379225, 6683916, 6690577, 7117851, 7094467, 6628351, 5897930, 6812437, 5379225, 7918467, 7918466] \n\n'
'I have a csv file that I need to read and parse as Pandas dataframe.\nTheoretically, all the columns should follow a known schema of numerical data and strings.\nI know that some records are broken, either with less number of fields or wrong order.\n\nWhat I would like to do is to get rid of all these problematic rows.\n\nAs a reference, on PySpark I used to use \'DROPMALFORMED\' to filter out records that don\'t match the schema. \n\ndataSchema = StructType([ \n    StructField("col1", LongType(), True), \n    StructField("col2", StringType(), True)])\n\ndataFrame = sqlContext.read \\\n    .format(\'com.databricks.spark.csv\') \\\n    .options(header=\'false\', delimiter=\'\\t\', mode=\'DROPMALFORMED\') \\\n    .load(filename, schema = dataSchema) \n\n\nWith Pandas, I cannot find a simple way to do so. \nFor instance, I thought the this snippet would do the trick but instead it just copies back the wrong value instead of dropping it. \n\ndataFrame[\'col1\'] = dataFrame[\'col1\'].astype(np.int64, errors=\'ignore\')\n\n'
"I need to intelligently combine the values of three columns in a dataframe like the one below. The code needs to select the first type prediction that is True, only the first even if another subsequent prediction is also True. If none of the predictions are True, the value returned should be NaN.\n\nindex    name       t1        t1_check  t2       t2_check  t3       t3_check\n----------------------------------------------------------------------------\n0        cow        animal    True      phone    False     fruit    False\n1        apple      animal    False     fruit    True      food     True\n2        carrot     vehicle   False     veg      True      animal   False\n3        dog        pet       True      animal   True      object   False\n4        horse      window    False     object   False     animal   True\n5        car        pet       False     food     False     fruit    False\n\n\nHere`s what I tried:\n\nFirst I combined the two related columns and dropped the old ones.\n\nIn:\ndf['t1_comb'] = str(df['t1']) + str(df['t1_check'])\ndf['t2_comb'] = str(df['t2']) + str(df['t2_check'])\ndf['t3_comb'] = str(df['t3']) + str(df['t3_check'])\n\ndf.drop(columns=['t1', 't1_check', 't2', 't2_check', 't3', 't3_check'], inplace=True)\n\nOut:\nindex    name       t1_comb         t2_comb        t3_comb\n---------------------------------------------------------------\n0        cow        animalTrue      phoneFalse     fruitFalse\n1        apple      animalFalse     fruitTrue      foodTrue\n2        carrot     vehicleFalse    vegTrue        animalFalse\n3        dog        petTrue         animalTrue     objectFalse\n4        horse      windowFalse     objectFalse    animalTrue\n5        car        petFalse        foodFalse      fruitFalse\n\n\nThen I tried replaces all the entries that contain False with NaN and remove the True string from each entry.\n\nIn:\ndf.loc[df['t1_comb'].str.contains('False'), 't1_comb'] = np.nan\ndf.loc[df['t2_comb'].str.contains('False'), 't2_comb'] = np.nan\ndf.loc[df['t3_comb'].str.contains('False'), 't3_comb'] = np.nan\n\ndf.t1_comb = df.t1_comb.str.replace('True', '')\ndf.t2_comb = df.t2_comb.str.replace('True', '')\ndf.t3_comb = df.t3_comb.str.replace('True', '')\n\nOut:\nindex    name       t1_comb         t2_comb        t3_comb\n---------------------------------------------------------------\n0        cow        animal          NaN            NaN\n1        apple      NaN             fruit          food\n2        carrot     NaN             veg            NaN\n3        dog        pet             animal         NaN\n4        horse      NaN             NaN            animal\n5        car        NaN             NaN            NaN\n\n\nThe next step is where I'm having some difficulties, the part where only the first value is considered.\n\nThe result I need should look something like this:\n\nindex    name       type\n----------------------------\n0        cow        animal\n1        apple      fruit\n2        carrot     veg\n3        dog        pet\n4        horse      animal\n5        car        NaN\n\n"
"My data set has following fields:\n\nUser        Product          Time\n A            10            10-JAN\n B            14            10-JAN\n C            20            10-JAN\n A            12            10-JAN\n B            12            11-JAN\n A            10            12-JAN\n D            08            12-JAN\n A            13            12-JAN\n B            14            13-JAN\n C            20            13-JAN\n A            12            14-JAN\n C            21            14-JAN\n A            10            15-JAN\n\n\nand so on\n\nI want to pull out and display only those users having bought similar product before, with the time stamp of purchase. So something like this:\n\nProductBought      User     Time           count\n    10              A        10-JAN          3\n    10              A        12-JAN          3\n    10              A        15-JAN          3\n    12              A        10-JAN          2\n    12              A        14-JAN          2\n    14              B        10-JAN          2\n    14              B        13-JAN          2\n    20              C        10-JAN          2\n    20              C        13-JAN          2\n\n\nand so on.\n\nI tried using the shift funtion like this\n\ndf.sort_values(by=['User','Time'],ascending=True)    \ndf[(df.User==df.User.shift())&amp;(df.productBought==df.productBought.shift()]\n\n\nbut I am not getting all the results using this. For example, only consecutive results which have same product is being captured. In our case, since before user A bought 10 again, it bought 12, so its not capturing that.\nAlso, if there are two consecutive records of the same user having the same product, the latest one is being showed, as\n\n\n  df==df.shift()\n\n\nonly shows the record which was encountered last, and not all the records which have the same product. \nIs there any way I can achieve what I have displayed above?\n"
'I am having a dataframe with the below example\n\n\n\n\nWhat I wanted to achieve is to combine 2 dataframes based on ColA and also the values in ColC should match between each columns (that is to check whether the value is present in the list) . Could you please suggest an efficient and simple approach to solve this problem? I know this can be done in a normal way by looping through the rows of dataframe 1 and comparing values. But I feel that there should be some other good approach (panda way) to solve the problem. \n\nThank you in advance\n'
'I have a csv file where the columns are all in one row, encased in quotation marks and separated by commas. The columns are in one line. \n\nThe rows in the csv are split by comma , if there are 2 commas this means that there is a missing value. I would like to separate these columns by these parameters. In cases where the row has a quotation mark this the comma in the quotation mark should not be a separator because this is an address.\n\nThis is a sample of the data (its a csv, I converted it into a dictionary to show you a sample)\n\n{\'Store code,"Biz","Add","Labels","TotalSe","DirectSe","DSe","TotalVe","SeVe","MaVe","Totalac","Webact","Dions","Ps"\': {0: \',,,,"Numsearching","Numsearchingbusiness","Numcatprod","Numview","Numviewed","Numviewed2","Numaction","Numwebsite","Numreques","Numcall"\',\n  1: \'Nora,"Ora","Sgo, Mp, 2000",,111,44,33,121,1232,53411,4,5,,3\',\n  2: \'mc11,"21 old","tjis that place, somewher, Netherlands, 2434",,3245,325,52454,3432,243,4353,343,23,23,18\'}}\n\n\nI have tried this so far and a bit stuck:\n\ndisc = pd.read_csv(\'/content/gdrive/My Drive/blank/blank.csv\',delimiter=\'",\')\n\n\nSample of csv:\ncsv sample\n'
"I have dates for each row in my dataframe and want to assign a value to a new column based on a condition of the date.\n\nNormally if I assign a value to a new column, I would do something like this:\n\ndef get_mean(df):\n   return df.assign(\n     grouped_mean = lambda df: df.groupby('group')['X']\n       .transform(lambda df: df.mean())\n   )\n\n\nNo I am looking for a solution like that, since the solution I have now is very slow and not beautiful.\n\nIs there a better way than my current solution and use assign?\n\nI currently came up with this solution:\n\ndef set_season(df):\n    df = df.copy()\n    for i in df.index:\n        if (df.loc[i, 'Date'] &gt;= pd.Timestamp('2008-08-30')) &amp; (df.loc[i, 'Date'] &lt;= pd.Timestamp('2009-05-31')):\n            df.at[i, 'season'] = '08-09'\n        elif  (df.loc[i, 'Date'] &gt;= pd.Timestamp('2009-08-22')) &amp; (df.loc[i, 'Date'] &lt;= pd.Timestamp('2010-05-16')):\n            df.at[i, 'season'] = '09-10'\n        elif  (df.loc[i, 'Date'] &gt;= pd.Timestamp('2010-08-28')) &amp; (df.loc[i, 'Date'] &lt;= pd.Timestamp('2011-05-22')):\n            df.at[i, 'season'] = '10-11'\n\n    return df\n\n"
'\n\ndef func1(dframe,Country,column_list,Role):\n\n    dframe1 = dframe[dframe.Country == Country]          \n    dframe1 = dframe1[column_list]                      \n    dframe1 = dframe1[dframe1.age != 2019]\n    dframe1 = dframe1[(dframe1["Role"]==Role)]           \n    dframe1 = int(round(dframe1.loc[:,"age"].mean()))\n    return dframe1\n\n\nI defined this function where the first line extracted rows where Country column\'s data matched with the Country passed, however how do I generalize this. Like if I want to extract data on the bases of the Gender column value.\nHow do I pass an argument where instead of dframe.Country I can use dframe.(passed argument)?\n\nWhat are the ways do apply one cleaning function to multiple columns in python?\n'
'I feel like this must have been asked elsewhere, but to the best of my ability I have not found a similar question here or elsewhere online.\n\nIn Python, when I was cleaning up a long text file, and had a long list of regex commands at the ready, I eventually saw that single-character words, like "I" or "a", were unfortunately being deleted.\n\nIs there a way, with regex (or something else), to perform the following operation?\n\nre.sub(r"\\non-word-single-character", "", "I want a b c cat")\n"I want a cat"\n\n\nThanks in advance.\n'
'I have a pandas dataframe with several hundred thousand rows and a column df[\'reviews\'] within which are text reviews of a product. I am cleaning the data, but pre-processing is taking a long time. Could you please offer suggestions on how to optimize my code? Thanks in advance.\n\n# import useful libraries\nimport pandas as pd\nfrom langdetect import detect\nimport nltk\nfrom html2text import unescape\nfrom nltk.corpus import stopwords\n\n# define corpus\nwords = set(nltk.corpus.words.words())\n\n# define stopwords\nstop = stopwords.words(\'english\')\nnewStopWords = [\'oz\',\'stopWord2\']\nstop.extend(newStopWords)\n\n# read csv into dataframe\ndf=pd.read_csv(\'./data.csv\')\n\n# unescape reviews (fix html encoding)\ndf[\'clean_reviews\'] = df[\'reviews\'].apply(unescape, unicode_snob=True)\n\n# remove non-ASCII characters\ndf[\'clean_reviews\'] = df["clean_reviews"].apply(lambda x: \'\'.join([" " if ord(i) &lt; 32 or ord(i) &gt; 126 else i for i in x]))\n\n# calculate number of stop words in raw reviews\ndf[\'stopwords\'] = df[\'reviews\'].apply(lambda x: len([x for x in x.split() if x in stop]))\n\n# lowercase reviews\ndf[\'clean_reviews\'] = df[\'clean_reviews\'].apply(lambda x: " ".join(x.lower() for x in x.split()))\n\n# add a space before and after every punctuation \ndf[\'clean_reviews\'] = df[\'clean_reviews\'].str.replace(r\'([^\\w\\s]+)\', \' \\\\1 \')\n\n# remove punctuation\ndf[\'clean_reviews\'] = df[\'clean_reviews\'].str.replace(\'[^\\w\\s]\',\'\')\n\n# remove stopwords\ndf[\'clean_reviews\'] = df[\'clean_reviews\'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))\n\n# remove digits\ndf[\'clean_reviews\'] = df[\'clean_reviews\'].str.replace(\'\\d+\', \'\')\n\n# remove non-corpus words\ndef remove_noncorpus(sentence):\n    print(sentence)\n    return " ".join(w for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha())\n\ndf[\'clean_reviews\'] = df[\'clean_reviews\'].map(remove_noncorpus)\n\n# count number of characters\ndf[\'character_count\'] = df[\'clean_reviews\'].apply(len)\n\n# count number of words\ndf[\'word_count\'] = df[\'clean_reviews\'].str.split().str.len()\n\n# average word length\ndef avg_word(sentence):\n  words = sentence.split()\n  print(sentence)\n  return (sum(len(word) for word in words)/len(words))\n\ndf[\'avg_word\'] = df[\'clean_reviews\'].apply(lambda x: avg_word(x))\ndf[[\'clean_reviews\',\'avg_word\']].head()\n\n# detect language of reviews\ndf[\'language\'] = df[\'clean_reviews\'].apply(detect)\n\n# filter out non-English reviews\nmsk = (df[\'language\'] == \'en\')\ndf_range = df[msk]\n\n# write dataframe to csv\ndf_range.to_csv(\'dataclean.csv\', index=False)\n\n\nThe code posted above does everything that I need it to; however, it takes hours to finish. I would appreciate any helpful suggestions on how to cut back processing time. Please let me know if you need any other details.\n'
'For all the columns in a pandas DataFrame, I want to transform the value into the following kind of code:\n\nfor col in list(df_sample.columns):\n    for val in col:\n        if val &gt; df_sample.col.median():\n            val=1\n        else:\n            val=0\n\n\nWhen the value is above the column median, then 1 else 0. Maybe new columns of median columns are needed first to compare them?\n'
'I have a LOT of survey results data, and one column asked for which state the user was from. For example, some people wrote "VA" and others wrote "Virginia"\n\nI was hoping to use a dictionary map, but things weren\'t working out so well. Does anyone have any suggestions for me? I am relatively new to Python, so I\'m still trying to get the hang of things.\n\nHere\'s what I\'ve tried:\n\nabv = {"Virginia": "VA", "Maryland": "MD",\n      "West Virginia": "WV", "Pennsylvania": "PA"}\nabv2 = dict(map(reversed, abv.items()))\nsurvey[\'New State\'] = survey.State.map(abv2)\nsurvey\n\n\n\nSome people typed "Virginia" and others wrote "VA". I only want the abbreviation version.\n'
'I have a folder with multiple csv files,I want to add one column with the name of the experiment condition to all the csv files in the folder.\n\nimport os, pandas as pd\nimport csv\nfile_list = list()\nos.chdir(\'.....name.../\')\ng=[]\nfor file in os.listdir():\n    if file.endswith(\'.csv\'):\n        df = pd.read_csv(file)\n        df[\'Condition\'] = "After food"\n        file_list.append(df)\n        g.append(file_list)\n\n'
"A bit strange question. Given a sample dataframe:\n\n    df = \n\n   1    1.1     2   2.1     3   3.1     4   4.1     5   5.1\n\n  11     22    33    44    55    66    77    88    99    12\n  12     13    14    15   Nan   Nan   Nan   NaN   NaN   NaN\n  11     22    33    44    55    66    77   NaN   NaN   NaN\n\n\nI want to drop columns containing NaN values. But sometimes my NaN's start from 3rd column, so I do not want to drop them. I want to do some approximation or mean. I want to check average length of the rows before NaN and drop the rest. \n\nExample:\n\n\n1st row has 10 points\n2nd row has 4 points\n3rd row has 6 points\n\n\nThus average will be 7. So my data will be:\n\n    df = \n\n   1    1.1     2   2.1     3   3.1     4   \n\n  11     22    33    44    55    66    77   \n  12     13    14    15   Nan   Nan   Nan   \n  11     22    33    44    55    66    77   \n\n\nOt maybe you can suggest other Data Preparing method\n"
'In the below code, I\'m using simple data manipulation to split the columns and remove unnecessary characters.\n\ninput_uni_towns = pd.read_fwf("university_towns.txt", sep = " ", header = None)\nuni_towns = input_uni_towns.rename(columns={0: "Raw_data"})\nuni_towns[\'Cleaned\'] = uni_towns["Raw_data"].replace(regex=True,to_replace=[r\'\\[[^()]*\\]\'],value=r\'\')\nuni_towns[["State","University"]] = uni_towns.Cleaned.str.split("(",n=1,expand=True) \nuni_towns["University"] = uni_towns["University"].str.rstrip(\')\')\ncleaned_uni_towns = uni_towns[["State","University"]]\n\n\nAfter this above step, I want to assign State to records whose above record has None assigned to it.\nFor Ex: Auburn (Auburn University)  current State is Auburn, but I want this to be updated to Alabama and similarly for records below Alabama till code encounters next State i.e. Alaska\n\nThis is the current output\n\n\nThis is the expected output\n\n\n'
"I have a data frame like this:\n\nabc = {'p1':[1,2,3,4,5,6,7,8,9,1],\n       'p2':[2,3,4,5,6,7,8,9,1,2],\n       'p3':[3,4,5,6,7,8,9,1,2,3]}\n\n\nI want to add another column to find if the number 1 exists or not for every row in those 3 columns like this:\n\n\n\nI have tried this one got nothing but error. here 1 = yes, 0 = no\n\nis_1st_exist = []\nfor p in abc['p1'],abc['p2'],abc['p3']:\n    if (p[0] | p[1] | p[2] == 1)\n        is_1st_exist.append(1)\n    else is_1st_exist.append(0)\n\n\nWhat should I do to get below is_1st_exist column?\n\nabc = {'p1':[1,2,3,4,5,6,7,8,9,1],\n       'p2':[2,3,4,5,6,7,8,9,1,2],\n       'p3':[3,4,5,6,7,8,9,1,2,3],\n  'is_1st_exist?':[1,0,0,0,0,0,0,1,1,1]} \n\n"
'I currently have a dataframe which contains several columns like this below:\n\nprint(df.WIN_COUNTRY_CODE[180:200])\n\n           WIN_COUNTRY_CODE\n180                        IT\n181                        IT\n182                        ES\n183    DE---UK---UK---UK---UK\n184         UK---UK---UK---UK\n185         DE---UK---UK---UK\n186    UK---UK---DE---UK---UK\n187                        SI\n188                        UK\n189                        FR\n\n\nEach cells of the column contain country codes, which can be more than one for each record.\nSince I would like to convert the country code from 2-letter into 3-letter iso code and also calculate the appearance frequency for this country, i apply this code:\n\n1. I split the string by the 3-dash that separates the countrycodes to convert from string to list:\n\ndf[\'WIN_COUNTRY_CODE_2\'] = df[\'WIN_COUNTRY_CODE\'].str.split("---")\n\n\nThis results in the column to be like this: \n\nprint(df.WIN_COUNTRY_CODE[180:200])\n\n           WIN_COUNTRY_CODE\n180                            [\'IT\']\n181                            [\'IT\']\n182                            [\'ES\']\n183    [\'DE\', \'UK\', \'UK\', \'UK\', \'UK\']\n184          [\'UK\', \'UK\', \'UK\', \'UK\']\n185          [\'DE\', \'UK\', \'UK\', \'UK\']\n186    [\'UK\', \'UK\', \'DE\', \'UK\', \'UK\']\n187                            [\'SI\']\n188                            [\'UK\']\n189                            [\'FR\']\n\n\n2. I apply the mapping method to convert from 2-letter to 3-letter country codes from conversion table that (cattable) and make it a dictionary type (catdict)\n\ncatdict= dict([(iso2,iso3) for iso2,iso3 in zip(cattable[\'iso_2_codes\'], cattable[\'iso_3_codes\'])])\ndf.assign(mapped=[[catdict[k] for k in row if catdict.get(k)] for row in df.WIN_COUNTRY_CODE_2])\n\n\nHowever whenever I apply the mapping it always return me this statement:\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-13-df7aad8ca868&gt; in &lt;module&gt;\n      1 cattable = pd.ExcelFile(\'D:/ROBERT LIBRARIES/Documents/ISD - LKPP Project/vardesc2.xlsx\').parse(\'WIN_COUNTRY_CODE\')\n      2 catdict= dict([(catnum,catdesc) for catnum,catdesc in zip(cattable[\'WIN_COUNTRY_CODE\'], cattable[\'Description\'])])\n----&gt; 3 df.assign(mapped=[[catdict[k] for k in row if catdict.get(k)] for row in df.WIN_COUNTRY_CODE])\n\n&lt;ipython-input-13-df7aad8ca868&gt; in &lt;listcomp&gt;(.0)\n      1 cattable = pd.ExcelFile(\'D:/ROBERT LIBRARIES/Documents/ISD - LKPP Project/vardesc2.xlsx\').parse(\'WIN_COUNTRY_CODE\')\n      2 catdict= dict([(catnum,catdesc) for catnum,catdesc in zip(cattable[\'WIN_COUNTRY_CODE\'], cattable[\'Description\'])])\n----&gt; 3 df.assign(mapped=[[catdict[k] for k in row if catdict.get(k)] for row in df.WIN_COUNTRY_CODE])\n\nTypeError: \'float\' object is not iterable\n\n\n\nIt seems likely that the code returns an error as the entries in the WIN_COUNTRY_CODE column are still in a string format, instead of a list of strings. This I learn after inspecting the objects within the list by this code:\n\ndf.WIN_COUNTRY_CODE_2[183][0]\n\n\nit always return one character instead of the 2-letter code as a string-object.\n\n\'[\'\n\n\nwhereas I expect the code to return a \'DE\' object.\n\n\n\nQuestion:\n\nHow to convert the WIN_COUNTRY_CODE column from a column of list into a column of list? And how can I find the most frequent country in the entire column? Thank you.\n'
'I\'ve a pandas DataFrame (list of video games)  with a column "classification". In that column, we can find:\n\n\nsimple classification: "RPG" or "Action"\nmultiple classifications: "Action Adventure RPG Roguelike", "Action Shoot\'em Up Wargame"\n\n\nYou have noticed? There is no separator...\n\nOf course, I need to split this in a new column, WITH separator (Or other structure with each separate element).\n\nSo\n\n"Action Adventure RPG Roguelike" =&gt; "Action, Adventure, RPG, Roguelike"\n"Action Shoot\'em Up Wargame" =&gt; "Action, Shoot\'em Up, Wargame"\n\n\nI can\'t use space to split, nor Caps ("Shoot\'em Up" is ONE value).\n\nSo, in my mind, I need to create a function to apply to this column, and check from a list of values (made by hand), find all of occurrence and return the string with separator...\n\nSomething like that:\n\nclassification = ["Action", "Adventure", "RPG", "Roguelike", "Shoot\'em Up", "Wargame"...]\n\ndef magic_tric(data):\n   # do the magic, comparing each classification possible / data\n   return data_separated\n\n\nBut I do not know how to do it. In the most efficient way...\n\nCan someone help me...?\nThanks in advance.\n'
"I want to set multiple regular expressions which when matched, must replace with a certain value. For example, I write a regular expression re.search('QuickPay with Zelle payment to *', re.IGNORECASE), and if matched in a DataFrame column, I want to replace it with 'Payment to *'. I want to have multiple such key value pairs of regular expressions.\n\nAs a practical example, if a column has 'QuickPay with Zelle payment to Zack', it should be replaced to 'Payment to Zack'. If a column has 'QuickPay with Zelle payment from Zack', it should be replaced to 'Payment from Zack'. If there is a match for *DD BR*, it should be replaced with 'Dunkin Donuts' and multiple such cases. I want this to be done in an automated way where I can just append to the key value pairs and then improve my cleaning function.\n\nI tried using df.apply() and df.replace() but did not know where to go from there. \n\nHere is some relevant code:\n\nimport pandas as pd\nimport re\n\nfilterMap = {\n    re.search('QuickPay with Zelle payment to ', re.IGNORECASE): 'Payment to',\n    re.search('QuickPay with Zelle payment from ', re.IGNORECASE): 'Payment from'\n}\n\ndf = pd.read_csv('./data/data.csv', header=None, skiprows=[0], usecols=[1, 2, 3])\n\ndate = df[1]\namount = df[3]\ntitle = df[2]\n\ncleanTitle = title.replace(to_replace=filterMap, value=filterMap)\n\nprint(cleanTitle)\n\n"
"Consider for example,\n\n                    Temp       Hum        WS\nDateTime                                         \n2019-08-01 00:00:00   35.9615  20.51460  1.287225\n2019-08-01 00:20:00   36.5795  21.92870  2.213225\n2019-08-01 00:40:00   36.2885  22.62970  2.331175\n2019-08-01 01:00:00   36.1095  22.76075  2.532800\n\n\nThe interval is clearly 20 minutes but is there a function to extract it?\nI am writing a script to resample to lower resolution using df.resample(rate).mean()\nI want to ensure that we run the script only when rate is larger than the rate of the df. It does not make sense to convert lower resolution data to a higher resolution. In this example, rate of '60T' will be acceptable because it will convert the 20 minute data to hourly data. But, a rate of '10T' should not be acceptable.\n"
"I have 2 CSV files. The first file has a list of all the states in the US but have missing values in the Longitude and Latitude column. I found another CSV file that contains all the longitude and latitude values for all states in the US.\n\nWhat I want to do now is to loop through the 'Location' column on the first file, match it with the 'Location' column on the 2nd file then get the corresponding values for its Longitude and Latitude. After which, I will need to append these values onto the Longitude and Latitude column in the first file\n\nCurrently, what I have is this:\n\naviationdata = pd.read_csv('AviationData.csv', sep = ',', header = 0, encoding = 'iso-8859-1') #this is the first file\nlocation = pd.read_csv('location.csv') #this is the 2nd file\n\nimport csv\n\nwith open('location.csv', 'r') as loc:\n    locationfile = loc.read()\n\nfor i in range(len(aviationdata['Location'])):\n    currentlocation = aviationdata['Location'].iloc[i]\n    axis = []\n    for i in currentlocation:\n        if i in aviationdata['Location']:\n   ... #i do not know how to continue from here\n\n\nI do not know how to come up with the codes to compare the location field to extract the longitude and latitude code from location.csv and append them to the longitude and latitude columns accordingly in aviationdata.\n\n \n\nThese are the fields for the first file (aviationdata)\n\n\n\nThese are the fields for the 2nd file (location)\n"
"I´m currently trying to webscrape the percentage saved between two different prices. The HTML code for the first element that I want to webscrape is:\n&lt;li class=&quot;price-save&quot;&gt;\n    &lt;span class=&quot;price-save-endtime price-save-endtime-current&quot;&gt;&lt;/span&gt;\n    &lt;span class=&quot;price-save-endtime price-save-endtime-another&quot; style=&quot;display:none;&quot;&gt;&lt;/span&gt;\n    &lt;span class=&quot;price-save-label&quot;&gt;Save: &lt;/span&gt;\n    &lt;span class=&quot;price-save-dollar&quot;&gt;&lt;/span&gt;\n    &lt;span class=&quot;price-save-percent&quot;&gt;22%&lt;/span&gt;   &lt;----------------------I WANT THIS ONE!\n&lt;/li&gt;\n\nTo do this, I coded the following in Python:\ntry:\n    percentage = soup.find('span',class_='price-save-percent').get_text()\nexcept:\n    print(&quot;Not found&quot;)\n\nHowever, when I print the results into a .csv file if the next element of the website does not contain the % then it copies the result until it finds the next element with a percentage. For a better understanding, please, see the url: https://www.newegg.com/Laptops-Notebooks/SubCategory/ID-32?Tid=6740\nYou can see that the first element has % Save, second one as well, but the third one no. In the .csv file, the third element gets the % saved of the second one. This happends repeteadly. Instead, I would like to have just a blank cell.\n"
"Table 1\n\n|Location|Type|Supplier|     ID    |Serial|\n|   MAB  |Ant |  A     |    A123   |456/56|\n|   MEB  |Ant |  B     |    A123   |456/56|\n\n\nTable 2\n\n|Location   |Type|Supplier|     ID      |Serial|#####|\n|  MAB+MEB  |Ant |  A/B   | A123        |456/56|123-4|\n|  MAB+MEB  |Ant |  A/B   | A123/B123   |456/56|432-1|\n|  MAB+MEB  |Ant |  A/B   | A123/B123   |456/56|432-1|\n\n\nTable 3\n\n|Location|Type|Supplier|     ID    |Serial|#####|\n|   MAB  |Ant |  A     | A123      |456/56|123-4|\n|   MAB  |Ant |  A     | A123      |456/56|432-1|\n|   MAB  |Ant |  A     | A123      |456/56|432-1|\n|   MEB  |Ant |  B     | A123      |456/56|123-4|\n|   MEB  |Ant |  B     | A123      |456/56|432-1|\n|   MEB  |Ant |  B     | A123      |456/56|432-1|\n\n\nAs illustrated above , if Table 1 column 'Location' , 'Supplier' , 'ID' , 'Serial' cell content is contained in the same column cells of Table 2 , to generate Table 3.\n\n*Note that Table 1 is used as the core template, if there the relevant column cells are contained in Table 2 , we are merely replicating the rows in Table 1 and adding the '####' column to each of the rows.\n\nPlease advice how do we produce Table 3. \n\nMy logic: for a,b,c,d in table 1 , if a,b,c,d contained in table 2 , append 'Subcon Part #' from table 2 to table 1 by column, Concate all 'Subcon Part #' by ',' explode concated 'Subcon Part #' to generate rows with unique 'Subcon Part #'\n\nWhere a,b,c,d are the columns of interests , the links between Table 1 and 2\n"
'I am attempting to remove all non-numeric characters from my dataframe (i.e. characters like ]$^M# etc.) with a single line of code. The data frame is a Google Play Store apps dataset.\n\n\ndf = pd.read_csv("googleplaystore.csv")\n\ndf[\'Rating\'].fillna(value = \'0.0\', inplace = True)\n\n#sample data#\n\n   Rating    Reviews                Size     Installs  Type Price  \\\n0        4.1     159                 19M      10,000+  Free     0   \n1        3.9     967                 14M     500,000+  Free     0   \n2        4.7   87510                8.7M   5,000,000+  Free     0   \n3        4.5  215644                 25M  50,000,000+  Free     0   \n4        4.3     967                2.8M     100,000+  Free     0   \n...      ...     ...                 ...          ...   ...   ...   \n10836    4.5      38                 53M       5,000+  Free     0   \n10837      5       4                3.6M         100+  Free     0   \n10838    0.0       3                9.5M       1,000+  Free     0   \n10839    4.5     114  Varies with device       1,000+  Free     0   \n10840    4.5  398307                 19M  10,000,000+  Free     0   \n\n\nContent Rating                     Genres      Last Updated  \\\n0           Everyone               Art &amp; Design   January 7, 2018   \n1           Everyone  Art &amp; Design;Pretend Play  January 15, 2018   \n2           Everyone               Art &amp; Design    August 1, 2018   \n3               Teen               Art &amp; Design      June 8, 2018   \n4           Everyone    Art &amp; Design;Creativity     June 20, 2018   \n...              ...                        ...               ...   \n10836       Everyone                  Education     July 25, 2017   \n10837       Everyone                  Education      July 6, 2018   \n10838       Everyone                    Medical  January 20, 2017   \n10839     Mature 17+          Books &amp; Reference  January 19, 2015   \n10840       Everyone                  Lifestyle     July 25, 2018\n\n\n\nclean_data = df.replace(\'[^\\d.]\', \'\', regex = True).astype(float)\n\nEssentially I am trying to remove the \'M\' from the Size column after the digits as well as the \'+\' sign in the Installs column.\n\nBut I\'m returned with this error message;\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-325-887d47a9889e&gt; in &lt;module&gt;\n----&gt; 1 data_ = df.replace(\'[^\\d.]\', \'\', regex = True).astype(float)\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py in astype(self, dtype, copy, errors)\n   5696         else:\n   5697             # else, only a single dtype is given\n-&gt; 5698             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors)\n   5699             return self._constructor(new_data).__finalize__(self)\n   5700 \n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in astype(self, dtype, copy, errors)\n    580 \n    581     def astype(self, dtype, copy: bool = False, errors: str = "raise"):\n--&gt; 582         return self.apply("astype", dtype=dtype, copy=copy, errors=errors)\n    583 \n    584     def convert(self, **kwargs):\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py in apply(self, f, filter, **kwargs)\n    440                 applied = b.apply(f, **kwargs)\n    441             else:\n--&gt; 442                 applied = getattr(b, f)(**kwargs)\n    443             result_blocks = _extend_blocks(applied, result_blocks)\n    444 \n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py in astype(self, dtype, copy, errors)\n    623             vals1d = values.ravel()\n    624             try:\n--&gt; 625                 values = astype_nansafe(vals1d, dtype, copy=True)\n    626             except (ValueError, TypeError):\n    627                 # e.g. astype_nansafe can fail on object-dtype of strings\n\n~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py in astype_nansafe(arr, dtype, copy, skipna)\n    895     if copy or is_object_dtype(arr) or is_object_dtype(dtype):\n    896         # Explicit copy, or required since NumPy can\'t view from / to object.\n--&gt; 897         return arr.astype(dtype, copy=True)\n    898 \n    899     return arr.view(dtype)\n\nValueError: could not convert string to float: \n\n\nKindly assist in debugging if possible. I would really like to keep it to one line of code for the entire data frame. Thank you in advance.\n'
'I want to fill the Null values in the first column based on the value of the 2nd column. \n(For example)\n\n\nFor "Apples" in col2, the value should be 12 in places of Nan in the col1  \nFor "Vegies", in col2 the value should be 134 in place of Nan in col1\n\n\nFor every description, there is a specific code(number) in the 1st column. I need to map it somehow.\n\n(IGNORE the . (dots)\n\nAll I can think of is to make a dictionary of codes and replace null but\'s that very hardcoded. \n\nCan anyone help? \n\ncol1. col2\n\n12.     Apple\n\n134.    Vegies\n\n23.     Oranges\n\nNan.    Apples\n\nNan.    Vegies\n\n324.    Sugar\n\nNan.    Apples\n\n'
'up till this function everything is fine, i get 4999 rows, that\'s the amount i got. Can you check the code down below, where do i make mistakes that i end up having 5095 instead of 4999 and in the second function i have 5032 instead of 4999 instances\n\nI have to get no more than 4999.\nAny help is appreciated\n\na=[]\nfor i in matches:\n    a.append([i for i in list(dict.fromkeys(i))])\nprint(len(a))\nprint ((a))\n\n\nresult:\n\n4999\n[[\'23-year-old\'], [\' \'], [\'42 years old\'], [\'-year-old\']..]\n\n\ncan the -year-old be a problem in here?\n\nNow here i face the problem\n\nt=[]\nfor i in a:\n    for j in i:\n        p=len(j)\n        if p&gt;1:\n            r=j.replace(\'-\', \' \').split(\' \')\n#             print(r)\n            t+=[s for s in r if s.isdigit()]\n        else:\n            t+=[\'\']\nprint(len(t))\nprint(t)\n\n\noutput:\n\n5095 #This should be 4999\n[\'23\', \'\', \'42\', \'\', \'\', \'30\', \'31\', \'\'...]\n\n\nI do have also the same issue with the list of the gender? i end up having 5032\n\nThis part is not answered yet\n\nimport re\n\nfil = data[\'transcription\']\nprint(fil)\n\ngender_aux = []\nfor i in fil:\n\n    try:\n        gender = re.findall("female|gentleman|woman|lady|man|male|girl|boy|she|he", i) or [" "]\n    except:\n        gender_aux.append(\' \')\n#         pass\n\n    gender_dict = {"male": ["gentleman", "man", "male", "boy",\'he\'],\n               "female": ["lady","female", "woman", "girl",\'she\']}\n\n    for g in gender:\n        if g in gender_dict[\'male\']:\n            gender_aux.append(\'male\')\n            break\n        elif g in gender_dict[\'female\']:\n            gender_aux.append(\'female\')\n            break\n        else:\n            gender_aux+=[\' \']\n            break\nprint(len(gender_aux))            \nprint(gender_aux)\n\n\noutput:\n\n5032 #this should be 4999\n[\'female\', \'male\', \'male\', \' \',\n\n'
'boxplot for the data\n\nplot = sns.boxplot(y = \'charges\', x = \'smoker\' , data = df)\n\n\nI was trying to remove outliers for the "no" category here. For which I filtered the no values\n\nno_charges = d[d.smoker == \'no\'].charges\n\n\nCalculated the IQR\n\nQ1 = no_charges.quantile(0.25)\nQ3 = no_charges.quantile(0.75)\nIQR = Q3-Q1\n\n\nFiltered the outliers\n\nda = df.copy()\nno = da[da.smoker == \'no\']\nrem = no[(no.charges &gt; (Q3 + 1.5*IQR)) | (no.charges &lt; (Q3 - 1.5*IQR))]\n\n\nBut I am facing issue when I try to drop the desired data from the main table\n\nda.drop[[(no.charges &gt; (Q3 + 1.5*IQR)) | (no.charges &lt; (Q3 - 1.5*IQR))], inplace=True]\n\n\nCan we do it this way? If not then how to filter the values?\n'
"I want to clean a pandas data frame using a dictionary of regular expressions representing allowed data entry formats. \n\nI'm trying to iterate over the input data frame so to check every row against the allowed data entry format for a given column.  \n\nIf an entry doesn't meet the format allowed for the column, I want to replace it with NaN (see desired output below). \n\nMy current code gives me an error message: 'DataFrame' object has no attribute 'col'.\n\nMy MWE features two representative regular expressions, but for my actual data set I've got ~40. \n\nThanks for any help!\n\n# Packages \nimport pandas as pd\nimport re\nimport numpy as np \n\n\n# Input data frame \ndata = {'score': [71,72,55,'a'],\n        'bet': [0.260,0.380,'0.8dd',0.260]\n        }\ndf1 = pd.DataFrame(data, columns = ['score', 'bet'])\n\n\n# Input dictionary \ndict1 = {'score':'^\\d+$', \n         'bet': '^\\d[\\.]\\d+$'}\n\n\n# Cleaning function  \ndef cleaner(df, dict):\n    for col in df.columns:\n        if col in dict: \n            for row in df.col:\n                if re.match(dict[col], str(row)):\n                    row = row \n                else:\n                    row = np.nan\n    return(df)\n\n\ncleaned_df = cleaner(df1, dict1)\n\n\n# ERROR MESSAGE \n# 'DataFrame' object has no attribute 'col'\n\n\n# Desired output \ngoal_data = {'score': [71,72,55, np.nan],\n        'bet': [0.260,0.380, np.nan, 0.260]\n        }\ngoal_df = pd.DataFrame(goal_data, columns = ['score', 'bet'])\n\n"
"I have the datasets in which a column has about 275 categories out of 9700 rows. Using get_dummies will get me many columns. Is doing this good practice.? or What other technique could be applied to deal with this.? (Here i have to determine the lowest price, company can fix to sell a product in market.)\ndf2['Market_Category'].nunique()\n\nOutput: 275\n"
"I am trying to fill the (pandas) dataframe's null/empty value using the mean of that specific column.\nThe data looks like this:\n    ID  Name        Industry            Year        Revenue\n    1   Treslam     Financial Services  2009        $5,387,469 \n    2   Rednimdox   Construction        2013    \n    3   Lamtone     IT Services         2009        $11,757,018 \n    4   Stripfind   Financial Services  2010        $12,329,371 \n    5   Openjocon   Construction        2013        $4,273,207 \n    6   Villadox    Construction        2012        $1,097,353 \n    7   Sumzoomit   Construction        2010        $7,703,652\n    8   Abcddd      Construction        2019    \n    .\n    .\n\nI am trying to fill that empty cell with the mean of Revenue column where Industry is == 'Construction'.\nTo get our numerical mean value I did:\ndf.groupby(['Industry'], as_index = False).mean()\n\nI am trying to do something like this to fill up that empty cell in-place:\n(df[df['Industry'] == &quot;Construction&quot;]['Revenue']).fillna(&quot;$21212121.01&quot;, inplace = True)\n\n..but it is not working. Can anyone tell me how to achieve it! Thanks a lot.\nExpected Output:\nID  Name        Industry            Year        Revenue\n1   Treslam     Financial Services  2009        $5,387,469 \n2   Rednimdox   Construction        2013        $21212121.01\n3   Lamtone     IT Services         2009        $11,757,018 \n4   Stripfind   Financial Services  2010        $12,329,371 \n5   Openjocon   Construction        2013        $4,273,207 \n6   Villadox    Construction        2012        $1,097,353 \n7   Sumzoomit   Construction        2010        $7,703,652\n8   Abcddd      Construction        2019        $21212121.01\n.\n.\n\n"
"I have some nan cells in the year column. I thought it'll be better to set the Mode of the year column grouped-By Industry type.\ndf\nID  Name     Industry    Year   Employees Expenses   Profit\n1   E-Zim    Health      2019   320       1,130,700  8553827\n2   Daltfase Software    NaN    78        804,035    13212508\n3   Hotlane  Government  2012   87        1,044,375  8701897\n4   Latho    Health      NaN    103       4,631,808  10727561\n5   Lambam   IT Services 2015   210       4,374,841  4193069\n6   Quozap   Health      2008   21        4,626,275  8179177\n7   Tampware Health      2008   13        2,127,984  3259485\n\nFor mode values, i did:\ndf_mode_year = df.groupby('Industry')['Year'].apply(lambda x: x.mode().iloc[0])\ndf_mode_year\n\nIndustry\nGovernment             2012\nHealth                 2008\nIT Services            2015\nSoftware\n\nAnd then to modify my df, i tried\n\ndf['Year'].fillna(df_mode_year)\ndf['Year'] = df['Year'].fillna(df_mode_year[df['Industry']=='Health'])\n\nBut none of these two are affecting the final df .\nExpected Output:\ndf\nID  Name     Industry    Year   Employees Expenses   Profit\n1   E-Zim    Health      2019   320       1,130,700  8553827\n2   Daltfase Software    NaN    78        804,035    13212508\n3   Hotlane  Government  2012   87        1,044,375  8701897\n4   Latho    Health      2008   103       4,631,808  10727561\n5   Lambam   IT Services 2012   210       4,374,841  4193069\n6   Quozap   Health      2008   21        4,626,275  8179177\n7   Tampware Health      2008   13        2,127,984  3259485\n\nWhat am i doing wrong?    Thanks a lot.\n"
"Noticed something very strange in pandas. My dataframe(with 3 rows and 3 columns) looks like this:\n\nWhen I try to extract ID and Name(separated by underscore) to their own columns using command below, it gives me an error:\ndf[['ID','Name']] = df.apply(lambda x: get_first_last(x['ID_Name']), axis=1, result_type='broadcast')\n\nError is:\nValueError: cannot broadcast result\n\nHere's the interesting part though..When I delete the &quot;From_To&quot; column from the original dataframe, performing the same df.apply() to split ID_Name works perfectly fine and I get the new columns like this:\n\nI have checked a lot of SO answers but none seem to help. What did I miss here?\nP.S. get_first_last is a very simple function like this:\ndef get_first_last(s):\n    str_lis = s.split(&quot;_&quot;)\n    return [str_lis[0], str_lis[1]]\n\n"
"I have the following:\npd.DataFrame({\n    'a' : { 1 : {}},\n    'b' : {1 : 3}\n})\n\nWhich looks as :\n    a  b\n1  {}  3\n\nAnd would like to be able to replace the {} with 0, or NaN, I'm not sure how to go about doing so though. I can't use .replace it seems\npd.DataFrame({\n    'a' : { 1 : {}},\n    'b' : {1 : 3}\n}).replace({ {} : 0})\n\nGives an error\n"
"I downloaded a dataset from this website https://aps.dac.gov.in/APY/Public_Report1.aspx One has to make selections and on the basis of my selection, I got a file that contained data for the year 2017-2018 for all the states and all the crops. This is how the dataset looks like after importing using read_excel()\nState/Crop/District             Season  Area (Hectare)  Production (Tonnes) Yield (Tonnes/Hectare)\n0   Andaman and Nicobar Islands NaN             NaN     NaN                 NaN\n1   Arecanut                    NaN             NaN     NaN                 NaN\n2   1.NICOBARS                  Rabi            534.10  125.23              0.234469\n3   2.NORTH AND MIDDLE ANDAMAN  Rabi            1744.00 4639.44             2.66023\n4   3.SOUTH ANDAMANS            Rabi            1220.20 10518.7             8.62047\n6   Arhar/Tur                   NaN             NaN     NaN                 NaN\n7   1.NORTH AND MIDDLE ANDAMAN  Rabi            1.20    0.6                 0.5\n9   Black pepper                NaN             NaN     NaN                 NaN\n10  1.NICOBARS                  Rabi            12.40   0.42                0.033871\n11  2.NORTH AND MIDDLE ANDAMAN  Rabi            8.76    2.13                0.243151\n12  3.SOUTH ANDAMANS            Rabi            69.46   349.72              5.03484\n\nThe first column which is named State/Crop/District contains three different values which according to me should have been in three different columns but are not. What's also interesting is that not all the crops are grown in all the districts. My goal is to have it in the following way.\nState                       Crop     District                 Season    Area    Production    Yield\nAndaman and Nicobar Islands Arecanut Nicobars                 Rabi      534     125.23        0.2344\nAndaman and Nicobar Islands Arecanut North and Middle Andaman Rabi      1744    4639.44       2.66023\n\nand so on. There are around 27 states like Andaman and Nicobar Islands and around 54 distinct crops and numerous districts. I tried three ways to solve this problem, but couldn't get success in any of them.\n\nI used pivot function in pandas but it made a dataframe containing around 12000 columns. That's not what I want.\nI used melt on the first column and values_vars for others. The result was similar to the first run.\nI created a custom function that scans for the row which contains the word Total. This is where two crops are separated in the raw file I downloaded, but I couldn't scale it to the other states.\n\nSome of the answers recommend using stack() and unstack() but I am not able to see how can that be used here because I don't see multiple indices. I am a newbie to Pandas and using Python 3. Help would be appreicated.\n"
'I got this DF\n   Instrument  Historic P/E\n0         RMD     44.983729\n1         RMD     42.941396\n2         RMD     32.880452\n3         RMD     32.408159\n4         RMD     25.393574\n5         RMD     22.792795\n6         RMD     21.168423\n7         RMD     21.513424\n8         RMD     18.280036\n9         RMD     21.433815\n10    GOOGL.O     27.243951\n11    GOOGL.O     23.910224\n12    GOOGL.O     32.960744\n13    GOOGL.O     28.426405\n14    GOOGL.O     34.064604\n15    GOOGL.O     26.769495\n16    GOOGL.O     31.409884\n17    GOOGL.O     20.366475\n18    GOOGL.O     21.726667\n19    GOOGL.O     22.597021\n20     AMZN.O     80.368614\n21     AMZN.O     75.734671\n22     AMZN.O    256.929249\n23     AMZN.O    153.073431\n24     AMZN.O    540.937030\n25     AMZN.O           NaN\n26     AMZN.O    676.775562\n27     AMZN.O           NaN\n28     AMZN.O    126.464829\n29     AMZN.O     71.249881\n30     BILI.O           NaN\n31     UBER.K           NaN\n32        MMM     22.587166\n33        MMM     20.761102\n34        MMM     25.660342\n35        MMM     21.877466\n36        MMM     19.860905\n37        MMM     21.949134\n38        MMM     20.879453\n39        MMM     14.694292\n40        MMM     13.720270\n41        MMM     15.326963\n42     MSFT.O     35.310141\n43     MSFT.O     26.362191\n44     MSFT.O     25.389494\n45     MSFT.O     21.180100\n46     MSFT.O     19.963249\n47     MSFT.O     29.887221\n48     MSFT.O     15.866554\n49     MSFT.O     13.383155\n\nI wish to have one DF where it shows the max P/E of each stock, if it is NA then just keep NA\n0         RMD     44.983729\n1    GOOGL.O     34.064604\n2     AMZN.O    676.775562\n3     BILI.O           NaN\n4     UBER.K           NaN\n\nThanks\n'
"I need to do feature extraction using the column 'Amenities' from the dataframe house_price.\nThe column Amenities has the following set of data\nhouse_data['Amenities']\n\n3                       3 beds 1 bath\n4              1 bed 1 bath 1 parking\n5                       3 beds 1 bath\n6            2 beds 2 baths 2 parking\n7             3 beds 1 bath 2 parking\n                    ...              \n2096    3 beds 2 baths 1 parking 419m\n2097          4 beds 1 bath 2 parking\n2098         3 beds 2 baths 2 parking\n2099         2 beds 2 baths 1 parking\n2100    3 beds 2 baths 1 parking 590m\nName: Amenities, Length: 1213, dtype: object\n\nI need to extract the number of beds, baths and parkings and store them into 3 seperate columns.\nhouse_data[&quot;bedrooms&quot;] = ''\nhouse_data[&quot;bedrooms&quot;] = house_data[&quot;Amenities&quot;].str.extract(&quot;(\\d*\\.?\\d+)&quot;, expand=True)\n\n\n\n3       3\n4       1\n5       3\n6       2\n7       3\n       ..\n2096    3\n2097    4\n2098    3\n2099    2\n2100    3\nName: bedrooms, Length: 1213, dtype: object\n\nThe above code extracts only the first digit of the entire string. How can I extract the digits representing the number of baths/parking and store them under different columns?\n"
'I was trying to convert the String in DateTime datatype. I was having four columns similar they got changed in the DateTime datatype. But for this particular column, it was showing me the below error\nDateParseError: Invalid date specified (71/23)\n\n\nIn this format, I was using  the code that I was using for the conversion:\nDOB_Permits[&quot;job_start_date&quot;] = pd.to_datetime(DOB_Permits[&quot;job_start_date&quot;])\n\n'
"import pandas as pd    \nsr = pd.Series([&quot;`1&quot;, &quot;2&quot;, &quot;`3&quot;, &quot;4&quot;, None, None, None])\n\nI have an object series (with len&gt;10000) that is very similar to the one above. I would like to keep the nones, but convert the numbers to integers. I'm not sure how to deal with the numbers that appear to be encoded with a backtick. What is the best way to go about this?\n"
"I'm currently carrying out work on the formatting of Ireland Phone Numbers. There's a number of different characters that need to be removed from the beginning. This is a small sample of the code. I'm wondering if there is another method to carry this out like a dictionary so that it is [353:3, 00353:5, 0353:4...]\nand it slices the beginning based on the length of the matched string? Thanks in advance.\nif s.startswith(&quot;353&quot;) == True:\n    s = s[3:]\nif s.startswith(&quot;00353&quot;) == True:\n    s = s[5:]\nif s.startswith(&quot;0353&quot;) == True:\n    s = s[4:] \nif s.startswith(&quot;00&quot;) == True:\n    s = s[2:]    \n\n"
'I have a data cleaning question. I ran two experiments in a row without turning off the equipment. I want all my data from Experiment 1 to go in one csv, and all my data from Experiment 2 to go into a different csv. The most obvious demarcation between experiments is a longer time period, but unfortunately, this was never a fixed time period. Another possibility is to split the data by peaks in the tension data, and then to recombine them ... somehow. Does anyone have any thoughts for an algorithm that might achieve this? Below is some mock-data. The time data is in a pandas DateTimeIndex.\n# Experiment 1, Trial 1\nDateTimeIndex  Tension\n7/25/2020 9:32 0\n7/25/2020 9:33 0\n7/25/2020 9:34 24\n7/25/2020 9:35 100\n7/25/2020 9:36 50\n7/25/2020 9:37 20\n7/25/2020 9:38 0\n#Noise\n7/25/2020 9:39 -25\n7/25/2020 9:40 4\n7/25/2020 9:41 11\n#Experiment 1: Trial 2\n7/25/2020 9:43 2\n7/25/2020 9:44 3\n7/25/2020 9:45 25\n7/25/2020 9:46 150\n7/25/2020 9:47 60\n7/25/2020 9:48 70\n7/25/2020 9:49 2\n# Lots and Lost of Noise Between Trials\n#Experiment 2: Trial 1\n7/25/2020 10:06 0\n7/25/2020 10:07 0\n7/25/2020 10:08 24\n7/25/2020 10:09 100\n7/25/2020 10:10 50\n7/25/2020 10:11 20\n7/25/2020 10:12 -3\n\n'
"I have some data in a column of a Pandas DataFrame where some of the numbers have a thousandth comma, as well as european style decimal comma. For example:\n40,910,27\n3,479.29\n34,561.09\n132,634,98\n\nI have tried using :\ndf['Orders'] = df['Orders'].replace(to_replace = ',', value = '.', regex = True)\n\nBut the issue is that this obviously replaces all commas with dots, so some numbers end up having two dots. Is there a way to remove the non-decimal commas only, and replace the actual decimal commas with a dot?\n"
"Here is the deal,\nI'm retrieving data from a non-consistent MongoDB collection, so I want to remove rows where the index is not a datetime type.\nE.g. removing the first 5 rows of this DataFrame:\n                                               _id  id  ... open connected\nupdated                                                 ...               \nTimestamp(0, 1610620202)  60001de14ed34a02743c4547  27  ...    1         1\n1610620382                60001de14ed34a02743c4548   5  ...    1         1\n1610620202                60001de14ed34a02743c4549  24  ...    1         1\n1610620382                60001de14ed34a02743c454a  17  ...    1         1\n1610620201                60001de14ed34a02743c454b   1  ...    1         1\n...                                            ...  ..  ...  ...       ...\n2021-01-14 20:12:01       6000a60cc299a51c09e20626  19  ...    1         1\n2021-01-14 20:12:01       6000a60cc299a51c09e20627  21  ...    1         1\n2021-01-14 20:12:01       6000a60cc299a51c09e20628  11  ...    1         1\n2021-01-14 20:12:01       6000a60cc299a51c09e20629  16  ...    1         1\n2021-01-14 20:12:01       6000a60cc299a51c09e2062a  14  ...    1         1\n\nAlso, how can I find the precise type used in a given cell ?\nThanks.\n"
"I'm creating a counter to count how many empty cells there are when a user uploads a CSV file. I am also using treeview to display the contents of the CSV. The print(&quot;There are&quot;, emptyCells.sum(), &quot;empty cells&quot;) works and prints the number to the console but I want to display this in a label so the user can view this in the GUI. It is not displaying anything but a &quot;row&quot; is being added to the application after a file has been uploaded where the label should be as everything moves down but no contents are being inserted into the label.\nemptyCells = (df[df.columns] == &quot; &quot;).sum()\n\n# print(&quot;There are&quot;, emptyCells.sum(), &quot;empty cells&quot;)\n\ntree.pack(side=BOTTOM, pady=50)\nmessagebox.showinfo(&quot;Success&quot;, &quot;File Uploaded Successfully&quot;)\n\nstringVariable = StringVar()\nprintVariable = (&quot;There are&quot;, emptyCells.sum(), &quot;empty cells&quot;)\n#print(printVariable)\n\nstringVariable.set(printVariable)\nlbl = Label(windowFrame, textvariable=stringVariable, font=25)\nlbl.pack()\n\n"
'I am working on a pandas dataframe and I have to replace column booking value if less than 25 with average of 7 days example x = 22-03-2020 has booking value 3, I have to replace it\nx = average(before 3 days + last year same date + after 3 days/7))\n\n\nAfter replace\n\nI have many values to replace, manually i am able to do it but i need some shortcut.\n'
'I read the how to check dictionary words \nAnd I got the idea to check my text file using dictionaries.  I have read the pyenchant instructions, and I thought that if I use get_tokenizer to give me back all the dictionary words in the text file.\n\nSo here is where I\'m stuck:  I want my program to give me all groups of dictionary words in the form of a paragraph. As soon as it encounters any junk characters, considers that a paragraph break, and ignores everything from there till it finds X number of consecutive words.\n\nI want it to read a text file in the sequence of filename_nnn.txt, parse it, and write to parsed_filname_nnn.txt.  I have not got around to doing any file manipulation. \n\nWhat I have so far:\n\nimport enchant\nfrom enchant.tokenize import get_tokenizer, HTMLChunker\ndictSentCheck = get_tokenizer("en_US")\nsentCheck = raw_input("Check Sentense: ")\n\ndef check_dictionary():\n    outcome = dictCheck.check(wordCheck) \n    test = [w[0] for w in dictSentCheck(sentCheck)]\n\n\n------ sample text -----\n\n\n  \n    English cricket cuts ties with Zimbabwe Wednesday, 25 June, 2008 text&amp;lt;void(0);&amp;gt;&amp;lt;void(0);&amp;gt; &amp;lt;void(0);&amp;gt;email &amp;lt;void(0);&amp;gt;print EMAIL THIS ARTICLE your name: your email address: recipient\'s name: recipient\'s email address: &amp;lt;;&amp;gt;add another recipient your comment: Send Mail&amp;lt;void(0);&amp;gt; close this form &amp;lt;http://ad.au.doubleclick.net/jump/sbs.com.au/worldnews;sz=300x250;tile=2;ord=123456789?&amp;gt; The England and Wales Cricket Board (ECB) announced it was suspending all ties with Zimbabwe and was cancelling Zimbabwe\'s tour of England next year.\n  \n\n\nThe script should return:\n\n\n  \n    English cricket cuts ties with Zimbabwe Wednesday\n    \n    The England and Wales Cricket Board (ECB) announced it was suspending all ties with Zimbabwe and was cancelling Zimbabwe\'s tour of England next year\n  \n\n\nI accepted abarnert\'s response.  Below is my final script.  Note it is VERY inefficient, and should be cleaned up some.  Also disclaimer I have not coded since college a LONG time ago.\n\nimport enchant\nfrom enchant.tokenize import get_tokenizer\nimport os\n\ndef clean_files():\n    os.chdir("TARGET_DIRECTORY")\n    for files in os.listdir("."):\n           #get the numbers out file names \n           file_number = files[files.rfind("_")+1:files.rfind(".")]\n\n           #Print status to screen\n           print "Working on file: ", files\n\n           #Read and process original file\n           original_file = open("name_"+file_number+".txt", "r+")\n           read_original_file = original_file.read();\n\n           #Start the parsing of the files\n           token_words = tokenize_words(read_original_file)\n           parse_result = (\'\\n\'.join(split_on_angle_brackets(token_words,file_number)))\n           original_file.close()\n\n           #Commit changes to parsed file\n           parsed_file = open("name_"+file_number+"_parse.txt", "wb")\n           parsed_file.write(parse_result);\n           parsed_file.close()\n\ndef tokenize_words(file_words):\n    tokenized_sentences = get_tokenizer("en_US")\n    word_tokens = tokenized_sentences(file_words)\n    token_result = [w[0] for w in word_tokens]\n    return token_result\n\ndef check_dictionary(dict_word):\n    check_word = enchant.Dict("en_US")\n    validated_word = check_word.check(dict_word)\n    return validated_word\n\ndef split_on_angle_brackets(token_words, file_number):\n    para = []\n    bracket_stack = 0\n    ignored_words_per_file = open("name_"+file_number+"_ignored_words.txt", "wb")\n    for word in token_words:\n        if bracket_stack:\n            if word == \'gt\':\n                bracket_stack -= 1\n            elif word == \'lt\':\n                bracket_stack += 1\n        else:\n            if word == \'lt\':\n                if len(para) &gt;= 7:\n                    yield \' \'.join(para)\n                para = []\n                bracket_stack = 1\n            elif word != \'amp\':\n                if check_dictionary(word) == True:\n                    para.append(word)\n                    #print "append ", word\n                else:\n                       print "Ignored word: ", word\n                       ignored_words_per_file.write(word + " \\n")\n    if para:\n        yield \' \'.join(para)\n\n    #Close opened files\n    ignored_words_per_file.close()\n\nclean_files()\n\n'
'import sys\nsys.version\n\n\n\n  \'2.7.8 |Anaconda 2.1.0 (64-bit)| (default, Jul  2 2014, 15:12:11) [MSC\n  v.1500 64 bit (AMD64)]\'\n\n\nfrom pandas import DataFrame,Series\nimport datetime\n\nid = [199995,199996,199997]\nhour = [14102101,14102102,14102103]\ndf = pd.DataFrame({\'id\':Series(id),\'hour\':Series(hour)})\nkk=df.hour.apply(str)\ndf_dt=datetime.datetime.strptime(kk,"%y%m%d%H").date()\n\n\n\n  TypeError: must be string, not Series\n\n\nI got this data from a time dimension table off a relational database so I cant really do much to change the importing format. How can I resolve this error?\n'
'I have a large-ish survey dataset to clean (300 columns, 30000 rows)  and the columns are mixed.  I\'m using Python with pandas and numpy.  Am very much in the learner wheels stage using Python.   \n\n\nSome of the columns had Y or N answers to questions (and these are filled "Y" or "N").  \nSome were likert scale questions with 5 possible answers.  In the CSV file each answer (agree, disagree etc.) has its own column.  This has imported as 1 for a yes and NaN otherwise. \nOther questions had up to 10 possible answers (e.g. for age) and these have imported as a string in one column - i.e. "a. 0-18" or "b. 19-25" and so on. Changing those will be interesting!\n\n\nAs I go through I\'m changing the Y/N answers to 1 or 0.  However, for the likert scale columns, I\'m concerned that there might be a risk with doing the same thing.  Does anyone have a view as to whether it would be preferable to leave the data for those as NaN for now? Gender is the same - there is a separate column for Males and one for Females, both populated with 1 for yes and NaN for no.  \n\nI\'m intending to use Python for the data analysis/charting (will import matplotlib &amp; seaborn). As this is new to me I\'m guessing that changes I make now may have unintended consequences later!\n\nAny guidance you can give would be much appreciated.\n\nThanks in advance.\n'
'So, as i need more detailed data I have to dig a bit deeper in the HTML code of a website. I wrote a script that returns me a list of specific links to detail pages, but I can\'t bring Python to search each link of this list for me, it always stops at the first one. What am I doing wrong?\n\n from BeautifulSoup import BeautifulSoup\n import urllib2\n from lxml import html\n import requests\n\n #Open site\n html_page = urllib2.urlopen("http://www.sitetoscrape.ch/somesite.aspx")\n\n#Inform BeautifulSoup\nsoup = BeautifulSoup(html_page)\n\n#Search for the specific links\nfor link in soup.findAll(\'a\', href=re.compile(\'/d/part/of/thelink/ineed.aspx\')):\n    #print found links\n    print link.get(\'href\')\n    #complete links\n    complete_links = \'http://www.sitetoscrape.ch\' + link.get(\'href\')\n    #print complete links\n    print complete_links\n#\n#EVERYTHING WORKS FINE TO THIS POINT\n#\n\npage = requests.get(complete_links)\ntree = html.fromstring(page.text)\n\n#Details\nname = tree.xpath(\'//dl[@class="services"]\')\n\nfor i in name:\n    print i.text_content()\n\n\nAlso: What tutorial can you recommend me to learn how to put my output in a file and clean it up, give variable names, etc?\n'
"I managed to replace the things i wanted with a ;  but now i struggle to remove the whitespace and newlines to get all the data until ; on a single line and then start the next.\n\nCode:\n\nreplacements = {'Geboren am':';', 'Nato/a il':';', 'Né(e) le':';'}\n\nwith open('DATEN2.txt') as infile, open('DATENBEARBEITET2.txt', 'w') as outfile:\nfor line in infile:\n    for src, target in replacements.iteritems():\n        line = line.replace(src, target)\noutfile.write(line)\n\n\nWhat the input file looks like: (after the replacement)\n\n       Kommissionen und Delegationen\n\n\n\n\n                        06.12.1999 - 30.11.2003\xa0\n\n\n\n\n                    Begnadigungskommission (BeK-V)     \n\n\n\n               ;\n\n\nWhat it should look like:\n\nKommissionen und Delegationen, 06.12.1999 - 30.11.2003, Begnadigungskommission (BeK-V);\n\n\nAfter a long time of searching I came to ask here if someone knows the correct repository or command to use for this kind of task, i'm really struggling to go to the next step.                \n\nEdit:/ Also, what was newlines before should turn into a comma, see sample output\n"
'I have strings such as "- memphis , tn! ", "~~~memphis,tn", ":) memphis , tn (:", ". - memphis,tn - .", "memphis tn?". I want to clean each of these strings such that each string becomes "memphis,tn". Currently, I use the code below, but is there a more efficient way of doing this? Perhaps using regex?\n\nNote that I currently have the issue that the ordering of the special characters affects the end result. For instance, ". - memphis,tn - ." gives the right result, whereas "- . memphis,tn . -" does not. This is not intended. If it could be fixed as a sideeffect, that would be great!\n\nThe strings are pure ASCII and I may be tempted to remove more special characters than the ones below.\n\nEdit:\nSorry, I should note that not all strings have the "x,y" format. Also strings such as "-- New York City --" or "* Texas *" should be cleaned up.\n\n# remove emoticons\nsmileys = [":)",":\\\\",":(",";)",\n           "(:","\\\\:","):","(;"]\nfor s in smileys:\n    loc = loc.replace(s, \'\')\n\n# cleaning whitespace uses\nloc = \' \'.join(loc.split())\nloc = loc.strip()\nloc = loc.replace(\' ,\', \',\')\nloc = loc.replace(\', \', \',\')\nloc = loc.replace(\' .\', \'.\')\nloc = loc.replace(\'. \', \'.\')\n\n# clean special symbols off the sides\nsymbols = \'.,!-#~*^?@" \'\nloc = loc.strip(symbols)\n\n'
'I have a very large number of very large files.\n\nEach file contains lines Like this:\n\nuuid1 (tab) data1 (vtab) data2 ...  dataN\nuuid2 (tab) data1\' (vtab) data2\' (vtab) data3\' (vtab) ...  dataN\'\n....\n\n\nwhere N will be different for every line. The result needs to look like:\n\nuuid1 (tab) data1\nuuid1 (tab) data2\n....\nuuid1 (tab) dataN\nuuid2 (tab) data1\'\nuuid2 (tab) data2\'\nuuid2 (tab) data3\'\n...  \nuuid2 (tab) dataN\'\n....\n\n\nI have a regex that does the job, replacing:\n\n^([abcdef0123456789]{8}-[abcdef0123456789]{4}-[abcdef0123456789]{4}-[abcdef0123456789]{4}-[abcdef0123456789]{12})\\t(.+?)\\x0B\n\n\nwith:\n\n\\1\\t\\2\\n\\1\\t\n\n\nbut it\'s slow, and needs repeated applications, obviously.\n\nIs there a quicker programmatic way to perform this across all the files?\n\nTools available in the toolbox: unix tools (sed, awk etc), python, possibly perl.\n\nNot looking for a religious war, just a pragmatic approach.\n\nAdditional Info\n\nHere\'s the complete script I used, based on Kristof\'s script, for handling the outer loop:\n\n#!/usr/bin/python\n\nimport os\nimport uuid\n\ndef processFile( in_filename ):\n\n  out_filename = os.path.splitext(in_filename)[0] + \'.result.txt\'\n\n  with open(in_filename) as f_in:\n    with open(out_filename, \'w\') as f_out:\n      for line in f_in:\n        try:\n          # Retrieve the line and split into UUID and data\n          line_uuid, data = line.split(\'\\t\')\n          # Validate UUID\n          uuid.UUID(line_uuid)\n        except ValueError:\n          # Ignore this line\n          continue\n        # Write each individual piece of data to a separate line\n        for data_part in data.rstrip().split(\'\\x0b\'):\n          f_out.write(line_uuid + \'\\t\' + data_part  + \'\\n\')\n\nfor i in os.listdir(os.getcwd()):\n  if i.endswith(".txt"): \n    print i\n    processFile( i )\n    continue\n  else:\n    continue\n\n'
"I am doing a data cleaning task using Python and reading from a text file which contains several sentences. After tokenizing the text file I keep getting a list with the tokens for each sentence as follows:\n\n[u'does', u'anyone', u'think', u'that', u'we', u'have', u'forgotten', u'the', u'days', u'of', u'favours', u'for', u'the', u'pn', u's', u'party', u's', u'friends', u'of', u'friends', u'and', u'paymasters', u'some', u'of', u'us', u'have', u'longer', u'memories']\n\n[u'but', u'is', u'the', u'value', u'at', u'which', u'vassallo', u'brothers', u'bought', u'this', u'property', u'actually', u'relevant', u'and', u'represents', u'the', u'actual', u'value', u'of', u'the', u'property']\n\n[u'these', u'monsters', u'are', u'wrecking', u'the', u'reef', u'the', u'cargo', u'vessels', u'have', u'been', u'there', u'for', u'weeks', u'and', u'the', u'passenger', u'ship', u'for', u'at', u'least', u'24', u'hours', u'now', u'https', u'uploads', u'disquscdn', u'com'].\n\n\nThe code I am doing is the following:\n\nwith open(file_path) as fp:\n    comments = fp.readlines()\n\n    for i in range (0, len(comments)):\n\n        tokens = tokenizer.tokenize(no_html.lower())\n        print tokens\n\n\nWhere no_html is the text file without any html tags. Is there anyone who could tell me how to get all these tokens into one list please ?\n"
'I have to clean special characters such as ðŸ‘‰ðŸ‘ŒðŸ’¦âœ¨ from tweets. In order to do that, I followed this strategy (I use Python 3): \n\n\nConvert tweets from bytes to strings to get the special characters as hex, so Ã becomes\\xc3\\;\nUsing regular expressions, delete the b\' and b" (at the beginning of the string) and the \' or " (at the end of the string) added by Python after the conversion process;\nFinally delete the hex representations, also using regex.\n\n\nHere is my code:\n\nimport re\ntweet = \'b"[/Very seldom~ will someone enter your life] to question\\xc3\\xa2\\xe2\\x82\\xac\\xc2\\xa6 "\'\n\n#encoding to \'utf8\'\ntweet_en = tweet.encode(\'utf8\')\n#converting to string\ntweet_str = str(tweet_en)\n#eliminating the b\' and b" at the begining of the string:\ntweet_nob = re.sub(r\'^(b\\\'b\\")\', \'\', tweet_str)\n#deleting the single or double quotation marks at the end of the string:\ntweet_noendquot = re.sub(r\'\\\'\\"$\', \'\', tweet_nob)\n#deleting hex\ntweet_regex = re.sub(r\'\\\\x[a-f0-9]{2,}\', \'\', tweet_noendquot)\nprint(\'this is tweet_regex: \', tweet_regex)\n\n\nThe final output is: [/Very seldom~ will someone enter your life] to question " (from which I still couldn\'t delete the final "). I was wondering if there is a better and more straightforward way to clean special characters in Twitter data. Any help will be appreciated. \n'
'I have a csv file containing a awards column with various different nominations and awards won. I want to extract data from the awards column in this dataset and split it into several columns. The awards has details of wins, nominations in general and also wins and nominations in certain categories(e.g. Oscar, BAFTA etc.) A sample input of awards column is shown below.\n\n\n\nAnd I want to split this data into several columns analyzing the data. Can we achieve this using python? I am using pandas for accessing dataframe. A sample expected output is shown below.\n\n\n'
"I have been attempting to replace strings in a data set for specific columns. Either with 1 or 0, 'Y' if 1, otherwise 0.\n\nI have managed to identify which columns to target, using a dataframe to rdd conversion with a lambda, but it is taking a while to process.\n\nA switch to an rdd is done for each column and then a distinct is performed, this is taking a while!\n\nIf a 'Y' exists in the distinct result set then the column is identified as requiring a transformation.\n\nI was wondering if anyone can suggest how I can use pyspark sql functions exclusively to obtain the same result instead of having to switch for each column?\n\nThe code, on sample data, is as follows:\n\n    import pyspark.sql.types as typ\n    import pyspark.sql.functions as func\n\n    col_names = [\n        ('ALIVE', typ.StringType()),\n        ('AGE', typ.IntegerType()),\n        ('CAGE', typ.IntegerType()),\n        ('CNT1', typ.IntegerType()),\n        ('CNT2', typ.IntegerType()),\n        ('CNT3', typ.IntegerType()),\n        ('HE', typ.IntegerType()),\n        ('WE', typ.IntegerType()),\n        ('WG', typ.IntegerType()),\n        ('DBP', typ.StringType()),\n        ('DBG', typ.StringType()),\n        ('HT1', typ.StringType()),\n        ('HT2', typ.StringType()),\n        ('PREV', typ.StringType())\n        ]\n\n    schema = typ.StructType([typ.StructField(c[0], c[1], False) for c in col_names])\n    df = spark.createDataFrame([('Y',22,56,4,3,65,180,198,18,'N','Y','N','N','N'),\n                                ('N',38,79,3,4,63,155,167,12,'N','N','N','Y','N'),\n                                ('Y',39,81,6,6,60,128,152,24,'N','N','N','N','Y')]\n                               ,schema=schema)\n\n    cols = [(col.name, col.dataType) for col in df.schema]\n\n    transform_cols = []\n\n    for s in cols:\n      if s[1] == typ.StringType():\n        distinct_result = df.select(s[0]).distinct().rdd.map(lambda row: row[0]).collect()\n        if 'Y' in distinct_result:\n          transform_cols.append(s[0])\n\n    print(transform_cols)\n\n\nThe output is :\n\n['ALIVE', 'DBG', 'HT2', 'PREV']\n\n"
'So, I\'m trying to extract a few dates displayed as dd.mm.yyyy.\n\nSome of the cells contain only one date, some of them contain multiple dates (like from dd.mm.yyyy to dd.mm.yyyy), along with more texts I don\'t care about.\n\nI would need to extract both dates in order to create two columns - "From" and "To", leaving blanks for the ones with the events which happened on only one date.\n\nI\'ve tried using the following expression in Python / Jython, but it only returns the first dates for the cells which contain more than one. \n\nimport re\ng = re.search("([0-9])([0-9])\\.([0-9])([0-9])\\.([0-9])([0-9])([0-9])([0-9])", value)\nreturn g.group()\n\n\nHow can I have both of the dates returned?\n\nThanks a lot!\n'
'I am working on a pandas dataframe  imported from a .csv file where there are about 18 columns. Each column has a item name and an image description in html format as column header. It is something like this:\n\nA  &lt;img width="300" alt="A" height="300".jpg"&gt;`  ` B &lt;img width="400" alt="B" height="600".jpg"`......\n\n\nand so on.\n\nWhat I am trying to achieve is to get only the item name for my column headers and trim the image part. I tried this replace function:\n\ndf.rename(columns=lambda x: x.replace(\'&lt;img width="300" alt="A" height="300".jpg"&gt;\', \'\'), inplace=True)\n\n\nBut it is not possible to give every column name as there are 30 similar files like this one and every image has a different description. I am looking for a more elegant solution here. I want my output to be something like:\n\nA   B   C   D   ......so on\n\n\nAny help would be greatly appreciated.\n'
'*Desicribe: in this txt file, the state names is ending by [edit]. And items between two [edit]s are names of university towns. I need to read the whole txt file into a pd.Dataframe with two columns named \'State\' and \'RegionName\'. However, the difficult thing is that the txt file have each item in a row, if I use df=pd.read_table(\'university_towns.txt\') directly, the Dataframe would be with only one column and put both state-names and region names in this columns. How can I deal with this?\n\nThanks in advance!*\n\nPart of my txt\n\n"Alabama[edit]\n\nAuburn (Auburn University)[1]\n\nFlorence (University of North Alabama)\n\nJacksonville (Jacksonville State University)[2]\n\nLivingston (University of West Alabama)[2]\n\nMontevallo (University of Montevallo)[2]\n\nTroy (Troy University)[2]\n\nTuscaloosa (University of Alabama, Stillman College, Shelton State)[3][4]\n\nTuskegee (Tuskegee University)[5]\n\nAlaska[edit]\n\nFairbanks (University of Alaska Fairbanks)[2]\n\nArizona[edit]\n\nFlagstaff (Northern Arizona University)[6]\n\nTempe (Arizona State University)\n\nTucson (University of Arizona)\n\nArkansas[edit]"\n'
"I am looking for a simple way of replacing several string and assigning it to a new df with the updated replacements\n\nThis the the sample column I am working with df['Column']\n\nColumn\n-----------------\nK700E\nR957Q\nDeletion\nL747_T751delinsP\nS752_I759del\nI491M\nD770_P772dup\nG719A\nG735S\nN771_H773dup\nK467T\nE746_T751insIP\nD770_N771insD\nG724S\nK745_A750del\nEGFRvIII\nV765A\nEGFRvII\nL858M\n\n\nSome entries contain text which I don't need, basically needs to be cleaned. Below is my code which I can't seem to get right.\n\nfor i in df['Column']:\ndf['Column'].replace('Truncating Mutations', '9999')\ndf['Column'].replace('Amplification', '9999')\nprint(i)\n\n\nThere are also some entries like \n\nEGFR-RAD51 Fusion\n\n\nI basically want to remove the word 'Fusion' but keep 'EGFR'.\n\nAny advise is very much appreciated from a novice. =)\n"
"I am trying to convert a messy notebook with dates into a sorted date series in pandas. \n\n0           03/25/93 Total time of visit (in minutes):\\n\n1                         6/18/85 Primary Care Doctor:\\n\n2      sshe plans to move as of 7/8/71 In-Home Servic...\n3                  7 on 9/27/75 Audit C Score Current:\\n\n4      2/6/96 sleep studyPain Treatment Pain Level (N...\n5                      .Per 7/06/79 Movement D/O note:\\n\n6      4, 5/18/78 Patient's thoughts about current su...\n7      10/24/89 CPT Code: 90801 - Psychiatric Diagnos...\n8                           3/7/86 SOS-10 Total Score:\\n\n9               (4/10/71)Score-1Audit C Score Current:\\n\n10     (5/11/85) Crt-1.96, BUN-26; AST/ALT-16/22; WBC...\n11                         4/09/75 SOS-10 Total Score:\\n\n12     8/01/98 Communication with referring physician...\n13     1/26/72 Communication with referring physician...\n14     5/24/1990 CPT Code: 90792: With medical servic...\n15     1/25/2011 CPT Code: 90792: With medical servic...\n\n\nI have multiple dates formats such as 04/20/2009; 04/20/09; 4/20/09; 4/3/09. And I would like to convert all these into mm/dd/yyyy to a new column. \n\nSo far I have done \n\ndf2['date']= df2['text'].str.extractall(r'(\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,})')\n\n\nAlso, I do not how to extract all lines with only mm/yy or yyyy format date without interfering with the code above. Bear in mind that with the absence of day or month I would consider 1st and January as default values. \n"
"Say I have two data frames x_2016 and y_2017 with the columns index, 0, 1, %, and date. I am interested in the columns index and %. The problem I'm running into is that I need them to be one dataframe where the index is W11 through W15 irrespective of the year for the values in the column %, but since the indices don't overlap completely in terms of weeks, I will have some rows that need to be filled in with 0.\n\nx_2016\n\n\n    index       0       1           %           date\n    2016 W12    16.0    0           2.5         2016-03-28\n    2016 W13    38.0    5.0         43.0        2016-04-04\n    2016 W14    48.0    13.0        63.0        2016-04-11\n    2016 W15    50.0    18.0        1.0         2016-04-18\n\ny_2017\n\n   index         0      1           %           date\n   2017 W11     16.0    8.0         40.0        2017-03-13\n   2017 W12     20.0    16.0        19.0        2017-03-27\n   2017 W13     34.0    27.0        6.0         2017-04-03\n   2017 W14     28.0    32.0        17.0        2017-04-10\n\n\nUltimately the dataframe should look like this:\n\nfinal\n\nindex     %_2016          %_2017\nW11       0               40.0\nW12       2.5             19.0\nW13       43.0            6.0\nW14       63.0            17.0       \nW15       1.0             0\n\n\nWhat's the most elegant way to do this in python?\n"
"I have several regexps like this,\n\nData['SUMMARY']=Data['SUMMARY'].str.replace(r'([^\\w])',' ')\nData['SUMMARY']=Data['SUMMARY'].str.replace(r'x{2,}',' ')\nData['SUMMARY']=Data['SUMMARY'].str.replace(r'_+',' ')\nData['SUMMARY']=Data['SUMMARY'].str.replace(r'\\d+',' ')\nData['SUMMARY']=Data['SUMMARY'].str.replace(r'\\s{2,}',' ')\n\n\ni want to replace all punctuations, XXXXXXXX,all digits, all non alphanumeric to the empty string ''. How can I combine it all into one replacing regexp?\n"
"I have a pandas dataframe with multiple duplicate IDs as such:\n\n id                col1          col2      col3\nENE80R             N             N         Y\nENE80R             N             N         N\nENE80R             Y             N         N\n\n\nWhat I'm trying to achieve is this:\n\n id                col1          col2      col3\nENE80R              Y             N         Y\n\n\nI tried using df.drop_duplicates but it doesn't update all the Ys into one column, it just keeps the first row and removes the duplicates. Any idea how I can go about this?\n\nEDIT: The data doesn't only contain Y and N in it, there are columns that contain data and other text and integer data. But the only data that differ in the duplicated data are the Ys and Ns.\n"
'I am working on a data set which is like in the attached image below.\n\nI have imported the data set which is in CSV in Python using pandas.\nI am looking to separate the entire data with all columns which has values like "a;b;c","lp;kl;jj" in the column PATR ( that is, values that has data with semi colon in it)  into a CSV and other values like ";" and "250" into a separate csv. \nI have tried splitting the values based on the semi-colon and separating it based on the length but i am not getting the exact match.\n\nActual data set:\n\n\n\nExpected Output 1 (All data which has PATR column with "ANY_DATA and a semicolon")\n\n\n\nExpected Output 2 (All data which has PATR column with "only semi colon or only data")\n\n\n\nThanks in advance.\n'
'I have extracted a HTML table using BeautifulSoup and would like to import it into a pandas DataFrame. However, data from the original table is spread out across multiple rows. Here are two entries for reference:\n\n\r\n\r\n&lt;table&gt;\r\n    \r\n    &lt;tbody&gt;&lt;tr&gt;\r\n      &lt;td&gt;Record : 1 of 749&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td width="111"&gt;Patients Name&lt;/td&gt;\r\n        &lt;td width="4"&gt;:&lt;/td&gt;\r\n        &lt;td colspan="4"&gt;Andrew Smith&lt;/td&gt;\r\n        &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Admit Date&lt;/td&gt;\r\n      &lt;td&gt;:&lt;/td&gt;\r\n      &lt;td width="189"&gt;20-MAR-2018&lt;/td&gt;\r\n      &lt;td&gt;Group Number &lt;/td&gt;\r\n      &lt;td&gt;:&lt;/td&gt;\r\n      &lt;td&gt;17&lt;/td&gt;\r\n    &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Address&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;123 Sunshine Ave &lt;/td&gt;\r\n        &lt;td&gt;Postal Code &lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;12345&lt;/td&gt;\r\n    &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Blood Type&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;A\t&lt;/td&gt;\r\n        &lt;td width="96"&gt;Ward Class&lt;/td&gt;\r\n        &lt;td width="4"&gt;:&lt;/td&gt;\r\n        &lt;td width="174"&gt;A&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Age&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;45&lt;/td&gt;\r\n        &lt;td&gt;Height&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;\r\n\t\t174cm\t\r\n\t\t&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Weight&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;102kg&lt;/td&gt;\r\n        &lt;td&gt;ID&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;\r\n\t\t013&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;&lt;hr/&gt;&lt;/td&gt;\r\n        &lt;/tr&gt;\r\n    \r\n    &lt;tr&gt;\r\n      &lt;td&gt;Record : 2 of 749&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td width="111"&gt;Patients Name&lt;/td&gt;\r\n        &lt;td width="4"&gt;:&lt;/td&gt;\r\n        &lt;td colspan="4"&gt;Margaret Chow&lt;/td&gt;\r\n        &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Admit Date&lt;/td&gt;\r\n      &lt;td&gt;:&lt;/td&gt;\r\n      &lt;td width="189"&gt;19-MAR-2018&lt;/td&gt;\r\n      &lt;td&gt;Group Number &lt;/td&gt;\r\n      &lt;td&gt;:&lt;/td&gt;\r\n      &lt;td&gt;14&lt;/td&gt;\r\n    &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Address&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;5 Mango Beach &lt;/td&gt;\r\n        &lt;td&gt;Postal Code &lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;54321&lt;/td&gt;\r\n    &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Blood Type&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;B\t&lt;/td&gt;\r\n        &lt;td width="96"&gt;Ward Class&lt;/td&gt;\r\n        &lt;td width="4"&gt;:&lt;/td&gt;\r\n        &lt;td width="174"&gt;B2&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Age&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;32&lt;/td&gt;\r\n        &lt;td&gt;Height&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;\r\n\t\t154cm\t\r\n\t\t&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;Weight&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;52kg&lt;/td&gt;\r\n        &lt;td&gt;ID&lt;/td&gt;\r\n        &lt;td&gt;:&lt;/td&gt;\r\n        &lt;td&gt;\r\n\t\t051&lt;/td&gt;\r\n      &lt;/tr&gt;\r\n    &lt;tr&gt;\r\n      &lt;td&gt;&lt;hr/&gt;&lt;/td&gt;\r\n        &lt;/tr&gt;\r\n    \r\n  &lt;/tbody&gt;&lt;/table&gt;\r\n\r\n\r\n\n\nI have used the following code to extract the above table into a pandas DataFrame:\n\nimport pandas as pd\ntable = str(table)\ndf = pd.read_html(table)\ndf = pd.DataFrame(df)\ndf\n\n\nMy df looks like this:\n\n\n\nbut I would like it to be a DataFrame with columns [\'Patients Name\', \'Admit Date\', \'Group Number\', \'Address\', \'Postal Code\', \'Blood Type\', \'Ward Class\', \'Age\', \'Height\', \'Weight\', \'ID\'].\n\nAm new to this. Greatly appreciate any advice!\n'
'Python noob here. (full disclosure)\n\nI\'ve got a list of Tweets that is formatted as a list of strings, like so:\n\n["This is a string that needs processing #ugh #yikes",\n"this string doesn\'t have hashtags",\n"this is another one #hooray"]\n\n\nI\'m trying to write a function that will create a list of the hashtags in each line but leave blank entries when there aren\'t any entries. This is because I want to join this list with the tweets themselves later. This is my desired output:\n\n[\'#ugh\', \'#yikes\'], [], [\'#hooray\']\n\n\nThis function which I found here works fine for ONE string.\n\n mystring = "I love #stackoverflow because #people are very #helpful!"\n\n\nBut it doesn\'t seem to work for several strings. This is my code:\n\n l = len(mystringlist)\n it = iter(mystringlist)\n\n taglist = []\n\n def extract_tags(it,l):\n      for item in mystringlist:\n         output = list([re.sub(r"(\\W+)$", "", j) for j in list([i for i in \n         item.split() if i.startswith("#")])])\n    taglist.append(output)\n\n multioutput = extract_tags(mystringlist,l)\n\n print(multioutput)\n\n'
'I have a df where each row has a code that indicates a department. \n\nOn the other hand, I have a dictionary in which each code corresponds to a region name (a region is constituted of multiple departments).\n\nI thought of a loop to put the value in a new column that indicates the region. \n\nHere is what I thought would work:\n\nfor r in df:\n    dep = r["dep"].astype(str)\n    r["region"] = dep_dict.get(dep)\n\n\nBut the only thing I get is "string indices must be integers".\nDo you people know how could I make it work ? Or if I should take a totally different route (like joining) ?\n\nThanks ! 🙏\n'
'I am using maketrans from string module in Python 3 to do simple text preprocessing like lowering, removing digits and punctuations. The problem is that during the punctuation removal all words are attached together with no empty space! For example, let\'s say I have the following text:\n\ntext=\'[{"Hello":"List:","Test"321:[{"Hello":"Airplane Towel for Kitchen"},{"Hello":2 "&amp;nbsp;Repair massive utilities&amp;nbsp;"2},{"Hello":"Some 3 appliance for our kitchen"2}\'\n\n\ntext=text.lower()\ntext=text.translate(str.maketrans(\' \',\' \',string.digits))     \n\nWorks just fine, it gives:\n\n\'[{"hello":"list:","test":[{"hello":"airplane towel for kitchen"},{"hello": "&amp;nbsp;repair massives utilities&amp;nbsp;"},{"hello":"some  appliance for our kitchen"}\'\n\n\nBut once I want to remove the punctuations:\n\ntext=text.translate(str.maketrans(\' \',\' \',string.punctuation))\n\n\nIt gives me this:\n\n\'hellolisttesthelloairplane towel for kitchenhello nbsprepair massives utilitiesnbsphellosome  appliance for our kitchen\'\n\n\nIdeally it should yield:\n\n\'hello list test hello airplane towel for kitchen hello nbsp repair massives utilities nbsp hello some  appliance for our kitchen\'\n\n\nThere is not specific reason I am doing it with maketrans, but I like as it is fast and easy and kind of stuck solving it. Thanks!\n\nDisclaimer: I already know how to do it with re like the following:\n\nimport re\ns = "string.]With. Punctuation?"\ns = re.sub(r\'[^\\w\\s]\',\'\',s)\n\n'
"I am trying to aggregate a dataframe based on values that are found in two columns. I am trying to aggregate the dataframe such that the rows that have some value X in either column A or column B are aggregated together.\n\nMore concretely, I am trying to do something like this. Let's say I have a dataframe gameStats:\n\nawayTeam  homeTeam  awayGoals  homeGoals\nChelsea   Barca     1          2\nR. Madrid Barca     2          5\nBarca     Valencia  2          2\nBarca     Sevilla   1          0\n\n\n... and so on\n\nI want to construct a dataframe such that among my rows I would have something like:\n\nteam    goalsFor  goalsAgainst\nBarca   10        5\n\n\nOne obvious solution, since the set of unique elements is small, is something like this:\n\nfor team in teamList:\n    aggregateDf = gameStats[(gameStats['homeTeam'] == team) | (gameStats['awayTeam'] == team)]\n# do other manipulations of the data then append it to a final dataframe\n\n\nHowever, going through a loop seems less elegant. And since I have had this problem before with many unique identifiers, I was wondering if there was a way to do this without using a loop as that seems very inefficient to me.\n"
'I am having a pandas data frame like below:-\n\n    Tweets\n0   RT @cizzorz: THE CHILLER TRAP *TEMPLE RUN* OBS...\n1   Disco Domination receives a change in order to...\n2   It\'s time for the Week 3 #FallSkirmish Trials!...\n3   Dance your way to victory in the new Disco Dom...\n4   Patch v6.02 is available now with a return fro...\n5   Downtime for patch v6.02 has begun. Find out a...\n6   💀⛏️... soon\n7   Launch into patch v6.02 Wednesday, October 10!...\n8   Righteous Fury.\\n\\nThe Wukong and Dark Vanguar...\n9   RT @wbgames: WB Games is happy to bring @Fortn...\n\n\nI also have a list suppose like below :-\n\nmy_list = [\'Launch\', \'Dance\', \'Issue\']\n\n\nwith below command it filters out the dataframe :-\n\n ndata = data[data[\'Tweets\'].str.contains( "|".join(my_list), regex=True)].reset_index(drop=True)\n\n\nfilter is not working if i am having \n\n    Working        Not Working\n    Launch        \'launch\' , \'launch,\' , \'Launch,\' ,\'LAUNCH\',\'@launch\'\n\n\nExpected output should be sentence havign any of the below word\n\n\'launch\' , \'launch,\' , \'Launch,\' ,\'LAUNCH\',\'@launch\'\n\n'
"I have a list of lists of lists, which looks like this: \n\n[['1', '1', '13', '23', '1.0', '9', '20051102', '20170330', '16', '9', '2', '2', '24', '46', '7232.17'], ['2', '1', '13', '23', '1.0', '9', '20051102', '20170331', '28', '4', '5', '4', '19', '51', '6171.145'], ['3', '1', '13', '23', '1.0', '9', '20051102', '20170327', '8', '3', '0', '2', '15', '14', '4666.224'], ['4', '1', '13', '23', '1.0', '9', '20051102', '20170329', '22', '2', '1', '4', '18', '42', '5479.682'], ['5', '1', '13', '23', '1.0', '9', '20051102', '20170328', '15', '5', '6', '9', '28', '37', '9411.681'], ['6', '1', '3', '27', '0.0', '9', '20051228', '20170303', '6', '1', '0', '0', '14', '21', '3757.115'], ['7', '1', '3', '27', '0.0', '9', '20051228', '20170301', '1', '0', '1', '3', '40', '45', '10521.261'], ['8', '1', '3', '27', '0.0', '9', '20051228', '20170320', '2', '0', '0', '0', '174', '171', '43113.562'].\n\nIt is created by this code:\nlines = [[x for x in line.strip().split(',')] for line in myfile.readlines()[1:3000]] \n\nNow, all of the elements in all of the lists are strings, and if I try \nlines = [[float(x) for x in line.strip().split(',')] for line in myfile.readlines()[1:3000]] I get an error. \n\nThe thing is that if i try something like:\n\nif str in lines: \n     print(lines)\n\nwhich returns nothing.\n\nI suspected that some empty strings might cause the problem, like: ['2976', '1', '1', '0', '', '4', '20160630', '20170318', '0', '0', '0', '0', '8', '2', '2125.364'] where the 4th element is empty..\n\nWhat to do? \n"
'can I any one help me with this code \nnow I get \'test2\' as same as \'test\', if I test is string it works good but as list it not working properly \n\n punc = set(string.punctuation)\n test=[" course content good though textbook badly written.not got energy \n though seems good union.it distance course i\'m sure facilities.n/an/ain \nlast year offer poor terms academic personal. seems un become overwhelmed trying become run"]\n\ntest2 = \'\'.join(w for w in test if w not in punc)\n print(test2)\n\n\nI want to remove all punctuation \n'
"I need to prepare my Data to feed it into an LSTM for predicting the next day. \nMy Dataset is a time series in seconds but I have just 3-5 hours a day of Data. (I just have this specific Dataset so can't change it)\nI have Date-Time and a certain Value.\nE.g.:\n\ndatetime..............Value      \n2015-03-15 12:00:00...1000\n\n2015-03-15 12:00:01....10\n\n.\n\n.\n\n\nI would like to write a code where I extract e.g. 4 hours and delete the first extracted hour just for specific months (because this data is faulty).\nI managed to write a code to extract e.g. 2 hours for x-Data (Input) and y-Data (Output).\nI hope I could explain my problem to you.\n\nThe Dataset is 1 Year in seconds Data, 6pm-11pm rest is missing.\nIn e.g. August-November the first hour is faulty data and needs to be deleted.\n\ninit = True\nfor day in np.unique(x_df.index.date):\n    temp = x_df.loc[(day + pd.DateOffset(hours=18)):(day + pd.DateOffset(hours=20))]\n\nif len(temp) == 7201:\nif init:\n    x_df1 = np.array([temp.values])\n    init = False\nelse:\n    #print (temp.values.shape)\n    x_df1 = np.append(x_df1, np.array([temp.values]), axis=0)\n#else:\n#if not temp.empty:\n    #print (temp.index[0].date(), len(temp))\n\nx_df1 = np.array(x_df1)\n\nprint('X-Shape:', x_df1.shape, \n'Y-Shape:', y_df1.shape)\n#sample, timesteps and features for LSTM\nX-Shape: (32, 7201, 6) Y-Shape: (32, 7201)\n\n\nMy expected result is to have a dataset of e.g. 4 hours a day where the first hour in e.g. August, September, and October is deleted.\nI would be also very happy if there is someone who can also provide me with a nicer code to do so.\n"
'I am doing a cleaning of my Database. In one of the tables, the time column has values like 0.013391204. I am unable to convert this to time [mm:ss] format. Is there a function to convert this to the required format [mm:ss]\nThe head for the column\n\n0           20:00\n1    0.013391204\n2    0.013333333\n3    0.012708333\n4    0.012280093\n\n\nUse the below reproducible data:\n\nimport pandas as pd\ndf = pd.DataFrame({"time": ["20:00", "0.013391204", "0.013333333", "0.012708333", "0.012280093"]})\n\n\nI expect the output to be like the first row of the column values shown above.\n'
"I have an extremely long dataframe with a lot of data which I have to clean so that I can proceed with data visualization. There are several things I have in mind that needs to be done and I can do each of them to a certain extent but I don't know how to, or if it's even possible, to do them together. \n\nThis is what I have to do:\n\n\nFind the highest arrival count every year and see if the mode of transport is by air, sea or land.\n\n\n    period  arv_count Mode of arrival\n0   2013-01  984350         Air\n1   2013-01  129074         Sea\n2   2013-01  178294         Land\n3   2013-02  916372         Air\n4   2013-02  125634         Sea\n5   2013-02  179359         Land\n6   2013-03  1026312    Air\n7   2013-03  143194         Sea\n8   2013-03  199385         Land\n...   ...      ...          ...\n78  2015-03  940077     Air\n79  2015-03  133632         Sea\n80  2015-03  127939     Land\n81  2015-04  939370     Air\n82  2015-04  118120     Sea\n83  2015-04  151134     Land\n84  2015-05  945080     Air\n85  2015-05  123136     Sea\n86  2015-05  154620     Land\n87  2015-06  930642     Air\n88  2015-06  115631     Sea\n89  2015-06  138474     Land\n\n\nThis is an example of what the data looks like. I don't know if it's necessary but I have created another column just for year like so:\n\ndef year_extract(year):\n    return year.split('-')[0].strip()\n\ndf1 = pd.DataFrame(df['period'])\n\ndf1 = df1.rename(columns={'period':'Year'})\n\ndf1 = df1['Year'].apply(year_extract)\ndf1 = pd.DataFrame(df1)\n\ndf = pd.merge(df, df1, left_index= True, right_index= True)\n\n\nI know how to use groupby and I know how to find maximum but I don't know if it is possible to find maximum in a group like finding the highest arrival count in 2013, 2014, 2015 etc\n\nThe data above is the total arrival count for all countries based on the mode of transport and period but the original data also had hundreds of additional rows of which region and country stated are stated but I dropped because I don't know how to use or clean them. It looks like this:\n\nperiod     region     country     moa     arv_count\n2013-01     Total      Total      Air      984350\n2013-01     Total      Total      Sea      129074\n2013-01     Total      Total      Land     178294\n2013-02     Total      Total      Air      916372\n...      ...    ...       ...       ...\n2015-12    AMERICAS     USA       Land      2698\n2015-12    AMERICAS    Canada     Land       924\n2013-01     ASIA        China     Air      136643\n2013-01     ASIA        India     Air       55369\n2013-01     ASIA        Japan     Air       51178\n\n\nI would also like to make use of the region data if it is possible. Hoping to create a clustered column chart with the 7 regions as the x axis and arrival count as y axis and each region showing the arrival count via land, sea and air but I feel like there are too much excess data that I don't know how to deal with right now.\n\nFor example, I don't know how to deal with the period and the country because all I need is the total arrival count of land, sea and air based on region and year regardless of country and months.\n"
"I have a df with DateTimeIndex (hourly readings) and light intensity.\n\nTime                   Light\n1/2/2017 18:00          31     \n1/2/2017 19:00          -5     \n1/2/2017 20:00          NA\n......\n......\n2/2/2017 05:00          NA\n2/2/2017 06:00          20\n\n\nThe issue is that after sunset (6 pm) until sunrise (6 am), the sensor doesn't work and has bad readings. I would like to set any readings in this period to 0.\n"
"I have a Pandas DataFrame with multiple columns. One of them is the year. From this column I want to create a new one with categorical values (I guess the therm is buckets), having the buckets auto-generated. It should result in something like that:\n\nyear_gr         year    other_cols\nA (1909 - 1917) 1911    abc\nB (1921 - 1930) 1923    def\nC (1932 - 1941) 1935    ghi\n\n\nI manage to create something close to it by:\n\nyear_gr = pd.cut(df.year, 10, labels=[\n   'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\ndf['year_gr'] = year_gr\n\ndf.head()\n\nyear_gr year    other_cols\nA       1911    abc\nB       1923    def\nC       1935    ghi\n\n\nBut how do I concatenate the rages generated automatically by pd.cut to my year_gr variable?\nI saw we can add the parameter retbins=True in the cut command to extract the bins, but I didn't manage to make use of it...\n\nThanks!\n"
"In my data frame, I have column 'countries', I am trying to change that column values into 'developed countries' and 'developing countries'. My data frame is as following:\n\n   countries age gender\n1  India     21  Male\n2  China     22  Female\n3  USA       23  Male\n4  UK        25  Male\n\n\nI have following two arrays:\n\ndeveloped = ['USA','UK']\ndeveloping = ['India', 'China']\n\n\nI want to convert array into following data frame:\n\n   countries    age gender\n1  developing   21  Male\n2  developing   22  Female\n3  developed    23  Male\n4  developed    25  Male\n\n\nI tried following code, but I got 'SettingWithCopyWarning' error:\n\ndf[df['countries'].isin(developed)]['countries'] = 'developed'\n\n\nI tried following code, but I got 'SettingWithCopyWarning' error and my jupyter notebook got hanged:\n\nfor i, x in enumerate(df['countries']):\n    if x in developed:\n        df['countries'][i] = 'developed'\n\n\nIs their alternative way to change column categories??\n"
'I want to get the parts of the meal ("breakfast", "lunch", "dinner") from a column of timings (9:30am-12noon,3pm-12midnight) \nfollowing is a sample of dataframe column:-\n\n0                                10am – 1am\n1                           12noon – 1:30am \n2                              9:30am – 1am\n3         12noon – 3:30pm, 7pm – 12midnight\n4        11am – 3:30pm, 6:30pm – 12midnight\n                       ...                 \n170                           11:40am – 4am\n171                            7pm – 1:30am\n172                            12noon – 1am\n173                            6pm – 3:30am\n174                              9am – 10pm\n\n\nI want to replace respective timings with respective part/parts of the meal\nfor example, if 11am - 3:30pm then replace it with ["breakfast","lunch"]  if 9am:10pm then replace it with ["breakfast","lunch","dinner"] and so on.\n'
"How do i split a single column in a DataFrame that has a string without creating more columns. And get rid of the brackets.\n\nFor example two rows looks like this:\n\ndf = pd.DataFrame({'Ala Carte':'||LA1: 53565 \\nCH2: 54565', \n                'Blistex':'|Cust: 65565\\nCarrier: 2565|', \n                'Dermatology':'||RTR1\\n65331\\n\\nRTR2\\n65331'})\n\n\n\nAnd I would like for the output dataframe to look like this, where the information column is a string:\n\nCustomer      Information\n\nAla Carte     LA1: 53565 \n              CH2: 54565\n\nBlistex       Cust: 65565\n              Carrier: 2565\n\nDermatology   RTR1: 65331\n              RTR2: 65331\n\n\n\nWithin the same column for Information\n"
"I have a dataset with several duplicate 'Email' fields, which I'd like to use as a uniqueID. However, each duplicate contains unique information about user 'Tags' that I'd like to compile and keep before deleting.\n\nEXAMPLE:\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame([[1234, 'Customer A', '123 Street', np.nan, np.nan],\n               [1234, 'Customer A', np.nan, '333 Street', np.nan],\n               [1234, 'Customer A', '12345 Street', np.nan, np.nan],\n               [1234, 'Customer A', np.nan, np.nan, np.nan],\n               [1233, 'Customer B', '444 Street', '3335 Street', np.nan],\n               [1233, 'Customer B', '555 Street', '666 Street', np.nan],\n               [1233, 'Customer B', '553 Street', '666 Street', 'abc@email.com'],\n               [1235, 'Customer C', '1553 Street', '644 Street', 'abc@email.com'],\n               [1235, 'Customer C', '2553 Street', '644 Street', 'abc@email.com']],     \n               columns=['ID', 'Customer', 'Billing Address', 'Shipping Address', 'Contact'])\ndf.head()\n\n\n    ID      Customer    Billing Address Shipping Address     Contact\n0   1234    Customer A  123 Street      NaN                  NaN\n1   1234    Customer A  NaN             333 Street           NaN\n2   1234    Customer A  12345 Street    NaN                  NaN\n3   1234    Customer A  NaN             NaN                  NaN\n4   1233    Customer B  444 Street      3335 Street          NaN\n\n\nI'd like to consolidate the Contact information for each row labeled 'Customer A' into the final row, separated by a , The end result would be NaN, NaN, NaN, NaN (or whatever other string data is in each field, just consolidated and separated by a column).\n\nHere's what I've tried, but there has to be a more elegant solution.\nAfter sorting by Email field:\n\ndef row_clean(df):\n    for i in range(0, len(df)-1):\n        if df.loc[i,'Customer'] == np.NaN:\n            return df\n        elif df.loc[i,'Customer'] == df.loc[(i+1),'Customer']:\n            df.loc[(i+1),'Contact'] = str(df.loc[(i+1),'Contact']) + ', ' + str(df.loc[i,'Contact'])            \n    return df\n\nrow_clean(df)\n\n\nAny thoughts here? Thank you!\n"
'I have a data set with a column contains STATE/UT names and also "Total" with state names.\n\nNow i want to filter data with text contains %Total% and delete those rows (below is the screen shot)\n\n\nCan anyone suggest how to clean this?\n'
"I'm decoding values from NLSY 79. They're occupational industries. Each industry has a number of occupations; for example: all occupations from 17 to 29 are in the Agriculture, Forestry, &amp; Fishery Industry. I've tried three strategies, but two return errors and the third doesn't store the value in the data frame.\n\nThe execution code looks like this (Survey respondents could list up to 5 jobs, all of which are included in the data)\n\ndf[['Job1', 'Job2', 'Job3', 'Job4', 'Job5']].replace(to_replace=jobs['code'], value=jobs['true'], inplace=True)\n\n\nStrategy 1\n\n\n  ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\n\njobs = {'code': ( tuple(range(17,29)), ... )\n        'true': ( 'Agriculture, Forestry &amp; Fisheries', ... )\n\n\nStrategy 2\n\n\n  TypeError: Cannot compare types 'ndarray(dtype=float64)' and 'range'\n\n\njobs = {'code': ( range(17,29), ... )\n        'true': ( 'Agriculture, Forestry &amp; Fisheries', ... )\n\n\nStrategy 3\n\n\n  SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame\n\n\njobs = {'code': ( any(tuple(range(17, 29))), any(tuple(range(47, 58))), ... )\n        'true': ( 'Agriculture, Forestry &amp; Fisheries', 'Mining', ... )\n\n\nI think a tweak to the third strategy/execution code would be best, but I'm still new to coding and am not sure what it would be. Any suggestions on how to solve this?\n\nInput:\n        Job1      ...  \n0       339       ...  \n1       757       ...  \n2       739       ...  \n3       448       ...  \n\nDesired Output:\n\n        Job1            ...  \n0       Utilities       ...  \n1       Professional    ...  \n2       Professional    ...  \n3       Retail          ...\n\njob = {'code': (list(range(17, 29)),\n                   list(range(47, 58)),\n                   list(range(67, 78)), ...)\n       'true': ('Agriculture, Forestry &amp; Fisheries',\n                  'Mining',\n                  'Construction', ...)}\n\n"
"\n\nSo i have dataset of movie name, date , revenue accumulated. there are multiple rows for same movie and there is a column which shows the accumulated revenue. I want to extract the last accumulated revenue for a certain movie and make a new column and insert the extracted value on the first row of a certain movie.\nFor example, i wanna know how to extract last revenue of movie 'a', last revenue of movie 'b'.. and insert these values on the first row of new column for each movie. SO in the picture that would be D2 for movie a, D33 movie b and so on...\n\ndf['Date'] = pd.to_datetime(df['Date']) \ndf = df.sort_values('Date') \ndf.groupby('Movie name')['Revenue accumulated'].last()\n"
'[{\'complete\': True, \'volume\': 116, \'time\': \'2020-01-17T19:15:00.000000000Z\', \'mid\': {\'o\': \'1.10916\', \'h\': \'1.10917\', \'l\': \'1.10906\', \'c\': \'1.10912\'}}, {\'complete\': True, \'volume\': 136, \'time\': \'2020-01-17T19:30:00.000000000Z\', \'mid\': {\'o\': \'1.10914\', \'h\': \'1.10922\', \'l\': \'1.10908\', \'c\': \'1.10919\'}}, {\'complete\': True, \'volume\': 223, \'time\': \'2020-01-17T19:45:00.000000000Z\', \'mid\': {\'o\': \'1.10920\', \'h\': \'1.10946\', \'l\': \'1.10920\', \'c\': \'1.10930\'}}, {\'complete\': True, \'volume\': 203, \'time\': \'2020-01-17T20:00:00.000000000Z\', \'mid\': {\'o\': \'1.10930\', \'h\': \'1.10931\', \'l\': \'1.10919\', \'c\': \'1.10928\'}}, {\'complete\': True, \'volume\': 87, \'time\': \'2020-01-17T20:15:00.000000000Z\', \'mid\': {\'o\': \'1.10926\', \'h\': \'1.10934\', \'l\': \'1.10922\', \'c\': \'1.10926\'}}, {\'complete\': True, \'volume\': 102, \'time\': \'2020-01-17T20:30:00.000000000Z\', \'mid\': {\'o\': \'1.10926\', \'h\': \'1.10928\', \'l\': \'1.10913\', \'c\': \'1.10920\'}}, {\'complete\': True, \'volume\': 277, \'time\': \'2020-01-17T20:45:00.000000000Z\', \'mid\': {\'o\': \'1.10918\', \'h\': \'1.10929\', \'l\': \'1.10913\', \'c\': \'1.10928\'}}, {\'complete\': True, \'volume\': 103, \'time\': \'2020-01-17T21:00:00.000000000Z\', \'mid\': {\'o\': \'1.10927\', \'h\': \'1.10929\', \'l\': \'1.10920\', \'c\': \'1.10924\'}}, {\'complete\': True, \'volume\': 54, \'time\': \'2020-01-17T21:15:00.000000000Z\', \'mid\': {\'o\': \'1.10926\', \'h\': \'1.10926\', \'l\': \'1.10910\', \'c\': \'1.10912\'}}, {\'complete\': False, \'volume\': 15, \'time\': \'2020-01-17T21:30:00.000000000Z\', \'mid\': {\'o\': \'1.10913\', \'h\': \'1.10918\', \'l\': \'1.10912\', \'c\': \'1.10913\'}}]\n\n\nI try to take out all the "time" and "mid" from this list. In the \'mid\', there are \'o\',\'h\',\'l\',\'c\'dictionaries. is there any way to combine \'time\' and these dictionaries into a dataframe?\n\n\n'
"I am having a dataset which look like follows(in dataframe):\n\n**_id** **paper_title**   **references**                                                                  **full_text**\n 1         XYZ              [{'abc':'something','def':'something'},{'def':'something'},...many others]       something\n 2         XYZ              [{'abc':'something','def':'something'},{'def':'something'},...many others]       something\n 3         XYZ              [{'abc':'something'},{'def':'something'},...many others]                         something\n\n\nExpected:\n\n**_id** **paper_title**   **abc**    **def**                               **full_text**\n   1         XYZ          something  something                               something               \n                          something  something\n                          .    \n                          .\n                         (all the dic in list with respect to_id column)\n   2         XYZ          something  something                               something               \n                          something  something\n                          .    \n                          .\n                         (all the dic in list with respect to_id column)\n\n\nI have tried df['column_name'].apply(pd.Series).apply(pd.Series) to split the list and dictionaries into columns of dataframe but doesn't help as it didn't split dictionaries.\n\nFirst row of my dataframe:\ndf.head(1)\n"
'Can somebody suggest me on how to create True or False values in a data-frame?\nFor example I have a data-frame like below:\n\ndf = pd.DataFrame({"a":[0, 1, 2, 3], "b":[1, 4, 7, 9],"c":["In, Out", "Out", "In, Out", "In, Out"]})\nprint(df)\n\na  b  c\n\n0  1  In, Out\n\n1  4  Out\n\n2  7  In, Out\n\n3  9  In, Out\n\n\nI would like to edit this one like below\n\na  b  In        Out\n\n0  1  True      True\n\n1  4  False     True\n\n2  7  False     False\n\n3  9  True      True \n\n'
'I have one data frame containing two entirely different data sets. The data sets are separated by two rows of all NAN values.\n\nI have provided a sample of the data frame below. \n\n+----+--------------------------------+-------------+-----+-----+-----+-----+-----+-----+------------+-----+--------+-----+\n| 13 | NaN                            | NaN         | NaN | NaN | NaN | NaN | NaN | NaN | Total Fees | NaN | 653    | NaN |\n+----+--------------------------------+-------------+-----+-----+-----+-----+-----+-----+------------+-----+--------+-----+\n| 14 | Expenses\\nDate Description ... | NaN         | NaN | NaN | NaN | NaN | NaN | NaN | NaN        | NaN | NaN    | NaN |\n+----+--------------------------------+-------------+-----+-----+-----+-----+-----+-----+------------+-----+--------+-----+\n| 15 | NaN                            | NaN         | NaN | NaN | NaN | NaN | NaN | NaN | NaN        | NaN | NaN    | NaN |\n+----+--------------------------------+-------------+-----+-----+-----+-----+-----+-----+------------+-----+--------+-----+\n| 16 | NaN                            | NaN         | NaN | NaN | NaN | NaN | NaN | NaN | NaN        | NaN | NaN    | NaN |\n+----+--------------------------------+-------------+-----+-----+-----+-----+-----+-----+------------+-----+--------+-----+\n| 17 | Date                           | Description | NaN | NaN | NaN | NaN | NaN | NaN | NaN        | NaN | Amount | NaN |\n+----+--------------------------------+-------------+-----+-----+-----+-----+-----+-----+------------+-----+--------+-----+\n\n\nRow 14 is the last row of the first data set, and row 17 is the first row of the second data set. \n\nI would like to end up with two data frames where the first ends at row 14 above and the second starts at row 17 above.  \n\nI have tried to split them like this: \n\nkey = df.isnull().all(1)\ndftopdata = df[:key] \ndfbottomdata = df[key:]\n\n\nWhen I run the code, I get an error saying, "cannot do slice indexing on class \'pandas.core.indexes.range.RangeIndex\' with these indexers"\n'
"I have a column of alpha-numeric string in pandas dataframe.\nThe goal is to only remove comma from number separators. For example:\n\nHello, world! -&gt; Hello, world!\n\n\nbut\n\nWarhammer 40,000 -&gt; Warhammer 40000\nCodename 1,337 -&gt; Codename 1337\n\n\nI can deduce that only when both sides are numerics '[0-9]+,[0-9]+' do I want to remove the comma. However, I can't seem to figure out a way to keep the same digits. Could someone help? If related, the size of the dataframe is a few hundred thousand rows and an average string is about 100 words.\n\np.s. this is my first post. I tried searching for related issue but found none. English is not my mother-tougue, chances are, I might have missed the keyword for this issue. Please do link me to the right place should this happens. Many thanks and much appreciated :)\n\nUpdate: added the keyword 'grouping' as reminder for myself.\n"
'I am relatively new to data science and machine learning and I am currently working on my first project with a very large data set, with over a million rows and 88 columns`.\n\nI am currently in the process of cleaning the data and trying to use features like data.isnull(), .sum() and data[data.isnull().values.any(axis=1)].head() but my Jupiter notebook file will only show the first ten and last ten columns. \n\nJust looking for the best way to view the data or to be directed to any resources that may help. \n'
"I have an array of numbers that I wish to turn into dummy variables (i.e. arrays with 1 if condition is met, 0 if otherwise). However, the conditions can be numerous and I was wondering if there was a more elegant solution than what I'm using.\n\narr = np.random.randint(0, 50, size=(100, 100))\n\n# What I'm doing\n\ndummy = np.zeros(arr.shape)\ndummy[np.where(np.logical_or.reduce((arr== 10, arr== 15, arr==16, arr==17)))] = 1\n\n\n\nIn the example, every value that is 10, 15, 16, or 17 becomes a one else a zero. For some dummy variables I have 10+ conditions and the expression can get lengthy, so I'm looking for something cleaner. I tried something like this but got a ValueError.\n\ndummy= [1 if x in [10, 15, 16, 17] else 0 for x in arr]\n\n"
"I have implemented an emotion analysis classification using lstm method. I have already train my model and saved it. I have load the train model and I am doing the classification part where I am saving it in a dataframe. I need to remove brackets along with its content I will show you below.\n\nhere are my codes:\n\n hotelname = []\nsentimentanalysis = []\n\nfor item in selection1:\n    name = item['name']\n    hotelname.append(name)\n    print (name)\n\n\nthe output is as follows:\n\nMystik Lifestyle (Save 34%)\nChalets Chamarel (Adults Only)\nAndrea Lodge (Save 18%)\nHibiscus Beach Resort &amp; Spa (Save 18%)\nLagoon Attitude (Adults Only)\nOcean V Hotel (Adults Only)\n\n\nbut I want my output to be like this::\n\nMystik Lifestyle \nChalets Chamarel \nAndrea Lodge \nHibiscus Beach Resort &amp; Spa \nLagoon Attitude \nOcean V Hotel \n\n\ncan someone please tell me what do I need to add in my codes please guys.\n"
'I am currently scraping into a database where I get minimum orders of product like: "3 Boxes", "1 Kilogram", "9 Cases".\nI would like to eliminate all words accompanying numbers and get only the numbers.\n\nMy code to filter those exceptions is:\n\nimport pandas as pd\n\nmin_order = element.find_element_by_class_name(\'gallery-offer-minorder\').find_element_by_tag_name(\'span\').text.replace(\' Pieces\', \'\').replace(\' Piece\', \'\').replace(\' Units\', \'\').replace(\n        \' Unit\', \'\').replace(\' Sets\', \'\').replace(\' Set\', \'\').replace(\' Pairs\', \'\').replace(\' Pair\', \'\').replace(\'Boxes\', \'\').replace(\'Box\', \'\').replace(\'Bags\', \'\').replace(\'Bag\', \'\').replace(\'Carton\', \'\').replace(\'Acre\', \'\').replace(\'Kilograms\', \'\').replace(\'Kilogram\', \'\')\n\n\nMy code works for all the cases I tried until I get an exception I haven\'t noticed. I want to know if it is any way to do this procedure using less lines of code and to eliminate all letters.\n'
'Kaggle Dataset(working on)- Newyork Airbnb\n\nCreated with a raw data code for running better explanation of the issue \n\n`airbnb= pd.read_csv("https://raw.githubusercontent.com/rafagarciac/Airbnb_NYC-Data-Science_Project/master/input/new-york-city-airbnb-open-data/AB_NYC_2019.csv")\n\nairbnb[airbnb["host_name"].isnull()][["host_name","neighbourhood_group"]]\n\n\n`DataFrame\n\nI would like to fill the null values of "host_name" based on the "neighbourhood_group" column entities. \nlike \n\nif airbnb[\'host_name\'].isnull():\n   airbnb["neighbourhood_group"]=="Bronx"\n   airbnb["host_name"]= "Vie"\n\nelif:\n        airbnb["neighbourhood_group"]=="Manhattan"\n        airbnb["host_name"]= "Sonder (NYC)"\n    else:\n        airbnb["host_name"]= "Michael"\n\n\n(this is wrong,just to represent the output format i want)\n\nI\'ve tried using if statement but I couldn\'t apply in a correct way. Could you please me solve this.\n\nThanks\n'
'i want to get sum of each column according to group by rows. how can i do this with pandas?\n\n'
'I have a list which looks something like this. \n\n["[\'brill building pop",\'quiet storm\',\'ballad\',\'easy listening\',"motown\'"," \'disco",\'soul jazz\',\n\'smooth jazz\',\'soul\',\'jazz\',\'soft rock\',"uk garage\'"," \'chill-out",\'german pop\',\'salsa\',\'r&amp;b\',\n\'chanson\',\'rock\',"pop\'"," \'blues-rock",\'vocal jazz\',\'funk\',\'oldies\',\'pop rock\',"downtempo\'",\n" \'hip hop",\'classic rock\',\'united states\',\'germany\',"adult contemporary\'"," \'folk rock",\'vocal\',\n\'soundtrack\',\'blues\',\'female vocalist\',"electronic\'"," \'new wave",\'urban\',\'reggae\',\'singer-songwriter\',\n \'swing\',\'60s\',"female\'"," \'american",\'80s\',\'90s\',"ambient\']"]\n\n\nIt is supposed to look like this:\n\n[\'brill building pop\',\'quiet storm\',\'ballad\',\'easy listening\',\'motown\',\'disco\',\'soul jazz\',\n\'smooth jazz\',\'soul\',\'jazz\',\'soft rock\',\'uk garage\',\'chill-out\',\'german pop\',\'salsa\',\'r&amp;b\',\n\'chanson\',\'rock\',\'pop\',\'blues-rock\',\'vocal jazz\',\'funk\',\'oldies\',\'pop rock\',\'downtempo\',\n\'hip hop\',\'classic rock\',\'united states\',\'germany\',\'adult contemporary\',\'folk rock\',\'vocal\',\n\'soundtrack\',\'blues\',\'female vocalist\',\'electronic\',\'new wave\',\'urban\',\'reggae\',\'singer-songwriter\',\n\'swing\',\'60s\',\'female\',\'american\',\'80s\',\'90s\',\'ambient\']\n\n\nAs you can see there is stray apostrophes, incomplete square brackets, whitespaces etc. The elements are meant to be phrases, so while I do not want to strip the whitespaces in the middle of the words, I want to remove them if they come at the start or at the end. Is there an easy way to do this?\n'
'I have a dataframe looks like below:\n\nimport numpy as np\nimport pandas as pd\nd = {\'col1\': [np.nan, 19, 32, np.nan, 54, 67], \'col2\': [0, 1, 0, 1, 1, 1]}\ndf = pd.DataFrame(d)\n\n\nI want to fill the missing values in "col1" based on the values of "col2". To be specific: I want to fill the missing values in  "col1" with 0 if "col2" is 0, else leave the "col1" as it is. In this case, my output should look like:\n\nd_updated = {\'col1\': [0, 19, 32, np.nan, 54, 67], \'col2\': [0, 1, 0, 1, 1, 1]}\ndf_updated = pd.DataFrame(d_updated)\n\n\nTo have the above output, I try to get the index which "col2" have values equal to 0 and use fillna():\n\nix = list(df[df["col2"] == 0].index)\ndf["col2"].loc[ix].fillna(0, inplace = True)\n\n\nHowever, my approach doesn\'t work and I don\'t know why. Thanks ahead.\n'
'I\'m performing a lot of data cleaning and want to keep track of the rows that I have manipulated. Is there an elegant way to keep track of the changes I\'ve made \n(ideally within a column of the dataframe)?\n\nAn example of my initial dataframe would be: \n\nimport numpy as np\nimport pandas as pd\n\nind = pd.Index([pd.Timestamp(\'2019-03-17\'), \n                pd.Timestamp(\'2019-03-18\'), \n                pd.Timestamp(\'2019-03-20\'),\n                pd.Timestamp(\'2019-03-21\'),\n                pd.Timestamp(\'2019-03-22\'),\n                pd.Timestamp(\'2019-03-24\')])\n\ndata = {\'col\':[25,25,24,3,25,24]}\n\ndf = pd.DataFrame(data, ind)\n\n\n            col\n2019-03-17   25\n2019-03-18   25\n2019-03-20   24\n2019-03-21    3\n2019-03-22   25\n2019-03-24   24\n\n\nI\'m performing several cleaning operations (which I\'ll call \'a\' and \'b\'), and I want to mark the rows that I have done these to in a new column. \n\n# operation a: create full date range and forward fill the missing days\n\ndf = df.asfreq(freq=\'D\', fill_value=np.nan)\ndf[\'col\'].fillna(method=\'ffill\', inplace=True)\n\n# operation b: check for rate changes larger than a particular value and forward fill those rows\n\ndf.loc[df[\'col\'].diff()&lt;-3, \'col\'] = np.nan\ndf[\'col\'].fillna(method=\'ffill\', inplace=True)\n\n\nI\'d like to add a column where I keep track of which rows I\'ve performed these on, such that the output looks something like this: \n\n             col changed\n2019-03-17  25.0       0\n2019-03-18  25.0       0\n2019-03-19  25.0       a\n2019-03-20  24.0       0\n2019-03-21  24.0       b\n2019-03-22  25.0       0\n2019-03-23  25.0       a\n2019-03-24  24.0       0\n\n\nThe best method I\'ve thought of would be to create "shadow" dfs at each step, and compare the values before (the "shadow") and after (the new df), then modify the "changed" column if there are differences, but this feels very clunky. Is there a more concise way to do this?\n\nThanks! \n'
"I'm cleaning some data.I have data from multiple subjects for multiple subjects over multiple trails.\n\nSubNo Trails Score \n1       1      4\n1       2      4\n1       3      8\n7       1      9\n7       2      8\n7       3      8\n19\n:\n:\n\n\nFor the same subject, I have another dataset for indifferent  order for SubNo\n\nSubNo Trails Height \n19      1      100\n19      2      400\n19      3      810\n7       1      911\n7       2      811\n7       3      811\n20      1      222\n20      2      222\n20      3      789\n1\n1\n:\n:\n\n\n\nI want to join these two on SubNo, such that in the end I have one CSV per subject for both score and height.\n\nSubNo Trails Score Height \n1        1     4     198\n1        2     4     209\n1        3     8     289\n2        1     :      :\n2        2\n2        3\n\n\nHere, I have joined the same data based on subNo: So,all the values of 1 together,all values of subject 2 together and so on.In my two csv files the order of subject is not the same.\nSo,I don't want to join based on header,but based on specific subject number.In my case ,that is 1,2,17,...like that.\nHow should I go about it? (I tried pandas merge,it works based on header).That's won't do what I want.\n"
'So, I have a simple doubt but I am new to regex. I am working with a Pandas DataFrame. One of the columns contains the names. However, some names are written like "John Doe" but some are written like "John.Doe" and I need to write all of them like "John Doe". I need to run this on the whole dataframe. What is the regex query to fix this and in an efficient manner. Col Name = \'Customer_Name\'. Let me know if more details are needed. \n'
"I am cleaning a google Playstore reviews dataset. It contains details of over 10k apps with columns like app name, price, reviews, installs and a few more cols. There is a row in which the data is displaced and the price column contains the value 'Everyone' instead of a price value.\nI tried to replace it with\ndf_path['Price'] = df_path[df_path.Price != 'Everyone']\n\nAnd after I did this, I noticed that the row got dropped when I checked with\ndf_path.info()  \n\nAfter this, when I take value counts using\ndf_path.Price.value_counts()\n\nSomething seriously wrong happens. The Price column gets replaced with the values of column names.\nAnd this isn't the only method I've tried to remove that row. I also tried using the df.drop() command and the same thing happens. And the worst part is that it shows the data type as int64 after this happens. Refer to the image link below for reference.\nWhat am I doing wrong? Price col gets replaced with the app names and type is int64\nThe row which has the value 'Everyone' in the price column\n"
'Hi I have a scenario where I have to maintain a number after every 6 six row.\nFor example here is my dataframe\nclient_id   patient_id  Total Clinic   Clinic Number  \n    172         6021        1            Clinic 1\n    172         6021        1            Clinic 1\n    172         6021        1            Clinic 1\n    172         6021        1            Clinic 1\n    172         6021        1            Clinic 1\n    172         6021        1            Clinic 1\n    172         6137        1            Clinic 1\n    172         6137        1            Clinic 1\n    172         6137        1            Clinic 1\n    172         6137        1            Clinic 1\n    172         6137        1            Clinic 1\n    172         6137        1            Clinic 1\n    187         5658        5            Clinic 1\n    187         5658        5            Clinic 1\n    187         5658        5            Clinic 1\n    187         5658        5            Clinic 1\n    187         5658        5            Clinic 1\n    187         5658        5            Clinic 1\n    187         5658        5            Clinic 2\n    187         5658        5            Clinic 2\n    187         5658        5            Clinic 2\n    187         5658        5            Clinic 2\n    187         5658        5            Clinic 2\n    187         5658        5            Clinic 2\n\nI want to achieve below results so that after every six rows index count will be updated\nclient_id  patient_id  Total Clinic  Clinic Number  Index_Number            \n    172        6021        1            Clinic 1            1\n    172        6021        1            Clinic 1            1\n    172        6021        1            Clinic 1            1\n    172        6021        1            Clinic 1            1\n    172        6021        1            Clinic 1            1\n    172        6021        1            Clinic 1            1\n    172        6137        1            Clinic 1            2\n    172        6137        1            Clinic 1            2\n    172        6137        1            Clinic 1            2\n    172        6137        1            Clinic 1            2\n    172        6137        1            Clinic 1            2\n    172        6137        1            Clinic 1            2\n    187        5658        5            Clinic 1            3\n    187        5658        5            Clinic 1            3\n    187        5658        5            Clinic 1            3\n    187        5658        5            Clinic 1            3\n    187        5658        5            Clinic 1            3\n    187        5658        5            Clinic 1            3\n    187        5658        5            Clinic 2            4\n    187        5658        5            Clinic 2            4\n    187        5658        5            Clinic 2            4\n    187        5658        5            Clinic 2            4\n    187        5658        5            Clinic 2            4\n    187        5658        5            Clinic 2            4\n\nNeed Help Thanks.\n'
"My scraper for single page:\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nurl = 'https://www.cvbankas.lt/?padalinys%5B0%5D=76&amp;page=1'\nsoup = BeautifulSoup(requests.get(url).content, 'html.parser')\n\nall_data = []\nfor h3 in soup.select('h3.list_h3'):\n    job_title = h3.get_text(strip=True)\n    company = h3.find_next(class_=&quot;heading_secondary&quot;).get_text(strip=True)\n    salary = h3.find_next(class_=&quot;salary_amount&quot;).get_text(strip=True)\n    location = h3.find_next(class_=&quot;list_city&quot;).get_text(strip=True)\n    print('{:&lt;50} {:&lt;15} {:&lt;15} {}'.format(company, salary, location, job_title))\n\n    all_data.append({\n        'Job Title': job_title,\n        'Company': company,\n        'Salary': salary,\n        'Location': location\n    })\n\ndf = pd.DataFrame(all_data)\ndf.to_csv('data.csv')\n\n#tips = sns.load_dataset('data.csv')\n#print(tips)\n\nGives me a csv file but only with 50 rows.\nI'm want to scrape all pages, was thinking to find in HTML code 'class=':'prev_next' but both BACK and FORWARD are the same, only with different href. So I decided to make range loop and change page with it:\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n#url = 'https://www.cvbankas.lt/?padalinys%5B0%5D=76&amp;page=1'\n#soup = BeautifulSoup(requests.get(url).content, 'html.parser')\nall_data = []\nfor i in range(1, 9):\n    url = 'https://www.cvbankas.lt/?padalinys%5B0%5D=76&amp;page='+str(i)\n    print(url)\n    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n    for h3 in soup.select('h3.list_h3'):\n        try:\n            job_title = h3.get_text(strip=True)\n            company = h3.find_next(class_=&quot;heading_secondary&quot;).get_text(strip=True)\n            salary = h3.find_next(class_=&quot;salary_amount&quot;).get_text(strip=True)\n            location = h3.find_next(class_=&quot;list_city&quot;).get_text(strip=True)\n            print('{:&lt;50} {:&lt;15} {:&lt;15} {}'.format(company, salary, location, job_title))\n        except AttributeError:\n            \n            all_data.append({\n                    'Job Title': job_title,\n                    'Company': company,\n                    'Salary': salary,\n                    'Location': location\n                })\n        \ndf = pd.DataFrame(all_data)\ndf.to_csv('data.csv')\n\nAfter running code it saves only 5 rows, so that's 10x less than code that I used just to scrape one page.\nHow would you loop pages? Pages are from 1 to 8\nAnd also how would clean Salary objects? as it comes as string which contains one of Nuo 2700 or Iki 2500 or has two numbers like 1000-3000. Because I would like to use Salary column as integers so I could do some plotting with Seaborn.\n"
'import requests, re\nfrom bs4 import BeautifulSoup\n\nr = requests.get(\'https://www.nrtcfresh.com/products/whole/vegetables-whole\', headers={\'User-agent\': \'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0\'})\nc = r.content\n\nsoup=BeautifulSoup(c,&quot;html.parser&quot;)\n#print(soup.prettify())\n\nall = soup.find_all(&quot;div&quot;,{&quot;class&quot;:&quot;col-sm-3 nrtc-p-10&quot;})\n\nall[1].find(&quot;h4&quot;).text\n\n\nThe output is provided below\n\n\'\\r\\n                Tomatoes\\t\\t\\t\\t  (Turkey)\\n\'\n\nTo get &quot;Turkey&quot; as the output, I can all[1].find(\'h4\').find(&quot;span&quot;).text.replace(&quot; &quot;, &quot;&quot;).replace(&quot;(&quot;,&quot;&quot;).replace(&quot;)&quot;,&quot;&quot;) Is there a better way to write this code and more importantly, how do I get just &quot;Tomatoes&quot; as the output?\n\r\n\r\n&lt;h4&gt;\n      " Tomatoes "                \n      &lt;span&gt;(Turkey)&lt;/span&gt;\n&lt;/h4&gt;\r\n\r\n\r\n\n'
"I have a data frame similar to this one\n| date      | Murders  | State  |\n|-----------|--------- |------- |\n| 6/2/2017  | 100      | Ags    |\n| 5/23/2017 | 200      | Ags    |\n| 5/20/2017 | 300      |  BC    |\n| 6/22/2017 | 400      |  BC    |\n| 6/21/2017 | 500      |  Ags   |\n\n\n\nI would like to group the above data by month and state to get an output as:\n| date      | Murders(SUM)  | State  |\n|-----------|---------      |------- |\n| January   | 100           | Ags    |\n| February  | 200           | Ags    |\n| March     | 300           | Ags    |\n|    ....   | ....          | Ags    | \n| January   | 400           |  BC    |\n| February  | 500           |  BC    |\n  ....         ....            ..\n\n\nI tried with this:\ndg = DF.groupby(pd.Grouper(key='date', freq='1M')).sum() # groupby each 1 month\ndg.index = dg.index.strftime('%B')\n\n\nBut these lines are only add the murders by month but without taking in count the State\n"
"I've got a pandas data frame with two columns, 'morning' and 'evening', which represent a morning and an evening pressure measurement. The values in the columns are either 'high', 'medium', or 'low'.\n    morning evening\n0   high    high\n1   high    medium\n2   high    medium\n3   high    low\n4   medium  high\n\nI want to create two lists, 'pressure_change' and 'pressure_change_likelihood'. 'pressure_change' describes what type of pressure change took place between the morning and the evening, eg 'high-low' is a change from a high morning pressure to a low evening pressure. 'pressure_change_likelihood' describes how frequent a given type of pressure change is, eg if the pressure was high in the morning it went to medium in the evening half of the time (0.50).\npressure_change = [['high-high', 'high-medium', 'high-low'],\n                  ['medium-high', 'medium-medium', 'medium-low'],\n                  ['low-high', 'low-medium', 'low-low']]\n\npressure_change_likelihood = [[0.25, 0.50, 0.25],\n                              [0.33, 0.00, 0.67],\n                             [0.00, 1.00, 0.00]]\n\nI've been able to create 'pressure_change' fine, but the problem is 'pressure_change_likelihood'. I thought I'd try to use pd.df.groupby() as a starting point and then convert the output to a list of lists, but the pandas series generated doesn't include a 0.00 value for events that never took place, and the order of the values is different from what I need it to be.\nimport pandas as pd \n\n# Data \ndata = [['high', 'high'], ['high', 'medium'], ['high', 'medium'], ['high', 'low'], ['medium', 'high'], ['medium', 'low'], ['medium', 'low'],['low', 'medium']] \ndf = pd.DataFrame(data, columns = ['morning', 'evening']) \n\n# pressure_change\nunique_array= df.morning.unique()\npressure_change = []\nfor i in unique_array:\n    sub_list = []\n    for k in unique_array:\n        sub_list.append(i+'-'+k)\n    pressure_change.append(sub_list)\n\n# pressure_change_likelihood   \nper = df.groupby(['morning', 'evening'])['evening'].size()\npressure_change_likelihood  = per.groupby(level=0).apply(lambda x: round(x / float(x.sum()), 2))\n\nprint(pressure_change_likelihood)\n\nmorning  evening\nhigh     high       0.25\n         low        0.25\n         medium     0.50\nlow      medium     1.00\nmedium   high       0.33\n         low        0.67\nName: evening, dtype: float64\n\nThanks for any help!\n"
"I'm stuck cleaning this sales database where data is collected from multiple sources and bill numbers are a mess yet they are the only column to refer multiple orders to the same bill, yet the use of different systems over time resulted in duplicated bill numbers.\nto fix this I need to give a new number to the bill number cell where the dates are different, for instance, if I have a bill no 1 and on a date in 2019 and another bill with the same bill number but in 2018 I need to give it a different bill number.\nsample of df:\n       bill_no  item_ser                date                  item size   price\n0         1       111 2018-12-15 15:09:50          Rockla Salad    R   39.00\n1         1       111 2018-12-15 15:09:50          Rockla Salad    R   39.00\n2         1       112 2018-12-15 15:10:16                   Tea    R    8.00\n3         1       112 2018-12-15 15:10:16                   Tea    R    8.00\n4         1       309 2019-02-21 10:02:24            Eggs Toast    R   35.00\n5         1       309 2019-02-21 10:02:24            Eggs Toast    R   35.00\n6         1         1 2020-07-20 12:38:16      Nody's Sfilatino    R   99.75\n7         1         1 2020-07-20 12:38:16      Nody's Sfilatino    R   99.75\n8         1      2715 2020-05-06 01:13:41  Basilico Buffalo - R    R  110.00\n9         1      2715 2020-05-06 01:13:41  Basilico Buffalo - R    R  110.00\n10        1      2716 2020-05-06 01:13:41   Timmy's Merguez - R    R  130.00\n11        1      2716 2020-05-06 01:13:41   Timmy's Merguez - R    R  130.00\n12        1      2717 2020-05-06 01:13:41            Funghi - R    R  105.00\n13        1      2717 2020-05-06 01:13:41            Funghi - R    R  105.00\n14        1      2718 2020-05-06 01:13:41          Extra Cheese    R   20.00\n15        1      2718 2020-05-06 01:13:41          Extra Cheese    R   20.00\n16        1         8 2020-07-05 16:27:37        Margherita - R    R   65.00\n17        1         8 2020-07-05 16:27:37        Margherita - R    R   65.00\n18        1         9 2020-07-05 16:27:39      Extra Vegetables    R   10.00\n19        1         9 2020-07-05 16:27:39      Extra Vegetables    R   10.00\n20        1        10 2020-07-05 16:27:40       Extra Mushrooms    R   20.00\n21        1        10 2020-07-05 16:27:40       Extra Mushrooms    R   20.00\n22        2        11 2020-07-05 16:36:31          Marinara - R    R   55.00\n23        2        11 2020-07-05 16:36:31          Marinara - R    R   55.00\n24        2        12 2020-07-05 16:36:38   Timmy's Merguez - R    R  130.00\n25        2        12 2020-07-05 16:36:38   Timmy's Merguez - R    R  130.00\n26        2        77 2018-12-15 16:25:19                   Can    R   12.00\n27        2        77 2018-12-15 16:25:19                   Can    R   12.00\n28        2        78 2018-12-15 16:25:34        Margherita - L    L   63.00\n29        2        78 2018-12-15 16:25:34        Margherita - L    L   63.00\n30        2        79 2018-12-15 16:25:40        Margherita - R    R   45.00\n31        2        79 2018-12-15 16:25:40        Margherita - R    R   45.00\n32        2     11172 2019-11-26 12:26:46        Margherita - L    L   75.00\n33        2     11172 2019-11-26 12:26:46        Margherita - L    L   75.00\n34        2         2 2020-07-20 12:38:32      Nody's Sfilatino    R   99.75\n35        2         2 2020-07-20 12:38:32      Nody's Sfilatino    R   99.75\n36        2      2719 2020-05-06 01:25:21  Basilico Buffalo - L    L  135.00\n37        2      2719 2020-05-06 01:25:21  Basilico Buffalo - L    L  135.00\n38        2      2720 2020-05-06 01:25:21        Gamberetti - L    L  175.00\n39        2      2720 2020-05-06 01:25:21        Gamberetti - L    L  175.00\n40        2      2721 2020-05-06 01:25:21          Marinara - L    L   70.00\n41        2      2721 2020-05-06 01:25:21          Marinara - L    L   70.00\n42        2      2722 2020-05-06 01:25:21          Marinara - L    L   70.00\n43        2      2722 2020-05-06 01:25:21          Marinara - L    L   70.00\n44        2      2723 2020-05-06 01:25:21            Flat White    R   35.00\n45        2      2723 2020-05-06 01:25:21            Flat White    R   35.00\n46        2      2724 2020-05-06 01:25:21            Flat White    R   35.00\n47        2      2724 2020-05-06 01:25:21            Flat White    R   35.00\n48        2      2725 2020-05-06 01:25:21           Banana Milk    R   40.00\n49        2      2725 2020-05-06 01:25:21           Banana Milk    R   40.00\n\nI Tried for loop yet with 150K rows it takes a lot of time.\n"
"I would like to take a dataset that offers a count of how many people are present at a location every 15 minutes (e.g. 13 people at Location A at 21:45, 29 people at Location A at 21:30, etc.), and instead have the dataset show the maximum number of people that occupied the location in any hour (e.g. between 21:00 and 21:59, the maximum occupancy at Location A was 33 - it doesn't matter if the maximum count of 33 people happened at 21:00, 21:15, 21:30 or 21:45).\nMy starting point is a csv that looks like this (it has thousands of entries, but this is just as an example):\n+--------------------+-------+-------+-------+-------+-------+-------+\n|        TIME        | LOC A | LOC B | LOC C | LOC D | LOC E | LOC F |\n+--------------------+-------+-------+-------+-------+-------+-------+\n|                    |       |       |       |       |       |       |\n| 8/28/2020 22:00:22 | 5     | 0     | 0     | 0     | 10    | 0     |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 21:45:21 | 13    | 10    | 23    | 14    | 24    | 0     |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 21:30:22 | 29    | 13    | 31    | 26    | 35    | 7     |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 21:15:22 | 32    | 17    | 41    | 32    | 49    | 12    |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 21:00:22 | 33    | 24    | 50    | 43    | 64    | 15    |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 20:45:23 | 44    | 31    | 60    | 47    | 88    | 15    |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 20:30:22 | 48    | 36    | 70    | 48    | 120   | 25    |\n|                    |       |       |       |       |       |       |\n| 8/28/2020 20:15:23 | 48    | 42    | 82    | 57    | 124   | 26    |\n+--------------------+-------+-------+-------+-------+-------+-------+\n\n\nAnd to confirm, the value in the columns is the number of people present at that location at that precise time.\nAnd my goal is something that looks like this:\n+--------------------+-------+-------+-------+-------+-------+-------+\n|        TIME        | LOC A | LOC B | LOC C | LOC D | LOC E | LOC F |\n+--------------------+-------+-------+-------+-------+-------+-------+\n|                    |       |       |       |       |       |       |\n| 2020-08-28 22:00   | 5     | 0     | 0     | 0     | 10    | 0     |\n|                    |       |       |       |       |       |       |\n| 2020-08-28 21:00   | 33    | 24    | 50    | 43    | 64    | 15    |\n|                    |       |       |       |       |       |       |\n| 2020-08-28 20:00   | 48    | 42    | 82    | 57    | 124   | 26    |\n+--------------------+-------+-------+-------+-------+-------+-------+\n\nSo, for example, if the counts for a location for 14:00, 14:15, 14:30, and 14:45 were 5,7,12,6 respectively, then the new entry for that location would just show 14:00 and the value of 12 (because that was the max count of people in that period).\nI hope I've explained that okay. I'm still very new to python, and I'm fully confident that this can be done, I just don't quite know how.\nAny help would be massively appreciated - thanks so much in advance ʕ •ᴥ•ʔ\n"
"From this:\n\n+------+------+--------------------------+-----------------+\n| code | type |           name           | final_component |\n+------+------+--------------------------+-----------------+\n| C001 | ACT  | Exhaust Blower Drive     |                 |\n| C001 | AL   |                          |                 |\n| C001 | AL   |                          |                 |\n| C001 | SET  | Exhaust Blower Drive     |                 |\n| C001 | AL   |                          |                 |\n| C001 | AL   |                          |                 |\n| C001 | AL   |                          |                 |\n| C002 | ACT  | Spray Pump Motor 1 Pump  |                 |\n| C002 | SET  | Spray Pump Motor 1 Pump  |                 |\n| C003 | ACT  | Spray Pump Motor 2 Pump  |                 |\n| C003 | SET  | Spray Pump Motor 2 Pump  |                 |\n| C004 | ACT  | Spray Pump Motor 3 Pump  |                 |\n| C004 | SET  | Spray Pump Motor 3 Pump  |                 |\n+------+------+--------------------------+-----------------+\n\n\n\nExpected:\n+------+------+--------------------------+--------------------------+\n| code | type |           name           |     final_component      |\n+------+------+--------------------------+--------------------------+\n| C001 | ACT  | Exhaust Blower Drive     | Exhaust Blower Drive     |\n| C001 | AL   |                          | Exhaust Blower Drive     |\n| C001 | AL   |                          | Exhaust Blower Drive     |\n| C001 | SET  | Exhaust Blower Drive     | Exhaust Blower Drive     |\n| C001 | AL   |                          | Exhaust Blower Drive     |\n| C001 | AL   |                          | Exhaust Blower Drive     |\n| C001 | AL   |                          | Exhaust Blower Drive     |\n| C002 | ACT  | Spray Pump Motor 1 Pump  | Spray Pump Motor 1 Pump  |\n| C002 | SET  | Spray Pump Motor 1 Pump  | Spray Pump Motor 1 Pump  |\n| C003 | ACT  | Spray Pump Motor 2 Pump  | Spray Pump Motor 2 Pump  |\n| C003 | SET  | Spray Pump Motor 2 Pump  | Spray Pump Motor 2 Pump  |\n| C004 | ACT  | Spray Pump Motor 3 Pump  | Spray Pump Motor 3 Pump  |\n| C004 | SET  | Spray Pump Motor 3 Pump  | Spray Pump Motor 3 Pump  |\n+------+------+--------------------------+--------------------------+\n\ni have to copy the name value whose type is 'SET' to final_component for all the same code\nlike for C001, name for type 'SET' is Exhaust Blower Drive\ni have to copy that to final_component for all C001\nfor ind in dataframe.index:         \n    if dataframe['final_component'][ind]!=None:\n        temp = dataframe['final_component'][ind]\n        temp_code = dataframe['code'][ind]\n    i = ind\n    while dataframe['code'][i] == temp_code:\n        dataframe['final_component'][ind] = temp\n        i+=1\n\ni could come up with this\nbut it gets stuck in the while loop\n"
'Background:\nI obtained a list of points from Google maps extracted data as a csv. Cleaned it in Pandas, and exported it as a JSON file. (Used Records for export)\nIssue:\nCoordinates are strings. Which makes sense because initially, the coordinates where tied in with a url\nExample: https://www.google.com/maps/search/{coordinates}\n\nI used the replace function to clear out the text to only remain with the coordinates. Is there a way to make my values in my Location column numerical type and putting them in a list.\nExample Mock-up data of what my exported JSON file looks like:\n[\n{\n      &quot;Bin&quot;:&quot;Yes&quot;,\n      &quot;Location&quot;:&quot;##.##,-###.##&quot;\n   },\n\nI was trying to clean my data to look like the example below\nExample of a GeoJSON file I was trying to model\n[\n{\n    location: [41.8781, -87.6298],\n    city: &quot;Chicago&quot;\n  },\n\nGoal:\nI am trying to make a custom map for my use in mapbox\nExample Mock-up of how my DataFrame looks like\n    Bin         Location\n0   Yes         ##.##,-###.##\n1   Yes         ##.##,-###.##\n\nInput: df.types\nOutput:\nBin          object\nLocation     object\ndtype: object\n\nThank you for the help.\n'
"Here's the thing, I have the dataset below where date is the index:\ndate            value\n2020-01-01      100\n2020-02-01      140\n2020-03-01      156\n2020-04-01      161\n2020-05-01      170\n.\n.\n.\n\nAnd I want to transform it in this other dataset:\nvalue_t0    value_t1    value_t2    value_t3    value_t4 ...\n100         NaN         NaN         NaN         NaN      ...\n140         100         NaN         NaN         NaN      ...\n156         140         100         NaN         NaN      ...\n161         156         140         100         NaN      ...\n170         161         156         140         100      ...\n\nFirst I thought about using pandas.pivot_table to do something, but that would just provide a different layout grouped by some column, which is not exactly what I want. Later, I thought about using pandasql and apply 'case when', but that wouldn't work because I would have to type dozens of lines of code. So I'm stuck here.\n"
'I am new to Data Science, I was trying to do some data cleaning and I had a column of Years in my data frame. Supposedly a year should be an integer, but as far as there are some NA values it automatically is denoted as float. I wonder if it is better to convert it to nullable int like pd.Int32Dtype() or leave it float. Is there any difference in terms of performance?\n'
"I am trying to clean data from a file. I have done a partial clean and the data looks like this.\n\nThe Price column still needs to be cleaned and updated into other columns. This is what I want to do\nStr '80 per piece' =&gt;\n\n80 -&gt; 'Price' column\n'piece' -&gt; 'Unit' column\n\nStr '110 per pack' =&gt;\n110 -&gt; 'Price' column\n'pack' -&gt; 'Unit' column\n\n\nI created a mask to retrieve the rows I need and then used regex to extract non-digits. I find that it affects all the rows. When I try to use only the rows retrieved by the mask - I get an error.\nHow to ensure only the column in the conditionally retrieved rows is affected ?\nThis is my code - Incorrect output without using mask on both sides.\n\nBut if I try this using the mask - I get this error\n\n"
"I have a column with missing categorical data and I am trying to replace them by existing categorical variables from the same column.\nI do not want to use the mode because I have too many missing data, it will skew the data and I would rather not drop the rows with missing data.\nI think the ideal way would be to get the proportion of each variables for my column and then replace the missing proportionally by the existing categorical variables.\nExample dataframe:\n   ClientId    Apple_cat    Region    Price\n0  21          cat_1        Reg_A     5\n1  15          cat_2        Nan       6\n2  6           Nan          Reg_B     7\n3  91          cat_3        Reg_A     3\n4  45          Nan          Reg_C     7\n5  89          cat_2        Nan       6\n\nNote: Ideally, I'd like to avoid hardcoding each category and region name.\n"
'I have a pandas dataframe like the following:\nDate   Title \nJan 1  Washington Running\nJan 2  Jefferson City Cycling\nJan 3  Springfield Running\n...\n\nHow can I remove the word &quot;Running&quot; or &quot;Cycling&quot; from all of the titles? I would like to get:\nDate   Title \nJan 1  Washington\nJan 2  Jefferson City\nJan 3  Springfield\n...\n\n'
"I have a csv file with the following sample output:\n3/12/1970\n3/1/1942\n10/20/1945  10/20/1945\n10/27/1960\n10/5/1952\n\nI bring it into pandas with df = pd.read_csv(filename).\nI know there are rows with double dates as noted above. The dtype of this column is object in pandas.\nWhen trying to convert this column to datetime format in pandas, I get errors on all the rows with this double date issue and have to find and edit them in the csv, one by one.\nSo, I have tried the following to clean out all the rows in my 50K rows which have this double date issue:\ndf[col] = df[col].str.strip()\ndf[col] = df[col].str[:10]\n\nDoes not affect any of the double dates at all.\nI also tried to calculate the length of each value in the col and then simply remove date values if the resulting col length exceeds 10.  Still, the double date rows remain.\nI have also tried the following to even locate this particular row to inspect it further, but this code results in 0 rows.\nbad_dates = df[df[col].str.contains('10/20/1945')]\n\nSo, any creative ideas to clean these double dates?  (It happens with probably one hundred randomly distributed column values)\n"
"I am cleaning a dataset and I need to replace the string in column A with &quot;foo&quot; if column B contains one of the strings from a list of options.\nI have a list of values contained in column B that I want to use to indicate that I need to replace column A\nA   B\nfoo apple\nfoo banana\nbar cherry\nfoo orange\nbar melon\nbar papaya\n\nI have multiple values that could trigger a replacement.\nlist = ['papaya', 'avocado', 'cherry', 'mango']\n\nusing that list to inform the replacement I want to change the value in column A to foo if column B contains one of the values in the list. The results in this example would look like this.\nA   B\nfoo apple\nfoo banana\nfoo cherry\nfoo orange\nbar melon\n\nAny advice helps, thanks.\n"
'I want to create a new data frame consisting of sex, amount of children, price of insurance, and if an individual is a smoker or not. Below is an example of my data frame.\nSex    Children Insurance Smoker\nMale      3      392.48    Yes\nMale      6      782.68    Yes\nMale      6      438.21    No \nFemale    1      125.98    Yes\nFemale    1      58.32     No\nFemale    4      585.12    Yes\nFemale    4      356.12    No\n\nSo far I got this using the code\ndf = pd.DataFrame(insurance).groupby([&quot;sex&quot;, &quot;children&quot;, &quot;smoker&quot;]).size()\n\n#which outputs\nsex      children   smoker\nfemale   1          yes      1\n         1          no       1\n         4          yes      1\n         4          no       1\nmale     3          yes      2\n         6          yes      1\n         6          no       1\n\n\nHow would I add a column of the average of insurance for each gender depending on how many children they have and if they smoke or not? I tried adding mean(&quot;insurance&quot;) but got an error, of course. Thank you so much for the help!\n'
"i wrote a function to remove an image link from text data (strings stored in pandas)\nimage_link_1 = 'â\\x80¦IMAGEâ\\x80¦' \nimage_link_2 = 'IMAGE'\n\ndef remove_image(text):\n    remove_im = ''.join([i for i in text if i not in image_link_1 and image_link_2])\n    return remove_im\n\ndf['title_and_abstract'] = df['title_and_abstract'].apply(lambda x: remove_image(x))\n\nThe problem is , that the function removes the first letter of some string. Espcially it seems that the function removes capital letter only. Weird.\nHere´s an example\n\n'This is an example string. Here is the IMAGE.'\n\nafter the function is used:\n\n'his is an example string. Here is the .'\n\n\nI realy dont get why this function does that.\nThank you in advance!\n"
"I am trying to encode integer categorical feature birth by\nbirth_encoded = encode_integer_categorical_feature(birth, &quot;birth&quot;, train_ds)\n\nwhere\ndef encode_integer_categorical_feature(feature, name, dataset):\n    # Create a CategoryEncoding for our integer indices\n    encoder = CategoryEncoding(output_mode=&quot;binary&quot;)\n\n    # Prepare a Dataset that only yields our feature\n    feature_ds = dataset.map(lambda x, y: x[name])\n    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n\n    # Learn the space of possible indices\n    encoder.adapt(feature_ds)\n\n    # Apply one-hot encoding to our indices\n    encoded_feature = encoder(feature)\n    return encoded_feature\n\n\nAnd get the error:\nTypeError: '&gt;' not supported between instances of 'list' and 'int'\n\nI am guessing this is because the birth data is not perfectly cleaned?\nThe data type of birth is int64 but somehow there is a list in there?\nSo (if this is the issue, which I'm pretty sure it is) i wonder how to check for  elements in a column of a dataframe that is a list? Or, rather, how to filter out all lists in the birth column for inspection.\n"
"I am trying to clean up my loaded CSV file using multiple columns values, so I can filter duplicate records out of it and hopefully drop them out, But I get and error related to date:\nMy sample data is:\n\n\n\n\nACTIVITY_DATE\nOWNNER_ID\nOWNER_NAME\n\n\n\n\n1/1/2020\n23344\nJAMES NELSON\n\n\n2/1/2020\n33445\nNIGEL THOMAS\n\n\n1/1/2020\n23344\nJAMES NELSON\n\n\n2/1/2020\n33445\nNIGEL THOMAS\n\n\n\n\nMy code is:\ninspections = inspections[inspections.duplicated(subset=['ACTIVITY_DATE','OWNER_ID'], keep=False)]\n\nMy error is:\nKeyError: 'ACTIVITY_DATE'\n\n÷ntended output\n\n\n\n\nACTIVITY_DATE\nOWNNER_ID\nOWNER_NAME\n\n\n\n\n1/1/2020\n23344\nJAMES NELSON\n\n\n2/1/2020\n33445\nNIGEL THOMAS\n\n\n\n"
'suppose that I have a dataset with this structure\npet_name    doggo     floofer    puppo     pupper \nA           None      floofer    None       None\nB           doggo     None       None       None  \nC           None      None       puppo      None \nD           None      None       None       pupper\nE           doggo     floofer    None       None \nF           None      None       puppo      pupper\nG           None      None       None       None \n\nand I want to have a new column with the name dog_stage that contains the variables(doggo , floofer, puppo, pupper)\nthe final result will be like that\nname    dog_stage\nA       floofer\nB       doggo\nC       puppo\nD       pupper\nE       doggo, floofer\nF       puppo, pupper\nG       None \n\nand drop the columns\n'
"I'm new to python and i'm not sure where to start with wrangling my dataset,\ni have customer e-commerce sales data and need one of the columns to contain the county part of the address. The county is in most cases already in the Address4 column but some of the customers have filled in their county in Address1, Address2 or Address3 instead.\nI have an array of all 32 counties so i think i need to check each column Address1, Address2,Address3,Address4 to see if they contain one of the counties in the array and if found, write the found county in Address4.\nHope this makes sense.\n\nin this case, i would need to find longford from Address3 and write over Co Longford and find Donegal in Address4 and write over Donegal in Address 4 and the same for all rows,\n"
"I have a dataset containing 1-year data which is sampled for each minute of each day. However, for some days or hours, the number of samples is less than 59 min (the sensor was turned off). Thus, there is no corresponding time step to it. Additionally, there are some NaN values in the series as well. The data looks like this:\n        time             x\n2019-01-01 00:00:00    10.0    # Day 1\n2019-01-01 00:01:00    9.0     # Day 1\n... ...\n2019-01-01 00:59:00   14.0    # Day 1\n\n... ...\n2019-01-02 00:00:00    10.0    # Day 2\n2019-01-02 00:01:00    9.0     # Day 2\n2019-01-02 00:02:00    NaN     # Day 2\n... ...\n2019-01-02 00:50:00    14.0    # Day 2\n\nAs you can see, for Day 1 the data set contains a valid value for each minute during the first hour of the day. The second day has only 50 minutes for the first hour. Also, there are some Nan Values there.\nSo my objective is to clean up these data in a rational way and reshape it for further process.\n\nif during some hours the sensor was off and has no reading (the time indices are less than 59 min, like Day 2 above), extend the indices to 59 min and indicate the corresponding values as Nan.\n\nif more than 80% of values for each hour is Nan, drop that particular hour from the dataset. Otherwise, replace the Nan values with the previous one.\n\nreshape the data frame by date-hour on the vertical axis and minutes on the horizontal one. (I need the final data frame to look like this)\nDate-Hour             min_00 ... min_59\n2019-01-01 00:01        10        14\n...\n\n\nHere is what I have tried so far which doesn't satisfy the above steps completely:\ndf.set_axis(['time', 'x'], axis=1, inplace=True)  # Setting readable name for columns\ndf.set_index('time', inplace = True) # Setting time column as index\ndf.index = pd.to_datetime(df.index)  # converting the index to time stamps\n\n\n# step 1 is needed (but how?) to extend the indices to have all 60 min long \n# even if for some periods there are no data and time index available in data set.\n\n\n# first of step 2\n# if more than 80% of values for each hour is Nan, drop that particular hour \n#  from the dataset, i.e. if more than 12 min (60 min - 80%*60 =12 min) has \n# Nan values, drop that hour. (How?)\n\n# filling all NaN values with their next value (second part of step 2) \ndf.fillna(method='ffill', inplace=True) \n\n# Step 3 (incomplete)  \n# Reshaping the df to Date vs time (hours, minutes)\ndf.set_index([df.index.date, df.index.time], inplace=True) \ndf = df.unstack(level=[1])\n# However I want it to be like Date-Hour vs minute  (but how?)\n\n# perhaps it would be easier to apply step 3 before step 2. Because removing \n# the non-sense hours (with more than 48 min NaN) would be easier as each hour \n# appears in a column\n\n"
"I got a Python dataframe which has &quot;Text&quot; and &quot;ID&quot; columns.\nI would like to be sure that each row of &quot;Text&quot; column contains only characters and white space.\nIf it's not the case (digit, special character, etc.), I would like to print all text and their ID in order to identify where is the problem.\nAny idea to solve this problem ?\n"
'I have dataframe which has months and years i want to convert it into days\n Name     details\n    \n    prem     6 months probation included\n    \n    shaves    3 years 6 months  suspended\n    \n    geroge    48 hours work time\n    \n    julvie    4 years 20 days terms included \n    \n   tiz        80 days work\n   lamp       44 days work\n\nhere i want to change 3 years as 1095 days,  6 months as 186 days, leap year can also be included, and i want to remove all other words like probation included, suspended, i want to get all the results in a new column.\nexpected result:\n Name     details                            Time\n    \n    prem     6 months probation included     186 days\n    \n    shaves    3 years 6 months suspended              1181 days\n    \n    geroge    48 hours work time             48 hours\n    \n    julvie    4 years 20 days terms included         1480 days\n   tiz        80 days  work                      80 days\n  lamp       44 days   work                      44 days\n\n'
"Hello I would like to clean a text file that holds a transcript.\nI have copy and pasted a small section of the text file.\n['he looked in &lt;the wellingtons&gt; [//] the boots .\\n',\n'&lt;last week&gt; [//] one night a boy and a dog was [*] staring at a jar\n,\\n', 'at this thing inside , wondering what they can do with it the next\nmorning .\\n',' at the same time &lt;the fr(og)&gt; [//] the thing jumped out that jar .']\n\nRetain those words that have either ‘ &lt;’ as prefix or ‘&gt; ’ as suffix but these two\nsymbols should be removed\nfor example\nhe looked in &lt;the wellingtons&gt; [//] the boots .\nshould be changed to\nhe looked in the wellingtons [//] the boots .\ni also need to:\nRetain those words that have either ‘ (’ as prefix or ‘) ’ as suffix but these two\nsymbols should also be removed similarly to the &lt; and &gt; but this time i need to keep three symbols\nas an exception (.), (..), and (...)\nthe code i have so far but it's really not working\n# prefix = '&lt;'\n# suffix = '&gt;'\n# clean = [' '.join(y for y in x.split(' ') if not (y[0] == '&lt;' and y[-1] == '&gt;') or y in {'&lt;','&gt;'}) for x in text]\n# return clean \n# the text here refers to a list of strings \n\n# for the brackets '(' and ')'\n# clean = [' '.join(y for y in x.replace('(','').replace(')','') if not (y[0] == '(' or y[-1] == ')') or y in {'(.)', '(..)', '(...)'}) for x in text]\n\ni would really like help on the how to remove '(' and ')' but not for the '(.)', '(..)', '(...)'\nthank you\n"
"I'm currently working on webscraping process by using python and more especially BeautifulSoup package in order to extract for each article of each page text and topics from a web page1)\nFor each article, I would like to regroup each texts extracted in one single string and associate to it a string of the topic.s. The goals is to iterate this process for all article and obtain a CSV file with a Text and Topic column (each line represent an article)\nTexts = []\nTopics = []\n\n\nwith open('urls.txt', 'r') as inf:\n    with open('text_file.csv', 'w') as outf:\n        outf.write('Text, labels\\n')\n        for row in inf:\n            url = row.strip()\n            response = requests.get(url, headers={'User-agent': 'Mozilla/5.0'})\n            if response.ok:\n                soup = BeautifulSoup(response.text,'lxml')\n                txt = soup.findAll('div', {'class': 'para_content_text'})\n                for div in txt:\n                    p = div.findAll('p')\n                    Texts.append(p)\n                for result in Texts:\n                    for item in result:\n                        full_text = ' '.join([item.text for result in Texts for item in result])\n                       \n\n                        \n            top = soup.find('div', {'class': 'article_tags_topics'})\n            a = top.findAll('a')\n            Topics.append(a)\n            for res in Topics:\n                for it in res :\n                    full_topic = ' '.join([it.text for res in Topics])\n            \n            outf.write(full_text.replace(',','') + ',' + full_topic + '\\n')\n\nBut after running my code I obtained text cells repeated several times because each repetition is associated to a different topic. the topics are also repeated themselves (see attached screenshot to have a better idea)\nHow can avoid these multiple line repeats ?\n"
'I have a data frame in pyspark with data two year 2019 and 2020.\n\nIf any booking value for date of year 2020 is less than 25 then replace that with same date of 2019 moving average value .\nExample :\n\nrequired:\n\nmanually i am able to do that\ntargetDf = df.withColumn(&quot;Booking&quot;,when(df[&quot;date&quot;] == &quot;2020-01-12&quot;, 75).otherwise(df[&quot;Booking&quot;])) \n\n\nBut I have so many values to replace so I tried below codes\ntargetDf = df.withColumn(&quot;Booking&quot;,\\\n when(df[&quot;Booking&quot;] &lt;= 25, (df[&quot;movingAvg&quot;].when(df[&quot;date&quot;] == ?)).otherwise(df[&quot;Booking&quot;]))\n\nI dont know how to write (?) last year same date moving average value.\n'
"My data is a nested Chinese characters list.\ntext1:\n\n[['沒人',\n  '關心',\n  '屏東',\n  '是否',\n  '淹水',\n  '了',\n  '天龍',\n  '新聞台',\n  '只',\n  '關心',\n  '還沒來',\n  '的',\n  '颱',\n  '風'],\n ['不報', '沒人', '知道', '一報', '一堆', '人去', '看然', '後', '就']]\n\nI want to delete empty strings and stopwords with this list comprehension:\nstopwords('zh')\n\n{'即或', '那些', '哪', '如此', '别处', '她', '这就是说', '自打', '只', '赶', '其二', '对比', '它', '；', '乌乎', '其', '宁', '不是', '个', '来自', '啥', '么', '就要', '纵然', '俺', '二', '尽管如此', '让', '吗', '不成', '（', '依照', '的话', '过', '作为', '些', '七', '要是', '各自', '这么些', '们', '总的来看', '犹且', '或', '几', '本着', '因此', '＆', '故', '如是', '＜', '倘或', '～', '以免', '顺着', '矣哉', '任凭', '某个', '或者', '以为', '哟', '恰恰相反', '今', '０', '待', '或曰', '至', '若', '固然', '别说', '要不', '除非', '况且', '嘘', '嗡', '介于', '甚且', '如果', '有', '随着', '其次', '尔尔', '那个', '他们', '曾', '只怕', '个别', '更', '可', '如下', '及', '临', '故而', '一方面', '针对', '尽管', '喔唷', '哇', '加之', '此处', '这里', '以至', '因了', '哪些', '那时', '依', '跟', '凭', '嗳', '含', '》', '通过', '还有', '倘使', '截至', '直到', '那么些', '除外', '＠', '一种', '乃至于', '纵令', '就是说', '呜', '否则', '啦', '往', '由此可见', '继后', '嗡嗡', '以期', '各位', '向着', '别是', '等等', '然则', '反过来说', '呀', '某些', '〉', '经过', '共', '呕', '何以', '非但', '该', '分别', '叫', '哪年', '除开', '这个', '这儿', '处在', '竟而', '趁', '相对而言', '比及', '后', '据此', '而', '那么样', '不问', '与其说', '假使', '呵', '比如', '打', '自从', '设或', '当地', '由于', '八', '再其次', '不', '及其', '一则', '设使', '是以', '多么', '诸', '当然', '这样', '哪儿', '：', '于是乎', '总之', '凡是', '人们', '那儿', '咦', '向', '不至于', '大', '再有', '替', '其余', '喏', '除了', '与', '却', '我', '有的', '当', '鉴于', '全部', '一个', '矣乎', '任何', '嘻', '照', '而外', '那么', '一何', '不仅', '说', '自后', '不得', '３', '望', '着', '开外', '得了', '既是', '离', '余外', '自个儿', '或是', '自', '哦', '云云', '一样', '以致', '故此', '｜', '不只', '而是', '前后', '［', '不过', '。', '了', '那会儿', '来', '甚至', '者', '诸位', '借傥然', '非特', '还要', '沿着', '非独', '称', '经', '嘿嘿', '赖以', '管', '也', '连同', '然后', '咧', '一切', '自家', '同时', '｝', '内', '只有', '极了', '起见', '不尽', '莫不然', '至若', '以故', '欤', '能', '哩', '旁人', '与否', '虽则', '首先', '无宁', '另外', '因而', '因着', '一来', '一些', '若夫', '＋', '六', '说来', '总的说来', '与其', '而况', '且不说', '反过来', '尔后', '随时', '诚如', '再说', '不光', '结果', '这时', '别人', '之类', '从此', '人家', '难道说', '两者', '庶几', '呼哧', '呜呼', '叮咚', '而已', '倘', '只是', '不若', '做', '嗬', '彼', '５', '唉', '不外乎', '这', '省得', '２', '并', '越是', '且', '啐', '怎么办', '顺', '可是', '所在', '各个', '哪样', '矣', '多', '哎哟', '哈', '还', '又及', '靠', '什么样', '照着', '般的', '另悉', '已', '譬如', '无', '例如', '一', '下', '有及', '其他', '有时', '焉', '它们', '似的', '，', '再者说', '此地', '也好', '前者', '你', '因', '较', '兮', '嘛', '虽然', '呵呵', '再者', '但凡', '从而', '哼唷', '若非', '一般', '用来', '这么样', '抑或', '所有', '何', '呢', '所', '还是', '不特', '谁料', '正是', '第', '￥', '别', '不但', '慢说', '紧接着', '要么', '谁', '所以', '漫说', '以便', '哪边', '哪里', '由', '对于', '类如', '假若', '可见', '那样', '怎么', '不单', '吱', '九', '大家', '除', '莫若', '、', '腾', '打从', '乃', '和', '具体地说', '咱们', '本身', '嗯', '接着', '甚而', '有关', '他们们', '进而', '后者', '本', '不怕', '咳', '自各儿', '起', '我们', '这么点儿', '只限', '于', '才', '谁人', '仍', '不比', '为何', '即便', '设若', '之所以', '且说', '如上', '不管', '较之', '凭借', '反之', '小', '果真', '人', '再则', '冲', '彼此', '就算', '于是', '上', '总的来说', '既', '就是了', '１', '莫如', '为什么', '云尔', '每当', '兼之', '４', '先不先', '要不是', '以及', '何处', '得', '为着', '或则', '不惟', '最', '以至于', '那', '即使', '彼时', '至于', '眨眼', '但是', '在于', '贼死', '那边', '能否', '咋', '不拘', '由此', '乃至', '嘿', '甚么', '没奈何', '纵', '把', '他人', '哉', '吧', '自身', '五', '全体', '非', '遵照', '喽', '光是', '即令', '向使', '宁肯', '点', '继之', '岂但', '纵使', '上下', '这边', '宁愿', '好', '！', '倘然', '︿', '不独', '她们', '喂', '开始', '甚或', '这么', '尔', '继而', '看', '何况', '中', '冒', '所幸', '〈', '啪达', '果然', '你们', '６', '给', '本人', '月', '这些', '乎', '距', '怎么样', '巴巴', '如', '替代', '用', '逐步', '可以', '尽', '如同', '＄', '使', '也罢', '与此同时', '几时', '始而', '不料', '只消', '甚至于', '正巧', '啊', '为', '此间', '不尽然', '去', '反而', '不如', '从', '关于具体地说', '日', '而且', '都', '儿', '到', '怎样', '为了', '哪个', '综上所述', '罢了', '哈哈', '谁知', '什', '才能', '加以', '借', '哼', '仍旧', '而后', '别的', '等', '对', '按', '其一', '＃', '论', '诸如', '既往', '吧哒', '另一方面', '再', '咚', '换言之', '年', '如其', '那里', '若果', '多少', '）', '但', '９', '如若', '并且', '其它', '各', '是的', '正如', '要不然', '这次', '比方', '＊', '毋宁', '《', '使得', '许多', '及至', '朝', '鄙人', '据', '何时', '出来', '其中', '７', '只当', '为此', '譬喻', '四', '来着', '咱', '简言之', '别管', '唯有', '哪天', '而言', '各种', '之一', '依据', '随', '乘', '在', '如上所述', '之', '按照', '朝着', '不论', '每', '地', '连', '此时', '已矣', '致', '很', '将', '一转眼', '基于', '关于', '吓', '巴', '换句话说', '哎呀', '倘若', '亦', '的确', '一旦', '８', '惟其', '即若', '具体说来', '被', '某某', '根据', '另', '受到', '就是', '万一', '尚且', '当着', '怎奈', '像', '要', '以上', '虽说', '着呢', '哪怕', '宁可', '不然', '孰料', '嘎登', '除此之外', '如何', '是', '则', '沿', '来说', '等到', '对待', '这一来', '为止', '即', '本地', '拿', '此', '然而', '请', '凡', '］', '哎', '您', '会', '这会儿', '这般', '三', '某', '＞', '总而言之', '秒', '随后', '不妨', '虽', '庶乎', '时候', '则甚', '又', '自己', '遵循', '什么', '时', '呸', '只要', '比', '以来', '犹自', '无论', '既然', '区', '至今', '？', '对方', '呃', '同', '此外', '哗', '孰知', '他', '由是', '出于', '诚然', '若是', '因为', '那般', '己', '就', '俺们', '嘎', '｛', '趁着', '便于', '非徒', '光', '在下', '假如', '边', '分', '归', '即如', '％', '零', '二来', '阿', '啷当', '归齐', '有些', '呗', '正值', '此次', '的', '以', '怎', '任', '前此'}\n\n\n\ntext2 = [w for x in text1 for w in x if not w in stopwords('zh') and w != '']\n\nbut for the result, each of the characters is separated and the inner list seems to be gone.\ntext2:\n\n['沒',\n '關',\n '心',\n '屏',\n '東',\n '否',\n '淹',\n '水',\n '天',\n '龍',\n '新',\n '聞',\n '台',\n '關',\n '心',\n '還',\n '沒',\n '來',\n '颱',\n '風',\n '央',\n '山',\n '脈',\n '減',\n '輕',\n '風',\n '勢',\n '遇',\n '初',\n '十',\n '潮',\n '台',\n '灣',\n '西',\n '部',\n '海',\n '區',\n '樣',\n '淹',\n '水',\n '給',\n '報',\n '沒',\n '知',\n '道',\n '報',\n '堆',\n '然',\n '後',\n '稀',\n '還',\n '記',\n '前',\n '淹',\n '水',\n '淹',\n '硬',\n '碟',\n '漲',\n '兩',\n '倍',\n '價',\n '格',\n '結',\n '果',\n '變',\n '旱',\n '災',\n '楊',\n '璨',\n '澤',\n '台',\n '南',\n '新',\n '市',\n '毛',\n '毛',\n '雨',\n '乖',\n '乖',\n '班',\n '改',\n '號',\n '後',\n '公',\n '車',\n '座',\n '沒',\n '拆',\n '拜',\n '託',\n '們',\n '回',\n '家',\n '讀',\n '書',\n '麼',\n '爛',\n '車',\n '送',\n '給',\n '天',\n '龍',\n '國',\n '新',\n '北',\n '雨',\n '天',\n '裡',\n '面',\n '會',\n '淹',\n '水',\n '太',\n '離',\n '譜',\n '台',\n '市',\n '個',\n '爽',\n '爆',\n '們',\n '免',\n '費',\n '爽',\n '錢',\n '們',\n '台',\n '市',\n '付',\n '錢',\n '颱',\n '風',\n '不',\n '報',\n '沒',\n '人',\n '知',\n '道',\n '一',\n '報',\n '一',\n '堆',\n '人',\n '去',\n '看',\n '然',\n '後']\n\nis there a way to clean stopwords and still both maintain it as a word (not characters) and as a nested list?\n"
"I have the following data-frame. \n\n\n\nand I have an input list of values\n\n\n\nI want to match each item from the input list  to the Symbol and Synonym column in the data-frame and to extract only those rows where the input value appears in either the Symbol column or Synonym column(Please note that here the values are separated by '|' symbol).\n\nIn the output data-frame I need an additional column Input_symbol which denotes the matching value. So here in this case the desired output will should be like the image bellow.\n\nHow can I do the same ?.\n\n\n"
'I have a word vector object from gensim\'s word2vec package and can access the \'username\' using model.wv.vocab and vectors using model.wv[w].\n\nHere\'s a sample of what I\'m working with\n\nfor w in sample:\n    print("ID:", w)\n    print("Vector subset: \\n", model.wv[w][:10])\n\nID: 1843\nVector subset: \n [ 0.08228672 -0.32398582 -0.16024925  0.44939137 -0.28749713  0.25965428\n -0.18141621  0.06290377  0.1270649   0.40421844]\nID: 866\nVector subset: \n [-0.21120088  0.10489845  0.17965898  0.18383555 -0.24510185 -0.00716993\n -0.18718664  0.3398481   0.07536748 -0.5193063 ]\nID: 2819\nVector subset: \n [ 0.33056906  0.20122662  0.0239714   0.1846028  -0.1632814  -0.4005747\n -0.02339112  0.22077617  0.20608544 -0.12747312]\nID: 4091\nVector subset: \n [ 0.5139592   0.1325652  -0.19846869  0.02061795 -0.72117347 -0.5065503\n -0.2806759   0.13045706  0.5880965  -0.497771  ]\nID: 4871\nVector subset: \n [-0.30731577  0.10253543  0.01026379  0.24779265  0.3701798  -0.16493073\n  0.07395677 -0.4943776   0.02144529 -0.12544158]\nID: 6557\nVector subset: \n [-0.01380698  0.03429209  0.11136885  0.10298727 -0.09034968 -0.09744099\n  0.04731373  0.12851992  0.5266305  -0.14707205]\nID: 4691\nVector subset: \n [-0.12838683  0.34491533  0.10016204 -0.00582217 -0.1514073   0.13864768\n  0.05341618 -0.15653287  0.37432986  0.09268643]\nID: 409\nVector subset: \n [ 0.01493216  0.06893755  0.10319904 -0.08454162 -0.08191169 -0.16257484\n -0.10028194 -0.02943738  0.3722616  -0.27091444]\nID: 8229\nVector subset: \n [-0.72491664  0.28790048  0.04535258  0.57867676 -0.09895556 -0.01902669\n -0.03930351  0.551734   -0.2825539   0.1426454 ]\nID: 5222\nVector subset: \n [-0.05142907 -0.3080357  -0.00205866 -0.02018788 -0.07856932 -0.46743438\n -0.29095295  0.44115666  0.34238762  0.2151215 ]\n\n\nI need to manipulate this information into a form that looks like the dataframe below to pass into a script: \n\n    username        1       2       3       4       5   6\n          00    0.023   0.232   -0.13   0.2424  -0.242  -0.22\n          01    0.001   0.013   -0.232  0.3232  0.2324  -0.023234\n          02    0.244   -0.24   -0.3555 0.444   -0.22   -0.2342\n          03    0.5333  -0.99   -0.9242 -0.43   0.242   0.423\n\n\nMy current idea was to create a dictionary of usernames &amp; transposed vectors and then create a dataframe from the dictionary.\n\nvect_dict = {}\nfor w in model.wv.vocab:\n    reshaped_vec = np.reshape(model.wv[w], (300, 1)).T\n    vect_dict[w] = reshaped_vec\n\n\nHowever, this won\'t give me a separate column for usernames and the row as the transposed vectors with each column being an ith index into the vector.  \n\nHow can I manipulate my given data into this form?\n\nThank you! \n'
'I am trying to figure out how to handle variations in number ranges I\'m receiving.  I\'ve compiled them from a dataframe and am trying to get all the ranges in a 100-200 / 200-300 / 300-400 format.\n\nSometimes they\'ll be entered as 300.400 instead of 300-400 and sometimes they\'ll be entered as 300.5-400.5.  If possible, I\'d also like to account for a situation where they are entered as 300.5.400.5 because as insane as it is, someone might do it.  I guess I should probably try to plan for any character that isn\'t "-" showing up between them.\n\nTo boil it down, I want to round up numbers with decimals and replace non "-" characters separating ranges without replacing periods in decimals.\n\nExcuse my terrible code:\n\nnum_range = [p.replace(\' \', \'\') for p in num_dirty]\nnum_range = [p.replace(\';\', \' \') for p in num_dirty]\nnum_range = [p.replace(\'.\', \'-\') for p in num_dirty]\n\nnum_clean = []\n\nfor r in num_dirty:\n    num_clean.append(r.split(\'/\'))\n\nnum_clean = list(itertools.chain.from_iterable(num_clean))\n\nnum_clean = \',\'.join(num_clean)\n\n\nI\'m also using this function I found on here to list out all of the numbers in the frequency range so that I can remove any duplicate ranges:\n\nnum_lines = num_clean.split(\',\')\nnum_numbers = []\ni = 0\nfor line in num_lines:\n    if line == \'\':\n        continue\n    elif \'-\' in line:\n        t = line.split(\'-\')\n        num_numbers += range(int(t[0]), int(t[1]) + 1)\n    else:\n        num_numbers.append(int(line))\n\n\nThank you!\n'
'I have some dirty data that is in user input, so it\'s not consistent.  They are all either a single number or a number range.\n\nnumber_ranges = [\n    \'11.6\', \'665.690, 705.715\', \'740.54-830.18ABC;900-930ABC\', \'1200\',\n    \'2100 / 2200; 2320 / 2350\', \'2300-2400 / 2500-2560 / 2730-2740\'\n]\n\nnumber_ranges = \',\'.join(number_ranges)\n\nnumber_ranges = number_ranges.replace(\' \', \'\')\n\nnumber_ranges= re.sub(r"[a-zA-Z]+", "", number_ranges)\n\nnumber_ranges= re.sub(r"[;]+", ",", number_ranges)\n\nnumber_ranges = str(number_ranges).split(\',\')\n\n\nThis is the resulting list:\n\n[\n    \'11.6\', \'665.690\', \'705.715\', \'740.54-830.18\', \'900-930\', \'1200\', \'2100/2200\',\n    \'2320/2350\', \'2300-2400/2500-2560/2730-2740\'\n]\n\n\nI know from here that\n\nfor i in number_ranges:\n    if (len(i) &gt;5) and (\'.\' in i) and (\'-\' not in i):\n        i = i.replace(\'.\',\'-\')\n\nfor i in number_ranges:\n    if (\'-\' in i) and (\'/\' in i):\n        i = i.split(\'/\')\n\nfor i in number_ranges:\n    if len(i) &lt; 3:\n        i = str(int(i) * 1000)\n\n\nI\'ve also tried this method:\n\nfor n, i in enumerate(number_ranges):\n    if (len(i) &gt;5) and (\'.\' in i) and (\'-\' not in i):\n        number_ranges[n] = i.replace(\'.\',\'-\')\n\n\n665.690 should be 665-690, 740.54-830.18ABC should be 741-830, 2100 / 2200 should be 2100-2200, 11.6 should be 11600\n\nThe end result should have the ranges in integer tuples, so:\n\n[(11600,), (665, 690), (705, 715), (741, 830), (900, 930), (1200,), (2100, 2200), (2320, 2350), (2300, 2400), (2500, 2560), (2730, 2740)]\n\n\nFrom there if I need them in a range I can use:\n\nfor pair in number_ranges:  \n    number_ranges.append("{}-{}".format(*pair))\n\n\nI know the logic, but not the implementation.\n\nI guess what I\'m trying to figure out is how to replace characters/manipulate strings based on certain conditions.\n\nThese are the most common formats, so I want to account for them.  I know I\'ll never be able to predict what someone will put in, but I think I can account for 95% plus of cases.\n\nMy apologies in advance if I\'ve left out any necessary information.  I will provide it as soon as I can.\n\nThank you.\n\nEdit:\nI got it to work with the below code:\n\nnumber_ranges = \',\'.join(number_ranges)\n\nnumber_ranges = number_ranges.replace(\' \', \'\')\n\nnumber_ranges= re.sub(r"[a-zA-Z]+", "", number_ranges)\n\nnumber_ranges= re.sub(r"[;]+", ",", number_ranges)\n\nnumber_ranges = str(number_ranges).split(\',\')\n\nfor n, i in enumerate(number_ranges):\n    if (\'-\' in i) and (\'/\' in i):\n        number_ranges[n] = i.replace(\'/\',\',\')\n\nfor n, i in enumerate(number_ranges):\n    if (\'-\' not in i) and (\'/\' in i):\n        number_ranges[n] = i.replace(\'/\',\'-\')\n\nfor n, i in enumerate(number_ranges):\n    if (\'-\' not in i) and (\'.\' in i) and (len(i)&gt;4):\n        number_ranges[n] = i.replace(\'.\',\'-\')\n\nfor n, i in enumerate(number_ranges):\n    if (\'.\' in i) and (len(i) &lt;= 4) and (float(i) &lt; 30):\n        number_ranges[n] = str(round(float(i) * 1000))\n\nnumber_ranges = [i.split(\',\') for i in number_ranges]\n\n'
"My Data \n\n30-Apr-18 A           30-Apr-18\n30-Apr-18 A           30-Apr-18\n\n\nI have a column with dates and some of the dates have A next to them. So when I import the data. Its getting recognized as an object. I want it to be just a date without A next to it. \n\ndf is the data frame and start is the column with dates I am using df[start] to bring in start column and function if it has A, remove A and return just date. \n\nI want to remove based on the given condition. I have tried to use this below line code. \n\ndf[Start].apply(lambda x = x.strip(x[-1]) if x[-1] == 'A'))\n\nAny help on this is greatly appreciated. \n"
"I've created a dataframe as:\n\nratings = imdb_data.sort('imdbRating').select('imdbRating').filter('imdbRating is NOT NULL')\n\n\nUpon doing ratings.show() as shown below, i can see that\nthe imdbRating field has a mixed type of data such as random strings, movie title, movie url and actual ratings. So the dirty data looks this:\n\n+--------------------+\n|          imdbRating|\n+--------------------+\n|Mary (TV Episode...|\n| Paranormal Activ...|\n| Sons (TV Episode...|\n|        Spion (2011)|\n| Winter... und Fr...|\n| and Gays (TV Epi...|\n| grAs - Die Serie...|\n| hat die Wahl (2000)|\n|                 1.0|\n|                 1.3|\n|                 1.4|\n|                 1.5|\n|                 1.5|\n|                 1.5|\n|                 1.6|\n|                 1.6|\n|                 1.7|\n|                 1.9|\n|                 1.9|\n|                 1.9|\n+--------------------+\nonly showing top 20 rows\n\n\nIs there anyway i can filter out the unwanted strings and all just get the ratings ? I tried using UDF as:\n\n ratings_udf = udf(lambda imdbRating: imdbRating if isinstance(imdbRating, float)  else None)\n\n\nand tried calling it as:\n\nratings = imdb_data.sort('imdbRating').select('imdbRating')\nfiltered = rating.withColumn('imdbRating',ratings_udf(ratings.imdbRating))\n\n\nThe problem with above is, since it tried calling the udf on each row, each row of the dataframe mapped to a Row type and hence returning None on all the values. \n\nIs there any straightforward way to filter out those data ?\nAny help will be much appreciated. Thank you\n"
'I am working on a survey data analysing project which consist 2 Excel files- in file pre-survey, it contains 800+ response records; while in post-survey file it contains 500ish responses. Both of them have (at least) one common column SID (Student ID). Something Y happened in between, and I am interested analysing the effectiveness of Y, and in what degrade Y impacts on different categories of people.\n\nWhat adds more complexity is that in each Excel file, it contains multiple tabs. Different interviewers interviewed several interviewees and documented in each tabs for different sections of survey. Columns may or may not be the same for different tabs, so it would be hard to be complied in one file. (Or does it actually make sense to combine them in one with lots of null values?)\n\nI am trying to find the students who did both pre- and post- surveys. How can I do it across sheets and files using python/pandas/other packages?\n\nBonus if you could also suggest the approach to solve the problem.\n'
'I have df like below (Example)\n\nindex       y       z\n0           118     .\n1           118     .\n2           118     .\n3           116\n4           116\n5           110\n6           110\n7           104\n8           104\n.\n.\n.\n.\n.\n320         3       .\n321         3       .\n322         3\n323         7\n324         7\n328         11\n329         11\n.\n.\n.\n350         25\n351         25\n\n\nAs you can see in column y the values starts from 118 and it keeps on decreasing till number 3 at index 322. Again it keeps increasing from index 323 of value 7 and it reaches the value of 25 at index 351. \n\nAlso you can find the repeated value in column y. (118 repeated thrice, 116 repeated twice and so on....)\n\nMy Requirement\n\nI would like to slice the first part of that df.(Value from 118 to 3)\ni.e., from index 0 till 322.\n\nhow my df should look like\n\nindex       y       z\n0           118     .\n1           118     .\n2           118     .\n3           116\n4           116\n5           110\n6           110\n7           104\n8           104\n.\n.\n.\n.\n.\n320         3       .\n321         3       .\n322         3\n\n\nI believe that there should be an alternate pythonic way which make it easier. (In-built function or using numpy) Any help would be appreciated.\n'
'I need to match contacts in a database by how they were contacted by a unique ID number. I\'ve created a very small mock dataframe below to help with a suggestion:\n\ndata = [\n    ["email", "emailperson1@email.com", 1],\n    ["phone", "555-555-1111", 1],\n    ["slack", "secondpersonslack", 2],\n    ["phone", "111-111-1111", 3],\n]\n\ndata2 = [\n    [1, "emailperson1@email.com", "555-555-1111", "slack1"],\n    [2, "emailperson2@gmail.com", "555-555-2222", "secondpersonslack"],\n    [3, "tomasconticello@gmail.com", "111-111-1111", "tomasslack"],\n]\n\nstackdata = pd.DataFrame(\n    data, columns=["contact method", "from:", "column that I dont know how to make"]\n)\nstackdata2 = pd.DataFrame(data2, columns=["id", "email", "phone", "slack"])\n\n\nIn my real dataset, what I want to fill out is the column \'column that I dont know how to make\'.\n\nSo, take first contact method in stackdata in row one, which was an email address. This email lines up with ID 1 in stack2, so it would populate the column in the stackdata1 with "1". \n\nI was thinking of some of kind of for loop like:\n\nfor i in stackdata[\'column that I dont know how to make\']:\n  if i matches one of the columns in any row, then populate column with id of that row\n\n\nThanks for any help and let me know if i can make this question easier to answer!\n'
"I want my final data to not have elements of the initial test data which I want to clean up. The process of copying and pasting data in the code has been extremely tedious and gets complicated as more and more criteria is added.\n\nOriginal Values:\n\n(1, 2, 3), (1, 2, 4),  (1, 2, 5), (1, 3, 4), (1, 3, 5)\n(1, 4, 5), (2, 3, 4),  (2, 3, 5), (2, 4, 5), (3, 4, 5)\n\n\nI want a combination which excludes the combinations contained in Test.csv\n\n(1,2,3),   (2,3,4),     (3,4,5),\n\n\nExpected Values\n\n(1, 2, 4),\n(1, 2, 5),\n(1, 3, 4),\n(1, 3, 5),\n(1, 4, 5),\n(2, 3, 5),\n(2, 4, 5)\n\n\nCode Attempt 1\n\na = [1,2,3,4,5]\n\nimport csv\n\nwith open('Test.csv', newline='') as myFile:  \n    reader = csv.reader(myFile)\n    list_a = list(reader)\n\ncombo_a = [(p,q,r) for p in a for q in a for r in a\n                 if q &gt; p and r &gt; q and r &gt; p\n                 and (p,q,r) not in list_a]\n\nprint (combo_a)\n\n\nCode Attempt 2\n\n a = [1,2,3,4,5]\n\nimport csv\n\nwith open('Test.csv', newline='') as myFile:  \n    reader = csv.reader(myFile)\n    list_a = list(map(tuple, reader))\n\ncombo_a = [(p,q,r) for p in a for q in a for r in a\n                 if q &gt; p and r &gt; q and r &gt; p\n                 and (p,q,r) not in list_a]\n\nprint (combo_a)\n\n\nBoth Codes output Incorrect Result\n\n(1, 2, 3),\n(1, 2, 4),\n(1, 2, 5),\n(1, 3, 4),\n(1, 3, 5),\n(1, 4, 5),\n(2, 3, 4),\n(2, 3, 5),\n(2, 4, 5),\n(3, 4, 5),\n\n"
'I have a dataframe df:\n\n    A   B   C  Value\n0   10  aa  MN     5\n1   10  aa  NaN    6\n2   12  bb  MN     5\n3   13  cc  BK     7\n4   13  cc  Nan    8\n5   14  cc  SI     8\n\n\nI am trying to clean the date so that where columns A and B are identical it will assign C to the matching value combine the rows and sum C\n\n    df:\n    A   B   C  Value\n0   10  aa  MN     11\n1   12  bb  MN     5\n2   13  cc  BK     15\n3   14  cc  SI     8\n\n\nShould be noted that only column C has NaN values. All three values should make unique groups but are not exclusive to there group.\nHow would I do this in my Jupyterbook?\n'
'I have a data frame with a column of job titles and the company name in the same string of each row df[\'Titles\'], I also have a list of all possible company names joblist\n\nIm attempting to create a new column df[\'Company\'] based on a match condition with the list/series but both options are failing due to what I think is a dirty list/series\n\nmy list/series is about 3000+ names and I\'m wondering whats the best way to clean it, using a general regex or something?\n\nthe code for the list, then series was given to me by a stack user and is as follows;\n\ndf[\'Company\'] = df[\'Title\'].str.contains(\'|\'.join(joblist))\n\ndf[\'Company\'] = df[\'Title\'].str.extract(f\'({"|".join(joblist)})\', expand=False)\n\n\nthe errors are as follows;\n\nThe list returns: \'error unbalanced parenthesis at position 8466\'\n\nThe series returns: \'bad escape \\C at position 9401\'\n\nI have cleaned some basic stuff like "\'s" to just "s" nut still get both errors, any help is appreciated, thanks!\n'
'I\'m forced to turn from Perl to Python now, took a course even.\nBut I\'m struggling already with a simple data cleaning task on CSV files.\nI want to replace a ¶ by a SPACE in a specific column, the other columns must be untouched:\n\nIn Perl this is very straight forward and works like a charm:\n\nperl -F";" -lane \'BEGIN {$,=";"} print $F[0],$F[1],$F[2],$F[3],$F[4],$F[5],$F[6],$F[7],$F[8],$F[9],$F[10],$F[11], $F[12]=~s/\\¶/ /g, $F[13]\' a.csv\n\n\nHow could I do that in Python? It doesn\'t need to be a oneliner...\n'
'Synopsis\n\nGiven a directory containing CSV files named with the pattern Prefix-Year.csv, create a new set of CSV files named Prefix-aggregate.csv where each aggregate file is the combination of all CSV files with the same Prefix.\n\nExplanation\n\nI have a directory containing 5,500 CSV files named in this pattern: Prefix-Year.csv. Example:\n\n18394-1999.csv\n   . . .       //consecutive years\n18394-2014.csv\n18395-1999.csv //next location\n\n\nI want to group and combine files with common Prefixes into files named Prefix-aggregate.csv.\n'
'I have a python script that pulls EPS information from streetinsider.com. Currently I\'m cleaning the data using an entirely inefficient method as seen below. Wondering if someone can show how this can be done more efficiently. \n\nThe following example is very very scaled down, there are many more columns and many many more rows. \n\neps_table = DataFrame({\'% Beat\': \'+1,405%\', \'% Week\': \'+123%\'}, index=[0])\n\nthings_to_remove = [\'% Beat\', \'% Week\']\nfor i in things_to_remove:\n    eps_table[i] = eps_table[i].replace("%", "",regex=True)\n    eps_table[i] = eps_table[i].replace("\\+", "", regex=True)\n    eps_table[i] = eps_table[i].replace("\\,", "", regex=True)\n\n\nThanks.\n'
"I am trying to add more than two timestamp values and I expect to see output in minutes/seconds. How can I add two timestamps? I basically want to do: '1995-07-01 00:00:01' + '1995-07-01 00:05:06' and see if total time&gt;=60minutes.\nI tried this code: df['timestamp'][0]+df['timestamp'][1]. I referred this post but my timestamps are coming from dataframe.\nHead of my dataframe column looks like this:\n\n\r\n\r\n0   1995-07-01 00:00:01\r\n1   1995-07-01 00:00:06\r\n2   1995-07-01 00:00:09\r\n3   1995-07-01 00:00:09\r\n4   1995-07-01 00:00:09\r\nName: timestamp, dtype: datetime64[ns]\r\n\r\n\r\n\n\nI am getting this error: \nTypeError: unsupported operand type(s) for +: 'Timestamp' and 'Timestamp'\n\n"
'I want to use some data from the NOAA website. It is a csv file with data for all hurricanes since 1851, with a format like this: Format example / README file\n\nAs you can see, although everything is contained within one csv file, each hurricane has it\'s own table, with a separate header. \n\nHow can I remove the headers and put the information in a "Hurricane Name" column instead? I want to combine everything into a single data frame, so it\'s easier to use. Thanks!\n\nExample:\n\n\n  AL092011,              IRENE,     3, \n  \n  20110821, 0000,  , TS, 15.0N,  59.0W,  45, 1006,  105,    0,    0,\n  45,    0,    0,    0,    0,    0,    0,    0,    0, \n  \n  20110821, 0600,  , TS, 16.0N,  60.6W,  45, 1006,  130,    0,    0,\n  80,    0,    0,    0,    0,    0,    0,    0,    0, \n  \n  20110821, 1200,  , TS, 16.8N,  62.2W,  45, 1005,  130,    0,    0,\n  70,    0,    0,    0,    0,    0,    0,    0,    0, \n  \n  AL092012,              ANOTHER_NAME,     2, \n  \n  20110821, 1800,  , TS, 17.5N,  63.7W,  50,  999,  130,   20,    0,\n  70,   30,    0,    0,    0,    0,    0,    0,    0, \n  \n  20110822, 0000,  , TS, 17.9N,  65.0W,  60,  993,  130,   30,   30,\n  90,   30,    0,    0,   30,    0,    0,    0,    0,\n\n\nI would like the header information into columns, like so:\n\n\n  AL092011, IRENE, 20110821, 0000,  , TS, 15.0N,  59.0W,  45, 1006,  105,    0,    0,\n  45,    0,    0,    0,    0,    0,    0,    0,    0, \n  \n  AL092011, IRENE, 20110821, 0600,  , TS, 16.0N,  60.6W,  45, 1006,  130,    0,    0,\n  80,    0,    0,    0,    0,    0,    0,    0,    0, \n  \n  AL092011, IRENE, 20110821, 1200,  , TS, 16.8N,  62.2W,  45, 1005,  130,    0,    0,\n  70,    0,    0,    0,    0,    0,    0,    0,    0, \n  \n  AL092012, ANOTHER_NAME, 20110821, 1800,  , TS, 17.5N,  63.7W,  50,  999,  130,   20,    0,\n  70,   30,    0,    0,    0,    0,    0,    0,    0, \n  \n  AL092012, ANOTHER_NAME, 20110822, 0000,  , TS, 17.9N,  65.0W,  60,  993,  130,   30,   30,\n  90,   30,    0,    0,   30,    0,    0,    0,    0,\n\n'
'We receive a csv file from our vendor everyday.\nHowever, the format is like this:\n\n&lt;table&gt;\n  &lt;tr&gt;\n    &lt;th colspan="6"&gt;#Receiver&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td colspan="6"&gt;#DateTime&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td colspan="6"&gt;#Address&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td colspan="6"&gt;&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;Col1&lt;/td&gt;\n    &lt;td&gt;Col2&lt;/td&gt;\n    &lt;td&gt;Col3&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;1&lt;/td&gt;\n    &lt;td&gt;A&lt;/td&gt;\n    &lt;td&gt;3&lt;/td&gt;\n    &lt;td&gt;10%&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;2&lt;/td&gt;\n    &lt;td&gt;B&lt;/td&gt;\n    &lt;td&gt;3&lt;/td&gt;\n    &lt;td&gt;20%&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;3&lt;/td&gt;\n    &lt;td&gt;C&lt;/td&gt;\n    &lt;td&gt;2&lt;/td&gt;\n    &lt;td&gt;10%&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\n\n\nI need to use pandas to read the table which starts at row 5 and column 3.\nHow could I skip the first few rows?\n\nThanks.\n'
"I'm currently working on this DataFrame python :\n\n\nThe data-set has one column and n lines.\n\nI would like to extract specifics components of specifics line, for exemple : \n\nFor each line i starting with 'n', store in variable x the second element of the line i.\n\nor \n\nFor each line i starting with 'e', store in variable x the second and third element of the line i. \n\nI would like to know which function/operation I can use for this problem.\n"
"Lets say I have 2 dataframes in pandas. I want to perform a left join on these dataframes in a very specific way as follows, and the easiest way to explain is probably via an example.\n\nFirst dataframe:\n\nDate &nbsp; Col1  &nbsp;Col2\n1/1\n2/1\n...\n\nSecond Dataframe:\n\nDate &nbsp;  A &nbsp; B &nbsp;  C\n1/1 &nbsp; &nbsp; 90 &nbsp;0 &nbsp;  0\n1/1  &nbsp; &nbsp; 0 &nbsp; 75&nbsp; 0\n1/1 &nbsp; &nbsp; 73 &nbsp;0 &nbsp; 0\n2/1 &nbsp; &nbsp; 0 &nbsp; &nbsp;0 &nbsp; 85\n2/1 &nbsp; &nbsp; 0 &nbsp; &nbsp;0 &nbsp; 75  \n\nThe dates in the first dataframe are unique and will be used to join to the second dataframe. Col1 and Col2 are not blank, but they are irrelevant for what I want to do. In second data frame, a date can appear multiple times, and exactly one of the columns A, B, and C contain a number greater than 0, with the other two containing 0. There could also be some extra columns which are irrelevant for what I want to do.\n\nI want to keep everything in the first dataframe and add columns A, B and C in such a way that the values in those columns would be the sum of all values on the given data in the second dataframe. So in the example above, I would want the output to look like this:\n\nDate &nbsp; Col1  &nbsp;Col2  &nbsp;  A &nbsp; B &nbsp;  C\n1/1 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;163&nbsp;75 &nbsp; 0\n2/1  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;0 &nbsp;&nbsp;160\n...\n\nI have looked at the pandas join function but it doesn't seem to give the option to sum columns in the way that I want. Is there any other way to achieve such a result? I realise I could probably write a for loop to do this, but I was hoping there might be a more efficient way. \n"
"I have a data frame containing the values in a columns:\n\ndf = pd.DataFrame({\n    'A': ['20*', 40, '30*' ],\n    'B': ['abc', 'bar', 'xyz'],\n})\n\n\nI want to remove the * in column A, result should be: ['20', 40, '30' ]\n\nHow can this be achieved?\n"
"I am working with pandas and would like to add columns to my dataframe from a list. Ideally I would like to iterate through my list in a for loop creating a single column in each pass. \n\nExample:\n\nimport pandas as pd\n\nd = {\n'name':['Ken','Bobby'],\n'age':[5,6],\n'score':[1,2]}\n\ndf = pd.DataFrame(d,columns=['name','age','score'])\n\nnew_columns = ['col1', 'col2']\n\n\nOutput:\n\n    name    age     score\n    Ken     5       1\n    Bobby   6       2\n\n\nDesired output:\n\n    name    age     score   col1     col2\n    Ken     5       1       1        1\n    Bobby   6       2       2        2\n\n\nCorrected solution:\n\nfor i in new_columns:\n     df[i] = pd.Series([1,2])\n\n\nEdit:\n\nI have corrected the code to fix a typo however there is a great additional solution that does not use for loops which I intend to use in the future. \n"
'I\'m trying to get the column names from a dirty dataset. The name of the column names start before the symbol "=". Is there a quick method to do this without looping over all the data?\nHow it looks now\n\nimport pandas as pd\nimport numpy as np\n\nmissing_values=["n/a", "na", "--"]\n\ndf = pd.read_csv("data/data_bestand_3.txt", sep="&amp;", na_values=missing_values)\ndf.head()\n\n'
"I am trying to clean my url data using regular expression. I have already cleaned it bypass, but I have a last problem that I don't know how to solve.\nIt is a data that I have scraped from some newshub and it consists from theme part and a source part.\nI need to scrape the source pattern from url and leave out the theme part in order to put it on to the numpy array for the further analysis.\nMy scraped urls look like this:\n/video/36225009-report-cnbc-russian-sanctions-ukraine/\n\n/health/36139780-cancer-rates-factors-of-stomach/\n\n/business/36187789-in-EU-IMF-reports-about-world-economic-environment/\n\n/video/35930625-30stm-in-last-tour-tv-album-o-llfl-/?smi2=1\n\n/head/36214416-GB-brexit-may-stops-process-by/\n\n/cis/36189830-kiev-arrested-property-in-crymea/\n\n/incidents/36173928-traffic-collapse-by-trucks-incident/\n\n..............................................................\n\nI have tried the following code to solve this problem, but it doesn't work and returns a whole string back instead of just theme parts.\nimport numpy as np\nimport pandas as pd\nimport re\n\nregex = r&quot;^/(\\b(\\w*)\\b)&quot;\n\npattern_two = regex\nprog_two = re.compile( pattern_two )\n\nwith open('urls.txt', 'r') as f:\n\n    for line in f:\n        line = line.strip()\n    \n    if prog_two.match( line ):\n          print( line )\n\nAlso I have checked the regular expression (on regex101.com) like regex = r&quot;^/(\\b(\\w*)\\b)&quot; and like regex = r&quot;^/[a-z]{0,9}./&quot;, but it also doesn't work properly. I don't have a strong skills in regex and maybe I am doing something wrong?\nThe final result that I expect is following:\nvideo\nhealth\nbusiness\nvideo\nhead\ncis\nincidents  \n...........\n\nThank you very much for helping!\n"
"I have some data manually digited but kind of disorganized, in the way sometimes its easy to get the numbers from the brackets but its hareded when they are alones. \nThis is for TONS of rows so it maybe has other types of entries\n\nI have tried to just separate numbers or use the extract function but whitout succes.\n\next = ext['ITEMS'].str.extractall(pat = '(/d{2})')\n\n\n\n"
"\n\nI want to replace all the 'k' by 000 in the column Vehicle Value, which means the data in Vehicle Value should be represented as 25000,100000,10000.....  \n\nHow can I do it? Does 'Pandas' have a function to achieve it?\n"
'I have the following code to create a column with cleaned up zip codes for the USA and Canada\n\ndf = pd.read_csv(file1)\nusa = df[\'Region\'] == \'USA\'\ncanada = df[\'Region\'] == \'Canada\'\ndf.loc[usa, \'ZipCleaned\'] = df.loc[usa, \'Zip\'].str.slice(stop=5)\ndf.loc[canada, \'ZipCleaned\'] = df.loc[canada, \'Zip\'].str.replace(\' |-\',\'\') \n\n\nThe problem is that some of the rows that have "USA" as the country contain Canadian postal codes in the dataset. So the USA logic from above is being applied to Canadian postal codes. \n\nI tried the edited code below along with teh above and experimented with one provinces ("BC") to prevent the USA logic from being applied in this case but it didn\'t work \n\nusa = df[\'Region\'] == \'USA\' and df[\'Ship To State\'] != \'BC\'\n\n'
'I am working through a DataFrame trying to clean things up, and came across a bit of an unusual element. Two test values are represented in the same results column posted below. I would like to split the CRP group into another 2 columns, tentatively titled "CRP_mgml" and "CRP_Result", but I am unsure how to find where my IL6 rows end and the CRP rows begin. I have a feeling I could use a regex function to parse through, but I\'m pretty new to this so need some help. Thank you in advance. \n\ndf2[[\'Group_Id\', \'Result\']]\nOut[46]: \n                                               Group_Id  Result\n0     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n1     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n2     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n3     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   392.0\n4     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n5     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n6     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n7     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n**8     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n9     LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   328.0\n10    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n11    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n12    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n13    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n14    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n15    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   400.0\n16    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    70.9\n17    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...     3.4\n18    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...     2.6\n19    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...     1.9\n20    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    30.0\n21    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    19.3\n22    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    39.4\n23    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    17.0\n24    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    36.9\n25    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...     2.6\n26    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    10.4\n27    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...     3.4\n28    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...    11.3\n29    LAB\\(LLB63) DBNull\\(LLB64) DBNull\\INTERL6\\BCIL...   281.0\n                                                ...     ...\n4558                                         CRP (mg/L)     1.5\n4559                                         CRP (mg/L)     2.0\n4560                                         CRP (mg/L)     5.3\n4561                                         CRP (mg/L)     0.8\n4562                                         CRP (mg/L)     1.2\n4563                                         CRP (mg/L)     2.0\n4564                                         CRP (mg/L)     2.6\n4565                                         CRP (mg/L)     3.1\n4566                                         CRP (mg/L)     3.9\n4567                                         CRP (mg/L)     5.5\n4568                                         CRP (mg/L)     7.8\n4569                                         CRP (mg/L)    13.0\n4570                                         CRP (mg/L)    20.2\n4571                                         CRP (mg/L)    25.0\n4572                                         CRP (mg/L)    24.8\n4573                                         CRP (mg/L)    42.3\n4574                                         CRP (mg/L)    71.0\n4575                                         CRP (mg/L)   171.2\n4576                                         CRP (mg/L)   271.6\n4577                                         CRP (mg/L)   289.0\n4578                                         CRP (mg/L)   148.7\n4579                                         CRP (mg/L)    77.3\n4580                                         CRP (mg/L)    69.0\n4581                                         CRP (mg/L)    68.5\n4582                                         CRP (mg/L)    69.7\n4583                                         CRP (mg/L)   125.5\n4584                                         CRP (mg/L)    41.9\n4585                                         CRP (mg/L)    12.9\n4586                                         CRP (mg/L)     4.1\n4587                                         CRP (mg/L)    10.9\n[4588 rows x 2 columns]**\n\n'
"I have a dataframe and I would like to filter it by multiple values within a single column, how can I accomplish this? when I filter by a singular value I usually use df_filtered = df[df['column'] == value], but that isn't working for the 61 values at least as I've tried it. Thank you. \n\n     MRN  ... Result\n0  13556832  ...  400.0\n1  13556832  ...  400.0\n2  13556832  ...  400.0\n3  13556832  ...  392.0\n4  13556832  ...  400.0\n\n\nhere is a sample of the dataframe (there are about 100k rows, and I need to filter for the 61 MRN values that I have identified for a project. So ultimately I would like to have a separate df that includes all MRN values that I have identified as important. \n\nI am essentially looking for a solution that is similar to the .isin() operator except for 61 values, not 2 max\n"
'I am cleaning some text data from unwanted strings. My text data include # in the first line, and when I save the file it will be gone cause it is not readable by Python. \n\nText example:\n\n@peak,+ID,#val\n1,nopeak\n2,nopeak\n3,peak\n4,nopeak\n\n@category,+ID,#val\n1,high\n2,low\n3,high\n4,high\n\n\nWhat I have done to remove unwanted strings in the lines:\n\ndata1 = np.genfromtxt(\'text.b\', dtype=str, delimiter="\\t")\n\nidxList = [1,2]\n\nfor p,q in enumerate(idxList):\n    OutArr1 = []\n    RemoveStr = str(q)\n    for i,j in enumerate(data1):\n        if j[:4] != RemoveStr: OutArr1.append(str(j))   \n    OutArr2 = np.asarray(OutArr1, dtype=np.str)\n    np.savetxt(\'sample_\' + str(q) + \'.txt\', OutArr2, fmt=\'%s\')\n\n\nCurrent output:\n\n@peak,+ID,\n3,peak\n4,nopeak\n\n@category,+ID,\n3,high\n4,high\n\n\nHow do I manage to keep the hash tag symbol (#val) in every first line?\n'
"\n\nThe picture is what my dataframe looks like. I have user_name, movie_name and time column. I want to extract only rows that are first day of certain movie. For example, if movie a's first date in the time column is 2018-06-27, i want all the rows in that date and if movie b's first date in the time column is 2018-06-12, i only want those rows. How would i do that with pandas?  \n"
'I\'m trying to capture some columns from the following link:\n\nhttps://es.wiktionary.org/wiki/Wikcionario:Frecuentes-(1-1000)-Subt%C3%ADtulos_de_pel%C3%ADculas\n\nThe code I\'ve come up with is as follows:\n\nimport requests\nwiki_url = "https://es.wiktionary.org/wiki/Wikcionario:Frecuentes-(1-1000)-Subt%C3%ADtulos_de_pel%C3%ADculas"\nwiki_texto = requests.get(wiki_url).text\n\nfrom bs4 import BeautifulSoup\nwiki_datos = BeautifulSoup(wiki_texto, "html")\n\nwiki_filas = wiki_datos.findAll("tr")\nprint(wiki_filas[1])\n\nprint("...............................")\n\nwiki_celdas = wiki_datos.findAll("td")\nprint(wiki_celdas[0:])\n\nfila_1 = wiki_celdas[0:]\ninfo_1 = [elemento.get_text() for elemento in fila_1]\nprint(fila_1)\nprint(info_1)\ninfo_1[0] = int(float(info_1[0]))\nprint(info_1)\n\n\nprint("...............................")\n\nnum_or = [int(float(elem.findAll("td")[0].get_text())) for elem in wiki_filas[1:]]\npalabras = [elem.findAll("td")[1].get_text() for elem in wiki_filas[1:]]\nfrecuencia = [elem.findAll("td")[2].get_text() for elem in wiki_filas[1:]]\n\nprint(num_or[0:])\nprint(palabras[0:])\nprint(frecuencia[0:])\n\nfrom pandas import DataFrame\ntabla = DataFrame([num_or, palabras, frecuencia]).T\ntabla.columns = ["Núm. orden", "Palabras", "Frecuencia"]\nprint(tabla.head())\n\n\nThe problem is that I can\'t remove the following /n from colums "Palabras" and "Frecuencia":\n\n\n\nAny ideas? Thanks in advance.\n'
'I have a data frame that looks like this: \n\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n|   | Date                                                                                 | Professional  | Description                                |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n| 0 | 2019-12-19 00:00:00                                                                  | Katie Cool    | Travel to Space ...                        |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n| 1 | 2019-12-20 00:00:00                                                                  | Jenn Blossoms | Review stuff; prepare cancellations of ... |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n| 2 | 2019-12-27 00:00:00                                                                  | Jenn Blossoms | Review lots of stuff/o...                  |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n| 3 | 2019-12-27 00:00:00                                                                  | Jenn Blossoms | Draft email to world leader...             |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n| 4 | 2019-12-30 00:00:00                                                                  | Jenn Blossoms | Review this thing.                         |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n| 5 | 12-30-2019 Jenn Blossoms Telephone   Call   to   A.   Bell   return   her   multiple | NaN           | NaN                                        |\n|   | voicemails.                                                                          |               |                                            |\n+---+--------------------------------------------------------------------------------------+---------------+--------------------------------------------+\n\n\nI would like for it to look like this: \n\n+---+---------------------+---------------+-------------------------------------------------------------+\n|   | Date                | Professional  | Description                                                 |\n+---+---------------------+---------------+-------------------------------------------------------------+\n| 0 | 2019-12-19 00:00:00 | Katie Cool    | Travel to Space ...                                         |\n+---+---------------------+---------------+-------------------------------------------------------------+\n| 1 | 2019-12-20 00:00:00 | Jenn Blossoms | Review stuff; prepare cancellations of ...                  |\n+---+---------------------+---------------+-------------------------------------------------------------+\n| 2 | 2019-12-27 00:00:00 | Jenn Blossoms | Review lots of stuff/o...                                   |\n+---+---------------------+---------------+-------------------------------------------------------------+\n| 3 | 2019-12-27 00:00:00 | Jenn Blossoms | Draft email to world leader...                              |\n+---+---------------------+---------------+-------------------------------------------------------------+\n| 4 | 2019-12-30 00:00:00 | Jenn Blossoms | Review this thing.                                          |\n+---+---------------------+---------------+-------------------------------------------------------------+\n| 5 | 12-30-2019          | Jenn Blossoms | Telephone   Call   to   A.   Bell   return   her   multiple |\n|   |                     |               | voicemails.                                                 |\n+---+---------------------+---------------+-------------------------------------------------------------+\n\n\n@Datanovice provided a great answer when my question was less specific and needed revision. \n\nI have since edited my question and have also tried to edit his code:\n\ns = pd.to_datetime(dftopdata[\'Date\'],errors=\'coerce\').isna() \n# gives us the error rows to filter.\n\n# split out our datetime column so we can extract the values.\ndate_err = (\n    dftopdata[s]["Date"]\n    .str.extract("\\d{2}-\\d{2}-\\d{4}\\s+(\\w+.*)")[0]\n    .str.split("\\s", expand=True)\n)\n\n# set your values with `.loc` \ndftopdata.loc[s,\'Professional\'] = date_err[0] + date_err[1]\ndftopdata.loc[s,\'Description\'] = date_err[2]  \n\n\nBut when I run the above code, I get a data frame that looks like this: \n\n+---+---------------------+---------------+--------------------------------------------+\n|   | Date                | Professional  | Description                                |\n+---+---------------------+---------------+--------------------------------------------+\n| 0 | 2019-12-19 00:00:00 | Katie Cool    | Travel to Space ...                        |\n+---+---------------------+---------------+--------------------------------------------+\n| 1 | 2019-12-20 00:00:00 | Jenn Blossoms | Review stuff; prepare cancellations of ... |\n+---+---------------------+---------------+--------------------------------------------+\n| 2 | 2019-12-27 00:00:00 | Jenn Blossoms | Review lots of stuff/o...                  |\n+---+---------------------+---------------+--------------------------------------------+\n| 3 | 2019-12-27 00:00:00 | Jenn Blossoms | Draft email to world leader...             |\n+---+---------------------+---------------+--------------------------------------------+\n| 4 | 2019-12-30 00:00:00 | Jenn Blossoms | Review this thing.                         |\n+---+---------------------+---------------+--------------------------------------------+\n| 5 | 12-30-2019          | JennBlossoms  |                                            |\n+---+---------------------+---------------+--------------------------------------------+\n\n\nI also get this error: A value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n'
"I have a csv file of 20K Tweets which a column of that is the user's location. the locations are from all regions of the world but only the states of America are important for us. The screenshot of the dataset is the following:\n\n\nHow can I filter this file to only keep rows that their user's location is a state of America, by Python or Tableau Prep? (remove all rows that their locations are not from USA)\n"
"I realize similar problems have been posted on StackOverflow, but this one is a bit different. \nBelow is the df I currently have:\n\n\n\nTo count for each Director which styles are he/she good at, I wanna transform the dataframe. Converting the categorical data to column names and values are the counts. \nThe problem is 'Genre1', 'Genre2', 'Genre3' contains repeat values, and I would like to convert all the unique values in this 3 columns into column_names. \n\nWhat I have tried: I sub-sampled them to 3 dataframes, 'Director'-'Genre1', 'Director'-'Genre2', 'Director'-'Genre3'. For each subset, I used 'pivot()' to transform and it looks like this:\n\n\n\nHowever, I have trouble to merge them together. \n\nThank you!\n"
"data1 = {0: [{'confident': False, 'iab': 'IAB25-3'}],\n 1: [{'confident': False, 'iab': 'IAB6-6'},\n  {'confident': True, 'iab': 'IAB6'}],\n 2: [{'confident': True, 'iab': 'IAB16-1'},\n  {'confident': True, 'iab': 'IAB16'},\n  {'confident': False, 'iab': 'IAB9'},\n  {'confident': False, 'iab': 'IAB9-28'}]}\n\nAbove format was originally the list/json in every row having = [{'confident': False, 'iab': 'IAB25-3'},{'confident': True, 'iab': 'IAB16'}] which is converted into dictionary with the help of to_dict() resulted in the data mentioned in the starting.\nMain problem is that array of collection(confident and iab) can be n times and n is unknown. So, I'm not able to format it.\nI'm trying really hard to convert it into below given dataframe format but haven't succeeded yet.\nrowid   confident    iab\n0       False        IAB25-3\n1       False        IAB6-6\n1       True         IAB6\n2       True         IAB16-1\n2       True         IAB16\n2       False        IAB9\n2       False        IAB9-28\n\nAny help is appreciated .\n"
"I have a dataset of several tables.  Some of the fields overlap but on some tables they may have a one to many relationship while on other tables they may have a one to one relationship.  I am trying to create a new dataframe where I can take the values associated with one field (one to one) and the values associated with that same field but in another table (one to many) and have them all listed on the new dataframe (one to many).\nOne dataframe:\n         finishtId eventId instanceId  ...       value statusId finishType\n0               1     18        1  ...           218.3        1   Positive\n1               2     18        2  ...         217.586        1   Positive\n2               3     18        3  ...         216.719        1   Positive\n3               4     18        4  ...         215.464        1   Positive\n4               5     18        5  ...         218.385        1   Negative\n\nAnother dataframe:\n      eventId  instanceId red blue     time duration  milliseconds\n0        841       153     1    1  17:05:23   26.898         26898\n1        841        30     1    1  17:05:52   25.021         25021\n2        841        17     1   11  17:20:48   23.426         23426\n3        841         4     1   12  17:22:34   23.251         23251\n4        841        13     1   13  17:24:10   23.842         23842\n5.       841.      153.    2   45. 17:45:30.  24.786.        26473\n     ...       ...   ...  ...       ...      ...           ...\n7633    1036       822     2   48  16:20:38   22.143         22143\n7634    1036         1     2   50  16:23:05   21.853         21853\n7635    1036       849     2   49  16:24:00   22.475         22475\n7636    1036       154     2   62  16:42:16   24.010         24010\n7637    1036       822     3   64  16:42:47   22.607         22607\n\nI want to create a new dataframe that appends all of the values from dataframe2 (red, blue, time, duration, milliseconds) to the instanceId and eventId fields so that dataframe1 shows the one to many relationships. Also I want to create a new field that tells me how many many reds per instanceId and eventId (numRed) Basically something like this:\n          eventId instanceId  red  numRed blue  ...     time  duration   value statusId finishType\n0             841    153        1       2   17  ... 17:05:23    26.898   218.3        1   Positive\n1             841    153        2       2   52  ... 17:45:30    24.786 217.586        1   Positive\n1             841    146        1       1   40  ... 17:32:30    24.986 217.586        1   Negative\n\nSo essentially every red, blue, time, duration, value, statusId, and finishType is listed for every instanceId for every eventId.  I'm new to Pandas so I've been digging through functions but I keep getting errors usually associated with data type (float vs str) etc...\nUPDATE: After getting the solution from Edunne I realized that what I think would work better for the dataset is actually something else.  I'd prefer to instead merge down the rows of 'red' values for each 'instanceId' for each 'eventId'.  The values that are different would be averaged, so the mean of the 'duration' and the mean of the 'value' fields.  Something looking like this:\n          eventId instanceId  numRed ...  duration   value statusId finishType\n0             841    153          2  ...     25.842   218.3        1   Positive\n1             841    146          1  ...     24.986 217.586        1   Negative\n\n"
"I dont know why I am getting this, i am trying to skip the rows containing '?' as column values. Example of dataset\nExample of csv file:\n59, Private, 109015, HS-grad, 9, Divorced, Tech-support, Unmarried, White, Female, 0, 0, 40, United-States, &lt;=50K\n56, Local-gov, 216851, Bachelors, 13, Married-civ-spouse, Tech-support, Husband, White, Male, 0, 0, 40, United-States, &gt;50K\n19, Private, 168294, HS-grad, 9, Never-married, Craft-repair, Own-child, White, Male, 0, 0, 40, United-States, &lt;=50K\n54, ?, 180211, Some-college, 10, Married-civ-spouse, ?, Husband, Asian-Pac-Islander, Male, 0, 0, 60, South, &gt;50K\n39, Private, 367260, HS-grad, 9, Divorced, Exec-managerial, Not-in-family, White, Male, 0, 0, 80, United-States, &lt;=50K\n49, Private, 193366, HS-grad, 9, Married-civ-spouse, Craft-repair, Husband, White, Male, 0, 0, 40, United-States, &lt;=50K\n23, Local-gov, 190709, Assoc-acdm, 12, Never-married, Protective-serv, Not-in-family, White, Male, 0, 0, 52, United-States, &lt;=50K\n20, Private, 266015, Some-college, 10, Never-married, Sales, Own-child, Black, Male, 0, 0, 44, United-States, &lt;=50K\n45, Private, 386940, Bachelors, 13, Divorced, Exec-managerial, Own-child, White, Male, 0, 1408, 40, United-States, &lt;=50K\n30, Federal-gov, 59951, Some-college, 10, Married-civ-spouse, Adm-clerical, Own-child, White, Male, 0, 0, 40, United-States, &lt;=50K\n18, Private, 226956, HS-grad, 9, Never-married, Other-service, Own-child, White, Female, 0, 0, 30, ?, &lt;=50K\n\nI am using python and here is my code :\n\n# Load the adult dataset\nimport csv\nf = open(&quot;./adult_data.csv&quot;)\nrecords = csv.reader(f, delimiter = ',')\n\n# We define a header ourselves since the dataset contains only the raw numbers.\ndataset = []\nheader = ['Age', 'Workclass', 'Fnlwgt', 'Education', 'Education-num', 'Marital-status', 'Occupation',\n  'Relationship', 'Race', 'Sex', 'Capital-gain', 'Capital-loss', 'Hours-per-week', 'Native-    country', 'Salary'\n]\n\nfor line in records:\n  question_mark = True\nfor i in range(len(header)):\n  if (line[i] == ' ?'):\n    question_mark = False\nif (question_mark):\n  d = dict(zip(header, line))\nd['Age'] = int(d['Age'])\nd['Fnlwgt'] = int(d['Fnlwgt'])\nd['Education-num'] = int(d['Education-num'])\nd['Capital-gain'] = int(d['Capital-gain'])\nd['Capital-loss'] = int(d['Capital-loss'])\nd['Hours-per-week'] = int(d['Hours-per-week'])\ndataset.append(d)\n\nHere is my Output :\n Output\n ---------------------------------------------------------------------------\n IndexError                                Traceback (most recent call last)\n &lt;ipython-input-6-a6f851085aed&gt; in &lt;module&gt;\n      12     question_mark = True\n      13     for i in range(len(header)):\n ---&gt; 14         if(line[i] == ' ?'):\n      15             question_mark = False\n      16     if(question_mark):\n\n IndexError: list index out of range\n\n"
"I have a column in my dataset, which has the listening time of audible books. The data is stored like\n10 hours and 43 minutes\nHow to extract them and change it into minutes, in a python dataframe?\nI have used\naudiob_adv['time'] = audiob_adv['Listening Time'].str.extract('(\\d\\d)')\nBut this is not working correctly.\nImage of the dataset\n"
"I want to loop through this csv that has country, data, and a number I need to extract. The file  looks like this:\n\nb'/O_o/\\ngoogle.visualization.Query.setResponse({&quot;version&quot;:&quot;0.6&quot;,&quot;reqId&quot;:&quot;0output=csv&quot;,&quot;status&quot;:&quot;ok&quot;,&quot;sig&quot;:&quot;1241529276&quot;,&quot;table&quot;:{&quot;cols&quot;:[{&quot;id&quot;:&quot;A&quot;,&quot;label&quot;:&quot;Entity&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;id&quot;:&quot;B&quot;,&quot;label&quot;:&quot;Week&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;},{&quot;id&quot;:&quot;C&quot;,&quot;label&quot;:&quot;Day&quot;,&quot;type&quot;:&quot;date&quot;,&quot;pattern&quot;:&quot;yyyy-mm-dd&quot;},{&quot;id&quot;:&quot;D&quot;,&quot;label&quot;:&quot;Flights\n2019\n(Reference)&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;},{&quot;id&quot;:&quot;E&quot;,&quot;label&quot;:&quot;Flights&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;},{&quot;id&quot;:&quot;F&quot;,&quot;label&quot;:&quot;%\nvs 2019\n(Daily)&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;},{&quot;id&quot;:&quot;G&quot;,&quot;label&quot;:&quot;Flights\n(7-day moving\naverage)&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;},{&quot;id&quot;:&quot;H&quot;,&quot;label&quot;:&quot;% vs\n2019 (7-day Moving\nAverage)&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;},{&quot;id&quot;:&quot;I&quot;,&quot;label&quot;:&quot;Day\n2019&quot;,&quot;type&quot;:&quot;date&quot;,&quot;pattern&quot;:&quot;yyyy-mm-dd&quot;},{&quot;id&quot;:&quot;J&quot;,&quot;label&quot;:&quot;Day\nPrevious\nYear&quot;,&quot;type&quot;:&quot;date&quot;,&quot;pattern&quot;:&quot;yyyy-mm-dd&quot;},{&quot;id&quot;:&quot;K&quot;,&quot;label&quot;:&quot;Flights\nPrevious\nYear&quot;,&quot;type&quot;:&quot;number&quot;,&quot;pattern&quot;:&quot;General&quot;}],&quot;rows&quot;:[{&quot;c&quot;:[{&quot;v&quot;:&quot;Albania&quot;},{&quot;v&quot;:36.0,&quot;f&quot;:&quot;36&quot;},{&quot;v&quot;:&quot;Date(2020,8,1)&quot;,&quot;f&quot;:&quot;2020-09-01&quot;},{&quot;v&quot;:129.0,&quot;f&quot;:&quot;129&quot;},{&quot;v&quot;:64.0,&quot;f&quot;:&quot;64&quot;},{&quot;v&quot;:-0.503875968992248,&quot;f&quot;:&quot;-0,503875969&quot;},{&quot;v&quot;:71.5714285714286,&quot;f&quot;:&quot;71,57142857&quot;},{&quot;v&quot;:-0.291371994342291,&quot;f&quot;:&quot;-0,2913719943&quot;},{&quot;v&quot;:&quot;Date(2019,8,3)&quot;,&quot;f&quot;:&quot;2019-09-03&quot;},{&quot;v&quot;:&quot;Date(2019,8,3)&quot;,&quot;f&quot;:&quot;2019-09-03&quot;},{&quot;v&quot;:129.0,&quot;f&quot;:&quot;129&quot;}]},{&quot;c&quot;:[{&quot;v&quot;:&quot;Albania&quot;},{&quot;v&quot;:36.0,&quot;f&quot;:&quot;36&quot;},{&quot;v&quot;:&quot;Date(2020,8,2)&quot;,&quot;f&quot;:&quot;2020-09-02&quot;},{&quot;v&quot;:92.0,&quot;f&quot;:&quot;92&quot;},{&quot;v&quot;:59.0,&quot;f&quot;:&quot;59&quot;},{&quot;v&quot;:-0.358695652173913,&quot;f&quot;:&quot;-0,3586956522&quot;},{&quot;v&quot;:70.0,&quot;f&quot;:&quot;70&quot;},{&quot;v&quot;:-0.300998573466476,&quot;f&quot;:&quot;-0,3009985735&quot;},{&quot;v&quot;:&quot;Date(2019,8,4)&quot;,&quot;f&quot;:&quot;2019-09-04&quot;},{&quot;v&quot;:&quot;Date(2019,8,4)&quot;,&quot;f&quot;:&quot;2019-09-04&quot;},{&quot;v&quot;:92.0,&quot;f&quot;:&quot;92&quot;}]},{&quot;c&quot;:[{&quot;v&quot;:&quot;Albania&quot;},{&quot;v&quot;:36.0,&quot;f&quot;:&quot;36&quot;},{&quot;v&quot;:&quot;Date(2020,8,3)&quot;,&quot;f&quot;:&quot;2020-09-03&quot;},{&quot;v&quot;:96.0,&quot;f&quot;:&quot;96&quot;},{&quot;v&quot;:67.0,&quot;f&quot;:&quot;67&quot;},{&quot;v&quot;:-0.302083333333333,&quot;f&quot;:&quot;-0,3020833333&quot;},{&quot;v&quot;:70.1428571428571,&quot;f&quot;:&quot;70,14285714&quot;},{&quot;v&quot;:-0.30354609929078,&quot;f&quot;:&quot;-0,3035460993&quot;},{&quot;v&quot;:&quot;Date(2019,8,5)&quot;,&quot;f&quot;:&quot;2019-09-05&quot;},{&quot;v&quot;:&quot;Date(2019,8,5)&quot;,&quot;f&quot;:&quot;2019-09-05&quot;},{&quot;v&quot;:96.0,&quot;f&quot;:&quot;96&quot;}]},{&quot;c&quot;:[{&quot;v&quot;:&quot;Albania&quot;},{&quot;v&quot;:36.0,&quot;f&quot;:&quot;36&quot;},{&quot;v&quot;:&quot;Date(2020,8,4)&quot;,&quot;f&quot;:&quot;2020-09-04&quot;},{&quot;v&quot;:103.0,&quot;f&quot;:&quot;103&quot;},{&quot;v&quot;:89.0,&quot;f&quot;:&quot;89&quot;},{&quot;v&quot;:-0.135922330097087,&quot;f&quot;:&quot;-0,1359223301&quot;},{&quot;v&quot;:69.2857142857143,&quot;f&quot;:&quot;69,28571429&quot;},{&quot;v&quot;:-0.312056737588652,&quot;f&quot;:&quot;-0,3120567376&quot;},{&quot;v&quot;:&quot;Date(2019,8,6)&quot;,&quot;f&quot;:&quot;2019-09-06&quot;},{&quot;v&quot;:&quot;Date(2019,8,6)&quot;,&quot;f&quot;:&quot;2019-09-06&quot;},{&quot;v&quot;:103.0,&quot;f&quot;:&quot;103&quot;}]},{&quot;c&quot;:[{&quot;v&quot;:&quot;Albania&quot;},{&quot;v&quot;:36.0,&quot;f&quot;:&quot;36&quot;},{&quot;v&quot;:&quot;Date(2020,8,5)&quot;,&quot;f&quot;:&quot;2020-09-05&quot;},{&quot;v&quot;:94.0,&quot;f&quot;:&quot;94&quot;},{&quot;v&quot;:53.0,&quot;f&quot;:&quot;53&quot;},{&quot;v&quot;:-0.436170212765957,&quot;f&quot;:&quot;-0,4361702128&quot;},{&quot;v&quot;:68.8571428571429,&quot;f&quot;:&quot;68,85714286&quot;},{&quot;v&quot;:-0.314366998577525,&quot;f&quot;:&quot;-0,3143669986&quot;},{&quot;v&quot;:&quot;Date(2019,8,7)&quot;,&quot;f&quot;:&quot;2019-09-07&quot;},{&quot;v&quot;:&quot;Date(2019,8,7)&quot;,&quot;f&quot;:&quot;2019-09-07&quot;},{&quot;v&quot;:94.0,&quot;f&quot;:&quot;94&quot;}]}, ...\n\nIn there it says Albania, which is a country with data I need to extract. For example:\n\n{&quot;c&quot;:[{&quot;v&quot;:&quot;Albania&quot;},{&quot;v&quot;:36.0,&quot;f&quot;:&quot;36&quot;},{&quot;v&quot;:&quot;Date(2020,8,4)&quot;,&quot;f&quot;:&quot;2020-09-04&quot;},{&quot;v&quot;:103.0,&quot;f&quot;:&quot;103&quot;},{&quot;v&quot;:89.0,&quot;f&quot;:&quot;89&quot;},{&quot;v&quot;:-0.135922330097087,&quot;f&quot;:&quot;-0,1359223301&quot;},{&quot;v&quot;:69.2857142857143,&quot;f&quot;:&quot;69,28571429&quot;},{&quot;v&quot;:-0.312056737588652,&quot;f&quot;:&quot;-0,3120567376&quot;},{&quot;v&quot;:&quot;Date(2019,8,6)&quot;,&quot;f&quot;:&quot;2019-09-06&quot;},{&quot;v&quot;:&quot;Date(2019,8,6)&quot;,&quot;f&quot;:&quot;2019-09-06&quot;},{&quot;v&quot;:103.0,&quot;f&quot;:&quot;103&quot;}]}\n\nHow would I write a python script to loop over the entire csv file and find every occurrence of the word &quot;Albania&quot;, save it, then go a little further and get the date &quot;2020-09-04&quot;, and then get the number -0.1359?\n"
'Can we perform replace on python list?\n\nI have that list that is imported from csv file:\n\n\n  [[\'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'], [\'2\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\'], [\'3\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\']]\n\n\nfist index(mark as bold) from list above will became costumer id followed by the item that they buy, from list above we can see costumer 1 purchased item in column 12 that marked by italic.\n\ndesire out put is :\n\n\n  costumer 1 purchased item 12 and item 22  and so it\'s for costumer 2 and\n  3\n\n\nNote that I have try used panda and not work, and I not sure how to use if statement inside the for loop.\n\nAlso, I have used rappid minner and they replaced column by column and they included the [0][0] to be replaced. Is there any other solution beside python? \n\nHere is my code:\n\nimport csv\n\n\ncsvfile = open("tran.csv", \'r\')\nreader = csv.reader(csvfile, delimiter=\',\')\n\nmy_list = list(reader)\n\na = len(my_list[1])\nb = (my_list)\n\n\nx=0\ny=0\n\nfor name in my_list:\n   print ("costummer", my_list[0][0], "buy", my_list[n][g])\n\n\nupdate for csv writer:\n\ncsvdict = {words[0]:words[1:] for words in csv}\nfor x in csvdict.keys(): # iterate over the keys \'1\', \'2\', ....\n    products = [index+1 for index,v in enumerate(csvdict[x]) if v == \'1\' ] # create list of purchases where the \'1\'s are indexed\n\n    f = open("trans.csv", \'w\')\n    result = ("costummer", x, "buy", products) \n    resultstr = (\',\'.join([str(x) for x in hasil]))#I try to convert it to str.\n    print (resultstr) #to check whether the conversion from list into str is working or not.\n    f.write(resultstr) #try to write to csv file\n\n'
'I have a problem in data cleansing for text analysis. Now I have done four Regex on my data and still unwanted words are there. I wanted to know if there\'s a way to select just words!\nI know that Scikit-learn has this option but I\'m not working on English text.\nThis is what I entered for each of the above but I was wondering if there\'s another way to do so\n\ndef clean(data):\n    e = re.compile(r"\\b[a-zA-Z]\\b") #single letters\n    data = e.sub(\'\', data)\n    r = re.compile(r\'&lt;[^&lt;]*?&gt;\') # html tags\n    data = r.sub(\'\', data)\n    p = re.compile(r\'[^-\\w]\') # remove characters like \\n\n    data = p.sub(\' \', data)\n    q = re.compile(r\'[\\d_\\.]*\') # remove numbers\n    return q.sub(\'\', data)\n\n'
"I'm trying to extract contact information from a client's digital notebook which has entries like this:\n\n\n  '\\r\\nContact Imported:\\r\\nBusinessPhone : 9547711900 Line1 : 2440\n  East Commercial Blvd.\\r\\n   City : Ft. Lauderdale\\r\\n   State : FL\\r\\n\n  PostalCode : 33308\\r\\n\\r\\nArt Womack recommends Steve Paul Dentist on\n  Commercial Blvd\n  area.\\r\\nA_womack@me.com>\\r\\nBond? Crowns? Veneer?\\r\\n\\r\\n\\r\\n'\n\n\nMy objective after the splitting is to have a list of elements containing relevant data (mostly which contais a ' : ' at the middle) so I can later convert it to a python dictionary.\n\nI've already tried breaking the string down by the '\\r' and '\\r' characters but I keep missing the Line1: yadayada information.\n\nI wanted something like:\n\n['BusinessPhone : 9547711900','BusinessPhone : 9547711900',\n'Line1 : 2440 East Commercial Blvd.', 'City : Ft. Lauderdale',\n 'State : FL', 'PostalCode : 3330']\n\n"
'I have a problem with reading some massive text-files. \n\nI firstly define reading my text file as follows:\n\ndef reader(filename):\n    with open(filename, encoding=\'latin-1\') as thefile:\n        contentsofthefile = f.read()\n    return contentsofthefile\n\n\nNow I want to have another function, that uses the above function, such as:\n\ndef remover(filename):\n    a = reader(filename)\n    for line in a:\n        do this \n\n\nThis yields the following problem: \n\nOSError: [Errno 63] File name too long: \'In search of lost time  - CHAPTER///1 \\nThe characters, plotlines,  ...."\n\n\nIt seems that it attempts to read the entire file as the filename? \n'
'I have a csv file that contains values that are badly written.\nI want to correct these mistakes. for example replace Toyouta by toyota, maxda by mazda, in the column named carCompany. .\n\nThe job I need to do is to predict the car price using these independent variables \n'
'Pandas:I have a dataframe given below which contains the same set of banks twice..I need to slice the data from 0th index that contains a bank name, upto the index that contains the same bank name..here in the problem -DEUTSCH BANK AG..I need to apply same logic to any such kind of dataframes.ty..\n\nI tried with logic:-  df25.iloc[0,1]==df25[1].any().. but it returns nly true but not the index position.\n\nDataFrame:-[1]:https://i.stack.imgur.com/iJ1hJ.png, https://i.stack.imgur.com/J2aDX.png\n'
'I want to iterate through a Result set, clean each item and then append to a list. I have a problem here: \n\nWhen trying to append the list I get an error, namely \'AttributeError: \'NoneType\' object has no attribute \'append\'. I tried to overcome with if pass, but I didnt work. Any ideas? \n\nHere is the input: \n\nI_Details = \n[\'27Dec2017\']\n[\'04Jan2018\']\n[\'22,000,000USD(fiat)\']\n[\'20,000,000USD\']\n[\'China,Japan,UnitedStates\']\n[\'ZIL\']\n[\'Utility-token\']\n[\'No\']\n[ETH,,]\n[40%-MiningRewards,,30%-Company,Team,Agencies,,30%-Early&amp;amp;CommunityContributors]\n[(ZIL)]\n[ETH,,,\'return11.89x\']\n\n\nHere is my attempt: \n\nCampaign_info_1 = list()\nfor Detail in I_Details:\n  Campaign_info = Detail.contents\n  Campaign_info = str(Campaign_info)\n  if Campaign_info==None or Campaign_info=="": \n    pass \n  Campaign_info_1 = Campaign_info_1.append(Campaign_info) \n  print(Campaign_info)\n\n'
"              DateTime  Junction  Vehicles           ID\n0  2015-11-01 00:00:00         1        15  20151101001\n1  2015-11-01 01:00:00         1        13  20151101011\n2  2015-11-01 02:00:00         1        10  20151101021\n3  2015-11-01 03:00:00         1         7  20151101031\n4  2015-11-01 04:00:00         1         9  20151101041\n5  2015-11-01 05:00:00         1         6  20151101051\n6  2015-11-01 06:00:00         1         9  20151101061\n7  2015-11-01 07:00:00         1         8  20151101071\n8  2015-11-01 08:00:00         1        11  20151101081\n9  2015-11-01 09:00:00         1        12  20151101091\n\n\nI want to split the ID column into two separate columns such that the first 4 digits are in one, and the remaining digits are in the second. \n\nCode I've tried:\n\nnew_ID = data.apply(lambda x: x.rsplit(4))\n\n\nBut it doesn't work. How can I do this with pandas?\n"
'I have used the twitter streaming API to gather a large number of tweets in python.\n\nAll tweets are dumped in a txt file.\n\n{"created_at":"Sun Jul 03 15:23:11 +0000 2016","id":749624538015621120,"id_str":"749624538015621120","text":"Et hop un petit muldo dor\\u00e9-indigo #enroutepourlaG2","source":"\\u003ca href=\\"http:\\/\\/twitter.com\\" rel=\\"nofollow\\"\\u003eTwitter Web Client\\u003c\\/a\\u003e","truncated":false,"in_reply_to_status_id":null,"in_reply_to_status_id_str":null,"in_reply_to_user_id":null,"in_reply_to_user_id_str":null,"in_reply_to_screen_name":null,"user":{"id":3050686557,"id_str":"3050686557","name":"Heresia","screen_name":"Air_Et_Zia","location":null,"url":null,"description":"Joueur de Dofus depuis 6 ans. Essentiellement ax\\u00e9 PvP. Actuellement sur #Amayiro !","protected":false,"verified":false,"followers_count":296,"friends_count":30,"listed_count":0,"favourites_count":23,"statuses_count":216,"created_at":"Sat Feb 21 20:45:02 +0000 2015","utc_offset":null,"time_zone":null,"geo_enabled":false,"lang":"fr","contributors_enabled":false,"is_translator":false,"profile_background_color":"000000","profile_background_image_url":"http:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png","profile_background_image_url_https":"https:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png","profile_background_tile":false,"profile_link_color":"9266CC","profile_sidebar_border_color":"000000","profile_sidebar_fill_color":"000000","profile_text_color":"000000","profile_use_background_image":false,"profile_image_url":"http:\\/\\/pbs.twimg.com\\/profile_images\\/569237837581545472\\/e_OJaGOl_normal.png","profile_image_url_https":"https:\\/\\/pbs.twimg.com\\/profile_images\\/569237837581545472\\/e_OJaGOl_normal.png","default_profile":false,"default_profile_image":false,"following":null,"follow_request_sent":null,"notifications":null},"geo":null,"coordinates":null,"place":null,"contributors":null,"is_quote_status":false,"retweet_count":0,"favorite_count":0,"entities":{"hashtags":[{"text":"enroutepourlaG2","indices":[34,50]}],"urls":[],"user_mentions":[],"symbols":[]},"favorited":false,"retweeted":false,"filter_level":"low","lang":"fr","timestamp_ms":"1467559391870"}\n\n\n\nHow do I read individual tweet text of hundreds of tweets? \nCan I save tweets with a particular tag into another text file?For example tweets which contain the word "Indigo" will be stored in another text file.\n\n\nOnly solution I can think of is using Regular Expressions. Are there any better solutions in python?\n'
