"# -------------------------\n# -----  Toy Context  -----\n# -------------------------\nimport tensorflow as tf\n\n\nclass Net(tf.keras.Model):\n    &quot;&quot;&quot;A simple linear model.&quot;&quot;&quot;\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.l1 = tf.keras.layers.Dense(5)\n\n    def call(self, x):\n        return self.l1(x)\n\n\ndef toy_dataset():\n    inputs = tf.range(10.0)[:, None]\n    labels = inputs * 5.0 + tf.range(5.0)[None, :]\n    return (\n        tf.data.Dataset.from_tensor_slices(dict(x=inputs, y=labels)).repeat().batch(2)\n    )\n\n\ndef train_step(net, example, optimizer):\n    &quot;&quot;&quot;Trains `net` on `example` using `optimizer`.&quot;&quot;&quot;\n    with tf.GradientTape() as tape:\n        output = net(example[&quot;x&quot;])\n        loss = tf.reduce_mean(tf.abs(output - example[&quot;y&quot;]))\n    variables = net.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss\n\n\n# ----------------------------\n# -----  Create Objects  -----\n# ----------------------------\n\nnet = Net()\nopt = tf.keras.optimizers.Adam(0.1)\ndataset = toy_dataset()\niterator = iter(dataset)\nckpt = tf.train.Checkpoint(\n    step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator\n)\nmanager = tf.train.CheckpointManager(ckpt, &quot;./tf_ckpts&quot;, max_to_keep=3)\n\n# ----------------------------\n# -----  Train and Save  -----\n# ----------------------------\n\nckpt.restore(manager.latest_checkpoint)\nif manager.latest_checkpoint:\n    print(&quot;Restored from {}&quot;.format(manager.latest_checkpoint))\nelse:\n    print(&quot;Initializing from scratch.&quot;)\n\nfor _ in range(50):\n    example = next(iterator)\n    loss = train_step(net, example, opt)\n    ckpt.step.assign_add(1)\n    if int(ckpt.step) % 10 == 0:\n        save_path = manager.save()\n        print(&quot;Saved checkpoint for step {}: {}&quot;.format(int(ckpt.step), save_path))\n        print(&quot;loss {:1.2f}&quot;.format(loss.numpy()))\n\n\n# ---------------------\n# -----  Restore  -----\n# ---------------------\n\n# In another script, re-initialize objects\nopt = tf.keras.optimizers.Adam(0.1)\nnet = Net()\ndataset = toy_dataset()\niterator = iter(dataset)\nckpt = tf.train.Checkpoint(\n    step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator\n)\nmanager = tf.train.CheckpointManager(ckpt, &quot;./tf_ckpts&quot;, max_to_keep=3)\n\n# Re-use the manager code above ^\n\nckpt.restore(manager.latest_checkpoint)\nif manager.latest_checkpoint:\n    print(&quot;Restored from {}&quot;.format(manager.latest_checkpoint))\nelse:\n    print(&quot;Initializing from scratch.&quot;)\n\nfor _ in range(50):\n    example = next(iterator)\n    # Continue training or evaluate etc.\n\n\n# Create some variables.\nv1 = tf.get_variable(&quot;v1&quot;, shape=[3], initializer = tf.zeros_initializer)\nv2 = tf.get_variable(&quot;v2&quot;, shape=[5], initializer = tf.zeros_initializer)\n\ninc_v1 = v1.assign(v1+1)\ndec_v2 = v2.assign(v2-1)\n\n# Add an op to initialize the variables.\ninit_op = tf.global_variables_initializer()\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, initialize the variables, do some work, and save the\n# variables to disk.\nwith tf.Session() as sess:\n  sess.run(init_op)\n  # Do some work with the model.\n  inc_v1.op.run()\n  dec_v2.op.run()\n  # Save the variables to disk.\n  save_path = saver.save(sess, &quot;/tmp/model.ckpt&quot;)\n  print(&quot;Model saved in path: %s&quot; % save_path)\n\ntf.reset_default_graph()\n\n# Create some variables.\nv1 = tf.get_variable(&quot;v1&quot;, shape=[3])\nv2 = tf.get_variable(&quot;v2&quot;, shape=[5])\n\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, use the saver to restore variables from disk, and\n# do some work with the model.\nwith tf.Session() as sess:\n  # Restore variables from disk.\n  saver.restore(sess, &quot;/tmp/model.ckpt&quot;)\n  print(&quot;Model restored.&quot;)\n  # Check the values of the variables\n  print(&quot;v1 : %s&quot; % v1.eval())\n  print(&quot;v2 : %s&quot; % v2.eval())\n\nimport tensorflow as tf\nfrom tensorflow.saved_model import tag_constants\n\nwith tf.Graph().as_default():\n    with tf.Session() as sess:\n        ...\n\n        # Saving\n        inputs = {\n            &quot;batch_size_placeholder&quot;: batch_size_placeholder,\n            &quot;features_placeholder&quot;: features_placeholder,\n            &quot;labels_placeholder&quot;: labels_placeholder,\n        }\n        outputs = {&quot;prediction&quot;: model_output}\n        tf.saved_model.simple_save(\n            sess, 'path/to/your/location/', inputs, outputs\n        )\n\ngraph = tf.Graph()\nwith restored_graph.as_default():\n    with tf.Session() as sess:\n        tf.saved_model.loader.load(\n            sess,\n            [tag_constants.SERVING],\n            'path/to/your/location/',\n        )\n        batch_size_placeholder = graph.get_tensor_by_name('batch_size_placeholder:0')\n        features_placeholder = graph.get_tensor_by_name('features_placeholder:0')\n        labels_placeholder = graph.get_tensor_by_name('labels_placeholder:0')\n        prediction = restored_graph.get_tensor_by_name('dense/BiasAdd:0')\n\n        sess.run(prediction, feed_dict={\n            batch_size_placeholder: some_value,\n            features_placeholder: some_other_value,\n            labels_placeholder: another_value\n        })\n\nimport os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.saved_model import tag_constants\n\n\ndef model(graph, input_tensor):\n    &quot;&quot;&quot;Create the model which consists of\n    a bidirectional rnn (GRU(10)) followed by a dense classifier\n\n    Args:\n        graph (tf.Graph): Tensors' graph\n        input_tensor (tf.Tensor): Tensor fed as input to the model\n\n    Returns:\n        tf.Tensor: the model's output layer Tensor\n    &quot;&quot;&quot;\n    cell = tf.nn.rnn_cell.GRUCell(10)\n    with graph.as_default():\n        ((fw_outputs, bw_outputs), (fw_state, bw_state)) = tf.nn.bidirectional_dynamic_rnn(\n            cell_fw=cell,\n            cell_bw=cell,\n            inputs=input_tensor,\n            sequence_length=[10] * 32,\n            dtype=tf.float32,\n            swap_memory=True,\n            scope=None)\n        outputs = tf.concat((fw_outputs, bw_outputs), 2)\n        mean = tf.reduce_mean(outputs, axis=1)\n        dense = tf.layers.dense(mean, 5, activation=None)\n\n        return dense\n\n\ndef get_opt_op(graph, logits, labels_tensor):\n    &quot;&quot;&quot;Create optimization operation from model's logits and labels\n\n    Args:\n        graph (tf.Graph): Tensors' graph\n        logits (tf.Tensor): The model's output without activation\n        labels_tensor (tf.Tensor): Target labels\n\n    Returns:\n        tf.Operation: the operation performing a stem of Adam optimizer\n    &quot;&quot;&quot;\n    with graph.as_default():\n        with tf.variable_scope('loss'):\n            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n                    logits=logits, labels=labels_tensor, name='xent'),\n                    name=&quot;mean-xent&quot;\n                    )\n        with tf.variable_scope('optimizer'):\n            opt_op = tf.train.AdamOptimizer(1e-2).minimize(loss)\n        return opt_op\n\n\nif __name__ == '__main__':\n    # Set random seed for reproducibility\n    # and create synthetic data\n    np.random.seed(0)\n    features = np.random.randn(64, 10, 30)\n    labels = np.eye(5)[np.random.randint(0, 5, (64,))]\n\n    graph1 = tf.Graph()\n    with graph1.as_default():\n        # Random seed for reproducibility\n        tf.set_random_seed(0)\n        # Placeholders\n        batch_size_ph = tf.placeholder(tf.int64, name='batch_size_ph')\n        features_data_ph = tf.placeholder(tf.float32, [None, None, 30], 'features_data_ph')\n        labels_data_ph = tf.placeholder(tf.int32, [None, 5], 'labels_data_ph')\n        # Dataset\n        dataset = tf.data.Dataset.from_tensor_slices((features_data_ph, labels_data_ph))\n        dataset = dataset.batch(batch_size_ph)\n        iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n        dataset_init_op = iterator.make_initializer(dataset, name='dataset_init')\n        input_tensor, labels_tensor = iterator.get_next()\n\n        # Model\n        logits = model(graph1, input_tensor)\n        # Optimization\n        opt_op = get_opt_op(graph1, logits, labels_tensor)\n\n        with tf.Session(graph=graph1) as sess:\n            # Initialize variables\n            tf.global_variables_initializer().run(session=sess)\n            for epoch in range(3):\n                batch = 0\n                # Initialize dataset (could feed epochs in Dataset.repeat(epochs))\n                sess.run(\n                    dataset_init_op,\n                    feed_dict={\n                        features_data_ph: features,\n                        labels_data_ph: labels,\n                        batch_size_ph: 32\n                    })\n                values = []\n                while True:\n                    try:\n                        if epoch &lt; 2:\n                            # Training\n                            _, value = sess.run([opt_op, logits])\n                            print('Epoch {}, batch {} | Sample value: {}'.format(epoch, batch, value[0]))\n                            batch += 1\n                        else:\n                            # Final inference\n                            values.append(sess.run(logits))\n                            print('Epoch {}, batch {} | Final inference | Sample value: {}'.format(epoch, batch, values[-1][0]))\n                            batch += 1\n                    except tf.errors.OutOfRangeError:\n                        break\n            # Save model state\n            print('\\nSaving...')\n            cwd = os.getcwd()\n            path = os.path.join(cwd, 'simple')\n            shutil.rmtree(path, ignore_errors=True)\n            inputs_dict = {\n                &quot;batch_size_ph&quot;: batch_size_ph,\n                &quot;features_data_ph&quot;: features_data_ph,\n                &quot;labels_data_ph&quot;: labels_data_ph\n            }\n            outputs_dict = {\n                &quot;logits&quot;: logits\n            }\n            tf.saved_model.simple_save(\n                sess, path, inputs_dict, outputs_dict\n            )\n            print('Ok')\n    # Restoring\n    graph2 = tf.Graph()\n    with graph2.as_default():\n        with tf.Session(graph=graph2) as sess:\n            # Restore saved values\n            print('\\nRestoring...')\n            tf.saved_model.loader.load(\n                sess,\n                [tag_constants.SERVING],\n                path\n            )\n            print('Ok')\n            # Get restored placeholders\n            labels_data_ph = graph2.get_tensor_by_name('labels_data_ph:0')\n            features_data_ph = graph2.get_tensor_by_name('features_data_ph:0')\n            batch_size_ph = graph2.get_tensor_by_name('batch_size_ph:0')\n            # Get restored model output\n            restored_logits = graph2.get_tensor_by_name('dense/BiasAdd:0')\n            # Get dataset initializing operation\n            dataset_init_op = graph2.get_operation_by_name('dataset_init')\n\n            # Initialize restored dataset\n            sess.run(\n                dataset_init_op,\n                feed_dict={\n                    features_data_ph: features,\n                    labels_data_ph: labels,\n                    batch_size_ph: 32\n                }\n\n            )\n            # Compute inference for both batches in dataset\n            restored_values = []\n            for i in range(2):\n                restored_values.append(sess.run(restored_logits))\n                print('Restored values: ', restored_values[i][0])\n\n    # Check if original inference and restored inference are equal\n    valid = all((v == rv).all() for v, rv in zip(values, restored_values))\n    print('\\nInferences match: ', valid)\n\n$ python3 save_and_restore.py\n\nEpoch 0, batch 0 | Sample value: [-0.13851789 -0.3087595   0.12804556  0.20013677 -0.08229901]\nEpoch 0, batch 1 | Sample value: [-0.00555491 -0.04339041 -0.05111827 -0.2480045  -0.00107776]\nEpoch 1, batch 0 | Sample value: [-0.19321944 -0.2104792  -0.00602257  0.07465433  0.11674127]\nEpoch 1, batch 1 | Sample value: [-0.05275984  0.05981954 -0.15913513 -0.3244143   0.10673307]\nEpoch 2, batch 0 | Final inference | Sample value: [-0.26331693 -0.13013336 -0.12553    -0.04276478  0.2933622 ]\nEpoch 2, batch 1 | Final inference | Sample value: [-0.07730117  0.11119192 -0.20817074 -0.35660955  0.16990358]\n\nSaving...\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:SavedModel written to: b'/some/path/simple/saved_model.pb'\nOk\n\nRestoring...\nINFO:tensorflow:Restoring parameters from b'/some/path/simple/variables/variables'\nOk\nRestored values:  [-0.26331693 -0.13013336 -0.12553    -0.04276478  0.2933622 ]\nRestored values:  [-0.07730117  0.11119192 -0.20817074 -0.35660955  0.16990358]\n\nInferences match:  True\n"
'a = tf.constant(np.array([[.1, .3, .5, .9]]))\nprint s.run(tf.nn.softmax(a))\n[[ 0.16838508  0.205666    0.25120102  0.37474789]]\n\nsm = tf.nn.softmax(x)\nce = cross_entropy(sm)\n'
'&gt;&gt;&gt; a = np.array([1, 0, 3])\n&gt;&gt;&gt; b = np.zeros((a.size, a.max()+1))\n&gt;&gt;&gt; b[np.arange(a.size),a] = 1\n&gt;&gt;&gt; b\narray([[ 0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.]])\n'
'e ^ (x - max(x)) / sum(e^(x - max(x))\n\n= e ^ x / (e ^ max(x) * sum(e ^ x / e ^ max(x)))\n\n= e ^ x / sum(e ^ x)\n'
"import cPickle\n# save the classifier\nwith open('my_dumped_classifier.pkl', 'wb') as fid:\n    cPickle.dump(gnb, fid)    \n\n# load it again\nwith open('my_dumped_classifier.pkl', 'rb') as fid:\n    gnb_loaded = cPickle.load(fid)\n"
'from sklearn.tree import _tree\n\ndef tree_to_code(tree, feature_names):\n    tree_ = tree.tree_\n    feature_name = [\n        feature_names[i] if i != _tree.TREE_UNDEFINED else "undefined!"\n        for i in tree_.feature\n    ]\n    print "def tree({}):".format(", ".join(feature_names))\n\n    def recurse(node, depth):\n        indent = "  " * depth\n        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n            name = feature_name[node]\n            threshold = tree_.threshold[node]\n            print "{}if {} &lt;= {}:".format(indent, name, threshold)\n            recurse(tree_.children_left[node], depth + 1)\n            print "{}else:  # if {} &gt; {}".format(indent, name, threshold)\n            recurse(tree_.children_right[node], depth + 1)\n        else:\n            print "{}return {}".format(indent, tree_.value[node])\n\n    recurse(0, 1)\n\ndef tree(f0):\n  if f0 &lt;= 6.0:\n    if f0 &lt;= 1.5:\n      return [[ 0.]]\n    else:  # if f0 &gt; 1.5\n      if f0 &lt;= 4.5:\n        if f0 &lt;= 3.5:\n          return [[ 3.]]\n        else:  # if f0 &gt; 3.5\n          return [[ 4.]]\n      else:  # if f0 &gt; 4.5\n        return [[ 5.]]\n  else:  # if f0 &gt; 6.0\n    if f0 &lt;= 8.5:\n      if f0 &lt;= 7.5:\n        return [[ 7.]]\n      else:  # if f0 &gt; 7.5\n        return [[ 8.]]\n    else:  # if f0 &gt; 8.5\n      return [[ 9.]]\n'
'h1 = tf.nn.relu(tf.matmul(l1, W1) + b1)\nh2 = ...\n\nsess.run(eval_results)\n'
"    vect = CountVectorizer()\n    tfidf = TfidfTransformer()\n    clf = SGDClassifier()\n\n    vX = vect.fit_transform(Xtrain)\n    tfidfX = tfidf.fit_transform(vX)\n    predicted = clf.fit_predict(tfidfX)\n\n    # Now evaluate all steps on test set\n    vX = vect.fit_transform(Xtest)\n    tfidfX = tfidf.fit_transform(vX)\n    predicted = clf.fit_predict(tfidfX)\n\npipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', SGDClassifier()),\n])\npredicted = pipeline.fit(Xtrain).predict(Xtrain)\n# Now evaluate all steps on test set\npredicted = pipeline.predict(Xtest)\n"
'             precision    recall  f1-score   support\n\n          0       0.65      1.00      0.79        17\n          1       0.57      0.75      0.65        16\n          2       0.33      0.06      0.10        17\navg / total       0.52      0.60      0.51        50\n\nF1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The \ndefault `weighted` averaging is deprecated, and from version 0.18, \nuse of precision, recall or F-score with multiclass or multilabel data  \nor pos_label=None will result in an exception. Please set an explicit \nvalue for `average`, one of (None, \'micro\', \'macro\', \'weighted\', \n\'samples\'). In cross validation use, for instance, \nscoring="f1_weighted" instead of scoring="f1".\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n# We use a utility to generate artificial classification data.\nX, y = make_classification(n_samples=100, n_informative=10, n_classes=3)\nsss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=0)\nfor train_idx, test_idx in sss:\n    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n    svc.fit(X_train, y_train)\n    y_pred = svc.predict(X_test)\n    print(f1_score(y_test, y_pred, average="macro"))\n    print(precision_score(y_test, y_pred, average="macro"))\n    print(recall_score(y_test, y_pred, average="macro"))    \n'
'optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ngvs = optimizer.compute_gradients(cost)\ncapped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\ntrain_op = optimizer.apply_gradients(capped_gvs)\n'
'import os\nos.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152\nos.environ["CUDA_VISIBLE_DEVICES"] = ""\n\n$ CUDA_VISIBLE_DEVICES="" ./your_keras_code.py\n'
'def create_model():\n   model = Sequential()\n   model.add(Dense(64, input_dim=14, init=\'uniform\'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5)) \n   model.add(Dense(64, init=\'uniform\'))\n   model.add(LeakyReLU(alpha=0.3))\n   model.add(BatchNormalization(epsilon=1e-06, mode=0, momentum=0.9, weights=None))\n   model.add(Dropout(0.5))\n   model.add(Dense(2, init=\'uniform\'))\n   model.add(Activation(\'softmax\'))\n   return model\n\ndef train():\n   model = create_model()\n   sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n   model.compile(loss=\'binary_crossentropy\', optimizer=sgd)\n\n   checkpointer = ModelCheckpoint(filepath="/tmp/weights.hdf5", verbose=1, save_best_only=True)\n   model.fit(X_train, y_train, nb_epoch=20, batch_size=16, show_accuracy=True, validation_split=0.2, verbose=2, callbacks=[checkpointer])\n\ndef load_trained_model(weights_path):\n   model = create_model()\n   model.load_weights(weights_path)\n'
'cosine_function = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport numpy.linalg as LA\n\ntrain_set = ["The sky is blue.", "The sun is bright."] #Documents\ntest_set = ["The sun in the sky is bright."] #Query\nstopWords = stopwords.words(\'english\')\n\nvectorizer = CountVectorizer(stop_words = stopWords)\n#print vectorizer\ntransformer = TfidfTransformer()\n#print transformer\n\ntrainVectorizerArray = vectorizer.fit_transform(train_set).toarray()\ntestVectorizerArray = vectorizer.transform(test_set).toarray()\nprint \'Fit Vectorizer to train set\', trainVectorizerArray\nprint \'Transform Vectorizer to test set\', testVectorizerArray\ncx = lambda a, b : round(np.inner(a, b)/(LA.norm(a)*LA.norm(b)), 3)\n\nfor vector in trainVectorizerArray:\n    print vector\n    for testV in testVectorizerArray:\n        print testV\n        cosine = cx(vector, testV)\n        print cosine\n\ntransformer.fit(trainVectorizerArray)\nprint\nprint transformer.transform(trainVectorizerArray).toarray()\n\ntransformer.fit(testVectorizerArray)\nprint \ntfidf = transformer.transform(testVectorizerArray)\nprint tfidf.todense()\n\nFit Vectorizer to train set [[1 0 1 0]\n [0 1 0 1]]\nTransform Vectorizer to test set [[0 1 1 1]]\n[1 0 1 0]\n[0 1 1 1]\n0.408\n[0 1 0 1]\n[0 1 1 1]\n0.816\n\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.70710678  0.          0.70710678]]\n\n[[ 0.          0.57735027  0.57735027  0.57735027]]\n'
'c = np.array([[3.,4], [5.,6], [6.,7]])\nprint(np.mean(c,1))\n\nMean = tf.reduce_mean(c,1)\nwith tf.Session() as sess:\n    result = sess.run(Mean)\n    print(result)\n\n[ 3.5  5.5  6.5]\n[ 3.5  5.5  6.5]\n\nnpMean = np.mean(c)\nprint(npMean+1)\n\ntfMean = tf.reduce_mean(c)\nAdd = tfMean + 1\nwith tf.Session() as sess:\n    result = sess.run(Add)\n    print(result)\n'
'[0.58,0.76]\n\n[[0.58,0.76]]\n'
"from pylab import imshow\nimport numpy as np\nimport mahotas\nwally = mahotas.imread('DepartmentStore.jpg')\n\nwfloat = wally.astype(float)\nr,g,b = wfloat.transpose((2,0,1))\n\nw = wfloat.mean(2)\n\npattern = np.ones((24,16), float)\nfor i in xrange(2):\n    pattern[i::4] = -1\n\nv = mahotas.convolve(r-w, pattern)\n\nmask = (v == v.max())\nmask = mahotas.dilate(mask, np.ones((48,24)))\n\nwally -= .8*wally * ~mask[:,:,None]\nimshow(wally)\n"
'class EarlyStoppingByLossVal(Callback):\n    def __init__(self, monitor=\'val_loss\', value=0.00001, verbose=0):\n        super(Callback, self).__init__()\n        self.monitor = monitor\n        self.value = value\n        self.verbose = verbose\n\n    def on_epoch_end(self, epoch, logs={}):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn("Early stopping requires %s available!" % self.monitor, RuntimeWarning)\n\n        if current &lt; self.value:\n            if self.verbose &gt; 0:\n                print("Epoch %05d: early stopping THR" % epoch)\n            self.model.stop_training = True\n\ncallbacks = [\n    EarlyStoppingByLossVal(monitor=\'val_loss\', value=0.00001, verbose=1),\n    # EarlyStopping(monitor=\'val_loss\', patience=2, verbose=0),\n    ModelCheckpoint(kfold_weights_path, monitor=\'val_loss\', save_best_only=True, verbose=0),\n]\nmodel.fit(X_train.astype(\'float32\'), Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n      shuffle=True, verbose=1, validation_data=(X_valid, Y_valid),\n      callbacks=callbacks)\n'
"import pandas as pd\nimport pylab as pl\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# load dataset\niris = datasets.load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\n\n# normalize data\nfrom sklearn import preprocessing\ndata_scaled = pd.DataFrame(preprocessing.scale(df),columns = df.columns) \n\n# PCA\npca = PCA(n_components=2)\npca.fit_transform(data_scaled)\n\n# Dump components relations with features:\nprint(pd.DataFrame(pca.components_,columns=data_scaled.columns,index = ['PC-1','PC-2']))\n\n      sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\nPC-1           0.522372         -0.263355           0.581254          0.565611\nPC-2          -0.372318         -0.925556          -0.021095         -0.065416\n"
'# minimal dataset\n&gt;&gt;&gt; X = [[1, 0], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1]\n# use empirical prior, learned from y\n&gt;&gt;&gt; MultinomialNB().fit(X,y).predict([1,1])\narray([0])\n# use custom prior to make 1 more likely\n&gt;&gt;&gt; MultinomialNB(class_prior=[.1, .9]).fit(X,y).predict([1,1])\narray([1])\n'
"    with open('/trainHistoryDict', 'wb') as file_pi:\n        pickle.dump(history.history, file_pi)\n"
'def perf_measure(y_actual, y_hat):\n    TP = 0\n    FP = 0\n    TN = 0\n    FN = 0\n\n    for i in range(len(y_hat)): \n        if y_actual[i]==y_hat[i]==1:\n           TP += 1\n        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n           FP += 1\n        if y_actual[i]==y_hat[i]==0:\n           TN += 1\n        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n           FN += 1\n\n    return(TP, FP, TN, FN)\n'
'&gt;&gt;&gt; x = np.random.random((3, 2)) - 0.5\n&gt;&gt;&gt; x\narray([[-0.00590765,  0.18932873],\n       [-0.32396051,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n&gt;&gt;&gt; np.maximum(x, 0)\narray([[ 0.        ,  0.18932873],\n       [ 0.        ,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n&gt;&gt;&gt; x * (x &gt; 0)\narray([[-0.        ,  0.18932873],\n       [-0.        ,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n&gt;&gt;&gt; (abs(x) + x) / 2\narray([[ 0.        ,  0.18932873],\n       [ 0.        ,  0.25586596],\n       [ 0.22358098,  0.02217555]])\n\nimport numpy as np\n\nx = np.random.random((5000, 5000)) - 0.5\nprint("max method:")\n%timeit -n10 np.maximum(x, 0)\n\nprint("multiplication method:")\n%timeit -n10 x * (x &gt; 0)\n\nprint("abs method:")\n%timeit -n10 (abs(x) + x) / 2\n\nmax method:\n10 loops, best of 3: 239 ms per loop\nmultiplication method:\n10 loops, best of 3: 145 ms per loop\nabs method:\n10 loops, best of 3: 288 ms per loop\n'
'num_labels = 10\n\n# label_batch is a tensor of numeric labels to process\n# 0 &lt;= label &lt; num_labels\n\nsparse_labels = tf.reshape(label_batch, [-1, 1])\nderived_size = tf.shape(label_batch)[0]\nindices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\nconcated = tf.concat(1, [indices, sparse_labels])\noutshape = tf.pack([derived_size, num_labels])\nlabels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n'
'def show_most_informative_features(vectorizer, clf, n=20):\n    feature_names = vectorizer.get_feature_names()\n    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n    for (coef_1, fn_1), (coef_2, fn_2) in top:\n        print "\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s" % (coef_1, fn_1, coef_2, fn_2)\n'
'import numpy as np\nimport random\n\n# m denotes the number of examples here, not the number of features\ndef gradientDescent(x, y, theta, alpha, m, numIterations):\n    xTrans = x.transpose()\n    for i in range(0, numIterations):\n        hypothesis = np.dot(x, theta)\n        loss = hypothesis - y\n        # avg cost per example (the 2 in 2*m doesn\'t really matter here.\n        # But to be consistent with the gradient, I include it)\n        cost = np.sum(loss ** 2) / (2 * m)\n        print("Iteration %d | Cost: %f" % (i, cost))\n        # avg gradient per example\n        gradient = np.dot(xTrans, loss) / m\n        # update\n        theta = theta - alpha * gradient\n    return theta\n\n\ndef genData(numPoints, bias, variance):\n    x = np.zeros(shape=(numPoints, 2))\n    y = np.zeros(shape=numPoints)\n    # basically a straight line\n    for i in range(0, numPoints):\n        # bias feature\n        x[i][0] = 1\n        x[i][1] = i\n        # our target variable\n        y[i] = (i + bias) + random.uniform(0, 1) * variance\n    return x, y\n\n# gen 100 points with a bias of 25 and 10 variance as a bit of noise\nx, y = genData(100, 25, 10)\nm, n = np.shape(x)\nnumIterations= 100000\nalpha = 0.0005\ntheta = np.ones(n)\ntheta = gradientDescent(x, y, theta, alpha, m, numIterations)\nprint(theta)\n\nIteration 99997 | Cost: 47883.706462\nIteration 99998 | Cost: 47883.706462\nIteration 99999 | Cost: 47883.706462\n[ 29.25567368   1.01108458]\n'
"idx color\n0   blue\n1   green\n2   green\n3   red\n\nidx blue green red\n0   1    0     0\n1   0    1     0\n2   0    1     0\n3   0    0     1\n\nimport pandas as pd\n\ndata = pd.DataFrame({'color': ['blue', 'green', 'green', 'red']})\nprint(pd.get_dummies(data))\n\n   color_blue  color_green  color_red\n0           1            0          0\n1           0            1          0\n2           0            1          0\n3           0            0          1\n\ndata = pd.DataFrame({'q': ['old', 'new', 'new', 'ren']})\ndata['q'] = data['q'].astype('category')\ndata['q'] = data['q'].cat.reorder_categories(['old', 'ren', 'new'], ordered=True)\ndata['q'] = data['q'].cat.codes\nprint(data['q'])\n\n0    0\n1    2\n2    2\n3    1\nName: q, dtype: int8\n\nprices = pd.DataFrame({\n    'city': ['A', 'A', 'A', 'B', 'B', 'C'],\n    'price': [1, 1, 1, 2, 2, 3],\n})\nmean_price = prices.groupby('city').mean()\ndata = pd.DataFrame({'city': ['A', 'B', 'C', 'A', 'B', 'A']})\n\nprint(data.merge(mean_price, on='city', how='left'))\n\n  city  price\n0    A      1\n1    B      2\n2    C      3\n3    A      1\n4    B      2\n5    A      1\n"
'# your class weights\nclass_weights = tf.constant([[1.0, 2.0, 3.0]])\n# deduce weights for batch samples based on their true label\nweights = tf.reduce_sum(class_weights * onehot_labels, axis=1)\n# compute your (unweighted) softmax cross entropy loss\nunweighted_losses = tf.nn.softmax_cross_entropy_with_logits(onehot_labels, logits)\n# apply the weights, relying on broadcasting of the multiplication\nweighted_losses = unweighted_losses * weights\n# reduce the result to get your final loss\nloss = tf.reduce_mean(weighted_losses)\n\nweights = class_weights\n'
"import pandas as pd\n\ncats = ['a', 'b', 'c']\ndf = pd.DataFrame({'cat': ['a', 'b', 'a']})\n\ndummies = pd.get_dummies(df, prefix='', prefix_sep='')\ndummies = dummies.T.reindex(cats).T.fillna(0)\n\nprint dummies\n\n    a    b    c\n0  1.0  0.0  0.0\n1  0.0  1.0  0.0\n2  1.0  0.0  0.0\n"
'def roc_auc_score(y_true, y_score, average="macro", sample_weight=None):\n    # &lt;...&gt; docstring &lt;...&gt;\n    def _binary_roc_auc_score(y_true, y_score, sample_weight=None):\n            # &lt;...&gt; bla-bla &lt;...&gt;\n\n            fpr, tpr, tresholds = roc_curve(y_true, y_score,\n                                            sample_weight=sample_weight)\n            return auc(fpr, tpr, reorder=True)\n\n    return _average_binary_score(\n        _binary_roc_auc_score, y_true, y_score, average,\n        sample_weight=sample_weight) \n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nest = LogisticRegression(class_weight=\'auto\')\nX = np.random.rand(10, 2)\ny = np.random.randint(2, size=10)\nest.fit(X, y)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y, est.predict(X))\nprint auc(false_positive_rate, true_positive_rate)\n# 0.857142857143\nprint roc_auc_score(y, est.predict(X))\n# 0.857142857143\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y, est.predict_proba(X)[:,1])\n# may differ\nprint auc(false_positive_rate, true_positive_rate)\nprint roc_auc_score(y, est.predict(X))\n'
'import numpy as np    \nfrom sklearn.linear_model import LogisticRegression\n\nx1 = np.random.randn(100)\nx2 = 4*np.random.randn(100)\nx3 = 0.5*np.random.randn(100)\ny = (3 + x1 + x2 + x3 + 0.2*np.random.randn()) &gt; 0\nX = np.column_stack([x1, x2, x3])\n\nm = LogisticRegression()\nm.fit(X, y)\n\n# The estimated coefficients will all be around 1:\nprint(m.coef_)\n\n# Those values, however, will show that the second parameter\n# is more influential\nprint(np.std(X, 0)*m.coef_)\n\nm.fit(X / np.std(X, 0), y)\nprint(m.coef_)\n'
"from __future__ import print_function\n\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\n\n\nX_train = [[0, 0, np.nan], [np.nan, 1, 1]]\nY_train = [0, 1]\nX_test_1 = [0, 0, np.nan]\nX_test_2 = [0, np.nan, np.nan]\nX_test_3 = [np.nan, 1, 1]\n\n# Create our imputer to replace missing values with the mean e.g.\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp = imp.fit(X_train)\n\n# Impute our data, then train\nX_train_imp = imp.transform(X_train)\nclf = RandomForestClassifier(n_estimators=10)\nclf = clf.fit(X_train_imp, Y_train)\n\nfor X_test in [X_test_1, X_test_2, X_test_3]:\n    # Impute each test item, then predict\n    X_test_imp = imp.transform(X_test)\n    print(X_test, '-&gt;', clf.predict(X_test_imp))\n\n# Results\n[0, 0, nan] -&gt; [0]\n[0, nan, nan] -&gt; [0]\n[nan, 1, 1] -&gt; [1]\n"
"DF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value)\n    DF = DF.append(SR_row,ignore_index=True)\n\nIn [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([[1,2],[3,4]],columns=['A','B'])\n\nIn [3]: df\nOut[3]:\n   A  B\n0  1  2\n1  3  4\n\nIn [5]: s = pd.Series([5,6],index=['A','B'])\n\nIn [6]: s\nOut[6]:\nA    5\nB    6\ndtype: int64\n\nIn [36]: df.append(s,ignore_index=True)\nOut[36]:\n   A  B\n0  1  2\n1  3  4\n2  5  6\n\nDF = DF.append(SR_row,ignore_index=True)\n\nDF = DataFrame()\nfor sample,data in D_sample_data.items():\n    SR_row = pd.Series(data.D_key_value,name=sample)\n    DF = DF.append(SR_row)\nDF.head()\n"
'model.fit(X, y, nb_epoch=40, batch_size=32, validation_split=0.2, verbose=1)\n\nEpoch 1/100\n0s - loss: 0.2506 - acc: 0.5750 - val_loss: 0.2501 - val_acc: 0.3750\nEpoch 2/100\n0s - loss: 0.2487 - acc: 0.6250 - val_loss: 0.2498 - val_acc: 0.6250\nEpoch 3/100\n0s - loss: 0.2495 - acc: 0.5750 - val_loss: 0.2496 - val_acc: 0.6250\n.....\n.....\n\nfrom time import sleep\nimport sys\n\nepochs = 10\n\nfor e in range(epochs):\n    sys.stdout.write(\'\\r\')\n\n    for X, y in data.next_batch():\n        model.fit(X, y, nb_epoch=1, batch_size=data.batch_size, verbose=0)\n\n    # print loss and accuracy\n\n    # the exact output you\'re looking for:\n    sys.stdout.write("[%-60s] %d%%" % (\'=\'*(60*(e+1)/10), (100*(e+1)/10)))\n    sys.stdout.flush()\n    sys.stdout.write(", epoch %d"% (e+1))\n    sys.stdout.flush()\n\nout_batch = NBatchLogger(display=1000)\nmodel.fit([X_train_aux,X_train_main],Y_train,batch_size=128,callbacks=[out_batch])\n\nclass NBatchLogger(Callback):\n    def __init__(self, display):\n        self.seen = 0\n        self.display = display\n\n    def on_batch_end(self, batch, logs={}):\n        self.seen += logs.get(\'size\', 0)\n        if self.seen % self.display == 0:\n            metrics_log = \'\'\n            for k in self.params[\'metrics\']:\n                if k in logs:\n                    val = logs[k]\n                    if abs(val) &gt; 1e-3:\n                        metrics_log += \' - %s: %.4f\' % (k, val)\n                    else:\n                        metrics_log += \' - %s: %.4e\' % (k, val)\n            print(\'{}/{} ... {}\'.format(self.seen,\n                                        self.params[\'samples\'],\n                                        metrics_log))\n\nfrom keras.utils import generic_utils\n\nprogbar = generic_utils.Progbar(X_train.shape[0])\n\nfor X_batch, Y_batch in datagen.flow(X_train, Y_train):\n    loss, acc = model_test.train([X_batch]*2, Y_batch, accuracy=True)\n    progbar.add(X_batch.shape[0], values=[("train loss", loss), ("acc", acc)])\n'
"x = v.fit_transform(df['Review'].values.astype('U'))  ## Even astype(str) would work\n"
"&gt;&gt;&gt; clf.classes_\narray(['one', 'three', 'two'], \n      dtype='|S5')\n"
'model.compile(loss = "categorical_crossentropy", optimizer = "adam")\n\nfrom keras.optimizers import SGD\nopt = SGD(lr=0.01)\nmodel.compile(loss = "categorical_crossentropy", optimizer = opt)\n'
'history_callback = model.fit(params...)\nloss_history = history_callback.history["loss"]\n\nimport numpy\nnumpy_loss_history = numpy.array(loss_history)\nnumpy.savetxt("loss_history.txt", numpy_loss_history, delimiter=",")\n'
"import scipy\nimport pylab\nimport scipy.cluster.hierarchy as sch\nfrom scipy.spatial.distance import squareform\n\n\n# Generate random features and distance matrix.\nx = scipy.rand(40)\nD = scipy.zeros([40,40])\nfor i in range(40):\n    for j in range(40):\n        D[i,j] = abs(x[i] - x[j])\n\ncondensedD = squareform(D)\n\n# Compute and plot first dendrogram.\nfig = pylab.figure(figsize=(8,8))\nax1 = fig.add_axes([0.09,0.1,0.2,0.6])\nY = sch.linkage(condensedD, method='centroid')\nZ1 = sch.dendrogram(Y, orientation='left')\nax1.set_xticks([])\nax1.set_yticks([])\n\n# Compute and plot second dendrogram.\nax2 = fig.add_axes([0.3,0.71,0.6,0.2])\nY = sch.linkage(condensedD, method='single')\nZ2 = sch.dendrogram(Y)\nax2.set_xticks([])\nax2.set_yticks([])\n\n# Plot distance matrix.\naxmatrix = fig.add_axes([0.3,0.1,0.6,0.6])\nidx1 = Z1['leaves']\nidx2 = Z2['leaves']\nD = D[idx1,:]\nD = D[:,idx2]\nim = axmatrix.matshow(D, aspect='auto', origin='lower', cmap=pylab.cm.YlGnBu)\naxmatrix.set_xticks([])\naxmatrix.set_yticks([])\n\n# Plot colorbar.\naxcolor = fig.add_axes([0.91,0.1,0.02,0.6])\npylab.colorbar(im, cax=axcolor)\nfig.show()\nfig.savefig('dendrogram.png')\n"
'parameters = []\nfor i in range(0, number_of_attributes, 1):\n    parameters.append(tf.Variable(\n        initial_parameters_of_hypothesis_function[i].initialized_value()))\n'
'model = MyModel()\n\nif torch.cuda.is_available():\n    model.cuda()\n'
"import nltk\nfrom nltk.collocations import *\nbigram_measures = nltk.collocations.BigramAssocMeasures()\n\n# change this to read in your data\nfinder = BigramCollocationFinder.from_words(\n   nltk.corpus.genesis.words('english-web.txt'))\n\n# only bigrams that appear 3+ times\nfinder.apply_freq_filter(3) \n\n# return the 5 n-grams with the highest PMI\nfinder.nbest(bigram_measures.pmi, 5)  \n"
"import pickle\nf = open('my_classifier.pickle', 'wb')\npickle.dump(classifier, f)\nf.close()\n\nimport pickle\nf = open('my_classifier.pickle', 'rb')\nclassifier = pickle.load(f)\nf.close()\n"
'$ cmake\nUsage\n\ncmake [options] &lt;path-to-source&gt;\ncmake [options] &lt;path-to-existing-build&gt;\n...\n\n$ git clone https://github.com/dmlc/xgboost xgboost_dir\n\n$ cd xgboost_dir\n$ mkdir build\n$ cd build\n$ cmake .. -G"Visual Studio 12 2013 Win64"\n\n$ python -c "import xgboost"\n'
"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(data, labels, validation_split=0.2)\n"
'graph = pydot.graph_from_dot_data(dot_data.getvalue())\ngraph[0].write_pdf("iris.pdf") \n'
'from io import StringIO\nout = StringIO()\nout = tree.export_graphviz(clf, out_file=out)\n\nclf.tree_.children_left #array of left children\nclf.tree_.children_right #array of right children\nclf.tree_.feature #array of nodes splitting feature\nclf.tree_.threshold #array of nodes splitting points\nclf.tree_.value #array of nodes values\n\nfrom inspect import getmembers\nprint( getmembers( clf.tree_ ) )\n'
"&gt;&gt;&gt; cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=['hot', 'cold', 'old'])\n&gt;&gt;&gt; cv.fit_transform(['pease porridge hot', 'pease porridge cold', 'pease porridge in the pot', 'nine days old']).toarray()\narray([[1, 0, 0],\n       [0, 1, 0],\n       [0, 0, 0],\n       [0, 0, 1]], dtype=int64)\n\nnewVec = CountVectorizer(vocabulary=vec.vocabulary_)\n"
"from sklearn.linear_model import SGDClassifier\nimport random\nclf2 = SGDClassifier(loss='log') # shuffle=True is useless here\nshuffledRange = range(len(X))\nn_iter = 5\nfor n in range(n_iter):\n    random.shuffle(shuffledRange)\n    shuffledX = [X[i] for i in shuffledRange]\n    shuffledY = [Y[i] for i in shuffledRange]\n    for batch in batches(range(len(shuffledX)), 10000):\n        clf2.partial_fit(shuffledX[batch[0]:batch[-1]+1], shuffledY[batch[0]:batch[-1]+1], classes=numpy.unique(Y))\n"
'cg = sns.clustermap(df, metric="correlation")\nplt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n\ncg = sns.clustermap(df, metric="correlation")\ncg.dendrogram_col.linkage # linkage matrix for columns\ncg.dendrogram_row.linkage # linkage matrix for rows\n'
"&gt;&gt;&gt; # import NumPy and the relevant scikits.learn module\n&gt;&gt;&gt; import numpy as NP\n&gt;&gt;&gt; from sklearn import neighbors as kNN\n\n&gt;&gt;&gt; # load one of the sklearn-suppplied data sets\n&gt;&gt;&gt; from sklearn import datasets\n&gt;&gt;&gt; iris = datasets.load_iris()\n&gt;&gt;&gt; # the call to load_iris() loaded both the data and the class labels, so\n&gt;&gt;&gt; # bind each to its own variable\n&gt;&gt;&gt; data = iris.data\n&gt;&gt;&gt; class_labels = iris.target\n\n&gt;&gt;&gt; # construct a classifier-builder by instantiating the kNN module's primary class\n&gt;&gt;&gt; kNN1 = kNN.NeighborsClassifier()\n\n&gt;&gt;&gt; # now construct ('train') the classifier by passing the data and class labels\n&gt;&gt;&gt; # to the classifier-builder\n&gt;&gt;&gt; kNN1.fit(data, class_labels)\n      NeighborsClassifier(n_neighbors=5, leaf_size=20, algorithm='auto')\n"
"# could use: import pickle... however let's do something else\nfrom sklearn.externals import joblib \n\n# this is more efficient than pickle for things like large numpy arrays\n# ... which sklearn models often have.   \n\n# then just 'dump' your file\njoblib.dump(clf, 'my_dope_model.pkl') \n"
"from keras.layers import *\nfrom keras.models import Model\n\ninpE = Input((10,5)) #here, you don't define the batch size   \noutE = LSTM(units = 20, return_sequences=False, ...optional parameters...)(inpE)\n\nencoder = Model(inpE,outE)   \n\ninpD = Input((20,))   \noutD = Reshape((10,2))(inpD) #supposing 10 steps of 2 elements    \n\noutD1 = LSTM(5,return_sequences=True,...optional parameters...)(outD)    \n#5 cells because we want a (None,10,5) vector.   \n\nalternativeOut = LSTM(50,return_sequences=False,...)(outD)    \nalternativeOut = Reshape((10,5))(alternativeOut)\n\ndecoder = Model(inpD,outD1)  \nalternativeDecoder = Model(inpD,alternativeOut)   \n\nencoderPredictions = encoder.predict(data)\n"
"import xgboost as xgb\nfrom sklearn.cross_validation import train_test_split as ttsplit\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_squared_error as mse\n\nX = load_boston()['data']\ny = load_boston()['target']\n\n# split data into training and testing sets\n# then split training set in half\nX_train, X_test, y_train, y_test = ttsplit(X, y, test_size=0.1, random_state=0)\nX_train_1, X_train_2, y_train_1, y_train_2 = ttsplit(X_train, \n                                                     y_train, \n                                                     test_size=0.5,\n                                                     random_state=0)\n\nxg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\nxg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\nxg_test = xgb.DMatrix(X_test, label=y_test)\n\nparams = {'objective': 'reg:linear', 'verbose': False}\nmodel_1 = xgb.train(params, xg_train_1, 30)\nmodel_1.save_model('model_1.model')\n\n# ================= train two versions of the model =====================#\nmodel_2_v1 = xgb.train(params, xg_train_2, 30)\nmodel_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model='model_1.model')\n\nprint(mse(model_1.predict(xg_test), y_test))     # benchmark\nprint(mse(model_2_v1.predict(xg_test), y_test))  # &quot;before&quot;\nprint(mse(model_2_v2.predict(xg_test), y_test))  # &quot;after&quot;\n\n# 23.0475232194\n# 39.6776876084\n# 27.2053239482\n"
'import keras.backend as K\ndef dice_coef(y_true, y_pred, smooth, thresh):\n    y_pred = y_pred &gt; thresh\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(smooth, thresh):\n  def dice(y_true, y_pred)\n    return -dice_coef(y_true, y_pred, smooth, thresh)\n  return dice\n\n# build model \nmodel = my_model()\n# get the loss function\nmodel_dice = dice_loss(smooth=1e-5, thresh=0.5)\n# compile model\nmodel.compile(loss=model_dice)\n'
"from keras.layers import LeakyReLU\n\n# instead of cnn_model.add(Activation('relu'))\n# use\ncnn_model.add(LeakyReLU(alpha=0.1))\n"
'print(grayscale_batch.shape)  # (64, 224, 224)\nrgb_batch = np.repeat(grayscale_batch[..., np.newaxis], 3, -1)\nprint(rgb_batch.shape)  # (64, 224, 224, 3)\n'
'clip_grad_value_(model.parameters(), clip_value)\n\nfor p in model.parameters():\n    p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))\n'
'from sklearn.datasets import load_iris\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import f1_score\n\niris = load_iris()\n\nmodel_to_set = OneVsRestClassifier(SVC(kernel="poly"))\n\nparameters = {\n    "estimator__C": [1,2,4,8],\n    "estimator__kernel": ["poly","rbf"],\n    "estimator__degree":[1, 2, 3, 4],\n}\n\nmodel_tunning = GridSearchCV(model_to_set, param_grid=parameters,\n                             score_func=f1_score)\n\nmodel_tunning.fit(iris.data, iris.target)\n\nprint model_tunning.best_score_\nprint model_tunning.best_params_\n\n0.973290762737\n{\'estimator__kernel\': \'poly\', \'estimator__C\': 1, \'estimator__degree\': 2}\n'
'import pandas, numpy\newma = pandas.stats.moments.ewma\nEMOV_n = ewma( ys, com=2 )\n\nXs = numpy.vstack((Xs,EMOV_n))\n\nfrom sklearn import linear_model\nclf = linear_model.LinearRegression()\nclf.fit ( Xs, ys )\nprint clf.coef_\n'
'x = torch.zeros(1, 3, 224, 224, dtype=torch.float, requires_grad=False)\nout = resnet(x)\nmake_dot(out)  # plot graph of variable, not of a nn.Module\n'
'probability: boolean, optional (default=False) \n'
'TP = tf.count_nonzero(predicted * actual)\nTN = tf.count_nonzero((predicted - 1) * (actual - 1))\nFP = tf.count_nonzero(predicted * (actual - 1))\nFN = tf.count_nonzero((predicted - 1) * actual)\n\nprecision = TP / (TP + FP)\nrecall = TP / (TP + FN)\nf1 = 2 * precision * recall / (precision + recall)\n'
'x = np.zeros((12, 12, 3))\nx.shape\n#yields: \n(12, 12, 3)\n\nx = np.moveaxis(x, -1, 0)\nx.shape\n#yields: \n(3, 12, 12)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n#In general a good idea is to scale the data\nscaler = StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)    \n\npca = PCA()\nx_new = pca.fit_transform(X)\n\ndef myplot(score,coeff,labels=None):\n    xs = score[:,0]\n    ys = score[:,1]\n    n = coeff.shape[0]\n    scalex = 1.0/(xs.max() - xs.min())\n    scaley = 1.0/(ys.max() - ys.min())\n    plt.scatter(xs * scalex,ys * scaley, c = y)\n    for i in range(n):\n        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = \'r\',alpha = 0.5)\n        if labels is None:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, "Var"+str(i+1), color = \'g\', ha = \'center\', va = \'center\')\n        else:\n            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = \'g\', ha = \'center\', va = \'center\')\nplt.xlim(-1,1)\nplt.ylim(-1,1)\nplt.xlabel("PC{}".format(1))\nplt.ylabel("PC{}".format(2))\nplt.grid()\n\n#Call the function. Use only the 2 PCs.\nmyplot(x_new[:,0:2],np.transpose(pca.components_[0:2, :]))\nplt.show()\n\npca.explained_variance_ratio_\n[0.72770452, 0.23030523, 0.03683832, 0.00515193]\n\nprint(abs( pca.components_ ))\n\n[[0.52237162 0.26335492 0.58125401 0.56561105]\n [0.37231836 0.92555649 0.02109478 0.06541577]\n [0.72101681 0.24203288 0.14089226 0.6338014 ]\n [0.26199559 0.12413481 0.80115427 0.52354627]]\n\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\n# 10 samples with 5 features\ntrain_features = np.random.rand(10,5)\n\nmodel = PCA(n_components=2).fit(train_features)\nX_pc = model.transform(train_features)\n\n# number of components\nn_pcs= model.components_.shape[0]\n\n# get the index of the most important feature on EACH component\n# LIST COMPREHENSION HERE\nmost_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n\ninitial_feature_names = [\'a\',\'b\',\'c\',\'d\',\'e\']\n# get the names\nmost_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n\n# LIST COMPREHENSION HERE AGAIN\ndic = {\'PC{}\'.format(i): most_important_names[i] for i in range(n_pcs)}\n\n# build the dataframe\ndf = pd.DataFrame(dic.items())\n\n     0  1\n 0  PC0  e\n 1  PC1  d\n'
'import input_data\nmnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)\n...\n'
'tf.reset_default_graph()\nvalue = tf.constant(1)\nprint(tf.get_default_graph().as_graph_def())\n\ntf.reset_default_graph()\nvalue = tf.Variable(tf.ones_initializer()(()))\nvalue2 = value+3\nprint(tf.get_default_graph().as_graph_def())\n'
'y_new_inverse = scalery.inverse_transform(y_new)\n'
"model.save_weights('model.h5')\n\nmodel.load_weights('model.h5')\n"
"df = pd.DataFrame(A.toarray())\n\nA = csr_matrix([[1, 0, 2], [0, 3, 0]])\n\n  (0, 0)    1\n  (0, 2)    2\n  (1, 1)    3\n\n&lt;class 'scipy.sparse.csr.csr_matrix'&gt;\n\npd.DataFrame(A.todense())\n\n   0  1  2\n0  1  0  2\n1  0  3  0\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 2 entries, 0 to 1\nData columns (total 3 columns):\n0    2 non-null int64\n1    2 non-null int64\n2    2 non-null int64\n"
'from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cross_validation import  cross_val_score\nimport time\nfrom sklearn.datasets import  load_iris\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\niris = load_iris()\n\nmodels = [GaussianNB(), DecisionTreeClassifier(), SVC()]\nnames = ["Naive Bayes", "Decision Tree", "SVM"]\n\ndef getScores(estimator, x, y):\n    yPred = estimator.predict(x)\n    return (accuracy_score(y, yPred), \n            precision_score(y, yPred, pos_label=3, average=\'macro\'), \n            recall_score(y, yPred, pos_label=3, average=\'macro\'))\n\ndef my_scorer(estimator, x, y):\n    a, p, r = getScores(estimator, x, y)\n    print a, p, r\n    return a+p+r\n\nfor model, name in zip(models, names):\n    print name\n    start = time.time()\n    m = cross_val_score(model, iris.data, iris.target,scoring=my_scorer, cv=10).mean()\n    print \'\\nSum:\',m, \'\\n\\n\'\n    print \'time\', time.time() - start, \'\\n\\n\'\n\nNaive Bayes\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n0.866666666667 0.904761904762 0.866666666667\n1.0 1.0 1.0\n1.0 1.0 1.0\n1.0 1.0 1.0\n\nSum: 2.86936507937 \n\n\ntime 0.0249638557434 \n\n\nDecision Tree\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n0.866666666667 0.866666666667 0.866666666667\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n1.0 1.0 1.0\n\nSum: 2.86555555556 \n\n\ntime 0.0237860679626 \n\n\nSVM\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n1.0 1.0 1.0\n1.0 1.0 1.0\n0.933333333333 0.944444444444 0.933333333333\n0.933333333333 0.944444444444 0.933333333333\n1.0 1.0 1.0\n1.0 1.0 1.0\n1.0 1.0 1.0\n\nSum: 2.94333333333 \n\n\ntime 0.043044090271 \n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.datasets import  load_iris\nfrom sklearn.svm import SVC\n\niris = load_iris()\nclf = SVC()\nscoring = {\'acc\': \'accuracy\',\n           \'prec_macro\': \'precision_macro\',\n           \'rec_micro\': \'recall_macro\'}\nscores = cross_validate(clf, iris.data, iris.target, scoring=scoring,\n                         cv=5, return_train_score=True)\nprint(scores.keys())\nprint(scores[\'test_acc\'])  \n\n[\'test_acc\', \'score_time\', \'train_acc\', \'fit_time\', \'test_rec_micro\', \'train_rec_micro\', \'train_prec_macro\', \'test_prec_macro\']\n[ 0.96666667  1.          0.96666667  0.96666667  1.        ]\n'
'import keras\nNUM_WORDS=1000 # only use top 1000 words\nINDEX_FROM=3   # word index offset\n\ntrain,test = keras.datasets.imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\ntrain_x,train_y = train\ntest_x,test_y = test\n\nword_to_id = keras.datasets.imdb.get_word_index()\nword_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\nword_to_id["&lt;PAD&gt;"] = 0\nword_to_id["&lt;START&gt;"] = 1\nword_to_id["&lt;UNK&gt;"] = 2\nword_to_id["&lt;UNUSED&gt;"] = 3\n\nid_to_word = {value:key for key,value in word_to_id.items()}\nprint(\' \'.join(id_to_word[id] for id in train_x[0] ))\n\n"&lt;START&gt; this film was just brilliant casting &lt;UNK&gt; &lt;UNK&gt; story\n direction &lt;UNK&gt; really &lt;UNK&gt; the part they played and you could just\n imagine being there robert &lt;UNK&gt; is an amazing actor ..."\n'
'sample_weight : array-like, shape = [n_samples] or None\n\nsample_weight = np.array([5 if i == 0 else 1 for i in y])\n'
"def cluster(order, distance, points, threshold):\n    ''' Given the output of the options algorithm,\n    compute the clusters:\n\n    @param order The order of the points\n    @param distance The relative distances of the points\n    @param points The actual points\n    @param threshold The threshold value to cluster on\n    @returns A list of cluster groups\n    '''\n    clusters = [[]]\n    points   = sorted(zip(order, distance, points))\n    splits   = ((v &gt; threshold, p) for i,v,p in points)\n    for iscluster, point in splits: \n        if iscluster: clusters[-1].append(point)\n        elif len(clusters[-1]) &gt; 0: clusters.append([])\n    return clusters\n\n    rd, cd, order = optics(points, 4)\n    print cluster(order, rd, points, 38.0)\n"
'&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; print inspect.getsource(pos_tag)\ndef pos_tag(tokens, tagset=None):\n    tagger = PerceptronTagger()\n    return _pos_tag(tokens, tagset, tagger) \n\n&gt;&gt;&gt; from nltk import pos_tag\n&gt;&gt;&gt; pos_tag("The quick brown fox jumps over the lazy dog".split())\n[(\'The\', \'DT\'), (\'quick\', \'JJ\'), (\'brown\', \'NN\'), (\'fox\', \'NN\'), (\'jumps\', \'VBZ\'), (\'over\', \'IN\'), (\'the\', \'DT\'), (\'lazy\', \'JJ\'), (\'dog\', \'NN\')]\n\n&gt;&gt;&gt; from nltk import word_tokenize, pos_tag\n&gt;&gt;&gt; text = "The quick brown fox jumps over the lazy dog"\n&gt;&gt;&gt; pos_tag(word_tokenize(text))\n[(\'The\', \'DT\'), (\'quick\', \'NN\'), (\'brown\', \'NN\'), (\'fox\', \'NN\'), (\'jumps\', \'NNS\'), (\'over\', \'IN\'), (\'the\', \'DT\'), (\'lazy\', \'NN\'), (\'dog\', \'NN\')]\n\n$ cd ~\n$ wget http://nlp.stanford.edu/software/stanford-postagger-2015-04-20.zip\n$ unzip stanford-postagger-2015-04-20.zip\n$ mv stanford-postagger-2015-04-20 stanford-postagger\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser("~")\n&gt;&gt;&gt; from nltk.tag.stanford import POSTagger\n&gt;&gt;&gt; _path_to_model = home + \'/stanford-postagger/models/english-bidirectional-distsim.tagger\'\n&gt;&gt;&gt; _path_to_jar = home + \'/stanford-postagger/stanford-postagger.jar\'\n&gt;&gt;&gt; st = POSTagger(path_to_model=_path_to_model, path_to_jar=_path_to_jar)\n&gt;&gt;&gt; text = "The quick brown fox jumps over the lazy dog"\n&gt;&gt;&gt; st.tag(text.split())\n[(u\'The\', u\'DT\'), (u\'quick\', u\'JJ\'), (u\'brown\', u\'JJ\'), (u\'fox\', u\'NN\'), (u\'jumps\', u\'VBZ\'), (u\'over\', u\'IN\'), (u\'the\', u\'DT\'), (u\'lazy\', u\'JJ\'), (u\'dog\', u\'NN\')]\n\n$ cd ~\n$ wget https://hunpos.googlecode.com/files/hunpos-1.0-linux.tgz\n$ tar zxvf hunpos-1.0-linux.tgz\n$ wget https://hunpos.googlecode.com/files/en_wsj.model.gz\n$ gzip -d en_wsj.model.gz \n$ mv en_wsj.model hunpos-1.0-linux/\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser("~")\n&gt;&gt;&gt; from nltk.tag.hunpos import HunposTagger\n&gt;&gt;&gt; _path_to_bin = home + \'/hunpos-1.0-linux/hunpos-tag\'\n&gt;&gt;&gt; _path_to_model = home + \'/hunpos-1.0-linux/en_wsj.model\'\n&gt;&gt;&gt; ht = HunposTagger(path_to_model=_path_to_model, path_to_bin=_path_to_bin)\n&gt;&gt;&gt; text = "The quick brown fox jumps over the lazy dog"\n&gt;&gt;&gt; ht.tag(text.split())\n[(\'The\', \'DT\'), (\'quick\', \'JJ\'), (\'brown\', \'JJ\'), (\'fox\', \'NN\'), (\'jumps\', \'NNS\'), (\'over\', \'IN\'), (\'the\', \'DT\'), (\'lazy\', \'JJ\'), (\'dog\', \'NN\')]\n\n$ cd ~\n$ wget http://ronan.collobert.com/senna/senna-v3.0.tgz\n$ tar zxvf senna-v3.0.tgz\n$ python\n&gt;&gt;&gt; from os.path import expanduser\n&gt;&gt;&gt; home = expanduser("~")\n&gt;&gt;&gt; from nltk.tag.senna import SennaTagger\n&gt;&gt;&gt; st = SennaTagger(home+\'/senna\')\n&gt;&gt;&gt; text = "The quick brown fox jumps over the lazy dog"\n&gt;&gt;&gt; st.tag(text.split())\n[(\'The\', u\'DT\'), (\'quick\', u\'JJ\'), (\'brown\', u\'JJ\'), (\'fox\', u\'NN\'), (\'jumps\', u\'VBZ\'), (\'over\', u\'IN\'), (\'the\', u\'DT\'), (\'lazy\', u\'JJ\'), (\'dog\', u\'NN\')]\n'
"model = Sequential()\nact = keras.layers.advanced_activations.PReLU(init='zero', weights=None)\nmodel.add(Dense(64, input_dim=14, init='uniform'))\nmodel.add(act)\n"
'4.65761066e-03 + 9.95342389e-01 = 1\n9.75851270e-01 + 2.41487300e-02 = 1\n9.99983374e-01 + 1.66258341e-05 = 1\n'
'net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])\n\nnet = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n'
'accuracy_summary = tf.scalar_summary("Training Accuracy", accuracy)\ntf.scalar_summary("SomethingElse", foo)\nsummary_op = tf.merge_all_summaries()\nsummaries_dir = \'/me/mydir/\'\ntrain_writer = tf.train.SummaryWriter(summaries_dir + \'/train\', sess.graph)\ntest_writer = tf.train.SummaryWriter(summaries_dir + \'/test\')\n\n# Record train set summaries, and train\nsummary, _ = sess.run([summary_op, train_step], feed_dict=...)\ntrain_writer.add_summary(summary, n)\nif n % 100 == 0:  # Record summaries and test-set accuracy\n  summary, acc = sess.run([accuracy_summary, accuracy], feed_dict=...)\n  test_writer.add_summary(summary, n)\n  print(\'Accuracy at step %s: %s\' % (n, acc))\n'
"from matplotlib import pyplot as plt\nfrom sklearn import svm\n\ndef f_importances(coef, names):\n    imp = coef\n    imp,names = zip(*sorted(zip(imp,names)))\n    plt.barh(range(len(names)), imp, align='center')\n    plt.yticks(range(len(names)), names)\n    plt.show()\n\nfeatures_names = ['input1', 'input2']\nsvm = svm.SVC(kernel='linear')\nsvm.fit(X, Y)\nf_importances(svm.coef_, features_names)\n"
"{'water but all': 23.7, \n 'water but ill': 24.5,\n 'water but lay': 24.8,\n 'water but let': 26.0,\n 'water but lie': 25.9,\n 'water but look': 26.6}\n\n{'bwling belia': 32.3,\n 'bwling bell': 33.6,\n 'bwling below': 32.1,\n 'bwling belt': 32.5,\n 'bwling black': 31.4,\n 'bwling bling': 32.9,\n 'bwling blow': 32.7,\n 'bwling blue': 30.7}\n"
'prob = tf.placeholder_with_default(1.0, shape=())\nlayer = tf.nn.dropout(layer, prob)\n\nsess.run(train_step, feed_dict={prob: 0.5})\n'
"from pybrain.tools.shortcuts import buildNetwork\nfrom pybrain.supervised.trainers import BackpropTrainer\nfrom pybrain.datasets import SupervisedDataSet,UnsupervisedDataSet\nfrom pybrain.structure import LinearLayer\nds = SupervisedDataSet(21, 21)\nds.addSample(map(int,'1 2 4 6 2 3 4 5 1 3 5 6 7 1 4 7 1 2 3 5 6'.split()),map(int,'1 2 5 6 2 4 4 5 1 2 5 6 7 1 4 6 1 2 3 3 6'.split()))\nds.addSample(map(int,'1 2 5 6 2 4 4 5 1 2 5 6 7 1 4 6 1 2 3 3 6'.split()),map(int,'1 3 5 7 2 4 6 7 1 3 5 6 7 1 4 6 1 2 2 3 7'.split()))\nnet = buildNetwork(21, 20, 21, outclass=LinearLayer,bias=True, recurrent=True)\ntrainer = BackpropTrainer(net, ds)\ntrainer.trainEpochs(100)\nts = UnsupervisedDataSet(21,)\nts.addSample(map(int,'1 3 5 7 2 4 6 7 1 3 5 6 7 1 4 6 1 2 2 3 7'.split()))\n[ int(round(i)) for i in net.activateOnDataset(ts)[0]]\n\nfrom pybrain.tools.shortcuts import buildNetwork\nfrom pybrain.supervised.trainers import BackpropTrainer\nfrom pybrain.datasets import SupervisedDataSet,UnsupervisedDataSet\nfrom pybrain.structure import LinearLayer\nds = SupervisedDataSet(10, 11)\nz = map(int,'1 2 4 6 2 3 4 5 1 3 5 6 7 1 4 7 1 2 3 5 6 1 2 5 6 2 4 4 5 1 2 5 6 7 1 4 6 1 2 3 3 6 1 3 5 7 2 4 6 7 1 3 5 6 7 1 4 6 1 2 2 3 7'.split())\nobsLen = 10\npredLen = 11\nfor i in xrange(len(z)):\n  if i+(obsLen-1)+predLen &lt; len(z):\n    ds.addSample([z[d] for d in range(i,i+obsLen)],[z[d] for d in range(i+1,i+1+predLen)])\n\nnet = buildNetwork(10, 20, 11, outclass=LinearLayer,bias=True, recurrent=True)\ntrainer = BackpropTrainer(net, ds)\ntrainer.trainEpochs(100)\nts = UnsupervisedDataSet(10,)\nts.addSample(map(int,'1 3 5 7 2 4 6 7 1 3'.split()))\n[ int(round(i)) for i in net.activateOnDataset(ts)[0]]\n"
'initial = np.loadtxt(filename).astype(np.float32)\n'
'f(x[0]) * ... * f(x[100])\n'
'clf = ensemble.GradientBoostingClassifier(verbose=3)\nclf.fit(X, y)\nOut:\n   Iter       Train Loss   Remaining Time\n     1           0.0769            0.10s\n     ...\n\nclf = ensemble.RandomForestClassifier(verbose=3)\nclf.fit(X, y)\nOut:\n  building tree 1 of 100\n  ...\n\nclf = svm.SVC(verbose=2)\nclf.fit(X, y)\nOut:\n   *\n    optimization finished, #iter = 1\n    obj = -1.802585, rho = 0.000000\n    nSV = 2, nBSV = 2\n    ...\n'
'sigmoid(W1 * x1 + W2 * x2 + B)\n\nsigmoid(W * (x1 + x2) + B)\n'
'X = numpy.array(X)\nY = numpy.array(Y)\n'
"grid = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression()),\n                    param_grid={'logisticregression__C': [0.1, 10.]},\n                    cv=2,\n                    refit=False)\n\nclf = make_pipeline(StandardScaler(), \n                    GridSearchCV(LogisticRegression(),\n                                 param_grid={'logisticregression__C': [0.1, 10.]},\n                                 cv=2,\n                                 refit=True))\n\nclf.fit()\nclf.predict()\n"
'import tensorflow as tf\nimport numpy as np\n\n# Create function to convert saved keras model to tensorflow graph\ndef convert_to_pb(weight_file,input_fld=\'\',output_fld=\'\'):\n\n    import os\n    import os.path as osp\n    from tensorflow.python.framework import graph_util\n    from tensorflow.python.framework import graph_io\n    from keras.models import load_model\n    from keras import backend as K\n\n\n    # weight_file is a .h5 keras model file\n    output_node_names_of_input_network = ["pred0"] \n    output_node_names_of_final_network = \'output_node\'\n\n    # change filename to a .pb tensorflow file\n    output_graph_name = weight_file[:-2]+\'pb\'\n    weight_file_path = osp.join(input_fld, weight_file)\n\n    net_model = load_model(weight_file_path)\n\n    num_output = len(output_node_names_of_input_network)\n    pred = [None]*num_output\n    pred_node_names = [None]*num_output\n\n    for i in range(num_output):\n        pred_node_names[i] = output_node_names_of_final_network+str(i)\n        pred[i] = tf.identity(net_model.output[i], name=pred_node_names[i])\n\n    sess = K.get_session()\n\n    constant_graph = graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), pred_node_names)\n    graph_io.write_graph(constant_graph, output_fld, output_graph_name, as_text=False)\n    print(\'saved the constant graph (ready for inference) at: \', osp.join(output_fld, output_graph_name))\n\n    return output_fld+output_graph_name\n\ntf_model_path = convert_to_pb(\'model_file.h5\',\'/model_dir/\',\'/model_dir/\')\n\ndef load_graph(frozen_graph_filename):\n    # We load the protobuf file from the disk and parse it to retrieve the \n    # unserialized graph_def\n    with tf.gfile.GFile(frozen_graph_filename, "rb") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we can use again a convenient built-in function to import a graph_def into the \n    # current default Graph\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(\n            graph_def, \n            input_map=None, \n            return_elements=None, \n            name="prefix", \n            op_dict=None, \n            producer_op_list=None\n        )\n\n    input_name = graph.get_operations()[0].name+\':0\'\n    output_name = graph.get_operations()[-1].name+\':0\'\n\n    return graph, input_name, output_name\n\ndef predict(model_path, input_data):\n    # load tf graph\n    tf_model,tf_input,tf_output = load_graph(model_path)\n\n    # Create tensors for model input and output\n    x = tf_model.get_tensor_by_name(tf_input)\n    y = tf_model.get_tensor_by_name(tf_output) \n\n    # Number of model outputs\n    num_outputs = y.shape.as_list()[0]\n    predictions = np.zeros((input_data.shape[0],num_outputs))\n    for i in range(input_data.shape[0]):        \n        with tf.Session(graph=tf_model) as sess:\n            y_out = sess.run(y, feed_dict={x: input_data[i:i+1]})\n            predictions[i] = y_out\n\n    return predictions\n\ntf_predictions = predict(tf_model_path,test_data)\n\ndef compute_jacobian(model_path,input_data):\n\n    tf_model,tf_input,tf_output = load_graph(model_path)    \n\n    x = tf_model.get_tensor_by_name(tf_input)\n    y = tf_model.get_tensor_by_name(tf_output)\n    y_list = tf.unstack(y)\n    num_outputs = y.shape.as_list()[0]\n    jacobian = np.zeros((num_outputs,input_data.shape[0],input_data.shape[1]))\n    for i in range(input_data.shape[0]):\n        with tf.Session(graph=tf_model) as sess:\n            y_out = sess.run([tf.gradients(y_, x)[0] for y_ in y_list], feed_dict={x: input_data[i:i+1]})\n            jac_temp = np.asarray(y_out)\n        jacobian[:,i:i+1,:]=jac_temp[:,:,:,0]\n    return jacobian\n\njacobians = compute_jacobian(tf_model_path,test_data)\n'
"from sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncategories = ['talk.religion.misc',\n              'comp.graphics', 'sci.space']\nnewsgroups_train = fetch_20newsgroups(subset='train',\n                                      categories=categories)\n\nX, Y = newsgroups_train.data, newsgroups_train.target\ncv = CountVectorizer(max_df=0.95, min_df=2,\n                                     max_features=10000,\n                                     stop_words='english')\nX_vec = cv.fit_transform(X)\n\nres = dict(zip(cv.get_feature_names(),\n               mutual_info_classif(X_vec, Y, discrete_features=True)\n               ))\nprint(res)\n\n{'bible': 0.072327479595571439,\n 'christ': 0.057293733680219089,\n 'christian': 0.12862867565281702,\n 'christians': 0.068511328611810071,\n 'file': 0.048056478042481157,\n 'god': 0.12252523919766867,\n 'gov': 0.053547274485785577,\n 'graphics': 0.13044709565039875,\n 'jesus': 0.09245436105573257,\n 'launch': 0.059882179387444862,\n 'moon': 0.064977781072557236,\n 'morality': 0.050235104394123153,\n 'nasa': 0.11146392824624819,\n 'orbit': 0.087254803670582998,\n 'people': 0.068118370234354936,\n 'prb': 0.049176995204404481,\n 'religion': 0.067695617096125316,\n 'shuttle': 0.053440976618359261,\n 'space': 0.20115901737978983,\n 'thanks': 0.060202010019767334}\n"
"# In [1]: from tensorflow.python.summary import event_accumulator  # deprecated\nIn [1]: from tensorboard.backend.event_processing import event_accumulator\n\nIn [2]: ea = event_accumulator.EventAccumulator('events.out.tfevents.x.ip-x-x-x-x',\n   ...:  size_guidance={ # see below regarding this argument\n   ...:      event_accumulator.COMPRESSED_HISTOGRAMS: 500,\n   ...:      event_accumulator.IMAGES: 4,\n   ...:      event_accumulator.AUDIO: 4,\n   ...:      event_accumulator.SCALARS: 0,\n   ...:      event_accumulator.HISTOGRAMS: 1,\n   ...:  })\n\nIn [3]: ea.Reload() # loads events from file\nOut[3]: &lt;tensorflow.python.summary.event_accumulator.EventAccumulator at 0x7fdbe5ff59e8&gt;\n\nIn [4]: ea.Tags()\nOut[4]: \n{'audio': [],\n 'compressedHistograms': [],\n 'graph': True,\n 'histograms': [],\n 'images': [],\n 'run_metadata': [],\n 'scalars': ['Loss', 'Epsilon', 'Learning_rate']}\n\nIn [5]: ea.Scalars('Loss')\nOut[5]: \n[ScalarEvent(wall_time=1481232633.080754, step=1, value=1.6365480422973633),\n ScalarEvent(wall_time=1481232633.2001867, step=2, value=1.2162202596664429),\n ScalarEvent(wall_time=1481232633.3877788, step=3, value=1.4660096168518066),\n ScalarEvent(wall_time=1481232633.5749283, step=4, value=1.2405034303665161),\n ScalarEvent(wall_time=1481232633.7419815, step=5, value=0.897326648235321),\n ...]\n\nsize_guidance: Information on how much data the EventAccumulator should\n  store in memory. The DEFAULT_SIZE_GUIDANCE tries not to store too much\n  so as to avoid OOMing the client. The size_guidance should be a map\n  from a `tagType` string to an integer representing the number of\n  items to keep per tag for items of that `tagType`. If the size is 0,\n  all events are stored.\n"
'class TimeHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.times = []\n\n    def on_epoch_begin(self, batch, logs={}):\n        self.epoch_time_start = time.time()\n\n    def on_epoch_end(self, batch, logs={}):\n        self.times.append(time.time() - self.epoch_time_start)\n\ntime_callback = TimeHistory()\nmodel.fit(..., callbacks=[..., time_callback],...)\ntimes = time_callback.times\n'
"pipe.steps.append(['step name',transformer()])\n\npipe.steps.insert(1,['estimator',transformer()]) #insert as second step\n"
'from multiprocessing import Pool\nimport contextlib\ndef my_model((param1, param2, param3)): # Note the extra (), required by the pool syntax\n    &lt; your code &gt;\n\nnum_pool_worker=1 # can be bigger than 1, to enable parallel execution \nwith contextlib.closing(Pool(num_pool_workers)) as po: # This ensures that the processes get closed once they are done\n     pool_results = po.map_async(my_model,\n                                    ((param1, param2, param3)\n                                     for param1, param2, param3 in params_list))\n     results_list = pool_results.get()\n\ngpu_memory_fraction = 0.3 # Choose this number through trial and error\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction,)\nsession_config = tf.ConfigProto(gpu_options=gpu_options)\nsess = tf.Session(config=session_config, graph=graph)\n\ndef train_mdl(params):\n    (x,y)=params\n    &lt; your code &gt;\n\nimport os # the import can be on the top of the python script\nos.environ["CUDA_VISIBLE_DEVICES"] = "{}".format(gpu_id)\n\nts &lt;your-command&gt;\n\ndef run_bash(cmd):\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, executable=\'/bin/bash\')\n    out = p.stdout.read().strip()\n    return out  # This is the stdout from the shell command\n\nmax_job_num_per_gpu = 2\nrun_bash(\'ts -S %d\'%max_job_num_per_gpu)\n\nfor combination_dict in combinations_list:\n\n    job_cmd = \'python train.py \' + \'  \'.join(\n            [\'--{}={}\'.format(flag, value) for flag, value in combination_dict.iteritems()])\n\n    submit_cmd = "ts bash -c \'%s\'" % job_cmd\n    run_bash(submit_cmd)\n\ndef build_string_from_dict(d, sep=\'%\'):\n    """\n     Builds a string from a dictionary.\n     Mainly used for formatting hyper-params to file names.\n     Key-value pairs are sorted by the key name.\n\n    Args:\n        d: dictionary\n\n    Returns: string\n    :param d: input dictionary\n    :param sep: key-value separator\n\n    """\n\n    return sep.join([\'{}={}\'.format(k, _value2str(d[k])) for k in sorted(d.keys())])\n\n\ndef _value2str(val):\n    if isinstance(val, float): \n        # %g means: "Floating point format.\n        # Uses lowercase exponential format if exponent is less than -4 or not less than precision,\n        # decimal format otherwise."\n        val = \'%g\' % val\n    else:\n        val = \'{}\'.format(val)\n    val = re.sub(\'\\.\', \'_\', val)\n    return val\n'
'model.layers[i].set_weights(listOfNumpyArrays)    \nmodel.get_layer(layerName).set_weights(...)\nmodel.set_weights(listOfNumpyArrays)\n'
'tf.reset_default_graph()\n'
'def compute_mask(self, inputs, mask=None):\n    if not self.mask_zero:\n        return None\n    output_mask = K.not_equal(inputs, 0)\n    return output_mask\n\n# Handle mask propagation.\nprevious_mask = _collect_previous_mask(inputs)\nuser_kwargs = copy.copy(kwargs)\nif not is_all_none(previous_mask):\n    # The previous layer generated a mask.\n    if has_arg(self.call, \'mask\'):\n        if \'mask\' not in kwargs:\n            # If mask is explicitly passed to __call__,\n            # we should override the default mask.\n            kwargs[\'mask\'] = previous_mask\n\ndata_in = np.array([\n  [1, 0, 2, 0]\n])\n\nx = Input(shape=(4,))\ne = Embedding(5, 5, mask_zero=True)(x)\nrnn = LSTM(3, return_sequences=True)(e)\n\nm = Model(inputs=x, outputs=rnn)\nm.predict(data_in)\n\narray([[[-0.00084503, -0.00413611,  0.00049972],\n        [-0.00084503, -0.00413611,  0.00049972],\n        [-0.00144554, -0.00115775, -0.00293898],\n        [-0.00144554, -0.00115775, -0.00293898]]], dtype=float32)\n\ndef weighted_masked_objective(fn):\n    """Adds support for masking and sample-weighting to an objective function.\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    """\n\nweighted_losses = [weighted_masked_objective(fn) for fn in loss_functions]\n\ndata_in = np.array([[1, 2, 0, 0]])\ndata_out = np.arange(12).reshape(1,4,3)\n\nx = Input(shape=(4,))\ne = Embedding(5, 5, mask_zero=True)(x)\nd = Dense(3)(e)\n\nm = Model(inputs=x, outputs=d)\nm.compile(loss=\'mse\', optimizer=\'adam\')\npreds = m.predict(data_in)\nloss = m.evaluate(data_in, data_out, verbose=0)\nprint(preds)\nprint(\'Computed Loss:\', loss)\n\n[[[ 0.009682    0.02505393 -0.00632722]\n  [ 0.01756451  0.05928303  0.0153951 ]\n  [-0.00146054 -0.02064196 -0.04356086]\n  [-0.00146054 -0.02064196 -0.04356086]]]\nComputed Loss: 9.041069030761719\n\n# verify that only the first two outputs \n# have been considered in the computation of loss\nprint(np.square(preds[0,0:2] - data_out[0,0:2]).mean())\n\n9.041070036475277\n'
'import numpy as np\n\ndef poly(x, degree):\n    xbar = np.mean(x)\n    x = x - xbar\n\n    # R: outer(x, 0L:degree, "^")\n    X = x[:, None] ** np.arange(0, degree+1)\n\n    #R: qr(X)$qr\n    q, r = np.linalg.qr(X)\n\n    #R: r * (row(r) == col(r))\n    z = np.diag((np.diagonal(r)))  \n\n    # R: Z = qr.qy(QR, z)\n    Zq, Zr = np.linalg.qr(q)\n    Z = np.matmul(Zq, z)\n\n    # R: colSums(Z^2)\n    norm1 = (Z**2).sum(0)\n\n    #R: (colSums(x * Z^2)/norm2 + xbar)[1L:degree]\n    alpha = ((x[:, None] * (Z**2)).sum(0) / norm1 +xbar)[0:degree]\n\n    # R: c(1, norm2)\n    norm2 = np.append(1, norm1)\n\n    # R: Z/rep(sqrt(norm1), each = length(x))\n    Z = Z / np.reshape(np.repeat(norm1**(1/2.0), repeats = x.size), (-1, x.size), order=\'F\')\n\n    #R: Z[, -1]\n    Z = np.delete(Z, 0, axis=1)\n    return [Z, alpha, norm2];\n\nx = np.arange(10) + 1\ndegree = 9\npoly(x, degree)\n\n[-0.49543369,  0.52223297, -0.45342519,  0.33658092, -0.21483446,\n  0.11677484, -0.05269379,  0.01869894, -0.00453516],\n\npoly(1:10, 9)\n# [1] -0.495433694  0.522232968 -0.453425193  0.336580916 -0.214834462\n# [6]  0.116774842 -0.052693786  0.018698940 -0.004535159\n'
"model = Sequential()\nmodel.add(LSTM(32, return_sequences=True, activation = 'sigmoid', input_shape=(x_train.shape[1], x_train.shape[2])))\n# model.add(Dropout(0.2))\n# model.add(BatchNormalization())\nmodel.add(LSTM(units = 64, return_sequences=False,))\nmodel.add(Dense(y_train.shape[1]))\nmodel.compile(optimizer = 'adam', loss = 'mse')\n\nmodel.fit(x_train, y_train, batch_size = 64, epochs = 1000, shuffle = True, validation_data = (x_test, y_test))\n"
'from sklearn.base import clone\n\nlr1 = LogisticRegression()\nlr2 = clone(lr1)\n'
"from sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\nclf_ = SVC(kernel='rbf')\nCs = [1, 10, 100, 1000]\nGammas = [1e-3, 1e-4]\nclf = GridSearchCV(clf_,\n            dict(C=Cs,\n                 gamma=Gammas),\n                 cv=2,\n                 pre_dispatch='1*n_jobs',\n                 n_jobs=1)\n\nclf.fit(X, y)\n\nscores = [x[1] for x in clf.grid_scores_]\nscores = np.array(scores).reshape(len(Cs), len(Gammas))\n\nfor ind, i in enumerate(Cs):\n    plt.plot(Gammas, scores[ind], label='C: ' + str(i))\nplt.legend()\nplt.xlabel('Gamma')\nplt.ylabel('Mean score')\nplt.show()\n"
'\'\'\'\nsave first example as item.basket with format\nA, B, C, E\nA, C\nA, C, D, E\nA, C, E\nopen ipython same directory as saved file or use os module\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; os.chdir("c:/orange")\n\'\'\'\nimport orange\n\nitems = orange.ExampleTable("item")\n#play with support argument to filter out rules\nrules = orange.AssociationRulesSparseInducer(items, support = 0.1) \nfor r in rules:\n    print "%5.3f %5.3f %s" % (r.support, r.confidence, r)\n'
'import numpy as np\nfrom nltk.probability import FreqDist\nfrom nltk.classify import SklearnClassifier\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([(\'tfidf\', TfidfTransformer()),\n                     (\'chi2\', SelectKBest(chi2, k=1000)),\n                     (\'nb\', MultinomialNB())])\nclassif = SklearnClassifier(pipeline)\n\nfrom nltk.corpus import movie_reviews\npos = [FreqDist(movie_reviews.words(i)) for i in movie_reviews.fileids(\'pos\')]\nneg = [FreqDist(movie_reviews.words(i)) for i in movie_reviews.fileids(\'neg\')]\nadd_label = lambda lst, lab: [(x, lab) for x in lst]\nclassif.train(add_label(pos[:100], \'pos\') + add_label(neg[:100], \'neg\'))\n\nl_pos = np.array(classif.classify_many(pos[100:]))\nl_neg = np.array(classif.classify_many(neg[100:]))\nprint "Confusion matrix:\\n%d\\t%d\\n%d\\t%d" % (\n          (l_pos == \'pos\').sum(), (l_pos == \'neg\').sum(),\n          (l_neg == \'pos\').sum(), (l_neg == \'neg\').sum())\n\nConfusion matrix:\n524     376\n202     698\n'
'class EnsembleClassifier(BaseEstimator, ClassifierMixin):\n    def __init__(self, classifiers=None):\n        self.classifiers = classifiers\n\n    def fit(self, X, y):\n        for classifier in self.classifiers:\n            classifier.fit(X, y)\n\n    def predict_proba(self, X):\n        self.predictions_ = list()\n        for classifier in self.classifiers:\n            self.predictions_.append(classifier.predict_proba(X))\n        return np.mean(self.predictions_, axis=0)\n'
"import matplotlib.pyplot as plt\nimport numpy as np\n\n# create some randomly ddistributed data:\ndata = np.random.randn(10000)\n\n# sort the data:\ndata_sorted = np.sort(data)\n\n# calculate the proportional values of samples\np = 1. * np.arange(len(data)) / (len(data) - 1)\n\n# plot the sorted data:\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.plot(p, data_sorted)\nax1.set_xlabel('$p$')\nax1.set_ylabel('$x$')\n\nax2 = fig.add_subplot(122)\nax2.plot(data_sorted, p)\nax2.set_xlabel('$x$')\nax2.set_ylabel('$p$')\n"
'corpus = np.array(["aaa bbb ccc", "aaa bbb ddd"])\nvectorizer = CountVectorizer(decode_error="replace")\nvec_train = vectorizer.fit_transform(corpus)\n#Save vectorizer.vocabulary_\npickle.dump(vectorizer.vocabulary_,open("feature.pkl","wb"))\n\n#Load it later\ntransformer = TfidfTransformer()\nloaded_vec = CountVectorizer(decode_error="replace",vocabulary=pickle.load(open("feature.pkl", "rb")))\ntfidf = transformer.fit_transform(loaded_vec.fit_transform(np.array(["aaa ccc eee"])))\n'
"fill_NaN = Imputer(missing_values=np.nan, strategy='mean', axis=1)\nimputed_DF = pd.DataFrame(fill_NaN.fit_transform(DF))\nimputed_DF.columns = DF.columns\nimputed_DF.index = DF.index\n\nDF[DF.isnull()] = 0\n"
'class MyRNNCell(RNNCell):\n  def __init__(...):\n\n  @property\n  def output_size(self):\n  ...\n\n  @property\n  def state_size(self):\n  ...\n\n  def __call__(self, input_, state, name=None):\n     ... your per-step iteration here ...\n'
"def generator_queue(generator, max_q_size=10,\n                    wait_time=0.05, nb_worker=1):\n    '''Builds a threading queue out of a data generator.\n    Used in `fit_generator`, `evaluate_generator`, `predict_generator`.\n    '''\n    q = queue.Queue()\n    _stop = threading.Event()\n\n    def data_generator_task():\n        while not _stop.is_set():\n            try:\n                if q.qsize() &lt; max_q_size:\n                    try:\n                        generator_output = next(generator)\n                    except ValueError:\n                        continue\n                    q.put(generator_output)\n                else:\n                    time.sleep(wait_time)\n            except Exception:\n                _stop.set()\n                raise\n\n    generator_threads = [threading.Thread(target=data_generator_task)\n                         for _ in range(nb_worker)]\n\n    for thread in generator_threads:\n        thread.daemon = True\n        thread.start()\n\n    return q, _stop\n\n while samples_seen &lt; samples_per_epoch:\n     generator_output = None\n     while not _stop.is_set():\n         if not data_gen_queue.empty():\n             generator_output = data_gen_queue.get()\n             break\n         else:\n             time.sleep(wait_time)\n"
'dtrain = xgb.DMatrix(Xtrain, label=ytrain, feature_names=feature_names)\n'
'from tensorflow.python.keras.layers import Lambda;\n\ndef norm(fc2):\n\n    fc2_norm = K.l2_normalize(fc2, axis = 3);\n    illum_est = tf.reduce_sum(fc2_norm, axis = (1, 2));\n    illum_est = K.l2_normalize(illum_est);\n\n    return illum_est;\n\nillum_est = Lambda(norm)(fc2);\n'
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.preprocessing import Imputer\n&gt;&gt;&gt; # missing_values is the value of your placeholder, strategy is if you'd like mean, median or mode, and axis=0 means it calculates the imputation based on the other feature values for that sample\n&gt;&gt;&gt; imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n&gt;&gt;&gt; imp.fit(train)\nImputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)\n&gt;&gt;&gt; train_imp = imp.transform(train)\n"
"In [1]: import sklearn\n\nIn [2]: sklearn.datasets\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-2-325a2bfc35d0&gt; in &lt;module&gt;()\n----&gt; 1 sklearn.datasets\n\nAttributeError: module 'sklearn' has no attribute 'datasets'\n\nIn [3]: from sklearn import datasets\n\nIn [4]: sklearn.datasets\nOut[4]: &lt;module 'sklearn.datasets' from '/home/ethan/.virtualenvs/test3/lib/python3.5/site-packages/sklearn/datasets/__init__.py'&gt;\n\nIn [1]: from sklearn import *\nIn [2]: datasets\nOut[2]: &lt;module 'sklearn.datasets' from '/home/ethan/.virtualenvs/test3/lib/python3.5/site-packages/sklearn/datasets/__init__.py'&gt;\n"
'0 10440\n1 304\n2 998\n3 67\n4 412\n5 114\n6 190\n7 311\n8 195\n9 78\n10 75\n'
'0.001\n'
"...\nimport cPickle\n\nrf = RandomForestRegresor()\nrf.fit(X, y)\n\nwith open('path/to/file', 'wb') as f:\n    cPickle.dump(rf, f)\n\n\n# in your prediction file                                                                                                                                                                                                           \n\nwith open('path/to/file', 'rb') as f:\n    rf = cPickle.load(f)\n\n\npreds = rf.predict(new_X)\n"
'import numpy as np\nimport pandas as pd\nimport math\nfrom sys import stdout\n\n#function Takes the pandas dataframe, Input features list and the target column name\ndef get_numpy_data(data, features, output):\n\n    #Adding a constant column with value 1 in the dataframe.\n    data[\'constant\'] = 1    \n    #Adding the name of the constant column in the feature list.\n    features = [\'constant\'] + features\n    #Creating Feature matrix(Selecting columns and converting to matrix).\n    features_matrix=data[features].as_matrix()\n    #Target column is converted to the numpy array\n    output_array=np.array(data[output])\n    return(features_matrix, output_array)\n\ndef predict_outcome(feature_matrix, weights):\n    weights=np.array(weights)\n    predictions = np.dot(feature_matrix, weights)\n    return predictions\n\ndef errors(output,predictions):\n    errors=predictions-output\n    return errors\n\ndef feature_derivative(errors, feature):\n    derivative=np.dot(2,np.dot(feature,errors))\n    return derivative\n\n\ndef regression_gradient_descent(feature_matrix, output, initial_weights, step_size, tolerance):\n    converged = False\n    #Initital weights are converted to numpy array\n    weights = np.array(initial_weights)\n    while not converged:\n        # compute the predictions based on feature_matrix and weights:\n        predictions=predict_outcome(feature_matrix,weights)\n        # compute the errors as predictions - output:\n        error=errors(output,predictions)\n        gradient_sum_squares = 0 # initialize the gradient\n        # while not converged, update each weight individually:\n        for i in range(len(weights)):\n            # Recall that feature_matrix[:, i] is the feature column associated with weights[i]\n            feature=feature_matrix[:, i]\n            # compute the derivative for weight[i]:\n            #predict=predict_outcome(feature,weights[i])\n            #err=errors(output,predict)\n            deriv=feature_derivative(error,feature)\n            # add the squared derivative to the gradient magnitude\n            gradient_sum_squares=gradient_sum_squares+(deriv**2)\n            # update the weight based on step size and derivative:\n            weights[i]=weights[i] - np.dot(step_size,deriv)\n\n        gradient_magnitude = math.sqrt(gradient_sum_squares)\n        stdout.write("\\r%d" % int(gradient_magnitude))\n        stdout.flush()\n        if gradient_magnitude &lt; tolerance:\n            converged = True\n    return(weights)\n\n\n#Example of Implementation\n#Importing Training and Testing Data\n# train_data=pd.read_csv("kc_house_train_data.csv")\n# test_data=pd.read_csv("kc_house_test_data.csv")\n\n# simple_features = [\'sqft_living\', \'sqft_living15\']\n# my_output= \'price\'\n# (simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n# initial_weights = np.array([-100000., 1., 1.])\n# step_size = 7e-12\n# tolerance = 2.5e7\n# simple_weights = regression_gradient_descent(simple_feature_matrix, output,initial_weights, step_size,tolerance)\n# print simple_weights\n'
"Error = X * theta - y;\nfor i = 1:2\n    S(i) = sum(Error.*X(:,i));\nend\n\ntheta = theta - alpha * (1/m) * S'\n"
'model = SVC(probability=True)\n\nclass_probabilities = model.predict_proba(sub_main)\n'
"In [1]: import pandas as pd\n\nIn [2]: pd.__version__\nOut[2]: u'0.18.1'\n\nIn [3]: s = pd.Series(list('abcbacb'))\n\nIn [4]: pd.get_dummies(s, drop_first=True)\nOut[4]: \n     b    c\n0  0.0  0.0\n1  1.0  0.0\n2  0.0  1.0\n3  1.0  0.0\n4  0.0  0.0\n5  0.0  1.0\n6  1.0  0.0\n"
'proj = pca.inverse_transform(X_train_pca)\n\nfrom sklearn.decomposition import PCA\nimport numpy as np\nfrom numpy.testing import assert_array_almost_equal\n\n#Should this variable be X_train instead of Xtrain?\nX_train = np.random.randn(100, 50)\n\npca = PCA(n_components=30)\npca.fit(X_train)\n\nU, S, VT = np.linalg.svd(X_train - X_train.mean(0))\n\nassert_array_almost_equal(VT[:30], pca.components_)\n\nX_train_pca = pca.transform(X_train)\n\nX_train_pca2 = (X_train - pca.mean_).dot(pca.components_.T)\n\nassert_array_almost_equal(X_train_pca, X_train_pca2)\n\nX_projected = pca.inverse_transform(X_train_pca)\nX_projected2 = X_train_pca.dot(pca.components_) + pca.mean_\n\nassert_array_almost_equal(X_projected, X_projected2)\n\nloss = ((X_train - X_projected) ** 2).mean()\n'
"#zip the two lists containing vectors and words\nzipped = zip(nmodel.wv.index2word, nmodel.wv.syn0)\n\n#the resulting list contains `(word, wordvector)` tuples. We can extract the entry for any `word` or `vector` (replace with the word/vector you're looking for) using a list comprehension:\nwordresult = [i for i in zipped if i[0] == word]\nvecresult = [i for i in zipped if i[1] == vector]\n"
'test_datagen = ImageDataGenerator(rescale=1./255)\n\ntest_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(200, 200),\n        color_mode="rgb",\n        shuffle = False,\n        class_mode=\'categorical\',\n        batch_size=1)\n\nfilenames = test_generator.filenames\nnb_samples = len(filenames)\n\npredict = model.predict_generator(test_generator,steps = nb_samples)\n'
"le = preprocessing.LabelEncoder()\ndf['label'] = le.fit_transform(df.label.values)\n\ndf['label'] = le.fit_transform(df['label'])\n"
"df = pd.DataFrame({'Traded Value':[67867869890077.96,78973434444543.44],\n                   'Deals':[789797, 789878]})\ndf\n\n    Deals   Traded Value\n0   789797  6.786787e+13\n1   789878  7.897343e+13\n\n\ndf['Deals'] = df['Deals'].apply(lambda x: '{:d}'.format(x))\ndf['Traded Value'] = df['Traded Value'].apply(lambda x: '{:.2f}'.format(x))\ndf    \n\n     Deals       Traded Value\n0   789797  67867869890077.96\n1   789878  78973434444543.44\n\npd.options.display.float_format = '{:.2f}'.format\n"
"df = pd.DataFrame({'A':['type1','type2','type2'],\n                   'B':['type1','type2','type3'],\n                   'C':['type1','type3','type3']})\n\nprint (df)\n       A      B      C\n0  type1  type1  type1\n1  type2  type2  type3\n2  type2  type3  type3\n\nprint (df.apply(lambda x: pd.factorize(x)[0]))\n   A  B  C\n0  0  0  0\n1  1  1  1\n2  1  2  1\n\nprint (df.stack().rank(method='dense').unstack())\n     A    B    C\n0  1.0  1.0  1.0\n1  2.0  2.0  3.0\n2  2.0  3.0  3.0\n\ndf[['B','C']] = df[['B','C']].stack().rank(method='dense').unstack()\nprint (df)\n       A    B    C\n0  type1  1.0  1.0\n1  type2  2.0  3.0\n2  type2  3.0  3.0\n\nstacked = df[['B','C']].stack()\ndf[['B','C']] = pd.Series(stacked.factorize()[0], index=stacked.index).unstack()\nprint (df)\n       A  B  C\n0  type1  0  0\n1  type2  1  2\n2  type2  2  2\n\nvals = df.stack().drop_duplicates().values\nb = [x for x in df.stack().drop_duplicates().rank(method='dense')]\n\nd1 = dict(zip(b, vals))\nprint (d1)\n{1.0: 'type1', 2.0: 'type2', 3.0: 'type3'}\n\ndf1 = df.stack().rank(method='dense').unstack()\nprint (df1)\n     A    B    C\n0  1.0  1.0  1.0\n1  2.0  2.0  3.0\n2  2.0  3.0  3.0\n\nprint (df1.stack().map(d1).unstack())\n       A      B      C\n0  type1  type1  type1\n1  type2  type2  type3\n2  type2  type3  type3\n"
"from sklearn import preprocessing    \n# Test data\ndf = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])    \ndf['Fact'] = pd.factorize(df['Col'])[0]\nle = preprocessing.LabelEncoder()\ndf['Lab'] = le.fit_transform(df['Col'])\n\nprint(df)\n#   Col  Fact  Lab\n# 0   A     0    0\n# 1   B     1    1\n# 2   B     1    1\n# 3   C     2    2\n\ndf = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])\ndf = pd.get_dummies(df)\n\nprint(df)\n#    Col_A  Col_B  Col_C\n# 0    1.0    0.0    0.0\n# 1    0.0    1.0    0.0\n# 2    0.0    1.0    0.0\n# 3    0.0    0.0    1.0\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ndf = DataFrame(['A', 'B', 'B', 'C'], columns=['Col'])\n# We need to transform first character into integer in order to use the OneHotEncoder\nle = preprocessing.LabelEncoder()\ndf['Col'] = le.fit_transform(df['Col'])\nenc = OneHotEncoder()\ndf = DataFrame(enc.fit_transform(df).toarray())\n\nprint(df)\n#      0    1    2\n# 0  1.0  0.0  0.0\n# 1  0.0  1.0  0.0\n# 2  0.0  1.0  0.0\n# 3  0.0  0.0  1.0\n"
'model.add(Dense(10, activation=\'softmax\'))\n\nmodel.add(Conv2D(filters=10, \n             kernel_size=(3, 3), \n             input_shape=(None, None, 1),\n             padding="same",\n             data_format=\'channels_last\'))\nmodel.add(GlobalMaxPooling2D())\nmodel.add(Dense(128, activation=\'relu\'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation=\'softmax\'))\n\nfrom six import next\n\nbatches_generator = get_cropped_batches_generator(X, batch_size=16)\nlosses = list()\nfor epoch_nb in range(nb_of_epochs):\n    epoch_losses = list()\n    for batch_nb in range(nb_of_batches):\n        # cropped_x has a different shape for different batches (in general)\n        cropped_x, cropped_y = next(batches_generator) \n        current_loss = model.train_on_batch(cropped_x, cropped_y)\n        epoch_losses.append(current_loss)\n    losses.append(epoch_losses.sum() / (1.0 * len(epoch_losses))\nfinal_loss = losses.sum() / (1.0 * len(losses))\n'
"        if hasattr(transform, &quot;fit_sample&quot;):\n            pass\n        else:\n            Xt = transform.transform(Xt)\n\nfrom imblearn.pipeline import Pipeline\nmodel = Pipeline([\n        ('sampling', SMOTE()),\n        ('classification', LogisticRegression())\n    ])\n\ngrid = GridSearchCV(model, params, ...)\ngrid.fit(X, y)\n"
"import numpy\nimport matplotlib.pyplot as plt\nimport multipolyfit as mpf\n\ndata = [[1,1],[4,3],[8,3],[11,4],[10,7],[15,11],[16,12]]\nx, y = zip(*data)\nplt.plot(x, y, 'kx')\n\nstacked_x = numpy.array([x,x+1,x-1])\ncoeffs = mpf(stacked_x, y, deg) \nx2 = numpy.arange(min(x)-1, max(x)+1, .01) #use more points for a smoother plot\ny2 = numpy.polyval(coeffs, x2) #Evaluates the polynomial for each x2 value\nplt.plot(x2, y2, label=&quot;deg=3&quot;)\n"
"&gt;&gt;&gt; param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n&gt;&gt;&gt; clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\nGridSearchCV(cv=None,\n             estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n               dual=False, fit_intercept=True, penalty='l2', tol=0.0001),\n             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n"
'from sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\n\n\n# simulate data with unbalanced weights\nX, y = make_classification(n_samples=1000, weights=[0.1, 0.9])\n# your GNB estimator\ngnb = GaussianNB()\ngnb.fit(X, y)\n\ngnb.class_prior_\nOut[168]: array([ 0.105,  0.895])\n\ngnb.get_params()\nOut[169]: {}\n'
'WITH_PYTHON_LAYER := 1\n\nimport caffe\nclass my_py_layer(caffe.Layer):\n  def setup(self, bottom, top):\n    pass\n\n  def reshape(self, bottom, top):\n    pass\n\n  def forward(self, bottom, top):\n    pass\n\n  def backward(self, top, propagate_down, bottom):\n    pass\n\nlayer {\n  name: \'rpn-data\'\n  type: \'Python\'  \n  bottom: \'rpn_cls_score\'\n  bottom: \'gt_boxes\'\n  bottom: \'im_info\'\n  bottom: \'data\'\n  top: \'rpn_labels\'\n  top: \'rpn_bbox_targets\'\n  top: \'rpn_bbox_inside_weights\'\n  top: \'rpn_bbox_outside_weights\'\n  python_param {\n    module: \'rpn.anchor_target_layer\'  # python module name where your implementation is\n    layer: \'AnchorTargetLayer\'   # the name of the class implementation\n    param_str: "\'feat_stride\': 16"   # optional parameters to the layer\n  }\n}\n\nimport caffe\nfrom caffe import layers as L\n\nns = caffe.NetSpec()\n# define layers here...\nns.rpn_labels, ns.rpn_bbox_targets, \\\n  ns.rpn_bbox_inside_weights, ns.rpn_bbox_outside_weights = \\\n    L.Python(ns.rpn_cls_score, ns.gt_boxes, ns.im_info, ns.data, \n             name=\'rpn-data\',\n             ntop=4, # tell caffe to expect four output blobs\n             python_param={\'module\': \'rpn.anchor_target_layer\',\n                           \'layer\': \'AnchorTargetLayer\',\n                           \'param_str\': \'"\\\'feat_stride\\\': 16"\'})\n\nPYTHONPATH=/path/to:$PYTHONPATH $CAFFE_ROOT/build/tools/caffe train -solver my_solver.prototxt\n\nimport numpy as np\nfrom test_gradient_for_python_layer import test_gradient_for_python_layer\n\n# set the inputs\ninput_names_and_values = [(\'in_cont\', np.random.randn(3,4)), \n                          (\'in_binary\', np.random.binomial(1, 0.4, (3,1))]\noutput_names = [\'out1\', \'out2\']\npy_module = \'folder.my_layer_module_name\'\npy_layer = \'my_layer_class_name\'\nparam_str = \'some params\'\npropagate_down = [True, False]\n\n# call the test\ntest_gradient_for_python_layer(input_names_and_values, output_names, \n                               py_module, py_layer, param_str, \n                               propagate_down)\n\n# you are done!\n'
'def gen_with_norm(gen, normalize):\n    for x, y in gen:\n        yield normalize(x), y\n'
'from sklearn.metrics import silhouette_score as sc\n\ndef cv_silhouette_scorer(estimator, X):\n    estimator.fit(X)\n    cluster_labels = estimator.labels_\n    num_labels = len(set(cluster_labels))\n    num_samples = len(X.index)\n    if num_labels == 1 or num_labels == num_samples:\n        return -1\n    else:\n        return sc(X, cluster_labels)\n\ncv = [(slice(None), slice(None))]\ngs = GridSearchCV(estimator=sklearn.cluster.MeanShift(), param_grid=param_dict, \n                  scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\ngs.fit(df[cols_of_interest])\n'
'min_score_thresh = 0.5\nprint([category_index.get(i) for i in classes[0] if scores[0, i] &gt; min_score_thresh)\n'
'def cross_entropy(predictions, targets, epsilon=1e-12):\n    """\n    Computes cross entropy between targets (encoded as one-hot vectors)\n    and predictions. \n    Input: predictions (N, k) ndarray\n           targets (N, k) ndarray        \n    Returns: scalar\n    """\n    predictions = np.clip(predictions, epsilon, 1. - epsilon)\n    N = predictions.shape[0]\n    ce = -np.sum(targets*np.log(predictions+1e-9))/N\n    return ce\n\npredictions = np.array([[0.25,0.25,0.25,0.25],\n                        [0.01,0.01,0.01,0.96]])\ntargets = np.array([[0,0,0,1],\n                   [0,0,0,1]])\nans = 0.71355817782  #Correct answer\nx = cross_entropy(predictions, targets)\nprint(np.isclose(x,ans))\n'
'import numpy as np\n...\nmodel.fit(np.array(train_X),np.array(train_Y), epochs=20, batch_size=10)\n'
"import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import Normalizer,StandardScaler\nfrom sklearn.preprocessing.data import normalize\n\nn=50\nx1 = np.random.normal(0, 2, size=n)\nx2 = np.random.normal(0, 2, size=n)\nnoise = np.random.normal(0, 1, size=n)\ny = 5 + 0.5*x1 + 2.5*x2 + noise\n\nfig,ax=plt.subplots(1,4,figsize=(20,6))\n\nax[0].scatter(x1,x2,c=y)\nax[0].set_title('raw_data',size=15)\n\nX = np.column_stack((x1,x2))\n\ncolumn_normalized=normalize(X, axis=0)\nax[1].scatter(column_normalized[:,0],column_normalized[:,1],c=y)\nax[1].set_title('column_normalized data',size=15)\n\nrow_normalized=Normalizer().fit_transform(X)\nax[2].scatter(row_normalized[:,0],row_normalized[:,1],c=y)\nax[2].set_title('row_normalized data',size=15)\n\nstandardized_data=StandardScaler().fit_transform(X)\nax[3].scatter(standardized_data[:,0],standardized_data[:,1],c=y)\nax[3].set_title('standardized data',size=15)\n\nplt.subplots_adjust(left=0.3, bottom=None, right=0.9, top=None, wspace=0.3, hspace=None)\nplt.show()\n"
'n_nodes = clf.tree_.node_count\nchildren_left = clf.tree_.children_left\nchildren_right = clf.tree_.children_right\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\n\ndef find_path(node_numb, path, x):\n        path.append(node_numb)\n        if node_numb == x:\n            return True\n        left = False\n        right = False\n        if (children_left[node_numb] !=-1):\n            left = find_path(children_left[node_numb], path, x)\n        if (children_right[node_numb] !=-1):\n            right = find_path(children_right[node_numb], path, x)\n        if left or right :\n            return True\n        path.remove(node_numb)\n        return False\n\n\ndef get_rule(path, column_names):\n    mask = \'\'\n    for index, node in enumerate(path):\n        #We check if we are not in the leaf\n        if index!=len(path)-1:\n            # Do we go under or over the threshold ?\n            if (children_left[node] == path[index+1]):\n                mask += "(df[\'{}\']&lt;= {}) \\t ".format(column_names[feature[node]], threshold[node])\n            else:\n                mask += "(df[\'{}\']&gt; {}) \\t ".format(column_names[feature[node]], threshold[node])\n    # We insert the &amp; at the right places\n    mask = mask.replace("\\t", "&amp;", mask.count("\\t") - 1)\n    mask = mask.replace("\\t", "")\n    return mask\n\n# Leaves\nleave_id = clf.apply(X_test)\n\npaths ={}\nfor leaf in np.unique(leave_id):\n    path_leaf = []\n    find_path(0, path_leaf, leaf)\n    paths[leaf] = np.unique(np.sort(path_leaf))\n\nrules = {}\nfor key in paths:\n    rules[key] = get_rule(paths[key], pima.columns)\n\nrules =\n{3: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&lt;= 26.450000762939453) &amp; (df[\'bp\']&lt;= 9.100000381469727)  ",\n 4: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&lt;= 26.450000762939453) &amp; (df[\'bp\']&gt; 9.100000381469727)  ",\n 6: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&gt; 26.450000762939453) &amp; (df[\'skin\']&lt;= 27.5)  ",\n 7: "(df[\'insulin\']&lt;= 127.5) &amp; (df[\'bp\']&gt; 26.450000762939453) &amp; (df[\'skin\']&gt; 27.5)  ",\n 10: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&lt;= 28.149999618530273) &amp; (df[\'insulin\']&lt;= 145.5)  ",\n 11: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&lt;= 28.149999618530273) &amp; (df[\'insulin\']&gt; 145.5)  ",\n 13: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&gt; 28.149999618530273) &amp; (df[\'insulin\']&lt;= 158.5)  ",\n 14: "(df[\'insulin\']&gt; 127.5) &amp; (df[\'bp\']&gt; 28.149999618530273) &amp; (df[\'insulin\']&gt; 158.5)  "}\n'
"In [72]: c=(a.view(np.ubyte)-96).astype('int32')\n\nIn [73]: print(c,c.dtype)\n(array([1, 2, 3, 1, 2, 3]), dtype('int32'))\n"
'&gt;&gt;&gt; from sklearn import cross_validation\n&gt;&gt;&gt; X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n&gt;&gt;&gt; y = np.array([0, 0, 1, 1])\n&gt;&gt;&gt; skf = cross_validation.StratifiedKFold(y, n_folds=2)\n&gt;&gt;&gt; len(skf)\n2\n&gt;&gt;&gt; print(skf)  \nsklearn.cross_validation.StratifiedKFold(labels=[0 0 1 1], n_folds=2,\n                                         shuffle=False, random_state=None)\n&gt;&gt;&gt; for train_index, test_index in skf:\n...    print("TRAIN:", train_index, "TEST:", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [1 3] TEST: [0 2]\nTRAIN: [0 2] TEST: [1 3]\n'
"In [18]: import numpy as np\n\nIn [19]: from sklearn.preprocessing import LabelBinarizer\n\nIn [20]: lb = LabelBinarizer()\n\nIn [21]: label = lb.fit_transform(['yes', 'no', 'no', 'yes'])\n\nIn [22]: label = np.hstack((label, 1 - label))\n\nIn [23]: label\nOut[23]:\narray([[1, 0],\n       [0, 1],\n       [0, 1],\n       [1, 0]])\n\nIn [24]: lb.inverse_transform(label[:, 0])\nOut[24]:\narray(['yes', 'no', 'no', 'yes'],\n      dtype='&lt;U3')\n\nfrom sklearn.preprocessing import LabelBinarizer\nimport numpy as np\n\nclass MyLabelBinarizer(LabelBinarizer):\n    def transform(self, y):\n        Y = super().transform(y)\n        if self.y_type_ == 'binary':\n            return np.hstack((Y, 1-Y))\n        else:\n            return Y\n\n    def inverse_transform(self, Y, threshold=None):\n        if self.y_type_ == 'binary':\n            return super().inverse_transform(Y[:, 0], threshold)\n        else:\n            return super().inverse_transform(Y, threshold)\n\nlb = MyLabelBinarizer()\nlabel1 = lb.fit_transform(['yes', 'no', 'no', 'yes'])\nprint(label1)\nprint(lb.inverse_transform(label1))\nlabel2 = lb.fit_transform(['yes', 'no', 'no', 'yes', 'maybe'])\nprint(label2)\nprint(lb.inverse_transform(label2))\n\n[[1 0]\n [0 1]\n [0 1]\n [1 0]]\n['yes' 'no' 'no' 'yes']\n[[0 0 1]\n [0 1 0]\n [0 1 0]\n [0 0 1]\n [1 0 0]]\n['yes' 'no' 'no' 'yes' 'maybe']\n"
'&gt;&gt;&gt; from sklearn.preprocessing import MultiLabelBinarizer\n&gt;&gt;&gt; y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]\n&gt;&gt;&gt; MultiLabelBinarizer().fit_transform(y)\narray([[0, 0, 1, 1, 1],\n       [0, 0, 1, 0, 0],\n       [1, 1, 0, 1, 0],\n       [1, 1, 1, 1, 1],\n       [1, 1, 1, 0, 0]])\n'
'loss = tf.cast(loss, tf.float32)\n'
"...\nfrom keras.models import load_model\nfrom sklearn.externals import joblib\n\n...\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('estimator', KerasRegressor(build_model))\n])\n\npipeline.fit(X_train, y_train)\n\n# Save the Keras model first:\npipeline.named_steps['estimator'].model.save('keras_model.h5')\n\n# This hack allows us to save the sklearn pipeline:\npipeline.named_steps['estimator'].model = None\n\n# Finally, save the pipeline:\njoblib.dump(pipeline, 'sklearn_pipeline.pkl')\n\ndel pipeline\n\n# Load the pipeline first:\npipeline = joblib.load('sklearn_pipeline.pkl')\n\n# Then, load the Keras model:\npipeline.named_steps['estimator'].model = load_model('keras_model.h5')\n\ny_pred = pipeline.predict(X_test)\n"
'from sklearn.linear_model import SGDRegressor\nimport numpy as np\n\nnp.random.seed(0)\nX = np.linspace(-1, 1, num=50).reshape(-1, 1)\nY = (X * 1.5 + 2).reshape(50,)\n\nmodelFit = SGDRegressor(learning_rate="adaptive", eta0=0.01, random_state=0, verbose=1,\n                     shuffle=True, max_iter=2000, tol=1e-3, warm_start=True)\nmodelPartialFit = SGDRegressor(learning_rate="adaptive", eta0=0.01, random_state=0, verbose=1,\n                     shuffle=True, max_iter=2000, tol=1e-3, warm_start=False)\n# first fit some data\nmodelFit.fit(X, Y)\nmodelPartialFit.fit(X, Y)\n# for both: Convergence after 50 epochs, Norm: 1.46, NNZs: 1, Bias: 2.000027, T: 2500, Avg. loss: 0.000237\nprint(modelFit.coef_, modelPartialFit.coef_) # for both: [1.46303288]\n\n# now fit new data (zeros)\nnewX = X\nnewY = 0 * Y\n\n# fits only for 1 epoch, Norm: 1.23, NNZs: 1, Bias: 1.208630, T: 50, Avg. loss: 1.595492:\nmodelPartialFit.partial_fit(newX, newY)\n\n# Convergence after 49 epochs, Norm: 0.04, NNZs: 1, Bias: 0.000077, T: 2450, Avg. loss: 0.000313:\nmodelFit.fit(newX, newY)\n\nprint(modelFit.coef_, modelPartialFit.coef_) # [0.04245779] vs. [1.22919864]\nnewX = np.reshape([2], (-1, 1))\nprint(modelFit.predict(newX), modelPartialFit.predict(newX)) # [0.08499296] vs. [3.66702685]\n'
'from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nX, y = df[[\'NumberofEmployees\',\'ValueofContract\']], df.AverageNumberofTickets\nmodel.fit(X, y)\n\n# compute with formulas from the theory\nyhat = model.predict(X)\nSS_Residual = sum((y-yhat)**2)       \nSS_Total = sum((y-np.mean(y))**2)     \nr_squared = 1 - (float(SS_Residual))/SS_Total\nadjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\nprint r_squared, adjusted_r_squared\n# 0.877643371323 0.863248473832\n\n# compute with sklearn linear_model, although could not find any function to compute adjusted-r-square directly from documentation\nprint model.score(X, y), 1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)\n# 0.877643371323 0.863248473832 \n\n# compute with statsmodels, by adding intercept manually\nimport statsmodels.api as sm\nX1 = sm.add_constant(X)\nresult = sm.OLS(y, X1).fit()\n#print dir(result)\nprint result.rsquared, result.rsquared_adj\n# 0.877643371323 0.863248473832\n\n# compute with statsmodels, another way, using formula\nimport statsmodels.formula.api as sm\nresult = sm.ols(formula="AverageNumberofTickets ~ NumberofEmployees + ValueofContract", data=df).fit()\n#print result.summary()\nprint result.rsquared, result.rsquared_adj\n# 0.877643371323 0.863248473832\n'
"# Save optimizer weights.\nsymbolic_weights = getattr(model.optimizer, 'weights')\nif symbolic_weights:\n    optimizer_weights_group = f.create_group('optimizer_weights')\n    weight_values = K.batch_get_value(symbolic_weights)\n\n# Set optimizer weights.\nif 'optimizer_weights' in f:\n    # Build train function (to get weight updates).\n    if isinstance(model, Sequential):\n        model.model._make_train_function()\n    else:\n        model._make_train_function()\n\n    # ...\n\n    try:\n        model.optimizer.set_weights(optimizer_weight_values)\n\nX, y = np.random.rand(100, 50), np.random.randint(2, size=100)\nx = Input((50,))\nout = Dense(1, activation='sigmoid')(x)\nmodel = Model(x, out)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\nmodel.fit(X, y, epochs=5)\n\nEpoch 1/5\n100/100 [==============================] - 0s 4ms/step - loss: 0.7716\nEpoch 2/5\n100/100 [==============================] - 0s 64us/step - loss: 0.7678\nEpoch 3/5\n100/100 [==============================] - 0s 82us/step - loss: 0.7665\nEpoch 4/5\n100/100 [==============================] - 0s 56us/step - loss: 0.7647\nEpoch 5/5\n100/100 [==============================] - 0s 76us/step - loss: 0.7638\n\nmodel.save_weights('weights.h5')\nsymbolic_weights = getattr(model.optimizer, 'weights')\nweight_values = K.batch_get_value(symbolic_weights)\nwith open('optimizer.pkl', 'wb') as f:\n    pickle.dump(weight_values, f)\n\nx = Input((50,))\nout = Dense(1, activation='sigmoid')(x)\nmodel = Model(x, out)\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\nmodel.load_weights('weights.h5')\nmodel._make_train_function()\nwith open('optimizer.pkl', 'rb') as f:\n    weight_values = pickle.load(f)\nmodel.optimizer.set_weights(weight_values)\n\nmodel.fit(X, y, epochs=5)\n\nEpoch 1/5\n100/100 [==============================] - 0s 674us/step - loss: 0.7629\nEpoch 2/5\n100/100 [==============================] - 0s 49us/step - loss: 0.7617\nEpoch 3/5\n100/100 [==============================] - 0s 49us/step - loss: 0.7611\nEpoch 4/5\n100/100 [==============================] - 0s 55us/step - loss: 0.7601\nEpoch 5/5\n100/100 [==============================] - 0s 49us/step - loss: 0.7594\n"
'# Instead of this . . . \ndef softmax(x, axis = 0):\n    return np.exp(x) / np.sum(np.exp(x), axis = axis, keepdims = True)\n\n# Do this\ndef softmax(x, axis = 0):\n    e_x = np.exp(x - np.max(x, axis = axis, keepdims = True))\n    return e_x / e_x.sum(axis, keepdims = True)\n\ndef rbf_kernel_safe(X, Y=None, gamma=None): \n\n      X, Y = sklearn.metrics.pairwise.check_pairwise_arrays(X, Y) \n      if gamma is None: \n          gamma = 1.0 / X.shape[1] \n\n      K = sklearn.metrics.pairwise.euclidean_distances(X, Y, squared=True) \n      K *= -gamma \n      K -= K.max()\n      np.exp(K, K)    # exponentiate K in-place \n      return K \n\nLabelPropagation(kernel = rbf_kernel_safe, tol = 0.01, gamma = 20).fit(X, Y)\n\nw = exp(-d**2/sigma**2)  # Equation (1)\n'
'import torch\n \nlabels = torch.tensor([1, 2, 3, 5])\none_hot = torch.zeros(4, 6)\none_hot[torch.arange(4), labels] = 1\n \nreverted = torch.argmax(one_hot, dim=1)\nassert (labels == reverted).all().item()\n'
'&gt;&gt;&gt; from sklearn import svm\n&gt;&gt;&gt; from sklearn import datasets\n&gt;&gt;&gt; clf = svm.SVC()\n&gt;&gt;&gt; iris = datasets.load_iris()\n&gt;&gt;&gt; X, y = iris.data, iris.target\n&gt;&gt;&gt; clf.fit(X, y)\nSVC(kernel=’rbf’, C=1.0, probability=False, degree=3, coef0=0.0, eps=0.001,\ncache_size=100.0, shrinking=True, gamma=0.00666666666667)\n&gt;&gt;&gt; import pickle\n&gt;&gt;&gt; s = pickle.dumps(clf)\n&gt;&gt;&gt; clf2 = pickle.loads(s)\n&gt;&gt;&gt; clf2.predict(X[0])\narray([ 0.])\n&gt;&gt;&gt; y[0]\n0\n\n&gt;&gt;&gt; from sklearn.externals import joblib\n&gt;&gt;&gt; joblib.dump(clf, ’filename.pkl’)\n'
'10           chased     \n\n11           dog        \n11           mouse      \n11           cat        \n\n110           dog        \n\n111           mouse      \n111           cat        \n'
"def most_informative_feature_for_class_svm(vectorizer, classifier,  classlabel, n=10):\n    labelid = ?? # this is the coef we're interested in. \n    feature_names = vectorizer.get_feature_names()\n    svm_coef = classifier.coef_.toarray() \n    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print feat, coef\n\nimport codecs, re, time\nfrom itertools import chain\n\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntrainfile = 'train.txt'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = CountVectorizer(analyzer='word')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,'r','utf8'))\ntags = ['bs','pt','es','sr']\n\n# Training NB\nmnb = MultinomialNB()\nmnb.fit(trainset, tags)\n\nfrom sklearn.svm import SVC\nsvcc = SVC(kernel='linear', C=1)\nsvcc.fit(trainset, tags)\n\ndef most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):\n    labelid = list(classifier.classes_).index(classlabel)\n    feature_names = vectorizer.get_feature_names()\n    topn = sorted(zip(classifier.coef_[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print classlabel, feat, coef\n\ndef most_informative_feature_for_class_svm(vectorizer, classifier,  n=10):\n    labelid = 3 # this is the coef we're interested in. \n    feature_names = vectorizer.get_feature_names()\n    svm_coef = classifier.coef_.toarray() \n    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]\n\n    for coef, feat in topn:\n        print feat, coef\n\nmost_informative_feature_for_class(word_vectorizer, mnb, 'pt')\nprint \nmost_informative_feature_for_class_svm(word_vectorizer, svcc)\n\npt teve -4.63472898823\npt tive -4.63472898823\npt todas -4.63472898823\npt vida -4.63472898823\npt de -4.22926388012\npt foi -4.22926388012\npt mais -4.22926388012\npt me -4.22926388012\npt as -3.94158180767\npt que -3.94158180767\n\nno 0.0204081632653\nparecer 0.0204081632653\npone 0.0204081632653\npor 0.0204081632653\nrelación 0.0204081632653\nuna 0.0204081632653\nvisto 0.0204081632653\nya 0.0204081632653\nes 0.0408163265306\nlo 0.0408163265306\n"
"df['Age_fill'][(df.Age.isnull()) &amp; (df.Gender == i) &amp; (df.Pclass == j+1)] \\\n                                                          = median_ages[i,j]\n"
'X = X[:, None]\n'
'# see question for code prior to "color mapping"\n\n# Color mapping\ndflt_col = "#808080"   # Unclustered gray\nD_leaf_colors = {"attr_1": dflt_col,\n\n                 "attr_4": "#B061FF", # Cluster 1 indigo\n                 "attr_5": "#B061FF",\n                 "attr_2": "#B061FF",\n                 "attr_8": "#B061FF",\n                 "attr_6": "#B061FF",\n                 "attr_7": "#B061FF",\n\n                 "attr_0": "#61ffff", # Cluster 2 cyan\n                 "attr_3": "#61ffff",\n                 "attr_9": "#61ffff",\n                 }\n\n# notes:\n# * rows in Z correspond to "inverted U" links that connect clusters\n# * rows are ordered by increasing distance\n# * if the colors of the connected clusters match, use that color for link\nlink_cols = {}\nfor i, i12 in enumerate(Z[:,:2].astype(int)):\n  c1, c2 = (link_cols[x] if x &gt; len(Z) else D_leaf_colors["attr_%d"%x]\n    for x in i12)\n  link_cols[i+1+len(Z)] = c1 if c1 == c2 else dflt_col\n\n# Dendrogram\nD = dendrogram(Z=Z, labels=DF_dism.index, color_threshold=None,\n  leaf_font_size=12, leaf_rotation=45, link_color_func=lambda x: link_cols[x])\n'
'# input dataset\nX = np.array([ [0,0,1],\n               [0,0,1],\n               [0,1,0],\n               [0,1,0]])\n\n# init weights randomly with mean 0\nweights0 = 2 * np.random.random((3,1)) - 1\n'
'forward_propgation_results = Process.forward_propagation(images)\n'
'from livelossplot import PlotLossesKeras\n\nmodel.fit(X_train, Y_train,\n          epochs=10,\n          validation_data=(X_test, Y_test),\n          callbacks=[PlotLossesKeras()],\n          verbose=0)\n'
'from keras.layers import *\n\nmergedOut = Add()([model1.output,model2.output])\n    #Add() -&gt; creates a merge layer that sums the inputs\n    #The second parentheses "calls" the layer with the output tensors of the two models\n    #it will demand that both model1 and model2 have the same output shape\n\nmergedOut = Flatten()(mergedOut)    \nmergedOut = Dense(256, activation=\'relu\')(mergedOut)\nmergedOut = Dropout(.5)(mergedOut)\nmergedOut = Dense(128, activation=\'relu\')(mergedOut)\nmergedOut = Dropout(.35)(mergedOut)\n\n# output layer\nmergedOut = Dense(5, activation=\'softmax\')(mergedOut)\n\nfrom keras.models import Model\n\nnewModel = Model([model1.input,model2.input], mergedOut)\n    #use lists if you want more than one input or output    \n\nnewModel.fit([X_train_1, X_train_2], Y_train, ....)    \n\ncommonInput = Input(input_shape)\n\nout1 = model1(commonInput)    \nout2 = model2(commonInput)    \n\nmergedOut = Add()([out1,out2])\n\noneInputModel = Model(commonInput,mergedOut)\n'
"a=np.matrix([[1,2],[3,4]], dtype='float')\n"
"import numpy\nimport tensorflow as tf\n\nX = numpy.zeros([157, 128])\nY = numpy.zeros([157], dtype=numpy.int32)\nexample_id = numpy.array(['%d' % i for i in range(len(Y))])\n\nx_column_name = 'x'\nexample_id_column_name = 'example_id'\n\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={x_column_name: X, example_id_column_name: example_id},\n    y=Y,\n    num_epochs=None,\n    shuffle=True)\n\nsvm = tf.contrib.learn.SVM(\n    example_id_column=example_id_column_name,\n    feature_columns=(tf.contrib.layers.real_valued_column(\n        column_name=x_column_name, dimension=128),),\n    l2_regularization=0.1)\n\nsvm.fit(input_fn=train_input_fn, steps=10)\n"
'x = Flatten()(x)  # first the layer is constructed and then it is called on x\n\nflatten_layer = Flatten()  # instantiate the layer\nx = flatten_layer(x)       # call it on the given tensor\n'
"model.fit(X_train, y_train, validation_data=[X_train.to_numpy(), y_train.to_numpy()], \nepochs=10, batch_size=64)\n\nEpoch 1/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.7898 - accuracy: 0.6087 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 2/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6710 - accuracy: 0.6500 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 3/10\n8/8 [==============================] - 0s 5ms/step - loss: 0.6748 - accuracy: 0.6500 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 4/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6716 - accuracy: 0.6370 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 5/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6085 - accuracy: 0.6326 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 6/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6744 - accuracy: 0.6326 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 7/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6102 - accuracy: 0.6522 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 8/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.7032 - accuracy: 0.6109 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 9/10\n8/8 [==============================] - 0s 5ms/step - loss: 0.6283 - accuracy: 0.6717 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\nEpoch 10/10\n8/8 [==============================] - 0s 5ms/step - loss: 0.6120 - accuracy: 0.6652 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n\n...\n...\n        # Run validation.\n        if validation_data and self._should_eval(epoch, validation_freq):\n          val_x, val_y, val_sample_weight = (\n              data_adapter.unpack_x_y_sample_weight(validation_data))\n          val_logs = self.evaluate(\n              x=val_x,\n              y=val_y,\n              sample_weight=val_sample_weight,\n              batch_size=validation_batch_size or batch_size,\n              steps=validation_steps,\n              callbacks=callbacks,\n              max_queue_size=max_queue_size,\n              workers=workers,\n              use_multiprocessing=use_multiprocessing,\n              return_dict=True)\n          val_logs = {'val_' + name: val for name, val in val_logs.items()}\n          epoch_logs.update(val_logs)\n\ndef unpack_x_y_sample_weight(data):\n  &quot;&quot;&quot;Unpacks user-provided data tuple.&quot;&quot;&quot;\n  if not isinstance(data, tuple):\n    return (data, None, None)\n  elif len(data) == 1:\n    return (data[0], None, None)\n  elif len(data) == 2:\n    return (data[0], data[1], None)\n  elif len(data) == 3:\n    return (data[0], data[1], data[2])\n\n  raise ValueError(&quot;Data not understood.&quot;)\n\n\nmodel.fit(X_train, y_train, validation_data=(X_train.to_numpy(), y_train.to_numpy()), \nepochs=10, batch_size=64)\n\nEpoch 1/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5832 - accuracy: 0.6696 - val_loss: 0.6892 - val_accuracy: 0.6674\nEpoch 2/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.6385 - accuracy: 0.6804 - val_loss: 0.8984 - val_accuracy: 0.5565\nEpoch 3/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.6822 - accuracy: 0.6391 - val_loss: 0.6556 - val_accuracy: 0.6739\nEpoch 4/10\n8/8 [==============================] - 0s 6ms/step - loss: 0.6276 - accuracy: 0.6609 - val_loss: 1.0691 - val_accuracy: 0.5630\nEpoch 5/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.7048 - accuracy: 0.6239 - val_loss: 0.6474 - val_accuracy: 0.6326\nEpoch 6/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.6500 - val_loss: 0.6659 - val_accuracy: 0.6043\nEpoch 7/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5796 - accuracy: 0.6913 - val_loss: 0.6891 - val_accuracy: 0.6435\nEpoch 8/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5915 - accuracy: 0.6891 - val_loss: 0.5307 - val_accuracy: 0.7152\nEpoch 9/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.5571 - accuracy: 0.7000 - val_loss: 0.5465 - val_accuracy: 0.6957\nEpoch 10/10\n8/8 [==============================] - 0s 7ms/step - loss: 0.7133 - accuracy: 0.6283 - val_loss: 0.7046 - val_accuracy: 0.6413\n"
'model = LassoCV()\nmodel.fit(DF_X,SR_y)\n\nimport tensorflow as tf\nx = tf.placeholder("float", [4, 3])      \ny_ = tf.placeholder("float",[4])\n\nW = tf.Variable(tf.zeros([3,1]))\nb = tf.Variable(tf.zeros([1]))\n\ny=tf.matmul(x,W) + b\n\nloss=tf.reduce_sum(tf.square(y_ - y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)       \nsess.run(train_step, feed_dict={x:np.asarray(DF_X),y_:np.asarray(SR_y)})\n\nsess.run(loss,feed_dict={x:np.asarray(DF_X),y_:np.asarray(SR_y)}) \n'
"df.reset_index(inplace=True)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.set_index('Date')\ns=sm.tsa.seasonal_decompose(df.divida)\n\n&lt;statsmodels.tsa.seasonal.DecomposeResult object at 0x110ec3710&gt;\n\ns.resid\ns.seasonal\ns.trend\n"
'def seasonal_decompose(x, model="additive", filt=None, freq=None):\n    """\n    Parameters\n    ----------\n    x : array-like\n        Time series\n    model : str {"additive", "multiplicative"}\n        Type of seasonal component. Abbreviations are accepted.\n    filt : array-like\n        The filter coefficients for filtering out the seasonal component.\n        The default is a symmetric moving average.\n    freq : int, optional\n        Frequency of the series. Must be used if x is not a pandas\n        object with a timeseries index.\n\nlength = 400\nx = np.sin(np.arange(length)) * 10 + np.random.randn(length)\ndf = pd.DataFrame(data=x, index=pd.date_range(start=datetime(2015, 1, 1), periods=length, freq=\'w\'), columns=[\'value\'])\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 400 entries, 2015-01-04 to 2022-08-28\nFreq: W-SUN\n\ndecomp = sm.tsa.seasonal_decompose(df)\ndata = pd.concat([df, decomp.trend, decomp.seasonal, decomp.resid], axis=1)\ndata.columns = [\'series\', \'trend\', \'seasonal\', \'resid\']\n\nData columns (total 4 columns):\nseries      400 non-null float64\ntrend       348 non-null float64\nseasonal    400 non-null float64\nresid       348 non-null float64\ndtypes: float64(4)\nmemory usage: 15.6 KB\n\ndf = df.iloc[np.unique(np.random.randint(low=0, high=length, size=length * .8))]\n\n&lt;class \'pandas.core.frame.DataFrame\'&gt;\nDatetimeIndex: 222 entries, 2015-01-11 to 2022-08-21\nData columns (total 1 columns):\nvalue    222 non-null float64\ndtypes: float64(1)\nmemory usage: 3.5 KB\n\ndf.index.freq\n\nNone\n\ndf.index.inferred_freq\n\nNone\n\ndecomp = sm.tsa.seasonal_decompose(df, freq=52)\n\ndata = pd.concat([df, decomp.trend, decomp.seasonal, decomp.resid], axis=1)\ndata.columns = [\'series\', \'trend\', \'seasonal\', \'resid\']\n\nDatetimeIndex: 224 entries, 2015-01-04 to 2022-08-07\nData columns (total 4 columns):\nseries      224 non-null float64\ntrend       172 non-null float64\nseasonal    224 non-null float64\nresid       172 non-null float64\ndtypes: float64(4)\nmemory usage: 8.8 KB\n\nNotes\n-----\nThis is a naive decomposition. More sophisticated methods should\nbe preferred.\n\nThe additive model is Y[t] = T[t] + S[t] + e[t]\n\nThe multiplicative model is Y[t] = T[t] * S[t] * e[t]\n\nThe seasonal component is first removed by applying a convolution\nfilter to the data. The average of this smoothed series for each\nperiod is the returned seasonal component.\n'
"input = tf.placeholder(tf.float32)\nfilter = tf.Variable(tf.truncated_normal([5, 5, 3, 32], stddev=0.1)\nconv = tf.nn.conv2d(input, filter, strides=[1, 1, 1, 1], padding='SAME')\n\nresult = sess.run(conv, feed_dict={input: ...})  # &lt;== Execution happens here.\n"
'if len(validation_images) == 0:\n   validation_images.append(base_name)\nelif percentage_hash &lt; validation_percentage:\n'
'state_placeholder = tf.placeholder(tf.float32, [num_layers, 2, batch_size, state_size])\nl = tf.unpack(state_placeholder, axis=0)\nrnn_tuple_state = tuple(\n         [tf.nn.rnn_cell.LSTMStateTuple(l[idx][0],l[idx][1])\n          for idx in range(num_layers)]\n)\n\ncell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\ncell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n\noutputs, state = tf.nn.dynamic_rnn(cell, series_batch_input, initial_state=rnn_tuple_state)\n'
'incorrects = np.nonzero(model.predict_class(X_test).reshape((-1,)) != y_test)\n'
'def balanced_flow_from_directory(flow_from_directory, options):\n    for x, y in flow_from_directory:\n         yield custom_balance(x, y, options)\n'
'image = tf.image.decode_jpeg(...\nimage = tf.cast(image, tf.float32)\n'
'pip uninstall tensorflow\npip install tensorflow==1.5.0\n'
'from sklearn.preprocessing import StandardScaler\n\nscalers = {}\nfor i in range(X_train.shape[1]):\n    scalers[i] = StandardScaler()\n    X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n\nfor i in range(X_test.shape[1]):\n    X_test[:, i, :] = scalers[i].transform(X_test[:, i, :]) \n'
'from sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\nlb.fit(y_train)\n\naccuracy = cross_val_score(classifier, X_train, y_train, cv=10)\n'
'import numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\n\nX_train = np.array(["new york is a hell of a town",\n                "new york was originally dutch",\n                "the big apple is great",\n                "new york is also called the big apple",\n                "nyc is nice",\n                "people abbreviate new york city as nyc",\n                "the capital of great britain is london",\n                "london is in the uk",\n                "london is in england",\n                "london is in great britain",\n                "it rains a lot in london",\n                "london hosts the british museum",\n                "new york is great and so is london",\n                "i like london better than new york"])\ny_train_text = [["new york"],["new york"],["new york"],["new york"],    \n                ["new york"],["new york"],["london"],["london"],         \n                ["london"],["london"],["london"],["london"],\n                ["new york","England"],["new york","london"]]\n\nX_test = np.array([\'nice day in nyc\',\n               \'welcome to london\',\n               \'london is rainy\',\n               \'it is raining in britian\',\n               \'it is raining in britian and the big apple\',\n               \'it is raining in britian and nyc\',\n               \'hello welcome to new york. enjoy it here and london too\'])\n\ny_test_text = [["new york"],["new york"],["new york"],["new york"],["new york"],["new york"],["new york"]]\n\n\nlb = preprocessing.MultiLabelBinarizer(classes=("new york","london","England"))\nY = lb.fit_transform(y_train_text)\nY_test = lb.fit_transform(y_test_text)\n\nprint Y_test\n\nclassifier = Pipeline([\n(\'vectorizer\', CountVectorizer()),\n(\'tfidf\', TfidfTransformer()),\n(\'clf\', OneVsRestClassifier(LinearSVC()))])\n\nclassifier.fit(X_train, Y)\npredicted = classifier.predict(X_test)\nprint predicted\n\nprint "Accuracy Score: ",accuracy_score(Y_test, predicted)\n\nAccuracy Score:  0.571428571429\n\ny_train_text = [["new york"],["new york"],["new york"],\n                ["new york"],["new york"],["new york"],\n                ["london"],["london"],["london"],["london"],\n                ["london"],["london"],["new york","England"],\n                ["new york","london"]]\n\nlb = preprocessing.MultiLabelBinarizer(classes=("new york","london","England"))\n'
'from fs.osfs import OSFS\nfolder = OSFS(FLAGS.test_dir)\ntest_n = len(list(n for n in folder.listdir() if n.startswith(\'test\')))\nthis_test = FLAGS.test_dir+"/test" + str(test_n+1)\ntest_writer = tf.train.SummaryWriter(this_test)\n'
"import tensorflow as tf \n\nx = tf.Variable(tf.zeros([1]))\nsaver = tf.train.Saver(max_to_keep=10)\nsess = tf.Session()\n\nfor i in range(10):\n  sess.run(tf.initialize_all_variables())\n  saver.save(sess, '/home/eneskocabey/Desktop/model' + str(i))\n"
'init = tf.placeholder(tf.float32, shape=(), name="init")\n'
'&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; hypothesis = [\'This\', \'is\', \'cat\'] \n&gt;&gt;&gt; reference = [\'This\', \'is\', \'a\', \'cat\']\n&gt;&gt;&gt; references = [reference] # list of references for 1 sentence.\n&gt;&gt;&gt; list_of_references = [references] # list of references for all sentences in corpus.\n&gt;&gt;&gt; list_of_hypotheses = [hypothesis] # list of hypotheses that corresponds to list of references.\n&gt;&gt;&gt; nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypotheses)\n0.6025286104785453\n&gt;&gt;&gt; nltk.translate.bleu_score.sentence_bleu(references, hypothesis)\n0.6025286104785453\n\ndef sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n                  smoothing_function=None):\n    return corpus_bleu([references], [hypothesis], weights, smoothing_function)\n\n def sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25),\n                      smoothing_function=None):\n    """"\n    :param references: reference sentences\n    :type references: list(list(str))\n    :param hypothesis: a hypothesis sentence\n    :type hypothesis: list(str)\n    :param weights: weights for unigrams, bigrams, trigrams and so on\n    :type weights: list(float)\n    :return: The sentence-level BLEU score.\n    :rtype: float\n    """\n\nreferences = [ ["This", "is", "a", "cat"], ["This", "is", "a", "feline"] ]\nhypothesis = ["This", "is", "cat"]\nsentence_bleu(references, hypothesis)\n\ndef corpus_bleu(list_of_references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25),\n                smoothing_function=None):\n    """\n    :param references: a corpus of lists of reference sentences, w.r.t. hypotheses\n    :type references: list(list(list(str)))\n    :param hypotheses: a list of hypothesis sentences\n    :type hypotheses: list(list(str))\n    :param weights: weights for unigrams, bigrams, trigrams and so on\n    :type weights: list(float)\n    :return: The corpus-level BLEU score.\n    :rtype: float\n    """\n\nfrom nltk.translate import bleu \n\nfrom nltk.translate.bleu_score import sentence_bleu\n\n&gt;&gt;&gt; from nltk.translate import bleu\n&gt;&gt;&gt; from nltk.translate.bleu_score import sentence_bleu\n&gt;&gt;&gt; from nltk.translate.bleu_score import corpus_bleu\n&gt;&gt;&gt; bleu == sentence_bleu\nTrue\n&gt;&gt;&gt; bleu == corpus_bleu\nFalse\n'
'scores=parallel(delayed(_fit_and_score)(clone(estimator),X,y,scorer,\n                                        train,test,verbose,None,fit_params)\n'
"estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, nb_epoch=300, batch_size=16, verbose=0, callbacks=[...your_callbacks...])))\n\nX = dataset[:,:60].astype(float)\n\nmodel.add(Dropout(0.2, input_shape=(33,)))\n\nmodel.add(Dropout(0.2, input_shape=(60,)))\n\nfrom keras.utils.np_utils import to_categorical\nencoded_Y = to_categorical(encoder.transform(Y))\n\nmodel.add(Dense(2, init='normal', activation='softmax'))\n\nmodel.add(Dense(1, init='normal', activation='sigmoid'))\n\nfit_params={'mlp__callbacks':calls}\n\nresults = cross_val_score(pipeline, X, encoded_Y, cv=kfold, fit_params={'mlp__callbacks':calls})\n"
'with tf.Session(graph=tf.Graph()) as sess:\n    tf.saved_model.loader.load(\n        sess,\n        [tf.saved_model.tag_constants.SERVING],\n        "/job/export/Servo/1503723455"\n    )\n\n    prediction = sess.run(\n        \'prefix/predictions/Identity:0\',\n        feed_dict={\n            \'Placeholder:0\': [20.9],\n            \'Placeholder_1:0\': [1.8],\n            \'Placeholder_2:0\': [0.9]\n        }\n    )\n\n    print(prediction)\n'
'neg_class_prob_sorted = NB_optimal.feature_log_prob_[0, :].argsort()\npos_class_prob_sorted = NB_optimal.feature_log_prob_[1, :].argsort()\n\nprint(np.take(count_vect.get_feature_names(), neg_class_prob_sorted[:10]))\nprint(np.take(count_vect.get_feature_names(), pos_class_prob_sorted[:10]))\n\nprint(np.take(count_vect.get_feature_names(), neg_class_prob_sorted[-10:]))\nprint(np.take(count_vect.get_feature_names(), pos_class_prob_sorted[-10:]))\n'
"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n"
'from sklearn.compose import ColumnTransformer \nct = ColumnTransformer([("Name_Of_Your_Step", OneHotEncoder(),[0])], remainder="passthrough")) # The last arg ([0]) is the list of columns you want to transform in this step\nct.fit_transform(X)    \n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n#Encode Country Column\nlabelencoder_X = LabelEncoder()\nX[:,0] = labelencoder_X.fit_transform(X[:,0])\nct = ColumnTransformer([("Country", OneHotEncoder(), [0])], remainder = \'passthrough\')\nX = ct.fit_transform(X)\n'
"from sklearn import svm\n\n# training data [[11.0, 2, 2], [11.3, 2, 2] ... etc]\ntrain_data = my_training_data()\n\n# create and fit the model\nclf = svm.OneClassSVM()\nclf.fit(train_data)\n\nif clf.predict(log_in_data) &lt; 0:\n    fire_alert_event()\nelse:\n    # log-in is not dissimilar to previous attempts\n    print('log in ok')\n"
"import numpy as np\nfrom sklearn import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation, Dropout\n\n# Create some random data\nnp.random.seed(42)\nX = np.random.random((10, 50))\n\n# Similar labels\nlabels = ['good', 'bad', 'soso', 'amazeballs', 'good']\nlabels += labels\nlabels = np.array(labels)\nnp.random.shuffle(labels)\n\n# Change the labels to the required format\nnumericalLabels = preprocessing.LabelEncoder().fit_transform(labels)\nnumericalLabels = numericalLabels.reshape(-1, 1)\ny = preprocessing.OneHotEncoder(sparse=False).fit_transform(numericalLabels)\n\n# Simple Keras model builder\ndef buildModel(nFeatures, nClasses, nLayers=3, nNeurons=10, dropout=0.2):\n    model = Sequential()\n    model.add(Dense(nNeurons, input_dim=nFeatures))\n    model.add(Activation('sigmoid'))\n    model.add(Dropout(dropout))\n    for i in xrange(nLayers-1):\n        model.add(Dense(nNeurons))\n        model.add(Activation('sigmoid'))\n        model.add(Dropout(dropout))\n    model.add(Dense(nClasses))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n\n    return model\n\n# Do an exhaustive search over a given parameter space\nfor nLayers in xrange(2, 4):\n    for nNeurons in xrange(5, 8):\n        model = buildModel(X.shape[1], y.shape[1], nLayers, nNeurons)\n        modelHist = model.fit(X, y, batch_size=32, nb_epoch=10,\n                              validation_split=0.3, shuffle=True, verbose=0)\n        minLoss = min(modelHist.history['val_loss'])\n        epochNum = modelHist.history['val_loss'].index(minLoss)\n        print '{0} layers, {1} neurons best validation at'.format(nLayers, nNeurons),\n        print 'epoch {0} loss = {1:.2f}'.format(epochNum, minLoss)\n\n2 layers, 5 neurons best validation at epoch 0 loss = 1.18\n2 layers, 6 neurons best validation at epoch 0 loss = 1.21\n2 layers, 7 neurons best validation at epoch 8 loss = 1.49\n3 layers, 5 neurons best validation at epoch 9 loss = 1.83\n3 layers, 6 neurons best validation at epoch 9 loss = 1.91\n3 layers, 7 neurons best validation at epoch 9 loss = 1.65\n"
"from matplotlib import pyplot as plt\nimport numpy as np\nimport pymc3 as pm\nimport seaborn as sns\nfrom theano import tensor as T\n\nblue = sns.color_palette()[0]\n\nnp.random.seed(462233) # from random.org\n\nN = 150\n\nCENTROIDS = np.array([0, 10, 50])\nWEIGHTS = np.array([0.4, 0.4, 0.2])\n\nx = np.random.normal(CENTROIDS[np.random.choice(3, size=N, p=WEIGHTS)], size=N)\nx_std = (x - x.mean()) / x.std()\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.hist(x_std, bins=30);\n\nK = 30\n\nwith pm.Model() as model:\n    alpha = pm.Gamma('alpha', 1., 1.)\n    beta = pm.Beta('beta', 1., alpha, shape=K)\n    w = pm.Deterministic('w', beta * T.concatenate([[1], T.extra_ops.cumprod(1 - beta)[:-1]]))\n\n    tau = pm.Gamma('tau', 1., 1., shape=K)\n    lambda_ = pm.Uniform('lambda', 0, 5, shape=K)\n    mu = pm.Normal('mu', 0, tau=lambda_ * tau, shape=K)\n    obs = pm.NormalMixture('obs', w, mu, tau=lambda_ * tau,\n                           observed=x_std)\n\nwith model:\n    trace = pm.sample(2000, n_init=100000)\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.bar(np.arange(K) - 0.4, trace['w'].mean(axis=0));\n\ntrace['mu'].mean(axis=0)[:3]\n\n(CENTROIDS - x.mean()) / x.std()\n"
'y_scores = cross_val_predict(classifier, x_train, y_train, cv=3,\n                             method="decision_function")\n\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n'
'N(100,2)=5151\nN(100,5)=96560646\n'
"import dateutil.parser as dparser\nimport parsedatetime.parsedatetime as pdt\nimport parsedatetime.parsedatetime_consts as pdc\nimport time\nimport datetime\nimport re\nimport pprint\npdt_parser = pdt.Calendar(pdc.Constants())   \nrecord_time_pat=re.compile(r'^(.+)\\s+:')\nsex_pat=re.compile(r'\\b(he|she)\\b',re.IGNORECASE)\ndeath_time_pat=re.compile(r'died\\s+(.+hours later).*$',re.IGNORECASE)\nsymptom_pat=re.compile(r'[,-]')\n\ndef parse_record(astr):    \n    match=record_time_pat.match(astr)\n    if match:\n        record_time=dparser.parse(match.group(1))\n        astr,_=record_time_pat.subn('',astr,1)\n    else: sys.exit('Can not find record time')\n    match=sex_pat.search(astr)    \n    if match:\n        sex=match.group(1)\n        sex='Female' if sex.lower().startswith('s') else 'Male'\n        astr,_=sex_pat.subn('',astr,1)\n    else: sys.exit('Can not find sex')\n    match=death_time_pat.search(astr)\n    if match:\n        death_time,date_type=pdt_parser.parse(match.group(1),record_time)\n        if date_type==2:\n            death_time=datetime.datetime.fromtimestamp(\n                time.mktime(death_time))\n        astr,_=death_time_pat.subn('',astr,1)\n        is_dead=True\n    else:\n        death_time=None\n        is_dead=False\n    astr=astr.replace('and','')    \n    symptoms=[s.strip() for s in symptom_pat.split(astr)]\n    return {'Record Time': record_time,\n            'Sex': sex,\n            'Death Time':death_time,\n            'Symptoms': symptoms,\n            'Death':is_dead}\n\n\nif __name__=='__main__':\n    tests=[('11/11/2010 - 09:00am : He got nausea, vomiting and died 4 hours later',\n            {'Sex':'Male',\n             'Symptoms':['got nausea', 'vomiting'],\n             'Death':True,\n             'Death Time':datetime.datetime(2010, 11, 11, 13, 0),\n             'Record Time':datetime.datetime(2010, 11, 11, 9, 0)}),\n           ('11/11/2010 - 09:00am : She got heart burn, vomiting of blood and died 1 hours later in the operation room',\n           {'Sex':'Female',\n             'Symptoms':['got heart burn', 'vomiting of blood'],\n             'Death':True,\n             'Death Time':datetime.datetime(2010, 11, 11, 10, 0),\n             'Record Time':datetime.datetime(2010, 11, 11, 9, 0)})\n           ]\n\n    for record,answer in tests:\n        result=parse_record(record)\n        pprint.pprint(result)\n        assert result==answer\n        print\n\n{'Death': True,\n 'Death Time': datetime.datetime(2010, 11, 11, 13, 0),\n 'Record Time': datetime.datetime(2010, 11, 11, 9, 0),\n 'Sex': 'Male',\n 'Symptoms': ['got nausea', 'vomiting']}\n\n{'Death': True,\n 'Death Time': datetime.datetime(2010, 11, 11, 10, 0),\n 'Record Time': datetime.datetime(2010, 11, 11, 9, 0),\n 'Sex': 'Female',\n 'Symptoms': ['got heart burn', 'vomiting of blood']}\n"
'# \'filename\' is the full path name for a data file whose size \n# exceeds the memory on the box it resides. #\n\nimport tokenize\n\ndata_reader = open(some_filename, \'r\')\ntokens = tokenize.generate_tokens(reader)\ntokens.next()           # returns a single line from the large data file.\n\nimport numpy as NP\nfrom scipy import linalg as LA\n\nD = NP.random.randn(8, 5)       # a simulated data set\n# calculate the covariance matrix: #\nR = NP.corrcoef(D, rowvar=1)\n# calculate the eigenvalues of the covariance matrix: #\neigval, eigvec = NP.eig(R)\n# sort them in descending order: #\negval = NP.sort(egval)[::-1]\n# make a value-proportion table #\ncs = NP.cumsum(egval)/NP.sum(egval)\nprint("{0}\\t{1}".format(\'eigenvalue\', \'var proportion\'))\nfor i in range(len(egval)) :\n    print("{0:.2f}\\t\\t{1:.2f}".format(egval[i], cs[i]))\n\n  eigenvalue    var proportion\n    2.22        0.44\n    1.81        0.81\n    0.67        0.94\n    0.23        0.99\n    0.06        1.00\n\nD = D[:,:-2]\n\nfrom redis import Redis\nr0 = Redis(db=0)\nr0.hmset(user_id : "100143321, {sex : \'M\', status : \'registered_user\', \n       traffic_source : \'affiliate\', page_views_per_session : 17, \n       total_purchases : 28.15})\n'
'#!/usr/bin/env python\n"""\nCS 65 Lab #3 -- 5 Oct 2008\nDougal Sutherland\n\nImplements a hidden Markov model, based on Jurafsky + Martin\'s presentation,\nwhich is in turn based off work by Jason Eisner. We test our program with\ndata from Eisner\'s spreadsheets.\n"""\n\n\nidentity = lambda x: x\n\nclass HiddenMarkovModel(object):\n    """A hidden Markov model."""\n\n    def __init__(self, states, transitions, emissions, vocab):\n        """\n        states - a list/tuple of states, e.g. (\'start\', \'hot\', \'cold\', \'end\')\n                 start state needs to be first, end state last\n                 states are numbered by their order here\n        transitions - the probabilities to go from one state to another\n                      transitions[from_state][to_state] = prob\n        emissions - the probabilities of an observation for a given state\n                    emissions[state][observation] = prob\n        vocab: a list/tuple of the names of observable values, in order\n        """\n        self.states = states\n        self.real_states = states[1:-1]\n        self.start_state = 0\n        self.end_state = len(states) - 1\n        self.transitions = transitions\n        self.emissions = emissions\n        self.vocab = vocab\n\n    # functions to get stuff one-indexed\n    state_num = lambda self, n: self.states[n]\n    state_nums = lambda self: xrange(1, len(self.real_states) + 1)\n\n    vocab_num = lambda self, n: self.vocab[n - 1]\n    vocab_nums = lambda self: xrange(1, len(self.vocab) + 1)\n    num_for_vocab = lambda self, s: self.vocab.index(s) + 1\n\n    def transition(self, from_state, to_state):\n        return self.transitions[from_state][to_state]\n\n    def emission(self, state, observed):\n        return self.emissions[state][observed - 1]\n\n\n    # helper stuff\n    def _normalize_observations(self, observations):\n        return [None] + [self.num_for_vocab(o) if o.__class__ == str else o\n                                               for o in observations]\n\n    def _init_trellis(self, observed, forward=True, init_func=identity):\n        trellis = [ [None for j in range(len(observed))]\n                          for i in range(len(self.real_states) + 1) ]\n\n        if forward:\n            v = lambda s: self.transition(0, s) * self.emission(s, observed[1])\n        else:\n            v = lambda s: self.transition(s, self.end_state)\n        init_pos = 1 if forward else -1\n\n        for state in self.state_nums():\n            trellis[state][init_pos] = init_func( v(state) )\n        return trellis\n\n    def _follow_backpointers(self, trellis, start):\n        # don\'t bother branching\n        pointer = start[0]\n        seq = [pointer, self.end_state]\n        for t in reversed(xrange(1, len(trellis[1]))):\n            val, backs = trellis[pointer][t]\n            pointer = backs[0]\n            seq.insert(0, pointer)\n        return seq\n\n\n    # actual algorithms\n\n    def forward_prob(self, observations, return_trellis=False):\n        """\n        Returns the probability of seeing the given `observations` sequence,\n        using the Forward algorithm.\n        """\n        observed = self._normalize_observations(observations)\n        trellis = self._init_trellis(observed)\n\n        for t in range(2, len(observed)):\n            for state in self.state_nums():\n                trellis[state][t] = sum(\n                    self.transition(old_state, state)\n                        * self.emission(state, observed[t])\n                        * trellis[old_state][t-1]\n                    for old_state in self.state_nums()\n                )\n        final = sum(trellis[state][-1] * self.transition(state, -1)\n                    for state in self.state_nums())\n        return (final, trellis) if return_trellis else final\n\n\n    def backward_prob(self, observations, return_trellis=False):\n        """\n        Returns the probability of seeing the given `observations` sequence,\n        using the Backward algorithm.\n        """\n        observed = self._normalize_observations(observations)\n        trellis = self._init_trellis(observed, forward=False)\n\n        for t in reversed(range(1, len(observed) - 1)):\n            for state in self.state_nums():\n                trellis[state][t] = sum(\n                    self.transition(state, next_state)\n                        * self.emission(next_state, observed[t+1])\n                        * trellis[next_state][t+1]\n                    for next_state in self.state_nums()\n                )\n        final = sum(self.transition(0, state)\n                        * self.emission(state, observed[1])\n                        * trellis[state][1]\n                    for state in self.state_nums())\n        return (final, trellis) if return_trellis else final\n\n\n    def viterbi_sequence(self, observations, return_trellis=False):\n        """\n        Returns the most likely sequence of hidden states, for a given\n        sequence of observations. Uses the Viterbi algorithm.\n        """\n        observed = self._normalize_observations(observations)\n        trellis = self._init_trellis(observed, init_func=lambda val: (val, [0]))\n\n        for t in range(2, len(observed)):\n            for state in self.state_nums():\n                emission_prob = self.emission(state, observed[t])\n                last = [(old_state, trellis[old_state][t-1][0] * \\\n                                    self.transition(old_state, state) * \\\n                                    emission_prob)\n                        for old_state in self.state_nums()]\n                highest = max(last, key=lambda p: p[1])[1]\n                backs = [s for s, val in last if val == highest]\n                trellis[state][t] = (highest, backs)\n\n        last = [(old_state, trellis[old_state][-1][0] * \\\n                            self.transition(old_state, self.end_state)) \n                for old_state in self.state_nums()]\n        highest = max(last, key = lambda p: p[1])[1]\n        backs = [s for s, val in last if val == highest]\n        seq = self._follow_backpointers(trellis, backs)\n\n        return (seq, trellis) if return_trellis else seq\n\n\n    def train_on_obs(self, observations, return_probs=False):\n        """\n        Trains the model once, using the forward-backward algorithm. This\n        function returns a new HMM instance rather than modifying this one.\n        """\n        observed = self._normalize_observations(observations)\n        forward_prob,  forwards  = self.forward_prob( observations, True)\n        backward_prob, backwards = self.backward_prob(observations, True)\n\n        # gamma values\n        prob_of_state_at_time = posat = [None] + [\n            [0] + [forwards[state][t] * backwards[state][t] / forward_prob\n                for t in range(1, len(observations)+1)]\n            for state in self.state_nums()]\n        # xi values\n        prob_of_transition = pot = [None] + [\n            [None] + [\n                [0] + [forwards[state1][t] \n                        * self.transition(state1, state2)\n                        * self.emission(state2, observed[t+1]) \n                        * backwards[state2][t+1]\n                        / forward_prob\n                  for t in range(1, len(observations))]\n              for state2 in self.state_nums()]\n          for state1 in self.state_nums()]\n\n        # new transition probabilities\n        trans = [[0 for j in range(len(self.states))]\n                    for i in range(len(self.states))]\n        trans[self.end_state][self.end_state] = 1\n\n        for state in self.state_nums():\n            state_prob = sum(posat[state])\n            trans[0][state] = posat[state][1]\n            trans[state][-1] = posat[state][-1] / state_prob\n            for oth in self.state_nums():\n                trans[state][oth] = sum(pot[state][oth]) / state_prob\n\n        # new emission probabilities\n        emit = [[0 for j in range(len(self.vocab))]\n                   for i in range(len(self.states))]\n        for state in self.state_nums():\n            for output in range(1, len(self.vocab) + 1):\n                n = sum(posat[state][t] for t in range(1, len(observations)+1)\n                                              if observed[t] == output)\n                emit[state][output-1] = n / sum(posat[state])\n\n        trained = HiddenMarkovModel(self.states, trans, emit, self.vocab)\n        return (trained, posat, pot) if return_probs else trained\n\n\n# ======================\n# = reading from files =\n# ======================\n\ndef normalize(string):\n    if \'#\' in string:\n        string = string[:string.index(\'#\')]\n    return string.strip()\n\ndef make_hmm_from_file(f):\n    def nextline():\n        line = f.readline()\n        if line == \'\': # EOF\n            return None\n        else:\n            return normalize(line) or nextline()\n\n    n = int(nextline())\n    states = [nextline() for i in range(n)] # &lt;3 list comprehension abuse\n\n    num_vocab = int(nextline())\n    vocab = [nextline() for i in range(num_vocab)]\n\n    transitions = [[float(x) for x in nextline().split()] for i in range(n)]\n    emissions   = [[float(x) for x in nextline().split()] for i in range(n)]\n\n    assert nextline() is None\n    return HiddenMarkovModel(states, transitions, emissions, vocab)\n\ndef read_observations_from_file(f):\n    return filter(lambda x: x, [normalize(line) for line in f.readlines()])\n\n# =========\n# = tests =\n# =========\n\nimport unittest\nclass TestHMM(unittest.TestCase):\n    def setUp(self):\n        # it\'s complicated to pass args to a testcase, so just use globals\n        self.hmm = make_hmm_from_file(file(HMM_FILENAME))\n        self.obs = read_observations_from_file(file(OBS_FILENAME))\n\n    def test_forward(self):\n        prob, trellis = self.hmm.forward_prob(self.obs, True)\n        self.assertAlmostEqual(prob,           9.1276e-19, 21)\n        self.assertAlmostEqual(trellis[1][1],  0.1,        4)\n        self.assertAlmostEqual(trellis[1][3],  0.00135,    5)\n        self.assertAlmostEqual(trellis[1][6],  8.71549e-5, 9)\n        self.assertAlmostEqual(trellis[1][13], 5.70827e-9, 9)\n        self.assertAlmostEqual(trellis[1][20], 1.3157e-10, 14)\n        self.assertAlmostEqual(trellis[1][27], 3.1912e-14, 13)\n        self.assertAlmostEqual(trellis[1][33], 2.0498e-18, 22)\n        self.assertAlmostEqual(trellis[2][1],  0.1,        4)\n        self.assertAlmostEqual(trellis[2][3],  0.03591,    5)\n        self.assertAlmostEqual(trellis[2][6],  5.30337e-4, 8)\n        self.assertAlmostEqual(trellis[2][13], 1.37864e-7, 11)\n        self.assertAlmostEqual(trellis[2][20], 2.7819e-12, 15)\n        self.assertAlmostEqual(trellis[2][27], 4.6599e-15, 18)\n        self.assertAlmostEqual(trellis[2][33], 7.0777e-18, 22)\n\n    def test_backward(self):\n        prob, trellis = self.hmm.backward_prob(self.obs, True)\n        self.assertAlmostEqual(prob,           9.1276e-19, 21)\n        self.assertAlmostEqual(trellis[1][1],  1.1780e-18, 22)\n        self.assertAlmostEqual(trellis[1][3],  7.2496e-18, 22)\n        self.assertAlmostEqual(trellis[1][6],  3.3422e-16, 20)\n        self.assertAlmostEqual(trellis[1][13], 3.5380e-11, 15)\n        self.assertAlmostEqual(trellis[1][20], 6.77837e-9, 14)\n        self.assertAlmostEqual(trellis[1][27], 1.44877e-5, 10)\n        self.assertAlmostEqual(trellis[1][33], 0.1,        4)\n        self.assertAlmostEqual(trellis[2][1],  7.9496e-18, 22)\n        self.assertAlmostEqual(trellis[2][3],  2.5145e-17, 21)\n        self.assertAlmostEqual(trellis[2][6],  1.6662e-15, 19)\n        self.assertAlmostEqual(trellis[2][13], 5.1558e-12, 16)\n        self.assertAlmostEqual(trellis[2][20], 7.52345e-9, 14)\n        self.assertAlmostEqual(trellis[2][27], 9.66609e-5, 9)\n        self.assertAlmostEqual(trellis[2][33], 0.1,        4)\n\n    def test_viterbi(self):\n        path, trellis = self.hmm.viterbi_sequence(self.obs, True)\n        self.assertEqual(path, [0] + [2]*13 + [1]*14 + [2]*6 + [3])\n        self.assertAlmostEqual(trellis[1][1] [0],  0.1,      4)\n        self.assertAlmostEqual(trellis[1][6] [0],  5.62e-05, 7)\n        self.assertAlmostEqual(trellis[1][7] [0],  4.50e-06, 8)\n        self.assertAlmostEqual(trellis[1][16][0], 1.99e-09, 11)\n        self.assertAlmostEqual(trellis[1][17][0], 3.18e-10, 12)\n        self.assertAlmostEqual(trellis[1][23][0], 4.00e-13, 15)\n        self.assertAlmostEqual(trellis[1][25][0], 1.26e-13, 15)\n        self.assertAlmostEqual(trellis[1][29][0], 7.20e-17, 19)\n        self.assertAlmostEqual(trellis[1][30][0], 1.15e-17, 19)\n        self.assertAlmostEqual(trellis[1][32][0], 7.90e-19, 21)\n        self.assertAlmostEqual(trellis[1][33][0], 1.26e-19, 21)  \n        self.assertAlmostEqual(trellis[2][ 1][0], 0.1,      4)\n        self.assertAlmostEqual(trellis[2][ 4][0], 0.00502,  5)\n        self.assertAlmostEqual(trellis[2][ 6][0], 0.00045,  5)\n        self.assertAlmostEqual(trellis[2][12][0], 1.62e-07, 9)\n        self.assertAlmostEqual(trellis[2][18][0], 3.18e-12, 14)\n        self.assertAlmostEqual(trellis[2][19][0], 1.78e-12, 14)\n        self.assertAlmostEqual(trellis[2][23][0], 5.00e-14, 16)\n        self.assertAlmostEqual(trellis[2][28][0], 7.87e-16, 18)\n        self.assertAlmostEqual(trellis[2][29][0], 4.41e-16, 18)\n        self.assertAlmostEqual(trellis[2][30][0], 7.06e-17, 19)\n        self.assertAlmostEqual(trellis[2][33][0], 1.01e-18, 20)\n\n    def test_learning_probs(self):\n        trained, gamma, xi = self.hmm.train_on_obs(self.obs, True)\n\n        self.assertAlmostEqual(gamma[1][1],  0.129, 3)\n        self.assertAlmostEqual(gamma[1][3],  0.011, 3)\n        self.assertAlmostEqual(gamma[1][7],  0.022, 3)\n        self.assertAlmostEqual(gamma[1][14], 0.887, 3)\n        self.assertAlmostEqual(gamma[1][18], 0.994, 3)\n        self.assertAlmostEqual(gamma[1][23], 0.961, 3)\n        self.assertAlmostEqual(gamma[1][27], 0.507, 3)\n        self.assertAlmostEqual(gamma[1][33], 0.225, 3)\n        self.assertAlmostEqual(gamma[2][1],  0.871, 3)\n        self.assertAlmostEqual(gamma[2][3],  0.989, 3)\n        self.assertAlmostEqual(gamma[2][7],  0.978, 3)\n        self.assertAlmostEqual(gamma[2][14], 0.113, 3)\n        self.assertAlmostEqual(gamma[2][18], 0.006, 3)\n        self.assertAlmostEqual(gamma[2][23], 0.039, 3)\n        self.assertAlmostEqual(gamma[2][27], 0.493, 3)\n        self.assertAlmostEqual(gamma[2][33], 0.775, 3)\n\n        self.assertAlmostEqual(xi[1][1][1],  0.021, 3)\n        self.assertAlmostEqual(xi[1][1][12], 0.128, 3)\n        self.assertAlmostEqual(xi[1][1][32], 0.13,  3)\n        self.assertAlmostEqual(xi[2][1][1],  0.003, 3)\n        self.assertAlmostEqual(xi[2][1][22], 0.017, 3)\n        self.assertAlmostEqual(xi[2][1][32], 0.095, 3)\n        self.assertAlmostEqual(xi[1][2][4],  0.02,  3)\n        self.assertAlmostEqual(xi[1][2][16], 0.018, 3)\n        self.assertAlmostEqual(xi[1][2][29], 0.010, 3)\n        self.assertAlmostEqual(xi[2][2][2],  0.972, 3)\n        self.assertAlmostEqual(xi[2][2][12], 0.762, 3)\n        self.assertAlmostEqual(xi[2][2][28], 0.907, 3)\n\n    def test_learning_results(self):\n        trained = self.hmm.train_on_obs(self.obs)\n\n        tr = trained.transition\n        self.assertAlmostEqual(tr(0, 0), 0,      5)\n        self.assertAlmostEqual(tr(0, 1), 0.1291, 4)\n        self.assertAlmostEqual(tr(0, 2), 0.8709, 4)\n        self.assertAlmostEqual(tr(0, 3), 0,      4)\n        self.assertAlmostEqual(tr(1, 0), 0,      5)\n        self.assertAlmostEqual(tr(1, 1), 0.8757, 4)\n        self.assertAlmostEqual(tr(1, 2), 0.1090, 4)\n        self.assertAlmostEqual(tr(1, 3), 0.0153, 4)\n        self.assertAlmostEqual(tr(2, 0), 0,      5)\n        self.assertAlmostEqual(tr(2, 1), 0.0925, 4)\n        self.assertAlmostEqual(tr(2, 2), 0.8652, 4)\n        self.assertAlmostEqual(tr(2, 3), 0.0423, 4)\n        self.assertAlmostEqual(tr(3, 0), 0,      5)\n        self.assertAlmostEqual(tr(3, 1), 0,      4)\n        self.assertAlmostEqual(tr(3, 2), 0,      4)\n        self.assertAlmostEqual(tr(3, 3), 1,      4)\n\n        em = trained.emission\n        self.assertAlmostEqual(em(0, 1), 0,      4)\n        self.assertAlmostEqual(em(0, 2), 0,      4)\n        self.assertAlmostEqual(em(0, 3), 0,      4)\n        self.assertAlmostEqual(em(1, 1), 0.6765, 4)\n        self.assertAlmostEqual(em(1, 2), 0.2188, 4)\n        self.assertAlmostEqual(em(1, 3), 0.1047, 4)\n        self.assertAlmostEqual(em(2, 1), 0.0584, 4)\n        self.assertAlmostEqual(em(2, 2), 0.4251, 4)\n        self.assertAlmostEqual(em(2, 3), 0.5165, 4)\n        self.assertAlmostEqual(em(3, 1), 0,      4)\n        self.assertAlmostEqual(em(3, 2), 0,      4)\n        self.assertAlmostEqual(em(3, 3), 0,      4)\n\n        # train 9 more times\n        for i in range(9):\n            trained = trained.train_on_obs(self.obs)\n\n        tr = trained.transition\n        self.assertAlmostEqual(tr(0, 0), 0,      4)\n        self.assertAlmostEqual(tr(0, 1), 0,      4)\n        self.assertAlmostEqual(tr(0, 2), 1,      4)\n        self.assertAlmostEqual(tr(0, 3), 0,      4)\n        self.assertAlmostEqual(tr(1, 0), 0,      4)\n        self.assertAlmostEqual(tr(1, 1), 0.9337, 4)\n        self.assertAlmostEqual(tr(1, 2), 0.0663, 4)\n        self.assertAlmostEqual(tr(1, 3), 0,      4)\n        self.assertAlmostEqual(tr(2, 0), 0,      4)\n        self.assertAlmostEqual(tr(2, 1), 0.0718, 4)\n        self.assertAlmostEqual(tr(2, 2), 0.8650, 4)\n        self.assertAlmostEqual(tr(2, 3), 0.0632, 4)\n        self.assertAlmostEqual(tr(3, 0), 0,      4)\n        self.assertAlmostEqual(tr(3, 1), 0,      4)\n        self.assertAlmostEqual(tr(3, 2), 0,      4)\n        self.assertAlmostEqual(tr(3, 3), 1,      4)\n\n        em = trained.emission\n        self.assertAlmostEqual(em(0, 1), 0,      4)\n        self.assertAlmostEqual(em(0, 2), 0,      4)\n        self.assertAlmostEqual(em(0, 3), 0,      4)\n        self.assertAlmostEqual(em(1, 1), 0.6407, 4)\n        self.assertAlmostEqual(em(1, 2), 0.1481, 4)\n        self.assertAlmostEqual(em(1, 3), 0.2112, 4)\n        self.assertAlmostEqual(em(2, 1), 0.00016,5)\n        self.assertAlmostEqual(em(2, 2), 0.5341, 4)\n        self.assertAlmostEqual(em(2, 3), 0.4657, 4)\n        self.assertAlmostEqual(em(3, 1), 0,      4)\n        self.assertAlmostEqual(em(3, 2), 0,      4)\n        self.assertAlmostEqual(em(3, 3), 0,      4)\n\nif __name__ == \'__main__\':\n    import sys\n    HMM_FILENAME = sys.argv[1] if len(sys.argv) &gt;= 2 else \'example.hmm\'\n    OBS_FILENAME = sys.argv[2] if len(sys.argv) &gt;= 3 else \'observations.txt\'\n\n    unittest.main()\n\n2\n3\n3\n2\n3\n2\n3\n2\n2\n3\n1\n3\n3\n1\n1\n1\n2\n1\n1\n1\n3\n1\n2\n1\n1\n1\n2\n3\n3\n2\n3\n2\n2\n\n4 # number of states\nSTART\nCOLD\nHOT\nEND\n\n3 # size of vocab\n1\n2\n3\n\n# transition matrix\n0.0 0.5 0.5 0.0  # from start\n0.0 0.8 0.1 0.1  # from cold\n0.0 0.1 0.8 0.1  # from hot\n0.0 0.0 0.0 1.0  # from end\n\n# emission matrix\n0.0 0.0 0.0  # from start\n0.7 0.2 0.1  # from cold\n0.1 0.2 0.7  # from hot\n0.0 0.0 0.0  # from end\n'
'zip(cv.get_feature_names(),\n    np.asarray(X.sum(axis=0)).ravel())\n'
'import numpy as np\nfrom scipy import linalg\nfrom numpy import dot\n\ndef nmf(X, latent_features, max_iter=100, error_limit=1e-6, fit_error_limit=1e-6):\n    """\n    Decompose X to A*Y\n    """\n    eps = 1e-5\n    print \'Starting NMF decomposition with {} latent features and {} iterations.\'.format(latent_features, max_iter)\n    X = X.toarray()  # I am passing in a scipy sparse matrix\n\n    # mask\n    mask = np.sign(X)\n\n    # initial matrices. A is random [0,1] and Y is A\\X.\n    rows, columns = X.shape\n    A = np.random.rand(rows, latent_features)\n    A = np.maximum(A, eps)\n\n    Y = linalg.lstsq(A, X)[0]\n    Y = np.maximum(Y, eps)\n\n    masked_X = mask * X\n    X_est_prev = dot(A, Y)\n    for i in range(1, max_iter + 1):\n        # ===== updates =====\n        # Matlab: A=A.*(((W.*X)*Y\')./((W.*(A*Y))*Y\'));\n        top = dot(masked_X, Y.T)\n        bottom = (dot((mask * dot(A, Y)), Y.T)) + eps\n        A *= top / bottom\n\n        A = np.maximum(A, eps)\n        # print \'A\',  np.round(A, 2)\n\n        # Matlab: Y=Y.*((A\'*(W.*X))./(A\'*(W.*(A*Y))));\n        top = dot(A.T, masked_X)\n        bottom = dot(A.T, mask * dot(A, Y)) + eps\n        Y *= top / bottom\n        Y = np.maximum(Y, eps)\n        # print \'Y\', np.round(Y, 2)\n\n\n        # ==== evaluation ====\n        if i % 5 == 0 or i == 1 or i == max_iter:\n            print \'Iteration {}:\'.format(i),\n            X_est = dot(A, Y)\n            err = mask * (X_est_prev - X_est)\n            fit_residual = np.sqrt(np.sum(err ** 2))\n            X_est_prev = X_est\n\n            curRes = linalg.norm(mask * (X - X_est), ord=\'fro\')\n            print \'fit residual\', np.round(fit_residual, 4),\n            print \'total residual\', np.round(curRes, 4)\n            if curRes &lt; error_limit or fit_residual &lt; fit_error_limit:\n                break\n\nreturn A, Y\n'
"from sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\n\nclf = AdaBoostClassifier(SVC(probability=True, kernel='linear'), ...)\n\nfrom sklearn.linear_model import SGDClassifier\n\nclf = AdaBoostClassifier(SGDClassifier(loss='hinge'), algorithm='SAMME', ...)\n"
"param_dist = {\n          'rbf_svm__C': [1, 10, 100, 1000], \n          'rbf_svm__gamma': [0.001, 0.0001], \n          'rbf_svm__kernel': ['rbf', 'linear'],\n}\n"
'from sklearn.dummy import DummyClassifier\nimport numpy as np\n\ntwo_dimensional_values = []\nclass_labels           = []\n\nfor i in xrange(90):\n    two_dimensional_values.append( [1,1] )\n    class_labels.append(1)\n\nfor i in xrange(10):\n    two_dimensional_values.append( [0,0] )\n    class_labels.append(0)\n\n#now 90% of the training data contains the target property\nX = np.array( two_dimensional_values )\ny = np.array( class_labels )\n\n#train a dummy classifier to make predictions based on the most_frequent class value\ndummy_classifier = DummyClassifier(strategy="most_frequent")\ndummy_classifier.fit( X,y )\n\n#this produces 100 predictions that say "1"\nfor i in two_dimensional_values:\n    print( dummy_classifier.predict( [i]) )\n\n#train a dummy classifier to make predictions based on the class values\nnew_dummy_classifier = DummyClassifier(strategy="stratified")\nnew_dummy_classifier.fit( X,y )\n\n#this produces roughly 90 guesses that say "1" and roughly 10 guesses that say "0"\nfor i in two_dimensional_values:\n    print( new_dummy_classifier.predict( [i]) )\n'
'print roc_auc_score(y_test, rf.predict_proba(X_test)[:,1])\n'
'input_sequence_length = 2; # the length of one vector in your input sequence\n\nfor i in xrange(350):  \n    encoder_inputs.append(tf.placeholder(tf.int32, shape=[None, input_sequence_length],\n                                              name="encoder{0}".format(i)))\n'
'import pandas as pd\nimport numpy as np\nfrom  sklearn import linear_model\nfrom sklearn.cross_validation import train_test_split\n\nreader = pd.io.parsers.read_csv("./data/all-stocks-cleaned.csv")\nstock = np.array(reader)\n\nopeningPrice = stock[:, 1]\nclosingPrice = stock[:, 5]\n\nopeningPriceTrain, openingPriceTest, closingPriceTrain, closingPriceTest = \\\n    train_test_split(openingPrice, closingPrice, test_size=0.25, random_state=42)\n\nopeningPriceTrain = openingPriceTrain.reshape(openingPriceTrain.size,1)\nopeningPriceTrain = openingPriceTrain.astype(np.float64, copy=False)\n\nclosingPriceTrain = closingPriceTrain.reshape(closingPriceTrain.size,1)\nclosingPriceTrain = closingPriceTrain.astype(np.float64, copy=False)\n\nopeningPriceTest = openingPriceTest.reshape(openingPriceTest.size,1)\nopeningPriceTest = openingPriceTest.astype(np.float64, copy=False)\n\nnp.isnan(openingPriceTrain).any(), np.isnan(closingPriceTrain).any(), np.isnan(openingPriceTest).any()\n\n(True, True, True)\n\nopeningPriceTrain[np.isnan(openingPriceTrain)] = np.median(openingPriceTrain[~np.isnan(openingPriceTrain)])\nclosingPriceTrain[np.isnan(closingPriceTrain)] = np.median(closingPriceTrain[~np.isnan(closingPriceTrain)])\nopeningPriceTest[np.isnan(openingPriceTest)] = np.median(openingPriceTest[~np.isnan(openingPriceTest)])\n\nregression = linear_model.LinearRegression()\n\nregression.fit(openingPriceTrain, closingPriceTrain)\n\npredicted = regression.predict(openingPriceTest)\n\npredicted[:5]\n\narray([[ 13598.74748173],\n       [ 53281.04442146],\n       [ 18305.4272186 ],\n       [ 50753.50958453],\n       [ 14937.65782778]])\n\ndata = pd.read_csv(\'./data/all-stocks-cleaned.csv\')\ndata.isnull().any()\nDate                    False\nOpen                     True\nHigh                     True\nLow                      True\nLast                     True\nClose                    True\nTotal Trade Quantity     True\nTurnover (Lacs)          True\n\ndata = data.fillna(lambda x: x.median())\n\ndata = data.fillna(method=\'ffill\')\n'
"from sklearn import datasets\nfrom sklearn import feature_selection\nfrom sklearn.svm import LinearSVC\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# classifier\nLinearSVC1 = LinearSVC(tol=1e-4,  C = 0.10000000000000001)\nf5 = feature_selection.RFE(estimator=LinearSVC1, n_features_to_select=2, step=1)\n\npipeline = Pipeline([\n    ('rfe_feature_selection', f5),\n    ('clf', LinearSVC1)\n    ])\n\npipeline.fit(X, y)\n\nsupport = pipeline.named_steps['rfe_feature_selection'].support_\n\nimport numpy as np\nfeature_names = np.array(iris.feature_names) # transformed list to array\n\nfeature_names[support]\n\narray(['sepal width (cm)', 'petal width (cm)'], \n      dtype='|S17')\n\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn import feature_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nimport numpy as np\n\nX = ['I am a sentence', 'an example']\nY = [1, 2]\nX_dev = ['another sentence']\n\n# classifier\nLinearSVC1 = LinearSVC(tol=1e-4,  C = 0.10000000000000001)\nf5 = feature_selection.RFE(estimator=LinearSVC1, n_features_to_select=500, step=1)\n\npipeline = Pipeline([\n    ('features', FeatureUnion([\n       ('tfidf', TfidfVectorizer(ngram_range=(1, 3), max_features= 4000))])), \n    ('rfe_feature_selection', f5),\n    ('clf', LinearSVC1),\n    ])\n\npipeline.fit(X, Y)\ny_pred = pipeline.predict(X_dev)\n\nsupport = pipeline.named_steps['rfe_feature_selection'].support_\nfeature_names = pipeline.named_steps['features'].get_feature_names()\nnp.array(feature_names)[support]\n"
'import os\nimport sys\n\n# Path for spark source folder\nos.environ[\'SPARK_HOME\']="E:\\\\Work\\\\spark\\\\installtion\\\\spark"\n\n# Append pyspark  to Python Path\nsys.path.append("E:\\\\Work\\\\spark\\\\installtion\\\\spark\\\\python")\n\ntry:\n    from pyspark.ml.feature import StringIndexer\n    # $example on$\n    from numpy import array\n    from math import sqrt\n    from pyspark import SparkConf\n    # $example off$\n\n    from pyspark import SparkContext\n    # $example on$\n    from pyspark.mllib.clustering import KMeans, KMeansModel\n\n    print ("Successfully imported Spark Modules")\n\nexcept ImportError as e:\n    sys.exit(1)\n\n\nif __name__ == "__main__":\n    sconf = SparkConf().setAppName("KMeansExample").set(\'spark.sql.warehouse.dir\', \'file:///E:/Work/spark/installtion/spark/spark-warehouse/\')\n    sc = SparkContext(conf=sconf)  # SparkContext\n    parsedData =  array([0.0,0.0, 1.0,1.0, 9.0,8.0, 8.0,9.0]).reshape(4,2)\n    clusters = KMeans.train(sc.parallelize(parsedData), 2, maxIterations=10,\n                            runs=10, initializationMode="random")\n    clusters.save(sc, "mymodel")  // this will save model to file system\n    sc.stop()\n\nfrom flask import jsonify, request, Flask\nfrom sklearn.externals import joblib\nimport os\nimport sys\n\n# Path for spark source folder\nos.environ[\'SPARK_HOME\']="E:\\\\Work\\\\spark\\\\installtion\\\\spark"\n\n# Append pyspark  to Python Path\nsys.path.append("E:\\\\Work\\\\spark\\\\installtion\\\\spark\\\\python")\n\ntry:\n    from pyspark.ml.feature import StringIndexer\n    # $example on$\n    from numpy import array\n    from math import sqrt\n    from pyspark import SparkConf\n    # $example off$\n\n    from pyspark import SparkContext\n    # $example on$\n    from pyspark.mllib.clustering import KMeans, KMeansModel\n\n    print ("Successfully imported Spark Modules")\n\nexcept ImportError as e:\n    sys.exit(1)\n\n\napp = Flask(__name__)\n\n@app.route(\'/\', methods=[\'GET\'])\ndef predict():\n\n    sconf = SparkConf().setAppName("KMeansExample").set(\'spark.sql.warehouse.dir\', \'file:///E:/Work/spark/installtion/spark/spark-warehouse/\')\n    sc = SparkContext(conf=sconf)  # SparkContext\n    sameModel = KMeansModel.load(sc, "clus")  // load from file system \n\n    response = sameModel.predict(array([0.0, 0.0]))  // pass your data\n\n    return jsonify(response)\n\nif __name__ == \'__main__\':\n    app.run()\n'
'if(f1 &lt; 127.5){\n  if(f7 &lt; 28.5){\n    if(f5 &lt; 45.4){\n      return 0.167528f;\n    } else {\n      return 0.05f;\n    }\n  }\n}\n'
'from sklearn.tree import ExtraTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm.classes import OneClassSVM\nfrom sklearn.neural_network.multilayer_perceptron import MLPClassifier\nfrom sklearn.neighbors.classification import RadiusNeighborsClassifier\nfrom sklearn.neighbors.classification import KNeighborsClassifier\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.multiclass import OutputCodeClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model.stochastic_gradient import SGDClassifier\nfrom sklearn.linear_model.ridge import RidgeClassifierCV\nfrom sklearn.linear_model.ridge import RidgeClassifier\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveClassifier    \nfrom sklearn.gaussian_process.gpc import GaussianProcessClassifier\nfrom sklearn.ensemble.voting_classifier import VotingClassifier\nfrom sklearn.ensemble.weight_boosting import AdaBoostClassifier\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\nfrom sklearn.ensemble.bagging import BaggingClassifier\nfrom sklearn.ensemble.forest import ExtraTreesClassifier\nfrom sklearn.ensemble.forest import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import MultinomialNB  \nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.svm import NuSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.mixture import DPGMM\nfrom sklearn.mixture import GMM \nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.mixture import VBGMM\n'
'batch_x = batch_x.reshape(-1 ,input_vec_size, 1)\n'
"import pandas as pd\ndf = pd.DataFrame({'name': ['Manie', 'Joyce', 'Ami'],\n                   'Org':  ['ABC2', 'ABC1', 'NSV2'],\n                   'Dept': ['Finance', 'HR', 'HR']        \n        })\n\n\ndf_2 = pd.get_dummies(df,drop_first=True)\n\nprint(df_2)\n   Dept_HR  Org_ABC2  Org_NSV2  name_Joyce  name_Manie\n0        0         1         0           0           1\n1        1         0         0           1           0\n2        1         0         1           0           0 \n\ndf_2 = pd.get_dummies(df, columns=['Org', 'Dept'], drop_first=True)\n\n    name  Org_ABC2  Org_NSV2  Dept_HR\n0  Manie         1         0        0\n1  Joyce         0         0        1\n2    Ami         0         1        1\n\ncolumn_names_for_onehot = df.columns[1:]\ndf_2 = pd.get_dummies(df, columns=column_names_for_onehot, drop_first=True)\n"
'import numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef fit_multiple_estimators(classifiers, X_list, y, sample_weights = None):\n\n    # Convert the labels `y` using LabelEncoder, because the predict method is using index-based pointers\n    # which will be converted back to original data later.\n    le_ = LabelEncoder()\n    le_.fit(y)\n    transformed_y = le_.transform(y)\n\n    # Fit all estimators with their respective feature arrays\n    estimators_ = [clf.fit(X, y) if sample_weights is None else clf.fit(X, y, sample_weights) for clf, X in zip([clf for _, clf in classifiers], X_list)]\n\n    return estimators_, le_\n\n\ndef predict_from_multiple_estimator(estimators, label_encoder, X_list, weights = None):\n\n    # Predict \'soft\' voting with probabilities\n\n    pred1 = np.asarray([clf.predict_proba(X) for clf, X in zip(estimators, X_list)])\n    pred2 = np.average(pred1, axis=0, weights=weights)\n    pred = np.argmax(pred2, axis=1)\n\n    # Convert integer predictions to original labels:\n    return label_encoder.inverse_transform(pred)\n\nfrom sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = []\n\n#Convert int classes to string labels\nfor x in data.target:\n    if x==0:\n        y.append(\'setosa\')\n    elif x==1:\n        y.append(\'versicolor\')\n    else:\n        y.append(\'virginica\')\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nX_train1, X_train2 = X_train[:,:2], X_train[:,2:]\nX_test1, X_test2 = X_test[:,:2], X_test[:,2:]\n\nX_train_list = [X_train1, X_train2]\nX_test_list = [X_test1, X_test2]\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Make sure the number of estimators here are equal to number of different feature datas\nclassifiers = [(\'knn\',  KNeighborsClassifier(3)),\n    (\'svc\', SVC(kernel="linear", C=0.025, probability=True))]\n\nfitted_estimators, label_encoder = fit_multiple_estimators(classifiers, X_train_list, y_train)\n\ny_pred = predict_from_multiple_estimator(fitted_estimators, label_encoder, X_test_list)\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))\n'
'new_knn_model = KNeighborsClassifier()\nnew_knn_model.set_params(**knn_gridsearch_model.best_params_)\n\nnew_knn_model = KNeighborsClassifier(**knn_gridsearch_model.best_params_)\n\ngs.best_estimator_\n'
'&gt;&gt;&gt; def reluDerivative(x):\n...     x[x&lt;=0] = 0\n...     x[x&gt;0] = 1\n...     return x\n\n&gt;&gt;&gt; z = np.random.uniform(-1, 1, (3,3))\n&gt;&gt;&gt; z\narray([[ 0.41287266, -0.73082379,  0.78215209],\n       [ 0.76983443,  0.46052273,  0.4283139 ],\n       [-0.18905708,  0.57197116,  0.53226954]])\n&gt;&gt;&gt; reluDerivative(z)\narray([[ 1.,  0.,  1.],\n       [ 1.,  1.,  1.],\n       [ 0.,  1.,  1.]])\n'
"class YourSampler(Sampler):\n    def __init__(self, mask):\n        self.mask = mask\n\n    def __iter__(self):\n        return (self.indices[i] for i in torch.nonzero(self.mask))\n\n    def __len__(self):\n        return len(self.mask)\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\n\nsampler1 = YourSampler(your_mask)\nsampler2 = YourSampler(your_other_mask)\ntrainloader_sampler1 = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          sampler = sampler1, shuffle=False, num_workers=2)\ntrainloader_sampler2 = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          sampler = sampler2, shuffle=False, num_workers=2)\n"
"from keras.models import Model\n\nembed = Embedding(word_index, 300, weights=[embedding_matrix], input_length=70, trainable=False)\nlstm = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(embed)\nagei = Input(shape=(1,))\nconc = Concatenate()(lstm, agei)\ndrop = Dropout(0.6)(conc)\ndens = Dense(1)(drop)\nacti = Activation('sigmoid')(dens)\n\nmodel = Model([embed, agei], acti)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics['accuracy'])\n"
'recall_average = recall_score(Y_test, y_predict, average="binary", pos_label="neg")\n'
'weighted_loss = weighted_losses[i]\n# ...\noutput_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\n\nweighted_losses = [\n    weighted_masked_objective(fn) for fn in loss_functions]\n\ndef weighted_masked_objective(fn):\n    """Adds support for masking and sample-weighting to an objective function.\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    """\n    if fn is None:\n        return None\n\n    def weighted(y_true, y_pred, weights, mask=None):\n        """Wrapper function.\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n        # Returns\n            Scalar tensor.\n        """\n        # score_array has ndim &gt;= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask)\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)\nreturn weighted\n\ndef binary_crossentropy(y_true, y_pred):\n    return K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n\ndef binary_crossentropy(target, output, from_logits=False):\n    """Binary crossentropy between an output tensor and a target tensor.\n    # Arguments\n        target: A tensor with the same shape as `output`.\n        output: A tensor.\n        from_logits: Whether `output` is expected to be a logits tensor.\n            By default, we consider that `output`\n            encodes a probability distribution.\n    # Returns\n        A tensor.\n    """\n    # Note: tf.nn.sigmoid_cross_entropy_with_logits\n    # expects logits, Keras expects probabilities.\n    if not from_logits:\n        # transform back to logits\n        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n        output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n        output = tf.log(output / (1 - output))\n\n    return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n                                                   logits=output)\n\ndef mean(x, axis=None, keepdims=False):\n    """Mean of a tensor, alongside the specified axis.\n    # Arguments\n        x: A tensor or variable.\n        axis: A list of integer. Axes to compute the mean.\n        keepdims: A boolean, whether to keep the dimensions or not.\n            If `keepdims` is `False`, the rank of the tensor is reduced\n            by 1 for each entry in `axis`. If `keepdims` is `True`,\n            the reduced dimensions are retained with length 1.\n    # Returns\n        A tensor with the mean of elements of `x`.\n    """\n    if x.dtype.base_dtype == tf.bool:\n        x = tf.cast(x, floatx())\nreturn tf.reduce_mean(x, axis, keepdims)\n\nweighted_loss = weighted_losses[i]\n# ...\noutput_loss = weighted_loss(y_true, y_pred, sample_weight, mask)\n\n# Compute total loss.\ntotal_loss = None\nwith K.name_scope(\'loss\'):\n    for i in range(len(self.outputs)):\n        if i in skip_target_indices:\n            continue\n        y_true = self.targets[i]\n        y_pred = self.outputs[i]\n        weighted_loss = weighted_losses[i]\n        sample_weight = sample_weights[i]\n        mask = masks[i]\n        loss_weight = loss_weights_list[i]\n        with K.name_scope(self.output_names[i] + \'_loss\'):\n            output_loss = weighted_loss(y_true, y_pred,\n                                        sample_weight, mask)\n        if len(self.outputs) &gt; 1:\n            self.metrics_tensors.append(output_loss)\n            self.metrics_names.append(self.output_names[i] + \'_loss\')\n        if total_loss is None:\n            total_loss = loss_weight * output_loss\n        else:\n            total_loss += loss_weight * output_loss\n    if total_loss is None:\n        if not self.losses:\n            raise ValueError(\'The model cannot be compiled \'\n                                \'because it has no loss to optimize.\')\n        else:\n            total_loss = 0.\n\n    # Add regularization penalties\n    # and other layer-specific losses.\n    for loss_tensor in self.losses:\n        total_loss += loss_tensor  \n'
"LogisticRegression(... solver='lbfgs', max_iter=100 ...)\n"
'import numpy as np\ndef computeMI(x, y):\n    sum_mi = 0.0\n    x_value_list = np.unique(x)\n    y_value_list = np.unique(y)\n    Px = np.array([ len(x[x==xval])/float(len(x)) for xval in x_value_list ]) #P(x)\n    Py = np.array([ len(y[y==yval])/float(len(y)) for yval in y_value_list ]) #P(y)\n    for i in xrange(len(x_value_list)):\n        if Px[i] ==0.:\n            continue\n        sy = y[x == x_value_list[i]]\n        if len(sy)== 0:\n            continue\n        pxy = np.array([len(sy[sy==yval])/float(len(y))  for yval in y_value_list]) #p(x,y)\n        t = pxy[Py&gt;0.]/Py[Py&gt;0.] /Px[i] # log(P(x,y)/( P(x)*P(y))\n        sum_mi += sum(pxy[t&gt;0]*np.log2( t[t&gt;0]) ) # sum ( P(x,y)* log(P(x,y)/( P(x)*P(y)) )\n    return sum_mi\n'
"label, pixel_1_1, pixel_1_2, ...\n\nimport numpy as np\nimport csv\nimport matplotlib.pyplot as plt\n\nwith open('mnist_test_10.csv', 'r') as csv_file:\n    for data in csv.reader(csv_file):\n        # The first column is the label\n        label = data[0]\n\n        # The rest of columns are pixels\n        pixels = data[1:]\n\n        # Make those columns into a array of 8-bits pixels\n        # This array will be of 1D with length 784\n        # The pixel intensity values are integers from 0 to 255\n        pixels = np.array(pixels, dtype='uint8')\n\n        # Reshape the array into 28 x 28 array (2-dimensional array)\n        pixels = pixels.reshape((28, 28))\n\n        # Plot\n        plt.title('Label is {label}'.format(label=label))\n        plt.imshow(pixels, cmap='gray')\n        plt.show()\n\n        break # This stops the loop, I just want to see one\n\n    title = 'Label is {label}'.format(label=label)\n\n    cv2.imshow(title, pixels)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n"
'from sklearn.metrics import f1_score\ny_true = [0, 1, 2, 0, 1, 2]\ny_pred = [0, 2, 1, 0, 0, 1]\n\nf1_score(y_true, y_pred, average=None)\n\narray([ 0.8,  0. ,  0. ])\n'
'import tensorflow as tf\ntf.python.control_flow_ops = tf\n'
"x = sentiment_model.predict_proba(test_matrix)\n\ntest_data['prediction0'] = x[:,0]\ntest_data['prediction1'] = x[:,1]\n"
"class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n        self.val_losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.val_losses.append(logs.get('val_loss'))\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, init='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nhistory = LossHistory()\nmodel.fit(X_train, Y_train, batch_size=128, nb_epoch=20, verbose=0, validation_split=0.1,\n          callbacks=[history])\n\nprint history.losses\n# outputs\n'''\n[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]\n'''\nprint history.val_losses\n"
"import numpy as np\n\n# Category -&gt; words\ndata = {\n  'Names': ['john','jay','dan','nathan','bob'],\n  'Colors': ['yellow', 'red','green'],\n  'Places': ['tokyo','bejing','washington','mumbai'],\n}\n# Words -&gt; category\ncategories = {word: key for key, words in data.items() for word in words}\n\n# Load the whole embedding matrix\nembeddings_index = {}\nwith open('glove.6B.100d.txt') as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    embed = np.array(values[1:], dtype=np.float32)\n    embeddings_index[word] = embed\nprint('Loaded %s word vectors.' % len(embeddings_index))\n# Embeddings for available words\ndata_embeddings = {key: value for key, value in embeddings_index.items() if key in categories.keys()}\n\n# Processing the query\ndef process(query):\n  query_embed = embeddings_index[query]\n  scores = {}\n  for word, embed in data_embeddings.items():\n    category = categories[word]\n    dist = query_embed.dot(embed)\n    dist /= len(data[category])\n    scores[category] = scores.get(category, 0) + dist\n  return scores\n\n# Testing\nprint(process('pink'))\nprint(process('frank'))\nprint(process('moscow'))\n\n{'Colors': 24.655489603678387, 'Names': 5.058711671829224, 'Places': 0.90213905274868011}\n{'Colors': 6.8597321510314941, 'Names': 15.570847320556641, 'Places': 3.5302454829216003}\n{'Colors': 8.2919375101725254, 'Names': 4.58830726146698, 'Places': 14.7840416431427}\n"
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\nclass FixedCovMixture:\n    """ The model to estimate gaussian mixture with fixed covariance matrix. """\n    def __init__(self, n_components, cov, max_iter=100, random_state=None, tol=1e-10):\n        self.n_components = n_components\n        self.cov = cov\n        self.random_state = random_state\n        self.max_iter = max_iter\n        self.tol=tol\n\n    def fit(self, X):\n        # initialize the process:\n        np.random.seed(self.random_state)\n        n_obs, n_features = X.shape\n        self.mean_ = X[np.random.choice(n_obs, size=self.n_components)]\n        # make EM loop until convergence\n        i = 0\n        for i in range(self.max_iter):\n            new_centers = self.updated_centers(X)\n            if np.sum(np.abs(new_centers-self.mean_)) &lt; self.tol:\n                break\n            else:\n                self.mean_ = new_centers\n        self.n_iter_ = i\n\n    def updated_centers(self, X):\n        """ A single iteration """\n        # E-step: estimate probability of each cluster given cluster centers\n        cluster_posterior = self.predict_proba(X)\n        # M-step: update cluster centers as weighted average of observations\n        weights = (cluster_posterior.T / cluster_posterior.sum(axis=1)).T\n        new_centers = np.dot(weights, X)\n        return new_centers\n\n\n    def predict_proba(self, X):\n        likelihood = np.stack([multivariate_normal.pdf(X, mean=center, cov=self.cov) \n                               for center in self.mean_])\n        cluster_posterior = (likelihood / likelihood.sum(axis=0))\n        return cluster_posterior\n\n    def predict(self, X):\n        return np.argmax(self.predict_proba(X), axis=0)\n\nnp.random.seed(1)\nX = np.random.normal(size=(100,2), scale=3)\nX[50:] += (10, 5)\n\nmodel = FixedCovMixture(2, cov=[[3,0],[0,3]], random_state=1)\nmodel.fit(X)\nprint(model.n_iter_, \'iterations\')\nprint(model.mean_)\n\nplt.scatter(X[:,0], X[:,1], s=10, c=model.predict(X))\nplt.scatter(model.mean_[:,0], model.mean_[:,1], s=100, c=\'k\')\nplt.axis(\'equal\')\nplt.show();\n\n11 iterations\n[[9.92301067 4.62282807]\n [0.09413883 0.03527411]]\n'
"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\n#training data\ndf = pd.DataFrame({'A':[1,2,3,7,9,15,16,1,5,6,2,4,8,9],'B':[15,12,10,11,8,14,17,20,4,12,4,5,17,19],'C':['Y','Y','Y','Y','N','N','N','Y','N','Y','N','N','Y','Y']})\n#fit and transform the training data and use them for the model training\ndf[['A','B']] = min_max_scaler.fit_transform(df[['A','B']])\ndf['C'] = df['C'].apply(lambda x: 0 if x.strip()=='N' else 1)\n\n#fit the model\nmodel.fit(df['A','B'])\n\n#after the model training on the transformed training data define the testing data df_test\ndf_test = pd.DataFrame({'A':[25,67,24,76,23],'B':[2,54,22,75,19]})\n\n#before the prediction of the test data, ONLY APPLY the scaler on them\ndf_test[['A','B']] = min_max_scaler.transform(df_test[['A','B']])\n\n#test the model\ny_predicted_from_model = model.predict(df_test['A','B'])\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVC\n\ndata = datasets.load_iris()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\nmodel = SVC()\nmodel.fit(X_train_scaled, y_train)\n\nX_test_scaled = scaler.transform(X_test)\ny_pred = model.predict(X_test_scaled)\n"
'def all_but_first_column(X):\n    return X[:, 1:]\n\ndef drop_first_component(X, y):\n    &quot;&quot;&quot;\n    Create a pipeline with PCA and the column selector and use it to\n    transform the dataset.\n    &quot;&quot;&quot;\n    pipeline = make_pipeline(PCA(), FunctionTransformer(all_but_first_column),)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    pipeline.fit(X_train, y_train)\n    return pipeline.transform(X_test), y_test\n\nclass FunctionFeaturizer(TransformerMixin):\n    def __init__(self, *featurizers):\n        self.featurizers = featurizers\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        #Do transformations and return\n        return transformed_data\n'
"from sklearn import datasets\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nkm = KMeans(n_clusters=3)\nkm.fit(X,y)\n\n# check how many unique labels do you have\nnp.unique(km.labels_)\n#array([0, 1, 2], dtype=int32)\n\nsilhouette_score(X, km.labels_, metric='euclidean')\n0.38788915189699597\n\nkm2 = KMeans(n_clusters=1)\nkm2.fit(X,y)\n\nsilhouette_score(X, km2.labels_, metric='euclidean')\n\nValueError: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)\n"
'prediction = model.predict(np.array(X).tolist()).tolist()\nreturn jsonify({\'prediction\': prediction})\n\na = np.array([1,2,3,4,5]).tolist()\njson.dumps({"prediction": a})\n\n\'{"prediction": [1, 2, 3, 4, 5]}\'\n'
"import keras.backend as K\nfrom keras.legacy import interfaces\nfrom keras.optimizers import Optimizer\n\n\nclass AdamAccumulate(Optimizer):\n\n    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n                 epsilon=None, decay=0., amsgrad=False, accum_iters=1, **kwargs):\n        if accum_iters &lt; 1:\n            raise ValueError('accum_iters must be &gt;= 1')\n        super(AdamAccumulate, self).__init__(**kwargs)\n        with K.name_scope(self.__class__.__name__):\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.lr = K.variable(lr, name='lr')\n            self.beta_1 = K.variable(beta_1, name='beta_1')\n            self.beta_2 = K.variable(beta_2, name='beta_2')\n            self.decay = K.variable(decay, name='decay')\n        if epsilon is None:\n            epsilon = K.epsilon()\n        self.epsilon = epsilon\n        self.initial_decay = decay\n        self.amsgrad = amsgrad\n        self.accum_iters = K.variable(accum_iters, K.dtype(self.iterations))\n        self.accum_iters_float = K.cast(self.accum_iters, K.floatx())\n\n    @interfaces.legacy_get_updates_support\n    def get_updates(self, loss, params):\n        grads = self.get_gradients(loss, params)\n        self.updates = [K.update_add(self.iterations, 1)]\n\n        lr = self.lr\n\n        completed_updates = K.cast(K.tf.floordiv(self.iterations, self.accum_iters), K.floatx())\n\n        if self.initial_decay &gt; 0:\n            lr = lr * (1. / (1. + self.decay * completed_updates))\n\n        t = completed_updates + 1\n\n        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) / (1. - K.pow(self.beta_1, t)))\n\n        # self.iterations incremented after processing a batch\n        # batch:              1 2 3 4 5 6 7 8 9\n        # self.iterations:    0 1 2 3 4 5 6 7 8\n        # update_switch = 1:        x       x    (if accum_iters=4)  \n        update_switch = K.equal((self.iterations + 1) % self.accum_iters, 0)\n        update_switch = K.cast(update_switch, K.floatx())\n\n        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        gs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n\n        if self.amsgrad:\n            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        else:\n            vhats = [K.zeros(1) for _ in params]\n\n        self.weights = [self.iterations] + ms + vs + vhats\n\n        for p, g, m, v, vhat, tg in zip(params, grads, ms, vs, vhats, gs):\n\n            sum_grad = tg + g\n            avg_grad = sum_grad / self.accum_iters_float\n\n            m_t = (self.beta_1 * m) + (1. - self.beta_1) * avg_grad\n            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(avg_grad)\n\n            if self.amsgrad:\n                vhat_t = K.maximum(vhat, v_t)\n                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)\n                self.updates.append(K.update(vhat, (1 - update_switch) * vhat + update_switch * vhat_t))\n            else:\n                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)\n\n            self.updates.append(K.update(m, (1 - update_switch) * m + update_switch * m_t))\n            self.updates.append(K.update(v, (1 - update_switch) * v + update_switch * v_t))\n            self.updates.append(K.update(tg, (1 - update_switch) * sum_grad))\n            new_p = p_t\n\n            # Apply constraints.\n            if getattr(p, 'constraint', None) is not None:\n                new_p = p.constraint(new_p)\n\n            self.updates.append(K.update(p, (1 - update_switch) * p + update_switch * new_p))\n        return self.updates\n\n    def get_config(self):\n        config = {'lr': float(K.get_value(self.lr)),\n                  'beta_1': float(K.get_value(self.beta_1)),\n                  'beta_2': float(K.get_value(self.beta_2)),\n                  'decay': float(K.get_value(self.decay)),\n                  'epsilon': self.epsilon,\n                  'amsgrad': self.amsgrad}\n        base_config = super(AdamAccumulate, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nopt = AdamAccumulate(lr=0.001, decay=1e-5, accum_iters=5)\nmodel.compile( loss='categorical_crossentropy',   # Loss function\n                            optimizer=opt,        # Optimization technique\n                            metrics=['accuracy']) # Accuracy matrix\nmodel.fit(X_train, y_train, batch_size = 10)\n"
" the: 4, player: 1, bats: 1, well: 2, today: 3,...\n\n the: 0, quick:5, flying:3, bats:1, caught:1, bugs:2\n\n('This', 'POS_DT', 'is', 'POS_VBZ', 'POS', 'POS_NNP', 'example', 'POS_NN')\n\n('This_DT', 'is_VBZ', 'POS_NNP', 'example_NN')\n"
"mask = ~dataset['v3'].isnull()\ndataset['v3'][mask] = enc.fit_transform(dataset['v3'][mask])\n\ndataset['v3'] = dataset['v3'].factorize()[0]\n"
"from sklearn import preprocessing \nfor f in train.columns: \n    if train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(train[f].values)) \n        train[f] = lbl.transform(list(train[f].values))\n\nfor f in test.columns: \n    if test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder() \n        lbl.fit(list(test[f].values)) \n        test[f] = lbl.transform(list(test[f].values))\n\ntrain.fillna((-999), inplace=True) \ntest.fillna((-999), inplace=True)\n\ntrain=np.array(train) \ntest=np.array(test) \ntrain = train.astype(float) \ntest = test.astype(float)\n"
'[\n (split1_train_idxs, split1_test_idxs),\n (split2_train_idxs, split2_test_idxs),\n (split3_train_idxs, split3_test_idxs),\n ...\n]\n\ngroups = df.groupby(df.date.dt.year).groups\n# {2012: [0, 1], 2013: [2], 2014: [3], 2015: [4, 5]}\nsorted_groups = [value for (key, value) in sorted(groups.items())] \n# [[0, 1], [2], [3], [4, 5]]\n\ncv = [(sorted_groups[i] + sorted_groups[i+1], sorted_groups[i+2])\n      for i in range(len(sorted_groups)-2)]\n\n[([0, 1, 2], [3]),  # idxs of first split as (train, test) tuple\n ([2, 3], [4, 5])]  # idxs of second split as (train, test) tuple\n\nGridSearchCV(estimator, param_grid, cv=cv, ...)\n'
'from sklearn.preprocessing import StandardScaler\nX_train = np.array([[ 1., -1.,  2.],\n                    [ 2.,  0.,  0.],\n                    [ 0.,  1., -1.]])\nscaler = preprocessing.StandardScaler().fit(X_train)\n\n&gt;&gt;&gt;scaler.mean_\narray([ 1. ...,  0. ...,  0.33...])\n&gt;&gt;&gt;scaler.scale_                                       \narray([ 0.81...,  0.81...,  1.24...])\n\nimport numpy as np\n\nX_train_scaled = scaler.transform(X_train)\nnew_data = np.array([-1.,  1., 0.])    \nnew_data_scaled = scaler.transform(new_data)\n&gt;&gt;&gt;new_data_scaled\narray([[-2.44...,  1.22..., -0.26...]])\n'
'def precision_threshold(threshold=0.5):\n    def precision(y_true, y_pred):\n        """Precision metric.\n        Computes the precision over the whole batch using threshold_value.\n        """\n        threshold_value = threshold\n        # Adaptation of the "round()" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.\n        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n        # count the predicted positives\n        predicted_positives = K.sum(y_pred)\n        # Get the precision ratio\n        precision_ratio = true_positives / (predicted_positives + K.epsilon())\n        return precision_ratio\n    return precision\n\ndef recall_threshold(threshold = 0.5):\n    def recall(y_true, y_pred):\n        """Recall metric.\n        Computes the recall over the whole batch using threshold_value.\n        """\n        threshold_value = threshold\n        # Adaptation of the "round()" used before to get the predictions. Clipping to make sure that the predicted raw values are between 0 and 1.\n        y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), threshold_value), K.floatx())\n        # Compute the number of true positives. Rounding in prevention to make sure we have an integer.\n        true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n        # Compute the number of positive targets.\n        possible_positives = K.sum(K.clip(y_true, 0, 1))\n        recall_ratio = true_positives / (possible_positives + K.epsilon())\n        return recall_ratio\n    return recall\n\nmodel.compile(..., metrics = [precision_threshold(0.1), precision_threshold(0.2),precision_threshold(0.8), recall_threshold(0.2,...)])\n'
"from sklearn import datasets\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\niris = datasets.load_iris()\ngbc = GradientBoostingClassifier()\nparameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 1], \n              'min_samples_split':[2,5,10,20], \n              'max_depth':[2,3,5,10]}\n\nclf = GridSearchCV(gbc, parameters)\nclf.fit(iris.data, iris.target)\n\nprint(clf.best_params_)\n# {'learning_rate': 1, 'max_depth': 2, 'min_samples_split': 2}\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(iris.data, iris.target, \n                                                 test_size=0.33, \n                                                 random_state=42)\n\nclf = GridSearchCV(gbc, parameters)\nclf.fit(X_train, y_train)\n\nprint(clf.best_params_)\n# {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2}\n"
"import tensorflow as tf\nimport numpy as np\n\nimage = np.arange(10 * 10 * 1).reshape(1, 10, 10, 1)\n\nimages = tf.convert_to_tensor(image.astype(np.float32))\n\nfilter_size = 3\nsobel_x = tf.constant([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], tf.float32)\nsobel_x_filter = tf.reshape(sobel_x, [3, 3, 1, 1])\n\nimage_patches = tf.extract_image_patches(images,\n                                         [1, filter_size, filter_size, 1],\n                                         [1, 1, 1, 1], [1, 1, 1, 1],\n                                         padding='SAME')\n\n\nactual = tf.reduce_sum(tf.multiply(image_patches, tf.reshape(sobel_x_filter, [9])), 3, keep_dims=True)\nexpected = tf.nn.conv2d(images, sobel_x_filter, strides=[1, 1, 1, 1], padding='SAME')\n\nwith tf.Session() as sess:\n    print sess.run(tf.reduce_sum(expected - actual))\n"
'data.rdd.partitions.size\n\nnewDF = data.repartition(3000)\n\nnewDF.rdd.partitions.size\n'
"from tensorflow.examples.tutorials.mnist import input_data\ninput_data.read_data_sets('my/directory')\n\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import extract_images, extract_labels\n\nwith open('my/directory/train-images-idx3-ubyte.gz', 'rb') as f:\n  train_images = extract_images(f)\nwith open('my/directory/train-labels-idx1-ubyte.gz', 'rb') as f:\n  train_labels = extract_labels(f)\n\nwith open('my/directory/t10k-images-idx3-ubyte.gz', 'rb') as f:\n  test_images = extract_images(f)\nwith open('my/directory/t10k-labels-idx1-ubyte.gz', 'rb') as f:\n  test_labels = extract_labels(f)\n"
'import tensorflow as tf\n\na = tf.add(1, 2, name="Add_these_numbers")\nb = tf.multiply(a, 3)\nc = tf.add(4, 5, name="And_These_ones")\nd = tf.multiply(c, 6, name="Multiply_these_numbers")\ne = tf.multiply(4, 5, name="B_add")\nf = tf.div(c, 6, name="B_mul")\ng = tf.add(b, d)\nh = tf.multiply(g, f)\n\nwith tf.Session() as sess:\n    writer = tf.summary.FileWriter("output", sess.graph)\n    print(sess.run(h))\n    writer.close()\n\ntensorboard --logdir=output\n'
"for g, v in grads_and_vars:\n  tf.summary.histogram(v.name, v)\n  tf.summary.histogram(v.name + '_grad', g)\n\nmerged = tf.summary.merge_all()\nwriter = tf.summary.FileWriter('train_log_layer2', tf.get_default_graph())\n\n...\n\n_, summary = sess.run([train_op, merged], feed_dict={I: 2*np.random.rand(1, 1)-1})\nif i % 10 == 0:\n  writer.add_summary(summary, global_step=i)\n\nW_init = tf.contrib.layers.xavier_initializer()\nb_init = tf.constant_initializer(0.1)\n"
'  # more code\n\n  X, y = load_iris(return_X_y=True)\n  index = [\'r%d\' % x for x in range(len(y))]\n  y_frame = pd.DataFrame(y, index=index)\n  sample_weight = np.array([1 + 100 * (i % 25) for i in range(len(X))])\n  sample_weight_frame = pd.DataFrame(sample_weight, index=index)\n\n  # more code\n\n  def score_f(y_true, y_pred, sample_weight):\n      return log_loss(y_true.values, y_pred,\n                      sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1),\n                      normalize=True)\n\n  score_params = {"sample_weight": sample_weight_frame}\n  my_scorer = make_scorer(score_f,\n                          greater_is_better=False, \n                          needs_proba=True, \n                          needs_threshold=False,\n                          **score_params)\n\n  grid_clf = GridSearchCV(estimator=rfc,\n                          scoring=my_scorer,\n                          cv=inner_cv,\n                          param_grid=search_params,\n                          refit=True,\n                          return_train_score=False,\n                          iid=False)  # in this usage, the results are the same for `iid=True` and `iid=False`\n  grid_clf.fit(X, y_frame)\n\n  # more code\n\nfrom __future__ import division\n\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import GridSearchCV, RepeatedKFold\nfrom sklearn.metrics import  make_scorer\nimport pandas as pd\n\ndef grid_cv(X_in, y_in, w_in, cv, max_features_grid, use_weighting):\n  out_results = dict()\n\n  for k in max_features_grid:\n    clf = RandomForestClassifier(n_estimators=256,\n                                 criterion="entropy",\n                                 warm_start=False,\n                                 n_jobs=1,\n                                 random_state=RANDOM_STATE,\n                                 max_features=k)\n    for train_ndx, test_ndx in cv.split(X=X_in, y=y_in):\n      X_train = X_in[train_ndx, :]\n      y_train = y_in[train_ndx]\n      w_train = w_in[train_ndx]\n      y_test = y_in[test_ndx]\n\n      clf.fit(X=X_train, y=y_train, sample_weight=w_train)\n\n      y_hat = clf.predict_proba(X=X_in[test_ndx, :])\n      if use_weighting:\n        w_test = w_in[test_ndx]\n        w_i_sum = w_test.sum()\n        score = w_i_sum / w_in.sum() * log_loss(y_true=y_test, y_pred=y_hat, sample_weight=w_test)\n      else:\n        score = log_loss(y_true=y_test, y_pred=y_hat)\n\n      results = out_results.get(k, [])\n      results.append(score)\n      out_results.update({k: results})\n\n  for k, v in out_results.items():\n    if use_weighting:\n      mean_score = sum(v)\n    else:\n      mean_score = np.mean(v)\n    out_results.update({k: mean_score})\n\n  best_score = min(out_results.values())\n  best_param = min(out_results, key=out_results.get)\n  return best_score, best_param\n\n\n#if __name__ == "__main__":\nif True:\n  RANDOM_STATE = 1337\n  X, y = load_iris(return_X_y=True)\n  index = [\'r%d\' % x for x in range(len(y))]\n  y_frame = pd.DataFrame(y, index=index)\n  sample_weight = np.array([1 + 100 * (i % 25) for i in range(len(X))])\n  sample_weight_frame = pd.DataFrame(sample_weight, index=index)\n  # sample_weight = np.array([1 for _ in range(len(X))])\n\n  inner_cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)\n\n  outer_cv = RepeatedKFold(n_splits=3, n_repeats=1, random_state=RANDOM_STATE)\n\n  rfc = RandomForestClassifier(n_estimators=256,\n                               criterion="entropy",\n                               warm_start=False,\n                               n_jobs=1,\n                               random_state=RANDOM_STATE)\n  search_params = {"max_features": [1, 2, 3, 4]}\n\n\n  def score_f(y_true, y_pred, sample_weight):\n      return log_loss(y_true.values, y_pred,\n                      sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1),\n                      normalize=True)\n\n  score_params = {"sample_weight": sample_weight_frame}\n  my_scorer = make_scorer(score_f,\n                          greater_is_better=False, \n                          needs_proba=True, \n                          needs_threshold=False,\n                          **score_params)\n\n  grid_clf = GridSearchCV(estimator=rfc,\n                          scoring=my_scorer,\n                          cv=inner_cv,\n                          param_grid=search_params,\n                          refit=True,\n                          return_train_score=False,\n                          iid=False)  # in this usage, the results are the same for `iid=True` and `iid=False`\n  grid_clf.fit(X, y_frame)\n  print("This is the best out-of-sample score using GridSearchCV: %.6f." % -grid_clf.best_score_)\n\n  msg = """This is the best out-of-sample score %s weighting using grid_cv: %.6f."""\n  score_with_weights, param_with_weights = grid_cv(X_in=X,\n                                                   y_in=y,\n                                                   w_in=sample_weight,\n                                                   cv=inner_cv,\n                                                   max_features_grid=search_params.get(\n                                                     "max_features"),\n                                                   use_weighting=True)\n  print(msg % ("WITH", score_with_weights))\n\n  score_without_weights, param_without_weights = grid_cv(X_in=X,\n                                                         y_in=y,\n                                                         w_in=sample_weight,\n                                                         cv=inner_cv,\n                                                         max_features_grid=search_params.get(\n                                                           "max_features"),\n                                                         use_weighting=False)\n  print(msg % ("WITHOUT", score_without_weights))\n\nThis is the best out-of-sample score using GridSearchCV: 0.095439.\nThis is the best out-of-sample score WITH weighting using grid_cv: 0.099367.\nThis is the best out-of-sample score WITHOUT weighting using grid_cv: 0.135692.\n'
'def bootstrap(x, f, nsamples=1000):\n    stats = [f(x[np.random.randint(x.shape[0], size=x.shape[0])]) for _ in range(nsamples)]\n    return np.percentile(stats, (2.5, 97.5))\n\ndef bootstrap_auc(clf, X_train, y_train, X_test, y_test, nsamples=1000):\n    auc_values = []\n    for b in range(nsamples):\n        idx = np.random.randint(X_train.shape[0], size=X_train.shape[0])\n        clf.fit(X_train[idx], y_train[idx])\n        pred = clf.predict_proba(X_test)[:, 1]\n        roc_auc = roc_auc_score(y_test.ravel(), pred.ravel())\n        auc_values.append(roc_auc)\n    return np.percentile(auc_values, (2.5, 97.5))\n\ndef permutation_test(clf, X_train, y_train, X_test, y_test, nsamples=1000):\n    idx1 = np.arange(X_train.shape[0])\n    idx2 = np.arange(X_test.shape[0])\n    auc_values = np.empty(nsamples)\n    for b in range(nsamples):\n        np.random.shuffle(idx1)  # Shuffles in-place\n        np.random.shuffle(idx2)\n        clf.fit(X_train, y_train[idx1])\n        pred = clf.predict_proba(X_test)[:, 1]\n        roc_auc = roc_auc_score(y_test[idx2].ravel(), pred.ravel())\n        auc_values[b] = roc_auc\n    clf.fit(X_train, y_train)\n    pred = clf.predict_proba(X_test)[:, 1]\n    roc_auc = roc_auc_score(y_test.ravel(), pred.ravel())\n    return roc_auc, np.mean(auc_values &gt;= roc_auc)\n\ndef permutation_test_between_clfs(y_test, pred_proba_1, pred_proba_2, nsamples=1000):\n    auc_differences = []\n    auc1 = roc_auc_score(y_test.ravel(), pred_proba_1.ravel())\n    auc2 = roc_auc_score(y_test.ravel(), pred_proba_2.ravel())\n    observed_difference = auc1 - auc2\n    for _ in range(nsamples):\n        mask = np.random.randint(2, size=len(pred_proba_1.ravel()))\n        p1 = np.where(mask, pred_proba_1.ravel(), pred_proba_2.ravel())\n        p2 = np.where(mask, pred_proba_2.ravel(), pred_proba_1.ravel())\n        auc1 = roc_auc_score(y_test.ravel(), p1)\n        auc2 = roc_auc_score(y_test.ravel(), p2)\n        auc_differences(auc1 - auc2)\n    return observed_difference, np.mean(auc_differences &gt;= observed_difference)\n'
'In [18]: torch.set_printoptions(edgeitems=1)\n\nIn [19]: a\nOut[19]:\ntensor([[-0.7698,  ..., -0.1949],\n        ...,\n        [-0.7321,  ...,  0.8537]])\n\nIn [20]: torch.set_printoptions(edgeitems=3)\n\nIn [21]: a\nOut[21]:\ntensor([[-0.7698,  1.3383,  0.5649,  ...,  1.3567,  0.6896, -0.1949],\n        [-0.5761, -0.9789, -0.2058,  ..., -0.5843,  2.6311, -0.0008],\n        [ 1.3152,  1.8851, -0.9761,  ...,  0.8639, -0.6237,  0.5646],\n        ...,\n        [ 0.2851,  0.5504, -0.9471,  ...,  0.0688, -0.7777,  0.1661],\n        [ 2.9616, -0.8685, -1.5467,  ..., -1.4646,  1.1098, -1.0873],\n        [-0.7321,  0.7610,  0.3182,  ...,  2.5859, -0.9709,  0.8537]])\n'
"#considering you used 'softplus' instead of 'PRelu' in speakers\ndef kullback_leibler_divergence(speakers):\n    x, y = speakers\n    x = x + ks.backend.epsilon()\n    y = y + ks.backend.epsilon()\n    return ks.backend.sum(x * ks.backend.log(x / y), axis=-1)\n\ndistance1 = ks.layers.Lambda(kullback_leibler_divergence,\n                            name='distance1')([processed_a, processed_b])\ndistance2 = ks.layers.Lambda(kullback_leibler_divergence,\n                            name='distance2')([processed_b, processed_a])\ndistance = ks.layers.Add(name='dist_add')([distance1,distance2])\n\nMARGIN = someValue\nhinge = ks.backend.mean(ks.backend.softplus(MARGIN - y_pred), axis=-1)\n\ndef customMetric(y_true_targets, y_pred_KBL):\n    isMatch = ks.backend.less(y_pred_KBL, threshold)\n    isMatch = ks.backend.cast(isMatch, ks.backend.floatx())\n\n    isMatch = ks.backend.equal(y_true_targets, isMatch)\n    isMatch = ks.backend.cast(isMatch, ks.backend.floatx())\n\n    return ks.backend.mean(isMatch)\n"
'for name, param in model.named_parameters():\n    if param.requires_grad:\n        print(name)\n\nmodel.state_dict().keys()\n'
'$ conda install -c conda-forge tensorboard\n\n$ conda install -c conda-forge protobuf\n'
'import cv2\nimport numpy as np\nimport pytesseract\n\npytesseract.pytesseract.tesseract_cmd = r"C:\\Program Files\\Tesseract-OCR\\tesseract.exe"\n\n# Load image, grayscale, Otsu\'s threshold\nimage = cv2.imread(\'1.jpg\')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\nclean = thresh.copy()\n\n# Remove horizontal lines\nhorizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15,1))\ndetect_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\ncnts = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    cv2.drawContours(clean, [c], -1, 0, 3)\n\n# Remove vertical lines\nvertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,30))\ndetect_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\ncnts = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    cv2.drawContours(clean, [c], -1, 0, 3)\n\ncnts = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    # Remove diagonal lines\n    area = cv2.contourArea(c)\n    if area &lt; 100:\n        cv2.drawContours(clean, [c], -1, 0, 3)\n    # Remove circle objects\n    elif area &gt; 1000:\n        cv2.drawContours(clean, [c], -1, 0, -1)\n    # Remove curve stuff\n    peri = cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n    x,y,w,h = cv2.boundingRect(c)\n    if len(approx) == 4:\n        cv2.rectangle(clean, (x, y), (x + w, y + h), 0, -1)\n\nopen_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\nopening = cv2.morphologyEx(clean, cv2.MORPH_OPEN, open_kernel, iterations=2)\nclose_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,2))\nclose = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, close_kernel, iterations=4)\ncnts = cv2.findContours(close, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    x,y,w,h = cv2.boundingRect(c)\n    area = cv2.contourArea(c)\n    if area &gt; 500:\n        ROI = image[y:y+h, x:x+w]\n        ROI = cv2.GaussianBlur(ROI, (3,3), 0)\n        data = pytesseract.image_to_string(ROI, lang=\'eng\',config=\'--psm 6\')\n        if data.isalnum():\n            cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 2)\n            print(data)\n\ncv2.imwrite(\'image.png\', image)\ncv2.imwrite(\'clean.png\', clean)\ncv2.imwrite(\'close.png\', close)\ncv2.imwrite(\'opening.png\', opening)\ncv2.waitKey()\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; train = pd.DataFrame({'a' : ['a', 'b', 'a'], 'd' : ['e', 'e', 'f'],\n...                       'b' : [0, 1, 1], 'c' : ['b', 'c', 'b']})\n&gt;&gt;&gt; samples = [dict(enumerate(sample)) for sample in train]\n&gt;&gt;&gt; samples\n[{0: 'a'}, {0: 'b'}, {0: 'c'}, {0: 'd'}]\n\n&gt;&gt;&gt; train_as_dicts = [dict(r.iteritems()) for _, r in train.iterrows()]\n&gt;&gt;&gt; train_as_dicts\n[{'a': 'a', 'c': 'b', 'b': 0, 'd': 'e'},\n {'a': 'b', 'c': 'c', 'b': 1, 'd': 'e'},\n {'a': 'a', 'c': 'b', 'b': 1, 'd': 'f'}]\n\n&gt;&gt;&gt; from sklearn.feature_extraction import DictVectorizer\n\n&gt;&gt;&gt; vectorizer = DictVectorizer()\n&gt;&gt;&gt; vectorized_sparse = vectorizer.fit_transform(train_as_dicts)\n&gt;&gt;&gt; vectorized_sparse\n&lt;3x7 sparse matrix of type '&lt;type 'numpy.float64'&gt;'\n    with 12 stored elements in Compressed Sparse Row format&gt;\n\n&gt;&gt;&gt; vectorized_array = vectorized_sparse.toarray()\n&gt;&gt;&gt; vectorized_array\narray([[ 1.,  0.,  0.,  1.,  0.,  1.,  0.],\n       [ 0.,  1.,  1.,  0.,  1.,  1.,  0.],\n       [ 1.,  0.,  1.,  1.,  0.,  0.,  1.]])\n\n&gt;&gt;&gt; vectorizer.get_feature_names()\n['a=a', 'a=b', 'b', 'c=b', 'c=c', 'd=e', 'd=f']\n"
'from mlxtend.classifier import EnsembleVoteClassifier\nimport copy\neclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1,1,1], refit=False)\n'
'import tensorflow as tf\ndef correlation_coefficient(y_true, y_pred):\n    pearson_r, update_op = tf.contrib.metrics.streaming_pearson_correlation(y_pred, y_true, name=\'pearson_r\'\n    # find all variables created for this metric\n    metric_vars = [i for i in tf.local_variables() if \'pearson_r\'  in i.name.split(\'/\')]\n\n    # Add metric variables to GLOBAL_VARIABLES collection.\n    # They will be initialized for new session.\n    for v in metric_vars:\n        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n\n    # force to update metric values\n    with tf.control_dependencies([update_op]):\n        pearson_r = tf.identity(pearson_r)\n        return 1-pearson_r**2\n\n...\n\nmodel.compile(loss=correlation_coefficient, optimizer=\'adam\')\n\nfrom keras import backend as K\ndef correlation_coefficient_loss(y_true, y_pred):\n    x = y_true\n    y = y_pred\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return 1 - K.square(r)\n\nimport tensorflow as tf\nfrom keras import backend as K\nimport numpy as np\nimport scipy.stats\n\ninputa = np.array([[3,1,2,3,4,5],\n                    [1,2,3,4,5,6],\n                    [1,2,3,4,5,6]])\ninputb = np.array([[3,1,2,3,4,5],\n                    [3,1,2,3,4,5],\n                    [6,5,4,3,2,1]])\n\nwith tf.Session() as sess:\n    a = tf.placeholder(tf.float32, shape=[None])\n    b = tf.placeholder(tf.float32, shape=[None])\n    f1 = correlation_coefficient(a, b)\n    f2 = correlation_coefficient_loss(a, b)\n\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(inputa.shape[0]):\n\n        f1_result, f2_result = sess.run([f1, f2], feed_dict={a: inputa[i], b: inputb[i]})\n        scipy_result =1- scipy.stats.pearsonr(inputa[i], inputb[i])[0]**2\n        print("a: "+ str(inputa[i]) + " b: " + str(inputb[i]))\n        print("correlation_coefficient: " + str(f1_result))\n        print("correlation_coefficient_loss: " + str(f2_result))\n        print("scipy.stats.pearsonr:" + str(scipy_result))\n\na: [3 1 2 3 4 5] b: [3 1 2 3 4 5]\ncorrelation_coefficient: -2.38419e-07\ncorrelation_coefficient_loss: 0.0\nscipy.stats.pearsonr:0.0\na: [1 2 3 4 5 6] b: [3 1 2 3 4 5]\ncorrelation_coefficient: 0.292036\ncorrelation_coefficient_loss: 0.428571\nscipy.stats.pearsonr:0.428571428571\na: [1 2 3 4 5 6] b: [6 5 4 3 2 1]\ncorrelation_coefficient: 0.994918\ncorrelation_coefficient_loss: 0.0\nscipy.stats.pearsonr:0.0\n'
'from imutils.object_detection import non_max_suppression\nimport numpy as np\nimport cv2\n\ndef EAST_text_detector(original, image, confidence=0.25):\n    # Set the new width and height and determine the changed ratio\n    (h, W) = image.shape[:2]\n    (newW, newH) = (640, 640)\n    rW = W / float(newW)\n    rH = h / float(newH)\n\n    # Resize the image and grab the new image dimensions\n    image = cv2.resize(image, (newW, newH))\n    (h, W) = image.shape[:2]\n\n    # Define the two output layer names for the EAST detector model that\n    # we are interested -- the first is the output probabilities and the\n    # second can be used to derive the bounding box coordinates of text\n    layerNames = [\n        "feature_fusion/Conv_7/Sigmoid",\n        "feature_fusion/concat_3"]\n\n    net = cv2.dnn.readNet(\'frozen_east_text_detection.pb\')\n\n    # Construct a blob from the image and then perform a forward pass of\n    # the model to obtain the two output layer sets\n    blob = cv2.dnn.blobFromImage(image, 1.0, (W, h), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n    net.setInput(blob)\n    (scores, geometry) = net.forward(layerNames)\n\n    # Grab the number of rows and columns from the scores volume, then\n    # initialize our set of bounding box rectangles and corresponding\n    # confidence scores\n    (numRows, numCols) = scores.shape[2:4]\n    rects = []\n    confidences = []\n\n    # Loop over the number of rows\n    for y in range(0, numRows):\n        # Extract the scores (probabilities), followed by the geometrical\n        # data used to derive potential bounding box coordinates that\n        # surround text\n        scoresData = scores[0, 0, y]\n        xData0 = geometry[0, 0, y]\n        xData1 = geometry[0, 1, y]\n        xData2 = geometry[0, 2, y]\n        xData3 = geometry[0, 3, y]\n        anglesData = geometry[0, 4, y]\n\n        # Loop over the number of columns\n        for x in range(0, numCols):\n            # If our score does not have sufficient probability, ignore it\n            if scoresData[x] &lt; confidence:\n                continue\n\n            # Compute the offset factor as our resulting feature maps will\n            # be 4x smaller than the input image\n            (offsetX, offsetY) = (x * 4.0, y * 4.0)\n\n            # Extract the rotation angle for the prediction and then\n            # compute the sin and cosine\n            angle = anglesData[x]\n            cos = np.cos(angle)\n            sin = np.sin(angle)\n\n            # Use the geometry volume to derive the width and height of\n            # the bounding box\n            h = xData0[x] + xData2[x]\n            w = xData1[x] + xData3[x]\n\n            # Compute both the starting and ending (x, y)-coordinates for\n            # the text prediction bounding box\n            endX = int(offsetX + (cos * xData1[x]) + (sin * xData2[x]))\n            endY = int(offsetY - (sin * xData1[x]) + (cos * xData2[x]))\n            startX = int(endX - w)\n            startY = int(endY - h)\n\n            # Add the bounding box coordinates and probability score to\n            # our respective lists\n            rects.append((startX, startY, endX, endY))\n            confidences.append(scoresData[x])\n\n    # Apply non-maxima suppression to suppress weak, overlapping bounding\n    # boxes\n    boxes = non_max_suppression(np.array(rects), probs=confidences)\n\n    # Loop over the bounding boxes\n    for (startX, startY, endX, endY) in boxes:\n        # Scale the bounding box coordinates based on the respective\n        # ratios\n        startX = int(startX * rW)\n        startY = int(startY * rH)\n        endX = int(endX * rW)\n        endY = int(endY * rH)\n\n        # Draw the bounding box on the image\n        cv2.rectangle(original, (startX, startY), (endX, endY), (36, 255, 12), 2)\n    return original\n\n# Convert to grayscale and Otsu\'s threshold\nimage = cv2.imread(\'1.png\')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\nclean = thresh.copy()\n\n# Remove horizontal lines\nhorizontal_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15,1))\ndetect_horizontal = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, horizontal_kernel, iterations=2)\ncnts = cv2.findContours(detect_horizontal, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    cv2.drawContours(clean, [c], -1, 0, 3)\n\n# Remove vertical lines\nvertical_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,30))\ndetect_vertical = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, vertical_kernel, iterations=2)\ncnts = cv2.findContours(detect_vertical, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    cv2.drawContours(clean, [c], -1, 0, 3)\n\n# Remove non-text contours (curves, diagonals, circlar shapes)\ncnts = cv2.findContours(clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    area = cv2.contourArea(c)\n    if area &gt; 1500:\n        cv2.drawContours(clean, [c], -1, 0, -1)\n    peri = cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n    x,y,w,h = cv2.boundingRect(c)\n    if len(approx) == 4:\n        cv2.rectangle(clean, (x, y), (x + w, y + h), 0, -1)\n\n# Bitwise-and with original image to remove contours\nfiltered = cv2.bitwise_and(image, image, mask=clean)\nfiltered[clean==0] = (255,255,255)\n\n# Perform EAST text detection\nresult = EAST_text_detector(image, filtered)\n\ncv2.imshow(\'filtered\', filtered)\ncv2.imshow(\'result\', result)\ncv2.waitKey()\n'
"Y=pdist(X, 'correlation')\n\nZ=linkage(X, 'single', 'correlation')\ndendrogram(Z, color_threshold=0)\n"
'&gt;&gt;&gt; import nltk\n&gt;&gt;&gt; s = "This is some sample data.  Nltk will use the words in this string to make ngrams.  I hope that this is useful.".split()\n&gt;&gt;&gt; model = nltk.NgramModel(2, s)\n&gt;&gt;&gt; model._ngrams\nset([(\'to\', \'make\'), (\'sample\', \'data.\'), (\'the\', \'words\'), (\'will\', \'use\'), (\'some\', \'sample\'), (\'\', \'This\'), (\'use\', \'the\'), (\'make\', \'ngrams.\'), (\'ngrams.\', \'I\'), (\'hope\', \'that\'\n), (\'is\', \'some\'), (\'is\', \'useful.\'), (\'I\', \'hope\'), (\'this\', \'string\'), (\'Nltk\', \'will\'), (\'words\', \'in\'), (\'this\', \'is\'), (\'data.\', \'Nltk\'), (\'that\', \'this\'), (\'string\', \'to\'), (\'\nin\', \'this\'), (\'This\', \'is\')])\n'
'cost_f = []\nwhile (abs(theta1_guess-theta1_last) &gt; variance or abs(theta0_guess - theta0_last) &gt; variance):\n\n    theta1_last = theta1_guess\n    theta0_last = theta0_guess\n\n    hypothesis = create_hypothesis(theta1_guess, theta0_guess)\n    cost_f.append((1./(2*m))*sum([ pow(hypothesis(point[0]) - point[1], 2) for point in data]))\n\n    theta0_guess = theta0_guess - learning_rate * (1./m) * sum([hypothesis(point[0]) - point[1] for point in data])\n    theta1_guess = theta1_guess - learning_rate * (1./m) * sum([ (hypothesis(point[0]) - point[1]) * point[0] for point in data])   \n\nimport pylab\npylab.plot(range(len(cost_f)), cost_f)\npylab.show()\n'
'import pymc as mc\nimport numpy as np\n\ndata = np.random.normal(-200,15,size=1000)\n\nmean = mc.Uniform(\'mean\', lower=min(data), upper=max(data))\nstd_dev = mc.Uniform(\'std_dev\', lower=0, upper=50)\n\n@mc.stochastic(observed=True)\ndef custom_stochastic(value=data, mean=mean, std_dev=std_dev):\n    return np.sum(-np.log(std_dev) - 0.5*np.log(2) - \n                  0.5*np.log(np.pi) - \n                  (value-mean)**2 / (2*(std_dev**2)))\n\n\nmodel = mc.MCMC([mean,std_dev,custom_stochastic])\nmodel.sample(iter=5000)\n\nprint "!"\nprint(model.stats()[\'mean\'][\'mean\'])\nprint(model.stats()[\'std_dev\'][\'mean\'])\n'
'    | y=0  y=1\n----+---------\nx=0 |  a    b\nx=1 |  c    d\n\n    |  y=0  y=1\n----+----------\nx=0 |   8    8\nx=1 |  20  188\n'
"&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; import re\n&gt;&gt;&gt; x = ['this is a foo bar', 'you are a foo bar black sheep']\n&gt;&gt;&gt; def words_and_char_bigrams(text):\n...     words = re.findall(r'\\w{3,}', text)\n...     for w in words:\n...         yield w\n...         for i in range(len(w) - 2):\n...             yield w[i:i+2]\n...             \n&gt;&gt;&gt; v = CountVectorizer(analyzer=words_and_char_bigrams)\n&gt;&gt;&gt; pprint(v.fit(x).vocabulary_)\n{'ac': 0,\n 'ar': 1,\n 'are': 2,\n 'ba': 3,\n 'bar': 4,\n 'bl': 5,\n 'black': 6,\n 'ee': 7,\n 'fo': 8,\n 'foo': 9,\n 'he': 10,\n 'hi': 11,\n 'la': 12,\n 'sh': 13,\n 'sheep': 14,\n 'th': 15,\n 'this': 16,\n 'yo': 17,\n 'you': 18}\n"
'switch(x&lt;0, 0, x)\n\ndef relu(x):\n    return theano.tensor.switch(x&lt;0, 0, x)\nHiddenLayer(..., activation=relu)\n'
' np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))\n'
'counts = Counter(zip(predicted, gold))\n\naccuracy = (true_pos + true_neg) / float(len(gold)) if gold else 0\n'
'make_pipeline(make_union(PolynomialFeatures(), PCA()), RFE(RandomForestClassifier()))\n'
'In [8]: g = sp_randint(1, 11)\n\nIn [9]: g.rvs(20)\nOut[9]: \narray([ 5,  2,  9, 10,  6,  9,  9,  8,  1,  5,  1,  8,  1,  5,  5,  4,  6,\n        5,  8,  4])\n\nparam_dist = {"n_estimators": [1, 3, 4], \n              "max_depth": [3, None],\n              "max_features": [1, 3, 4],\n              "min_samples_split": [1, 3, 4],\n              "min_samples_leaf": [1, 3, 4],\n             }\n'
'stop_words = frozenset(["word1", "word2","word3"])\n'
"~/nltk_data/corpora/omw/xxx/wn-data-xxx.tab\n\n&gt;&gt;&gt; wn.synset('dog.n.01').lemma_names('ita') # change 'ita' to 'xxx'\n['cane', 'Canis_familiaris']\n\nHe        PRP  B-NP\nreckons   VBZ  B-VP\nthe       DT   B-NP\ncurrent   JJ   I-NP\naccount   NN   I-NP\ndeficit   NN   I-NP\nwill      MD   B-VP\nnarrow    VB   I-VP\nto        TO   B-PP\nonly      RB   B-NP\n#         #    I-NP\n1.8       CD   I-NP\nbillion   CD   I-NP\nin        IN   B-PP\nSeptember NNP  B-NP\n.         .    O\n\nHe        PRP  B-NP\nreckons   VBZ  B-VP\n..\n"
'f0 f1 f2\n0, 0, 3\n1, 1, 0\n0, 2, 1\n1, 0, 2\n\n|f0|  |  f1 |  |   f2   |\n\n1, 0, 1, 0, 0, 0, 0, 0, 1 \n0, 1, 0, 1, 0, 1, 0, 0, 0\n1, 0, 0, 0, 1, 0, 1, 0, 0\n0, 1, 1, 0, 0, 0, 0, 1, 0\n\nenc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]),\n\nenc.n_values_\narray([2, 3, 4])\n\nenc.feature_indices_\narray([0, 2, 5, 9])\n\nf1: [0, 1], f2: [2, 3, 4], f3: [5, 6, 7, 8]\n\n1, 0, 0, 1, 0, 0, 1, 0, 0\n'
'label_lines = [line.rstrip() for line \n                   in tf.gfile.GFile("/tf_files/retrained_labels.txt")]\n'
'import tensorflow as tf\nfrom tensorflow.python.framework import ops\n\n@ops.RegisterGradient("MyopGrad")\ndef frop_grad(op, grad):\n    x = op.inputs[0]\n    return 0 * x  # zero out to see the difference:\n\ndef fprop(x):\n    x = tf.sqrt(x)\n    out = tf.maximum(x, .2)\n    return out\n\na = tf.Variable(tf.constant([5., 4., 3., 2., 1.], dtype=tf.float32))\nh = fprop(a)\n\ng = tf.get_default_graph()\nwith g.gradient_override_map({\'Identity\': \'MyopGrad\'}):\n    h = tf.identity(h, name="Myop")\n    grad = tf.gradients(h, a)\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    result = sess.run(grad)\n\nprint(result[0])\n\n[ 0.  0.  0.  0.  0.]\n'
"pipeline = Pipeline([('fillna', FillNa()),\n                     ('categorical_to_numerical', CategoricalToNumerical()),\n                     ('features_selection', SelectKBest(k=nb_features)),\n                     ('random_forest', clf)])\n\n#Fit the data in the pipeline\npipeline.fit(X_train, y_train)\n\nperformance_meas = Perf()\nperformance_meas.fit(pipeline, X_train, y_train)\n"
'np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n\nIn [522]: np.arange(10)==np.arange(5,15)\nOut[522]: array([False, False, False, False, False, False, False, False, False, False], dtype=bool)\nIn [523]: np.arange(10)==np.arange(5,14)\n/usr/local/bin/ipython3:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n  #!/usr/bin/python3\nOut[523]: False\n'
"model.add(embedding_layer)\nmodel.add(LSTM(n_hidden, return_sequences=False))\nmodel.add(Dropout(dropout_keep_prob))\nmodel.add(Dense(vocab_size))\nmodel.add(Activation('linear'))\noptimizer = RMSprop(lr=self.lr)\n\n\ndef my_sparse_categorical_crossentropy(y_true, y_pred):\n    return K.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n\nmodel.compile(optimizer=optimizer,loss=my_sparse_categorical_crossentropy)\n"
"# these lines are copied from the example for loading MNIST data\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(60000, 784)\nx_train = x_train.astype('float32') # this line was missing\nx_train /= 255 # this line was missing too\n"
"import numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# Generate some data\nnp.random.seed(0)\nn = 1500\ndates = np.array('2005-01-01', dtype=np.datetime64) + np.arange(n)\ndata = 12*np.sin(2*np.pi*np.arange(n)/365) + np.random.normal(12, 2, 1500)\ndf = pd.DataFrame({'data': data}, index=dates)\n\n# Reproduce the example in OP\nseasonal_decompose(df, model='additive', freq=1).plot()\n\n# Redo the same thing, but with the known frequency\nseasonal_decompose(df, model='additive', freq=365).plot()\n"
'import tensorflow as tf\nimport tensorflow.contrib.opt as opt\n\nX = tf.Variable([1.0, 2.0])\n\npart_X = tf.scatter_nd([[0]], [X[0]], [2])\n\nX_2 = part_X + tf.stop_gradient(-part_X + X)\n\nY = tf.constant([2.0, -3.0])\n\nloss = tf.reduce_sum(tf.squared_difference(X_2, Y))\n\nopt = opt.ScipyOptimizerInterface(loss, [X])\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    opt.minimize(sess)\n\n    print("X: {}".format(X.eval()))\n'
"df = pd.DataFrame([['656', 341.341, 4535],\n                   ['545', 4325.132, 562]],\n                  columns=['col1', 'col2', 'col3'])\n\nprint(df.dtypes)\n\ncol1     object\ncol2    float64\ncol3      int64\ndtype: object\n\ncols = df.select_dtypes(exclude=['float']).columns\n\ndf[cols] = df[cols].apply(pd.to_numeric, downcast='float', errors='coerce')\n\nprint(df.dtypes)\n\ncol1    float32\ncol2    float64\ncol3    float32\ndtype: object\n\nprint(df)\n\n    col1      col2    col3\n0  656.0   341.341  4535.0\n1  545.0  4325.132   562.0\n"
'import torch\nfrom torch import nn\n\nclass TwoInputsNet(nn.Module):\n  def __init__(self):\n    super(TwoInputsNet, self).__init__()\n    self.conv = nn.Conv2d( ... )  # set up your layer here\n    self.fc1 = nn.Linear( ... )  # set up first FC layer\n    self.fc2 = nn.Linear( ... )  # set up the other FC layer\n\n  def forward(self, input1, input2):\n    c = self.conv(input1)\n    f = self.fc1(input2)\n    # now we can reshape `c` and `f` to 2D and concat them\n    combined = torch.cat((c.view(c.size(0), -1),\n                          f.view(f.size(0), -1)), dim=1)\n    out = self.fc2(combined)\n    return out\n'
"import numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nimport matplotlib.pyplot as plt\n\nmodel = Sequential()\nmodel.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape = (1,)))\nmodel.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(),loss='mse')\n\n# generate 10,000 random numbers in [-50, 50], along with their squares\nx = np.random.random((10000,1))*100-50\ny = x**2\n\n# fit the model, keeping 2,000 samples as validation set\nhist = model.fit(x,y,validation_split=0.2,\n             epochs= 15000,\n             batch_size=256)\n\n# check some predictions:\nprint(model.predict([4, -4, 11, 20, 8, -5]))\n# result:\n[[ 16.633354]\n [ 15.031291]\n [121.26833 ]\n [397.78638 ]\n [ 65.70035 ]\n [ 27.040245]]\n\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\np = np.random.random((1000,1))*100-50 # new random data in [-50, 50]\nplt.plot(p,model.predict(p), '.')\nplt.xlabel('x')\nplt.ylabel('prediction')\nplt.title('Predictions on NEW data in [-50,50]')\n\nplt.subplot(1,2,2)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,y,'.')\nplt.title('Original data')\n"
'def hash_trick(features, n_features):\n     for f in features:\n         res = np.zero_like(features)\n         h = usual_hash_function(f) # just the usual hashing\n         index = h % n_features  # find the modulo to get index to place f in res\n         if single_bit_hash_function(f) == 1:  # to reduce collision\n             res[index] += 1\n         else:\n             res[index] -= 1 # &lt;--- this will make values to become negative\n\n     return res \n'
'#!/usr/bin/env python\n\nfrom svm import *\n\n# a three-class problem\nlabels = [0, 1, 1, 2]\nsamples = [[0, 0], [0, 1], [1, 0], [1, 1]]\nproblem = svm_problem(labels, samples);\nsize = len(samples)\n\nkernels = [LINEAR, POLY, RBF]\nkname = [\'linear\',\'polynomial\',\'rbf\']\n\nparam = svm_parameter(C = 10,nr_weight = 2,weight_label = [1,0],weight = [10,1])\nfor k in kernels:\n    param.kernel_type = k;\n    model = svm_model(problem,param)\n    errors = 0\n    for i in range(size):\n        prediction = model.predict(samples[i])\n        probability = model.predict_probability\n        if (labels[i] != prediction):\n            errors = errors + 1\n    print "##########################################"\n    print " kernel %s: error rate = %d / %d" % (kname[param.kernel_type], errors, size)\n    print "##########################################"\n\nparam = svm_parameter(kernel_type = RBF, C=10)\nmodel = svm_model(problem, param)\nprint "##########################################"\nprint " Decision values of predicting %s" % (samples[0])\nprint "##########################################"\n\nprint "Numer of Classes:", model.get_nr_class()\nd = model.predict_values(samples[0])\nfor i in model.get_labels():\n    for j in model.get_labels():\n        if j&gt;i:\n            print "{%d, %d} = %9.5f" % (i, j, d[i,j])\n\nparam = svm_parameter(kernel_type = RBF, C=10, probability = 1)\nmodel = svm_model(problem, param)\npred_label, pred_probability = model.predict_probability(samples[1])\nprint "##########################################"\nprint " Probability estimate of predicting %s" % (samples[1])\nprint "##########################################"\nprint "predicted class: %d" % (pred_label)\nfor i in model.get_labels():\n    print "prob(label=%d) = %f" % (i, pred_probability[i])\n\nprint "##########################################"\nprint " Precomputed kernels"\nprint "##########################################"\nsamples = [[1, 0, 0, 0, 0], [2, 0, 1, 0, 1], [3, 0, 0, 1, 1], [4, 0, 1, 1, 2]]\nproblem = svm_problem(labels, samples);\nparam = svm_parameter(kernel_type=PRECOMPUTED,C = 10,nr_weight = 2,weight_label = [1,0],weight = [10,1])\nmodel = svm_model(problem, param)\npred_label = model.predict(samples[0])   \n'
'# suppose these are publication dates\n&gt;&gt;&gt; pd0 = "04-09-2011"      \n&gt;&gt;&gt; pd1 = "17-05-2010"\n# convert them to python datetime instances, e.g., \n&gt;&gt;&gt; pd0 = datetime.strptime(pd0, "%d-%m-%Y")\n# gather them in a python list and then call sort on that list:\n&gt;&gt;&gt; pd_all = [pd0, pd1, pd2, pd3, ...]\n&gt;&gt;&gt; pd_all.sort()\n# \'sort\' will perform an in-place sort on the list of datetime objects,\n# such that the eariest date is at index 0, etc.\n# now the first item in that list is of course the earliest publication date\n&gt;&gt;&gt; pd_all[0]\ndatetime.datetime(2010, 5, 17, 0, 0)\n# express all dates except the earliest one as the absolute differenece in days\n# from that earliest date\n&gt;&gt;&gt; td0 = pd_all[1] - pd_all[0]           # t0 is a timedelta object\n&gt;&gt;&gt; td0\ndatetime.timedelta(475)     \n# convert the time deltas to integers:\n&gt;&gt;&gt; fnx = lambda v : int(str(v).split()[0])\n&gt;&gt;&gt; time_deltas = [td0,....]\n# d is jsut a python list of integers representing number of days from a common baseline date\n&gt;&gt;&gt; d = map(fnx, time_deltas)    \n'
"self.a.append(numpy.hstack((numpy.ones((self.z[-1].shape[0], 1)),numpy.tanh(self.z[-1])))) #tanh is a fancy sigmoid\n\nself.a.append(numpy.hstack((numpy.ones((self.z[-1].shape[0], 1)), 1/(1+numpy.exp(-self.z[-1])))))\n\nmatplotlib.pyplot.scatter(n.predict(nfeatures), targets)\n\nmatplotlib.pyplot.scatter(n.predict(nfeatures).tolist(), targets.tolist())\n\nimport numpy\n\nclass NN:\n\n    def __init__(self, sl):\n\n        #sl = number of units (not counting bias unit) in layer l\n        self.sl = sl\n        self.layers = len(sl)\n\n        #Create weights\n        self.weights = []\n        for idx in range(1, self.layers):\n            self.weights.append(numpy.matrix(numpy.random.rand(self.sl[idx-1]+1, self.sl[idx]))/5)\n\n        self.cost = []\n\n    def update(self, input):\n\n        if input.shape[1] != self.sl[0]:\n            raise ValueError, 'The first layer must have a node for every feature'\n\n        self.z = []\n        self.a = []\n\n        #Input activations.  Expected inputs as numpy matrix (Examples x Featrues) \n        self.a.append(numpy.hstack((numpy.ones((input.shape[0], 1)), input)))#Set inputs ai + bias unit\n\n        #Hidden activations\n        for weight in self.weights: \n            self.z.append(self.a[-1]*weight)\n            self.a.append(numpy.hstack((numpy.ones((self.z[-1].shape[0], 1)), 1/(1+numpy.exp(-self.z[-1]))))) #sigmoid\n\n        #Output activation\n        self.a[-1] = self.z[-1] #Not logistic regression thus no sigmoid function\n        del self.z[-1]\n\n    def backPropagate(self, targets, lamda):\n\n        m = float(targets.shape[0]) #m is number of examples\n\n        #Calculate cost\n        Cost = -1/m*sum(numpy.power(self.a[-1] - targets, 2))\n        for weight in self.weights:\n            Cost = Cost + lamda/(2*m)*numpy.power(weight[1:, :], 2).sum()\n        self.cost.append(abs(float(Cost)))\n\n        #Calculate error for each layer\n        delta = []\n        delta.append(self.a[-1] - targets)\n        for idx in range(1, self.layers-1): #No delta for the input layer because it is the input\n            weight = self.weights[-idx][1:, :] #Ignore bias unit\n            dsigmoid = numpy.multiply(self.a[-(idx+1)][:,1:], 1-self.a[-(idx+1)][:,1:]) #dsigmoid is a(l).*(1-a(l))\n            delta.append(numpy.multiply(delta[-1]*weight.T, dsigmoid)) #Ignore Regularization\n\n        Delta = []\n        for idx in range(self.layers-1):\n            Delta.append(self.a[idx].T*delta[-(idx+1)])\n\n        self.weight_gradient = []\n        for idx in range(len(Delta)):\n            self.weight_gradient.append(numpy.nan_to_num(1/m*Delta[idx] + numpy.vstack((numpy.zeros((1, self.weights[idx].shape[1])), lamda/m*self.weights[idx][1:, :]))))\n\n    def train(self, input, targets, alpha, lamda, iterations = 1000):\n\n        #alpha: learning rate\n        #lamda: regularization term\n\n        for i in range(iterations):\n            self.update(input)\n            self.backPropagate(targets, lamda)\n            self.weights = [self.weights[idx] - alpha*self.weight_gradient[idx] for idx in range(len(self.weights))]\n\n    def autoparam(self, data, alpha = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3], lamda = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]):\n\n        #data: numpy matrix with targets in last column\n        #alpha: learning rate\n        #lamda: regularization term\n\n        #Create training, cross validation, and test sets\n        while 1:\n            try:\n                numpy.seterr(invalid = 'raise')\n                numpy.random.shuffle(data) #Shuffle data\n                training_set = data[0:data.shape[0]/10*6, 0:-1]\n                self.ntraining_set = (training_set-training_set.mean(axis=0))/training_set.std(axis=0)\n                self.training_tgt = numpy.matrix(data[0:data.shape[0]/10*6, -1]).T\n\n                cv_set = data[data.shape[0]/10*6:data.shape[0]/10*8, 0:-1]\n                self.ncv_set = (cv_set-cv_set.mean(axis=0))/cv_set.std(axis=0)\n                self.cv_tgt = numpy.matrix(data[data.shape[0]/10*6:data.shape[0]/10*8, -1]).T\n\n                test_set = data[data.shape[0]/10*8:, 0:-1]\n                self.ntest_set = (test_set-test_set.mean(axis=0))/test_set.std(axis=0)\n                self.test_tgt = numpy.matrix(data[data.shape[0]/10*8:, -1]).T\n\n                break\n\n            except FloatingPointError:\n                pass\n\n        numpy.seterr(invalid = 'warn')\n        cost = 999999\n        for i in alpha:\n            for j in lamda:\n                self.__init__(self.sl)\n                self.train(self.ntraining_set, self.training_tgt, i, j, 2000)\n                current_cost = 1/float(cv_set.shape[0])*sum(numpy.square(self.predict(self.ncv_set) - self.cv_tgt)).tolist()[0][0]\n                print current_cost\n                if current_cost &lt; cost:\n                    cost = current_cost\n                    self.learning_rate = i\n                    self.regularization = j\n        self.__init__(self.sl)\n\n    def predict(self, input):\n\n        self.update(input)\n        return self.a[-1]\n\ndata = numpy.loadtxt(open('FF-data.csv', 'rb'), delimiter = ',', skiprows = 1)#Load\nnumpy.random.shuffle(data)\n\nfeatures = data[:,0:11]\nnfeatures = (features-features.mean(axis=0))/features.std(axis=0)\ntargets = numpy.matrix(data[:, 12]).T\n\nn = NN([11, 50, 1])\n\nn.train(nfeatures, targets, 0.07, 0.0, 2000)\n\nimport matplotlib.pyplot\nmatplotlib.pyplot.subplot(221)\nmatplotlib.pyplot.plot(n.cost)\nmatplotlib.pyplot.title('Cost vs. Iteration')\n\nmatplotlib.pyplot.subplot(222)\nmatplotlib.pyplot.scatter(n.predict(nfeatures).tolist(), targets.tolist())\nmatplotlib.pyplot.plot(targets.tolist(), targets.tolist(), c = 'r')\nmatplotlib.pyplot.title('Data vs. Predicted')\n\nmatplotlib.pyplot.savefig('Report.png', format = 'png')\nmatplotlib.pyplot.close()\n"
'from pybrain.datasets            import ClassificationDataSet\nfrom pybrain.utilities           import percentError\nfrom pybrain.tools.shortcuts     import buildNetwork\nfrom pybrain.supervised.trainers import BackpropTrainer\nfrom pybrain.structure.modules   import SoftmaxLayer\n\nfrom pylab import ion, ioff, figure, draw, contourf, clf, show, hold, plot\nfrom scipy import diag, arange, meshgrid, where\nfrom numpy.random import multivariate_normal\n\nmeans = [(-1,0),(2,4),(3,1)]\ncov = [diag([1,1]), diag([0.5,1.2]), diag([1.5,0.7])]\nn_klass = 2\nalldata = ClassificationDataSet(2, 1, nb_classes=n_klass)\nfor n in xrange(400):\n    for klass in range(n_klass):\n        input = multivariate_normal(means[klass],cov[klass])\n        alldata.addSample(input, [klass])\n\ntstdata, trndata = alldata.splitWithProportion(0.25)\n\ntrndata._convertToOneOfMany()\ntstdata._convertToOneOfMany()\n\nfnn = buildNetwork( trndata.indim, 5, trndata.outdim, outclass=SoftmaxLayer )\n\ntrainer = BackpropTrainer( fnn, dataset=trndata, momentum=0.1, verbose=True,             weightdecay=0.01)\n\nticks = arange(-3.,6.,0.2)\nX, Y = meshgrid(ticks, ticks)\n# need column vectors in dataset, not arrays\ngriddata = ClassificationDataSet(2,1, nb_classes=n_klass)\nfor i in xrange(X.size):\n    griddata.addSample([X.ravel()[i],Y.ravel()[i]], [0])\ngriddata._convertToOneOfMany()  # this is still needed to make the fnn feel comfy\n\nfor i in range(20):\n    trainer.trainEpochs( 1 )\n    trnresult = percentError( trainer.testOnClassData(),\n                              trndata[\'class\'] )\n    tstresult = percentError( trainer.testOnClassData(\n           dataset=tstdata ), tstdata[\'class\'] )\n\n    print "epoch: %4d" % trainer.totalepochs, \\\n          "  train error: %5.2f%%" % trnresult, \\\n          "  test error: %5.2f%%" % tstresult\n\n    out = fnn.activateOnDataset(griddata)\n    out = out.argmax(axis=1)  # the highest output activation gives the class\n    out = out.reshape(X.shape)\n\n    figure(1)\n    ioff()  # interactive graphics off\n    clf()   # clear the plot\n    hold(True) # overplot on\n    for c in range(n_klass):\n        here, _ = where(tstdata[\'class\']==c)\n        plot(tstdata[\'input\'][here,0],tstdata[\'input\'][here,1],\'o\')\n    if out.max()!=out.min():  # safety check against flat field\n        contourf(X, Y, out)   # plot the contour\n    ion()   # interactive graphics on\n    draw()  # update the plot\n\nfrom pyroc import *\nrandom_sample  = random_mixture_model()  # Generate a custom set randomly\n\n#Example instance labels (first index) with the decision function , score (second index)\n#-- positive class should be +1 and negative 0.\nroc = ROCData(random_sample)  #Create the ROC Object\nroc.auc() #get the area under the curve\nroc.plot(title=\'ROC Curve\') #Create a plot of the ROC curve\n\nx = random_mixture_model()\nr1 = ROCData(x)\ny = random_mixture_model()\nr2 = ROCData(y)\nlista = [r1,r2]\nplot_multiple_roc(lista,\'Multiple ROC Curves\',include_baseline=True)\n'
'# you\'ll have to set a few other options to get good estimates,\n# in particular n_iterations, but this should get you going\nlr = SGDClassifier(loss="log")\n\nimport numpy as np\nclasses = np.unique(["ham", "spam", "eggs"])\n\nfor xs, ys in minibatches:\n    lr.partial_fit(xs, ys, classes=classes)\n'
'from sklearn.metrics import accuracy_score\n\n# ... everything else the same ...\n\n# create an answer key\n# I hope this is correct!\ny_test = [[1], [2], [3]]\n\n# same as yours...\nclassifier.fit(X_train, y_train)\npredicted = classifier.predict(X_test)\n\n# get the accuracy\nprint accuracy_score(y_test, predicted)\n'
'setattr(object, \'threshold\', \'mean\')\n\nclass BaseEstimator(object):\n"""Base class for all estimators in scikit-learn\n\nNotes\n-----\nAll estimators should specify all the parameters that can be set\nat the class level in their __init__ as explicit keyword\narguments (no *args, **kwargs).\n"""\n\n@classmethod\ndef _get_param_names(cls):\n    """Get parameter names for the estimator"""\n    try:\n        # fetch the constructor or the original constructor before\n        # deprecation wrapping if any\n        init = getattr(cls.__init__, \'deprecated_original\', cls.__init__)\n\n        # introspect the constructor arguments to find the model parameters\n        # to represent\n        args, varargs, kw, default = inspect.getargspec(init)\n        if not varargs is None:\n            raise RuntimeError("scikit-learn estimators should always "\n                               "specify their parameters in the signature"\n                               " of their __init__ (no varargs)."\n                               " %s doesn\'t follow this convention."\n                               % (cls, ))\n        # Remove \'self\'\n        # XXX: This is going to fail if the init is a staticmethod, but\n        # who would do this?\n        args.pop(0)\n    except TypeError:\n        # No explicit __init__\n        args = []\n    args.sort()\n    return args\n\n    def __init__(self,\n             n_estimators=10,\n             criterion="gini",\n             max_depth=None,\n             min_samples_split=2,\n             min_samples_leaf=1,\n             max_features="auto",\n             bootstrap=True,\n             oob_score=False,\n             n_jobs=1,\n             random_state=None,\n             verbose=0,\n             min_density=None,\n             compute_importances=None,\n             threshold="mean"): # ADD THIS!\n\n    self.threshold = threshold # ADD THIS LINE SOMEWHERE IN THE FUNCTION __INIT__\n'
"from sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom statsmodels.tools import categorical\nimport numpy as np\n\niris = datasets.load_iris()\n# Use only data for 2 classes.\nX = iris.data[(iris.target==0) | (iris.target==1)]\nY = iris.target[(iris.target==0) | (iris.target==1)]\n\n# Class 0 has indices 0-49. Class 1 has indices 50-99.\n# Divide data into 80% training, 20% testing.\ntrain_indices = list(range(40)) + list(range(50,90))\ntest_indices = list(range(40,50)) + list(range(90,100))\nX_train = X[train_indices]\nX_test = X[test_indices]\ny_train = Y[train_indices]\ny_test = Y[test_indices]\n\n\n###########################################################################\n###### Convert categorical variable to matrix and merge back with training\n###### data.\n\n# Fake categorical variable.\ncatVar = np.array(['a']*40 + ['b']*40)\ncatVar = categorical(catVar, drop=True)\nX_train = np.concatenate((X_train, catVar), axis = 1)\n\ncatVar = np.array(['a']*10 + ['b']*10)\ncatVar = categorical(catVar, drop=True)\nX_test = np.concatenate((X_test, catVar), axis = 1)\n###########################################################################\n\n# Model and test.\nclf = GradientBoostingClassifier(learning_rate=0.01,max_depth=8,n_estimators=50).fit(X_train, y_train)\n\nprob = clf.predict_proba(X_test)[:,1]   # Only look at P(y==1).\n\nfpr, tpr, thresholds = roc_curve(y_test, prob)\nroc_auc_prob = auc(fpr, tpr)\n\nprint(prob)\nprint(y_test)\nprint(roc_auc_prob)\n"
"from sklearn import svm\n\nX = [[0, 0], [1, 1]]\ny = [0, 1]\n\nclf = svm.SVC(kernel='linear')\nclf.fit(X, y)\nprint clf.support_vectors_\n\n[[ 0.  0.]\n [ 1.  1.]]\n"
"def _h5_fast_bool_ix(self, h5_array, ix, read_chunksize=100000):\n    '''Iterate over an h5 array chunkwise to select a random subset\n    of the array. `h5_array` should be the array itself; `ix` should\n    be a boolean index array with as many values as `h5_array` has\n    rows; and you can optionally set the number of rows to read per\n    chunk with `read_chunksize` (default is 100000). For some reason\n    this is much faster than using `ix` to index the array directly.'''\n\n    n_chunks = h5_array.shape[0] / read_chunksize\n    slices = [slice(i * read_chunksize, (i + 1) * read_chunksize)\n              for i in range(n_chunks)]\n\n    a = numpy.empty((ix.sum(), h5_array.shape[1]), dtype=float)\n    a_start = 0\n    for sl in slices:\n        chunk = h5_array[sl][ix[sl]]\n        a_end = a_start + chunk.shape[0]\n        a[a_start:a_end] = chunk\n        a_start = a_end\n\n    return a\n\nimport numpy\nimport os\nimport random\n\nX = []\nY = []\n\nfor filename in os.listdir('input'):\n    X.append(numpy.load(os.path.join('input', filename), mmap_mode='r'))\n\nfor filename in os.listdir('output'):\n    Y.append(numpy.load(os.path.join('output', filename), mmap_mode='r'))\n\nindices = [(chunk, row) for chunk, rows in enumerate(X) \n                        for row in range(rows.shape[0])]\nrandom.shuffle(indices)\n\nnewchunks = 50\nnewchunksize = len(indices) / newchunks\n\nfor i in range(0, len(indices), newchunksize):\n    print i\n    rows = [X[chunk][row] for chunk, row in indices[i:i + newchunksize]]\n    numpy.save('X_shuffled_' + str(i), numpy.array(rows))\n    rows = [Y[chunk][row] for chunk, row in indices[i:i + newchunksize]]\n    numpy.save('Y_shuffled_' + str(i), numpy.array(rows))\n"
"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vect= TfidfVectorizer(  use_idf=True, smooth_idf=True, sublinear_tf=False)\nfrom sklearn.cross_validation import train_test_split\n\ndf= pd.DataFrame({'text':['cat on the','angel eyes has','blue red angel','one two blue','blue whales eat','hot tin roof','angel eyes has','have a cat']\\\n              ,'class': [0,0,0,1,1,1,0,3]})\n\n\n\nX = tfidf_vect.fit_transform(df['text'].values)\ny = df['class'].values\n\nfrom sklearn.decomposition.truncated_svd import TruncatedSVD        \npca = TruncatedSVD(n_components=2)                                \nX_reduced_train = pca.fit_transform(X)  \n\na_train, a_test, b_train, b_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier \n\nclassifier=RandomForestClassifier(n_estimators=10)                  \nclassifier.fit(a_train.toarray(), b_train)                            \nprediction = classifier.predict(a_test.toarray()) \n"
"import matplotlib.pyplot as plt\nimport numpy\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC\n\nFOLDS = 5\n\nX, y = make_blobs(n_samples=1000, n_features=2, centers=2, cluster_std=10.0,\n    random_state=12345)\n\nf, axes = plt.subplots(1, 2, figsize=(10, 5))\n\naxes[0].scatter(X[y==0,0], X[y==0,1], color='blue', s=2, label='y=0')\naxes[0].scatter(X[y!=0,0], X[y!=0,1], color='red', s=2, label='y=1')\naxes[0].set_xlabel('X[:,0]')\naxes[0].set_ylabel('X[:,1]')\naxes[0].legend(loc='lower left', fontsize='small')\n\nk_fold = KFold(n_splits=FOLDS, shuffle=True, random_state=12345)\npredictor = SVC(kernel='linear', C=1.0, probability=True, random_state=12345)\n\ny_real = []\ny_proba = []\nfor i, (train_index, test_index) in enumerate(k_fold.split(X)):\n    Xtrain, Xtest = X[train_index], X[test_index]\n    ytrain, ytest = y[train_index], y[test_index]\n    predictor.fit(Xtrain, ytrain)\n    pred_proba = predictor.predict_proba(Xtest)\n    precision, recall, _ = precision_recall_curve(ytest, pred_proba[:,1])\n    lab = 'Fold %d AUC=%.4f' % (i+1, auc(recall, precision))\n    axes[1].step(recall, precision, label=lab)\n    y_real.append(ytest)\n    y_proba.append(pred_proba[:,1])\n\ny_real = numpy.concatenate(y_real)\ny_proba = numpy.concatenate(y_proba)\nprecision, recall, _ = precision_recall_curve(y_real, y_proba)\nlab = 'Overall AUC=%.4f' % (auc(recall, precision))\naxes[1].step(recall, precision, label=lab, lw=2, color='black')\naxes[1].set_xlabel('Recall')\naxes[1].set_ylabel('Precision')\naxes[1].legend(loc='lower left', fontsize='small')\n\nf.tight_layout()\nf.savefig('result.png')\n"
'z = np.poly1d(np.polyfit(x,y,2))\n'
'from sklearn.preprocessing import Imputer\nimputer = Imputer()\ny_imputed = imputer.fit_transform(y)\n'
'import numpy as np\nimages_and_labels_array = np.array([[...], ...],  # [[1,12,34,24,53,...,102],\n                                                  #  [12,112,43,24,52,...,98],\n                                                  #  ...]\n                                   dtype=np.uint8)\n\nimages_and_labels_array.tofile("/tmp/images.bin")\n\ndef read_my_data(filename_queue):\n\n  class ImageRecord(object):\n    pass\n  result = ImageRecord()\n\n  # Dimensions of the images in the dataset.\n  label_bytes = 1\n  # Set the following constants as appropriate.\n  result.height = IMAGE_HEIGHT\n  result.width = IMAGE_WIDTH\n  result.depth = IMAGE_DEPTH\n  image_bytes = result.height * result.width * result.depth\n  # Every record consists of a label followed by the image, with a\n  # fixed number of bytes for each.\n  record_bytes = label_bytes + image_bytes\n\n  assert record_bytes == 22501  # Based on your question.\n\n  # Read a record, getting filenames from the filename_queue.  No\n  # header or footer in the binary, so we leave header_bytes\n  # and footer_bytes at their default of 0.\n  reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\n  result.key, value = reader.read(filename_queue)\n\n  # Convert from a string to a vector of uint8 that is record_bytes long.\n  record_bytes = tf.decode_raw(value, tf.uint8)\n\n  # The first bytes represent the label, which we convert from uint8-&gt;int32.\n  result.label = tf.cast(\n      tf.slice(record_bytes, [0], [label_bytes]), tf.int32)\n\n  # The remaining bytes after the label represent the image, which we reshape\n  # from [depth * height * width] to [depth, height, width].\n  depth_major = tf.reshape(tf.slice(record_bytes, [label_bytes], [image_bytes]),\n                           [result.depth, result.height, result.width])\n  # Convert from [depth, height, width] to [height, width, depth].\n  result.uint8image = tf.transpose(depth_major, [1, 2, 0])\n\n  return result\n\ndef distorted_inputs(data_dir, batch_size):\n  """[...]"""\n  filenames = ["/tmp/images.bin"]  # Or a list of filenames if you\n                                   # generated multiple files in step 1.\n  for f in filenames:\n    if not gfile.Exists(f):\n      raise ValueError(\'Failed to find file: \' + f)\n\n  # Create a queue that produces the filenames to read.\n  filename_queue = tf.train.string_input_producer(filenames)\n\n  # Read examples from files in the filename queue.\n  read_input = read_my_data(filename_queue)\n  reshaped_image = tf.cast(read_input.uint8image, tf.float32)\n\n  # [...] (Maybe modify other parameters in here depending on your problem.)\n'
'word_file = "/usr/share/dict/words"\nwords = open(word_file).read().splitlines()[10:50]\nrandom_word_list = [[\' \'.join(np.random.choice(words, size=1000, replace=True))] for i in range(50)]\n\ndf = pd.DataFrame(random_word_list, columns=[\'text\'])\ndf.head()\n\n                                                text\n0  Aaru Aaronic abandonable abandonedly abaction ...\n1  abampere abampere abacus aback abalone abactor...\n2  abaisance abalienate abandonedly abaff abacina...\n3  Ababdeh abalone abac abaiser abandonable abact...\n4  abandonable abandon aba abaiser abaft Abama ab...\n\nlen(df)\n\n50\n\ntxt = df.text.apply(word_tokenize)\ntxt.head()\n\n0    [Aaru, Aaronic, abandonable, abandonedly, abac...\n1    [abampere, abampere, abacus, aback, abalone, a...\n2    [abaisance, abalienate, abandonedly, abaff, ab...\n3    [Ababdeh, abalone, abac, abaiser, abandonable,...\n4    [abandonable, abandon, aba, abaiser, abaft, Ab...\n\ntxt.apply(len)\n\n0     1000\n1     1000\n2     1000\n3     1000\n4     1000\n....\n44    1000\n45    1000\n46    1000\n47    1000\n48    1000\n49    1000\nName: text, dtype: int64\n\ntxt = txt.apply(lambda x: nltk.Text(x).count(\'abac\'))\ntxt.head()\n\n0    27\n1    24\n2    17\n3    25\n4    32\n\ntxt.sum()\n\n1239\n'
'import subprocess\nimport tensorflow as tf\nimport time\nimport sys\n\nflags = tf.flags\nflags.DEFINE_string("port1", "12222", "port of worker1")\nflags.DEFINE_string("port2", "12223", "port of worker2")\nflags.DEFINE_string("task", "", "internal use")\nFLAGS = flags.FLAGS\n\n# setup local cluster from flags\nhost = "127.0.0.1:"\ncluster = {"worker": [host+FLAGS.port1, host+FLAGS.port2]}\nclusterspec = tf.train.ClusterSpec(cluster).as_cluster_def()\n\nif __name__==\'__main__\':\n  if not FLAGS.task:  # start servers and run client\n\n      # launch distributed service\n      def runcmd(cmd): subprocess.Popen(cmd, shell=True, stderr=subprocess.STDOUT)\n      runcmd("python %s --task=0"%(sys.argv[0]))\n      runcmd("python %s --task=1"%(sys.argv[0]))\n      time.sleep(1)\n\n      # bring down distributed service\n      sess = tf.Session("grpc://"+host+FLAGS.port1)\n      queue0 = tf.FIFOQueue(1, tf.int32, shared_name="queue0")\n      queue1 = tf.FIFOQueue(1, tf.int32, shared_name="queue1")\n      with tf.device("/job:worker/task:0"):\n          add_op0 = tf.add(tf.ones(()), tf.ones(()))\n      with tf.device("/job:worker/task:1"):\n          add_op1 = tf.add(tf.ones(()), tf.ones(()))\n\n      print("Running computation on server 0")\n      print(sess.run(add_op0))\n      print("Running computation on server 1")\n      print(sess.run(add_op1))\n\n      print("Bringing down server 0")\n      sess.run(queue0.enqueue(1))\n      print("Bringing down server 1")\n      sess.run(queue1.enqueue(1))\n\n  else: # Launch TensorFlow server\n    server = tf.train.Server(clusterspec, config=None,\n                             job_name="worker",\n                             task_index=int(FLAGS.task))\n    print("Starting server "+FLAGS.task)\n    sess = tf.Session(server.target)\n    queue = tf.FIFOQueue(1, tf.int32, shared_name="queue"+FLAGS.task)\n    sess.run(queue.dequeue())\n    print("Terminating server"+FLAGS.task)\n'
'inputs=Input((784,))\nencode=Dense(10, input_shape=[784])(inputs)\ndecode=Dense(784, input_shape=[10])\n\nmodel=Model(input=inputs, output=decode(encode))\n\n\nmodel.compile(loss="mse",\n             optimizer="adadelta",\n             metrics=["accuracy"])\n\ninputs_2=Input((10,))\ndecode_model=Model(input=inputs_2, output=decode(inputs_2))\n'
'ax = xgboost.plot_importance(...)\nfig = ax.figure\nfig.set_size_inches(h, w)\n\nfig, ax = plt.subplots(figsize=(h, w))\nxgboost.plot_importance(..., ax=ax)\n'
'index = tf.argmax(one_hot_vector, axis=0)\n'
'A.reshape(-1, 28*28)\n'
'output_path = "/tmp/myTest"\nsummary_writer = tf.summary.FileWriter(output_path)\n\nfor x in range(100):\n   myVar = 2*x\n\n   summary=tf.Summary()\n   summary.value.add(tag=\'myVar\', simple_value = myVar)\n   summary_writer.add_summary(summary, x)\n\nsummary_writer.flush()\n'
'nsamples = 10000\nfor i, image, label in enumerate(train_loader):\n    if i &gt; nsamples:\n        break\n\n    # Your training code here.\n\nfor image, label in itertools.islice(train_loader, stop=10000):\n\n    # your training code here.\n'
'X = train_data.iloc[:, 0:30].as_matrix()\nY = train_data.iloc[:,30].as_matrix()\n'
'[1000, 800, 700, 600, 200, 30, 10, 5] - number of ratings\n[   0,   1,   2,   3,   4,  5,  6, 7] - position in sorted index\n[   0,   1,   2,   0,   1,  2,  0, 1] - group to partition by\n'
"train.loc[train['Pclass'] == 1, 'Cabin'] = 1\n\ntrain['Cabin'] = 1\n\ntrain = pd.DataFrame({'Pclass':[1,2,3,1,2],\n                      'Cabin':[10,20,30,40,50]})\nprint (train)\n   Cabin  Pclass\n0     10       1\n1     20       2\n2     30       3\n3     40       1\n4     50       2\n\ntrain.loc[train['Pclass'] == 1, 'Cabin'] = 1\nprint (train)\n   Cabin  Pclass\n0      1       1\n1     20       2\n2     30       3\n3      1       1\n4     50       2\n"
"model = Sequential()\n# the shape of one training example is\ninput_shape = tfidf_matrix[0].shape\nmodel.add(Dense(units=20, activation='relu', input_shape=input_shape))\nmodel.add(Dense(units=20, activation='softmax'))\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', \nmetrics=['accuracy'])\nmodel.fit(tfidf_matrix, train_data['cuisine_id'], epochs=10)\n"
'import pandas as pd\nimport numpy as np\n\nnrows = 10000\np1 = {\'Orange\': 0.6, \'Banana\': 0.4}\np2 = {\'Monkey\': 0.2, \'Cat\': 0.7, \'Dog\': 0.1}\n\nc1 = [key for key, val in p1.items() for i in range(int(nrows * val))]\nc2 = [key for key, val in p2.items() for i in range(int(nrows * val))]\nrandom.shuffle(c1)\nrandom.shuffle(c2)\n\ndf = pd.DataFrame({"c1":c1, "c2":c2, "val":np.random.randint(0, 100, nrows)})\n\nindex = []\nfor key, idx in df.groupby(["c1", "c2"]).groups.items():\n    arr = idx.values.copy()\n    np.random.shuffle(arr)\n    p1 = int(0.6 * len(arr))\n    p2 = int(0.8 * len(arr))\n    index.append(np.split(arr, [p1, p2]))\n\nidx_train, idx_test, idx_validate = list(map(np.concatenate, zip(*index)))\n'
'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.08, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nNh = Ns/(α∗ (Ni + No))\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(32, activation = \'relu\', input_dim = 6))\n\n# Adding the second hidden layer\nmodel.add(Dense(units = 32, activation = \'relu\'))\n\n# Adding the third hidden layer\nmodel.add(Dense(units = 32, activation = \'relu\'))\n\n# Adding the output layer\nmodel.add(Dense(units = 1))\n\nmodel.compile(optimizer = \'adam\',loss = \'mean_squared_error\')\n\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 100)\n\ny_pred = model.predict(X_test)\n\nplt.plot(y_test, color = \'red\', label = \'Real data\')\nplt.plot(y_pred, color = \'blue\', label = \'Predicted data\')\nplt.title(\'Prediction\')\nplt.legend()\nplt.show()\n\nimport numpy as np\nfrom keras.layers import Dense, Activation\nfrom keras.models import Sequential\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# Importing the dataset\ndataset = np.genfromtxt("data.txt", delimiter=\'\')\nX = dataset[:, :-1]\ny = dataset[:, -1]\n\n# Splitting the dataset into the Training set and Test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.08, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Initialising the ANN\nmodel = Sequential()\n\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(32, activation = \'relu\', input_dim = 6))\n\n# Adding the second hidden layer\nmodel.add(Dense(units = 32, activation = \'relu\'))\n\n# Adding the third hidden layer\nmodel.add(Dense(units = 32, activation = \'relu\'))\n\n# Adding the output layer\n\nmodel.add(Dense(units = 1))\n\n#model.add(Dense(1))\n# Compiling the ANN\nmodel.compile(optimizer = \'adam\', loss = \'mean_squared_error\')\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, batch_size = 10, epochs = 100)\n\ny_pred = model.predict(X_test)\n\nplt.plot(y_test, color = \'red\', label = \'Real data\')\nplt.plot(y_pred, color = \'blue\', label = \'Predicted data\')\nplt.title(\'Prediction\')\nplt.legend()\nplt.show()\n'
'def mean_absolute_percentage_error(y_true, y_pred):\ndiff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                        K.epsilon(),\n                                        None))\nreturn 100. * K.mean(diff, axis=-1)\n'
"import numpy as np\n\nmodel.fit(...)\nhist = model.history.history['val_acc']\nn_epochs_best = np.argmax(hist)\n"
'X2 = X2[X1.columns]\n\n&gt;&gt;&gt; X1\n   a  b\n0  1  5\n1  2  6\n2  3  7\n\n&gt;&gt;&gt; X2\n   b  a\n0  5  3\n1  4  2\n2  6  1\n\narray([[1, 5],\n       [2, 6],\n       [3, 7]])\n\n&gt;&gt;&gt; X2.values\narray([[5, 3],\n       [4, 2],\n       [6, 1]])\n\nX2 = X2[X1.columns]\n\n&gt;&gt;&gt; X2\n   a  b\n0  3  5\n1  2  4\n2  1  6\n'
"num_cats = 3 # number of categorical features\nn_steps = 100 # number of timesteps in each sample\nn_numerical_feats = 10 # number of numerical features in each sample\ncat_size = [1000, 500, 100] # number of categories in each categorical feature\ncat_embd_dim = [50, 10, 100] # embedding dimension for each categorical feature\n\nnumerical_input = Input(shape=(n_steps, n_numerical_feats), name='numeric_input')\ncat_inputs = []\nfor i in range(num_cats):\n    cat_inputs.append(Input(shape=(n_steps,1), name='cat' + str(i+1) + '_input'))\n\ncat_embedded = []\nfor i in range(num_cats):\n    embed = TimeDistributed(Embedding(cat_size[i], cat_embd_dim[i]))(cat_inputs[i])\n    cat_embedded.append(embed)\n\ncat_merged = concatenate(cat_embedded)\ncat_merged = Reshape((n_steps, -1))(cat_merged)\nmerged = concatenate([numerical_input, cat_merged])\nlstm_out = LSTM(64)(merged)\n\nmodel = Model([numerical_input] + cat_inputs, lstm_out)\nmodel.summary()\n\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ncat1_input (InputLayer)         (None, 100, 1)       0                                            \n__________________________________________________________________________________________________\ncat2_input (InputLayer)         (None, 100, 1)       0                                            \n__________________________________________________________________________________________________\ncat3_input (InputLayer)         (None, 100, 1)       0                                            \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 100, 1, 50)   50000       cat1_input[0][0]                 \n__________________________________________________________________________________________________\ntime_distributed_2 (TimeDistrib (None, 100, 1, 10)   5000        cat2_input[0][0]                 \n__________________________________________________________________________________________________\ntime_distributed_3 (TimeDistrib (None, 100, 1, 100)  10000       cat3_input[0][0]                 \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 100, 1, 160)  0           time_distributed_1[0][0]         \n                                                                 time_distributed_2[0][0]         \n                                                                 time_distributed_3[0][0]         \n__________________________________________________________________________________________________\nnumeric_input (InputLayer)      (None, 100, 10)      0                                            \n__________________________________________________________________________________________________\nreshape_1 (Reshape)             (None, 100, 160)     0           concatenate_1[0][0]              \n__________________________________________________________________________________________________\nconcatenate_2 (Concatenate)     (None, 100, 170)     0           numeric_input[0][0]              \n                                                                 reshape_1[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   (None, 64)           60160       concatenate_2[0][0]              \n==================================================================================================\nTotal params: 125,160\nTrainable params: 125,160\nNon-trainable params: 0\n__________________________________________________________________________________________________\n\nnumerical_input = Input(shape=(n_steps, n_numerical_feats), name='numeric_input')\ncat_inputs = []\nfor i in range(num_cats):\n    cat_inputs.append(Input(shape=(n_steps,), name='cat' + str(i+1) + '_input'))\n\ncat_embedded = []\nfor i in range(num_cats):\n    embed = Embedding(cat_size[i], cat_embd_dim[i])(cat_inputs[i])\n    cat_embedded.append(embed)\n\ncat_merged = concatenate(cat_embedded)\nmerged = concatenate([numerical_input, cat_merged])\nlstm_out = LSTM(64)(merged)\n\nmodel = Model([numerical_input] + cat_inputs, lstm_out)\n\nX_tr_numerical = X_train[:,:,:n_numerical_feats]\n\n# extract categorical features: you can use a for loop to this as well.\n# note that we reshape categorical features to make them consistent with the updated solution\nX_tr_cat1 = X_train[:,:,cat1_idx].reshape(-1, n_steps) \nX_tr_cat2 = X_train[:,:,cat2_idx].reshape(-1, n_steps)\nX_tr_cat3 = X_train[:,:,cat3_idx].reshape(-1, n_steps)\n\n# don't forget to compile the model ...\n\n# fit the model\nmodel.fit([X_tr_numerical, X_tr_cat1, X_tr_cat2, X_tr_cat3], y_train, ...)\n\n# or you can use input layer names instead\nmodel.fit({'numeric_input': X_tr_numerical,\n           'cat1_input': X_tr_cat1,\n           'cat2_input': X_tr_cat2,\n           'cat3_input': X_tr_cat3}, y_train, ...)\n\n# if you are using a generator\ndef my_generator(...):\n\n    # prep the data ...\n\n    yield [batch_tr_numerical, batch_tr_cat1, batch_tr_cat2, batch_tr_cat3], batch_tr_y\n\n    # or use the names\n    yield {'numeric_input': batch_tr_numerical,\n           'cat1_input': batch_tr_cat1,\n           'cat2_input': batch_tr_cat2,\n           'cat3_input': batch_tr_cat3}, batch_tr_y\n\nmodel.fit_generator(my_generator(...), ...)\n\n# or if you are subclassing Sequence class\nclass MySequnece(Sequence):\n    def __init__(self, x_set, y_set, batch_size):\n        # initialize the data\n\n    def __getitem__(self, idx):\n        # fetch data for the given batch index (i.e. idx)\n\n        # same as the generator above but use `return` instead of `yield`\n\nmodel.fit_generator(MySequence(...), ...)\n"
'pd.DataFrame(DTC_Bow.cv_results_)\n'
"import arff, numpy as np\ndataset = arff.load(open('mydataset.arff', 'rb'))\ndata = np.array(dataset['data'])\n"
"import tensorflow as tf\nimport tflearn\n\nX = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]\nY_xor = [[0.], [1.], [1.], [0.]]\n\n# Graph definition\nwith tf.Graph().as_default():\n    tnorm = tflearn.initializations.uniform(minval=-1.0, maxval=1.0)\n    net = tflearn.input_data(shape=[None, 2])\n    net = tflearn.fully_connected(net, 2, activation='sigmoid', weights_init=tnorm)\n    net = tflearn.fully_connected(net, 1, activation='sigmoid', weights_init=tnorm)\n    regressor = tflearn.regression(net, optimizer='sgd', learning_rate=2., loss='mean_square')\n\n    # Training\n    m = tflearn.DNN(regressor)\n    m.fit(X, Y_xor, n_epoch=10000, snapshot_epoch=False) \n\n    # Testing\n    print(&quot;Testing XOR operator&quot;)\n    print(&quot;0 xor 0:&quot;, m.predict([[0., 0.]]))\n    print(&quot;0 xor 1:&quot;, m.predict([[0., 1.]]))\n    print(&quot;1 xor 0:&quot;, m.predict([[1., 0.]]))\n    print(&quot;1 xor 1:&quot;, m.predict([[1., 1.]]))\n"
"model = nn_model()\nmodel = KerasRegressor(build_fn=model, nb_epoch=2)\n\ndata = read_db()\ny = data.pop('PRICE').as_matrix()\nx = data.as_matrix()\n# model = nn_model() # Don't do this!\n# set build_fn equal to the nn_model function\nmodel = KerasRegressor(build_fn=nn_model, nb_epoch=2) # note that you do not call the function!\nmodel.fit(x,y)  # fixed!\n"
"test_encoded = pd.get_dummies(test_data, columns=['your columns'])\ntest_encoded_for_model = test_encoded.reindex(columns = training_encoded.columns, \n    fill_value=0)\n"
'queryTFIDF = TfidfVectorizer().fit(words)\n\nqueryTFIDF = queryTFIDF.transform([query])\n\ncosine_similarities = cosine_similarity(queryTFIDF, datasetTFIDF).flatten()\nrelated_product_indices = cosine_similarities.argsort()[:-11:-1]\n'
"x = DF['Brain']\nx = x.tolist()\nx = np.asarray(x)\n\n# 16 samples, None feature\nx.shape\n(16,)\n\n# 16 samples, 1 feature\nx.reshape(-1,1).shape\n(16,1)\n\nplt.plot(x,body_reg.predict(x.reshape(-1,1)))\n\nx = DF['Brain'].values.reshape(1,-1)\ny = DF['Body'].values.reshape(1,-1)\n\nbody_reg = linear_model.LinearRegression()\nbody_reg.fit(x, y)\n"
'estimator = KerasClassifier(build_fn=SS.create_model, nb_epoch=10, verbose=0)\n'
'&gt;&gt;&gt; from sklearn.preprocessing import Imputer\n&gt;&gt;&gt; imp = Imputer(strategy="mean")\n&gt;&gt;&gt; a = np.random.random((5,5))\n&gt;&gt;&gt; a[(1,4,0,3),(2,4,2,0)] = np.nan\n&gt;&gt;&gt; a\narray([[ 0.77473361,  0.62987193,         nan,  0.11367791,  0.17633671],\n       [ 0.68555944,  0.54680378,         nan,  0.64186838,  0.15563309],\n       [ 0.37784422,  0.59678177,  0.08103329,  0.60760487,  0.65288022],\n       [        nan,  0.54097945,  0.30680838,  0.82303869,  0.22784574],\n       [ 0.21223024,  0.06426663,  0.34254093,  0.22115931,         nan]])\n&gt;&gt;&gt; a = imp.fit_transform(a)\n&gt;&gt;&gt; a\narray([[ 0.77473361,  0.62987193,  0.24346087,  0.11367791,  0.17633671],\n       [ 0.68555944,  0.54680378,  0.24346087,  0.64186838,  0.15563309],\n       [ 0.37784422,  0.59678177,  0.08103329,  0.60760487,  0.65288022],\n       [ 0.51259188,  0.54097945,  0.30680838,  0.82303869,  0.22784574],\n       [ 0.21223024,  0.06426663,  0.34254093,  0.22115931,  0.30317394]])\n'
"data = {'numeric_1':[12.1, 3.2, 5.5, 6.8, 9.9], \n        'categorical_1':['A', 'B', 'C', 'B', 'B']}\nframe = pd.DataFrame(data)\ndummy_values = pd.get_dummies(data['categorical_1'] + ['D','E'])\n\n['A','B','C','B','B'] + ['D','E']\n\n['A', 'B', 'C', 'B', 'B', 'D', 'E']\n\nA   B   C\n0   0   0\n"
'dataset[ ~np.isfinite(dataset) ] = 0  # Set non-finite (nan, inf, -inf) to zero\n\nnp.where(~np.isfinite(dataset))\n\n&gt;&gt;&gt; import numpy as np\n\n&gt;&gt;&gt; dataset = np.array([[0,1,1],[np.nan,0,0],[1,2,np.inf]])\n&gt;&gt;&gt; dataset\narray([[  0.,   1.,   1.],\n       [ nan,   0.,   0.],\n       [  1.,   2.,  inf]])\n\n&gt;&gt;&gt; np.where(~np.isfinite(dataset))\n(array([1, 2]), array([0, 2]))\n'
'def get_model(A, y, lamb=0):\n    n_col = A.shape[1]\n    return np.linalg.lstsq(A.T.dot(A) + lamb * np.identity(n_col), A.T.dot(y))\n'
'import input_data\nimport datetime\nimport numpy as np\nimport tensorflow as tf\nimport cv2\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom random import randint\n\n\nmnist = input_data.read_data_sets("MNIST_data/", one_hot=True)\n\nx = tf.placeholder("float", [None, 784])\n\nW = tf.Variable(tf.zeros([784,10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x,W) + b)\ny_ = tf.placeholder("float", [None,10])\n\ncross_entropy = -tf.reduce_sum(y_*tf.log(y))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\n\n#Train our model\niter = 1000\nfor i in range(iter):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n#Evaluationg our model:\ncorrect_prediction=tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,"float"))\nprint "Accuracy: ", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n\n#1: Using our model to classify a random MNIST image from the original test set:\nnum = randint(0, mnist.test.images.shape[0])\nimg = mnist.test.images[num]\n\nclassification = sess.run(tf.argmax(y, 1), feed_dict={x: [img]})\n\'\'\'\n#Uncomment this part if you want to plot the classified image.\nplt.imshow(img.reshape(28, 28), cmap=plt.cm.binary)\nplt.show()\n\'\'\'\nprint \'Neural Network predicted\', classification[0]\nprint \'Real label is:\', np.argmax(mnist.test.labels[num])\n\n\n#2: Using our model to classify MNIST digit from a custom image:\n\n# create an an array where we can store 1 picture\nimages = np.zeros((1,784))\n# and the correct values\ncorrect_vals = np.zeros((1,10))\n\n# read the image\ngray = cv2.imread("my_digit.png", 0 ) #0=cv2.CV_LOAD_IMAGE_GRAYSCALE #must be .png!\n\n# rescale it\ngray = cv2.resize(255-gray, (28, 28))\n\n# save the processed images\ncv2.imwrite("my_grayscale_digit.png", gray)\n"""\nall images in the training set have an range from 0-1\nand not from 0-255 so we divide our flatten images\n(a one dimensional vector with our 784 pixels)\nto use the same 0-1 based range\n"""\nflatten = gray.flatten() / 255.0\n"""\nwe need to store the flatten image and generate\nthe correct_vals array\ncorrect_val for a digit (9) would be\n[0,0,0,0,0,0,0,0,0,1]\n"""\nimages[0] = flatten\n\n\nmy_classification = sess.run(tf.argmax(y, 1), feed_dict={x: [images[0]]})\n\n"""\nwe want to run the prediction and the accuracy function\nusing our generated arrays (images and correct_vals)\n"""\nprint \'Neural Network predicted\', my_classification[0], "for your digit"\n'
"pipeline = Pipeline([\n  ('features', FeatureUnion([\n    ('ngram_tf_idf', Pipeline([\n      ('counts_ngram', CountVectorizer()),\n      ('tf_idf_ngram', TfidfTransformer())\n    ])),\n    ('pos_tf_idf', Pipeline([\n      ('pos', POSTransformer()),          \n      ('counts_pos', CountVectorizer()),\n      ('tf_idf_pos', TfidfTransformer())\n    ])),\n    ('measure_features', MeasureFeatures())\n  ])),\n  ('classifier', LinearSVC())\n])\n"
'new_conv_weights = dense_weights.transpose(1,0).reshape(new_conv_shape)[:,:,::-1,::-1]\n\nweights[0] = weights[0].transpose(1,0).reshape((4096,512,7,7))[:,:,::-1,::-1]\n\n1 0\n0 0\n\n1 2 3 4 5\n6 7 8 9 0\n1 2 3 4 5\n\n7 8 9 0 \n2 3 4 5\n\n1 2 3 4\n6 7 8 9\n'
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import cv2\n&gt;&gt;&gt; xbash = np.fromfile('/bin/bash', dtype='uint8')\n&gt;&gt;&gt; xbash.shape\n(1086744,)\n&gt;&gt;&gt; cv2.imwrite('bash1.png', xbash[:10000].reshape(100,100))\n"
"X=np.random.random((100,5))\ny=np.random.randint(0,2,(100,))\n\nclf = RandomForestClassifier(random_state=1)\ncv = StratifiedKFold(y, random_state=1)        # Setting random_state is not necessary here\ns = cross_val_score(clf, X,y,scoring='roc_auc', cv=cv)\nprint(s)\n##[ 0.57612457  0.29044118  0.30514706]\nprint(s)\n##[ 0.57612457  0.29044118  0.30514706]\n"
'X_train = []\n\nfor row in cur:\n    X_train.append(row)\n\nX_train = list(cur)\n\nX_train = [[x for x in r if type(x)==int] for r in cur]\n\n[[ 6  1  1  1  2  1  0  0 19]\n [ 6  1  1  1  2  1  0  0 14]]\n'
'from sklearn.model_selection import GridSearchCV\n'
'from keras.layers.core import *\nfrom keras.models import Model\n\n# this is your image input definition. You have to specify a shape. \nimage_input = Input(shape=(32,32,3))\n# Some more data input with 10 features (eg.)\nother_data_input = Input(shape=(10,))    \n\n# First convolution filled with random parameters for the example\nconv1 = Convolution2D(nb_filter = nb_filter1, nb_row = nb_row1, nb_col=_nb_col1, padding = "same", activation = "tanh")(image_input)\n# MaxPool it \nconv1 = MaxPooling2D(pool_size=(pool_1,pool_2))(conv1)\n# Second Convolution\nconv2 = Convolution2D(nb_filter = nb_filter2, nb_row = nb_row2, nb_col=_nb_col2, padding = "same", activation = "tanh")(conv1)\n# MaxPool it\nconv2  = MaxPooling2D(pool_size=(pool_1,pool_2))(conv2)\n# Flatten the output to enable the merge to happen with the other input\nfirst_part_output = Flatten()(conv2)\n\n# Merge the output of the convNet with your added features by concatenation\nmerged_model = keras.layers.concatenate([first_part_output, other_data_input])\n\n# Predict on the output (say you want a binary classification)\npredictions = Dense(1, activation =\'sigmoid\')(merged_model)\n\n# Now create the model\nmodel = Model(inputs=[image_input, other_data_input], outputs=predictions)\n# see your model \nmodel.summary()\n\n# compile it\nmodel.compile(optimizer=\'adamax\', loss=\'binary_crossentropy\')\n'
'old_stdout = sys.stdout\nsys.stdout = mystdout = StringIO()\nclf = SGDClassifier(**kwargs, verbose=1)\nclf.fit(X_tr, y_tr)\nsys.stdout = old_stdout\nloss_history = mystdout.getvalue()\nloss_list = []\nfor line in loss_history.split(\'\\n\'):\n    if(len(line.split("loss: ")) == 1):\n        continue\n    loss_list.append(float(line.split("loss: ")[-1]))\nplt.figure()\nplt.plot(np.arange(len(loss_list)), loss_list)\nplt.savefig("warmstart_plots/pure_SGD:"+str(kwargs)+".png")\nplt.xlabel("Time in epochs")\nplt.ylabel("Loss")\nplt.close()\n'
'classifier.fit(X_train, y_train)\n'
"import numpy as np\n\nX = np.array([\n  [200, 100], \n  [320, 90], \n  [150, 60], \n  [170, 20], \n  [169, 75], \n  [190, 65], \n  [212, 132]\n])\ny = np.array([[1], [1], [0], [0], [0], [0], [1]])\n\nfrom sklearn.linear_model import LogisticRegression\n\n# 'sag' is stochastic average gradient descent\nlr = LogisticRegression(penalty='l2', solver='sag', max_iter=100)\n\nlr.fit(X, y)\nlr.score(X, y)\n# 0.5714285714285714\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.regularizers import l2\n\nmodel = Sequential([\n  Dense(units=1, activation='sigmoid', kernel_regularizer=l2(0.), input_shape=(2,))\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\nmodel.fit(X, y, epochs=100)\nmodel.evaluate(X, y)\n# 0.57142859697341919\n"
"                             alcohol -&gt; 0.04727507393151268\n                          malic_acid -&gt; 0.0\n                                 ash -&gt; 0.0\n                   alcalinity_of_ash -&gt; 0.0\n                           magnesium -&gt; 0.0329784450464887\n                       total_phenols -&gt; 0.0\n                          flavanoids -&gt; 0.1414466773122087\n                nonflavanoid_phenols -&gt; 0.0\n                     proanthocyanins -&gt; 0.0\n                     color_intensity -&gt; 0.0\n                                 hue -&gt; 0.08378677906228588\n        od280/od315_of_diluted_wines -&gt; 0.3120425747831769\n                             proline -&gt; 0.38247044986432716\n\n                             alcohol -&gt; 0.014123729330936566\n                          malic_acid -&gt; 0.0\n                                 ash -&gt; 0.0\n                   alcalinity_of_ash -&gt; 0.02525179137252771\n                           magnesium -&gt; 0.0\n                       total_phenols -&gt; 0.0\n                          flavanoids -&gt; 0.4128453371544815\n                nonflavanoid_phenols -&gt; 0.0\n                     proanthocyanins -&gt; 0.0\n                     color_intensity -&gt; 0.22278576133186542\n                                 hue -&gt; 0.011635633063349873\n        od280/od315_of_diluted_wines -&gt; 0.0\n                             proline -&gt; 0.31335774774683883\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, _tree\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwine = datasets.load_wine()\n\n# Feature importance\n\nclf = DecisionTreeClassifier(criterion=&quot;gini&quot;, splitter='best', random_state=42)\nclf = clf.fit(wine.data, wine.target)\n\nfor name, val in zip(wine.feature_names, clf.feature_importances_):\n    print(f&quot;{name:&gt;40} -&gt; {val}&quot;)\n\nprint(&quot;&quot;)\nclf = DecisionTreeClassifier(criterion=&quot;entropy&quot;, splitter='best', random_state=42)\nclf = clf.fit(wine.data, wine.target)\n\nfor name, val in zip(wine.feature_names, clf.feature_importances_):\n    print(f&quot;{name:&gt;40} -&gt; {val}&quot;)\n\n# Feature selected first and threshold\n\nfeatures = []\ntresholds = []\nfor random in range(1000):\n    clf = DecisionTreeClassifier(criterion=&quot;gini&quot;, splitter='best', random_state=random)\n    clf = clf.fit(wine.data, wine.target)\n    features.append(clf.tree_.feature[0])\n    tresholds.append(clf.tree_.threshold[0])\n\n# plot distribution\nfig, (ax, ax2) = plt.subplots(1, 2, figsize=(20, 5))\nax.hist(features, bins=np.arange(14)-0.5)\nax2.hist(tresholds)\nax.set_title(&quot;Number of the first used for split&quot;)\nax2.set_title(&quot;Value of the threshold&quot;)\nplt.show()\n\n# plot model\nplt.figure(figsize=(20, 12))\nplot_tree(clf) \nplt.show()\n\n# plot filtered result\nthreshold_filtered = [val for feat, val in zip(features, tresholds) if feat==12]\nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\nax.hist(threshold_filtered)\nax.set_title(&quot;Number of the first used for split&quot;)\nplt.show()\n\nfeature_number = 12\nX1, X2, X3 = wine.data[wine.target==0][:, feature_number], wine.data[wine.target==1][:, feature_number], wine.data[wine.target==2][:, feature_number]\n\nfig, ax = plt.subplots()\nax.set_title(f'feature {feature_number} - distribution')\nax.boxplot([X1, X2, X3])\nax.hlines(755, 0.5, 3.5, colors=&quot;r&quot;, linestyles=&quot;dashed&quot;)\nax.hlines(min(threshold_filtered), 0.5, 3.5, colors=&quot;b&quot;, linestyles=&quot;dashed&quot;)\nax.hlines(max(threshold_filtered), 0.5, 3.5, colors=&quot;b&quot;, linestyles=&quot;dashed&quot;)\nax.hlines(sum(threshold_filtered)/len(threshold_filtered), 0.5, 3.5, colors=&quot;g&quot;, linestyles=&quot;dashed&quot;)\nplt.xlabel(&quot;Class&quot;)\nplt.show()\n"
"from recurrentshop import *\nfrom keras.layers import Concatenate\n\nx_t = Input(shape=(128, 128, 3,))\nh_tm1 = Input(shape=(128, 128, 3,))\n\nh_t1 = Concatenate()([x_t, h_tm1])\nlast = Conv2D(3, kernel_size=(3, 3), strides=(1, 1), padding='same', name='conv2')(h_t1)\n\nrnn = RecurrentModel(input=x_t,\n                     initial_states=[h_tm1],\n                     output=last,\n                     final_states=[last],\n                     state_initializer=['zeros'])\n\nx = Input(shape=(1, 128, 128, 3,))  # a series of 3D tensors -&gt; 4D\ny = rnn(x)\n\nmodel = Model(x, y)\nmodel.predict(np.random.random((1, 1, 128, 128, 3)))  # a batch of x -&gt; 5D\n"
"x = Input(shape=(input_shape), dtype='int32')\nx = LSTM(128,return_sequences=True)(x)\nx = Dropout(0.5)(x)\n"
'for epoch in range(epochs):\n    for batchX,batchY in batches: #adapt this loop to your way of creating/getting batches\n\n        weights = calculateOrGetTheWeights(batch)\n        model.train_on_batch(batchX,batchY,...,class_weight=weights)\n\nimport keras.backend as K\n\ndef customLoss(yTrue,yPred):\n\n    classes = K.argmax(yTrue)\n    classCount = K.sum(yTrue,axis=0)\n\n    loss = K.some_loss_function(yTrue,yPred)\n\n    return loss / K.gather(classCount, classes)\n\nimport keras.backend as K\n\ndef binaryCustomLoss(yTrue,yPred):\n\n    positives = yTrue\n    negatives = 1 - yTrue\n\n    positiveRatio = K.mean(positives)\n    negativeRatio = 1 - positiveRatio #or K.mean(negatives)\n\n    weights = (positives / positiveRatio) + (negatives / negativeRatio)\n\n    #you may need K.squeeze(weights) here\n\n    return weights * K.some_loss_function(yTrue,yPred)\n'
'train_cols = train.columns\ntest_cols = test.columns\n\ncommon_cols = train_cols.intersection(test_cols)\ntrain_not_test = train_cols.difference(test_cols)\n'
'from itertools import chain\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nimport numpy as np\n\n# prepare string data\nwith open(\'spam.txt\', \'r\') as f:\n   spam = f.readlines()\n\nwith open(\'ham.txt\', \'r\') as f:\n   ham = f.readlines()\n\ntext_train = list(chain(spam, ham))\n\n# prepare labels\nlabels_train = np.concatenate((np.zeros(len(spam)),np.ones(len(ham))))\n\n# build pipeline\nvectorizer = TfidfVectorizer()\nregressor = LogisticRegression()\n\npipeline = Pipeline([(\'vectorizer\', vectorizer), (\'regressor\', regressor)])\n\n# fit pipeline\npipeline.fit(text_train, labels_train)\n\n# test predict\ntest = ["Is this spam or ham?"]\npipeline.predict(test) # value in [0,1] \n'
'def cost(thetas, x, y, hidden, lam):\n    theta1, theta2 = get_theta_from(thetas, x, y, hidden)\n    _, _, p = feed_forward(x, theta1, theta2)\n\n    regularization = (lam / (len(x) * 2)) * (\n        np.sum(np.square(np.delete(theta1, 0, 1)))\n        + np.sum(np.square(np.delete(theta2, 0, 1))))\n\n    complete = np.nan_to_num(np.multiply((-y), np.log(\n        p)) - np.multiply((1 - y), np.log(1 - p)))\n    avg = np.sum(complete) / len(x)\n    return avg + regularization\n\ndef gradient(thetas, x, y, hidden, lam):\n    theta1, theta2 = get_theta_from(thetas, x, y, hidden)\n    hidden_dot, hidden_p, p = feed_forward(x, theta1, theta2)\n\n    error_o = p - y\n    error_h = np.multiply(np.dot(\n        error_o, theta2),\n        sigmoid_gradient(add_bias(hidden_dot)))\n\n    x = add_bias(x)\n    error_h = np.delete(error_h, 0, 1)\n\n    theta1_grad, theta2_grad = \\\n        np.zeros(theta1.shape[::-1]), np.zeros(theta2.shape[::-1])\n    records = y.shape[0]\n\n    for i in range(records):\n        theta1_grad = theta1_grad + np.dot(\n            vector(x[i]), np.transpose(vector(error_h[i])))\n        theta2_grad = theta2_grad + np.dot(\n            vector(hidden_p[i]), np.transpose(vector(error_o[i])))\n\n    reg_theta1 = theta1.copy()\n    reg_theta1[:, 0] = 0\n\n    theta1_grad = np.transpose(\n        theta1_grad / records) + ((lam / records) * reg_theta1)\n\n    reg_theta2 = theta2.copy()\n    reg_theta2[:, 0] = 0\n\n    theta2_grad = np.transpose(\n        theta2_grad / records) + ((lam / records) * reg_theta2)\n\n    return np.append(\n        theta1_grad, theta2_grad)\n'
"fractions = np.array([0.6, 0.2, 0.2])\n# shuffle your input\ndf = df.sample(frac=1) \n# split into 3 parts\ntrain, val, test = np.array_split(\n    df, (fractions[:-1].cumsum() * len(df)).astype(int))\n\ny = df.pop('diagnosis').to_frame()\nX = df\n\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y,stratify=y, test_size=0.4)\n\nX_test, X_val, y_test, y_val = train_test_split(\n        X_test, y_test, stratify=y_test, test_size=0.5)\n"
'# Code in file nn/two_layer_net_module.py\nimport torch\n\nclass TwoLayerNet(torch.nn.Module):\n    def __init__(self, D_in, H, D_out):\n        """\n        In the constructor we instantiate two nn.Linear modules and \n        assign them as\n        member variables.\n        """\n        super(TwoLayerNet, self).__init__()\n        self.linear1 = torch.nn.Linear(D_in, H)\n        self.linear2 = torch.nn.Linear(H, D_out)\n\n    def forward(self, x):\n        """\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary (differentiable) operations on Tensors.\n        """\n        h_relu = self.linear1(x).clamp(min=0)\n        y_pred = self.linear2(h_relu)\n        return y_pred\n\n# N is batch size; D_in is input dimension;\n# H is hidden dimension; D_out is output dimension.\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold inputs and outputs\nx = torch.randn(N, D_in)\ny = torch.randn(N, D_out)\n\n# Construct our model by instantiating the class defined above.\nmodel = TwoLayerNet(D_in, H, D_out)\n\n# Construct our loss function and an Optimizer. The call to \nmodel.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.\nloss_fn = torch.nn.MSELoss(size_average=False)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\nfor t in range(500):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = loss_fn(y_pred, y)\n    print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\ny_pred = model(x)\n'
'preds = [1 if x &gt;= 0.5 else 0 for x in preds]\n'
'model = Sequential()\nmodel.add(Dense(10, input_shape=(20, 5)))\n\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 20, 10)            60        \n=================================================================\nTotal params: 60\nTrainable params: 60\nNon-trainable params: 0\n_________________________________________________________________\n'
'import seaborn as sns \nimport pandas as pd\n\ndf = pd.DataFrame([[ 1.82716998, -1.75449225],\n [ 0.09258069,  0.16245259],\n [ 1.09240926,  0.08617436]], columns=["x", "y"])\n\ndf["val"] = pd.Series([1, -1, 1]).apply(lambda x: "red" if x==1 else "blue")\n\n\nsns.scatterplot(df["x"], df["y"], c=df["val"]).plot()\n\nimport seaborn as sns \n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\n\ndf = pd.DataFrame([[ 1.82716998, -1.75449225],\n [ 0.09258069,  0.16245259],\n [ 1.09240926,  0.08617436]], columns=["x", "y"])\ndf["val"] = pd.Series([1, -1, 1]).apply(lambda x: "red" if x==1 else "blue")\nax.scatter(x=df["x"], y=df["y"], c=df["val"])\nplt.plot()\n'
'y = predicted label\np = probability of predicted label\n\n# epoch 300\ny = [0.1, 0.9] =&gt; argmax(y) =&gt; 1 (class label 1)\nloss = -(1 * log(0.9)) = 0.10\n\n# epoch 500\ny = [0.4, 0.6] =&gt; argmax(y) =&gt; 1 (class label 1)\nloss = -(1 * log(0.6)) = 0.51\n'
"inputs = keras.Input(shape=(None, 3))  # variable number of timesteps each with length 3\ninputs = keras.Input(shape=(4, 3))     # 4 timesteps each with length 3\ninputs = keras.Input(shape=(4, None))  # this is WRONG! you can't do this. Number of features must be fixed\n\nlabel = keras.layers.SimpleRNN(units=5, activation='softmax')(inputs)\n"
"import pandas as pd\nfrom sklearn.multioutput import MultiOutputRegressor, RegressorChain\nfrom sklearn.linear_model import LinearRegression\n\n\ndic = {'par_1': [10, 30, 13, 19, 25, 33, 23],\n       'par_2': [1, 3, 1, 2, 3, 3, 2],\n       'outcome': [101, 905, 182, 268, 646, 624, 465]}\n\ndf = pd.DataFrame(dic)\n\nvariables = df.iloc[:,:-1]\nresults = df.iloc[:,-1]\n\nmulti_output_reg = MultiOutputRegressor(LinearRegression())\nmulti_output_reg.fit(results.values.reshape(-1, 1),variables)\n\nmulti_output_reg.predict([[100]])\n\n# array([[12.43124217,  1.12571947]])\n# sounds sensible according to the training data\n\n#if input variables needs to be treated as categories,\n# go for multiOutputClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nmulti_output_clf = MultiOutputClassifier(LogisticRegression(solver='lbfgs'))\nmulti_output_clf.fit(results.values.reshape(-1, 1),variables)\n\nmulti_output_clf.predict([[100]])\n\n# array([[10,  1]])\n\n\ndic = {'par_1': [10, 30, 13, 19, 25, 33, 23],\n       'par_2': [1, 3, 1, 2, 3, 3, 2],\n       'outcome': [0, 1, 1, 1, 1, 1 , 0]}\n\ndf = pd.DataFrame(dic)\n\nvariables = df.iloc[:,:-1]\nresults = df.iloc[:,-1]\n\nmulti_output_clf = MultiOutputClassifier(LogisticRegression(solver='lbfgs',\n                                                            multi_class='ovr'))\nmulti_output_clf.fit(results.values.reshape(-1, 1),variables)\n\nmulti_output_clf.predict([[1]])\n# array([[13,  3]])\n\n"
'## Do this\n#pip install tensorflow==2.0.0\n\nimport tensorflow.keras as keras\nimport numpy as np\nfrom tensorflow.keras.models import Model\n\n\ndata_1=np.array([[25,  5, 11, 24,  6],\n       [25,  5, 11, 24,  6],\n       [25,  0, 11, 24,  6],\n       [25, 11, 28, 11, 24],\n       [25, 11,  6, 11, 11]])\n\ndata_2=np.array([[25, 11, 31,  6, 11],\n       [25, 11, 28, 11, 31],\n       [25, 11, 11, 11, 31]])\n\nY_1=np.array([[2.33],\n       [2.59],\n       [2.59],\n       [2.54],\n       [4.06]])\n\n\nY_2=np.array([[2.9],\n       [2.54],\n       [4.06]])\n\n\n\nuser_input = keras.layers.Input(shape=((None,)), name=\'Input_1\')\nproducts_input =  keras.layers.Input(shape=((None,)), name=\'Input_2\')\n\nshared_embed=(keras.layers.Embedding(37, 3, input_length=5))\nuser_vec_1 = shared_embed(user_input )\nuser_vec_2 = shared_embed(products_input )\n\nx = keras.layers.GlobalAveragePooling1D()(user_vec_1)\nnn = keras.layers.Dense(90, activation=\'relu\',name=\'layer_1\')(x)\nresult_a = keras.layers.Dense(1, activation=\'linear\', name=\'output_1\')(nn)\n\n# Task 2 FC layers\nx = keras.layers.GlobalAveragePooling1D()(user_vec_2)\nnn1 = keras.layers.Dense(90, activation=\'relu\', name=\'layer_2\')(x)\n\nresult_b = keras.layers.Dense(1, activation=\'linear\',name=\'output_2\')(nn1)\n\nmodel = Model(inputs=[user_input , products_input], outputs=[result_a, result_b])\n\n\nloss = tf.keras.losses.MeanSquaredError()\noptimizer = tf.keras.optimizers.Adam()\n\nloss_values = []\nnum_iter = 300\nfor i in range(num_iter):\n    with tf.GradientTape() as tape:\n        # Forward pass.\n        logits = model([data_1, data_2])          \n        loss_value = loss(Y_1, logits[0]) + loss(Y_2, logits[1]) \n        loss_values.append(loss_value)\n    gradients = tape.gradient(loss_value, model.trainable_weights)          \n    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n\nimport matplotlib.pyplot as plt\nplt.plot(range(num_iter), loss_values)\nplt.xlabel("iterations")\nplt.ylabel(\'loss value\')\n\nrandom_indices = np.random.choice(data_2.shape[0],\ndata_1.shape[0], replace=True) \n\nupsampled_data_2 = data_2[random_indices]\n'
'pictures[torch.randint(len(pictures), (10,))]  \n\nindices = torch.randperm(len(pictures))[:10]\n\npictures[indices]\n'
'60000/32 = 1875\n'
'metric = \'val_accuracy\'\nModelCheckpoint(filepath=r"C:\\Users\\reda.elhail\\Desktop\\checkpoints\\{}".format(Name), monitor=metric,\n                    verbose=2, save_best_only=True, mode=\'max\')]\n'
'## fill_mode: One of {&quot;constant&quot;, &quot;nearest&quot;, &quot;reflect&quot; or &quot;wrap&quot;}. \n## Points outside the boundaries of the input are filled according to the given mode:\n## &quot;constant&quot;: kkkkkkkk|abcd|kkkkkkkk (cval=k)\n## &quot;nearest&quot;:  aaaaaaaa|abcd|dddddddd\n## &quot;reflect&quot;:  abcddcba|abcd|dcbaabcd\n## &quot;wrap&quot;:  abcdabcd|abcd|abcdabcd\n'
"a = np.random.multivariate_normal((1.5, 3), [[0.5, 0], [0, .05]], 30)\nb = np.random.multivariate_normal((4, 1.5), [[0.5, 0], [0, .05]], 30)\nplt.plot(a[:,0], a[:,1], 'b.', b[:,0], b[:,1], 'r.')\nmu_a, mu_b = a.mean(axis=0).reshape(-1,1), b.mean(axis=0).reshape(-1,1)\nSw = np.cov(a.T) + np.cov(b.T)\ninv_S = np.linalg.inv(Sw)\nres = inv_S.dot(mu_a-mu_b)  # the trick\n####\n# more general solution\n#\n# Sb = (mu_a-mu_b)*((mu_a-mu_b).T)\n# eig_vals, eig_vecs = np.linalg.eig(inv_S.dot(Sb))\n# res = sorted(zip(eig_vals, eig_vecs), reverse=True)[0][1] # take only eigenvec corresponding to largest (and the only one) eigenvalue\n# res = res / np.linalg.norm(res)\n\nplt.plot([-res[0], res[0]], [-res[1], res[1]]) # this is the solution\nplt.plot(mu_a[0], mu_a[1], 'cx')\nplt.plot(mu_b[0], mu_b[1], 'yx')\nplt.gca().axis('square')\n\n# let's project data point on it\nr = res.reshape(2,)\nn2 = np.linalg.norm(r)**2\nfor pt in a:\n    prj = r * r.dot(pt) / n2\n    plt.plot([prj[0], pt[0]], [prj[1], pt[1]], 'b.:', alpha=0.2)\nfor pt in b:\n    prj = r * r.dot(pt) / n2\n    plt.plot([prj[0], pt[0]], [prj[1], pt[1]], 'r.:', alpha=0.2)\n"
"#Training - \nmodel1 = model.fit(all images, P(cat/dog))\nmodel2 = model.fit(all images, P(cat))\nmodel3 = model.fit(all images, P(dog))\nfinal prediction = argmax(model2, model3)\n\n#Inference - \nif model1.predict == Cat: \n    model2.predict\nelse:\n    model3.predict\n\nfrom tensorflow.keras import layers, Model, utils\nimport numpy as np\n\nX = np.random.random((10,500,500,3))\ny = np.random.random((10,2))\n\n#Model\ninp = layers.Input((500,500,3))\n\nx = layers.Conv2D(6, 3, name='conv1')(inp)\nx = layers.MaxPooling2D(3)(x)\n\nc2 = layers.Conv2D(9, 3, name='conv2')(x)\nc2 = layers.MaxPooling2D(3)(c2)\n\nc3 = layers.Conv2D(12, 3, name='conv3')(c2)\nc3 = layers.MaxPooling2D(3)(c3)\n\nx = layers.Conv2D(15, 3, name='conv4')(c3)\nx = layers.MaxPooling2D(3)(x)\n\nx = layers.Flatten()(x)\nout1 = layers.Dense(2, activation='softmax', name='first')(x)\n\nc = layers.Lambda(lambda x: x[:,:1])(out1)\nd = layers.Lambda(lambda x: x[:,1:])(out1)\n\nc = layers.Multiply()([c3, c])\nd = layers.Multiply()([c2, d])\n\nc = layers.Conv2D(15, 3, name='conv5')(c)\nc = layers.MaxPooling2D(3)(c)\nc = layers.Flatten()(c)\n\nd = layers.Conv2D(12, 3, name='conv6')(d)\nd = layers.MaxPooling2D(3)(d)\nd = layers.Conv2D(15, 3, name='conv7')(d)\nd = layers.MaxPooling2D(3)(d)\nd = layers.Flatten()(d)\n\nx = layers.concatenate([c,d])\nx = layers.Dense(32)(x)\nout2 = layers.Dense(2, activation='softmax',name='second')(x)\n\nmodel = Model(inp, [out1, out2])\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', loss_weights=[0.5, 0.5])\n\nmodel.fit(X, [y, y], epochs=5)\n\nutils.plot_model(model, show_layer_names=False, show_shapes=True)\n\nEpoch 1/5\n1/1 [==============================] - 1s 1s/step - loss: 0.6819 - first_loss: 0.7424 - second_loss: 0.6214\nEpoch 2/5\n1/1 [==============================] - 0s 423ms/step - loss: 0.6381 - first_loss: 0.6361 - second_loss: 0.6400\nEpoch 3/5\n1/1 [==============================] - 0s 442ms/step - loss: 0.6137 - first_loss: 0.6126 - second_loss: 0.6147\nEpoch 4/5\n1/1 [==============================] - 0s 434ms/step - loss: 0.6214 - first_loss: 0.6159 - second_loss: 0.6268\nEpoch 5/5\n1/1 [==============================] - 0s 427ms/step - loss: 0.6248 - first_loss: 0.6184 - second_loss: 0.6311\n"
"#=====================================================\n# purpose: logistic regression \nimport numpy as np\nimport scipy as sp\nimport scipy.optimize\n\nimport matplotlib as mpl\nimport os\n\n# prepare the data\ndata = np.loadtxt('data.csv', delimiter=',', skiprows=1)\nvY = data[:, 0]\nmX = data[:, 1:]\n# mX = (mX - np.mean(mX))/np.std(mX)  # standardize the data; if required\n\nintercept = np.ones(mX.shape[0]).reshape(mX.shape[0], 1)\nmX = np.concatenate((intercept, mX), axis = 1)\niK = mX.shape[1]\niN = mX.shape[0]\n\n# logistic transformation\ndef logit(mX, vBeta):\n    return((np.exp(np.dot(mX, vBeta))/(1.0 + np.exp(np.dot(mX, vBeta)))))\n\n# test function call\nvBeta0 = np.array([-.10296645, -.0332327, -.01209484, .44626211, .92554137, .53973828, \n    1.7993371, .7148045  ])\nlogit(mX, vBeta0)\n\n# cost function\ndef logLikelihoodLogit(vBeta, mX, vY):\n    return(-(np.sum(vY*np.log(logit(mX, vBeta)) + (1-vY)*(np.log(1-logit(mX, vBeta))))))\nlogLikelihoodLogit(vBeta0, mX, vY) # test function call\n\n# different parametrization of the cost function\ndef logLikelihoodLogitVerbose(vBeta, mX, vY):\n    return(-(np.sum(vY*(np.dot(mX, vBeta) - np.log((1.0 + np.exp(np.dot(mX, vBeta))))) +\n                    (1-vY)*(-np.log((1.0 + np.exp(np.dot(mX, vBeta))))))))\nlogLikelihoodLogitVerbose(vBeta0, mX, vY)  # test function call\n\n# gradient function\ndef likelihoodScore(vBeta, mX, vY):\n    return(np.dot(mX.T, \n                  (logit(mX, vBeta) - vY)))\nlikelihoodScore(vBeta0, mX, vY).shape # test function call\nsp.optimize.check_grad(logLikelihoodLogitVerbose, likelihoodScore, \n                       vBeta0, mX, vY)  # check that the analytical gradient is close to \n                                                # numerical gradient\n\n# optimize the function (without gradient)\noptimLogit = scipy.optimize.fmin_bfgs(logLikelihoodLogitVerbose, \n                                  x0 = np.array([-.1, -.03, -.01, .44, .92, .53,\n                                            1.8, .71]), \n                                  args = (mX, vY), gtol = 1e-3)\n\n# optimize the function (with gradient)\noptimLogit = scipy.optimize.fmin_bfgs(logLikelihoodLogitVerbose, \n                                  x0 = np.array([-.1, -.03, -.01, .44, .92, .53,\n                                            1.8, .71]), fprime = likelihoodScore, \n                                  args = (mX, vY), gtol = 1e-3)\n#=====================================================\n"
'import codecs, re, time\nfrom itertools import chain\n\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntrainfile = \'train.txt\'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = CountVectorizer(analyzer=\'word\')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,\'r\',\'utf8\'))\ntags = [\'bs\',\'pt\',\'bs\',\'pt\']\n\n# Training NB\nmnb = MultinomialNB()\nmnb.fit(trainset, tags)\n\nprint mnb.classes_\nprint mnb.coef_[0]\nprint mnb.coef_[1]\n\n[\'bs\' \'pt\']\n[-5.55682806 -4.86368088 -4.86368088 -5.55682806 -5.55682806 -5.55682806\n -4.86368088 -4.86368088 -5.55682806 -5.55682806 -4.86368088 -4.86368088\n -4.1705337  -5.55682806 -4.86368088 -5.55682806 -4.86368088 -5.55682806\n -5.55682806 -5.55682806 -4.86368088 -4.45821577 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -5.55682806 -4.86368088 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -4.86368088 -5.55682806 -5.55682806\n -5.55682806 -5.55682806 -5.55682806 -4.45821577 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -5.55682806 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088\n -4.86368088 -5.55682806 -5.55682806 -5.55682806 -5.55682806 -5.55682806\n -5.55682806 -5.55682806 -5.55682806 -4.86368088 -4.86368088 -4.86368088\n -4.86368088 -5.55682806 -5.55682806 -4.86368088 -5.55682806 -4.86368088\n -5.55682806 -5.55682806 -4.86368088 -4.86368088 -4.45821577 -4.86368088\n -4.86368088 -4.45821577 -4.86368088 -4.86368088 -4.86368088 -5.55682806\n -4.86368088 -5.55682806 -5.55682806 -4.86368088 -5.55682806 -5.55682806\n -4.86368088 -5.55682806 -4.86368088 -4.86368088 -4.86368088 -5.55682806\n -5.55682806 -5.55682806 -4.86368088 -4.86368088 -5.55682806 -4.86368088\n -5.55682806 -4.86368088 -5.55682806 -4.86368088 -5.55682806 -5.55682806\n -5.55682806 -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088\n -4.86368088 -4.1705337  -4.86368088 -4.86368088 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -4.86368088 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088\n -5.55682806 -5.55682806 -4.86368088 -4.86368088 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088 -5.55682806\n -4.86368088 -4.45821577 -4.86368088 -4.86368088]\nTraceback (most recent call last):\n  File "test.py", line 24, in &lt;module&gt;\n    print mnb.coef_[1]\nIndexError: index 1 is out of bounds for axis 0 with size 1\n\nprint mnb.feature_count_\nprint mnb.coef_[0]\n\n[[ 1.  0.  0.  1.  1.  1.  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.  0.  1.\n   1.  1.  2.  2.  0.  0.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  2.  1.\n   1.  1.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.\n   0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  1.  0.\n   1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  1.  0.  1.  1.\n   0.  1.  0.  0.  0.  1.  1.  1.  0.  0.  1.  0.  1.  0.  1.  0.  1.  1.\n   1.  0.  0.  1.  0.  0.  0.  4.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.\n   0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  0.\n   0.  0.  1.  0.  0.  1.  0.  0.  0.  0.]\n [ 0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  1.  1.  3.  0.  1.  0.  1.  0.\n   0.  0.  1.  2.  1.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  0.  0.\n   0.  0.  0.  2.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  0.  1.  1.\n   1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  1.  0.  1.\n   0.  0.  1.  1.  2.  1.  1.  2.  1.  1.  1.  0.  1.  0.  0.  1.  0.  0.\n   1.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.  1.  0.  1.  0.  1.  0.  0.\n   0.  1.  1.  0.  1.  1.  1.  3.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.\n   1.  1.  0.  1.  1.  1.  1.  1.  1.  0.  1.  1.  0.  0.  1.  1.  1.  1.\n   1.  1.  0.  1.  1.  0.  1.  2.  1.  1.]]\n[-5.55682806 -4.86368088 -4.86368088 -5.55682806 -5.55682806 -5.55682806\n -4.86368088 -4.86368088 -5.55682806 -5.55682806 -4.86368088 -4.86368088\n -4.1705337  -5.55682806 -4.86368088 -5.55682806 -4.86368088 -5.55682806\n -5.55682806 -5.55682806 -4.86368088 -4.45821577 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -5.55682806 -4.86368088 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -4.86368088 -5.55682806 -5.55682806\n -5.55682806 -5.55682806 -5.55682806 -4.45821577 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -5.55682806 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088\n -4.86368088 -5.55682806 -5.55682806 -5.55682806 -5.55682806 -5.55682806\n -5.55682806 -5.55682806 -5.55682806 -4.86368088 -4.86368088 -4.86368088\n -4.86368088 -5.55682806 -5.55682806 -4.86368088 -5.55682806 -4.86368088\n -5.55682806 -5.55682806 -4.86368088 -4.86368088 -4.45821577 -4.86368088\n -4.86368088 -4.45821577 -4.86368088 -4.86368088 -4.86368088 -5.55682806\n -4.86368088 -5.55682806 -5.55682806 -4.86368088 -5.55682806 -5.55682806\n -4.86368088 -5.55682806 -4.86368088 -4.86368088 -4.86368088 -5.55682806\n -5.55682806 -5.55682806 -4.86368088 -4.86368088 -5.55682806 -4.86368088\n -5.55682806 -4.86368088 -5.55682806 -4.86368088 -5.55682806 -5.55682806\n -5.55682806 -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088\n -4.86368088 -4.1705337  -4.86368088 -4.86368088 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -4.86368088 -5.55682806 -4.86368088\n -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088\n -5.55682806 -5.55682806 -4.86368088 -4.86368088 -4.86368088 -4.86368088\n -4.86368088 -4.86368088 -5.55682806 -4.86368088 -4.86368088 -5.55682806\n -4.86368088 -4.45821577 -4.86368088 -4.86368088]\n\nindex = 0\ncoef_features_c1_c2 = []\n\nfor feat, c1, c2 in zip(word_vectorizer.get_feature_names(), mnb.feature_count_[0], mnb.feature_count_[1]):\n    coef_features_c1_c2.append(tuple([mnb.coef_[0][index], feat, c1, c2]))\n    index+=1\n\nfor i in sorted(coef_features_c1_c2):\n    print i\n\n(-5.5568280616995374, u\'acuerdo\', 1.0, 0.0)\n(-5.5568280616995374, u\'al\', 1.0, 0.0)\n(-5.5568280616995374, u\'alex\', 1.0, 0.0)\n(-5.5568280616995374, u\'algo\', 1.0, 0.0)\n(-5.5568280616995374, u\'andaba\', 1.0, 0.0)\n(-5.5568280616995374, u\'andrea\', 1.0, 0.0)\n(-5.5568280616995374, u\'bien\', 1.0, 0.0)\n(-5.5568280616995374, u\'buscando\', 1.0, 0.0)\n(-5.5568280616995374, u\'como\', 1.0, 0.0)\n(-5.5568280616995374, u\'con\', 1.0, 0.0)\n(-5.5568280616995374, u\'conseguido\', 1.0, 0.0)\n(-5.5568280616995374, u\'distancia\', 1.0, 0.0)\n(-5.5568280616995374, u\'doprinese\', 1.0, 0.0)\n(-5.5568280616995374, u\'es\', 2.0, 0.0)\n(-5.5568280616995374, u\'est\\xe1\', 1.0, 0.0)\n(-5.5568280616995374, u\'eulex\', 1.0, 0.0)\n(-5.5568280616995374, u\'excusa\', 1.0, 0.0)\n(-5.5568280616995374, u\'fama\', 1.0, 0.0)\n(-5.5568280616995374, u\'guasch\', 1.0, 0.0)\n(-5.5568280616995374, u\'ha\', 1.0, 0.0)\n(-5.5568280616995374, u\'incident\', 1.0, 0.0)\n(-5.5568280616995374, u\'ispit\', 1.0, 0.0)\n(-5.5568280616995374, u\'istragu\', 1.0, 0.0)\n(-5.5568280616995374, u\'izbijanju\', 1.0, 0.0)\n(-5.5568280616995374, u\'ja\\u010danju\', 1.0, 0.0)\n(-5.5568280616995374, u\'je\', 1.0, 0.0)\n(-5.5568280616995374, u\'jedan\', 1.0, 0.0)\n(-5.5568280616995374, u\'jo\\u0161\', 1.0, 0.0)\n(-5.5568280616995374, u\'kapaciteta\', 1.0, 0.0)\n(-5.5568280616995374, u\'kosova\', 1.0, 0.0)\n(-5.5568280616995374, u\'la\', 1.0, 0.0)\n(-5.5568280616995374, u\'lequio\', 1.0, 0.0)\n(-5.5568280616995374, u\'llevar\', 1.0, 0.0)\n(-5.5568280616995374, u\'lo\', 2.0, 0.0)\n(-5.5568280616995374, u\'misije\', 1.0, 0.0)\n(-5.5568280616995374, u\'muy\', 1.0, 0.0)\n(-5.5568280616995374, u\'m\\xe1s\', 1.0, 0.0)\n(-5.5568280616995374, u\'na\', 1.0, 0.0)\n(-5.5568280616995374, u\'nada\', 1.0, 0.0)\n(-5.5568280616995374, u\'nasilja\', 1.0, 0.0)\n(-5.5568280616995374, u\'no\', 1.0, 0.0)\n(-5.5568280616995374, u\'obaviti\', 1.0, 0.0)\n(-5.5568280616995374, u\'obe\\u0107ao\', 1.0, 0.0)\n(-5.5568280616995374, u\'parecer\', 1.0, 0.0)\n(-5.5568280616995374, u\'pone\', 1.0, 0.0)\n(-5.5568280616995374, u\'por\', 1.0, 0.0)\n(-5.5568280616995374, u\'po\\u0161to\', 1.0, 0.0)\n(-5.5568280616995374, u\'prava\', 1.0, 0.0)\n(-5.5568280616995374, u\'predstavlja\', 1.0, 0.0)\n(-5.5568280616995374, u\'pro\\u0161losedmi\\u010dnom\', 1.0, 0.0)\n(-5.5568280616995374, u\'relaci\\xf3n\', 1.0, 0.0)\n(-5.5568280616995374, u\'sjeveru\', 1.0, 0.0)\n(-5.5568280616995374, u\'taj\', 1.0, 0.0)\n(-5.5568280616995374, u\'una\', 1.0, 0.0)\n(-5.5568280616995374, u\'visto\', 1.0, 0.0)\n(-5.5568280616995374, u\'vladavine\', 1.0, 0.0)\n(-5.5568280616995374, u\'ya\', 1.0, 0.0)\n(-5.5568280616995374, u\'\\u0107e\', 1.0, 0.0)\n(-4.863680881139592, u\'aj\', 0.0, 1.0)\n(-4.863680881139592, u\'ajudou\', 0.0, 1.0)\n(-4.863680881139592, u\'alpsk\\xfdmi\', 0.0, 1.0)\n(-4.863680881139592, u\'alpy\', 0.0, 1.0)\n(-4.863680881139592, u\'ao\', 0.0, 1.0)\n(-4.863680881139592, u\'apresenta\', 0.0, 1.0)\n(-4.863680881139592, u\'bl\\xedzko\', 0.0, 1.0)\n(-4.863680881139592, u\'come\\xe7o\', 0.0, 1.0)\n(-4.863680881139592, u\'da\', 2.0, 1.0)\n(-4.863680881139592, u\'decepcionantes\', 0.0, 1.0)\n(-4.863680881139592, u\'deti\', 0.0, 1.0)\n(-4.863680881139592, u\'dificuldades\', 0.0, 1.0)\n(-4.863680881139592, u\'dif\\xedcil\', 1.0, 1.0)\n(-4.863680881139592, u\'do\', 0.0, 1.0)\n(-4.863680881139592, u\'druh\', 0.0, 1.0)\n(-4.863680881139592, u\'d\\xe1\', 0.0, 1.0)\n(-4.863680881139592, u\'ela\', 0.0, 1.0)\n(-4.863680881139592, u\'encontrar\', 0.0, 1.0)\n(-4.863680881139592, u\'enfrentar\', 0.0, 1.0)\n(-4.863680881139592, u\'for\\xe7as\', 0.0, 1.0)\n(-4.863680881139592, u\'furiosa\', 0.0, 1.0)\n(-4.863680881139592, u\'golf\', 0.0, 1.0)\n(-4.863680881139592, u\'golfistami\', 0.0, 1.0)\n(-4.863680881139592, u\'golfov\\xfdch\', 0.0, 1.0)\n(-4.863680881139592, u\'hotelmi\', 0.0, 1.0)\n(-4.863680881139592, u\'hra\\u0165\', 0.0, 1.0)\n(-4.863680881139592, u\'ide\', 0.0, 1.0)\n(-4.863680881139592, u\'ihr\\xedsk\', 0.0, 1.0)\n(-4.863680881139592, u\'intranspon\\xedveis\', 0.0, 1.0)\n(-4.863680881139592, u\'in\\xedcio\', 0.0, 1.0)\n(-4.863680881139592, u\'in\\xfd\', 0.0, 1.0)\n(-4.863680881139592, u\'kde\', 0.0, 1.0)\n(-4.863680881139592, u\'kombin\\xe1cie\', 0.0, 1.0)\n(-4.863680881139592, u\'komplex\', 0.0, 1.0)\n(-4.863680881139592, u\'kon\\u010diarmi\', 0.0, 1.0)\n(-4.863680881139592, u\'lado\', 0.0, 1.0)\n(-4.863680881139592, u\'lete\', 0.0, 1.0)\n(-4.863680881139592, u\'longo\', 0.0, 1.0)\n(-4.863680881139592, u\'ly\\u017eova\\u0165\', 0.0, 1.0)\n(-4.863680881139592, u\'man\\u017eelky\', 0.0, 1.0)\n(-4.863680881139592, u\'mas\', 0.0, 1.0)\n(-4.863680881139592, u\'mesmo\', 0.0, 1.0)\n(-4.863680881139592, u\'meu\', 0.0, 1.0)\n(-4.863680881139592, u\'minha\', 0.0, 1.0)\n(-4.863680881139592, u\'mo\\u017enos\\u0165ami\', 0.0, 1.0)\n(-4.863680881139592, u\'m\\xe3e\', 0.0, 1.0)\n(-4.863680881139592, u\'nad\\u0161en\\xfdmi\', 0.0, 1.0)\n(-4.863680881139592, u\'negativas\', 0.0, 1.0)\n(-4.863680881139592, u\'nie\', 0.0, 1.0)\n(-4.863680881139592, u\'nieko\\u013ek\\xfdch\', 0.0, 1.0)\n(-4.863680881139592, u\'para\', 0.0, 1.0)\n(-4.863680881139592, u\'parecem\', 0.0, 1.0)\n(-4.863680881139592, u\'pod\', 0.0, 1.0)\n(-4.863680881139592, u\'pon\\xfakaj\\xfa\', 0.0, 1.0)\n(-4.863680881139592, u\'potrebuj\\xfa\', 0.0, 1.0)\n(-4.863680881139592, u\'pri\', 0.0, 1.0)\n(-4.863680881139592, u\'prova\\xe7\\xf5es\', 0.0, 1.0)\n(-4.863680881139592, u\'punham\', 0.0, 1.0)\n(-4.863680881139592, u\'qual\', 0.0, 1.0)\n(-4.863680881139592, u\'qualquer\', 0.0, 1.0)\n(-4.863680881139592, u\'quem\', 0.0, 1.0)\n(-4.863680881139592, u\'rak\\xfaske\', 0.0, 1.0)\n(-4.863680881139592, u\'rezortov\', 0.0, 1.0)\n(-4.863680881139592, u\'sa\', 0.0, 1.0)\n(-4.863680881139592, u\'sebe\', 0.0, 1.0)\n(-4.863680881139592, u\'sempre\', 0.0, 1.0)\n(-4.863680881139592, u\'situa\\xe7\\xf5es\', 0.0, 1.0)\n(-4.863680881139592, u\'spojen\\xfdch\', 0.0, 1.0)\n(-4.863680881139592, u\'suplantar\', 0.0, 1.0)\n(-4.863680881139592, u\'s\\xfa\', 0.0, 1.0)\n(-4.863680881139592, u\'tak\', 0.0, 1.0)\n(-4.863680881139592, u\'talianske\', 0.0, 1.0)\n(-4.863680881139592, u\'teve\', 0.0, 1.0)\n(-4.863680881139592, u\'tive\', 0.0, 1.0)\n(-4.863680881139592, u\'todas\', 0.0, 1.0)\n(-4.863680881139592, u\'tr\\xe1venia\', 0.0, 1.0)\n(-4.863680881139592, u\'ve\\u013ek\\xfd\', 0.0, 1.0)\n(-4.863680881139592, u\'vida\', 0.0, 1.0)\n(-4.863680881139592, u\'vo\', 0.0, 1.0)\n(-4.863680881139592, u\'vo\\u013en\\xe9ho\', 0.0, 1.0)\n(-4.863680881139592, u\'vysok\\xfdmi\', 0.0, 1.0)\n(-4.863680881139592, u\'vy\\u017eitia\', 0.0, 1.0)\n(-4.863680881139592, u\'v\\xe4\\u010d\\u0161ine\', 0.0, 1.0)\n(-4.863680881139592, u\'v\\u017edy\', 0.0, 1.0)\n(-4.863680881139592, u\'zauj\\xedmav\\xe9\', 0.0, 1.0)\n(-4.863680881139592, u\'zime\', 0.0, 1.0)\n(-4.863680881139592, u\'\\u010dasu\', 0.0, 1.0)\n(-4.863680881139592, u\'\\u010fal\\u0161\\xedmi\', 0.0, 1.0)\n(-4.863680881139592, u\'\\u0161vaj\\u010diarske\', 0.0, 1.0)\n(-4.4582157730314274, u\'de\', 2.0, 2.0)\n(-4.4582157730314274, u\'foi\', 0.0, 2.0)\n(-4.4582157730314274, u\'mais\', 0.0, 2.0)\n(-4.4582157730314274, u\'me\', 0.0, 2.0)\n(-4.4582157730314274, u\'\\u010di\', 0.0, 2.0)\n(-4.1705337005796466, u\'as\', 0.0, 3.0)\n(-4.1705337005796466, u\'que\', 4.0, 3.0)\n\nimport codecs, re, time\nfrom itertools import chain\n\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntrainfile = \'train.txt\'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = CountVectorizer(analyzer=\'word\')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,\'r\',\'utf8\'))\ntags = [\'bs\',\'pt\',\'bs\',\'pt\']\n\n# Training NB\nmnb = MultinomialNB()\nmnb.fit(trainset, tags)\n\ndef most_informative_feature_for_binary_classification(vectorizer, classifier, n=10):\n    class_labels = classifier.classes_\n    feature_names = vectorizer.get_feature_names()\n    topn_class1 = sorted(zip(classifier.coef_[0], feature_names))[:n]\n    topn_class2 = sorted(zip(classifier.coef_[0], feature_names))[-n:]\n\n    for coef, feat in topn_class1:\n        print class_labels[0], coef, feat\n\n    print\n\n    for coef, feat in reversed(topn_class2):\n        print class_labels[1], coef, feat\n\n\nmost_informative_feature_for_binary_classification(word_vectorizer, mnb)\n\nbs -5.5568280617 acuerdo\nbs -5.5568280617 al\nbs -5.5568280617 alex\nbs -5.5568280617 algo\nbs -5.5568280617 andaba\nbs -5.5568280617 andrea\nbs -5.5568280617 bien\nbs -5.5568280617 buscando\nbs -5.5568280617 como\nbs -5.5568280617 con\n\npt -4.17053370058 que\npt -4.17053370058 as\npt -4.45821577303 či\npt -4.45821577303 me\npt -4.45821577303 mais\npt -4.45821577303 foi\npt -4.45821577303 de\npt -4.86368088114 švajčiarske\npt -4.86368088114 ďalšími\npt -4.86368088114 času\n'
'import numpy as np\nfrom sklearn.cluster import AgglomerativeClustering\nimport itertools\n\nX = np.concatenate([np.random.randn(3, 10), np.random.randn(2, 10) + 100])\nmodel = AgglomerativeClustering(linkage="average", affinity="cosine")\nmodel.fit(X)\n\nii = itertools.count(X.shape[0])\n[{\'node_id\': next(ii), \'left\': x[0], \'right\':x[1]} for x in model.children_]\n'
'X_test=tvect.transform(test)\nclassifier.predict(X_test)\n'
'def just_transforms(self, X):\n    """Applies all transforms to the data, without applying last \n       estimator.\n\n    Parameters\n    ----------\n    X : iterable\n        Data to predict on. Must fulfill input requirements of first step of\n        the pipeline.\n    """\n    Xt = X\n    for name, transform in self.steps[:-1]:\n        Xt = transform.transform(Xt)\n    return Xt\n'
"import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nclass Neuralnet:\n    def __init__(self, neurons, activation):\n        self.weights = []\n        self.inputs = []\n        self.outputs = []\n        self.errors = []\n        self.rate = 0.5\n        self.activation = activation    #sigmoid or tanh\n\n        self.neurons = neurons\n        self.L = len(self.neurons)      #number of layers\n\n        eps = 0.12;    # range for uniform distribution   -eps..+eps              \n        for layer in range(len(neurons)-1):\n            self.weights.append(np.random.uniform(-eps,eps,size=(neurons[layer+1], neurons[layer]+1)))            \n\n\n    ###################################################################################################    \n    def train(self, X, Y, iter_count):\n\n        m = X.shape[0];\n\n        for layer in range(self.L):\n            self.inputs.append(np.empty([m, self.neurons[layer]]))        \n            self.errors.append(np.empty([m, self.neurons[layer]]))\n\n            if (layer &lt; self.L -1):\n                self.outputs.append(np.empty([m, self.neurons[layer]+1]))\n            else:\n                self.outputs.append(np.empty([m, self.neurons[layer]]))\n\n        #accumulate the cost function\n        J_history = np.zeros([iter_count, 1])\n\n\n        for i in range(iter_count):\n\n            self.feedforward(X)\n\n            J = self.cost(Y, self.outputs[self.L-1])\n            J_history[i, 0] = J\n\n            self.backpropagate(Y)\n\n\n        #plot the cost function to check the descent\n        plt.plot(J_history)\n        plt.show()\n\n\n    ###################################################################################################    \n    def cost(self, Y, H):     \n        J = np.sum(np.sum(np.power((Y - H), 2), axis=0))/(2*m)\n        return J\n\n    ###################################################################################################\n    def feedforward(self, X):\n\n        m = X.shape[0];\n\n        self.outputs[0] = np.concatenate(  (np.ones([m, 1]),   X),   axis=1)\n\n        for i in range(1, self.L):\n            self.inputs[i] = np.dot( self.outputs[i-1], self.weights[i-1].T  )\n\n            if (self.activation == 'sigmoid'):\n                output_temp = self.sigmoid(self.inputs[i])\n            elif (self.activation == 'tanh'):\n                output_temp = np.tanh(self.inputs[i])\n\n\n            if (i &lt; self.L - 1):\n                self.outputs[i] = np.concatenate(  (np.ones([m, 1]),   output_temp),   axis=1)\n            else:\n                self.outputs[i] = output_temp\n\n    ###################################################################################################\n    def backpropagate(self, Y):\n\n        self.errors[self.L-1] = self.outputs[self.L-1] - Y\n\n        for i in range(self.L - 2, 0, -1):\n\n            if (self.activation == 'sigmoid'):\n                self.errors[i] = np.dot(  self.errors[i+1],   self.weights[i][:, 1:]  ) *  self.sigmoid_prime(self.inputs[i])\n            elif (self.activation == 'tanh'):\n                self.errors[i] = np.dot(  self.errors[i+1],   self.weights[i][:, 1:]  ) *  (1 - self.outputs[i][:, 1:]*self.outputs[i][:, 1:])\n\n        for i in range(0, self.L-1):\n            grad = np.dot(self.errors[i+1].T, self.outputs[i]) / m\n            self.weights[i] = self.weights[i] - self.rate*grad\n\n    ###################################################################################################\n    def sigmoid(self, z):\n        s = 1.0/(1.0 + np.exp(-z))\n        return s\n\n    ###################################################################################################\n    def sigmoid_prime(self, z):\n        s = self.sigmoid(z)*(1 - self.sigmoid(z))\n        return s    \n\n    ###################################################################################################\n    def predict(self, X, weights):\n\n        m = X.shape[0];\n\n        self.inputs = []\n        self.outputs = []\n        self.weights = weights\n\n        for layer in range(self.L):\n            self.inputs.append(np.empty([m, self.neurons[layer]]))        \n\n            if (layer &lt; self.L -1):\n                self.outputs.append(np.empty([m, self.neurons[layer]+1]))\n            else:\n                self.outputs.append(np.empty([m, self.neurons[layer]]))\n\n        self.feedforward(X)\n\n        return self.outputs[self.L-1]\n\n\n###################################################################################################\n#                MAIN PART\n\nactivation1 = 'sigmoid'     # the input should be scaled into [ 0..1]\nactivation2 = 'tanh'        # the input should be scaled into [-1..1]\n\nactivation = activation1\n\nnet = Neuralnet([1, 6, 1], activation) # structure of the NN and its activation function\n\n\n##########################################################################################\n#                TRAINING\n\nm = 1000 #size of the training set\nX = np.linspace(0, 4*math.pi, num = m).reshape(m, 1); # input training set\n\n\nY = np.sin(X) # target\n\nkx = 0.1 # noise parameter\nnoise = (2.0*np.random.uniform(0, kx, m) - kx).reshape(m, 1)\nY = Y + noise # noisy target\n\n# scaling of the target depending on the activation function\nif (activation == 'sigmoid'):\n    Y_scaled = (Y/(1+kx) + 1)/2.0\nelif (activation == 'tanh'):\n    Y_scaled = Y/(1+kx)\n\n\n# number of the iteration for the training stage\niter_count = 20000\nnet.train(X, Y_scaled, iter_count) #training\n\n# gained weights\ntrained_weights = net.weights\n\n##########################################################################################\n#                 PREDICTION\n\nm_new = 40 #size of the prediction set\nX_new = np.linspace(0, 4*math.pi, num = m_new).reshape(m_new, 1);\n\nY_new = net.predict(X_new, trained_weights) # prediction\n\n#rescaling of the result \nif (activation == 'sigmoid'):\n    Y_new = (2.0*Y_new - 1.0) * (1+kx)\nelif (activation == 'tanh'):\n    Y_new = Y_new * (1+kx)\n\n# visualization\nplt.plot(X, Y)\nplt.plot(X_new, Y_new, 'ro')\nplt.show()\n\nraw_input('press any key to exit')\n\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nclass Neuralnet:\n    def __init__(self, neurons, activation):\n        self.weights = []\n        self.inputs = []\n        self.outputs = []\n        self.errors = []\n        self.rate = 0.2\n        self.activation = activation    #sigmoid or tanh\n\n        self.neurons = neurons\n        self.L = len(self.neurons)      #number of layers\n\n        eps = 0.12;    #range for uniform distribution   -eps..+eps              \n        for layer in range(len(neurons)-1):\n            self.weights.append(np.random.uniform(-eps,eps,size=(neurons[layer+1], neurons[layer]+1)))            \n\n\n    ###################################################################################################    \n    def train(self, X, Y, iter_count):\n\n        m = X.shape[0];\n\n        for layer in range(self.L):\n            self.inputs.append(np.empty([m, self.neurons[layer]]))        \n            self.errors.append(np.empty([m, self.neurons[layer]]))\n\n            if (layer &lt; self.L -1):\n                self.outputs.append(np.empty([m, self.neurons[layer]+1]))\n            else:\n                self.outputs.append(np.empty([m, self.neurons[layer]]))\n\n        #accumulate the cost function\n        J_history = np.zeros([iter_count, 1])\n\n\n        for i in range(iter_count):\n\n            self.feedforward(X)\n\n            J = self.cost(Y, self.outputs[self.L-1])\n            J_history[i, 0] = J\n\n            self.backpropagate(Y)\n\n\n        #plot the cost function to check the descent\n        #plt.plot(J_history)\n        #plt.show()\n\n\n    ###################################################################################################    \n    def cost(self, Y, H):     \n        J = np.sum(np.sum(np.power((Y - H), 2), axis=0))/(2*m)\n        return J\n\n\n    ###################################################################################################\n    def cost_online(self, min_x, max_x, iter_number):\n        h_arr = np.zeros([iter_number, 1])\n        y_arr = np.zeros([iter_number, 1])\n\n        for step in range(iter_number):\n            x = np.random.uniform(min_x, max_x, 1).reshape(1, 1)\n\n            self.feedforward(x)\n            h_arr[step, 0] = self.outputs[-1]\n            y_arr[step, 0] = np.sin(x)\n\n\n\n        J = np.sum(np.sum(np.power((y_arr - h_arr), 2), axis=0))/(2*iter_number)\n        return J\n\n    ###################################################################################################\n    def feedforward(self, X):\n\n        m = X.shape[0];\n\n        self.outputs[0] = np.concatenate(  (np.ones([m, 1]),   X),   axis=1)\n\n        for i in range(1, self.L):\n            self.inputs[i] = np.dot( self.outputs[i-1], self.weights[i-1].T  )\n\n            if (self.activation == 'sigmoid'):\n                output_temp = self.sigmoid(self.inputs[i])\n            elif (self.activation == 'tanh'):\n                output_temp = np.tanh(self.inputs[i])\n\n\n            if (i &lt; self.L - 1):\n                self.outputs[i] = np.concatenate(  (np.ones([m, 1]),   output_temp),   axis=1)\n            else:\n                self.outputs[i] = output_temp\n\n    ###################################################################################################\n    def backpropagate(self, Y):\n\n        self.errors[self.L-1] = self.outputs[self.L-1] - Y\n\n        for i in range(self.L - 2, 0, -1):\n\n            if (self.activation == 'sigmoid'):\n                self.errors[i] = np.dot(  self.errors[i+1],   self.weights[i][:, 1:]  ) *  self.sigmoid_prime(self.inputs[i])\n            elif (self.activation == 'tanh'):\n                self.errors[i] = np.dot(  self.errors[i+1],   self.weights[i][:, 1:]  ) *  (1 - self.outputs[i][:, 1:]*self.outputs[i][:, 1:])\n\n        for i in range(0, self.L-1):\n            grad = np.dot(self.errors[i+1].T, self.outputs[i]) / m\n            self.weights[i] = self.weights[i] - self.rate*grad\n\n\n    ###################################################################################################\n    def sigmoid(self, z):\n        s = 1.0/(1.0 + np.exp(-z))\n        return s\n\n    ###################################################################################################\n    def sigmoid_prime(self, z):\n        s = self.sigmoid(z)*(1 - self.sigmoid(z))\n        return s    \n\n    ###################################################################################################\n    def predict(self, X, weights):\n\n        m = X.shape[0];\n\n        self.inputs = []\n        self.outputs = []\n        self.weights = weights\n\n        for layer in range(self.L):\n            self.inputs.append(np.empty([m, self.neurons[layer]]))        \n\n            if (layer &lt; self.L -1):\n                self.outputs.append(np.empty([m, self.neurons[layer]+1]))\n            else:\n                self.outputs.append(np.empty([m, self.neurons[layer]]))\n\n        self.feedforward(X)\n\n        return self.outputs[self.L-1]\n\n\n###################################################################################################\n#                MAIN PART\n\nactivation1 = 'sigmoid'     #the input should be scaled into [0..1]\nactivation2 = 'tanh'        #the input should be scaled into [-1..1]\n\nactivation = activation1\n\nnet = Neuralnet([1, 6, 1], activation) # structure of the NN and its activation function\n\n\nmethod1 = 'online'\nmethod2 = 'offline'\n\nmethod = method1\n\nkx = 0.1 #noise parameter\n\n###################################################################################################\n#                TRAINING\n\nif (method == 'offline'):\n\n    m = 1000 #size of the training set\n    X = np.linspace(0, 4*math.pi, num = m).reshape(m, 1); #input training set\n\n\n    Y = np.sin(X) #target\n\n\n    noise = (2.0*np.random.uniform(0, kx, m) - kx).reshape(m, 1)\n    Y = Y + noise #noisy target\n\n    #scaling of the target depending on the activation function\n    if (activation == 'sigmoid'):\n        Y_scaled = (Y/(1+kx) + 1)/2.0\n    elif (activation == 'tanh'):\n        Y_scaled = Y/(1+kx)\n\n\n    #number of the iteration for the training stage\n    iter_count = 20000\n    net.train(X, Y_scaled, iter_count) #training\n\nelif (method == 'online'):\n\n    sampling_count = 100000 # number of samplings during the training stage\n\n\n    m = 1 #batch size\n\n    iter_count = sampling_count/m\n\n    for layer in range(net.L):\n        net.inputs.append(np.empty([m, net.neurons[layer]]))        \n        net.errors.append(np.empty([m, net.neurons[layer]]))\n\n        if (layer &lt; net.L -1):\n            net.outputs.append(np.empty([m, net.neurons[layer]+1]))\n        else:\n            net.outputs.append(np.empty([m, net.neurons[layer]]))    \n\n    J_history = []\n    step_history = []\n\n    for i in range(iter_count):\n        X = np.random.uniform(0, 4*math.pi, m).reshape(m, 1)\n\n        Y = np.sin(X) #target\n        noise = (2.0*np.random.uniform(0, kx, m) - kx).reshape(m, 1)\n        Y = Y + noise #noisy target\n\n        #scaling of the target depending on the activation function\n        if (activation == 'sigmoid'):\n            Y_scaled = (Y/(1+kx) + 1)/2.0\n        elif (activation == 'tanh'):\n            Y_scaled = Y/(1+kx)\n\n        net.feedforward(X)\n        net.backpropagate(Y_scaled)\n\n\n        if (np.remainder(i, 1000) == 0):\n            J = net.cost_online(0, 4*math.pi, 1000)\n            J_history.append(J)\n            step_history.append(i)\n\n    plt.plot(step_history, J_history)\n    plt.title('Batch size ' + str(m) + ', rate ' + str(net.rate) + ', samples ' + str(sampling_count))\n    #plt.ylim([0, 0.1])\n\n    plt.show()\n\n#gained weights\ntrained_weights = net.weights\n\n##########################################################################################\n#                 PREDICTION\n\nm_new = 40 #size of the prediction set\nX_new = np.linspace(0, 4*math.pi, num = m_new).reshape(m_new, 1);\n\nY_new = net.predict(X_new, trained_weights) #prediction\n\n#rescaling of the result \nif (activation == 'sigmoid'):\n    Y_new = (2.0*Y_new - 1.0) * (1+kx)\nelif (activation == 'tanh'):\n    Y_new = Y_new * (1+kx)\n\n#visualization\n\n#fake sine curve to show the ideal signal\nif (method == 'online'):\n    X = np.linspace(0, 4*math.pi, num = 100)\n    Y = np.sin(X)\n\nplt.plot(X, Y)\n\nplt.plot(X_new, Y_new, 'ro')\nif (method == 'online'):\n    plt.title('Batch size ' + str(m) + ', rate ' + str(net.rate) + ', samples ' + str(sampling_count))\nplt.ylim([-1.5, 1.5])\nplt.show()\n\nraw_input('press any key to exit')\n\ndef sine_example():\n    net = Neuralnet([1, 6, 1])\n    for step in range(100000):\n        x = np.random.normal()\n        net.feedforward([x])\n        net.backpropagate([np.tanh(np.sin(x))])\n    net.feedforward([3])\n    print(net.outputs[-1])\n"
'In [7]: x = np.arange(100)\n\nIn [8]: delta = np.random.uniform(-10,10, size=(100,))\n\nIn [9]: y = .4 * x +3 + delta\n'
'from skimage import io, segmentation as seg\nimport matplotlib as plt\nimport numpy as np\ncolor_image = io.imread(\'smallimg.jpg\')\nlabels = seg.slic(color_image, n_segments=4, compactness=4)\nfor section in np.unique(labels):\n    rows, cols = np.where(labels == section)\n    print("Image="+str(section))\n    print("Top-Left pixel = {},{}".format(min(rows), min(cols)))\n    print("Bottom-Right pixel = {},{}".format(max(rows), max(cols)))\n    print("---")\n\nImage=0\nTop-Left pixel = 3,1\nBottom-Right pixel = 15,18\n---\nImage=1\nTop-Left pixel = 26,1\nBottom-Right pixel = 34,18\n---\nImage=2\nTop-Left pixel = 43,1\nBottom-Right pixel = 52,16\n---\nImage=3\nTop-Left pixel = 0,0\nBottom-Right pixel = 59,19\n---\n'
'loss = (1-accuracy)\n\nloss = average(prediction_probabilities)\n'
"placeholder1 = tf.placeholder(tf.float32, name='NAME')\nplaceholder2 = tf.get_default_graph().get_tensor_by_name('NAME:0')\nassert placeholder1 == placeholder2\n\ndef get_all_variables_with_name(var_name):\n    name = var_name + ':0'\n    return [var for var in tf.all_variables() if var.name.endswith(name)]\n"
'import tensorflow.contrib.learn as skflow\nfrom sklearn import datasets, metrics\niris = datasets.load_iris()\n# made a change in the next line\nclassifier = skflow.DNNClassifier(hidden_units=[10, 20, 10], n_classes=3)\nclassifier.fit(iris.data, iris.target)\nscore = metrics.accuracy_score(iris.target, classifier.predict(iris.data))\n\nprint("Accuracy: %f" % score)\n'
"model.add(Dense(3))\n\nmodel.add(Dense(10, input_dim=2, activation='relu'))\n"
'image = tf.cast(tf.image.decode_jpeg(input, channels = 3), tf.float32)\nbatch = tf.expand_dims(image, 0);\nresized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])\nnormalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])\n'
'self._vectorizer = vectorizer\n\ninput_counts = self._vectorizer.transform(input_text)\n'
"y1 = tf.stop_gradient(W1x+b1)\ny2 = W2y1+b2\ncost = cost_function(y2, y)\n# this following op wont optimize the cost with respect to W1 and b1\ntrain_op_w2_b2 = tf.train.MomentumOptimizer(0.001, 0.9).minimize(cost)\n\nW1 = tf.get_variable('w1', trainable=False)\ny1 = W1x+b1\ny2 = W2y1+b2\ncost = cost_function(y2, y)\n# this following op wont optimize the cost with respect to W1\ntrain_op = tf.train.MomentumOptimizer(0.001, 0.9).minimize(cost)\n"
"from sklearn.utils.class_weight import compute_sample_weight\ny = [1,1,1,1,0,0,1]\ncompute_sample_weight(class_weight='balanced', y=y)\n\narray([ 0.7 ,  0.7 ,  0.7 ,  0.7 ,  1.75,  1.75,  0.7 ])\n"
"max_size = frame['class'].value_counts().max()\n\nlst = [frame]\nfor class_index, group in frame.groupby('class'):\n    lst.append(group.sample(max_size-len(group), replace=True))\nframe_new = pd.concat(lst)\n"
"In [6]: with tf.variable_scope('myscope') as ms1:\n   ...:   tf.Variable(1.0, name='var1')\n   ...: \n   ...: with tf.variable_scope(ms1.original_name_scope) as ms2:\n   ...:   tf.Variable(2.0, name='var2')\n   ...: \n   ...: print([n.name for n in tf.get_default_graph().as_graph_def().node])\n   ...: \n['myscope/var1/initial_value', \n 'myscope/var1', \n 'myscope/var1/Assign', \n 'myscope/var1/read', \n 'myscope/var2/initial_value', \n 'myscope/var2', \n 'myscope/var2/Assign', \n 'myscope/var2/read']\n\nIn [13]: with tf.variable_scope('myscope'):\n    ...:   tf.Variable(1.0, name='var1')\n    ...: \n    ...: # reuse variable scope by appending `/` to the target variable scope\n    ...: with tf.variable_scope('myscope/', reuse=True):\n    ...:   tf.Variable(2.0, name='var2')\n    ...: \n    ...: print([n.name for n in tf.get_default_graph().as_graph_def().node])\n    ...: \n['myscope/var1/initial_value', \n 'myscope/var1', \n 'myscope/var1/Assign', \n 'myscope/var1/read', \n 'myscope/var2/initial_value', \n 'myscope/var2', \n 'myscope/var2/Assign', \n 'myscope/var2/read']\n"
"X = np.array([[1.,2], [3.,4], [5.,1], [6.,5],[4, 7.],[ 9,8.], [1.,2], [3.,4], [5.,1], [6.,5],[4, 7.],[ 9,8.],[1.,2], [3.,4], [5.,1], [6.,5],[4, 7.],[ 9,8.]])\n\n# rest of your code as is\n\ngp.fit(X, y)\n\n# result:\n\nGaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n             kernel=RBF(length_scale=[1, 2]) * Matern(length_scale=[3, 4], nu=1.5),\n             n_restarts_optimizer=0, normalize_y=False,\n             optimizer='fmin_l_bfgs_b', random_state=None)\n\nkernel = R([1,0]) * M([0,1]) \n"
'v<sub>t+1</sub> = μ.v<sub>t</sub> - λ.∇f(θ<sub>t</sub> + μ.v<sub>t</sub>)\nθ<sub>t+1</sub> = θ<sub>t</sub> + v<sub>t+1</sub>\n\na<sub>t+1</sub> = μ.a<sub>t</sub> - ∇f(θ<sub>t</sub> + μ.λ.a<sub>t</sub>)\nθ<sub>t+1</sub> = θ<sub>t</sub> + λ.a<sub>t+1</sub>\n\na<sub>t+1</sub> = μ.a<sub>t</sub> - ∇f(ψ<sub>t</sub>)\nψ<sub>t+1</sub> = θ<sub>t+1</sub> + μ.λ.a<sub>t+1</sub>\n    = θ<sub>t</sub> + λ.a<sub>t+1</sub> + μ.λ.a<sub>t+1</sub>\n    = ψ<sub>t</sub> + λ.a<sub>t+1</sub> + μ.λ.(a<sub>t+1</sub> - a<sub>t</sub>)\n    = ψ<sub>t</sub> + λ.a<sub>t+1</sub> + μ.λ.[(μ-1)a<sub>t</sub> - ∇f(ψ<sub>t</sub>)]\n    ≈ ψ<sub>t</sub> + λ.a<sub>t+1</sub>\n'
'self.nn_layers = nn.ModuleList()\n'
'import tensorflow as tf\n\nv1 = tf.Variable(0)\nv2 = tf.Variable(0)\nupd1 = tf.assign(v1, v2 + 1)\nupd2 = tf.assign(v2, v1 + 1)\ninit = tf.global_variables_initializer()\n\nfor i in range(10):\n    with tf.Session() as sess:\n        sess.run(init)\n        sess.run([upd1, upd2])\n        print(*sess.run([v1, v2]))\n\nimport tensorflow as tf\n\nv1 = tf.Variable(0)\nv2 = tf.Variable(0)\nupd1 = tf.assign(v1, v2 + 1)\nupd2 = tf.assign(v2, upd1 + 1)\ninit = tf.global_variables_initializer()\n\nimport tensorflow as tf\n\nv1 = tf.Variable(0)\nv2 = tf.Variable(0)\nnew_v1 = v2 + 1\nnew_v2 = v1 + 1\nwith tf.control_dependencies([new_v1, new_v2]):\n    upd1 = tf.assign(v1, new_v1)\n    upd2 = tf.assign(v2, new_v2)\ninit = tf.global_variables_initializer()\n'
'optimizer.zero_grad()\noptimizer.step()\nscheduler.step()\n'
'class Replacer(object):\n    def __call__(self, match):\n        group = match.group(0)\n\n        if group[1:].lower().endswith(\'_nm\'):\n            return \'(?:\' + Matcher(group).regex[1:]\n        else:\n            return \'(?P&lt;\' + group[1:] + \'&gt;\' + Matcher(group).regex[1:]\n\nclass Matcher(object):\n    name_component =    r"([A-Z][A-Za-z|\'|\\-]+|[A-Z][a-z]{2,})"\n    name_component_upper = r"([A-Z][A-Z|\'|\\-]+|[A-Z]{2,})"\n\n    year = r\'(1[89][0-9]{2}|20[0-9]{2})\'\n    year_upper = year\n\n    age = r\'([1-9][0-9]|1[01][0-9])\'\n    age_upper = age\n\n    ordinal = r\'([1-9][0-9]|1[01][0-9])\\s*(?:th|rd|nd|st|TH|RD|ND|ST)\'\n    ordinal_upper = ordinal\n\n    date = r\'((?:{0})\\.? [0-9]{{1,2}}(?:th|rd|nd|st|TH|RD|ND|ST)?,? \\d{{2,4}}|[0-9]{{1,2}} (?:{0}),? \\d{{2,4}}|[0-9]{{1,2}}[\\-/\\.][0-9]{{1,2}}[\\-/\\.][0-9]{{2,4}})\'.format(\'|\'.join(months + months_short) + \'|\' + \'|\'.join(months + months_short).upper())\n    date_upper = date\n\n    matchers = [\n        \'name_component\',\n        \'year\',\n        \'age\',\n        \'ordinal\',\n        \'date\',\n    ]\n\n    def __init__(self, match=\'\'):\n        capitalized = \'_upper\' if match.isupper() else \'\'\n        match = match.lower()[1:]\n\n        if match.endswith(\'_instant\'):\n            match = match[:-8]\n\n        if match in self.matchers:\n            self.regex = getattr(self, match + capitalized)\n        elif len(match) == 1:\n        elif \'year\' in match:\n            self.regex = getattr(self, \'year\')\n        else:\n            self.regex = getattr(self, \'name_component\' + capitalized)\n\nclass Pattern(object):\n    def __init__(self, text=\'\', escape=None):\n        self.text = text\n        self.matchers = []\n\n        escape = not self.text.startswith(\'!\') if escape is None else False\n\n        if escape:\n            self.regex = re.sub(r\'([\\[\\].?+\\-()\\^\\\\])\', r\'\\\\\\1\', self.text)\n        else:\n            self.regex = self.text[1:]\n\n        self.size = len(re.findall(r\'(\\$[A-Za-z0-9\\-_]+)\', self.regex))\n\n        self.regex = re.sub(r\'(\\$[A-Za-z0-9\\-_]+)\', Replacer(), self.regex)\n        self.regex = re.sub(r\'\\s+\', r\'\\\\s+\', self.regex)\n\n    def search(self, text):\n        return re.search(self.regex, text)\n\n    def findall(self, text, max_depth=1.0):\n        results = []\n        length = float(len(text))\n\n        for result in re.finditer(self.regex, text):\n            if result.start() / length &lt; max_depth:\n                results.extend(result.groups())\n\n        return results\n\n    def match(self, text):\n        result = map(lambda x: (x.groupdict(), x.start()), re.finditer(self.regex, text))\n\n        if result:\n            return result\n        else:\n            return []\n\n$LASTNAME, $FirstName $I. said on $date\n'
'from nltk.corpus import wordnet\nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport itertools\n\n\ndef Synonym_Checker(word1, word2):\n    """Checks if word1 and word2 and synonyms. Returns True if they are, otherwise False"""\n    equivalence = WordNetLemmatizer()\n    word1 = equivalence.lemmatize(word1)\n    word2 = equivalence.lemmatize(word2)\n\n    word1_synonyms = wordnet.synsets(word1)\n    word2_synonyms = wordnet.synsets(word2)\n\n    scores = [i.wup_similarity(j) for i, j in list(itertools.product(word1_synonyms, word2_synonyms))]\n    max_index = scores.index(max(scores))\n    best_match = (max_index/len(word1_synonyms), max_index % len(word1_synonyms)-1)\n\n    word1_set = word1_synonyms[best_match[0]].lemma_names\n    word2_set = word2_synonyms[best_match[1]].lemma_names\n    match = False\n    match = [match or word in word2_set for word in word1_set][0]\n\n    return match\n\nprint Synonym_Checker("tomato", "Lycopersicon_esculentum")\n'
'def __init__(self, est):\n   self.est = est\ndef predict(self, X):\n    return self.est.predict_proba(X)[:, 1]\ndef fit(self, X, y):\n    self.est.fit(X, y)\n'
"from nltk.corpora import wordnet as wn\n\nfor synset in list(wn.all_synsets('n')):\n    print synset\n\n# Or, equivalently\nfor synset in list(wn.all_synsets(wn.NOUN)):\n    print synset\n"
'&gt;&gt;&gt; def fromdeg(d):\n...     r = d * np.pi / 180.\n...     return np.array([np.cos(r), np.sin(r)])\n... \n&gt;&gt;&gt; np.linalg.norm(fromdeg(1) - fromdeg(359))\n0.03490481287456796\n&gt;&gt;&gt; np.linalg.norm(fromdeg(1) - fromdeg(180))\n1.9999238461283426\n&gt;&gt;&gt; np.linalg.norm(fromdeg(90) - fromdeg(270))\n2.0\n'
'import numpy as np\n\ny_test = np.asarray(y_test)\nmisclassified = np.where(y_test != clf.predict(X_test))\n'
'# Initialize models\nvectorizer = CountVectorizer(min_df=1)\npca = RandomizedPCA(n_components=50, whiten=True)\nkm = KMeans(n_clusters=2, init=\'random\', n_init=1, verbose=1)\n\n# Fit models\nX = vectorizer.fit_transform(sentences)\nX2 = pca.fit_transform(X)\nkm.fit(X2)\n\n# Predict with models\nX_new = vectorizer.transform(["hello world"])\nX2_new = pca.transform(X_new)\nkm.predict(X2_new)\n'
'What is the best way to handle missing values in data set?\n\nHow to handle missing values in datasets before applying machine learning algorithm??\n'
"import numpy as np\n\nconfusion_matrix = np.array([[2,0,3,4],\n                             [0,4,5,1],\n                             [1,0,3,2],\n                             [5,0,0,4]])\n\ndef process_cm(confusion_mat, i=0, to_print=True):\n    # i means which class to choose to do one-vs-the-rest calculation\n    # rows are actual obs whereas columns are predictions\n    TP = confusion_mat[i,i]  # correctly labeled as i\n    FP = confusion_mat[:,i].sum() - TP  # incorrectly labeled as i\n    FN = confusion_mat[i,:].sum() - TP  # incorrectly labeled as non-i\n    TN = confusion_mat.sum().sum() - TP - FP - FN\n    if to_print:\n        print('TP: {}'.format(TP))\n        print('FP: {}'.format(FP))\n        print('FN: {}'.format(FN))\n        print('TN: {}'.format(TN))\n    return TP, FP, FN, TN\n\nfor i in range(4):\n    print('Calculating 2x2 contigency table for label{}'.format(i))\n    process_cm(confusion_matrix, i, to_print=True)\n\nCalculating 2x2 contigency table for label0\nTP: 2\nFP: 6\nFN: 7\nTN: 19\nCalculating 2x2 contigency table for label1\nTP: 4\nFP: 0\nFN: 6\nTN: 24\nCalculating 2x2 contigency table for label2\nTP: 3\nFP: 8\nFN: 3\nTN: 20\nCalculating 2x2 contigency table for label3\nTP: 4\nFP: 7\nFN: 5\nTN: 18\n"
'from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\n\n# classifier example\nfrom sklearn.svm import SVC\n\npipeline = make_pipeline(Normalizer(), SVC())\n'
'#forward\nactivationPrevious = np.copy(activation)\nskimage.measure.block_reduce(activation, block_size=(1, 1, 2, 2), func=np.max)\nmaxs = activations.repeat(2, axis=2).repeat(2, axis=3)\nmask = np.equal(activationPrevious, maxs).astype(int)\n\n#backward\ndelta = delta.repeat(2, axis=2).repeat(2, axis=3)\ndelta = np.multiply(delta, mask)\n'
'# hidden layer\nweights_hidden = tf.Variable(tf.random_normal([numFeatures, num_nodes])\nbias_hidden = tf.Variable(tf.random_normal([num_nodes])\npreactivations_hidden = tf.add(tf.matmul(X, weights_hidden), bias_hidden)\nactivations_hidden = tf.nn.sigmoid(preactivations_hidden)\n\n# output layer\nweights_output = tf.Variable(tf.random_normal([num_nodes, numLabels])\nbias_output = tf.Variable(tf.random_normal([numLabels]) \npreactivations_output = tf.add(tf.matmul(activations_hidden, weights_output), bias_output)\n'
"sentences = ['hi', 'hello', 'hi hello', 'goodbye', 'bye', 'goodbye bye']\nsentences_split = [s.lower().split(' ') for s in sentences]\n\nimport gensim\nmodel = gensim.models.Word2Vec(sentences_split, min_count=2)\n\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nl = linkage(model.wv.syn0, method='complete', metric='seuclidean')\n\n# calculate full dendrogram\nplt.figure(figsize=(25, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.ylabel('word')\nplt.xlabel('distance')\n\ndendrogram(\n    l,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=16.,  # font size for the x axis labels\n    orientation='left',\n    leaf_label_func=lambda v: str(model.wv.index2word[v])\n)\nplt.show()\n"
'def mape_vectorized(a, b): \n    mask = a &lt;&gt; 0\n    return (np.fabs(a[mask] - b[mask])/a[mask]).mean()\n\ndef mape_vectorized_v2(a, b): \n    mask = a &lt;&gt; 0\n    return (np.fabs(a - b)/a)[mask].mean() \n\nIn [217]: a = np.random.randint(-10,10,(10000))\n     ...: b = np.random.randint(-10,10,(10000))\n     ...: \n\nIn [218]: %timeit mape(a,b)\n100 loops, best of 3: 11.7 ms per loop\n\nIn [219]: %timeit mape_vectorized(a,b)\n1000 loops, best of 3: 273 µs per loop\n\nIn [220]: %timeit mape_vectorized_v2(a,b)\n1000 loops, best of 3: 220 µs per loop\n'
"import numpy as np\ncoefs=logmodel.coef_[0]\ntop_three = np.argpartition(coefs, -3)[-3:]\nprint(cancer.feature_names[top_three])\n\n['worst radius' 'texture error' 'mean radius']\n\nimport numpy as np\ncoefs=logmodel.coef_[0]\ntop_three = np.argpartition(coefs, -3)[-3:]\ntop_three_sorted=top_three[np.argsort(coefs[top_three])]\nprint(cancer.feature_names[top_three_sorted])\n"
'clf.apply(iris.data)\n\ndec_paths = clf.decision_path(iris.data)\n\nfor d, dec in enumerate(dec_paths):\n    for i in range(clf.tree_.node_count):\n        if dec.toarray()[0][i] == 1:\n            samples[i].append(d)\n\nimport sklearn.datasets\nimport sklearn.tree\nimport collections\n\nclf = sklearn.tree.DecisionTreeClassifier(random_state=42)\niris = sklearn.datasets.load_iris()\nclf = clf.fit(iris.data, iris.target)\n\nsamples = collections.defaultdict(list)\ndec_paths = clf.decision_path(iris.data)\n\nfor d, dec in enumerate(dec_paths):\n    for i in range(clf.tree_.node_count):\n        if dec.toarray()[0][i] == 1:\n            samples[i].append(d) \n\nprint(samples[13])\n'
'import tensorflow as tf\n\n\nclass TensorFlowState(object):\n\n    def __init__(self):\n\n        # Get the graph.\n        graph = tf.get_default_graph()\n\n        # Extract the global varibles from the graph.\n        self.gvars = graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n\n        # Exract the Assign operations for later use.\n        self.assign_ops = [graph.get_operation_by_name(v.op.name + "/Assign")\n                           for v in self.gvars]\n\n        # Extract the initial value ops from each Assign op for later use.\n        self.init_values = [op.inputs[1] for op in self.assign_ops]\n\n    def start(self, sess):\n\n        self.sess = sess\n\n    def store(self):\n\n        # Record the current state of the TF global varaibles\n        self.state = self.sess.run(self.gvars)\n\n    def restore(self):\n    # Create a dictionary of the iniailizers and stored state of globals.\n    feed_dict = {init_value: val\n                 for init_value, val in zip(self.init_values, self.state)}\n\n    # Use the initializer ops for each variable to load the stored values.\n    return(self.sess.run(self.assign_ops, feed_dict=feed_dict))\n'
'# keep dim 0 for padding token position encoding zero vector\nposition_enc = np.array([\n    [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n    if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n\nposition_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\nposition_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\nreturn torch.from_numpy(position_enc).type(torch.FloatTensor)\n'
"X = np.array([1,1,1,1], [2,2,2,2], [3,3,3,3], [4,4,4,4]])\ny = np.array([1, 2, 3, 4])\n# Now you create your Kfolds by the way you just have to pass number of splits and if you want to shuffle.\nfold = KFold(2,shuffle=False)\n# For iterate over the folds just use split\nfor train_index, test_index in fold.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    # Follow fitting the classifier\n\nfor i, train_index, test_index in enumerate(fold.split(X)):\n    print('Iteration:', i)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n"
"from keras import applications\nfrom keras.layers import Input\n\n# Loading without top layers, since you only need convolution. Note that by not\n# specifying the shape of top layers, the input tensor shape is (None, None, 3),\n# so you can use them for any size of images.\nvgg_model = applications.VGG16(weights='imagenet', include_top=False)\n\n# If you want to specify input tensor shape, e.g. 256x256 with 3 channels:\ninput_tensor = Input(shape=(256, 256, 3))\nvgg_model = applications.VGG16(weights='imagenet',\n                               include_top=False,\n                               input_tensor=input_tensor)\n\n# To see the models' architecture and layer names, run the following\nvgg_model.summary()\n\n+-----------------+-------------------+\n| VGG conv blocks | U-Net conv blocks |\n+-----------------+-------------------+\n| blockX_conv1    | convN             |\n| ...             | poolN             |\n| blockX_convN    |                   |\n| blockX_pool     |                   |\n+-----------------+-------------------+\n"
'X_train_scaled = scale(X_train)\n\nX_test_scaled = scale(X_test, center=attr(X_train_scaled, "scaled:center"), \n                              scale=attr(X_train_scaled, "scaled:scale"))\n'
'class Siamese(Dataset):\n\n\n    def __init__(self, transform=None):\n    \n       #init data here\n    \n    def __len__(self):\n        return   #length of the data\n\n    def __getitem__(self, idx):\n        #get images and labels here \n        #returned images must be tensor\n        #labels should be int \n        return img1, img2 , label1, label2 \n'
'label_array = label_array.reshape(1, -1)\n'
'Model performance:\n             precision    recall  f1-score   support\n\n        0.0       0.95      0.97      0.96       995\n        1.0       0.96      0.98      0.97      1121\n        2.0       0.91      0.90      0.90      1015\n        3.0       0.90      0.89      0.89      1033\n        4.0       0.93      0.92      0.92       976\n        5.0       0.90      0.88      0.89       884\n        6.0       0.94      0.94      0.94       999\n        7.0       0.92      0.93      0.93      1034\n        8.0       0.89      0.87      0.88       923\n        9.0       0.89      0.90      0.89      1020\n\navg / total       0.92      0.92      0.92     10000\n\nModel performance:\n             precision    recall  f1-score   support\n\n        0.0       0.98      0.98      0.98       995\n        1.0       0.98      0.99      0.99      1121\n        2.0       0.95      0.97      0.96      1015\n        3.0       0.97      0.96      0.96      1033\n        4.0       0.98      0.97      0.97       976\n        5.0       0.97      0.96      0.96       884\n        6.0       0.98      0.98      0.98       999\n        7.0       0.96      0.97      0.97      1034\n        8.0       0.96      0.94      0.95       923\n        9.0       0.96      0.96      0.96      1020\n\navg / total       0.97      0.97      0.97     10000\n\nModel performance:\n             precision    recall  f1-score   support\n\n        0.0       0.99      0.99      0.99       995\n        1.0       0.99      0.99      0.99      1121\n        2.0       0.97      0.98      0.98      1015\n        3.0       0.98      0.97      0.97      1033\n        4.0       0.98      0.97      0.98       976\n        5.0       0.96      0.97      0.97       884\n        6.0       0.99      0.98      0.98       999\n        7.0       0.98      0.98      0.98      1034\n        8.0       0.98      0.97      0.97       923\n        9.0       0.96      0.97      0.96      1020\n\navg / total       0.98      0.98      0.98     10000\n\n#!/usr/bin/env python\n\n"""\nUsing MNIST, compare classification performance of:\n1) logistic regression by itself,\n2) logistic regression on outputs of an RBM, and\n3) logistic regression on outputs of a stacks of RBMs / a DBN.\n"""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_mldata\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.base import clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\n\n\ndef norm(arr):\n    arr = arr.astype(np.float)\n    arr -= arr.min()\n    arr /= arr.max()\n    return arr\n\n\nif __name__ == \'__main__\':\n\n    # load MNIST data set\n    mnist = fetch_mldata(\'MNIST original\')\n    X, Y = mnist.data, mnist.target\n\n    # normalize inputs to 0-1 range\n    X = norm(X)\n\n    # split into train, validation, and test data sets\n    X_train, X_test, Y_train, Y_test = train_test_split(X,       Y,       test_size=10000, random_state=0)\n    X_train, X_val,  Y_train, Y_val  = train_test_split(X_train, Y_train, test_size=10000, random_state=0)\n\n    # --------------------------------------------------------------------------------\n    # set hyperparameters\n\n    learning_rate = 0.02 # from Erhan et el. (2010): median value in grid-search\n    total_units   =  800 # from Erhan et el. (2010): optimal for MNIST / only slightly worse than 1200 units when using InfiniteMNIST\n    total_epochs  =   50 # from Erhan et el. (2010): optimal for MNIST\n    batch_size    =  128 # seems like a representative sample; backprop literature often uses 256 or 512 samples\n\n    C = 100. # optimum for benchmark model according to sklearn docs: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py)\n\n    # TODO optimize using grid search, etc\n\n    # --------------------------------------------------------------------------------\n    # construct models\n\n    # RBM\n    rbm = BernoulliRBM(n_components=total_units, learning_rate=learning_rate, batch_size=batch_size, n_iter=total_epochs, verbose=1)\n\n    # "output layer"\n    logistic = LogisticRegression(C=C, solver=\'lbfgs\', multi_class=\'multinomial\', max_iter=200, verbose=1)\n\n    models = []\n    models.append(Pipeline(steps=[(\'logistic\', clone(logistic))]))                                              # base model / benchmark\n    models.append(Pipeline(steps=[(\'rbm1\', clone(rbm)), (\'logistic\', clone(logistic))]))                        # single RBM\n    models.append(Pipeline(steps=[(\'rbm1\', clone(rbm)), (\'rbm2\', clone(rbm)), (\'logistic\', clone(logistic))]))  # RBM stack / DBN\n\n    # --------------------------------------------------------------------------------\n    # train and evaluate models\n\n    for model in models:\n        # train\n        model.fit(X_train, Y_train)\n\n        # evaluate using validation set\n        print("Model performance:\\n%s\\n" % (\n            classification_report(Y_val, model.predict(X_val))))\n\n    # TODO: after parameter optimization, evaluate on test set\n'
"from tensorflow.keras import layers, Model, utils, optimizers\n\n#Encoder\nenc = layers.Input((99,))\nx = layers.Dense(128, activation='relu')(enc)\nx = layers.Dense(56, activation='relu')(x)\nx = layers.Dense(8, activation='relu')(x) #Compression happens here\n\n#Decoder\nx = layers.Dense(8, activation='relu')(x)\nx = layers.Dense(56, activation='relu')(x)\nx = layers.Dense(28, activation='relu')(x)\ndec = layers.Dense(99)(x)\n\nmodel = Model(enc, dec)\n\nopt = optimizers.Adam(learning_rate=0.01)\n\nmodel.compile(optimizer = opt, loss = 'MSE')\n\nmodel.fit(x_train, y_train, epochs = 20)\n\nfrom tensorflow.keras import layers, Model, utils, optimizers\n\n#Encoder with conv1d\ninp = layers.Input((99,))\nx = layers.Reshape((99,1))(inp)\nx = layers.Conv1D(5, 10)(x)\nx = layers.MaxPool1D(10)(x)\nx = layers.Flatten()(x)\nx = layers.Dense(4, activation='relu')(x) #&lt;- Bottleneck!\n\n#Decoder with Deconv1d\nx = layers.Reshape((-1,1))(x)\nx = layers.Conv1DTranspose(5, 10)(x)\nx = layers.Conv1DTranspose(2, 10)(x)\nx = layers.Flatten()(x)\nout = layers.Dense(99)(x)\n\nmodel = Model(inp, out)\n\nopt = optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer = opt, loss = 'MSE')\nmodel.fit(x_train, y_train, epochs = 10, validation_data=(x_test, y_test))\n\nEpoch 1/10\n188/188 [==============================] - 1s 7ms/step - loss: 2.1205 - val_loss: 0.0031\nEpoch 2/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0032 - val_loss: 0.0032\nEpoch 3/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0032 - val_loss: 0.0030\nEpoch 4/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0031 - val_loss: 0.0029\nEpoch 5/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0030 - val_loss: 0.0030\nEpoch 6/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0029 - val_loss: 0.0027\nEpoch 7/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0028 - val_loss: 0.0029\nEpoch 8/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0028 - val_loss: 0.0025\nEpoch 9/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0028 - val_loss: 0.0025\nEpoch 10/10\n188/188 [==============================] - 1s 5ms/step - loss: 0.0026 - val_loss: 0.0024\n\nutils.plot_model(model, show_layer_names=False, show_shapes=True)\n"
'theta0 = theta0 - step * dEdtheta0\ntheta1 = theta1 - step * dEdtheta1\ntheta2 = theta2 - step * dEdtheta2\n\nn = max( [ dEdtheta1, dEdtheta1, dEdtheta2 ] )    \ntheta0 = theta0 - step * dEdtheta0 / n\ntheta1 = theta1 - step * dEdtheta1 / n\ntheta2 = theta2 - step * dEdtheta2 / n\n'
'&gt;&gt;&gt; import numpy as NP\n&gt;&gt;&gt; # just made up values--based on your spec (2D data + 2 clusters)\n&gt;&gt;&gt; centroids\n      array([[54, 85],\n             [99, 78]])\n\n&gt;&gt;&gt; # randomly generate a new data point within the problem domain:\n&gt;&gt;&gt; new_data = NP.array([67, 78])\n\n&gt;&gt;&gt; # to assign a new data point to a cluster ID,\n&gt;&gt;&gt; # find its closest centroid:\n&gt;&gt;&gt; diff = centroids - new_data[0,:]  # NumPy broadcasting\n&gt;&gt;&gt; diff\n      array([[-13,   7],\n             [ 32,   0]])\n\n&gt;&gt;&gt; dist = NP.sqrt(NP.sum(diff**2, axis=-1))  # Euclidean distance\n&gt;&gt;&gt; dist\n      array([ 14.76,  32.  ])\n\n&gt;&gt;&gt; closest_centroid = centroids[NP.argmin(dist),]\n&gt;&gt;&gt; closest_centroid\n       array([54, 85])\n'
'buildNetwork(INPUTS, HIDDEN, OUTPUTS, hiddenclass=LSTMLayer, outclass=SigmoidLayer, recurrent=True, bias=True)\n'
"import pydot\n\nmenu = {'dinner':\n            {'chicken':'good',\n             'beef':'average',\n             'vegetarian':{\n                   'tofu':'good',\n                   'salad':{\n                            'caeser':'bad',\n                            'italian':'average'}\n                   },\n             'pork':'bad'}\n        }\n\ndef draw(parent_name, child_name):\n    edge = pydot.Edge(parent_name, child_name)\n    graph.add_edge(edge)\n\ndef visit(node, parent=None):\n    for k,v in node.iteritems():\n        if isinstance(v, dict):\n            # We start with the root node whose parent is None\n            # we don't want to graph the None node\n            if parent:\n                draw(parent, k)\n            visit(v, k)\n        else:\n            draw(parent, k)\n            # drawing the label using a distinct name\n            draw(k, k+'_'+v)\n\ngraph = pydot.Dot(graph_type='graph')\nvisit(menu)\ngraph.write_png('example1_graph.png')\n"
'max_features : optional, None by default\n    If not None, build a vocabulary that only consider the top\n    max_features ordered by term frequency across the corpus.\n\nemoticons = {":)":0, ":P":1, ":(":2}\nvect = TfidfVectorizer(vocabulary=emoticons)\nmatrix = vect.fit_transform(traindata)\n\n# calculate the most frequent features first\nvect = TfidfVectorizer(vocabulary=emoticons, max_features=10)\nmatrix = vect.fit_transform(traindata)\ntop_features = vect.vocabulary_\nn = len(top_features)\n\n# insert the emoticons into the vocabulary of common features\nemoticons = {":)":0, ":P":1, ":(":2)}\nfor feature, index in emoticons.items():\n    top_features[feature] = n + index\n\n# re-vectorize using both sets of features\n# at this point len(top_features) == 13\nvect = TfidfVectorizer(vocabulary=top_features)\nmatrix = vect.fit_transform(traindata)\n'
'$ python2.7 phrase_extract_new.py \n( 1) (0, 1) michael — michael\n( 2) (0, 2) michael assumes — michael geht davon aus ; michael geht davon aus ,\n( 3) (0, 3) michael assumes that — michael geht davon aus , dass\n( 4) (0, 4) michael assumes that he — michael geht davon aus , dass er\n( 5) (0, 9) michael assumes that he will stay in the house — michael geht davon aus , dass er im haus bleibt\n( 6) (1, 2) assumes — geht davon aus ; geht davon aus ,\n( 7) (1, 3) assumes that — geht davon aus , dass\n( 8) (1, 4) assumes that he — geht davon aus , dass er\n( 9) (1, 9) assumes that he will stay in the house — geht davon aus , dass er im haus bleibt\n(10) (2, 3) that — dass ; , dass\n(11) (2, 4) that he — dass er ; , dass er\n(12) (2, 9) that he will stay in the house — dass er im haus bleibt ; , dass er im haus bleibt\n(13) (3, 4) he — er\n(14) (3, 9) he will stay in the house — er im haus bleibt\n(15) (4, 6) will stay — bleibt\n(16) (4, 9) will stay in the house — im haus bleibt\n(17) (6, 8) in the — im\n(18) (6, 9) in the house — im haus\n(19) (8, 9) house — haus\n$ python2.7 phrase_extract_new.py | grep -c \';\'\n5\n\n# -*- coding: utf-8 -*-\n\ndef phrase_extraction(srctext, trgtext, alignment):\n    """\n    Phrase extraction algorithm.\n    """\n    def extract(f_start, f_end, e_start, e_end):\n        if f_end &lt; 0:  # 0-based indexing.\n            return {}\n        # Check if alignement points are consistent.\n        for e,f in alignment:\n            if ((f_start &lt;= f &lt;= f_end) and\n               (e &lt; e_start or e &gt; e_end)):\n                return {}\n\n        # Add phrase pairs (incl. additional unaligned f)\n        # Remark:  how to interpret "additional unaligned f"?\n        phrases = set()\n        fs = f_start\n        # repeat-\n        while True:\n            fe = f_end\n            # repeat-\n            while True:\n                # add phrase pair ([e_start, e_end], [fs, fe]) to set E\n                # Need to +1 in range  to include the end-point.\n                src_phrase = " ".join(srctext[i] for i in range(e_start,e_end+1))\n                trg_phrase = " ".join(trgtext[i] for i in range(fs,fe+1))\n                # Include more data for later ordering.\n                phrases.add(((e_start, e_end+1), src_phrase, trg_phrase))\n                fe += 1 # fe++\n                # -until fe aligned or out-of-bounds\n                if fe in f_aligned or fe == trglen:\n                    break\n            fs -=1  # fe--\n            # -until fs aligned or out-of- bounds\n            if fs in f_aligned or fs &lt; 0:\n                break\n        return phrases\n\n    # Calculate no. of tokens in source and target texts.\n    srctext = srctext.split()   # e\n    trgtext = trgtext.split()   # f\n    srclen = len(srctext)       # len(e)\n    trglen = len(trgtext)       # len(f)\n    # Keeps an index of which source/target words are aligned.\n    e_aligned = [i for i,_ in alignment]\n    f_aligned = [j for _,j in alignment]\n\n    bp = set() # set of phrase pairs BP\n    # for e start = 1 ... length(e) do\n    # Index e_start from 0 to len(e) - 1\n    for e_start in range(srclen):\n        # for e end = e start ... length(e) do\n        # Index e_end from e_start to len(e) - 1\n        for e_end in range(e_start, srclen):\n            # // find the minimally matching foreign phrase\n            # (f start , f end ) = ( length(f), 0 )\n            # f_start ∈ [0, len(f) - 1]; f_end ∈ [0, len(f) - 1]\n            f_start, f_end = trglen-1 , -1  #  0-based indexing\n            # for all (e,f) ∈ A do\n            for e,f in alignment:\n                # if e start ≤ e ≤ e end then\n                if e_start &lt;= e &lt;= e_end:\n                    f_start = min(f, f_start)\n                    f_end = max(f, f_end)\n            # add extract (f start , f end , e start , e end ) to set BP\n            phrases = extract(f_start, f_end, e_start, e_end)\n            if phrases:\n                bp.update(phrases)\n    return bp\n\n# Refer to match matrix.\n#             0      1      2   3  4     5   6   7    8\nsrctext = "michael assumes that he will stay in the house"\n#             0      1    2    3  4  5   6  7   8     9\ntrgtext = "michael geht davon aus , dass er im haus bleibt"\nalignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9), (5,9), (6,7), (7,7), (8,8)]\n\nphrases = phrase_extraction(srctext, trgtext, alignment)\n\n# Keep track of translations of each phrase in srctext and its\n# alignement using a dictionary with keys as phrases and values being\n# a list [e_alignement pair, [f_extractions, ...] ]\ndlist = {}\nfor p, a, b in phrases:\n    if a in dlist:\n        dlist[a][1].append(b)\n    else:\n        dlist[a] = [p, [b]]\n\n# Sort the list of translations based on their length.  Shorter phrases first.\nfor v in dlist.values():\n    v[1].sort(key=lambda x: len(x))\n\n\n# Function to help sort according to book example.\ndef ordering(p):\n    k,v = p\n    return v[0]\n#\nfor i, p in enumerate(sorted(dlist.items(), key = ordering), 1):\n    k, v = p\n    print "({0:2}) {1} {2} — {3}".format( i, v[0], k, " ; ".join(v[1]))\n'
"classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state))\n\nclassifier = OneVsRestClassifier(AdaBoostClassifier())\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Add noisy features to make the problem harder\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\nrandom_state = 0\n"
'from sklearn.datasets import make_classification\nfrom sklearn.cross_validation import cross_val_predict\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\n\n# generate some artificial data with 11 classes\nX, y = make_classification(n_samples=2000, n_features=20, n_informative=10, n_classes=11, random_state=0)\n\n# your classifier, assume GaussianNB here for non-integer data X\nestimator = GaussianNB()\n# generate your cross-validation prediction with 10 fold Stratified sampling\ny_pred = cross_val_predict(estimator, X, y, cv=10)\ny_pred.shape\n\nOut[91]: (2000,)\n\n# generate report\nprint(classification_report(y, y_pred))\n\n             precision    recall  f1-score   support\n\n          0       0.47      0.36      0.41       181\n          1       0.38      0.46      0.41       181\n          2       0.45      0.53      0.48       182\n          3       0.29      0.45      0.35       183\n          4       0.37      0.33      0.35       183\n          5       0.40      0.44      0.42       182\n          6       0.27      0.13      0.17       183\n          7       0.47      0.44      0.45       182\n          8       0.34      0.27      0.30       182\n          9       0.41      0.44      0.42       179\n         10       0.42      0.41      0.41       182\n\navg / total       0.39      0.39      0.38      2000\n'
'from sklearn.decomposition import IncrementalPCA\nimport csv\nimport sys\nimport numpy as np\nimport pandas as pd\n\ndataset = sys.argv[1]\nchunksize_ = 5 * 25000\ndimensions = 300\n\nreader = pd.read_csv(dataset, sep = \',\', chunksize = chunksize_)\nsklearn_pca = IncrementalPCA(n_components=dimensions)\nfor chunk in reader:\n    y = chunk.pop("Y")\n    sklearn_pca.partial_fit(chunk)\n\n# Computed mean per feature\nmean = sklearn_pca.mean_\n# and stddev\nstddev = np.sqrt(sklearn_pca.var_)\n\nXtransformed = None\nfor chunk in pd.read_csv(dataset, sep = \',\', chunksize = chunksize_):\n    y = chunk.pop("Y")\n    Xchunk = sklearn_pca.transform(chunk)\n    if Xtransformed == None:\n        Xtransformed = Xchunk\n    else:\n        Xtransformed = np.vstack((Xtransformed, Xchunk))\n'
'tfidf = TfidfVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x)\ntfidf_matrix = tfidf.fit_transform(corpus)\n'
'max_key = env.stat()["entries"]\n\nmax_key = 0\nfor key, value in env.cursor():\n    max_key = max(max_key, key)\n\nstr_id = \'{:08}\'.format(i)\n\nstr_id = \'{:08}\'.format(max_key + 1 + i)\n'
'In [112]: a\nOut[112]: \narray([[ 0.2,  0.3,  0.5],\n       [ 0.7,  0.1,  0.1]])\n\nIn [113]: a == a.max(axis=1, keepdims=True)\nOut[113]: \narray([[False, False,  True],\n       [ True, False, False]], dtype=bool)\n\nIn [114]: (a == a.max(axis=1, keepdims=True)).astype(int)\nOut[114]: \narray([[0, 0, 1],\n       [1, 0, 0]])\n'
'x = x if predict else X[0]\n\nx = x if predict else X[i]\n\ndef s_prime(z):\n    return np.multiply(sigmoid(z), sigmoid(1.0-z))\n\ndef s_prime(z):\n    return np.multiply(z, 1.0-z)\n\n[[0 0]] : [[ 0.00239857]]\n[[0 1]] : [[ 0.99816778]]\n[[1 0]] : [[ 0.99816596]]\n[[1 1]] : [[ 0.0021052]]\n\n[[0 0]] : [[ 0.03029435]]\n[[0 1]] : [[ 0.95397528]]\n[[1 0]] : [[ 0.95371525]]\n[[1 1]] : [[ 0.04796917]]\n'
'sample = X[np.argmin(np.abs(clf.decision_function(X)))] \n'
'def feature_normalize(train_X):\n\n    global mean, std\n    mean = np.mean(train_X, axis=0)\n    std = np.std(train_X, axis=0)\n\n    return (train_X - mean) / std\n\npredict_X = (predict_X - mean)/std\n'
'tf.reset_default_graph()\na = tf.Variable(tf.ones_initializer(()))\ninit_op = tf.initialize_all_variables()\nmodify_op = a.assign(5.0)\n\nsess = tf.InteractiveSession()\nsess.run(init_op)\nprint(a.eval())\nsess.run(modify_op)\nprint(a.eval())\nsess.run(init_op)\nprint(a.eval())\n\n1.0\n5.0\n1.0\n'
'from PIL import Image\nnumber_of_batches = len(names)/ batch_size\nfor i in range(number_of_batches):\n     batch_x = names[i*batch_size:i*batch_size+batch_size]\n     batch_y = labels[i*batch_size:i*batch_size+batch_size]\n     batch_image_data = np.empty([batch_size, image_height, image_width, image_depth], dtype=np.int)\n     for ix in range(len(batch_x)):\n        f = batch_x[ix]\n        batch_image_data[ix] = np.array(Image.open(data_dir+f))\n     sess.run(train_op, feed_dict={xs:batch_image_data, ys:batch_y})\n'
'w = tf.zeros([748,10])\nb = tf.zeros([10])\n\nw = tf.Variable(tf.zeros([748,10]))\nb = tf.Variable(tf.zeros([10]))\n'
"p1 = (-((X[:,None] - Y)**2)/c1)-c2\np11 = p1.sum(2)\np2 = np.exp(p11+c3)\nout = np.log(p2.sum(0)).mean()\n\nc10 = -c1\nc20 = X.shape[1]*c2\n\nsubs = (X[:,None] - Y)**2\np00 = subs.sum(2)\np10 = p00/c10\np11 = p10-c20\np2 = np.exp(p11+c3)\nout = np.log(p2.sum(0)).mean()\n\nfrom scipy.spatial.distance import cdist\n\n# Setup constants\nc10 = -c1\nc20 = X.shape[1]*c2\nc30 = c20-c3\nc40 = np.exp(c30)\nc50 = np.log(c40)\n\n# Get stagewise operations corresponding to loopy ones\np1 = cdist(X, Y, 'sqeuclidean')\np2 = np.exp(p1/c10).sum(0)\nout = np.log(p2).mean() - c50\n\ndef loopy_app(X, Y, sigma):\n    k, d = X.shape\n    m = Y.shape[0]\n\n    c1 = 2.0*sigma**2\n    c2 = 0.5*np.log(np.pi*c1)\n    c3 = np.log(1.0/k)\n\n    L_B = np.zeros((m,))\n    for i in xrange(m):\n        L_B[i] = np.log(np.sum(np.exp(np.sum(-np.divide(\n                    np.power(X-Y[i,:],2), c1)-c2,1)+c3)))\n\n    return np.mean(L_B)\n\ndef vectorized_app(X, Y, sigma):\n    # Setup constants\n    k, d = D_A.shape\n    c1 = 2.0*sigma**2\n    c2 = 0.5*np.log(np.pi*c1)\n    c3 = np.log(1.0/k)\n\n    c10 = -c1\n    c20 = X.shape[1]*c2\n    c30 = c20-c3\n    c40 = np.exp(c30)\n    c50 = np.log(c40)\n\n    # Get stagewise operations corresponding to loopy ones\n    p1 = cdist(X, Y, 'sqeuclidean')\n    p2 = np.exp(p1/c10).sum(0)\n    out = np.log(p2).mean() - c50\n    return out\n\nIn [294]: # Setup inputs with m(=D_B.shape[0]) being a large number\n     ...: X = np.random.randint(0,9,(100,10))\n     ...: Y = np.random.randint(0,9,(10000,10))\n     ...: sigma = 2.34\n     ...: \n\nIn [295]: np.allclose(loopy_app(X, Y, sigma),vectorized_app(X, Y, sigma))\nOut[295]: True\n\nIn [296]: %timeit loopy_app(X, Y, sigma)\n1 loops, best of 3: 225 ms per loop\n\nIn [297]: %timeit vectorized_app(X, Y, sigma)\n10 loops, best of 3: 23.6 ms per loop\n\nIn [298]: # Setup inputs with m(=Y.shape[0]) being a much large number\n     ...: X = np.random.randint(0,9,(100,10))\n     ...: Y = np.random.randint(0,9,(100000,10))\n     ...: sigma = 2.34\n     ...: \n\nIn [299]: np.allclose(loopy_app(X, Y, sigma),vectorized_app(X, Y, sigma))\nOut[299]: True\n\nIn [300]: %timeit loopy_app(X, Y, sigma)\n1 loops, best of 3: 2.27 s per loop\n\nIn [301]: %timeit vectorized_app(X, Y, sigma)\n1 loops, best of 3: 243 ms per loop\n"
"def my_init(shape, dtype=None):\n    # Note it must take arguments 'shape' and 'dtype'.\n    return my_constant_weight_matrix\nmodel.add(Conv2D(..., kernel_initializer=my_init))  # replace '...' with your args\n"
'image_path = os.path.join(data_path, dataset, img)\nif os.path.exist():\n    # Do stuff\n'
'#df contains the data above.\n\n#generate a linear model (note that \'Vehicle\' is not numerical)\nmd &lt;- lm(data=df, Miles.per.Gal ~ Load + Vehicle)\n\n#generate predictions based on the model; for this illustration, plotting only for \'Tundra\' \nnewx &lt;- seq(min(df$Load), max(df$Load), length.out=100)\npreds_df &lt;- as.data.frame(predict(md, newdata = data.frame(Load=newx, model="Tundra"))\n\n#plot\n# fit + confidence\nplt &lt;- ggplot(data=preds_df) + geom_line(aes(x=x, y=fit)) + geom_ribbon(aes(x = x, ymin=lwr, ymax=upr), alpha=0.3) \n# points for illustration \nplt + geom_point(aes(x=1100, y=7.8), color="red", size=4) +geom_point(aes(x=1300, y=4), color="blue", size=4) + geom_point(aes(x=1400, y=9), color="green", size=4)   \n'
'spike = tf.Variable(False)\n\nspike.initializer.run()\n\nimport tensorflow.contrib.eager as tfe\ntfe.enable_eager_execution()\n'
'from keras.layers import multiply\noutput = multiply([dense_all, dense_att])\n'
'Train on 60000 samples, validate on 10000 samples\nEpoch 1/3\n60000/60000 [==============================] - 15s 251us/step - loss: 0.2180 - acc: 0.9324 - val_loss: 0.1072 - val_acc: 0.9654\nEpoch 2/3\n60000/60000 [==============================] - 15s 246us/step - loss: 0.0831 - acc: 0.9743 - val_loss: 0.0719 - val_acc: 0.9788\nEpoch 3/3\n60000/60000 [==============================] - 15s 245us/step - loss: 0.0526 - acc: 0.9837 - val_loss: 0.0997 - val_acc: 0.9723\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/3\n60000/60000 [==============================] - 16s 265us/step - loss: 3.4344 - acc: 0.1064 - val_loss: 2.3008 - val_acc: 0.1136\nEpoch 2/3\n60000/60000 [==============================] - 16s 261us/step - loss: 2.3342 - acc: 0.1112 - val_loss: 2.3010 - val_acc: 0.1135\nEpoch 3/3\n60000/60000 [==============================] - 16s 266us/step - loss: 2.3167 - acc: 0.1122 - val_loss: 2.3010 - val_acc: 0.1135\n'
'1 11 21 31\n2 12 22 32\n3 13 23 33\n...\n100 111 121 131\n\n1 2 3 ... 100\n11 12 13 ... 111\n21 22 23 ... 121\n31 32 33 ... 131\n'
'# Do all of the following once for training data, AND once for validation data    \ntot_num_labels = 0\nfor image in images:\n    tot_num_labels += len(image.get_all_labels())\navg_labels_per_image = tot_num_labels / float(num_images)\nprint("Avg. num labels per image = ", avg_labels_per_image)\n\nfor label in range(num_labels):\n    tot_shared_labels = 0\n    for image in images_with_label(label):\n        tot_shared_labels += (len(image.get_all_labels()) - 1)\n    avg_shared_labels = tot_shared_labels / float(len(images_with_label(label)))\n    print("On average, images with label ", label, " also have ", avg_shared_labels, " other labels.")\n'
"def custom_metric(y_true,y_pred):\n     result = K.abs((y_true-y_pred) / y_pred, axis = 1)\n     return result\n\nhistory = CustomLossHistory()\nmodel.fit(callbacks = [history])\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.errors= []\n\n    def on_batch_end(self, batch, logs={}):\n         loss = logs.get('loss')\n         self.errors.append(self.loss_mapper(loss))\n\n    def loss_mapper(self, loss):\n         if loss &lt;= 0.1:\n             return 0\n         elif loss &gt; 0.1 &amp; loss &lt;= 0.15:\n             return 5/3\n         elif loss &gt; 0.15 &amp; loss &lt;= 0.2:\n             return 5\n         else:\n             return 2000\n\nerrors = history.errors\n"
"tiles = sorted(glob.glob(os.path.join(inws, '*.tif')))\n"
'lstm1 = LSTM(256, return_sequences=True, name=\'lstm1\')\nlstm2 = LSTM(256, return_sequences=False, name=\'lstm2\')\ndense = Dense(NUM_OF_LABELS, name=\'Susie Dense\')\n\ndef foo(...):\n    sentence_indices = Input(input_shape, dtype="int32")\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    embeddings = embedding_layer(sentence_indices)\n    X = lstm1(embeddings)\n    X = Dropout(0.5)(X)\n    X = lstm2(X)\n    X = Dropout(0.5)(X)    \n    X = dense(X)\n    X = Activation("softmax")(X)\n    return Model(inputs=sentence_indices, outputs=X)\n\n\ndef bar(...):\n    embeddings = Input(embedding_shape, dtype="float32")\n    X = lstm1(embeddings)\n    X = Dropout(0.5)(X)\n    X = lstm2(X)\n    X = Dropout(0.5)(X)    \n    X = dense(X)\n    X = Activation("softmax")(X)\n    return Model(inputs=sentence_indices, outputs=X)\n\nfoo_model = foo(...)\nbar_model = bar(...)\n\nfoo_model.fit(...)\nbar_model.save_weights(...)\n\nfoo_model.load_weights(\'bar_model.h5\', by_name=True)\n'
"model = Sequential()\nmodel.add(Convolution1D(filters=16, kernel_size=35, activation='relu', input_shape=(1, 100), data_format='channels_first'))\nmodel.add(Permute((2, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Convolution1D(filters=16, kernel_size=10, activation='relu'))\n\nmodel.summary()\n\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_7 (Conv1D)            (None, 16, 66)            576       \n_________________________________________________________________\npermute_1 (Permute)          (None, 66, 16)            0         \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 13, 16)            0         \n_________________________________________________________________\nconv1d_8 (Conv1D)            (None, 4, 16)              2096      \n=================================================================\nTotal params: 2,672\nTrainable params: 2,672\nNon-trainable params: 0\n_________________________________________________________________\n"
'def generator_model_v2():\n    input_image = Input((IN_CH, img_cols, img_rows)) \n    input_conditional = Input((n_classes))  \n    e0 = Flatten()(input_image) \n    e1 = Concatenate()([e0, input_conditional])   \n    e2 = BatchNormalization(mode=0)(e1)\n    e3 = BatchNormalization(mode=0)(e2)\n    e4 = Dense(1024, activation="relu")(e3)\n    e5 = BatchNormalization(mode=0)(e4)\n    e6 = Dense(512, activation="relu")(e5)\n    e7 = BatchNormalization(mode=0)(e6)\n    e8 = Dense(512, activation="relu")(e7)\n    e9 = BatchNormalization(mode=0)(e8)\n    e10 = Dense(IN_CH * img_cols *img_rows, activation="relu")(e9)\n    e11  = Reshape((3, 28, 28))(e10)\n    e12 = BatchNormalization(mode=0)(e11)\n    e13 = Activation(\'tanh\')(e12)\n\n    model = Model(input=[input_image, input_conditional] , output=e13)\n    return model\n\nclassifier.train_on_batch((image_batch, class_batch), label_batch)\n'
'idx = 3  # index of desired layer\ninput_shape = model.layers[idx].get_input_shape_at(0) # get the input shape of desired layer\nlayer_input = Input(shape=input_shape) # a new input tensor to be able to feed the desired layer\n\n# create the new nodes for each layer in the path\nx = layer_input\nfor layer in model.layers[idx:]:\n    x = layer(x)\n\n# create the model\nnew_model = Model(layer_input, x)\n'
'Train on 0 samples, validate on 1 samples\n'
"from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom graphviz import Source\n\ndata = load_iris()\nX, y = data.data, data.target\n\nclf = DecisionTreeClassifier(max_depth=2, random_state=42)\nclf.fit(X, y)\n\ngraph = Source(export_graphviz(clf, out_file=None, feature_names=data.feature_names))\ngraph.format = 'png'\ngraph.render('dt', view=True);\n\nclf.tree_.impurity\narray([0.66666667, 0.        , 0.5       , 0.16803841, 0.04253308])\n"
"round(exp(uniform(low, high)) / q) * q \n\nfrom hyperopt import pyll, hp\nn_samples = 10\n\nspace = hp.loguniform('x', np.log(0.001), np.log(0.1))\nevaluated = [pyll.stochastic.sample(space) for _ in range(n_samples)]\n# Output: [0.04645754, 0.0083128 , 0.04931957, 0.09468335, 0.00660693,\n#          0.00282584, 0.01877195, 0.02958924, 0.00568617, 0.00102252]\n\nq = 0.005\nqevaluated = np.round(np.array(evaluated)/q) * q\n# Output: [0.045, 0.01 , 0.05 , 0.095, 0.005, 0.005, 0.02 , 0.03 , 0.005, 0.])\n\nnp.log(0.001)\n# Output: -6.907755278982137\n\nq = np.log(0.001)\nqevaluated = np.round(np.array(evaluated)/q) * q\n# Output: [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]\n"
'class LSTM(nn.Module):\n    def __init__(self, input_dim, latent_dim, num_layers):\n        super(LSTM, self).__init__()\n        self.input_dim = input_dim\n        self.latent_dim = latent_dim\n        self.num_layers = num_layers\n\n        self.encoder = nn.LSTM(self.input_dim, self.latent_dim, self.num_layers)\n\n        self.decoder = nn.LSTM(self.latent_dim, self.input_dim, self.num_layers)\n\n    def forward(self, input):\n        # Encode\n        _, (last_hidden, _) = self.encoder(input)\n        encoded = last_hidden.repeat(5, 1, 1)\n\n        # Decode\n        y, _ = self.decoder(encoded)\n        return torch.squeeze(y)\n\nmodel = LSTM(input_dim=1, latent_dim=20, num_layers=1)\nloss_function = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ny = torch.Tensor([[0.0], [0.1], [0.2], [0.3], [0.4]])\n# Sequence x batch x dimension\nx = y.view(len(y), 1, -1)\n\nwhile True:\n    y_pred = model(x)\n    optimizer.zero_grad()\n    loss = loss_function(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    print(y_pred)\n\n# batch_size and hidden_size should be inferred cluttering the code further    \nencoded[-1].view(batch_size, 2, hidden_size).sum(dim=1)\n\n# Your output\ntorch.Size([5, 1, 1])\n# Your target\ntorch.Size([5, 1])\n\nmodel = LSTM(input_dim=1, latent_dim=20, num_layers=1)\nloss_function = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\ny = torch.Tensor([0.0, 0.1, 0.2, 0.3, 0.4])\nx = y.view(len(y), 1, -1)\n\nwhile True:\n    y_pred = model(x)\n    optimizer.zero_grad()\n    loss = loss_function(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    print(y_pred)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nclass LSTM(nn.Module):\n    def __init__(self, input_dim, latent_dim, num_layers):\n        super(LSTM, self).__init__()\n        self.input_dim = input_dim\n        self.latent_dim = latent_dim\n        self.num_layers = num_layers\n\n        self.encoder = nn.LSTM(self.input_dim, self.latent_dim, self.num_layers)\n\n        self.decoder = nn.LSTM(self.latent_dim, self.input_dim, self.num_layers)\n\n    def forward(self, input):\n        # Encode\n        _, (last_hidden, _) = self.encoder(input)\n        # It is way more general that way\n        encoded = last_hidden.repeat(input.shape)\n\n        # Decode\n        y, _ = self.decoder(encoded)\n        return torch.squeeze(y)\n\n\nmodel = LSTM(input_dim=1, latent_dim=20, num_layers=1)\nloss_function = nn.MSELoss()\noptimizer = optim.Adam(model.parameters())\n\ny = torch.Tensor([0.0, 0.1, 0.2, 0.3, 0.4])\nx = y.view(len(y), 1, -1)\n\nwhile True:\n    y_pred = model(x)\n    optimizer.zero_grad()\n    loss = loss_function(y_pred, y)\n    loss.backward()\n    optimizer.step()\n    print(y_pred)\n\nstep=59682                       \ntensor([0.0260, 0.0886, 0.1976, 0.3079, 0.3962], grad_fn=&lt;SqueezeBackward0&gt;)\n\nstep=10645                        \ntensor([0.0405, 0.1049, 0.1986, 0.3098, 0.4027], grad_fn=&lt;SqueezeBackward0&gt;)\n'
"Classification metrics can't handle a mix of binary and continuous target\n"
'v = model(s)\nv.backward()\n\nfor i, p in enumerate(model.parameters()):\n    z_theta[i][:] = gamma * lamda * z_theta[i] + l * p.grad\n    p.grad[:] = alpha * delta * z_theta[i]\n'
'exporter = tf.estimator.BestExporter(\n      compare_fn=_loss_smaller,\n      exports_to_keep=5)\n\neval_spec = tf.estimator.EvalSpec(\n    input_fn,\n    steps,\n    exporters)\n'
"model = XGBClassifier()\nmodel.fit(x_train, y_train)\n ...\nimport m2cgen as m2c\n\nwith open('./model.c','w') as f:\n    code = m2c.export_to_c(model)\n    f.write(code)\n"
"def f_True(x):\n    # Compute Bump Function\n    bump_value = 1-tf.math.pow(x,2)\n    bump_value = -tf.math.pow(bump_value,-1)\n    bump_value = tf.math.exp(bump_value)\n    return(bump_value)\n\ndef f_False(x):\n    # Compute Bump Function\n    x_out = 0*x\n    return(x_out)\n\nimport tensorflow as tf\nfrom tensorflow.keras.initializers import RandomUniform\nfrom tensorflow.keras.constraints import NonNeg\n\nclass BumpLayer(tf.keras.layers.Layer):\n    def __init__(self, *args, **kwargs):\n        super(BumpLayer, self).__init__(*args, **kwargs)\n\n    def build(self, input_shape):\n        self.sigma = self.add_weight(\n            name='sigma',\n            shape=[1],\n            initializer=RandomUniform(minval=0.0, maxval=0.1),\n            trainable=True,\n            constraint=tf.keras.constraints.NonNeg()\n        )\n        super().build(input_shape)\n\n    def bump_function(self, x):\n        return tf.math.exp(-self.sigma / (self.sigma - tf.math.pow(x, 2)))\n\n    def call(self, inputs):\n        greater = tf.math.greater(inputs, -self.sigma)\n        less = tf.math.less(inputs, self.sigma)\n        condition = tf.logical_and(greater, less)\n\n        output = tf.where(\n            condition, \n            self.bump_function(inputs),\n            0.0\n        )\n        return output\n\nimport numpy as np\n\ndef generate_data(sigma, min_x=-1, max_x=1, shape=(100000,1)):\n    assert sigma &gt;= 0, 'Sigma should be non-negative!'\n    x = np.random.uniform(min_x, max_x, size=shape)\n    xp2 = np.power(x, 2)\n    condition = np.logical_and(x &lt; sigma, x &gt; -sigma)\n    y = np.where(condition, np.exp(-sigma / (sigma - xp2)), 0.0)\n    dy = np.where(condition, xp2 * y / np.power((sigma - xp2), 2), 0)\n    return x, y, dy\n\ndef make_model(input_shape=(1,)):\n    model = tf.keras.Sequential()\n    model.add(BumpLayer(input_shape=input_shape))\n\n    model.compile(loss='mse', optimizer='adam')\n    return model\n\n# Generate training data using a fixed sigma value.\nsigma = 0.5\nx, y, _ = generate_data(sigma=sigma, min_x=-0.1, max_x=0.1)\n\nmodel = make_model()\n\n# Store initial value of sigma, so that it could be compared after training.\nsigma_before = model.layers[0].get_weights()[0][0]\n\nmodel.fit(x, y, epochs=5)\n\nprint('Sigma before training:', sigma_before)\nprint('Sigma after training:', model.layers[0].get_weights()[0][0])\nprint('Sigma used for generating data:', sigma)\n\n# Sigma before training: 0.08271004\n# Sigma after training: 0.5000002\n# Sigma used for generating data: 0.5\n\n     output = tf.where(\n            condition, \n            self.bump_function(tf.where(condition, inputs, 0.0)),\n            0.0\n     )\n\ntrue_learned_sigma = []\nfor s in np.arange(0.1, 10.0, 0.1):\n    model = make_model()\n    x, y, dy = generate_data(sigma=s, shape=(100000,1))\n    model.fit(x, y, epochs=3 if s &lt; 1 else (5 if s &lt; 5 else 10), verbose=False)\n    sigma = model.layers[0].get_weights()[0][0]\n    true_learned_sigma.append([s, sigma])\n    print(s, sigma)\n\n# Check if the learned values of sigma\n# are actually close to true values of sigma, for all the experiments.\nres = np.array(true_learned_sigma)\nprint(np.allclose(res[:,0], res[:,1], atol=1e-2))\n# True\n"
'Height      Weight      Age     90 min aerobics/wk?     completed 5 mile run?\n 155         45          31           Yes                      True\n 160         51          33           No                       False\n 168         52          28           No                       False\n 155         61          25           Yes                      True\n 169         57          52           Yes                      True\n 172         81          35           No                       False\n 164         70          23           Yes                      False\n\ndef entropy(arr1) :\n    import numpy as NP\n    ue = NP.unique(x)\n    p, entropy = 0., 0.\n    for itm in ue :\n        ndx = arr1 == itm\n        p += NP.size(x[ndx]) / float(x.size)\n        entropy -= p * NP.log2(p)\n    return entropy\n\np(i) = frequency(outcome) = count(outcome) / count(total_rows)\n\nentropy = sum of p(i) x log2(p(i))\n\n# simulate a data set with three class labels (0 1, 2)\n# for your problem, the class labels are the keywords, \n# so just map each unique keyword to an integer value (e.g., { \'keyword1\' : 0, \'keyword2\' : 1}\n&gt;&gt;&gt; x = NP.random.randint(0, 3, 20)\n&gt;&gt;&gt; x\n   array([1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 2, 2, 0, 2, 0, 1, 1, 1, 1, 1])\n\n&gt;&gt;&gt; print("{0:.3f}".format(entropy(x)))\n   0.758\n'
'np.random.random(d) * 2 - 1\n\nnp.random.random((n, d)) * 2 - 1\n'
'&gt;&gt;&gt; ss = ShuffleSplit(X.shape[0], n_iterations=100, test_fraction=0.1,\n...     random_state=42)\n\n&gt;&gt;&gt; from sklearn.utils import shuffle\n&gt;&gt;&gt; from sklearn.cross_validation import StratifiedKFold, cross_val_score\n&gt;&gt;&gt; for i in range(10):\n...    X, y = shuffle(X_orig, y_orig, random_state=i)\n...    skf = StratifiedKFold(y, 10)\n...    print cross_val_score(clf, X, y, cv=skf)\n'
'from nltk import FreqDist\nfrom nltk.classify.naivebayes import NaiveBayesClassifier\n\ndef make_training_data(rdr):\n    for c in rdr.categories():\n        for f in rdr.fileids(c):\n            yield FreqDist(rdr.words(fileids=[f])), c\n\nclf = NaiveBayesClassifier.train(list(make_training_data(reader)))\n'
"import pandas as pd\n\ndata = pd.read_csv(filename, options...)\nstore = pd.HDFStore('data.h5')\nstore['mydata'] = data\nstore.close()\n\nimport pandas as pd\n\nstore = pd.HDFStore('data.h5')\ndata = store['mydata']\nstore.close()\n"
'import numpy as np\nimport sklearn.tree\nclf = sklearn.tree.DecisionTreeClassifier()\nclf.fit(X,y)\nclf.tree_.apply(np.asfortranarray(X.astype(sklearn.tree._tree.DTYPE)))\n'
'numpy.array([network.activate(x) for x, _ in train])\n\nfrom datasets import XORDataSet \nfrom pybrain.tools.shortcuts import buildNetwork\nfrom pybrain.supervised import BackpropTrainer\nimport numpy\nd = XORDataSet()\nn = buildNetwork(d.indim, 4, d.outdim, bias=True)\nt = BackpropTrainer(n, learningrate=0.01, momentum=0.99, verbose=True)\nt.trainOnDataset(d, 1000)\nt.testOnData(verbose=True)\nprint numpy.array([n.activate(x) for x, _ in d])\n'
"@cython.boundscheck(False)\n@cython.wraparound(False)\n@cython.nonecheck(False)\ndef get_centers_fast(np.ndarray[DTYPE_t, ndim = 2] x, double radius):\n\n    cdef int N = x.shape[0]\n    cdef int D = x.shape[1]\n    cdef int m = 1\n    cdef np.ndarray[DTYPE_t, ndim = 2] xc = np.zeros([10000, D])\n    cdef double r = 0\n    cdef double r_min = 10\n    cdef int i, j, k\n\n    for k in range(D):\n        xc[0,k] = x[0,k]\n\n    for i in range(1, N):\n        r_min = 10\n        for j in range(m):\n            r = 0\n            for k in range(D):\n                r += (x[i, k] - xc[j, k])**2\n            r = r**0.5\n            if r &lt; r_min:\n                r_min = r\n        if r_min &gt; radius:\n            m = m + 1\n            for k in range(D):\n                xc[m - 1,k] = x[i,k]\n\n    nonzero = np.nonzero(xc[:,0])[0]\n    xc = xc[nonzero,:]\n\n    return xc\n\nN = 40000\nr = 0.1\nx1 = np.random.normal(size = N)\nx1 = (x1 - min(x1)) / (max(x1)-min(x1))\nx2 = np.random.normal(size = N)\nx2 = (x2 - min(x2)) / (max(x2)-min(x2))\nX = np.vstack([x1, x2]).T\n\ntic = time.time()\ngrid0 = gt.get_centers0(X, r)\ntoc = time.time()\nprint 'Method 0: ' + str(toc - tic)\n\ntic = time.time()\ngrid2 = gt.get_centers2(X, r, 10)\ntoc = time.time()\nprint 'Method 2: ' + str(toc - tic)\n\ntic = time.time()\ngrid3 = gt.get_centers_fast(X, r)\ntoc = time.time()\nprint 'Method 3: ' + str(toc - tic)\n\nMethod 0: 0.219595909119\nMethod 2: 0.191949129105\nMethod 3: 0.0127329826355\n"
'nmf_model.fit(A);\nH = nmf_model.components_.T;\n\nW = nmf_model.fit_transform(A);\nH = nmf_model.components_;\n'
'w_0  w_1  ...  w_n,\n\nw_10  w_11  ...  w_1n\nw_20  w_21  ...  w_2n\n....             ....\nw_m0  w_m1  ...  w_mn\n'
'In [1]: from sklearn.datasets import load_files\n\nIn [2]: from sklearn.cross_validation import train_test_split\n\nIn [3]: bunch = load_files(\'./Topics\')\n\nIn [4]: X_train, X_test, y_train, y_test = train_test_split(bunch.data, bunch.target, test_size=.4)\n\n# Then proceed to train your model and validate.\n\nIn [14]: X_test[:2]\nOut[14]:\n[\'Psychologist Philip Zimbardo asks, "Why are boys struggling?" He shares some stats (lower graduation rates, greater worries about intimacy and relationships) and suggests a few reasons -- and challenges the TED community to think about solutions.Philip Zimbardo was the leader of the notorious 1971 Stanford Prison Experiment -- and an expert witness at Abu Ghraib. His book The Lucifer Effect explores the nature of evil; now, in his new work, he studies the nature of heroism.\',\n \'Human growth has strained the Earth\\\'s resources, but as Johan Rockstrom reminds us, our advances also give us the science to recognize this and change behavior. His research has found nine "planetary boundaries" that can guide us in protecting our planet\\\'s many overlapping ecosystems.If Earth is a self-regulating system, it\\\'s clear that human activity is capable of disrupting it. Johan Rockstrom has led a team of scientists to define the nine Earth systems that need to be kept within bounds for Earth to keep itself in balance.\']\n\nIn [15]: y_test[:2]\nOut[15]: array([ 84, 113])\n\nIn [16]: [bunch.target_names[idx] for idx in y_test[:2]]\nOut[16]: [\'Education\', \'Global issues\']\n'
"clf=sklearn.svm.SVC(kernel='linear',probability=True)\nclf.fit(X,y)\nclf.predict_proba(X_test)\n"
"{'type':'ineq', 'fun': lambda x: x}\n\n{'type':'ineq', 'fun': lambda x: np.sum(x, 0) - 1}  # row sum &gt;= 1\n{'type':'ineq', 'fun': lambda x: 1 - np.sum(x, 0)}  # row sum &lt;= 1\n"
"import tensorflow as tf\nimport numpy as np\n\n# Define dimensions\nd = 10     # Size of the parameter space\nN = 1000   # Number of data sample\n\n# create random data\nw = .5*np.ones(d)\nx_data = np.random.random((N, d)).astype(np.float32)\ny_data = x_data.dot(w).reshape((-1, 1))\n\n# Define placeholders to feed mini_batches\nX = tf.placeholder(tf.float32, shape=[None, d], name='X')\ny_ = tf.placeholder(tf.float32, shape=[None, 1], name='y')\n\n# Find values for W that compute y_data = &lt;x, W&gt;\nW = tf.Variable(tf.random_uniform([d, 1], -1.0, 1.0))\ny = tf.matmul(X, W, name='y_pred')\n\n# Minimize the mean squared errors.\nloss = tf.reduce_mean(tf.square(y_ - y))\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)\n\n# Before starting, initialize the variables\ninit = tf.initialize_all_variables()\n\n# Launch the graph.\nsess = tf.Session()\nsess.run(init)\n\n# Fit the line.\nmini_batch_size = 100\nn_batch = N // mini_batch_size + (N % mini_batch_size != 0)\nfor step in range(2001):\n    i_batch = (step % n_batch)*mini_batch_size\n    batch = x_data[i_batch:i_batch+mini_batch_size], y_data[i_batch:i_batch+mini_batch_size]\n    sess.run(train, feed_dict={X: batch[0], y_: batch[1]})\n    if step % 200 == 0:\n        print(step, sess.run(W))\n"
'from keras.utils.np_utils import to_categorical\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n'
'def cost(X, y, theta, regTerm):\n    m = X.shape[0]  # or y.shape, or even p.shape after the next line, number of training set\n    p = expit(X @ theta)\n    log_loss = -np.average(y*np.log(p) + (1-y)*np.log(1-p))\n    J = log_loss + regTerm * np.linalg.norm(theta[1:]) / (2*m)\n    return J\n'
'gradients, v = zip(*optimizer.compute_gradients(loss))\n\ngradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n\noptimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n'
"In [13]: paramsd = dict(zip(('shape','loc','scale'),params))\n\nIn [14]: a = paramsd['shape']\n\nIn [15]: del paramsd['shape']\n\nIn [16]: paramsd\nOut[16]: {'loc': -71.588039241913037, 'scale': 0.051114096301755507}\n\nIn [17]: X = np.linspace(-55, -45, 100)\n\nIn [18]: plt.plot(X, stats.gamma.pdf(X,a,**paramsd))\nOut[18]: [&lt;matplotlib.lines.Line2D at 0x7ff820f21d68&gt;]\n\nIn [32]: def ecdf(x):\n   .....:         xs = np.sort(x)\n   .....:         ys = np.arange(1, len(xs)+1)/float(len(xs))\n   .....:         return xs, ys\n   .....: \n\nIn [33]: plt.plot(X, stats.gamma.cdf(X,a,**paramsd))\nOut[33]: [&lt;matplotlib.lines.Line2D at 0x7ff805223a20&gt;]\n\nIn [34]: plt.plot(*ecdf(x))\nOut[34]: [&lt;matplotlib.lines.Line2D at 0x7ff80524c208&gt;]\n"
'\noptimizer = RMSprop(lr=0.01) \nembedding_vecor_length = 32\nmax_review_length = 28\nnb_classes= 8\nmodel = Sequential()\nmodel.add(Embedding(input_dim=900, output_dim=embedding_vecor_length,\n                    input_length=max_review_length)) \n\nmodel.add(LSTM(150))\n\n#output_dim is a categorical variable with 8 classes\nmodel.add(Dense(output_dim=nb_classes, activation=\'softmax\'))\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=optimizer, metrics=[\'accuracy\'])\nprint(model.summary())\n\nmodel.fit(X_train, y_train, nb_epoch=3, batch_size=64)\n\n# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint("Accuracy: %.2f%%" % (scores[1]*100))\n\n'
'm = np.zeros([length,length]) # n is the count of all words\ndef cal_occ(sentence,m):\n    for i,word in enumerate(sentence):\n        for j in range(max(i-window,0),min(i+window,length)):\n             m[word,sentence[j]]+=1\nfor sentence in X:\n    cal_occ(sentence, m)\n'
"from keras.layers.wrappers import TimeDistributed\n\nmodel = Sequential()\nmodel.add(LSTM(100, input_dim=num_features, return_sequences=True))\nmodel.add(TimeDistributed(Dense(1, activation='sigmoid')))\n\nprint(model.summary()) \n"
'def soft_acc(y_true, y_pred):\n    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))\n\nmodel.compile(..., metrics=[soft_acc])\n'
'def get_features_for_input(input):\n    current_words = word_tokenize(input.lower())\n    current_words = [lemmatizer.lemmatize(i) for i in current_words]\n    features = np.zeros((1, len(lexicon)))\n\n    for word in current_words:\n        if word.lower() in lexicon:\n            index_value = lexicon.index(word.lower())\n            # OR DO +=1, test both\n            features[0, index_value] += 1\n\n    return features\n'
'sklearn.metrics.cohen_kappa_score(y1, y2, labels=None, weights=None)\n\nY_pred = new_model.predict(X_test_dtm)\ncohen_score = cohen_kappa_score(Y_test, Y_pred)\n'
'l2 = tf.add(tf.matmul(data, hidden_2_layer[\'weights\']), \n                      hidden_2_layer[\'biases\'])\n\nl2 = tf.add(tf.matmul(l1, hidden_2_layer[\'weights\']), \n                      hidden_2_layer[\'biases\'])\n\nl3 = tf.add(tf.matmul(data, hidden_3_layer[\'weights\']), \n                      hidden_3_layer[\'biases\'])\n\nl3 = tf.add(tf.matmul(l2, hidden_3_layer[\'weights\']), \n                      hidden_3_layer[\'biases\'])\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets("/tmp/data/", one_hot=True)\n\nn_nodes_hl1 = 500\nn_nodes_hl2 = 500\nn_nodes_hl3 = 500\n\nn_classes = 10\nbatch_size = 100\n\nx = tf.placeholder(\'float\', [None, 784])\ny = tf.placeholder(\'float\')\n\ndef print_shape(obj):\n    print(obj.get_shape().as_list())\n\ndef neural_network_model(data):\n    hidden_1_layer = {\'weights\': tf.Variable(tf.random_normal([784,\n                                                               n_nodes_hl1])),\n                      \'biases\':\n                      tf.Variable(tf.random_normal([n_nodes_hl1]))}\n\n    hidden_2_layer = {\'weights\':\n                      tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n                      \'biases\':\n                      tf.Variable(tf.random_normal([n_nodes_hl2]))}\n\n    hidden_3_layer = {\'weights\':\n                      tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n                      \'biases\':\n                      tf.Variable(tf.random_normal([n_nodes_hl3]))}\n\n    output_layer = {\'weights\': tf.Variable(tf.random_normal([n_nodes_hl3,\n                                                             n_classes])),\n                    \'biases\': tf.Variable(tf.random_normal([n_classes]))}\n    print_shape(data)\n    l1 = tf.add(tf.matmul(data, hidden_1_layer[\'weights\']),\n                hidden_1_layer[\'biases\'])\n    print_shape(l1)\n    l1 = tf.nn.relu(l1)\n    print_shape(l1)\n    l2 = tf.add(tf.matmul(l1, hidden_2_layer[\'weights\']),\n                hidden_2_layer[\'biases\'])\n    l2 = tf.nn.relu(l2)\n\n    l3 = tf.add(tf.matmul(l2, hidden_3_layer[\'weights\']),\n                hidden_3_layer[\'biases\'])\n    l3 = tf.nn.relu(l3)\n\n    output = tf.add(tf.matmul(l3, output_layer[\'weights\']),\n                    output_layer[\'biases\'])\n\n    return output\n\n\ndef train_neural_network(x):\n    prediction = neural_network_model(x)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits\n                          (logits=prediction, labels=y))\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n\n    hm_epochs = 10\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for epoch in range(hm_epochs):\n            epoch_loss = 0\n            for _ in range(int(mnist.train.num_examples / batch_size)):\n                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x,\n                                                              y: epoch_y})\n                epoch_loss += c\n            print(\'Epoch\', epoch, \'completed out of\', hm_epochs, \'loss:\',\n                  epoch_loss)\n\n        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct, \'float\'))\n        print(\'Accuracy:\', accuracy.eval({x: mnist.test.images, y:\n                                          mnist.test.labels}))\n\n\ntrain_neural_network(x)\n'
"# Convert labels to categorical one-hot encoding\nlabels = np.array([1, 2]) # 0 - num_classes - 1\ny_train = keras.utils.to_categorical(labels, num_classes=3)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.add(Flatten(input_shape=X_train.shape[1:]))\nmodel.add(Dense(3, activation='softmax'))\n"
"from hyperopt import space_eval\nspace_eval(parameter_space_svc, best)\n\nOutput:\n{'C': 13.271912841932233, 'gamma': 0.0017394328334592358, 'kernel': 'rbf'}\n"
"import sys\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nMIN_BLOCK_SIZE = 100 # pixels\n\nimg = plt.imread(sys.argv[1])\n\n# find blank rows\nrow_sums = np.mean(img, axis=1)\nthreshold = np.percentile(row_sums, 75)\nis_blank = row_sums &gt; threshold\n\n# find blocks between blank rows\nblock_edges = np.diff(is_blank.astype(np.int))\nstarts, = np.where(block_edges == -1)\nstops, = np.where(block_edges == 1)\nblocks = np.c_[starts, stops]\n\n# plot steps\nfig, axes = plt.subplots(3,1, sharex=True, figsize=(6.85, 6))\naxes[0].plot(row_sums)\naxes[0].axhline(threshold, c='r', ls='--')\naxes[1].plot(is_blank)\nfor (start, stop) in blocks:\n    if stop - start &gt; MIN_BLOCK_SIZE:\n        axes[2].axvspan(start, stop, facecolor='red')\nplt.show()\n"
'db1_labels = db1.labels_\nlabels, counts = np.unique(db1_labels[db1_labels&gt;=0], return_counts=True)\nprint labels[np.argsort(-counts)[:3]]\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\ndef plot_grid(x,y, ax=None, **kwargs):\n    ax = ax or plt.gca()\n    segs1 = np.stack((x,y), axis=2)\n    segs2 = segs1.transpose(1,0,2)\n    ax.add_collection(LineCollection(segs1, **kwargs))\n    ax.add_collection(LineCollection(segs2, **kwargs))\n    ax.autoscale()\n\n\nf = lambda x,y : ( x+0.8*np.exp(-x**2-y**2),y )\n\nfig, ax = plt.subplots()\n\ngrid_x,grid_y = np.meshgrid(np.linspace(-3,3,20),np.linspace(-3,3,20))\nplot_grid(grid_x,grid_y, ax=ax,  color="lightgrey")\n\ndistx, disty = f(grid_x,grid_y)\nplot_grid(distx, disty, ax=ax, color="C0")\n\nplt.show()\n'
'(1 - y[2]) * log(1 - p[2]) = 1 * log(1) = log(1) = 0\n'
'onehotencoder1 = OneHotEncoder(categorical_features = [0])\nX = onehotencoder1.fit_transform(X).toarray()\n'
"from keras.layers import (Conv1D, MaxPool1D, Dropout, Flatten, Dense,\n                          Input, concatenate)\nfrom keras.models import Model, Sequential\n\ntimesteps = 50\nn = 5\n\ndef network():\n    sequence = Input(shape=(timesteps, 1), name='Sequence')\n    features = Input(shape=(n,), name='Features')\n\n    conv = Sequential()\n    conv.add(Conv1D(10, 5, activation='relu', input_shape=(timesteps, 1)))\n    conv.add(Conv1D(10, 5, activation='relu'))\n    conv.add(MaxPool1D(2))\n    conv.add(Dropout(0.5, seed=789))\n\n    conv.add(Conv1D(5, 6, activation='relu'))\n    conv.add(Conv1D(5, 6, activation='relu'))\n    conv.add(MaxPool1D(2))\n    conv.add(Dropout(0.5, seed=789))\n    conv.add(Flatten())\n    part1 = conv(sequence)\n\n    merged = concatenate([part1, features])\n\n    final = Dense(512, activation='relu')(merged)\n    final = Dropout(0.5, seed=789)(final)\n    final = Dense(2, activation='softmax')(final)\n\n    model = Model(inputs=[sequence, features], outputs=[final])\n\n    model.compile(loss='logcosh', optimizer='adam', metrics=['accuracy'])\n\n    return model\n\nm = network()\n"
"from keras.models import load_model\nmodel = load_model('my_model.h5')\n\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nfrom keras.preprocessing import image\n\ntest_image = image.load_img(imagePath, target_size = (64, 64)) \ntest_image = image.img_to_array(test_image)\ntest_image = np.expand_dims(test_image, axis = 0)\n\n#predict the result\nresult = model.predict(test_image)\n"
'import numpy as np\n\n# generate some random data for demonstration purpose, use your original dataset here\nX = np.random.rand(1000,100)     # 1000 x 100 data\ny = np.random.rand(1000).round() # 0, 1 labels\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nseed=0\ntest_size=0.30\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size, random_state=seed)\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\nimport matplotlib.pylab as plt\nfrom matplotlib import pyplot\nfrom xgboost import plot_importance\nplot_importance(model, max_num_features=10) # top 10 most important features\nplt.show()\n'
"import tensorflow as tf\n\n\nlogits = tf.constant([[0, 1],\n                      [1, 1],\n                      [2, -4]], dtype=tf.float32)\ny_true = tf.constant([[1, 1],\n                      [1, 0],\n                      [1, 0]], dtype=tf.float32)\n# tensorflow api\nloss = tf.losses.sigmoid_cross_entropy(multi_class_labels=y_true,\n                                       logits=logits)\n\n# manul computing\nprobs = tf.nn.sigmoid(logits)\nloss_t = tf.reduce_mean(y_true * (-tf.log(probs)) +\n                        (1 - y_true) * (-tf.log(1 - probs)))\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True  # pylint: disable=no-member\nwith tf.Session(config=config) as sess:\n    loss_ = loss.eval()\n    loss_t_ = loss_t.eval()\n    print('sigmoid_cross_entropy: {: .3f}\\nmanual computing: {: .3f}'.format(\n        loss_, loss_t_))\n------------------------------------------------------------------------------\n#output: \n    sigmoid_cross_entropy:  0.463\n    manual computing:  0.463\n"
'&gt;&gt;&gt; arrTest1.ndim\n1\n\narrTest1 = np.array([[0.1, 0.1, 0.1, 0.1, 0.1, 0.5, 0.1, 0., 0.1, 0.6, 0.1, 0.1, 0., 0., 0., 0.1, 0., 0., 0.1, 0., 0.]])\n\n&gt;&gt;&gt; arrTest1.shape\n(1, 21)\n'
"from keras import backend as K\n\nK.set_learning_phase(0)\n\nbase_model = applications.inception_v3.InceptionV3(weights='imagenet', include_top=False, input_shape=(img_width,img_height,3))\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\nK.set_learning_phase(1)\n\ntop_model = Sequential()\ntop_model.add(Flatten(input_shape=base_model.output_shape[1:]))\ntop_model.add(Dense(1000, activation='relu'))\ntop_model.add(Dense(inclusive_images, activation='softmax'))\n\ntop_model.load_weights(top_model_weights_path)\n\n#combine base and top model\nfullModel = Model(input= base_model.input, output= top_model(base_model.output))\n\nfullModel.compile(loss='categorical_crossentropy',\n             optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), \n             metrics=['accuracy'])\n\n\n#####################################################################\n# Here, define the generators and then fit the model same as before #\n#####################################################################\n\nfor layer in model2.layers[:-2]:\n    layer.trainable = False \n"
"labels = df.groupby('label').name.unique()\n# Sort the over-represented class to the head.\nlabels = labels[labels.apply(len).sort_values(ascending=False).index]\nexcess = len(labels.iloc[0]) - len(labels.iloc[1])\nremove = np.random.choice(labels.iloc[0], excess, replace=False)\ndf2 = df[~df.name.isin(remove)]\n"
"y_scores = cross_val_score(knn_cv, X, y, cv=76)\nfpr, tpr, threshold = roc_curve(y_test, y_scores)\n\ny_scores = knn.predict_proba(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nimport matplotlib.pyplot as plt\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nknn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train,y_train)\n\ny_scores = knn.predict_proba(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of kNN')\nplt.show()\n"
"model = load_model(path, custom_objects={'loss': weighted_loss})\n"
'from sklearn import datasets\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import  RandomForestClassifier\nimport pandas as pd\n\ndiabetes = datasets.load_diabetes()\nX, y = diabetes.data, diabetes.target\n\nclf=RandomForestClassifier(n_estimators =10, random_state = 42, class_weight="balanced")\noutput = cross_validate(clf, X, y, cv=2, scoring = \'accuracy\', return_estimator =True)\n\nfor idx,estimator in enumerate(output[\'estimator\']):\n    print("Features sorted by their score for estimator {}:".format(idx))\n    feature_importances = pd.DataFrame(estimator.feature_importances_,\n                                       index = diabetes.feature_names,\n                                        columns=[\'importance\']).sort_values(\'importance\', ascending=False)\n    print(feature_importances)\n\nFeatures sorted by their score for estimator 0:\n     importance\ns6     0.137735\nage    0.130152\ns5     0.114561\ns2     0.113683\ns3     0.112952\nbmi    0.111057\nbp     0.108682\ns1     0.090763\ns4     0.056805\nsex    0.023609\nFeatures sorted by their score for estimator 1:\n     importance\nage    0.129671\nbmi    0.125706\ns2     0.125304\ns1     0.113903\nbp     0.111979\ns6     0.110505\ns5     0.106099\ns3     0.098392\ns4     0.054542\nsex    0.023900\n'
"    def generate_data(path, imagesize, nBatches):\n        datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n        generator = datagen.flow_from_directory(directory=path,     # path to the target directory\n             target_size=(imagesize,imagesize),                     # dimensions to which all images found will be resize\n             color_mode='rgb',                                      # whether the images will be converted to have 1, 3, or 4 channels\n             classes=None,                                          # optional list of class subdirectories\n             class_mode='categorical',                              # type of label arrays that are returned\n             batch_size=nBatches,                                   # size of the batches of data\n             shuffle=True,                                          # whether to shuffle the data\n             seed=42)                                               # random seed for shuffling and transformations\n        return generator\n\n    def create_model(imagesize, nBands, nClasses):\n        # Create pre-trained base model\n        basemodel = VGG19(include_top=False,                        # exclude final pooling and fully connected layer in the original model\n                             weights='imagenet',                    # pre-training on ImageNet\n                             input_tensor=None,                     # optional tensor to use as image input for the model\n                             input_shape=(imagesize,                # shape tuple\n                                          imagesize,\n                                          nBands),\n                             pooling=None,                          # output of the model will be the 4D tensor output of the last convolutional layer\n                             classes=nClasses)                      # number of classes to classify images into\n\n        # Freeze weights on pre-trained layers\n        for layer in basemodel.layers:\n            layer.trainable = False   \n\n        # Create new untrained layers\n        x = basemodel.output\n        x = GlobalAveragePooling2D()(x)                             # global spatial average pooling layer\n        x = Dense(1024, activation='relu')(x)                       # fully-connected layer\n        x = Dropout(rate=0.8)(x)                                    # dropout layer\n        y = Dense(nClasses, activation='softmax')(x)                # logistic layer making sure that probabilities sum up to 1\n\n        # Create model combining pre-trained base model and new untrained layers\n        model = Model(inputs=basemodel.input,\n                      outputs=y)\n\n        # Define learning optimizer\n        optimizerSGD = optimizers.SGD(lr=0.001,                     # learning rate.\n                                      momentum=0.9,                 # parameter that accelerates SGD in the relevant direction and dampens oscillations\n                                      decay=learningRate/nEpochs,   # learning rate decay over each update\n                                      nesterov=True)                # whether to apply Nesterov momentum\n        # Compile model\n        model.compile(optimizer=optimizerSGD,                       # stochastic gradient descent optimizer\n                      loss='categorical_crossentropy',              # objective function\n                      metrics=['accuracy'],                         # metrics to be evaluated by the model during training and testing\n                      loss_weights=None,                            # scalar coefficients to weight the loss contributions of different model outputs\n                      sample_weight_mode=None,                      # sample-wise weights\n                      weighted_metrics=None,                        # metrics to be evaluated and weighted by sample_weight or class_weight during training and testing\n                      target_tensors=None)                          # tensor model's target, which will be fed with the target data during training\n        return model\n\n    def train_model(model, nBatches, nEpochs, trainGenerator, valGenerator, resultPath):\n        history = model.fit_generator(generator=trainGenerator,\n                                      steps_per_epoch=trainGenerator.samples // nBatches,   # total number of steps (batches of samples)\n                                      epochs=nEpochs,               # number of epochs to train the model\n                                      verbose=2,                    # verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch\n                                      callbacks=None,               # keras.callbacks.Callback instances to apply during training\n                                      validation_data=valGenerator, # generator or tuple on which to evaluate the loss and any model metrics at the end of each epoch\n                                      validation_steps=\n                                      valGenerator.samples // nBatches,                     # number of steps (batches of samples) to yield from validation_data generator before stopping at the end of every epoch\n                                      class_weight=None,            # optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function\n                                      max_queue_size=10,            # maximum size for the generator queue\n                                      workers=1,                    # maximum number of processes to spin up when using process-based threading\n                                      use_multiprocessing=False,    # whether to use process-based threading\n                                      shuffle=True,                 # whether to shuffle the order of the batches at the beginning of each epoch\n                                      initial_epoch=0)              # epoch at which to start training\n\n        return history, model\n"
'kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n'
"import keras.models\nimport numpy as np\nfrom python_toolbox import random_tools\n\nRADIX = 7\nFEATURE_BITS = 20\n\ndef _get_number(vector):\n    return sum(x * 2 ** i for i, x in enumerate(vector))\n\ndef _get_mod_result(vector):\n    return _get_number(vector) % RADIX\n\ndef _number_to_vector(number):\n    binary_string = bin(number)[2:]\n    if len(binary_string) &gt; FEATURE_BITS:\n        raise NotImplementedError\n    bits = (((0,) * (FEATURE_BITS - len(binary_string))) +\n            tuple(map(int, binary_string)))[::-1]\n    assert len(bits) == FEATURE_BITS\n    return np.c_[bits]\n\n\ndef get_mod_result_vector(vector):\n    v = np.repeat(0, 7)\n    v[_get_mod_result(vector)] = 1\n    return v\n\n\ndef main():\n    model = keras.models.Sequential(\n        (\n            keras.layers.Reshape(\n                (1, -1)\n            ),\n            keras.layers.LSTM(\n                units=100,\n            ),\n            keras.layers.Dense(\n                units=7, activation='softmax'\n            )\n        )\n    )\n    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    data = np.random.randint(2, size=(50000, FEATURE_BITS))\n    labels = np.vstack(map(get_mod_result_vector, data))\n\n    model.fit(data, labels, epochs=40, batch_size=50)\n    def predict(number):\n        foo = model.predict(_number_to_vector(number))\n        return np.argmax(foo)\n    def is_correct_for_number(x):\n        return bool(predict(x) == x % RADIX)\n    sample = random_tools.shuffled(range(2 ** FEATURE_BITS))[:500]\n    print('Total accuracy:')\n    print(sum(map(is_correct_for_number, sample)) / len(sample))\n    print(f'(Accuracy of random algorithm is {1/RADIX:.2f}')\n\n\nif __name__ == '__main__':\n    main()\n\nimport keras.models\nimport numpy as np\nfrom python_toolbox import random_tools\n\nRADIX = 7\nFEATURE_BITS = 8\n\ndef _get_number(vector):\n    return sum(x * 2 ** i for i, x in enumerate(vector))\n\ndef _get_mod_result(vector):\n    return _get_number(vector) % RADIX\n\ndef _number_to_vector(number):\n    binary_string = bin(number)[2:]\n    if len(binary_string) &gt; FEATURE_BITS:\n        raise NotImplementedError\n    bits = (((0,) * (FEATURE_BITS - len(binary_string))) +\n            tuple(map(int, binary_string)))[::-1]\n    assert len(bits) == FEATURE_BITS\n    return np.c_[bits]\n\n\ndef get_mod_result_vector(vector):\n    v = np.repeat(0, 7)\n    v[_get_mod_result(vector)] = 1\n    return v\n\n\ndef main():\n    model = keras.models.Sequential(\n        (\n            keras.layers.Dense(\n                units=20, activation='relu', input_dim=FEATURE_BITS\n            ),\n            keras.layers.Dense(\n                units=20, activation='relu'\n            ),\n            keras.layers.Dense(\n                units=7, activation='softmax'\n            )\n        )\n    )\n    model.compile(optimizer='sgd',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    data = np.random.randint(2, size=(10000, FEATURE_BITS))\n    labels = np.vstack(map(get_mod_result_vector, data))\n\n    model.fit(data, labels, epochs=100, batch_size=50)\n    def predict(number):\n        foo = model.predict(_number_to_vector(number))\n        return np.argmax(foo)\n    def is_correct_for_number(x):\n        return bool(predict(x) == x % RADIX)\n    sample = random_tools.shuffled(range(2 ** FEATURE_BITS))[:500]\n    print('Total accuracy:')\n    print(sum(map(is_correct_for_number, sample)) / len(sample))\n    print(f'(Accuracy of random algorithm is {1/RADIX:.2f}')\n\n\nif __name__ == '__main__':\n    main()\n"
"import tensorflow as tf\nimport keras\nfrom tensorflow.python.keras.layers import Dense, Input\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.callbacks import Callback\nfrom sklearn.metrics import recall_score, classification_report\nfrom sklearn.datasets import make_classification\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Model -- Binary classifier\nbinary_model = Sequential()\nbinary_model.add(Dense(16, input_shape=(2,), activation='relu'))\nbinary_model.add(Dense(8, activation='relu'))\nbinary_model.add(Dense(1, activation='sigmoid'))\nbinary_model.compile('adam', loss='binary_crossentropy')\n\n# Model -- Multiclass classifier\nmulticlass_model = Sequential()\nmulticlass_model.add(Dense(16, input_shape=(2,), activation='relu'))\nmulticlass_model.add(Dense(8, activation='relu'))\nmulticlass_model.add(Dense(3, activation='softmax'))\nmulticlass_model.compile('adam', loss='categorical_crossentropy')\n\n# callback to find metrics at epoch end\nclass Metrics(Callback):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y if (y.ndim == 1 or y.shape[1] == 1) else np.argmax(y, axis=1)\n        self.reports = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_hat = np.asarray(self.model.predict(self.x))\n        y_hat = np.where(y_hat &gt; 0.5, 1, 0) if (y_hat.ndim == 1 or y_hat.shape[1] == 1)  else np.argmax(y_hat, axis=1)\n        report = classification_report(self.y,y_hat,output_dict=True)\n        self.reports.append(report)\n        return\n   \n    # Utility method\n    def get(self, metrics, of_class):\n        return [report[str(of_class)][metrics] for report in self.reports]\n    \n# Generate some train data (2 class) and train\nx, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nmetrics_binary = Metrics(x,y)\nbinary_model.fit(x, y, epochs=30, callbacks=[metrics_binary])\n\n# Generate some train data (3 class) and train\nx, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1, n_classes=3)\ny = keras.utils.to_categorical(y,3)\nmetrics_multiclass = Metrics(x,y)\nmulticlass_model.fit(x, y, epochs=30, callbacks=[metrics_multiclass])\n\n# Plotting \nplt.close('all')\nplt.plot(metrics_binary.get('recall',0), label='Class 0 recall') \nplt.plot(metrics_binary.get('recall',1), label='Class 1 recall') \n\nplt.plot(metrics_binary.get('precision',0), label='Class 0 precision') \nplt.plot(metrics_binary.get('precision',1), label='Class 1 precision') \n\nplt.plot(metrics_binary.get('f1-score',0), label='Class 0 f1-score') \nplt.plot(metrics_binary.get('f1-score',1), label='Class 1 f1-score') \nplt.legend(loc='lower right')\nplt.show()\n\nplt.close('all')\nfor m in ['recall', 'precision', 'f1-score']:\n    for c in [0,1,2]:\n        plt.plot(metrics_multiclass.get(m,c), label='Class {0} {1}'.format(c,m))\n        \nplt.legend(loc='lower right')\nplt.show()\n"
'from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE\nfrom sklearn.svm import SVR\nX, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\nestimator = SVR(kernel="linear")\nselector = RFE(estimator, step=1, n_features_to_select=1)\nselector = selector.fit(X, y)\nselector.ranking_\n'
'input = torch.randn(3, 5, requires_grad=True)\ninput = torch.randn(5, requires_grad=True)\n\ninput = torch.randn(5, requires_grad=True)\n\ntrain = torch.tensor([1, 0, 4])\ntrain = torch.tensor([1, 0, 0])\n'
"def f1(y_true, y_pred):\n    return 1\n\nmodel = tf.keras.models.load_model(path_to_model, custom_objects={'f1':f1})\n"
"scaler = StandardScaler()\nregressor = KerasRegressor(...)\n\npipe = Pipeline(steps=[\n    ('scaler', scaler),\n    ('ttregressor', TransformedTargetRegressor(regressor, transformer=scaler))\n])\n\n# Use `__regressor` to access the regressor hyperparameters\nparam_grid = {'ttregressor__regressor__hyperparam_name' : ...}\n\ngridcv = GridSearchCV(estimator=pipe, param_grid=param_grid, ...)\ngridcv.fit(X, X)\n\n ttgridcv = TransformedTargetRegressor(GridSearchCV(...), transformer=scalar)\n ttgridcv.fit(X, X)\n\n # Use `regressor_` attribute to access the fitted regressor (i.e. `GridSearchCV` instance) \n print(ttgridcv.regressor_.best_score_, ttgridcv.regressor_.best_params_))\n"
'# Keras prediction\nimg = image.load_img(img_path, target_size=(224, 224))\n\n   # OpenCV prediction\nimgcv = cv2.imread(img_path)\ndim = (224, 224)\nimgcv_resized = cv2.resize(imgcv, dim, interpolation=cv2.INTER_LINEAR)\n'
"&lt;snip&gt;\n# Test classifiers.\nkernels = [LINEAR, POLY, RBF]\nkname = ['linear','polynomial','rbf']\ncorrect = defaultdict(int)\nfor kn,kt in zip(kname,kernels):\n  print kt\n  param = svm_parameter(kernel_type = kt, C=10) # Here -&gt; rm probability = 1\n  model = svm_model(problem, param)\n  for test_sample,correct_label in test:\n      # Here -&gt; change predict_probability to just predict\n      pred_label = model.predict(test_sample)\n      correct[kn] += pred_label == correct_label\n&lt;/snip&gt;\n\n--------------------------------------------------------------------------------\nAccuracy:\n        polynomial 1.000000 (4 of 4)\n        rbf 1.000000 (4 of 4)\n        linear 1.000000 (4 of 4)\n"
'In [11]: df_null = df.isnull().unstack()\n\nIn [12]: t = df_null[df_null]\n\nIn [13]: t\nOut[13]:\nA  3    True\nB  3    True\nC  1    True\nD  0    True\n   1    True\ndtype: bool\n\nIn [14]: s = pd.Series(t2.index.get_level_values(1), t2.index.get_level_values(0))\n\nIn [15]: s\nOut[15]:\n0    D\n1    C\n1    D\n3    A\n3    B\ndtype: object\n\nIn [16]: s.groupby(level=0).apply(list)\nOut[16]:\n0       [D]\n1    [C, D]\n3    [A, B]\ndtype: object\n'
'from sklearn.utils import shuffle    \n\nprocessor = Data_Preprocessor()\ntd, tl = processor.raw_to_vector(path="C:/Users/Pankaj/Downloads/ng/")\nvectorizer = CountVectorizer(stop_words=\'english\', lowercase=True, min_df=2, analyzer="word")\ndata = vectorizer.fit_transform(td)\n# Shuffle the data and labels\ndata, tl = shuffle(data, tl, random_state=0)\nclfMNB = MultinomialNB(alpha=.0001)\nscore = Cross_Validation.Cross_Validation(clfMNB, 10, data, tl)\n\nprint("Train score" + str(score[0]))\nprint("Test score" + str(score[1]))\n'
'def transform (self, X):\n    print ("Structure of the data: \\n {}".format(X.head(5)))\n    print ("Features names: \\n {}".format(X.columns))\n    print ("Target: \\n {}".format(X.columns[0]))\n    print ("Shape of the data: \\n {}".format(X.shape))\n    return X\n'
'def save_model(session, input_tensor, output_tensor):\n  signature = tf.saved_model.signature_def_utils.build_signature_def(\n    inputs = {\'input\': tf.saved_model.utils.build_tensor_info(input_tensor)},\n    outputs = {\'output\': tf.saved_model.utils.build_tensor_info(output_tensor)},\n  )\n  b = saved_model_builder.SavedModelBuilder(\'/tmp/model\')\n  b.add_meta_graph_and_variables(session,\n                                 [tf.saved_model.tag_constants.SERVING],\n                                 signature_def_map={tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature})\n  b.save() \n\ntry (SavedModelBundle b = SavedModelBundle.load("/tmp/mymodel", "serve")) {\n  // b.session().run(...)\n}\n'
'input: "data"\ninput_dim: 10\ninput_dim: 3\ninput_dim: 224\ninput_dim: 224\n\ninput: "data"\ninput_shape {\n  dim: 1\n  dim: 3\n  dim: 227\n  dim: 227\n}\n\nimport coremltools\n\ncoreml_model = coremltools.converters.caffe.convert((\'mymodel.caffemodel\', \'deploy.prototxt\'),\n                                                    image_input_names = "data",\n                                                    is_bgr = True,\n                                                    class_labels=\'labels.txt\'\n                                                   )\n\n\ncoreml_model.save(\'MyModel.mlmodel\')\n'
'doc_topic = lda.transform(tf)\n\nfor n in range(doc_topic.shape[0]):\n    topic_most_pr = doc_topic[n].argmax()\n    print("doc: {} topic: {}\\n".format(n,topic_most_pr))\n'
'    _, c, p = sess.run([optimizer, cost, predictions], ...)\n    .\n    .\n    .\ncorrect_prediction = np.equal(np.argmax(p, axis=1), np.argmax(batch_y, axis=1))\naccuracy = np.mean(correct_prediction)\n\ncost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions,labels=Y))\ncorrect_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n.\n.\n.\n    for i in range(total_batch):\n        batch_x, batch_y = x_batches[i], y_batches[i]\n        _, c, a = sess.run([optimizer, cost, accuracy], \n                        feed_dict={\n                            X: batch_x, \n                            Y: batch_y, \n                            keep_prob: 0.8\n                        })\n        print(a)\n'
'from keras.layers import Input, concatenate\n\nx = Input(shape=...)\ny = Input(shape=...)\n\nshared_layer = MySharedLayer(...)\nout_x = shared_layer(x)\nout_y = shared_layer(y)\n\nconcat = concatenate([out_x, out_y])\n\n# pass concat to other layers ...\n'
"from gensim.models import doc2vec\nfrom scipy import spatial\n\nd2v_model = doc2vec.Doc2Vec.load(model_file)\n\nfisrt_text = '..'\nsecond_text = '..'\n\nvec1 = d2v_model.infer_vector(fisrt_text.split())\nvec2 = d2v_model.infer_vector(second_text.split())\n\ncos_distance = spatial.distance.cosine(vec1, vec2)\n# cos_distance indicates how much the two texts differ from each other:\n# higher values mean more distant (i.e. different) texts\n"
"from tensorflow.keras.models import Model\ndef  Mymodel(backbone_model, classes):\n    backbone = backbone_model\n    x = backbone.output\n    x = tf.keras.layers.Dense(classes,activation='sigmoid')(x)\n    model = Model(inputs=backbone.input, outputs=x)\n    return model\n\ninput_shape = (224, 224, 3)\nmodel = Mymodel(backbone_model=tf.keras.applications.MobileNet(input_shape=input_shape, include_top=False, pooling='avg'),\n                classes=61)\n\nmodel.summary()\n"
"finished = false\nwhile not finished:\n    trueLine = testFile.readline()\n    if not trueLine: # end of file\n        finished = true\n    else:\n        trueLine = trueLine.split() # tokenise by whitespace\n        taggedLine = taggedFile.readline()\n        if not taggedLine:\n            print 'Error: files are out of sync.'\n        taggedLine = taggedLine.split()\n        if len(trueLine) != len(taggedLine):\n            print 'Error: files are out of sync.'\n        for i in range(len(trueLine)):\n            truePair = trueLine[i].split('/')\n            taggedPair = taggedLine[i].split('/')\n            if truePair[0] != taggedPair[0]: # the words should match\n                print 'Error: files are out of sync.'\n            trueTag = truePair[1]\n            guessedTag = taggedPair[1]\n            if trueTag == guessedTag:\n                totals[trueTag]['truePositives'] += 1\n            else:\n                totals[trueTag]['falseNegatives'] += 1\n                totals[guessedTag]['falsePositives'] += 1\n"
"class Seeder:\n    def __init__(self):\n        self.seeds = set()\n        self.cache = dict()\n\n    def get_seed(self, word):\n        LIMIT = 2\n        seed = self.cache.get(word,None)\n        if seed is not None:\n            return seed\n        for seed in self.seeds:\n            if self.distance(seed, word) &lt;= LIMIT:\n                self.cache[word] = seed\n                return seed\n        self.seeds.add(word)\n        self.cache[word] = word\n        return word\n\n    def distance(self, s1, s2):\n        l1 = len(s1)\n        l2 = len(s2)\n        matrix = [range(zz,zz + l1 + 1) for zz in xrange(l2 + 1)]\n        for zz in xrange(0,l2):\n            for sz in xrange(0,l1):\n                if s1[sz] == s2[zz]:\n                    matrix[zz+1][sz+1] = min(matrix[zz+1][sz] + 1, matrix[zz][sz+1] + 1, matrix[zz][sz])\n                else:\n                    matrix[zz+1][sz+1] = min(matrix[zz+1][sz] + 1, matrix[zz][sz+1] + 1, matrix[zz][sz] + 1)\n        return matrix[l2][l1]\n\nimport itertools\n\ndef group_similar(words):\n    seeder = Seeder()\n    words = sorted(words, key=seeder.get_seed)\n    groups = itertools.groupby(words, key=seeder.get_seed)\n    return [list(v) for k,v in groups]\n\nimport pprint\n\nprint pprint.pprint(group_similar([\n    'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have',\n    'I', 'it', 'for', 'not', 'on', 'with', 'he', 'as', 'you',\n    'do', 'at', 'this', 'but', 'his', 'by', 'from', 'they', 'we',\n    'say', 'her', 'she', 'or', 'an', 'will', 'my', 'one', 'all',\n    'would', 'there', 'their', 'what', 'so', 'up', 'out', 'if',\n    'about', 'who', 'get', 'which', 'go', 'me', 'when', 'make',\n    'can', 'like', 'time', 'no', 'just', 'him', 'know', 'take',\n    'people', 'into', 'year', 'your', 'good', 'some', 'could',\n    'them', 'see', 'other', 'than', 'then', 'now', 'look',\n    'only', 'come', 'its', 'over', 'think', 'also', 'back',\n    'after', 'use', 'two', 'how', 'our', 'work', 'first', 'well',\n    'way', 'even', 'new', 'want', 'because', 'any', 'these',\n    'give', 'day', 'most', 'us'\n]), width=120)\n\n[['after'],\n ['also'],\n ['and', 'a', 'in', 'on', 'as', 'at', 'an', 'one', 'all', 'can', 'no', 'want', 'any'],\n ['back'],\n ['because'],\n ['but', 'about', 'get', 'just'],\n ['first'],\n ['from'],\n ['good', 'look'],\n ['have', 'make', 'give'],\n ['his', 'her', 'if', 'him', 'its', 'how', 'us'],\n ['into'],\n ['know', 'new'],\n ['like', 'time', 'take'],\n ['most'],\n ['of', 'I', 'it', 'for', 'not', 'he', 'you', 'do', 'by', 'we', 'or', 'my', 'so', 'up', 'out', 'go', 'me', 'now'],\n ['only'],\n ['over', 'our', 'even'],\n ['people'],\n ['say', 'she', 'way', 'day'],\n ['some', 'see', 'come'],\n ['the', 'be', 'to', 'that', 'this', 'they', 'there', 'their', 'them', 'other', 'then', 'use', 'two', 'these'],\n ['think'],\n ['well'],\n ['what', 'who', 'when', 'than'],\n ['with', 'will', 'which'],\n ['work'],\n ['would', 'could'],\n ['year', 'your']]\n"
'  #Compute log-likelihood \n  #NLTK Naive bayes classifier prob_classify func gives logprob(class) + logprob(doc|class))\n  #for labeled data, sum logprobs output by the classifier for the label\n  #for unlabeled data, sum logprobs output by the classifier for each label\n\n  log_lh = sum([C.prob_classify(ftdic).prob(label) for (ftdic,label) in labeled_data])\n  log_lh += sum([C.prob_classify(ftdic).prob(label) for (ftdic,ignore) in unlabeled_data for label in l_freqdist_act.samples()])\n'
"In [3]: rf = RandomForestRegressor(n_estimators=10, min_samples_split=2, n_jobs=-1)\n\nIn [4]: rf.fit([[1,2,3],[4,5,6]],[-1,1])\nOut[4]: \nRandomForestRegressor(bootstrap=True, compute_importances=False,\n           criterion='mse', max_depth=None, max_features='auto',\n           min_density=0.1, min_samples_leaf=1, min_samples_split=2,\n           n_estimators=10, n_jobs=-1, oob_score=False,\n           random_state=&lt;mtrand.RandomState object at 0x7fd894d59528&gt;,\n           verbose=0)\n\nIn [5]: rf.predict([1,2,3])\nOut[5]: array([-0.6])\n\nIn [6]: rf.predict([[1,2,3],[4,5,6]])\nOut[6]: array([-0.6,  0.4])\n"
'[0] is the feature vector of the first data example\n[1] is the feature vector of the second data example\n....\n[[0],[1],[2],[3]] is a list of all data examples, \n  each example has only 1 feature.\n\n    print(neigh.predict_proba([[0.9]]))\n\n    [0] # green label\n    [[ 0.66666667  0.33333333]]  # green label has greater probability\n\nX = [ [h1, e1, s1], \n      [h2, e2, s2],\n      ...\n    ]\ny = [label1, label2, ..., ]\n'
'library(&quot;CHAID&quot;)\n \n### fit tree to subsample\nset.seed(290875)\nUSvoteS &lt;- USvote[sample(1:nrow(USvote), 1000),]\nctrl &lt;- chaid_control(minsplit = 200, minprob = 0.1)\nchaidUS &lt;- chaid(vote3 ~ ., data = USvoteS, control = ctrl)\nprint(chaidUS)\n'
'train_set = np.array(train_set)\ntest_set = np.array(test_set)\n\npca.fit(train_set)\npca.fit(test_set)\n\ntrain_set = pca.transform(train_set) "line where error occurs"\ntest_set = pca.transform(test_set)\n\ntrain_set = np.array([[]])\npca.fit(train_set)\n\nimage.wide = image.reshape(1,s)\n\nimage_wide = image.reshape(1,s)\nreturn image_wide[0]\n\nprint("changing size from %s to %s" % str(image.size), str(Standard_size))\n\nprint("changing size from %s to %s" % (str(image.size), str(Standard_size)))\n\ntrain_set = [flatten_image(matrix_image(image)) for image in train_images]\ntest_set = [flatten_image(matrix_image(image)) for image in test_images]\n'
'pip install arff\n# or easy_install arff\n# or pypm install arff\n'
'from collections import namedtuple\nfrom keyword import iskeyword\nimport re\n\ndef NotDone(msg):\n    raise NotImplemented(msg)\n\ndef nominal(spec):\n    """\n    Create an ARFF nominal (enumerated) data type\n    """\n    spec = spec.lstrip("{ \\t").rstrip("} \\t")\n    good_values = set(val.strip() for val in spec.split(","))\n\n    def fn(s):\n        s = s.strip()\n        if s in good_values:\n            return s\n        else:\n            raise ValueError("\'{}\' is not a recognized value".format(s))\n\n    # patch docstring\n    fn.__name__ = "nominal"\n    fn.__doc__ = """\n    ARFF nominal (enumerated) data type\n\n    Legal values are {}\n    """.format(sorted(good_values))\n    return fn\n\ndef numeric(s):\n    """\n    Convert string to int or float\n    """\n    try:\n        return int(s)\n    except ValueError:\n        return float(s)\n\nfield_maker = {\n    "date":       (lambda spec: NotDone("date data type not implemented")),\n    "integer":    (lambda spec: int),\n    "nominal":    (lambda spec: nominal(spec)),\n    "numeric":    (lambda spec: numeric),\n    "string":     (lambda spec: str),\n    "real":       (lambda spec: float),\n    "relational": (lambda spec: NotDone("relational data type not implemented")),\n}\n\ndef file_lines(fname):\n    # lazy file reader; ensures file is closed when done,\n    # returns lines without trailing spaces or newline\n    with open(fname) as inf:\n        for line in inf:\n            yield line.rstrip()\n\ndef no_data_yet(*items):\n    raise ValueError("AarfRow not fully defined (haven\'t seen a @data directive yet)")\n\ndef make_field_name(s):\n    """\n    Mangle string to make it a valid Python identifier\n    """\n    s = s.lower()                               # force to lowercase\n    s = "_".join(re.findall("[a-z0-9]+", s))    # strip all invalid chars; join what\'s left with "_"\n    if iskeyword(s) or re.match("[0-9]", s):    # if the result is a keyword or starts with a digit\n        s = "f_"+s                              #   make it a safe field name\n    return s  \n\nclass ArffReader:\n    line_types = ["blank", "comment", "relation", "attribute", "data"]\n\n    def __init__(self, fname):\n        # get input file\n        self.fname = fname\n        self.lines = file_lines(fname)\n\n        # prepare to read file header\n        self.relation = \'(not specified)\'\n        self.data_names = []\n        self.data_types = []\n        self.dtype = no_data_yet\n\n        # read file header\n        line_tests = [\n            (getattr(self, "line_is_{}".format(item)), getattr(self, "line_do_{}".format(item)))\n            for item in self.__class__.line_types\n        ]\n        for line in self.lines:\n            for is_, do in line_tests:\n                if is_(line):\n                    done = do(line)\n                    break\n            if done:\n                break\n\n        # use header fields to build data type (and make it print as requested)\n        class ArffRow(namedtuple(\'ArffRow\', self.data_names)):\n            __slots__ = ()\n            def __str__(self):\n                items = (getattr(self, field) for field in self._fields)\n                return "({})".format(", ".join(repr(it) for it in items))\n        self.dtype = ArffRow\n\n    #\n    # figure out input-line type\n    #\n\n    def line_is_blank(self, line):\n        return not line\n\n    def line_is_comment(self, line):\n        return line.lower().startswith(\'%\')\n\n    def line_is_relation(self, line):\n        return line.lower().startswith(\'@relation\')\n\n    def line_is_attribute(self, line):\n        return line.lower().startswith(\'@attribute\')\n\n    def line_is_data(self, line):\n        return line.lower().startswith(\'@data\')\n\n    #    \n    # handle input-line type\n    #    \n\n    def line_do_blank(self, line):\n        pass\n\n    def line_do_comment(self, line):\n        pass\n\n    def line_do_relation(self, line):\n        self.relation = line[10:].strip()\n\n    def line_do_attribute(self, line):\n        m = re.match(\n            "^@attribute"           #   line starts with \'@attribute\'\n            "\\s+"                   #\n            "("                     # name is one of:\n                "(?:\'[^\']+\')"       #   \' string in single-quotes \'\n                "|(?:\\"[^\\"]+\\")"   #   " string in double-quotes "\n                "|(?:[^ \\t\'\\"]+)"   #   single_word_string (no spaces)\n            ")"                     #\n            "\\s+"                   #\n            "("                     # type is one of:\n                "(?:{[^}]+})"       #   { set, of, nominal, values }\n                "|(?:\\w+)"          #   datatype\n            ")"                     #\n            "\\s*"                   #\n            "("                     # spec string\n                ".*"                #   anything to end of line\n            ")$",                   #\n            line, flags=re.I)       #   case-insensitive\n        if m:\n            name, type_, spec = m.groups()\n            self.data_names.append(make_field_name(name))\n            if type_[0] == \'{\':\n                type_, spec = \'nominal\', type_\n            self.data_types.append(field_maker[type_](spec))\n        else:\n            raise ValueError("failed parsing attribute line \'{}\'".format(line))\n\n    def line_do_data(self, line):\n        return True  # flag end of header\n\n    #\n    # make the class iterable\n    #\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        """\n        Return one data row at a time\n        """\n        data = next(self.lines).split(\',\')\n        return self.dtype(*(fn(dat) for fn,dat in zip(self.data_types, data)))\n\nfor row in ArffReader(\'mydata.arff\'):\n    print(row)\n\n(63.0, \'male\', \'typ_angina\', 145.0, 233.0, \'t\', \'left_vent_hyper\', 150.0, \'no\', 2.3, \'down\', 0.0, \'fixed_defect\', \'negative\')\n(37.0, \'male\', \'non_anginal\', 130.0, 250.0, \'f\', \'normal\', 187.0, \'no\', 3.5, \'down\', 0.0, \'normal\', \'negative\')\n(41.0, \'female\', \'atyp_angina\', 130.0, 204.0, \'f\', \'left_vent_hyper\', 172.0, \'no\', 1.4, \'up\', 0.0, \'normal\', \'negative\')\n(56.0, \'male\', \'atyp_angina\', 120.0, 236.0, \'f\', \'normal\', 178.0, \'no\', 0.8, \'up\', 0.0, \'normal\', \'negative\')\n(57.0, \'female\', \'asympt\', 120.0, 354.0, \'f\', \'normal\', 163.0, \'yes\', 0.6, \'up\', 0.0, \'normal\', \'negative\')\n(57.0, \'male\', \'asympt\', 140.0, 192.0, \'f\', \'normal\', 148.0, \'no\', 0.4, \'flat\', 0.0, \'fixed_defect\', \'negative\')\n(56.0, \'female\', \'atyp_angina\', 140.0, 294.0, \'f\', \'left_vent_hyper\', 153.0, \'no\', 1.3, \'flat\', 0.0, \'normal\', \'negative\')\n(44.0, \'male\', \'atyp_angina\', 120.0, 263.0, \'f\', \'normal\', 173.0, \'no\', 0.0, \'up\', 0.0, \'reversable_defect\', \'negative\')\n(52.0, \'male\', \'non_anginal\', 172.0, 199.0, \'t\', \'normal\', 162.0, \'no\', 0.5, \'up\', 0.0, \'reversable_defect\', \'negative\')\n\nfor patient in ArffReader(\'mydata.arff\'):\n    print("{} year old {}".format(patient.age, patient.sex))\n\n63.0 year old male\n37.0 year old male\n41.0 year old female\n56.0 year old male\n57.0 year old female\n57.0 year old male\n56.0 year old female\n44.0 year old male\n52.0 year old male\n\n&gt;&gt;&gt; print(repr(patient))\nArffRow(age=63.0, sex=\'male\', cp=\'typ_angina\', trestbps=145.0, chol=233.0, fbs=\'t\', restecg=\'left_vent_hyper\', thalach=150.0, exang=\'no\', oldpeak=2.3, slope=\'down\', ca=0.0, thal=\'fixed_defect\', f_class=\'negative\')\n'
'd = clf.decision_function(x)[0]\nprobs = np.exp(d) / np.sum(np.exp(d))\n'
'&gt;&gt;&gt; y = linspace(-5, 5, 200)\n&gt;&gt;&gt; X = (y + np.random.randn(200)).reshape(-1, 1)\n&gt;&gt;&gt; threefold = list(KFold(len(y)))\n\n&gt;&gt;&gt; cross_val_score(LinearRegression(), X, y, cv=threefold)\narray([-0.86060164,  0.2035956 , -0.81309259])\n&gt;&gt;&gt; gs = GridSearchCV(LinearRegression(), {}, cv=threefold, verbose=3).fit(X, y) \nFitting 3 folds for each of 1 candidates, totalling 3 fits\n[CV]  ................................................................\n[CV] ...................................... , score=-0.860602 -   0.0s\n[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.0s\n[CV]  ................................................................\n[CV] ....................................... , score=0.203596 -   0.0s\n[CV]  ................................................................\n[CV] ...................................... , score=-0.813093 -   0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s finished\n'
'from sklearn.feature_extraction.text import TfidfVectorizer\nimport numpy as np\n\ntrain_data1 = ["football is the sport", "gravity is the movie", "education is important"]\nvectorizer = TfidfVectorizer(stop_words=\'english\')\n\nprint("Applying first train data")\nX_train = vectorizer.fit_transform(train_data1)\nprint(vectorizer.get_feature_names())\n\nprint("\\n\\nApplying second train data")\ntrain_data2 = ["cricket", "Transformers is a film", "AIMS is a college"]\nX_train = vectorizer.transform(train_data2)\nprint(vectorizer.get_feature_names())\n\nprint("\\n\\nApplying fit transform onto second train data")\nX_train = vectorizer.fit_transform(train_data1 + train_data2)\nprint(vectorizer.get_feature_names())\n\nApplying first train data\n[\'education\', \'football\', \'gravity\', \'important\', \'movie\', \'sport\']\n\nApplying second train data\n[\'education\', \'football\', \'gravity\', \'important\', \'movie\', \'sport\']\n\nApplying fit transform onto second train data\n[\'aims\', \'college\', \'cricket\', \'education\', \'film\', \'football\', \'gravity\', \'important\', \'movie\', \'sport\', \'transformers\']\n'
'(y_train == model.predict(X_train)).shape == (301, 301)\n\nnp.mean(y_train.ravel() == model.predict(X_train))\n\nmodel.score(X_train, y_train)\n'
'&gt;&gt;&gt; clf = cluster.KMeans()\n&gt;&gt;&gt; score = cross_validation.cross_val_score(clf, X)\n# (no error)    \n\n&gt;&gt;&gt; clf = cluster.KMeans()\n&gt;&gt;&gt; clf.fit(X)\n&gt;&gt;&gt; type(clf.score(X))\nnumpy.float64\n\n&gt;&gt;&gt; clf = mixture.GMM()\n&gt;&gt;&gt; clf.fit(X)\n&gt;&gt;&gt; type(clf.score(X))\nnumpy.ndarray\n\n&gt;&gt;&gt; scorer = lambda est, data: np.mean(est.score(data))\n\n&gt;&gt;&gt; score = cross_validation.cross_val_score(clf, X, scoring=scorer)\n'
'from sklearn.cross_validation import ShuffleSplit\nfrom collections import defaultdict\n\nnames = db_train.iloc[:,1:].columns.tolist()\n\n# -- Gridsearched parameters\nmodel_rf = RandomForestClassifier(n_estimators=500,\n                                 class_weight="auto",\n                                 criterion=\'gini\',\n                                 bootstrap=True,\n                                 max_features=10,\n                                 min_samples_split=1,\n                                 min_samples_leaf=6,\n                                 max_depth=3,\n                                 n_jobs=-1)\nscores = defaultdict(list)\n\n# -- Fit the model (could be cross-validated)\nrf = model_rf.fit(X_train, Y_train)\nacc = roc_auc_score(Y_test, rf.predict(X_test))\n\nfor i in range(X_train.shape[1]):\n    X_t = X_test.copy()\n    np.random.shuffle(X_t[:, i])\n    shuff_acc = roc_auc_score(Y_test, rf.predict(X_t))\n    scores[names[i]].append((acc-shuff_acc)/acc)\n\nprint("Features sorted by their score:")\nprint(sorted([(round(np.mean(score), 4), feat) for\n              feat, score in scores.items()], reverse=True))\n\nFeatures sorted by their score:\n[(0.0028999999999999998, \'Var1\'), (0.0027000000000000001, \'Var2\'), (0.0023999999999999998, \'Var3\'), (0.0022000000000000001, \'Var4\'), (0.0022000000000000001, \'Var5\'), (0.0022000000000000001, \'Var6\'), (0.002, \'Var7\'), (0.002, \'Var8\'), ...]\n'
'alphas = np.abs(svm.dual_coef_)\n\nsvm.dual_coef_[i] = labels[i] * alphas[i]\n\nlabels = np.sign(svm.dual_coef_)\n'
' ===# EPOCH 0 #===\nError: 0.370000004768\n ===# EPOCH 1 #===\nError: 0.333999991417\n ===# EPOCH 2 #===\nError: 0.282000005245\n ===# EPOCH 3 #===\nError: 0.222000002861\n ===# EPOCH 4 #===\nError: 0.152000010014\n ===# EPOCH 5 #===\nError: 0.111999988556\n ===# EPOCH 6 #===\nError: 0.0680000185966\n ===# EPOCH 7 #===\nError: 0.0239999890327\n ===# EPOCH 8 #===\nError: 0.00999999046326\n ===# EPOCH 9 #===\nError: 0.00400000810623\n'
"df['action'] = df.keyword.str.split('_').str.get(-1)\ndf['keyword'] = df.keyword.str.split('_').str.get(0)\ndf = df.set_index(['datetime', 'keyword', 'action']).unstack().loc[:, 'COUNT']\ndf['ctr'] = df.click.div(df.pv)\n\n\naction              click   pv   ctr\ndatetime   keyword                  \n2016-01-05 a          100  200  0.50\n           b           90  150  0.60\n           c           90  120  0.75\n"
'import numpy as np\nimport theano.tensor as T\nT.config.floatX = \'float32\'\ndataPoints = np.random.random((5000, 256 * 256)).astype(T.config.floatX)\n#float32 data type requires 4 bytes\nsizeinGBs = 5000 * 256 * 256 * 4 / 1024. / 1024 / 1024 + (some small over-head constant)\nprint "Data will need %2f GBs of free memory" % sizeInGB\n\n&gt;&gt;&gt; Data will need 1.22 GBs of free memory\n\nimport theano.sandbox.cuda.basic_ops as sbcuda\nimport numpy as np\nimport theano.tensor as T\nT.config.floatX = \'float32\'\nGPUFreeMemoryInBytes = sbcuda.cuda_ndarray.cuda_ndarray.mem_info()[0]\nfreeGPUMemInGBs = GPUFreeMemoryInBytes/1024./1024/1024\nprint "Your GPU has %s GBs of free memory" % str(freeGPUMemInGBs)\n#An operation is to be executed below\ntestData = shared(np.random.random((5000, 256 * 256)).astype(T.config.floatX), borrow = True)\nprint "The tasks above used %s GBs of your GPU memory. The available memory is %s GBs" % (str(freeGPUMemInGBs - GPUFreeMemoryInBytes/1024./1024/1024), str(GPUFreeMemoryInBytes/1024./1024/1024))\n\n&gt;&gt;&gt; Your GPU has 11.2557678223 GBs of free memory\n&gt;&gt;&gt; The tasks above used 1.22077941895 GBs of your GPU memory. The available memory is 10.0349884033 GBs\n'
'W = tf.Variable(\n    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0) ,name="W")\nembedding_layer = tf.nn.embedding_lookup(W, _X)\n\n# Reduce along dimension 1 (`n_input`) to get a single vector (row)\n# per input example.\nembedding_aggregated = tf.reduce_sum(embedding_layer, [1])\n\nlayer_1 = tf.nn.sigmoid(tf.add(tf.matmul(\n    embedding_aggregated, _weights[\'h1\']), _biases[\'b1\'])) \n'
'    W = tf.Variable(tf.truncated_normal([115713, 2], dtype=tf.float64))\n    b = tf.Variable(tf.zeros([2], dtype=tf.float64))\n'
'%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom six.moves import cPickle \n\nf = open(\'data/cifar10/cifar-10-batches-py/data_batch_1\', \'rb\')\ndatadict = cPickle.load(f,encoding=\'latin1\')\nf.close()\nX = datadict["data"] \nY = datadict[\'labels\']\nX = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype("uint8")\nY = np.array(Y)\n\n#Visualizing CIFAR 10\nfig, axes1 = plt.subplots(5,5,figsize=(3,3))\nfor j in range(5):\n    for k in range(5):\n        i = np.random.choice(range(len(X)))\n        axes1[j][k].set_axis_off()\n        axes1[j][k].imshow(X[i:i+1][0])\n'
'X = [[0], [1], [2], [3]]\ny = [0, 0, 1, 1]\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y) \nprint(neigh.predict([[1.1]]))\nprint(neigh.predict_proba([[0.9]]))\n'
'docker run -it grc.io/tensorflow/tensorflow /bin/bash\n\nsudo apt-get install python-pandas\n\nsudo docker ps –a # Get list of all containers previously started with run command\n\nsudo docker commit container_id image_name \n'
'np.diag(s)\n\narray([[ 2.00604441,  0.        ,  0.        ,  0.        ,  0.        ,         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  1.22160478,  0.        ,  0.        ,  0.        ,         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  1.09816315,  0.        ,  0.        ,         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.97748473,  0.        ,         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.81374786,         0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,         0.77634993,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,         0.        ,  0.73250287,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,         0.        ,  0.        ,  0.65854628,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,         0.        ,  0.        ,  0.        ,  0.27985695,  0.        ],\n       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,         0.        ,  0.        ,  0.        ,  0.        ,  0.09252313]])\n'
'model = Sequential()\nmodel.add(Reshape((3, 4), input_shape=(12,)))\n# now: model.output_shape == (None, 3, 4)\n# note: `None` is the batch dimension\n\nmodel.add(Reshape((6, 2)))\n# now: model.output_shape == (None, 6, 2)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, Reshape\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\nmodel.add(Dense(150, input_dim=23,init=\'normal\',activation=\'relu\'))\nmodel.add(Dense(80,activation=\'relu\',init=\'normal\'))\nmodel.add(Reshape((1, 80)))\nmodel.add(SimpleRNN(2,init=\'normal\')) \nadam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(loss="mean_squared_error", optimizer="rmsprop")\n'
'if len(x.shape) == 0:\n\nTypeError("Singleton array %r cannot be considered a valid collection." % x)\n'
'#define a function to conver label to letter\ndef letter(i):\n    return \'abcdefghij\'[i]\n\n\n# you need a matplotlib inline to be able to show images in python notebook\n%matplotlib inline\n#some random number in range 0 - length of dataset\nsample_idx = np.random.randint(0, len(train_dataset))\n#now we show it\nplt.imshow(train_dataset[sample_idx])\nplt.title("Char " + letter(train_labels[sample_idx]))\n\n#will give you train_dataset and labels\ntrain_dataset = loaded_pickle[\'train_dataset\']\ntrain_labels = loaded_pickle[\'train_labels\']\n'
"import re, math\nfrom collections import Counter\n\nWORD = re.compile(r'\\w+')\n\ndef get_cosine(vec1, vec2):\n    # print vec1, vec2\n    intersection = set(vec1.keys()) &amp; set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef text_to_vector(text):\n    return Counter(WORD.findall(text))\n\ndef get_similarity(a, b):\n    a = text_to_vector(a.strip().lower())\n    b = text_to_vector(b.strip().lower())\n\n    return get_cosine(a, b)\n\nget_similarity('L &amp; L AIR CONDITIONING', 'L &amp; L AIR CONDITIONING Service') # returns 0.9258200997725514\n\nimport re, math\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom nltk.corpus import wordnet as wn\n\nstop = stopwords.words('english')\n\nWORD = re.compile(r'\\w+')\nstemmer = PorterStemmer()\n\ndef get_cosine(vec1, vec2):\n    # print vec1, vec2\n    intersection = set(vec1.keys()) &amp; set(vec2.keys())\n    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n\n    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n\n    if not denominator:\n        return 0.0\n    else:\n        return float(numerator) / denominator\n\ndef text_to_vector(text):\n    words = WORD.findall(text)\n    a = []\n    for i in words:\n        for ss in wn.synsets(i):\n            a.extend(ss.lemma_names())\n    for i in words:\n        if i not in a:\n            a.append(i)\n    a = set(a)\n    w = [stemmer.stem(i) for i in a if i not in stop]\n    return Counter(w)\n\ndef get_similarity(a, b):\n    a = text_to_vector(a.strip().lower())\n    b = text_to_vector(b.strip().lower())\n\n    return get_cosine(a, b)\n\ndef get_char_wise_similarity(a, b):\n    a = text_to_vector(a.strip().lower())\n    b = text_to_vector(b.strip().lower())\n    s = []\n\n    for i in a:\n        for j in b:\n            s.append(get_similarity(str(i), str(j)))\n    try:\n        return sum(s)/float(len(s))\n    except: # len(s) == 0\n        return 0\n\nget_similarity('I am a good boy', 'I am a very disciplined guy')\n# Returns 0.5491201525567068\n"
'import numpy as np\n\nW_val, b_val = sess.run([W, b])\n\nnp.savetxt("W.csv", W_val, delimiter=",")\nnp.savetxt("b.csv", b_val, delimiter=",")\n'
'valid = TrainValidationSplit(\n    estimator=pipeline,\n    estimatorParamMaps=paramGrid,\n    evaluator=evaluator,            \n    collectSubModels=True)\n\nmodel = valid.fit(df)\n\nmodel.subModels\n\nfrom typing import Dict, Tuple, List, Any\nfrom pyspark.ml.param import Param\nfrom pyspark.ml.tuning import TrainValidationSplitModel\n\nEvalParam = List[Tuple[float, Dict[Param, Any]]]\n\ndef get_metrics_and_params(model: TrainValidationSplitModel) -&gt; EvalParam:\n    return list(zip(model.validationMetrics, model.getEstimatorParamMaps()))\n\nmodels = pipeline.fit(df, params=paramGrid)\n\nzip(models, params)\n'
'from oauth2client.client import GoogleCredentials\nfrom googleapiclient import discovery\n\nprojectID = \'projects/&lt;your_project_id&gt;\'\nmodelName = projectID+\'/models/&lt;your_model_name&gt;\'\n\ncredentials = GoogleCredentials.get_application_default()\n\n\nml = discovery.build(\'ml\', \'v1beta1\', credentials=credentials)\n# Create a dictionary with the fields from the request body.\nrequestDict = {"instances":[\n                    {"image": [0.0,..., 0.0, 0.0], "key": 0}\n        ]}\n# Create a request to call projects.models.create.\nrequest = ml.projects().predict(\n      name=modelName,\n      body=requestDict)\nresponse = request.execute()\n'
'from sklearn.datasets import make_regression\n\nX,y = make_regression(n_samples=1000, n_features=6,\n                                 n_informative=3, n_targets=6,  \n                                 tail_strength=0.5, noise=0.02, \n                                 shuffle=True, coef=False, random_state=0)\n\n# Convert to a pandas dataframe like in your example\nicols = [\'i0\',\'i1\',\'i2\',\'i3\',\'i4\',\'i5\']\njcols = [\'j0\', \'j1\', \'j2\', \'j3\', \'j4\', \'j5\']\ndf = pd.concat([pd.DataFrame(X, columns=icols),\n                pd.DataFrame(y, columns=jcols)], axis=1)\n\n# Introduce a few np.nans in there\ndf.loc[0, jcols] = np.nan\ndf.loc[10, jcols] = np.nan\ndf.loc[100, jcols] = np.nan\n\ndf.head()\n\nOut:\n     i0    i1    i2    i3    i4    i5     j0     j1     j2     j3     j4  \\\n0 -0.21 -0.18 -0.06  0.27 -0.32  0.00    NaN    NaN    NaN    NaN    NaN   \n1  0.65 -2.16  0.46  1.82  0.22 -0.13  33.08  39.85   9.63  13.52  16.72   \n2 -0.75 -0.52 -1.08  0.14  1.12 -1.05  -0.96 -96.02  14.37  25.19 -44.90   \n3  0.01  0.62  0.20  0.53  0.35 -0.73   6.09 -12.07 -28.88  10.49   0.96   \n4  0.39 -0.70 -0.55  0.10  1.65 -0.69  83.15  -3.16  93.61  57.44 -17.33   \n\n      j5  \n0    NaN  \n1  17.79  \n2 -77.48  \n3 -35.61  \n4  -2.47  \n\nnotnans = df[jcols].notnull().all(axis=1)\ndf_notnans = df[notnans]\n\n# Split into 75% train and 25% test\nX_train, X_test, y_train, y_test = train_test_split(df_notnans[icols], df_notnans[jcols],\n                                                    train_size=0.75,\n                                                    random_state=4)\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.model_selection import train_test_split\n\nregr_multirf = MultiOutputRegressor(RandomForestRegressor(max_depth=30,\n                                                          random_state=0))\n\n# Fit on the train data\nregr_multirf.fit(X_train, y_train)\n\n# Check the prediction score\nscore = regr_multirf.score(X_test, y_test)\nprint("The prediction score on the test data is {:.2f}%".format(score*100))\n\nOut: The prediction score on the test data is 96.76%\n\ndf_nans = df.loc[~notnans].copy()\ndf_nans[jcols] = regr_multirf.predict(df_nans[icols])\ndf_nans\n\n           i0        i1        i2        i3        i4        i5         j0  \\\n0   -0.211620 -0.177927 -0.062205  0.267484 -0.317349  0.000341 -41.254983   \n10   1.138974 -1.326378  0.123960  0.982841  0.273958  0.414307  46.406351   \n100 -0.682390 -1.431414 -0.328235 -0.886463  1.212363 -0.577676  94.971966   \n\n            j1         j2         j3         j4         j5  \n0   -18.197513 -31.029952 -14.749244  -5.990595  -9.296744  \n10   67.915628  59.750032  15.612843  10.177314  38.226387  \n100  -3.724223  65.630692  44.636895 -14.372414  11.947185  \n'
'# Train the model (a.k.a. `fit` training data to it).\nlr.fit(train_df[features_columns], train_df["target"])\n# Use the model to make predictions based on testing data.\ny_pred = lr.predict(test_df[feature_columns])\n# Compare the predicted y values to actual y values.\naccuracy = (y_pred == test_df["target"]).mean()\n'
'&gt;&gt;&gt; affprop = sklearn.cluster.AffinityPropagation(affinity="precomputed", damping=.95)\n&gt;&gt;&gt; affprop.fit(similarity)\nAffinityPropagation(affinity=\'precomputed\', convergence_iter=15, copy=True,\n          damping=0.95, max_iter=200, preference=None, verbose=False)\n&gt;&gt;&gt; print affprop.labels_\n[0 0 0 1]\n\n&gt;&gt;&gt; c = [[0], [0], [0], [0], [0], [0], [0], [0]]\n&gt;&gt;&gt; af = sklearn.cluster.AffinityPropagation (affinity = \'euclidean\').fit (c)\n&gt;&gt;&gt; print (af.labels_)\n[0 1 0 1 2 1 1 0]\n\n&gt;&gt;&gt; c = [[0], [0], [0], [0], [0], [0], [0], [0]]\n&gt;&gt;&gt; af = sklearn.cluster.AffinityPropagation (affinity = \'euclidean\', damping=.99).fit (c)\n&gt;&gt;&gt; print (af.labels_)\n[0 0 0 0 0 0 0 0]\n\n&gt;&gt;&gt; c = [[0], [0], [0], [1], [2], [1], [2], [1]]\n&gt;&gt;&gt; af = sklearn.cluster.AffinityPropagation (affinity = \'euclidean\', damping=.5).fit (c)\n&gt;&gt;&gt; print (af.labels_)\n[0 0 0 2 1 2 1 2]\n'
'gru_out = Bidirectional(GRU(hiddenlayer_num, return_sequences=True))(embedded)\nmax_pooled = GlobalMaxPooling1D(gru_out)\n'
'#vecotrizing text\ntf_vectorizer = CountVectorizer(max_df=0.5, min_df=2,\n                                max_features=n_features,\n                                stop_words=\'english\')\n\n#getting the TF matrix\ntf = tf_vectorizer.fit_transform(df.pop(\'reviewText\'))\n\n# adding "features" columns as SparseSeries\nfor i, col in enumerate(tf_vectorizer.get_feature_names()):\n    df[col] = pd.SparseSeries(tf[:, i].toarray().ravel(), fill_value=0)\n\nIn [107]: df.head(3)\nOut[107]:\n        asin  price      reviewerID  LenReview                  Summary  LenSummary  overall  helpful  reviewSentiment         0  \\\n0  151972036   8.48  A14NU55NQZXML2        199  really a difficult read          23        3        2          -0.7203  0.002632\n1  151972036   8.48  A1CSBLAPMYV8Y0         77                      wha           3        4        0          -0.1260  0.005556\n2  151972036   8.48  A1DDECXCGHDYZK        114       wordy and drags on          18        1        4           0.5707  0.004545\n\n   ...    think  thought  trailers  trying  wanted  words  worth  wouldn  writing  young\n0  ...        0        0         0       0       1      0      0       0        0      0\n1  ...        0        0         0       1       0      0      0       0        0      0\n2  ...        0        0         0       0       1      0      1       0        0      0\n\n[3 rows x 78 columns]\n\nIn [108]: df.memory_usage()\nOut[108]:\nIndex               80\nasin               112\nprice              112\nreviewerID         112\nLenReview          112\nSummary            112\nLenSummary         112\noverall            112\nhelpful            112\nreviewSentiment    112\n0                  112\n1                  112\n2                  112\n3                  112\n4                  112\n5                  112\n6                  112\n7                  112\n8                  112\n9                  112\n10                 112\n11                 112\n12                 112\n13                 112\n14                 112\n                  ...\nparts               16   # memory used: # of ones multiplied by 8 (np.int64)\npeter               16\npicked              16\npoint               16\nquick               16\nrating              16\nreader              16\nreading             24\nreally              24\nreviews             16\nstars               16\nstart               16\nstory               32\ntedious             16\nthings              16\nthink               16\nthought             16\ntrailers            16\ntrying              16\nwanted              24\nwords               16\nworth               16\nwouldn              16\nwriting             24\nyoung               16\ndtype: int64\n'
"# Imports library\nfrom sklearn.preprocessing import Imputer\n\n# Create a new instance of the Imputer object\n# Missing values are replaced with NaN\n# Missing values are replaced by the mean later on\n# The axis determines whether you want to move column or row wise\nimputer = Imputer(missing_values='NaN', strategy='mean',axis=0)\n\n# Fit the imputer to X\nimputer = imputer.fit(X[:, 1:3])\n\n# Replace in the original matrix X\n# with the new values after the transformation of X\nX[:, 1:3]=imputer.transform(X[:, 1:3]) \n"
"In [220]: from sklearn.preprocessing import LabelEncoder\n\nIn [221]: x = df.select_dtypes(exclude=['number']) \\\n                .apply(LabelEncoder().fit_transform) \\\n                .join(df.select_dtypes(include=['number']))\n\nIn [228]: x\nOut[228]:\n        status  country  city      datetime  amount\n601766       0        0     1  1.453916e+09     4.5\n669244       0        1     0  1.454109e+09     6.9\n\nIn [230]: classifier.fit(x.drop('status',1), x['status'])\nOut[230]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n"
'from pyspark.sql.functions import collect_list, max, lit\nfrom pyspark.ml.linalg import Vectors, VectorUDT\n\ndef encode(arr, length):\n\n  vec_args =  length, [(x,1.0) for x in arr]\n\n  return Vectors.sparse(*vec_args)   \n\nencode_udf = udf(encode, VectorUDT())\n\nfeats = indexed.agg(max(indexed["newsIndex"])).take(1)[0][0] + 1\n\nindexed.groupBy("uuid") \\\n       .agg(collect_list("newsIndex")\n       .alias("newsArr")) \\\n       .select("uuid", \n               encode_udf("newsArr", lit(feats))\n               .alias("OHE")) \\\n       .show(truncate = False)\n+---------------+-----------------------------------------+\n|uuid           |OHE                                      |\n+---------------+-----------------------------------------+\n|009092130698762|(24,[0],[1.0])                           |\n|010003000431538|(24,[0,3,15],[1.0,1.0,1.0])              |\n|010720006581483|(24,[11],[1.0])                          |\n|010216216021063|(24,[10,22],[1.0,1.0])                   |\n|001436800277225|(24,[2,12,23],[1.0,1.0,1.0])             |\n|011425002581540|(24,[1,5,9],[1.0,1.0,1.0])               |\n|010156461231357|(24,[13,18],[1.0,1.0])                   |\n|011199797794333|(24,[7,8,17,19,20],[1.0,1.0,1.0,1.0,1.0])|\n|011414545455156|(24,[4,6,14,21],[1.0,1.0,1.0,1.0])       |\n|011337201765123|(24,[1,16],[1.0,1.0])                    |\n+---------------+-----------------------------------------+\n'
'    floatValues[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF) / 255;\n    floatValues[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF) / 255;\n    floatValues[i * 3 + 2] = (val &amp; 0xFF) / 255;\n\n((val &amp; 0xff) / 256.0) + (0.5/256.0)\n'
"######################### import stuff ##########################\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.datasets import load_linnerud\nfrom sklearn.model_selection import train_test_split\n\n######################## prepare the data ########################\nX, y = load_linnerud(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)\n\n######################## set learning variables ##################\nlearning_rate = 0.0005\nepochs = 2000\nbatch_size = 3\n\n######################## set some variables #######################\nx = tf.placeholder(tf.float32, [None, 3], name='x')  # 3 features\ny = tf.placeholder(tf.float32, [None, 3], name='y')  # 3 outputs\n\n# hidden layer 1\nW1 = tf.Variable(tf.truncated_normal([3, 10], stddev=0.03), name='W1')\nb1 = tf.Variable(tf.truncated_normal([10]), name='b1')\n\n# hidden layer 2\nW2 = tf.Variable(tf.truncated_normal([10, 3], stddev=0.03), name='W2')\nb2 = tf.Variable(tf.truncated_normal([3]), name='b2')\n\n######################## Activations, outputs ######################\n# output hidden layer 1\nhidden_out = tf.nn.relu(tf.add(tf.matmul(x, W1), b1))\n\n# total output\ny_ = tf.nn.relu(tf.add(tf.matmul(hidden_out, W2), b2))\n\n####################### Loss Function  #########################\nmse = tf.losses.mean_squared_error(y, y_)\n\n####################### Optimizer      #########################\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(mse)\n\n###################### Initialize, Accuracy and Run #################\n# initialize variables\ninit_op = tf.global_variables_initializer()\n\n# accuracy for the test set\naccuracy = tf.reduce_mean(tf.square(tf.subtract(y, y_)))  # or could use tf.losses.mean_squared_error\n\n# run\nwith tf.Session() as sess:\n  sess.run(init_op)\n  total_batch = int(len(y_train) / batch_size)\n  for epoch in range(epochs):\n    avg_cost = 0\n    for i in range(total_batch):\n      batch_x, batch_y = X_train[i * batch_size:min(i * batch_size + batch_size, len(X_train)), :], \\\n                         y_train[i * batch_size:min(i * batch_size + batch_size, len(y_train)), :]\n      _, c = sess.run([optimizer, mse], feed_dict={x: batch_x, y: batch_y})\n      avg_cost += c / total_batch\n    if epoch % 10 == 0:\n      print 'Epoch:', (epoch + 1), 'cost =', '{:.3f}'.format(avg_cost)\n  print sess.run(mse, feed_dict={x: X_test, y: y_test})\n\nEpoch: 1901 cost = 173.914\nEpoch: 1911 cost = 171.928\nEpoch: 1921 cost = 169.993\nEpoch: 1931 cost = 168.110\nEpoch: 1941 cost = 166.277\nEpoch: 1951 cost = 164.492\nEpoch: 1961 cost = 162.753\nEpoch: 1971 cost = 161.061\nEpoch: 1981 cost = 159.413\nEpoch: 1991 cost = 157.808\n482.433\n"
'from sklearn import tree\n\n#load data\nX = [[65,9],[67,7],[70,11],[62,6],[60,7],[72,13],[66,10],[67,7.5]]\nY=["male","female","male","female","female","male","male","female"]\n\n#build model\nclf = tree.DecisionTreeClassifier()\n\n#fit\nclf.fit(X, Y)\n\n#predict\nprediction = clf.predict([[68,9],[66,9]])\n\n#probabilities\nprobs = clf.predict_proba([[68,9],[66,9]])\n\n#print the predicted gender\nprint(prediction)\nprint(probs)\n'
"# prepare the pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.externals import joblib\n\npipe = make_pipeline(StandardScaler(), LogisticRegression)\npipe.fit(X_train, y_train)\njoblib.dump(pipe, 'model.pkl')\n\n#Loading the saved model with joblib\npipe = joblib.load('model.pkl')\n\n# New data to predict\npr = pd.read_csv('set_to_predict.csv')\npred_cols = list(pr.columns.values)[:-1]\n\n# apply the whole pipeline to data\npred = pd.Series(pipe.predict(pr[pred_cols]))\nprint pred\n"
'import keras\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\narray([[ 965,    0,    1,    0,    0,    2,    6,    1,    5,    0],\n       [   0, 1113,    4,    2,    0,    0,    3,    0,   13,    0],\n       [   8,    0,  963,   14,    5,    1,    7,    8,   21,    5],\n       [   0,    0,    3,  978,    0,    7,    0,    6,   12,    4],\n       [   1,    0,    4,    0,  922,    0,    9,    3,    3,   40],\n       [   4,    1,    1,   27,    0,  824,    6,    1,   20,    8],\n       [  11,    3,    1,    1,    5,    6,  925,    0,    6,    0],\n       [   2,    6,   17,    8,    2,    0,    1,  961,    2,   29],\n       [   5,    1,    2,   13,    4,    6,    2,    6,  929,    6],\n       [   6,    5,    0,    7,    5,    6,    1,    6,   10,  963]])\n\nTruePositive = np.diag(cm1)\nTruePositive\n# array([ 965, 1113,  963,  978,  922,  824,  925,  961,  929,  963])\n\nFalsePositive = []\nfor i in range(num_classes):\n    FalsePositive.append(sum(cm1[:,i]) - cm1[i,i])\nFalsePositive\n# [37, 16, 33, 72, 21, 28, 35, 31, 92, 92]\n\nFalseNegative = []\nfor i in range(num_classes):\n    FalseNegative.append(sum(cm1[i,:]) - cm1[i,i])\nFalseNegative\n# [15, 22, 69, 32, 60, 68, 33, 67, 45, 46]\n\nTrueNegative = []\nfor i in range(num_classes):\n    temp = np.delete(cm1, i, 0)   # delete ith row\n    temp = np.delete(temp, i, 1)  # delete ith column\n    TrueNegative.append(sum(sum(temp)))\nTrueNegative\n# [8998, 8871, 9004, 8950, 9057, 9148, 9040, 9008, 8979, 8945]\n\nl = len(y_test)\nfor i in range(num_classes):\n    print(TruePositive[i] + FalsePositive[i] + FalseNegative[i] + TrueNegative[i] == l)\n\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n'
'array([[ 963,    0,    0,    1,    0,    2,   11,    1,    2,    0],\n       [   0, 1119,    3,    2,    1,    0,    4,    1,    4,    1],\n       [  12,    3,  972,    9,    6,    0,    6,    9,   13,    2],\n       [   0,    0,    8,  975,    0,    2,    2,   10,   10,    3],\n       [   0,    2,    3,    0,  953,    0,   11,    2,    3,    8],\n       [   8,    1,    0,   21,    2,  818,   17,    2,   15,    8],\n       [   9,    3,    1,    1,    4,    2,  938,    0,    0,    0],\n       [   2,    7,   19,    2,    2,    0,    0,  975,    2,   19],\n       [   8,    5,    4,    8,    6,    4,   14,   11,  906,    8],\n       [  11,    7,    1,   12,   16,    1,    1,    6,    5,  949]])\n\n# numpy should have already been imported as np\nTP = np.diag(cm)\nTP\n# array([ 963, 1119,  972,  975,  953,  818,  938,  975,  906,  949])\n\nFP = np.sum(cm, axis=0) - TP\nFP\n# array([50, 28, 39, 56, 37, 11, 66, 42, 54, 49])\n\nFN = np.sum(cm, axis=1) - TP\nFN\n# array([17, 16, 60, 35, 29, 74, 20, 53, 68, 60])\n\nnum_classes = 10\nTN = []\nfor i in range(num_classes):\n    temp = np.delete(cm, i, 0)    # delete ith row\n    temp = np.delete(temp, i, 1)  # delete ith column\n    TN.append(sum(sum(temp)))\nTN\n# [8970, 8837, 8929, 8934, 8981, 9097, 8976, 8930, 8972, 8942]\n\nl = 10000\nfor i in range(num_classes):\n    print(TP[i] + FP[i] + FN[i] + TN[i] == l)\n\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\nprecision = TP/(TP+FP)\nrecall = TP/(TP+FN)\n\nprecision\n# array([ 0.95064166,  0.97558849,  0.96142433,  0.9456838 ,  0.96262626,\n#         0.986731  ,  0.93426295,  0.95870206,  0.94375   ,  0.9509018])\n\nrecall\n# array([ 0.98265306,  0.98590308,  0.94186047,  0.96534653,  0.97046843,\n#         0.91704036,  0.97912317,  0.94844358,  0.9301848 ,  0.94053518])\n\nspecificity = TN/(TN+FP)\n\nspecificity\n# array([0.99445676, 0.99684151, 0.9956512 , 0.99377086, 0.99589709,\n#        0.99879227, 0.99270073, 0.99531877, 0.99401728, 0.99455011])\n'
'feature_input = Input(shape=(5,))\ndense_1_h = Dense(4, activation=\'relu\')(feature_input)\ndense_2_h = Dense(8, activation=\'relu\')(dense_1_h)\ndense_1_c = Dense(4, activation=\'relu\')(feature_input)\ndense_2_c = Dense(8, activation=\'relu\')(dense_1_c)\n\nseries_input = Input(shape=(None, 5))\nlstm = LSTM(8)(series_input, initial_state=[dense_2_h, dense_2_c])\nout = Dense(1, activation="sigmoid")(lstm)\nmodel = Model(inputs=[feature_input,series_input], outputs=out)\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\', metrics=["mape"])\n'
"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=444)\n\nsel = SelectFromModel(ExtraTreesClassifier(n_estimators=10, random_state=444), \n                      threshold='mean')\nclf = RandomForestClassifier(n_estimators=5000, random_state=444)\n\nmodel = Pipeline([('sel', sel), ('clf', clf)])\nparams = {'clf__max_features': ['auto', 'sqrt', 'log2']}\n\ngs = GridSearchCV(model, params)\ngs.fit(X_train, y_train)\n\n# How well do your hyperparameter optimizations generalize\n# to unseen test data?\ngs.score(X_test, y_test)\n"
'# new script, dump_model.py, does the creation and dumping of the NeuralNetwork\n\nfrom sklearn.externals import joblib\n\nfrom model import NeuralNetwork\n\nif __name__ == "__main__":\n    model = NeuralNetwork(input_dim=12, units=64)\n    joblib.dump(model, \'demo_model.pkl\')\n'
"from pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql import functions as F\n\nud_f = F.udf(lambda r : Vectors.dense(r),VectorUDT())\ndf = df.withColumn('b',ud_f('a'))\ndf.show()\n+-------------------------+---------------------+\n|a                        |b                    |\n+-------------------------+---------------------+\n|[0.1, 0.2, 0.3, 0.4, 0.5]|[0.1,0.2,0.3,0.4,0.5]|\n+-------------------------+---------------------+\n\ndf.printSchema()\nroot\n  |-- a: array (nullable = true)\n  |    |-- element: double (containsNull = true)\n  |-- b: vector (nullable = true)\n"
'output = Dense(1, trainable = False)(hidden_a)\n'
'plt.imshow(np.transpose(images[0].numpy(), (1, 2, 0)))\n\nplt.imshow(np.transpose(images[0].cpu().detach().numpy(), (1, 2, 0)))\n'
"from keras.models import Model\nfrom keras.layers import concatenate\n\nmerged_layers = concatenate([model1.output, model2.output, model3.output,\n                             model4.output, model5.output, model6.output])\nx = BatchNormalization()(merged_layers)\nx = Dense(300)(x)\nx = PReLU()(x)\nx = Dropout(0.2)(x)\nx = Dense(1)(x)\nx = BatchNormalization()(x)\nout = Activation('sigmoid')(x)\nmerged_model = Model([model1.input, model2.input, model3.input,\n                      model4.input, model5.input, model6.input], [out])\nmerged_model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n"
"generator = ImagaDataGenerator(..., validation_split=0.3)\n\ntrain_gen = generator.flow_from_directory(dir_path, ..., subset='training')\nval_gen = generator.flow_from_directory(dir_path, ..., subset='validation')\n"
'x = ...\ny = ...\n\n...\n\n# For example, if x and y are numpy arrays &lt; 2 GB\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val_, y_val))\n\n...\n\nestimator = ...\n\nfor epoch in n_epochs:\n    estimator.train(input_fn = train_dataset)\n    estimator.evaluate(input_fn = val_dataset)\n'
" corpus = ['sentence1', 'sentence 2', 12930, 'sentence 100']\n\n corpus = ['sentence1', 'sentence 2', 12930, 'sentence 100']\n corpus = [str (item) for item in corpus]\n\ncorpus = ['sentence1', 'sentence 2', 12930, 'sentence 100']\ncorpus = [item for item in corpus if not isinstance(item, int)]\n"
"from sklearn.metrics import precision_score, f1_score, classification_report\n\ny_true = [0, 1, 2, 0, 1, 2] # 3-class problem\ny_pred = [0, 0, 1, 0, 0, 1] # we never predict '2'\n\nprecision_score(y_true, y_pred, average='macro') \n[...] UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. \n  'precision', 'predicted', average, warn_for)\n0.16666666666666666\n\nprecision_score(y_true, y_pred, average='micro') # no warning\n0.3333333333333333\n\nprecision_score(y_true, y_pred, average=None) \n[...] UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. \n  'precision', 'predicted', average, warn_for)\narray([0.5, 0. , 0. ])\n\nprint(classification_report(y_true, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       0.50      1.00      0.67         2\n           1       0.00      0.00      0.00         2\n           2       0.00      0.00      0.00         2\n\n   micro avg       0.33      0.33      0.33         6\n   macro avg       0.17      0.33      0.22         6\nweighted avg       0.17      0.33      0.22         6\n\n[...] UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. \n  'precision', 'predicted', average, warn_for)\n"
'nn = neigh.kneighbors(T[i].reshape(1, -1), return_distance=False)\n'
'model = Sequential()\nmodel.add(Dense(60, input_dim=7, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(55, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(50, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(45, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(30, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(20, kernel_initializer=\'normal\', activation=\'relu\'))\nmodel.add(Dense(1, kernel_initializer=\'normal\'))\nmodel.load_weights("kwhFinal.h5")\nmodel.compile(loss=\'mse\', optimizer=\'adam\', metrics=[rmse])\n\nmodel.save("kwhFinal.h5")\n\nfrom keras.models import load_model\nmodel=load_model("kwhFinal.h5")\n'
"fit():\n\n    regressor.fit(X, func(y))\n\npredict():\n\n    inverse_func(regressor.predict(X))\n\npipe = TransformedTargetRegressor(regressos = DummyEstimator(),\n                                  func=np.log1p, \n                                  inverse_func=np.expm1)),\n\n### Candidate learning algorithms and their hyperparameters\nalphas = [0.001, 0.01, 0.1, 1, 10, 100]\nparam_grid = [  \n                {'transformer__regressor': Lasso(),\n                 'transformer__regressor__alpha': alphas},\n                {'transformer__regressor': LassoLars(),\n                 'transformer__regressor__alpha': alphas},\n                {'transformer__regressor': Ridge(),\n                 'transformer__regressor__alpha': alphas},\n                {'transformer__regressor': ElasticNet(),\n                 'transformer__regressor__alpha': alphas,\n                 'transformer__regressor__l1_ratio': [0.25, 0.5, 0.75]}\n              ]\n"
'import tensorflow as tf\nfrom tensorflow.python.training.saver import BaseSaverBuilder\n\nclass CastFromFloat32SaverBuilder(BaseSaverBuilder):\n  # Based on tensorflow.python.training.saver.BulkSaverBuilder.bulk_restore\n  def bulk_restore(self, filename_tensor, saveables, preferred_shard,\n                   restore_sequentially):\n    from tensorflow.python.ops import io_ops\n    restore_specs = []\n    for saveable in saveables:\n      for spec in saveable.specs:\n        restore_specs.append((spec.name, spec.slice_spec, spec.dtype))\n    names, slices, dtypes = zip(*restore_specs)\n    restore_dtypes = [tf.float32 for _ in dtypes]\n    with tf.device("cpu:0"):\n      restored = io_ops.restore_v2(filename_tensor, names, slices, restore_dtypes)\n      return [tf.cast(r, dt) for r, dt in zip(restored, dtypes)]\n\nimport tensorflow as tf\n\nwith tf.Graph().as_default(), tf.Session() as sess:\n    A = tf.get_variable(name=\'foo\', shape=[3, 3], dtype=tf.float32)\n    dense = tf.layers.dense(inputs=A, units=3)\n    varis = tf.trainable_variables(scope=None)\n    assign = {vari.name: vari for vari in varis}\n    saver = tf.train.Saver(assign)\n    sess.run(tf.global_variables_initializer())\n    print(\'Value to save:\')\n    print(sess.run(dense))\n    save_path = saver.save(sess, "ckpt/tmp.ckpt")\n\nwith tf.Graph().as_default(), tf.Session() as sess:\n    A = tf.get_variable(name=\'foo\', shape=[3, 3], dtype=tf.float16)\n    dense = tf.layers.dense(inputs=A, units=3)\n    varis = tf.trainable_variables(scope=None)\n    assign = {vari.name: vari for vari in varis}\n    saver = tf.train.Saver(assign, builder=CastFromFloat32SaverBuilder())\n    saver.restore(sess, "ckpt/tmp.ckpt")\n    print(\'Restored value:\')\n    print(sess.run(dense))\n'
'def StratifiedGroupShuffleSplit(df_main):\n\n    df_main = df_main.reindex(np.random.permutation(df_main.index)) # shuffle dataset\n\n    # create empty train, val and test datasets\n    df_train = pd.DataFrame()\n    df_val = pd.DataFrame()\n    df_test = pd.DataFrame()\n\n    hparam_mse_wgt = 0.1 # must be between 0 and 1\n    assert(0 &lt;= hparam_mse_wgt &lt;= 1)\n    train_proportion = 0.6 # must be between 0 and 1\n    assert(0 &lt;= train_proportion &lt;= 1)\n    val_test_proportion = (1-train_proportion)/2\n\n    subject_grouped_df_main = df_main.groupby([\'subject_id\'], sort=False, as_index=False)\n    category_grouped_df_main = df_main.groupby(\'category\').count()[[\'subject_id\']]/len(df_main)*100\n\n    def calc_mse_loss(df):\n        grouped_df = df.groupby(\'category\').count()[[\'subject_id\']]/len(df)*100\n        df_temp = category_grouped_df_main.join(grouped_df, on = \'category\', how = \'left\', lsuffix = \'_main\')\n        df_temp.fillna(0, inplace=True)\n        df_temp[\'diff\'] = (df_temp[\'subject_id_main\'] - df_temp[\'subject_id\'])**2\n        mse_loss = np.mean(df_temp[\'diff\'])\n        return mse_loss\n\n    i = 0\n    for _, group in subject_grouped_df_main:\n\n        if (i &lt; 3):\n            if (i == 0):\n                df_train = df_train.append(pd.DataFrame(group), ignore_index=True)\n                i += 1\n                continue\n            elif (i == 1):\n                df_val = df_val.append(pd.DataFrame(group), ignore_index=True)\n                i += 1\n                continue\n            else:\n                df_test = df_test.append(pd.DataFrame(group), ignore_index=True)\n                i += 1\n                continue\n\n        mse_loss_diff_train = calc_mse_loss(df_train) - calc_mse_loss(df_train.append(pd.DataFrame(group), ignore_index=True))\n        mse_loss_diff_val = calc_mse_loss(df_val) - calc_mse_loss(df_val.append(pd.DataFrame(group), ignore_index=True))\n        mse_loss_diff_test = calc_mse_loss(df_test) - calc_mse_loss(df_test.append(pd.DataFrame(group), ignore_index=True))\n\n        total_records = len(df_train) + len(df_val) + len(df_test)\n\n        len_diff_train = (train_proportion - (len(df_train)/total_records))\n        len_diff_val = (val_test_proportion - (len(df_val)/total_records))\n        len_diff_test = (val_test_proportion - (len(df_test)/total_records)) \n\n        len_loss_diff_train = len_diff_train * abs(len_diff_train)\n        len_loss_diff_val = len_diff_val * abs(len_diff_val)\n        len_loss_diff_test = len_diff_test * abs(len_diff_test)\n\n        loss_train = (hparam_mse_wgt * mse_loss_diff_train) + ((1-hparam_mse_wgt) * len_loss_diff_train)\n        loss_val = (hparam_mse_wgt * mse_loss_diff_val) + ((1-hparam_mse_wgt) * len_loss_diff_val)\n        loss_test = (hparam_mse_wgt * mse_loss_diff_test) + ((1-hparam_mse_wgt) * len_loss_diff_test)\n\n        if (max(loss_train,loss_val,loss_test) == loss_train):\n            df_train = df_train.append(pd.DataFrame(group), ignore_index=True)\n        elif (max(loss_train,loss_val,loss_test) == loss_val):\n            df_val = df_val.append(pd.DataFrame(group), ignore_index=True)\n        else:\n            df_test = df_test.append(pd.DataFrame(group), ignore_index=True)\n\n        print ("Group " + str(i) + ". loss_train: " + str(loss_train) + " | " + "loss_val: " + str(loss_val) + " | " + "loss_test: " + str(loss_test) + " | ")\n        i += 1\n\n    return df_train, df_val, df_test\n\ndf_train, df_val, df_test = StratifiedGroupShuffleSplit(df_main)\n'
'X_train_t = ct.fit_transform(X_train)\nX_test_t  = ct.transform(X_test)\n\nct = ColumnTransformer([("onehot", OneHotEncoder(sparse=False, drop="first"), [\'Sex\',\'Embarked\'])], remainder=\'passthrough\')\n\nct = ColumnTransformer([("onehot", OneHotEncoder(sparse=False, drop="first"), [\'Sex\', \'Embarked\'])], remainder=\'drop\')\nA = pd.concat([X_train.drop(["Sex", "Embarked"], axis=1), pd.DataFrame(X_train_t, columns=ct.get_feature_names())], axis=1) \nA.head()\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncols = [\'Survived\', \'Pclass\', \'Sex\', \'Age\', \'SibSp\', \'Parch\', \'Fare\', \'Embarked\']\n\ntrain_df = pd.read_csv(\'train.csv\', usecols=cols)\ntest_df = pd.read_csv(\'test.csv\', usecols=[e for e in cols if e != \'Survived\'])\n\ncols = [\'Survived\', \'Pclass\', \'Sex\', \'Age\', \'SibSp\', \'Parch\', \'Fare\', \'Embarked\']\n\ntrain_df = train_df.dropna()\ntest_df = test_df.dropna()\n\ntrain_df = train_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n\nX_train = train_df.drop("Survived", axis=1)\nY_train = train_df["Survived"]\nX_test = test_df.copy()\n\ncategorical_values = [\'Sex\', \'Embarked\']\nX_train_cont = X_train.drop(categorical_values, axis=1)\nX_test_cont = X_test.drop(categorical_values, axis=1)\n\nct = ColumnTransformer([("onehot", OneHotEncoder(sparse=False, drop="first"), categorical_values)], remainder=\'drop\')\n\nX_train_categorical = ct.fit_transform(X_train)\nX_test_categorical  = ct.transform(X_test)\n\nX_train_t = pd.concat([X_train_cont, pd.DataFrame(X_train_categorical, columns=ct.get_feature_names())], axis=1)\nX_test_t = pd.concat([X_test_cont, pd.DataFrame(X_test_categorical, columns=ct.get_feature_names())], axis=1)\n\nlogreg = LogisticRegression(max_iter=5000)\nlogreg.fit(X_train_t, Y_train)\nY_pred = logreg.predict(X_test_t)\n\nacc_log = round(logreg.score(X_train_t, Y_train) * 100, 2)\n\nprint(acc_log)\n\n80.34\n'
'python train.py /manifest/path --save-dir /model/path ...(etc.).........\n\nfairseq-train\n\n#!/bin/bash\nfairseq-train /home/user/4fairseq --save-dir /home/user/4fairseq --fp16 --max-update 400000 --save-interval 1 --no-epoch-checkpoints \\\n--arch wav2vec --task audio_pretraining --lr 1e-06 --min-lr 1e-09 --optimizer adam --max-lr 0.005 --lr-scheduler cosine \\\n--conv-feature-layers "[(512, 10, 5), (512, 8, 4), (512, 4, 2), (512, 4, 2), (512, 4, 2), (512, 1, 1), (512, 1, 1)]" \\\n--conv-aggregator-layers "[(512, 2, 1), (512, 3, 1), (512, 4, 1), (512, 5, 1), (512, 6, 1), (512, 7, 1), (512, 8, 1), (512, 9, 1), (512, 10, 1), (512, 11, 1), (512, 12, 1), (512, 13, 1)]" \\\n--skip-connections-agg --residual-scale 0.5 --log-compression --warmup-updates 500 --warmup-init-lr 1e-07 --criterion binary_cross_entropy --num-negatives 10 \\\n--max-sample-size 150000 --max-tokens 1500000\n\n\n#!/bin/bash\nfor file in /home/user/data/soundFiles/*\ndo\n  echo "$file"\n  echo "${file%.*}.wav"\n  ffmpeg -i "$file" "${file%.*}.wav"\ndone\n\n'
"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nimport numpy as np\nimport pandas as pd\nfrom sklearn import set_config\nset_config(print_changed_only=True)\ndf = pd.DataFrame({'feature_1': np.random.rand(20),\n                   'feature_2': np.random.choice(['male', 'female'], (20,))})\ntarget = pd.Series(np.random.choice(['yes', 'no'], (20,)))\n\nmodel = Pipeline([('preprocess',\n                   ColumnTransformer([('ohe',\n                                       OneHotEncoder(handle_unknown='ignore'), [1])],\n                                       remainder='passthrough')),\n                  ('lr', LogisticRegression())])\n\nmodel.fit(df, target)\n\n# let us introduce new categories in feature_2 in test data\ntest_df = pd.DataFrame({'feature_1': np.random.rand(20),\n                        'feature_2': np.random.choice(['male', 'female', 'neutral', 'unknown'], (20,))})\nmodel.predict(test_df)\n# array(['yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes',\n#       'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes',\n#       'yes', 'yes'], dtype=object)\n"
'grid_scores_\n\narray of shape [n_subsets_of_features]\n\nThe cross-validation scores such that grid_scores_[i] corresponds to the CV score of the i-th subset of features.\n'
"$ cd ~/scikit_learn_data'\n$ rm -rf 20news_home\n$ rm 20news-bydate.pkz\n"
"metric = sklearn.neighbors.DistanceMetric.get_metric('pyfunc', func=func)\n"
'&gt;&gt;&gt; from sklearn.cross_validation import KFold\n&gt;&gt;&gt; X = # your data [samples x features]\n&gt;&gt;&gt; y = # gt labels\n&gt;&gt;&gt; kf = KFold(X.shape[0], n_folds=5)\n\n&gt;&gt;&gt; for train_index, test_index in kf:\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        # do something\n'
"np.hstack([address_feature.A, dayweek_feature.A])\n\nIn [296]: A=sparse.csr_matrix([0,1,2,0,0,1])\n\nIn [297]: B=sparse.csr_matrix([0,0,0,1,0,1])\n\nIn [298]: C=sparse.csr_matrix([1,0,0,0,1,0])\n\nIn [299]: sparse.vstack([A,B,C])\nOut[299]: \n&lt;3x6 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 7 stored elements in Compressed Sparse Row format&gt;\n\nIn [300]: sparse.vstack([A,B,C]).A\nOut[300]: \narray([[0, 1, 2, 0, 0, 1],\n       [0, 0, 0, 1, 0, 1],\n       [1, 0, 0, 0, 1, 0]], dtype=int32)\n\nIn [301]: sparse.hstack([A,B,C]).A\nOut[301]: array([[0, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0]], dtype=int32)\n\nIn [302]: np.vstack([A.A,B.A,C.A])\nOut[302]: \narray([[0, 1, 2, 0, 0, 1],\n       [0, 0, 0, 1, 0, 1],\n       [1, 0, 0, 0, 1, 0]], dtype=int32)\n"
"In [194]:    \ndf = pd.DataFrame({'a':['low','low',np.NaN,'medium','medium','medium','medium']})\ndf\n\nOut[194]:\n        a\n0     low\n1     low\n2     NaN\n3  medium\n4  medium\n5  medium\n6  medium\n\nIn [195]:    \ndf['a'].fillna(df['a'].mode())\n\nOut[195]:\n0       low\n1       low\n2       NaN\n3    medium\n4    medium\n5    medium\n6    medium\nName: a, dtype: object\n\nIn [196]:    \ndf['a'].mode()\n\nOut[196]:\n0    medium\ndtype: object\n\nIn [197]:    \ndf['a'].fillna(df['a'].mode()[0])\n\nOut[197]:\n0       low\n1       low\n2    medium\n3    medium\n4    medium\n5    medium\n6    medium\nName: a, dtype: object\n\nIn [204]:\ndf = pd.DataFrame({'a':['low','low',np.NaN,'medium','medium','medium','medium',np.NaN,np.NaN,np.NaN,np.NaN]})\ndf['a'].mode()\n\nOut[204]:\n0    medium\ndtype: object\n"
"In [4]: from sklearn import preprocessing\n   ...: import numpy as np\n\nIn [5]: le = preprocessing.LabelEncoder()\n\nIn [6]: le.fit(np.unique(df.values))\nOut[6]: LabelEncoder()\n\nIn [7]: list(le.classes_)\nOut[7]: ['A', 'B', 'C', 'D', 'E']\n\nIn [8]: df.apply(le.transform)\nOut[8]: \n   Feat1  Feat2  Feat3  Feat4  Feat5\n0      0      0      0      0      4\n1      1      1      2      2      4\n2      2      3      2      2      4\n3      3      0      2      3      4\n\nIn [9]: labels = ['A', 'B', 'C', 'D', 'E']\n\nIn [10]: enc = le.fit(labels)\n\nIn [11]: enc.classes_                       # sorts the labels in alphabetical order\nOut[11]: \narray(['A', 'B', 'C', 'D', 'E'], \n      dtype='&lt;U1')\n\nIn [12]: enc.transform('E')\nOut[12]: 4\n"
"In [526]: M=sparse.random(4,10,.1)\nIn [527]: M\nOut[527]: \n&lt;4x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 4 stored elements in COOrdinate format&gt;\nIn [528]: print(M)\n  (3, 1)    0.281301619779\n  (2, 6)    0.830780358032\n  (1, 1)    0.242503399296\n  (2, 2)    0.190933579917\n\nIn [529]: Mc=M.tocoo()\nIn [530]: Mc.data\nOut[530]: array([ 0.28130162,  0.83078036,  0.2425034 ,  0.19093358])\nIn [532]: Mc.row\nOut[532]: array([3, 2, 1, 2], dtype=int32)\nIn [533]: Mc.col\nOut[533]: array([1, 6, 1, 2], dtype=int32)\n\nIn [534]: {k:v for k,v in zip(Mc.col, Mc.data)}\nOut[534]: {1: 0.24250339929583264, 2: 0.19093357991697379, 6: 0.83078035803205375}\n\nIn [535]: np.column_stack((Mc.col, Mc.data))\nOut[535]: \narray([[ 1.        ,  0.28130162],\n       [ 6.        ,  0.83078036],\n       [ 1.        ,  0.2425034 ],\n       [ 2.        ,  0.19093358]])\n"
'with ops.name_scope(name, "l2_normalize", [x]) as name:\n   x = ops.convert_to_tensor(x, name="x")\n   square_sum = math_ops.reduce_sum(math_ops.square(x), dim, keep_dims=True)\n   x_inv_norm = math_ops.rsqrt(math_ops.maximum(square_sum, epsilon))\nreturn math_ops.mul(x, x_inv_norm, name=name)\n'
'x_ij = [category, reactions, comments, shares]_i for day j\ni = 1, 2, ..., n_j (number of posts in day "j")\nj = 1, 2, ..., N (number of days in dataset)\n\nX_j = [x_1j, x_2j, ..., x_nj]\n\ncell = tf.contrib.rnn.GRUCell(num_hidden)\n# Any additional things like tf.contrib.rnn.DropoutWrapper you want here\ncell = tf.contrib.rnn.OutputProjectionWrapper(cell, 1)  # only one output number, right?\noutput, _ = tf.nn.dynamic_rnn(cell, data, sequence_length=sequence_length)\n\nwith tf.Session() as sess:\n  for epoch in range(num_epochs):\n    shuffle_training_set()\n    for batch in range(num_batches):\n      d = get_next_batch()\n      t = get_next_target_batch()\n      s = # length of each data sample in your batch\n      sess.run(optimize, feed_dict={data: d, targets: t, sequence_length: s})\n      # periodically validate and stop when you stop improving\n\ncost = # your cost function here...\noptimizer = tf.train.AdamOptimizer()  # I usually have luck with this optimizer\noptimize = optimizer.minimize(cost)\n'
"dummy_var_encoder(column_to_dummy=None)\n\ndef __init__(self, column='default_col_name'):\n    self.column = column\n    print(self.column)\n\ndef transform(self, X_DF):\n    ''' Update X_DF to have set of dummy-variables instead of orig column'''        \n\n    # convert self-attribute to local var for ease of stepping through function\n    column = self.column\n\n    # add columns for new dummy vars, and drop original categorical column\n    dummy_matrix = pd.get_dummies(X_DF[column], prefix=column)\n\n    return dummy_matrix\n"
"# Your image shape is (, , 3)\ntest_image = cv2.imread('media/'+request.FILES['test_image'].name)\n\nif test_image is not None:\n    test_image = cv2.resize(test_image, (128, 128))\n    test_image = np.array(test_image)\n    test_image = test_image.astype('float32')\n    test_image /= 255\n    print(test_image.shape)\nelse:\n    print('image didnt load')\n\n# Your image shape is (, , 4)\ntest_image = np.expand_dims(test_image, axis=0)\nprint(test_image)\nprint(test_image.shape)\n\npred = model.predict_classes(test_image)\n"
'(360, 480) -&gt; (180, 240) -&gt; (90, 120) -&gt; (45, 60) -&gt; (22, 30) -&gt; (11, 15)\n\n(11, 15) -&gt; (22, 30) -&gt; (44, 60) -&gt; (88, 120) -&gt; (176, 240) -&gt; (352, 480)\n'
"for g, v in grads_and_vars:\n  tf.summary.histogram(v.name, v)\n  tf.summary.histogram(v.name + '_grad', g)\n\nmerged = tf.summary.merge_all()\nwriter = tf.summary.FileWriter('train_log_layer', tf.get_default_graph())\n\n...\n\n_, summary = sess.run([train_op, merged], feed_dict={I: 2*np.random.rand(1, 1)-1})\nif i % 10 == 0:\n  writer.add_summary(summary, global_step=i)\n"
'input_length: Length of input sequences, when it is constant.\n      This argument is required if you are going to connect\n      `Flatten` then `Dense` layers upstream\n      (without it, the shape of the dense outputs cannot be computed).\n\n# example sequence of input, e.g. batch size is 1.\n[\n [34], \n [27], \n ...\n] \n--&gt; # feed into embedding layer\n\n[\n  [64-d representation of token 34 ...],\n  [64-d representation of token 27 ...],\n  ...\n] \n--&gt; # feed into LSTM layer\n\n[32-d output vector of the final sequence step of LSTM]\n'
"from sklearn.metrics import f1_score\n\ndef lgb_f1_score(y_hat, data):\n    y_true = data.get_label()\n    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n    return 'f1', f1_score(y_true, y_hat), True\n\nevals_result = {}\n\nclf = lgb.train(param, train_data, valid_sets=[val_data, train_data], valid_names=['val', 'train'], feval=lgb_f1_score, evals_result=evals_result)\n\nlgb.plot_metric(evals_result, metric='f1')\n"
'import numpy as np\n\nX_set = np.arange(20).reshape(10, 2)\ny_set = np.array([0, 1, 1, 1, 0, 0, 1, 1, 0, 1])\n\nplt.scatter(filtered_values_in_first_column_of X_set, \n            filtered_values_in_second_column_of X_set)\n\nprint("Where y_set == 0: Boolean mask.")\nprint(y_set == 0)\nprint()\n\nprint("All rows of X_set indexed by the Boolean mask")\nprint(X_set[y_set == 0])\nprint()\n\nprint("2D indexing to get only the first column of the above")\nprint(X_set[y_set == 0, 0])\nprint()\n'
'import numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import _tree\n\nX, y = make_classification(n_informative=3, n_features=4, n_samples=200, n_redundant=1, random_state=42, n_classes=2)\nfeature_names = [\'X0\',\'X1\',\'X2\',\'X3\']\nXtrain, Xtest, ytrain, ytest = train_test_split(X,y, random_state=42)\nclf = DecisionTreeClassifier(max_depth=2)\nclf.fit(Xtrain, ytrain)\n\nfrom sklearn.externals.six import StringIO  \nfrom sklearn import tree\nimport pydot \ndot_data = StringIO() \ntree.export_graphviz(clf, out_file=dot_data) \ngraph = pydot.graph_from_dot_data(dot_data.getvalue()) [0]\ngraph.write_jpeg(\'1.jpeg\')\n\nnode_indicator = clf.decision_path(Xtrain)\nn_nodes = clf.tree_.node_count\nfeature = clf.tree_.feature\nthreshold = clf.tree_.threshold\nleave_id = clf.apply(Xtrain)\n\n\ndef value2prob(value):\n    return value / value.sum(axis=1).reshape(-1, 1)\n\n\ndef print_condition(sample_id):\n    print("WHEN", end=\' \')\n    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                        node_indicator.indptr[sample_id + 1]]\n    for n, node_id in enumerate(node_index):\n        if leave_id[sample_id] == node_id:\n            values = clf.tree_.value[node_id]\n            probs = value2prob(values)\n            print(\'THEN Y={} (probability={}) (values={})\'.format(\n                probs.argmax(), probs.max(), values))\n            continue\n        if n &gt; 0:\n            print(\'&amp;&amp; \', end=\'\')\n        if (Xtrain[sample_id, feature[node_id]] &lt;= threshold[node_id]):\n            threshold_sign = "&lt;="\n        else:\n            threshold_sign = "&gt;"\n        if feature[node_id] != _tree.TREE_UNDEFINED:\n            print(\n                "%s %s %s" % (\n                    feature_names[feature[node_id]],\n                    #Xtrain[sample_id,feature[node_id]] # actual value\n                    threshold_sign,\n                    threshold[node_id]),\n                end=\' \')\n\n&gt;&gt;&gt; print_condition(0)\nWHEN X1 &gt; -0.2662498950958252 &amp;&amp; X0 &gt; -1.1966443061828613 THEN Y=1 (probability=0.9672131147540983) (values=[[ 2. 59.]])\n\n[print_condition(i) for i in (clf.predict(Xtrain) == 0).nonzero()[0]]\n'
'y, outputs, loss, h, c, caches = f(params, h, c, inputs, targets)\n\n_, _, loss_minus, _, _, _ = f(params, h, c, inputs, targets)\np.flat[pix] = old_val\n\n_, outputs, loss, _, _, caches = f(params, h, c, inputs, targets)\n'
"because the model function graph is defined statically in the SavedModel.\n\nest = tf.contrib.estimator.SavedModelEstimator(MODEL_FILE_PATH)\nest.params['batch_size'] = 1\npredictions = getPrediction1(pred_sentences)\n"
'from keras import regularizers\n\nmodel.add(Dense(..., kernel_regularizer=regularizers.l2(0.1)))\n\n# Add regularization penalties\n# and other layer-specific losses.\nfor loss_tensor in self.losses:\n    total_loss += loss_tensor\n'
'OOM when allocating tensor with shape[800000,32,30,62]\n'
"residual_plant = minimize(lambda x: -model.predict(scaler.transform(x)), scaler.transform(x0), method='SLSQP',bounds=bnds)\n\nmodel.fit(X_train_scaled,y_train)\n"
"def create_model_multiple():\n\n    input1 = tf.keras.Input(shape=(13,), name = 'I1')\n    input2 = tf.keras.Input(shape=(6,), name = 'I2')\n    hidden1 = tf.keras.layers.Dense(units = 4, activation='relu')(input1)\n    hidden2 = tf.keras.layers.Dense(units = 4, activation='relu')(input2)\n    merge = tf.keras.layers.concatenate([hidden1, hidden2])\n    hidden3 = tf.keras.layers.Dense(units = 3, activation='relu')(merge)\n    output1 = tf.keras.layers.Dense(units = 2, activation='softmax', name ='O1')(hidden3)\n    output2 = tf.keras.layers.Dense(units = 2, activation='softmax', name = 'O2')(hidden3)\n    model = tf.keras.models.Model(inputs = [input1,input2], outputs = [output1,output2])\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n\nx1 = np.random.uniform(0,1, (190,13))\nx2 = np.random.uniform(0,1, (190,6))\nval_x1 = np.random.uniform(0,1, (50,13))\nval_x2 = np.random.uniform(0,1, (50,6))\n\ny1 = np.random.randint(0,2, 190)\ny2 = np.random.randint(0,2, 190)\nval_y1 = np.random.randint(0,2, 50)\nval_y2 = np.random.randint(0,2, 50)\n\n\nmodel = create_model_multiple()\n\nhistory = model.fit({'I1':x1, 'I2':x2},\n                    {'O1':y1, 'O2': y2},\n                    validation_data=([val_x1,val_x2], [val_y1,val_y2]), # &lt;=========\n                    epochs=100,\n                    verbose = 1)\n"
'import numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = np.array([\n     [[0, 0, 0, 0, 0],\n      [0, 0, 0, 0, 0],\n      [0, 0, 0, 0, 0]],\n\n     [[1,  2,  3,  4,  5],\n      [4,  5,  6,  7,  8],\n      [9, 10, 11, 12, 13]],\n\n     [[14, 15, 16, 17, 18],\n      [0, 0, 0, 0, 0],\n      [24, 25, 26, 27, 28]],\n\n     [[0, 0, 0, 0, 0],\n      [423, 2, 230, 60, 70],\n      [0, 0, 0, 0, 0]]\n], dtype = np.float64)\n\nnz = np.any(X, -1)\nX[nz] = MinMaxScaler().fit_transform(X[nz])\n\nprint(X)\n\n[[[0.         0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.        ]\n  [0.         0.         0.         0.         0.        ]]\n\n [[0.         0.         0.         0.         0.        ]\n  [0.007109   0.13043478 0.01321586 0.05357143 0.04615385]\n  [0.01895735 0.34782609 0.03524229 0.14285714 0.12307692]]\n\n [[0.03080569 0.56521739 0.05726872 0.23214286 0.2       ]\n  [0.         0.         0.         0.         0.        ]\n  [0.05450237 1.         0.10132159 0.41071429 0.35384615]]\n\n [[0.         0.         0.         0.         0.        ]\n  [1.         0.         1.         1.         1.        ]\n  [0.         0.         0.         0.         0.        ]]]\n\nscaler = MinMaxScaler().fit(X[np.any(X, -1)])\nX[np.any(X, -1)] = scaler.transform(X[np.any(X, -1)])\nY[np.any(Y, -1)] = scaler.transform(Y[np.any(Y, -1)])\n'
'# get a matrix where the (i, j)th element is |x[i] - x[j]|^2\n# using the identity (x - y)^T (x - y) = x^T x + y^T y - 2 x^T y\npt_sq_norms = (x ** 2).sum(axis=1)\ndists_sq = np.dot(x, x.T)\ndists_sq *= -2\ndists_sq += pt_sq_norms.reshape(-1, 1)\ndists_sq += pt_sq_norms\n\n# turn into an RBF gram matrix\nkm = dists_sq; del dists_sq\nkm /= -2 * sigma**2\nnp.exp(km, km)  # exponentiates in-place\n'
'&gt;&gt;&gt; from sklearn import svm\n&gt;&gt;&gt; clt = svm.SVC()\n&gt;&gt;&gt; clt.fit( [[1],[2],[3]], ["a","b","a"] )\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n  kernel=\'rbf\', max_iter=-1, probability=False, shrinking=True, tol=0.001,\n  verbose=False)\n&gt;&gt;&gt; clt.classes_\narray([\'a\', \'b\'], \n      dtype=\'|S2\')\n'
'&gt;&gt;&gt; x\narray([[5, 3, 0],\n       [3, 0, 5],\n       [5, 5, 0],\n       [1, 1, 7]])\n\n&gt;&gt;&gt; x\narray([[5, 300, 0],\n       [3, 0, 5],\n       [5, 500, 0],\n       [1, 100, 7]])\n\n&gt;&gt;&gt; np.cov(x)\narray([[  6.33333333,  -3.16666667,   6.66666667,  -8.        ],\n       [ -3.16666667,   6.33333333,  -5.83333333,   7.        ],\n       [  6.66666667,  -5.83333333,   8.33333333, -10.        ],\n       [ -8.        ,   7.        , -10.        ,  12.        ]])\n\n&gt;&gt;&gt; np.corrcoef(x)\narray([[ 1.        , -0.5       ,  0.91766294, -0.91766294],\n       [-0.5       ,  1.        , -0.80295507,  0.80295507],\n       [ 0.91766294, -0.80295507,  1.        , -1.        ],\n       [-0.91766294,  0.80295507, -1.        ,  1.        ]])\n'
'n_active_vars=0\n\nnactive_vars=0\n\nterm_crit=(cv2.TERM_CRITERIA_MAX_ITER,1000,1)\n'
'from itertools import combinations\ndef get_support(df):\n    pp = []\n    for cnum in range(1, len(df.columns)+1):\n        for cols in combinations(df, cnum):\n            s = df[list(cols)].all(axis=1).sum()\n            pp.append([",".join(cols), s])\n    sdf = pd.DataFrame(pp, columns=["Pattern", "Support"])\n    return sdf\n\n&gt;&gt;&gt; s = get_support(df)\n&gt;&gt;&gt; s[s.Support &gt;= 3]\n   Pattern  Support\n0        a        6\n1        b        7\n2        c        7\n3        d        7\n4        e        3\n5      a,b        5\n6      a,c        4\n7      a,d        4\n9      b,c        6\n10     b,d        4\n12     c,d        4\n14     d,e        3\n15   a,b,c        4\n16   a,b,d        3\n21   b,c,d        3\n\n[15 rows x 2 columns]\n'
"import numpy as np\nfrom shogun import StringCharFeatures, RAWBYTE\nfrom shogun import BinaryLabels\nfrom shogun import SubsequenceStringKernel\nfrom shogun import LibSVM\n\nstrings = ['cat', 'doom', 'car', 'boom','caboom','cartoon','cart']\ntest = ['bat', 'soon', 'it is your doom', 'i love your cat cart','i love loonytoons']\n\ntrain_labels  = np.array([1, -1, 1, -1,-1,-1,1])\ntest_labels = np.array([1, -1, -1, 1])\n\nfeatures = StringCharFeatures(strings, RAWBYTE)\ntest_features = StringCharFeatures(test, RAWBYTE)\n\n# 1 is n and 0.5 is lambda as described in Lodhi 2002\nsk = SubsequenceStringKernel(features, features, 3, 0.5)\n\n# Train the Support Vector Machine\nlabels = BinaryLabels(train_labels)\nC = 1.0\nsvm = LibSVM(C, sk, labels)\nsvm.train()\n\n# Prediction\npredicted_labels = svm.apply(test_features).get_labels()\nprint(predicted_labels)\n"
'class GridSeachWithCoef(GridSearchCV):\n    @property\n    def coef_(self):\n        return self.best_estimator_.coef_\n'
"class StratifiedKFold_ByColumn( object ):\n    def __init__( self, n_folds, X, y, colname ):\n        groupable = pd.concat( [X[colname], y], axis=1 )\n        grouped = groupable.groupby( [colname] ).aggregate( max )\n        self.column = X[colname]\n        self.colname = colname\n\n        # import pdb; pdb.set_trace()\n\n        self.folds = [\n            (train,val) for (train,val) in\n            sklearn.cross_validation.StratifiedKFold( y=grouped.values[:,0], n_folds=n_folds, shuffle=True )\n            ]\n        self.n_folds = n_folds\n        self.i = 0\n        self.y=y\n\n        # self.test()\n\n    def __len__(self):\n        return self.n_folds\n    def __iter__( self ):\n        self.i = 0\n        return self\n\n    def test( self ):\n        for train,val in self.folds:\n            train_mask = self.column.isin( train )\n            val_mask = self.column.isin( val )\n            print 'train:',self.y[train_mask].sum(), (1-self.y[train_mask]).sum()\n            print 'val:',self.y[val_mask].sum(), (1-self.y[val_mask]).sum()\n\n\n\n    def next( self ):\n        if self.i &lt; self.n_folds:\n            train,val = self.folds[self.i]\n            self.i += 1\n\n            # import pdb; pdb.set_trace()\n            train_mask = self.column.isin( train )\n            val_mask = self.column.isin( val )\n\n            y_train = self.y[train_mask]\n            X_train = self.column[train_mask]\n\n            n_tr_1 = (y_train!=0).sum()\n            n_tr_0 = (y_train==0).sum()\n            # import pdb; pdb.set_trace()\n            assert n_tr_1 &lt; n_tr_0\n            stride = n_tr_0/n_tr_1\n\n            X_train_1 = X_train[y_train!=0]\n            y_train_1 = y_train[y_train!=0]\n            X_train_0 = X_train[y_train==0]\n            y_train_0 = y_train[y_train==0]\n\n            train_idxs = []\n            for i_1 in range(0,n_tr_1):\n                train_idxs.append( X_train_1[i_1:(i_1+1)].index )\n                train_idxs.append( X_train_0[i_1*stride:(i_1+1)*stride].index )\n            train_idxs = flatten(train_idxs)\n\n            val_idxs = val_mask[val_mask].index\n\n            return np.array(train_idxs), np.array(val_idxs)\n        else:\n            raise StopIteration()\n"
'&gt;&gt;&gt; mdl_fit = mdl.fit(method=\'bfgs\')\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.057112\n         Iterations: 35\n         Function evaluations: 37\n         Gradient evaluations: 37\ne:\\josef\\eclipsegworkspace\\statsmodels-git\\statsmodels-all-new2_py27\\statsmodels\\statsmodels\\base\\model.py:471: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  "Check mle_retvals", ConvergenceWarning)\n\n&gt;&gt;&gt; fitted = mdl_fit.predict()\n&gt;&gt;&gt; fitted[y==\'setosa\'].min(0)\narray([  9.99497636e-01,   2.07389867e-11,   1.71740822e-38])\n&gt;&gt;&gt; fitted[y==\'setosa\'].max(0)\narray([  1.00000000e+00,   5.02363854e-04,   1.05778255e-20])\n\n&gt;&gt;&gt; print(mdl_fit.summary())\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                Species   No. Observations:                  150\nModel:                        MNLogit   Df Residuals:                      140\nMethod:                           MLE   Df Model:                            8\nDate:                Mon, 20 Jul 2015   Pseudo R-squ.:                  0.9480\nTime:                        04:08:04   Log-Likelihood:                -8.5668\nconverged:                      False   LL-Null:                       -164.79\n                                        LLR p-value:                 9.200e-63\n=====================================================================================\nSpecies=versicolor       coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]\n--------------------------------------------------------------------------------------\nSepal.Length          -1.4959    444.817     -0.003      0.997      -873.321   870.330\nSepal.Width           -8.0560    282.766     -0.028      0.977      -562.267   546.155\nPetal.Length          11.9301    374.116      0.032      0.975      -721.323   745.184\nPetal.Width            1.7039    759.366      0.002      0.998     -1486.627  1490.035\nconst                  1.6444   1550.515      0.001      0.999     -3037.309  3040.597\n--------------------------------------------------------------------------------------\nSpecies=virginica       coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]\n-------------------------------------------------------------------------------------\nSepal.Length         -8.0348    444.835     -0.018      0.986      -879.896   863.827\nSepal.Width         -15.8195    282.793     -0.056      0.955      -570.083   538.444\nPetal.Length         22.1797    374.155      0.059      0.953      -711.152   755.511\nPetal.Width          14.0603    759.384      0.019      0.985     -1474.304  1502.425\nconst                -6.5053   1550.533     -0.004      0.997     -3045.494  3032.483\n=====================================================================================\n'
'z = a + b*x + c*y\n\nz = a + b*x + c*x^2\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nx = np.arange(10)[:, None]\ny = np.ravel(x) ** 2\n\np = np.array([1, 2])\nmodel = LinearRegression().fit(x ** p, y)\nmodel.predict(11 ** p)\n# [121]\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(PolynomialFeatures(2), LinearRegression())\nmodel.fit(x, y).predict(11)\n# [121]\n'
'P(gets score X|has feature Y)\n'
"pipe_svc = Pipeline([('scl', StandardScaler()),\n                     ('clf', SVC(random_state=1))])\n\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n\nparam_grid = [{'clf__C': param_range, \n               'clf__kernel': ['linear']},\n             {'clf__C': param_range, \n               'clf__gamma': param_range, \n               'clf__kernel': ['rbf']}]\n\n\n# Nested Cross-validation (here: 5 x 2 cross validation)\n# =====================================\ngs = GridSearchCV(estimator=pipe_svc, \n                            param_grid=param_grid, \n                            scoring='accuracy', \n                            cv=5)\nscores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=2)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n"
"import hmmlearn.hmm as hmm\nimport numpy as np\n\ntransmat = np.array([[0.7, 0.3],\n                      [0.3, 0.7]])\nemitmat = np.array([[0.9, 0.1],\n                    [0.2, 0.8]])\n\nstartprob = np.array([0.5, 0.5])\nh = hmm.MultinomialHMM(n_components=2, startprob=startprob,\n                       transmat=transmat)\nh.emissionprob_ = emitmat\n# works fine\nh.fit([[0, 0, 1, 0, 0]]) \n# h.fit([[0, 0, 1, 0, 0], [0, 0], [1,1,1]]) # this is the reason for such \n                                            # syntax, you can fit to multiple\n                                            # sequences    \nprint h.decode([0, 0, 1, 0, 0])\nprint h\n\n(-4.125363362578882, array([1, 1, 1, 1, 1]))\nMultinomialHMM(algorithm='viterbi',\n        init_params='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n        n_components=2, n_iter=10,\n        params='abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ',\n        random_state=&lt;mtrand.RandomState object at 0x7fe245ac7510&gt;,\n        startprob=None, startprob_prior=1.0, thresh=0.01, transmat=None,\n        transmat_prior=1.0)\n"
'losses = tf.reshape(tf.concat(1, losses), [-1, size])\n'
'from music21 import *\n\nallBach = corpus.search(\'bach\')\n\nx = allBach[0]\np = x.parse()\n\npartStream = p.parts.stream()\n\nfor n in p.flat.notes:\n    print "Note: %s%d %0.1f" % (n.pitch.name, n.pitch.octave, n.duration.quarterLength)\n'
"children_left : array of int, shape [node_count]\n    children_left[i] holds the node id of the left child of node i.\n    For leaves, children_left[i] == TREE_LEAF. Otherwise,\n    children_left[i] &gt; i. This child handles the case where\n    X[:, feature[i]] &lt;= threshold[i].\n\nchildren_right : array of int, shape [node_count]\n    children_right[i] holds the node id of the right child of node i.\n    For leaves, children_right[i] == TREE_LEAF. Otherwise,\n    children_right[i] &gt; i. This child handles the case where\n    X[:, feature[i]] &gt; threshold[i].\n\nfeature : array of int, shape [node_count]\n    feature[i] holds the feature to split on, for the internal node i.\n\nthreshold : array of double, shape [node_count]\n    threshold[i] holds the threshold for the internal node i.\n\ndef prune(decisiontree, min_samples_leaf = 1):\n    if decisiontree.min_samples_leaf &gt;= min_samples_leaf:\n        raise Exception('Tree already more pruned')\n    else:\n        decisiontree.min_samples_leaf = min_samples_leaf\n        tree = decisiontree.tree_\n        for i in range(tree.node_count):\n            n_samples = tree.n_node_samples[i]\n            if n_samples &lt;= min_samples_leaf:\n                tree.children_left[i]=-1\n                tree.children_right[i]=-1\n\n[from sklearn.tree import DecisionTreeRegressor as DTR\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.tree import export_graphviz as export\n\nbunch = load_diabetes()\ndata = bunch.data\ntarget = bunch.target\n\ndtr = DTR(max_depth = 4)\ndtr.fit(data,target)\n\nexport(decision_tree=dtr.tree_, out_file='before.dot')\nprune(dtr, min_samples_leaf = 100)\nexport(decision_tree=dtr.tree_, out_file='after.dot')][1]\n"
"def make_model(batch_size, nb_epoch):\n    model = Sequential()\n    model.add(Dense(in_size, input_dim=in_size, init='uniform', activation='relu'))\n    model.add(Dense(8, init='uniform', activation='relu'))\n    model.add(Dense(1, init='uniform', activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n"
'def relu_prime(data, epsilon=0.1):\n    gradients = 1. * (data &gt; 0)\n    gradients[gradients == 0] = epsilon\n    return gradients\n'
'import numpy\n\ndef fliping_gen(image_generator, flip_p=0.5):\n    for x, y in image_generator:\n        flip_selector = numpy.random.binomial(1, flip_p, size=x.shape[0]) == 1\n        x[flip_selector,:,:,:] = x[flip_selector,:,::-1,:]\n        y[flip_selector] = (-1) * y[flip_selector]\n        yield x, y\n'
'import tensorflow as tf\n\ndef range_mean(index_ranges, values):\n  """Take the mean of `values` along ranges specified by `index_ranges`.\n\n  return[i, ...] = tf.reduce_mean(\n    values[i, index_ranges[i, 0]:index_ranges[i, 1], ...], axis=0)\n\n  Args:\n    index_ranges: An integer Tensor with shape [N x 2]\n    values: A Tensor with shape [N x M x ...].\n  Returns:\n    A Tensor with shape [N x ...] containing the means of `values` having\n    indices in the ranges specified.\n  """\n  m_indices = tf.range(tf.shape(values)[1])[None]\n  # Determine which parts of `values` will be in the result\n  selected = tf.logical_and(tf.greater_equal(m_indices, index_ranges[:, :1]),\n                            tf.less(m_indices, index_ranges[:, 1:]))\n  n_indices = tf.tile(tf.range(tf.shape(values)[0])[..., None],\n                      [1, tf.shape(values)[1]])\n  segments = tf.where(selected, n_indices + 1, tf.zeros_like(n_indices))\n  # Throw out segment 0, since that\'s our "not included" segment\n  segment_sums = tf.unsorted_segment_sum(\n      data=values,\n      segment_ids=segments, \n      num_segments=tf.shape(values)[0] + 1)[1:]\n  divisor = tf.cast(index_ranges[:, 1] - index_ranges[:, 0],\n                    dtype=values.dtype)\n  # Pad the shape of `divisor` so that it broadcasts against `segment_sums`.\n  divisor_shape_padded = tf.reshape(\n      divisor,\n      tf.concat([tf.shape(divisor), \n                 tf.ones([tf.rank(values) - 2], dtype=tf.int32)], axis=0))\n  return segment_sums / divisor_shape_padded\n\nindex_range_tensor = tf.constant([[2, 4], [1, 6], [0, 3], [0, 9]])\nvalues_tensor = tf.reshape(tf.range(4 * 10 * 5, dtype=tf.float32), [4, 10, 5])\nwith tf.Session():\n  tf_result = range_mean(index_range_tensor, values_tensor).eval()\n  index_range_np = index_range_tensor.eval()\n  values_np = values_tensor.eval()\n\nfor i in range(values_np.shape[0]):\n  print("Slice {}: ".format(i),\n        tf_result[i],\n        numpy.mean(values_np[i, index_range_np[i, 0]:index_range_np[i, 1], :],\n                   axis=0))\n\nSlice 0:  [ 12.5  13.5  14.5  15.5  16.5] [ 12.5  13.5  14.5  15.5  16.5]\nSlice 1:  [ 65.  66.  67.  68.  69.] [ 65.  66.  67.  68.  69.]\nSlice 2:  [ 105.  106.  107.  108.  109.] [ 105.  106.  107.  108.  109.]\nSlice 3:  [ 170.  171.  172.  173.  174.] [ 170.  171.  172.  173.  174.]\n'
'sliced = Lambda(lambda x: x[:,slicing_indeces], output_shape=(sliced_shape))(input)\n'
"# shape=(100, 100, 100, 3)\n\nx = Conv2D(32, (3, 3), activation='relu')(input_layer)\n# shape=(100, row, col, 32)\n\nx = Flatten()(x)\n# shape=(100, row*col*32)    \n\nx = Dense(256, activation='relu')(x)\n# shape=(100, 256)\n\nx = Dense(10, activation='softmax')(x)\n# shape=(100, 10)\n\n# shape=(100, 100, 100, 3)\n\nx = Conv2D(32, (3, 3), activation='relu')(input_layer)\n# shape=(100, row, col, 32)\n\nx = Dense(256, activation='relu')(x)\n# shape=(100, row, col, 256)\n\nx = Dense(10, activation='softmax')(x)\n# shape=(100, row, col, 10)\n"
'm.add(Embedding(features, embedding_dims, input_length=maxlen, input_shape=(features, ) ))\n'
'price = a * surface + b\n'
'ar = np.random.randint(10,100,(5,5))\n\n[[43, 79, 67, 20, 13],    #&lt;---Monday---\n [80, 86, 78, 76, 71],    #&lt;---Tuesday---\n [35, 23, 62, 31, 59],    #&lt;---Wednesday---\n [67, 53, 92, 80, 15],    #&lt;---Thursday---\n [60, 20, 10, 45, 47]]    #&lt;---Firday---\n\na2 = np.concatenate([ar[x:x+2,:] for x in range(ar.shape[0]-1)])\na2 = a2.reshape(4,2,5)\n\n[[[43, 79, 67, 20, 13],    #See Monday First\n  [80, 86, 78, 76, 71]],   #See Tuesday second ---&gt; Predict Value originally set for Tuesday\n [[80, 86, 78, 76, 71],    #See Tuesday First\n  [35, 23, 62, 31, 59]],   #See Wednesday Second ---&gt; Predict Value originally set for Wednesday\n [[35, 23, 62, 31, 59],    #See Wednesday Value First\n  [67, 53, 92, 80, 15]],   #See Thursday Values Second ---&gt; Predict value originally set for Thursday\n [[67, 53, 92, 80, 15],    #And so on\n  [60, 20, 10, 45, 47]]])\n\nmodel = Sequential()\nmodel.add(LSTM(hidden_dims,input_shape=(a2.shape[1],a2.shape[2]))\nmodel.add(Dense(1))\n'
"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2],[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2,1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[ \n    ('svm', SVC(probability=True)),\n    ('lr', LogisticRegression()),\n    ], voting='soft')\n\n#Use the key for the classifier followed by __ and the attribute\nparams = {'lr__C': [1.0, 100.0],\n      'svm__C': [2,3,4],}\n\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=2)\n\ngrid.fit(X,y)\n\nprint (grid.best_params_)\n#{'lr__C': 1.0, 'svm__C': 2}\n"
'import numpy as np\nimport tensorflow as tf\n\ndef tf_cov(x):\n    mean_x = tf.reduce_mean(x, axis=0, keep_dims=True)\n    mx = tf.matmul(tf.transpose(mean_x), mean_x)\n    vx = tf.matmul(tf.transpose(x), x)/tf.cast(tf.shape(x)[0], tf.float32)\n    cov_xx = vx - mx\n    return cov_xx\n\ndata = np.array([[1., 4, 2], [5, 6, 24], [15, 1, 5], [7,3,8], [9,4,7]])\n\nwith tf.Session() as sess:\n    print(sess.run(tf_cov(tf.constant(data, dtype=tf.float32))))\n\n\n## validating with numpy solution\npc = np.cov(data.T, bias=True)\nprint(pc)\n'
'import numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\nX, y = make_classification(n_samples=1000,\n                           n_features=6,\n                           n_informative=3,\n                           n_classes=2,\n                           random_state=0,\n                           shuffle=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nestimator = RandomForestClassifier(n_estimators=10,\n                               random_state=0)\nestimator.fit(X_train, y_train)\n\n# The decision estimator has an attribute called tree_  which stores the entire\n# tree structure and allows access to low level attributes. The binary tree\n# tree_ is represented as a number of parallel arrays. The i-th element of each\n# array holds information about the node `i`. Node 0 is the tree\'s root. NOTE:\n# Some of the arrays only apply to either leaves or split nodes, resp. In this\n# case the values of nodes of the other type are arbitrary!\n#\n# Among those arrays, we have:\n#   - left_child, id of the left child of the node\n#   - right_child, id of the right child of the node\n#   - feature, feature used for splitting the node\n#   - threshold, threshold value at the node\n#\n\n# Using those arrays, we can parse the tree structure:\n\n#n_nodes = estimator.tree_.node_count\nn_nodes_ = [t.tree_.node_count for t in estimator.estimators_]\nchildren_left_ = [t.tree_.children_left for t in estimator.estimators_]\nchildren_right_ = [t.tree_.children_right for t in estimator.estimators_]\nfeature_ = [t.tree_.feature for t in estimator.estimators_]\nthreshold_ = [t.tree_.threshold for t in estimator.estimators_]\n\n\ndef explore_tree(estimator, n_nodes, children_left,children_right, feature,threshold,\n                suffix=\'\', print_tree= False, sample_id=0, feature_names=None):\n\n    if not feature_names:\n        feature_names = feature\n\n\n    assert len(feature_names) == X.shape[1], "The feature names do not match the number of features."\n    # The tree structure can be traversed to compute various properties such\n    # as the depth of each node and whether or not it is a leaf.\n    node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n    is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n\n    stack = [(0, -1)]  # seed is the root node id and its parent depth\n    while len(stack) &gt; 0:\n        node_id, parent_depth = stack.pop()\n        node_depth[node_id] = parent_depth + 1\n\n        # If we have a test node\n        if (children_left[node_id] != children_right[node_id]):\n            stack.append((children_left[node_id], parent_depth + 1))\n            stack.append((children_right[node_id], parent_depth + 1))\n        else:\n            is_leaves[node_id] = True\n\n    print("The binary tree structure has %s nodes"\n          % n_nodes)\n    if print_tree:\n        print("Tree structure: \\n")\n        for i in range(n_nodes):\n            if is_leaves[i]:\n                print("%snode=%s leaf node." % (node_depth[i] * "\\t", i))\n            else:\n                print("%snode=%s test node: go to node %s if X[:, %s] &lt;= %s else to "\n                      "node %s."\n                      % (node_depth[i] * "\\t",\n                         i,\n                         children_left[i],\n                         feature[i],\n                         threshold[i],\n                         children_right[i],\n                         ))\n            print("\\n")\n        print()\n\n    # First let\'s retrieve the decision path of each sample. The decision_path\n    # method allows to retrieve the node indicator functions. A non zero element of\n    # indicator matrix at the position (i, j) indicates that the sample i goes\n    # through the node j.\n\n    node_indicator = estimator.decision_path(X_test)\n\n    # Similarly, we can also have the leaves ids reached by each sample.\n\n    leave_id = estimator.apply(X_test)\n\n    # Now, it\'s possible to get the tests that were used to predict a sample or\n    # a group of samples. First, let\'s make it for the sample.\n\n    #sample_id = 0\n    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                        node_indicator.indptr[sample_id + 1]]\n\n    print(X_test[sample_id,:])\n\n    print(\'Rules used to predict sample %s: \' % sample_id)\n    for node_id in node_index:\n        # tabulation = " "*node_depth[node_id] #-&gt; makes tabulation of each level of the tree\n        tabulation = ""\n        if leave_id[sample_id] == node_id:\n            print("%s==&gt; Predicted leaf index \\n"%(tabulation))\n            #continue\n\n        if (X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]):\n            threshold_sign = "&lt;="\n        else:\n            threshold_sign = "&gt;"\n\n        print("%sdecision id node %s : (X_test[%s, \'%s\'] (= %s) %s %s)"\n              % (tabulation,\n                 node_id,\n                 sample_id,\n                 feature_names[feature[node_id]],\n                 X_test[sample_id, feature[node_id]],\n                 threshold_sign,\n                 threshold[node_id]))\n    print("%sPrediction for sample %d: %s"%(tabulation,\n                                          sample_id,\n                                          estimator.predict(X_test)[sample_id]))\n\n    # For a group of samples, we have the following common node.\n    sample_ids = [sample_id, 1]\n    common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n                    len(sample_ids))\n\n    common_node_id = np.arange(n_nodes)[common_nodes]\n\n    print("\\nThe following samples %s share the node %s in the tree"\n          % (sample_ids, common_node_id))\n    print("It is %s %% of all nodes." % (100 * len(common_node_id) / n_nodes,))\n\n    for sample_id_ in sample_ids:\n        print("Prediction for sample %d: %s"%(sample_id_,\n                                          estimator.predict(X_test)[sample_id_]))\n\nfor i,e in enumerate(estimator.estimators_):\n\n    print("Tree %d\\n"%i)\n    explore_tree(estimator.estimators_[i],n_nodes_[i],children_left_[i],\n                 children_right_[i], feature_[i],threshold_[i],\n                suffix=i, sample_id=1, feature_names=["Feature_%d"%i for i in range(X.shape[1])])\n    print(\'\\n\'*2)\n\nTree 1\n\nThe binary tree structure has 115 nodes\n[ 2.36609963  1.32658511 -0.08002818  0.88295736  2.24224824 -0.71469736]\nRules used to predict sample 1: \ndecision id node 0 : (X_test[1, \'Feature_3\'] (= 0.8829573603562209) &gt; 0.7038955688476562)\ndecision id node 86 : (X_test[1, \'Feature_2\'] (= -0.08002817952064323) &gt; -1.4465678930282593)\ndecision id node 92 : (X_test[1, \'Feature_0\'] (= 2.366099632530947) &gt; 0.7020512223243713)\ndecision id node 102 : (X_test[1, \'Feature_5\'] (= -0.7146973587899221) &gt; -1.2842652797698975)\ndecision id node 106 : (X_test[1, \'Feature_2\'] (= -0.08002817952064323) &gt; -0.4031955599784851)\ndecision id node 110 : (X_test[1, \'Feature_0\'] (= 2.366099632530947) &gt; 0.717217206954956)\ndecision id node 112 : (X_test[1, \'Feature_4\'] (= 2.2422482391211678) &lt;= 3.0181679725646973)\n==&gt; Predicted leaf index\ndecision id node 113 : (X_test[1, \'Feature_4\'] (= 2.2422482391211678) &gt; -2.0)\nPrediction for sample 1: 1.0\n\nThe following samples [1, 1] share the node [  0  86  92 102 106 110 112 113] in the tree\nIt is 6.956521739130435 % of all nodes.\nPrediction for sample 1: 1.0\nPrediction for sample 1: 1.0\n\n\n\nTree 2\n\nThe binary tree structure has 135 nodes\n[ 2.36609963  1.32658511 -0.08002818  0.88295736  2.24224824 -0.71469736]\nRules used to predict sample 1: \ndecision id node 0 : (X_test[1, \'Feature_3\'] (= 0.8829573603562209) &gt; 0.5484486818313599)\ndecision id node 88 : (X_test[1, \'Feature_2\'] (= -0.08002817952064323) &gt; -0.7239605188369751)\ndecision id node 102 : (X_test[1, \'Feature_5\'] (= -0.7146973587899221) &gt; -1.6143207550048828)\ndecision id node 110 : (X_test[1, \'Feature_0\'] (= 2.366099632530947) &gt; 2.3399271965026855)\ndecision id node 130 : (X_test[1, \'Feature_5\'] (= -0.7146973587899221) &lt;= -0.5680553913116455)\ndecision id node 131 : (X_test[1, \'Feature_0\'] (= 2.366099632530947) &lt;= 2.4545814990997314)\n==&gt; Predicted leaf index\ndecision id node 132 : (X_test[1, \'Feature_4\'] (= 2.2422482391211678) &gt; -2.0)\nPrediction for sample 1: 0.0\n\nThe following samples [1, 1] share the node [  0  88 102 110 130 131 132] in the tree\nIt is 5.185185185185185 % of all nodes.\nPrediction for sample 1: 0.0\nPrediction for sample 1: 0.0\n'
"from keras.applications.resnet50 import decode_predictions\n\nbase_model = resnet50 (weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 7 classes\npredictions = Dense(7, activation='softmax')(x) \nmodel = Model(inputs=base_model.input, outputs=predictions)\n...\n"
'tf_train_dataset = tf.placeholder(tf.float32, shape=(None, image_size * image_size),name="train_to_restore")\ntf_train_labels = tf.placeholder(tf.float32, shape=(None, num_labels))\n\nfrom skimage import io\nimg = io.imread(\'newimage.png\', as_grey=True)\nnx, ny = img.shape\nimg_flat = img.reshape(nx * ny)\nIMG = np.reshape(img,(1,784))\nanswer = session.run(train_prediction, feed_dict={tf_train_dataset: IMG})\nprint(answer)\n'
'review_indices = tft.compute_and_apply_vocabulary(\n                    review_tokens, top_k=VOCAB_SIZE)\n'
"from keras import optimizers\n\n# ...\nmodel.compile(optimizer=optimizers.RMSprop(lr=0.1), loss='mean_squared_error', metrics=['mae'])\n"
"import numpy as np\nimport tensorflow as tf\n\nx_dims = 3\nbatch_size = 4\n\nx = tf.placeholder(tf.float32, (None, x_dims))\ny = 2*(x**2)\nz = tf.stack([y, y]) # There are twice as many z's as x's\n\ndy_dx = tf.gradients(y,x)\ndz_dx = tf.gradients(z,x)\n\nsess = tf.Session()\n\nx_val = np.random.randint(0, 10, (batch_size, x_dims))\ny_val, z_val, dy_dx_val, dz_dx_val = sess.run([y, z, dy_dx, dz_dx], {x:x_val})\n\nprint('x.shape =', x_val.shape)\nprint('x = \\n', x_val)\nprint('y.shape = ', y_val.shape)\nprint('y = \\n', y_val)\nprint('z.shape = ', z_val.shape)\nprint('z = \\n', z_val)\nprint('dy/dx = \\n', dy_dx_val[0])\nprint('dz/dx = \\n', dz_dx_val[0])\n\nx.shape = (4, 3)\nx = \n [[1 4 8]\n [0 2 8]\n [2 8 1]\n [4 5 2]]\n\ny.shape =  (4, 3)\ny = \n [[  2.  32. 128.]\n [  0.   8. 128.]\n [  8. 128.   2.]\n [ 32.  50.   8.]]\n\nz.shape =  (2, 4, 3)\nz = \n [[[  2.  32. 128.]\n  [  0.   8. 128.]\n  [  8. 128.   2.]\n  [ 32.  50.   8.]]\n\n [[  2.  32. 128.]\n  [  0.   8. 128.]\n  [  8. 128.   2.]\n  [ 32.  50.   8.]]]\n\ndy/dx = \n [[ 4. 16. 32.]\n [ 0.  8. 32.]\n [ 8. 32.  4.]\n [16. 20.  8.]]\ndz/dx = \n [[ 8. 32. 64.]\n [ 0. 16. 64.]\n [16. 64.  8.]\n [32. 40. 16.]]\n"
"def _set_index_array(self):\n    self.index_array = np.arange(self.n)\n    if self.shuffle: # if shuffle==True, shuffle the indices\n        self.index_array = np.random.permutation(self.n) \n\ndef _get_batches_of_transformed_samples(self, index_array):\n    batch_x = np.zeros(tuple([len(index_array)] + list(self.x.shape)[1:]),\n                       dtype=self.dtype)\n    # use index_array to get the x's\n    for i, j in enumerate(index_array):\n        x = self.x[j]\n        ... # data augmentation is done here\n        batch_x[i] = x\n     ...\n     # use the same index_array to fetch the labels\n     output += (self.y[index_array],)\n\n    return output\n"
'from sklearn.linear_model import LinearRegression\ntrain_copy = train[[\'OverallQual\', \'AllSF\',\'GrLivArea\',\'GarageCars\']]\ntrain_copy =pd.get_dummies(train_copy)\ntrain_copy=train_copy.fillna(0)\nlinear_regr_test = LinearRegression()\n\nfig, axes = plt.subplots(1,len(train_copy.columns.values),sharey=True,constrained_layout=True,figsize=(30,15))\n\nfor i,e in enumerate(train_copy.columns):\n  linear_regr_test.fit(train_copy[e].values[:,np.newaxis], y.values)\n  axes[i].set_title("Best fit line")\n  axes[i].set_xlabel(str(e))\n  axes[i].set_ylabel(\'SalePrice\')\n  axes[i].scatter(train_copy[e].values[:,np.newaxis], y,color=\'g\')\n  axes[i].plot(train_copy[e].values[:,np.newaxis], \n  linear_regr_test.predict(train_copy[e].values[:,np.newaxis]),color=\'k\')\n'
'ValueError: For multi-metric scoring, the parameter refit must be set to a scorer \nkey to refit an estimator with the best parameter setting on the whole data and make\nthe best_* attributes available for that metric. If this is not needed, refit should \nbe set to False explicitly. True was passed.\n'
'orig = torch.randint(low=0, high=10, size=(2,3,2,2))\nfake = torch.randint(low=111, high=119, size=(2,1,2,2))\norig[:,[2],:,:] = fake\n\ntensor([[[[0, 1],\n      [8, 0]],\n\n     [[4, 9],\n      [6, 1]],\n\n     [[8, 2],\n      [7, 6]]],\n\n\n    [[[1, 1],\n      [8, 5]],\n\n     [[5, 0],\n      [8, 6]],\n\n     [[5, 5],\n      [2, 8]]]])\n\ntensor([[[[117, 115],\n      [114, 111]]],\n\n\n    [[[115, 115],\n      [118, 115]]]])\n\ntensor([[[[  0,   1],\n      [  8,   0]],\n\n     [[  4,   9],\n      [  6,   1]],\n\n     [[117, 115],\n      [114, 111]]],\n\n\n    [[[  1,   1],\n      [  8,   5]],\n\n     [[  5,   0],\n      [  8,   6]],\n\n     [[115, 115],\n      [118, 115]]]])\n'
'In [1]: x\nOut[1]:\ntensor([[0.9752, 0.5587, 0.0972],\n        [0.9534, 0.2731, 0.6953]])\n\nl = torch.tensor([[0.2, 0.3, 0.]])\nu = torch.tensor([[0.8, 1., 0.65]])\n\nclipped_x = torch.max(torch.min(x, u), l)\n\ntensor([[0.8000, 0.5587, 0.0972],\n        [0.8000, 0.3000, 0.6500]])\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n\n&gt;&gt;&gt; df = pd.DataFrame({'number': np.random.randint(0,10,size=5),\n                  'device': np.random.choice(['a','b'],size=5),\n                  'application': ['app2;app3','app1','app2;app4', 'app1;app2', 'app1'],\n                  'district': np.random.choice(['aa', 'bb', 'cc'],size=5)})\n\n&gt;&gt;&gt; df\n\n    application device  district    number\n0   app2;app3   b         aa    3\n1   app1        a         cc    7\n2   app2;app4   a         aa    3\n3   app1;app2   b         bb    9\n4   app1        a         cc    4\n\nfrom sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\n# Assuming appl names are separated by ;\nmhv = mlb.fit_transform(df['application'].apply(lambda x: set(x.split(';'))))\ndf_out = pd.DataFrame(mhv,columns=mlb.classes_)\n\nenc = OneHotEncoder(sparse=False)\nohe_vars = ['device','district'] # specify the list of columns here\nohv = enc.fit_transform(df.loc[:,ohe_vars])\nohe_col_names = ['%s_%s'%(var,cat) for var,cats in zip(ohe_vars, enc.categories_) for cat in cats]\n\ndf_out.assign(**dict(zip(ohe_col_names,ohv.T)))\n\ndf_out\n"
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from scipy.optimize import curve_fit\n&gt;&gt;&gt;\n&gt;&gt;&gt; def func(x, a, b, c):\n...     return a * np.exp(-b * x) + c\n\n&gt;&gt;&gt; xdata = np.linspace(0, 4, 50)\n&gt;&gt;&gt; y = func(xdata, 2.5, 1.3, 0.5)\n&gt;&gt;&gt; np.random.seed(1729)\n&gt;&gt;&gt; y_noise = 0.2 * np.random.normal(size=xdata.size)\n&gt;&gt;&gt; ydata = y + y_noise\n&gt;&gt;&gt; plt.plot(xdata, ydata, 'b-', label='data')\n\npopt, pcov = curve_fit(func, xdata, ydata)\n\ndef func(x,k,t,s):\n    return ((x[0]*k+x[1]*t)*60*x[2])*s\nX = np.array([[a,b,c] for a,b,c in zip(initial_votes_list,initial_comment_list,[i for i in range(len(initial_votes_list))])]).T\ny = score \n\npopt, pcov = curve_fit(func, X, y) \n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\ninitial_votes_list = [1.41, 0.9, 0.94, 0.47, 0]\ninitial_comment_list = [0, 3, 0, 1, 64]\n\nfinal_score = [26,12,13,14,229]\n\ndef func(x,k,t,s):\n    return ((x[0]*k+x[1]*t)*60*x[2])*s\nX = np.array([[a,b,c] for a,b,c in zip(initial_votes_list,initial_comment_list,[i for i in range(len(initial_votes_list))])]).T\ny = [0.12,0.20,0.5,0.9,1] \n\npopt, pcov = curve_fit(func, X, y)\n\n\n\nprint(popt)\n&gt;&gt;&gt;[-6.65969099e+00 -6.99241803e-02 -9.33412000e-04]\n"
'train_generator = train_datagen.flow_from_directory(\n        \'train_directory\',\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode = "categorical"\n        )\n\nclass_weights = class_weight.compute_class_weight(\n           \'balanced\',\n            np.unique(train_generator.classes), \n            train_generator.classes)\n\ncur.execute("SELECT class, count(*) FROM table group by classes order by 1")\nrows = cur.fetchall()\n\nclass_weights = {}\nfor row in rows:\n    class_weights[row[0]]=rows[0][1]/row[1] \n    #dividing the least value the current value to get the weight, \n    # so that the least value becomes 1, \n    # and other values becomes &lt; 1\n'
"from keras.models import load_model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')\n"
" @if_delegate_has_method(delegate=&quot;_final_estimator&quot;)\n    def fit_predict(self, X, y=None, **fit_params):\n        &quot;&quot;&quot;Apply `fit_predict` of last step in pipeline after transforms.\n        Applies fit_transforms of a pipeline to the data, followed by the\n        fit_predict method of the final estimator in the pipeline. Valid\n        only if the final estimator implements fit_predict.\n        Parameters\n        ----------\n        X : iterable\n            Training data. Must fulfill input requirements of first step of\n            the pipeline.\n        y : iterable, default=None\n            Training targets. Must fulfill label requirements for all steps\n            of the pipeline.\n        **fit_params : dict of string -&gt; object\n            Parameters passed to the ``fit`` method of each step, where\n            each parameter name is prefixed such that parameter ``p`` for step\n            ``s`` has key ``s__p``.\n        Returns\n        -------\n        y_pred : ndarray of shape (n_samples,)\n            The predicted target.\n        &quot;&quot;&quot;\n        Xt, yt, fit_params = self._fit(X, y, **fit_params)\n        with _print_elapsed_time('Pipeline',\n                                 self._log_message(len(self.steps) - 1)):\n            y_pred = self.steps[-1][-1].fit_predict(Xt, yt, **fit_params)\n        return y_pred\n\n def predict(self, X):\n        &quot;&quot;&quot;Predict the class labels for the provided data.\n        Parameters\n        ----------\n        X : array-like of shape (n_queries, n_features), \\\n                or (n_queries, n_indexed) if metric == 'precomputed'\n            Test samples.\n        Returns\n        -------\n        y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n            Class labels for each data sample.\n        &quot;&quot;&quot;\n        X = check_array(X, accept_sparse='csr')\n\n        neigh_dist, neigh_ind = self.kneighbors(X)\n        classes_ = self.classes_\n        _y = self._y\n        if not self.outputs_2d_:\n            _y = self._y.reshape((-1, 1))\n            classes_ = [self.classes_]\n\n        n_outputs = len(classes_)\n        n_queries = _num_samples(X)\n        weights = _get_weights(neigh_dist, self.weights)\n\n        y_pred = np.empty((n_queries, n_outputs), dtype=classes_[0].dtype)\n        for k, classes_k in enumerate(classes_):\n            if weights is None:\n                mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n            else:\n                mode, _ = weighted_mode(_y[neigh_ind, k], weights, axis=1)\n\n            mode = np.asarray(mode.ravel(), dtype=np.intp)\n            y_pred[:, k] = classes_k.take(mode)\n\n        if not self.outputs_2d_:\n            y_pred = y_pred.ravel()\n\n        return y_pred\n\nimport pandas as pd, numpy as np\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nparam_grid = [\n    {\n        'classification__n_neighbors': [1,3,5,7,10],\n    }\n]\n\nX, y = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20)\n\npipe = Pipeline([\n    ('sampling', SMOTE()),\n    ('classification', KNeighborsClassifier())\n])\n\ngrid = GridSearchCV(pipe, param_grid=param_grid)\ngrid.fit(X_train, y_train)\nmean_scores = np.array(grid.cv_results_['mean_test_score'])\nprint(mean_scores)\n\n# [0.98051926 0.98121129 0.97981998 0.98050474 0.97494193]\n"
"count = CountVectorizer(vocabulary=myvocab)\nX_vectorized = count.transform(X_train)\n\ntext_classifier = Pipeline([\n    ('tfidf', TfidfTransformer()),\n    ('clf', LinearSVC(C=1000))\n])\n\ntext_classifier.fit(X_vectorized, y_train)\n"
'&gt;&gt;&gt; from sklearn.metrics import classification_report\n&gt;&gt;&gt; print classification_report(y_true, y_predicted)\n'
'print rf.predict([testdataset[-1]])\n\nprint rf.predict(testdataset[-1:])\n\n&gt;&gt;&gt; print np.asarray(train).shape\n\n&gt;&gt;&gt; print np.asarray(target).shape\n\n&gt;&gt;&gt; print np.asarray(testdataset).shape\n'
"&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n&gt;&gt;&gt; CountVectorizer(strip_accents='ascii').build_analyzer()(u'\\xe9t\\xe9')\n[u'ete']\n&gt;&gt;&gt; CountVectorizer(strip_accents=False).build_analyzer()(u'\\xe9t\\xe9')\n[u'\\xe9t\\xe9']\n&gt;&gt;&gt; CountVectorizer(strip_accents=None).build_analyzer()(u'\\xe9t\\xe9')\n[u'\\xe9t\\xe9']\n"
'# X - some data in 2dimensional np.array\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# here "model" is your model\'s prediction (classification) function\nZ = model(np.c_[xx.ravel(), yy.ravel()]) \n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=pl.cm.Paired)\nplt.axis(\'off\')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=pl.cm.Paired)\n'
'v = sigma * np.random.randn(v_size) + b + sigma * W.dot(h)\n'
'sc = proprocessing.StandardScaler().fit(X)\nX = sc.transform(X)\n\nX_test = sc.transform(X_test)\n\nrd.fit(X, y)\nre.predict_proba(X_test)\n\nAlexaAndGoogleTrainData = p.read_table(\'train.tsv\', header=0)[["AlexaRank", "GooglePageRank"]]\nAlexaAndGoogleTestData = p.read_table(\'test.tsv\', header=0)[["AlexaRank", "GooglePageRank"]]\nAllAlexaAndGoogleInfo = AlexaAndGoogleTestData.append(AlexaAndGoogleTrainData)\n\nX = np.hstack((X, AllAlexaAndGoogleInfo))\n'
"import numpy as np\n#create a 5X5 dataframe\ndf = pd.DataFrame(np.random.random_integers(0, 100, (5, 5)), columns = ['X1','X2','X3','X4','y'])\n\nX = df[['X1','X2','X3','X4']].as_matrix()\n\ny =df['y'].values\n\nX = df[['X1','X2','X3','X4']].values\n"
'from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\n# training data\ntrain_x = [0,1,2,6,\'true\',\'false\']\nle.fit_transform(train_x)\n# array([0, 1, 1, 2, 4, 3])\n\n# transform some new data\nnew_x = [0,0,0,2,2,2,\'false\']\nle.transform(new_x)\n# array([0, 0, 0, 1, 1, 1, 3])\n\n# transform data with a new feature\nbad_x = [0,2,6,\'new_word\']\nle.transform(bad_x)\n# ValueError: y contains new labels: [\'0\' \'new_word\']\n\nimport cPickle as pickle\nfrom sklearn.externals import joblib\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\ntrain_x = [0,1,2,6,\'true\',\'false\']\nle.fit_transform(train_x)\n\n# Save your encoding\njoblib.dump(le, \'/path/to/save/model\')\n# OR\npickle.dump(le, open( \'/path/to/model\', "wb" ) )\n\n# Load those encodings\nle = joblib.load(\'/path/to/save/model\') \n# OR\nle = pickle.load( open( \'/path/to/model\', "rb" ) )\n\n# Then use as normal\nnew_x = [0,0,0,2,2,2,\'false\']\nle.transform(new_x)\n# array([0, 0, 0, 1, 1, 1, 3])\n'
'pip install pandas\npip install scikit-learn\n\nimport pandas as pd\n\ntrain_data_contents = """\nclass_label,distance_from_beginning,distance_from_end,contains_digit,capitalized\nB,1,10,1,0\nM,10,1,0,1\nC,2,3,0,1\nS,23,2,0,0\nN,12,0,0,1"""\n\n\nwith open(\'train.csv\', \'w\') as output:\n    output.write(train_data_contents)\n\ntrain_dataframe = pd.read_csv(\'train.csv\')\n\nimport numpy as np\n\ntrain_labels = train_dataframe.class_label\nlabels = list(set(train_labels))\ntrain_labels = np.array([labels.index(x) for x in train_labels])\ntrain_features = train_dataframe.iloc[:,1:]\ntrain_features = np.array(train_features)\n\nprint "train labels: "\nprint train_labels\nprint \nprint "train features:"\nprint train_features\n\nfrom sklearn import svm\nclassifier = svm.SVC()\nclassifier.fit(train_features, train_labels)\n\ntest_data_contents = """\nclass_label,distance_from_beginning,distance_from_end,contains_digit,capitalized\nB,1,10,1,0\nM,10,1,0,1\nC,2,3,0,1\nS,23,2,0,0\nN,12,0,0,1\n"""\n\nwith open(\'test.csv\', \'w\') as output:\n    output.write(test_data_contents)\n\ntest_dataframe = pd.read_csv(\'test.csv\')\n\ntest_labels = test_dataframe.class_label\nlabels = list(set(test_labels))\ntest_labels = np.array([labels.index(x) for x in test_labels])\n\ntest_features = test_dataframe.iloc[:,1:]\ntest_features = np.array(test_features)\n\nresults = classifier.predict(test_features)\nnum_correct = (results == test_labels).sum()\nrecall = num_correct / len(test_labels)\nprint "model accuracy (%): ", recall * 100, "%"\n'
'step: 2000\nloss: 0.00103311243281\n\ninput: [0.0, 0.0] | output: [[ 0.00019799]]\ninput: [0.0, 1.0] | output: [[ 0.99979786]]\ninput: [1.0, 0.0] | output: [[ 0.99996307]]\ninput: [1.0, 1.0] | output: [[ 0.00033751]]\n\nimport tensorflow as tf    \n\n#####################\n# preparation stuff #\n#####################\n\n# define input and output data\ninput_data = [[0., 0.], [0., 1.], [1., 0.], [1., 1.]]  # XOR input\noutput_data = [[0.], [1.], [1.], [0.]]  # XOR output\n\n# create a placeholder for the input\n# None indicates a variable batch size for the input\n# one input\'s dimension is [1, 2] and output\'s [1, 1]\nn_input = tf.placeholder(tf.float32, shape=[None, 2], name="n_input")\nn_output = tf.placeholder(tf.float32, shape=[None, 1], name="n_output")\n\n# number of neurons in the hidden layer\nhidden_nodes = 5\n\n\n################\n# hidden layer #\n################\n\n# hidden layer\'s bias neuron\nb_hidden = tf.Variable(tf.random_normal([hidden_nodes]), name="hidden_bias")\n\n# hidden layer\'s weight matrix initialized with a uniform distribution\nW_hidden = tf.Variable(tf.random_normal([2, hidden_nodes]), name="hidden_weights")\n\n# calc hidden layer\'s activation\nhidden = tf.sigmoid(tf.matmul(n_input, W_hidden) + b_hidden)\n\n\n################\n# output layer #\n################\n\nW_output = tf.Variable(tf.random_normal([hidden_nodes, 1]), name="output_weights")  # output layer\'s weight matrix\noutput = tf.sigmoid(tf.matmul(hidden, W_output))  # calc output layer\'s activation\n\n\n############\n# learning #\n############\ncross_entropy = -(n_output * tf.log(output) + (1 - n_output) * tf.log(1 - output))\n# cross_entropy = tf.square(n_output - output)  # simpler, but also works\n\nloss = tf.reduce_mean(cross_entropy)  # mean the cross_entropy\noptimizer = tf.train.AdamOptimizer(0.01)  # take a gradient descent for optimizing with a "stepsize" of 0.1\ntrain = optimizer.minimize(loss)  # let the optimizer train\n\n\n####################\n# initialize graph #\n####################\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()  # create the session and therefore the graph\nsess.run(init)  # initialize all variables  \n\n#####################\n# train the network #\n#####################\nfor epoch in xrange(0, 2001):\n    # run the training operation\n    cvalues = sess.run([train, loss, W_hidden, b_hidden, W_output],\n                       feed_dict={n_input: input_data, n_output: output_data})\n\n    # print some debug stuff\n    if epoch % 200 == 0:\n        print("")\n        print("step: {:&gt;3}".format(epoch))\n        print("loss: {}".format(cvalues[1]))\n        # print("b_hidden: {}".format(cvalues[3]))\n        # print("W_hidden: {}".format(cvalues[2]))\n        # print("W_output: {}".format(cvalues[4]))\n\n\nprint("")\nprint("input: {} | output: {}".format(input_data[0], sess.run(output, feed_dict={n_input: [input_data[0]]})))\nprint("input: {} | output: {}".format(input_data[1], sess.run(output, feed_dict={n_input: [input_data[1]]})))\nprint("input: {} | output: {}".format(input_data[2], sess.run(output, feed_dict={n_input: [input_data[2]]})))\nprint("input: {} | output: {}".format(input_data[3], sess.run(output, feed_dict={n_input: [input_data[3]]})))\n'
"# Train network\nprint('Starting training....')\ntrainingComplete = False\nwhile not trainingComplete:\n    error = net.train(trainingData, TS, epochs=epochs, show=10, goal=0.001)\n    if len(error) &lt; 0.8 * epochs:\n       if len(error) &gt; 0 and min(error) &lt; 0.01:\n           trainingComplete = True\n       else:\n           print('Restarting....')\n           net = createNeuralNetwork(trainingData, [hidden], 1)\n           net.trainf = train_bfgs\n    else:  \n       trainingComplete = True\n\nStarting training....\nRestarting....\nRestarting....\nRestarting....\nRestarting....\nRestarting....\nRestarting....\nRestarting....\nRestarting....\nEpoch: 10; Error: 1.46314116045;\nEpoch: 20; Error: 0.759613243435;\nEpoch: 30; Error: 0.529574731856;\n.\n.\n"
"&gt; print progress\n{'train-rmse': {'error': ['0.50000', ....]}, 'eval-rmse': { 'error': ['0.5000',....]}}\n"
'from sklearn.linear_model.logistic import _logistic_loss\nprint _logistic_loss(clf.coef_, X, y, 1 / clf.C)\n'
"import caffe\nfrom caffe import layers a L, params as P\n\nns = caffe.NetSpec()\nns.conv = L.Convolution(bottom, convolution_param={'kernel_size':ks,\n                                                   'stride':stride,\n                                                   'num_output':nout, \n                                                   'pad':pad, \n                                                   'group':group},\n                                param=[{'lr_mult':1, 'decay_mult':1},\n                                       {'lr_mult':2, 'decay_mult':0}],\n                                include={'phase': caffe.TRAIN})\n"
'~/.keras/keras.json\n'
"df = pd.DataFrame({'A': [0, 8, 2, 5, 9, 15, 1]})\n\npd.cut(df['A'], bins=[0, 2, 8, 10], include_lowest=True)\nOut[33]: \n0     [0, 2]\n1     (2, 8]\n2     [0, 2]\n3     (2, 8]\n4    (8, 10]\n5        NaN\n6     [0, 2]\nName: A, dtype: category\nCategories (3, object): [[0, 2] &lt; (2, 8] &lt; (8, 10]]\n\npd.cut(df['A'], bins=[0, 2, 8, 10], include_lowest=True, labels=['low', 'mid', 'high'])\nOut[34]: \n0     low\n1     mid\n2     low\n3     mid\n4    high\n5     NaN\n6     low\nName: A, dtype: category\nCategories (3, object): [low &lt; mid &lt; high]\n\npd.cut(df['A'], bins=[0, 2, 8, 10, 1000], include_lowest=True, labels=['low', 'mid', 'high', 'excluded'])\nOut[35]: \n0         low\n1         mid\n2         low\n3         mid\n4        high\n5    excluded\n6         low\nName: A, dtype: category\nCategories (4, object): [low &lt; mid &lt; high &lt; excluded]\n"
'    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    if gradient_clipping:\n        gradients = optimizer.compute_gradients(loss)\n\n        def ClipIfNotNone(grad):\n            if grad is None:\n                return grad\n            return tf.clip_by_value(grad, -1, 1)\n        clipped_gradients = [(ClipIfNotNone(grad), var) for grad, var in gradients]\n        opt = optimizer.apply_gradients(clipped_gradients, global_step=global_step)\n    else:\n        opt = optimizer.minimize(loss, global_step=global_step)\n'
"1 -&gt; 1 - ϵ\n0 -&gt; ϵ / (k-1) \n\nx -&gt; x * (1 - ϵ) + (1-x) * ϵ / (k-1)\n\nindices = ['a', 'b', 'c', 'd']\neps = 0.1\ndf[indices] = df[indices] * (1 - eps) + (1-df[indices]) * eps / (len(indices) - 1)\n\n&gt;&gt;&gt; df\n   a  b  c  d\n0  1  0  0  0\n1  0  1  0  0\n2  0  0  0  1\n3  1  0  0  0\n4  0  1  0  0\n5  0  0  1  0\n\n        a         b         c         d\n0  0.900000  0.033333  0.033333  0.033333\n1  0.033333  0.900000  0.033333  0.033333\n2  0.033333  0.033333  0.033333  0.900000\n3  0.900000  0.033333  0.033333  0.033333\n4  0.033333  0.900000  0.033333  0.033333\n5  0.033333  0.033333  0.900000  0.033333\n"
'import pandas as pd\ndf2 = pd.merge(df0,df1, left_index=True, right_index=True)\n'
" data = coo_matrix(data)\n\nIn [20]: data = [\n    ...:     [1, 0], \n    ...:     [2, 1], \n    ...:     [3, 2],\n    ...:     [4, 3]\n    ...: ]\n\nIn [21]: ds = sparse.coo_matrix(data)\nIn [22]: ds.A\nOut[22]: \narray([[1, 0],\n       [2, 1],\n       [3, 2],\n       [4, 3]])\n\nIn [23]: data=np.array(data)\nIn [24]: ds=sparse.coo_matrix((np.ones(4,int),(data[:,0],data[:,1])))\nIn [25]: ds\nOut[25]: \n&lt;5x4 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 4 stored elements in COOrdinate format&gt;\nIn [26]: ds.A\nOut[26]: \narray([[0, 0, 0, 0],\n       [1, 0, 0, 0],\n       [0, 1, 0, 0],\n       [0, 0, 1, 0],\n       [0, 0, 0, 1]])\n"
"rows = (df[['A','A2']] == df2[['A','A2']]).all(axis=1)\ndf.loc[rows,'B'] = df2.loc[rows,'B']\n"
'# First build 100 trees on X1, y1\nclf = RandomForestClassifier(n_estimators=100, warm_start=True)\nclf.fit(X1, y1)\n\n# Build 100 additional trees on X2, y2\nclf.set_params(n_estimators=200)\nclf.fit(X2, y2)\n\ndef generate_rf(X_train, y_train, X_test, y_test):\n    rf = RandomForestClassifier(n_estimators=5, min_samples_leaf=3)\n    rf.fit(X_train, y_train)\n    print "rf score ", rf.score(X_test, y_test)\n    return rf\n\ndef combine_rfs(rf_a, rf_b):\n    rf_a.estimators_ += rf_b.estimators_\n    rf_a.n_estimators = len(rf_a.estimators_)\n    return rf_a\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)\n# Create \'n\' random forests classifiers\nrf_clf = [generate_rf(X_train, y_train, X_test, y_test) for i in range(n)]\n# combine classifiers\nrf_clf_combined = reduce(combine_rfs, rfs)\n'
'scalerY = StandardScaler().fit(trainY)  # fit y scaler\npipeline.fit(trainX, scalerY.transform(trainY))  # fit your pipeline to scaled Y\ntestY = scalerY.inverse_transform(pipeline.predict(testX))  # predict and rescale\n'
'omp = OrthogonalMatchingPursuit(n_nonzero_coefs=target_sparsity)\nomp.fit(D, y)\n'
'from sklearn.utils.testing import all_estimators\nfrom sklearn import base\n\nestimators = all_estimators()\n\nfor name, class_ in estimators:\n    if issubclass(class_, base.ClassifierMixin):\n        print(name)\n'
'from pyspark.ml.clustering import KMeans\nfrom pyspark.ml.feature import VectorAssembler\ndf = spark.read.option("inferSchema", "true").option("header", "true").csv("whole_customers_data.csv")\ncols = df.columns\nvectorAss = VectorAssembler(inputCols=cols, outputCol="features")\nvdf = vectorAss.transform(df)\nkmeans = KMeans(k=2, maxIter=10, seed=1)\nkmm = kmeans.fit(vdf)\nkmm.clusterCenters()\n'
"x = mx.sym.Variable('x')\ny = mx.sym.Variable('y')\nz = x + y\nexecutor = z.bind(mx.cpu(), {'x': mx.nd.array([100,200]), 'y':mx.nd.array([300,400])})\noutput = executor.forward()\n\n[&lt;NDArray 2 @cpu(0)&gt;]\n\nprint output[0].asnumpy()\narray([ 400.,  600.], dtype=float32)\n"
"a = mx.sym.Variable('a')\nb = mx.sym.Variable('b')\nc = mx.sym.Concat(a,b,dim=0)\n\ne = c.bind(mx.cpu(), {'a': mx.nd.array([100,200]), 'b':mx.nd.array([300,400])})\ny = e.forward()\ny[0].asnumpy()\n\narray([ 100.,  200.,  300.,  400.], dtype=float32)\n"
'pr = model.predict_classes(im.reshape((1, 1, 28, 28)))\n'
'x_test = pd.read_csv("x_test.csv",sep=\';\',header=None)\ndf = pdml.ModelFrame(x_test.to_dict(orient=\'list\'))\n'
'import pandas as pd                                                                                                                                                                                                                                                                \nimport numpy as np                                                                                                                                                                                                                                                                 \nimport keras.optimizers                                                                                                                                                                                                                                                            \nfrom keras.models import Sequential                                                                                                                                                                                                                                                \nfrom keras.layers import Dense,Activation                                                                                                                                                                                                                                          \nfrom keras.layers import LSTM                                                                                                                                                                                                                                                      \n\n#random df                                                                                                                                                                                                                                                                        \ndf = pd.DataFrame({\'date\': np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),                                                                                                                                                                                                               \n               \'feature_1\': np.random.randint(10, size=10),                                                                                                                                                                                                                    \n               \'feature_2\': np.random.randint(10, size=10),                                                                                                                                                                                                                    \n               \'feature_3\': np.random.randint(10, size=10),                                                                                                                                                                                                                    \n               \'feature_4\': np.random.randint(10, size=10),                                                                                                                                                                                                                    \n               \'output\': np.random.randint(10, size=10)}                                                                                                                                                                                                                       \n             )                                                                                                                                                                                                                                                                 \n\n# set date as index                                                                                                                                                                                                                                                                \ndf.index = df.date                                                                                                                                                                                                                                                                 \ndf = df.drop(\'date\', 1)                                                                                                                                                                                                                                                            \n\nnb_epoch = 10                                                                                                                                                                                                                                                                      \nbatch_size = 10                                                                                                                                                                                                                                                                    \nlearning_rate = 0.01                                                                                                                                                                                                                                                               \nnb_units = 50                                                                                                                                                                                                                                                                       \ntimeStep = 3                                                                                                                                                                                                                                                                       \n\nX = df[[\'feature_\'+str(i) for i in range(1,5)]].values # Select good columns                                                                                                                                                                                                        \nsizeX = X.shape[0]-X.shape[0]%timeStep  # Choose a number of observations that is a multiple of the timstep                                                                                                                                                                            \nX = X[:sizeX]                                                                                                                                                                                                                                                                      \nX = X.reshape(X.shape[0]/timeStep,timeStep,X.shape[1]) # Create X with shape (nb_sample,timestep,nb_features)                                                                                                                                                                       \n\nY = df[[\'output\']].values                                                                                                                                                                                                                                                          \nY = Y[range(3,len(Y),3)] #Select the good output                                                                                                                                                                                                                                   \n\n\nmodel = Sequential()                                                                                                                                                                                                                                                               \nmodel.add(LSTM(input_dim = X.shape[2],output_dim = nb_units,return_sequences = False)) # One LSTM layer with 50 units                                                                                                                                                               \nmodel.add(Activation("sigmoid"))                                                                                                                                                                                                                                                   \nmodel.add(Dense(1)) #A dense layer which is the final layer                                                                                                                                                                                                                        \nmodel.add(Activation(\'linear\'))                \n\nKerasOptimizer = keras.optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=1e-08, decay=0.0)                                                                                                                                                                                     \nmodel.compile(loss="mse", optimizer=KerasOptimizer)                                                                                                                                                                                                                                \nmodel.fit(X,Y,nb_epoch = nb_epoch,batch_size = batch_size)                                                                                                                                                                                                                         \nprediction = model.predict(X)  \n'
'conda install ecos  \nconda install CVXcanon  \npip install fancyimpute  \n'
'import tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn import learn_runner\nfrom tensorflow.contrib.learn.python.learn.utils import (\n    saved_model_export_utils)\nfrom tensorflow.contrib.training.python.training import hparam\n\ndef csv_serving_input_fn():\n    """Build the serving inputs."""\n    csv_row = tf.placeholder(\n        shape=[None],\n        dtype=tf.string\n    )\n    features = parse_csv(csv_row)\n    # Ignore label column\n    features.pop(LABEL_COLUMN)\n    return tf.estimator.export.ServingInputReceiver(\n        features, {\'csv_row\': csv_row})\n\n\ndef example_serving_input_fn():\n    """Build the serving inputs."""\n    example_bytestring = tf.placeholder(\n        shape=[None],\n        dtype=tf.string,\n    )\n    features = tf.parse_example(\n        example_bytestring,\n        tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n    )\n    return tf.estimator.export.ServingInputReceiver(\n        features, {\'example_proto\': example_bytestring})\n\n\ndef json_serving_input_fn():\n  """Build the serving inputs."""\n  inputs = {}\n  for feat in INPUT_COLUMNS:\n    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\n  return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\nSERVING_FUNCTIONS = {\n    \'JSON\': json_serving_input_fn,\n    \'EXAMPLE\': example_serving_input_fn,\n    \'CSV\': csv_serving_input_fn\n}\n\n# Run the training job\n# learn_runner pulls configuration information from environment\n# variables using tf.learn.RunConfig and uses this configuration\n# to conditionally execute Experiment, or param server code\nlearn_runner.run(\n  generate_experiment_fn(\n      min_eval_frequency=args.min_eval_frequency,\n      eval_delay_secs=args.eval_delay_secs,\n      train_steps=args.train_steps,\n      eval_steps=args.eval_steps,\n      export_strategies=[saved_model_export_utils.make_export_strategy(\n          SERVING_FUNCTIONS[args.export_format],\n          exports_to_keep=1\n      )]\n  ),\n  run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n  hparams=hparam.HParams(**args.__dict__)\n)\n'
"tfidf_vec = TfidfVectorizer()\ntransformed = tfidf_vec.fit_transform(raw_documents=['this is a quick example','just to show off'])\nindex_value={i[1]:i[0] for i in tfidf_vec.vocabulary_.items()}\n\nfully_indexed = []\nfor row in transformed:\n    fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n\n[{'example': 0.5, 'is': 0.5, 'quick': 0.5, 'this': 0.5},\n {'just': 0.5, 'off': 0.5, 'show': 0.5, 'to': 0.5}]\n\nfully_indexed = []\ntransformed = np.array(transformed.todense())\nfor row in transformed:\n    fully_indexed.append({index_value[column]:value for (column,value) in enumerate(row)})\n\n[{'example': 0.5,'is': 0.5,'just': 0.0,'off': 0.0,'quick': 0.5,'show': 0.0,'this': 0.5,'to': 0.0},\n {'example': 0.0,'is': 0.0,'just': 0.5,'off': 0.5,'quick': 0.0,'show': 0.5,'this': 0.0,'to': 0.5}]\n\ndf['tf_idf'] = fully_indexed\n"
"In [2]: w = tf.Variable([1,2,3], collections=[tf.GraphKeys.WEIGHTS], dtype=tf.float32)\nIn [3]: w2 = tf.Variable([11,22,32], collections=[tf.GraphKeys.WEIGHTS], dtype=tf.float32)\n\ntf.get_collection_ref(tf.GraphKeys.WEIGHTS)\n\n[&lt;tf.Variable 'Variable:0' shape=(3,) dtype=float32_ref&gt;,\n &lt;tf.Variable 'Variable_1:0' shape=(3,) dtype=float32_ref&gt;]\n"
"fig, ax = plt.subplots()\n\ngroups = pd.DataFrame(X_tsne, columns=['x', 'y']).assign(category=y).groupby('category')\nfor name, points in groups:\n    ax.scatter(points.x, points.y, label=name)\n\nax.legend()\n"
'import pandas as pd\nimport tensorflow as tf\nimport tempfile\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\n\n\ndef split_data(data, rate, label):\n    data = data.dropna()\n\n    train_data, test_data = train_test_split(data, test_size=rate)\n\n    train_label = train_data[label]\n    train_data = train_data.drop(label, 1)\n\n    test_label = test_data[label]\n    test_data = test_data.drop(label, 1)\n    return train_data, train_label, test_data, test_label\n\n\n\nLABEL = "Exited"\n\ndata = pd.read_csv("Churn_Modelling.csv", skipinitialspace=True, \n    header=0)\n\ndata.drop("Surname", axis=1, inplace=True)\ndata.drop("RowNumber", axis=1, inplace=True)\ndata.drop("CustomerId", axis=1, inplace=True)\ndata.drop("Geography", axis=1, inplace=True)\ndata.drop("Gender", axis=1, inplace=True)\nx_train, y_train, x_test, y_test = split_data(data, 0.20, LABEL)\n\n\n\ndef get_input_fn_train():\n    input_fn = tf.estimator.inputs.pandas_input_fn(\n        x=x_train,\n        y=y_train,\n        shuffle=False\n    )\n    return input_fn\n\ndef get_input_fn_test():\n    input_fn = tf.estimator.inputs.pandas_input_fn(\n        x=x_test,\n        y=y_test,\n        shuffle=False\n    )\n    return input_fn\n\n\nfeature_columns = tf.contrib.learn.infer_real_valued_columns_from_input_fn\n(get_input_fn_train())\n\n\nmodel_dir = tempfile.mkdtemp()\nm = tf.estimator.LinearClassifier(model_dir=model_dir, \nfeature_columns=feature_columns)\n\n# train data\nm.train(input_fn=get_input_fn_train(), steps=5000)\n\n# you can get accuracy, accuracy_baseline, auc, auc_precision_recall, \n#average_loss, global_step, label/mean, lossprediction/mean\n\nresults = m.evaluate(input_fn=get_input_fn_test(), steps=None)\n\nprint("model directory = %s" % model_dir)\nfor key in sorted(results):\n    print("%s: %s" % (key, results[key]))\n\n# get prediction results\ny = m.predict(input_fn=get_input_fn_test())\npredictions = list(y)\npred1=pd.DataFrame(data=predictions)\nprediction=pd.DataFrame(data=pred1[\'class_ids\'])\npred=[]\nfor row in prediction["class_ids"]:\n    pred.append(row[0])\n\nrowNumber = 0\nfor i in pred:\n    print(str(rowNumber) + \': \' + str(i))\n    rowNumber = rowNumber + 1\n\n\ndef calculate(prediction, LABEL):\n    arr = {"accuracy": accuracy_score(prediction, LABEL),\n           "report": classification_report(prediction, LABEL),\n           "Confusion_Matrix": confusion_matrix(prediction, LABEL),\n           "F1 score": f1_score(prediction, LABEL),\n           "Recall Score": recall_score(prediction, LABEL),\n           "cohen_kappa": cohen_kappa_score(prediction, LABEL)\n           }\n    return arr\n\n\npred2 = pd.DataFrame(data=pred)\n\nprint(calculate(pred2.round(), y_test))\n'
'try:\n    model.fit(X, y)\nfinally:\n    model.save_model()\n'
'error1 = np.dot((A2_sig - Y_trainT.T).T , A1_sig / M)\n\nerror1 = np.dot((A2_sig - Y_trainT.T).T , A1_sig / M)\n'
"correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_predicted, 1))\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nx = tf.placeholder(tf.float32, [None, 784], 'images')\ny = tf.placeholder(tf.float32, [None, 10], 'labels')\n\nW0 = tf.Variable(dtype=tf.float32, name='InputLayerWeights', initial_value=tf.truncated_normal([784, 16]) * 0.001)\nW1 = tf.Variable(dtype=tf.float32, name='HiddenLayer1Weights', initial_value=tf.truncated_normal([16, 16]) * 0.001)\nW2 = tf.Variable(dtype=tf.float32, name='HiddenLayer2Weights', initial_value=tf.truncated_normal([16, 10]) * 0.001)\n\nB0 = tf.Variable(dtype=tf.float32, name='HiddenLayer1Biases', initial_value=tf.ones([16]))\nB1 = tf.Variable(dtype=tf.float32, name='HiddenLayer2Biases', initial_value=tf.ones([16]))\nB2 = tf.Variable(dtype=tf.float32, name='OutputLayerBiases', initial_value=tf.ones([10]))\n\nA1 = tf.nn.relu(tf.matmul(x, W0) + B0)\nA2 = tf.nn.relu(tf.matmul(A1, W1) + B1)\ny_predicted = tf.matmul(A2, W2) + B2\ncorrect_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_predicted, 1))\ncorrect_prediction_float = tf.cast(correct_prediction, dtype=tf.float32)\naccuracy = tf.reduce_mean(correct_prediction_float)\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_predicted))\noptimizer = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n\nmnist = input_data.read_data_sets('mnist', one_hot=True)\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  for i in range(20000):\n    batch_x, batch_y = mnist.train.next_batch(64)\n    _, cost_val, acc_val = sess.run([optimizer, cross_entropy, accuracy], feed_dict={x: batch_x, y: batch_y})\n    if i % 100 == 0:\n      print('cost=%.3f accuracy=%.3f' % (cost_val, acc_val))\n"
"from scipy.io import wavfile as wav\nfrom scipy.fftpack import fft\nimport numpy as np\nrate, data = wav.read('music.wav')\nfft_out = fft(data)\n"
'import numpy as np\nfrom sklearn.metrics import log_loss\n\n\ndef cross_entropy(predictions, targets):\n    N = predictions.shape[0]\n    ce = -np.sum(targets * np.log(predictions)) / N\n    return ce\n\npredictions = np.array([[0.25,0.25,0.25,0.25],\n                        [0.01,0.01,0.01,0.97]])\ntargets = np.array([[1,0,0,0],\n                   [0,0,0,1]])\n\ncross_entropy(predictions, targets)\n# 0.7083767843022996\n\nlog_loss(targets, predictions)\n# 0.7083767843022996\n\nlog_loss(targets, predictions) == cross_entropy(predictions, targets)\n# True\n\nres = 0\nfor act_row, pred_row in zip(targets, np.array(predictions)):\n    for class_act, class_pred in zip(act_row, pred_row):\n        res += - class_act * np.log(class_pred)\n\nres/len(targets)\n# 0.7083767843022996\n\nres/len(targets) == log_loss(targets, predictions)\n# True\n'
'    Bitmap bitmap = createScaledBitmap(faces[0], INPUT_SIZE , INPUT_SIZE , true);\n    // get pixel values\n    bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\n\n    for (int i = 0; i &lt; intValues.length; ++i) {\n\n        final int val = intValues[i];\n\n        // extract colors using bit-wise shifting.\n        floatValues[i * 3 + 0] = ((val &gt;&gt; 16) &amp; 0xFF );\n        floatValues[i * 3 + 1] = ((val &gt;&gt; 8) &amp; 0xFF );\n        floatValues[i * 3 + 2] = (val &amp; 0xFF );\n\n        // reverse the color orderings.\n        floatValues[i*3 + 2] = Color.red(val);\n        floatValues[i*3 + 1] = Color.green(val);\n        floatValues[i*3] = Color.blue(val);\n    }\n'
'from pygeocoder import Geocoder\nlocation = Geocoder.reverse_geocode(12.9716,77.5946)\nprint("City:",location.city)\nprint("Country:",location.country)\n'
"from keras import Model, Input\nfrom keras.layers import (LSTM, Dense, Activation, BatchNormalization, \n                      Dropout, Bidirectional, Add)\n\ninputs = Input(shape=(look_back, 1))\n\nbd_seq = Bidirectional(LSTM(128, return_sequences=True,\n                            kernel_regularizer='l2'), \n                       merge_mode='sum')(inputs)\nbd_sin = Bidirectional(LSTM(32, return_sequences=True, \n                            kernel_regularizer='l2'), \n                       merge_mode='sum') (bd_seq)\n\nbd_1 = Bidirectional(LSTM(1, activation='linear'), \n                     merge_mode='sum')(bd_seq)\nbd_2 = Bidirectional(LSTM(1, activation='tanh'), \n                     merge_mode='sum')(bd_sin)\noutput = Add()([bd_1, bd_2])\n\nmodel = Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n"
'import psutil;                  print( "{0:17s}{1:} CPUs PHYSICAL".format(\n      "psutil:",\n       psutil.cpu_count( logical = False ) ) )\npass;                           print( "{0:17s}{1:} CPUs LOGICAL".format(\n      "psutil:",\n       psutil.cpu_count( logical = True  ) ) )\n...\n\n\'\'\'\nsys:             linux \n                 3.6.1 (default, Jun 27 2017, 14:35:15)  .. [GCC 7.1.1 20170622 (Red Hat 7.1.1-3)]\n\nmultiprocessing: 1 CPU(s)\npsutil:          1 CPUs PHYSICAL\npsutil:          1 CPUs LOGICAL\npsutil:          psutil.cpu_freq(  per_cpu = True  ) not able to report. ?( v5.1.0+ )\npsutil:          5.0.1\npsutil:          psutil.cpu_times( per_cpu = True  ) not able to report. ?( vX.Y.Z+ )\npsutil:          5.0.1\npsutil:          svmem(total=1039192064, available=257290240, percent=75.2, used=641396736, free=190361600, active=581107712, inactive=140537856, buffers=12210176, cached=195223552, shared=32768)\nnumexpr:         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ModuleNotFoundError: No module named \'numexpr\'.\njoblib:          ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ModuleNotFoundError: No module named \'joblib\'.\nsklearn/joblib:  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ModuleNotFoundError: No module named \'sklearn.externals.joblib\' \n\'\'\'\n\n\'\'\' [i5]\n&gt;&gt;&gt; numexpr.print_versions()\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\nNumexpr version:   2.5\nNumPy version:     1.10.4\nPython version:    2.7.13 |Anaconda 4.0.0 (32-bit)| (default, May 11 2017, 14:07:41) [MSC v.1500 32 bit (Intel)]\nAMD/Intel CPU?     True\nVML available?     True\nVML/MKL version:   Intel(R) Math Kernel Library Version 11.3.1 Product Build 20151021 for 32-bit applications\nNumber of threads used by default: 4 (out of 4 detected cores)\n-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n\'\'\'\n'
"from multiprocessing import Process\n    #this is the function to be parallelised\n    def image_load_here(image_path):\n        pass \n\nif __name__ == '__main__':\n    #Start the multiprocesses and provide your dataset.\n    p = Process(target=image_load_here,['img1', 'img2', 'img3', 'img4'])\n    p.start()\n    p.join()\n"
"from sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('y label here')\nax.set_xlabel('x label here')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title(title)\nax.legend()\nplt.show()\n\nfrom sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\nnewsgroups_train = fetch_20newsgroups(subset='train', \n                                      categories=['alt.atheism', 'sci.space'])\npipeline = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer())])        \nX = pipeline.fit_transform(newsgroups_train.data).todense()\n\n# Select ONLY 2 features\nX = np.array(X)\nX = X[:, [0,1]]\ny = newsgroups_train.target\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('y label here')\nax.set_xlabel('x label here')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title(title)\nax.legend()\nplt.show()\n"
'The mask of selected features.\n\narray([ True,  True,  True,  True,  True,\n    False, False, False, False, False], dtype=bool)\n\nselector.ranking_\n#output : array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n'
"&gt;&gt;&gt; df\n   gender\n0  Female\n1    Male\n2    Male\n3    Male\n4  Female\n5  Female\n6    Male\n7  Female\n8  Female\n9  Female\n\ndf['gender_factor'] = pd.factorize(df.gender)[0]\n\n&gt;&gt;&gt; df\n   gender  gender_factor\n0  Female              0\n1    Male              1\n2    Male              1\n3    Male              1\n4  Female              0\n5  Female              0\n6    Male              1\n7  Female              0\n8  Female              0\n9  Female              0\n\ndf['gender_factor'] = df['gender'].astype('category').cat.codes\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\n\n# Transform the gender column\ndf['gender_factor'] = le.fit_transform(df.gender)\n\n&gt;&gt;&gt; df\n   gender  gender_factor\n0  Female              0\n1    Male              1\n2    Male              1\n3    Male              1\n4  Female              0\n5  Female              0\n6    Male              1\n7  Female              0\n8  Female              0\n9  Female              0\n\n# Easy to back transform:\n\ndf['gender_factor'] = le.inverse_transform(df.gender_factor)\n\n&gt;&gt;&gt; df\n   gender gender_factor\n0  Female        Female\n1    Male          Male\n2    Male          Male\n3    Male          Male\n4  Female        Female\n5  Female        Female\n6    Male          Male\n7  Female        Female\n8  Female        Female\n9  Female        Female\n\ndf.join(pd.get_dummies(df.gender))\n\n   gender  Female  Male\n0  Female       1     0\n1    Male       0     1\n2    Male       0     1\n3    Male       0     1\n4  Female       1     0\n5  Female       1     0\n6    Male       0     1\n7  Female       1     0\n8  Female       1     0\n9  Female       1     0\n\ndf.join(pd.get_dummies(df.gender, drop_first=True))\n\n   gender  Male\n0  Female     0\n1    Male     1\n2    Male     1\n3    Male     1\n4  Female     0\n5  Female     0\n6    Male     1\n7  Female     0\n8  Female     0\n9  Female     0\n"
"model.fit(X, Y)\n\nmodel.model.save('saved_model.h5')\n\nfrom keras.models import load_model\n\n# Instantiate the model as you please (we are not going to use this)\nmodel2 = KerasRegressor(build_fn=model_build_fn, epochs=10, batch_size=10, verbose=1)\n\n# This is where you load the actual saved model into new variable.\nmodel2.model = load_model('hh.h5')\n\n# Now you can use this to predict on new data (without fitting model2, because it uses the older saved model)\nmodel2.predict(X)\n"
'opt = tf.keras.optimizers.Adam(lr=0.001)\n'
'def sigmoid(x):\n    sig = 1 / (1 + np.exp(-x))     # Define sigmoid function\n    sig = np.minimum(sig, 0.9999)  # Set upper bound\n    sig = np.maximum(sig, 0.0001)  # Set lower bound\n    return sig\n'
'X_train = X_train.reshape(10000, 20, -1)\n\n# ...\nmodel.add(LSTM(...,input_shape=(20,15*4), ...)) # modify input_shape accordingly\n\nmodel.add(TimeDistributed(Flatten(input_shape=(15,4))))\n'
"&gt;&gt;&gt; df\n   a  b  c\n0  a  1  d\n1  b  2  e\n2  c  3  f\n\n&gt;&gt;&gt; pd.get_dummies(df.values)\n\n&gt;&gt;&gt; pd.get_dummies(df)\n   b  a_a  a_b  a_c  c_d  c_e  c_f\n0  1    1    0    0    1    0    0\n1  2    0    1    0    0    1    0\n2  3    0    0    1    0    0    1\n\n&gt;&gt;&gt; pd.get_dummies(df['a'].values)\n   a  b  c\n0  1  0  0\n1  0  1  0\n2  0  0  1\n"
"np.random.seed(0)\ndf1 = pd.DataFrame(np.random.choice(10, (5, 4)), columns=list('ABCD'))\ny = df1.pop('C')\nz = df1.pop('D')\nX = df1\n\nsplits = train_test_split(X, y, z, test_size=0.2)\nlen(splits)\n# 6\n\ntrain_test_split(X, y, 0.2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
"....\nweights = np.ones((3,))\n\nm = y.size\nfor boom in range(100):\n  currentCost = cost(normalizedX, weights, y)\n  if boom % 1 == 0:\n    print(boom, 'iteration', weights[0], weights[1], weights[2])\n    print('Cost', currentCost)\n\n  for i in range(47):\n    errorDiff = h(normalizedX[i], weights) - y[i]\n    weights[0] = weights[0] - alpha *(1/m)* (errorDiff) * normalizedX[i][0]\n    weights[1] = weights[1] - alpha *(1/m)*  (errorDiff) * normalizedX[i][1]\n    weights[2] = weights[2] - alpha *(1/m)* (errorDiff) * normalizedX[i][2]\n\n...\n\nimport numpy\nimport matplotlib.pyplot as plot\nimport pandas\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, SGDRegressor\n\ndataset = pandas.read_csv('Housing.csv', header=None)\n\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 2].values\n\nsgdRegressor = SGDRegressor(penalty='none', learning_rate='constant', eta0=0.1, max_iter=1000, tol = 1E-6)\n\nxnorm = sklearn.preprocessing.scale(x)\nscaleCoef = sklearn.preprocessing.StandardScaler().fit(x)\nmean = scaleCoef.mean_\nstd = numpy.sqrt(scaleCoef.var_)\nprint('stf')\nprint(std)\n\nyPrediction = []\npredictedX = [[(2100 - mean[0]) / std[0], (3 - mean[1]) / std[1]]]\nprint('predictedX', predictedX)\nfor trials in range(10):\n    stuff = sgdRegressor.fit(xnorm, y)\n    yPrediction.extend(sgdRegressor.predict(predictedX))\nprint('predict', np.mean(yPrediction))\n\npredict 355533.10119985335\n"
'pip uninstall keras-preprocessing\n\npip install git+https://github.com/keras-team/keras-preprocessing.git\n\nfrom keras_preprocessing.image import ImageDataGenerator\n'
"# Get one hot encoding of columns 'vehicleType'\none_hot = pd.get_dummies(data_df['vehicleType'])\n# Drop column as it is now encoded\ndata_df = data_df.drop('vehicleType',axis = 1)\n# Join the encoded df\ndata_df = data_df.join(one_hot)\ndata_df \n"
'import numpy as np\ndef ridgeRegression(X, y, lambdaRange):\n    wList = []\n    # Get normal form of `X`\n    A = X.T @ X \n    # Get Identity matrix\n    I = np.eye(A.shape[0])\n    # Get right hand side\n    c = X.T @ y\n    for lambVal in range(1, lambdaRange+1):\n        # Set up equations Bw = c        \n        lamb_I = lambVal * I\n        B = A + lamb_I\n        # Solve for w\n        w = np.linalg.solve(B,c)\n        wList.append(w)        \n    return wList\n'
"# Use the ColumnTransformer to apply the transformations to the correct columns in the dataframe.\ninteger_features = list(X.columns[X.dtypes == 'int64'])\ncontinuous_features = list(X.columns[X.dtypes == 'float64'])\ncategorical_features = list(X.columns[X.dtypes == 'object'])\n"
'[X_train_1,X_train_2]\n\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport keras\nfrom keras import layers\nfrom keras.layers import Conv1D, Dense\nfrom keras.utils.np_utils import to_categorical\n\n# This is just for dummy data ##################################\nX_train_1 = np.random.randint(0, 10000, (921, 10080, 1))\nX_train_2 = np.random.randint(0, 10000, (921, 85))\nY_kat = np.random.randint(0, 2, (921))\nY = to_categorical(Y_kat, num_classes=2)\n# This is just for dummy data ##################################\n\n\ndef my_model():\n\n    inputs_1 = keras.Input(shape=(10080, 1))\n    layer1 = Conv1D(64,14)(inputs_1)\n    layer2 = layers.MaxPool1D(5)(layer1)\n    layer3 = Conv1D(64, 14)(layer2)       \n    layer4 = layers.GlobalMaxPooling1D()(layer3)\n\n    inputs_2 = keras.Input(shape=(85,))\n    layer5 = layers.concatenate([layer4, inputs_2])\n    layer6 = Dense(128, activation=\'relu\')(layer5)\n    layer7 = Dense(2, activation=\'softmax\')(layer6)\n\n    model_2 = keras.models.Model(inputs = [inputs_1, inputs_2], output = [layer7])\n    # model_2.summary()    \n    adam = keras.optimizers.Adam(lr = 0.0001)\n    model_2.compile(loss=\'categorical_crossentropy\', optimizer=adam, metrics=[\'acc\'])\n    return model_2    \n\n# We need convert one_hot encoded labels to categorical labels for skf\nY_kat = np.argmax(Y, axis=1)\n\nn_folds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True)\nskf = skf.split(X_train_1, Y_kat)\n\ncv_score = []\n\nfor i, (train, test) in enumerate(skf):\n    # currently keras doesn\'t have like model.reset(), so the easiest way\n    # recompiling our model in every step of the loop see below more\n    # create model\n    model_2 = my_model()\n\n    print("Running Fold", i+1, "/", n_folds)\n    model_2.fit([X_train_1[train], X_train_2[train]], Y[train], epochs=150, batch_size=10)\n    result = model_2.evaluate([X_train_1[test], X_train_2[test]], Y[test])\n    # if we want only the accuracy metric\n    cv_score.append(result[1])\n    # we have to clear previous model to reset weights\n    # currently keras doesn\'t have like model.reset()\n    keras.backend.clear_session()\n\nprint("\\nMean accuracy of the crossvalidation: {}".format(np.mean(cv_score)))\n\nMean accuracy of the crossvalidation: 0.5049177408218384\n'
"X = tf.cast(iris[:, :3], tf.float32) \ny = tf.cast(iris[:, 3], tf.float32)\n\nX = np.array(iris[:, :3], dtype=np.float32)\ny = np.array(iris[:, 3], dtype=np.float32)\n\nimport tensorflow as tf\ntf.keras.backend.floatx()\n\nOut[3]: 'float32'\n"
'roc = {label: [] for label in multi_class_series.unique()}\nfor label in multi_class_series.unique():\n    selected_classifier.fit(train_set_dataframe, train_class == label)\n    predictions_proba = selected_classifier.predict_proba(test_set_dataframe)\n    roc[label] += roc_auc_score(test_class, predictions_proba[:,1])\n'
'Yt_train = Yt_train.type(torch.LongTensor)\n\n╔══════════════════════════╦═══════════════════════════════╦════════════════════╦═════════════════════════╗\n║        Data type         ║             dtype             ║     CPU tensor     ║       GPU tensor        ║\n╠══════════════════════════╬═══════════════════════════════╬════════════════════╬═════════════════════════╣\n║ 32-bit floating point    ║ torch.float32 or torch.float  ║ torch.FloatTensor  ║ torch.cuda.FloatTensor  ║\n║ 64-bit floating point    ║ torch.float64 or torch.double ║ torch.DoubleTensor ║ torch.cuda.DoubleTensor ║\n║ 16-bit floating point    ║ torch.float16 or torch.half   ║ torch.HalfTensor   ║ torch.cuda.HalfTensor   ║\n║ 8-bit integer (unsigned) ║ torch.uint8                   ║ torch.ByteTensor   ║ torch.cuda.ByteTensor   ║\n║ 8-bit integer (signed)   ║ torch.int8                    ║ torch.CharTensor   ║ torch.cuda.CharTensor   ║\n║ 16-bit integer (signed)  ║ torch.int16 or torch.short    ║ torch.ShortTensor  ║ torch.cuda.ShortTensor  ║\n║ 32-bit integer (signed)  ║ torch.int32 or torch.int      ║ torch.IntTensor    ║ torch.cuda.IntTensor    ║\n║ 64-bit integer (signed)  ║ torch.int64 or torch.long     ║ torch.LongTensor   ║ torch.cuda.LongTensor   ║\n║ Boolean                  ║ torch.bool                    ║ torch.BoolTensor   ║ torch.cuda.BoolTensor   ║\n╚══════════════════════════╩═══════════════════════════════╩════════════════════╩═════════════════════════╝\n'
'import torch\nfrom sklearn.datasets import make_regression\n\n\nclass RegressionDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        data = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n        self.x = torch.from_numpy(data[0]).float()\n        self.y = torch.from_numpy(data[1]).float()\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n# 2. Calculate Loss\nloss = criterion(pred.flatten(), yb)\n\nmodel = torch.nn.Linear(1, 1)\n\ndataset = RegressionDataset()\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=3e-4)\n\nfit(5000, model, criterion, optimizer, dataloader)\n\nimport torch\nfrom sklearn.datasets import make_regression\n\n\nclass RegressionDataset(torch.utils.data.Dataset):\n    def __init__(self):\n        data = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n        self.x = torch.from_numpy(data[0]).float()\n        self.y = torch.from_numpy(data[1]).float()\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n\n\n# Funcao para treinar\ndef fit(num_epochs, model, criterion, optimizer, train_dl):\n    # Repeat for given number of epochs\n    for epoch in range(num_epochs):\n\n        # Train with batches of data\n        for xb, yb in train_dl:\n\n            # 1. Generate predictions\n            pred = model(xb)\n\n            # 2. Calculate Loss\n            loss = criterion(pred.flatten(), yb)\n\n            # 3. Compute gradients\n            loss.backward()\n\n            # 4. Update parameters using gradients\n            optimizer.step()\n\n            # 5. Reset the gradients to zero\n            optimizer.zero_grad()\n\n        # Print the progress\n        if (epoch + 1) % 10 == 0:\n            print(\n                &quot;Epoch [{}/{}], Loss: {:.4f}&quot;.format(epoch + 1, num_epochs, loss.item())\n            )\n\n\ndataset = RegressionDataset()\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=32)\nmodel = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=3e-4)\n\nfit(5000, model, criterion, optimizer, dataloader)\n'
"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\n\n\nx = np.array([[1,1.1],[1,2],[2,1]])\ny = np.array([0,100,250])\nclassifier = OneVsRestClassifier(SVC(C=1e5,kernel='linear'))\nclassifier.fit(x,y)\n\nfig, ax = plt.subplots()\n# create a mesh to plot in\nx_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\ny_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\nxx2, yy2 = np.meshgrid(np.arange(x_min, x_max, .2),np.arange(y_min, y_max, .2))\nZ = classifier.predict(np.c_[xx2.ravel(), yy2.ravel()])\nZ = Z.reshape(xx2.shape)\nax.contourf(xx2, yy2, Z, cmap=plt.cm.winter, alpha=0.3)\nax.scatter(x[:, 0], x[:, 1], c=y, cmap=plt.cm.winter, s=25)\n\ndef reconstruct(w,b):\n\n    k = - w[0] / w[1]\n    b = - b[0] / w[1]\n\n    if k &gt;= 0:\n        x0 = max((y_min-b)/k,x_min)\n        x1 = min((y_max-b)/k,x_max)\n    else:\n        x0 = max((y_max-b)/k,x_min)\n        x1 = min((y_min-b)/k,x_max)\n    if np.abs(x0) == np.inf: x0 = x_min\n    if np.abs(x1) == np.inf: x1 = x_max\n    \n    xx = np.linspace(x0,x1)\n    yy = k*xx+b\n\n    return xx,yy\n\nxx,yy = reconstruct(classifier.coef_[0],classifier.intercept_[0])\nax.plot(xx,yy,'r')\nxx,yy = reconstruct(classifier.coef_[1],classifier.intercept_[1])\nax.plot(xx,yy,'g')\nxx,yy = reconstruct(classifier.coef_[2],classifier.intercept_[2])\nax.plot(xx,yy,'b')\n"
'CV_rfc.fit(x_train, y_train)\nprint("Finished feature selection and parameter tuning")\n\nprint("Optimal number of features : %d" % rfecv.n_features_)\nfeatures=list(X.columns[CV_rfc.best_estimator_.support_])\nprint(features)\n'
"steps = [\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2)),\n    ('model', Ridge())  # &lt;------ Whatever string you assign here will be used later\n]\n\n# Since you have named it as 'model', you need change it to 'model_alpha'\nparameters = [ {'model__alpha': np.arange(0, 0.2, 0.01) } ]\n\nfrom sklearn.metrics import mean_squared_error, make_scorer\nscoring_func = make_scorer(mean_squared_error)\n\ngrid_search = GridSearchCV(estimator = ridge_pipe, \n                           param_grid = parameters,\n                           scoring = scoring_func,  #&lt;--- Use the scoring func defined above\n                           cv = 10,\n                           n_jobs = -1)\n"
'docs=[[0,1],[0,0],[1,0,1]]\nD=len(docs)\nz_d_n=[[0 for _ in xrange(len(d))] for d in docs]\nl_d_n=[[0 for _ in xrange(len(d))] for d in docs]\n\nV=2\nT=2\nS=2\nn_m_j_k=numpy.zeros( (V,T,S) )\nn_j_k_d=numpy.zeros( (T,S,D) )\nn_j_k=numpy.zeros( (T,S) )\nn_k_d=numpy.zeros( (S,D) )\nn_d=numpy.zeros( (D) )\n\nbeta=.1\nalpha=.1\ngamma=.1\n\nfor d, doc in enumerate(docs): #d: doc id\n    for n, m in enumerate(doc): #i: index of the word inside document, m: id of the word in the vocabulary\n        # j is the topic\n        j = z_d_n[d][n]\n        # k is the sentiment\n        k = l_d_n[d][n]\n        n_m_j_k[m][j][k] += 1\n        n_j_k_d[j][k][d] += 1\n        n_j_k[j][k] += 1\n        n_k_d[k][d] += 1\n        n_d[d] += 1 \n\nfor d, doc in enumerate(docs): #d: doc id\n    for n, m in enumerate(doc): #i: index of the word inside document, m: id of the word in the vocabulary\n        # j is the topic\n        j = z_d_n[d][n]\n        # k is the sentiment\n        k = l_d_n[d][n]\n        n_m_j_k[m][j][k] -= 1\n        n_j_k_d[j][k][d] -= 1\n        n_j_k[j][k] -= 1\n        n_k_d[k][d] -= 1\n        n_d[d] -= 1 \n\n        # sample a new topic and sentiment label jointly\n        # T is the number of topics\n        # S is the number of sentiments\n        p_left = (n_m_j_k[m] + beta) / (n_j_k + V * beta) # T x S array\n        p_mid = (n_j_k_d[:,:,d] + alpha) / numpy.tile(n_k_d[:,d] + T * alpha, (T,1) )\n        p_right = numpy.tile(n_k_d[:,d] + gamma,(T,1)) /  numpy.tile(n_d[d] + S * gamma,(T,S))\n        p = p_left * p_mid * p_right\n        p /= numpy.sum(p)\n        new_jk = numpy.random.multinomial(1, numpy.reshape(p, (T*S) )).argmax()\n        j=new_jk/T\n        k=new_jk%T\n\n        z_d_n[d][n]=j\n        l_d_n[d][n]=k\n        n_m_j_k[m][j][k] += 1\n        n_j_k[j][k] += 1\n        n_k_d[k][d] += 1\n        n_d[d] += 1\n'
'&gt;&gt;&gt; from sklearn.naive_bayes import MultinomialNB\n&gt;&gt;&gt; from sklearn.cross_validation import train_test_split\n&gt;&gt;&gt; from sklearn.datasets import load_iris\n&gt;&gt;&gt; from copy import copy\n# prepare dataset\n&gt;&gt;&gt; iris = load_iris()\n&gt;&gt;&gt; X = iris.data[:, :2]\n&gt;&gt;&gt; y = iris.target\n&gt;&gt;&gt; X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n# model\n&gt;&gt;&gt; clf1 = MultinomialNB()\n&gt;&gt;&gt; clf2 = MultinomialNB()\n&gt;&gt;&gt; print id(clf1), id(clf2) # two different instances\n 4337289232 4337289296\n&gt;&gt;&gt; clf1.fit(X_train, y_train)\n&gt;&gt;&gt; print clf1.score(X_test, y_test)\n 0.633333333333\n&gt;&gt;&gt; print clf2.fit(X_train, y_train).score(X_test, y_test)\n 0.633333333333\n'
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; k = 5\n\n&gt;&gt;&gt; a = np.arange(1.*2*2*k*k).reshape(2,2,k,k)\n&gt;&gt;&gt; x = np.arange(1.*2*k).reshape(2,k)\n&gt;&gt;&gt; x\narray([[ 0.,  1.,  2.,  3.,  4.],\n       [ 5.,  6.,  7.,  8.,  9.]])\n\n&gt;&gt;&gt; result = np.tensordot(a,x,([1,3],[0,1]))\n&gt;&gt;&gt; result\narray([[  985.,  1210.,  1435.,  1660.,  1885.],\n       [ 3235.,  3460.,  3685.,  3910.,  4135.]])\n&gt;&gt;&gt; np.shape(result)\n(2, 5)\n'
'from __future__ import division\nfrom collections import Counter\nfrom itertools import product\nfrom toolz.curried import sliding_window, map, pipe, concat\nfrom toolz.dicttoolz import merge\n\n# Generate all possible transitions \ndefaults = sc.broadcast(dict(map(\n    lambda x: ("".join(concat(x)), 0.0), \n    product(product("HNL", "NL", "HNL"), repeat=2))))\n\nrdd = sc.parallelize(["500, HNL, LNH, NLH, HNL", "600, HNN, NNN, NNN, HNN, LNH"])\n\ndef process(line):\n    """\n    &gt;&gt;&gt; process("000, HHH, LLL, NNN")\n    (\'000\', {\'LLLNNN\': 0.5, \'HHHLLL\': 0.5})\n    """\n    bits = line.split(", ")\n    transactions = bits[1:]\n    n = len(transactions) - 1\n    frequencies = pipe(\n        sliding_window(2, transactions), # Get all transitions\n        map(lambda p: "".join(p)), # Joins strings\n        Counter, # Count \n        lambda cnt: {k: v / n for (k, v) in cnt.items()} # Get frequencies\n    )\n    return bits[0], frequencies\n\ndef store_partition(iter):\n    for (k, v) in iter:\n        db_insert(k, merge([defaults.value, v]))\n\nrdd.map(process).foreachPartition(store_partition)\n'
'import matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom itertools import cycle\nplt.style.use(\'ggplot\')\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=0)\n\nclassifier = OneVsRestClassifier(svm.SVC(kernel=\'linear\', probability=True,\n                                 random_state=0))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\ncolors = cycle([\'blue\', \'red\', \'green\'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=1.5,\n             label=\'ROC curve of class {0} (area = {1:0.2f})\'\n             \'\'.format(i, roc_auc[i]))\nplt.plot([0, 1], [0, 1], \'k--\', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic for multi-class data\')\nplt.legend(loc="lower right")\nplt.show()\n'
"class MyTokenizer(object):\n    def __call__(self,s):\n        if(s.find('#')==-1):\n            return s.split(' ')\n        else:\n            return s.split('#')[0].split(' ')\n"
'from sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.grid_search import RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import randint as sp_randint\n\nseed = np.random.seed(22)\n\nX_train, X_test, y_train, y_test = \n   train_test_split(data[features], data[\'target\'])\n\n\nclf = RandomForestClassifier()\nkbest = SelectKBest()\npipe = make_pipeline(kbest,clf)\n\nupLim = X_train.shape[1]\nparam_dist = {\'selectkbest__k\':sp_randint(upLim/2,upLim+1),\n  \'randomforestclassifier__n_estimators\': sp_randint(5,150),\n  \'randomforestclassifier__max_depth\': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None],\n  \'randomforestclassifier__criterion\': ["gini", "entropy"],\n  \'randomforestclassifier__max_features\': [\'auto\', \'sqrt\', \'log2\']}\nclf_opt = RandomizedSearchCV(pipe, param_distributions= param_dist, \n                             scoring=\'roc_auc\', n_jobs=1, cv=3)\nclf_opt.fit(X_train,y_train)\ny_pred = clf_opt.predict(X_test)\n'
'if np.setdiff1d(y, self.classes_):\nraise ValueError(("Mini-batch contains {0} while classes " +\n                 "must be subset of {1}").format(np.unique(y),\n                                              self.classes_))\n'
'def debinarize(x):\n    return tf.to_float(tf.argmax(x,axis=2))  # get the character with most probability\n'
"In [2]: from sklearn.preprocessing import LabelEncoder\n\nIn [3]: label_encoder = LabelEncoder()\n\nIn [4]: label_encoder.fit_transform(['a', 'b', 'c'])\nOut[4]: array([0, 1, 2])\n\nIn [5]: label_encoder.transform(['a'])\nOut[5]: array([0])\n\nIn [59]: from sklearn.ensemble import RandomForestClassifier\n\nIn [60]: X = ['a', 'b', 'c']\n\nIn [61]: y = ['male', 'female', 'female']\n\nIn [62]: X_encoded = label_encoder.fit_transform(X)\n\nIn [63]: rf_model = RandomForestClassifier()\n\nIn [64]: rf_model.fit(X_encoded[:, None], y)\nOut[64]: \nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)\n\nIn [65]: x = ['a']\n\nIn [66]: x_encoded = label_encoder.transform(x)\n\nIn [67]: rf_model.predict(x_encoded[:, None])\nOut[67]: \narray(['male'], \n      dtype='&lt;U6')\n"
'X = tf.placeholder(tf.float32, [None, vlimit ], name=\'X_placeholder\')\nY = tf.placeholder(tf.int32, [None, classes], name=\'Y_placeholder\')\n...\n\nb = tf.Variable(tf.ones([classes]), name="bias")\n'
'def absolute_error(z, l):\n  return cntk.reduce_mean(cntk.abs(z - l))\n'
'import tensorflow as tf;\nimport tensorflow.contrib.eager as tfe;\n\ntf.enable_eager_execution();\n\niris_dataset_url = \'http://download.tensorflow.org/data/iris_training.csv\';\niris_csv_file = tf.keras.utils.get_file(\'iris_dataset.csv\', iris_dataset_url);\n\niris_dataset_tests_url = \'http://download.tensorflow.org/data/iris_test.csv\';\niris_tests_csv_file = tf.keras.utils.get_file(\'iris_tests_dataset.csv\', iris_dataset_tests_url);\n\ndef iris_data_parse_line(line):\n    default_feature = [[0.0], [0.0], [0.0], [0.0], [0]]; #UPDATED SPOT!!!\n    parsed_line = tf.decode_csv(line, default_feature);\n\n    features = tf.reshape(parsed_line[:-1], shape=(4,), name="features");\n    label = tf.reshape(parsed_line[-1], shape=(), name="label");\n\n    return features, label;\n\ndef prediction_loss_diff(features, label, model):\n    predicted_label = model(features);\n    return tf.losses.sparse_softmax_cross_entropy(label, predicted_label);\n\ndef gradient_tune(features, targets, model):\n    with tf.GradientTape() as tape:\n        prediction_loss = prediction_loss_diff(features, targets, model);\n    return tape.gradient(prediction_loss, model.variables);\n\ndef train_model(training_dataset, model, optimizer):\n    train_loss_results = []\n    train_accuracy_results = []\n    rounds = 201;\n\n\n    for round_num in range(rounds):\n        epoch_loss_avg = tfe.metrics.Mean();\n        epoch_accuracy = tfe.metrics.Accuracy();\n\n        for features, label in training_dataset:\n            gradients = gradient_tune(features, label, model);\n            optimizer.apply_gradients(\n                    zip(gradients, model.variables),\n                    global_step=tf.train.get_or_create_global_step());\n\n\n\ndef main():\n    print("TensorFlow version: {}".format(tf.VERSION));\n    print("Eager execution: {}".format(tf.executing_eagerly()));\n\n    iris_dataset = (tf.data.TextLineDataset(iris_csv_file)\n                           .skip(1)\n                           .map(iris_data_parse_line)\n                           .shuffle(1000)\n                           .batch(32));\n\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(10, activation="relu", input_shape=(4,)),\n        tf.keras.layers.Dense(10, activation="relu"),\n        tf.keras.layers.Dense(3)\n    ]);\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01);\n\n    train_model(iris_dataset, model, optimizer);\n\nif __name__ == "__main__":\n    main();\n'
'# Combining Wide and Deep Models into One\nmodel_dir = tempfile.mkdtemp()\nm = tf.contrib.learn.DNNLinearCombinedClassifier(model_dir=model_dir,\n                                                 linear_feature_columns=wide_columns,\n                                                 dnn_feature_columns=deep_columns,\n                                                 dnn_hidden_units=[100, 50],\n                                                 config=tf.contrib.learn.RunConfig(tf_random_seed=123))\n'
'print("\\n====== classifier model_dir, latest_checkpoint ===========")\nprint(classifier.model_dir)\nprint(classifier.latest_checkpoint())\ndebug = False\n\nwith tf.Session() as sess:\n    # First let\'s load meta graph and restore weights\n    latest_checkpoint_path = classifier.latest_checkpoint()\n    saver = tf.train.import_meta_graph(latest_checkpoint_path + \'.meta\')\n    saver.restore(sess, latest_checkpoint_path)\n\n    # Get the input and output tensors needed for toco.\n    # These were determined based on the debugging info printed / saved below.\n    input_tensor = sess.graph.get_tensor_by_name("dnn/input_from_feature_columns/input_layer/concat:0")\n    input_tensor.set_shape([1, 4])\n    out_tensor = sess.graph.get_tensor_by_name("dnn/logits/BiasAdd:0")\n    out_tensor.set_shape([1, 3])\n\n    # Pass the output node name we are interested in.\n    # Based on the debugging info printed / saved below, pulled out the\n    # name of the node for the logits (before the softmax is applied).\n    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess, sess.graph_def, output_node_names=["dnn/logits/BiasAdd"])\n\n    if debug is True:\n        print("\\nORIGINAL GRAPH DEF Ops ===========================================")\n        ops = sess.graph.get_operations()\n        for op in ops:\n            if "BiasAdd" in op.name or "input_layer" in op.name:\n                print([op.name, op.values()])\n        # save original graphdef to text file\n        with open("estimator_graph.pbtxt", "w") as fp:\n            fp.write(str(sess.graph_def))\n\n        print("\\nFROZEN GRAPH DEF Nodes ===========================================")\n        for node in frozen_graph_def.node:\n            print(node.name)\n        # save frozen graph def to text file\n        with open("estimator_frozen_graph.pbtxt", "w") as fp:\n            fp.write(str(frozen_graph_def))\n\ntflite_model = tf.contrib.lite.toco_convert(frozen_graph_def, [input_tensor], [out_tensor])\nopen("estimator_model.tflite", "wb").write(tflite_model)\n'
"numeric_input = Input(shape=(x_numeric_train.values.shape[1],), name='numeric_input')\nnlp_seq = Input(shape=(number_of_messages ,seq_length,), name='nlp_input'+str(i))\n\n# shared layers\nemb = TimeDistributed(Embedding(input_dim=num_features, output_dim=embedding_size,\n                input_length=seq_length, mask_zero=True,\n                input_shape=(seq_length, )))(nlp_seq)    \nx = TimeDistributed(Bidirectional(LSTM(32, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01))))(emb)      \n\nc1 = Conv1D(filter_size, kernel1, padding='valid', activation='relu', strides=1, kernel_regularizer=regularizers.l2(kernel_reg))(x)\np1 = GlobalMaxPooling1D()(c1)\nc2 = Conv1D(filter_size, kernel2, padding='valid', activation='relu', strides=1, kernel_regularizer=regularizers.l2(kernel_reg))(x)\np2 = GlobalMaxPooling1D()(c2)\nc3 = Conv1D(filter_size, kernel3, padding='valid', activation='relu', strides=1, kernel_regularizer=regularizers.l2(kernel_reg))(x)\np3 = GlobalMaxPooling1D()(c3)\n\nx = concatenate([p1, p2, p3, numeric_input])    \nx = Dense(1, activation='sigmoid')(x)        \nmodel = Model(inputs=[nlp_seq, meta_input] , outputs=[x])\nmodel.compile('adam', 'binary_crossentropy', metrics=['accuracy'])    \n\nmodel.fit([x_train, x_numeric_train], y_train)\n# where x_train is a a array of num_samples * num_messages * seq_length\n"
'inputs = inputs.to(device)  \n\nnet.to(device)\n\ninputs.to(device)\n\ninputs = inputs.to(device)\n'
'data_gen = ImageDataGenerator(...)\n\ntrain_gen = data_gen.flow_from_directory(...)\n\ndef my_gen(gen):\n    while True:\n        try:\n            data, labels = next(gen)\n            yield data, labels\n        except:\n            pass\n\n# ... define your model and compile it\n\n# fit the model\nmodel.fit_generator(my_gen(train_gen), ...)\n'
'def normalize(x, axis=-1, order=2):\n    """Normalizes a Numpy array.\n    # Arguments\n        x: Numpy array to normalize.\n        axis: axis along which to normalize.\n        order: Normalization order (e.g. 2 for L2 norm).\n    # Returns\n        A normalized copy of the array.\n    """\n    l2 = np.atleast_1d(np.linalg.norm(x, order, axis))\n    l2[l2 == 0] = 1\n    return x / np.expand_dims(l2, axis)\n'
"rep_y_train = np.repeat(y_train, num_reps).reshape(-1, num_reps, 1)\n\nfrom keras.layers import *\nfrom keras.models import Sequential, Model\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\nvocab_size = 10000\nmax_len = 200\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\nX_train = pad_sequences(x_train, maxlen=max_len)\n\ndef create_model(return_seq=False, stateful=False):\n    batch_size = 1 if stateful else None\n    model = Sequential()\n    model.add(Embedding(vocab_size, 128, batch_input_shape=(batch_size, None)))\n    model.add(CuDNNLSTM(64, return_sequences=return_seq, stateful=stateful))\n    model.add(Dense(1, activation='sigmoid'))\n\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n    return model\n\n# train model with one label per sample\ntrain_model = create_model()\ntrain_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.3)\n\n# replicate the labels\ny_train_rep = np.repeat(y_train, max_len).reshape(-1, max_len, 1)\n\n# train model with one label per timestep\nrep_train_model = create_model(True)\nrep_train_model.fit(X_train, y_train_rep, epochs=10, batch_size=128, validation_split=0.3)\n\n# replica of `train_model` with the same weights\ntest_model = create_model(False, True)\ntest_model.set_weights(train_model.get_weights())\ntest_model.reset_states()\n\n# replica of `rep_train_model` with the same weights\nrep_test_model = create_model(True, True)\nrep_test_model.set_weights(rep_train_model.get_weights())\nrep_test_model.reset_states()\n\ndef stateful_predict(model, samples):\n    preds = []\n    for s in samples:\n        model.reset_states()\n        ps = []\n        for ts in s:\n            p = model.predict(np.array([[ts]]))\n            ps.append(p[0,0])\n        preds.append(list(ps))\n    return preds\n\nX_test = pad_sequences(x_test, maxlen=max_len)\n\nimport matplotlib.pyplot as plt\n\npreds = stateful_predict(test_model, X_test[0:2])\n\nplt.plot(preds[0])\nplt.plot(preds[1])\nplt.legend(['Class 0', 'Class 1'])\n\npreds = stateful_predict(rep_test_model, X_test[0:2])\n\nplt.plot(preds[0])\nplt.plot(preds[1])\nplt.legend(['Class 0', 'Class 1'])\n"
'import numpy as np\nimport pandas as pd\nfrom multiprocessing import Pool\n\nno_cores = 4\n\nlarge_df = pd.concat([pd.Series(np.random.rand(1111)), pd.Series(np.random.rand(1111))], axis = 1)\nchunk_size = len(large_df) // no_cores + no_cores\nchunks = [df_chunk for g, df_chunk in large_df.groupby(np.arange(len(large_df)) // chunk_size)]\n\nclass model(object):\n    @staticmethod\n    def predict(df):\n        return np.random.randint(0,2)\n\ndef perform_model_predictions(model, dataFrame, cores): \n    try:\n        with Pool(processes=cores) as pool:\n            result = pool.map(model.predict, dataFrame)\n            return result\n        # return model.predict(dataFrame)\n    except AttributeError:\n        logging.error("AttributeError occurred", exc_info=True)\n\nperform_model_predictions(model, chunks, no_cores)\n'
'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# Load Data\niris = datasets.load_iris()\nX = iris.data[:, :2][iris.target != 2]\ny = iris.target[iris.target != 2]\n\n# Change labels to +1 and -1 \ny = np.where(y==1, y, -1)\n\n# Linear Model with L2 regularization\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(1, activation=\'linear\', kernel_regularizer=tf.keras.regularizers.l2()))\n\n# Hinge loss\ndef hinge_loss(y_true, y_pred):    \n    return tf.maximum(0., 1- y_true*y_pred)\n\n# Train the model\nmodel.compile(optimizer=\'adam\', loss=hinge_loss)\nmodel.fit(X, y,  epochs=50000, verbose=False)\n\n# Plot the learned decision boundary \nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)\nplt.show()\n\nfrom sklearn.svm import SVC\n# SVM with linear kernel\nclf = SVC(kernel=\'linear\')\nclf.fit(X, y) \n\n# Plot the learned decision boundary \nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)\nplt.show() \n\n# Prepare Data \n# 10 Binary features\ndf = pd.DataFrame(np.random.randint(0,2,size=(1000, 10)))\n# 1 floating value feature \ndf[11] = np.random.uniform(0,100000, size=(1000))\n# True Label \ndf[12] = pd.DataFrame(np.random.randint(0, 2, size=(1000)))\n\n# Convert data to zero mean unit variance \nscalar = StandardScaler().fit(df[df.columns.drop(12)])\nX = scalar.transform(df[df.columns.drop(12)])\ny = np.array(df[12])\n\n# convert label to +1 and -1. Needed for hinge loss\ny = np.where(y==1, +1, -1)\n\n# Model \nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(1, activation=\'linear\', \n                                kernel_regularizer=tf.keras.regularizers.l2()))\n# Hinge Loss\ndef my_loss(y_true, y_pred):    \n    return tf.maximum(0., 1- y_true*y_pred)\n\n# Train model \nmodel.compile(optimizer=\'adam\', loss=my_loss)\nmodel.fit(X, y,  epochs=100, verbose=True)\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_curve, auc\n\n# Load Data\niris = datasets.load_iris()\nX = iris.data[:, :2][iris.target != 2]\ny_ = iris.target[iris.target != 2]\n\n# Change labels to +1 and -1 \ny = np.where(y_==1, +1, -1)\n\n\n# Hinge loss\ndef hinge_loss(y_true, y_pred):    \n    return tf.maximum(0., 1- y_true*y_pred)\n\ndef get_model():\n    # Linear Model with L2 regularization\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(1, activation=\'linear\', kernel_regularizer=tf.keras.regularizers.l2()))\n    model.compile(optimizer=\'adam\', loss=hinge_loss)\n    return model\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\npredict = lambda model, x : sigmoid(model.predict(x).reshape(-1))\npredict_class = lambda model, x : np.where(predict(model, x)&gt;0.5, 1, 0)\n\n\nkf = KFold(n_splits=2, shuffle=True)\n\n# K Fold cross validation\nbest = (None, -1)\n\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model = get_model()\n    model.fit(X_train, y_train, epochs=5000, verbose=False, batch_size=128)\n    y_pred = model.predict_classes(X_test)\n    val = roc_auc_score(y_test, y_pred)    \n    print ("CV Fold {0}: AUC: {1}".format(i+1, auc))\n    if best[1] &lt; val:\n        best = (model, val)\n\n# ROC Curve using the best model\ny_score = predict(best[0], X)\nfpr, tpr, _ = roc_curve(y_, y_score)\nroc_auc = auc(fpr, tpr)\nprint (roc_auc)\n\n# Plot ROC\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color=\'darkorange\',\n         lw=lw, label=\'ROC curve (area = %0.2f)\' % roc_auc)\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.legend(loc="lower right")\nplt.show()\n\n# Make predictions\ny_score = predict_class(best[0], X)\n\npredict = lambda model, x : sigmoid(model.predict(x).reshape(-1))\npredict_class = lambda model, x : np.where(predict(model, x)&gt;0.5, 1, 0)\n'
'X = np.array([3.4, 3.4, 3. , 2.8, 2.7, 2.9, 3.3, 3. , 3.8, 2.5])\ny = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2, 2])\n\n     X  y\n0  3.4  0\n1  3.4  0\n2  3.0  0\n3  2.8  1\n4  2.7  1\n5  2.9  1\n6  3.3  2\n7  3.0  2\n8  3.8  2\n9  2.5  2\n\ny = LabelBinarizer().fit_transform(y)\n\n     X  y1  y2  y3\n0  3.4   1   0   0\n1  3.4   1   0   0\n2  3.0   1   0   0\n3  2.8   0   1   0\n4  2.7   0   1   0\n5  2.9   0   1   0\n6  3.3   0   0   1\n7  3.0   0   0   1\n8  3.8   0   0   1\n9  2.5   0   0   1\n\nobserved = y.T.dot(X) \n&gt;&gt;&gt; observed \narray([ 9.8,  8.4, 12.6])\n\nfeature_count = X.sum(axis=0).reshape(1, -1)\nclass_prob = y.mean(axis=0).reshape(1, -1)\n\n&gt;&gt;&gt; class_prob, feature_count\n(array([[0.3, 0.3, 0.4]]), array([[30.8]]))\n\nexpected = np.dot(class_prob.T, feature_count)\n&gt;&gt;&gt; expected \narray([[ 9.24],[ 9.24],[12.32]])\n\nchi2 = ((observed.reshape(-1,1) - expected) ** 2 / expected).sum(axis=0)\n&gt;&gt;&gt; chi2 \narray([0.11666667])\n\np = scipy.special.chdtrc(3 - 1, chi2)\n&gt;&gt;&gt; p\narray([0.94333545])\n\ns = SelectKBest(chi2, k=1)\ns.fit(X.reshape(-1,1),y)\n&gt;&gt;&gt; s.scores_, s.pvalues_\n(array([0.11666667]), [0.943335449873492])\n'
'0123456789 #input\nKKKK456789 \n0KKKK56789 \n12KKKK6789 \n123KKKK789 \n1234KKKK89 \n12345KKKK9\n123456KKKK\n\n1234\n5678   -&gt; 123456789012\n9012\n'
"df = df.apply(pd.to_numeric, errors='coerce', downcast='float')\n\ndf.dropna(how='any', axis=0, inplace=True)\n"
"+----+-------------+--------------+\n| id | engine_type | engine_value |\n+----+-------------+--------------+\n|  1 |           0 | 0.25         |\n|  2 |           0 | 0.40         |\n|  3 |           1 | 0.16         |\n|  4 |           1 | 0.30         |\n|  5 |           0 | 5.3          | &lt;- anomaly\n|  6 |           1 | 14.4         | &lt;- anomaly\n|  7 |           0 | 16.30        | &lt;- anomaly\n+----+-------------+--------------+\n\n+----+-------------+--------------+\n| id | engine_type | engine_value |\n+----+-------------+--------------+\n|  8 |           1 | 3.25         | &lt;- anomaly\n|  9 |           1 | 4.40         | &lt;- anomaly\n| 10 |           0 | 2.16         |\n+----+-------------+--------------+\n\n+----+-------------+--------------+\n| id | engine_type | engine_value |\n+----+-------------+--------------+\n|  1 |           0 | 0.25         |\n|  2 |           0 | 0.40         |\n|  3 |           1 | 0.16         |\n|  4 |           1 | 0.30         |\n|  7 |           0 | 16.30        | &lt;- anomaly\n+----+-------------+--------------+\n\n+----+-------------+--------------+\n| id | engine_type | engine_value |\n+----+-------------+--------------+\n|  5 |           0 | 5.3          | &lt;- anomaly\n|  6 |           1 | 14.4         | &lt;- anomaly\n+----+-------------+--------------+\n\niForest.fit(df.values.reshape(-1,1))\n\npred = iForest.predict(df.values.reshape(-1,1))\n\npred=df['anomaly']\n\ndf['anomaly'] = iForest.fit_predict(df.values.reshape(-1,1))\n\ndf['anomaly'] = iForest.fit_predict(df.to_numpy().reshape(-1,1))\n"
"def score(self, *x):\n    print(x[0].mean_)\n    if len(x[1]) &gt; 50:\n        return 1.0\n    else:\n        return 0.5\n\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\n\nclass FilterElems:\n    def __init__(self, thres):\n        self.thres = thres\n\n    def fit(self, X, y=None, **kwargs):\n        self.mean_ = np.mean(X)\n        self.std_ = np.std(X)\n        return self\n\n    def predict(self, X):\n        X = (X - self.mean_) / self.std_\n        return X[X &gt; self.thres]\n\n    def get_params(self, deep=False):\n        return {'thres': self.thres}\n\n    def score(self, estimator, *x):\n        print(estimator.mean_, estimator.std_) \n        if len(x[0]) &gt; 50:\n            return 1.0\n        else:\n            return 0.5\n\nmodel = FilterElems(thres=0.5)\nprint(cross_val_score(model,\n                      np.random.randint(1, 1000, (100, 100)),\n                      None,\n                      scoring=model.score,\n                      cv=5))\n\n504.750125 288.84916035447355\n501.7295 289.47825925231416\n503.743375 288.8964170227962\n503.0325 287.8292687406025\n500.041 289.3488678377712\n[0.5 0.5 0.5 0.5 0.5]\n"
"import svmutil\nparam = svmutil.svm_parameter('-q')\n...\n\nimport svmutil\nx = [[0.2, 0.1], [0.7, 0.6]]\ny = [0, 1]\nsvmutil.svm_train(y, x, '-q')\n"
'H = H * H_coeff\n\nWH = W.dot(H)\nW = W * W_coeff\n\navg_V = sum(sum(V))/n/m\n\navg_V = V.mean()\n\ndivergence = sum(sum(V * np.log(V/WH) - V + WH)) # equation (3)\n\ndivergence = ((V * np.log(V_over_WH)) - V + WH).sum() \n\nH_coeff = np.zeros(H.shape)\nfor a in range(r):\n    for mu in range(m):\n        for i in range(n):\n            H_coeff[a, mu] += W[i, a] * V[i, mu] / WH[i, mu]\n        H_coeff[a, mu] /= sum(W)[a]\nH = H * H_coeff\n\nV_over_WH = V/WH\nH *= (np.dot(V_over_WH.T, W) / W.sum(axis=0)).T\n\nV_over_WH = V/WH\n\nnp.dot(V_over_WH.T, W).T\n\nW.sum(axis=0).T\n\n(np.dot(V_over_WH.T, W) / W.sum(axis=0)).T\n\nH *= (np.dot(V_over_WH.T, W) / W.sum(axis=0)).T\n\nimport numpy as np\nnp.random.seed(1)\n\n\ndef update(V, W, H, WH, V_over_WH):\n    # equation (5)\n    H *= (np.dot(V_over_WH.T, W) / W.sum(axis=0)).T\n\n    WH = W.dot(H)\n    V_over_WH = V / WH\n    W *= np.dot(V_over_WH, H.T) / H.sum(axis=1)\n\n    WH = W.dot(H)\n    V_over_WH = V / WH\n    return W, H, WH, V_over_WH\n\n\ndef factor(V, r, iterations=100):\n    n, m = V.shape\n    avg_V = V.mean()\n    W = np.random.random(n * r).reshape(n, r) * avg_V\n    H = np.random.random(r * m).reshape(r, m) * avg_V\n    WH = W.dot(H)\n    V_over_WH = V / WH\n\n    for i in range(iterations):\n        W, H, WH, V_over_WH = update(V, W, H, WH, V_over_WH)\n        # equation (3)\n        divergence = ((V * np.log(V_over_WH)) - V + WH).sum()\n        print("At iteration {i}, the Kullback-Liebler divergence is {d}".format(\n            i=i, d=divergence))\n    return W, H\n\nV = np.arange(0.01, 1.01, 0.01).reshape(10, 10)\n# V = np.arange(1,101).reshape(10,10).astype(\'float\')\nW, H = factor(V, 6)\n'
"raw_data = [\n['975676924', '1345207523', '-1953633084', '-2041119774', '587903155'],\n['1619201613', '-1384105381', '1433106581', '1445361759', '587903155'],\n['-1470352544', '-1068707556', '-1002282042', '-563691616', '587903155'],\n['-1958275692', '-739953679', '69580355', '-481818422', '587903155'],\n['1619201613', '-739953679', '-1002282042', '-481818422', '587903155']\n]\n\nimport collections\ndata = collections.defaultdict(list)\n\nfor line in raw_data:\n    data[line[0]].extend(line[1:])\n\ndefaultdict(&lt;type 'list'&gt;, {\n'1619201613': \n         ['-1384105381', '1433106581', '1445361759', '587903155',\n          '-739953679', '-1002282042', '-481818422', '587903155'],  \n'-1470352544': \n         ['-1068707556', '-1002282042', '-563691616', '587903155'], \n '975676924': \n        ['1345207523', '-1953633084', '-2041119774', '587903155'],\n '-1958275692':\n         ['-739953679', '69580355', '-481818422', '587903155']})  \n\ndata_list = [[key] + value for key, value in data.items()]\n"
'from sklearn.datasets import fetch_rcv1\nrcv1 = fetch_rcv1()\n\nlyrl2004_tokens_train.dat, \nlyrl2004_tokens_test_pt0.dat, \nlyrl2004_tokens_test_pt1.dat, \nlyrl2004_tokens_test_pt2.dat, \nlyrl2004_tokens_test_pt3.dat, \n\nrcv1-v2.topics.qrels\n\ndef categorize(sText):\n    import numpy as np\n    aTokens = np.array([d[\'lsTokens\'] for d in dTrainingData.values()], str)\n    lCats = [d[\'lsCats\'] for d in dTrainingData.values()]\n\n    print("creating binary cats")\n\n    from sklearn import preprocessing\n    oBinarizer = preprocessing.MultiLabelBinarizer()\n    aBinaryCats = oBinarizer.fit_transform(lCats)\n\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.feature_extraction.text import TfidfTransformer\n    from sklearn.svm import LinearSVC\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.pipeline import Pipeline\n\n    oClassifier = Pipeline([\n        (\'vectorizer\', CountVectorizer()),\n        (\'tfidf\', TfidfTransformer()),\n        (\'clf\', OneVsRestClassifier(LinearSVC()))])\n\n    print("fitting data to classifier...")\n    oClassifier.fit(aTokens, aBinaryCats)\n\n    aText = np.array([sText])\n\n    aPredicted = oClassifier.predict(aText)\n    lAllCats = oBinarizer.inverse_transform(aPredicted)\n'
'Z_square = linkage((DF_dissm.values),method="average")\nZ_triu = linkage(np.triu(DF_dissm.values),method="average")\nZ_tril = linkage(np.tril(DF_dissm.values),method="average")\n\nh, w = arr.shape\nZ = linkage(arr[np.triu_indices(h, 1)], method="average")\n\nfrom scipy.spatial import distance as ssd\nZ = linkage(ssd.squareform(arr), method="average")\n\nZ = hierarchy.linkage(ssd.pdist(points), method="average")\n\nZ = hierarchy.linkage(points, method="average")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.cluster import hierarchy as hier\nfrom scipy.spatial import distance as ssd\nnp.random.seed(2016)\n\npoints = np.random.random((10, 2))\narr = ssd.cdist(points, points)\n\nfig, ax = plt.subplots(nrows=4)\n\nax[0].set_title("condensed upper triangular")\nZ = hier.linkage(arr[np.triu_indices(arr.shape[0], 1)], method="average")\nhier.dendrogram(Z, ax=ax[0])\n\nax[1].set_title("squareform")\nZ = hier.linkage(ssd.squareform(arr), method="average")\nhier.dendrogram(Z, ax=ax[1])\n\nax[2].set_title("pdist")\nZ = hier.linkage(ssd.pdist(points), method="average")\nhier.dendrogram(Z, ax=ax[2])\n\nax[3].set_title("sequence of observations")\nZ = hier.linkage(points, method="average")\nhier.dendrogram(Z, ax=ax[3])\n\nplt.show()\n'
"rfc_model_3 = RandomForestClassifier(n_estimators=200)\nrfc_model_3.predict(X_test)\n\nX_test['survived'] = rfc_model_3.predict(X_test)\n"
"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nplt.style.use('ggplot')\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\n\nn = 100\nidx = pd.date_range(start=datetime(2016, 1, 1, 10), freq='10Min', periods=n)\ndata = pd.DataFrame(data={'price': np.cumsum([0.0001] * n + np.random.random(n)),\n                          'volume': np.random.randint(low=100, high=10000, size=n)}, index=idx)\n\nfig, ax = plt.subplots(nrows=2, sharex=True, figsize=(15,8))\n\nax[0].plot(data.index, data.price)\nax[1].bar(data.index, data.volume, width=1/(5*len(data.index)))\n\nxfmt = mpl.dates.DateFormatter('%H:%M')\nax[1].xaxis.set_major_locator(mpl.dates.HourLocator(interval=3))\nax[1].xaxis.set_major_formatter(xfmt)\n\nax[1].xaxis.set_minor_locator(mpl.dates.HourLocator(interval=1))\nax[1].xaxis.set_minor_formatter(xfmt)\n\nax[1].get_xaxis().set_tick_params(which='major', pad=25)\n\nfig.autofmt_xdate()\nplt.show()\n"
'library(logistf)\n\niris = read.table("path_to _iris.txt", sep="\\t", header=TRUE)\niris$Species &lt;- as.factor(iris$Species)\nsapply(iris, class)\n\nmodel1 &lt;- glm(Species ~ ., data = irisdf, family = binomial)\n# Does not converge, throws warnings.\n\nmodel2 &lt;- logistf(Species ~ ., data = irisdf, family = binomial)\n# Does converge.\n'
'from __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nK = 4\nmnist = input_data.read_data_sets("/tmp/data/", one_hot=True)\n\n# In this example, we limit mnist data\nXtr, Ytr = mnist.train.next_batch(55000)  # whole training set\nXte, Yte = mnist.test.next_batch(10000)  # whole test set\n\n# tf Graph Input\nxtr = tf.placeholder("float", [None, 784])\nytr = tf.placeholder("float", [None, 10])\nxte = tf.placeholder("float", [784])\n\n# Euclidean Distance\ndistance = tf.negative(tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(xtr, xte)), reduction_indices=1)))\n# Prediction: Get min distance neighbors\nvalues, indices = tf.nn.top_k(distance, k=K, sorted=False)\n\nnearest_neighbors = []\nfor i in range(K):\n    nearest_neighbors.append(tf.argmax(ytr[indices[i]], 0))\n\nneighbors_tensor = tf.stack(nearest_neighbors)\ny, idx, count = tf.unique_with_counts(neighbors_tensor)\npred = tf.slice(y, begin=[tf.argmax(count, 0)], size=tf.constant([1], dtype=tf.int64))[0]\n\naccuracy = 0.\n\n# Initializing the variables\ninit = tf.initialize_all_variables()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # loop over test data\n    for i in range(len(Xte)):\n        # Get nearest neighbor\n        nn_index = sess.run(pred, feed_dict={xtr: Xtr, ytr: Ytr, xte: Xte[i, :]})\n        # Get nearest neighbor class label and compare it to its true label\n        print("Test", i, "Prediction:", nn_index,\n             "True Class:", np.argmax(Yte[i]))\n        #Calculate accuracy\n        if nn_index == np.argmax(Yte[i]):\n            accuracy += 1. / len(Xte)\n    print("Done!")\n    print("Accuracy:", accuracy)\n'
'x = tf.placeholder("float", shape=[None, 784])\ny_ = tf.placeholder("float", shape=[None, 10])\n\nbatch = mnist.train.next_batch(50)\n\nbatch = mnist.train.next_batch(1)\n'
'state = cell.zero_state(batchsize, tf.float32).eval()\n\nstate = tf.get_default_session().run(cell.zero_state(batchsize, tf.float32))\n'
"python C:\\Users\\script.py \n\nProcess.Start('C:\\Users.script_runner.bat');\n"
'data = np.random.rand(1000,3)\n\ntrue_theta = np.array([1,2,3])\ntrue_measurements = np.dot(data, true_theta)\n\nnoise = np.random.rand(1000) * 1\n\nnoisy_measurements = true_measurements + noise\n\nestimated_theta = np.linalg.inv(data.T @ data) @ data.T @ noisy_measurements\n\nMTM_inv = np.linalg.inv(np.dot(data.T, data))\nMTy = np.dot(data.T, noisy_measurements)\nestimated_theta = np.dot(MTM_inv, MTy)\n\nnp.linalg.lstsq(data, noisy_measurements)\n'
'import tensorflow as tf\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nc = tf.add(a, b)\n\nsess = tf.Session()\nprint(sess.run(c, feed_dict={a: 1, b: 2}))\nsess.close()\n\n1. accuracy, cost = sess.run([accuracy, cost], feed_dict = {X:X_batch, Y:Y_batch})\n\n2. accuracy  = sess.run(accuracy, feed_dict = {X:X_batch, Y: Y_batch})\n   cost = sess.run(cost, feed_dict = {X:X_batch, Y:Y_batch})\n\n# Retrieve the values of the weight-variables from TensorFlow.\n# A feed-dict is not necessary because nothing is calculated.\nw = session.run(weights)\n'
'softmax(x)_i = exp(x_i) / SUM_j exp(x_j)\n\nsoftmax(x) = exp(x) / exp(x) = 1\n\ntf.sigmoid_cross_entropy_with_logits\n'
'model_vars = tf.get_collection(tf.GraphKeys.MODEL_VARIABLES)\n'
'#!python\n# -*- coding: utf-8 -*-#\n"""\nPerceptron Algorithm.\n\n@author: Bhishan Poudel\n\n@date:  Oct 31, 2017\n\n"""\n# Imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.linalg import norm\nimport os, shutil\nnp.random.seed(100)\n\ndef read_data(infile):\n    data = np.loadtxt(infile)\n    X = data[:,:-1]\n    Y = data[:,-1]\n\n    return X, Y\n\ndef plot_boundary(X,Y,w,epoch):\n    try:\n        plt.style.use(\'seaborn-darkgrid\')\n        # plt.style.use(\'ggplot\')\n        #plt.style.available\n    except:\n        pass\n\n    # Get data for two classes\n    idxN = np.where(np.array(Y)==-1)\n    idxP = np.where(np.array(Y)==1)\n    XN = X[idxN]\n    XP = X[idxP]\n\n    # plot two classes\n    plt.scatter(XN[:,0],XN[:,1],c=\'b\', marker=\'_\', label="Negative class")\n    plt.scatter(XP[:,0],XP[:,1],c=\'r\', marker=\'+\', label="Positive class")\n    # plt.plot(XN[:,0],XN[:,1],\'b_\', markersize=8, label="Negative class")\n    # plt.plot(XP[:,0],XP[:,1],\'r+\', markersize=8, label="Positive class")\n    plt.title("Perceptron Algorithm iteration: {}".format(epoch))\n\n    # plot decision boundary orthogonal to w\n    # w is w2,w1, w0  last term is bias.\n    if len(w) == 3:\n        a  = -w[0] / w[1]\n        b  = -w[0] / w[2]\n        xx = [ 0, a]\n        yy = [b, 0]\n        plt.plot(xx,yy,\'--g\',label=\'Decision Boundary\')\n\n    if len(w) == 2:\n        x2=[ w[0],  w[1],  -w[1],  w[0]]\n        x3=[ w[0],  w[1],   w[1], -w[0]]\n\n        x2x3 =np.array([x2,x3])\n        XX,YY,U,V = list(zip(*x2x3))\n        ax = plt.gca()\n        ax.quiver(XX,YY,U,V,scale=1, color=\'g\')\n\n    # Add labels\n    plt.xlabel(\'X\')\n    plt.ylabel(\'Y\')\n\n    # limits\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    plt.xlim(x_min,x_max)\n    plt.ylim(y_min,y_max)\n\n    # lines from origin\n    plt.axhline(y=0, color=\'k\', linestyle=\'--\',alpha=0.2)\n    plt.axvline(x=0, color=\'k\', linestyle=\'--\',alpha=0.2)\n    plt.grid(True)\n    plt.legend(loc=1)\n    plt.show()\n    plt.savefig(\'img/iter_{:03d}\'.format(int(epoch)))\n\n    # Always clost the plot\n    plt.close()\n\n\ndef predict(X,w):\n    return np.sign(np.dot(X, w))\n\ndef plot_contour(X,Y,w,mesh_stepsize):\n    try:\n        plt.style.use(\'seaborn-darkgrid\')\n        # plt.style.use(\'ggplot\')\n        #plt.style.available\n    except:\n        pass    \n    # Get data for two classes\n    idxN = np.where(np.array(Y)==-1)\n    idxP = np.where(np.array(Y)==1)\n    XN = X[idxN]\n    XP = X[idxP]\n\n    # plot two classes with + and - sign\n    fig, ax = plt.subplots()\n    ax.set_title(\'Perceptron Algorithm\')\n    plt.xlabel("X")\n    plt.ylabel("Y")\n    plt.plot(XN[:,0],XN[:,1],\'b_\', markersize=8, label="Negative class")\n    plt.plot(XP[:,0],XP[:,1],\'y+\', markersize=8, label="Positive class")\n    plt.legend()\n\n    # create a mesh for contour plot\n    # We first make a meshgrid (rectangle full of pts) from xmin to xmax and ymin to ymax.\n    # We then predict the label for each grid point and color it.\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    # Get 2D array for grid axes xx and yy  (shape = 700, 1000)\n    # xx has 700 rows.\n    # xx[0] has 1000 values.\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_stepsize),\n                         np.arange(y_min, y_max, mesh_stepsize))\n\n    # Get 1d array for x and y axes\n    xxr = xx.ravel()  # shape (700000,)\n    yyr = yy.ravel()  # shape (700000,)\n\n    # ones vector\n    # ones = np.ones(xxr.shape[0]) # shape (700000,)\n    ones = np.ones(len(xxr)) # shape (700000,)\n\n    # Predict the score\n    Xvals  = np.c_[ones, xxr, yyr]\n    scores = predict(Xvals, w)\n\n    # Plot contour plot\n    scores = scores.reshape(xx.shape)\n    ax.contourf(xx, yy, scores, cmap=plt.cm.Paired)\n    # print("xx.shape = {}".format(xx.shape))               # (700, 1000)\n    # print("scores.shape = {}".format(scores.shape))       # (700, 1000)\n    # print("scores[0].shape = {}".format(scores[0].shape)) # (1000,)\n\n    # show the plot\n    plt.savefig("Perceptron.png")\n    plt.show()\n    plt.close()\n\ndef perceptron_sgd(X, Y,epochs):\n    """\n    X: data matrix without bias.\n    Y: target\n    """\n    # add bias to X\'s first column\n    ones = np.ones(X.shape[0]).reshape(X.shape[0],1)\n    X1 = np.append(ones, X, axis=1)\n\n\n    w = np.zeros(X1.shape[1])\n    final_iter = epochs\n\n    for epoch in range(epochs):\n        print("\\n")\n        print("epoch: {} {}".format(epoch, \'-\'*30))\n\n        misclassified = 0\n        for i, x in enumerate(X1):\n            y = Y[i]\n            h = np.dot(x, w)*y\n\n            if h &lt;= 0:\n                w = w + x*y\n                misclassified += 1\n                print(\'misclassified? yes  w: {} \'.format(w,i))\n\n            else:\n                print(\'misclassified? no  w: {}\'.format(w))\n                pass\n\n        if misclassified == 0:\n            final_iter = epoch\n            break\n\n    return w, final_iter\n\ndef aperceptron_sgd(X, Y,epochs):    \n    # initialize weights\n    w = np.zeros(X.shape[1] )\n    u = np.zeros(X.shape[1] )\n    b = 0\n    beta = 0\n\n    # counters    \n    final_iter = epochs\n    c = 1\n    converged = False\n\n    # main average perceptron algorithm\n    for epoch in range(epochs):\n        # initialize misclassified\n        misclassified = 0\n\n        # go through all training examples\n        for  x,y in zip(X,Y):\n            h = y * (np.dot(x, w) + b)\n\n            if h &lt;= 0:\n                w = w + y*x\n                b = b + y\n\n                u = u+ y*c*x\n                beta = beta + y*c\n                misclassified += 1\n\n        # update counter regardless of good or bad classification        \n        c = c + 1\n\n        # break loop if w converges\n        if misclassified == 0:\n            final_iter = epoch\n            converged = True\n            print("Averaged Perceptron converged after: {} iterations".format(final_iter))\n            break\n\n    if converged == False:\n        print("Averaged Perceptron DID NOT converged.")\n\n    # prints\n    # print("final_iter = {}".format(final_iter))\n    # print("b, beta, c , (b-beta/c)= {} {} {} {}".format(b, beta, c, (b-beta/c)))\n    # print("w, u, (w-u/c) {} {} {}".format(w, u, (w-u/c)) )\n\n\n    # return w and final_iter\n    w = w - u/c\n    b = np.array([b- beta/c])\n    w = np.append(b, w)\n\n    return w, final_iter\n\ndef main():\n    """Run main function."""\n\n    X, Y = read_data(\'data.txt\') # X is without bias\n    max_iter = 20\n    w, final_iter = aperceptron_sgd(X,Y,max_iter)\n    print(\'w = \', w)\n\n    plot_boundary(X,Y,w,final_iter)\n\n    # contour plot\n    mesh_stepsize = 0.01\n    plot_contour(X,Y,w,mesh_stepsize)\n\nif __name__ == "__main__":\n    main()\n'
'from sklearn.metrics import accuracy_score\ny_pred=classifer_name.predict(X_test) #classifier_name=trained SVM/LogiReg/VotingClassifier\nprint(classifier_name.__class__.__name__,accuracy_score(y_true,y_pred))\n'
'def categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n                          K.argmax(y_pred, axis=-1)),\n                          K.floatx()) \n\n(amount of correct guesses)/(total amount of guesses) \n'
'text = "I love money" #My test sentence\ntext2 = "Lorem ipsum dolor sit amet, consectetur adipiscing elit, " \\\n        "sed do eiusmod tempor incididunt ut labore et dolore magna aliqua"\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts([text, text2])\n...\n\n(i , sit ) -&gt; 0\n(love , i ) -&gt; 1\n(love , money ) -&gt; 1\n(love , ut ) -&gt; 0\n(love , sit ) -&gt; 0\n(money , consectetur ) -&gt; 0\n(money , love ) -&gt; 1\n(i , love ) -&gt; 1\n'
'data.sort_values(by=\'Pi_values\', ascending=True, inplace=True)\n\nx1 = data["Pi_values"].values\ny1 = data["CO2_at_solubility"].values\n# Curve fitting with scipy.optimize.curve_fit\npopt, pcov = opt.curve_fit(func, x1, y1)\n# Use the optimized parameters to plot the best fit\nplt.plot(x1, y1, \'o\', x1, func(x1, *popt))\n'
"swords = set(stopwords.words('english'))\ntv = TfidfVectorizer(stop_words = swords , strip_accents='ascii')\n\nqueslst = [q for (q,a,c) in qlist]\nqlen = len(set([c for (q,a,c) in qlist]))\n\nmtx = tv.fit_transform(queslst)\n\nindices = []\nfor i,mx in enumerate(mtx):\n    if np.sum(mx, axis=1) == 0:\n        indices.append(i)\n\nmask = np.ones(mtx.shape[0], dtype=bool)\nmask[indices] = False\nmtx = mtx[mask]        \n\ncocluster = SpectralCoclustering(n_clusters=qlen, svd_method='arpack', random_state=0) #\n\nt = time()\n\ncocluster.fit(mtx)\n"
'tree.children.append(Tree(value=unique_targets[0]))\n\n        for unique_val in np.unique(data[best_split]):\n            new_tree = Tree()\n            new_tree.value = unique_val\n            tree.children.append(run_id3(data[data[best_split] == unique_val], target, features, new_tree))\n\n        for unique_val in np.unique(data[best_split]):\n            new_tree = Tree()\n            new_tree.value = unique_val\n            run_id3(data[data[best_split] == unique_val], target, features, new_tree)\n\nclass Tree():\n    def __init__(self, children = [], label = None, value = None):\n        self.children = children\n\nclass Tree: \n\n    def __init__(self, children=None, label=None, value=None):\n        self.children = children if children is not None else []\n        ...\n'
"grad_y_K = (X[:, None, :] - Y) / h * K[:, :, None]  # Shape: N_x, N_y, D\nterm_1 = np.einsum('ij,ikl-&gt;ikjl', grad_log_px, grad_y_K)  # Shape: N_x, N_y, D_x, D_y\nterm_2_h = np.einsum('ij,kl-&gt;ijkl', K, np.eye(D)) / h  # Shape: N_x, N_y, D_x, D_y\nterm_2_h2_xy = np.einsum('ijk,ijl-&gt;ijkl', grad_y_K, grad_y_K)  # Shape: N_x, N_y, D_x, D_y\nterm_2_h2 = K[:, :, None, None] * term_2_h2_xy / h**2  # Shape: N_x, N_y, D_x, D_y\nterm_2 = term_2_h - term_2_h2  # Shape: N_x, N_y, D_x, D_y\n\n(term_1 + term_2).sum(axis=0) / N  # Shape: N_y, D_x, D_y\n"
'import numpy as np\nimport statsmodels.api as sm\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\n\n# dummy data:\ny = np.array([1,3,4,5,2,3,4])\nX = np.array(range(1,8)).reshape(-1,1) # reshape to column\n\n# scikit-learn:\nlr = LinearRegression()\nlr.fit(X,y)\n# LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n#     normalize=False)\n\nlr.score(X,y)\n# 0.16118421052631582\n\ny_pred=lr.predict(X)\nr2_score(y, y_pred)\n# 0.16118421052631582\n\n\n# statsmodels\n# first artificially add intercept to X, as advised in the docs:\nX_ = sm.add_constant(X)\n\nmodel = sm.OLS(y,X_) # X_ here\nresults = model.fit()\nresults.rsquared\n# 0.16118421052631593\n\nlr2 = LinearRegression(fit_intercept=False)\nlr2.fit(X_,y) # X_ here\n# LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n#         normalize=False)\n\nlr2.score(X_, y)\n# 0.16118421052631593\n\ny_pred2 = lr2.predict(X_)\nr2_score(y, y_pred2)\n# 0.16118421052631593\n\nmodel3 = sm.OLS(y,X) # X here, i.e. no intercept\nresults3 = model2.fit()\nresults3.rsquared\n# 0.8058035714285714\n\n# scikit-learn\nlr3 = LinearRegression(fit_intercept=False)\nlr3.fit(X,y) # X here\nlr3.score(X,y)\n# -0.4309210526315792\n\ny_pred3 = lr3.predict(X)\nr2_score(y, y_pred3)\n# -0.4309210526315792\n'
'for i, _ in enumerate(self.dataloader):\n\n    # Get training data\n    # inputs, labels = data\n\n    inputs, labels = self.dataloader.dataset[sample_idx]\n    inputs = inputs.unsqueeze(0)\n    labels = labels.unsqueeze(0)\n\n    # Train the network\n    # [...]\n'
"from sklearn.model_selection import KFold\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import f1_score\n\nkf = KFold(n_splits=5)\n\nfor fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n    X_train = X[train_index]\n    y_train = y[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n    X_test = X[test_index]\n    y_test = y[test_index]  # See comment on ravel and  y_train\n    sm = SMOTE()\n    X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)\n    model = ...  # Choose a model here\n    model.fit(X_train_oversampled, y_train_oversampled )  \n    y_pred = model.predict(X_test)\n    print(f'For fold {fold}:')\n    print(f'Accuracy: {model.score(X_test, y_test)}')\n    print(f'f-score: {f1_score(y_test, y_pred)}')\n"
'class Generator(nn.Module):\n\ndef __init__(self, input_size, hidden_size, output_size, f):\n'
'def bleu_score(original,machine_translated):\n    \'\'\'\n    Bleu score function given a orginal and a machine translated sentences\n    \'\'\'\n    mt_length = len(machine_translated.split())\n    o_length = len(original.split())\n\n    # Brevity Penalty \n    if mt_length&gt;o_length:\n        BP=1\n    else:\n        penality=1-(mt_length/o_length)\n        BP=np.exp(penality)\n\n    # Clipped precision\n    clipped_precision_score = []\n    for i in range(1, 5):\n        original_n_gram = Counter(n_gram_generator(original,i))\n        machine_n_gram = Counter(n_gram_generator(machine_translated,i))\n\n        c = sum(machine_n_gram.values())\n        for j in machine_n_gram:\n            if j in original_n_gram:\n                if machine_n_gram[j] &gt; original_n_gram[j]:\n                    machine_n_gram[j] = original_n_gram[j]\n            else:\n                machine_n_gram[j] = 0\n\n        #print (sum(machine_n_gram.values()), c)\n        clipped_precision_score.append(sum(machine_n_gram.values())/c)\n\n    #print (clipped_precision_score)\n\n    weights =[0.25]*4\n\n    s = (w_i * math.log(p_i) for w_i, p_i in zip(weights, clipped_precision_score))\n    s = BP * math.exp(math.fsum(s))\n    return s\n\noriginal = "It is a guide to action which ensures that the military alwasy obeys the command of the party"\nmachine_translated = "It is the guiding principle which guarantees the military forces alwasy being under the command of the party"\n\nprint (bleu_score(original, machine_translated))\nprint (sentence_bleu([original.split()], machine_translated.split()))\n\n0.27098211583470044\n0.27098211583470044\n'
'def ranking_loss(y_true, y_pred):\n    y_true_ = tf.cast(y_true, tf.float32)\n    partial_losses = tf.maximum(0.0, 1 - y_pred[:, None, :] + y_pred[:, :, None])\n    loss = partial_losses * y_true_[:, None, :] * (1 - y_true_[:, :, None])\n    return tf.reduce_sum(loss)\n'
'df.iloc[:, list(range(0,10))+list(range(29,40))]\n'
"({'words': words_data, 'importance': importance_data},\n {'output1': output1_data, 'output2': output2_data})\n\nexample = tuple([\n     {'inputs': text_encoder.encode(inputs) + [EOS_ID]},\n     {'targets': text_encoder.encode(targets) + [EOS_ID]},\n])\n\noutput_types=(\n    {'inputs': tf.int64},\n    {'targets': tf.int64}\n)\n\npadded_shapes=(\n    {'inputs': (None,)},\n    {'targets': (None,)}\n)\n\ndef prepare_example(ex_inputs: dict, ex_outputs: dict, params: dict):\n    # Make sure targets are one-hot encoded\n    ex_outputs['targets'] = tf.one_hot(ex_outputs['targets'], depth=params['vocab_size'])\n    return ex_inputs, ex_outputs\n\nreturn {'targets': x}\n\nself.out_layer = keras.layers.Dense(params['vocab_size'], activation='softmax')\n\nself.out_layer = keras.layers.Dense(params['vocab_size'], activation='softmax')\n\nself.model_layers = [\n    keras.layers.Embedding(params['vocab_size'], params['vocab_size']),\n    keras.layers.Conv1D(32, 4, padding='same'),\n    keras.layers.TimeDistributed(self.out_layer)\n]\n\n\n# ...\npadded_shapes=(\n    {'inputs': (10,)},\n    {'targets': (10,)}\n)\n"
"svr = SVR(kernel='linear', C=1.0, epsilon=0.2)\n\nnTrain = np.floor(nCases *2.0 / 3.0)\nimport random\nids = range(nCases)\nrandom.shuffle(ids)\n\ntrainX,trainY,testX,testY = [],[],[],[]\nfor i, idx in enumerate(ids):\n    if i &lt; nTrain:\n        trainX.append(X[idx])\n        trainY.append(y[idx])\n    else:\n        testX.append(X[idx])\n        testY.append(y[idx])\n\nsvr = SVR(kernel='rbf',  C=1.0, epsilon=0.2, gamma=.0001)\n"
"from itertools import repeat\n\n# determine the classes that were not present in the training set;\n# the ones that were are listed in clf.classes_.\nclasses_not_trained = set(clf.classes_).symmetric_difference(all_classes)\n\n# the order of classes in predict_proba's output matches that in clf.classes_.\nprob = clf.predict_proba(test_samples)\nfor row in prob:\n    prob_per_class = (zip(clf.classes_, prob)\n                    + zip(classes_not_trained, repeat(0.)))\n"
'&gt;&gt;&gt; model.fit(X_train, y_train).score(X_train, y_train)\n'
'x_train=vectorizer.fit_transform(f1)\nx_test=vectorizer.transform(data2)\n'
'f_first_layer = theano.function([x], first_layer)\n'
'from collections import Counter\nfrom itertools import chain\nfrom string import punctuation\n\nfrom nltk.corpus import brown, stopwords\n\n# Let\'s say the training/testing data is a list of words and POS\nsentences = brown.sents()[:2]\n\n# Extract the content words as features, i.e. columns.\nvocabulary = list(chain(*sentences))\nstops = stopwords.words(\'english\') + list(punctuation)\nvocab_nostop = [i.lower() for i in vocabulary if i not in stops]\n\n# Create a matrix from the sentences\nmatrix = [Counter([w for w in words if w in vocab_nostop]) for words in sentences]\n\nprint matrix\n\n[Counter({u"\'\'": 1, u\'``\': 1, u\'said\': 1, u\'took\': 1, u\'primary\': 1, u\'evidence\': 1, u\'produced\': 1, u\'investigation\': 1, u\'place\': 1, u\'election\': 1, u\'irregularities\': 1, u\'recent\': 1}), Counter({u\'the\': 6, u\'election\': 2, u\'presentments\': 1, u\'``\': 1, u\'said\': 1, u\'jury\': 1, u\'conducted\': 1, u"\'\'": 1, u\'deserves\': 1, u\'charge\': 1, u\'over-all\': 1, u\'praise\': 1, u\'manner\': 1, u\'term-end\': 1, u\'thanks\': 1})]\n\nPošto je EULEX obećao da će obaviti istragu o prošlosedmičnom izbijanju nasilja na sjeveru Kosova, taj incident predstavlja još jedan ispit kapaciteta misije da doprinese jačanju vladavine prava.\nDe todas as provações que teve de suplantar ao longo da vida, qual foi a mais difícil? O início. Qualquer começo apresenta dificuldades que parecem intransponíveis. Mas tive sempre a minha mãe do meu lado. Foi ela quem me ajudou a encontrar forças para enfrentar as situações mais decepcionantes, negativas, as que me punham mesmo furiosa.\nAl parecer, Andrea Guasch pone que una relación a distancia es muy difícil de llevar como excusa. Algo con lo que, por lo visto, Alex Lequio no está nada de acuerdo. ¿O es que más bien ya ha conseguido la fama que andaba buscando?\nVo väčšine golfových rezortov ide o veľký komplex niekoľkých ihrísk blízko pri sebe spojených s hotelmi a ďalšími možnosťami trávenia voľného času – nie vždy sú manželky či deti nadšenými golfistami, a tak potrebujú iný druh vyžitia. Zaujímavé kombinácie ponúkajú aj rakúske, švajčiarske či talianske Alpy, kde sa dá v zime lyžovať a v lete hrať golf pod vysokými alpskými končiarmi.\n\n[bs,pt,es,sr]\n\nimport codecs\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntrainfile = \'train.txt\'\ntestfile = \'test.txt\'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = CountVectorizer(analyzer=\'word\')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,\'r\',\'utf8\'))\ntags = [\'bs\',\'pt\',\'es\',\'sr\']\nprint word_vectorizer.get_feature_names()\n\n[u\'acuerdo\', u\'aj\', u\'ajudou\', u\'al\', u\'alex\', u\'algo\', u\'alpsk\\xfdmi\', u\'alpy\', u\'andaba\', u\'andrea\', u\'ao\', u\'apresenta\', u\'as\', u\'bien\', u\'bl\\xedzko\', u\'buscando\', u\'come\\xe7o\', u\'como\', u\'con\', u\'conseguido\', u\'da\', u\'de\', u\'decepcionantes\', u\'deti\', u\'dificuldades\', u\'dif\\xedcil\', u\'distancia\', u\'do\', u\'doprinese\', u\'druh\', u\'d\\xe1\', u\'ela\', u\'encontrar\', u\'enfrentar\', u\'es\', u\'est\\xe1\', u\'eulex\', u\'excusa\', u\'fama\', u\'foi\', u\'for\\xe7as\', u\'furiosa\', u\'golf\', u\'golfistami\', u\'golfov\\xfdch\', u\'guasch\', u\'ha\', u\'hotelmi\', u\'hra\\u0165\', u\'ide\', u\'ihr\\xedsk\', u\'incident\', u\'intranspon\\xedveis\', u\'in\\xedcio\', u\'in\\xfd\', u\'ispit\', u\'istragu\', u\'izbijanju\', u\'ja\\u010danju\', u\'je\', u\'jedan\', u\'jo\\u0161\', u\'kapaciteta\', u\'kde\', u\'kombin\\xe1cie\', u\'komplex\', u\'kon\\u010diarmi\', u\'kosova\', u\'la\', u\'lado\', u\'lequio\', u\'lete\', u\'llevar\', u\'lo\', u\'longo\', u\'ly\\u017eova\\u0165\', u\'mais\', u\'man\\u017eelky\', u\'mas\', u\'me\', u\'mesmo\', u\'meu\', u\'minha\', u\'misije\', u\'mo\\u017enos\\u0165ami\', u\'muy\', u\'m\\xe1s\', u\'m\\xe3e\', u\'na\', u\'nada\', u\'nad\\u0161en\\xfdmi\', u\'nasilja\', u\'negativas\', u\'nie\', u\'nieko\\u013ek\\xfdch\', u\'no\', u\'obaviti\', u\'obe\\u0107ao\', u\'para\', u\'parecem\', u\'parecer\', u\'pod\', u\'pone\', u\'pon\\xfakaj\\xfa\', u\'por\', u\'potrebuj\\xfa\', u\'po\\u0161to\', u\'prava\', u\'predstavlja\', u\'pri\', u\'prova\\xe7\\xf5es\', u\'pro\\u0161losedmi\\u010dnom\', u\'punham\', u\'qual\', u\'qualquer\', u\'que\', u\'quem\', u\'rak\\xfaske\', u\'relaci\\xf3n\', u\'rezortov\', u\'sa\', u\'sebe\', u\'sempre\', u\'situa\\xe7\\xf5es\', u\'sjeveru\', u\'spojen\\xfdch\', u\'suplantar\', u\'s\\xfa\', u\'taj\', u\'tak\', u\'talianske\', u\'teve\', u\'tive\', u\'todas\', u\'tr\\xe1venia\', u\'una\', u\'ve\\u013ek\\xfd\', u\'vida\', u\'visto\', u\'vladavine\', u\'vo\', u\'vo\\u013en\\xe9ho\', u\'vysok\\xfdmi\', u\'vy\\u017eitia\', u\'v\\xe4\\u010d\\u0161ine\', u\'v\\u017edy\', u\'ya\', u\'zauj\\xedmav\\xe9\', u\'zime\', u\'\\u0107e\', u\'\\u010dasu\', u\'\\u010di\', u\'\\u010fal\\u0161\\xedmi\', u\'\\u0161vaj\\u010diarske\']\n\nPor ello, ha insistido en que Europa tiene que darle un toque de atención porque Portugal esta incumpliendo la directiva del establecimiento del peaje\nEstima-se que o mercado homossexual só na Cidade do México movimente cerca de oito mil milhões de dólares, aproximadamente seis mil milhões de euros\n\nimport codecs, re, time\nfrom itertools import chain\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntrainfile = \'train.txt\'\ntestfile = \'test.txt\'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = CountVectorizer(analyzer=\'word\')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,\'r\',\'utf8\'))\ntags = [\'bs\',\'pt\',\'es\',\'sr\']\n\n# Training NB\nmnb = MultinomialNB()\nmnb.fit(trainset, tags)\n\n# Tagging the documents\ncodecs.open(testfile,\'r\',\'utf8\')\ntestset = word_vectorizer.transform(codecs.open(testfile,\'r\',\'utf8\'))\nresults = mnb.predict(testset)\n\nprint results\n\n[\'es\' \'pt\']\n\nimport codecs, re, time\nfrom itertools import chain\n\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import Perceptron\n\ntrainfile = \'train.txt\'\ntestfile = \'test.txt\'\n\n# Vectorizing data.\ntrain = []\nword_vectorizer = HashingVectorizer(analyzer=\'word\')\ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,\'r\',\'utf8\'))\ntags = [\'bs\',\'pt\',\'es\',\'sr\']\n\n# Training Perceptron\npct = Perceptron(n_iter=100)\npct.fit(trainset, tags)\n\n# Tagging the documents\ncodecs.open(testfile,\'r\',\'utf8\')\ntestset = word_vectorizer.transform(codecs.open(testfile,\'r\',\'utf8\'))\nresults = pct.predict(testset)\n\nprint results\n\n[\'es\' \'es\']\n'
"from sklearn.datasets import load_iris\nfrom sklearn import tree\n\n# load data and divide it to train and validation\niris = load_iris()\n\nnum_train = 100\nX_train = iris.data[:num_train,:]\nX_valida = iris.data[num_train:,:]\n\ny_train = iris.target[:num_train]\ny_valida = iris.target[num_train:]\n\n# convert data to float32\nX_train = X_train.astype('float32')\n\n# fit the decision tree using the train data set\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X_train, y_train)\n\n# Now I want to know the corresponding leaf node id for each of my training data point\nclf.tree_.apply(X_train)\n\n# This gives the leaf node id:\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2])\n"
'import numpy as np\nfrom scipy import stats\ndata = np.array([[1, 4, 3], [2, .6, 1.2], [2, 1, 1.2],\n         [2, 0.5, 1.4], [5, .5, 0], [0, 0, 0],\n         [1, 4, 3], [5, .5, 0], [2, .5, 1.2]])\n\ndata = data.T #The KDE takes N vectors of length K for K data points\n              #rather than K vectors of length N\n\nkde = stats.gaussian_kde(data)\n\n# You now have your kde!!  Interpreting it / visualising it can be difficult with 3D data\n# You might like to try 2D data first - then you can plot the resulting estimated pdf\n# as the height in the third dimension, making visualisation easier.\n\n# Here is the basic way to evaluate the estimated pdf on a regular n-dimensional mesh\n# Create a regular N-dimensional grid with (arbitrary) 20 points in each dimension\nminima = data.T.min(axis=0)\nmaxima = data.T.max(axis=0)\nspace = [np.linspace(mini,maxi,20) for mini, maxi in zip(minima,maxima)]\ngrid = np.meshgrid(*space)\n\n#Turn the grid into N-dimensional coordinates for each point\n#Note - coords will get very large as N increases...\ncoords = np.vstack(map(np.ravel, grid))\n\n#Evaluate the KD estimated pdf at each coordinate\ndensity = kde(coords)\n\n#Do what you like with the density values here..\n#plot them, output them, use them elsewhere...\n'
'def output(self, input):\n    input = input.flatten(2)\n    self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n    return self.p_y_given_x\n\ndef neg_log_likelihood(self, x, y):\n    p_y_given_x = self.output(x)\n    return -T.mean(T.log(p_y_given_x)[T.arange(y.shape[0]), y])\n\ncost = mlp.neg_log_likelihood(x_, y)\n'
'cv = cross_validation.KFold(len(train), n_folds=5, indices=False)\n'
'saveRDS(model, file = "model.rds")\n\nloadedModel &lt;- readRDS(model.rds)\n'
"pipeline = Pipeline([\n  ('fs', RandomizedLogisticRegression()),\n  ('clf', LogisticRegression())\n])\n\nparams = {'fs__C':[0.1, 1, 10]}\n\ngrid_search = GridSearchCV(pipeline, params)\n"
'(x&gt;q)*((x-q)*a)+(x&lt;q)*((x-q)*c)+b\n'
'test = [[np.random.uniform(-1, 1) for _ in xrange(len(X[0]))]]\n\nneighbors, distances = knn.kneighbors(test)\nfor d in distances:\n    weight = 1.0/d\nprint weight\n'
'lstm = rnn_cell.BasicLSTMCell(lstm_size, forget_bias=1.0)\n\nX_split = tf.split(0, time_step_size, X)\noutputs, states = rnn.rnn(lstm, X_split, initial_state=init_state)\n'
"from datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom pandas_datareader.data import DataReader\n\nprices = DataReader('IBM', 'yahoo', datetime(2015, 1, 1), datetime.today().utcnow())['Open'].resample('D').fillna(method='ffill')\nprices.head()\n\nDate\n2015-01-02    161.309998\n2015-01-03    161.309998\n2015-01-04    161.309998\n2015-01-05    161.270004\n2015-01-06    159.669998\nFreq: D, Name: Open, dtype: float64\n\ndef get_pnl(prices, start_date, holding_period=90, profit_goal=0.10, cut_loss=.10):\n    end_date = start_date + timedelta(days=holding_period)\n    data = prices[start_date: end_date]\n\n    start_price = data.iloc[0]\n    take_profit = start_price * (1 + profit_goal)\n    cut_loss = start_price * (1 - cut_loss)\n    exit_date = end_date\n\n    if (data &gt; take_profit).any():\n        exit_date = data[data &gt; take_profit].index[0]\n    if (data[:exit_date] &lt; cut_loss).any():\n        exit_date = data[data &lt; cut_loss].index[0]\n\n    exit_price = data.loc[exit_date]\n    print('Entered on {0} at: {1:.2f}, exited on {2} at {3:.2f} for {4:.2f}%'.format(start_date.strftime('%Y-%b-%d'), start_price, exit_date.strftime('%Y-%b-%d'), exit_price, (exit_price/start_price-1)*100))\n\nfor start_date in [datetime(2015, 1, 1) + relativedelta(months=i) for i in range(12)]:\n    get_pnl(prices, start_date)\n\nEntered on 2015-Jan-01 at 161.31, exited on 2015-Apr-01 at 160.23 for -0.67%\nEntered on 2015-Feb-01 at 153.91, exited on 2015-Apr-24 at 170.23 for 10.60%\nEntered on 2015-Mar-01 at 160.87, exited on 2015-May-30 at 171.35 for 6.51%\nEntered on 2015-Apr-01 at 160.23, exited on 2015-Jun-30 at 163.99 for 2.35%\nEntered on 2015-May-01 at 173.20, exited on 2015-Jul-30 at 160.50 for -7.33%\nEntered on 2015-Jun-01 at 170.21, exited on 2015-Aug-20 at 152.74 for -10.26%\nEntered on 2015-Jul-01 at 163.97, exited on 2015-Aug-24 at 143.47 for -12.50%\nEntered on 2015-Aug-01 at 161.40, exited on 2015-Aug-24 at 143.47 for -11.11%\nEntered on 2015-Sep-01 at 144.91, exited on 2015-Nov-30 at 138.61 for -4.35%\nEntered on 2015-Oct-01 at 145.31, exited on 2015-Dec-30 at 139.58 for -3.94%\nEntered on 2015-Nov-01 at 140.44, exited on 2016-Jan-20 at 118.46 for -15.65%\nEntered on 2015-Dec-01 at 139.58, exited on 2016-Jan-20 at 118.46 for -15.13%\n"
'x = tf.placeholder(tf.float32, shape=[None, self.image_height,  self.image_width, 1], name="data")\n\ny_ = tf.placeholder(tf.float32, shape=[None, num_labels], name="labels")\n'
"df = pd.DataFrame({'Categorical': ['apple', 'mango', 'apple', \n                                   'orange', 'mango', 'apple', \n                                   'orange', np.NaN]})\n\nenc = LabelEncoder()\nenc.fit_transform(df['Categorical'])\n\npd.factorize(df['Categorical'])[0]\narray([ 0,  1,  0,  2,  1,  0,  2, -1])\n\ndf = pd.read_csv(data, na_filter=False, ...)\n\ndf.fillna('Na', inplace=True)\n"
'from sklearn.grid_search import GridSearchCV\nprint("Fitting the classifier to the training set")\nparam_grid = {\'C\': [0.01, 0.1, 1, 10, 100], \'kernel\': [\'rbf\', \'linear\']}\nclf = GridSearchCV(SVC(class_weight=\'balanced\'), param_grid)\nclf = clf.fit(train_data, train_labels)\nprint("Best estimator found by grid search:")\nprint(clf.best_estimator_)\n'
'from sklearn.linear_model import Ridge, LinearRegression\ndata = [[0, 0], [1, 1], [2, 2]]\ntarget = [0, 1, 2]\n\nridge_model = Ridge(alpha=1e-8).fit(data, target)\nprint("RIDGE COEFS: " + str(ridge_model.coef_))\nols = LinearRegression().fit(data,target)\nprint("OLS COEFS: " + str(ols.coef_))\n\n# RIDGE COEFS: [ 0.49999999  0.50000001]\n# OLS COEFS: [ 0.5  0.5]\n#\n# VS. with alpha=0:\n# RIDGE COEFS: [  1.57009246e-16   1.00000000e+00]\n# OLS COEFS: [ 0.5  0.5]\n\nimport pandas as pd\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures, scale\n\ndataset = pd.read_csv(\'house_price_data.csv\')\n\n# scale the X data to prevent numerical errors.\nX = scale(dataset[\'sqft_living\'].reshape(-1, 1))\nY = dataset[\'price\'].reshape(-1, 1)\n\npolyX = PolynomialFeatures(degree=15).fit_transform(X)\n\nmodel1 = LinearRegression().fit(polyX, Y)\nmodel2 = Ridge(alpha=0).fit(polyX, Y)\n\nprint("OLS Coefs: " + str(model1.coef_[0]))\nprint("Ridge Coefs: " + str(model2.coef_[0]))\n\n#OLS Coefs: [  0.00000000e+00   2.69625315e+04   3.20058010e+04  -8.23455994e+04\n#  -7.67529485e+04   1.27831360e+05   9.61619464e+04  -8.47728622e+04\n#  -5.67810971e+04   2.94638384e+04   1.60272961e+04  -5.71555266e+03\n#  -2.10880344e+03   5.92090729e+02   1.03986456e+02  -2.55313741e+01]\n#Ridge Coefs: [  0.00000000e+00   2.69625315e+04   3.20058010e+04  -8.23455994e+04\n#  -7.67529485e+04   1.27831360e+05   9.61619464e+04  -8.47728622e+04\n#  -5.67810971e+04   2.94638384e+04   1.60272961e+04  -5.71555266e+03\n#  -2.10880344e+03   5.92090729e+02   1.03986456e+02  -2.55313741e+01]\n'
"model.booster().get_score(importance_type='gain')\n"
'from sklearn.model_selection import ShuffleSplit\n\nfrom sklearn.cross_validation import ShuffleSplit\n'
'import tensorflow as tf\n\ntruth_filenames_np = [\'dir/%d.jpg\' % j for j in range(66)]\ntruth_filenames_tf = tf.convert_to_tensor(truth_filenames_np)\n# get the labels\nlabels = [f.rsplit("/", 1)[1] for f in truth_filenames_np]\nlabels_tf = tf.convert_to_tensor(labels)\n\n# My list is also already shuffled, so I set shuffle=False\ntruth_image_name, truth_label = tf.train.slice_input_producer(\n    [truth_filenames_tf, labels_tf], shuffle=False)\n\n# # Another key step, where I batch them together\n# truth_images_batch, truth_label_batch = tf.train.batch(\n#     [truth_image_name, truth_label], batch_size=11)\n\nepochs = 7\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n    for i in range(epochs):\n        print("Epoch ", i)\n        X_truth_batch = truth_image_name.eval()\n        X_label_batch = truth_label.eval()\n        # Here I display all the images in this batch, and then I check\n        # which file numbers they actually are.\n        # BUT, the images that are displayed don\'t correspond with what is\n        # printed by X_label_batch!\n        print(X_truth_batch)\n        print(X_label_batch)\n    coord.request_stop()\n    coord.join(threads)\n\nEpoch  0\nb\'dir/0.jpg\'\nb\'1.jpg\'\nEpoch  1\nb\'dir/2.jpg\'\nb\'3.jpg\'\nEpoch  2\nb\'dir/4.jpg\'\nb\'5.jpg\'\nEpoch  3\nb\'dir/6.jpg\'\nb\'7.jpg\'\nEpoch  4\nb\'dir/8.jpg\'\nb\'9.jpg\'\nEpoch  5\nb\'dir/10.jpg\'\nb\'11.jpg\'\nEpoch  6\nb\'dir/12.jpg\'\nb\'13.jpg\'\n\nfor i in range(epochs):\n    print("Epoch ", i)\n    pair = tf.convert_to_tensor([truth_image_name, truth_label]).eval()\n    print(pair[0])\n    print(pair[1])\n\nEpoch  0\nb\'dir/0.jpg\'\nb\'0.jpg\'\nEpoch  1\nb\'dir/1.jpg\'\nb\'1.jpg\'\n# ...\n\nimport tensorflow as tf\n\ntruth_filenames_np = [\'dir/%d.jpg\' % j for j in range(66)]\ntruth_filenames_tf = tf.convert_to_tensor(truth_filenames_np)\nlabels = [f.rsplit("/", 1)[1] for f in truth_filenames_np]\nlabels_tf = tf.convert_to_tensor(labels)\ntruth_image_name, truth_label = tf.train.slice_input_producer(\n    [truth_filenames_tf, labels_tf], shuffle=False)\nepochs = 7\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    tf.train.start_queue_runners(sess=sess)\n    for i in range(epochs):\n        print("Epoch ", i)\n        X_truth_batch, X_label_batch = sess.run(\n            [truth_image_name, truth_label])\n        print(X_truth_batch)\n        print(X_label_batch)\n'
'b = tf.add(true_areas, true_areas).eval({input_2: someInputValues})\n'
'import numpy\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.framework import dtypes\n\nwith tf.Graph().as_default():\n  batch_size = 100\n  data = [\'a\']*9000+[\'b\']*1000\n  labels = [1]*9000+[0]*1000\n  data_tensor = ops.convert_to_tensor(data, dtype=dtypes.string)\n  label_tensor = ops.convert_to_tensor(labels, dtype=dtypes.int32)\n  shuffled_data, shuffled_labels = tf.train.slice_input_producer(\n      [data_tensor, label_tensor], shuffle=True, capacity=3*batch_size)\n  target_probs = numpy.array([0.5,0.5])\n  data_batch, label_batch = tf.contrib.training.stratified_sample(\n      [shuffled_data], shuffled_labels, target_probs, batch_size,\n      queue_capacity=2*batch_size)\n\n  with tf.Session() as session:\n    tf.local_variables_initializer().run()\n    tf.global_variables_initializer().run()\n    coordinator = tf.train.Coordinator()\n    tf.train.start_queue_runners(session, coord=coordinator)\n    num_iter = 10\n    sum_ones = 0.\n    for _ in range(num_iter):\n      d, l = session.run([data_batch, label_batch])\n      count_ones = l.sum()\n      sum_ones += float(count_ones)\n      print(\'percentage "a" = %.3f\' % (float(count_ones) / len(l)))\n    print(\'Overall: {}\'.format(sum_ones / (num_iter * batch_size)))\n    coordinator.request_stop()\n    coordinator.join()\n\npercentage "a" = 0.480\npercentage "a" = 0.440\npercentage "a" = 0.580\npercentage "a" = 0.570\npercentage "a" = 0.580\npercentage "a" = 0.520\npercentage "a" = 0.480\npercentage "a" = 0.460\npercentage "a" = 0.390\npercentage "a" = 0.530\nOverall: 0.503\n'
'regressor = estimator.DNNRegressor(feature_columns=my_feature_columns,\n                                   label_dimension=2,\n                                   hidden_units=hidden_layers,\n                                   model_dir=MODEL_PATH)\n'
"In [250]: df\nOut[250]:\n           Town\n0      New York\n1        Munich\n2          Kiev\n3         Paris\n4        Berlin\n5      New York\n6  Zaporizhzhia\n\nIn [251]: r1 = pd.DataFrame(lb.fit_transform(df['Town']), columns=lb.classes_)\n\nIn [252]: r1\nOut[252]:\n   Berlin  Kiev  Munich  New York  Paris  Zaporizhzhia\n0       0     0       0         1      0             0\n1       0     0       1         0      0             0\n2       0     1       0         0      0             0\n3       0     0       0         0      1             0\n4       1     0       0         0      0             0\n5       0     0       0         1      0             0\n6       0     0       0         0      0             1\n\nIn [253]: new\nOut[253]:\n       Town\n0    Munich\n1  New York\n2     Dubai  # &lt;--- new (not trained) town\n\nIn [254]: r2 = pd.DataFrame(lb.transform(new['Town']), columns=lb.classes_)\n\nIn [255]: r2\nOut[255]:\n   Berlin  Kiev  Munich  New York  Paris  Zaporizhzhia\n0       0     0       1         0      0             0\n1       0     0       0         1      0             0\n2       0     0       0         0      0             0\n"
"In [1]: from sklearn.ensemble import RandomForestClassifier\n   ...: from sklearn.neighbors import KNeighborsClassifier\n   ...:\n\nIn [2]: knn =  KNeighborsClassifier(4)\n\nIn [3]: forest = RandomForestClassifier(n_estimators=10, max_depth=4)\n\nIn [4]: knn.__bool__\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-4-ef1cfe16be77&gt; in &lt;module&gt;()\n----&gt; 1 knn.__bool__\n\nAttributeError: 'KNeighborsClassifier' object has no attribute '__bool__'\n\nIn [5]: knn.__len__\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-5-dc98bf8c50e0&gt; in &lt;module&gt;()\n----&gt; 1 knn.__len__\n\nAttributeError: 'KNeighborsClassifier' object has no attribute '__len__'\n\nIn [6]: forest.__bool__\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-6-fbdd7f01e843&gt; in &lt;module&gt;()\n----&gt; 1 forest.__bool__\n\nAttributeError: 'RandomForestClassifier' object has no attribute '__bool__'\n\nIn [7]: forest.__len__\nOut[7]:\n&lt;bound method BaseEnsemble.__len__ of RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=4, max_features='auto', max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)&gt;\n\nIn [8]: len(forest)\nOut[8]: 0\n\nIn [9]: from sklearn.datasets import make_classification\n   ...: X, y = make_classification(n_samples=1000, n_features=4,\n   ...:             n_informative=2, n_redundant=0,\n   ...:             random_state=0, shuffle=False)\n   ...:\n\nIn [10]: X.shape\nOut[10]: (1000, 4)\n\nIn [11]: y.shape\nOut[11]: (1000,)\n\nIn [12]: forest.fit(X,y)\nOut[12]:\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=4, max_features='auto', max_leaf_nodes=None,\n            min_impurity_split=1e-07, min_samples_leaf=1,\n            min_samples_split=2, min_weight_fraction_leaf=0.0,\n            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n            verbose=0, warm_start=False)\n\nIn [13]: len(forest)\nOut[13]: 10\n"
"df.col1 = df.col1.fillna(0)\n\ndf.col2 = df.DISTANCE_GROUP.fillna('')\n"
'import tensorflow as tf\nimport numpy as np\n\nd = {1:1,2:5,3:7,4:5,5:8,6:2}\nkeys = list(d.keys())\nvalues = [d[k] for k in keys]\ntable = tf.contrib.lookup.HashTable(\n  tf.contrib.lookup.KeyValueTensorInitializer(keys, values, key_dtype=tf.int64, value_dtype=tf.int64), -1\n)\nelems = tf.convert_to_tensor(np.array([1, 2, 3, 4, 5, 6]), dtype=tf.int64)\nres = tf.map_fn(lambda x: table.lookup(x), elems)\nsess=tf.Session()\nsess.run(table.init)\nprint(sess.run(res))\n'
"import tensorflow as tf\nimport numpy as np\n\nbb = tf.placeholder(tf.bool)\nxx = tf.Variable(initial_value=0.0,validate_shape=False,trainable=False,name='xx')\nyy = tf.placeholder(tf.float32, name='yy')\n\nzz = tf.cond(bb, lambda: xx + yy, lambda: 100 + yy)\n\nwith tf.Session() as sess:\n   sess.run(tf.global_variables_initializer())\n   dict1 = {bb:False, yy:np.array([1., 3, 4]), xx:np.array([5., 6, 7])}\n   print(sess.run(zz, feed_dict=dict1))\n   dict2 = {bb:False, yy:np.array([1., 3, 4])}\n   print(sess.run(zz, feed_dict=dict2))\n"
"cluster_centers = km.cluster_centers_\ncentroids = cluster_centers[y_km]\ndistortion = ((df_tr_std - centroids)**2.0).sum(axis=1)\n\ndistortion = ((df_tr_std - km.cluster_centers_[y_km])**2.0).sum(axis=1)\n\nIn [27]: import numpy as np\n\nIn [28]: from sklearn.cluster import KMeans\n\nIn [29]: df_tr_std = np.random.rand(1000,3)\n\nIn [30]: km = KMeans(n_clusters=3, init='k-means++',n_init=10,max_iter=300,tol=\n    ...: 1e-04,random_state=0)\n\nIn [31]: y_km = km.fit_predict(df_tr_std)\n\nIn [32]: distortion = ((df_tr_std - km.cluster_centers_[y_km])**2.0).sum(axis=1)\n\nIn [33]: km.inertia_\nOut[33]: 147.01626670004867\n\nIn [34]: distortion.sum()\nOut[34]: 147.01626670004865\n"
'def get_moves(self):\n    """\n    return remaining possible board moves\n    (ie where there are no O\'s or X\'s)\n    """\n    if self.result():\n        return []\n\n    return np.argwhere(self.state[0] + self.state[1] == 0).tolist()\n'
"from scipy.spatial.distance import cosine\nimport numpy as np\n\nword_weights = {'something': 2}\nfeature_names = vectorizer.get_feature_names()\nweights = np.ones(len(feature_names))\n\nfor key, value in word_weights.items():\n    weights[feature_names.index(key)] = value\n\nfor vector in trainVectorizerArray:\n    print(vector)\n    for testV in testVectorizerArray:\n        print(testV)\n        cosine_unweight = cosine(vector, testV)\n        cosine_weighted = cosine(vector, testV, w=weights)\n        print(cosine_unweight, cosine_weighted)\n"
'extract = Model(model.inputs, model.layers[-3].output) # Dense(128,...)\nfeatures = extract.predict(data)\n'
"from sklearn.preprocessing import FunctionTransformer\n\nget_numeric_data = FunctionTransformer(lambda x: x[['t1_prob', 't2_prob', 't3_prob', 't4_prob', 't5_prob']], validate=False)\n\npipeline = Pipeline(\n    [\n        (\n            &quot;features&quot;,\n            FeatureUnion(\n                [\n                    (&quot;numeric_features&quot;, Pipeline([(&quot;selector&quot;, get_numeric_data)])),\n                    (\n                        &quot;word_features&quot;,\n                        Pipeline(\n                            [\n                                (&quot;vect&quot;, CountVectorizer(analyzer=&quot;word&quot;, stop_words=&quot;english&quot;)),\n                                (&quot;tfidf&quot;, TfidfTransformer(use_idf=True)),\n                            ]\n                        ),\n                    ),\n                ]\n            ),\n        ),\n        (&quot;clf&quot;, OneVsRestClassifier(svm.LinearSVC(random_state=10))),\n    ]\n)\n"
"tsne = TSNE(n_components=2, n_jobs=5).fit_transform(X)\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(*zip(*tsne[:,:2]))\nplt.show()\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(*zip(*tsne))\nplt.show()\n\nplt.scatter(*zip(*tsne[:,:2]), c=X[:,2])\n\nplt.scatter(*zip(*tsne[:,:2]), c=X[:,2], cmap='RdBu')\n"
'my_classifier = ScrappyKNN \n\nmy_classifier = ScrappyKNN() \n\nfit(self,X_train,y_train)\n'
"imputer = Imputer(missing_values='NaN', strategy='mean')\nimputer.fit(x['Age'].values.reshape(-1, 1))\nx['Age'] = imputer.transform(x['Age'].values.reshape(-1, 1))\n\nx['Age'].fillna(x['Age'].mean(), inplace=True)\n"
'fc_model.fit_generator(mygen(q_train, i_train, a_train), ...)\n'
'x, y = next(iter(training_loader))\n\ntraining_loader_iter = iter(training_loader)\n\nfor i in range(num_batches_in_epoch):\n  x, y = next(training_loader_iter)\n'
"from keras.layers import Layer\nfrom keras import backend as K\n\nclass RBFLayer(Layer):\n    def __init__(self, units, gamma, **kwargs):\n        super(RBFLayer, self).__init__(**kwargs)\n        self.units = units\n        self.gamma = K.cast_to_floatx(gamma)\n\n    def build(self, input_shape):\n        self.mu = self.add_weight(name='mu',\n                                  shape=(int(input_shape[1]), self.units),\n                                  initializer='uniform',\n                                  trainable=True)\n        super(RBFLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        diff = K.expand_dims(inputs) - self.mu\n        l2 = K.sum(K.pow(diff,2), axis=1)\n        res = K.exp(-1 * self.gamma * l2)\n        return res\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.units)\n\nmodel = Sequential()\nmodel.add(Dense(20, input_shape=(100,)))\nmodel.add(RBFLayer(10, 0.5))\n"
'import tensorflow_probability as tfp\n\nmodel = tf.keras.Sequential([\n    tfp.layers.DenseFlipout(512, activation=tf.nn.relu),\n    tfp.layers.DenseFlipout(10),\n])\n\nlogits = model(features)\nneg_log_likelihood = tf.nn.softmax_cross_entropy_with_logits(\n    labels=labels, logits=logits)\n\nkl = sum(model.losses) # Losses are summed\n\n# The negative log-likelihood and the KL term are combined\nloss = neg_log_likelihood + kl \n\ntrain_op = tf.train.AdamOptimizer().minimize(loss)\n'
"# List of image paths, doesn't matter here\nimage_paths = ['./img_{}.png'.format(i) for i in range(5)] \nlabels = ...  # List of labels\n\ndf = pd.DataFrame()\ndf['filename'] = image_paths\ndf['class'] = labels\n\ngenerator = ImageDataGenerator().flow_from_dataframe(dataframe=df, \n                                                    directory='./',\n                                                    x_col='filename',\n                                                    y_col='class')\n\nlabels = ['cat', 'cat', 'cat', 'dog', 'dog']\n\ngenerator.class_indices\n&gt; {'cat': 0, 'dog': 1}\n\nlabels = ['dog', 'cat', 'cat', 'dog', 'dog']\n# After re-instantiating the generator\ngenerator.class_indices\n&gt; {'dog': 0, 'cat': 1}\n\nlabels = [1, 0, 1, 0, 0] # ['cat', 'dog', 'cat', 'dog', 'dog']\n# After re-instantiating the generator\ngenerator.class_indices\n&gt; {1: 0, 0: 1}  # !\n"
"import tensorflow as tf\nimport random\n\nrandom.seed(100)\n# Input data\nlabel = list(range(15))\n# Shuffle data\nrandom.shuffle(label)\n# Make feature from label data\nfeature = [lbl // 5 for lbl in label]\nbatch_size = 3\n\nprint('Data:')\nprint(*zip(feature, label), sep='\\n')\n\nwith tf.Graph().as_default(), tf.Session() as sess:\n    # Make dataset from data arrays\n    ds = tf.data.Dataset.from_tensor_slices({'feature': feature, 'label': label})\n    # Group by window\n    ds = ds.apply(tf.data.experimental.group_by_window(\n        # Use feature as key\n        key_func=lambda elem: tf.to_int64(elem['feature']),\n        # Convert each window to a batch\n        reduce_func=lambda _, window: window.batch(batch_size),\n        # Use batch size as window size\n        window_size=batch_size))\n    # Iterator\n    iter = ds.make_one_shot_iterator().get_next()\n    # Show dataset contents\n    print('Result:')\n    while True:\n        try:\n            print(sess.run(iter))\n        except tf.errors.OutOfRangeError: break\n"
'iris = load_iris()\n\nX, y = iris.data, iris.target\n\n# Maybe some original features where good, too?\nselection = SelectKBest()\n\n# Build SVC\nsvm = SVC(kernel="linear")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([("features", selection), ("svm", svm)])\n\nparam_grid = dict(features__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)\n'
'train_x, val_x, train_y, val_y = train_test_split(X, y,test_size=0.3)\n\nmodel = DecisionTreeRegressor() \nmodel.fit(train_x, train_y)\nval_predictions = model.predict(val_x) # contains y values predicted by the model\nscore = model.score(val_x, val_y) # evaluates predicted y against actual y of test data\nprint(score)\n'
'import numpy as np\n\nr = np.random.rand(3, 3, 1)\nr_swapped = np.swapaxes(r, 0, 2)\nprint(r)\nprint(r.shape)\nprint(r_swapped)\nprint(r_swapped.shape)\n'
"import matplotlib.pyplot as plt\nimport numpy as np\nfrom operator import itemgetter\nfrom itertools import groupby\nfrom scipy.optimize import leastsq\n\ndef moving_average( data, windowsize, **kwargs ):\n    mode=kwargs.pop('mode', 'valid')\n    weight = np.ones( windowsize )/ float( windowsize )\n    return np.convolve( data, weight, mode=mode)\n\ndef moving_sigma( data, windowsize ):\n    l = len( data )\n    sout = list()\n    for i in range( l - windowsize + 1 ):\n        sout += [ np.std( data[ i : i + windowsize ] ) ]\n    return np.array( sout )\n\ndef m_step( x, s, x0List, ampList, off ):\n    assert len( x0List ) == len( ampList )\n    out = [ a * 0.5 * ( 1 + np.tanh( s * (x - x0) ) ) for a, x0 in zip( ampList, x0List )]\n    return sum( out ) + off\n\ndef residuals( params, xdata, ydata ):\n    assert not len(params) % 2\n    off = params[0]\n    s = params[1]\n    rest = params[2:]\n    n = len( rest )\n    x0 = rest[:n/2]\n    am = rest[n/2:]\n    diff = [ y - m_step( x,s, x0, am, off ) for x, y in zip( xdata, ydata ) ]\n    return diff \n\n### generate data\nnp.random.seed(776)\na=0\ndata = list()\nfor i in range(15):\n    a = 50 * ( 1 - 2 * np.random.random() )\n    for _ in range ( np.random.randint( 5, 35 ) ):\n        data += [a]\n\nxx =  np.arange( len( data ), dtype=np.float )\n\nnoisydata = list()\nfor x in data:\n    noisydata += [ x + np.random.normal( scale=5 ) ]\n\n###define problem specific parameters\nmyWindow = 10\nthresh = 8\ncutoffscale = 1.1\n\n### data treatment\navdata = moving_average( noisydata, myWindow )\navx = moving_average( xx, myWindow )\nsdata = moving_sigma( noisydata, myWindow )\n\n### getting start values for the fit\nseq = [x for x in np.where( sdata &gt; thresh )[0] ]\n# from https://stackoverflow.com/a/3149493/803359\njumps = [ map( itemgetter(1), g ) for k, g in groupby( enumerate( seq ), lambda ( i, x ) : i - x ) ]\nxjumps = [ [ avx[ k ] for k in xList ] for xList in jumps ]\nmeans = [ np.mean( x ) for x in xjumps ]\n### the delta is too small as one only looks above thresh so lets increase it by guessing\ndeltas = [ ( avdata[ x[-1] ] - avdata[ x[0] ] ) * cutoffscale for x in jumps ]\n\nguess = [ avdata[0], 2] + means + deltas\n\n### fitting\nsol, err = leastsq( residuals, guess, args=( xx, noisydata ), maxfev=10000 )\nfittedoff = sol[0]\nfitteds = sol[1]\nfittedx0 = sol[ 2 : 2 + len( means ) ] \nfittedam = sol[ 2 + len( means ) : ] \n\n### plotting\nfig = plt.figure()\nax = fig.add_subplot( 1, 1, 1 )\n# ~ ax.plot( data )\nax.plot( xx, noisydata, label='noisy data' )\nax.plot( avx, avdata, label='moving average' )\nax.plot( avx, sdata, label='window sigma' )\n\nfor j in means:\n    ax.axvline( j, c='#606060', linestyle=':' )\n\nyy = [ m_step( x, 2, means, deltas, avdata[0] ) for x in xx]\nbestyy = [ m_step( x, fitteds, fittedx0, fittedam, fittedoff ) for x in xx ]\nax.plot( xx, yy, label='guess' )\nax.plot( xx, bestyy, label='fit' )\n\nplt.legend( loc=0 )\nplt.show()\n"
"shape=60\nbatch_size = 30\nnb_classes = 10\nimg_rows, img_cols = shape, shape\nnb_filters = 32\npool_size = (2, 2)\nkernel_size = (3, 3)\ninput_shape=(shape,shape,1)\n\nreg=0.001\nlearning_rate = 0.013\ndecay_rate = 5e-5\nmomentum = 0.9\n\nsgd = SGD(lr=learning_rate,momentum=momentum, decay=decay_rate, nesterov=True)\nshape2\n\nrecog0 = Sequential()\nrecog0.add(Convolution2D(20, 3,3,\n                        border_mode='valid',\n                        input_shape=input_shape))\nrecog0.add(BatchNormalization(mode=2))\n\nrecog=recog0\nrecog.add(Activation('relu'))\nrecog.add(MaxPooling2D(pool_size=(2,2)))\nrecog.add(UpSampling2D(size=(2, 2)))\nrecog.add(Convolution2D(20, 3, 3,init='glorot_uniform'))\nrecog.add(BatchNormalization(mode=2))\nrecog.add(Activation('relu'))\n\nfor i in range(0,2):\n    print(i,recog0.layers[i].name)\n\nrecog_res=recog0\npart=1\nrecog0.layers[part].name\nget_0_layer_output = K.function([recog0.layers[0].input, K.learning_phase()],[recog0.layers[part].output])\n\nget_0_layer_output([x_train, 0])[0][0]\n\npred=[np.argmax(get_0_layer_output([x_train, 0])[0][i]) for i in range(0,len(x_train))]\n\nloss=x_train-pred\nloss=loss.astype('float32')\n\nrecog_res.add(Lambda(lambda x: x,input_shape=(56,56,20),output_shape=(56,56,20)))\nrecog2=Sequential()\nrecog2.add(Merge([recog,recog_res],mode='ave'))\nrecog2.add(Activation('relu'))\nrecog2.add(Convolution2D(20, 3, 3,init='glorot_uniform'))\nrecog2.add(BatchNormalization(mode=2))\nrecog2.add(Activation('relu'))\nrecog2.add(Convolution2D(1, 1, 1,init='glorot_uniform'))\nrecog2.add(Reshape((shape2,shape2,1)))\nrecog2.add(Activation('relu'))\n\nrecog2.compile(loss='mean_squared_error', optimizer=sgd,metrics = ['mae'])\nrecog2.summary()\n\nx_train3=x_train2.reshape((1,shape2,shape2,1))\n\nrecog2.fit(x_train,x_train3,\n                nb_epoch=25,\n                batch_size=30,verbose=1)\n\nshape=60\nbatch_size = 30\nnb_classes = 10\nimg_rows, img_cols = shape, shape\nnb_filters = 32\npool_size = (2, 2)\nkernel_size = (3, 3)\ninput_shape=(shape,shape,1)\n\nreg=0.001\nlearning_rate = 0.012\ndecay_rate = 5e-5\nmomentum = 0.9\n\nsgd = SGD(lr=learning_rate,momentum=momentum, decay=decay_rate, nesterov=True)\n\nrecog0 = Sequential()\nrecog0.add(Convolution2D(20, 4,4,\n                        border_mode='valid',\n                        input_shape=input_shape))\nrecog0.add(BatchNormalization(mode=2))\nrecog0.add(MaxPooling2D(pool_size=(2,2)))\n\nrecog=recog0\nrecog.add(Activation('relu'))\nrecog.add(MaxPooling2D(pool_size=(2,2)))\nrecog.add(UpSampling2D(size=(2, 2)))\nrecog.add(Convolution2D(20, 1, 1,init='glorot_uniform'))\nrecog.add(BatchNormalization(mode=2))\nrecog.add(Activation('relu'))\n\nfor i in range(0,8):\n    print(i,recog0.layers[i].name)\n\nrecog_res=recog0\npart=8\nrecog0.layers[part].name\nget_0_layer_output = K.function([recog0.layers[0].input, K.learning_phase()],[recog0.layers[part].output])\nget_0_layer_output([x_train, 0])[0][0]\npred=[np.argmax(get_0_layer_output([x_train, 0])[0][i]) for i in range(0,len(x_train))]\n\nloss=x_train-pred\nloss=loss.astype('float32')\n\nrecog_res.add(Lambda(lambda x: x-np.mean(loss),input_shape=(28,28,20),output_shape=(28,28,20)))\n\nrecog2=Sequential()\nrecog2.add(Merge([recog,recog_res],mode='sum'))\nrecog2.add(UpSampling2D(size=(2, 2)))\nrecog2.add(Convolution2D(1, 3, 3,init='glorot_uniform'))\nrecog2.add(BatchNormalization(mode=2))\nrecog2.add(Reshape((shape2*shape2,)))\nrecog2.add(Reshape((shape2,shape2,1)))\nrecog2.add(Activation('relu'))\nrecog2.compile(loss='mean_squared_error', optimizer=sgd,metrics = ['mae'])\nrecog2.summary()\n\nx_train3=x_train2.reshape((1,shape2,shape2,1))\n\nrecog2.fit(x_train,x_train3,\n                nb_epoch=400,\n                batch_size=30,verbose=1)\n"
"class ChainEstimator(BaseEstimator,ClassifierMixin):\n    def __init__(self,est1,est2):\n        self.est1 = est1\n        self.est2 = est2\n\n    def fit(self,X,y):\n        self.est1.fit(X,y)\n        self.est2.fit(X,y)\n        return self\n\n    def predict(self,X):\n        ans = np.zeros((len(X),)) - 1\n        probs = self.est1.predict_proba(X)       #averaging confidence of Ada &amp; SVC\n        conf_samples = np.any(probs&gt;=.8,axis=1)  #samples with &gt;80% confidence\n        ans[conf_samples] = np.argmax(probs[conf_samples,:],axis=1) #Predicted Classes of confident samples\n        if conf_samples.sum()&lt;len(X):            #Use est2 for non-confident samples\n            ans[~conf_samples] = self.est2.predict(X[~conf_samples])\n        return ans\n\nest1 = VotingClassifier(estimators=[('ada',AdaBoostClassifier()),('svm',SVC(probability=True))],voting='soft')\nest2 = VotingClassifier(estimators=[('dt',DecisionTreeClassifier()),('knn',KNeighborsClassifier())])\nclf = ChainEstimator(est1,est2).fit(X_train,Y_train)\nans = clf.predict(X_test)\n\ndef fit(self,X,y):\n    self.est1.fit(X,y)\n    self.est1_perf = cross_val_score(self.est1,X,y,cv=4,scoring='f1_macro')\n    self.est2.fit(X,y)\n    self.est2_perf = cross_val_score(self.est2,X,y,cv=4,scoring='f1_macro')\n    return self\n"
'import numpy as np\ncnt = 0\ndef use_a_function_which_calls_training_and_computes_cv_instead_of_this(x):\n    global cnt\n    cnt += 1\n    return ((x - np.array([-1.6, 0.7, 0.3, 3.15, 4.53, 6.5, 6.77, 9.0]))**2).sum()\n\nmy_best_guess_for_the_initial_parameters = np.array([1.,2.,3.,4.,5.,6.,7.,8.])\noptimization_results = scipy.optimize.basinhopping(\n    use_a_function_which_calls_training_and_computes_cv_instead_of_this,\n    my_best_guess_for_the_initial_parameters,\n    niter=100)\nprint(&quot;Times function was called: {0}&quot;.format(cnt))\nprint(optimization_results.x)\n\nTimes function was called: 3080\n[-1.6         0.7         0.3         3.15        4.52999999  6.5\n  6.77        8.99999999]\n'
'pip install matplotlib --force-reinstall\n\nconda install freetype --force-reinstall\n'
"import numpy as np\nimport matplotlib.pyplot as plt \n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nX = np.arange(100).reshape(100, 1)\ny = X**4 + X**3 + X + 1\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nrmses = []\ndegrees = np.arange(1, 10)\nmin_rmse, min_deg = 1e10, 0\n\nfor deg in degrees:\n\n    # Train features\n    poly_features = PolynomialFeatures(degree=deg, include_bias=False)\n    x_poly_train = poly_features.fit_transform(x_train)\n\n    # Linear regression\n    poly_reg = LinearRegression()\n    poly_reg.fit(x_poly_train, y_train)\n\n    # Compare with test data\n    x_poly_test = poly_features.fit_transform(x_test)\n    poly_predict = poly_reg.predict(x_poly_test)\n    poly_mse = mean_squared_error(y_test, poly_predict)\n    poly_rmse = np.sqrt(poly_mse)\n    rmses.append(poly_rmse)\n    \n    # Cross-validation of degree\n    if min_rmse &gt; poly_rmse:\n        min_rmse = poly_rmse\n        min_deg = deg\n\n# Plot and present results\nprint('Best degree {} with RMSE {}'.format(min_deg, min_rmse))\n        \nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(degrees, rmses)\nax.set_yscale('log')\nax.set_xlabel('Degree')\nax.set_ylabel('RMSE')\n"
'import clips\nclips.Reset()\n\nuser = True\n\ndef py_getvar(k):\n    return (clips.Symbol(\'TRUE\') if globals().get(k) else clips.Symbol(\'FALSE\'))\n\nclips.RegisterPythonFunction(py_getvar)\n\n# if globals().get(\'user\') is not None: assert something\nclips.BuildRule("user-rule", "(test (eq (python-call py_getvar user) TRUE))",\n                \'(assert (user-present))\',\n                "the user rule")\n\nclips.Run()\nclips.PrintFacts()\n'
'    currency = r"((USD)|(GBP)(...))"\n\n    numbers = r"([0-9]+[0-9\\.,]*)"\n\n    matcher = re.compile(numbers+r"[\\s]*+"currency)\n\n    matcher2 = re.compile(currency+r"[\\s]*"+numbers)\n\n    curren = m.group(1)\n    amount = m.group(2)\n'
'    import numpy as np\n\n    from sklearn import svm\n    X = np.array([[0, 0], [1, 1]])\n    y = [0, 1]\n    clf = svm.SVC(kernel=\'precomputed\')\n\n    # kernel computation\n    K = numpy.zeros(shape = (n, n))\n\n    # "At the moment, the kernel values between all training vectors \n    #  and the test vectors must be provided." \n    #  according to scikit learn web page. \n    #  -- This is the problem!\n    # v1: array, shape (n, d)\n    # w1: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(v1, v1)\n    mu = 1.0 / numpy.mean(chi)\n    K += w1 * numpy.exp(-mu * chi)\n\n    # v2: array, shape (n, d)\n    # w2: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(v2, v2)\n    mu = 1.0 / numpy.mean(chi)\n    K += w2 * numpy.exp(-mu * chi)\n\n    # v3: array, shape (n, d)\n    # w3: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(v3, v3)\n    mu = 1.0 / numpy.mean(chi)\n    K += w3 * numpy.exp(-mu * chi)\n\n    # v4: array, shape (n, d)\n    # w4: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(v4, v4)\n    mu = 1.0 / numpy.mean(chi)\n    K += w4 * numpy.exp(-mu * chi)\n\n    # scikit-learn is a wrapper LIBSVM and looking at the LIBSVM Readme file\n    # it seems you need kernel values for test data something like this:    \n\n    Kt = numpy.zeros(shape = (nt, n))\n    # t1: array, shape (nt, d)\n    # w1: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(t1, v1)\n    mu = 1.0 / numpy.mean(chi)\n    Kt += w1 * numpy.exp(-mu * chi)\n\n    # v2: array, shape (n, d)\n    # w2: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(t2, v2)\n    mu = 1.0 / numpy.mean(chi)\n    Kt += w2 * numpy.exp(-mu * chi)\n\n    # v3: array, shape (n, d)\n    # w3: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(t3, v3)\n    mu = 1.0 / numpy.mean(chi)\n    Kt += w3 * numpy.exp(-mu * chi)\n\n    # v4: array, shape (n, d)\n    # w4: float in [0, 1)\n    chi = sklearn.metrics.pairwise.chi2_kernel(t4, v4)\n    mu = 1.0 / numpy.mean(chi)\n    Kt += w4 * numpy.exp(-mu * chi)\n\n    clf.fit(K, y) \n\n    # predict on testing examples\n    probas_ = clf.predict_proba(Kt)\n'
'In [1]: import networkx as nx\n\nIn [2]: G = nx.Graph([(1,2),(1,3),(2,3)])\n\nIn [3]: G.add_edges_from([(10,20),(10,30),(20,30)])\n\nIn [4]: nx.k_core(G,k=2).edges()\nOut[4]: [(1, 2), (1, 3), (2, 3), (10, 20), (10, 30), (20, 30)]\n\nIn [5]: graphs = nx.connected_component_subgraphs(nx.k_core(G,k=2))\n\nIn [6]: for g in graphs:\n   ...:     print g.edges()\n   ...:        \n[(1, 2), (1, 3), (2, 3)]\n[(10, 20), (10, 30), (20, 30)]\n'
'# Logistic loss is the negative of the log of the logistic function.\nout = -np.sum(sample_weight * log_logistic(yz)) + .5 * alpha * np.dot(w, w)\n'
"&gt;&gt;&gt; training_data = [[('this', 'is'), ('is', 'a'),('a', 'text'), 'POS'],\n                 [('and', 'one'), ('one', 'more'), 'NEG'],\n                 [('and', 'other'), ('one', 'more'), 'NEU']]\n&gt;&gt;&gt; count_vect = CountVectorizer(preprocessor=lambda x:x,\n                                 tokenizer=lambda x:x)\n&gt;&gt;&gt; X = count_vect.fit_transform(doc[:-1] for doc in training_data)\n\n&gt;&gt;&gt; print count_vect.vocabulary_\n{('and', 'one'): 1, ('a', 'text'): 0, ('is', 'a'): 3, ('and', 'other'): 2, ('this', 'is'): 5, ('one', 'more'): 4}\n&gt;&gt;&gt; print X.toarray()\n[[1 0 0 1 0 1]\n [0 1 0 0 1 0]\n [0 0 1 0 1 0]]\n\ny = [doc[-1] for doc in training_data] # ['POS', 'NEG', 'NEU']\n\nmodel = SVC()\nmodel.fit(X, y)\n"
"print 'Neutral: ', prob_s[0,1]\nprint 'Smiling: ', prob_s[0,3]\nprint 'Shocked: ', prob_s[0,2]\nprint 'Angry: ', prob_s[0,0]\n"
'model = LogisticRegression(C=100000, fit_intercept=False)\n\nmodel = LogisticRegression(C=1000000)\n\nIntercept -2.038853 # this is actually half the intercept\nstudy_hrs  1.504643 # this is correct\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\n\nX = [0.5,0.75,1.0,1.25,1.5,1.75,1.75,2.0,2.25,2.5,2.75,3.0,3.25,\n3.5,4.0,4.25,4.5,4.75,5.0,5.5]\ny = [0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1]\n\nX = np.array([[x] for x in X])\ny = np.ravel(y)\n\nmodel = LogisticRegression(C=1000000.)\nmodel = model.fit(X,y)\n\nprint(\'coef\', model.coef_)\nprint(\'intercept\', model.intercept_)\n\ncoef [[ 1.50464059]]\nintercept [-4.07769916]\n\nX = [0.5,0.75,1.0,1.25,1.5,1.75,1.75,2.0,2.25,2.5,2.75,3.0,3.25,\n3.5,4.0,4.25,4.5,4.75,5.0,5.5]\ny = [0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1]\n\nzipped = list(zip(X,y))\ndf = pd.DataFrame(zipped,columns = [\'study_hrs\',\'p_or_f\'])\n\ny, X = dmatrices(\'p_or_f ~ study_hrs\',\n                  df, return_type="dataframe")\n\nprint(X)\n\n    Intercept  study_hrs\n0           1       0.50\n1           1       0.75\n2           1       1.00\n3           1       1.25\n4           1       1.50\n5           1       1.75\n6           1       1.75\n7           1       2.00\n8           1       2.25\n9           1       2.50\n10          1       2.75\n11          1       3.00\n12          1       3.25\n13          1       3.50\n14          1       4.00\n15          1       4.25\n16          1       4.50\n17          1       4.75\n18          1       5.00\n19          1       5.50\n\nimport numpy as np\nimport pandas as pd\nfrom patsy import dmatrices\nfrom sklearn.linear_model import LogisticRegression\n\nX = [0.5,0.75,1.0,1.25,1.5,1.75,1.75,2.0,2.25,2.5,2.75,3.0,3.25,\n3.5,4.0,4.25,4.5,4.75,5.0,5.5]\ny = [0,0,0,0,0,0,1,0,1,0,1,0,1,0,1,1,1,1,1,1]\n\nzipped = list(zip(X,y))\ndf = pd.DataFrame(zipped,columns = [\'study_hrs\',\'p_or_f\'])\n\ny, X = dmatrices(\'p_or_f ~ study_hrs\',\n                  df, return_type="dataframe")\n\ny = np.ravel(y)\n\nmodel = LogisticRegression(C=100000, fit_intercept=False)\nmodel = model.fit(X,y)\nprint(pd.DataFrame(np.transpose(model.coef_),X.columns))\n\nIntercept -4.077571\nstudy_hrs  1.504597\n'
"tuned_parameters = {'alpha': [10 ** a for a in range(-6, -2)]}\nclf = GridSearchCV(SGDClassifier(loss='hinge', penalty='elasticnet',l1_ratio=0.15, n_iter=5, shuffle=True, verbose=False, n_jobs=10, average=False, class_weight='balanced')\n                  , tuned_parameters, cv=10, scoring='f1_macro')\n\n#now clf is the best classifier found given the search space\nclf.fit(X_train, Y_train)\n#you can find the best alpha here\nprint(clf.best_params_)    \n\nfor i in range(0, clf.best_estimator_.coef_.shape[0]):\n    top10 = np.argsort(clf.best_estimator_.coef_[i])[-10:]\n"
' scores &lt;- predict(results.rf, X, type="prob",\n    norm.votes=FALSE, predict.all=FALSE, proximity=FALSE, nodes=FALSE)\n'
'return out\n\nreturn dense2, out\n\npred = alex_net(x, weights, biases, keep_prob)\n\nbefore_classification_layer, pred = alex_net(...)\n\n# Calculate batch accuracy and loss\nacc, loss = sess.run([accuracy, cost], feed_dict={...})\n\n# Calculate batch accuracy\nacc = sess.run(accuracy, feed_dict={...})\n# Calculate batch loss\nloss = sess.run(cost, feed_dict={...})\n'
'np.exp(y/np.sum(np.exp(y),axis=1))\n\nnp.exp(y)/np.sum(np.exp(y),axis=1, keepdims=True)\n'
"joblib.dump(pipeline.steps[0][1].transformer_list[0][1], dump_path)\n\njoblib.dump(pipeline.get_params()['features__tfidf'], dump_path)\n\npipeline.steps[0][1].transformer_list[0][1] = joblib.load(dump_path)\n"
"df = df.groupby(pd.TimeGrouper('M')).fillna(method='ffill')\ndf = df.resample(rule='M', how='last')\n"
'df = pd.read_csv(\'swarm.csv\', header=[0, 1], tupleize_cols=True, index_col=None)\ncols = [\'sizes\', \'distance_measure\']\ndf.columns = pd.MultiIndex.from_tuples(df.columns, names=cols)\n\nsizes                  1              2\ndistance_measure       0      1       0\n0                -2.1881  1.262 -2.7001\n1                -2.3671  1.699 -2.4431\n2                -2.3071  0.716 -2.2841\n3                -2.2521  0.967 -1.9451\n4                -2.4651  1.800 -2.3421\n\ndf = df.stack(cols).reset_index(cols).rename(columns={0: \'value\'})\ndf.info()\n\nInt64Index: 30 entries, 0 to 9\nData columns (total 3 columns):\nsizes               30 non-null object\ndistance_measure    30 non-null object\nvalue               30 non-null float64\n\ndf.head()\n\n  sizes distance_measure   value\n0     1                0 -2.1881\n0     1                1  1.2620\n0     2                0 -2.7001\n1     1                0 -2.3671\n1     1                1  1.6990\n\nax = sns.swarmplot(x="sizes", y=\'value\', hue="distance_measure", data=df, split=True)\nplt.show()\n'
'loss1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction1, labels_1))\nloss2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction2, labels_2))\n\nloss = loss1 + loss2\n'
'def custom_normilze_generator(directory, mean):\n    for img in flow_from_directory(directory):\n        yield (img - mean)\n'
'import sys\nimport tensorflow as tf\nfrom tensorflow.contrib.session_bundle import exporter\n\nimport numpy as np\nfrom newpreprocess import create_feature_sets_and_labels\nimport json \nimport os \n\ntrain_x,train_y,test_x,test_y = create_feature_sets_and_labels()\n\nn_nodes_hl1 = 20\nn_nodes_hl2 = 20\nn_classes = 1\nbatch_size = 100\n\nx = tf.placeholder(\'float\', [None, 13])\ny = tf.placeholder(\'float\', [None, 1])\nkeys_placeholder = tf.placeholder(tf.int64, shape=(None,))\n\nkeys = tf.identity(keys_placeholder)\n\ndef neural_network_model(data):\n    hidden_1_layer = {\'weights\':tf.Variable(tf.random_normal([13, n_nodes_hl1])),\n                      \'biases\':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n    hidden_2_layer = {\'weights\':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n                      \'biases\':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n    output_layer = {\'weights\':tf.Variable(tf.random_normal([n_nodes_hl2, n_classes])),\n                    \'biases\':tf.Variable(tf.random_normal([n_classes]))}\n    l1 = tf.add(tf.matmul(data, hidden_1_layer[\'weights\']), hidden_1_layer[\'biases\'])\n    l1 = tf.tanh(l1)\n    l2 = tf.add(tf.matmul(l1, hidden_2_layer[\'weights\']), hidden_2_layer[\'biases\'])\n    l2 = tf.tanh(l2)\n    output = tf.add(tf.matmul(l2, output_layer[\'weights\']), output_layer[\'biases\'])\n    return output\n\noutput = neural_network_model(x)\nprediction = tf.sigmoid(output)\npredicted_class = tf.greater(prediction,0.5)\n\n\ninputs = {\'key\': keys_placeholder.name, \'x\': x.name}\ntf.add_to_collection(\'inputs\', json.dumps(inputs))\n\noutputs = {\'key\': keys.name,\n           \'prediction\': predicted_class.name}\ntf.add_to_collection(\'outputs\', json.dumps(outputs))\n\n\ndef train_neural_network(x):\n    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(output, y))\n    optimizer = tf.train.AdamOptimizer(0.003).minimize(cost)\n    hm_epochs = 700\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        for epoch in range(hm_epochs):\n            epoch_loss = 0\n            i = 0\n            while i &lt; len(train_x):\n                start = i\n                end = i + batch_size\n                batch_x = np.array(train_x[start:end])\n                batch_y = np.array(train_y[start:end])\n\n                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                              y: batch_y})\n                epoch_loss += c\n                i+=batch_size\n            print(\'Epoch\', epoch, \'completed out of\', hm_epochs, \'loss:\', epoch_loss/(len(train_x)/batch_size))\n\n        correct = tf.equal(predicted_class, tf.equal(y,1.0))\n        accuracy = tf.reduce_mean( tf.cast(correct, \'float\') )\n        print(\'Accuracy:\', accuracy.eval({x: test_x, y: test_y}))\n\n        export_path = "~/Documents/cloudcomputing/Project/RNN_timeseries/"\n        print ("Exporting trained model to %s", %export_path)\n        init_op = tf.group(tf.initialize_all_tables(), name="init_op")\n\n        saver = tf.train.Saver(sharded = True)\n        saver.save(sess, os.path.join(export_path, \'export\'))\n\n        print("Done exporting!")\n\ntrain_neural_network(x)\n'
"# Assume you want to save 2 variables `v1` and `v2`\nsaver = tf.train.Saver([v1, v2])\n\nsaver.save(sess, 'filename');\n"
"def inception2d(x, in_channels, filter_count):\n    # bias dimension = 3*filter_count and then the extra in_channels for the avg pooling\n    bias = tf.Variable(tf.truncated_normal([3*filter_count + in_channels], mu, sigma)),\n\n    # 1x1\n    one_filter = tf.Variable(tf.truncated_normal([1, 1, in_channels, filter_count], mu, sigma))\n    one_by_one = tf.nn.conv2d(x, one_filter, strides=[1, 1, 1, 1], padding='SAME')\n\n    # 3x3\n    three_filter = tf.Variable(tf.truncated_normal([3, 3, in_channels, filter_count], mu, sigma))\n    three_by_three = tf.nn.conv2d(x, three_filter, strides=[1, 1, 1, 1], padding='SAME')\n\n    # 5x5\n    five_filter = tf.Variable(tf.truncated_normal([5, 5, in_channels, filter_count], mu, sigma))\n    five_by_five = tf.nn.conv2d(x, five_filter, strides=[1, 1, 1, 1], padding='SAME')\n\n    # avg pooling\n    pooling = tf.nn.avg_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n\n    x = tf.concat([one_by_one, three_by_three, five_by_five, pooling], axis=3)  # Concat in the 4th dim to stack\n    x = tf.nn.bias_add(x, bias)\n    return tf.nn.relu(x)\n"
"import numpy as np\n\ndef closestN(X_array, n):\n    # array of sample distances to the hyperplane\n    dists = clf.decision_function(X_array)\n    # absolute distance to hyperplane\n    absdists = np.abs(dists)\n\n    return absdists.argsort()[:n]\n\nclosest_samples = closestN(X, 5)\nplt.scatter(X[closest_samples][:, 0], X[closest_samples][:, 1], color='yellow')\n\nclosestN(X, 5)\narray([ 1, 20, 14, 31, 24])\n\nX[closestN(X, 5)]\narray([[-1.02126202,  0.2408932 ],\n       [ 0.95144703,  0.57998206],\n       [-0.46722079, -0.53064123],\n       [ 1.18685372,  0.2737174 ],\n       [ 0.38610215,  1.78725972]])\n"
'def predict_proba(self, x, batch_size=32, verbose=1):\n        """Generates class probability predictions for the input samples\n        batch by batch.\n        # Arguments\n            x: input data, as a Numpy array or list of Numpy arrays\n                (if the model has multiple inputs).\n            batch_size: integer.\n            verbose: verbosity mode, 0 or 1.\n        # Returns\n            A Numpy array of probability predictions.\n        """\n        preds = self.predict(x, batch_size, verbose)\n        if preds.min() &lt; 0. or preds.max() &gt; 1.:\n            warnings.warn(\'Network returning invalid probability values. \'\n                          \'The last layer might not normalize predictions \'\n                          \'into probabilities \'\n                          \'(like softmax or sigmoid would).\')\n        return preds\n'
'X = ... your data\ny = ... your targets\nW0 = ... target weights\nalpha = ... pulling strength \nlr = ... learning rate (step size of gradient descent)\n\n\nfrom autograd import numpy as np\nfrom autograd import grad\n\n\ndef your_loss(W):\n  return np.mean((np.dot(X, W) - y)**2) + alpha * np.sum(np.abs(W - W0))\n\ng = grad(your_loss)\n\nW = np.random.normal(size=(X.shape[1], 1))\nfor i in range(100):\n   W = W - lr * g(W)\n\nprint(W) \n'
"#put below code inside your NUM_TRIALS for loop\ncv_iter = 0\ntemp_nested_scores_train = np.zeros(4)\ntemp_nested_scores_test = np.zeros(4)\nfor train, test in outer_cv.split(X_iris):\n    clf.fit(X_iris[train], y_iris[train])\n    temp_nested_scores_train[cv_iter] = clf.best_score_\n    temp_nested_scores_test[cv_iter] = clf.score(X_iris[test], y_iris[test])\n    #You can access grid search's params here\nnested_scores_train[i] = temp_nested_scores_train.mean()\nnested_scores_test[i] = temp_nested_scores_test.mean()\n"
"logits, probabilities, predictions, labels = sess.run([logits, probabilities, predictions, labels])\nprint 'logits: \\n', logits\nprint 'Probabilities: \\n', probabilities\nprint 'predictions: \\n', predictions\nprint 'Labels:\\n:', labels\n\nlogits = tf.Print(logits, [logits], message = 'logits: \\n', summarize = 100)\n"
'summary(lm(as.numeric(Species)~., iris))[c(\'coefficients\', \'r.squared\')]\n\n$coefficients\n                Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   1.18649525 0.20484104  5.792273 4.150495e-08\nSepal.Length -0.11190585 0.05764674 -1.941235 5.416918e-02\nSepal.Width  -0.04007949 0.05968881 -0.671474 5.029869e-01\nPetal.Length  0.22864503 0.05685036  4.021874 9.255215e-05\nPetal.Width   0.60925205 0.09445750  6.450013 1.564180e-09\n\n$r.squared\n[1] 0.9303939\n\nfrom sklearn.datasets import load_iris\nimport pandas as pd\nfrom patsy import dmatrices\n\niris = load_iris()\nnames = [f_name.replace(" ", "_").strip("_(cm)") for f_name in iris.feature_names]\niris_df = pd.DataFrame(iris.data, columns=names)\niris_df[\'species\'] = iris.target\n\n# pasty does not support \'.\' at least in windows python 2.7, so here is the workaround \ny, X = dmatrices(\'species ~ \' + \'+\'.join(iris_df.columns - [\'species\']),\n                  iris_df, return_type="dataframe")\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint model.score(X,y)\n# 0.930422367533\n\nprint model.intercept_, model.coef_\n# [ 0.19208399] [[0.22700138  0.60989412 -0.10974146 -0.04424045]]\n'
"import numpy as np\nfrom skimage import io\nfrom skimage.util import view_as_windows\n\nimg = io.imread('image_name.png')    \nwindow_shape = (25, 25)\n\nwindows = view_as_windows(img, window_shape)    \nn_windows = np.prod(windows.shape[:2])\nn_pixels = np.prod(windows.shape[2:])\n\nx_test = windows.reshape(n_windows, n_pixels)\n\nclf.apply(x_test)\n"
'class TransformerWrapper(sklearn.base.BaseEstimator):\n\n    def __init__(self, func):\n        self._func = func\n\n    def fit(self, *args, **kwargs):\n        return self\n\n    def transform(self, X, *args, **kwargs):\n        return self._func(X, *args, **kwargs)\n\n@TransformerWrapper\ndef foo(x):\n  return x*2\n\ndef foo(x):\n  return x*2\n\nfoo = TransformerWrapper(foo)\n\nfrom sklearn.preprocessing import FunctionTransformer\n\n@FunctionTransformer\ndef foo(x):\n  return x*2\n'
'delta = 1\nN = X.shape[0]\nM = W.shape[1]\nscoresv = X.dot(W)\nmarginv = scoresv - scoresv[np.arange(N), y][:,None] + delta\n\nmask0 = np.zeros((N,M),dtype=bool)\nmask0[np.arange(N),y] = 1\nmask = (marginv&lt;0) | mask0\nmarginv[mask] = 0\n\nloss_out = marginv.sum()/num_train # mean\nloss_out += 0.5 * reg * np.sum(W * W) # l2 regularization\n\nfloat(np.tensordot(W,W,axes=((0,1),(0,1))))\n\ndef svm_loss_vectorized_v2(W, X, y, reg):\n    delta = 1\n    N = X.shape[0]\n    M = W.shape[1]\n    scoresv = X.dot(W)\n    marginv = scoresv - scoresv[np.arange(N), y][:,None] + delta\n\n    mask0 = np.zeros((N,M),dtype=bool)\n    mask0[np.arange(N),y] = 1\n    mask = (marginv&lt;=0) | mask0\n    marginv[mask] = 0\n\n    loss_out = marginv.sum()/num_train # mean\n    loss_out += 0.5 * reg * float(np.tensordot(W,W,axes=((0,1),(0,1))))\n    return loss_out\n\nIn [86]: W= np.random.randn(3073,10)\n    ...: X= np.random.randn(500,3073)\n    ...: y= np.random.randint(0,10,(500))\n    ...: reg = 4.56\n    ...: \n\nIn [87]: svm_loss_naive(W, X, y, reg)\nOut[87]: 70380.938069371899\n\nIn [88]: svm_loss_vectorized_v2(W, X, y, reg)\nOut[88]: 70380.938069371914\n\nIn [89]: %timeit svm_loss_naive(W, X, y, reg)\n100 loops, best of 3: 10.2 ms per loop\n\nIn [90]: %timeit svm_loss_vectorized_v2(W, X, y, reg)\n100 loops, best of 3: 2.94 ms per loop\n'
'x_history = tf.reshape(x, [-1, 290, 290, 1])\n\nx_history.eval(feed_dict={x:batch[0], y_:batch[1], keep_prob:1.0}).shape\n\n=&gt; (1, 290, 290, 1)\n'
"for area in buckets:\n    area_docs = []\n    for value in buckets[area]:   \n        if 'filename_%s.txt' % value in os.listdir(directory):\n            fin = open(directory+'/filename_%s.txt' % value, 'r').read() \n            area_docs.append(fin)\n            buckets[area] = area_docs\n\ncorpus = list(buckets.values()) # Get your list of lists of strings\ncorpus = sum(corpus, []) # Good trick for flattening 2D lists to 1D\nvectorizer = TfidfVectorizer(min_df=1, stop_words='english')\nX = vectorizer.fit_transform(corpus)\nidf = vectorizer.idf_\nd = dict(zip(vectorizer.get_feature_names(), idf))\nsorted_d = sorted(d.items(), key=operator.itemgetter(1))\nsorted_d[:50]\n"
'def my_score(X, y):\n    return mutual_info_regression(X, y, random_state=0)\n\nSelectKBest(score_func=my_score)\n\nfrom functools import partial\nmy_score = partial(mutual_info_regression, random_state=0)\nSelectKBest(score_func=my_score)\n\nfrom functools import partial\nSelectKBest(score_func=partial(mutual_info_classif, random_state=0))\n'
'from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder()\nX_hotEncoded = enc.fit_transform(X)\n\njoblib.dump(enc, "encoder.pkl")\n\nenc = joblib.load("encoder.pkl")\nX_hotEncoded = enc.transform(X)\n'
"import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.cluster import DBSCAN\n\nX, y = make_classification()\nmetric = DistanceMetric.get_metric('mahalanobis', V=np.cov(X))\n\nsklearn.cluster.DBSCAN(eps=0.15, min_samples=8, metric=metric, \n                       algorithm='brute', leaf_size=30, n_jobs=-1)\n"
'NNmodelList.append(nn_model.fit(i,j))\n\nNNmodelList.append(nn_model)\nnn_model.fit(i,j)\n'
'export PYTHONPATH=~/xgboost/python-package\n'
"in -&gt; A -&gt; B -&gt; C -&gt; D -&gt; E -&gt; out\n\nbottleneck_in -&gt; E' -&gt; out\n\nin -&gt; A -&gt; B -&gt; C -&gt; D -&gt; E' -&gt; out\n"
'from sklearn.preprocessing import scale\ny = scale(y)\n\nimport numpy as np\ny = np.array(y).reshape(-1,1)\ny = sc_y.fit_transform(y)\n'
'x_test = onehotencoder.transform(x_test)\nx_pred = regressor.predict(x_test)\n'
"K.cast_to_floatx(K.int_shape(K.flatten(d_i))[0]\n\ndef get_log_rmse(normalization_constant):\n    def log_rmse(y_true, y_pred):\n        d_i = (K.log(y_pred) -  K.log(y_true))\n        loss1 = K.mean(K.square(d_i))\n        loss2 = K.square(\n            K.sum(\n                K.flatten(d_i),axis=-1))/(K.cast_to_floatx(\n                    2 * normalization_constant ** 2) \n        loss = loss1 - loss2\n        return loss\n    return log_rmse\n\nmodel.compile(..., loss=get_log_rmse(normalization_constant))\n\ndef get_log_rmse(normalization_constant):\n    def log_rmse(y_true, y_pred):\n        d_i = (K.log(y_pred) -  K.log(y_true))\n        loss1 = K.mean(K.square(d_i))\n        loss2 = K.square(K.sum(K.flatten(d_i),axis=-1))/K.cast_to_floatx(2 * normalization_constant ** 2)\n        loss = loss1 - loss2\n        return loss\n    return log_rmse\n\nmodel.compile(optimizer='adam', loss=get_log_rmse(batch_size))\n\ninput_shape = (160, 256, 3)\nprint('Input_shape: %s'%str(input_shape))\nbase_model = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet', \n                               input_tensor=None, input_shape=input_shape, \n                               pooling=None, # None, 'avg', 'max'\n                               classes=1000)\nfor i in range(5):\n    base_model.layers.pop()\nbase_model = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output)\nprint('VGG19 output_shape: ' + str(base_model.output_shape))\n\nx = Deconv(128, kernel_size=(4, 4), strides=1, padding='same', activation='relu')(base_model.output)\nx = UpSampling2D((2, 2))(x)\nx = Deconv(64, kernel_size=(4, 4), strides=1, padding='same', activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\nx = Deconv(32, kernel_size=(4, 4), strides=1, padding='same', activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\nx = Deconv(16, kernel_size=(4, 4), strides=1, padding='same', activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(1, kernel_size=(5, 5), strides=1, padding='same')(x)\nmodel = Model(inputs=base_model.input, outputs=x)\n"
'In [34]: chk_set = set([\'PRP\',\'MD\',\'NN\'])\n\nIn [35]: chk_set.issubset(t.tag_ for t in nlp("I will go to the mall"))\nOut[35]: True\n\nIn [36]: chk_set.issubset(t.tag_ for t in nlp("I will go"))\nOut[36]: False\n\nIn [53]: [t.text for t in nlp("I will go to the mall") if t.tag_ in [\'NN\']]\nOut[53]: [\'mall\']\n'
"Dense(64, kernel_initializer='random_uniform', bias_initializer='zeros')\n"
'# df = df.replace(\'None\', np.nan) # optional step, if `None` is a string\ndf \n\n                               ID Error1 Error2\nTime                                           \n2010-01-01 00:00:31.690  105278.0    NaN      5\n2010-01-01 00:00:32.000  105278.0      1    NaN\n2010-01-01 00:00:32.140  105278.0      3    NaN\n2010-01-01 00:00:32.350  105278.0    NaN      7\n2010-01-01 00:00:32.460  105278.0    NaN      1\n\npd.get_dummies(\n     df.set_index(\'ID\', append=True), prefix=\'\', prefix_sep=\'\')\\\n  .add_prefix("Error")\\\n  .reset_index()\n\n                      Time        ID  Error1  Error3  Error1  Error5  Error7\n0  2010-01-01 00:00:31.690  105278.0       0       0       0       1       0\n1  2010-01-01 00:00:32.000  105278.0       1       0       0       0       0\n2  2010-01-01 00:00:32.140  105278.0       0       1       0       0       0\n3  2010-01-01 00:00:32.350  105278.0       0       0       0       0       1\n4  2010-01-01 00:00:32.460  105278.0       0       0       1       0       0\n'
"def confusion(y_true, y_pred):\n    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n    y_pos = K.round(K.clip(y_true, 0, 1))\n    y_neg = 1 - y_pos\n    tp = K.sum(y_pos * y_pred_pos) / K.sum(y_pos)\n    tn = K.sum(y_neg * y_pred_neg) / K.sum(y_neg)\n    return {'true_pos': tp, 'true_neg': tn}\n"
'#df["Distance"] = abs(y - model.predict(x))  # if you only want magnitude\ndf["Distance"] = y - model.predict(x)\nprint(df)\n#   Exam Results  Hours Studied  Distance\n#0            93       8.232795 -0.478739\n#1            94       7.879095  1.198511\n#2            92       6.972698  0.934043\n#3            88       6.854017 -2.838712\n#4            91       6.043066  1.714063\n#5            87       5.510013 -1.265269\n#6            89       5.509297  0.736102\n'
'import loompy as lp\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nmodel_input_name = ""\ninput_size = 10000\nbatch_size = 32\nepochs = 10\n\n# Input functions for train, test and eval sets.\ndef train_input_fn():\n    return _input_fn(\'TRAIN\')\n\ndef test_input_fn():\n    return _input_fn(\'TEST\')\n\ndef eval_input_fn():\n    return _input_fn(\'EVAL\')\n\n# General purpose input function\ndef _input_fn(mode = \'TRAIN\'):\n\n    """\n        Arguments\n            mode : \'TRAIN\', \'TEST\', \'EVAL\'\n    """\n\n    # A generator to yield data and labels from the given FILE,\n    # based on the indices assigned to the "indices" variable.\n    # If you change the labels, remember to update the from_generator()\n    # parameters below, to reflect their datatype.\n    def gen():\n        with lp.connect(FILE, \'r\') as ds:\n            if ae:\n                for i in indices:\n                    yield {model_input_name: ds[:, i]}, ds[:, i]\n            else:\n                for i in indices:\n                    yield {model_input_name: ds[:, i]}, ds.ca.x_CellType[i]\n\n    # Get the indices for train, test and eval sets\n    train_idx, test_idx, eval_idx = train_test_set_idx_split(TRAIN_RT, TEST_RT, EVAL_RT)\n\n    # Check condition and assign the respective set to the "indices" variable\n    if mode == \'TRAIN\':\n        indices = train_idx\n    elif mode == \'TEST\':\n        indices = test_idx\n    elif mode == \'EVAL\':\n        indices = eval_idx\n    else:\n        print("Wrong mode choice: ", mode)\n        exit(1)\n\n    dataset = tf.data.Dataset.from_generator(gen, ({model_input_name: tf.int64}, tf.int64),\n                                             output_shapes=({model_input_name: [input_size,]}, []))\n\n    # Shuffle, batch, map, prefetch and repeat your dataset.\n    # If you need to do some preprocessing on the data, create your function on\n    # the cell above, and call it within a map() function.\n\n    dataset = dataset.shuffle(buffer_size=batch_size*50)\n    dataset = dataset.batch(batch_size)\n\n    dataset = dataset.map(_reshape_labels)\n    dataset = dataset.map(_int2float)\n\n    # Map on whatever other functions you need\n    dataset = dataset.map( ... )\n\n    dataset = dataset.prefetch(2)\n    dataset = dataset.repeat(epochs)\n\n    iterator = dataset.make_one_shot_iterator()\n\n    return iterator.get_next()\n\n\n# Get train, test, eval indices for the given dataset\ndef train_test_set_idx_split(train_rt, test_rt, eval_rt):\n    """ This function returns indices for the train, test and evaluation sets,\n        given an input Dataset.\n        Arguments:\n            train_rt: ratio of the train dataset\n            test_rt:  ratio of the test dataset\n            eval_rt:  ratio of the evaluation dataset\n\n        Returns:\n            train_idx: indices (of the given dataset) for the train dataset\n            test_idx:  indices (of the given dataset) for the test dataset\n            evel_idx:  indices (of the given dataset) for the evaluation dataset\n\n        Note:\n            This function will work correctly as long as (test_rt == evel_rt) is True.\n            If you need (test_rt != evel_rt), you need something more sophisticated.\n    """\n\n    with lp.connect(FILE, \'r\') as ds:\n        idx = np.array(range(0, ds.shape[1]))\n\n    train_idx, test_idx = train_test_split(idx, train_size=train_rt, test_size=test_rt+eval_rt)\n    test_idx, eval_idx = train_test_split(test_idx, train_size=0.5, test_size=0.5)\n\n    return train_idx, test_idx, eval_idx\n\n# Reshape labels as needed\ndef _reshape_labels(data, labels):\n    return data, tf.reshape(labels, (-1,1))\n'
'preds = model.predict(data)\nclass_one = preds &gt; 0.5\n\nacc = np.mean(class_one == true_labels)\n\npred_labels = model.predict_classes(data)\n'
'import numpy as np\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\n# get the max class with respect to the number of elements\nmax_class = np.max(np.bincount(y))\n# you can add other parameters after doing your homework research\n# for example, you can add \'algorithm\' : [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']\ngrid_param = {\'n_neighbors\': range(1, max_class)}\nmodel = KNeighborsClassifier()\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=2)\nclf = GridSearchCV(model, grid_param, cv=cv, scoring=\'accuracy\')\nclf.fit(X, y)\nprint("Best Estimator: \\n{}\\n".format(clf.best_estimator_))\nprint("Best Parameters: \\n{}\\n".format(clf.best_params_))\nprint("Best Score: \\n{}\\n".format(clf.best_score_))\n\nBest Estimator: \nKNeighborsClassifier(algorithm=\'auto\', leaf_size=30, metric=\'minkowski\',\n           metric_params=None, n_jobs=1, n_neighbors=17, p=2,\n           weights=\'uniform\')\n\nBest Parameters: \n{\'n_neighbors\': 17}\n\nBest Score: \n0.98\n'
'from sklearn.linear_model import SGDClassifier  \n\nX = [[0., 0.], [1., 1.]]\ny = [0, 1]\nclf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)\nclf.fit(X, y) \n\nclf.feature_count_\n[...]\nAttributeError: \'SGDClassifier\' object has no attribute \'feature_count_\'\n\nclf.classes_\n# array([0, 1])\n'
'in1 = Input((400, 200))\nin2 = Input((400, 200, 10))\n\nr_in1 = Reshape((400, 200, 1))(in1) # you can also use `K.expand_dims()` in a Lambda layer\nconcat = concatenate([r_in1, in2])\n\nout_model = TimeDistributed(Lambda(lambda x: model([x[:,:,0], x[:,:,1:]])))(concat)\n\nnew_model = Model([in1, in2], [out_model])\n'
'x_train = x_train.values/255\nx_test = x_test.values/255\n'
'y_pred_proba = CV_rfc.predict_proba(x_test)\nprint(roc_auc_score(y_test, y_pred_proba[:,1]))\n'
'pad_idx = TGT.vocab.stoi["&lt;blank&gt;"]\nmodel = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\nmodel = model.to(device)\ncriterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\ncriterion = criterion.to(device)\nBATCH_SIZE = 12000\ntrain_iter = MyIterator(train, batch_size=BATCH_SIZE, device = torch.device(\'cuda\'),\n                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n                        batch_size_fn=batch_size_fn, train=True)\nvalid_iter = MyIterator(val, batch_size=BATCH_SIZE, device = torch.device(\'cuda\'),\n                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n                        batch_size_fn=batch_size_fn, train=False)\n'
"numerical['Preds'] = pred\nnumerical.to_csv('report.csv')\n"
'import numpy as np\nfrom sklearn.linear_model import LinearRegression \n\na = np.array([[5,8],[12,24],[19,11],[10,15]])\n\n## weights\nw = np.array([0.2, 0.5])\n\n## bias  \nb = 0.1  \n\ny = np.matmul(w, a.T) + b\n\nlr = LinearRegression()\nlr.fit(a, y)\n\nprint(lr.coef_)\n# array([0.2, 0.5])\n\nprint(lr.intercept_)\n# 0.099\n'
"import itertools\nimport numpy as np\n\ncolumn = np.array(list(itertools.zip_longest(*column, fillvalue='UNK'))).T\nprint(column)\n\n[['Adventure' 'Animation' 'Comedy']\n ['Adventure' 'Comedy' 'UNK']\n ['Adventure' 'Children' 'Comedy']]\n\n# if you have big vocabulary list in files, you can use tf.feature_column.categorical_column_with_vocabulary_file\ncat_fc = tf.feature_column.categorical_column_with_vocabulary_list(\n    'cat_data', # identifying the input feature\n    ['Adventure', 'Animation', 'Comedy', 'Children'], # vocabulary list\n    dtype=tf.string,\n    default_value=-1)\n\ncat_column = tf.feature_column.embedding_column(\n    categorical_column =cat_fc,\n    dimension = 5,\n    combiner='mean')\n\ntensor = tf.feature_column.input_layer({'cat_data':column}, [cat_column])\n\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    print(session.run(tensor))\n\n[[-0.694761   -0.0711766   0.05720187  0.01770079 -0.09884425]\n [-0.8362482   0.11640486 -0.01767573 -0.00548441 -0.05738768]\n [-0.71162754 -0.03012567  0.15568805  0.00752804 -0.1422816 ]]\n"
"def Find_Optimal_Cutoff(target, predicted):\n    fpr, tpr, threshold = roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n\n    return list(roc_t['threshold']) \n\nthreshold = Find_Optimal_Cutoff(target_column,predicted_column)\n"
"def fixCategoryId(category_id):\n    return category_id - 1;\n\nwith open(coco_json_path) as f:\n    js = json.load(f)\n    images = js['images']\n    categories = js['categories']\n    annotations = js['annotations']\n    for i in images:\n        jsonFile = i['file_name']\n        jsonFile = jsonFile.split('.')[0] + '.json'\n\n        line = {}\n        line['file'] = i['file_name']\n        line['image_size'] = [{\n            'width': int(i['width']),\n            'height': int(i['height']),\n            'depth': 3\n        }]\n        line['annotations'] = []\n        line['categories'] = []\n        for j in annotations:\n            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:\n                line['annotations'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'top': int(j['bbox'][1]),\n                    'left': int(j['bbox'][0]),\n                    'width': int(j['bbox'][2]),\n                    'height': int(j['bbox'][3])\n                })\n                class_name = ''\n                for k in categories:\n                    if int(j['category_id']) == k['id']:\n                        class_name = str(k['name'])\n                assert class_name is not ''\n                line['categories'].append({\n                    'class_id': fixCategoryId(int(j['category_id'])),\n                    'name': class_name\n                })\n        if line['annotations']:\n            with open(os.path.join(sagemaker_json_path, jsonFile), 'w') as p:\n                json.dump(line, p)\n\nimport json\nimport logging\n\ndef get_coco_mapper():\n    original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20,\n                    21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,\n                    41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,\n                    61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80,\n                    81, 82, 84, 85, 86, 87, 88, 89, 90]\n    iter_counter = 0\n    COCO = {}\n    for orig in original_list:\n        COCO[orig] = iter_counter\n        iter_counter += 1\n    return COCO\n\nod_model.fit(inputs=data_channels, logs=True)\n\n[11/04/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train cross_entropy &lt;loss&gt;=(0.20304460724736212)\n[11/04/2019 09:26:46 INFO 140651482974016] #quality_metric: host=algo-1, epoch=499, batch=11 train smooth_l1 &lt;loss&gt;=(0.06970448779799958)\n"
"results = model_selection.cross_validate(dtree, data1[data1_x_bin],  data1[Target], scoring = 'roc_auc' ... )\n\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(random_state=0), ...)\n"
"X_new = df_new[['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']] #Let's say this is a pandas dataframe\nnew_sale_price = final_model.predict(X_new) #This will return an array\ndf_new['SalePrice'] = new_sale_price #The length will be of equal length so you should have no trouble.\n\ndf_new['SalePrice'] = final_model.predict(X_new) \n"
'from keras.applications.vgg16 import preprocess_input\n...\nresized_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\nprocessedimage = preprocess_input(resized_image)\n'
'import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.metrics import AUC\n\n\ndef dummy_network(input_shape):\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10,\n                                 input_shape=input_shape,\n                                 activation=tf.nn.relu,\n                                 kernel_initializer=\'he_normal\',\n                                 kernel_regularizer=keras.regularizers.l2(l=1e-3)))\n\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(11, activation=\'softmax\'))\n\n    model.compile(optimizer=\'adagrad\',\n                  loss=\'binary_crossentropy\',\n                  metrics=[AUC(name=\'auc\')])\n    return model\n\n\ndef train():\n    CB_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor="val_auc",\n        patience=3,\n        verbose=1,\n        mode="max",\n        min_delta=0.0001,\n        min_lr=1e-6)\n\n    CB_es = tf.keras.callbacks.EarlyStopping(\n        monitor="val_auc",\n        min_delta=0.00001,\n        verbose=1,\n        patience=10,\n        mode="max",\n        restore_best_weights=True)\n    callbacks = [CB_lr, CB_es]\n    y = tf.keras.utils.to_categorical([np.random.randint(0, 11) for _ in range(1000)])\n    x = [np.ones((37, 12, 1)) for _ in range(1000)]\n    dummy_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size=100).repeat()\n    val_dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size=100).repeat()\n    model = dummy_network(input_shape=((37, 12, 1)))\n    model.fit(dummy_dataset, validation_data=val_dataset, epochs=2,\n              steps_per_epoch=len(x) // 100,\n              validation_steps=len(x) // 100, callbacks=callbacks)\n\n\nfor i in range(3):\n    print(f\'\\n\\n **** Loop {i} **** \\n\\n\')\n    train()\n\nTrain for 10 steps, validate for 10 steps\nEpoch 1/2\n 1/10 [==&gt;...........................] - ETA: 6s - loss: 0.3426 - auc: 0.4530\n 7/10 [====================&gt;.........] - ETA: 0s - loss: 0.3318 - auc: 0.4895\n10/10 [==============================] - 1s 117ms/step - loss: 0.3301 - \n                                         auc: 0.4893 - val_loss: 0.3222 - val_auc: 0.5085\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.metrics import AUC\n\nauc = AUC()\n\ndef dummy_network(input_shape):\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10,\n                                 input_shape=input_shape,\n                                 activation=tf.nn.relu,\n                                 kernel_initializer=\'he_normal\',\n                                 kernel_regularizer=keras.regularizers.l2(l=1e-3)))\n\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(11, activation=\'softmax\'))\n    model.compile(optimizer=\'adagrad\',\n                  loss=\'binary_crossentropy\',\n                  metrics=[auc])\n    return model\n\nauc = tf.keras.metrics.AUC()\n\nauc.update_state(np.random.randint(0, 2, 10), np.random.randint(0, 2, 10)) \n\nprint(auc.result())\n\nauc.reset_states()\n\nprint(auc.result())\n\nOut[6]: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.875&gt;  # state updated\n\nOut[8]: &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.0&gt;  # state reset\n'
'def errPDFs(var, eVal, q, bWidth, pts=1000):\n    print(&quot;var:&quot;+var)\n    pdf0 = mpPDF(var[0], q, pts) #theoretical pdf\n    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values) #empirical pdf\n    sse = np.sum((pdf1-pdf0)**2)\n    print(&quot;sse:&quot;+str(sse))\n    return sse \n\n&gt;&gt;&gt; out = minimize(lambda *x: errPDFs(*x), .5, args=(eVal, q, bWidth),bounds= \n ((1E-5, 1-1E-5),))\n var:[0.5]\n sse:743.6200749295413\n var:[0.50000001]\n sse:743.6199819531047\n var:[0.99999]\n sse:289.1462047531385\n ...\n'
'plt.imshow(b.permute(2, 0, 1))\n'
'class_weight = {0:2 , 1:1}\n'
'X, y = make_classification(n_samples=1000, n_features=5, n_informative=5, \n                           n_redundant=0, n_repeated=0, n_classes=2, \n                           shuffle=True, random_state=2020)\n\nX, y = make_classification(n_samples=1000, n_features=5, n_informative=2, \n                           n_redundant=3, n_repeated=0, n_classes=2, \n                           shuffle=True, random_state=2020)\n'
"a,b,c,target\n1,1,1,0\n1,0,1,0\n1,1,0,1\n0,0,1,1\n0,1,1,0\n\ndata = np.genfromtxt('file.csv', skip_header=True)\n\nfeatures = data[:, :3]\ntargets = data[:, 3]   # The last column is identified as the target\n\nfeatures = array([[ 0, 1, 0],\n              [ 1, 1, 0],\n              [ 0, 1, 1],\n              [ 0, 0, 0]])  # shape = ( 4, 3)\n\ntargets = array([ 1, 1, 1, 0])  # shape = ( 4, )\n\n&gt;&gt;&gt; from sklearn.svm import LinearSVC\n&gt;&gt;&gt; linear_svc_model = LinearSVC()\n&gt;&gt;&gt; linear_svc_model.fit(X=features, y=targets) \n"
'import scipy.sparse\n\nX = scipy.sparse.hstack([vectorized, count_vectorized])\n\nmodel.fit(X, y)  # y is optional in some models\n'
"import numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport collections\n\ndef error(m, b, points):\n    totalError = 0\n    for i in range(0, len(points)):\n        totalError += (points[i].y - (m * points[i].x + b)) ** 2\n    return totalError / float(len(points))\n\nx = y = np.arange(-3.0, 3.0, 0.05)\nPoint = collections.namedtuple('Point', ['x', 'y'])\n\nm, b = 3, 2\nnoise = np.random.random(x.size)\npoints = [Point(xp, m*xp+b+err) for xp,err in zip(x, noise)]\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nms = np.linspace(2.0, 4.0, 10)\nbs = np.linspace(1.5, 2.5, 10)\n\nM, B = np.meshgrid(ms, bs)\nzs = np.array([error(mp, bp, points) \n               for mp, bp in zip(np.ravel(M), np.ravel(B))])\nZ = zs.reshape(M.shape)\n\nax.plot_surface(M, B, Z, rstride=1, cstride=1, color='b', alpha=0.5)\n\nax.set_xlabel('m')\nax.set_ylabel('b')\nax.set_zlabel('error')\n\nplt.show()\n"
"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(min_df=2)\ncorpus = [\n    'This is the first document.',\n    'This is the second second document.',\n    'And the third one.',\n    'Is this the first document?',\n]\nvectorizer = vectorizer.fit(corpus)\nprint vectorizer.vocabulary_ \n#prints {u'this': 4, u'is': 2, u'the': 3, u'document': 0, u'first': 1}\nX = vectorizer.transform(corpus)\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nMIN_VAL_ALLOWED = 2\n\nX = csr_matrix([[7,8,0],\n                [2,1,1],\n                [5,5,0]])\n\nz = np.squeeze(np.asarray(X.sum(axis=0) &gt; MIN_VAL_ALLOWED)) #z is the non-sparse terms \n\nprint X[:,z].toarray()\n#prints X without the third term (as it is sparse)\n[[7 8]\n[2 1]\n[5 5]]\n\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n\nMIN_DF_ALLOWED = 2\n\nX = csr_matrix([[7, 1.3, 0.9, 0],\n                [2, 1.2, 0.8  , 1],\n                [5, 1.5, 0  , 0]])\n\n#Creating a copy of the data\nB = csr_matrix(X, copy=True)\nB[B&gt;0] = 1\nz = np.squeeze(np.asarray(X.sum(axis=0) &gt; MIN_DF_ALLOWED))\nprint  X[:,z].toarray()\n#prints\n[[ 7.   1.3]\n[ 2.   1.2]\n[ 5.   1.5]]\n"
'from sklearn.cluster import KMeans\nimport pandas as pd\n\ndata = {\'one\': [1., 2., 3., 4., 3., 2., 1.], \'two\': [4., 3., 2., 1., 2., 3., 4.]}\ndata = pd.DataFrame(data)\n\nn_clusters = 2\n\nfor col in data.columns:\n    kmeans = KMeans(n_clusters=n_clusters)\n    X = data[col].reshape(-1, 1)\n    kmeans.fit(X)\n    print "{}: {}".format(col, kmeans.predict(X))\n'
'tfidf_vectorizer=TfidfVectorizer()\ntfidf_matrix=tfidf_vectorizer.fit_transform(documents)\n\nprint(tfidf_matrix.toarray())\n[[ 1.]\n [ 1.]]\n\ntfidf_vectorizer=TfidfVectorizer(analyzer="char")\n\ntfidf_vectorizer=TfidfVectorizer(token_pattern=u\'(?u)\\\\b\\w+\\\\b\')\n'
"toy_data = [['1', 5723, frozenset(1,2)],['2', 5723, frozenset(2,3)]]\n"
' from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel, LogisticRegressionWithSGD\n from pyspark.mllib.regression import LabeledPoint\n parsed_data = [LabeledPoint(0.0, [4.6,3.6,1.0,0.2]),\n                LabeledPoint(0.0, [5.7,4.4,1.5,0.4]),\n                LabeledPoint(1.0, [6.7,3.1,4.4,1.4]),\n                LabeledPoint(0.0, [4.8,3.4,1.6,0.2]),\n                LabeledPoint(2.0, [4.4,3.2,1.3,0.2])]     \n\n model = LogisticRegressionWithSGD.train(sc.parallelize(parsed_data)) # gives error:\n # org.apache.spark.SparkException: Input validation failed.\n\n model = LogisticRegressionWithLBFGS.train(sc.parallelize(parsed_data), numClasses=3)  # works OK\n'
"gridSearchClassifier.fit(Xnew, yNew)\ntransformed = gridSearchClassifier.transform(Xnew)\n\ngridSearchClassifier.fit(Xtrain, ytrain)\nclf = gridSearchClassifier.best_estimator_\n# do something with clf, its elements etc. \n# for example print clf.named_steps['vect']\n"
'X_train = np.concatenate((majority_x,minority_x))\ny_train = np.concatenate((majority_y,minority_y))\n'
'import apollocaffe\nfrom apollocaffe.layers import NumpyData, Convolution, EuclideanLoss\nimport numpy as np\n\ndef save():\n    net = apollocaffe.ApolloNet()\n    for i in range(1000):\n        example = np.array(np.random.random()).reshape((1, 1, 1, 1)) \n        net.clear_forward()\n        net.f(NumpyData(\'data\', example))\n        net.f(NumpyData(\'label\', example*3))\n        net.f(Convolution(\'conv\', (1,1), 1, bottoms=[\'data\']))\n        net.f(EuclideanLoss(\'loss\', bottoms=[\'conv\', \'label\']))\n        net.backward()\n        net.update(lr=0.1)\n        if i % 100 == 0:\n            print net.loss\n        net.save("model.h5")\n\n\ndef load():\n    print "LOAD"\n    net = apollocaffe.ApolloNet()\n    net.load("model.h5")\n    #example = np.array(np.random.random()).reshape((1, 1, 1, 1))\n    example = np.asarray([[[[ 0.92890837]]]])\n    net.clear_forward()\n    net.f(NumpyData(\'data\', example))\n    net.f(NumpyData(\'label\', example*3))\n    net.f(Convolution(\'conv\', (1,1), 1, bottoms=[\'data\']))\n    net.f(EuclideanLoss(\'loss\', bottoms=[\'conv\', \'label\']))\n    net.backward()\n    net.update(lr=0.1)\n    print net.loss\n\nsave()\nload()\n'
"lower, upper = df.total_bill.quantile([.25, .75]).values.tolist()\ndf = df.join(df.loc[df.total_bill &lt; lower, 'total_bill'], rsuffix='_lower')\ndf = df.join(df.loc[df.total_bill &gt; upper, 'total_bill'], rsuffix='_upper')\nsns.pointplot(data=df.loc[:, [c for c in df.columns if c.startswith('total')]])\n\ndf = df.loc[:, ['total_bill', 'total_bill_upper', 'total_bill_lower']].unstack().reset_index().drop('level_1', axis=1).dropna()\ndf.columns = ['grp', 'val']\n\nsns.pointplot(x='grp', y='val', hue='grp', data=df)\n"
'import numpy as np\n\nwith np.load("arrays.npz") as data:\n\n    thrLayer = data[\'thrLayer\'] # The final layer post activation; you\n    # can derive this final layer, if verification needed, using weights below\n\n    thetaO = data[\'thetaO\'] # The weight array between layers 1 and 2\n    thetaT = data[\'thetaT\'] # The weight array between layers 2 and 3\n\n    Ynew = data[\'Ynew\'] # The output array with a 1 in position i and 0s elsewhere\n\n    #class i is the class that the data described by X[i,:] belongs to\n\n    X = data[\'X\'] #Raw data with 1s appended to the first column\n    Y = data[\'Y\'] #One dimensional column vector; entry i contains the class of entry i\n\n\nm = len(thrLayer)\nk = thrLayer.shape[1]\ncost = 0\n\nY_arr = np.zeros(Ynew.shape)\nfor i in xrange(m):\n    Y_arr[i,int(Y[i,0])-1] = 1\n\nfor i in range(m):\n    for j in range(k):\n        cost += -Y_arr[i,j]*np.log(thrLayer[i,j]) - (1 - Y_arr[i,j])*np.log(1 - thrLayer[i,j])\ncost /= m\n\n\'\'\'\nRegularized Cost Component\n\'\'\'\n\nregCost = 0\n\nfor i in range(len(thetaO)):\n    for j in range(1,len(thetaO[0])):\n        regCost += thetaO[i,j]**2\n\nfor i in range(len(thetaT)):\n    for j in range(1,len(thetaT[0])):\n        regCost += thetaT[i,j]**2\nlam=1\nregCost *= lam/(2.*m)\n\n\nprint(cost)\nprint(cost + regCost)\n\n0.287629165161\n0.383769859091\n'
"    from .metrics import accuracy_score\n    return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\n\ndef accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):\n    ...\n    # Compute accuracy for each possible representation\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n    if y_type.startswith('multilabel'):\n        differing_labels = count_nonzero(y_true - y_pred, axis=1)\n        score = differing_labels == 0\n    else:\n        score = y_true == y_pred\n\n    return _weighted_sum(score, sample_weight, normalize)\n\ndef _weighted_sum(sample_score, sample_weight, normalize=False):\n    if normalize:\n        return np.average(sample_score, weights=sample_weight)\n    elif sample_weight is not None:\n        return np.dot(sample_score, sample_weight)\n    else:\n        return sample_score.sum()\n"
'new_samples = np.array([test_data[8]], dtype=float)\n\ny = list(classifier.predict(new_samples, as_iterable=True))\nprint(\'Predictions: {}\'.format(str(y)))\n\nprint ("Predicted %s, Label: %d" % (str(y), test_labels[8]))\n'
'tv = tf.trainable_variables()\nregularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\ncost = tf.reduce_sum(tf.pow(pred - y, 2)) + regularization_cost\noptimizer = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)\n'
'X = training_save_file["X"][[idx]]\n'
'from sklearn.preprocessing import RobustScaler\nrbX = RobustScaler()\nX = rbX.fit_transform(X)\n\nrbY = RobustScaler()\nY = rbY.fit_transform(Y)\n\nsvm = SVR()\nsvm.fit(X,Y)\n\nsvm_pred = svm.predict(rbX.transform(predict))\n\nsvm_pred = rbY.inverse_transform(svm_pred)\n'
"import numpy as np\n\na = np.matrix('0.1 0.82')\nprint(a)\n\na[a &gt; 0.5] = 1\na[a &lt;= 0.5] = 0\nprint(a)\n\n[[ 0.1   0.82]]\n[[ 0.  1.]]\n\nimport numpy as np\n\na = np.matrix('0.1 0.82')\nprint(a)\n\na = np.where(a &gt; 0.5, 1, 0)\nprint(a)\n"
'# retrieve the last layer of the autoencoder model \ndecoder_layer1 = autoencoder.layers[-3]\ndecoder_layer2 = autoencoder.layers[-2]\ndecoder_layer3 = autoencoder.layers[-1]\n\n# create the decoder model\ndecoder = Model(input=encoded_input, \noutput=decoder_layer3(decoder_layer2(decoder_layer1(encoded_input))))\n'
'""" data """\nfrom time import perf_counter as pc\nimport numpy as np\nfrom sklearn import datasets\ndiabetes = datasets.load_diabetes()\nA = diabetes.data\ny = diabetes.target\nalpha=0.1\n\nprint(\'Problem-size: \', A.shape)\n\ndef obj(x):  # following sklearn\'s definition from user-guide!\n    return (1. / (2*A.shape[0])) * np.square(np.linalg.norm(A.dot(x) - y, 2)) + alpha * np.linalg.norm(x, 1)\n\n\n""" sklearn """\nprint(\'\\nsklearn classic l1\')\nfrom sklearn import linear_model\nclf = linear_model.Lasso(alpha=alpha, fit_intercept=False)\nt0 = pc()\nclf.fit(A, y)\nprint(\'used (secs): \', pc() - t0)\nprint(obj(clf.coef_))\nprint(\'sum x: \', np.sum(clf.coef_))\n\n""" cvxpy """\nprint(\'\\ncvxpy + scs classic l1\')\nfrom cvxpy import *\nx = Variable(A.shape[1])\nobjective = Minimize((1. / (2*A.shape[0])) * sum_squares(A*x - y) + alpha * norm(x, 1))\nproblem = Problem(objective, [])\nt0 = pc()\nproblem.solve(solver=SCS, use_indirect=False, max_iters=10000, verbose=False)\nprint(\'used (secs): \', pc() - t0)\nprint(obj(x.value.flat))\nprint(\'sum x: \', np.sum(x.value.flat))\n\n""" cvxpy -&gt; sum x == 1 """\nprint(\'\\ncvxpy + scs sum == 1 / 1st approach\')\nobjective = Minimize((1. / (2*A.shape[0])) * sum_squares(A*x - y))\nconstraints = [sum(x) == 1]\nproblem = Problem(objective, constraints)\nt0 = pc()\nproblem.solve(solver=SCS, use_indirect=False, max_iters=10000, verbose=False)\nprint(\'used (secs): \', pc() - t0)\nprint(obj(x.value.flat))\nprint(\'sum x: \', np.sum(x.value.flat))\n\n""" cvxpy approach 2 -&gt; sum x == 1 """\nprint(\'\\ncvxpy + scs sum == 1 / 2nd approach\')\nM = 1e6\nobjective = Minimize((1. / (2*A.shape[0])) * sum_squares(A*x - y) + M*(sum(x) - 1))\nconstraints = [sum(x) == 1]\nproblem = Problem(objective, constraints)\nt0 = pc()\nproblem.solve(solver=SCS, use_indirect=False, max_iters=10000, verbose=False)\nprint(\'used (secs): \', pc() - t0)\nprint(obj(x.value.flat))\nprint(\'sum x: \', np.sum(x.value.flat))\n\nProblem-size:  (442, 10)\n\nsklearn classic l1\nused (secs):  0.001451024380348898\n13201.3508496\nsum x:  891.78869298\n\ncvxpy + scs classic l1\nused (secs):  0.011165673357417458\n13203.6549995\nsum x:  872.520510561\n\ncvxpy + scs sum == 1 / 1st approach\nused (secs):  0.15350853891775978\n13400.1272148\nsum x:  -8.43795102327\n\ncvxpy + scs sum == 1 / 2nd approach\nused (secs):  0.012579569383536493\n13397.2932976\nsum x:  1.01207061047\n\n""" accelerated pg  -&gt; sum x == 1 """\ndef solve_pg(A, b, momentum=0.9, maxiter=1000):\n    """ remarks:\n            algorithm: accelerated projected gradient\n            projection: proj on probability-simplex\n                -&gt; naive and slow using cvxpy + ecos\n            line-search: armijo-rule along projection-arc (Bertsekas book)\n                -&gt; suffers from slow projection\n            stopping-criterion: naive\n            gradient-calculation: precomputes AtA\n                -&gt; not needed and not recommended for huge sparse data!\n    """\n\n    M, N = A.shape\n    x = np.zeros(N)\n\n    AtA = A.T.dot(A)\n    Atb = A.T.dot(b)\n\n    stop_count = 0\n\n    # projection helper\n    x_ = Variable(N)\n    v_ = Parameter(N)\n    objective_ =  Minimize(0.5 * square(norm(x_ - v_, 2)))\n    constraints_ = [sum(x_) == 1]\n    problem_ = Problem(objective_, constraints_)\n\n    def gradient(x):\n        return AtA.dot(x) - Atb\n\n    def obj(x):\n        return 0.5 * np.linalg.norm(A.dot(x) - b)**2\n\n    it = 0\n    while True:\n        grad = gradient(x)\n\n        # line search\n        alpha = 1\n        beta = 0.5\n        sigma=1e-2\n        old_obj = obj(x)\n        while True:\n            new_x = x - alpha * grad\n            new_obj = obj(new_x)\n            if old_obj - new_obj &gt;= sigma * grad.dot(x - new_x):\n                break\n            else:\n                alpha *= beta\n\n        x_old = x[:]\n        x = x - alpha*grad\n\n        # projection\n        v_.value = x\n        problem_.solve()\n        x = np.array(x_.value.flat)\n\n        y = x + momentum * (x - x_old)\n\n        if np.abs(old_obj - obj(x)) &lt; 1e-2:\n            stop_count += 1\n        else:\n            stop_count = 0\n\n        if stop_count == 3:\n            print(\'early-stopping @ it: \', it)\n            return x\n\n        it += 1\n\n        if it == maxiter:\n            return x\n\n\nprint(\'\\n acc pg\')\nt0 = pc()\nx = solve_pg(A, y)\nprint(\'used (secs): \', pc() - t0)\nprint(obj(x))\nprint(\'sum x: \', np.sum(x))\n\nacc pg\nearly-stopping @ it:  367\nused (secs):  0.7714511330487027\n13396.8642379\nsum x:  1.00000000002\n'
'scores = cross_val_score(simple_tree, df.loc[:,\'system\':\'gwno\'], df[\'gdp_growth\'], cv=cv)\n\nscorer = check_scoring(estimator, scoring=scoring)\n\nhas_scoring = scoring is not None\nif not hasattr(estimator, \'fit\'):\n    raise TypeError("estimator should be an estimator implementing "\n                    "\'fit\' method, %r was passed" % estimator)\nif isinstance(scoring, six.string_types):\n    return get_scorer(scoring)\nelif has_scoring:\n    # Heuristic to ensure user has not passed a metric\n    module = getattr(scoring, \'__module__\', None)\n    if hasattr(module, \'startswith\') and \\\n       module.startswith(\'sklearn.metrics.\') and \\\n       not module.startswith(\'sklearn.metrics.scorer\') and \\\n       not module.startswith(\'sklearn.metrics.tests.\'):\n        raise ValueError(\'scoring value %r looks like it is a metric \'\n                         \'function rather than a scorer. A scorer should \'\n                         \'require an estimator as its first parameter. \'\n                         \'Please use `make_scorer` to convert a metric \'\n                         \'to a scorer.\' % scoring)\n    return get_scorer(scoring)\nelif hasattr(estimator, \'score\'):\n    return _passthrough_scorer\nelif allow_none:\n    return None\nelse:\n    raise TypeError(\n        "If no scoring is specified, the estimator passed should "\n        "have a \'score\' method. The estimator %r does not." % estimator)\n\nhas_scoring = scoring is not None\n\nelif hasattr(estimator, \'score\'):\n    return _passthrough_scorer\n\ndef _passthrough_scorer(estimator, *args, **kwargs):\n    """Function that wraps estimator.score"""\n    return estimator.score(*args, **kwargs)\n'
'# ...\ncost = -(1/m)*np.sum(np.multiply(Y,np.log(A)) + np.multiply((1-Y),np.log(1-A)), axis=1)\n# ...\n'
"lr = LogisticRegression()             # initialize the model\n\ngrid = GridSearchCV(lr, param_grid, cv=12, scoring = 'accuracy', )\ngrid.fit(X5, y5)\n"
'def myGenerator(train_generator,train_generator1):\n\n    while True:\n\n        xy = train_generator.next() #or next(train_generator)\n        xy1 = train_generator1.next() #or next(train_generator1)\n        yield (xy[0],xy1[0])\n\ntrain_generator2 = myGenerator(train_generator,train_generator1)\n\nConv\n... Maybe more convs\nMaxPooling\nConv\n... Maybe more convs\nMaxPooling\nConv\n\n......\n\nUpSampling\nConv\n...\nUpSampling\nConv\n....\n'
"predicted_distribution = tf.nn.softmax(logits, name='distribution')\n\n[0.14286 0.14286 0.14286 0.14286 0.14286 0.14286 0.14286]\n[0.14286 0.14286 0.14286 0.14286 0.14286 0.14286 0.14286]\n\ntrue_correct = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\nalternative_accuracy = tf.reduce_mean(tf.cast(true_correct, tf.float32))\n\niteration=2  loss=3.992  train-acc=0.13086  train-alt-acc=0.13086\niteration=4  loss=3.590  train-acc=0.13086  train-alt-acc=0.12207\niteration=6  loss=2.871  train-acc=0.21777  train-alt-acc=0.13672\niteration=8  loss=2.466  train-acc=0.37695  train-alt-acc=0.16211\niteration=10  loss=2.099  train-acc=0.62305  train-alt-acc=0.10742\niteration=12  loss=2.066  train-acc=0.79980  train-alt-acc=0.17090\niteration=14  loss=2.016  train-acc=0.84277  train-alt-acc=0.17285\niteration=16  loss=1.954  train-acc=0.91309  train-alt-acc=0.13574\niteration=18  loss=1.956  train-acc=0.95508  train-alt-acc=0.06445\niteration=20  loss=1.923  train-acc=0.97754  train-alt-acc=0.11328\n\nx = tf.constant([0, 1e-8, 1e-8, 1e-9])\ntf.nn.softmax(x).eval()\n# &gt;&gt;&gt; array([0.25, 0.25, 0.25, 0.25], dtype=float32)\n\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64)), tf.float32))\n"
"import nltk\nnltk.download('nps_chat')\nposts = nltk.corpus.nps_chat.xml_posts()[:10000]\n\n\ndef dialogue_act_features(post):\n    features = {}\n    for word in nltk.word_tokenize(post):\n        features['contains({})'.format(word.lower())] = True\n    return features\n\nfeaturesets = [(dialogue_act_features(post.text), post.get('class')) for post in posts]\nsize = int(len(featuresets) * 0.1)\ntrain_set, test_set = featuresets[size:], featuresets[:size]\nclassifier = nltk.NaiveBayesClassifier.train(train_set)\nprint(nltk.classify.accuracy(classifier, test_set))\n\nprint(classifier.classify(dialogue_act_features(line)))\n"
'def fit(self, X, y):\n  ...\n  self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n  Y = self.label_binarizer_.fit_transform(y)\n  Y = Y.tocsc()\n  self.classes_ = self.label_binarizer_.classes_\n'
'optimal_features = X[:, selector.support_] # selector is a RFECV fitted object\n\nn = 6 # to select top 6 features\nfeature_ranks = selector.ranking_  # selector is a RFECV fitted object\nfeature_ranks_with_idx = enumerate(feature_ranks)\nsorted_ranks_with_idx = sorted(feature_ranks_with_idx, key=lambda x: x[1])\ntop_n_idx = [idx for idx, rnk in sorted_ranks_with_idx[:n]]\n\ntop_n_features = X[:5, top_n_idx]\n'
"score_dict = {}\nfor nhn in nhn_range:\n    mlp = MLPRegressor(hidden_layer_sizes=(nhn,), activation='tanh', \n                       solver='adam', shuffle=False, random_state=42, \n                       max_iter=20000, momentum=0.7, early_stopping=True, \n                       validation_fraction=0.15)\n\n\n    nhn_scores = []\n    for _ in range(n):\n\n        df_train = shuffle(df_train)\n        score = np.sqrt(-cross_val_score(mlp, df_train[feature_cols], \n                    df_train[response_cols], \n                    cv=5, scoring='neg_mean_squared_error')).mean()\n        nhn_scores.append(score)\n    score_dict[nhn] = nhn_scores\n\nimport pandas as pd\nscore_df = pd.DataFrame.from_dict(score_dict)\n"
"top_model.add(Dense(1,activation='softmax')) \n\ntop_model.add(Dense(1,activation='sigmoid'))\n"
'model = models.Sequential()\nmodel.add(layers.Dense(10, input_shape=(28*28,)))\nmodel.summary()\n\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_2 (Dense)              (None, 10)                7850      \n=================================================================\nTotal params: 7,850\nTrainable params: 7,850\nNon-trainable params: 0\n_________________________________________________________________\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(10, input_shape=(28,28)))\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 28, 10)            290       \n=================================================================\nTotal params: 290\nTrainable params: 290\nNon-trainable params: 0\n_________________________________________________________________\n'
"from keras.callbacks import Callback\n\nclass TestCallback(Callback):\n    def __init__(self, test_data):\n        self.test_data = test_data\n\n    def on_batch_end(self, batch, logs={}):\n        x, y = self.test_data\n        loss, acc = self.model.evaluate(x, y, verbose=0)\n        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=1,\n          verbose=1,\n          validation_data=(x_test, y_test),\n          callbacks=[TestCallback((x_test, y_test))]\n         )\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n\nTesting loss: 0.0672039743446745, acc: 0.9781\n\n  128/60000 [..............................] - ETA: 7484s - loss: 0.1450 - acc: 0.9531\n\n/var/venv/DSTL/lib/python3.4/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (15.416976). Check your callbacks.\n  % delta_t_median)\n\n\nTesting loss: 0.06644540682602673, acc: 0.9781\n\n  256/60000 [..............................] - ETA: 7476s - loss: 0.1187 - acc: 0.9570\n\n/var/venv/DSTL/lib/python3.4/site-packages/keras/callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (15.450395). Check your callbacks.\n  % delta_t_median)\n\n\nTesting loss: 0.06575664376271889, acc: 0.9782\n\nclass TestCallback2(Callback):\n    def __init__(self, test_data):\n        self.test_data = test_data\n\n    def on_batch_end(self, batch, logs={}):\n        print()  # just a dummy print command\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n\n  128/60000 [..............................] - ETA: 346s - loss: 0.8503 - acc: 0.7188\n  256/60000 [..............................] - ETA: 355s - loss: 0.8496 - acc: 0.7109\n  384/60000 [..............................] - ETA: 339s - loss: 0.7718 - acc: 0.7396\n  [...]\n\nfrom keras.callbacks import Callback\n\nclass Histories(Callback):\n\n    def on_train_begin(self,logs={}):\n        self.losses = []\n        self.accuracies = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n        self.accuracies.append(logs.get('acc'))\n\n\nhistories = Histories()\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=1,\n          verbose=1,\n          validation_data=(x_test, y_test),\n          callbacks=[histories]\n         )\n\nhistories.losses[:5]\n# [2.3115866, 2.3008101, 2.2479887, 2.1895032, 2.1491694]\n\nhistories.accuracies[:5]\n# [0.0703125, 0.1484375, 0.1875, 0.296875, 0.359375]\n"
'test = (1,2,3)\ntester = iter(test)\n\nwhile True:\n    nextItem = next(tester)\n    print(nextItem)\n'
'import eli5\nfrom xgboost import XGBClassifier, XGBRegressor\n\ndef _check_booster_args(xgb, is_regression=None):\n    # type: (Any, bool) -&gt; Tuple[Booster, bool]\n    if isinstance(xgb, eli5.xgboost.Booster): # patch (from "xgb, Booster")\n        booster = xgb\n    else:\n        booster = xgb.get_booster() # patch (from "xgb.booster()" where `booster` is now a string)\n        _is_regression = isinstance(xgb, XGBRegressor)\n        if is_regression is not None and is_regression != _is_regression:\n            raise ValueError(\n                \'Inconsistent is_regression={} passed. \'\n                \'You don\\\'t have to pass it when using scikit-learn API\'\n                .format(is_regression))\n        is_regression = _is_regression\n    return booster, is_regression\n\neli5.xgboost._check_booster_args = _check_booster_args\n\nshow_prediction(xgb_model, test[0], show_feature_values=True, feature_names=feature_names)\n'
"# Decision tree\n...\ny_pred = decision.predict(testX)\ny_score = decision.score(testX, testY)\nprint('Accuracy: ', y_score)\n\n# Compute the average precision score\nfrom sklearn.metrics import precision_score\nmicro_precision = precision_score(y_pred, testY, average='micro')\nprint('Micro-averaged precision score: {0:0.2f}'.format(\n      micro_precision))\n\nmacro_precision = precision_score(y_pred, testY, average='macro')\nprint('Macro-averaged precision score: {0:0.2f}'.format(\n      macro_precision))\n\nper_class_precision = precision_score(y_pred, testY, average=None)\nprint('Per-class precision score:', per_class_precision)\n"
"from sklearn.model_selection import KFold, cross_validate\nfrom sklearn.datasets import load_boston\nfrom sklearn.tree import DecisionTreeRegressor\n\nX, y = load_boston(return_X_y=True)\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True)\n\nmodel = DecisionTreeRegressor()\nscoring=('r2', 'neg_mean_squared_error')\n\ncv_results = cross_validate(model, X, y, cv=kf, scoring=scoring, return_train_score=False)\ncv_results\n\n{'fit_time': array([0.00901461, 0.00563478, 0.00539804, 0.00529385, 0.00638533]),\n 'score_time': array([0.00132656, 0.00214362, 0.00134897, 0.00134444, 0.00176597]),\n 'test_neg_mean_squared_error': array([-11.15872549, -30.1549505 , -25.51841584, -16.39346535,\n        -15.63425743]),\n 'test_r2': array([0.7765484 , 0.68106786, 0.73327311, 0.83008371, 0.79572363])}\n"
"import pandas.util.testing as testing\nimport numpy as np\nnp.random.seed(1)\n\ntesting.N, testing.K = 5, 3  # Setting the rows and columns of the desired data\n\nprint testing.makeTimeDataFrame(freq='MS')\n&gt;&gt;&gt;\n                   A         B         C\n2000-01-01 -0.488392  0.429949 -0.723245\n2000-02-01  1.247192 -0.513568 -0.512677\n2000-03-01  0.293828  0.284909  1.190453\n2000-04-01 -0.326079 -1.274735 -0.008266\n2000-05-01 -0.001980  0.745803  1.519243\n\nimport numpy as np\nimport pandas as pd\nnp.random.seed(1)\n\nrows,cols = 5,3\ndata = np.random.rand(rows,cols) # You can use other random functions to generate values with constraints\ntidx = pd.date_range('2019-01-01', periods=rows, freq='MS') # freq='MS'set the frequency of date in months and start from day 1. You can use 'T' for minutes and so on\ndata_frame = pd.DataFrame(data, columns=['a','b','c'], index=tidx)\nprint data_frame\n&gt;&gt;&gt;\n                   a         b         c\n2019-01-01  0.992856  0.217750  0.538663\n2019-02-01  0.189226  0.847022  0.156730\n2019-03-01  0.572417  0.722094  0.868219\n2019-04-01  0.023791  0.653147  0.857148\n2019-05-01  0.729236  0.076817  0.743955\n"
'#Declaring necessary modules\nimport tensorflow as tf\nimport numpy as np\n"""\nA simple numpy implementation of a XOR gate to understand the backpropagation\nalgorithm\n"""\n\nx = tf.placeholder(tf.float32,shape = [4,2],name = "x")\n#declaring a place holder for input x\ny = tf.placeholder(tf.float32,shape = [4,1],name = "y")\n#declaring a place holder for desired output y\n\nm = np.shape(x)[0]#number of training examples\nn = np.shape(x)[1]#number of features\nhidden_s = 2 #number of nodes in the hidden layer\nl_r = 1#learning rate initialization\n\ntheta1 = tf.SparseTensor(indices=[[0, 0],[0, 1], [1, 1]], values=[0.1, 0.2, 0.1], dense_shape=[3, 2])\n#theta1 = tf.cast(tf.Variable(tf.random_normal([3,hidden_s]),name = "theta1"),tf.float64)\ntheta2 = tf.cast(tf.Variable(tf.random_normal([hidden_s+1,1]),name = "theta2"),tf.float32)\n\n#conducting forward propagation\na1 = tf.concat([np.c_[np.ones(x.shape[0])],x],1)\n#the weights of the first layer are multiplied by the input of the first layer\n\n#z1 = tf.sparse_tensor_dense_matmul(theta1, a1)\n\nz1 = tf.matmul(a1,tf.sparse_tensor_to_dense(theta1))\n#the input of the second layer is the output of the first layer, passed through the \n\na2 = tf.concat([np.c_[np.ones(x.shape[0])],tf.sigmoid(z1)],1)\n#the input of the second layer is multiplied by the weights\n\nz3 = tf.matmul(a2,theta2)\n#the output is passed through the activation function to obtain the final probability\n\nh3 = tf.sigmoid(z3)\ncost_func = -tf.reduce_sum(y*tf.log(h3)+(1-y)*tf.log(1-h3),axis = 1)\n\n#built in tensorflow optimizer that conducts gradient descent using specified \n\noptimiser = tf.train.GradientDescentOptimizer(learning_rate = l_r).minimize(cost_func)\n\n#setting required X and Y values to perform XOR operation\nX = [[0,0],[0,1],[1,0],[1,1]]\nY = [[0],[1],[1],[0]]\n\n#initializing all variables, creating a session and running a tensorflow session\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\n#running gradient descent for each iterati\nfor i in range(200):\n   sess.run(optimiser, feed_dict = {x:X,y:Y})#setting place holder values using feed_dict\n   if i%100==0:\n      print("Epoch:",i)\n      print(sess.run(theta1))\n\nEpoch: 0\nSparseTensorValue(indices=array([[0, 0],\n       [0, 1],\n       [1, 1]]), values=array([0.1, 0.2, 0.1], dtype=float32), dense_shape=array([3, 2]))\nEpoch: 100\nSparseTensorValue(indices=array([[0, 0],\n       [0, 1],\n       [1, 1]]), values=array([0.1, 0.2, 0.1], dtype=float32), dense_shape=array([3, 2]))\n\n\nmask = tf.Variable([[1,0,0],[0,1,0],[0,0,1]],name =\'mask\', trainable=False)\nweight = tf.cast(tf.Variable(tf.random_normal([3,3])),tf.float32)\ndesired_tensor = tf.matmul(weight, mask)\n\nmask = tf.Variable([[1,0,0],[0,1,0],[0,0,1]],name =\'mask\', trainable=False)\nweight = tf.cast(tf.Variable(tf.random_normal([3,3])),tf.float32)\ndesired_tensor = tf.where(mask &gt; 0, tf.ones_like(weight), weight)\n\nSparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n\n\n[[1, 0, 0, 0]\n [0, 0, 2, 0]\n [0, 0, 0, 0]]\n\n'
"coherence_model_lda = CoherenceModel(model=lda_model, texts=data_df['corpus'].tolist(), dictionary=dictionary, coherence='c_v')\nwith np.errstate(invalid='ignore'):\n    lda_score = coherence_model_lda.get_coherence()\n"
'import plotly.figure_factory as ff\n\nz = [[0.1, 0.3, 0.5, 0.2],\n     [1.0, 0.8, 0.6, 0.1],\n     [0.1, 0.3, 0.6, 0.9],\n     [0.6, 0.4, 0.2, 0.2]]\n\nx = [\'healthy\', \'multiple diseases\', \'rust\', \'scab\']\ny =  [\'healthy\', \'multiple diseases\', \'rust\', \'scab\']\n\n# change each element of z to type string for annotations\nz_text = [[str(y) for y in x] for x in z]\n\n# set up figure \nfig = ff.create_annotated_heatmap(z, x=x, y=y, annotation_text=z_text, colorscale=\'Viridis\')\n\n# add title\nfig.update_layout(title_text=\'&lt;i&gt;&lt;b&gt;Confusion matrix&lt;/b&gt;&lt;/i&gt;\',\n                  #xaxis = dict(title=\'x\'),\n                  #yaxis = dict(title=\'x\')\n                 )\n\n# add custom xaxis title\nfig.add_annotation(dict(font=dict(color="black",size=14),\n                        x=0.5,\n                        y=-0.15,\n                        showarrow=False,\n                        text="Predicted value",\n                        xref="paper",\n                        yref="paper"))\n\n# add custom yaxis title\nfig.add_annotation(dict(font=dict(color="black",size=14),\n                        x=-0.35,\n                        y=0.5,\n                        showarrow=False,\n                        text="Real value",\n                        textangle=-90,\n                        xref="paper",\n                        yref="paper"))\n\n# adjust margins to make room for yaxis title\nfig.update_layout(margin=dict(t=50, l=200))\n\n# add colorbar\nfig[\'data\'][0][\'showscale\'] = True\nfig.show()\n'
"df = pd.read_csv('http://web.stanford.edu/~oleg2/hse/wage/wage.csv').sort_values(by=['age'])\ndf['wage_factor'] = (df.wage &gt; 250).astype('int')\nfig,ax = plt.subplots(1,2,figsize=(8,3))\ndf.plot.scatter(x='age',y='wage',ax=ax[0])\ndf.plot.scatter(x='age',y='wage_factor',ax=ax[1])\n\nd = 3\nknots = [30,60]\n\nmy_spline_transformation = f&quot;bs(train, knots={knots}, degree={d}, include_intercept=True)&quot;\ntransformed = dmatrix( my_spline_transformation, {&quot;train&quot;: df.age}, return_type='dataframe' )\n\nlft = sm.Logit( (df.wage&gt;250), transformed)\nres = lft.fit()\ny_grid1 = res.predict(transformed)\n"
"import re, collections\n\ndef words(text): return re.findall('[a-z]+', text.lower()) \n\ndef train(features):\n    model = collections.defaultdict(lambda: 1)\n    for f in features:\n        model[f] += 1\n    return model\n\nNWORDS = train(words(file('big.txt').read()))\n\nalphabet = 'abcdefghijklmnopqrstuvwxyz'\n\ndef edits1(word):\n    splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    deletes    = [a + b[1:] for a, b in splits if b]\n    transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)&gt;1]\n    replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n    inserts    = [a + c + b     for a, b in splits for c in alphabet]\n    return set(deletes + transposes + replaces + inserts)\n\ndef known_edits2(word):\n    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n\ndef known(words): return set(w for w in words if w in NWORDS)\n\ndef correct(word):\n    candidates = known([word]) or known(edits1(word)) or known_edits2(word) or [word]\n    return max(candidates, key=NWORDS.get)\n"
'# the high order items can be integrated into X (such as x1^2,x1*x2), and change it into a linear regression problem again\nlasso.fit(X, y) \n# the selection range of lambda can be determined by yourself.\nLassoCV(lambda=array([ 2, 1,9, ..., 0.2 , 0.1]),  \ncopy_X=True, cv=None, eps=0.001, fit_intercept=True, max_iter=1000,\nn_alphas=100, normalize=False, precompute=’auto’, tol=0.0001,\nverbose=False)\n'
'def precision(actual, predicted, k):\n    act_set = set(actual)\n    pred_set = set(predicted[:k])\n    result = len(act_set &amp; pred_set) / float(k)\n    return result\n\ndef recall(actual, predicted, k):\n    act_set = set(actual)\n    pred_set = set(predicted[:k])\n    result = len(act_set &amp; pred_set) / float(len(act_set))\n    return result\n'
"from sklearn import linear_model\nimport csv\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef process_chunk(chuk):\n\n    training_set_feature_list = []\n    training_set_label_list = []\n    test_set_feature_list = []\n    test_set_label_list = []\n    count = 1\n    # to divide into training &amp; test\n    chuk = map(lambda x: x[2:], chuk) # Removing first 2 columns\n    chunk = np.array(chuk,dtype = np.float) # Make floats array from strings\n    ########## Testing dataset: Data after 30th row =########################################\n    test_set_feature_list = chunk[30:,3:5]  #4rd and 5th column of chunk \n    test_set_label_list = chunk[30:,2] #3rd column of chunk\n\n    ########## Training dataset: All data before 30th row########################################\n    training_set_feature_list = chunk[:30,3:5]\n    training_set_label_list = chunk[:30, 2]\n\n    # Create linear regression object\n    regr = linear_model.LinearRegression()\n    # Train the model using the training sets\n    regr.fit(training_set_feature_list, training_set_label_list)\n\n    predictedTestSet = regr.predict(test_set_feature_list)\n\n     # The coefficients\n    print 'Coefficients: {}'.format(regr.coef_)\n    # The mean square error\n    print 'Residual sum of squares: %.2f' % np.mean(predictedTestSet - test_set_label_list) ** 2\n    # Explained variance score: 1 is perfect prediction\n    print 'Variance score: %.2f' % regr.score( test_set_feature_list, test_set_label_list)\n    X = [x for (y,x) in sorted(zip(test_set_label_list, predictedTestSet))]\n    Y = [y for (y,x) in sorted(zip(test_set_label_list, predictedTestSet))]\n    plt.plot(range(len(X)),X , 'r.', label='predicted')    \n    plt.plot(range(len(Y)),Y , 'g-',label='test_set')    \n    plt.legend()\n    plt.show()\n    return predictedTestSet\n\n\n# Load and parse the data\nfile_read = open('file1.csv', 'r')\n\nreader = csv.reader(file_read)\n\nchunk, chunksize = [], 12\n\nfor i, line in enumerate(reader):\n    if ( i &gt; 0):\n        chunk.append(line)\n\npredictedSet = process_chunk(chunk)\nprint predictedSet\n\nCoefficients: [ 0.06821406]\nResidual sum of squares: 0.00\nVariance score: 1.00\n[ 121.39022086  170.9286349    64.34416748   96.61828528  124.28181483\n  174.99828567]\n"
'# Load the "autoreload" extension\n%load_ext autoreload\n\n# always reload modules marked with "%aimport"\n%autoreload 1\n\nimport os\nimport sys\n\n# add the \'src\' directory as one where we can import modules\nsrc_dir = os.path.join(os.getcwd(), os.pardir, \'src\')\nsys.path.append(src_dir)\n\n# import my method from the source code\n%aimport preprocess.build_features\n'
"# -*- coding: utf-8 -*-\nimport sys\nimport os\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.datasets import load_files\nfrom sklearn.svm import SVC\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\n\nstring = sys.argv[1] #the string i'd like to predict\nsets = load_files('scikit') #load my personal dataset\n\n\n\n\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(sets.data)\n\n\n\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\n\n\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape\n\n\nclf = MultinomialNB().fit(X_train_tfidf, sets.target)\ndocs_new = [string]\nX_new_counts = count_vect.transform(docs_new)\nX_new_tfidf = tfidf_transformer.transform(X_new_counts)\npredicted = clf.predict(X_new_tfidf)\n\nfor doc, category in zip(docs_new, predicted):\n     print('%r =&gt; %s' % (doc, sets.target_names[category])) #print result of prediction\n"
'Tensor("Relu:0", shape=(?, 60, 60, 32), dtype=float32)\nTensor("MaxPool:0", shape=(?, 30, 30, 32), dtype=float32)\nTensor("Relu_1:0", shape=(?, 30, 30, 64), dtype=float32)\nTensor("MaxPool_1:0", shape=(?, 15, 15, 64), dtype=float32)\nTensor("Relu_2:0", shape=(?, 15, 15, 128), dtype=float32)\nTensor("MaxPool_2:0", shape=(?, 8, 8, 128), dtype=float32)\n\nw4 = init_weights([128 * 8 * 8, 625])\n'
'    check_classification_targets(y)\n\n    # actual graph construction (implementations should override this)\n    graph_matrix = self._build_graph()\n\n    # label construction\n    # construct a categorical distribution for classification only\n    classes = np.unique(y)\n    classes = (classes[classes != -1])\n'
'X = np.loadtxt("my_data")\n\ncar_type = [1, 2, 3, 2, 6, 1, 0, 2, 5, 4, 2, 0, 3, 3, 2, 1, 0]\nclf = tree.DecisionTreeClassifier()\nclf.fit(X, car_type)\n\nX = np.loadtxt("my_data")\ncars = [0, 1, 2, 3, 3, 0, 2] \nclf = tree.DecisionTreeClassifier()\nclf.fit(X, cars)\n'
"from sklearn.cluster import FeatureAgglomeration\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#iris.data from https://archive.ics.uci.edu/ml/machine-learning-databases/iris/\niris=pd.read_csv('iris.data',sep=',',header=None)\n#store labels\nlabel=iris[4]\niris=iris.drop([4],1)\n\n#set n_clusters to 2, the output will be two columns of agglomerated features ( iris has 4 features)\nagglo=FeatureAgglomeration(n_clusters=2).fit_transform(iris)\n\n#plotting\ncolor=[]\nfor i in label:\n    if i=='Iris-setosa':\n        color.append('g')\n    if  i=='Iris-versicolor':\n        color.append('b')\n    if i=='Iris-virginica':\n        color.append('r')\nplt.scatter(agglo[:,0],agglo[:,1],c=color)\nplt.show()\n"
'# Predicting the train set results\ny_train_pred = knn.predict(X_train)\ncm_train = confusion_matrix(y_train, y_train_pred)\n'
"n_steps = 2\nn_inputs = 3\nn_neurons = 5    \nX = tf.placeholder(dtype=tf.float32, shape=[None, n_steps, n_inputs])\nbasic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)\noutputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\nprint(tf.trainable_variables())\n\n[&lt;tf.Variable 'rnn/basic_rnn_cell/kernel:0' shape=(8, 5) dtype=float32_ref&gt;, \n &lt;tf.Variable 'rnn/basic_rnn_cell/bias:0' shape=(5,) dtype=float32_ref&gt;]\n"
"df['Float_a'] = pd.cut(x=df['Float_a'],bins=10)\n\nbins = 10\ndf['Float_a'] = pd.cut(x=df['Float_a'],bins=bins, labels=[f'bin_{i}' for i in range(bins)])\n"
'plot_conv_heat_map(1 - model_prediction, ...)\n\n# last layer in VGG model\nx = layers.Dense(classes, activation=\'softmax\', name=\'predictions\')(x)\n\nmodel_prediction = model.output[:, np.argmax(preds[0])]\n                                        \\\n                                         \\___ finds the index of the neuron with maximum output\n\nplot_conv_heat_map(model_prediction, ...)\n\nplot_conv_heat_map(1 - model_prediction, ...)\n\ntemp_model = layers.Dense(2, activation="softmax")(temp_model)\n'
'## initialize chatter bot\nbot = ChatBot(\n    \'robot\',\n    storage_adapter=\'chatterbot.storage.SQLStorageAdapter\',\n    preprocessors=[\n        \'chatterbot.preprocessors.clean_whitespace\',\n    ],\n    logic_adapters=[\n        {\n            \'import_path\': \'chatterbot.logic.BestMatch\',\n            \'default_response\': \'I am sorry, but I do not understand.\',\n            \'maximum_similarity_threshold\': 0.90,\n            \'statement_comparison_function\': chatterbot.comparisons.levenshtein_distance,\n            \'response_selection_method\': chatterbot.response_selection.get_first_response\n        },\n        \'chatterbot.logic.MathematicalEvaluation\'\n    ],\n    database_uri=\'sqlite:///database.db\',\n    read_only=True\n)\n\n\n## training corpus list\n## Disable these two lines below AFTER first run when a *.db file is generated in project directory\ntrainer = ChatterBotCorpusTrainer(bot)\ntrainer.train("static/chatterbot_data.yml")\n'
'padded_shapes = ([9000], ())#None.\n\n padded_shapes = ([9000], ())#None.  # this line throws tracing error as the shape of text is varying for each step in an epoch.\n    # as the data size is varying, tf.function will start retracing it\n    # For the demonstration, I used 9000 as max length, but please change it accordingly \n'
"true_categories = tf.concat([y for x, y in test_dataset], axis=0)\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\n\ndata, info = tfds.load('iris', split='train',\n                       as_supervised=True,\n                       shuffle_files=True,\n                       with_info=True)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = data.take(120).batch(4).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = data.skip(120).take(30).batch(4).prefetch(buffer_size=AUTOTUNE)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(info.features['label'].num_classes, activation='softmax')\n    ])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n              metrics='accuracy')\n\nhistory = model.fit(train_dataset, validation_data=test_dataset, epochs=50, verbose=0)\n\ny_pred = model.predict(test_dataset)\n\narray([[2.2177568e-05, 3.0841196e-01, 6.9156587e-01],\n       [4.3539176e-06, 1.2779665e-01, 8.7219906e-01],\n       [1.0816366e-03, 9.2667454e-01, 7.2243840e-02],\n       [9.9921310e-01, 7.8686583e-04, 9.8775059e-09]], dtype=float32)\n\npredicted_categories = tf.argmax(y_pred, axis=1)\n\n&lt;tf.Tensor: shape=(30,), dtype=int64, numpy=\narray([2, 2, 2, 0, 2, 2, 2, 2, 1, 1, 2, 0, 0, 2, 1, 1, 1, 2, 0, 2, 1, 2,\n       1, 0, 2, 0, 1, 2, 1, 0], dtype=int64)&gt;\n\ntrue_categories = tf.concat([y for x, y in test_dataset], axis=0)\n\n[&lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 1, 0], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([2, 2, 2, 2], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 1, 0], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 2, 1, 1], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 0, 2], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 2, 1, 0], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int64, numpy=array([2, 0, 1, 2], dtype=int64)&gt;,\n &lt;tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 0], dtype=int64)&gt;]\n\nconfusion_matrix(predicted_categories, true_categories)\n\narray([[ 9,  0,  0],\n       [ 0,  9,  0],\n       [ 0,  2, 10]], dtype=int64)\n\n8/8 [==============================] - 0s 785us/step - loss: 0.1907 - accuracy: 0.9333\n\n              precision    recall  f1-score   support\n           0       1.00      1.00      1.00         8\n           1       0.82      1.00      0.90         9\n           2       1.00      0.85      0.92        13\n    accuracy                           0.93        30\n   macro avg       0.94      0.95      0.94        30\nweighted avg       0.95      0.93      0.93        30\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\n\ndata, info = tfds.load('iris', split='train',\n                       as_supervised=True,\n                       shuffle_files=True,\n                       with_info=True)\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = data.take(120).batch(4).prefetch(buffer_size=AUTOTUNE)\ntest_dataset = data.skip(120).take(30).batch(4).prefetch(buffer_size=AUTOTUNE)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(info.features['label'].num_classes, activation='softmax')\n    ])\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', \n              metrics='accuracy')\n\nhistory = model.fit(train_dataset, validation_data=test_dataset, epochs=50, verbose=0)\n\ny_pred = model.predict(test_dataset)\n\npredicted_categories = tf.argmax(y_pred, axis=1)\n\ntrue_categories = tf.concat([y for x, y in test_dataset], axis=0)\n\nconfusion_matrix(predicted_categories, true_categories)\n"
"def _contents(items, laplace=False):\n    # count occurrences of values\n    counts = {}\n    for item in items:\n        counts[item] = counts.get(item,0) + 1.0\n    # normalize\n    for k in counts:\n        if laplace:\n            counts[k] += 1.0\n            counts[k] /= (len(items)+len(counts))\n        else:\n            counts[k] /= len(items)\n    return counts\n\n# Estimate P(value|class,dim)\nnb.p_conditional[i][j] = _contents(values, True)\n\n# without\n&gt;&gt;&gt; carmodel.p_conditional\n[[{'Red': 0.40000000000000002, 'Yellow': 0.59999999999999998},\n  {'SUV': 0.59999999999999998, 'Sports': 0.40000000000000002},\n  {'Domestic': 0.59999999999999998, 'Imported': 0.40000000000000002}],\n [{'Red': 0.59999999999999998, 'Yellow': 0.40000000000000002},\n  {'SUV': 0.20000000000000001, 'Sports': 0.80000000000000004},\n  {'Domestic': 0.40000000000000002, 'Imported': 0.59999999999999998}]]\n\n# with\n&gt;&gt;&gt; carmodel.p_conditional\n[[{'Red': 0.42857142857142855, 'Yellow': 0.5714285714285714},\n  {'SUV': 0.5714285714285714, 'Sports': 0.42857142857142855},\n  {'Domestic': 0.5714285714285714, 'Imported': 0.42857142857142855}],\n [{'Red': 0.5714285714285714, 'Yellow': 0.42857142857142855},\n  {'SUV': 0.2857142857142857, 'Sports': 0.7142857142857143},\n  {'Domestic': 0.42857142857142855, 'Imported': 0.5714285714285714}]]\n"
"&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer as CV\n&gt;&gt;&gt; count_vect=CV()\n&gt;&gt;&gt; docs_new = ['Modern Warfare 2', 'Modern Warfare 3', 'Modern Warfare 1', 'Modern Warfare 4', 'Modern Warfare A', 'Modern Warfare 44', 'Modern Warfare AA', 'Modern Warfare', 'Mahjong Kakutou Club', 'Mass Effect 2']\n&gt;&gt;&gt; new_counts = count_vect.fit_transform(docs_new)\n&gt;&gt;&gt; count_vect.inverse_transform(new_counts)\n[array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'44', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'aa', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'club', u'kakutou', u'mahjong'], \n      dtype='&lt;U7'), array([u'effect', u'mass'], \n      dtype='&lt;U7')]\n\n&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer as CV\n&gt;&gt;&gt; count_vect=CV(token_pattern=r'(?u)\\b\\w+\\b')\n&gt;&gt;&gt; docs_new = ['Modern Warfare 2', 'Modern Warfare 3', 'Modern Warfare 1', 'Modern Warfare 4', 'Modern Warfare A', 'Modern Warfare 44', 'Modern Warfare AA', 'Modern Warfare', 'Mahjong Kakutou Club', 'Mass Effect 2']\n&gt;&gt;&gt; new_counts = count_vect.fit_transform(docs_new)\n&gt;&gt;&gt; count_vect.inverse_transform(new_counts)\n[array([u'2', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'3', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'1', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'4', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'a', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'44', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'aa', u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'modern', u'warfare'], \n      dtype='&lt;U7'), array([u'club', u'kakutou', u'mahjong'], \n      dtype='&lt;U7'), array([u'2', u'effect', u'mass'], \n      dtype='&lt;U7')]\n"
"   feature1 one of numpy.linspace(1,2,11) = [1., 1.1, 1.2, 1.3, ..., 2]\n   feature2 one of True or False\n\n    Your examples have all the possible linear combinations \n    You have sum(all gini nodes) = 0\n    The future examples are inside the condition boundaries of the training set\n\n    You covered all possible examples\n    All possible examples are labeled correctly\n\n    if f1 &gt; 3: f1 = 'many'\n    if 3 &lt;= f1 &lt; 0 = 'little'\n"
'from sklearn import preprocessing\ntrdata = preprocessing.scale(trdata) \n'
'&gt;&gt;&gt; xs = np.array(["NY", "LA", "GA"])\n&gt;&gt;&gt; \'\'.join(\'1\' if f else \'0\' for f in np.in1d(xs, \'NY GA\'.split(\' \')))\n\'101\'\n\n&gt;&gt;&gt; \'\'.join(np.where(np.in1d(xs, \'NY GA\'.split(\' \')), \'1\', \'0\'))\n\'101\'\n'
"import numpy as np\nimport scipy       #older versions may require `import scipy.sparse`\n\nmatrix = np.matrix(np.random.randn(10, 5))\nmaxes = matrix.argmax(axis=1).A1           \n                      # was .A[:,0], slightly faster, but .A1 seems more readable\nn_rows = len(matrix)  # could do matrix.shape[0], but that's slower\ndata = np.ones(n_rows)\nrow = np.arange(n_rows)\nsparse_matrix = scipy.sparse.coo_matrix((data, (row, maxes)), \n                                        shape=matrix.shape, \n                                        dtype=np.int8)\n\nsparse_matrix.todense()\n\nmatrix([[0, 0, 0, 0, 1],\n        [0, 0, 1, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 0, 0, 1],\n        [1, 0, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [0, 0, 0, 1, 0],\n        [0, 1, 0, 0, 0],\n        [1, 0, 0, 0, 0],\n        [0, 0, 0, 1, 0]], dtype=int8)\n\nmatrix([[ 1.41049496,  0.24737968, -0.70849012,  0.24794031,  1.9231408 ],\n        [-0.08323096, -0.32134873,  2.14154425, -1.30430663,  0.64934781],\n        [ 0.56249379,  0.07851507,  0.63024234, -0.38683508, -1.75887624],\n        [-0.41063182,  0.15657594,  0.11175805,  0.37646245,  1.58261556],\n        [ 1.10421356, -0.26151637,  0.64442885, -1.23544526, -0.91119517],\n        [ 0.51384883,  1.5901419 ,  1.92496778, -1.23541699,  1.00231508],\n        [-2.42759787, -0.23592018, -0.33534536,  0.17577329, -1.14793293],\n        [-0.06051458,  1.24004714,  1.23588228, -0.11727146, -0.02627196],\n        [ 1.66071534, -0.07734444,  1.40305686, -1.02098911, -1.10752638],\n        [ 0.12466003, -1.60874191,  1.81127175,  2.26257234, -1.26008476]])\n"
'data/\n    positive/     # class label\n        1.txt     # arbitrary filename\n        2.txt\n        ...\n    negative/\n        1.txt\n        2.txt\n        ...\n    ...\n'
'joblib.dump(neigh, FName)\n\nneigh = joblib.load(FName)\nneigh.predict([[1.1]])\n'
"sample = df[['catA','catB','catC']]\nsample = df.apply(lambda col: col.str.strip())\n\ncatA_US   catA_CA ... cat_B_chiquita_banana   cat_B_morningstar_tomato ... catC_China ...\n1         0           1                       0                            1   \n...\n\nimport scipy.sparse as sp\nvect = CountVectorizer(ngram_range=(1, 3))\ntrain = sp.hstack(sample.apply(lambda col: vect.fit_transform(col)))\n"
'forest.setp_param( oob_score    = True,   # set True to be able to read\n                   #                      #     oob-samples score\n                   random_state = 2015    # set so as to keep retesting\n                   #                      #     possible / meaniningfull on\n                   #                      #     an otherwise randomised\n                   #                      #     learner construction\n                   )\n\ndef printLDF( aPopulationSET ):\n    LDF_example, LDF_counts = np.unique( aPopulationSET, return_counts = True )\n    GDF_sum_scaler          = float( LDF_counts.sum() )\n    for i in xrange( LDF_example.shape[0] ):\n        print "{0: &gt; 6d}: {1: &gt; 6d} x {2: &gt; 15.2f}            {3: &gt; 15.4f} % {4: &gt; 15.1f} %".format( i, LDF_counts[i], LDF_example[i], 100 * LDF_counts[i] / GDF_sum_scaler, 100 * LDF_counts[:i].sum() / GDF_sum_scaler )\n    return\n\n&gt;&gt;&gt; printLDF( forest.estimators_[:].predict( anExample ) )\n\ndef prediction_up_dn_intervals( aPredictorMODEL,                        # &gt;&gt;&gt; http://blog.datadive.net/prediction-intervals-for-random-forests/\n                                X_,                                     # aStateVECTOR: X_sampled\n                                aPredictorOutputIDX =  0,               # (4,2,2) -&gt; singleQUAD ( LONG.TP/SL, SHORT.TP/SL ) &lt;-- idxMAP( \'LONG\', \'TP\', 1 )\n                                aRequiredPercentile = 95\n                                ):                                      \n    err_dn      = []\n    err_up      = []\n    #-----------------------------------------------------------------------------------------------\n    if len( X_.shape ) == 1:                                            # for a single X_example run\n        preds   = []\n        for pred in aPredictorMODEL.estimators_:\n            preds.append( pred.predict( X_ )[0,aPredictorOutputIDX] )   # de-array-ification\n\n        err_dn.append( np.percentile( preds,       ( 100 - aRequiredPercentile ) / 2. ) )\n        err_up.append( np.percentile( preds, 100 - ( 100 - aRequiredPercentile ) / 2. ) )\n    else:\n        #------------------------------------------------------------------------------------------\n        for x in xrange( len( X_ ) ):                                   # for a multi X_example run\n            preds   = []\n            for pred in aPredictorMODEL.estimators_:\n                preds.append( pred.predict( X_[x] )[0,aPredictorOutputIDX] ) # de-array-ification\n\n            err_dn.append( np.percentile( preds,       ( 100 - aRequiredPercentile ) / 2. ) )\n            err_up.append( np.percentile( preds, 100 - ( 100 - aRequiredPercentile ) / 2. ) )\n    #-----------------------------------------------------------------------------------------------\n    return err_up, err_dn\n\n#numba.jit( \'f8(&lt;&lt;OBJECT&gt;&gt;,f8[:,:],f8[:,:],i8,f8)\' )                    # &lt;&lt;OBJECT&gt;&gt; prevents JIT\ndef getPredictionsOnINTERVAL(   aPredictorENGINE,                       # a MULTI-OBJECTIVE PREDICTOR -&gt; a singleQUAD or a full 4-QUAD (16,0) &lt;-(4,2,2)\n                                X_,\n                                y_GndTRUTH,                             # (4,2,2) -&gt; (16,0) a MULTI-OBJECTIVE PREDICTOR\n                                aPredictionIDX  =  0,                   # (4,2,2) -&gt; singleQUAD ( LONG.TP/SL, SHORT.TP/SL ) &lt;-- idxMAP( \'LONG\', \'TP\', 1 )\n                                percentile      = 75\n                                ):\n    """\n    |&gt;&gt;&gt; getPredictionsOnINTERVAL( loc_PREDICTOR, X_sampled, y_sampled, idxMAP( "LONG", "TP", 1 ), 75 )     1.0                         +0:01:29.375000\n    |&gt;&gt;&gt; getPredictionsOnINTERVAL( loc_PREDICTOR, X_sampled, y_sampled, idxMAP( "LONG", "TP", 1 ), 55 )     0.9992532724237898          +0:03:59.922000\n    |&gt;&gt;&gt; getPredictionsOnINTERVAL( loc_PREDICTOR, X_sampled, y_sampled, idxMAP( "LONG", "TP", 1 ), 50 )     0.997100939998243           +0:09:16.328000\n    |&gt;&gt;&gt; getPredictionsOnINTERVAL( loc_PREDICTOR, X_sampled, y_sampled, idxMAP( "LONG", "TP", 1 ),  5 )     0.31375735746288325         +0:01:16.422000\n    """\n    correct_on_interval = 0                                                 # correct        = 0. ____________________- faster to keep asINTEGER ... +=1 and only finally make DIV on FLOAT(s) in RET\n    #ruth               = y_                                                # Y[idx[trainsize:]]\n    err_up, err_dn      = prediction_up_dn_intervals(   aPredictorENGINE,   # ( rf,\n                                                        X_,                 #   X[idx[trainsize:]],\n                                                        aPredictionIDX,     #   idxMAP( "LONG", "TP", 1 ),\n                                                        percentile          #   percentile = 90\n                                                        )                   #   )\n\n    #-------------------------------------------------------------------# for a single X_ run\n    if ( len( X_.shape ) == 1 ):\n        if ( err_dn[0] &lt;= y_GndTRUTH[aPredictionIDX] &lt;= err_up[0] ):\n            return 1.\n        else:\n            return 0.\n    #-------------------------------------------------------------------# for a multi X_ run\n    for i, val in enumerate( y_GndTRUTH[:,aPredictionIDX] ):            # enumerate( truth )\n        if err_dn[i] &lt;= val &lt;= err_up[i]:\n            correct_on_interval += 1\n    #-------------------------------------------------------------------\n    return correct_on_interval / float( y_GndTRUTH.shape[0] )           # print correct / len( truth )\n\ndef mapPredictionsOnINTERVAL(   aPredictorENGINE,                       #\n                                X_,\n                                y_GndTRUTH,\n                                aPredictionIDX      =  0,\n                                aPercentilleSTEP    =  5\n                                ):\n    for aPercentille in xrange( aPercentilleSTEP, 100, aPercentilleSTEP ):\n        Quotient = getPredictionsOnINTERVAL( aPredictorENGINE, X_, y_GndTRUTH, aPredictionIDX, aPercentille )\n        print "{0: &gt; 3d}-percentil   {1: &gt; 6.3f} %".format( aPercentille, 100 * Quotient )\n        """\n          5%  0.313757\n         10%  0.420847\n         15%  0.510191\n         20%  0.628481\n         25%  0.719758\n         30%  0.839058\n         35%  0.909646\n         40%  0.963454\n         45%  0.986603\n         50%  0.997101\n         55%  0.999253\n         60%  0.999912\n         65%  1.000000 &gt;&gt;&gt; RET/JIT\n         70%  1.000000 xxxxxxxxxxxxxx \n         75%  1.000000 xxxxxxxxxxxxxx       ???? .fit( X_, y_[:,8:12] ) # .fit() on HORIZON-T0+3???? ... y_GndTRUTH.shape[1] v/s .predict().shape[1]\n        """\n        if ( Quotient == 1 ):\n             return\n'
'def update_weights(w,x,y, learning_rate):\n    inner_product = 0.0    \n    for f_ in range(len(x)):\n        inner_product += (w[f_] * x[f_])\n    dloss = inner_product - y\n    for f_ in range(len(x)):\n        w[f_] += (learning_rate * (-x[f_] * dloss))\n    return w\n'
'class ColumnSelector(TransformerMixin):\n   ...\n\n   def transform(self, data_frame):\n       return data_frame[[self.column]]\n\nimport numpy as np\na = np.array([[1,0],\n              [0,1]])\nb = np.array([2,3])\nprint np.hstack((a,b))\n# ValueError: all the input arrays must have same number of dimensions\n\nprint np.hstack((a, b[:, np.newaxis]))\n# array([[1, 0, 2],\n#        [0, 1, 3]])\n'
'import theano\nimport theano.tensor as T\nimport numpy\n\n\nclass Layer(object):\n    """\n    this is a layer in the mlp\n    it\'s not meant to predict the outcome hence it does not compute a loss.\n    apply the functions for negative log likelihood = cost on the output of the last layer\n    """\n\n    def __init__(self, input, n_in, n_out):\n        self.x = input\n        self.W = theano.shared(\n                value=numpy.zeros(\n                        (n_in, n_out),\n                        dtype=theano.config.floatX\n                ),\n                name="W",\n                borrow=True\n        )\n        self.b = theano.shared(\n                value=numpy.zeros(n_out,\n                                  dtype=theano.config.floatX),\n                name="b",\n                borrow=True\n        )\n\n        self.output = T.nnet.softmax(T.dot(self.x, self.W) + self.b)\n        self.params = [self.W, self.b]\n        self.input = input\n\n\ndef y_pred(output):\n    return T.argmax(output, axis=1)\n\n\ndef negative_log_likelihood(output, y):\n    return -T.mean(T.log(output)[T.arange(y.shape[0]), y])\n\n\ndef errors(output, y):\n    # check if y has same dimension of y_pred\n    if y.ndim != y_pred(output).ndim:\n        raise TypeError(\n                \'y should have the same shape as self.y_pred\',\n                (\'y\', y.type, \'y_pred\', y_pred(output).type)\n        )\n    # check if y is of the correct datatype\n    if y.dtype.startswith(\'int\'):\n        # the T.neq operator returns a vector of 0s and 1s, where 1\n        # represents a mistake in prediction\n        return T.mean(T.neq(y_pred(output), y))\n    else:\n        raise NotImplementedError()\n\ndata_x = numpy.matrix([[0, 0],\n                       [1, 0],\n                       [0, 1],\n                       [1, 1]])\n\ndata_y = numpy.array([0,\n                      0,\n                      0,\n                      1])\n\ntrain_set_x = theano.shared(numpy.asarray(data_x,\n                         dtype=theano.config.floatX),\n                         borrow=True)\n\ntrain_set_y = T.cast(theano.shared(numpy.asarray(data_y,\n                         dtype=theano.config.floatX),\n                         borrow=True),"int32")\n\nx = T.matrix("x")  # data\ny = T.ivector("y")  # labels\n\nclassifier = Layer(input=x, n_in=2, n_out=1)\n\ncost = negative_log_likelihood(classifier.output, y)\n\ng_W = T.grad(cost=cost, wrt=classifier.W)\ng_b = T.grad(cost=cost, wrt=classifier.b)\nindex = T.iscalar()\n\nlearning_rate = 0.15\n\nupdates = (\n    (classifier.W, classifier.W - learning_rate * g_W),\n    (classifier.b, classifier.b - learning_rate * g_b)\n)\n\ntrain_model = theano.function(\n        inputs=[index],\n        outputs=cost,\n        updates=updates,\n        givens={\n            x: train_set_x[index:index + 1],\n            y: train_set_y[index:index + 1]\n        }\n)\nvalidate_model = theano.function(\n        inputs=[index],\n        outputs=errors(classifier.output, y),\n        givens={\n            x: train_set_x[index:index + 1],\n            y: train_set_y[index:index + 1]\n        }\n)\n\n#train the model\nfor i in range(train_set_x.shape[0].eval()):\n    train_model(i)\n\nimport theano\nimport theano.tensor as T\nimport numpy\n\n\nclass Layer(object):\n    """\n    this is a layer in the mlp\n    it\'s not meant to predict the outcome hence it does not compute a loss.\n    apply the functions for negative log likelihood = cost on the output of the last layer\n    """\n\n    def __init__(self, input, n_in, n_out):\n        self.x = input\n        self.W = theano.shared(\n                value=numpy.zeros(\n                        (n_in, n_out),\n                        dtype=theano.config.floatX\n                ),\n                name="W",\n                borrow=True\n        )\n        self.b = theano.shared(\n                value=numpy.zeros(n_out,\n                                  dtype=theano.config.floatX),\n                name="b",\n                borrow=True\n        )\n\n        self.output = T.reshape(T.nnet.sigmoid(T.dot(self.x, self.W) + self.b), (input.shape[0],))\n        self.params = [self.W, self.b]\n        self.input = input\n\n\ndef y_pred(output):\n    return output\n\n\ndef negative_log_likelihood(output, y):\n    return T.mean(T.nnet.binary_crossentropy(output,y))\n\n\ndef errors(output, y):\n    # check if y has same dimension of y_pred\n    if y.ndim != y_pred(output).ndim:\n        raise TypeError(\n                \'y should have the same shape as self.y_pred\',\n                (\'y\', y.type, \'y_pred\', y_pred(output).type)\n        )\n    # check if y is of the correct datatype\n    if y.dtype.startswith(\'int\'):\n        # the T.neq operator returns a vector of 0s and 1s, where 1\n        # represents a mistake in prediction\n        return T.mean(T.neq(y_pred(output), y))\n    else:\n        raise NotImplementedError()\n\ndata_x = numpy.matrix([[0, 0],\n                       [1, 0],\n                       [0, 1],\n                       [1, 1]])\n\ndata_y = numpy.array([0,\n                      0,\n                      0,\n                      1])\n\ntrain_set_x = theano.shared(numpy.asarray(data_x,\n                         dtype=theano.config.floatX),\n                         borrow=True)\n\ntrain_set_y = T.cast(theano.shared(numpy.asarray(data_y,\n                         dtype=theano.config.floatX),\n                         borrow=True),"int32")\n\nx = T.matrix("x")  # data\ny = T.ivector("y")  # labels\n\nclassifier = Layer(input=x, n_in=2, n_out=1)\n\ncost = negative_log_likelihood(classifier.output, y)\n\ng_W = T.grad(cost=cost, wrt=classifier.W)\ng_b = T.grad(cost=cost, wrt=classifier.b)\nindex = T.iscalar()\n\nlearning_rate = 0.15\n\nupdates = (\n    (classifier.W, classifier.W - learning_rate * g_W),\n    (classifier.b, classifier.b - learning_rate * g_b)\n)\n\ntrain_model = theano.function(\n        inputs=[index],\n        outputs=cost,\n        updates=updates,\n        givens={\n            x: train_set_x[index:index+1],\n            y: train_set_y[index:index+1]\n        }\n)\nvalidate_model = theano.function(\n        inputs=[index],\n        outputs=errors(classifier.output, y),\n        givens={\n            x: train_set_x[index:index + 1],\n            y: train_set_y[index:index + 1]\n        }\n)\n\n#train the model\nfor i in range(train_set_x.shape[0].eval()):\n    train_model(i)\n'
'import numpy\nimport theano\nimport theano.tensor as T\n\n\ninput_data = numpy.matrix([[28, 1], [35, 2], [18, 1], [56, 2], [80, 3]])\noutput_data = numpy.matrix([1600, 2100, 1400, 2500, 3200])\n\nTS = theano.shared(input_data.astype(\'float32\'), "training-set")\nE = theano.shared(output_data.astype(\'float32\'), "expected")\nW1 = theano.shared(numpy.zeros((1, 2), dtype = \'float32\'))\n\nO = T.dot(TS, W1.T)\ncost = T.mean(T.sqr(E - O.T))\ngradient = T.grad(cost=cost, wrt=W1)\nupdate = [[W1, W1 - gradient * 0.0001]]\ntrain = theano.function([], cost, updates=update, allow_input_downcast=True, profile = True)\n\nfor i in range(1000):\n    train()\n\ntrain.profile.print_summary()\n\nMessage: LearnTheano.py:18\n  Time in 1000 calls to Function.__call__: 2.642968e-01s\n  Time in Function.fn.__call__: 2.460811e-01s (93.108%)\n  Time in thunks: 1.877530e-01s (71.039%)\n  Total compile time: 2.483290e+01s\n    Number of Apply nodes: 17\n    Theano Optimizer time: 2.818849e-01s\n       Theano validate time: 3.435850e-03s\n    Theano Linker time (includes C, CUDA code generation/compiling): 2.453926e+01s\n       Import time 1.241469e-02s\n\nTime in all call to theano.grad() 1.206994e-02s\nClass\n---\n&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;\n  34.8%    34.8%       0.065s       3.27e-05s     C     2000       2   theano.sandbox.cuda.blas.GpuGemm\n  28.8%    63.5%       0.054s       1.80e-05s     C     3000       3   theano.sandbox.cuda.basic_ops.GpuElemwise\n  12.9%    76.4%       0.024s       2.42e-05s     C     1000       1   theano.sandbox.cuda.basic_ops.GpuCAReduce\n  10.3%    86.7%       0.019s       1.93e-05s     C     1000       1   theano.sandbox.cuda.basic_ops.GpuFromHost\n   7.2%    93.9%       0.014s       1.36e-05s     C     1000       1   theano.sandbox.cuda.basic_ops.HostFromGpu\n   1.8%    95.7%       0.003s       1.13e-06s     C     3000       3   theano.sandbox.cuda.basic_ops.GpuDimShuffle\n   1.5%    97.2%       0.003s       2.81e-06s     C     1000       1   theano.tensor.elemwise.Elemwise\n   1.1%    98.4%       0.002s       1.08e-06s     C     2000       2   theano.compile.ops.Shape_i\n   1.1%    99.5%       0.002s       1.02e-06s     C     2000       2   theano.sandbox.cuda.basic_ops.GpuSubtensor\n   0.5%   100.0%       0.001s       9.96e-07s     C     1000       1   theano.tensor.opt.MakeVector\n   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n\nOps\n---\n&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;\n  25.3%    25.3%       0.047s       4.74e-05s     C     1000        1   GpuGemm{no_inplace}\n  12.9%    38.1%       0.024s       2.42e-05s     C     1000        1   GpuCAReduce{pre=sqr,red=add}{1,1}\n  12.8%    51.0%       0.024s       2.41e-05s     C     1000        1   GpuElemwise{mul,no_inplace}\n  10.3%    61.3%       0.019s       1.93e-05s     C     1000        1   GpuFromHost\n   9.5%    70.8%       0.018s       1.79e-05s     C     1000        1   GpuGemm{inplace}\n   8.2%    79.0%       0.015s       1.55e-05s     C     1000        1   GpuElemwise{Composite{((i0 / i1) / i2)}}[(0, 0)]\n   7.7%    86.7%       0.014s       1.44e-05s     C     1000        1   GpuElemwise{Composite{((i0 * i1) / i2)}}[(0, 1)]\n   7.2%    93.9%       0.014s       1.36e-05s     C     1000        1   HostFromGpu\n   1.5%    95.4%       0.003s       2.81e-06s     C     1000        1   Elemwise{Cast{float32}}\n   1.1%    96.5%       0.002s       1.02e-06s     C     2000        2   GpuSubtensor{int64}\n   1.0%    97.5%       0.002s       9.00e-07s     C     2000        2   GpuDimShuffle{x,x}\n   0.8%    98.3%       0.002s       1.59e-06s     C     1000        1   GpuDimShuffle{1,0}\n   0.7%    99.1%       0.001s       1.38e-06s     C     1000        1   Shape_i{0}\n   0.5%    99.6%       0.001s       9.96e-07s     C     1000        1   MakeVector\n   0.4%   100.0%       0.001s       7.76e-07s     C     1000        1   Shape_i{1}\n   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)\n\nApply\n------\n&lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;#call&gt; &lt;id&gt; &lt;Apply name&gt;\n  25.3%    25.3%       0.047s       4.74e-05s   1000     3   GpuGemm{no_inplace}(expected, TensorConstant{-1.0}, &lt;CudaNdarrayType(float32, matrix)&gt;, GpuDimShuffle{1,0}.0, TensorConstant{1.0})\n  12.9%    38.1%       0.024s       2.42e-05s   1000     5   GpuCAReduce{pre=sqr,red=add}{1,1}(GpuGemm{no_inplace}.0)\n  12.8%    51.0%       0.024s       2.41e-05s   1000    13   GpuElemwise{mul,no_inplace}(GpuDimShuffle{x,x}.0, GpuDimShuffle{x,x}.0)\n  10.3%    61.3%       0.019s       1.93e-05s   1000     7   GpuFromHost(Elemwise{Cast{float32}}.0)\n   9.5%    70.8%       0.018s       1.79e-05s   1000    16   GpuGemm{inplace}(&lt;CudaNdarrayType(float32, matrix)&gt;, TensorConstant{-9.99999974738e-05}, GpuElemwise{Composite{((i0 * i1) / i2)}}[(0, 1)].0, training-set, TensorConstant{1.0})\n   8.2%    79.0%       0.015s       1.55e-05s   1000    12   GpuElemwise{Composite{((i0 / i1) / i2)}}[(0, 0)](GpuCAReduce{pre=sqr,red=add}{1,1}.0, GpuSubtensor{int64}.0, GpuSubtensor{int64}.0)\n   7.7%    86.7%       0.014s       1.44e-05s   1000    15   GpuElemwise{Composite{((i0 * i1) / i2)}}[(0, 1)](CudaNdarrayConstant{[[-2.]]}, GpuGemm{no_inplace}.0, GpuElemwise{mul,no_inplace}.0)\n   7.2%    93.9%       0.014s       1.36e-05s   1000    14   HostFromGpu(GpuElemwise{Composite{((i0 / i1) / i2)}}[(0, 0)].0)\n   1.5%    95.4%       0.003s       2.81e-06s   1000     6   Elemwise{Cast{float32}}(MakeVector.0)\n   0.8%    96.3%       0.002s       1.59e-06s   1000     0   GpuDimShuffle{1,0}(training-set)\n   0.7%    97.0%       0.001s       1.38e-06s   1000     2   Shape_i{0}(expected)\n   0.7%    97.7%       0.001s       1.30e-06s   1000     8   GpuSubtensor{int64}(GpuFromHost.0, Constant{0})\n   0.6%    98.3%       0.001s       1.08e-06s   1000    11   GpuDimShuffle{x,x}(GpuSubtensor{int64}.0)\n   0.5%    98.8%       0.001s       9.96e-07s   1000     4   MakeVector(Shape_i{0}.0, Shape_i{1}.0)\n   0.4%    99.2%       0.001s       7.76e-07s   1000     1   Shape_i{1}(expected)\n   0.4%    99.6%       0.001s       7.40e-07s   1000     9   GpuSubtensor{int64}(GpuFromHost.0, Constant{1})\n   0.4%   100.0%       0.001s       7.25e-07s   1000    10   GpuDimShuffle{x,x}(GpuSubtensor{int64}.0)\n   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)\n'
'from sklearn import svm\nimport numpy as np \nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nraw_data1 = open("Dataset.csv")\nraw_data2 = open("Result.csv")\ndataset1 = np.loadtxt(raw_data1,delimiter=",")\nresult1 = np.loadtxt(raw_data2,delimiter=",")\n\nclf = svm.NuSVC(kernel=\'rbf\',nu=0.01)\nX_train, X_test, y_train, y_test = train_test_split(dataset1,result1, test_size=0.25, random_state=42)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred, normalize=True, sample_weight=None)\n'
"from sklearn.svm import SVC\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.cross_validation import KFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import datasets\nimport numpy as np\n\nnewsgroups = datasets.fetch_20newsgroups(\n                subset='all',\n                categories=['alt.atheism', 'sci.space']\n         )\nX = newsgroups.data\ny = newsgroups.target\n\nTD_IF = TfidfVectorizer()\nX_scaled = TD_IF.fit_transform(X, y)\ngrid = {'C': np.power(10.0, np.arange(-1, 1))}\ncv = KFold(y_scaled.size, n_folds=5, shuffle=True, random_state=241) \nclf = SVC(kernel='linear', random_state=241)\n\ngs = GridSearchCV(estimator=clf, param_grid=grid, scoring='accuracy', cv=cv)\ngs.fit(X_scaled, y)\n"
'f_i = (f_i - mean(f_i)) / std(f_i)\n'
'prediction_op = tf.round(Output)\n\nprint(sess.run(prediction_op, feed_dict={X: np.array([[1., 0.]])}))\n'
"def predict_win(X, y, x):\n    win = np.reshape(X,(len(X), 1))  # &lt;----This line\n\n    svr_lin = SVR(kernel= 'linear', C= 1e3)\n    svr_poly = SVR(kernel= 'poly', C= 1e3, degree= 2)\n    svr_rbf = SVR(kernel= 'rbf', C= 1e3, gamma= 0.1) \n    svr_rbf.fit(X, y) \n    svr_lin.fit(X, y)\n    svr_poly.fit(X, y)\n\n    plt.scatter(X, y, color= 'black', label= 'Data') \n    plt.plot(y, svr_rbf.predict(X), color= 'red', label= 'RBF model') \n    plt.plot(y,svr_lin.predict(X), color= 'green', label= 'Linear model') \n    plt.plot(y,svr_poly.predict(X), color= 'blue', label= 'Polynomial model') \n    plt.xlabel('X, other features')\n    plt.ylabel('win')\n    plt.title('Support Vector Regression')\n    plt.legend()\n    plt.show()\n\n    return svr_rbf.predict(x)[0], svr_lin.predict(x)[0], svr_poly.predict(x)[0]\n\nfrom sklearn import svm\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef predict_win(X,y,x):\n    svr_lin = svm.SVR(kernel='linear',C=1e3)\n    svr_poly = svm.SVR(kernel='poly',C=1e3, degree=2)\n    svr_rbf = svm.SVR(kernel='rbf',C=1e3,gamma=0.1)\n    svr_rbf.fit(X,y)\n    svr_lin.fit(X,y)\n    svr_poly.fit(X,y)\n\n    plt.plot(y,svr_rbf.predict(X),color='red',label='RBF model')\n    plt.plot(y,svr_lin.predict(X),color='green',label='Linear model')\n    plt.plot(y,svr_poly.predict(X),color='blue', label='Polynomial model')\n    plt.xlabel('X, other features')\n    plt.ylabel('win')\n    plt.title('Support Vector Regression')\n    plt.legend()\n    plt.show()\n    return [svr_rbf.predict(x)[0],svr_lin.predict(x)[0],svr_poly.predict(x)[0]]\n\ndf = pd.read_csv('data.csv')\n\ndata_np_array = df.values\n\ny = np.ndarray.copy(data_np_array[:,6])\nXleft = np.ndarray.copy(data_np_array[:,:6])\nXright = np.ndarray.copy(data_np_array[:,7:])\nX = np.hstack((Xleft,Xright))\n\nx0 = np.ndarray.copy(X[0,:])\nxp = predict_win(X,y,x0)\n\npercent_off = [min(data_np_array[0,2],prediction)/max(data_np_array[0,2],prediction) for prediction in xp]\n\ndf = pd.read_csv('data.csv')\n\ndata_np_array = df.values\n\ny = np.ndarray.copy(data_np_array[:,6])\nXleft = np.ndarray.copy(data_np_array[:,:6])\nXright = np.ndarray.copy(data_np_array[:,7:])\nX = np.hstack((Xleft,Xright))\n"
'ind = sub2ind(size(delta3), 1 : num_examples, y + 1);\ndelta3(ind) = delta3(ind) - 1;\n\nymatrix = full(sparse(1 : num_examples, y + 1, 1, size(delta3, 1), size(delta3, 2));\ndelta3 = probs - ymatrix;\n\npip install sklearn\n\neasy_install sklearn\n\npip install numpy\n\neasy_install numpy\n'
"multi_metrics = MulticlassMetrics(rdd)\nprint 'fMeasure: ', multi_metrics.fMeasure(1.0,1.0)\n"
'pca = decomposition.PCA()\nsvd = decomposition.TruncatedSVD()\nsvm = SVC()\nn_components = [20, 40, 64]\n\npipe = Pipeline(steps=[(\'pca\', pca), (\'svd\', svd), (\'svm\', svm)])\n\n# Change params_grid -&gt; Instead of dict, make it a list of dict**\n# In the first element, pass `svd = None`, and in second `pca = None`\nparams_grid = [{\n\'svm__C\': [1, 10, 100, 1000],\n\'svm__kernel\': [\'linear\', \'rbf\'],\n\'svm__gamma\': [0.001, 0.0001],\n\'pca__n_components\': n_components,\n\'svd\':[None]\n},\n{\n\'svm__C\': [1, 10, 100, 1000],\n\'svm__kernel\': [\'linear\', \'rbf\'],\n\'svm__gamma\': [0.001, 0.0001],\n\'pca\':[None],\n\'svd__n_components\': n_components,\n\'svd__algorithm\':[\'randomized\']\n}]\n\ngrd = GridSearchCV(pipe, param_grid = params_grid)\n\n#Here I have changed the name to `preprocessor`\npipe = Pipeline(steps=[(\'preprocessor\', pca), (\'svm\', svm)])\n\n#Now assign both estimators to `preprocessor` as below:\nparams_grid = {\n\'svm__C\': [1, 10, 100, 1000],\n\'svm__kernel\': [\'linear\', \'rbf\'],\n\'svm__gamma\': [0.001, 0.0001],\n\'preprocessor\':[pca, svd],\n\'preprocessor__n_components\': n_components,\n}\n\ndef make_param_grids(steps, param_grids):\n\n    final_params=[]\n\n    # Itertools.product will do a permutation such that \n    # (pca OR svd) AND (svm OR rf) will become -&gt;\n    # (pca, svm) , (pca, rf) , (svd, svm) , (svd, rf)\n    for estimator_names in itertools.product(*steps.values()):\n        current_grid = {}\n\n        # Step_name and estimator_name should correspond\n        # i.e preprocessor must be from pca and select.\n        for step_name, estimator_name in zip(steps.keys(), estimator_names):\n            for param, value in param_grids.get(estimator_name).iteritems():\n                if param == \'object\':\n                    # Set actual estimator in pipeline\n                    current_grid[step_name]=[value]\n                else:\n                    # Set parameters corresponding to above estimator\n                    current_grid[step_name+\'__\'+param]=value\n        #Append this dictionary to final params            \n        final_params.append(current_grid)\n\nreturn final_params\n\n# add all the estimators you want to "OR" in single key\n# use OR between `pca` and `select`, \n# use OR between `svm` and `rf`\n# different keys will be evaluated as serial estimator in pipeline\npipeline_steps = {\'preprocessor\':[\'pca\', \'select\'],\n                  \'classifier\':[\'svm\', \'rf\']}\n\n# fill parameters to be searched in this dict\nall_param_grids = {\'svm\':{\'object\':SVC(), \n                          \'C\':[0.1,0.2]\n                         }, \n\n                   \'rf\':{\'object\':RandomForestClassifier(),\n                         \'n_estimators\':[10,20]\n                        },\n\n                   \'pca\':{\'object\':PCA(),\n                          \'n_components\':[10,20]\n                         },\n\n                   \'select\':{\'object\':SelectKBest(),\n                             \'k\':[5,10]\n                            }\n                  }  \n\n\n# Call the method on the above declared variables\nparam_grids_list = make_param_grids(pipeline_steps, all_param_grids)\n\n# The PCA() and SVC() used here are just to initialize the pipeline,\n# actual estimators will be used from our `param_grids_list`\npipe = Pipeline(steps=[(\'preprocessor\',PCA()), (\'classifier\', SVC())])  \n\ngrd = GridSearchCV(pipe, param_grid = param_grids_list)\ngrd.fit(X, y)\n'
"import matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.manifold import TSNE\n\ntrain = [&quot;is this good?&quot;, &quot;this is bad&quot;, &quot;some other text here&quot;, &quot;i am hero&quot;, &quot;blue jeans&quot;, &quot;red carpet&quot;, &quot;red dog&quot;,\n     &quot;blue sweater&quot;, &quot;red hat&quot;, &quot;kitty blue&quot;]\n\nvect = TfidfVectorizer()  \nX = vect.fit_transform(train)\nrandom_state = 1\nclf = KMeans(n_clusters=3, random_state = random_state)\ndata = clf.fit(X)\ncentroids = clf.cluster_centers_\n\ntsne_init = 'pca'  # could also be 'random'\ntsne_perplexity = 20.0\ntsne_early_exaggeration = 4.0\ntsne_learning_rate = 1000\nmodel = TSNE(n_components=2, random_state=random_state, init=tsne_init, perplexity=tsne_perplexity,\n         early_exaggeration=tsne_early_exaggeration, learning_rate=tsne_learning_rate)\n\ntransformed_centroids = model.fit_transform(centroids)\nprint transformed_centroids\nplt.scatter(transformed_centroids[:, 0], transformed_centroids[:, 1], marker='x')\nplt.show()\n"
"from sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.spatial import distance\n\ncount_vect = CountVectorizer(tokenizer=lambda x: x.split(', '))\n\nls = ['mango, guava, litchi, apple', \n      'mango, guava, litchi, orange',\n      'mango, guava, pineapple, grape',\n      'pen, pencil, book, copy, notebook',\n      'pen, pencil, book, copy, scale']\n\nX = count_vect.fit_transform(ls).toarray()\nD = distance.cdist(X, X, metric='cosine')\n\n[[ 0.  ,  0.25,  0.5 ,  1.  ,  1.  ],\n [ 0.25,  0.  ,  0.5 ,  1.  ,  1.  ],\n [ 0.5 ,  0.5 ,  0.  ,  1.  ,  1.  ],\n [ 1.  ,  1.  ,  1.  ,  0.  ,  0.2 ],\n [ 1.  ,  1.  ,  1.  ,  0.2 ,  0.  ]])\n\nfrom scipy.cluster import hierarchy\n\nD = distance.pdist(X, metric='cosine')\nZ = hierarchy.linkage(D, metric='euclidean')\npartition = hcluster.fcluster(Z, t=0.8, criterion='distance') # [2, 2, 2, 1, 1] \n\nfrom scipy.cluster.hierarchy import dendrogram\nimport matplotlib.pyplot as plt\n\nhierarchy.dendrogram(Z, above_threshold_color='#bcbddc',\n                     orientation='top')\n"
"model = sklearn.neural_network.MLPClassifier(\n    activation='relu', max_iter=10000, hidden_layer_sizes=(4,2))\nmodel.fit(xs, ys)\n\nscore: 1.0\npredictions: [0 1 1 0]\nexpected: [0 1 1 0]\n"
'x = \'...\' # Whatever prefix you called your chairs\nz = 100 # Load in 100 images for example\nlst = hoggify(x, z)\n\ndata = list_to_matrix(lst)\n\nlabels = ... # Define labels here as a numpy array\nclf = svmClassify(data, labels)\n\nxtest = \'...\' # Define new test prefix here\nztest = 50 # 50 test images\nlst_test = hoggify(xtest, ztest)\ntest_data = list_to_matrix(lst_test)\npred = clf.predict(test_data)\n\npred_training = clf.predict(data)\n\ncap = cv2.VideoCapture(0)\ndim = 128 # For HOG\n\nwhile True:\n    # Capture the frame\n    ret, frame = cap.read()\n\n    # Show the image on the screen\n    cv2.imshow(\'Webcam\', frame)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Convert the image into a HOG descriptor\n    gray = cv2.resize(gray, (dim, dim), interpolation = cv2.INTER_AREA)\n    features = hog.compute(gray)\n    features = features.T # Transpose so that the feature is in a single row\n\n    # Predict the label\n    pred = clf.predict(features)\n\n    # Show the label on the screen\n    print("The label of the image is: " + str(pred))\n\n    # Pause for 25 ms and keep going until you push q on the keyboard\n    if cv2.waitKey(25) == ord(\'q\'):\n        break\n\ncap.release() # Release the camera resource\ncv2.destroyAllWindows() # Close the image window\n'
'from sklearn import svm\nimport numpy as np\n\nclf = svm.SVC(kernel="linear")\n\nX = np.array([[1, 2], [3, 4], [5, 1], [6, 2]])\ny = np.array(["A", "B", "A", "C"])\n\nclf.fit(X, y)\n\nfor (intercept, coef) in zip(clf.intercept_, clf.coef_):\n    s = "y = {0:.3f}".format(intercept)\n    for (i, c) in enumerate(coef):\n        s += " + {0:.3f} * x{1}".format(c, i)\n\n    print(s)\n\ny = 2.800 + -0.200 * x0 + -0.800 * x1\ny = 7.000 + -1.000 * x0 + -1.000 * x1\ny = 1.154 + -0.462 * x0 + 0.308 * x1\n'
'graph_def = tf.GraphDef()\nwith tf.gfile.FastGFile("path/to/graphdef") as f:\n  s = f.read()\ngraph_def.ParseFromString(s)\n\ntf.train.import_meta_graph(\'checkpoint.meta\')\ntf.get_default_graph().as_graph_def()\n'
'from sklearn.model_selection import StratifiedKFold\ncv_stf = StratifiedKFold(n_splits=3)\nfor train_index, test_index in skf.split(X, y):\n    print("TRAIN:", train_index, "TEST:", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\nX_train, X_test, X_validate  = np.split(X, [int(.7*len(X)), int(.8*len(X))])\ny_train, y_test, y_validate  = np.split(y, [int(.7*len(y)), int(.8*len(y))])\n'
'model = DecisionTreeClassifier(criterion="entropy")\nmodel.fit(X_train, y_train)\n'
'# WRONG\ndef my_func(pred, tensor):\n  t = tf.matmul(tensor, tensor)\n  with tf.control_dependencies([pred]):\n    # The matmul op is created outside the context, so no control\n    # dependency will be added.\n    return t\n\n# RIGHT\ndef my_func(pred, tensor):\n  with tf.control_dependencies([pred]):\n    # The matmul op is created in the context, so a control dependency\n    # will be added.\n    return tf.matmul(tensor, tensor)\n'
"import matplotlib.pyplot as plt\n#assuming you got the correct conversion to a pandas dataframe  \npd.plotting.scatter_matrix(df,c=y_train,\n                       figsize=(15,15), marker='o', \n                       hist_kwds={'bins':20},s=60, \n                       alpha=.8,\n                       cmap=mglearn.cm3)\n\n\nplt.show()\n"
'y=ydata.astype(np.int32)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\n\n# Prepare the data\ninput_ = np.linspace(0, 10, 100)  # Don\'t assign user data to Python\'s input builtin\noutput = np.array([x**2 + 2 for x in input_])\n\n# Define model\na = 1\nb = 1\n\n# Train model\nN = input_.shape[0]  # Number of samples\nfor e in range(10):\n    loss = 0.\n    for x, y in zip(input_, output):\n        y_hat = a * x + b\n        a = a - 0.1 * (2. / N) * (-x) * (y - y_hat)\n        b = b - 0.1 * (2. / N) * (-1) * (y - y_hat)\n        loss +=  0.5 * ((y - y_hat) ** 2)\n    loss /= N\n\n    print("Epoch {:2d}\\tLoss: {:4f}".format(e, loss))\n\n\n# Predict on test data\ntest_input = np.linspace(0, 15, 150) # Training data [0-10] + test data [10 - 15]\ntest_output = np.array([x**2.0 + 2 for x in test_input])\nmodel_predictions = np.array([a*x + b for x in test_input])\n\nplt.plot(test_input, test_output, \'ro\')\nplt.plot(test_input, model_predictions, \'-\')\nplt.show()\n\nEpoch  0    Loss: 33.117127\nEpoch  1    Loss: 42.949756\nEpoch  2    Loss: 40.733332\nEpoch  3    Loss: 38.657764\nEpoch  4    Loss: 36.774646\nEpoch  5    Loss: 35.067299\nEpoch  6    Loss: 33.520409\nEpoch  7    Loss: 32.119958\nEpoch  8    Loss: 30.853112\nEpoch  9    Loss: 29.708126\n\nfor e in range(10):\n    for x, y in zip(input_, output):\n        y_hat = a * x + b\n        loss =  0.5 * ((y - y_hat) ** 2)\n        a = a - 0.01 * (2.) * (-x) * (y - y_hat)\n        b = b - 0.01 * (2.) * (-1) * (y - y_hat)\n\n    print("Epoch {:2d}\\tLoss: {:4f}".format(e, loss))\n\nEpoch  0    Loss: 0.130379\nEpoch  1    Loss: 0.123007\nEpoch  2    Loss: 0.117352\nEpoch  3    Loss: 0.112991\nEpoch  4    Loss: 0.109615\nEpoch  5    Loss: 0.106992\nEpoch  6    Loss: 0.104948\nEpoch  7    Loss: 0.103353\nEpoch  8    Loss: 0.102105\nEpoch  9    Loss: 0.101127\n'
'h1_w = tf.Variable(tf.random_normal([input_dim, h1_dim], stddev=0.1))\n'
"&gt;&gt;&gt; from sklearn.preprocessing import LabelEncoder\n\n&gt;&gt;&gt; c = ['France', 'UK', 'US', 'US', 'UK', 'China', 'France']\n&gt;&gt;&gt; enc = LabelEncoder().fit(c)\n&gt;&gt;&gt; encoded = enc.transform(c)\n&gt;&gt;&gt; encoded\narray([1, 2, 3, 3, 2, 0, 1])\n\n&gt;&gt;&gt; encoded.transform(['France'])\narray([1])\n\n&gt;&gt;&gt; enc.inverse_transform(encoded)\narray(['France', 'UK', 'US', 'US', 'UK', 'China', 'France'], dtype='&lt;U6')\n\nimport pickle\n\nwith open('enc.pickle', 'wb') as file:\n    pickle.dump(enc, file, pickle.HIGHEST_PROTOCOL)\n"
'Variable Name: CUDA_PATH \nVariable Value: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\n'
'print(mlp.predict(scaler.transform(test_val)))\n'
'Layer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_2 (Conv1D)            (None, 3000, 75)          450       \n_________________________________________________________________\nmax_pooling1d_1 (MaxPooling1 (None, 600, 75)           0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 45000)             0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 45001     \n=================================================================\nTotal params: 45,451\nTrainable params: 45,451\nNon-trainable params: 0\n_________________________________________________________________\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv1d_4 (Conv1D)            (None, 3000, 75)          450       \n_________________________________________________________________\nmax_pooling1d_3 (MaxPooling1 (None, 600, 75)           0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 16)                5888      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 17        \n=================================================================\nTotal params: 6,355\nTrainable params: 6,355\nNon-trainable params: 0\n_________________________________________________________________\n'
'import scipy\nfrom sklearn.metrics import accuracy_score\n\ndef thr_to_accuracy(thr, Y_test, predictions):\n   return -accuracy_score(Y_test, np.array(predictions&gt;thr, dtype=np.int))\n\nbest_thr = scipy.optimize.fmin(thr_to_accuracy, args=(Y_test, predictions), x0=0.5)\n'
"import tensorflow as tf\n\n\n#load the data\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data', validation_size=0)\n\n#considering only first 2 data points\nimg = mnist.train.images[:2]\nx = tf.reshape(img, shape=[-1, 28, 28, 1]) # -1 refers to standard feature which is equivalent to 28*28*1 here\n"
'(x2 - x1) * (y1 + y2) / 2 \n'
'example = tf.train.Example(\n                features = tf.train.Features(feature={           \n    "label": tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n    "mfcc": tf.train.Feature(float_list=tf.train.FloatList(value=mfcc.flatten())),\n    "shape": tf.train.Feature(int64_list=tf.train.Int64List(value=mfcc.shape))\n                }))\n'
'A.index_add_(0, B, C)\n'
'yHat[i] =sol_val[0][0]*xArr[i][1] + sol_val[1][0] \n\nyHat[i] =sol_val[0][0]*xArr[i][0] + sol_val[1][0]*xArr[i][1]\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom numpy import *\n\nimport tensorflow as tf\n\ntrX = np.linspace(0, 1, 100) \ntrY= trX + np.random.normal(0,1,100)\n#print(\'trY = \', trY)\n\nsess = tf.Session()\nxArr = []\nyArr = []\nfor i in range(len(trX)):\n    xArr.append([1.0,float(trX[i])])\n    yArr.append(float(trY[i]))\n\nxMat = mat(xArr); \nyMat = mat(yArr).T\n\nA_tensor = tf.constant(xMat)\nb_tensor = tf.constant(yMat)\n\n#print("A_Tensor = xMat = ", sess.run(A_tensor))\n#print("B_Tensor = yMat = ", sess.run(b_tensor))\n\nm = shape(xMat)[0]\nweights = mat(eye((m)))\nk = 0.01\nyHat = zeros(m)\nfor i in range(m):\n    for j in range(m):\n        diffMat = xMat[i]- xMat[j,:]\n        weights[j,j] = exp(diffMat*diffMat.T/(-2.0*k**2))\n    weights_tensor = tf.constant(weights)    \n    # Matrix inverse solution\n    wA = tf.matmul(weights_tensor, A_tensor)\n    tA_A = tf.matmul(tf.transpose(A_tensor), wA)\n    tA_A_inv = tf.matrix_inverse(tA_A)\n    wb = tf.matmul(weights_tensor, b_tensor)\n    tA_wb = tf.matmul(tf.transpose(A_tensor), wb)\n    solution = tf.matmul(tA_A_inv, tA_wb)\n    sol_val = sess.run(solution)\n    #plt.plot(sol_val, \'b\')\n    #plt.show()\n    #print("Sol_Val = ", sol_val)\n    #print("Sol_Val[0][0] = ", sol_val[0][0])\n    #print("Sol_Val[1][0] = ", sol_val[1][0])\n    #print(\'xArr[i] = \', np.array(xArr[i]))\n    #print(\'xArr[i][0] = \', np.array(xArr[i][0]))\n    #print(\'xArr[i][1] = \', np.array(xArr[i][1]))\n    #yHat[i] =sol_val[0][0]*xArr[i][1] + sol_val[1][0]\n    yHat[i] =sol_val[0][0]*xArr[i][0] + sol_val[1][0]*xArr[i][1]\n    #print("Weights = ", sess.run(weights_tensor))\n    #yHat[i] = np.array(xArr[i])*sol_val\n    #print(sol_val)\n\nplt.scatter(trX, trY) \n\nplt.plot(trX, yHat, \'r\')\nplt.show()\n'
'global_loss_list.append(global_loss.detach_())\n'
'    -&gt; 2380                 eval_name, val, is_higher_better = feval_ret // this is the return of mlogloss\n       2381                 ret.append((data_name, eval_name, val, is_higher_better))\n       2382         return ret\nTypeError: \'numpy.float64\' object is not iterable\n\ndef mlogloss(...):\n...\nreturn "my_loss_name", loss_value, False\n'
'import numpy as np\npredictions_all = np.array([tree.predict(X) for tree in rf.estimators_])\nprint(predictions_all.shape) #(1000, 6565) 1000 rows: one for every Tree, 6565 columns, one for every target\n'
"import numpy as np\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n\nprint('scikit-learn package version: {}'.format(sklearn.__version__))\n# scikit-learn package version: 0.21.3\n\nscaler = MinMaxScaler()\nx_sample = [-90, 90]\nscaler.fit(np.array(x_sample)[:, np.newaxis]) # reshape data to satisfy fit() method requirements\nx_data = np.array([[66,74,89], [1,44,53], [85,86,33], [30,23,80]])\n\nprint(scaler.transform(x_data))\n\n# [[0.86666667 0.91111111 0.99444444]\n# [0.50555556 0.74444444 0.79444444]\n# [0.97222222 0.97777778 0.68333333]\n# [0.66666667 0.62777778 0.94444444]]\n"
'import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport pickle\n\ncolumn_names = [\'Reynolds Number\', \'Blockage Ratio\', \'Prandtl Number\', \'Nusselt Number\', \'Drag Coefficient\']        \ndataset = pd.read_csv(\'WW.csv\', names=column_names, skipinitialspace=True)      \ntrain_dataset = dataset.sample(frac=0.9,random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)    \ntrain_labels = train_dataset.iloc[:, 3:].values\ntest_labels = test_dataset.iloc[:, 3:].values   \n\nprint(train_dataset)\nprint(test_dataset)                         \n\n\n# We are setting the input size as (None, 3)\ndef build_model():\n  model = keras.Sequential([\n    keras.layers.Dense(3, activation=\'relu\', input_shape=(3,)),\n    keras.layers.Dense(4, activation=\'relu\'),\n    keras.layers.Dense(2)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss=\'mse\',\n                optimizer=optimizer,\n                metrics=[\'mae\', \'mse\'])\n  return model\n\nmodel = build_model()\nmodel.summary()\n\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print(\'\')\n    print(\'.\', end=\'\')\n\nEPOCHS = 5000\n\nearly_stop = keras.callbacks.EarlyStopping(monitor=\'val_loss\', patience=500)\n\n# Note that the input only takes the first three columns\nhistory = model.fit(train_dataset.iloc[:,:3], train_labels, epochs=EPOCHS, validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\nmodel.save("model.h5")\n\ntest_predictions = model.predict(test_dataset.iloc[:,:3])\nprint(test_dataset)\ntest_dataset[\'Predicted Nu\'], test_dataset[\'Predicted CD\'] = test_predictions[:,0], test_predictions[:,1]\nprint("\\nPredicted\\n")\nprint(test_dataset)\n'
'z = (x - min)/(max - min)\n\nx = z(max - min) + min\n\ny_max_pre_normalize = max(label)\ny_min_pre_normalize = min(label) \n\ndef denormalize(y):\n    final_value = y(y_max_pre_normalize - y_min_pre_normalize) + y_min_pre_normalize \n    return final_value\n'
'from keras.callbacks import ModelCheckpoint\n\nfilepath="weights.best.hdf5"\n\ncheckpoint = ModelCheckpoint(filepath, monitor=\'val_acc\', verbose=1,save_best_only=True, mode=\'max\')\ncallbacks_list = [checkpoint]\n\n\nmodel.fit(x_train, y_train, epochs=200,,callbacks=callbacks_list, batch_size=32,\n          validation_data=(x_test, y_test))\n'
'import numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix\ny_true = np.array([[1, 0, 1, 0, 0],\n                   [0, 1, 0, 1, 1],\n                   [1, 1, 1, 0, 1]])\ny_pred = np.array([[1, 0, 0, 0, 1],\n                   [0, 1, 1, 1, 0],\n                   [1, 1, 1, 0, 0]])\n\nmultilabel_confusion_matrix(y_true, y_pred)\n# result:\narray([[[1, 0],\n        [0, 2]],\n\n       [[1, 0],\n        [0, 2]],\n\n       [[0, 1],\n        [1, 1]],\n\n       [[2, 0],\n        [0, 1]],\n\n       [[0, 1],\n        [2, 0]]])\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred))\n# result\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         2\n           1       1.00      1.00      1.00         2\n           2       0.50      0.50      0.50         2\n           3       1.00      1.00      1.00         1\n           4       0.00      0.00      0.00         2\n\n   micro avg       0.75      0.67      0.71         9\n   macro avg       0.70      0.70      0.70         9\nweighted avg       0.67      0.67      0.67         9\n samples avg       0.72      0.64      0.67         9\n\nimport numpy as np\n\ny_prob = np.array([[0.9, 0.05, 0.12, 0.23, 0.78],\n                   [0.11, 0.81, 0.51, 0.63, 0.34],\n                   [0.68, 0.89, 0.76, 0.43, 0.27]])\n\nthresh = 0.5\n\ny_pred = np.array([[1 if i &gt; thresh else 0 for i in j] for j in y_prob])\n\ny_pred\n# result:\narray([[1, 0, 0, 0, 1],\n       [0, 1, 1, 1, 0],\n       [1, 1, 1, 0, 0]])\n'
"import numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_features=10, random_state=0, n_classes=2, n_samples=1000, n_informative=8)\n\nclass MyLGBClassifier(LGBMClassifier):\n    def predict(self,X, threshold=0.22,raw_score=False, num_iteration=None,\n                pred_leaf=False, pred_contrib=False, **kwargs):\n        result = super(MyLGBClassifier, self).predict_proba(X, raw_score, num_iteration,\n                                    pred_leaf, pred_contrib, **kwargs)\n        predictions = [1 if p&gt;threshold else 0 for p in result[:,0]]\n        return predictions\n\nclf = MyLGBClassifier()\nclf.fit(X,y)\nclf.predict(X,threshold=2)  # just testing the implementation\n# [0,0,0,0,..,0,0,0]        # we get all zeros since we have set threshold as 2\n\nF1Scores = cross_val_score(MyLGBClassifier(random_state=101,learning_rate=0.01,max_depth=-1,min_data_in_leaf=60,num_iterations=2,num_leaves=5),X,y,cv=5,scoring='f1')\nF1Scores\n#array([0.84263959, 0.83333333, 0.8       , 0.78787879, 0.87684729])\n"
'np.random.seed(42) # for reproducibility\n\n#### Statsmodels\n# first artificially add intercept to x, as advised in the docs:\nx_ = sm.add_constant(x)\nres_sm = sm.Logit(y, x_).fit(method="ncg", maxiter=max_iter) # x_ here\nprint(res_sm.params)\n\nOptimization terminated successfully.\n         Current function value: 0.403297\n         Iterations: 5\n         Function evaluations: 6\n         Gradient evaluations: 10\n         Hessian evaluations: 5\n[-1.65822763  3.65065752]\n\n#### Scikit-Learn\n\nres_sk = LogisticRegression(solver=\'newton-cg\', max_iter=max_iter, fit_intercept=True, penalty=\'none\')\nres_sk.fit( x.reshape(n, 1), y )\nprint(res_sk.intercept_, res_sk.coef_)\n\n[-1.65822806] [[3.65065707]]\n'
'from sklearn.datasets import make_regression\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ndata = make_regression(\n    n_samples=100,\n    random_state=0\n)\nX, y = data[0], data[1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\n\nfit &lt;- glmnet(x=as.matrix(X_train), y=as.matrix(y_train))\ny_pred &lt;- predict(fit, newx = as.matrix(X_test))\ndim(y_pred)\n[1] 25 89\n\nfit &lt;- cv.glmnet(x=as.matrix(X_train), y=as.matrix(y_train))\ny_pred &lt;- predict(fit, newx = as.matrix(X_test))\ny_error &lt;- y_test - y_pred\nmean(as.matrix(y_error)^2)\n[1] 22.03504\n\ndim(y_error)\n[1] 25  1\nfit$lambda.1se\n[1] 1.278699\n\nfit &lt;- glmnet(x=as.matrix(X_train), y=as.matrix(y_train))\nsel = which.min(fit$lambda-1.278699)\ny_pred &lt;- predict(fit, newx = as.matrix(X_test))[,sel]\nmean((y_test - y_pred)^2)\ndim(y_error)\n\nmean(as.matrix((y_test - y_pred)^2))\n[1] 20.0775\n\nL = c(0.01,0.05,0.1,0.2,0.5,1,2)\nfit &lt;- cv.glmnet(x=as.matrix(X_train), y=as.matrix(y_train),lambda=L)\ny_pred &lt;- predict(fit, newx = as.matrix(X_test))\ny_error &lt;- y_test - y_pred\nmean(as.matrix(y_error)^2)\n[1] 0.003065869\n\nmodel = ElasticNetCV(l1_ratio=1,fit_intercept=True,alphas=[0.01,0.05,0.1,0.2,0.5,1,2])\nmodel.fit(X=X_train,y=y_train)\ny_pred = model.predict(X=X_test)\nmean_squared_error(y_true=y_test,y_pred=y_pred)\n\n0.0018007824874741929\n'
"X = np.random.uniform(0,1, (32,10)).astype('float32')\n\nx = Dense(1)\npred = x(X)\n\nW, b = x.get_weights()\n\n(pred == (tf.matmul(X, W) + b)).numpy().all() # TRUE\n"
'X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\n\nrkf = RepeatedKFold(n_splits=2, n_repeats=1, random_state=2652124)\nfor train_index, test_index in rkf.split(X):\n  print("TRAIN:", train_index, "TEST:", test_index)\n\nTRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\n\nTRAIN: [0 1] TEST: [2 3]\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\n'
'pca = PCA(n_components=8)\nX_pca = pca.fit_transform(X)\n\nmodel.fit(X_pca,y)\n'
'def predict(self, image):\n    return np.argmax(self.__feedforward(image.astype(bool).astype(int)))\n\nprediction = NN.predict(image)\n\n"""\nDeep Neural Net \n\n(Name: Classic Feedforward)\n\n"""\n\nimport numpy as np\nimport pickle, json\nimport sklearn.datasets\nimport random\nimport time\nimport os\n\n# cataloguing the various activation functions and their derivatives\n\ndef sigmoid(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\ndef relU(z):\n    return np.maximum(z, 0, z)\n\ndef relU_prime(z):\n    return z * (z &lt;= 0)\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef tanh_prime(z):\n    return 1 - (tanh(z) ** 2)\n\ndef transform_target(y):\n    t = np.zeros((10, 1))\n    t[int(y)] = 1.0\n    return t\n\n\nclass NeuralNet:\n\n    def __init__(self, layers, learning_rate=0.05, reg_lambda=0.01):\n        self.num_layers = len(layers)  \n\n        # initialising network parameters\n        self.layers = layers          \n        self.biases = [np.zeros((y, 1)) for y in layers[1:]]    \n        self.weights = [np.random.normal(loc=0.0, scale=0.1, size=(y, x)) \n                                       for x, y in zip(layers[:-1], layers[1:])]\n        self.learning_rate = learning_rate\n        self.reg_lambda = reg_lambda\n\n        # initialising network activation function \n        self.nonlinearity = relU\n        self.nonlinearity_prime = relU_prime\n\n    def __feedforward(self, x):\n        \'\'\' Returns softmax probabilities for the output layer \'\'\'\n\n        for w, b in zip(self.weights, self.biases):\n            x = self.nonlinearity(np.dot(w, np.reshape(x, (len(x), 1))) + b)\n\n        return np.exp(x) / np.sum(np.exp(x))\n\n    def __backpropagation(self, x, y):\n        \'\'\'\n        Perform the forward pass followed by backprop \n        :param x: input\n        :param y: target\n\n        \'\'\'\n\n        weight_gradients = [np.zeros(w.shape) for w in self.weights]\n        bias_gradients = [np.zeros(b.shape) for b in self.biases]\n\n        # forward pass - transform input to output softmax probabilities\n        activation = x\n        hidden_activations = [np.reshape(x, (len(x), 1))]\n        z_list = []\n\n        for w, b in zip(self.weights, self.biases):    \n            z = np.dot(w, np.reshape(activation, (len(activation), 1))) + b\n            z_list.append(z)\n            activation = self.nonlinearity(z)\n            hidden_activations.append(activation)\n\n        t = hidden_activations[-1] \n        hidden_activations[-1] = np.exp(t) / np.sum(np.exp(t))   # softmax layer\n\n        # backward pass\n        delta = (hidden_activations[-1] - y) * (z_list[-1] &gt; 0)\n        weight_gradients[-1] = np.dot(delta, hidden_activations[-2].T)\n        bias_gradients[-1] = delta\n\n        for l in range(2, self.num_layers):\n            z = z_list[-l]\n            delta = np.dot(self.weights[-l + 1].T, delta) * (z &gt; 0)\n            weight_gradients[-l] = np.dot(delta, hidden_activations[-l - 1].T)\n            bias_gradients[-l] = delta\n\n        return (weight_gradients, bias_gradients)\n\n    def __update_params(self, weight_gradients, bias_gradients):\n        \'\'\' Update network parameters after backprop step \'\'\'\n        for i in xrange(len(self.weights)):\n            self.weights[i] += -self.learning_rate * weight_gradients[i]\n            self.biases[i] += -self.learning_rate * bias_gradients[i]\n\n    def train(self, training_data, validation_data=None, epochs=10):\n        \'\'\' Train the network for `epoch` iterations \'\'\'\n\n        bias_gradients = None\n        for i in xrange(epochs):\n            random.shuffle(training_data)\n            inputs = [data[0] for data in training_data]\n            targets = [data[1] for data in training_data]\n\n            for j in xrange(len(inputs)):\n                (weight_gradients, bias_gradients) = self.__backpropagation(inputs[j], targets[j])\n                self.__update_params(weight_gradients, bias_gradients)\n\n            if validation_data: \n                random.shuffle(validation_data)\n                inputs = [data[0] for data in validation_data]\n                targets = [data[1] for data in validation_data]\n\n                for j in xrange(len(inputs)):\n                    (weight_gradients, bias_gradients) = self.__backpropagation(inputs[j], targets[j])\n                    self.__update_params(weight_gradients, bias_gradients)\n\n            print("{} epoch(s) done".format(i + 1))\n\n        print("Training done.")\n\n    def test(self, test_data):\n        test_results = [(np.argmax(self.__feedforward(x[0])), np.argmax(x[1])) for x in test_data]\n        return float(sum([int(x == y) for (x, y) in test_results])) / len(test_data) * 100\n\n    def dump(self, file):\n        pickle.dump(self, open(file, "wb"))\n\n\n\nif __name__ == "__main__":\n    total = 5000\n    training = int(total * 0.7)\n    val = int(total * 0.15)\n    test = int(total * 0.15)\n\n    mnist = sklearn.datasets.fetch_mldata(\'MNIST original\', data_home=\'./data\')\n\n    data = zip(mnist.data, mnist.target)\n    random.shuffle(data)\n    data = data[:total]\n    data = [(x[0].astype(bool).astype(int), transform_target(x[1])) for x in data]\n\n    train_data = data[:training]\n    val_data = data[training:training+val]\n    test_data = data[training+val:]\n\n    print "Data fetched"\n\n    NN = NeuralNet([784, 32, 10]) # defining an ANN with 1 input layer (size 784 = size of the image flattened), 1 hidden layer (size 32), and 1 output layer (size 10, unit at index i will predict the probability of the image being digit i, where 0 &lt;= i &lt;= 9)  \n\n    NN.train(train_data, val_data, epochs=5)\n\n    print "Network trained"\n\n    print "Accuracy:", str(NN.test(test_data)) + "%"\n'
"sqrt(n-len(k)-3)*abs(z(sigma_inverse[i][j])) &lt;= phi(1-alpha/2)\n\nif len(K) == 0: #CM is the correlation matrix, we have no variables conditioning (K has 0 length)\n    r = CM[i, j] #r is the partial correlation of i and j \nelif len(K) == 1: #we have one variable conditioning, not very different from the previous version except for the fact that i have not to compute the correlations matrix since i start from it, and pandas provide such a feature on a DataFrame\n    r = (CM[i, j] - CM[i, K] * CM[j, K]) / math.sqrt((1 - math.pow(CM[j, K], 2)) * (1 - math.pow(CM[i, K], 2))) #r is the partial correlation of i and j given K\nelse: #more than one conditioning variable\n    CM_SUBSET = CM[np.ix_([i]+[j]+K, [i]+[j]+K)] #subset of the correlation matrix i'm looking for\n    PM_SUBSET = np.linalg.pinv(CM_SUBSET) #constructing the precision matrix of the given subset\n    r = -1 * PM_SUBSET[0, 1] / math.sqrt(abs(PM_SUBSET[0, 0] * PM_SUBSET[1, 1]))\nr = min(0.999999, max(-0.999999,r)) \nres = math.sqrt(n - len(K) - 3) * 0.5 * math.log1p((2*r)/(1-r)) #estimating partial correlation with fisher's transofrmation\nreturn 2 * (1 - norm.cdf(abs(res))) #obtaining p-value\n"
'class MyAnalyzer(object):\n    @staticmethod\n    def analyze(s):\n        return s.split()\n\nv = CountVectorizer(analyzer=MyAnalyzer())\n'
"sum = np.tensordot(A, B, [[0,2],[0,1]])\n\nimport numpy as np\n\nn_examples = 100\nA = np.random.randn(n_examples, 20,30)\nB = np.random.randn(n_examples, 30,5)\n\ndef sol1():\n    sum = np.zeros([20,5])\n    for i in range(len(A)):\n      sum += np.dot(A[i],B[i])\n    return sum\n\ndef sol2():\n    return np.array(map(np.dot, A,B)).sum(0)\n\ndef sol3():\n    return np.einsum('nmk,nkj-&gt;mj',A,B)\n\ndef sol4():\n    return np.tensordot(A, B, [[2,0],[1,0]])\n\ndef sol5():\n    return np.tensordot(A, B, [[0,2],[0,1]])\n\ntimeit sol1()\n1000 loops, best of 3: 1.46 ms per loop\n\ntimeit sol2()\n100 loops, best of 3: 4.22 ms per loop\n\ntimeit sol3()\n1000 loops, best of 3: 1.87 ms per loop\n\ntimeit sol4()\n10000 loops, best of 3: 205 µs per loop\n\ntimeit sol5()\n10000 loops, best of 3: 172 µs per loop\n"
'from collections import defaultdict\n\nclass invertedIndex(object):\n\n  def __init__(self,docs):\n      self.docSets = defaultdict(set)\n      for index, doc in enumerate(docs):\n          for term in doc.split():\n              self.docSets[term].add(index)\n\n  def search(self,term):\n        return self.docSets[term]\n\ndocs=["new home sales top forecasts june june june",\n                     "home sales rise in july june",\n                     "increase in home sales in july",\n                     "july new home sales rise"]\n\ni=invertedIndex(docs)\nprint i.search("sales") # outputs: set([0, 1, 2, 3])\n'
"In [26]: cols = ['buying', 'maint', 'lug_boot', 'safety', 'class']\n\nIn [27]: df[cols].apply(lambda x: pd.factorize(x)[0])\nOut[27]: \n   buying  maint  lug_boot  safety  class\n0       0      0         0       0      0\n1       0      0         0       1      0\n2       0      0         0       2      0\n3       0      0         1       0      0\n4       0      0         1       1      0\n5       0      0         1       2      0\n\nIn [37]: dummies = []\n\nIn [38]: for col in cols:\n   ....:     dummies.append(pd.get_dummies(df[col]))\n   ....:     \n\nIn [39]: pd.concat(dummies, axis=1)\nOut[39]: \n   vhigh  vhigh  med  small  high  low  med  unacc\n0      1      1    0      1     0    1    0      1\n1      1      1    0      1     0    0    1      1\n2      1      1    0      1     1    0    0      1\n3      1      1    1      0     0    1    0      1\n4      1      1    1      0     0    0    1      1\n5      1      1    1      0     1    0    0      1\n"
"sgd = SGDClassifier(loss='log', alpha=alpha, verbose=1, shuffle=True, \n                warm_start=True, n_iter=1)\nsgd.partial_fit(X, y)\n# after 1st iteration\nsgd.partial_fit(X, y)\n# after 2nd iteration\n...\n"
'from sklearn.preprocessing import OneHotEncoder\nimport itertools\n\n# two example documents\ndocs = ["A B", "B B"]\n\n# split documents to tokens\ntokens_docs = [doc.split(" ") for doc in docs]\n\n# convert list of of token-lists to one flat list of tokens\n# and then create a dictionary that maps word to id of word,\n# like {A: 1, B: 2} here\nall_tokens = itertools.chain.from_iterable(tokens_docs)\nword_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n\n# convert token lists to token-id lists, e.g. [[1, 2], [2, 2]] here\ntoken_ids = [[word_to_id[token] for token in tokens_doc] for tokens_doc in tokens_docs]\n\n# convert list of token-id lists to one-hot representation\nvec = OneHotEncoder(n_values=len(word_to_id))\nX = vec.fit_transform(token_ids)\n\nprint X.toarray()\n\n[[ 1.  0.  0.  1.]\n [ 0.  1.  0.  1.]]\n'
'from sklearn.feature_extraction.text import CountVectorizer\n\ncol1 = ["cherry banana", "apple appricote", "cherry apple", "banana apple appricote cherry apple"]\n\ncv = CountVectorizer()\ncv.fit_transform(col1) \n#&lt;4x4 sparse matrix of type \'&lt;class \'numpy.int64\'&gt;\'\n#   with 10 stored elements in Compressed Sparse Row format&gt;\n\ncv.fit_transform(col1).toarray()\n#array([[0, 0, 1, 1],\n#       [1, 1, 0, 0],\n#       [1, 0, 0, 1],\n#       [2, 1, 1, 1]], dtype=int64)\n'
'&gt;&gt;&gt; numpy.polyfit(x, y, 2) # The 2 signifies a polynomial of degree 2\narray([  -1.04978546,  115.16698544,  236.16191491])\n\nx = ...\ny = ...\n\ndef ss(k):\n    # use numpy.linalg.norm to find the sum-of-squares error between y and kx\n'
'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({ \'Column A\' : [\'null\',20,30,40,\'null\'],\n                    \'Column B\' : [100,\'null\',30,50,\'null\']});\nprint df\n  Column A Column B\n0     null      100\n1       20     null\n2       30       30\n3       40       50\n4     null     null\n\n#replace null to NaN\ndf = df.replace("null", np.nan)\nprint df\n   Column A  Column B\n0       NaN       100\n1        20       NaN\n2        30        30\n3        40        50\n4       NaN       NaN\n\ndf[\'Column A\'] = df[\'Column A\'].combine_first(df[\'Column B\'])\ndf[\'Column B\'] = df[\'Column B\'].combine_first(df[\'Column A\'])\nprint df\n   Column A  Column B\n0       100       100\n1        20        20\n2        30        30\n3        40        50\n4       NaN       NaN\n\n#inconsistent values replace to NaN\ndf[df[\'Column A\'] != df[\'Column B\']] = np.nan\nprint df\n   Column A  Column B\n0       100       100\n1        20        20\n2        30        30\n3       NaN       NaN\n4       NaN       NaN\n'
'log_density -= np.log(N)\nreturn log_density\n\n1/N SUM_i K(x_i - x)\n\n1/(hN) SUM_i K((x_i - x)/h)\n\n case __pyx_e_7sklearn_9neighbors_9ball_tree_GAUSSIAN_KERNEL:\n\n /* "binary_tree.pxi":475\n *     cdef ITYPE_t k\n *     if kernel == GAUSSIAN_KERNEL:\n *         factor = 0.5 * d * LOG_2PI             # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;\n *     elif kernel == TOPHAT_KERNEL:\n *         factor = logVn(d)\n */\n    __pyx_v_factor = ((0.5 * __pyx_v_d) * __pyx_v_7sklearn_9neighbors_9ball_tree_LOG_2PI);\n    break;\n'
'X_new = SelectKBest(mutual_info_classif, k=100).fit_transform(X, y)\n'
"import numpy as np\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\n\n# a batch of inputs of 2 value each\ninputs = tf.placeholder(tf.float32, shape=[None, 2])\n\n# a batch of output of 1 value each\ndesired_outputs = tf.placeholder(tf.float32, shape=[None, 1])\n\n# [!] define the number of hidden units in the first layer\nHIDDEN_UNITS = 4 \n\n# connect 2 inputs to 3 hidden units\n# [!] Initialize weights with random numbers, to make the network learn\nweights_1 = tf.Variable(tf.truncated_normal([2, HIDDEN_UNITS]))\n\n# [!] The biases are single values per hidden unit\nbiases_1 = tf.Variable(tf.zeros([HIDDEN_UNITS]))\n\n# connect 2 inputs to every hidden unit. Add bias\nlayer_1_outputs = tf.nn.sigmoid(tf.matmul(inputs, weights_1) + biases_1)\n\n# [!] The XOR problem is that the function is not linearly separable\n# [!] A MLP (Multi layer perceptron) can learn to separe non linearly separable points ( you can\n# think that it will learn hypercurves, not only hyperplanes)\n# [!] Lets' add a new layer and change the layer 2 to output more than 1 value\n\n# connect first hidden units to 2 hidden units in the second hidden layer\nweights_2 = tf.Variable(tf.truncated_normal([HIDDEN_UNITS, 2]))\n# [!] The same of above\nbiases_2 = tf.Variable(tf.zeros([2]))\n\n# connect the hidden units to the second hidden layer\nlayer_2_outputs = tf.nn.sigmoid(\n    tf.matmul(layer_1_outputs, weights_2) + biases_2)\n\n# [!] create the new layer\nweights_3 = tf.Variable(tf.truncated_normal([2, 1]))\nbiases_3 = tf.Variable(tf.zeros([1]))\n\nlogits = tf.nn.sigmoid(tf.matmul(layer_2_outputs, weights_3) + biases_3)\n\n# [!] The error function chosen is good for a multiclass classification taks, not for a XOR.\nerror_function = 0.5 * tf.reduce_sum(tf.sub(logits, desired_outputs) * tf.sub(logits, desired_outputs))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.05).minimize(error_function)\n\nsess.run(tf.initialize_all_variables())\n\ntraining_inputs = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n\ntraining_outputs = [[0.0], [1.0], [1.0], [0.0]]\n\nfor i in range(20000):\n    _, loss = sess.run([train_step, error_function],\n                       feed_dict={inputs: np.array(training_inputs),\n                                  desired_outputs: np.array(training_outputs)})\n    print(loss)\n\nprint(sess.run(logits, feed_dict={inputs: np.array([[0.0, 0.0]])}))\nprint(sess.run(logits, feed_dict={inputs: np.array([[0.0, 1.0]])}))\nprint(sess.run(logits, feed_dict={inputs: np.array([[1.0, 0.0]])}))\nprint(sess.run(logits, feed_dict={inputs: np.array([[1.0, 1.0]])}))\n\n[[ 0.01759939]]\n[[ 0.97418505]]\n[[ 0.97734243]]\n[[ 0.0310041]]\n"
'for i in range(total_batch):\n\ntotal_batch = int(num_examples / mini_batch_size)\n\ntotal_batch = int(training_set_size / mini_batch_size)\n'
'import multiprocessing\npool = multiprocessing.Pool()\nl = pool.map_async(product_helper,job_args)\n'
'W_conv1 = weight_variable([5, 5, 1, 32])\n'
'index_dict = dict(zip(df.columns,range(df.shape[1])))\n\nnew_vector = np.zeroes(297)\ntry:\n    new_vector[index_dict[origin]] = 1\nexcept:\n    pass\ntry:\n    new_vector[index_dict[destination]] = 1\nexcept:\n    pass\ntry:\n    new_vector[index_dict[carrier]] = 1\nexcept:\n    pass\n'
'for x, y in train_generator:\n    x = None\n'
'from sklearn.feature_extraction.text import TfidfVectorizer\ntf = TfidfVectorizer(stop_words = None, token_pattern=\'(?u)\\\\b\\\\w\\\\w*\\\\b\')\n\u200b\nwords_list = ["歯","が","痛い"]\ntfidf_matrix =  tf.fit_transform(words_list)\nfeature_names = tf.get_feature_names() \nprint(feature_names)\n# [\'が\', \'歯\', \'痛い\']\n'
'def input_generator(folder, directories):\n\n    Streams = []\n    for i in range(len(directories)):\n        Streams.append(os.listdir(folder + "/" + directories[i]))\n        for j in range(len(Streams[i])):\n            Streams[i][j] = "Stream" + str(i + 1) + "/" + Streams[i][j]   \n        Streams[i].sort()\n\n\n    length = len(Streams[0])\n    index = 0\n    while True:\n        X = []\n        y = np.zeros(4)\n        for Stream in Streams:\n            image = load_img(folder + \'/\' + Stream[index], grayscale = True)\n            array = img_to_array(image).reshape((1,100,100,1))\n            X.append(array)\n        y[int(Stream[index][15]) - 1] = 1\n        index += 1\n        index = index % length\n        yield np.vstack(X), y\n'
"x = tf.placeholder('float32', (None, 784), name='x')\ny = tf.placeholder('float32', (None, 10), name='y')\nphase = tf.placeholder(tf.bool, name='phase')\n...\n\n# training (phase = 1)\nsess.run([loss, accuracy], \n         feed_dict={'x:0': mnist.train.images,\n                    'y:0': mnist.train.labels,\n                    'phase:0': 1})\n...\n\n# testing (phase = 0)\nsess.run([loss, accuracy],\n         feed_dict={'x:0': mnist.test.images,\n                    'y:0': mnist.test.labels,\n                    'phase:0': 0})\n"
'def get_activations(clf, X):\n        hidden_layer_sizes = clf.hidden_layer_sizes\n        if not hasattr(hidden_layer_sizes, "__iter__"):\n            hidden_layer_sizes = [hidden_layer_sizes]\n        hidden_layer_sizes = list(hidden_layer_sizes)\n        layer_units = [X.shape[1]] + hidden_layer_sizes + \\\n            [clf.n_outputs_]\n        activations = [X]\n        for i in range(clf.n_layers_ - 1):\n            activations.append(np.empty((X.shape[0],\n                                         layer_units[i + 1])))\n        clf._forward_pass(activations)\n        return activations\n'
'a = tf.Variable(name="a", initial_value=1.0, trainable=False)\nb = tf.Variable(name="b", initial_value=0.0, trainable=False)\ndependent_op = tf.assign(b, a * 3)\nwith tf.control_dependencies([dependent_op]):\n  c = a + 1\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  print(sess.run([c, b]))\n  print(sess.run([b]))\n'
'def softmax(A):\n    """\n    Computes a softmax function. \n    Input: A (N, k) ndarray.\n    Returns: (N, k) ndarray.\n    """\n    e = np.exp(A)\n    return e / np.sum(e, axis=1, keepdims=True)\n\n[[ 1.10627664  1.22384801  1.35391446]\n [ 1.49780395  1.65698552  1.83308438]]\n\n[[ 3.68403911]\n [ 4.98787384]]\n\ne = np.exp(A - np.sum(A, axis=1, keepdims=True))\n'
'reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\nloss = tf.add_n([base_loss] + reg_losses, name="loss")\n'
"import pandas as pd\nresults = pd.DataFrame(grid.cv_results_)\nresults.sort_values(by='rank_test_score', inplace=True)\n\nparams_2nd_best = results.loc[1, 'params']\nclf_2nd_best = grid.best_estimator_.set_params(**params_2nd_best)\n"
"model = Sequential()\nmodel.add(Bidirectional(LSTM(20, return_sequences=True),\n                    input_shape=Train_X.shape[1:]))\nmodel.add(Bidirectional(LSTM(10, return_sequences=True)))\nmodel.add(Dense(5))\nmodel.compile(loss='mae', \n          optimizer='rmsprop')\n"
'X_train, v1 = make_regression()\n'
"lenet_model = mx.mod.Module.load('model_directory/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n\nimport mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n\nlabels = ['a','b','c', 'd','e', 'f']\npredict('https://eximagesite/img_tst_a.jpg', labels )\n"
"# this assumes you're using numpy ndarrays\nword_vecs_matrix = get_wv_matrix()  # pseudo-code\ndef transform(x):\n    return word_vecs_matrix[x]\ntransformer = FunctionTransformer(transform)\n"
'print(self.conv1.state_dict()["weight"][0])\nprint(self.conv2[0].state_dict()["weight"][0])\n'
'pd.get_dummies(data["a"],prefix="a")\n\n    a_0 a_1 a_2\n0   1   0   0\n1   0   1   0\n2   0   0   1\n3   1   0   0\n\ndf = pd.DataFrame({"a": [0, 1, 2,0], "b": [0,1,4, 5], "c":[0,1,4, 5]})\ndata = df.as_matrix()\n\ncolumns = df.columns\nmy_result = pd.DataFrame()\ntemp = pd.DataFrame()\nfor runner in columns:\n    temp = pd.get_dummies(df[runner], prefix=runner)\n    my_result[temp.columns] = temp\nprint(my_result.columns)\n\n&gt;&gt;Index([\'a_0\', \'a_1\', \'a_2\', \'b_0\', \'b_1\', \'b_4\', \'b_5\', \'c_0\', \'c_1\', \'c_4\',\n       \'c_5\'],\n      dtype=\'object\')\n'
"def argNmax(a, N, axis=None):\n    if axis is None:\n        return np.argpartition(a.ravel(), -N)[-N]\n    else:\n        return np.take(np.argpartition(a, -N, axis=axis), -N, axis=axis)\n\nIn [66]: a\nOut[66]: \narray([[908, 770, 258, 534],\n       [399, 376, 808, 750],\n       [655, 654, 825, 355]])\n\nIn [67]: argNmax(a, N=2, axis=0)\nOut[67]: array([2, 2, 1, 0])\n\nIn [68]: argNmax(a, N=2, axis=1)\nOut[68]: array([1, 3, 0])\n\nIn [69]: argNmax(a, N=2) # global second largest index\nOut[69]: 10\n\ndef argNmin(a, N, axis=None):\n    if axis is None:\n        return np.argpartition(a.ravel(), N-1)[N-1]\n    else:\n        return np.take(np.argpartition(a, N-1, axis=axis), N-1, axis=axis)\n\nIn [105]: a\nOut[105]: \narray([[908, 770, 258, 534],\n       [399, 376, 808, 750],\n       [655, 654, 825, 355]])\n\nIn [106]: argNmin(a, N=2, axis=0)\nOut[106]: array([2, 2, 1, 0])\n\nIn [107]: argNmin(a, N=2, axis=1)\nOut[107]: array([3, 0, 1])\n\nIn [108]: argNmin(a, N=2)\nOut[108]: 11\n\nIn [70]: a = np.random.randint(0,99999,(1000,1000))\n\nIn [72]: %timeit np.argsort(a)[-2] # @pythonic833's soln\n10 loops, best of 3: 40.6 ms per loop\n\nIn [73]: %timeit argNmax(a, N=2)\n100 loops, best of 3: 2.12 ms per loop\n"
"from keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\n\ndata_dir = 'path/to/image/directory'  # path to the directory where the images are stored\nindex = 0  # select a number here\n\nig = ImageDataGenerator()\ngen = ig.flow_from_directory(data_dir, batch_size=1)  # if you want batch_size &gt; 1 you need to\n                                                      # add as many indices as your batch_size.\nimage, label = gen._get_batches_of_transformed_samples(np.array([index]))\nimage_name = gen.filenames[index]\n# do whatever you want with your image and label\n\nindex = next(gen.index_generator)\nimage, label = gen._get_batches_of_transformed_samples(index)\nimage_name = gen.filenames[index]\n\nprint(fname)\n"
"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n\ndf = pd.read_csv('iris_data.csv',header=None)\ndf.columns=[Sepal Length,Sepal Width,Petal Length,Petal Width,Class]\n\nenc=LabelEncoder()\ndf['Class']=enc.fit_transform(df['Class'])\nprint df.head(5)\n\nenc=LabelEncoder()\nenc_1=OneHotEncoder()\ndf['Class']=enc.fit_transform(df['Class'])\ndf['Class']=enc_1.fit_transform([df['Class']]).toarray()\nprint df.head(5)\n\nfor k in list(enc.classes_) :\n   print 'name ::{}, label ::{}'.format(k,enc.transform([k]))\n\ndf.to_csv('Processed_Irisdataset.csv',sep=',')\n"
'    train_x, val_x, train_y, val_x = train_test_split(x, y,random_state = 0)\n'
'from sklearn.neighbors.nearest_centroid import NearestCentroid\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\ny = np.array([1, 1, 1, 2, 2, 2])\nclf = NearestCentroid()\nclf.fit(X, y)\n\nprint(clf.centroids_)\n# [[-2.         -1.33333333]\n#  [ 2.          1.33333333]]\n'
'from sklearn.preprocessing import StandardScaler\nX_scaled = StandardScaler().fit_transform(X)\n'
"import tensorflow as tf\nimport numpy as np\nfrom tqdm import tqdm\n\n# define a random seed for (somewhat) reproducible results:\nseed = 0\nnp.random.seed(seed)\nprint('Creating Datasets:')\n\n# much faster dataset creation\nx_train = np.random.uniform(low=0, high=255, size=[10000, 3])\n# easier label creation\n# if the average color is greater than half the color space than use black, otherwise use white\n# classes:\n# white = 0\n# black = 1\ny_train = ((np.mean(x_train, axis=1) / 255.0) &gt; 0.5).astype(int)\n\n# now transform dataset to be within range [-1, 1] instead of [0, 255] \n# for numeric stability and quicker model training\nx_train = (2 * (x_train / 255)) - 1\n\ngraph = tf.Graph()\n\nwith graph.as_default():\n    # must do this within graph scope\n    tf.set_random_seed(seed)\n    # specify input dims for clarity\n    x = tf.placeholder(tf.float32, shape=[None, 3])\n    # y is now integer label [0 or 1]\n    y = tf.placeholder(tf.int32, shape=[None])\n    # use relu, usually better than sigmoid \n    activation_fn = tf.nn.relu\n    # from https://arxiv.org/abs/1502.01852v1\n    initializer = tf.initializers.variance_scaling(\n        scale=2.0, \n        mode='fan_in',\n        distribution='truncated_normal')\n    # better api to reduce clutter\n    l_1 = tf.layers.dense(\n        x,\n        10,\n        activation=activation_fn,\n        kernel_initializer=initializer)\n    l_2 = tf.layers.dense(\n        l_1,\n        10,\n        activation=activation_fn,\n        kernel_initializer=initializer)\n    l_3 = tf.layers.dense(\n        l_2,\n        5,\n        activation=activation_fn,\n        kernel_initializer=initializer)\n    y_logits = tf.layers.dense(\n        l_3,\n        2,\n        activation=None,\n        kernel_initializer=initializer)\n\n    y_ = tf.nn.softmax(y_logits)\n    # much better loss function for classification\n    loss = tf.reduce_mean(\n        tf.losses.sparse_softmax_cross_entropy(\n            labels=y, \n            logits=y_logits))\n    # much better default optimizer for new problems\n    # good learning rate, but probably can tune\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate=0.01)\n    # seperate train op for easier calling\n    train_op = optimizer.minimize(loss)\n\n    # tell tensorflow not to allocate all gpu memory at start\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth=True\n    with tf.Session(config=config) as sess:\n\n        sess.run(tf.global_variables_initializer())\n\n        print('Training:')\n\n        for step in tqdm(range(5000)):\n            index = np.random.randint(0, len(x_train) - 129)\n            feed_dict = {x : x_train[index:index+128], \n                         y : y_train[index:index+128]}\n            # can train and get loss in single run, much more efficient\n            _, b_loss = sess.run([train_op, loss], feed_dict=feed_dict)\n            if step % 1000 == 0:\n                print(b_loss)\n\n        while True:\n            inp1 = int(input('Enter R pixel color: '))\n            inp2 = int(input('Enter G pixel color: '))\n            inp3 = int(input('Enter B pixel color: '))\n            # scale to model train range [-1, 1]\n            model_input = (2 * (np.array([inp1, inp2, inp3], dtype=float) / 255.0)) - 1\n            if (model_input &gt;= -1).all() and (model_input &lt;= 1).all():\n                # y_ is now two probabilities (white_prob, black_prob) but they will sum to 1.\n                white_prob, black_prob = sess.run(y_, feed_dict={x : [model_input]})[0]\n                print('White prob: {:.2f} Black prob: {:.2f}'.format(white_prob, black_prob))\n            else:\n                print('Values not within [0, 255]!')\n\nCreating Datasets:\n2018-10-05 00:50:59.156822: I T:\\src\\github\\tensorflow\\tensorflow\\core\\platform\\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n2018-10-05 00:50:59.411003: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1405] Found device 0 with properties:\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:03:00.0\ntotalMemory: 8.00GiB freeMemory: 6.60GiB\n2018-10-05 00:50:59.417736: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1484] Adding visible gpu devices: 0\n2018-10-05 00:51:00.109351: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-10-05 00:51:00.113660: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:971]      0\n2018-10-05 00:51:00.118545: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:984] 0:   N\n2018-10-05 00:51:00.121605: I T:\\src\\github\\tensorflow\\tensorflow\\core\\common_runtime\\gpu\\gpu_device.cc:1097] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6370 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)\nTraining:\n  0%|                                                                                         | 0/5000 [00:00&lt;?, ?it/s]0.6222609\n 19%|██████████████▋                                                               | 940/5000 [00:01&lt;00:14, 275.57it/s]0.013466636\n 39%|██████████████████████████████                                               | 1951/5000 [00:02&lt;00:04, 708.07it/s]0.0067519126\n 59%|█████████████████████████████████████████████▊                               | 2971/5000 [00:04&lt;00:02, 733.24it/s]0.0028143923\n 79%|████████████████████████████████████████████████████████████▌                | 3935/5000 [00:05&lt;00:01, 726.36it/s]0.0073514087\n100%|█████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:07&lt;00:00, 698.32it/s]\nEnter R pixel color: 1\nEnter G pixel color: 1\nEnter B pixel color: 1\nWhite prob: 1.00 Black prob: 0.00\nEnter R pixel color: 255\nEnter G pixel color: 255\nEnter B pixel color: 255\nWhite prob: 0.00 Black prob: 1.00\nEnter R pixel color: 128\nEnter G pixel color: 128\nEnter B pixel color: 128\nWhite prob: 0.08 Black prob: 0.92\nEnter R pixel color: 126\nEnter G pixel color: 126\nEnter B pixel color: 126\nWhite prob: 0.99 Black prob: 0.01\n"
'arr = np.ndarray((1,80,80,1))#This is your tensor\narr_ = np.squeeze(arr) # you can give axis attribute if you wanna squeeze in specific dimension\nplt.imshow(arr_)\nplt.show()\n'
"def transform(self, raw_documents, copy=True):\n\n    # This is what you need.\n    check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')\n\n    X = super(TfidfVectorizer, self).transform(raw_documents)\n    return self._tfidf.transform(X, copy=False)\n\nfrom sklearn.utils.validation import check_is_fitted\n\ndef vectorize_data(texts):\n\n    try:\n        check_is_fitted(vectorizer, '_tfidf', 'The tfidf vector is not fitted')\n    except NotFittedError:\n        vectorizer.fit(texts)\n\n    # In all cases vectorizer if fit here, so just call transform()\n    vectorizer.transform(texts)\n"
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; result = np.random.rand(5,1)\n&gt;&gt;&gt; result\narray([[ 0.54719484],\n       [ 0.31675804],\n       [ 0.55151251],\n       [ 0.25014937],\n       [ 0.00724972]])\n&gt;&gt;&gt; result.argmax(axis=-1)\narray([0, 0, 0, 0, 0])\n&gt;&gt;&gt; (result &gt; 0.5).astype(int)\narray([[1],\n       [0],\n       [1],\n       [0],\n       [0]])\n&gt;&gt;&gt;\n'
'dfCorpusDescr = dfCorpus.fieldname\ndoc_list={i: nlp(i) for i in dfCorpus}\nwith open("filename.pickle", \'wb\') as pfile:\n    pickle.dump(doc_list, pfile, protocol=pickle.HIGHEST_PROTOCOL)\n'
"classifier = tf.estimator.DNNClassifier(..., model_dir='checkpoints-cnn')\n"
'label_map = (generator.class_indices)\n'
'# base model creation\nbase_model = applications.inception_v3.InceptionV3(include_top=False, \n                                                   weights=\'imagenet\',\n                                                   pooling=\'avg\', \n                                                   input_shape=(150, 150, 3))\n# adding custom Layers\nx = Dense(128, activation=\'relu\',input_shape=base_model.output_shape[1:],\n                    kernel_regularizer=regularizers.l2(0.001))(base_model.output)\nx = Dropout(0.60)(x)\nout = Dense(2, activation=\'sigmoid\')(x)\n\n# creating the final model\nmodel = Model(inputs=base_model.input, outputs=out)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\')\n\n# construct a new model to get the activations of custom layers\nnew_model = Model(model.inputs, [model.layers[-3].output,\n                                 model.layers[-2].output,\n                                 model.layers[-1].output])\n\n# predict one one random input sample\ninp = np.random.rand(1, 150, 150, 3)\noutput = new_model.predict([inp])\n\n# verify that\'s what we want\nprint(output[0].shape)  # shape of first dense layer output, prints: (1, 128) \nprint(output[1].shape)  # shape of dropout layer output, prints: (1, 128)\nprint(output[2].shape)  # shape of second dense layer output, prints: (1, 2)\n\nfrom keras import backend as K\n\nfunc = K.function(inputs=model.inputs + [K.learning_phase()],\n                  outputs=[model.layers[-3].output,\n                           model.layers[-2].output, \n                           model.layers[-1].output])\n\n# usage of the defined function: \n#     the inputs should be a *list* of input arrays\n#     plus 1 or 0 for the train/test mode\nsample_input = np.random.rand(1, 150, 150, 3)\n\n# train mode\noutput = func([sample_input, 1])\n\n# test mode\nouput = func([sample_input, 0])\n\nnew_model = Model(model.inputs, model.layers[-1].layers[1].output)\nnew_model.predict(...)\n\nValueError: Graph disconnected: cannot obtain value for tensor Tensor("dense_7_input:0", shape=(?, 2048), dtype=float32) at layer "dense_7_input". The following previous layers were accessed without issue: []\n'
"input_shape = (3,2)\nX = LSTM(124, activation = 'sigmoid', name='layer1', dropout = 0.4) (temp)\n\ninput_shape = (3,2)\n    X = LSTM(124, activation = 'sigmoid', name='layer1', dropout = 0.4,return_sequences=True) (temp)\n    X = LSTM(64, activation = 'sigmoid', name='layer2', dropout = 0.4) (X)\n\ndef create_model():\n    model = keras.models.Sequential()\n\n    model.add(keras.layers.CuDNNLSTM(512, input_shape=(3,2), return_sequences=True, name='inputlstm1'))\n    model.add(keras.layers.Dropout(0.2))\n\n    model.add(keras.layers.CuDNNLSTM(512, return_sequences=True,name='lstm2'))\n    model.add(keras.layers.Dropout(0.2))\n\n    # The last layer of Stacked LSTM need not to return the input sequences\n    model.add(keras.layers.CuDNNLSTM(512,name='lstm3'))\n    model.add(keras.layers.Dropout(0.2))\n\n    model.add(keras.layers.Dense(32, activation='relu', name='dense1'))\n    model.add(keras.layers.Dropout(0.2))\n\n    model.add(keras.layers.Dense(1, activation='softmax', name='denseoutput2'))\n\n\n    # Compile model\n    model.compile(\n        loss='mse',\n        optimizer='adam',\n        metrics=['accuracy'],\n    )\n    return model \n"
'import tensorflow as tf\n\nscaled = Lambda(lambda x: tf.where(x &gt;= 1000, tf.ones_like(x), x/1000.))(input_tensor)\n\nfrom keras import backend as K\n\ndef rescale(x):\n    mask = K.cast(x &gt;= 1000., dtype=K.floatx())\n    return mask + (x/1000.0) * (1-mask)\n\n#...\nscaled = Lambda(rescale)(input_tensor)\n\nfrom keras import backend as K\n\nscaled = Lambda(lambda x: K.switch(x &gt;= 1000., K.ones_like(x), x / 1000.))(input_tensor)\n'
'import tensorflow as tf\nfrom keras import backend as K\n\n\ndef custom_loss(y_true, y_pred):\n    y_pred = K.reshape(y_pred, (K.get_variable_shape(y_pred)[0], -1))\n    y_true = K.reshape(y_true, (K.get_variable_shape(y_true)[0], -1))\n    y_pred = K.std(y_pred, axis=0)\n    y_true = K.std(y_true, axis=0)\n    loss = (1 / 2) * (y_pred - y_true) ** 2\n\n    return loss\n\n\na = tf.constant([[1.0, 2., 3.]])\nb = tf.constant([[1., 2., 3.]])\nloss = custom_loss(a, b)\nloss = tf.Print(loss, [loss], "loss")\n\nwith tf.Session() as sess:\n    _ = sess.run([loss])\n\na = tf.placeholder(tf.float32, (None, 32))\n'
"from keras.layers import Input, Embedding, Subtract, Lambda\nimport keras.backend as K\nfrom keras.models import Model\n\ninput1 = Input((1,)) #word1\ninput2 = Input((1,)) #word2\n\nembeddingLayer = Embedding(...params...)\n\nword1 = embeddingLayer(input1)\nword2 = embeddingLayer(input2)\n\n#naive distance rule, subtract, expect zero difference\nword_distance = Subtract()([word1,word2])\n\n#reduce all dimensions to a single dimension\nword_distance = Lambda(lambda x: K.mean(x, axis=-1))(word_distance)\n\nmodel = Model([input1,input2], word_distance)\n\nmodel.compile(optimizer='adam', loss='mse')\n\nxTrain = entireText\nxTrain1 = entireText[:-1]\nxTrain2 = entireText[1:]\nyTrain = np.zeros((len(xTrain1),))\n\nmodel.fit([xTrain1,xTrain2], yTrain, .... more params.... ) \n"
'#THE MISSING STUFF\n#_________________________________________\nY_train = Y_train.reshape(5) #Dense layer contains a single unit so need to input single dimension array\nmax_len = len(charset)\nmax_features = embed-1\ninputshape = (max_features, max_len) #input shape didn\'t define. Embedding layer can accept 3D input by using input_shape\n#__________________________________________\n\nmodel = Sequential()\n#model.add(Embedding(len(charset), 10, input_length=14))\n\nmodel.add(Embedding(max_features, 10, input_shape=inputshape))#input_length=max_len))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation=\'linear\'))\nprint(model.summary())\n\noptimizer = Adam(lr=0.00025)\nlr_metric = get_lr_metric(optimizer)\nmodel.compile(loss="mse", optimizer=optimizer, metrics=[coeff_determination, lr_metric])\n\n\ncallbacks_list = [\n    ReduceLROnPlateau(monitor=\'val_loss\', factor=0.5, patience=5, min_lr=1e-15, verbose=1, mode=\'auto\',cooldown=0),\n    ModelCheckpoint(filepath="weights.best.hdf5", monitor=\'val_loss\', save_best_only=True, verbose=1, mode=\'auto\')]\n\nhistory =model.fit(x=X_train, y=Y_train,\n                              batch_size=10,\n                              epochs=10,\n                              validation_data=(X_test,Y_test),\n                              callbacks=callbacks_list)\n'
"from keras.layers import Dense, Flatten, LSTM, Activation\nfrom keras.layers import Dropout, RepeatVector, TimeDistributed\nfrom keras import Input, Model\n\nseq_length = 15\ninput_dims = 10\noutput_dims = 8 # number of classes\nn_hidden = 10\nmodel1_inputs = Input(shape=(seq_length,input_dims,))\nmodel1_outputs = Input(shape=(output_dims,))\n\nnet1 = LSTM(n_hidden, return_sequences=True)(model1_inputs)\nnet1 = LSTM(n_hidden, return_sequences=False)(net1)\nnet1 = Dense(output_dims, activation='relu')(net1)\nmodel1_outputs = net1\n\nmodel1 = Model(inputs=model1_inputs, outputs = model1_outputs, name='model1')\n\n## Fit the model\nmodel1.summary()\n\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)        (None, 15, 10)            0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 15, 10)            840       \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 10)                840       \n_________________________________________________________________\ndense_3 (Dense)              (None, 8)                 88        \n_________________________________________________________________\n"
'from copy import deepcopy    \nmodelList = []\nthisCNN = NNet()\n\nfor x in range(3):\n    train = torch.utils.data.DataLoader(Subset(train_set, indexes[x]), batch_size=32)\n    bb = trainMyNet(deepcopy(thisCNN), train, test)\n    modelList.append(list(bb.parameters()))\n\nprint modelList[0][1]\nprint modelList[1][1]\nprint modelList[2][1]\n'
'execution_path = "/home/priyanshu/PycharmProjects/untitled/images/"\n\nprediction = ImagePrediction()\nprediction.setModelTypeAsResNet()\nprediction.setModelPath( os.path.join(execution_path, "/home/priyanshu/Downloads/resnet50_weights_tf_dim_ordering_tf_kernels.h    5"))\nprediction.loadModel()\n\nfor eachfile in result:\n    id = eachfile[0]\n    print(id)\n    filename = "image.jpg"\nurl = eachfile[1]\nfilepath = "/home/priyanshu/PycharmProjects/untitled/images/"\nprint(filename)\nprint(url)\nprint(filepath)\ndl_img(url, filepath, filename)\n\npredictions, probabilities = prediction.predictImage(os.path.join(execution_path, "image.jpg"), result_count=1)\nfor eachPrediction, eachProbability in zip(predictions, probabilities):\n    per = 0.00\n    label = ""\n    print(eachPrediction, " : ", eachProbability)\n    label = eachPrediction\n    per = eachProbability\n\n    print("Label: " + label)\n    print("Per:" + str(per))\n    counter = counter + 1\n    print("Picture Number: " + str(counter))\n\n    sql1 = "UPDATE used_cars SET is_processed = \'1\' WHERE id = \'%s\'" % id\n    sql2 = "INSERT into label (used_car_image_id, object_label, percentage) " \\\n       "VALUE (\'%s\', \'%s\', \'%s\') " % (id, label, per)\n    print("done")\n\n    mycursor.execute(sql1)\n    mycursor.execute(sql2)\n\n    mydb.commit()\n    tracker.print_diff()\n'
'review_encoder = TimeDistributed(sentEncoder)(review_input)\n'
"model.add(Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1), adding='same'))\n"
'y_true = tf.constant([0.1, 0.2])\ny_pred = tf.constant([0.11, 0.19])\n\ncustom_loss(y_true, y_pred)                         # == 0.41316\ntf.keras.losses.binary_crossentropy(y_true, y_pred) # == 0.41317\n'
"for feature, importance in zip(countvect.get_feature_names(), rf.feature_importances_):\n    print('{}: {}'.format(feature, importance))\n"
"# Scale the test dataset\nX_test_scaled = transformer_x.transform(X_test)\n\n# Predict with the trained model\nprediction = lasso.predict(X_test_scaled)\n\n# Inverse transform the prediction\nprediction_in_dollars = transformer_y.inverse_transform(prediction)\n\nX_scaled = (X - median(X))/IQR(X)\ny_scaled = (y - median(y))/IQR(y)\n\na * X_scaled + b = y_scaled\n\n# Substituting X_scaled and y_scaled from the 1st equation\n# In this equation `median(X), IQR(X), median(y) and IQR(y) are plain numbers you already know from the training phase\na * (X - median(X))/IQR(X) + b = (y - median(y))/IQR(y)\n\na_new = (a * (X - median(X)) / (X * IQR(X))) * IQR(y)\nb_new = b * IQR(y) + median(y)\na_new * X + b_new = y\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import Lasso\n\ndf = pd.DataFrame({'Y':[5, -10, 10, .5, 2.5, 15], 'X1':[1., -2.,  2., .1, .5, 3], 'X2':[1, 1, 2, 1, 1, 1],\n              'X3':[6, 6, 6, 5, 6, 4], 'X4':[6, 5, 4, 3, 2, 1]})\n\nX = df[['X1','X2','X3','X4']]\ny = df[['Y']]\n\n#Scaling\ntransformer_x = RobustScaler().fit(X)\ntransformer_y = RobustScaler().fit(y)\nX_scal = transformer_x.transform(X)\ny_scal = transformer_y.transform(y)\n\n#LASSO\nlasso = Lasso()\nlasso = lasso.fit(X_scal, y_scal)\n\ndef pred_val(X_test):\n\n    print('X entered: ',)\n    print (X_test.values[0])\n\n    #Scale X value that user entered - by hand\n    med_X = X.median()\n    Q1_X = X.quantile(0.25)\n    Q3_X = X.quantile(0.75)\n    IQR_X = Q3_X - Q1_X\n    X_scaled = ((X_test - med_X)/IQR_X).fillna(0).values\n    print('X_test scaled by hand: ',)\n    print (X_scaled[0])\n\n    #Scale X value that user entered - by function\n    X_scaled2 = transformer_x.transform(X_test)\n    print('X_test scaled by function: ',)\n    print (X_scaled2[0])\n\n    #Intercept by hand\n    med_y = y.median()\n    Q1_y = y.quantile(0.25)\n    Q3_y = y.quantile(0.75)\n    IQR_y = Q3_y - Q1_y\n\n    a = lasso.coef_\n    coef_new = ((a * (X_test - med_X).values) / (X_test * IQR_X).values) * float(IQR_y)\n    coef_new = np.nan_to_num(coef_new)[0]\n\n    b = lasso.intercept_[0]\n    intercept_new = b * float(IQR_y) + float(med_y)\n\n    custom_pred = sum((coef_new * X_test.values)[0]) + intercept_new\n\n    pred = lasso.predict(X_scaled2)\n    final_pred = transformer_y.inverse_transform(pred.reshape(-1, 1))[0][0]\n\n\n    print('Original intercept: ', lasso.intercept_[0].round(2))\n    print('New intercept: ', intercept_new.round(2))\n    print('Original coefficients: ', lasso.coef_.round(2))\n    print('New coefficients: ', coef_new.round(2))\n    print('Your predicted value by function is: ', final_pred.round(2))\n    print('Your predicted value by hand is: ', custom_pred.round(2))\n\n\nX_test = pd.DataFrame([10,1,1,1]).T\nX_test.columns = ['X1', 'X2', 'X3', 'X4']\n\npred_val(X_test)\n\nX entered: \n[10  1  1  1]\n\nX_test scaled by hand: \n[ 5.96774194  0.         -6.66666667 -1.        ]\nX_test scaled by function: \n[ 5.96774194  0.         -6.66666667 -1.        ]\n\nOriginal intercept:  0.01\nNew intercept:  3.83\n\nOriginal coefficients:  [ 0.02  0.   -0.   -0.  ]\nNew coefficients:  [0.1 0.  0.  0. ]\n\nYour predicted value by function is:  4.83\nYour predicted value by hand is:  4.83\n"
'def sigmoid(x):\n  return 1/(1 + np.exp(-x))\n\nsigmoid(0)\n# 0.5\nsigmoid(20)\n# 0.99999999793884631\n\ndef your_sigmoid(x):\n  return x*(1-x)\n  return 1/(1 + np.exp(-x))\n\nyour_sigmoid(20)\n# -380\n\ndef Sigmoid_Derivative(x):\n    return sigmoid(x) * (1-sigmoid(x))\n'
'Big_model.layers[1].summary()   #this is inner_Model1.summary()\nBig_model.layers[2].summary()   #this is inner_Model2.summary()\n\ninner_Model1.save_weights(...)\ninner_Model2.save_weights(...)\n\ninner_Model1.load_weights(...)\ninner_Model2.load_weights(...)\n'
"# Make a copy to avoid changing original data\nX_valid_eval=X_valid.copy()\n# Remove the model from pipeline\neval_set_pipe = Pipeline(steps = [('preprocessor', preprocessor)])\n# fit transform X_valid.copy()\nX_valid_eval = eval_set_pipe.fit(X_train, y_train).transform (X_valid_eval)\n\nmy_model.fit(X_train, y_train, model__early_stopping_rounds=5, model__eval_metric = &quot;mae&quot;, model__eval_set=[(X_valid_eval, y_valid)])\n"
"keras_train = train_generator[0][0] #first image from first batch\nkeras_val = validation_generator[0][0]\n\nimg = cv2.imread(os.path.join(testing_dir, img_file))\nimg_resized = cv2.resize(img, (image_size, image_size), interpolation = cv2.INTER_AREA) \nimg1 = np.reshape(img_resized, \n                  (1, img_resized.shape[0], img_resized.shape[1], img_resized.shape[2]))    \nyour_image = img1[0]/255. #first image from your batch rescaled \n\nplt.imshow(keras_train)\nplt.plot()\nplt.imshow(keras_val)\nplt.plot()\nplt.imshow(your_image)\nplt.plot()\nprint(keras_train.max(), keras_val.max(), img1.max())\n\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image #use this instead of cv2   \nfrom keras.applications.resnet50 import preprocess_input #use this in the generators    \n\n#no rescale, only preprocessing function\ntrain_datagen = keras.preprocessing.image.ImageDataGenerator(\n                               rotation_range=15,\n                               width_shift_range=0.1,\n                               height_shift_range=0.1,\n                               shear_range=0.01,\n                               zoom_range=[0.9, 1.25],\n                               horizontal_flip=False,\n                               vertical_flip=False,\n                               fill_mode='reflect',\n                               data_format='channels_last',\n                               brightness_range=[0.5, 1.5],\n                               preprocessing_function=preprocess_input)\nvalidation_datagen = keras.preprocessing.image.ImageDataGenerator(\n                               preprocessing_function=preprocess_input)\n\nimg = image.load_img(img_path, target_size=(image_size,image_size))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n"
"bow4 = bow_transformer.transform([message4])\ntfidf4 = tfidf_transformer.transform(bow4)\ncounter = 0\npredicted = spam_detect_model.predict_proba(tfidf4)\nfor x in spam_detect_model.classes_: #classes_ gives you the labels,\n  proba  = round(predicted[0][counter],2)\n  if proba &gt; 0.01: #only return the labels with a prob of larger then 0,10%\n      print(x + ' probility '+ str(proba))\n  counter +=1 ```\n"
"df = pd.DataFrame({'workclass':['class1', np.nan, 'Some other class', 'class1', \n                                np.nan, 'class12', 'class2', 'class121'], \n                   'color':['red', 'blue', np.nan, 'pink',\n                            'green', 'magenta', np.nan, 'yellow']})\n# train test split of X\ndf_train = df[:3]\ndf_test = df[3:]\n\nprint(df_test)\n\n  workclass    color\n3    class1     pink\n4       NaN    green\n5   class12  magenta\n6    class2      NaN\n7  class121   yellow\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin\n\nclass InputColName(TransformerMixin):\n\n    def fit(self, X, y):\n        self.fill_with = X.columns\n        return self\n\n    def transform(self, X):\n        return np.where(X.isna(), 'No ' + self.fill_with, X)\n\npipeline = Pipeline(steps=[\n  ('inputter', InputColName())\n])\npipeline.fit(df_train)\n\nprint(pd.DataFrame(pipeline.transform(df_test), columns=df.columns))\n\n      workclass     color\n0        class1      pink\n1  No workclass     green\n2       class12   magenta\n3        class2  No color\n4      class121    yellow\n"
'import numpy as np\nimport cv2\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Activation, Conv2D, Flatten, GlobalAveragePooling2D, Dropout, Reshape\nfrom tensorflow.keras import optimizers\nimport os\nfrom tensorflow.keras import applications\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing.image import ImageDataGenerator\n\nhead_model = Reshape(2049000, )(head_model)\n\nhead_model = Reshape((2049000, ))(head_model)\n'
"opt = tf.keras.optimizers.SGD(learning_rate=0.001,decay=1e-5)\nmodel.compile(loss='categorical_crossentropy',optimizer= opt, metrics=['accuracy'])\n\n    opt = tf.keras.optimizers.RMSprop()\n    model.compile(loss='categorical_crossentropy',\n                  optimizer= opt,\n                  metrics=['accuracy'])\n"
'images = self.data.iloc[idx, 1:-1].values.astype(np.uint8).reshape((1, 16, 16))\n\nimport torchvision\n\nimages = torchvision.transforms.functional.to_tensor(\n    self.data.iloc[idx, 1:-1].values.astype(np.uint8).reshape((1, 16, 16))\n)\n'
'x = matrix([[0.],[0],[1]])\ntheta = matrix(zeros([3,1]))\nfor i in range(5):\n  grad = matrix(zeros([3,1]))\n  hess = matrix(zeros([3,3]))\n  [xfile, yfile] = [open(\'q1\'+a+\'.dat\', \'r\') for a in \'xy\']\n  for xline, yline in zip(xfile, yfile):\n    x.transpose()[0,:2] = [map(float, xline.split("  ")[1:3])]\n    y = float(yline)\n    hypoth = 1 / (1 + math.exp(theta.transpose() * x))\n    grad += (y - hypoth) * x\n    hess -= hypoth * (1 - hypoth) * x * x.transpose()\n  theta += inv(hess) * grad\nprint "done"\nprint theta\n'
"idf = log ( n_samples / df ) + 1\n\ndf += 1\nn_samples += 1\n\nidf = log ( n_samples / (1+df) )\n\nvocabulary = {'blue':0, 'sun':1, 'bright':2, 'sky':3}\nvectorizer = CountVectorizer(vocabulary=vocabulary) # You don't need stop_words if you use vocabulary\nvectorizer.fit_transform(train_set)\nprint 'Vocabulary:', vectorizer.vocabulary_\n# Vocabulary: {'blue': 0, 'sun': 1, 'bright': 2, 'sky': 3}\n"
'P = A / (1 + A)\n\nP /= P.sum(axis=1).reshape((-1, 1))\n'
'import numpy as np\nfrom sklearn.metrics import r2_score, mean_squared_error\nsecret_mses = []\n\ndef r2_secret_mse(estimator, X_test, y_test):\n    predictions = estimator.predict(X_test)\n    secret_mses.append(mean_squared_error(y_test, predictions))\n    return r2_score(y_test, predictions)\n\nX = np.random.randn(20, 10)\ny = np.random.randn(20)\n\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.linear_model import Ridge\n\nr2_scores = cross_val_score(Ridge(), X, y, scoring=r2_secret_mse, cv=5)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000,\n                           n_features=10,\n                           n_informative=3,\n                           n_redundant=0,\n                           n_repeated=0,\n                           n_classes=2,\n                           random_state=0,\n                           shuffle=False)\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\n\nforest.fit(X, y)\n\nimportances = forest.feature_importances_ #array with importances of each feature\n\nidx = np.arange(0, X.shape[1]) #create an index array, with the number of features\n\nfeatures_to_keep = idx[importances &gt; np.mean(importances)] #only keep features whose importance is greater than the mean importance\n#should be about an array of size 3 (about)\nprint features_to_keep.shape\n\nx_feature_selected = X[:,features_to_keep] #pull X values corresponding to the most important features\n\nprint x_feature_selected\n'
'sig(0) = 0.5\nsig(x &gt; 0) &gt; 0.5\nsig(x &lt; 0) &lt; 0.5\n\nfor i in range(max_iterations):\n    hx = s.sigmoid(np.dot(X, theta)) # this will probably be &gt; 0.5 initially\n    d = y - hx # then this will be "very" negative when y is 0\n    theta = theta + alpha*np.dot(np.transpose(X),d) # (1)\n    ll[i] = sum(y * np.log(hx) + (1-y) * np.log(1- hx))\n'
'bcw = bcw[bcw[7] != \'?\']\n\nfor col in bcw.columns:\n    if bcw[col].dtype != \'int64\':\n        print "Removing possible \'?\' in column %s..." % col\n        bcw = bcw[bcw[col] != \'?\']\n\n&gt;&gt;&gt; Removing possible \'?\' in column 6...\n'
"import pandas as pd\n\n#pandas\ndf = pandas.DataFrame(columns = ['Date','Unix','Ticker','DE Ratio'])\n\nsave = gather.replace(' ','').replace(')','').replace('(','').replace('/',''+('.csv'))\n"
'&gt;&gt;&gt; result = df.groupby("c3")[["c1","c2"]].apply(lambda x: dict(x.values)).to_dict()\n&gt;&gt;&gt; pprint.pprint(result)\n{\'alpha\': {\'one\': 0.440958516531,\n           \'three\': 0.677464637887,\n           \'two\': 0.8827243364640001},\n \'beta\': {\'one\': 0.47643995372299996, \'three\': 0.29292767009599996},\n \'gamma\': {\'one\': 0.254235673552,\n           \'three\': 0.0971956881825,\n           \'two\': 0.79817899139},\n \'zeta\': {\'three\': 0.993934915508}}\n\n&gt;&gt;&gt; grouped = df.groupby("c3")[["c1", "c2"]]\n&gt;&gt;&gt; grouped.apply(lambda x: print(x,"\\n","--")) # just for display purposes\n      c1                   c2\n0    one    0.679926178687387\n3    two  0.11495090934413166\n5  three   0.7458197179794177 \n --\n      c1                   c2\n0    one    0.679926178687387\n3    two  0.11495090934413166\n5  three   0.7458197179794177 \n --\n      c1                   c2\n1    one  0.12943266757277916\n6  three  0.28944292691097817 \n --\n      c1                   c2\n2    one  0.36642834809341274\n4    two   0.5690944224514624\n7  three   0.7018221838129789 \n --\n      c1                  c2\n8  three  0.7195852795555373 \n --\n\n&gt;&gt;&gt; d3\n      c1        c2\n2    one  0.366428\n4    two  0.569094\n7  three  0.701822\n\n&gt;&gt;&gt; dict(d3)\n{\'c1\': 2      one\n4      two\n7    three\nName: c1, dtype: object, \'c2\': 2    0.366428\n4    0.569094\n7    0.701822\nName: c2, dtype: float64}\n&gt;&gt;&gt; d3.to_dict()\n{\'c1\': {2: \'one\', 4: \'two\', 7: \'three\'}, \'c2\': {2: 0.36642834809341279, 4: 0.56909442245146236, 7: 0.70182218381297889}}\n\n&gt;&gt;&gt; d3.values\narray([[\'one\', 0.3664283480934128],\n       [\'two\', 0.5690944224514624],\n       [\'three\', 0.7018221838129789]], dtype=object)\n&gt;&gt;&gt; dict(d3.values)\n{\'three\': 0.7018221838129789, \'one\': 0.3664283480934128, \'two\': 0.5690944224514624}\n\n&gt;&gt;&gt; result = df.groupby("c3")[["c1", "c2"]].apply(lambda x: dict(x.values))\n&gt;&gt;&gt; result\nc3\nalpha    {\'three\': \'0.7458197179794177\', \'one\': \'0.6799...\nbeta     {\'one\': \'0.12943266757277916\', \'three\': \'0.289...\ngamma    {\'three\': \'0.7018221838129789\', \'one\': \'0.3664...\nzeta                       {\'three\': \'0.7195852795555373\'}\ndtype: object\n&gt;&gt;&gt; result.to_dict()\n{\'zeta\': {\'three\': \'0.7195852795555373\'}, \'gamma\': {\'three\': \'0.7018221838129789\', \'one\': \'0.36642834809341274\', \'two\': \'0.5690944224514624\'}, \'beta\': {\'one\': \'0.12943266757277916\', \'three\': \'0.28944292691097817\'}, \'alpha\': {\'three\': \'0.7458197179794177\', \'one\': \'0.679926178687387\', \'two\': \'0.11495090934413166\'}}\n'
'W = tf.Variable(tf.zeros([1,2]), name="weight")\n\nW = tf.Variable(tf.zeros([2, 1]), name="weight")\n\nW = tf.Variable(tf.truncated_normal([2, 1], stddev=0.5), name="weight")\n'
"print ('coefficients',rfe.estimator_.coef_)\n"
'def chunks(l, n):\n    """ Yield successive n-sized chunks from l."""\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n\ntest_x = pd.DataFrame(test_x)\ntest_result = pd.DataFrame()\nfor chunk in chunks(test_x.index, 10000):\n    test_data = test_x.ix[chunk]\n    test_result = pd.concat([test_result, pd.DataFrame(clf.predict(test_data))])\n'
'from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=2)\nt = dataset.target\nfor train_index, test_index in skf.split(np.zeros(len(t)), t):\n    train = dataset.loc[train_index]\n    test = dataset.loc[test_index]\n'
"stddev = 100\n\nstddev = 1\n\nX = np.sort(low_x + (high_x - low_x) * np.random.rand(N,1), axis=0)\n\nimport numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nimport matplotlib.pyplot as plt\n\nN = 5000\n\nlow_x =-2*np.pi\nhigh_x=2*np.pi\nX = np.sort(low_x + (high_x - low_x) * np.random.rand(N,1), axis=0)\nf = lambda x: 2*np.power( 2*np.power( np.cos(x) ,2) - 1, 2) - 1\nY = f(X) \n\nK = 30 # number of centers for RBF\nindices=np.random.choice(a=N,size=K) # choose numbers from 0 to D^(1)\nsubsampled_data_points=X[indices,:] # M_sub x D\nstddev = 1\n\nbeta = 0.5*np.power(1.0/stddev,2)\nKern = np.exp(-beta*euclidean_distances(X=X, Y=subsampled_data_points,squared=True))\nC = np.linalg.lstsq(Kern, Y)[0]\n\nY_pred = np.dot(Kern, C)\n\nplt.plot(X, Y, 'o', label='Original data', markersize=1)\nplt.plot(X, Y_pred, 'r', label='Fitted line', markersize=1)\nplt.legend()\nplt.show()\n"
'my_matrix = tfidf_matrix.toarray()\n\ntfidf_vectorizer.vocabulary_.get(keyword)\n\n    for i in range(0, len(my_matrix)):\n    for key in keyword_list:\n        if key != None:\n            key = (int)(key)\n        if my_matrix[i][key] &gt; 0.0:\n            my_matrix[i][key] = new_value\n\ntfidf_matrix = sparse.csr_matrix(my_matrix)\n'
'from numpy import *\nU, S, Vh = linalg.svd(dot((tile(sum(x*x,0),(x.shape[0],1))*x),x.T))\n'
'from cleanlab.latent_estimation import estimate_cv_predicted_probabilities\n\n# Find the indices of label errors in 2 lines of code.\n\nprobabilities = estimate_cv_predicted_probabilities(\n    X_train_data, \n    train_noisy_labels, \n    clf=LogisticRegression(),\n)\nlabel_error_indices = get_noise_indices(\n    s = train_noisy_labels, \n    psx = probabilities, \n)\n\n# Code taken from https://github.com/cgnorthcutt/cleanlab\nfrom cleanlab.classification import LearningWithNoisyLabels\nfrom sklearn.linear_model import LogisticRegression\n\n# Learning with noisy labels in 3 lines of code.\n\n# Wrap around any classifier. Works with sklearn/pyTorch/Tensorflow/FastText/etc.\nlnl = LearningWithNoisyLabels(clf=LogisticRegression())\nlnl.fit(X = X_train_data, s = train_noisy_labels)\n# Estimate the predictions you would have gotten by training with *no* label errors.\npredicted_test_labels = lnl.predict(X_test)\n'
'import tensorflow as tf\n\n# Graph 1 - set v1 to have value [1.0]\ng1 = tf.Graph()\nwith g1.as_default():\n    v1 = tf.Variable(tf.zeros([1]), name="v1")\n    assign1 = v1.assign(tf.constant([1.0]))\n    init1 = tf.initialize_all_variables()\n    save1 = tf.train.Saver()\n\n# Graph 2 - set v2 to have value [2.0]\ng2 = tf.Graph()\nwith g2.as_default():\n    v2 = tf.Variable(tf.zeros([1]), name="v2")\n    assign2 = v2.assign(tf.constant([2.0]))\n    init2 = tf.initialize_all_variables()\n    save2 = tf.train.Saver()\n\n# Do the computation for graph 1 and save\nsess1 = tf.Session(graph=g1)\nsess1.run(init1)\nprint sess1.run(assign1)\nsave1.save(sess1, "tmp/v1.ckpt")\n\n# Do the computation for graph 2 and save\nsess2 = tf.Session(graph=g2)\nsess2.run(init2)\nprint sess2.run(assign2)\nsave2.save(sess2, "tmp/v2.ckpt")\n\nimport tensorflow as tf\n\n# The variables v1 and v2 that we want to restore\nv1 = tf.Variable(tf.zeros([1]), name="v1")\nv2 = tf.Variable(tf.zeros([1]), name="v2")\n\n# saver1 will only look for v1\nsaver1 = tf.train.Saver([v1])\n# saver2 will only look for v2\nsaver2 = tf.train.Saver([v2])\nwith tf.Session() as sess:\n    saver1.restore(sess, "tmp/v1.ckpt")\n    saver2.restore(sess, "tmp/v2.ckpt")\n    print sess.run(v1)\n    print sess.run(v2)\n\n[1.]\n[2.]\n'
'from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nx = np.random.randn(3785,16000)\ny = (x[:,0]&gt;0.).astype(np.float)    \nclf = AdaBoostClassifier(n_estimators = 1)\nclf2 = DecisionTreeClassifier()\n%timeit clf.fit(x,y)\n1 loop, best of 3: 5.56 s per loop\n%timeit clf2.fit(x,y)\n1 loop, best of 3: 5.51 s per loop\n'
"klass = estimator.__class__\nnew_object_params = estimator.get_params(deep=False)\nfor name, param in six.iteritems(new_object_params):\n    new_object_params[name] = clone(param, safe=False)\nnew_object = klass(**new_object_params) \n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nclass myTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, my_np_array):\n        self.data = my_np_array\n        print self.data\n\n    def transform(self, X):\n        return X\n\n    def fit(self, X, y=None):\n        return self\n\n    def get_params(self, deep=False):\n        return {'my_np_array': self.data}\n"
"num_words vector_size  # this is the header\nlabel0 x00 x01 ... x0N\nlabel1 x10 x11 ... x1N\n...\n\n2 3\nword0 -0.000737 -0.002106 0.001851\nword1 -0.000878 -0.002106 0.002834\n\nmodel = Word2Vec.load_word2vec_format(filename, binary=False)\n\nprint(model['word0'])\n\nfrom gensim.models.keyedvectors import KeyedVectors\n\nmodel = KeyedVectors.load_word2vec_format(model_path, binary=False)\n"
'setLayers([len(features), 20, 10, 2])\n\nfeature_pipeline_model = (Pipeline()\n     .setStages(...)  # Only feature extraction\n     .fit(train_df))\n\ntrain_df_features = feature_pipeline_model.transform(train_df)\nlayers = [\n    train_df_features.schema["features"].metadata["ml_attr"]["num_attrs"],\n    20, 10, 2\n]\n'
'seq3 = merge([seq2, seq1], mode="concat", concat_axis=1)\n'
'(\'Eat\', 34)\n(\'Sleep\', 54)\n(\'Eat\', 76)\n   ...\n(\'Watch a movie\', 93)\n\n(\'Eat\', 2)\n(\'Sleep\', 1)\n\nd(e1, e2) = a * |timeOfDay(e1) - timeOfDay(e2)| * (1/1440) + \n            b * |dayOfWeek(e1) - dayOfWeek(e2)| * (1/7)\n\n          1\nw_o = ---------\n      d(e, o)^2\n\ndef rank(e, db, d):\n    """ Rank the examples in db with respect to e using\n        distance function d.\n    """\n    return sorted([(o, d(e, o)) for o in db],\n                  key=lambda x: x[1])\n'
"df_train[df_train.isnull().any(axis=1)]\n\ndf_train.fillna('', inplace=True)\n\ndf_train = pd.read_csv('Digital_Music_5.csv', names=COLUMNS, \n                        skipinitialspace=True, low_memory=False, \n                        skiprows=1, na_values=[])\n"
'network = Network()\noptimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)\n\noptimizer = optim.SGD(Network().parameters(), lr=0.001, momentum=0.9)\n'
'# do the loop:\nr = tf.while_loop(while_condition, row_loop, [i, rows, areas])\nareas = r[2].stack()\n'
'import numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndata = np.random.randint(0,100,200).reshape(20,10)\ntarget = np.random.randint(0,1,20)\n\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2)\n\ndef Normalize(data, mean_data =None, std_data =None):\n    if not mean_data:\n        mean_data = np.mean(data)\n    if not std_data:\n        std_data = np.std(data)\n    norm_data = (data-mean_data)/std_data\n    return norm_data, mean_data, std_data\n\nX_train, mean_data, std_data = Normalize(X_train)\nX_test, _, _ = Normalize(X_test, mean_data, std_data)\n\nmodel.fit(X_train, y_train, validation_data=(X_test,y_test), batch_size=15, callbacks=[early_stopping], verbose=1)\n'
"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit([s for l in df.A for s in l])\n\ndf.A.apply(le.transform)\n#0    [2, 1, 0, 1, 2]\n#1          [0, 2, 1]\n#Name: A, dtype: object\n\nle.classes_\n#array(['Female', 'Male', 'Other'], \n#      dtype='&lt;U6')\n"
'import dlib\nimport pickle    \n\nx = dlib.vectors()\ny = dlib.array()\n\n# Make a training dataset.  Here we have just two training examples.  Normally\n# you would use a much larger training dataset, but for the purpose of example\n# this is plenty.  For binary classification, the y labels should all be either +1 or -1.\nx.append(dlib.vector([1, 2, 3, -1, -2, -3]))\ny.append(+1)\n\nx.append(dlib.vector([-1, -2, -3, 1, 2, 3]))\ny.append(-1)\n\n\n# Now make a training object.  This object is responsible for turning a\n# training dataset into a prediction model.  This one here is a SVM trainer\n# that uses a linear kernel.  If you wanted to use a RBF kernel or histogram\n# intersection kernel you could change it to one of these lines:\n#  svm = dlib.svm_c_trainer_histogram_intersection()\n#  svm = dlib.svm_c_trainer_radial_basis()\nsvm = dlib.svm_c_trainer_linear()\nsvm.be_verbose()\nsvm.set_c(10)\n\n# Now train the model.  The return value is the trained model capable of making predictions.\nclassifier = svm.train(x, y)\n\n# Now run the model on our data and look at the results.\nprint("prediction for first sample:  {}".format(classifier(x[0])))\nprint("prediction for second sample: {}".format(classifier(x[1])))\n\n\n# classifier models can also be pickled in the same was as any other python object.\nwith open(\'saved_model.pickle\', \'wb\') as handle:\n    pickle.dump(classifier, handle)\n'
'x = np.array([1,5,3,6,8,4,6,8,4,2,3,5,8,6,4])\ny = x.cumsum()/np.arange(1,len(x)+1)\n\narray([ 1.        ,  3.        ,  3.        ,  3.75      ,  4.6       ,\n     4.5       ,  4.71428571,  5.125     ,  5.        ,  4.7       ,\n     4.54545455,  4.58333333,  4.84615385,  4.92857143,  4.86666667])\n'
'import numpy as np\nfrom keras import backend as K\nfrom keras.layers import Input\nfrom keras.layers import Lambda\nfrom keras.models import Model\n\n\ndef sampling(preds, temperature=1.0):\n    preds = K.log(preds) / temperature\n    exp_preds = K.exp(preds)\n    preds = exp_preds / K.sum(exp_preds)\n    return K.argmax(preds, axis=2)\n\n\nif __name__ == \'__main__\':\n    batch_size = 10\n\n    ########### MODEL\n    inp = Input(shape=(15, 38))\n    exa = Lambda(lambda x: sampling(x), output_shape=(15,))(inp)\n    model = Model(inputs=inp, outputs=exa)\n    model.summary()\n\n    noise = np.random.uniform(0, 1, size=(batch_size, 15, 38))\n    print("NOISE INPUT")\n    print(noise)\n    decoded = model.predict_on_batch(noise)\n    print("\\n\\nSAMPLED OUTPUT")\n    print(decoded)\n    #################\n\nUsing TensorFlow backend.\n2017-11-16 18:26:51.398569: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 15, 38)            0         \n_________________________________________________________________\nlambda_1 (Lambda)            (None, 15)                0         \n=================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n_________________________________________________________________\nNOISE INPUT\n[[[ 0.30891316  0.15221787  0.5038829  ...,  0.8848083   0.08956024\n    0.3606536 ]\n  [ 0.44807492  0.23133616  0.70644087 ...,  0.27361268  0.65620316\n    0.85804126]\n  [ 0.72102694  0.09523426  0.16791966 ...,  0.66441456  0.80056542\n    0.86870569]\n  ..., \n  [ 0.98217747  0.77091951  0.59332161 ...,  0.79585449  0.73915857\n    0.46059018]\n  [ 0.88017517  0.17193309  0.51066406 ...,  0.78946729  0.88111187\n    0.26728708]\n  [ 0.34269771  0.62430603  0.84418251 ...,  0.80441201  0.54334108\n    0.98493448]]\n\n [[ 0.8669257   0.82135396  0.10069927 ...,  0.90542972  0.67465482\n    0.55842347]\n  [ 0.72956178  0.08731971  0.95467123 ...,  0.95472095  0.88684042\n    0.37218502]\n  [ 0.46721374  0.96649476  0.1933913  ...,  0.06071162  0.14450657\n    0.90962683]\n  ..., \n  [ 0.12344553  0.96051246  0.68679955 ...,  0.99269888  0.20232171\n    0.67095509]\n  [ 0.68686574  0.94280854  0.80347876 ...,  0.04743072  0.95419244\n    0.85568141]\n  [ 0.38312468  0.51157601  0.71631078 ...,  0.26982261  0.94738435\n    0.01725109]]\n\n [[ 0.60993944  0.78323704  0.44851841 ...,  0.88221101  0.32755589\n    0.82010709]\n  [ 0.83341658  0.55235978  0.03581224 ...,  0.03711514  0.82935275\n    0.1729678 ]\n  [ 0.78978104  0.2181397   0.49759489 ...,  0.20598122  0.87555217\n    0.1310053 ]\n  ..., \n  [ 0.0136351   0.1470119   0.55496631 ...,  0.93888501  0.29122596\n    0.73619966]\n  [ 0.15711732  0.99471253  0.35647437 ...,  0.90060837  0.75228682\n    0.3414452 ]\n  [ 0.35007325  0.54940108  0.60591077 ...,  0.90813329  0.04030722\n    0.09562064]]\n\n ..., \n [[ 0.02168732  0.95698804  0.09280446 ...,  0.71586791  0.00920231\n    0.38907889]\n  [ 0.76853587  0.25528251  0.61563489 ...,  0.97880311  0.88628481\n    0.30254836]\n  [ 0.75778522  0.27014167  0.86857289 ...,  0.7196309   0.47614798\n    0.50452127]\n  ..., \n  [ 0.38104386  0.95507951  0.8337219  ...,  0.86317049  0.56698557\n    0.67915409]\n  [ 0.2014119   0.18607705  0.11220879 ...,  0.11445029  0.54958169\n    0.332358  ]\n  [ 0.26280992  0.73237823  0.72684251 ...,  0.43947476  0.56091593\n    0.32524556]]\n\n [[ 0.96829533  0.68190408  0.3739879  ...,  0.9606717   0.22280759\n    0.3031305 ]\n  [ 0.82955492  0.04815685  0.43250177 ...,  0.58623864  0.37036309\n    0.23000677]\n  [ 0.47910544  0.44406259  0.22996476 ...,  0.05613431  0.77214874\n    0.95063867]\n  ..., \n  [ 0.60491445  0.33447544  0.22093127 ...,  0.82935465  0.08349322\n    0.2298346 ]\n  [ 0.81513182  0.27917361  0.34890291 ...,  0.4860525   0.72408671\n    0.91163499]\n  [ 0.87716181  0.90365287  0.12285839 ...,  0.9010089   0.05510581\n    0.61629317]]\n\n [[ 0.80663206  0.35334483  0.81901437 ...,  0.55569487  0.15872563\n    0.71420715]\n  [ 0.78043711  0.37595655  0.7021172  ...,  0.57781578  0.68621989\n    0.0617801 ]\n  [ 0.74502178  0.51097854  0.35836054 ...,  0.39165413  0.63403207\n    0.1674981 ]\n  ..., \n  [ 0.87651597  0.34229628  0.99998675 ...,  0.64886858  0.87177124\n    0.83341321]\n  [ 0.037897    0.00723697  0.05077544 ...,  0.54169736  0.18409638\n    0.1442172 ]\n  [ 0.10817155  0.82998391  0.1356064  ...,  0.89285585  0.7613262\n    0.33972169]]]\n\n\nSAMPLED OUTPUT\n[[18 12 30  3 15  9 28 36 29 14 15  5 33  4 28]\n [ 3 35  5 10 23 16 10 11  3 15  6  1 29 30 23]\n [27 17 11 32 14 19  4 20  7 20 29 19 29  1 26]\n [ 3 27  5 11 21 21 11 27  0 21 21 34 15 24  0]\n [ 5  5 21  0  1 10 21  5 19 19 13 25 37 28 33]\n [ 7 34 11  3 18 24 18 32 11 21  5 32 25  0 16]\n [ 0 25 29 21 15 15 35 23  0 18  6 27  4 11 21]\n [27 35 29 10 11 11 18  6 29 35 36 29 34 21 33]\n [ 0 29  8 11 30 21 32 27 15 10 16 15 13 34 27]\n [17 26  3 20 10 13 27 35  4 10 10 20  2 15 34]]\n'
'cost:  0.23668000993020666\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv("cancerdata.csv")\nX = df.values[:,2:-1].astype(\'float64\')\nX = (X - np.mean(X, axis =0)) /  np.std(X, axis = 0)\n\n## Add a bias column to the data\nX = np.hstack([np.ones((X.shape[0], 1)),X])\nX = MinMaxScaler().fit_transform(X)\nY = df["diagnosis"].map({\'M\':1,\'B\':0})\nY = np.array(Y)\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25)\n\n\ndef Sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\ndef Hypothesis(theta, x):   \n    return Sigmoid(x @ theta) \n\ndef Cost_Function(X,Y,theta,m):\n    hi = Hypothesis(theta, X)\n    _y = Y.reshape(-1, 1)\n    J = 1/float(m) * np.sum(-_y * np.log(hi) - (1-_y) * np.log(1-hi))\n    return J\n\ndef Cost_Function_Derivative(X,Y,theta,m,alpha):\n    hi = Hypothesis(theta,X)\n    _y = Y.reshape(-1, 1)\n    J = alpha/float(m) * X.T @ (hi - _y)\n    return J\n\ndef Gradient_Descent(X,Y,theta,m,alpha):\n    new_theta = theta - Cost_Function_Derivative(X,Y,theta,m,alpha)\n    return new_theta\n\ndef Accuracy(theta):\n    correct = 0\n    length = len(X_test)\n    prediction = (Hypothesis(theta, X_test) &gt; 0.5)\n    _y = Y_test.reshape(-1, 1)\n    correct = prediction == _y\n    my_accuracy = (np.sum(correct) / length)*100\n    print (\'LR Accuracy %: \', my_accuracy)\n\ndef Logistic_Regression(X,Y,alpha,theta,num_iters):\n    m = len(Y)\n    for x in range(num_iters):\n        new_theta = Gradient_Descent(X,Y,theta,m,alpha)\n        theta = new_theta\n        if x % 100 == 0:\n            #print (\'theta: \', theta)    \n            print (\'cost: \', Cost_Function(X,Y,theta,m))\n    Accuracy(theta)\n\nep = .012\n\ninitial_theta = np.random.rand(X_train.shape[1],1) * 2 * ep - ep\nalpha = 0.5\niterations = 2000\nLogistic_Regression(X_train,Y_train,alpha,initial_theta,iterations)\n'
'pd.isnull(train_data).sum() &gt; 0\n\nportfolio_id      False\ndesk_id           False\noffice_id         False\npf_category       False\nstart_date        False\nsold               True\ncountry_code      False\neuribor_rate      False\ncurrency          False\nlibor_rate         True\nbought             True\ncreation_date     False\nindicator_code    False\nsell_date         False\ntype              False\nhedge_value       False\nstatus            False\nreturn            False\ndtype: bool\n'
'layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)\n\noptimize_tensor_layout=True)\n'
"df = df.withColumn('cat_var_2_final',df['cat_var_2']+100).withColumn('cat_var_3_final',df['cat_var_3']+1000)\n\nudf1 = udf(lambda c1,c2,c3 : (c1,c2,c3),ArrayType(IntegerType()))\ndf = df.withColumn('features',udf1(df['cat_var_1'],df['cat_var_2_final'],df['cat_var_3_final']))\n"
'locals = {\n    "add": Add,\n    "mul": Mul,\n    "sub": Lambda((x, y), x - y),\n    "div": Lambda((x, y), x/y)\n}\n\nsympify(\'sqrt(div(add(1.000, sub(div(sqrt(log(0.978)), X0), mul(-0.993, X0))), add(-0.583, 0.592)))\', locals=locals)\n\nsqrt(110.333333333333*X0 + 111.111111111111 + 16.5721799259414*I/X0)\n\nX0 = symbols("X0")\nlocals = {\n    "add": Add,\n    "mul": Mul,\n    "sub": Lambda((x, y), x - y),\n    "div": Lambda((x, y), x/y),\n    "X0": X0\n}\n'
'def evaluation_metric(y_true, y_pred):\n\n    y_true = y_true * (y_true != 0) \n    y_pred = y_pred * (y_true != 0)\n\n    error = sqrt(mean_squared_error(y_true, y_pred))\n    return error\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\nX = np.array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\nY = np.array([ 3,  5,  9,  9, 11, 13, 16, 17, 19, 21])\n\nlearning_rate = 0.015\nm = 1\nc = 2\n\ngues = []\nfor xi, yi in zip(X, Y):\n\n    guess = m * xi + c\n    error = guess - yi\n\n    m = m - error * xi * learning_rate\n    c = c - error * learning_rate\n\n    gues.append(guess)\n\nt = np.array(gues)\n\n# Plot the modeled line.\ny_hat = m * X + c\nplt.figure(figsize=(10,5))\nplt.plot(X, y_hat, c='red')\n\n# Plot the data.\nplt.scatter(X, Y)\n\n# Plot the evolution of guesses.\nplt.plot(X, t)\nplt.show()\n"
'node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                    node_indicator.indptr[sample_id + 1]]\n\n(node_indicator, _) = rf.decision_path(X_train)\n\nX_train = X_train.values\n\nsample_id = 0\n\nfor j, tree in enumerate(rf.estimators_):\n\n    n_nodes = tree.tree_.node_count\n    children_left = tree.tree_.children_left\n    children_right = tree.tree_.children_right\n    feature = tree.tree_.feature\n    threshold = tree.tree_.threshold\n\n    print("Decision path for DecisionTree {0}".format(j))\n    node_indicator = tree.decision_path(X_train)\n    leave_id = tree.apply(X_train)\n    node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                        node_indicator.indptr[sample_id + 1]]\n\n\n\n    print(\'Rules used to predict sample %s: \' % sample_id)\n    for node_id in node_index:\n        if leave_id[sample_id] != node_id:\n            continue\n\n        if (X_train[sample_id, feature[node_id]] &lt;= threshold[node_id]):\n            threshold_sign = "&lt;="\n        else:\n            threshold_sign = "&gt;"\n\n        print("decision id node %s : (X_train[%s, %s] (= %s) %s %s)"\n              % (node_id,\n                 sample_id,\n                 feature[node_id],\n                 X_train[sample_id, feature[node_id]],\n                 threshold_sign,\n                 threshold[node_id]))\n'
'cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_predicted), reduction_indices=[1]))\n\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y_predicted,1e-10,1.0)), reduction_indices=[1]))\n'
'output = Dense(3, activation="sigmoid")(dense)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# import file\nurl = ".../Python/outliner.csv"\nnames = [\'R1\', \'P1\', \'T1\', \'P2\', \'Flag\']\ndataset = pandas.read_csv(url, names=names)\nX = dataset.values[:, 0:2]\n\n# fit the model\nclf = LocalOutlierFactor(n_neighbors=50, algorithm=\'auto\', leaf_size=30)\ny_pred = clf.fit_predict(X)\n\n# map results\nX_normals = X[y_pred == 1]\nX_outliers = X[y_pred == -1]\n\n# plot the level sets of the decision function\nxx, yy = np.meshgrid(np.linspace(0, 1000, 50), np.linspace(0, 200, 50))\nZ = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title("Local Outlier Factor (LOF)")\nplt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n\na = plt.scatter(X_normals[:, 0], X_normals[:, 1], c=\'white\', edgecolor=\'k\', s=20)\nb = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\'red\', edgecolor=\'k\', s=20)\nplt.axis(\'tight\')\nplt.xlim((0, 1000))\nplt.ylim((0, 200))\nplt.legend([a, b], ["normal predictions", "abnormal predictions"], loc="upper left")\nplt.show()\n'
'def cross_val_score(model, X, *args, **kwargs):\n    warnings.warn(DEPRECATION_MSG, DeprecationWarning)\n    X = DataWrapper(X)\n    return sk_cross_val_score(model, X, *args, **kwargs)\n\n if self.df_out:\n        # if no rows were dropped preserve the original index,\n        # otherwise use a new integer one\n        no_rows_dropped = len(X) == len(stacked)\n        if no_rows_dropped:\n            index = X.index      # &lt;== This here is the source of error\n        else:\n            index = None\n\nfrom sklearn_pandas import cross_val_score\n\nfrom sklearn.model_selection import cross_val_score\n\n AttributeError: \'numpy.ndarray\' object has no attribute \'to_dict\'\n\nnum_cat_union = FeatureUnion([("num_mapper", num_transf_mapper),\n                            ("cat_mapper", cat_transf_mapper)])\n\npipeline = Pipeline([("featureunion", num_cat_union),\n                    ("dictifier", Dictifier()),\n                    ("vectorizer", DictVectorizer(sort=False)),\n                    ("clf", xgb.XGBClassifier(max_depth=3))])\n\ntransformers = []\n\n# Combine both your operations here only\ntransformers.extend([([num_col], [Imputer(strategy="median"), \n                                  StandardScaler()]) for num_col in num_cols])\ntransformers.extend([(cat_col , [CategoricalImputer()]) for cat_col in cat_cols])\n\nnum_cat_union = DataFrameMapper(transformers,\n                                input_df=True,\n                                df_out=True)\n\n# Your other code\n...\n...\n'
'def custom_activation_4(x):\n  orig = x\n  x = tf.where(orig &lt; -6, tf.zeros_like(x), x)\n  x = tf.where(orig &gt;= -6 and orig &lt; -4, (0.0078*x + 0.049), x)\n  x = tf.where(orig &gt;= -4 and orig &lt; 0, (0.1205*x + 0.5), x)\n  x = tf.where(orig &gt;= 0 and orig &lt; 4, (0.1205*x + 0.5), x)\n  x = tf.where(orig  &gt;= 4 and orig &lt; 6, (0.0078*x + 0.951), x)\n  return tf.where(orig &gt;= 6, 1, x)\n'
'from sklearn.utils.extmath import softmax\nfrom sklearn.metrics.pairwise import pairwise_distances\n\ndef predict_proba(self, X):\n    distances = pairwise_distances(X, self.centroids_, metric=self.metric)\n    probs = softmax(distances)\n    return probs\n\nclf = NearestCentroid()\nclf.predict_proba = predict_proba.__get__(clf)\nclf.fit(X_train, y_train)\nclf.predict_proba(X_test)\n'
"from keras.callbacks import Callback\n\nclass MyLogger(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        with open('log.txt', 'a+') as f:\n            f.write('%02d %.3f\\n' % (epoch, logs['loss']))\n\nmylogger = MyLogger()\nmodel.fit(X, Y, batch_size=32, epochs=32, verbose=1, callbacks=[mylogger])\n\nmodel.fit(X, Y, batch_size=32, epochs=32, verbose=1, callbacks=[MyLogger()])\n"
'while max(lrmodel.pvalues)&gt;0.05:\n    for x,y in zip(lrmodel.pvalues,xtrain.columns): \n        if x&gt;0.05:\n            xtrain = xtrain.drop(y,axis=1)\n            xtest = xtest.drop(y,axis=1)\n            lrmodel = sm.OLS(ytrain,xtrain).fit()\n            break\n# after all the values are less than 0.05, assign the model to final model\nfinalmodel = lrmodel\n'
'my_scorer = make_scorer(adjusted_rsquare, greater_is_better=True)\n\ndef adjusted_rsquare(y_true, y_pred, **kwargs):\n'
'import tensorflow as tf\nfrom tensorflow.python.ops import lookup_ops  # This is inside core TensorFlow\n\nprint(tf.contrib.lookup.HashTable is lookup_ops.HashTable)\n# True\n'
'layer = BatchNormalization(axis=3, name="a")\nnode = layer(input)\n\nlayer.name \n'
'corpus = ["this is a red apple", "this is a green apple", "this is a cat"]\n\ntf(word, document) = count(word, document) # Number of times word appears in the document\n\ntf(\'a\',d1)     = 1      tf(\'a\',d2)     = 1      tf(\'a\',d3)     = 1\ntf(\'apple\',d1) = 1      tf(\'apple\',d2) = 1      tf(\'apple\',d3) = 0\ntf(\'cat\',d1)   = 0      tf(\'cat\',d2)   = 0      tf(\'cat\',d3)   = 1\ntf(\'green\',d1) = 0      tf(\'green\',d2) = 1      tf(\'green\',d3) = 0\ntf(\'is\',d1)    = 1      tf(\'is\',d2)    = 1      tf(\'is\',d3)    = 1\ntf(\'red\',d1)   = 1      tf(\'red\',d2)   = 0      tf(\'red\',d3)   = 0\ntf(\'this\',d1)  = 1      tf(\'this\',d2)  = 1      tf(\'this\',d3)  = 1\n\ntf(word, document, normalize=\'l1\') = count(word, document)/|document|\ntf(word, document, normalize=\'l2\') = count(word, document)/l2_norm(document)\n\n|d1| = 5, |d2| = 5, |d3| = 4\nl2_norm(d1) = 0.447, l2_norm(d2) = 0.447, l2_norm(d3) = 0.5, \n\ncorpus = ["this is a red apple", "this is a green apple", "this is a cat"]\n# Convert docs to textacy format\ntextacy_docs = [textacy.Doc(doc) for doc in corpus]\n\nfor norm in [None, \'l1\', \'l2\']:\n    # tokenize the documents\n    tokenized_docs = [\n    doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True, filter_stops=False, normalize=\'lower\')\n    for doc in textacy_docs]\n\n    # Fit the tf matrix \n    vectorizer = textacy.Vectorizer(apply_idf=False, norm=norm)\n    doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n\n    print ("\\nVocabulary: ", vectorizer.vocabulary_terms)\n    print ("TF with {0} normalize".format(norm))\n    print (doc_term_matrix.toarray())\n\nVocabulary:  {\'this\': 6, \'is\': 4, \'a\': 0, \'red\': 5, \'apple\': 1, \'green\': 3, \'cat\': 2}\nTF with None normalize\n[[1 1 0 0 1 1 1]\n [1 1 0 1 1 0 1]\n [1 0 1 0 1 0 1]]\n\nVocabulary:  {\'this\': 6, \'is\': 4, \'a\': 0, \'red\': 5, \'apple\': 1, \'green\': 3, \'cat\': 2}\nTF with l1 normalize\n[[0.2  0.2  0.   0.   0.2  0.2  0.2 ]\n [0.2  0.2  0.   0.2  0.2  0.   0.2 ]\n [0.25 0.   0.25 0.   0.25 0.   0.25]]\n\nVocabulary:  {\'this\': 6, \'is\': 4, \'a\': 0, \'red\': 5, \'apple\': 1, \'green\': 3, \'cat\': 2}\nTF with l2 normalize\n[[0.4472136 0.4472136 0.        0.        0.4472136 0.4472136 0.4472136]\n [0.4472136 0.4472136 0.        0.4472136 0.4472136 0.        0.4472136]\n [0.5       0.        0.5       0.        0.5       0.        0.5      ]]\n\nidf(word, corpus) = log(|corpus| / No:of documents containing word) + 1  # standard idf\n\nidf(\'apple\', corpus) = log(3/2) + 1 = 1.405 \nidf(\'cat\', corpus) = log(3/1) + 1 = 2.098\nidf(\'this\', corpus) = log(3/3) + 1 = 1.0\n\ntextacy_docs = [textacy.Doc(doc) for doc in corpus]    \ntokenized_docs = [\n    doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True, filter_stops=False, normalize=\'lower\')\n    for doc in textacy_docs]\n\nvectorizer = textacy.Vectorizer(apply_idf=False, norm=None)\ndoc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n\nprint ("\\nVocabulary: ", vectorizer.vocabulary_terms)\nprint ("standard idf: ")\nprint (textacy.vsm.matrix_utils.get_inverse_doc_freqs(doc_term_matrix, type_=\'standard\'))\n\nVocabulary:  {\'this\': 6, \'is\': 4, \'a\': 0, \'red\': 5, \'apple\': 1, \'green\': 3, \'cat\': 2}\nstandard idf: \n[1.     1.405       2.098       2.098       1.      2.098       1.]\n\ntf-idf(word, document, corpus) = tf(word, docuemnt) * idf(word, corpus)\n\ntf-idf(\'apple\', \'d1\', corpus) = tf(\'apple\', \'d1\') * idf(\'apple\', corpus) = 1 * 1.405 = 1.405\ntf-idf(\'cat\', \'d3\', corpus) = tf(\'cat\', \'d3\') * idf(\'cat\', corpus) = 1 * 2.098 = 2.098\n\ntextacy_docs = [textacy.Doc(doc) for doc in corpus]\n\ntokenized_docs = [\n    doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True, filter_stops=False, normalize=\'lower\')\n    for doc in textacy_docs]\n\nprint ("\\nVocabulary: ", vectorizer.vocabulary_terms)\nprint ("tf-idf: ")\n\nvectorizer = textacy.Vectorizer(apply_idf=True, norm=None, idf_type=\'standard\')\ndoc_term_matrix = vectorizer.fit_transform(tokenized_docs)\nprint (doc_term_matrix.toarray())\n\nVocabulary:  {\'this\': 6, \'is\': 4, \'a\': 0, \'red\': 5, \'apple\': 1, \'green\': 3, \'cat\': 2}\ntf-idf: \n[[1.         1.405   0.         0.         1.         2.098   1.        ]\n [1.         1.405   0.         2.098      1.         0.      1.        ]\n [1.         0.      2.098      0.         1.         0.      1.        ]]\n\ncosine_similarity(doc_term_matrix)\n\narray([[1.        ,     0.53044716,     0.35999211],\n       [0.53044716,     1.        ,     0.35999211],\n       [0.35999211,     0.35999211,     1.        ]])\n'
'# X_test - your untransformed test set\n\nX_test_reduced = pca.transform(X_test)\n'
"train['IncomeBand']= pd.cut(train['Income'] , 4).cat.codes\ntrain['LoanAmountBand']=pd.cut(train['LoanAmount'] , 4).cat.codes\n"
'  tmp = torch.tensor([[0, 0, 1, 0, 1, 0, 0],\n                     [0, 0, 0, 1, 1, 0, 0]], dtype=torch.float)\n  idx = reversed(torch.Tensor(range(1,8)))\n  print(idx)\n\n  tmp2= torch.einsum("ab,b-&gt;ab", (tmp, idx))\n\n  print(tmp2)\n\n  indices = torch.argmax(tmp2, 1, keepdim=True)\n  print(indeces)\n\ntensor([7., 6., 5., 4., 3., 2., 1.])\ntensor([[0., 0., 5., 0., 3., 0., 0.],\n       [0., 0., 0., 4., 3., 0., 0.]])\ntensor([[2],\n        [3]])\n'
'x = Input(shape=...)\nshared_layer = MySharedLayer(...)\noutputs=[]\n# Step 2: Iterate for Ty steps\nfor t in range(Ty):\n   out = output_layer(x)\n   outputs.append(out)\nconcat = concatenate(outputs)\nlastlayer=Dense(shape=...)(concat)\n'
'import pandas as pd\nimport numpy as np\nimport datetime\narrays = [np.array([\'A\', \'A\', \'A\', \'B\', \'B\', \'C\', \'C\', \'D\', \'D\', \'D\', \'D\']),\n           np.array([\'2000-01\', \'2000-02\', \'2000-03\', \'1999-12\', \'2000-01\', \n          \'2000-01\', \'2000-02\', \'1999-12\', \'2000-01\', \'2000-02\', \'2000-03\'])]\n\n# Cast as datetime\narrays[1] = pd.to_datetime(arrays[1])\n\n\ndf = pd.DataFrame(np.random.randn(11, 4), index=arrays)\ndf.index.sort_values()\n\n\nfolds = df.reset_index() # df still has its multindex after this\n\n# You can tack an .iloc[:, 2:] to the end of these lines for just the values\n# Use your predefined conditions to access the datetimes\nfold1 = folds[folds["level_1"] &lt;=datetime.datetime(2000, 1, 1)]\nfold2 = folds[folds["level_1"] == datetime.datetime(2000, 2, 1)]\nfold3 = folds[folds["level_1"] == datetime.datetime(2000, 3, 1)]\n'
'def fashion_model(): #&lt;--\n'
'import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX, y = np.arange(10).reshape((5, 2)), range(5)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=42)\n\nX_train\n# array([[4, 5],\n#        [0, 1],\n#        [6, 7]])\n\ny_train\n# [2, 0, 3]\n\nX_test\n# array([[2, 3],\n#        [8, 9]])\n\ny_test\n# [1, 4]\n'
"import joblib\nimport pandas as pd \nfrom sklearn.preprocessing import OneHotEncoder\n\ncol_names = ['name', 'age']\ndata = [['tom', 10], ['nick', 15], ['juli', 14]] \n\nenc = OneHotEncoder(handle_unknown='error')\nenc.fit(data)\njoblib.dump(enc, 'encoder.joblib')\n\nenc = joblib.load('encoder.joblib')\ndata_df = pd.DataFrame(data=data, columns=col_names)\nenc_df = pd.DataFrame(data=enc.transform(data).toarray(), columns=enc.get_feature_names(col_names), dtype=bool)\ndf = pd.concat([data_df, enc_df], axis=1)\n\n|   | name | age | name_juli | name_nick | name_tom | age_10 | age_14 | age_15 |\n|---|------|-----|-----------|-----------|----------|--------|--------|--------|\n| 0 | tom  | 10  | False     | False     | True     | True   | False  | False  |\n| 1 | nick | 15  | False     | True      | False    | False  | False  | True   |\n| 2 | juli | 14  | True      | False     | False    | False  | True   | False  |\n"
'Location  Time1 Time2 Time3  Label\nA         3       2    1      1\nB         100     99   98     1\nC         98      99   100    0\n'
"for sign in 'ABCD':\n    df[sign] = (df['type'].where(df['sign'].eq(sign))\n                 .ffill()\n                 .eq('open')\n                 .astype(int)\n               )   \n\n    type sign  A  B  C  D\n0   open    A  1  0  0  0\n1   open    B  1  1  0  0\n2   open    D  1  1  0  1\n3  close    B  1  0  0  1\n4  close    D  1  0  0  0\n5   open    B  1  1  0  0\n6  close    B  1  0  0  0\n7  close    A  0  0  0  0\n"
'def vgg_to_coco(vgg_path: str, outfile: str=None, class_keyword: str = &quot;Class&quot;):\n    with open(vgg_path) as f:\n        vgg = json.load(f)\n\n    images_ids_dict = {v[&quot;filename&quot;]: i for i, v in enumerate(vgg.values())}\n    # TDOD fix\n    images_info = [{&quot;file_name&quot;: k, &quot;id&quot;: v, &quot;width&quot;: 1024, &quot;height&quot;: 1024} for k, v in images_ids_dict.items()]\n\n    classes = {class_keyword} | {r[&quot;region_attributes&quot;][class_keyword] for v in vgg.values() for r in v[&quot;regions&quot;]\n                                 if class_keyword in r[&quot;region_attributes&quot;]}\n    category_ids_dict = {c: i for i, c in enumerate(classes, 1)}\n    categories = [{&quot;supercategory&quot;: class_keyword, &quot;id&quot;: v, &quot;name&quot;: k} for k, v in category_ids_dict.items()]\n    annotations = []\n    suffix_zeros = math.ceil(math.log10(len(vgg)))\n    for i, v in enumerate(vgg.values()):\n        for j, r in enumerate(v[&quot;regions&quot;]):\n            if class_keyword in r[&quot;region_attributes&quot;]:\n                x, y = r[&quot;shape_attributes&quot;][&quot;all_points_x&quot;], r[&quot;shape_attributes&quot;][&quot;all_points_y&quot;]\n                annotations.append({\n                    &quot;segmentation&quot;: [list(chain.from_iterable(zip(x, y)))],\n                    &quot;area&quot;: helper.polygon_area(x, y),\n                    &quot;bbox&quot;: helper.bbox(x, y, out_format=&quot;width_height&quot;),\n                    &quot;image_id&quot;: images_ids_dict[v[&quot;filename&quot;]],\n                    &quot;category_id&quot;: category_ids_dict[r[&quot;region_attributes&quot;][class_keyword]],\n                    &quot;id&quot;: int(f&quot;{i:0&gt;{suffix_zeros}}{j:0&gt;{suffix_zeros}}&quot;),\n                    &quot;iscrowd&quot;: 0\n                })\n\n    coco = {\n        &quot;images&quot;: images_info,\n        &quot;categories&quot;: categories,\n        &quot;annotations&quot;: annotations\n    }\n    if outfile is None:\n        outfile = vgg_path.replace(&quot;.json&quot;, &quot;_coco.json&quot;)\n    with open(outfile, &quot;w&quot;) as f:\n        json.dump(coco, f)\n'
"for m in my_rng:\n    modelSGD = SGDRegressor(alpha=0.00001, penalty='l1')\n    modelSGD.fit(X_train[:m], y_train[:m])\n    [...]\n\nfor m in range(10, 180001, 30000):\n    print(m)\n\n\n10\n30010\n60010\n90010\n120010\n150010\n\nprevious = 0\nfor m in range(30000, 180001, 30000):\n    modelSGD.partial_fit(X_train[previous:m], y_train[previous:m])\n    previous = m\n\n# training set ranges\n0 30000\n30000 60000\n60000 90000\n90000 120000\n120000 150000\n150000 180000\n\nmy_rng = range(0 ,len(X_train), 30000)\nprevious = 0\nmodelSGD = SGDRegressor(alpha=0.00001, penalty='l1')\nfor m in my_rng:\n    modelSGD.partial_fit(X_train[previous:m], y_train[previous:m])\n    ypred_train = modelSGD.predict(X_train[previous:m])\n    ypred_test = modelSGD.predict(X_test)\n    mse_train = mean_squared_error(y_train[previous:m], ypred_train)\n    mse_test = mean_squared_error(y_test, ypred_test)\n    scores_train.append(mse_train)\n    scores_test.append(mse_test)\n"
' color   shape   fruit\n orange  oblong  orange\n red     round   apple\n orange  round   orange\n red     oblong  apple\n red     round   apple\n\nfeature   class  |  feature  class\norange    orange |  oblong   orange\nred       apple  |  round    apple\norange    orange |  round    orange\nred       apple  |  oblong   apple\nred       apple  |  round    apple\n\n        color\n___________________\n|                 |\n|                 |\nred               orange\napple             orange\n\nfrom sklearn.trees import DecisionTreeClassifier\n#make your sample matrix \nsamples = [[1,1], [0,0], [1,0], [0,1], [0,0]]\n#make your target vector ( in this case fruit)\nfruitname = [1, 0, 1, 0, 0]\n#create and fit the model\ndtree =  DecisionTreeClassifier()\ndtree =  dtree.fit(samples, fruitname)\n#test an unknown red fruit that is oblong\ndtree.predict([0,1])\n'
"In [23]: classifier.prob_classify({'unknown_words': True}).prob('spam')\nOut[23]: 0.40000333322222587\n"
"new_matrix = scipy.sparse.hstack((big_feature_matrix, small_feature_matrix),\n                                 format='csr')\n"
'import numpy as np\nrng = np.random.RandomState(42)\n\nn_samples, n_features, n_active_vars = 20, 10, 5\nX = rng.randn(n_samples, n_features)\nX = ((X - X.mean(0)) / X.std(0))\n\nbeta = rng.randn(n_features)\nbeta[rng.permutation(n_features)[:n_active_vars]] = 0.\n\ny = X.dot(beta)\n\nprint X.std(0)\nprint X.mean(0)\n\nfrom sklearn.linear_model import Lasso\n\nlasso1 = Lasso(alpha=.1)\nprint lasso1.fit(X, y).coef_\n\nlasso2 = Lasso(alpha=.1, normalize=True)\nprint lasso2.fit(X, y).coef_\n\nlasso1.fit(X / np.sqrt(n_samples), y).coef_ / np.sqrt(n_samples)\n\nlasso2.fit(X, y).coef_\n\nlasso3 = Lasso(alpha=.1 / np.sqrt(n_samples), normalize=True)\nprint lasso3.fit(X, y).coef_  # yields the same coefficients as lasso1.fit(X, y).coef_\n'
'A.shape == (20, 1, 3)\nB.shape ==     (4, 3)\n'
'# Vector of features per example.\nx = tf.placeholder(tf.float32, shape=[batch_size, num_features])\n\n# Scalar weight per example.\nx_weights = tf.placeholder(tf.float32, shape=[batch_size])\n\n# Vector of outputs per example.\ny = tf.placeholder(tf.float32, shape=[batch_size, num_outputs])\n\n# ...\nlogits = ...\n\n# Insert appropriate cost function here.\ncost = tf.nn.softmax_cross_entropy_with_logits(logits, y)\n\noverall_cost = tf.mul(cost, x_weights) / batch_size\n'
"return [tf.matmul(output, _weights['out']) + _biases['out'] for output in outputs]\n\ntransformed_outputs = [tf.matmul(output, _weights['out']) + _biases['out'] for output in outputs]\nreturn tf.concat(concat_dim=0, values=transformed_outputs)\n"
"def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n    assert inputs.shape[0] == targets.shape[0]\n    if shuffle:\n        indices = np.arange(inputs.shape[0])\n        np.random.shuffle(indices)\n    for start_idx in range(0, inputs.shape[0] - batchsize + 1, batchsize):\n        if shuffle:\n            excerpt = indices[start_idx:start_idx + batchsize]\n        else:\n            excerpt = slice(start_idx, start_idx + batchsize)\n        yield inputs[excerpt], targets[excerpt]\n\nfor n in xrange(n_epochs):\n    for batch in iterate_minibatches(X, Y, batch_size, shuffle=True):\n        x_batch, y_batch = batch\n        l_train, acc_train = f_train(x_batch, y_batch)\n\n    l_val, acc_val = f_val(Xt, Yt)\n    logging.info('epoch ' + str(n) + ' ,train_loss ' + str(l_train) + ' ,acc ' + str(acc_train) + ' ,val_loss ' + str(l_val) + ' ,acc ' + str(acc_val))\n"
'# Assuming all variables to be fine-tuned have a name that starts with\n# "layer17/".\nopt_vars = [v for v in tf.trainable_variables() if v.name.startswith("layer17/")]\n\ntrain_op = optimizer.minimize(loss, var_list=opt_vars)\n'
'graph.get_tensor_by_name("import/layer_name")\n'
'clf.fit(X,y)\n'
'itertools.izip(*iterables)\n\nMake an iterator that aggregates elements from each of the iterables. Like zip() except that it returns an iterator instead of a list. Used for lock-step iteration over several iterables at a time.\n'
'train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n'
'inputs = [[0,0], [1,1], [0,1], [1,0]]\noutputs = [0, 1, 0, 0]\noutputs = np.asarray(outputs, dtype=\'int32\').reshape((len(outputs), 1))\n\nx = T.dmatrix("x")\n# y = T.dvector("y")\ny = T.dmatrix("y")\nb = theano.shared(value=1.0, name=\'b\')\n\nalpha = 0.01\ntraining_steps = 30000\n\nw_values = np.asarray(np.random.uniform(low=-1, high=1, size=(2, 1)), dtype=theano.config.floatX)\nw = theano.shared(value=w_values, name=\'w\', borrow=True)\n\nhypothesis = T.nnet.sigmoid(T.dot(x, w) + b)\n# hypothesis = T.flatten(hypothesis)\ncost = T.sum((y - hypothesis) ** 2)\nupdates = [\n    (w, w - alpha * T.grad(cost, wrt=w)),\n    (b, b - alpha * T.grad(cost, wrt=b))\n]\n\ntrain = theano.function(inputs=[x, y], outputs=[hypothesis, cost], updates=updates)\ntest = theano.function(inputs=[x], outputs=[hypothesis])\n\n# Training\ncost_history = []\n\nfor i in range(training_steps):\n    if (i+1) % 5000 == 0:\n        print "Iteration #%s: " % str(i+1)\n        print "Cost: %s" % str(cost)\n    h, cost = train(inputs, outputs)\n    cost_history.append(cost)\n'
'n_folds = 5\nbest_nrounds = int((res.shape[0] - estop) / (1 - 1 / n_folds))\n\nvalidation_slice = 0.2\nbest_nrounds = int((res.shape[0] - estop) / (1 - validation_slice))\n'
"output = tf.add(tf.matmul(l3, output_layer['weights']), output_layer['biases'])\n\nprediction = tf.sigmoid(output)\n\n predicted_class = tf.greater(prediction,0.5)\n correct = tf.equal(predicted_class, tf.equal(y,1.0))\n accuracy = tf.reduce_mean( tf.cast(correct, 'float') )\n"
'x = tf.placeholder(tf.float32, [None, 784])\n'
"df &lt;- read.csv('gapminder.csv')\ndf &lt;- df[c('internetuserate', 'urbanrate')]\ndf &lt;- df[complete.cases(df),]\ndim(df)\n# [1] 190   2\nm &lt;- lm(internetuserate~urbanrate, df)\nsummary(m)\n#Call:\n#lm(formula = internetuserate ~ urbanrate, data = df)\n\n#Residuals:\n#    Min      1Q  Median      3Q     Max \n#-51.474 -15.857  -3.954  14.305  74.590 \n\n#Coefficients:\n#            Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept) -4.90375    4.11485  -1.192    0.235    \n#urbanrate    0.72022    0.06753  10.665   &lt;2e-16 ***\n#---\n#Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n# \n#Residual standard error: 22.03 on 188 degrees of freedom\n#Multiple R-squared:  0.3769,   Adjusted R-squared:  0.3736 \n#F-statistic: 113.7 on 1 and 188 DF,  p-value: &lt; 2.2e-16\n\nimport pandas\nimport statsmodels.formula.api as smf \ndata = pandas.read_csv('gapminder.csv')\ndata = data[['internetuserate', 'urbanrate']]\ndata['internetuserate'] = pandas.to_numeric(data['internetuserate'], errors='coerce')\ndata['urbanrate'] = pandas.to_numeric(data['urbanrate'], errors='coerce')\ndata = data.dropna(axis=0, how='any')\nprint data.shape\n# (190, 2)\nreg1 = smf.ols('internetuserate ~  urbanrate', data=data).fit()\nprint (reg1.summary())\n#                           OLS Regression Results\n#==============================================================================\n#Dep. Variable:        internetuserate   R-squared:                       0.377\n#Model:                            OLS   Adj. R-squared:                  0.374\n#Method:                 Least Squares   F-statistic:                     113.7\n#Date:                Fri, 20 Jan 2017   Prob (F-statistic):           4.56e-21\n#Time:                        23:27:50   Log-Likelihood:                -856.14\n#No. Observations:                 190   AIC:                             1716.\n#Df Residuals:                     188   BIC:                             1723.\n#Df Model:                           1\n#Covariance Type:            nonrobust\n#================================================================================\n#                     coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]\n#    ------------------------------------------------------------------------------\n#    Intercept     -4.9037      4.115     -1.192      0.235       -13.021     3.213\n#    urbanrate      0.7202      0.068     10.665      0.000         0.587     0.853\n#================================================================================\n#    Omnibus:                       10.750   Durbin-Watson:                   2.097\n#    Prob(Omnibus):                  0.005   Jarque-Bera (JB):               10.990\n#    Skew:                           0.574   Prob(JB):                      0.00411\n#    Kurtosis:                       3.262   Cond. No.                         157.\n#==============================================================================\n"
'tokenizer_predict = Tokenizer(nb_words=MAX_NB_WORDS)\ntokenizer_predict.fit_on_texts(texts_predict)\nsequence_predict = tokenizer_predict.texts_to_sequences(predict_data)\n\ntokenizer_predict = Tokenizer(nb_words=MAX_NB_WORDS)\ntokenizer_predict.fit_on_texts(texts_train)\nsequence_predict = tokenizer_predict.texts_to_sequences(predict_data)\n'
"conv1d_on_image = Convolution2D(output_channels, 1, dim_y, border_mode='valid')(input)\n\nconv1d_on_image = Reshape((dim_x, output_channels))(conv1d_on_image)\n"
'm.add(Lambda(lambda x: x[0], output_shape=(1,)))\n\nm.add(Lambda(lambda x: x[:,:1], output_shape=(1,))) \n'
'import pandas as pd\n\n# Made-up training dataset\ntrain = pd.DataFrame({\'animal\': [\'cat\', \'cat\', \'dog\', \'dog\', \'fish\', \'fish\', \'bear\'],\n                      \'age\': [12, 13, 31, 12, 12, 32, 90]})\n\n# Made-up test dataset (notice how two classes are from train are missing entirely)\ntest = pd.DataFrame({\'animal\': [\'fish\', \'fish\', \'dog\'],\n                      \'age\': [15, 62, 1]})\n\n# Discrete column to be one-hot-encoded\ncol = \'animal\'\n\n# Create dummy variables for each level of `col`\ntrain_animal_dummies = pd.get_dummies(train[col], prefix=col)\ntrain = train.join(train_animal_dummies)\n\ntest_animal_dummies = pd.get_dummies(test[col], prefix=col)\ntest = test.join(test_animal_dummies)\n\n# Find the difference in columns between the two datasets\n# This will work in trivial case, but if you want to limit to just one feature\n# use this: f = lambda c: col in c; feature_difference = set(filter(f, train)) - set(filter(f, test))\nfeature_difference = set(train) - set(test)\n\n# create zero-filled matrix where the rows are equal to the number\n# of row in `test` and columns equal the number of categories missing (i.e. set difference \n# between relevant `train` and `test` columns\nfeature_difference_df = pd.DataFrame(data=np.zeros((test.shape[0], len(feature_difference))),\n                                     columns=list(feature_difference))\n\n# add "missing" features back to `test\ntest = test.join(feature_difference_df)\n\n   age animal  animal_dog  animal_fish\n0   15   fish         0.0          1.0\n1   62   fish         0.0          1.0\n2    1    dog         1.0          0.0\n\n   age animal  animal_dog  animal_fish  animal_cat  animal_bear\n0   15   fish         0.0          1.0         0.0          0.0\n1   62   fish         0.0          1.0         0.0          0.0\n2    1    dog         1.0          0.0         0.0          0.0\n'
'x % 2\n\nactivation(x * w + b)\n'
'neg_correct = float(np.sum(np.invert(makePrediction(theta,neg).astype(np.int))))\n\nfrom __future__ import division\n'
'def g():\n  yield 1, 10.0, "foo"\n  yield 2, 20.0, "bar"\n  yield 3, 30.0, "baz"\n\ndataset = tf.data.Dataset.from_generator(g, (tf.int32, tf.float32, tf.string))\n\niterator = dataset.make_one_shot_iterator()\n\nint_tensor, float_tensor, str_tensor = iterator.get_next()\n'
"encoder_inputs = model.input[0] #input_1\nencoder_outputs, state_h_enc, state_c_enc = model.layers[2].output # lstm_1\nencoder_states = [state_h_enc, state_c_enc]\nencoder_model = Model(encoder_inputs, encoder_states)\n\ndecoder_inputs = model.input[1] #input_2\ndecoder_state_input_h = Input(shape=(latent_dim,),name='input_3')\ndecoder_state_input_c = Input(shape=(latent_dim,),name='input_4')\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_lstm = model.layers[3]\ndecoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h_dec, state_c_dec]\ndecoder_dense = model.layers[4]\ndecoder_outputs=decoder_dense(decoder_outputs)\n\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n"
'J = J + e * np.sum(abs(theta))\n\nJ = J + alpha * e * (theta &gt;= 0).astype(float)\n'
"lightgbm.cv(params,metrics=['auc','ks'])\n"
"service = googleapiclient.discovery.build('ml', 'v1')\nname = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\nname += '/versions/{}'.format(VERSION_NAME)\n\nresponse = service.projects().predict(\n    name=name,\n    body={'instances': data}\n).execute()\n\nif 'error' in response:\n    print (response['error'])\nelse:\n  online_results = response['predictions']\n"
"# Plot regression line\nplt.plot(prediction_space, y_pred, color='black', linewidth=3)\nplt.show()\n"
"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.seed(42)\nX = np.random.rand(2000, 2)\ny = np.random.randint(0,2,2000)\n\nC = np.array([1, 10, 100, 1000])\ngamma = np.array([1e-3, 1e-4])\n\navg_rbf_f1 = []\n\nfor a in C:\n    for b in gamma:\n        rbf_model = svm.SVC(kernel='rbf',C=a, gamma=b)\n        rbf_scores = cross_val_score(rbf_model, X, y, cv=10, scoring='f1_macro')\n        avg_rbf_f1.append(np.mean(rbf_scores))\n\nbest_gamma = gamma[np.argmax(avg_rbf_f1)]\nbest_C = C[np.argmax(avg_rbf_f1)]\n\nprint('The gamma with the highest accuracy is {}'.format(best_gamma))\nprint('The C with the highest accuracy is {}'.format(best_C))\n\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-30-84d1adf5e2d9&gt; in &lt;module&gt;()\n     17         avg_rbf_f1.append(np.mean(rbf_scores))\n     18 \n---&gt; 19 best_gamma = gamma[np.argmax(avg_rbf_f1)]\n     20 best_C = C[np.argmax(avg_rbf_f1)]\n     21 \n\nIndexError: index 6 is out of bounds for axis 0 with size 2\n\nfrom sklearn import svm\nfrom sklearn.model_selection import cross_val_score\n\nnp.random.rand(42)\nX = np.random.rand(2000, 2)\ny = np.random.randint(0,2,2000)\n\nC = np.array([1, 10, 100, 1000])\ngamma = np.array([1e-3, 1e-4])\n\navg_rbf_f1 = []\nsearch = []\n\nfor a in C:\n    for b in gamma:\n        search.append((a,b))\n        rbf_model = svm.SVC(kernel='rbf',C=a, gamma=b)\n        rbf_scores = cross_val_score(rbf_model, X, y, cv=10, scoring='f1_macro')\n        avg_rbf_f1.append(np.mean(rbf_scores))\n\nbest_C, best_gamma = search[np.argmax(avg_rbf_f1)]\n\nprint('The gamma with the highest accuracy is {}'.format(best_gamma))\nprint('The C with the highest accuracy is {}'.format(best_C))\n"
'from sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\npipe_svc = make_pipeline(Imputer(),StandardScaler(),PCA(n_components=2),SVC(random_state=1))\nparam_range = [0.001,0.01,0.1,1,10,100,1000]\nparam_grid = {\'svc__C\': [0.001,0.01,0.1,1,10,100,1000], \'svc__kernel\': [\'linear\', \'rbf\'],\n              \'svc__gamma\': [0.001,0.01,0.1,1,10,100,1000]}\ncv = StratifiedKFold(n_splits=5)\ngs = GridSearchCV(estimator=pipe_svc,param_grid=param_grid, scoring=\'accuracy\', cv = cv,\n                  return_train_score=True)\ngs.fit(X_train, y_train)\n\nprint("Best Estimator: \\n{}\\n".format(gs.best_estimator_))\nprint("Best Parameters: \\n{}\\n".format(gs.best_params_))\nprint("Best Test Score: \\n{}\\n".format(gs.best_score_))\nprint("Best Training Score: \\n{}\\n".format(gs.cv_results_[\'mean_train_score\'][gs.best_index_]))\nprint("All Training Scores: \\n{}\\n".format(gs.cv_results_[\'mean_train_score\']))\nprint("All Test Scores: \\n{}\\n".format(gs.cv_results_[\'mean_test_score\']))\n# # This prints out all results during Cross-Validation in details\n#print("All Meta Results During CV Search: \\n{}\\n".format(gs.cv_results_))\n\nBest Estimator: \nPipeline(memory=None,\n         steps=[(\'imputer\', Imputer(axis=0, copy=True,\n         missing_values=\'NaN\', strategy=\'mean\', verbose=0)),\n         (\'standardscaler\', StandardScaler(copy=True, with_mean=True, \n         with_std=True)), (\'pca\', PCA(copy=True, iterated_power=\'auto\', \n         n_components=2, random_state=None,\n         svd_solver=\'auto\', tol=0.0, whiten=False)...ar\',\n         max_iter=-1, probability=False, random_state=1, shrinking=True,\n         tol=0.001, verbose=False))])\n\nBest Parameters: \n{\'svc__gamma\': 0.001, \'svc__kernel\': \'linear\', \'svc__C\': 1}\n\nBest Test Score: \n0.9422110552763819\n\nBest Training Score: \n0.9440783896216558\n\nAll Training Scores: \n[0.90012027 0.64070503 0.90012027 0.64070503 0.90012027 0.64070503\n 0.90012027 0.64070503 0.90012027 0.64070503 0.90012027 0.64070503\n 0.90012027 0.64070503 0.92587291 0.64070503 0.92587291 0.64070503\n 0.92587291 0.64070503 0.92587291 0.64070503 0.92587291 0.64070503\n 0.92587291 0.64070503 0.92587291 0.64070503 0.93779697 0.68906962\n 0.93779697 0.91582382 0.93779697 0.92901362 0.93779697 0.88879951\n 0.93779697 0.64070503 0.93779697 0.64070503 0.93779697 0.64070503\n 0.94407839 0.91394491 0.94407839 0.93277932 0.94407839 0.93968376\n 0.94407839 0.95413931 0.94407839 0.98052483 0.94407839 0.9949725\n 0.94407839 0.99937304 0.94533822 0.93090042 0.94533822 0.94345143\n 0.94533822 0.94911575 0.94533822 0.96293448 0.94533822 0.99434357\n 0.94533822 1.         0.94533822 1.         0.94533822 0.94219554\n 0.94533822 0.94219357 0.94533822 0.95099466 0.94533822 0.98052286\n 0.94533822 1.         0.94533822 1.         0.94533822 1.\n 0.94596518 0.9428225  0.94596518 0.94345537 0.94596518 0.95539323\n 0.94596518 0.99371858 0.94596518 1.         0.94596518 1.\n 0.94596518 1.        ]\n\nAll Test Scores: \n[0.88944724 0.64070352 0.88944724 0.64070352 0.88944724 0.64070352\n 0.88944724 0.64070352 0.88944724 0.64070352 0.88944724 0.64070352\n 0.88944724 0.64070352 0.92713568 0.64070352 0.92713568 0.64070352\n 0.92713568 0.64070352 0.92713568 0.64070352 0.92713568 0.64070352\n 0.92713568 0.64070352 0.92713568 0.64070352 0.9321608  0.68090452\n 0.9321608  0.90954774 0.9321608  0.92211055 0.9321608  0.84422111\n 0.9321608  0.64070352 0.9321608  0.64070352 0.9321608  0.64070352\n 0.94221106 0.9120603  0.94221106 0.92713568 0.94221106 0.91959799\n 0.94221106 0.93969849 0.94221106 0.81407035 0.94221106 0.65075377\n 0.94221106 0.64572864 0.94221106 0.92964824 0.94221106 0.92964824\n 0.94221106 0.92462312 0.94221106 0.92211055 0.94221106 0.80653266\n 0.94221106 0.65326633 0.94221106 0.64572864 0.94221106 0.92964824\n 0.94221106 0.93969849 0.94221106 0.92713568 0.94221106 0.90954774\n 0.94221106 0.82663317 0.94221106 0.65326633 0.94221106 0.64572864\n 0.93969849 0.94221106 0.93969849 0.93467337 0.93969849 0.92964824\n 0.93969849 0.87939698 0.93969849 0.8241206  0.93969849 0.65326633\n 0.93969849 0.64572864]\n'
'# Suppose X_train is your 100 x 100 dataset\n# and y_train is your array of labels\nidx = np.arange(len(X_train))\nnp.shuffle(idx)\n\nNUM_SAMPLES = 50\nsampled_idxs = idx[:NUM_SAMPLES]\nrest_idxs = idx[NUM_SAMPLES:]\n\nX_samples = X_train[sampled_idxs]\nX_rest = X_train[rest_idxs]\ny_samples = y_train[sampled_idxs]\ny_rest = y_train[rest_idxs]\n\nfrom sklearn.model_selection import test_train_split\nX_samples, X_rest, y_samples, y_rest = train_test_split(X_train, y_train,\n                                                        train_size=NUM_SAMPLES,\n                                                        random_state=123)\n'
'kv_model.n_similarity(sent1.split(), sent2.split())\n'
"model.add(layers.Dense(16, activation='relu', input_shape=(6,)))\n"
'model = Model(inputs=[train_x_1,train_x_2], outputs=train_y_class)\n\nmodel = Model(inputs=[first_input,second_input], outputs=merge_activation2)\n'
"from keras.layers import Layer\nfrom keras import backend as K\n\nclass Swish(Layer):\n    def __init__(self, beta, **kwargs):\n        super(Swish, self).__init__(**kwargs)\n        self.beta = K.cast_to_floatx(beta)\n\n    def call(self, inputs):\n        return K.sigmoid(self.beta * inputs) * inputs\n\n    def get_config(self):\n        config = {'beta': float(self.beta)}\n        base_config = super(Swish, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n# ...\nmodel.add(Swish(beta=0.3))\n"
'method = data[feature].values\n\nfor feature in label_encode:\n    data[feature] = le.fit_transform(data[feature].values)\n'
"import sklearn.datasets\nX,Y = sklearn.datasets.make_classification(n_samples=100, n_features=6, n_redundant=0,n_informative=6, n_classes=20)\n\nimport numpy as np\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical\nfrom keras import backend as K\nK.clear_session()\n\nmodel = Sequential()\nmodel.add(Dense(50,input_dim=X.shape[1],activation='softmax'))\nmodel.add(Dense(20,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', \n              metrics=['accuracy'])\nmodel.summary()\nmodel.fit(X, to_categorical(Y), epochs=1000, verbose=1) # &lt;---\n"
'z = zscore(x.reshape(-1, 3)).reshape(x.shape)\n'
'self.w2v_rnode = nn.GRU(embeddings.size(1), hidden_dim, bidirectional=True, dropout=drop_prob)\n\nw2v_out, _ = self.w2v_rnode(embeds.view(-1, batch_size, embeds.size(2)))\n\nw2v_out, _ = self.w2v_rnode(embeds.permute(1, 0, 2))\n\n_, last_hidden = self.w2v_rnode(embeds.permute(1, 0, 2))\n\nimport torch\n\n\ndef length_sort(features):\n    # Get length of each sentence in batch\n    sentences_lengths = torch.tensor(list(map(len, features)))\n    # Get indices which sort the sentences based on descending length\n    _, sorter = sentences_lengths.sort(descending=True)\n    # Pad batch as you have the lengths and sorter saved already\n    padded_features = torch.nn.utils.rnn.pad_sequence(features, batch_first=True)\n    return padded_features, sentences_lengths, sorter\n\n\ndef pad_collate_fn(batch):\n    # DataLoader return batch like that unluckily, check it on your own\n    features, labels = (\n        [element[0] for element in batch],\n        [element[1] for element in batch],\n    )\n    padded_features, sentences_lengths, sorter = length_sort(features)\n    # Sort by length features and labels accordingly\n    sorted_padded_features, sorted_labels = (\n        padded_features[sorter],\n        torch.tensor(labels)[sorter],\n    )\n    return sorted_padded_features, sorted_labels, sentences_lengths\n\nimport torch\n\na = torch.tensor([[1, 2], [3, 4], [5, 6]])\n\nprint(a)\nprint(a.permute(1, 0))\nprint(a.view(2, 3))\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\ntensor([[1, 3, 5],\n        [2, 4, 6]])\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n'
'const image = document.getElementById(id)\n\ncocoSsd.load()\n  .then(model =&gt; model.detect(image))\n  .then(prediction =&gt; {\nif (prediction.class === OBJECT_DETECTED) {\n  // display it the bbox to the user}\n})\n'
'wights=[12,10,31,1];  \nmod_loss = mymodel.train_on_batch([X_train], [Y1, Y2],sample_weight={"y2_layername":wights})\n'
"# sample data creation\ndata = pd.DataFrame(np.random.rand(4000,2),columns=['Item_Identifier','Item_Visibility'])\ndata.loc[:,'Item_Identifier']= data.loc[:,'Item_Identifier'].apply(\n        lambda x: 'id1' if x&gt; 0.4 else 'id2')\n# creating map_table so we could map values\nmap_table = data.groupby('Item_Identifier').mean()\n# mapping values\ndata.loc[:,'Item_Visibility'] = data.loc[:,'Item_Identifier'].map(\n        map_table.to_dict()['Item_Visibility'])\n"
"#convert column to numeric\ndf['label_weight'] = df['label_weight'].astype(int)\n#pandas 0.24+\ndf['assigned_label'] = (df.set_index('label')\n                          .groupby('Id')['label_weight']\n                          .transform('idxmax')\n                          .to_numpy())\n\n#pandas below 0.24\ndf['assigned_label'] = (df.set_index('label')\n                          .groupby('Id')['label_weight']\n                          .transform('idxmax')\n                          .values)\n\nprint (df)\n   Id  label_weight label assgined_label\n0   A            30     H              H\n1   A            30     H              H\n2   A            30     H              H\n3   A            28     M              H\n4   B            29     H              M\n5   B            31     M              M\n6   B            31     M              M\n7   B            30     L              M\n8   C            26     H              L\n9   C            26     H              L\n10  C            28     L              L\n11  C            28     L              L\n"
'model = Inception_V3(..., weights=None,...)\n'
'def answer_four():\n    from sklearn.metrics import confusion_matrix\n    from sklearn.svm import SVC\n    from sklearn.calibration import CalibratedClassifierCV\n    from sklearn.model_selection import train_test_split\n\n    #SVC without mencions of kernel, the default is rbf\n    svc = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)\n\n    #decision_function scores: Predict confidence scores for samples\n    y_score = svc.decision_function(X_test)\n\n    #Set a threshold -220\n    y_score = np.where(y_score &gt; -220, 1, 0)\n    conf_matrix = confusion_matrix(y_test, y_score)\n\n####threshold###\n#input threshold in the model after trained this model\n#threshold is a limiar of separation of class   \n\nreturn conf_matrix\n\nanswer_four()\n#output: \narray([[5320,   24],\n       [  14,   66]])\n'
'train, validate, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n'
'tag_space   = self.hidden2tag(lstm_out[-1])\n'
"# make a prediction for a new image.\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.models import load_model\nfrom numpy import argmax\n# load and prepare the image\ndef load_image(filename):\n    # load the image\n    img = load_img(filename, target_size=(224, 224))\n    # convert to array\n    img = img_to_array(img)\n    # reshape into a single sample with 3 channels\n    img = img.reshape(1, 224, 224, 3)\n    # center pixel data\n    img = img.astype('float32')\n    img = img - [123.68, 116.779, 103.939]\n    return img\n\n# load an image and predict the class\ndef run_example():\n    # load the image\n    img = load_image('D:/___COMMON/Project/Test_area/VGG16-classifier_animals/train/horse-90.jpg')\n    # load model    \n    model = load_model('animal_model_sparse.h5')\n    # predict the class\n    result = model.predict(img)\n    print(result[0])\n\n# entry point, run the example\nrun_example()\n\n[0. 0. 1.]\n"
"model.add(TimeDistributed(Conv2D(64, 5, activation='relu', padding='same', name='conv1'), input_shape=(5, 32, 32, 4)))\n"
'params=old_params - lr* grad \n'
"df1['locationid']=df1.locationid.fillna(df1.groupby('geo_loc')['locationid'].transform('max'))\nprint (df1)\n   locationid geo_loc\n0       111.0     G12\n1       158.0     K11\n2       145.0     B16\n3       111.0     G12\n4       189.0     B22\n5       145.0     B16\n6       158.0     K11\n7       145.0     B16\n\ndf1 = pd.DataFrame({'locationid':[111, np.nan, 145, np.nan, 189,np.nan, 158, 145],\n                     'geo_loc':['G12','K11','B16','G12','B22','B16', 'K11', 'B16']})\n\n#sample data strings with missing values\ndf1['locationid'] = df1['locationid'].dropna().astype(str) + 'a'\n\n\ndf1['locationid']= (df1.groupby('geo_loc')['locationid']\n                       .transform(lambda x: x.fillna(x.dropna().max())))\n\nprint (df1)\n  locationid geo_loc\n0     111.0a     G12\n1     158.0a     K11\n2     145.0a     B16\n3     111.0a     G12\n4     189.0a     B22\n5     145.0a     B16\n6     158.0a     K11\n7     145.0a     B16\n"
'self.dloss = lambda x, y: -(np.divide(y, clip(x)) - np.divide(1 - y, 1 - clip(x)))\n\nself.dloss = lambda x, y: -(np.divide(clip(y), clip(x)) - np.divide(1 - clip(y), 1 - clip(x)))\n\ndfunction = self.dactivate(last_derivative)\n\ndfunction = last_derivative*self.dactivate(np.dot(self.layer_input, self.weights) + self.biases)\n\nd_b = (1./self.layer_input.shape[1]) * np.dot(np.ones((self.biases.shape[0], last_derivative.shape[0])), last_derivative)\n\nd_b = (1./self.layer_input.shape[1]) * np.dot(np.ones((self.biases.shape[0], last_derivative.shape[0])), dfunction)\n\nself.weights = np.random.randn(neurons, self.neurons) * np.divide(6, np.sqrt(self.neurons * neurons))\nself.biases = np.random.randn(1, self.neurons) * np.divide(6, np.sqrt(self.neurons * neurons))\n\nself.weights = np.random.randn(neurons, self.neurons) * np.divide(6, np.sqrt(self.neurons * neurons)) / 100\nself.biases = np.random.randn(1, self.neurons) * np.divide(6, np.sqrt(self.neurons * neurons)) / 100\n'
"# Set the classifier as an XGBClassifier\n\nclf_pipeline = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', XGBClassifier(n_jobs=6, n_estimators=20))\n    ]\n)\n\n\n# In[41]:\n\n# Cross validation: 60 iterations with 3 fold CV.\n\nn_features_after_transform = clf_pipeline.named_steps.preprocessor.fit_transform(df).shape[1]\n\nparam_grid = {\n    'classifier__max_depth':stats.randint(low=2, high=100),\n    'classifier__max_features':stats.randint(low=2, high=n_features_after_transform),\n    'classifier__gamma':stats.uniform.rvs(0, 0.25, size=10000),\n    'classifier__subsample':stats.uniform.rvs(0.5, 0.5, size=10000),\n    'classifier__reg_alpha':stats.uniform.rvs(0.5, 1., size=10000),\n    'classifier__reg_lambda':stats.uniform.rvs(0.5, 1., size=10000)\n}\n\nrscv = RandomizedSearchCV(\n    clf_pipeline,\n    param_grid,\n    n_iter=60,\n    scoring='roc_auc',\n    cv=StratifiedKFold(n_splits=3, shuffle=True)\n\n)\n\nrscv.fit(df, y)\n\n\n# In[42]:\n\n\n# Set the tuned best params and beef up the number of estimators.\n\nclf_pipeline.set_params(**rscv.best_params_)\nclf_pipeline.named_steps.classifier.set_params(n_estimators=200)  \n"
"import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\nclasses = np.array(['A', 'B'])\nvals = np.array(['A', 'A', 'A'])\nvals = vals.reshape(-1, 1)\n\nohe = OneHotEncoder(sparse=False)\nohe.fit(classes.reshape(-1, 1))\n\nohe.transform(vals)\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.]])\n"
'deep_items = value.get_params().items()\n\ndef __init__(self, distance=10):\n    self.la = (34.05, -118.24)\n    self.sf = (37.77, -122.41)\n    self.distance = distance # &lt;-- give same name\n'
'# fill nans with zeros\nif weight_matrix is not None:\n    weight_matrix[np.isnan(weight_matrix)] = 0.0\n\ndist_pot_donors : ndarray of shape (n_receivers, n_potential_donors)\n    Distance matrix between the receivers and potential donors from\n    training set. There must be at least one non-nan distance between\n    a receiver and a potential donor.\n\nX = np.array([[np.nan,7,4,5],[2,8,4,5],[3,7,4,6],[1,np.nan,np.nan,5]])\n\nprint(X)\narray([[nan,  7.,  4.,  5.],\n       [ 2.,  8.,  4.,  5.],\n       [ 3.,  7.,  4.,  6.],\n       [ 1., nan, nan,  5.]])\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=1)\n\nimputer.fit_transform(X)\narray([[1., 7., 4., 5.],\n       [2., 8., 4., 5.],\n       [3., 7., 4., 6.],\n       [1., 7., 4., 5.]])\n'
"books = pd.crosstab(df.shop, df.book_id)\n\n# underlying numpy\narr = books.values\n\ncommon = (arr[None,...] | arr[:,None,:]).sum(-1)\n\noutput = (books @ books.T)/common\n\nshop         A         B    C         D    E\nshop                                        \nA     1.000000  0.333333  0.0  1.000000  0.5\nB     0.333333  1.000000  0.0  0.333333  0.0\nC     0.000000  0.000000  1.0  0.000000  0.0\nD     1.000000  0.333333  0.0  1.000000  0.5\nE     0.500000  0.000000  0.0  0.500000  1.0\n\noutput = (output.stack().rename_axis(['shop_1','shop_2'])\n                .reset_index(name='jaccard')\n                .query('shop_1 != shop_2')\n         )\n\n   shop_1 shop_2   jaccard\n1       A      B  0.333333\n2       A      C  0.000000\n3       A      D  1.000000\n4       A      E  0.500000\n5       B      A  0.333333\n7       B      C  0.000000\n8       B      D  0.333333\n9       B      E  0.000000\n10      C      A  0.000000\n11      C      B  0.000000\n13      C      D  0.000000\n14      C      E  0.000000\n15      D      A  1.000000\n16      D      B  0.333333\n17      D      C  0.000000\n19      D      E  0.500000\n20      E      A  0.500000\n21      E      B  0.000000\n22      E      C  0.000000\n23      E      D  0.500000\n"
"freq=[10, 2, 1, 10, 6, 4, 1, 1, 6, 3, 4, 10, 6, 3, 9, 5, 5, 5, 4, 2, 2, 9, 11, 7, 5, 1, 3, 10, 7, 5, 5, 5, 8, 7, 25, 17, 9, 6, 7, 8, 4, 10, 3, 1, 7, 11, 6, 5, 10, 11, 8, 11, 15, 4, 6, 11, 6, 10, 10, 10, 4, 5, 7, 15, 15, 10, 12, 17, 25, 26, 22, 14, 15, 15, 7, 9, 8, 6, 1]\n\ndate=[737444, 737445, 737446, 737447, 737448, 737449, 737450, 737451, 737452, 737453, 737454, 737455, 737456, 737457, 737458, 737459, 737460, 737461, 737462, 737463, 737464, 737465, 737466, 737467, 737468, 737469, 737470, 737472, 737473, 737474, 737475, 737476, 737477, 737478, 737479, 737480, 737481, 737482, 737483, 737484, 737485, 737486, 737487, 737488, 737489, 737490, 737491, 737492, 737493, 737494, 737495, 737496, 737497, 737498, 737499, 737500, 737501, 737502, 737503, 737504, 737505, 737506, 737507, 737508, 737509, 737510, 737511, 737512, 737513, 737514, 737515, 737516, 737517, 737518, 737519, 737520, 737521, 737522, 737523]\n\nX = np.array(date)\ny = np.array(freq)\n\nX_train, X_test, y_train, y_test = train_test_split(X[:,None],y,test_size=0.3,\n                                                   random_state=42)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nplt.subplots(figsize=(15, 8))\nplt.scatter(date, freq, color='lightblue')\n# here's how using the actual equation so is is clear how it works\n# essentially same as model.precict(X)\nplt.plot(date, X*model.coef_ + model.intercept_)\n"
'data_min = multi_data.min(axis=1, keepdims=True).min(axis=0, keepdims=True)\ndata_max = multi_data.max(axis=1, keepdims=True).max(axis=0, keepdims=True)\nmulti_data = (2*(multi_data-data_min)/(data_max-data_min))-1\n\ndata_min = multi_data.min(axis=(0,1), keepdims=True)\ndata_max = multi_data.max(axis=(0,1), keepdims=True)\nmulti_data = (2*(multi_data-data_min)/(data_max-data_min))-1\n\ndata_min = multi_data.min(axis=(0,1))\ndata_max = multi_data.max(axis=(0,1))\nmulti_data = (2*(multi_data-data_min)/(data_max-data_min))-1\n'
"out = np.zeros(a.shape[:-1] + (m,), dtype=bool)\nout[(*np.indices(a.shape[:-1], sparse=True), a[..., 0])] = True\n\na = np.squeeze(a)\nout = np.zeros(a.shape + (m,), bool)\nout[(*np.indices(a.shape, sparse=True), a)] = True\n\ndef onehot(a, axis=-1, dtype=bool):\n    pos = axis if axis &gt;= 0 else a.ndim + axis + 1\n    shape = list(a.shape)\n    shape.insert(pos, a.max() + 1)\n    out = np.zeros(shape, dtype)\n    ind = list(np.indices(a.shape, sparse=True))\n    ind.insert(pos, a)\n    out[tuple(ind)] = True\n    return out\n\ndef onehot2(a, axis=None, dtype=bool):\n    shape = np.array(a.shape)\n    if axis is None:\n        axis = (shape == 1).argmax()\n    if shape[axis] != 1:\n        raise ValueError(f'Dimension at {axis} is non-singleton')\n    shape[axis] = a.max() + 1\n    out = np.zeros(shape, dtype)\n    ind = list(np.indices(a.shape, sparse=True))\n    ind[axis] = a\n    out[tuple(ind)] = True\n    return out\n\naxis = a.ndim - 1 - (shape[::-1] == 1).argmax()\n\n&gt;&gt;&gt; np.random.seed(0x111)\n&gt;&gt;&gt; x = np.random.randint(5, size=(3, 2))\n&gt;&gt;&gt; x\narray([[2, 3],\n       [3, 1],\n       [4, 0]])\n\n&gt;&gt;&gt; a = onehot(x, axis=-1, dtype=int)\n&gt;&gt;&gt; a.shape\n(3, 2, 5)\n&gt;&gt;&gt; a\narray([[[0, 0, 1, 0, 0],    # 2\n        [0, 0, 0, 1, 0]],   # 3\n\n       [[0, 0, 0, 1, 0],    # 3\n        [0, 1, 0, 0, 0]],   # 1\n\n       [[0, 0, 0, 0, 1],    # 4\n        [1, 0, 0, 0, 0]]]   # 0\n\n&gt;&gt;&gt; b = onehot(x, axis=-2, dtype=int)\n&gt;&gt;&gt; b.shape\n(3, 5, 2)\n&gt;&gt;&gt; b\narray([[[0, 0],\n        [0, 0],\n        [1, 0],\n        [0, 1],\n        [0, 0]],\n\n       [[0, 0],\n        [0, 1],\n        [0, 0],\n        [1, 0],\n        [0, 0]],\n\n       [[0, 1],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [1, 0]]])\n\n&gt;&gt;&gt; np.random.seed(0x111)\n&gt;&gt;&gt; y = np.random.randint(5, size=(3, 1, 2, 1))\n&gt;&gt;&gt; y\narray([[[[2],\n         [3]]],\n       [[[3],\n         [1]]],\n       [[[4],\n         [0]]]])\n\n&gt;&gt;&gt; c = onehot2(y, axis=-1, dtype=int)\n&gt;&gt;&gt; c.shape\n(3, 1, 2, 5)\n&gt;&gt;&gt; c\narray([[[[0, 0, 1, 0, 0],\n         [0, 0, 0, 1, 0]]],\n\n       [[[0, 0, 0, 1, 0],\n         [0, 1, 0, 0, 0]]],\n\n       [[[0, 0, 0, 0, 1],\n         [1, 0, 0, 0, 0]]]])\n\n&gt;&gt;&gt; d = onehot2(y, axis=-2, dtype=int)\nValueError: Dimension at -2 is non-singleton\n\n&gt;&gt;&gt; e = onehot2(y, dtype=int)\n&gt;&gt;&gt; e.shape\n(3, 5, 2, 1)\n&gt;&gt;&gt; e.squeeze()\narray([[[0, 0],\n        [0, 0],\n        [1, 0],\n        [0, 1],\n        [0, 0]],\n\n       [[0, 0],\n        [0, 1],\n        [0, 0],\n        [1, 0],\n        [0, 0]],\n\n       [[0, 1],\n        [0, 0],\n        [0, 0],\n        [0, 0],\n        [1, 0]]])\n"
'from sklearn.feature_extraction.text import CountVectorizer\n\nword = [&quot;Orange&quot;,&quot;Apple&quot;, &quot;I&quot;]\nn=3\nvect = CountVectorizer(analyzer=lambda x: (x[-i-1:] for i in range(0,min(n,len(x)))))\nmat = vect.fit_transform(word).todense()\n\npd.DataFrame(mat, columns=vect.get_feature_names())\n\n   I  e  ge  le  nge  ple\n0  0  1   1   0    1    0\n1  0  1   0   1    0    1\n2  1  0   0   0    0    0\n'
'tfidf = term freq * log(document count / (document frequency + 1))\ntfidf = [# of t in d] * log([#d in c] / ([#d with t in c] + 1))\n'
"samples = [['asdf', '1'], ['asdf', '0']]\n# turn the samples into dicts\nsamples = [dict(enumerate(sample)) for sample in samples]\n\n# turn list of dicts into a numpy array\nvect = DictVectorizer(sparse=False)\nX = vect.fit_transform(samples)\n\nclf = DecisionTreeClassifier()\nclf.fit(X, ['2', '3'])\n"
'K = scip.exp(YOUR_DISTANCE_HERE / s**2)\n'
'import numpy as np\n\nN_TYPES = 3\n\ninstream = [ [(0,0.3), (1,0.5), (2,0.1)],\n             [(0,0.5), (1,0.3), (2,0.3)],\n             [(0,0.4), (1,0.4), (2,0.5)],\n             [(0,0.3), (1,0.7), (2,0.2)],\n             [(0,0.2), (1,0.6), (2,0.1)] ]\ninstream = np.array(instream)\n\n# this removes document tags because we only consider probabilities here\nvalues = [map(lambda x: x[1], doc) for doc in instream]\n\n# determine the cluster of each document by using maximum probability\nbelongs_to = map(lambda x: np.argmax(x), values)\nbelongs_to = np.array(belongs_to)\n\n# construct clusters of indices to your instream\ncluster_indices = [(belongs_to == k).nonzero()[0] for k in range(N_TYPES)]\n\n# apply the indices to obtain full output\nout = [instream[cluster_indices[k]].tolist() for k in range(N_TYPES)]   \n\n[[[[0.0, 0.5], [1.0, 0.3], [2.0, 0.3]]],\n\n [[[0.0, 0.3], [1.0, 0.5], [2.0, 0.1]],\n  [[0.0, 0.3], [1.0, 0.7], [2.0, 0.2]],\n  [[0.0, 0.2], [1.0, 0.6], [2.0, 0.1]]],\n\n [[[0.0, 0.4], [1.0, 0.4], [2.0, 0.5]]]]\n'
'AlexaTrainData = p.read_csv(\'FinalCSVFin.csv\', delimiter=";")[["alexarank"]] #reading integer columns\nAlexaTestData = p.read_csv(\'FinalTestCSVFin.csv\', delimiter=";")[["alexarank"]]\nAllAlexaAndGoogleInfo = AlexaTestData.append(AlexaTrainData) #joining integer columns\n\nAllAlexaAndGoogleInfo = AlexaTrainData.append(AlexaTestData)\n\nX = X_all[:lentrain]\nAllAlexaAndGoogleInfo = AllAlexaAndGoogleInfo[:lentrain]\nX_test = X_all[lentrain:]\nX = np.hstack((X, AllAlexaAndGoogleInfo))\nsc = preprocessing.StandardScaler().fit(X)\nX = sc.transform(X)\nX_test = sc.transform(X_test)\n\npred = rd.predict_proba(X_test)[:,1]\n\nsc.transform(X_test)\n'
'class Quantizer(base.BaseEstimator, base.TransformerMixin):\n    def __init__(self):\n\n    def transform(self, X, y=None):\n      # some code \n\n    def fit(self, X, y=None, **fit_params):\n      return self\n'
'refit : boolean\n    Refit the best estimator with the entire dataset. \n    If “False”, it is impossible to make predictions using \n    this GridSearchCV instance after fitting.\n'
"tags = ['SPAM','HAM','another_class']\n"
"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndf = pd.read_csv('example.csv', header=None, sep=',', \n                 names=['tweets', 'class'])   # columns names if no header\nvect = TfidfVectorizer()\nX = vect.fit_transform(df['tweets']) \ny = df['class']\n"
'#! /usr/bin/python\n\nimport numpy as np\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\nvec_sigmoid = np.vectorize(sigmoid)\n\n# Binesh - just cleaning it up, so you can easily change the number of hiddens.\n# Also, initializing with a heuristic from Yoshua Bengio.\n# In many places you were using matrix multiplication and elementwise multiplication\n# interchangably... You can\'t do that.. (So I explicitly changed everything to be\n# dot products and multiplies so it\'s clear.)\ninput_sz = 2;\nhidden_sz = 3;\noutput_sz = 1;\ntheta1 = np.matrix(0.5 * np.sqrt(6.0 / (input_sz+hidden_sz)) * (np.random.rand(1+input_sz,hidden_sz)-0.5))\ntheta2 = np.matrix(0.5 * np.sqrt(6.0 / (hidden_sz+output_sz)) * (np.random.rand(1+hidden_sz,output_sz)-0.5))\n\ndef fit(x, y, theta1, theta2, learn_rate=.1):\n    #forward pass\n    layer1 = np.matrix(x, dtype=\'f\')\n    layer1 = np.c_[np.ones(1), layer1]\n    # Binesh - for layer2 we need to add a bias term.\n    layer2 = np.c_[np.ones(1), vec_sigmoid(layer1.dot(theta1))]\n    layer3 = sigmoid(layer2.dot(theta2))\n\n    #backprop\n    delta3 = y - layer3\n    # Binesh - In reality, this is the _negative_ derivative of the cross entropy function\n    # wrt the _input_ to the final sigmoid function.\n\n    delta2 = np.multiply(delta3.dot(theta2.T), np.multiply(layer2, (1-layer2)))\n    # Binesh - We actually don\'t use the delta for the bias term. (What would be the point?\n    # it has no inputs. Hence the line below.\n    delta2 = delta2[:,1:]\n\n    # But, delta\'s are just derivatives wrt the inputs to the sigmoid.\n    # We don\'t add those to theta directly. We have to multiply these by\n    # the preceding layer to get the theta2d\'s and theta1d\'s\n    theta2d = np.dot(layer2.T, delta3)\n    theta1d = np.dot(layer1.T, delta2)\n\n    #update weights\n    # Binesh - here you had delta3 and delta2... Those are not the\n    # the derivatives wrt the theta\'s, they are the derivatives wrt\n    # the inputs to the sigmoids.. (As I mention above)\n    theta2 += learn_rate * theta2d #??\n    theta1 += learn_rate * theta1d #??\n\ndef train(X, Y):\n    for _ in range(10000):\n        for i in range(4):\n            x = X[i]\n            y = Y[i]\n            fit(x, y, theta1, theta2)\n\n\n# Binesh - Here\'s a little test function to see that it actually works\ndef test(X):\n    for i in range(4):\n        layer1 = np.matrix(X[i],dtype=\'f\')\n        layer1 = np.c_[np.ones(1), layer1]\n        layer2 = np.c_[np.ones(1), vec_sigmoid(layer1.dot(theta1))]\n        layer3 = sigmoid(layer2.dot(theta2))\n        print "%d xor %d = %.7f" % (layer1[0,1], layer1[0,2], layer3[0,0])\n\nX = [(0,0), (1,0), (0,1), (1,1)]\nY = [0, 1, 1, 0]    \ntrain(X, Y)\n\n# Binesh - Alright, let\'s see!\ntest(X)\n\nCE = -(Y log(L3) + (1-Y) log(1-L3))\n\ndCE/dL3 = -((Y/L3) - (1-Y)/(1-L3))\n        = -((Y(1-L3) - (1-Y)L3) / (L3(1-L3)))\n        = -(((Y-Y*L3) - (L3-Y*L3)) / (L3(1-L3)))\n        = -((Y-Y3*L3 + Y3*L3 - L3) / (L3(1-L3)))\n        = -((Y-L3) / (L3(1-L3)))\n        = ((L3-Y) / (L3(1-L3)))\n\nL3      = sigmoid(Z3)\ndL3/dZ3 = L3(1-L3)\n\ndCE/dZ3 = (dCE/dL3) * (dL3/dZ3)\n        = ((L3-Y)/(L3(1-L3)) * (L3(1-L3)) # Hey, look at that. The denominator gets cancelled out and\n        = (L3-Y) # This is why in my comments I was saying what you are computing is the _negative_ derivative.\n\nZ3 = theta2(0) + theta2(1) * L2(1) + theta2(2) * L2(2) + theta2(3) * L2(3)\n\ndZ3/dL2(1) = theta2(1)\ndZ3/dL2(2) = theta2(2)\ndZ3/dL2(3) = theta2(3)\n\ndZ3/dBias  = theta2(0)\n\ndL2(1)/dZ2(0) = L2(1) * (1-L2(1))\ndL2(2)/dZ2(1) = L2(2) * (1-L2(2))\ndL2(3)/dZ2(2) = L2(3) * (1-L2(3))\n\ndCE/dZ2(0) = dCE/dZ3 * dZ3/dL2(1) * dL2(1)/dZ2(0)\n           = (L3-Y)  * theta2(1)  * L2(1) * (1-L2(1))\n\ndCE/dZ2(1) = dCE/dZ3 * dZ3/dL2(2) * dL2(2)/dZ2(1)\n           = (L3-Y)  * theta2(2)  * L2(2) * (1-L2(2))\n\ndCE/dZ2(2) = dCE/dZ3 * dZ3/dL2(3) * dL2(3)/dZ2(2)\n           = (L3-Y)  * theta2(3)  * L2(3) * (1-L2(3))\n\nZ3 = theta2(0) + theta2(1) * L2(1) + theta2(2) * L2(2) + theta2(3) * L2(3)\ndZ3/dtheta2(0) = 1\ndZ3/dtheta2(1) = L2(1)\ndZ3/dtheta2(2) = L2(2)\ndZ3/dtheta2(3) = L2(3)\n\ndCE/dtheta2(0) = dCE/dZ3 * dZ3/dtheta2(0)\n               = (L3-Y) * 1\ndCE/dtheta2(1) = dCE/dZ3 * dZ3/dtheta2(1)\n               = (L3-Y) * L2(1)\ndCE/dtheta2(2) = dCE/dZ3 * dZ3/dtheta2(2)\n               = (L3-Y) * L2(2)\ndCE/dtheta2(3) = dCE/dZ3 * dZ3/dtheta2(3)\n               = (L3-Y) * L2(3)\n\nZ2(1) = theta1(0,1) + theta1(1,1) * L1(1) + theta1(2,1) * L1(2)\ndZ2(1)/dtheta1(0,1) = 1\ndZ2(1)/dtheta1(1,1) = L1(1)\ndZ2(1)/dtheta1(2,1) = L1(2)\n\nZ2(2) = theta1(0,2) + theta1(1,2) * L1(1) + theta1(2,2) * L1(2)\ndZ2(2)/dtheta1(0,2) = 1\ndZ2(2)/dtheta1(1,2) = L1(1)\ndZ2(2)/dtheta1(2,2) = L1(2)\n'
'print np.bincount(idx)\n\nfrom numpy import vstack,array\nimport numpy as np\nfrom numpy.random import rand\nfrom scipy.cluster.vq import kmeans,vq\n# data generation\ndata = vstack((rand(150,2) + array([.5,.5]),rand(150,2)))\n# computing K-Means with K = 2 (2 clusters)\ncentroids,_ = kmeans(data,2)\n# assign each sample to a cluster\nidx,_ = vq(data,centroids)\n\n#Print number of elements per cluster\nprint np.bincount(idx)\n'
'from sklearn.metrics import accuracy_score #works\nprint(accuracy_score([1, 1, 0], [1, 0, 1]))\n\n0.333333333333\n\nimport sklearn.metrics.accuracy_score #error\n'
'df.apply(lambda x: x-x.mean())\n\n%timeit df.apply(lambda x: x-x.mean())\n1000 loops, best of 3: 2.09 ms per loop\n\ndf.subtract(df.mean())\n\n%timeit df.subtract(df.mean())\n1000 loops, best of 3: 902 µs per loop\n\n        i  we  you  shehe  they  ipron\n0  0.0725   0    0  0.195 -0.18 -0.145\n1  0.8025   0    0 -0.065 -0.18  0.495\n2 -0.4375   0    0 -0.065  0.54  0.285\n3 -0.4375   0    0 -0.065 -0.18 -0.635\n'
'd = np.dtype((np.void, target.dtype.itemsize * target.shape[1]))\n_, ulabels = np.unique(np.ascontiguousarray(target).view(d), return_inverse=True)\n\nclf = svm.SVC()\nclf.fit(data, ulabels)\n'
'print(errors[-1], errors2[-1])\n\n(0, -8.4289020207961585e-11)\n'
'np.polyfit(data.values.flatten(), data1.values.flatten(), 1)\n\n&gt;&gt;&gt; data.values.shape\n(546, 1)\n\n&gt;&gt; data.values.flatten().shape\n(546,)\n\ndf = pd.read_csv("Housing.csv")\nnp.polyfit(df[\'price\'], df[\'bedrooms\'], 1)\n'
"# Define a lstm cell with tensorflow\nwith tf.variable_scope('cell_def'):\n    lstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, forget_bias=1.0)\n\n# Get lstm cell output\nwith tf.variable_scope('rnn_def'):\n    outputs, states = tf.nn.rnn(lstm_cell, x, dtype=tf.float32)\n"
'import Algorithmia\n\nclient = Algorithmia.client("&lt;YOUR_API_KEY&gt;")\n\ndataFile = client.file("data://&lt;USER_NAME&gt;/&lt;COLLECTION_NAME&gt;/&lt;FILE_NAME&gt;").getFile()\n\ndataText = client.file("data://&lt;USER_NAME&gt;/&lt;COLLECTION_NAME&gt;/&lt;FILE_NAME&gt;").getString()\n\ndataJSON = client.file("data://&lt;USER_NAME&gt;/&lt;COLLECTION_NAME&gt;/&lt;FILE_NAME&gt;").getJson()\n\ndataBytes = client.file("data://&lt;USER_NAME&gt;/&lt;COLLECTION_NAME&gt;/&lt;FILE_NAME&gt;").getBytes()\n\nimport Algorithmia\n\ndef apply(input):\n\n    # You don\'t need to write your API key if you\'re editing in the web editor\n    client = Algorithmia.client()\n\n    modelFile = client.file("data://(username)/testcollection/model.pkl").getFile()\n\n    modelFilePath = modelFile.name\n\n    model = joblib.load(modelFilePath)\n\n    return "empty"\n\nimport Algorithmia\n\ndef apply(input):\n\n    # You don\'t need to write your API key if you\'re editing in the web editor\n    client = Algorithmia.client()\n\n    modelFile = client.file("data://(username)/testcollection/model.pkl").getFile()\n\n    model = joblib.load(modelFile)\n\n    return "empty"\n'
'def maximization(data, probabilities): #M-step. this updates the means, covariances, and priors of all clusters\n    m, n = data.shape\n    numOfClusters = probabilities.shape[1]\n\n    means = np.zeros((numOfClusters, n))\n    covs = np.zeros((numOfClusters, n, n))\n    priors = np.zeros((numOfClusters, 1))\n\n    for i in range(0, numOfClusters):\n        priors[i, 0] = np.sum(probabilities[:, i]) / m   #update priors\n\n        for j in range(0, m): #update means\n            means[i] += probabilities[j, i] * data[j, :]\n\n        means[i] /= np.sum(probabilities[:, i])\n\n    for i in range(0, numOfClusters):\n        for j in range(0, m): #update means\n            vec = np.reshape(data[j, :] - means[i, :], (n, 1))\n            covs[i] += probabilities[j, i] * np.multiply(vec, vec.T) #update covs\n\n        covs[i] /= np.sum(probabilities[:, i])\n\n    return [means, covs, priors]\n'
"from sklearn.ensemble import VotingClassifier\nclf1 = LogisticRegression()\nclf2 = LogisticRegression()\neclf1 = VotingClassifier(estimators=[('lr1', clf1), ('lr2', clf2),voting='hard')\neclf1 = eclf1.fit(X, Y)\n\nfrom mlxtend.classifier import StackingClassifier\nclf1 = LogisticRegression()\nclf2 = LogisticRegression()\nlr = Your_Meta_Classifier()\nsclf = StackingClassifier(classifiers=[clf1, clf2], \n                      meta_classifier=lr)\n"
'prediction = model.predict(x_test, batch_size = 32, verbose = 1)\n\nx_test = np.array([[8],[6],[0],[2],[0],[0],[0],[0],[112.128],[0],[0],[2],[0],[1],[1],[2],[2]])\n'
'bottleneck_features_train = model.predict_generator(\n        generator, nb_train_samples // batch_size)\n\nbottleneck_features_train = model.predict_generator(\n        generator, (nb_train_samples // batch_size) + 1)\n'
'import pandas as pd\n\ndef add_TZ(df):\n    df[\'date\'] = df[\'date\'].astype(str) + "Z"\n\ndata = {  \'date\' : ["2015-01-01 11:00:00", "2015-01-01 11:15:00", "2015-01-01 11:30:00"],\n        \'value\' : [4., 3., 2.]}\n\ndf = pd.DataFrame(data)\n\nft = FunctionTransformer(func=add_TZ)\nft.fit_transform(df)\n\nValueError: could not convert string to float: \'2015-01-01 11:30:00\'\n\nft = FunctionTransformer(func=add_TZ, validate=False)\nft.fit_transform(df)\n\n    date                    value\n0   2015-01-01 11:00:00Z    4.0\n1   2015-01-01 11:15:00Z    3.0\n2   2015-01-01 11:30:00Z    2.0\n'
'export TERM=xterm-new\nexport TERMINFO=/etc/terminfo\n'
"filepath = '{0}/checkpoints/checkpoint-{{epoch:02d}}-{{val_loss:.2f}}.hdf5'.format(directory)\n"
'futureData = np.array(range(101,121)) #[101,102,...,120]\nfuturePred = model.predict(futureData)\n'
'pred = cv.fit_transform(test).toarray()\n\npred = cv.transform(test).toarray()\n'
'import numpy as np\nimport tensorflow as tf\nfrom sklearn import datasets, metrics\n\niris = datasets.load_iris()\n\n# The classifier\nfeature_columns = [tf.feature_column.numeric_column("x", shape=[4])]\nclassifier = tf.estimator.LinearClassifier(feature_columns=feature_columns,\n                                           n_classes=3)\n\n# Training\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(x={"x": iris.data},\n                                                    y=iris.target,\n                                                    num_epochs=50,\n                                                    shuffle=True)\nclassifier.train(train_input_fn)\n\n# Testing\ntest_input_fn = tf.estimator.inputs.numpy_input_fn(x={"x": iris.data},\n                                                   num_epochs=1,\n                                                   shuffle=False)\npredictions = classifier.predict(test_input_fn)\npredicted_classes = [p["classes"].astype(np.float)[0] for p in predictions]\n\nscore = metrics.accuracy_score(iris.target, predicted_classes)\nprint("Accuracy: %f" % score)\n'
'H = theta_0 + theta_1 * x\n\nH = theta_1 * x\n'
'scoring_function = make_scorer(fbeta_score, beta=2)\n\nregressor = DecisionTreeRegressor()\n'
'target_f = self.target_model.predict(state)\n\nself.target_model.fit(state, target_f, epochs=1, verbose=0)\n'
'X = [...]\ny = [...]\n\ndef custom_scorer_function(y, y_pred, **kwargs):\n   a_feature = X[:,1]\n   # now have y, y_pred and the feature you want\n\ncustom_scorer = make_scorer(custom_scorer_function, greater_is_better=True)\n...\n'
"list(iris.target_names)\n['setosa', 'versicolor', 'virginica']\n\nnp.unique(Y)\narray([0, 1, 2])\n\n# Encode for string labels\nlabel_encoder = LabelEncoder().fit(y)\ny = label_encoder.transform(y)\n"
"test['Value'] = (test['ColA']==0) * 1 * (test['ColA'].groupby((test['ColA'] != test['ColA'].shift()).cumsum()).cumcount() + 1)\n"
"df=pd.DataFrame({'number': np.arange(100), })\n\nprint(df[::10])\n\n    number\n0        0\n10      10\n20      20\n30      30\n40      40\n50      50\n60      60\n70      70\n80      80\n90      90\n\nnp.arange(100)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\nnp.arange(100)[::9]\n\narray([ 0,  9, 18, 27, 36, 45, 54, 63, 72, 81, 90, 99])\n\ndef getting_train_val(dataframe, interval=10):\n    x_valid = dataframe[::interval]\n    x_test = dataframe[~ dataframe(dataframe[::interval])].dropna()\n    return x_valid, x_test\n"
'TfidfVectorizer(min_df=0.01, max_df=0.9)\n'
"test = pd.DataFrame(est.predict(X),columns=['price'])\n"
'from hypopt import GridSearch\n\nparam_grid = {"penalty": [\'l1\'], \'C\': [0.001, 0.01]}\nopt = GridSearch(model=LogisticRegression(), param_grid = param_grid)\n# This will use f1 score as the scoring metric that you optimize.\nopt.fit(X_train, y_train, X_val, y_val, scoring=\'f1\')\n\nfrom sklearn.metrics import make_scorer\nscorer = make_scorer(your_custom_score_func)\nopt.fit(X_train, y_train, X_val, y_val, scoring=scorer)\n'
'vectorizer = TfidfVectorizer(ngram_range=(1,2))\nvectors_train = vectorizer.fit_transform(rev_train)\nvectors_test = vectorizer.fit_transform(rev_test)\n\nvectorizer = TfidfVectorizer(ngram_range=(1,2))\nvectors_train = vectorizer.fit_transform(rev_train)\nvectors_test = vectorizer.transform(rev_test) # Change here\n'
"regressor = SVR(kernel='rbf')\nregressor.fit(X.reshape(-1, 1), y)\n"
'univ_gate_en = Lambda(lambda x: 1. - x)(gate_en)\n\nuniv_gate_en = Lambda(lambda x: K.ones_like(x) - x)(gate_en)\n\nonly_ones_en = Lambda(lambda x: K.ones_like(x))(gate_en)\nuniv_gate_en = Subtract()([only_ones_en, gate_en])\n'
'In [27]: df = pd.DataFrame(np.random.randint(10, size=(5,3)), columns=list(\'abc\'))\n\nIn [28]: df[\'d\'] = df[\'a\'] * 10 - df[\'b\'] / np.pi\n\nIn [29]: df[\'e\'] = np.log(df[\'c\'] **2)\n\nIn [30]: c = df.corr()\n\nIn [31]: c\nOut[31]:\n          a         b         c         d         e\na  1.000000  0.734858  0.113787  0.999837  0.067358\nb  0.734858  1.000000 -0.523635  0.722485 -0.598739\nc  0.113787 -0.523635  1.000000  0.129945  0.984257\nd  0.999837  0.722485  0.129945  1.000000  0.084615\ne  0.067358 -0.598739  0.984257  0.084615  1.000000\n\nIn [32]: c[c &gt;= 0.7]\nOut[32]:\n          a         b         c         d         e\na  1.000000  0.734858       NaN  0.999837       NaN\nb  0.734858  1.000000       NaN  0.722485       NaN\nc       NaN       NaN  1.000000       NaN  0.984257\nd  0.999837  0.722485       NaN  1.000000       NaN\ne       NaN       NaN  0.984257       NaN  1.000000\n\nIn [33]: c[c &gt;= 0.7].stack().reset_index(name=\'cor\').query("abs(cor) &lt; 1.0")\nOut[33]:\n   level_0 level_1       cor\n1        a       b  0.734858\n2        a       d  0.999837\n3        b       a  0.734858\n5        b       d  0.722485\n7        c       e  0.984257\n8        d       a  0.999837\n9        d       b  0.722485\n11       e       c  0.984257\n'
'import keras.backend as K\n\ndef balanced_recall(y_true, y_pred):\n    """\n    Computes the average per-column recall metric\n    for a multi-class classification problem\n    """ \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)  \n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)   \n    recall = true_positives / (possible_positives + K.epsilon())    \n    balanced_recall = K.mean(recall)\n    return balanced_recall\n'
'ds = tf.data.TFRecordDataset( filename )\nds = ds.map( _parse_function )\n\n# apply windowing\nds = ds.window( size=50, shift=25, drop_remainder=True ).flat_map( lambda x, y: tf.data.Dataset.zip( (x.batch(50), y.batch(50)) ) )\n# enumerate dataset and filter every 40th window\nds = ds.apply( tf.data.experimental.enumerate_dataset(start=1) ).filter( lambda i, x: tf.not_equal( i % 40, 0) )\n# get rid of enumerations\nds = ds.map( lambda i, x: x )\n\n# batching, shuffling etc...\n...\n'
'base_model = SGDClassifier()\nmodel = CalibratedClassifierCV(base_model)\n\nmodel.fit(X, y)\nmodel.predict_proba(X)\n'
'm = data.mean()\ns = data.std()\n\ntheta * s + m\n'
"train = ImageList.from_df(df,'../input/train/images')\ntest = ImageList.from_df(df_test, '../input/train/images')\n\ndata = ImageDataBunch.from_df('../input/train/images', df, \nds_tfms=get_transforms(), size=224, bs=64 ).normalize(imagenet_stats)\n\ndata.add_test(test)\n\npredictions, *_ = learn.get_preds(DatasetType.Test)\nlabels = np.argmax(predictions, 1)\ndf_test['category'] = labels\n"
'probas = np.exp(logits)/np.sum(np.exp(logits), axis=1)\n\n[0.5761168847658291,  0.21194155761708547,  0.21194155761708547]\n[0.21194155761708547, 0.5761168847658291, 0.21194155761708547]\n[0.21194155761708547,  0.21194155761708547, 0.5761168847658291]\n'
"flags.DEFINE_boolean('eval_training_data_and_eval_data', False,\n                     'This will evaluate botht the training data and evaluation data sequentially')\n\n  if FLAGS.checkpoint_dir:\n    if FLAGS.eval_training_data_and_eval_data:\n\n      name = 'training_data'\n      input_fn = eval_on_train_input_fn\n      if FLAGS.run_once:\n        estimator.evaluate(input_fn,\n                           steps=None,\n                           checkpoint_path=tf.train.latest_checkpoint(\n                               FLAGS.checkpoint_dir))\n      else:\n        model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,\n                                  train_steps, name)\n\n      name = 'validation_data'\n      # The first eval input will be evaluated.\n      input_fn = eval_input_fns[0]\n      if FLAGS.run_once:\n        estimator.evaluate(input_fn,\n                           steps=None,\n                           checkpoint_path=tf.train.latest_checkpoint(\n                               FLAGS.checkpoint_dir))\n      else:\n        model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,\n                                  train_steps, name)\n\n  else:\n    train_spec, eval_specs = model_lib.create_train_and_eval_specs(\n        train_input_fn,\n        eval_input_fns,\n        eval_on_train_input_fn,\n        predict_input_fn,\n        train_steps,\n        eval_on_train_data=False)\n\n    # Currently only a single Eval Spec is allowed.\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n"
"from joblib import dump, load\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\n\nmodel = Pipeline(steps=[\n    ('scaler', StandardScaler()),\n    ('polyfeats', PolynomialFeatures()),\n    ('ridge', Ridge())\n])\n\ndump(model, 'model.joblib')\nmodel = load('model.joblib')\n"
'ts = ~ts\n'
'model = tf.keras.Sequential()\ntf.keras.layers.Dense(1024, input_shape=(3072,), activation="sigmoid")\ntf.keras.layers.Dense(512, activation="sigmoid")\ntf.keras.layers.Dense(len(lb.classes_), activation="softmax")\n\nmodel.add(tf.keras.layers.Dense(1024, input_shape=(3072,), activation="sigmoid"))\nmodel.add(tf.keras.layers.Dense(512, activation="sigmoid"))\nmodel.add(tf.keras.layers.Dense(len(lb.classes_), activation="softmax"))\n'
'Iteration: 0\n58305.102166924036\nIteration: 1\n25952.192344178206\nIteration: 2\n11551.585414406314\nIteration: 3\n5141.729521746186\nIteration: 4\n2288.6353484460747\nIteration: 5\n1018.6952280352172\nIteration: 6\n453.4320214875039\nIteration: 7\n201.82728832044089\nIteration: 8\n89.83519431606754\nIteration: 9\n39.98665864625944\nIteration: 10\n17.798416262435936\nIteration: 11\n7.92229454258205\nIteration: 12\n3.526272890501929\nIteration: 13\n1.5696002444816197\nIteration: 14\n0.6986516574778796\nIteration: 15\n0.3109875219688626\nIteration: 16\n0.13843156434074647\nIteration: 17\n0.061616235257299326\nIteration: 18\n0.027424318402401473\nIteration: 19\n0.012205888201891543\nIteration: 20\n0.005434012356344396\nIteration: 21\n0.0024188644277583476\nIteration: 22\n0.0010770380211645404\nIteration: 23\n0.0004796730257022216\nIteration: 24\n0.00021339295719587025\nIteration: 25\n9.499628306355218e-05\nIteration: 26\n4.244764386691682e-05\nIteration: 27\n1.8965112443214162e-05\nIteration: 28\n8.56069334821767e-06\nIteration: 29\n3.848135476439999e-06\nIteration: 30\n1.7367004907528985e-06\nIteration: 31\n8.07976330965736e-07\nIteration: 32\n4.0167090640020525e-07\nIteration: 33\n2.253979336583221e-07\nIteration: 34\n1.5365746125585947e-07\nIteration: 35\n1.2480275459766612e-07\nIteration: 36\n1.1147859663321005e-07\nIteration: 37\n1.0288427880059631e-07\nIteration: 38\n1.0036079530613815e-07\nIteration: 39\n9.901975516098116e-08\nIteration: 40\n9.901971962009025e-08\nIteration: 41\n9.901968407922984e-08\nIteration: 42\n9.901964853839991e-08\nIteration: 43\n9.901961299760048e-08\nIteration: 44\n9.901957745683155e-08\nIteration: 45\n9.90195419160931e-08\nIteration: 46\n9.901950637538515e-08\nIteration: 47\n9.90194708347077e-08\nIteration: 48\n9.901943529406073e-08\nIteration: 49\n9.901939975344426e-08\nIteration: 50\n9.901936421285829e-08\nIteration: 51\n9.90193286723028e-08\nIteration: 52\n9.901929313177781e-08\nIteration: 53\n9.901925759128331e-08\nIteration: 54\n9.901922205081931e-08\nIteration: 55\n9.90191865103858e-08\nIteration: 56\n9.901915096998278e-08\nIteration: 57\n9.901911542961026e-08\nIteration: 58\n9.901907988926822e-08\nIteration: 59\n9.901904434895669e-08\nIteration: 60\n9.901900880867564e-08\nIteration: 61\n9.901897326842509e-08\nIteration: 62\n9.901893772820503e-08\nIteration: 63\n9.901890218801546e-08\nIteration: 64\n9.901886664785639e-08\nIteration: 65\n9.901883110772781e-08\nIteration: 66\n9.901879556762973e-08\nIteration: 67\n9.901876002756213e-08\nIteration: 68\n9.901872448752503e-08\nIteration: 69\n9.901868894751843e-08\nIteration: 70\n9.901865340754231e-08\nIteration: 71\n9.901861786759669e-08\nIteration: 72\n9.901858232768157e-08\nIteration: 73\n9.901854678779693e-08\nIteration: 74\n9.901851124794279e-08\nIteration: 75\n9.901847570811914e-08\nIteration: 76\n9.901844016832599e-08\nIteration: 77\n9.901840462856333e-08\nIteration: 78\n9.901836908883116e-08\nIteration: 79\n9.901833354912948e-08\nIteration: 80\n9.90182980094583e-08\nIteration: 81\n9.901826246981762e-08\nIteration: 82\n9.901822693020742e-08\nIteration: 83\n9.901819139062772e-08\nIteration: 84\n9.901815585107851e-08\nIteration: 85\n9.90181203115598e-08\nIteration: 86\n9.901808477207157e-08\nIteration: 87\n9.901804923261384e-08\nIteration: 88\n9.90180136931866e-08\nIteration: 89\n9.901797815378986e-08\nIteration: 90\n9.901794261442361e-08\nIteration: 91\n9.901790707508786e-08\nIteration: 92\n9.901787153578259e-08\nIteration: 93\n9.901783599650782e-08\nIteration: 94\n9.901780045726355e-08\nIteration: 95\n9.901776491804976e-08\nIteration: 96\n9.901772937886647e-08\nIteration: 97\n9.901769383971367e-08\nIteration: 98\n9.901765830059137e-08\nIteration: 99\n9.901762276149956e-08\n'
"from tensorflow.keras import Model\n\nclass TrainWithCustomLogsModel(Model):\n\n    def __init__(self, **kwargs):\n        super(TrainWithCustomLogsModel, self).__init__(**kwargs)\n        self.step = tf.Variable(0, dtype=tf.int64,trainable=False)\n\n    def train_step(self, data):\n\n        # Get batch images and labels\n        x, y = data\n        \n        # Compute the batch loss\n        with tf.GradientTape() as tape:\n            p = self(x , training = True)\n            loss = self.compiled_loss(y, p, regularization_losses=self.losses)\n        \n        # Compute gradients for each weight of the network. Note trainable_vars and gradients are list of tensors\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Log gradients in Tensorboard\n        self.step.assign_add(tf.constant(1, dtype=tf.int64))\n        #tf.print(self.step)\n        with train_summary_writer.as_default():\n          for var, grad in zip(trainable_vars, gradients):\n            name = var.name\n            var, grad = tf.squeeze(var), tf.squeeze(grad)\n            tf.summary.histogram(name, var, step = self.step)\n            tf.summary.histogram('Gradients_'+name, grad, step = self.step)\n    \n        # Update model's weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        del tape\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, p)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n"
'y_pred = np.random.rand(1,6).round(2)\n# array([[0.53, 0.54, 0.68, 0.34, 0.53, 0.46]])\nnp.where(y_pred&gt; 0.5, np.ones((1,6)), np.zeros((1,6)))\n# array([[1., 1., 1., 0., 1., 0.]])\n\nnp.where(y_pred&gt; 0.5,1,0)\n# array([[1, 1, 1, 0, 1, 0]])\n'
"from sklearn.ensemble import VotingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.pipeline import make_pipeline\n\nX,y = make_classification(random_state=123)\n\nscaled_svc = make_pipeline(StandardScaler(), SVC())\n\nvoting = VotingClassifier(estimators=[\n    ('scaled_svc', scaled_svc),\n    ('unscaled_svc', SVC())\n])\n\nv = voting.fit(X,y)\nv.predict(X)\n\narray([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0])\n"
"class WeightedAverage(Layer):\n\n    def __init__(self, n_output):\n        super(WeightedAverage, self).__init__()\n        self.W = tf.Variable(initial_value=tf.random.uniform(shape=[1,1,n_output], minval=0, maxval=1),\n            trainable=True) # (1,1,n_inputs)\n\n    def call(self, inputs):\n\n        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n        inputs = [tf.expand_dims(i, -1) for i in inputs]\n        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n        # weights sum up to one on last dim\n\n        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)\n\ninp1 = Input((100,))\ninp2 = Input((100,))\nx1 = Dense(32, activation='relu')(inp1)\nx2 = Dense(32, activation='relu')(inp2)\nx = [x1,x2]\nW_Avg = WeightedAverage(n_output=len(x))(x)\nout = Dense(1)(W_Avg)\n\nm = Model([inp1,inp2], out)\nm.compile('adam','mse')\n\nn_sample = 1000\nX1 = np.random.uniform(0,1, (n_sample,100))\nX2 = np.random.uniform(0,1, (n_sample,100))\ny = np.random.uniform(0,1, (n_sample,1))\n\nm.fit([X1,X2], y, epochs=10)\n\ntf.nn.softmax(m.get_weights()[-3]).numpy()\n"
"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\na1 = np.asarray([(216, 236, 235, 230, 229), (237, 192, 191, 193, 199), (218, 189, 191, 192, 193), (201, 239, 230, 229, 220), (237, 210, 200, 236, 235)])\na2 = np.asarray([(202, 202, 201, 203, 204), (210, 211, 213, 209, 208), (203, 206, 202, 201, 199), (201, 207, 206, 199, 205), (190, 191, 192, 193, 194)])\n\nb1 = np.asarray([(236, 237, 238, 239, 240), (215, 216, 217, 218, 219), (201, 202, 203, 209, 210), (240, 241, 243, 244, 245), (220, 221, 222, 231, 242)])\nb2 = np.asarray([(242, 243, 245, 246, 247), (248, 249, 250, 251, 252), (210, 203, 209, 210, 211), (247, 248, 249, 250, 251), (230, 231, 235, 236, 240)])\n\nX = np.vstack([a1.T, a2.T, b1.T, b2.T])\ny = [1]*5 + [2]*5 + [3]*5 + [4]*5\nclf = LinearDiscriminantAnalysis(n_components=2)\nclf.fit(X, y)\n\nXem = clf.transform(X)\nplt.scatter(Xem[0:5,0], Xem[0:5,1], c='b', marker='o')\nplt.scatter(Xem[5:10,0], Xem[5:10,1], c='b', marker='s')\nplt.scatter(Xem[10:15,0], Xem[10:15,1], c='r', marker='o')\nplt.scatter(Xem[15:20,0], Xem[15:20,1], c='r', marker='s')\n"
"data_orig.select_dtypes(['object']).apply(lambda x:min(pd.Series.value_counts(x)))\n\nmodel              1\ntransmission    2708\nfuelType          28\n\ndata_orig['model'].value_counts().tail()\n\n SQ7    8\n S8     4\n S5     3\n RS7    1\n A2     1\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=22)\n\nimport matplotlib.pyplot as plt\n\ndef fit(X_tr,X_ts,y_tr,y_ts,is_sparse=True):\n    data_train = full_pipeline.fit_transform(X_tr)\n    data_test = full_pipeline.transform(X_ts)\n    \n    if not is_sparse:\n        data_train = data_train.toarray()\n        data_test = data_test.toarray()\n    \n    lin_reg = LinearRegression().fit(data_train, y_tr)\n    pred = lin_reg.predict(data_test)\n    \n    plt.scatter(y_ts,pred)\n    return {'r2_train':r2_score(y_tr, lin_reg.predict(data_train)),\n            'r2_test':r2_score(y_ts, pred),\n            'pred':pred}\n\nsparse_pred = fit(X_train,X_test,y_train,y_test,is_sparse=True)\n[sparse_pred['r2_train'],sparse_pred['r2_test']]\n\n[0.8896333645670668, 0.898030271986993]\n\nsparse_pred = fit(X_train,X_test,y_train,y_test,is_sparse=False)\n[sparse_pred['r2_train'],sparse_pred['r2_test']]\n\n[0.8896302753422759, 0.8980115229388697]\n"
"import tensorflow as tf\nimport numpy as np\n\nx = np.array([[0, 0],\n              [1, 1],\n              [1, 0],\n              [0, 1]],  dtype=np.float32)\n\ny = np.array([[0], \n              [0], \n              [1], \n              [1]],     dtype=np.float32)\n\nmodel =  tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(2,)))\nmodel.add(tf.keras.layers.Dense(2, activation=tf.keras.activations.sigmoid))\nmodel.add(tf.keras.layers.Dense(2, activation=tf.keras.activations.sigmoid))\nmodel.add(tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid))\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.02), # learning rate was 0.001 prior to this change\n              loss=tf.keras.losses.MeanSquaredError(), \n              metrics=['mse', 'binary_accuracy'])\nmodel.summary()\nprint(&quot;Tensorflow version: &quot;, tf.__version__)\npredictions = model.predict_on_batch(x)\nprint(predictions)history = model.fit(x, y, batch_size=1, epochs=500)\n\n[[0.05162644]\n[0.06670767]\n[0.9240402 ]\n[0.923379  ]]\n"
'from treeinterpreter import treeinterpreter as ti\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\nrf = RandomForestClassifier(max_depth = 4)\nidx = range(len(iris.target))\nnp.random.shuffle(idx)\n\nrf.fit(iris.data[idx][:100], iris.target[idx][:100])\n\nprediction, bias, contributions = ti.predict(rf, instance)\nprint "Prediction", prediction\nprint "Bias (trainset prior)", bias\nprint "Feature contributions:"\nfor c, feature in zip(contributions[0], \n                             iris.feature_names):\n    print feature, c\n\nPrediction [[ 0. 0.9 0.1]]\nBias (trainset prior) [[ 0.36 0.262 0.378]]\nFeature contributions:\nsepal length (cm) [-0.1228614 0.07971035 0.04315104]\nsepal width (cm) [ 0. -0.01352012 0.01352012]\npetal length (cm) [-0.11716058 0.24709886 -0.12993828]\npetal width (cm) [-0.11997802 0.32471091 -0.20473289]\n\nprediction = bias + feature_1_contribution + ... + feature_n_contribution\n'
'from pyspark.ml.feature import SQLTransformer\n\ndef column_selector(columns):\n    return SQLTransformer(\n        statement="SELECT {} FROM __THIS__".format(", ".join(columns))\n    )\n\ndef na_dropper(columns):\n    return SQLTransformer(\n        statement="SELECT * FROM __THIS__ WHERE {}".format(\n            " AND ".join(["{} IS NOT NULL".format(x) for x in columns])\n        )\n    )\n'
"&gt;&gt;&gt; from scipy.sparse import csr_matrix\n&gt;&gt;&gt; X = csr_matrix([[0, 0, 1], [2, 3, 0]])\n&gt;&gt;&gt; X\n&lt;2x3 sparse matrix of type '&lt;type 'numpy.int64'&gt;'\n    with 3 stored elements in Compressed Sparse Row format&gt;\n&gt;&gt;&gt; X.toarray()\narray([[0, 0, 1],\n       [2, 3, 0]])\n&gt;&gt;&gt; print(X)\n  (0, 2)    1\n  (1, 0)    2\n  (1, 1)    3\n"
"In [11]: df.pivot(values='correct', index='userID', columns='questionID')\nOut[11]: \nquestionID  1   3   5   6   8   10\nuserID                            \n1            1 NaN   1   0   0   1\n2          NaN   1   1   0 NaN NaN\n\nIn [12]: _.reindex_axis(np.arange(1, 10), 1)\nOut[12]: \n         1   2   3   4  5  6   7   8   9\nuserID                                  \n1        1 NaN NaN NaN  1  0 NaN   0 NaN\n2      NaN NaN   1 NaN  1  0 NaN NaN NaN\n\ndf.pivot_table(values='correct', rows='userID', cols='questionID')\n"
'trainer = RPropMinusTrainer(net, dataset=trndata, verbose=True, weightdecay=0.01)\n'
'REquired.lower()\n'
'def euclidean_dist2(x1, x2):\n    assert(len(x1) == len(x2))\n\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n\n    norm = np.linalg.norm(x1-x2)\n\n    return norm\n\nprint euclidean_dist2([1,2],[4,7])\n\ndef euclidean_dist3(x1, x2):\n    assert(len(x1) == len(x2))\n\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n\n    diff = x1 - x2\n\n    squared = np.transpose(diff) * diff\n\n    summed = sum(squared)\n\n    norm = np.sqrt(summed)\n\n    return norm\n\ndef euclidean_dist4(x1, x2):\n    assert(len(x1) == len(x2))\n\n    x1 = np.array(x1)\n    x2 = np.array(x2)\n\n    diff = x1 - x2\n\n    dot = np.dot(diff, diff)\n\n    norm = np.sqrt(dot)\n\n    return norm\n'
'model = Sequential()\nmodel.add(Reshape((X.size,), input_shape=(X.shape)))\nmodel.add(Dense(64))\n'
"docs = collection.find({}, {'tag_data': 1, '_id': 1})\nids = [doc['_id'] for doc in docs]\ntags = [doc['tag_data'] for doc in docs]\n\nclusters = cluster_text(tags)\n\ndoc_clusters = zip(ids, clusters)\n"
'optimizer = tf.train.AdamOptimizer()\ngrads_and_vars = optimizer.compute_gradients(a_loss_function)\n\ngrad_norms = [tf.nn.l2_loss(g) for g, v in grads_and_vars]\ngrad_norm = tf.add_n(grad_norms)\n\nfor step in range(TotSteps):\n    l, gn = sess.run([loss, grad_norm], feed_dict=some_dict)\n    if(gn &lt; some_treshold):\n        print("Training finished.")\n        break\n'
"predictor = tf.argmax(y_conv,1)\n\nprint(sess.run(predictor, feed_dict={ x = new_data }))\n\nacc, predictions = sess.run([accuracy, predictor],\n                            feed_dict={x: folds.test.images,\n                                       y_: folds.test.labels,\n                                       keep_prob: 1.0}))\n\nprint('Accuracy', acc)\nprint('Predictions', predictions)\n"
"x = Input((seq_len, input_dim))\nlstm = LSTM(128, return_sequences=True, activation='tanh')(x)\ntd = TimeDistributed(Dense(out_size, activation='softmax'))(lstm)\nsecond_input = Input((seq_len, out_size)) # object instanciated and hold as a var.\nout = merge([td, second_input], mode='mul')\nmodel = Model(input=[x, second_input], output=out) # second input provided to model\n"
'        sub_patch[i:(i + filter_sz), j:(j + filter_sz), :] += (sub_filt_patch * pred_patch[i,j]).sum(dim=3)\n\nsub_patch[i:(i + filter_sz), j:(j + filter_sz), :] = sub_patch[i:(i + filter_sz), j:(j + filter_sz), :] + (sub_filt_patch * pred_patch[i,j]).sum(dim=3)\n'
'folds=kf\n\nfolds=list(kf.split(x_train,y_train))\n\nfolds=kf.split(x_train,y_train)\n'
'http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n\nhttp://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html\n'
'test_x = test_x.reshape((1, 40))\n\ntest_dataframe = pd.DataFrame({\'filename\': ["here path to test file"]}\n'
"from pyspark.ml.regression import LinearRegressionModel\n\nLinearRegressionModel.load('model_lin_reg')\n"
'foo/x\nfoo/y\nfoo/c\n...\nfoo/z1/foo/x\nfoo/z1/foo/y\nfoo/z1/foo/c\n...\n\n# note that it\'s defined before `g1.as_graph_def()` to be a part of graph def\ninit_op = tf.global_variables_initializer()\n\ng1_def = g1.as_graph_def()\nz1, = tf.import_graph_def(g1_def, input_map={\'foo/x:0\': y}, return_elements=["foo/z:0"],\n                          name=\'z1\')\n\n# find the init op\nz1_init_op = tf.get_default_graph().get_operation_by_name(\'foo/z1/foo/init\')\n\n...\n\nsess.run(z1_init_op)\n'
'z = tf.Variable(tf.zeros([20,2], tf.float32))  # a variable, not a const\nassign21 = tf.assign(z[2, 0], 1.0)             # an op to update z\nassign22 = tf.assign(z[2, 1], 1.0)             # an op to update z\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  print(sess.run(z))                           # prints all zeros\n  sess.run([assign21, assign22])\n  print(sess.run(z))                           # prints 1.0 in the 3d row\n'
'encoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_sequences=True)\nencoder_outputs, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder(encoder_inputs))\n'
'                         Impact(category = Ci) = E[y|Ci] - E[y]\n'
'X = [[0, 0, 6], [1, 0, 0], [9, 9, 9], [3, 0, 3], [0, 0, 0]]\n\nX = [[0, 0, 0, 6], [0, 1, 0, 0], [0, 9, 9, 9], [0, 3, 0, 3], [1, 0, 0, 0]]\n'
'import numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# randomly generate some data with 4 distinct clusters, use your own data here    \nDemo_df_Processed = pd.DataFrame(np.random.randint(-2100,-2000,size=(100, 4)), columns=list(\'ABCD\'))\nDemo_df_Processed = Demo_df_Processed.append(pd.DataFrame(np.random.randint(-600,-500,size=(100, 4)), columns=list(\'ABCD\')))\nDemo_df_Processed = Demo_df_Processed.append(pd.DataFrame(np.random.randint(500,600,size=(100, 4)), columns=list(\'ABCD\')))\nDemo_df_Processed = Demo_df_Processed.append(pd.DataFrame(np.random.randint(2000,2100,size=(100, 4)), columns=list(\'ABCD\')))\n\npca_ = PCA(n_components=3)\nX_Demo_fit_pca = pca_.fit_transform(Demo_df_Processed)\n\nkmeans_PCA = KMeans(n_clusters=4, init=\'k-means++\', max_iter= 300, n_init= 10, random_state= 3)\n\ny_kmeans_PCA = kmeans_PCA.fit_predict(X_Demo_fit_pca)\ny_kmeans_PCA\n\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111, projection=\'3d\')\nax.scatter(X_Demo_fit_pca[:,0],X_Demo_fit_pca[:,1],X_Demo_fit_pca[:,2], \n            c=y_kmeans_PCA, cmap=\'viridis\',\n            edgecolor=\'k\', s=40, alpha = 0.5)\n\n\nax.set_title("First three PCA directions")\nax.set_xlabel("Educational_Degree")\nax.set_ylabel("Gross_Monthly_Salary")\nax.set_zlabel("Claim_Rate")\nax.dist = 10\n\nax.scatter(kmeans_PCA.cluster_centers_[:,0], kmeans_PCA.cluster_centers_[:,1], \n           kmeans_PCA.cluster_centers_[:,2], \n           s = 300, c = \'r\', marker=\'*\', label = \'Centroid\')\n\nplt.autoscale(enable=True, axis=\'x\', tight=True)    \n\nplt.show()\n'
"in = Input(shape=(4, ))\n\ndense_1 = Dense(units=4, activation='relu')(in)\n\nout_1 = Dense(units=1, activation='sigmoid')(dense_1)\nout_2 = Dense(units=1, activation='sigmoid')(dense_1)\nout_3 = Dense(units=1, activation='sigmoid')(dense_1)\n\nmodel = Model(inputs=[in], outputs=[out_1, out_2, out_3])\n"
"print (df)\n   row_num     set_id\n0   988681        NaN\n1   988680  [31965,0]\n2   988679  [0,78464]\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\n\n#create boolean mask matched non NaNs values\nmask = df['set_id'].notnull()\n\n#filter by boolean indexing\narr = mlb.fit_transform(df.loc[mask, 'set_id'].dropna().str.strip('[]').str.split(','))\n\n#create DataFrame and add missing (NaN)s index values\ndf = (pd.DataFrame(arr, index=df.index[mask], columns=mlb.classes_)\n               .reindex(df.index, fill_value=0))\n\nprint (df)\n   0  31965  78464\n0  0      0      0\n1  1      1      0\n2  1      0      1\n"
'import numpy as np\n\npred_class = np.argmax(probs, axis=-1) \n'
'model = model.compile(optimizer=adam(lr=0.0005), loss="mae")\n\n\nhistory = model.fit_generator(train_gen,\n                          steps_per_epoch=1000,\n                          epochs=30,\n                          verbose=0,\n                          callbacks=cb,\n                          validation_data=valid_gen,\n                          validation_steps=200)\n\nmodel.compile(optimizer=adam(lr=0.0005), loss="mae")\n\n\nhistory = model.fit_generator(train_gen,\n                          steps_per_epoch=1000,\n                          epochs=30,\n                          verbose=0,\n                          callbacks=cb,\n                          validation_data=valid_gen,\n                          validation_steps=200)\n'
'import tensorflow as tf\n\ndef filter_votes_vec(probs, dists, prob_threshold, num_landmarks, sample_shape: tf.Tensor):\n    probs = probs[:, :, :, 1:]\n    indices = tf.where(probs &gt;= prob_threshold)\n    landmark = tf.to_float(indices[:, 3])\n    p = tf.gather_nd(probs, indices)\n    indices_dists = tf.stack([\n        indices,\n        tf.concat([indices[..., :-1], indices[..., -1:] + 3], axis=-1),\n        tf.concat([indices[..., :-1], indices[..., -1:] + 6], axis=-1)\n    ], axis=1)\n    d = tf.gather_nd(dists, indices_dists) + tf.to_float(indices[:, :3])\n    res = tf.concat([tf.expand_dims(landmark, 1), tf.expand_dims(p, 1), d], axis=1)\n    mask = tf.reduce_all((d &gt;= 0) &amp; (d &lt; tf.cast(sample_shape, tf.float32)), axis=1)\n    res =  tf.boolean_mask(res, mask)\n    return res\n\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.Graph().as_default(), tf.Session() as sess:\n    np.random.seed(100)\n    probs = np.random.rand(70, 70, 70, 3 + 1).astype(np.float32)\n    probs /= probs.sum(-1, keepdims=True)\n    probs = tf.convert_to_tensor(probs, tf.float32)\n    dists = tf.convert_to_tensor(100 * np.random.rand(70, 70, 70, 3 * 3), tf.float32)\n    prob_threshold = tf.convert_to_tensor(0.5, tf.float32)\n    num_landmarks = tf.convert_to_tensor(3, tf.int32)  # This is not actually used in the code\n    sample_shape = tf.convert_to_tensor([50, 60, 70], tf.int32)\n\n    result = filter_votes(probs, dists, prob_threshold, num_landmarks, sample_shape)\n    result_vec = filter_votes_vec(probs, dists, prob_threshold, num_landmarks, sample_shape)\n    value, value_vec = sess.run([result, result_vec])\n    print(np.allclose(value, value_vec))\n    # True\n    %timeit sess.run(result)\n    # CPU\n    # 2.55 s ± 21.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n    # GPU\n    # 54 s ± 596 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n    %timeit sess.run(result_vec)\n    # CPU\n    # 63.2 µs ± 781 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n    # GPU\n    # 216 µs ± 2.29 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n'
"# output theta in your function\ndef gradient(x, y, theta, alpha, iterations):\n    cost_history = [0] * iterations\n\n    for i in range(iterations):\n        h = theta.dot(x.T) #hypothesis\n        #print('h:', h)\n        loss = h - y\n        #print('loss:', loss)\n        g = loss.dot(x) / len(y)\n        #print('g:', g)\n        theta = theta - alpha * g\n        print('theta:', theta)\n        cost_history[i] = costfn(x, y, theta)\n    #print(theta)\n    return theta, cost_history\n\n# set up example data with a simple linear relationship\n# where we can play around with different numbers of parameters\n# conveniently\n# with some noise\nnum_params= 2   # how many params do you want to estimate (up to 5)\n# take some fixed params (we only take num_params of them)\nreal_params= [2.3, -0.1, 8.5, -1.8, 3.2]\n\n# now generate the data for the number of parameters chosen\nx_train= np.random.randint(-100, 100, size=(80, num_params))\nx_noise= np.random.randint(-100, 100, size=(80, num_params)) * 0.001\ny_train= (x_train + x_noise).dot(np.array(real_params[:num_params]))\ntheta= np.zeros(num_params)\n\ntheta, cost_history = gradient(x_train, y_train, theta, 0.1, 1000)\n\ntheta: [ 0.07734451 -0.00357339]\ntheta: [ 0.15208803 -0.007018  ]\ntheta: [ 0.22431803 -0.01033852]\ntheta: [ 0.29411905 -0.01353942]\ntheta: [ 0.36157275 -0.01662507]\ntheta: [ 0.42675808 -0.01959962]\ntheta: [ 0.48975132 -0.02246712]\ntheta: [ 0.55062617 -0.02523144]\n...\ntheta: [ 2.29993382 -0.09981407]\ntheta: [ 2.29993382 -0.09981407]\ntheta: [ 2.29993382 -0.09981407]\ntheta: [ 2.29993382 -0.09981407]\n\ndef gradient(\n        x, \n        y, \n        theta=None, \n        alpha=0.1, \n        alpha_factor=0.1 ** (1/5), \n        change_threshold=1e-10, \n        max_iterations=500, \n        verbose=False):\n    cost_history = list()\n    if theta is None:\n        # theta was not passed explicitely\n        # so initialize it\n        theta= np.zeros(x.shape[1])\n    last_loss_sum= float('inf')\n    len_y= len(y)\n    for i in range(1, max_iterations+1):\n        h = theta.dot(x.T) #hypothesis\n        loss = h - y\n        loss_sum= np.sum(np.abs(loss))\n        if last_loss_sum &lt;= loss_sum:\n            # the loss didn't decrease\n            # so decrease alpha\n            alpha= alpha * alpha_factor\n        if verbose:\n            print(f'pass: {i:4d} loss: {loss_sum:.8f} / alpha: {alpha}')\n        theta_old= theta\n        g= loss.dot(x) / len_y\n        if loss_sum &lt;= last_loss_sum and last_loss_sum &lt; float('inf'):\n            # only apply the change if the loss is\n            # finite to avoid infinite entries in theta\n            theta = theta - alpha * g\n            theta_change= np.sum(np.abs(theta_old - theta))\n            if theta_change &lt; change_threshold:\n                # Maybe this seems a bit awkward, but\n                # the comparison of change_threshold\n                # takes the relationship between theta and g\n                # into account. Note that g will not have\n                # an effect if theta is orders of magnitude\n                # larger than g, even if g itself is large.\n                # (I mean if you consider g and theta elementwise)\n                cost_history.append(costfn(x, y, theta))\n                break\n        cost_history.append(costfn(x, y, theta))\n        last_loss_sum= loss_sum\n    return theta, cost_history\n"
'transforms.RandomRotation(degrees=(90, -90), fill=(0,))\n'
'[10, 20, 30, 40, 50, 60, 70, 80, 90]\n\nX,              y\n10, 20, 30      40\n20, 30, 40      50\n30, 40, 50      60\n# ...\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the sequence\n        if end_ix &gt; len(sequence)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps = 3\n# split into samples\nX, y = split_sequence(raw_seq, n_steps)\n# summarize the data\nfor i in range(len(X)):\n    print(X[i], y[i])\n\n# [10 20 30] 40\n# [20 30 40] 50\n# [30 40 50] 60\n# [40 50 60] 70\n# [50 60 70] 80\n# [60 70 80] 90\n'
"epochs = 5\nn_batches = 20\nn_features = 10\nbatches = [[np.random.uniform(0,1, (1,n_features)), np.random.uniform(0,1, (1,1))] for _ in range(n_batches)]\n\ninp = Input((n_features,))\nx = Dense(32)(inp)\nout = Dense(1)(x)\nmodel = Model(inp, out)\nmodel.compile('adam', 'mse')\n\nfor i in range(epochs):\n    for batch in batches:\n        model.fit(batch[0], batch[1], epochs=1, verbose=0)\n    print(f&quot;EPOCH {i}&quot;, model.history.history)\n\nEPOCH 0 {'loss': [0.9013449549674988]}\nEPOCH 1 {'loss': [0.7315107583999634]}\nEPOCH 2 {'loss': [0.5937882661819458]}\nEPOCH 3 {'loss': [0.5331881046295166]}\nEPOCH 4 {'loss': [0.47262871265411377]}\n"
'set cluster means to equal k randomly generated points\nwhile not converged:\n     # expectation step:\n     for each point:\n          assign it to its expected cluster (cluster whose mean it is closest to)\n     # maximization step:\n     for each cluster:\n          # maximizes likelihood for cluster mean\n          set cluster mean to be the average of all points assigned to it\n'
'import numpy as np\n\nn_features = 10\n\n# Make some data\nA = np.random.randn(3, n_features)\nB = np.random.randn(5, n_features)\nC = np.random.randn(4, n_features)\nD = np.random.randn(7, n_features)\nE = np.random.randn(9, n_features)\n\n# Group it\nK1 = np.concatenate([A, B])\nK2 = np.concatenate([C, D])\nK3 = E\n\ndata = np.concatenate([K1, K2, K3])\n\n# Make some dummy prediction target\ntarget = np.random.randn(len(data)) &gt; 0\n\n# Make the corresponding labels\nlabels = np.concatenate([[i] * len(K) for i, K in enumerate([K1, K2, K3])])\n\nfrom sklearn.cross_validation import LeaveOneLabelOut, cross_val_score\n\ncv = LeaveOneLabelOut(labels)\n\n# Use some classifier in crossvalidation on data\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nscores = cross_val_score(lr, data, target, cv=cv)\n\n# create train and test folds from our labels:\ncv_by_hand = [(np.where(labels != label)[0], np.where(labels == label)[0])\n               for label in np.unique(labels)]\n\n# We check this against our existing cv by converting the latter to a list\ncv_to_list = list(cv)\n\nprint cv_by_hand\nprint cv_to_list\n\n# Check equality\nfor (train1, test1), (train2, test2) in zip(cv_by_hand, cv_to_list):\n    assert (train1 == train2).all() and (test1 == test2).all()\n\n# Use the created cv_by_hand in cross validation\nscores2 = cross_val_score(lr, data, target, cv=cv_by_hand)\n\n\n# assert equality again\nassert (scores == scores2).all()\n'
'model = CountVectorizer()\ntransformed_train = model.fit_transform(train_corpus)\ntransformed_test = model.transform(test_corpus)\n\nmodel = CountVectorizer()\ntransformed_train = model.fit_transform(train_corpus)\ntransformed_test = model.fit_transform(test_corpus)\n'
'K_test = chi2_kernel(X_test_scaled)\n\nK_test = chi2_kernel(X_test_scaled, X_train_scaled)\n'
'syn1 = 2*np.random.random((len(train_sample), 10)) - 1\n\nsyn1 += alpha * l1.T.dot(l2_delta)\nsyn0 += alpha * l0.T.dot(l1_delta)\n\nsyn0 = (np.random.random((784,len(train_sample))) - 0.5) / 4\nsyn1 = (np.random.random((len(train_sample),1)) - 0.5) / 4\n\nsyn1 += alpha * l1.T.dot(l2_delta) - alpha * lambda * syn1\nsyn0 += alpha * l0.T.dot(l1_delta) - alpha * lambda * syn0\n\nimport pickle, gzip\nimport numpy as np\n\n#from deeplearning.net\n# Load the dataset\nf = gzip.open(\'mnist.pkl.gz\', \'rb\')\ntrain_set, valid_set, test_set = pickle.load(f, encoding=\'latin1\')\nf.close()\n\n\n\n\n\n#sigmoid function\ndef nonlin(x, deriv=False):\n    if (deriv ==True):\n        return 1-x*x\n    return np.tanh(x)\n\n#seed random numbers to make calculation\n#deterministic (just a good practice)\n\nnp.random.seed(1)\n\ndef make_proper_pairs_from_set(data_set):\n    data_set_x, data_set_y = data_set\n\n    data_set_y = np.eye(10)[:, data_set_y].T\n\n    return data_set_x, data_set_y\n\n\ntrain_x, train_y = make_proper_pairs_from_set(train_set)\ntrain_x = train_x\ntrain_y = train_y\n\ntest_x, test_y = make_proper_pairs_from_set(test_set)\n\nprint(len(train_y))\n\n#train_set\'s dimension for the pixels are 50000(samples) x 784 (28x28 for each sample)\n#therefore the coefficients should be 784x50000 to make the hidden layer 50k x 50k\n\n# changed to 200 hidden neurons, should be plenty\nsyn0 = (2*np.random.random((785,200)) - 1) / 10\nsyn1 = (2*np.random.random((201,10)) - 1) / 10\n\nvelocities0 = np.zeros(syn0.shape)\nvelocities1 = np.zeros(syn1.shape)\n\nalpha = 0.01\nbeta = 0.0001\nmomentum = 0.99\n\nm = len(train_x) # number of training samples\n\n# moved the forward propagation to a function and added bias neurons\ndef forward_prop(set_x, m):\n\n    l0 = np.c_[np.ones((m, 1)), set_x]\n\n    l1 = nonlin(np.dot(l0, syn0))\n    l1 = np.c_[np.ones((m, 1)), l1]\n\n    l2 = nonlin(np.dot(l1, syn1))\n\n\n    return l0, l1, l2, l2.argmax(axis=1)\n\nnum_epochs = 100\nfor i in range(num_epochs):\n    # forward propagation\n\n    l0, l1, l2, _ = forward_prop(train_x, m)\n\n    # calculate error\n    l2_error = l2 - train_y\n\n\n    print("Error " + str(i) + ": " + str(np.mean(np.abs(l2_error))))\n    # apply sigmoid to the error \n    l2_delta = l2_error * nonlin(l2,deriv=True)\n\n    l1_error = l2_delta.dot(syn1.T)\n    l1_delta = l1_error * nonlin(l1,deriv=True)\n    l1_delta = l1_delta[:, 1:]\n\n    # update weights\n    # divide gradients by the number of samples\n    grad0 = l0.T.dot(l1_delta) / m\n    grad1 = l1.T.dot(l2_delta) / m\n\n    v0 = velocities0\n    v1 = velocities1\n\n    velocities0 = velocities0 * momentum - alpha * grad0\n    velocities1 = velocities1 * momentum - alpha * grad1\n\n\n    # divide regularization by number of samples\n    # because L2 regularization reduces to this\n    syn1 += -v1 * momentum + (1 + momentum) * velocities1 - alpha * beta * syn1 / m\n    syn0 += -v0 * momentum + (1 + momentum) * velocities0 - alpha * beta * syn0 / m\n\n\n\n# find accuracy on test set\n\npredictions = []\ncorrects = []\nfor i in range(len(test_x)): # you can eliminate this loop too with a bit of work, but this part is very fast anyway\n    _, _, _, rez = forward_prop([test_x[i, :]], 1)\n\n    predictions.append(rez[0])\n    corrects.append(test_y[i].argmax())\n\npredictions = np.array(predictions)\ncorrects = np.array(corrects)\n\nprint(np.sum(predictions == corrects) / len(test_x))\n'
'import numpy as np\nimport scipy.optimize\n\n# our forward model, paired layers of already-trained\n# weights and biases.\nweights = [np.array(...) ...]\nbiases = [np.array(...) ...]\ndef f(x):\n    for W, b in zip(weights, biases):\n        # relu activation.\n        x = np.clip(np.dot(W, x) + b, 0, np.inf)\n    return x\n\n# set our sights on class #4.\nzstar = np.array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n\n# the loss we want to optimize: minimize difference\n# between zstar and f(x).\ndef loss(x):\n    return abs(f(x) - zstar).sum()\n\nx0 = np.zeros(784)\nresult = scipy.optimize.minimize(loss, x0)\n'
'y = tf.nn.softmax(tf.matmul(x,W) + b)\n\ny_softmax = tf.nn.softmax(tf.matmul(x,W) + b)\ncross_entropy = -tf.reduce_sum(y_*tf.log(y_softmax))\n'
'session.run(states[-1], feed_dict={encoder1:[#values for encoder1 here\n                                            ]})\n\n_, state_values = session.run([output_feed, states[-1]], input_feed)\n'
'classifier.fit(or_input, or_output, steps=1000, batch_size=3)\n\narray([1, 1, 0, 1])\n\nlast_step &lt; max_steps\n'
'import numpy as np\nimport tensorflow as tf\n\ndef addone(x):\n    # print(type(x)\n    return x + 1\n\ndef addone_grad(op, grad):\n    x = op.inputs[0]\n    return x\n\nfrom tensorflow.python.framework import ops\nimport numpy as np\n\n# Define custom py_func which takes also a grad op as argument:\ndef py_func(func, inp, Tout, stateful=True, name=None, grad=None):\n\n    # Need to generate a unique name to avoid duplicates:\n    rnd_name = \'PyFuncGrad\' + str(np.random.randint(0, 1E+8))\n\n    tf.RegisterGradient(rnd_name)(grad)  # see _MySquareGrad for grad example\n    g = tf.get_default_graph()\n    with g.gradient_override_map({"PyFunc": rnd_name}):\n        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n\ndef pyfunc_test():\n\n    # create data\n    x_data = tf.placeholder(dtype=tf.float32, shape=[None])\n    y_data = tf.placeholder(dtype=tf.float32, shape=[None])\n\n    w = tf.Variable(tf.constant([0.5]))\n    b = tf.Variable(tf.zeros([1]))\n\n    y1 = tf.mul(w, x_data, name=\'y1\')\n    y2 = py_func(addone, [y1], [tf.float32], grad=addone_grad)[0]\n    y = tf.add(y2, b)\n\n    loss = tf.reduce_mean(tf.square(y - y_data))\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    train = optimizer.minimize(loss)\n\n    print("Pyfunc grad", ops.get_gradient_function(y2.op))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        for step in range(10):\n            #            ran = np.random.rand(115).astype(np.float32)\n            ran = np.ones((115)).astype(np.float32)\n            ans = ran * 1.5 + 3\n            dic = {x_data: ran, y_data: ans}\n            tt, yy, yy1= sess.run([train, y1, y2], feed_dict=dic)\n            if step % 1 == 0:\n                print(\'step {}\'.format(step))\n                print(\'{}, {}\'.format(w.eval(), b.eval()))\n\n        test = sess.run(y, feed_dict={x_data:[1]})\n        print(\'test = {}\'.format(test))\n\n\nif __name__ == \'__main__\':\n    pyfunc_test()\n'
"s_n_hat = [1, -1, 1, 1, -1, 1, 1, 1, 1]\nx1 = list(range(0, 5))\nx2 = list(range(5, 8))\nmarkerline1, stemlines, _ = plt.stem(x1, s_n_hat[0:5], '-.')\nplt.setp(markerline1, 'markerfacecolor', 'b')\nmarkerline2, stemlines, _ = plt.stem(x2, s_n_hat[6:9], '-.')\nplt.setp(markerline2, 'markerfacecolor', 'r')\nplt.show()\n"
'0. 56887 INFP 1. 54607 INFJ 2. 52511 INTJ 3. 52028 ENFP 4. 24294 INTP 5. 19032 ENTJ 6. 14284 ENFJ 7. 12502 ISFJ 8. 12268 ISTP 9. 10713 ISTJ 10. 10523 ESFP 11. 8103 ESTP 12. 7436 ESFJ 13. 7016 ESTJ 14. 6725 ISFP\n'
'(_, C, H, W) = x.data.size()\nx = x.view( -1 , C * H * W)\n'
"import numpy as np\nimport matplotlib.pyplot as plt\nzeros = np.zeros([35000])\nmodifier = 100\nones = np.ones([145*modifier])\narr = np.hstack((zeros, ones))\nbins = np.asarray([-0.5, 0.5, 1.5])\nplt.hist(arr, bins=bins, facecolor='green', alpha=0.75, log=False)\nplt.xticks([0,1])\nplt.title('Multiplied with a factor')\nplt.savefig('multiplied.png')\nplt.show()\nplt.clf()\nmodifier = 1\nones = np.ones([145*modifier])\narr = np.hstack((zeros, ones))\nplt.hist(arr, bins=bins, facecolor='green', alpha=0.75, log=True)\nplt.xticks([0,1])\nplt.title('Logarithmic')\nplt.savefig('log.png')\nplt.show()\nplt.clf()\n\nax1 = plt.gca()\nax2 = ax1.twinx()\nax1.set_yticks([0, 35000, 40000])\nax1.set_ylim(0, 40000)\nax2.set_yticks([0, 145, 200])\nax2.set_ylim(0, 200)\nax1.hist(arr, bins=[bins[0], bins[1]], facecolor='green', alpha=0.75, log=False)#, histtype='bar')#, rwidth=1.0)\nax2.hist(arr, bins=[bins[1], bins[2]], facecolor='green', alpha=0.75, log=False)#, histtype='bar')#, rwidth=1.0)\nplt.xticks([0,1])\nplt.title('Two y axes')\nplt.savefig('two_axes.png')\nplt.show()\nplt.clf()\n"
'import org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.Identifiable\nimport org.apache.spark.sql.{DataFrame, Dataset}\nimport org.apache.spark.sql.types.{\n  ArrayType,\n  StringType,\n  StructField,\n  StructType\n}\nimport org.apache.spark.sql.functions.collect_list\n\nclass Dropper(override val uid: String) extends Transformer {\n\n  def this() = this(Identifiable.randomUID("dropper"))\n\n  override def transform(dataset: Dataset[_]): DataFrame = {\n    dataset.drop("your-column-name-here")\n  }\n\n  override def copy(extra: ParamMap): Transformer = defaultCopy(extra)\n\n  override def transformSchema(schema: StructType): StructType = {\n    //here you should right your result schema i.e. the schema without the dropped column\n  }\n\n}\n'
'import tensorflow as tf\n\nW = tf.Variable(tf.zeros((2,1)))\nWt = tf.tile(W, (1,3))\n\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\nprint(Wt.eval())\n# [[ 0.  0.  0.]\n#  [ 0.  0.  0.]]\nW[0,0].assign(1).eval()\nprint(Wt.eval())\n# [[ 1.  1.  1.]\n#  [ 0.  0.  0.]]\n\nWt[0,0].assign(1).eval()\n# ValueError: Sliced assignment is only supported for variables\n'
'import numpy as np;\n\ndimension = 300 \nsingleIdentityMatrix = np.identity(dimension, dtype= np.float32) \nstackedMatrix = np.dstack( [singleIdentityMatrix] * 6)\n\nself.R = tf.Variable(initial_value = stackedMatrix)\n'
"from keras.models import Model\n\ninpImg = Input((rows,columns,channels))\ninpFloat = Input((1,))\n\noutputTensor = SomeLayer(....)(inputTensor)\n\n#example:\nconvOut = Conv2D(20,kernel_size=3,activation='relu')(inpImg)\n\nmultOut = Lambda(lambda x: x[0] + x[1],output_shape=(10,))([lastOut,inpFloat])\n\nmodel = Model([inpImg,inpFloat],multOut)\n\nmodel.fit([trainImages, trainFloats],labels)\n"
"m = lambda x: map(str.strip, x.split(','))\n\nwith open('test.csv') as f:\n    df = pd.DataFrame(\n        [[x, y] for x, *ys in map(m, f.readlines()) for y in ys if y],\n        columns=['Example', 'Class']\n    )\n\ndf\n\n     Example    Class\n0  example 1  class 1\n1  example 2  class 1\n2  example 2  class 2\n3  example 3  class 2\n4  example 4  class 1\n5  example 4  class 2\n6  example 4  class 4\n"
"hl1 = tf.add(tf.matmul(x, weights['hl1']), biases['hl1'])    \nol = tf.nn.sigmoid(tf.add(tf.matmul(hl1, weights['ol']), biases['ol']))\n\nol = tf.nn.sigmoid(tf.add(tf.matmul(hl1, weights['ol']) ...\nD_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(...)\n\ntf.Variable(tf.random_normal([784, 200]))\n"
'X_train, X_test, y_train,y_test = train_test_split(X,y, test_size = .5, random_state = 0)\n'
'label = {\n    "image/label/class" : features["image/label/class"],\n    "image/label/numbb" : features["image/label/numbb"],\n    "image/label/by" : tf.sparse_tensor_to_dense(features["image/label/by"], default_value=-1),\n    "image/label/bx" : tf.sparse_tensor_to_dense(features["image/label/bx"], default_value=-1)\n    "image/label/bh" : tf.sparse_tensor_to_dense(features["image/label/bh"], default_value=-1)\n    "image/label/bw" : tf.sparse_tensor_to_dense(features["image/label/bw"], default_value=-1)\n}\n'
"X = X.astype('float32')\n"
"from keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\ntrain_img, train_lbl = loadmnist('mnist//train-images-idx3-ubyte.gz'\n                                 , 'mnist//train-labels-idx1-ubyte.gz')\ntest_img, test_lbl = loadmnist('mnist//t10k-images-idx3-ubyte.gz'\n                               , 'mnist//t10k-labels-idx1-ubyte.gz')\n"
'"the quick brown fox"\n"another test"\n'
"X = df.drop(['Usage'],1)\nX_train = df[df['Year'] &lt; 2018]\nX_test = df[df['Year'] &gt; 2017]\ny_train = df[df['Year'] &lt; 2018]\ny_test = df[df['Year'] &gt; 2017]\ny_train = y_train['Usage']\ny_test = y_test['Usage']\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n"
"history_dict = dict()\n\nfor i in range(num_epochs):\n    x, y = generate_data()\n    history_dict['epoch_%i' % i] = model.fit(x, y, epochs=1, batch_size=64)\n\nOut[4]: \n{'epoch_0': &lt;your history 1&gt;,\n 'epoch_1': &lt;your history 2&gt;,\n 'epoch_2': &lt;your history 3&gt;,\n"
"data = np.array([['country','time','x','y'],\n                ['USA',1, 5, 10],\n                ['USA',2, 5, 12],\n                ['USA',3,6, 13],\n                ['CAN',1,2, 2],\n                ['CAN',2,2, 3],\n                ['CAN',3,4, 5]],                \n               )\n\ndf = pd.DataFrame(data=data[1:,1:],\n                  index=data[1:,0],\n                  columns=data[0,1:])\n\ndf1 = df.reset_index().set_index(['time','index']).unstack(-1)\nprint(df1)\n\n        x       y    \nindex CAN USA CAN USA\ntime                 \n1       2   5   2  10\n2       2   5   3  12\n3       4   6   5  13\n\nfrom sklearn.model_selection import TimeSeriesSplit\ntscv = TimeSeriesSplit(n_splits = 2, max_train_size=3)\n\nX_cols = ['time', 'index', 'x']\ny_cols = ['y']\n\nfor train_index, test_index in tscv.split(df1):\n    print(&quot;TRAIN:&quot;, train_index, &quot;TEST:&quot;, test_index)\n    X_train, X_test = df1.iloc[train_index].stack(-1).reset_index()[X_cols].to_numpy(), df1.iloc[test_index].stack(-1).reset_index()[X_cols].to_numpy()\n    y_train, y_test = df1.iloc[train_index].stack(-1).reset_index()[y_cols].to_numpy(), df1.iloc[test_index].stack(-1).reset_index()[y_cols].to_numpy()\n\nTRAIN: [0] TEST: [1]\nTRAIN: [0 1] TEST: [2]\n\nprint('For - TRAIN: [0 1] TEST: [2]')\nprint(&quot; &quot;)\nprint(&quot;X_train:&quot;)\nprint(X_train)\nprint(&quot; &quot;)\nprint(&quot;X_test:&quot;)\nprint(X_test)\nprint(&quot; &quot;)\nprint(&quot;y_train:&quot;)\nprint(y_train)\nprint(&quot; &quot;)\nprint(&quot;y_test:&quot;)\nprint(y_test)\nprint(&quot;X_train:&quot;)\nprint(X_train)\nprint(&quot; &quot;)\nprint(&quot;X_test:&quot;)\nprint(X_test)\nprint(&quot; &quot;)\nprint(&quot;y_train:&quot;)\nprint(y_train)\nprint(&quot; &quot;)\nprint(&quot;y_test:&quot;)\nprint(y_test)\n\nFor - TRAIN: [0 1] TEST: [2]\n\nX_train:\n[['1' 'CAN' '2']\n ['1' 'USA' '5']\n ['2' 'CAN' '2']\n ['2' 'USA' '5']]\n \nX_test:\n[['3' 'CAN' '4']\n ['3' 'USA' '6']]\n \ny_train:\n[['2']\n ['10']\n ['3']\n ['12']]\n \ny_test:\n[['5']\n ['13']]\n"
'for model in my_networks: #hyperparameters selection\n    model.fit(X_train, Y_train) # parameters fitting\n    model.predict(X_valid) # no train, only check on performances\n    save model performances on validation\n\npick the best model (the one with best scores on the validation set)\nthen check results on the test set\nmodel.predict(X_test) # this will be the estimated performance of your model\n\nfrom sklearn.metrics import mean_squared_error\nepochs = 5000\n\nmlp = MLPRegressor(activation=&quot;relu&quot;, max_iter=1, solver=&quot;adam&quot;, random_state=2, early_stopping=True)\ntraining_mse = []\nvalidation_mse = []\nfor epoch in epochs:\n    mlp.fit(X_train, Y_train) \n    Y_pred = mlp.predict(X_train)\n    curr_train_score = mean_squared_error(Y_train, Y_pred) # training performances\n    Y_pred = mlp.predict(X_valid) \n    curr_valid_score = mean_squared_error(Y_valid, Y_pred) # validation performances\n    training_mse.append(curr_train_score) # list of training perf to plot\n    validation_mse.append(curr_valid_score) # list of valid perf to plot\n'
"model = StackingClassifier(estimators=[\n        ('tree', Pipeline([('tree', DecisionTreeClassifier(random_state=42))])),\n        ('knn', Pipeline([('knn', KNeighborsClassifier())])),\n    ],final_estimator = final_estimator, n_jobs = 10, passthrough = False, cv = KFold(n_splits=2))\n\nn =5\nx = range(90,100)\ncv = KFold(n_splits=n).split(x)\n\nfor i,j in cv:\n    print(&quot;TRAIN:&quot;,i,&quot;TEST&quot;,j)\nTRAIN: [2 3 4 5 6 7 8 9] TEST [0 1]\nTRAIN: [0 1 4 5 6 7 8 9] TEST [2 3]\nTRAIN: [0 1 2 3 6 7 8 9] TEST [4 5]\nTRAIN: [0 1 2 3 4 5 8 9] TEST [6 7]\nTRAIN: [0 1 2 3 4 5 6 7] TEST [8 9]\n\nn =5\nx = range(90,100)\n# cv = KFold(n_splits=n).split(x)\ncv = ShuffleSplit(n_splits=n, train_size=.8).split(x)\n\nfor i,j in cv:\n    print(&quot;TRAIN:&quot;,i,&quot;TEST&quot;,j)\nTRAIN: [7 9 0 1 6 4 8 3] TEST [2 5]\nTRAIN: [3 2 7 0 8 4 6 1] TEST [5 9]\nTRAIN: [5 1 8 7 4 0 2 6] TEST [9 3]\nTRAIN: [7 1 5 8 6 9 4 0] TEST [2 3]\nTRAIN: [7 0 3 2 6 1 5 9] TEST [4 8]\n"
"from seaborn import load_dataset\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nimport shap\n\ntitanic = load_dataset(&quot;titanic&quot;)\nX = titanic.drop([&quot;survived&quot;,&quot;alive&quot;,&quot;adult_male&quot;,&quot;who&quot;,'deck'],1)\ny = titanic[&quot;survived&quot;]\n\nfeatures = X.columns\ncat_features = []\nfor cat in X.select_dtypes(exclude=&quot;number&quot;):\n    cat_features.append(cat)\n#   think about meaningful ordering instead\n    X[cat] = X[cat].astype(&quot;category&quot;).cat.codes.astype(&quot;category&quot;)\n\nX_train, X_val, y_train, y_val = train_test_split(X,y,train_size=.8, random_state=42)\n\nclf = LGBMClassifier(max_depth=3, n_estimators=1000, objective=&quot;binary&quot;)\nclf.fit(X_train,y_train, eval_set=(X_val,y_val), early_stopping_rounds=100, verbose=100) \n\nexplainer = shap.TreeExplainer(clf)\nshap_values = explainer.shap_values(X_train)\nsv = np.array(shap_values)\ny = clf.predict(X_train).astype(&quot;bool&quot;)\n# shap values for survival\nsv_survive = sv[:,y,:]\n# shap values for dying\nsv_die = sv[:,~y,:]\n\nshap.summary_plot(shap_values[1], X_train.astype(&quot;float&quot;))\n\nidx = np.abs(sv[1,:,:]).mean(0).argsort()\nfeatures[idx[:-4:-1]]\n# Index(['sex', 'pclass', 'age'], dtype='object')\n\n# top3 features for probability of survival\nidx = sv[1,y,:].mean(0).argsort()\nfeatures[idx[:-4:-1]]\n# Index(['sex', 'pclass', 'age'], dtype='object')\n\n# top3 features for probability of dieing\nidx = sv[0,~y,:].mean(0).argsort()\nfeatures[idx[:3]]\n# Index(['alone', 'embark_town', 'parch'], dtype='object')\n\n#shap values\nsv = np.array(shap_values)\n#base values\nev = np.array(explainer.expected_value)\nsv_died, sv_survived = sv[:,0,:] # + constant\nprint(sv_died, sv_survived, sep=&quot;\\n&quot;)\n# [-0.73585563  1.24520748  0.70440429 -0.15443337 -0.01855845 -0.08430467  0.02916375 -0.04846619  0.         -0.01035171]\n# [ 0.73585563 -1.24520748 -0.70440429  0.15443337  0.01855845  0.08430467 -0.02916375  0.04846619  0.          0.01035171]\n\nshap.dependence_plot(&quot;sex&quot;, shap_values[1], X_train)\n"
"# to minimize!\ndef regularization_term(true, prediction):\n    order = list(range(1,4))\n    order.append(0)\n    \n    deviation = (true*true[:,order]) - (prediction*prediction[:,order])\n    deviation = abs(deviation)**2\n    return 0.2 * deviation\n\ndef my_custom_loss(y_true, y_pred):\n    return tf.keras.losses.BinaryCrossentropy()(y_true, y_pred) + regularization_term(y_true, y_pred)\n\n\nmodel.compile(optimizer='Adam', loss=my_custom_loss)\n"
"class_weight : {dict, 'auto'}, optional\n    Set the parameter C of class i to class_weight[i]*C for\n    SVC. If not given, all classes are supposed to have\n    weight one. The 'auto' mode uses the values of y to\n    automatically adjust weights inversely proportional to\n    class frequencies.\n"
'data[\'count\'] = data.groupby(\'UserID\')["HashtagCount"].transform(\'sum\')\n'
'W_hidden = tf.Variable(tf.random_uniform([hidden_nodes, 2], -1.0, 1.0))\n\nW_hidden = tf.Variable(tf.random_uniform([2, hidden_nodes], -1.0, 1.0))\n\nhidden = tf.sigmoid(tf.matmul(W_hidden, n_input) + b_hidden)\n\noutput = tf.sigmoid(tf.matmul(W_output, hidden))\n\nsess.run(train)\n\nsess.run(train, feed_dict={n_input: input_data}) \n'
'self.eigVec.append(tmpEigVec.tolist())\n\nself.eigVec.append(np.transpose(tmpEigVec).tolist())\n'
"dtype={'col1': int,\n       'col2': pd.CategoricalDtype(['cat1', 'cat2', 'cat3']),\n       'col3': float}\n"
"df = pd.DataFrame(dict_data)\ndf.fillna(0, inplace=True)\n\nclass FillingNans(object):\n    '''\n    Custom function for assembling into the pipeline object \n    '''\n    def transform(self, X):\n        nans_replaced = X.fillna(0)\n        return nans_replaced\n\n    def fit(self, X, y=None):\n        return self\n\npipe = skl.pipeline.Pipeline([('vectorize', vectorize),\n                             ('fill_nans', FillingNans()),\n                             ('variance', variance),\n                             ('knn', knn)])\n"
'random_search.fit(X, y)\n\nrandom_search.fit(X, y, gp)\n'
'result = model.predict_classes(numpy.array(X[0]).reshape((1,4)))\n'
'temparr = vstack(temparr,essages_bow)\n'
'xgboost.cv(params, dtrain, num_boost_round=10, nfold=3, \nstratified=False, folds=None, metrics=(), obj=None, feval=None, \nmaximize=False, early_stopping_rounds=None, fpreproc=None, \nas_pandas=True, verbose_eval=None, show_stdv=True, seed=0, \ncallbacks=None, shuffle=True)\n'
'output = tf.contrib.layers.flatten(output)\nlogits = tf.contrib.layers.fully_connected(output, some_size, activation_fn=None)\n\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=target,\n                                                        logits=logits)\n'
'probability = clf.predict_proba([[160, 0]])\n\nif x1 &lt; 10:\n    if x2 &gt; 150:\n        # in our training set of `n` examples, 100 fell under \n        # this rule set. 75 of them were apple, and 25 were orange - thus:\n        probability = [0.75, 0.25]  # P(apple) = .75, P(orange) = .25\n'
'logits = tf.matmul(x, weight) + bias\n\nlogits = tf.matmul(last, weight) + bias\n'
'def train(training_file, vocab_path, hidden_units=[10, 20, 10]):\n    """\n    Given a training CSV file, train a Tensorflow neural network\n    """\n\n    training_set = data.load(training_file)\n\n    vocab = tf.contrib.learn.preprocessing.VocabularyProcessor(data.DOC_LENGTH)\n    vocab = vocab.restore(vocab_path)\n    # Note not defining the variables here\n    training_data = training_set.data\n    training_targets = np.array(training_set.targets, dtype=np.int32)\n\n    classifier = tf.contrib.learn.Estimator(model_fn=lambda features, targets, mode, params: model_fn(features, targets, mode, params, hidden_units))\n\n    # Note the variable definition here\n    classifier.fit(\n        input_fn=lambda: \n            (tf.one_hot(training_data, len(vocab.vocabulary_._mapping), dtype=tf.float32)\n             tf.constant(training_targets)),\n        steps=2000))\n\n    return classifier\n'
'cells = []\nfor _ in range(n)\n    cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)  \n    cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n    cells.append(cell)\ncell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n'
'import keras.backend as K\n\nwith K.get_session() as sess:\n    sess.run(grad, feed_dict={input_tens: np.zeros((1, n_features))})\n'
'import numpy as np\nimg = np.expand_dims(img, axis=0)\nprediction = model.predict(img)\n'
'The target that this loss expects is a class index (0 to N-1, where N = number of classes)\n\nloss = loss_fn(y_pred, torch.max(y, 1)[1])\n'
'with tf.Session(graph=graph) as sess:\n    saver = tf.train.Saver()\n\nsaver.save(sess, \'path/to/model_file\')\n\nwith tf.Session() as sess:\n    new_saver = tf.train.import_meta_graph(\'path/to/model_file.meta\')\n    new_saver.restore(sess, tf.train.latest_checkpoint(\'path/to/model_dir/\'))\n\n    # restore the tensors you want (usually, the ones you use in feed_dict and sess.run)\n    graph = tf.get_default_graph()\n    x = graph.get_tensor_by_name("x:0")\n    output = graph.get_tensor_by_name("output:0")\n\n    feed_dict = {x:x}\n    [result] = sess.run([output], feed_dict=feed_dict)\n'
"def para_space():\n    space_paras = {'model_type': hp.choice('model_type', ['features_and_hours', 'features_only', 'hours_only', 'no_features_no_hours']),\n                    'output_units': hp.uniform('output_units', 1, 10),\n                    'kernel_reg': hp.choice('kernel_reg', ['l1', 'l2', 'l1_l2']),\n                    'kernel_reg_value': hp.uniform('kernel_reg_value', 0.0, 0.5),\n                    'activity_reg': hp.choice('activity_reg', ['l1', 'l2', 'l1_l2']),\n                    'activity_reg_value': hp.uniform('activity_reg_value', 0.0, 0.5),\n                     'optimizer': hp.choice('optimizer', ['adadelta', 'adam', 'rmsprop']),\n                     'attention': hp.choice('attention', ['before', 'after', 'none'])} \n   return space_paras\n"
"from sklearn.cluster import KMeans\nkm = KMeans()\nprint(km.labels_)\n&gt;&gt;&gt;AttributeError: 'KMeans' object has no attribute 'labels_'\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\nkm = KMeans()\nX = np.random.rand(100, 2)\nkm.fit(X)\nprint(km.labels_)\n&gt;&gt;&gt;[1 6 7 4 6 6 7 5 6 0 0 7 3 4 5 7 5 0 3 4 0 6 1 6 7 5 4 3 4 2 1 2 1 4 6 3 6 1 7 6 6 7 4 1 1 0 4 2 5 0 6 3 1 0 7 6 2 7 7 5 2 7 7 3 2 1 2 2 4 7 5 3 2 65 1 6 2 4 2 3 2 2 2 1 2 0 5 7 2 4 4 5 4 4 1 1 4 5 0]\n"
"class get_Vendor(BaseEstimator,TransformerMixin):\n\n    def transform(self, X,y):\n        return \n\nlr_tfidf = Pipeline([('features',FeatureUnion([('other',get_vendor()),\n        ('vect', tfidf)])),('clf', RandomForestClassifier())])\n"
'count_the_feat = StringCount(es[\'transactions\'][\'product_id\'], es[\'sessions\'], string="5")\n\nsessions.STRING_COUNT(product_id, "5")\n'
'import keras.backend as K\ndef custom_1loss(y_true, y_pred):\n    const = 2\n    mask = K.less(y_pred, y_true) #i.e. y_pred - y_true &lt; 0\n    return (const - 1) * mask * mean_squared_error(y_true, y_pred) + mean_squared_error(y_true, y_pred)\n\nimport keras.backend as K\ndef custom_loss2(y_true, y_pred):\n    beta = 0.1\n    return mean_squared_error(y_true, y_pred) + beta*K.mean(y_true - y_pred)\n'
'def push_to_tensor_alternative(tensor, x):\n    return torch.cat((tensor[1:], Tensor([x])))\n\n# Small Tensor\ntensor = Tensor([1,2,3,4])\n\n%timeit push_to_tensor(tensor, 5)\n&gt;&gt; 30.9 µs ± 1.26 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n%timeit push_to_tensor_alternative(tensor, 5)\n&gt;&gt; 22.1 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n# Larger Tensor\ntensor = torch.arange(10000)\n\n%timeit push_to_tensor(tensor, 5)\n&gt;&gt; 57.7 µs ± 4.88 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n%timeit push_to_tensor_alternative(tensor, 5)\n&gt;&gt; 28.9 µs ± 570 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n'
'Layer (type)                 Output Shape              Param #   \n=================================================================\ndense_13 (Dense)             (None, 128, 50)           150       \n_________________________________________________________________\ndense_14 (Dense)             (None, 128, 20)           1020      \n_________________________________________________________________\ndense_15 (Dense)             (None, 128, 5)            105       \n_________________________________________________________________\ndense_16 (Dense)             (None, 128, 2)            12        \n=================================================================\nTotal params: 1,287\nTrainable params: 1,287\nNon-trainable params: 0\n_________________________________________________________________\n\nmodel = models.Sequential()\nmodel.add(Dense(50, batch_input_shape=(None, 128, 2), kernel_initializer="he_normal" ,activation="relu"))\nmodel.add(Dense(20, kernel_initializer="he_normal", activation="relu"))\nmodel.add(Dense(5, kernel_initializer="he_normal", activation="relu"))\nmodel.add(Flatten())\nmodel.add(Dense(2))\n\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_17 (Dense)             (None, 128, 50)           150       \n_________________________________________________________________\ndense_18 (Dense)             (None, 128, 20)           1020      \n_________________________________________________________________\ndense_19 (Dense)             (None, 128, 5)            105       \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 640)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 2)                 1282      \n=================================================================\nTotal params: 2,557\nTrainable params: 2,557\nNon-trainable params: 0\n_________________________________________________________________\n\nmodel = models.Sequential()\nmodel.add(Dense(50, batch_input_shape=(None, 128, 2), kernel_initializer="he_normal" ,activation="relu"))\nmodel.add(Dense(20, kernel_initializer="he_normal", activation="relu"))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(5, kernel_initializer="he_normal", activation="relu"))\nmodel.add(Dense(2))\n\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_21 (Dense)             (None, 128, 50)           150       \n_________________________________________________________________\ndense_22 (Dense)             (None, 128, 20)           1020      \n_________________________________________________________________\nglobal_average_pooling1d_2 ( (None, 20)                0         \n_________________________________________________________________\ndense_23 (Dense)             (None, 5)                 105       \n_________________________________________________________________\ndense_24 (Dense)             (None, 2)                 12        \n=================================================================\nTotal params: 1,287\nTrainable params: 1,287\nNon-trainable params: 0\n_________________________________________________________________\n'
"clf = SVC(kernel = 'linear', probability=True)\nscoring = ['f1']\nparam_grid = {'C': np.linspace(start=1000, stop=10000, num=4, endpoint = True)}\ngrid = GridSearchCV(clf, param_grid = param_grid, scoring = scoring, cv = 3,\n                refit = 'f1', verbose = 42, n_jobs=-1, pre_dispatch=3)\ngrid.fit(X_train, y_train)\n\nclf = SVC(kernel = 'linear', probability=True)\nscoring = ['f1']\nparam_grid = {'C': np.linspace(start=1000, stop=10000, num=4, endpoint = True)}\ngrid = GridSearchCV(clf, param_grid = param_grid, scoring = scoring, cv = 3,\n                refit = 'f1', verbose = 42, pre_dispatch=3)\ngrid.fit(X_train, y_train)\n"
"def __init__(self, restore=True):\n    self.build_decoder = tf.make_template('decoder', self._build_decoder)\n    self.session = tf.Session()\n    if restore:\n        self.restore_model()\n"
"x_sample = [[-17,  -7, -7,  0, -5, -18, 73, 9, -282, 28550, 67],\n            [-21, -16, -7, -6, -8,  15, 60, 6, -239, 28550, 94]]\n\ny_sample = [0, 0]\n\nx_train = np.array(example)\n\n#Here x_train.shape = (2,11), we want to reshape it to (2,11,1) to\n#fit the network's input dimension\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n\ny_train = np.array(target)\ny_train = to_categorical(y_train, 2)\n\ndef lstm_baseline(x_train, y_train):\n   batch_size = 200\n   model = Sequential()\n   #We are gonna change input shape for input_dim\n   model.add(LSTM(batch_size, input_dim=1,\n                  activation='relu', return_sequences=True))\n   model.add(Dropout(0.2))\n\n   model.add(LSTM(128, activation='relu'))\n   model.add(Dropout(0.1))\n\n   model.add(Dense(32, activation='relu'))\n   model.add(Dropout(0.2))\n\n   #We are gonna set the number of outputs to 2, to match with the\n   #number of categories\n   model.add(Dense(2, activation='softmax'))\n\n   model.compile(\n       loss='categorical_crossentropy',\n       optimizer='rmsprop',\n       metrics=['accuracy'])\n\n   model.fit(x_train, y_train, epochs=15)\n\nreturn model\n"
'predict_nodes = decision_path.indices\npredicted_row = to_predict\ncols = predicted_row.columns\nfor node in predict_nodes:\n    col = model.tree_.feature[node]\n    print( cols[col], predicted_row[ cols[col] ].values )\n'
'layers=[x.data for x in myModel.parameters()]\n\nprint(layers[0])\n\nprint(layers[1])\n'
'training_epochs = 100\nlearning_rate = 0.01\n# the training set\nx_train = np.linspace(0, 10, 100)\ny_train = x_train + np.random.normal(0,1,100)\n                                     \n# # Normalize the data\nx_mean = np.mean(x_train)\nx_std = np.std(x_train)\nx_train_ = (x_train - x_mean)/x_std\n\nX = tf.placeholder(tf.float32)\nY = tf.placeholder(tf.float32)\n# set up variables for weights\nw0 = tf.Variable(0.0, name=&quot;w0&quot;)\nw1 = tf.Variable(0.0, name=&quot;w1&quot;)\nw2 = tf.Variable(0.0, name=&quot;w3&quot;)\n\ny_predicted =  X*X*w1 + X*w2 + w0\n# Define the cost function\ncostF = 0.5*tf.square(Y-y_predicted)\n# Define the operation that will be called on each iteration\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(costF)\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n# Loop through the data training\nfor epoch in range(training_epochs):\n       for (x, y) in zip(x_train_, y_train):\n            sess.run(train_op, feed_dict={X: x, Y: y})                                \n\n\ny_hat = sess.run(y_predicted, feed_dict={X: x_train_})\nprint (sess.run([w0,w1,w2]))\nsess.close()\n\nplt.plot(x_train, y_train)\nplt.plot(x_train, y_hat)\nplt.show()\n\n[4.9228806, -0.08735728, 3.029659]\n'
"ValueError: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: {'image': &lt;tf.Tensor: id=1012, shape=(32, 28, 28, 1), dtype=float64, numpy=array([...])&gt;, 'label': &lt;tf.Tensor: id=1013, shape=(32, 10), dtype=uint8, numpy=array([...]), dtype=uint8)&gt;}\n"
'X_train /= 255.0\nX_test /= 255.0\n'
'model.fit(\n    train_images,\n    train_labels,\n    epochs=100,\n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            "cp.ckpt", monitor="mean_absolute_error", save_best_only=True, verbose=1\n        )\n    ],\n)\n\nmodel.load_weights("cp.ckpt")\n\nmodel.fit(\n    train_images,\n    train_labels,\n    epochs=100,\n    callbacks=[\n        keras.callbacks.ModelCheckpoint(\n            "weights.hdf5",\n            monitor="mean_absolute_error",\n            save_best_only=True,\n            verbose=1,\n            save_weights_only=True,  # Specify this\n        )\n    ],\n)\n'
"import cv2\nimport numpy as np\n\n# Load image, grayscale, Otsu's threshold\nimage = cv2.imread('1.png')\noriginal = image.copy()\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n\n# Dilate with horizontal kernel\nkernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20,10))\ndilate = cv2.dilate(thresh, kernel, iterations=2)\n\n# Find contours and remove non-diagram contours\ncnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    x,y,w,h = cv2.boundingRect(c)\n    area = cv2.contourArea(c)\n    if w/h &gt; 2 and area &gt; 10000:\n        cv2.drawContours(dilate, [c], -1, (0,0,0), -1)\n\n# Iterate through diagram contours and form single bounding box\nboxes = []\ncnts = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\nfor c in cnts:\n    x, y, w, h = cv2.boundingRect(c)\n    boxes.append([x,y, x+w,y+h])\n\nboxes = np.asarray(boxes)\nx = np.min(boxes[:,0])\ny = np.min(boxes[:,1])\nw = np.max(boxes[:,2]) - x\nh = np.max(boxes[:,3]) - y\n\n# Extract ROI\ncv2.rectangle(image, (x,y), (x + w,y + h), (36,255,12), 3)\nROI = original[y:y+h, x:x+w]\n\ncv2.imshow('image', image)\ncv2.imshow('thresh', thresh)\ncv2.imshow('dilate', dilate)\ncv2.imshow('ROI', ROI)\ncv2.waitKey()\n"
'[[1,2,3],\n [2,1,3],\n [1,4,0]]\n\n[3,3,4]\n'
'Could not find a version that satisfies the requirement tensorflow&gt;=2.0.0 (from turicreate) (from versions: 0.12.1, 1.0.0, 1.0.1, 1.1.0rc0, 1.1.0rc1, 1.1.0rc2, 1.1.0, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.3.0rc0, 1.3.0rc1, 1.3.0rc2, 1.3.0, 1.4.0rc0, 1.4.0rc1, 1.4.0, 1.4.1, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.8.0rc0, 1.8.0rc1, 1.8.0, 1.9.0rc0, 1.9.0rc1, 1.9.0rc2, 1.9.0, 1.10.0rc0, 1.10.0rc1, 1.10.0, 1.10.1, 1.11.0rc0, 1.11.0rc1, 1.11.0rc2, 1.11.0, 1.12.0rc0, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.12.2, 1.12.3, 1.13.0rc0, 1.13.0rc1, 1.13.0rc2, 1.13.1, 1.13.2, 1.14.0rc0, 1.14.0rc1, 1.14.0, 2.0.0a0, 2.0.0b0, 2.0.0b1)\nNo matching distribution found for tensorflow&gt;=2.0.0 (from turicreate)\n\nfrom __future__ import print_function\nimport sys\n\n!{sys.executable} -m pip install turicreate\n\nSuccessfully installed absl-py-0.9.0 astor-0.8.1 cachetools-4.0.0 coremltools-3.1 gast-0.2.2 google-auth-1.10.0 google-auth-oauthlib-0.4.1 google-pasta-0.1.8 grpcio-1.26.0 h5py-2.10.0 keras-applications-1.0.8 keras-preprocessing-1.1.0 llvmlite-0.30.0 markdown-3.1.1 numba-0.46.0 numpy-1.16.4 opt-einsum-3.1.0 pandas-0.25.3 pillow-6.2.1 prettytable-0.7.2 protobuf-3.11.2 pyasn1-0.4.8 pyasn1-modules-0.2.7 pytz-2019.3 requests-oauthlib-1.3.0 resampy-0.2.1 rsa-4.0 scipy-1.4.1 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 turicreate-6.0 werkzeug-0.16.0 wrapt-1.11.2\n'
'&gt;&gt;&gt; A = np.array([[1,2,3],[4,5,6]])\n&gt;&gt;&gt; A.reshape(3,2)\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n&gt;&gt;&gt; A.transpose()\narray([[1, 4],\n       [2, 5],\n       [3, 6]])\n'
'import pandas as pd\nfrom io import StringIO\n\n\ndata = StringIO("""\nCol1 Col2 Col3\n12 10 3\n3 5 2\n100 12 10\n13 4 1\n""")\n\n# load data into data frame\ndf = pd.read_csv(data, sep=\' \')\n\nimport statsmodels.tsa.stattools as tsa\nadf_results = {}\nfor col in df.columns.values:\n    adf_results[col] = tsa.adfuller(df[col])\n\n# loop over dictionary data\ncolumns_big = []\ncolumns_small = []\nfor key, value in adf_results.items():\n    if value[1] &gt; 0.05:\n        columns_big.append(key)\n    else:\n        columns_small.append(key)\n\ncolumns_big = [\'Col1\', \'Col3\']\ncolumns_small = [\'Col2\']\n'
"clf_rf_4 = RandomForestRegressor()\nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=4,scoring='accuracy')\n\nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=4,scoring='r2')\n"
'class Bottleneck(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        # ...\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:    # &lt;-- conditional execution!\n            identity = self.downsample(x)\n\n        out += identity  # &lt;-- inplace operations\n        out = self.relu(out)\n\n        return out\n'
'for name, model in models.items():\n'
'from neuraxle.base import Identity\nfrom neuraxle.steps.flow import TrainOnlyWrapper, ChooseOneStepOf\nfrom neuraxle.steps.numpy import NumpyConcatenateInnerFeatures, NumpyShapePrinter, NumpyFlattenDatum\nfrom neuraxle.union import FeatureUnion\n\n\npipeline = Pipeline([\n    TrainOnlyWrapper(NumpyShapePrinter(custom_message=&quot;Input shape before feature union&quot;)),\n    FeatureUnion([\n        Pipeline([\n            NumpyFFT(),\n            NumpyAbs(),\n            FeatureUnion([\n                NumpyFlattenDatum(),  # Reshape from 3D to flat 2D: flattening data except on batch size\n                FFTPeakBinWithValue()  # Extract 2D features from the 3D FFT bins\n            ], joiner=NumpyConcatenateInnerFeatures())\n        ]),\n        NumpyMean(),\n        NumpyMedian(),\n        NumpyMin(),\n        NumpyMax()\n    ], joiner=NumpyConcatenateInnerFeatures()),\n    # TODO, optional: Add some feature selection right here for the motivated ones:\n    #      https://scikit-learn.org/stable/modules/feature_selection.html\n    # TODO, optional: Add normalization right here (if using other classifiers)\n    #      https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html\n    TrainOnlyWrapper(NumpyShapePrinter(custom_message=&quot;Shape after feature union, before classification&quot;)),\n    # Shape: [batch_size, remade_features]\n    ChooseOneStepOf([\n        decision_tree_classifier,\n        # extra_tree_classifier,  # TODO\n        # ridge_classifier,  # TODO\n        logistic_regression,\n        # random_forest_classifier  # TODO\n    ]),\n    TrainOnlyWrapper(NumpyShapePrinter(custom_message=&quot;Shape at output after classification&quot;)),\n    # Shape: [batch_size]\n    Identity()\n])\n\nfrom neuraxle.metaopt.auto_ml import AutoML, InMemoryHyperparamsRepository, validation_splitter, \\\n    RandomSearchHyperparameterSelectionStrategy\nfrom neuraxle.metaopt.callbacks import ScoringCallback\nfrom sklearn.metrics import accuracy_score\n\n\nauto_ml = AutoML(\n    pipeline=pipeline,\n    hyperparams_optimizer=RandomSearchHyperparameterSelectionStrategy(),\n    validation_split_function=validation_splitter(test_size=0.20),\n    scoring_callback=ScoringCallback(accuracy_score, higher_score_is_better=False),\n    n_trials=7,\n    epochs=1,\n    hyperparams_repository=InMemoryHyperparamsRepository(cache_folder=cache_folder),\n    refit_trial=True,\n)\n'
"model = Sequential([\n    Dense(30, input_shape=(2,), activation='relu'),\n    # Dense(10, activation='relu'), # uncomment for experimentation\n    Dense(2, activation='linear'),\n])\n\nmodel.compile(SGD(lr=0.01), loss='mean_squared_error')\n"
'import numpy as np\nX_test = np.array([[30]])\nprediction = model.predict(X_test)\n'
"from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import plot_confusion_matrix\n\ndata = load_iris()\nX = data.data\ny = data.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nclassifiers = [LogisticRegression(solver='lbfgs'), \n               AdaBoostClassifier(),\n               GradientBoostingClassifier(), \n               SVC()]\nfor cls in classifiers:\n    cls.fit(X_train, y_train)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n\nfor cls, ax in zip(classifiers, axes.flatten()):\n    plot_confusion_matrix(cls, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap='Blues',\n                         display_labels=data.target_names)\n    ax.title.set_text(type(cls).__name__)\nplt.tight_layout()  \nplt.show()\n"
'def calculate_gains(self):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n        result_list = executor.map(self.calculate_gains_helper,\n            ((i, i+10) for i in range(0, len(self.attributes), 10)))\n    for return_value in result_list:\n        self.gains = {**self.gains, **return_value}\n\ndef calculate_gains_helper(self, start_end):\n    start, end = start_end\n    inter_result = {}\n    for attribute in self.attributes[start:end]:\n        inter_result[attribute] = self.gain(attribute)\n    return inter_result\n'
"mirrored_strategy = tf.distribute.MirroredStrategy()\nwith mirrored_strategy.scope():\n\n    new_model = load_model('\\\\models\\\\model_0610.h5', \n                custom_objects = {'dice_coef_loss': dice_coef_loss, \n                'dice_coef': dice_coef}, compile = False)\n    new_model.compile(optimizer = Adam(learning_rate = 1e-4, loss = dice_coef_loss,\n                metrics = [dice_coef])\n"
'for i in range(1, 1001):\n    X_train, y_train = data.iloc[i:1000+i], target.iloc[i:1000+i]\n    X_test, y_test = data.iloc[1000+i], target.iloc[1000+i]\n'
'import numpy as np\nfrom tensorflow.keras import layers, models\n\n\nboard_inputs = layers.Input(shape=(8, 8, 12))\nconv1= layers.Conv2D(10, 3, activation=\'relu\')\nconv2 = layers.Conv2D(10, 3, activation=\'relu\')\npooling1 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)\npooling2 = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)\nflatten = layers.Flatten(data_format=None)\n\n\nx = conv1(board_inputs)\nx = pooling1(x)\nx = conv2(x)\nx = flatten(x)\npiece_output = layers.Dense(12,name = \'piece\')(x)\nalpha_output = layers.Dense(7,name = \'alpha\')(x)\nnumbers_output = layers.Dense(7,name = \'number\')(x)\n\n\nmodel = models.Model(inputs=board_inputs, outputs=[piece_output,alpha_output,numbers_output], name="chess_ai_v3")\nmodel.compile(loss=\'mse\', optimizer=\'adam\')\nmodel.summary()\n\nX = np.random.uniform(0,1, (100,8,8,12))\ny = {"piece": np.random.uniform(0,1,(100,12)), \n     "alpha": np.random.uniform(0,1,(100,7)),\n     "number": np.random.uniform(0,1,(100,7))}\n\nhistory = model.fit(X,y, epochs=2, batch_size=32)\n'
'from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport matplotlib. pyplot as plt\nimport seaborn as sns\n\niris = sns.load_dataset(\'iris\')\n\nX_train, X_test, Y_train, Y_test = train_test_split(iris.iloc[:,:3], iris.iloc[:,3],random_state=11)\n\nLR = LinearRegression()\nLR.fit(X_train,Y_train)\nY_pred = LR.predict(X_test)\n\nsns.set(style="whitegrid")\nfig, ax = plt.subplots(figsize =(5,5))\nsns.regplot(x=Y_pred,y=Y_test-Y_pred,ax=ax,lowess=True)\nax.set(ylabel=\'residuals\',xlabel=\'fitted values\')\n\nplotfit = LinearRegression()\nplotfit.fit(Y_test.to_numpy().reshape(-1,1),Y_pred)\nresidual = Y_pred - plotfit.predict(Y_test.to_numpy().reshape(-1,1))\n\nsns.set(style="whitegrid")\nfig, ax = plt.subplots(1,2,figsize =(10,5))\n\nsns.residplot(Y_test,Y_pred,lowess=True, color="g",ax=ax[0])\nax[0].set_xlim(0,2.5)\nsns.regplot(x=Y_test,y=residual,lowess=True)\nax[1].set_xlim(0,2.5)\n'
"from matplotlib import pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier(random_state=0)\niris = load_iris()\nclf = clf.fit(iris.data, iris.target)\nfig, ax = plt.subplots(figsize=(10,10))\nout = tree.plot_tree(clf)\nfor o in out:\n    arrow = o.arrow_patch\n    if arrow is not None:\n        arrow.set_edgecolor('red')\n        arrow.set_linewidth(3)\n"
'output = (output*2)-1\n'
'    def get_features(self, y, sample_rate):\n\n        # convert to mono\n        if self.mono:\n            y = np.mean(y, axis=1, keepdims=True)\n            y = np.squeeze(y)    # Add this line\n        \n        # resample if sample rates mismatch\n        if (self.sample_rate is not None) and (self.sample_rate != sample_rate):\n            y = librosa.core.resample(y.T, sample_rate, self.sample_rate).T\n            sample_rate = self.sample_rate\n\n        # augment data\n        if self.augmentation is not None:\n            y = self.augmentation(y, sample_rate)\n\n        # TODO: how time consuming is this thing (needs profiling...)\n        if len(y.shape) == 1:     # Add this line\n            y = y[:,np.newaxis]   # Add this line\n            \n        try:\n            valid = valid_audio(y[:, 0], mono=True)\n        except ParameterError as e:\n            msg = f&quot;Something went wrong when augmenting waveform.&quot;\n            raise ValueError(msg)\n\n        return y\n'
'final MetaGraphDef metaGraphDef = MetaGraphDef.parseFrom(bundle.metaGraphDef());\nfinal SignatureDef signatureDef = metaGraphDef.getSignatureDefMap().get(&quot;serving_default&quot;);\n\nfinal TensorInfo inputTensorInfo = signatureDef.getInputsMap()\n    .values()\n    .stream()\n    .filter(Objects::nonNull)\n    .findFirst()\n    .orElseThrow(() -&gt; ...);\n\nfinal TensorInfo outputTensorInfo = signatureDef.getOutputsMap()\n    .values()\n    .stream()\n    .filter(Objects::nonNull)\n    .findFirst()\n    .orElseThrow(() -&gt; ...);\n'
"import numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom mlxtend.feature_selection import ColumnSelector\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import BernoulliNB\n\n\n\ndf_Xtrain = pd.DataFrame({'tweet': ['This is a tweet']*10,\n                          'label': 0})\ny_train = df_Xtrain['label'].to_numpy().ravel()\n\npipe = Pipeline([\n    ('col_selector', ColumnSelector(cols=('tweet'),drop_axis=True)),\n    ('tfidf', TfidfVectorizer()),\n    ('bernoulli', BernoulliNB()),\n])\n\n\npipe.fit(df_Xtrain,y_train)\n\nPipeline(steps=[('col_selector', ColumnSelector(cols='tweet', drop_axis=True)),\n                ('tfidf', TfidfVectorizer()), ('bernoulli', BernoulliNB())])\n\nclass SelectColumnsTransformer():\n    def __init__(self, columns=None):\n        self.columns = columns\n\n    def transform(self, X, **transform_params):\n        cpy_df = X[self.columns].copy()\n        return cpy_df\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n\n# Add it to a pipeline \npipe = Pipeline([\n    ('selector', SelectColumnsTransformer([&lt;input col name here&gt;]))\n])\n"
"x = np.ones(shape = (1,20,6))\narray([[[1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1., 1.]]])\n\n\ny = np.ones(shape = (1,6))\narray([[1., 1., 1., 1., 1., 1.]])\n\n\ny-x\narray([[[0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0.]]])\n\nx = np.ones(shape = (10,20,6))\ny = np.ones(shape = (10,6))\ny-x\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-102-4a65323a80fa&gt; in &lt;module&gt;\n      1 x = np.ones(shape = (10,20,6))\n      2 y = np.ones(shape = (10,6))\n----&gt; 3 y-x\n\nValueError: operands could not be broadcast together with shapes (10,6) (10,20,6)\n\nmodel = Sequential()\nmodel.add(LSTM(242, input_shape=Input_shape, return_sequences=True))\nmodel.add(Dropout(0.3)); model.add(BatchNormalization())  \n\nmodel.add(LSTM(242, return_sequences=True))\nmodel.add(Dropout(0.3)); model.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(labels, activation='tanh')) \n\nopt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\nmodel.compile(loss='mean_absolute_error',optimizer=opt,metrics=['mse'])\ntf.keras.utils.plot_model(model, 'my_first_model.png', show_shapes=True)\n\nmodel.fit(train_batch_gen, epochs=EPOCHS, validation_data = validation_batch_gen) \n\nEpoch 1/3\n2/2 [==============================] - 1s 708ms/step - loss: 0.2891 - mse: 0.5739 - val_loss: 0.4078 - val_mse: 0.2461\nEpoch 2/3\n2/2 [==============================] - 0s 46ms/step - loss: 0.2229 - mse: 0.3151 - val_loss: 0.3867 - val_mse: 0.2225\nEpoch 3/3\n2/2 [==============================] - 0s 49ms/step - loss: 0.2315 - mse: 0.3341 - val_loss: 0.3813 - val_mse: 0.2161\n"
'\n#method to use generator to split data into mini batches of 256 each loaded at run time\ndef generate_data(X1,X2,Y,batch_size):\n  count=0\n  p_input=[]\n  c_input=[]\n  target=[]\n  batch_count=0\n  while True:\n    for i in range(len(X1)):\n      p_input.append(X1[i])\n      c_input.append(X2[i])\n      target.append(Y[i])\n      batch_count+=1\n      if batch_count&gt;batch_size:\n        count=count+1\n        prev_X=np.array(p_input,dtype=np.int64)\n        cur_X=np.array(c_input,dtype=np.int64)\n        cur_y=np.array(target,dtype=np.int32)\n        yield ([prev_X,cur_X],cur_y ) \n        p_input=[]\n        c_input=[]\n        target=[]\n        batch_count=0\n    print(count)\n  return\n\nEpoch 1/3\n335/347 [===========================&gt;..] - ETA: 30s - batch: 167.0000 - size: 257.0000 - loss: 1.2734 - accuracy: 0.8105346\n347/347 [==============================] - ETA: 0s - batch: 173.0000 - size: 257.0000 - loss: 1.2635 - accuracy: 0.8113WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2048: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\n86\n347/347 [==============================] - 964s 3s/step - batch: 173.0000 - size: 257.0000 - loss: 1.2635 - accuracy: 0.8113 - val_loss: 0.5700 - val_accuracy: 0.8367\n'
"import pandas as pd\nfrom pandas.tseries.offsets import Minute\n\ndata = pd.read_csv(&quot;example_all.csv&quot;, parse_dates=[0])\n\ndata = data.sort_values('_time')\n\n\ndef all_over_800(values):\n    return values.map(lambda x: x &gt;= 800).all()\n\n\ndata['over_threshold'] = data[['_time', 'duration']].rolling(\n    Minute(3), on='_time').apply(lambda win: all_over_800(win))['duration']\n\n"
"# probabilites\ny = clf.predict_proba(X_train)[:,1]\n# raw scores, default link=&quot;identity&quot;\ny_raw = np.log(y/(1-y))\n# expected raw score\nprint(np.mean(y_raw))\nprint(np.isclose(explainer.expected_value, np.mean(y_raw), 1e-12))\n2.065861773054686\n[ True]\n\nshap.force_plot(explainer.expected_value[0], shap_values[0,:], X_train.iloc[0,:], link=&quot;identity&quot;)\n\nfrom scipy.special import expit, logit\n# probabilites\ny = clf.predict_proba(X_train)[:,1]\n# exected raw base value\ny_raw = logit(y).mean()\n# expected probability, i.e. base value in probability spacy\nprint(expit(y_raw))\n0.8875405774316522\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport pandas as pd\nimport shap\nprint(shap.__version__)\n\nX, y = load_breast_cancer(return_X_y=True)\nX = pd.DataFrame(data=X)\ny = pd.DataFrame(data=y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nclf = GradientBoostingClassifier(random_state=0)\nclf.fit(X_train, y_train.values.ravel())\n\n# load JS visualization code to notebook\nshap.initjs()\n\nexplainer = shap.TreeExplainer(clf, model_output=&quot;raw&quot;)\nshap_values = explainer.shap_values(X_train)\n\nfrom scipy.special import expit, logit\n# probabilites\ny = clf.predict_proba(X_train)[:,1]\n# exected raw base value\ny_raw = logit(y).mean()\n# expected probability, i.e. base value in probability spacy\nprint(&quot;Expected raw score (before sigmoid):&quot;, y_raw)\nprint(&quot;Expected probability:&quot;, expit(y_raw))\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value[0], shap_values[0,:], X_train.iloc[0,:], link=&quot;logit&quot;)\n\n0.36.0\nExpected raw score (before sigmoid): 2.065861773054686\nExpected probability: 0.8875405774316522\n"
'from sklearn.svm import SVC\nclf=SVC(probability=True)\nclf.fit(X,Y)\nprint clf.predict_proba(W) #No error\n'
'self.weights[i] += (self.alpha * theta * Fsa[i])\nnormalize(self.weights[i],wmin,wmax)\n'
"import numpy as np\nimport cv2\n\nsrc = cv2.imread('objects.png')\nsrc_flatten = np.reshape(np.ravel(src, 'C'), (-1, 3))\ndst = np.zeros(src.shape, np.float32)\n\ncolors = np.array([[0x00, 0x00, 0x00],\n                   [0xff, 0xff, 0xff],\n                   [0xff, 0x00, 0x00],\n                   [0x00, 0xff, 0x00],\n                   [0x00, 0x00, 0xff]], dtype=np.float32)\nclasses = np.array([[0], [1], [2], [3], [4]], np.float32)\n\nknn = cv2.KNearest()\nknn.train(colors, classes)\nretval, result, neighbors, dist = knn.find_nearest(src_flatten.astype(np.float32), 1)\n\ndst = colors[np.ravel(result, 'C').astype(np.uint8)]\ndst = dst.reshape(src.shape).astype(np.uint8)\n\ncv2.imshow('src', src)\ncv2.imshow('dst', dst)\ncv2.waitKey()\n"
'p(word1|like)   -- among all the articles I like, the probability of word1 appears\np(word2|like)   -- among all the articles I like, the probability of word2 appears\n...\np(wordn|like)   -- among all the articles I like, the probability of wordn appears\n\np(word1|unlike) -- among all the articles I do not like, the prob of word1 appears\n...\n\np(like)  -- the portion of articles I like (should be 0.2 in your example)\np(unlike) -- the portion of articles I do not like. (0.8)\n\n   prob(like|51th article)      p(like) x p(word2|like) x p(word5|like)\n ---------------------------- = -----------------------------------------\n   prob(unlike|51th article)    p(unlike)xp(word2|unlike)xp(word5|unlike)\n'
"result = s.predict (doc)\n\ndata = [[('this', 'is'), ('is', 'a'), ('a', 'text'), 'SPAM'], \n[('and', 'one'), ('one', 'more'), 'HAM']] \n\nFeatures:   'this is'  'is a'   'a text'  'and one'   'one more'     Label\ndoc 1:         1         1         1          0           0           SPAM (or as I explained 0)\ndoc 2:         0         0         0          1           1           HAM (or as I explained 1)\n\ndata = [([1,1,1,0,0),(0)],[(0,0,0,1,1),(1)]]\n\nlabels = [0,0,0,0,0,0,0,0,...,1,1,1,1,1,1,1,...]\n\nlabels = [0,1,0,1,0,1, ...] \n"
'tfidf_vect= TfidfVectorizer(use_idf=False,binary=True, norm=False, ngram_range=(2, 2))\n'
'&gt;&gt;&gt; from nltk.corpus import names\n&gt;&gt;&gt; names[:5]\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nTypeError: \'LazyCorpusLoader\' object has no attribute \'__getitem__\'\n&gt;&gt;&gt; names.words()[:5]\n[u\'Abagael\', u\'Abagail\', u\'Abbe\', u\'Abbey\', u\'Abbi\']\n\n# To get the input list of tuples for apply_features, we do this:\n&gt;&gt;&gt; [(word,\'female\') for word in names.words(\'female.txt\')[:10]]\n[(u\'Abagael\', \'female\'), (u\'Abagail\', \'female\'), (u\'Abbe\', \'female\'), (u\'Abbey\', \'female\'), (u\'Abbi\', \'female\'), (u\'Abbie\', \'female\'), (u\'Abby\', \'female\'), (u\'Abigael\', \'female\'), (u\'Abigail\', \'female\'), (u\'Abigale\', \'female\')]\n\n# Let\'s get 250 from female and 250 from male names.\n&gt;&gt;&gt; train_female = [(word,\'female\') for word in names.words(\'female.txt\')[:250]] \n&gt;&gt;&gt; train_male = [(word,\'male\') for word in names.words(\'male.txt\')[:250]]\n&gt;&gt;&gt; train_data = train_female + train_male\n&gt;&gt;&gt; apply_features(gender_features, train_data)\n[({\'last_letter\': u\'l\'}, \'female\'), ({\'last_letter\': u\'l\'}, \'female\'), ...]\n\nfrom nltk.corpus import names\nfrom nltk.classify import apply_features, NaiveBayesClassifier\n\ndef gender_features(word):\n    return {\'last_letter\': word[-1]}\n\n\ntrain_female = [(word,\'female\') for word in names.words(\'female.txt\')[:250]] \ntrain_male = [(word,\'male\') for word in names.words(\'male.txt\')[:250]]\ntrain_data = train_female + train_male\ntrain_set = apply_features(gender_features, train_data)\n\n# Do like wise for the test set.\n\'\'\'\ntest_female = [(word,\'female\') for word in names.words(\'female.txt\')[250:]]\ntest_male = [(word,\'male\') for word in names.words(\'male.txt\')[250:]] \ntest_data = test_female + test_male\ntest_set = apply_features(gender_features, test_data)\n\'\'\'\n\nclassifier = NaiveBayesClassifier.train(train_set)\nprint classifier.classify(gender_features(\'Neo\'))\n\n\'male\'\n'
'from sklearn import linear_model                                                                                                                                              \nfrom sklearn import datasets                                                                                                                                                  \nfrom sklearn import metrics    \nboston = datasets.load_boston()                                                                                                                                                      \n\n\nX_train = boston.data[:450]  #define training X set                                                                                                                           \ny_train = boston.target[:450] #define training y set                                                                                                                          \n\nX_test = boston.data[450:]  #define test X set                                                                                                                                \ny_test = boston.target[450:] #define test y set                                                                                                                               \n\nlin = linear_model.LinearRegression() #initialize regressor                                                                                                                   \n\nlin.fit(X_train, y_train) #fit training data                                                                                                                                  \npreds = lin.predict(X_test) #make prediction on X test set                                                                                                                    \n\nprint metrics.mean_absolute_error(y_test, preds) #evaluate performance \n'
"df = pd.DataFrame(np.random.choice(['yes', 'no'], size=(5,3)), columns=list('ABC'))\ndf\n\n     A    B    C\n0   no  yes   no\n1   no  yes  yes\n2  yes  yes   no\n3  yes   no   no\n4  yes  yes  yes\n\n\ndf.replace(['yes', 'no'], [1, 0])\n\n   A  B  C\n0  0  1  0\n1  0  1  1\n2  1  1  0\n3  1  0  0\n4  1  1  1\n\npd.DataFrame(np.where(df=='yes', 1, 0), columns=df.columns, index=df.index)\n"
"import numpy\nimport theano\nimport theano.tensor as tt\n\n\ndef compile_model(input_size, hidden_size, output_size, learning_rate):\n    w_h = theano.shared(numpy.random.randn(input_size, hidden_size).astype(theano.config.floatX))\n    b_h = theano.shared(numpy.zeros(hidden_size, dtype=theano.config.floatX))\n    w_y = theano.shared(numpy.random.randn(hidden_size, output_size).astype(theano.config.floatX))\n    b_y = theano.shared(numpy.zeros(output_size, dtype=theano.config.floatX))\n    parameters = (w_h, b_h, w_y, b_y)\n    x = tt.matrix()\n    z = tt.lvector()\n    h = tt.tanh(theano.dot(x, w_h) + b_h)\n    y = tt.nnet.softmax(theano.dot(h, w_y) + b_y)\n    c = tt.nnet.categorical_crossentropy(y, z).mean()\n    u = [(p, p - learning_rate * theano.grad(c, p)) for p in parameters]\n    trainer = theano.function([x, z], outputs=[c], updates=u)\n    tester = theano.function([x], outputs=[y])\n    return trainer, tester, parameters\n\n\ndef refresh_model(parameters, input_size, hidden_size, output_size):\n    w_h, b_h, w_y, b_y = parameters\n    w_h.set_value(numpy.random.randn(input_size, hidden_size).astype(theano.config.floatX))\n    b_h.set_value(numpy.zeros(hidden_size, dtype=theano.config.floatX))\n    w_y.set_value(numpy.random.randn(hidden_size, output_size).astype(theano.config.floatX))\n    b_y.set_value(numpy.zeros(output_size, dtype=theano.config.floatX))\n\n\ndef main():\n    input_size = 30\n    hidden_size = 10\n    output_size = 20\n    learning_rate = 0.01\n    batch_size = 40\n    epoch_count = 50\n\n    trainer, tester, parameters = compile_model(input_size, hidden_size, output_size, learning_rate)\n    x = numpy.random.randn(batch_size, input_size)\n    z = numpy.random.randint(output_size, size=(batch_size,))\n    print 'Training model with hidden size', hidden_size\n\n    for _ in xrange(epoch_count):\n        print trainer(x, z)\n\n    hidden_size = 15\n    refresh_model(parameters, input_size, hidden_size, output_size)\n    print 'Training model with hidden size', hidden_size\n\n    for _ in xrange(epoch_count):\n        print trainer(x, z)\n\n\nmain()\n"
'[CONV]\n\n[CONV]\n\n[POOL]\n'
"y = np.arange(0, 1000, .01) + np.random.normal(0, 100, 100000)\ndata = pd.DataFrame(data={'bc_conc': y}, index=pd.date_range(freq='H', start=datetime(2000, 1, 1), periods=len(y)))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 100000 entries, 2000-01-01 00:00:00 to 2011-05-29 15:00:00\nFreq: H\nData columns (total 1 columns):\nbc_conc    100000 non-null float64\ndtypes: float64(1)\n\n                        bc_conc\n2000-01-01 00:00:00  -30.639811\n2000-01-01 01:00:00  -26.791396\n2000-01-01 02:00:00 -121.542718\n2000-01-01 03:00:00  -69.267944\n2000-01-01 04:00:00  117.731532\n\ndata = data.resample('D', closed='left', label='left').mean() # optional for daily data\nx2 = matplotlib.dates.date2num(data.index.to_pydatetime()) # Dates to float representing (fraction of) days since 0001-01-01 00:00:00 UTC plus one\n\n[ 730120.  730121.  730122. ...,  734284.  734285.  734286.]\n\nz = np.polyfit(x2, data.bc_conc, 1)\n\n[  2.39988999e-01  -1.75220741e+05]  # coefficients\n\np = np.poly1d(z)\n\n0.24 x - 1.752e+05 # fitted polynomial\n\ndata['trend'] = p(x2)  # trend from polynomial fit\n\n              bc_conc     trend\n2000-01-01 -29.794608  0.026983\n2000-01-02   6.727729  0.266972\n2000-01-03   9.815476  0.506961\n2000-01-04 -27.954068  0.746950\n2000-01-05 -13.726714  0.986939\n\ndata.plot()\nplt.show()\n"
'GridSearchCV(..., verbose=1)\n'
"types = {'col1': np.dtype(type),\n     'col2': np.dtype(type),\n     'col3' : np.dtype(type),\n     'col4': np.dtype(type),\n     'col5': np.dtype(type) }\n\ntrain = pd.read_csv('train.csv', dtype=types)\n"
"def main():\n    pipeline = Pipeline([\n        ('vect', TfidfVectorizer(ngram_range=(2,2), min_df=1)),\n        ('clf',SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n                     gamma=1e-3, kernel='rbf', max_iter=-1, probability=False, random_state=None,\n                     shrinking=True, tol=0.001, verbose=False))\n    ])\n\n\n    parameters = {\n        'vect__max_df': (0.25, 0.5),\n        'vect__use_idf': (True, False),\n        'clf__C': [1, 10, 100, 1000],\n\n    }\n\n\n    X, y = X, Y.as_matrix()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n    print 'Best score: %0.3f' % grid_search.best_score_\n    print 'Best parameters set:'\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print '\\t%s: %r' % (param_name, best_parameters[param_name])\n\n\nif __name__ == '__main__':\nmain()\n"
"for train_index, val_index in kf:\n    cv_train_x = X_train.iloc[train_index]\n    cv_val_x = X_train.iloc[val_index]\n    cv_train_y = y_train[train_index]\n    cv_val_y = y_train[val_index]\n    print cv_train_x\n\n    logreg = LogisticRegression(C = .01)\n    logreg.fit(cv_train_x, cv_train_y)\n    pred = logreg.predict(cv_val_x)\n    print accuracy_score(cv_val_y, pred)\n\n        length       tempo  variation\n4   509.931973  135.999178   0.001631\n2   397.500952  112.347147   0.008146\n7   502.083628   99.384014   0.009262\n6   763.377778  107.666016   0.002513\n5   560.365714  151.999081   0.001620\n3  1109.819501  172.265625   0.005367\n9   269.001723  117.453835   0.000733\n\nX_train.loc[1]\n\nKeyError: 'the label [1] is not in the [index]'\n\n X_train.loc[[1,4]]\n\n       length       tempo  variation\n1         NaN         NaN        NaN\n4  509.931973  135.999178   0.001631\n"
"for tr, te in zip(train_list, test_list):\n    svm.SVC(kernel='linear', C=1).train(X[tr, :], y[tr]).score(X[te, :], y[te])\n\nscore(X, y, sample_weight=None)\n"
'def regression_target(label_name=None,\n                      weight_column_name=None,\n                      target_dimension=1):\n  """Creates a _TargetColumn for linear regression.\n  Args:\n    label_name: String, name of the key in label dict. Can be null if label\n        is a tensor (single headed models).\n    weight_column_name: A string defining feature column name representing\n      weights. It is used to down weight or boost examples during training. It\n      will be multiplied by the loss of the example.\n    target_dimension: dimension of the target for multilabels.\n  Returns:\n    An instance of _TargetColumn\n  """\n  return _RegressionTargetColumn(loss_fn=_mean_squared_loss,\n                                 label_name=label_name,\n                                 weight_column_name=weight_column_name,\n                                 target_dimension=target_dimension)\n'
'from scipy.optimize import brentq\nimport statsmodels.api as sm\nimport numpy as np\n\n# fit\nkde = sm.nonparametric.KDEMultivariate()  # ... you already did this\n\n# sample\nu = np.random.random()\n\n# 1-d root-finding\ndef func(x):\n    return kde.cdf([x]) - u\nsample_x = brentq(func, -99999999, 99999999)  # read brentq-docs about these constants\n                                              # constants need to be sign-changing for the function\n'
"import pickle\npickle.dump(object, file)\n\nwith open('model.pkl', 'wb') as nb_classifier_model:\n    pickle.dump(nb_classifier, nb_classifier_model)\n\nwith open('model.pkl', 'rb') as nb_classifier_model:\n    nb_classifier = pickle.load(nb_classifier_model)\n"
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\n\nX = [1, 2, 3, 10, 11, 12]\nX = np.reshape(X, (6, 1))\nY = [0, 0, 0, 1, 1, 1]\nY = np.reshape(Y, (6, 1))\n\nlr = LogisticRegression()\nlr.fit(X, Y)\n\nplt.figure(1, figsize=(4, 3))\nplt.scatter(X.ravel(), Y, color=\'black\', zorder=20)\n\ndef model(x):\n    return 1 / (1 + np.exp(-x))\n\nX_test = np.linspace(-5, 15, 300)\nloss = model(X_test * lr.coef_ + lr.intercept_).ravel()\n\nplt.plot(X_test, loss, color=\'red\', linewidth=3)\nplt.axhline(y=0, color=\'k\', linestyle=\'-\')\nplt.axhline(y=1, color=\'k\', linestyle=\'-\')\nplt.axhline(y=0.5, color=\'b\', linestyle=\'--\')\nplt.axvline(x=X_test[123], color=\'b\', linestyle=\'--\')\n\nplt.ylabel(\'y\')\nplt.xlabel(\'X\')\nplt.xlim(0, 13)\nplt.show()\n\nprint ("2 --&gt; {0}".format(lr.predict(2)))\nprint ("3 --&gt; {0}".format(lr.predict(3)))\nprint ("3.1 --&gt; {0}".format(lr.predict(3.1)))\nprint ("3.3 --&gt; {0}".format(lr.predict(3.3)))\nprint ("4 --&gt; {0}".format(lr.predict(4)))\n\n2 --&gt; [0]\n3 --&gt; [0]\n3.1 --&gt; [0]  # Below threshold\n3.3 --&gt; [1]  # Above threshold\n4 --&gt; [1]\n'
'loss_sample = y_true * ln(y_pred) + (1-y_true) * ln(1-y_pred)\n'
'import tensorflow as tf\n\nW = tf.Variable([.3], tf.float32)\nb = tf.Variable([-3], tf.float32)\nx = tf.placeholder(tf.float32)\n\nlinear_model = W * x + tf.cast(b, tf.float32)\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\nprint(sess.run(linear_model, {x:[1,2,3,4]}))\n'
'# Existing features\nage = tf.feature_column.numeric_column("age")\neducation_num = tf.feature_column.numeric_column("education_num")\n# Declare a custom column just like other columns\nmy_feature = tf.feature_column.numeric_column("my_feature")\n\n...\n# Add to the list of features\nfeature_columns = { ... age, education_num, my_feature, ... }\n\n...\ndef input_fn():\n  df_data = pd.read_csv("input.csv")\n  df_data = df_data.dropna(how="any", axis=0)\n  # Manually update the dataframe\n  df_data["my_feature"] = df_data["age"] * df_data["education_num"]\n\n  return tf.estimator.inputs.pandas_input_fn(x=df_data,\n                                             y=labels,\n                                             batch_size=100,\n                                             num_epochs=10)\n\n...\nmodel.train(input_fn=input_fn())\n'
'math_ops.cast(q.size(), tf.float32) * (1. / capacity)\n'
'&gt;&gt;&gt; np.array([1, 2]).shape\n(2,)\n\nnet = Network([2,1,2])\nx = np.array([1, 2]).reshape([2, 1])  # one example of size 2\ny = np.array([3, 4]).reshape([2, 1])  # one example of size 2\nnet.backprop(x, y)\n'
"feature = [df['age'], df['job'], df['marital'], df['education'], df['default'], df['housing'], df['loan'], df['contact'],\n       df['month'], df['day_of_week'], df['campaign'], df['pdays'], df['previous'], df['emp.var.rate'], df['cons.price.idx'],\n       df['cons.conf.idx'], df['euribor3m'], df['nr.employed']]\nlabel = [df['y']]\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfeature = df[['age', 'job', 'marital', 'education', 'default', 'housing', \n              'loan', 'contact', 'month', 'day_of_week', 'campaign', \n              'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', \n              'cons.conf.idx', 'euribor3m', 'nr.employed']]\nlabel = df['y']\n\n# Model Training \nx = feature[:]\ny = label\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)\n"
'GridSearchCV(\n    sc=SparkContext.getOrCreate(),\n    estimator=ensemble.GradientBoostingRegressor(**params), \n    param_grid=param_test2, n_jobs=1)\n'
'theta = np.ones((2, 1))\n\nValueError: shapes (12,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)\n\ntheta = np.ones(2)\n\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport scipy.optimize as optimize \nfrom scipy.io import loadmat\n\ndata = loadmat(\'ex5data1.mat\')\nx = data[\'X\']\nX = data[\'X\']\nX = np.insert(X, 0, 1, axis=1)\ny = data[\'y\']\ntheta = np.ones(2)\n\ndef cost_function(theta, X, y, reg_param):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    m = float(y.shape[0])\n    h = X * theta.T\n    error = np.power((h - y), 2)\n    error = np.sum(error)\n    term = error / (2*m)\n    reg = (reg_param * np.sum(np.power(theta[1:, :], 2))) / (2*m)\n\nreturn term + reg\n\nprint "Cost function: \\n %s" % (cost_function(theta, X, y, 1))\n\ndef cost_function_gradient(theta, X, y, reg_param):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(y)\n    m = float(y.shape[0])\n\n    grad = np.zeros((len(X[0]) + 1, 1))\n    reg = np.multiply(theta.T[1:, :], reg_param/m)\n\n    for j in xrange(len(X[0])):\n        term = np.multiply((X * theta.T) - y, X[:, j + 1])\n        term = np.sum(term) / m\n        grad[j + 1, 0] = term + reg\n\n    grad[0, 0] = np.sum(np.multiply((X*theta.T - y), X[:, 0])) / m\n\n    return grad\n\nprint "Cost function gradient: \\n %s" % (cost_function_gradient(theta, X, y, 1))\n\nreg_param = 0\nopt = optimize.fmin_cg(cost_function, theta, args=(X, y, reg_param), maxiter=200)\n'
'$ conda install -c anaconda mkl\n\n$ conda install -c pytorch pytorch torchvision\n'
'    optimizer = tf.train.AdamOptimizer(learning_rate)\n\n    # Calculate and clip gradients\n    params = tf.trainable_variables()\n    gradients = tf.gradients(rnn.loss, params)\n    clipped_gradients, _ = tf.clip_by_global_norm(\n        gradients, FLAGS.max_gradient_norm)\n    train_op = optimizer.apply_gradients(zip(clipped_gradients,params), global_step=global_step)\n'
'X=np.matrix([[1,2104,5,1,45],[1,1416,3,2,40],[1,1534,3,2,30],[1,852,2,1,36]])\ny=np.matrix([[460],[232],[315],[178]])\n\nXT=X.T\nXTX=XT@X\n\ninv=np.linalg.pinv(XTX)\n\ntheta=(inv@XT)@y\nprint(theta)\n\n[[188.40031946]\n [  0.3866255 ]\n [-56.13824955]\n [-92.9672536 ]\n [ -3.73781915]]\n\n 0 0 0 0 ... 0 0 \n 0 1 0 0 ... 0 0 \n 0 0 1 0 ... 0 0\n 0 0 0 1 ... 0 0\n .\n .\n .\n 0 0 0 0 0 0 0 1\n'
'# READING DATA FROM train and validation (dev set) CSV FILES by using INITIALIZABLE ITERATORS\n\n# All csv files have same # columns. First column is assumed to be train example ID, the next 5 columns are feature\n# columns, and the last column is the label column\n\n# ASSUMPTIONS: (Otherwise, decode_csv function needs update)\n# 1) The first column is NOT a feature. (It is most probably a training example ID or similar)\n# 2) The last column is always the label. And there is ONLY 1 column that represents the label.\n#    If more than 1 column represents the label, see the next example down below\n\nfeature_names = [\'f1\',\'f2\',\'f3\',\'f4\',\'f5\']\nrecord_defaults = [[""], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n\n\ndef decode_csv(line):\n   parsed_line = tf.decode_csv(line, record_defaults)\n   label =  parsed_line[-1]      # label is the last element of the list\n   del parsed_line[-1]           # delete the last element from the list\n   del parsed_line[0]            # even delete the first element bcz it is assumed NOT to be a feature\n   features = tf.stack(parsed_line)  # Stack features so that you can later vectorize forward prop., etc.\n   #label = tf.stack(label)          #NOT needed. Only if more than 1 column makes the label...\n   batch_to_return = features, label\n   return batch_to_return\n\nfilenames = tf.placeholder(tf.string, shape=[None])\ndataset5 = tf.data.Dataset.from_tensor_slices(filenames)\ndataset5 = dataset5.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))\ndataset5 = dataset5.shuffle(buffer_size=1000)\ndataset5 = dataset5.batch(7)\niterator5 = dataset5.make_initializable_iterator()\nnext_element5 = iterator5.get_next()\n\n# Initialize `iterator` with training data.\ntraining_filenames = ["train_data1.csv", \n                      "train_data2.csv"]\n\n# Initialize `iterator` with validation data.\nvalidation_filenames = ["dev_data1.csv"]\n\nwith tf.Session() as sess:\n    # Train 2 epochs. Then validate train set. Then validate dev set.\n    for _ in range(2):     \n        sess.run(iterator5.initializer, feed_dict={filenames: training_filenames})\n        while True:\n            try:\n              features, labels = sess.run(next_element5)\n              # Train...\n              print("(train) features: ")\n              print(features)\n              print("(train) labels: ")\n              print(labels)  \n            except tf.errors.OutOfRangeError:\n              print("Out of range error triggered (looped through training set 1 time)")\n              break\n\n    # Validate (cost, accuracy) on train set\n    print("\\nDone with the first iterator\\n")\n\n    sess.run(iterator5.initializer, feed_dict={filenames: validation_filenames})\n    while True:\n        try:\n          features, labels = sess.run(next_element5)\n          # Validate (cost, accuracy) on dev set\n          print("(dev) features: ")\n          print(features)\n          print("(dev) labels: ")\n          print(labels)\n        except tf.errors.OutOfRangeError:\n          print("Out of range error triggered (looped through dev set 1 time only)")\n          break  \n'
'for column in data.columns:\n    if data[column].dtype == type(object):\n        le = LabelEncoder()\n        data[column] = le.fit_transform(data[column])\n        X = data[column]\n    X = data[column]        #  &lt;------- NOTE: \n    Y = data[target]\n\nX = data.drop(target, 1)\nY = data[target]\n'
"pd.options.display.float_format = '{:.lf}'.format\n\npd.options.display.float_format = '{:.1f}'.format\n"
"from sklearn.neural_network._base import ACTIVATIONS\nACTIVATIONS['relu'](np.matmul(data, mlp.coefs_[0]) + mlp.intercepts_[0]))\n"
'# First transform the sentence to bag-of-words according to the already learnt vocabulary\nx = cout_vect.transform([x])\n\n# Then send the feature vector to the predict\nprint(model.predict(x, batch_size=None, verbose=0, steps=None))\n\ncout_vect.fit_transform([x])\n'
'val[0] / (2 * len(x))\n\nprint(t_check_grad)\n# 1.20853633278e-07\n'
'N, D_in, H, D_out = 128, 1000, 500, 10\ndtype = torch.float32\n'
'labelencoder_dict = {}\nonehotencoder_dict = {}\nX_train = None\nfor i in range(0, X.shape[1]):\n    label_encoder = LabelEncoder()\n    labelencoder_dict[i] = label_encoder\n    feature = label_encoder.fit_transform(X[:,i])\n    feature = feature.reshape(X.shape[0], 1)\n    onehot_encoder = OneHotEncoder(sparse=False)\n    feature = onehot_encoder.fit_transform(feature)\n    onehotencoder_dict[i] = onehot_encoder\n    if X_train is None:\n      X_train = feature\n    else:\n      X_train = np.concatenate((X_train, feature), axis=1)\n\ndef getEncoded(test_data,labelencoder_dict,onehotencoder_dict):\n    test_encoded_x = None\n    for i in range(0,test_data.shape[1]):\n        label_encoder =  labelencoder_dict[i]\n        feature = label_encoder.transform(test_data[:,i])\n        feature = feature.reshape(test_data.shape[0], 1)\n        onehot_encoder = onehotencoder_dict[i]\n        feature = onehot_encoder.transform(feature)\n        if test_encoded_x is None:\n          test_encoded_x = feature\n        else:\n          test_encoded_x = np.concatenate((test_encoded_x, feature), axis=1)\n  return test_encoded_x\n'
"input_img = Input(shape=(28, 28, 1))\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = MaxPooling2D((2, 2), padding='same')(x)\n\nencoder = Model(input_img, encoded)\n\ndecoder_input= Input(shape_equal_to_encoder_output_shape)\n\ndecoder = Conv2D(32, (3, 3), activation='relu', padding='same')(decoder_input)\nx = UpSampling2D((2, 2))(decoder)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n\ndecoder = Model(decoder_input, decoded)\n\nauto_input = Input(shape=(28,28,1))\nencoded = encoder(auto_input)\ndecoded = decoder(encoded)\n\nauto_encoder = Model(auto_input, decoded)\n"
"np.array([np.array(x) for x in xs])\n\nmodel = tf.keras.Sequential([layers.Dense(units=1, input_shape=[2,4])])\nmodel.add(layers.Dense(100))\nmodel.add(layers.Dense(4))\nmodel.compile(optimizer='sgd', loss='mean_squared_error')\n\nxs = np.array([np.random.rand(2,4) for i in range(100)])\nys = np.array([xs[i]**2 for i in range(100)])\n\nmodel.fit(xs, ys, epochs=10, batch_size=1)\n\np = np.random.rand(1,2,4)\nprint(model.predict(p))\n"
'voca = ["java", "spring", "net", "csharp", "python", "numpy", "nodejs", "javascript"]\n\nunits = ["MicrosoftTech", "JavaTech", "Pythoneers", "JavascriptRoots"]\ndesc1 = "Company X is looking for a Java Developer. Requirements: Has worked with Java. 3+ years experience with Java, Maven and Spring."\ndesc2 = "Company Y is looking for a csharp Developer. Requirements: Has wored with csharp. 5+ years experience with csharp, Net."\n\nx_train = []\ny_train = []\n\nx_train.append(bagOfWords(desc1, voca))\ny_train.append(units.index("JavaTech"))\nx_train.append(bagOfWords(desc2, voca))\ny_train.append(units.index("MicrosoftTech"))\n\n[array([3, 1, 0, 0, 0, 0, 0, 0]), array([0, 0, 1, 3, 0, 0, 0, 0])] [1, 0]\n\narray([3, 1, 0, 0, 0, 0, 0, 0]) =&gt; 1 (It means JavaTech)\narray([0, 0, 1, 3, 0, 0, 0, 0]) =&gt; 0 (It means MicrosoftTech)\n\nimport tensorflow as tf\nimport numpy as np\n\nunits = ["MicrosoftTech", "JavaTech", "Pythoneers", "JavascriptRoots"]\nx_train = np.array([[3, 1, 0, 0, 0, 0, 0, 0],\n                [1, 0, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 1, 0, 0, 0, 0],\n                [0, 0, 2, 0, 0, 0, 0, 0],\n                [0, 0, 0, 0, 2, 1, 0, 0],\n                [0, 0, 0, 0, 1, 2, 0, 0],\n                [0, 0, 0, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 0, 1, 0]])\ny_train = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(len(units), activation=tf.nn.softmax)])\nmodel.compile(optimizer=\'adam\',\n                         loss=\'sparse_categorical_crossentropy\',\n                         metrics=[\'accuracy\'])\n\nmodel.fit(x_train, y_train, epochs=50)\n\nnewSample = np.array([[2, 2, 0, 0, 0, 0, 0, 0]])\nprediction = model.predict(newSample)\nprint (prediction)\nprint (units[np.argmax(prediction)])\n\n[[0.96280855 0.00981709 0.0102595  0.01711495]]\nMicrosoftTech\n\nMicrosoftTech : 0.96280855\nJavaTech : 0.00981709\n....\n\nEpoch 1/50\n8/8 [==============================] - 0s 48ms/step - loss: 1.3978 - acc: 0.0000e+00\nEpoch 2/50\n8/8 [==============================] - 0s 356us/step - loss: 1.3618 - acc: 0.1250\nEpoch 3/50\n8/8 [==============================] - 0s 201us/step - loss: 1.3313 - acc: 0.3750\nEpoch 4/50\n8/8 [==============================] - 0s 167us/step - loss: 1.2965 - acc: 0.7500\nEpoch 5/50\n8/8 [==============================] - 0s 139us/step - loss: 1.2643 - acc: 0.8750\n........\n........\nEpoch 45/50\n8/8 [==============================] - 0s 122us/step - loss: 0.3500 - acc: 1.0000\nEpoch 46/50\n8/8 [==============================] - 0s 140us/step - loss: 0.3376 - acc: 1.0000\nEpoch 47/50\n8/8 [==============================] - 0s 134us/step - loss: 0.3257 - acc: 1.0000\nEpoch 48/50\n8/8 [==============================] - 0s 137us/step - loss: 0.3143 - acc: 1.0000\nEpoch 49/50\n8/8 [==============================] - 0s 141us/step - loss: 0.3032 - acc: 1.0000\nEpoch 50/50\n8/8 [==============================] - 0s 177us/step - loss: 0.2925 - acc: 1.0000\n'
'    data = load_files(r"...\\docs",encoding="utf-8")\n    data.to_csv(\'train_data.csv\', encoding = \'utf-8\', index = False)\n\n    train_data = pd.read_csv(\'train_data.csv\')\n\n    def split_dataset(dataset, train_percentage, feature_headers, target_header):\n        train_x, test_x, train_y, test_y = train_test_split(dataset[feature_headers], \n        dataset[target_header], train_size = train_percentage)\n        return train_x, test_x, train_y, test_y\n\n    def random_forest_classifier(features, target):\n        model = RandomForestClassifier(n_estimators = 500, oob_score = True, n_jobs \n        =-1,random_state = 1, min_impurity_decrease = .01)\n        model.fit(features, target)\n        return model\n\n    train_x, test_x, train_y, test_y = split_dataset(train_data, 0.80, \n    train_data.columns[0:24], train_data.columns[-1])\n\n    trained_model = random_forest_classifier(train_x,train_y)\n'
'{\'a\',\'b\'} == {\'b\',\'a\'}\n&gt;&gt;&gt; True\n\nvectorizer = CountVectorizer(stop_words="english",binary=True,lowercase=False,vocabulary=[\'Jscript\',\'.Net\',\'TypeScript\',\'SQL\', \'NodeJS\',\'Angular\',\'Mongo\',\'CSS\',\'Python\',\'PHP\',\'Photoshop\',\'Oracle\',\'Linux\',\'C++\',"Java",\'TeamCity\',\'Frontend\',\'Backend\',\'Full stack\', \'UI Design\', \'Web\',\'Integration\',\'Database design\',\'UX\'])\n\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())\nprint(X.toarray()) \n\n&gt;&gt;&gt; [\'Jscript\', \'.Net\', \'TypeScript\', \'SQL\', \'NodeJS\', \'Angular\', \'Mongo\', \n\'CSS\', \'Python\', \'PHP\', \'Photoshop\', \'Oracle\', \'Linux\', \'C++\', \'Java\', \n\'TeamCity\', \'Frontend\', \'Backend\', \'Full stack\', \'UI Design\', \'Web\', \n\'Integration\', \'Database design\', \'UX\']\n\n&gt;&gt;&gt; [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n     [1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n'
'features_subject = [f for f in vectsubject.get_feature_names() if len(f) &gt; 3]\n\ndfbodyfeatures = gettop5(features_subject)\nprint(dfbodyfeatures)\n\n0      aiding\n1       syria\n2      latest\n3      exchange\n4      helping\n'
"# Calculate cost and plot\ncost = np.zeros(10)\n\nfor k in range(2,10):\n    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol('features')\n    model = kmeans.fit(df)\n    cost[k] = model.summary.trainingCost\n\n# Plot the cost\ndf_cost = pd.DataFrame(cost[2:])\ndf_cost.columns = [&quot;cost&quot;]\nnew_col = [2,3,4,5,6,7,8, 9]\ndf_cost.insert(0, 'cluster', new_col)\n\nimport pylab as pl\npl.plot(df_cost.cluster, df_cost.cost)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()\n"
'class my_base_gradient_boost(BaseGradientBoosting, metaclass=ABCMeta):\n    @abstractmethod\n    def __init__(self, *args):\n       super().__init__(loss=\'my_custom_loss\', *other_args)\n\n    def _check_params(self):\n      try:\n        super()._check_params()\n      except ValueError as e:\n        if str(e) == "Loss \'my_costum_loss\' not supported. ":\n            self.loss_ = self.my_costum_loss\n        else:\n            raise\n\nclass my_classifier(my_base_gradient_boost, GradientBoostingClassifier):\n\n    _SUPPORTED_LOSS = (\'my_costum_loss\')\n\n    @_deprecate_positional_args           \n    def __init__(self, *args):\n        super().__init__(*args)\n'
'from scipy.spatial.distance import cdist\ndm = cdist(df, centroids)\n'
'classifier.fit(X_train_res[train], y_train_res[train])\n\n predict_proba(X_train_res[test])\n'
'from tqdm import tqdm\nfor i in tqdm(range(10000)):\n    ...\n'
'def load_and_preprocess_image_batch(batch_of_paths, batch_of_labels):\n    batch_of_images = tf.map_fn(load_and_preprocess_single_image_from_tensor, batch_of_paths, dtype=tf.float32)\n    return batch_of_images, batch_of_labels\n\nIMG_SIZE = 224\n\n  def preprocess_image(image):\n  image = tf.image.decode_jpeg(image, channels=3)\n  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n  image /= 255.0  # normalize to [0,1] range\n\n  return image\n\ndef load_and_preprocess_single_image_from_tensor(path):\n   image = tf.read_file(path)\n   if image == None:\n    print("Unable to load image at path:" + path )\n   return preprocess_image(image) \n\nimage_and_labels_ds = split_dataset.map(load_and_preprocess_image_batch, num_parallel_calls=AUTOTUNE)\n'
'def map_example(height, width, image_raw, label):\n    image_data = tf.io.decode_raw(image_raw, tf.uint8)\n    image_data = tf.reshape(image_data, [1, height, width])\n    return image_data, label\n\ndef make_dataset(partition):\n    files = tf.data.Dataset.list_files("images_" + partition + "*.tfrecord")\n    dataset = tf.data.TFRecordDataset(files)\n    # dataset = dataset.shuffle(buffer_size=FLAGS.shuffle_buffer_size)\n    dataset = dataset.map(decode_example)\n    dataset = dataset.map(\n        lambda x: map_example(x[\'height\'], x[\'width\'], x[\'image_raw\'], x[\'label\']))\n    # dataset = dataset.batch(batch_size=FLAGS.batch_size)\n    return dataset\n'
"np.random.seed(42)\ndf = pd.DataFrame({'Sex': list('MFMMFFMMFM'),\n                   'Age': np.random.choice([1, 10, 11, 13, np.NaN], 10)},\n                   index=list('ABCDEFGHIJ'))\ndf.groupby('Sex')['Age'].mean()\n\n#Sex\n#F    10.5                # One F row\n#M    11.5                # One M row\n#Name: Age, dtype: float64\n\ndf.groupby('Sex')['Age'].transform('mean')\n\n#A    11.5  # Belonged to M\n#B    10.5  # Belonged to F\n#C    11.5  # Belonged to M\n#D    11.5\n#E    10.5\n#F    10.5\n#G    11.5\n#H    11.5\n#I    10.5\n#J    11.5\n#Name: Age, dtype: float64\n\ndf['Sex_mean'] = df.groupby('Sex')['Age'].transform('mean')\n\n  Sex   Age  Sex_mean\nA   M  13.0      11.5\nB   F   NaN      10.5  # NaN will be filled with 10.5\nC   M  11.0      11.5\nD   M   NaN      11.5  # NaN will be filled with 11.5\nE   F   NaN      10.5  # Nan will be filled with 10.5\nF   F  10.0      10.5\nG   M  11.0      11.5\nH   M  11.0      11.5\nI   F  11.0      10.5\nJ   M   NaN      11.5  # Nan will be filled with 11.5\n"
"model = Sequential()\nweights = model.get_weights()\n\nimport json\njson_log = open('loss_log.json', mode='wt', buffering=1)\njson_logging_callback = LambdaCallback(\n            on_epoch_end=lambda epoch, logs: json_log.write(\n                json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n            on_train_end=lambda logs: json_log.close()\n)\n\nmodel.fit(...,\n          callbacks=[json_logging_callback])\n\nimport json\nfrom keras.callbacks import LambdaCallback\n\njson_log = open('loss_log.json', mode='wt', buffering=1)\njson_logging_callback = LambdaCallback(\n            on_epoch_end=lambda epoch, logs: json_log.write(\n                json.dumps({'epoch': epoch, \n                            'loss': logs['loss'],\n                            'weights': model.get_weights()}) + '\\n'),\n            on_train_end=lambda logs: json_log.close()\n)\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt_adam, \n              metrics=['accuracy'])\n\nmodel.fit_generator(..., callbacks=[json_logging_callback])\n\nmodel.layers[0].get_weights()\n"
"def cmap_transf(image):\n    return cv2.applyColorMap(image.astype(np.uint8), cv2.COLORMAP_JET)\n\ntrain_generator = ImageDataGenerator(preprocessing_function=cmap_transf)\n\nimport cv2\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport os\nimport matplotlib.pyplot as plt\n\nos.makedirs('test', exist_ok=True)\ncv2.imwrite('test/img.jpg', np.random.randint(0, 256, (200, 200, 3)))\n\ndef cmap_transf(image):\n    return cv2.applyColorMap(image.astype(np.uint8), cv2.COLORMAP_JET)\n\ntrain_generator = ImageDataGenerator(preprocessing_function=cmap_transf)\n\nplt.imshow(next(iter(train_generator.\n                     flow_from_directory('.', \n                                         classes=['test'])))[0][0, ...])\n\nos.unlink('test/img.jpg')\nos.rmdir('test')\n"
'1 * 4 * units = kernel\n\nunits * (4 * units) = recurrent kernel\n'
"X_FUTURE = 100\npredictions = np.array([])\nlast = x_test[-1]\nfor i in range(X_FUTURE):\n  curr_prediction = model.predict(np.array([last]))\n  print(curr_prediction)\n  last = np.concatenate([last[1:], curr_prediction])\n  predictions = np.concatenate([predictions, curr_prediction[0]])\npredictions = scaler.inverse_transform([predictions])[0]\nprint(predictions)\n\nimport datetime\nfrom datetime import timedelta\ndicts = []\ncurr_date = data.index[-1]\nfor i in range(X_FUTURE):\n  curr_date = curr_date + timedelta(days=1)\n  dicts.append({'Predictions':predictions[i], &quot;Date&quot;: curr_date})\n\nnew_data = pd.DataFrame(dicts).set_index(&quot;Date&quot;)\n\n#Plot the data\ntrain = data\n#Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.plot(train['Close'])\nplt.plot(new_data['Predictions'])\nplt.legend(['Train', 'Predictions'], loc='lower right')\nplt.show()\n"
"from surprise import Dataset, KNNBaseline, Reader\nimport pandas as pd\nimport numpy as np\nfrom surprise.model_selection import cross_validate\nreader = Reader(rating_scale=(1, 5))\n\ntrain_df = pd.DataFrame({'user_id':np.random.choice(['1','2','3','4'],100),\n                         'item_id':np.random.choice(['101','102','103','104'],100),\n                         'rating':np.random.uniform(1,5,100)})\n\nvalid_df = pd.DataFrame({'user_id':np.random.choice(['1','2','3','4'],100),\n                         'item_id':np.random.choice(['101','102','103','104'],100),\n                         'rating':np.random.uniform(1,5,100)})\n\ntrain_Dataset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)\nvalid_Dataset = Dataset.load_from_df(valid_df[['user_id', 'item_id', 'rating']], reader)\n\ntrain_Dataset = train_Dataset.build_full_trainset()\n\nalgo = KNNBaseline(k=60, min_k=2, sim_options={'name': 'msd', 'user_based': True})\nalgo.fit(train_Dataset)\n\ntestset = [valid_Dataset.df.loc[i].to_list() for i in range(len(valid_Dataset.df))]\nalgo.test(testset)[:2] \n\n[Prediction(uid='2', iid='103', r_ui=3.0224818872683845, est=2.8486558674146125, details={'actual_k': 25, 'was_impossible': False}),\n Prediction(uid='2', iid='103', r_ui=4.609064535195377, est=2.8486558674146125, details={'actual_k': 25, 'was_impossible': False})]\n\nalgo.test([['1','101',None]])\n"
"if self.training:\n    # it's in train mode\nelse:\n    # it's in eval mode\n"
'np.random.seed(10)\ntf.set_random_seed(10)\n'
"from sklearn.datasets import load_breast_cancer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport numpy as np\n\ndata = load_breast_cancer()\nX = pd.DataFrame(data.data, columns=data.feature_names)\ny = pd.Series(data.target)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nfolds = KFold(5, random_state=42)\n\n\nparams = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'objective':'binary',\n        'metric':'auc',\n        }\n\ndata_all = lgb.Dataset(X_train, y_train)\n\nresults = lgb.cv(params, data_all, \n                 folds=folds.split(X_train), \n                 num_boost_round=1000, \n                 early_stopping_rounds=100)\n\nprint('LGBM\\'s cv score: ', results['auc-mean'][-1])\n\nval_scores = []\nfor train_idx, val_idx in folds.split(X_train):\n\n    data_trd = lgb.Dataset(X_train.iloc[train_idx], \n                           y_train.iloc[train_idx], \n                           reference=data_all)\n\n    gbm = lgb.train(params,\n                    data_trd,\n                    num_boost_round=len(results['auc-mean']),\n                    verbose_eval=100)\n\n    val_scores.append(roc_auc_score(y_train.iloc[val_idx], gbm.predict(X_train.iloc[val_idx])))\nprint('Manual score: ', np.mean(np.array(val_scores)))\n\nLGBM's cv score:  0.9914524426410262\nManual score:  0.9914524426410262\n"
'df_cpu = pd.DataFrame(["Q1\'17","Q3\'16"], columns=[\'Launch_Date\'])\n\n&gt;&gt; pd.to_datetime(df_cpu.Launch_Date.str[3:5],format=\'%y\') + np.multiply(pd.offsets.QuarterBegin(startingMonth=1), df_cpu.Launch_Date.str[1:2].values.astype(int)-1)\n0   2017-01-01\n1   2016-07-01\nName: Launch_Date, dtype: datetime64[ns]\n'
'models = [model1, model2, ...]\ndisplaymetrics(code, models, X_train, X_test, X)\n'
'df = pd.DataFrame(columns=["Python", "Scikit-learn", "Pandas", "Fit to Job"], data=np.random.randint(1, 10,size=(400,4)))    \n\nclass LinearRegressionInt(LinearRegression):\n    def predict(self,X):\n        predictions = self._decision_function(X)\n\n        return np.asarray(predictions, dtype=np.int64).ravel()\n... \nlr = LinearRegressionInt()\n...\n\nensemble = VotingClassifier(estimators=[("lr",lr),("Random forest", rfc), ("KNN",knn), ("Naive Bayes", nb), ("SVC",svc)] )\n\ncval_score = cross_val_score(ensemble, X, y, cv=10)\ncval_score\n\narray([ 0.09090909,  0.11904762,  0.17073171,  0.14634146,  0.17073171,\n    0.15384615,  0.07692308,  0.15384615,  0.10810811,  0.08108108])\n'
'model = Net(len(classes))\nmodel.load_state_dict(torch.load(PATH))\nmodel.eval()\n'
"import pandas as pd\nimport tensorflow as tf;\n##Change.\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nimport json;\nimport numpy as np;\n\nclassifier = Sequential()\nclassifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=4))\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics = ['accuracy'])\n\nclassifier.save('/tmp/keras-model.pb', save_format='tf')\n\nINFO:tensorflow:Assets written to: /tmp/keras-model.pb/assets\n"
'observed_sequence = [5, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 4, 4, 4]\nstates = range(20) # experiment with this number\nsymbols = set(observed_clusters)\ntrainer = HiddenMarkovModelTrainer(states, symbols)\nmodel = trainer.train_unsupervised([observed_sequence])\n'
'# alpha is increasing. This is because the updates of Cov are\n# bringing in too much numerical error that is greater than\n# than the remaining correlation with the\n# regressors. Time to bail out\n'
"x = np.arange(1, 200, 20)\nN = x.size\na,b = np.meshrid(x,x)\nit = np.array([b.ravel(),a.ravel(),np.ones(N)])\n\nresult = trained_w[0] * x_surf + trained_w[1] * y_surf + trained_w[2]\n# instead of\n# result = trained_w[0] * it[:,0] + trained_w[1] * it[:,1] + trained_w[2]\n\nplt3d = plt.figure().gca(projection='3d')\nplt3d.hold(True)\nplt3d.scatter(it[:,0], it[:,1], y)\nplt3d.plot_surface(np.reshape(it[:,0],(N,N)), np.reshape(it[:,1],(N,N)), np.reshape(result,(N,N)))\n# where N = x.size still\n"
"clf = OneVsRestClassifier(svm.SVC(kernel='rbf', class_weight='balanced'))\n"
"training_out = [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\ntraining_out = numpy.eye(2)[training_out]\n\ny = tf.placeholder(tf.float32, [None, 2])\nW = tf.Variable(tf.zeros([feature_count, 2]))\nb = tf.Variable(tf.zeros([2]))\n...\nfor i in range(1000):\n    for (item_x, item_y) in zip(new_training_in, training_out):\n        sess.run(opti, feed_dict={x: [item_x], y: [item_y]})\n...\nresults = sess.run(guess, feed_dict={x: new_training_in})[:,1]\n\nimport tensorflow as tf\nimport numpy\nimport matplotlib.pyplot as plt\n\ntraining_in = numpy.array(\n    [[0, 0], [1, 1], [2, 0], [-2, 0], [-1, -1], [-1, 1], [-1.5, 1], [3, 3], [3, 0], [-3, 0], [0, -3], [-1, 3], [1, -2],\n     [-2, -1.5]])\ntraining_out = numpy.array([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])\n\ndef transform_data(x):\n    return [x[0], x[1], x[0] ** 2, x[1] ** 2, x[0] * x[1]]\n\nnew_training_in = numpy.apply_along_axis(transform_data, 1, training_in)\n\nfeature_count = new_training_in.shape[1]\n\nx = tf.placeholder(tf.float32, [None, feature_count])\ny = tf.placeholder(tf.int32, [None])\n\nW = tf.Variable(tf.zeros([feature_count, 2]))\nb = tf.Variable(tf.zeros([2]))\n\nguess = tf.nn.softmax(tf.matmul(x, W) + b)\n\ncost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(tf.matmul(x, W) + b, y))\n\nopti = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n\ninit = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init)\n\nfor i in range(1000):\n    for (item_x, item_y) in zip(new_training_in, training_out):\n        sess.run(opti, feed_dict={x: [item_x], y: [item_y]})\n\nprint(sess.run(W))\nprint(sess.run(b))\n\nplt.plot(training_in[:6, 0], training_in[:6, 1], 'bo')\nplt.plot(training_in[6:, 0], training_in[6:, 1], 'rx')\n\nresults = sess.run(guess, feed_dict={x: new_training_in})\n\nfor i in range(training_in.shape[0]):\n    xx = [training_in[i:, 0]]\n    yy = [training_in[i:, 1]]\n    res = results[i]\n\n    # this always prints `[ 1.]`\n    print(res)\n\n    # uncomment these lines to see the guesses\n    if res[0] == 0:\n        plt.plot(xx, yy, 'c+')\n    else:\n        plt.plot(xx, yy, 'g+')\nplt.show()\n"
'pdd = r2_10.map(lambda x: (x[0], len(x[1]))).reduceByKey(lambda a, b: a + b)\n\ntotal = r2_10.map(lambda x: len(x[1])).sum()\n'
'In [52]: x_train= [5.5997066,4.759385,2.573958,5.586931,3.019574,4.296047,1.586953,0.5997066,3.683957]\n\nIn [53]: linear.fit(x_train, y_train)\n&lt;PYTHON_PATH&gt;\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data\n is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshap\ne(1, -1) if it contains a single sample.\n  DeprecationWarning)\n...\nskipped\n...\nValueError: Found arrays with inconsistent numbers of samples: [1 9]\n\nIn [54]: x_train = np.array(x_train).reshape(len(x_train), -1)\n\nIn [55]: x_train.shape\nOut[55]: (9, 1)\n\nIn [56]: linear.fit(x_train, y_train)\nOut[56]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\n\nIn [60]: x_test = x_test.reshape(-1, 1)\n\nIn [61]: x_test.shape\nOut[61]: (5, 1)\n\nIn [62]: predicted= linear.predict(x_test)\n\nIn [63]: predicted\nOut[63]: array([ 5.7559457,  5.7559457,  5.7559457,  5.7559457,  5.7559457])\n'
"In [111]: import numpy as np\n\nIn [112]: real, fake = 5, 5\n\nIn [113]: rows, cols = 800, 1200\n\nIn [114]: bits = 8\n\nIn [115]: target = np.hstack([np.ones(real), np.zeros(fake)])\n\nIn [116]: np.random.seed(2017)\n\nIn [117]: images = np.random.randint(2**bits, size=(real + fake, rows, cols))\n\nIn [118]: data = images.reshape(images.shape[0], -1)\n\nIn [119]: data\nOut[119]: \narray([[ 59,   9, 198, ..., 189, 201,  38],\n       [150, 251, 145, ...,  95, 214, 175],\n       [156, 212, 220, ..., 179,  63,  48],\n       ..., \n       [ 25,  94, 108, ..., 159, 144, 216],\n       [179, 103, 217, ...,  92, 219,  34],\n       [198, 209, 177, ...,   6,   4, 144]])\n\nIn [120]: data.shape\nOut[120]: (10L, 960000L)\n\nIn [121]: from sklearn import svm\n\nIn [122]: classifier = svm.SVC(gamma=0.001)\n\nIn [123]: classifier.fit(data[:-1], target[:-1])\nOut[123]: \nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)\n\nIn [124]: classifier.predict(np.atleast_2d(data[-1]))\nOut[124]: array([ 1.])\n"
"bst = xgb.train(param, dtrain,num_boost_round=param['n_estimators'])\n"
"loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_,labels=y))\n\nimport numpy as np\nimport tensorflow as tf\n\nsess = tf.InteractiveSession()\n\n# generate data\n\nnp.random.seed(10)\n\ninputs = np.random.normal(size=[1000,150]).astype('float32')*1.5\n\nlabel = np.round(np.random.uniform(low=0,high=1,size=[1000,1])*0.8)\nreverse_label = 1-label\nlabels = np.append(label,reverse_label,1)\n\n# parameters\n\nlearn_rate = 0.002\nepochs = 400\nn_input = 150\nn_hidden = 60\nn_output = 2\n\n# set weights/biases\n\nx = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_output])\n\nb0 = tf.Variable(tf.truncated_normal([n_hidden],stddev=0.2,seed=0))\nb1 = tf.Variable(tf.truncated_normal([n_output],stddev=0.2,seed=0))\n\nw0 = tf.Variable(tf.truncated_normal([n_input,n_hidden],stddev=0.2,seed=0))\nw1 = tf.Variable(tf.truncated_normal([n_hidden,n_output],stddev=0.2,seed=0))\n\n# step function\n\ndef returnPred(x,w0,w1,b0,b1):\n\n    z1 = tf.add(tf.matmul(x, w0), b0)\n    a2 = tf.nn.relu(z1)\n\n    z2 = tf.add(tf.matmul(a2, w1), b1)\n    h = tf.nn.relu(z2)\n\n    return h  #return the first response vector from the \n\ny_ = returnPred(x,w0,w1,b0,b1) # predict operation\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_,labels=y)) # calculate loss between prediction and actual\nmodel = tf.train.AdamOptimizer(learning_rate=learn_rate).minimize(loss) # apply gradient descent based on loss\n\n\ninit = tf.global_variables_initializer()\ntf.Session = sess\nsess.run(init) #initialize graph\n\nfor step in range(0,epochs):\n    sess.run([model,loss],feed_dict={x: inputs, y: labels }) #train model\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) \naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(sess.run(accuracy, feed_dict={x: inputs, y: labels})) # print accuracy\n"
'from sklearn.model_selection import PredefinedSplit\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\n\n#This is what you need\ntest_fold = [0, 1, -1, 1]\n\nps = PredefinedSplit(test_fold)\nps.get_n_splits()\n#OUTPUT\n2\n\nfor train_index, test_index in ps.split():\n   print("TRAIN:", train_index, "TEST:", test_index)\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n#OUTPUT\nTRAIN: [1 2 3] TEST: [0]\nTRAIN: [0 2] TEST: [1 3]\n\nmy_test_fold = []\n\n# put -1 here, so they will be in training set\nfor i in range(len(X_train)):\n    my_test_fold.append(-1)\n\n# for all greater indices, assign 0, so they will be put in test set\nfor i in range(len(X_test)):\n    my_test_fold.append(0)\n\n#Combine the X_train and X_test into one array:\nimport numpy as np\n\nclf = RandomizedSearchCV( ...    cv = PredefinedSplit(test_fold=my_test_fold))\nclf.fit(np.concatenate((X_train, X_test), axis=0), np.concatenate((y_train, y_test), axis=0))\n'
'from sklearn.metrics.pairwise import pairwise_distances\ndist_matrix = pairwise_distances(X)\n\nfor point in unclustered_points:\n    distances = []\n    for cluster in clusters:\n        distance = dist_matrix[point, cluster].min()  # Single linkage\n        distances.append(distance)\n    print("The cluster for {} is {}".format(point, cluster)\n'
'model.add(Conv1D(filters=n_filter,\n         kernel_size=input_filter_length,\n         strides=1,\n         activation=\'relu\',\n         input_shape=(20000,1)))\nmodel.add(BatchNormalization())\nmodel.add(GlobalMaxPooling1D(pool_size=4, strides=None))\n\nmodel.add(Dense(1))\nmodel.add(Activation("sigmoid"))\n\nmodel.add(Conv1D(filters=n_filter,\n         kernel_size=input_filter_length,\n         strides=1,\n         activation=\'relu\',\n         input_shape=(20000,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=4, strides=None))\nmodel.add(Flatten())\n\nmodel.add(Dense(1))\nmodel.add(Activation("sigmoid"))\n\nmodel.add(Conv1D(filters=n_filter,\n         kernel_size=input_filter_length,\n         strides=1,\n         activation=\'relu\',\n         input_shape=(20000,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=4, strides=None))\nmodel.add(SimpleRNN(10, return_sequences=False))\n\nmodel.add(Dense(1))\nmodel.add(Activation("sigmoid"))\n'
'out = my_new_features_dataset # shape N x 2\nfor i in range(0, num_rows//chunk_size):\n    out[i*chunk_size:(i+1) * chunk_size] = ipca.transform(features[i*chunk_size : (i+1)*chunk_size])\n\n&gt;&gt;&gt; print(ipca.explained_variance_ratio_.cumsum())\n[ 0.32047581  0.59549787  0.80178824  0.932976    1.        ]\n\nipca = IncrementalPCA(n_components=features.shape[1])\n\nk = np.argmax(ipca.explained_variance_ratio_.cumsum() &gt; 0.9)\n\ncs = chunk_size\nout = my_new_features_dataset # shape N x k\nfor i in range(0, num_rows//chunk_size):\n    out[i*cs:(i+1)*cs] = ipca.transform(features[i*cs:(i+1)*cs])[:, :k]\n'
'if normalize:\n    cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n    print("Normalized confusion matrix")\nelse:\n    print(\'Confusion matrix, without normalization\')\n\ncm = np.around(cm, decimals=3)\n'
'def _get_feature_importances(estimator):\n    """Retrieve or aggregate feature importances from estimator"""\n    importances = getattr(estimator, "feature_importances_", None)\n\n    if importances is None and hasattr(estimator, "coef_"):\n        if estimator.coef_.ndim == 1:\n            importances = np.abs(estimator.coef_)\n\n        else:\n            importances = np.sum(np.abs(estimator.coef_), axis=0)\n\n    elif importances is None:\n        raise ValueError(\n            "The underlying estimator %s has no `coef_` or "\n            "`feature_importances_` attribute. Either pass a fitted estimator"\n            " to SelectFromModel or call fit before calling transform."\n            % estimator.__class__.__name__)\n\n    return importances\n\nsfm = SelectFromModel(LassoCV(), 0.25)\nsfm.fit(X, y)\nprint(sfm.estimator_.coef_)  # print "importance" scores\n'
"import numpy as np\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n\nnp.random.seed(7)\n\n\ndef inverse_lda(lda, C):\n    c = np.flatnonzero(model.classes_ == C)\n    return model.means_[c]\n\n\nAB, C = make_blobs(n_samples=333, n_features=2, centers=2)  # toy data\nA, B = AB.T\n\nplt.scatter(A, B, c=C, alpha=0.5)\nplt.xlabel('A')\nplt.ylabel('B')\n\nmodel = LDA(store_covariance=True).fit(AB, C)\n\n# reconstruct A and B for C=[0, 1]\nABout = inverse_lda(model, C=[0, 1])\n\nplt.plot(ABout[0, 0], ABout[0, 1], 'o', label='C=0')\nplt.plot(ABout[1, 0], ABout[1, 1], 'o', label='C=1')\nplt.legend()\n"
"df1 = df.pivot(index='Page Name',columns='UserName',values='Count of Page Views by The User')\n\ndf1 = df.set_index(['Page Name','UserName'])['Count of Page Views by The User'].unstack()\n\nprint (df1)\nUserName   David  Michael  Mike  Minerva\nPage Name                               \nBuy          2.0      NaN  12.0      NaN\nHome        12.0   1112.0   NaN     56.0\n"
'import sklearn.cluster\nfrom sklearn.manifold import SpectralEmbedding\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nsims =  np.array([[0, 17, 10, 32, 32], [18, 0, 6, 20, 15], [10, 8, 0, 20, 21], [30, 16, 20, 0, 17], [30, 15, 21, 17, 0]])\n\naffprop = sklearn.cluster.AffinityPropagation(affinity="precomputed", damping=0.5)\naffprop.fit(sims)\n\ncluster_centers_indices = affprop.cluster_centers_indices_\nprint(cluster_centers_indices)\nlabels = affprop.labels_\nn_clusters_ = len(cluster_centers_indices)\nprint(n_clusters_)\n\nse = SpectralEmbedding(n_components=2, affinity=\'precomputed\')\nX = se.fit_transform(sims)\n\nplt.close(\'all\')\nplt.figure(1)\nplt.clf()\n\ncolors = cycle(\'bgrcmykbgrcmykbgrcmykbgrcmyk\')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + \'.\')\n    plt.plot(cluster_center[0], cluster_center[1], \'o\', markerfacecolor=col,\n             markeredgecolor=\'k\', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title(\'Estimated number of clusters: %d\' % n_clusters_)\nplt.show()       \n'
"(boxes, scores, classes, num) = sess.run(\n          [detection_boxes, detection_scores, detection_classes, num_detections],\n          feed_dict={image_tensor: image_np_expanded})\n\nfor class, score in zip(classes, scores):\n    print(class, ':', score)\n"
'def loop_fn_initial():\n    init_elements_finished = (0 &gt;= encoder_inputs_length)\n    init_input = tf.zeros([batch_size, input_embedding_size], dtype=tf.float32)\n    init_cell_state = cell.zero_state(batch_size, tf.float32)\n    init_cell_output = None\n    init_loop_state = None\n    return (init_elements_finished, init_input,\n            init_cell_state, init_cell_output, init_loop_state)\n'
' plt.barh(*zip(* (sorted_labels[:5] +sorted_labels[-5:])))\n\n labels, values = zip(*(sorted_labels[:5] +sorted_labels[-5:]))\n    plt.barh(range(len(labels)),values)\n    plt.yticks(range(len(values)),values)\n    plt.show()\n'
'&gt;&gt;&gt; A = np.matrix([3,3,3])\n&gt;&gt;&gt; B = np.matrix([[1,1,1], [2,2,2]])\n&gt;&gt;&gt; A-B\nmatrix([[2, 2, 2],\n        [1, 1, 1]])\n\ntheta = theta - gradient.T\n'
"model = Model(rn50.input, \n        Dense(len(possible_labels), activation='softmax')\n        (rn50.get_layer(layer_name).output))\n"
'logits = tf.nn.xw_plus_b(last_layer, self.output_w, self.output_b)\nfloss = tf.losses.sigmoid_cross_entropy\n#floss = tf.nn.sigmoid_cross_entropy_with_logits\nloss = floss(self.targets_input, logits, weights=1.0, label_smoothing=0,\n             scope="sigmoid_cross_entropy", loss_collection=tf.GraphKeys.LOSSES)\n'
'def __sigmoid_derivative(x):\n    return sigmoid(x) * (1 - sigmoid(x))\n\ndef __sigmoid_derivative(x):\n    return x * (1 - x)\n'
'from nltk import word_tokenize\nfrom keras.preprocessing import sequence\nword2index = imdb.get_word_index()\ntest=[]\nfor word in word_tokenize( "i love this movie"):\n     test.append(word2index[word])\n\ntest=sequence.pad_sequences([test],maxlen=max_review_length)\nmodel.predict(test)\n'
'from sklearn import model_selection, svm\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nkfold = model_selection.KFold(n_splits=10, random_state=42)\nmodel= svm.SVC(kernel=\'linear\', C=1)\nresults = model_selection.cross_val_score(estimator=model,\n                                              X=iris.data,\n                                              y=iris.target,\n                                              cv=kfold,\n                                              scoring="accuracy")  # change \n'
"features = np.reshape(features, (features.shape[0], 1, features.shape[1]))\n\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, features.shape[1])))  \nmodel.add(Dense(1))    \nmodel.summary()\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n"
"class ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, kernel_size=5) \n        self.conv2 = nn.Conv2d(6, 16, kernel_size=1)  # kernel_size 5----&gt; 1\n        self.fc1   = nn.Linear(384, 120)              # FC NodeCount864--&gt; 384\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, num_classes)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out                        # Don't forget to return the output\n"
'decoder_input = Input(shape=(coded_size,))\nnext_input = decoder_input\n\n# get the decoder layers and apply them consecutively\nfor layer in autoencoder.layers[-2:]:\n    next_input = layer(next_input)\n\ndecoder = Model(inputs=decoder_input, outputs=next_input)\n'
'import tensorflow as tf\nimport numpy as np\nnp.random.seed(1)\n\nx = tf.placeholder(dtype=tf.float32, shape=(None, 1), name="x")\ny = tf.placeholder(dtype=tf.float32, shape=(None), name="y")\n\nW = tf.get_variable(name="W", shape=(1, 1), dtype=tf.float32, initializer=tf.constant_initializer(0))\nb = tf.get_variable(name="b", shape=1, dtype=tf.float32, initializer=tf.constant_initializer(0))\nz = tf.matmul(x, W) + b\n\nerror = tf.square(z - y)\nobj = tf.reduce_mean(error, name="obj")\n\nopt = tf.train.MomentumOptimizer(learning_rate=0.025, momentum=0.9)\ngrads = opt.compute_gradients(obj)\ntrain_step = opt.apply_gradients(grads)\n\nN = 100\nx_np = np.random.randn(N).reshape(-1, 1)\ny_np = 2*x_np + 3 + np.random.randn(N)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for i in range(2):\n        res = sess.run([obj, W, b, train_step], feed_dict={x: x_np, y: y_np})\n        print(\'MSE: {}, W: {}, b: {}\'.format(res[0], res[1][0, 0], res[2][0]))\n\nMSE: 14.721437454223633, W: 0.0, b: 0.0\nMSE: 13.372591018676758, W: 0.08826743811368942, b: 0.1636980175971985\n'
'import tensorflow as tf\nimport tensorflow_probability as tfp\ntfd = tfp.distributions\n\ntf.enable_eager_execution()\n\nshift = tf.Variable(1., dtype=tf.float32)\ndef f():\n  myBij = tfp.bijectors.Affine(shift=shift)\n\n  # Normal distribution centered in zero, then shifted to 1 using the bijection\n  myDistr = tfd.TransformedDistribution(\n            distribution=tfd.Normal(loc=0., scale=1.),\n            bijector=myBij,\n            name="test")\n\n  # 2 samples of a normal centered at 1:\n  y = myDistr.sample(2)\n  # 2 samples of a normal centered at 0, obtained using inverse\n  # transform of myBij:\n  x = myBij.inverse(y)\n  return x, y\nx, y = f()\nshift.assign(2.)\ngx, _ = f()\n'
'tensor = ...\nm = K.mean(tensor)\n...\n\ntensor = ...\nm = tf.mean(tensor)\n...\n\ntensor = ...\nres = K.some_submodule.svd(tensor)\n...\n\ntensor = ...\nres = tensorflow.python.ops.linalg_ops.svd(tensor)\n...\n'
"from keras import backend as K\n\n# an input layer to feed labels\ny_true = Input(shape=labels_shape)\n# compute loss based on model's output and true labels\nce = K.mean(K.categorical_crossentropy(y_true, model.output))\n# compute gradient of loss with respect to inputs\ngrad_ce = K.gradients(ce, model.inputs)\n# create a function to be able to run this computation graph\nfunc = K.function(model.inputs + [y_true], grad_ce)\n\n# usage\noutput = func([model_input_array(s), true_labels])\n"
"def welcome(x):\n    print('Hello world!')\n\nif __name__ == '__main__':\n    a = list(map(welcome, [1,2]))\n\nHello world!\nHello world!\n"
"X[model.support_]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\n\nsvc = SVC(kernel='linear', C=0.025)\nX, y = make_classification(n_samples=500, n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nX = StandardScaler().fit_transform(X)\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=.4, random_state=42)\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\nfig, ax = plt.subplots(figsize=(18,12))\nax.scatter(X_tr[:, 0], X_tr[:, 1], c=y_tr, cmap=cm_bright)\nsvc.fit(X_tr, y_tr)\ny_tr[svc.support_]\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1])\nfig2, ax2 = plt.subplots(figsize=(18,12))\nax2.scatter(X_tr[:, 0], X_tr[:, 1], c=y_tr, cmap=cm_bright)\nax2.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1])    \nfig3, ax3 = plt.subplots(figsize=(18,12))\nax3.scatter(X_tr[:, 0], X_tr[:, 1], c=y_tr, cmap=cm_bright)\nax3.scatter(svc.support_vectors_[:, 0], svc.support_vectors_[:, 1], c=y_tr[svc.support_], cmap=cm_bright)\n"
'# Input data\ntorch.Size([64, 1, 96, 96])\nx = F.relu(F.max_pool2d(self.conv1(x), 2))\ntorch.Size([64, 32, 48, 48])\nx = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\ntorch.Size([64, 64, 24, 24])\nx = x.view(-1, 64 * 96 * 96)\ntorch.Size([4, 589824])\nx = F.relu(self.fc1(x))\ntorch.Size([4, 100])\nx = F.dropout(x, training=self.training)\ntorch.Size([4, 100])\nx = self.fc2(x)\ntorch.Size([4, 30])\nreturn F.log_softmax(x, dim=1)    \ntorch.Size([4, 30])\n\ntarget = target.view(64, -1) # gives 64X30 ie, 30 values per channel\nloss = F.nll_loss(x, torch.max(t, 1)[1]) # takes max amongst the 30 values as class label\n'
"a=pd.DataFrame(np.concatenate([np.zeros(3),np.ones(3)]) ).astype('int').astype('category')\nfrom keras.utils import to_categorical\nto_categorical(a, 2)\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.]], dtype=float32)\n"
"import torch\nimport torch.nn as nn\nfrom torch.utils import data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# for reproducibility\ntorch.manual_seed(0)\nnp.random.seed(0)\n\nclass Dataset(data.Dataset):\n\n    def __init__(self, init, end, n):\n\n        self.n = n\n        self.x = np.random.rand(self.n, 1) * (end - init) + init\n        self.y = np.sin(self.x)\n\n    def __len__(self):\n\n        return self.n\n\n    def __getitem__(self, idx):\n\n        x = self.x[idx, np.newaxis]\n        y = self.y[idx, np.newaxis]\n\n        return torch.Tensor(x), torch.Tensor(y)\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.model = nn.Sequential(\n        nn.Linear(1, 20),\n        nn.Sigmoid(),\n        nn.Linear(20, 50),\n        nn.Sigmoid(),\n        nn.Linear(50, 50),\n        nn.Sigmoid(),\n        nn.Linear(50, 1)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\ndef train(net, trainloader, valloader, n_epochs):\n\n    loss = nn.MSELoss()\n    # Switch the two following lines and run the code\n    # optimizer = torch.optim.SGD(net.parameters(), lr = 0.0001)\n    optimizer = torch.optim.Adam(net.parameters())\n\n    for epoch in range(n_epochs):\n\n        net.train()\n        for x, y in trainloader:\n            optimizer.zero_grad()\n            outputs = net(x).view(-1)\n            error   = loss(outputs, y)\n            error.backward()\n            optimizer.step()\n\n        net.eval()\n        total_loss = 0\n        for x, y in valloader:\n            outputs = net(x)\n            error   = loss(outputs, y)\n            total_loss += error.data\n\n        print('Val loss for epoch', epoch, 'is', total_loss / len(valloader) )    \n\n    net.eval()\n\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n\n    def plot_result(ax, dataloader):\n        out, xx, yy = [], [], []\n        for x, y in dataloader:\n            out.append(net(x))\n            xx.append(x)\n            yy.append(y)\n        out = torch.cat(out, dim=0).detach().numpy().reshape(-1)\n        xx = torch.cat(xx, dim=0).numpy().reshape(-1)\n        yy = torch.cat(yy, dim=0).numpy().reshape(-1)\n        ax.scatter(xx, yy, facecolor='green')\n        ax.scatter(xx, out, facecolor='red')\n        xx = np.linspace(0.0, 3.14159*2, 1000)\n        ax.plot(xx, np.sin(xx), color='green')\n\n    plot_result(ax1, trainloader)\n    plot_result(ax2, valloader)\n    plt.show()\n\n\ntrain_dataset = Dataset(0.0, 3.14159*2, 100)\nval_dataset = Dataset(0.0, 3.14159*2, 30)\n\nparams = {'batch_size': 1,\n          'shuffle': True,\n          'num_workers': 4}\n\ntrainloader = data.DataLoader(train_dataset, **params)\nvalloader = data.DataLoader(val_dataset, **params)\n\nnet = Net()\nlosslist = train(net, trainloader, valloader, n_epochs = 100)        \n"
'!pip install statsmodels==0.9.0rc1\n'
'[[1, 2, 3],\n[4, 5, 6],   ==&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9]\n[7, 8, 9]]\n'
"df['Team No.'] = dummies.cumsum(axis=1).ne(1).sum(axis=1)\n\ndf = pd.DataFrame({'Toss winner': ['Chennai', 'Mumbai', 'Rajasthan', 'Banglore', 'Hyderabad']})\ndummies = pd.get_dummies(df['Toss winner'])\ndf['Team No.'] = dummies.cumsum(axis=1).ne(1).sum(axis=1)\n\n# print(dummies)\n   Banglore  Chennai  Hyderabad  Mumbai  Rajasthan\n0         0        1          0       0          0\n1         0        0          0       1          0\n2         0        0          0       0          1\n3         1        0          0       0          0\n4         0        0          1       0          0\n\n# print (df)\n  Toss winner  Team No.\n0     Chennai         1\n1      Mumbai         3\n2   Rajasthan         4\n3    Banglore         0\n4   Hyderabad         2\n"
"from sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\nX1, y1 = make_classification(random_state=1)\nX2, y2 = make_classification(random_state=2)\n\n\nclf1 = LogisticRegression(random_state=1)\nclf2 = LogisticRegression(random_state=2)\nclf3 = LogisticRegression(random_state=3)\n\n\nvoting = VotingClassifier(estimators=[\n    ('a', clf1),\n    ('b', clf2),\n    ('c', clf3),\n])\n\n# fit all\nvoting = voting.fit(X1,y1)\n\n# fit individual one\nvoting.estimators_[-1].fit(X2,y2)\nvoting.predict(X2)\n\nfor e in voting.estimators:\n    print(e)\n\n('a', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=1, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False))\n('b', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=2, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False))\n('c', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=3, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False))\n\nfor e in voting.estimators_:\n    print(e)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=1, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=2, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='l2',\n                   random_state=3, solver='warn', tol=0.0001, verbose=0,\n                   warm_start=False)\n"
'import numpy as np\nfrom skimage.util.shape import view_as_windows\n\n## create dummy data\nn_sample = 2000\nn_features = 5\n\nX = np.tile(np.arange(n_sample), (n_features,1)).T\n\nX_train = X[:int(n_sample*0.8)]\nX_test = X[int(n_sample*0.8):]\n\n## create windows\nlook_back = 90\nX_train = view_as_windows(X_train, (look_back,n_features), step=1)[:-1,0]\nX_test = view_as_windows(X_test, (look_back,n_features), step=1)[:-1,0]\nprint(X_train.shape, X_test.shape) # (1510, 90, 5) (310, 90, 5)\n'
"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=input_shape))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten()) #&lt;========================\nmodel.add(Dense(64))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n"
'x = np.random.rand(1, 100)\n\nx = np.random.rand(100)\n'
'dW2 = ( 1 / M ) * np.dot((A2 - Y) , A1.T)\n'
"interactions.set_index(['ItemID','UserID'])\\\n            .unstack().fillna(0).astype(int).stack()\\\n            .reset_index()\n\n     ItemID UserID  Ratings\n0    136771    176        0\n1    136771    383        0\n2    136771    67E        3\n3    136771    70C        0\n4    136771    A74        0\n5    341785    176        0\n6    341785    383        0\n7    341785    67E        0\n8    341785    70C        4\n9    341785    A74        0\n10  1172952    176        0\n....\n"
'from sklearn import tree\nX = [[0, 0], [1, 1]]\nY = [0, 1]\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(X, Y)\n\nprint(clf.predict(X))\n# [0 1]\n\ntype(clf.predict(X))\n# numpy.ndarray\n\npred = clf.predict(X)\n[",".join(item) for item in pred.astype(str)]\n# [\'0\', \'1\']\n'
'from pattern.es import parsetree\nparsetree("buena", lemmata=True)\n# Returns [Sentence(\'buena/JJ/B-ADJP/O/bueno\')]\n'
'[[1.0, 1.0], \n [2.0, 2.0]]\n\n[1,\n 2]\n\n[[1.0, 1.0],\n [1.0, 1.0], \n [1.0, 1.0],\n [1.0, 1.0], \n [1.0, 1.0],\n [1.0, 1.0]]\n'
"return getattr(d.feed, 'title', 'Unknown title'), wc\n"
'from sklearn.feature_extraction.text import TfidfVectorizer\nall_lines = ["This is an example doc", "Another short example document .", "Just a third example"]\n\ntf = TfidfVectorizer(analyzer=\'word\')\ntfidf_matrix =  tf.fit_transform(all_lines)\nquery_string = "This is a short example string"\nprint "Query String:"\nprint tf.transform([query_string])\nprint "Example doc:"\nprint tf.transform(["This is a short example doc"])\n\nQuery String:\n  (0, 9)        0.546454011634\n  (0, 7)        0.546454011634\n  (0, 5)        0.546454011634\n  (0, 4)        0.32274454218\nExample doc:\n  (0, 9)        0.479527938029\n  (0, 7)        0.479527938029\n  (0, 5)        0.479527938029\n  (0, 4)        0.283216924987\n  (0, 2)        0.479527938029\n'
"DATA SETS\nlabel   Karpathy's      yours\n  1     [1.2, 0.7]      [1.2, 0.7]\n -1     [-0.3, -0.5]    [-0.3, 0.5]\n  1     [3.0, 0.1]      [-3, -1]\n -1     [-0.1, -1.0]    [0.1, 1.0]\n -1     [-1.0, 1.1]     [3.0, 1.1]\n  1     [2.1, -3]       [2.1, -3]\n"
"import arff\n\nX = []\ny = []\n\nfor row in arff.load('cpu.arff'):   \n    X.append(row[:-1])\n    y.append(row[-1])\n"
'accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))\nprint(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n\nprint(sess.run(y, feed_dict={x: mnist.test.images}))\n'
"import pickle\npickle.dump(model, open('./output.bin', 'w'))\n"
'from nltk import compat\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\n\n_vectorizer = DictVectorizer(dtype=float, sparse=True)\n\ndef prepare_scikit_x_and_y(labeled_featuresets):\n    X, y = list(compat.izip(*labeled_featuresets))\n    X = _vectorizer.fit_transform(X)\n\n    set_of_labels = []\n    for label in y:\n        set_of_labels.append(set(label))\n\n    y = self.mlb.fit_transform(set_of_labels)\n\n    return X, y\n\ndef train_classifier(labeled_featuresets):\n    X, y = prepare_scikit_x_and_y(labeled_featuresets)\n    classifier.fit(X, y)\n\ntraining_set = [\n    [{"car": True, ...}, ["Label 1"]],\n    [{"car": False, ...}, ["Label 2"]],\n    ...\n    [{"car": False, ...}, ["Label 1"]],\n]\n\n\novr = OneVsRestClassifier(MultinomialNB())\novr.train(training_set)\n'
'if(classes[max_index] == y[q]):\n    corr += 1\n'
"estimator = RandomForestRegressor(oob_score=True,n_estimators=10,max_features='auto')\nestimator.fit(tarning_data,traning_target)\ntree1 = estimator.estimators_[0]\nleftChilds = tree1.tree_.children_left # array of left children\nrightChilds = tree1.tree_.children_right #array of right children\n"
'# Construct an initial dictionary object, note partial fit will be done later inside\n# the loop, here we only specify that for partial_fit it needs just to run just 1 \n# epoch (n_iter=1) with batch_size=batch_size on the current batch provided \n# (otherwise by default it can run upto 1000 iterations with batch_size=3 for a \n# single partial_fit() and on each of the batches, which makes the a single run of \n# partial_fit() very slow. Since we control the epoch on our own and it restarts \n# when all the batches are done, we need not provide more than 1 iteration here. \n# This will make the code to execute fast.\n\nbatch_size = 128 # e.g.,\ndico = MiniBatchDictionaryLearning(n_components = 100,\n                                   alpha = 2,\n                                   n_iter = 1,  # epoch per partial_fit()\n                                   batch_size = batch_size,\n                                   transform_algorithm=\'omp\')\n\nn_updates = 0\nfor epoch in range(n_epochs):\n    for i in range(n_batches):\n        batch = patches[i * batch_size:(i + 1) * batch_size]\n        dico.partial_fit(batch)\n        n_updates += 1\n        if n_updates % 100 == 0:\n            img_r = reconstruct_image(dico, dico.components_, patches)\n            err = np.sqrt(np.sum((img - img_r)**2))\n            print("[epoch #%02d] Err = %s" % (epoch, err))\n'
"import pandas as pd\npd.DataFrame(pca.components_, columns=['PC1', 'PC2', 'PC3', 'PC4'], index=['F1', 'F2', 'F3', 'F4'])\n#         PC1       PC2       PC3       PC4\n#F1 -0.718162 -0.682117 -0.081261 -0.111158\n#F2  0.617457 -0.659969  0.372151 -0.211403\n#F3  0.292697 -0.159279 -0.909427 -0.248801\n#F4 -0.131601  0.271638  0.166864 -0.938643\n\nX = pca.transform(X)\nprint(np.var(X, axis=0))\n#[ 0.31612993  0.02264438  0.01811324  0.00253745]\nprint(pca.explained_variance_)\n#[ 0.31612993  0.02264438  0.01811324  0.00253745]\n"
"shape of input = [batch, in_height, in_width, in_channels]\nshape of filter = [filter_height, filter_width, in_channels, out_channels]\n\nValueError: Dimensions must be equal, but are 1 and 3 for 'Conv2D_94' (op: 'Conv2D') with \ninput shapes: [?,100,150,1], [3,3,3,32].\n"
'x_values = np.linspace(2, 10, 100)\nx_test = np.reshape(x_values, (100,1))\n\nC = list(range(1, 11))\nlabels = map(str, C)\nfor i in range(len(C)): \n    lgs = LogisticRegression(C = C[i]) # pass a value for the regularization parameter C\n    lgs.fit(lengths, is_setosa)\n    y_values = lgs.predict_proba(x_test)[:,1] # use this function to compute probability directly\n    plt.plot(x_values, y_values, label=labels[i])\n\nplt.scatter(lengths, is_setosa, c=\'r\', s=2)\nplt.xlabel("Sepal Length")\nplt.ylabel("Probability is Setosa")\nplt.legend()\nplt.show()\n'
'optimizer = GradientDescentOptimizer(0.01)\noptimizer.minimize(loss,[w1,w2])\n'
"import numpy as np\nfrom skimage import io\n\nimg = io.imread('https://i.stack.imgur.com/TFOv7.png')\n\nrows, cols, bands = img.shape\nclasses = {'building': 0, 'vegetation': 1, 'water': 2}\nn_classes = len(classes)\npalette = np.uint8([[255, 0, 0], [0, 255, 0], [0, 0, 255]])\n\nfrom sklearn.cluster import KMeans\n\nX = img.reshape(rows*cols, bands)\nkmeans = KMeans(n_clusters=n_classes, random_state=3).fit(X)\nunsupervised = kmeans.labels_.reshape(rows, cols)\n\nio.imshow(palette[unsupervised])\n\nsupervised = n_classes*np.ones(shape=(rows, cols), dtype=np.int)\n\nsupervised[200:220, 150:170] = classes['building']\nsupervised[40:60, 40:60] = classes['vegetation']\nsupervised[100:120, 200:220] = classes['water']\n\ny = supervised.ravel()\ntrain = np.flatnonzero(supervised &lt; n_classes)\ntest = np.flatnonzero(supervised == n_classes)\n\nfrom sklearn.svm import SVC\n\nclf = SVC(gamma='auto')\nclf.fit(X[train], y[train])\ny[test] = clf.predict(X[test])\nsupervised = y.reshape(rows, cols)\n\nio.imshow(palette[supervised])\n"
'import caffe\nfrom sklearn import metrics\n# load the net with trained weights\nnet = caffe.Net(\'/path/to/deploy.prototxt\', \'/path/to/weights.caffemodel\', caffe.TEST)\ny_score = []  \ny_true = []\nfor i in xrange(N): # assuming you have N validation samples\n    x_i = ... # get i-th validation sample\n    y_true.append( y_i )  # y_i is 0 or 1 the TRUE label of x_i\n    out = net.forward( data=x_i )  # get prediction for x_i\n    y_score.append( out[\'prob\'][1] ) # get score for "1" class\n# once you have N y_score and y_true values\nfpr, tpr, thresholds = metrics.roc_curve(y_true, y_score, pos_label=1)\nauc = metrics.roc_auc_score(y_true, y_scores)\n'
"model.add(k.layers.convolutional.Convolution1D(32,2, input_shape = (32, 1)))\nmodel.add(k.layers.Activation('relu'))\n\nmodel.add(k.layers.convolutional.Convolution1D(32,2))\nmodel.add(k.layers.Activation('relu'))\n\nmodel.add(k.layers.convolutional.Convolution1D(32,2))\nmodel.add(k.layers.Activation('relu'))\n\nmodel.add(k.layers.convolutional.Convolution1D(32,2))\nmodel.add(k.layers.Activation('relu'))\n\nmodel.add(k.layers.core.Dense(32))\nmodel.add(k.layers.Activation('sigmoid'))\n\nmodel.add(k.layers.core.Dense(1))\nmodel.add(k.layers.Activation('sigmoid'))\n\nmodel = k.models.Sequential()\nmodel.add(k.layers.convolutional.Convolution1D(32,2, input_shape = (32, 1)))\nmodel.add(k.layers.Activation('relu'))\n\nmodel.add(k.layers.convolutional.Convolution1D(32,2))\nmodel.add(k.layers.Activation('relu'))\n\nmodel.add(k.layers.convolutional.Convolution1D(32,2))\nmodel.add(k.layers.Activation('relu'))\n\n# Only use one filter so that the output will be a sequence of 28 values, not a matrix.\nmodel.add(k.layers.convolutional.Convolution1D(1,2))\nmodel.add(k.layers.Activation('relu'))\n\n# Change the shape from (None, 28, 1) to (None, 28)\nmodel.add(k.layers.core.Flatten())\n\n# Only one neuron as output to get the binary target.\nmodel.add(k.layers.core.Dense(1))\nmodel.add(k.layers.Activation('sigmoid'))\n"
"callbacks=([checkpointer]))\n\ncallbacks=[checkpointer]\n\ncheckpointer=ModelCheckpoint(filepath_for_w, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1),\n"
'&gt;&gt;&gt; x = np.array([3,5,7])\n&gt;&gt;&gt; np.mean(x)\n5.0\n&gt;&gt;&gt; x - np.mean(x)\narray([-2.,  0.,  2.])\n'
'mask = sum_Ta &lt; (percent * np.ones((250,1)))\nES = np.zeros((250, 1000))\nES[mask] = sum_Ta[mask]\n'
"model = sm.Logit(y_data, x_data)\nmodel_fit = model.fit()\n\ncov = model_fit.cov_params()\nstd_err = np.sqrt(np.diag(cov))\n\nz_values = model_fit.params / std_err\n\n'\\n'.join(str(model_fit.summary()).split('\\n')[1:10])\n"
'confusion_matrix_np = eval_session.run(\n  confusion,\n  feed_dict={\n      bottleneck_input: test_bottlenecks,\n      ground_truth_input: test_ground_truth\n  })\n\nprint(confusion_matrix_np)\n'
"train_images = 'C:\\\\Users\\\\sm50014\\\\Desktop\\\\new\\\\t10k-images-idx3-ubyte'\ntrain_labels = 'C:\\\\Users\\\\sm50014\\\\Desktop\\\\new\\\\t10k-labels-idx1-ubyte'\ntest_images = 'C:\\\\Users\\\\sm50014\\\\Desktop\\\\new\\\\test-images-idx3-ubyte'\ntest_labels = 'C:\\\\Users\\\\sm50014\\\\Desktop\\\\new\\\\test-labels-idx1-ubyte'\n"
'lg_loan_status_probas = logreg.predict_proba(X_test)\nlg_log_loss = log_loss(lg_y_test, lg_loan_status_probas)\n'
'gen.flow(...\n\ngen = ImageDataGenerator(**data_gen_args)\n\ngenX1 = image_datagen.flow(X1, y, batch_size=batch_size, seed=seed)\ngenX2 = image_datagen.flow(y, X1, batch_size=batch_size, seed=seed)\n'
"SELECT UserID, STRING_AGG(SKU) AS SKU_string FROM my_transactions_table GROUP BY UserID\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.read_csv('~/Desktop/test.csv', sep='\\t')\n&gt;&gt;&gt; df\n   UserID SKU_string\n0       1      a,b,c\n1       2        b,b\n2       3      c,b,a\n\n&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n&gt;&gt;&gt; vec = CountVectorizer(tokenizer=lambda x: x.split(','))\n&gt;&gt;&gt; X = vec.fit_transform(df['SKU_string'])\n&gt;&gt;&gt; X\n&lt;3x3 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 7 stored elements in Compressed Sparse Row format&gt;\n&gt;&gt;&gt; pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n   a  b  c\n0  1  1  1\n1  0  2  0\n2  1  1  1\n\n&gt;&gt;&gt; df = df.join(pd.DataFrame(X.toarray(), columns=['product_{}'.format(x) for x in vec.get_feature_names()]))\n&gt;&gt;&gt; df\n   UserID SKU_string  product_a  product_b  product_c\n0       1      a,b,c          1          1          1\n1       2        b,b          0          2          0\n2       3      c,b,a          1          1          1\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import scipy.sparse\n&gt;&gt;&gt; def booleanize_csr_matrix(mat):\n...     ''' Convert sparse matrix with positive integer elements to 1s '''\n...     nnz_inds = mat.nonzero()\n...     keep = np.where(mat.data &gt; 0)[0]\n...     n_keep = len(keep)\n...     result = scipy.sparse.csr_matrix(\n...             (np.ones(n_keep), (nnz_inds[0][keep], nnz_inds[1][keep])),\n...             shape=mat.shape\n...     )\n...     return result\n... \n&gt;&gt;&gt; pd.DataFrame(booleanize_csr_matrix(X).toarray(), columns=vec.get_feature_names())\n     a    b    c\n0  1.0  1.0  1.0\n1  0.0  1.0  0.0\n2  1.0  1.0  1.0\n"
'def time_backward(do_detach):\n    x = torch.tensor(torch.rand(100000000), requires_grad=True)\n    y = torch.tensor(torch.rand(100000000), requires_grad=True)\n    s2 = torch.sum(x * y)\n    s1 = torch.sum(x * y)\n    if do_detach:\n        s2 = s2.detach()\n    s = s1 + 0 * s2\n    t = time.time()\n    s.backward()\n    print(time.time() - t)\n\ntime_backward(do_detach= False)\ntime_backward(do_detach= True)\n\n0.502875089645\n0.198422908783\n'
'X = np.array([1,2,4,3])\ny = np.array([0,0.5,2,1.5])\n\nplt.plot(X,y)\n\nX1 = X[np.argsort(X)]\ny1 = y[np.argsort(X)]\n\nplt.plot(X1,y1)\n\nX_test, y_rbf = X_test[np.argsort(X_test)], y_rbf[np.argsort(X_test)]\n'
"(X&gt;=0)+0\n\nimport numexpr as ne\n\nne.evaluate('(X&gt;=0)+0')\n\n(X&gt;=0).view('i1')\n\nIn [14]: np.random.seed(0)\n    ...: X = np.random.randn(3072,10000)\n\nIn [15]: # OP's soln-1\n    ...: def relu_derivative_v1(x):\n    ...:      return (x&gt;0)*np.ones(x.shape)\n    ...: \n    ...: # OP's soln-2     \n    ...: def relu_derivative_v2(x):\n    ...:    x[x&gt;=0]=1\n    ...:    x[x&lt;0]=0\n    ...:    return x\n\nIn [16]: %timeit ne.evaluate('(X&gt;=0)+0')\n10 loops, best of 3: 27.8 ms per loop\n\nIn [17]: %timeit (X&gt;=0).view('i1')\n100 loops, best of 3: 19.3 ms per loop\n\nIn [18]: %timeit relu_derivative_v1(X)\n1 loop, best of 3: 269 ms per loop\n\nIn [19]: %timeit relu_derivative_v2(X)\n1 loop, best of 3: 89.5 ms per loop\n\nIn [27]: np.random.seed(0)\n    ...: X = np.random.randn(3072,10000)\n\nIn [28]: %timeit ne.evaluate('X&gt;=0').view('i1')\n100 loops, best of 3: 14.7 ms per loop\n"
"model.compile(loss='categorical_crossentropy',\n              optimizer='SGD',\n              metrics=['accuracy'])\n\nx_train = x_train.values/255\nx_test = x_test.values/255\n\nmodel = Sequential()\nmodel.add(Dense(784, activation='linear', input_shape=(784,)))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=SGD(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=128,\n                    epochs=5,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/10\n60000/60000 [==============================] - 4s - loss: 0.8974 - acc: 0.7801 - val_loss: 0.4650 - val_acc: 0.8823\nEpoch 2/10\n60000/60000 [==============================] - 4s - loss: 0.4236 - acc: 0.8868 - val_loss: 0.3582 - val_acc: 0.9034\nEpoch 3/10\n60000/60000 [==============================] - 4s - loss: 0.3572 - acc: 0.9009 - val_loss: 0.3228 - val_acc: 0.9099\nEpoch 4/10\n60000/60000 [==============================] - 4s - loss: 0.3263 - acc: 0.9082 - val_loss: 0.3024 - val_acc: 0.9156\nEpoch 5/10\n60000/60000 [==============================] - 4s - loss: 0.3061 - acc: 0.9132 - val_loss: 0.2845 - val_acc: 0.9196\n\nmodel = tf.keras.Sequential()\nmodel.add(layers.Dense(784, activation = 'relu',input_shape=(784,)))\nmodel.add(layers.Dense(h1, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))\n"
"%matplotlib inline \nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15,  stratify=Y)\nclf = make_pipeline(StandardScaler(), LogisticRegression())\nclf.fit(x_train, y_train)\nimport matplotlib.pyplot as plt\nplt.scatter(clf.named_steps['standardscaler'].transform(x_train),y_train)\nplt.scatter(clf.named_steps['standardscaler'].transform(x_test),y_test)\nprint(clf.score(x_test,y_test))\n"
'# input arrays to work with\nIn [2]: A = np.random.random_sample([32,1024,128])\nIn [3]: B = np.random.random_sample([32,1024,1024])\n\n# inspect their memory usage\n\nIn [12]: A.nbytes/1000000\nOut[12]: 33.554432   # ~ 33.5 Mb\n\nIn [13]: B.nbytes/1000000\nOut[13]: 268.435456  # ~ 268 Mb\n\n# your desired multiplication\nIn [14]: res = B[:2, ..., np.newaxis] * A[:2, :, np.newaxis, ...]\n\n# desired shape of the output\nIn [15]: res.shape\nOut[15]: (2, 1024, 1024, 128)\n\n# inspect memory usage\nIn [16]: res.nbytes/1000000\nOut[16]: 2147.483648  # ~ 2.1 GB\n'
"In [10]: df\nOut[10]:\n            0  1\n2018-01-01  2  1\n2018-03-01  0  0\n\nIn [12]: index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='MS')\n\nIn [13]: index\nOut[13]: DatetimeIndex(['2018-01-01', '2018-02-01', '2018-03-01'], dtype='datetime64[ns]', freq='MS')\n\nIn [14]: df.reindex(index)\nOut[14]:\n            0    1\n2018-01-01  2.0  1.0\n2018-02-01  NaN  NaN\n2018-03-01  0.0  0.0\n"
'class_probabilitiesDec = clf.predict_proba(X_test) \n\n[[ 0.00490808  0.00765327  0.01123035  0.00332751  0.00665502  0.00357707\n   0.05182597  0.03169453  0.04267532  0.02761833  0.01988187  0.01281091\n   0.02936528  0.03934781  0.02329257  0.02961484  0.0353548   0.02503951\n   0.03577073  0.04700108  0.07661592  0.04433907  0.03019715  0.02196157\n   0.0108976   0.0074869   0.0291989   0.03951418  0.01372598  0.0176358\n   0.02345895  0.0169703   0.02487314  0.01813493  0.0482489   0.01988187\n   0.03252641  0.01572249  0.01455786  0.00457533  0.00083188]\n\nclf = DecisionTreeClassifier()\nclf.fit([[1],[2],[3],[4],[5],[6],[7]], [[11],[12],[13],[13],[12],[11],[13]])\n\nclf.predict([[5]])\n\nclf.predict_proba([[5]])\n\nprobabilities = clf.predict_proba([[5]])[0]\n{clf.classes_[i] : probabilities[i] for i in range(len(probabilities))}\n\n{11: 0.0, 12: 1.0, 13: 0.0}\n'
'from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=4,\n                          n_informative=2, n_redundant=0,\n                           n_classes=2, random_state=0, shuffle=False)\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=2,\n                            random_state=0)\n\nclf.fit(X, y)\n\nclf.predict(X)[0] \n# 0\n\nclf.predict_proba(X)[0]\n# array([0.85266881, 0.14733119])\n\nprob_preds = clf.predict_proba(X)\nthreshold = 0.11 # define threshold here\npreds = [1 if prob_preds[i][1]&gt; threshold else 0 for i in range(len(prob_preds))]\n\npreds[0]\n# 1\n'
'x_time_train = x_train[:, 0]\nx_train = x_train[:, 1:]\nx_time_test = x_test[:, 0]\nx_test = x_test[:, 1:]\n'
'p = tf.argmax(tf.nn.softmax(Z_L, axis=0))\n'
'stories = [\n    "Lorem ipsum",\n    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. This is some extra text i don\'t care about",\n    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. A different gibberish this time.",\n    "Lorem ipsum dolor sit amet, consectetur",\n    "Lorem ipsum dolor sit amet, consectetur adipiscing elit.", # This is the full story\n]\n\nstories.sort(key=lambda s: len(s))\n\nstory = ""\nfor i, short_story in enumerate(stories[:-1]):\n    for long_story in stories[i+1:]:\n        if not long_story.startswith(short_story):\n            break\n    else:\n        story = short_story\n\nprint(story)\n'
'P = (n! / (tp! * (n-tp)!)) * (p ** tp) * ((1-p) ** (n-tp))\n\n# we\'ll need this for z-score\nfrom scipy.stats import norm\n\ndef ci(tp, n, alpha=0.05):\n    """ Estimates confidence interval for Bernoulli p\n    Args:\n      tp: number of positive outcomes, TP in this case\n      n: number of attemps, TP+FP for Precision, TP+FN for Recall\n      alpha: confidence level\n    Returns:\n      Tuple[float, float]: lower and upper bounds of the confidence interval\n    """\n    p_hat = float(tp) / n\n    z_score = norm.isf(alpha * 0.5)  # two sides, so alpha/2 on each side\n    variance_of_sum = p_hat * (1-p_hat) / n\n    std = variance_of_sum ** 0.5\n    return p_hat - z_score * std, p_hat + z_score * std\n\n# assuming data is a list of (upper_precision, precision, lower precision, upper_recall, recall, lower_recall)\n\nauc = 0\nsort(data, key=lambda x: x[1])  # sort by precision\nlast_point = (0, 0)  # last values of x,y\nfor up, p, lp, ur, r, lr in data:\n    # whatever was used to sort should come first\n    new_point = (p, ur)  # or (r, up) for upper bound; (p, lr), (r, lp) for lower bound\n    dx = new_point[0] - last_point[0]\n    y = last_point[1]\n    auc += dx * last_point[1] + dx * (new_point[1] - last_point[1]) * 0.5        \n'
"def gradient_descent(x, y, theta, alpha):\n    ''' simultaneously update theta0 and theta1 where\n        theta0 = theta0 - apha * 1/m * (sum of square error) ''' \n    theta_return = np.zeros((2, 1))\n    theta_return[0] = theta[0] - (alpha / m) * ((x.dot(theta) - y).sum())\n    theta_return[1] = theta[1] - (alpha / m) * (((x.dot(theta) - y)*x[:, 1][:, None]).sum())\n\n    return theta_return\n\ndef gradient_descent(x, y, theta, alpha):\n    ''' simultaneously update theta0 and theta1 where\n        theta0 = theta0 - apha * 1/m * (sum of square error) ''' \n    return theta - (alpha / m) * x.T.dot(x.dot(theta) - y)\n\ndef gradient_descent(x, y, theta, alpha, iterations):\n\n    ''' simultaneously update theta0 and theta1 where\n    theta0 = theta0 - apha * 1/m * (sum of square error) ''' \n\n    theta_return = np.zeros((2, 1))\n    for i in range(iterations):\n        theta_return[0] = theta[0] - (alpha / m) * ((x.dot(theta) - y).sum())\n        theta_return[1] = theta[1] - (alpha / m) * (((x.dot(theta) - y)*x[:, 1][:, None]).sum())\n        theta = theta_return\n\n    return theta\n\ntheta = gradient_descent(x, y, theta, 0.01, 1000)\n"
'from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(80, 155))\nX = scaler.fit_transform(X)\ny = scaler.fit_transform(y)\n'
'X = reshape([X.shape[0], X.shape[1], 1])\n\nX = X.reshape((X.shape[0], X.shape[1], 1))\n'
"for tname in tens_name:         # looping through each tensor name\n\n            if tensor_number &lt;= 812:      # I am interested in first 812 tensors\n                training = tf.placeholder(tf.bool, name = 'training')\n                is_training = tf.placeholder(tf.bool, name = 'is_training')\n                tensor = graph.get_tensor_by_name(tname)\n                tensor_values = sess.run(tensor, feed_dict={is_training: False, training: False, input_place: input_data})\n\ndef load_cnn(self,keep_prob = 0.5, num_filt = 32, num_layers = 2,is_training=True):\n        self.reuse=False\n        with tf.name_scope('input'):\n            self.image_input=tf.placeholder(tf.float32,shape=[None,None,None,3],name='image_input')\n            net=self.image_input\n\n            with slim.arg_scope([slim.separable_conv2d],\n            depth_multiplier=1,\n            normalizer_fn=slim.batch_norm,\n            normalizer_params={'is_training':is_training},\n            activation_fn=tf.nn.relu,weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n            weights_regularizer=slim.l2_regularizer(0.0005)):\n\n                # Down Scaling\n                # Block 1\n                net=slim.repeat(net, 2, slim.separable_conv2d, num_filt, [3, 3], scope = 'conv1')\n                print('en_conv1',net.shape,net.name) # 320x240x3 -&gt; 316x236x32\n                self.cnn_layer1=net\n                #Down Sampling\n                net=slim.max_pool2d(net,[2,2],scope='pool1') \n                print('en_maxpool1',net.shape,net.name) # 316x236x32 -&gt; 158x118x32\n"
'X,y = load_breast_cancer(return_X_y=True)\ndt = DecisionTreeClassifier(max_depth=3).fit(X,y)\nprint(dt.tree_.threshold)     #All the thresholds, size equals "dt.tree_.node_count"\ndt.tree_.threshold[3] = 10.0  #Manually modifying a threshold    \n'
"fig, ax = plt.subplots(1, 1, figsize=(8,5))\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.scatter(x1, x2, marker='o', color=color)\nfor i, line in enumerate(boundary_lines):\n    Θo, Θ1  = line\n    if i == len(boundary_lines) - 1:\n        c, ls, lw = 'k', '-', 2\n    else:\n        c, ls, lw = 'g', '--', 1.5\n    ax.plot(x_lin, Θo * x_lin + Θ1, c=c, ls=ls, lw=lw)\nplt.show()\n"
'from sklearn.linear_model import LinearRegression\nimport numpy as np\n\nmpg = np.array([27, 27, 26, 30, 22, 28, 25, 23, 27, 25]).reshape(-1, 1)\nprice = np.array([13495.0, 16500.0, 16500.0, 13950.0, 17450.0, 16845.0, 19045.0, 21485.0, 22470.0, 22625.0])\n\nlm = LinearRegression()\nlm.fit(mpg, price)\n\nprint(lm.intercept_)\nprint(lm.coef_)\n\nValueError: Expected 2D array, got 1D array instead:\narray=[27 27 26 30 22 28 25 23 27 25].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n'
'p = Pipeline([\n    PreprocessData(),\n    ColumnTransformer([\n        (0, model1(params)),  # Model 1 will receive Column 0 of data\n        ([1, 2], model2(params)),  # Model 2 will receive Column 1 and 2 of data\n    ], n_dimension=2, n_jobs=2),\n    (evaluate)\n])\n'
'{\n    "start":\n    {\n        "question": "Is it smaller than a bicycle?",\n        "yes": {\n            "question": "Is it a rabbit?",\n            "yes": null,\n            "no": null\n        },\n        "no": {\n            "question": "Is it an elephant?",\n            "yes": null,\n            "no": null\n        }\n    }\n}\n'
'rezised = cv2.resize(frame, (150, 150)) / 255\nexpandArrayImage = np.expand_dims(rezised, axis=0)\n# rest of the code\n'
'model=None\ndef load_model():\n\n    global model\n    model = ResNet50(weights="imagenet")\n\nif __name__ == "__main__":\n    print(("* Loading Keras model and Flask starting server..."\n        "please wait until server has fully started"))\n    load_model()\n    app.run()\n\n@app.route("/predict", methods=["POST"])\ndef predict():\n\n    if flask.request.method == "POST":\n\n            output=model.predict(data)  #what you want to do with frozen model goes here\n'
'# Given that X is a numpy array\nsamples = X.shape[0]\nsteps = X.shape[1]\nX = X.reshape(samples, steps, 1)\n'
'(np.array(c_A_array).T-c_A_array_mean).T \n\nnp.array(c_A_array)-c_A_array_mean.reshape((5,1))\n'
'opt.apply_gradients(zip([grads], [init_image]))\n'
'C= cp.random.random([10000,10000], dtype=cp.float32)\nD = cp.random.random([10000,10000], dtype=cp.float32)\n'
"# class labels are 0 and 1\nlabeled_data = [\n    (1, featureset_1),\n    (0, featureset_2),\n    (1, featureset_3),\n    # ...\n]\n\n# naive_bayes is your already trained classifier,\n# preferrably not on the data you're testing on :)\n\nfrom pyroc import ROCData\n\nroc_data = ROCData(\n    (label, naive_bayes.prob_classify(featureset).prob(1))\n    for label, featureset\n    in labeled_data\n)\nroc_data.plot()\n"
'from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nv = TfidfVectorizer()\nX = v.fit_transform(your_documents)\nD = euclidean_distances(X)\n'
'&gt;&gt;&gt; X = [[1, 2, 2, 3, 3, 1, 2, 4, 4, 1]]\n\n&gt;&gt;&gt; n_values = np.repeat(4, len(X[0]))\n&gt;&gt;&gt; n_values\narray([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n\n&gt;&gt;&gt; oh = OneHotEncoder(n_values=n_values)\n&gt;&gt;&gt; Xt = oh.fit_transform(X)\n&gt;&gt;&gt; Xt.toarray()\narray([[ 0.,  1.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,\n         0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n         1.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,\n         0.]])\n&gt;&gt;&gt; Xt.shape\n(1, 40)\n'
"&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n&gt;&gt;&gt; corpus = ['This is the first document.',\n              'This is the second second document.',\n              'And the third one.',\n              'Is this the first document?']\n&gt;&gt;&gt; vect = TfidfVectorizer()\n&gt;&gt;&gt; X = vect.fit_transform(corpus)\n&gt;&gt;&gt; X.todense()\n\nmatrix([[ 0.        ,  0.43877674,  0.54197657,  0.43877674,  0.        ,\n          0.        ,  0.35872874,  0.        ,  0.43877674],\n        [ 0.        ,  0.27230147,  0.        ,  0.27230147,  0.        ,\n          0.85322574,  0.22262429,  0.        ,  0.27230147],\n        [ 0.55280532,  0.        ,  0.        ,  0.        ,  0.55280532,\n          0.        ,  0.28847675,  0.55280532,  0.        ],\n        [ 0.        ,  0.43877674,  0.54197657,  0.43877674,  0.        ,\n          0.        ,  0.35872874,  0.        ,  0.43877674]])\n\n&gt;&gt;&gt; y = ['Relationships', 'Games', ...]\n\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; model = SGDClassifier()\n&gt;&gt;&gt; model.fit(X, y)\n\nX_pred = vect.transform(['My new document'])\ny_pred = model.predict(X_pred)\n\nvect = TfidfVectorizer(tokenizer=your_custom_tokenizer_function)\n"
'with StringIO as theta1IO:\n    numpy.savetxt(theta1IO, theta1)\n    data = {"theta1": theta1IO.getvalue() }\n    # write as JSON as usual\n\n# read data from JSON\nwith StringIO as theta1IO:\n    theta1IO.write(data[\'theta1\'])\n    theta1 = numpy.loadtxt(theta1IO)\n'
"class Preprocessor(object):\n    # hard code the stopwords for now\n    stopwords = nltk.corpus.stopwords.words()\n\n    def __init__(self, stemmer_cls, vectorizer_cls):\n        self.stemmer_cls = stemmer_cls\n        self.vectorizer_cls = vectorizer_cls\n\n    def _build_analyzer(self, stemmer, vectorizer_cls):\n        # analyzer tokenizes and lowercases\n        analyzer = super(vectorizer_cls, self).build_analyzer()\n        return lambda doc: (stemmer.stem(w) for w in analyzer(doc))\n\n    def fit(self, **kwargs):\n        analyzer = self._build_analyzer(self.stemmer_cls(), vectorizer_cls)\n        self.vectorizer_cls = vectorizer_cls(stopwords=stopwords,\n                                         analyzer=analyzer,\n                                         decode_error='ignore')\n\n        return self.vectorizer_cls.fit(kwargs)\n\n    def transform(self, **kwargs):\n        return self.vectorizer_cls.transform(kwargs)\n\n    def fit_transform(self, **kwargs):\n        return self.vectorizer_cls.fit_transform(kwargs)\n"
'def get_accuracy(X_train, y_train, X_test, y_test):\n    perceptron = Perceptron(random_state=241)\n    perceptron.fit(X_train, y_train)\n    pred_test = perceptron.predict(X_test)\n    result = accuracy_score(y_test, pred_test)\n    return result\n'
'from sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\nprint "Classification report: \\n", (classification_report(y_test, predictedlabel))\nprint "F1 micro averaging:",(f1_score(y_test, predictedlabel, average=\'micro\'))\nprint "ROC: ",(roc_auc_score(y_test, predictedlabel))\n\n        precision    recall  f1-score   support\n\n      0       0.74      0.93      0.82        57\n      1       0.00      0.00      0.00         3\n      2       0.57      0.38      0.46        21\n      3       0.75      0.75      0.75        12\n      4       0.44      0.68      0.54        22\n      5       0.81      0.93      0.87       226\n      6       0.57      0.54      0.55        48\n      7       0.71      0.38      0.50        13\n      8       0.70      0.72      0.71       142\n      9       0.33      0.33      0.33        33\n     10       0.42      0.52      0.47        21\n     11       0.80      0.91      0.85       145\n\n     av/total 0.71      0.78      0.74       743\n\n F1 micro averaging: 0.746153846154\n ROC:  0.77407943841\n'
'import numpy as np\nimport mord as m\nc = m.LogisticIT() #Default parameters: alpha=1.0, verbose=0, maxiter=10000\nc.fit(np.array([[0,0,0,1],[0,1,0,0],[1,0,0,0]]), np.array([1,2,3]))\nc.predict(np.array([0,0,0,1]))\nc.predict(np.array([0,1,0,0]))\nc.predict(np.array([1,0,0,0]))\n'
'public static double guardedLogarithm(double input) {\n    if (Double.isNaN(input) || Double.isInfinite(input)) {\n      return 0d;\n    } else if (input &lt;= 0d || input &lt;= -0d) {\n      // assume a quite low value of log(1e-5) ~= -11.51\n      return -10d;\n    } else {\n      return FastMath.log(input);\n    }\n  }\n'
'  print image_path_tf.eval()\n  print label_tf.eval()\n\nimage_path_tf, label_tf = tf.train.slice_input_producer([image_paths_tf, labels_tf], shuffle=False)\n\nimage_paths, labels = sess.run([images_batch_tf, labels_batch_tf])\nprint(image_paths)\nprint(labels)\n'
"random.seed(42)\n# Create example df with alphabetic col names.\nalphabet_cols = list(string.ascii_uppercase)[:26]\ndf = pd.DataFrame(np.random.randint(1000, size=(1000, 26)),\n                  columns=alphabet_cols)\ndf['Target'] = df['A']\ndf.drop(['A'], axis=1, inplace=True)\nprint(df.head())\ny = df.Target.values  # df['Target'] is not an np.array.\nfeature_cols = [i for i in list(df.columns) if i != 'Target']\nX = df.ix[:, feature_cols].as_matrix()\n# Illustrated here for manual splitting of training and testing data.\nX_train, X_test, y_train, y_test = \\\n    model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Initialize model.\nlogreg = linear_model.LinearRegression()\n\n# Use cross_val_score to automatically split, fit, and score.\nscores = model_selection.cross_val_score(logreg, X, y, cv=10)\nprint(scores)\nprint('average score: {}'.format(scores.mean()))\n\n     B    C    D    E    F    G    H    I    J    K   ...    Target\n0   20   33  451    0  420  657  954  156  200  935   ...    253\n1  427  533  801  183  894  822  303  623  455  668   ...    421\n2  148  681  339  450  376  482  834   90   82  684   ...    903\n3  289  612  472  105  515  845  752  389  532  306   ...    639\n4  556  103  132  823  149  974  161  632  153  782   ...    347\n\n[5 rows x 26 columns]\n[-0.0367 -0.0874 -0.0094 -0.0469 -0.0279 -0.0694 -0.1002 -0.0399  0.0328\n -0.0409]\naverage score: -0.04258093018969249\n"
" train_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=16,\n    color_mode='grayscale',\n    class_mode='binary')\n\n    validation_generator = test_datagen.flow_from_directory(\n    test_data_dir,\n    target_size=(img_width, img_height),\n    batch_size=16,\n    color_mode='grayscale\n    class_mode='binary')\n"
"import caffe\n\nns = caffe.NetSpec() # use this object to store the layers\nns.data, ns.label = L.Data(name='data',  ntop=2, \n                           data_param={'source':'', 'batch_size': 32})\nns.conv1_1 = L.Convolution(ns.data, name='conv1_1',\n                    convolution_param=\n                    {'kernel_size':3,'num_output':64,'pad':1},\n                    param=[{'lr_mult':1, 'decay_mult':1},\n                           {'lr_mult':2,'decay_mult':0}])\nprint str(ns.to_proto()) # print the net stored in ns object\n"
"df.rate / df.reservation_num.map(df.reservation_num.value_counts())\n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\ndf.rate / df.groupby('reservation_num').rate.transform('size')\n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\nu, f = np.unique(df.reservation_num.values, return_inverse=True)\ndf.rate / np.bincount(f)[f]\n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\ndef factor(a):\n    if len(a) &gt; 10000:\n        return pd.factorize(a)[0]\n    else:\n        return np.unique(a, return_inverse=True)[1]\n\ndef count(a):\n    f = factor(a)\n    return np.bincount(f)[f]\n\ndf.rate / count(df.reservation_num.values)  \n\n0    169.950\n1    129.475\n2    129.475\n3    385.950\n4    224.975\n5    482.950\n6    224.975\ndtype: float64\n\n%timeit df.rate / df.reservation_num.map(df.reservation_num.value_counts())\n%timeit df.rate / df.groupby('reservation_num').rate.transform('size')\n\n1000 loops, best of 3: 650 µs per loop\n1000 loops, best of 3: 768 µs per loop\n\n%%timeit\nu, f = np.unique(df.reservation_num.values, return_inverse=True)\ndf.rate / np.bincount(f)[f]\n\n10000 loops, best of 3: 131 µs per loop\n"
"model = Sequential()\nmodel.add(Dense(200, input_dim=400, init='glorot_uniform', activation='relu'))\nmodel.add(Reshape((200, 1))\nmodel.add(Conv1D(100,\n                 4,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n\nmodel = Sequential()\nmodel.add(Dense(200, input_shape=(400, 1), init='glorot_uniform', activation='relu'))\nmodel.add(Conv1D(100,\n                 4,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n"
'import numpy as np\n\ndef fillwithzeros(inputarray, outputshape):\n    """\n    Fills input array with dtype \'object\' so that all arrays have the same shape as \'outputshape\'\n    inputarray: input numpy array\n    outputshape: max dimensions in inputarray (obtained with the function \'findmaxshape\')\n\n    output: inputarray filled with zeros\n    """\n    length = len(inputarray)\n    output = np.zeros((length,)+outputshape, dtype=np.uint8)\n    for i in range(length):\n        output[i][:inputarray[i].shape[0],:inputarray[i].shape[1],:] = inputarray[i]\n    return output\n\ndef findmaxshape(inputarray):\n    """\n    Finds maximum x and y in an inputarray with dtype \'object\' and 3 dimensions\n    inputarray: input numpy array\n\n    output: detected maximum shape\n    """\n    max_x, max_y, max_z = 0, 0, 0\n    for array in inputarray:\n        x, y, z = array.shape\n        if x &gt; max_x:\n            max_x = x\n        if y &gt; max_y:\n            max_y = y\n        if z &gt; max_z:\n            max_z = z\n    return(max_x, max_y, max_z)\n\n#Create random data similar to your data\nrandom_data1 = np.random.randint(0,255, 210*224*3).reshape((210, 224, 3))\nrandom_data2 = np.random.randint(0,255, 220*180*3).reshape((220, 180, 3))\nX_train = np.array([random_data1, random_data2])\n\n#Convert X_train so that all images have the same shape\nnew_shape = findmaxshape(X_train)\nnew_X_train = fillwithzeros(X_train, new_shape)\n'
'In [72]: import numpy as np\n\nIn [73]: from scipy.stats import multivariate_normal\n\nIn [74]: mean = np.array([0, 1])\n\nIn [75]: cov = np.array([[2, -0.5], [-0.5, 4]])\n\nIn [76]: x = np.array([[0, 1], [1, 1], [0.5, 0.25], [1, 2], [-1, 0]])\n\nIn [77]: x\nOut[77]: \narray([[ 0.  ,  1.  ],\n       [ 1.  ,  1.  ],\n       [ 0.5 ,  0.25],\n       [ 1.  ,  2.  ],\n       [-1.  ,  0.  ]])\n\nIn [78]: p = multivariate_normal.pdf(x, mean, cov)\n\nIn [79]: p\nOut[79]: array([ 0.05717014,  0.04416653,  0.05106649,  0.03639454,  0.03639454])\n'
'm,n = inputs.shape\na = inputs.reshape(m//2,2,n//2,2).swapaxes(1,2)\nrow, col = np.unravel_index(a.reshape(a.shape[:-2] + (4,)).argmax(-1), (2,2))\n\nout = np.zeros_like(a)\nM,N = a.shape[:2]\nindx_tuple = np.arange(M)[:,None],np.arange(N), row, col\nout[indx_tuple] = a[indx_tuple]\n\nout2d = out.reshape(a.shape[:2]+(2,2)).swapaxes(1,2).reshape(m,n)\n\nIn [291]: np.random.seed(0)\n     ...: inputs = np.random.randint(11,99,(6,4))\n\nIn [292]: inputs\nOut[292]: \narray([[55, 58, 75, 78],\n       [78, 20, 94, 32],\n       [47, 98, 81, 23],\n       [69, 76, 50, 98],\n       [57, 92, 48, 36],\n       [88, 83, 20, 31]])\n\nIn [286]: out2d\nOut[286]: \narray([[ 0,  0,  0,  0],\n       [78,  0, 94,  0],\n       [ 0, 98,  0,  0],\n       [ 0,  0,  0, 98],\n       [ 0, 92, 48,  0],\n       [ 0,  0,  0,  0]])\n'
'import numpy as np\n\nbh_sne(X, random_state=np.random.RandomState(0))  # init with integer 0\n'
'fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, \n    validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, \n    sample_weight=None, initial_epoch=0, steps_per_epoch=None, \n    validation_steps=None)\n'
'import h2o\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nh2o.init()\n\n# Import a sample binary outcome train/test set into H2O\ntrain = h2o.import_file("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")\ntest = h2o.import_file("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")\n\n# Identify predictors and response\nx = train.columns\ny = "response"\nx.remove(y)\n\n# For binary classification, response should be a factor\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()\n\n# Train and cross-validate a GBM\nmodel = H2OGradientBoostingEstimator(distribution="bernoulli", seed=1)\nmodel.train(x=x, y=y, training_frame=train)\n\n# Test AUC\nmodel.model_performance(test).auc()\n# 0.7817203808052897\n\n# Generate predictions on a test set\npred = model.predict(test)\n\nIn [4]: pred.head()\nOut[4]:\n  predict        p0        p1\n---------  --------  --------\n        0  0.715077  0.284923\n        0  0.778536  0.221464\n        0  0.580118  0.419882\n        1  0.316875  0.683125\n        0  0.71118   0.28882\n        1  0.342766  0.657234\n        1  0.297636  0.702364\n        0  0.594192  0.405808\n        1  0.513834  0.486166\n        0  0.70859   0.29141\n\n[10 rows x 3 columns]\n\nfrom sklearn.metrics import roc_auc_score\n\npred_df = pred.as_data_frame()\ny_true = test[y].as_data_frame()\n\nroc_auc_score(y_true, pred_df[\'p1\'].tolist())\n# 0.78170751032654806\n'
'import sklearn.pipeline\nimport sklearn.tree\nimport sklearn.model_selection\nimport mlxtend.feature_selection\n\n\nclass CSequentialFeatureSelector(mlxtend.feature_selection.SequentialFeatureSelector):\n    def predict(self, X):\n        X = self.transform(X)\n        return self.estimator.predict(X)\n\n    def predict_proba(self, X):\n        X = self.transform(X)\n        return self.estimator.predict_proba(X)\n\n    def fit(self, X, y):\n        self.fit_helper(X, y) # fit helper is the \'old\' fit method, which I copied and renamed to fit_helper\n        self.estimator.fit(self.transform(X), y)\n        return self\n\n\ndef sfs(x, y):\n    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size=0.2, random_state=0)\n\n    clf = sklearn.tree.DecisionTreeClassifier()\n\n    param_grid = {\n        "sfs__estimator__max_depth": [3, 4, 5]\n    }\n\n    sfs = mlxtend.feature_selection.SequentialFeatureSelector(clone_estimator=True,\n                                                              estimator=clf,\n                                                              k_features=10,\n                                                              forward=True,\n                                                              floating=False,\n                                                              scoring=\'accuracy\',\n                                                              cv=3,\n                                                              n_jobs=1)\n\n    # Now only one object in the pipeline (in fact this is not even needed anymore)\n    pipe = sklearn.pipeline.Pipeline([(\'sfs\', sfs)])\n\n    gs = sklearn.model_selection.GridSearchCV(estimator=pipe,\n                                              param_grid=param_grid,\n                                              scoring=\'accuracy\',\n                                              n_jobs=1,\n                                              cv=3,\n                                              refit=True)\n\n    gs = gs.fit(x_train, y_train)\n\n    print("SFS Final Estimator Depth: " + str(gs.best_estimator_.named_steps.sfs.estimator.max_depth))\n\n    y_test_pred = gs.predict(x_test)\n    # Evaluate performance of y_test_pred\n'
'y_proba = tf.sigmoid(y_pred)\n'
'prediction = sess.run(y4, feed_dict={x: sampletest})\n\nprediction = sess.run(y4, feed_dict={x: [sampletest]})\n'
"X = np.array(df.ix[:, 2:6])\n\nu = df['Close'].iloc[-1]\n"
'model = Pipeline([\n        (\'feat\', FeatureUnion([\n            (\'tfidf\', TfidfVectorizer(analyzer=\'char\',  \n                                      ngram_range=(3, 5), \n                                      min_df=0.01, \n                                      lowercase=True, \n                                      tokenizer=tokenizeTfidf)),    \n        ])),\n        (\'clf\', VotingClassifier(estimators=[("pip1", GradientBoostingClassifier(n_estimators=1000, \n                                                                                 random_state=7)), \n                                             ("pip2", SVC()), \n                                             ("pip3", RandomForestClassifier())]))\n    ])\n\nmodel = Pipeline([\n        (\'tfidf\', TfidfVectorizer(analyzer=\'char\',  \n                                  ngram_range=(3, 5), \n                                  min_df=0.01, \n                                  lowercase=True, \n                                  tokenizer=tokenizeTfidf)),    \n        (\'clf\', VotingClassifier(estimators=[("pip1", GradientBoostingClassifier(n_estimators=1000, \n                                                                                 random_state=7)), \n                                             ("pip2", SVC()), \n                                             ("pip3", RandomForestClassifier())]))\n    ])\n'
"clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)\n\nX : {array-like, sparse matrix}, shape = [n_samples, n_features]\nTraining vectors, where n_samples is the number of samples and n_features is \nthe number of features.\n\ny : array-like, shape = [n_samples]\nTarget values.\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nnewsgroups_train = fetch_20newsgroups(subset='train')\ncategories = ['alt.atheism', 'talk.religion.misc',\n              'comp.graphics', 'sci.space']\n\nnewsgroups_train = fetch_20newsgroups(subset='train',\n                                      categories=categories)\nvectorizer = TfidfVectorizer()\n# the following will be the training data\nvectors = vectorizer.fit_transform(newsgroups_train.data)\nvectors.shape\n\nnewsgroups_test = fetch_20newsgroups(subset='test',\n                                     categories=categories)\n# this is the test data\nvectors_test = vectorizer.transform(newsgroups_test.data)\n\nclf = MultinomialNB(alpha=.01)\n\n# the fitting is done using the TRAINING data\n# Check the shapes before fitting\nvectors.shape\n#(2034, 34118)\nnewsgroups_train.target.shape\n#(2034,)\n\n# fit the model using the TRAINING data\nclf.fit(vectors, newsgroups_train.target)\n\n# the PREDICTION is done using the TEST data\npred = clf.predict(vectors_test)\n\nimport numpy as np\n\nnewsgroups_train.target\narray([1, 3, 2, ..., 1, 0, 1])\n\nnp.unique(newsgroups_train.target)\narray([0, 1, 2, 3])\n"
"# momentums of W in the first iteration\nm_vals = opt.get_slot(W, 'm')\nv_vals = opt.get_slot(W, 'v')\n# mask them before running the second iteration\nmasked_m_vals[[1,2]]=0\nmasked_v_vals[[1,2]]=0\nsess.run(opt.get_slot(W, 'm').assign(masked_m_vals))\nsess.run(opt.get_slot(W, 'v').assign(masked_v_vals))\n"
"net = tflearn.fully_connected(net, 1, activation='softmax')\n\nnet = tflearn.fully_connected(net, 1, activation='sigmoid')\n"
'best_clf = clf\n\nbest_clf = grid_search\n\nbest_clf = grid_search.best_estimator_\n'
'video = rnn.predict(input_fn = lambda:misc.input_fn_eval(1))\n'
'vgg_model = VGG16(weights=\'imagenet\', include_top=False, input_shape = (224,224, 3)) \n\nblock5_conv3 = vgg_model.get_layer("block5_conv3").output\nf0 = Flatten()(block5_conv3)\n\ntest_model = Model(inputs=vgg_model.input, outputs=f0)\n\nfrom keras import backend as K\n\n# ... (use the code above except the last line)\n\nfunc = K.function([vgg_model.input], [f0])\n\n# to call it:\noutputs = func([your_image_arrays])\n'
'dataset = tf.contrib.data.CsvDataset(train1_path,\n                                     [tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32],\n                                     header=True, field_delim=\' \')\n\ndataset = dataset.map(lambda *x: tf.convert_to_tensor(x))\ndataset = dataset.batch(ITERATOR_BATCH_SIZE)\n\nwith tf.Session() as sess:\n    for i in range (NR_EPOCHS):\n        print(\'\\nepoch: \', i)\n        iterator = dataset.make_one_shot_iterator()\n        next_element = iterator.get_next()\n        while True:            \n            try:\n              data_and_target = sess.run(next_element)\n            except tf.errors.OutOfRangeError:\n              break\n            print("\\n\\n", data_and_target)\n'
'sqrt((a1-b1)^2+(a2-b2)^2+(a3-b3)^2+(a4-b4)^2)\n'
"def ensemble(models):\n    input_img = Input(shape=input_shape)\n\n    outputs = [model(input_img) for model in models] # get the output of model given the input image\n    y = Average()(outputs)\n\n    model = Model(inputs=input_img, outputs=y, name='ensemble')\n    return model\n\nensemble_model = ensemble(models)\n"
'import numpy as np\nfrom sklearn import preprocessing\n\nX_train = np.array([[ 1., -1.,  2.], [ 2.,  0.,  0.],[ 0.,  1., -1.]])\nX_test = np.array([[ 0, -1.,  1.5], [ 2.5,  0.,  1]])\n\nscaler = preprocessing.MinMaxScaler()\nscaler = scaler.fit(X_train)\n\nX_train_minmax = scaler.transform(X_train)\nX_test_minmax = scaler.transform(X_test)\n'
'X1 = (array([-8.1530527e-10,  8.9952795e-10, -9.1185753e-10,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=\'float32\'),\n array([0., 0., 0., 0., 0., 0.], dtype=\'float32\'),\n array([0., 0., 0., 0., 0., 0.], dtype=\'float32\'))\n\nX2 = (array([-8.1530527e-10,  8.9952795e-10, -9.1185753e-10,\n         0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=\'float32\'),\n array([0., 0., 0., 0., 0., 0., 1], dtype=\'float32\'),\n array([0., 0., 0., 0., 0., 0.], dtype=\'float32\'))\n\nprint("X1:", np.array(X1).dtype, "\\nX2:", np.array(X2).dtype)\n'
'x_sample, z_mu, z_var = vae(X[None, ...])\n'
'd1 = dense(i1)\n\nexpected axis -1 of input shape to have value 10\n'
"from tensorflow.python.keras.layers import GlobalAveragePooling2D, Dense, Input\nfrom tensorflow.python.keras.applications.xception import Xception  \n\ninp = Input(shape=(299, 299, 3))\nbase_model = Xception(include_top=False, input_tensor=inp, weights='imagenet')\ny = base_model.layers[-1].output\ny = GlobalAveragePooling2D()(y)\ny = Dense(4, activation='sigmoid')(y)\nmodel = Model(inputs=inp, outputs=y)\n\n[0.48, 0.4 , 0.58, 0.37]\n"
"x= ['GA','TA','SA','TA','GA','TA','SA']\n\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing.label import _encode\nfrom sklearn.utils import column_or_1d\nx = column_or_1d(x, warn=True)\nclasses_,encoded_values = _encode(x,uniques=np.array(['GA','TA','SA']),encode=True)\nencoded_values, classes_\n\n#(array([0, 1, 2, 1, 0, 1, 2]), ['GA', 'TA', 'SA'])\n\n#comparing with labelencoder, which will sort the labels before encoding\nle = LabelEncoder()\n\nle.fit_transform(x),le.classes_\n\n#\n(array([0, 2, 1, 2, 0, 2, 1], dtype=int64),\n array(['GA', 'SA', 'TA'], dtype='&lt;U2'))\n"
"cat_cols = ['Item_Identifier', 'Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', \n         'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type', 'Item_Type_Combined']\nenc = LabelEncoder()\n\nfor col in cat_cols:\n    train[col] = train[col].astype('str')\n    test[col] = test[col].astype('str')\n    train[col] = enc.fit_transform(train[col])\n    test[col] = enc.transform(test[col])\n"
'df_with_cat = pd.DataFrame({\n           \'A\'      : [\'ios\', \'android\', \'web\', \'NaN\'],\n           \'B\'      : [4, 4, \'NaN\', 2], \n           \'target\' : [1, 1, 0, 0] \n       })\n\npip3 install scikit-learn==0.20.2\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncolumnTransformer = ColumnTransformer(\n    transformers=[\n        (\'cat\', OneHotEncoder(), CATEGORICAL_FEATURES),\n        (\'num\', Imputer( strategy=\'most_frequent\'), NUMERICAL_FEATURES)\n    ])\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder \n\nCATEGORICAL_FEATURES = [\'A\']\nNUMERICAL_FEATURES = [\'B\']\nTARGET = [\'target\']\n\nnumerical_pipline = Pipeline([\n    (\'selector\', DataFrameSelector(NUMERICAL_FEATURES)),\n    (\'imputer\', Imputer(strategy=\'most_frequent\'))\n])\n\ncategorical_pipeline = Pipeline([\n    (\'selector\', DataFrameSelector(CATEGORICAL_FEATURES)),\n    (\'cat_encoder\', LabelBinarizerPipelineFriendly())\n])\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values\n\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\n\npreprocessing_pipeline = FeatureUnion(transformer_list=[\n    (\'numerical_pipline\', numerical_pipline),\n    (\'categorical_pipeline\', categorical_pipeline)\n])\n\npreprocessing_pipeline.fit_transform(df_with_cat[CATEGORICAL_FEATURES+NUMERICAL_FEATURES])\n\nfrom sklearn import tree\nclf = tree.DecisionTreeClassifier()\nfull_pipeline = make_pipeline(preprocessing_pipeline, clf)\n\nfull_pipeline.fit(df_with_cat[CATEGORICAL_FEATURES+NUMERICAL_FEATURES], df_with_cat[TARGET])\n\nclass LabelBinarizerPipelineFriendly(LabelBinarizer):\n    \'\'\'\n     Wrapper to LabelBinarizer to allow usage in sklearn.pipeline\n    \'\'\'\n\n    def fit(self, X, y=None):\n        """this would allow us to fit the model based on the X input."""\n        super(LabelBinarizerPipelineFriendly, self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\n'
"TARGET  Predictions\n     1  0\n     0  0\n     0  0\n     0  0\n\nautism = pd.read_csv('10-features-uns.csv')\n\nx = autism.drop(['TARGET'], axis = 1)  \ny = autism['TARGET']\nx_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state=1)\n"
'from sklearn.feature_selection import VarianceThreshold\n\nselector = VarianceThreshold()\nselector.fit_transform(dfX)\nprint(selector.variances_)\n\n# outputs [1.57412087 1.08363799 1.11752334 0.58501874 2.2983772  0.2857617\n# 1.09782539 0.98715471 0.93262548]\n'
'scaler1 = MinMaxScaler(feature_range=(0, 1))\n\ny_train = scaler1.fit_transform(y_train)\n\ny_test = scaler1.transform(y_test)\n'
"def is_tuning_required(similarity_matrix, rows_of_cluster):\n    rows = similarity_matrix[rows_of_cluster]\n\n    for row in rows:\n        for col_index in rows_of_cluster:\n            score = row[col_index]\n\n            if score &gt; 0.5:\n                continue\n\n            return True\n\n    return False\n\ndef get_pref_range(similarity):\n    starting_point = np.median(similarity)\n\n    if starting_point == 0:\n        starting_point = np.mean(similarity)\n\n    # Let's try to accelerate the pace of values picking\n    step = 1 if starting_point &gt;= 0.05 else step = 2\n\n    preference_tuning_range = [starting_point]\n    max_val = starting_point\n    while max_val &lt; 1:\n        max_val *= 1.25 if max_val &gt; 0.1 and step == 2 else step\n\n    preference_tuning_range.append(max_val)\n\n    min_val = starting_point\n    if starting_point &gt;= 0.05:\n        while min_val &gt; 0.01:\n            min_val /= step\n            preference_tuning_range.append(min_val)\n\n    return preference_tuning_range\n\ndef run_clustering(similarity, preference):\n    clusterer = AffinityPropagation(damping=0.9, \n                                    affinity='precomputed', \n                                    max_iter=5000, \n                                    convergence_iter=2500, \n                                    verbose=False, \n                                    preference=preference)\n\n    affinity = clusterer.fit(similarity)\n\n    labels = affinity.labels_\n\n    return labels, len(set(labels)), affinity.cluster_centers_indices_\n\ndef run_ideal_clustering(similarity):\n    preference_tuning_range = get_pref_range(similarity)\n\n    best_tested_preference = None\n    for preference in preference_tuning_range:\n        labels, labels_count, cluster_centers_indices = run_clustering(similarity, preference)\n\n        needs_tuning = False\n        wrong_clusters = 0\n        for label_index in range(labels_count):\n            cluster_elements_indexes = np.where(labels == label_index)[0]\n\n            tuning_required = is_tuning_required(similarity, cluster_elements_indexes)\n            if tuning_required:\n                wrong_clusters += 1\n\n                if not needs_tuning:\n                    needs_tuning = True\n\n        if best_tested_preference is None or wrong_clusters &lt; best_tested_preference[1]:\n            best_tested_preference = (preference, wrong_clusters)\n\n        if not needs_tuning:\n            return labels, labels_count, cluster_centers_indices\n\n     # The clustering has not been tuned enough during the iterations, we choose the less wrong clusters\n    return run_clustering(similarity, preference)\n"
'def Sigmoid_Derivative(self, x):\n    return self.Sigmoid(x) * (1-self.Sigmoid(x))\n'
"import numpy as np\n\ndata = list(range(36))\nwindow_size = 12\nsplits = []\n\nfor i in range(window_size, len(data)):\n    train = np.array(data[i-window_size:i])\n    test = np.array(data[i:i+3])\n    splits.append(('TRAIN:', train, 'TEST:', test))\n\n# View result\nfor a_tuple in splits:\n    print(a_tuple)\n\n# ('TRAIN:', array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), 'TEST:', array([12, 13, 14]))\n# ('TRAIN:', array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]), 'TEST:', array([13, 14, 15]))\n# ('TRAIN:', array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]), 'TEST:', array([14, 15, 16]))\n"
"def create_model():\n    model = Sequential()\n    model.add(Lambda(UniversalEmbedding, output_shape=(2, 512), input_shape=(2,)))\n    # (2, 512)\n    model.add(Flatten()) # (2*512)\n    model.add(Dense(2*256, activation='relu')) # (2*256)\n    model.add(Dense(2, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n"
'import torch.nn as nn\nimport torch.nn.functional as F\n\nclass AMNI_Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, bias=True):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, bias=bias, padding=padding)\n        self.crow = self.conv.kernel_size[0] // 2\n        self.ccol = self.conv.kernel_size[1] // 2\n\n        # this module only works with odd sized kernels\n        assert self.conv.kernel_size[0] % 2 == 1 and self.conv.kernel_size[1] % 2 == 1\n\n    def forward(self, x):\n        # currently only padding with zeros is supported\n        if self.conv.padding[0] != 0 or self.conv.padding[1] != 0:\n            x = F.pad(x, pad=(self.conv.padding[1], self.conv.padding[1], self.conv.padding[0], self.conv.padding[0]))\n\n        # center filters on the "zeros" according to the diagram by AMNI, starting column for even/odd rows may need to change depending on padding/kernel size\n        if (self.crow + self.ccol + self.conv.padding[0] + self.conv.padding[1]) % 2 == 0:\n            x_even = F.conv2d(x[:, :, :-1, 1:], self.conv.weight, self.conv.bias, stride=2)\n            x_odd = F.conv2d(x[:, :, 1:, :-1], self.conv.weight, self.conv.bias, stride=2)\n        else:\n            x_even = F.conv2d(x[:, :, :-1, :-1], self.conv.weight, self.conv.bias, stride=2)\n            x_odd = F.conv2d(x[:, :, 1:, 1:], self.conv.weight, self.conv.bias, stride=2)\n        b, c, h, w = x_even.shape\n\n        # interleave even and odd rows back together\n        return torch.stack((x_even, x_odd), dim=3).contiguous().view(b, c, -1, w)\n\n&gt;&gt;&gt; x = torch.arange(64).view(1, 1, 8, 8).float()\ntensor([[[[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11., 12., 13., 14., 15.],\n          [16., 17., 18., 19., 20., 21., 22., 23.],\n          [24., 25., 26., 27., 28., 29., 30., 31.],\n          [32., 33., 34., 35., 36., 37., 38., 39.],\n          [40., 41., 42., 43., 44., 45., 46., 47.],\n          [48., 49., 50., 51., 52., 53., 54., 55.],\n          [56., 57., 58., 59., 60., 61., 62., 63.]]]])\n\n&gt;&gt;&gt; layer = AMNI_Conv2d(1, 1, (3, 5), bias=False)\n\n# set kernels to delta functions to demonstrate kernel centers\n&gt;&gt;&gt; with torch.no_grad():\n...     layer.conv.weight.zero_()\n...     layer.conv.weight[:,:,1,2] = 1\n\n&gt;&gt;&gt; result = layer(x)\ntensor([[[[10., 12.],\n          [19., 21.],\n          [26., 28.],\n          [35., 37.],\n          [42., 44.],\n          [51., 53.]]]], grad_fn=&lt;ViewBackward&gt;)\n\n&gt;&gt;&gt; layer = AMNI_Conv2d(1, 1, (3, 5), padding=(1, 2), bias=False)\n\n# set kernels to delta functions to demonstrate kernel centers\n&gt;&gt;&gt; with torch.no_grad():\n...     layer.conv.weight.zero_()\n...     layer.conv.weight[:,:,1,2] = 1\n\n&gt;&gt;&gt; result = layer(x)\ntensor([[[[ 1.,  3.,  5.,  7.],\n          [ 8., 10., 12., 14.],\n          [17., 19., 21., 23.],\n          [24., 26., 28., 30.],\n          [33., 35., 37., 39.],\n          [40., 42., 44., 46.],\n          [49., 51., 53., 55.],\n          [56., 58., 60., 62.]]]], grad_fn=&lt;ViewBackward&gt;)\n'
'def custom_kernel_constraint(var):\n    return tf.math.maximum(tf.math.minimum(var, 1.), 0.)\n\ndef disconnection_kernel_constraint(var):\n    return tf.multiply(var, projection_matrix)\n\nkernel_initializer= tf.constant_initializer(tf.multiply(tf.random_normal(shape=[12,12),projetion_matrix))\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\n# You should probably edit this variable\nNUM_OF_CURVES = 4\n\n# &lt;data&gt; should be a 1-D array containing the Y values of the series\n# &lt;x_of_data&gt; should be a 1-D array containing the corresponding X values of the series\ndata, x_of_data = np.loadtxt('...')\n\n# clustering of first 2*num_of_curves points\n# I started at NUM_OF_CURVES instead of 0 because my xs started at 0. \n#     The range (0:NUM_OF_CURVES*2) will probably be better for you.\nraw_data = data[NUM_OF_CURVES:NUM_OF_CURVES*3]\nraw_xs = x_of_data[NUM_OF_CURVES:NUM_OF_CURVES*3]\nsort_ind = np.argsort(raw_data)\nY = raw_data[sort_ind].reshape(NUM_OF_CURVES,-1).T\nX = raw_xs[sort_ind].reshape(NUM_OF_CURVES,-1).T\n\n# approximation of A and B for each curve\nA = ((Y[0]*Y[1])*(X[0]-X[1]))/(Y[1]-Y[0])  \nB = (A / Y[0]) - X[0] \n\n# creating approximating curves\nf = []\nfor i in range(NUM_OF_CURVES):\n    f.append(A[i]/(x_of_data+B[i]))\ncurves = np.vstack(f)\n\n# clustering the points to the approximating curves\nraw_clusters = [[] for _ in range(NUM_OF_CURVES)]\nfor i in range(len(data)):\n    raw_clusters[np.abs(curves[:,i]-data[i]).argmin()].append((x_of_data[i],data[i]))\n\n# changing the clusters to np.arrays of the shape (2,-1) \n# where row 0 contains the X coordinates and row 1 the Y coordinates\nclusters = []\nfor i in range(len(raw_clusters)):\n    clusters.append(np.array(list(zip(*raw_clusters[i]))))\n"
'test_image = image.load_img(path_to_image, target_size=(128, 128))\ntest_image = image.img_to_array(test_image) / 255  # &lt; - division by 255\ntest_image = np.expand_dims(test_image, axis=0)\n'
'import copy\nimport numpy as np\nimport lightgbm as lgb\n\nX = np.random.rand(100,3)\ny = np.random.rand(100)\ntrain = lgb.Dataset(X, y)\nmodel = lgb.train({"verbose": -1}, train, num_boost_round=1)\n\n# the printout is here:\nmodel2 = copy.deepcopy(model)\n\nimport os\nimport contextlib\n\nwith open(os.devnull, "w") as f, contextlib.redirect_stdout(f):\n    model2 = copy.deepcopy(model)\n'
'from os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n% matplotlib inline\n\ndf1 = df[df[\'label\']==48000000]\ntext = " ".join(review for review in df1.text)\nwordcloud = WordCloud().generate(text)\nplt.imshow(wordcloud, interpolation=\'bilinear\')\nplt.axis("off")\nplt.show()\n\nimport nltk\nnltk.download(\'stopwords\')\nfrom nltk.corpus import stopwords\n\ndef preprocess_text(sentence):\n\n    # Convert to lowercase\n    sentence = sentence.lower()\n\n    new_stopwords = [\'service\',\'contract\',\'solution\',\'county\',\'supplier\',\n             \'district\',\'council\',\'borough\',\'management\',\n             \'provider\',\'provision\'\n              \'project\',\'contractor\']\n\n    stop_words = set(stopwords.words(\'english\'))\n    stop_words.update(new_stopwords)\n    sentence = [w for w in sentence.split(" ") if not w in stop_words]\n    sentence = \' \'.join(w for w in sentence)\nreturn sentence\n\ndf[\'text\'] = df[\'text\'].apply(preprocess_text)\n'
'       SetA      SetB\n0  personA0  personB0\n1  personA1  personB1\n2  personA2  personB2\n3  personA3  personB3\n'
"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(tokenizer=lambda txt: txt.split())\nX = vectorizer.fit_transform(words)\nprint (vectorizer.get_feature_names())\n\n['a', 'am', 'is', 'the', 'this']\n"
'preds = model.predict(x1)\ny_pred = np.argmax(preds, axis = 1)\nprint(classification_report(y1, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.71      0.08      0.15        59\n           1       0.42      0.95      0.58        41\n\n    accuracy                           0.44       100\n   macro avg       0.57      0.52      0.37       100\nweighted avg       0.59      0.44      0.33       100\n'
"poly=PolynomialFeatures(degree=7)\n\npoly.get_feature_names()\n# ['1', 'x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7']\n\nreg.coef_\n# array([[   0.        ,    5.43894411,  -68.14277256,  364.28508827,\n#         -941.70924401, 1254.89358662, -831.27091422,  216.43304954]])\n\nreg.intercept_\n# array([0.51228593])\n\ncoef = reg.coef_[0]\n\ny = reg.intercept_ + coef[0] + coef[1]*x + coef[2]*x**2 + coef[3]*x**3 + coef[4]*x**4 + coef[5]*x**5 + coef[6]*x**6 + coef[7]*x**7\n\nx = np.linspace(0, 1, 15) \n\nplt.plot(x, y)\n\nx = np.random.rand(1,10)\ny_eq = reg.intercept_ + coef[0] + coef[1]*x + coef[2]*x**2 + coef[3]*x**3 + coef[4]*x**4 + coef[5]*x**5 + coef[6]*x**6 + coef[7]*x**7\ny_reg = np.concatenate(reg.predict(poly.transform(x.reshape(-1,1)))) \n\ny_eq\n# array([[0.72452703, 0.64106819, 0.67394222, 0.71756648, 0.71102853,\n#         0.63582055, 0.54243177, 0.71104983, 0.71287962, 0.6311952 ]])\n\ny_reg\n# array([0.72452703, 0.64106819, 0.67394222, 0.71756648, 0.71102853,\n#        0.63582055, 0.54243177, 0.71104983, 0.71287962, 0.6311952 ])\n\nnp.allclose(y_reg, y_eq)\n# True\n"
"In [52]: from nltk.sentiment.vader import SentimentIntensityAnalyzer                                                \n\nIn [53]: sia = SentimentIntensityAnalyzer()                                                                         \n\nIn [54]: sia.polarity_scores(&quot;I am not going to miss using this product.&quot;)                                          \nOut[54]: {'neg': 0.0, 'neu': 0.829, 'pos': 0.171, 'compound': 0.1139}\n"
"    model = tf.keras.models.Sequential()\n    \n    model.add(Conv1D(filters=32, kernel_size=8, strides=1, activation=&quot;relu&quot;, padding=&quot;same&quot;,input_shape=(X_train.shape[1], X_train.shape[2])))\n    model.add(MaxPooling1D(pool_size = 2))\n    model.add(Conv1D(filters=16, kernel_size=8, strides=1, activation=&quot;relu&quot;, padding=&quot;same&quot;))\n    model.add(MaxPooling1D(pool_size = 2))\n    \n    \n    model.add(Masking(mask_value=0.0))\n    model.add(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True))\n    model.add(Bidirectional(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True)))\n    model.add(Bidirectional(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout, return_sequences=True)))\n    model.add(Bidirectional(LSTM(units, dropout=dropout, recurrent_dropout=recurrent_dropout)))\n    model.add(Dense(30, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    \n    model.add(Dense(num_classes, activation='softmax'))\n"
'Y = [["foo", "bar"],          # the first sample is a foo and a bar\n     ["foo"],                 # the second is only a foo\n     ["bar", "baz"]]          # the third is a bar and a baz\n\nfrom sklearn.multiclass import OneVsRestClassifier\nclf = OneVsRestClassifier(LogisticRegression())\n\n&gt;&gt;&gt; from sklearn.preprocessing import MultiLabelBinarizer\n&gt;&gt;&gt; mlb = MultiLabelBinarizer()\n&gt;&gt;&gt; mlb.fit_transform(Y)\narray([[1, 0, 1],\n       [0, 0, 1],\n       [1, 1, 0]])\n\n&gt;&gt;&gt; mlb.inverse_transform(mlb.transform(Y))\n[(\'bar\', \'foo\'), (\'foo\',), (\'bar\', \'baz\')]\n'
'desc_vect = desc_vect.tocsr()\n\nn_docs = desc_vect.shape[0]\ntfidftables = [{} for _ in xrange(n_docs)]\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i, j in zip(*desc_vect.nonzero()):\n    tfidftables[i][terms[j]] = X[i, j]\n'
"&gt;&gt;&gt; lr = LogisticRegression()\n&gt;&gt;&gt; X = np.random.randn(3, 4)\n&gt;&gt;&gt; y = [1, 0, 0]\n&gt;&gt;&gt; lr.fit(X, y)\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001)\n&gt;&gt;&gt; lr.predict_proba(X[0])\narray([[ 0.49197272,  0.50802728]])\n"
"&gt;&gt;&gt; clf = GridSearchCV(LogisticRegression(), {'C': [1, 2, 3]})\n&gt;&gt;&gt; clf.fit(np.random.randn(10, 4), np.random.randint(0, 2, 10))\nGridSearchCV(cv=None,\n       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, penalty='l2', random_state=None, tol=0.0001),\n       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n       param_grid={'C': [1, 2, 3]}, pre_dispatch='2*n_jobs', refit=True,\n       score_func=None, scoring=None, verbose=0)\n&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; pprint(clf.grid_scores_)\n[mean: 0.40000, std: 0.11785, params: {'C': 1},\n mean: 0.40000, std: 0.11785, params: {'C': 2},\n mean: 0.40000, std: 0.11785, params: {'C': 3}]\n"
'customerIndex = ... # Put index of the column\n\ndef extract(line):\n    """Given a line create a tuple (customerId, labeledPoint)"""\n    label = 1 if line[5] == \'True\' else 0\n    point =  LabeledPoint(label, [line[6], line[7]])\n    customerId = line[customerIndex]\n    return (customerId, point)\n\nfinal_data = (data\n    .map(lambda line: line.split(","))\n    .filter(lambda line: len(line) &gt;1 )\n    .map(extract)) # Map to tuples\n\n# As before\n(trainingdata, testdata) = final_data.randomSplit([0.7, 0.3])\n\n# Use only points, put the rest of the arguments in place of ...\nmodel = DecisionTree.trainRegressor(trainingdata.map(lambda x: x[1]), ...)\n\n# Make predictions using points\npredictions = model.predict(testdata.map(lambda x: x[1].features))\n\n# Add customer id and label\nlabelsIdsAndPredictions = (testData\n    .map(lambda x: (x[0], x[1].label))\n    .zip(predictions))\n\ntop50 = labelsIdsAndPredictions.top(50, key=lambda x: x[1])\n'
'# Build the matrix [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]].\nidentity_matrix = tf.diag(tf.ones([3]))\n\nfor ...:\n\n  # Compute a vector of predicted letter indices.\n  argmax_val = ...\n\n  # For each element of `argmax_val`, select the corresponding row\n  # from `identity_matrix` and concatenate them into matrix.\n  input = tf.gather(identity_matrix, argmax_val)\n'
'logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\ny_conv = tf.nn.softmax(logits)\n\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, y_)\n\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n'
"print df\n     ID  ATTR  QUANTITY\n0    17   203        14\n1    17   201         8\n2    17   111         1\n3    17   203        14\n4    17   201         8\n5    17   111         1\n6    17   203        14\n7    17   201         8\n8    17   111         1\n9    17   203        14\n10   17   201         8\n11   17   111         1\n12  159    80         5\n13  178    82        16\n\n#add column att for counting groups items\ndf['att'] = df.groupby('ID')['ID'].cumcount() + 1\n#pivot df with column att\ndf1 = df.pivot(index='ID', columns='att', values='ATTR').reset_index()\n#add string 'attr' to columns names\ndf1.columns =  [df1.columns[0]] + ['attr' + str(col) for col in df1.columns[1:]] \ndf2 = df.groupby('ID')['QUANTITY'].sum().reset_index()\n#merge with sum\nprint pd.merge(df1, df2 , on=['ID']).fillna(0)\n\n    ID  attr1  attr2  attr3  attr4  attr5  attr6  attr7  attr8  attr9  attr10  \\\n0   17    203    201    111    203    201    111    203    201    111     203   \n1  159     80      0      0      0      0      0      0      0      0       0   \n2  178     82      0      0      0      0      0      0      0      0       0   \n\n   attr11  attr12  QUANTITY  \n0     201     111        92  \n1       0       0         5  \n2       0       0        16  \n"
"df.loc[:, pd.notnull(df).sum()&gt;len(df)*.8]\n\ndf.loc[:, (df &gt; 1).sum() &gt; len(df) *. 8]\n\ndf.dropna(thresh=0.8*len(df), axis=1)\n\ndf = pd.DataFrame(np.random.random((100, 5)), columns=list('ABCDE'))\nfor col in df:\n    df.loc[np.random.choice(list(range(100)), np.random.randint(10, 30)), col] = np.nan\n\n%timeit df.loc[:, pd.notnull(df).sum()&gt;len(df)*.8]\n1000 loops, best of 3: 716 µs per loop\n\n%timeit df.dropna(thresh=0.8*len(df), axis=1)\n1000 loops, best of 3: 537 µs per loop\n"
'from scipy.sparse import coo_matrix\n\n# Construct row IDs\nlens = np.array([len(item) for item in dataset])\nshifts_arr = np.zeros(lens.sum(),dtype=int)\nshifts_arr[lens[:-1].cumsum()] = 1\nrow = shifts_arr.cumsum()\n\n# Extract values from dataset into a NumPy array\narr = np.concatenate(dataset)\n\n# Get the unique column IDs to be used for col-indexing into output array\ncol = np.unique(arr[:,0],return_inverse=True)[1]\n\n# Determine the output shape\nout_shp = (row.max()+1,col.max()+1)\n\n# Finally create a sparse marix with the row,col indices and col-2 of arr\nsp_out = coo_matrix((arr[:,1],(row,col)), shape=out_shp)\n\ncol = (arr[:,0]-1).astype(int)\n\nIn [264]: dataset = [[(1, 0.13), (2, 2.05)],\n     ...:            [(2, 0.23), (4, 7.35), (5, 5.60)],\n     ...:            [(2, 0.61), (3, 4.45)]]\n\nIn [265]: sp_out.todense() # Using .todense() to show output\nOut[265]: \nmatrix([[ 0.13,  2.05,  0.  ,  0.  ,  0.  ],\n        [ 0.  ,  0.23,  0.  ,  7.35,  5.6 ],\n        [ 0.  ,  0.61,  4.45,  0.  ,  0.  ]])\n'
"model = Sequential([\n  Dense(classes * 8, input_dim=200), \n  Activation('sigmoid'), \n  Dense(classes), \n  Activation('softmax'), \n])\n"
"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_dim=100),\n    Dense(output_dim=10),\n    Activation('sigmoid'),\n])\n\nmodel.summary()\n\nmodel2 = Sequential([\n    Dense(32, input_dim=100,trainable=False),\n    Dense(output_dim=10),\n    Activation('sigmoid'),\n])\n\nmodel2.summary()\n\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ndense_1 (Dense)                  (None, 32)            3232        dense_input_1[0][0]              \n____________________________________________________________________________________________________\ndense_2 (Dense)                  (None, 10)            330         dense_1[0][0]                    \n____________________________________________________________________________________________________\nactivation_1 (Activation)        (None, 10)            0           dense_2[0][0]                    \n====================================================================================================\nTotal params: 3,562\nTrainable params: 3,562\nNon-trainable params: 0\n____________________________________________________________________________________________________\n____________________________________________________________________________________________________\nLayer (type)                     Output Shape          Param #     Connected to                     \n====================================================================================================\ndense_3 (Dense)                  (None, 32)            3232        dense_input_2[0][0]              \n____________________________________________________________________________________________________\ndense_4 (Dense)                  (None, 10)            330         dense_3[0][0]                    \n____________________________________________________________________________________________________\nactivation_2 (Activation)        (None, 10)            0           dense_4[0][0]                    \n====================================================================================================\nTotal params: 3,562\nTrainable params: 330\nNon-trainable params: 3,232 \n"
'class Model:\n  def __init__(self, train=True, params):\n  """ Build the model """\n\n  tf.placeholder( ... ) \n\n  tf.get_variable( ...) \n\n\n\ndef main(_):\n  with tf.Graph.as_default() as g:\n    with tf.name_scope("Train"):\n      with tf.variable_scope("Model", reuse=None):\n        train = Model(train=True, params ) \n    with tf.name_scope("Valid"):\n      # Now reuse variables = no memory cost\n      with tf.variable_scope("Model", reuse=True):\n        # But you can set different parameters\n        valid = Model(train=False, params)\n\n    session = tf.Session\n    ...\n'
'# Create some variables.\nv1 = tf.Variable(..., name="v1")\nv2 = tf.Variable(..., name="v2")\n...\n# Add ops to save and restore all the variables.\nsaver = tf.train.Saver()\n\n# Later, launch the model, use the saver to restore variables from disk, and\n# do some work with the model.\nwith tf.Session() as sess:\n  # Restore variables from disk.\n  saver.restore(sess, "/tmp/model.ckpt")\n  print("Model restored.")\n  # Do some work with the model\n  ...\n'
'C = np.ones((3, 3)) + np.eye(3)  # full transform matrix\nU = C[:2, :]  # dimensionality reduction matrix\nV1 = np.linalg.inv(C)[:, :2]  # PCA-style reconstruction matrix\nprint(V1)\n#array([[ 0.75, -0.25],\n#       [-0.25,  0.75],\n#       [-0.25, -0.25]])\n\nV2 = np.linalg.pinv(U)  # LDA-style reconstruction matrix\nprint(V2)\n#array([[ 0.63636364, -0.36363636],\n#       [-0.36363636,  0.63636364],\n#       [ 0.09090909,  0.09090909]])\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils import check_array, check_X_y\n\nimport numpy as np\n\n\ndef inverse_transform(lda, x):\n    if lda.solver == \'lsqr\':\n        raise NotImplementedError("(inverse) transform not implemented for \'lsqr\' "\n                                  "solver (use \'svd\' or \'eigen\').")\n    check_is_fitted(lda, [\'xbar_\', \'scalings_\'], all_or_any=any)\n\n    inv = np.linalg.pinv(lda.scalings_)\n\n    x = check_array(x)\n    if lda.solver == \'svd\':\n        x_back = np.dot(x, inv) + lda.xbar_\n    elif lda.solver == \'eigen\':\n        x_back = np.dot(x, inv)\n\n    return x_back\n\n\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\nlda = LinearDiscriminantAnalysis()\nZ = lda.fit(X, y).transform(X)\n\nXr = inverse_transform(lda, Z)\n\n# plot first two dimensions of original and reconstructed data\nplt.plot(X[:, 0], X[:, 1], \'.\', label=\'original\')\nplt.plot(Xr[:, 0], Xr[:, 1], \'.\', label=\'reconstructed\')\nplt.legend()\n'
'url = \'http://localhost:9000/spend_api\'\nfiles = {\'file\': open(\'Test_data_final.csv\',\'rb\')}\nr = request.post(url,files=files)\n\nclf_model = joblib.load(\'MNB_Clf.pkl\',\'r\')\n\napp = Flask(__name__)\n\n@app.route(\'/spend_api\',methods=[\'POST\'])\ndef make_predict():\n    data = request.get_json(force=True)\n\n    test_data = pd.read_csv(data)\n    pred_proba_class = clf_model.predict_proba(test_data[\'ColumnName1\'])\n    final_pred_file = pd.DataFrame(pred_proba_class)\n    return jsonify(results=final_pred_file.to_dict(orient="records"))\nif __name__ == \'__main__\':\n  app.run(port = 9000,debug=True)\n\nfrom flask import Flask \nfrom flask import send_file \nfrom StringIO import StringIO \nimport pandas as pd \n\napp = Flask("csv")\n\n@app.route("/get_csv")\ndef hello():\n\n    st = """col1|col2\n    1|2\n    3|4 \n    """\n    df = pd.read_csv(StringIO(st), sep="|")\n    df.to_csv("/tmp/my_test_csv.csv", index=False, sep="|")\n    return send_file("/tmp/my_test_csv.csv")\n\n\nif __name__ == \'__main__\':\n    app.run(port=5000)\n'
'digits.target[mask] # the set of all true labels of cluster `mask`\n\nmode(digits.target[mask])  # finds the most frequent digit in this cluster\n\nlabels[mask]=mode(digits.target[mask])[0]\n'
"import pickle \n\n#Do the classification and name the fitted object clf\nwith open('clf.pickle', 'wb') as file :\n    pickle.dump(clf,file,pickle.HIGHEST_PROTOCOL)\n\nimport pickle \n\nwith open('clf.pickle', 'rb') as file :\n    clf =pickle.load(file)\n\n# Now predict on the new dataframe df as \npred = clf.predict(df.values)\n"
" plt.tight_layout()\n plt.ylabel('True label')\n plt.xlabel('Predicted label')\n plt.figure()\n plot_confusion_matrix(cm,['points above 90', 'points below 90'])\n"
'print(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, svm\nfrom sklearn.feature_selection import SelectPercentile, f_classif\n\n###############################################################################\n# import some data to play with\n\n# The iris dataset\niris = datasets.load_iris()\n\n# Some noisy data not correlated\nE = np.random.uniform(0, 0.1, size=(len(iris.data), 20))\n\n# Add the noisy data to the informative features\nX = np.hstack((iris.data, E))\ny = iris.target\n\n###############################################################################\nplt.figure(1)\nplt.clf()\n\nX_indices = np.arange(X.shape[-1])\n\n###############################################################################\n# Univariate feature selection with F-test for feature scoring\n# We use the default selection function: the 10% most significant features\nselector = SelectPercentile(f_classif, percentile=10)\nselector.fit(X, y)\nscores = -np.log10(selector.pvalues_)\nscores /= scores.max()\nplt.bar(X_indices - .45, scores, width=.2,\n        label=r\'Univariate score ($-Log(p_{value})$)\', color=\'g\')\n\n###############################################################################\n# Compare to the weights of an SVM\nclf = svm.SVC(kernel=\'linear\')\nclf.fit(X, y)\n\nsvm_weights = (clf.coef_ ** 2).sum(axis=0)\nsvm_weights /= svm_weights.max()\n\nplt.bar(X_indices - .25, svm_weights, width=.2, label=\'SVM weight\', color=\'r\')\n\nclf_selected = svm.SVC(kernel=\'linear\')\nclf_selected.fit(selector.transform(X), y)\n\nsvm_weights_selected = (clf_selected.coef_ ** 2).sum(axis=0)\nsvm_weights_selected /= svm_weights_selected.max()\n\nplt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n        width=.2, label=\'SVM weights after selection\', color=\'b\')\n\n\nplt.title("Comparing feature selection")\nplt.xlabel(\'Feature number\')\nplt.yticks(())\nplt.axis(\'tight\')\nplt.legend(loc=\'upper right\')\nplt.show()\n'
'd.shape(28,28)\n\nd.shape\n\nd = d.reshape(28,28)\n'
"from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\ndatagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nimg = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\nx = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\nx = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n\n# the .flow() command below generates batches of randomly transformed images\n# and saves the results to the `preview/` directory\ni = 0\nfor batch in datagen.flow(x, batch_size=1,\n                          save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n    i += 1\n    if i &gt; 20:\n        break  # otherwise the generator would loop indefinitely\n"
'Subset of the training data\n\nSubset of the target values\n\nValueError: `classes=array([5])` is not the same as on last call to\n    partial_fit, was: array([1, 2, 3, 4])\n\nX1 = X[2:3]\ny1 = y[2:3]\n\nclasses = np.unique(y)\nf1 = sgd_clf.partial_fit(X1, y1, classes=classes)\n'
"df['time_stamp'] = df['time_stamp'].dt.floor('d').astype(np.int64)\n#sorting and remove duplicated days per users \ndf = df.sort_values(['user_id', 'time_stamp']).drop_duplicates()\n\na = df.groupby('user_id')['time_stamp'].rolling(window=3)\nb = pd.to_timedelta((a.max()- a.min())).dt.days\nprint (b)\nuser_id    \n1        0     NaN\n         1     NaN\n         2     5.0\n         3     7.0\n2        4     NaN\n3        5     NaN\n         6     NaN\n         7     7.0\n4        8     NaN\n         9     NaN\n         10    8.0\nName: time_stamp, dtype: float64\n\nc = b[b == 7].index.get_level_values('user_id').tolist()\nprint (c)\n[1, 3]\n"
"def double_conv(var1, input):\n    x = k.layers.Conv2d(some_parameters) (input)\n    x = k.layers.Conv2d(some_parameters) (x)\n    x = k.layers.MaxPooling2d(some_parameters) (x)\n    return x\n\ndef conv_bn_relu(filters, kernel=(3,3)):\n    def inside(x):\n        x = Conv2D(filters, kernel, padding='same') (x)\n        x = BatchNormalization() (x)\n        x = Activation('relu') (x)\n        return x\n    return inside\n\n# usage:\nx = conv_bn_relu(params) (x)\n\ndef ConvBnRelu(filters, kernel=(3,3)):\n    def inside(x):\n        x = Conv2D(filters, kernel, padding='same') (x)\n        x = BatchNormalization() (x)\n        x = Activation('relu') (x)\n        return x\n    return inside\n\n# usage:\nx = ConvBnRelu(params) (x)\n"
"import pandas as pd\nimport networkx as nx    \nimport matplotlib.pyplot as plt\n\nG = nx.karate_club_graph()\n\ndf = (pd.DataFrame(list(G.degree), columns=['node','degree'])\n        .set_index('node'))\ndf['club'] = pd.Series({node:data['club']\n                        for node,data in G.nodes(data=True)})\ndf['color'] = df.groupby('club')['degree'].transform(lambda c: c/c.max())\ndf.loc[df['club']=='Officer', 'color'] *= -1\n\nlayout = nx.fruchterman_reingold_layout(G)\nvmin = df['color'].min()\nvmax = df['color'].max()\ncmap = plt.cm.coolwarm\n\nnx.draw_networkx(G, pos=layout, with_labels=True, node_color=df['color'],\n                 cmap=cmap, vmin=vmin, vmax=vmax)\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm.set_array([])\ncbar = plt.colorbar(sm)\n"
"print (df_full)\n   discount  tax  total  subtotal productid\n0      3.00    0     20        13       002\n1     40.00    0    106        94       003\n2     46.49    6     21        20       004\n\nprint(df_full[pd.to_numeric(df_full['discount'], errors='coerce').isnull()]\n\n#for convert to numeric - non numeric are convert to `NaN`s\ndf_full['discount'] = pd.to_numeric(df_full['discount'], errors='coerce')\n\ndf_full['Class'] = ((df_full['discount'] &gt; 20) &amp; \n                    (df_full['tax'] == 0) &amp; \n                    (df_full['total'] &gt; 100)).astype(int)\nprint (df_full)\n   discount  tax  total  subtotal productid  Class\n0      3.00    0     20        13       002      0\n1     40.00    0    106        94       003      1\n2     46.49    6     21        20       004      0\n"
'# y is ground truth with shape[0] = batch and shape[1] = token index\nnp.concatenate([y[:, 1:], np.full([y.shape[0], 1], EOS)], axis=1)\n'
"training_sizes = [25,50,75,100,150,200]\n\nfrom sklearn.metrics import log_loss\n\n# This will calculate the 'neg_log_loss' as you wanted, just with one extra param\nscorer = make_scorer(log_loss, greater_is_better=False, \n                     needs_proba=True, \n                     labels=[0.0, 1.0])   #&lt;== This is what you need.\n\n....\n.... \ntrain_size, train_scores, validation_scores = learning_curve(KNeighborsClassifier(n_neighbors=1), \n                                           X=dd[features], \n                                           y=dd[target],\n                                           train_sizes=training_sizes, \n                                           cv=5, \n                                           scoring=scorer)  #&lt;== Add that here\n"
'def predict(self, X_test):\n    predictions=[]\n    for row in X_test:\n        label = random.choice(self.Y_train)\n        predictions.append(label)\n\n    return predictions\n'
"df=df[['Adj. Close','HL_PCT','PCT_change','Adj.Volume']]\n\ndf=df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]\n\nimport pandas as pd \nimport quandl \ndf=quandl.get('WIKI/GOOGL') \ndf=df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\ndf['HL_PCT']=(df['Adj. High']-df['Adj. Low'])/df['Adj. Close']\ndf['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']\ndf=df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]\nprint(df.head())\n"
"def defineModel(nkernels, nstrides, dropout, input_shape):\n    l_input = Input( shape=input_shape )\n    model = Conv1D(nkernels, nstrides, activation='relu')(l_input)\n    model = Conv1D(nkernels*2, nstrides, activation='relu')(model)\n    model = BatchNormalization()(model)\n    model = MaxPooling1D(nstrides)(model)\n    model = Dropout(dropout)(model)\n    return model, l_input\n\n\nmodels = []\ninputs = []\nfor i in range(15):\n    model, input = defineModel(64,2,0.75,(64,1))\n    models.append( model )\n    inputs.append( input )\n\nmerged = Concatenate()(models)\n\nmerged = Dense(512, activation='relu')(merged)\nmerged = Dropout(0.75)(merged)\nmerged = Dense(1024, activation='relu')(merged)\nmerged = Dropout(0.75)(merged)\nmerged = Dense(40, activation='softmax')(merged)\nmodel = Model(inputs=inputs, outputs=merged)\n"
'import numpy as np\nimport matplotlib.pyplot as plt\n\nX = [(0, 0), (1, 0), (0, 1), (1, 1)]\nY = [0, 1, 1, 0]\n\neta = 0.7\n\nw1 = 2 * np.random.random(size=(2, 3)) - 1\nw2 = 2 * np.random.random(size=(3, 1)) - 1\nb1 = 2 * np.random.random(size=(1, 3)) - 1\nb2 = 2 * np.random.random(size=(1, 1)) - 1\n\n\ndef sigmoid(x):\n    return 1. / (1 + np.exp(-x))\n\n\ndef dsigmoid(y):\n    return y * (1 - y)\n\n\nN = 2000\nerror = []\nfor n in range(N):\n    Dw_1 = np.zeros((2, 3))\n    Dw_2 = np.zeros((3, 1))\n    Db_1 = np.zeros((1, 3))\n    Db_2 = np.zeros((1, 1))\n\n    tmp_error = 0\n    for i in range(len(X)):  # iterate over all examples\n        x = np.array(X[i]).reshape(1, 2)\n        y = np.array(Y[i])\n\n        layer1 = sigmoid(np.dot(x, w1) + b1)\n        output = sigmoid(np.dot(layer1, w2) + b2)\n\n        tmp_error += np.mean(np.abs(output - y))\n\n        d_w2 = np.dot(layer1.T, ((output - y) * dsigmoid(output)))\n        d_b2 = np.dot(1, ((output - y) * dsigmoid(output)))\n\n        d_w1 = np.dot(x.T, (np.dot((output - y) * dsigmoid(output), w2.T) * dsigmoid(layer1)))\n        d_b1 = np.dot(1, (np.dot((output - y) * dsigmoid(output), w2.T) * dsigmoid(layer1)))\n\n        Dw_2 += d_w2\n        Dw_1 += d_w1\n        Db_1 += d_b1\n        Db_2 += d_b2\n\n    w2 = w2 - eta * Dw_2\n    w1 = w1 - eta * Dw_1\n    b1 = b1 - eta * Db_1\n    b2 = b2 - eta * Db_2\n\n    error.append(tmp_error)\n\nerror = np.array(error)\nprint(error.shape)\nplt.plot(error)\nplt.show()\n'
'model.layers[1].get_weights()\n'
"pd.crosstab(df.MonthDate.dt.strftime('%b'),df.DayCategory).rename_axis(None,1)\n\n           Event  Federal Holiday\nMonthDate                        \nFeb            1                3\nJan            2                2\n"
'from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols=[list_of_header_names],outputCol="features")\nspDF = assembler.transform(spDF)\n\ntri=LogisticRegression(maxIter=10,\n                       regParam=0.01,\n                       featuresCol="features",\n                       labelCol="label")\nlr_model = tri.fit(spDF)\n'
"pred = baseline.predict(X_test)\npred_original_data = ds.iloc[X_test.index]\npred_original_data['prediction'] = pred\n"
"import cv2\nimport numpy as np\n\ncv2.namedWindow('Result')\nimg = cv2.imread('qkEuE.png')\n\nv1 = 0\nv2 = 0\n\ndef doEdges():\n    edges = cv2.Canny(img,v1,v2)\n    edges = cv2.cvtColor(edges,cv2.COLOR_GRAY2BGR)\n    res = np.concatenate((img,edges),axis = 0)\n    cv2.imshow('Result',res)\ndef setVal1(val):\n    global v1\n    v1 = val\n    doEdges()\ndef setVal2(val):\n    global v2\n    v2 = val\n    doEdges()\n\ncv2.createTrackbar('Val1','Result',0,500,setVal1)\ncv2.createTrackbar('Val2','Result',0,500,setVal2)\n\ncv2.imshow('Result',img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n"
"model.compile(loss='mean_squared_error', optimizer='sgd', metrics='acc')\n\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\nmodel.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mse','mae'])\n"
"def fix_layer0(filename, batch_input_shape, dtype):\n    with h5py.File(filename, 'r+') as f:\n        model_config = json.loads(f.attrs['model_config'].decode('utf-8'))\n        layer0 = model_config['config']['layers'][0]['config']\n        layer0['batch_input_shape'] = batch_input_shape\n        layer0['dtype'] = dtype\n        f.attrs['model_config'] = json.dumps(model_config).encode('utf-8')\n\nfix_layer0('model.h5', [None, 224, 224, 3], 'float32')\n\nloaded_model = load_model('model.h5')\n"
'# show mnist image\nindex_of_mnist_img = 0\nplt.imshow(x_test[index_of_mnist_img], cmap = plt.cm.binary)\nplt.show()\n\n\nimg = cv2.imread("4.png")\nimg = cv2.resize(img, (28,28))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nplt.imshow(img, cmap = plt.cm.binary)\n\n\nimg = cv2.imread(r"4.png")\nimg = cv2.resize(img, (28,28))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimg= cv2.bitwise_not(img) # invert image\nplt.imshow(img, cmap = plt.cm.binary)\n\n\npredictionsB = model.predict(img)\nprint(np.argmax(predictionsB[0]))\n\n4\n'
'from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport pandas as pd\nfrom csv import reader,writer\nimport operator as op\nimport string\nfrom sklearn import neighbors\n\n#Read data from corpus\nr = reader(open(\'corpus.csv\',\'r\'))\nabstract_list = []\nscore_list = []\ninstitute_list = []\nrow_count = 0\nfor row in list(r)[1:]:\n    institute,score,abstract = row[0], row[1], row[2]\n    if len(abstract.split()) &gt; 0:\n      institute_list.append(institute)\n      score = float(score)\n      score_list.append(score)\n      abstract = abstract.translate(string.punctuation).lower()\n      abstract_list.append(abstract)\n      row_count = row_count + 1\n\nprint("Total processed data: ", row_count)\n\n#Vectorize (TF-IDF, ngrams 1-4, no stop words) using sklearn --&gt;\nvectorizer = TfidfVectorizer(analyzer=\'word\', ngram_range=(1,4),\n                     min_df = 0, stop_words = \'english\', sublinear_tf=True)\nresponse = vectorizer.fit_transform(abstract_list)\nclasses = score_list\nfeature_names = vectorizer.get_feature_names()\n\nclf = neighbors.KNeighborsRegressor(n_neighbors=1)\nclf.fit(response, classes)\nclf.predict(response)\n'
'import os\nimport  imageio\nimport pandas as pd\n\ncatimages = os.listdir("Cat")\ndogimages = os.listdir("Dog")\ncatVec = []\ndogVec = []\nfor img in catimages:\n       img = imageio.imread(f"Cat/{img}")\n       ar = img.flatten()\n       catVec.append(ar)    \ncatdf = pd.DataFrame(catVec)    \ncatdf.insert(loc=0,column ="label",value=1)\n\nfor img in dogimages:\n       img = imageio.imread(f"Dog/{img}")\n       ar = img.flatten()\n       dogVec.append(ar)    \ndogdf = pd.DataFrame(dogVec)    \ndogdf.insert(loc=0,column ="label",value=0)\n\ndata = pd.concat([catdf,dogdf])      \ndata = data.sample(frac=1)\n'
"bins = (2, 3, 5)\n\nimport pandas as pd\n\nappStore = pd.DataFrame()\nappStore['user_rating'] = [2.3, 3.3, 4, 6]\n\nbins = (2, 3, 5)\ngroup_names = ['bad', 'good']\nappStore['user_rating'] = pd.cut(appStore['user_rating'], bins=bins, labels=group_names)\nprint(appStore['user_rating'].unique())\n\nprint()\nprint(appStore)\n\n[bad, good, NaN]\nCategories (2, object): [bad &lt; good]\n\n  user_rating\n0         bad\n1        good\n2        good\n3         NaN\n\nimport pandas as pd\n\nappStore = pd.DataFrame()\nappStore['user_rating'] = [2.3, 3.3, 4, 4.5]\n\nbins = (2, 3, 5)\ngroup_names = ['bad', 'good']\nappStore['user_rating'] = pd.cut(appStore['user_rating'], bins=bins, labels=group_names)\nprint(appStore['user_rating'].unique())\n\nprint()\nprint(appStore)\n\n[bad, good]\nCategories (2, object): [bad &lt; good]\n\n  user_rating\n0         bad\n1        good\n2        good\n3        good\n"
'rfr = RandomForestRegressor(max_features=0.5, min_samples_leaf=4, max_depth=6)\n'
'import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntraining_data = np.array([[1, 1, 1], [2, 3, 1], [0, -1, 4], [0, 3, 0], [10, -6, 8], [-3, -12, 4]])\ntesting_data = np.array([6, 11, 1, 9, 10, -38])\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(units = 1, activation = tf.keras.activations.relu, input_shape = (3,)))\nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(0.001), loss = tf.keras.losses.mean_squared_error, metrics = [tf.keras.metrics.mean_squared_error])\nmodel.summary()\n\nmodel.fit(training_data, testing_data, epochs = 1, verbose = \'False\')\nprint("Traning completed.")\nmodel.predict(np.array([[1, 2, 1]]))\n\narray([[0.08026636]], dtype=float32)\n'
'import tensorflow as tf\n\nimport numpy as np\n\nimport os\nimport time\n\nnow = time.localtime()\nsubdir = time.strftime("%d-%b-%Y_%H.%M.%S", now)\n\nsummary_dir1 = os.path.join("stackoverflow", subdir, "t1")\nsummary_writer1 = tf.summary.create_file_writer(summary_dir1)\n\nfor cont in range(200):\n    with summary_writer1.as_default():\n        tf.summary.scalar(name="unify/sin_x", data=np.math.sin(cont) ,step=cont)\n        tf.summary.scalar(name="unify/sin_x_2", data=np.math.sin(cont/2), step=cont)\n    summary_writer1.flush()\n'
'hidden_errors = output_weights.T.dot(output_errors)\n\nhidden_errors = output_weights.T.dot(d_predicted_output)\n'
"model.compile(loss=perplexity_loss, optimizer='adam', metrics=['accuracy'])\n\nfrom keras.models import load_model\n\nmodel = load_model('my_model.h5', custom_objects={'perplexity_loss': perplexity_loss})\n"
"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nx_train.iloc[:, 3:] = sc.fit_transform(x_train.iloc[:, 3:])\nprint (x_train)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nfeatures = ['col1','col2',..., 'col24']\nx_train[features] = sc.fit_transform(x_train[features])\nprint (x_train)\n"
"df['A1'] = (df.groupby('user')['A'].diff()\n              .fillna(df['A'])\n              .where(df['E'].ne(df['E'].shift()))\n              .ffill()\n              .astype(int))\nprint (df)\n     A  E  user  A1\n0    0  0     1   0\n1   12  1     1  12\n2   12  1     1  12\n3   13  2     1   1\n4   15  3     1   2\n5   15  3     1   2\n6   15  3     1   2\n7   19  4     2  19\n8   20  5     2   1\n9   25  6     2   5\n10  25  6     2   5\n"
'In [2]: treelite_model\nOut[2]: &lt;cuml.fil.fil.TreeliteModel at 0x7f11ceeca840&gt;\n\n'
'eps = Lambda(lambda t: K.random_normal(stddev=1.0, shape=(K.shape(t)[0], latent_dim)))(z_log_var)\n'
'import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import linear_model\n\nfull_comment_data = pd.DataFrame({"Comment":["This is bad", "Good job", "I do not like this"],\n                                  "Result":[0,1,0]})\n\nfeatures = full_comment_data["Comment"]\nresults = full_comment_data["Result"]\n\ncv = CountVectorizer()  \nfeatures = cv.fit_transform(features)\n\n\nlogistic_regression = linear_model.LogisticRegression(solver="lbfgs")\nmodel = logistic_regression.fit(features, results)\n\ninput_values = ["I love this comment"] #This value should be evaluated\n\nprediction = logistic_regression.predict(cv.transform(input_values)) #adding values for prediction\nprediction = prediction[0]\nprint(prediction)\n'
'!pip install pgmpy==0.1.6\n'
"import tensorflow as tf\nimport tensorflow.keras as keras\n\ninputs = keras.layers.Input(shape=(2,))\nhidden = keras.layers.Dense(10)(inputs)\noutput1 = keras.layers.Dense(3, activation='sigmoid')(hidden)\n\n@tf.function\ndef const(tensor):\n    batch_size = tf.shape(tensor)[0]\n    constant = tf.constant(['out1','out2','out3'], dtype=tf.string)\n    constant = tf.expand_dims(constant, axis=0)\n    return tf.broadcast_to(constant, shape=(batch_size, 3))\n\noutput2 = keras.layers.Lambda(const)(inputs)\nmodel = keras.models.Model(inputs=inputs, outputs=[output1, output2])\n\nclass ConstantOnBatch(keras.layers.Layer):\n    def __init__(self, constant, *args, **kwargs):\n        self._initial_constant = copy.deepcopy(constant)\n        self.constant = K.constant(constant)\n        self.out_shape = self.constant.shape.as_list()\n        self.constant = tf.reshape(self.constant, [1]+self.out_shape)\n        super().__init__(*args, **kwargs)\n\n    def build(self, input_shape):\n        super().build(input_shape)\n\n    def call(self, inputs):\n        batch_size = tf.shape(inputs)[0]\n        output_shape = [batch_size]+self.out_shape\n        return tf.broadcast_to(self.constant, output_shape)\n\n    def compute_output_shape(self, input_shape):\n        input_shape = input_shape.as_list()\n        return [input_shape[0]]+self.out_shape\n\n    def get_config(self):\n        base_config = super().get_config()\n        base_config['constant'] = self._initial_constant\n\n    @classmethod\n    def from_config(cls, config):\n        return cls(**config)\n"
"X = df[['Open','Close','High','Low','Volume']].values\n\ny = df['adj close'].values\n"
"a = np.array([[0, 1, 1],\n              [1, 0, 0],\n              [1, 1, 1]])\n\nfrom itertools import chain, combinations\n\nfeatures = a.T.tolist()\npower_set = []\nfor comb in chain.from_iterable(combinations(features, r) \n                               for r in range(2,len(features)+1)):\n    power_set.append(np.logical_and.reduce(comb).view('i1').tolist())\n\nnp.array(power_set).T\n\narray([[0, 0, 1, 0],\n       [0, 0, 0, 0],\n       [1, 1, 1, 1]])\n"
"Y_metadata1 = {'output_index': np.array([[0]])}\ny1_pred = m.predict(np.array(x_pred).reshape(1,-1), Y_metadata=Y_metadata1)\n\nY_metadata1 = {'output_index': np.array([[0]])}\na = np.array(x_pred[0]).reshape(1,-1)\nprint(a.shape)\ny1_pred = m.predict(a,Y_metadata=Y_metadata1)\n"
'1. {News: 0.8, Sports: 0.5}\n2. {News: 0.1, Sports: 0.8}\n\nP(w1 | News) = (5*0.8 + 2*0.1) / (#of weighted occurrences of all words in all your News docs)\nP(w1 | Sports) = (5*0.5 + 2*0.8) / (# weighted occurrences of all words in all your Sports docs)\n'
'&gt;&gt;&gt; from nltk import word_tokenize\n&gt;&gt;&gt; from nltk.stem import SnowballStemmer\n&gt;&gt;&gt; stemmer = SnowballStemmer(\'spanish\')\n&gt;&gt;&gt; \n&gt;&gt;&gt; stemmer.stem(\'cuando\')\nu\'cuand\'\n&gt;&gt;&gt; stemmer.stem(\'apprenderla\')\nu\'apprend\'\n&gt;&gt;&gt; \n&gt;&gt;&gt; text = \'En su parte de arriba encontramos la ";zona de mandos";, donde se puede echar el detergente, aunque en nuestro caso lo al ser gel lo ponemos directamente junto con la ropa.\'\n&gt;&gt;&gt; stemmed_text = [stemmer.stem(i) for i in word_tokenize(text)]\n&gt;&gt;&gt; stemmed_text\n[u\'en\', u\'su\', u\'part\', u\'de\', u\'arrib\', u\'encontr\', u\'la\', u\'``\', u\';\', u\'zon\', u\'de\', u\'mand\', u"\'\'", u\';\', u\',\', u\'dond\', u\'se\', u\'pued\', u\'echar\', u\'el\', u\'detergent\', u\',\', u\'aunqu\', u\'en\', u\'nuestr\', u\'cas\', u\'lo\', u\'al\', u\'ser\', u\'gel\', u\'lo\', u\'pon\', u\'direct\', u\'junt\', u\'con\', u\'la\', u\'rop\', u\'.\']\n'
"&gt;&gt;&gt; from sklearn.externals import joblib\n&gt;&gt;&gt; joblib.dump(model, 'saved_model.pkl') \n\n&gt;&gt;&gt; model = joblib.load('saved_model.pkl')\n"
"from sklearn.preprocessing import LabelEncoder\nimport pandas as pd\n\ndata = pd.DataFrame()\n\ndata['age'] = [17,33,47]\ndata['gender'] = ['m','f','m']\n\nenc = LabelEncoder()\n\nprint(data)\nenc.fit(data['gender'])\ndata['gender'] = enc.transform(data['gender'])\nprint(data)\n\n   age gender\n0    17      m\n1    33      f\n2    47      m\n   age  gender\n0    17       1\n1    33       0\n2    47       1\n"
"import numpy as np\nimport random\nfrom numpy import zeros\n\nclass KMeansFK():\n    def __init__(self, K, X):\n        self.K = K\n        self.X = X\n        self.N = len(X)\n        self.mu = None\n        self.clusters = None\n        self.method = None\n\n    def _cluster_points(self):\n        mu = self.mu\n        clusters  = {}\n        for x in self.X:\n            bestmukey = min([(i[0], np.linalg.norm(x-mu[i[0]])) \\\n                             for i in enumerate(mu)], key=lambda t:t[1])[0]\n            try:\n                clusters[bestmukey].append(x)\n            except KeyError:\n                clusters[bestmukey] = [x]\n        self.clusters = clusters\n\n    def _reevaluate_centers(self):\n        clusters = self.clusters\n        newmu = []\n        keys = sorted(self.clusters.keys())\n        for k in keys:\n            newmu.append(np.mean(clusters[k], axis = 0))\n        self.mu = newmu\n\n    def _has_converged(self):\n        K = len(self.oldmu)\n        return(set([tuple(a) for a in self.mu]) == \\\n               set([tuple(a) for a in self.oldmu])\\\n               and len(set([tuple(a) for a in self.mu])) == K)\n\n    def find_centers(self, K, method='random'):\n        self.method = method\n        X = self.X\n        K = self.K\n        # https://stackoverflow.com/questions/44372231/population-must-be-a-sequence-or-set-for-dicts-use-listd\n        self.oldmu = random.sample(list(X), K)\n        if method != '++':\n            # Initialize to K random centers\n            self.mu = random.sample(list(X), K)\n        while not self._has_converged():\n            self.oldmu = self.mu\n            # Assign all points in X to clusters\n            self._cluster_points()\n            # Reevaluate centers\n            self._reevaluate_centers()\n\n    def _dist_from_centers(self):\n        cent = self.mu\n        X = self.X\n        D2 = np.array([min([np.linalg.norm(x-c)**2 for c in cent]) for x in X])\n        self.D2 = D2\n\n    def _choose_next_center(self):\n        self.probs = self.D2/self.D2.sum()\n        self.cumprobs = self.probs.cumsum()\n        r = random.random()\n        ind = np.where(self.cumprobs &gt;= r)[0][0]\n        return(self.X[ind])\n\n    def init_centers(self,K):\n        self.K = K\n        #self.mu = random.sample(self.X, 1)\n        self.mu = random.sample(list(self.X), 1)\n        while len(self.mu) &lt; self.K:\n            self._dist_from_centers()\n            self.mu.append(self._choose_next_center())\n\n    def get_ak(self,k, Nd):\n        if k == 2:\n            return( 1 - 3.0 / (4.0 * Nd ) )\n        else:\n            previous_a = self.get_ak(k-1, Nd)\n            return ( previous_a + (1.0-previous_a)/6.0 )\n\n    def fK(self, thisk, Skm1=0):\n        X = self.X\n        Nd = len(X[0])\n\n        self.find_centers(thisk, method='++')\n        mu, clusters = self.mu, self.clusters\n        Sk = sum([np.linalg.norm(mu[i]-c)**2 \\\n                 for i in range(thisk) for c in clusters[i]])\n        if thisk == 1:\n            fs = 1\n        elif Skm1 == 0:\n            fs = 1\n        else:\n            fs = Sk/(self.get_ak(thisk,Nd)*Skm1)\n        return fs, Sk\n\n    def run(self, maxk):\n        ks = range(1,maxk)\n        fs = zeros(len(ks))\n        Wks,Wkbs,sks = zeros(len(ks)+1),zeros(len(ks)+1),zeros(len(ks)+1)\n        # Special case K=1\n        self.init_centers(1)\n        fs[0], Sk = self.fK(1)\n        # Rest of Ks\n        for k in ks[1:]:\n            self.init_centers(k)\n            fs[k-1], Sk = self.fK(k, Skm1=Sk)\n        self.fs = fs\n\nX = np.array([Variable1, Variable2, Variable3, Variable4, Variable5])\nkm = kmeans.KMeansFK(2, X)\nkm.run(5)\n"
'g_W = theano.function(inputs = [ x, y ], outputs = T.grad(cost=NLL, wrt=W)) \ng_b = theano.function(inputs = [ x, y ], outputs = T.grad(cost=NLL, wrt=b))\n'
"In [236]: if np.arange(10)&gt;5:print('yes')\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-236-633002262b65&gt; in &lt;module&gt;()\n----&gt; 1 if np.arange(10)&gt;5:print('yes')\n\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n\nif (phi_a1 &gt; phi0 + c1 * alpha1 * derphi0) or \\\n"
"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nimport random\n\nX = list()\ny = list()\nfiles = list()\n\nrandom_state = 42\ntest_size = 0.9\nn = 100\n\nfor i in range(n):\n    X.append(i)\n    y.append(i + random.random())\n    files.append('file_{0:02d}.csv'.format(i))\n\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                y,\n                                                test_size=test_size,\n                                                random_state=random_state)\nX_shuffle = shuffle(X, random_state=random_state)\ny_shuffle = shuffle(y, random_state=random_state)\nfile_shuffle = shuffle(files, random_state=random_state)\n\nprint(X_train)\nprint(X_shuffle[int(n * test_size):])\nprint(y_shuffle[int(n * test_size):])\nprint(file_shuffle[int(n * test_size):])\nprint(X_train == X_shuffle[int(n * test_size):])\n\n[91, 74, 86, 82, 20, 60, 71, 14, 92, 51]\n[91, 74, 86, 82, 20, 60, 71, 14, 92, 51]\n[91.64119581793204, 74.77493553783724, 86.62410189510936, 82.40452263996107, 20.22784747831378, 60.913989700418675, 71.1940538438253, 14.644282494118647, 92.97808337955185, 51.289858815186356]\n['file_91.csv', 'file_74.csv', 'file_86.csv', 'file_82.csv', 'file_20.csv', 'file_60.csv', 'file_71.csv', 'file_14.csv', 'file_92.csv', 'file_51.csv']\n\nTrue\n"
'32 453 65 0, sample_weight = 45\n32 453 65 1, sample_weight = 55\n15 34 222 0, sample_weight = 12\n15 34 222 1, sample_weight = 88\n33 66 161 0, sample_weight = 24\n33 66 161 1, sample_weight = 76\n'
' def _extend_graph(self):\n    # Ensure any changes to the graph are reflected in the runtime.\n    with self._extend_lock:\n      if self._graph.version &gt; self._current_version:\n        # pylint: disable=protected-access\n        graph_def, self._current_version = self._graph._as_graph_def(\n            from_version=self._current_version,\n            add_shapes=self._add_shapes)\n        # pylint: enable=protected-access\n\n        with errors.raise_exception_on_not_ok_status() as status:\n          tf_session.TF_ExtendGraph(\n              self._session, graph_def.SerializeToString(), status)\n        self._opened = True\n'
'clf = LinearRegression()\n\nprint(accuracy_score(y_test, y_predict))\n\nfrom sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_test, y_predict))\n'
"inputLayer = Input(shape=(2,))\nautoencoder = Dense(4, activation='relu')(inputLayer)\nautoencoder = Dense(4, activation='relu')(autoencoder)\nautoencoder = Dense(2, activation='relu')(autoencoder) # Possible problems here\n"
"def ResBlock(n_filt, l_filt, pool):\n    conv_1 = Conv1D(n_filt, l_filt, padding='same')\n    bn = BatchNormalization()\n    dropout = Dropout(0.1)\n    conv_2 = Conv1D(n_filt, l_filt, padding='same')\n    maxpool_1 = MaxPooling1D()\n    maxpool_2 = MaxPooling1D()\n\n    def unit(x_in):\n        x = conv_1(x_in)\n        x = bn(x)\n        x = relu(x)\n        x = dropout(x)\n        x = conv_2(x)\n        if pool:\n            x = maxpool_1(x)\n            x_in = maxpool_2(x_in)\n        y = keras.layers.add([x, x_in])    \n        return y\n\n    return unit\n\nx = ResBlock(32, 16, 0)(x)\n\nresblock = ResBlock(32, 16, 0)\n\nx = resblock(x)\nx = resblock(x)\n"
's = pd.Series([1,1,2,2])\nnot pd.isnull(s.mode())\n\nfillna(self, value, method, limit)\n   1465         else:\n   1466 \n-&gt; 1467             if not isnull(value) and value not in self.categories:\n   1468                 raise ValueError("fill value must be in categories")\n   1469 \n\ntrain[\'Married\']=train[\'Married\'].fillna(train[\'Married\'].mode().iloc[0])\n\ns = pd.Series(["YES", "NO", "YES", "YES", None])    \ns1 = s.astype(\'category\')\ns1.cat.categories = [0, 1]\n\ns1\n#0    1.0\n#1    0.0\n#2    1.0\n#3    1.0\n#4    NaN\n#dtype: category\n#Categories (2, int64): [0, 1]\n\ns1.fillna(s1.mode().iloc[0])\n#0    1\n#1    0\n#2    1\n#3    1\n#4    1\n#dtype: category\n#Categories (2, int64): [0, 1]\n'
"config = tf.ConfigProto(allow_soft_placement=True)\nwith tf.Session(config=config) as sess:\n    # define the new is_training tensor\n    is_training = tf.constant(False, dtype=tf.bool, name='is_training')\n\n    # now import the graph using the .meta file of the checkpoint\n    saver = tf.train.import_meta_graph(\n    '/path/to/model.meta', input_map={'is_training:0':is_training})\n\n    # restore all weights using the model checkpoint \n    saver.restore(sess, '/path/to/model')\n\n    # save updated graph and variables values\n    saver.save(sess, '/path/to/new-model-name')\n"
'model.fit(X, Y, validation_set=0.1)\n\nmodel.fit(X_train, Y_train, validation_set=(X_test, Y_test))\n'
'&gt;&gt;&gt; clf = LogisticRegression()\n&gt;&gt;&gt; clf.fit([[1,2], [1,3], [0, 1]], [[0],[1],[0]])\n&gt;&gt;&gt; clf.coef_\narray([[ 0.02917282,  0.12584457]])\n&gt;&gt;&gt; clf.intercept_\narray([-0.40218649])\n&gt;&gt;&gt; clf.fit([[1,2], [1,3], [0, 1]], [[0],[1],[2]])\n&gt;&gt;&gt; clf.coef_\narray([[ 0.25096507, -0.24586515],\n       [ 0.02917282,  0.12584457],\n       [-0.41626058, -0.43503612]])\n&gt;&gt;&gt; clf.intercept_\narray([-0.15108918, -0.40218649,  0.1536541 ])\n'
"from sklearn.datasets import make_gaussian_quantiles;\nimport numpy as np;\n\n\nX,y = make_gaussian_quantiles(mean=None, cov=1.0, n_samples=100, n_features=2, n_classes=2, shuffle=True, random_state=5);\nY,_ = make_gaussian_quantiles(mean=None, cov=1.0, n_samples=200, n_features=2, n_classes=2, shuffle=True, random_state=2);\n\nm = X.shape[0];\nn = Y.shape[0]\n\ndef kernel(a,b,d=20,poly=True,sigma=0.5):\n    if (poly):\n        return np.inner(a,b) ** d;\n    else:\n        return np.exp(-np.linalg.norm((a - b) ** 2)/sigma**2)\n\n# Need to vectorize these loops\n\nPOLY = False\nLOW_MEM = 0\n\nK = np.array([kernel(X[i], Y[j], poly=POLY) \n              for i in range(m)\n              for j in range(n)]).reshape((m, n))\n\ndef kernel_v(X, Y=None, d=20, poly=True, sigma=0.5):\n    Z = X if Y is None else Y\n    if poly:\n        return np.einsum('ik,jk', X, Z)**d\n    elif X.shape[1] &lt; LOW_MEM:\n        return np.exp(-np.sqrt(((X[:, None, :] - Z[None, :, :])**4).sum(axis=-1)) / sigma**2)\n    elif Y is None or Y is X:\n        X2 = X*X\n        H = np.einsum('ij,ij-&gt;i', X2, X2) + np.einsum('ik,jk', X2, 3*X2) - np.einsum('ik,jk', X2*X, 4*X)\n        return np.exp(-np.sqrt(np.maximum(0, H+H.T)) / sigma**2)\n    else:\n        X2, Y2 = X*X, Y*Y\n        E = np.einsum('ik,jk', X2, 6*Y2) - np.einsum('ik,jk', X2*X, 4*Y) - np.einsum('ik,jk', X, 4*Y2*Y)\n        E += np.add.outer(np.einsum('ij,ij-&gt;i', X2, X2), np.einsum('ij,ij-&gt;i', Y2, Y2))\n        return np.exp(-np.sqrt(np.maximum(0, E)) / sigma**2)\n\nprint(np.allclose(K, kernel_v(X, Y, poly=POLY)))\n"
'saver.restore(session, model_checkpoint_path)\ninitial_global_step = global_step.assign(50)\nsession.run(initial_global_step)\n...\n# do the training\n'
"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\n# Create a dataframe of random ints\ndf = pd.DataFrame(np.random.randint(0, 4, size=(100, 4)),\n                  columns=['pattern_id', 'B', 'C', 'D'])\nonehotencoder = OneHotEncoder(categorical_features=[df.columns.tolist().index('pattern_id')])\ndf = onehotencoder.fit_transform(df)\n"
'with tf.Session() as training_sess:\n\n    training_sess.run(tf.global_variables_initializer())\n    training_sess.run(a_iterator.initializer, feed_dict = {a_placeholder_feed: training_set.data})\n    current_a = training_sess.run(next_a)   \n    training_sess.run(s_iterator.initializer, feed_dict = {s_placeholder_feed: training_set.target})\n    current_s = training_sess.run(next_s) \n\n    s_one_hot = training_sess.run(tf.one_hot((current_s - 1), number_of_s))\n\n    for i in range (1,len(hidden_layers)+1):\n        hidden_layer[i] = tf.tanh(tf.matmul(hidden_weights[i-1], (hidden_layer[i-1])) + hidden_biases[i-1])\n\n    output = tf.nn.softmax(tf.transpose(tf.matmul(hidden_weights[-1],hidden_layer[-1]) + hidden_biases[-1]))\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n    # using the AdamOptimizer does not help, nor does choosing a much bigger and smaller learning rate\n    train = optimizer.minimize(loss(s_one_hot, output))\n\n    training_sess.run(train)\n\n    for i in range (0, (number_of_p)):\n\n        current_a = training_sess.run(next_a)\n        current_s = training_sess.run(next_s)\n        s_one_hot = training_sess.run(tf.transpose(tf.one_hot((current_s - 1), number_of_s)))\n        # (no idea why I have to declare those twice for the datastream to move)\n\n        training_sess.run(train)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((inputs, targets))\n\ntrain_dataset = train_dataset.batch(batch_size)\ntrain_dataset = train_dataset.repeat(num_epochs)\niterator = train_dataset.make_one_shot_iterator()\n\nnext_inputs, next_targets = iterator.get_next()\n\n# Define Training procedure\nglobal_step = tf.Variable(0, name="global_step", trainable=False)\nloss = Neural_net_function(next_inputs, next_targets)\noptimizer = tf.train.AdamOptimizer(learning_rate)\ngrads_and_vars = optimizer.compute_gradients(loss)\ntrain_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n\n\n\nwith tf.Session() as training_sess:\n    for i in range(number_of_training_samples * num_epochs):\n            taining_sess.run(train_op)\n'
'from scipy.stats import zscore\n\nzscore(data, ddof=1)\narray([[-0.8660254, -0.8660254],\n       [-0.8660254, -0.8660254],\n       [ 0.8660254,  0.8660254],\n       [ 0.8660254,  0.8660254]])\n'
'feature_names = [\'f1\',\'f2\',\'f3\',\'f4\',\'f5\']\nrecord_defaults = [[""], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n\n\ndef decode_csv(line):\n   parsed_line = tf.decode_csv(line, record_defaults)\n   label =  parsed_line[-1]\n   del parsed_line[-1]\n   del parsed_line[0]\n   features = tf.stack(parsed_line)    # ADDED LINE\n   d = features, label\n   return d\n\nfilenames = tf.placeholder(tf.string, shape=[None])\ndataset5 = tf.data.Dataset.from_tensor_slices(filenames)\ndataset5 = dataset5.flat_map(lambda filename: tf.data.TextLineDataset(filename).skip(1).map(decode_csv))\ndataset5 = dataset5.shuffle(buffer_size=1000)\ndataset5 = dataset5.batch(7)\niterator5 = dataset5.make_initializable_iterator()\n'
'output =sess.run(y,feed_dict={x: y_pred})\n'
"dataset = pd.read_csv('estSize.csv')\n\ndataset = dataset.sort_values(by=['col1'])\n"
"from keras.callbacks import EarlyStopping # use as base class\n\nclass MyCallBack(EarlyStopping):\n    def __init__(self, threshold, min_epochs, **kwargs):\n        super(MyCallBack, self).__init__(**kwargs)\n        self.threshold = threshold # threshold for validation loss\n        self.min_epochs = min_epochs # min number of epochs to run\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        if current is None:\n            warnings.warn(\n                'Early stopping conditioned on metric `%s` '\n                'which is not available. Available metrics are: %s' %\n                (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n            )\n            return\n\n        # implement your own logic here\n        if (epoch &gt;= self.min_epochs) &amp; (current &gt;= self.threshold):\n            self.stopped_epoch = epoch\n            self.model.stop_training = True\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\nimport numpy as np\n\n# Generate some random data\nfeatures = np.random.rand(100, 5)\nlabels = np.random.rand(100, 1)\n\nvalidation_feat = np.random.rand(100, 5)\nvalidation_labels = np.random.rand(100, 1)\n\n# Define a simple model\ninput_layer = Input((5, ))\ndense_layer = Dense(10)(input_layer)\noutput_layer = Dense(1)(dense_layer)\nmodel = Model(inputs=input_layer, outputs=output_layer)\nmodel.compile(loss='mse', optimizer='sgd')\n\n# Fit with custom callback\ncallbacks = [MyCallBack(threshold=0.001, min_epochs=10, verbose=1)] \nmodel.fit(features, labels, validation_data=(validation_feat, validation_labels), callbacks=callbacks, epochs=100)   \n"
'vgg = VGG16(...)\n\ninput_img = Input(shape=...)\npreproc_img = Lambda(vgg16preprocessing)(input_img)\noutput = vgg(preproc_img)\n\nmodel = Model(input_img, output)\n'
"def make_covariance_matrix(sigma, rho):\n    return torch.tensor([[sigma[0]**2, rho * torch.prod(sigma)],\n                         [rho * torch.prod(sigma), sigma[1]**2]])\n\nimport torch\n\nparam1 = torch.rand(1, requires_grad=True)\nparam2 = torch.rand(1, requires_grad=True)\ntensor_from_params = torch.tensor([param1, param2])\n\nprint('Original parameter 1:')\nprint(param1, param1.requires_grad)\nprint('Original parameter 2:')\nprint(param2, param2.requires_grad)\nprint('New tensor form params:')\nprint(tensor_from_params, tensor_from_params.requires_grad)\n\nOriginal parameter 1:\ntensor([ 0.8913]) True\nOriginal parameter 2:\ntensor([ 0.4785]) True\nNew tensor form params:\ntensor([ 0.8913,  0.4785]) False\n\ndef make_covariance_matrix(sigma, rho):\n    conv = torch.cat([(sigma[0]**2).view(-1), rho * torch.prod(sigma), rho * torch.prod(sigma), (sigma[1]**2).view(-1)])\n    return conv.view(2, 2)\n\nBefore:\nmu: tensor([ 0.1191,  0.7215]), mu_hat: tensor([ 0.,  0.])\nsigma: tensor([ 1.4222,  1.0949]), sigma_hat: tensor([ 1.,  1.])\nrho: tensor([ 0.2558]), rho_hat: tensor([ 0.])\n\nAfter:\nmu: tensor([ 0.1191,  0.7215]), mu_hat: tensor([ 0.0712,  0.7781])\nsigma: tensor([ 1.4222,  1.0949]), sigma_hat: tensor([ 1.4410,  1.0807])\nrho: tensor([ 0.2558]), rho_hat: tensor([ 0.2235])\n"
"from sklearn.naive_bayes import MultinomialNB\nfrom imblearn.combine import SMOTEENN\n\n# Observe how I imported Pipeline from IMBLEARN and not SKLEARN\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# This pipeline will resample the data and  \n# pass the output to MultinomialNB\npipe = Pipeline([('sampl', SMOTEENN()), \n                 ('clf', MultinomialNB())])\n\n# OVR will transform the `y` as you know and \n# then pass single label data to different copies of pipe \n# multiple times (as many labels in data)\novr = OneVsRestClassifier(pipe)\novr.fit(X, y)\n"
"# create a list containing the class labels\nclass_labels = ['class1', 'class2', 'class3', ...., 'class12']\n\n# find the index of the class with maximum score\npred = np.argmax(class_labels, axis=-1)\n\n# print the label of the class with maximum score\nprint(class_labels[pred[0]])\n"
"for row in df2.iterrows():\n    if (row[1][df.columns.get_loc('nonTrivial') == True):\n        n = n+1\n\nn += (df2['nonTrivial']==True).sum()\n"
'{"images": [[[0.0, 0.0, 0.0], [0,0,0], [...]], [...], ...]}\n'
'import tensorflow as tf\n\ntf.enable_eager_execution()\n\ndata = tf.Variable([[2],\n                    [3],\n                    [4],\n                    [5],\n                    [6]])\n\ncond = tf.where(tf.less(data, 5)) # update value less than 5\nmatch_data = tf.gather_nd(data, cond)\nsquare_data = tf.square(match_data) # square value less than 5\n\ndata = tf.scatter_nd_update(data, cond, square_data)\n\nprint(data)\n\n# array([[ 4],\n#    [ 9],\n#    [16],\n#    [ 5],\n#    [ 6]], dtype=int32)&gt;\n'
'import tensorflow as tf\nimport json\n# download mnist data and split into train and test sets\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n# reshape data to fit model\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\nX_train, X_test = X_train/255, X_test/255\n# one-hot encode target column\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)\n# create model\nmodel = tf.keras.models.Sequential()\n# add model layers\nmodel.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5),\n                                 activation=\'relu\', input_shape=(28, 28, 1)))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Conv2D(64, kernel_size=(5, 5), activation=\'relu\'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(10, activation=\'softmax\'))\n# compile model using accuracy as a measure of model performance\nmodel.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\',\n              metrics=[\'accuracy\'])\n\n# train model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)\n\njson.dump({\'model\': model.to_json()}, open("model.json", "w"))\nmodel.save_weights("model_weights.h5")\n\nimport json\nimport foolbox\nfrom foolbox.attacks import FGSM\nfrom foolbox.criteria import Misclassification\nimport numpy as np\nimport tensorflow as tf\n\n\n############## Loading the model and preprocessing #####################\ntf.enable_eager_execution()\ntf.keras.backend.set_learning_phase(False)\n\nmodel = tf.keras.models.model_from_json(\n    json.load(open("model.json"))["model"], custom_objects={})\nmodel.load_weights("model_weights.h5")\nmodel.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\',\n              metrics=[\'accuracy\'])\n\n_, (images, labels) = tf.keras.datasets.mnist.load_data()\nimages = images.reshape(images.shape[0], 28, 28, 1)\nimages = images/255\nimages = images.astype(np.float32)\n\nfmodel = foolbox.models.TensorFlowEagerModel(model, bounds=(0, 1))\n\n\n######################### Attacking the model ##########################\n\nattack = foolbox.attacks.FGSM(fmodel, criterion=Misclassification())\nadversarial = np.array([attack(images[0], label=labels[0])])\n\nmodel_predictions = model.predict(adversarial)\nprint(\'real label: {}, label prediction; {}\'.format(\n    labels[0], np.argmax(model_predictions)))\n'
'm = Sequential()\nm.add(Flatten(input_shape=(943, 1)))\nm.add(Dense(912, activation=\'relu\'))\nm.add(Dense(728, activation=\'relu\'))\nm.add(Dense(528, activation=\'relu\'))\nm.add(Dense(500, activation=\'relu\', name="bottleneck"))\nm.add(Dense(528, activation=\'relu\'))\nm.add(Dense(728, activation=\'relu\'))\nm.add(Dense(943, activation=\'linear\'))\n\nm.compile(loss=\'mean_squared_error\', optimizer=\'SGD\')\nm.summary()\n\nModel: "sequential"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 943)               0         \n_________________________________________________________________\ndense (Dense)                (None, 912)               860928    \n_________________________________________________________________\ndense_1 (Dense)              (None, 728)               664664    \n_________________________________________________________________\ndense_2 (Dense)              (None, 528)               384912    \n_________________________________________________________________\nbottleneck (Dense)           (None, 500)               264500    \n_________________________________________________________________\ndense_3 (Dense)              (None, 528)               264528    \n_________________________________________________________________\ndense_4 (Dense)              (None, 728)               385112    \n_________________________________________________________________\ndense_5 (Dense)              (None, 943)               687447    \n=================================================================\nTotal params: 3,512,091\nTrainable params: 3,512,091\nNon-trainable params: 0\n\nmodel = Sequential()\nmodel.add(Dense(930, activation=\'relu\', input_shape=(943, 1)))\nmodel.add(Flatten())\nmodel.add(Dense(528, activation=\'relu\'))\nmodel.add(m.get_layer(\'bottleneck\'))\nmodel.add(Flatten())\nmodel.add(Dense(100, activation=\'linear\'))\nmodel.summary()\n\nModel: "sequential_2"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_9 (Dense)              (None, 943, 930)          1860      \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 876990)            0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 528)               463051248 \n_________________________________________________________________\nbottleneck (Dense)           (None, 500)               264500    \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 500)               0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 100)               50100     \n=================================================================\nTotal params: 463,367,708\nTrainable params: 463,367,708\nNon-trainable params: 0\n'
"sex = train_dataset['Sex'].replace(['female','male'],[0,1])\nprint(sex)\n"
'# our input tensor\nIn [50]: A = torch.tensor([0,1,2,3,0,0,1,1,2,2,3,3])\n\n# construct an intermediate boolean tensor\nIn [51]: boolean = A[:, None] == torch.unique(A)\n\nIn [52]: boolean\nOut[52]: \ntensor([[1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [1, 0, 0, 0],\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [0, 0, 0, 1]], dtype=torch.uint8)\n\nIn [53]: torch.nonzero(boolean.t())[:, -1]\nOut[53]: tensor([ 0,  4,  5,  1,  6,  7,  2,  8,  9,  3, 10, 11])\n\nIn [55]: A_large = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9])\n\nIn [56]: boolean_large = A_large[:, None] == torch.unique(A_large)\n\nIn [57]: torch.nonzero(boolean_large.t())[:, -1]\nOut[57]: \ntensor([ 0, 10, 11,  1, 12, 13,  2, 14, 15,  3, 16, 17,  4, 18, 19,  5, 20, 21,\n         6, 22, 23,  7, 24, 25,  8, 26, 27,  9, 28, 29])\n'
"class CustomLayer(tf.keras.layers.Layer):\n\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(CustomLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.kernel = self.add_weight(name='kernel', \n                                      shape=tf.TensorShape((input_shape[1], self.output_dim)),\n                                      initializer=tf.truncated_normal_initializer(stddev=0.01),\n                                      trainable=True)\n        super(CustomLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, x):\n        return tf.nn.relu(tf.matmul(x, self.kernel))\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)\n\ninputs = tf.keras.Input(shape=(32,), dtype=tf.float32)\nh = CustomLayer(output_dim=64)(inputs)\npredictions = CustomLayer(output_dim=10)(h)\nmodel = tf.keras.Model(inputs=inputs, outputs=predictions)\nmodel.compile(loss='mean_squared_error', optimizer='sgd') # sgd stands for stochastic gradient descent\nmodel.summary()\nmodel.fit(np.random.rand(100,32), np.random.rand(100,10), batch_size=32, epochs=5)\n\nX = tf.placeholder(shape=None, dtype=tf.int32)\nY = tf.placeholder(shape=None, dtype=tf.int32)\nadd = X + Y\nwith tf.Session() as sess:\n    print(sess.run(add, feed_dict={X: 2, Y: 3}))\n    # 5\n    print(sess.run(add, feed_dict={X: 10, Y: 9}))\n    # 19\n"
"import numpy as np\nimport cv2\n\nimg = cv2.imread('test_image.jpg')\nimg = cv2.resize(img, (28,28))\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimg = np.reshape(img, [1,28,28])\n\npredictions = model.predict(img)\nprint(np.argmax(predictions[0]))\n"
'out1 = tf.layers.dense(inputs=codeword, units=21, activation=None, use_bias=False)\n'
"from sklearn.ensemble import GradientBoostingRegressor\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom copy import copy\n\n\nclass Example:\n\n    def __init__(self):\n        self.X = np.random.random((10000, 9))\n        self.y = self.get_y()\n        self.clf = GradientBoostingRegressor()\n        self.fit()\n\n    def get_y(self):\n        # sum of squares, is minimum at x = [0, 0, 0, 0, 0 ... ]\n        return np.array([[self._func(i)] for i in self.X])\n\n    def _func(self, i):\n        return sum(i * i)\n\n    def fit(self):\n        self.clf.fit(self.X, self.y)\n\n    def optimize(self):\n        x0 = [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n        initial_simplex = self._get_simplex(x0, 0.1)\n        result = minimize(fun=self._call_model,\n                          x0=np.array(x0),\n                          method='Nelder-Mead',\n                          options={'xatol': 0.1,\n                                   'initial_simplex': np.array(initial_simplex)})\n        return result\n\n    def _get_simplex(self, x0, step):\n        simplex = []\n        for i in range(len(x0)):\n            point = copy(x0)\n            point[i] -= step\n            simplex.append(point)\n\n        point2 = copy(x0)\n        point2[-1] += step\n        simplex.append(point2)\n        return simplex\n\n    def _call_model(self, x):\n        prediction = self.clf.predict([x])\n        return prediction[0]\n\nexample = Example()\nresult = example.optimize()\nprint(result)\n"
'&gt;&gt;&gt; from sklearn.svm import LinearSVC\n&gt;&gt;&gt; from sklearn.exceptions import NotFittedError\n&gt;&gt;&gt; try:\n...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])\n... except NotFittedError as e:\n...     print(repr(e))\nNotFittedError("This LinearSVC instance is not fitted yet. Call \'fit\' with\nappropriate arguments before using this estimator."...)\n'
"mapping_df = data[['buying']].copy() #Create an extra dataframe which will be used to address only the encoded values\nmapping_df['buying_encoded'] = le.fit_transform(data['buying'].values) #Using values is faster than using list\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata = pd.DataFrame({'index':[0,1,2,3,4,5,6],\n        'buying':['Luffy','Nami','Luffy','Franky','Sanji','Zoro','Luffy']})\ndata['buying_encoded'] = le.fit_transform(data['buying'].values)\ndata = data.drop_duplicates('buying').set_index('index')\nprint(data)\n\n       buying  buying_encoded\nindex                        \n0       Luffy               1\n1        Nami               2\n3      Franky               0\n4       Sanji               3\n5        Zoro               4\n"
'# (1) Importing dependency\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nimport numpy as np\nnp.random.seed(1000)\n\n# (2) Get Data\nimport tflearn.datasets.oxflower17 as oxflower17\nx, y = oxflower17.load_data(one_hot=True)\n\n# (3) Create a sequential model\nmodel = Sequential()\n\n# 1st Convolutional Layer\nmodel.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11), strides=(4,4), padding=\'valid\'))\nmodel.add(Activation(\'relu\'))\n# Pooling \nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'valid\'))\n# Batch Normalisation before passing it to the next layer\nmodel.add(BatchNormalization())\n\n# 2nd Convolutional Layer\nmodel.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding=\'valid\'))\nmodel.add(Activation(\'relu\'))\n# Pooling\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'valid\'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 3rd Convolutional Layer\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\'valid\'))\nmodel.add(Activation(\'relu\'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 4th Convolutional Layer\nmodel.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding=\'valid\'))\nmodel.add(Activation(\'relu\'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 5th Convolutional Layer\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding=\'valid\'))\nmodel.add(Activation(\'relu\'))\n# Pooling\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\'valid\'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# Passing it to a dense layer\nmodel.add(Flatten())\n# 1st Dense Layer\nmodel.add(Dense(4096, input_shape=(224*224*3,)))\nmodel.add(Activation(\'relu\'))\n# Add Dropout to prevent overfitting\nmodel.add(Dropout(0.4))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 2nd Dense Layer\nmodel.add(Dense(4096))\nmodel.add(Activation(\'relu\'))\n# Add Dropout\nmodel.add(Dropout(0.4))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 3rd Dense Layer\nmodel.add(Dense(1000))\nmodel.add(Activation(\'relu\'))\n# Add Dropout\nmodel.add(Dropout(0.4))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# Output Layer\nmodel.add(Dense(17))\nmodel.add(Activation(\'softmax\'))\n\nmodel.summary()\n\n# (4) Compile \nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n\n# (5) Define Gradient Function\ndef get_gradient_func(model):\n    grads = K.gradients(model.total_loss, model.trainable_weights)\n    inputs = model.model._feed_inputs + model.model._feed_targets + model.model._feed_sample_weights\n    func = K.function(inputs, grads)\n    return func\n\n# (6) Train the model such that gradients are captured for every epoch\nepoch_gradient = []\nfor epoch in range(1,5):\n    model.fit(x, y, batch_size=64, epochs= epoch, initial_epoch = (epoch-1), verbose=1, validation_split=0.2, shuffle=True)\n    get_gradient = get_gradient_func(model)\n    grads = get_gradient([x, y, np.ones(len(y))])\n    epoch_gradient.append(grads)\n\n# (7) Convert to a 2 dimensiaonal array of (epoch, gradients) type\ngradient = np.asarray(epoch_gradient)\nprint("Total number of epochs run:", epoch)\nprint("Gradient Array has the shape:",gradient.shape)\n\nModel: "sequential_34"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_115 (Conv2D)          (None, 54, 54, 96)        34944     \n_________________________________________________________________\nactivation_213 (Activation)  (None, 54, 54, 96)        0         \n_________________________________________________________________\nmax_pooling2d_83 (MaxPooling (None, 27, 27, 96)        0         \n_________________________________________________________________\nbatch_normalization_180 (Bat (None, 27, 27, 96)        384       \n_________________________________________________________________\nconv2d_116 (Conv2D)          (None, 17, 17, 256)       2973952   \n_________________________________________________________________\nactivation_214 (Activation)  (None, 17, 17, 256)       0         \n_________________________________________________________________\nmax_pooling2d_84 (MaxPooling (None, 8, 8, 256)         0         \n_________________________________________________________________\nbatch_normalization_181 (Bat (None, 8, 8, 256)         1024      \n_________________________________________________________________\nconv2d_117 (Conv2D)          (None, 6, 6, 384)         885120    \n_________________________________________________________________\nactivation_215 (Activation)  (None, 6, 6, 384)         0         \n_________________________________________________________________\nbatch_normalization_182 (Bat (None, 6, 6, 384)         1536      \n_________________________________________________________________\nconv2d_118 (Conv2D)          (None, 4, 4, 384)         1327488   \n_________________________________________________________________\nactivation_216 (Activation)  (None, 4, 4, 384)         0         \n_________________________________________________________________\nbatch_normalization_183 (Bat (None, 4, 4, 384)         1536      \n_________________________________________________________________\nconv2d_119 (Conv2D)          (None, 2, 2, 256)         884992    \n_________________________________________________________________\nactivation_217 (Activation)  (None, 2, 2, 256)         0         \n_________________________________________________________________\nmax_pooling2d_85 (MaxPooling (None, 1, 1, 256)         0         \n_________________________________________________________________\nbatch_normalization_184 (Bat (None, 1, 1, 256)         1024      \n_________________________________________________________________\nflatten_34 (Flatten)         (None, 256)               0         \n_________________________________________________________________\ndense_99 (Dense)             (None, 4096)              1052672   \n_________________________________________________________________\nactivation_218 (Activation)  (None, 4096)              0         \n_________________________________________________________________\ndropout_66 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\nbatch_normalization_185 (Bat (None, 4096)              16384     \n_________________________________________________________________\ndense_100 (Dense)            (None, 4096)              16781312  \n_________________________________________________________________\nactivation_219 (Activation)  (None, 4096)              0         \n_________________________________________________________________\ndropout_67 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\nbatch_normalization_186 (Bat (None, 4096)              16384     \n_________________________________________________________________\ndense_101 (Dense)            (None, 1000)              4097000   \n_________________________________________________________________\nactivation_220 (Activation)  (None, 1000)              0         \n_________________________________________________________________\ndropout_68 (Dropout)         (None, 1000)              0         \n_________________________________________________________________\nbatch_normalization_187 (Bat (None, 1000)              4000      \n_________________________________________________________________\ndense_102 (Dense)            (None, 17)                17017     \n_________________________________________________________________\nactivation_221 (Activation)  (None, 17)                0         \n=================================================================\nTotal params: 28,096,769\nTrainable params: 28,075,633\nNon-trainable params: 21,136\n_________________________________________________________________\nTrain on 1088 samples, validate on 272 samples\nEpoch 1/1\n1088/1088 [==============================] - 22s 20ms/step - loss: 3.1251 - acc: 0.2178 - val_loss: 13.0005 - val_acc: 0.1140\nTrain on 1088 samples, validate on 272 samples\nEpoch 2/2\n 128/1088 [==&gt;...........................] - ETA: 1s - loss: 2.3913 - acc: 0.2656/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n  warnings.warn(\'`Sequential.model` is deprecated. \'\n1088/1088 [==============================] - 2s 2ms/step - loss: 2.2318 - acc: 0.3465 - val_loss: 9.6171 - val_acc: 0.1912\nTrain on 1088 samples, validate on 272 samples\nEpoch 3/3\n  64/1088 [&gt;.............................] - ETA: 1s - loss: 1.5143 - acc: 0.5000/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n  warnings.warn(\'`Sequential.model` is deprecated. \'\n1088/1088 [==============================] - 2s 2ms/step - loss: 1.8109 - acc: 0.4320 - val_loss: 4.3375 - val_acc: 0.3162\nTrain on 1088 samples, validate on 272 samples\nEpoch 4/4\n  64/1088 [&gt;.............................] - ETA: 1s - loss: 1.7827 - acc: 0.4688/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n  warnings.warn(\'`Sequential.model` is deprecated. \'\n1088/1088 [==============================] - 2s 2ms/step - loss: 1.5861 - acc: 0.4871 - val_loss: 3.4091 - val_acc: 0.3787\nTotal number of epochs run: 4\nGradient Array has the shape: (4, 34)\n/usr/local/lib/python3.6/dist-packages/keras/engine/sequential.py:111: UserWarning: `Sequential.model` is deprecated. `Sequential` is a subclass of `Model`, you can just use your `Sequential` instance directly.\n  warnings.warn(\'`Sequential.model` is deprecated. \'\n'
"folds = np.array_split(kdata, k) # each fold is 19 rows x 9 columns\nnp.random.shuffle(kdata) # Shuffle all rows\nfolds = np.array_split(kdata, k)\n\nfor i in range (k):\n    xtest = folds[i][:,:8] # Set ith fold to be test\n    ytest = folds[i][:,8]\n    new_folds = np.row_stack(np.delete(folds,i,0))\n    xtrain = new_folds[:, :8]\n    ytrain = new_folds[:,8]\n\n    # some print functions to help you debug\n    print(f'Fold {i}')\n    print(f'xtest shape  : {xtest.shape}')\n    print(f'ytest shape  : {ytest.shape}')\n    print(f'xtrain shape : {xtrain.shape}')\n    print(f'ytrain shape : {ytrain.shape}\\n')\n\nFold 0\nxtest shape  : (19, 8)\nytest shape  : (19,)\nxtrain shape : (76, 8)\nytrain shape : (76,)\n\nFold 1\nxtest shape  : (19, 8)\nytest shape  : (19,)\nxtrain shape : (76, 8)\nytrain shape : (76,)\n\nFold 2\nxtest shape  : (19, 8)\nytest shape  : (19,)\nxtrain shape : (76, 8)\nytrain shape : (76,)\n\nFold 3\nxtest shape  : (19, 8)\nytest shape  : (19,)\nxtrain shape : (76, 8)\nytrain shape : (76,)\n\nFold 4\nxtest shape  : (19, 8)\nytest shape  : (19,)\nxtrain shape : (76, 8)\nytrain shape : (76,)\n"
"import numpy as np\nfrom keras import models, layers\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import KFold\n\nnp.random.seed(0)\n\n# Number of features\nnumber_of_features = 100\n\n# Generate features matrix and target vector\nfeatures, target = make_classification(n_samples = 10000,\n                                       n_features = number_of_features,\n                                       n_informative = 3,\n                                       n_redundant = 0,\n                                       n_classes = 2,\n                                       weights = [.5, .5],\n                                       random_state = 0)\n\ndef create_network():\n    network = models.Sequential()\n    network.add(layers.Dense(units=16, activation='relu', input_shape=(number_of_features,)))\n    network.add(layers.Dense(units=16, activation='relu'))\n    network.add(layers.Dense(units=1, activation='sigmoid'))\n\n    network.compile(loss='binary_crossentropy', \n                    optimizer='rmsprop', \n                    metrics=['accuracy']) \n\n    return network\n\nn_splits = 3\nkf = KFold(n_splits=n_splits, shuffle=True)\n\nloss = []\nacc = []\nval_loss = []\nval_acc = []\n\n# cross validate:\nfor train_index, val_index in kf.split(features):\n    model = create_network()\n    hist = model.fit(features[train_index], target[train_index],\n                     epochs=10,\n                     batch_size=100,\n                     validation_data = (features[val_index], target[val_index]),\n                     verbose=0)\n    loss.append(hist.history['loss'])\n    acc.append(hist.history['acc'])\n    val_loss.append([hist.history['val_loss']])\n    val_acc.append(hist.history['val_acc'])\n\n[[0.7251979386058971,\n  0.6640552306833333,\n  0.6190941931069023,\n  0.5602273066015956,\n  0.48771809028534785,\n  0.40796665995284814,\n  0.33154681897220617,\n  0.2698465999525444,\n  0.227492357244586,\n  0.1998490962115201],\n [0.7109123742507104,\n  0.674812126485093,\n  0.6452083222258479,\n  0.6074533335751673,\n  0.5627432800365635,\n  0.51291748379345,\n  0.45645068427406726,\n  0.3928780094229408,\n  0.3282097149542538,\n  0.26993170230619656],\n [0.7191790426458682,\n  0.6618405645963258,\n  0.6253172250296091,\n  0.5855853647883192,\n  0.5438901918195831,\n  0.4999895181964501,\n  0.4495182811042725,\n  0.3896359298090465,\n  0.3210068798340545,\n  0.25932698793518183]]\n"
'def f(node,x,orig_label):\n    global dt,tree\n    if tree.children_left[node]==tree.children_right[node]: #Meaning node is a leaf\n        return [x] if dt.predict([x])[0]!=orig_label else [None]\n\n    if x[tree.feature[node]]&lt;=tree.threshold[node]:\n        orig = f(tree.children_left[node],x,orig_label)\n        xc = x.copy()\n        xc[tree.feature[node]] = tree.threshold[node] + .01\n        modif = f(tree.children_right[node],xc,orig_label)\n    else:\n        orig = f(tree.children_right[node],x,orig_label)\n        xc = x.copy()\n        xc[tree.feature[node]] = tree.threshold[node] \n        modif = f(tree.children_left[node],xc,orig_label)\n    return [s for s in orig+modif if s is not None]\n\ndt =  DecisionTreeClassifier(max_depth=2).fit(X,y)\ntree = dt.tree_\nres = f(0,x,dt.predict([x])[0]) # 0 is index of root node\nans = np.min([np.linalg.norm(x-n) for n in res]) \n'
'params = {\n    \'max_depth\': 6,\n    \'objective\': \'multi:softprob\',\n    \'num_class\': 3,\n    \'n_gpus\': 0\n}\npipe_xgb = Pipeline([\n    (\'clf\', xgb.XGBClassifier(**params))\n])\n\n#################################################################\n# Libraries\n#################################################################\nimport time\nimport pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport xgboost as xgb\n\n#################################################################\n# Data loading and Symlinks\n#################################################################\ntrain = pd.read_csv("https://dl.dropbox.com/s/bnomyoidkcgyb2y/data_train.csv?dl=0")\ntest = pd.read_csv("https://dl.dropbox.com/s/kn1bgde3hsf6ngy/data_test.csv?dl=0")\n\n#################################################################\n# Train Test Split\n#################################################################\n# Selected features - Training data\nX = train.drop(columns=\'fault_severity\')\n\n# Training data\ny = train.fault_severity\n\n# Test data\nx = test\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\n\n#################################################################\n# Pipeline\n#################################################################\nparams = {\n    \'max_depth\': 6,\n    \'objective\': \'multi:softprob\',  # error evaluation for multiclass training\n    \'num_class\': 3,\n    \'n_gpus\': 0\n}\npipe_xgb = Pipeline([\n    (\'clf\', xgb.XGBClassifier(**params))\n    ])\n\nparameters_xgb = {\n        \'clf__n_estimators\':[30,40], \n        \'clf__criterion\':[\'entropy\'], \n        \'clf__min_samples_split\':[15,20], \n        \'clf__min_samples_leaf\':[3,4]\n    }\n\ngrid_xgb = GridSearchCV(pipe_xgb,\n    param_grid=parameters_xgb,\n    scoring=\'accuracy\',\n    cv=5,\n    refit=True)\n\n#################################################################\n# Modeling\n#################################################################\nstart_time = time.time()\n\ngrid_xgb.fit(X_train, y_train)\n\n#Calculate the score once and use when needed\nacc = grid_xgb.score(X_valid,y_valid)\n\nprint("Best params                        : %s" % grid_xgb.best_params_)\nprint("Best training data accuracy        : %s" % grid_xgb.best_score_)    \nprint("Best validation data accuracy (*)  : %s" % acc)\nprint("Modeling time                      : %s" % time.strftime("%H:%M:%S", time.gmtime(time.time() - start_time)))\n\n#################################################################\n# Prediction\n#################################################################\n#Predict using the test data with selected features\ny_pred = grid_xgb.predict(X_valid)\n\n# Transform numpy array to dataframe\ny_pred = pd.DataFrame(y_pred)\n\n# Rearrange dataframe\ny_pred.columns = [\'prediction\']\ny_pred.insert(0, \'id\', x[\'id\'])\naccuracy_score(y_valid, y_pred.prediction)\n\ny_pred = pd.DataFrame(grid_xgb.predict_proba(X_valid),\n                      columns=[\'prediction_0\', \'prediction_1\', \'prediction_2\'])\ny_pred.insert(0, \'id\', x[\'id\'])\n\n      id  prediction_0  prediction_1  prediction_2\n0  11066      0.490955      0.436085      0.072961\n1  18000      0.718351      0.236274      0.045375\n2  16964      0.920252      0.052558      0.027190\n3   4795      0.958216      0.021558      0.020226\n4   3392      0.306204      0.155550      0.538246\n'
'features = convert_examples_to_tf_dataset(test_examples, tokenizer)\n\nfeatures = features.batch(BATCH_SIZE)\n'
'# Create Inception Res Net model as used in paper\n\nresnet = tf.keras.applications.inception_resnet_v2.InceptionResNetV2()\n\nprint("Layers of ResNet: "+str(len(resnet.layers))) //782 layers\n\nx = resnet.layers[-28].output\n\nx = tf.keras.layers.Dropout(0.25)(x)\n\n### Edit here.\nx = tf.keras.layers.Flatten()(x)\n# Make a prediction layer with 7 nodes for the 7 dir in our train_dir.\npredictions_layer = tf.keras.layers.Dense(7, activation=\'softmax\')(x)\n\n# print(resnet.input)\n\n# inputs=resnet.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = tf.keras.Model(inputs=resnet.input, outputs=predictions_layer)\n'
"X = np.random.randint(0,10, (1000,100))\ny = np.random.randint(0,3, 1000)\n\nmodel = Sequential([\n    Dense(128, input_dim = 100),\n    Dense(3, activation='softmax'),\n])\nmodel.summary()\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(X, y, epochs=3)\n\nX = np.random.randint(0,10, (1000,100))\ny = pd.get_dummies(np.random.randint(0,3, 1000)).values\n\nmodel = Sequential([\n    Dense(128, input_dim = 100),\n    Dense(3, activation='softmax'),\n])\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(X, y, epochs=3)\n"
"!export PYTHONPATH=/content/pegasus\n\nimport os\nos.environ['PYTHONPATH'] += ':/content/pegasus'\n"
"f(x) = max(-1, min(x, 1))\n\nf'(x) = 1 if -1 &lt; x &lt; 1\n        0 otherwise\n"
'# UNTESTED CODE, may contain a bug or two; also, you need to decide how to\n# implement split_words\ndatareader = csv.reader(csvfile)\ndicts = []\ny = []\n\nfor row in datareader:\n    y.append(row[-1])\n    d = {"From": row[0]}\n    for word in split_words(row[1]):\n        d["Subject_" + word] = 1\n    for word in split_words(row[2]):\n        d["Body_" + word] = 1\n    # etc.\n    dicts.append(d)\n\n# vectorize!\nvectorizer = DictVectorizer()\nX_train = vectorizer.fit_transform(dicts)\n'
"&gt;&gt;&gt; hasher = sklearn.feature_extraction.FeatureHasher(n_features=10,\n...                                                   non_negative=True,\n...                                                   input_type='dict')\n&gt;&gt;&gt; X_new = hasher.fit_transform([{'a':1, 'b':2}, {'a':0, 'c':5}])\n&gt;&gt;&gt; X_new.toarray()\narray([[ 1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.]])\n\n&gt;&gt;&gt; hasher = sklearn.feature_extraction.FeatureHasher(n_features=10,\n...                                                   non_negative=True,\n...                                                   input_type='pair')\n&gt;&gt;&gt; X_new = hasher.fit_transform([[('a', 1), ('b', 2)], [('a', 0), ('c', 5)]])\n&gt;&gt;&gt; X_new.toarray()\narray([[ 1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.,  0.,  0.]])\n\n&gt;&gt;&gt; hasher = sklearn.feature_extraction.FeatureHasher(n_features=10,\n...                                                   non_negative=True,\n...                                                   input_type='string')\n&gt;&gt;&gt; X_new = hasher.fit_transform([['a', 'b'], ['a', 'c']])\n&gt;&gt;&gt; X_new.toarray()\narray([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.]])\n"
"n = 200\np = 2\nX = np.random.normal(0, 1, (n, p))\ny = X[:,0]* X[:, 1] + np.random.normal(0, .1, n)\ny.shape = (n, 1)\n\nds = DenseDesignMatrix(X=X, y=y)\n\n\nhidden_layer = mlp.Sigmoid(layer_name='hidden', dim=10, irange=.1, init_bias=1.)\noutput_layer = mlp.Linear(dim=1, layer_name='y', irange=.1)\ntrainer = sgd.SGD(learning_rate=.05, batch_size=10, \n                  termination_criterion=EpochCounter(200))\nlayers = [hidden_layer, output_layer]\nann = mlp.MLP(layers, nvis=2)\ntrainer.setup(ann, ds)\n\nwhile True:\n    trainer.train(dataset=ds)\n    ann.monitor.report_epoch()\n    ann.monitor()\n    if not trainer.continue_learning(ann):\n        break\n\ninputs = X \ny_est = ann.fprop(theano.shared(inputs, name='inputs')).eval()\n"
'Parameters: \nn_estimators : integer, optional (default=10)\nThe number of trees in the forest.\n\nRandomForestClassifier(n_estimators=25, n_jobs=2)\n\nThe predicted class probabilities of an input sample is computed as the\nmean predicted class probabilities of the trees in the forest. The class\nprobability of a single tree is the fraction of samples of the same \nclass in a leaf.\n'
'A1, A2, B1, B2, C1, C2 = range(6)\n\ndef isAdjacent(actual, classifierOutput):\n    return actual - 2 &lt; classifierOutput &lt; actual + 2\n'
'from pyspark.ml.feature import Word2Vec\n\n+----+--------------------+\n|word|              vector|\n+----+--------------------+\n|   a|[-0.3511952459812...|\n|   b|[0.29077222943305...|\n|   c|[0.02315592765808...|\n+----+--------------------+\n\n+----+-------------------+\n|word|         similarity|\n+----+-------------------+\n|   b|0.29255685145799626|\n|   c|-0.5414068302988307|\n+----+-------------------+\n'
'from neon.backends import gen_backend\n\nbe = gen_backend()\n(...)\n'
'df = sel.fit_transform(X)\n\ndf_prediction = sel.transform(X_prediction)\n'
"import numpy as np\n\nnp.random.seed([3, 1415])\nx = np.random.rand(1000000, 1)\ny = np.random.rand(1000000, 1)\n\n%%timeit\ndiff = x - y\ndiff.T.dot(diff)\n\n%%timeit\ndiff = x - y\nnp.square(diff).sum()\n\nidx = pd.Index(np.arange(1000, 501000, 1000)).sort_values()\nddd = pd.DataFrame(index=idx, columns=['dot', 'dif'])\n\ndef fill_ddd(row):\n    i = row.name\n    x = np.random.rand(i, 1)\n\n    s = pd.datetime.now()\n    for _ in range(100):\n        x.T.dot(x)\n    row.loc['dot'] = (pd.datetime.now() - s).total_seconds() / 100.\n\n    s = pd.datetime.now()\n    for _ in range(100):\n        np.square(x).sum()\n    row.loc['dif'] = (pd.datetime.now() - s).total_seconds() / 100.\n\n    return row\n\n\nnp.random.seed([3, 1415])\n\nddd.apply(fill_ddd, axis=1)    \n\n\nddd.plot()\n"
"C = diag([0,1,1,0]) * A * B1 + diag([1,0,0,1]) * A * B2\n\nk = 2\nn = 4\np = 3\nq = 2\na = array([[1, 0, 1],\n           [0, 0, 1],\n           [1, 1, 0],\n           [0, 1, 0]])\nindex_input = [1, 0, 0, 1]\n\nimport tensorflow as tf\n\n# Creates a dim·dim tensor having the same vector 'vector' in every row\ndef square_matrix(vector, dim):\n    return tf.reshape(tf.tile(vector,[dim]), [dim,dim])\n\nA = tf.placeholder(tf.float32, [None, p])\nB = tf.Variable(tf.random_normal(shape=[k,p,q]))\n# For the first example (with k=2): B = tf.constant([[[1, 1],[2, 1],[3, 6]],[[1, 5],[3, 2],[0, 2]]], tf.float32)\nC = tf.Variable(tf.zeros((n, q)))\nI = tf.placeholder(tf.int32,[None])\n\n# Create a n·n tensor 'indices_matrix' having indices_matrix[i]=I for 0&lt;=i&lt;n (each row vector is I)\nindices_matrix = square_matrix(I, n)\n\n# Create a n·n tensor 'row_matrix' having row_matrix[i]=[i,...,i] for 0&lt;=i&lt;n (each row vector is a vector of i's)\nrow_matrix = tf.transpose(square_matrix(tf.range(0, n, 1), n))\n\n# Find diagonal values by comparing tensors indices_matrix and row_matrix\nequal = tf.cast(tf.equal(indices_matrix, row_matrix), tf.float32)\n\n# Compute C\nfor i in range(k):\n    diag = tf.diag(tf.gather(equal, i))\n    mul = tf.matmul(diag, tf.matmul(A, tf.gather(B, i)))\n    C = C + mul\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nprint(sess.run(C, feed_dict={A : a, I : index_input}))\n"
'net = ops.conv2d(net, 2048, [1, 1])\n'
'out_layer = tf.nn.tanh(out_layer)\n'
'from sklearn.preprocessing import FunctionTransformer, maxabs_scale\nfrom scipy.sparse import csc_matrix\nimport numpy as np\nlogtran = FunctionTransformer(np.log1p, accept_sparse=True)\nX = csc_matrix([[ 1., 0, 8], [ 2., 0,  0], [ 0,  1., 2]])\nY = maxabs_scale(logtran.transform(X))\n\n  (0, 0)        0.630929753571\n  (1, 0)        1.0\n  (2, 1)        1.0\n  (0, 2)        1.0\n  (2, 2)        0.5\n'
"np.tensordot(A,W,axes=((1),(0))).swapaxes(1,2)\nnp.tensordot(A,W,axes=((1),(0))).reshape(A.shape[0],W.shape[1],1)\n\nA[:,:,0].dot(W)[...,None]\n\nnp.einsum('ijk,jl-&gt;ilk',A,W)\n"
"import tensorflow as tf\n\nfeatures = tf.placeholder(tf.float32, [None, 3])\nlabels = tf.placeholder(tf.float32, [None,1])\n\n#Random weights\nW = tf.Variable([[10.0], [000.0], [0.200]], tf.float32)\ninit = tf.initialize_all_variables()\nwith tf.Session() as sess:\n    sess.run(init)\n\n    predict = tf.nn.sigmoid(tf.matmul(features, W))\n\n    print(sess.run(predict, feed_dict={features:[[0, 1, 1]]}))\n\n    lbls= [[0], [1], [1], [0]]\n    print(sess.run(predict,\n                 feed_dict={features: [[0, 1, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]], labels:lbls}))\n\n\n    #    error = labels - predict\n    error = tf.reduce_mean((labels - predict)**2) \n    # Training\n    optimizer = tf.train.GradientDescentOptimizer(10)\n    train = optimizer.minimize(error)\n\n    for i in range(100):\n        sess.run(train,\n        feed_dict={features: [[0, 1, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]], labels: lbls})\n        training_cost = sess.run(error,\n                             feed_dict={features: [[0, 1, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]],\n                                        labels: lbls})\n        classe = sess.run((labels-predict),\n                             feed_dict={features: [[0, 1, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]],\n                                        labels: lbls})\n        print('Training cost = ', training_cost, 'W = ', classe)\n\n    print(sess.run(predict,\n                 feed_dict={features: [[0, 1, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]]}))\n"
'user_train = pd.read_csv("input.csv")\nuser_features = user_train.columns[1:]\ny_train = user_train[user_features]\nuser_y = user_train[\'label\']\n\ninput_train = pd.read_csv("input.csv")    \ninput_features = user_train.columns[1:]    \ninput_data = user_train[input_features]    \ninput_labels = user_train[\'label\']\ndata_train, data_test, labels_train, labels_test = model_selection.train_test_split(input_data/255.,input_labels,test_size=1,random_state=0)\n\nclf_rf = RandomForestClassifier()\nclf_rf.fit(data_train, labels_train)\nlabels_pred_rf = clf_rf.predict(data_test)\nacc_rf = accuracy_score(labels_test, labels_pred_rf)\n\ninput_train = pd.read_csv("input.csv")    \ninput_features = user_train.columns[1:]    \ninput_data = user_train[input_features]    \ninput_labels = user_train[\'label\']\n\nlabels_pred_rf = clf_rf.predict(input_data)\nacc_rf = accuracy_score(input_labels, labels_pred_rf)\n'
'submission_df[[\'question1\',\'question2\']]=submission_df[[\'question1\',\'question2\']].astype(str).apply(lambda row: [nltk.word_tokenize(row[\'question1\']),nltk.word_tokenize(row[\'question2\'])], axis=1)\n\nimport nltk\ndf = pd.DataFrame({"A":["Nice to meet you ","Nice to meet you ","Nice to meet you ",8,9,10],"B":[7,6,7,"Nice to meet you ","Nice to meet you ","Nice to meet you "]})\ndf[[\'A\',\'B\']] = df[[\'A\',\'B\']].astype(str).apply(lambda row: [nltk.word_tokenize(row[\'A\']),nltk.word_tokenize(row[\'B\'])], axis=1)\n'
'def metric(real,predictions)\n\ndef metric(predictions,real)\n\ndef rmsle(real, predicted):\n    sum=0.0\n    for x in range(len(predicted)):\n        if predicted[x]&lt;0 or real[x]&lt;0: #check for negative values\n            continue\n        p = np.log(predicted[x]+1)\n        r = np.log(real[x]+1)\n        sum = sum + (p - r)**2\n    return (sum/len(predicted))**0.5\n'
"images = []\nfor i in range(many):\n    images[i] = load_img(i) # here's the first image\n\nx = np.array(images) # joint them all together into a second copy\n\nx = np.zeros((many, 250, 500, 3)\nfor i in range(many):\n    x[i] = load_img(i)\n\nx0 = load_img(0)\nx = np.zeros((many,) + x0.shape, x0.dtype)\nfor i in range(1, many):\n    x[i] = load_img(i)\n"
'predictions = knn.predict(X_validation)\n\npd.DataFrame({"x1": [1,4,2,1,4,1], "x2": [7,9,7,7,6,8], ...})\n\ndatapoint = pd.DataFrame({"x1": [1], "x2": [8], ...})\n\ndatapoint_predict = knn.predict(datapoint)\n'
'from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\np1 = Pipeline([[\'clf1\', RandomForestClassifier()]])\np2 = Pipeline([[\'clf2\', AdaBoostClassifier()]])\np3 = Pipeline([[\'clf3\', VotingClassifier(estimators=[("p1",p1), ("p2",p2)])]])\np3.get_params()\n\n{\'clf3\': VotingClassifier(estimators=[(\'p1\', Pipeline(steps=[[\'clf1\', RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n             max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n             min_impurity_split=1e-07, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction...SAMME.R\', base_estimator=None,\n           learning_rate=1.0, n_estimators=50, random_state=None)]]))],\n          n_jobs=1, voting=\'hard\', weights=None),\n \'clf3__estimators\': [(\'p1\',\n   Pipeline(steps=[[\'clf1\', RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n               max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n               min_impurity_split=1e-07, min_samples_leaf=1,\n               min_samples_split=2, min_weight_fraction_leaf=0.0,\n               n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n               verbose=0, warm_start=False)]])),\n  (\'p2\',\n   Pipeline(steps=[[\'clf2\', AdaBoostClassifier(algorithm=\'SAMME.R\', base_estimator=None,\n             learning_rate=1.0, n_estimators=50, random_state=None)]]))],\n \'clf3__n_jobs\': 1,\n \'clf3__p1\': Pipeline(steps=[[\'clf1\', RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n             max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n             min_impurity_split=1e-07, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n             verbose=0, warm_start=False)]]),\n \'clf3__p1__clf1\': RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n             max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n             min_impurity_split=1e-07, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n             verbose=0, warm_start=False),\n \'clf3__p1__clf1__bootstrap\': True,\n \'clf3__p1__clf1__class_weight\': None,\n \'clf3__p1__clf1__criterion\': \'gini\',\n \'clf3__p1__clf1__max_depth\': None,\n \'clf3__p1__clf1__max_features\': \'auto\',\n \'clf3__p1__clf1__max_leaf_nodes\': None,\n \'clf3__p1__clf1__min_impurity_split\': 1e-07,\n \'clf3__p1__clf1__min_samples_leaf\': 1,\n \'clf3__p1__clf1__min_samples_split\': 2,\n \'clf3__p1__clf1__min_weight_fraction_leaf\': 0.0,\n \'clf3__p1__clf1__n_estimators\': 10,\n \'clf3__p1__clf1__n_jobs\': 1,\n \'clf3__p1__clf1__oob_score\': False,\n \'clf3__p1__clf1__random_state\': None,\n \'clf3__p1__clf1__verbose\': 0,\n \'clf3__p1__clf1__warm_start\': False,\n \'clf3__p1__steps\': [[\'clf1\',\n   RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n               max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n               min_impurity_split=1e-07, min_samples_leaf=1,\n               min_samples_split=2, min_weight_fraction_leaf=0.0,\n               n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n               verbose=0, warm_start=False)]],\n \'clf3__p2\': Pipeline(steps=[[\'clf2\', AdaBoostClassifier(algorithm=\'SAMME.R\', base_estimator=None,\n           learning_rate=1.0, n_estimators=50, random_state=None)]]),\n \'clf3__p2__clf2\': AdaBoostClassifier(algorithm=\'SAMME.R\', base_estimator=None,\n           learning_rate=1.0, n_estimators=50, random_state=None),\n \'clf3__p2__clf2__algorithm\': \'SAMME.R\',\n \'clf3__p2__clf2__base_estimator\': None,\n \'clf3__p2__clf2__learning_rate\': 1.0,\n \'clf3__p2__clf2__n_estimators\': 50,\n \'clf3__p2__clf2__random_state\': None,\n \'clf3__p2__steps\': [[\'clf2\',\n   AdaBoostClassifier(algorithm=\'SAMME.R\', base_estimator=None,\n             learning_rate=1.0, n_estimators=50, random_state=None)]],\n \'clf3__voting\': \'hard\',\n \'clf3__weights\': None,\n \'steps\': [[\'clf3\',\n   VotingClassifier(estimators=[(\'p1\', Pipeline(steps=[[\'clf1\', RandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n               max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n               min_impurity_split=1e-07, min_samples_leaf=1,\n               min_samples_split=2, min_weight_fraction...SAMME.R\', base_estimator=None,\n             learning_rate=1.0, n_estimators=50, random_state=None)]]))],\n            n_jobs=1, voting=\'hard\', weights=None)]]}\n'
'min_shape = (20, 345)\nMFCCs = [arr1, arr2, arr3, ...]    \n\nfor idx, arr in enumerate(MFCCs):\n    MFCCs[idx] = arr[:, :min_shape[1]]\n\nbatch_arr = np.array(MFCCs)\n\nIn [33]: a1 = np.random.randn(2, 3)    \nIn [34]: a2 = np.random.randn(2, 5)    \nIn [35]: a3 = np.random.randn(2, 10)\n\nIn [36]: MFCCs = [a1, a2, a3]\n\nIn [37]: min_shape = (2, 2)\n\nIn [38]: for idx, arr in enumerate(MFCCs):\n    ...:     MFCCs[idx] = arr[:, :min_shape[1]]\n    ...:     \n\nIn [42]: batch_arr = np.array(MFCCs)\n\nIn [43]: batch_arr.shape\nOut[43]: (3, 2, 2)\n'
"model.add(layers.Dense(32, activation='relu', input_shape=(10,)))\n\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, input_shape=(10,)))\nmodel.add(layers.Dense(1))\n\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(32, input_shape=(2,)))\nmodel.add(layers.Dense(1))\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nprint model.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ndense_1 (Dense)              (None, 32)                96\n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33\n=================================================================\nTotal params: 129\nTrainable params: 129\nNon-trainable params: 0\n_________________________________________________________________\n\noutput = dot(W, input) + b\n\noutput = relu(dot(W, input) + b) #relu here is the activation function\n\n96 = 32 (Hidden Units) * 2 (Data Dimension) + 32 (Bias Value Same as Hidden Units)\n\n33 = 1 (Hidden Units) * 32 (Data Dimension) + 1 (Bias Value Same as Hidden Units)\n\nTotal Params = 96+33 = 129\n"
'----------------------------- [SVR_rbf] ------------------------------\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n---------------------------- [SVR_linear] ----------------------------\nFitting 3 folds for each of 4 candidates, totalling 12 fits\n------------------------------ [Ridge] -------------------------------\nFitting 3 folds for each of 7 candidates, totalling 21 fits\n------------------------------ [Lasso] -------------------------------\nFitting 3 folds for each of 6 candidates, totalling 18 fits\n--------------------------- [RandomForest] ---------------------------\nFitting 3 folds for each of 3 candidates, totalling 9 fits\n----------------------------- [SVR_rbf] ------------------------------\nScore:      44.88%\nParameters: {\'SVR_rbf__C\': 10, \'SVR_rbf__max_iter\': 500}\n**********************************************************************\n---------------------------- [SVR_linear] ----------------------------\nScore:      33.40%\nParameters: {\'SVR_linear__C\': 0.01, \'SVR_linear__max_iter\': 1000}\n**********************************************************************\n------------------------------ [Ridge] -------------------------------\nScore:      34.83%\nParameters: {\'Ridge__alpha\': 500, \'Ridge__max_iter\': 200}\n**********************************************************************\n------------------------------ [Lasso] -------------------------------\nScore:      22.90%\nParameters: {\'Lasso__alpha\': 0.1, \'Lasso__max_iter\': 1000}\n**********************************************************************\n--------------------------- [RandomForest] ---------------------------\nScore:      36.87%\nParameters: {\'RandomForest__max_depth\': 5, \'RandomForest__n_estimators\': 250}\n**********************************************************************\nMean Squared Error: {\'SVR_rbf\': 5.375, \'SVR_linear\': 7.036, \'Ridge\': 7.02, \'Lasso\': 8.108, \'RandomForest\': 9.475}\n\nimport os\n#import contextlib\nfrom operator import itemgetter\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDRegressor, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.externals import joblib\n\n\ndef get_data(path=\'.\'):\n    p = Path(path)\n    kwargs = dict(delim_whitespace=True, header=None)\n    X_train = pd.read_csv(list(p.glob(\'trainX.txt*\'))[0], **kwargs)\n    y_train = pd.read_csv(list(p.glob(\'trainY.txt*\'))[0], **kwargs)\n    X_test = pd.read_csv(list(p.glob(\'testX.txt*\'))[0], **kwargs)\n    y_test = pd.read_csv(list(p.glob(\'testY.txt*\'))[0], **kwargs)\n    return (pd.concat([X_train, X_test], ignore_index=True),\n            pd.concat([y_train, y_test], ignore_index=True)[0])\n\n\ndef get_data_split(path=\'.\', test_size=0.25):\n    X, y = get_data(path)\n    return train_test_split(X, y, test_size=test_size)\n\n\ndef tune_models_hyperparams(X, y, models, **common_grid_kwargs):\n    grids = {}\n    for model in models:\n        print(\'{:-^70}\'.format(\' [\' + model[\'name\'] + \'] \'))\n        pipe = Pipeline([\n                    ("scale", StandardScaler()),\n                    (model[\'name\'], model[\'model\'])   ])\n        grids[model[\'name\']] = (GridSearchCV(pipe,\n                                           param_grid=model[\'param_grid\'],\n                                           **common_grid_kwargs)\n                                  .fit(X, y))\n        # saving single trained model ...\n        joblib.dump(grids[model[\'name\']], \'./{}.pkl\'.format(model[\'name\']))\n    return grids\n\n\ndef get_best_model(grid, X_test, y_test,\n                        metric_func=mean_squared_error):\n    res = {name : round(metric_func(y_test, model.predict(X_test)), 3)\n           for name, model in grid.items()}\n    print(\'Mean Squared Error:\', res)\n    best_model_name = min(res, key=itemgetter(1))\n    return grid[best_model_name]\n\n\ndef test_dataset(grid, X_test, y_test):\n    res = {}\n    for name, model in grid.items():\n        y_pred = model.predict(X_test)\n        res[name] = {\'MSE\': mean_squared_error(y_test, y_pred),\n                       \'R2\': r2_score(y_test, y_pred)\n                      }\n    return res\n\ndef predict(grid, X_test, model_name):\n    return grid[model_name].predict(X_test)\n\n\ndef print_grid_results(grids):\n    for name, model in grids.items():\n        print(\'{:-^70}\'.format(\' [\' + name + \'] \'))\n        print(\'Score:\\t\\t{:.2%}\'.format(model.best_score_))\n        print(\'Parameters:\\t{}\'.format(model.best_params_))\n        print(\'*\' * 70)\n\n\nmodels = [\n    {   \'name\':     \'SVR_rbf\',\n        \'model\':    SVR(),\n        \'title\':    "SVR_rbf",\n        \'param_grid\': {\n            \'SVR_rbf__C\':           [0.1, 1, 5, 10],\n            \'SVR_rbf__max_iter\':    [500]\n        } \n    },\n    {   \'name\':     \'SVR_linear\',\n        \'model\':      SVR(kernel=\'linear\'),\n        \'title\':    "SVR_rbf",\n        \'param_grid\': {\n            \'SVR_linear__C\':           [0.01, 0.1, 1, 5],\n            \'SVR_linear__max_iter\':    [1000]\n        } \n    },\n    {   \'name\':     \'Ridge\',\n        \'model\':    Ridge(),\n        \'title\':    "Ridge",\n        \'param_grid\': {\n            \'Ridge__alpha\':         [0.1, 0.5, 5, 10, 50, 100, 500],\n            \'Ridge__max_iter\':      [200]\n        } \n    },\n    {   \'name\':     \'Lasso\',\n        \'model\':    Lasso(),\n        \'title\':    "Lasso",\n        \'param_grid\':  {\n            \'Lasso__alpha\':         [0.0001, 0.001, 0.01, 0.1, 1, 10],\n            \'Lasso__max_iter\':      [1000]\n        } \n    },\n    {   \'name\':     \'RandomForest\',\n        \'model\':    RandomForestRegressor(),\n        \'title\':    "RandomForest",\n        \'param_grid\':  {\n            \'RandomForest__n_estimators\':   [50, 250, 500],\n            \'RandomForest__max_depth\':      [5],\n        } \n    },\n]\n\n\ndef main(path):\n    os.chdir(str(path))\n\n    X_train, X_test, y_train, y_test = \\\n        get_data_split(path, test_size=127/510.)\n    grid = tune_models_hyperparams(X_train, y_train, models, cv=3,\n                                   verbose=2, n_jobs=-1)\n    print_grid_results(grid)\n    model = get_best_model(grid, X_test, y_test)\n    df = pd.DataFrame({\'predicted\': model.predict(X_test)})\n    df.to_csv(\'predicted.csv\', index=False)\n\nif __name__ == "__main__":\n    p =  Path(__file__).parent.resolve()\n    main(p)\n'
"df_full = df_full[df_full.filter(regex='^(?!Unnamed)').columns]\n"
'model.add(Conv1D(5, 5, input_shape=(df_x.shape[1], 1)))\n'
'...\nlogits = tf.nn.softmax(...)\nlabels = tf.one_hot(q_actions, n_actions)\nloss = tf.losses.softmax_cross_entropy(labels, logits, weights=q_rewards)\n\nloss[i] = -q_rewards[i] * tf.log( tf.nn.softmax( logits[i] ) )\n'
"def get_integer_mapping(le):\n    '''\n    Return a dict mapping labels to their integer values\n    from an SKlearn LabelEncoder\n    le = a fitted SKlearn LabelEncoder\n    '''\n    res = {}\n    for cl in le.classes_:\n        res.update({cl:le.transform([cl])[0]})\n\n    return res\n\nclasses = ['blue', 'green']\nle = LabelEncoder()\nle.fit(classes)\nintegerMapping = get_integer_mapping(le)\n\nintegerMapping['blue']  # Returns 0\nintegerMapping['green']  # Returns 1\n\n%timeit get_integer_mapping(le)\n1 loop, best of 3: 17.1 s per loop\n"
"y_resampled = label_binarize(y_resampled, classes=['Good','Bad','Ok'])\nn_classes = y_resampled.shape[1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                random_state=0)\n\n# First divide the data into train test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                    random_state=0)\n\n# Then only resample the training data\nX_resampled, y_resampled = SMOTE().fit_sample(X_train, y_train)\n\n# Then label binarize them to be used in multi-class roc\ny_resampled = label_binarize(y_resampled, classes=['Good','Bad','Ok'])\n\n# Do this to the test data too\ny_test = label_binarize(y_test, classes=['Good','Bad','Ok'])\n\ny_score=classifier.fit(X_resampled, y_resampled).predict_proba(X_test)\n\n# Then you can do this and other parts of code\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n"
'X_tf_r = Reshape((n_timesteps, -1))(X_tf)\nembeddings_r = Reshape((n_timesteps, -1))(embeddings)\n\nconcat = concatenate([X_tf_r, embeddings_r])\nlow_encoder_out = TimeDistributed(AutoregressiveDecoder(...))(concat)\n'
'x = self.pool(F.relu(self.conv1(x)))\nx = self.pool(F.relu(self.conv2(x)))\n\ndef forward(self, x):\n    x = self.pool(F.relu(self.conv1(x)))\n    x = F.relu(self.conv2(x))\n    x = x.view(-1, 16 *44*44)\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = self.fc3(x)\n    return x\n\ndef __init__(self):\n    super(Net, self).__init__()\n    self.conv1 = nn.Conv2d(3, 6, 5)\n    self.pool = nn.MaxPool2d(2, 2)\n    self.conv2 = nn.Conv2d(6, 16, 5)\n    self.fc1 = nn.Linear(22*22*16, 120)\n    self.fc2 = nn.Linear(120, 84)\n    self.fc3 = nn.Linear(84, 10)\n'
"model.set_weights(weights)\n\nmodel.layers[layer_index].set_weights([layer_weights, layer_biases])\n\n# or using layer's name if you have specified names for them\nmodel.get_layer(layer_name).set_weights([layer_weights, layer_biases])\n"
"op = tf.group(a,b)\n\ntf.add_to_collection('coll', a)\ntf.add_to_collection('coll', b)\n"
'X[col].fillna(0)\n\nX.loc[:,col] = X[col].fillna(0)\n'
"aggregator = ('agg', Aggregator())`\n\npipe = Pipeline(steps=[\n                      ('preprocessor', preprocessor),\n                      ('aggregator', aggregator),\n])\n"
'from sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\niris = load_iris()\nX = iris.data\n\nsil_score_max = -1 #this is the minimum possible score\n\nfor n_clusters in range(2,10):\n  model = KMeans(n_clusters = n_clusters, init=\'k-means++\', max_iter=100, n_init=1)\n  labels = model.fit_predict(X)\n  sil_score = silhouette_score(X, labels)\n  print("The average silhouette score for %i clusters is %0.2f" %(n_clusters,sil_score))\n  if sil_score &gt; sil_score_max:\n    sil_score_max = sil_score\n    best_n_clusters = n_clusters\n\nThe average silhouette score for 2 clusters is 0.68\nThe average silhouette score for 3 clusters is 0.55\nThe average silhouette score for 4 clusters is 0.50\nThe average silhouette score for 5 clusters is 0.49\nThe average silhouette score for 6 clusters is 0.36\nThe average silhouette score for 7 clusters is 0.46\nThe average silhouette score for 8 clusters is 0.34\nThe average silhouette score for 9 clusters is 0.31\n'
'for i in range(n_iterations):\n    y_hat = X_train1.dot(theta)\n    error = y_hat - y_train[:, None]\n    gradients = 2/m * X_train1.T.dot(error)\n\n    if np.linalg.norm(X_train1) &lt; tol:\n        break\n    theta = theta - (eta * gradients)\n'
'print(outputs.shape, labels.shape)\n#out: torch.Size([7, 10]) torch.Size([7,1])\n# these are broadcastable\n'
'import numpy, scipy, scipy.optimize\nimport matplotlib\nfrom mpl_toolkits.mplot3d import  Axes3D\nfrom matplotlib import cm # to colormap 3D surfaces from blue to red\nimport matplotlib.pyplot as plt\n\n\ngraphWidth = 800 # units are pixels\ngraphHeight = 600 # units are pixels\n\n# 3D contour plot lines\nnumberOfContourLines = 16\n\nxData = numpy.array([971.0, 691.0, 841.0, 970.0, 755.0, 684.0, 938.0, 956.0, 658.0, 838.0, 879.0, 752.0, 690.0, 970.0, 964.0, 966.0, 901.0, 671.0, 660.0, 666.0, 765.0, 831.0, 899.0, 668.0, 969.0, 967.0, 651.0, 929.0, 805.0, 812.0, 936.0, 650.0, 964.0, 719.0, 654.0, 646.0, 932.0, 827.0, 917.0, 945.0, 724.0, 956.0, 966.0, 969.0, 968.0, 967.0, 718.0, 966.0, 812.0, 649.0, 645.0, 675.0, 959.0, 966.0, 962.0, 967.0, 956.0, 757.0, 964.0, 817.0, 666.0, 812.0, 902.0, 969.0, 661.0, 962.0, 752.0, 802.0, 670.0, 663.0, 966.0, 967.0, 773.0, 663.0, 818.0, 917.0, 952.0, 834.0, 516.0, 547.0, 846.0, 458.0, 490.0, 835.0, 579.0, 472.0, 557.0, 652.0, 471.0, 455.0, 837.0, 842.0, 832.0, 675.0, 529.0, 509.0, 533.0, 493.0, 572.0, 695.0, 464.0, 846.0, 845.0, 505.0, 833.0, 544.0, 550.0, 594.0, 486.0, 847.0, 471.0, 533.0, 497.0, 838.0, 832.0, 830.0, 847.0, 844.0, 837.0, 831.0, 671.0, 844.0, 824.0, 841.0, 532.0, 576.0, 852.0, 471.0, 496.0, 839.0, 587.0, 478.0, 565.0, 657.0, 481.0, 463.0, 841.0, 842.0, 832.0, 682.0, 532.0, 509.0, 539.0, 497.0, 574.0, 704.0, 472.0, 850.0, 849.0, 512.0, 834.0, 540.0, 542.0, 603.0, 481.0, 847.0, 472.0, 529.0, 496.0, 836.0, 570.0, 588.0, 837.0, 474.0, 781.0, 842.0, 855.0, 846.0, 845.0, 518.0, 854.0, 585.0, 531.0, 539.0, 536.0])\nyData = numpy.array([956.0, 825.0, 963.0, 731.0, 939.0, 879.0, 523.0, 962.0, 880.0, 962.0, 536.0, 942.0, 902.0, 954.0, 662.0, 959.0, 550.0, 798.0, 836.0, 778.0, 945.0, 959.0, 532.0, 880.0, 783.0, 733.0, 833.0, 526.0, 955.0, 956.0, 959.0, 863.0, 714.0, 924.0, 778.0, 849.0, 523.0, 957.0, 960.0, 559.0, 925.0, 959.0, 955.0, 760.0, 953.0, 952.0, 921.0, 713.0, 955.0, 838.0, 819.0, 781.0, 956.0, 950.0, 714.0, 937.0, 955.0, 947.0, 739.0, 957.0, 864.0, 957.0, 531.0, 896.0, 796.0, 954.0, 945.0, 955.0, 762.0, 878.0, 951.0, 953.0, 951.0, 877.0, 959.0, 958.0, 609.0, 791.0, 496.0, 786.0, 597.0, 615.0, 574.0, 432.0, 805.0, 599.0, 793.0, 344.0, 617.0, 615.0, 792.0, 456.0, 807.0, 328.0, 504.0, 543.0, 494.0, 644.0, 803.0, 319.0, 611.0, 690.0, 471.0, 543.0, 392.0, 774.0, 783.0, 812.0, 597.0, 478.0, 627.0, 508.0, 576.0, 799.0, 803.0, 421.0, 534.0, 645.0, 791.0, 422.0, 321.0, 790.0, 384.0, 803.0, 520.0, 797.0, 563.0, 629.0, 581.0, 441.0, 809.0, 602.0, 797.0, 354.0, 625.0, 621.0, 796.0, 463.0, 806.0, 333.0, 511.0, 543.0, 501.0, 648.0, 804.0, 323.0, 620.0, 689.0, 483.0, 554.0, 396.0, 767.0, 777.0, 806.0, 596.0, 479.0, 625.0, 506.0, 574.0, 411.0, 801.0, 811.0, 426.0, 626.0, 811.0, 809.0, 515.0, 805.0, 804.0, 651.0, 564.0, 795.0, 589.0, 576.0, 495.0])\nzData = numpy.array([-2.0, -105.0, -26.0, 44.0, -69.0, -65.0, 60.0, -22.0, -77.0, -24.0, 58.0, -36.0, -66.0, -3.0, 34.0, -8.0, 57.0, -82.0, -98.0, -90.0, -55.0, -23.0, 60.0, -61.0, 29.0, 36.0, -72.0, 61.0, -44.0, -47.0, -27.0, -73.0, 40.0, -37.0, -107.0, -89.0, 68.0, -32.0, -38.0, 63.0, -54.0, -33.0, 16.0, 34.0, 3.0, 15.0, -61.0, 54.0, -39.0, -72.0, -77.0, -97.0, -16.0, 0.0, 45.0, 11.0, -9.0, -57.0, 47.0, -37.0, -82.0, -15.0, 63.0, 21.0, -73.0, 4.0, -55.0, -23.0, -87.0, -74.0, 24.0, -1.0, -46.0, -59.0, -47.0, -18.0, 41.0, 18.0, -104.0, -25.0, 18.0, -55.0, -64.0, 55.0, -35.0, -56.0, -25.0, 63.0, -46.0, -70.0, 16.0, 59.0, -17.0, 78.0, -86.0, -102.0, -113.0, -41.0, -53.0, 68.0, -56.0, 28.0, 24.0, -88.0, 42.0, -59.0, -35.0, -38.0, -79.0, 48.0, -65.0, -113.0, -73.0, 4.0, -8.0, 63.0, 28.0, 23.0, 25.0, 48.0, 74.0, 7.0, 45.0, 11.0, -92.0, -38.0, 29.0, -69.0, -87.0, 56.0, -31.0, -60.0, -29.0, 59.0, -43.0, -53.0, -4.0, 50.0, -5.0, 74.0, -89.0, -84.0, -116.0, -53.0, -42.0, 46.0, -69.0, 32.0, 36.0, -83.0, 57.0, -64.0, -36.0, -18.0, -94.0, 52.0, -72.0, -87.0, -77.0, 44.0, -57.0, -33.0, 53.0, -76.0, -33.0, -12.0, 15.0, 9.0, -6.0, -70.0, 43.0, -58.0, -100.0, -78.0, -97.0])\n\n\n# Simple_SimpleEquation_42_Offset_model from zunzun.com\ndef curvedModel(data, a, b, c, Offset):\n    x = data[0]\n    y = data[1]\n    return numpy.exp(a+b/y+c*numpy.log(x)) + Offset\n\ndef flatModel(data, a, b, Offset):\n    x = data[0]\n    y = data[1]\n    return a*x + b*y + Offset\n\n\n# choose model by commenting\n#func = flatModel\n#initialParameters = [1.0, 1.0, 1.0] # these are the same as scipy default values in this example\nfunc = curvedModel \ninitialParameters = [1.0, 1.0, 1.0, 1.0] # these are the same as scipy default values in this example\n\n\n\ndef SurfacePlot(func, data, fittedParameters):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n\n    matplotlib.pyplot.grid(True)\n    axes = Axes3D(f)\n\n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    xModel = numpy.linspace(min(x_data), max(x_data), 20)\n    yModel = numpy.linspace(min(y_data), max(y_data), 20)\n    X, Y = numpy.meshgrid(xModel, yModel)\n\n    Z = func(numpy.array([X, Y]), *fittedParameters)\n\n    axes.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=1, antialiased=True)\n\n    axes.scatter(x_data, y_data, z_data) # show data along with plotted surface\n\n    axes.set_title(\'Surface Plot (click-drag with mouse)\') # add a title for surface plot\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n    axes.set_zlabel(\'Z Data\') # Z axis data label\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot or else thaere can be memory and process problems\n\n\ndef ContourPlot(func, data, fittedParameters):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    xModel = numpy.linspace(min(x_data), max(x_data), 20)\n    yModel = numpy.linspace(min(y_data), max(y_data), 20)\n    X, Y = numpy.meshgrid(xModel, yModel)\n\n    Z = func(numpy.array([X, Y]), *fittedParameters)\n\n    axes.plot(x_data, y_data, \'o\')\n\n    axes.set_title(\'Contour Plot\') # add a title for contour plot\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n\n    CS = matplotlib.pyplot.contour(X, Y, Z, numberOfContourLines, colors=\'k\')\n    matplotlib.pyplot.clabel(CS, inline=1, fontsize=10) # labels for contours\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot or else thaere can be memory and process problems\n\n\ndef ScatterPlot(data):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n\n    matplotlib.pyplot.grid(True)\n    axes = Axes3D(f)\n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    axes.scatter(x_data, y_data, z_data)\n\n    axes.set_title(\'Scatter Plot (click-drag with mouse)\')\n    axes.set_xlabel(\'X Data\')\n    axes.set_ylabel(\'Y Data\')\n    axes.set_zlabel(\'Z Data\')\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot or else thaere can be memory and process problems\n\n\nif __name__ == "__main__":\n    data = [xData, yData, zData]\n\n    # here a non-linear surface fit is made with scipy\'s curve_fit()\n    fittedParameters, pcov = scipy.optimize.curve_fit(func, [xData, yData], zData, p0 = initialParameters)\n\n    ScatterPlot(data)\n    SurfacePlot(func, data, fittedParameters)\n    ContourPlot(func, data, fittedParameters)\n\n    print(\'fitted prameters\', fittedParameters)\n\n    modelPredictions = func(data, *fittedParameters) \n\n    absError = modelPredictions - zData\n\n    SE = numpy.square(absError) # squared errors\n    MSE = numpy.mean(SE) # mean squared errors\n    RMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\n    Rsquared = 1.0 - (numpy.var(absError) / numpy.var(zData))\n    print(\'RMSE:\', RMSE)\n    print(\'R-squared:\', Rsquared)\n'
"pipe = Pipeline([\n('tfidf', TfidfVectorizer()), \n('ros', RandomOverSampler()),\n('oversampler', SMOTE()),\n('clf', LinearSVC()),\n])\n\n"
'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nclf = LinearRegression(n_jobs=-1)\n\nclf.fit(X_train, y_train)\n'
"import torch\nimport torch.nn as nn\n\nclass AE(nn.Module):\n    def __init__(self):\n        super(AE, self).__init__()\n\n        self.encoder = nn.Sequential(\n            # conv 1\n            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # conv 2\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # conv 3\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # conv 4\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # conv 5\n            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(1024),\n            nn.ReLU()\n        )\n\n        self.decoder = nn.Sequential(\n            # conv 6\n            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n\n            # conv 7\n            nn.Upsample(scale_factor=2, mode='bilinear'),\n            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n\n            # conv 8\n            nn.Upsample(scale_factor=2, mode='bilinear'),\n            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n\n            # conv 9\n            nn.Upsample(scale_factor=2, mode='bilinear'),\n            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n\n            # conv 10 out\n            nn.Upsample(scale_factor=2, mode='bilinear'),\n            nn.ConvTranspose2d(in_channels=64, out_channels=2, kernel_size=5, stride=1, padding=2),\n            nn.Softmax()    # multi-class classification\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\ninput = torch.randn(1, 3, 6*16, 7*16)\noutput = AE()(input)\nprint(input.shape)\nprint(output.shape)\n"
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.linear_model import LinearRegression\n&gt;&gt;&gt; X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n&gt;&gt;&gt; # y = 1 * x_0 + 2 * x_1 + 3\n&gt;&gt;&gt; y = np.dot(X, np.array([1, 2])) + 3\n&gt;&gt;&gt; reg = LinearRegression().fit(X, y)\n&gt;&gt;&gt; reg.score(X, y)\n1.0\n&gt;&gt;&gt; reg.coef_\narray([1., 2.])\n&gt;&gt;&gt; reg.intercept_ \n3.0000...\n&gt;&gt;&gt; reg.predict(np.array([[3, 5]]))\narray([16.])\n'
'for image 0 angle: 0.7416666666666667, pred: [0.7266706]\nfor image 1 angle: 0.8111111111111111, pred: [0.8449749]\nfor image 2 angle: 0.7777777777777778, pred: [0.84269005]\nfor image 3 angle: 0.12222222222222222, pred: [0.14173588]\nfor image 4 angle: 0.7388888888888889, pred: [0.730219]\nfor image 5 angle: 0.9694444444444444, pred: [0.9117564]\nfor image 6 angle: 0.075, pred: [0.07597628]\nfor image 7 angle: 0.29444444444444445, pred: [0.1829494]\nfor image 8 angle: 0.10277777777777777, pred: [0.12209181]\nfor image 9 angle: 0.21388888888888888, pred: [0.31544465]\n'
'precision : int, optional (default=3)\n\n    Number of digits of precision for floating point in the values of impurity, threshold and value attributes of each node.\n\ntree.plot_tree(clf, precision=0)\n'
'tokenizer = Tokenizer(num_words=100)\n\ntokenizer.fit_on_texts(x)\n\ntokenizer.fit_on_texts(word_Arr)\n\ntokenizer = Tokenizer(num_words=100)\ntokenizer.fit_on_texts(["dog, cat, horse"])\next_input = str(input("Enter a word for analysis: "))\n\nword_Arr = []\nword_Arr.append(text_input)\n\n# here is your problem!!!\ntokenizer.fit_on_texts(word_Arr)\n\nword_final = tokenizer.texts_to_sequences(word_Arr)\nword_final_final = np.asarray(word_final)\n\nprint(word_final_final)\n\nEnter a word for analysis: dog\n[[1]]\nEnter a word for analysis: cat\n[[1]]\n\ntokenizer = Tokenizer(num_words=100)\n\ntokenizer.fit_on_texts(["dog, cat, horse"])\next_input = str(input("Enter a word for analysis: "))\n\nword_Arr = []\nword_Arr.append(text_input)\n\n# commenting out your problem!!!\n# tokenizer.fit_on_texts(word_Arr)\n\nword_final = tokenizer.texts_to_sequences(word_Arr)\nword_final_final = np.asarray(word_final)\n\nprint(word_final_final)\n\nEnter a word for analysis: cat\n[[2]]\nEnter a word for analysis: dog\n[[1]]\n'
'squared_diff = (pp[:, :, None, :, None] - pp[:, None, :, None, :]) ** 2\nweighted_diff = s * squared_diff\nb_eq_b_1_removed = b.sum(axis=(3,4)) * (1 - np.eye(nb))\nfirst_term = b_eq_b_1_removed.sum(axis=(1,2))\n\nnormalized_s = np.linalg.norm(s, ord=1, axis=1)\nsquared_diff = (pp - p)**2\nsecond_term = nb * (normalized_s * squared_diff).sum(axis=(1,2))\n\nloss = ((1 - epsilon) * first_term) + (epsilon * second_term)\n'
"dataset = read_csv('pollution.csv', header=0, index_col=0)\nx = dataset[['pollution', 'dew', 'temp', 'press', 'wnd_spd', 'snow']].values\ny = dataset[['wnd_dir']].values\nfrom sklearn.ensemble import RandomForestClassifier\n\ncls = RandomForestClassifier(random_state=0)\ncls.fit(x, y)\n\nz = cls.predict_proba(x)\nlabels = np.argmax(z, axis=1)\nclasses = cls.classes_\nlabels = [classes[i] for i in labels]\nprint(accuracy_score(y, labels))\n"
'import tensorflow as tf\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import classification_report \n\nclass MetricsCallback(Callback):\n    def __init__(self, test_data, y_true):\n        # Should be the label encoding of your classes\n        self.y_true = y_true\n        self.test_data = test_data\n        \n    def on_epoch_end(self, epoch, logs=None):\n        # Here we get the probabilities\n        y_pred = self.model.predict(self.test_data))\n        # Here we get the actual classes\n        y_pred = tf.argmax(y_pred,axis=1)\n        # Actual dictionary\n        report_dictionary = classification_report(self.y_true, y_pred, output_dict = True)\n        # Only printing the report\n        print(classification_report(self.y_true,y_pred,output_dict=False)              \n           \n\nmetrics_callback = MetricsCallback(test_data = my_test_data, y_true = my_y_true)\n...\n...\n#train the model\nmodel.fit(x_train, y_train, callbacks = [cp_callback, metrics_callback,tensorboard], epochs=5)\n\n         \n'
'from sklearn.metrics import accuracy_score\n\nimages, actual = next(train_data_gen)\npredictions = model.predict(images)\npredictions = (predictions &gt; 0).flatten()\naccuracy_score(results, pred)\n'
'  bi_gram         frequency\n1:  United States         2\n2: States America         2\n3: climate change         2\n\n uni_gram         frequency\n1:          climate       1\n2:           change       1\n3:          America       1\n'
"import numpy as np \nfrom sklearn.preprocessing import StandardScaler\n\nX_train = np.array([[1, 2], [3, 4], [5, 6]])\nX_test = np.array([[7, 8], [9, 10]])\n\nX_train:\narray([[1, 2],\n       [3, 4],\n       [5, 6]])\n\nX_test:\narray([[ 7,  8],\n       [ 9, 10]])\n\nsc = StandardScaler()\n\nprint(sc.mean_)\n\nAttributeError: 'StandardScaler' object has no attribute 'mean_'\n\nsc.fit(X_train)\n\nprint(sc.mean_)\n\n[3. 4.]\n\nsc.transform(X_test)\n\narray([[2.44949 , 2.44949 ],\n       [3.674235, 3.674235]])\n\nsc.transform(X_train)\n\narray([[-1.224745, -1.224745],\n       [ 0.      ,  0.      ],\n       [ 1.224745,  1.224745]])\n\nX_train = sc.fit_transform(X_train)\n\narray([[-1.224745, -1.224745],\n       [ 0.      ,  0.      ],\n       [ 1.224745,  1.224745]])\n"
"n_sample, time_step, n_features = 288, 3, 393\nX = np.random.uniform(0,1, (n_sample, time_step, n_features))\ny = np.random.randint(0,2, n_sample)\n\nmodel = Sequential()\nmodel.add(Conv1D(128, 5, padding='same', activation='relu', \n                 input_shape=(time_step, n_features)))\nmodel.add(MaxPooling1D())\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(LSTM(16, return_sequences=False))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(X,y, epochs=3)\n"
"import numpy as np\nx = np.random.randint(0, 255, size=(100, 32, 32), dtype=np.int16)\n\nprint('Present data type', x.dtype)\n# What you did\ny = x/255\nprint('Present data type', y.dtype)\n# What you should do\nz = (x/255).astype(np.float16)\nprint('Present data type', z.dtype)\n\nPresent data type int16\nPresent data type float64\nPresent data type float16\n\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\n\ndata = []\n\nfor emot in os.listdir('./data/'):\n    for file_ in os.listdir(f'./data/{emot}'):\n        img = cv2.imread(f'./data/{emot}/{file_}', 0)\n        img = cv2.bitwise_not(img)\n        img = (img/255.0).astype(np.float16) # &lt;--- This is the suggestion\n        data.append([img, emotions.index(emot)])\n"
"from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX, Y = make_regression(n_samples=500, n_features=9, bias=0, random_state=1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.30, random_state=1)\nclassifier = LinearRegression(fit_intercept=False)\nclassifier.fit(X_train,Y_train)\ncost = np.sqrt(np.sum((np.dot(X_train,classifier.coef_.reshape(9,1)) + classifier.intercept_ - Y_train.reshape(-1,1))**2))\n\nprint(cost)\nprint('Manual regression')\nY_train = Y_train.reshape(-1,1)\n\nalpha = 0.0005 # Learning_Rate\ncoefficient = np.random.randn(1,9) # Initialisation of coefficients including intercept\n\n# Loop through iterations\nfor i in range(100000):\n    cost = np.sqrt(np.sum((np.dot(X_train,coefficient.T) - Y_train)**2)) # cost result\n    if i % 10000 == 0: print(cost)\n    grad = np.dot((np.dot(X_train,coefficient.T) - Y_train).T, X_train) # Compute Gradients\n    coefficient = coefficient - (alpha * grad) # adjust coefficients including intercept\n"
"import h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init()\n\n# import prostate dataset\nprostate = h2o.import_file(&quot;https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv&quot;)\n# convert columns to factors\nprostate['CAPSULE'] = prostate['CAPSULE'].asfactor()\nprostate['RACE'] = prostate['RACE'].asfactor()\nprostate['DCAPS'] = prostate['DCAPS'].asfactor()\nprostate['DPROS'] = prostate['DPROS'].asfactor()\n\n# set the predictor and response columns\npredictors = [&quot;AGE&quot;, &quot;RACE&quot;, &quot;VOL&quot;, &quot;GLEASON&quot;]\nresponse_col = &quot;CAPSULE&quot;\n\n# split into train and testing sets\ntrain, test = prostate.split_frame(ratios = [0.8], seed = 1234)\n\n# run AutoML for 100 seconds\naml = H2OAutoML(seed=1, max_runtime_secs=100, exclude_algos=[&quot;DeepLearning&quot;, &quot;GLM&quot;],\n                    nfolds=5, keep_cross_validation_predictions=True)\naml.train(x=predictors, y=response_col, training_frame=prostate)\n\n# Get the leader model\nleader = aml.leader\n\n# print CV AUC for leader model\nprint(leader.model_performance(xval=True).auc())\n\n# print CV metrics summary\nleader.cross_validation_metrics_summary()\n\nCross-Validation Metrics Summary:\n             mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n-----------  ----------  -----------  ------------  ------------  ------------  ------------  ------------\naccuracy     0.71842104  0.06419111   0.7631579     0.6447368     0.7368421     0.7894737     0.65789473\nauc          0.7767409   0.053587236  0.8206676     0.70905924    0.7982079     0.82538515    0.7303846\naucpr        0.6907578   0.0834025    0.78737605    0.7141305     0.7147677     0.67790955    0.55960524\nerr          0.28157896  0.06419111   0.23684211    0.35526314    0.2631579     0.21052632    0.34210527\nerr_count    21.4        4.8785243    18.0          27.0          20.0          16.0          26.0\n---          ---         ---          ---           ---           ---           ---           ---\nprecision    0.61751753  0.08747421   0.675         0.5714286     0.61702126    0.7241379     0.5\nr2           0.20118153  0.10781976   0.3014902     0.09386432    0.25050205    0.28393403    0.07611712\nrecall       0.84506994  0.08513061   0.84375       0.9142857     0.9354839     0.7241379     0.8076923\nrmse         0.435928    0.028099842  0.41264254    0.47447023    0.42546       0.41106534    0.4560018\nspecificity  0.62579334  0.15424488   0.70454544    0.41463414    0.6           0.82978725    0.58\n\nSee the whole table with table.as_data_frame()\n\n# print the whole Leaderboard (all CV metrics for all models)\nlb = aml.leaderboard\nprint(lb)\n\nmodel_id                                                  auc    logloss     aucpr    mean_per_class_error      rmse       mse\n---------------------------------------------------  --------  ---------  --------  ----------------------  --------  --------\nXGBoost_grid__1_AutoML_20200924_200634_model_2       0.769716   0.565326  0.668827                0.290806  0.436652  0.190665\nGBM_grid__1_AutoML_20200924_200634_model_4           0.762993   0.56685   0.666984                0.279145  0.437634  0.191524\nXGBoost_grid__1_AutoML_20200924_200634_model_9       0.762417   0.570041  0.645664                0.300121  0.440255  0.193824\nGBM_grid__1_AutoML_20200924_200634_model_6           0.759912   0.572651  0.636713                0.30097   0.440755  0.194265\nStackedEnsemble_BestOfFamily_AutoML_20200924_200634  0.756486   0.574461  0.646087                0.294002  0.441413  0.194845\nGBM_grid__1_AutoML_20200924_200634_model_7           0.754153   0.576821  0.641462                0.286041  0.442533  0.195836\nXGBoost_1_AutoML_20200924_200634                     0.75411    0.584216  0.626074                0.289237  0.443911  0.197057\nXGBoost_grid__1_AutoML_20200924_200634_model_3       0.753347   0.57999   0.629876                0.312056  0.4428    0.196072\nGBM_grid__1_AutoML_20200924_200634_model_1           0.751706   0.577175  0.628564                0.273603  0.442751  0.196029\nXGBoost_grid__1_AutoML_20200924_200634_model_8       0.749446   0.576686  0.610544                0.27844   0.442314  0.195642\n\n[28 rows x 7 columns]\n"
'X_train = X_train.to_dict(&quot;list&quot;)\nX_test = X_test.to_dict(&quot;list&quot;)\n\nX_train = {k:np.array(v) for k,v in X_train.items()}\nX_test = {k:np.array(v) for k,v in X_test.items()}\n'
"X, y_true = gen_linear_regression_dataset(numofsamples=1)\nprint(X)\nprint(y_true)\n\n[[0.37454012 0.90385769 0.39221343]]\n[25.72962531]\n\npredict_new_sample(model, X)\n# result:\ny actual value:  22.134424269890232\ny pred value:  25.729633\n\ndef gen_linear_regression_dataset(numofsamples=500, a=3, b=5, c=7, d=9, e=11):\n    np.random.seed(42)\n    X_init = np.random.rand(numofsamples,3)  # data to be returned\n    # y = a + bx1 + cx2^2 + dx3^3+ e\n    X = X_init.copy()  # temporary data\n    for idx in range(numofsamples):\n        X[idx][1] = X[idx][1]**2\n        X[idx][2] = X[idx][2]**3\n    coef = np.array([b,c,d])\n    bias = e\n\n    y = a + np.matmul(X,coef.transpose()) + bias\n    return X_init, y\n\nX, y_true = gen_linear_regression_dataset(numofsamples=1)\nprint(X)\nprint(y_true)\n\n[[0.37454012 0.95071431 0.73199394]]\n[25.72962531]\n\npredict_new_sample(model, X)\n# result:\ny actual value:  25.729625308532768\ny pred value:  25.443237\n\npredict_new_sample(model, np.array([0.07,0.6,0.5]))\n# result:\ny actual value:  17.995\ny pred value:  19.69147\n\ndef gen_sequential_model():\n    model = Sequential([Input(3,name='input_layer'),\n    Dense(16, activation = 'relu', name = 'hidden_layer1'),\n    Dense(16, activation = 'relu', name = 'hidden_layer2'),\n    Dense(1, activation = 'linear', name = 'output_layer'),\n    ])\n\n    model.summary()\n    model.compile(optimizer='adam',loss='mse')\n    return model\n\npredict_new_sample(model, np.array([0.07,0.6,0.5]))\n# result:\ny actual value:  17.995\ny pred value:  18.272991\n"
'{"Apple", "Banana", "Cherry", "Date", "Eggplant"}\n\nbag("Apple Banana Date")\n#: [1, 1, 0, 1, 0]\nbag("Cherry")\n#: [0, 0, 1, 0, 0]\nbag("Date Eggplant Banana Banana")\n#: [0, 1, 0, 1, 1]\n# For this case, I have no clue if Banana having the value 2 would improve results.\n# It might. It might not. Something you\'d need to test.\n'
'import keras.backend as K\n\ndef regression_nll_loss(sigma_sq, epsilon = 1e-6):\n    def nll_loss(y_true, y_pred):\n        return 0.5 * K.mean(K.log(sigma_sq + epsilon) + K.square(y_true - y_pred) / (sigma_sq + epsilon))\n\n    return nll_loss\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input\n\ninp = Input(shape=(1,))\nx = Dense(32, activation="relu")(inp)\nx = Dense(32, activation="relu")(x)\nmean = Dense(1, activation="linear")(x)\nvar = Dense(1, activation="softplus")(x)\n\ntrain_model = Model(inp, mean)\npred_model = Model(inp, [mean, var])\n\ntrain_model.compile(loss=regression_nll_loss(var), optimizer="adam")\n\ntrain_model.fit(x, y, ...)\n\nmean, var = pred_model.predict(some_input)\n'
'def f(x):\n    values = [v for vals in x.values for v in vals[2:]]\n    return pd.Series(values,name=x.values[0][0])\n\nres = df.groupby(["Activity", "Iteration"]).apply(f)\nres = res.T.rename(columns={(t,i):t for t,i in res.index})\nprint df   \nprint res\n\n   Activity  Iteration  col_A  col_B  col_C  col_D\n0     test1          1      4      6      5      7\n1     test1          1      5      9      5      4\n2     test1          1      1      8      7      9\n3     test1          1      4      8      1      9\n4     test1          2      4      5      5      6\n5     test1          2      6      3      8      6\n6     test1          2      8      1      1      2\n7     test1          2      5      1      8      1\n8     test2          1      6      3      9      9\n9     test2          1      4      9      9      7\n10    test2          1      5      0      1      3\n11    test2          1      5      8      9      5\n12    test2          2      4      8      3      2\n13    test2          2      8      9      9      4\n14    test2          2      6      1      1      8\n15    test2          2      6      4      4      8\n    test1  test1  test2  test2\n0       4      4      6      4\n1       6      5      3      8\n2       5      5      9      3\n3       7      6      9      2\n4       5      6      4      8\n5       9      3      9      9\n6       5      8      9      9\n7       4      6      7      4\n8       1      8      5      6\n9       8      1      0      1\n10      7      1      1      1\n11      9      2      3      8\n12      4      5      5      6\n13      8      1      8      4\n14      1      8      9      4\n15      9      1      5      8\n\ndef g(x):\n    values = [v for vals in x.values for v in vals[2:]]\n    return pd.DataFrame({1: values[:N/2*len(x)], 2: values[N/2*len(x):]})\n\nres = df.groupby(["Activity", "Iteration"]).apply(g).unstack()\nr1 = res[1].T.rename(columns={(t,i):t+str(i)+"1" for t,i in res.index})\nr2 = res[2].T.rename(columns={(t,i):t+str(i)+"2" for t,i in res.index})\nres = pd.concat([r1,r2],axis=1).sort(axis=1)\nres = res.rename(columns={t:t[:-2] for t in res.columns})\n\nprint df\nprint res\n\n   Activity  Iteration  col_A  col_B  col_C  col_D\n0     test1          1      0      8      1      7\n1     test1          1      2      0      5      0\n2     test1          1      2      6      6      6\n3     test1          1      5      0      1      4\n4     test1          2      4      5      6      8\n5     test1          2      8      0      1      6\n6     test1          2      6      7      2      4\n7     test1          2      3      2      2      3\n8     test2          1      5      2      1      9\n9     test2          1      8      3      5      9\n10    test2          1      3      7      7      1\n11    test2          1      7      4      5      1\n12    test2          2      9      2      4      0\n13    test2          2      3      1      8      7\n14    test2          2      1      2      7      8\n15    test2          2      4      9      7      0\n   test1  test1  test1  test1  test2  test2  test2  test2\n0      0      2      4      6      5      3      9      1\n1      8      6      5      7      2      7      2      2\n2      1      6      6      2      1      7      4      7\n3      7      6      8      4      9      1      0      8\n4      2      5      8      3      8      7      3      4\n5      0      0      0      2      3      4      1      9\n6      5      1      1      2      5      5      8      7\n7      0      4      6      3      9      1      7      0\n'
'spectral = cluster.SpectralClustering(n_clusters=2, eigen_solver=\'arpack\', affinity="nearest_neighbors")\nspectral.fit(X)\ny_pred = spectral.labels_.astype(np.int)\n'
"df['Body-Collocation'] = df.Body.apply(lambda x: BigramCollocationFinder.from_words(x))\n"
'ALS.trainImplicit(ratings, rank, numIterations, alpha=100.0)\n\nALS.trainImplicit(ratings, rank, numIterations, alpha=100)\n'
"weights = {\n  'wdc1' : tf.Variable(tf.random_normal([3, 3, 64, 128])),\n  'wdc2' : tf.Variable(tf.random_normal([3, 3, 32, 64])),\n  'wdc3' : tf.Variable(tf.random_normal([3, 3, 1, 32]))\n}\n"
'model.predict(X_test) \n'
"import numpy as np\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nx1 = range(1, 70)\nx2 = [26]*69\n\nX = np.column_stack([x1, x2])\n\ny = '''  192770 14817993  1393537   437541   514014   412468   509393   172715\n   329806   425876   404031   524371   362817   692020   585431   446286\n   744061   458805   330027   495654   459060   734793   701697   663319\n   750496   525311  1045502   250641   500360   507594   456444   478666\n   431382   495689   458200   349161   538770   355879   535924   549858\n   611428   517146   239513   354071   342354   698360   467248   500903\n   625170   404462  1057368   564703   700988  1352634   727453   782708\n   1023673  1046348  1175588   698072   605187   684739   884551  1067267\n   728643   790098   580151   340890   299185'''\n\nY = np.array(map(int, y.split()))\nregr = linear_model.LinearRegression()\n\nregr.fit(X, Y)\n\nplt.scatter(X[:,0], Y,  color='black')\nplt.plot(X[:,0], regr.predict(X), color='blue',\n     linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\nprint regr.predict([[49,26]])\n# 611830.33589088\n"
'model.computeCost(d)\n\nsplit_vecs = d.map(lambda x: (x[0], np.split(x[1], 2)))\ncosts_per_split = [KMeansModel(model.Cs[i]).computeCost(split_vecs.map(lambda x: x[1][i])) for i in range(2)]\n'
'import caffe\nimport caffe.io\n\nfrom . import io\n'
"input = 'sittin'\nfor num in 1 ... n:  # suppose you want to have n strings generated\n  my_input_ = input\n  # suppose the edit distance should be smaller or equal to k;\n  # but greater or equal to one\n  for i in in 1 ... randint(k): \n    pick a random edit mode from (delete, add, substitute)\n    do it! and update my_input_\n"
'pltrdy\n'
'#apply the segmentation to each layer and for each slice-direction\ntime_dist=TimeDistributed(convmodel)\n\nx_dist=time_dist(x_perm)\ny_dist=time_dist(y_perm)\nz_dist=time_dist(z_perm)\n'
"tf.scatter_update(S,inds_new,updates)\n\ninds_new = sampled_ind + line*K\n\ninit = tf.initialize_all_variables()\ninds_new = sampled_ind + line*K\nupdate_op = tf.scatter_update(S, inds_new, updates)\nsess = tf.Session()\nsess.run(init)\nfor line in range(N):\n    sess.run(update_op, feed_dict={tfx: X[line:line+1]})\n\nsummary_writer = tf.train.SummaryWriter('some_logdir', sess.graph_def)\n\ntensorboard --logdir=some_logdir\n"
'X_train = np.zeros((len(training_data,MAX_QUERY_LENGTH,300))\nfor i in range(len(training_data)):\n     tup = training_data[i]\n     X_train[i,:,:]=tup[0]\n'
'models = [clf.fit(base, fact) for fact in facts]\n\npredicted_facts = [model.predict(unseen_dataset) for model in models]\n\nfactzero=dataset[:,0]\nfactone=dataset[:,1]\nfacttwo=dataset[:,2]\nfactthree=dataset[:,3]\nfactfour=dataset[:,4]\nfactfive=dataset[:,5]\nfactsix=dataset[:,6]\nfactseven=dataset[:,7]\nfacts=[factzero,factone,facttwo,factthree,factfour,factfive,factsix,factseven]\n\nfacts = [fact for fact in dataset[:, :8]]\n'
"def _load_pascal_annotation(self, index):\n\n    # ...\n    # ...\n    # ...\n\n    # Load object bounding boxes into a data frame.\n    for ix, obj in enumerate(objs):\n        bbox = obj.find('bndbox')\n        # Make pixel indexes 0-based\n        x1 = float(bbox.find('xmin').text) #- 1 &lt;- comment these out\n        y1 = float(bbox.find('ymin').text) #- 1\n        x2 = float(bbox.find('xmax').text) #- 1\n        y2 = float(bbox.find('ymax').text) #- 1\n\n    # ...\n    # ...\n    # ...\n"
"from sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import BaggingClassifier\n\n\niris = datasets.load_iris()\nclf = BaggingClassifier(n_estimators=3)\nclf.fit(iris.data, iris.target)\nclf.estimators_\n\n[DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n             max_features=None, max_leaf_nodes=None,\n             min_impurity_split=1e-07, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             presort=False, random_state=1422640898, splitter='best'),\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n             max_features=None, max_leaf_nodes=None,\n             min_impurity_split=1e-07, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             presort=False, random_state=1968165419, splitter='best'),\n DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n             max_features=None, max_leaf_nodes=None,\n             min_impurity_split=1e-07, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             presort=False, random_state=2103976874, splitter='best')]\n"
'from tensorflow.contrib.learn.python.learn.utils.input_fn_utils import build_parsing_serving_input_fn\ninput_receiver_fn = build_parsing_serving_input_fn(feature_spec)\n'
'print(sess.run([pred], feed_dict={x: _INPUT_GOES_HERE_ })\n'
'A = 1 / (1 + np.exp(-(np.dot(np.transpose(w), X) + b)))\n'
"df.reset_index(inplace=True)    \ndf['PREVIOUS_TIME']= df.DATE.shift(-2)\ndf['duration']=(df.PREVIOUS_TIME-df.DATE)/np.timedelta64(1,'s')\ndf.drop('PREVIOUS_TIME',axis=1,inplace=True)\ndf.set_index('DATE',inplace=True)\n"
"#if using channels_last - the default configuration in keras\nx_train = x_train.reshape(40000,16,16,1)\n\n#if using channels_first\nx_train = x_train.reshape(40000,1, 16,16)\n\n#channels last (if you have 1 channel only, but if you have RGB, use 3 instead of 1   \nmodel.add(Conv2D(32, (2,2), input_shape=(16, 16, 1), activation='relu'))\n\n#channels first (if you have 1 channel only, but if you have RGB, use 3 instead of 1   \nmodel.add(Conv2D(32, (2,2), input_shape=(1,16, 16), activation='relu'))\n"
'&gt;&gt;&gt; C.leaky_relu([[-1, -0.5, 0, 1, 2]]).eval()\narray([[-0.01 , -0.005,  0.   ,  1.   ,  2.   ]], dtype=float32)\n'
'def func(x):\n\n    greater = K.greater_equal(x, 0.5) #will return boolean values\n    greater = K.cast(greater, dtype=K.floatx()) #will convert bool to 0 and 1    \n    return greater \n\nmodel.add(Lambda(func, output_shape=yourOutputShapeIfUsingTheano))\n'
'cluster_idx, scores = lump(X)\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  idx, d = sess.run([cluster_idx, scores], feed_dict={X: data})\n'
"# Another option:\n# prediction = graph.get_operation_by_name('output/BiasAdd')\nprediction = graph.get_tensor_by_name('output/BiasAdd:0')\n\nresult = tf.nn.softmax(prediction, name='softmax')\n\nprediction = graph.get_operation_by_name('softmax')\n"
'zip(importances, contributions[0])\n\nfor name, values in sorted(zip(importances, contributions[0]), key=lambda pair: pair[1][0]):\n    print(name, values)\n\nfor name, values in sorted(zip(importances, contributions[0]), key=lambda pair: pair[1][0]):\n    if -0.01 &lt; values[0] &gt; 0.01: \n        print(name, values)\n'
"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\ny = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n\nX = X.reshape(1,10,1)\ny = y.reshape(1,10,1)\n\ndata_dim = 1\ntimesteps = 10\n\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, input_shape=(timesteps, data_dim)))\nmodel.add(LSTM(32, return_sequences=True))\nmodel.add(Dense(1, activation='linear'))\n\nprint(model.summary())\n\nmodel.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy'])\n\nmodel.fit(X,y, batch_size=1, epochs=1000)\n"
"cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n\ndef knn(self,X_train,X_test,Y_train,Y_test):\n\n   #implementación del algoritmo\n   knn = KNeighborsClassifier(n_neighbors=3).fit(X_train,Y_train)\n   #10XV\n   cv = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n   puntajes = sum(cross_val_score(knn, X_test, Y_test, \n                                        cv=cv,scoring='f1_weighted'))/10\n\n   print(puntajes)\n"
'TypeError: field prediction: DoubleType can not accept object 0 in type &lt;type \'int\'&gt;\n\npredictions = (predictions\n    .withColumn("label", predictions["label"].cast("double")))\n'
'setup(ext_modules=cythonize([Extension(\n    "pca", ["pca.pyx"], language=\'c++\'),\n    libraries=\'mlpack\',\n]))\n'
'predicted = sess.run(pred_, feed_dict={x: test_images[0:10]})\n'
'class UpdateHook(SessionRunHook):\n    def __init__(update_variable, other_variables):\n        self.update_op = tf.assign(update_variable, some_fn(other_variables))\n\n    def end(session):\n        session.run(self.update_op)\n'
"x = Convolution2D(16, (3, 3), activation='relu', padding='same')(x)  \n"
'pip install -U scikit-learn\n\nfrom six.moves import urllib\nfrom scipy.io import loadmat\nimport numpy as np\nfrom sklearn.linear_model  import SGDClassifier\nfrom scipy.io import loadmat\n\n# Load MNIST data #\nmnist_alternative_url = "https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat"\nmnist_path = "./mnist-original.mat"\nresponse = urllib.request.urlopen(mnist_alternative_url)\nwith open(mnist_path, "wb") as f:\n  content = response.read()\n  f.write(content)\nmnist_raw = loadmat(mnist_path)\nmnist = {\n  "data": mnist_raw["data"].T,\n  "target": mnist_raw["label"][0],\n  "COL_NAMES": ["label", "data"],\n  "DESCR": "mldata.org dataset: mnist-original",\n}\n\n# Assign X and y #\nX, y = mnist[\'data\'], mnist[\'target\']\n\n# Select first 60000 numbers #\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n\n# Shuffle order #\nshuffle_index  = np.random.permutation(60000)\nX_train, y_train = X_train[shuffle_index], y_train[shuffle_index]\n\n# Convert labels to binary (5 or "not 5") #\ny_train_5 = (y_train == 5)\ny_test_5 = (y_test == 5)\n\n# Train SGDClassifier #\nsgd_clf = SGDClassifier(max_iter=5, random_state=42)\nsgd_clf.fit(X_train, y_train_5)\n'
'def compute_distances_one_loop(self, X):\n    """\n    Compute the distance between each test point in X and each training point\n    in self.X_train using a single loop over the test data.\n\n    Input / Output: Same as compute_distances_two_loops\n    """\n    num_test = X.shape[0]\n    num_train = self.X_train.shape[0]\n    dists = np.zeros((num_test, num_train))\n\n    for i in range(num_test):\n      tmp = \'%s %d\' % ("\\nfor i:", i)\n      print(tmp)\n\n      #######################################################################\n      # TODO:                                                               #\n      # Compute the l2 distance between the ith test point and all training #\n      # points, and store the result in dists[i, :].                        #\n      #######################################################################\n      dists[i] = np.sum(np.square(X[i] - self.X_train), axis=1)\n      print(dists[i])\n      #######################################################################\n      #                         END OF YOUR CODE                            #\n      #######################################################################\n    return dists\n'
'    weights_gradient_output += neural_input * output_delta\n    #bias_output += output_delta\n'
'y_pred_prob = np.array(gaussian.predict_proba(X_test))\nmetrics.roc_auc_score(y_test, y_pred_prob[:,1])\n'
'decoder.build(input_shape=(None, encoding_dim))   # note that batch axis must be included\n'
"elmo = ElmoEmbedder(\n    options_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_options.json', \n    weight_file='https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway_5.5B/elmo_2x4096_512_2048cnn_2xhighway_5.5B_weights.hdf5'\n)\n"
'target = target.to(dtype=torch.long)\n'
"model.add(LSTM(128, input_shape=(None,4), activation='relu', return_sequences=True))\n"
'import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n# fix random seed for reproducibility\nseed = 1\nnp.random.seed(seed)\n\n# Generate dataset with linear outputs.\nsample_count = 1000\nrow_count = 2\ncolumn_count = 3\nX = np.random.uniform(size=sample_count * row_count * column_count)\nX = X.reshape(sample_count, row_count, column_count)\nY = 2*X[:, 0, 0] + X[:, 0, 1] + 2*X[:, 0, 2] + 11*X[:, 1, 0] + 3*X[:, 1, 1]\nY = Y.reshape(sample_count, 1)\n\n\n# define base model\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(row_count * column_count * 2,\n                    input_shape=(row_count, column_count),\n                    kernel_initializer=\'normal\',\n                    activation=\'relu\'))\n    model.add(Flatten())\n    model.add(Dense(1, kernel_initializer=\'normal\'))\n    # Compile model\n    model.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n    return model\n\n\n# evaluate model with standardized dataset\nestimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n\nkfold = KFold(n_splits=10, random_state=seed)\nresults = cross_val_score(estimator, X, Y, cv=kfold)\nprint("Results: %.2f (%.2f) MSE" % (results.mean(), results.std()))\n\nestimator.fit(X, Y)\n\ntest_samples = np.zeros((row_count*column_count, row_count, column_count))\nfor sample_num, (row_num, column_num) in enumerate((row_num, column_num)\n                                                   for row_num in range(row_count)\n                                                   for column_num in range(column_count)):\n    test_samples[sample_num, row_num, column_num] = 1\npredictions = estimator.predict(test_samples)\nprint(predictions)\n'
"X_original = [\n    {'a': 2, 'b': 3},\n    {'a': 7, 'b': 6},\n    {'a': 1, 'b': 7},\n]\n\nkeys = sorted(X_original[0].keys())\nX_values = [\n    [d[k] for k in keys]\n    for d in X_original]\n\nscaler = RobustScaler()\nX_transformed = scaler.fit_transform(X_values)\n\nX_final = [\n    dict(zip(keys, x))\n    for x in X_transformed]\n\n&gt;&gt;&gt; X_original\n[{'a': 2, 'b': 3}, {'a': 7, 'b': 6}, {'a': 1, 'b': 7}]\n&gt;&gt;&gt; X_values\n[[2, 3], [7, 6], [1, 7]]\n&gt;&gt;&gt; X_transformed\n[[ 0.         -1.5       ]\n [ 1.66666667  0.        ]\n [-0.33333333  0.5       ]]\n&gt;&gt;&gt; X_final\n[{'a': 0.0, 'b': -1.5},\n {'a': 1.6666666666666667, 'b': 0.0},\n {'a': -0.3333333333333333, 'b': 0.5}]\n\nX_original = [\n    {'a': 2, 'b': 3},\n    {'a': 7, 'b': 6},\n    {'a': 1, 'b': 7},\n]\nkeys = sorted(X_original[0].keys())\n\nscaler = RobustScaler()\nX_transformed = scaler.fit_transform([[d[k] for k in keys] for d in X_original])\nX_final = [dict(zip(keys, x)) for x in X_transformed]\n"
'iris.target[0:50]\n# result\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0])\n\niris.target[50:100]\n# result:\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1])\n\niris.target[100:150]\n# result:\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2])\n\nValueError: Only one class present in y_true\n\nfrom sklearn.utils import shuffle\nX_s, y_s = shuffle(X, y)\ncross_val_score(model, X_s, y_s, cv=3, scoring="roc_auc")\n'
'import numpy as np\n\n# either reshape it\ninput_arr = np.reshape(input_arr, (1, 24))\n\n# or add a new axis to the beginning\ninput_arr = np.expand_dims(input_arr, axis=0)\n\n# then call the predict method\npreds = model.predict(input_arr)  # Note that the `preds` would have a shape of `(1, action_size)`\n'
'xtrain,ytrain,xtest,ytest=traintestsplit(c,d,testsize=0.30)\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n[...]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n xtrain, xtest, ytrain, ytest = train_test_split(c,d,testsize=0.30)\n'
"...\nrun.log_image('Plot', plot=plt)\n"
'import numpy, scipy, scipy.optimize\nimport matplotlib\nfrom mpl_toolkits.mplot3d import  Axes3D\nfrom matplotlib import cm # to colormap 3D surfaces from blue to red\nimport matplotlib.pyplot as plt\n\ngraphWidth = 800 # units are pixels\ngraphHeight = 600 # units are pixels\n\n# 3D contour plot lines\nnumberOfContourLines = 16\n\n\ndef SurfacePlot(func, data, fittedParameters):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n\n    matplotlib.pyplot.grid(True)\n    axes = Axes3D(f)\n\n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    xModel = numpy.linspace(min(x_data), max(x_data), 20)\n    yModel = numpy.linspace(min(y_data), max(y_data), 20)\n    X, Y = numpy.meshgrid(xModel, yModel)\n\n    Z = func(numpy.array([X, Y]), *fittedParameters)\n\n    axes.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=1, antialiased=True)\n\n    axes.scatter(x_data, y_data, z_data) # show data along with plotted surface\n\n    axes.set_title(\'Surface Plot (click-drag with mouse)\') # add a title for surface plot\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n    axes.set_zlabel(\'Z Data\') # Z axis data label\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot or else thaere can be memory and process problems\n\n\ndef ContourPlot(func, data, fittedParameters):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    xModel = numpy.linspace(min(x_data), max(x_data), 20)\n    yModel = numpy.linspace(min(y_data), max(y_data), 20)\n    X, Y = numpy.meshgrid(xModel, yModel)\n\n    Z = func(numpy.array([X, Y]), *fittedParameters)\n\n    axes.plot(x_data, y_data, \'o\')\n\n    axes.set_title(\'Contour Plot\') # add a title for contour plot\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n\n    CS = matplotlib.pyplot.contour(X, Y, Z, numberOfContourLines, colors=\'k\')\n    matplotlib.pyplot.clabel(CS, inline=1, fontsize=10) # labels for contours\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot or else thaere can be memory and process problems\n\n\ndef ScatterPlot(data):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n\n    matplotlib.pyplot.grid(True)\n    axes = Axes3D(f)\n    x_data = data[0]\n    y_data = data[1]\n    z_data = data[2]\n\n    axes.scatter(x_data, y_data, z_data)\n\n    axes.set_title(\'Scatter Plot (click-drag with mouse)\')\n    axes.set_xlabel(\'X Data\')\n    axes.set_ylabel(\'Y Data\')\n    axes.set_zlabel(\'Z Data\')\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot or else thaere can be memory and process problems\n\n\ndef func(data, a, b, c):\n    x = data[0]\n    y = data[1]\n    return (a * x) + (y * b) + c\n\n\nif __name__ == "__main__":\n    xData = numpy.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n    yData = numpy.array([11.0, 12.1, 13.0, 14.1, 15.0, 16.1, 17.0, 18.1, 90.0])\n    zData = numpy.array([1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.0, 9.9])\n\n    data = [xData, yData, zData]\n\n    initialParameters = [1.0, 1.0, 1.0] # these are the same as scipy default values in this example\n\n    # here a non-linear surface fit is made with scipy\'s curve_fit()\n    fittedParameters, pcov = scipy.optimize.curve_fit(func, [xData, yData], zData, p0 = initialParameters)\n\n    ScatterPlot(data)\n    SurfacePlot(func, data, fittedParameters)\n    ContourPlot(func, data, fittedParameters)\n\n    print(\'fitted prameters\', fittedParameters)\n\n    modelPredictions = func(data, *fittedParameters) \n\n    absError = modelPredictions - zData\n\n    SE = numpy.square(absError) # squared errors\n    MSE = numpy.mean(SE) # mean squared errors\n    RMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\n    Rsquared = 1.0 - (numpy.var(absError) / numpy.var(zData))\n    print(\'RMSE:\', RMSE)\n    print(\'R-squared:\', Rsquared)\n'
'import tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow import feature_column\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence\nimport tensorflow.keras.utils as ku\nfrom tensorflow.keras.utils import plot_model\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nDATA_PATH = \'C:\\SoloLearnMachineLearning\\Stackoverflow\\TextDataset.csv\'\n\n#it is just two column csv, like:\n# text;label\n# A wiki is run using wiki software;0\n# otherwise known as a wiki engine.;1\n\ndataframe = pd.read_csv(DATA_PATH, delimiter = \';\')\ndataframe.head()\n\n# Preprocessing before feature_clolumn includes\n# - getting the vocabulary\n# - tokenization, which means only splitting on tokens.\n#   Encoding sentences with vocablary will be done by feature_column!\n# - padding\n# - truncating\n\n# Build vacabulary\nvocab_size = 100\noov_tok = \'&lt;OOV&gt;\'\n\nsentences = dataframe[\'text\'].to_list()\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token="&lt;OOV&gt;")\n\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\n\n# if word_index shorter then default value of vocab_size we\'ll save actual size\nvocab_size=len(word_index)\nprint("vocab_size = word_index = ",len(word_index))\n\n# Split sentensec on tokens. here token = word\n# text_to_word_sequence() has good default filter for \n# charachters include basic punctuation, tabs, and newlines\ndataframe[\'text\'] = dataframe[\'text\'].apply(text_to_word_sequence)\n\ndataframe.head()\n\nmax_length = 6\n\n# paddind and trancating setnences\n# do that directly with strings without using tokenizer.texts_to_sequences()\n# the feature_colunm will convert strings into numbers\ndataframe[\'text\']=dataframe[\'text\'].apply(lambda x, N=max_length: (x + N * [\'\'])[:N])\ndataframe[\'text\']=dataframe[\'text\'].apply(lambda x, N=max_length: x[:N])\ndataframe.head()\n\n# Define method to create tf.data dataset from Pandas Dataframe\ndef df_to_dataset(dataframe, label_column, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    #labels = dataframe.pop(label_column)\n    labels = dataframe[label_column]\n\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds\n\n# Split dataframe into train and validation sets\ntrain_df, val_df = train_test_split(dataframe, test_size=0.2)\n\nprint(len(train_df), \'train examples\')\nprint(len(val_df), \'validation examples\')\n\nbatch_size = 32\nds = df_to_dataset(dataframe, \'label\',shuffle=False,batch_size=batch_size)\n\ntrain_ds = df_to_dataset(train_df, \'label\',  shuffle=False, batch_size=batch_size)\nval_ds = df_to_dataset(val_df, \'label\', shuffle=False, batch_size=batch_size)\n\n# and small batch for demo\nexample_batch = next(iter(ds))[0]\nexample_batch\n\n# Helper methods to print exxample outputs of for defined feature_column\n\ndef demo(feature_column):\n    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n    print(feature_layer(example_batch).numpy())\n\ndef seqdemo(feature_column):\n    sequence_feature_layer = tf.keras.experimental.SequenceFeatures(feature_column)\n    print(sequence_feature_layer(example_batch))\n\n# Define categorical colunm for our text feature, \n# which is preprocessed into lists of tokens\n# Note that key name should be the same as original column name in dataframe\ntext_column = feature_column.\n            categorical_column_with_vocabulary_list(key=\'text\', \n                                                    vocabulary_list=list(word_index))\n#indicator_column produce one-hot-encoding. These lines just to compare with embedding\n#print(demo(feature_column.indicator_column(payment_description_3)))\n#print(payment_description_2,\'\\n\')\n\n# argument dimention here is exactly the dimension of the space in which tokens \n# will be presented during model\'s learning\n# see the tutorial at https://www.tensorflow.org/beta/tutorials/text/word_embeddings\ntext_embedding = feature_column.embedding_column(text_column, dimension=8)\nprint(demo(text_embedding))\n\n# The define the layers and model it self\n# This example uses Keras Functional API instead of Sequential just for more generallity\n\n# Define DenseFeatures layer to pass feature_columns into Keras model\nfeature_layer = tf.keras.layers.DenseFeatures(text_embedding)\n\n# Define inputs for each feature column.\n# See https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-502218673\nfeature_layer_inputs = {}\n\n# Here we have just one column\n# Important to define tf.keras.Input with shape \n# corresponding to lentgh of our sequence of words\nfeature_layer_inputs[\'text\'] = tf.keras.Input(shape=(max_length,),\n                                              name=\'text\',\n                                              dtype=tf.string)\nprint(feature_layer_inputs)\n\n# Define outputs of DenseFeatures layer \n# And accually use them as first layer of the model\nfeature_layer_outputs = feature_layer(feature_layer_inputs)\nprint(feature_layer_outputs)\n\n# Add consequences layers.\n# See https://keras.io/getting-started/functional-api-guide/\nx = tf.keras.layers.Dense(256, activation=\'relu\')(feature_layer_outputs)\nx = tf.keras.layers.Dropout(0.2)(x)\n\n# This example supposes binary classification, as labels are 0 or 1\nx = tf.keras.layers.Dense(1, activation=\'sigmoid\')(x)\n\nmodel = tf.keras.models.Model(inputs=[v for v in feature_layer_inputs.values()],\n                              outputs=x)\n\nmodel.summary()\n\n# This example supposes binary classification, as labels are 0 or 1\nmodel.compile(optimizer=\'adam\',\n              loss=\'binary_crossentropy\',\n              metrics=[\'accuracy\']\n              #run_eagerly=True\n             )\n\n# Note that fit() method looking up features in train_ds and valdation_ds by name in \n# tf.keras.Input(shape=(max_length,), name=\'text\'\n\n# This model of cause will learn nothing because of fake data.\n\nnum_epochs = 5\nhistory = model.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs=num_epochs,\n                    verbose=1\n                    )\n\n# Define categorical colunm for our text feature, \n# which is preprocessed into lists of tokens\n# Note that key name should be the same as original column name in dataframe\ntext_column = feature_column.\n              sequence_categorical_column_with_vocabulary_list(key=\'text\', \n                                                vocabulary_list=list(word_index))\n\n# arguemnt dimention here is exactly the dimension of the space in \n# which tokens will be presented during model\'s learning\n# see the tutorial at https://www.tensorflow.org/beta/tutorials/text/word_embeddings\ntext_embedding = feature_column.embedding_column(text_column, dimension=8)\nprint(seqdemo(text_embedding))\n\n# The define the layers and model it self\n# This example uses Keras Functional API instead of Sequential \n# just for more generallity\n\n# Define SequenceFeatures layer to pass feature_columns into Keras model\nsequence_feature_layer = tf.keras.experimental.SequenceFeatures(text_embedding)\n\n# Define inputs for each feature column. See\n# см. https://github.com/tensorflow/tensorflow/issues/27416#issuecomment-502218673\nfeature_layer_inputs = {}\nsequence_feature_layer_inputs = {}\n\n# Here we have just one column\n\nsequence_feature_layer_inputs[\'text\'] = tf.keras.Input(shape=(max_length,),\n                                                       name=\'text\',\n                                                       dtype=tf.string)\nprint(sequence_feature_layer_inputs)\n\n# Define outputs of SequenceFeatures layer \n# And accually use them as first layer of the model\n\n# Note here that SequenceFeatures layer produce tuple of two tensors as output.\n# We need just first to pass next.\nsequence_feature_layer_outputs, _ = sequence_feature_layer(sequence_feature_layer_inputs)\nprint(sequence_feature_layer_outputs)\n# Add consequences layers. See https://keras.io/getting-started/functional-api-guide/\n\n# Conv1D and MaxPooling1D will learn features from words order\nx = tf.keras.layers.Conv1D(8,4)(sequence_feature_layer_outputs)\nx = tf.keras.layers.MaxPooling1D(2)(x)\n# Add consequences layers. See https://keras.io/getting-started/functional-api-guide/\nx = tf.keras.layers.Dense(256, activation=\'relu\')(x)\nx = tf.keras.layers.Dropout(0.2)(x)\n\n# This example supposes binary classification, as labels are 0 or 1\nx = tf.keras.layers.Dense(1, activation=\'sigmoid\')(x)\n\nmodel = tf.keras.models.Model(inputs=[v for v in sequence_feature_layer_inputs.values()],\n                              outputs=x)\nmodel.summary()\n\n# This example supposes binary classification, as labels are 0 or 1\nmodel.compile(optimizer=\'adam\',\n              loss=\'binary_crossentropy\',\n              metrics=[\'accuracy\']\n              #run_eagerly=True\n             )\n\n# Note that fit() method looking up features in train_ds and valdation_ds by name in \n# tf.keras.Input(shape=(max_length,), name=\'text\'\n\n# This model of cause will learn nothing because of fake data.\n\nnum_epochs = 5\nhistory = model.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs=num_epochs,\n                    verbose=1\n                    )\n'
"from sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nX=np.load('Mistery.npy')\n\nwx = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, random_state = 0)\n    kmeans.fit(X)\n    wx.append(kmeans.inertia_)\nplt.plot(range(1, 11), wx)\nplt.xlabel('Number of clusters')\nplt.ylabel('Variance Explained')\nplt.show()\n\narray([0.86992608, 0.11252552, 0.25573737, ..., 0.32652233, 0.14927118,\n        0.1662449 ])\n\n[0.86992608, 0.11252552, 0.25573737, ..., 0.32652233, 0.14927118,\n        0.1662449 ]\n\nimport pandas as pd\npd.DataFrame(X).dtypes\n\nn=5\nkmeans=KMeans(n_clusters=n, random_state=20).fit(X)\nlabels_of_clusters = kmeans.fit_predict(X)\n\narray([1, 4, 0, 0, 4, 1, 4, 0, 2, 0, 0, 4, 3, 1, 4, 2, 2, 3, 0, 1, 1, 0,\n       4, 4, 2, 0, 3, 0, 3, 1, 1, 2, 1, 0, 2, 4, 0, 3, 2, 1, 1, 2, 2, 2,\n       2, 0, 0, 4, 1, 3, 1, 0, 1, 4, 1, 0, 0, 0, 2, 0, 1, 2, 2, 1, 2, 2,\n       0, 4, 4, 4, 4, 3, 1, 2, 1, 2, 2, 1, 1, 3, 4, 3, 3, 1, 0, 1, 2, 2,\n       1, 2, 3, 1, 3, 3, 4, 2, 2, 0, 2, 1, 3, 4, 2, 0, 2, 1, 3, 3, 3, 4,\n       3, 1, 4, 4, 4, 2, 0, 3, 2, 0, 1, 2, 2, 0, 3, 1, 1, 1, 4, 0, 2, 2,\n       0, 0, 1, 1, 0, 3, 0, 2, 2, 1, 2, 2, 4, 0, 1, 0, 3, 1, 4, 4, 0, 4,\n       1, 2, 0, 2, 4, 0, 1, 2, 3, 1, 1, 0, 3, 2, 4, 0, 1, 3, 1, 2, 4, 3,\n       1, 1, 2, 0, 0, 2, 3, 1, 3, 4, 1, 2, 2, 0, 2, 1, 4, 3, 1, 0, 3, 2,\n       4, 1, 4, 1, 4, 4, 0, 4, 4, 3, 1, 3, 4, 0, 4, 2, 1, 1, 3, 4, 0, 4,\n       4, 4, 4, 2, 4, 2, 3, 4, 3, 3, 1, 1, 4, 2, 3, 0, 2, 4])\n\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=200, centers=4,\n                       cluster_std=0.60, random_state=0)\n\nkmeans = KMeans(n_clusters=4, random_state=0).fit(X)\ncc=kmeans.fit_predict(X)\n\nplt.scatter(X[:, 0], X[:, 1], c=cc, s=50, cmap='viridis')\n"
'model.add(Dense(4, input_dim=4, activation="relu", kernel_initializer="normal"))\n\nmodel.add(Dense(16, activation="relu"))\nmodel.add(Dense(32, activation="relu"))\n\nmodel.add(Dense(3, activation="softmax", kernel_initializer="normal"))\n\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\n\nseed = 7\nnumpy.random.seed(seed)\n\nfrom sklearn.datasets import load_iris\n\nX, encoded_Y = load_iris(return_X_y=True)\nmms = MinMaxScaler()\nX = mms.fit_transform(X)\n\ndummy_y = np_utils.to_categorical(encoded_Y)\n\ndef baseline_model():\n\n    model = Sequential()\n    model.add(Dense(4, input_dim=4, activation="relu", kernel_initializer="normal"))\n    model.add(Dense(8, activation="relu", kernel_initializer="normal"))\n    model.add(Dense(3, activation="softmax", kernel_initializer="normal"))\n\n    model.compile(loss= \'categorical_crossentropy\' , optimizer=\'adam\', metrics=[\n        \'accuracy\' ])\n\n    return model\n\nestimator = KerasClassifier(build_fn=baseline_model, epochs=200, verbose=0)\nkfold = KFold(n_splits=10, shuffle=True, random_state=seed)\nresults = cross_val_score(estimator, X, dummy_y, cv=kfold)\nprint(results)\n\nOut[5]: \narray([0.60000002, 0.93333334, 1.        , 0.66666669, 0.80000001,\n       1.        , 1.        , 0.93333334, 0.80000001, 0.86666667])\n'
'temp = np.zeros(shape=[train_label_list.shape[0], img_size[1], img_size[0], 3])\ntemp[:, :, :, 0] = train_label_list == 0\ntemp[:, :, :, 1] = train_label_list == 1\ntemp[:, :, :, 2] = train_label_list == 2\n'
"from keras import optimizers\n\ndef create_model(layers, learn_rate):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,input_dim = 20,activation = 'relu'))\n        else:\n            model.add(Dense(nodes,activation = 'relu'))\n    model.add(Dense(units = 4,activation = 'softmax')) \n\n    model.compile(optimizer=optimizers.adam(lr=learn_rate), loss='categorical_crossentropy',metrics=['accuracy']) \n    return model\n"
'z_scaled = scaler.transform(z) \npredictions = model.predict_classes(z_scaled)\n'
"    def __init__(self):    \n        ##Nothing special to be done here\n        super(peel_the_layer, self).__init__()\n        \n    def build(self, input_shape):\n        ##Define the shape of the weights and bias in this layer\n        ##This is a 1 unit layer. \n        units=1\n        ##last index of the input_shape is the number of dimensions of the prev\n        ##RNN layer. last but 1 index is the num of timesteps\n        self.w=self.add_weight(name=&quot;att_weights&quot;, shape=(input_shape[-1], units), initializer=&quot;normal&quot;) #name property is useful for avoiding RuntimeError: Unable to create link.\n        self.b=self.add_weight(name=&quot;att_bias&quot;, shape=(input_shape[-2], units), initializer=&quot;zeros&quot;)\n        super(peel_the_layer,self).build(input_shape)\n        \n    def call(self, x):\n        ##x is the input tensor..each word that needs to be attended to\n        ##Below is the main processing done during training\n        ##K is the Keras Backend import\n        e = K.tanh(K.dot(x,self.w)+self.b)\n        a = K.softmax(e, axis=1)\n        output = x*a\n        \n        ##return the ouputs. 'a' is the set of attention weights\n        ##the second variable is the 'attention adjusted o/p state' or context\n        return a, K.sum(output, axis=1)\n\n        a, context = peel_the_layer()(lstm_out)\n        ##context is the o/p which be the input to your classification layer\n        ##a is the set of attention weights and you may want to route them to a display\n"
"import numpy as np\nimport imgaug.augmenters as iaa\n\nimg = np.random.randint(0,256, (1,224,224,3)).astype('float32')\n\naug = iaa.AdditiveGaussianNoise(scale=(0, 0.2*255))\naugmented_image = aug(images=img)\n"
"from tensorflow.keras.activations import relu\n\ndef lrelu(x):\n   return relu(x, alpha=0.01)\n\nmodel = Sequential()\nmodel.add(Dense( 10, activation=lrelu, input_dim=12 ))\n\n&gt;&gt;&gt; lrelu1 = lambda x: 0\n&gt;&gt;&gt; def lrelu2(x):\n...   return 0\n...\n&gt;&gt;&gt; lrelu1.__name__\n'&lt;lambda&gt;'\n&gt;&gt;&gt; lrelu2.__name__\n'lrelu2'\n&gt;&gt;&gt;\n"
"import pandas as pd\nimport numpy as np\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\ndf = pd.DataFrame(columns=['m', 'c', 'mm', 'cc', 't', 'target'])\ninput_list = []\nfor i in range (800):\n    m = np.random.rand(10,20, 5, 5)\n    c = np.random.rand(10, 3)\n    mm = np.random.rand(10)\n    cc = np.random.rand(20, 5, 6, 2)\n    t = np.random.rand(10, 3)\n\n    dict = {'m': m.flatten(),\n            'c': c.flatten(),\n            'mm': mm.flatten(),\n            'cc': cc.flatten(),\n            't': t.flatten(),\n            'target': random.randint(1, 3)}\n    input_list.append(dict)\n\ndf = df.append(input_list, ignore_index=True)\n\n\n#\n# step 2 - split to train and test\n# \ntrain, test = train_test_split(df, test_size=0.2)\nx_train = np.asarray([np.concatenate(x) for x in train.to_numpy()[:,0:-2]])\ny_train = train.to_numpy()[:,-1]\nx_test = np.asarray([np.concatenate(x) for x in test.to_numpy()[:, 0:-2]])\ny_test = test.to_numpy()[:,-1]\n\n\nlb = LabelEncoder()\ny_train_hot = to_categorical(lb.fit_transform(y_train))\ny_test_hot = to_categorical(lb.fit_transform(y_test))\n\n\n#\n# step 3 - build simple model\n#\nmodel = Sequential()\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n\n#\n# step 4 - try to fit the model\n#\nmodel.fit(x_train, y_train_hot, batch_size=20, epochs=20, verbose=1, validation_data=(x_test, y_test_hot))\n"
'from sklearn.metrics import zero_one_score\n\ny_pred = svm.predict(test_samples)\naccuracy = zero_one_score(y_test, y_pred)\nerror_rate = 1 - accuracy\n'
'testOutput = { [1,0,0,1] : [1], [1,1,0,1] : [0], [1,0,1,1]:[0], [1,0,1,0]:[1] }\n\nfor input, expectedOutput in testInput.items():\n    output = net.activate(input)\n    if output != expectedOutput:\n        print "{} didn\'t match the desired output." \n        print "Expected {}, got {}".format(input, expectedOutput, output)\n'
"tfv = TfidfVectorizer(\n    min_df=3,\n    max_features=None,\n    strip_accents='unicode',                    \n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    ngram_range=(1, 2), \n    use_idf=1,\n    smooth_idf=1,\n    sublinear_tf=1)\ndiv = DictVectorizer()\n\nX = []\n\n# fit/transform the TfidfVectorizer on the training data\nvectors = tfv.fit_transform(traindata)\n\nfor i, pagerank in enumerate(pageranks):\n    feature_dict = {'pagerank': pagerank}\n    # get ith row of the tfidf matrix (corresponding to sample)\n    row = vect.getrow(i)    \n\n    # filter the feature names corresponding to the sample\n    all_words = tfv.get_feature_names()\n    words = [all_words[ind] for ind in row.indices] \n\n    # associate each word (feature) with its corresponding score\n    word_score = dict(zip(words, row.data)) \n\n    # concatenate the word feature/score with the datamining feature/value\n    X.append(dict(word_score.items() + feature_dict.items()))\n\ndiv.fit_transform(X)  # training data based on both Tfidf features and pagerank\n"
'training = ["this was a good movie",\n            "this was a bad movie",\n            "i went to the movies",\n            "this movie was very exiting it was great", \n            "this is a boring film"]\n\nlabels = [\'POS\', \'NEG\', \'NEU\', \'POS\', \'NEG\']\n\n&gt;&gt;&gt; from sklearn.feature_extraction.text import HashingVectorizer\n&gt;&gt;&gt; vect = HashingVectorizer(n_features=5, stop_words=\'english\', non_negative=True)\n&gt;&gt;&gt; X_train = vect.fit_transform(training)\n&gt;&gt;&gt; X_train.toarray()\n[[ 0.          0.70710678  0.          0.          0.70710678]\n [ 0.70710678  0.70710678  0.          0.          0.        ]\n [ 0.          0.          0.          0.          0.        ]\n [ 0.          0.89442719  0.          0.4472136   0.        ]\n [ 1.          0.          0.          0.          0.        ]]\n\nfrom sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X_train, labels)\n\n&gt;&gt;&gt; test = ["I don\'t like this movie it sucks it doesn\'t liked me"]\n&gt;&gt;&gt; X_pred = vect.transform(test)\n&gt;&gt;&gt; model.predict(X_pred)\n[\'NEG\']\n\n&gt;&gt;&gt; test = ["I think it was a good movie"]\n&gt;&gt;&gt; X_pred = vect.transform(test)\n&gt;&gt;&gt; model.predict(X_pred)\n[\'POS\']\n'
'0 = approval request\n2 = laptop working\n1 = bw table\n'
'from io import StringIO\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\n\ns="""\n2015-01-01,1,2,3\n2015-01-03,1,2,4\n2015-01-05,1,2,4\n2015-01-07,1,4,3\n"""\ndf = pd.read_csv(StringIO(s), index_col=0, parse_dates=True, header=None)\n\nmlb = MultiLabelBinarizer()\nlabels = mlb.fit_transform(df.values)\nlabels\n[[1 1 1 0]\n [1 1 0 1]\n [1 1 0 1]\n [1 0 1 1]]\n\nX = labels[:-1]   \n[[1 1 1 0]\n [1 1 0 1]\n [1 1 0 1]]\n\n&gt;&gt;&gt; X.flatten()\n[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n\n&gt;&gt;&gt; labels[-1]\n[1 0 1 1]\n'
"test_matrix = dict_vect.transform(testing_data[['G1','G2','sex','school','age']])\n\n['G1','G2','sex','school','age']\n\nIn [43]:\n\ndf = pd.DataFrame(columns=['a','b'])\ndf\nOut[43]:\nEmpty DataFrame\nColumns: [a, b]\nIndex: []\nIn [44]:\n\ndf['a','b']\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-44-33332c7e7227&gt; in &lt;module&gt;()\n----&gt; 1 df['a','b']\n\n......    \npandas\\hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12349)()\n\npandas\\hashtable.pyx in pandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:12300)()\n\nKeyError: ('a', 'b')\n\nIn [45]:\n\ndf[['a','b']]\nOut[45]:\nEmpty DataFrame\nColumns: [a, b]\nIndex: []\n"
'clf = MultinomialNB(alpha=0.1).fit( idfX, ["dog" , "dog", "human"])\n'
'lda = LDA(n_components=2) #creating a LDA object\nlda = lda.fit(X, y) #learning the projection matrix\nX_lda = lda.transform(X) #using the model to project X \n# .... getting Z as test data....\nZ = lda.transform(Z) #using the model to project Z\nz_labels = lda.predict(Z) #gives you the predicted label for each sample\nz_prob = lda.predict_proba(Z) #the probability of each sample to belong to each class\n'
'for i, col in enumerate(y.columns.tolist(), 1):\n    y.loc[:, col] *= i\ny = y.sum(axis=1)\n'
"X=np.array(arrFinal[:,1:-17]).astype(np.float64)\nXtest=np.array(X)\nY=np.array(arrFinal[:,522:]).astype(int)\n\nclf = Pipeline([\n      ('vt', VarianceThreshold()),\n      ('chi2', SelectKBest(chi2, k=100)),\n      ('rbf',SVC())\n])\nclf = OneVsRestClassifier(clf)\nclf.fit(X, Y)\nans=clf.predict(X_test)\n"
'this_score *= this_n_test_samples \nn_test_samples += this_n_test_samples\n'
'ssh -X user@host\n'
'fit(X, y, sample_weight=None)\n\n    X : numpy array or sparse matrix of shape [n_samples, n_features]\n        Training data\n    y : numpy array of shape [n_samples, n_targets]\n        Target values\n    sample_weight : numpy array of shape [n_samples]\n        Individual weights for each sample\n\nxs = [[visit for section, visit in user] for user in xs]\n\nxs = [[1, 0, 1, 0], # user1\n      [0, 1, 1, 0], # user2\n      ...\n      ]\n\nimport numpy as np\nxs = np.asarray(xs)[:,:,1]\n'
'y    = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\npred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n#                                          -^- see the change here \n\n#Output:\n\nfpr → [ 0.  1.]\ntpr → [ 0.  1.]\n\ny    = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\npred = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n#                                          -^- see the change here \n\n#Output:\n\nfpr → [ 0.  1.]\ntpr → [ 1.  1.]\n'
"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\nfig, ax = plt.subplots()\nim = OffsetImage(np.arange(100).reshape((10, 10)))\nab1 = AnnotationBbox(im, (0.5, 0.5),\n                    bboxprops =dict(edgecolor='red'))\nab2 = AnnotationBbox(im, (0.75, 0.75),\n                    bboxprops =dict(edgecolor=[0.2,1.,0.5]  ))\n\nax.add_artist(ab1)\nax.add_artist(ab2)\nplt.show()\n"
'import matplotlib.pyplot as plt\n\n[...]\n\nIn [x]: df.boxplot()\n        plt.show()\n'
'magick page-28.png -alpha off +dither -colors 2 -colorspace gray -normalize -statistic median 1x200 result.png\n\nconvert page-28.png -alpha off +dither -colors 2 -colorspace gray -normalize -statistic median 1x200 -negate -format "%[fx:mean*w*h]" info:\n90224\n\nconvert page-27.png -alpha off +dither -colors 2 -colorspace gray -normalize -statistic median 1x200 -negate -format "%[fx:mean*w*h]" info:\n0\n\nconvert -density 18 book.pdf info:\n\nconvert -density 288 book.pdf[25] page-25.png\n'
"import pandas as pd\n\ndf = pd.DataFrame({'clusters' : k.labels_, 'labels' : y})\n\ndf.groupby('clusters').apply(lambda cluster: cluster.sum()/cluster.count())\n"
'   ./darknet detector demo cfg/coco.data cfg/yolo.cfg yolo.weights http://192.168.1.xx/cam.jpg\n'
'classification_report(y_true, y_pred, ...)\n\ny_true : 1d array-like, or label indicator array / sparse matrix    \n         Ground truth (correct) target values.\n\ny_pred : 1d array-like, or label indicator array / sparse matrix\n         Estimated targets as returned by a classifier.\n\nprint(classification_report(test_data["hand"], predictions))\n'
'&gt;&gt;&gt; y\narray([1, 0, 1, 1, 1, 0, 0, 1, 0, 1])\n&gt;&gt;&gt; X\narray([[-25,  62,  94,  70,  96,  70,  38, -18, -57,   1],\n       [ 40,  86, -98, -48,  40,  29,   4, -83,  44, -12],\n       [ 57,  23, -96,  97, -24, -93, -33, -64,  61,  15],\n       [ 44,  29,  31, -38,  11,  85,  37, -96, -37, -70],\n       [-10, -37, -24, -66,  27, -44, -16, -50,   3, -91],\n       [-97,  81,  52,  41,  39, -14,  95,  76,  28, -32],\n       [-74,  49, -91, -65, -96,  86, -13,  43,  22,  80],\n       [  5,  20, -77,  74, -89,  46, -90,  95,  30,  13],\n       [ 36,   6,  55, -74, -49, -66,  38,  37, -84,  28],\n       [-23, -28, -32, -30,  -4, -52,  -4,  99, -67, -98]])\n\n&gt;&gt;&gt; def sample_positive(X, y, num):\n...     pos_index = np.where(y == 1)[0]\n...     rows = np.random.choice(pos_index, size=num, replace=False)\n...     mat = X[rows,:]\n...     return (mat, rows)\n...\n&gt;&gt;&gt; X_sample, idx = sample_positive(X, y, 2)\n&gt;&gt;&gt; X_sample\narray([[-23, -28, -32, -30,  -4, -52,  -4,  99, -67, -98],\n       [-10, -37, -24, -66,  27, -44, -16, -50,   3, -91]])\n&gt;&gt;&gt; idx\narray([9, 4])\n&gt;&gt;&gt; X\narray([[-25,  62,  94,  70,  96,  70,  38, -18, -57,   1],\n       [ 40,  86, -98, -48,  40,  29,   4, -83,  44, -12],\n       [ 57,  23, -96,  97, -24, -93, -33, -64,  61,  15],\n       [ 44,  29,  31, -38,  11,  85,  37, -96, -37, -70],\n       [-10, -37, -24, -66,  27, -44, -16, -50,   3, -91],\n       [-97,  81,  52,  41,  39, -14,  95,  76,  28, -32],\n       [-74,  49, -91, -65, -96,  86, -13,  43,  22,  80],\n       [  5,  20, -77,  74, -89,  46, -90,  95,  30,  13],\n       [ 36,   6,  55, -74, -49, -66,  38,  37, -84,  28],\n       [-23, -28, -32, -30,  -4, -52,  -4,  99, -67, -98]])\n&gt;&gt;&gt; y\narray([1, 0, 1, 1, 1, 0, 0, 1, 0, 1])\n'
"import numpy as np\nfrom scipy.optimize import curve_fit\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\ndef f(x, a, b, n):\n    return a * x ** n  / (x ** n + b)\n\n\ndata = pd.read_csv('data.txt.txt', sep='\\t')\n\ny = data['y'].astype(float)\nx = data['X'].astype(float)\n\npopt, pcov = curve_fit(f, x, y, p0=[1800., 20., 1.])\n\nplt.scatter(x, y)\nplt.plot(x, f(x, *popt), 'r-')\n\nplt.show()\n"
"# Add enumeration per sample using `comcount` on group\ndfRaw['reading'] = dfRaw.groupby('Sample').cumcount()\n\n# Pivot\ndfFin = dfRaw.pivot(index='Sample', columns='reading')\n\n# If desired: reduce multi-index header to single-index header\ndfFin.columns = [''.join(str(col)).strip() for col in dfFin.columns.values]\ndfFin.reset_index(inplace=True)\n\n        ('A1', 0L)  ('A1', 1L)  ('A1', 2L)  ('A1', 3L)  ('A2', 0L)  \\\nSample                                                               \n1              2.0         4.0         3.0         NaN         3.0   \n2             38.0        45.0         NaN         NaN        80.0   \n3            100.0        90.0       115.0        99.0       120.0   \n\n        ('A2', 1L)  ('A2', 2L)  ('A2', 3L)  \nSample                                      \n1              6.0         6.0         NaN  \n2             66.0         NaN         NaN  \n3            110.0       125.0       101.0  \n"
'def sigmoid(z):\n  return 1 / (1 + np.exp(-z))\n\ndef sigmoid_derivative(x):\n  return sigmoid(x) * (1 - sigmoid(x))\n'
'result = tf.layers.dense(input=dropout, classes_num, tf.identity)\n\nresult = tf.layers.dense(input=dropout, classes_num,\n             tf.identity if is_training else tf.nn.softmax)\n'
"poly_grid.fit(X, y)\n\nRun fit with all sets of parameters.\n\npoly_grid = GridSearchCV(PolynomialRegression(), param_grid, \n                         cv=10, \n                         scoring='neg_mean_squared_error', \n                         verbose=3) \n"
'predictions = clf.predict_proba(test)\nk = 5\ntop_k = np.argsort(probs, axis=1)[-k:]\ntop_k_preds = clf.classes_[top_k]\n'
'In [49]: x_arr = np.array(x, dtype=np.float32)\n\nIn [50]: x_arr\nOut[50]: \narray([[ 1.,  2.,  3.,  4.,  5.],\n       [ 0.,  2.,  3.,  4.,  5.]], dtype=float32)\n\n\n# compute (mean) cosine distance between `x[0]` &amp; `x[1]`\n# where `x[0]` can be considered as `labels`\n# while `x[1]` can be considered as `predictions`\nIn [51]: cosine_dist_axis0 = tf.metrics.mean_cosine_distance(x_arr[0], x_arr[1], 0)\n\nIn [52]: x_arr\nOut[52]: \narray([[ 1.,  2.,  3.,  4.,  5.],\n       [ 0.,  2.,  3.,  4.,  5.]], dtype=float32)\n\nIn [53]: np.sum(x_arr, axis=0)\nOut[53]: array([  1.,   4.,   6.,   8.,  10.], dtype=float32)\n\n# if your `label` is a column vector\nIn [66]: (x_arr[0])[:, None]\nOut[66]: \narray([[ 1.],\n       [ 2.],\n       [ 3.],\n       [ 4.],\n       [ 5.]], dtype=float32)\n\n# if your `prediction` is a column vector\nIn [67]: (x_arr[1])[:, None]\nOut[67]: \narray([[ 0.],\n       [ 2.],\n       [ 3.],\n       [ 4.],\n       [ 5.]], dtype=float32)\n\n# inputs\nIn [68]: labels = (x_arr[0])[:, None]\nIn [69]: predictions = (x_arr[1])[:, None]\n\n# compute mean cosine distance between them\nIn [70]: cosine_dist_dim1 = tf.metrics.mean_cosine_distance(labels, predictions, 1)\n\nIn [77]: x\nOut[77]: [[1, 2, 3, 4, 5], [0, 2, 3, 4, 5]]\n\nIn [78]: import scipy\n\nIn [79]: scipy.spatial.distance.cosine(x[0], x[1])\nOut[79]: 0.009132\n'
"df = pd.get_dummies(df, columns=['PatientSerial', 'MachineID'], drop_first=True)\n\nnp.random.seed(444)\nv = np.random.choice([0, 1, 2], size=(2, 10))\ndf = pd.DataFrame({'other_col': np.empty_like(v[0]),\n                   'PatientSerial': v[0],\n                   'MachineID': v[1]})\n\npd.get_dummies(df, columns=['PatientSerial', 'MachineID'],\n               drop_first=True, prefix=['Serial', 'MachineID'])\n\n   other_col  Serial_1  Serial_2  MachineID_1  MachineID_2\n0          2         0         0            0            1\n1          1         0         0            0            1\n2          2         0         0            0            0\n3          2         1         0            1            0\n4          2         0         1            0            0\n5          2         1         0            0            1\n6          2         0         1            0            0\n7          2         1         0            0            1\n8          2         1         0            0            0\n9          2         1         0            0            1\n"
'Layer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, None, 6)           192       \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 6)                 312       \n=================================================================\n'
'    [ 9.3402149e-04],\n    [ 5.8139337e-04],\n    [-9.9929601e-01],\n    [ 1.0009530e+00]\n\n    [0, 0, -1, 1] \n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense\nfrom keras.optimizers import Adadelta, Adam\nimport keras.backend as K\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt( K.mean( K.square( y_pred - y_true ) ) )\n\nX_train = np.random.random(240000*4)\nX_train = np.reshape( X_train, ( 240000, 1, 4 ) )\n\ny_train = X_train[:,0,3] - X_train[:,0,2]\n\ninputShape = ( X_train.shape[1], X_train.shape[2] )\n\n# create model\nmodel = Sequential()\nmodel.add( Flatten( input_shape=inputShape  ) )\nmodel.add( Dense( 1 ) )\n\nmodel.compile( loss=root_mean_squared_error, optimizer=Adam( lr=0.01 ) )\n\n# train model\nbatchSize = 8\n\nmodel.fit( X_train, y_train, nb_epoch=1, batch_size=batchSize, shuffle=True )\n\ny_train_predicted = model.predict( X_train)\ny_train_predicted = np.asarray(y_train_predicted).ravel()\n\ny_train_predicted_rmse = np.sqrt( np.mean( np.square( y_train_predicted - y_train ) ) )\n\nprint( "y_train RMSE = " + str( y_train_predicted_rmse ) )\n\n\nx = [model.layers]\nx[0][1].get_weights()\n'
'import urllib2\nfrom bs4 import BeautifulSoup\n\nopener = urllib2.build_opener()\nopener.addheaders = [(\'User-Agent\', \'Mozilla/5.0\')]\npage = opener.open(\'https://ethereumprice.org/\')\nsoup = BeautifulSoup(page, "lxml")\ndiv = soup.find(\'span\', id=\'ep-price\')\nethereum_rate = div.contents[0]\n\nprint ethereum_rate\n\nimport feedparser\n\npython_wiki_rss_url = "http://www.python.org/cgi-bin/moinmoin/" \\\n                       "RecentChanges?action=rss_rc"\n\nfeed = feedparser.parse( python_wiki_rss_url )\n\nprint feed\n'
'loss_d_real = -tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator(real_data),labels= tf.ones_like(discriminator(real_data))))\n\nloss_d_fake=-tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator(noise_input),labels= tf.zeros_like(discriminator(real_data))))\n\nloss_g= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=discriminator(genereted_samples), labels=tf.ones_like(genereted_samples)))\n'
'oneToFive=input("Enter 5 numbers separated by commas")\noneToFive=oneToFive.split(",")\noneToFive=[np.float32(c) for c in oneToFive]\nmodel.predict(x=np.array(oneToFive))\n'
'model.add(GRU(units = self.neurons, dropout = self.dropval,  bias_initializer = bias))\nmodel.add(GRU(units = self.neurons, dropout = self.dropval,  bias_initializer = bias))\nmodel.add(GRU(units = self.neurons, dropout = self.dropval,  bias_initializer = bias))\n\nmodel.add(GRU(units = self.neurons, dropout = self.dropval,  bias_initializer = bias, return_sequences=True))\nmodel.add(GRU(units = self.neurons, dropout = self.dropval,  bias_initializer = bias, return_sequences=True))\nmodel.add(GRU(units = self.neurons, dropout = self.dropval,  bias_initializer = bias))\n'
'from sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n\n# example data\niris = datasets.load_iris()\nX, y = iris.data, iris.target \nclf = svm.SVC(probability=True, random_state=0)\n\ndef get_preds(clf, X, y): # y is required for a scorer but we won\'t use it\n    with open("pred.csv", "ab+") as f: # append each fold to file\n        np.savetxt(f, clf.predict(X))\n    return 0\n\nscoring = {\'preds\': get_preds,\n           \'accuracy\': \'accuracy\',\n           \'recall\': \'recall_macro\'} # add desired scorers here\n\nk = 5\ncross_validate(clf, X, y, \n               scoring=scoring, \n               return_train_score=True,\n               cv = k)\n\npreds = np.loadtxt("pred.csv").reshape(k, len(X))\nmy_preds = np.mean(my_preds, axis=0).round()\n\ncv_preds = cross_val_predict(clf, X, y, cv=k)\n\nnp.equal(my_preds, cv_preds).sum() # 487 out of 500\n'
'indx = devData[classTrainFeatures].index[devData[classTrainFeatures].apply(np.isnan)]\ndevData=devData.drop(devData.index[indx]).copy()\ndevData=devData.reset_index(drop=True)\n'
"from scipy.spatial import distance\n\ndf.set_index('category', inplace = True)\n\n&gt;&gt; df.apply(lambda x: distance.cityblock(x, df.loc['apple',:]), axis=1\n        ).drop('apple', axis=1).nsmallest(4).index.values.tolist()\n\n ['strawberry', 'berry', 'kiwi', 'orange']\n"
"X_train = X_train.reshape(-1, 128*2)\n\ninp = Input(shape=(128*2,))\nx = Dense(128, activation='relu')(inp)\nx = Dense(1, activation='sigmoid'))(x)\nmodel=Model(inputs=[inp], outputs=[x])\n\n# assuming X_train have a shape of `(2000, 2, 128)` as you suggested\nmodel.fit([X_train[:,0], X_train[:,1]], y_train, ...)\n"
'poly_svm_search = SVC(kernel="poly", degree="2")\n\npoly_svm_search = SVC(kernel="poly", degree=2)\n'
'reg = MultiOutputRegressor(RandomForestRegressor()) \n\nreg.fit(X_train, y_train)\n'
"from keras.models import Model\nmodel2= Model(model.input,model.get_layer('dense_5').output)\n\nfrom keras.models import Model\nmodel2= Model(model.input,model.layers[4].output)\n\npreds=model2.predict(x)\n"
"import numpy as np\nfrom sklearn import svm\n\nx=np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10],[11]], dtype=np.float64)\ny=np.array([2,3,4,5,6,7,8,9,10,11,12], dtype=np.float64)\n\nclf = svm.SVR(kernel='linear')\nclf.fit(x, y)\nprint(clf.predict([[50]]))\nprint(clf.score(x, y))\n\n[50.12]\n0.9996\n"
'...\nwith open("coords.txt","w+") as file:\n    for idx in range(len(contours)):\n        x, y, w, h = cv2.boundingRect(contours[idx])\n        mask[y:y+h, x:x+w] = 0\n        file.write("Box {0}: ({1},{2}), ({3},{4}), ({5},{6}), ({7},{8})".format(idx,x,y,x+w,y,x+w,y+h,x,y+h))\n        cv2.drawContours(mask, contours, idx, (255, 255, 255), -1)\n        r = float(cv2.countNonZero(mask[y:y+h, x:x+w])) / (w * h)\n...\n\nBox 0: (360,259), (364,259), (364,261), (360,261)\nBox 1: (380,258), (385,258), (385,262), (380,262)\nBox 2: (365,258), (370,258), (370,262), (365,262)\nBox 3: (386,256), (393,256), (393,260), (386,260)\nBox 4: (358,256), (361,256), (361,258), (358,258)\n'
"class DefaultEstimator:\n    def set_params(self, **kwargs):\n        for k, v in kwargs.items():\n            parts = k.split('__')\n            if parts[0].startswith('pipeline'):\n                pipe_num = int(parts[0].split('_')[1])\n                param_name = '__'.join(parts[1:])\n                self.pipelines[pipe_num].set_params(*{param_name: v})\n            else:\n                # other logic\n"
'X_test_scaled  = scaler.fit_transform(X_test)\n\nX_train = df.values\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Save scaler\nimport pickle as pkl\nwith open("scaler.pkl", "wb") as outfile:\n    pkl.dump(scaler, outfile)\n\n# Some other code for training your autoencoder\n# ...\n\n# During test time\n# Load scaler that was fitted on training data\nwith open("scaler.pkl", "rb") as infile:\n    scaler = pkl.load(infile)\n    X_test_scaled = scaler.transform(X_test)  # Note: not fit_transform.\n'
'# Set up environment\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics, model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport pandas as pd\nimport numpy as np\n\n#Read in data and do just a bit of preprocessing\n\n# User\'s Location of git repository\nGit_Location = \'C:/Documents\'\n\n# Set Data Location:\ndata = Git_Location + \'Data.csv\'\n\n# load the data\ndf = pd.read_csv(data,low_memory=False,thousands=\',\', encoding=\'latin-1\')\ndf = df[[\'CODE\',\'Description\']] #select only these columns\ndf = df.rename(index=float, columns={"CODE": "label", "Description": "text"})\n\n#Convert label to float so you don\'t need to encode for processing later on\ndf[\'label\']=df[\'label\'].str.replace(\'-\', \'\',regex=True, case = False).str.strip()\ndf[\'label\'].astype(\'float64\', raise_on_error = True)\n\n# drop any labels with count LT 500 to build a strong model and make our testing run faster -- we will get more data later\ndf = df.groupby(\'label\').filter(lambda x : len(x)&gt;500)\n\n#split data into testing and training\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(df.text, df.label,test_size=0.33, random_state=6,stratify=df.label)\n\n#reset the index \nvalid_y = valid_y.reset_index(drop=True)\nvalid_x = valid_x.reset_index(drop=True)\n\n# cast validation datasets to dataframes to allow to merging later on\nvalid_x_df = pd.DataFrame(valid_x)\nvalid_y_df = pd.DataFrame(valid_y)\n\n\n# Extracting features from data\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(train_x_list)\nX_test_counts = count_vect.transform(valid_x_list)\n\n# Define the model training and validation function\ndef TV_model(classifier, feature_vector_train, label, feature_vector_valid, valid_y, valid_x, is_neural_net=False):\n\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n\n    # predict the top n labels on validation dataset\n    n = 5\n    #classifier.probability = True\n    probas = classifier.predict_proba(feature_vector_valid)\n    predictions = classifier.predict(feature_vector_valid)\n\n    #Identify the indexes of the top predictions\n    top_n_predictions = np.argsort(probas, axis = 1)[:,-n:]\n\n    #then find the associated SOC code for each prediction\n    top_class = classifier.classes_[top_n_predictions]\n\n    #cast to a new dataframe\n    top_class_df = pd.DataFrame(data=top_class)\n\n    #merge it up with the validation labels and descriptions\n    results = pd.merge(valid_y, valid_x, left_index=True, right_index=True)\n    results = pd.merge(results, top_class_df, left_index=True, right_index=True)\n\n\n    top5_conditions = [\n        (results.iloc[:,0] == results[0]),\n        (results.iloc[:,0] == results[1]),\n        (results.iloc[:,0] == results[2]),\n        (results.iloc[:,0] == results[3]),\n        (results.iloc[:,0] == results[4])]\n    top5_choices = [1, 1, 1, 1, 1]\n\n    #Top 1 Result\n    #top1_conditions = [(results[\'0_x\'] == results[4])]\n    top1_conditions = [(results.iloc[:,0] == results[4])]\n    top1_choices = [1]\n\n    # Create the success columns\n    results[\'Top 5 Successes\'] = np.select(top5_conditions, top5_choices, default=0)\n    results[\'Top 1 Successes\'] = np.select(top1_conditions, top1_choices, default=0)\n\n    print("Are Top 5 Results greater than Top 1 Result?: ", (sum(results[\'Top 5 Successes\'])/results.shape[0])&gt;(metrics.accuracy_score(valid_y, predictions)))\n   print("Are Top 1 Results equal from predict() and predict_proba()?: ", (sum(results[\'Top 1 Successes\'])/results.shape[0])==(metrics.accuracy_score(valid_y, predictions)))\n\n    print(" ")\n    print("Details: ")\n    print("Top 5 Accuracy Rate (predict_proba)= ", sum(results[\'Top 5 Successes\'])/results.shape[0])\n    print("Top 1 Accuracy Rate (predict_proba)= ", sum(results[\'Top 1 Successes\'])/results.shape[0])\n    print("Top 1 Accuracy Rate = (predict)=", metrics.accuracy_score(valid_y, predictions)) \n'
'costL =Gini(16,9,0) = 0.4608\ncostR =Gini(3,12,40) = 0.4205\n\ncostx1&lt;2.0623 = 25/80 costL + 55/80 costR = 0.4331\n\ncostx1&lt;1 = FractionL Gini(8,4,0) + FractionR Gini(11,17,40) = 12/80 * 0.4444 + 68/80 * 0.5653 = 0.5417\n'
'b, m = gradient_descent(a, b, 5e-25, 100)\nprint(b, m)\nOut: -3.7387067636195266e-13 0.13854551291084335\n'
'def my_deployable_function():\n\n   import subprocess\n   subprocess.check_output( "pip install ipython--user",stderr=subprocess.STDOUT,shell=True )\n\n    def score( payload ):\n\n       num1=int(payload["values"][0])\n       num2=int(payload["values"][1])  \n       ans=num1+num2\n\n    return ans\n\nfunction_result = my_deployable_function()( { "values" : [ 100,200] } )\nprint( function_result )\n\n300\n'
"def loss(y_true, y_pred):\n    a = K.equal(y_true[:, 5], 0)\n    b = K.greater(y_pred[:, 5], 0.5)\n    condition = K.cast(a, 'float') * K.cast(b, 'float')\n    wt = 10 * condition + (1 - condition)\n    return K.mean(wt[:, None] * K.binary_crossentropy(y_true, y_pred), axis=-1)\n"
"fig, ax = plt.subplots(3,1,figsize=(14,30))\n\nnfeats = 15\nimportance_types = ['weight', 'cover', 'gain']\n\nfor i, imp_i in enumerate(importance_types):\n    plot_importance(xgboost_model, ax=ax[i], max_num_features=nfeats\n                    , importance_type=imp_i\n                    , xlabel=imp_i)\n"
'from sklearn import tree\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\n\n\ndef process_df_for_ml(df):\n    """\n    Process a dataframe for model training/prediction use.\n\n    Returns X/y tensors.\n    """\n\n    df = df.copy()\n    # Map salary to 0,1,2\n    df.salary = df.salary.map({"low": 0, "medium": 1, "high": 2})\n    # dropping left and sales X for the df, y for the left\n    X = df.drop(["left", "sales"], axis=1)\n    y = df["left"]\n    return (X, y)\n\n# Read and reindex CSV.\ndf = pd.read_csv("HR_comma_sep.csv")\ndf = df.reindex()\n\n# Train a decision tree.\nX, y = process_df_for_ml(df)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)\nclftree = tree.DecisionTreeClassifier(max_depth=3)\nclftree.fit(X_train, y_train)\n\n# Test the decision tree on people who haven\'t left yet.\nnotleftdf = df[df["left"] == 0].copy()\nX, y = process_df_for_ml(notleftdf)\n# Plug in a new column with ones and zeroes from the prediction.\nnotleftdf["will_leave"] = clftree.predict(X)\n# Print those with the will-leave flag on.\nprint(notleftdf[notleftdf["will_leave"] == 1])\n'
'def kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, N=4, bsize=256, name=\'kappa\'):\n"""A continuous differentiable approximation of discrete kappa loss.\n    Args:\n        y_pred: 2D tensor or array, [batch_size, num_classes]\n        y_true: 2D tensor or array,[batch_size, num_classes]\n        y_pow: int,  e.g. y_pow=2\n        N: typically num_classes of the model\n        bsize: batch_size of the training or validation ops\n        eps: a float, prevents divide by zero\n        name: Optional scope/name for op_scope.\n    Returns:\n        A tensor with the kappa loss."""\n\nwith tf.name_scope(name):\n    y_true = tf.cast(y_true,dtype=\'float\')\n    repeat_op = tf.cast(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]), dtype=\'float\')\n    repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n    weights = repeat_op_sq / tf.cast((N - 1) ** 2, dtype=\'float\')\n\n    pred_ = y_pred ** y_pow\n    try:\n        pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n    except Exception:\n        pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n\n    hist_rater_a = tf.reduce_sum(pred_norm, 0)\n    hist_rater_b = tf.reduce_sum(y_true, 0)\n\n    conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n\n    nom = tf.reduce_sum(weights * conf_mat)\n    denom = tf.reduce_sum(weights * tf.matmul(\n        tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n                          tf.cast(bsize, dtype=\'float\'))\n\n    return nom / (denom + eps)\n\n lossMetric = kappa_loss\n model.compile(optimizer=optimizer, loss=lossMetric, metrics=metricsToWatch)\n\ntf.cast(nn_x_train.values, dtype=\'float\')\n\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e / a1.shape[0]\n\n    return sum(1 - o / e)/len(1 - o / e)\n\nnn_y_valid=tf.cast(nn_y_train.values, dtype=\'float\')\nprint(qwk3(nn_y_valid, trainPredict))\n'
'import math\n\ndata_set = [\n    (2,9,8,4, "Good"),\n    (3,7,7,9, "Bad"),\n    (10,3,10,3, "Good"),\n    (2,9,6,10, "Good"),\n    (3,3,2,5, "Bad"),\n    (2,8,5,6, "Bad"),\n    (7,2,3,10, "Good"),\n    (1,10,8,10, "Bad"),\n    (2,8,1,10, "Good"),\n]\n\nA = (3,2,1,5)\nB = (8,3,1,2)\nC = (6,10,8,3)\nD = (9,6,4,1)\n\ndef calc_distance(datas, test):\n    distances = []\n    for data in datas:\n        distances.append(\n            ( round(math.sqrt(((data[0] - test[0])**2 + (data[1] - test[1])**2 + (data[2] - test[2])**2 + (data[3] - test[3])**2)), 3), data[4] ))\n    return distances\n\ndef most_frequent(list1):\n    return max(set(list1), key = list1.count)\n\ndef get_neibours(distances, k):\n    labels = []\n    distances.sort()\n    print(distances[:k])\n    for distance in distances[:k]:\n        labels.append(distance[1])\n    print("It can be classified as: ", end="")\n    print(most_frequent(labels))\n\ndistances = calc_distance(data_set,D)\nget_neibours(distances, 7)\n\ndistances = calc_distance(data_set,D)\nget_neibours(distances, 7) \n'
'Model: "sequential_9"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, 295, 50)           5000      \n_________________________________________________________________\ndense_12 (Dense)             (None, 295, 32)           1632      \n_________________________________________________________________\ndense_13 (Dense)             (None, 295, 3)            99        \n=================================================================\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim, output_dim, input_length=max_sequence_length, input_shape=(295,)))\n#model.add(lambda x: tf.reduce_mean(x, axis=1))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation=\'relu\'))\nmodel.add(Dense(3, activation=\'softmax\'))\nmodel.summary()\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim, output_dim, input_length=max_sequence_length, input_shape=(None,)))\nmodel.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation=\'relu\'))\nmodel.add(Dense(3, activation=\'softmax\'))\nmodel.summary()\n'
"import numpy as np\nfrom sklearn.svm import SVC\n\nX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\ny = np.array([1, 1, 2, 2])\n\nclf = SVC(gamma='scale')\nclf.fit(X, y)\n\nn_features = X.shape[1]\ngamma = 1 / (n_features * X.var())\n\nclf._gamma\n\nOut[24]: \narray([[-1, -1],\n       [-2, -1],\n       [ 1,  1],\n       [ 2,  1]])\n\nn_features\nOut[25]: 2\n\nX.var()\nOut[26]: 1.75\n\ngamma\nOut[27]: 0.2857142857142857\n\nclf._gamma\nOut[28]: 0.2857142857142857\n"
'from sklearn.cross_validation import train_test_split\n\nfrom sklearn.model_selection import train_test_split\n'
'%tensorflow_version 1.x\nimport tensorflow as tf\n\ng = tf.Graph()\nwith g.as_default():\n  x = tf.constant(1.0)  # x is created in graph g\n\nwith tf.Session().as_default() as sess:\n  y = tf.constant(2.0) # y is created in TensorFlow\'s default graph!!!\n  print(y.eval(session=sess)) # y was created in TF\'s default graph, and is evaluated in\n                  # default session, so everything is ok.  \n  print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n                  # which is tied to graph g, but it is evaluated in\n                  # session s which is tied to graph g =&gt; ERROR\n\n2.0\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-5-f35cb204cf59&gt; in &lt;module&gt;()\n     10   print(y.eval(session=sess)) # y was created in TF\'s default graph, and is evaluated in\n     11                   # default session, so everything is ok.\n---&gt; 12   print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n     13                   # which is tied to graph g, but it is evaluated in\n     14                   # session s which is tied to graph g =&gt; ERROR\n\n1 frames\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py in _eval_using_default_session(tensors, feed_dict, graph, session)\n   5402   else:\n   5403     if session.graph is not graph:\n-&gt; 5404       raise ValueError("Cannot use the given session to evaluate tensor: "\n   5405                        "the tensor\'s graph is different from the session\'s "\n   5406                        "graph.")\n\nValueError: Cannot use the given session to evaluate tensor: the tensor\'s graph is different from the session\'s graph.\n\n%tensorflow_version 1.x\nimport tensorflow as tf\n\ng = tf.Graph()\nwith g.as_default():\n  x = tf.constant(1.0)  # x is created in graph g\n\nwith tf.Session(graph=g).as_default() as sess:\n  print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n                         # which is tied to graph g, so everything is ok.\n  y = tf.constant(2.0) # y is created in TensorFlow\'s default graph!!!\n  print(y.eval()) # y was created in TF\'s default graph, but it is evaluated in\n                  # session s which is tied to graph g =&gt; ERROR\n\n1.0\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-15-6b8b687c5178&gt; in &lt;module&gt;()\n     10                          # which is tied to graph g, so everything is ok.\n     11   y = tf.constant(2.0) # y is created in TensorFlow\'s default graph!!!\n---&gt; 12   print(y.eval()) # y was created in TF\'s default graph, but it is evaluated in\n     13                   # session s which is tied to graph g =&gt; ERROR\n\n1 frames\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py in _eval_using_default_session(tensors, feed_dict, graph, session)\n   5396                        "`eval(session=sess)`")\n   5397     if session.graph is not graph:\n-&gt; 5398       raise ValueError("Cannot use the default session to evaluate tensor: "\n   5399                        "the tensor\'s graph is different from the session\'s "\n   5400                        "graph. Pass an explicit session to "\n\nValueError: Cannot use the default session to evaluate tensor: the tensor\'s graph is different from the session\'s graph. Pass an explicit session to `eval(session=sess)`.\n\n%tensorflow_version 1.x\nimport tensorflow as tf\n\ng = tf.Graph()\nwith g.as_default():\n  x = tf.constant(1.0)  # x is created in graph g\n\nwith tf.Session(graph=g).as_default() as sess:\n  print(x.eval(session=sess)) # x was created in graph g and it is evaluated in session s\n                         # which is tied to graph g, so everything is ok.\n  y = tf.constant(2.0) # y is created in TensorFlow\'s default graph!!!\n  print(y.eval(session=sess)) # y was created in TF\'s default graph, but it is evaluated in\n                  # session s which is tied to graph g =&gt; ERROR\n\n1.0\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-16-83809aa4e485&gt; in &lt;module&gt;()\n     10                          # which is tied to graph g, so everything is ok.\n     11   y = tf.constant(2.0) # y is created in TensorFlow\'s default graph!!!\n---&gt; 12   print(y.eval(session=sess)) # y was created in TF\'s default graph, but it is evaluated in\n     13                   # session s which is tied to graph g =&gt; ERROR\n\n1 frames\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/ops.py in _eval_using_default_session(tensors, feed_dict, graph, session)\n   5402   else:\n   5403     if session.graph is not graph:\n-&gt; 5404       raise ValueError("Cannot use the given session to evaluate tensor: "\n   5405                        "the tensor\'s graph is different from the session\'s "\n   5406                        "graph.")\n\nValueError: Cannot use the given session to evaluate tensor: the tensor\'s graph is different from the session\'s graph.\n\n%tensorflow_version 1.x\nimport tensorflow as tf\n\nx = tf.constant(1.0)  # x is in not assigned to any graph\n\nwith tf.Session().as_default() as sess:\n  y = tf.constant(2.0) # y is created in TensorFlow\'s default graph!!!\n  print(y.eval(session=sess)) # y was created in TF\'s default graph, and is evaluated in\n                  # default session, so everything is ok.  \n  print(x.eval(session=sess)) # x not assigned to any graph, and is evaluated in\n                  # default session, so everything is ok.  \n\n2.0\n1.0\n\nimport tensorflow as tf\n\ng = tf.Graph()\nwith g.as_default():\n  x = tf.constant(1.0)  # x is created in graph g\n  y = tf.constant(2.0) # y is created in graph g\n\nwith tf.Session(graph=g).as_default() as sess:\n  print(x.eval()) # x was created in graph g and it is evaluated in session s\n                         # which is tied to graph g, so everything is ok.\n  print(y.eval()) # y was created in graph g and it is evaluated in session s\n                         # which is tied to graph g, so everything is ok.\n\n1.0\n2.0\n'
"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\nX = np.random.randint(0, 256, (1945, 1800)) # fake data\ny = np.random.randint(0, 38, 1945)\n\nmodel = Sequential([\n            Dense(128, activation='relu', input_shape=(1800,)),\n            Dense(39)])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n              from_logits=True), metrics=['accuracy'])\n\nhist = model.fit(X, y, epochs=10)\n"
"results = lgb.cv({'num_threads':1}, lgb.Dataset(X_train, y_train), folds=folds,metrics=['rmse'])\n"
'full_pipeline_with_predictor = Pipeline([\n        ("preparation", full_pipeline),\n        ("linear", LinearRegression())\n    ])\n\nfinal_predictions = full_pipeline_with_predictor.predict(X_test)\n\nfull_pipeline_with_predictor.predict(some_data)\n'
"image = cv2.imread('my_image.jpg')\nheight, width = image.shape[:2]\nimage = torch.as_tensor(image.astype(&quot;float32&quot;).transpose(2, 0, 1))\ninputs = [{&quot;image&quot;: image, &quot;height&quot;: height, &quot;width&quot;: width}]\nwith torch.no_grad():\n    images = model.preprocess_image(inputs)  # don't forget to preprocess\n    features = model.backbone(images.tensor)  # set of cnn features\n    proposals, _ = model.proposal_generator(images, features, None)  # RPN\n\n    features_ = [features[f] for f in model.roi_heads.box_in_features]\n    box_features = model.roi_heads.box_pooler(features_, [x.proposal_boxes for x in proposals])\n    box_features = model.roi_heads.box_head(box_features)  # features of all 1k candidates\n    predictions = model.roi_heads.box_predictor(box_features)\n    pred_instances, pred_inds = model.roi_heads.box_predictor.inference(predictions, proposals)\n    pred_instances = model.roi_heads.forward_with_given_boxes(features, pred_instances)\n\n    # output boxes, masks, scores, etc\n    pred_instances = model._postprocess(pred_instances, inputs, images.image_sizes)  # scale box to orig size\n    # features of the proposed boxes\n    feats = box_features[pred_inds]\n"
'CV.best_estimator_.get_params()\n'
"import tensorflow as tf\nimport numpy as np\n\n(xtrain, train_target), (xtest, test_target) = tf.keras.datasets.mnist.load_data()\n\n# 10 categories, one for each digit\nytrain1 = tf.keras.utils.to_categorical(train_target, num_classes=10)\nytest1 = tf.keras.utils.to_categorical(test_target, num_classes=10)\n\n# 2 categories, if the digit is odd or not\nytrain2 = tf.keras.utils.to_categorical((train_target % 2 == 0).astype(int), \n                                        num_classes=2)\nytest2 = tf.keras.utils.to_categorical((test_target % 2 == 0).astype(int), \n                                       num_classes=2)\n\n# 4 categories, based on the interval of the digit\nytrain3 = tf.keras.utils.to_categorical(np.digitize(train_target, [3, 6, 8]), \n                                        num_classes=4)\nytest3 = tf.keras.utils.to_categorical(np.digitize(test_target, [3, 6, 8]), \n                                       num_classes=4)\n\n# Regression, the square of the digit\nytrain4 = tf.square(tf.cast(train_target, tf.float32))\nytest4 = tf.square(tf.cast(test_target, tf.float32))\n\n# train dataset\ntrain_ds = tf.data.Dataset. \\\n    from_tensor_slices((xtrain, ytrain1, ytrain2, ytrain3, ytrain4)). \\\n    shuffle(32). \\\n    batch(32).map(lambda a, *rest: (tf.divide(a[..., None], 255), rest)). \\\n    prefetch(tf.data.experimental.AUTOTUNE)\n\n# test dataset\ntest_ds = tf.data.Dataset. \\\n    from_tensor_slices((xtest, ytest1, ytest2, ytest3, ytest4)). \\\n    shuffle(32). \\\n    batch(32).map(lambda a, *rest: (tf.divide(a[..., None], 255), rest)). \\\n    prefetch(tf.data.experimental.AUTOTUNE)\n\n\n# architecture\nclass Net(tf.keras.Model):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3),\n                                            strides=(1, 1), input_shape=(28, 28, 1),\n                                            activation='relu')\n        self.maxp1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n        self.conv2 = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3),\n                                            strides=(1, 1),\n                                            activation='relu')\n        self.maxp2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n        self.conv3 = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3),\n                                            strides=(1, 1),\n                                            activation='relu')\n        self.maxp3 = tf.keras.layers.MaxPool2D(pool_size=(2, 2))\n        self.gap = tf.keras.layers.Flatten()\n        self.dense = tf.keras.layers.Dense(64, activation='relu')\n        self.output1 = tf.keras.layers.Dense(10, activation='softmax')\n        self.output2 = tf.keras.layers.Dense(2, activation='softmax')\n        self.output3 = tf.keras.layers.Dense(4, activation='softmax')\n        self.output4 = tf.keras.layers.Dense(1, activation='linear')\n\n    def call(self, inputs, training=False, **kwargs):\n        x = self.conv1(inputs)\n        x = self.maxp1(x)\n        x = self.conv2(x)\n        x = self.maxp2(x)\n        x = self.conv3(x)\n        x = self.maxp3(x)\n        x = self.gap(x)\n        x = self.dense(x)\n        out1 = self.output1(x)\n        out2 = self.output2(x)\n        out3 = self.output3(x)\n        out4 = self.output4(x)\n        return out1, out2, out3, out4\n\n\nmodel = Net()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n# the three losses\nloss_1 = tf.losses.CategoricalCrossentropy()\nloss_2 = tf.losses.CategoricalCrossentropy()\nloss_3 = tf.losses.CategoricalCrossentropy()\nloss_4 = tf.losses.MeanAbsoluteError()\n\n# mean object that keeps track of the train losses\nloss_1_train = tf.metrics.Mean(name='tr_loss_1')\nloss_2_train = tf.metrics.Mean(name='tr_loss_2')\nloss_3_train = tf.metrics.Mean(name='tr_loss_3')\nloss_4_train = tf.metrics.Mean(name='tr_loss_4')\n\n# mean object that keeps track of the test losses\nloss_1_test = tf.metrics.Mean(name='ts_loss_1')\nloss_2_test = tf.metrics.Mean(name='ts_loss_2')\nloss_3_test = tf.metrics.Mean(name='ts_loss_3')\nloss_4_test = tf.metrics.Mean(name='ts_loss_4')\n\n# accuracies for printout\nacc_1_train = tf.metrics.CategoricalAccuracy(name='tr_acc_1')\nacc_2_train = tf.metrics.CategoricalAccuracy(name='tr_acc_2')\nacc_3_train = tf.metrics.CategoricalAccuracy(name='tr_acc_3')\n\n# accuracies for printout\nacc_1_test = tf.metrics.CategoricalAccuracy(name='ts_acc_1')\nacc_2_test = tf.metrics.CategoricalAccuracy(name='ts_acc_2')\nacc_3_test = tf.metrics.CategoricalAccuracy(name='ts_acc_3')\n\n\n# custom training loop\n@tf.function\ndef train_step(x, y1, y2, y3, y4):\n    with tf.GradientTape(persistent=True) as tape:\n        out1, out2, out3, out4 = model(x, training=True)\n        loss_1_value = loss_1(y1, out1)\n        loss_2_value = loss_2(y2, out2)\n        loss_3_value = loss_3(y3, out3)\n        loss_4_value = loss_4(y4, out4)\n\n    losses = [loss_1_value, loss_2_value, loss_3_value, loss_4_value]\n\n    # a list of losses is passed\n    grads = tape.gradient(losses, model.trainable_variables)\n\n    # gradients are applied\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    # losses are updated\n    loss_1_train(loss_1_value)\n    loss_2_train(loss_2_value)\n    loss_3_train(loss_3_value)\n    loss_4_train(loss_4_value)\n\n    # accuracies are updated\n    acc_1_train.update_state(y1, out1)\n    acc_2_train.update_state(y2, out2)\n    acc_3_train.update_state(y3, out3)\n\n\n@tf.function\ndef test_step(x, y1, y2, y3, y4):\n    out1, out2, out3, out4 = model(x, training=False)\n    loss_1_value = loss_1(y1, out1)\n    loss_2_value = loss_2(y2, out2)\n    loss_3_value = loss_3(y3, out3)\n    loss_4_value = loss_4(y4, out4)\n\n    loss_1_test(loss_1_value)\n    loss_2_test(loss_2_value)\n    loss_3_test(loss_3_value)\n    loss_4_test(loss_4_value)\n\n    acc_1_test.update_state(y1, out1)\n    acc_2_test.update_state(y2, out2)\n    acc_3_test.update_state(y3, out3)\n\n\nfor epoch in range(5):\n    # train step\n    for inputs, outputs1, outputs2, outputs3, outputs4 in train_ds:\n        train_step(inputs, outputs1, outputs2, outputs3, outputs4)\n\n    # test step\n    for inputs, outputs1, outputs2, outputs3, outputs4 in test_ds:\n        test_step(inputs, outputs1, outputs2, outputs3, outputs4)\n\n    metrics = [acc_1_train, acc_1_test,\n               acc_2_train, acc_2_test,\n               acc_3_train, acc_3_test,\n               loss_4_train, loss_4_test]\n\n    # printing metrics\n    for metric in metrics:\n        print(f'{metric.name}:{metric.result():=6.4f}', end=' ')   \n    print()\n\n    # resetting the states of the metrics\n    loss_1_train.reset_states()\n    loss_2_train.reset_states()\n    loss_3_train.reset_states()\n\n    loss_1_test.reset_states()\n    loss_2_test.reset_states()\n    loss_3_test.reset_states()\n\n    acc_1_train.reset_states()\n    acc_2_train.reset_states()\n    acc_3_train.reset_states()\n\n    acc_1_test.reset_states()\n    acc_2_test.reset_states()\n    acc_3_test.reset_states()\n\nts_acc_1:0.9495 ts_acc_2:0.9685 ts_acc_3:0.9589 ts_loss_4:5.5617 \nts_acc_1:0.9628 ts_acc_2:0.9747 ts_acc_3:0.9697 ts_loss_4:4.8953 \nts_acc_1:0.9697 ts_acc_2:0.9758 ts_acc_3:0.9733 ts_loss_4:4.5209 \nts_acc_1:0.9715 ts_acc_2:0.9796 ts_acc_3:0.9745 ts_loss_4:4.2175 \nts_acc_1:0.9742 ts_acc_2:0.9834 ts_acc_3:0.9775 ts_loss_4:3.9825\n\nfrom collections import deque\nimport numpy as np\n\nepochs = 100\nearly_stopping = 5\n\nloss_hist = deque(maxlen=early_stopping)\n\nfor epoch in range(epochs):\n    loss_value = np.random.rand()\n    loss_hist.append(loss_value)\n\n    print('Last 5 values: ', *np.round(loss_hist, 3))\n\n    if len(loss_hist) == early_stopping and loss_hist.popleft() &lt; min(loss_hist):\n        print('Early stopping. No loss decrease in %i epochs.\\n' % early_stopping)\n        break\n\nLast 5 values:  0.456\nLast 5 values:  0.456 0.153\nLast 5 values:  0.456 0.153 0.2\nLast 5 values:  0.456 0.153 0.2 0.433\nLast 5 values:  0.456 0.153 0.2 0.433 0.528\nLast 5 values:  0.153 0.2 0.433 0.528 0.349\nEarly stopping. No loss decrease in 5 epochs.\n"
'from sklearn.preprocessing import OneHotEncoder\n\ntrain_y = np.array([[0],[1],[1]]) # Input\n\nenc = OneHotEncoder()\nenc.fit(train_y)\nout = enc.transform(train_y).toarray()\n\nIn [314]: train_y\nOut[314]: \narray([[0],\n       [1],\n       [1]])\n\nIn [315]: out\nOut[315]: \narray([[ 1.,  0.],\n       [ 0.,  1.],\n       [ 0.,  1.]])\n\nIn [320]: train_y\nOut[320]: \narray([[9],\n       [4],\n       [1],\n       [6],\n       [2]])\n\nIn [321]: out\nOut[321]: \narray([[ 0.,  0.,  0.,  0.,  1.],\n       [ 0.,  0.,  1.,  0.,  0.],\n       [ 1.,  0.,  0.,  0.,  0.],\n       [ 0.,  0.,  0.,  1.,  0.],\n       [ 0.,  1.,  0.,  0.,  0.]])\n\ndef initialization_based(A): # A is Input array\n    a = np.unique(A, return_inverse=1)[1]\n    out = np.zeros((a.shape[0],a.max()+1),dtype=int)\n    out[np.arange(out.shape[0]), a.ravel()] = 1\n    return out\n\ndef broadcasting_based(A):  # A is Input array\n    a = np.unique(A, return_inverse=1)[1]\n    return (a.ravel()[:,None] == np.arange(a.max()+1)).astype(int)\n'
"#assuming you converted X_data correctly to numpy arrays and word vectors\nmodel.add(Embedding(embedding_vecor_length, top_words, input_length=X_data.shape[1]))\n\nfrom keras.utils import to_categorical\n\ny = to_categorical(y)\n\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
'Y = iris.target\nfor index, value in enumerate(Y):\n  Y[index] = value % 2\n'
'# Scaling prior to splitting\nscaler_x = MinMaxScaler(feature_range=(0.01, 0.99))\nscaler_y = MinMaxScaler(feature_range=(0.01, 0.99))\n\nscaled_x = scaler_x.fit_transform(df.loc[:, "X"].reshape([-1, 1]))\nscaled_y = scaler_y.fit_transform(df.loc[:, "Y"].reshape([-1, 1]))\n\nscaled_data = np.column_stack((scaled_x, scaled_y))\n\ny_pred = scaler_y.inverse_transform(y_pred)\n\ny_pred_reshaped = np.zeros((len(y_pred), 2))\ny_pred_reshaped[:,1] = y_pred\ny_pred = scaler.inverse_transform(y_pred_reshaped)[:,1]\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n\n\nnp.random.seed(7)\n\n# Load data\n#df = pd.read_csv(\'test32_C_data.csv\')\ndf = pd.DataFrame(np.random.randint(0,100, size=(100,3)), columns = [\'time\', \'X\', \'Y\'])\nn_features = 100\n\n\ndef create_sequences(data, window=15, step=1, prediction_distance=15):\n    x = []\n    y = []\n\n    for i in range(0, len(data) - window - prediction_distance, step):\n        x.append(data[i:i + window])\n        y.append(data[i + window + prediction_distance][1])\n\n    x, y = np.asarray(x), np.asarray(y)\n\n    return x, y\n\n\n# Scaling prior to splitting\nscaler_x = MinMaxScaler(feature_range=(0.01, 0.99))\nscaler_y = MinMaxScaler(feature_range=(0.01, 0.99))\n\nscaled_x = scaler_x.fit_transform(df.loc[:, "X"].reshape([-1,1]))\nscaled_y = scaler_y.fit_transform(df.loc[:, "Y"].reshape([-1,1]))\n\nscaled_data = np.column_stack((scaled_x, scaled_y))\n\n# Build sequences\nx_sequence, y_sequence = create_sequences(scaled_data)\n\ntest_len = int(len(x_sequence) * 0.90)\nvalid_len = int(len(x_sequence) * 0.90)\ntrain_end = len(x_sequence) - (test_len + valid_len)\nx_train, y_train = x_sequence[:train_end], y_sequence[:train_end]\nx_valid, y_valid = x_sequence[train_end:train_end + valid_len], y_sequence[train_end:train_end + valid_len]\nx_test, y_test = x_sequence[train_end + valid_len:], y_sequence[train_end + valid_len:]\n\n# Initialising the RNN\nmodel = Sequential()\n\n# Adding the input layerand the LSTM layer\nmodel.add(LSTM(15, input_shape=(15, 2)))\n\n# Adding the output layer\nmodel.add(Dense(1))\n\n# Compiling the RNN\nmodel.compile(loss=\'mse\', optimizer=\'rmsprop\')\n\n# Fitting the RNN to the Training set\nmodel.fit(x_train, y_train, epochs=5)\n\n# Getting the predicted values\ny_pred = model.predict(x_test)\ny_pred = scaler_y.inverse_transform(y_pred)\n'
'x_train, x_test, y_train, y_test = train_test_split(df_x.values,\n                                                    df_y.values,\n                                                    test_size=0.2,\n                                                    random_state=4\n                                                   )\n'
"def mse(y_true, y_pred, hidden):\n\n    error = y_true-y_pred\n    return K.mean(K.sqrt(error)) + K.mean(hidden)\n\n\nX = np.random.uniform(0,1, (1000,10))\ny = np.random.uniform(0,1, 1000)\n\ninp = Input((10,))\ntrue = Input((1,))\nx1 = Dense(32, activation='relu')(inp)\nx2 = Dense(16, activation='relu')(x1)\nout = Dense(1)(x2)\n\nm = Model([inp,true], out)\nm.add_loss( mse( true, out, x1 ) )\nm.compile(loss=None, optimizer='adam')\nm.summary()\n\nhistory = m.fit([X, y], y, epochs=10)\n\n## final fitted model to compute predictions\nfinal_m = Model(inp, out)\n"
'{\n    "John": 1,\n    "likes": 2,\n    "to": 3,\n    "watch": 4,\n    "movies": 5,\n    "also": 6,\n    "football": 7,\n    "games": 8,\n    "Mary": 9,\n    "too": 10\n}\n\n[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]\n[1, 1, 1, 1, 0, 1, 1, 1, 0, 0]\n'
'scaled_array = (original_array - mean_of_array)/std_of_array\n'
'           I_day    I_night     I_snow     I_no_snow\nobs 1:         1          0          1             0\nobs 2:         0          1          1             0\nobs 3:         1          0          0             1\nobs 4:         0          1          0             1\netc...\n\n           const    I_day     I_snow \nobs 1:         1        1          1\nobs 2:         1        0          1\nobs 3:         1        1          0\nobs 4:         1        0          0\netc...\n'
"scores = df.cross_validation.cross_val_score(estimator=estimator, cv=7, scoring='r2')\n"
'def tokenize(data, col_of_category):\n    str_to_int, int_to_str = {}, {}\n    for row in data:\n        cat = row[col_of_category]\n        if cat in str_to_int.keys(): token = str_to_int[cat]\n        else:\n            token = len(str_to_int.keys())\n            str_to_int[cat] = token\n            int_to_str[token] = cat\n        row[col_of_category] = token # assuming your rows are mutable\n    return str_to_int, int_to_str\n'
"self.y_pred = T.dot(input, self.W) + self.b[:, None]\n\n$ THEANO_FLAGS='exception_verbosity=high' python path/to/script.py\n\nDebugprint of the apply node:\nElemwise{Composite{((-i0) + i1)}}[(0, 1)] [@A] &lt;TensorType(float64, vector)&gt; ''\n |b [@B] &lt;TensorType(float64, vector)&gt;\n |CGemv{no_inplace} [@C] &lt;TensorType(float64, vector)&gt; ''\n   |&lt;TensorType(float64, vector)&gt; [@D] &lt;TensorType(float64, vector)&gt;\n   |TensorConstant{-1.0} [@E] &lt;TensorType(float64, scalar)&gt;\n   |&lt;TensorType(float64, matrix)&gt; [@F] &lt;TensorType(float64, matrix)&gt;\n   |W [@G] &lt;TensorType(float64, vector)&gt;\n   |TensorConstant{1.0} [@H] &lt;TensorType(float64, scalar)&gt;\n\nself.y_pred = T.dot(input, self.W) + self.b\n"
"for file in listing1:\n img = cv2.imread(path1 + file)\n res=cv2.resize(img,(250,250))\n gray_image = cv2.cvtColor(res, cv2.COLOR_BGR2GRAY)\n xarr=np.squeeze(np.array(gray_image).astype(np.float32))\n m,v=cv2.PCACompute(xarr)\n arr= np.array(v)\n flat_arr= arr.ravel()\n training_set.append(flat_arr)\n training_labels.append(1)\n\ntrainData=np.float32(training_set)\nresponses=np.float32(training_labels)\nsvm = cv2.SVM()\nsvm.train(trainData,responses, params=svm_params)\nsvm.save('svm_data.dat')\n"
"conv6 = tf.nn.bias_add(conv6, biases['bdc3'])\n\nfor j in xrange(3):\n"
'import numpy as np\nclasses = np.argmax(model.predict(x), axis = 1)\n\nimport numpy as np\nfrom keras.utils.np_utils import to_categorical\nclasses_one_hot = to_categorical(np.argmax(model.predict(x), axis = 1))\n'
'import numpy as np\nimport cplex\n\nnn = 5  # a small example size here\n\nXXt = np.random.rand(nn,nn) # the gramm matrix of the dataset\nyy = np.random.rand(nn)     # the label vector of the dataset\ntemp = ((yy*XXt).T)*yy\n\n# create symetric matrix\ntempu = np.triu(temp)     # upper triangle\niu1 = np.triu_indices(nn, 1)\ntempu.T[iu1] = tempu[iu1] # copy upper into lower\n\nind = np.array([[x for x in range(nn)] for x in range(nn)])\n\nqmat = []\nfor i in range(nn):\n    qmat.append([np.arange(nn), tempu[i]])\n\nc = cplex.Cplex()\nc.variables.add(lb=[0]*nn)\nc.objective.set_quadratic(qmat)\nc.write("test2.lp")\n'
'#!/usr/bin/env python\n\n"""Example for reading and writing tfrecords."""\n\nimport tensorflow as tf\nfrom PIL import Image\nimport numpy as np\nimport scipy.misc\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef write_images(filenames=[\'Aurelia-aurita-3.jpg\'],\n                 labels=[0],\n                 tf_records_filename="example.tfrecords"):\n    """\n    Write images to tfrecords file.\n\n    Parameters\n    ----------\n    filenames : list of strings\n        List containing the paths to image files.\n    labels : list of integers\n    tf_records_filename : string\n        Where the file gets stored\n    """\n    filename_queue = tf.train.string_input_producer(filenames)\n\n    reader = tf.WholeFileReader()\n    key, value = reader.read(filename_queue)\n\n    my_img = tf.image.decode_jpeg(value)\n\n    init_op = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init_op)\n\n        # Start populating the filename queue.\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n\n        writer = tf.python_io.TFRecordWriter(tf_records_filename)\n        for i in range(len(filenames)):\n            image = my_img.eval()  # image is an image tensor\n\n            image_raw = image.tostring()\n            rows = image.shape[0]\n            cols = image.shape[1]\n\n            if np.ndim(image) == 3:\n                depth = image.shape[2]\n            else:\n                depth = 1\n\n            example = tf.train.Example(features=tf.train.Features(feature={\n                \'height\': _int64_feature(rows),\n                \'width\': _int64_feature(cols),\n                \'depth\': _int64_feature(depth),\n                \'label\': _int64_feature(labels[i]),\n                \'image_raw\': _bytes_feature(image_raw),\n                \'src\': _bytes_feature(filenames[i])}))\n            writer.write(example.SerializeToString())\n        coord.request_stop()\n        coord.join(threads)\n\n\ndef read_and_decode(filename_queue):\n    """Read and decode them from filename_queue."""\n    reader = tf.TFRecordReader()\n    _, serialized_example = reader.read(filename_queue)\n    features = tf.parse_single_example(\n        serialized_example,\n        # Defaults are not specified since both keys are required.\n        features={\n            \'image_raw\': tf.FixedLenFeature([], tf.string),\n            \'label\': tf.FixedLenFeature([], tf.int64),\n            \'height\': tf.FixedLenFeature([], tf.int64),\n            \'width\': tf.FixedLenFeature([], tf.int64),\n            \'depth\': tf.FixedLenFeature([], tf.int64),\n            \'src\': tf.FixedLenFeature([], tf.string)\n        })\n    image = tf.decode_raw(features[\'image_raw\'], tf.uint8)\n    label = tf.cast(features[\'label\'], tf.int32)\n    height = tf.cast(features[\'height\'], tf.int32)\n    width = tf.cast(features[\'width\'], tf.int32)\n    depth = tf.cast(features[\'depth\'], tf.int32)\n    # fn = tf.cast(features[\'filename\'], tf.str)\n    return image, label, height, width, depth, features[\'src\']\n\n\ndef get_all_records(record_filename):\n    """Get all records from record_filename."""\n    records = []\n    with tf.Session() as sess:\n        fn_queue = tf.train.string_input_producer([record_filename])\n        image, label, height, width, depth, src = read_and_decode(fn_queue)\n        image = tf.reshape(image, tf.stack([height, width, 3]))\n        init_op = tf.global_variables_initializer()\n        sess.run(init_op)\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord)\n        nr_of_images = 1\n        for i in range(nr_of_images):\n            example, label, src = sess.run([image, label, src])\n            img = Image.fromarray(example, \'RGB\')\n            records.append({\'image\': img, \'label\': label,\n                            \'src\': src})\n        coord.request_stop()\n        coord.join(threads)\n    return records\n\nwrite_images()\nrecords = get_all_records(\'example.tfrecords\')\nprint(records[0][\'src\'])\nscipy.misc.imshow(records[0][\'image\'])\n'
'tf.multiply(y_pre_final, tf.expand_dims(W_pre_final,0))\n'
"demo_df = pd.read_csv('data.csv', header = None)\n"
"url='https://raw.githubusercontent.com/Aniruddh-SK/Loan-Prediction-Problem/master/train.csv'\n\ndf = pd.read_csv(url) #Reading the dataset in a dataframe using Pandas\n\n#df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)\n\ndf['Self_Employed'].fillna('No',inplace=True)\n\ntable = df.pivot_table(values='LoanAmount', \n                       index='Self_Employed', \n                       columns='Education', \n                       aggfunc=np.median)\n\nprint (table.unstack())\nEducation     Self_Employed\nGraduate      No               130.0\n              Yes              157.5\nNot Graduate  No               113.0\n              Yes              130.0\ndtype: float64\n\n#check all values with NaN in LoanAmount column\nprint (df.loc[df['LoanAmount'].isnull(), ['Self_Employed','Education', 'LoanAmount']])\n    Self_Employed     Education  LoanAmount\n0              No      Graduate         NaN\n35             No      Graduate         NaN\n63             No      Graduate         NaN\n81            Yes      Graduate         NaN\n95             No      Graduate         NaN\n102            No      Graduate         NaN\n103            No      Graduate         NaN\n113           Yes      Graduate         NaN\n127            No      Graduate         NaN\n202            No  Not Graduate         NaN\n284            No      Graduate         NaN\n305            No  Not Graduate         NaN\n322            No  Not Graduate         NaN\n338            No  Not Graduate         NaN\n387            No  Not Graduate         NaN\n435            No      Graduate         NaN\n437            No      Graduate         NaN\n479            No      Graduate         NaN\n524            No      Graduate         NaN\n550           Yes      Graduate         NaN\n551            No  Not Graduate         NaN\n605            No  Not Graduate         NaN\n\n#for check get all indexes where NaNs\nidx = df.loc[df['LoanAmount'].isnull(), ['Self_Employed','Education', 'LoanAmount']].index\nprint (idx)\nInt64Index([  0,  35,  63,  81,  95, 102, 103, 113, 127, 202, 284, 305, 322,\n            338, 387, 435, 437, 479, 524, 550, 551, 605],\n\n# Replace missing values\ndf = df.set_index(['Education','Self_Employed'])\ndf['LoanAmount'].fillna(table.unstack(), inplace=True)\ndf = df.reset_index()\n\n#check output - filter only indexes where NaNs before\nprint (df.loc[df.index.isin(idx), ['Self_Employed','Education', 'LoanAmount']])\n    Self_Employed     Education  LoanAmount\n0              No      Graduate       130.0\n35             No      Graduate       130.0\n63             No      Graduate       130.0\n81            Yes      Graduate       157.5\n95             No      Graduate       130.0\n102            No      Graduate       130.0\n103            No      Graduate       130.0\n113           Yes      Graduate       157.5\n127            No      Graduate       130.0\n202            No  Not Graduate       113.0\n284            No      Graduate       130.0\n305            No  Not Graduate       113.0\n322            No  Not Graduate       113.0\n338            No  Not Graduate       113.0\n387            No  Not Graduate       113.0\n435            No      Graduate       130.0\n437            No      Graduate       130.0\n479            No      Graduate       130.0\n524            No      Graduate       130.0\n550           Yes      Graduate       157.5\n551            No  Not Graduate       113.0\n605            No  Not Graduate       113.0\n\nurl='https://raw.githubusercontent.com/Aniruddh-SK/Loan-Prediction-Problem/master/train.csv'\n\ndf = pd.read_csv(url) #Reading the dataset in a dataframe using Pandas\n\n#df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)\n\ndf['Self_Employed'].fillna('No',inplace=True)\n\n\nprint (df.loc[df['LoanAmount'].isnull(), ['Self_Employed','Education', 'LoanAmount']])\n    Self_Employed     Education  LoanAmount\n0              No      Graduate         NaN\n35             No      Graduate         NaN\n63             No      Graduate         NaN\n81            Yes      Graduate         NaN\n95             No      Graduate         NaN\n102            No      Graduate         NaN\n103            No      Graduate         NaN\n113           Yes      Graduate         NaN\n127            No      Graduate         NaN\n202            No  Not Graduate         NaN\n284            No      Graduate         NaN\n305            No  Not Graduate         NaN\n322            No  Not Graduate         NaN\n338            No  Not Graduate         NaN\n387            No  Not Graduate         NaN\n435            No      Graduate         NaN\n437            No      Graduate         NaN\n479            No      Graduate         NaN\n524            No      Graduate         NaN\n550           Yes      Graduate         NaN\n551            No  Not Graduate         NaN\n605            No  Not Graduate         NaN\n\nidx = df.loc[df['LoanAmount'].isnull(), ['Self_Employed','Education', 'LoanAmount']].index\nprint (idx)\nInt64Index([  0,  35,  63,  81,  95, 102, 103, 113, 127, 202, 284, 305, 322,\n            338, 387, 435, 437, 479, 524, 550, 551, 605],\n           dtype='int64')\n\n# Replace missing values\ndf['LoanAmount'] = df.groupby(['Education','Self_Employed'])['LoanAmount']\n                     .apply(lambda x: x.fillna(x.median()))\n\nprint (df.loc[df.index.isin(idx), ['Self_Employed','Education', 'LoanAmount']])\n    Self_Employed     Education  LoanAmount\n0              No      Graduate       130.0\n35             No      Graduate       130.0\n63             No      Graduate       130.0\n81            Yes      Graduate       157.5\n95             No      Graduate       130.0\n102            No      Graduate       130.0\n103            No      Graduate       130.0\n113           Yes      Graduate       157.5\n127            No      Graduate       130.0\n202            No  Not Graduate       113.0\n284            No      Graduate       130.0\n305            No  Not Graduate       113.0\n322            No  Not Graduate       113.0\n338            No  Not Graduate       113.0\n387            No  Not Graduate       113.0\n435            No      Graduate       130.0\n437            No      Graduate       130.0\n479            No      Graduate       130.0\n524            No      Graduate       130.0\n550           Yes      Graduate       157.5\n551            No  Not Graduate       113.0\n605            No  Not Graduate       113.0\n\ndf['Loan_Status'].fillna('No',inplace=True)\ndf['Credit_History'].fillna(0,inplace=True) \n\noutcome_var = 'Loan_Status'\nmodel = LogisticRegression()\npredictor_var = ['Credit_History']\n\nclassification_model(model, df, predictor_var,outcome_var)\n"
'import numpy as np\nimport xgboost as xgb\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef logregobj(preds, dtrain):\n    """log likelihood loss"""\n    labels = dtrain.get_label()\n    preds = sigmoid(preds)\n    grad = preds - labels\n    hess = preds * (1.0-preds)\n    return grad, hess\n\n\n# Build a toy dataset.\nX, Y = make_classification(n_samples=1000, n_features=5, n_redundant=0, n_informative=3,\n                           random_state=1, n_clusters_per_class=1)\n\n# Instantiate a Booster object to do the heavy lifting\ndtrain = xgb.DMatrix(X, label=Y)\nparams = {\'max_depth\': 2, \'eta\': 1, \'silent\': 1}\nnum_round = 2\nmodel = xgb.Booster(params, [dtrain])\n\n# Run 10 boosting iterations\n# g and h can be monitored for gradient statistics\nfor _ in range(10):\n    pred = model.predict(dtrain)\n    g, h = logregobj(pred, dtrain)\n    model.boost(dtrain, g, h)\n\n# Evaluate predictions    \nyhat = model.predict(dtrain)\nyhat = 1.0 / (1.0 + np.exp(-yhat))\nyhat_labels = np.round(yhat)\nconfusion_matrix(Y, yhat_labels)\n'
'cv = PredefinedSplit(test_fold=my_test_fold)\n\ncv = list(PredefinedSplit(test_fold=my_test_fold).split(new_data_df, df_y))\n'
"capacity=1000 # your queue capacity\ndtype=tf.float32\nqueue_diff = tf.FIFOQueue(capacity, dtype)\nenqueue_op = tf.cond(tf.equal(queue_diff.size(), capacity), lambda:is_full(), lambda: enqueue(queue_diff, enqueue_elements..))\n\n# a function to takecare when the queue is full\ndef is_full():\n    return 'Enqueue_Failed'\n\n# a function for enqueue ops\ndef enqueue(queue, element...):\n    queue.enqueue(element)\n    return 'Enqueue_Success'\n"
'q = tf.FIFOQueue(...)\n\n# Define a dummy dataset that contains the same value repeated indefinitely.\ndummy = tf.contrib.data.Dataset.from_tensors(0).repeat(None)\n\ndataset_from_queue = dummy.map(lambda _: q.dequeue())\n'
'#!/usr/bin/env python\nimport sys\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\n\ndata = pd.read_csv("data.csv")\n\nfeatures = data.drop([\'good buy\'], axis = 1)\nlbls = data.drop([ \'area\', \'bathrooms\', \'price\', \'sq_price\'], axis = 1)\n\nfeatures = features[0:20]\nlbls = lbls[0:20]\n\nmu = np.mean(features, axis=0)\nsigma = (np.std(features, axis=0))\nfeatures = (features - mu) / sigma\n\nn_examples = len(lbls)\n\n# Model\n\n# Hyper parameters\n\nepochs = 100\nlearning_rate = 0.01\nbatch_size = 5\n\ninput_data = tf.placeholder(\'float\', [None, 4])\nlabels = tf.placeholder(\'float\', [None, 1])\n\nweights = {\n      \'hl1\': tf.Variable(tf.random_normal([4, 10])),\n      \'hl2\': tf.Variable(tf.random_normal([10, 10])),\n      \'hl3\': tf.Variable(tf.random_normal([10, 4])),\n      \'ol\': tf.Variable(tf.random_normal([4, 1]))\n      }\n\nbiases = {\n      \'hl1\': tf.Variable(tf.zeros([10])),\n      \'hl2\': tf.Variable(tf.zeros([10])),\n      \'hl3\': tf.Variable(tf.zeros([4])),\n      \'ol\': tf.Variable(tf.zeros([1]))\n      }\n\n\n\nhl1 = tf.nn.relu(tf.add(tf.matmul(input_data, weights[\'hl1\']), biases[\'hl1\']))\nhl2 = tf.nn.relu(tf.add(tf.matmul(hl1, weights[\'hl2\']), biases[\'hl2\']))\nhl3 = tf.nn.relu(tf.add(tf.matmul(hl2, weights[\'hl3\']), biases[\'hl3\']))\nol = tf.nn.sigmoid(tf.add(tf.matmul(hl3, weights[\'ol\']), biases[\'ol\']))\n\nloss = tf.reduce_mean((labels - ol)**2)\ntrain = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\niterations = int(n_examples/batch_size)\n\n\ndef training_accuracy():\n  foo,  = sess.run([ol], feed_dict={input_data: features, labels: lbls})\n  return (float(np.count_nonzero(np.equal(np.round(foo), lbls))) / float(lbls.shape[0]))\n\n\nprint("Initial training accuracy %f" % training_accuracy())\n\n\nfor epoch_no in range(epochs):\n  ptr = 0\n  for iteration_no in range(iterations):\n    epoch_input = features[ptr:ptr+batch_size]\n    epoch_label = lbls[ptr: ptr+batch_size]\n    ptr = (ptr + batch_size)%len(features)\n    _, err = sess.run([train, loss], feed_dict={input_data: epoch_input, labels: epoch_label})\n  print("Error at epoch ", epoch_no, ": ", err)\n  print("  Training accuracy %f" % training_accuracy())\n'
"def generate_input_data(num_examples, num_features):\n    features = []\n    labels = []\n    for i in xrange(num_examples):\n        features.append(np.random.rand(num_features) * np.random.randint(1, 10) + np.random.rand(num_features))\n    if np.random.randint(101) &gt; 90:\n        features[i-1][np.random.randint(num_features)] = 0\n\n    hard = ceil(np.sum(features[i-1])) % 2\n    easy = 0\n    if features[i-1][0] &gt; 3:\n        easy = 1\n    labels.append(easy)\n\n    df = pd.concat(\n    [\n        pd.DataFrame(features),\n        pd.Series(labels).rename('labels')\n    ],\n    axis=1,\n    )\n    return df\n"
"df = df.set_index('Test')\ndf.T.drop_duplicates(keep='first').T\n\n       A  D  E  H  N\nTest                \ntest1  0  0  0  1  1\ntest2  1  0  1  1  0\ntest3  1  0  1  1  0\ntest4  1  1  0  1  0\ntest5  1  1  0  1  0\n"
"train_texts = ['apple mango', 'mango banana']\ntest_texts = ['apple banana', 'mango orange']\nvocab = ['apple', 'mango', 'banana', 'orange']\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer1 = TfidfVectorizer(smooth_idf=True, vocabulary=vocab).fit(train_texts)\nvectorizer2 = TfidfVectorizer(smooth_idf=False, vocabulary=vocab).fit(train_texts)\nprint(vectorizer1.transform(test_texts).todense()) # works okay\nprint(vectorizer2.transform(test_texts).todense()) # raises a ValueError\n\n[[ 0.70710678  0.          0.70710678  0.        ]\n [ 0.          0.43016528  0.          0.90275015]]\n...\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n"
'variance = variance + ((percentage[i]) - mean) ** 2\n\ndef checkDropVariance(df, column):\n    # get the variance of column data\n    v = df[column].var()\n    # drop the column if the variance is higher than 10\n    if v &gt; 10:\n        df = df.drop(column, axis=1)\n    return df\n'
"# Import dataset\ndataset = pd.read_csv('NBA.csv')\nX = dataset.iloc[:, 1].values\ny = dataset.iloc[:, 0].values\n\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n"
'y_val_true, val_pred = y_val_true.reshape((-1)), val_pred.reshape((-1))\nval_ap = average_precision_score(y_val_true, val_pred)\n'
'from __future__ import division\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SQLContext, functions as fn\nfrom sklearn.metrics import confusion_matrix\n\ndef getFirstColumn(line):\n    parts = line.split(\',\')\n    return parts[0]\n\ndef getSecondColumn(line):\n    parts = line.split(\',\')\n    return parts[1]\n\n# Initialization\nconf= SparkConf()\nconf.setAppName("ConfusionMatrixPrecisionRecall")\n\nsc = SparkContext(conf= conf) # SparkContext\nsqlContext = SQLContext(sc) # SqlContext\n\ndata = sc.textFile(\'YOUR_FILE_PATH\') # Load dataset\n\ny_true = data.map(getFirstColumn).collect() # Split from line the class\ny_pred = data.map(getSecondColumn).collect() # Split from line the tags\n\nconfusion_matrix = confusion_matrix(y_true, y_pred)\nprint("Confusion matrix:\\n%s" % confusion_matrix)\n\n# The True Positives are simply the diagonal elements\nTP = np.diag(confusion_matrix)\nprint("\\nTP:\\n%s" % TP)\n\n# The False Positives are the sum of the respective column, minus the diagonal element (i.e. the TP element\nFP = np.sum(confusion_matrix, axis=0) - TP\nprint("\\nFP:\\n%s" % FP)\n\n# The False Negatives are the sum of the respective row, minus the         diagonal (i.e. TP) element:\nFN = np.sum(confusion_matrix, axis=1) - TP\nprint("\\nFN:\\n%s" % FN)\n\nnum_classes = INTEGER #static kwnow a priori, put your number of classes\nTN = []\n\nfor i in range(num_classes):\n    temp = np.delete(confusion_matrix, i, 0)    # delete ith row\n    temp = np.delete(temp, i, 1)  # delete ith column\n    TN.append(sum(sum(temp)))\nprint("\\nTN:\\n%s" % TN)\n\n\n\n\nprecision = TP/(TP+FP)\nrecall = TP/(TP+FN)\n\nprint("\\nPrecision:\\n%s" % precision)\n\nprint("\\nRecall:\\n%s" % recall)\n'
'loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n'
'For each feature\n    Sort data by feature\n    Calculate cumulative counts of different values of target\n    Calculate reverse cumulative counts of different values of target\n    At each feature value\n         Calculate gini value based on cumulative counts\n         Keep the maximum\n'
'pred_prob = naive_b.predict_proba(test_features)\n\ntest_data["p_malw"] = naive_b.predict_proba(test_features)\n\nfrom sklearn.metrics import confusion_matrix\n\nnaive_B.fit(train_features, train_label)\n\npred_label = naive_B.predict(test_features)\n\nconfusion_m = confusion_matrix(test_label, pred_label)\nconfusion_m\n'
"from itertools import chain\n\n'.'.join(chain.from_iterable(ser.to_dict().items()))\n#'outlook.sunny.temperature.mild.humidity.normal....yes'\n"
' foldx.append(X[j:j+143,:])\n\nfoldy.append(y[foldx[j]])\n'
'data.dropna(axis=0,inplace=True)\n'
'baskets = sc.parallelize([("Rock Salt", "Blankets"), ("Blankets", "Dry Fruits", Canned Food")])\n\n data.map(lambda line: line.strip().split())\n'
'tensor_list =[]\nfor i,chunk in enumerate(tensor.chunk(100,dim=0)):\n    output = hiddenlayer(chunk).squeeze()\n    tensor_list.append(output)\n\nresult = torch.reshape(torch.stack(tensor_list,0), (-1, 1))\n'
'samples[j] = data[indices]\ntargets[j] = data[rows[j] + delay][1]\n'
'class Autoencoder(Chain):\n    def __init__(self):\n       super().__init__()\n       with self.init_scope():\n           self.l1 = L.Linear(3,2)\n           self.l2 = L.Linear(2,3)\n\n    def forward(self,x):\n       h1 = self.l1(x)\n       h2 = self.l2(h1) \n       return h2\n\n    def __call__(self,x):\n       h = self.forward(x)\n       # Instead of h, __call__ should return loss.\n       loss = F.mean_squared_error(h, x)\n       return loss\n'
"&gt;&gt;&gt; from sklearn.cluster import DBSCAN\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[1, 2], [2, 2], [2, 3],\n...               [8, 7], [8, 8], [25, 80]])\n&gt;&gt;&gt; clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n&gt;&gt;&gt; clustering.labels_\narray([ 0,  0,  0,  1,  1, -1])\n&gt;&gt;&gt; clustering \nDBSCAN(algorithm='auto', eps=3, leaf_size=30, metric='euclidean',\n    metric_params=None, min_samples=2, n_jobs=None, p=None)\n"
"concat = concatenate([p0.output, q0.output])\nx = Dense(10)(concat)\nout = Activation('softmax')(x)\n\nmodel = Model([p0.input, q0.input], out)\n\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n"
'import numpy as np\n\nimage_resize = np.expand_dims(image_resize, axis=0)  # shape would be: (1, 64, 64, 3)\n'
'print(gen.append(bird.gen))\n'
'code_book = code_book[has_members]\n'
"xTrain = pd.DataFrame({'address':['a', 'b', 'c'],'b':[1,2, np.nan]})\nprint (xTrain)\n  address    b\n0       a  1.0\n1       b  2.0\n2       c  NaN\n\ncols = xTrain.select_dtypes(include=np.number).columns\n\nxTrain[cols] = xTrain[cols].fillna(xTrain.mean())\nprint (xTrain)\n  address    b\n0       a  1.0\n1       b  2.0\n2       c  1.5\n"
'data_input = np.array(data_input).reshape(5 ,2)\ndata_output = np.array(data_output).reshape(5)\n'
"outputMobilenet = await model.infer(readImage('./hospitalTest.jpg'), 'conv_preds')\npredicted = await classifier.predictClass(outputMobilenet)\n"
'class LossHistory(keras.callbacks.Callback):\n\n    def __init__(self, data_generator, **kwargs):\n        self.generator = data_generator\n        Super(LossHistory, self).__init__(**kwargs)\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_true = self.generator.y\n        y_pred = self.model.predict_generator(self.generator)\n\n\nloss_history = LossHistory(train_data_generator)\n'
'from sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\nfrom sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(random_state = 0, class_weight="balanced")\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_auc_score\n\nmyscore = make_scorer(roc_auc_score, multi_class=\'ovo\',needs_proba=True)\n\nfrom sklearn.model_selection import cross_validate\ncross_validate(clf, X, y, cv=10, scoring = myscore)\n\n'
'# run prediction\npredictions, probabilities = prediction.predictImage(os.path.join(execution_path, "HandTextTest.jpg"), result_count=2)\n'
"params = {\n    ......\n    'objective': 'binary',\n    'metric': 'custom',\n    ......\n}\n\ngbm = lgb.train(params,\n                lgb_train,\n                feval=f_pr_auc,\n                valid_sets=lgb_eval)\n"
"from sklearn.linear_model import LogisticRegression\nX = pd.DataFrame({'a': [1,2,3], 'b': [4,5,6]})\ny = pd.Series([0, 1, 0])\n\nmodel = LogisticRegression()\nmodel.fit(X, y)\nprint(model.coef_)\n\n[[-1.87654483e-05  7.42492539e-05]]\n\nimport tensorflow as tf\ndataset = tf.data.Dataset.from_tensor_slices((X.values, y.values))\n"
'class CustomCallback(keras.callbacks.Callback):\n    def __init__(self, save_path, VAE):\n        self.save_path = save_path\n        self.VAE = VAE\n    def on_epoch_end(self, epoch, logs={}):\n        #load the image\n        #get latent_space with self.VAE.encoder.predict(image)\n        #get reconstructed image wtih self.VAE.decoder.predict(latent_space)\n        #plot reconstructed image with matplotlib.pyplot\n'
"n_sample = 100\nfeatures = 1024\nX_train = np.random.uniform(0,1, (n_sample,features))\ny_train_R = np.random.uniform(0,1, n_sample)\ny_train_C = np.random.randint(0,2, n_sample)\n\ndef run(X_train, y_train_C, y_train_R):\n    \n    _input = keras.layers.Input(shape=(features,))\n\n    hidden1 = keras.layers.Dense(500, activation = 'elu')(_input)\n\n    hidden2 = keras.layers.Dense(300, activation = 'elu')(hidden1)\n\n    classification = keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;classification&quot;)(hidden2)\n\n    regression = keras.layers.Dense(1, activation=&quot;linear&quot;, name=&quot;regression&quot;)(hidden2)\n\n    multi_model = keras.Model(inputs=[_input], outputs=[classification, regression])\n    \n    multi_model.compile(loss={'classification': 'binary_crossentropy','regression': 'mse'},\n                        optimizer='Nadam',\n                        metrics={'classification':'AUC', 'regression': 'mse'})\n   \n    multi_model.fit(X_train,\n                    [y_train_C, y_train_R],\n                    validation_split=0.2,\n                    batch_size=128,\n                    epochs=5,\n                    verbose=1)\n    \n    return multi_model\n\nmulti_model = run(X_train, y_train_C, y_train_R)\nprediction_class, prediction_reg = multi_model.predict(X_train)\nprediction_class = (prediction_class&gt;0.5).ravel()+0\n"
'tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n'
'for epoch in range(EPOCHS):\n    for i in range(0, len(train_X), BATCH_SIZE):\n        batch_X = train_X[i:i + BATCH_SIZE].view(-1, 1, 10000)\n        batch_y = train_y[i:i + BATCH_SIZE]\n\n        optimizer.zero_grad()\n\n        outputs = net(batch_X)\n        loss = loss_function(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(IMAGE_SIZE * IMAGE_SIZE, 64)\n        self.fc2 = nn.Linear(64, 64)\n        self.fc3 = nn.Linear(64, 64)\n        self.fc4 = nn.Linear(64, 1)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n    \nloss_function = nn.BCEWithLogitsLoss()\n'
'def create_model(neurons):\n    ....\nreturn model\n\ndef create_model(neurons):\n    ....\n    return model\n\nmodel = KerasClassifier(build_fn=create_model(), epochs=100, batch_size=10, verbose=0)\n'
'class CustomVariationalLayer(keras.layers.Layer):\n    def vae_loss(self, x, z_decoded, z_mean, z_log_var):\n        # ...\n\n    def call(self, inputs):\n        x = inputs[0]\n        z_decoded = inputs[1]\n        z_mean = inputs[2]\n        z_log_var = inputs[3]\n        loss = self.vae_loss(x, z_decoded, z_mean, z_log_var)\n        # ...\n\ny = CustomVariationalLayer()([input_img, z_decoded, z_mean, z_log_var])\n'
'def generate_data(X1,X2,Y,batch_size):\n  p_input=[]\n  c_input=[]\n  target=[]\n  batch_count=0\n  for i in range(len(X1)):\n    p_input.append(X1[i])\n    c_input.append(X2[i])\n    target.append(Y[i])\n    batch_count+=1\n    if batch_count&gt;batch_size:\n      prev_X=np.array(p_input,dtype=np.int64)\n      cur_X=np.array(c_input,dtype=np.int64)\n      cur_y=np.array(target,dtype=np.int32)\n      print(len(prev_X),len(cur_X))\n      yield ([prev_X,cur_X],cur_y ) \n      p_input=[]\n      c_input=[]\n      target=[]\n      batch_count=0\n  return\n\nbatch_size=256\nepoch_steps=math.ceil(len(previous_sentences)/ batch_size)\nhist = model.fit_generator(generate_data(previous_sentences,current_sentences, y_train, batch_size),\n                steps_per_epoch=epoch_steps,\n                callbacks = [early_stopping_cb],\n                validation_data=generate_data(val_prev, val_curr,y_val,batch_size),\n                validation_steps=val_steps,  class_weight=custom_weight_dict,\n                 verbose=1)\n'
"clf_3 = SVC(kernel='linear', \n            class_weight='balanced', # penalize\n            probability=True)\n\n&quot;SVM&quot;: {\n        'kernel': [&quot;poly&quot;],\n        'C': np.linspace(0, 15, 30),\n        'class_weight': 'balanced'\n\n    }\n\nLogisticRegression(class_weight={0:1, 1:10}) # if problem is a binary one\n\n&quot;LogisticRegression&quot;: {\n        'C': np.linspace(0, 15, 30),\n        'penalty': [&quot;l1&quot;, &quot;l2&quot;, &quot;elasticnet&quot;, &quot;none&quot;],\n        'class_weight':{0:1, 1:10}\n    }\n"
"def sampling(args):\n    \n    z_mean, z_log_sigma = args\n    batch_size = tf.shape(z_mean)[0]\n    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n    \n    return z_mean + K.exp(0.5 * z_log_sigma) * epsilon\n\ndef vae_loss(x, x_decoded_mean, z_log_var, z_mean):\n\n    xent_loss = original_dim * K.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var))\n    vae_loss = K.mean(xent_loss + kl_loss)\n\n    return vae_loss\n\ndef get_model():\n    \n    ### encoder ###\n    \n    inp = Input(shape=(n_features,))\n    enc = Dense(64)(inp)\n    \n    z = Dense(32, activation=&quot;relu&quot;)(enc)\n    z_mean = Dense(latent_dim)(z)\n    z_log_var = Dense(latent_dim)(z)\n            \n    encoder = Model(inp, [z_mean, z_log_var])\n    \n    ### decoder ###\n    \n    inp_z = Input(shape=(latent_dim,))\n    dec = Dense(64)(inp_z)\n\n    out = Dense(n_features)(dec)\n    \n    decoder = Model(inp_z, out)   \n    \n    ### encoder + decoder ###\n    \n    z_mean, z_log_sigma = encoder(inp)\n    z = Lambda(sampling)([z_mean, z_log_var])\n    pred = decoder(z)\n    \n    vae = Model(inp, pred)\n    vae.add_loss(vae_loss(inp, pred, z_log_var, z_mean))  # &lt;======= add_loss\n    vae.compile(loss=None, optimizer='adam')\n    \n    return vae, encoder, decoder\n"
"    a   b\n1   2   3\n4   2   5\n\n    a   b\n1   6   3\n5   2   8\n\ndf1['c'] = df2['a']\n\n    a   b   c\n1   2   3   6.0\n4   2   5   NaN\n\ndf1 = df1.reset_index(drop=True)\ndf2 = df2.reset_index(drop=True)\n\ndf1['c'] = df2['a']\n\n    a   b   c\n0   2   3   6\n1   2   5   2\n"
'X = vectorizer.transform(docs)\nterms = np.array(vectorizer.get_feature_names())\nterms_for_first_doc = zip(terms, X.toarray()[0])\n'
'class ProbSVC(SVC):\n    def predict(self, X):\n        return super(ProbSVC, self).predict_proba(X)\n'
"&gt;&gt;&gt; 'How'.lower()\n'how'\nif word.lower() in word_list: # where word_list contains lower case only\n\nif link_word in current_dir == TRUE:\n\nif link_word in current_dir:\n"
"from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom scipy.sparse import csr_matrix\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nX, y = digits.data, digits.target\n\nfor CLF in [GaussianNB, MultinomialNB, BernoulliNB, LogisticRegression, LinearSVC, GradientBoostingClassifier]:\n    print(CLF.__name__, end='')\n    try:\n        CLF().fit(csr_matrix(X), y == 0)\n        print(' PASS')\n    except TypeError:\n        print(' FAIL')\n\nGaussianNB FAIL\nMultinomialNB PASS\nBernoulliNB PASS\nLogisticRegression PASS\nLinearSVC PASS\nGradientBoostingClassifier PASS\n"
'[LibLinear]\niter  1 act 1.107e-01 pre 1.107e-01 delta 4.189e-01 f 2.079e+00 |g| 5.541e-01 CG   2\niter  2 act 2.825e-06 pre 2.825e-06 delta 4.189e-01 f 1.969e+00 |g| 2.547e-03 CG   2\n'
"import numpy as np\ndata_train.fillna('Credit_History', np.median(data_train['Credit_History']))\n"
'estimator.fit(X_train)\ny_pred_test = estimator.predict(X_test)\n'
'from __future__ import division\nfrom collections import Counter\nimport re, nltk\n\nWORDS = nltk.corpus.reuters.words()\n\nCOUNTS = Counter(WORDS)\n\ndef pdist(counter):\n    "Make a probability distribution, given evidence from a Counter."\n    N = sum(counter.values())\n    return lambda x: counter[x]/N\n\nP = pdist(COUNTS)\n\ndef Pwords(words):\n    "Probability of words, assuming each word is independent of others."\n    return product(P(w) for w in words)\n\ndef product(nums):\n    "Multiply the numbers together.  (Like `sum`, but with multiplication.)"\n    result = 1\n    for x in nums:\n        result *= x\n    return result\n\ndef splits(text, start=0, L=20):\n    "Return a list of all (first, rest) pairs; start &lt;= len(first) &lt;= L."\n    return [(text[:i], text[i:]) \n            for i in range(start, min(len(text), L)+1)]\n\ndef segment(text):\n    "Return a list of words that is the most probable segmentation of text."\n    if not text: \n        return []\n    else:\n        candidates = ([first] + segment(rest) \n                      for (first, rest) in splits(text, 1))\n        return max(candidates, key=Pwords)\n\nprint segment(\'britainnews\')     # [\'britain\', \'news\']\n'
'import pandas as pd\n\n# create a pandas dataframe that contains your features\nX = pd.DataFrame({\'quantity\': [13, 7, 42, 11],\n                  \'item_name\': [\'nut\', \'bolt\', \'bolt\', \'chair\'],\n                  \'item_type\': [\'hardware\', \'hardware\', \'hardware\', \'furniture\'],\n                  \'item_price\': [1.95, 4.95, 2.79, 19.95]})\n\n# create corresponding target (this is often just one of the dataframe columns)\ny = pd.Series([0, 1, 1, 0], index=X.index)\n\nfrom sklearn.pipeline import make_union, make_pipeline\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# create your preprocessor that handles different feature types separately\npreprocessor = make_union(\n    make_pipeline(\n        FeatureTypeSelector(\'continuous\'),\n        RobustScaler(),\n    ),\n    make_pipeline(\n        FeatureTypeSelector(\'categorical\'),\n        RowToDictTransformer(),\n        DictVectorizer(sparse=False),  # set sparse=True if you get MemoryError\n    ),\n)\n\n# example use of your combined preprocessor\npreprocessor.fit_transform(X)\n\n# choose some estimator\nestimator = LogisticRegression()\n\n# your prediction model can be created as follows\nmodel = make_pipeline(preprocessor, estimator)\n\n# and training is done as follows\nmodel.fit(X, y)\n\n# predict (preferably not on training data X)\nmodel.predict(X)\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\n\nclass FeatureTypeSelector(TransformerMixin, BaseEstimator):\n    """ Selects a subset of features based on their type """\n\n    FEATURE_TYPES = {\n        \'categorical\': [\n            \'item_name\',\n            \'item_type\',\n        ],\n        \'continuous\': [\n            \'quantity\',\n            \'item_price\',\n        ]\n    }\n\n    def __init__(self, feature_type):\n        self.columns = self.FEATURE_TYPES[feature_type]\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.columns]\n\n\nclass RowToDictTransformer(TransformerMixin, BaseEstimator):\n    """ Prepare dataframe for DictVectorizer """\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return (row[1] for row in X.iterrows())\n'
'import Orange\nfrom Orange.widgets.visualize.owruleviewer import OWRuleViewer\nfrom AnyQt.QtWidgets import QApplication\nfrom Orange.classification import CN2Learner\n\ndata = Orange.data.Table("titanic")\nlearner = Orange.classification.CN2Learner()\nmodel = learner(data)\nmodel.instances = data\n\na = QApplication([])\now = OWRuleViewer()\now.set_classifier(model)\n\now.show()\na.exec()\n'
"probs = LRC.predict_proba(x)\nclass_indexes = np.argmax(probs,axis=1)\nmax_probs = probs[np.arange(len(x)),class_indexes]\nclass_output = lrc.classes_[class_indexes]\nclass_prob_list = zip(class_output,max_probs)\nprint [str(cls)+'.'+str(prob) for cls,prob in class_prob_list]\n"
'x = tf.placeholder(tf.float32, [None, seq_size, 1])\n\nx = tf.placeholder(tf.float32, [None, seq_size, d])\n'
'df = df.dropna(axis=0)\n\ndf.dropna(axis=0, inplace=True)\n'
'def radial_basis(x, y, gamma=10):\n    return np.exp(-gamma * la.norm(np.subtract(x, y)))\n\ndef proxy_kernel(X, Y, K=radial_basis):\n    """Another function to return the gram_matrix,\n    which is needed in SVC\'s kernel or fit\n    """\n    gram_matrix = np.zeros((X.shape[0], Y.shape[0]))\n    for i, x in enumerate(X):\n        for j, y in enumerate(Y):\n            gram_matrix[i, j] = K(x, y)\n    return gram_matrix\n\nclf_SVM_radial_basis = SVC(kernel=proxy_kernel) # Note that it\'s proxy_kernel here now\nclf_SVM_radial_basis.fit(X_train, y_train)\n'
'train/\n   cat/\n   dog/\n   eel/\n'
'import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nX = X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([1, 2, 3, 4])\nkf = KFold(n_splits=2)\nlinreg = LinearRegression()\nxval_err = 0\n'
'data_transformer.cpp:168] Check failed: height&lt;=datum_height (224 vs. 199)\n\nCHECK_LE(height, datum_height);\n'
'  learn_runner.run(\n      generate_experiment_fn(\n          min_eval_frequency=args.min_eval_frequency,\n          eval_delay_secs=args.eval_delay_secs,\n          train_steps=args.train_steps,\n          eval_steps=args.eval_steps,\n          export_strategies=[saved_model_export_utils.make_export_strategy(\n              model.SERVING_FUNCTIONS[args.export_format],\n              exports_to_keep=1\n          )]\n      ),\n      run_config=tf.contrib.learn.RunConfig(model_dir=args.job_dir),\n      hparams=hparam.HParams(**args.__dict__)\n  )\n\ndef csv_serving_input_fn():\n  """Build the serving inputs."""\n  csv_row = tf.placeholder(\n      shape=[None],\n      dtype=tf.string\n  )\n  features = parse_csv(csv_row)\n  # Ignore label column\n  features.pop(LABEL_COLUMN)\n  return tf.estimator.export.ServingInputReceiver(\n      features, {\'csv_row\': csv_row})\ndef example_serving_input_fn():\n  """Build the serving inputs."""\n  example_bytestring = tf.placeholder(\n      shape=[None],\n      dtype=tf.string,\n  )\n  features = tf.parse_example(\n      example_bytestring,\n      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS)\n  )\n  return tf.estimator.export.ServingInputReceiver(\n      features, {\'example_proto\': example_bytestring})\n\n\ndef json_serving_input_fn():\n  """Build the serving inputs."""\n  inputs = {}\n  for feat in INPUT_COLUMNS:\n    inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\n  return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\n\nSERVING_FUNCTIONS = {\n    \'JSON\': json_serving_input_fn,\n    \'EXAMPLE\': example_serving_input_fn,\n    \'CSV\': csv_serving_input_fn\n}\n'
'data = tf.placeholder(tf.float32, [None, 20,1]) \n\n[\n array([[0],[0],[1],[0],[0],[1],[0],[1],[1],[0],[0],[0],[1],[1],[1],[1],[1],[1],[0],[0]]), \n array([[1],[1],[0],[0],[0],[0],[1],[1],[1],[1],[1],[0],[0],[1],[0],[0],[0],[1],[0],[1]]), \n .....\n]\n\nimport numpy as np\ntrain_input=np.reshape(train_input,(len(train_input),39,1))\ndata = tf.placeholder(tf.float32, [None, 39,1]) \n\nimport tensorflow as tf\nimport numpy as np\nnum_hidden=5\ntrain_input=np.random.rand(89102,39)\ntrain_input=np.reshape(train_input,(len(train_input),39,1))\ntrain_output=np.random.rand(89102,3)\n\ndata = tf.placeholder(tf.float32, [None, 39, 1])\ntarget = tf.placeholder(tf.float32, [None, 3])\ncell = tf.nn.rnn_cell.LSTMCell(num_hidden)\n\nval, state = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\nval = tf.transpose(val, [1, 0, 2])\nlast = tf.gather(val, int(val.get_shape()[0]) - 1)\n\n\nweight = tf.Variable(tf.truncated_normal([num_hidden, int(target.get_shape()[1])]))\nbias = tf.Variable(tf.constant(0.1, shape=[target.get_shape()[1]]))\n\nprediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n\ncross_entropy = -tf.reduce_sum(target * tf.log(tf.clip_by_value(prediction, 1e-10, 1.0)))\n\noptimizer = tf.train.AdamOptimizer()\nminimize = optimizer.minimize(cross_entropy)\n\nmistakes = tf.not_equal(tf.argmax(target, 1), tf.argmax(prediction, 1))\nerror = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n\n\ninit_op = tf.initialize_all_variables()\nsess = tf.Session()\nsess.run(init_op)\nbatch_size = 1000\nno_of_batches = int(len(train_input) / batch_size)\nepoch = 5000\nfor i in range(epoch):\n    ptr = 0\n    for j in range(no_of_batches):\n        inp, out = train_input[ptr:ptr + batch_size], train_output[ptr:ptr + batch_size]\n        ptr += batch_size\n        sess.run(minimize, {data: inp, target: out})\n    print( "Epoch - ", str(i))\n'
'train_x = [[1, 2, 3, 4], [5, 6, 7, 8]]\ntrain_y = [24, 1680]\n\ntrain_x = np.asarray(train_x)\ntrain_y = np.asarray(train_y)\n\nm = train_x.shape[0]\nn = train_x.shape[1]\n\nX = tf.placeholder(tf.float32, [None, n])\nY = tf.placeholder(tf.float32, [None, 1])\n\nW = tf.Variable(tf.random_normal((n, 1)))\nb = tf.Variable(tf.zeros(1, 1))\n\nmodel = tf.add(tf.matmul(X, W), b)\ncost = tf.reduce_sum(tf.pow(model - Y, 2)) / (2 * m)\n\n...\nfor i in range(1000):\n    for x, y in zip(train_x, train_y):\n        x = np.reshape(x, (-1, 4))\n        y = np.reshape(y, (-1, 1))\n        sess.run(optimizer, feed_dict={X: x, Y: y})\n'
'import numpy as np\nimport matplotlib.pyplot as py\n\nnp.random.seed(42)\nw = np.random.rand(2)\nb = 0\nerrors = [] \neta = .2\nn = 10\n\nfor i in range(n):\n    for x, y in training_set:\n        u = np.sum(x*w)+b     \n        error = y - activation_function(u)       \n        errors.append(error) \n\n        for index, value in enumerate(x):\n            #print(index, " ", value)\n            w[index] += eta * error * value\n            b += eta*error\n\nlim = 3.0\nX1 = [x1 for x1 in np.arange(-lim,lim,0.1)]\nX2 = [x2 for x2 in np.arange(-lim,lim,0.1)]\nXX = [(x1,x2) for x1 in np.arange(-lim,lim,0.1) for x2 in np.arange(-lim,lim,0.1)]\nXcolor = ["blue" if np.sum(w[0]*x1+w[1]*x2)+b &gt; 0  else "red" for x1 in X1 for x2 in X2]\nx,y = zip(*XX)\npy.scatter(x,y, c = Xcolor)\npy.scatter(0,0, c = "black")\npy.scatter(1,0, c = "white")\npy.scatter(0,1, c = "white")\npy.scatter(1,1, c = "white")\npy.show()\n'
'from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import make_scorer\n\ndef get_TN_rate(y_true,y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    specificity = float(tn)/(float(tn)+float(fp))\n    return specificity\n\ntn_rate = make_scorer(get_TN_rate,greater_is_better=True)\n'
'labellabelfrom 0 1 2 4\nlabellabelto 0 1 1 1\n'
'a = tf.placeholder(tf.float32, shape=[None, 3], name="input_placeholder_a")\nb = tf.placeholder(tf.float32, shape=[3, 10], name="input_placeholder_b")\n\nnormalize_a = tf.nn.l2_normalize(a, dim=0)\nnormalize_b = tf.nn.l2_normalize(b, dim=0)\ncos_similarity=tf.matmul(normalize_a, normalize_b)\n\nsess=tf.Session()\ncos_sim=sess.run(cos_similarity,feed_dict={\n  a: np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),\n  b: np.arange(30).reshape([3, 10])\n})\nprint cos_sim\n'
'self.optimizer =  tf.train.GradientDescentOptimizer(self.lr)\nself.train_step = self.optimizer.minimize(self.loss)\ngvs = self.optimizer.compute_gradients(self.loss)\n'
'Keywords={u\'secondary\': [u\'sales growth\', u\'next generation store\', \n                         u\'Steps Down\', u\' Profit warning\', \n                         u\'Store Of The Future\', u\'groceries\']}\n\nkeywords = {u\'sales growth\', u\'next generation store\', \n            u\'Steps Down\', u\' Profit warning\', \n            u\'Store Of The Future\', u\'groceries\'}\n\nparagraph="""HOUSTON -- Target has unveiled its first "next generation" store in the Houston area, part of a multibillion-dollar effort to reimagine more than 1,000 stores nationwide to compete with e-commerce giants.\nThe 124,000-square-foot store, which opened earlier this week at Aliana market center in Richmond, Texas, has two distinct entrances and aims to appeal to consumers on both ends of the shopping spectrum.\nBusy families seeking convenience can enter the "ease" side of the store, which offers a supermarket-style experience. Customers can pick up online orders, both in store and curbside, and buy grab-and-go items like groceries, wine, last-minute gifts, cleaning supplies and prepared meals."""\n\nfrom nltk.tokenize import MWETokenizer\nfrom nltk import sent_tokenize, word_tokenize\n\nmwe = MWETokenizer([k.lower().split() for k in keywords], separator=\'_\')\n\n# Clean out the punctuations in your sentence.\nimport string\npuncts = list(string.punctuation)\ncleaned_paragraph = \'\'.join([ch if ch not in puncts else \'\' for ch in paragraph.lower()])\n\ntokenized_paragraph = [token for token in mwe.tokenize(word_tokenize(cleaned_paragraph))\n                       if token.replace(\'_\', \' \') in keywords]\n\nprint(tokenized_paragraph)\n\n&gt;&gt;&gt; print(tokenized_paragraph)\n[\'next_generation_store\', \'groceries\'\n'
'Y = df["Response"]\nX=...\npipeline = Pipeline([(\'scaling\', StandardScaler()),\n                     (\'pca\', PCA(n_components=20, whiten=True)),\n                      (\'regr\',LinearRegression())])\nnewDF = pipeline.fit_predict(numericDF)\n'
"Data['Married'].fillna(Data['Married'].mode(), inplace=True)\n\nData['Married'].fillna(Data['Married'].value_counts().index[0], inplace=True)\n"
"RandomForest_model = RandomForestRegressor(n_estimators=300,n_jobs=-1)\nRandomForest_model.fit(train_x,train_y)\n\nimportance_df=pd.DataFrame({'feature':train_x.columns, 'importance':RandomForest_model.feature_importances_})\n#sort feature importance data frame\nimportance_df.sort_values('importance', ascending=False, inplace=True)\n#select 100 most important features\nfeatures= importance_df.feature[:100]\n#\ntrain_100_x= train_x.loc[:,features]\ntest_100_x = test_x.loc[:,features]\n"
"model_top.add(keras.layers.Dense(1, activation = 'sigmoid'))\n"
'X= scale(trade_data)\n\nlog_prediction = LogReg.predict_log_proba(\n[\n   [2, 14],[3,1], [1, 503],[1, 122],[1, 101],[1, 610],[1, 2120],[3, 85],[3, 91],[2, 167],[2, 553],[2, 144]\n])\n\nX = scale(trade_data)\n\nscaler = StandardScaler().fit(trade_date)\nX = scaler.transform(trade_data)\n\nscaled_test = scaler.transform(test_x)\n'
'for feature_colunm_name in feature_columns_to_use:\n    X_train[feature_colunm_name] = CountVectorizer().fit_transform(X_train[feature_colunm_name]).todense()\n    X_test[feature_colunm_name] = CountVectorizer().fit_transform(X_test[feature_colunm_name]).todense()\n\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train[feature_colunm_name])\nX_test_cv = cv.transform(X_test[feature_colunm_name])\n\n# To merge sparse matrices\nfrom scipy.sparse import hstack\n\nresult_matrix_train = None\nresult_matrix_test = None\n\nfor feature_colunm_name in feature_columns_to_use:\n    cv = CountVectorizer()\n    X_train_cv = cv.fit_transform(X_train[feature_colunm_name])\n\n    # Merge the vector with others\n    result_matrix_train = hstack((result_matrix_train, X_train_cv)) \n                          if result_matrix_train is not None else X_train_cv\n\n    # Now transform the test data\n    X_test_cv = cv.transform(X_test[feature_colunm_name])\n    result_matrix_test = hstack((result_matrix_test, X_test_cv)) \n                         if result_matrix_test is not None else X_test_cv\n\nresult_matrix_train = hstack((result_matrix_test, X_train[other_columns].values)) \nresult_matrix_test = hstack((result_matrix_test, X_test[other_columns].values)) \n\n...\ngrd.fit(result_matrix_train, y_train.values)\n'
'Titanic_train = pd.read_csv(\'train.csv\').values\nTitanic_test = pd.read_csv(\'test.csv\').values\n\ncolumns = [\'PassengerId\', \'Survived\', \'Pclass\', \'Name\', \'Sex\', \'Age\', \'SibSp\', \'Parch\', \'Ticket\', \'Fare\', \'Cabin\', \'Embarked\']\nTitanic_train = pd.DataFrame(Titanic_train, columns = columns )\n\n\n#splitting the training data into dependent and independent variable\nX = Titanic_train.loc[:,[\'Pclass\', \'Sex\',\'Age\',\'SibSp\',\'Parch\',\'Fare\']].values\nY = Titanic_train.loc[:, \'Survived\'].values\n\nX = pd.DataFrame(Titanic_train, columns = [\'Pclass\', \'Sex\',\'Age\',\'SibSp\',\'Parch\',\'Fare\'])\nY = pd.DataFrame(Titanic_train, columns = [\'Survived\'])\nY = Y.Survived.astype("float")\n\n#working with missing data\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = \'NaN\', strategy = \'mean\', axis = 0)\nimputer = imputer.fit(X[[\'Age\']])\nX[[\'Age\']] = imputer.transform(X[[\'Age\']])  \n\n#dealing with categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nLabelEncoder_X = LabelEncoder()\nX[\'Sex\'] = LabelEncoder_X.fit_transform(X[\'Sex\'])\n\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, y_test = train_test_split(X,Y,test_size = 0.4, random_state = 0)\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, Y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\nY = Y.Survived.astype("float")\n'
"from google.oauth2 import service_account\n\ncredentials = service_account.Credentials.from_service_account_file(\n'cred.json')\n"
"from keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, merge, concatenate, Dense, LSTM, CuDNNLSTM\nfrom keras.engine.topology import Layer\nfrom keras import backend as K\nimport tensorflow_probability as tfp\nimport tensorflow as tf\n\n# check tfp version, as tfp causes cryptic error if out of date\nassert float(tfp.__version__.split('.')[1]) &gt;= 5\n\nclass MDN(Layer):\n  '''Mixture Density Network with unigaussian kernel'''\n  def __init__(self, n_mixes, output_dim, **kwargs):\n    self.n_mixes = n_mixes\n    self.output_dim = output_dim\n\n    with tf.name_scope('MDN'):\n      self.mdn_mus    = Dense(self.n_mixes * self.output_dim, name='mdn_mus')\n      self.mdn_sigmas = Dense(self.n_mixes, activation=K.exp, name='mdn_sigmas')\n      self.mdn_alphas = Dense(self.n_mixes, activation=K.softmax, name='mdn_alphas')\n    super(MDN, self).__init__(**kwargs)\n\n  def build(self, input_shape):\n    self.mdn_mus.build(input_shape)\n    self.mdn_sigmas.build(input_shape)\n    self.mdn_alphas.build(input_shape)\n    self.trainable_weights = self.mdn_mus.trainable_weights + \\\n      self.mdn_sigmas.trainable_weights + \\\n      self.mdn_alphas.trainable_weights\n    self.non_trainable_weights = self.mdn_mus.non_trainable_weights + \\\n      self.mdn_sigmas.non_trainable_weights + \\\n      self.mdn_alphas.non_trainable_weights\n    self.built = True\n\n  def call(self, x, mask=None):\n    with tf.name_scope('MDN'):\n      mdn_out = concatenate([\n        self.mdn_mus(x),\n        self.mdn_sigmas(x),\n        self.mdn_alphas(x)\n      ], name='mdn_outputs')\n    return mdn_out\n\n  def get_output_shape_for(self, input_shape):\n    return (input_shape[0], self.output_dim)\n\n  def get_config(self):\n    config = {\n      'output_dim': self.output_dim,\n      'n_mixes': self.n_mixes,\n    }\n    base_config = super(MDN, self).get_config()\n    return dict(list(base_config.items()) + list(config.items()))\n\n  def get_loss_func(self):\n    def unigaussian_loss(y_true, y_pred):\n      mix = tf.range(start = 0, limit = self.n_mixes)\n      out_mu, out_sigma, out_alphas = tf.split(y_pred, num_or_size_splits=[\n        self.n_mixes * self.output_dim,\n        self.n_mixes,\n        self.n_mixes,\n      ], axis=-1, name='mdn_coef_split')\n\n      def loss_i(i):\n        batch_size = tf.shape(out_sigma)[0]\n        sigma_i = tf.slice(out_sigma, [0, i], [batch_size, 1], name='mdn_sigma_slice')\n        alpha_i = tf.slice(out_alphas, [0, i], [batch_size, 1], name='mdn_alpha_slice')\n        mu_i = tf.slice(out_mu, [0, i * self.output_dim], [batch_size, self.output_dim], name='mdn_mu_slice')\n        dist = tfp.distributions.Normal(loc=mu_i, scale=sigma_i)\n        loss = dist.prob(y_true) # find the pdf around each value in y_true\n        loss = alpha_i * loss\n        return loss\n\n      result = tf.map_fn(lambda  m: loss_i(m), mix, dtype=tf.float32, name='mix_map_fn')\n      result = tf.reduce_sum(result, axis=0, keepdims=False)\n      result = -tf.log(result)\n      result = tf.reduce_mean(result)\n      return result\n\n    with tf.name_scope('MDNLayer'):\n      return unigaussian_loss\n\nclass LSTM_MDN:\n  def __init__(self, n_verts=15, n_dims=3, n_mixes=2, look_back=1, cells=[32,32,32,32], use_mdn=True):\n    self.n_verts = n_verts\n    self.n_dims = n_dims\n    self.n_mixes = n_mixes\n    self.look_back = look_back\n    self.cells = cells\n    self.use_mdn = use_mdn\n    self.LSTM = CuDNNLSTM if len(gpus) &gt; 0 else LSTM\n    self.model = self.build_model()\n    if use_mdn:\n      self.model.compile(loss=MDN(n_mixes, n_verts*n_dims).get_loss_func(), optimizer='adam', metrics=['accuracy'])\n    else:\n      self.model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n\n  def build_model(self):\n    i = Input((self.look_back, self.n_verts*self.n_dims))\n    h = self.LSTM(self.cells[0], return_sequences=True)(i) # return sequences, stateful\n    h = self.LSTM(self.cells[1], return_sequences=True)(h)\n    h = self.LSTM(self.cells[2])(h)\n    h = Dense(self.cells[3])(h)\n    if self.use_mdn:\n      o = MDN(self.n_mixes, self.n_verts*self.n_dims)(h)\n    else:\n      o = Dense(self.n_verts*self.n_dims)(h)\n    return Model(inputs=[i], outputs=[o])\n\n  def prepare_inputs(self, X, look_back=2):\n    '''\n    Prepare inputs in shape expected by LSTM\n    @returns:\n      numpy.ndarray train_X: has shape: n_samples, lookback, verts * dims\n      numpy.ndarray train_Y: has shape: n_samples, verts * dims\n    '''\n    # prepare data for the LSTM_MDN\n    X = X.swapaxes(0, 1) # reshape to time, vert, dim\n    n_time, n_verts, n_dims = X.shape\n\n    # validate shape attributes\n    if n_verts != self.n_verts: raise Exception(' ! got', n_verts, 'vertices, expected', self.n_verts)\n    if n_dims != self.n_dims: raise Exception(' ! got', n_dims, 'dims, expected', self.n_dims)\n    if look_back != self.look_back: raise Exception(' ! got', look_back, 'for look_back, expected', self.look_back)\n\n    # lstm expects data in shape [samples_in_batch, timestamps, values]\n    train_X = []\n    train_Y = []\n    for i in range(look_back, n_time, 1):\n      train_X.append( X[i-look_back:i,:,:].reshape(look_back, n_verts * n_dims) ) # look_back, verts * dims\n      train_Y.append( X[i,:,:].reshape(n_verts * n_dims) ) # verts * dims\n    train_X = np.array(train_X) # n_samples, lookback, verts * dims\n    train_Y = np.array(train_Y) # n_samples, verts * dims\n    return [train_X, train_Y]\n\n  def predict_positions(self, input_X):\n    '''\n    Predict the output for a series of input frames. Each prediction has shape (1, y), where y contains:\n      mus = y[:n_mixes*n_verts*n_dims]\n      sigs = y[n_mixes*n_verts*n_dims:-n_mixes]\n      alphas = softmax(y[-n_mixes:])\n    @param numpy.ndarray input_X: has shape: n_samples, look_back, n_verts * n_dims\n    @returns:\n      numpy.ndarray X: has shape: verts, time, dims\n    '''\n    predictions = []\n    for i in range(input_X.shape[0]):\n      y = self.model.predict( train_X[i:i+1] ).squeeze()\n      mus = y[:n_mixes*n_verts*n_dims]\n      sigs = y[n_mixes*n_verts*n_dims:-n_mixes]\n      alphas = self.softmax(y[-n_mixes:])\n\n      # find the most likely distribution then pull out the mus that correspond to that selected index\n      alpha_idx = np.argmax(alphas) # 0\n      alpha_idx = 0\n      predictions.append( mus[alpha_idx*self.n_verts*self.n_dims:(alpha_idx+1)*self.n_verts*self.n_dims] )\n    predictions = np.array(predictions).reshape(train_X.shape[0], self.n_verts, self.n_dims).swapaxes(0, 1)\n    return predictions # shape = n_verts, n_time, n_dims\n\n  def softmax(self, x):\n    ''''Compute softmax values for vector `x`'''\n    r = np.exp(x - np.max(x))\n    return r / r.sum()\n\nX = data.selected.X\nn_verts, n_time, n_dims = X.shape\nn_mixes = 3\nlook_back = 2\n\nlstm_mdn = LSTM_MDN(n_verts=n_verts, n_dims=n_dims, n_mixes=n_mixes, look_back=look_back)\ntrain_X, train_Y = lstm_mdn.prepare_inputs(X, look_back=look_back)\n"
'inp = Input(shape=(num_words_per_sample,))\nx = Embedding(vocab_size, emb_dim)(inp)\n\nimport numpy as np\ntexts = np.array(texts)\nprint(texts.shape)  # (3,5)  &lt;--- three samples each containing 5 words\n\nlabels = np.array(labels)\nprint(labels.shape) # (3,)   &lt;--- three labels, one for each sample\n'
"shap_values = None\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\n(X_train, X_test, y_train, y_test) = train_test_split(df[feat], df['target'].values, \n                                     test_size=0.2, shuffle  = True,stratify =df['target'].values,\n                                     random_state=42) \n\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfolds_idx = [(train_idx, val_idx) \n                 for train_idx, val_idx in folds.split(X_train, y=y_train)]\nauc_scores = []\noof_preds = np.zeros(df[feat].shape[0])\ntest_preds = []\n\nfor n_fold, (train_idx, valid_idx) in enumerate(folds_idx):\n    train_x, train_y = df[feat].iloc[train_idx], df['target'].iloc[train_idx]\n    valid_x, valid_y = df[feat].iloc[valid_idx], df['target'].iloc[valid_idx]    \n    clf = lgb.LGBMClassifier(nthread=4,            boosting_type= 'gbdt', is_unbalance= True,random_state = 42,\n            learning_rate= 0.05, max_depth= 3,\n            reg_lambda=0.1 , reg_alpha= 0.01,min_child_samples= 21,subsample_for_bin= 5000,\n            metric= 'auc', n_estimators= 5000    )\n    clf.fit(train_x, train_y, \n            eval_set=[(train_x, train_y), (valid_x, valid_y)], \n            eval_metric='auc', verbose=False, early_stopping_rounds=100)\n    explainer = shap.TreeExplainer(clf)\n    if shap_values is None:\n        shap_values = explainer.shap_values(X_test)\n    else:\n        shap_values += explainer.shap_values(X_test)       \n    oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]   \n    auc_scores.append(roc_auc_score(valid_y, oof_preds[valid_idx]))\nprint( 'AUC: ', np.mean(auc_scores))\nshap_values /= 10 # number of folds\nshap.summary_plot(shap_values, X_test)\n"
'from scipy.optimize import minimize\nresults = minimize(cost, b, args = (x,y),\n                   method = \'CG\', jac = compute_gradient, \n                   options = {"maxiter": 400, "disp" : True})\n'
'r,c = np.triu_indices(d,1)\nres = X[r]*X[c]\n\nn = d-1\nidx = np.concatenate(( [0], np.arange(n,0,-1).cumsum() ))\nstart, stop = idx[:-1], idx[1:]\nL = n*(n+1)//2\nres_out = np.empty((L,X.shape[1]), dtype=X.dtype)\nfor i,(s0,s1) in enumerate(zip(start,stop)):\n    res_out[s0:s1] = X[i] * X[i+1:]\n\nn = d-1\nN = len(X)\nidx = 2*N + np.concatenate(( [0], np.arange(n,0,-1).cumsum() ))\nstart, stop = idx[:-1], idx[1:]\nL = n*(n+1)//2\nZ_out = np.empty((2*N + L,X.shape[1]), dtype=X.dtype)\nZ_out[:N] = X\nZ_out[N:2*N] = X**2\nfor i,(s0,s1) in enumerate(zip(start,stop)):\n    Z_out[s0:s1] = X[i] * X[i+1:]\n'
'RNN(\n  (dense1): Linear(in_features=12, out_features=100, bias=True)\n  (dense2): Linear(in_features=100, out_features=100, bias=False)\n  (dense3): Linear(in_features=12, out_features=100, bias=True)\n  (dense4): Linear(in_features=100, out_features=100, bias=False)\n  (dense5): Linear(in_features=100, out_features=6, bias=True)\n)\n\ndef forward(self, input):\n    return F.linear(input, self.weight, self.bias)\n\noutput = input.matmul(weight.t())\n\n# dummy input of length 5\ninput = torch.rand(5, 12)\n# apply layer dense1 (without bias, for bias just add + model.dense1.bias)\noutput_first_layer = input.matmul(model.dense1.weight.t())\nprint(output_first_layer.shape)\n\ntorch.Size([5, 100])\n'
"x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # output of conv layers\nx = F.interpolate(x, size=(5, 5), mode='bilinear')  # resize to the size expected by the linear unit\nx = x.view(x.size(0), 5 * 5 * 16)\nx = F.relu(self.fc1(x))  # you can go on from here...\n"
'# broadcasting will copy the input to every output class neuron\ninput_dense = tf.expand_dims(dense1, axis=2)\n\n# broadcasting here will copy the weights across the batch\nweights = tf.expand_dims(tf.transpose(dense2_W), axis=0)\n\ndense2 = tf.square(tf.norm(input_dense - weights, axis=1))\n'
"#Constructing the tree\nimport cv2\ndtree=cv.ml.DTrees_create()\n\n&gt;&gt;&gt; dir(cv2.ml)\n['ANN_MLP_ANNEAL', 'ANN_MLP_BACKPROP', 'ANN_MLP_GAUSSIAN', 'ANN_MLP_IDENTITY', 'ANN_MLP_LEAKYRELU', 'ANN_MLP_NO_INPUT_SCALE', 'ANN_MLP_NO_OUTPUT_SCALE', 'ANN_MLP_RELU', 'ANN_MLP_RPROP', 'ANN_MLP_SIGMOID_SYM', 'ANN_MLP_UPDATE_WEIGHTS', 'ANN_MLP_create', 'ANN_MLP_load', 'BOOST_DISCRETE', 'BOOST_GENTLE', 'BOOST_LOGIT', 'BOOST_REAL', 'Boost_DISCRETE', 'Boost_GENTLE', 'Boost_LOGIT', 'Boost_REAL', 'Boost_create', 'Boost_load', 'COL_SAMPLE', 'DTREES_PREDICT_AUTO', 'DTREES_PREDICT_MASK', 'DTREES_PREDICT_MAX_VOTE', 'DTREES_PREDICT_SUM', 'DTrees_PREDICT_AUTO', 'DTrees_PREDICT_MASK', 'DTrees_PREDICT_MAX_VOTE', 'DTrees_PREDICT_SUM', 'DTrees_create', 'DTrees_load', 'EM_COV_MAT_DEFAULT', 'EM_COV_MAT_DIAGONAL', 'EM_COV_MAT_GENERIC', 'EM_COV_MAT_SPHERICAL', 'EM_DEFAULT_MAX_ITERS', 'EM_DEFAULT_NCLUSTERS', 'EM_START_AUTO_STEP', 'EM_START_E_STEP', 'EM_START_M_STEP', 'EM_create', 'EM_load', 'KNEAREST_BRUTE_FORCE', 'KNEAREST_KDTREE', 'KNearest_BRUTE_FORCE', 'KNearest_KDTREE', 'KNearest_create', 'LOGISTIC_REGRESSION_BATCH', 'LOGISTIC_REGRESSION_MINI_BATCH', 'LOGISTIC_REGRESSION_REG_DISABLE', 'LOGISTIC_REGRESSION_REG_L1', 'LOGISTIC_REGRESSION_REG_L2', 'LogisticRegression_BATCH', 'LogisticRegression_MINI_BATCH', 'LogisticRegression_REG_DISABLE', 'LogisticRegression_REG_L1', 'LogisticRegression_REG_L2', 'LogisticRegression_create', 'LogisticRegression_load', 'NormalBayesClassifier_create', 'NormalBayesClassifier_load', 'ParamGrid_create', 'ROW_SAMPLE', 'RTrees_create', 'RTrees_load', 'STAT_MODEL_COMPRESSED_INPUT', 'STAT_MODEL_PREPROCESSED_INPUT', 'STAT_MODEL_RAW_OUTPUT', 'STAT_MODEL_UPDATE_MODEL', 'SVMSGD_ASGD', 'SVMSGD_HARD_MARGIN', 'SVMSGD_SGD', 'SVMSGD_SOFT_MARGIN', 'SVMSGD_create', 'SVMSGD_load', 'SVM_C', 'SVM_CHI2', 'SVM_COEF', 'SVM_CUSTOM', 'SVM_C_SVC', 'SVM_DEGREE', 'SVM_EPS_SVR', 'SVM_GAMMA', 'SVM_INTER', 'SVM_LINEAR', 'SVM_NU', 'SVM_NU_SVC', 'SVM_NU_SVR', 'SVM_ONE_CLASS', 'SVM_P', 'SVM_POLY', 'SVM_RBF', 'SVM_SIGMOID', 'SVM_create', 'SVM_getDefaultGridPtr', 'SVM_load', 'StatModel_COMPRESSED_INPUT', 'StatModel_PREPROCESSED_INPUT', 'StatModel_RAW_OUTPUT', 'StatModel_UPDATE_MODEL', 'TEST_ERROR', 'TRAIN_ERROR', 'TrainData_create', 'TrainData_getSubVector', 'VAR_CATEGORICAL', 'VAR_NUMERICAL', 'VAR_ORDERED', '__doc__', '__loader__', '__name__', '__package__', '__spec__']\n"
"import pandas as pd\nimport numpy as np\n\nx1 = np.random.rand(100,1)\nx2 = np.random.rand(100,1)\nx3 = np.random.rand(100,1)\ny = 2 + 10 * x1 + 5 * x2 + 4 * x3 + 2 * np.random.randn(100,1)\ndf = pd.DataFrame(np.c_[y,x1,x2,x3],columns=['y','x1','x2','x3'])\n\n#df.head()\n#            y        x1        x2        x3\n# 0  11.970573  0.785165  0.012989  0.634274\n# 1  19.980349  0.919672  0.971063  0.752341\n# 2   2.884538  0.170164  0.991058  0.003270\n# 3   8.437686  0.474261  0.326746  0.653011\n# 4  14.026173  0.509091  0.921010  0.375524\n\n(m1, m2, m3, b) = (0.001, 0.001, 0.001, 0.002)\n\n(m1, m2, m3, b) = (9.382, 4.841, 4.117, 2.485)\n"
"trainX = trainX.reshape( 43164,1, 17)\ntrainY = trainY.reshape(43164, 1)\n\nmodel = Sequential()\nmodel.add(LSTM(2, input_shape=(1, 17)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY[0], epochs=100)\n\ntestX.shape # (8633, 17)\ntestX = testX.reshape(8633,1, 17)\n"
'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.\n\nParameters: \nX : array-like, shape = (n_samples, n_features)\nTest samples.\n\ny : array-like, shape = (n_samples) or (n_samples, n_outputs)\nTrue labels for X.\n\nsample_weight : array-like, shape = [n_samples], optional\nSample weights.\n\nReturns:    \nscore : float\nMean accuracy of self.predict(X) wrt. y\n\nprint classifier.score(X_test, np.array(y_test))\n'
'yk_grd_probs = k_grd.predict_proba(X_test)\nprint(log_loss(y_test, yk_grd_probs))\n'
'# Predicting the Test set results\ny_pred = model.predict(X_test)\ny_pred = (y_pred &gt; 0.5)\nmatrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n\n[0.87812372 0.77490434 0.30319547 0.84999743]\n\nClassification metrics can\'t handle a mix of binary and continuous target\n\nfrom keras import models\nfrom keras.layers import Dense, Dropout\nfrom keras.utils import to_categorical\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.cross_validation import  train_test_split\nfrom sklearn import metrics\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n\n\n# read the csv file and convert into arrays for the machine to process\ndf = pd.read_csv(\'dataset_ori.csv\')\ndataset = df.values\n\n# split the dataset into input features and the feature to predict\nX = dataset[:,0:7]\nY = dataset[:,7]\n\n# Splitting into Train and Test Set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset,\n                                                    response,\n                                                    test_size = 0.2,\n                                                    random_state = 0)\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 10, kernel_initializer = \'uniform\', activation = \'relu\', input_dim =7 ))\nmodel.add(Dropout(0.5))\n# Adding the second hidden layer\nclassifier.add(Dense(units = 10, kernel_initializer = \'uniform\', activation = \'relu\'))\nmodel.add(Dropout(0.5))\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = \'uniform\', activation = \'sigmoid\'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = \'adam\', loss = \'sparse_categorical_crossentropy\', metrics = [\'accuracy\'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 20)\n\n# Train model\nscaler = StandardScaler()\nclassifier.fit(scaler.fit_transform(X_train.values), y_train)\n\n# Summary of neural network\nclassifier.summary()\n\n# Predicting the Test set results &amp; Giving a threshold probability\ny_prediction = classifier.predict_classes(scaler.transform(X_test.values))\nprint ("\\n\\naccuracy" , np.sum(y_prediction == y_test) / float(len(y_test)))\ny_prediction = (y_prediction &gt; 0.5)\n\n\n\n\n## EXTRA: Confusion Matrix Visualize\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm = confusion_matrix(y_test, y_pred) # rows = truth, cols = prediction\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsn.set(font_scale=1.4)\nsn.heatmap(df_cm, annot=True, fmt=\'g\')\nprint("Test Data Accuracy: %0.4f" % accuracy_score(y_test, y_pred))\n\n#Let\'s see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\n'
"import tensorflow as tf\nimport numpy as np\n\ndef basic_dataset(numPoints):\n    data = np.linspace(0,1,numPoints)\n    dataset = dict({'points': data})\n    labels = []\n    for d in data:\n        labels.append(1)\n    return tf.data.Dataset.from_tensor_slices((dataset, np.array(labels)))\n\ndef input_train_set():\n    dataset = basic_dataset(11)\n    return dataset.repeat(100).shuffle(1000).batch(1)\n\npoint = tf.feature_column.numeric_column('points')\nestimator = tf.estimator.DNNRegressor(feature_columns = [point],hidden_units = [100,100,100], label_dimension = 1)\n\nestimator.train(input_fn = input_train_set)\n"
"'one hot encoding' -&gt; [0, 2, 17]\n"
"onehot.get_feature_names(input_features=df.columns)\n\narray(['a_c1', 'a_c2', 'a_c3', 'b_c1', 'b_c4'], dtype=object)\n"
'sp = spotipy.Spotify()\n'
'from sklearn.datasets import make_friedman1\nfrom sklearn.feature_selection import RFE \nfrom sklearn.svm import SVR \n\nX, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\nestimator = SVR(kernel="linear")\nselector = RFE(estimator, 5, step=1)\nselector = selector.fit(X, y)\n'
"sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n                                                    shuffle=True, \n                                stratify = X['YOUR_COLUMN_LABEL'])\n"
"start=10\nstop=20\nstep = 5\n\nparam_grid = {'hidden_layer_sizes': \n               [(n, min(n+step, stop)) for n in range(start, stop, step)]}\n\nparam_grid\nOut[29]: {'hidden_layer_sizes': [(10, 15), (15, 20)]}\n"
'0 0 0 0 0\n0 0 0 0 0\n0 0 1 0 0\n0 0 0 0 0\n0 0 0 0 0\n'
'credit_test["Class"].value_counts()\n\n1    138\n0     54\n\n[clf.intercept_,clf.coef_]\n[array([0.59140229]), array([[0.9820343]])]\n\nclf = LogisticRegression(random_state=0,fit_intercept=False).fit(X, y)\ny_pred=clf.predict(credit_test[[\'CreditHistory.Critical\']])\nconfusion_matrix(y_pred=y_pred, y_true=y_test)\n\narray([[42, 12],\n       [84, 54]])\n'
"feature_matrix, feature_defs = ft.dfs(\n    entityset=es,\n    target_entity='data',\n    trans_primitives=['add_numeric', 'multiply_numeric'],\n    ignore_variables={'data': ['species']},\n)\n"
'import pickle\n\ndef save_model(clf, filename):\n    with open(filename, \'wb\') as f:\n        pickle.dump(clf, f)\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n\n    # fit the data and tag outliers\n    if clf_name == "Local Outlier Factor":\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n        save_model(clf, \'Outlier.pkl\')  # Saving the LOF\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        save_model(clf, \'Isolation.pkl\')  # Saving the isolation forest\n\n    ...\n\ndef load_model(filename):\n    with open(filename, \'rb\') as f:\n        clf = pickle.load(f)\n    return clf\n'
"def perplexity(y_true, y_pred):\n    cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n    perplexity = K.exp(cross_entropy)\n    return perplexity\n\nvocab_size = 10\nX = np.random.uniform(0,1, (1000,10))\ny = np.random.randint(0,vocab_size, 1000)\n\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=(10)))\nmodel.add(Dense(vocab_size, activation='softmax'))\n# compile network\nmodel.compile(loss=perplexity, optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(X, y, epochs=10, verbose=2)\n\ndef perplexity(y_true, y_pred):\n    cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n    perplexity = K.exp(cross_entropy)\n    return perplexity\n\nvocab_size = 10\nX = np.random.uniform(0,1, (1000,10))\ny = pd.get_dummies(np.random.randint(0,vocab_size, 1000)).values # one-hot\n\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_dim=(10)))\nmodel.add(Dense(vocab_size, activation='softmax'))\n# compile network\nmodel.compile(loss=perplexity, optimizer='adam', metrics=['accuracy'])\n# fit network\nmodel.fit(X, y, epochs=10, verbose=2)\n"
'model = Sequential()\nmodel.add(LSTM(32, activation=\'tanh\', return_sequences=True, input_shape=(3, 19))),\nmodel.add(TimeDistributed(Dense(32, activation=\'relu\')))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dense(4, activation=\'sigmoid\'))\n\nmodel.summary()\n\nModel: "sequential_9"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_9 (LSTM)                (None, 3, 32)             6656      \n_________________________________________________________________\ntime_distributed_4 (TimeDist (None, 3, 32)             1056      \n_________________________________________________________________\nglobal_average_pooling1d_2 ( (None, 32)                0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 4)                 132       \n=================================================================\nTotal params: 7,844\nTrainable params: 7,844\nNon-trainable params: 0\n'
"import numpy as np\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import optimizers\nfrom sklearn.model_selection import GridSearchCV\n\n\n#Creating the neural network\ndef create_model(learn_rate, batch_size, epochs):\n    model=Sequential()\n\n    model.add(Dense(512, activation='relu',input_shape=(2606,)))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1, activation='relu'))\n\n    opt=optimizers.Adam(lr=learn_rate)\n    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n\n    #I commented this out because I believe it is delegated to the grid.fit() fn later on.\n    #model.fit(X_train, Y_train, batch_size=30, epochs=6000, verbose=1)\n\n    return model\n\n#Now setting up the grid search\nX_train = np.empty((167,2606), dtype=float, order='C')\nY_train = np.empty((167,), dtype=float, order='C')\n\nlearn_rate=np.arange(.00001,.001,.00002).tolist()\nbatch_size=np.arange(10,2606,2).tolist()\nepochs=np.arange(1000,10000,100).tolist()\n\nparam_grid=dict(learn_rate=learn_rate, batch_size=batch_size, epochs=epochs)\n\ngrid = GridSearchCV(estimator=KerasClassifier(build_fn=create_model), \nparam_grid=param_grid, n_jobs=-1, cv=3)\n\ngrid_results=grid.fit(X_train,Y_train) #This is the line referenced in the error message.\n\nprint(&quot;Best: %f using %s&quot; % (grid_result.best_score_, grid_result.best_params_))\n"
"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (12.0, 9.0)\n\n# Preprocessing Input data\ndata = pd.read_csv('gre.csv')\nX = data.iloc[:, 1]\nY = data.iloc[:, 8]\n\nm = 0\nc = 0\nL = 0.0000001  # The learning Rate\nepochs = 100  # The number of iterations to perform gradient descent\nn = float(len(X))  # Number of elements in X\n\n# Performing Gradient Descent\nfor i in range(epochs):    \n    Y_pred = m*X + c  # The current predicted value of Y\n    D_m = (-2/n) * sum(X * (Y - Y_pred))  # Derivative wrt m\n    D_c = (-2/n) * sum(Y - Y_pred)  # Derivative wrt c\n    m = m - L * D_m  # Update m\n    c = c - L * D_c  # Update c    \n\nprint(&quot;Slope, Intercept:&quot;, m, c)\n\nplt.xlabel('GRE score')\nplt.ylabel('Chance of getting into university %')\naxes = plt.gca()\nY_preds = c + m * X\nplt.scatter(X, Y)\nplt.plot(X, Y_preds, '--')\nplt.show()\n\nSlope, Intercept: 0.0019885000304672488 6.212311206699001e-06\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ndata = pd.read_csv('gre.csv')\n\nX, y = data.iloc[:, 1], data.iloc[:, 8].values\nX = X.values.reshape(-1, 1)\n\nregr = linear_model.LinearRegression()\nregr.fit(X, y)\n\ny_pred = regr.predict(X)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n\n# The mean squared error\nprint('Mean squared error: %.2f' % mean_squared_error(y, y_pred))\n\n# The coefficient of determination: 1 is perfect prediction\nprint('Coefficient of determination: %.2f' % r2_score(y, y_pred))\n\n# Plot outputs\nplt.rcParams['figure.figsize'] = (12.0, 9.0)\nplt.xlabel('GRE score')\nplt.ylabel('Chance of getting into university %')\nplt.scatter(X, y,  color='black')\nplt.plot(X, y_pred, color='blue', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\nCoefficients: \n [0.01012587]\nMean squared error: 0.01\nCoefficient of determination: 0.66\n"
"image = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_rows, img_cols))\n\nimport numpy as np\n\ninput_arr = tf.keras.preprocessing.image.img_to_array(image)\ninput_arr = np.array([input_arr])  # Convert single image to a batch.\n\ninput_arr = input_arr.astype('float32') / 255.  # This is VERY important\n\npredictions = model.predict(input_arr)\n\npredicted_class = np.argmax(predictions, axis=-1)\n"
"(n_samples, time_steps, features)\n\n(n_samples, features)\n\n(40101, 1, 3)\n\nx_data = np.random.rand(40101, 1, 3)\ny_data = np.random.rand(40101, 3)\n\ninput_shape=(1, 3)\n\nimport numpy as np\nfrom sklearn.model_selection import TimeSeriesSplit\nimport tensorflow as tf\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras.layers import *\n\nx_data = np.random.rand(40101, 1, 3)\ny_data = np.random.rand(40101, 3)\n\ntimeSeriesCrossValidation = TimeSeriesSplit(n_splits=5)\nfor train, validation in timeSeriesCrossValidation.split(x_data, y_data):\n    # create model\n    model = tf.keras.models.Sequential()\n\n    # input layer\n    model.add(LSTM(units=5,\n                   input_shape=(1, 3),\n                   dropout=0.01,\n                   recurrent_dropout=0.2,\n                   kernel_initializer=initializers.RandomNormal(mean=0, stddev=.5),\n                   bias_initializer=initializers.Zeros(),\n                   return_sequences=True))\n\n    # 1st hidden layer\n    model.add(LSTM(units=5,\n                   dropout=0.01,\n                   recurrent_dropout=0.2,\n                   kernel_initializer=initializers.RandomNormal(mean=0, stddev=.5),\n                   bias_initializer=initializers.Zeros(),\n                   return_sequences=True))\n\n    # 2nd hidder layer\n    model.add(LSTM(units=50,\n                   dropout=0.01,\n                   recurrent_dropout=0.2,\n                   kernel_initializer=initializers.RandomNormal(mean=0, stddev=.5),\n                   bias_initializer=initializers.Zeros(),\n                   return_sequences=False))\n\n    # output layer\n    model.add(tf.keras.layers.Dense(3))\n\n    model.compile(loss='mse', optimizer='nadam', metrics=['accuracy'])\n\n    model.fit(x_data[train], y_data[train],\n              verbose=2,\n              batch_size=None,\n              epochs=1,\n              validation_data=(x_data[validation], y_data[validation])\n              # callbacks=early_stop\n              )\n\n    prediction = model.predict(x_data[validation])\n    y_validation = y_data[validation]\n\ndef multivariate_data(dataset, target, start_index, end_index, history_size,\n                      target_size, step, single_step=False):\n  data = []\n  labels = []\n\n  start_index = start_index + history_size\n  if end_index is None:\n    end_index = len(dataset) - target_size\n\n  for i in range(start_index, end_index):\n    indices = range(i-history_size, i, step)\n    data.append(dataset[indices])\n\n    if single_step:\n      labels.append(target[i+target_size])\n    else:\n      labels.append(target[i:i+target_size])\n\n  return np.array(data), np.array(labels)\n\nmultivariate_data(dataset=np.random.rand(40101, 3), \n                  target=np.random.rand(40101, 3), \n                  0, len(x_data), 5, 0, 1, True)[0].shape\n\n(40096, 5, 3)\n"
'y = np.reshape(Output, (X.shape[0], 1))\n\ny = np.reshape(Output, (X.shape[0],))\n\ny = np.array(result)\n\ny = data[&quot;result&quot;].replace({&quot;positive&quot;: 1, &quot;negative&quot;: 0})\n'
"df1 = (df.set_index(['id', 'CID', 'U_lot'])\n         .stack()\n         .loc[lambda x: x!=0]\n         .reset_index(-1)\n         .drop(columns=0)\n         .rename(columns={'level_3': 'Label'}))\n\nidx = df1.set_index('Label', append=True).index\n\ndf1 = (df1.merge(df1, left_index=True, right_index=True, suffixes=['', '_r'])\n          .query('Label != Label_r'))\n\ndf1 = (df1.groupby(['id', 'CID', 'U_lot', 'Label'])\n          .agg(P_lot=('Label_r', list))\n          .reindex(idx)\n          .reset_index())\n\n    id    CID U_lot Label     P_lot\n0    0  A0694     M    P5  [P7, P8]\n1    0  A0694     M    P7  [P5, P8]\n2    0  A0694     M    P8  [P5, P7]\n3    1  A1486     M    P6       NaN\n4    2  C0973     S    P5      [P6]\n5    2  C0973     S    P6      [P5]\n6    3  B4251     D    P7      [P9]\n7    3  B4251     D    P9      [P7]\n8    4  I0041     S    P4  [P7, P8]\n9    4  I0041     S    P7  [P4, P8]\n10   4  I0041     S    P8  [P4, P7]\n11   5  J1102     F    P9       NaN\n"
'new_model = Model(model.input, model.layers[312].output)\nnew_model.predict(...)\n'
"from sklearn.datasets import load_iris\n\nfrom sklearn.model_selection import train_test_split\nimport shap\nimport xgboost\n\nX,y = shap.datasets.iris()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\nmodel = xgboost.train(params={&quot;learning_rate&quot;: 0.01}, \n                      dtrain=xgboost.DMatrix(X_train, label=y_train), \n                      num_boost_round =100)\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test)\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, X_test)\n\ny_pred = model.predict(xgboost.DMatrix(X_test))\nm = (y_pred &lt;= 1.7) &amp; (y_test == 2)\n\nshap.initjs()\nc= explainer.shap_values(X_test[m])\nshap.force_plot(explainer.expected_value, shap_values, X_test[m])\n\nshap.decision_plot(explainer.expected_value, shap_values, \n                   X_test[m], feature_order='hclust', \n                   return_objects=True)\n"
'z = np.abs(stats.zscore(X))\nmask = (z &lt; 3).all(axis=1)\nX = X[mask]\nY = Y[mask]\n'
'ft = Lasso(alpha=0).fit(X, y)\nprint(ft.intercept_)\nft = LinearRegression().fit(X, y)\nprint(ft.intercept_)\n\n-485.3744897927984\n-480.89071679937854 \n'
"from sklearn import svm\nfrom sklearn import datasets\nclf = svm.SVC()\nX, y= datasets.load_iris(return_X_y=True)\nclf.fit(X, y)\n\nimport pickle\ns = pickle.dumps(clf)\nclf2 = pickle.loads(s)\nclf2.predict(X[0:1])\n\ndef predict_svm(to_predict):\n    with open(&quot;'your_svm_model'&quot;,'rb') as f_input:\n        clf = pickle.loads(f_input) # maybe handled with a singleton to reduce loading for multiple predictions\n    return clf.predict(to_predict)\n\nfrom joblib import dump, load\ndump(clf, 'filename.joblib') \n\nclf = load('filename.joblib') \n"
"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import svm\nfrom mlxtend.plotting import plot_decision_regions\n\nplt.figure(figsize=(5,5))\nin_cir = lambda x,y: True if x**2 + y**2 &lt;= 4 else False # Checking if point is in the purple circle\nf = lambda x,e: 1.16*x + 0.1 + e                         # True function\nran = np.arange(-5,6)\nlsp = np.linspace(-5,5,170)                              # X1 axis\nnp.random.seed(69)\ndots = f(lsp,[np.random.normal(0,1.5) for i in lsp])     # X2 axis\nblue_dots, pur_dots, lsp1, lsp2 = [], [], [], []\nfor i, x in zip(dots, lsp):\n  if in_cir(x,i): pur_dots.append(i); lsp2.append(x)     # Getting all purple dots's X1 and X2\n  else: blue_dots.append(i); lsp1.append(x)              # Same for blue ones\n\nx, y = np.array(list(zip(lsp, dots))), np.where(np.array([in_cir(x,i) for x,i in zip(lsp,dots)]), 'p','b')\n\ny[y == 'b'] = 0  # replacing letters with integers as the plot_decision_regions function accepts only integers\ny[y == 'p'] = 1\ny = y.astype(int)\n\nft = svm.SVC(kernel='rbf', C=1).fit(x, y)            # Fitting svc\nplot_decision_regions(X=x,\n                      y=y,\n                      clf=ft,\n                      legend=2)\nplt.show()\n"
'class_accuracies = []\nfor class_ in np.unique(y_true):\n    class_acc = np.mean(y_pred[y_true == class_] == class_)\n    class_acuracies.append(class_acc)\n'
'import numpy as np\n\nsample_num = 100\nx_dim = 10\nx = np.random.rand(sample_num, x_dim)\nw_tar = np.random.rand(x_dim)\nb_tar = np.random.rand(1)[0]\ny = np.matmul(x, np.transpose([w_tar])) + b_tar\nC = 1e-6\n\ndef ridge_regression_GD(x,y,C):\n    x = np.insert(x,0,1,axis=1) # adding a feature 1 to x at beggining nxd+1\n    x_len = len(x[0,:])\n    w = np.zeros(x_len) # d+1\n    t = 0\n    eta = 3e-3\n    summ = np.zeros(x_len)\n    grad = np.zeros(x_len)\n    losses = np.array([0])\n    loss_stry = 0\n\n    for i in range(50):\n        for i in range(len(y)): # here we calculate the summation for all rows for loss and gradient\n            summ = summ + (y[i,] - np.dot(w, x[i,])) * x[i,]\n            loss_stry += (y[i,] - np.dot(w, x[i,]))**2\n            \n        losses = np.insert(losses, len(losses), loss_stry + C * np.dot(w, w))\n        grad = -2 * summ + np.dot(2 * C,w)\n        w -= eta * grad\n\n        eta *= 0.9\n        t += 1\n        summ = np.zeros(1)\n        loss_stry = 0\n\n    return w[1:], w[0], losses\n\nw, b, losses = ridge_regression_GD(x, y, C)\nprint(&quot;losses: &quot;, losses)\nprint(&quot;b: &quot;, b)\nprint(&quot;b_tar: &quot;, b_tar)\nprint(&quot;w: &quot;, w)\nprint(&quot;w_tar&quot;, w_tar)\n\nx_pre = np.random.rand(3, x_dim)\ny_tar = np.matmul(x_pre, np.transpose([w_tar])) + b_tar\ny_pre = np.matmul(x_pre, np.transpose([w])) + b\nprint(&quot;y_pre: &quot;, y_pre)\nprint(&quot;y_tar: &quot;, y_tar)\n\nlosses: [   0 1888 2450 2098 1128  354   59    5    1    1    1    1    1    1\n    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n    1    1    1    1    1    1    1    1    1    1    1    1    1    1\n    1    1    1    1    1    1    1    1    1]\nb:  1.170527138363387\nb_tar:  0.894306608050021\nw:  [0.7625987  0.6027163  0.58350218 0.49854847 0.52451963 0.59963663\n 0.65156702 0.61188389 0.74257133 0.67164963]\nw_tar [0.82757802 0.76593551 0.74074476 0.37049698 0.40177269 0.60734677\n 0.72304859 0.65733725 0.91989305 0.79020028]\ny_pre:  [[3.44989377]\n [4.77838804]\n [3.53541958]]\ny_tar:  [[3.32865041]\n [4.74528037]\n [3.42093559]]\n'
"df=pd.DataFrame({'cat1':['A','N'],'cat2':['C','S']})\n\ndf['cat1'] = df['cat1'].astype('category', categories=['A','N','K','P'])\n# then run the get_dummies\nb=pd.get_dummies(df['cat1'],prefix='cat1').astype('int')\n\ncat1_categories = ['A','N','K','P']\ncat2_categories = ['C','S','T','B']\n\ndf_test=df=pd.DataFrame({'cat1':['A','N',],'cat2':['T','B']})\ndf['cat1'] = df['cat1'].astype('category', categories=cat1_categories)\nc=pd.get_dummies(df['cat1'],prefix='cat1').astype('int')\nprint(c)\n\n   cat1_A  cat1_N  cat1_K  cat1_P\n0       1       0       0       0\n1       0       1       0       0\n"
'from sklearn import linear_model\n\nclf = linear_model.SGDRegressor()\nclf.fit(X, y)\n\nprediction = clf.predict(newCase)\n'
"$ sage -pip install theano\n\nsage: from theano import *\nsage: import theano.tensor as T\nsage: from theano import function\nsage: x = T.dscalar('x')\nsage: y = T.dscalar('y')\nsage: z = x + y\nsage: f = function([x, y], z)\nsage: f(2, 3)\narray(5.0)\nsage: numpy.allclose(f(16.3, 12.1), 28.4)\nTrue\nsage: type(x)\n&lt;class 'theano.tensor.var.TensorVariable'&gt;\n"
"class MyFeatureSelector():\n    def __init__(self, features=5, method='pca'):\n        self.features = features\n        self.method = method\n        self.selector = None\n        self.init_selector()\n\n\n    def init_selector():\n        if self.method == 'pca':\n            self.selector = PCA(n_components=self.features)\n        elif self.method == 'rfe':\n        self.selector = RFE(estimator=LinearRegression(n_jobs=-1),\n                               n_features_to_select=self.features,\n                               step=1)\n\n    def fit(self, X, Y):\n       return self\n\n    def transform(self, X, Y=None):\n        try:\n            if self.features &lt; X.shape[1]:\n                if Y is not None:\n                    self.selector.fit(X, Y)\n                return selector.transform(X)\n        except Exception as err:\n            print('MyFeatureSelector.transform(): {}'.format(err))\n       return X\n\ndef fit_transform(self, X, Y=None):\n    self.fit(X, Y)\n    return self.transform(X, Y)\n"
'predictions = model.predict(images_as_numpy_array)\n\nimage = cv2.imread(imagePath)\nlabel = imagePath.split(os.path.sep)[-1].split(".")[0]\nfeatures = image_to_feature_vector(image)\n'
'dnn_regressor = DNNRegressor(feature_columns=feature_cols, hidden_units=[50, 50], label_dimension=7, model_dir=os.getcwd())\n'
'def makeRand():\n   while True:\n      yield np.random.rand(1)\n'
'from sqlalchemy import create_engine\nimport pymysql\nimport pandas as pd\n\ndb_connection = \'mysql+pymysql://mysql_user:mysql_password@mysql_host/mysql_db\'\nconn = create_engine(db_connection)\n\ndf = pd.read_sql("select * from tab_name", conn)\n'
"class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3, gamma=0.0, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, random_state=None)\n"
"film_list=['title','article_size','producer','release_date','running_time','country','budget']\nflist = [(i,j) for i, j in enumerate(film_list)]\nlabel = [ seq[0] for seq in flist ]\nname = [ seq[1] for seq in flist ]\nprint label \nprint name\n\n&gt;&gt;[0, 1, 2, 3, 4, 5, 6]\n['title', 'article_size', 'producer', 'release_date', 'running_time', 'country', 'budget']\n\nlabels = film_1.keys()\nprint labels\n\n# But the keys are sorted, labels[0] will give you 'producer' instead of 'title':\n&gt;&gt;['producer', 'title', 'country', 'release_date', 'budget', 'article_size', 'running_time']\n"
'vect_mail = vectorizer.transform([mail])\n'
'Precision = No. of relevant documents retrieved / No. of total documents retrieved\n\nRecall = No. of relevant documents retrieved / No. of total relevant documents\n\nF-Score = 2 * Precision * Recall / Precision + Recall\n'
'input_to_state = None\nRNN = None\ndef main(dataset, n_h, n_y, batch_size, dev_split, n_epochs):\n    # Calling \'global\' allows you to modify these variables\n    global input_to_state\n    global RNN\n    input_to_state = Linear(name=\'input_to_state\',\n                            input_dim=seq_u.shape[-1],\n                            output_dim=n_h)\n    RNN = SimpleRecurrent(activation=Tanh(),\n                          dim=n_h, name="RNN")\n\n\ndef predict(dev_X):\n    dev_transform = input_to_state.apply(dev_X)\n    dev_h = RNN.apply(dev_transform)\n\nif __name__ == "__main__":   \n    main(args) \n    predict(dev_X)\n\ndef main(dataset, n_h, n_y, batch_size, dev_split, n_epochs):\n    input_to_state = Linear(name=\'input_to_state\',\n                            input_dim=seq_u.shape[-1],\n                            output_dim=n_h)\n    RNN = SimpleRecurrent(activation=Tanh(),\n                          dim=n_h, name="RNN")\n    return input_to_state, RNN\n\ndef predict(dev_X, input_to_state, RNN):\n    dev_transform = input_to_state.apply(dev_X)\n    dev_h = RNN.apply(dev_transform)\n\nif __name__ == "__main__":   \n    input_to_state, RNN = main(args) \n    predict(dev_X, input_to_state, RNN)\n'
'from pybrain.structure import FeedForwardNetwork\nn = FeedForwardNetwork()\n\nfrom pybrain.structure import LinearLayer, SigmoidLayer\ninLayer = LinearLayer(8)\nhiddenLayer = SigmoidLayer(10)\nhiddenLayer2 = SigmoidLayer(10)\noutLayer = LinearLayer(2)\n\nn.addInputModule(inLayer)\nn.addModule(hiddenLayer)\nn.addModule(hiddenLayer2)\nn.addOutputModule(outLayer)\n\nfrom pybrain.structure import FullConnection\nin_to_hidden = FullConnection(inLayer, hiddenLayer1)\nhidden_to_hidden = FullConnection(hiddenLayer1, hiddenLayer2)\nhidden_to_out = FullConnection(hiddenLayer2, outLayer)\n\nn.addConnection(in_to_hidden)\nn.addConnection(hidden_to_hidden)\nn.addConnection(hidden_to_out)\n\nn.sortModules()\n'
"d = train_data.to_dict(orient='index')\n"
'    inertia = _k_means._assign_labels_array(\n        X, x_squared_norms, centers, labels, distances=distances)\n\nimport numpy as np\nfrom sklearn.metrics import euclidean_distances\n\np = np.random.rand(1000000,2)\np[:p.shape[0]/2, :] += 100 #I move half of points far away\n\nfrom sklearn.cluster import KMeans\na = KMeans(n_clusters=2).fit(p) #changed to two clusters\nprint a.inertia_ , "****"\n\nmeans = a.cluster_centers_\ns = 0\nfor x in p:\n    best = float("inf")\n    for y in means:\n        d = (x-y).T.dot(x-y)\n        if d &lt; best:\n            best = d\n    s += best\nprint s, "*****"\n\n166805.190832 ****\n166805.190946 *****\n\nprint -a.score(p)\n'
'count_matrix_X_train = count_vect_item_group.transform(X_test)\nX_train_tf_idf = tf_idf(count_matrix_X_train)\n\nmodel_predicted_item_group.partial_fit(X_train_tf_idf, labels_test)\n'
'[[0.0], [1.0], [2.0], [3.0]]\n\n[[0.0], [0.3333], [0.6666], [1.0]]\n\n[[0.0], [2.0], [4.0], [6.0]]\n\n[[0.0], [0.333], [0.666], [1.0]]\n\n[[2.0], [3.0], [4.0], [5.0]]\n\nX:\n [[0.]\n [1.]\n [2.]\n [3.]] \n\nshape:  (4, 1) \n [[0.30926124]\n [2.1030826 ]\n [3.89690395]\n [5.6907253 ]]\n\nshape:  (4, 1) \n [[2.]\n [3.]\n [4.]\n [5.]]\n\nshape:  (4, 1) \n [[3.89690395]\n [5.6907253 ]\n [7.48454666]\n [9.27836801]]\n'
'print (df1)  \n  df1\n1   a\n2   b\n3   c\n\nprint (df2)\n  df1\n1   b\n2   c\n3   d\n\ndf1 = pd.get_dummies(df1)\ndf2 = pd.get_dummies(df2)\n\nunion = df1.columns | df2.columns\ndf1 = df1.reindex(columns=union, fill_value=0)\ndf2 = df2.reindex(columns=union, fill_value=0)\nprint (df1)\n   df1_a  df1_b  df1_c  df1_d\n1      1      0      0      0\n2      0      1      0      0\n3      0      0      1      0\nprint (df2)\n   df1_a  df1_b  df1_c  df1_d\n1      0      1      0      0\n2      0      0      1      0\n3      0      0      0      1\n'
'sns.pairplot(frame(num_cols),size=2)\n\nsns.pairplot(frame[num_cols],size=2)\n'
'weights_n = tf.Variable(tf.truncated_normal([a, b], stddev=0.1))\n\nbias_n = tf.Variable(tf.constant(0.1, shape=[b]))) \n'
"import numpy as np\nfrom sklearn.linear_model import Lasso\n\n# Make some random data\nx = np.random.random((100,4))\ny = np.random.random(100)\n\nm = Lasso(warm_start = True)\nm.fit(x,y)\n\n# Print out the current params\nprint(m.get_params())\n# The output will be\n#{'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, \n# 'max_iter': 1000, 'normalize': False, 'positive': False, \n# 'precompute': False, 'random_state': None, 'selection': 'cyclic',\n# 'tol': 0.0001, 'warm_start': True}\n\n# We can update the alpha value\nm.set_params(alpha = 2.0)\n\n# Fit again if we want\nm.fit(x,y)\n\n# Print out the current params\nprint(m.get_params())\n# The output will be\n#{'alpha': 2.0, 'copy_X': True, 'fit_intercept': True, \n# 'max_iter': 1000, 'normalize': False, 'positive': False, \n# 'precompute': False, 'random_state': None, 'selection': 'cyclic',\n# 'tol': 0.0001, 'warm_start': True}\n"
'import matplotlib\nimport numpy as np\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\n\n#Generate Sample data\nx = np.sort(5 * np.random.rand(40, 1), axis = 0)\ny = np.sin(x).ravel()\n\n#Add noise to targets\ny[::5] += 3 * (0.5 - np.random.rand(8))\n\n#create classifier regression model\nsvr_rbf = SVR(kernel="rbf", C=1000, gamma=0.1)\nsvr_lin = SVR(kernel="linear", C=1000, gamma=0.1)\nsvr_poly = SVR(kernel="poly", C=1000, gamma=0.1)\n\n#Fit regression model\ny_rbf = svr_rbf.fit(x,y).predict(x)\ny_lin = svr_lin.fit(x,y).predict(x)\ny_poly = svr_poly.fit(x,y).predict(x)\n\n#Plotting of results\nlw = 2\nplt.scatter(x, y, color="darkorange", label="data")\nplt.plot(x, y_rbf, color="navy", lw=lw, label="RBF Model")\nplt.plot(x, y_lin, color="c", lw=lw, label="Linear Model")\nplt.plot(x, y_poly, color="cornflowerblue", lw=lw, label="Polynomial Model")\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Support Vector Regression")\nplt.legend()\nplt.show()\n'
'# make sure the labels have are in shape (num_samples, 12)\ny = np.reshape(y, (-1, 12))\n\npower_in = Input(shape=(X.shape[1:],))\npower_lstm = LSTM(50, recurrent_dropout=0.4128,\n                  dropout=0.412563,\n                  kernel_initializer=power_lstm_init)(power_in)\n\nmain_out = Dense(12, kernel_initializer=power_lstm_init)(power_lstm)\n\n# make sure the labels have are in shape (num_samples, 12, 1)\ny = np.reshape(y, (-1, 12, 1))\n\npower_in = Input(shape=(48,1))\npower_lstm = LSTM(50, recurrent_dropout=0.4128,\n                  dropout=0.412563,\n                  kernel_initializer=power_lstm_init)(power_in)\n\nrep = RepeatVector(12)(power_lstm)\nout_lstm = LSTM(32, return_sequences=True)(rep)\nmain_out = TimeDistributed(Dense(1))(out_lstm)\n\nmodel = Model(power_in, main_out)\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, 48, 1)             0         \n_________________________________________________________________\nlstm_3 (LSTM)                (None, 50)                10400     \n_________________________________________________________________\nrepeat_vector_2 (RepeatVecto (None, 12, 50)            0         \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 12, 32)            10624     \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 12, 1)             33        \n=================================================================\nTotal params: 21,057\nTrainable params: 21,057\nNon-trainable params: 0\n_________________________________________________________________\n'
'feature_defs = ft.dfs(entityset=es,\n                      target_entity="cust",\n                      agg_primitives=["count", "sum"],\n                      verbose = True, \n                      max_depth=3, # add max_depth\n                      features_only = True)\n\n[&lt;Feature: AGE&gt;,\n &lt;Feature: SUM(order_items.QTY)&gt;,\n &lt;Feature: SUM(order_items.PRICE)&gt;,\n &lt;Feature: SUM(orders.PRICE)&gt;,\n &lt;Feature: SUM(orders.QTY)&gt;,\n &lt;Feature: COUNT(order_items)&gt;,\n &lt;Feature: COUNT(orders)&gt;,\n &lt;Feature: SUM(order_items.prods.PRICE)&gt;]\n\ninteresting_products = es["prods"].df.PROD_ID.unique()\nes["order_items"]["PROD_ID"].interesting_values=interesting_products\nfeature_defs = ft.dfs(entityset=es,\n                      target_entity="cust",\n                      agg_primitives=["count", "sum"],\n                      where_primitives=["count", "sum"],\n                      verbose=True, \n                      max_depth=3, \n                      features_only=True)\n\n[&lt;Feature: AGE&gt;,\n &lt;Feature: SUM(order_items.QTY)&gt;,\n &lt;Feature: SUM(order_items.PRICE)&gt;,\n &lt;Feature: SUM(orders.PRICE)&gt;,\n &lt;Feature: SUM(orders.QTY)&gt;,\n &lt;Feature: COUNT(order_items)&gt;,\n &lt;Feature: COUNT(orders)&gt;,\n &lt;Feature: SUM(order_items.prods.PRICE WHERE PROD_ID = 2)&gt;,\n &lt;Feature: SUM(order_items.QTY WHERE PROD_ID = 2)&gt;,\n &lt;Feature: SUM(order_items.QTY WHERE PROD_ID = 3)&gt;,\n &lt;Feature: SUM(order_items.prods.PRICE)&gt;,\n &lt;Feature: COUNT(order_items WHERE PROD_ID = 2)&gt;,\n &lt;Feature: SUM(order_items.prods.PRICE WHERE PROD_ID = 1)&gt;,\n &lt;Feature: SUM(order_items.PRICE WHERE PROD_ID = 3)&gt;,\n &lt;Feature: COUNT(order_items WHERE PROD_ID = 1)&gt;,\n &lt;Feature: COUNT(order_items WHERE PROD_ID = 3)&gt;,\n &lt;Feature: SUM(order_items.prods.PRICE WHERE PROD_ID = 3)&gt;,\n &lt;Feature: SUM(order_items.QTY WHERE PROD_ID = 1)&gt;,\n &lt;Feature: SUM(order_items.PRICE WHERE PROD_ID = 2)&gt;,\n &lt;Feature: SUM(order_items.PRICE WHERE PROD_ID = 1)&gt;]\n'
"punct = set(string.punctuation)\nstopwords = set(stopwords)\n\ndef clean_text(text):\n    text = ''.join(char.lower() for char in text if char not in punct)\n    tokens = re.split('\\W+', text)\n    text = [lm.lemmatize(word) for word in tokens if word not in stopwords]\n    return text\n"
'    label_idx = [unique.index(l) for l in labels] """ labels= class. works for your class is string or so. \nhere labels can be more than two"""\n    label_idx = np.array(label_idx) # just get your class into array\n    vectors = np.array(vecs) # vecs are any vectorised form of your text data\n    clf = LinearSVC() # classifier of your choice\n    clf.fit(vectors, label_idx)\n'
"image1 = np.random.rand(1, 1, 270, 480) #First dimension is batch size for test purpose\nimage2 = np.random.rand(1, 4, 268, 1) #Or any other arbitrary dimensions\n\ninput_img = layers.Input(shape=image1[0].shape)\nx = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(32, (3, 3), activation='relu')(x)\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\nmodel = tf.keras.Model(input_img, decoded)\nmodel.compile('adam', 'mean_squared_error')\nmodel.summary()\n\nmodel.fit(image1, nb_epoch=1, batch_size=1)\n\nmodel.fit(image2, nb_epoch=1, batch_size=1)\n\ninput_img = layers.Input(shape=image1[0].shape)\nx = layers.Conv2D(32, 3, activation='relu', padding='same')(input_img)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(16, 3, activation='relu', padding='same')(x)\nx = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, 3, activation='relu', padding='same')(x)\nencoded = layers.MaxPooling2D((2, 2), padding='same')(x)\nx = layers.Conv2D(8, 3, activation='relu', padding='same')(encoded)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(16, 3, activation='relu', padding='same')(x)\nx = layers.UpSampling2D((2, 2))(x)\nx = layers.Conv2D(32, 1, activation='relu')(x) # set kernel size to 1 for example\nx = layers.UpSampling2D((2, 2))(x)\ndecoded = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)\nmodel = tf.keras.Model(input_img, decoded)\nmodel.compile('adam', 'mean_squared_error')\nmodel.summary()\n"
'Brier = np.mean((Pred-drug2["DUQ290"]**2))\n\nBrier = np.mean((Pred[:,0]-drug2["DUQ290"]**2))\n'
'import cv2\nimport numpy as np \n\n# load image\nimage = cv2.imread(\'pw12b.jpg\')\n# crop image\nh,w = image.shape[:2]\nimage = image[200:h-20,20:550]\n# create hsv\nhsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n# set lower and upper color limits\nlow_val = (0,0,0)\nhigh_val = (179,45,96)\n# Threshold the HSV image \nmask = cv2.inRange(hsv, low_val,high_val)\n# remove noise\nmask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel=np.ones((8,8),dtype=np.uint8))\n# close mask\nmask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel=np.ones((20,20),dtype=np.uint8))\n\n# improve mask by drawing the convexhull \nret, contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\nfor cnt in contours:\n    hull = cv2.convexHull(cnt)\n    cv2.drawContours(mask,[hull],0,(255), -1)\n# erode mask a bit to migitate mask bleed of convexhull\nmask = cv2.morphologyEx(mask, cv2.MORPH_ERODE, kernel=np.ones((5,5),dtype=np.uint8))\n\n# remove this line, used to show intermediate result of masked road\nroad = cv2.bitwise_and(image, image,mask=mask)\n\n# apply mask to hsv image\nroad_hsv = cv2.bitwise_and(hsv, hsv,mask=mask)\n# set lower and upper color limits\nlow_val = (0,0,102)\nhigh_val = (179,255,255)\n# Threshold the HSV image \nmask2 = cv2.inRange(road_hsv, low_val,high_val)\n# apply mask to original image\nresult = cv2.bitwise_and(image, image,mask=mask2)\n\n#show image\ncv2.imshow("Result", result)\ncv2.imshow("Road", road)\ncv2.imshow("Mask", mask)\ncv2.imshow("Image", image)\n\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'
'transformer_x = RobustScaler().fit(X_train)\ntransformer_y = RobustScaler().fit(y_train) \nX_rtrain = transformer_x.transform(X_train)\ny_rtrain = transformer_y.transform(y_train)\nX_rtest = transformer_x.transform(X_test)\ny_rtest = transformer_y.transform(y_test)\n\n#Fit Train Model\nlasso = Lasso()\nlasso_alg = lasso.fit(X_rtrain,y_rtrain)\n\ntrain_score =lasso_alg.score(X_rtrain,y_rtrain)\ntest_score = lasso_alg.score(X_rtest,y_rtest)\n\nprint ("training score:", train_score)\nprint ("test score:", test_score)\n\nexample = [[10,100]]\ntransformer_y.inverse_transform(lasso.predict(example).reshape(-1, 1))\n'
'result = pd.concat([adult_train, Prediction], axis=1)\n\npred_gnb = gnb.predict(data_test)\n\nresult = pd.concat([pd.DataFrame(data_test), pd.DataFrame(target_test), Prediction], axis=1)\n'
'model.compile(loss=_loss, optimizer=_optimizer, metrics=[custom_loss_wrapper_2(model.input)])\n'
"UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n\nset(Y_test) - set(prediction_int)\n\nX_train, X_test, Y_train, Y_test = train_test_split(bow, Y, test_size=0.3, stratify=Y, random_state=42)\n\nsvc = svm.SVC(kernel='linear', C=1, probability=True, class_weight='balanced').fit(X_train, Y_train) \n"
'import numpy as np\nimport matplotlib.pyplot as plt\n\n#fake centers \ncenters = np.random.random((10,100,100,3))\n\n#print centers\nfor ci in centers:\n    print(ci)\n\n#visualize centers:\nfor ci in centers: \n    plt.imshow(ci)\n    plt.show()\n\nfrom scipy import ndimage\nfrom sklearn.cluster import KMeans\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n#PARAMS\nn_clusters=10  \n\n#fake train data\noriginal_train = np.random.random((100, 100, 100, 3)) #100 images of each 100 px,py and RGB \n\nn,x,y,c = original_train.shape\n\nflat_train = original_train.reshape((n,x*y*c))\n\nkmeans = KMeans(n_clusters, random_state=0)\n\nclusters = kmeans.fit_predict(flat_train)\n\ncenters = kmeans.cluster_centers_\n\n#visualize centers:\nfor ci in centers: \n    plt.imshow(ci.reshape(x,y,c))\n    plt.show()\n\n#visualize other members\nfor cluster in np.arange(n_clusters):\n\n    cluster_member_indices = np.where(clusters == cluster)[0]\n    print("There are %s members in cluster %s" % (len(cluster_member_indices), cluster))\n\n    #pick a random member\n    random_member = random.choice(cluster_member_indices)\n    plt.imshow(original_train[random_member,:,:,:])\n    plt.show()\n'
'signal_data = scaler.fit_transform(signal_data)\n'
'import numpy as np\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nX = np.array([[1, 1], [2, 2], [1, 3]])\nencoder = ColumnTransformer(\n    [(\'number1\', OneHotEncoder(dtype=\'int\'), [1])],\n    remainder="passthrough"\n)\n\nprint(encoder.fit_transform(X))\n'
"model.compile(optimizer='adam',\n              loss=losses.sparse_categorical_crossentropy,\n              metrics=['categorical_accuracy', 'sparse_categorical_accuracy'])\nmodel.fit(train_images, train_labels, epochs=1)\n'''\nTrain on 60000 samples\n60000/60000 [==============================] - 5s 86us/sample - loss: 0.4955 - categorical_accuracy: 0.1045 - sparse_categorical_accuracy: 0.8255\n'''\n\n\nmodel.compile(optimizer='adam',\n              loss=my_loss,\n              metrics=['accuracy', 'sparse_categorical_accuracy'])\nmodel.fit(train_images, train_labels, epochs=1)\n\n'''\nTrain on 60000 samples\n60000/60000 [==============================] - 5s 87us/sample - loss: 0.4956 - acc: 0.1043 - sparse_categorical_accuracy: 0.8256\n'''\n"
'import requests\nimport os\n\npic_list=[\'https://i8.amplience.net/i/nlyscandinavia/146368-0014_01/i-straight-crepe-pant/\', \'https://i8.amplience.net/i/nlyscandinavia/146368-0014_02/i-straight-crepe-pant/\', \'https://i8.amplience.net/i/nlyscandinavia/146368-0014_04/i-straight-crepe-pant/\', \'https://i8.amplience.net/i/nlyscandinavia/146368-0014_05/i-straight-crepe-pant/\']\nDIR_TO_SAVE = \'.\'\nheaders = {\n    \'User-Agent\': \'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:71.0) Gecko/20100101 Firefox/71.0\'\n}\ni=0\nfor pic_url in pic_list:\n    url = pic_url.strip()\n    print(\'pic_url: \'+url)\n    if url[-1] == \'/\':\n        filename = url.rstrip(\'/\').split(\'/\')[-1]+str(i)+\'.jpeg\'\n        i+=1\n    else:\n        filename = url.split(\'/\')[-1]\n    output = requests.get(url, headers=headers)\n    if output.status_code == 200:\n        with open(os.path.join(DIR_TO_SAVE, filename), \'wb\') as f:\n            f.write(output.content)\n    else:\n        print("Couldnt get file: "+url)\n'
"import pandas as pd\nfrom collections import Counter\n\ndf.fillna(method='ffill', inplace=True)\n\n# Create a counter object and pass it the origin-destination tuples\ncounter = Counter()\nfor col in df.columns:\n    routes = list(zip(df[col].shift(1, fill_value=df[col][0]), df[col]))\n    routes = [(k, v) for k, v in routes if k != v]\n    counter.update(routes)\ncounter.most_common(3)\n\ncounter.most_common(3)\nOut[76]: \n[(('Spain', 'USA'), 3),\n (('Portugal', 'Spain'), 2),\n (('Bulgaria', 'Portugal'), 1)]\n"
'model.add(LSTM(128,\n                   input_shape=(2, 2, 4), return_sequences=True))\n\nmodel.add(LSTM(128,\n                   input_shape=(2, 4), return_sequences=True))\n'
'data_elem_data                  = data_elem\ndata_elem_label                 = labels go here (must be 1 dimensional)\n\ndata_elem_label  = data_elem_label.values.flatten() \n\ntsne_df = pd.concat([tsne_df, pd.Series(data_elem_label)], axis = 1) \n'
'dist^2 = (x-.7)^2 + (y-.2)^2\n'
'for i in range(len(test_generator)):\n\nif test_generator.filenames[i].find(".png") != -1:\n    pred = model.predict(test_generator[i])\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Get A and B\nA = np.random.normal(1, 1, 1000000)\nB = np.random.normal(3, 0.1, 20000)\n\n# Count the number of observations in A for each B\nB.sort()\na = A[np.logical_and(A &gt;= B.min(), A &lt;= B.max())]\na = [(a&lt;i).sum() for i in B]\n\n# Plot results\nplt.plot(B, np.arange(B.shape[0]), label='Class B')\nplt.plot(B, a, label='Class A')\nplt.ylabel('Count of samples')\nplt.xlabel('Values')\nplt.legend()\nplt.show()\n"
'!pip uninstall tensorflow -y\n!pip install  tensorflow==1.14\n\n%tensorflow_version 1.x\nimport tensorflow as tf\nprint(tf.__version__)\n\n1.14.0\n'
'test_images=train_images/255.0\n\ntest_images=test_images/255.0\n'
"df['salary_level']= df['salary'].apply(lambda x : Salary(x).sal())\n"
'predicted_y = tf.nn.softmax(logits)\nprobas=tf.argmax(predicted_y, axis=1)\n\npredicted_y = tf.nn.sigmoid(logits)\n\nprediction_function=lambda vector1: predicted_y.eval({input_x:vector1})\n\nloss at iter 0:0.0085\ntrain auc: 0.9998902365402557\ntest auc: 1.0\nloss at iter 1:0.0066\ntrain auc: 1.0\ntest auc: 1.0\nloss at iter 2:0.0052\ntrain auc: 1.0\ntest auc: 1.0\nloss at iter 3:0.0042\ntrain auc: 1.0\ntest auc: 1.0\nloss at iter 4:0.0035\ntrain auc: 1.0\ntest auc: 1.0\n\nnp.round(prediction_function(X_train)).reshape(1,-1)\n# result:\narray([[0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n        1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0.,\n        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n        0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n        0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n        0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n        1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n        1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1.,\n        1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n        1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n        0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.]],\n      dtype=float32)\n'
'import pandas as pd \ndf = pd.read_csv("../data/student-mat.csv", sep=\';\')\n\n# pass "df" to pd.get_dummies()\ndf_one_hot = pd.get_dummies(df, columns=["reason"],prefix=["reason"])\ndf_one_hot.head()\n\n# passed again "df" to pd.get_dummies()\ndf_one_hot = pd.get_dummies(df, columns=["guardian"], prefix=["guardian"])\n\n## solution =&gt; pass the df_one_hot to pd.get_dummies\n# df_one_hot = pd.get_dummies(df_one_hot, columns=["guardian"], prefix=["guardian"])\n'
'categorical_features = X.select_dtypes(include="object").columns\ninteger_features = X.select_dtypes(exclude="object").columns\n'
'm = tf.keras.Sequential()\nm.add(tf.keras.Input(name="feature", shape=(2206, 2)))\n\nm.add(tf.keras.layers.Flatten())\nm.add(tf.keras.layers.Dense(...))\n'
'#Initialise\nn_hidden_1 = 14\n\nW1 = tf.get_variable("W1", [n_input,n_hidden_1], dtype=tf.float64, initializer = tf.contrib.layers.xavier_initializer())\nb1 = tf.get_variable("b1", [n_hidden_1], dtype=tf.float64, initializer = tf.zeros_initializer())\nW2 = tf.get_variable("W2", [n_hidden_1,n_output], dtype=tf.float64, initializer = tf.contrib.layers.xavier_initializer())\nb2 = tf.get_variable("b2", [n_output], dtype=tf.float64, initializer = tf.zeros_initializer())\n\nkeep_prob = tf.placeholder(tf.float64)\n\n#creating placeholders\nx = tf.placeholder(tf.float64, [None,n_input])\ny = tf.placeholder(tf.float64)\n\n#Model\ndef model(x, W1, b1, W2, b2, keep_prob):\n    layer_1 = tf.add(tf.matmul(x, W1), b1)\n    layer_1 = tf.nn.relu(layer_1)\n    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n    out_layer = tf.add(tf.matmul(layer_1, W2),b2)\n    return out_layer\n\npredictions = model(x, W1,b1,W2,b2, keep_prob)\ncost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels = y,logits = predictions))\noptimizer = tf.train.AdamOptimizer().minimize(cost)\n\nwith tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n\n        for epoch in range(training_epochs):\n            epoch_loss = 0\n            i = 0\n            while i &lt; len(x_train):\n                start = i\n                end = i + batch_size\n                batch_x = np.array(x_train[start:end])\n                batch_y = np.array(y_train[start:end])\n\n                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n                                        y: batch_y,keep_prob:0.5})\n                epoch_loss += c\n                i+=batch_size\n\n            print(\'Epoch\', epoch, \'completed out of\', training_epochs, \'loss:\', epoch_loss)\n\n\n        # correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n        # accuracy = tf.reduce_mean(tf.cast(correct, \'float\'))\n\n        print (test_x.shape)\n        accuracy = tf.nn.l2_loss(prediction-y,name="squared_error_test_cost")/test_x.shape[0]\n        print(\'Accuracy:\', accuracy.eval({x: test_x, y: test_y}))\n'
'# Fitting Linear Regression to the dataset\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_test, y_test)\n\n# Fitting Polynomial Regression to the dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=15)\nX_poly = poly_reg.fit_transform(X_test)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y_test)\n'
"nb_classifier = MultinomialNB()\nsvm_classifier = LinearSVC()\nlr_classifier = LogisticRegression(multi_class=&quot;ovr&quot;)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(df_train.data, df_train.label, test_size=0.2 , stratify = df_train['label'])\nvect = CountVectorizer(stop_words='english', max_features=10000,\n                       token_pattern=r'[a-zA-Z]{3,}' , ngram_range=(1,2))\nX_train_dtm = vect.fit_transform(X_train)\nX_test_dtm = vect.transform(X_test)\nnb_classifier.fit(X_train_dtm, y_train)\n\nsvm_classifier.fit(X_train_dtm, y_train)\nlr_classifier.fit(X_train_dtm, y_train)\nnb_predictions = nb_classifier.predict(X_test_dtm)\nsvm_predictions = svm_classifier.predict(X_test_dtm)\nlr_predictions = lr_classifier.predict(X_test_dtm)\n\n#plot Precision-Recall curve and display average precision-recall score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import average_precision_score\n\ndisp = plot_precision_recall_curve(svm_classifier, X_test_dtm, y_test) #display Precision-Recall curve for svm_classifier\naverage_precision = average_precision_score(y_test, svm_predictions)\nprint('Average precision-recall score for svm_classifier: {0:0.2f}'.format(\n      average_precision))\n\ndisp = plot_precision_recall_curve(nb_classifier, X_test_dtm, y_test) #display Precision-Recall curve for nb_classifier\naverage_precision = average_precision_score(y_test, nb_predictions)\nprint('Average precision-recall score for nb_classifier: {0:0.2f}'.format(\n      average_precision))\n\ndisp = plot_precision_recall_curve(lr_classifier, X_test_dtm, y_test) #display Precision-Recall curve for nb_classifier\naverage_precision = average_precision_score(y_test, lr_predictions)\nprint('Average precision-recall score for lr_classifier: {0:0.2f}'.format(\n      average_precision))\n"
"timestamp, features, n_sample = 45, 2, 1000\nn_class = 10\nX = np.random.uniform(0,1, (n_sample, timestamp, features))\ny = np.random.randint(0,n_class, n_sample)\n\nmodel = Sequential()\nmodel.add(Conv1D(8, 3, activation='relu', input_shape=(timestamp, features)))\nmodel.add(MaxPooling1D(3))\nmodel.add(Conv1D(8, 3, activation='relu'))\nmodel.add(GlobalAveragePooling1D())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(n_class, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nhistory = model.fit(X, y, batch_size=128, epochs=5, validation_data=(X, y))\n\nhistory.history['accuracy'][-1] # (a)\nhistory.history['val_accuracy'][-1] # (b)\naccuracy_score(y, np.argmax(model.predict(X), axis=1)) # (c)\n"
"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;))\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation=&quot;relu&quot;, padding=&quot;same&quot;))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=&quot;relu&quot;))\nmodel.add(Dense(128, activation=&quot;relu&quot;))\nmodel.add(Dense(len(class_names), activation=&quot;softmax&quot;))\n\nmodel.compile(optimizer='RMSProp', loss=&quot;sparse_categorical_crossentropy&quot;, metrics=['accuracy'])\nmodel.summary() \n"
"cnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n          loss='categorical_crossentropy',\n          metrics=['accuracy'])\n\ncnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n          loss='sparse_categorical_crossentropy',\n          metrics=['accuracy'])\n"
"model = Sequential([\n    Dense(32, activation='relu', input_shape=(7,)),\n    Dense(1, activation='relu'),\n])\n"
'np.unique(y_pred)\n# array([0, 1])\n\ny_pred = clf_lr.predict_proba(X_test)     # get probabilities\ny_prob = np.array([x[1] for x in y_pred]) # keep the prob for the positive class 1\nroc_auc = roc_auc_score(y_test, y_prob)\navg_precision = average_precision_score(y_test, y_prob)\nprint(f&quot;ROC_AUC: {roc_auc}&quot;)\nprint(f&quot;Average_precision: {avg_precision}&quot;)\n\nROC_AUC: 0.9545954595459546\nAverage_precision: 0.9541994473779806\n\nviz3 = ROCAUC(LogisticRegression(random_state=seed), binary=True) # similarly for the PrecisionRecall curve\n\nviz3.score(X_test, y_test)\n# 0.88\n\n# verify this is the accuracy:\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, clf_lr.predict(X_test))\n# 0.88\n'
'saved_model.layers[0].layers\n\nsaved_model.layers[0].layers[-cnt].trainable = True\n'
"def decay_func(step_tensor, **other_arguments_your_func_needs):\n    # body of the function. The input argument step tensor must be used\n    # to determine the learning rate that will be returned\n    return learning_rate\n\nstep = tf.Variable(0, trainable=False, name='Step', dtype=tf.int64)\n\nfrom functools import partial\n\ndecaying_learning_rate = partial(decay_func, step_tensor=step, **other_arguments_your_func_needs)\n\nopt = tf.keras.optimizers.Adam(decaying_learning_rate)\n\nstep.assing_add(1)\n\nthis_step_learning_rate = decaying_learning_rate()\n"
'for value in [.2, .999, .0001, 100., -100.]:\n    print(tf.nn.softmax([value]))\n\ntf.Tensor([1.], shape=(1,), dtype=float32)\ntf.Tensor([1.], shape=(1,), dtype=float32)\ntf.Tensor([1.], shape=(1,), dtype=float32)\ntf.Tensor([1.], shape=(1,), dtype=float32)\ntf.Tensor([1.], shape=(1,), dtype=float32)\n\nfor value in [.2, .999, .0001, 100., -100.]:\n    print(tf.nn.sigmoid([value]))\n\ntf.Tensor([0.549834], shape=(1,), dtype=float32)\ntf.Tensor([0.7308619], shape=(1,), dtype=float32)\ntf.Tensor([0.500025], shape=(1,), dtype=float32)\ntf.Tensor([1.], shape=(1,), dtype=float32)\ntf.Tensor([0.], shape=(1,), dtype=float32)\n\ntf.round(tf.nn.sigmoid([.1]))\n'
"# difference here\nXnew = productMarketResearch.loc[[50]]\n# usually you don't need this\n# Xnew = np.array(Xnew.values.tolist())\n\nXnew = sc.transform(Xnew)\nynew = rfc.predict(Xnew)\n"
'x_train, x_test, y_train, y_test = train_test_split(x,y.flatten(),test_size=0.2,random_state=0)\n'
'def kernel(X1, X2):\n    X1 = np.array([[(x[0] - x[1]) ** 2] for x in X1])\n    X2 = np.array([[(x[0] - x[1]) ** 2] for x in X2])\n    return np.dot(X1, X2.T)\n\nclf = svm.SVC(kernel=kernel, max_iter=100)\n'
'sudo pip install scikits.learn\n\nsudo python2.6 pip install scikits.learn\n\npip searh foo\n'
'    ratio = (60.00 / 5.00)\n    class_weights = tf.constant([ratio, 1 - ratio])\n    weighted_logits = tf.mul(logits, class_weights)\n\nloss = tf.nn.softmax_cross_entropy_with_logits(weighted_logits, self._targets)\n\nloss = loss * weights\n'
'(((a - b)**2).sum(axis=2)**0.5).sum()\n\nb = numpy.roll(a, 1, axis=0)\n'
"input_img = Input(shape=(1, 32, 32))\n\nx = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(input_img)\nx = MaxPooling2D((2, 2), border_mode='same')(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nx = MaxPooling2D((2, 2), border_mode='same')(x)\nx = Convolution2D(8, 2, 2, activation='relu', border_mode='same')(x)\nencoded = MaxPooling2D((2, 2), border_mode='same')(x)\n\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(encoded)\nx = UpSampling2D((2, 2))(x)\nx = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Convolution2D(1, 3, 3, activation='sigmoid', border_mode='same')(x)\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n"
"df.set_index('AMID', inplace=True)\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\ndf = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\ndf\n\n                  A         B         C\nAMID                                   \nAMID-1000 -0.660181  0.663739  0.095517\nAMID-1001 -0.584459  0.001476 -0.987447\nAMID-1002  1.101281  0.822116  1.615059\nAMID-1003 -1.190499 -0.924988  1.118286\nAMID-1004 -0.536990  0.433827  0.994909\nAMID-1005  0.200213  0.348455 -0.613294\nAMID-1006  2.340943 -1.800818  0.667911\nAMID-1007  0.412372  0.326088 -1.456467\nAMID-1008 -0.659357  1.541636 -0.405293\nAMID-1009 -0.423322 -1.411532 -1.029181\n"
'import tensorflow as tf\nmy_a = tf.Variable(2,name = "my_a")\nmy_b = tf.Variable(3,name = "my_b")\nmy_c = tf.Variable(4,name="my_c")\n# Use the assign() function to set the new value\nadd = my_c.assign(tf.add(my_a,my_b))\n\nwith tf.Session() as sess:\n    init = tf.initialize_all_variables()\n    sess.run(init)\n    # Execute the add operator\n    sess.run(add)\n    print("my_c =  ",sess.run(my_c))\n    saver = tf.train.Saver()\n    saver.save(sess,"test.ckpt")\n'
'import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.neighbors import KNeighborsRegressor\n\ndf = pd.read_csv("data.csv")\nX = np.asarray(df.loc[:, [\'A\', \'B\', \'C\', \'D\', \'E\']])\ny = np.asarray(df[\'X\'])\n\nrs = ShuffleSplit(n_splits=1, test_size=1./5, random_state=0)\ntrain_indices, test_indices = rs.split(X).next()\n\nknn = KNeighborsRegressor(n_neighbors=100, weights=\'distance\')\nknn.fit(X[train_indices], y[train_indices])\n\npredictions = knn.predict(X)\n'
'import numpy as np\nfrom sklearn import ensemble\n\n# generate random 2d arrays\nimage_data = np.random.rand(10,10, 100)\n\n# generate random labels\nlabels = np.random.randint(0,2, 100)\n\nX = image_data.reshape(100, -1)\n\n# then use any scikit-learn classification model\nclf = ensemble.RandomForestClassifier()\nclf.fit(X, y)\n'
"for core in range(num_cores):\n  with tf.device('/gpu:%d'%core):\n    prediction = model(example_batch)\n# ...\nfor step in range(iter):\n  myprediction = sess.run(prediction, feed_dict={example_batch:input_array})\n"
'in=in.reshape((-1,3,2))\n\nw = np.random.rand(3)\n\nout1 = np.average(in, weights = w, axis = 1)\n\nout1 = np.sum(t*w[None,:, None], axis = 1) / np.sum(w)\n'
"import os\nimport glob\nimport pandas as pd\n\nfiles = glob.glob(r'D:\\temp\\.data\\43076965\\*.csv')\n\nx = pd.concat(\n    [pd.read_csv(f, header=None, delim_whitespace=True)\n       .assign(f=os.path.basename(os.path.splitext(f)[0]))\n     for f in files],\n    ignore_index=True\n)\n\nx.to_csv(r'd:/temp/out.csv', header=None, index=False, sep='\\t')\n\n4   -1.6144 -0.7137 1.2791  -0.8205 A\n5   -2.4333 -0.9433 0.9616  -2.3392 A\n6   -2.3548 0.0945  0.9933  -3.1174 A\n7   -3.0944 -0.8559 0.6441  -2.8976 A\n8   -0.4791 -0.8795 0.7027  -2.7338 A\n9   -3.654  -6.27   0.265   -4.041  A\n0   -3.1024 -1.871  0.1311  -5.7071 B\n1   -0.1855 -0.8241 -0.2756 -6.6962 B\n2   -3.2941 -2.0557 0.2471  -4.4969 B\n3   -1.5004 -0.2482 0.9565  -0.6866 B\n"
"from sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import f1_score\n\ny_true = [['a','b','c']]\ny_pred = [['a','c','b']]\n\nbinarizer = MultiLabelBinarizer()\n\n# This should be your original approach\n#binarizer.fit(your actual true output consisting of all labels)\n\n# In this case, I am considering only the given labels.\nbinarizer.fit(y_true)\n\nf1_score(binarizer.transform(y_true), \n         binarizer.transform(y_pred), \n         average='macro')\n\nOutput:  1.0\n"
"classifier = neural_network.MLPRegressor(hidden_layer_sizes=(200, 200), solver='lbfgs', activation='tanh', learning_rate='adaptive')\n"
'import time\n\nruns = 10000000\n\nclass A:\n    def __init__(self):\n    self.val = 1\n\n    def get_val(self):\n    return self.val\n\n# Using method to then call object attribute\nobj = A()\nstart = time.time()\ntotal = 0\nfor i in xrange(runs):\n    total += obj.get_val()\nend = time.time()\nprint end - start\n\n# Using object attribute directly\nstart = time.time()\ntotal = 0\nfor i in xrange(runs):\n    total += obj.val\nend = time.time()\nprint end - start\n\n# Assign to local_var first\nstart = time.time()\ntotal = 0\nlocal_var = obj.get_val()\nfor i in xrange(runs):\n    total += local_var\nend = time.time()\nprint end - start\n\n1.49576115608\n0.656110048294\n0.551875114441\n'
"for _ in range(int(mnist.train.num_examples/batch_size)):\n    epoch_x, epoch_y = mnist.train.next_batch(batch_size) \n    _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n\nprint('Accuracy: ', accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n"
"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import load_iris \nimport numpy as np\n\ndata = load_iris()\n\nx = data.data\ny = data.target\n\nclf = RandomForestClassifier()\n\nclf.fit(x,y)\n\nclf.decision_path(x)\n\n(&lt;150x140 sparse matrix of type '&lt;type 'numpy.int64'&gt;'\nwith 5406 stored elements in Compressed Sparse Row format&gt;, array([  0,  13,  \n26,  41,  54,  71,  86,  97, 106, 119, 140]))\n"
'np.array([np.ones((150,150,3)), np.ones((150,150,3))]).shape\n&gt;&gt;&gt; (2, 150, 150, 3)\n\nreturn np.array(train_data)\n\nimport numpy as np\nimg_1 = np.ones((150, 150, 3))\nimg_2 = np.ones((150, 150, 3))\n\nstacked_img = np.stack((img_1, img_2))\nstacked_img.shape\n&gt;&gt;&gt; (2, 150, 150, 3)\n'
"keywords = [''.join(i) for i in itertools.product(ascii_lowercase, repeat = 3)]\nvector = CountVectorizer(analyzer='char', ngram_range=(3,3), vocabulary=keywords)\ntr_test = vector.transform(['word1'])\nprint(tr_test)\n\n  (0, 9909)  1\n  (0, 15253) 1\n\ntest = vector.transform(['aaa aab'])\nprint(test)\n\n(0, 0)  1\n(0, 1)  1\n"
"mlp.fit(X_test, y_test)\n\nprint('Accuracy testing : {:.3f}'.format(mlp.score(X_test, y_test)))\n"
"-tf.reduce_sum(labels*tf.log(ol) + (1-labels)*tf.log(1-ol), name = 'loss')\n"
'if f1_start_pos &lt;= f2_start_pos &lt;= f1_start_pos + f1_length - 1:\n\nif f2_start_pos &lt;= f1_start_pos &lt;= f2_start_pos + f2_length - 1:\n'
'theta = theta - (alpha / m) * np.dot(X.T.reshape(2, 97), np.dot(X, theta).flatten() - y).reshape(2, 1)\n'
'1.2 ^ max(0, g-2)\n'
"# your model definition\n# your model.compile()\n\nbatch_size = 128\nepochs = 5\n\nhist = model.fit(x_train, y_train,\n      batch_size=batch_size,\n      epochs=epochs,\n      verbose=1,\n      validation_data=(x_test, y_test)  # optional\n      )\n\n# output\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/5\n60000/60000 [==============================] - 76s - loss: 0.3367 - acc: 0.8974 - val_loss: 0.0765 - val_acc: 0.9742\nEpoch 2/5\n60000/60000 [==============================] - 73s - loss: 0.1164 - acc: 0.9656 - val_loss: 0.0516 - val_acc: 0.9835\nEpoch 3/5\n60000/60000 [==============================] - 74s - loss: 0.0866 - acc: 0.9741 - val_loss: 0.0411 - val_acc: 0.9863\nEpoch 4/5\n60000/60000 [==============================] - 73s - loss: 0.0730 - acc: 0.9781 - val_loss: 0.0376 - val_acc: 0.9871\nEpoch 5/5\n60000/60000 [==============================] - 73s - loss: 0.0639 - acc: 0.9810 - val_loss: 0.0354 - val_acc: 0.9881\n\nhist.history\n# result:\n{'acc': [0.8973833333969117,\n  0.9656000000635783,\n  0.9740500000317891,\n  0.9780500000635783,\n  0.9810333334604899],\n 'loss': [0.3367467244784037,\n  0.11638248273332914,\n  0.08664042545557023,\n  0.07301943883101146,\n  0.06391783343354861],\n 'val_acc': [0.9742, 0.9835, 0.9863, 0.9871, 0.9881],\n 'val_loss': [0.07650674062222243,\n  0.051606363496184346,\n  0.04107686730045825,\n  0.03761903735231608,\n  0.03537947320453823]}\n\nhist.history['acc']\n# result:\n[0.8973833333969117,\n 0.9656000000635783,\n 0.9740500000317891,\n 0.9780500000635783,\n 0.9810333334604899]\n\nnp.mean(hist.history['acc']) # numpy assumed imported as np\n# 0.9592233334032695\n"
'# First, name your input tensor\ninput_layer = tf.reshape(features["image"], [-1, _DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, 3], name=\'input_layer\')\n\n...\n\npredictions = sess.run(\'y_pred:0\',\n                       {\'input_layer:0\': image_data})\n'
"model.add(Dense(1))\n\nmodel.add(Dense(1, activation='softplus'))\n"
'import numpy as np\nX = np.random.rand(1000,2)\ny = np.random.randint(0, 5, 1000)\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier().fit(X, y)\ntree.feature_importances_\n# array([ 0.51390759,  0.48609241])\n'
'num_hidden = 10\nnet = gluon.nn.Sequential()\nwith net.name_scope():\n    net.add(gluon.nn.Dense(num_hidden, activation="relu"))\n    net.add(gluon.nn.Dense(num_hidden, activation="relu"))\n    net.add(gluon.nn.Dense(num_outputs))\n\nlayers_to_freeze = set([\'sequential1_dense0_weight\', \'sequential1_dense0_bias\', \'sequential1_dense1_weight\', \'sequential1_dense1_bias\'])    \n\nfor p in net.collect_params().items():\n    if p[0] in layers_to_freeze:\n        p[1].grad_req = \'null\'\n\nnet.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n'
'y_pred = classifier.predict(X_test)\n\ny_pred = classifier.fit(X_test)\n'
"&gt;&gt;&gt; df\n   0\n0  2\n1  2\n2  2\n3  1\n4  1\n5  2\n6  2\n7  1\n8  1\n9  1\n\nonehotencoder = OneHotEncoder(categorical_features= [0])\n\n&gt;&gt;&gt; onehotencoder.fit_transform(df[0].values.reshape(1,-1))\n&lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 10 stored elements in COOrdinate format&gt;\n\n&gt;&gt;&gt; onehotencoder.fit_transform(df[0].values.reshape(1,-1)).toarray()\narray([[1., 2., 2., 1., 1., 2., 2., 1., 1., 1.]])\n\nonehotencoder = OneHotEncoder(categorical_features= [0], sparse=False)\n\n&gt;&gt;&gt; onehotencoder.fit_transform(df[0].values.reshape(1,-1))\narray([[1., 2., 2., 1., 1., 2., 2., 1., 1., 1.]])\n"
'from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\ndata = [[0, 0], [0, 1], [1, 0], [1, 1]]\n\nscaler = StandardScaler()\n\nmyData = scaler.fit_transform(data)\n\nrestored = scaler.inverse_transform(myData)\n\nassert np.allclose(restored, data)  # check we got the original data back\n'
'delta = a - y\nfor k in [2, 1, 0]:\n    tmp = delta * sigmoid_prime(A[k+1])\n    delta = np.dot(self.weights[k].T, tmp)\n    self.weights[k] -= self.learning_rate * np.dot(tmp, A[k].T) \n\ndelta = a - y\nfor k in [2, 1, 0]:\n    tmp = delta * sigmoid_prime(A[k+1])\n    delta = np.dot(self.weights[k].T, delta)  # WRONG HERE\n    self.weights[k] -= self.learning_rate * np.dot(tmp, A[k].T) \n\noutput_errors = np.dot(self.weights_matrices[layer_index-1].T, output_errors)\n\noutput_errors = np.dot(self.weights_matrices[layer_index-1].T, output_errors * out_vector * (1.0 - out_vector))\n'
'table = [[15, 29, 6, 2],\n    [16, 9, 8, 0],\n    [7, 27, 16, 0]]\n\ndef averages(table, col, by):\n    columns = tuple(([table[i][col] for i in range(len(table))]))  #Place col column into tuple so it can be placed into dictionary\n    groupby = tuple(([table[i][by] for i in range(len(table))]))   #Place groupby column into tuple so it can be placed into dictionary\n\n    avgdict = {}\n\n    for x in range(len(groupby)):\n        key = groupby[x]\n        if key in avgdict:\n            avgdict[key] += columns[x]\n        else:\n            avgdict[key] = columns[x]\n\n    print(avgdict)\n\naverages(table, 1, 3)\n\ndef averages(table, col, by):\n    columns = tuple(([table[i][col] for i in range(len(table))]))  #Place col column into tuple so it can be placed into dictionary\n    groupby = tuple(([table[i][by] for i in range(len(table))]))   #Place groupby column into tuple so it can be placed into dictionary\n\n    avgdict = {}\n    avgdict[groupby] = [columns]\n\n    newdict = {}\n\n    for key in avgdict:\n        for x in range(len(key)):\n            if key[x] in newdict:\n                newdict[key[x]] += avgdict[key][0][x]\n            else:\n                newdict[key[x]] = avgdict[key][0][x]\n\n    print(newdict)\n'
"df['T-size'].astype(pd.api.types.CategoricalDtype(['S','M','L'],ordered=True)).\n"
'input_ =  keras.layers.Input(shape=(8,))\nx =  keras.layers.Dense(16)(input_)\noutput1 = keras.layers.Dense(32, name="output1")(x)\noutput2 = keras.layers.Dense(32, name="output2")(x)\nmodel = keras.models.Model(inputs=input_, outputs=[output1, output2])\nmodel.compile(loss=["mse", "mae"], optimizer="adam", metrics={"output1":"accuracy","output2":"accuracy"})\n\n[\'loss\', \'output1_loss\', \'output2_loss\', \'output1_acc\', \'output2_acc\']\n'
"classifier.add(Dense(units = 512, activation = 'relu'))\nclassifier.add(Dropout(0.2)) #prevent overfitting\nclassifier.add(Dense(units=1, activation='sigmoid')) # HERE ---\n"
'd_train = lgb.Dataset(x_train, label=y_train, free_raw_data = False)\n'
"model.add(keras.layers.SimpleRNN(7, activation='tanh', \n                       return_sequences=True, \n                        input_shape=[7,7]))\nmodel.add(keras.layers.Dense(7))\nmodel.summary()\n\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsimple_rnn_12 (SimpleRNN)    (None, 7, 7)              105       \n_________________________________________________________________\ndense_2 (Dense)              (None, 7, 7)              56        \n=================================================================\nTotal params: 161\nTrainable params: 161\nNon-trainable params: 0\n_________________________________________________________________\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')# define any loss, you want\nmodel.fit(x_train, y_train, epochs=2)\n"
'import matplotlib.pyplot as plt\nimport numpy as np\nimport pdb\nimport cv2\n\ndef rgb2gray(rgb):\n\n    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n\n    return gray\n\nif __name__ == "__main__":\n\n    ball = plt.imread(\'ball.jpg\');\n    ball = rgb2gray(ball);\n    findtheballcol = plt.imread(\'findtheball.jpg\');\n    findtheball = rgb2gray(findtheballcol)\n    matching_img = np.zeros((findtheball.shape[0], findtheball.shape[1]));\n\n    #METHOD 1\n    width = ball.shape[1]\n    height = ball.shape[0]\n    for i in range(ball.shape[0], findtheball.shape[0]-ball.shape[0]):\n        for j in range(ball.shape[1], findtheball.shape[1]-ball.shape[1]):\n\n\n            # here use NCC or something better\n            matching_score = np.abs(ball - findtheball[i:i+ball.shape[0], j:j+ball.shape[1]]);\n            # inverting so that max is what we are looking for\n            matching_img[i,j] = 1 / np.sum(matching_score);\n\n\n    plt.subplot(221);\n    plt.imshow(findtheball); \n    plt.title(\'Image\')\n    plt.subplot(222);\n    plt.imshow(matching_img, cmap=\'jet\');\n    plt.title(\'Matching Score\')\n    plt.subplot(223);\n    #pick a threshold\n    threshold_val = np.mean(matching_img) * 2; #np.max(matching_img - (np.mean(matching_img)))\n    found_at = np.where(matching_img &gt; threshold_val)\n    show_match = np.zeros_like(findtheball)\n    for l in range(len(found_at[0])):\n        yb = round(found_at[0][l]-height/2).astype(int)\n        yt = round(found_at[0][l]+height/2).astype(int)\n        xl = round(found_at[1][l]-width/2).astype(int)\n        xr = round(found_at[1][l]+width/2).astype(int)\n        show_match[yb: yt, xl: xr] = 1;\n    plt.imshow(show_match)\n    plt.title(\'Candidates\')\n    plt.subplot(224)\n    # higher threshold\n    threshold_val = np.mean(matching_img) * 3; #np.max(matching_img - (np.mean(matching_img)))\n    found_at = np.where(matching_img &gt; threshold_val)\n    show_match = np.zeros_like(findtheball)\n    for l in range(len(found_at[0])):\n        yb = round(found_at[0][l]-height/2).astype(int)\n        yt = round(found_at[0][l]+height/2).astype(int)\n        xl = round(found_at[1][l]-width/2).astype(int)\n        xr = round(found_at[1][l]+width/2).astype(int)\n        show_match[yb: yt, xl: xr] = 1;\n    plt.imshow(show_match)\n    plt.title(\'Best Candidate\')\n    plt.show()\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\n\ndigits = load_digits()\nX, y = digits.data, digits.target\nfor i in [GaussianNB(), SVC(gamma=0.001)]:\n    (train_sizes,\n     train_scores,\n     test_scores) = learning_curve(i, X, y, cv=5)\n    test_mean = np.mean(test_scores, axis=1)\n    plt.plot(train_sizes, test_mean, label="Cross-validation score")\n\nplt.legend()\nplt.show()\n'
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; x = np.linspace(0, 10, 20)\n&gt;&gt;&gt; x\narray([ 0.        ,  0.52631579,  1.05263158,  1.57894737,  2.10526316,\n        2.63157895,  3.15789474,  3.68421053,  4.21052632,  4.73684211,\n        5.26315789,  5.78947368,  6.31578947,  6.84210526,  7.36842105,\n        7.89473684,  8.42105263,  8.94736842,  9.47368421, 10.        ])\n&gt;&gt;&gt; q = 0.5     # The continuous value between two discrete points\n&gt;&gt;&gt; y = q * np.round(x/q)\n&gt;&gt;&gt; y\narray([ 0. ,  0.5,  1. ,  1.5,  2. ,  2.5,  3. ,  3.5,  4. ,  4.5,  5.5,\n        6. ,  6.5,  7. ,  7.5,  8. ,  8.5,  9. ,  9.5, 10. ])\n'
"df = pd.DataFrame({'Time':['19:00-20:00', '20:00-21:00', '21:00-22:00']})\n\ndf[['first', 'second']] = (df['Time'].str.split('-', expand=True)\n                                     .add(':00')\n                                     .apply(pd.to_timedelta))\nprint (df)\n          Time    first   second\n0  19:00-20:00 19:00:00 20:00:00\n1  20:00-21:00 20:00:00 21:00:00\n2  21:00-22:00 21:00:00 22:00:00\n\nprint (df.dtypes)\nTime               object\nfirst     timedelta64[ns]\nsecond    timedelta64[ns]\ndtype: object\n\ndf[['first', 'second']] = df['Time'].str.split('-', expand=True).apply(pd.to_datetime)\nprint (df)\n          Time               first              second\n0  19:00-20:00 2019-08-04 19:00:00 2019-08-04 20:00:00\n1  20:00-21:00 2019-08-04 20:00:00 2019-08-04 21:00:00\n2  21:00-22:00 2019-08-04 21:00:00 2019-08-04 22:00:00\n\nprint (df.dtypes)\nTime              object\nfirst     datetime64[ns]\nsecond    datetime64[ns]\ndtype: object\n\ndf['hour1'] = df['first'].dt.hour\ndf['time1'] = df['first'].dt.time\nprint (df)\n          Time               first              second  hour1     time1\n0  19:00-20:00 2019-08-04 19:00:00 2019-08-04 20:00:00     19  19:00:00\n1  20:00-21:00 2019-08-04 20:00:00 2019-08-04 21:00:00     20  20:00:00\n2  21:00-22:00 2019-08-04 21:00:00 2019-08-04 22:00:00     21  21:00:00\n\nprint (df.dtypes)\nTime              object\nfirst     datetime64[ns]\nsecond    datetime64[ns]\nhour1              int64\ntime1             object\ndtype: object\n"
"print(f1_score(y_train, y_test, average='macro')) \nError: Found input variables with inconsistent numbers of samples: [6271, 3089]\n\nprint(f1_score(y_pred, y_test, average='macro')) \nError: continuous is not supported \n"
"p = np.linspace(0.001, 1 - 0.001, 1000)[:, None]\nq = 1 - p\nplt.plot(p, -p * np.log(p) - q * np.log(q), label='entropy')\nplt.plot(p, p * q, label='variance')\nplt.legend()\nplt.xlabel('probability')\n"
'sess.run([upm, upc])\n'
'from sklearn import svm\n\nn_samples = 100\nX = np.concatenate([np.random.normal(0,0.1,n_samples), np.random.normal(10,0.1,n_samples)]).reshape(-1,1)\ny = np.array([0]*n_samples+[1]*n_samples)\nclf = svm.LinearSVC(max_iter = 10000)\nclf.fit(X,y)  \nslope = clf.coef_\nintercept = clf.intercept_\nprint(slope, intercept)\nprint(-intercept/slope)\n'
'A = np.array([[.8, .6], [.1, 0]])\nB2 = tf.keras.utils.normalize(A, axis=0, order=2)\nprint(B2)\n\narray([[0.99227788, 1.        ],\n       [0.12403473, 0.        ]])\n\nB2_manual = np.zeros((2,2))\nB2_manual[0][0] = 0.8/np.sqrt(0.8 ** 2 + 0.1 ** 2)\nB2_manual[1][0] = 0.1/np.sqrt(0.8 ** 2 + 0.1 ** 2)\nB2_manual[0][1] = 0.6/np.sqrt(0.6 ** 2 + 0 ** 2)\nB2_manual[1][1] =  0 /np.sqrt(0.6 ** 2 + 0 ** 2)\nprint(B2_manual)\n\narray([[0.99227788, 1.        ],\n       [0.12403473, 0.        ]])\n'
'l1_reg_term = sum([tf.reduce_sum(tf.abs(_var)) for _var in para_list])\nreg_loss = loss + alpha * l1_reg_term\n'
'import numpy as np\n\ndef logloss(true_label, predicted, eps=1e-15):\n  p = np.clip(predicted, eps, 1 - eps)\n  if true_label == 1:\n    return -np.log(p)\n  else:\n    return -np.log(1 - p)\n\npredictions = np.array([0.25,0.65,0.2,0.51,\n                        0.01,0.1,0.34,0.97])\ntargets = np.array([1,0,0,0,\n                   0,0,0,1])\n\nll = [logloss(x,y) for (x,y) in zip(targets, predictions)]\nll\n# result:\n[1.3862943611198906,\n 1.0498221244986778,\n 0.2231435513142097,\n 0.7133498878774648,\n 0.01005033585350145,\n 0.10536051565782628,\n 0.41551544396166595,\n 0.030459207484708574]\n\nfrom sklearn.metrics import log_loss\n\nll_sk = log_loss(targets, predictions)\nll_sk\n# 0.4917494284709932\n\nnp.mean(ll)\n# 0.4917494284709932\n\nnp.mean(ll) == ll_sk\n# True\n'
'x = x.view(-1, 16*5*5)\n\nx = torch.flatten(x, start_dim=1)\n'
"df['percentage'] = df.mean(axis=1)\n\ndf['new'] = np.where(df['percentage'] &gt; 0.8, 1, 0)\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'var1':[0,0,1],'var2':[0,1,1], 'var3':[1,1,1]})\ndf['percentage'] = df.mean(axis=1)\ndf['new'] = np.where(df['percentage'] &gt; 0.8, 1, 0)\n\nprint(df)\n\n   var1  var2  var3  percentage  new\n0     0     0     1    0.333333    0\n1     0     1     1    0.666667    0\n2     1     1     1    1.000000    1\n"
" plt.scatter(X_test.ENGINESIZE, test_pred, color='yello') # , linewidth=1)\n\nplt.scatter(X_test.ENGINESIZE,Y_test,  color='gray')\nimport statsmodels.formula.api  as smf\ny = Y_train\nX = X_train\ndf = pd.DataFrame({'x' : X.ENGINESIZE, 'y': y})\nsmod = smf.ols(formula ='y~ x', data=df)\nresult = smod.fit()\nplt.plot(df['x'], result.predict(df['x']), color='red', linewidth=1)\nplt.show()\n\nprint(result.summary())\n"
"from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport io\nimport pandas as pd\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom object_detection.utils import dataset_util\nfrom collections import namedtuple, OrderedDict\n\nflags = tf.compat.v1.app.flags\nflags.DEFINE_string('csv_input', '', 'Path to the CSV input')\nflags.DEFINE_string('output_path', '', 'Path to output TFRecord')\nflags.DEFINE_string('image_dir', '', 'Path to images')\nFLAGS = flags.FLAGS\n\n\n# TO-DO replace this with label map\ndef class_text_to_int(row_label):\n    if row_label == 'Button':\n        return 1\n    if row_label == 'Text Box':\n        return 2\n    if row_label == 'Check Box':\n        return 3\n    if row_label == 'Link':\n        return 4\n    if row_label == 'Hyperlink':\n        return 5\n    if row_label == 'Icon':\n        return 6\n    if row_label == 'Text':\n        return 7\n    if row_label == 'Image':\n        return 8\n    else:\n        None\n\ndef split(df, group):\n    data = namedtuple('data', ['filename', 'object'])\n    gb = df.groupby(group)\n    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n\n\ndef create_tf_example(group, path):\n    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    width, height = image.size\n\n    filename = group.filename.encode('utf8')\n    image_format = b'jpg'\n    xmins = []\n    xmaxs = []\n    ymins = []\n    ymaxs = []\n    classes_text = []\n    classes = []\n\n    for index, row in group.object.iterrows():\n        xmins.append(row['xmin'] / width)\n        xmaxs.append(row['xmax'] / width)\n        ymins.append(row['ymin'] / height)\n        ymaxs.append(row['ymax'] / height)\n        classes_text.append(row['class'].encode('utf8'))\n        classes.append(class_text_to_int(row['class']))\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        'image/height': dataset_util.int64_feature(height),\n        'image/width': dataset_util.int64_feature(width),\n        'image/filename': dataset_util.bytes_feature(filename),\n        'image/source_id': dataset_util.bytes_feature(filename),\n        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n        'image/format': dataset_util.bytes_feature(image_format),\n        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n        'image/object/class/label': dataset_util.int64_list_feature(classes),\n    }))\n    return tf_example\n\n\ndef main(_):\n    writer = tf.io.TFRecordWriter(FLAGS.output_path)\n    path = os.path.join(FLAGS.image_dir)\n    examples = pd.read_csv(FLAGS.csv_input)\n    grouped = split(examples, 'filename')\n    for group in grouped:\n        tf_example = create_tf_example(group, path)\n        writer.write(tf_example.SerializeToString())\n\n    writer.close()\n    output_path = os.path.join(os.getcwd(), FLAGS.output_path)\n    print('Successfully created the TFRecords: {}'.format(output_path))\n\n\nif __name__ == '__main__':\n    tf.compat.v1.app.run() \n"
'low_inp_channels = int(int(inp_channels) * self.alpha_in)\nhigh_inp_channels = int(inp_channels) - low_inp_channels\n\nINPUT_SHAPE : (?, 224, 224, 3)\nHigh 2 low : (3, 3, 2, 9)\nHigh 2 high : (3, 3, 2, 7)\nLow 2 high : (3, 3, 1, 7)\nLow 2 low : (3, 3, 1, 9)\nHigh input shape : (?, 224, 224, 2)\nLow input shape : (?, 112, 112, 1)\nOUT HIGH HIGH shape : (?, 112, 112, 7)\nOUT LOW HIGH shape : (?, 112, 112, 7)\nOUT LOW LOW shape : (?, 56, 56, 9)\nOUT HIGH LOW shape : (?, 56, 56, 9)\nOUT HIGH shape : (?, 112, 112, 7)\nOUT LOW shape : (?, 112, 112, 9)\nOUT SHAPE : (?, 112, 112, 16)\n\nINPUT_SHAPE : (?, 112, 112, 16)\nHigh 2 low : (3, 3, 7, 9)\nHigh 2 high : (3, 3, 7, 7)\nLow 2 high : (3, 3, 9, 7)\nLow 2 low : (3, 3, 9, 9)\nHigh input shape : (?, 112, 112, 7)\nLow input shape : (?, 56, 56, 9)\nOUT HIGH HIGH shape : (?, 56, 56, 7)\nOUT LOW HIGH shape : (?, 56, 56, 7)\nOUT LOW LOW shape : (?, 28, 28, 9)\nOUT HIGH LOW shape : (?, 28, 28, 9)\nOUT HIGH shape : (?, 56, 56, 7)\nOUT LOW shape : (?, 56, 56, 9)\nOUT SHAPE : (?, 56, 56, 16)\n'
"logmmse.logmmse(A, r.rate, output_file = 'log.wav')\n"
'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#%matplotib inline\nfrom sklearn.manifold import TSNE\nfrom sklearn.datasets import load_digits\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndigits = load_digits()\ndigits_df = pd.DataFrame(digits.data,)\ndigits_df["target"] = pd.Series(digits.target)\n\ntsne = TSNE(n_components=3)\ndigits_tsne = tsne.fit_transform(digits_df.iloc[:,:64])\ndigits_df_tsne = pd.DataFrame(digits_tsne,\n                            columns =["Component1","Component2","Component3"])\n\nfinalDf = pd.concat([digits_df_tsne, digits_df["target"]], axis = 1)\n\n#Visualizing 3D\nfigure = plt.figure(figsize=(9,9))\naxes = figure.add_subplot(111,projection = "3d")\ndots = axes.scatter(xs = finalDf.to_numpy()[:,0],ys = finalDf.to_numpy()[:,1],zs = finalDf.to_numpy()[:,2],\n                   c = digits.target, cmap = plt.cm.get_cmap("nipy_spectral_r",10))\n'
'disc_loss = disc_loss + discriminator(batch, training=True)\n'
"nb_pipeline = Pipeline([('vect', CountVectorizer()),\n                        ('tfidf', TfidfTransformer())])\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnb_pipeline = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2)))])\n"
'class Trainer:\n    def __init__(self, model):\n        self.model = model\n        self.log_variable = []\n\n    def train(self, xs, ys, lambda):\n        for x,y in zip(xs,ys):\n            v = learn(x,y,lambda)\n            self.log_variable.append(v.numpy())\n\n    @tf.function\n    def learn(self, x, y, lambda):\n        err = y - self.model(x)\n        model.apply_weights( grad(err) * self.custom_alpha( self.model.weights )\n        return variable_that_I_want_to_save\n\n    def custom_optimizer( weights ):\n        x = some operations with weights \n        alpha = some operation with x\n        return alpha\n\n    def get_log(self):\n        return self.log_variable\n\ntrainer = Trainer(model)\ntrainer.train(xs, ys, lambda)\nlog = trainer.log_variable()\n'
'layers.Input(shape = input_shape[1:]),\n'
'    managment   marketing   customers   statistics  programming\nname1   1           1           0            0           0\nname2   0           0           0                1               1\nname3   0           0           1                1               0\n'
'Inputs strides: [(9223372036854775807, 4), (4, 9223372036854775807)]\n\nhttps://github.com/Theano/Theano/pull/2008\n'
"&gt;&gt;&gt; x = np.vstack([np.arange(100), np.arange(100)]).T\n&gt;&gt;&gt; y = x[:, 0] % 2\n&gt;&gt;&gt; from sklearn import svm\n&gt;&gt;&gt; clf = svm.SVC()\n&gt;&gt;&gt; clf.fit(x, y)\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n  shrinking=True, tol=0.001, verbose=False)\n&gt;&gt;&gt; clf.support_vectors_.shape\n(100, 2)\n\n&gt;&gt;&gt; clf.predict(x * 2)\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1])\n"
'text_data = load_files("C:/Users/USERNAME/projects/machine_learning/my_project/train", ...)\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDClassifier\ntext_clf = Pipeline([(\'vect\', CountVectorizer()),\n                    (\'tfidf\', TfidfTransformer()),\n                    (\'clf\', LinearSVC(loss=\'hinge\', penalty=\'l2\',\n                                            random_state=42)),\n])\n\n_ = text_clf.fit(text_data.data, text_data.target)\n\ndocs_new = ["Some test sentence here.",]\npredicted = text_clf.predict(docs_new)\n\nprint np.mean(predicted == text_data.target)\n\ndocs_new = ["Some test sentence here."]\ndocs_new_labels = [1] # correct label index of the document\n\npredicted = text_clf.predict(docs_new)\nprint np.mean(predicted == docs_new_labels) \n'
'[[5, 5, 5, 5, 5], [5, 5, 5, 5, 5]]\n\n[5, 5, 5, 5, 5, 5, 5, 5, 5, 5] \n'
'np.hsplit(np.dstack(rgb).flatten(), len(face.flatten()))\n\ndt_clf = dt_clf.fit(np.hsplit(np.dstack(rgb).flatten(), len(face.flatten())), \n                    face.flatten())\n\nred         green        blue \n\n1100        1100         0011  \n1100        1100         0001  \n0000        0000         0000\n\npredict = np.array([[[1,1,0,0],[1,1,0,0],[0,0,0,0]],\n                    [[1,1,0,0],[1,1,0,0],[0,0,0,0]],\n                    [[0,0,1,1],[0,0,0,1],[0,0,0,0]]])\n\npredicted = dt_clf.predict(np.hsplit(np.dstack(predict).flatten(),\n                           len(face.flatten())))\n\npredicted = np.array(np.hsplit(predicted, face.shape[0]))\n\narray([[1, 1, 0, 0],\n       [1, 1, 0, 0],\n       [0, 0, 0, 0]])\n'
'from sklearn import linear_model\nclf = linear_model.LinearRegression()\nclf.fit([[5, 8], [0 , 0]], [100, 0])\n\nprint(clf.predict([[2.5, 4], [5, 8], [0, 0]]))\n'
"from sklearn.metrics import f1_score\n\n# here we have only 4 labels of 4 samples\nf1_score([0,0,1,0],[0,0,0,0])\n/usr/local/lib/python3.4/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n'precision', 'predicted', average, warn_for)\n"
"data = pd.read_csv('data.txt',names=['X1','X2','Y'])\nclf = LDA()\nclf.fit(data.loc[:,'X1':'X2'].values, data.Y)\n"
'&gt;&gt;&gt; FPGrowth.train(\n... spark_df.rdd.map(lambda x: list(set(x))),\n... minSupport=0.2, numPartitions=10)\n'
"nrows = input_x.shape[0] - window_size + 1\np,q = input_x.shape\nm,n = input_x.strides\nstrided = np.lib.stride_tricks.as_strided\nout = strided(input_x,shape=(nrows,window_size,q),strides=(m,m,n))\n\nIn [83]: input_x\nOut[83]: \narray([[ 0.73089384,  0.98555845,  0.59818726],\n       [ 0.08763718,  0.30853945,  0.77390923],\n       [ 0.88835985,  0.90506367,  0.06204614],\n       [ 0.21791334,  0.77523643,  0.47313278],\n       [ 0.93324799,  0.61507976,  0.40587073],\n       [ 0.49462016,  0.00400835,  0.66401908]])\n\nIn [84]: window_size = 4\n\nIn [85]: out\nOut[85]: \narray([[[ 0.73089384,  0.98555845,  0.59818726],\n        [ 0.08763718,  0.30853945,  0.77390923],\n        [ 0.88835985,  0.90506367,  0.06204614],\n        [ 0.21791334,  0.77523643,  0.47313278]],\n\n       [[ 0.08763718,  0.30853945,  0.77390923],\n        [ 0.88835985,  0.90506367,  0.06204614],\n        [ 0.21791334,  0.77523643,  0.47313278],\n        [ 0.93324799,  0.61507976,  0.40587073]],\n\n       [[ 0.88835985,  0.90506367,  0.06204614],\n        [ 0.21791334,  0.77523643,  0.47313278],\n        [ 0.93324799,  0.61507976,  0.40587073],\n        [ 0.49462016,  0.00400835,  0.66401908]]])\n\nIn [86]: np.may_share_memory(out,input_x)\nOut[86]: True   # Doesn't guarantee, but is sufficient in most cases\n\nIn [87]: out[0] = 0\n\nIn [88]: input_x\nOut[88]: \narray([[ 0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        ],\n       [ 0.93324799,  0.61507976,  0.40587073],\n       [ 0.49462016,  0.00400835,  0.66401908]])\n"
'class Bunch(dict):\n    """Container object for datasets\n    Dictionary-like object that exposes its keys as attributes.\n    &gt;&gt;&gt; b = Bunch(a=1, b=2)\n    &gt;&gt;&gt; b[\'b\']\n    2\n    &gt;&gt;&gt; b.b\n    2\n    &gt;&gt;&gt; b.a = 3\n    &gt;&gt;&gt; b[\'a\']\n    3\n    &gt;&gt;&gt; b.c = 6\n    &gt;&gt;&gt; b[\'c\']\n    6\n    """\n\n    def __init__(self, **kwargs):\n        super(Bunch, self).__init__(kwargs)\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\n    def __dir__(self):\n        return self.keys()\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError:\n            raise AttributeError(key)\n\nBunch(data=data, target=target,\n                 target_names=target_names,\n                 DESCR=fdescr,\n                 feature_names=[\'feat_1\', \'feat_2\',\n                                \'feat_3\', \'feat_4\'])\n'
'import keras.backend as K\n\nmodel = Sequential()\nmodel.add(Lambda(lambda x: K.expand_dims(x, -1)))\nmodel.add(TimeDistributed(Convolution1D(1, 5, input_dim = one_input_length, border_mode = "same", W_regularizer = l2(0.01))))\nmodel.add(TimeDistributed(MaxPooling1D(10, border_mode = "same")))\nmodel.add(TimeDistributed(Convolution1D(1, 5, border_mode = "same", W_regularizer = l2(0.01))))\nmodel.add(TimeDistributed(MaxPooling1D(10, border_mode = "same")))\nmodel.add(Lambda(lambda x: K.squeeze(x, -1)))\nmodel.add(GRU(300, return_sequences = True, W_regularizer = l2(0.01), U_regularizer = l2(0.01)))\nmodel.add(TimeDistributed(Dense(2, activation=\'sigmoid\')))\n'
'sess.run([x, W, b])\n'
'import numpy as np\n\ndef sigmoid(x,derivative=False):\n    if(derivative==True):\n        return x*(1-x)\n    return 1/(1+np.exp(-x))\n\nnp.random.seed(1)\n\nweights = np.random.randn(1, 3)\n\ntraining = np.array([[np.array([0, 0, 0]).reshape(1, -1), 1],\n                    [np.array([0,0,1]).reshape(1, -1), 0],\n                    [np.array([0,1,0]).reshape(1, -1), 0],\n                    [np.array([0,1,1]).reshape(1, -1), 0],\n                    [np.array([1, 0, 0]).reshape(1, -1), 1],\n                    [np.array([1,0, 1]).reshape(1, -1), 0],\n                    [np.array([1,1,0]).reshape(1, -1), 0],\n                    [np.array([1,1,1]).reshape(1, -1), 1],\n\n                    ])\n\nfor iter in xrange(training.shape[0]):\n#forwardPropagation:\n        a_layer1 = training[iter][0]\n        z_layer2 = np.dot(weights,a_layer1.reshape(-1, 1))\n        a_layer2 = sigmoid(z_layer2)\n        hypothesis_theta = a_layer2\n\n#backPropagation:\n        delta_neuron1_layer2 =  (a_layer2 - training[iter][1] ) * sigmoid(a_layer2 , derivative=True)\n        Delta_neuron1_layer2 = np.dot(delta_neuron1_layer2 , a_layer1)\n        update = Delta_neuron1_layer2\n        weights = weights - update \n\n\nx = np.array([0,0, 1])\nprint sigmoid(np.dot(weights,x.reshape(-1, 1)))\n\nx = np.array([0,1,1])\nprint sigmoid(np.dot(weights,x.reshape(-1, 1)))\n\nx = np.array([1,1,1])\nprint sigmoid(np.dot(weights,x.reshape(-1, 1))) \n\n[[ 0.34224604]]\n[[ 0.19976054]]\n[[ 0.52710321]]\n'
"`Negative dimension size caused by subtracting 5 from 1 for 'conv1d'` \n"
"df = pd.read_csv('file.txt', delim_whitespace=True, header=None)\ndf.to_csv('file1.txt', index=False, header=None)\n\ndf = pd.read_csv('file.txt', sep='\\s+', header=None)\ndf.to_csv('file1.txt', index=False, header=None)\n"
'import pandas as pd\nimport numpy as np\nfrom io import StringIO\n\nmystr = StringIO("""FEATURE_1   FEATURE_2   FEATURE_3   FEATURE_4\n1               1         &lt;1.5        &gt;3.4\nNaN             2           2           4\n4            CANCELED       3          4.5\n1.34            2         &lt;1.5         &lt;2""")\n\n# replace mystr with \'file.csv\'\ndf = pd.read_csv(mystr, delim_whitespace=True)\n\n# define float converter check\ndef converter(x):\n    try:\n        x = float(x)\n        return np.nan\n    except ValueError:\n        return x\n\n# use list comprehension to apply function and clean up\nres = {col: df[col].apply(converter).dropna()\\\n                   .drop_duplicates().tolist() for col in df}\n\n{\'FEATURE_1\': [],\n \'FEATURE_2\': [\'CANCELED\'],\n \'FEATURE_3\': [\'&lt;1.5\'],\n \'FEATURE_4\': [\'&gt;3.4\', \'&lt;2\']}\n'
'features = np.zeros((len(samples), n_features))\nfor i, s in enumerate(samples):\n   features[i] = feature_extraction(s)\npreds = clf.predict(features)\n'
'x = train_data[columns_of_interest]\n\nx = filtered_titanic_data[columns_of_interest]\n'
'X_mean = X_train.mean(axis=0)\nX_train -= X_mean\nX_std = X_train.std(axis=0)\nX_train /= X_std + 1e-8\n'
"from sklearn.linear_model import LinearRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom numpy.random import rand\n\nx_train = rand(1000,1)\ny_train = rand(1000,1)\n\npoly = PolynomialFeatures(degree=8, include_bias=False) #the bias is avoiding the need to intercept\nx_new = poly.fit_transform(x_train)\nnew_model = LinearRegression()\nnew_model.fit(x_new,y_train)\n\n#plotting\ny_prediction = new_model.predict(x_new) #this predicts y\nplt.scatter(x_train,y_train)\nplt.plot(x_new[:,0], y_prediction, 'r')\nplt.legend(['Predicted line', 'Observed data'])\nplt.show()\n\nprint(new_model.predict(poly.fit_transform(0.25)))\n"
"import pandas as pd\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import precision_score\n\n# Import datasets from edge node\ndata_train = pd.read_csv('data.csv')\ndata_valid = pd.read_csv('data_valid.csv')\n\n# training data &amp; labels:\nX = data_train.iloc[:, :-1].values\ny = data_train.iloc[:, -1].values   \n\n# validation data &amp; labels:\nX_valid = data_valid.iloc[:, :-1].values\ny_true = data_valid.iloc[:, -1].values \n\nn_estimators = [150, 175, 200]\nperf = []\n\nfor k_estimators in n_estimators:\n    clf = XGBClassifier(n_estimators=k_estimators, random_state=0)\n    clf.fit(X, y)\n\n    y_predict = clf.predict(X_valid)\n    precision = precision_score(y_true, y_predict, average='binary')\n    perf.append(precision)\n"
'from keras import backend as K\n\ninput_image = Input(shape=(128,128,1))\n\nunet_out = model_unet(input_image)\nrgb_image = Lambda(lambda x: K.repeat_elements(x, 3, -1))(input_image)\nresnet_out = model_resnet(rgb_image)\n\noutput = Average()([unet_out, resnet_out])\n\nensemble_model = Model(input_image, output)\n\npred_val = ensemble_model.predict(X_val)\n\nX_val_rgb = np.repeat(X_val, 3, -1)\n\npred_val = ensemble_model.predict([X_val, X_val_rgb])\n'
'from nltk.tag import StanfordPOSTagger\njar = \'D:/Downloads/stanford-postagger-full-2018-10-16/stanford-postagger-3.9.2.jar\'\nmodel = \'D:/Downloads/stanford-postagger-full-2018-10-16/models/spanish.tagger\'\n\nimport os\njava_path = "C:/Program Files/Java/jre1.8.0_191/bin/java.exe"\nos.environ[\'JAVAHOME\'] = java_path\n\npos_tagger = StanfordPOSTagger(model, jar, encoding=\'utf8\' )\npos_tagger.tag(\'El gato está bajo la mesa de cristal\'.split())\n\n[(\'El\', \'da0000\'),\n (\'gato\', \'nc0s000\'),\n (\'está\', \'vmip000\'),\n (\'bajo\', \'sp000\'),\n (\'la\', \'da0000\'),\n (\'mesa\', \'nc0s000\'),\n (\'de\', \'sp000\'),\n (\'cristal\', \'nc0s000\')]\n'
'print(np.all(np.isinf(standardized_data)))\n...\n\na = [np.inf, 0, 1]\n\nnp.all(np.isinf(a))\n#False\n\nnp.any(np.isinf(a))\n#True\n'
'inp_img = Input(shape=image_shape)\ninp_others = Input(shape=others_shape)\n\n# branch 1: process input image\nx = Conv2D(...)(inp_img)\nx = Conv2D(...)(x)\nx = MaxPool2D(...)(x)\nout_b1 = Flatten()(x)\n\n# branch 2: process other input\nout_b2 = Dense(...)(inp_other)\n\n\n# merge the results by concatenation\nmerged = concatenate([out_b1, out_b2])\n\n# pass merged tensor to some other layers\nx = Dense(...)(merged)\noutput = Dense(...)(x)\n\n# build the model and compile it\nmodel = Model([inp_img, inp_other], output)\nmodel.compile(...)\n\n# fit on training data\nmodel.fit([img_array, other_array], label_array, ...)\n'
"def calcWSumF(x, idx):\n    w_vec = K.constant(np.arange(idx))\n    y = K.sum(x[:, 0:idx] * w_vec, axis=-1, keepdims=True)\n    return y\n\nd_inputs = Input((15,))\nwsum_first = Lambda(calcWSumF, arguments={'idx': 10})(d_inputs)\nd_input = concatenate([d_inputs, wsum_first], axis=-1)\n\nmodel = Model(d_inputs, d_input)\nmodel.predict(np.arange(15).reshape(1, 15))\n\n# output:\narray([[  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n         11.,  12.,  13.,  14., 285.]], dtype=float32)\n\n# Note: 0*0 + 1*1 + 2*2 + ... + 9*9 = 285\n"
'model.compile(loss=\'binary_crossentropy\', optimizer="sgd", metrics=[\'accuracy\'])\n'
"df.groupby(['placeID','latitude','longitude','price','Rcuisine']).mean()\n"
'clf = DecisionTreeClassifier(random_state=42) \n'
'def my_generator(args):\n    # ...\n    yield [first_pair, second_pair], labels\n'
'import numpy as np\nimport cv2\nfrom PIL import ImageGrab\n\nfourcc = cv2.VideoWriter_fourcc(*\'XVID\')\n\nface_csc = cv2.CascadeClassifier(\'new_cascade.xml\')\n\nout = cv2.VideoWriter("test_output.avi", fourcc, 5.0, (1366, 768))\n\nwhile True:\n\n    img = ImageGrab.grab(bbox=(100, 10, 750, 750))\n    # convert image to numpy array\n    img_np = np.array(img)\n    # convert color space from BGR to RGB\n    frame = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n    # show image on OpenCV frame\n    faces = face_csc.detectMultiScale(frame, 1.1 , 4)\n\n    for (x,y,w,h) in faces:\n        cv2.rectangle(frame,(x,y),(x+w,y+h), (255,0,0), 2)\n        roi_gray = frame[y:y+h, x:x+w]\n        roi_color = img_np[y:y+h,x:x+w]\n        if cv2.waitKey(1) == 27:\n            break\n\n    cv2.imshow("stream", frame)\n    # write frame to video writer\n    out.write(frame)\n\ncv2.waitKey(0)\nout.release()\n'
"#for valid set \n\nv = valid.reshape(15150,)\n\nor_fpath = '/content/food-101/images/' #path of original folder\ncp_fpath = '/content/food101/valid/'   #path of destination folder\n\nfor y in tqdm(v):\n\n foldername = y.split('/')[0]\n\n img = y.split('/')[1] +'.jpg'\n\n ip_path = or_fpath+foldername\n op_path = cp_fpath+foldername\n\n if not os.path.exists(op_path):\n   os.mkdir(op_path)\n\n os.rename(os.path.join(ip_path, img), os.path.join(op_path, img))\n\n\n\n"
'if __name__ == "__main__":\n'
"param_grid = {'svc__C':[1,5,10,50],\n             'svc__gamma':[0.0001, 0.0005, 0.001, 0.005]}\n"
'from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer()\n\nfrom sklearn.impute ...\n\nimport SimpleImputer\n'
'# Output tensor has shape [2, 3].\nfill([2, 3], 0.9) ==&gt; [[0.9, 0.9, 0.9]\n                 [0.9, 0.9, 0.9]]\n'
"&gt;&gt;&gt; from sklearn.utils.extmath import weighted_mode\n&gt;&gt;&gt; x = [4, 1, 4, 2, 4, 2]\n&gt;&gt;&gt; weights = [1, 1, 1, 1, 1, 1]\n&gt;&gt;&gt; weighted_mode(x, weights)\n(array([4.]), array([3.]))\nThe value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.\n\n&gt;&gt;&gt;\n&gt;&gt;&gt; weights = [1, 3, 0.5, 1.5, 1, 2]  # deweight the 4's\n&gt;&gt;&gt; weighted_mode(x, weights)\n(array([2.]), array([3.5]))\n"
"print('Actual:', y_test[index_to_predict])\n"
'lr = 1e-3\nfor i in range(100):\n  # 100 updates\n  loss = 1 - f(sm, freq_m)\n  print(loss)\n  loss.backward()\n  with torch.no_grad():\n    sm -= lr * sm.grad\n    freq_m -= lr * freq_m.grad\n    # Manually zero the gradients after updating weights\n    sm.grad.zero_()\n    freq_m.grad.zero_()\n'
"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nx = ['Apple', 'Orange', 'Apple', 'Pear']\ny = label_encoder.fit_transform(x)\nprint(y)\n\narray([0, 1, 0, 2])\n\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import OneHotEncoder\nonehot_encoder = OneHotEncoder(sparse=False)\ny = y.reshape(len(y), 1)\nonehot_encoded = onehot_encoder.fit_transform(y)\nprint(onehot_encoded)\n\n[[1. 0. 0.]\n[0. 1. 0.]\n[1. 0. 0.]\n[0. 0. 1.]]\n"
'input_layer = Input((1, 1000, 597))\n\ninput_layer = Input((1000, 597, 1))\n\n(batch_size, 1, 1000, 597)\n\n(batch_size, 1000, 597, 1)\n'
"face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n\nface_cascade = cv2.CascadeClassifier(os.path.join(cv2.data.haarcascades, 'haarcascade_frontalface_default.xml'))\neye_cascade = cv2.CascadeClassifier(os.path.join(cv2.data.haarcascades, 'haarcascade_eye.xml'))\n\nprint(face_cascade.empty())\nprint(eye_cascade.empty())\n"
'my_int = 1\nsecond_array = [(my_int, my_int) for i in range(8)]\n\nI_tried = (my_array, second_array)\n\n'
'&gt;&gt;&gt; 2**np.arange(30)\n\narray([        1,         2,         4,         8,        16,        32,\n              64,       128,       256,       512,      1024,      2048,\n            4096,      8192,     16384,     32768,     65536,    131072,\n          262144,    524288,   1048576,   2097152,   4194304,   8388608,\n        16777216,  33554432,  67108864, 134217728, 268435456, 536870912])\n'
'tree_disp = plot_partial_dependence(tree, X, ["LSTAT", "RM"], ax=ax)\n'
'import xgboost as xgb\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(noise=0.3, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train, y_train)\n\nprint(xgb_clf.predict(X_test))\nprint(xgb_clf.predict_proba(X_test))\n\n\n[1 1 1 0 1 0 1 0 0 1]\n[[0.0394336  0.9605664 ]\n [0.03201818 0.9679818 ]\n [0.1275925  0.8724075 ]\n [0.94218    0.05782   ]\n [0.01464975 0.98535025]\n [0.966953   0.03304701]\n [0.01640552 0.9835945 ]\n [0.9297296  0.07027044]\n [0.9580196  0.0419804 ]\n [0.02849442 0.9715056 ]]\n'
'import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\n\ndf = pd.read_csv(\'C:\\\\Users\\\\ryans\\\\OneDrive\\\\Desktop\\\\mushrooms.csv\')\n\ndf.columns\n\ndf.head(5)\n\n# The data is categorial so I convert it with LabelEncoder to transfer to ordinal.\n\nlabelencoder=LabelEncoder()\nfor column in df.columns:\n    df[column] = labelencoder.fit_transform(df[column])\n\n#df.describe()\n\n\n#df=df.drop(["veil-type"],axis=1)\n\n#df_div = pd.melt(df, "class", var_name="Characteristics")\n#fig, ax = plt.subplots(figsize=(10,5))\n#p = sns.violinplot(ax = ax, x="Characteristics", y="value", hue="class", split = True, data=df_div, inner = \'quartile\', palette = \'Set1\')\n#df_no_class = df.drop(["class"],axis = 1)\n#p.set_xticklabels(rotation = 90, labels = list(df_no_class.columns));\n\n#plt.figure()\n#pd.Series(df[\'class\']).value_counts().sort_index().plot(kind = \'bar\')\n#plt.ylabel("Count")\n#plt.xlabel("class")\n#plt.title(\'Number of poisonous/edible mushrooms (0=edible, 1=poisonous)\');\n\n\nplt.figure(figsize=(14,12))\nsns.heatmap(df.corr(),linewidths=.1,cmap="YlGnBu", annot=True)\nplt.yticks(rotation=0);\n\ndfDummies = pd.get_dummies(df)\n\nplt.figure(figsize=(14,12))\nsns.heatmap(dfDummies.corr(),linewidths=.1,cmap="YlGnBu", annot=True)\nplt.yticks(rotation=0);\n'
'&gt;&gt;&gt; print(categorical_crossentropy(to_categorical(16, num_classes = 27),\n                                   y_pred, from_logits = False).numpy())\n2.264979e-06\n\nprint(sparse_categorical_crossentropy(16, y_pred))\n'
'test_predictions = min_max_scaler.inverse_transform(test_predictions.reshape(-1,1)).reshape(test_predictions.shape)\n'
"decoded_image = Conv2D(3, (3, 3), activation='sigmoid', padding='valid')(x)\n"
"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\ntraining_data = pd.read_csv(&quot;/Users/aus10/Desktop/PGA/History/Memorial/PGA_Training_data.csv&quot;)\n\ntest_data = pd.read_csv(&quot;/Users/aus10/Desktop/PGA/History/Memorial/PGA_Test_Data.csv&quot;)\n\nX = training_data.iloc[:,1:4]  #independent columns\ny = training_data.iloc[:,-1]   #target column\n\ndegree = 2 \n\nmodel=make_pipeline(PolynomialFeatures(degree), LinearRegression())\n\nmodel.fit(X,y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2)\n\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nprint('MSE train: %.3f, test: %.3f' % (\n    round(mean_squared_error(y_train, y_train_pred),2),\n    round(mean_squared_error(y_test, y_test_pred),2)\n))\n\nprint('R^2 train: %.3f, test: %.3f' % (r2_score(y_train, y_train_pred), r2_score(y_test, y_test_pred)))\n\nresults = []\n\nindex = 0\ncount = 0\n\nwhile count &lt; len(test_data):\n    name = test_data.loc[index].at['Player_Name']\n    Scrambling = test_data.loc[index].at['Scrambling']\n    Total_Putts_GIR = test_data.loc[index].at['Total_Putts_GIR']\n    SG_Putting = test_data.loc[index].at['SG_Putting']\n\n    Xnew = [[ Scrambling, Total_Putts_GIR, SG_Putting ]]\n    # make a prediction\n    ynew = model.predict(Xnew)\n    # show the inputs and predicted outputs\n    results.append(\n        {\n            'Name': name,\n            'Projection': (round(ynew[0],2))\n        }\n        )\n    index += 1\n    count += 1\nsorted_results = sorted(results, key=lambda k: k['Projection'], reverse=True)\n\ndf = pd.DataFrame(sorted_results, columns=[\n    'Name', 'Projection'])\nwriter = pd.ExcelWriter('/Users/aus10/Desktop/PGA/Regressions/Linear_Regressions/Results/Projections_LR_LR.xlsx', engine='xlsxwriter')\ndf.to_excel(writer, sheet_name='Sheet1', index=False)\ndf.style.set_properties(**{'text-align': 'center'})\npd.set_option('display.max_colwidth', 100)\npd.set_option('display.width', 1000)\nwriter.save()\n"
"train_datagen = ImageDataGenerator(\n    rescale=1./255,\n    validation_split=0.3)   #Splits the data 70/30 for training and validation\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_data_dir,\n        color_mode='grayscale', \n        target_size=(img_width, img_height),\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=True,\n        subset='training')\n\nvalidation_generator = train_datagen.flow_from_directory(\n        train_data_dir,  \n        color_mode='grayscale',\n        target_size=(img_width, img_height),\n        batch_size=batch_size,\n        class_mode='categorical',\n        shuffle=True,\n        subset='validation')\n\nTestAndTrain\n -Train\n   -in\n   -notin\n -Test\n   -in\n   -notin\n"
'self.kernel = self.add_weight(&quot;kernel&quot;,\n                               shape=[int(input_shape[-1]), self.num_outputs],\n                               regularizer=tf.keras.regularizers.l1_l2())\n\nfrom tensorflow.keras import regularizers\n\nclass MyDenseLayer(tf.keras.layers.Layer):\n  def __init__(self, num_outputs, kernel_regularizer=None):\n    super(MyDenseLayer, self).__init__()\n    self.num_outputs = num_outputs\n    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n\n  def build(self, input_shape):\n    self.kernel = self.add_weight(&quot;kernel&quot;,\n                                  shape=[int(input_shape[-1]), self.num_outputs],\n                                  regularizer=self.kernel_regularizer)\n'
'from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n#Classifiers \nclassifiers = [KNeighborsClassifier(30),\n                DecisionTreeClassifier(),\n                RandomForestClassifier(),\n                AdaBoostClassifier(),\n                LogisticRegression()]\n\nfrom sklearn.datasets import load_iris\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nk = 5\npreds = pd.DataFrame(index=[*range(k)])\n\nfor cls in classifiers:\n    scores = cross_val_score(cls, X, y, cv=k, scoring=&quot;accuracy&quot;)\n    preds[type(cls).__name__] = scores\n\nprint(preds)\n   KNeighborsClassifier  DecisionTreeClassifier  RandomForestClassifier  \\\n0              0.900000                0.966667                0.966667   \n1              0.966667                0.966667                0.966667   \n2              0.933333                0.900000                0.933333   \n3              0.900000                0.966667                0.966667   \n4              1.000000                1.000000                1.000000   \n\n   AdaBoostClassifier  LogisticRegression  \n0            0.966667            0.966667  \n1            0.933333            1.000000  \n2            0.900000            0.933333  \n3            0.933333            0.966667  \n4            1.000000            1.000000   \n'
'(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n'
"model.add(Dense(1))\nmodel.add(Activation('softmax'))\n\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\n\nDATADIR = &quot;/Users/...pathname&quot;\nCATEGORIES = [&quot;disease0&quot;, &quot;disease1&quot;, &quot;disease2&quot;, &quot;disease3&quot;, &quot;non_disease&quot;]\n\ntrain_x = []\ntrain_y = []\nIMG_SIZE = 228 #for resizing the image, but need to find the right size for this, (5472,3648) is original image size\n\ndef creating_training_data():\n    for category in CATEGORIES:\n        labels = [0, 0, 0, 0, 0]\n        path = os.path.join(DATADIR, category)  # gets us into the path for 5 diseases directory\n        class_num = CATEGORIES.index(category) #assign one hot encoding to each disease.. [1,0,0,0,0]\n        labels[class_num] = 1\n        for img in os.listdir(path):\n            try:\n                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)  # convert images to an array, IMREAD_COLOR for rgb\n                new_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA) #would be (5472,3648) at full size. INTER_AREA for shrinking an image\n                train_x.append(new_array) #would be img_array if not resizing\n                train_y.append(labels)\n            except Exception as e:\n                pass\ntrain_X = np.array(train_X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ntrain_X = train_X/255.0  #may need to use keras.utils.normalize to perform this instead\n\nprint(train_X.shape)\nprint(train_y.shape)\n#Creating our CNN\nmodel = Sequential()\nmodel.add(Conv2D(64, (3,3), input_shape=train_X.shape[1:])) #skip the -1...using the shape of the data (228,228,1)\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(64, (3,3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2))) # Now we have a 2x64 CNN\n\nmodel.add(Flatten()) #Flatten the data because Convolutional is 2D whereas the dense layer wants a 1D data set\nmodel.add(Dense(64))\n\n#Adding Output Layer\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss=&quot;categorical_crossentropy&quot;,\n              optimizer=&quot;adam&quot;,\n              metrics=[&quot;accuracy&quot;])\n\nmodel.fit(train_X, train_y, batch_size=5, epochs=10,validation_split=0.2, shuffle=True)\n"
"import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\nreg = LinearRegression()\nreg.fit(X, y)\n\n# save it\n\nimport pickle\n\nfilename = 'model1.pkl'\npickle.dump(reg, open(filename, 'wb'))\n\nloaded_model = pickle.load(open(filename, 'rb'))\n\ntype(loaded_model)\n# sklearn.linear_model._base.LinearRegression\n\nloaded_model\n# LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n"
"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nSEED = 2020  # for reproducibility due to the shuffling\n\n# create some random classification data - make it small for printing out\nX, Y = make_classification(n_samples=20, n_features=3, n_informative=3,\n                           n_redundant=0, n_repeated=0, n_clusters_per_class=1,\n                           n_classes=3, random_state=SEED)\n\nprint(&quot;X Original: \\n{}\\n&quot;.format(X))\nprint(&quot;Y Original: \\n{}\\n&quot;.format(Y))\n\n# perform stratified shuffle split. Note the SEED usage for shuffling.\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=SEED)\ntrain_index, test_index = next(sss.split(X, Y))\nX_train, X_test = X[train_index], X[test_index]\nY_train, Y_test = Y[train_index], Y[test_index]\n\nprint(&quot;X_Train: \\n{}\\n&quot;.format(X_train))\nprint(&quot;Y_Train: \\n{}\\n&quot;.format(Y_train))\nprint(&quot;X_Test: \\n{}\\n&quot;.format(X_test))\nprint(&quot;Y_Test: \\n{}\\n&quot;.format(Y_test))\n\n# your code for saving X_train and X_test in separate NPY files, goes here\n# your code for saving Y_train and Y_test in separate CSV files, goes here\n\nX Original: \n[[-0.69590064 -0.67561329  0.62524618]\n [-1.09492175  1.27630932  2.15598887]\n [-0.51743065  0.63402055  2.12912755]\n [-1.18819319 -0.42454412  1.49949316]\n [-2.09612492  0.89610929 -0.34134785]\n [ 1.06615086 -2.74141467 -0.26813435]\n [-0.88205757  0.84812284 -0.65742989]\n [-0.95747896 -1.70466278  0.69822828]\n [-0.15885567 -0.15289292 -1.00694331]\n [-0.93374229 -0.79402593  1.00909515]\n [-0.90636868  2.75448909  1.772864  ]\n [ 0.62005229 -1.3732454  -0.39237323]\n [ 0.74139934 -1.05271986 -0.9964703 ]\n [-1.81968206  1.53213677 -0.94698653]\n [-0.43419928  0.90834502  2.05707125]\n [-0.19206677  0.3104947   0.11505178]\n [-0.19129044 -0.39785095 -0.13277081]\n [-1.64958117  1.57707358  0.67063495]\n [-1.27544266 -1.26647034  1.3965837 ]\n [ 1.63351975 -0.85734405 -1.52143762]]\n\nY Original: \n[1 0 0 1 0 2 2 1 2 1 0 2 2 0 0 1 1 0 1 2]\n\nX_Train: \n[[-0.51743065  0.63402055  2.12912755]\n [ 1.63351975 -0.85734405 -1.52143762]\n [-0.93374229 -0.79402593  1.00909515]\n [ 0.74139934 -1.05271986 -0.9964703 ]\n [ 1.06615086 -2.74141467 -0.26813435]\n [-2.09612492  0.89610929 -0.34134785]\n [-1.27544266 -1.26647034  1.3965837 ]\n [-0.15885567 -0.15289292 -1.00694331]\n [-0.19206677  0.3104947   0.11505178]\n [-0.43419928  0.90834502  2.05707125]\n [-1.64958117  1.57707358  0.67063495]\n [-1.18819319 -0.42454412  1.49949316]\n [-0.95747896 -1.70466278  0.69822828]\n [-1.81968206  1.53213677 -0.94698653]]\n\nY_Train: \n[0 2 1 2 2 0 1 2 1 0 0 1 1 0]\n\nX_Test: \n[[-0.88205757  0.84812284 -0.65742989]\n [-0.90636868  2.75448909  1.772864  ]\n [-0.69590064 -0.67561329  0.62524618]\n [ 0.62005229 -1.3732454  -0.39237323]\n [-0.19129044 -0.39785095 -0.13277081]\n [-1.09492175  1.27630932  2.15598887]]\n\nY_Test: \n[2 0 1 2 1 0]\n\ncrowd_annotations_ = pd.read_csv(path + 'crowd_annotationsGold.csv', encoding = 'utf-8')\n\ncrowd_annotations = crowd_annotations_['label'].to_numpy()\n\ncrowd_annotations_train = crowd_annotations_.iloc[train_index]\ncrowd_annotations_test = crowd_annotations_.iloc[test_index]\n# save crowd_annotations_train and crowd_annotations_test as `CSV`\n"
"multilabel = MultiLabelBinarizer()\ny = multilabel.fit_transform('target_labels')\n\npredicted_list = classifier.predict_proba(X_test)\n\ndef get_labels(predicted_list):\n    mlb =[(i1,c1)for i1, c1 in enumerate(multilabel.classes_)]    \n    temp_list = sorted([(i,c) for i,c in enumerate(list(predicted_list))],key = lambda x: x[1], reverse=True)\n    tag_list = [item1 for item1 in temp_list if item1[1]&gt;=0.35] # here 0.35 is the threshold i choose\n    tags = [item[1] for item2 in tag_list[:5] for item in mlb if item2[0] == item[0] ] # here I choose to get top 5 labels only if there are more than that\n    return tags\n\nget_tags(predicted_list[0]) \n&gt;&gt; ['New York']\n"
'&gt;&gt;&gt; s\n0    a\n1    b\n2    c\n3    a\ndtype: object\n\n&gt;&gt;&gt; pd.get_dummies(s)\n   a  b  c\n0  1  0  0\n1  0  1  0\n2  0  0  1\n3  1  0  0\n\n&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder\n\n&gt;&gt;&gt; enc = OneHotEncoder()\n&gt;&gt;&gt; a = np.array([1, 1, 3, 2, 2]).reshape(-1, 1)\n&gt;&gt;&gt; a\narray([[1],\n       [1],\n       [3],\n       [2],\n       [2]]\n\n&gt;&gt;&gt; one_hot = enc.fit_transform(a)\n&gt;&gt;&gt; one_hot.toarray()\narray([[ 1.,  0.,  0.],\n       [ 1.,  0.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  1.,  0.]])\n'
"from __future__ import division\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.feature_extraction.dict_vectorizer import DictVectorizer\nfrom sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.metrics.classification import classification_report, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree.tree import DecisionTreeClassifier\n\nimport numpy as np\nimport pandas as pd\n\n\n# Read the data into a pandas dataframe\ndf = pd.read_csv('adult.data.csv')\n\n# Columns names\ncols = np.array(['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n                 'marital-status', 'occupation', 'relationship', 'race', 'sex',\n                 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n                 'target'])\n\n# numeric columns\nnumeric_cols = ['age', 'fnlwgt', 'education-num',\n                'capital-gain', 'capital-loss', 'hours-per-week']\n\n# assign names to the columns in the dataframe\ndf.columns = cols\n\n# replace the target variable to 0 and 1 for &lt;50K and &gt;50k\ndf1 = df.copy()\ndf1.loc[df1['target'] == ' &lt;=50K', 'target'] = 0\ndf1.loc[df1['target'] == ' &gt;50K', 'target'] = 1\n\n# split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    df1.drop('target', axis=1), df1['target'], test_size=0.2)\n\n\n# numeric attributes\n\nx_num_train = X_train[numeric_cols].as_matrix()\nx_num_test = X_test[numeric_cols].as_matrix()\n\n# scale to &lt;0,1&gt;\n\nmax_train = np.amax(x_num_train, 0)\nmax_test = np.amax(x_num_test, 0)        # not really needed\n\nx_num_train = x_num_train / max_train\nx_num_test = x_num_test / max_train        # scale test by max_train\n\n# labels or target attribute\n\ny_train = y_train.astype(int)\ny_test = y_test.astype(int)\n\n# categorical attributes\n\ncat_train = X_train.drop(numeric_cols, axis=1)\ncat_test = X_test.drop(numeric_cols, axis=1)\n\ncat_train.fillna('NA', inplace=True)\ncat_test.fillna('NA', inplace=True)\n\nx_cat_train = cat_train.T.to_dict().values()\nx_cat_test = cat_test.T.to_dict().values()\n\n# vectorize (encode as one hot)\n\nvectorizer = DictVectorizer(sparse=False)\nvec_x_cat_train = vectorizer.fit_transform(x_cat_train)\nvec_x_cat_test = vectorizer.transform(x_cat_test)\n\n# build the feature vector\n\nx_train = np.hstack((x_num_train, vec_x_cat_train))\nx_test = np.hstack((x_num_test, vec_x_cat_test))\n\n\nclf = LogisticRegression().fit(x_train, y_train.values)\npred = clf.predict(x_test)\nprint classification_report(y_test.values, pred, digits=4)\nprint accuracy_score(y_test.values, pred)\n\nclf = DecisionTreeClassifier().fit(x_train, y_train)\npredict = clf.predict(x_test)\nprint classification_report(y_test.values, pred, digits=4)\nprint accuracy_score(y_test.values, pred)\n\nclf = GaussianNB().fit(x_train, y_train)\npredict = clf.predict(x_test)\nprint classification_report(y_test.values, pred, digits=4)\nprint accuracy_score(y_test.values, pred)\n"
'from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, KFold\nimport numpy as np\n\nif __name__ ==\'main\':\n\n    clf = RandomForestRegressor(n_estimators=250, max_features = 0.8, verbose = 2)\n    score = cross_val_score(estimator = clf, X = X1, y = Y1, cv = KFold(n_splits = 5, random_state = 100), n_jobs = -1,scoring = "neg_mean_squared_error")\n    np.mean([np.sqrt(-x) for x in score])\n'
'import numpy as np\nfrom patsy import ContrastMatrix\n\nclass FullRankOneHot(object):\n    def __init__(self, reference=0):\n        self.reference = reference\n\n    # Called to generate a full-rank encoding\n    def code_with_intercept(self, levels):\n        return ContrastMatrix(np.eye(len(levels)),\n                              ["[My.%s]" % (level,) for level in levels])\n\n    # Called to generate a non-full-rank encoding. But we don\'t care,\n    # we do what we want, and return a full-rank encoding anyway.\n    # Take that, patsy.\n    def code_without_intercept(self, levels):\n        return self.code_with_intercept(levels)\n'
'num_complete_minibatches = int(math.floor(m / mini_batch_size))\n'
'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# Model input and output\nx = tf.placeholder(tf.float32, [None, 1])\n\n# training data\nx_plot = np.arange(0, math.pi*2*2, 0.1)\nx_train = x_plot.reshape(-1, 1)\ny_train_tf = tf.sin(x)\n\n# Model parameters\nW1 = tf.Variable(tf.random_normal([1,10], stddev=0.03), dtype=tf.float32, name=\'W1\')\nb1 = tf.Variable(tf.random_normal([10], stddev=0.03), dtype=tf.float32, name=\'b1\')\nW2 = tf.Variable(tf.random_normal([10,3], stddev=0.03), dtype=tf.float32, name=\'W2\')\nb2 = tf.Variable(tf.random_normal([3], stddev=0.03), dtype=tf.float32, name=\'b2\')\nW3 = tf.Variable(tf.random_normal([3,1], stddev=0.03), dtype=tf.float32, name=\'W3\')\nb3 = tf.Variable(tf.random_normal([1], stddev=0.03), dtype=tf.float32, name=\'b3\')\n\nlayer1 = tf.tanh(tf.multiply(x,W1) + b1)\nlayer2 = tf.tanh(tf.matmul(layer1, W2) + b2)\nlinear_model = tf.reduce_sum(tf.matmul(layer2, W3) + b3, 1, keep_dims=True)\n\n# loss\n#loss = tf.reduce_sum(tf.square(linear_model - y_train_tf)) # sum of the squares\nloss = tf.losses.mean_squared_error(y_train_tf,linear_model)\n\ntf.summary.scalar(\'loss\', loss)\n# optimizer\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(loss)\n\n# training loop\ninit = tf.global_variables_initializer()\nsess = tf.Session()\n# Merge all the summaries\nmerged = tf.summary.merge_all()\ntrain_writer = tf.summary.FileWriter(\'train_tensorboard\',sess.graph)\n\nsess.run(init) # reset values to wrong\n\nfig, ax = plt.subplots()\n\nfor i in range(40000):\n    summary, f_predict, _ = sess.run([merged, linear_model, train], feed_dict={x: x_train})\n    y_train, curr_layer1, curr_layer2, curr_W1, curr_b1, curr_W2, curr_b2, curr_W3, curr_b3, curr_loss = sess.run([y_train_tf,layer1, layer2, W1, b1, W2, b2, W3, b3, loss],\n                                                                               {x: x_train})\n    train_writer.add_summary(summary, i)\n    if i % 1000 == 999:\n        print "step ", i\n        print("W1: %s b1: %s" % (curr_W1, curr_b1))\n        print("W2: %s b2: %s" % (curr_W2, curr_b2))\n        print("W3: %s b3: %s" % (curr_W3, curr_b3))\n        print("layer1: %s layer2: %s" % (curr_layer1, curr_layer2))\n        print("linear_model: %s loss: %s" % (f_predict, curr_loss))\n        print " "\n        y_plot = y_train.reshape(1, -1)[0]\n        pred_plot = f_predict.reshape(1, -1)[0]\n        plt.hold(False)\n        ax.plot(x_plot, y_train[:])\n        plt.hold(True)\n        ax.plot(x_plot, f_predict, \'g--\')\n        ax.set(xlabel=\'X Value\', ylabel=\'Y / Predicted Value\', title=[str(i)," Loss: ", curr_loss])\n        plt.pause(0.001)\n\nfig.savefig("fig1.png")\nplt.show()\n'
"train_data = ['127.0.0.1', '8.8.8.8', '231.58.91.112', '127.0.0.1']\ntest_data = ['8.8.8.8', '0.0.0.0']\n\nip_encoder = LabelBinarizer()\nprint('Train Inputs:\\n', ip_encoder.fit_transform(train_data))\nprint('Test Inputs:\\n', ip_encoder.transform(test_data))\n\nTrain Inputs:\n [[1 0 0]\n [0 0 1]\n [0 1 0]\n [1 0 0]]\nTest Inputs:\n [[0 0 1]\n [0 0 0]]\n"
"for i in range(0, len(y_test)):\n    if (y_pred[i] != y_test[i]).any(): # Change\n        image = x_test_copy[i]\n        path = 'path'\n        cv2.imwrite(os.path.join(path , str(i)+'.jpg'), image)\n"
'enc = OneHotEncoder(sparse=False)\n'
'# code for predicting an image stored locally against a trained model\n# my local image is 28 x 28 already\nimport numpy as np\nfrom PIL import Image\nfrom keras.preprocessing import image\nimg = image.load_img(\'file path include full file name\')# , target_size=(32,32))\nimg  = image.img_to_array(img)\nimg  = img.reshape((1,) + img.shape)\n# img  = img/255\nimg = img.reshape(-1,784)\nimg_class=model.predict_classes(img) \n# this model above was already trained \n# code from https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-#neural-networks-python-keras/\nprediction = img_class[0]\nclassname = img_class[0]\nprint("Class: ",classname)\n'
'from keras import models, layers\n\n# the following numbers are just for demonstration\nvocab_size = 1000\nembed_dim = 50\n\nnum_arrays = 100\nnum_sentences = 200\nlen_sentence = 300\n\nmodel = models.Sequential()\nmodel.add(layers.Reshape((num_sentences*len_sentence,), input_shape=(num_sentences, len_sentence)))\nmodel.add(layers.Embedding(vocab_size, embed_dim, input_length=num_sentences*len_sentence))\nmodel.add(layers.Reshape((num_sentences, len_sentence, embed_dim)))\n# add whatever layers as you wish to complete your model\n\nmodel.summary()\n'
'for train, test in cv_iter:\n     for n_train_samples in train_sizes_abs:\n          train_test_proportions.append((train[:n_train_samples], test))\n'
'In [39]: x = np.arange(12).reshape(4,3)\n\nIn [40]: y = np.array([0,3,2])\n\nIn [41]: x[y[None, :], np.arange(len(y))[None,:]][0]\nOut[41]: array([ 0, 10,  8])\n\nIn [42]: x\nOut[42]: \narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n'
'|                       P(label) * P(features|label)\n|  P(label|features) = ------------------------------\n|                              P(features)\n'
"In [4]:\n\na = np.random.randn(5)\na\nOut[4]:\narray([ 0.85185988, -0.85148163,  0.5456541 , -0.26908162, -0.11920702])\n\nnp.savetxt(r'c:\\data\\np.dat', a, fmt='%1.1f')\n"
'In [11]: res = df.pop("A").str.get_dummies()  # Note: pop removes column A from df\n\nIn [12]: res.columns = res.columns.map(lambda x: "A_" + x)\n\nIn [13]: res\nOut[13]:\n   A_a  A_b  A_c\n0    1    0    0\n1    0    1    0\n2    1    0    0\n3    0    0    1\n\nIn [14]: res.join(df)\nOut[14]:\n   A_a  A_b  A_c  B\n0    1    0    0  0\n1    0    1    0  1\n2    1    0    0  2\n3    0    0    1  3\n\ndf2.reindex_axis(df1.columns, axis=1, fill_value=0)\n'
'pip install http://h2o-release.s3.amazonaws.com/h2o/reltibshirani/8/Python/h2o-3.6.0.8-py2.py3-none-any.whl\n\nimport h2o\nh2o.init()\n'
'try:\n    from sklearn.feature_selection import VarianceThreshold\nexcept:\n    pass  # it will catch any exception here\n\ntry:\n    from sklearn.feature_selection import VarianceThreshold\nexcept AttributeError:\n    pass # catches only Attribute Exception\n'
'url = "irecommend.ru/content/kogda-somnenii-byt-ne-mozhet-tolko-klear-blyu-pomozhet"    \n\nf1  = len(url) = 76\nf2 = base = str(url).split("/",1)[0] = "irecommend.ru"\nf3 = segments = str(a).count("/") = 2\n\nimport string\ncount = lambda l1,l2: sum([1 for x in l1 if x in l2])\n\nf4 = count_punctuation = count(a,set(string.punctuation))\nf5 = count_ascii = count(a,set(string.ascii_letters))\n'
'def create_feature_sets_and_labels(test_size = 0.2):\n    df = pd.read_csv("ibm.csv")\n    df = df.iloc[::-1]\n    features = df.values\n    testing_size = int(test_size*len(features))\n\n    train_x = np.array(features[1:,1:6][:-testing_size]).astype(np.float32)\n    train_y = np.array(features[1:,7][:-testing_size]).reshape(-1, 1).astype(np.float32)\n\n    test_x = np.array(features[1:,1:6][-testing_size:]).astype(np.float32)\n    test_y = np.array(features[1:,7][-testing_size:]).reshape(-1, 1).astype(np.float32)\n\n    scaler = MinMaxScaler(feature_range=(-5,5))\n\n    scaler.fit(train_x)\n\n    train_x = scaler.transform(train_x)\n    test_x = scaler.transform(test_x)\n\n    return train_x, train_y, test_x, test_y\n\noutput = neural_network_model(x)\nprediction = tf.sigmoid(output)\ncost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(output, y))\noptimizer = tf.train.AdamOptimizer().minimize(cost)\n'
'import colorsys\nN = 5\nHSV = [(x*1.0/N, 0.5, 0.5) for x in range(N)]\nRGB = map(lambda x: colorsys.hsv_to_rgb(*x), HSV)\n'
'h = tf.nn.sigmoid(tf.matmul(x_i,W)+b)\ncost = tf.reduce_sum(tf.add(tf.multiply(y_i,tf.log(h)),tf.multiply(1-\ny_i,tf.log(1-h)))) / -m\n\nh = tf.matmul(x_i,W)+b\ncost = tf.reduce_mean(tf.sigmoid_cross_entropy_with_logits(labels=y_i, logits=h))\n'
'import tensorflow as tf\nimport numpy as np\nIMAGE_HEIGHT = 384\nIMAGE_WIDTH = 384\nIMAGE_CHANNELS = 3\n\ndef preprocessor(image):\n    image = tf.reshape(image, (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n    image = tf.image.central_crop(image,0.8)\n    shape = tf.shape(image)\n    return image,shape\n\nimage = tf.random_normal([IMAGE_HEIGHT,IMAGE_WIDTH,IMAGE_CHANNELS])\nimage_cropped,shape = preprocessor(image)\n\nsess = tf.Session()\nim_v,im_crop_v,shape_v = sess.run([image,image_cropped,shape])\nprint(im_v.shape)\nprint(im_crop_v.shape)\nprint(shape_v)\n\n(384, 384, 3)\n(308, 308, 3)\n[308 308   3]\n'
"import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\n\ndf1 = pd.DataFrame({'sexid': list('MMFFMFFMMF'), 'x1': [0, 12, 2, 3, 4, 2, 0, 12, 12, 12], 'x2': [0, 1, 1, 1, 0, 1, 1, 0, 0, 1]})\n\ndf2 = pd.DataFrame({'x1': [0, 12, 2, 3, 4, 2, 0, 12, 12, 12], 'x2': [0, 1, 1, 1, 0, 1, 1, 0, 0, 1]})\n\nX = df1[['x1', 'x2']]\ny = df1['sexid']\n\nmodel = LogisticRegression()\n\nmodel.fit(X, y)\n\nmodel.predict(df2)\n\narray(['F', 'M', 'F', 'F', 'M', 'F', 'F', 'M', 'M', 'M'], dtype=object)\n"
'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=123, stratify=y)\n'
'from six import next\n\npatience = 4\nbest_loss = 1e6\nrounds_without_improvement = 0\n\nfor epoch_nb in range(nb_of_epochs):\n    losses_list = list()\n    for batch in range(nb_of_batches):\n        x, y = next(train_data_gen)\n        losses_list.append(model.train_on_batch(x, y))\n    mean_loss = sum(losses_list) / len(losses_list)\n\n    if mean_loss &lt; best_loss:\n        best_loss = mean_loss\n        rounds_witout_improvement = 0\n    else:\n        rounds_without_improvement +=1\n\n    if rounds_without_improvement == patience:\n        break\n'
'coordinates = [\n    (-73.973052978515625,40.793209075927734),\n    (-73.972923278808594,40.782520294189453), \n    (-75.9,40.7)\n    ]\n\nfiltered = list()\n\n# filtering coordinates\nfor c in coordinates:\n    if  -74.25 &lt;= c[0] &lt;= -73.9 and 40.6 &lt;= c[1] &lt;= 40.9:\n        filtered.append(c)\n\nprint filtered # here you have your filtered coordinates\n\n[(-73.97305297851562, 40.793209075927734), (-73.9729232788086, 40.78252029418945)]\n'
"if __name__ == '__main__':\n  main(None)\n"
'X = TSNE(n_components=2, perplexity=2.0).fit_transform( data )\n'
"df_new = df_new.assign(\n    newlabels_tobeReplaced=\n    lambda d: d['newlabels_tobeReplaced'].mask(\n        d.newlabels.str.contains('trifurcation'), 'trifurcation'\n    )\n)\n"
'import numpy as np\nimport pandas as pd\nfrom keras import losses\nfrom keras import backend as K\n\npoints = 100\ndf = pd.DataFrame({"error": np.linspace(-3,3,points)})\nmse_loss = losses.mean_squared_error(np.zeros((points,1)), df["error"].values.reshape(-1,1))\ndf["mean_squared_error"] = K.eval(mse_loss)\ndf.plot(x="error")\n'
'import tensorflow as tf\nfrom google.protobuf import text_format\n\nfrom object_detection.protos import pipeline_pb2\n\ndef get_configs_from_pipeline_file(pipeline_config_path, config_override=None):\n\n  \'\'\'\n  read .config and convert it to proto_buffer_object\n  \'\'\'\n\n  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n  with tf.gfile.GFile(pipeline_config_path, "r") as f:\n    proto_str = f.read()\n    text_format.Merge(proto_str, pipeline_config)\n  if config_override:\n    text_format.Merge(config_override, pipeline_config)\n  #print(pipeline_config)\n  return pipeline_config\n\n\ndef create_configs_from_pipeline_proto(pipeline_config):\n  \'\'\'\n  Returns the configurations as dictionary\n  \'\'\'\n\n  configs = {}\n  configs["model"] = pipeline_config.model\n  configs["train_config"] = pipeline_config.train_config\n  configs["train_input_config"] = pipeline_config.train_input_reader\n  configs["eval_config"] = pipeline_config.eval_config\n  configs["eval_input_configs"] = pipeline_config.eval_input_reader\n  # Keeps eval_input_config only for backwards compatibility. All clients should\n  # read eval_input_configs instead.\n  if configs["eval_input_configs"]:\n    configs["eval_input_config"] = configs["eval_input_configs"][0]\n  if pipeline_config.HasField("graph_rewriter"):\n    configs["graph_rewriter_config"] = pipeline_config.graph_rewriter\n\n  return configs\n\n\nconfigs = get_configs_from_pipeline_file(\'faster_rcnn_resnet101_pets.config\')\nconfig_as_dict = create_configs_from_pipeline_proto(configs)\n'
"dataset = df[['Rate', 'Weights', 'Change', 'Price', 'CategoryOne']].copy()\ndataset['CategoryOne'] = dataset['CategoryOne'].map(lambda x : str(x))\n"
'df = (df\n      .assign(Mean_MRP = lambda x:x.groupby(\'item\')[\'MRP\']\n                                   .transform(\'mean\')))\n\ndf\n\n\n  item  MRP     sold    Mean_MRP\n0   A   10      10       24\n1   A   36      4        24\n2   B   32      6        31\n3   A   26      7        24\n4   B   30      9        31\n\nimport janitor\n\ndf.groupby_agg(by=\'item\',\n               agg=\'mean\',\n               agg_column_name="MRP",\n               new_column_name=\'Mean_MRP\')\n'
'import tensorflow as tf\n\ndef funa():\n    return tf.constant(32)\n\ndef funb():\n    return tf.constant(25)\n\nfoo = True\nfoo_p = tf.placeholder(tf.bool)\n\nsess = tf.Session()\n\nx = tf.cond(foo_p, lambda: funa(), lambda: funb())\nfor i in range(20):\n    if i &gt; 10:\n        foo = False\n    print(sess.run(x, {foo_p:foo}))\n'
'TP + TN / (P+ N)\n\n25 + 25 / (50+50) = 0.5\n\n(recall_posivite*number_positve)+(recall_negative*number_negative)/(number_positive + number_negativ) = 0.5*50+0.5*50/(50+50) = 50/100 = 0.5\n'
'model.fit_generator(\n    training_set,\n    steps_per_epoch=8000//32,\n    epochs=25,\n    validation_data=test_set,\n    validation_steps=2000//32\n )\n'
"boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W  \nlabels (Int64Tensor[N]): the predicted labels for each image  \nscores (Tensor[N]): the scores or each prediction  \nmasks (UInt8Tensor[N, 1, H, W]): the predicted masks for each instance, in 0-1 range.\n\nimg_cv = cv2.imread('input.jpg', cv2.COLOR_BGR2RGB)\n\nfor i in range(len(prediction[0]['masks'])):\n    # iterate over masks\n    mask = prediction[0]['masks'][i, 0]\n    mask = mask.mul(255).byte().cpu().numpy()\n    contours, _ = cv2.findContours(\n            mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n    cv2.drawContours(img_cv, contours, -1, (255, 0, 0), 2, cv2.LINE_AA)\n\ncv2.imshow('img output', img_cv)\n"
"from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nX, y = load_iris(return_X_y=True)\nclf = LogisticRegression(random_state=0)\nclf.fit(X, y)\n\n# by default, the coefficients are in dense format (numpy.ndarray):\nclf.coef_\n# array([[-0.41878528,  0.96703041, -2.5209973 , -1.08417682],\n#        [ 0.53124457, -0.31475282, -0.20008433, -0.94861142],\n#        [-0.1124593 , -0.65227759,  2.72108162,  2.03278825]])\n\ntype(clf.coef_)\n# numpy.ndarray\n\n# switch to sparse format:\nclf.sparsify()\nclf.coef_\n# &lt;3x4 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n#   with 12 stored elements in Compressed Sparse Row format&gt;\n\ntype(clf.coef_)\n# scipy.sparse.csr.csr_matrix\n\n# switch back to dense format:\nclf.densify()\n\ntype(clf.coef_)\n# numpy.ndarray\n"
'from sklearn.metrics import plot_confusion_matrix\nfig, ax = plt.subplots(figsize=(8,8))\ndisp = plot_confusion_matrix(clf, X_test, Y_test,\n                               labels=np.unique(y),\n                               cmap=plt.cm.Blues,ax=ax)\n'
'X_test = pd.get_dummies(test_data[features])\n'
"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n\ng1=np.array([1003, 145, 1344, 66, 171, 962,100,200,300,400])\ng2=np.array([602, 140, 390, 1955, 289, 90,80,170,245,380])\n\nth=np.array([1999, 341, 1151, 2605, 568, 864,1000,300,184,411])\n\ndataset = pd.DataFrame({'Group 1 Frames':g1,'Group 2 Frames':g2,'Total Hours':th})\n\nX = dataset[['Group 1 Frames', 'Group 2 Frames']]\n# print(X)\ny = dataset['Total Hours']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=0)\n\npipeline=Pipeline([('norm', StandardScaler()), ('linreg', LinearRegression())])\n\npipeline.fit(X_train, y_train)\n\nprint(pipeline.score(X_test,y_test))\n\ny_pred=pipeline.predict(X_test)\n\nfig,ax=plt.subplots()\nax.plot(X_test,y_test,label='Actual')\nax.plot(X_test,y_pred,label='Predicted')\n\nax.legend()\nplt.show()\n\ncoeff_df = pd.DataFrame(pipeline['linreg'].coef_, X.columns, columns=['Coefficient'])\nprint(coeff_df)\n"
'movie = Movie()\nid = movie.details(int(movies.tmdbId)) # Not sure why this variables is defined?\n\nfor id in movies[&quot;tmdbId&quot;]: # my dataframe, I am assuming this is iterable\n   if movie.details(int(id)): # tmdb database,\n      m = movie.details(int(id))\n      print (m.overview)\n'
'from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nle = preprocessing.LabelEncoder()\ndata.name = le.fit_transform(data.name)\nX = data.iloc[:, 0:4]\ny = data.iloc[:, 5]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\n\nprint(classifier.coef_,classifier.intercept_)\n\n\n[[ 0.09253555  0.09253555 -0.15407024  0.        ]] [-0.1015314]\n'
'model.fit(X, y, sample_weight=X_weight, ...)\n'
"from sklearn.datasets import load_iris\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\ny_train_categorical = load_iris()['target_names'][y_train]\n# array(['setosa', 'setosa', 'versicolor',...\n\nsv = SVC()\nsv.fit(X_train, y_train_categorical)\nsv.classes_\n# array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10')\n"
'all_theta = np.array((num_labels,n+1),dtype = &quot;object&quot;)\n\nall_theta = np.empty((num_labels,n+1))\n'
'Accuracy:  tensor(0.5059)\nAccuracy:  tensor(0.8702)\nAccuracy:  tensor(0.9159)\nAccuracy:  tensor(0.9233)\nAccuracy:  tensor(0.9336)\nAccuracy:  tensor(0.9484)\nAccuracy:  tensor(0.9602)\nAccuracy:  tensor(0.9676)\nAccuracy:  tensor(0.9705)\nAccuracy:  tensor(0.9749)\n'
'from sklearn.cluster import AffinityPropagation\n\nX = np.array([1, 1.5, 0.4, 1.1, 23, 24, 22.5, \n              21, 20, 25, 40, 50, 50, 51, 52, 53]).reshape(-1, 1)\nap = AffinityPropagation(random_state=12).fit(X)\ny = ap.predict(X)\nprint(y)\n# array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2], dtype=int64)\n\nfirst_cluster = X[y==0].ravel()\nfirst_cluster\n# array([1. , 1.5, 0.4, 1.1])\nsecond_cluster = X[y==1].ravel()\nsecond_cluster\n# array([23. , 24. , 22.5, 21. , 20. , 25. ])\n'
"import sys, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nnp.random.seed(0)\n\nclass PieceWiseLinearRegression:\n    @classmethod\n    def nargs_func(cls, f, n):\n        return eval('lambda ' + ', '.join([f'a{i}'for i in range(n)]) + ': f(' + ', '.join([f'a{i}'for i in range(n)]) + ')', locals())\n        \n    @classmethod\n    def piecewise_linear(cls, n):\n        condlist = lambda xs, xa: [(lambda x: (\n            (xs[i] &lt;= x if i &gt; 0 else np.full_like(x, True, dtype = np.bool_)) &amp;\n            (x &lt; xs[i + 1] if i &lt; n - 1 else np.full_like(x, True, dtype = np.bool_))\n        ))(xa) for i in range(n)]\n        funclist = lambda xs, ys: [(lambda i: (\n            lambda x: (\n                (x - xs[i]) * (ys[i + 1] - ys[i]) / (\n                    (xs[i + 1] - xs[i]) if abs(xs[i + 1] - xs[i]) &gt; 10 ** -7 else 10 ** -7 * (-1, 1)[xs[i + 1] - xs[i] &gt;= 0]\n                ) + ys[i]\n            )\n        ))(j) for j in range(n)]\n        def f(x, *pargs):\n            assert len(pargs) == (n + 1) * 2, (n, pargs)\n            xs, ys = pargs[0::2], pargs[1::2]\n            xa = x.ravel().astype(np.float64)\n            ya = np.piecewise(x = xa, condlist = condlist(xs, xa), funclist = funclist(xs, ys)).ravel()\n            #print('xs', xs, 'ys', ys, 'xa', xa, 'ya', ya)\n            return ya\n        return cls.nargs_func(f, 1 + (n + 1) * 2)\n        \n    def __init__(self, n):\n        self.n = n\n        self.f = self.piecewise_linear(self.n)\n\n    def fit(self, x, y):\n        from scipy import optimize\n        self.p, self.e = optimize.curve_fit(self.f, x, y, p0 = [j for i in range(self.n + 1) for j in (np.amin(x) + i * (np.amax(x) - np.amin(x)) / self.n, 1)])\n        #print('p', self.p)\n        \n    def predict(self, x):\n        return self.f(x, *self.p)\n\ndata = [5.269, 5.346, 5.375, 5.482, 5.519, 5.57, 5.593999999999999, 5.627000000000001, 5.724, 5.818, 5.792999999999999, 5.817, 5.8389999999999995, 5.882000000000001, 5.92, 6.025, 6.064, 6.111000000000001, 6.1160000000000005, 6.138, 6.247000000000001, 6.279, 6.332000000000001, 6.3389999999999995, 6.3420000000000005, 6.412999999999999, 6.442, 6.519, 6.596, 6.603, 6.627999999999999, 6.76, 6.837000000000001, 6.781000000000001, 6.8260000000000005, 6.849, 6.875, 6.982, 7.018, 7.042000000000001, 7.068, 7.091, 7.204, 7.228, 7.261, 7.3420000000000005, 7.414, 7.44, 7.516, 7.542000000000001, 7.627000000000001, 7.667000000000001, 7.821000000000001, 7.792999999999999, 7.756, 7.871, 8.006, 8.078, 7.916, 7.974, 8.074, 8.119, 8.228, 7.976, 8.045, 8.312999999999999, 8.335, 8.388, 8.437999999999999, 8.456, 8.227, 8.266, 8.277999999999999, 8.289, 8.299, 8.318, 8.332, 8.34, 8.349, 8.36, 8.363999999999999, 8.368, 8.282, 8.283999999999999]\ntime = list(range(1, 85))\ndf = pd.DataFrame(list(zip(time, data)), columns = ['time', 'data'])\n\nchoose_train = np.random.uniform(size = (len(df),)) &lt; 0.8\nchoose_valid = ~choose_train\n\nx_all = df.iloc[:, 0].values\ny_all = df.iloc[:, 1].values\nx_train = df.iloc[:, 0][choose_train].values\ny_train = df.iloc[:, 1][choose_train].values\nx_valid = df.iloc[:, 0][choose_valid].values\ny_valid = df.iloc[:, 1][choose_valid].values\nx_all_lin = np.linspace(np.amin(x_all), np.amax(x_all), 500)\n\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('PWLR2', PieceWiseLinearRegression(2)))\n        \nfor imodel, (name, model) in enumerate(models):\n    model.fit(x_train[:, None], y_train)\n    x_all_lin_pred = model.predict(x_all_lin[:, None])\n    plt.plot(x_all_lin, x_all_lin_pred, label = f'pred {name}')\n\nplt.plot(x_train, y_train, label='train')\nplt.plot(x_valid, y_valid, label='valid')\nplt.xlabel('time')\nplt.ylabel('data')\nplt.legend()\nplt.show()\n"
"import tensorflow as tf \nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input,Dense\n\nx = np.linspace(0,100,1000)\ny = np.sin(x) + x**2\n\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.3)\n\ninput_x = Input(shape=(1,))\ny = Dense(10,activation='relu')(input_x)\ny = Dense(1,activation='relu')(y)\nmodel = Model(inputs=input_x,outputs=y)\n\nadamopt = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\ndef schedule_func(epoch):\n    print()\n    print('calling lr_scheduler on epoch %i' % epoch)\n    print('current learning rate %.8f' % K.eval(model.optimizer.lr))\n    print('returned value %.8f' % (1e-8 * 10**(epoch / 20)))\n    return 1e-8 * 10**(epoch / 20)\n    \nlr_schedule = tf.keras.callbacks.LearningRateScheduler(schedule_func)\n\nmodel.compile(loss='mse',optimizer=adamopt,metrics=['mae'])\nhistory = model.fit(x_train,y_train,\n                    batch_size=8,\n                    epochs=10,\n                    validation_data=(x_val, y_val),\n                    verbose=1,\n                    callbacks=[lr_schedule])\n"
'folder\n│     \n│\n└───class1\n│   │   file011\n│   │   file012\n│   \n└───class2\n    │   file021\n    │   file022\n\nroot/dog/xxx.png\nroot/dog/xxy.png\nroot/dog/xxz.png\n\nroot/cat/123.png\nroot/cat/nsdf3.png\nroot/cat/asd932_.png\n\ndataset = torchvision.datasets.Imagefolder(YOUR_PATH, ...)\n\ntest_size = 0.1 * len(dataset)\ntest_set = torch.utils.data.Subset(dataset, range(test_size))  # take 10% for test\ntrain_set = torch.utils.data.Subset(dataset, range(test_size, len(dataset)) # the last part for train\n\nindexes = shuffle(range(len(dataset)))\nindexes_train = indexes[:int(len(dataset)*0.9)]\nindexes_test = = indexes[int(len(dataset)*0.9):]\n'
"features = np.concatenate([features1, features2], 1)\n\n# generate dummy data\nn_sample = 600\nset1 = np.random.uniform(0,1, (n_sample,30))\nset2 = np.random.uniform(0,1, (n_sample,30))\n\n# model 1\ninp1 = Input((30,))\nx1 = Dense(512,)(inp1)\nx1 = Dropout(0.3)(x1)\nx1 = BatchNormalization()(x1)\nout1 = Dense(3, activation='softmax')(x1)\nm1 = Model(inp1, out1)\n# m1.fit(...)\n\n# model 2\ninp2 = Input((30,))\nx2 = Dense(512,)(inp2)\nx2 = Dropout(0.3)(x2)\nx2 = BatchNormalization()(x2)\nout2 = Dense(3, activation='softmax')(x2)\nm2 = Model(inp2, out2)\n# m2.fit(...)\n\n# concatenate the desired output\nconcat = Concatenate()([m1.layers[1].output, m2.layers[1].output]) # get the outputs of dense 512 layers\nmerge = Model([m1.input, m2.input], concat)\n\n# make combined predictions\nmerge.predict([set1,set2]).shape  # (n_sample, 1024)\n"
'0        0.371495  0.598211  0.038224  ...  0.777405  0.193472     0.0\n1        0.356371  0.636690  0.841467  ...  0.403570  0.330145     0.0\n2        0.793879  0.008617  0.701122  ...  0.021139  0.514559     0.0\n3        0.318618  0.798823  0.844345  ...  0.931606  0.467469     0.0\n4        0.307109  0.076505  0.865164  ...  0.809495  0.914563     0.0\n...           ...       ...       ...  ...       ...       ...     ...\n2199995  0.215133  0.239560  0.477092  ...  0.050997  0.727986  2199.0\n2199996  0.249206  0.881694  0.985973  ...  0.897410  0.564516  2199.0\n2199997  0.378455  0.697581  0.016306  ...  0.985966  0.638413  2199.0\n2199998  0.233829  0.158274  0.478611  ...  0.825343  0.215944  2199.0\n2199999  0.351320  0.980258  0.677298  ...  0.791046  0.736788  2199.0\n'
'import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nstd = 7\nX = np.random.multivariate_normal([2, 2], [[std, 0], [0, std]], size=100)\n\nY = np.random.multivariate_normal([10, 10], [[std, 0], [0, std]], size=100)\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.scatter(Y[:, 0], Y[:, 1])\nplt.show()\n\nscaler = MaxAbsScaler()\nX_scaled = scaler.fit_transform(X)\nY_scaled = scaler.fit_transform(Y)\n\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1])\nplt.scatter(Y_scaled[:, 0], Y_scaled[:, 1])\nplt.show()  \n'
"p1 = tf.layers.max_pooling1d(h1, pool_size=2, strides=1, padding='VALID')\n"
'ActInA = Input(shape=(4,), dtype=\'int32\')\nActInB = Input(shape=(4,), dtype=\'int32\')\nActInC = Input(shape=(1,), dtype=\'int32\')\n\nNetA = Dense(4, activation="relu")(ActInA)\nNetA = Dense(3, activation="relu")(NetA)\n\nNetB = Dense(4, activation="relu")(ActInB)\nNetB = Dense(3, activation="relu")(NetB)\n\nNetAB = concatenate([NetA, NetB])\nNetAB = Dense(1, activation="relu")(NetAB)\n\nmymodel = Model([ActInA, ActInB], NetAB)\n\nfor i in range(100):\n   NetMergeABC.append(mymodel([ActInA_array[i], ActInB_array[i]]))\n'
"vgg_output tensor:\n[&lt;tf.Tensor 'block5_pool/MaxPool:0' shape=(?, 1, 1, 512) dtype=float32&gt;] &lt;-- this is a list\n\nmodel_tensor = Flatten()(vgg_output[0])  # pass the first element of output\n"
"weights = []\n\nfor param in model.parameters():\n    weights.append(param.clone())\n\ncriterion = nn.BCELoss() # criterion and optimizer setup\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nfoo = torch.randn(3, 10) # fake input\ntarget = torch.randn(3, 5) # fake target\n\nresult = model(foo) # predictions and comparison and backprop\nloss = criterion(result, target)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\n\nweights_after_backprop = [] # weights after backprop\nfor param in model.parameters():\n    weights_after_backprop.append(param.clone()) # only layer1's weight should update, layer2 is not used\n\nfor i in zip(weights, weights_after_backprop):\n    print(torch.equal(i[0], i[1]))\n\nFalse\nFalse\nTrue\nTrue\n"
"model.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', ...)\n# is the same as\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', ...)\n"
'import pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import recall_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nimport numpy as np\n\n\ndef getDataset(path, x_attr, y_attr, mapping):\n    """\n    Extract dataset from CSV file\n    :param path: location of csv file\n    :param x_attr: list of Features Names\n    :param y_attr: Y header name in CSV file\n    :param mapping: dictionary of the classes integers\n    :return: tuple, (X, Y)\n    """\n    df = pd.read_csv(path)\n    df.replace(mapping, inplace=True)\n    X = np.array(df[x_attr]).reshape(len(df), len(x_attr))\n    Y = np.array(df[y_attr])\n    return X, Y\n\n\ndef custom_recall_score(y_true, y_pred):\n    """\n    Workaround for the recall score\n    :param y_true: Ground Truth during iterations\n    :param y_pred: Y predicted during iterations\n    :return: float, recall\n    """\n    wanted_labels = [0, 1]\n    assert set(wanted_labels).issubset(y_true)\n    wanted_indices = [y_true.tolist().index(x) for x in wanted_labels]\n    wanted_y_true = [y_true[x] for x in wanted_indices]\n    wanted_y_pred = [y_pred[x] for x in wanted_indices]\n    recall_ = recall_score(wanted_y_true, wanted_y_pred,\n                           labels=wanted_labels, average=\'macro\')\n    print("Wanted Indices: {}".format(wanted_indices))\n    print("Wanted y_true: {}".format(wanted_y_true))\n    print("Wanted y_pred: {}".format(wanted_y_pred))\n    print("Recall during cross validation: {}".format(recall_))\n    return recall_\n\n\ndef run(X_data, Y_data):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n    train_index, test_index = next(sss.split(X_data, Y_data))\n    X_train, X_test = X_data[train_index], X_data[test_index]\n    Y_train, Y_test = Y_data[train_index], Y_data[test_index]\n    param_grid = {\'C\': [0.1, 1]} # or whatever parameter you want\n    # I am using LR just for example\n    model = LogisticRegression(solver=\'saga\', random_state=0)\n    clf = GridSearchCV(model, param_grid,\n                       cv=StratifiedKFold(n_splits=2),\n                       return_train_score=True,\n                       scoring=make_scorer(custom_recall_score))\n    clf.fit(X_train, Y_train)\n    print(clf.cv_results_)\n\n\nX_data, Y_data = getDataset("dataset_example.csv", [\'TSH\', \'T4\'], \'diagnosis\',\n                            {\'compensated_hypothyroid\': 0, \'primary_hypothyroid\': 1,\n                             \'hyperthyroid\': 2, \'normal\': 3})\nrun(X_data, Y_data)\n\nWanted Indices: [3, 5]\nWanted y_true: [0, 1]\nWanted y_pred: [3, 3]\nRecall during cross validation: 0.0\n...\n...\nWanted Indices: [0, 4]\nWanted y_true: [0, 1]\nWanted y_pred: [1, 1]\nRecall during cross validation: 0.5\n...\n...\n{\'param_C\': masked_array(data=[0.1, 1], mask=[False, False],\n  fill_value=\'?\', dtype=object), \n  \'mean_score_time\': array([0.00094521, 0.00086224]), \n  \'mean_fit_time\': array([0.00298035, 0.0023526 ]), \n  \'std_score_time\': array([7.02142715e-05, 1.78813934e-06]), \n  \'mean_test_score\': array([0.21428571, 0.5       ]), \n  \'std_test_score\': array([0.24743583, 0.        ]), \n  \'params\': [{\'C\': 0.1}, {\'C\': 1}], \n  \'mean_train_score\': array([0.25, 0.5 ]), \n  \'std_train_score\': array([0.25, 0.  ]), \n  ....\n  ....}\n'
'pip3 uninstall h2o\npip3 install http://h2o-release.s3.amazonaws.com/h2o/rel-turchin/9/Python/h2o-3.8.2.9-py2.py3-none-any.whl\n\nimport h2o\nh2o.init()\n'
'y = tf.placeholder(tf.float32, [None, n_input, n_input, n_classes], name="ground_truth")\n\nbatch_y = convert_to_2_channel(batch_y, batch_size)\n# do not reshape batch_y now\nsess.run([cost, accuracy], feed_dict={x: batch_x,\n                                      y: batch_y,\n                                      keep_prob: 1.0})\n'
'monthly_income = tf.contrib.layers.sparse_column_with_integerized_feature("monthly_income", bucket_size=7)\n'
'import numpy as np\nimport scipy.io.wavfile as wave\nimport python_speech_features as psf\nfrom pydub import AudioSegment\n\n#your sound file\nfilepath = \'my-sound.wav\'\n\ndef convert(path):\n\n    #open file (supports all ffmpeg supported filetypes) \n    audio = AudioSegment.from_file(path, path.split(\'.\')[-1].lower())\n\n    #set to mono\n    audio = audio.set_channels(1)\n\n    #set to 44.1 KHz\n    audio = audio.set_frame_rate(44100)\n\n    #save as wav\n    audio.export(path, format="wav")\n\ndef getSpectrogram(path, winlen=0.025, winstep=0.01, NFFT=512):\n\n    #open wav file\n    (rate,sig) = wave.read(path)\n\n    #get frames\n    winfunc=lambda x:np.ones((x,))\n    frames = psf.sigproc.framesig(sig, winlen*rate, winstep*rate, winfunc)\n\n    #Magnitude Spectrogram\n    magspec = np.rot90(psf.sigproc.magspec(frames, NFFT))\n\n    #noise reduction (mean substract)\n    magspec -= magspec.mean(axis=0)\n\n    #normalize values between 0 and 1\n    magspec -= magspec.min(axis=0)\n    magspec /= magspec.max(axis=0)\n\n    #show spec dimensions\n    print magspec.shape    \n\n    return magspec\n\n#convert file if you need to\nconvert(filepath)\n\n#get spectrogram\nspec = getSpectrogram(filepath)\n'
"validation_generator = val_datagen.flow_from_directory(...\n        class_mode = 'categorical')\n"
'#Variables to use with model\ngraph = tf.get_default_graph()\nx = graph.get_tensor_by_name("x:0")\ny_ = graph.get_tensor_by_name("y_:0")\nkeep_prob = graph.get_tensor_by_name("keep_prob:0")#Changed this\naccuracy = graph.get_tensor_by_name("accuracy:0")\n'
"titanic['age'] = titanic.groupby(['survived','embarked'])['age']\n                        .apply(lambda x: x.fillna(x.mean()))\n\nimport seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\n#check NaN rows in age\nprint (titanic[titanic['age'].isnull()].head(10))\n    survived  pclass     sex  age  sibsp  parch      fare embarked   class  \\\n5          0       3    male  NaN      0      0    8.4583        Q   Third   \n17         1       2    male  NaN      0      0   13.0000        S  Second   \n19         1       3  female  NaN      0      0    7.2250        C   Third   \n26         0       3    male  NaN      0      0    7.2250        C   Third   \n28         1       3  female  NaN      0      0    7.8792        Q   Third   \n29         0       3    male  NaN      0      0    7.8958        S   Third   \n31         1       1  female  NaN      1      0  146.5208        C   First   \n32         1       3  female  NaN      0      0    7.7500        Q   Third   \n36         1       3    male  NaN      0      0    7.2292        C   Third   \n42         0       3    male  NaN      0      0    7.8958        C   Third   \n\n      who  adult_male deck  embark_town alive  alone  \n5     man        True  NaN   Queenstown    no   True  \n17    man        True  NaN  Southampton   yes   True  \n19  woman       False  NaN    Cherbourg   yes   True  \n26    man        True  NaN    Cherbourg    no   True  \n28  woman       False  NaN   Queenstown   yes   True  \n29    man        True  NaN  Southampton    no   True  \n31  woman       False    B    Cherbourg   yes  False  \n32  woman       False  NaN   Queenstown   yes   True  \n36    man        True  NaN    Cherbourg   yes   True  \n42    man        True  NaN    Cherbourg    no   True \n\nidx = titanic[titanic['age'].isnull()].index\ntitanic['age'] = titanic.groupby(['survived','embarked'])['age']\n                        .apply(lambda x: x.fillna(x.mean()))\n\n#check if values was replaced\nprint (titanic.loc[idx].head(10))\n    survived  pclass     sex        age  sibsp  parch      fare embarked  \\\n5          0       3    male  30.325000      0      0    8.4583        Q   \n17         1       2    male  28.113184      0      0   13.0000        S   \n19         1       3  female  28.973671      0      0    7.2250        C   \n26         0       3    male  33.666667      0      0    7.2250        C   \n28         1       3  female  22.500000      0      0    7.8792        Q   \n29         0       3    male  30.203966      0      0    7.8958        S   \n31         1       1  female  28.973671      1      0  146.5208        C   \n32         1       3  female  22.500000      0      0    7.7500        Q   \n36         1       3    male  28.973671      0      0    7.2292        C   \n42         0       3    male  33.666667      0      0    7.8958        C   \n\n     class    who  adult_male deck  embark_town alive  alone  \n5    Third    man        True  NaN   Queenstown    no   True  \n17  Second    man        True  NaN  Southampton   yes   True  \n19   Third  woman       False  NaN    Cherbourg   yes   True  \n26   Third    man        True  NaN    Cherbourg    no   True  \n28   Third  woman       False  NaN   Queenstown   yes   True  \n29   Third    man        True  NaN  Southampton    no   True  \n31   First  woman       False    B    Cherbourg   yes  False  \n32   Third  woman       False  NaN   Queenstown   yes   True  \n36   Third    man        True  NaN    Cherbourg   yes   True  \n42   Third    man        True  NaN    Cherbourg    no   True  \n\n#check mean values\nprint (titanic.groupby(['survived','embarked'])['age'].mean())\nsurvived  embarked\n0         C           33.666667\n          Q           30.325000\n          S           30.203966\n1         C           28.973671\n          Q           22.500000\n          S           28.113184\nName: age, dtype: float64\n"
'def compute_cost_regularized(theta, X, y, lda):\n    reg =lda/(2*len(y)) * np.sum(theta[1:]**2) # Change here\n    return 1/len(y) * np.sum(-y @ np.log(sigmoid(X@theta)) \n                         - (1-y) @ np.log(1-sigmoid(X@theta))) + reg\n\nIn [8]: def compute_cost_regularized(theta, X, y, lda):\n   ...:     reg =lda/(2*len(y)) * np.sum(theta[1:]**2)\n   ...:     return 1/len(y) * np.sum(-y @ np.log(sigmoid(X@theta))\n   ...:                          - (1-y) @ np.log(1-sigmoid(X@theta))) + reg\n   ...:\n\nIn [9]: def sigmoid(z):\n   ...:     return 1 / (1 + np.exp(-z))\n   ...:\n\nIn [10]: theta_test = np.array([-2,-1,1,2])\n    ...: X_test = np.concatenate((np.ones((5,1)),\n    ...:          np.fromiter((x/10 for x in range(1,16)), float).reshape((3,5)).T), axis = 1)\n    ...: y_test = np.array([1,0,1,0,1])\n    ...: lambda_test = 3\n    ...:\n\nIn [11]: compute_cost_regularized(theta_test, X_test, y_test, lambda_test)\nOut[11]: 2.5348193961097438\n'
'def softmax_loss(x, y):\n  """\n  - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n    for the ith input.\n  - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n    0 &lt;= y[i] &lt; C\n  """\n  probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n  probs /= np.sum(probs, axis=1, keepdims=True)\n  N = x.shape[0]\n  return -np.sum(np.log(probs[np.arange(N), y])) / N\n'
'def top_decile_conversion_rate(y_prob, y_actual):\n    # Function goes in here\n    print "---prob--"\n    print y_prob\n    print "---actual--"\n    print y_actual\n    print "---end--"\n\n    return 0.5\n\n\nfeatures = pd.DataFrame({"f1":np.random.randint(1,1000,500) , "f2":np.random.randint(1,1000,500), \n                         "label":[round(x) for x in np.random.random_sample(500)]})\n\n\nmy_scorer = make_scorer(top_decile_conversion_rate, greater_is_better=True,needs_proba=True)\ngs = grid_search.GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={\'C\': [i for i in range(1, 3)], \'class_weight\': [None], \'penalty\':[\'l2\']},\n    cv=20,\n    scoring=my_scorer ) \nmodel = gs.fit(features[["f1","f2"]], features.label)\n'
'"""Example code for TensorFlow Wide &amp; Deep Tutorial using TF.Learn API."""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport pandas as pd\nimport tensorflow as tf\n\nCSV_COLUMNS = [\n  \'Store\', \'DayOfWeek\', \'Sales\', \'Customers\', \'Open\', \'Promo\',\n  \'StateHoliday\', \'SchoolHoliday\', \'StoreType\', \'Assortment\',\n  \'CompetitionDistance\', \'trend\', \'Max_TemperatureC\', \'Mean_TemperatureC\',\n  \'Min_TemperatureC\', \'Max_Humidity\', \'Mean_Humidity\', \'Min_Humidity\'\n]\n\nStore = tf.feature_column.numeric_column("Store")\nDayOfWeek = tf.feature_column.numeric_column("DayOfWeek")\nCustomers = tf.feature_column.numeric_column("Customers")\nOpen = tf.feature_column.numeric_column("Open")\nPromo = tf.feature_column.numeric_column("Promo")\nStateHoliday = tf.feature_column.categorical_column_with_vocabulary_list("StateHoliday", ["True", "False"])\nSchoolHoliday = tf.feature_column.numeric_column("SchoolHoliday")\nStoreType = tf.feature_column.categorical_column_with_vocabulary_list("StoreType", [\'a\', \'b\', \'c\', \'d\'])\nAssortment = tf.feature_column.categorical_column_with_vocabulary_list("Assortment", [\'a\', \'b\', \'c\'])\nCompetitionDistance = tf.feature_column.numeric_column("CompetitionDistance")\ntrend = tf.feature_column.numeric_column("trend")\nMax_TemperatureC = tf.feature_column.numeric_column("Max_TemperatureC")\nMean_TemperatureC = tf.feature_column.numeric_column("Mean_TemperatureC")\nMin_TemperatureC = tf.feature_column.numeric_column("Min_TemperatureC")\nMax_Humidity = tf.feature_column.numeric_column("Max_Humidity")\nMean_Humidity = tf.feature_column.numeric_column("Mean_Humidity")\nMin_Humidity = tf.feature_column.numeric_column("Min_Humidity")\n\ndeep_columns = [\n  Store,\n  DayOfWeek,\n  Customers,\n  Open,\n  Promo,\n  tf.feature_column.indicator_column(StateHoliday),\n  SchoolHoliday,\n  tf.feature_column.indicator_column(StoreType),\n  tf.feature_column.indicator_column(Assortment),\n  CompetitionDistance,\n  trend,\n  Max_TemperatureC,\n  Mean_TemperatureC,\n  Min_TemperatureC,\n  Max_Humidity,\n  Mean_Humidity,\n  Min_Humidity\n]\n\n\ndef build_estimator(model_dir):\n  """Build an estimator."""\n  return tf.estimator.DNNLinearCombinedClassifier(\n    model_dir=model_dir,\n    dnn_feature_columns=deep_columns,\n    dnn_hidden_units=[100, 50])\n\n\ndef input_fn(data_file, num_epochs, shuffle):\n  df_data = pd.read_csv(data_file + ".csv",\n                        names=CSV_COLUMNS,\n                        dtype={"StateHoliday": str},\n                        skipinitialspace=True,\n                        engine="python",\n                        skiprows=1)\n\n  # remove NaN elements\n  df_data = df_data.dropna(how="any", axis=0)\n  df_data = df_data.sort_values([\'Sales\'], ascending=[True])\n  labels = df_data["Sales"].apply(lambda x: 1 if x &gt;= 20000 else 0)\n\n  return tf.estimator.inputs.pandas_input_fn(\n    x=df_data,\n    y=labels,\n    batch_size=100,\n    num_epochs=num_epochs,\n    shuffle=shuffle,\n    num_threads=5)\n\n\nm = build_estimator(model_dir="./model")\nm.train(input_fn=input_fn("df1", num_epochs=None, shuffle=True),\n        steps=2000)\n'
"parameters = [{'C': [0,1,5], 'kernel':['linear']},\n             {'C': [0,1,5], 'kernel':['rbf'], 'gamma':[0.01, 0.05]}]\n\nparameters = [{'C': [0.001, 0.1 ,1,5], 'kernel':['linear']},\n             {'C': [0.001, 0.1, 1,5], 'kernel':['rbf'], 'gamma':[0.01, 0.05]}]\n\ngs = GridSearchCV(estimator = reg, param_grid = parameters,cv =10)\n\ngs = GridSearchCV(estimator = reg, param_grid = parameters,\n                  scoring='neg_mean_squared_error', cv =10)\n"
'         class  max_speed\nfalcon    bird      389.0\nparrot    bird       24.0\nlion    mammal       80.5\nmonkey  mammal        NaN\n\ndf = df.reset_index()\nprint(df)\n\n    index   class   max_speed\n0   falcon  bird    389.0\n1   parrot  bird    24.0\n2   lion    mammal  80.5\n3   monkey  mammal  NaN\n\ndf = df.reset_index(drop = True)\ndf\n\n    class   max_speed\n0   bird    389.0\n1   bird    24.0\n2   mammal  80.5\n3   mammal  NaN\n'
'def get_inputs(feature_data, label_data, batch_size, n_epochs=None, shuffle=True):\n    dataset = tf.data.Dataset.from_tensor_slices( #from_tensor_slices\n        (feature_data, label_data))\n\n    dataset = dataset.repeat(n_epochs)\n    if shuffle:\n        dataset = dataset.shuffle(len(feature_data))\n    dataset = dataset.batch(batch_size)\n    features, labels = dataset.make_one_shot_iterator().get_next()\n    return features, labels\n\ndef get_pred_inputs(feature_data,n_epochs=None, shuffle=False):\n    dataset = tf.data.Dataset.from_tensor_slices( #from_tensor_slices\n        (feature_data))\n\n    dataset = dataset.repeat(n_epochs)\n    if shuffle:\n        dataset = dataset.shuffle(len(feature_data))\n    dataset = dataset.batch(1)\n    features = dataset\n    return features\n'
'from io import StringIO\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk import everygrams\n\ndef sent_process(sent):\n    return [\'\'.join(ng) for ng in everygrams(sent.replace(\' \', \'_ _\'), 1, 4) \n            if \' \' not in ng and \'\\n\' not in ng and ng != (\'_\',)]\n\nsent1 = "The quick brown fox jumps over the lazy brown dog."\nsent2 = "Mr brown jumps over the lazy fox."\nsent3 = \'Mr brown quickly jumps over the lazy dog.\'\nsent4 = \'The brown quickly jumps over the lazy fox.\'\n\nwith StringIO(\'\\n\'.join([sent1, sent2])) as fin:\n    # Override the analyzer totally with our preprocess text\n    count_vect = CountVectorizer(analyzer=sent_process)\n    count_vect.fit_transform(fin)\ncount_vect.vocabulary_ \n\n\ntrain_set = count_vect.fit_transform([sent1, sent2])\n\n# To train the classifier\nclf = MultinomialNB() \nclf.fit(train_set, [\'pos\', \'neg\']) \n\ntest_set = count_vect.transform([sent3, sent4])\nclf.predict(test_set)\n\nfrom collections import Counter\nfrom nltk import ngrams, word_tokenize\n\nfeatures = Counter(ngrams(word_tokenize(\'This is a something foo foo bar foo foo sentence\'), 2))\n\n&gt;&gt;&gt; features\nCounter({(\'This\', \'is\'): 1,\n         (\'a\', \'something\'): 1,\n         (\'bar\', \'foo\'): 1,\n         (\'foo\', \'bar\'): 1,\n         (\'foo\', \'foo\'): 2,\n         (\'foo\', \'sentence\'): 1,\n         (\'is\', \'a\'): 1,\n         (\'something\', \'foo\'): 1})\n\nfrom nltk import everygrams\n\nsent = word_tokenize(\'This is a something foo foo bar foo foo sentence\')\nCounter(everygrams(sent, 1, 4))\n\nCounter({(\'This\',): 1,\n         (\'This\', \'is\'): 1,\n         (\'This\', \'is\', \'a\'): 1,\n         (\'This\', \'is\', \'a\', \'something\'): 1,\n         (\'a\',): 1,\n         (\'a\', \'something\'): 1,\n         (\'a\', \'something\', \'foo\'): 1,\n         (\'a\', \'something\', \'foo\', \'foo\'): 1,\n         (\'bar\',): 1,\n         (\'bar\', \'foo\'): 1,\n         (\'bar\', \'foo\', \'foo\'): 1,\n         (\'bar\', \'foo\', \'foo\', \'sentence\'): 1,\n         (\'foo\',): 4,\n         (\'foo\', \'bar\'): 1,\n         (\'foo\', \'bar\', \'foo\'): 1,\n         (\'foo\', \'bar\', \'foo\', \'foo\'): 1,\n         (\'foo\', \'foo\'): 2,\n         (\'foo\', \'foo\', \'bar\'): 1,\n         (\'foo\', \'foo\', \'bar\', \'foo\'): 1,\n         (\'foo\', \'foo\', \'sentence\'): 1,\n         (\'foo\', \'sentence\'): 1,\n         (\'is\',): 1,\n         (\'is\', \'a\'): 1,\n         (\'is\', \'a\', \'something\'): 1,\n         (\'is\', \'a\', \'something\', \'foo\'): 1,\n         (\'sentence\',): 1,\n         (\'something\',): 1,\n         (\'something\', \'foo\'): 1,\n         (\'something\', \'foo\', \'foo\'): 1,\n         (\'something\', \'foo\', \'foo\', \'bar\'): 1})\n\ndef sent_vectorizer(sent):\n    return [\'\'.join(ng) for ng in everygrams(sent.replace(\' \', \'_ _\'), 1, 4) \n            if \' \' not in ng and ng != (\'_\',)]\nCounter(sent_vectorizer(\'This is a something foo foo bar foo foo sentence\'))\n\nCounter({\'o\': 9, \'s\': 4, \'e\': 4, \'f\': 4, \'_f\': 4, \'fo\': 4, \'oo\': 4, \'o_\': 4, \'_fo\': 4, \'foo\': 4, \'oo_\': 4, \'_foo\': 4, \'foo_\': 4, \'i\': 3, \'n\': 3, \'h\': 2, \'a\': 2, \'t\': 2, \'hi\': 2, \'is\': 2, \'s_\': 2, \'_s\': 2, \'en\': 2, \'is_\': 2, \'T\': 1, \'m\': 1, \'g\': 1, \'b\': 1, \'r\': 1, \'c\': 1, \'Th\': 1, \'_i\': 1, \'_a\': 1, \'a_\': 1, \'so\': 1, \'om\': 1, \'me\': 1, \'et\': 1, \'th\': 1, \'in\': 1, \'ng\': 1, \'g_\': 1, \'_b\': 1, \'ba\': 1, \'ar\': 1, \'r_\': 1, \'se\': 1, \'nt\': 1, \'te\': 1, \'nc\': 1, \'ce\': 1, \'Thi\': 1, \'his\': 1, \'_is\': 1, \'_a_\': 1, \'_so\': 1, \'som\': 1, \'ome\': 1, \'met\': 1, \'eth\': 1, \'thi\': 1, \'hin\': 1, \'ing\': 1, \'ng_\': 1, \'_ba\': 1, \'bar\': 1, \'ar_\': 1, \'_se\': 1, \'sen\': 1, \'ent\': 1, \'nte\': 1, \'ten\': 1, \'enc\': 1, \'nce\': 1, \'This\': 1, \'his_\': 1, \'_is_\': 1, \'_som\': 1, \'some\': 1, \'omet\': 1, \'meth\': 1, \'ethi\': 1, \'thin\': 1, \'hing\': 1, \'ing_\': 1, \'_bar\': 1, \'bar_\': 1, \'_sen\': 1, \'sent\': 1, \'ente\': 1, \'nten\': 1, \'tenc\': 1, \'ence\': 1})\n'
"model.add(Dense(6, activation='relu')) \n"
'def lossFunction(self,y_true,y_pred):\n\n    maxi=K.argmax(y_true) #ok\n\n    #invert the axes\n    y_pred = K.permute_dimensions(y_pred,(1,0))\n\n    return K.mean((K.max(y_true,axis=-1) -(K.gather(y_pred,maxi)))**2)\n'
'# Initialize real_training_set as a 2-tuple with (input, expected_result)\nif load_model_file is not None:\n    # Initialize dummy_training_set as a 2-tuple with (input, expected_result)\n    model.fit_generator(batch_generator_function(dummy_training_set[0],\n                                         dummy_training_set[1], ... ), epochs = 1)\n    save_load_utils.load_all_weights(model, load_from_model_file)\nmodel.fit_generator(batch_generator_function(real_training_set[0],\n                                             real_training_set[1], ... ), epochs = 1)\n'
'def add_missing_dummy_columns( d, columns ):\n        missing_cols = set( columns ) - set( d.columns )\n        for c in missing_cols:\n            d[c] = 0\n\ndef fix_columns( d, columns ):  \n\n    add_missing_dummy_columns( d, columns )\n\n    # make sure we have all the columns we need\n    assert( set( columns ) - set( d.columns ) == set())\n\n    extra_cols = set( d.columns ) - set( columns )\n    if extra_cols: print ("extra columns:", extra_cols)\n\n    d = d[ columns ]\n    return d\n\ntestFeatures= fix_columns( testFeatures, features.columns )\n'
"from sklearn.model_selection import KFold\nfrom sklearn.datasets import load_boston\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\nX, y = load_boston(return_X_y=True)\nn_splits = 5\nkf = KFold(n_splits=n_splits, shuffle=True)\nmodel_1 = DecisionTreeRegressor(max_depth = 4, criterion='mae',random_state=1)\nmodel_2 = DecisionTreeRegressor(max_depth = 8, criterion='mae', random_state=1)\n\ncv_mae_1 = []\ncv_mae_2 = []\n\nfor train_index, val_index in kf.split(X):\n    model_1.fit(X[train_index], y[train_index])\n    pred_1 = model_1.predict(X[val_index])\n    err_1 = mean_absolute_error(y[val_index], pred_1)\n    cv_mae_1.append(err_1)\n\n    model_2.fit(X[train_index], y[train_index])\n    pred_2 = model_2.predict(X[val_index])\n    err_2 = mean_absolute_error(y[val_index], pred_2)\n    cv_mae_2.append(err_2)\n\ncv_mae_1\n# result:\n[3.080392156862745,\n 2.8262376237623767,\n 3.164851485148514,\n 3.5514851485148515,\n 3.162376237623762] \n\ncv_mae_2\n# result\n[3.1460784313725494,\n 3.288613861386139,\n 3.462871287128713,\n 3.143069306930693,\n 3.2490099009900986]\n\nfrom scipy import stats\nstats.ttest_rel(cv_mae_1,cv_mae_2)\n# Ttest_relResult(statistic=-0.6875659723031529, pvalue=0.5295196273427171)\n"
"from sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.svm import SVC\n\ncls = OneVsRestClassifier(estimator=SVC(gamma ='auto'))\n\nimport numpy as np\ncls.fit(np.random.rand(20,10),np.random.binomial(1,0.2,size=(20,5)))\n"
'   # do not add burn-in samples\n    if n_sampled &gt; burn_in:\n        sample_list.append(z)\n\n    # Only keep iterations after burn-in and for every m-th iteration\n    if n_sampled &gt; burn_in and n_sampled % m == 0:\n        samples[(n_sampled - burn_in) // m] = z\n\ndef my_Metropolis_Gaussian(p, z0, sigma, n_samples=100, burn_in=0, m=1):\n    # Pre-allocate memory for samples (much more efficient than using append)\n    samples = np.zeros(n_samples)\n\n    # Store initial value\n    samples[0] = z0\n    z = z0\n    # Compute the current likelihood\n    l_cur = p(z)\n\n    # Counter\n    iter = 0\n    # Total number of iterations to make to achieve desired number of samples\n    iters = (n_samples * m) + burn_in\n\n    # Sample outside the for loop\n    innov = np.random.normal(loc=0, scale=sigma, size=iters)\n    u = np.random.rand(iters)\n\n    while iter &lt; iters:\n        # Random walk innovation on z\n        cand = z + innov[iter]\n\n        # Compute candidate likelihood\n        l_cand = p(cand)\n\n        # Accept or reject candidate\n        if l_cand / l_cur &gt; u[iter]:\n            z = cand\n            l_cur = l_cand\n\n        # Only keep iterations after burn-in and for every m-th iteration\n        if iter &gt; burn_in and iter % m == 0:\n            samples[(iter - burn_in) // m] = z\n\n        iter += 1\n\n    return samples\n\nIn [1]: %timeit Metropolis_Gaussian(pdf_t, 3, 1, n_samples=100, burn_in=100, m=10)\n205 ms ± 2.16 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nIn [2]: %timeit my_Metropolis_Gaussian(pdf_t, 3, 1, n_samples=100, burn_in=100, m=10)\n102 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n'
"from sklearn.feature_extraction.text import CountVectorizer\nraw_text = [\n    'The dog hates the black cat',\n    'The black dog is good'\n]\n\ncv = CountVectorizer()\ncv.fit_transform(raw_text)\n\n\nvocab = cv.vocabulary_.copy()\n\ndef lookup_key(string):\n    s = string.lower()\n    return [vocab[w] for w in s.split()]\n\nlist(map(lookup_key, raw_text))\n\n[[6, 2, 4, 6, 0, 1], [6, 0, 2, 5, 3]]\n"
"from sklearn import metrics\n\n# Get and show confussion matrix\ncm = metrics.confusion_matrix(y_test, y_pred)\nprint(cm)\n\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport seaborn as sns; sns.set()\n\ncm = confusion_matrix(y_test, y_pred)\ncmat_df = pd.DataFrame(cm, index=class_names, columns=class_names)\nax = sns.heatmap(cmat_df, square=True, annot=True, cbar=False)\nax.set_xlabel('Predicción')\nax.set_ylabel('Real')`\n"
"  model.add(Convolution2D(n_filters_1, d_filter, d_filter, border_mode='valid', input_shape=(data_w, data_h,2))) # 11 x 76\n  model.add(Activation('relu'))\n  model.add(Convolution2D(n_filters_1, d_filter, d_filter)) # 9 x 74\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2))) # 4 x 37\n  model.add(Dropout(p_drop_1))\n  model.add(Convolution2D(n_filters_2, d_filter, d_filter, border_mode='valid')) # 2 x 35\n  model.add(Activation('relu'))\n  model.add(Convolution2D(n_filters_2, d_filter, d_filter)) # 0 x 33 (!!!!)\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(p_drop_1))\n\n  model.add(Convolution2D(n_filters_1, d_filter, d_filter, border_mode='same', input_shape=(data_w, data_h,2))) # 13 x 78\n  model.add(Activation('relu'))\n  model.add(Convolution2D(n_filters_1, d_filter, d_filter), border_mode='same') # 13 x 78\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2))) # 6 x 39\n  model.add(Dropout(p_drop_1))\n  model.add(Convolution2D(n_filters_2, d_filter, d_filter, border_mode='same')) # 6 x 39\n  model.add(Activation('relu'))\n  model.add(Convolution2D(n_filters_2, d_filter, d_filter), border_mode='same') # 6 x 39\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2))) # 3 x 19\n  model.add(Dropout(p_drop_1))\n\n  model.add(Convolution2D(n_filters_1, d_filter, d_filter, border_mode='valid', input_shape=(data_w, data_h,2))) # 11 x 76\n  model.add(Activation('relu'))\n  model.add(Convolution2D(n_filters_1, d_filter, d_filter)) # 9 x 74\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2))) # 4 x 37\n  model.add(Dropout(p_drop_1))\n  model.add(Convolution2D(n_filters_2, d_filter, d_filter, border_mode='valid')) # 2 x 35\n  model.add(Activation('relu'))\n#  model.add(Convolution2D(n_filters_2, d_filter, d_filter)) # 0 x 33 (!!!!)\n#  model.add(Activation('relu'))\n#  model.add(MaxPooling2D(pool_size=(2, 2)))\n#  model.add(Dropout(p_drop_1))\n"
'from sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ntexts = [\n    "good movie", "not a good movie", "did not like", \n    "i like it", "good one"\n]\n# using default tokenizer in TfidfVectorizer\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5,norm=None,smooth_idf=False, ngram_range=(1, 2))\nfeatures = tfidf.fit_transform(texts)\npd.DataFrame(\n    features.todense(),\n    columns=tfidf.get_feature_names()\n)\n\n    good movie  like        movie       not\n0   1.916291    0.000000    1.916291    0.000000\n1   1.916291    0.000000    1.916291    1.916291\n2   0.000000    1.916291    0.000000    1.916291\n3   0.000000    1.916291    0.000000    0.000000\n4   0.000000    0.000000    0.000000    0.000000\n\n    good movie    like      movie       not\n0   1.693147    0.000000    1.693147    0.000000\n1   1.693147    0.000000    1.693147    1.693147\n2   0.000000    1.693147    0.000000    1.693147\n3   0.000000    1.693147    0.000000    0.000000\n4   0.000000    0.000000    0.000000    0.000000\n'
"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, cross_val_predict\nfrom sklearn.feature_selection import SelectKBest, f_regression \nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nnp.random.seed(0)\n\n\nd1 = np.random.randint(2, size=(50, 10))\nd2 = np.random.randint(3, size=(50, 10))\nd3 = np.random.randint(4, size=(50, 10))\nY = np.random.randint(7, size=(50,))\n\n\nX = np.column_stack([d1, d2, d3])\n\n\nn_smples, n_feats = X.shape\nprint (n_smples, n_feats)\n\n\nkf = KFold(n_splits=5, shuffle=True, random_state=0)\n\nregr = RandomForestRegressor(max_features=None,random_state=0)                \n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(regr, param_distributions={'n_estimators': [100, 300]},\n                                   n_iter=20, cv=kf,verbose=1,return_train_score=True)\nrandom_search.fit(X, Y)\n\nypredicts=random_search.predict(X)\nrmse = mean_squared_error(Y, ypredicts)\nprint(rmse)\nprint(random_search.best_params_)\nrandom_search.cv_results_\n"
"df['cluster'] = db.labels_\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\n\nX = np.array([[1, 2], [5, 8], [2, 3],\n               [8, 7], [8, 8], [2, 2]])\n\ndb = DBSCAN(eps=3, min_samples=2).fit(X)\ndb.labels_\n# array([0, 1, 0, 1, 1, 0], dtype=int64)\n\n# convert our numpy array to pandas:\ndf = pd.DataFrame({'Column1':X[:,0],'Column2':X[:,1]})\nprint(df)\n# result:\n   Column1  Column2\n0        1        2\n1        5        8\n2        2        3\n3        8        7\n4        8        8\n5        2        2\n\n# add new column with the belonging cluster:\ndf['cluster'] = db.labels_\n\nprint(df)\n# result:\n   Column1  Column2  cluster\n0        1        2        0\n1        5        8        1\n2        2        3        0\n3        8        7        1\n4        8        8        1\n5        2        2        0  \n"
'for epoch in range(EPOCHS):\n    avg_train_loss = 0\n    for trainbatch in train_loader:\n        X,y = trainbatch\n        net.zero_grad()\n        output = net(X.float())\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n        avg_train_loss += loss.item() / len(train_loader)\n    lloss.append(avg_train_loss)\n'
"from numpy import loadtxt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import optimizers\n\n# laod dataset\n# dataset = loadtxt('pima-indians-diabetes.csv', delimiter=',')\ndataset = loadtxt('sqrt.csv', delimiter=',')\n\n# split into input (X) and output (y) variables\nX = dataset[:,0:1] * 1.0\ny = dataset[:,1] * 1.0\n\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(6, input_dim=1, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1))\n\n# compile the keras model\nopt = optimizers.adam(lr=0.001)\nmodel.compile(loss='mean_squared_error', optimizer=opt)\n\n# fit the keras model on the dataset (CPU)\nmodel.fit(X, y, epochs=1500, batch_size=10, verbose=0)\n\n# evaluate the keras model\n_, accuracy = model.evaluate(X, y, verbose=0)\nprint('Accuracy: %.2f' % (accuracy*100))\n\n# make class predictions with the model\npredicitions = model.predict(X)\n\n# summarize the first 10 cases\nfor i in range(10):\n    print('%s =&gt; %.2f (expected %.2f)' % (X[i].tolist(), predicitions[i], y[i]))\n\n[1.0] =&gt; 1.00 (expected 1.00)\n[4.0] =&gt; 2.00 (expected 2.00)\n[9.0] =&gt; 3.32 (expected 3.00)\n[16.0] =&gt; 3.89 (expected 4.00)\n[25.0] =&gt; 4.61 (expected 5.00)\n[36.0] =&gt; 5.49 (expected 6.00)\n[49.0] =&gt; 6.52 (expected 7.00)\n[64.0] =&gt; 7.72 (expected 8.00)\n[81.0] =&gt; 9.07 (expected 9.00)\n[100.0] =&gt; 10.58 (expected 10.00)\n\ndef r_squared(y_true, y_pred):\n    from keras import backend as K\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n\nmodel.compile(loss='mean_squared_error', optimizer=opt, metrics=[r_squared])\n"
'# a list of 2 sentences\ninput_embedding = [\n    torch.FloatTensor([[-0.8264],  [0.2524]]),\n    torch.FloatTensor([[-0.3259],  [0.3564]])\n]\nfor item in input_embedding:\n    # item is a 2d tensor of shape `seq_len, input_size`\n    # so, we unsqueeze it to make it 3d `seq_len, 1, input_size` where batch_size = 1\n    y_pred = custom_model(item.unsqueeze(1))\n    print(y_pred.size()) # 2, 1, 2\n'
"def pca(X, number_of_pcs):\n    num_data, dim = X.shape\n\n    mean_X = X.mean(axis=0)\n    X = X - mean_X\n\n    if dim &gt; num_data:\n        # PCA compact trick\n        M = np.dot(X, X.T) # covariance matrix\n        e, U = np.linalg.eigh(M) # calculate eigenvalues an deigenvectors\n        tmp = np.dot(X.T, U).T\n        V = tmp[::-1] # reverse since the last eigenvectors are the ones we want\n        S = np.sqrt(e)[::-1] #reverse since the last eigenvalues are in increasing order\n        for i in range(V.shape[1]):\n            V[:,i] /= S\n\n        return V, S, mean_X\n\n    else:\n        # normal PCA, SVD method\n        U, S, V = np.linalg.svd(X, full_matrices=False)\n\n        # reconstruct the image using U, S and V\n        # otherwise you're just outputting the eigenvectors of X*X^T\n        V = V.T\n        S = np.diag(S)\n        X_hat = np.dot(U[:, :number_of_pcs], np.dot(S[:number_of_pcs, :number_of_pcs], V[:,:number_of_pcs].T))      \n\n        return X_hat, S, mean_X\n\nX_hat, S, mean_X = pca(img, 1)\nplt.imshow(X_hat)\n\nX_hat, S, mean_X = pca(img, 10)\nplt.imshow(X_hat)\n"
'x_train = x_train.reshape(len(x_train), x_train.shape[1], 1)\n\nx_train = numpy.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nmodel.add(LSTM(50, input_shape=(1,len(x_train[0]) )))\n\nx_train = x_train.reshape(len(x_train), x_train.shape[1], 1)\nmodel.add(LSTM(50, input_shape=(x_train.shape[1], 1) )))\n'
'new_df_reindexed = new_df[df_columns]\n'
'X = X.values\n\nX = pd.DataFrame(np.random.randn(100,2), columns=["Duration","Grand Mean"])\nX = X.values # &lt;--- put this line\ny = np.random.choice([11,12,13],100,True,[.33,.33,.34])\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap([\'orange\', \'cyan\', \'cornflowerblue\'])\ncmap_bold = ListedColormap([\'darkorange\', \'c\', \'darkblue\'])\n\nfor weights in [\'uniform\', \'distance\']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor=\'k\', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title("3-Class classification (k = %i, weights = \'%s\')"\n              % (n_neighbors, weights))\n\nplt.show()\n'
'train(\n    input_fn, hooks=None, steps=None, max_steps=None, saving_listeners=None\n)\n'
"from sklearn.model_selection import GroupShuffleSplit\n\nX = df.drop('label',1)\ny=df.label\n\ngs = GroupShuffleSplit(n_splits=2, test_size=.6, random_state=0)\ntrain_ix, test_ix = next(gs.split(X, y, groups=X.id))\n\nX_train = X.loc[train_ix]\ny_train = y.loc[train_ix]\n\nX_test = X.loc[test_ix]\ny_test = y.loc[test_ix]\n\nprint(X_train)\n\n      x  id\n4    50   2\n5    60   2\n8    90   4\n9   100   4\n10  110   5\n11  120   5\n\nprint(X_test)\n\n   x  id\n0  10   1\n1  20   1\n2  30   1\n3  40   1\n6  70   3\n7  80   3\n"
'library(mlbench)\nlibrary(randomForestSRC)\ndata(Sonar)\nset.seed(911)\ntrn = sample(nrow(Sonar),150)\nrf &lt;- rfsrc(Class ~ ., data = Sonar[trn,],ntree=500,block.size=1,importance=TRUE)\npred &lt;- predict(rf,Sonar[-trn,],block.size=1)\nplot(rf<span class="math-container">$err.rate[,1],type="l",col="steelblue",xlab="ntrees",ylab="err.rate",\nylim=c(0,0.5))\nlines(pred$</span>err.rate[,1],col="orange")\nlegend("topright",fill=c("steelblue","orange"),c("test","OOB.train"))\n\nlibrary(randomForest)\nrf &lt;- randomForest(Class ~ ., data = Sonar[trn,],ntree=500)\npred &lt;- predict(rf,Sonar[-trn,],predict.all=TRUE)\n\nerr_by_tree = sapply(1:ncol(pred<span class="math-container">$individual),function(i){\napply(pred$</span>individual[,1:i,drop=FALSE],1,\nfunction(i)with(rle(i),values[which.max(lengths)]))\n})\n\nerr_by_tree = colMeans(err_by_tree!=Sonar$Class[-trn])\n\nplot(rf$err.rate[,1],type="l",col="steelblue",xlab="ntrees",ylab="err.rate",\n    ylim=c(0,0.5))\n    lines(err_by_tree,col="orange")\n    legend("topright",fill=c("steelblue","orange"),c("test","OOB.train"))\n'
'from tensorflow.keras.layers import Input, Dense, Add, Activation, Flatten\nfrom tensorflow.keras.models import Model, Sequential\nimport tensorflow as tf\nimport numpy as np\nimport random\n\n\nfrom tensorflow.python.keras.layers import Input, GaussianNoise, BatchNormalization\n\ndef train_mnist():\n  class myCallback(tf.keras.callbacks.Callback):\n      def on_epoch_end(self, epoch,logs={}):\n          print(logs.get(\'accuracy\'))\n          if (logs.get(\'accuracy\')&gt;0.9):\n              print("Reached 90% accuracy so cancelling training!")\n              self.model.stop_training=True\n\n\n\n  mnist = tf.keras.datasets.mnist\n\n  (x_train, y_train),(x_test, y_test) = mnist.load_data()\n\n  x_train= x_train/255.0\n  x_test= x_test/255.0\n\n  callbacks=myCallback()\n\n  model = tf.keras.models.Sequential([\n      # YOUR CODE SHOULD START HERE\n      tf.keras.layers.Flatten(input_shape=(28, 28)),\n      tf.keras.layers.Dense(256, activation=tf.nn.relu),\n      tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n\n  ])\n\n  model.compile(optimizer=\'adam\',\n                loss=\'sparse_categorical_crossentropy\',\n                metrics=[\'accuracy\'])\n\n  # model fitting\n  history = model.fit(x_train,y_train, epochs=10,callbacks=[callbacks]) \n  # model fitting\n  return history.epoch, history.history[\'accuracy\'][-1]\n\ntrain_mnist()\n\n\n\n\nEpoch 1/10\n1859/1875 [============================&gt;.] - ETA: 0s - loss: 0.2273 - accuracy: 0.93580.93586665391922\nReached 90% accuracy so cancelling training!\n1875/1875 [==============================] - 3s 2ms/step - loss: 0.2265 - accuracy: 0.9359\n\n([0], 0.93586665391922)\n'
'import numpy as np\n\ny_pred = [ [0,0,0,0,1], [0,1,0,0,0], [0,0,0,1,0], [1,0,0,0,0], [0,0,1,0,0]]\n\ny_pred = np.array(y_pred)\n\nprint(y_pred)\n\nshift = 3\n\none_pos = np.where(y_pred==1)[1] # indices where the y_pred is 1\n# updating the new positions with 1\ny_pred[range(y_pred.shape[1]),one_pos - shift] = np.ones((y_pred.shape[1],))\n# making the old positions zero\ny_pred[range(y_pred.shape[1]),one_pos] = np.zeros((y_pred.shape[1],))\n\nprint(y_pred)\n\n[[0 0 0 0 1]\n [0 1 0 0 0]\n [0 0 0 1 0]\n [1 0 0 0 0]\n [0 0 1 0 0]]\n[[0 1 0 0 0]\n [0 0 0 1 0]\n [1 0 0 0 0]\n [0 0 1 0 0]\n [0 0 0 0 1]]\n\nimport numpy as np\n\ny_pred = [ [0,0,0,0,1], [0,1,0,0,0], [0,0,0,1,0], [1,0,0,0,0], [0,0,1,0,0]]\n\ny_pred = np.array(y_pred)\n\nprint(y_pred)\n\nshift = 3\n\none_pos = np.where(y_pred==1)[1]# indices where the y_pred is 1\n\nprint(one_pos)\n\ny_pred[range(y_pred.shape[1]),one_pos - shift] = [1 if (i == 3 or i == 4) else 0 for i in one_pos]\ny_pred[range(y_pred.shape[1]),one_pos] = [0 if (i == 3 or i == 4) else 1 for i in one_pos]\n\nprint(y_pred)\n\n[[0 0 0 0 1]\n [0 1 0 0 0]\n [0 0 0 1 0]\n [1 0 0 0 0]\n [0 0 1 0 0]]\n[4 1 3 0 2]\n[[0 1 0 0 0]\n [0 1 0 0 0]\n [1 0 0 0 0]\n [1 0 0 0 0]\n [0 0 1 0 0]]\n'
'model.fit(X_train, y_train, ..., validation_data = (X_test, y_test))\n'
"#filter only columns by Iteration with number\ndf = Input1.filter(regex='Iteration\\d+$')\n#get first mode\ns = df.mode(axis=1).iloc[:, 0]\n#compare df for all possible modes, add suffix for match errors columns, \n#last filter original with min\ns1 = Input1.where(df.eq(s, axis=0).add_suffix('_error')).min(axis=1)\n\n#add new columns\nOutput1 = Input1.assign(best_mode = s, best_error=s1)\nprint (Output1)\n   Iteration1  Iteration1_error Iteration2  Iteration2_error Iteration3  \\\nI1         M2                96         M3                76         M3   \nI2         M1                98         M1                88         M1   \nI3         M3                34         M1                54         M1   \nI4         M5                19         M5                12         M5   \nI5         M4                22         M6                92         M6   \nI6         M6                 9         M4                19         M4   \n\n    Iteration3_error best_mode  best_error  \nI1                66        M3        66.0  \nI2                68        M1        68.0  \nI3                84        M1        54.0  \nI4                52        M5        12.0  \nI5                72        M6        72.0  \nI6                89        M4        19.0  \n\ndf = Input1.iloc[:, ::2]\ns = df.mode(axis=1).iloc[:, 0]\ns1 = Input1.iloc[:, 1::2].where(df.eq(s, axis=0).to_numpy()).min(axis=1)\n\nOutput1 = Input1.assign(best_mode = s, best_error=s1)\n"
'def call(self, x):\n    q_z = self.encode(x)\n    z = q_z.sample()\n    x_recon = self.decode(z)\n'
'scores = []\nfor other in prefs:\n    if other != person:\n        scores.append((similarity(prefs, person, other))\n'
'C=[]\ngamma=[]\nfor i in range(21): C.append(10.0**(i-5))\nfor i in range(17): gamma.append(10**(i-14))\n'
'mask = (feature_series &gt; 10) | (feature_series &lt; 10)\n'
"return { \n     'word_after_you': after_keyword, \n     'word_before_you': before_keyword \n      }\n\nlang_detector = NaiveBayesClassifier(train, feature_extractor=get_word_features)\nlang_detector.show_most_informative_features(5)\n"
'package edu.umass.cs.iesl.wikilink.expanded.process\n\nimport org.apache.thrift.protocol.TBinaryProtocol\nimport org.apache.thrift.transport.TIOStreamTransport\nimport java.io.File\nimport java.io.BufferedOutputStream\nimport java.io.FileOutputStream\nimport java.io.BufferedInputStream\nimport java.io.FileInputStream\nimport java.util.zip.{GZIPOutputStream, GZIPInputStream}\n\n object ThriftSerializerFactory {\n\n   def getWriter(f: File) = {\n      val stream = new BufferedOutputStream(new GZIPOutputStream(new FileOutputStream(f)), 2048)\n      val protocol= new TBinaryProtocol(new TIOStreamTransport(stream))\n      (stream, protocol)\n   }\n\n   def getReader(f: File) = {\n      val stream = new BufferedInputStream(new GZIPInputStream(new FileInputStream(f)), 2048)\n      val protocol = new TBinaryProtocol(new TIOStreamTransport(stream))\n      (stream, protocol)\n   }\n } \n\nclass PerFileWebpageIterator(f: File) extends Iterator[WikiLinkItem] {\n    var done = false\n    val (stream, proto) = ThriftSerializerFactory.getReader(f)\n    private var _next: Option[WikiLinkItem] = getNext()\n\n    private def getNext(): Option[WikiLinkItem] = try {\n        Some(WikiLinkItem.decode(proto))\n    } catch {case _: TTransportException =&gt; {done = true; stream.close(); None}}\n\n    def hasNext(): Boolean = !done &amp;&amp; (_next != None || {_next = getNext(); _next != None})\n\n    def next(): WikiLinkItem = if (hasNext()) _next match {\n        case Some(wli) =&gt; {_next = None; wli}\n        case None =&gt; {throw new Exception("Next on empty iterator.")}\n    } else throw new Exception("Next on empty iterator.")\n}\n'
'model = MultinomialNB(0.5).fit(X, y)\n# or\nmodel = LogisticRegression().fit(X, y)\n'
'for inProp, num in ds:\n    out = net.activate(inProp).argmax()\n    if out == num.argmax():\n        true+=1\n    total+=1\nres = true/total\n'
'./examples/mnist/train_lenet.sh | grep -v "Read time:" | grep -v "Prefetch batch:"\n\nI0112 22:50:49.671303 114020 data_layer.cpp:104] Transform time: 0.791 ms.\nI0112 22:50:49.672757 114020 data_layer.cpp:104] Transform time: 0.767 ms.\nI0112 22:50:49.674334 114020 data_layer.cpp:104] Transform time: 0.836 ms.\nI0112 22:50:49.675853 114020 data_layer.cpp:104] Transform time: 0.806 ms.\nI0112 22:50:49.677273 114020 data_layer.cpp:104] Transform time: 0.762 ms.\nI0112 22:50:49.678861 114020 data_layer.cpp:104] Transform time: 0.861 ms.\nI0112 22:50:49.680376 114020 data_layer.cpp:104] Transform time: 0.821 ms.\nI0112 22:50:49.681077 113921 solver.cpp:409]     Test net output #0: accuracy = 0.9902\nI0112 22:50:49.681115 113921 solver.cpp:409]     Test net output #1: loss = 0.0292544 (* 1 = 0.0292544 loss)\nI0112 22:50:49.681125 113921 solver.cpp:326] Optimization Done.\nI0112 22:50:49.681133 113921 caffe.cpp:215] Optimization Done.\nI0112 22:50:49.681948 114020 data_layer.cpp:104] Transform time: 0.829 ms.\n'
'y = df.iloc[:, -1].values\nX = df.iloc[:, :-1].values\n\nIn [153]: X\nOut[153]:\narray([[2, 3, 4, 5, 5, 7],\n       [3, 4, 5, 3, 2, 3]], dtype=int64)\n\nIn [154]: y\nOut[154]: array([ 11.32,  10.99])\n\ny = df.iloc[:, -1]\nX = df.iloc[:, :-1]\n\nIn [156]: X\nOut[156]:\n   x1  x2  x3  x4  x5  x6\n0   2   3   4   5   5   7\n1   3   4   5   3   2   3\n\nIn [157]: y\nOut[157]:\n0    11.32\n1    10.99\nName: y, dtype: float64\n'
'with tf.Session() as sess:\n\nsess = tf.Session()\n'
"X = dataset.drop('column_9', 1).values\ny = dataset['column_9'].values\n"
"In [204]: from sklearn.model_selection import cross_val_score, StratifiedKFold\n\nIn [205]: from sklearn import datasets\n\nIn [206]: from sklearn import svm\n\nIn [207]: iris = datasets.load_iris()\n\nIn [208]: X, y = iris.data, iris.target\n\nIn [209]: clf = svm.SVC(kernel='linear', C=1)\n\nIn [210]: skf = StratifiedKFold(n_splits=5, random_state=0)\n\nIn [211]: scores = cross_val_score(clf, X, y, cv=skf)\n\nIn [212]: scores\nOut[212]: array([ 0.9667,  1.    ,  0.9667,  0.9667,  1.    ])\n\nIn [213]: scores.mean()\nOut[213]: 0.98000000000000009\n\nClassification No.   Training Samples   Test Samples   Accuracy\n1                    A + B + C + D      E              0.9667\n2                    A + B + C + E      D              1.\n3                    A + B + D + E      C              0.9667\n4                    A + C + D + E      B              0.9667\n5                    B + C + D + E      A              1.\n"
'Non-negativity: d(x, y) &gt;= 0\nIdentity: d(x, y) = 0 if and only if x == y\nSymmetry: d(x, y) = d(y, x)\nTriangle Inequality: d(x, y) + d(y, z) &gt;= d(x, z)\n'
'nn = Conv3DDNNLayer(nn, 8, 3)\n\nmodel.add(Convolution3D(8, 3, 3, 3, ...)\n\nconv_3d_output = Convolution3D(8, 3, 3, 3, ...)(conv_3d_input)\n'
"shell_exec('python c://xampp/htdocs/fbchat/public/test.py ' . $s);\n"
'tensor_2 = tf.reduce_mean(tf.reshape(tensor_1, shape=[12, -1, 2]), axis=1)\n\nsec_len = 12\n_shape = tf.shape(tensor_1)\npad_size =  tf.cast(tf.ceil(_shape[0] / sec_len)*sec_len, tf.int32) - _shape[0]\nzero_padding = tf.zeros((pad_size, _shape[1]), dtype=tensor_1.dtype)\npadded = tf.concat([tensor_1, zero_padding], 0)\ntensor_2 = tf.reduce_mean(tf.reshape(padded, shape=[sec_len, -1, 2]), axis=1)\n'
'df = pd.merge(df1,df2,"inner")\n\ndf = pd.merge(df1,df2,"outer")\n\na = df1.groupby(\'ID\')[\'asset\'].unique()\n# to have something like\n#ID\n#A    [1, 2, 4] \n#B          [1]\n\nx = a.size\ny = max([max(x) for x in a])\nz = np.zeros((x,y))\n# just instantiating the final matrix with the right shape and zeros\n\nfor row in range(x):\n    np.put(z[row],a[row]-1,1)\n\n[[ 1.  1.  1.]\n [ 1.  0.  0.]]\n'
'  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, W) + b\n\n  # Define loss and optimizer\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  cross_entropy = tf.reduce_mean(\n      tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession()\n  tf.global_variables_initializer().run()\n  # Train\n  for _ in range(1000):\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n  # Test trained model\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                      y_: mnist.test.labels}))\n'
"W1 = tf.Variable(tf.zeros([hidden1_size, input_size], tf.float32, name='weights_1st_layer'))\nW2 = tf.Variable(tf.zeros([output_size, hidden1_size], tf.float32, name='weights_2nd_layer'))\n\nW1 = tf.Variable(tf.truncated_normal([hidden1_size, input_size], tf.float32, name='weights_1st_layer'), stddev=0.1))\nW2 = tf.Variable(tf.truncated_normal([output_size, hidden1_size], tf.float32, name='weights_2nd_layer'), stddev=0.1))\n"
'sess.run(training_op, feed_dict={X: [X_batch], y: y_batch})\n'
"test_matrix = vectorizer.fit_transform(test_data['review_clean'])\n\ntest_matrix = vectorizer.transform(test_data['review_clean'])\n"
'def get_model():\n    model = Model(sequence_input, preds)\n    model.compile(loss=\'binary_crossentropy\',\n                  optimizer=\'adam\',\n                  metrics=[\'acc\'])\n\n    print("model fitting - Bidirectional LSTM")\n    model.summary()\n    return model\n\n#estimate the accuracy\nestimator = KerasClassifier(build_fn=get_model, epochs=50, batch_size=64, verbose=2)\n'
'new_saver.restore(sess, tf.train.latest_checkpoint(\'./\'))\n\nnew_saver.restore(sess, args.model_meta_filename.replace(".meta", ""))\n'
"class_zero = df.index[df['Class'] == 0].tolist()\n\ninstances = X_train.iloc[class_zero].values\n\nfor n in class_zero:\n    instances = X_train.iloc[n].values.reshape(1,-1)\n\n    predictValue = rf.predict(instances)\n    actualValue = y_train.iloc[n]\n\n    print('##')\n    print(n)\n    print(predictValue)\n    print(actualValue)\n    print('##')\n"
'from sklearn import linear_model\nx=np.arange(12).reshape(3,4)\ny=np.arange(3,6).reshape(3,1)    \nreg=linear_model.LinearRegression()\nreg.fit(x,y)\n\nIn [32]: reg.coef_\nOut[32]: array([[ 0.0625,  0.0625,  0.0625,  0.0625]])\n\nIn [33]: reg.intercept_\nOut[33]: array([ 2.625])\n\nIn [34]: x.dot(reg.coef_.T) + reg.intercept_\nOut[34]:\narray([[ 3.],\n       [ 4.],\n       [ 5.]])\n\nx=np.arange(12).reshape(3,4)\ny=np.arange(3,6).reshape(3,1)\nx=np.insert(x,0,1,axis=1)\nreg = linear_model.LinearRegression(fit_intercept=False)\nreg.fit(x,y)\n\nIn [37]: reg.coef_\nOut[37]: array([[ 0.4375 , -0.59375, -0.15625,  0.28125,  0.71875]])\n'
'from scipy.sparse import hstack\nnew_matrix = hstack([matrix, density])\n'
"df = pd.DataFrame({'A' : [1,np.nan,3], 'B' : [20,30,40]})\nprint(df)\n\ndf_t = df.describe()\nprint(type(df_t))\nprint(df_t)\nprint(df_t.columns)\nprint(df_t.index)\n\ncol_names = []\nfor stat_name in df_t.index:\n    for col_name in df_t.columns:\n        col_names.append(str(col_name)+'_'+str(stat_name))\nprint('col_names',col_names)\nN = len(col_names)\nprint('len(col_names)', N)\n\nrow = df_t.values.reshape(1,N)\nprint('row.shape',row.shape)\ndf_stat = pd.DataFrame(data=row, columns=col_names)\nprint(df_stat)\n\n     A   B\n0  1.0  20\n1  NaN  30\n2  3.0  40\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n              A     B\ncount  2.000000   3.0\nmean   2.000000  30.0\nstd    1.414214  10.0\nmin    1.000000  20.0\n25%    1.500000  25.0\n50%    2.000000  30.0\n75%    2.500000  35.0\nmax    3.000000  40.0\nIndex(['A', 'B'], dtype='object')\nIndex(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'], dtype='object')\ncol_names ['A_count', 'B_count', 'A_mean', 'B_mean', 'A_std', 'B_std', 'A_min', 'B_min', 'A_25%', 'B_25%', 'A_50%', 'B_50%', 'A_75%', 'B_75%', 'A_max', 'B_max']\nlen(col_names) 16\nrow.shape (1, 16)\n   A_count  B_count  A_mean  B_mean     A_std  B_std  A_min  B_min  A_25%  \\\n0      2.0      3.0     2.0    30.0  1.414214   10.0    1.0   20.0    1.5   \n\n   B_25%  A_50%  B_50%  A_75%  B_75%  A_max  B_max  \n0   25.0    2.0   30.0    2.5   35.0    3.0   40.0\n\ndf = pd.DataFrame({'A' : [1,np.nan,3], 'B' : [20,30,40]})\nprint(df)\n\ndf_t = df.describe()\nprint(type(df_t))\nprint(df_t)\nprint(df_t.columns)\nprint(df_t.index)\n\ndf_s = df_t.stack()\nprint(type(df_s))\nprint(df_s)\nprint(df_s.shape)\n\ndf_s.index = df_s.index.map(lambda x : '_'.join(x[::-1]))\nprint(type(df_s))\nprint(df_s)\ndf_s = df_s.to_frame().T\nprint(type(df_s))\nprint(df_s)\n\n     A   B\n0  1.0  20\n1  NaN  30\n2  3.0  40\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n              A     B\ncount  2.000000   3.0\nmean   2.000000  30.0\nstd    1.414214  10.0\nmin    1.000000  20.0\n25%    1.500000  25.0\n50%    2.000000  30.0\n75%    2.500000  35.0\nmax    3.000000  40.0\nIndex(['A', 'B'], dtype='object')\nIndex(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'], dtype='object')\n&lt;class 'pandas.core.series.Series'&gt;\ncount  A     2.000000\n       B     3.000000\nmean   A     2.000000\n       B    30.000000\nstd    A     1.414214\n       B    10.000000\nmin    A     1.000000\n       B    20.000000\n25%    A     1.500000\n       B    25.000000\n50%    A     2.000000\n       B    30.000000\n75%    A     2.500000\n       B    35.000000\nmax    A     3.000000\n       B    40.000000\ndtype: float64\n(16,)\n&lt;class 'pandas.core.series.Series'&gt;\nA_count     2.000000\nB_count     3.000000\nA_mean      2.000000\nB_mean     30.000000\nA_std       1.414214\nB_std      10.000000\nA_min       1.000000\nB_min      20.000000\nA_25%       1.500000\nB_25%      25.000000\nA_50%       2.000000\nB_50%      30.000000\nA_75%       2.500000\nB_75%      35.000000\nA_max       3.000000\nB_max      40.000000\ndtype: float64\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n   A_count  B_count  A_mean  B_mean     A_std  B_std  A_min  B_min  A_25%  \\\n0      2.0      3.0     2.0    30.0  1.414214   10.0    1.0   20.0    1.5   \n\n   B_25%  A_50%  B_50%  A_75%  B_75%  A_max  B_max  \n0   25.0    2.0   30.0    2.5   35.0    3.0   40.0  \n\ndf = pd.DataFrame({'A' : [1,np.nan,3], 'B' : [20,30,40]})\nprint(df)\n\ndef func(df, func_name):    \n    if func_name == 'max':\n        df_t = df.max(axis=0)\n    elif func_name == 'min':\n        df_t = df.min(axis=0)\n    elif func_name == 'sum':\n        df_t = df.sum(axis=0)\n    else:\n        raise NotImplementedError\n\n    df_t = df_t.to_frame().T\n    print(type(df_t))\n    print(df_t)\n    df_t.rename(columns=lambda x: x+'_'+func_name,inplace=True)\n    print(type(df_t))\n    print(df_t)\n\n    return df_t\n\nfunc_names = ['min','max','sum']\ndf_list = []\nfor func_name in func_names:\n    df_t = func(df, func_name)\n    df_list.append(df_t)\n\ndf_stat = pd.concat(df_list, axis=1)\nprint(df_stat)\n\n     A   B\n0  1.0  20\n1  NaN  30\n2  3.0  40\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n     A     B\n0  1.0  20.0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n   A_min  B_min\n0    1.0   20.0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n     A     B\n0  3.0  40.0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n   A_max  B_max\n0    3.0   40.0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n     A     B\n0  4.0  90.0\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n   A_sum  B_sum\n0    4.0   90.0\n   A_min  B_min  A_max  B_max  A_sum  B_sum\n0    1.0   20.0    3.0   40.0    4.0   90.0\n"
"with tf.variable_scope('input'):\n    x = tf.placeholder(tf.float32,shape=[None ,window_size,3]) #input tensor with 3 input channels\n    y = tf.placeholder(tf.float32,shape=[None,6]) #Labels\n\nwith tf.variable_scope('net'):\n\n    con_layer_1 = convolution_layer(x,shape=[4,3,32])#filter  of shape [filter_width, in_channels, out_channels]\n\n    max_pool_1=tf.layers.max_pooling1d(inputs=con_layer_1,pool_size=2,strides=2,padding='Valid')\n\n    con_layer_2 = convolution_layer(max_pool_1,shape=[4,32,64])\n\n    max_pool_2 = tf.layers.max_pooling1d(inputs=con_layer_2,pool_size=2,strides=2,padding='Valid')\n\n    flat = tf.reshape(max_pool_2,[-1,max_pool_2.get_shape()[1]*max_pool_2.get_shape()[2]])\n\n    fully_conected = tf.nn.relu(normal_full_layer(flat,1024))\n\n\n    second_hidden_layer = tf.nn.relu(normal_full_layer(fully_conected,512))\n    hold_prob = tf.placeholder(tf.float32)\n    full_one_dropout = tf.nn.dropout(second_hidden_layer,keep_prob=hold_prob)\n\n\n    y_pred = normal_full_layer(full_one_dropout,6)\n    pred_softmax = tf.nn.softmax(y_pred)\n\nwith tf.variable_scope('loss'):\n\n    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_pred))\n\nwith tf.variable_scope('optimizer'):\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(cross_entropy)\n"
'import torch\n\nD_in = 100\nD_out = 100\nbatch = 512\n\nmodel=torch.nn.Sequential(\n     torch.nn.Linear(D_in,D_out),\n)\n\nx_data=torch.rand(batch,D_in)\ny_data=torch.randn(batch,D_out)\nfor i in range(batch):\n    for j in range(D_in):\n         y_data[i][j]=3*x_data[i][j]+4 # model thinks y=mx+c -&gt; y=mx+2c?\n\nloss_fn=torch.nn.MSELoss(size_average=False)\noptimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n\nfor epoch in range(10000):\n    y_pred=model(x_data)\n    loss=loss_fn(y_pred,y_data)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\ntest_data=torch.ones(batch,D_in)\ny_pred=model(test_data)\nprint(y_pred)\n'
"def POLICY_EVALUATION(policy_vec, utility_vec, mdp):\n    new_utility_vector = utility_vec\n    delta = 100000.0\n\n    while delta &gt; 0.00001:\n        delta = 0.0\n\n        for s in mdp.states:\n            old_vs = {s: new_utility_vector[s] for s in new_utility_vector}\n            to_sum = [(mdp.P_dest_start_action[(s_tag, s, policy_vec[s])] * new_utility_vector[s_tag])\n                      for s_tag in mdp.states]\n            new_utility_vector[s] = mdp.rewards[s] + gamma * sum(to_sum)\n            delta = max(delta, max([abs(new_utility_vector[s] - old_vs[s]) for s in old_vs]))\n\n    return new_utility_vector\n\n===========================END===============================\n('S_O policy =', 'blue', ' ,S_1 Policy =', 'red')\n"
'dataset = tf.data.Dataset.from_tensor_slices((images,labels))\ndataset = dataset.map(parse_function)\npatches_dataset = dataset.apply(tf.contrib.data.unbatch())\nbatched_dataset = dataset.batch(batch_size)\n\ndef parse_function(images, labels):\n  # ...\n  return image, tf.tile([labels], tf.shape(image)[0:1])\n'
"import tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nx_data = np.random.randint(1000, 8000, 10000)\ny_true = x_data + 250\n\n\nfeat_cols = tf.feature_column.numeric_column('x')\noptimizer = tf.train.AdamOptimizer(1e-3)\n\nestimator = tf.estimator.LinearRegressor(feature_columns=[feat_cols],optimizer=optimizer)\n\nx_train, x_eval, y_train, y_eval = train_test_split(x_data, y_true, test_size=0.3, random_state=101)\n\n\ntrain_input_func = tf.estimator.inputs.numpy_input_fn({'x': x_train}, y_train, batch_size=1, num_epochs=None,\n                                                      shuffle=True)\n\neval_input_func = tf.estimator.inputs.numpy_input_fn({'x': x_eval}, y_eval, batch_size=1, num_epochs=None,\n                                                     shuffle=True)\n\nestimator.train(input_fn=train_input_func, steps=1005555)\n\ntrain_metrics = estimator.evaluate(input_fn=train_input_func, steps=10000)\neval_metrics = estimator.evaluate(input_fn=eval_input_func, steps=10000)\n\nprint(train_metrics)\nprint(eval_metrics)\n\nbrand_new_data = np.array([1000, 2000, 7000])\ninput_fn_predict = tf.estimator.inputs.numpy_input_fn({'x': brand_new_data}, num_epochs=1,shuffle=False)\n\nprediction_result = estimator.predict(input_fn=input_fn_predict)\n\nfor prediction in prediction_result:\n    print(prediction['predictions'])\n"
'def derivative_activation(z):\n    """\n        compute the derivative of the activation (derivative of sigmoide)\n    """\n    return activation(z) * (1 - activation(z))\n\ndef derivative_activation(z):\n    """\n        compute the derivative of the activation (derivative of sigmoide)\n    """\n    return z * (1 - z)\n'
"resnet_model = tf.keras.applications.resnet50.ResNet50(include_top=False, \nweights=None, input_tensor=None, input_shape=(height, width, channels), \npooling='max')\n\nx = Dense(128, activation='relu')(resnet_model.output)\nx = Dense(1, activation='relu')(x)\n\nmodel = Model(resnet_model.input, x)\n"
'@property\ndef layers(self):\n    # Historically, `sequential.layers` only returns layers that were added\n    # via `add`, and omits the auto-generated `InputLayer`\n    # that comes at the bottom of the stack.\n    if self._layers and isinstance(self._layers[0], InputLayer):\n        return self._layers[1:]\n    return self._layers\n'
'import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\n# dummy data\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\nestimator = DecisionTreeRegressor(max_depth=3)\nestimator.fit(X, y)\n\nThe binary tree structure has 15 nodes and has the following tree structure: \nnode=0 test node: go to node 1 if X[:, 0] &lt;= 3.13275051117 else to node 8. \n       node=1 test node: go to node 2 if X[:, 0] &lt;= 0.513901114464 else to node 5. \n              node=2 test node: go to node 3 if X[:, 0] &lt;= 0.0460066311061 else to node 4. \n                     node=3 leaf node. \n                     node=4 leaf node. \n              node=5 test node: go to node 6 if X[:, 0] &lt;= 2.02933192253 else to node 7. \n                     node=6 leaf node. \n                     node=7 leaf node. \n       node=8 test node: go to node 9 if X[:, 0] &lt;= 3.85022854805 else to node 12. \n              node=9 test node: go to node 10 if X[:, 0] &lt;= 3.42930102348 else to node 11. \n                     node=10 leaf node. \n                     node=11 leaf node. \n              node=12 test node: go to node 13 if X[:, 0] &lt;= 4.68025827408 else to node 14. \n                     node=13 leaf node. \n                     node=14 leaf node.\n\nlen(estimator.tree_.value)\n# 15\n\nestimator.tree_.value[3]\n# array([[-1.1493464]])\n'
'samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n\nfrom sklearn.neighbors import LocalOutlierFactor\nlof = LocalOutlierFactor(n_neighbors=3,novelty=True)\nlof.fit(samples) \nroc_auc(1/lof.score_samples(X_test),y_test)\n'
"import numpy, scipy, matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\n\nxData = numpy.array([0.048, 0.049, 0.05, 0.05, 0.06, 0.089, 0.1, 0.12, 0.134])\nyData = numpy.array([60, 59, 58, 57, 56, 55, 50, 30, 10])\n\n\ndef func(x, a, b, c): # simple quadratic example\n    return (a * numpy.square(x)) + b * x + c\n\n\n# these are the same as the scipy defaults\ninitialParameters = numpy.array([1.0, 1.0, 1.0])\n\n# curve fit the test data\nfittedParameters, pcov = curve_fit(func, xData, yData, initialParameters)\n\nmodelPredictions = func(xData, *fittedParameters) \n\nabsError = modelPredictions - yData\n\nSE = numpy.square(absError) # squared errors\nMSE = numpy.mean(SE) # mean squared errors\nRMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\nRsquared = 1.0 - (numpy.var(absError) / numpy.var(yData))\n\nprint('Parameters:', fittedParameters)\nprint('RMSE:', RMSE)\nprint('R-squared:', Rsquared)\n\nprint()\n\n\n##########################################################\n# graphics output section\ndef ModelAndScatterPlot(graphWidth, graphHeight):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    # first the raw data as a scatter plot\n    axes.plot(xData, yData,  'D')\n\n    # create data for the fitted equation plot\n    xModel = numpy.linspace(min(xData), max(xData))\n    yModel = func(xModel, *fittedParameters)\n\n    # now the model as a line plot\n    axes.plot(xModel, yModel)\n\n    axes.set_xlabel('X Data') # X axis data label\n    axes.set_ylabel('Y Data') # Y axis data label\n\n    plt.show()\n    plt.close('all') # clean up after using pyplot\n\ngraphWidth = 800\ngraphHeight = 600\nModelAndScatterPlot(graphWidth, graphHeight)\n"
"encoder_output = Flatten()(encoded)\ndecoder_input = Reshape((7, 7, 32))(encoder_output)\n\ndecoder = Conv2D(32, (3, 3), activation='relu', padding='same')(decoder_input)\n"
"model.compile(loss='mae', optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n\n#get the lowest validation loss of the training epochs\nvalidation_loss = np.amin(result.history['val_loss']) \nprint('Best validation loss of epoch:', validation_loss)\nreturn {'loss': validation_loss, 'status': STATUS_OK, 'model': model}\n"
"&gt;&gt;&gt; features = grid_search.best_estimator_.named_steps['features']\n\n# number of components chosen from pca\n&gt;&gt;&gt; pca=features.transformer_list[0][1]\n\n&gt;&gt;&gt; pca.n_components\n3\n\n# features chosen by selectKbest\n&gt;&gt;&gt; select_k_best=features.transformer_list[1][1]\n\n&gt;&gt;&gt; select_k_best.get_support()\narray([False, False,  True, False])\n"
"model = keras.Sequential([\nkeras.layers.Dense(1),\nkeras.layers.Dense(5, activation=tf.nn.relu),\nkeras.layers.Dense(5, activation=tf.nn.relu),\nkeras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nmodel.fit(np.array(inputs_training), np.array(targets_training), epochs=5, verbose=1, class_weight = {0:4, 1:1})\n"
"&gt;&gt;&gt; model = Sequential()\n&gt;&gt;&gt; model.add(SimpleRNN(50, input_shape=(20, 26), return_sequences=True))\n&gt;&gt;&gt; from keras.layers import Flatten\n&gt;&gt;&gt; model.add(Flatten())\n&gt;&gt;&gt; model.add(Dense(40, activation='relu'))\n&gt;&gt;&gt; model.add(Dense(num_categories, activation='softmax'))\n&gt;&gt;&gt; model.compile(loss='categorical_crossentropy', optimizer='adam')\n&gt;&gt;&gt; model.fit(X, Y, epochs=20, batch_size=5, verbose=1)\n1043/1043 [==============================] - 3s 3ms/step - loss: -0.0735\n\n&gt;&gt;&gt; model = Sequential()\n&gt;&gt;&gt; model.add(SimpleRNN(50, input_shape=(20, 26)))\n&gt;&gt;&gt; model.add(Dense(40, activation='relu'))\n&gt;&gt;&gt; model.add(Dense(num_categories, activation='softmax'))\n&gt;&gt;&gt; model.compile(loss='categorical_crossentropy', optimizer='adam')\n&gt;&gt;&gt; model.fit(X, Y, epochs=20, batch_size=5, verbose=1)\nEpoch 1/20\n 910/1043 [=========================&gt;....] - ETA: 0s - loss: -0.3609\n"
'[estimator.tree_.max_depth for estimator in RF_optimised.estimators_]\n'
'from flask import request\n\nif __name__ == \'__main__\':\n    app.run(port=5000, debug=True)\n\nimport requests, json\n# URL\nurl = \'http://localhost:5000/predict\'\nr = requests.post(url, json={"feature_array":[7.4,0.66,0,1.8,0.075,13,40,0.9978,3.51,0.56,9.4]})\nprint(r.json())\n'
"targets = np.asarray(match_train).astype('float32').reshape((-1,1))\n"
"&gt;&gt;&gt; predictions = logReg.predict(x_validate)\n\n&gt;&gt;&gt; x_validate.info(verbose=True)                                                                                                                                                          \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 197 entries, 495 to 45\nData columns (total 7 columns):\nPclass       197 non-null int64\nSex          197 non-null int64\nRelatives    197 non-null int64\nFare         197 non-null float64\nAge          197 non-null float64\nEmbarked     197 non-null int64\nHasCabin     197 non-null int64\ndtypes: float64(2), int64(5)\nmemory usage: 12.3 KB\n\n&gt;&gt;&gt; test_data.info(verbose=True)                                                                                                                                                           \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 7 columns):\nPclass       418 non-null int64\nSex          418 non-null int64\nRelatives    418 non-null int64\nFare         417 non-null float64\nAge          418 non-null float64\nEmbarked     418 non-null int64\nHasCabin     418 non-null int64\ndtypes: float64(2), int64(5)\nmemory usage: 22.9 KB\n\nFare         417 non-null float64    \n"
"pipe = Pipeline([\n        ('scale', StandardScaler()),\n        ('clf', SVR())])\n\nparam_grid = dict(clf__gamma = [.01,.001,1],\n                  clf__C = [1,100],\n                  clf__kernel = ['rbf','linear'])\n\ngsc = GridSearchCV(pipe, param_grid = param_grid, scoring='neg_mean_squared_error',\n            cv=TimeSeriesSplit(n_splits=5).split(X), verbose=10, n_jobs=-1, refit=True)\n\ngsc.fit(X,y)\nprint(gsc.best_estimator_)\n"
'from sklearn.datasets import samples_generator\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n# generate some data to play with\nX, y = samples_generator.make_classification(\n    n_informative=5, n_redundant=0, random_state=42)\n\npipeline = Pipeline([\n    ("kmeans", KMeans(n_clusters=50)),\n    ("log_reg", LogisticRegression(solver=\'lbfgs\')),\n])\npipeline.fit(X, y)\npipeline[\'kmeans\'].labels_\n\n# array([ 2, 42, 40, 38, ...])\n\n'
'from keras import backend as K\ndel model\nK.clear_session()\ngc.collect()\n'
"model = Sequential()\nmodel.add(Dense(200, input_dim=28, activation='relu')) \n# model.add(Dropout(0.5))\nmodel.add(Dense(300, activation='relu')) \n# model.add(Dropout(0.5))\nmodel.add(Dense(300, activation='relu')) \n# model.add(Dropout(0.5))\nmodel.add(Dense(150, activation='relu')) \n# model.add(Dropout(0.4))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
"m_lr=MultiOutputRegressor(LinearRegression())\nm_lr.fit(X, Y)\n...\nfor estimator in m_lr.estimators_:\n    weights = pd.DataFrame(estimator.coef_, X.columns, columns=['Coefficients'])\n"
"import numpy as np\nfor gbrCount in np.arange(0, 1.0, 0.1):\n    for xgbCount in np.arange(0, 1.0, 0.1):\n        for regCount in np.arange(0, 1.0, 0.1):\n            y_p = (xgbCount*xgb.predict(testset)+ gbrCount*gbr.predict(testset)+regCount*regressor.predict(testset))\n            testset['SalePrice']=np.expm1(y_p)\n            y_train_p = xgb.predict(dataset)\n            y_train_p = np.expm1(y_train_p)\n            rmse.append(np.sqrt(mean_squared_error(y, y_train_p)))\n            rmse.append(xgbCount)\n            rmse.append(gbrCount)\n            rmse.append(regCount)\n\nimport numpy as np\nfor gbrCount in np.arange(0, 1.0, 0.1):\n    for xgbCount in np.arange(0, 1.0, 0.1):\n        for regCount in np.arange(0, 1.0, 0.1):\n            #check if sum is 1\n            if int(gbrCount+xgbCount+regCount) == 1:\n\n                y_p = (xgbCount*xgb.predict(testset)+ gbrCount*gbr.predict(testset)+regCount*regressor.predict(testset))\n                testset['SalePrice']=np.expm1(y_p)\n                y_train_p = xgb.predict(dataset)\n                y_train_p = np.expm1(y_train_p)\n                rmse.append(np.sqrt(mean_squared_error(y, y_train_p)))\n                rmse.append(xgbCount)\n                rmse.append(gbrCount)\n                rmse.append(regCount)\n\nimport numpy as np\nfor gbrCount in np.arange(0, 1.0, 0.1):\n    for xgbCount in np.arange(0, 1.0, 0.1):\n        for regCount in np.arange(0, 1.0, 0.1):\n            #check if sum is 1\n            if int(gbrCount+xgbCount+regCount) == 1:\n\n                y_p = (xgbCount*xgb.predict(testset)+ gbrCount*gbr.predict(testset)+regCount*regressor.predict(testset))\n                testset['SalePrice']=np.expm1(y_p)\n                y_train_p = xgb.predict(dataset)\n                y_train_p = np.expm1(y_train_p)\n\n                rmse.append([np.sqrt(mean_squared_error(y, y_train_p)), xgbCount, gbrCount, regCount ])\n"
"return [np.asarray(input_ids, dtype=np.int32),\n         np.asarray(attention_masks, dtype=np.int32),\n         np.asarray(token_type_ids, dtype=np.int32)]\n\ninput_ids = tf.convert_to_tensor(input_ids)\nattention_masks = tf.convert_to_tensor(attention_masks)\n\n return input_ids, attention_masks\n\n train_ids, train_masks = create_input_array(df[:], tokenizer=tokenizer)\n\ntrain_ids = tf.reshape(train_ids, (-1, 128, 1) )\ntrain_masks = tf.reshape(train_masks, (-1, 128, 1) )\n\nlabels = tf.convert_to_tensor(y[:])\nn_classes = np.unique(y).max() + 1\n\ndataset = tf.data.Dataset.from_tensors(( (train_ids, train_masks), labels ))\n\nmodel = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', trainable=False)\n\n# Input layers\ninput_layer = Input(shape=(128, ), dtype=np.int32)\ninput_mask_layer = Input(shape=(128, ), dtype=np.int32)\n\n# Bert layer, return first output\nbert_layer = model([input_layer, input_mask_layer])[0]\n\n# Flatten layer\nflat_layer = Flatten() (bert_layer)\n\n# Dense layer\ndense_output = Dense(n_classes, activation='softmax') (flat_layer)\n\nmodel_ = Model(inputs=[input_layer, input_mask_layer], outputs=dense_output)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-08, clipnorm=1.0)\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nmodel_.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n\nmodel_.fit(dataset, epochs=4, batch_size=4, verbose=1)\n"
'rnn.fit(\n        [\n            x_training["act"].reshape(episode_length, 1),\n            x_training["b0"].reshape(episode_length, 1),\n            x_training["b1"].reshape(episode_length, 1),\n            x_training["b2"].reshape(episode_length, 1),\n            x_training["b3"].reshape(episode_length, 1),\n            x_training["class_id"].reshape(episode_length, 1),\n            x_training["score"].reshape(episode_length, 1)\n        ],\n        [\n            y_training["b_box"]\n        ],\n        validation_data=(\n                    [\n                        x_test["act"].reshape(episode_length, 1),\n                        x_test["b0"].reshape(episode_length, 1),\n                        x_test["b1"].reshape(episode_length, 1),\n                        x_test["b2"].reshape(episode_length, 1),\n                        x_test["b3"].reshape(episode_length, 1),\n                        x_test["class_id"].reshape(episode_length, 1),\n                        x_test["score"].reshape(episode_length, 1)        \n                    ],\n                    [\n                        y_test["b_box"]        \n                    ]\n                ),\n        epochs=1,\n        batch_size=3200\n    )\n'
"from sklearn.feature_extraction.text import CountVectorizer\n\npipeline = Pipeline([\n           ('vect', TfidfVectorizer()),\n           ('clf', SGDClassifier()),\n])\nparameters = [{\n    'vect__max_df': (0.5, 0.75, 1.0),\n    'vect__max_features': (None, 5000, 10000, 50000),\n    'vect__ngram_range': ((1, 1), (1, 2), (1,3),)  \n    'tfidf__use_idf': (True, False),\n    'tfidf__norm': ('l1', 'l2', None),\n    'clf__max_iter': (20,),\n    'clf__alpha': (0.00001, 0.000001),\n    'clf__penalty': ('l2', 'elasticnet'),\n    'clf__max_iter': (10, 50, 80)\n},{\n    'vect': (CountVectorizer(),)\n    # count_vect_params...\n    'clf__max_iter': (20,),\n    'clf__alpha': (0.00001, 0.000001),\n    'clf__penalty': ('l2', 'elasticnet'),\n    'clf__max_iter': (10, 50, 80)\n}]\n\ngrid_search = GridSearchCV(pipeline, parameters)\n"
"acc_bench = 0.9736842105263158 # accuracy on all features\nres = {}\nf = x_train.shape[1]\npcpt = Perceptron(n_jobs=-1)\nfrom itertools import combinations\nfor i in tqdm(range(2,10)):\n    features_list = combinations(range(f),i)\n    for features in features_list:\n        pcpt.fit(x_train[:,features],y_train)\n        preds = pcpt.predict(x_test[:, features])\n        acc = accuracy_score(y_test, preds)\n        if acc &gt; acc_bench:\n            acc_bench = acc\n            res[&quot;accuracy&quot;] = acc_bench\n            res[&quot;features&quot;] = features\nprint(res)\n{'accuracy': 1.0, 'features': (0, 15, 22)}\n\nfeatrues = pd.DataFrame(cancer.data, columns=cancer.feature_names) \ntarget = pd.DataFrame(cancer.target, columns=['target']) \ncancer_data = pd.concat([featrues,target], axis=1) \nfeatures_list = np.argsort(np.abs(cancer_data.corr()['target'])[:-1].values)[::-1]\nfeature_list\narray([27, 22,  7, 20,  2, 23,  0,  3,  6, 26,  5, 25, 10, 12, 13, 21, 24,\n       28,  1, 17,  4,  8, 29, 15, 16, 19, 14,  9, 11, 18])\n\nres = dict()\nfor i in tqdm(range(2,10)):\n    features=features_list[:i]\n    pcpt.fit(x_train[:,features],y_train)\n    preds = pcpt.predict(x_test[:, features])\n    acc = accuracy_score(y_test, preds)\n    res[i]=[acc]\npd.DataFrame(res).T.plot.bar()\nplt.ylim([.9,1])\n"
"from sklearn.ensemble import StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.base import ClassifierMixin, BaseEstimator\n\nX, y = load_breast_cancer(return_X_y=True, as_frame=True)\n\nclass MyOwnClassifier(ClassifierMixin, BaseEstimator):\n    \n    def __init__(self,classifier):\n        self.classifier = classifier\n        \n    def fit(self, X, y):\n        self.classifier.fit(X,y)\n        self.classes_ = self.classifier.classes_\n        return self\n    \n    def predict(self, X):\n        return self.classifier.predict(X)\n    \n    def predict_proba(self, X):\n        return self.classifier.predict_proba(X)\n\nmodel = StackingClassifier(estimators=[\n        ('tree', DecisionTreeClassifier(random_state=42)),\n        ('knn', MyOwnClassifier(KNeighborsClassifier()))])\n\nmodel.fit(X,y)\nStackingClassifier(estimators=[('tree',\n                                DecisionTreeClassifier(random_state=42)),\n                               ('knn',\n                                MyOwnClassifier(classifier=KNeighborsClassifier()))])\n"
'import sagemaker\n\n\nsagemaker_session = sagemaker.session.Session()\nsagemaker_session.describe_training_job(&quot;Job...&quot;)\n'
'sents=["I am x","I am Y"]\n'
"#get rid of items with fewer than 2 occurrences.\ncorpus=corpus[corpus.groupby('label').label.transform(len)&gt;1]\n\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nsss=StratifiedShuffleSplit(corpus['label'].tolist(), 1, test_size=0.5, random_state=None)\n\ntrain_index, test_index =list(*sss)\ntraining_data=corpus.iloc[train_index]\ntest_data=corpus.iloc[test_index]\n\n#create random data with labels 0 to 39, then add 2 label case and one label case.     \ncorpus=pd.DataFrame({'data':np.random.randn(49998),'label':np.random.randint(40,size=49998)})\ncorpus.loc[49998]=[random.random(),40]\ncorpus.loc[49999]=[random.random(),40]\ncorpus.loc[50000]=[random.random(),41]\n\ntest_data[test_data['label']==40]\nOut[110]: \n           data  label\n49999  0.231547     40\n\ntraining_data[training_data['label']==40]\nOut[111]: \n           data  label\n49998  0.253789     40\n\ntest_data[test_data['label']==41]\nOut[112]: \nEmpty DataFrame\nColumns: [data, label]\nIndex: []\n\ntraining_data[training_data['label']==41]\nOut[113]: \nEmpty DataFrame\nColumns: [data, label]\nIndex: []\n"
"predicted = text_clf.predict(line);\n\nline = 'I am talking about the product apple computer by Steve Jobs'\n\nline = [];    \nline.append('I am talking about apple the fruit we eat.');\n\ntext_clf.predict([line]) \n"
'alg = sklearn.linear_model.LogisticRegression()\nalg.fit(x_train, y_train)\ntest_score = alg.score(x_test, y_test)\n'
"bins = np.linspace(0, 5, 20, endpoint=False)\nprint bins\n[ 0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.    2.25  2.5   2.75\n  3.    3.25  3.5   3.75  4.    4.25  4.5   4.75]\n\nprint df.groupby([df.LEAGUE, pd.cut(df.HOME, bins)]).sum()\n\n                    HOME  DRAW  AWAY  WINNER  PREDICTED  PROFIT\nLEAGUE HOME                                                    \n2      (0, 0.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (0.25, 0.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (0.5, 0.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (0.75, 1]     NaN   NaN   NaN     NaN        NaN     NaN\n       (1, 1.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (1.25, 1.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (1.5, 1.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (1.75, 2]     NaN   NaN   NaN     NaN        NaN     NaN\n       (2, 2.25]    2.25  3.30  3.20       2          0   -10.0\n       (2.25, 2.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.5, 2.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.75, 3]     NaN   NaN   NaN     NaN        NaN     NaN\n       (3, 3.25]    3.25  3.25  2.10       0          2   -10.0\n       (3.25, 3.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (3.5, 3.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (3.75, 4]     NaN   NaN   NaN     NaN        NaN     NaN\n       (4, 4.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (4.25, 4.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (4.5, 4.75]   NaN   NaN   NaN     NaN        NaN     NaN\n11     (0, 0.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (0.25, 0.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (0.5, 0.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (0.75, 1]     NaN   NaN   NaN     NaN        NaN     NaN\n       (1, 1.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (1.25, 1.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (1.5, 1.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (1.75, 2]     NaN   NaN   NaN     NaN        NaN     NaN\n       (2, 2.25]    2.25  3.00  2.88       0          0    12.5\n       (2.25, 2.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.5, 2.75]   NaN   NaN   NaN     NaN        NaN     NaN\n...                  ...   ...   ...     ...        ...     ...\n14     (2, 2.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (2.25, 2.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.5, 2.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.75, 3]     NaN   NaN   NaN     NaN        NaN     NaN\n       (3, 3.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (3.25, 3.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (3.5, 3.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (3.75, 4]     NaN   NaN   NaN     NaN        NaN     NaN\n       (4, 4.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (4.25, 4.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (4.5, 4.75]   NaN   NaN   NaN     NaN        NaN     NaN\n17     (0, 0.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (0.25, 0.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (0.5, 0.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (0.75, 1]     NaN   NaN   NaN     NaN        NaN     NaN\n       (1, 1.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (1.25, 1.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (1.5, 1.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (1.75, 2]     NaN   NaN   NaN     NaN        NaN     NaN\n       (2, 2.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (2.25, 2.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.5, 2.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (2.75, 3]     NaN   NaN   NaN     NaN        NaN     NaN\n       (3, 3.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (3.25, 3.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (3.5, 3.75]   NaN   NaN   NaN     NaN        NaN     NaN\n       (3.75, 4]     NaN   NaN   NaN     NaN        NaN     NaN\n       (4, 4.25]     NaN   NaN   NaN     NaN        NaN     NaN\n       (4.25, 4.5]   NaN   NaN   NaN     NaN        NaN     NaN\n       (4.5, 4.75]   NaN   NaN   NaN     NaN        NaN     NaN\n\n[76 rows x 6 columns]\n\nprint df.groupby([df.LEAGUE, pd.cut(df.HOME, bins)]).agg({'HOME' : min, \n                                                          'DRAW' : min, \n                                                          'AWAY' : min, \n                                                          'WINNER' : 'count', \n                                                          'PREDICTED' : 'count', \n                                                          'PROFIT': sum})\n\n                    DRAW  PROFIT  AWAY  WINNER  PREDICTED  HOME\nLEAGUE HOME                                                    \n2      (2, 2.25]    3.30   -10.0  3.20       1          1  2.25\n       (3, 3.25]    3.25   -10.0  2.10       1          1  3.25\n11     (2, 2.25]    3.00    12.5  2.88       1          1  2.25\n14     (1.25, 1.5]  3.50     5.0  6.00       1          1  1.50\n"
"def sigmoid(z):\n    return 1 / (1+np.exp(-z))\n\n('Function Gradient', 'Numerical Gradient')\n(-0.0087363416123043425, 0.0)\n(0.017468375248392107, 0.0174683752529603)\n(-0.0016267134050363559, -0.0016267134039793518)\n(0.0018882373947080224, 0.0018882373997719526)\n(-0.0063531428795779391, -0.0063531428762253483)\n(0.0029882213493977773, 0.0029882213481435826)\n(0.014295787205089885, 0.014295787205131916)\n(-0.026668095974979808, -0.026668095973736428)\n(0.0043373799514851595, 0.0043373799440971084)\n(0.0063740837472641377, 0.0063740837497050506)\n(0.0027102260448642525, 0.0027102260435896142)\n(0.0067009063282609839, 0.0067009063298151261)\n(-0.0029645476578591843, -0.0029645476562478734)\n(-0.012000477453137556, -0.012000477451756808)\n(-0.020065071389262716, -0.020065071393293721)\n(0.010308693441913186, 0.010308693438876304)\n(-0.0015996484140612609, -0.0015996484115099463)\n(-0.0086037766244218914, -0.0086037766244828617)\n(-0.0099431361329477934, -0.0099431361344493041)\n(0.0062574996404342166, 0.0062574996406716821)\n(0.30213488769328123, 0.3021348876908192)\n(0.14900524972537924, 0.14900524972549789)\n(0.13305168538400619, 0.13305168538479961)\n(0.16730920742910549, 0.16730920743279754)\n(0.14245586995768528, 0.14245586995365045)\n(0.15465244296463604, 0.15465244296519742)\n(0.10813908901043021, 0.10813908900342284)\n(0.040844058224880242, 0.04084405822446513)\n(0.040566215206120269, 0.040566215204762557)\n(0.036451467449020114, 0.036451467448905817)\n(0.065664340475228455, 0.065664340476168093)\n(0.070753692265581092, 0.07075369226283712)\n(0.088651862157018618, 0.088651862166777562)\n(0.028272897964677978, 0.028272897965031518)\n(0.026876928049457398, 0.026876928049812676)\n(0.056512225949437798, 0.056512225949933992)\n(0.051775047342360533, 0.051775047342772496)\n(0.025689087137289929, 0.025689087135294386)\nRelative Difference: \n0.00878484310135\n\nimport numpy as np\n\ndef sigmoid(z):\n    return 1 / (1+np.exp(-z))\n\ndef sigmoid_gradient(z):\n    return sigmoid(z)*(1-sigmoid(z))\n\ndef randInitializeWeights(layer_in, layer_out):\n    matrix = np.zeros((layer_out, 1 + layer_in))\n    epsilon_init = 0.12\n    matrix = np.random.rand(layer_out, 1+layer_in) * 2 * epsilon_init -epsilon_init\n    return matrix\n\ndef gradient(theta, *args):\n\n    X, y, num_inputs, num_hidden_units, num_labels, lamb = args\n\n    m = len(X)\n\n    y_bin = np.zeros((m, num_labels))\n\n    for i in range(m):\n        y_bin[i, y[i]] = 1\n\n    theta1 = np.reshape(theta[0:(num_hidden_units*(num_inputs+1))],(num_hidden_units, (num_inputs+1)))  #5x4\n    theta2 = np.reshape(theta[(num_hidden_units*(num_inputs+1)):],(num_labels, num_hidden_units+1))     #3x6\n\n    theta1_grad = np.zeros(theta1.shape)\n    theta2_grad = np.zeros(theta2.shape)\n\n    delta1 = np.zeros(theta1.shape)\n    delta2 = np.zeros(theta2.shape)\n\n\n    #forward\n\n    a_1 = np.hstack((np.ones((m, 1)), X))   #5x4\n\n    z_2 = np.dot(a_1, theta1.transpose())   #5x5\n    a_2 = sigmoid(z_2)                      #5x5\n\n    a_2 = np.hstack((np.ones((m, 1)), a_2)) #5x6\n    z_3 = np.dot(a_2, theta2.transpose())   #5x3\n\n    h = sigmoid(z_3)                        #5x3\n\n\n    #backward\n\n    delta3 = h - y_bin                      #5x3\n    delta2 = np.dot(delta3, theta2[:, 1:num_hidden_units+1]) * sigmoid_gradient(z_2) #5x5\n\n    D1 = np.dot(delta2.transpose(), a_1)    #5x4\n    D2 = np.dot(delta3.transpose(), a_2)    #3x6\n\n    theta1_grad = D1/m      #5x4\n    theta2_grad = D2/m      #3x6\n\n    #regularization\n    theta1_grad[:, 1:num_inputs+1] = theta1_grad[:, 1:num_inputs+1] +lamb/m*  theta1[:, 1:num_inputs+1]\n    theta2_grad[:, 1:num_hidden_units+1] = theta2_grad[:, 1:num_hidden_units+1] +lamb/m*  theta2[:, 1:num_hidden_units+1]\n\n    #unroll\n    grad = np.hstack([theta1_grad.ravel(), theta2_grad.ravel()])\n    return grad\n\ndef gradientChecking(lamb):\n    input_layer_size = 3\n    hidden_layer_size = 5\n    num_labels = 3\n    m = 5\n\n    theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)\n    theta2 = randInitializeWeights(hidden_layer_size, num_labels)\n\n    X = np.random.rand(m, input_layer_size)\n    y = np.array([1, 2, 0, 1, 2])\n\n    nn_params = np.hstack([theta1.ravel(), theta2.ravel()])\n\n    #calculate gradient with function\n    grad = gradient(nn_params, X, y, input_layer_size, hidden_layer_size, num_labels, lamb)\n    #calculate numerical gradient\n    num_grad = computeNumericalGradient(lambda theta: computeCost(theta, X, y, input_layer_size, hidden_layer_size, num_labels, lamb), nn_params)\n\n    print('Function Gradient', 'Numerical Gradient')\n    for i in range(len(grad)):\n        print(grad[i], num_grad[i])\n\n    diff = np.linalg.norm(num_grad-grad)/np.linalg.norm(num_grad+grad)\n    print('Relative Difference: ')\n    print(diff)\n\ndef computeCost(theta, X, y, num_inputs, num_hidden_units, num_labels, lamb):\n\n    m = len(X)\n\n    y_bin = np.zeros((m, num_labels))\n\n    for i in range(m):\n        y_bin[i, y[i]] = 1\n\n    theta1 = np.reshape(theta[0:(num_hidden_units*(num_inputs+1))],(num_hidden_units, (num_inputs+1))) #5x4\n    theta2 = np.reshape(theta[(num_hidden_units*(num_inputs+1)):],(num_labels, num_hidden_units+1)) #3x6\n\n    a_1 = np.hstack((np.ones((m, 1)), X))   #5x4\n\n    z_2 = np.dot(a_1, theta1.transpose())   #5x5\n    a_2 = sigmoid(z_2)                      #5x5\n\n    a_2 = np.hstack((np.ones((m, 1)), a_2)) #5x6\n    z_3 = np.dot(a_2, theta2.transpose())   #5x3\n\n    h = sigmoid(z_3)\n\n    cost = np.sum(-y_bin * np.log(h)    -    (1-y_bin) * np.log(1-h))/m\n\n\n    #regularization\n\n    theta1_sq = theta1[:, 1:num_inputs+1] * theta1[:, 1:num_inputs+1];\n    theta2_sq = theta2[:, 1:num_hidden_units+1] * theta2[:, 1:num_hidden_units+1];\n\n    cost = cost + lamb/(2.0*m)*(np.sum(theta1_sq) + np.sum(theta2_sq))\n\n    return cost\n\ndef computeNumericalGradient(J, theta):\n    numgrad = np.zeros(theta.shape)\n    perturb = np.zeros(theta.shape)\n    e = 0.0001\n\n    for p in range(1, np.size(theta)):\n        perturb[p] = e\n        loss1 = J(theta - perturb)\n        loss2 = J(theta + perturb)\n\n        numgrad[p] = (loss2 - loss1) / (2*e)\n        perturb[p] = 0\n    return numgrad\n\n\ngradientChecking(1.0)\n"
'pip freeze | grep pyspark\n\npip install pyspark --upgrade\n'
'doc = nlp.tokenizer(text)\nnlp.tagger(doc)\nnlp.parser(doc)\nnlp.entity(doc)\n'
'def epoch(): \n    err_sum = 0\n    learn_rate = 0.1\n    global w\n    for i in range(int(ceil(len(ytr) / batch_size))):\n        batch = Xtr[i:i+batch_size]\n        target = ytr[i:i+batch_size]\n        dw = np.zeros_like(w)\n        for j in range(batch_size):\n            s_l1 = batch[j].T.dot(w)\n            x_l1 = np.tanh(s_l1)\n            err = x_l1 - target[j]\n            err_sum += err\n            delta_l1 = 2 * err * (1 - x_l1**2)\n            dw += batch[j] * delta_l1\n        w -= learn_rate * (dw / batch_size)\n    print("Mean error: %f" % (err_sum / len(ytr)))\n\norder = np.random.permutation(len(ytr))\n'
'from bhtsne import tsne\ndata_nd_tsne = tsne(diff_df)\n'
'from sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom copy import deepcopy\n\nkf = StratifiedKFold(n_splits=8)\nclf = RandomForestClassifier(class_weight="balanced")\nclf_models = []\n\n# keep in mind your X and y should be indexed same here\nkf.get_n_splits(X_data)\nfor train_index, test_index in kf.split(X_data, y_data):\n    print("TRAIN:", train_index, "TEST:", test_index)\n    X_train, X_test = X_data[train_index], X_data[test_index]\n    y_train, y_test = y_data[train_index], y_data[test_index]\n    tmp_clf = deepcopy(clf)\n    tmp_clf.fit(X_train, y_train)\n\n    print("Got a score of {}".format(tmp_clf.score(X_test, y_test)))\n    clf_models.append(tmp_clf)\n'
'FROM gcr.io/tensorflow/tensorflow:latest\nCOPY script.py /usr/bin/\nCMD ["python", "/usr/bin/script.py"]\n\n$ docker build -t mytensorflow .\n\n$ docker run -it --rm mytensorflow\n'
"newdata=img_data.reshape(800*800,4)\n#Apply a Gaussian smoothing filter over a pixel neighborhood\nnewdata=sy.ndimage.filters.gaussian_filter(newdata,(1.5,1.5))\n#Create the vector of n_components you wish to test using the BIC alogrithm\nn_components = np.arange(1, 10)\n#Create an empty vector in which to store BIC scores\nBIC = np.zeros(n_components.shape)\n\nfor i, n in enumerate(n_components):\n    #Fit gmm to data for each value in n_components vector\n    gmm = GaussianMixture(n_components=n,\n          covariance_type='tied')\n    gmm.fit(newdata)\n    #Store BIC scores in a list\n    BIC[i] = gmm.bic(newdata)\n\n#Plot resulting BIC list (Scores(n_components))\nplt.plot(BIC)\nplt.show()\n"
"(Avocado, Bread, Butter, Garlic, Tomato)\n(0,1,1,0,0) = 'Buttered Bread'\n(1,0,0,1,1) = 'Guacamloe'\n"
"df = df[['id']].join(df['amentieis'].str.get_dummies(','))\nprint (df)\n   id  Heating  Hot tub  Internet  Shower  TV\n0   1        0        0         1       1   1\n1   2        0        1         1       0   1\n2   3        1        0         1       1   0\n\ndf = pd.concat([df['id'], df['amentieis'].str.get_dummies(',')], axis=1)\nprint (df)\n   id  Heating  Hot tub  Internet  Shower  TV\n0   1        0        0         1       1   1\n1   2        0        1         1       0   1\n2   3        1        0         1       1   0\n"
'y_predicted = cross_val_predict(gs_clf.best_estimator_, X, y)\n\nFitting 3 folds for each of 12 candidates, totalling 36 fits\n[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed:   43.6s finished\n             precision    recall  f1-score   support\n\n          0      0.920     0.911     0.916       584\n          1      0.894     0.943     0.918       597\n          2      0.929     0.887     0.908       594\n\navg / total      0.914     0.914     0.914      1775\n'
'm.metric("recall")\n\n[[0.8160852636726422, 1.0]]\n\nmDL.metric("recall",thresholds=[x/100.0 for x in range(1,100)])\n\nCould not find exact threshold 0.01; using closest threshold found 0.010396965719556233.\nCould not find exact threshold 0.02; using closest threshold found 0.016617060110009896.\n...\nCould not find exact threshold 0.92; using closest threshold found 0.9469528904679438.\nCould not find exact threshold 0.93; using closest threshold found 0.9469528904679438.\nCould not find exact threshold 0.94; using closest threshold found 0.9469528904679438.\nCould not find exact threshold 0.95; using closest threshold found 0.9469528904679438.\nCould not find exact threshold 0.96; using closest threshold found 0.9469528904679438.\nCould not find exact threshold 0.97; using closest threshold found 0.9760293572153097.\nCould not find exact threshold 0.98; using closest threshold found 0.9787491606489236.\nCould not find exact threshold 0.99; using closest threshold found 0.9909817370067531.\n\n[[0.01, 1.0],\n [0.02, 1.0],\n [0.03, 1.0],\n ...\n [0.87, 1.0],\n [0.88, 1.0],\n [0.89, 0.9850746268656716],\n [0.9, 0.9850746268656716],\n [0.91, 0.9850746268656716],\n [0.92, 0.9850746268656716],\n [0.93, 0.9850746268656716],\n [0.94, 0.9850746268656716],\n [0.95, 0.9850746268656716],\n [0.96, 0.9850746268656716],\n [0.97, 0.9701492537313433],\n [0.98, 0.9552238805970149],\n [0.99, 0.8955223880597015]]\n'
"df.ID.str.get_dummies().rolling(3).sum().shift(-2).add_suffix('_count')\n\n                     s_2_count  s_3_count  s_5_count\nDATE                                                \n2017-05-17 15:49:51        2.0        0.0        1.0\n2017-05-17 15:49:52        1.0        1.0        1.0\n2017-05-17 15:49:55        1.0        1.0        1.0\n2017-05-17 15:49:56        0.0        1.0        2.0\n2017-05-17 15:49:58        NaN        NaN        NaN\n2017-05-17 15:49:59        NaN        NaN        NaN\n\ndf.assign(**df.ID.str.get_dummies().rolling(3).sum().shift(-2).add_suffix('_count'))\n\ndf.join(df.ID.str.get_dummies().rolling(3).sum().shift(-2).add_suffix('_count'))\n\n                      ID  s_2_count  s_3_count  s_5_count\nDATE                                                     \n2017-05-17 15:49:51  s_2        2.0        0.0        1.0\n2017-05-17 15:49:52  s_5        1.0        1.0        1.0\n2017-05-17 15:49:55  s_2        1.0        1.0        1.0\n2017-05-17 15:49:56  s_3        0.0        1.0        2.0\n2017-05-17 15:49:58  s_5        NaN        NaN        NaN\n2017-05-17 15:49:59  s_5        NaN        NaN        NaN\n\ndf.assign(**pd.crosstab(df.index,df.ID).rolling(3).sum().shift(-2))\n\ndf.join(pd.crosstab(df.index,df.ID).rolling(3).sum().shift(-2))\n"
'regressor = tf.estimator.DNNRegressor(\n    feature_columns=feature_columns,\n    hidden_units=[40, 30, 20],\n    model_dir="model1",\n    optimizer=\'RMSProp\'\n)\nregressor.fit(input_fn=train_input_fn, steps=5)\nregressor.export_savedmodel("test",json_serving_input_fn)\n\nCSV_COLUMNS = ["ad_provider", "gold", "video_success"] \nFEATURES = ["ad_provider", "gold"] \nTYPES = [tf.string, tf.float32] \nLABEL = "video_success" \n\ndef json_serving_input_fn(): \n  """Build the serving inputs.""" \n  inputs = {} \n  for feat, dtype in zip(FEATURES, TYPES): \n    inputs[feat] = tf.placeholder(shape=[None], dtype=dtype) \n\n  features = {\n    key: tf.expand_dims(tensor, -1)\n    for key, tensor in inputs.items()\n  }\n  return tf.contrib.learn.InputFnOps(features, None, inputs)\n'
"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport numpy as np\n\ndef sigmoid(x):\n    output = 1.0 / (1.0 + np.exp(-x))\n    return output\n\ndef sigmoid_output_to_derivative(output):\n    return output*(1-output)\n\nX = np.array([\n    [0,1],\n    [0,1],\n    [1,0],\n    [1,0]\n])\ny = np.array([[0, 0, 1, 1]]).T\n\nsynapse_0 = np.empty((2,1))\n\n# the error aggregation starts here\nx_range = np.linspace(-10, 10, 20, dtype=np.float)\ny_range = np.linspace(-10, 10, 20, dtype=np.float)\nerrors = []\nfor _x in x_range:\n    synapse_0[0] = _x\n    for _y in y_range:\n        synapse_0[1] = _y\n\n        # apply the model to the input\n        layer_0 = X\n        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n\n        # evaluate the error using the RMSE\n        error = np.mean(np.sqrt((layer_1 - y) ** 2))\n        errors.append(error)\n\n# in order to plot we need to transform x,y and z in 2D array \nerror_surface = np.reshape(np.array(errors), (x_range.shape[0], y_range.shape[0]))\n_X, _Y = np.meshgrid(x_range, y_range, indexing='ij')\n\n# plot\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.plot_surface(_X, _Y, error_surface, cmap=cm.YlOrBr_r, edgecolor='gray', linewidth=0.004, antialiased=False)\nplt.show()\n"
'resultdata = loandataforclassification.append(declinedataforclassification,ignore_index=True)\n'
"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nclf = estimator\nclf.fit(x,y)\nz = clf.predict(x)\n\ndef pe_score(y, y_pred):\n    pe = prob_error(y_pred, y)\n    return pe\n\npe_error = make_scorer(pe_score)\ngrid = GridSearchCV(SVC(), param_grid={'kernel':('linear', 'rbf'), 'C':[1, 10, 100,1000,10000]}, scoring= pe_error)\n"
'[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4] # houses\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # move origin\n[0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0] # move destinations / #of stones at each\n[0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0] # capture locations / sizes\n'
'def fit_transform(self, X, y=None, **fit_params):\n    return self.fit(X)\n\ndef fit_transform(self, X, y=None, **fit_params):\n    return self.fit(X).transform(X)\n'
'pip install numpy --upgrade --user\n'
"keyword_tuple = [('AAAA', {'in': ['kwrd1'], 'out':[]}), \n                 ('AAAA', {'in': ['kwrd2', 'kwrd3'], 'out': []),\n                 ('AAAA', {'in': ['kwrd3'], 'out': ['kwrd4']}), \n                 ('BBBB', {'in': ['kwrd4'], 'out': [])]\n\nresult_tuple = []\n\nfor description in description_list:\n    # Find categories that satisfy the include condition\n    categories_in = [cat[0] for cat in keyword_tuple if all([kw in description for kw in cat[1]['in']])]\n    # Find categories that satisfy the exclude condition\n    categories_out = [cat[0] for cat in keyword_tuple if all([kw not in description for kw in cat[1]['out']])]\n\n    # Find the categories that satisfy both \n    # If there are multiple categories satisfying the condition, you need to come with a decision rule\n    categories = list(set(categories_in).intersection(categories_out))\n\n    # Append to the result list (Takes the first that is satisfied)\n    if len(categories) &gt; 0:\n        category = categories[0]\n    else:\n        category = 'NO CATEGORY'\n\n    result_tuple.append(description, category)\n"
'percent=600\npca=PCA(percent)\ntrain_x=pca.fit_transform(train_x)\ntest_x=pca.fit_transform(test_x)\n\npercent=.80\npca=PCA(percent)\npca.fit(train_x)\ntrain_x=pca.transform(train_x)\ntest_x=pca.transform(test_x)\n\ntrain_x=pca.inverse_transform(train_x)\ntest_x=pca.inverse_transform(test_x)\nc=pca.n_components_\n&lt;plotting code&gt;    \ninput_l=percent\n\nc=pca.n_components_\n#plotting commented out   \ninput_l=c\n'
'f = K.function(model.inputs, model.outputs)\n\nouts = f([[[3]], [[23]], [[0.0]]])\n'
'    X = array[:,:]\n    Y = array[:,:]\n\nX = values[:,:-1]\nY = values[:,-1:]\n'
"from keras import backend as K\nimport tensorflow as tf\ndef mask_output2(x):\n    inp, soft_out = x\n    # add a very small value in order to avoid having 0 everywhere\n    c = K.constant(0.0000001, dtype='float32', shape=(32, 13))\n    y = soft_out + c\n\n    y = Lambda(lambda x: K.switch(K.equal(x[0],0), x[1], K.zeros_like(x[1])))([inp, soft_out])\n    y_sum =  K.sum(y, axis=-1)\n\n    y_sum_corrected = Lambda(lambda x: K.switch(K.equal(x[0],0), K.ones_like(x[0]), x[0] ))([y_sum])\n\n    y_sum_corrected = tf.divide(1,y_sum_corrected)\n\n    y = tf.einsum('ij,i-&gt;ij', y, y_sum_corrected)\n    return y\n"
'model = Sequential()\nmodel.add(Bidirectional(GRU(hidden_size, return_sequences=True, dropout=0.2), merge_mode=\'concat\',\n                            input_shape=(None, input_size)))  # Encoder\nmodel.add(Attention())\nmodel.add(RepeatVector(max_out_seq_len))\nmodel.add(GRU(hidden_size * 2, return_sequences=True))  # Decoder\nmodel.add(TimeDistributed(Dense(units=output_size, activation="softmax")))\nmodel.compile(loss="categorical_crossentropy", optimizer="rmsprop", metrics=[\'accuracy\'])\n'
'path = "datasets/images/images/"\nimagelist = listdir(path)\n\nimg_name = x_train_names[0]\nimg = Image.open(path + str(img_name))\n\nX_train = np.ndarray((len(imagelist),img.height,img.width,3))\n\nfor i in range(len(x_train_names)):\n img_name = x_train_names[i]\n img = Image.open(path + str(img_name))\n X_train[i,:,:,:] = np.asarray(img)\n\nprint(X_train.shape)\n&gt; (len(x_train_names), img.height, img.width, 3)\n\n#### Build and compile your classifier up here here ####\n\nnum_batches = 5\nlen_batch = np.floor(len(x_train_names)/num_batches).astype(int) \n\nX_train = np.ndarray((len_batch,img.height,img.width,3))\n\nfor batch_idx in range(num_batches):\n    idx_start = batch_idx*len_batch\n    idx_end = (batch_idx+1)*len_batch-1\n    x_train_names_batch = x_train_names[idx_start:idx_end]\n\n    for i in range(len(x_train_names_batch)):\n        img_name = x_train_names_batch[i]\n        img = Image.open(path + str(img_name))\n        X_train[i,:,:,:] = np.asarray(img)\n\n    classifier.fit(x=X_train,y=y_train, steps_per_epoch=num_batches, batch_size=len(x_train_names_batch), epochs=2)\n'
'import time\nimport pickle\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\n\nmodel = load_model(\'model.h5\')\ntokenizer = pickle.load(open(\'tokenizer.pkl\', "rb"))\nSEQUENCE_LENGTH = 300\ndecode_map = {0: "NEGATIVE", 2: "NEUTRAL", 4: "POSITIVE"}\n\nPOSITIVE = "POSITIVE"\nNEGATIVE = "NEGATIVE"\nNEUTRAL = "NEUTRAL"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\ndef decode_sentiment(score, include_neutral=True):\n    if include_neutral:        \n        label = NEUTRAL\n        if score &lt;= SENTIMENT_THRESHOLDS[0]:\n            label = NEGATIVE\n        elif score &gt;= SENTIMENT_THRESHOLDS[1]:\n            label = POSITIVE\n\n        return label\n    else:\n        return NEGATIVE if score &lt; 0.5 else POSITIVE\n\ndef predict(text, include_neutral=True):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {"label": label, "score": float(score),\n       "elapsed_time": time.time()-start_at}  \n\npredict("hello")\n\npredict("hello")\n\n{\'elapsed_time\': 0.6313169002532959,\n \'label\': \'POSITIVE\',\n \'score\': 0.9836862683296204}\n'
"from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten\nfrom keras.utils import to_categorical\nimport keras\n\n#create model\nmodel = Sequential()\n#add model layers\nmodel.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(10,10,1)))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nclass DataGenerator(keras.utils.Sequence):\n    def __init__(self, X, y, batch_size):\n        self.X = X\n        self.y = y\n        self.batch_size = batch_size\n\n    def __len__(self):\n        l = int(len(self.X) / self.batch_size)\n        if l*self.batch_size &lt; len(self.X):\n            l += 1\n        return l\n\n    def __getitem__(self, index):\n        X = self.X[index*self.batch_size:(index+1)*self.batch_size]\n        y = self.y[index*self.batch_size:(index+1)*self.batch_size]\n        return X, y\n\nX = np.random.rand(200,10,10,1)\ny = to_categorical(np.random.randint(0,2,200))\nmodel.fit_generator(DataGenerator(X,y,13), epochs=10)\n"
"from sklearn.cluster import MiniBatchKMeans\n\nwith open('np_array.pickle', 'rb') as handle:\n     np_list = pickle.load(handle)\n\nmbk = MiniBatchKMeans(init ='k-means++', n_clusters = 5, \n                      batch_size = 200, \n                      max_no_improvement = 10, verbose = 0) \n\nmbk.fit(np_list)\n"
'topk_values, linear_indices = stacked.flatten().topk(2)\ntopk_indices = linear_indices % stacked.shape[-1]\n\ntopk_values, topk_indices = stacked.max(dim=0)[0].flatten().topk(2)\n\nstacked = torch.tensor([[[11,8,0]],\n                        [[10,9,0]]])\n\ntopk_values=[11, 10]\ntopk_indices=[0, 0]\n\ntopk_values=[11, 9]\ntopk_indices=[0, 1]\n'
'inp1 = Input(shape=(None, None, 3)) # Inputs\ninp2 = Input(shape=(None, None, 3)) # Outputs\n\nout = Lambda(lambda x: tf.where(tf.equal(x[1], 1), x[1], x[0]))([inp1, inp2])\n'
"from sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn import datasets\nimport numpy as np\n\niris = datasets.load_iris()\nX = np.random.rand(20000,2)\n\nY = np.random.choice(a=[False, True], size=(20000, 1))\n\n# hinge is used as the default\nsvc = SVC(kernel='linear')\n\nsgd = SGDClassifier(loss='hinge')\n\nsvcl = LinearSVC(loss='hinge')\n\n%%time\nsvc.fit(X, Y)\n\n%%time\nsgd.fit(X, Y)\n\n%%time\nsvcl.fit(X, Y)\n"
'self.h = nn.Parameter(torch.zeros(10,10)\n'
"import pydot\n\n(graph,) = pydot.graph_from_dot_file('tree_real_data.dot')\ngraph.write_png('somefile.png')\n"
'gc = GridSearchCV(random_forest,\n                  param_grid={"n_estimators":[5, 10]},\n                  scoring="neg_brier_score")\n\ngc.fit(X, y)\nprint(gc.scorer_) \n# make_scorer(brier_score_loss, greater_is_better=False, needs_proba=True)\n'
'from xgboost import XGBRegressor\n\n# dummy data\n\nX_train = [[0,1], [1,2], [3,2]]\ny_train = [0, 1, 0]\n\nmodel=XGBRegressor(n_estimators=500,learning_rate=0.05)\nmodel.fit(X_train,y_train, early_stopping_rounds=5, eval_set=[(X_train,y_train)])\nX_train_preds = model.predict(X_train)\n\n eval_set(evals, iteration=0, feval=None)\n\n    Evaluate a set of data.\n\n    Parameters\n\n            evals (list of tuples (DMatrix, string)) – List of items to be evaluated.\n\n            iteration (int) – Current iteration.\n\n            feval (function) – Custom evaluation function.\n\n    Returns\n\n        result – Evaluation result string.\n'
"tops = df.ProductID.value_counts().head(10)\n\ndf.ProductID[~df.ProductID.isin(tops)] = 'other'\n"
'from PIL import Image\nimport numpy as np\nx = []\npath = "data/25_12024_010.jpg"\n#load image\nimg = Image.open(path)\n# convert to numpy\nimg = np.array(img)\n# Remove noise using Gaussian Blur\nblur = cv2.GaussianBlur(img, (5, 5), 0)\n# Segmentation\ngray = cv2.cvtColor(blur, cv2.COLOR_RGB2GRAY)\nret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n# Further noise removal (Morphology)\nkernel = np.ones((3, 3), np.uint8)\nopening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n# sure background area\nsure_bg = cv2.dilate(opening, kernel, iterations=3)\n# Finding sure foreground area\ndist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\nret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n# Finding unknown region\nsure_fg = np.uint8(sure_fg)\nunknown = cv2.subtract(sure_bg, sure_fg)\n# Marker labelling\nret, markers = cv2.connectedComponents(sure_fg)\n# Add one to all labels so that sure background is not 0, but 1\nmarkers = markers + 1\n# Now, mark the region of unknown with zero\nmarkers[unknown == 255] = 0\nmarkers = cv2.watershed(img, markers)\nimg[markers == -1] = [255, 0, 0]\n#save to X\nx.append(markers)\n\nprint(x[0].shape) # (120,120)\n\nmarkers = np.stack((markers,)*3, axis=-1)\n\nx.append(markers)\n\nprint(x[1].shape) # (120,120,3)\n'
'from sklearn.feature_selection import SelectKBest, chi2, f_classif\n\n# chi-square\ntop_10_features = SelectKBest(chi2, k=10).fit_transform(X, y)\n\n# or ANOVA\ntop_10_features = SelectKBest(f_classif, k=10).fit_transform(X, y)\n\ndef get_feature_correlation(df, top_n=None, corr_method=\'spearman\',\n                            remove_duplicates=True, remove_self_correlations=True):\n    """\n    Compute the feature correlation and sort feature pairs based on their correlation\n\n    :param df: The dataframe with the predictor variables\n    :type df: pandas.core.frame.DataFrame\n    :param top_n: Top N feature pairs to be reported (if None, all of the pairs will be returned)\n    :param corr_method: Correlation compuation method\n    :type corr_method: str\n    :param remove_duplicates: Indicates whether duplicate features must be removed\n    :type remove_duplicates: bool\n    :param remove_self_correlations: Indicates whether self correlations will be removed\n    :type remove_self_correlations: bool\n\n    :return: pandas.core.frame.DataFrame\n    """\n    corr_matrix_abs = df.corr(method=corr_method).abs()\n    corr_matrix_abs_us = corr_matrix_abs.unstack()\n    sorted_correlated_features = corr_matrix_abs_us \\\n        .sort_values(kind="quicksort", ascending=False) \\\n        .reset_index()\n\n    # Remove comparisons of the same feature\n    if remove_self_correlations:\n        sorted_correlated_features = sorted_correlated_features[\n            (sorted_correlated_features.level_0 != sorted_correlated_features.level_1)\n        ]\n\n    # Remove duplicates\n    if remove_duplicates:\n        sorted_correlated_features = sorted_correlated_features.iloc[:-2:2]\n\n    # Create meaningful names for the columns\n    sorted_correlated_features.columns = [\'Feature 1\', \'Feature 2\', \'Correlation (abs)\'] \n\n    if top_n:\n        return sorted_correlated_features[:top_n]\n\n    return sorted_correlated_features\n'
'var_mod.predict(var_res.params, start=train_norm.index[p], end=train_norm.index[-1], lags=p)\n'
"import numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import precision_recall_curve\n\nX, y = load_iris(return_X_y=True)\n# reduce multiclass to binary problem, i.e. class 0 or 1 (class 2 starts at index 100)\nX = X[0:100]\ny = y[0:100]\n\nlr = LogisticRegression(random_state=0).fit(X, y)\n\ny_test_hat = lr.predict_proba(X)\n\n# just look at probabilities for class 1\ny_test_hat_class_1 = y_test_hat[:,1]\n\nprecisions, recalls, thresholds = precision_recall_curve(y, y_test_hat_class_1)\nf_scores = np.nan_to_num((2 * precisions * recalls) / (precisions + recalls))\n\nfor p, r, f, t in zip(precisions, recalls, f_scores, thresholds):\n    print('Using threshold={} as decision boundary, we reach '\n          'precision={}, recall={}, and f-score={}'.format(t, p, r, f))\n\nf_max_index = np.argmax(f_scores)\nmax_f_score = f_scores[f_max_index]\nmax_f_score_threshold = thresholds[f_max_index]\n\nprint('The threshold for the max f-score is {}'.format(max_f_score_threshold))\n\nUsing threshold=0.8628645363798557 as decision boundary, we reach precision=1.0, recall=1.0, and f-score=1.0\nUsing threshold=0.9218669507660147 as decision boundary, we reach precision=1.0, recall=0.98, and f-score=0.98989898989899\nUsing threshold=0.93066642297958 as decision boundary, we reach precision=1.0, recall=0.96, and f-score=0.9795918367346939\nUsing threshold=0.9332685743944795 as decision boundary, we reach precision=1.0, recall=0.94, and f-score=0.9690721649484536\nUsing threshold=0.9395382533408563 as decision boundary, we reach precision=1.0, recall=0.92, and f-score=0.9583333333333334\nUsing threshold=0.9640718757241656 as decision boundary, we reach precision=1.0, recall=0.9, and f-score=0.9473684210526316\nUsing threshold=0.9670374623286897 as decision boundary, we reach precision=1.0, recall=0.88, and f-score=0.9361702127659575\nUsing threshold=0.9687934720210198 as decision boundary, we reach precision=1.0, recall=0.86, and f-score=0.924731182795699\nUsing threshold=0.9726392263137621 as decision boundary, we reach precision=1.0, recall=0.84, and f-score=0.9130434782608696\nUsing threshold=0.973775627114333 as decision boundary, we reach precision=1.0, recall=0.82, and f-score=0.9010989010989011\nUsing threshold=0.9740474969329987 as decision boundary, we reach precision=1.0, recall=0.8, and f-score=0.888888888888889\nUsing threshold=0.9741603105458991 as decision boundary, we reach precision=1.0, recall=0.78, and f-score=0.8764044943820225\nUsing threshold=0.9747085542467909 as decision boundary, we reach precision=1.0, recall=0.76, and f-score=0.8636363636363636\nUsing threshold=0.974749494774799 as decision boundary, we reach precision=1.0, recall=0.74, and f-score=0.8505747126436781\nUsing threshold=0.9769993303678443 as decision boundary, we reach precision=1.0, recall=0.72, and f-score=0.8372093023255813\nUsing threshold=0.9770140294088295 as decision boundary, we reach precision=1.0, recall=0.7, and f-score=0.8235294117647058\nUsing threshold=0.9785921201646789 as decision boundary, we reach precision=1.0, recall=0.68, and f-score=0.8095238095238095\nUsing threshold=0.9786461690308931 as decision boundary, we reach precision=1.0, recall=0.66, and f-score=0.7951807228915663\nUsing threshold=0.9789411518223052 as decision boundary, we reach precision=1.0, recall=0.64, and f-score=0.7804878048780487\nUsing threshold=0.9796555988114017 as decision boundary, we reach precision=1.0, recall=0.62, and f-score=0.7654320987654321\nUsing threshold=0.9801649093623934 as decision boundary, we reach precision=1.0, recall=0.6, and f-score=0.7499999999999999\nUsing threshold=0.9805566289582609 as decision boundary, we reach precision=1.0, recall=0.58, and f-score=0.7341772151898733\nUsing threshold=0.9808560894443067 as decision boundary, we reach precision=1.0, recall=0.56, and f-score=0.717948717948718\nUsing threshold=0.982400866419342 as decision boundary, we reach precision=1.0, recall=0.54, and f-score=0.7012987012987013\nUsing threshold=0.9828790909959155 as decision boundary, we reach precision=1.0, recall=0.52, and f-score=0.6842105263157895\nUsing threshold=0.9828854909335458 as decision boundary, we reach precision=1.0, recall=0.5, and f-score=0.6666666666666666\nUsing threshold=0.9839851081942663 as decision boundary, we reach precision=1.0, recall=0.48, and f-score=0.6486486486486487\nUsing threshold=0.9845312460821358 as decision boundary, we reach precision=1.0, recall=0.46, and f-score=0.6301369863013699\nUsing threshold=0.9857012993403023 as decision boundary, we reach precision=1.0, recall=0.44, and f-score=0.6111111111111112\nUsing threshold=0.9879940756602601 as decision boundary, we reach precision=1.0, recall=0.42, and f-score=0.5915492957746479\nUsing threshold=0.9882223190984861 as decision boundary, we reach precision=1.0, recall=0.4, and f-score=0.5714285714285715\nUsing threshold=0.9889482842475497 as decision boundary, we reach precision=1.0, recall=0.38, and f-score=0.5507246376811594\nUsing threshold=0.9892545856218082 as decision boundary, we reach precision=1.0, recall=0.36, and f-score=0.5294117647058824\nUsing threshold=0.9899303560728386 as decision boundary, we reach precision=1.0, recall=0.34, and f-score=0.5074626865671642\nUsing threshold=0.9905455482163618 as decision boundary, we reach precision=1.0, recall=0.32, and f-score=0.48484848484848486\nUsing threshold=0.9907019104721698 as decision boundary, we reach precision=1.0, recall=0.3, and f-score=0.4615384615384615\nUsing threshold=0.9911493537429485 as decision boundary, we reach precision=1.0, recall=0.28, and f-score=0.43750000000000006\nUsing threshold=0.9914230947944308 as decision boundary, we reach precision=1.0, recall=0.26, and f-score=0.41269841269841273\nUsing threshold=0.9915673581329265 as decision boundary, we reach precision=1.0, recall=0.24, and f-score=0.3870967741935484\nUsing threshold=0.9919835313724615 as decision boundary, we reach precision=1.0, recall=0.22, and f-score=0.36065573770491804\nUsing threshold=0.9925274516087134 as decision boundary, we reach precision=1.0, recall=0.2, and f-score=0.33333333333333337\nUsing threshold=0.9926276253093826 as decision boundary, we reach precision=1.0, recall=0.18, and f-score=0.3050847457627119\nUsing threshold=0.9930234956465036 as decision boundary, we reach precision=1.0, recall=0.16, and f-score=0.2758620689655173\nUsing threshold=0.9931758599517743 as decision boundary, we reach precision=1.0, recall=0.14, and f-score=0.24561403508771928\nUsing threshold=0.9935881899997894 as decision boundary, we reach precision=1.0, recall=0.12, and f-score=0.21428571428571425\nUsing threshold=0.9946684285206863 as decision boundary, we reach precision=1.0, recall=0.1, and f-score=0.18181818181818182\nUsing threshold=0.9960976336416663 as decision boundary, we reach precision=1.0, recall=0.08, and f-score=0.14814814814814814\nUsing threshold=0.996289803123931 as decision boundary, we reach precision=1.0, recall=0.06, and f-score=0.11320754716981131\nUsing threshold=0.9975518299472802 as decision boundary, we reach precision=1.0, recall=0.04, and f-score=0.07692307692307693\nUsing threshold=0.998322588642525 as decision boundary, we reach precision=1.0, recall=0.02, and f-score=0.0392156862745098\nThe threshold for the max f-score is 0.8628645363798557\n"
"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression(n_samples=100, n_features=4,\n                    n_informative=2, n_targets=1,\n                    random_state=0, shuffle=False)\nregr = BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=3),\n                        n_estimators=3, random_state=0)\nregr.fit(X, y)\n\nregr.estimators_\n# result:\n[DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=2087557356, splitter='best'),\n DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=132990059, splitter='best'),\n DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=1109697837, splitter='best')]\n\nfor model in regr.estimators_:\n    model.fit(Xs, Ys)\n"
'from sklearn.preprocessing import MultiLabelBinarizer\n\ncount_vec = MultiLabelBinarizer()\nmlb = count_vec.fit(df["comment text"])\npd.DataFrame(mlb.transform(df["comment text"]), columns=[mlb.classes_])\n\n  bedrijf bedrijfje behandeld goed leuk slecht\n0       1         0         0    0    0      1\n1       0         1         1    1    1      0\n2       0         1         1    1    1      0\n3       0         1         0    0    1      0\n\ntext = df["comment text"].map(\' \'.join)\ncount_vec = CountVectorizer()\ncv = count_vec.fit(text)\n\npd.DataFrame(cv.transform(text).toarray(), columns=[mlb.classes_])\n\n  bedrijf bedrijfje behandeld goed leuk slecht\n0       1         0         0    0    0      1\n1       0         1         1    1    1      0\n2       0         1         1    1    1      0\n3       0         1         0    0    1      0\n\n    bedrijf bedrijfje behandeld      goed      leuk    slecht\n0  0.707107  0.000000  0.000000  0.000000  0.000000  0.707107\n1  0.000000  0.444931  0.549578  0.549578  0.444931  0.000000\n2  0.000000  0.444931  0.549578  0.549578  0.444931  0.000000\n3  0.000000  0.707107  0.000000  0.000000  0.707107  0.000000\n'
"from textblob import TextBlob\ndf['sentiment'] = df['Tweet'].apply(lambda Tweet: TextBlob(Tweet).sentiment)\nprint(df)\n\n    Date     ...                                  sentiment\n0  1/1/2020  ...                                 (0.0, 0.0)\n1  2/1/2020  ...                                 (0.0, 0.0)\n2  3/2/2020  ...                                 (0.0, 0.1)\n3  4/2/2020  ...  (-0.6999999999999998, 0.6666666666666666)\n4  5/2/2020  ...                                 (0.5, 0.6)\n\n[5 rows x 4 columns]\n\ndf_new = df\ndf_new['polarity'] = df1['polarity']\ndf_new.polarity = df1.polarity.astype(float)\ndf_new['subjectivity'] = df1['subjectivity']\ndf_new.subjectivity = df1.polarity.astype(float)\n\nimport numpy as np\nconditionList = [\n    df_new['polarity'] == 0,\n    df_new['polarity'] &gt; 0,\n    df_new['polarity'] &lt; 0]\nchoiceList = ['neutral', 'positive', 'negative']\ndf_new['label'] = np.select(conditionList, choiceList, default='no_label')\nprint(df_new)\n\n[5 rows x 6 columns]\n       Date  ID                 Tweet  ... polarity  subjectivity     label\n0  1/1/2020   1  the weather is sunny  ...      0.0           0.0   neutral\n1  2/1/2020   2       tom likes harry  ...      0.0           0.0   neutral\n2  3/2/2020   3       the sky is blue  ...      0.0           0.0   neutral\n3  4/2/2020   4    the weather is bad  ...     -0.7          -0.7  negative\n4  5/2/2020   5         i love apples  ...      0.5           0.5  positive\n\n[5 rows x 7 columns]\n\nimport pandas as pd\n\n# create a dictionary\ndata = {&quot;Date&quot;:[&quot;1/1/2020&quot;,&quot;2/1/2020&quot;,&quot;3/2/2020&quot;,&quot;4/2/2020&quot;,&quot;5/2/2020&quot;],\n    &quot;ID&quot;:[1,2,3,4,5],\n    &quot;Tweet&quot;:[&quot;the weather is sunny&quot;,\n             &quot;tom likes harry&quot;, &quot;the sky is blue&quot;,\n             &quot;the weather is bad&quot;,&quot;i love apples&quot;]}\n# convert data to dataframe\ndf = pd.DataFrame(data)\n\n# create some dummy data\nimport pandas as pd\nimport numpy as np\n\n# create a dictionary\ndata = {&quot;Date&quot;:[&quot;1/1/2020&quot;,&quot;2/1/2020&quot;,&quot;3/2/2020&quot;,&quot;4/2/2020&quot;,&quot;5/2/2020&quot;],\n        &quot;ID&quot;:[1,2,3,4,5],\n        &quot;Tweet&quot;:[&quot;the weather is sunny&quot;,\n                 &quot;tom likes harry&quot;, &quot;the sky is blue&quot;,\n                 &quot;the weather is bad&quot;,&quot;i love apples&quot;]}\n# convert data to dataframe\ndf = pd.DataFrame(data)\n\nfrom textblob import TextBlob\ndf['sentiment'] = df['Tweet'].apply(lambda Tweet: TextBlob(Tweet).sentiment)\nprint(df)\n\n# split the sentiment column into two\ndf1=pd.DataFrame(df['sentiment'].tolist(), index= df.index)\n\n# append cols to original dataframe\ndf_new = df\ndf_new['polarity'] = df1['polarity']\ndf_new.polarity = df1.polarity.astype(float)\ndf_new['subjectivity'] = df1['subjectivity']\ndf_new.subjectivity = df1.polarity.astype(float)\nprint(df_new)\n\n# add label to dataframe based on condition\nconditionList = [\n    df_new['polarity'] == 0,\n    df_new['polarity'] &gt; 0,\n    df_new['polarity'] &lt; 0]\nchoiceList = ['neutral', 'positive', 'negative']\ndf_new['label'] = np.select(conditionList, choiceList, default='no_label')\nprint(df_new)\n"
'from sklearn.feature_extraction.text import TfidfVectorizer\n\ntokens_raw = [\n    [&quot;kitchen&quot;, &quot;getting&quot;, &quot;children&quot;, &quot;ready&quot;, &quot;school&quot;],\n    [&quot;shanghai&quot;, &quot;appointed&quot;, &quot;manager&quot;, &quot;taco&quot;, &quot;bell&quot;],\n]\n\nvectorizer = TfidfVectorizer(analyzer=lambda x: x)\nX = vectorizer.fit_transform(tokens_raw)\n'
'modelpersister\n├───modelpersister\n│   ├───model.pkl\n│   ├───__init__.py\n│   ├───model_definition.py\n│   ├───train.py\n│   └───analyze.py\n└───pyproject.toml\n\n[tool.poetry]\nname = &quot;modelpersister&quot;\nversion = &quot;0.1.0&quot;\ndescription = &quot;Ship a sentiment analysis model.&quot;\nauthors = [&quot;Mishaal &lt;my@mail.com&gt;&quot;]\nlicense = &quot;MIT&quot;  # a good default as far as licenses go\n\n[tool.poetry.dependencies]\npython = &quot;^3.8&quot;\nsklearn = &quot;^0.23&quot;  # or whichever ML library you used for your model definition\n\n[tool.poetry.dev-dependencies]\n\n[build-system]\nrequires = [&quot;poetry&gt;=0.12&quot;]\nbuild-backend = &quot;poetry.masonry.api&quot;\n'
"from nltk.corpus import brown\nfrom nltk import FreqDist\n\ntagged_words = brown.tagged_words(categories='mystery')\n\n# get list of lowercased nouns    \nnouns = [word[0].lower() for word in tagged_words if word[1] in ['NP', 'NN']]    \nnouns_freq = FreqDist(nouns)\n"
' True: [1, 2, 0, 0, 1]\nFalse: [0, 0, 1, 2, 1]\n\n True: 1/3 * [1, 2, 1, 2, 2] = [1/3 2/3 1/3 2/3 2/3]\nFalse: 2/3 * [1, 2, 1, 2, 2] = [2/3 4/3 2/3 4/3 4/3]\n\n(1-2/3)^2 / (2/3) + (1-4/3)^2 / (4/3) = 1/6 + 1/12 = 1/4 = 0.25\n'
"files = glob2.glob('class_*\\\\*.jpg')\n\ndef load(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, size=(299, 299))\n    label = tf.strings.split(file_path, os.sep)[0]\n    label = tf.cast(tf.equal(label, 'class_a'), tf.int32)\n    return img, label\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(files).map(load).batch(4)\n\nmodel.fit(train_ds)\n"
'from sklearn.metrics import mean_squared_error \n\nlist_ = [[489066.76, 300334.], \n[227458.2,  200352.  ],\n[928249.59, 946729.  ],\n[339032.27, 350116.  ],\n[689668.21, 600322.  ],\n[489179.58, 577936.  ]]\n\ny_true = [y[0] for y in list_]\ny_pred = [y[1] for y in list_]\n\nmse = mean_squared_error(y_true, y_pred)\nprint(mse)\n# 8779930962.14985\n\ndef my_mse(y_true, y_pred):\n  diff = 0\n  for couple in zip(y_true, y_pred):\n    diff+=pow(couple[0]-couple[1], 2)\n  return diff/len(y_true)\n\nprint(my_mse(y_true, y_pred))\n# 8779930962.14985\n'
"dense1 = 2**7\ndense2 = 2**8\ndense3 = 2**9\ndropout = 0.8\nprice_loss = 1\ncut_loss = 1\nactivation= LeakyReLU()\nbatch_size = 32\n\n#====================================================================\n# INPUTS\n#====================================================================\ncarat = Input(shape= (1,), batch_size= batch_size, name= 'carat')\nColor = Input(shape= (1,), batch_size= batch_size, name= 'color')\nClarity = Input(shape= (1,), batch_size= batch_size, name= 'clarity')\ndepth = Input(shape= (1,), batch_size= batch_size, name= 'depth')\ntable = Input(shape= (1,), batch_size= batch_size, name= 'table')\nX = Input(shape= (1,), batch_size= batch_size, name= 'x')\ny = Input(shape= (1,), batch_size= batch_size, name= 'y')\nz = Input(shape= (1,), batch_size= batch_size, name= 'z')\n#====================================================================\n# CREATE EMBEDDINGS FOR CATEGORICAL FEATURES &quot;COLOR&quot; AND &quot;CLARITY&quot;\n#====================================================================\ncolor = Embedding(input_dim = 7, output_dim = 1, name = 'color_emb')(Color)\nclarity = Embedding(input_dim = 8, output_dim = 1, name = 'clarity_emb')(Clarity)\ncolor = Flatten()(color)\nclarity = Flatten()(clarity)\n#====================================================================\n# CONCATENATE FEATURES\n#====================================================================\nx = Concatenate()([color, clarity, carat, depth, table, X, y, z])\n#====================================================================\n# DENSE NETWORK\n#====================================================================\nx = Dense(dense1, activation = activation)(x)\nx = BatchNormalization()(x)\nx = Dense(dense2, activation = activation)(x)\nx = BatchNormalization()(x)\nx = Dense(dense3, activation = activation)(x)\nx = BatchNormalization()(x)\nx = Dropout(dropout)(x)\n#====================================================================\n# PREDICTIONS\n# ====================================================================\ncut = Dense(1, activation = 'sigmoid')(x)\nprice = Dense(1)(x)\n#====================================================================\n# DEFINE THE MODEL\n# ====================================================================\nmodel = Model(inputs = [carat, Color, Clarity, depth, table, X, y, z] , \n              outputs = [cut , price])\nmodel.compile('adam', 'mse')\nmodel.summary()\n"
"df3 = df1.merge(df2.groupby('Business_id')['Review_text'].apply(list).reset_index(),\n               how='left', on='Business_id').rename({'Review_text':'All_reviews'}, axis=1)\n\nOut[1]: \n   Business_id       category  star  Review_count               All_reviews\n0            1       shopping   3.5             3  [Text_1, Text_2, Text_4]\n1            2     restaurant   5.0             1          [Text_3, Text_5]\n2            3  Home services   4.0             6                       NaN\n"
"from fuzzywuzzy import fuzz\n\n\ndef replace_similars(input_list):\n    # Replaces %90 and more similar strings\n    for i in range(len(input_list)):\n        for j in range(len(input_list)):\n            if i &lt; j and fuzz.ratio(input_list[i], input_list[j]) &gt;= 90:\n                input_list[j] = input_list[i]\n\n\ndef generate_mapping(input_list):\n    new_list = input_list[:]  # copy list\n    replace_similars(new_list)\n\n    mapping = {}\n    for i in range(len(input_list)):\n        mapping[input_list[i]] = new_list[i]\n\n    return mapping\n\n# Let's assume items in labels are unique.\n# If they are not unique, it will work anyway but will be slower.\nlabels = [\n    &quot;Cable replaced&quot;,\n    &quot;Cable replaced.&quot;,\n    &quot;Camera is up and recording&quot;,\n    &quot;Chat closed due to inactivity.&quot;,\n    &quot;Closing as duplicate&quot;,\n    &quot;Closing as duplicate.&quot;,\n    &quot;Closing duplicate ticket.&quot;,\n    &quot;Closing ticket.&quot;,\n    &quot;Completed&quot;,\n    &quot;Connection to IDF restored&quot;,\n]\n\nmapping = generate_mapping(labels)\n\n\n# Print to see mapping\nprint(&quot;\\n&quot;.join([&quot;{:&lt;50}: {}&quot;.format(k, v) for k, v in mapping.items()]))\n\nCable replaced                                    : Cable replaced\nCable replaced.                                   : Cable replaced\nCamera is up and recording                        : Camera is up and recording\nChat closed due to inactivity.                    : Chat closed due to inactivity.\nClosing as duplicate                              : Closing as duplicate\nClosing as duplicate.                             : Closing as duplicate\nClosing duplicate ticket.                         : Closing duplicate ticket.\nClosing ticket.                                   : Closing ticket.\nCompleted                                         : Completed\nConnection to IDF restored                        : Connection to IDF restored\n\nfor k, v in mapping.items():\n    if k != v:\n        h.loc[h['resolution'] == k, 'resolution'] = v\n"
'    x0          x1          y\n0   10.354468   7.655144    168.061214\n1   8.786243    6.244283    156.570749\n2   10.450548   8.084427    152.102614\n3   10.869778   9.165630    129.721267\n4   11.236594   5.798762    55.294962\n5   9.111226    10.289447   308.747597\n6   9.753313    9.803181    163.337342\n7   9.752270    9.004989    271.944276\n8   8.671618    9.801712    158.096221\n9   8.830913    6.632544    316.239129\n\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import mutual_info_regression\n\nX_new = SelectKBest(f_regression, k=2).fit_transform(X_train, y_train)\nX_new = SelectKBest(mutual_info_regression, k=2).fit_transform(X_train, y_train)\n'
' The validation data is selected from the last samples in the x and y data provided, before shuffling. \n'
'ts = np.array([[1,2,3,4,5],[2,3,4,5,6],[9,8,7,4,1]])\nexpectation_maximization(ts, 2)\n'
'pip install theano==0.8.2\n'
"df = pd.DataFrame({'Full Date': ['07/31/2016','05/28/2011']})\ndef date_breaker(date):\n    year = date[-4:]\n    month = date[:2]\n    return (year,month)\nx = df['Full Date'].map(lambda x: date_breaker(x))\ndf['year'],df['month'] = zip(*x)\n"
"tf.summary.image('input', image_batch, 10)\n"
'import numpy as np\nimport time\nimport sys\nimport os\n\n\nfrom scipy.stats import norm\n\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import metrics\nfrom keras.datasets import mnist\n\nfrom keras.callbacks import ModelCheckpoint\n\nfilepath_for_w=\'denoise_by_VAE_weights_1.h5\'\n\n\n\n###########\n##########\nexperiment_dir= \'exp_\'+str(int(time.time()))\nos.mkdir(experiment_dir)\nthis_script=sys.argv[0]\nfrom shutil import copyfile\ncopyfile(this_script, experiment_dir+\'/\'+this_script)\n##########\n###########\n\n\nbatch_size = 100\noriginal_dim = 784\nlatent_dim = 2\nintermediate_dim = 256\nepochs = 10\nepsilon_std = 1.0\n\nx = Input(batch_shape=(batch_size, original_dim))\nh = Dense(intermediate_dim, activation=\'relu\')(x)\nz_mean = Dense(latent_dim)(h)\nz_log_var = Dense(latent_dim)(h)\n\n\ndef sampling(args):\n    z_mean, z_log_var = args\n    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n                              stddev=epsilon_std)\n    return z_mean + K.exp(z_log_var / 2) * epsilon\n\n# note that "output_shape" isn\'t necessary with the TensorFlow backend\nz = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_h = Dense(intermediate_dim, activation=\'relu\')\ndecoder_mean = Dense(original_dim, activation=\'sigmoid\')\nh_decoded = decoder_h(z)\nx_decoded_mean = decoder_mean(h_decoded)\n\n\ndef vae_loss(x, x_decoded_mean):\n    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n    return xent_loss + kl_loss\n\nvae = Model(x, x_decoded_mean)\nvae.compile(optimizer=\'rmsprop\', loss=vae_loss)\n\n\n\n\n# train the VAE on MNIST digits\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\n#after loading the data, change to the new experiment dir\nos.chdir(experiment_dir) #\n##########################\n\nx_train = x_train.astype(\'float32\') / 255.\nx_test = x_test.astype(\'float32\') / 255.\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\n\nnoise_factor = 0.5\n\nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \nx_test_noisy = np.clip(x_test_noisy, 0., 1.)\n\n\nfor i in range (10):\n\n    x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n    x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n\n    checkpointer=ModelCheckpoint(filepath_for_w, monitor=\'val_loss\', verbose=0, save_best_only=True, save_weights_only=True, mode=\'auto\', period=1)\n    vae.fit(x_train_noisy, x_train,\n            shuffle=True,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data=(x_test_noisy, x_test),\n            callbacks=[checkpointer])\n    vae.load_weights(filepath_for_w) \n\n    #print (x_train.shape)\n    #print (x_test.shape)\n\n    decoded_imgs = vae.predict(x_test,batch_size=batch_size)\n    np.save(\'decoded\'+str(i)+\'.npy\',decoded_imgs)\n\n\nnp.save(\'tested.npy\',x_test_noisy)\n#np.save (\'true_catagories.npy\',y_test)\nnp.save(\'original.npy\',x_test)\n'
'clf = GradientBoostingClassifier(verbose=True)\n'
"vect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\n\nIn [29]: vect.fit_transform(['q'])\nOut[29]:\n&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n        with 1 stored elements in Compressed Sparse Row format&gt;\n\nIn [30]: vect.get_feature_names()\nOut[30]: ['q']\n"
"import tensorflow as tf\nweight = tf.Variable(0.0)\nop = tf.assign_add(weight, 1) # update weight by adding 1 to it\nwith tf.Session() as sess:   \n     sess.run(tf.global_variables_initializer())\n     print(sess.run(weight)) # Get the value of the weight\n     print(sess.run(op))     # Update the weight\n     print(sess.run(weight)) # Get the value of the weight but don't update it\n     print(sess.run(weight)) # Get the value of the weight\n\n0.0\n1.0\n1.0\n1.0\n"
"try:\n    from pathlib import Path\nexcept ImportError:             # Python 2\n    from pathlib2 import Path\nimport os\nimport re\nfrom pprint import pprint\nimport pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, LabelBinarizer, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB, GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.externals import joblib\nfrom scipy.sparse import csr_matrix, hstack\n\n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n\n    def __init__(self, name=None, position=None,\n                 as_cat_codes=False, sparse=False):\n        self.name = name\n        self.position = position\n        self.as_cat_codes = as_cat_codes\n        self.sparse = sparse\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, **kwargs):\n        if self.name is not None:\n            col_pos = X.columns.get_loc(self.name)\n        elif self.position is not None:\n            col_pos = self.position\n        else:\n            raise Exception('either [name] or [position] parameter must be not-None')\n        if self.as_cat_codes and X.dtypes.iloc[col_pos] == 'category':\n                ret = X.iloc[:, col_pos].cat.codes\n        else:\n            ret = X.iloc[:, col_pos]\n        if self.sparse:\n            ret = csr_matrix(ret.values.reshape(-1,1))\n        return ret\n\nunion = FeatureUnion([\n            ('text', \n             Pipeline([\n                ('select', ColumnSelector('url')),\n                #('pct', SelectPercentile(percentile=1)),\n                ('vect', TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                         stop_words='english')),\n             ]) ),\n            ('ads',\n             Pipeline([\n                ('select', ColumnSelector('ads_keyword', sparse=True,\n                                          as_cat_codes=True)),\n                #('scale', StandardScaler(with_mean=False)),\n             ]) )\n        ])\n\npipe = Pipeline([\n    ('union', union),\n    ('clf', MultinomialNB())\n])\n\nparam_grid = [\n    {\n        'union__text__vect': [TfidfVectorizer(sublinear_tf=True,\n                                              max_df=0.5,\n                                              stop_words='english')],\n        'clf': [SGDClassifier(max_iter=500)],\n        'union__text__vect__ngram_range': [(1,1), (2,5)],\n        'union__text__vect__analyzer': ['word','char_wb'],\n        'clf__alpha': np.logspace(-5, 0, 6),\n        #'clf__max_iter': [500],\n    },\n    {\n        'union__text__vect': [TfidfVectorizer(sublinear_tf=True,\n                                              max_df=0.5,\n                                              stop_words='english')],\n        'clf': [MultinomialNB()],\n        'union__text__vect__ngram_range': [(1,1), (2,5)],\n        'union__text__vect__analyzer': ['word','char_wb'],\n        'clf__alpha': np.logspace(-4, 2, 7),\n    },\n    #{        # NOTE: does NOT support sparse matrices!\n    #    'union__text__vect': [TfidfVectorizer(sublinear_tf=True,\n    #                                          max_df=0.5,\n    #                                          stop_words='english')],\n    #    'clf': [GaussianNB()],\n    #    'union__text__vect__ngram_range': [(1,1), (2,5)],\n    #    'union__text__vect__analyzer': ['word','char_wb'],\n    #},\n]\n\ngs_kwargs = dict(scoring='roc_auc', cv=3, n_jobs=1, verbose=2)\nX_train, X_test, y_train, y_test = \\\n    train_test_split(df[['url','ads_keyword']], df['target'], test_size=0.33)\ngrid = GridSearchCV(pipe, param_grid=param_grid, **gs_kwargs)\ngrid.fit(X_train, y_train)\n\n# prediction\npredicted = grid.predict(X_test)\n"
"&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sklearn.base import BaseEstimator, RegressorMixin\n&gt;&gt;&gt; from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n&gt;&gt;&gt; from sklearn.utils.multiclass import unique_labels\n&gt;&gt;&gt; from sklearn.metrics import euclidean_distances\n&gt;&gt;&gt; class TemplateClassifier(BaseEstimator, RegressorMixin):\n...\n...     def __init__(self, demo_param='demo'):\n...         self.demo_param = demo_param\n...\n...     def fit(self, X, y):\n...\n...         # Check that X and y have correct shape\n...         X, y = check_X_y(X, y)\n...         # Store the classes seen during fit\n...         self.classes_ = unique_labels(y)\n...\n...         self.X_ = X\n...         self.y_ = y\n...         # Return the classifier\n...         return self\n...\n...     def predict(self, X):\n...\n...         # Check is fit had been called\n...         check_is_fitted(self, ['X_', 'y_'])\n...\n...         # Input validation\n...         X = check_array(X)\n...\n...         closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n...         return self.y_[closest]\n"
'# Author: Raghav RV &lt;rvraghav93@gmail.com&gt;\n# License: BSD\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = make_hastie_10_2(n_samples=8000, random_state=42)\n\n# The scorers can be either be one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {\'AUC\': \'roc_auc\', \'Accuracy\': make_scorer(accuracy_score)}\n\n# Setting refit=\'AUC\', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_parameters_`` and\n# ``gs.best_index_``\ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid={\'min_samples_split\': range(2, 403, 10)},\n                  scoring=scoring, cv=5, refit=\'AUC\')\ngs.fit(X, y)\nresults = gs.cv_results_\n\nplt.figure(figsize=(13, 13))\nplt.title("GridSearchCV evaluating using multiple scorers simultaneously",\n          fontsize=16)\n\nplt.xlabel("min_samples_split")\nplt.ylabel("Score")\nplt.grid()\n\nax = plt.axes()\nax.set_xlim(0, 402)\nax.set_ylim(0.73, 1)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results[\'param_min_samples_split\'].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), [\'g\', \'k\']):\n    for sample, style in ((\'train\', \'--\'), (\'test\', \'-\')):\n        sample_score_mean = results[\'mean_%s_%s\' % (sample, scorer)]\n        sample_score_std = results[\'std_%s_%s\' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == \'test\' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == \'test\' else 0.7,\n                label="%s (%s)" % (scorer, sample))\n\n        best_index = np.nonzero(results[\'rank_test_%s\' % scorer] == 1)[0][0]\n        best_score = results[\'mean_test_%s\' % scorer][best_index]\n\n        # Plot a dotted vertical line at the best score for that scorer marked by x\n        ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n                linestyle=\'-.\', color=color, marker=\'x\', markeredgewidth=3, ms=8)\n\n\n    # Annotate the best score for that scorer\n    ax.annotate("%0.2f" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc="best")\nplt.grid(\'off\')\nplt.show()\n'
"from sklearn.metrics import classification_report\ny_true = [0, 1, 2, 2, 0]\ny_pred = [0, 0, 2, 2, 0] # no 1's predicted\ntarget_names = ['class 0', 'class 1', 'class 2']\n\nprint classification_report(y_true, y_pred, target_names=target_names)\n\n             precision    recall  f1-score   support\n\n    class 0       0.67      1.00      0.80         2\n    class 1       0.00      0.00      0.00         1\n    class 2       1.00      1.00      1.00         2\n\navg / total       0.67      0.80      0.72         5\n\n/home/ctsats/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: \nUndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n  \n\nf1_score(y_true, y_pred, average='micro')\n# 0.8000000000000002\n"
"for data in dataloaders['val']:\n    images, labels = data\n    outputs = model(inputs)\n\nfor data in dataloaders['val']:\n    inputs, labels = data\n    outputs = model(inputs)\n"
"from keras.layers import Concatenate\n\ninput_1 = Input(shape=(1,), name='date')           # input layers\ninput_2 = Input(shape=(1,), name='km')\ninput_3 = Input(shape=(10,), name='consume')\ninput_4 = Input(shape=(440,), name='type')\n\nx = Concatenate()([input_1 , input_2 , input_3 , input_4]) # Concatenation of the inputs.\n\ndense_1 = Dense(256, activation='relu')(x)   # hidden layers\ndropout_1 = Dropout(0.5)(dense_1)\n\ndense_2 = Dense(256, activation='relu')(dropout_1)\ndropout_2 = Dropout(0.5)(dense_2)\n\noutputs = Dense(1, activation='linear')(dropout_2) # output layer\n\nmodel = Model([input_1,input_2,input_3,input_4], outputs)\n"
'# Model\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(N_HIDDEN_NEURONS,\n                             return_sequences=True,\n                             activation="tanh"), \n                        input_shape=(SEGMENT_TIME_SIZE, N_FEATURES)))\nmodel.add(Bidirectional(LSTM(N_HIDDEN_NEURONS)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(N_CLASSES, activation=\'sigmoid\'))\nmodel.compile(\'adam\', \'binary_crossentropy\', metrics=[\'accuracy\'])\n\nmodel.fit(X_train, y_train,\n          batch_size=BATCH_SIZE,\n          epochs=N_EPOCHS,\n          validation_data=[X_test, y_test])\n\nmodel.save(\'model_keras/model.h5\')\n\nmodel = load_model(\'model_keras/model.h5\')\n'
'bias = np.ones(50)\ntrainX = np.arange(0, 10, 0.2)\ntrainY = (3 * trainX + np.random.rand(trainX.shape[0]) * 20 - 10) + 10\ntrainX = np.vstack([bias, trainX]).T\n\nsess.run(train_op, feed_dict={X: x.reshape((1, 2)), Y: y})\n\nX = tf.placeholder("float", shape=(2,))\nY = tf.placeholder("float")\nw = tf.Variable([0.0, 0.0], name="weights")\n\nmodel = tf.tensordot(X, w, 1)\n\nX = tf.placeholder("float", shape=(50, 2))\nY = tf.placeholder("float", shape=(50, 1))\nw = tf.Variable(tf.zeros([2, 1], "float"), name="weights")\n\nmodel = tf.matmul(X, w)\ncost = tf.reduce_sum(tf.pow((Y - model), 2))\n\ninit = tf.global_variables_initializer()\ntrain_op = tf.train.GradientDescentOptimizer(0.0001).minimize(cost)\n\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(10000):\n        sess.run(train_op, feed_dict={X: trainX, Y: trainY.reshape((50, 1))})\n'
'preds_prob = model.predict([X_test_feature1, X_test_feature2])\npreds = model.predict_classes([X_test_feature1, X_test_feature2])\n\nloss_metrics = model.evaluate([X_test_feature1, X_test_feature2], y_test)\n'
"myBopt.run_optimization(..., context={'var1': .3, 'var2': 0.4})\n"
"def build(...):\n    self.kernel = self.add_variable('kernel', ...\n    self.bias = self.add_variable\ndef call(...):\n    # ...\n    outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n    # ...\n    if self.activation is not None:\n        return self.activation(outputs)\n    return outputs\n"
"vec_train = TfidfVectorizer()\nX_train = vec_train.fit_transform(df_train['text'])\n\nX_test = vec_train.transform(df_test['text'])\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n"
'def generate_batch_data(num):\n    #load X images here\n    return images\n\nmodel.fit_generator(generate_batch_data(X),\n        samples_per_epoch=10000, nb_epoch=10)\n'
'import numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_features=20, n_samples=1000, random_state=10\n)\nclf = LogisticRegression(class_weight="balanced")\nscoring = {\'accuracy\': \'accuracy\',\n           \'recall\': \'recall\',\n           \'precision\': \'precision\',\n           \'roc_auc\': \'roc_auc\'}\ncross_val_scores = cross_validate(clf, X, y, cv=3, scoring=scoring)\n\n{\'fit_time\': array([ 0.        ,  0.        ,  0.01559997]),\n \'score_time\': array([ 0.01559997,  0.        ,  0.        ]),\n \'test_accuracy\': array([ 0.9251497 ,  0.95808383,  0.93674699]),\n \'test_precision\': array([ 0.59183673,  0.70833333,  0.63636364]),\n \'test_recall\': array([ 0.85294118,  1.        ,  0.84848485]),\n \'test_roc_auc\': array([ 0.96401961,  0.99343137,  0.96787271]),\n \'train_accuracy\': array([ 0.96096096,  0.93693694,  0.95209581]),\n \'train_precision\': array([ 0.73033708,  0.62376238,  0.69148936]),\n \'train_recall\': array([ 0.97014925,  0.94029851,  0.95588235]),\n \'train_roc_auc\': array([ 0.99426906,  0.98509954,  0.99223039])}\n\nFOLD, METRIC = (0, \'test_precision\')\ncross_val_scores[METRIC][FOLD]\n\nnp.std(cross_val_scores[METRIC])\n'
'xs = tf.placeholder(tf.float32, [None, IMAGE_WIDTH * IMAGE_HEIGHT], name="images") / 255\n\nxs = tf.placeholder(tf.float32, [None, IMAGE_WIDTH * IMAGE_HEIGHT], name="images")\n'
'rois = rois.data.float()\nnum_rois = rois.size(0)\n\nrois[:,1:].mul_(self.spatial_scale)\nrois = rois.round().long() ## Check this here !!\n'
"scheduler.add_job(train, 'interval', hours=1, \n                  next_run_time=datetime.datetime.now(), replace_existing=True)\n\nscheduler = BlockingScheduler()\nscheduler.add_job(train, 'interval', hours=1, next_run_time=datetime.datetime.now())\n\ndef my_listener(event):\n    if event.exception:       \n        global scheduler\n        scheduler.shutdown()\n        gc.collect()\n        scheduler = BlockingScheduler()\n        scheduler.add_job(train, 'interval', hours=1, next_run_time=datetime.datetime.now())\n        scheduler.add_listener(my_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\n        scheduler.start()\n\nscheduler.add_listener(my_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)\nscheduler.start()\n"
'ratio = num_B / (num_A + num_B)\nweights = [ratio, 1.0 - ratio]\n'
'decoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\n\ndecoder_outputs = decoder_dense(decoder_outputs)\n'
'selector = SelectKBest(chi2, k=1000)\nselector.fit(features_train_tfidfv, labels_train)\n\nclf = MultinomialNB()\nclf.fit(selector.transform(features_train_tfidfv), labels_train)\n\nfeatures_test_cv = selector.transform(tfidfv.transform(cv.transform(features_test)))\npred = clf.predict(features_test_cv)    \n'
"from scipy.sparse import coo_matrix, hstack\nfrom sklearn.preprocessing import OneHotEncoder\nwith_prod_tfidf = text_pipeline.fit_transform(with_prod['Text'])\n\n#as per https://stackoverflow.com/questions/19710602/concatenate-sparse-matrices-in-python-using-scipy-numpy\nwith_prod_all = hstack([with_prod_tfidf, OneHotEncoder().fit_transform(with_prod[cat_features])])\nprint(with_prod_all.shape)\n"
'# Test Data\nX = vectorizer.transform(testList)\n'
'from sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import log_loss, make_scorer\n\niris = datasets.load_iris()\nparameters = {\'kernel\':(\'linear\', \'rbf\'), \'C\':[1, 10]}\nsvc = svm.SVC(gamma="scale", probability=True)\nLogLoss = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\nclf = GridSearchCV(svc, parameters, cv=5, scoring=LogLoss)\nclf.fit(iris.data, iris.target)\n\nprint(clf.best_score_, clf.best_estimator_)\n'
'test_vectors = vectorizer.transform(test_text_data)\n'
'from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(categories = "auto", handle_unknown = "ignore")\nX_train_encoded = encoder.fit_transform(X_train)\n\ntest_data = encoder.transform(test_data)\n\n(pd.DataFrame(test_data.toarray())).shape\n'
'labelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)\ny = y.reshape(-1, 1)\nonehotencoder = OneHotEncoder()\ny = onehotencoder.fit_transform(y).toarray()\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats\n\ntest_y = np.array([0]*100 + [1]*100)\npredicted_y_probs = np.concatenate((np.random.beta(2,5,100), np.random.beta(8,3,100)))\n\ndef estimate_beta(X):\n    xbar = np.mean(X)\n    vbar = np.var(X,ddof=1)\n    alphahat = xbar*(xbar*(1-xbar)/vbar - 1)\n    betahat = (1-xbar)*(xbar*(1-xbar)/vbar - 1)\n    return alphahat, betahat\n\npositive_beta_estimates = estimate_beta(predicted_y_probs[test_y == 1])\nnegative_beta_estimates = estimate_beta(predicted_y_probs[test_y == 0])\n\nunit_interval = np.linspace(0,1,100)\nplt.plot(unit_interval, scipy.stats.beta.pdf(unit_interval, *positive_beta_estimates), c=\'r\', label="positive")\nplt.plot(unit_interval, scipy.stats.beta.pdf(unit_interval, *negative_beta_estimates), c=\'g\', label="negative")\n\n# Show the threshold.\nplt.axvline(0.5, c=\'black\', ls=\'dashed\')\nplt.xlim(0,1)\n\n# Add labels\nplt.legend()\n'
'import numpy as  np\nimport numpy as  np\n\ndata = np.zeros((10,4))\nX = data[:,0:3].reshape(-1,1,3)\ny = data[:,3].reshape(-1,1)\nprint(X.shape)\nprint(y.shape)\n\n(10, 1, 3)\n(10, 1)\n\nmodel.fit(X, y)\n'
'df = pandas.read_csv(input_file, header=None)\ndf.columns = [&quot;data&quot;, &quot;target&quot;]\n\ndf, y = df.data, df.target\n\ntest_documents = []\nfor body in range(0, len(df)):\n    document = str(df[body])\n    test_documents.append(document)\nX = vectorizer.transform(test_documents).toarray()  #here is where change is!\nprediction = pd.DataFrame(classifier.predict(X))\n'
'clf = SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape=\'ovr\', degree=3, gamma=1, kernel=\'rbf\', max_iter=-1,\n    probability=False, random_state=None, shrinking=True, tol=0.001,\n    verbose=False)\n\nimport numpy as np\ndef show_top10(classifier, vectorizer, categories):\n feature_names = np.asarray(vectorizer.get_feature_names())\n for i, category in enumerate(categories):\n   top10 = np.argsort(classifier.coef_[i])[-10:]\n   print("%s: %s" % (category, " ".join(feature_names[top10])))\n'
'print("predicted result : ", model.predict(X_test[0].reshape(-1,num_pixels)))\n\nprint("predicted result : ",  np.argmax(model.predict(X_test[0].reshape(-1,num_pixels)), axis = 1))\n'
"export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, \n    special_characters=True, \n    feature_names = feature_cols, \n    class_names=['0', '1'])\n"
'import numpy as np\n\ndef correlationCoefficient(X, Y):\n    n = X.size\n    sum_X = X.sum()\n    sum_Y = Y.sum()\n    sum_XY = (X*Y).sum()\n    squareSum_X = (X*X).sum()\n    squareSum_Y = (Y*Y).sum()\n    corr = (n * sum_XY - sum_X * sum_Y)/(np.sqrt((n * squareSum_X - sum_X * sum_X)* (n * squareSum_Y - sum_Y * sum_Y))) \n    return corr\n\nfrom PIL import Image\n\nimg1 = Image.open("1.jpg").convert("L")\nim1 = np.array(img1)/255\n\nimg2 = Image.open("2.jpg").convert("L")\nim2 = np.array(img2)/255\n\nprint (\'{0:.6f}\'.format(correlationCoefficient(im1, im2))) \n'
'from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.clustering import KMeans, KMeansModel\nfrom pyspark.ml.pipeline import Pipeline\n\n\ndata = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),\n        (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\nmatrix_normalized = spark.createDataFrame(data, ["scaledFeatures"])\n\nkmeans = KMeans() \\\n          .setK(3) \\\n          .setFeaturesCol("scaledFeatures")\\\n          .setPredictionCol("cluster")\n\n# Chain indexer and tree in a Pipeline\npipeline = Pipeline(stages=[kmeans])\n\nmodel = pipeline.fit(matrix_normalized)\n\ncluster = model.transform(matrix_normalized)\n\nmodel.stages[0].clusterCenters()\n\n[array([0.5, 0.5]), array([8., 9.]), array([9., 8.])]\n'
'_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 100, 50)           2550      \n_________________________________________________________________\ndense_2 (Dense)              (None, 100, 50)           2550      \n=================================================================\nTotal params: 5,100\nTrainable params: 5,100\nNon-trainable params: 0\n_________________________________________________________________\n'
"import matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nmask = (y!=2)\ny = y[mask]\nX = X[mask,:]\nprint(y)\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n\npositive_class = 1\n\nclf = OneVsRestClassifier(LogisticRegression())\ny_score = cross_val_predict(clf, X, y, cv=10 , method='predict_proba')\n\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfpr[positive_class], tpr[positive_class], _ = roc_curve(y, y_score[:, positive_class])\nroc_auc[positive_class] = auc(fpr[positive_class], tpr[positive_class])\nprint(roc_auc)\n\n{1: 1.0}\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_validate\n\nmyscore = make_scorer(roc_auc_score, needs_proba=True)\n\nclf = OneVsRestClassifier(LogisticRegression())\nmy_value = cross_validate(clf, X, y, cv=10, scoring = myscore)\nprint(np.mean(my_value['test_score'].tolist()))\n1.0\n"
"import ludwig\nludwig.visualize.learning_curves(\n  [train_stats],\n  TARGET,\n  model_names=None,\n  output_directory=None,\n  file_format='pdf'\n)\n"
'selection.estimator_.score(x_test, y_test)\n'
'history = model.fit(...)\nprint(history.history)  # &lt;-- a dict which contains all the loss and metric values per epoch\n\nfrom keras import layers\nfrom keras import Model\nimport numpy as np\n\ninp = layers.Input((1,))\nout1 = layers.Dense(2, name="output1")(inp)\nout2 = layers.Dense(3, name="output2")(inp)\n\nmodel = Model(inp, [out1, out2])\nmodel.compile(loss=\'mse\', optimizer=\'adam\')\n\nx = np.random.rand(2, 1)\ny1 = np.random.rand(2, 2)\ny2 = np.random.rand(2, 3)\nhistory = model.fit(x, [y1,y2], epochs=5)\n\nprint(history.history)\n\n#{\'loss\': [1.0881365537643433, 1.084699034690857, 1.081269383430481, 1.0781562328338623, 1.0747418403625488],\n# \'output1_loss\': [0.87154925, 0.8690172, 0.86648905, 0.8641926, 0.8616721],\n# \'output2_loss\': [0.21658726, 0.21568182, 0.2147803, 0.21396361, 0.2130697]}\n'
'class MyCallback(Callback):\n #def on_epochs_end(self, epoch, logs={}): # should be epoch (not epochs)\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get(\'accuracy\') &gt; 0.9):\n      print("\\n Training stopping now. accuracy reached 90 !")\n      self.model.stop_training = True\n'
'norm = FRmodel.outputs[0]\nnorm = Lambda(...)(norm)\n'
"# Basic LSTM that attempts to produce a prediction vector from a sequence of time data. \nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(20), \n])\n\n# Maybe throw a few LSTM layers to help learn the information, add some dimensions. \nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(5,return_sequences=True), \n    tf.keras.layers.LSTM(50,return_sequences=True),\n    tf.keras.layers.LSTM(20),\n])\n\n# Maybe some Dense layers should be used to re-arrange information. \nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(20), \n    tf.keras.layers.Dense(500),\n    tf.keras.layers.Dense(20),\n])\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(5,return_sequences=True,activation='relu'), \n    tf.keras.layers.Dropout(0.10),\n    tf.keras.layers.LSTM(50,return_sequences=True,activation='relu'),\n    tf.keras.layers.Dropout(0.10),\n    tf.keras.layers.LSTM(20),\n])\n"
'import numpy as np\n\nY = np.random.rand(3, 4)\nprint(Y)\n\nY = np.reshape(Y, (Y.shape[0], Y.shape[1], 1))\nprint(Y)\n\n[[0.94716449 0.46469876 0.74290887 0.11051443]\n [0.31187829 0.26831897 0.37580931 0.23038081]\n [0.46578756 0.81175453 0.98348175 0.02975313]]\n\n[[[0.94716449]\n  [0.46469876]\n  [0.74290887]\n  [0.11051443]]\n\n [[0.31187829]\n  [0.26831897]\n  [0.37580931]\n  [0.23038081]]\n\n [[0.46578756]\n  [0.81175453]\n  [0.98348175]\n  [0.02975313]]]\n'
"import tensorflow as tf \nimport numpy as np\n\ndef get_rand_seq():\n    return [np.random.uniform(-1, 1) for _ in range(6)]\n\nn = 1000\nX = np.array([get_rand_seq() for _ in range(n)])\ny = np.array([0 if sum(seq) &lt; 0 else 1 for seq in X])\nds = tf.data.Dataset.from_tensor_slices((X, y)).batch(4)\n\n# equivalent is \n# ds = tf.data.Dataset.from_tensor_slices((X, y))\n# ds = ds.batch(4) # not in-place\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(16, input_shape=(6, ), activation='relu'))\nmodel.add(tf.keras.layers.Dense(4, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\nmodel.fit(ds, epochs=1000)\n"
"df['Cabin_Filled'] = df['Ticket'].map(df.groupby('Ticket')['Cabin'].first())\ndf\n\n   Ticket Cabin Cabin_Filled\n0     123   NaN          C12\n1     162   B14          B14\n2     123   C12          C12\n3     122   D13          D13\n4     162   NaN          B14\n5     122   NaN          D13\n"
'batch = tf.shape(inputs)[0]\ndim = tf.shape(inputs)[1]\n'
'// draw the selected object\nrectangle(selectorParams.image, selectorParams.box, Scalar(255, 0, 0), 2, 1);\n'
'batch_dim, seq_len, img_height, img_width = 3, 17, 25, 25\nX = np.random.uniform(0,1, (batch_dim, seq_len, img_height, img_width))\ny = np.random.randint(0,6, batch_dim)\nprint(X.shape)\n\n# expand input dimension\nX = X[...,np.newaxis]\nprint(X.shape)\n\nmodel = Sequential()\nmodel.add(ConvLSTM2D(filters = 64, kernel_size = (3, 3), return_sequences = False, \n                     data_format = &quot;channels_last&quot;, \n                     input_shape = (seq_len, img_height, img_width, 1)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation=&quot;relu&quot;))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6, activation = &quot;softmax&quot;))\nmodel.summary()\n\nmodel.predict(X).shape\n'
'for layer in model.layers:\n    layer.trainable = False\n\nfor layer in model.layers[-10:]:\n    layer.trainable = False\n\nmodel.trainable_variables\n\n[]\n'
'X.replace([np.inf, -np.inf], np.nan, inplace=True)\nX = X.fillna(0)\n\nX[np.isnan(X)] = 0\n\nX[X == np.inf] = 0 \nX[X == -np.inf] = 0\n'
'import pandas as pd\n\nclass SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, nlp):\n        self.nlp = nlp\n        self.dim = 300\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return pd.DataFrame([self.nlp(text).vector for text in X])\n'
'Xnew = np.array(Xnew).reshape((1,-1))\n'
"ohe = OneHotEncoder(drop='first')\n\ndropList = ['Blue','Triangle']\nohe = OneHotEncoder(drop=dropList)\n"
'model.predict([np.expand_dims(x_1,0), np.expand_dims(x_2,0)])\n'
"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\nfrom glob2 import glob\nfrom shutil import copy\nimport numpy as np\n\nfiles = glob('group1\\\\*\\\\*.jpg')\n\nimsize = 64\n\ndef load(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, size=(imsize, imsize))\n    return img, file_path\n\nds = tf.data.Dataset.from_tensor_slices(files).\\\n    take(100).\\\n    shuffle(100).\\\n    map(load).batch(4)\n\nmodel = Sequential()\nmodel.add(Conv2D(8, (3, 3), input_shape=(imsize, imsize, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=2, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.build(input_shape=(imsize, imsize, 3))\n\ncategories = np.array(['cats', 'dogs'])\n\ntarget_dir = 'newpics'\n\nfor cat in categories:\n    os.makedirs(os.path.join(target_dir, cat), exist_ok=True)\n\nfor images, filenames in ds:\n    preds = model(images)\n    targets = categories[np.argmax(preds, axis=1)]\n    for file, destination in zip(filenames, targets):\n        copy(file.numpy().decode(), os.path.join(target_dir, destination,\n                                os.path.basename(file.numpy().decode())\n                                ))\n        print(file.numpy().decode(), '--&gt;', os.path.join(target_dir, destination,\n                                os.path.basename(file.numpy().decode())\n                                ))\n\ngroup1\\cats\\cat.4051.jpg --&gt; newpics\\cats\\cat.4051.jpg\ngroup1\\cats\\cat.4091.jpg --&gt; newpics\\dogs\\cat.4091.jpg\ngroup1\\cats\\cat.4055.jpg --&gt; newpics\\cats\\cat.4055.jpg\ngroup1\\cats\\cat.4041.jpg --&gt; newpics\\cats\\cat.4041.jpg\ngroup1\\cats\\cat.4090.jpg --&gt; newpics\\cats\\cat.4090.jpg\ngroup1\\cats\\cat.4071.jpg --&gt; newpics\\dogs\\cat.4071.jpg\ngroup1\\cats\\cat.4082.jpg --&gt; newpics\\cats\\cat.4082.jpg\ngroup1\\cats\\cat.4037.jpg --&gt; newpics\\cats\\cat.4037.jpg\ngroup1\\cats\\cat.4005.jpg --&gt; newpics\\cats\\cat.4005.jpg\n"
'inputs: A 3D tensor with shape [batch, timesteps, feature].\n\n[batch, timesteps, feature] == [4, 5, 5]\n\nlist2 = np.expand_dim(list2, axis=0)\nprint(list2)\nprint(list2.shape)\n\n[[[0 1 2 3 4]\n  [1 2 3 4 5]\n  [2 3 4 5 6]\n  [3 4 5 6 7]\n  [4 5 6 7 8]]]\n(1, 5, 5)\n'
"pipeline = Pipeline([\n    ('vect', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('clf', MultinomialNB()),\n])\n"
'import xgboost\nimport shap\n\n# train XGBoost model\nX,y = shap.datasets.adult()\nmodel = xgboost.XGBClassifier().fit(X, y)\n\n# compute SHAP values\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X)\n\nimport statsmodels.api as sm\n\nidx = np.where(X.columns==&quot;Age&quot;)[0][0]\nx = X.iloc[:,idx]\ny_sv = shap_values[:,idx]\nlowess = sm.nonparametric.lowess(y_sv, x, frac=.3)\n\n_,ax = plt.subplots()\nax.plot(*list(zip(*lowess)), color=&quot;red&quot;, )\n\nshap.dependence_plot(&quot;Age&quot;, shap_values, X, ax=ax)\n'
'import numpy as np\nfrom sklearn.linear_model import LinearRegression\n\nX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\ny = np.dot(X, np.array([1, 2])) + 3\n\nreg = LinearRegression().fit(X, y)\n\nprint(reg.coef_)\n# Output: [1. 2.]\nprint(reg.intercept_)\n# Output: 3.00\n\n# y = θo + θ1 * x_0 + θ2 * x_1 =&gt; 3.00 + 1 * x_0 + 2 * x_1\n'
"In [1]:\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom numpy.random import random\nfrom scipy.spatial.distance import euclidean\n\n# I'm going to generate some random data so you can just copy this and see it work\n\nrandom_data = []\n\nfor i in range(0,10):\n    random_data.append({'dns_query_count': random(),\n 'http_hostnames_count': random(),\n 'dest_port_count': random(),\n 'ip_count': random(),\n 'signature_count': random(),\n 'src_ip': random(),\n 'http_user_agent_count': random()}\n)\n\ndf = pd.DataFrame(random_data)\n\nkm = KMeans(n_clusters=2).fit(df)\n\ndf['cluster_id'] = km.labels_\n\n# get the cluster centers and compute the distance from each point to the center\n# this will show that all points are assigned to the correct cluster\n\ndef distance_to_centroid(row, centroid):\n    row = row[['dns_query_count',\n                'http_hostnames_count',\n                'dest_port_count',\n                'ip_count',\n                'signature_count',\n                'src_ip',\n                'http_user_agent_count']]\n    return euclidean(row, centroid)\n\n# to get the cluster centers use km.cluster_centers_\n\ndf['distance_to_center0'] = df.apply(lambda r: distance_to_centroid(r,\n    km.cluster_centers_[0]),1)\n\ndf['distance_to_center1'] = df.apply(lambda r: distance_to_centroid(r,\n    km.cluster_centers_[1]),1)\n\ndf.head()\n\nOut [1]:\n   dest_port_count  dns_query_count  http_hostnames_count  \\\n0         0.516920         0.135925              0.090209   \n1         0.528907         0.898578              0.752862   \n2         0.426108         0.604251              0.524905   \n3         0.373985         0.606492              0.503487   \n4         0.319943         0.970707              0.707207   \n\n   http_user_agent_count  ip_count  signature_count    src_ip  cluster_id  \\\n0               0.987878  0.808556         0.860859  0.642014           0   \n1               0.417033  0.130365         0.067021  0.322509           1   \n2               0.528679  0.216118         0.041491  0.522445           1   \n3               0.780292  0.130404         0.048353  0.911599           1   \n4               0.156117  0.719902         0.484865  0.752840           1   \n\n   distance_to_center0  distance_to_center1  \n0             0.846099             1.124509  \n1             1.175765             0.760310  \n2             0.970046             0.615725  \n3             1.054555             0.946233  \n4             0.640906             1.020849  \n"
"train_fea = np.array([[1,1,0],[0,0,1],[1,np.nan,0]])\ntrain_fea\narray([[  1.,   1.,   0.],\n       [  0.,   0.,   1.],\n       [  1.,  nan,   0.]])\n\n#initialise the model\nimp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n\n#train the model\nimp.fit(train_fea)\n\n#get the predictions\ntrain_fea_imputed = imp.transform(train_fea)\ntrain_fea_imputed\narray([[ 1. ,  1. ,  0. ],\n       [ 0. ,  0. ,  1. ],\n       [ 1. ,  0.5,  0. ]])\n"
'ganado={\n    "a": 50,\n    "b": 33\n}\nperdido={\n        "a": 4,\n        "b": 3\n    }\npruebaGanado={\n        "a": 10,\n        "b": 33\n    }\npruebaPerdido={\n        "a": 2,\n        "b": 3\n    }\n\nP((a,b)=(10,33)|class=0)*P(class=0)   &gt;   P((a,b)=(10,33)|class=1)*P(class=1)\n\nt = [pruebaGanado,pruebaPerdido]\nt = vec.fit_transform(t)\nprint model.predict_proba(t.toarray())\n#prints:\n[[ 1.  0.]\n[ 1.  0.]]\n\npruebaGanado={\n    "Puntuacion Final Pasteles": 20,\n    "Puntuacion Final Botellas": 33\n}\n\n[[ 0.  1.]\n[ 1.  0.]]\n'
"from sklearn.preprocessing import LabelBinarizer\n\n\n# your data\n# ===========================\ncontinent = [1, 2, 3, 2]\ncontinent_dict = {1:'is_europe', 2:'is_asia', 3:'is_america'}\nprint(continent_dict)\n\n{1: 'is_europe', 2: 'is_asia', 3: 'is_america'}\n\n# processing\n# =============================\nbinarizer = LabelBinarizer()\n# fit on the categorical feature\ncontinent_dummy = binarizer.fit_transform(continent)\nprint(continent_dummy)\n\n[[1 0 0]\n [0 1 0]\n [0 0 1]\n [0 1 0]]\n"
'import pandas as pd\n\ndf = pd.DataFrame([[1,2],[3,4],[np.NaN,6]])\n\ndf\nOut[12]: \n    0  1\n0   1  2\n1   3  4\n2 NaN  6\n\npd.isnull(df).sum()\nOut[13]: \n0    1\n1    0\ndtype: int64\n\ndf3 = pd.DataFrame([[1,2],[3,4],[np.inf,6]])\n\npd.isnull(df3).sum()\nOut[23]: \n0    0\n1    0\ndtype: int64\n\nimport numpy as np\n\nnp.isinf(df3).sum()\nOut[25]: \n0    1\n1    0\ndtype: int64\n'
'sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n\nsgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n'
'stack = [root]\nwhile stack is not empty:\n    node = stack.pop()\n    stack.push (node.right)\n    stack.push (node.left)\n    process node  # assign node number, etc.\n'
'import numpy as np\n\na = np.array([1,2,4])\nb = np.array([5,6,7])\nprint(a-b)\n# [-4 -4 -3]\n'
'from keras.layers import Flatten, Dense \nmodel.add(Flatten())\nmodel.add(Dense(13))\n'
"important_features_dict = {}\nfor x,i in enumerate(rf.feature_importances_):\n    important_features_dict[x]=i\n\n\nimportant_features_list = sorted(important_features_dict,\n                                 key=important_features_dict.get,\n                                 reverse=True)\n\nprint 'Most important features: %s' %important_features_list\n"
"x_train.shape == (batch_size, 3)\ny_train.shape == (batch_size,)\n\nself.inpt = T.dmatrix('inpt')\nself.out  = T.dvector('out')\n\ny_train = np.reshape(y_train, y_train.shape + (1,))\n# y_train.shape == (batch_size, 1)\n\nself.out = T.dmatrix('out')\n"
"model.add(Dense(150, activation='sigmoid'))\n\nimport keras.backend as K\n\ndef multiclass_loss(y_true, y_pred):\n    EPS = 1e-5\n    y_pred = K.clip(y_pred, EPS, 1 - EPS)\n    return -K.mean((1 - y_true) * K.log(1 - y_pred) + y_true * K.log(y_pred))\n\nmodel.compile(optimizer=..., loss=multiclass_loss)\n"
"import numpy\nnumpy.finfo('d')\nfinfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n"
"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn.fit(X_train, y_train)\ny_train_prob = knn.predict_proba(X_train)\ny_test_prob = knn.predict_proba(X_test)\n"
'from sklearn.datasets import load_wine\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX, y = load_wine(return_X_y=True)\nX_scaled = MinMaxScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y,\n                                                    test_size=0.3)\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\n'
"df.sort_values(['VisitNumber']).groupby('VisitorID').\\\n     agg({'TimeSpentOnVist':'sum','Channel':'last','Media type':'last'})\n"
'original_data = original_data.reshape(numberOfVideos, frames, rows, cols, channels)\n'
"def train_test_eq_split(X, y, n_per_class, random_state=None):\n    if random_state:\n        np.random.seed(random_state)\n    sampled = X.groupby(y, sort=False).apply(\n        lambda frame: frame.sample(n_per_class))\n    mask = sampled.index.get_level_values(1)\n\n    X_train = X.drop(mask)\n    X_test = X.loc[mask]\n    y_train = y.drop(mask)\n    y_test = y.loc[mask]\n\n    return X_train, X_test, y_train, y_test\n\ndata = pd.DataFrame({'classes': np.repeat([1, 2, 3], [10, 20, 30]),\n                     'b': np.random.randn(60),\n                     'c': np.random.randn(60)})\ny = data.pop('classes')\n\nX_train, X_test, y_train, y_test = train_test_eq_split(\n    data, y, n_per_class=5, random_state=123)\n\ny_test.value_counts()\n# 3    5\n# 2    5\n# 1    5\n# Name: classes, dtype: int64\n"
'ranking\n# Output: array([1, 1, 3, 2])\n'
"predictions = model.predict({'data_55': image} , useCPUOnly = True) # Run the prediction\n\nmap_final = predictions['fc8_seg'][0,0,:,:] # fc8_seg is the output of the neural network\nmap_final = map_final.reshape((50,50)) # Reshape the output from shape (2500) to (50, 50)\nmap_final = numpy.flip(map_final, 1) # Flip axis 1 to unmirror the image\n\n# Scale the values in the array to a range between 0 and 255\nmap_final -= map_final.min() \nmap_final /= map_final.max()\nmap_final = numpy.ceil(map_final*255)\n\nmap_final_unint8 = map_final.astype(numpy.uint8) # Convert the data type to an uint8\npil_image = Image.fromarray(map_final_unint8, mode = 'L') # Create the PIL image\n"
'(max_indices == labels).sum()\n(max_indices != labels).sum()\n\n(max_indices != labels).int().sum()\n'
'import keras.backend as K\n\ndef rmsle(y_test, y_pred):\n    return K.sqrt(K.mean(K.square(K.log(1 + y_pred) - K.log(1 + y_test))))\n'
"from sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.ensemble import RandomForestClassifier  \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\ncount_vect = CountVectorizer()\n\ntext = ['text1', ..] \n\ntargets = ['abuse', ...]\n\nmatrix = count_vect.fit_transform(text)\n\nencoder = LabelEncoder()\ntargets = encoder.fit_transform(targets)\n\nrandomForest = RandomForestClassifier()\n\nrandomForest.fit(matrix, targets)\n"
'import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot=True)\n\n\n# &gt;&gt;&gt;&gt; Config. Vars &lt;&lt;&lt;&lt;\n\nTRAIN_STEPS = 1000\n\nSAVE_EVERY  = 100\n\n\n# &gt;&gt;&gt;&gt; Network &lt;&lt;&lt;&lt;\n\ninputs = tf.placeholder(tf.float32, shape=[None, 784])\n\nlabels = tf.placeholder(tf.float32, shape=[None, 10])\n\nh1     = tf.layers.dense(inputs, 256, activation=tf.nn.relu, use_bias=True)\n\nlogits = tf.layers.dense(h1, 10, use_bias=True)\n\npredictions = tf.nn.softmax(logits)\n\nprediction_ids = tf.argmax(predictions, axis=1)\n\n# &gt;&gt;&gt;&gt; Loss &amp; Optimisation &lt;&lt;&lt;&lt;\n\nloss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n\nopt  = tf.train.AdamOptimizer().minimize(loss)\n\n# &gt;&gt;&gt;&gt; Utilities &lt;&lt;&lt;&lt;\n\ninit  = tf.global_variables_initializer()\n\nsaver = tf.train.Saver()\n\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n\n    # &gt;&gt;&gt;&gt; Training - run on remote, comment out locally &lt;&lt;&lt;&lt;\n\n    for i in range(TRAIN_STEPS):\n\n        print("Train step {}".format(i), end="\\r")\n\n        batch_data, batch_labels = mnist.train.next_batch(batch_size=128)\n\n        feed_dict = {\n            inputs: batch_data,\n            labels: batch_labels\n        }\n\n        l, _ = sess.run([loss, opt], feed_dict=feed_dict)\n\n        if i % SAVE_EVERY == 0:\n\n            saver.save(sess, "saved_model/network_weights.ckpt")\n\n\n    # &gt;&gt;&gt;&gt; Using the network - run locally to use the network &lt;&lt;&lt;\n\n    saver.restore(sess, "saved_model/network_weights.ckpt")\n\n    test_data, test_labels = mnist.test.images, mnist.test.labels\n\n    feed_dict = {\n        inputs: test_data,\n        labels: test_labels\n    }\n\n    preds = sess.run(prediction_ids, feed_dict=feed_dict)\n\n    print(preds)\n'
'assert len(set(comprehensive_ngrams)) == len(comprehensive_ngrams)\n'
'def my_cropping(a):\n    cropping_list = []\n    n_patches = 256/32\n    for x in range(256//32):\n        for y in  range(256//32):\n\n            cropping_list += [\n             K.expand_dims(\n                Cropping2D((( x * 32,  256 - (x+1) * 32), ( y * 32,  256 - (y+1) * 32)))(a)\n                , axis=1)\n            ]\n    return cropping_list\n\ncropping_list = Lambda(my_cropping)(a)\n'
'l1_decoder_outputs = l1_decoder_gru(decoder_inputs)\n'
'input = tf.Placeholder(dtype=tf.string, shape=[None])\n...\n'
'import cv2\nimport numpy as np\n\nimage=cv2.imread("/home/pi/Downloads/test.jpg")\nface_cascade=cv2.CascadeClassifier("/home/pi/opencv-3.4.0/data/haarcascades/haarcascade_frontalface_alt.xml")\nprofil_cascade=cv2.CascadeClassifier("/home/pi/opencv-3.4.0/data/haarcascades/haarcascade_profileface_alt.xml")\n\ngray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\nface=face_cascade.detectMultiScale(gray, 1.06, 5)\nprofil=profil_cascade.detectMultiScale(gray, 1.1, 5)\n\ncombined_array=np.append(face, profil, axis=0)\ncombined_list=combined_array.tolist()\nresult=cv2.groupRectangles(combined_list,1,0.85)\n\nprint("I\'ve found "+str(len(combined_list)-str(len(result[1]))+ " face(s)")\n\nfor (x,y,w,h) in result[0]:\n    cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,255),2)\n\ncv2.imwrite("/home/pi/Download/result.jpg", image)\n'
'array = np.random.random((10**5, 3))*10\ntree = KDTree(array)\n\nneighbors = tree.query_radius(array, 1)\ncounts = tree.query_radius(array, 1, count_only=1)\n\nneighbors = tree.query_radius(array, 1)\ncounts = []\nfor i in range(len(neighbors)):\n    counts.append(len(neighbors[i]))\n\nneighbors = tree.query_radius(array, 1)\nlen_array = np.frompyfunc(len, 1, 1)\ncounts = len_array(neighbors)\n'
"# Logits1 trained with the base model\nwith tf.variable_scope('logits1', reuse=False):\n    #logits1 = tf.contrib.layers.fully_connected(features1, 10, tf.nn.relu)\n    logits1 = tf.contrib.layers.fully_connected(features1, 10, None)\n\n# Logits2 trained while the base model is frozen\nwith tf.variable_scope('logits2', reuse=False):\n     #logits2 = tf.contrib.layers.fully_connected(features2, 10, tf.nn.relu)\n     logits2 = tf.contrib.layers.fully_connected(features2, 10, None)\n\nInitial Model Accuracy After training final model: 0.9805\nFinal Model Accuracy After Training: 0.9658\n"
'from metal.label_model import LabelModel\ngen_model = LabelModel()\n%time gen_model.train_model(L_train[0], n_epochs=500, print_every=100)\n'
'dataset = [\n   {"text": "I don\'t like this large device", "rating": "2"},\n   {"text": "Really love this small device", "rating": "5"},\n   {"text": "Some other text", "rating": "3"}\n]\n\ndf = pd.DataFrame(dataset)\ndf[\'rating\'] = df[\'rating\'].astype(int)\ndf[\'text\'] = df[\'text\'].str.split().apply(set)\nx1 = [\'short\', \'slim\', \'small\', \'shrink\']\nx2 = [\'big\', \'huge\', \'large\']\ndf[\'x1\'] =  df.text.apply(lambda x: x.intersection(x1)).astype(bool)\ndf[\'x2\'] =  df.text.apply(lambda x: x.intersection(x2)).astype(bool)\n\n   rating                                   text     x1     x2\n0       2  {this, large, don\'t, like, device, I}  False   True\n1       5    {this, small, love, Really, device}   True  False\n2       3                    {other, Some, text}  False  False\n\nmodel = LinearRegression()\nmodel.fit(df[[\'x1\', \'x2\']], df.rating)\nprint(model.coef_)  # array([ 2., -1.])\nprint(model.intercept_)  # 3.0\n'
"## Second LSTM Layer\nX = LSTM(10, return_sequences=True)(X)\n\nd = 5  # how many days in the future you want to predict?\n\n## First LSTM Layer\nX = LSTM(10, input_shape = (10,14), name = 'LSTM_1')(X_input);\n\nX = RepeatVector(d)(X)\n\n## Second LSTM Layer\nX = LSTM(10, return_sequences=True)(X)\n"
"import keras\n\nimport numpy as np\n\n\n\nX1_Train = np.ones((500,400))\nX2_Train = np.ones((500,1500))\ny_train = np.ones((500))\nprint(X1_Train.shape)\nprint(X2_Train.shape)\nprint(y_train.shape)\n\nnum_class = 1\n\n\ntimesteps = 50 * 8\nn = 50 * 30\n\ndef createClassifier():\n    sequence = keras.layers.Input(shape=(timesteps, 1), name='Sequence')\n    features = keras.layers.Input(shape=(n,), name='Features')\n\n    conv = keras.Sequential()\n    conv.add(keras.layers.Conv1D(10, 5, activation='relu', input_shape=(timesteps, 1)))\n    conv.add(keras.layers.Conv1D(10, 5, activation='relu'))\n    conv.add(keras.layers.MaxPool1D(2))\n    conv.add(keras.layers.Dropout(0.5))\n\n    conv.add(keras.layers.Conv1D(5, 6, activation='relu'))\n    conv.add(keras.layers.Conv1D(5, 6, activation='relu'))\n    conv.add(keras.layers.MaxPool1D(2))\n    conv.add(keras.layers.Dropout(0.5))\n    conv.add(keras.layers.Flatten())\n    part1 = conv(sequence)\n\n    merged = keras.layers.concatenate([part1, features])\n\n    final = keras.layers.Dense(512, activation='relu')(merged)\n    final = keras.layers.Dropout(0.5)(final)\n    final = keras.layers.Dense(num_class, activation='softmax')(final)\n\n    model = keras.Model(inputs=[sequence, features], outputs=[final])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = createClassifier()\n# print(model.summary())\nX1_Train = X1_Train.reshape((500,400,1))\nhistory = model.fit([X1_Train, X2_Train], y_train, epochs =5)\n\nUsing TensorFlow backend.\n(500, 400)\n(500, 1500)\n(500,)\nEpoch 1/5\n500/500 [==============================] - 1s 3ms/step - loss: 1.1921e-07 - acc: 1.0000\nEpoch 2/5\n500/500 [==============================] - 0s 160us/step - loss: 1.1921e-07 - acc: 1.0000\nEpoch 3/5\n500/500 [==============================] - 0s 166us/step - loss: 1.1921e-07 - acc: 1.0000\nEpoch 4/5\n500/500 [==============================] - 0s 154us/step - loss: 1.1921e-07 - acc: 1.0000\nEpoch 5/5\n500/500 [==============================] - 0s 157us/step - loss: 1.1921e-07 - acc: 1.0000\n"
"file = open(filename,'r', errors = 'ignore', encoding='utf8')\n"
'some_list = [[1,3,4,2],[2,2,3,1],[2,1,2,3]]\nprint(list(zip(*some_list)))\n\n[(1, 2, 2), (3, 2, 1), (4, 3, 2), (2, 1, 3)]\n'
'from ast import literal_eval\nimport numpy as np\nimport pandas as pd\n\n# the raw data\n\nd = \'\'\'datetime |  mood |  activities |  notes\n\n8/27/2017 |  "good" | ["friends", "party", "gaming"] | NaN\n\n8/28/2017 |  "meh" |  ["work", "friends", "good food"] | "Stuff stuff"\n\n8/29/2017 |  "bad" |  ["work", "travel"] |  "Fell off my bike"\'\'\'\n\n# parse the raw data\ndf = pd.read_csv(pd.compat.StringIO(d), sep=\'\\s*\\|\\s*\', engine=\'python\')\n\n# parse the lists of activities (which are still strings)\nacts = df[\'activities\'].apply(literal_eval)\n\n# get the unique activities\nactcols = np.unique([a for al in acts for a in al])\n\n# assemble the desired one hot array from the activities\nactarr = np.array([np.in1d(actcols, al) for al in acts])\nactdf = pd.DataFrame(actarr, columns=actcols)\n\n# stick the dataframe with the one hot array onto the main dataframe\ndf = pd.concat([df.drop(columns=\'activities\'), actdf], axis=1)\n\n# fancy print\nwith pd.option_context("display.max_columns", 20, \'display.width\', 9999):\n    print(df)\n\n    datetime    mood               notes  friends  gaming  good food  party  travel   work\n0  8/27/2017  "good"                 NaN     True    True      False   True   False  False\n1  8/28/2017   "meh"       "Stuff stuff"     True   False       True  False   False   True\n2  8/29/2017   "bad"  "Fell off my bike"    False   False      False  False    True   True\n'
"split_idx = int(len(x) * image_data_generator._validation_split)\n\n# ...\nif subset == 'validation':\n    x = x[:split_idx]\n    x_misc = [np.asarray(xx[:split_idx]) for xx in x_misc]\n    if y is not None:\n        y = y[:split_idx]\nelse:\n    x = x[split_idx:]\n    x_misc = [np.asarray(xx[split_idx:]) for xx in x_misc]\n    if y is not None:\n        y = y[split_idx:]\n\nif not np.array_equal(\n        np.unique(y[:split_idx]),\n        np.unique(y[split_idx:])):\n    raise ValueError('Training and validation subsets '\n                     'have different number of classes after '\n                     'the split. If your numpy arrays are '\n                     'sorted by the label, you might want '\n                     'to shuffle them.')\n\nfrom sklearn.model_selection import train_test_split\n\nval_split = 0.25\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_split, stratify=y)\n\nX = np.concatenate((X_train, X_val))\ny = np.concatenate((y_train, y_val))\n"
"layer_outputs = [layer.output for layer in w_extraction.layers[:102] if not layer.name.startswith('input')]\n"
'p* = argmax p_i p_i(sentence)\n'
'Image 0 is 7\nImage 1 is 2\nImage 2 is 1\nImage 3 is 0\nImage 4 is 4\n...\n'
'In [9]: df = pd.DataFrame({\'feature20\':pd.date_range(\'2010-01-01\', periods=10)})\n\nIn [10]: df["new"] = df["feature20"].astype("int64") // 10**9\n\nIn [11]: df\nOut[11]:\n   feature20         new\n0 2010-01-01  1262304000\n1 2010-01-02  1262390400\n2 2010-01-03  1262476800\n3 2010-01-04  1262563200\n4 2010-01-05  1262649600\n5 2010-01-06  1262736000\n6 2010-01-07  1262822400\n7 2010-01-08  1262908800\n8 2010-01-09  1262995200\n9 2010-01-10  1263081600\n\nIn [12]: df["date"] = pd.to_datetime(df["new"], unit="s")\n\nIn [13]: df\nOut[13]:\n   feature20         new       date\n0 2010-01-01  1262304000 2010-01-01\n1 2010-01-02  1262390400 2010-01-02\n2 2010-01-03  1262476800 2010-01-03\n3 2010-01-04  1262563200 2010-01-04\n4 2010-01-05  1262649600 2010-01-05\n5 2010-01-06  1262736000 2010-01-06\n6 2010-01-07  1262822400 2010-01-07\n7 2010-01-08  1262908800 2010-01-08\n8 2010-01-09  1262995200 2010-01-09\n9 2010-01-10  1263081600 2010-01-10\n\nIn [28]: df = pd.DataFrame({\'feature20\':pd.date_range(\'2010-01-01 01:01:01.123456\', freq="123S", periods=10)})\n\nIn [29]: df\nOut[29]:\n                   feature20\n0 2010-01-01 01:01:01.123456\n1 2010-01-01 01:03:04.123456\n2 2010-01-01 01:05:07.123456\n3 2010-01-01 01:07:10.123456\n4 2010-01-01 01:09:13.123456\n5 2010-01-01 01:11:16.123456\n6 2010-01-01 01:13:19.123456\n7 2010-01-01 01:15:22.123456\n8 2010-01-01 01:17:25.123456\n9 2010-01-01 01:19:28.123456\n\nIn [30]: df["new"] = df["feature20"].astype("int64") // 10**3\n\nIn [31]: df\nOut[31]:\n                   feature20               new\n0 2010-01-01 01:01:01.123456  1262307661123456\n1 2010-01-01 01:03:04.123456  1262307784123456\n2 2010-01-01 01:05:07.123456  1262307907123456\n3 2010-01-01 01:07:10.123456  1262308030123456\n4 2010-01-01 01:09:13.123456  1262308153123456\n5 2010-01-01 01:11:16.123456  1262308276123456\n6 2010-01-01 01:13:19.123456  1262308399123456\n7 2010-01-01 01:15:22.123456  1262308522123456\n8 2010-01-01 01:17:25.123456  1262308645123456\n9 2010-01-01 01:19:28.123456  1262308768123456\n\nIn [32]: df["date"] = pd.to_datetime(df["new"], unit="us")\n\nIn [33]: df\nOut[33]:\n                   feature20               new                       date\n0 2010-01-01 01:01:01.123456  1262307661123456 2010-01-01 01:01:01.123456\n1 2010-01-01 01:03:04.123456  1262307784123456 2010-01-01 01:03:04.123456\n2 2010-01-01 01:05:07.123456  1262307907123456 2010-01-01 01:05:07.123456\n3 2010-01-01 01:07:10.123456  1262308030123456 2010-01-01 01:07:10.123456\n4 2010-01-01 01:09:13.123456  1262308153123456 2010-01-01 01:09:13.123456\n5 2010-01-01 01:11:16.123456  1262308276123456 2010-01-01 01:11:16.123456\n6 2010-01-01 01:13:19.123456  1262308399123456 2010-01-01 01:13:19.123456\n7 2010-01-01 01:15:22.123456  1262308522123456 2010-01-01 01:15:22.123456\n8 2010-01-01 01:17:25.123456  1262308645123456 2010-01-01 01:17:25.123456\n9 2010-01-01 01:19:28.123456  1262308768123456 2010-01-01 01:19:28.123456\n'
'print(grid_search.cv_results_)\n\nprint(grid_search.best_estimator_)\n\nprint(grid_search.best_params_)\n'
'ys = (-theta[0] - theta[1] * xs) / theta[2]\ntheta[2] * ys = (-theta[0] - theta[1] * xs)\n\n theta[0] + theta[1] * xs + theta[2] * ys\n'
'model = lambda b, X: b[3] + b[0] * X[:,0] + b[1] * X[:,1] + b[2] * X[:,2] \n\nmodel2.intercept_ = b[3]\n'
'import keras.backend as K \ndef my_top_k(true, pred, num):\n    true = K.reshape(true, (-1, features_num))   \n    pred = K.reshape(pred, (-1, features_num))\n    return top_k_categorical_accuracy(true, pred, k=num)\n'
"if(selected_list[i] == 'True'):\n    columns_list.append(test[i])\n\n if selected_list[i]:\n    columns_list.append(test[i])\n\ntest.iloc[0] # first row of data frame- Note a Series data type output.\ntest.iloc[1] # second row of data frame \ntest.iloc[-1] # last row of data frame \n# Columns:\ntest.iloc[:,0] # first column of data frame \ntest.iloc[:,1] # second column of data frame \ntest.iloc[:,-1] # last column of data frame\n\ncolumns_selected = test.iloc[:, [i for i in range(len(selected_list)) if selected_list[i]]]\n"
'(?&lt;!\\S)(?!\\S*theo\\S*)\\S*the\\S*\n\n\\b(?!\\w*theo\\w*)\\w*the\\w*\\b\n\n\\b(?=\\S*the\\S*)[^t\\s]*(?:t(?!heo)[^t\\s]*)+\\b\n'
'def MSE(y_prediction, y_true, deriv=(False, 1)):\n    if deriv[0]:\n        return 2 * np.mean(np.subtract(y_prediction, y_true) * deriv[1])\n    return np.mean(np.square(np.subtract(y_true, y_prediction)))\n\ndef MSE(y_prediction, y_true, deriv=None):\n    if deriv is not None:\n        return 2 * np.mean((y_prediction - y_true)*deriv)\n    return np.mean((y_prediction - y_true)**2)\n\ndelta_theta_0 = MSE(predictions, y, deriv=1)\ndelta_theta_1 = MSE(predictions, y, deriv=x)\n'
"df['gender'] = df['gender'].map({\n      'mostly_female': 'female', \n      'mostly_male': 'male', \n      'andy': 'male',\n      'unknown': np.random.choice(['female', 'male'], size=1)\n})\n"
'dataset = pd.DataFrame({\'Consignor Code\':["6402106844","6402106844","6402106844","6402107662","6402107662","6402107662","6408507648"],\n                   \'Consignee Code\': ["66903717","66903717","6404814143","66974631","6404518090","6404518090","6403601344"],\n                   \'Origin\':["DKCPH","DKCPH","DKCPH","DKCPH","DKCPH","DKBLL","DKCPH"],\n                   \'Destination\':["CNPVG","CNPVG","CNPVG","VNSGN","THBKK","THBKK","USTPA"],\n                   \'Carrier Code\':["6402746387","6402746387","6402746387","6402746393","6402746393","6402746393","66565231"]})\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.externals import joblib\nfrom sklearn import preprocessing\nimport pandas as pd\n\n#Import the dataset (A CSV file)\n#Drop any rows containing NaN values\ndataset.dropna(subset=[\'Consignor Code\', \'Consignee Code\',\n                       \'Origin\', \'Destination\', \'Carrier Code\'], inplace=True)\n\n\n#Define our target (What we want to be able to predict)\ntarget = dataset.pop(\'Destination\')\n\n#Convert all our data to numeric values, so we can use the .fit function.\n#For that, we use LabelEncoder\nle_origin = preprocessing.LabelEncoder()\nle_consignor = preprocessing.LabelEncoder()\nle_consignee = preprocessing.LabelEncoder()\nle_carrier = preprocessing.LabelEncoder()\nle_target = preprocessing.LabelEncoder()\ntarget = le_target.fit_transform(list(target))\ndataset[\'Origin\'] = le_origin.fit_transform(list(dataset[\'Origin\']))\ndataset[\'Consignor Code\'] = le_consignor.fit_transform(list(dataset[\'Consignor Code\']))\ndataset[\'Consignee Code\'] = le_consignee.fit_transform(list(dataset[\'Consignee Code\']))\ndataset[\'Carrier Code\'] = le_carrier.fit_transform(list(dataset[\'Carrier Code\']))\n\n#Prepare the dataset.\nX_train, X_test, y_train, y_test = train_test_split(\n    dataset, target, test_size=0.3, random_state=42)\n\n\n#Prepare the model and .fit it.\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n#Make a prediction on the test set.\npredictions = model.predict(X_test)\n\n#Print the accuracy score.\nprint("Accuracy score: {}".format(accuracy_score(y_test, predictions)))\n\nnew_input = ["6408507648","6403601344","DKCPH","66565231"]\nfitted_new_input = np.array([le_consignor.transform([new_input[0]])[0],\n                                le_consignee.transform([new_input[1]])[0],\n                                le_origin.transform([new_input[2]])[0],\n                                le_carrier.transform([new_input[3]])[0]])\nnew_predictions = model.predict(fitted_new_input.reshape(1,-1))\n\nprint(le_target.inverse_transform(new_predictions))\n\n[\'THBKK\']\n'
'def trainPipeline(pipeline, X, y):\n    X_transformed = X\n    for name, step in pipeline.steps[:-1]:\n        X_transformed = step.fit_transform(X_transformed, y)\n    pipeline.steps[-1][1].fit(X_transformed, y)\n'
'Conv2D(filters=6, kernel_size=5, stride=2)\n'
'a = [\'Self employed\', \'Government Dependent\',\n       \'Formally employed Private\', \'Informally employed\',\n       \'Formally employed Government\', \'Farming and Fishing\',\n       \'Remittance Dependent\', \'Other Income\',\n       \'Dont Know/Refuse to answer\', \'No Income\']\n\nTRAIN = pd.DataFrame({\'job_type\':a})\n\n#add another groups to dict\nd = {0: [\'Government Dependent\',\'Formally employed Government\',\'Formally employed Private\'],\n     1: [\'Remittance Dependent\', \'Informally employed\'],\n     2: ["Don\'t Know/Refuse to answer", \'No Income\']}\n\n#swap key values in dict\n#http://stackoverflow.com/a/31674731/2901002\nd1 = {k: oldk for oldk, oldv in d.items() for k in oldv}\nTRAIN[\'new\'] = TRAIN[\'job_type\'].map(d1)\nprint (TRAIN)\n                       job_type  new\n0                 Self employed  NaN\n1          Government Dependent  0.0\n2     Formally employed Private  0.0\n3           Informally employed  1.0\n4  Formally employed Government  0.0\n5           Farming and Fishing  NaN\n6          Remittance Dependent  1.0\n7                  Other Income  NaN\n8    Dont Know/Refuse to answer  NaN\n9                     No Income  2.0\n\nm1 = TRAIN[\'job_type\'].isin([\'Government Dependent\',\'Formally employed Government\',\'Formally employed Private\'])\nm2 = TRAIN[\'job_type\'].isin([\'Remittance Dependent\', \'Informally employed\'])\nm3 = TRAIN[\'job_type\'].isin(["Don\'t Know/Refuse to answer", \'No Income\'])\nTRAIN[\'new\'] = np.select([m1, m2, m3], [0, 1, 2], np.nan)\n'
'def weighted_masked_objective(fn):\n    """Adds support for masking and sample-weighting to an objective function.\n    It transforms an objective function `fn(y_true, y_pred)`\n    into a sample-weighted, cost-masked objective function\n    `fn(y_true, y_pred, weights, mask)`.\n    # Arguments\n        fn: The objective function to wrap,\n            with signature `fn(y_true, y_pred)`.\n    # Returns\n        A function with signature `fn(y_true, y_pred, weights, mask)`.\n    """\n    if fn is None:\n        return None\n\n    def weighted(y_true, y_pred, weights, mask=None):\n        """Wrapper function.\n        # Arguments\n            y_true: `y_true` argument of `fn`.\n            y_pred: `y_pred` argument of `fn`.\n            weights: Weights tensor.\n            mask: Mask tensor.\n        # Returns\n            Scalar tensor.\n        """\n        # score_array has ndim &gt;= 2\n        score_array = fn(y_true, y_pred)\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in Theano\n            mask = K.cast(mask, K.floatx())\n            # mask should have the same shape as score_array\n            score_array *= mask\n            #  the loss per batch should be proportional\n            #  to the number of unmasked samples.\n            score_array /= K.mean(mask) + K.epsilon()\n\n        # apply sample weighting\n        if weights is not None:\n            # reduce score_array to same ndim as weight array\n            ndim = K.ndim(score_array)\n            weight_ndim = K.ndim(weights)\n            score_array = K.mean(score_array,\n                                 axis=list(range(weight_ndim, ndim)))\n            score_array *= weights\n            score_array /= K.mean(K.cast(K.not_equal(weights, 0), K.floatx()))\n        return K.mean(score_array)\n    return weighted\n'
"keras.layers.Dense(128, activation='relu')\n"
'scaler=MinMaxScaler(feature_range=(0,1))\nx_train = scaler.fit_transform(x_train) # you missed this line\n# rest of your code for training Neural network with x_train\n...\n# now convert the result\nscaler.inverse_transform(result)\n'
"generator = datagen.flow_from_directory(directory,\n                                        target_size=(img_width,img_height),\n                                        batch_size = batch_size,\n                                        class_mode='categorical')\n"
'from sklearn.tree import DecisionTreeRegressor\n\nfeatures = np.random.random((6, 5))\ntargets = np.random.random((6, 3))\n\nrfr = DecisionTreeRegressor(random_state=42)\nrfr.fit(features, targets)\n\nfeatures2 = np.random.random((6, 5))\npreds = rfr.predict(features2)\n\nprint(preds)\narray([[0.0017143 , 0.05348525, 0.60877828],  #0\n       [0.05232433, 0.37249988, 0.27844562],  #1\n       [0.08177551, 0.39454957, 0.28182183],\n       [0.05232433, 0.37249988, 0.27844562],\n       [0.08177551, 0.39454957, 0.28182183],\n       [0.80068346, 0.577799  , 0.66706668]])\n\nprint(targets)\narray([[0.80068346, 0.577799  , 0.66706668],\n       [0.0017143 , 0.05348525, 0.60877828],  #0\n       [0.08177551, 0.39454957, 0.28182183],\n       [0.75093787, 0.29467892, 0.11253746],\n       [0.87035059, 0.32162589, 0.57288903],\n       [0.05232433, 0.37249988, 0.27844562]]) #1\n\npreds_sum = np.unique(preds.sum(1))\ntargets_sum = np.unique(targets.sum(1))\nlen(np.intersect1d(targets_sum, preds_sum)) == len(features)\n#\xa0True\n'
'conv1 = Conv2D(2, 3, activation=\'relu\')\nconv2 = Conv2D(2, 3, activation=\'relu\')\npooling = MaxPooling2D(pool_size=(2, 2), strides=None, padding="valid", data_format=None,)\nflatten = Flatten(data_format=None)\n\nboard_inputs = Input(shape=(8, 8, 12))\nx = conv1(board_inputs)\nx = pooling(x)\nx = conv2(x)\nx = flatten(x)\noutput = Dense(12)(x)\nmodel = Model(inputs=board_inputs, outputs=output, name="chess_ai_v3")\nmodel.summary()\n'
"{'min_samples_split': 322}\nDecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=322,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=42, splitter='best')\n\nimport pandas as pd\ncv_results = pd.DataFrame.from_dict(gs.cv_results_)\n\ndf = cv_results[['params', 'mean_test_PRECISION', 'rank_test_PRECISION', 'mean_test_F1', 'rank_test_F1']]\n\npd.set_option(&quot;display.max_rows&quot;, None, &quot;display.max_columns&quot;, None)\npd.set_option('expand_frame_repr', False)\nprint(df)\n\n                        params  mean_test_PRECISION  rank_test_PRECISION  mean_test_F1  rank_test_F1\n0     {'min_samples_split': 2}             0.771782                    1      0.763041            41\n1    {'min_samples_split': 12}             0.768040                    2      0.767331            38\n2    {'min_samples_split': 22}             0.767196                    3      0.776677            29\n3    {'min_samples_split': 32}             0.760282                    4      0.773634            32\n4    {'min_samples_split': 42}             0.754572                    8      0.777967            26\n5    {'min_samples_split': 52}             0.754034                    9      0.777550            27\n6    {'min_samples_split': 62}             0.758131                    5      0.773348            33\n7    {'min_samples_split': 72}             0.756021                    6      0.774301            30\n8    {'min_samples_split': 82}             0.755612                    7      0.768065            37\n9    {'min_samples_split': 92}             0.750527                   10      0.771023            34\n10  {'min_samples_split': 102}             0.741016                   11      0.769896            35\n11  {'min_samples_split': 112}             0.740965                   12      0.765353            39\n12  {'min_samples_split': 122}             0.731790                   13      0.763620            40\n13  {'min_samples_split': 132}             0.723085                   14      0.768605            36\n14  {'min_samples_split': 142}             0.713345                   15      0.774117            31\n15  {'min_samples_split': 152}             0.712958                   16      0.776721            28\n16  {'min_samples_split': 162}             0.709804                   17      0.778287            24\n17  {'min_samples_split': 172}             0.707080                   18      0.778528            22\n18  {'min_samples_split': 182}             0.702621                   19      0.778516            23\n19  {'min_samples_split': 192}             0.697630                   20      0.778103            25\n20  {'min_samples_split': 202}             0.693011                   21      0.781047            10\n21  {'min_samples_split': 212}             0.693011                   21      0.781047            10\n22  {'min_samples_split': 222}             0.693011                   21      0.781047            10\n23  {'min_samples_split': 232}             0.692810                   24      0.779705            13\n24  {'min_samples_split': 242}             0.692810                   24      0.779705            13\n25  {'min_samples_split': 252}             0.692810                   24      0.779705            13\n26  {'min_samples_split': 262}             0.692810                   24      0.779705            13\n27  {'min_samples_split': 272}             0.692810                   24      0.779705            13\n28  {'min_samples_split': 282}             0.692810                   24      0.779705            13\n29  {'min_samples_split': 292}             0.692810                   24      0.779705            13\n30  {'min_samples_split': 302}             0.692810                   24      0.779705            13\n31  {'min_samples_split': 312}             0.692810                   24      0.779705            13\n32  {'min_samples_split': 322}             0.688417                   33      0.782772             1\n33  {'min_samples_split': 332}             0.688417                   33      0.782772             1\n34  {'min_samples_split': 342}             0.688417                   33      0.782772             1\n35  {'min_samples_split': 352}             0.688417                   33      0.782772             1\n36  {'min_samples_split': 362}             0.688417                   33      0.782772             1\n37  {'min_samples_split': 372}             0.688417                   33      0.782772             1\n38  {'min_samples_split': 382}             0.688417                   33      0.782772             1\n39  {'min_samples_split': 392}             0.688417                   33      0.782772             1\n40  {'min_samples_split': 402}             0.688417                   33      0.782772             1\n\nprint(df.loc[df['rank_test_PRECISION']==1]) # best precision\n# result:\n                     params  mean_test_PRECISION  rank_test_PRECISION  mean_test_F1  rank_test_F1\n0  {'min_samples_split': 2}             0.771782                    1      0.763041            41\n\nprint(df.loc[df['rank_test_F1']==1]) # best F1\n# result:\n                        params  mean_test_PRECISION  rank_test_PRECISION  mean_test_F1  rank_test_F1\n32  {'min_samples_split': 322}             0.688417                   33      0.782772             1\n33  {'min_samples_split': 332}             0.688417                   33      0.782772             1\n34  {'min_samples_split': 342}             0.688417                   33      0.782772             1\n35  {'min_samples_split': 352}             0.688417                   33      0.782772             1\n36  {'min_samples_split': 362}             0.688417                   33      0.782772             1\n37  {'min_samples_split': 372}             0.688417                   33      0.782772             1\n38  {'min_samples_split': 382}             0.688417                   33      0.782772             1\n39  {'min_samples_split': 392}             0.688417                   33      0.782772             1\n40  {'min_samples_split': 402}             0.688417                   33      0.782772             1\n"
'loss = F.nll_loss(output, targets)\n'
'plt.plot(event_data.created, event_data.tickets_sold_sum)\n\nevent_data.plot(x=\'created\', y=\'tickets_sold_sum\')\n\ndef load_event_data():\n    df = pd.read_csv(\'created.csv\', index_col=\'created\',parse_dates=[\'created\'])\n    return df\n\nevent_data = load_event_data()\n\nprint (event_data.index)\nDatetimeIndex([\'2019-05-22 18:01:38.425533+00:00\',\n               \'2019-05-22 18:02:17.867726+00:00\',\n               \'2019-05-22 18:02:32.441820+00:00\',\n               \'2019-05-22 18:03:07.093599+00:00\',\n               \'2019-05-22 18:03:22.857492+00:00\',\n               \'2019-05-22 18:04:07.453356+00:00\',\n               \'2019-05-22 18:04:24.382271+00:00\',\n               \'2019-05-22 18:04:34.670751+00:00\',\n               \'2019-05-22 18:05:04.781586+00:00\',\n               \'2019-05-22 18:05:28.475102+00:00\',\n               \'2019-05-22 18:05:41.469483+00:00\',\n               \'2019-05-22 18:06:04.184309+00:00\',\n               \'2019-05-22 18:06:07.344332+00:00\',\n               \'2019-05-22 18:06:21.596053+00:00\',\n               \'2019-05-22 18:06:29.980078+00:00\',\n               \'2019-05-22 18:06:36.331180+00:00\',\n               \'2019-05-22 18:06:46.557717+00:00\',\n               \'2019-05-22 18:06:50.681479+00:00\',\n               \'2019-05-22 18:07:07.288164+00:00\',\n               \'2019-05-22 18:07:12.296925+00:00\',\n               \'2019-05-22 18:07:42.836565+00:00\',\n               \'2019-05-22 18:07:56.903366+00:00\',\n               \'2019-05-22 18:09:03.798696+00:00\',\n               \'2019-05-22 18:09:20.485152+00:00\',\n               \'2019-05-22 18:10:22.913068+00:00\',\n               \'2019-05-22 18:10:30.922313+00:00\',\n               \'2019-05-22 18:11:36.149465+00:00\',\n               \'2019-05-22 18:11:45.239620+00:00\',\n               \'2019-05-22 18:11:48.826544+00:00\'],\n              dtype=\'datetime64[ns, UTC]\', name=\'created\', freq=None)\n\nprint("The defined index is", event_data.index.name)\n\n# Visualize data\nplt.figure(figsize=(15, 6))\nplt.plot(event_data.index, event_data.tickets_sold_sum)\nplt.xlabel("Date")\nplt.ylabel("Rentals")\n'
'def create_sampled_data_set(n_samples_by_bin=1000,\n                            n_bins=10,\n                            replace=True,\n                            save_csv=True):\n    """In order to have the same distribution for S1..S3 between training\n    set and test set, this function will generate a new\n    training set resampled\n\n    Return: (X_train, y_train)\n    """\n    def stratified_sample_df_(df, col, n_samples, replace=True):\n        if replace:\n            n = n_samples\n        else:\n            n = min(n_samples, df[col].value_counts().min())\n\n        df_ = df.groupby(col).apply(lambda x: x.sample(n, replace=replace))\n        df_.index = df_.index.droplevel(0)\n        return df_\n\n    X_train, y_train = load_data_for_train()\n\n    # merge the dataframe for the sampling. Target will be removed after\n    X_train = pd.merge(\n        X_train, y_train[[\'Target\']], left_index=True, right_index=True)\n    del y_train\n\n    # build a categorical feature, from S1..S3 distribution\n    disc = KBinsDiscretizer(n_bins=n_bins, encode=\'ordinal\', strategy=\'kmeans\')\n    disc.fit(X_train[[\'S1\', \'S2\', \'S3\']])\n    y_bin = disc.transform(X_train[[\'S1\', \'S2\', \'S3\']])\n    del disc\n    vint = np.vectorize(np.int)\n    y_bin = vint(y_bin)\n\n    y_concat = []\n    for i in range(len(y_bin)):\n        a = y_bin[i, 0].astype(\'str\')\n        b = y_bin[i, 1].astype(\'str\')\n        c = y_bin[i, 2].astype(\'str\')\n        y_concat.append(a + \';\' + b + \';\' + c)\n    del y_bin\n\n    X_train[\'S_Class\'] = y_concat\n    del y_concat\n\n    X_train_resampled = stratified_sample_df_(\n        X_train, \'S_Class\', n_samples_by_bin)\n    del X_train\n    y_train_resampled = X_train_resampled[[\'Target\']].copy()\n    y_train_resampled.rename(\n        columns={y_train_resampled.columns[0]: \'Target\'}, inplace=True)\n\n    X_train_resampled = X_train_resampled.drop([\'S_Class\', \'Target\'], axis=1)\n\n    # save in file for further usage\n    if save_csv:\n        X_train_resampled.to_csv(\n            "./data/training_input_resampled.csv", sep=",")\n        y_train_resampled.to_csv(\n            "./data/training_output_resampled.csv", sep=",")\n\n    return(X_train_resampled,\n           y_train_resampled)\n'
"===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nsepal_width     1.8690      0.033     57.246      0.000       1.804       1.934\n==============================================================================\nOmnibus:                       18.144   Durbin-Watson:                   0.427\nProb(Omnibus):                  0.000   Jarque-Bera (JB):                7.909\nSkew:                          -0.338   Prob(JB):                       0.0192\nKurtosis:                       2.101   Cond. No.                         1.00\n==============================================================================\n\n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nsepal_width       1.4512      0.015     94.613      0.000       1.420       1.482\noutlier_dummy    -6.6097      0.394    -16.791      0.000      -7.401      -5.819\n==============================================================================\nOmnibus:                        1.917   Durbin-Watson:                   2.188\nProb(Omnibus):                  0.383   Jarque-Bera (JB):                1.066\nSkew:                           0.218   Prob(JB):                        0.587\nKurtosis:                       3.558   Cond. No.                         27.0\n==============================================================================\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sample data\ndf = pd.DataFrame(sns.load_dataset('iris'))\n\n# subset of sample data\ndf=df[df['species']=='setosa']\n\n# add column for dummy variable\ndf['outlier_dummy']=0\n\n# append line with extreme value for sepal width\n# as well as a dummy variable = 1 for that row.\ndf.loc[len(df)] = [5,8,1.4, 0.3, 'setosa', 1]\n\n# define independent variables\nx=['sepal_width', 'outlier_dummy']\n\n# run regression\nmod_fit = sm.OLS(df['sepal_length'], df[x]).fit()\nres = mod_fit.resid\n\nfig = sm.qqplot(res)\nplt.show()\nmod_fit.summary()\n"
"df['pta'] = pd.to_datetime(df['pta']).astype(np.int64)\n"
'model_input = np.array([model_input])\n\nmodel_input = np.expand_dims(model_input, axis=0)\nmodel_input = model_input[None,:]\n\narray([[0.759178  , 0.40589622, 2.0082092 ]], dtype=float32)\n'
'container_folder/\n    CLASS_1_folder/\n        file_1.txt, file_2.txt ... \n    CLASS_2_folder/\n        file_1.txt, file_2.txt, ....\n\nX_train = vectorizer.fit_transform(text_train_subset)\n\nX_train = vectorizer.fit_transform(text_train_subset.data) # added .data\n\nfrom sklearn.datasets import load_files\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\ntext_train_subset = load_files(\'sample-data/web\')\ntext_test_subset = text_train_subset # load your actual test data here\n\n# Turn the text documents into vectors of word frequencies\nvectorizer = CountVectorizer()\nX_train = vectorizer.fit_transform(text_train_subset.data)\ny_train = text_train_subset.target\n\n\nclassifier = MultinomialNB().fit(X_train, y_train)\nprint("Training score: {0:.1f}%".format(\n    classifier.score(X_train, y_train) * 100))\n\n# Evaluate the classifier on the testing set\nX_test = vectorizer.transform(text_test_subset.data)\ny_test = text_test_subset.target\nprint("Testing score: {0:.1f}%".format(\n    classifier.score(X_test, y_test) * 100))\n\nsample-data/web\n├── de\n│\xa0\xa0 ├── apollo8.txt\n│\xa0\xa0 ├── fiv.txt\n│\xa0\xa0 ├── habichtsadler.txt\n└── en\n    ├── elizabeth_needham.txt\n    ├── equipartition_theorem.txt\n    ├── sunderland_echo.txt\n    └── thespis.txt\n'
'|--------------------------------------------------------------------------&gt;\n\n|--------------------(cccccccccccc)----------------------------------------&gt;\n\n|--------------------(rrrrrrrrrrrtt)----------------------------------------&gt;\n'
'reg = theta\n\nreg = theta.copy()\n\ninitial_theta = np.random.randn(n+1)\n'
'y = kmeans.predict(X)\n\nsvm = SVM()\nsvm.fit(X, y)\n'
'for file in listing1:\n img = cv2.imread(path1 + file)\n res=cv2.resize(img,(250,250))\n h=hog(res)\n training_set.append(h)\n\n training_labels.append(1)\n\ntrainData=np.float32(training_set)\nresponses=np.float32(training_labels)\n\nsvm.train(trainData,responses, params=svm_params)\n\nresult = svm.predict_all(testData)\nprint result\n'
'from sklearn.metrics import confusion_matrix\n\ny_true = [2, 0, 2, 2, 0, 1]\ny_pred = [0, 0, 2, 2, 0, 2]\nconfusion_matrix(y_true, y_pred)\n\narray([[2, 0, 0],\n       [0, 0, 1],\n       [1, 0, 2]])\n\ny_true = [1, 0, 1, 0, 0, 1]\ny_pred = [1, 0, 1, 1, 0, 1]\n\ncm = confusion_matrix(y_true, y_pred)\nprint cm\n\ntp = float(cm[0][0])/np.sum(cm[0])\ntn = float(cm[1][1])/np.sum(cm[1])\n\nprint tp\nprint tn\n\n[[2 1]\n [0 3]]\n0.666666666667\n1.0\n'
'cmat = TruncatedSVD(n_components=2, random_state=42).fit_transform(cmat)\n'
'train_X = vectorizer.fit_transform(train)\ntest_X = vectorizer.fit_transform(test)\n\ntrain_X = vectorizer.fit_transform(train)\ntest_X = vectorizer.transform(test)\n'
'# Calculate Accuracy\nwith tf.name_scope("accuracy"):\n    correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n    self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, "float"), name="accuracy")\n'
'with tf.Session() as sess:   \n    img = sess.run(image)\n    print(img)\n'
"import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nclf = RandomForestClassifier()\n\nfeaturesets = # your code here\ngender = {'male': 0, 'female': 1}\n\nX = np.asarray([[i[1] for i in sorted(d.items())] for d, _ in featuresets])\ny = np.asarray([gender[s] for _, s in featuresets])\n\nscores = cross_val_score(clf, X, y, cv=10)\nprint('Scores =', scores)\n"
'tf.contrib.legacy_seq2seq.rnn_decoder\n'
'net.blobs[net.blobs.keys()[1]].data.shape\n'
'selector = SelectKBest(chi2, k=2)\nX_train_clean = selector.fit_transform(X_train, y_train)\nX_test_clean = selector.transform(X_test)\n'
'x = tf.placeholder(tf.bool, [None,60,4])\n\nx_flat = tf.reshape( x , [ -1 , 60*4 ] )\nij1 = tf.reshape( tf.one_hot( [i1*4+j1] , 60*4 , dtype=tf.float32 ) , [ 60*4 , 1 ] )\nij2 = tf.reshape( tf.one_hot( [i2*4+j2] , 60*4 , dtype=tf.float32 ) , [ 60*4 , 1 ] )\n\ntf.logical_and( tf.matmul( x_flat , ij1 ) , tf.matmul( x_flat , ij2 ) )\n'
"import pandas as pd\ndf = pd.read_csv('example.csv')\n\ndf.values\n"
'    $ python setup.py build\n    $ python\n    &gt; import sys\n    &gt; sys.path.insert(0, \'build/lib.linux-x86_64-2.7\')\n    &gt; from nnet_ts import *\n    &gt; time_series = np.array(pd.read_csv("nnet_ts/AirPassengers.csv")["x"]) \n    &gt; neural_net = TimeSeriesNnet(hidden_layers = [20, 15, 5], activation_functions = [\'sigmoid\', \'sigmoid\', \'sigmoid\'])\n'
'X_male[:,0][~np.isnan(X_male[:,0])].reshape(1, -1)\n\nimputer = imputer.fit(X_male[:,0].reshape(-1, 1))\nX_male[:,0] = imputer.transform(X_male[:,0].reshape(-1, 1)).reshape(-1)\n'
'tf.slice(varTensor, [0, 0], [2, 2]);\n\nimport tensorflow as tf\ntf.Session().run(tf.slice([[0,1,-1,-2,-3],[1,0,-9, -2, -3]], [0, 0], [2,2]))\n'
'&gt;&gt;&gt; p\n\ndef fu(x,a=0.4285,b=-1.71,c=0.057):\n    return x*x*a + x * b + c\n\n&gt;&gt;&gt; gmodel = Model(fu)\n&gt;&gt;&gt; gmodel.param_names\n[\'a\', \'c\', \'b\']\n&gt;&gt;&gt; gmodel.independent_vars\n[\'x\']\n\n&gt;&gt;&gt; result = gmodel.fit(y_test, x=x_test)\n&gt;&gt;&gt; print(result.fit_report())\n[[Model]]\n    Model(fu)\n[[Fit Statistics]]\n    # function evals   = 11\n    # data points      = 8\n    # variables        = 3\n    chi-square         = 2.159\n    reduced chi-square = 0.432\n    Akaike info crit   = -4.479\n    Bayesian info crit = -4.241\n[[Variables]]\n    a:   0.12619047 +/- 0.050695 (40.17%) (init= 0.4285)\n    c:  -0.55833335 +/- 0.553020 (99.05%) (init= 0.057)\n    b:  -0.52857141 +/- 0.369067 (69.82%) (init=-1.71)\n[[Correlations]] (unreported correlations are &lt;  0.100)\n    C(a, b)                      = -0.962 \n    C(c, b)                      = -0.793 \n    C(a, c)                      =  0.642 \n\nimport matplotlib.pyplot as plt\nfrom lmfit import Model\nimport numpy as np\n\ndef fu(x,a=0.4285,b=-1.71,c=0.057):\n    return x*x*a + x * b + c\n\ngmodel = Model(fu)\nprint "Params" , gmodel.param_names\nprint "Independent var", gmodel.independent_vars\n\nparams = gmodel.make_params()\nprint " Params prop", params\n\ndata_test = np.array([[0,0], [1,-1.2], [2, -2], [3,-1.3], [4,0], [5,0.5], [6,0.9], [7, 1.5]])\nx_test = data_test[:,0]\ny_test = data_test[:,1]\nresult = gmodel.fit(y_test, x=x_test)\nprint(result.fit_report())\nplt.plot(x_test, y_test,         \'bo\')\nplt.plot(x_test, result.init_fit, \'k--\')\nplt.plot(x_test, result.best_fit, \'r-\')\nplt.show()\n'
"metrics.pairwise_distances(i, reduced_train_data)\n\nnp.where(KM.labels_==ind)[0]\n\nfor i in centroids:\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef KMeansModel(n):\n    pca = PCA(n_components=2)\n    reduced_train_data = pca.fit_transform(train_data)\n    KM = KMeans(n_clusters=n)\n    KM.fit(reduced_train_data)\n    plt.plot(reduced_train_data[:, 0], reduced_train_data[:, 1], 'k.', markersize=2)\n    centroids = KM.cluster_centers_\n    # Plot the centroids as a red X\n    plt.scatter(centroids[:, 0], centroids[:, 1],\n                marker='x', color='r')\n    for ind,i in enumerate(centroids):\n        class_inds=np.where(KM.labels_==ind)[0]\n        max_dist=np.max(metrics.pairwise_distances(i, reduced_train_data[class_inds]))\n        print(max_dist)\n        plt.gca().add_artist(plt.Circle(i, max_dist, fill=False))\n    plt.show()\n\nout = [KMeansModel(n) for n in np.arange(1,16,1)]\n"
'fsp = y_pred - K.mean(y_pred) #being K.mean a scalar here, it will be automatically subtracted from all elements in y_pred\nfst = y_true - K.mean(y_true)\n\ndevP = K.std(y_pred)\ndevT = K.std(y_true)\n\nreturn K.mean(fsp*fst)/(devP*devT)\n\n#original shapes: (batch, 10)\n\nfsp = y_pred - K.mean(y_pred,axis=0) #you take the mean over the batch, keeping the features separate.   \nfst = y_true - K.mean(y_true,axis=0) \n    #mean shape: (1,10)\n    #fst shape keeps (batch,10)\n\ndevP = K.std(y_pred,axis=0)  \ndevt = K.std(y_true,axis=0)\n    #dev shape: (1,10)\n\nreturn K.sum(K.mean(fsp*fst,axis=0)/(devP*devT))\n    #mean shape: (1,10), making all tensors in the expression be (1,10). \n    #sum is only necessary because we need a single loss value\n'
'network = layers.join(\n   layers.Input(10),\n\n   layers.Linear(500),\n   layers.Relu(),\n\n   layers.Linear(300),\n   layers.Relu(),\n\n   layers.Linear(1),  # Single output\n   layers.Sigmoid(),  # Sigmoid works better for 2-class classification\n)\n\nnetwork = layers.join(\n   layers.Input(10),\n   layers.Relu(500),\n   layers.Relu(300),\n   layers.Sigmoid(1),\n)\n'
"label = PIL.Image.open(dir).convert('P')\np = label.getpalette()\nlabel = np.asarray(label)\nimg = PIL.Image.fromarray(label, mode='P')\nimg.setpalette(p)\nimg.save('test.png')\n"
'X = np.array(X, dtype=np.float32)\nY = np.array(Y, dtype=np.float32)\n\nX_mean = X.mean(axis=0)\nX -= X_mean\nX_std = X.std(axis=0)\nX /= X_std + 1e-8  # add a very small constant to prevent division by zero\n\nX_test -= X_mean\nX_test /= X_std + 1e-8\n\nfrom keras import optimizers\nmodel.compile(loss=\'mean_squared_error\', optimizer=optimizers.Adam(lr=0.0001))\n\n# first shuffle the data to make sure it isn\'t in any particular order\nindices = np.arange(X.shape[0])\nnp.random.shuffle(indices)\nX = X[indices]\nY = Y[indices]\n\n# you have 200 images\n# we select 100 images for training,\n# 50 images for validation and 50 images for test data\nX_train = X[:100]\nX_val = X[100:150]\nX_test = X[150:]\nY_train = Y[:100]\nY_val = Y[100:150]\nY_test = Y[150:]\n\n# train and tune the model \n# you can attempt train and tune the model multiple times,\n# each time with different architecture, hyper-parameters, etc.\nmodel.fit(X_train, Y_train, epochs=15, batch_size=10, validation_data=(X_val, Y_val))\n\n# only and only after completing the tuning of your model\n# you should evaluate it on the test data for just one time\nmodel.evaluate(X_test, Y_test)\n\n# after you are satisfied with the model performance\n# and want to deploy your model for production use (i.e. real world)\n# you can train your model once more on the whole data available\n# with the best configurations you have found out in your tunings\nmodel.fit(X, Y, epochs=15, batch_size=10)\n\n# machine learning code mostly from https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport numpy as np\nimport pickle\n\ndef pil_image_to_np_array(image):\n    \'\'\'Takes an image and converts it to a numpy array\'\'\'\n    # from https://stackoverflow.com/a/45208895\n    # all my images are black and white, so I only need one channel\n    return np.array(image)[:, :, 0]\n\ndef data_to_training_set(data):\n    # split the list in the form [(frame 1 image, frame 1 player position), ...] into [[all images], [all player positions]]\n    inputs, outputs = zip(*data)\n    inputs = [pil_image_to_np_array(image) for image in inputs]\n    inputs = np.array(inputs, dtype=np.float32)\n    outputs = np.array(outputs, dtype=np.float32)\n    return (inputs, outputs)\n\nif __name__ == "__main__":\n    # fix random seed for reproducibility\n    np.random.seed(7)\n\n    # load data\n    # data will be in the form [(frame 1 image, frame 1 player position), (frame 2 image, frame 2 player position), ...]\n    with open("position_data1.pkl", "rb") as pickled_data:\n        data = pickle.load(pickled_data)\n    X, Y = data_to_training_set(data)\n\n    # get the width of the images\n    width = X.shape[2] # == 400\n    # convert the player position (a value between 0 and the width of the image) to values between 0 and 1\n    Y /= width\n\n    # flatten the image inputs so they can be passed to a neural network\n    X = np.reshape(X, (X.shape[0], -1))\n\n    # create model\n    model = Sequential()\n    # my images are 300 x 400 pixels, so each input will be a flattened array of 120000 gray-scale pixel values\n    # keep it super simple by not having any deep learning\n    model.add(Dense(1, input_dim=120000, activation=\'sigmoid\'))\n\n    # Compile model\n    model.compile(loss=\'mean_squared_error\', optimizer=\'adam\')\n\n    # Fit the model\n    model.fit(X, Y, epochs=15, batch_size=10)\n\n    # see what the model is doing\n    predictions = model.predict(X, batch_size=10)\n    print(predictions) # this prints all 1s! # TODO fix\n'
'input shape: (number of samples, number of time steps, input data)\ni.e. (1, 1, 1) would mean you have 1 sample with 1 step and 1 feature dimension\n\noutput shape: (number of samples, output data)\ni.e. (1, 1) would mean you have 1 sample with 1 output dimension\n'
'return 1/(1+e**(-(intercept + coeff[0]*bodyPart+ coeff[1]*shotQuality+coeff[2]*defPressure+coeff[3]*numDefPlayers+coeff[4]*numAttPlayers+coeff[5]*shotdist+ coeff[6]*angle+coeff[7]*chanceRating+coeff[8]*type))\n'
'severity_S1   severity_S2   severity_S3  \n\n  1              0              0                  # when value is S1\n  0              1              0                  # when value is S2  \n  0              0              1                  # when value is S3\n\n severity_S2   severity_S3\n\n   0              0                  # when value is S1\n   1              0                  # when value is S2  \n   0              1                  # when value is S3\n\ns2  s3  p2  p3  B  C  D\n0   0   1   0   1  0  0     # For row with S1, P2 and B\n0   1   0   1   0  1  0     # For row with S3, P3 and C\n1   0   0   0   0  0  1     # For row with S2, P1 and D\n1   0   0   0   0  0  0     # For row with S2, P1 and A\n'
'FROM ubuntu\n\n# Install pip and git and clone repo into /app\nRUN apt-get update &amp;&amp; apt-get install --assume-yes --fix-missing python-pip git &amp;&amp; git clone https://github.com/amirziai/sklearnflask.git /app\n\n# Change WORKDIR\nWORKDIR /app\n\n# Install dependencies\nRUN pip install -r requirements.txt\n\n# Expose port and run the application when the container is started\nEXPOSE 9999\nENTRYPOINT python main.py 9999\n'
'import numpy, scipy, matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom scipy.optimize import differential_evolution\nimport warnings\n\nxData = numpy.array([19.1647, 18.0189, 16.9550, 15.7683, 14.7044, 13.6269, 12.6040, 11.4309, 10.2987, 9.23465, 8.18440, 7.89789, 7.62498, 7.36571, 7.01106, 6.71094, 6.46548, 6.27436, 6.16543, 6.05569, 5.91904, 5.78247, 5.53661, 4.85425, 4.29468, 3.74888, 3.16206, 2.58882, 1.93371, 1.52426, 1.14211, 0.719035, 0.377708, 0.0226971, -0.223181, -0.537231, -0.878491, -1.27484, -1.45266, -1.57583, -1.61717])\nyData = numpy.array([0.644557, 0.641059, 0.637555, 0.634059, 0.634135, 0.631825, 0.631899, 0.627209, 0.622516, 0.617818, 0.616103, 0.613736, 0.610175, 0.606613, 0.605445, 0.603676, 0.604887, 0.600127, 0.604909, 0.588207, 0.581056, 0.576292, 0.566761, 0.555472, 0.545367, 0.538842, 0.529336, 0.518635, 0.506747, 0.499018, 0.491885, 0.484754, 0.475230, 0.464514, 0.454387, 0.444861, 0.437128, 0.415076, 0.401363, 0.390034, 0.378698])\n\n\ndef func(xArray, breakpoint, slopeA, offsetA, slopeB, offsetB):\n    returnArray = []\n    for x in xArray:\n        if x &lt; breakpoint:\n            returnArray.append(slopeA * x + offsetA)\n        else:\n            returnArray.append(slopeB * x + offsetB)\n    return returnArray\n\n\n# function for genetic algorithm to minimize (sum of squared error)\ndef sumOfSquaredError(parameterTuple):\n    warnings.filterwarnings("ignore") # do not print warnings by genetic algorithm\n    val = func(xData, *parameterTuple)\n    return numpy.sum((yData - val) ** 2.0)\n\n\ndef generate_Initial_Parameters():\n    # min and max used for bounds\n    maxX = max(xData)\n    minX = min(xData)\n    maxY = max(yData)\n    minY = min(yData)\n    slope = 10.0 * (maxY - minY) / (maxX - minX) # times 10 for safety margin\n\n    parameterBounds = []\n    parameterBounds.append([minX, maxX]) # search bounds for breakpoint\n    parameterBounds.append([-slope, slope]) # search bounds for slopeA\n    parameterBounds.append([minY, maxY]) # search bounds for offsetA\n    parameterBounds.append([-slope, slope]) # search bounds for slopeB\n    parameterBounds.append([minY, maxY]) # search bounds for offsetB\n\n\n    result = differential_evolution(sumOfSquaredError, parameterBounds, seed=3)\n    return result.x\n\n# by default, differential_evolution completes by calling curve_fit() using parameter bounds\ngeneticParameters = generate_Initial_Parameters()\n\nfittedParameters, pcov = curve_fit(func, xData, yData, geneticParameters)\nprint(\'Parameters:\', fittedParameters)\nprint()\n\nmodelPredictions = func(xData, *fittedParameters) \n\nabsError = modelPredictions - yData\n\nSE = numpy.square(absError) # squared errors\nMSE = numpy.mean(SE) # mean squared errors\nRMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\nRsquared = 1.0 - (numpy.var(absError) / numpy.var(yData))\n\nprint()\nprint(\'RMSE:\', RMSE)\nprint(\'R-squared:\', Rsquared)\n\nprint()\n\n\n##########################################################\n# graphics output section\ndef ModelAndScatterPlot(graphWidth, graphHeight):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    # first the raw data as a scatter plot\n    axes.plot(xData, yData,  \'D\')\n\n    # create data for the fitted equation plot\n    xModel = numpy.linspace(min(xData), max(xData))\n    yModel = func(xModel, *fittedParameters)\n\n    # now the model as a line plot\n    axes.plot(xModel, yModel)\n\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot\n\ngraphWidth = 800\ngraphHeight = 600\nModelAndScatterPlot(graphWidth, graphHeight)\n'
"from sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X[y &lt; 2], y[y &lt; 2],\n                                                    test_size=.5,\n                                                    random_state=random_state)\n\n# Create a simple classifier\nclassifier = svm.LinearSVC(random_state=random_state)\nclassifier.fit(X_train, y_train)\ny_score = classifier.decision_function(X_test)\n\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.fixes import signature\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\n\n# In matplotlib &lt; 1.5, plt.fill_between does not have a 'step' argument\nstep_kwargs = ({'step': 'post'}\n               if 'step' in signature(plt.fill_between).parameters\n               else {})\nplt.step(recall, precision, color='b', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n"
'feature_cols_tensor = [tf.feature_column.numeric_column(feature_names)]\n\nfeature_cols_tensor = list(map(tf.feature_column.numeric_column, feature_names))\n'
"data = numpy.loadtxt(data, delimiter=',')\n\n# Loop through the data in the array\nfor index in range(len(data)):\n    # Utilize a try catch to try and convert to float, if it can't convert to float, converts to 0\n    try:\n        data[index] = [float(x) for x in data[index]]\n    except ValueError:\n        data[index] = 0\n"
'#true value affter 3 days: 16.28 MPa\n'
'from sklearn.feature_selection import RFE\nrfe = RFE(estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_\n\nfrom sklearn.feature_selection import VarianceThreshold\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\nsel.fit_transform(X)\n'
'# Normalize images from 0-255 to 0-1\ntrain_images /= 255\ntest_images /= 255\n\ndef load_image_for_cnn(filename):\n    img = Image.open(filename).convert("L")\n    img = np.resize(img, (28, 28, 1))\n    im2arr = np.array(img)\n    return im2arr.reshape(1, 28, 28, 1)\n\ndef load_image_for_cnn(filename):\n    img = Image.open(filename).convert("L")\n    img = np.resize(img, (28, 28, 1))\n    im2arr = np.array(img)\n    im2arr = im2arr / 255.0\n    return im2arr.reshape(1, 28, 28, 1)\n'
'mask = cv2.inRange(img, vermelho_inicio, vermelho_fim)\nnp_points = np.transpose(np.nonzero(mask))\npoints = np.fliplr(np_points) # opencv uses flipped x,y coordinates \napprox = cv2.convexHull(points)\ncv2.polylines(img, [approx],True, (0,255,255), 5)\n'
'import torch \nlogit = torch.rand(100,10)\ntarget = torch.randint(10, size=(100,)) \nm = torch.nn.functional.nll_loss(logit, target)\ns = torch.nn.functional.nll_loss(logit, target, reduction="sum") \nl = torch.nn.functional.nll_loss(logit, target, reduction="none")\nprint(torch.abs(m-s/100))\nprint(torch.abs(l.mean()-m))\n'
'inputs = tf.keras.Input(shape=(H,W,))\nx1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3)(inputs)\nx2 = tf.keras.layers.Conv2D(filters=16, kernel_size=5)(inputs)\n#match dimensions (height and width) of x1 or x2 here \nx3 = tf.keras.layers.Concatenate(axis=-1)[x1,x2]\n'
'early_stopping_callback.best = new_value\nearly_stopping_callback.wait = 0\n\nclass CustomCallback(...):\n    def __init__ (....., early_stopper, ...)\n        self.early_stopping_callback = early_stopper\n\n    .......\n\nearly_stopping = EarlyStopping(...)\ncustom = CustomCallback(...., early_stopping, ...)\n\nmodel.fit(..., callbacks=[custom, early_stopping])    \n'
'class MultiInputCNN(K.Model):\n    def __init__(self):\n        super(MultiInputCNN, self).__init__()\n\n        self.custom_net1 = K.applications.MobileNetV2(\n                             input_shape=(32, 32, 3),\n                             include_top=False,\n                             weights=None)\n        \n        self.custom_net2 = K.applications.MobileNetV2(\n                             input_shape=(32, 32, 3),\n                             include_top=False,\n                             weights=None)\n  \n        self.custom_net3 = K.applications.MobileNetV2(\n                             input_shape=(32, 32, 3),\n                             include_top=False,\n                             weights=None)\n\n        self.concat = K.layers.Concatenate()\n        self.pool = K.layers.GlobalAveragePooling2D()\n        self.dropout = K.layers.Dropout(.5)\n        self.dense = K.layers.Dense(2)\n\n    def call(self, inputs, training=None, **kwargs):\n        x, y, z = inputs[0]\n        net1 = self.custom_net1(x)\n        net2 = self.custom_net2(y)\n        net3 = self.custom_net3(z)\n\n        x = self.concat([net1, net2, net3])\n        x = self.pool(x)\n        x = self.dropout(x)\n        x = tf.nn.sigmoid(self.dense(x))\n        return x\n\n\nmodel = MultiInputCNN()\n\nmodel(next(iter(train_ds)))\n'
"def plot_image(i, predictions_array, true_label, img):\n    predictions_array, true_labels, img = predictions_array, true_label[i], img[i]\n    plt.grid=False\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(img, cmap=plt.cm.binary)\n    predicted_label = np.argmax(predictions_array)\n\n    if predicted_label == true_labels:\n        color = 'blue'\n    else:\n        color = 'red'\n    \n    print(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_labels])\n\n    plt.xlabel('{}{:2.0f}%({})'.format(class_names[predicted_label], 100*np.max(predictions_array), class_names[true_labels]), color=color)\n"
'scores = parallel(\n    delayed(_fit_and_score)(\n        clone(estimator), X, y, scorers, train, test, verbose, None,\n        fit_params, return_train_score=return_train_score,\n        return_times=True, return_estimator=return_estimator,\n        error_score=error_score)\n    for train, test in cv.split(X, y, groups))\n\ndef clone(estimator, *, safe=True):\n    &quot;&quot;&quot;Constructs a new estimator with the same parameters.\n    Clone does a deep copy of the model in an estimator\n    without actually copying attached data. It yields a new estimator\n    with the same parameters that has not been fit on any data.\n    ...\n'
"    p.py:38: UserWarning: Using a target size (torch.Size([50])) that is \ndifferent to the input size (torch.Size([50, 1])). This will likely lead \nto incorrect results due to broadcasting. Please ensure they have the same size.\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import TensorDataset\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\n\ninputs = np.random.rand(50, 10)\ntargets = np.random.randint(0, 2, 50)\n\n# Tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\ntrain_ds = TensorDataset(inputs, targets.squeeze())\nbatch_size = 100\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True)\n\nmodel = nn.Linear(10, 1)\n# Define Loss func\nloss_fn = F.mse_loss\n# Optimizer\nopt = torch.optim.SGD(model.parameters(), lr = 1e-1)\n\n\nnum_epochs = 100\nmodel.train()\nfor epoch in range(num_epochs):\n    # Train with batches of data\n    for xb, yb in train_dl:\n\n\n\n# 1. Generate predictions\n    pred = model(xb.float())\n\n    # 2. Calculate loss\n    yb = yb.view(yb.size(0), -1)\n    loss = loss_fn(pred, yb.float())\n\n    # 3. Compute gradients\n    loss.backward()\n\n    # 4. Update parameters using gradients\n    opt.step()\n\n    # 5. Reset the gradients to zero\n    opt.zero_grad()\n\n    if (epoch+1) % 10 == 0:\n        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch +\n                                               1, num_epochs, \n                                               loss.item()))\n"
"import tensorflow as tf\nimport numpy as np\n\ndata=tf.Variable(np.random.random((9923,1000)),dtype=tf.float32) #your data has shape (9923,1000)\n#print(data)\ndata=tf.reshape(data,(9923,1000,1)) #reshape data\n#print(data)\nconv1d=tf.keras.layers.Conv1D(64,3, activation ='relu', input_shape= (1000, 1)) # the layer accpets any input with shape (*,1). \n# That means the last dimention should be 1\ncnn_data=conv1d(data) # output cnn_data has shape  (9923,998,64)\n"
'cnn.fit(x=training_set,validation_data=test_set,epochs=10)\n\nbatch_size = 20\nNo_Of_Training_Images = Train_Generator.classes.shape[0]\nsteps_per_epoch = No_Of_Training_Images/batch_size\ncnn.fit(x=training_set,validation_data=test_set,epochs=10, \nsteps_per_epoch = steps_per_epoch)\n\nEpoch 1/10\n    28/100 - 12s 416ms/step - loss: 0.9420 - accuracy: 0.4964\n'
"mnist_dataset, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n\n(\n    {\n'test': &lt;PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)&gt;,\n'train': &lt;PrefetchDataset shapes: ((28, 28, 1), ()), types: (tf.uint8, tf.int64)&gt;\n    },\n    \ntfds.core.DatasetInfo(name='mnist', etc)\n)\n"
"dataset = pd.read_csv(&quot;model_data.csv&quot;)\n\ndataset = pd.get_dummies(dataset , columns=['Col1', 'Col2', 'name', 'ID'])\ndataset.Result = pd.factorize(dataset.Result)[0]\n\n  Col1 Col2     name    ID Result\n0   AB    A     John -2500      N\n1   AB    A     John -2500      N\n2    A    A     John -2500      N\n3    A    A    Jacob -2500      Y\n4    A    A  Micheal -2500      Y\n5    A   AB     John -2500      N\n6    A    A  Sheldon -2500      Y\n7   AB   AB  Sheldon -2500      N\n8   AB   AB    Jacob -2500      Y\n\n\n   Result  Col1_A  Col1_AB  Col2_A  Col2_AB  name_Jacob  name_John  name_Micheal  name_Sheldon  ID_-2500\n0       0       0        1       1        0           0          1             0             0         1\n1       0       0        1       1        0           0          1             0             0         1\n2       0       1        0       1        0           0          1             0             0         1\n3       1       1        0       1        0           1          0             0             0         1\n4       1       1        0       1        0           0          0             1             0         1\n5       0       1        0       0        1           0          1             0             0         1\n6       1       1        0       1        0           0          0             0             1         1\n7       0       0        1       0        1           0          0             0             1         1\n8       1       0        1       0        1           1          0             0             0         1\n"
'for comp in components:\n    comp_rep[comp] = (comp_rep[&quot;datetime&quot;] - pd.to_datetime(comp_rep[comp])) / np.timedelta64(1, &quot;D&quot;) comp_rep.head()\n'
'N=100 #For example\nk=10 #For example\nstacked_predictions = torch.randn(N, k, 1)  # (N, k, 1) tensor with the predictions\nt = torch.randn(N,1)  # common target\n\nloss = k * nn.MSELoss()(stacked_predictions, t[:, None, :])\n'
'import numpy as np\n\nrng = np.random.default_rng(312345)\n\nq = np.reshape(rng.integers(0, 10, size=200), (-1, 1)) # array with (200,1) shape\nt = rng.integers(0, 10, size=200) # array with (200,) shape\n\nz = q.flatten() # values from q array with (200,) shape\n\nprint(np.mean(z == t))\nprint(np.mean(q == t))\nprint(&quot;---------------------------------&quot;)\nprint(q.shape)\nprint(z.shape)\nprint(t.shape)\nprint(&quot;---------------------------------&quot;)\nprint(len(z == t))\nprint(len((q == t).flatten()))\n\n0.1\n0.0998\n---------------------------------\n(200, 1)\n(200,)\n(200,)\n---------------------------------\n200\n40000\n'
'#Output: &lt;TakeDataset shapes: ((), (), ..., tf.float32)&gt;\n\n[0, 1, 0, 1, 0, 0, 0 ..., 1]\n'
'price = number_of_rooms * 5 + (year-2000) * 20\n'
"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom tpot import TPOTClassifier\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data \ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n\nmodel_parameters = {'n_estimators': [100,200],\n                      &quot;max_depth&quot; : [None, 5, 10],\n                      &quot;max_features&quot; : [len(X_train[0])]}  \n\n\nmodel_tuned = TPOTClassifier(generations= 2, \n                             population_size= 2, \n                             offspring_size= 2,\n                             verbosity= 2, \n                             early_stop= 10, \n                             config_dict={'sklearn.ensemble.RandomForestClassifier': \n                             model_parameters}, \n                             cv = 5)\n \n\nmodel_tuned.fit(X_train,y_train)\n\nX = X.to_numpy\ny = y.to_numpy\n"
"intersection = tp # you have 1\nunion = tp+fp # you have 2 \njaccard = intersection / union\n\nimport pandas as pd\n\nlabels = pd.Categorical(['nao_doentes','doentes'],categories=['nao_doentes','doentes'])\nprediction = [1 ,0 ,0, 0]\ny = [1 ,0, 1, 0]\n\npd.crosstab(labels[y],labels[prediction])\n\ncol_0   nao_doentes doentes\nrow_0       \nnao_doentes 2   0\ndoentes     1   1\n"
"&quot;&quot;&quot;Implement StackingClassifier that can handle sample-weighted Pipelines.&quot;&quot;&quot;\n\nfrom sklearn.ensemble import StackingRegressor, StackingClassifier\nfrom copy import deepcopy\n\nimport numpy as np\nfrom joblib import Parallel\n\nfrom sklearn.base import clone\nfrom sklearn.base import is_classifier, is_regressor\n\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import check_cv\n\nfrom sklearn.utils import Bunch\nfrom sklearn.utils.fixes import delayed\n\nfrom sklearn.pipeline import Pipeline\n\nESTIMATOR_NAME_IN_PIPELINE = 'estimator'\n\ndef new_fit_single_estimator(estimator, X, y, sample_weight=None,\n                             message_clsname=None, message=None):\n    &quot;&quot;&quot;Private function used to fit an estimator within a job.&quot;&quot;&quot;\n    if sample_weight is not None:\n        try:\n            if isinstance(estimator, Pipeline):\n                # determine name of final estimator\n                estimator_name = estimator.steps[-1][0]\n                kwargs = {estimator_name + '__sample_weight': sample_weight}\n                estimator.fit(X, y, **kwargs)\n            else:\n                estimator.fit(X, y, sample_weight=sample_weight)\n        except TypeError as exc:\n            if &quot;unexpected keyword argument 'sample_weight'&quot; in str(exc):\n                raise TypeError(\n                    &quot;Underlying estimator {} does not support sample weights.&quot;\n                    .format(estimator.__class__.__name__)\n                ) from exc\n            raise\n    else:\n        estimator.fit(X, y)\n    return estimator\n\n\nclass FlexibleStackingClassifier(StackingClassifier):\n\n    def __init__(self, estimators, final_estimator=None, *, cv=None,\n                 n_jobs=None, passthrough=False, verbose=0):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose\n        )\n\n    def fit(self, X, y, sample_weight=None):\n        &quot;&quot;&quot;Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n        y : array-like of shape (n_samples,)\n            Target values.\n        sample_weight : array-like of shape (n_samples,) or default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n            .. versionchanged:: 0.23\n               when not None, `sample_weight` is passed to all underlying\n               estimators\n\n        Returns\n        -------\n        self : object\n        &quot;&quot;&quot;\n        # all_estimators contains all estimators, the one to be fitted and the\n        # 'drop' string.\n        names, all_estimators = self._validate_estimators()\n        self._validate_final_estimator()\n\n        stack_method = [self.stack_method] * len(all_estimators)\n\n        # Fit the base estimators on the whole training data. Those\n        # base estimators will be used in transform, predict, and\n        # predict_proba. They are exposed publicly.\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n            delayed(new_fit_single_estimator)(clone(est), X, y, sample_weight)\n            for est in all_estimators if est != 'drop'\n        )\n\n        self.named_estimators_ = Bunch()\n        est_fitted_idx = 0\n        for name_est, org_est in zip(names, all_estimators):\n            if org_est != 'drop':\n                self.named_estimators_[name_est] = self.estimators_[\n                    est_fitted_idx]\n                est_fitted_idx += 1\n            else:\n                self.named_estimators_[name_est] = 'drop'\n\n        # To train the meta-classifier using the most data as possible, we use\n        # a cross-validation to obtain the output of the stacked estimators.\n\n        # To ensure that the data provided to each estimator are the same, we\n        # need to set the random state of the cv if there is one and we need to\n        # take a copy.\n        cv = check_cv(self.cv, y=y, classifier=is_classifier(self))\n        if hasattr(cv, 'random_state') and cv.random_state is None:\n            cv.random_state = np.random.RandomState()\n\n        self.stack_method_ = [\n            self._method_name(name, est, meth)\n            for name, est, meth in zip(names, all_estimators, stack_method)\n        ]\n        fit_params = ({f&quot;{ESTIMATOR_NAME_IN_PIPELINE}__sample_weight&quot;: sample_weight}\n                      if sample_weight is not None\n                      else None)\n        predictions = Parallel(n_jobs=self.n_jobs)(\n            delayed(cross_val_predict)(clone(est), X, y, cv=deepcopy(cv),\n                                       method=meth, n_jobs=self.n_jobs,\n                                       fit_params=fit_params,\n                                       verbose=self.verbose)\n            for est, meth in zip(all_estimators, self.stack_method_)\n            if est != 'drop'\n        )\n\n        # Only not None or not 'drop' estimators will be used in transform.\n        # Remove the None from the method as well.\n        self.stack_method_ = [\n            meth for (meth, est) in zip(self.stack_method_, all_estimators)\n            if est != 'drop'\n        ]\n\n        X_meta = self._concatenate_predictions(X, predictions)\n        new_fit_single_estimator(self.final_estimator_, X_meta, y,\n                                 sample_weight=sample_weight)\n\n        return self\n\n\nclass FlexibleStackingRegressor(StackingRegressor):\n\n    def __init__(self, estimators, final_estimator=None, *, cv=None,\n                 n_jobs=None, passthrough=False, verbose=0):\n        super().__init__(\n            estimators=estimators,\n            final_estimator=final_estimator,\n            cv=cv,\n            n_jobs=n_jobs,\n            passthrough=passthrough,\n            verbose=verbose\n        )\n\n    def fit(self, X, y, sample_weight=None):\n        &quot;&quot;&quot;Fit the estimators.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vectors, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n        y : array-like of shape (n_samples,)\n            Target values.\n        sample_weight : array-like of shape (n_samples,) or default=None\n            Sample weights. If None, then samples are equally weighted.\n            Note that this is supported only if all underlying estimators\n            support sample weights.\n            .. versionchanged:: 0.23\n               when not None, `sample_weight` is passed to all underlying\n               estimators\n\n        Returns\n        -------\n        self : object\n        &quot;&quot;&quot;\n        # all_estimators contains all estimators, the one to be fitted and the\n        # 'drop' string.\n        names, all_estimators = self._validate_estimators()\n        self._validate_final_estimator()\n\n        stack_method = [self.stack_method] * len(all_estimators)\n\n        # Fit the base estimators on the whole training data. Those\n        # base estimators will be used in transform, predict, and\n        # predict_proba. They are exposed publicly.\n        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n            delayed(new_fit_single_estimator)(clone(est), X, y, sample_weight)\n            for est in all_estimators if est != 'drop'\n        )\n\n        self.named_estimators_ = Bunch()\n        est_fitted_idx = 0\n        for name_est, org_est in zip(names, all_estimators):\n            if org_est != 'drop':\n                self.named_estimators_[name_est] = self.estimators_[\n                    est_fitted_idx]\n                est_fitted_idx += 1\n            else:\n                self.named_estimators_[name_est] = 'drop'\n\n        # To train the meta-classifier using the most data as possible, we use\n        # a cross-validation to obtain the output of the stacked estimators.\n\n        # To ensure that the data provided to each estimator are the same, we\n        # need to set the random state of the cv if there is one and we need to\n        # take a copy.\n        cv = check_cv(self.cv, y=y, classifier=is_classifier(self))\n        if hasattr(cv, 'random_state') and cv.random_state is None:\n            cv.random_state = np.random.RandomState()\n\n        self.stack_method_ = [\n            self._method_name(name, est, meth)\n            for name, est, meth in zip(names, all_estimators, stack_method)\n        ]\n        fit_params = ({f&quot;{ESTIMATOR_NAME_IN_PIPELINE}__sample_weight&quot;: sample_weight}\n                      if sample_weight is not None\n                      else None)\n        predictions = Parallel(n_jobs=self.n_jobs)(\n            delayed(cross_val_predict)(clone(est), X, y, cv=deepcopy(cv),\n                                       method=meth, n_jobs=self.n_jobs,\n                                       fit_params=fit_params,\n                                       verbose=self.verbose)\n            for est, meth in zip(all_estimators, self.stack_method_)\n            if est != 'drop'\n        )\n\n        # Only not None or not 'drop' estimators will be used in transform.\n        # Remove the None from the method as well.\n        self.stack_method_ = [\n            meth for (meth, est) in zip(self.stack_method_, all_estimators)\n            if est != 'drop'\n        ]\n\n        X_meta = self._concatenate_predictions(X, predictions)\n        new_fit_single_estimator(self.final_estimator_, X_meta, y,\n                                 sample_weight=sample_weight)\n\n        return self\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.feature_selection import VarianceThreshold\n\nvalidly_named_pipeline = Pipeline([\n    ('variance_threshold', VarianceThreshold()),\n    ('scaler', StandardScaler()),\n    ('estimator', TweedieRegressor())\n])\n"
"Expected OrderedDict or OrderedUpdates, got &lt;class 'dict'&gt;. This can make your script non-deterministic.\n\ndef cost(inputs, outputs, learn_rate, theta, bias):\n    hyp = T.dot(theta, inputs) + bias\n    loss = T.mean((hyp - outputs)**2)/2\n\n    grad_t, grad_b = T.grad(loss, [theta, bias])\n\n    return loss, OrderedDict([(theta, theta-learn_rate*grad_t),\n                              (bias, bias-learn_rate*grad_b)])\n\nresults, updates = theano.scan(fn=cost,\n                               non_sequences=[x, y, learn_rate, theta, bias],\n                               n_steps=epochs)\n\ntrain = theano.function(inputs=[x, y, learn_rate, epochs], outputs=results,\n                        updates=updates)\n\n#### Libraries\n# Standard Libraries\nfrom collections import OrderedDict\n\n# Third Party Libraries\n# import matplotlib.pyplot as plt\nimport numpy as np\n# from sklearn import linear_model\nimport theano\nimport theano.tensor as T\n\n# def gen_data(num_points=50, slope=1, bias=10, x_max=50):\n#     pass # Use the code in the above post to generate sample points\n\n########################################################################\n# Generate Data\ntrain_x, train_y = gen_data(num_points=50, slope=2)\n\n# Declaring variable\nx = T.vector(name='x', dtype=theano.config.floatX)\ny = T.vector(name='y', dtype=theano.config.floatX)\n\nlearn_rate = T.scalar(name='learn_rate', dtype=theano.config.floatX)\nepochs = T.iscalar(name='epochs')\n\n# Variables that will be updated, hence are declared as `theano.share`\ntheta = theano.shared(np.random.rand(), name='theta')\nbias = theano.shared(np.random.rand(), name='bias')\n\ndef cost(inputs, outputs, learn_rate, theta, bias):\n    hyp = T.dot(theta, inputs) + bias\n    loss = T.mean((hyp - outputs)**2)/2\n\n    grad_t, grad_b = T.grad(loss, [theta, bias])\n\n    return loss, OrderedDict([(theta, theta-learn_rate*grad_t),\n                              (bias, bias-learn_rate*grad_b)])\n\nresults, updates = theano.scan(fn=cost,\n                               non_sequences=[x, y, learn_rate, theta, bias],\n                               n_steps=epochs)\n\n# results, updates = theano.scan(fn=cost,\n#                              sequences=[x, y],\n#                              non_sequences = [learn_rate, theta, bias],\n#                              n_steps=epochs)\n\ntrain = theano.function(inputs=[x, y, learn_rate, epochs], outputs=results,\n                        updates=updates)\n\nprint('weight: {}, bias: {}'.format(theta.get_value(), bias.get_value()))\ntrain(train_x, train_y, 0.001, 30)\nprint('------------------------------')\nprint('weight: {}, bias: {}'.format(theta.get_value(), bias.get_value()))\n"
"col1,col2,col3\n1,2,3\n4,XXX,6\n\ndata = pd.read_csv(filename)\n\nIn [84]: data\nOut[84]:\n   col1 col2  col3\n0     1    2     3\n1     4  XXX     6\n\nIn [85]: data.dtypes\nOut[85]:\ncol1     int64\ncol2    object\ncol3     int64\ndtype: object\n\nIn [86]: data.dtypes == np.object\nOut[86]:\ncol1    False\ncol2     True\ncol3    False\ndtype: bool\n\nIn [87]: data['col4'] = [[1,2], [3,4,5]]\n\nIn [88]: data\nOut[88]:\n   col1 col2  col3       col4\n0     1    2     3     [1, 2]\n1     4  XXX     6  [3, 4, 5]\n\nIn [89]: data.dtypes\nOut[89]:\ncol1     int64\ncol2    object\ncol3     int64\ncol4    object\ndtype: object\n\nIn [90]: data.dtypes == np.object\nOut[90]:\ncol1    False\ncol2     True\ncol3    False\ncol4     True\ndtype: bool\n"
'# This function returns the list of classes, and their associated weights (i.e. distributions)\ndef class_distribution(dataset):\n    dataset = numpy.asarray(dataset)\n    num_total_rows = dataset.shape[0]\n    num_columns = dataset.shape[1]\n    classes = dataset[:, num_columns - 1]\n    classes = numpy.unique(classes)\n    class_weights = []\n\n    # Loop through the classes one by one\n    for aclass in classes:\n        total = 0\n        weight = 0\n        for row in dataset:\n            if numpy.array_equal(aclass, row[-1]):\n                total = total + 1\n            else:\n                continue\n        weight = float((total / num_total_rows))\n        class_weights.append(weight)\n\n    class_weights = numpy.asarray(class_weights)\n    return classes, class_weights\n\n# This functions performs k cross fold validation for classification\ndef cross_fold_validation_classification(dataset, k):\n    temp_dataset = numpy.asarray(dataset)\n    classes, class_weights = class_distribution(temp_dataset)\n    total_num_rows = temp_dataset.shape[0]\n    data = numpy.copy(temp_dataset)\n    total_fold_array = []\n\n    for _ in range(k):\n        curr_fold_array = []\n\n        # Loop through each class and its associated weight\n        for a_class, a_class_weight in zip(classes, class_weights):\n            numpy.random.shuffle(data)\n            num_added = 0\n            num_to_add = float((((a_class_weight * total_num_rows)) / k))\n            tot = 0\n            for row in data:\n                curr = row[-1]\n                if num_added &gt;= num_to_add:\n                    break\n                else:\n                    if (a_class == curr):\n                        curr_fold_array.append(row)\n                        num_added = num_added + 1\n                        numpy.delete(data, tot)\n                tot = tot + 1\n        total_fold_array.append(curr_fold_array)\n\nreturn total_fold_array\n'
"from keras.applications import *\nfrom keras.models import Model\n\nbase_model = Xception(input_shape=(img_width, img_height, 3), weights='imagenet', include_top=False\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(2, activation='softmax')(x)\nmodel = Model(base_model.input, predictions)\n# freezing the base layer weights\nfor layer in base_model.layers:\n    layer.trainable = False\n"
"for row in pd.DataFrame(state_mean_series).itertuples(): #row format is [STATE, mean_value]\n    self.group_averages[row[0]] = row[1]\n\nprediction = dictionary.get(row.STATE, None) # None is the default value here in case the 'AS' doesn't exist. you may replace it with what ever you want\n"
'&gt;&gt;&gt; df = pd.DataFrame({\n...     "ProArticle": ["a", "b", "c", "d"],\n...     "Vector": [[0, 0], [1, 1], [2, 2], [3, 3]]\n... })\n&gt;&gt;&gt; vs = np.vstack(df.Vector)\n&gt;&gt;&gt; vs\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3]])\n\n&gt;&gt;&gt; kmeans = KMeans(n_clusters=2)\n&gt;&gt;&gt; kmeans.fit_predict(vs)\narray([1, 1, 0, 0], dtype=int32)\n\n&gt;&gt;&gt; df.Vector.apply(pd.Series)\n   0  1\n0  0  0\n1  1  1\n2  2  2\n3  3  3\n'
"list1 = [(10,15),(40,50),(10,60)]\nlist2 = [(12,17),(38,48),(12,63),(11,17),(10,59)]\ndict1, dict2 = {}, {}\nfor i,j in list1:\n    for k,l in list2:\n        dist = ((i-k)**2 + (j-l)**2)**0.5\n        dict1[(k,l)] = int(dist)           # should use round(dist) instead\n    min_dist = min(dict1.values())\n    points = [keys for keys, values in dict1.items() if values == min_dist]\n    dict2[(i,j)] = points\n\nfor k in dict2:\n    print(k,'-',dict2[k])\n\n(10, 15) - [(12, 17), (11, 17)]\n(40, 50) - [(38, 48)]\n(10, 60) - [(10, 59)]\n"
'from sklearn.model_selection import train_test_split\nfrom urllib.request import urlopen\nfrom functools import reduce\nfrom os.path import exists\nfrom os import listdir\nfrom sys import exit\nimport tensorflow as tf\nimport pandas as pd\nimport pickle\nimport re\n\n# Specify dataframe path\ndf_path = \'df.pickle\'\n# Check if the file exists\nif not exists(df_path):\n  # Specify url of the dataframe binary\n  url = \'https://www.dropbox.com/s/76hibe24hmpz3bk/df.pickle?dl=1\'\n  # Read the byte content from url\n  content = urlopen(url).read()\n  # Write to a file to save up time\n  with open(df_path, \'wb\') as file: file.write(pickle.dumps(content))\n  # Unpickle the dataframe\n  df = pickle.loads(content)\nelse:\n  # Load the pickle dataframe\n  df = pickle.load(open(df_path, \'rb\'))\n\n# Useful variables\nMAX_NUM_WORDS = 50000                        # Vocabulary size for our tokenizer\nMAX_SEQ_LENGTH = 600                         # Maximum length of tokens (for padding later)\nEMBEDDING_SIZE = 256                         # Embedding size (Tweak to improve accuracy)\nOUTPUT_LENGTH = len(df[\'Category\'].unique()) # Number of class to be classified\n\n# Create our tokenizer\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, lower=True)\n# Fit our tokenizer with words/tokens\ntokenizer.fit_on_texts(df[\'Content_Parsed\'].values)\n# Get our token vocabulary\nword_index = tokenizer.word_index\nprint(\'Found {} unique tokens\'.format(len(word_index)))\n\n# Parse our text into sequence of numbers using our tokenizer\nX = tokenizer.texts_to_sequences(df[\'Content_Parsed\'].values)\n# Pad the sequence up to the MAX_SEQ_LENGTH\nX = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SEQ_LENGTH)\nprint(\'Shape of feature tensor: {}\'.format(X.shape))\n\n# Convert our labels into dummy variable (More info on the link provided above)\nY = pd.get_dummies(df[\'Category\']).values\nprint(\'Shape of label tensor: {}\'.format(Y.shape))\n\n# Split our features and labels into test and train dataset\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)\n\n# Creating our model\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(MAX_NUM_WORDS, EMBEDDING_SIZE, input_length=MAX_SEQ_LENGTH))\nmodel.add(tf.keras.layers.SpatialDropout1D(0.2))\n# The number 64 could be changed based on your model performance\nmodel.add(tf.keras.layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n# Our output layer with length similar to the OUTPUT_LENGTH\nmodel.add(tf.keras.layers.Dense(OUTPUT_LENGTH, activation=\'softmax\'))\n# Compile our model with "categorical_crossentropy" loss function\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n\n# Model variables\nEPOCHS = 100                          # Number of cycle to run (The early stopping may stop the training process accordingly)\nBATCH_SIZE = 64                       # Batch size (Tweaking this may improve model performance a bit)\ncheckpoint_path = \'model_checkpoints\' # Checkpoint path of our model\n\n# Use GPU if available\nwith tf.device(\'/GPU:0\'):\n  # Fit/Train our model\n  history = model.fit(\n    x_train, y_train,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_split=0.1,\n    callbacks=[\n      tf.keras.callbacks.EarlyStopping(monitor=\'val_loss\', min_delta=0.0001),\n      tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        monitor=\'val_acc\', \n        save_best_only=True, \n        save_weights_only=False\n      )\n    ],\n    verbose=1\n  )\n'
'df\n    col1   col2  col3\n0   blue    car   new\n1  green  truck  used\n2    red    van   new\n\nct = ColumnTransformer([(&quot;onehot&quot;,OneHotEncoder(),[0,1,2])])\n\nct.fit_transform(df.values)\narray([[1., 0., 0., 1., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 1., 1., 0.]])\n\no = OneHotEncoder()\no.fit_transform(df).toarray()\n\narray([[1., 0., 0., 1., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 1., 0., 0., 1.],\n       [0., 0., 1., 0., 0., 1., 1., 0.]])\n'
'df[\'group\'] = df.index//5 # add extra column to hold the group value\nnew_df = df.groupby(\'group\').identifier.apply(list).apply(pd.Series)\ndf.drop(\'group\', axis=1) # drop the extra column that was created.\nprint(new_df.head())\n\ndf = pd.DataFrame(np.random.randint(0,1000,size=6026), columns=["identifier"])\ndf.head()\n\nidentifier\n0   752\n1   14\n2   184\n3   139\n4   37\n\ndf[\'group\'] = df.index//5\ndf1 = df.groupby(\'group\').identifier.apply(list).apply(pd.Series).fillna(0)\ndf1 = df1.astype(\'int32\')\ndf1.head()\n\n    0   1   2   3   4\ngroup                   \n0   752 14  184 139 37\n1   716 499 902 54  565\n2   74  427 939 380 244\n3   651 803 97  78  492\n4   169 376 737 342 616\n\ndf[\'group\'] = df.index//5\ndf1 = pd.DataFrame(df.groupby(\'group\').identifier.apply(list))\ndf1.head()\n\n    identifier\ngroup   \n0   [752, 14, 184, 139, 37]\n1   [716, 499, 902, 54, 565]\n2   [74, 427, 939, 380, 244]\n3   [651, 803, 97, 78, 492]\n4   [169, 376, 737, 342, 616]\n'
'[0.0, -2.2, 1.3, -0.45]\n\nclf.coeffs_\n\n[\n    [0.1, 2.2, -0.1, 0.133], # Features of class 0\n    [-2, -1.1, 0, 4.56],\n    [-0.1, 0, 0.3, 0.4],\n    [3.3, -2, 15, -9.4],\n    [0.45, 0.5, 0.66, 5.5],\n]\n\n[3.3, -2, 15, -9.4]\n'
'&gt;&gt;&gt;from sklearn import preprocessing\n&gt;&gt;&gt;data2 = preprocessing.scale(dataarray)\n'
'def micro_average_precision_score(y_true, y_pred):\n    metrics.precision_score(y_true, y_pred, average="micro")\n\nfrom sklearn.metrics.score import make_scorer\nscorer = make_scorer(micro_average_precision_score, greater_is_better=True)\n\nscorer = make_scorer(metrics.precision_score, \n                     greater_is_better=True, average="micro")\n'
'import pickle\n\nmodel = pickle.load(path)\n\ndef predict(val):\n    return model.predict(val)\n'
'[Category, DayOfWeek, PdDistrict, Resolution] {Descript, Address}\n\nnew_domain = Orange.data.Domain(list(data.domain.attributes[1:]), \n             data.domain.attributes[0], \n             metas=data.domain.metas)\n\n[DayOfWeek, PdDistrict, Resolution | Category] {Descript, Address}\n'
'def nextbatch(x,i,j):\n    return x[i:j,...]\n\nx_train = np.reshape(x_train,(batch_size, n_steps, n_input))\n'
"    plt.scatter(test[:, [1]], test[:, [0]], c='r', s=1)\n"
"y = df[df.columns[60]].apply(lambda x: 0 if x == 'R' else 1).values.reshape(-1, 1)\nencoder = OneHotEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\n"
'# note: [rows, columns, channel]\noutput = np.empty((mr - kr + 1, mc - kc + 1, 3))\n\n# note: `row` loops over matrix rows, `column` - over columns\nfor row in range(mr - kr + 1):\n  for column in range(mc - kc + 1):\n    for dim in range(3):\n      output[row][column][dim] = (matrix[row:row + kr, column:column + kc] * kernel).sum()\n\nprint(output)\n'
'import catboost\nimport numpy as np\nx_train3 = np.array([[1,2,3,], [2,3,4], [3,4,5]])\nx_train1 = np.array([[1], [2], [3]])\ny_train = np.array([1,2,3])\nx_test3_2 = np.array([[4,5,6], [5,6,7]])\nx_test3_1 = np.array([[4,5,6,]])\nx_test1_2 = np.array([[4], [5]])\nx_test1_1 = np.array([[4]])\nmodel3 = catboost.CatBoostRegressor().fit(x_train3, y_train)\nmodel1 = catboost.CatBoostRegressor().fit(x_train1, y_train)\nprint(model3.predict(x_test3_2)) # OK\nprint(model3.predict(x_test3_1)) # OK\nprint(model1.predict(x_test1_2)) # OK\nprint(model1.predict(x_test1_1)) # Throws an error!\n'
'regressor.predict([years_of_xp])\n'
'sent = nlp("it is very good")\nfor token in sent:\n    print(token, token.dep_)\n\n&gt;&gt; it nsubj\n&gt;&gt; is ROOT\n&gt;&gt; very advmod\n&gt;&gt; good acomp\n\nsent = nlp("do you like it")\nfor token in sent:\n    print(token, token.dep_)\n\n&gt;&gt; do aux\n&gt;&gt; you nsubj\n&gt;&gt; like ROOT\n&gt;&gt; it dobj\n'
"xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n"
"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nimport pandas as pd\n\ncsv = 'C:\\df_low_X.csv'\ndf = pd.read_csv(csv, header=None)\nprint(df)\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nclf = KNeighborsClassifier()\nkf = StratifiedKFold(n_splits = 3)\n\nac = []\ncm = []\n\nfor train_index, test_index in kf.split(X,y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print(X_train, X_test)\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    ac.append(accuracy_score(y_test, y_pred))\n    cm.append(confusion_matrix(y_test, y_pred))\nprint(ac)\nprint(cm)\n\n# ac\n[0.25, 0.75, 0.5]\n\n# cm\n[array([[1, 1],\n       [2, 0]], dtype=int64), \n\narray([[1, 1],\n       [0, 2]], dtype=int64),\n\n array([[0, 2],\n       [0, 2]], dtype=int64)]\n"
"# unchanged from your code\ntr_datagen = ImageDataGenerator(\n    featurewise_center=False,\n    featurewise_std_normalization=False,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\n# create new generator for validation\nval_datagen = ImageDataGenerator()    # don't perform augmentation on validation data\n\n\n# compute quantities required for featurewise normalization\n# (std, mean, and principal components if ZCA whitening is applied)\n\ntr_datagen.fit(x_train)    # can leave this out if not standardising or whitening \nval_datagen.fit(x_val)     # can leave this out if not standardising or whitening\n\n# alternative to model.fit_generator\nfor e in range(epochs):\n    print('Epoch', e)\n    batches = 0\n\n    # combine both generators, in python 3 using zip()\n    for (x_batch, y_batch), (val_x, val_y) in zip(\n                                 tr_datagen.flow(x_train, y_train, batch_size=32),\n                                 val_datagen.flow(x_val, y_val, batch_size=32)):\n        model.fit(x_batch, y_batch, validation_Data=(val_x, val_y))\n        batches += 1\n        if batches &gt;= len(x_train) / 32:\n            # we need to break the loop by hand because\n            # the generator loops indefinitely\n            break\n"
'weights = [layer.get_weights() for layer in model.layers]\n'
"kmeans = KMeans(n_clusters=3)\nkmeans = kmeans.fit(mat[:, 1])] \nlabels = kmeans.predict(mat[:, 1])\nplt.scatter(mat[:,0],mat[:,1], c=labels, cmap='rainbow')\n"
'# (39000, 64, 64, 1)\ngray_dataset_1c = np.sum((dataset / 255.) * [0.299, 0.587, 0.114], axis=-1, keepdims=True)\n# (39000, 64, 64, 3)\ngray_dataset = np.tile(gray_dataset_1c, (1, 1, 1, 3))\n\n# (39000, 64, 64)\ngray_dataset_1c = np.dot((dataset / 255.), [0.299, 0.587, 0.114])\n# Since Python 3.5 this can be written like this:\ngray_dataset_1c = (dataset / 255.) @ [0.299, 0.587, 0.114]\n# (39000, 64, 64, 3)\ngray_dataset = np.tile(gray_dataset_1c[..., np.newaxis], (1, 1, 1, 3))\n'
'class PreProcessing(BaseEstimator, TransformerMixin):\n  def __init__(self):\n    self.encoder = ce.BinaryEncoder(cols=selectedCols)\n\n  def fit(self, df, **kwargs):\n    self.encoder.fit(df)\n\n  def transform(self, df):\n    # ...\n    # No fitting, just transform\n    df = self.encoder.transform(df)\n    return df\n\npipe = make_pipeline(PreProcessing(),\n                     BinaryEncoder(cols=selectedCols),\n                     SelectKBest(f_classif,k=23),\n                     RandomForestClassifier())\n'
'df_test = df_train[df_train.customer_id.isin(test_customer_id_set)]\ndf_train = df_train[~df_train.customer_id.isin(test_customer_id_set)]\n'
"from sklearn.pipeline import Pipeline\n\n# takes a list of tuples where the first arg is the step name,\n# and the second is the estimator itself.\npipe = Pipeline([\n    ('cvec', CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')),\n    ('clf', naive_bayes.MultinomialNB())\n])\n\n# you can fit a pipeline in the same way you would any other estimator,\n# and it will go sequentially through every stage\npipe.fit(train_x, train_y)\n\n# you can produce predictions by feeding your test data into the pipe\npipe.predict(test_x)\n\njoblib.dump(pipe, 'models/NB-COUNT.pkl')\nloaded_model = joblib.load('models/NB-COUNT.pkl')\nloaded_model.predict(test_df)\n"
'disjoint_set = {}\nvalue_lookup = {}\nfor row in range(len(list_1)):\n  disjoint_set[row] = row  # Mark it as independent set.\n  for key, value in list_1[row].items():  # not sure how to get key value with pandas\n    if (key, value) not in value_lookup:\n      value_lookup[(key, value)] = row\n    else:\n      other_row = value_lookup[(key, value)]\n      actual_other = recursive_lookup(disjoint_set, other_row)\n      actual_row = recursive_lookup(disjoint_set, row)\n      disjoint_set[actual_row] = actual_other \n\n def recursive_lookup(disjoint_set, row):\n   if disjoin_set[row] != row:\n     disjoint_set[row] = recursive_lookup(disjoint_set, disjoint_set[row])\n   return disjoint_set[row]\n'
"from keras import backend as K\nimport tensorflow as tf\n\ndef custom_metric(y_true, y_pred):\n    tr = tf.floor(K.argmax(y_true, axis=-1) / 2)\n    pr = tf.floor(K.argmax(y_pred, axis=-1) / 2)\n    return K.cast(K.equal(tr, pr), K.floatx())\n\nmodel = Sequential()\nmodel.add(Dense(4, activation='softmax', input_shape=(2,)))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', custom_metric])\n\nimport numpy as np\ndata = np.array([1, 2]).reshape(1, 2)\n\nprint(model.predict(data))\n# prints: [0.04662106, 0.8046941 , 0.07660434, 0.0720804 ] \n\ntrue_labels = np.array([1, 0, 0, 0]).reshape(1,4)\nprint(model.evaluate(data, true_labels))    # gives: [3.0657029151916504, 0.0, 1.0]  \n\ntrue_labels = np.array([0, 1, 0, 0]).reshape(1,4)\nprint(model.evaluate(data, true_labels))    # gives: [0.21729297935962677, 1.0, 1.0]\n"
"# define the generator\ndatagen = ImageDataGenerator(...)\n\n# assign class_mode to 'sparse' to make our work easier\ngen = datagen.flow_from_directory(..., class_mode= 'sparse')\n\n# define a mapping from old classes to new classes (i.e. 0,1 -&gt; 0 and 2,3 -&gt; 1)\nold_to_new = np.array([0, 0, 1, 1])\n\n# the wrapping generator\ndef new_gen(gen):\n    for data, labels in gen:\n        labels = old_to_new[labels]\n        # now you can call np_utils.to_categorical method \n        # if you would like one-hot encoded labels\n        yield data, labels\n\n# ... define your model\n\n# fit the model\nmodel.fit(new_gen(gen), ...)\n"
'submission = pd.DataFrame({ \'PassengerId\': test_data.passengerid.values, \'Survived\': test_predictions })\nsubmission.to_csv("my_submission.csv", index=False)\n'
"df=pd.DataFrame({'a':[1,2],'b':[2,3],'c':[1,9]})\ntarget_col=['c'] # column 'c' is the target column here\nX=df[list(set(df.columns).difference(target_col))].values # X-&gt; features\nY=df[target_col].values # Y -&gt; target\n\ndata=df.values\nX=data[:,:2] # from column 1 upto second last column including all the rows\nY=data[:,2] # only last column(target) including all the rows\n"
'#simple feed forward\ndef feedforward(self):\n    self.layer1 = sigmoid(np.dot(self.input, self.weights1) + self.bias1)\n    self.layer2 = sigmoid(np.dot(self.layer1, self.weights2) + self.bias2)\n    self.layer3 = sigmoid(np.dot(self.layer1, self.weights3) + self.bias3)\n    self.output = sigmoid(np.dot(self.layer2, self.weights4) + self.bias4)\n\n#simple feed forward\ndef feedforward(self):\n    self.layer1 = sigmoid(np.dot(self.input, self.weights1) + self.bias1)\n    self.layer2 = sigmoid(np.dot(self.layer1, self.weights2) + self.bias2)\n    self.layer3 = sigmoid(np.dot(self.layer2, self.weights3) + self.bias3)\n    self.output = sigmoid(np.dot(self.layer3, self.weights4) + self.bias4)\n'
"# loss _should_(?) be the same for 'channels_first' and 'channels_last' data_format\n# test example_1\ne1 = np.isclose(l1, t_l1.T).all()\n# test example 2\ne2 = np.isclose(l2, t_l2.T).all()\n\n# loss calculated for each example and then batched together should be the same \n# as the loss calculated on the batched examples\nea = np.isclose(np.array([l1, l2]), bl).all()\nt_ea = np.isclose(np.array([t_l1, t_l2]), t_bl).all()\n\n# loss calculated on the batched examples for 'channels_first' should be the same\n# as loss calculated on the batched examples for 'channels_last'\neb = np.isclose(bl, np.transpose(t_bl, (0, 2, 1))).all()\n\n\ne1, e2, ea, t_ea, eb\n# (True, True, True, True, True)\n\nl_e1 = np.isclose(tf_l1, rm_l1)\nl_e2 = np.isclose(tf_l2, rm_l2)\nl_eb = np.isclose(tf_bl, rm_bl)\n\nl_t_e1 = np.isclose(tf_t_l1, rm_t_l1)\nl_t_e2 = np.isclose(tf_t_l2, rm_t_l2)\nl_t_eb = np.isclose(tf_t_bl, rm_t_bl)\n\nl_e1, l_e2, l_eb, l_t_e1, l_t_e2, l_t_eb\n# (True, True, True, True, True, True)\n"
" skimage.feature.hog(img, orientations=9, pixels_per_cell=(img.shape[0]/8,img.shape[1]/8),cells_per_block=(4, 4), block_norm='L2-Hys', visualize=False, visualise=None, transform_sqrt=False, feature_vector=True, multichannel=None)\n"
"img = img.astype('float32') / 255.\n"
'import pickle\n\n# building and training of the model as you have done ...\n\n# store all the data we need later: model and tokenizer    \nmodel.save("m.hdf5")\nwith open(\'tokenizer.pkl\', \'wb\') as handler:\n    pickle.dump(tokenizer, handler)\n\nimport pickle\n\nmodel = load_model(\'m.hdf5\')\nwith open(\'tokenizer.pkl\', \'rb\') as handler:\n    tokenizer = pickle.load(handler)\n\nlist_sentences_train = ["I love you Stackoverflow"]\n\n# use the the same tokenizer instance you used in training phase\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nmaxlen = 200\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n\nprint(model.predict(X_t))\n'
'sentence_filtered = " ".join([word for word in sentence.split() if word.lower() not in vector_of_words])\n'
"from keras.layers import GlobalAveragePooling2D\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3),input_shape=(300,300,3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(GlobalAveragePooling2D()) \nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nfrom keras.optimizers import RMSprop\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.0001),metrics=['accuracy'])\n"
'self.wg_diff = np.zeros((mem_cell_ct, concat_len)) \n\nself.wg -= lr * self.wg_diff\n\nself.wg_diff = np.zeros_like(self.wg)\n'
"x = dfCardio.drop('cardio',axis = 1, inplace=False)\n"
'%%timeit -r 10 -n 10\nA[A[:,-1].argsort()]\n\n38.6 µs ± 23 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n\n%%timeit -r 10 -n 10\nsorted(A, key = lambda x: x[-1])\n\n69.6 µs ± 34.8 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n\ntensor([[0.5951, 0.9315, 0.6548, 1.0000],\n        [0.7704, 0.0720, 0.0330, 2.0000],\n        [0.9133, 0.5071, 0.6222, 3.0000]])\n\n%%timeit -r 10 -n 10\na, b = torch.sort(A, dim=-2)\n\nThe slowest run took 8.45 times longer than the fastest. This could mean that an intermediate result is being cached.\n14.3 µs ± 18.1 µs per loop (mean ± std. dev. of 10 runs, 10 loops each)\n'
"df = pd.DataFrame(np.random.randint(0,100,(5,5)), index=[*'abcde'], columns=[*'ABCDE'])\n\ndf.iloc[:,:-1]\n\n    A   B   C   D\na  79  23   9  89\nb  67  60  32  82\nc  66  18  41  67\nd  90  51  63  29\ne  34  65  82  82\n\ndf.iloc[:, 0]\n\na    79\nb    67\nc    66\nd    90\ne    34\nName: A, dtype: int3\n"
"from sklearn.preprocessing import OneHotEncoder\n\nX_train = pd.Series(['West Miami', 'Biscayne Park', 'Medley'])\noh = OneHotEncoder(handle_unknown='ignore')\noh.fit(X_train.values[:,None])\n\noh.transform(X_train.values[:,None]).toarray()\n\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 1., 0.]])\n\nX_test = pd.Series(['West Miami', 'Biscayne Park', 'Atlanta'])\n\noh.transform(X_test.values[:,None]).toarray()\n\narray([[0., 0., 1.],\n       [1., 0., 0.],\n       [0., 0., 0.]])\n"
'centroids_x = centroids[:,0]\ncentroids_y = centroids[:,1]\n\nplt.scatter(centroids_x,centroids_y,marker = "x", s=150,linewidths = 5, zorder = 10, c=[\'green\', \'red\',\'blue\'])\n'
'def _initialize(self, y, layer_units):\n\n    # set all attributes, allocate weights etc for first call\n    # Initialize parameters\n    self.n_iter_ = 0\n    self.t_ = 0\n    self.n_outputs_ = y.shape[1]\n\n    # Compute the number of layers\n    self.n_layers_ = len(layer_units)\n\n    # Output for regression\n    if not is_classifier(self):\n        self.out_activation_ = \'identity\'\n    # Output for multi class\n    elif self._label_binarizer.y_type_ == \'multiclass\':\n        self.out_activation_ = \'softmax\'\n    # Output for binary class and multi-label\n    else:\n        self.out_activation_ = \'logistic\'\n\ndef predict_proba(self, X):\n    """Probability estimates.\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The input data.\n    Returns\n    -------\n    y_prob : ndarray of shape (n_samples, n_classes)\n        The predicted probability of the sample for each class in the\n        model, where classes are ordered as they are in `self.classes_`.\n    """\n    check_is_fitted(self)\n    y_pred = self._predict(X)\n\n    if self.n_outputs_ == 1:\n        y_pred = y_pred.ravel()\n\n    if y_pred.ndim == 1:\n        return np.vstack([1 - y_pred, y_pred]).T\n    else:\n        return y_pred\n'
"from sklearn import set_config\n\nset_config(print_changed_only=True)\n\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import TimeSeriesSplit, cross_validate\nfrom sklearn import metrics\nfrom sklearn.metrics import balanced_accuracy_score, make_scorer\nfrom sklearn.pipeline import Pipeline\n\nX, y = make_classification(\n    n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n\n# create pipeline\nrfecv_model = RFECV(estimator=DecisionTreeClassifier())\nmodel = DecisionTreeClassifier()\npipeline = Pipeline(steps=[('s', rfecv_model), ('m', model)])\n\n# make balanced scorer\nscorer = make_scorer(balanced_accuracy_score)\n\n# evaluate model\ncv = TimeSeriesSplit(n_splits=3)\nresult = cross_validate(pipeline, X, y, scoring=scorer,\n                          cv=cv, return_estimator=True)\n\n{'fit_time': array([0.07009673, 0.09101987, 0.11680794]),\n 'score_time': array([0.00072193, 0.00065613, 0.00060487]),\n 'estimator': (Pipeline(steps=[('s', RFECV(estimator=DecisionTreeClassifier())),\n                  ('m', DecisionTreeClassifier())]),\n  Pipeline(steps=[('s', RFECV(estimator=DecisionTreeClassifier())),\n                  ('m', DecisionTreeClassifier())]),\n  Pipeline(steps=[('s', RFECV(estimator=DecisionTreeClassifier())),\n                  ('m', DecisionTreeClassifier())])),\n 'test_score': array([0.812     , 0.83170092, 0.8510502 ])}\n\nfor iter, pipe in enumerate(result['estimator']):\n    print(f'Iteration no: {iter}')\n    for i in range(X.shape[1]):\n        print('Column: %d, Selected %s, Rank: %d' %\n            (i, pipe['s'].support_[i], pipe['s'].ranking_[i]))\n\n# output\nIteration no: 0\nColumn: 0, Selected False, Rank: 4\nColumn: 1, Selected True, Rank: 1\nColumn: 2, Selected True, Rank: 1\nColumn: 3, Selected True, Rank: 1\nColumn: 4, Selected False, Rank: 3\nColumn: 5, Selected False, Rank: 5\nColumn: 6, Selected True, Rank: 1\nColumn: 7, Selected True, Rank: 1\nColumn: 8, Selected True, Rank: 1\nColumn: 9, Selected False, Rank: 2\nIteration no: 1\nColumn: 0, Selected False, Rank: 2\nColumn: 1, Selected False, Rank: 4\nColumn: 2, Selected True, Rank: 1\nColumn: 3, Selected True, Rank: 1\nColumn: 4, Selected True, Rank: 1\nColumn: 5, Selected False, Rank: 6\nColumn: 6, Selected True, Rank: 1\nColumn: 7, Selected False, Rank: 5\nColumn: 8, Selected True, Rank: 1\nColumn: 9, Selected False, Rank: 3\nIteration no: 2\nColumn: 0, Selected True, Rank: 1\nColumn: 1, Selected False, Rank: 4\nColumn: 2, Selected True, Rank: 1\nColumn: 3, Selected True, Rank: 1\nColumn: 4, Selected True, Rank: 1\nColumn: 5, Selected False, Rank: 3\nColumn: 6, Selected False, Rank: 2\nColumn: 7, Selected False, Rank: 5\nColumn: 8, Selected True, Rank: 1\nColumn: 9, Selected True, Rank: 1\n"
"type(survey.target)\npandas.core.series.Series\n\ntype(survey.target.values)\nnumpy.ndarray\n\ny = survey.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\ntrain_dataset = X_train.copy()\ntrain_dataset.insert(0, &quot;WAGE&quot;, y_train)\nsns.pairplot(train_dataset, kind='reg', diag_kind='kde')\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LinearRegression\n\ndat = load_iris(as_frame=True).frame\n\nX = dat[['sepal length (cm)','sepal width (cm)','petal length (cm)']]\ny = dat[['petal width (cm)']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nLR = LinearRegression()\nLR.fit(X_train,y_train)\nplt.scatter(x=y_test,y=LR.predict(X_test))\n"
'from sklearn.model_selection import StratifiedKFold\ndef stratified_cv(X, y, clf_class, shuffle=True, n_folds=10, **kwargs):\n    stratified_k_fold = StratifiedKFold(n_splits=n_folds, shuffle=shuffle)\n    y_pred = y.copy()\n    # ii -&gt; train\n    # jj -&gt; test indices\n    for ii, jj in stratified_k_fold.split(X,y): \n        X_train, X_test = X[ii], X[jj]\n        y_train = y[ii]\n        clf = clf_class(**kwargs)\n        clf.fit(X_train,y_train)\n        y_pred[jj] = clf.predict(X_test)\n    return y_pred\n'
"class BaseDataRepository(ABC): \n\n    @abstractmethod\n    def get_all_ids(self) -&gt; List[int]: \n        pass\n\n    @abstractmethod\n    def get_data_from_id(self, _id: int) -&gt; object: \n        pass\n\nclass InMemoryDataRepository(BaseDataRepository): \n    def __init__(self, ids, data): \n        self.ids: List[int] = ids\n        self.data: Dict[int, object] = data\n\n    def get_all_ids(self) -&gt; List[int]: \n        return list(self.ids)\n\n    def get_data_from_id(self, _id: int) -&gt; object: \n        return self.data[_id]\n\nclass ConvertIDsToLoadedData(BaseStep): \n    def _handle_transform(self, data_container: DataContainer, context: ExecutionContext): \n        repo: BaseDataRepository = context.get_service(BaseDataRepository)\n        ids = data_container.data_inputs\n\n        # Replace data ids by their loaded object counterpart: \n        data_container.data_inputs = [repo.get_data_from_id(_id) for _id in ids]\n\n        return data_container, context\n\ncontext = ExecutionContext('caching_folder').set_service_locator({\n    BaseDataRepository: InMemoryDataRepository(ids, data)  # or insert here any other replacement class that inherits from `BaseDataRepository` when you'll change the database to a real one (e.g.: SQL) rather than a cheap &quot;InMemory&quot; stub. \n})\n"
"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv('housing.csv')\n\nprices = data['median_house_value']\nfeatures = data.drop(['median_house_value', 'ocean_proximity'], axis = 1)\n\nprices.shape\n(20640,)\n\nfeatures.shape\n(20640, 8)\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.2, random_state=42)\n\nX_train = X_train.dropna()\ny_train = y_train.dropna()\n\ny_train.shape\n(16512,)\n\nX_train.shape\n(16512, 8)\n\nmodel.fit(X_train, y_train)\n\nDecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')\n"
"import numpy as np\nfrom sklearn import metrics\n\nx, y = test_generator.next()\nprediction = model.predict(x)\n\npredict_label1 = np.argmax(prediction, axis=-1)\ntrue_label1 = np.argmax(y, axis=-1)\n\ny = np.array(true_label1)\n\nscores = np.array(predict_label1)\nfpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=9)\nroc_auc = metrics.auc(fpr, tpr)\n\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n     lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC)')\nplt.legend(loc=&quot;lower right&quot;)\nplt.show()\n"
"for i in range(1, len(dense_layers)):\n   \n    layer = Dense(dense_layers[i],\n                  activity_regularizer=l2(reg_layers[i]),\n                  activation='relu',\n                  name='layer%d' % i)\n    mlp_vector = layer(mlp_vector)\n    mlp_vector = Dropout(0.2)(mlp_vector)\n"
'def mask_layer1(tensor):\n    return layers.Multiply()([tensor, tf.ones([1, 128])])\n\ndef mask_layer2(tensor):\n    return layers.Multiply()([tensor, tf.ones([1, 128])*1.2])\n\n\ndef get_model(mask_kind):\n\n    inp = keras.Input(shape=(64, 101, 1), name=&quot;input&quot;)\n    \n    x = layers.Conv2D(256, kernel_size=(3, 3), kernel_regularizer=l2(1e-6), \n                      strides=(3, 3), padding=&quot;same&quot;)(inp)\n    x = layers.LeakyReLU(alpha=0.3)(x)\n    x = layers.Conv2D(128, kernel_size=(3, 3), kernel_regularizer=l2(1e-6), \n                      strides=(3, 3), padding=&quot;same&quot;)(x)\n    x = layers.LeakyReLU(alpha=0.3)(x)\n    x = layers.Flatten()(x)\n    x = layers.Dense(512)(x)\n    x = layers.LeakyReLU(alpha=0.3)(x)\n    x = layers.Dense(256)(x)\n    x = layers.LeakyReLU(alpha=0.3)(x)\n    x = layers.Dense(128, name=&quot;output1&quot;)(x)\n    \n    if mask_kind == 1:\n        mask = layers.Lambda(mask_layer1, name=&quot;lambda_layer&quot;)(x)\n    elif mask_kind == 2:\n        mask = layers.Lambda(mask_layer2, name=&quot;lambda_layer&quot;)(x)\n    else:\n        mask = x\n    \n    out = layers.Dense(40000, name=&quot;output2&quot;)(mask)\n\n    model = keras.Model(inp, [mask, out], name=&quot;2_out_model&quot;)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=&quot;mean_squared_error&quot;)\n    \n    return model\n\n\nmodel1 = get_model(mask_kind=1)\nmodel1.fit(...)\n\nmodel2 = get_model(mask_kind=2)\nmodel2.set_weights(model1.get_weights())\nmodel2.fit(...)\n'
"import pandas as pd\n\ndata = pd.DataFrame({'product_code': ['1', '2', '3', '4'],\n                 'technology_type': ['4G, 4G LAA, 5G NR',\n                            '4G,4G CBRS,5G FIXED',\n                            '4G, 5G, NR',\n                            '4G, NR']},\n                columns=['product_code', 'technology_type'])\n\nproduct_code    technology_type\n1               4G, 4G LAA, 5G NR\n2               4G,4G CBRS,5G FIXED\n3               4G, 5G, NR\n4               4G, NR\n\ncleaned = data.set_index('product_code').technology_type.str.split(',', expand=True).stack()\n\nproduct_code   \n1             0          4G\n              1      4G LAA\n              2       5G NR\n2             0          4G\n              1     4G CBRS\n              2    5G FIXED\n3             0          4G\n              1          5G\n              2          NR\n4             0          4G\n              1          NR\n\ntechnology_type_dummies = pd.get_dummies(cleaned).groupby(level=0).sum()\nnewData = data.merge(technology_type_dummies, left_on='product_code', right_index=True)\n\nproduct_code    technology_type     4G LAA  5G  5G NR   NR     4G   4G CBRS    5G FIXED\n1               4G, 4G LAA, 5G NR   1       0   1       0      1    0          0\n2               4G,4G CBRS,5G FIXED 0       0   0       0      1    1          1\n3               4G, 5G, NR          0       1   0       1      1    0          0\n4               4G, NR              0       0   0       1      1    0          0\n\nnewData.columns = newData.columns.str.strip()\n"
"self.docs_list.append(self.__filter__(d))\n\na = np.zeros(len(self.word_dict), dtype='i2')\nfor word in split_into_words(d):\n    try:\n        idx = self.word_dict[word]\n    except KeyError:\n        idx = len(self.word_dict)\n        self.word_dict[word] = idx\n        np.resize(a, idx+1)\n        a[idx] = 1\n    else:\n        a[idx] += 1\nself.doc_vectors.append(a)\n"
"get_freq = lambda ts: (ts.last() - ts.first()).astype('timedelta64[h]') / len(ts)\ntwitterDataFrame['frequency'] = twitterDataFrame.groupby('UserID')['CreatedAtForCalculations'].transform(get_freq)\n"
"import codecs\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB,BernoulliNB\nfrom sklearn import svm\nfrom sklearn.grid_search import GridSearchCV\n\ntestfile = 'testing_words.txt'\n\nword_vectorizer = CountVectorizer(analyzer='word')  \ntrainset = word_vectorizer.fit_transform(codecs.open(trainfile,'r','utf8'))\ntags = training_labels\n\n\nmnb = svm.LinearSVC() # or any other classifier\n# check out the sklearn online docs to see what params choice we have for your\n# particular choice of estimator, for SVM, C, class_weight are important ones to tune \nparams_space = {'C': np.logspace(-5, 0, 10), 'class_weight':[None, 'auto']}\n# build a grid search cv, n_jobs=-1 to use all your processor cores\ngscv = GridSearchCV(mnb, params_space, cv=10, n_jobs=-1)\n# fit the model\ngscv.fit(trainset, tags)\n# give a look at your best params combination and best score you have\ngscv.best_estimator_\ngscv.best_params_\ngscv.best_score_\n\n\ncodecs.open(testfile,'r','utf8')\ntestset = word_vectorizer.transform(codecs.open(testfile,'r','utf8'))\nresults = gscv.predict(testset)\n\nprint results\n"
'X=np.array(arrFinal[:,1:-17])\nX=X.astype(float)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nXtest=np.array(X)\n\nY=np.array(arrFinal[:,522:]).astype(float)\nclf = OneVsRestClassifier(SVC(kernel=\'linear\', C=100))\nclf.fit(X, Y)\nans=clf.predict(Xtest) \nprint(ans)\nprint("\\n\\n\\n")\n'
'import scipy.ndimage\nimport scipy.misc\n\ndata = scipy.misc.imread(...)\nassert data.ndim == 2, "Image must be monochromatic"\n\n# finds and number all disjoint white regions of the image\nis_white = data &gt; 128\nlabels, n = scipy.ndimage.measurements.label(is_white)\n\n# get a set of all the region ids which are on the edge - we should not fill these\non_border = set(labels[:,0]) | set(labels[:,-1]) | set(labels[0,:]) | set(labels[-1,:])\n\nfor label in range(1, n+1):  # label 0 is all the black pixels\n    if label not in on_border:\n        # turn every pixel with that label to black\n        data[labels == label] = 0\n'
'np.dot(clf.coef_, x) - clf.intercept_ = 0\n'
"for weight in layer_weights:\n    tf.scalar_summary([ ['%s_w%d%d' % (weight.name, i,j) for i in xrange(len(layer_weights))]  for j in xrange(5) ], weight)\n"
"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nsvr_poly = make_pipeline(StandardScaler(), SVR(kernel='poly', C=1e3, degree=2))\ny_poly = svr_poly.fit(x_train,y_train).predict(x_train)\n"
'import csv\nimport math\ndef loadCsv(filename):\n    lines = csv.reader(open(filename, "r"))\n    dataset = list(lines)\n    for i in range(len(dataset)):\n        dataset[i] = [float(x) for x in dataset[i]]\n    return dataset\n\ndef h(o1,o2,x):\n    ans=o1+o2*x\n    return ans\n\ndef costf(massiv,p1,p2):\n    sum1=0.0\n    sum2=0.0\n    for x,y in massiv:\n        sum1+=(math.pow(h(o1,o2,x)-y,2))\n    sum2=(1.0/(2*len(massiv)))*sum1\n    return sum1,sum2\n\ndef gradient(massiv,er,alpha,o1,o2,max_loop=1000):\n    i=0\n    J,e=costf(massiv,o1,o2)\n    conv=False\n    m=len(massiv)\n    while conv!=True:\n        sum1=0.0\n        sum2=0.0\n        for x,y in massiv:\n            sum1+=(o1+o2*x-y)\n            sum2+=(o1+o2*x-y)*x\n        grad0=1.0/m*sum1\n        grad1=1.0/m*sum2\n\n        temp0=o1-alpha*grad0\n        temp1=o2-alpha*grad1\n        print(temp0,temp1)\n        o1=temp0\n        o2=temp1\n        e=0.0\n        for x,y in massiv:\n            e+=(math.pow(h(o1,o2,x)-y,2))\n        if abs(J-e)&lt;=ep:\n            print(\'Successful\\n\')\n            conv=True\n\n        J=e\n\n        i+=1\n        if i&gt;=max_loop:\n            print(\'Too much\\n\')\n            break\n    return o1,o2\n\n\n#data = massiv\ndata=loadCsv(\'ex1data1.txt\')\no1=0.0 #temp0=0\no2=1.0 #temp1=1\nalpha=0.01\nep=0.01\nt0,t1=gradient(data,ep,alpha,o1,o2)\nprint(\'temp0=\'+str(t0)+\' \\ntemp1=\'+str(t1))\n\nx=35000\nwhile x&lt;=70000:\n    y=h(t0,t1,x)\n    print(\'x=\'+str(x)+\'\\ny=\'+str(y)+\'\\n\')\n    x+=5000\n\nmaxx=data[0][0]\nfor q,w in data:\n    maxx=max(maxx,q)\nmaxx=round(maxx)+1\nline=[]\nll=0\nwhile ll&lt;maxx:\n    line.append(h(t0,t1,ll))\n    ll+=1\nx=[]\ny=[]\nfor q,w in data:\n    x.append(q)\n    y.append(w)\n\nimport matplotlib.pyplot as plt\nplt.plot(x,y,\'ro\',line)\nplt.ylabel(\'some numbers\')\nplt.show()\n'
'python word2vec.py --train_data &lt;path to training data&gt; --eval_data &lt;path to eval&gt; --save_path &lt;save directory&gt;\n'
'filename_queue = tf.train.string_input_producer(["file0.csv", "file1.csv"])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[1], [1], [1], [1], [1]]\ncol1, col2, col3, col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.stack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  for     filename_queue = tf.train.string_input_producer(["file0.csv", "file1.csv"])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\n# Default values, in case of empty columns. Also specifies the type of the\n# decoded result.\nrecord_defaults = [[1], [1], [1], [1], [1]]\ncol1, col2, col3, col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.stack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n\n  for i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n\n  coord.request_stop()\n  coord.join(threads)i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n\n  coord.request_stop()\n  coord.join(threads)\n'
'data = stats.exponweib.rvs(a=1, c=2.09, scale=10.895, loc=0, size=2500)\ndata.sort()\nplt.plot(data, stats.exponweib.pdf(data, *stats.exponweib.fit(data, 1, 1, scale=2, loc=0)))\n'
'new_data = np.concatenate((audio,image), axis=1)\n\nmeans = np.mean(new_data, axis=0)\nvars = np.var(new_data, axis=0)\nnorm_data = (new_data - means) / vars\n'
"# Original format: NHWDC.\noriginal = tf.placeholder(dtype=tf.float32, shape=[None, 16, 16, 4, 192])\nprint original.shape\n\n# Convert to NDHWC format.\ninput = tf.reshape(original, shape=[-1, 4, 16, 16, 192])\nprint input.shape\n\n# input shape:  [batch, depth, height, width, in_channels].\n# filter shape: [depth, height, width, output_channels, in_channels].\n# output shape: [batch, depth, height, width, output_channels].\nfilter = tf.get_variable('filter', shape=[4, 2, 2, 192, 192], dtype=tf.float32)\nconv = tf.nn.conv3d_transpose(input,\n                              filter=filter,\n                              output_shape=[-1, 7, 32, 32, 192],\n                              strides=[1, 1, 2, 2, 1],\n                              padding='SAME')\nprint conv.shape\n\nfinal = tf.reshape(conv, shape=[-1, 32, 32, 7, 192])\nprint final.shape\n\n(?, 16, 16, 4, 192)\n(?, 4, 16, 16, 192)\n(?, 7, 32, 32, 192)\n(?, 32, 32, 7, 192)\n"
'spokenMean = features(spoken, np.mean)\nspokenMean = np.vstack(spokenMean[:]).astype(np.float32)\n\nresult = np.concatenate([spokenMean, written], axis=1)\n'
'pred_list = pred.tolist()\nreturn JsonResponse(pred_list, safe=False)\n'
'with tf.device("cpu"):\n    inp = tf.placeholder(tf.float32, [None, 31, 31, 1])\n    init_op = tf.global_variables_initializer()\n    init_op2 = tf.local_variables_initializer()\n\n    with tf.Session(config=tf.ConfigProto()) as sess:\n        m = get_model(inp, True)\n        sess.run(init_op)\n        sess.run(init_op2)\n        print(sess.run(tf.report_uninitialized_variables()))\n        res = sess.run(m, feed_dict={ inp: np.zeros((1, 31, 31, 1))})\n\nwith tf.device("cpu"):\n    inp = tf.placeholder(tf.float32, [None, 31, 31, 1])\n\n    with tf.Session(config=tf.ConfigProto()) as sess:\n        m = get_model(inp, True)\n        sess.run(tf.initialize_all_variables())\n        res = sess.run(tf.report_uninitialized_variables())\n        #print(res) -- outputs [] (none)\n        res = sess.run(m, feed_dict={ inp: np.zeros((1, 31, 31, 1))})\n        print(res)\n'
"classifier = nn.Sequential(OrderedDict([            \n                          ('fc1', nn.Linear(25088, 10000)), \n                          ('relu', nn.ReLU()),\n                          ('fc2', nn.Linear(10000, 5000)),\n                          ('relu', nn.ReLU()),\n                          ('fc3', nn.Linear(5000, 102)),\n                          ('output', nn.LogSoftmax(dim=1))\n                          ]))\n"
'X.loc[:, "Private"] = X.Private.map({\'Yes\':1, \'No\':0})\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\n\nmatplotlib.style.use(\'ggplot\') # Look Pretty\n\n# Reading in data\nX = pd.read_csv(\'College.csv\', index_col=0)\n\n# Wrangling data\nX.loc[:, "Private"] = X.Private.map({\'Yes\':1, \'No\':0})\n\n# Splitting data\nroomBoard = X.loc[:, \'Room.Board\'].values.reshape((len(X),1))\naccStudent = X.loc[:, \'Accept\'].values.reshape((len(X),1))\nX_train, X_test, y_train, y_test = train_test_split(roomBoard, accStudent, test_size=0.3, random_state=7)\n\n# Training model\nmodel = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)\n\n# Visualise results\ndef drawLine(model, X_test, y_test, title, R2):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plt.scatter(X_test, y_test, c=\'g\', marker=\'o\')\n    y_pred =  model.predict(X_test)\n    plt.plot(X_test, y_pred, color=\'orange\', linewidth=1, alpha=0.7)\n\n    title += " R2: " + str(R2)\n    ax.set_title(title)\n    print(title)\n    print("Intercept(s): ", model.intercept_)\n\n    plt.xticks(())\n    plt.yticks(())\n\n    plt.show()\n\ndrawLine(model, X_test, y_test, "Accept(Room&amp;Board)", score)\n'
"# convert class labels in numerical data, assuming you have two classes\ndf['Level'].replace(['Low Level'],0)\ndf['Level'].replace(['High Level'],1)\n\n# extra data and class labels\ndata = df[['Salary','LoanAmt']]\ntarget = df['Level']\n\n# convert df to numpy arrays\ndata = data.values\ntarget =  target.values\n\n# you would ideally want to do a test train split.\n#Train the model on training data and test on the test data for accuracy\n\n#pass in fit function\nneigh = KNeighborsClassifier(n_neighbors=5,algorithm='auto')\nneigh.fit(data,target) ## how to passs the parameters here?\n"
'from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(C=1e10, tol=1e-6).fit(X, Y.ravel())\nmodel.coef_[0] + np.array([model.intercept_[0], 0, 0])\n# array([-25.16127356,   0.20623123,   0.20147112])\n\nfrom statsmodels.discrete.discrete_model import Logit\nmodel = Logit(Y.ravel(), X)\nmodel.fit().params\n# array([-25.16133357,   0.20623171,   0.2014716 ])\n\ndef hypothesis(theta, X):\n    return 1/(1+np.exp(-np.dot(X, theta)))\n\ndef cost_function(X, Y, theta):\n    h = hypothesis(theta, X)\n    return -np.mean(Y*np.log(h) + (1-Y)*np.log(1-h))\n\ndef gradient_descent(X, Y, theta, alpha):\n    h = hypothesis(theta, X)\n    return theta - alpha*np.dot(X.T, h - Y)/len(X)\n\ndef logistic_regression(X, Y, alpha, theta, iterations):\n    for iter in range(iterations):\n        theta = gradient_descent(X, Y, theta, alpha)     \n        if iter % 100000 == 0:\n            cost = cost_function(X, Y, theta)\n            cost_history[0].append(cost)\n            cost_history[1].append(iter)\n            print(theta, cost, iter)\n    return theta\n'
'a = numpy.array(a)\ne.append(a.reshape((a.shape + (1,)))\n'
"model.add(Dense(numClasses, activation='softmax', name='some_unique_name'))\n"
'model = load_trained_model(weights_path)  # load ONCE\n\nestimatedKwh = model.predict(params)\n\nparams_array = []\n\nfor i, row in data.iterrows():\n    params = row.values\n\n    if (params.ndim == 1):\n        params = np.array([params])  # is this if really necessary??\n\n    params_array.append(params)\n\nparams_array = np.asarray(params_array, dtype=np.float32)\ntotal_estKwh = load_trained_model(weights_path).predict(params_array)\n\n\ndf = pd.DataFrame.from_records(total_estKwh)\ntotal = df.sum()\ntotalStd = np.std(df.values)\ntotalMean = df.mean()\n'
'#your_matrix.shape = (569, 30)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprojected_data = pca.fit_transform(your_matrix)\nplt.scatter(projected_data[:,0], projected_data[:, 1]) # This might be very helpful for data structure understanding... \nplt.show() \n'
'model = Sequential()\n\nmodel.add(Dense(30, input_dim = 30))\nmodel.add(BatchNormalization(axis = 1))\nmodel.add(Activation(tfnn.relu))\n\nmodel.add(Dense(60)\nmodel.add(BatchNormalization(axis = 1))\nmodel.add(Activation(tfnn.relu))\n\nmodel.add(Dense(1, activation = tfnn.sigmoid))\n'
"def render(self, screen):\n    ...\n    # or create a new function, it's up to you, just to this once per frame\n    events = pygame.events.get()\n    for e in events:\n        if e.type == pygame.QUIT:\n            sys.exit() # or whatever to quit the program\n"
'output1 = Dense(1, activation = \'sigmoid\')(x)\noutput2 = Dense(1, activation = \'sigmoid\')(x)\noutput3 = Dense(1, activation = \'sigmoid\')(x)\noutput4 = Dense(1, activation = \'sigmoid\')(x)\noutput5 = Dense(1, activation = \'sigmoid\')(x)\noutput6 = Dense(1, activation = \'sigmoid\')(x)\noutput7 = Dense(1, activation = \'sigmoid\')(x)\noutput8 = Dense(1, activation = \'sigmoid\')(x)\n\nloss = ["binary_crossentropy","binary_crossentropy","binary_crossentropy","binary_crossentropy", "binary_crossentropy","binary_crossentropy","binary_crossentropy","binary_crossentropy"]#,metrics = ["accuracy"])\n\n#leave sigmoid here, don\'t change with softmax if it is a multilabel problem\noutput = Dense(8, activation = \'sigmoid\')(x) \nloss = "binary_crossentropy"\n'
'def modelR(weights, biases, data):\n   # This is the input layer.\n   y = np.matmul(data,weights[0])+biases[0][None,:] \n   y_act = relu(y) #also dropout or any other function you use here\n   z = np.matmul(y_act,weights[1])+biases[1][None,:]\n   z_act = relu(z) #also dropout and any other function you use here\n   p = np.matmul(z_act,weights[2])+biases[2][None,:]\n   p_act = sigmoid(p)   \n   return p_act\n'
"from gensim import models\nw = models.KeyedVectors.load_word2vec_format('model', binary=True)\n\nfrom gensim import models\nw = models.Word2Vec.load_word2vec_format('model', binary=True)\n"
'processed_TRAIN_features = csr_matrix((processed_TRAIN_features),shape=(new row length,new column length))\n'
'---------------------------\n|      x    |true 0  |true 1|\n---------------------------\n|predicted 0| 128838 |  8968|\n|predicted 1|   54   |  279 |\n'
' import tensorflow as tf\n from tf.keras.models import load_model\n\n model=load_model("model.h5")\n\n # Convert the model.\n converter = tf.lite.TFLiteConverter.from_keras_model(model)\n tflite_model = converter.convert()\n\nfrom keras.callbacks import ModelCheckpoint\n\n\n\'\'\'\nsaves the model weights after each epoch\n\'\'\'\ncheckpointer = ModelCheckpoint(filepath=\'model-{epoch:02d}-{val_loss:.2f}.h5\', verbose=1)\nmodel.fit(x_train, y_train, batch_size=batchsize, epochs=epochs, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n'
'poly_features = PolynomialFeatures(degree=2)\nX_train_poly = poly_features.fit_transform(X_train) # transform the data as well\n'
'from sklearn.preprocessing import LabelEncoder\ny = ["A","B","D","A","C"]\nle = LabelEncoder()\nle.fit_transform(y)\n# array([0, 1, 3, 0, 2], dtype=int64)\n\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\n&gt;&gt;&gt; X, y = make_classification(n_samples=1000, n_features=4,\n...                            n_informative=2, n_redundant=0,\n...                            random_state=0, shuffle=False)\ny = np.random.choice(["A","B","C","D"],1000)\nprint(y.shape)\n&gt;&gt;&gt; clf = RandomForestClassifier(max_depth=2, random_state=0)\n&gt;&gt;&gt; clf.fit(X, y)\n&gt;&gt;&gt; clf.classes_\n# array([\'A\', \'B\', \'C\', \'D\'], dtype=\'&lt;U1\')\n'
"Feature         Label\nAATGTCG         3\nAATGTCG         0\nAATGTCG         1\n\nGCTAGATGACAGT   (0.9,0.1,0.2)\nTTTTAAAACAG     (0.7,0.6,0.3)\nTAGCTATACT      (0.3,0.3,0.2)    \nTGGGGCAAAAAAAA  (0.1,0.5,0.6)\n\nmodel.add(Dense(3, activation='softmax') # no. of units here should be equal to the no. of classes)\n\nX_train, X_test = data_encoded[train], data_encoded[test]\ny_train, y_test = data_encoded[train], data_encoded[test]\n\ny_train, y_test = y[train], y[test]\n\nError when checking input: expected embedding_3_input to have shape (5,) but got array with shape (1,)\n"
'outputs = tf.keras.layers.Conv2D(1, (1,1), activation="softmax")(c9)\n\noutputs = model.add(Dense(3, activation=\'softmax\'))(c9)\n'
"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import metrics\n\ndf = pd.DataFrame({\n    'col1': [1, 2, 3, 4, 5, 6],\n    'col2': [16, 32, 64, 12, 5, 256],\n    'col3': [7, 8, 9, 10, 12, 11],\n    'out': [40, 5, 60, 7, 9, 100]})\n\n# print(df)\nX_df = df[['col1', 'col2', 'col3']]\ny_df = df['out']\nregr = linear_model.ElasticNet(alpha=0.1, random_state=0, tol=1e-12)\n\nregr.fit(X_df, y_df)\ny_pred = regr.predict(X_df)\nprint(regr.coef_)\nprint(&quot;R2:&quot;, regr.score(X_df, y_df))\nprint(&quot;MSE:&quot;, metrics.mean_squared_error(y_df, y_pred))\n\n# change the order to: [col2, col1, col3]\nfirst_cols = ['col2']\ncols = first_cols.copy()\nfor c in X_df.columns:\n    if c not in cols:\n        cols.append(c)\nX_df = X_df[cols]\n\n\nregr = linear_model.ElasticNet(alpha=0.1, random_state=0, tol=1e-12)\nregr.fit(X_df, y_df)\ny_pred = regr.predict(X_df)\nprint(&quot;\\nReorder:&quot;)\nprint(regr.coef_)\nprint(&quot;R2:&quot;, regr.score(X_df, y_df))\nprint(&quot;MSE:&quot;, metrics.mean_squared_error(y_df, y_pred))\n\n[-8.92519779  0.42980208  3.59812779]\nR2: 0.8277593357239204\nMSE: 207.11461432908925\n\nReorder:\n[ 0.42980208 -8.92519779  3.59812779]\nR2: 0.8277593357240851\nMSE: 207.11461432889112\n"
"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nfrom sklearn import set_config\n\nset_config(print_changed_only='True', display='diagram')\n\ndata = pd.DataFrame({'Impression': ['this is the first text',\n                                    'second one goes like this',\n                                    'third one is very short',\n                                    'This is the final statement'],\n                     'Volume': [123, 1, 2, 123],\n                     'Cancer': [1, 0, 0, 1]})\n\nX_train, X_test, y_train, y_test = train_test_split(\n    data[['Impression', 'Volume']], data['Cancer'], test_size=0.5)\n\nct = make_column_transformer(\n    (CountVectorizer(), 'Impression'), remainder='passthrough')\n\npipeline = make_pipeline(ct, DecisionTreeClassifier())\npipeline.fit(X_train, y_train)\npipeline.score(X_test, y_test)\n"
'# The default regressor for LogitBoost is a decision stump\n_BASE_ESTIMATOR_DEFAULT = DecisionTreeRegressor(max_depth=1)\n\nimport logitboost\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\nX_train , X_test, y_train, y_test = train_test_split(X,y)\nlg = logitboost.LogitBoost()\nlg.fit(X_train, y_train)\n\nl_feat_imp = [sum(cls.feature_importances_ for cls in cls_list) \n              for cls_list in lg.estimators_]\nimp = np.array(l_feat_imp).sum(0)\n# array([ 9., 19., 51., 71.])\n\npd.Series(imp, index=load_iris().feature_names).sort_values(ascending=False).plot.bar()\n'
"class WeightedAverage(Layer):\n\n    def __init__(self, n_output):\n        super(WeightedAverage, self).__init__()\n        self.W = tf.Variable(initial_value=tf.random.uniform(shape=[1,1,n_output], minval=0, maxval=1),\n            trainable=True) # (1,1,n_inputs)\n\n    def call(self, inputs):\n\n        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n        inputs = [tf.expand_dims(i, -1) for i in inputs]\n        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n        # weights sum up to one on last dim\n\n        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)\n\ninp1 = Input((100,))\ninp2 = Input((100,))\nx1 = Dense(32, activation='relu')(inp1)\nx2 = Dense(32, activation='relu')(inp2)\nx = [x1,x2]\nW_Avg = WeightedAverage(n_output=len(x))(x)\nout = Dense(1)(W_Avg)\n\nm = Model([inp1,inp2], out)\nm.compile('adam','mse')\n\nn_sample = 1000\nX1 = np.random.uniform(0,1, (n_sample,100))\nX2 = np.random.uniform(0,1, (n_sample,100))\ny = np.random.uniform(0,1, (n_sample,1))\n\nm.fit([X1,X2], y, epochs=10)\n\ntf.nn.softmax(m.get_weights()[-3]).numpy()\n"
'Y_train = tf.one_hot(Y_train, 4)\n'
"opt = tf.keras.optimizers.Adadelta(learning_rate=1.0)\nautoencoder.compile(optimizer = opt, loss='binary_crossentropy')\n"
"import tensorflow as tf\ntf.keras.backend.floatx()\n\n'float32'\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense \nfrom tensorflow.keras import Model\nfrom sklearn.datasets import load_iris\niris, target = load_iris(return_X_y=True)\n\nX = iris[:, :3]\ny = iris[:, 3]\n\nds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(25).batch(8)\n\nclass MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.d0 = Dense(16, activation='relu')\n    self.d1 = Dense(32, activation='relu')\n    self.d2 = Dense(1, activation='linear')\n\n  def call(self, x):\n    x = self.d0(x)\n    x = self.d1(x)\n    x = self.d2(x)\n    return x\n\nmodel = MyModel()\n\n_ = model(X)\n"
"import numpy as np\nimport pandas as pd\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n\ndef func(x, a, b):\n    return a*np.exp(b*x)\n\nN = 1001\nn1 = N//3\nn2 = 2*n1\n\nt = np.linspace(0, 10, N)\n\nx0 = func(t[:n1], 1, -0.2)\nx1 = func(t[n1:n2]-t[n1], 5, -0.4)\nx2 = func(t[n2:]-t[n2], 2, -1.2)\n\nx = np.hstack([x0, x1, x2])\nxr = x + 0.025*np.random.randn(x.size)\n\ndxrdt = np.abs(np.diff(xr)/np.diff(t))\n\nxcrit = 20\nq = np.where(dxrdt &gt; xcrit) # (array([332, 665], dtype=int64),)\n\nidx = [0] + list(q[0]+1) + [t.size] # [0, 333, 666, 1001]\n\ntrials = []\nfig, axe = plt.subplots()\nfor k, (i, j) in enumerate(zip(idx[:-1], idx[1:])):\n    p, s = optimize.curve_fit(func, t[i:j]-t[i], xr[i:j])\n    axe.plot(t[i:j], xr[i:j], '.', label=&quot;Data #{}&quot;.format(k+1))\n    axe.plot(t[i:j], func(t[i:j]-t[i], *p), label=&quot;Data Fit #{}&quot;.format(k+1))\n    trials.append({&quot;n0&quot;: i, &quot;n1&quot;: j, &quot;t0&quot;: t[i], &quot;a&quot;: p[0], &quot;b&quot;: p[1],\n                   &quot;s_a&quot;: s[0,0], &quot;s_b&quot;: s[1,1], &quot;s_ab&quot;: s[0,1]})\naxe.set_title(&quot;Curve Fits&quot;)\naxe.set_xlabel(&quot;Time, $t$&quot;)\naxe.set_ylabel(&quot;Signal Estimate, $\\hat{g}(t)$&quot;)\naxe.legend()\naxe.grid()\ndf = pd.DataFrame(trials)\n\n    n0    n1    t0         a         b       s_a           s_b      s_ab\n0    0   333  0.00  0.998032 -0.199102  0.000011  4.199937e-06 -0.000005\n1  333   666  3.33  5.001710 -0.399537  0.000013  3.072542e-07 -0.000002\n2  666  1001  6.66  2.002495 -1.203943  0.000030  2.256274e-05 -0.000018\n"
'def some_function(price: int) -&gt;int:\n    return price\n\nfrom fastapi import Depends\n\nclass Data(BaseModel):\n    age: float\n    live_province: str\n    live_city: str\n    live_area_big: str\n    live_area_small: str\n    sex: float\n    marital: float\n    bank: str\n    salary: float\n    amount: float\n\n\n@app.post(&quot;/predict&quot;)\ndef predict(data: Data = Depends()):\n    predictions_df = predict_model(estimator=model, data=data)\n    predictions = predictions_df[&quot;Score&quot;][0]\n    return predictions\n'
'import tensorflow as tf\n\nx = tf.reshape(tf.range(1, 10), (3, 3))\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])&gt;\n\ntf.keras.layers.RepeatVector(n=3)(x)\n\n&lt;tf.Tensor: shape=(3, 3, 3), dtype=int32, numpy=\narray([[[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]],\n       [[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]],\n       [[7, 8, 9],\n        [7, 8, 9],\n        [7, 8, 9]]])&gt;\n\ntf.keras.layers.concatenate([x, x, x])\n\n&lt;tf.Tensor: shape=(3, 9), dtype=int32, numpy=\narray([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n       [4, 5, 6, 4, 5, 6, 4, 5, 6],\n       [7, 8, 9, 7, 8, 9, 7, 8, 9]])&gt;\n\n[0, 0, 0, 1, 1, 1, 2, 2, 2]\n\n[0, 1, 2, 0, 1, 2, 0, 1, 2]\n'
'dummies = pd.get_dummies(df.stack()).max(level=0)\n\nprint(dummies)\n   Architecture  Beauty  Black  Clothing  Fashion  Footwear  Jeans  Outerwear  Photograph  Property  Street fashion  White\n0             0       0      0         1        1         0      0          0           0         0               1      0\n1             0       0      0         1        0         0      1          1           0         0               0      0\n2             1       0      0         1        0         0      0          0           0         1               0      0\n3             0       0      1         1        0         1      0          0           0         0               0      0\n4             0       1      0         0        0         0      0          0           1         0               0      1\n'
'from itertools import product\n\ngrid_values= [{&quot;param1&quot;: [1, 2, 3, 4, 5], &quot;param2&quot;: [1, 2]}]\n\ndef grid(grid_values):\n    for p in grid_values:\n        # Always sort the keys of a dictionary, for reproducibility\n        print(p)\n        items = sorted(p.items())\n        if not items:\n            yield {}\n        else:\n            keys, values = zip(*items)\n            for v in product(*values):\n                params = dict(zip(keys, v))\n                yield params\n\ngrid_values= [{&quot;param1&quot;: [1, 2, 3, 4, 5], &quot;param2&quot;: [1, 2]}]\n\n  items = sorted(p.items())\n\nfor v in product(*values):\n    params = dict(zip(keys, v))\n    yield params\n'
"ovr_svc = OneVsRestClassifier(SVC(kernel='linear'))\n"
'import torch\nimport torch.nn as nn\nimport numpy as np\n\npredictions = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\n\ndef l1_loss_smooth(predictions, targets, beta = 1.0):\n    \n    loss = 0\n\n    diff = predictions-targets\n    mask = (diff.abs() &lt; beta)\n    loss += mask * (0.5*diff**2 / beta)\n    loss += (~mask) * (diff.abs() - 0.5*beta)\n    \n    return loss.mean()\n\noutput = l1_loss_smooth(predictions, target)\nprint(output)\n\nloss = nn.SmoothL1Loss(beta=1.0)\noutput = loss(predictions, target)\nprint(output)\n\n'
"scipy.stats.ks_2samp(data1, data2, alternative='two-sided', mode='auto')\n"
"    def Algo_search(models , params):\n\n       max_score = 0\n       max_model = None\n       max_model_params = None\n\n       for i,j in zip(models.keys() , models.values() ):\n\n            gs = GridSearchCV(estimator=j,param_grid=params[i])\n            a = gs.fit(X_train,y_train)\n            score = gs.score(X_test,y_test)\n\n            if score &gt; max_score:\n                max_score = score\n                max_model = gs.best_estimator_\n                max_model_params = gs.best_params_\n\n       return max_score, max_model, max_model_params\n\n      #Data points\n    models = {'model_gbm':GradientBoostingClassifier(), 'model_rf':RandomForestClassifier(), \n      'model_dt':DecisionTreeClassifier(), 'model_svm':SVC(), 'model_ada':AdaBoostClassifier()}\n   params_gbm = {'learning_rate':[0.1,0.2,0.3,0.4], 'n_estimators':[50,100,500,1000,2000]}\n   params_rf = {'n_estimators':[50,100,500,1000,2000]}\n   params_dt = {'splitter':['best','random'], 'max_depth':[1, 5, 10, 50, 100]}\n   params_svm = {'C':[1,2,5,10,50,100,500], 'kernel':['rbf','poly','sigmoid','linear']}\n   params_ada = {'n_estimators':[10,20,30,50,100,500,1000], 'learning_rate':[0.5,1,2,5,10]}\n   params = {'model_gbm':params_gbm, 'model_rf':params_rf, 'model_dt':params_dt, 'model_svm':params_svm, 'model_ada':params_ada}\n   grid_ml = Algo_search(models = models, params = params)\n"
'print(clf.predict(X[10, :]))\n'
"{\n'DESCR': None,\n 'data': [],\n 'filenames': array(),\n 'target': array(),\n 'target_names': []\n}\n"
"from sklearn import svm, grid_search, datasets\nfrom sklearn.cross_validation import train_test_split\niris = datasets.load_iris()\nclf = svm.SVC()\ntrain_x, test_x, train_y, test_y = train_test_split(iris.data, iris.target)\n&gt;&gt; clf.fit(train_x, train_y)\nSVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)\n\n&gt;&gt; clf.predict(test_x)\narray([1, 0, 0, 2, 0, 1, 1, 1, 0, 2, 2, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 2, 0,\n   1, 0, 2, 0, 2, 1, 2, 1, 2, 2, 2, 1, 0, 0, 0])\n"
"vectorizer = load_model(name)\nvect_train = vectorizer.transform(corpus) # (1) or any unseen data\n\ndef save_sparse_csr(filename,array):\n    np.savez(filename,data = array.data ,indices=array.indices,\n             indptr =array.indptr, shape=array.shape )\n\ndef load_sparse_csr(filename):\n    loader = np.load(filename)\n    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n                         shape = loader['shape'])\n"
'fullname=$(bazel query tensorflow/core/framework/common_shape_fns_test.cc)\nbazel query "attr(\'srcs\', $fullname, ${fullname//:*/}:*)"\n'
'g = tf.map_fn(f, tf.transpose(tf.concat(0, [optimal, p1, p2])))\n'
'model.fit(X_r, Y_cat, nb_epoch=10, batch_size=1)\n'
"w = np.full((4,), 3, dtype=float) # number of features = 4, namely p1, p2, p3, p4\nprint X.shape, type(X), y.shape, type(y), w.shape, type(w)\n#(774L, 4L) &lt;type 'numpy.ndarray'&gt; (774L,) &lt;type 'numpy.ndarray'&gt; (4L,) &lt;type 'numpy.ndarray'&gt;\n"
'import numpy as np\nx = np.array([1.1, 2.2, 3.3], dtype=np.float32)\ny = np.array([1, 1, 1], dtype=np.float32)\nx+y\n\n&gt;&gt;&gt; array([ 2.0999999 ,  3.20000005,  4.30000019], dtype=float32)\n'
'import numpy as np\n...\nx_in = np.expand_dims(mnist.test.images[0], axis=0)\nclassification = sess.run(tf.argmax(pred, 1), feed_dict={x:x_in})\n'
'In [6]: x.shape\nOut[6]: (200,)\n\nlm.fit(x.reshape(-1,1) ,y)\n'
"text = '''123\ndpqvjp jpo fjiqo iq[woe f\nwf q  q[ewfp wqervg\n436\noiwbrveojibnpibn eprvnj erv p eprwoij\n536\noiberv ih reip rewp wepri pirvnep\nerpvj pre\ner\nerpno poj rgwe epo\n'''\ndct = {}\ncurrent_id = None\nfor el in text.split():\n    if el.isdigit():\n        current_id = el\n        dct[current_id] = []\n    else:\n        dct[current_id].append(el)\nprint(dct)\n\n{'123': ['dpqvjp', 'jpo', 'fjiqo', 'iq[woe', 'f', 'wf', 'q', 'q[ewfp', 'wqervg'], '536': ['oiberv', 'ih', 'reip', 'rewp', 'wepri', 'pirvnep', 'erpvj', 'pre', 'er', 'erpno', 'poj', 'rgwe', 'epo'], '436': ['oiwbrveojibnpibn', 'eprvnj', 'erv', 'p', 'eprwoij']}\n\n&lt;ID=2647&gt;\n\n&lt;ID=123&gt;\ndpqvjp jpo fjiqo iq[woe f\nwf q  q[ewfp wqervg\n&lt;ID=436&gt;\noiwbrveojibnpibn eprvnj erv p eprwoij\n&lt;ID=536&gt;\noiberv ih reip rewp wepri pirvnep\nerpvj pre\ner\nerpno poj rgwe epo\n\nimport re\ntext = '''&lt;ID=123&gt;\ndpqvjp jpo fjiqo iq[woe f\nwf q  q[ewfp wqervg\n&lt;ID=436&gt;\noiwbrveojibnpibn eprvnj erv p eprwoij\n&lt;ID=536&gt;\noiberv ih reip rewp wepri pirvnep\nerpvj pre\ner\nerpno poj rgwe epo\n'''\ndct = {}\ncurrent_id = None\nfor el in text.split():\n    match = re.match(r'&lt;ID=(\\d+)&gt;', el)\n    if match:\n        current_id = match.group(1)\n        dct[current_id] = []\n    else:\n        dct[current_id].append(el)\nprint(dct)\n"
"X1 = vectorizer.fit_transform(train['question'])\nt1= vectorizer.transform(corpus)\n"
'[1. ,2., 3.]\n\n[[1., 2., 3.], [4., 5., 6.]]\n'
'predict = tf.argmax(tf.add(tf.matmul(hidden_output,A2),b2), 1)\n\npredict = tf.argmax(final_output)\n\nimport numpy as np\nimport tensorflow as tf\n\nimport os\nimport urllib\n\n# Data sets\nIRIS_TRAINING = "iris_training.csv"\nIRIS_TRAINING_URL = "http://download.tensorflow.org/data/iris_training.csv"\n\nIRIS_TEST = "iris_test.csv"\nIRIS_TEST_URL = "http://download.tensorflow.org/data/iris_test.csv"\n\n# If the training and test sets aren\'t stored locally, download them.\nif not os.path.exists(IRIS_TRAINING):\n  raw = urllib.urlopen(IRIS_TRAINING_URL).read()\n  with open(IRIS_TRAINING, "w") as f:\n    f.write(raw)\n\nif not os.path.exists(IRIS_TEST):\n  raw = urllib.urlopen(IRIS_TEST_URL).read()\n  with open(IRIS_TEST, "w") as f:\n    f.write(raw)\n\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header( filename=IRIS_TRAINING, target_dtype=np.int, features_dtype=np.float32)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header( filename=IRIS_TEST, target_dtype=np.int, features_dtype=np.float32)\n\nx_val_train = training_set.data[:,:3]\nx_val_test = test_set.data[:,:3]\ny_val_train = training_set.data[:,3].reshape([-1,1])\ny_val_test = test_set.data[:,3].reshape([-1,1])\n\nx_data = tf.placeholder(shape=[None, 3], dtype = tf.float32)\ny_target = tf.placeholder(shape = [None, 1], dtype = tf.float32) #Figure out usage of None\n\n#Create Layers for NN\nhidden_layer_size = 20\n\nA1 = tf.Variable(tf.random_normal(shape = [3,hidden_layer_size])) #Input -&gt; Hidden\nb1 = tf.Variable(tf.random_normal(shape = [hidden_layer_size])) #bias in Input for hidden\n\nA2 = tf.Variable(tf.random_normal(shape = [hidden_layer_size,1])) #Hidden -&gt; Output\nb2 = tf.Variable(tf.random_normal(shape = [1])) #Hidden Layer Bias\n\n#Generation of model\n\nhidden_output = tf.nn.relu(tf.add(tf.matmul(x_data,A1),b1))\nfinal_output = tf.add(tf.matmul(hidden_output,A2),b2)\n\nloss = tf.reduce_mean(tf.square(y_target - final_output))\n\nlearning_rate = 0.01\ntrain = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n\n#Training Loop\n\nloss_vec = []\ntest_loss = []\nepoch = 2000\nbatch_size = 100\n\n\ndef oneTrainingSession(epoch,loss_vec,test_loss,batch_size) :\n    rand_index = np.random.choice(len(x_val_train), size = batch_size)\n\n    rand_x = x_val_train #[rand_index,:]\n    rand_y = y_val_train #[rand_index,:]\n\n    temp_loss,_ = sess.run([loss,train], feed_dict = {x_data: rand_x, y_target : rand_y})\n    loss_vec.append(np.sqrt(temp_loss))\n\n    test_temp_loss = sess.run(loss, feed_dict = {x_data : x_val_test, y_target : y_val_test})\n    test_loss.append(np.sqrt(test_temp_loss))\n\n    if (i+1)%500 == 0:\n        print(\'Generation: \' + str(i+1) + \'.loss = \' + str(temp_loss))\n\nfor i in range(epoch):\n    oneTrainingSession(epoch,loss_vec,test_loss,batch_size)\n\ntest = x_val_test[:3,:]\nprint "The test values are"\nprint test\nprint ""\npred = sess.run(final_output, feed_dict = {x_data : test})\nprint("pred: ", pred)\n\nGeneration: 500.loss = 0.12768\nGeneration: 1000.loss = 0.0389756\nGeneration: 1500.loss = 0.0370268\nGeneration: 2000.loss = 0.0361797\nThe test values are\n[[ 5.9000001   3.          4.19999981]\n [ 6.9000001   3.0999999   5.4000001 ]\n [ 5.0999999   3.29999995  1.70000005]]\n\n(\'pred: \', array([[ 1.45187187],\n       [ 1.92516518],\n       [ 0.36887735]], dtype=float32))\n'
'id8         id9         id10    id11    id12\n1451865600  1451865600  -19.8   87.1    0.5701\n1451865600  1451865600  -1.6    3.6     0.57192\n1451865600  1451865600  -5.3    23.9    0.57155\n\nid236   id237   id238   id239   id240\n0       0       0       0       0\n0       0       0       0       0\n0       0       0       0       0 \n'
'def rollout(model, state):\n  while not state.terminated:\n    predictions = model.predict(state.as_nn_input())\n    for _, prediction in enumerate(predictions):\n      index = np.random.choice(bt.ACTIONS, p=prediction)\n      state.apply_mode(index)\n'
'# Test data\ndf = pd.DataFrame({\'ASIN\': [0,1], \'Summary\': [\'This is the first text\', \'Second text\']})\n\n# Example function\ndef summarize(text, n=5):\n\n    """A very basic summary"""\n    return (text[:n] + \'..\') if len(text) &gt; n else text\n\n# Applying the function to the text\ndf[\'Result\'] = df[\'Summary\'].map(summarize)\n\n#    ASIN                 Summary   Result\n# 0     0  This is the first text  This ..\n# 1     1             Second text  Secon..\n'
'from sklearn.utils import check_random_state\nimport numpy as np\n\ndef expand_dataset(X, y_proba, factor=10, random_state=None):\n    """\n    Convert a dataset with float multiclass probabilities to a dataset\n    with indicator probabilities by duplicating X rows and sampling\n    true labels.\n    """\n    rng = check_random_state(random_state)\n    n_classes = y_proba.shape[1]\n    classes = np.arange(n_classes, dtype=int)\n    for x, probs in zip(X, y_proba):\n        for label in rng.choice(classes, size=factor, p=probs):\n            yield x, label\n'
"Datasets = {'data1':load_data_1}\n\nData = Datasets['data1']()\n"
'import numpy as np\na = np.random.rand(10, 4)\n\na[:, 3] = a[:, 3] &gt; 0.5\n\na\n\nnp.array([[ 0.93011873,  0.80167023,  0.46502502,  0.        ],\n       [ 0.48754049,  0.331763  ,  0.19391945,  1.        ],\n       [ 0.17976529,  0.73625689,  0.6550934 ,  0.        ],\n       [ 0.17797159,  0.89597292,  0.67507392,  1.        ],\n       [ 0.89972382,  0.86131195,  0.85239512,  1.        ],\n       [ 0.59199271,  0.14223656,  0.12101887,  1.        ],\n       [ 0.71962168,  0.89132196,  0.61149278,  0.        ],\n       [ 0.63606024,  0.04821054,  0.49971309,  1.        ],\n       [ 0.18976505,  0.49880633,  0.93362872,  1.        ],\n       [ 0.00154421,  0.79748799,  0.46080879,  0.        ]])\n\nts = a[np.where(a[:, -1] == 0), :-1].T\n\ntc = a[np.where(a[:, -1] == 1), :-1].T\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\'3d\')\n\nax.scatter(*ts, c=\'g\', marker=\'o\', label="Train_Safe")\nax.scatter(*tc, c=\'b\', marker=\'o\', label="Train_Cracked")\nfig.show()\n'
"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\ninput_df = pd.DataFrame(dict(fruit=['Apple', 'Orange', 'Pine'], \n                             color=['Red', 'Orange','Green'],\n                             is_sweet = [0,0,1],\n                             country=['USA','India','Asia']))\n\nfiltered_df = input_df.apply(pd.to_numeric, errors='ignore')\n\n# This is what you need\nle_dict = {}\nfor col in filtered_df.columns:\n    le_dict[col] = LabelEncoder().fit(filtered_df[col])\n    filtered_df[col] = le_dict[col].transform(filtered_df[col])\n\nenc = OneHotEncoder()\nenc.fit(filtered_df)\nrefreshed_df = enc.transform(filtered_df).toarray()\n\nnew_df = pd.DataFrame(dict(fruit=['Apple'], \n                             color=['Red'],\n                             is_sweet = [0],\n                             country=['USA']))\nfor col in new_df.columns:\n    new_df[col] = le_dict[col].transform(new_df[col])\n\nnew_refreshed_df = enc.transform(new_df).toarray()\n\nprint(filtered_df)\n      color  country  fruit  is_sweet\n0      2        2      0         0\n1      1        1      1         0\n2      0        0      2         1\n\nprint(refreshed_df)\n[[ 0.  0.  1.  0.  0.  1.  1.  0.  0.  1.  0.]\n [ 0.  1.  0.  0.  1.  0.  0.  1.  0.  1.  0.]\n [ 1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.]]\n\nprint(new_df)\n      color  country  fruit  is_sweet\n0      2        2      0         0\n\nprint(new_refreshed_df)\n[[ 0.  0.  1.  0.  0.  1.  1.  0.  0.  1.  0.]]\n"
"from sklearn.externals import joblib\njoblib.dump(clf, 'filename.pkl') \n\nclf = joblib.load('filename.pkl')\n"
"with tf.variable_scope('fully_connected_1', reuse=True):\n  weights = tf.get_variable('weights')\n  biases = tf.get_variable('biases')\n\n  sess.run([weights.initializer, biases.initializer])\n"
'from sklearn.model_selection import cross_val_score\nX_train, X_valid, y_train, y_valid, indices_train, indices_test = train_test_split(train_X, train_y, np.arange(X_train.shape[0]), test_size=0.2, random_state=42)\n\nclass HoldOut:\n\n    def __init__(self, indices_train, indices_test):\n        self.ind_train = indices_train\n        self.ind_test = indices_test\n\n    def __iter__(self):\n        yield self.ind_train, self.ind_test\n\ncross_val_score(RandomForestClassifier(random_state=42, n_estimators=10), train_X, train_y, \n                cv=HoldOut(indices_train, indices_test), verbose=1)\n'
'def launch_network(self):\n    db = tf.matmul(self.X, tf.reshape(self.W, [-1, 1])) + self.b\n    hyp = tf.sigmoid(db)\n\n    cost0 = self.y * tf.log(tf.clip_by_value(hyp, 1e-10, 1.0))\n    ... (skip) ...\n\ncorrect_answers = np.array([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1])[:,np.newaxis]\n'
"# Varying the hidden layer size to observe underfitting and overfitting\nplt.figure(figsize=(16, 32))\nhidden_layer_dimensions = [50]\nfor i, hidden_layer_size in enumerate(hidden_layer_dimensions):\n    fig = plt.subplot(4, 2, i+1)\n    plt.title('Hidden Layer size: {:d}'.format(hidden_layer_size))\n\n    model = Sequential()\n    model.add(Dense(hidden_layer_size, activation='relu', input_shape=(2,)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(SGD(lr=1.0), 'binary_crossentropy', metrics=['accuracy'])\n    history = model.fit(data, targets, verbose=0, epochs=5000)\n\n    plot_decision_boundary(lambda x: model.predict(x) &gt; 0.5, data, targets, fig)\n"
'epx = np.vstack(xs)\neph = np.vstack(hs)\nepdlogp = np.vstack(dlogps)\nepr = np.vstack(drs)\n'
'from neupy import layers\nfrom neupy.utils import as_tuple\nimport theano.tensor as T\n\nclass Flatten(layers.BaseLayer):\n    """\n    Slight modification of the Reshape layer from the neupy library:\n    https://github.com/itdxer/neupy/blob/master/neupy/layers/reshape.py\n    """\n    @property \n    def output_shape(self):\n        # Number of output feature depends on the input shape \n        # When layer receives input with shape (10, 3, 4)\n        # than output will be (10, 12). First number 10 defines\n        # number of samples which you typically don\'t need to\n        # change during propagation\n        n_output_features = np.prod(self.input_shape)\n        return (n_output_features,)\n\n    def output(self, input_value):\n        n_samples = input_value.shape[0]\n        return T.reshape(input_value, as_tuple(n_samples, self.output_shape))\n\n&gt;&gt;&gt; network = layers.Input((3, 4)) &gt; Flatten()\n&gt;&gt;&gt; predict = network.compile()\n&gt;&gt;&gt; predict(np.random.random((10, 3, 4))).shape\n(10, 12)\n\nimport numpy as np\nfrom neupy import layers, init\nimport theano.tensor as T\n\n\ndef norm(value, axis=None):\n    return T.sqrt(T.sum(T.square(value), axis=axis))\n\n\nclass RBF(layers.BaseLayer):\n    def initialize(self):\n        super(RBF, self).initialize()\n\n        # It\'s more flexible when shape of the parameters\n        # denend on the input shape\n        self.add_parameter(\n            name=\'mean\', shape=self.input_shape,\n            value=init.Constant(0.), trainable=True)\n\n        self.add_parameter(\n            name=\'std_dev\', shape=self.input_shape,\n            value=init.Constant(1.), trainable=True)\n\n    def output(self, input_value):\n        K = input_value - self.mean\n        return T.exp(-norm(K, axis=0) / self.std_dev)\n\n\nnetwork = layers.Input(1) &gt; RBF()\npredict = network.compile()\nprint(predict(np.random.random((10, 1))))\n\nnetwork = layers.Input(4) &gt; RBF()\npredict = network.compile()\nprint(predict(np.random.random((10, 4))))\n'
'Index   Sys1_Cls1  Sys1_Cls2  Sys1_Cls3  Sys2_Cls1  Sys2_Cls2  Sys2_Cls3  \\\n    0   0.310903   0.521839   0.167258   0.034925   0.509087   0.455988   \n    1   0.402701   0.315302   0.281997   0.044981   0.137326   0.817693   \n    2   0.272443   0.409210   0.318347   0.591514   0.170707   0.237778   \n    3   0.272599   0.304014   0.423388   0.175838   0.324275   0.499887   \n    4   0.339352   0.341860   0.318788   0.574995   0.169180   0.255824   \n\n       Sys3_Cls1  Sys3_Cls2  Sys3_Cls3  Sys4_Cls1  Sys4_Cls2  Sys4_Cls3  \n       0.173293   0.279590   0.547117   0.441913   0.251394   0.306692  \n       0.224656   0.425100   0.350244   0.430451   0.382072   0.187476  \n       0.198573   0.603826   0.197600   0.412734   0.185472   0.401795  \n       0.011399   0.598892   0.389709   0.057813   0.651510   0.290677  \n       0.025087   0.478595   0.496317   0.539963   0.288596   0.171440  \n\n  Index       Cls1      Cls2      Cls3\n    0       0.187362  0.151723  0.660914\n    1       0.378118  0.293932  0.327950\n    2       0.424903  0.278271  0.296825\n    3       0.342273  0.274003  0.383723\n    4       0.405926  0.104094  0.489981\n\ncls1_val = Sys1_Cls1 + Sys2_Cls1 + Sys3_Cls1 + ...\n\nOR\n\ncls1_val = (Sys1_Cls1 + Sys2_Cls1 + Sys3_Cls1 + ...)/number_systems\n\nOR\n\ncls1_val = (weight1 x Sys1_Cls1 + weight2 x Sys2_Cls1 + weight3 x Sys3_Cls1 + ...)/number_systems\n'
"random_search = RandomizedSearchCV(xgb, param_distributions=params,\n                                   n_iter=param_comb, \n                                   scoring=['f1_macro','precision_macro'], \n                                   n_jobs=4, \n                                   cv=skf.split(X_train,y_train), \n                                   verbose=3, random_state=1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params,\n                                   ...\n                                   scoring=['f1_macro','precision_macro'], \n                                   ...\n                                   refit = 'f1_macro')\n"
'a -&gt; [1, 0, 0, 0, 0]  # a one-hot 1 x 5 vector\nb -&gt; [0, 1, 0, 0, 0]  # a one-hot 1 x 5 vector\nc -&gt; [0, 0, 1, 0, 0]  # a one-hot 1 x 5 vector\nd -&gt; [0, 0, 0, 1, 0]  # a one-hot 1 x 5 vector\ne -&gt; [0, 0, 0, 0, 1]  # a one-hot 1 x 5 vector\n# total five one-hot 1 x 5 vectors which can be expressed in a 5 x 5 matrix.\n'
'X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nXpred = np.array([X1.ravel(), X2.ravel()] + [np.repeat(0, X1.ravel().size) for _ in range(11)]).T\n# Xpred now has a grid for x1 and x2 and average value (0) for x3 through x13\npred = classifier.predict(Xpred).reshape(X1.shape)   # is a matrix of 0\'s and 1\'s !\nplt.contourf(X1, X2, pred,\n             alpha = 0.75, cmap = ListedColormap((\'red\', \'green\')))\n\npred = classifier.decision_function(Xpred).reshape(X1.shape)    \nplt.contourf(X1, X2, pred,\n             alpha=1.0, cmap="RdYlGn", levels=np.linspace(pred.min(), pred.max(), 100))\n\ndataset = pd.read_csv(\'C:/Users/Vishnu/Desktop/Lung Cancer/lung_cancer.csv\')\ndataset.loc[dataset[\'GENDER\'] == \'F\', \'GENDER\'] = 0\ndataset.loc[dataset[\'GENDER\'] == \'M\', \'GENDER\'] = 1\nX = dataset.iloc[:, 0:14].values\ny = dataset.iloc[:, 14].values\n'
'model.fit(X, y_train, [...])\n\nmodel.fit(X_train, y_train, [...])\n'
'print(prediction.name)\n'
'python deep_learning_object_detection.py \\\n    --prototxt MobileNetSSD_deploy.prototxt.txt \\\n    --model MobileNetSSD_deploy.caffemodel --image images/example_01.jpg \n\nname: "MobileNet-SSD"\ninput: "data"\ninput_shape {\n  dim: 1\n  dim: 3\n  dim: 300\n  dim: 300\n}\n'
"from keras.models import Model\n\nimage_model = get_image_model()\nlanguage_model = get_language_model(vocab_size)\n\nmerged = concatenate([image_model.output, language_model.output])\nx = LSTM(256, return_sequences = False)(merged)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\n\nmodel = Model([image_model.input, language_model.input], out)\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\nmodel.fit([images, encoded_captions], one_hot_captions, ...)\n\ndef get_concatenated_model(image_model, language_model, vocab_size):\n    merged = concatenate([image_model.output, language_model.output])\n    x = LSTM(256, return_sequences = False)(merged)\n    x = Dense(vocab_size)(x)\n    out = Activation('softmax')(x)\n\n    model = Model([image_model.input, language_model.input], out)\n    return model\n"
'from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# First pass\n# some_generator can be anything which reads the data in batches\nfor data in some_generator:\n    scaler.partial_fit(data)\n\n    # View the updated mean and std variance at each batch\n    print(scaler.mean_)\n    print(scaler.var_)\n\n\n# Second pass\nfor data in some_generator:\n    scaled_data = scaler.transform(data)\n\n    # Do whatever you want with the scaled_data\n'
'def hinge_grad_input(target_pred, target_true):\n    """Compute the partial derivative \n        of Hinge loss with respect to its input\n    # Arguments\n        target_pred: predictions - np.array of size `(n_objects,)`\n        target_true: ground truth - np.array of size `(n_objects,)`\n    # Output\n        the partial derivative \n        of Hinge loss with respect to its input\n        np.array of size `(n_objects,)`\n    """\n    grad_input = np.where(target_pred * target_true &lt; 1, -target_true / target_pred.size, 0)\n\n    return grad_input\n'
'loss = tf.reduce_mean(tf.square(labels-logits))\n'
'from sklearn.preprocessing import StandardScaler\ndata = [[0, 0], [0, 0], [1, 1], [1, 1]]\nscaler = StandardScaler()\nnew_data = scaler.fit_transform(data)\n...\n# feed new_data to the neural network\n'
"df.loc[df['rating'].str.contains('-', na = False), 'rating'] = np.nan\n\ndf['rating'] = pd.to_numeric(df['rating'], errors = 'coerce')\n"
'....\n....\n\ndef fit(self, X, y=None):\n    df = pd.concat([X, self.response], axis=1)\n    self.cols = df.columns[abs(df.corr()[df.columns[-1]]) &gt; self.threshold].drop(self.response.columns)\n    return self\ndef transform(self, X, y=None):\n    return X[self.cols]\n'
'from nltk import word_tokenize\nfrom nltk.lm import MLE\nfrom nltk.lm.preprocessing import pad_both_ends, padded_everygram_pipeline\n\ns = "Natural-language processing (NLP) is an area of computer science " \\\n    "and artificial intelligence concerned with the interactions " \\\n    "between computers and human (natural) languages."\ns = s.lower()\n\npaddedLine = [list(pad_both_ends(word_tokenize(s), n=2))]\n\ntrain, vocab = padded_everygram_pipeline(2, paddedLine)\n\nlm = MLE(2)\n\nlm.fit(train, vocab)\n\nprint(lm.counts)\n\n'
'y = model.predict(np.expand_dims(x, axis=0), batch_size=1)\n'
'Frame model\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 11, 214, 16)       160       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 5, 71, 16)         0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 3, 69, 16)         2320      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 1, 34, 16)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 544)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                17440     \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndense_3 (Dense)              (None, 3)                 99        \n=================================================================\nTotal params: 21,075\nTrainable params: 21,075\nNon-trainable params: 0\n\n_________________________________________________________________\nSong model\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntime_distributed_1 (TimeDist (None, 23, 3)             21075     \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 3)                 0         \n=================================================================\nTotal params: 21,075\nTrainable params: 21,075\nNon-trainable params: 0\n_________________________________________________________________\n\nfeatures (8, 23, 13, 216, 1)\n'
"import pandas as pd \ndata = [[1, 60, 'RL'], [2, 53, 'RR'], [3, 49, 'RL'], [4, 60, 'RL'], [5, 95, 'RR']] \ndf = pd.DataFrame(data, columns = ['Id', 'MSSubClass', 'MSZoning']) \n\ndf['MSZoning_factor'] = pd.Categorical(df.MSZoning).codes + 1\ndf\n#   Id  MSSubClass MSZoning  MSZoning_factor\n#0   1          60       RL                1\n#1   2          53       RR                2\n#2   3          49       RL                1\n#3   4          60       RL                1\n#4   5          95       RR                2\n\ndf['MSZoning_factor'] = pd.factorize(df.MSZoning)[0] + 1\n"
'model = Sequential()\nmodel.add(LSTM(5, return_sequences=True))\nmodel.add(LSTM(5, activation="softmax"))\n'
"training_endog = endog.loc[:'2017']\ntraining_exog = exog.loc[:'2017']\n\ntraining_mod = sm.tsa.SARIMAX(training_endog, order=(2, 0, 0), exog=training_exog)\ntraining_res = training_mod.fit()\n\nmod = sm.tsa.SARIMAX(endog, order=(2, 0, 0), exog=exog)\nres = mod.smooth(training_res.params)\nprint(res.forecast(1, exog=exog_fcast))\n\ntraining_endog = endog.loc[:'2017']\ntraining_exog = exog.loc[:'2017']\n\ntraining_mod = sm.tsa.SARIMAX(training_endog, order=(2, 0, 0), exog=training_exog)\ntraining_res = training_mod.fit()\n\nres = training_res.append(endog.loc['2018':], exog=exog.loc['2018':])\nprint(res.forecast(1, exog=exog_fcast))\n"
'res = tf.nn.in_top_k([[0,1], [1,0], [0,1], [1, 0], [0, 1]], [0, 1, 1, 1, 1], 1)\n\nres = tf.nn.in_top_k([[0,1], [1,0], [0,1], [1, 0], [0, 1]], [0, 2, 1, 1, 1], 1)\n\nn_inputs = 28 \nn_hidden1 = 15\nn_hidden2 = 5\nn_outputs = 2\n\nX = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X") \ny = tf.placeholder(tf.int32, shape=(None, 2), name="y")   #None =&gt; any\n\ndef neuron_layer(X, n_neurons, name, activation=None):\n    with tf.name_scope(name):\n        n_inputs = int(X.shape[1])\n        stddev = 2 / np.sqrt(n_inputs) \n        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) #matrice n_inputs x n_neurons values proche de 0    \n        W = tf.Variable(init,name="kernel")  #weights random\n        b = tf.Variable(tf.zeros([n_neurons]), name="bias")\n        Z = tf.matmul(X, W) + b\n        tf.cast(Z,tf.int32)\n        if activation is not None:\n            return activation(Z)\n        else:\n            return Z\n\ndef to_one_hot(y):\n    n_classes = y.max() + 1\n    m = len(y)\n    Y_one_hot = np.zeros((m, n_classes))\n    Y_one_hot[np.arange(m), y] = 1\n    return Y_one_hot\n\nhidden1 = neuron_layer(X, n_hidden1, name="hidden1",\n                           activation=tf.nn.relu)\nhidden2 = neuron_layer(hidden1, n_hidden2, name="hidden2",\n                           activation=tf.nn.relu)\nlogits = neuron_layer(hidden2, n_outputs, name="outputs")\nxentropy = tf.keras.backend.binary_crossentropy(tf.to_float(y),logits) \nloss = tf.reduce_mean(xentropy)\nlearning_rate = 0.01\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\ntraining_op = optimizer.minimize(loss)\ncorrect = tf.nn.in_top_k(logits,tf.argmax(y, 1),1)\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nn_epochs = 1\n\nX_train = np.random.rand(100, 28)\nX_train = X_train.astype(np.float32)\n\nY_train = np.random.randint(low = 0, high = 2, size = 100, dtype=np.int32) \n\nwith tf.Session() as sess:\n    init.run()\n    for epoch in range(n_epochs):\n        _, cost, corr, acc = sess.run([training_op, loss, correct, accuracy], feed_dict={X: X_train, y: to_one_hot(Y_train)})\n        print(corr)\n        print(\'Loss: {} Accuracy: {}\'.format(cost, acc))\n'
'from sklearn.cluster import SpectralClustering\nimport numpy as np\n\nX = np.array([[1, 2], [1, 4], [10, 0],\n               [10, 2], [10, 4], [1, 0]])\n\n# 1st way - use fit and get the labels_\nclustering = SpectralClustering(n_clusters=2,\n     assign_labels="discretize",\n     random_state=0)\n\nclustering.fit(X)\nclustering.labels_\n# array([1, 1, 0, 0, 0, 1])\n\n# 2nd way - using fit_predict\nclustering2 = SpectralClustering(n_clusters=2,\n     assign_labels="discretize",\n     random_state=0)\n\nclustering2.fit_predict(X)\n# array([1, 1, 0, 0, 0, 1])\n\nnp.array_equal(clustering.labels_, clustering2.fit_predict(X))\n# True\n'
"\nexample = [[3,1,1,1]]\nlasso.predict(example)\n\n# array([0.07533937])\n\n\n#Predictions without using the .predict function \ndef lasso_predict_value_(X1,X2,X3,X4): \n    x_test = np.array([X1,X2, X3, X4])\n    preds = lasso.intercept_ + sum(x_test*lasso.coef_)\n    print('Your predicted value is: ', preds)\n\n\nlasso_predict_value_(3,1,1,1)\n\n# Your predicted value is:  [0.07533937]\n\n\nexample = [[3,1,1,1]]\nscaled_pred = lasso.predict(transformer_x.transform(example))\ntransformer_y.inverse_transform([scaled_pred])\n# array([[4.07460407]])\n\n#Predictions without using the .predict function \ndef lasso_predict_value_(X1,X2,X3,X4): \n    x_test = transformer_x.transform(np.array([X1,X2, X3, X4]).reshape(1,-1))[0]\n    preds = lasso.intercept_ + sum(x_test*lasso.coef_)\n    print('Your predicted value is: ', preds)\n    print('Your unscaled predicted value is: ', \n          transformer_y.inverse_transform([scaled_pred]))\n\n\nlasso_predict_value_(3,1,1,1)\n# Your predicted value is:  [0.0418844]    \n# Your unscaled predicted value is:  [[4.07460407]]\n"
"mycolumns = data.columns\nnumerical_columns = data._get_numeric_data().columns\ncat_features= list(set(mycolumns) - set(numerical_columns))\n\ncat_features = df.select_dtypes(['object']).columns\n\n  # Define function\n  def mean_encoding(df, cols, target):\n\n     for c in cols:\n        means = df.groupby(c)[target].mean()\n        df[c].map(means)\n\n    return df\n\n# Encode\ndata = mean_encoding(data, cat_features, target)\n"
'df = pd.DataFrame(columns=["id","text"],index=None)\ndf.loc[0] = ["1","this is a sample text"]\ndf.loc[1] = ["2","this is a second sample text"]\n\nimport collections\n\nsample_dict = collections.defaultdict()\nsample_dict["r1"] = "this is sample 1" \nsample_dict["r2"] = "is sample" \nsample_dict["r3"] = "sample text 2"\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\nngram_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer="char")\n\nall_data = np.concatenate((df.text.to_numpy(),np.array(list(sample_dict.values()))))\n\nngrammed = ngram_vectorizer.fit_transform(all_data) &gt;0\n\ntexts = ngrammed[:len(df)]\nsamples = ngrammed[len(df):]\ntext_rows = len(df)\n\njaccard_similarities = []\nfor key, ngram_sample in zip(sample_dict.keys(), samples):\n    repeated_row_matrix = (csr_matrix(np.ones([text_rows,1])) * ngram_sample).astype(bool)\n    support = texts.maximum(repeated_row_matrix)\n    intersection = texts.multiply(repeated_row_matrix).todense()\n    jaccard_similarities.append(pd.Series((intersection.sum(axis=1)/support.sum(axis=1)).A1, name=key))\n\npd.concat(jaccard_similarities, axis=1)\n\n         r1        r2        r3\n0  0.631579  0.444444  0.500000\n1  0.480000  0.333333  0.384615\n\npd.concat([df, pd.concat(jaccard_similarities, axis=1)], axis=1)\n  id                          text        r1        r2        r3\n0  1         this is a sample text  0.631579  0.444444  0.500000\n1  2  this is a second sample text  0.480000  0.333333  0.384615\n'
'# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\n'
'state = np.array([10])\n'
"parameters = {'clf__estimator__learning_rate': [0.1, 0.01, 0.001]}\n"
'from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Embedding, Input\nimport numpy as np\n\nip = Input(shape = (3,))\nemb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)\n\nmodel = Model(ip, emb)\ninput_array = np.array([[5, 3, 1], [1, 2, 3]])\n\nmodel.compile("rmsprop", "mse")\n\noutput_array = model.predict(input_array)\n\nprint(output_array)\n\nprint(output_array.shape)\n\nmodel.summary()\n\nfrom keras.models import Model\nfrom keras.layers import Embedding, Input\nimport numpy as np\n\nip = Input(shape = (3,))\nemb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)\n\nmodel = Model(ip, emb)\ninput_array = np.array([[5, 3, 1], [1, 2, 3]])\n\nmodel.compile("rmsprop", "mse")\n\noutput_array = model.predict(input_array)\n\nprint(output_array)\n\nprint(output_array.shape)\n\nmodel.summary()\n\n    def call(self, inputs):\n        if K.dtype(inputs) != \'int32\':\n            inputs = K.cast(inputs, \'int32\')\n        out = K.gather(self.embeddings, inputs)\n        return out\n\n  def call(self, inputs):\n    dtype = K.dtype(inputs)\n    if dtype != \'int32\' and dtype != \'int64\':\n      inputs = math_ops.cast(inputs, \'int32\')\n    out = embedding_ops.embedding_lookup(self.embeddings, inputs)\n    return out\n\nfrom keras.models import Model\nfrom keras.layers import Embedding, Input\nimport numpy as np\n\nip = Input(shape = (3,))\nemb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)\n\nmodel = Model(ip, emb)\ninput_array = np.array([[5, 3, 1], [1, 2, 3]])\n\nmodel.compile("rmsprop", "mse")\n\noutput_array = model.predict(input_array)\n\nprint(output_array)\n\nprint(output_array.shape)\n\nmodel.summary()\n\n[[[0. 0.]\n  [0. 0.]\n  [0. 0.]]\n\n [[0. 0.]\n  [0. 0.]\n  [0. 0.]]]\n(2, 3, 2)\nModel: "model_18"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_21 (InputLayer)        (None, 3)                 0         \n_________________________________________________________________\nembedding_33 (Embedding)     (None, 3, 2)              2         \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n\nimport keras.backend as K\n\nw = model.layers[1].get_weights()\nprint(w)\n\n\n[array([[ 0.03680499, -0.04904002]], dtype=float32)]\n\nfrom keras.models import Model\nfrom keras.layers import Embedding, Input\nimport numpy as np\n\nip = Input(shape = (3,))\nemb = Embedding(1, 2, trainable=True, mask_zero=True)(ip)\n\nmodel = Model(ip, emb)\ninput_array = np.array([[5, 0, 1], [1, 2, 0]])\n\nmodel.compile("rmsprop", "mse")\n\noutput_array = model.predict(input_array)\n\nprint(output_array)\n\nprint(output_array.shape)\n\nmodel.summary()\n\n[[[ 0.          0.        ]\n  [-0.04339869 -0.04900574]\n  [ 0.          0.        ]]\n\n [[ 0.          0.        ]\n  [ 0.          0.        ]\n  [-0.04339869 -0.04900574]]]\n(2, 3, 2)\nModel: "model_19"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_22 (InputLayer)        (None, 3)                 0         \n_________________________________________________________________\nembedding_34 (Embedding)     (None, 3, 2)              2         \n=================================================================\nTotal params: 2\nTrainable params: 2\nNon-trainable params: 0\n'
'inp = Input((3))\nx = Dense(10)(inp)\nmodel = Model(inp, x)\nX = np.array([5.1, 3.5, 1.4])\n\nmodel.predict(X[None,:])\n'
'gradient = (-2/N)*tot\n\ngradient = (-2)*tot\n\nm, b = gradient_descent(FG_pct, W_L_pct, 6, -1, 0.003, 10000)\nm = 6.465\nb = -2.44\n\nfrom matplotlib.pyplot import plot, scatter\nimport numpy as np\n\nY = np.array(W_L_pct)\nX = np.array([np.ones(len(FG_pct)), FG_pct]).reshape(2, 270).T\n\nA = np.linalg.inv(np.matmul(X.T, X))\nB = np.matmul(X.T, Y)\n\nbeta = np.matmul(A, B)\nm, b = beta[1], beta[0]\nprint(m, b)\nr = np.arange(0.4, 0.52, 0.01)\nscatter(FG_pct, Y)\nplot(r, m * r + b)\n'
'model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))\n\nmodel.add(tf.keras.layers.Dense(units=1))\n'
'test_data = torch.Tensor(test)\nraw_preds = logreg_clf(test_data)\npreds = (raw_preds &gt; 0.5).long()\n\npreds = (logreg_clf(torch.Tensor(test)) &gt; 0.5).long()    \n\ntensor([[0], [0], [0], [1]])\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# the ground truth for the given test data\ntruth = torch.Tensor([0, 1, 0, 1]).view(-1, 1) \n\nconf_mat = confusion_matrix(truth, preds)\nacc = accuracy_score(truth, preds)\n\narray([[2, 0],\n       [1, 1]], dtype=int64)\n\n0.75\n'
'for k in range(num_complete_minibatches+1):\n### START CODE HERE ### (approx. 2 lines)\n    mini_batch_X = shuffled_X[:,mini_batch_size*(k):mini_batch_size*(k+1)]\n    mini_batch_Y = shuffled_Y[:,mini_batch_size*(k):mini_batch_size*(k+1)]\n    print(mini_batch_X.shape,mini_batch_Y.shape)\n'
"s1 0 (a1 s1 0.5) (a1 s2 0.5) (a2 s1 1.0)\ns2 0 (a1 s2 1.0) (a2 s1 0.5) (a2 s3 0.5)\ns3 10 (a1 s2 1.0) (a2 s3 0.5) (a2 s4 0.5)\n\ndata = { 's1': { 'reward': 0,\n                 'action': { 'a1': { 's1': 0.5,\n                                     's2': 0.5 },\n                             'a2': { 's1': 1.0 }\n                           },\n               },\n         's2': { 'reward': 0,\n                 'action': { 'a1': { 's1': 1.0 },\n                             'a2': { 's1': 0.5,\n                                     's2': 0.5 },\n                           },\n               },\n         's3': { 'reward': 10,\n                 'action': { 'a1': { 's2': 1.0 },\n                             'a2': { 's3': 0.5,\n                                     's4': 0.5 },\n                           }\n               }\n        }\n"
'cm_t = confusion_matrix(y_test[228:317,t[228:317])\n\ncm_t = confusion_matrix(y_test[228:317],t[228:317])\n'
'inputs=T.dmatrix()\ntemp=self.layers[0].forwardPass(inputs)\nfor i in range(1,len(self.layers[:-1])):\n    temp=self.layers[i].forwardPass(T.dot\n    (temp,self.weights[i-1].transpose())+self.bias[i-1])\n    output=self.layers[-1].forwardPass(T.dot(temp,self.weights[-1].\n    transpose())+self.bias[-1])\nself.activate=theano.function([inputs],output)\n\nlabel=T.dmatrix()\nreg_lambda=T.dscalar()\nalpha=T.dscalar()\nmomentum=T.dscalar()\ncost=T.nnet.categorical_crossentropy(output,label).mean()\nfor i,j in zip(self.weights,self.bias):\n    cost+=T.sum(i**2)*reg_lambda\n    cost+=T.sum(j**2)*reg_lambda\nparameters=self.weights+self.bias\nrates=[(alpha*T.grad(cost,parameter)+momentum*prev_rate) \nfor parameter,prev_rate in zip(parameters,self.prev_rate)]\nupdates=[(weight,weight-rate) for weight,rate\n in zip(parameters,rates)]+[(prev_rate,rate) \nfor prev_rate,rate in zip(self.prev_rate,rates)]\nself.backprop=theano.function([inputs,label,reg_lambda,alpha,momentum],\ncost,updates=updates)\n'
"from sklearn import preprocessing\nenc = preprocessing.LabelEncoder()\nmushrooms = ['p','x','s','n','t','p','f','c','n','k','e','e','s','s','w','w','p','w','o']\nenc.fit(mushrooms)\nclasses = enc.transform(mushrooms)\nprint classes\nprint enc.inverse_transform(classes)\n\n[ 6 10  7  4  8  6  2  0  4  3  1  1  7  7  9  9  6  9  5]\n['p' 'x' 's' 'n' 't' 'p' 'f' 'c' 'n' 'k' 'e' 'e' 's' 's' 'w' 'w' 'p' 'w''o']\n\nclf.fit(enc.tranform(train.iloc[:, 1:], train.iloc[:, 0]))\n"
'clf.predict_proba(X_new_tfidf)\n'
"import pandas as pd\n\ndf = pd.read_csv('file_name.csv')\ny  =  df[['gini']] \nX  =  df.drop(['gini'])\n\nimport pandas as pd\n\ndf = pd.read_csv('file_name.csv')\ny  =  df[['gini']] \nX  =  df[df.columns.difference(['gini'])]\n\nimport pandas as pd\n\ndf = pd.read_csv('file_name.csv')\ny  =  df[[-1]] \nX  =  df[df.columns[0:-1]]\n\n&gt;&gt; y\n    gini\n0  0.381\n1  0.381\n2  0.381\n3  0.381\n4  0.381\n5  0.381\n6  0.381\n7  0.381\n8  0.381\n\n&gt;&gt; X \n   cty_pop2000  countyfipscode county_name statename\n0        43671            1001     Autauga   Alabama\n1        43671            1001     Autauga   Alabama\n2        43671            1001     Autauga   Alabama\n3        43671            1001     Autauga   Alabama\n4        43671            1001     Autauga   Alabama\n5        43671            1001     Autauga   Alabama\n6        43671            1001     Autauga   Alabama\n7        43671            1001     Autauga   Alabama\n8        43671            1001     Autauga   Alabama\n"
"from sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(vocabulary=vocabulary, tokenizer=lambda x: x.split(', '))\nprint(vec.fit_transform(tags).toarray())\n\n[[0 0 0 1 1 0]\n [0 1 0 0 1 1]\n [1 1 1 0 1 0]]\n"
'def score(self, X, lengths=None):\n"""Compute the log probability under the model.\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Feature matrix of individual samples.\n\n    lengths : array-like of integers, shape (n_sequences, ), optional\n        Lengths of the individual sequences in ``X``. The sum of\n        these should be ``n_samples``.\n\n    Returns\n    -------\n    logprob : float\n        Log likelihood of ``X``.\n'
"jobDict = { 'jobId': 'test_job_2', 'trainingInput': { 'scaleTier': 'BASIC', 'packageUris': [ 'gs://testtf-ml/cloudmldist/1479282298/trainer-0.0.0.tar.gz' ], 'pythonModule': 'trainer.task', 'args': [ '--train_dir=gs://testtf-ml/test_output' ], 'region': 'us-central1' } }\n"
'y = tf.add(tf.matmul(x, w), b)\n\ny = tf.add(tf.matmul(x, w), b)\ny = tf.reshape(y, shape=[-1])\n\nimport tensorflow as tf\nimport pandas as pd\n\nw = tf.Variable([[4]], dtype=tf.float64)\nb = tf.Variable([10.0], dtype=tf.float64, trainable=True)\nx = tf.placeholder(shape=(None, 1), dtype=tf.float64)\ny = tf.add(tf.matmul(x, w), b)\ny = tf.reshape(y, shape=[-1])\nlabel = tf.placeholder(shape=(None), dtype=tf.float64)\nloss = tf.reduce_mean(tf.squared_difference(y, label))\n\nmy_path = "/media/sf_ShareVM/data2.csv"\ndata = pd.read_csv(my_path, sep=";")\nmax_n_samples_to_use = 50\nxs = data.iloc[:max_n_samples_to_use, :1].as_matrix()\nys = data.iloc[:max_n_samples_to_use, 1].as_matrix()\nlr = 0.000001\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\nsess = tf.InteractiveSession()\nsess.run(tf.global_variables_initializer())\n\nfor i in range(100000):\n    _, loss_value, w_value, b_value, y_val, lab_val = sess.run([optimizer, loss, w, b, y, label], {x: xs, label: ys})\n    if i % 100 == 0:  print(i, loss_value, w_value, b_value)\n    if (i%2000 == 0 and 0&lt; i &lt; 10000):  # We use a smaller LR at first to avoid exploding gradient. It would be MUCH cleaner to use gradient clipping (by global norm)\n        lr*=2\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss)\n\nprint(sess.run(w))\n'
"--- a/Lib/weakref.py\n+++ b/Lib/weakref.py\n@@ -106,7 +106,7 @@ class WeakValueDictionary(collections.Mu\n         self, *args = args\n         if len(args) &gt; 1:\n             raise TypeError('expected at most 1 arguments, got %d' % len(args))\n-        def remove(wr, selfref=ref(self)):\n+        def remove(wr, selfref=ref(self), _atomic_removal=_remove_dead_weakref):\n             self = selfref()\n             if self is not None:\n                 if self._iterating:\n@@ -114,7 +114,7 @@ class WeakValueDictionary(collections.Mu\n                 else:\n                     # Atomic removal is necessary since this function\n                     # can be called asynchronously by the GC\n-                    _remove_dead_weakref(d, wr.key)\n+                    _atomic_removal(d, wr.key)\n         self._remove = remove\n         # A list of keys to be removed\n         self._pending_removals = []\n"
"gscv = GridSearchCV(pipeline, param_grid={'{step_name}__{parameter_name}': [possible values]}, cv=cv)\n\n# this would perform cv for the 3 different values of n_components for pca\ngscv = GridSearchCV(pipeline, param_grid={'pca__n_components': [3, 6, 10]}, cv=cv)\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n\npca = PCA()\nmodel = GaussianNB()\nsteps = [('pca', pca), ('model', model)]\npipeline = Pipeline(steps)\n\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n# get the default parameters of your model and use them as a param_grid\nmodelwithpca = GridSearchCV(pipeline, param_grid={'model__' + k: [v] for k, v in model.get_params().items()}, cv=cv)\n\n# will run 5 times as your cv is configured\nmodelwithpca.fit(X_train,y_train)\n"
'y = (x[:, 0::2, 0::2] +\n     x[:, 1::2, 0::2] +\n     x[:, 0::2, 1::2] +\n     x[:, 1::2, 1::2])\n'
"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n#Load Iris data, X: features and y:target/labels\ndf = load_iris()\ny = df.target\nX = df.data\n\n#Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state= 99)\n\n#Fit the 2 classifiers\ndt = DecisionTreeClassifier(min_samples_split=20, random_state=99)\nclf = SVC(kernel='linear', C=1)\n\ndt.fit(X_train, y_train)\ny_predicted_dt = dt.predict(X_test)\nscores_dt = accuracy_score(y_test, y_predicted_dt)\nprint(scores_dt)\n\nclf.fit(X_train, y_train)\ny_predicted_clf = clf.predict(X_test)\nscores_clf = accuracy_score(y_test, y_predicted_clf)\nprint(scores_clf)\n\n#Accuracy of dt classifier\n0.933333333333\n\n#Accuracy of clf classifier\n0.983333333333\n"
"In [9]: d1.stack().map(d2.set_index('member_ID')['Label']).unstack()\nOut[9]:\n   col1 col2\n0    a1   a3\n1    a1   b4\n2    a1   b5\n3    b2   a3\n4    b2   b4\n5    a3   a1\n6    a3   b2\n7    a3   b5\n8    b4   a1\n9    b4   b2\n10   b5   a1\n11   b5   a3\n"
'def softmax_loss_vectorized(W, X, y, reg):\n  num_train = X.shape[0]\n\n  scores = X.dot(W)\n  scores -= np.max(scores)\n  correct_scores = scores[np.arange(num_train), y]\n\n  # Compute the softmax per correct scores in bulk, and sum over its logs.\n  exponents = np.exp(scores)\n  sums_per_row = np.sum(exponents, axis=1)\n  softmax_array = np.exp(correct_scores) / sums_per_row\n  information_array = -np.log(softmax_array)\n  loss = np.mean(information_array)\n\n  # Compute the softmax per whole scores matrix, which gives the matrix for X rows coefficients.\n  # Their linear combination is algebraically dot product X transpose.\n  all_softmax_matrix = (exponents.T / sums_per_row).T\n  grad_coeff = np.zeros_like(scores)\n  grad_coeff[np.arange(num_train), y] = -1\n  grad_coeff += all_softmax_matrix\n  dW = np.dot(X.T, grad_coeff) / num_train\n\n  # Regularization\n  loss += 0.5 * reg * np.sum(W * W)\n  dW += reg * W\n\n  return loss, dW\n'
'&gt;&gt; print(clf.predict(np.array([10,10]).reshape(1,-1)))\n[1]\n'
'import numpy as np\na = np.random.random(size=(3,1))   # NOT TO USE!\na.shape  # (3, 1)\na.ndim   # 2\nb = np.random.random(size=3)       # TO USE!\nb.shape  # (3,)                    \nb.ndim   # 1\n\ngradient = (1 / m) * np.dot(X.T, (hypothesis - y)).ravel()  # .ravel()!\n...      \ninitial_theta = np.zeros(n + 1)  # drop extra-dim\n\nOptimization terminated successfully.\n         Current function value: 0.203498\n         Iterations: 27\n         Function evaluations: 71\n         Gradient evaluations: 229\n         Hessian evaluations: 0\n(array([-25.13045417,   0.20598475,   0.2012217 ]), 0.2034978435366513, 71, 229, 0, 0)\n\nfrom scipy.optimize import check_grad as cg\nprint(cg(compute_cost, compute_gradient, initial_theta, X, y))\n# 1.24034933954e-05\n'
'(x[:,None] == np.arange(x.max()+1)).astype(int)\n\n#array([[1, 0, 0],\n#       [1, 0, 0],\n#       [0, 1, 0],\n#       [1, 0, 0],\n#       [0, 0, 1]])\n\nx_ohe = np.zeros((len(x), 3), dtype=int)\nx_ohe[np.arange(len(x)), x] = 1\nx_ohe\n\n#array([[1, 0, 0],\n#       [1, 0, 0],\n#       [0, 1, 0],\n#       [1, 0, 0],\n#       [0, 0, 1]])\n'
'conv3 = AddNL([conv1, conv2])\n'
'classifier.train(input_fn = lambda: train_input_fn)\n\nclassifier.train(input_fn=train_input_fn)\n'
"num_images = is_training and _NUM_IMAGES['train'] or _NUM_IMAGES['validation']\n\nif is_training:\n    num_images = _NUM_IMAGES['train']\nelse:\n    num_images = _NUM_IMAGES['validation']\n\ninput_function = FLAGS.use_synthetic_data and get_synth_input_fn() or input_fn\n\nif FLAGS.use_synthetic_data:\n   input_function = get_synth_input_fn()\nelse:\n    input_function = input_fn()\n\n(A and B)\n\nA and B or C\n"
'if average_across_timesteps:\n  total_size = math_ops.add_n(weights)\n  total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n  log_perps /= total_size\n'
'def gradient(theta, x, y, lam):\n    theta_len = len(theta)\n    theta = theta.reshape(1, theta_len)\n\n    predictions = sigmoid(np.dot(x, np.transpose(theta))).reshape(len(x), 1)\n\n    theta_wo_bias = theta.copy()\n    theta_wo_bias[0, 0] = 0\n\n    assert (theta_wo_bias.shape == theta.shape)\n    regularization = np.squeeze(((lam / len(x)) *\n                  theta_wo_bias).reshape(theta_len, 1))\n\n    return np.sum(np.multiply((predictions - y), x), 0) / len(x) + regularization\n\nStarting loss value: 0.69314718056 \nOptimization terminated successfully.\n         Current function value: 0.201681\n         Iterations: 30\n         Function evaluations: 32\n         Gradient evaluations: 32\n7.53668131651e-08\nTrained loss value: 0.201680992316 \n'
'# If you call fit_transform(), the imputer will again learn the \n# new mean from the test data\n# Which will lead to differences and data leakage.\nimp_test_X = pd.DataFrame(imputer.transform(test_X))\n'
"temp['y'] = temp['temp_actual']\n\ny = temp['y'].values\n\ntemp = temp.drop(['y'],axis=1)\n\ndel temp['temp_actual']\n\ntemp = temp.drop(['temp_actual'], axis=1)\n"
' a = X1[i:(i+look_back), 0]   in your case metrics is just single dimention.\n\n[0.03454225 0.02062136 0.00186715 ... 0.92857565 0.64930691 0.20325924]\n\n&gt;&gt;&gt; np.ndarray(4)\narray([2.0e-323, 1.5e-323, 2.0e-323, 1.5e-323])\n&gt;&gt;&gt; a[1:2,0]\nTraceback (most recent call last):\n  File "&lt;pyshell#38&gt;", line 1, in &lt;module&gt;\n    a[1:2,0]\nIndexError: too many indices for array\n&gt;&gt;&gt; a[1:2]\narray([-2.68156159e+154])\n&gt;&gt;&gt; \n'
'for l in labels:\n    combos = list(it.combinations(df.loc[df.label == l].as_matrix(), 2))\n    for c in combos:\n        new_combos = [np.concatenate((c, (s,)), axis=0) for s in df.loc[df.label != l].as_matrix()]\n        all_combos.extend(new_combos)\n'
"# convert image and label information to array information\ndef label_img(img):\n    #split images\n    word_label = img.split('.')[-3]\n    if word_label == 'X': return [1,0,0] \n    elif word_label == 'Y': return [0,1,0]\n    elif word_label == 'Z' : return [0,0,1]\n\nconvnet = fully_connected(convnet, 3, activation='softmax')\n\n###\nif os.path.exists('{}.meta'.format(MODEL_NAME)):\n    model.load(MODEL_NAME)\n    print('model loaded!')\n###\n"
'from keras.callbacks import Callback\n\nclass LogThirdLayerOutput(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        layer_output = get_3rd_layer_output(self.validation_data)[0]\n        print(layer_output.shape)\n\nhistory = model.fit(X_train, y_train, batch_size=batch_size, verbose=1, nb_epoch=10, validation_data=(X_test,y_test), callbacks=[LogThirdLayerOutput()])\n'
'means = cv2.mean(frame)\nmeans = means[:3]\nprint(means)\n\nfrom imutils.video import VideoStream\nfrom imutils.video import FPS\nimport numpy as np\nimport imutils\nimport time\nimport cv2\nimport pyautogui\n\nfps = FPS().start()\n\nwhile True:\n    # grab the frame from the threaded video stream and resize it\n    # to have a maximum width of 400 pixels\n    frame = np.array(pyautogui.screenshot(region = (0,200, 800,400)))\n    frame = cv2.cvtColor((frame), cv2.COLOR_RGB2BGR)\n    frame = imutils.resize(frame, width=400)\n\n    #grab color in BGR order, blue value comes first, then the green, and finally the red\n    means = cv2.mean(frame)\n    #disregard 4th value\n    means = means[:3]\n    print(means)\n\n\n    # show the output frame\n    cv2.imshow("Frame", frame)\n    key = cv2.waitKey(1) &amp; 0xFF\n\n    # if the `q` key was pressed, break from the loop\n    if key == ord("q"):\n        break\n\n    # update the FPS counter\n    fps.update()\n\n# stop the timer and display FPS information\nfps.stop()\nprint("[INFO] elapsed time: {:.2f}".format(fps.elapsed()))\nprint("[INFO] approx. FPS: {:.2f}".format(fps.fps()))\n'
'probs = clf.predict_proba(new)\nprint(probs)\n\ntop3_classes = np.argsort(probs)[:3]\n\nprint(your_classes[top3_classes])\n'
'from keras.preprocessing.sequences import pad_sequences\n\nvocab_size = 10000 # only consider the 10000 most frequent words\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n\nx_train = pad_sequences(x_train, maxlen=500)  # truncate or pad sequences to make them all have a length of 500\n'
'XScaler = StandardScaler()\nXScaler.fit(XTrain)\n\nXScaler = StandardScaler()\nXScaler.fit(XFinalVal)\n'
'import keras\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n'
"model.add(layers.Dense(10, activation='relu', input_shape=X_train.shape[1:]))\n"
'model = Model(inputs=[x1, x2, x3],outputs=prediction)\n'
'data = pd.DataFrame(imp.em(data))\ndata.columns = columns\n\n...\nif pd_DataFrame and isinstance(args[0], pd_DataFrame):\n    args[0] = args[0].as_matrix()\n    return pd_DataFrame(fn(*args, **kwargs))\n\nfrom impyute.util import preprocess\n\n@preprocess\ndef test(data):\n    return data\n\ndata = pd.DataFrame({"time": [1,2,3], "size": [3,2,1]})\ncolumns = data.columns\n\ndata = pd.DataFrame(test(data), columns = columns))\n\nsize    time\n0   NaN NaN\n1   NaN NaN\n2   NaN NaN\n\n# Make new pseudo dataset\ndata = pd.DataFrame({"time": [1,2,3], "size": [3,2,1]})\ndata\n    size    time\n0   3   1\n1   2   2\n2   1   3\n\n#Make new dataset with original `data`\ndata = pd.DataFrame(data, columns = ["a", "b"])\ndata\na   b\n0   NaN NaN\n1   NaN NaN\n2   NaN NaN\n'
'import numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nN = 100\nX_train = np.random.rand(N, 4)\nY_train = np.random.choice([0,1,2], N, p=[.5, .3, .2])\n\n# Function to create model, required for KerasClassifier\ndef create_model(optimizer=\'rmsprop\', init=\'glorot_uniform\'):\n    # create model\n    model = Sequential()\n    model.add(Dense(2048, input_dim=X_train.shape[1], kernel_initializer=init, activation=\'relu\'))\n    model.add(Dense(512, kernel_initializer=init, activation=\'relu\'))\n    model.add(Dense(len(np.unique(Y_train)), kernel_initializer=init, activation=\'softmax\'))\n    # Compile model\n    model.compile(loss=\'sparse_categorical_crossentropy\', optimizer=optimizer, metrics=[\'sparse_categorical_accuracy\'])\n    return model\n\n# create model\nmodel = KerasClassifier(build_fn=create_model, class_weight="balanced", verbose=2)\n\n# grid search epochs, batch size and optimizer\noptimizers = [\'rmsprop\', \'adam\']\nepochs = [10, 50]\nbatches = [5, 10, 20]\ninit = [\'glorot_uniform\', \'normal\', \'uniform\']\n\nparam_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=\'accuracy\')\n\ngrid_result = grid.fit(X_train, Y_train)\n'
'     data = ["Caller1 5:30AM Mexico USA 2-22-19",\n            "Caller2 1:30AM Mexico USA 2-22-19",\n            "Caller3 2:30AM Mexico USA 2-22-19",\n            "Caller1 5:30AM Mexico USA 2-22-19",\n            "Caller5 3:30AM Mexico USA 2-22-19",\n            "Caller3 4:30AM Mexico USA 2-22-19",\n            "Caller2 5:30AM Mexico USA 2-22-19",\n            "Caller1 7:30AM Mexico USA 2-22-19",\n            "Caller12 9:39AM Mexico USA 2-22-19",\n            "Caller14 8:36AM Mexico USA 2-22-19",\n            "Caller15 2:39AM Mexico USA 2-22-19",\n            "Caller16 3:32AM Mexico USA 2-22-19"]\n\n    grouped_data = {}\n\n    # ITERATE THE INPUT AND STORE DATA WITH KEY IN DICTIONARY OF LIST \n    for x in data:\n        temp: list = []\n        key = x.split(\' \')[0]\n        if key in grouped_data:\n            temp = grouped_data.get(key)\n        temp.append(x)\n        grouped_data[key] = temp\n\n    # PRINT THE DATA AS GROUPED\n    for k, v in grouped_data.items():\n        print(f"data for {k}")\n        for d in v:\n            print(d)\n'
'train_data = train_data / 255.0\nvalidation_data = validation_data / 255.0\n'
'server_eval = sidekit.FeaturesServer(feature_filename_structure="./enroll/{}.h5",\n                                     ...)\n'
"import pymc3 as pm\n\nwith pm.Model() as model:\n    lambda_1 = pm.Exponential('lambda_1', 1)\n    lambda_1 = pm.Exponential('lambda_2', 1)\n"
'def display_face_and_prediction(b):\nindex = randint(0, 400)\nface = faces.images[index]\ndisplay_face(face)\nprint("this person is smiling: {0}".format(svc_1.predict(faces.data[index, :])==1))\n'
"grid_search = GridSearchCV(xgboost_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True, verbose=2)\ngrid_search.fit(my_data, my_labels, verbose=False)\n\nFitting 3 folds for each of 5 candidates, totalling 15 fits\n[CV] C=0.1 ...........................................................\n[CV] ............................................ C=0.1, total=   0.0s\n[CV] C=0.1 ...........................................................\n[CV] ............................................ C=0.1, total=   0.0s\n[CV] C=0.1 ...........................................................\n[CV] ............................................ C=0.1, total=   0.0s\n[CV] C=0.5 ...........................................................\n[CV] ............................................ C=0.5, total=   0.0s\n[CV] C=0.5 ...........................................................\n[CV] ............................................ C=0.5, total=   0.0s\n[CV] C=0.5 ...........................................................\n[CV] ............................................ C=0.5, total=   0.0s\n"
"so_data = {\n    'passenger_id': [1,2,3,4,5],\n    'survived': [1,0,0,1,0],\n    'age': [24,25,68,39,5],\n    'sex': ['female', 'male', 'male', 'female', 'female'],\n    'first_name': ['Joanne', 'Mark', 'Josh', 'Petka', 'Ariel']\n}\nso_df = pd.DataFrame(so_data)\n\n    passenger_id    survived    age   sex       first_name\n0              1           1    24  female        Joanne\n1              2           0    25  male          Mark\n2              3           0    68  male          Josh\n3              4           1    39  female        Petka\n4              5           0    5   female        Ariel\n\npd.get_dummies(so_df)\n"
"employerPredictions['predictedChurn'] = np.round(predictions).astype(np.int8)\n\n#Or you just downcast it to int\nemployerPredictions['predictedChurn'] = predictions.astype(np.int8)\n\n#Or use np.where\nemployerPredictions['predictedChurn'] = np.where(predictions&gt;=0.5,1,0)\n\nemployerPredictions['ConfidenceWillChurn %'] = np.where(predictions&gt;=0.5,predictions*100,np.nan)\n\nemployerPredictions['ConfidenceWillNotChurn %'] = np.where(predictions&lt;0.5,(1-predictions)*100,np.nan)\n"
"import pickle \nresult = model.fit()\n# save the model to disk\nfilename = 'my_model.sav'\npickle.dump(model, open(filename, 'wb'))\n\n# load the model from file for later use\nreload_model = pickle.load(open(filename, 'rb'))\nresult_final = reload_model.score(X_test, y_test)\nprint result_final\n"
'# initialize &amp; do all default training\nmodel = Word2Vec(sentences[:2], min_count=1)\n# now train again even more with a slightly different mix\nmodel.train(sentences, total_examples = len(sentences), epochs=model.epochs)\n'
'print(df)\n          0         1         2         3         4         5         6  \\\n0 -0.040404 -0.289274 -0.604957 -0.748797 -0.206201  0.573717 -0.177778   \n\n          7         8         9        10        11     inter  \n0 -0.530802 -0.210549  0.099326 -0.539109  0.879504 -1.795323 \n'
'result = np.sum(np.log1p(np.exp(w)))\n'
"import numpy as np\nimport pandas as pd\n\nnp.random.seed(2292020)\ndfBDT = pd.DataFrame({'EventNumber': np.random.randint(1, 15, 500),\n                      'Class': np.random.randint(0, 1, 500),\n                      'score': np.random.randn(500)\n                     })\n\n\ndfNN = pd.DataFrame({'EventNumber': np.random.randint(1, 15, 500),\n                     'Class': np.random.randint(0, 1, 500),\n                     'score': np.random.randn(500)\n                    })\n\ndfBDT = dfBDT.sort_values(['Class', 'EventNumber']).reset_index(drop=True)    \ndfNN = dfNN.sort_values(['Class', 'EventNumber']).reset_index(drop=True)  \n\n# ALL ROWS (NO FILTER)\ndfTotal = (dfBDT.reindex(['EventNumber', 'score'], axis='columns')\n                .join(dfNN.reindex(['EventNumber', 'score'], axis='columns'),\n                      rsuffix = '_')\n                .set_axis(['EventNumberBDT', 'BDT', 'EventNumberNN', 'NN'], \n                          axis='columns', inplace = False)\n                .reindex(['EventNumberBDT','EventNumberNN','BDT','NN'], \n                         axis='columns'))    \ndfTotal.corr()\n\n# TWO FILTERED DATA FRAMES CLASS (0 FOR BACKGROUND, 1 FOR SIGNAL)\ndf_list = [(dfBDT.query('Class == {}'.format(i))\n                 .reindex(['EventNumber', 'score'], axis='columns')\n                 .join(dfNN.query('Class == {}'.format(i))\n                           .reindex(['EventNumber', 'score'], axis='columns'),\n                       rsuffix = '_')\n                 .set_axis(['EventNumberBDT', 'BDT', 'EventNumberNN', 'NN'],\n                           axis='columns', inplace = False)\n\n                 .reindex(['EventNumberBDT','EventNumberNN','BDT','NN'],\n                          axis='columns')\n           ) for i in range(0,2)]\n\ndfSub = pd.concat(df_list)\n\ndfSub.corr()\n\ndfTotal.corr()\n#                 EventNumberBDT  EventNumberNN       BDT        NN\n# EventNumberBDT        1.000000       0.912279 -0.024121  0.115754\n# EventNumberNN         0.912279       1.000000 -0.039038  0.122905\n# BDT                  -0.024121      -0.039038  1.000000  0.012143\n# NN                    0.115754       0.122905  0.012143  1.000000\n\ndfSub.corr()\n#                 EventNumberBDT  EventNumberNN       BDT        NN\n# EventNumberBDT        1.000000       0.974140 -0.024121  0.120102\n# EventNumberNN         0.974140       1.000000 -0.026026  0.122905\n# BDT                  -0.024121      -0.026026  1.000000  0.025548\n# NN                    0.120102       0.122905  0.025548  1.000000\n\nnp.random.seed(2292020)\ndfBDT = pd.DataFrame({'EventNumber': np.random.randint(1, 15, 500),\n                      'Class': np.concatenate((np.zeros(250), np.ones(250))),\n                      'score': np.random.randn(500)\n                     })\n\n\ndfNN = pd.DataFrame({'EventNumber': np.random.randint(1, 15, 500),\n                     'Class': np.concatenate((np.zeros(250), np.ones(250))),\n                     'score': np.random.randn(500)\n                    })\n\n...\n\ndfTotal.corr()\n#                 EventNumberBDT  EventNumberNN       BDT        NN\n# EventNumberBDT        1.000000       0.992846 -0.026130  0.023623\n# EventNumberNN         0.992846       1.000000 -0.023411  0.022093\n# BDT                  -0.026130      -0.023411  1.000000 -0.026454\n# NN                    0.023623       0.022093 -0.026454  1.000000\n\n\ndfSub.corr()\n#                 EventNumberBDT  EventNumberNN       BDT        NN\n# EventNumberBDT        1.000000       0.992846 -0.026130  0.023623\n# EventNumberNN         0.992846       1.000000 -0.023411  0.022093\n# BDT                  -0.026130      -0.023411  1.000000 -0.026454\n# NN                    0.023623       0.022093 -0.026454  1.000000\n\ndef op_approach_total():\n    dfscore = pd.concat([dfBDT['score'],dfNN['score']], axis = 1)\n    dfnum = pd.concat([dfBDT['EventNumber'],dfNN['EventNumber']], axis = 1)\n\n    dfTotal = pd.concat([dfnum,dfscore], axis = 1)\n    dfTotal.columns = ['EventNumberBDT', 'EventNumberNN', 'BDT', 'NN']\n\n    return dfTotal.corr()\n\n\ndef op_approach_split():\n    # not defaulted by Event Number by default\n    BDT_back = (dfBDT.loc[dfBDT['Class'] == 0])['score']\n    BDT_back.reset_index(drop=True, inplace=True)\n\n    BDT_back_num = (dfBDT.loc[dfBDT['Class'] == 0])['EventNumber']\n    BDT_back_num.reset_index(drop=True, inplace=True)\n\n\n    NN_back = (dfNN.loc[dfNN['Class'] == 0])['score']\n    NN_back.reset_index(drop=True, inplace=True)\n\n    NN_back_num = (dfNN.loc[dfNN['Class'] == 0])['EventNumber'] \n    NN_back_num.reset_index(drop=True, inplace=True)\n\n\n    dfBack = pd.concat([BDT_back_num,NN_back_num,BDT_back,NN_back],\n                       axis = 1)\n    dfBack.reset_index(drop=True, inplace=True)\n    dfBack.columns = ['EventNumberBDT','EventNumberNN','BDT','NN']\n\n\n    # not defaulted by Event Number by default\n    BDT_sig = (dfBDT.loc[dfBDT['Class'] == 1])['score']\n    BDT_sig.reset_index(drop=True, inplace=True)\n\n    BDT_sig_num = (dfBDT.loc[dfBDT['Class'] == 1])['EventNumber']\n    BDT_sig_num.reset_index(drop=True, inplace=True)\n\n    NN_sig = (dfNN.loc[dfNN['Class'] == 1])['score']\n    NN_sig.reset_index(drop=True, inplace=True)\n\n    NN_sig_num = (dfNN.loc[dfNN['Class'] == 1])['EventNumber']\n    NN_sig_num.reset_index(drop=True, inplace=True)\n\n\n    dfSig = pd.concat([BDT_sig_num, NN_sig_num, BDT_sig, NN_sig],\n                       axis = 1)\n    dfSig.reset_index(drop=True, inplace=True)\n    dfSig.columns = ['EventNumberBDT','EventNumberNN','BDT','NN']\n\n    # ADDING EventNumber COLUMNS\n    ev_back = pd.concat([dfBack['EventNumberBDT'], dfSig['EventNumberBDT']])\n    ev_sig = pd.concat([dfBack['EventNumberNN'], dfSig['EventNumberNN']])\n\n\n    ab = pd.concat([dfBack['BDT'], dfSig['BDT']])\n\n    ba = pd.concat([dfBack['NN'], dfSig['NN']])\n\n    # HORIZONTAL MERGE\n    abba = pd.concat([ev_back, ev_sig, ab, ba], axis = 1)\n\n    return abba.corr()\n\nopTotal = op_approach_total()\nopSub = op_approach_split()\n\nopTotal = op_approach_total()\nopTotal\n#                 EventNumberBDT  EventNumberNN       BDT        NN\n# EventNumberBDT        1.000000       0.992846 -0.026130  0.023623\n# EventNumberNN         0.992846       1.000000 -0.023411  0.022093\n# BDT                  -0.026130      -0.023411  1.000000 -0.026454\n# NN                    0.023623       0.022093 -0.026454  1.000000\n\nopSub = op_approach_split()\nopSub\n#                 EventNumberBDT  EventNumberNN       BDT        NN\n# EventNumberBDT        1.000000       0.992846 -0.026130  0.023623\n# EventNumberNN         0.992846       1.000000 -0.023411  0.022093\n# BDT                  -0.026130      -0.023411  1.000000 -0.026454\n# NN                    0.023623       0.022093 -0.026454  1.000000\n"
"{'clf': [OneVsRestClassifier(SVC(tol=0.1, gamma='scale', probability=True), n_jobs=-1],\n 'clf__estimator__kernel': ['rbf', 'linear'],\n 'clf__estimator__C': [1, 10, 100]}\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples = 10000, n_features=5, n_redundant=0, n_informative=5,\n                             n_classes = 3, n_clusters_per_class=1, random_state=42)\n\n%timeit for x in range(10): SVC().fit(X,y)\n# 1 loop, best of 3: 7.72 s per loop\n\n%timeit for x in range(10): OneVsRestClassifier(SVC()).fit(X, y)\n# 1 loop, best of 3: 21.1 s per loop\n\n%timeit for x in range(10): OneVsRestClassifier(SVC(), n_jobs=-1).fit(X, y)\n# 1 loop, best of 3: 19 s per loop\n\nX1, y1 = make_classification(n_samples = 10000, n_features=10, n_redundant=0, n_informative=10,\n                             n_classes = 5, n_clusters_per_class=1, random_state=42)\n\n%timeit for x in range(10): SVC().fit(X1,y1)\n# 1 loop, best of 3: 10.3 s per loop\n\n%timeit for x in range(10): OneVsRestClassifier(SVC()).fit(X1, y1)\n# 1 loop, best of 3: 30.7 s per loop\n\n%timeit for x in range(10): OneVsRestClassifier(SVC(), n_jobs=-1).fit(X1, y1)\n# 1 loop, best of 3: 24.9 s per loop\n"
'from sklearn.utils.murmurhash import murmurhash3_bytes_s32\n\nraw_X = test[\'type\']\nraw_X = iter(raw_X)\nraw_X = (((f, 1) for f in x) for x in raw_X)\n\nfor x in raw_X:\n    for f, v in x:\n        f = f\'{f}={v}\'\n        fb = (f).encode("utf-8")\n        h = murmurhash3_bytes_s32(fb, seed=0)\n        print(f\'{f[0]} -&gt; {h}\')\n\na -&gt; -424864564\nb -&gt; -992685778\nc -&gt; -1984769100\nd -&gt; 728527081\ne -&gt; 2077529484\nf -&gt; 2074045163\ng -&gt; -1877798433\nh -&gt; -51608576\n'
'dJ/dW2 = dJ/do * do/dW2 (chain rule)\ndJ/dW2 = (2*(o - y)) * (o*(1 - o)*h)\ndW2 (equals above equation)\nW2 -= learning_rate*dW2\n\ndJ/dh = dJ/do * do/dh = (2*(o - y)) * (o*(1 - o)*W2  \ndJ/dW1 = dJ/dh * dh/dW1 = ((2*(o - y)) * (o*(1 - o)*W2)) * (h*(1- h)*x)  \ndW1 (equals above equation)\nW1 -= learning_rate*dW2\n'
'## define left model\nleft = Input((33))\nxl = Dense(512)(left)\nleft_model = Model(left, xl)\n\n## define right model\nright = Input((10))\nxr = Dense(64)(right)\nright_model = Model(right, xr)\n\n## define final shared model\nconcat_inp = Input((576))\nx = BatchNormalization()(concat_inp)\nout = Dense(1)(x)\ncombi_model = Model(concat_inp, out)\n\n## combine left and right model\nconcat = Concatenate()([left_model.output, right_model.output])\n## combine branches with final shared model\ncombi = combi_model(concat)\n\nfull_model = Model([left_model.input, right_model.input], combi)\n\n# full_model.fit(...)\n\n## replace left branch in fitted model\nfake_left_input = Input((512))\n\n## combine fake left branch with right fitted branch \nnew_concat = Concatenate()([fake_left_input, right_model.output])\n## combine branches with final shared model\nnew_combi = combi_model(new_concat)\n\nnew_full_model = Model([fake_left_input, right_model.input], new_combi)\nnew_full_model.summary()\n\nX_right_test = np.random.uniform(0,1, (20,10))\nX_left_test = np.zeros((len(X_right_test),512)) \nnew_full_model([X_left_test, X_right_test])\n'
"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', categorical_transformer, [0])\n    ])\n\ndf = pd.DataFrame(['Male', 'Female', np.nan])\npreprocessor.fit_transform(df)\narray([[0., 1.],\n       [1., 0.],\n       [1., 0.]])\n"
'def sampling(args):\n    # ...\n    eps = K.random_normal(shape=(K.shape(mu)[0], min_dim)\n'
"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom tensorflow.keras import Model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Concatenate\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\nfrom collections import deque\n\ndataset, info = tfds.load('mnist',\n                          with_info=True,\n                          split='train',\n                          as_supervised=False)\n\nTAKE = 1_000\n\ndata = dataset.map(lambda x: (tf.cast(x['image'],\n                       tf.float32), x['label'])).shuffle(TAKE).take(TAKE)\n\nlen_train = int(8e-1*TAKE)\n\ntrain = data.take(len_train).batch(8)\ntest = data.skip(len_train).take(info.splits['train'].num_examples - len_train).batch(8)\n\n\nclass CNN(Model):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.layer1 = Dense(32, activation=tf.nn.relu,\n                            kernel_regularizer=l1(1e-2),\n                            input_shape=info.features['image'].shape)\n        self.layer2 = Conv2D(filters=16,\n                             kernel_size=(3, 3),\n                             strides=(1, 1),\n                             activation='relu',\n                             input_shape=info.features['image'].shape)\n        self.layer3 = MaxPooling2D(pool_size=(2, 2))\n        self.layer4 = Conv2D(filters=32,\n                             kernel_size=(3, 3),\n                             strides=(1, 1),\n                             activation=tf.nn.elu,\n                             kernel_initializer=tf.keras.initializers.glorot_normal)\n        self.layer5 = MaxPooling2D(pool_size=(2, 2))\n        self.layer6 = Flatten()\n        self.layer7 = Dense(units=64,\n                            activation=tf.nn.relu,\n                            kernel_regularizer=l2(1e-2))\n        self.layer8 = Dense(units=64,\n                            activation=tf.nn.relu,\n                            kernel_regularizer=l1_l2(l1=1e-2, l2=1e-2))\n        self.layer9 = Concatenate()\n        self.layer10 = Dense(units=info.features['label'].num_classes)\n\n    def call(self, inputs, training=None, **kwargs):\n        b = self.layer1(inputs)\n        a = self.layer2(inputs)\n        a = self.layer3(a)\n        a = self.layer4(a)\n        a = self.layer5(a)\n        a = self.layer6(a)\n        a = self.layer8(a)\n        b = self.layer7(b)\n        b = self.layer6(b)\n        x = self.layer9([a, b])\n        x = self.layer10(x)\n        return x\n\n\ncnn = CNN()\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\ntrain_loss = tf.keras.metrics.Mean()\ntest_loss = tf.keras.metrics.Mean()\n\ntrain_acc = tf.keras.metrics.SparseCategoricalAccuracy()\ntest_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n\noptimizer = tf.keras.optimizers.Nadam()\n\ntemplate = 'Epoch {:3} Train Loss {:7.4f} Test Loss {:7.4f} ' \\\n           'Train Acc {:6.2%} Test Acc {:6.2%} '\n\nepochs = 5\nearly_stop = epochs//50\n\nloss_hist = deque()\nacc_hist = deque(maxlen=1)\nacc_hist.append(0)\n\nfor epoch in range(1, epochs + 1):\n    train_loss.reset_states()\n    test_loss.reset_states()\n    train_acc.reset_states()\n    test_acc.reset_states()\n\n    for images, labels in train:\n        with tf.GradientTape() as tape:\n            logits = cnn(images, training=True)\n            loss = loss_object(labels, logits)\n            train_loss(loss)\n            train_acc(labels, logits)\n\n            current_acc = tf.metrics.SparseCategoricalAccuracy()(labels, logits)\n\n            if tf.greater(current_acc, acc_hist[-1]):\n                print('IMPROVEMENT.')\n                gradients = tape.gradient(loss, cnn.trainable_variables)\n                optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n                acc_hist.append(current_acc)\n\n    for images, labels in test:\n        logits = cnn(images, training=False)\n        loss = loss_object(labels, logits)\n        test_loss(loss)\n        test_acc(labels, logits)\n\n    print(template.format(epoch,\n                          train_loss.result(),\n                          test_loss.result(),\n                          train_acc.result(),\n                          test_acc.result()))\n\n    if len(loss_hist) &gt; early_stop and loss_hist.popleft() &lt; min(loss_hist):\n        print('Early stopping. No validation loss decrease in %i epochs.' % early_stop)\n        break\n\nIMPROVEMENT.\nIMPROVEMENT.\nIMPROVEMENT.\nIMPROVEMENT.\nEpoch   1 Train Loss 21.1698 Test Loss 21.3391 Train Acc 37.13% Test Acc 38.50% \nIMPROVEMENT.\nIMPROVEMENT.\nIMPROVEMENT.\nEpoch   2 Train Loss 13.8314 Test Loss 12.2496 Train Acc 50.88% Test Acc 52.50% \nEpoch   3 Train Loss 13.7594 Test Loss 12.5884 Train Acc 51.75% Test Acc 53.00% \nEpoch   4 Train Loss 13.1418 Test Loss 13.2374 Train Acc 52.75% Test Acc 51.50% \nEpoch   5 Train Loss 13.6471 Test Loss 13.3157 Train Acc 49.63% Test Acc 51.50% \n\n    for images, labels in train:\n        with tf.GradientTape() as tape:\n            logits = cnn(images, training=True)\n            loss = loss_object(labels, logits)\n            train_loss(loss)\n            train_acc(labels, logits)\n\n            current_acc = tf.metrics.SparseCategoricalAccuracy()(labels, logits)\n\n            if tf.greater(current_acc, acc_hist[-1]):\n                print('IMPROVEMENT.')\n                gradients = tape.gradient(loss, cnn.trainable_variables)\n                optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n                acc_hist.append(current_acc)\n"
"df_final = pd.merge(df['review'], df['result'], left_index=True, right_index=True)\ndf_final\n\n0                                              review  neutral\n1                      https://www.cnbc.com/business/  neutral\n2   https://www.cnbc.com/2020/09/15/stocks-making-...  neutral\n3   https://www.cnbc.com/2020/09/15/stocks-making-...  neutral\n4             https://www.cnbc.com/maggie-fitzgerald/  neutral\n..                                                ...      ...\n90                      https://www.cnbc.com/finance/  neutral\n91  https://www.cnbc.com/2020/09/10/citi-ceo-micha...  neutral\n92                https://www.cnbc.com/central-banks/  neutral\n93  https://www.cnbc.com/2020/09/10/watch-ecb-pres...  neutral\n94               https://www.cnbc.com/finance/?page=2  neutral\n"
'   ob = cv2.resize(ob, (inx,iny)) # 1\n   ob = cv2.cvtColor(ob, cv2.COLOR_BGR2GRAY) # 2\n'
"1 2 3\n4 5 6\n\n1 2\n3 4\n\n1  2  3  4\n5  6  7  8\n9 10 11 12\n\n1 2 0\n3 4 0\n\n0 1 2\n0 3 4\n\n1  2  3\n5  6  7\n\ny_pred = new_model.predict(\n    np.pad(X, ((0, 0), (0, 2)))\n)\n\nmodel = Sequential([\n    Dense(units=11, activation='relu', input_shape = (44,), kernel_regularizer=keras.regularizers.l2(0.001)),\n    Dense(units=1, activation='sigmoid')\n])\n"
"9, border&lt;257.23    #feature index, border value\n\nSex, value=Female    #feature name, value\n\n{five} pr_num1 tb0 type0, value&gt;9  #Label, value\n\n##Inspecting github, the label seems to be from a multihash\n##The multihash seems to be made from (CatFeatureIdx, CtrIdx, TargetBorderIdx, PriorIdx)\n##https://github.com/catboost/catboost/blob/master/catboost/libs/data/ctrs.h\n\nfrom catboost import CatBoostClassifier, Pool\nimport pandas as pd\n\nX_train.describe().loc['unique']\n\none      6\ntwo      5\nthree    8\nfour     8\nfive     4\nsix      6\nseven    5\nName: unique, dtype: object\n\ncat_features = list(X_train.columns)\npool = Pool(X_train, y_train, cat_features=list(range(7)), feature_names=cat_features)\nmodel = CatBoostClassifier(verbose=0, one_hot_max_size=4).fit(pool)\n\nmodel.plot_tree(tree_idx=1,pool=pool)\n\ncat_features = list(X_train.columns)\npool = Pool(X_train, y_train, cat_features=list(range(7)), feature_names=cat_features)\nmodel = CatBoostClassifier(verbose=0, one_hot_max_size=8).fit(pool)\n\nmodel.plot_tree(tree_idx=1,pool=pool)\n"
"def train_kmeans(X):\n    kmeans = KMeans(n_clusters=5, verbose=2, n_init=1)\n    kmeans.fit(X)\n    return kmeans\n\nX = np.random.random((1000,7))\ntrain_kmeans(X)\n\nInitialization complete\nIteration 0, inertia 545.5728914456803\nIteration 1, inertia 440.5225419317938\nIteration 2, inertia 431.87478970379755\nIteration 3, inertia 427.52125502838504\nIteration 4, inertia 425.75105209622967\nIteration 5, inertia 424.7788124997543\nIteration 6, inertia 424.2111904252263\nIteration 7, inertia 423.7217490965455\nIteration 8, inertia 423.29439165408354\nIteration 9, inertia 422.9243615021072\nIteration 10, inertia 422.54144662407566\nIteration 11, inertia 422.2677910840504\nIteration 12, inertia 421.98686844470336\nIteration 13, inertia 421.76289612029376\nIteration 14, inertia 421.59241427498324\nIteration 15, inertia 421.36516415785724\nIteration 16, inertia 421.23801796298704\nIteration 17, inertia 421.1065220191125\nIteration 18, inertia 420.85788031236586\nIteration 19, inertia 420.6053961581343\nIteration 20, inertia 420.4998816171483\nIteration 21, inertia 420.4436034595902\nIteration 22, inertia 420.39833211852346\nIteration 23, inertia 420.3583721574586\nIteration 24, inertia 420.32684273674226\nIteration 25, inertia 420.2786269304449\nIteration 26, inertia 420.24149714604516\nIteration 27, inertia 420.22255866139835\nIteration 28, inertia 420.2075247585145\nIteration 29, inertia 420.19985517233584\nIteration 30, inertia 420.18983415887305\nIteration 31, inertia 420.18584733421886\nConverged at iteration 31: center shift 8.716337631121295e-33 within tolerance 8.370287188573764e-06\n\nimport io\nimport sys\nimport numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n#Dummy data\nX = np.random.random((1000,7)) \n\ndef train_kmeans(X):\n    kmeans = KMeans(n_clusters=5, verbose=2, n_init=1) #&lt;-- init=1, verbose=2\n    kmeans.fit(X)\n    return kmeans\n\n#HELPER FUNCTION\n#Takes the returned and printed output of a function and returns it as variables\n#In this case, the returned output is the model and printed is the verbose intertia at each iteration\n\ndef redirect_wrapper(f, inp):\n    old_stdout = sys.stdout\n    new_stdout = io.StringIO()\n    sys.stdout = new_stdout\n\n    returned = f(inp)                #&lt;- Call function\n    printed = new_stdout.getvalue()  #&lt;- store printed output\n\n    sys.stdout = old_stdout\n    return returned, printed\n\n\nreturned, printed = redirect_wrapper(train_kmeans, X)\n\n#Extract inertia values\ninertia = [float(i[i.find('inertia')+len('inertia')+1:]) for i in printed.split('\\n')[1:-2]]\n\n#Plot!\nplt.plot(inertia)\n"
"X = np.random.uniform(0,1, (100,30))\ny = np.random.uniform(0,1, (100,1))\n\ndef custom_metric(true, pred):\n    abs_error = tf.abs(true - pred)\n    error = tf.keras.backend.switch(pred &lt; true, abs_error/2, abs_error*2)\n    return tf.reduce_mean(error)\n    \ninp = Input((30,))\nx = Dense(32)(inp)\nout = Dense(1)(x)\n\nmodel = Model(inp, out)\nmodel.compile('adam', 'mse', metrics=custom_metric)\nmodel.fit(X,y, epochs=3)\n"
'y_pred = classifier.predict(x_train)\n\ny_pred = classifier.predict(x_test)\n'
'onehotencoder = OneHotEncoder()\nX_oh = onehotencoder.fit_transform(X).toarray()\n\nonehotencoder = OneHotEncoder()\nX_oh = onehotencoder.fit_transform(X.reshape(-1,1)).toarray()\n\nX.reshape(-1,1)\n'
'set_1 = [["yes", 1], ["maybe", 1], ["never", 0], ["nopes", 0], ["si", 1]]\nset_2 = ["of course", "yes", "always", "never", "no way", "no"]\n\ndef predict_label(item):\n    return 2 # just to check which items got predicted\n\ndset_1 = dict(set_1)\n\nlabeled_set_2 = [[item, dset_1.get(item, predict_label(item))] for item in set_2]\nprint labeled_set_2\n\n[[\'of course\', 2], [\'yes\', 1], [\'always\', 2], [\'never\', 0], [\'no way\', 2], [\'no\', 2]]\n\nset_1 = [["yes", 1], ["maybe", 1], ["never", 0], ["nopes", 0], ["si", 1]]\nset_2 = ["of course", "yes", "always", "never", "no way", "no"]\n\ndef predict_label(item):\n    return 2 # just to check which items got predicted\n\ndef labeled_set(set1, set2):\n    dset_1 = dict(set1)\n    labeled_set_2 = []\n    for item in set2:\n        if item in dset_1.keys():\n            labeled_set_2.append([item, dset_1[item]])\n        else:\n            labeled_set_2.append([item, predict_label(item)])\n    return labeled_set_2\n\nprint labeled_set(set_1, set_2)\n'
'from sklearn.metrics import f1_score\n'
'self.fc.setdefault(cat,0)\n\nself.fc[f].setdefault(cat,0)\n'
'from nltk import word_tokenize\ndef tokenizer(x):\n    return ( w for w in word_tokenize(x) if len(w) &gt;3)\n'
'1,1,0\nA,3,1\n5,5,0\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; pd.get_dummies(['male', 'female'])\n   female  male\n0       0     1\n1       1     0\n"
"prediction_list.append(regr.predict(test_set_feature_list))\nnp.set_printoptions(formatter={'float_kind':'{:f}'.format})\nfor items in prediction_list:\n    final_prediction.append(items)\n\nfinal_prediction.extend(regr.predict(test_set_feature_list))\n"
'LabelEncoder : turn your string into incremental value\nOneHotEncoder : use One-of-K algorithm to transform your String into integer\n'
"import pandas as pd\nimport numpy as np\n\nN = 300\nD = 31\n\ny_train = pd.Series([0,1]*(N/2))\nX_train = np.matrix(y_train).T.repeat(D, axis=1) + np.random.normal(size=(N, D))\nX_train = pd.DataFrame(X_train)\n\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nparameters = {'loss': [ 'hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], 'alpha': [0.1, 0.01], 'n_iter': [1,2, 1000]}\nclf = GridSearchCV(estimator = SGDClassifier(), param_grid = parameters, scoring = 'f1')\nprint(clf)\nclf.fit(X_train, y_train) \n"
'def new_predict(clf, new_bias, X):\n  return np.sign(clf.decision_function(X) + clf.intercept_ - new_bias)\n'
"import pandas as pd\ndf = pd.DataFrame(np.random.randint(100, size=(150, 3)), columns=list('XYZ'))\ndf.info()\n\nRangeIndex: 150 entries, 0 to 149\nData columns (total 3 columns):\nX    150 non-null int64\nY    150 non-null int64\nZ    150 non-null int64\n\nimport numpy as np\nimport statsmodels.api as sm\n\nmodel = sm.OLS(df['Z'], df[['X', 'Y']])\nresults = model.fit()\n\nresults.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Z   R-squared:                       0.652\nModel:                            OLS   Adj. R-squared:                  0.647\nMethod:                 Least Squares   F-statistic:                     138.6\nDate:                Fri, 17 Jun 2016   Prob (F-statistic):           1.21e-34\nTime:                        13:48:38   Log-Likelihood:                -741.94\nNo. Observations:                 150   AIC:                             1488.\nDf Residuals:                     148   BIC:                             1494.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]\n------------------------------------------------------------------------------\nX              0.5224      0.076      6.874      0.000         0.372     0.673\nY              0.3531      0.076      4.667      0.000         0.204     0.503\n==============================================================================\nOmnibus:                        5.869   Durbin-Watson:                   1.921\nProb(Omnibus):                  0.053   Jarque-Bera (JB):                2.990\nSkew:                          -0.000   Prob(JB):                        0.224\nKurtosis:                       2.308   Cond. No.                         2.70\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nparams = results.params\nparams = results.params\ndf['predictions'] = model.predict(params)\n\n    X   Y   Z  predictions\n0  31  85  75    54.701830\n1  36  46  43    34.828605\n2  77  42   8    43.795386\n3  78  84  65    66.932761\n4  27  54  50    36.737606\n"
'activation = dot(array([[0,0,-1],[1,0,-1],[1,1,-1],[0,1,-1]]),wei)\n\n[[ 0.30021868]\n [ 0.67476151]\n [ 1.0276208 ]\n [ 0.65307797]]\n\n[[ 0.]\n [ 1.]\n [ 1.]\n [ 1.]]\n\n[[ 0.25000001]\n [ 0.75      ]\n [ 1.24999999]\n [ 0.75      ]]\n'
" Warning: Desired error not necessarily achieved due to precision loss.\n     Current function value: 0.693147\n     Iterations: 0\n     Function evaluations: 43\n     Gradient evaluations: 41\n\nResult = op.minimize(fun = costFn, \n                x0 = initial_theta, \n                args = (X, y, m),\n                method = 'Nelder-Mead',\n                options={'disp': True})#,\n                #jac = grad)\n\nOptimization terminated successfully.\n     Current function value: 0.203498\n     Iterations: 157\n     Function evaluations: 287\n"
'model = BayesianGaussianMixture(n_components=30).fit(X)\nprint("active components: %d" % np.sum(model.weights_ &gt; 0.01)\n'
'bestclf = copy.deepcopy(clf)\n'
"with tf.variable_scope('test') as scope:\n    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n    state = cell.zero_state(batch_size, dtype=tf.float32)\n    outputs = []\n    for step, input_ in enumerate(words):\n        if step &gt; 0:\n            scope.reuse_variables()\n        output, state = cell(input_, state)\n        outputs.append(output)\n"
"import tensorflow as tf\n\nx = tf.Variable([1, -2, 3], tf.int32, name='x')\n\nimport tensorflow as tf\n\nx = tf.placeholder(tf.int32, name='x')\n"
"summary_imgs = tf.transpose(imgs[:summary_img_num], perm=[0,3,1,2])\ntf.summary.image('images', summary_imgs, max_outputs=summary_img_num)\n\nsummary_imgs = tf.transpose(imgs[:3], perm=[0,3,1,2])\ntf.summary.image('images', summary_imgs)\n"
'from pyspark.sql.window import Window\nfrom pyspark.sql import functions as func\n\nwindow = Window.partitionBy(func.col("CustomerID"))\\\n        .orderBy(func.col("InvoiceNo").desc())\ndf = df.select(\'*\', func.rank().over(window).alias(\'rank\'))\n\ntrain = df.filter("rank &gt; 2")\ntest = df.filter("rank &lt;= 2")\n'
"for ind in range(len(self.dataset_names)):\n    # execute your experiments and save results\n\n    df = pd.DataFrame({\n        'best_score': [best_seq.score(X_test, y_test)], \n        'duration': [seq_tic-seq_toc], \n        'train_samples': [X_train.shape[0]], \n        'test_shape': [X_test.shape[0]], \n        'train_time_points': [X_train.shape[1]]\n    })\n\n    df.to_csv('%s_results.csv' % self.dataset_names[ind])\n"
'with tf.name_scope("train"):\n    X = tf.placeholder(tf.float32, [1, 224, 224, 3], name=\'X\')\n    y = tf.placeholder(tf.float32, [1, 224, 224, 3], name=\'y\') \n\n    X_var = tf.get_variable(\'X_var\', dtype = tf.float32, initializer = tf.random_normal((1, 224, 224, 3)))\n    y_var = tf.get_variable(\'y_var\', dtype = tf.float32, initializer = tf.random_normal((1, 224, 224, 3)))\n    Z = tf.nn.l2_loss((X_var - X) ** 2 + (y_var - y) ** 2, name="loss")\n\n    step_loss = tf.reduce_mean(Z)\n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(step_loss)\n\n...\nwith tf.Session() as sess:\n    ....\n    sess.run(training_op, feed_dict={X: c, y: n})\n'
'cTe = sess.run([loss], feed_dict={input_tensor: batch_xTe, output_tensor: batch_yTe})\n'
'aInput = Input(...)\nencodedA = LotsOfLayers(...)(aInput)\n\nself.encoderA = Model(aInput,encodedA)\n\nbInput = Input(...)\nencodedB = LotsOfLayers(...)(bInput)\n\nself.encoderB = Model(bInput,encodedB)\n\nencodedInput = Input(...)\ndecodedOutput = LotsOfLayers(...)(encodedInput)\n\nself.decoder = Model(encodedInput, decodedOutput)\n\nautoInput = Input(sameShapeAsEncoderBInput)\nencoded = self.encoderB(autoInput)\ndecoded = self.decoder(encoded)\n\nself.autoencoderB = Model(autoInput,decoded)\n\nanotherAInput = Input(sameShapeAsEncoderAInput)\nencoded = self.encoderA(anotherAInput)\ndecoded = self.decoder(encoded)\n\nself.predictorFromA = Model(anotherAInput,decoded)\n'
"regressor = Sequential()\nregressor.add(Dense(units=20, activation='relu', input_dim=1)) \nregressor.add(Dense(units=20, activation='relu')) \nregressor.add(Dense(units=20, activation='relu')) \nregressor.add(Dense(units=1))\nregressor.compile(loss='mean_squared_error', optimizer='adam')\n\nregressor.fit(X, Y, epochs=100, verbose=1, batch_size=32)\n"
'[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3] \n\n[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]\n'
"from sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = [\n'All my cats in a row',\n'When my cat sits down, she looks like a Furby toy!',\n'The cat from outer space',\n'Sunshine loves to sit like this for some reason.'\n]\n\nvocabulary = {u'all': 0, u'sunshine': 1, u'some': 2, u'down': 3, u'reason': 4}\n\nvectorizer = CountVectorizer(vocabulary=vocabulary)\n\nprint( vectorizer.transform(corpus).todense() )\n[[1 0 0 0 0]\n [0 0 0 1 0]\n [0 0 0 0 0]\n [0 1 1 0 1]]\n\nprint( vectorizer.vocabulary_ )\n{'all': 0, 'sunshine': 1, 'some': 2, 'down': 3, 'reason': 4}\n"
"classifier.add(Dense(output_dim = 1, activation = 'softmax'))\n\nclassifier.add(Dense(output_dim = 5, activation = 'softmax'))\n"
'import pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.datasets import samples_generator\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\ndf = pd.read_csv("finalupdatedothers.csv")\ncategories = [\'ADR\',\'WD\',\'EF\',\'INF\',\'SSI\',\'DI\',\'others\']\n\ntrain,test = train_test_split(df,random_state=42,test_size=0.3,shuffle=True)\nX_train = train.sentences\nX_test = test.sentences\n\nSVC_pipeline = Pipeline([\n                (\'tfidf\', TfidfVectorizer(stop_words=[])),\n                (\'clf\', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n            ])\n\n\nfor category in categories:\n    print(\'... Processing {} \'.format(category))\n    SVC_pipeline.fit(X_train,train[category])\n    prediction = SVC_pipeline.predict(X_test)\n    print([{X_test.iloc[i]:categories[prediction[i]]} for i in range(len(list(prediction)))  ])\n\n    print(\'SVM Linear Test accuracy is {} \'.format(accuracy_score(test[category], prediction)))\n    print (\'SVM Linear f1 measurement is {} \'.format(f1_score(test[category], prediction, average=\'weighted\')))\n    print ("\\n")\n\n... Processing ADR \n[{\'extreme weight gain, short-term memory loss, hair loss.\': \'ADR\'}, {\'I am detoxing from Lexapro now.\': \'ADR\'}]\nSVM Linear Test accuracy is 0.5 \nSVM Linear f1 measurement is 0.3333333333333333 \n\n\n... Processing WD \n[{\'extreme weight gain, short-term memory loss, hair loss.\': \'ADR\'}, {\'I am detoxing from Lexapro now.\': \'ADR\'}]\nSVM Linear Test accuracy is 1.0 \nSVM Linear f1 measurement is 1.0 \n'
'┏━━━━━━━━━━━━┳━━━━━┳━━━━━┓\n┃ Eye Colour ┃ x11 ┃ x12 ┃\n┣━━━━━━━━━━━━╋━━━━━╋━━━━━┫\n┃ Blue       ┃  0  ┃  0  ┃\n┣━━━━━━━━━━━━╋━━━━━╋━━━━━┫\n┃ Brown      ┃  1  ┃  0  ┃\n┣━━━━━━━━━━━━╋━━━━━╋━━━━━┫\n┃ Green      ┃  0  ┃  1  ┃\n┗━━━━━━━━━━━━┻━━━━━┻━━━━━┛\n'
"df[df['PQ'] &gt; (df['TGGs'] * 0.5)]\n"
'params = alg_params[alg.__class__.__name__][0] \n'
'c = Lambda(mix, output_shape=(6,))([a, b])\n\ndef mix_output_shape(input_shape):\n    # input_shape[0] is the shape of first input tensor\n    # input_shape[1] is the shape of second input tensor\n    return (input_shape[0][0], input_shape[0][1] * input_shape[1][1])\n\n# ...\nc = Lambda(mix, mix_output_shape)([a, b])\n'
'def inverse_prob(model_probs):\n    model_probs[model_probs == 0 ] = 1e-5\n    inverse = 1/model_probs\n    return inverse/inverse.sum(axis=0)\n'
"from keras.layers import Dense\nfrom keras.models import Model\nmy_input = Input(shape = (4, ))\nx = Dense(25, activation='relu')(x)\nx = Dense(4)(x)\nmy_model = Model(input=my_input, output=x, loss='mse', metrics='mse')\nmy_model.compile(optimizer=Adam(LEARNING_RATE), loss='binary_crossentropy', metrics=['mse'])\n"
'def predict(self, X):\n    """\n    Predict class labels for samples in X.\n    Parameters\n    ----------\n    X : array_like or sparse matrix, shape (n_samples, n_features)\n        Samples.\n    Returns\n    -------\n    C : array, shape [n_samples]\n        Predicted class label per sample.\n    """\n    scores = self.decision_function(X)\n    if len(scores.shape) == 1:\n        indices = (scores &gt; 0).astype(np.int)\n    else:\n        indices = scores.argmax(axis=1)\n    return self.classes_[indices]\n\nscores = np.array([[0.5, 0.5]])\nscores.argmax(axis=1)\nOut[5]: array([0])\n'
"# dummy dataframe\nN=20\nN_features = 10\nN_classes = 5\ndf = pd.DataFrame({f'feat_{i+1}': np.random.random(size=(N,)) for i in range(N_features)})\ndf['target'] = np.random.choice([f'Class_{i+1}' for i in range(N_classes)], size=(N,))\n\n# transform from wide to long, then plot using the column 'features' to facet\ndf2 = df.melt(id_vars=['target'], var_name='features')\nsns.catplot(data=df2, x='value', y='target', col='features', col_wrap=5, height=3, aspect=0.5)\n"
"from sklearn.model_selection import train_test_split\n\nnum_train = 50\n\ntrain_x = []\ntrain_y = []\n\nfor name, features in data.items():\n    for f in features[:num_train]:\n        train_x.append(f)\n        train_y.append(1 if name == 'a' else 0)\n\ntrain_x = np.array(train_x)\ntrain_y = np.array(train_y)\n\n# Split your data, and stratify according to the target label `train_y`\n# Set a random_state, so that the train-test split is reproducible\n\nx_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, stratify=train_y, random_state=123)\n\nmodel = Sequential()\nmodel.add(Dense(1, activation='sigmoid', input_dim=5))\nmodel.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.fit(x=x_train, y=y_train, \n          validation_data=(x_test, y_test), # Use this instead\n          class_weight={0:1,1:17},  # See explanation in 2. Imbalanced class\n          batch_size=10, epochs=500)\n\nclass_weight={0:1,1:17}\n\npreds = model.predict(x_test)\n\n[[0.33624142]\n [0.58196825]\n [0.5549609 ]\n [0.38138568]\n [0.45235538]\n [0.32419187]\n [0.37660158]\n [0.37013668]\n [0.5794893 ]\n [0.5611163 ]\n ......]\n\nthreshold_output = np.where(preds &gt; 0.5, 1, 0)\n\n[[0]\n [1]\n [1]\n [0]\n [0]\n [0]\n [0]\n [0]\n [1]\n [1]\n ...]\n\nfrom sklearn.metrics import accuracy_score\n\ntrain_preds = np.where(model.predict(x_train) &gt; 0.5, 1, 0)\ntest_preds = np.where(model.predict(x_test) &gt; 0.5, 1, 0)\n\ntrain_accuracy = accuracy_score(y_train, train_preds)\ntest_accuracy = accuracy_score(y_test, test_preds)\n\nprint(f'Train Accuracy : {train_accuracy:.4f}')\nprint(f'Test Accuracy  : {test_accuracy:.4f}')\n\nTrain Accuracy : 0.7443\nTest Accuracy  : 0.7073\n"
'{image:*a_part_of_your_image*, line:*line_number*, columns:*columns_number*, original_image:*original_image_id*}\n'
'from spafe.features.lfcc import lfcc\n'
'model = HalfFrozenModel()\nmodel.set_freeze(hid1=True)\n# Do some training.\nmodel.set_freeze(hid2=True)\n# Do some more training.\n# ...\n'
"model = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels = 5)\n"
'with tf.GradientTape() as tape:\n    loss_value = self.loss()\ntape.gradient(loss_value, vars)\n\ndef __enter__(self):\n    """Enters a context inside which operations are recorded on this tape."""\n    self._push_tape()\n    return self\n\ndef __exit__(self, typ, value, traceback):\n    """Exits the recording context, no further operations are traced."""\n    if self._recording:\n        self._pop_tape()\n\n# Initialize outer and inner tapes\nself.gt_outer = tf.GradientTape(persistent=True)\nself.gt_inner = tf.GradientTape(persistent=True)\n\n# Begin Recording\nself.gt_outer._push_tape()\nself.gt_inner._push_tape()\n\n# evaluate loss which uses self.variables\nloss_val = self.loss()\n\n# stop recording on inner tape\nself.gt_inner._pop_tape()\n\n# Evaluate the gradient on the inner tape\nself.gt_grad = self.gt_inner.gradient(loss_val, self.variables)\n\n# Stop recording on the outer tape\nself.gt_outer._pop_tape()\n\ndef hessian_v_prod(self, v):\n    self.gt_outer._push_tape()\n    v_hat = tf.reduce(tf.multiply(v, self.gt_grad))\n    self.gt_outer._pop_tape()\n    return self.gt_outer.gradient(v_hat, self.variables)\n\n# reset tapes\nself.gt_outer._tape = None\nself.gt_inner._tape = None\n'
'mputed_training=impyute.imputation.cs.em(X_train2.values, loops=50)\nX_train2[:]= mputed_training\n'
'def custom_dataset(dataset, req_cols):\n    in_, out_ = [], []\n    if isinstance(dataset, pd.DataFrame):  # optional\n        for col in req_cols:  # check for every existing column\n            if col in dataset.columns:\n                in_.append(col)  # append those that are in (i.e. valid)\n            else:\n                out_.append(col)  # append those that are NOT in (i.e. invalid)\n    return dataset[in_] if in_ else None, out_ if out_ else None\n'
'dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n'
'c_1_mean = c_A_array.mean(axis=0)\nc_2_mean = c_B_array.mean(axis=0)\nS1_c1 = np.cov(c_A_array.T)\nS2_c2 = np.cov(c_B_array.T)\nSw = S1_c1 + S2_c2 \n\nSb = (c_1_mean - c_2_mean) * (c_1_mean - c_2_mean).reshape([-1, 1])\n'
"vaeModel=Model(encoderInput,decoder)\nvaeModel.add_loss(vae_loss(encoderInput, decoder))\nvaeModel.compile(optimizer='adam') # No need to pass any loss function to compile method\n"
"test_labels = df_test['Sentiment'].values + 1\n\nimport tensorflow as tf\n\nsparse_entropy = tf.losses.SparseCategoricalCrossentropy()\n\na = tf.convert_to_tensor([[-1., 0., 1.]]) #+ 1\nb = tf.convert_to_tensor([[.4, .2, .4], [.1, .7, .2], [.8, .1, .1]])\n\nsparse_entropy(a, b)\n\nnan\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.1918503&gt;\n"
'train_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=1, num_workers=1)\n'
"# Random Forest Classification\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('finalplacementdata3.csv')\nX = dataset.iloc[:, range(1, 12)].values\ny = dataset.iloc[:, 12].values\n\nsiX = np.lexsort((X[:, 1], X[:, 0]))\nsX, sy = X[siX], y[siX]\n\n# Splitting the dataset into the Training set and Test set\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n                     \nriX = np.minimum(sX.shape[0] - 1, np.searchsorted(sX[:, 0], X1.ravel()))\nrX = X[riX]\n\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()] + list(rX[:, 2:].T)).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Training set)')\nplt.xlabel('Quants')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n\nriX = np.minimum(sX.shape[0] - 1, np.searchsorted(sX[:, 0], X1.ravel()))\nrX = X[riX]\n\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()] + list(rX[:, 2:].T)).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Random Forest Classification (Test set)')\nplt.xlabel('Quants')\nplt.ylabel('CGPA')\nplt.legend()\nplt.show()\n"
"class Problem:\n    def __init__(self, initial_state, goal):\n        self.goal = goal\n        self.record = [[0, initial_state, &quot;LEFT&quot;, []]]\n            # list of results [score][state][boat_side][listActions]\n\n    def actions(self, state, boat_side):\n        side = 0 if boat_side == 'LEFT' else 1\n        boat_dir = 'RIGHT' if boat_side == 'LEFT' else 'LEFT'\n\n        group = [i for i, v in enumerate(state) if v == side]\n        onboard_2 = [[boat_dir, a, b] for a in group for b in group if \n            a &lt; b and (     # not the same person and unique group \n            (a%2==0 and b - a == 1) or (        # wife and husband\n            a%2==0 and b%2==0) or (             # two wife's\n            a%2==1 and b%2==1)                  # two husbands\n        )]\n        onboard_1 = [[boat_dir, a] for a in group]\n        return onboard_1 + onboard_2\n\n    def result(self, state, action):\n        new_boat_side = action[0]\n        new_state = []\n        for i, v in enumerate(state):\n            if i in action[1:]:\n                new_state.append(1 if v == 0 else 0)\n            else:\n                new_state.append(v)\n\n        # check if invalid\n        for p, side, in enumerate(new_state):\n            if p%2 == 0: # is woman\n                if side != new_state[p+1]: # not with husband\n                    if any(men == side for men in new_state[1::2]):\n                        new_state = False\n                        break\n\n        return new_state, new_boat_side\n\n    def goal_test(self, state):\n        return state == self.goal\n\n    def value(self, state):\n        # how many people already crossed\n        return state.count(1)\n\n\n# optimization process\ninitial_state = [0]*6\ngoal = [1]*6\ntask = Problem(initial_state, goal)\n\nwhile True:\n    batch_result = []\n    for score, state, side, l_a in task.record:\n        possible_actions = task.actions(state, side)\n        for a in possible_actions:\n            new_state, new_boat_side = task.result(state, a)\n            if new_state: # is a valid state\n                batch_result.append([\n                    task.value(new_state),\n                    new_state,\n                    new_boat_side,\n                    l_a + a,\n                ])\n\n    batch_result.sort(key= lambda x: x[0], reverse= True)\n        # sort the results with the most people crossed\n    task.record = batch_result[:5]\n        # I am only sticking with the best 5 results but\n        #   any number should be fine on this problem\n    if task.goal_test(task.record[0][1]):\n        break\n\n#   for i in task.record[:5]: # uncomment these lines to see full progress\n#       print(i)\n#   x = input() # press any key to continue\n\nprint(task.record[0][3])\n"
"with open(data_json_path, 'r') as f:\n    data_json = json.load(f)\n\nenglish_as_list = [sample['text'] for sample in data_json]\narabic = [translate(sample) for sample in english_as_list]\n \n"
'from tf.keras.utils import to_categorical\n# define example\ndata = [1, 3, 2, 0, 3, 2, 2, 1, 0, 1]\ndata = array(data)\nprint(data)\n# one hot encode\nencoded = to_categorical(data)\n'
'eps = Lambda(lambda t: K.random_normal(stddev=1.0, shape=(K.shape(t)[0], latent_dim)))(z_log_var)\n'
'!pip install sklearn_features\n\nfrom sklearn_features.transformers import DataFrameSelector\n'
'def predict(model, sample):\n    res = model.predict(sample)\n    return res[res &lt; 0] = 0\n'
'class Document(object):\n\n    def __init__(self, index, label, bowdict):\n        self.index = index\n        self.label = label\n        self.bowdict = bowdict\n\n{ 9:3, 94:1, 109:1,  ... } \n\nfrom collections import defaultdict\n\ndef aggregate(docs, label):\n    bow = defaultdict(int)\n    for doc in docs:\n        if doc.label == label:\n           for (word, counter) in doc.bowdict.items():\n                bow[word] += counter  \n    return bow    \n'
"        for row in data:\n              if row[15] == ' &gt;50K':\n                    self.greaterThan_data.append(row)\n              else:\n                    self.lessThan_data.append(row)\n\n        self.greater_class_prob_dist = self.getCatProbs(self.greaterThan_data,2)\n"
"testset = word_vectorizer.transform(\n    codecs.open('/Users/user/Desktop/test.txt','r','utf8'))\n"
"from sklearn.feature_extraction import DictVectorizer\n\ndata  = [\n        {'location': 'store 1', 'quality': 8},\n        {'location': 'store 1', 'quality': 9},\n        {'location': 'store 2', 'quality': 2},\n        {'location': 'store 2', 'quality': 3},\n        ]\nprices = [100.00, 99.9, 11.25, 9.99]\nvec = DictVectorizer()\nX = vec.fit_transform(data)\ny = prices\n\n╔═════════════════╦═════════════════╦═════════╗\n║ location=store1 ║ location=store2 ║ quality ║\n╠═════════════════╬═════════════════╬═════════╣\n║               1 ║               0 ║       8 ║\n║               1 ║               0 ║       9 ║\n║               0 ║               1 ║       2 ║\n║               0 ║               1 ║       3 ║\n╚═════════════════╩═════════════════╩═════════╝\n\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n&gt;&gt;&gt; test_data = [{'location': 'store 2', 'quality': 3}]\n&gt;&gt;&gt; X_test = vec.transform(test_data)\n&gt;&gt;&gt; model.predict(X_test)\narray([ 10.28])\n"
'l2.reshape((l2.shape[0],))\n\nIn [1]: import numpy as np\n\nIn [2]: l1 = np.array([1,2,3,4])\n\nIn [3]: l2 = np.array([[5],[6],[7],[8]])\n\nIn [7]: l2.shape\nOut[7]: (4, 1)\n\nIn [8]: l2-l1\nOut[8]:\narray([[4, 3, 2, 1],                 #Just to show that you get the behaviour when arrays are in \n       [5, 4, 3, 2],                 #different dimensions.\n       [6, 5, 4, 3],\n       [7, 6, 5, 4]])\n\nIn [19]: l2 = l2.reshape((l2.shape[0],))\n\nIn [25]: l2 = l2.reshape((l2.shape[0],))\n\nIn [26]: l2-l1\nOut[26]: array([4, 4, 4, 4])\n'
'theta = fmin_bfgs(cost_function_reg, initial_theta)\n'
'# Normalise input data between 0 and 1.\nX -= X.min()\nX /= X.max()\n\nfrom sklearn import preprocessing\nX = preprocessing.scale(X)\n'
"weights = {\n    'wc1': tf.Variable(tf.random_normal([7, 7, 3, 96])), # ...\n    # ...\n}\n\n_X = tf.image.rgb_to_grayscale(_X)\n"
'X = 0 0 3\n    1 1 0\n    0 2 1\n    1 0 2\n'
'w[1:]:  [-0.68  1.82]\n\nX: [ 3.3   0.  ]\n'
"cross_val_score(classifier, X, y, cv=cv, scoring='roc_auc')\n"
'e = np.sum((np.square(function(x, theta[0], theta[1])) - y))\n\ne = np.sum((np.square(function(x, theta[0], theta[1]) - y)))\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# constants my gradient descent model should find:\na = 4.0\nb = 4.0\ny0 = 800.0\n\n# function to fit on!\ndef function(x, a, b):\n    # y0 = 800\n    return y0 - a * np.log(b + x)\n\n# Generates data\ndef gen_data(numpoints):\n    # a = 4\n    # b = 4\n    x = np.array(range(0, numpoints))\n    y = function(x, a, b)\n    return x, y\nx, y = gen_data(600)\n\ndef grad_model(x, y, iterations):\n    converged = False\n\n    # length of dataset\n    m = len(x)\n\n    # guess   a ,  b\n    theta = [0.1, 0.1]\n    alpha = 0.00001\n\n    # initial error\n    # e = np.sum((np.square(function(x, theta[0], theta[1])) - y))    #  This was a bug\n    e = np.sum((np.square(function(x, theta[0], theta[1]) - y)))\n\n    costs = np.zeros(iterations)\n\n    for iteration in range(iterations):\n        hypothesis = function(x, theta[0], theta[1])\n        loss = hypothesis - y\n\n        # compute partial deritaves to find slope to "fall" into\n        # theta0_grad = (np.mean(np.sum(-np.log(x + y)))) / (m)\n        # theta1_grad = (np.mean((((np.log(theta[1] + x)) / theta[0]) - (x*(np.log(theta[1] + x)) / theta[0])))) / (2*m)\n        theta0_grad = 2*np.sum((y0 - theta[0]*np.log(theta[1] + x) - y)*(-np.log(theta[1] + x)))\n        theta1_grad = 2*np.sum((y0 - theta[0]*np.log(theta[1] + x) - y)*(-theta[0]/(b + x)))\n\n        theta0 = theta[0] - (alpha * theta0_grad)\n        theta1 = theta[1] - (alpha * theta1_grad)\n\n        theta[1] = theta1\n        theta[0] = theta0\n\n        # new_e = np.sum(np.square((function(x, theta[0], theta[1])) - y)) # This was a bug\n        new_e = np.sum(np.square((function(x, theta[0], theta[1]) - y)))\n        costs[iteration] = new_e\n        if new_e &gt; e:\n            print "AHHHH!"\n            print "Iteration: "+ str(iteration)\n            # break\n        print theta\n    return theta[0], theta[1], costs\n\n(theta0,theta1,costs) = grad_model(x,y,100000)\nplt.semilogy(costs)\n'
"x = tf.placeholder(tf.float32, shape=(batch_size, image_width, image_height, image_depth), name='x')\n\nx = batch_normalization(x, training=phase)\n\nx = batch_normalization(x, training=phase)\n\nx_bn = batch_normalization(x, training=phase)\n"
'Returns the coefficient of determination R^2 of the prediction.\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares\n((y_true - y_pred) ** 2).sum() and v is the residual sum of squares\n((y_true - y_true.mean()) ** 2).sum().\n'
'R^d -&gt; [0,1]\n'
'WARNING: This op expects unscaled logits, since it performs a softmax on logits internally \nfor efficiency. Do not call this op with the output of softmax, as it will produce \nincorrect results.\n'
'tf.reduce_sum(tf.square(y - y_data))\n'
"sentences = [doc2vec.TaggedDocument(sentence, 'tag') for sentence in titlelist]\nmodel.build_vocab(sentences)\n"
'cross_entropy = tf.reduce_mean(\n    tf.nn.l2_loss(y_ - y2)\n)\n'
' model.add(LSTM({{choice([32,64,128,256,512])}},W_regularizer=l2({{uniform(0, 1)}})))\n\nmodel.add(LSTM(units={{choice([32,64,128,256,512])}},...)\n\nmodel.add(Dropout(rate=..))\n'
'image\nboxes\nclasses\nscores\ncategory_index\nuse_normalized_coordinates\nline_thickness\n\nmin_score_tresh=.1\n'
'history = model.fit(signal.reshape(1,400,1), out.reshape(1,3))\n'
'x_batch = x_batch.reshape((-1, 218 * 178 * 3))\n'
'count_1 count_2 count_3 count_4 count_5 count_6 fair_dice\n5       6       4       7       6       5       1\n3       2       1       2       1       13      0     \n'
'perl -E \'for($i=0;$i&lt;30000000;$i++){say "Line $i,field2,field3,",int rand 100}\' &gt; BigBoy.csv\n\nLine 0,field2,field3,49\nLine 1,field2,field3,6\nLine 2,field2,field3,15\n...\nLine 29999998,field2,field3,79\nLine 29999999,field2,field3,19\n\nawk \'rand()&gt;0.99\' BigBoy.csv | gshuf &gt; RandomSet.csv\n\nLine 15348259,field2,field3,95\nLine 1642442,field2,field3,93\nLine 29199452,field2,field3,52\n\nbrew install coreutils\n'
'from sklearn.metrics import accuracy_score\n\npred_prob_P_test = qst.predict(P_test)\naccuracy_score(q_test, pred_prob_P_test)\n\npred_prob_P_test = qst.predict_proba(P_test)\npreds = np.argmax(pred_prob_P_test, axis=1)\naccuracy_score(q_test, preds)\n'
'class CustomLabelBinarizer(BaseEstimator, TransformerMixin):\n    def __init__(self, sparse_output=False):\n      self.sparse_output = sparse_output\n    def fit(self, X, y=None):\n      self.enc = LabelBinarizer(sparse_output=self.sparse_output)\n      self.enc.fit(X)\n      return self\n    def transform(self, X, y=None):\n      return self.enc.transform(X)\n\n(5, 14)\n(1000, 14)\n(10000, 14)\n'
'learning_rate = tf.placeholder(tf.float32, shape=[])\n# ...\ntrain_step = tf.train.GradientDescentOptimizer(\n    learning_rate=learning_rate).minimize(mse)\n\nsess = tf.Session()\n\n# Feed different values for learning rate to each training step.\nsess.run(train_step, feed_dict={learning_rate: 0.1})\nsess.run(train_step, feed_dict={learning_rate: 0.1})\nsess.run(train_step, feed_dict={learning_rate: 0.01})\nsess.run(train_step, feed_dict={learning_rate: 0.01})\n'
"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('fivethirtyeight')\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Normalize the input data\nA = np.array([[10,8],[1,2],[7,5],[3,5],[7,6],[8,7],[9,9],[4,5],[6,5],[6,8],\n             [1,9],[10,2],[6,3],[2,5],[1,14],[8,8],[9,5],[4,4],[5,6],[8,8],\n             [11,9],[10,12],[6,4],[5,2],[10,2],[8,3],[6,9],[0,4],[13,6],[9,6]])\n\nA = StandardScaler(with_std=False,copy=False).fit_transform(A)\n\nfig = plt.figure(figsize=(10,10))\nax0 = fig.add_subplot(111)\nax0.set_aspect('equal')\nax0.set_xlim((-10,10))\nax0.set_ylim((-10,10))\n\nax0.scatter(A[:,0],A[:,1])\n\n\n# Run through all the data\n\nfor i in range(len(A[:,0])):\n\n    # v\n    v = np.array([3,2])\n    ax0.plot(np.linspace(-10,10),np.linspace(-10,10)*(v[1]/v[0]),color='black',linestyle='--',linewidth=1.5)   \n\n    # w\n    w = np.array([A[i][0],A[i][1]])\n    #ax0.arrow(0,0,w[0],w[1],length_includes_head=True,width=0.01,color='green')\n\n    # cv\n    cv = (np.dot(w,v))/np.dot(v,np.transpose(v))*v\n    #ax0.arrow(0,0,cv[0],cv[1],length_includes_head=True,width=0.005,color='black')\n    print(cv)\n\n    # line between w and cv\n    ax0.plot([w[0],cv[0]],[w[1],cv[1]],'r--',linewidth=1.5)\n\n\n    # Check the result\n    print(np.dot((w-cv),cv))\n\nplt.show()\n"
"import pandas as pd\nimport numpy as np\n\ndf=pd.read_csv('test.csv')\n\nX=df.drop(['label'],1)\ny=np.array(df['label'])\n\ndata = pd.get_dummies(X)\n\nonehotencoder = OneHotEncoder(categorical_features=[1])\ndata = onehotencoder.fit_transform(data).toarray()\n"
'from sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nvariables = pandas.read_csv("/Users/srikanth/Desktop/sample1.csv")\nprint(variables)\nx1 = variables[[\'X\']]\nx2 = variables[[\'Y\']]\nplt.plot()\nplt.xlim([150, 190])\nplt.ylim([40, 90])\nplt.title(\'Dataset\')\nplt.xlabel(\'X - Values\')\nplt.ylabel(\'Y - Values\')\nplt.scatter(x1, x2)\nplt.show()\n'
'mnist = torchvision.datasets.mnist.MNIST("/")\nlabels = mnist.train_labels\nfives = (labels == 5).nonzero()\n\n'
'model = nn.Linear(2, 1)  # you have 2 inputs now\nX_input = torch.cat((X, X**2), dim=1)  # have 2 inputs per entry\n# ...\n\n    predictions = model(X_input)  # 2 inputs -&gt; 1 output\n    loss = loss_fn(predictions, t)\n    # ...\n    # learning t = c*x^2 + a*x + b\n    print("learned a = {}".format(list(model.parameters())[0].data[0, 0]))\n    print("learned c = {}".format(list(model.parameters())[0].data[0, 1])) \n    print("learned b = {}".format(list(model.parameters())[1].data[0])) \n'
'import pandas as pd\ndf = pd.DataFrame({\'starttime\': pd.date_range(\'2018-01-01\', freq=\'15min\', periods=20)})\n\ndf[\'one\'] = 1\nprint(df.head())\n#            starttime  one\n#0 2018-01-01 00:00:00    1\n#1 2018-01-01 00:15:00    1\n#2 2018-01-01 00:30:00    1\n#3 2018-01-01 00:45:00    1\n#4 2018-01-01 01:00:00    1\n\ndf[\'starttime\'] = pd.to_datetime(df.starttime)\ndf_starttime = df.set_index("starttime")\ndf_resampled = df_starttime.resample("3h").sum().fillna(0)\n\nprint(df_resampled)\n#                     one\n#starttime               \n#2018-01-01 00:00:00   12\n#2018-01-01 03:00:00    8\n\ndef load_citibike():\n    df = pd.read_csv(\'citibike.csv\')\n    df[\'starttime\'] = pd.to_datetime(df.starttime)\n    return df.set_index(\'starttime\').resample(\'3H\').size().fillna(0)\n'
"train_datagen = ImageDataGenerator(rescale=1. / 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_width, img_height),\n    color_mode='grayscale',\n    shuffle = True,\n    batch_size=batch_size,\n    class_mode='binary')\n\nprint(train_generator.class_indices)\n{'cat': 0, 'dog': 1}\n"
"train_images = train_images.astype('float32') / 255.0\ntest_images = test_images.astype('float32') / 255.0\n"
'svm = cv2.ml.SVM_create()\nsvm.getTermCriteria()\n\nsvm.setTermCriteria((cv2.TermCriteria_MAX_ITER, 10000, 0))\n\nsvm.setTermCriteria((cv2.TermCriteria_MAX_ITER + cv2.TermCriteria_EPS, 10000, 1.1920928955078125e-07))\n'
"df.rename(columns = lambda x: x.replace(' ', '_'), inplace=True)\n"
'def download_from_s3(url):\n    """ex: url = s3://sagemakerbucketname/data/validation.tfrecords"""\n    url_parts = url.split("/")  # =&gt; [\'s3:\', \'\', \'sagemakerbucketname\', \'data\', ...\n    bucket_name = url_parts[2]\n    key = os.path.join(*url_parts[3:])\n    filename = url_parts[-1]\n    if not os.path.exists(filename):\n        try:\n            # Create an S3 client\n            s3 = boto3.resource(\'s3\')\n            print(\'Downloading {} to {}\'.format(url, filename))\n            s3.Bucket(bucket_name).download_file(key, filename)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\'Error\'][\'Code\'] == "404":\n                print(\'The object {} does not exist in bucket {}\'.format(\n                    key, bucket_name))\n            else:\n                raise\n'
"import pickle\n\ntfidf_vectorizer = TfidfVectorizer()\ntrain_data = tfidf_vectorizer.fit_transform(train_corpus) # fit on train\n\n# You could just save the vectorizer with pickle\npickle.dump(tfidf_vectorizer, open('tfidf_vectorizer.pkl', 'wb'))\n\n# then later load the vectorizer and transform on test-dataset.\ntfidf_vectorizer = pickle.load(open('tfidf_vectorizer.pkl', 'rb'))\ntest_data = tfidf_vectorizer.transform(test_corpus)\n"
"from sklearn import preprocessing\nle=preprocessing.LabelEncoder()\nle.fit_transform(['a','b','a'])\n\noutput: array([0, 1, 0])\n"
"dt.groupby('month')['number'].sum()\n"
"# Saving a model\njoblib.dump(ml_model, 'path/to/saved_model.pkl')\n\n# Loading a model\nml_model = joblib.load('path/to/saved_model.pkl')\n"
"most_working_borough = london_working_hours.sort_values(by='Working Hours%', ascending=False).Boroughs.tolist()[0]\npopulation = asian_london_table.loc[asian_london_table['Area'] == most_working_borough].Area.tolist()[0]\nearning = london_table_earning.loc[london_table_earning['Area'] == most_working_borough].Area.tolist()[0]\n"
'import xgboost\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import SelectFromModel\n\nseed=42\n\ngbr = GradientBoostingRegressor(random_state=seed)\ngbr_params = {\n    "learning_rate": [0.001, 0.01, 0.1],\n    "min_samples_split": [50, 100],\n    "min_samples_leaf": [50, 100],\n    "max_depth":[5, 10, 20]}\n\nxgbr = xgboost.XGBRegressor(random_state=seed) \nxgbr_params = {  \n    "learning_rate": [0.001, 0.01, 0.1],\n    "min_samples_leaf": [50, 100],\n    "max_depth":[5, 10, 20],\n    \'reg_alpha\': [1.1, 1.2, 1.3],\n    \'reg_lambda\': [1.1, 1.2, 1.3]}\n\nrfr = RandomForestRegressor(random_state=seed)\nrfr_params={\'n_estimators\':[100, 500, 1000], \n             \'max_features\':[10,14,18],\n             \'min_samples_split\': [50, 100],\n             \'min_samples_leaf\': [50, 100],} \n\nfs_xgbr = RandomizedSearchCV(xgbr, xgbr_params, cv=5, iid=False, n_jobs=-1)\nfs_gbr = RandomizedSearchCV(gbr, gbr_params, cv=5,iid=False, n_jobs=-1)\nfs_rfr = RandomizedSearchCV(rfr, rfr_params, cv=5,iid=False, n_jobs=-1)\n\nX, y = make_regression(1000,10)\n\nfs_xgbr.fit(X, y)\nfs_gbr.fit(X, y)\nfs_rfr.fit(X, y)\n\nmodel = SelectFromModel(fs_rfr.best_estimator_, prefit=True)\nX_rfr = model.transform(X)\nprint(\'rfr\', X_rfr.shape)\n\nmodel = SelectFromModel(fs_xgbr.best_estimator_, prefit=True)\nX_xgbr = model.transform(X)\nprint(\'xgbr\', X_xgbr.shape)\n\nmodel = SelectFromModel(fs_gbr.best_estimator_, prefit=True)\nX_gbr = model.transform(X)\nprint(\'gbr\', X_gbr.shape)\n\nrfr (1000, 3)\nxgbr (1000, 3)\ngbr (1000, 4)\n'
"history10.history['mean_squared_error'][-1]\n\nhistory10.history['mean_squared_error']\n"
'images = x_train[0:1000].reshape(1000, 28*28)\nimages = images / 255\nlabels = y_train[0:1000]\n\ntest_images = x_test[0:1000].reshape(1000, 28*28)\ntest_images = test_images / 255\ntest_labels = y_test[0:1000]\n'
"model = Sequential()\nmodel.add(Embedding(max_words, 30, input_length=max_len))\nmodel.add(BatchNormalization())\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(BatchNormalization())\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n"
'output = input.matmul(weight.t())\n'
"outputs = keras.Dense(32,activation='softmax')(hidden_2)\n"
'from sklearn.preprocessing import LabelEncoder\n\nl = [1,4,6,1,4]\nle = LabelEncoder()\nle.fit(l)\nle.transform(l)\n# array([0, 1, 2, 0, 1], dtype=int64)\nle.transform([1,6,4])\n# array([0, 2, 1], dtype=int64)\n\npd.factorize(l)[0]\n# array([0, 1, 2, 0, 1], dtype=int64)\npd.factorize([1,6,4])[0]\n# array([0, 1, 2], dtype=int64)\n'
"tf.Tensor(b'xf wl dy fp ke dj ye xp fs', shape=(), dtype=string)\ntf.Tensor(b'ek xn ir yd jp pz cw', shape=(), dtype=string)\ntf.Tensor(b'gu iz hp jl uf', shape=(), dtype=string)\ntf.Tensor(b'nu kc ai zo du qo fu bj nn', shape=(), dtype=string)\ntf.Tensor(b'xw zo az mn vf nu', shape=(), dtype=string)\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nfrom string import ascii_lowercase as letters\n\n\ninputs = [' '.join([''.join(np.random.choice(list(letters), 2))\n                    for i in range(np.random.randint(5, 10))])\n          for ii in range(100)]\n\noutputs = np.random.randint(0, 2, 100).astype(str)\n\ndf = pd.DataFrame(zip(inputs, outputs), columns=['text', 'string'])\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        tf.cast(df.text.values, tf.string)))\n\nfor ex in training_dataset.take(5):\n    print(ex)\n\n\ntokenizer = tfds.features.text.Tokenizer()\n\nlowercase = True\nvocabulary = Counter()\nfor text in training_dataset:\n    if lowercase:\n        text = tf.strings.lower(text)\n    tokens = tokenizer.tokenize(text.numpy())\n    vocabulary.update(tokens)\n\nvocab_size = 128\nvocabulary, _ = zip(*vocabulary.most_common(vocab_size))\n\nmax_len = 50\nmax_sent = 5\n\nencoder = tfds.features.text.TokenTextEncoder(vocabulary,\n                                              lowercase=True,\n                                              tokenizer=tokenizer)\n\ndef encode(text):\n    sent_list = []\n    sents = tf.strings.split(text, sep=&quot;. &quot;).numpy()\n    sents = sents[:max_sent]\n    for sent in sents:\n        text_encoded = encoder.encode(sent.decode())\n        if max_len:\n            text_encoded = text_encoded[:max_len]\n            sent_list.append(text_encoded)\n    encoded_text = tf.stack(sent_list)\n\n    return encoded_text\n\n\ndef encode_pyfn(text):\n    [text_encoded] = tf.py_function(encode,\n                                  inp=[text],\n                                  Tout=[tf.int32])\n    return text_encoded\n\ntraining_dataset = training_dataset.map(encode_pyfn).\\\n    padded_batch(batch_size=3, padded_shapes=([1, max_len]))\n\nnext(iter(training_dataset))\n\n&lt;tf.Tensor: shape=(3, 1, 50), dtype=int32, numpy=\narray([[[129,   1,  14, 129,  56,  15,  57, 129, 129,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0]],\n       [[129,  16, 129,  58,  59,  60, 129, 129,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0]],\n       [[129,  61, 129,  17, 129, 129, 129,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n           0,   0]]])&gt;\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom collections import Counter\n\n\ntext = [&quot;This game is a bit hard to get the hang of, but when you do it's great.&quot;,\n        &quot;I played it a while but it was alright. The steam was a bit of trouble.&quot;\n        &quot; The more they move these game to steam the more of a hard time I have&quot;\n        &quot; activating and playing a game. But in spite of that it was fun, I &quot;\n        &quot;liked it. Now I am looking forward to anno 2205 I really want to &quot;\n        &quot;play my way to the moon.&quot;]\n\n\ndf = pd.DataFrame({&quot;text&quot;: text})\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        tf.cast(df.text.values, tf.string)))\n\nfor ex in training_dataset.take(5):\n    print(ex)\n\ntokenizer = tfds.features.text.Tokenizer()\n\nlowercase = True\nvocabulary = Counter()\nfor text in training_dataset:\n    if lowercase:\n        text = tf.strings.lower(text)\n    tokens = tokenizer.tokenize(text.numpy())\n    vocabulary.update(tokens)\n\n\nvocab_size = 5000\nvocabulary, _ = zip(*vocabulary.most_common(vocab_size))\n\nmax_len = 15\nmax_sent = 5\nencoder = tfds.features.text.TokenTextEncoder(vocabulary,\n                                              lowercase=True,\n                                              tokenizer=tokenizer)\n\ndef encode(text):\n    sent_list = []\n    sents = tf.strings.split(text, sep=&quot;. &quot;).numpy()\n    if max_sent:\n        sents = sents[:max_sent]\n    for sent in sents:\n        text_encoded = encoder.encode(sent.decode())\n        if max_len:\n            text_encoded = text_encoded[:max_len]\n            sent_list.append(text_encoded)\n    encoded_text = tf.concat(sent_list, axis=0)\n\n    return encoded_text\n\n\ndef encode_pyfn(text):\n    [text_encoded] = tf.py_function(encode, inp=[text], Tout=[tf.int32])\n    return text_encoded\n\n\ntraining_dataset = training_dataset.map(encode_pyfn).\\\n    padded_batch(batch_size=4, padded_shapes=([max_len*max_sent,]))\n\nnext(iter(training_dataset))\n\n&lt;tf.Tensor: shape=(2, 75), dtype=int32, numpy=\narray([[14,  7, 15,  1, 10, 11,  2, 16,  3, 17,  6,  8, 18, 19, 20,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n       [ 5, 23,  4,  1, 24,  8,  4,  9, 25,  3, 12,  9,  1, 10,  6, 26,\n         3, 13, 27, 28, 29,  7,  2, 12,  3, 13,  6,  1, 11, 30,  5,  8,\n        35, 36,  6, 37,  4,  9, 38,  5, 39,  4, 40,  5, 41, 42, 43,  2,\n        44, 45,  5, 46, 47,  2, 48, 49, 50,  0,  0,  0,  0,  0,  0,  0,\n         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])&gt;\n\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\ntext = [&quot;I played it a while but it was alright. The steam was a bit of trouble.&quot;\n        &quot; The more they move these game to steam the more of a hard time I have&quot;\n        &quot; activating and playing a game. But in spite of that it was fun, I &quot;\n        &quot;liked it. Now I am looking forward to anno 2205 I really want to &quot;\n        &quot;play my way to the moon.&quot;,\n        &quot;This game is a bit hard to get the hang of, but when you do it's great.&quot;]\n\n\ndf = pd.DataFrame({&quot;text&quot;: text})\n\ntraining_dataset = (\n    tf.data.Dataset.from_tensor_slices(\n        tf.cast(df.text.values, tf.string)))\n\nfor ex in training_dataset.take(5):\n    print(ex)\n\ntokenizer = tfds.features.text.Tokenizer()\n\nlowercase = True\nvocabulary = Counter()\nfor text in training_dataset:\n    if lowercase:\n        text = tf.strings.lower(text)\n    tokens = tokenizer.tokenize(text.numpy())\n    vocabulary.update(tokens)\n\n\nvocab_size = 5000\nvocabulary, _ = zip(*vocabulary.most_common(vocab_size))\n\nmax_len = 15\nmax_sent = 5\nencoder = tfds.features.text.TokenTextEncoder(vocabulary,\n                                              lowercase=True,\n                                              tokenizer=tokenizer)\n\ndef encode(text):\n    sent_list = []\n    sents = tf.strings.split(text, sep=&quot;. &quot;).numpy()\n    if max_sent:\n        sents = sents[:max_sent]\n    for sent in sents:\n        text_encoded = encoder.encode(sent.decode())\n        if max_len:\n            text_encoded = text_encoded[:max_len]\n            sent_list.append(pad_sequences([text_encoded], max_len))\n    if len(sent_list) &lt; 5:\n        sent_list.append([tf.zeros(max_len) for _ in range(5 - len(sent_list))])\n    return tf.concat(sent_list, axis=0)\n\n\ndef encode_pyfn(text):\n    [text_encoded] = tf.py_function(encode, inp=[text], Tout=[tf.int32])\n    return text_encoded\n\n\ntraining_dataset = training_dataset.map(encode_pyfn).batch(batch_size=4)\n\nnext(iter(training_dataset))\n\n&lt;tf.Tensor: shape=(2, 5, 15), dtype=int32, numpy=\narray([[[14,  7, 15,  1, 10, 11,  2, 16,  3, 17,  6,  8, 18, 19, 20],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n\n       [[ 0,  0,  0,  0,  0,  0,  5, 23,  4,  1, 24,  8,  4,  9, 25],\n        [ 0,  0,  0,  0,  0,  0,  0,  0,  3, 12,  9,  1, 10,  6, 26],\n        [ 3, 13, 27, 28, 29,  7,  2, 12,  3, 13,  6,  1, 11, 30,  5],\n        [ 0,  0,  0,  0,  8, 35, 36,  6, 37,  4,  9, 38,  5, 39,  4],\n        [40,  5, 41, 42, 43,  2, 44, 45,  5, 46, 47,  2, 48, 49, 50]]])&gt;\n"
"scaler = ColumnTransformer([('scaler (or any name)', \n                              StandardScaler(),\n                             ['distance', 'angle']\n                            )], remainder='passthrough')\n"
"import os\nos.chdir(r'catsanddogs')\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\nfrom collections import deque\nfrom glob2 import glob\nimport numpy as np\n\nfiles = glob('*\\\\*\\\\*.jpg')\nfiles = files[:-(len(files)%3)] # dataset is now divisible by 3 \n\nindices = np.random.permutation(len(files)).reshape(3, -1)\n\nimsize = 64\n\n\ndef load(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, size=(imsize, imsize))\n    label = tf.strings.split(file_path, os.sep)[1]\n    label = tf.cast(tf.equal(label, 'dogs'), tf.int32)\n    return img, label\n\naccuracies_on_test_set = {}\n\nfor i in range(len(indices)):\n    d = deque(np.array(files)[indices].tolist())\n    d.rotate(-i)\n    train1, train2, test1 = d\n    train_ds = tf.data.Dataset.from_tensor_slices(train1 + train2).\\\n        shuffle(len(train1) + len(train2)).map(load).batch(4)\n    test_ds = tf.data.Dataset.from_tensor_slices(test1).\\\n        shuffle(len(test1)).map(load).batch(4)\n\n    classifier = Sequential()\n    classifier.add(Conv2D(8, (3, 3), input_shape=(imsize, imsize, 3), activation='relu'))\n    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n    classifier.add(Flatten())\n    classifier.add(Dense(units=32, activation='relu'))\n    classifier.add(Dropout(0.5))\n    classifier.add(Dense(units=1, activation='sigmoid'))\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n    classifier.fit(train_ds, validation_data=test_ds, epochs=2, verbose=0)\n    loss, accuracy = classifier.evaluate(test_ds, verbose=0)\n    accuracies_on_test_set[f'epoch_{i + 1}_accuracy'] = accuracy\n\nprint(accuracies_on_test_set)\n\n{'epoch_1_accuracy': 0.8235, 'epoch_2_accuracy': 0.7765, 'epoch_3_accuracy': 0.736}\n\nfrom collections import deque\n\ngroups = ['group1', 'group2', 'group3']\n\nfor i in range(3):\n    d = deque(groups)\n    d.rotate(-i)\n    print(list(d))\n\n['group1', 'group2', 'group3']\n['group2', 'group3', 'group1']\n['group3', 'group1', 'group2']\n\ntrain1, train2, test1 = d\n"
'def preprocess(*fields):\n    return tf.stack(fields[:-1]), tf.stack(fields[-1:]) # x, y\n\nvalidate_ds = dataset.map(preprocess).take(1000).batch(32).repeat()\ntrain_ds = dataset.map(preprocess).skip(1000).take(1000).batch(32).repeat()\n'
"import pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nvalues = [{'gender':'Female'} if i%2==0 else {'gender':'Male'} for i in range(100)]\n\nX = pd.DataFrame(values)\ny = [0 if i%2==0 else 1 for i in range(100)]\n\ndef binary_data(df):\n    df.gender = df.gender.map({'Female': 0, 'Male': 1})\n    print(df.shape)\n    return df\n\ncolumntransf = ColumnTransformer([('binarydata', FunctionTransformer(binary_data), ['gender'])])\nmodel_pipeline = Pipeline([\n    ('preprocessing', columntransf),\n    ('classifier', LogisticRegression(solver='lbfgs'))\n])\nparam_grid = {}\nsearch = GridSearchCV(model_pipeline, param_grid, scoring='accuracy')\nsearch.fit(X, y) \n\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(80, 1)\n(20, 1)\n(100, 1)\n\nsearch.best_estimator_\n"
"clf = SVC(kernel='sigmoid',gamma='auto') \n#normalizing features\nsepal_length_norm = normalize(sepal_length.reshape(1, -1))[0] \nsepal_width_norm = normalize(sepal_width.reshape(1, -1))[0] \n\nclf.fit(np.c_[sepal_length_norm, sepal_width_norm], iris.target)\n\npr_norm=clf.predict(np.c_[sepal_length_norm, sepal_width_norm])\nprint(pd.DataFrame(classification_report(iris.target, pr_norm, output_dict=True)))\n\n#                    0          1          2  accuracy   macro avg  weighted avg\n# precision   0.907407   0.650000   0.750000  0.766667    0.769136      0.769136\n# recall      0.980000   0.780000   0.540000  0.766667    0.766667      0.766667\n# f1-score    0.942308   0.709091   0.627907  0.766667    0.759769      0.759769\n# support    50.000000  50.000000  50.000000  0.766667  150.000000    150.000000\n\nclf = SVC(kernel='sigmoid') \n#normalizing features\nsepal_length_norm = normalize(sepal_length.reshape(1, -1))[0] \nsepal_width_norm = normalize(sepal_width.reshape(1, -1))[0] \n\nclf.fit(np.c_[sepal_length_norm, sepal_width_norm], iris.target) \n\nX = np.c_[sepal_length_norm, sepal_width_norm]\n\npr_norm=clf.predict(np.c_[sepal_length_norm, sepal_width_norm])\nprint(pd.DataFrame(classification_report(iris.target, pr_norm, output_dict=True)))\n\n#               0     1          2  accuracy   macro avg  weighted avg\n# precision   0.0   0.0   0.333333  0.333333    0.111111      0.111111\n# recall      0.0   0.0   1.000000  0.333333    0.333333      0.333333\n# f1-score    0.0   0.0   0.500000  0.333333    0.166667      0.166667\n# support    50.0  50.0  50.000000  0.333333  150.000000    150.000000\n\n"
'input = keras.layers.Input(shape=(28,28))\nflatten = keras.layers.Flatten()(input)\nhidden1 = keras.layers.Dense(128, activation=&quot;relu&quot;)(flatten)\nhidden2 = keras.layers.Dense(128, activation=&quot;relu&quot;)(hidden1)\nhidden3 = keras.layers.Dense(28, activation=&quot;relu&quot;)(hidden2)\noutput = keras.layers.Dense(10, activation=&quot;softmax&quot;)(hidden3)\nmodel = keras.models.Model(inputs=[input], outputs=[output])\n'
'1. / (1 + exp(-decision_function(X)))\n'
'from sklearn.externals import joblib\nhelp(joblib.parallel)\n\nimport numpy\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\n\nn = 1000\n\nX, y = make_blobs(n_samples=n)\nX_train, y_train = X[0:n // 2], y[0:n // 2]\nX_valid, y_valid = X[n // 2:], y[n // 2:]\n\nclf = RandomForestClassifier(n_estimators=350, verbose=100)\nclf.fit(X_train, y_train)\n\nbuilding tree 1 of 350\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\nbuilding tree 2 of 350\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\nbuilding tree 3 of 350\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\nbuilding tree 4 of 350\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\nbuilding tree 5 of 350\n\n[...]\n\nbuilding tree 100 of 350\nbuilding tree 101 of 350\nbuilding tree 102 of 350\nbuilding tree 103 of 350\nbuilding tree 104 of 350\n[...]\nbuilding tree 350 of 350\n[Parallel(n_jobs=1)]: Done 350 out of 350 | elapsed:    1.6s finished\n'
'import numpy as np\nfeatures=np.array([[3, 2, 1, 3, 1],\n       [2, 0, 1, 2, 2],\n       [1, 3, 2, 1, 3],\n       [1, 1, 3, 2, 3],\n       [1, 1, 2, 1, 3]])\nfrom sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=2)\nsvd = TruncatedSVD(n_components=2)\npart_1 = svd.fit_transform(features[0:2, :])\npart_2 = svd.fit_transform(features[2:, :])\nsvd_features = np.concatenate((part_1, part_2), axis=0)\nsvd_b = TruncatedSVD(n_components=2)\nsvd_features_b = svd_b.fit_transform(features)\nprint(svd_features)\nprint(svd_features_b)\n\n[[ 4.81379561 -0.90959982]\n [ 3.36212985  1.30233746]\n [ 4.70088886  1.37354278]\n [ 4.76960857 -1.06524658]\n [ 3.94551566 -0.34876626]]\n\n\n[[ 4.17420185  2.47515867]\n [ 3.23525763  0.9479915 ]\n [ 4.53499272 -1.13912762]\n [ 4.69967028 -0.89231578]\n [ 3.81909069 -1.05765576]]\n'
'from collections import OrderedDict\n\nmalware = []\n\nmalware.append(OrderedDict.fromkeys(dataset.index[clf.predict(dataset) == 0]))\n\nprint (malware)\n'
"lr = sklearn.linear_model.LinearRegression()\nlr.fit(df[['HL_PCT','PCT_change','Adj. Volume']], df[forecast_col])\n"
'import os\nos.environ[\'KERAS_BACKEND\'] = "theano" #or "tensorflow"\nimport keras\n'
'1488 = (1500 // batch_size) * batch_size\n496 = (500 // batch_size) * batch_size\n'
'w = np.array([0,0,0])\nrestart = True\nwhile restart:  \n    ii = 0\n    restart = False\n    for x,y in pts10:\n        if(restart == False):\n            ii += 1\n\n    r = np.array([x,y,1])    \n    if (np.dot(w,r) &gt;= 0) and int(label_dict[\'point{}\'.format(ii)][1]) &gt;= 0:\n        print "Point " + str(ii) + " is correctly above the line --&gt; no adjustments"      \n    elif (np.dot(w,r) &lt; 0) and int(label_dict[\'point{}\'.format(ii)][1]) &lt; 0:\n        print "Point " + str(ii) + " is correctly below the line --&gt; no adjustments"        \n    elif (np.dot(w,r) &gt;= 0) and int(label_dict[\'point{}\'.format(ii)][1]) &lt; 0:\n        print "Point " + str(ii) + " should have been below the line"  \n        w = np.subtract(w,r)\n        restart = True\n        break       \n    elif (np.dot(w,r) &lt; 0) and int(label_dict[\'point{}\'.format(ii)][1]) &gt;= 0:\n        print "Point " + str(ii) + " should have been above the line"           \n        w = np.add(w,r)\n        restart = True\n        break           \n    else:\n        print "THERE IS AN ERROR, A POINT PASSED THROUGH HERE"\n\nprint w\nslope_w = (-w[0])/w[1] \nintercept_w = (-w[2])/w[1]\n'
'Suppose you want the predictions to be 80% sure than use thresholding to remove all prediction with `scores &lt; 0.8`\n'
"points = zip(x_train, y_learned)\npoints = sorted(points, key=lambda p: p[0])\nx_plot, y_plot = zip(*points)\nplt.plot(x_plot, y_plot, 'r')\n"
'tf.get_default_graph().as_graph_def()\n\nnode {\n  name: "conv2d/Conv2D"\n  op: "Conv2D"\n  input: "input_layer"\n  input: "conv2d/kernel/read"\n  attr {\n    key: "T"\n    value {\n      type: DT_FLOAT\n    }\n  }\n  attr {\n    key: "data_format"\n    value {\n      s: "NHWC"\n    }\n  }\n  attr {\n    key: "padding"\n    value {\n      s: "SAME"\n    }\n  }\n  attr {\n    key: "strides"\n    value {\n      list {\n        i: 1\n        i: 1\n        i: 1\n        i: 1\n      }\n    }\n  }\n  attr {\n    key: "use_cudnn_on_gpu"\n    value {\n      b: true\n    }\n  }\n}\n'
"import h5py\nh5f = h5py.File('your_file_name.h5', 'w')\nh5f.create_dataset('layer_model', data=intermediate_layer_model)\nh5f.create_dataset('output', data=intermediate_output)\nh5f.close()\n\nimport h5py\nh5f = h5py.File('your_file_name.h5', 'r')\nintermediate_layer_model = h5f['layer_model'][:]\nintermediate_output = h5f['output'][:]\nh5f.close()\n"
"turn_graph = tf.Graph()\nposn_graph = tf.Graph()\n\nwith turn_graph.as_default():\n    from models import inception_v3 as googlenet\n    turn_model = googlenet(227,227,3,1.0e-3,output=4)\n    turn_model.load('turn_model_01')\n\nwith posn_graph.as_default():\n    from models import inception_v3 as googlenet\n    posn_model = googlenet(227,227,3,1.0e-3,output=4)\n    posn_model.load('posn_model_01')\n\n## Other code\n\nwhile True :\n    posn_model.predict([image])\n    turn_model.predict([image])\n"
"model=Sequential()\nmodel.add(Dense(output_dim=2, init='uniform', activation='relu', input_dim=2))\nmodel.add(Dense(output_dim=3, init='uniform', activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(x_train, y_train, batch_size=12, epochs=14)\nloss_and_metrics = model.evaluate(x_test, y_test, batch_size=12)\nprint(loss_and_metrics)\n\nmodel.add(Dense(output_dim=4, init='uniform', activation='relu', input_dim=2))\n"
'# Get missing columns in the training test\nmissing_cols = set( X_train.columns ) - set( X_test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    X_test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\nX_test = X_test[X_train.columns]\n'
"X_train_sample_class_1 = X_train[X_train['third_column_name'] == 1][:30000]\nX_train_sample_class_0 = X_train[X_train['third_column_name'] == 0][:30000]\n"
"def binary_accuracy(y_true, y_pred):\n    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)\n\n\ndef categorical_accuracy(y_true, y_pred):\n    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n                          K.argmax(y_pred, axis=-1)),\n                  K.floatx())\n\n\ndef sparse_categorical_accuracy(y_true, y_pred):\n    # flatten y_true in case it's in shape (num_samples, 1) instead of (num_samples,)\n    return K.cast(K.equal(K.flatten(y_true),\n                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n                          K.floatx())\n\ndef top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)\n\n\ndef sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n    return K.mean(K.in_top_k(y_pred, K.cast(K.max(y_true, axis=-1), 'int32'), k), axis=-1)\n"
"to_predict = random_onehot((1, SEQUENCE_LENGTH, SEQUENCE_CHANNELS))\\\n        .astype(tf_type_string(I_DTYPE))\npred_features = {'input_tensors': to_predict}\n\npred_ds = tf.data.Dataset.from_tensor_slices(pred_features)\npredicted = est.predict(lambda: pred_ds, yield_single_examples=True)\n\nnext(predicted)\n\ndef predict_input_fn(data, batch_size=1):\n  dataset = tf.data.Dataset.from_tensor_slices(data)\n  return dataset.batch(batch_size).prefetch(None)\n\npredicted = est.predict(lambda: predict_input_fn(pred_features), yield_single_examples=True)\nnext(predicted)\n"
'# Creates a `colmun_name1_tokenized` column by \n# taking the `colmun_name1` column and \n# applying the word_tokenize function on every cell in the column. \n\n&gt;&gt;&gt; df[\'colmun_name1_tokenized\'] = df[\'colmun_name1\'].apply(word_tokenize)\n\n&gt;&gt;&gt; df.head()\n    colmun_name1     column_name2  column_name3  column_name4  classify  \\\n0  This is a cat    This is a dog             1             2         0   \n1  This is a rat  This is a mouse            45            32         1   \n2              a       Good mouse             0             0         0   \n\n  colmun_name1_tokenized  \n0     [This, is, a, cat]  \n1     [This, is, a, rat]  \n2                    [a]  \n\n&gt;&gt;&gt; with StringIO(file_str) as fin:\n...     df = pd.read_csv(fin, sep=\'\\t\')\n... \n&gt;&gt;&gt; for col_name in [\'colmun_name1\', \'column_name2\']:\n...     df[col_name] = df[col_name].apply(word_tokenize)\n... \n&gt;&gt;&gt; df.head()\n         colmun_name1          column_name2  column_name3  column_name4  \\\n0  [This, is, a, cat]    [This, is, a, dog]             1             2   \n1  [This, is, a, rat]  [This, is, a, mouse]            45            32   \n2                 [a]         [Good, mouse]             0             0   \n\n   classify  \n0         0  \n1         1  \n2         0  \n\nfrom io import StringIO\n\nimport pandas as pd\n\nfrom nltk import word_tokenize\n\nfile_str = """colmun_name1\\tcolumn_name2\\tcolumn_name3\\tcolumn_name4\\tclassify\nThis is a cat\\tThis is a dog\\t1\\t2\\t0\nThis is a rat\\tThis is a mouse\\t45\\t32\\t1\na\\tGood mouse\\t0\\t0\\t0 """\n\nwith StringIO(file_str) as fin:\n    df = pd.read_csv(fin, sep=\'\\t\')\n\nfor col_name in [\'colmun_name1\', \'column_name2\']:\n    df[col_name] = df[col_name].apply(word_tokenize)\n'
'"goals": {"food": "dontcare", "pricerange": "cheap", "area": "south"},\n"db_result": null,\n"dialog-acts": [{"slots": [], "act": "thankyou"}, {"slots": [], "act": "bye"}]}\n\nimport os\nfrom deeppavlov import build_model, configs, train_model\nfrom deeppavlov.core.common.file import read_json\n\nos.environ["KERAS_BACKEND"] = "tensorflow"\n\nmodel_config = read_json(configs.classifiers.intents_dstc2_big)\nmodel_config[\'chainer\'][\'out\'] =  [\'y_pred_labels\']\n\nmodel = build_model(model_config, download=True)\nprint(model(["thank you good bye"]))\n'
'import numpy as np\nfrom tensorflow.python import keras\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=(135, 240, 3)))\nmodel.add(keras.layers.Dense(1024, activation="relu"))\nmodel.add(keras.layers.Dense(512, activation="relu"))\nmodel.add(keras.layers.Dense(9, activation="softmax"))\n\nmodel.compile(loss="categorical_crossentropy", optimizer=\'adam\', metrics=["accuracy"])\n\n# dummy data\nimages = np.zeros(shape=(7990, 135, 240, 3))\nlabels = np.zeros(shape=(7990, 9))\n\n# train model\nmodel.fit(x=images, y=labels, batch_size=128)\n'
"feature_importances = grid_search.best_estimator_.feature_importances_\n\n# Get the best estimator's coefficients\nestimator_coeff = grid_search.best_estimator_.coef_\n# Multiply the model coefficients by the standard deviation of the data\ncoeff_magnitude = np.std(all_inputs, 0) * estimator_coeff)\n"
'k_means = KMeans(algorithm=\'auto\', copy_x=True, init=\'k-means++\', max_iter=300,\n    n_clusters=3, n_init=10, n_jobs=None, precompute_distances=\'auto\',\n    random_state=0, tol=0.0001, verbose=0)\n\nquotient_2d = quotient.reshape(-1,1)\nk_means.fit(quotient_2d)\n\ncolors = [\'r\',\'g\',\'b\']\ncentroids = k_means.cluster_centers_\nfor n, y in enumerate(centroids):\n    plt.plot(1, y, marker=\'x\', color=colors[n], ms=10)\nplt.title(\'Kmeans cluster centroids\')\n\n&gt;&gt;&gt; Z = k_means.predict(quotient_2d)\n&gt;&gt;&gt; Z\narray([1, 1, 0, 1, 2, 1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0], dtype=int32)\n\n# Plot each class as a separate colour\nn_clusters = 3 \nfor n in range(n_clusters):\n    #\xa0Filter data points to plot each in turn.\n    ys = quotient[ Z==n ]\n    xs = quotient_times[ Z==n ]\n\n    plt.scatter(xs, ys, color=colors[n])\n\nplt.title("Points by cluster")\n'
"1 / 0.377 = 2.65\n\nimport matplotlib.pyplot as plt\n\ndays = [1,2,3,4,5,6]\nsales1 = [1,4,6,8,10,15]\nsales2 = [1,2,3,4,5,6]\n\ndf = pd.DataFrame({'days': days, 'sales1': sales1, 'sales2': sales2})\ndf = df.set_index('days')\ndf.plot(marker='o', linestyle='--')\n"
"import numpy as np\nimport cv2\n\n# Load an color image in grayscale\nimg = cv2.imread('chest-ct-lungs.jpg',0)\nret,thresh = cv2.threshold(img,3,255,cv2.THRESH_BINARY)\ncv2.imwrite('output.png',thresh)\n"
"from sklearn.multioutput import MultiOutputClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nclassifier = MultiOutputClassifier(knn, n_jobs=-1)\nclassifier.fit(X,Y)\n\n&gt;&gt;&gt; from sklearn.feature_extraction.text import TfidfVectorizer\n&gt;&gt;&gt; corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n&gt;&gt;&gt; vectorizer = TfidfVectorizer()\n&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)\n\n&gt;&gt;&gt; Y = [[124323,1234132,1234],[124323,4132,14],[1,4132,1234],[1,4132,14]]\n\n&gt;&gt;&gt; from sklearn.multioutput import MultiOutputClassifier\n&gt;&gt;&gt; from sklearn.neighbors import KNeighborsClassifier\n&gt;&gt;&gt; knn = KNeighborsClassifier(n_neighbors=3)\n&gt;&gt;&gt; classifier = MultiOutputClassifier(knn, n_jobs=-1)\n&gt;&gt;&gt; classifier.fit(X,Y)\n&gt;&gt;&gt; predictions = classifier.predict(X)\n\narray([[124323,   4132,     14],\n       [124323,   4132,     14],\n       [     1,   4132,   1234],\n       [124323,   4132,     14]])\n\n&gt;&gt;&gt; classifier.score(X,np.array(Y))\n0.5\n\n&gt;&gt;&gt; test_data = ['I want to test this']\n&gt;&gt;&gt; classifier.predict(vectorizer.transform(test_data))\narray([[124323,   4132,     14]])\n"
"# change\nvec_anchor = base_model(input_anchor)\nvec_positive = base_model(input_positive)\nvec_negative = base_model(input_negative)\n# to\nvec_anchor = base_model(input_shape)\nvec_positive = base_model(input_shape)\nvec_negative = base_model(input_shape)\n\nconcat_layer = concatenate([vec_anchor.output,vec_positive.output,vec_negative.output], axis = -1, name='concat_layer')\n\nmodel = Model(inputs = [vec_anchor.input,vec_positive.input,vec_negative.input], outputs = concat_layer, name = 'speech_to_vec')\n"
'hi:     1\nyou:    2\nhello:  3\ncan:    4\nare:    5\nhow:    6\n...\n\n1:  [0.43, 0.09, 0.13, 1.65]\n2:  [0.43, 0.11, 0.23, -1.75]\n3:  [-0.88, 0.19, 2.33, 0.55]\n4:  [0.93, 0.79, -0.23, 2.05]\n5:  [0.27, 0.61, 0.10, 0.85]\n6:  [0.03, 1.09, -3.19, 2.25]\n...\n\n[[0.43, 0.09, 0.13, 1.65],\n [0.03, 1.09, -3.19, 2.25],\n [0.27, 0.61, 0.10, 0.85],\n [0.43, 0.11, 0.23, -1.75]]\n'
'prediction_int = prediction[:,1] &gt;= 0.3 \n\npred = svc.predict(X_test)  \n'
"model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mean_squared_error'])\n"
'prob = model.predict(x)\nif prob &lt; 0.5:\n    output = 0\nelse:\n    output = 1\n\n[[0.03993018]] =&gt; &lt; 0.5, class 0 correct\n[[0.9984968]]  =&gt; &gt; 0.5, class 1 incorrect\n[[1.]]         =&gt; &gt; 0.5, class 1 correct\n[[1.]]         =&gt; &gt; 0.5, class 1 correct\n[[0.]]         =&gt; &lt; 0.5, class 0 incorrect\n[[0.9999999]]  =&gt; &gt; 0.5, class 1 correct\n[[0.8691623]]  =&gt; &gt; 0.5, class 1 correct\n[[1.01611796e-07]] =&gt; &lt; 0.5, class 0 incorrect\n[[1.]]             =&gt; &gt; 0.5, class 1 correct\n[[0.]]             =&gt; &lt; 0.5, class 0 incorrect\n[[1.]]             =&gt; &gt; 0.5, class 1 correct\n[[0.17786741]]     =&gt; &lt; 0.5, class 0 incorrect\n'
'image = image / 255.0\n'
'neighbors.append(distances[i][0])\n\nneighbors.append(train.index(distances[i][0]) + 1)\n\n[1, 5, 2]\n'
"df = pd.read_csv('your_file_here.csv', sep='[\\s\\|]+')\nresults = [(a,b) for a,b in zip(zip(df['X0'],df['X1']), df['Y'])]\n\n[((55.5, 69.5), 1), ((71.0, 81.5), 1)]\n"
'from imblearn.oversampling import SMOTE\nsmote=SMOTE("minority")\nX,Y=smote.fit_sample(x_train_data,y_train_data)\n'
'train_seq = np.array([1,2,3,4,5])\ntest_seq = np.array([1,2,3,4,5,6,7,8,9,10], dtype=np.float32)\n\ntest_seq[~np.isin(test_seq, train_seq)] = np.nan\n\ndf = pd.get_dummies(test_seq, dummy_na=True)\nprint(df)\n\n\n   1.0  2.0  3.0  4.0  5.0  NaN\n0    1    0    0    0    0    0\n1    0    1    0    0    0    0\n2    0    0    1    0    0    0\n3    0    0    0    1    0    0\n4    0    0    0    0    1    0\n5    0    0    0    0    0    1\n6    0    0    0    0    0    1\n7    0    0    0    0    0    1\n8    0    0    0    0    0    1\n9    0    0    0    0    0    1\n'
'    def __init__(self, x):\n        super(ReLUTransformer, self).__init__()\n        self.x = torch.nn.Parameter(x)\n'
'rm -f /usr/include/cudnn.h \nrm -f /usr/lib/x86_64-linux-gnu/*libcudnn* \nrm -f /usr/local/cuda-/lib64/*libcudnn \n\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n'
'pd.DataFrame({\n   "fruit": ["Apple"],\n   "size": ["Large"], \n   "color": ["Red"], \n   "grade": [None], \n   "bool": [True]\n})\n\npd.DataFrame([{\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n}])\n\npd.DataFrame({\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n})\n\npd.DataFrame({\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n}, index=[0])\n\n  fruit   size color grade  bool\n0  Apple  Large   Red  None  True\n\npd.DataFrame({\n   "fruit": "Apple",\n   "size": "Large", \n   "color": "Red", \n   "grade": None, \n   "bool": True\n}, index=[0,1])\n\n   fruit   size color grade  bool\n0  Apple  Large   Red  None  True\n1  Apple  Large   Red  None  True\n'
'y_pred_test = lm.predict(X_test)\nmetrics.mean_absolute_error(y_test, y_pred_test)\n\ny_pred_train = lm.predict(X_train)\nmetrics.mean_absolute_error(y_train, y_pred_train)\n'
"import pandas as pd\n\ndata = [446.6565,  454.4733,  455.663 ,  423.6322,  456.2713,  440.5881, 425.3325,  485.1494,  506.0482,  526.792 ,  514.2689,  494.211 ]\nindex= pd.date_range(start='1996', end='2008', freq='A')\noildata = pd.Series(data, index)\n\nfrom statsmodels.tsa.api import SimpleExpSmoothing\nfit1 = SimpleExpSmoothing(oildata).fit(smoothing_level=0.2,optimized=False)\n# fcast1 = fit1.forecast(3).rename(r'$\\alpha=0.2$')\n\nimport matplotlib.pyplot as plt\nplt.plot(oildata)\nplt.plot(fit1.fittedvalues)\nplt.show()\n"
"# utility variable\nlook_back = 3\nbatch_size = 3\nn_feat = 11\nn_class = 11\nn_train = 200\nn_test = 60\n\n# data simulation\nX_train = np.random.uniform(0,1, (n_train,n_feat)) # 2D!\nX_test = np.random.uniform(0,1, (n_test,n_feat)) # 2D!\ny_train = np.random.randint(0,2, (n_train,n_class)) # 2D!\ny_test = np.random.randint(0,2, (n_test,n_class)) # 2D!\n\n\ntrain_data_gen = TimeseriesGenerator(X_train, y_train, length=look_back, batch_size=batch_size)\ntest_data_gen = TimeseriesGenerator(X_test, y_test, length=look_back, batch_size=batch_size)\n\n# check generator dimensions\nfor i in range(len(train_data_gen)):\n    x, y = train_data_gen[i]\n    print(x.shape, y.shape)\n\nBi_LSTM = Sequential()\nBi_LSTM.add(Bidirectional(LSTM(512), input_shape=(look_back, n_feat)))\nBi_LSTM.add(Dropout(.5))\nBi_LSTM.add(Dense(n_class, activation='softmax'))\nprint(Bi_LSTM.summary())\n\nBi_LSTM.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nBi_LSTM_history = Bi_LSTM.fit_generator(train_data_gen,\n                                        steps_per_epoch=50,\n                                        epochs=3,\n                                        verbose=1,\n                                        validation_data=test_data_gen) # class_weight=class_weights)\n"
'    m = m - learningRate * sum(Dm)/n\n    c = c - learningRate* sum(Dc)/n\n\nerror : 4.627422172738745 m: 6.2021421523611355 c: 1.3127611648190132\nerror : 5.407226002504083 m: -0.5251044659276074 c: 0.013389352211670591\nerror : 6.318019832044391 m: 6.721890877404075 c: 1.53485336818056\n'
"from typing import Tuple\n\nimport torch\n\n\nclass F1Score:\n    &quot;&quot;&quot;\n    Class for f1 calculation in Pytorch.\n    &quot;&quot;&quot;\n\n    def __init__(self, average: str = 'weighted'):\n        &quot;&quot;&quot;\n        Init.\n\n        Args:\n            average: averaging method\n        &quot;&quot;&quot;\n        self.average = average\n        if average not in [None, 'micro', 'macro', 'weighted']:\n            raise ValueError('Wrong value of average parameter')\n\n    @staticmethod\n    def calc_f1_micro(predictions: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:\n        &quot;&quot;&quot;\n        Calculate f1 micro.\n\n        Args:\n            predictions: tensor with predictions\n            labels: tensor with original labels\n\n        Returns:\n            f1 score\n        &quot;&quot;&quot;\n        true_positive = torch.eq(labels, predictions).sum().float()\n        f1_score = torch.div(true_positive, len(labels))\n        return f1_score\n\n    @staticmethod\n    def calc_f1_count_for_label(predictions: torch.Tensor,\n                                labels: torch.Tensor, label_id: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        &quot;&quot;&quot;\n        Calculate f1 and true count for the label\n\n        Args:\n            predictions: tensor with predictions\n            labels: tensor with original labels\n            label_id: id of current label\n\n        Returns:\n            f1 score and true count for label\n        &quot;&quot;&quot;\n        # label count\n        true_count = torch.eq(labels, label_id).sum()\n\n        # true positives: labels equal to prediction and to label_id\n        true_positive = torch.logical_and(torch.eq(labels, predictions),\n                                          torch.eq(labels, label_id)).sum().float()\n        # precision for label\n        precision = torch.div(true_positive, torch.eq(predictions, label_id).sum().float())\n        # replace nan values with 0\n        precision = torch.where(torch.isnan(precision),\n                                torch.zeros_like(precision).type_as(true_positive),\n                                precision)\n\n        # recall for label\n        recall = torch.div(true_positive, true_count)\n        # f1\n        f1 = 2 * precision * recall / (precision + recall)\n        # replace nan values with 0\n        f1 = torch.where(torch.isnan(f1), torch.zeros_like(f1).type_as(true_positive), f1)\n        return f1, true_count\n\n    def __call__(self, predictions: torch.Tensor, labels: torch.Tensor) -&gt; torch.Tensor:\n        &quot;&quot;&quot;\n        Calculate f1 score based on averaging method defined in init.\n\n        Args:\n            predictions: tensor with predictions\n            labels: tensor with original labels\n\n        Returns:\n            f1 score\n        &quot;&quot;&quot;\n\n        # simpler calculation for micro\n        if self.average == 'micro':\n            return self.calc_f1_micro(predictions, labels)\n\n        f1_score = 0\n        for label_id in range(1, len(labels.unique()) + 1):\n            f1, true_count = self.calc_f1_count_for_label(predictions, labels, label_id)\n\n            if self.average == 'weighted':\n                f1_score += f1 * true_count\n            elif self.average == 'macro':\n                f1_score += f1\n\n        if self.average == 'weighted':\n            f1_score = torch.div(f1_score, len(labels))\n        elif self.average == 'macro':\n            f1_score = torch.div(f1_score, len(labels.unique()))\n\n        return f1_score\n\nfrom sklearn.metrics import f1_score\nimport numpy as np\nerrors = 0\nfor _ in range(10):\n    labels = torch.randint(1, 10, (4096, 100)).flatten()\n    predictions = torch.randint(1, 10, (4096, 100)).flatten()\n    labels1 = labels.numpy()\n    predictions1 = predictions.numpy()\n\n    for av in ['micro', 'macro', 'weighted']:\n        f1_metric = F1Score(av)\n        my_pred = f1_metric(predictions, labels)\n        \n        f1_pred = f1_score(labels1, predictions1, average=av)\n        \n        if not np.isclose(my_pred.item(), f1_pred.item()):\n            print('!' * 50)\n            print(f1_pred, my_pred, av)\n            errors += 1\n\nif errors == 0:\n    print('No errors!')\n"
"import keras\nkeras.backend.set_floatx('float64')\n"
"col_means = X.mean(axis=0)\ncol_stds = X.std(axis=0)\nprint('column means: ', col_means)\nprint('column stdevs: ', col_stds)\n\ncolumn means:  [  3.84505208 120.89453125  69.10546875  20.53645833  79.79947917\n  31.99257812   0.4718763   33.24088542]\ncolumn stdevs:  [  3.36738361  31.95179591  19.34320163  15.94182863 115.16894926\n   7.87902573   0.33111282  11.75257265]\n\ncol_means = X.mean(axis=0)\ncol_stds = X.std(axis=0)\nprint('column means: ', col_means)\nprint('column stdevs: ', col_stds)\nX -= col_means\nX /= col_stds\nW, b = optimizer(W, b, X, Y, 100, 1.0)\n\ncolumn means:  [  3.84505208 120.89453125  69.10546875  20.53645833  79.79947917\n  31.99257812   0.4718763   33.24088542]\ncolumn stdevs:  [  3.36738361  31.95179591  19.34320163  15.94182863 115.16894926\n   7.87902573   0.33111282  11.75257265]\n0.6931471805599452\n0.5902957589079032\n0.5481784378158732\n0.5254804089153315\n...\n0.4709931321295562\n0.4709931263193595\n0.47099312122176273\n0.4709931167488006\n0.470993112823447\n"
"import numpy as np\nimport pandas as pd\nfrom sklearn.naive_bayes import BernoulliNB\n\ndef important_features(classifier,feature_names, n=20):\n    class_labels = classifier.classes_\n\n    for i,feature in enumerate(feature_names): \n        print(&quot;Important features in &quot;, class_labels[i])\n        topn_class = sorted(zip(classifier.feature_log_prob_[i], feature_names),\n                            reverse=True)[:n]\n        \n        for coef, feat in topn_class:\n            print(coef, feat)\n        print('-----------------------')\n\nd = {}\nd['fever'] = np.array([0,0,0,1,0,0,1])\nd['headache'] = np.array([0,0,1,0,0,1,0])\nd['sorethroat'] = np.array([1,0,0,0,1,1,0])\nd['drowsiness'] = np.array([0,1,0,1,1,0,0])\nd['disease'] = ['Fungal infection','Fungal infection','liver infection',\n           'diarrhoea','common cold','diarrhoea','flu']\n\ndf = pd.DataFrame(d)\n\nX = df[df.columns[:-1]]\ny = df['disease']\n\nclf = BernoulliNB()\nclf.fit(X, y)\nBernoulliNB()\n\nimportant_features(clf,df.columns[:-1])\n\nImportant features in  Fungal infection\n-0.6931471805599453 sorethroat\n-0.6931471805599453 drowsiness\n-1.3862943611198906 headache\n-1.3862943611198906 fever\n-----------------------\nImportant features in  common cold\n-0.4054651081081645 sorethroat\n-0.4054651081081645 drowsiness\n-1.0986122886681098 headache\n-1.0986122886681098 fever\n-----------------------\nImportant features in  diarrhoea\n-0.6931471805599453 sorethroat\n-0.6931471805599453 headache\n-0.6931471805599453 fever\n-0.6931471805599453 drowsiness\n-----------------------\nImportant features in  flu\n-0.4054651081081645 fever\n-1.0986122886681098 sorethroat\n-1.0986122886681098 headache\n-1.0986122886681098 drowsiness\n-----------------------\n"
"input_model_A = Input(shape=(12,))\nmodel_A = Dense(24)(input_model_A)\ninput_model_B = Input(shape=(12,))\nmodel_B = Dense(24)(input_model_B)\n# model_A and model_B must have the same last dimensionality\n# otherwise it is impossible to apply Add operation below\n\n# input for model weighting\ninput_weighting = Input(shape=(1,), name=&quot;vola_input&quot;)\n\nclass MyLayer(Layer):\n    def __init__(self, **kwargs):\n        \n        super(MyLayer, self).__init__(**kwargs)\n        self._c = K.variable(0.5)\n        self._gamma = K.variable(0.5)\n\n    def call(self, vola, **kwargs):\n        x = self._gamma * (vola - self._c) # gamma * (input_weighting - c)\n        result = tf.nn.sigmoid(x) # 1 / (1 + exp(-x))\n        return result\n\nG = MyLayer()(input_weighting) # 1/(1+exp(-gamma * (input_weighting - c)))\n\nweighted_A = Lambda(lambda x: x[0]*x[1])([model_A,G]) # A*G\nweighted_B = Lambda(lambda x: x[0]*(1-x[1]))([model_B,G]) # B*(1-G)\n\nmerge_layer = Add(name=&quot;merge_layer&quot;)([weighted_A, weighted_B]) # A*G + B*(1-G)\noutput_layer = Dense(units=1, activation='relu', name=&quot;output_layer&quot;)(merge_layer)\n\nmodel = Model(inputs=[input_model_A, input_model_B, input_weighting], outputs=[output_layer])\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n\n# create dummy data and fit\nn_sample = 100\nXa = np.random.uniform(0,1, (n_sample,12))\nXb = np.random.uniform(0,1, (n_sample,12))\nW = np.random.uniform(0,1, n_sample)\ny = np.random.uniform(0,1, n_sample)\n\nmodel.fit([Xa,Xb,W], y, epochs=3)\n"
"list(model.parameters())[0].shape # weights of the first layer in the format (N,C,Kernel dimensions) # 64, 3, 7 ,7\n\nimport torch\nimport torchvision\n\nnet = torchvision.models.resnet18(pretrained = True)\n\nshape_of_first_layer = list(net.parameters())[0].shape #shape_of_first_layer\n\nN,C = shape_of_first_layer[:2]\n\ndummy_input = torch.Tensor(N,C)\n\ndummy_input = dummy_input[...,:, None,None] #adding the None for height and weight\n\ntorch.onnx.export(net, dummy_input, './alpha')\n"
'clf = KerasClassifier(lambda: model_arch(3), epochs=10)\n\nclf = KerasClassifier(model_arch, n_features=3, epochs=10)\n'
'import pandas as pd\n\ndata = [\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/01/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/02/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/03/2020&quot;, &quot;value&quot;: 4},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/04/2020&quot;, &quot;value&quot;: 6},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/05/2020&quot;, &quot;value&quot;: 6},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/06/2020&quot;, &quot;value&quot;: 6},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/07/2020&quot;, &quot;value&quot;: 6},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/08/2020&quot;, &quot;value&quot;: 7},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/01/2020&quot;, &quot;value&quot;: 3},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/02/2020&quot;, &quot;value&quot;: 3},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/03/2020&quot;, &quot;value&quot;: 4},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/04/2020&quot;, &quot;value&quot;: 4},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/05/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/06/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/07/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/08/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/01/2020&quot;, &quot;value&quot;: 18},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/02/2020&quot;, &quot;value&quot;: 18},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/03/2020&quot;, &quot;value&quot;: 17},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/04/2020&quot;, &quot;value&quot;: 11},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/05/2020&quot;, &quot;value&quot;: 13},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/06/2020&quot;, &quot;value&quot;: 13},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/07/2020&quot;, &quot;value&quot;: 13},\n    {&quot;rb&quot;: 111, &quot;date&quot;: &quot;01/08/2020&quot;, &quot;value&quot;: 13},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/01/2020&quot;, &quot;value&quot;: 14},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/02/2020&quot;, &quot;value&quot;: 14},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/03/2020&quot;, &quot;value&quot;: 17},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/04/2020&quot;, &quot;value&quot;: 17},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/05/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/06/2020&quot;, &quot;value&quot;: 5},\n    {&quot;rb&quot;: 112, &quot;date&quot;: &quot;01/07/2020&quot;, &quot;value&quot;: 5}\n]\n\ndf = pd.DataFrame(data)\ndf[&quot;flag&quot;] = 0\nfor index in range(len(df) - 1):\n    df.loc[index, &quot;flag&quot;] = int(df.loc[index, &quot;rb&quot;] == df.loc[index + 1, &quot;rb&quot;] and\n                                df.loc[index, &quot;value&quot;] &lt; df.loc[index + 1, &quot;value&quot;])\n\ndf[&quot;flag_3m&quot;] = 0\nfor index in range(len(df)):\n    try:\n        df.loc[index, &quot;flag_3m&quot;] = int(df.loc[index, &quot;flag_3m&quot;] != 1 and\n           ((df.loc[index, &quot;value&quot;] &lt; df.loc[index + 1, &quot;value&quot;] and df.loc[index, &quot;rb&quot;] == df.loc[index + 1, &quot;rb&quot;]) or\n           (df.loc[index + 1, &quot;value&quot;] &lt; df.loc[index + 2, &quot;value&quot;] and df.loc[index, &quot;rb&quot;] == df.loc[index + 2, &quot;rb&quot;]) or\n           (df.loc[index + 2, &quot;value&quot;] &lt; df.loc[index + 3, &quot;value&quot;] and df.loc[index, &quot;rb&quot;] == df.loc[index + 3, &quot;rb&quot;])))\n    except:\n        # Dirty way ;)\n        pass\n\nprint(df)\n'
'def fit_transform(self, y):\n    &quot;&quot;&quot;Fit label encoder and return encoded labels\n    ...\n'
"preds = model.predict(X)\n\nprint(np.argmax(outs))\n\npip install pandas-datareader\n\nfrom pandas_datareader import data as wb\n\nticker=wb.DataReader('TSLA',start='2015-1-1',data_source='yahoo')\nprint(ticker)\n\n                  High         Low  ...      Volume   Adj Close\nDate                                ...                        \n2015-01-02   44.650002   42.652000  ...  23822000.0   43.862000\n2015-01-05   43.299999   41.431999  ...  26842500.0   42.018002\n2015-01-06   42.840000   40.841999  ...  31309500.0   42.256001\n2015-01-07   42.956001   41.956001  ...  14842000.0   42.189999\n2015-01-08   42.759998   42.001999  ...  17212500.0   42.124001\n                ...         ...  ...         ...         ...\n2020-10-13  448.890015  436.600006  ...  34463700.0  446.649994\n2020-10-14  465.899994  447.350006  ...  48045400.0  461.299988\n2020-10-15  456.570007  442.500000  ...  35672400.0  448.880005\n2020-10-16  455.950012  438.850006  ...  32620000.0  439.670013\n2020-10-19  447.000000  437.649994  ...   9422697.0  442.840607\n"
'import numpy as np\n\n\ndata = np.loadtxt(&quot;Adaboost-trainer.txt&quot;)\n\n# Determine your training/test split. I opted for 80/20\ntest_size = 0.2\nsplit_index = int(data.shape[0] * test_size)\n\n# Get the full train and test splits\nindices = np.random.permutation(data.shape[0])\ntest_idx = indices[split_index:]\ntrain_idx = indices[:split_index]\ntest = data[test_idx,:]\ntrain = data[train_idx,:]\n\n# Split the X and y for use in models\ny_train = train[:,-1]\nX_train = np.delete(train, 2, axis=1)\ny_test = test[:,-1]\nX_test = np.delete(test, 2, axis=1)\n'
'with open(&quot;sample.txt&quot;, &quot;r&quot;) as a_file:\n  for line in a_file:\n    text = line.strip()\n    print(text)\n'
'image = 255-image\nimage /= 255\n\nimage /= 255.0\n'
'train_info.artist = train_info.artist.str.strip()\n\ntrain_info[~train_info.artist.isin(lst)]\n'
"if not args.distributed:\n    if args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n        model.features = model.features\n        model.cuda()\n    else:\n        model = model.cuda()\nelse:\n    model.cuda()\n    model = model\n\nif args.arch.startswith('alexnet') or args.arch.startswith('vgg'):\n    model.features = model.features\nmodel = model.cuda()\n"
'x[0] - x[1]**2 + x[2]**3\n\nimport numpy as np\nimport tensorflow as tf\n\nnumber_of_records = 1000\nequation_order = 3\n\n# toy data\nX_data = np.random.randn(number_of_records, equation_order)\nX_data[1] = X_data[1]**2\nX_data[2] = X_data[2]**3\nY_data = np.matmul(X_data, np.array([[1], [-1], [1.]]))\n\nw = tf.Variable(np.zeros([equation_order, 1]), dtype="float32", name="w")\nb = tf.Variable(np.zeros([1]), dtype="float32", name="b")\n\nX = tf.placeholder("float32", shape=[None, equation_order])\nY = tf.placeholder("float32", shape=[None, 1])\n\nYpred = tf.add(tf.matmul(X, w), b)\n\nerror = tf.reduce_mean(tf.squared_difference(Ypred, Y))\noptimizer = tf.train.GradientDescentOptimizer(.1).minimize(error)\n\n\ninit = tf.global_variables_initializer()\n\n# Create the session to run the algorith\nwith tf.Session() as session:\n    session.run(init)\n\n    # Run the algorithm\n    for _ in range(200):\n        _, loss, Wcur = session.run([optimizer, error, w], feed_dict={X: X_data, Y: Y_data})\n        print loss, Wcur.transpose()\n\n5.33256e-14 [[ 0.99999988 -0.99999988  0.99999988]]\n\nfor i in range(equation_order):\n  print (i)\n  content_info_temp[:, i] = np.power(data["article_length"], (i+1))/np.max(np.power(data["article_length"], (i+1)))\n'
'test_model.predict_generator(...)\n'
'from sklearn.linear_model import SGDClassifier\nimport numpy as np\nX = np.random.random_sample((1000,3))\ny = np.random.binomial(3, 0.5, 1000)\nmodel = SGDClassifier(loss="modified_huber")\nmodel.partial_fit(X, y, classes=np.unique(y))\nprint(model.predict_proba([[0.5,0.6,0.7]]))\n'
'class sklearn.linear_model.Lasso(alpha=1.0, fit_intercept=True, normalize=False,\n    precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False,\n    positive=False, random_state=None, selection=’cyclic’)[source]\n\nreturn safe_sparse_dot(X, self.coef_.T,\n                       dense_output=True) + self.intercept_\n'
"x = tf.placeholder(tf.float32)\nseq = tf.placeholder(tf.int32)\n\ndef mask_by_length(input_matrix, length):\n    '''\n        Input matrix is a 2d tensor [batch_size, time_steps]\n        length is a 1d tensor\n        length refers to the length of input matrix axis 1\n    '''\n    length_transposed = tf.expand_dims(length, 1)\n\n    # Create range in order to compare length to\n    range = tf.range(tf.shape(input_matrix)[1])\n    range_row = tf.expand_dims(range, 0)\n\n    # Use the logical operations to create a mask\n    mask = tf.less(range_row, length_transposed)\n\n    # cast boolean to int to finalize mask\n    mask_result = tf.cast(mask, dtype=tf.float32)\n\n    # Element-wise multiplication to cancel out values in the mask\n    result = tf.multiply(mask_result, input_matrix)\n\n    return result\n\nmask_values = mask_by_length(x, seq)\n"
'all_words = []\n\nfor words_list, categ in documents:      #&lt;-- each wordlist is a list of words\n    for w in words_list:                 #&lt;-- Then access each word in list\n        all_words.append(w.lower())\n\nnp.random.shuffle(featuresets)\n\ntraining_set = featuresets[:1800]\ntesting_set = featuresets[1800:]\n\ndocuments = [ (list(movie_reviews.words(fileid)), category)\n             for category in movie_reviews.categories()\n             for fileid in movie_reviews.fileids(category) ]\n\nnp.random.shuffle(documents)\n\ntraining_set = documents[:1800]\ntesting_set = documents[1800:]\n\nall_words = []\n\nfor words_list, categ in documents:\n    for w in words_list:\n        all_words.append(w.lower())\n\nall_words = nltk.FreqDist(all_words)\n\nword_features = list(all_words.keys())[:3000]\n\ndef find_features(training_set):\n    words = set(training_set)\n    features = {}\n    for w in word_features:\n        features[w] = (w in words)\n\n    return features\n\nfeaturesets = [(find_features(rev), category) for (rev, category) in documents]\n\nnp.random.shuffle(featuresets)\n\ntraining_set = featuresets[:1800]\ntesting_set = featuresets[1800:]\n\nMNB_classifier = SklearnClassifier(MultinomialNB())\nMNB_classifier.train(training_set)\nprint("MNB_classifier accuracy:", (nltk.classify.accuracy(MNB_classifier, testing_set)) *100)\n'
'    [500, 250, 750, 125, 625, 375, ...]\n\n    [0, 1000, 500, 250, 750, 125, 625, 375, ...]\n'
'class Densifier(object):\n    def fit(self, X, y=None):\n        pass\n    def fit_transform(self, X, y=None):\n        return self.transform(X)\n    def transform(self, X, y=None):\n        return X.toarray()\n'
'@optunity.cross_validated(x=data, y=labels, num_folds=10, num_iter=2)\n\noptimal_model = sklearn.svm.SVC(**optimal_pars).fit(data, labels)\n\nimport optunity\nimport optunity.metrics\nimport sklearn.svm\nimport numpy as np\n\n#print len(array)   #=&gt; 1247\n#print len(labels)  #=&gt; 1247\n\n# make dummy data\narray = np.array([[i] for i in range(1247)])\nlabels = [True] * 100 + [False] * 1147\n\n# score function: twice iterated 10-fold cross-validated accuracy\n@optunity.cross_validated(x=array, y=labels, num_folds=10, num_iter=2)\ndef svm_auc(x_train, y_train, x_test, y_test, C, gamma):\n    model = sklearn.svm.SVC(C=C, gamma=gamma).fit(x_train, y_train)\n    decision_values = model.decision_function(x_test)\n    return optunity.metrics.roc_auc(y_test, decision_values)\n\n# perform tuning\noptimal_pars, _, _ = optunity.maximize(svm_auc, num_evals=200, C=[0, 10], gamma=[0, 1])\n\n# train model on the full training set with tuned hyperparameters\noptimal_model = sklearn.svm.SVC(**optimal_pars).fit(array, labels)\nprint(optimal_pars)\n'
"In [1]: # Here I'm just mapping very simply, you can definitely use regex or something for your case if you have a lot of classes \ndf['class'] = df['class'].map({'class1':0, 'class2':1, 'class3':2})\n\nIn [2]: df\nOut[2]: \na   b   ...     y   z   class\n0   1   ...     1   1   0\n1   0   ...     0   1   1\n2   0   ...     0   1   1 \n3   1   ...     0   1   2\n\nIn [3]: # Break between X (independent variables) and y (dependent, class)\nX = df.iloc[:,:-1]\ny = df['class']\n\nIn [4]: # Now you can do your fit etc...\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\nresult = gnb.fit(X,y)\n\nIn [5]:\ny_pred = result.predict(X)\ny_pred\nOut[5]: array([0, 1, 1, 2], dtype=int64)\n"
'nltk.chunk.conllstr2tree(text, chunk_types=[\'NP\']).draw()\n\ntext = """\n   he PRP B-NP\n   accepted VBD B-VP\n   the DT B-NP\n   position NN I-NP\n   of IN B-PP\n   vice NN B-NP\n   chairman NN I-NP\n   of IN B-PP\n   Carlyle NNP B-NP\n   Group NNP I-NP\n   , , O\n   a DT B-NP\n   merchant NN I-NP\n   banking NN I-NP\n   concern NN I-NP\n   . . O\n"""\n\ntoken / POS Tag / IOB-Chunk Type\n'
'class Storage(object):\n    pass\n\nimport pickle\nfrom storage import Storage\n\nD_var_instance = {}\nL = ["A","B","C"]\n\nfor var in L: \n    D_var_instance[var] = Storage(label=var,x=random.random(),y=random.random())\n\npickle.dump(D_var_instance, open("/path/pickle.txt", "wb"))\n\nD_var_instance = pickle.load(open("/path/pickle.txt", "rb"))\n\nipython boo.py\n{\'A\': &lt;storage.Storage instance at 0x1107b77e8&gt;, \'C\': &lt;storage.Storage instance at 0x1107b7680&gt;, \'B\': &lt;storage.Storage instance at 0x1107b7908&gt;}\n'
"SVC(kernel='linear')(**kwargs) # error!\n\nSVC(kernel='linear', **kwargs)\n\nfrom functools import partial\nlinear_svm = partial(svm.SVC, kernel='linear')\n\nlinear_svm(**kwargs)\n"
"import numpy as np\nfrom sklearn import datasets\nfrom sklearn.metrics import  pairwise \n\niris = datasets.load_iris()\n\nX = iris.data\nY = iris.target\n\n# dividing X by classes {0,1,2} to perform intra-distances\nX0 = X[np.where(Y==0)] \nX1 = X[np.where(Y==1)]\nX2 = X[np.where(Y==2)]\n\nsim0_intra = pairwise.pairwise_distances(X0, metric='euclidean')\nsim1_intra = pairwise.pairwise_distances(X1, metric='euclidean')\nsim2_intra = pairwise.pairwise_distances(X2, metric='euclidean')\n\nsimilarityClass0 = sim0_intra.sum()/(50*50-50) # output: 0.69812194319103826\nsimilarityClass1 = sim1_intra.sum()/(50*50-50) # output: 0.99736067331161615\nsimilarityClass2 = sim2_intra.sum()/(50*50-50) # output: 1.1767808010528609\n"
'assign_op = self.input_layer.w.assign(refined_w)\nself.sess.run(assign_op)\n'
"from sklearn.externals import joblib\n\njoblib.dump(regr, 'file_name.pkl')\n\n# load pickled model later\nregr = joblib.load('file_name.pkl') \n"
"from itertools import product\n\nfor dosage_comb in product(*dict_of_dose_ranges.values()):\n    dosage_items = zip(dict_of_dose_ranges.keys(), dosage_comb)\n    query_str = ' &amp; '.join('{} == {}'.format(*x) for x in dosage_items)\n    sub_df = dosage_df.query(query_str)\n\n    # Do Stuff...\n"
'y = np.array([[i * 0.4 + j * 0.9 + 1 for i, j in x]]).astype(np.float32).T\n\ny_hat = tf.squeeze(tf.matmul(x, w))\n'
'from sklearn.externals.joblib import Parallel, delayed\nimport numpy as np\nimport scipy.sparse as sp\n\ndef transform_parallel(self, X, n_jobs):\n    transform_splits = Parallel(n_jobs=n_jobs, backend="threading")(\n        delayed(self.transform)(X_split)\n        for X_split in np.array_split(X, n_jobs))\n\n    return sp.vstack(transform_splits)\n\nFeatureHasher.transform_parallel = transform_parallel\nf = FeatureHasher()\nf.transform_parallel(np.array([{\'a\':3,\'b\':2}]*10), n_jobs=5)\n\n&lt;10x1048576 sparse matrix of type \'&lt;class \'numpy.float64\'&gt;\'\n    with 20 stored elements in Compressed Sparse Row format&gt;\n'
'&gt;&gt;&gt; encoder.active_features_\narray([1, 3, 5])\n\n&gt;&gt;&gt; len(encoder.active_features_)\n3\n'
'&gt;&gt;&gt; x\narray([[0, 3, 6],\n       [1, 4, 7],\n       [2, 5, 8]])\n&gt;&gt;&gt; y\narray([[ 6,  0,     0],\n       [ 7, -1,  -123],\n       [ 8,  0,  1000]])\n&gt;&gt;&gt; similarity(x,y)\n1.0\n'
's.groupby(level=0).max()\n\n# ip\n# 192.168.1.1    22\n# 192.168.1.2     4\n# 192.168.1.3     3\n# 192.168.1.4     4\n# Name: p, dtype: int64\n'
'Y_pred = model.predict([tX, tXq])\nfor pred in Y_pred:\n    print (vocab[pred.argmax()-1])\n'
"import pandas as pd\nimport numpy as np\nX = pd.DataFrame(np.random.randint(0,100,size=(100, 2)), columns=list('XZ'))\ny = np.random.randint(2,size=100) # labels for binary classification\nX['Z2'] = X.Z**2    # add more features\nprint X.head() # note the added feature Z^2\n#    X   Z    Z2\n#0  88  90  8100\n#1  49  63  3969\n#2  27  23   529\n#3  47  71  5041\n#4  21  98  9604\ntrain_samples = 80  # Samples used for training the models\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pandas_ml import ConfusionMatrix\nimport matplotlib.pyplot as plt\nmodel = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n#print confusion_matrix(y_test, y_pred)\ncm = ConfusionMatrix(y_test, y_pred)\nprint cm\n# Predicted  0   1  __all__\n# Actual\n# 0          3   4        7\n# 1          4   9       13\n# __all__    7  13       20\ncm.plot()\nplt.show()\n"
'      loss = tf.reduce_mean(\n      tf.nn.nce_loss(weights=nce_weights,\n                     biases=nce_biases,\n                     labels=train_labels,\n                     inputs=embed,\n                     num_sampled=num_sampled,\n                     num_classes=vocabulary_size))\n'
'def lossf(w, X, y, l1, l2):\n    w.resize((w.shape[0],1))\n    y.resize((y.shape[0],1))\n\n    lossf1 = np.sum(ss.log1p(1 + np.nan_to_num(ss.expm1(-y * np.dot(X, w)))))\n    lossf2 = l2 * (np.dot(np.transpose(w), w))\n    lossf3 = l1 * sum(abs(w))\n    lossf = np.float(lossf1 + lossf2 + lossf3)\n    return lossf\n\ndef gradf(w, X, y, l1, l2):\n    w.resize((w.shape[0],1))\n    y.resize((y.shape[0],1))\n\n    gradw1 = l2 * 2 * w \n    gradw2 = l1 * np.sign(w)\n    gradw3 = -y * (1 + np.nan_to_num(ss.expm1(-y * np.dot(X, w))))\n    gradw3 = gradw3 / (2 + np.nan_to_num(ss.expm1(-y * np.dot(X, w))))\n    gradw3 = np.sum(gradw3 * X, axis=0)\n    gradw3.resize(gradw3.shape[0],1)\n    gradw = gradw1 + gradw2 + gradw3\n    gradw.resize(gradw.shape[0],)\n    return np.transpose(gradw)\nclass LR(ClassifierMixin, BaseEstimator):\n    def __init__(self, lr=0.000001, l1=0.1, l2=0.1, num_iter=100, verbose=0):\n        self.l1 = l1\n        self.l2 = l2\n        self.w = None\n        self.lr = lr\n        self.verbose = verbose\n        self.num_iter = num_iter\n\n    def fit(self, X, y):       \n        n, d = X.shape \n        self.w = np.zeros(shape=(d,))\n        for i in range(self.num_iter):\n            print "\\n", "Iteration ", i\n            g = gradf(self.w, X, y, self.l1, self.l2)\n            g.resize((g.shape[0],1))\n            self.w = self.w - g\n            print "Loss: ", lossf(self.w, X, y, self.l1, self.l2)\n        return self\n\n    def predict_proba(self, X):\n        probs = 1/(2 + ss.expm1(np.dot(-X, self.w)))\n        return probs \n\n    def predict(self, X):\n        probs = self.predict_proba(X)\n        probs = np.sign(2 * probs - 1)\n        probs.resize((probs.shape[0],))\n        return probs \n'
"&gt;&gt;&gt; x = ['-0.9148', '-1.7609', '0.8441', '-3.0872', '-3.3303', '-2.5054', '1.5679', '-4.6378', '-3.5720', '-3.3940']\n&gt;&gt;&gt; zip(*[x[i::3] for i in range(4)])\n[('-0.9148', '-1.7609', '0.8441', '-3.0872'), ('-3.0872', '-3.3303', '-2.5054', '1.5679'), ('1.5679', '-4.6378', '-3.5720', '-3.3940')]\n\nwith open('filename.csv', 'wb') as csvfile:\n    w = csv.writer(csvfile)\n    w.writerows(zip(*[x[i::3] for i in range(4)]))\n"
'batch_size = 1\nwith tf.Session() as sess:\n    sess.run(init)\n    step = 1\n\n    for i in range(len(training_data)):\n        batch_x = training_data[i]\n        batch_y = training_target[i]\n        batch_x = np.reshape(batch_x, [batch_size, 1, 2])\n        batch_y = np.reshape(batch_y, [batch_size, 2])\n        [_, acc, loss] = sess.run([optimizer, accuracy, cost], feed_dict={x: batch_x, y: batch_y})\n        print("Iter " + str(step) + ", Minibatch Loss= " + "{:.6f}".format(loss) + ", Training Accuracy= " + "{:.5f}".format(acc))\n'
"d = {'Bad':0, 'Good':1, 'Not Sure':2}\ndf['voting_result'] = df['voting_result'].map(d)\nprint (df)\n   voting_result project   scene\n0              0    ccus  345943\n1              1    ccus  311129\n2              0    ccus  309082\n3              0    ccus  331613\n4              1    ccus  331615\n5              2    ccus  331616\n"
'newY = pred_subj/y\ncrossE = tf.nn.softmax_cross_entropy_with_logits(pred_subj, newY)\naccr_subj_test = tf.reduce_mean(-crossE)\n'
'Y = tf.nn.relu(tf.add(tf.matmul(tf.transpose(x), W),b))\n'
"model = mx.mod.Module(symbol=symbol, context=mx.cpu(), label_names=None)\n...\nmodel.set_params(arg_params, aux_params, allow_missing=True)\n\n&gt;&gt;&gt;print(model.label_names)\n['softmax_label']\n\nmodel.bind(data_shapes=[('data', (1, 3, 224, 244))], for_training=False)\n"
"def prediction(spotId):\n    epoch = [5, 15, 25, 35, 45, 50, 100]\n    for e in epoch:\n        tf.reset_default_graph()\n\n        # Load CSV file, indicate that the first column represents labels\n        data = read_csv('nowcastScaled'+str(spotId)+'.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n\n        # transform data to be stationary\n        raw_values = data.values\n        diff_values = difference(raw_values, 1)\n\n        # transform data to be supervised learning\n        supervised = timeseries_to_supervised(diff_values, 1)\n        supervised_values = supervised.values\n\n        # split data into train and test-sets (I removed the testing data from the excel file)\n        train = supervised_values[x:]\n\n        # transform the scale of the data (and removed anything related to testing set)\n        scaler, train_scaled = scale(train)\n        # Build neural network\n        net = tflearn.input_data(shape=[None, 1, 1])\n        tnorm = tflearn.initializations.uniform(minval=-1.0, maxval=1.0)\n        net = tflearn.lstm(net, 1, dropout=0.8)\n        net = tflearn.fully_connected(net, 1, activation='linear', weights_init=tnorm)\n        net = tflearn.regression(net, optimizer='adam', learning_rate=0.0001,\n                                     loss='mean_square', metric='R2')\n        lstm_model = tflearn.DNN(net, clip_gradients=0.)\n        lstm_model.load('ModeSpot'+str(spotId)+'Epoch'+str(e)+'.tflearn')\n\n        # forecast the entire training dataset to build up state for forecasting\n        train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n        lstm_model.predict(train_reshaped)\n        # walk-forward validation on the test data\n        predictions = list()\n        predictionFeeder = list()\n        X, y = train_scaled[0, 0:-1], train_scaled[0, -1]\n        predictionFeeder.append(X)\n        for i in range(0, 10000):\n            # make one-step forecast\n            yhat = forecast_lstm2(lstm_model, predictionFeeder[i])\n            predictionFeeder.append(yhat)\n            # invert scaling\n            yhat2 = invert_scale(scaler, predictionFeeder[i + 1], yhat)\n            yhat3 = inverse_difference(raw_values, yhat2, 10000 + 1 - i)\n            predictions.append(yhat3)\n"
"popt, pcov = curve_fit(f_exp, train_x, train_y)\nprint(popt)\nimport functools\n# preparing arguments\nkwargs = dict(zip(['a', 'b', 'c'], popt))\noptimized_f_exp = functools.partial(f_exp, **kwargs)\nthreshold = fsolve(optimized_f_exp, 50)\n"
'{\n"image_data_format": "channels_last",\n"epsilon": 1e-07,\n"floatx": "float32",\n"backend": "tensorflow"\n}\n'
'import numpy as np\nimport scipy.optimize as spo\nnp.set_printoptions(linewidth=120)\nnp.random.seed(1)\n\n""" Create random data """\nM, N = 2, 3\nA = np.random.random(size=(M,N))\nx_hidden = np.random.random(size=N)\nb = A.dot(x_hidden)  # target\nn = N\n\nprint(\'Original A\')\nprint(A)\nprint(\'hidden x: \', x_hidden)\nprint(\'target: \', b)\n\n""" Optimize as LP """\ndef solve(A, b, n):\n    print(\'Reformulation\')\n    am, an = A.shape\n\n    # Introduce aux-vars\n    # 1: y = mean\n    # n: z = x - mean\n    # n: abs(z)\n    n_plus_aux_vars = 3*n + 1\n\n    # Equality constraint: y = mean\n    eq_mean_A = np.zeros(n_plus_aux_vars)\n    eq_mean_A[:n] = 1. / n\n    eq_mean_A[n] = -1.\n    eq_mean_b = np.array([0])\n\n    print(\'y = mean A:\')\n    print(eq_mean_A)\n    print(\'y = mean b:\')\n    print(eq_mean_b)\n\n    # Equality constraints: Ax = b\n    eq_A = np.zeros((am, n_plus_aux_vars))\n    eq_A[:, :n] = A[:, :n]\n    eq_b = np.copy(b)\n\n    print(\'Ax=b A:\')\n    print(eq_A)\n    print(\'Ax=b b:\')\n    print(eq_b)\n\n    # Equality constraints: z = x - mean\n    eq_mean_A_z = np.hstack([-np.eye(n), np.ones((n, 1)), + np.eye(n), np.zeros((n, n))])\n    eq_mean_b_z = np.zeros(n)\n\n    print(\'z = x - mean A:\')\n    print(eq_mean_A_z)\n    print(\'z = x - mean b:\')\n    print(eq_mean_b_z)\n\n    # Inequality constraints: absolute values -&gt; x &lt;= x\' ; -x &lt;= x\'\n\n    ineq_abs_0_A = np.hstack([np.zeros((n, n)), np.zeros((n, 1)), np.eye(n), -np.eye(n)])\n    ineq_abs_0_b = np.zeros(n)\n\n    ineq_abs_1_A = np.hstack([np.zeros((n, n)), np.zeros((n, 1)), -np.eye(n), -np.eye(n)])\n    ineq_abs_1_b = np.zeros(n)\n\n    # Bounds\n    # REMARK: Do not touch anything besides the first bounds-row!\n    bounds = [(0., 1.) for i in range(n)] +     \\\n             [(None, None)] +                   \\\n             [(None, None) for i in range(n)] + \\\n             [(0, None) for i in range(n)]\n\n    # Objective\n    c = np.zeros(n_plus_aux_vars)\n    c[-n:] = 1\n\n    A_eq = np.vstack((eq_mean_A, eq_A, eq_mean_A_z))\n    b_eq = np.hstack([eq_mean_b, eq_b, eq_mean_b_z])\n\n    A_ineq = np.vstack((ineq_abs_0_A, ineq_abs_1_A))\n    b_ineq = np.hstack([ineq_abs_0_b, ineq_abs_1_b])\n\n    print(\'solve...\')\n    result = spo.linprog(c, A_ineq, b_ineq, A_eq, b_eq, bounds=bounds, method=\'simplex\')\n    print(result)\n    x = result.x[:n]\n    print(\'x: \', x)\n    print(\'residual Ax-b: \', A.dot(x) - b)\n    print(\'mean: \', result.x[n])\n    print(\'x - mean: \', x - result.x[n])\n    print(\'l1-norm(x - mean) / objective: \', np.linalg.norm(x - result.x[n], 1))\n\nsolve(A, b, n)\n\nOriginal A\n[[  4.17022005e-01   7.20324493e-01   1.14374817e-04]\n [  3.02332573e-01   1.46755891e-01   9.23385948e-02]]\nhidden x:  [ 0.18626021  0.34556073  0.39676747]\ntarget:  [ 0.32663584  0.14366255]\nReformulation\ny = mean A:\n[ 0.33333333  0.33333333  0.33333333 -1.          0.          0.          0.          0.          0.          0.        ]\ny = mean b:\n[0]\nAx=b A:\n[[  4.17022005e-01   7.20324493e-01   1.14374817e-04   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n    0.00000000e+00   0.00000000e+00   0.00000000e+00]\n [  3.02332573e-01   1.46755891e-01   9.23385948e-02   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n    0.00000000e+00   0.00000000e+00   0.00000000e+00]]\nAx=b b:\n[ 0.32663584  0.14366255]\nz = x - mean A:\n[[-1. -0. -0.  1.  1.  0.  0.  0.  0.  0.]\n [-0. -1. -0.  1.  0.  1.  0.  0.  0.  0.]\n [-0. -0. -1.  1.  0.  0.  1.  0.  0.  0.]]\nz = x - mean b:\n[ 0.  0.  0.]\nsolve...\n     fun: 0.078779576294411263\n message: \'Optimization terminated successfully.\'\n     nit: 10\n   slack: array([ 0.07877958,  0.        ,  0.        ,  0.        ,  0.07877958,  0.        ,  0.76273076,  0.68395118,\n        0.72334097])\n  status: 0\n success: True\n       x: array([ 0.23726924,  0.31604882,  0.27665903,  0.27665903, -0.03938979,  0.03938979,  0.        ,  0.03938979,\n        0.03938979,  0.        ])\nx:  [ 0.23726924  0.31604882  0.27665903]\nresidual Ax-b:  [  5.55111512e-17   0.00000000e+00]\nmean:  0.276659030053\nx - mean:  [-0.03938979  0.03938979  0.        ]\nl1-norm(x - mean) / objective:  0.0787795762944\n\nA = np.array([[3582.000000, 3394.000000, 3356.000000, 3256.000000, 3415.000000,\n        3505.000000, 3442.000000, 3381.000000, 3392.000000],\n       [233445193.000000, 217344811.000000, 237746918.000000,\n        219204892.000000, 225272825.000000, 242819442.000000,\n        215258725.000000, 227681178.000000, 215189377.000000],\n       [559090945.000000, 496500751.000000, 493639029.000000,\n        461547877.000000, 501057960.000000, 505073223.000000,\n        490458632.000000, 503102998.000000, 487026744.000000]])\nb = np.array([8531., 1079115532., 429386951.])\n\nA /= 10000.  # scaling\nb /= 10000.  # scaling\n\nbounds = [(-50., 50.) for i in range(n)] +     \\\n...\n\nresult = spo.linprog(c, A_ineq, b_ineq, A_eq, b_eq, bounds=bounds, method=\'interior-point\')\n\nsolve...\n     con: array([  3.98410760e-09,   1.18067724e-08,   8.12879938e-04,   1.75969041e-03,  -3.93853838e-09,  -3.96305566e-09,\n        -4.10043555e-09,  -3.94957667e-09,  -3.88362764e-09,  -3.89452381e-09,  -3.95134592e-09,  -3.92182287e-09,\n        -3.85762178e-09])\n     fun: 52.742900697626389\n message: \'Optimization terminated successfully.\'\n     nit: 8\n   slack: array([  5.13245265e+01,   1.89309145e-08,   1.83429094e-09,   4.28687782e-09,   1.03726911e-08,   2.77000474e-09,\n         1.41837413e+00,   6.75769654e-09,   8.65285462e-10,   2.78501844e-09,   3.09591539e-09,   5.27429006e+01,\n         1.30944103e-08,   5.32994799e-09,   3.15369669e-08,   2.51943821e-09,   7.54848797e-09,   3.22510447e-09])\n  status: 0\n success: True\n       x: array([ -2.51938304e+01,   4.68432810e-01,   2.68398831e+01,   4.68432822e-01,   4.68432815e-01,   4.68432832e-01,\n        -2.40754247e-01,   4.68432818e-01,   4.68432819e-01,   4.68432822e-01,  -2.56622633e+01,  -7.91749954e-09,\n         2.63714503e+01,   4.40376624e-09,  -2.52137156e-09,   1.43834811e-08,  -7.09187065e-01,   3.95395716e-10,\n         1.17990950e-09,   2.56622633e+01,   1.10134149e-08,   2.63714503e+01,   8.69064406e-09,   7.85131955e-09,\n         1.71534858e-08,   7.09187068e-01,   7.15309226e-09,   2.04519496e-09])\nx:  [-25.19383044   0.46843281  26.83988313   0.46843282   0.46843282   0.46843283  -0.24075425   0.46843282   0.46843282]\nresidual Ax-b:  [ -1.18067724e-08  -8.12879938e-04  -1.75969041e-03]\nmean:  0.468432821891\nx - mean:  [ -2.56622633e+01  -1.18805552e-08   2.63714503e+01   4.54189575e-10  -6.40499920e-09   1.04889573e-08  -7.09187069e-01\n  -3.52642715e-09  -2.67771227e-09]\nl1-norm(x - mean) / objective:  52.7429006758\n\nimport numpy as np\nimport cvxpy as cvx\n\n""" DATA """\nA = np.array([[3582.000000, 3394.000000, 3356.000000, 3256.000000, 3415.000000,\n        3505.000000, 3442.000000, 3381.000000, 3392.000000],\n       [233445193.000000, 217344811.000000, 237746918.000000,\n        219204892.000000, 225272825.000000, 242819442.000000,\n        215258725.000000, 227681178.000000, 215189377.000000],\n       [559090945.000000, 496500751.000000, 493639029.000000,\n        461547877.000000, 501057960.000000, 505073223.000000,\n        490458632.000000, 503102998.000000, 487026744.000000]])\nb = np.array([8531., 1079115532., 429386951.])\nn = 9\n\n# A /= 10000.  scaling would be a good idea\n# b /= 10000.  """\n\n""" SOCP-based least-squares approach """\ndef solve(A, b, n, c=1e-1):\n    x = cvx.Variable(n)\n    y = cvx.Variable(1)             # mean\n\n    lower_bounds = np.zeros(n) - 50  # -50\n    upper_bounds = np.zeros(n) + 50  #  50\n\n    constraints = []\n    constraints.append(x &gt;= lower_bounds)\n    constraints.append(x &lt;= upper_bounds)\n    constraints.append(y == cvx.sum_entries(x) / n)\n\n    objective = cvx.Minimize(cvx.norm(A*x-b, 2) + c * cvx.norm(x - y, 1))\n    problem = cvx.Problem(objective, constraints)\n\n    problem.solve(solver=cvx.ECOS, verbose=True)\n\n    print(\'Objective: \', problem.value)\n    print(\'x: \', x.T.value)\n    print(\'mean: \', y.value)\n    print(\'Ax-b: \')\n    print((A*x - b).value)\n    print(\'x - mean: \', (x - y).T.value)\n\nsolve(A, b, n)\n\n ECOS 2.0.4 - (C) embotech GmbH, Zurich Switzerland, 2012-15. Web: www.embotech.com/ECOS\n\nIt     pcost       dcost      gap   pres   dres    k/t    mu     step   sigma     IR    |   BT\n 0  +2.637e-17  -1.550e+06  +7e+08  1e-01  2e-04  1e+00  2e+07    ---    ---    1  1  - |  -  -\n 1  -8.613e+04  -1.014e+05  +8e+06  1e-03  2e-06  2e+03  2e+05  0.9890  1e-04   2  1  1 |  0  0\n 2  -1.287e+03  -1.464e+03  +1e+05  1e-05  9e-08  4e+01  3e+03  0.9872  1e-04   3  1  1 |  0  0\n 3  +1.794e+02  +1.900e+02  +2e+03  2e-07  1e-07  1e+01  5e+01  0.9890  5e-03   5  3  4 |  0  0\n 4  -1.388e+00  -6.826e-01  +1e+02  1e-08  7e-08  9e-01  3e+00  0.9458  4e-03   7  6  6 |  0  0\n 5  +5.491e+00  +5.683e+00  +1e+01  1e-09  8e-09  2e-01  3e-01  0.9617  6e-02   1  1  1 |  0  0\n 6  +6.480e+00  +6.505e+00  +1e+00  2e-10  5e-10  3e-02  4e-02  0.8928  2e-02   1  1  1 |  0  0\n 7  +6.746e+00  +6.746e+00  +2e-02  3e-12  5e-10  5e-04  6e-04  0.9890  5e-03   1  0  0 |  0  0\n 8  +6.759e+00  +6.759e+00  +3e-04  2e-12  2e-10  6e-06  7e-06  0.9890  1e-04   1  0  0 |  0  0\n 9  +6.759e+00  +6.759e+00  +3e-06  2e-13  2e-10  6e-08  8e-08  0.9890  1e-04   2  0  0 |  0  0\n10  +6.758e+00  +6.758e+00  +3e-08  5e-14  2e-10  7e-10  9e-10  0.9890  1e-04   1  0  0 |  0  0\n\nOPTIMAL (within feastol=2.0e-10, reltol=4.7e-09, abstol=3.2e-08).\nRuntime: 0.002901 seconds.\n\nObjective:  6.757722879805085\nx:  [[-18.09169736  -5.55768047  11.12130645  11.48355878  -1.13982006\n   12.4290884   -3.00165819  -1.05158589  -2.4468432 ]]\nmean:  0.416074272576\nAx-b:\n[[  2.17051777e-03]\n [  1.90734863e-06]\n [ -5.72204590e-06]]\nx - mean:  [[-18.50777164  -5.97375474  10.70523218  11.0674845   -1.55589434\n   12.01301413  -3.41773246  -1.46766016  -2.86291747]]\n\nObjective:  785913288.2410747\nx:  [[ -5.57966858e-08  -4.74997454e-08   1.56066068e+00   1.68021234e-07\n   -3.55602958e-08   1.75340641e-06  -4.69609562e-08  -3.10216680e-08\n   -4.39482554e-08]]\nmean:  0.173406926909\nAx-b:\n[[ -3.29341696e+03]\n [ -7.08072860e+08]\n [  3.41016903e+08]]\nx - mean:  [[-0.17340698 -0.17340697  1.38725375 -0.17340676 -0.17340696 -0.17340517\n  -0.17340697 -0.17340696 -0.17340697]]\n'
'# Build your classifier\nclassifier = svm.SVC()\n\n# Train it on the entire training data set\nclassifier.fit(X_train, y_train)\n\n# Get predictions on the test set\ny_pred = classifier.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test, y_pred))\n'
'# Data Preprocessing\n\n# Import Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Import Dataset\ndataset = pd.read_csv(\'Data2.csv\')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 5].values\ndf_X = pd.DataFrame(X)\ndf_y = pd.DataFrame(y)\n\n# Replace Missing Values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(missing_values = \'NaN\', strategy = \'mean\', axis = 0)\nimputer = imputer.fit(X[:, 3:5 ])\nX[:, 3:5] = imputer.transform(X[:, 3:5])\n\n\n# Encoding Categorical Data "Name"\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_x = LabelEncoder()\nX[:, 0] = labelencoder_x.fit_transform(X[:, 0])\n\n\n# Encoding Categorical Data "University"\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_x1 = LabelEncoder()\nX[:, 1] = labelencoder_x1.fit_transform(X[:, 1])\n\n\n# Transform Name into a Matrix\nonehotencoder1 = OneHotEncoder(categorical_features = [0])\nX = onehotencoder1.fit_transform(X).toarray()\n\n# Transform University into a Matrix\nonehotencoder2 = OneHotEncoder(categorical_features = [6])\nX = onehotencoder2.fit_transform(X).toarray()\n'
'pip install scipy\n'
'hyp = torch.exp(scores - torch.max(scores))\nprobs = hyp / torch.sum(hyp)\ncorrect_probs = probs[0:N,y] # problem solved\nlogprobs = torch.log(correct_probs)\ncost_data = -1/N * torch.sum(logprobs)\n\ny = Variable(torch.from_numpy(y),requires_grad=False).type(torch.LongTensor)\n'
'from sklearn.svm import SVR\nsvr = SVR()\nsvr.fit(X_train, y_train)\nprint(svr.score(X_train, y_train))\nprint(svr.score(X_test, y_test))\n'
'&gt;&gt;&gt; cd = torch.LongTensor([1,0])\n&gt;&gt;&gt; gd = [0.39613232016563416]\n&gt;&gt;&gt; v = torch.FloatTensor(gd)\n&gt;&gt;&gt; p = torch.rand((2,2))\n&gt;&gt;&gt; p\n\n 0.9342  0.8539\n 0.7044  0.0823\n\n&gt;&gt;&gt; p[cd[0], cd[1]] = v[0]\n&gt;&gt;&gt; p\n\n0.9342  0.8539\n0.3961  0.0823\n'
'&gt;&gt;&gt; from sklearn.svm import SVC\n&gt;&gt;&gt; svc = SVC()\n&gt;&gt;&gt; svc.score??\nSignature: svc.score(X, y, sample_weight=None)\nSource:   \n    def score(self, X, y, sample_weight=None):\n        """Returns the mean accuracy on the given test data and labels.\n\n        In multi-label classification, this is the subset accuracy\n        which is a harsh metric since you require for each sample that\n        each label set be correctly predicted.\n\n        Parameters\n        ----------\n        X : array-like, shape = (n_samples, n_features)\n            Test samples.\n\n        y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n            True labels for X.\n\n        sample_weight : array-like, shape = [n_samples], optional\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Mean accuracy of self.predict(X) wrt. y.\n\n        """\n        from .metrics import accuracy_score\n        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)\nFile:      ~/miniconda/lib/python3.6/site-packages/sklearn/base.py\nType:      method\n'
"from keras.models import load_model\nmodel = load_model('model.h5')\n\nclassifier.compile(loss='your_loss', optimizer='your_optimizer', metrics=['your_metrics'])\n\nfrom keras.preprocessing import image\n\ntest_image= image.load_img(picturePath, target_size = (img_width, img_height)) \ntest_image = image.img_to_array(test_image)\ntest_image = numpy.expand_dims(test_image, axis = 0)\ntest_image = test_image.reshape(img_width, img_height)\nresult = model.predict(test_image)   \n"
"from sklearn.grid_search import GridSearchCV\n\nfrom sklearn.model_selection import GridSearchCV\n\ntimeseries_split = TimeSeriesSplit(n_splits=3)\nclf = GridSearchCV(reg, param, cv=timeseries_split, scoring='neg_mean_squared_error')\n"
'new_im.paste(im,(int((size[0]-old_im_size[0])/2), int((size[1]- old_im_size[1])/2)))\n'
"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_10 (InputLayer)        (None, 400, 16)           0         \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 400, 32)           4128      \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 32)                8320      \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 12,481\nTrainable params: 12,481\nNon-trainable params: 0\n_________________________________________________________________\n\nfor layer in model.layers:\n    if layer.name != `lstm_2`\n        layer.trainable = False\n\nfor layer in model.layers:\n    layer.trainable = False\n\nmodel.layers[2].trainable = True  # set lstm to be trainable\n\n# to make sure 2 is the index of the layer\nprint(model.layers[2].name)    # prints 'lstm_2'\n"
'df_numeric = df.select_dtypes(exclude=[\'object\'])\ndf_obj = df.select_dtypes(include=[\'object\']).copy()\n\n# factorize categoricals columnwise\nfor c in df_obj:\n     df_obj[c] = pd.factorize(df_obj[c])[0]\n\n# if you want to one hot encode then add this line:\ndf_obj = pd.get_dummies(df_obj, prefix_sep=\'_\', drop_first = True)\n\n# merge dataframes back to one dataframe\ndf_final = pd.concat([df_numeric, df_obj], axis=1)\n\nfrom sklearn import preprocessing\n\ndef scale_data(data, scale="robust"):\n    x = data.values     \n    if scale == "minmax":\n        scaler = preprocessing.MinMaxScaler()\n        x_scaled = scaler.fit_transform(x)\n    elif scale == "standard":\n        scaler = preprocessing.StandardScaler()\n        x_scaled = scaler.fit_transform(x)\n    elif scale == "quantile":\n        scaler = preprocessing.QuantileTransformer()\n        x_scaled = scaler.fit_transform(x)      \n    elif scale == "robust":\n        scaler = preprocessing.RobustScaler()\n        x_scaled = scaler.fit_transform(x)  \n    data = pd.DataFrame(x_scaled, columns = data.columns) \n    return data\n\nscaled_df = scale_data(df_numeric, "robust")\n\nfrom sklearn import preprocessing\n\ndf = pd.read_excel("default of credit card clients.xls", skiprows=1)\n\ny = df[\'default payment next month\'] #target variable\ndel df[\'default payment next month\']\n\nc = [2,3,4] # index of categorical data columns\nr = list(range(0,24)) \nr = [x for x in r if x not in c] # get list of all other columns\ndf_cat = df.iloc[:, [2,3,4]].copy()\ndf_con = df.iloc[:, r].copy()\n\n# factorize categorical data\nfor c in df_cat:\n     df_cat[c] = pd.factorize(df_cat[c])[0]\n\n# scale continuous data\nscaler = preprocessing.MinMaxScaler()\ndf_scaled = scaler.fit_transform(df_con)\ndf_scaled = pd.DataFrame(df_scaled, columns=df_con.columns)\n\ndf_final = pd.concat([df_cat, df_scaled], axis=1)\n\n#reorder columns back to original order\ncols = df.columns\ndf_final = df_final[cols]\n'
'new_model = models.alexnet(pretrained=True)\nnew_classifier = nn.Sequential(*list(new_model.classifier.children())[:-1])\nnew_model.classifier = new_classifier\n'
"vectorizer = TfidfVectorizer(token_pattern=r'(?u)\\b\\w\\w+__\\([\\w\\s]*\\)')\nX = vectorizer.fit_transform(doc)\n"
"import numpy as np\nfrom sklearn.neighbors import KDTree\n\nnp.random.seed(42)\nfeatures = np.random.random((10, 2))\n\nX = np.array(features[0:2])\ntree = KDTree(features, leaf_size=40)\nindices = tree.query_radius(X, r=np.array([0.1, 0.5]))\n\nfor cursor, ix in enumerate(indices):\n    np.savetxt('file{}.txt'.format(cursor), features[ix], fmt='%s')\n"
"#PLOT TRAINING DATA\n\nfor d, sample in enumerate(X):\n    # Plot the negative samples\nif d &lt; 2:\n    plt.scatter(sample[0], sample[1], s=120, marker='_', linewidths=2)\n    # Plot the positive samples\nelse:\n    plt.scatter(sample[0], sample[1], s=120, marker='+', linewidths=2)\n\n#PLOT TESTING DATA\n\n# Add our test samples\nplt.scatter(2,2, s=120, marker='_', linewidths=2, color='yellow')\nplt.scatter(4,3, s=120, marker='+', linewidths=2, color='blue')\nplt.show()\n\n#CALL YOUR METHOD\nw = svm_sgd_plot(X,Y)\n\n# Print the hyperplane calculated by svm_sgd()\nx2=[ w[0],w[1],-w[1],w[0] ]\nx3=[ w[0],w[1],w[1],-w[0] ]\n\nx2x3 = np.array([x2,x3])\nX,Y,U,V = zip(*x2x3)\nax = plt.gca()\nax.quiver(X,Y,U,V,scale=1, color='blue')\n#I ADDED THE FOLLOWING THREE LINES SO THAT YOU CAN SEE HOW YOU TESTING DATA IS BEING CLASSIFIED BY YOUR SVM METHOD\nplt.scatter(2,2, s=120, marker='_', linewidths=2, color='yellow')\nplt.scatter(4,3, s=120, marker='+', linewidths=2, color='blue')\nplt.show()\n"
'import numpy as np\n\nX_train = np.transpose(X_train)\nY_train = np.transpose(Y_train)\nX_test = np.transpose(X_test)\nY_test = np.transpose(Y_test)\n'
'def locate_symbol(x, template):\n    w, h = filter_num2.shape[::-1]\n\n    res = cv2.matchTemplate(x, template, cv2.TM_SQDIFF_NORMED)\n    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n\n    min_thresh = 0.45\n    match_locations = np.where(res&lt;=min_thresh)\n\n    return w, h, match_locations\n\nw, h, locs = locate_symbol(grayscale_image, filter_num2)\n\nfor (x, y) in zip(locs[1], locs[0]):\n    cv2.rectangle(printable_image, (x, y), (x+w, y+h), [255, 0, 0], 2)\n'
"df = pd.DataFrame({'Country' : ['Spain', 'Spain', 'Germany'],\n                   'IndicatorName':['Indicator1', 'Indicator2', 'Indicator16'],\n                  'Value':[3, 4, 24]\n                  })\n\n\ndf.pivot(index = 'Country', columns='IndicatorName', values='Value').fillna(0)\n\n\nIndicatorName   Indicator1  Indicator16     Indicator2\n    Country             \n    Germany            0.0        24.0              0.0\n    Spain              3.0         0.0              4.0\n"
"new_col = df[['column1', 'column2', 'column3', 'column4']].apply(lambda x: '|'.join(x), axis=1)\n\n0    A|A|D|D\n1    B|B|D|D\n2    C|C|B|D\n3    A|D|D|A\n\ndf = pd.concat([df, df[['column1', 'column2', 'column3', 'column4']]\n       .apply(lambda x: '|'.join(x), axis=1)\n       .str\n       .get_dummies(sep='|')], axis=1)\n"
'self.fcLayers = nn.ModuleList([])\n'
'writer.add_summary(lossSummary, step)\nwriter.add_summary(epochSummary, step)\n'
"autoencoder = Sequential()\n\nautoencoder.add(Conv2D(64, (3, 3), input_shape=(200, 200, 3), padding='same'))\nautoencoder.add(Activation('relu'))\nautoencoder.add(MaxPooling2D(pool_size=(2, 2)))\n\nautoencoder.add(Conv2D(32, (3, 3), padding='same'))\nautoencoder.add(Activation('relu'))\nautoencoder.add(MaxPooling2D(pool_size=(2, 2)))\n\nautoencoder.add(Conv2D(32, (3, 3), padding='same'))\nautoencoder.add(Activation('relu'))\nautoencoder.add(MaxPooling2D(pool_size=(2, 2))) # encoded\n\nautoencoder.add(Conv2D(32, (3, 3), padding='same'))\nautoencoder.add(Activation('relu'))\nautoencoder.add(UpSampling2D((2,2)))\n\nautoencoder.add(Conv2D(32, (3, 3), padding='same'))\nautoencoder.add(Activation('relu'))\nautoencoder.add(UpSampling2D((2,2)))\n\nautoencoder.add(Conv2D(64, (3, 3), padding='same'))\nautoencoder.add(Activation('relu'))\nautoencoder.add(UpSampling2D((2,2)))\n\nautoencoder.add(Conv2D(1, (3, 3), padding='same'))\nautoencoder.add(Activation('sigmoid'))\n\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n\nautoencoder.add(Conv2D(1, (3, 3), padding='same'))\n\nautoencoder.add(Conv2D(3, (3, 3), padding='same'))\n"
'text [(192, 157), (192, 223), (530, 157), (530, 223)]\ntext [(561, 159), (561, 219), (645, 159), (645, 219)]\ntext [(74, 247), (74, 311), (465, 247), (465, 311)]\ntext [(493, 255), (493, 305), (625, 255), (625, 305)]\ntext [(85, 339), (85, 400), (496, 339), (496, 400)]\n'
'degreeArr = [3,4,5,6]\nCArr = [0.7,0.8,0.9,1]\n\nfor x in range(len(degreeArr)): #This assumes degreeArr and CArr are the same length\n  model = svm.SVC(kernel=\'poly\', degree=degreeArr[x], C=CArr[x])\n\n  print("Training Model")\n\n  #train model\n  model.fit(features_matrix, labels)\n\n  predicted_labels = model.predict(test_feature_matrix)\n\n  print("FINISHED classifying. accuracy score : ")\n\n  print (accuracy_score(test_labels, predicted_labels))\n\nfor x in range(len(degreeArr)):\n  for i in range(len(CArr)):\n    model = svm.SVC(kernel=\'poly\', degree=degreeArr[x], C=CArr[i])\n'
"preprocessor = ColumnTransformer(transformers = [\n          ('missing_ind',MissingIndicator(), ['C3','C4']),\n          ('impute_num',SimpleImputer(strategy='median'),['C3','C4','C5']),\n          ('ordinalEncoder', OrdinalEncoder(), ['C2']),\n          ('scaler', StandardScaler())\n], remainder='passthrough')\n"
"x = pd.DataFrame({'Topics':['Apples are great','Balloon is red','cars are running',\n                           'dear diary','elephant is huge','facebook is great'],\n                  'value':[-0.99,-0.98,-0.93,0.8,0.91,0.97,],\n                  'label':[0,1,0,1,1,0]})\n\ncountvector=CountVectorizer(ngram_range=(2,2))\ntraindataset=countvector.fit_transform(x['Topics'])\n\ntrain_set = pd.concat([x['value'], pd.DataFrame(traindataset)], axis=1)\n\ntrain_set.head(2)\n\n    value   0\n0   -0.99   (0, 0)\\t1\\n (0, 1)\\t1\n1   -0.98   (0, 3)\\t1\\n (0, 10)\\t1\n\nfrom scipy import sparse\n\ntrain_set = scipy.sparse.hstack([sparse.csr_matrix(x['value']).reshape(-1,1),traindataset])\n\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(train_set,x['label'])\n"
'model = tf.keras.models.Sequential([\n     tf.keras.layers.Conv2D(filters=64, \n                       kernel_size=(3,3), \n                       input_shape=X.shape[1:], \n                       activation="relu", \n                       data_format=\'channels_first\'),\n     ])\n'
"model.add(LSTM(8, batch_input_shape=(None, 100, 4), return_sequences=True))\nmodel.add(LeakyReLU())\nmodel.add(LSTM(4, return_sequences=True))\nmodel.add(LeakyReLU())\nmodel.add(LSTM(1, return_sequences=False))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.add(LSTM(4, batch_input_shape=(None, 100, 4), return_sequences=True))\nmodel.add(LeakyReLU())\nmodel.add(LSTM(8, return_sequences=True))\nmodel.add(LeakyReLU())\nmodel.add(LSTM(4, return_sequences=False))\nmodel.add(Dense(1, activation='sigmoid')) # or activation='linear' if it is a regression problem\n"
'import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX, y = np.arange(10).reshape((5, 2)), range(5)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX, y = zip(*lst)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n'
'from sklearn.utils import resample\n# Separate majority and minority classes\ndf_majority = df[df.balance==0]\ndf_minority = df[df.balance==1]\n\n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=576,    # to match majority class\n                                 random_state=123) # reproducible results\n\n# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])\n\n# Display new class counts\ndf_upsampled.balance.value_counts()\n# 1    576\n# 0    576\n# Name: balance, dtype: int64\n'
'X_features=X[:,:-1]\nX_labels=X[:,-1]\n\nmodel=MLPClassifier(args...)\n\nmodel.fit(X_features,X_labels)\n\nY=model.predict(input_vector)\n'
"from keras import backend as K\nfrom keras import losses\n\ndef custom_loss(y_true, y_pred):\n    loss = losses.mean_squared_error(y_true, y_pred)\n    return K.switch(K.flatten(K.equal(y_true, 0.)), K.zeros_like(loss), loss)\n\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(1, input_shape=(1,)))\n\nmodel.compile(loss=custom_loss, optimizer='adam')\n\nweights, bias = model.layers[0].get_weights()\n\nx = np.array([1, 2, 3])\ny = np.array([0, 0, 0])\n\nmodel.train_on_batch(x, y)\n\n# check if the parameters has not changed after training on the batch\n&gt;&gt;&gt; (weights == model.layers[0].get_weights()[0]).all()\nTrue\n\n&gt;&gt;&gt; (bias == model.layers[0].get_weights()[1]).all()\nTrue\n"
"res = pd.crosstab(df['Customer'], df['Product'])\n\nprint(res)\n\nProduct   Chair  Desk  Table\nCustomer                    \nA             1     1      1\n"
'tf.image.rgb_to_grayscale(images)\n'
'def pack_features_vector(features, labels):\n    """Pack the features into a single array."""\n    features = tf.stack(list(features.values()), axis=1)\n    return features, tf.one_hot(tf.cast(labels-1, tf.int32), depth=4)\n\nx = train_df[feature_names].values #returns a numpy array\nmin_max_scaler = preprocessing.StandardScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ntrain_df = pd.DataFrame(x_scaled)\n'
"df['datetime'] = pd.to_datetime(df['datetime'])\n\nf= lambda x: x.reindex(pd.date_range(x.index.min().floor('d'),\n                                      .index.max().floor('d')+pd.Timedelta(23, 'H'),freq='H'))\ndf1 = (df.set_index('datetime')\n         .groupby('contract')\n         .apply(f)\n         .drop('contract', axis=1)\n         .reset_index())\nprint (df1)\n"
'from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import decomposition, ensemble\nimport pandas, xgboost, numpy, string\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.svm import SVC\n\ndata = open(\'dataset.csv\').read()\nlabels = []\ntexts = []\n\nfor i ,line in enumerate(data.split("\\n")):\n    content = line.split("\\",")\n    texts.append(str(content[0]))\n    labels.append(str(content[1:]))\n\ntrainDF = pandas.DataFrame()\ntrainDF[\'text\'] = texts\ntrainDF[\'label\'] = labels\n\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF[\'text\'],trainDF[\'label\'],test_size = 0.2,random_state = 0)\nencoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)\n\nfrom sklearn.pipeline import Pipeline\ntext_clf = Pipeline([(\'vect\', CountVectorizer()), (\'tfidf\', TfidfTransformer()), (\'clf\', SVC(kernel=\'rbf\'))])\ntext_clf.fit(train_x, train_y)\n\npredicted = text_clf.predict(valid_x)\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(valid_y,predicted))\nprint(classification_report(valid_y,predicted))\nprint(accuracy_score(valid_y,predicted))\n'
'df["B+B"] = df["B"] + df["B"]\ndf["A+B"] = df["A"] + df["B"]\ndf["A+A"] = df["A"] + df["A"]\n\nfor i in range(1, 8):\n    df[f"S{i}"][df[f"S{i}"] == "001"] = df["B+B"][df[f"S{i}"] == "001"]\n    df[f"S{i}"][df[f"S{i}"] == "010"] = df["A+B"][df[f"S{i}"] == "010"]\n    df[f"S{i}"][df[f"S{i}"] == "100"] = df["A+A"][df[f"S{i}"] == "100"]\ndf.drop(["B+B", "A+B", "A+A"], axis=1)\n\n\n    SNP     A   B   S1  S2  S3  S4  S5  S6  S7\n0   rs123   T   C   CC  TT  TT  TT  CC  TT  TT\n1   rs126   G   A   GA  GG  GA  GG  GA  GG  GA\n'
'pip install imbalanced-learn==0.5.0\n'
"from sklearn.model_selection import cross_val_score\n\nmodels = {'lr':LogisticRegression(multi_class = 'multinomial', solver = 'newton-cg'),\n          'nb':MultinomialNB(alpha = 0.0001),\n          'sgd':SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,\n                      max_iter=5, tol=None)}\n\nfor name,clf in models.items():\n    pipe = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', clf)])\n    res = cross_val_score(pipe,all_post,all_target,cv=10) #res is an array of size 10\n    print(name,res.mean(),res.std())\n"
'import numpy as np\n\ndef fisher_criterion(v1, v2):\n    return abs(np.mean(v1) - np.mean(v2)) / (np.var(v1) + np.var(v2))\n\n&gt;&gt;&gt; fisher_criterion([0, 1, 2], [0, 1])\n0.54545454545454553\n'
'results.append(accuracy_score(target[testcv], [x[1] for x in pred]) )\n\nresults.append(accuracy_score(target[testcv], pred) )\n\nresults.append(accuracy_score(target[testcv], [x for x in pred]) )\n'
'import numpy as np\n\nhits = np.array([100, 500, 300, 800, 900])\n\ndef predict(hits, weights):\n    return np.average(hits[-len(weights) :], weights=weights)\n\n&gt;&gt;&gt; predict(hits, [0.2, 0.3, 0.5])\n750.0\n&gt;&gt;&gt; 900 * 0.5 + 800 * 0.3 + 300 * 0.2\n750.0\n'
'goodwords = ((countmatrix &gt; 1).mean(axis=0) &lt;= 0.8).nonzero()[0]\n\ncountmatrix = countmatrix[:, goodwords]\n'
"(testdata['X']).shape is (x1, x2, x3, x4)\n\n(testdata['X']).reshape(x1*x2*x3,x4)\n"
'output = tf.nn.softmax(input)\n'
'import numpy as np\nfrom sklearn import cross_validation\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score\n\niris = datasets.load_iris()\nskf = StratifiedKFold(n_splits=10)\nclf = svm.SVC(kernel=\'linear\', C=1)\n\nX = iris.data\ny = iris.target\nprecision_scores = []\nrecall_scores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    y_pred = clf.fit(X_train, y_train).predict(X_test)\n    precision_scores.append(precision_score(y_test, y_pred, average=\'micro\'))\n    recall_scores.append(recall_score(y_test, y_pred, average=\'micro\'))\n\nprint(precision_scores)\nprint("Recall: %0.2f (+/- %0.2f)" % (np.mean(precision_scores), np.std(precision_scores) * 2))\nprint(recall_scores)\nprint("Recall: %0.2f (+/- %0.2f)" % (np.mean(recall_scores), np.std(recall_scores) * 2))\n'
'import numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\ny_true = np.array([\'0\', \'1\', \'1\', \'0\', \'1\'])\ny_pred = np.array([\'1\', \'0\', \'1\', \'0\', \'1\'])\n\n#keep 1\'s\ny_true, y_pred = zip(*[[ytrue[i], ypred[i]] for i in range(len(ytrue)) if ytrue[i]=="1"])\n\nout = precision_recall_fscore_support(y_true, y_pred, average=\'micro\')\n'
'def biggest_k_indices(mat, k):\n    _, indices_mat =tf.nn.top_k(mat, tf.shape(mat)[3], sorted=False)\n    _, indices_k =tf.nn.top_k(mat, k, sorted=False)\n    index= []\n    eq =[]\n\n    for i in range(k):\n        index.append(tf.expand_dims(indices_k[:,:,:,i],-1))\n        eq.append(tf.equal(indices_mat,index[i]))\n\n    bool_comb =tf.logical_or(eq[0],eq[1])\n    if (k==2):\n        index.clear() \n        eq.clear()\n        return bool_comb\n\n    for i in eq[2:]:\n        bool_comb=tf.logical_or(bool_comb,i)\n\n    index.clear()\n    eq.clear()\n    return bool_comb\n'
"  self.logits_flat = tf.argmax(logits, axis=1, output_type=tf.int32)\n  labels_flat = tf.argmax(labels, axis=1, output_type=tf.int32)\n  accuracy = tf.cast(tf.equal(self.logits_flat, labels_flat), tf.float32, name='accuracy')\n\nsess.run([train_op, accuracy], feed_dict=...)\n\nsess.run([accuracy, logits], feed_dict=...)\n"
'LSTM(units=128, input_shape=X_train.shape[1:]))\n'
'&gt;&gt;&gt; import networkx as nx\n&gt;&gt;&gt; G = nx.Graph()    \n&gt;&gt;&gt; G.add_nodes_from([1, 2, 3, 4])   \n&gt;&gt;&gt; G.add_edges_from([(1, 2), (2, 3), (3, 4)])\n&gt;&gt;&gt; nx.algorithms.shortest_path(G, 1, 4)\n[1, 2, 3, 4]\n'
"encoded_train_text = t.texts_to_sequences(df_train_text)\n\npadding_train_text = np.asarray(padded_train_text, dtype='int32')\n"
'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n# in-place version\nt = torch.tensor(np.ones((100,200)))\ntorch.exp(t, out=t)\nsummed = torch.sum(t, dim=1, keepdim=True)\nt /= summed\n\n# original version\nt2 = torch.tensor(np.ones((100,200)))\nsoftmax = F.softmax(t2, 1)\n\nassert torch.allclose(t, softmax)\n\nt = t / summed  #not in-place\nt /= summed  #in-place\n'
'print(X_train.shape)\n(16016, 224, 224, 3)\n\nprint(X_test.shape)\n(16016, 1, 163)\n\n# WRONG ORDER:\nX_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# CORRECT ORDER:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n'
'_, target = torch.max(target.data, 1)\n'
'warnings.filterwarnings("ignore")\n'
'def max_depth_prediction(X_train, y_train, X_test, y_test, y):\n    max_depths = np.linspace(1, 32, 32, endpoint=True)\n    train_results = []\n    test_results = []\n    for max_depth in max_depths:\n        dt = DecisionTreeClassifier(max_depth=max_depth)\n        dt.fit(X_train, y_train)\n        train_pred = dt.predict(X_train)\n        train_pred = [int(i) for i in train_pred]\n        y_train = [int(i) for i in y_train]\n        print(y_train)\n        print(train_pred)\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train.astype(int), train_pred.astype(int))\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        # Add auc score to previous train results\n        train_results.append(roc_auc)\n        y_pred = dt.predict(X_test)\n        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n        roc_auc = auc(false_positive_rate, true_positive_rate)\n        # Add auc score to previous test results\n        test_results.append(roc_auc)\n'
'# Setting the plot window\nfigsize = plt.subplots(figsize = (12, 9))\n\nfeatues_mask = tree.feature_importances_&gt; 0.005\n\n# Specifying the contents of the plot\nplt.barh(range(sum(featues_mask)), tree.feature_importances_[featues_mask], align = \'center\')\nplt.yticks(pd.np.arange(sum(featues_mask)), X_train.columns[featues_mask])\nplt.xlabel("The degree of importance")\nplt.ylabel("Feature")\n'
"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\n\niris = load_iris()\nknn_parameters = [{\n    'n_neighbors': [1,3,5,7, 9, 11],\n    'leaf_size': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'weights': ['uniform', 'distance']}]\n\nknn_classifier = KNeighborsClassifier()\nclf = GridSearchCV(estimator = knn_classifier, param_grid = knn_parameters, scoring = 'accuracy', n_jobs=-1, cv=3)\nclf.fit(iris.data, iris.target)\n\nclf.best_estimator_\n# result:\nKNeighborsClassifier(algorithm='auto', leaf_size=5, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                     weights='uniform')\n\nclf.best_estimator_.n_jobs = -1\nclf.best_estimator_\n# result\nKNeighborsClassifier(algorithm='auto', leaf_size=5, metric='minkowski',\n                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n                     weights='uniform')\n\ncv_results = pd.DataFrame.from_dict(clf.cv_results_)\n\ncv_results['rank_test_score']\n# result:\n0      481\n1      481\n2      145\n3      145\n4        1\n      ... \n571      1\n572    145\n573    145\n574    433\n575      1\nName: rank_test_score, Length: 576, dtype: int32\n\ncv_results.iloc[4]\n# result:\nmean_fit_time                                              0.000669559\nstd_fit_time                                               1.55811e-05\nmean_score_time                                             0.00474652\nstd_score_time                                             0.000488042\nparam_algorithm                                                   auto\nparam_leaf_size                                                      5\nparam_n_neighbors                                                    5\nparam_weights                                                  uniform\nparams               {'algorithm': 'auto', 'leaf_size': 5, 'n_neigh...\nsplit0_test_score                                                 0.98\nsplit1_test_score                                                 0.98\nsplit2_test_score                                                 0.98\nmean_test_score                                                   0.98\nstd_test_score                                                       0\nrank_test_score                                                      1\nName: 4, dtype: object\n\ncv_results.loc[cv_results['rank_test_score']==1]\n"
'loss=tf.keras.losses.SparseCategoricalCrossentropy() \n\n"sparse_categorical_accuracy" not "accuracy"\n'
"def customized_weights(distances: nparray)-&gt;nparray:\n\n    weights: nparray = nparray(numpy.full(distances.shape, 0), dtype='float')\n# create a new array 'weights' with the same dimension of  'distances' and fill \n# the array with 0 element.\n    for i in range(distances.shape[0]): # for each prediction:\n        if distances[i, 0] &gt;= 100: # if the smaller distance is greather than 100, \n                                   # consider the nearest neighbor's weight as 1 \n                                   # and the neighbor weights will stay zero\n            weights[i, 0] = 1\n                                   # than continue to the next prediction\n            continue\n\n        for j in range(distances.shape[1]): # aply the weight function for each distance\n\n            if (distances[i, j] &gt;= 100):\n                continue\n\n            weights[i, j] = 1 - distances[i, j]/100\n\n    return weights\n"
"import pandas as pd\nfrom collections import defaultdict\n\ndata = {'msg':['free home service','get free data','we live in a home','i drive the car'],'spam':[1,1,0,0]}\ndf = pd.DataFrame(data=data)\nprint(df)\n\ndef word_counter(sentence_list):\n    word_count = defaultdict(lambda:0)\n    for sentence in sentence_list:\n        for word in sentence:\n            word_count[word] += 1\n    return word_count\n\nspam = [x.split() for x in set(df['msg'][df['spam']==1])]\nspam_total = sum([len(sentence) for sentence in spam])\nspam = word_counter(spam)\n\nham = [x.split() for x in set(df['msg'][df['spam']==0])]\nham_total = sum([len(sentence) for sentence in ham])\nham = word_counter(ham)\n\n# Prior\nspam_prior = len(df['spam'][df['spam']==1])/len(df)\nham_prior = len(df['spam'][df['spam']==0])/len(df)\n\nnew_data = [&quot;get free home service&quot;,&quot;i live in car&quot;]\nprint(&quot;\\n\\tSpamminess&quot;)\nfor msg in new_data:\n    data = msg.split()\n    \n    # Likelihood\n    spam_likelihood = 1\n    ham_likelihood = 1\n    for word in data:\n        spam_likelihood *= (spam[word] + 1) / (spam_total + 1)\n        ham_likelihood *= (ham[word] + 1) / (ham_total + 1)\n    \n    # marginal likelihood\n    marginal = (spam_likelihood * spam_prior) + (ham_likelihood * ham_prior)\n    \n    spam_posterior = (spam_likelihood * spam_prior) / marginal\n    print(msg,round(spam_posterior*100,2))\n\n    Spamminess\nget free home service 98.04\ni live in car 20.65\n"
"import json\nimport nltk\n\n## Need this to download Punkt Sentence Tokenizer\n## Use it once, then comment the following line\nnltk.download('punkt')\n\nwords=[]\nclasses = []\ndocuments = []\nignore_words = ['?', '!']\ndata_file = open('intents.json').read()\nintents = json.loads(data_file)\n\nfor intent in intents['intents']:\n    for pattern in intent['patterns']:\n        #tokenize each word\n        w = nltk.word_tokenize(pattern)\n        words.extend(w)\n        #add documents in the corpus\n        documents.append((w, intent['tag']))\n        # add to our classes list\n        if intent['tag'] not in classes:\n            classes.append(intent['tag'])\n\nprint(words)\nprint(classes)\nprint(documents)\n"
"# imports\nimport tensorflow as tf\nimport pandas as pd \nimport numpy as np\n\n# genration of dummy data\nx1 = np.random.randint(100, size =(5, 5, 5), dtype = np.int16)\nx2 = np.random.randint(100, size =(5, 4, 4), dtype = np.int16)\ny1 = np.random.randint(2, size =(5,), dtype = np.int16)\ny2 = np.random.randint(2, size =(5,), dtype = np.int16)\n\n# creation of model\ndef create_model3():\n    input1 = tf.keras.Input(shape=(5,5,), name = 'I1')\n    input2 = tf.keras.Input(shape=(4,4,), name = 'I2')\n    \n    hidden1 = tf.keras.layers.LSTM(units = 4)(input1)\n    hidden2 = tf.keras.layers.LSTM(units = 4)(input2)\n    merge = tf.keras.layers.concatenate([hidden1, hidden2])\n    hidden3 = tf.keras.layers.Dense(units = 3, activation='relu')(merge)\n    output1 = tf.keras.layers.Dense(units = 2, activation='softmax', name ='O1')(hidden3)\n    output2 = tf.keras.layers.Dense(units = 2, activation='softmax', name = 'O2')(hidden3)\n    \n    model = tf.keras.models.Model(inputs = [input1,input2], outputs = [output1,output2])\n    \n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model\n\n\n\nmodel = create_model3()\ntf.keras.utils.plot_model(model, 'my_first_model.png', show_shapes=True)\n\n# training the model\nhistory = model.fit(\n    x = {'I1':x1, 'I2':x2}, \n    y = {'O1':y1, 'O2': y2},\n    batch_size = 32,\n    epochs = 10,\n    verbose = 1,\n    callbacks = None,\n#     validation_data = [(val_data,new_val_data),(val_labels, new_val_labels)]\n)\n"
'def predict(data: Data = Depends()):\n'
"single_test_sample = pd.DataFrame({'Age':[30], 'EstimatedSalary':[50000]}).iloc[:,[0,1]].values\nsingle_test_sample = sc.transform(single_test_sample)\nsingle_test_prediction = classifier.predict(single_test_sample)\n"
"[0, 1, 2, ..., 307510, 0, 1, 2, 3, ... 48743]\n\ndata['isTrain'] = trainlen # I am doing an assignment with a Series object that contains also indexes!\n\n        inst  isTrain\n0          0        1\n1          1        1\n2          2        1\n3          3        1\n4          4        1\n...      ...      ...\n48739  48739        1\n48740  48740        1\n48741  48741        1\n48742  48742        1\n48743  48743        1\n\ndata.index = [i for i in range(len(data))] # here I am changing\\resetting the indexes\ndata['isTrain'] = trainlen\n\nprint(trainlen.sum())\nprint(data.isTrain.sum())\n\n         inst  isTrain\n0           0        1\n1           1        1\n2           2        1\n3           3        1\n4           4        1\n...       ...      ...\n356250  48739        0\n356251  48740        0\n356252  48741        0\n356253  48742        0\n356254  48743        0\n"
"minmax_transformer = Pipeline(steps=[\n        ('minmax', MinMaxScaler())])\n\n# perform normalization on the dataset, avoiding ordinal columns\npreprocessor = ColumnTransformer(\n        remainder='passthrough', \n        transformers=[\n            ('mm', minmax_transformer , columns_to_normalize)\n        ])\n\n# fit and transform model on data\ndf_norm_values = preprocessor.fit_transform(df)\n\n# convert the array back to a dataframe\ndf_norm = DataFrame(df_norm_values)\n\n# set columns' names\npassthrough_cols = list(df.columns)\nfor col in columns_to_normalize: # remove cols that will be scaled\n    passthrough_cols.remove(col)\n\ncolumn_names = columns_to_normalize\ncolumn_names.extend(passthrough_cols) # stack columns names\n\ndf_norm.columns = column_names\n# normalized input variable's summarry\ndf_norm.describe() \n"
"data['Time'] = pd.to_datetime(data['Time'], format='%I:%M:%S %p')\n"
'plt.scatter(X[:, 0], X[:,1], c=y)\nplt.show()\n'
'import tensorflow as tf \nA = tf.keras.models.Sequential(\n    [\n        tf.keras.Input((10,)),\n        tf.keras.layers.Dense(5, activation=&quot;tanh&quot;)\n    ],\n    name=&quot;A&quot;\n)\nB = tf.keras.models.Sequential(\n    [\n        tf.keras.Input((5,)),\n        tf.keras.layers.Dense(10, activation=&quot;tanh&quot;)\n    ],\n    name=&quot;B&quot;\n)\n\nmerged_input = tf.keras.Input((10,))\nx = A(merged_input)\nmerged_output = B(x)\nmerged_model = tf.keras.Model(inputs=merged_input, outputs=merged_output, name=&quot;merged_AB&quot;)\n\n&gt;&gt;&gt; merged_model.summary()\nModel: &quot;merged_AB&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 10)]              0         \n_________________________________________________________________\nA (Sequential)               (None, 5)                 55        \n_________________________________________________________________\nB (Sequential)               (None, 10)                60        \n=================================================================\nTotal params: 115\nTrainable params: 115\nNon-trainable params: 0\n_________________________________________________________________\n'
'prediction = model(img_reshape)\n'
'onehotencoder = OneHotEncoder(categorical_features = [3])\n\nonehotencoder = OneHotEncoder()\n'
'class Parallel(torch.nn.Module):\n    def __init__(self, *modules: torch.nn.Module):\n        super().__init__()\n        self.modules = modules\n\n    def forward(self, inputs):\n        return [module(inputs) for module in self.modules]\n\nself._mean_logvar_layers = Parallel(\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding=0),\n    nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1, padding=0),\n)\n\nmean, logvar = self._mean_logvar_layers(x)\n\nclass Split(torch.nn.Module):\n    def __init__(self, module, parts: int, dim=1):\n        super().__init__()\n        self.parts\n        self.dim = dim\n        self.module = module\n\n    def forward(self, inputs):\n        output = self.module(inputs)\n        chunk_size = output.shape[self.dim] // self.parts\n        return torch.split(output, chunk_size, dim=self.dim)\n\nself._mean_logvar_layers = Split(\n    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=1, padding=0),\n    parts=2,\n)\n\nmean, logvar = self._mean_logvar_layers(x)\n\ndef forward(self, inp):\n    for module in self:\n        inp = module(inp)\n    return inp\n'
"def find_best_threshold(threshold, fpr, tpr):\n    t = threshold[np.argmax(tpr * (1-fpr))]\n    ### TPR * TNR ---&gt; We are trying to maximize TNR and TPR\n    print(&quot;the maximum value of tpr*(1-fpr)&quot;, max(tpr*(1-fpr)), &quot;for threshold&quot;, np.round(t,3))\n    return t\n\ndef predict_with_best_thresh(prob,t):\n    pred=[1 if i&gt;=t else 0 for i in prob  ]\n    return pred\n\n### https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\ndef conf_matrix_plot(cf_matrix,title):\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [&quot;{0:0.0f}&quot;.format(value) for value in cf_matrix.flatten()]\n    group_percentages = [&quot;{0:.2%}&quot;.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n    labels = [f&quot;{v1}\\n{v2}\\n{v3}&quot; for v1, v2, vQ3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    #sns.set(font_scale=1.5) \n    sns.heatmap(cf_matrix, annot=labels, fmt='',cmap='coolwarm').set_title(title + ' Confusion Matrix for TFIDF')\n    plt.xlabel('Actual')\n    plt.ylabel('Predicted')\n\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nbest_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\ncf_matrix_train = confusion_matrix(y_train, predict_with_best_thresh(y_train_pred[:,1], best_t))\ncf_matrix_test = confusion_matrix(y_test, predict_with_best_thresh(y_test_pred[:,1], best_t))\n\nconf_matrix_plot(cf_matrix_train,'Train')\n\n\n\n"
"key = 'n_estimators'\nestimator = estimator.set_params(**{key: 100})\n"
'"User-ID";"ISBN";"Book-Rating"\n"11676";" 9022906116";"7"\n"11676";"\\"0432534220\\"";"6"\n"11676";"\\"2842053052\\"";"7"\n"11676";"0 7336 1053 6";"0"\n"11676";"0=965044153";"7"\n"11676";"0000000000";"9"\n"11676";"00000000000";"8"\n"146859";"01402.9182(PB";"7"\n"158509";"0672=630155(P";"0"\n"194500";"(THEWINDMILLP";"0"\n\ndf.ISBN = df.ISBN.str.replace(r\'[^\\w\\d]+\', \'\')\n\navg_ratings = df.groupby(\'ISBN\')[\'Book-Rating\'].mean().round().astype(np.int8)\n\ndf.loc[df[\'Book-Rating\'] == 0, \'Book-Rating\'] = df.loc[df[\'Book-Rating\'] == 0, \'ISBN\'].map(avg_ratings)\n'
'from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=K)\nmodel.fit(X_train)\nlabel = model.predict(X_test)\n'
'import tensorflow as tf\nsess = tf.InteractiveSession()\n\n# define placeholder for input, None as first argument means tensor can be any length\nx_ = tf.placeholder(tf.float32, shape=[4,2], name="x-input")\ny_ = tf.placeholder(tf.float32, shape=[4,1], name="y-input")\n\n# Configure weights and layers\nW = tf.Variable(tf.random_uniform([2, 10], 0.001, .01))\nb = tf.Variable(tf.zeros([10]))\nhidden  = tf.nn.relu(tf.matmul(x_,W) + b) # first layer.\n\nW2 = tf.Variable(tf.random_uniform([10,1], -1, 1))\nb2 = tf.Variable(tf.zeros([1]))\nhidden2 = tf.matmul(hidden, W2) + b2\ny = tf.nn.sigmoid(hidden2)\n\n# Training function + data\ncost = tf.reduce_mean(( (y_ * tf.log(y)) +\n((1 - y_) * tf.log(1.0 - y)) ) * -1)\ntrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n\nXOR_X = [[0,0],[0,1],[1,0],[1,1]]\nXOR_Y = [[0],[1],[1],[0]]\n\ninit = tf.global_variables_initializer()\nsess.run(init)\n# Train on the input data\nfor i in range(100000):\n    sess.run(train_step, feed_dict={x_: XOR_X, y_: XOR_Y})\n    if i % 2000 == 0:\n        print (\'W1\', sess.run(W))\n        print(\'Output \', sess.run(y, feed_dict={x_: XOR_X, y_: XOR_Y}))\n'
"def max_pool_2x2(X):\n        return tf.nn.max_pool(X,[1,2,2,1],[1,2,2,1],padding='SAME')\n"
'sc = StandardScaler().fit(inputX)\ninputX = sc.transform(inputX)\ninputX_test = sc.transform(inputX_test)\n\nfor batch_X, batch_Y in get_batch(input_X, input_Y, batch_size):\n   _, c = sess.run([optimizer, cost], feed_dict={x: batch_X,\n                                              y: batch_Y})\n\ninputX = df_train.drop(\'SalePrice\', 1).as_matrix()\ninputX = inputX.astype(int)\nsc = StandardScaler().fit(inputX)\ninputX = sc.transform(inputX)\n\ninputY = df_train.loc[:, [\'SalePrice\']].as_matrix()\ninputY = inputY.astype(int)\nsc1 = StandardScaler().fit(inputY)\ninputY = sc1.transform(inputY)\n\ninputX_test = df_test.drop(\'SalePrice\', 1).as_matrix()\ninputX_test = inputX_test.astype(int)\ninputX_test = sc.transform(inputX_test)\n\ninputY_test = df_test.loc[:, [\'SalePrice\']].as_matrix()\ninputY_test = inputY_test.astype(int)\ninputY_test = sc1.transform(inputY_test)\n\nlearning_rate = 0.01\ntraining_epochs = 1000\nbatch_size = 50\ndisplay_step = 50\n\nn_samples = inputX.shape[0]\n\nx = tf.placeholder(tf.float32, [None, 5])\ny = tf.placeholder(tf.float32, [None, 1])\n\ndef get_batch(inputX, inputY, batch_size):\n  duration = len(inputX)\n  for i in range(0,duration//batch_size):\n    idx = i*batch_size\n    yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n\n\ndef add_layer(inputs, in_size, out_size, activation_function=None):\n  Weights = tf.Variable(tf.random_normal([in_size, out_size], stddev=0.005))\n  biases = tf.Variable(tf.zeros([1, out_size]))\n  Wx_plus_b = tf.matmul(inputs, Weights) + biases\n  if activation_function is None:\n    output = Wx_plus_b\n  else:\n    output = activation_function(Wx_plus_b)\n  return output\n\n\nl1 = add_layer(x, 5, 3, activation_function=tf.nn.relu)\n\npred = add_layer(l1, 3, 1)\n\n# Mean squared error\ncost = tf.reduce_mean(tf.pow(tf.subtract(pred, y), 2))\n# Gradient descent\noptimizer =   tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n\n# Launch the graph\nwith tf.Session() as sess:\n sess.run(init)\n\n # Training cycle\n for epoch in range(training_epochs):\n    avg_cost = 0.\n    total_batch = batch_size\n    # Loop over all batches\n    #for i in range(total_batch):\n    for batch_x, batch_y in get_batch(inputX, inputY, batch_size):\n        # Run optimization op (backprop) and cost op (to get loss value)\n        _, c, _l1, _pred = sess.run([optimizer, cost, l1, pred], feed_dict={x: batch_x, y: batch_y})\n        # Compute average loss\n        avg_cost += c / total_batch\n    # Display logs per epoch step\n    if epoch % display_step == 0:\n        print("Epoch:", \'%04d\' % (epoch+1), "cost=", "{:.9f} ".format(avg_cost))\n        #print(_l1, _pred)\nprint("Optimization Finished!")\n'
"def load_X(X_signals_paths):\n    X_signals = []\n\n    for signal_type_path in X_signals_paths:\n        file = open(signal_type_path, 'r')\n        # ^ the error comes where you have file = pandas.read_csv(...)\n\n        # Read dataset from disk, dealing with text files' syntax\n        X_signals.append(\n            [np.array(serie, dtype=np.float32) for serie in [\n                row.replace('  ', ' ').strip().split(' ') for row in file\n            ]]\n        )\n        file.close()\n\n    for signal_type_path in X_signals_paths:\n        with open(signal_type_path, 'r') as csvfile:\n            reader = csv.reader(csvfile)\n            X_signals.append([np.array(row[0:2], dtype=np.float32) for row in reader])\n"
"from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFECV\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline        \nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nfeat_sel = SelectKBest(mutual_info_classif, k=200) \n\nclf = MultinomialNB()\npipe = Pipeline([('mutual_info',feat_sel), ('naive_bayes',clf)])\n\nscores = cross_val_score(pipe, X_scaled, y, cv =10, scoring = 'accuracy')\nprint(np.mean(scores))\n"
'def train_classifier(...):\n    ...\n    init = tf.global_variables_initializer()\n    init_l = tf.local_variables_initializer()\n    with tf.Session(config=config) as sess, tf.device(device):\n        sess.run(init)\n        sess.run(init_l)\n'
'cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n       labels=prediction,logits=y)\n)    \n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n       labels=y,logits=prediction)\n)\n\n_, c = sess.run([optimizer,cost],feed_dict = {x:epoch_x,y:epoch_y})\n'
'y_pred = classifier.best_estimator_.predict_proba(X)\n\nprint(log_loss(y_train,y_pred)) \nprint(classifier.best_score_)\n\n0.165794760809\n-0.185370083771\n\nlog_loss_scorer = make_scorer(log_loss, greater_is_better=False,\n                              needs_proba=True)\n'
'input_layer = tf.reshape(input_feat, [1,-1])\nfirst_hidden_layer = tf.layers.dense(input_layer, 4, activation=tf.nn.relu)\nsecond_hidden_layer = tf.layers.dense(first_hidden_layer, 5, activation=tf.nn.relu)\noutput_layer = tf.layers.dense(second_hidden_layer, 6)\n\ninput_layer = tf.identity(input_feat)\nfirst_hidden_layer = tf.layers.dense(input_layer, 4, activation=tf.nn.relu)\nsecond_hidden_layer = tf.layers.dense(first_hidden_layer, 5, activation=tf.nn.relu)\noutput_layer = tf.layers.dense(second_hidden_layer, 6)\n'
'T = preprocessing.Normalizer().fit(X)\n\nT = preprocessing.Normalizer().fit_transform(X)\n'
"import numpy as np\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\n\nn_samples = 1000\nn_features = 50\nX, y = make_regression(n_samples=n_samples, n_features=n_features)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\npipe = Pipeline([('scaler', StandardScaler()),\n                 ('sgd', SGDRegressor())])\n\nparameters = {'sgd__loss': ['squared_loss','huber'],\n              'sgd__n_iter': [np.ceil(10**6 / n_samples)],\n              'sgd__alpha': 10.0**np.arange(1,7)} \n\ng_search = RandomizedSearchCV(pipe, param_distributions=parameters, random_state=2)\n\ng_search.fit(X_train, y_train)   \n"
'def f(C, I, U, N):\n    loss = 0\n    for i in range(N):\n         sum_ = 0\n         for k in range(N):\n              sum_ += I[i,k] * tf.exp(d(u[i]-u[k])\n         loss += C[i]*tf.log(sum)\n    return loss    \nloss = f(C,I,U, batch_size)\n'
'def think(self, inputs):\n    return self.__sigmoid(dot(inputs, self.synaptic_weights) + 1)\n\nclass NeuralNetwork():\ndef __init__(self):\n    random.seed(1)\n    self.synaptic_weights = 2 * random.random((3, 1)) - 1\n    # Now you have to initialize bias\n    # You can either keep it 0 or and random number\n    self.bias = random.randn(1,1)\n    # Here as there is only 1 neuron, dimension becomes (1, 1)\n'
"decoder_inputs = model.input[1] # (Input(shape=(None,)))\n# pass the inputs to the embedding layer\ndecoder_embedding = model.get_layer(name='dec_embedding')(decoder_inputs) \n\n# ...\n\ndecoder_lstm = model.get_layer(name='dec_lstm') # dec_lstm\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, ...)\n\ndecoder_lstm, _, _ = LSTM(latent_dim, return_sequences=True, return_state=True, name='dec_lstm')(decoder_embedding, initial_state=encoder_states)\n"
"y_train_lstm = keras.utils.to_categorical(y_train_lstm)\n\nmodel.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\nX_test_lstm = sequence.pad_sequences(X_test_lstm, maxlen=longest_string)\n"
'model = load_model("classification.h5")\n\nlast_conv_layer_output = model.layers[last_conv_index].output\nconv = Conv2D(...)(last_conv_layer_output)\nconv = Conv2D(...)(conv)\noutput = Conv2D(...)(conv)\n\nnew_model = Model(model.inputs, output)\n\n# compile the new model and save it ...\n'
'import os\nimport subprocess\n\nenv = os.environ.copy()\nenv["PYTHONHASHSEED"] = "0"\n\nsubprocess.check_call([\'python\', \'main.py\'], env=env)\n'
'# returns an action based on how good the next state is\nget_action(state, epsilon):\n    actions = []\n    for each action possible in state:\n         game.apply(action)\n         val = nnet.predict(game.get_state())\n         action.value = val\n         actions.append(action)\n\n    if random() &lt; epsilon\n        return randomChoice(actions)\n    else\n        return action with_max_value from actions\n'
'# using np.reshape\nmy_data = my_data.reshape(my_data.shape + (1,))\n\n# using np.expand_dims\nmy_data = np.expand_dims(my_data, axis=-1)\n'
'x = Input(shape=(256,256,1))\ninp = x\n\n# the rest is the same...\n\nmodel = tf.keras.models.Model(inputs=inp, outputs=out) # pass `inp` as inputs\n'
'from sklearn.linear_model import LinearRegression\n\nX = np.array([[1], [2], [3]])\ny = np.array([1, 2, 3])\n\nlg = LinearRegression()\nlg.fit(X, y)\nlg.coef_ # 1\nlg.intercept_ # ~ 0\n'
'indices = np.arange(163)\ndepth = 163\ny = tf.one_hot(indices,depth)\nresult = sess.run(y)\n'
'prob_invert = 1 - prob\n'
"import pandas as pd\ndf = pd.DataFrame({'col1': [1,2,3,4]})\ndf1 = pd.DataFrame({'col2': [11,12,13,14]})\ndf2 = pd.DataFrame({'col3': [111,112,113,114]})\n\nd = {'df':df, 'df1':df1, 'df2':df2}\n\n\nfirst = True\nfor key, value in d.items():\n    if first:\n        n = value\n        first = False\n    else:\n        n = n.merge(value, left_index=True, right_index=True)\n\nn.head()\n\n   col1  col2  col3\n0     1    11   111\n1     2    12   112\n2     3    13   113\n3     4    14   114\n"
'native = graph.get_tensor_by_name("input:0")\n\noutputs = graph.get_tensor_by_name("output:0")\n'
'import random\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n##Based on your data\ninitial_room=["Standard single sea view","Deluxe twin Single","Suite Superior room ocean view","Superior Double twin","Deluxe Double room"]\n\n\n##Based on your data created 100 data points\n##Its repeating\nroom_class=[initial_room[random.randint(0,len(initial_room)-1)] for i in range(100)]\n\n##Based on room_cluster\ninitial_cluster=["Standard","Deluxe","Suite","Superior"]\n\n##Find intersection between room_class and room_cluster the matching word is the Y_Label\nroom_cluster=[\'\'.join(list(set(each_room.split()).intersection(set(initial_cluster)))[0]) for each_room in room_class]\n\n\n##Helps to embed \nembedding={}\nindex=0\n\n\n##For each unique word in the total room_class assign a unique number.\nfor each_room in room_class:\n    for each_word in each_room.split():\n        if each_word not in embedding:\n            embedding[each_word]=index\n            index+=1\n\n##Find max_len of the room name\nmax_len=max([len(i.split()) for i in room_class])\n\n##Needed for embedding the matrix\nembedded_rooms=[]\n\n\n##For each room in room_class\nfor each_room in room_class:\n    embedded_room=[]\n    for each_word in each_room.split():\n        ##Each word assign that unique number\n        embedded_room.append(embedding[each_word])\n\n    #Get the length of the row\n    room_len=len(embedded_room)\n\n    ##If it is length max_len pad it with -1\n    ##Single for embedding I have already used 0 so I cant use it\n    while(room_len&lt;max_len):\n        embedded_room.append(-1)\n        room_len+=1\n    ##Append it to embedded rooms\n    embedded_rooms.append(embedded_room)\n\nY=[]\n\n##Embed Y based on same technique\nfor each_cluster in room_cluster:\n    Y.append(embedding[each_cluster])\n\n\nX=np.array(embedded_rooms)\n\n\n##Apply KNN\nclassifier = KNeighborsClassifier(n_neighbors=3)\nclassifier.fit(X,Y)\n\n##Data for testing goes within this list\ntest=["Single Standard"]\ntest_label=["Standard"]\n\n\nembed_tests=[]\n##Convert the test to embedding \n#Use the same embedding\nfor each_test in test:\n    embed_test=[]\n    for each_word in each_test.split():\n        embed_test.append(embedding[each_word])\n    ##Again Padding the data    \n    n=len(embed_test)\n    while(n&lt;max_len):\n        embed_test.append(-1)\n        n+=1\n    embed_tests.append(embed_test)  \n\n#Predict the X_test\nX_test=np.array(embed_tests)\npredictions = classifier.predict(X_test)\n\n##Convert class_labels to encoding\nembed_test_label=[]\nfor each_class in test_label:\n    embed_test_label.append(embedding[each_class])\n\n##Print out the accuracy\nprint(accuracy_score(embed_test_label,predictions))\n'
'from keras.layers import Lambda\n\n# the model definition goes here...\n\nout1 = Lambda(preprocess1)(final_out) # you can also implement this using existing layers\nout2 = Lambda(preprocess2)(final_out)\n\nmodel = Model(inp, [out1, out2])\n\nmodel.compile(loss=[rmse_loss, ce_loss], loss_weights=[0.2, 0.8], ...)\n'
"df = pd.DataFrame(np.random.randn(100,9))\n\nx_train = df.iloc[:,1:8].values\ny_train = df.iloc[:,8].values\n\n# No:of sample, times_steps, input_size (1 in your case)\nx_train = x_train.reshape(x_train.shape[0],x_train.shape[1], 1)\n\nmodel = Sequential()\n# 16 outputs of first LSTM per time step\nmodel.add(LSTM(16, input_dim=1, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(8, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', \n              optimizer='rmsprop', \n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=15, batch_size=32)\n"
'output_1 = [0.2, 0.7, 0.1] \noutput_2 = [0.3, 0.6, 0.1]\n\noutputs = np.array([output_1, output_2])\n\noutputs[np.where(outputs == np.max(outputs))[0],:]\n'
'best_fit = []\nfor i in xArr:\n    best_fit.append(slope*i[0]+y_intercept)\n\n...\n[1.0, 0.24242424242424243]\n[1.0, 0.25252525252525254]\n[1.0, 0.26262626262626265]\n[1.0, 0.27272727272727276]\n[1.0, 0.2828282828282829]\n[1.0, 0.29292929292929293]\n[1.0, 0.30303030303030304]\n[1.0, 0.31313131313131315]\n[1.0, 0.32323232323232326]\n[1.0, 0.33333333333333337]\n[1.0, 0.3434343434343435]\n[1.0, 0.3535353535353536]\n...\n\nbest_fit = []\nfor i in xArr:\n    best_fit.append(slope*i[1]+y_intercept)\n'
'python detectron/tools/infer_simple.py --wts /home/arij_sediri/ModelResNeXt152.pkl --output-dir /tmp/detectron-tablebank --image-ext jpg --cfg /home/arij_sediri/config_X152.yaml /home/arij_sediri/TableBank/TableBank/data/Sampled_Detection_data/Latex/images\n'
"#IMPORT AND SPLIT\n\nfrom cam_img_split import cam_img_split\nfrom cam_pad import cam_pad\nfrom cam_img_bow import cam_img_bow\nimport cv2\nimport numpy as np\n\nimg_tr_in=cv2.imread('frame 1.png',0)[0:767,0:767]/255\nimg_tr_out=cv2.imread('frame 1 so far bnw 2.png',0)[0:767,0:767]/255\nimg_tr_out=(cam_img_bow(img_tr_out,0.5)).astype(np.uint8)\n\nseg_shape=[15,15] #needs to be odd and equal to each other\n\npl_max=img_tr_in.shape[0:2]\npl=np.array([0.15*pl_max[0],pl_max[1]]).astype(np.uint32)\n\npad_in=int(np.floor(seg_shape[0]/2))\n\nimg_tr_in_pad=cam_pad(img_tr_in,pad_in)\n\ntr_in=np.zeros([pl[0],pl[1],seg_shape[0],seg_shape[1]])\n\nfor n1 in range(0,pl[0]):\n        for n2 in range(0,pl[1]):\n                tr_in[n1,n2]=img_tr_in_pad[n1:n1+seg_shape[0],n2:n2+seg_shape[1]]\n\n\n##################### NEURAL NETWORK\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import adam\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n\npad=4\n\ninput_shape=(seg_shape[0]+2*pad,seg_shape[1]+2*pad,1)\noutput_shape=(1,1,1)\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3),input_shape=input_shape, activation='relu'))\nmodel.add(Conv2D(64,(3, 3), activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(units=2, activation='softmax'))\n\nmodel.compile(optimizer=adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n\n##################### FITTING THE MODEL\n\ntr_in_flat=tr_in.reshape([pl[0]*pl[1],seg_shape[0],seg_shape[1],1])\ntr_out_flat=img_tr_out.reshape([pl_max[0]*pl_max[1]])\n\ntr_in_flat_pad=np.zeros(tr_in_flat.shape+np.array([0,2*pad,2*pad,0]))\n\nfor n3 in range(0,tr_in_flat.shape[0]):\n        tr_in_flat_pad[n3,:,:,0]=cam_pad(tr_in_flat[n3,:,:,0], pad)\n\nmodel.fit(tr_in_flat_pad, to_categorical(tr_out_flat[0:pl[0]*pl[1]]), epochs=5, batch_size=int(16*pl[0]),shuffle=True)\n\n\n##################### PLOTTING PREDICTIONS\n\ntr_in_full=np.zeros([pl_max[0],pl_max[1],seg_shape[0]+2*pad,seg_shape[1]+2*pad])\n\nfor n1 in range(0,pl_max[0]):\n        for n2 in range(0,pl_max[1]):\n                tr_in_full[n1,n2]=cam_pad(img_tr_in_pad[n1:n1+seg_shape[0],n2:n2+seg_shape[1]],pad)\n\n\ntr_in_full_flat=tr_in_full.reshape([pl_max[0]*pl_max[1],seg_shape[0]+2*pad,seg_shape[1]+2*pad,1])\n\npred = model.predict(tr_in_full_flat)\n\npred_img=np.zeros(pred.shape[0])\n\nfor n1 in range(0,pred.shape[0]):\n        pred_img[n1]=round(pred[n1,0])\n\npred_img_out=(pred_img.reshape([pl_max[0],pl_max[1]]))\n\nplt.subplot(1,2,1)\nplt.imshow(pred_img_out)\n\nplt.subplot(1,2,2)\nplt.imshow(img_tr_in)\n\nplt.show()\n"
'(0, 1057) 1\n(0, 4920) 1\n(0, 5563) 1\n'
'from sklearn import metrics\ndef generateAccVectors(y_truth, percentage):\n  y = list(y_truth)\n  for i in range(len(y)):\n    if metrics.accuracy_score(y_truth, y)&lt;= percentage:\n      break\n    if(y[i] ==1):\n      y[i]= 0\n    else:\n      y[i] =1\n  return y\n\nfrom sklearn import metrics\ny = [1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1 ] \ny_1= generateAccVectors(y, 0.85)\nprint(metrics.accuracy_score(y, y_1))\n#output: 0.8333333333333334\n'
'def generator_multiple(generator, dataframe, batch_size, img_height, img_width):\n\n    genX1 = generator.flow_from_dataframe(\n        dataframe=dataframe, x_col="reconstruction", y_col=None, class_mode=None,\n        batch_size=batch_size, shuffle=False, target_size=(img_height, img_width), color_mode=\'grayscale\')\n\n    genX2 = generator.flow_from_dataframe(\n        dataframe=dataframe, x_col="observation", y_col=None, class_mode=None,\n        batch_size=batch_size, shuffle=False, target_size=(img_height, img_width), color_mode=\'grayscale\')\n\n    while True:\n        X1i = genX1.next()\n        X2i = genX2.next()\n        yield [X1i, X2i]\n\n'
'Support multilabel:\n\nsklearn.tree.DecisionTreeClassifier\nsklearn.tree.ExtraTreeClassifier\nsklearn.ensemble.ExtraTreesClassifier\nsklearn.neighbors.KNeighborsClassifier\nsklearn.neural_network.MLPClassifier\nsklearn.neighbors.RadiusNeighborsClassifier\nsklearn.ensemble.RandomForestClassifier\nsklearn.linear_model.RidgeClassifierCV\n'
"model.add(Dense(1, activation='sigmoid'))\n\noptimizer=keras.optimizers.Adam()\n"
"def tripler(s):\n    triples = 0\n    s = [ss.strip() for ss in s.split()][::-1]\n\n    for i in range(len(s) - 1):\n        if s[i - triples + 1] == 'Triple':\n            s[i - triples] *= 3\n\n            del s[i - triples + 1]\n            triples += 1\n\n    return ' '.join(s[::-1])\n\nrepeat_keywords = {'Double':2, 'Triple':3}\n\ndef repeater(s):\n    repeats = 0\n    s = [ss.strip() for ss in s.split()][::-1]\n\n    for i in range(len(s) - 1):\n        if s[i - repeats + 1] in repeat_keywords:\n            s[i - repeats] *= repeat_keywords[s[i - repeats + 1]]\n\n            del s[i - repeats + 1]\n            repeats += 1\n\n    return ' '.join(s[::-1])\n"
'import statsmodels.api as sm\nX = np.append(arr = np.ones((len(X), 1)).astype(int), values = X , axis = 1)\nX_opt = X[:, [0, 1, 2, 3, 4, 5]]\nregressor_OLS = sm.OLS(endog = y, exog = X_opt)\nres=regressor_OLS.fit()\n'
'X = data[["SepalLength", "SepalWidth", "PetalLength", "PetalWidth"]]\ny = data[\'Class\']\nmodel.fit(X, y)\n'
'y = label_binarize(y, classes=[0, 1, 2])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=24)\n\ny = label_binarize(y, classes=[0, 1, 2])\n\n# Instantiate and fit the model:\nlogreg = OneVsRestClassifier(LogisticRegression(solver="liblinear", random_state=24))\ny_score = logreg.fit(X_train, y_train).decision_function(X_test)\n'
"import cv2\nimport matplotlib.pyplot as plt\n\nlabel = [0.536328, 0.5, 0.349219, 0.611111]\nimg = cv2.imread('P6A4J.jpg')\n\nH, W, _ = img.shape\nobject = [int(label[0]*W/2), int(label[1]*H/2), int(label[2]*W), int(label[3]*H)]\n\nx,y,w,h = object\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(img[y:y+h, x:x+w])\nplt.show()\n\n\nplt.show()\n"
'result = model0.predict_classes(feature.reshape(1,num_features,1))\nprint(label.inverse_transform(result))\n'
"# OneHot encoding categorical columns\noh_cols = df.select_dtypes('object').columns\nX_cat = df[oh_cols].to_numpy()\noh = OneHotEncoder()\none_hot_cols = oh.fit(X_cat)\n\ndf_prepr = pd.DataFrame(one_hot_cols.transform(X_cat).toarray(),\n                        columns=one_hot_cols.get_feature_names(input_features=oh_cols))\n"
'X : {array-like, sparse matrix} of shape (n_samples, n_features)\n     Training vector, where n_samples is the number of samples and\n     n_features is the number of features.\n\nInput validation on an array, list, sparse matrix or similar.\nBy default, the input is checked to be a non-empty 2D array containing\nonly finite values. If the dtype of the array is object, attempt\nconverting to float, raising on failure.\n'
"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Recall(), ...])\n"
'plt.plot(accuracy.values())\n\nplt.plot(list(accuracy.values()))\n'
's = pd.Series(selected, name=\'bools\')\nprint("The selected values from the test set are: " + test[s.values])\n\ns = pd.Series(selected, name=\'bools\')\ntestA = test[s.values]\nprint("The selected values from the test set are: " + testA[\'columnname\'])\n'
"keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.1)\n\nrestore_best_weights=True\n"
"dense = tf.keras.activations.sigmoid(dense)\n\ntf.keras.layers.Dense(units=1, activation='sigmoid')(dense)\n"
"model = Sequential()\nmodel.add(Bidirectional(LSTM(\n    100,\n    batch_input_shape=(batch_size, look_back, features),\n)))\nmodel.add(Dense(1))\n\nmodel.predict(np.zeros((batch_size, look_back, features)))\nprint(hasattr(model.layers[-1], 'kernel'))\nmodel.compile(optimizer=CustomOptimizer(), loss='mse')\n"
"class data_generator(keras.utils.Sequence):\n    def __init__(self, frames, labels, batch_size, data_dir, shuffle=False):\n        'Initialization'\n        self.batch_size = batch_size\n        self.labels = labels\n        self.frames = frames\n        self.data_dir = data_dir\n        self.shuffle = shuffle\n        self.size = len(self.frames)\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(self.size/self.batch_size))\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.frames))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        x_batch, y_batch = self.__data_generation(index)\n        return x_batch, y_batch\n\n    # def __data_generation(self, index):\n    #     'Generates data containing batch_size samples'\n    #     current_indices = self.indexes[index*self.batch_size:(index + 1)*self.batch_size]\n    #     x_batch = []\n    #     y_batch = []\n    #     for idx in current_indices:\n    #         # video_array = np.load(self.data_dir + '/' + self.frames[idx] + '.npy')\n    #         # x_batch.append(np.array(video_array))\n    #         y_batch.append(self.labels[idx])\n\n    #     return np.array(x_batch), y_batch\n\n    def __data_generation(self, index):\n        'Generates data containing batch_size samples'\n        limit = min(self.size, (index + 1)*self.batch_size)\n        x_batch = []\n        print('\\nbatch start: ' + str(index*self.batch_size) + ', batch end: ' + str(limit))\n        for frame in self.frames[index*self.batch_size:limit]:\n            video_array = np.load(self.data_dir + '/' + frame + '.npy')\n            x_batch.append(np.array(video_array))\n        return np.array(x_batch), self.labels[index*self.batch_size:limit]\n"
"model = tf.keras.Sequential() \nmodel.add(layers.Dense(64, activation='relu', input_shape=(numInputColumns,)))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n"
'estim = BaggingClassifier(base_estimator=MLPClassifier(), n_estimators=33)\nfor model in estim.estimators_:\n    model.fit(X, Y)\n'
"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nX_train = train.loc[:, train.columns != 'Outcome'] # &lt;---- Here you exclude the Outcome from train\ny_train = train['Outcome'] # &lt;---- This is your Outcome \n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train) # &lt;---- Here you convert your A-F values to numeric(0-5)\n\nrf = RandomForestClassifier() # &lt;---- Here you call the classifier\nrf.fit(X_train, y_train) # &lt;---- Fit the classifier on train data\nrf.score(X_train, y_train) # &lt;---- Check model accuracy\ny_pred = pd.DataFrame(rf.predict(test), columns = ['Outcome']) # &lt;---- Make predictions on test data\ntest_pred = pd.concat([test, y_pred['Outcome']], axis = 1) # &lt;---- Here you add predictions column to your test dataset\ntest_pred.to_excel(r'path\\Test.xlsx')\n"
"sb.lmplot(x='Year', y='Obesity (%)', data=dt) \nplt.show()\n"
'xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                       np.arange(x2_min, x2_max, resolution))\n\nZ = classifier.net_input(np.array([xx1.ravel(), xx2.ravel()]).T)\nZ = Z.reshape(xx1.shape)\n'
"binary_map = {'Yes':1.0, 'No':0.0, True : 1.0, False : 0.0}\ntoNum = UserDefinedFunction(lambda k: binary_map[k], DoubleType())\n\nroot\n |-- State: string (nullable = true)\n |-- Account length: integer (nullable = true)\n |-- Area code: integer (nullable = true)\n |-- International plan: string (nullable = true)\n |-- Voice mail plan: string (nullable = true)\n  .\n  .\n  .\n |-- Customer service calls: integer (nullable = true)\n |-- Churn: boolean (nullable = true)\n"
"x=testImages,\n\ntestImages = []   # and list append's\n"
'sub_models_outputs = [m(inputs) for m in sub_models]\n\nsub_models_outputs = [m.layers[-1] for m in sub_models]\n'
'sess = tf.InteractiveSession()\n\nsess.run(tf.global_variables_initializer())\ncoordinator = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess,coord=coordinator)\n\nfeature_classifier = learn.SKCompat(learn.Estimator(model_fn=cnn_model_fn, model_dir="/tmp/feature_model"))\n ...\n\nprint(eval_results)\ncoordinator.request_stop()\ncoordinator.join(threads)\n\nsess = tf.InteractiveSession()\n\nfeature_classifier = learn.Estimator(model_fn=cnn_model_fn, model_dir="/tmp/feature_model")\ntensors_to_log = {"probabilities": "softmax_tensor"}\nlogging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=10)\nfeature_classifier.fit( input_fn=lambda:image_processing.inputs(training_data), train=True), steps=200000, monitors=[logging_hook])\nmetrics = {\n        "accuracy":\n                learn.MetricSpec(metric_fn=tf.metrics.accuracy, prediction_key="classes"),\n}\n'
'softmax_tensor = sess.graph.get_tensor_by_name(\'final_layer/final_result/Softmax:0\')\npredictions = sess.run(softmax_tensor, { \'DecodeJpeg/contents:0\':image_data, \'final_layer/dropout/Placeholder:0\': 1.})\n\nStatus run_status = session-&gt;Run({{input_layer, resized_tensor}}, {output_layer}, {}, &amp;outputs);\n\n{{input_layer, resized_tensor}}\n\nStatus run_status = session-&gt;Run({\n    {"DecodeJpeg/contents", resized_tensor},\n    {"final_layer/dropout/Placeholder", 1f}\n}, {"final_layer/final_result/Softmax"}, {}, &amp;outputs);\n'
'xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n\nxx, yy = np.meshgrid(np.linspace(*xlim, num=2000),\n                         np.linspace(*ylim, num=2000))\n'
'        model = Sequential()\n        # crop the images to get rid of irrelevant features if needed...\n        model.add(Cropping2D(cropping=((0, 0), (0,0)), input_shape=("your_input_shape tuple x,y,rgb_depth")))\n        model.add(Lambda(lambda x: (x - 128) / 128)) # normalize all pixels to a mean of 0 +-1\n        model.add(Conv2D(24, (2,2), strides=(2,2), padding=\'valid\', activation=\'elu\')) # 1st convolution\n        model.add(BatchNormalization()) # normalize between layers\n        model.add(Conv2D(36, (2,2), strides=(2,2), padding=\'valid\', activation=\'elu\')) # 2nd convolution\n        model.add(BatchNormalization())\n        model.add(Conv2D(48, (1,1), strides=(2,2), padding=\'valid\', activation=\'elu\')) # 3rd convolution\n        model.add(BatchNormalization())\n        # model.add(Conv2D(64, (3,3), strides=(1,1), padding=\'valid\', activation=\'elu\')) # 4th convolution\n        # model.add(BatchNormalization())\n        # model.add(Conv2D(64, (3,3), strides=(1,1), padding=\'valid\', activation=\'elu\')) # 4th convolution\n        # model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        model.add(Flatten()) # flatten the dimensions\n        model.add(Dense(100, activation=\'elu\')) # 1st fully connected layer\n        model.add(BatchNormalization())\n        model.add(Dropout(0.5))\n        model.add(Dense(51, activation= \'softmax\')) # label output as probabilites\n'
'python --version\n'
'if self.transform is not None:\n     img = self.transform(img)\n'
"import onnx\nimport caffe2.python.onnx.backend\nfrom caffe2.python import core, workspace\n\nimport numpy as np\n\n# make input Numpy array of correct dimensions and type as required by the model\n\nmodelFile = onnx.load('model.onnx')\noutput = caffe2.python.onnx.backend.run_model(modelFile, inputArray.astype(np.float32))\n"
'custom_text = input("Enter Text")\ncustom_text = count_vectorizer.transform(df[\'custom_text\'])\nvalue_predicted = random_forest.predict(custom_text)\nprint(value_predicted[0])\n'
"'precision_macro'\n'precision_micro'\n'precision_weighted'\n'recall_macro'\n'recall_micro'\n'recall_weighted'\n\n   ValueError: 'balanced_accuracy' is not a valid scoring value.\n"
'from sklearn.metrics import confusion_matrix\ny_true = [0, 1, 0, 1]\ny_pred = [1, 1, 1, 0]\ncm = confusion_matrix(y_true, y_pred) # careful with the order of arguments!\ntn, fp, fn, tp = cm.ravel()\n(tn, fp, fn, tp)\n# (0, 2, 1, 1)\n\nTPR = tp/(tp+fn)\nTPR\n# 0.5\n\nTNR = tn/(tn+fp)\nTNR\n# 0.0\n'
'build_fn=nnmodel.create_model()\n\nbuild_fn=nnmodel.create_model\n'
'clf = RandomForestClassifier(n_estimators=100, warm_start=True)\n'
"nnModel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n"
'text = " If your import is failing due to a missing package, you can use pip. Non-current assets 5 675 5 512 4 789 4 586. We also expect cash equivalents 909 861 912 630 in toal"\n\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\'stopwords\')\nnltk.download(\'punkt\')\nstop = stopwords.words(\'english\')\ndocument = \' \'.join([i for i in text.split() if i not in stop])\nsentences = nltk.sent_tokenize(document)\nsentences = [nltk.word_tokenize(sent) for sent in sentences]\nnltk.download(\'averaged_perceptron_tagger\')\nsentences = [nltk.pos_tag(sent) for sent in sentences]\n\n[[(\'If\', \'IN\'),\n  (\'import\', \'NN\'),\n  (\'failing\', \'VBG\'),\n  (\'due\', \'JJ\'),\n  (\'missing\', \'VBG\'),\n  (\'package\', \'NN\'),\n  (\',\', \',\'),\n  (\'use\', \'NN\'),\n  (\'pip\', \'NN\'),\n  (\'.\', \'.\')],\n[(\'Non-current\', \'JJ\'),\n  (\'assets\', \'NNS\'),\n  (\'5\', \'CD\'),\n  (\'675\', \'CD\'),\n  (\'5\', \'CD\'),\n  (\'512\', \'CD\'),\n  (\'4\', \'CD\'),\n  (\'789\', \'CD\'),\n  (\'4\', \'CD\'),\n  (\'586\', \'CD\'),\n  (\'.\', \'.\')],\n[(\'We\', \'PRP\'),\n  (\'also\', \'RB\'),\n  (\'expect\', \'VBP\'),\n  (\'cash\', \'NN\'),\n  (\'equivalents\', \'NNS\'),\n  (\'909\', \'CD\'),\n  (\'861\', \'CD\'),\n  (\'912\', \'CD\'),\n  (\'630\', \'CD\'),\n  (\'toal\', \'NN\')]]\n\ngrammar = """MATCH:{&lt;JJ&gt;&lt;NNS&gt;&lt;CD&gt;}""" #grammar would need to be completed\ncp = nltk.RegexpParser(grammar)\nfor sentence in sentences:\n  print(cp.parse(sentence))\n\n(S\n  If/IN\n  import/NN\n  failing/VBG\n  due/JJ\n  missing/VBG\n  package/NN\n  ,/,\n  use/NN\n  pip/NN\n  ./.)\n(S\n  (MATCH Non-current/JJ assets/NNS 5/CD)\n  675/CD\n  5/CD\n  512/CD\n  4/CD\n  789/CD\n  4/CD\n  586/CD\n  ./.)\n(S\n  We/PRP\n  also/RB\n  expect/VBP\n  cash/NN\n  equivalents/NNS\n  909/CD\n  861/CD\n  912/CD\n  630/CD\n  toal/NN)\n'
'0    1\n1    0\n2    0\n3    0\n4    0\n5    0\n6    1\n'
'history = model.fit(...)\n'
"def predict(self, X):\n    # decision func on input array\n    scores = self.decision_function(X)\n    # column indices of max values per row\n    indices = scores.argmax(axis=1)\n    # index class array using indices\n    return self.classes_[indices]\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\nlr= LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred_prob = lr.predict_proba(X_test)\n\ny_pred_prob\narray([[1.06906558e-02, 9.02308167e-01, 8.70011771e-02],\n       [2.57953117e-06, 7.88832490e-03, 9.92109096e-01],\n       [2.66690975e-05, 6.73454730e-02, 9.32627858e-01],\n       [9.88612145e-01, 1.13878133e-02, 4.12714660e-08],\n       ...\n\nclasses = load_iris().target_names\nclasses[indices]\narray(['virginica', 'virginica', 'versicolor', 'virginica', 'setosa',\n       'versicolor', 'versicolor', 'setosa', 'virginica', 'setosa',...\n\ny_pred_prob = lr.predict_proba(X_test[0,None])\nix = y_pred_prob.argmax(1).item()\n\nprint(f'predicted class = {classes[ix]} and confidence = {y_pred_prob[0,ix]:.2%}')\n# predicted class = virginica and confidence = 90.75%\n"
'automobile_df[&quot;price&quot;] = automobile_df[&quot;price&quot;].astype(&quot;int64&quot;)\nautomobile_df[&quot;km/100L&quot;] = automobile_df[&quot;km/100L&quot;].astype(&quot;int64&quot;)\n\nsns.regplot(x=automobile_df[&quot;km/100L&quot;].astype(&quot;int64&quot;),y=automobile_df[&quot;price&quot;].astype(&quot;int64&quot;))\n'
"import pandas as pd\n\n\ndf = pd.DataFrame({\n    'Actual Label': ['A', 'A', 'A', 'A', 'B', 'B', 'C', 'D'],\n    'Predicted Label': ['A', 'B', 'C', 'D', 'B', 'C', 'D', 'D'],\n    'Count Occurences': [200, 150, 100, 150, 50, 100, 70, 80]\n})\n\n  Actual Label Predicted Label  Count Occurences\n0            A               A               200\n1            A               B               150\n2            A               C               100\n3            A               D               150\n4            B               B                50\n5            B               C               100\n6            C               D                70\n7            D               D                80\n\ndf = df.pivot_table(values='Count Occurences', index='Actual Label', columns='Predicted Label')\n\nPredicted Label      A      B      C      D\nActual Label                               \nA                200.0  150.0  100.0  150.0\nB                  NaN   50.0  100.0    NaN\nC                  NaN    NaN    NaN   70.0\nD                  NaN    NaN    NaN   80.0\n\ndf.fillna(0, inplace=True)\nprint(df.values)\n\n# Output\n\n[[200. 150. 100. 150.]\n [  0.  50. 100.   0.]\n [  0.   0.   0.  70.]\n [  0.   0.   0.  80.]]\n\n  Actual Label Predicted Label  Count Occurences\n0            A               A               200\n1            A               B               150\n2            A               C               100\n3            B               B               150\n4            B               C                50\n5            C               D               100\n6            D               D                70\n7            E               A                80\n\n[[200. 150. 100.   0.]\n [  0. 150.  50.   0.]\n [  0.   0.   0. 100.]\n [  0.   0.   0.  70.]\n [ 80.   0.   0.   0.]]\n\nmissing_cols = [col for col in df.index if col not in df.columns]\n\nfor col in missing_cols:\n    df[col] = 0\n\n# This will ensure that the index and columns have the same order\ndf = df[df.index.values]\n\n[[200. 150. 100.   0.   0.]\n [  0. 150.  50.   0.   0.]\n [  0.   0.   0. 100.   0.]\n [  0.   0.   0.  70.   0.]\n [ 80.   0.   0.   0.   0.]]\n"
"df2['Market_Category'].value_counts() \n"
"import numpy as np\nimport tensorflow as tf\n\ntrain_x = np.array([1] * 1000 + [2] * 1000 + [3] * 1000)\ntrain_x = tf.keras.utils.to_categorical(train_x - 1)\ntrain_y = np.zeros((3000, 3))\ntrain_y[:1000,0] = 1\ntrain_y[1000:2000,1] = 1\ntrain_y[2000:3000,2] = 1\nval_x = train_x\nval_y = train_y\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(3, activation='relu'))\nmodel.add(tf.keras.layers.Dense(3, activation='softmax'))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n             loss=tf.keras.losses.categorical_crossentropy,\n             metrics=[tf.keras.metrics.categorical_accuracy])\n\nmodel.fit(train_x, train_y, epochs = 10, batch_size = 32, verbose = 1,\n          shuffle = False,\n          validation_data=(val_x, val_y))\n\nEpoch 9/10\n  32/3000 [..............................] - ETA: 0s - loss: 0.0067 - cat_acc: 1.0000\n 608/3000 [=====&gt;........................] - ETA: 0s - loss: 0.0063 - cat_acc: 1.0000\n1184/3000 [==========&gt;...................] - ETA: 0s - loss: 0.0244 - cat_acc: 1.0000\n1760/3000 [================&gt;.............] - ETA: 0s - loss: 0.0553 - cat_acc: 1.0000\n2272/3000 [=====================&gt;........] - ETA: 0s - loss: 0.0550 - cat_acc: 1.0000\n2848/3000 [===========================&gt;..] - ETA: 0s - loss: 0.0447 - cat_acc: 1.0000\n"
"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, Flatten\nfrom tensorflow.keras import optimizers\nimport numpy as np\n\ndata = np.random.rand(1000,22)\n\ntrain_X = data[0:data.shape[0],0:12]\ntrain_X = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))\n\ntrain_y = data[0:data.shape[0],12:data.shape[1]]\n\nneurons = 10\nmodel = Sequential()\nmodel.add(Conv1D(filters=64,input_shape=train_X.shape[1:], \n    activation='relu',kernel_size = 3))\nmodel.add(Flatten())\nmodel.add(Dense(neurons,activation='relu')) # first hidden layer\nmodel.add(Dense(10, activation='softmax'))\nsgd = optimizers.SGD(lr=0.05, decay=1e-6, momentum=0.95, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\nhistory = model.fit(train_X, train_y, validation_split=0.2, epochs=1, batch_size=100)\n\nTrain on 800 samples, validate on 200 samples\n100/800 [==&gt;...........................] - ETA: 2s - loss: 11.4786 - acc: 0.0800\n800/800 [==============================] - 0s 547us/sample - loss: 55.3883 - acc: 0.1000 \n"
"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ndef makeModel(num_inputs, num_classes, train_X, train_y):\n    model = Sequential()\n    model.add(Dense(8, input_dim=num_inputs, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.fit(train_X, train_y, epochs=10, batch_size=10)\n\n    return model\n\nlabel_encoder = LabelEncoder()\niris = datasets.load_iris()\ny = iris.target\ny = label_encoder.fit_transform(y)\n\ntrain_x, val_x, train_y, val_y = train_test_split(iris.data, y, test_size=0.2)\niris_model = makeModel(4, 3, train_x, train_y)\n"
"import tensorflow as tf\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\n\n(X_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\nX_train = X_train[..., None]\n\ndef build_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')])\n    model.compile(loss='sparse_categorical_crossentropy', \n        optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = build_model()\n\nhistory = model.fit(X_train, y_train, epochs=1)\n\nkeras_clf = KerasClassifier(build_model)\n\naccuracies = cross_val_score(estimator=keras_clf, scoring=&quot;accuracy&quot;, \n    X=X_train, y=y_train, cv=5)\n\nprint(accuracies)\n\narray([0.74008333, 0.65      , 0.71075   , 0.561     , 0.66683333])\n"
'probs = clf.predict_proba(X)\n# probs will output the probability of the prediction on each class. \n'
"['[CLS]', '[UNK]', '[UNK]', '[SEP]']\n['[CLS]', 'hello', ',', 'my', 'dog', 'is', 'cute', '[SEP]', 'he', 'is', 'really', 'nice', '[SEP]']\n['[CLS]', '[UNK]', '[SEP]']\n['[CLS]', 'hello', ',', 'my', 'dog', 'is', 'cute', '[SEP]']\n\n{'input_ids': [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102], [101, 2002, 2003, 2428, 3835, 102]]}\n\n[{'input_ids': [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102], [101, 2002, 2003, 2428, 3835, 102]]}, {'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102, 2002, 2003, 2428, 3835, 102]}, {'input_ids': [[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]]}, {'input_ids': [101, 7592, 1010, 2026, 3899, 2003, 10140, 102]}]\n\n"
'&gt;&gt;&gt; nn.BCELoss()(torch.softmax(input, axis=1), torch.softmax(target.float(), axis=1))\n&gt;&gt;&gt; tensor(0.6376, grad_fn=&lt;BinaryCrossEntropyBackward&gt;)\n'
"import pickle\nimport matplotlib.pyplot as plt\n\npkl = open('pickled_image.pickle', 'rb')\nim = pickle.load(pkl)\n\nplt.imshow(im)\n"
'from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)\ncls = tree.fit(X_train, y_train)\ny_pred = cls.predict(X_test)\n\nscore = cross_val_score(cls, X_test, y_test)\nscore\n# array([1., 1., 1., 1., 1.])\n\nscore = cross_val_score(cls, X_test, y_test, cv=3)\nscore\n# array([1., 1., 1.])\n'
"self.w = tf.Variable(\n      tf.initializers.GlorotUniform()([in_features, out_features]), name='w')\n\nEpoch  1 Loss 0.515 Acc 0.776 TLoss 0.537 TAcc 0.720\nEpoch  2 Loss 0.186 Acc 0.928 TLoss 0.136 TAcc 0.920\nEpoch  3 Loss 0.171 Acc 0.944 TLoss 0.104 TAcc 0.920\nEpoch  4 Loss 0.230 Acc 0.920 TLoss 0.268 TAcc 0.880\nEpoch  5 Loss 0.177 Acc 0.928 TLoss 0.284 TAcc 0.880\nEpoch  6 Loss 0.144 Acc 0.944 TLoss 0.111 TAcc 0.920\nEpoch  7 Loss 0.151 Acc 0.952 TLoss 0.137 TAcc 0.920\nEpoch  8 Loss 0.192 Acc 0.952 TLoss 0.111 TAcc 0.960\nEpoch  9 Loss 0.081 Acc 0.968 TLoss 0.074 TAcc 0.960\nEpoch 10 Loss 0.222 Acc 0.920 TLoss 0.097 TAcc 1.000\n"
'new_data_vectorized = cv.transform(new_data) # NOT fit_transform\nnew_predictions = naive_bayes.predict(new_data_vectorized)\n'
'import numpy as np\nacc = np.sum(np.equal(y_true,y_pred))/len(y_true)\n'
'vector = np.random.randn(100,1) # vector.shape = (100, 1)\nrank_1_array = vector.ravel() # rank_1_array.shape = (100, )\n'
'import pickle\npickle.dump( tokenizer, open( &quot;tokenizer.pickle&quot;, &quot;wb&quot; ) )\n\ntokenizer = pickle.load( open( tokenizer.pickle&quot;, &quot;rb&quot; ) )\n'
"train_x = []\nfor  rows in plays.itertuples():\n     play = isolatePlay(week, getattr(rows, 'gameId'), getattr(rows, 'playId'))\n     train_x.append([play])\n     count=len(train_x)\n"
'class FunctionWrapperLayer(Layer):\n    def __init__(self, fn):\n        super(FunctionWrapperLayer, self).__init__()\n        self.fn = fn\n        \n    def build(self, input_shapes):\n        super(FunctionWrapperLayer, self).build(input_shapes)\n        if type(input_shapes) is list:\n            inputs = [Input(shape[1:]) for shape in input_shapes]\n        else:\n            inputs = Input(input_shapes[1:])\n        outputs = self.fn(inputs)\n        self.fn_model = Model(inputs=inputs, outputs=outputs)\n        self.fn_model.compile()\n        \n    def call(self, x):\n        return self.fn_model(x)\n'
"{'max_depth': None,  # full tree depth\n 'max_features': 20, # all features (default)\n 'min_samples_split': 2,\n 'n_estimators': 100}\n"
"from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport scipy\n\ndata = pd.read_csv('Hotel_Reviews.csv.zip')\nstopset = set(stopwords.words('english'))\nvectorizer = TfidfVectorizer(use_idf=True, lowercase=True, strip_accents='ascii', stop_words=stopset)\n\ny = data[&quot;Reviewer_Score&quot;]\nx = scipy.sparse.hstack([vectorizer.fit_transform(data['Negative_Review']),\n                        vectorizer.fit_transform(data['Positive_Review'])]\n                       )\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=123)\n"
'from sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\nencoder = LabelEncoder()\nencoder.fit(y)\ny = encoder.transform(y)\ny = to_categorical(y)\n\nres = np.argmax(res, axis=None, out=None)\n'
"CUDA_VISIBLE_DEVICES=&quot;&quot;\n\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n"
'for i in range(x_train.shape[1]):\n    x_train[i] = (x_train[i]-np.mean(x_train[i]))/np.std(x_train[i])\n    x_test[i] = (x_test[i]-np.mean(x_test[i]))/np.std(x_test[i])\n\nfor i in range(x_train.shape[1]):\n    x_train[:,i] = (x_train[:,i]-np.mean(x_train,axis=1))/np.std(x_train,axis=1)\n    x_test[:,i] = (x_test[:,i]-np.mean(x_test,axis=1))/np.std(x_test,axis=1)\n'
"x_test = dataset.loc[dataset['clear_date'].isnull()].copy()\n\nx_test = dataset.loc[pd.to_date_time(dataset['clear_date']).isnull()].copy()\n"
'#!/bin/bash\n\n# set parameter:\nmyParameter = 1 \n\n# call MATLAB with no interface at all and send it to background\n# run MATLAB function with parameter, exit afterwards\n# print command window output to a textfile\nmatlab -nodesktop -nosplash -nodisplay -r "myMatlabFunction(${myParameter});exit" &gt; matlab_output.txt &amp;\n'
'from random import uniform\nlearning_rate = 0.0001\nN = 100\nk, k0 = uniform(-100, 100), uniform(-100, 100)\nfor _ in range(N):\n    x = uniform(-100, 100)\n    k = k - learning_rate * x * (k * x - k0 * x)\n    print k, k0\n'
'import pandas as pd\n\ndf = pd.DataFrame(feature_list, columns = mega_list)\n\nfrom sklearn import cross_validation\n\nx_train, x_test, y_train, y_test = cross_validation.train_test_split(\n    df, Y, test_size=0.8, random_state=0)\n'
'n = df["value"].isnull()\nclusters = (n != n.shift()).cumsum()\ndf["cumsum"] = df["value"].groupby(clusters).cumsum().fillna(0)\n\nto_zero = n &amp; (df["value"].groupby(clusters).transform(\'size\') == 1)\ntmp_value = df["value"].where(~to_zero, 0)\nn2 = tmp_value.isnull()\nnew_clusters = (n2 != n2.shift()).cumsum()\ndf["cumsum_skip1"] = tmp_value.groupby(new_clusters).cumsum().fillna(0)\n\n&gt;&gt;&gt; df\n    index  value  cumsum  cumsum_skip1\n0       1    0.8     0.8           0.8\n1       2    0.9     1.7           1.7\n2       3    1.0     2.7           2.7\n3       4    0.9     3.6           3.6\n4       5    NaN     0.0           0.0\n5       6    NaN     0.0           0.0\n6       7    NaN     0.0           0.0\n7       8    0.4     0.4           0.4\n8       9    0.9     1.3           1.3\n9      10    NaN     0.0           1.3\n10     11    0.8     0.8           2.1\n11     12    2.0     2.8           4.1\n12     13    1.4     4.2           5.5\n13     14    1.9     6.1           7.4\n14     15    NaN     0.0           0.0\n15     16    NaN     0.0           0.0\n16     17    NaN     0.0           0.0\n17     18    8.4     8.4           8.4\n18     19    9.9    18.3          18.3\n19     20   10.0    28.3          28.3\n'
"if isinstance(covfunc, FITCOfKernel): \n    Ks = covfunc.getCovMatrix(x=x, z=xs[id,:], mode='cross')   #   cross-covariances\n    Ks = Ks[nz,:]\nelse:\n    Ks  = covfunc.getCovMatrix(x=x[nz,:], z=xs[id,:], mode='cross')   # cross-covariances\n"
'concated = tf.concat(1, [indices, tf.cast(labels,tf.int32)])\n'
"print df\n          0         1         2\n0  0.423201  0.368718  0.338091\n1  0.246899  0.437535  0.000262\n2  0.978685  0.136219  0.027693\n\ndf1 = df.rank(method='max', axis=1)\nprint df1\n   0  1  2\n0  3  2  1\n1  2  3  1\n2  3  2  1\n\n#get max value of df1\nma = df1.max().max()\nprint ma\n3.0\n\nprint (df1 == ma)\n       0      1      2\n0   True  False  False\n1  False   True  False\n2   True  False  False\n\nprint (df1 == ma).astype(int)\n   0  1  2\n0  1  0  0\n1  0  1  0\n2  1  0  0\n\nprint df.max(axis=1)\n0    10\n1     8\n2     9\ndtype: int64\n\nprint df.eq(df.max(axis=1), axis=0).astype(int)\n   0  1  2\n0  1  0  0\n1  0  1  0\n2  1  0  0\n\nIn [418]: %timeit df.eq(df.max(axis=1), axis=0).astype(int)\nThe slowest run took 5.44 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 334 µs per loop\n\nIn [419]: %timeit df.apply(lambda x: x == x.max(), axis='columns').astype(int)\nThe slowest run took 4.49 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 1.44 ms per loop\n\nIn [420]: %timeit (df.rank(method='max', axis=1) == df.rank(method='max', axis=1).max().max()).astype(int)\nThe slowest run took 4.83 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 656 µs per loop\n\nIn [426]: %timeit df.eq(df.max(axis=1), axis=0).astype(int)\nThe slowest run took 5.44 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 456 µs per loop\n\nIn [427]: %timeit df.apply(lambda x: x == x.max(), axis='columns').astype(int)\n1 loops, best of 3: 496 ms per loop\n\nIn [428]: %timeit (df.rank(method='max', axis=1) == df.rank(method='max', axis=1).max().max()).astype(int)\nThe slowest run took 4.50 times longer than the fastest. This could mean that an intermediate result is being cached \n1000 loops, best of 3: 1.32 ms per loop\n"
'&gt;&gt;&gt; (x1 - np.mean(x1)) / np.std(x1)\narray([-0.94627295, -0.90205459, -0.54830769,  0.95511663,  1.44151861])\n\n&gt;&gt;&gt; (x2 - np.mean(x1)) / np.std(x1)\narray([ -0.94627295,  -0.90205459,  -0.54830769,   0.95511663, 28.50315638])\n'
"from sklearn.pipeline import Pipeline\n\ndef create_and_fit_model(data):\n    # ... get your train and expect data\n    vectorizer = TfidfVectorizer(min_df=1)\n    nb = MultinomialNB()\n    model = Pipeline([('vectorizer', vectorizer), ('nb', nb)])\n    model.fit(train, expect)\n    return model\n"
'e0 = np.array([1, 0])\ne1 = np.array([0, 1])\neps = 1e-5\n\nx0 = np.array([1, 1])\n\ndf_yours = gradf(x0)\n# array([  3.54000000e+03,   4.05583000e+06])\n\ndf_approx = np.array([\n    cost_Function(x0 + eps*e0) - cost_Function(x0 - eps*e0),\n    cost_Function(x0 + eps*e1) - cost_Function(x0 - eps*e1)\n]) / (2 * eps)\n# array([ -7.07999999e+03,  -8.11166000e+06])\n'
'In [1]: from sklearn.ensemble import RandomForestClassifier\nIn [2]: from sklearn import datasets\nIn [3]: from sklearn.externals import joblib\nIn [4]: iris = datasets.load_iris()\nIn [5]: X, y = iris.data, iris.target\nIn [6]: m = RandomForestClassifier(2).fit(X, y)\nIn [7]: m\nOut[7]: \nRandomForestClassifier(bootstrap=True, class_weight=None, criterion=\'gini\',\n            max_depth=None, max_features=\'auto\', max_leaf_nodes=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=2, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nIn [8]: joblib.dump(m, "filename.cls")\n'
'X = X[:200]\ny = y[:200]\n'
'for k in k_range:\n    clf = KNeighborsClassifier(n_neighbors = k)\n    clf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\nscores.append(metrics.accuracy_score(y_test,y_pred))\n\nfor k in k_range:\n    clf = KNeighborsClassifier(n_neighbors = k)\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    scores.append(metrics.accuracy_score(y_test,y_pred))\n'
'batch2 (BatchNormalization)      (None, 256, 28, 28)   512         maxpooling2d_3[0][0]             \nupsample1 (UpSampling2D)         (None, 256, 28, 28)   0           residual1[0][0]                  \nmerge1 (Merge)                   (None, 256, 28, 28)   0           batch2[0][0]                     \n'
'indexes, metrics = model.cosine(normalized_phrase)\nmodel.generate_response(indexes, metrics)\n'
'X = np.reshape(X,(5))\nY = np.reshape(Y,(5))\n'
'#split X and y in training and test\nX_train = raw_data.loc[:,X_vars] \ny_train = raw_data.loc[:,y_var]\nX_test = raw_test.loc[:,X_vars]\ny_test = raw_test.loc[:,y_var]\n'
'import pandas as pn\nimport sklearn as sk\nfrom sklearn.model_selection import train_test_split as lk\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import StandardScaler\n\ndata = datasets.load_iris().data\nlabels = datasets.load_iris().target\n\ndata = StandardScaler().fit_transform(data)\n\ntrain_data,test_data,train_label,test_label=lk(data,labels,test_size=0.3)\nclassifier = BernoulliNB().fit(train_data,train_label)\nresult= classifier.predict(test_data)\nprint(result)\n'
'# fit the model\nclf_weights = svm.SVC()\nclf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n'
'tf.gather_nd(\n    my_tensor,\n    tf.stack([tf.range(batch_size), tf.squeeze(selecter)], axis=-1))\n'
'def forward(self, x):\n    x = F.relu(self.input_layer(x))\n    x = F.dropout(F.relu(self.hidden_layer(x)),training=self.training)\n    x = self.output_layer(x)\n    return x\n'
"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('data/kc_house_data.csv')\n\ndrop_cols = ['id', 'date', 'zipcode', 'long']\nfeature_cols = ['bathrooms', 'floors', 'bedrooms', \n             'sqft_living', 'sqft_lot', 'waterfront', 'view', \n             'condition', 'grade', 'lat', 'sqft_above']\ntarget_col = ['price']\n\nall_cols = feature_cols + target_col\n\ndataset = df.drop(drop_cols, axis=1)\ndataset[all_cols] = dataset[all_cols].applymap(np.int64)\n\n# split dataset for cross-validation\ntrain, test = train_test_split(dataset, test_size=0.3, random_state=176)\n\n# set up our random forest model\nrf = RandomForestRegressor(max_depth=30, n_estimators=5)\n\n# fit our model\nrf.fit(train[feature_cols].values, train[target_col].values )\n\n# look at our predictions\ny_pred = rf.predict(test[feature_cols].values)\nr2 = r2_score(y_pred, test[target_col].values)\nprint('R-squared: {}'.format(r2))\n\n# look at the feature importance\nimportance =  rf.feature_importances_\nimportance = pd.DataFrame(importance, index=feature_cols, columns=['importance'])\nprint('Feature importance:\\n {}'.format(importance))\n\nR-squared: 0.532308123273\n\nFeature importance:\n              importance\nbathrooms      0.016268\nfloors         0.010330\nbedrooms       0.017346\nsqft_living    0.422269\nsqft_lot       0.104096\nwaterfront     0.021439\nview           0.037015\ncondition      0.025751\ngrade          0.279991\nlat            0.000000\nsqft_above     0.065496\n\n# fake price classes, this is for later\ntarget_binary = ['price_binary']\ndataset[target_binary] = (dataset[target_col] &gt; 221900).astype(int)\n\n# Using a Random Forest Classifier\n# fit the classifier\nrfc = RandomForestClassifier(max_depth=30, n_estimators=5)\nrfc.fit(train[feature_cols], train[target_binary])\n\n# look at the feature importance\nimportance_c =  rfc.feature_importances_\nimportance_c = pd.DataFrame(importance_c, index=feature_cols, columns=['importance'])\nprint('Feature importance:\\n {}'.format(importance))\n\n# look at our predictions\ny_pred_c = rfc.predict(test[feature_cols])\ncm = confusion_matrix(y_pred_c, test[target_binary])\nprint('Confusion matrix:\\n {}'.format(cm))\n\nFeature importance:\n              importance\nbathrooms      0.018511\nfloors         0.011572\nbedrooms       0.019063\nsqft_living    0.455199\nsqft_lot       0.113200\nwaterfront     0.026671\nview           0.030930\ncondition      0.021197\ngrade          0.235906\nlat            0.000000\nsqft_above     0.067749\n\nConfusion matrix:\n [[  92  155]\n [ 324 5913]]\n"
'from sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nimport mglearn.plots\n\nX, y = mglearn.datasets.make_forge()\n\n\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nplt.show()\n'
'data_vectors = [[tweet_w2v.wv(token) for token in each_example]\n                for each_example in data.head(n).tokens]\n# ...*then* do your split...\nx_train, x_test, y_train, y_test = train_test_split(np.array(data_vectors),np.array(data.head(n).Sentiment), test_size=0.2)\n'
'#create a model the way you like it, it can be Functional API or Sequential, no problem\nxOrgModel = createAModelForXOrgData(...)\n\nfrom keras.models import Model\nfrom keras.layers import Input, Subtract\n\ninput1 = Input(shapeOfInput)\ninput2 = Input(shapeOfInput)\n\noutput1 = xOrgModel(input1)\noutput2 = xOrgModel(input2)\n\noutput = Subtract()([output1,output2])\n\npairWiseModel = Model([input1,input2],output)\n\nL = len(X_org)\nx1 = []\nx2 = []\ny = []\n\nfor i in range(L):\n    for j in range(i+1,L):\n        x1.append(X_org[i])\n        x2.append(X_org[j])\n        y.append(Y_org[i] - Y_org[j])\n\nx1 = np.array(x1) \nx2 = np.array(x2) \ny = np.array(y) \n\npairWiseModel.fit([x1,x2],y,...)\n'
'plt.scatter(X_test.index,X_test.values,c=y_predict_test)\nplt.show()\n'
"array = np.fromfile('path/to/ex2x.dat', sep=' ', dtype=float)\n\n[[ 0.75016254]\n[ 0.06388117]]\n"
'print( sess.run( prediction, feed_dict= {\n    x: two_images[0].reshape((1, 784)),\n    y: np.zeros( shape = ( 1, 10 ), dtype = np.float32 ) } ) )\n'
'prediction = tf.nn.softmax(pred)\ncorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n\ncorrect = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n\nop , ac, loss_value = sess.run([train_op, acc, loss], feed_dict={x: train_batch_x, y: train_batch_y})\n'
'inputs = tf.keras.Inputs((100,))\nuninteresting, interesting, more_uninteresting = tf.split(inputs, [50, 10, 40], axis=1)\ninputs = tf.concat([uninteresting, interesting, more_uninteresting], axis=1)\nmodel = Model(inputs)\n...\nJ, = tf.gradients(y_list[i], interesting)\n\nJ, = tf.gradients(y_list[i], model.input[0])\nJ = J[:, 50:60]\n'
"Y_train = [batch_size,img_height,img_width,number_of_classes]\n\n1 = batch_size ,65536 flattened image and 4 = num_of_classes\n\nY_train = [1,256,256,4] \n\nfrom keras import backend as K\nK.flatten()\n\noutputs = Conv2DTranspose(1, (1, 1), activation='sigmoid') (c9) \n\noutputs = Conv2DTranspose(4, (1, 1), activation='softmax') (c9)\n#as you have 4 output classes\n"
'import re\nres = []\nwith open(filename, "r") as infile:\n    for line in infile.readlines():\n        data = re.findall("-?\\d+\\.\\d+", line)\n        if data:\n            floatData = list(map(float, data))\n            res.append(floatData)\nprint(res)\n\n[[0.03518, -0.02543], [0.0025865, -0.01867]]\n'
'sentence = ["the","book","is","on","the","table"]\n\nfor ix in range(len(sentence)):\n    x = sentence[ix] #so this is the ix-th word of the sentence\n    from_index = max((ix-window_size) # this is the initial index of the window\n    to_index = (ix+window_size+1) # this is the final index of the windows (excluding itself)\n    window = sentence[from_index, to_index] # we pick the words of the sentence\n\nix=0, x="the", from_index=0, to_index=4, window = ["the", "book", "is", "on"]\nix=3, x="on", from_index=0, to_index=7, window = ["the", "book", "is", "on", "the", "table"]\n\nsentence = [2,45,7,13,2,67]\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\n#doesnt work\nx_train = np.array([50,100,150,200])\ny_train= np.array([150,100,50,0])\n\n#work\n# x_train = np.array([30,60,70,100])\n# y_train= np.array([60,120,145,195])\n\ndef model(x,w,b):\n    return x*w+b;\n\ndef cost(y,y_hat):\n    return np.sum((y-y_hat)**2)/y.size\n\nlearning_rate=0.0001\ndef trainning_round(x_train,y_train,w,b,learning_rate):\n\n    y_hat=model(x_train,w,b)\n    j = cost(y_train,y_hat)\n\n    # w_gradient=-2*x_train.dot(y_train-y_hat)\n    # b_gradient=-2*np.sum(y_train-y_hat)\n    w_gradient=x_train.dot(y_hat-y_train) / y_train.size\n    b_gradient=np.sum(y_hat-y_train) / y_train.size\n\n    print(w_gradient, b_gradient)\n\n    w=w-learning_rate*w_gradient\n    b=b-learning_rate*b_gradient\n    print(j, w,b)\n    return w,b\n\nnum_epoch=200000\ndef train(X,Y):\n\n    w=2.1\n    b=1.5\n\n    #for plt\n    ar = np.arange(0, 200, 0.5)\n\n    for i in range(num_epoch):\n        w,b=trainning_round(X,Y,w,b,learning_rate)\n\n    plt.plot(ar,model(ar, w, b))\n    plt.axis([0, 300, 0, 200])\n\n    plt.plot(X, Y, 'ro')\n\ntrain(x_train,y_train)\nplt.show()\n"
'training set score: 0.67\ntest set score: 0.66\n\ntraining set score: 0.892\ntest set score: 0.876\n'
's = sess.run(merged_summary, feed_dict={X: batch_x, Y_: batch_y, step: i})\n'
'loss.backward()\n'
'A = add([Y, Z])\nM = concatenate([Y, Z, A])\n'
'x = Cropping1D(cropping=(0, 1))(x) # Crop nothing from input but crop 1 elemnt from the end \n'
'        max_score = None\n...\n            if score &gt; max_score:\n'
'    with tf.Session() as sess:\n         number_prediction = tf.argmax(ntwk_output_2 , 1)\n         number_prediction = sess.run(number_prediction , feed_dict={network_input : \n                              yourImageNdArray } )\n         print(&quot;your prediction : &quot;,number_prediction)\n'
'import numpy as np\nimport pandas as pd\nfrom sklearn import cross_validation\n\nX_train, X_test, y_train, y_test=cross_validation.train_test_split(X_features,y_target,test_size=0.4,random_state=0)\n'
"print(K.eval(model.optimizer.lr))\n\nmodel.save('my_model.h5')\n\nfrom keras.models import load_model\n\nmodel = load_model('my_model.h5')\n\n# load the architecture of model from json file ...\n\n# load the weights\nmodel.load_weights('model_weights.h5')\n"
'# Loop over possible values of "n_neighbors"\nfor i in range(1, 41):  \n\n    # Collect the actual and predicted values for all splits for a single "n_neighbors"\n    actual = []\n    predicted = []\n\n\n    for train_index, test_index in loo.split(X):\n        #print("TRAIN:", train_index, "TEST:", test_index)\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        classifier = KNeighborsClassifier(n_neighbors=i)  \n        classifier.fit(X_train, y_train)\n        y_pred = classifier.predict(X_test)\n\n        # Append the single predictions and actual values here.\n        actual.append(y_test[0])\n        predicted.append(y_pred[0])\n\n    # Outside the loop, calculate the error.\n    error.append(np.mean(np.array(predicted) != np.array(actual))) \n\nfrom sklearn.model_selection import cross_val_predict\n\nfor i in range(1, 41):  \n\n    classifier = KNeighborsClassifier(n_neighbors=i)  \n    y_pred = cross_val_predict(classifier, X, y, cv=loo)\n    error.append(np.mean(y_pred != y))\n'
'    classifier = MultinomialNB()\n    model = classifier.fit(xTrain, yTrain)\n    yPred = model.predict_proba(xTest)\n'
"X_train, X_test, age_train, age_test, gender_train, gender_test = train_test_split(messages_bow, import_data['age'], import_data['gender'], test_size=0.20, random_state=0)\n"
'if torch.argmax(i) == label[idx]:\n    correct += 1\n    total += 1\n'
"cols=['plant', 'time','date','hour','NDVI','Treatment','Line',\n      '397.01', '398.32', '399.63', '400.93', '402.24', '403.55','1005']\n\ndf = pd.DataFrame(columns=cols)\n\nnum = pd.to_numeric(df.columns, errors='coerce')\n\ndf = df.loc[:, (num &gt; 410) | num.isna()]\nprint (df)\nEmpty DataFrame\nColumns: [plant, time, date, hour, NDVI, Treatment, Line, 1005]\nIndex: []\n\ndef f(x):\n    try:\n        return float(x)\n    except:\n        return x\n\ndf = df.rename(columns=f)\n\ndef comp(x):\n    try:\n        return x &gt; 410\n    except:\n        return True\n\n\ndf = df.loc[:, df.columns.map(comp)]\nprint (df)\nEmpty DataFrame\nColumns: [plant, time, date, hour, NDVI, Treatment, Line, 1005.0]\nIndex: []\n"
'import numpy as np\nnp.random.seed(1234)\n'
'x = 10\n-1(x)\n\ncrossentropy += -1 * (Y[i]*np.log(P[i]) + (1-Y[i])*np.log(1-P[i]))\n'
"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import Pipeline\n\ndf = pd.DataFrame({'Cat_Var': np.random.choice(['a', 'b'], size=5),\n                   'Num_Var': np.arange(5)})\n\ncat_cols = ['Cat_Var']\nnum_cols = ['Num_Var']\n\ncol_transformer = make_column_transformer(\n        (OneHotEncoder(), cat_cols),\n        remainder=StandardScaler())\n\nX = col_transformer.fit_transform(df)\n\ndf\nOut[57]: \n  Cat_Var  Num_Var\n0       b        0\n1       a        1\n2       b        2\n3       a        3\n4       a        4\n\nX\nOut[58]: \narray([[ 0.        ,  1.        , -1.41421356],\n       [ 1.        ,  0.        , -0.70710678],\n       [ 0.        ,  1.        ,  0.        ],\n       [ 1.        ,  0.        ,  0.70710678],\n       [ 1.        ,  0.        ,  1.41421356]])\n\ncol_transformer_2 = ColumnTransformer(\n        [('cat_transform', OneHotEncoder(), cat_cols)],\n        remainder='passthrough'\n        )\n\npipe = Pipeline(\n        [\n         ('col_tranform', col_transformer_2),\n         ('standard_scaler', StandardScaler())\n         ])\n\nX_2 = pipe.fit_transform(df)\n\nX_2\nOut[62]: \narray([[-1.22474487,  1.22474487, -1.41421356],\n       [ 0.81649658, -0.81649658, -0.70710678],\n       [-1.22474487,  1.22474487,  0.        ],\n       [ 0.81649658, -0.81649658,  0.70710678],\n       [ 0.81649658, -0.81649658,  1.41421356]])\n"
"'/anaconda3/bin/python'\n\n'/anaconda3/envs/myenv/bin/python'\n"
"scoring=('precision_macro', 'recall_macro', 'f1_macro')\n"
'\ny_pred2 = tf.add(tf.matmul(X,W2), b2)\n\n'
'with tf.GradientTape() as tape:\n    tape.watch(interpolated_img)\n    y_pred = critic(interpolated_img)\n'
'import sklearn.impute.SimpleImputer\n'
"transformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\ncounts = count_vectorizer.fit_transform(train['total'].values)\ntfidf = transformer.fit_transform(counts)\n\n\ntargets = train['label'].values\ntest_counts = count_vectorizer.transform(test['total'].values)\ntest_tfidf = transformer.fit_transform(test_counts)\n\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n                ('counts', CountVectorizer(ngram_range=(1, 2)),\n                ('tf-idf', TfidfTransformer(smooth_idf=False))\n            ])\n\npipeline.fit(train['total'].values)\n\ntfidf = pipeline.transform(train['total'].values)\ntargets = train['label'].values\n\ntest_tfidf = pipeline.transform(test['total'].values)\n\ndump(pipeline, 'transform_predict.joblib')\n\n#check\ntransformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\n\ntest_counts = count_vectorizer.fit_transform(test['total'].values)\ntest_tfidf = transformer.fit_transform(test_counts)\n#check\n\npipeline = load('transform_predict.joblib')\ntest_tfidf = pipeline.transform(test['total'].values)\n\npredictions = logreg.predict(test_tfidf)\n"
'from sklearn.model_selection import train_test_split\nX_train, _, y_train, _ = train_test_split(X, y, stratify=y, test_size=0.70)\n'
"class GD_SVM(BaseEstimator, ClassifierMixin):\n    def __init__(self):\n        self.sgd = SGDClassifier(loss='hinge',random_state=42,fit_intercept=True,l1_ratio=0,tol=.001)\n\n    def fit(self,X,y):\n        cv = KFold(n_splits=10,random_state=42,shuffle=True)\n        for _,test_id in cv.split(X,y):\n            xt,yt = X[test_id],y[test_id]\n            self.sgd = self.sgd.partial_fit(xt,yt,classes=np.unique(y))\n\n    def predict(self,X):\n        return self.sgd.predict(X)\n\nX,y = load_breast_cancer(return_X_y=True)\nX = StandardScaler().fit_transform(X)   #For simplicity, Pipeline is better choice\ncv = RepeatedStratifiedKFold(n_splits=5,n_repeats=5,random_state=43)\nsgd = GD_SVM()\nsvm = LinearSVC(loss='hinge',max_iter=1,random_state=42,\n                C=1.0,fit_intercept=True,tol=.001)\nr = cross_val_score(sgd,X,y,cv=cv)    #cross_val_score(svm,X,y,cv=cv)\nprint(r.mean())\n"
'model.compile(optimizer="adam", loss=keras.losses.sparse_categorical_crossentropy, metrics=["accuracy"])\n\nmodel.compile(optimizer="adam", loss=keras.losses.categorical_crossentropy, metrics=["accuracy"])\n'
'import time\n\ndef somefunc(time_started):\n    a = true\n    time_limit = 5000\n    while a: \n         # do something / run an iteration\n         if round(time.monotonic() * 1000) - time_started &gt; time_limit: \n              # check how much time/ms has elapsed since you called the function\n              a = false # will break on next "while-iteration" \n\nt0= round(time.monotonic() * 1000)\nfunc(t0)\n'
"enc = KBinsDiscretizer(n_bins=5, encode='ordinal')\nX_binned = enc.fit_transform(x)\n"
"thegmm = GMM(cvtype='tied', params='mc')\nthegmm.fit(mydata)\n"
'data = \'\'\'Language    Trustworthy\n           en   0\n           du   0\n           li   0\n           tm   0\n           en   1\n           en   0\n           en   0\n           en   1\n           fr   0\n           en   1\'\'\'\n\nimport pandas as pd\nfrom StringIO import StringIO\n\ndf = pd.DataFrame.from_csv( StringIO(data), index_col=None, sep=\'\\s+\')\n\n#--------------------------------------------------------------------\n\nprint df.groupby(\'Trustworthy\').size()\n\n\'\'\'\nTrustworthy\n0              7\n1              3\ndtype: int64\n\'\'\'\n\nfor name, group in df.groupby(\'Trustworthy\'):\n    print "name:", name, "| len:", len(group)\n\n\'\'\'\nname: 0 | len: 7\nname: 1 | len: 3\n\'\'\'\n\ndf = df.drop( df[ df[\'Trustworthy\'] == 0 ].tail(4).index )\n\nprint df\n\n  Language  Trustworthy\n0       en            0\n1       du            0\n2       li            0\n4       en            1\n7       en            1\n9       en            1\n'
"vect = TfidfVectorizer()\nX = vect.fit_transform(X)\n\nclf.predict( vect.transform(['hello']) )\n"
'Mat testSample = images[images.size() - 1];\nint testLabel = labels[labels.size() - 1];\nimages.pop_back();\nlabels.pop_back();\n// The following lines create an Fisherfaces model for\n// face recognition and train it with the images and\n// labels read from the given CSV file.\n// If you just want to keep 10 Fisherfaces, then call\n// the factory method like this:\n//\n//      cv::createFisherFaceRecognizer(10);\n//\n// However it is not useful to discard Fisherfaces! Please\n// always try to use _all_ available Fisherfaces for\n// classification.\n//\n// If you want to create a FaceRecognizer with a\n// confidence threshold (e.g. 123.0) and use _all_\n// Fisherfaces, then call it with:\n//\n//      cv::createFisherFaceRecognizer(0, 123.0);\n//\nPtr&lt;FaceRecognizer&gt; model = createFisherFaceRecognizer();\nmodel-&gt;train(images, labels);\n// The following line predicts the label of a given\n// test image:\nint predictedLabel = model-&gt;predict(testSample);\n'
'[[128 0 Fraction(1, 12) 0.08333333333333333]\n [128 0 Fraction(1, 24) 0.041666666666666664]]\n\n[[  1.28000000e+02   0.00000000e+00   8.33333333e-02]\n [  1.28000000e+02   0.00000000e+00   4.16666667e-02]]\n\ntemp_arr = [get_midi_representation(note)),\n           note.duration.dots,\n           note.duration.quarterLength,\n           float(note.duration.quarterLength.numerator) /            \n                (note.duration.quarterLength.denominator)]\n'
"import numpy as np\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\nx_train = np.array([[0,0],[2,0],[4,0],[6,0],[8,0],[10,0],[12,0],[14,0],[16,0],[0,2],\n                    [2,2],[4,2],[6,2],[8,2],[10,2],[12,2],[14,2],[16,2]])\n\ny_train = np.array([-54,-60,-62,-64,-66,-68,-70,-72,-74,-60,-62,-64,-66,\n                    -68,-70,-72,-74,-76])\n\n# This is a test set?\nx1min = 0\nx1max = 16\nx2min = 0\nx2max = 16\nx1 = np.linspace(x1min, x1max)\nx2 = np.linspace(x2min, x2max)\nx_test =(np.array([x1, x2])).T\n\ngp = GaussianProcessRegressor()\ngp.fit(x_train, y_train)\n\n# predict on training data \ny_pred_train = gp.predict(x_train)\nprint('Avg MSE: ', ((y_train - y_pred_train)**2).mean()) # MSE is 0\n\n# predict on test (?) data \ny_pred_test = gp.predict(x_test)\n# it is unclear how good this result without y_test (e.g., held out labeled test samples)\n"
"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nfor label in dataset.columns:\n    dataset[label] = LabelEncoder().fit(dataset[label]).transform(dataset[label])\n\nX = dataset.drop(['target'],axis=1)\nY = dataset['target']\n\n\nAdaBoost = AdaBoostClassifier(n_estimators=400,learning_rate=0.01,algorithm='SAMME')\n\nAdaBoost.fit(X,Y)\n\nprediction = AdaBoost.score(X,Y)\n\nprint(prediction)\n"
"model.add(Flatten())\nmodel.add(Dense(128,activation='relu'))  \nmodel.add(Dropout(0.3)) \n\n# ... the rest\n\ntrY = np.squeeze(trY, axis=2)\nteYY = np.squeeze(teYY, axis=2)\n"
"logreg.fit(X_train, y_train)\n\nX = products['review'].astype(str)\n\nlogreg.fit(X_train_dtm, y_train)\n"
'randomTestingSamples = [i for i in range(maxIndex) if i not in randomTrainingSamples]\ntestX =  X[randomTestingSamples, :]  # testing samples\ntestY =  y[randomTestingSamples, :]  # labels of testing samples nTest x 1\n'
"import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nkeras.__version__\n# '2.2.4'\n\nmodel = Sequential()\nmodel.add(LSTM(units=64, input_shape=(77, 1), output_dim=1))\n[...]\nTypeError: For the `units` argument, the layer received both the legacy keyword argument `output_dim` and the Keras 2 keyword argument `units`. Stick to the latter!\n\nmodel = Sequential()\nmodel.add(LSTM(units=64, input_shape=(77, 1)))\n\nmodel.summary()\n# result:\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, 64)                16896     \n=================================================================\nTotal params: 16,896\nTrainable params: 16,896\nNon-trainable params: 0\n_________________________________________________________________\n"
'[0 5] =&gt; 0 \n[5 10] =&gt; 1 \n'
"features=tf.train.Features(feature={'height': _int64_feature(h),\n                                    'width': _int64_feature(w),\n                                    'channels': _int64_feature(c)\n                                    'image_1': _bytes_feature(image1)\n                                    'image_2': _bytes_feature(image2)\n                                    }\n                          ))\nexample = tf.train.Example(features=tf.train.Features(feature=feature))\n\nlist = np.array([image_1, image_2,...image_n])\nimages = np.split(np.fromstring(list.tostring()), number_of_images)\n"
"model.save('my_model.h5')\n\nmodel = load_model('my_model.h5')\n"
'ret,thresh1 = cv2.threshold(img,220,255,cv2.THRESH_BINARY)\nret,thresh2 = cv2.threshold(img,220,255,cv2.THRESH_BINARY_INV)\n'
"sum([0.58502114, 0.41497886])\n# 1.0\n\nfrom sklearn.svm import SVC\nmodel = SVC(probability=True)\nX = [[1,2,3], [2,3,4]] # feature vectors\nY = [0, 1] # classes\nmodel.fit(X, Y)\n\nmodel.predict_proba(X)[0]\n# array([0.39097541, 0.60902459])\n\nmodel.classes_\n# array([0, 1])\n\nsum([0.39097541, 0.60902459])\n# 1.0\n\npred = model.predict_proba(X)\npred\n# array([[ 0.39097541,  0.60902459],\n#        [ 0.60705475,  0.39294525]])\n\nimport pandas as pd\nout = pd.DataFrame(pred[:,1],columns=['y']) # keep only the second element of the arrays in pred, i.e. the probability for class 1\nprint(out)\n\n          y\n0  0.609025\n1  0.392945\n"
"s = pd.Series(list('abca'))\n\nOutput:\n0    a\n1    b\n2    c\n3    a\n\npd.get_dummies(s)\n\nOutput:\n    a   b   c\n0   1   0   0\n1   0   1   0\n2   0   0   1\n3   1   0   0\n"
'from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nf = [[1, 2], [3.2, 4.5], [2.0, 0.75], [0.25, 3.68]]\n\nt = [1,\n3,\n2,\n0]\n\nlr = LogisticRegression().fit(f, t)\nd = DecisionTreeClassifier().fit(f, t)\nr = RandomForestClassifier().fit(f, t)\nn = KNeighborsClassifier(n_neighbors=3).fit(f, t)\n\nlr.predict(f) # array([3, 3, 2, 0])\nd.predict(f) # array([3, 3, 2, 0])\nr.predict(f) # array([3, 3, 2, 0])\nn.predict(f) # array([0, 0, 0, 0])\n'
"import cv2\n\nimg = cv2.imread('image.png')\n\ndef fourSectionAvgColor(image):\n    rows, cols, ch = image.shape\n    colsMid = int(cols/2)\n    rowsMid = int(rows/2)\n\n    numSections = 4\n    section0 = image[0:rowsMid, 0:colsMid]\n    section1 = image[0:rowsMid, colsMid:cols]\n    section2 = image[rowsMid: rows, 0:colsMid]\n    section3 = image[rowsMid:rows, colsMid:cols]\n    sectionsList = [section0, section1, section2, section3]\n\n    sectionAvgColorList = []\n    for i in sectionsList:\n        pixelSum = 0\n        yRows, xCols, chs = i.shape\n        pixelCount = yRows*xCols\n        totRed = 0\n        totBlue = 0\n        totGreen = 0\n        for x in range(xCols):\n            for y in range(yRows):\n                bgr = i[y,x]\n                b = bgr[0]\n                g = bgr[1]\n                r = bgr[2]\n                totBlue = totBlue+b\n                totGreen = totGreen+g\n                totRed = totRed+r\n\n        avgBlue = int(totBlue/pixelCount)\n        avgGreen = int(totGreen/pixelCount)\n        avgRed = int(totRed/pixelCount)\n        avgPixel = (avgBlue, avgGreen, avgRed)\n        sectionAvgColorList.append(avgPixel)\n    return sectionAvgColorList\n\nprint(fourSectionAvgColor(img))\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n"
'from sklearn.model_selection import train_test_split\n\nX_1, X_2, Y_1, Y_2 = train_test_split(X, Y, stratify=Y, test_size=0.5)\n'
'conda create --name tfpy3p6 python=3.6 -y\n\nconda activate tfpy3p6\n\nconda install -c anaconda cudatoolkit==9.0\nconda install -c anaconda cudnn\n\npip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/gpu/tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl\n\nconda install ipython\nconda install notebook==5.7.5\nipython kernel install --user\n'
"scoring='neg_mean_squared_error'\n"
"if regression_type == 'LASSO':\n    # Declare Lasso loss function\n    # Lasso Loss = L2_Loss + heavyside_step,\n    # Where heavyside_step ~ 0 if A &lt; constant, otherwise ~ 99\n    lasso_param = tf.constant(0.9)\n    heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(A, lasso_param)))))\n    regularization_param = tf.multiply(heavyside_step, 99.)\nloss = tf.add(tf.reduce_mean(tf.square(y_target - model_output)), regularization_param)\n"
"train['Sex'].replace(['female', 'male'], [0, 1])\n\ntrain['sex'] = train['Sex'].replace(['female', 'male'], [0, 1])\n"
"import tensorflow as tf\nA = tf.get_variable(name='foo', shape=[3, 3], dtype=tf.float16)\ndense = tf.layers.dense(inputs=A, units=3)\nvaris = tf.trainable_variables(scope=None)\nprint(varis[1])  # &lt;tf.Variable 'dense/kernel:0' shape=(3, 3) dtype=float16_ref&gt;\n"
"model.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
'def custom_mean_squared_loss(y_true, y_pred):\n    diff = tf.abs(y_true - y_pred)\n    angle_diff = tf.minimum(diff[:, :, 6:], 360 - diff[:, :, 6:])\n    error = tf.concat([diff[:, :, :6], angle_diff], axis=-1)\n    return tf.mean(error ** 2, axis=-1)\n'
"from sklearn.metrics.pairwise import cosine_similarity\ncosine_similarities = cosine_similarity(x, v.transform(['user input'])).flatten()\nbest_match_index = cosine_similarities.argmax()\n"
"from sklearn.metrics import mean_squared_error\ncv_mse = []\n\nfor train_index, val_index in kfold.split(X):\n    history = estimator.fit(X[train_index], y[train_index])\n    pred = estimator.predict(X[val_index])\n    err = mean_squared_error(y[val_index], pred)\n    cv_mse.append(err)\n    plt.plot(history.history['loss'])\n"
"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\n\n# Importing the dataset\n# dataset = pd.read_csv('data.csv')\ndataset = pd.read_csv('data.csv')\nX = dataset.iloc[:, 1:2].values\ny = dataset.iloc[:, 2].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Fitting Polynomial Regression to the dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=4)\nX_poly = poly_reg.fit_transform(X)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y)\n\n# Visualizing the Polymonial Regression results\ndef viz_polymonial():\n    plt.scatter(X, y, color='red')\n    indices = np.argsort(X[:, 0])\n    plt.scatter(X, pol_reg.predict(poly_reg.fit_transform(X)), color='green')\n    plt.plot(X[indices], pol_reg.predict(poly_reg.fit_transform(X))[indices], color='black')\n    plt.title('Polynomial Regression for CPU')\n    plt.xlabel('Time range')\n    plt.ylabel('Consume')\n    plt.show()\n    return\nviz_polymonial()\n\n# 20 = time\nprint(pol_reg.predict(poly_reg.fit_transform([[20]])))\n"
"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(corpus)\n"
'pickle.dump(bow_transformer, open("vector.pkl", "wb"))\n\nbow_transformer = CountVectorizer().fit_transform(df[\'message\'])\n\nbow_transformer = CountVectorizer().fit(df[\'message\'])\nbow_transformer_dtm = bow_transformer.transform(df[\'message\'])\n\npickle.dump(bow_transformer, open("vector.pkl", "wb"))\n\nselector = pickle.load(open("vector.pkl", "rb"))\ntest_set=["heloo how are u"]\nnew_test=selector.transform(test_set)\n'
"from sklearn.pipeline import FeatureUnion, Pipeline\n\nprepare_select_pipeline = Pipeline([\n    ('preparation', full_pipeline),\n    ('feature_selection', TopFeatureSelector(feature_importances, k))\n])\n\nfeats = FeatureUnion([('prepare_and_select', prepare_select_pipeline)])\n\nprepare_select_and_predict_pipeline = Pipeline([('feats', feats),\n                               ('svm_reg', SVR(**rnd_search.best_params_))])\n"
'model.add(layers.Embedding(input_dim=vocab_size+1,\n      output_dim=embedding_dim,\n      mask_zero=True))\n'
"from sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=(32893+4176), n_classes=2, weights=[0.88, 0.12])\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\nprint(pd.DataFrame(y_pred).astype('category').describe())\n\n# output\ncount   7414\nunique     1\ntop        0\nfreq    7414\n\nUse `zero_division` parameter to control this behavior.\n"
"predicted = model.predict(featuresHyderabad)\npredictedArr = np.zeros(shape = ( nBands, imageHeight, imageWidth ))\nfor i in range(nBands):\n  prediction = np.reshape(predicted[:,i], (imageHeight, imageWidth))\n  predictedArr[i] = prediction\n\nraster.export(predictedArr, metaData, filename=outFile, dtype='float', bands='all')\n"
'model_ft = models.resnet50(pretrained=True)\nct = 0\nfor child in model_ft.children():\n    ct += 1\n    if ct &lt; 7:\n        for param in child.parameters():\n            param.requires_grad = False\n'
"X = ['SB-01_0-1_20200701', '11-22-4334', 'MW-01_20200621', 'Benzene']\nX = [[ord(c) for c in x] for x in X]\nX = [x + [0] * (20 - len(x)) for x in X]\nX = np.array(X)\n\nDNNmodel = keras.Sequential([\n    keras.layers.Dense(20),  #input layer size\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(1)   #output layer size\n])\n"
"from sklearn.preprocessing import OneHotEncoder\n\nenc = OneHotEncoder(handle_unknown='ignore')\nX = [['Male', 1], ['Female', 3], ['Female', 2]]\nenc.fit(X)\nenc.categories_\n\n[array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n\nenc.transform([['Female', 1], ['Male', 4]]).toarray()\n\narray([[1., 0., 1., 0., 0.],\n       [0., 1., 0., 0., 0.]])\n"
"X = [[0], [1], [2], [3]]\ny = ['zero', 'zero', 'one', 'one']\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X, y)\n\nprint(neigh.predict([[3]]))\n\n#output\n#array(['one'], dtype='&lt;U4')\n"
"from sklearn.svm import SVR\nsvr = SVR()\n\nparam_grid = {&quot;kernel&quot; : ['linear', 'poly', 'rbf', 'sigmoid'],\n             &quot;max_iter&quot; : [1,10,20],\n             'C' : np.arange(0,20,1)} \n\nmodel = GridSearchCV(estimator = svr, param_grid = param_grid, \n                     cv = 5, verbose = 3, n_jobs = -1)\nm1 = model.fit(X_train, y_train)\n"
"out=basemodel.layers[-1].output \noutput = layers.Dense(1, activation='sigmoid')(out)\n"
"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.datasets import load_iris\n\npipeline = Pipeline([\n    ('dim_reduction', PCA()),\n    ('clf', LogisticRegression()),\n])\nparameters = [\n    {\n        'clf': (LogisticRegression(),),\n        'clf__C': (0.001,0.01,0.1,1,10,100)\n    }, {\n        'clf': (RandomForestClassifier(),),\n        'clf__n_estimators': (10, 30),\n    }\n]\ngrid_search = GridSearchCV(pipeline, parameters)\n\n# some example dataset\nX, y = load_iris(return_X_y=True)\nX_train, X_tes, y_train, y_test = train_test_split(X, y)\ngrid_search.fit(X_train, y_train)\n"
'result = cross_validate(tree, data.data, data.target, cv=5, return_train_score=True)\n\nresult = cross_validate(tree, data.data, c, cv=5, return_train_score=True)\n'
"onehotencoder = OneHotEncoder()\none_hot = onehotencoder.fit_transform(X[:,0:1]).toarray()\n\nfrom sklearn.compose import ColumnTransformer\n\nct = ColumnTransformer([(&quot;country&quot;, OneHotEncoder(), [0])], remainder = 'passthrough')\nX = ct.fit_transform(X)\n"
'reverse_top_10_varieties = {idx:i[0] for idx, i in enumerate(counter.most_common(50))}\n[reverse_top_10_varieties[id] for id in y_pred]\n'
'model_json = model.to_json()\nwith open(&quot;my_model.json&quot;, &quot;w&quot;) as json_file:\n    json_file.write(model_json)\n'
'X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3)\n\nX_train_sampled , y_train_sampled = sm.fit_sample(X_train,y_train.ravel())\n\nmodel.fit(X_train_sampled,y_train_sampled) \n\nmodel.predict(X_test)\n'
'weights -= lr * sum(X*(y_hat - y)) / N\n\nweights -= lr * X*(y_hat - y) / N\n'
"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import  load_model\nimport numpy as np\nimport cv2\nimport os\nmodel_location =r'c:\\temp\\people\\Mobilenet-99.00.h5' # location of the saved model\nmodel=load_model(model_location) # load the saved model\nimage_location=r'c:\\temp\\people\\storage' # directory storing the images that you want to predict\nfile_list=os.listdir(image_location)  # list of files\nfor f in file_list: # iterate through the files in the directory list\n    f_path=os.path.join(image_location, f)  # create the path to the image file\n    img=cv2.imread(f_path)    # read in the image - note cv2 reads in images in BGR format\n    img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # model was trained on RGB images so convert to RGB\n    img=cv2.resize(img, (128,128)) # model was trained on images of size 128  X 128 X 3 so resize the images\n    img=img/127.5-1 # model was trained with pixel value scalled between -1 to +1 so convert the pixel range    \n    img=np.expand_dims(img, axis=0) # model predict expects the input to have dimension (batch_size, width, height, bands)\n    #print (img.shape)  # uncomment to see effect of expanding dimension\n    prediction =model.predict (img, batch_size=1, verbose=0) # make predictions    \n    pred=np.argmax(prediction)# find the index of the column with the highest probability\n    print ('for file ', f_path, ' the index of the predicted class is ', pred, ' with a probability of ', prediction[0][pred]  )\n"
"test_data_generator = ImageDataGenerator(rescale=1./255).flow_from_dataframe(dataframe = df,\n                                                                                  directory=test_path,\n                                                                                  x_col = &quot;id&quot;,\n                                                                                  y_col = &quot;label&quot;,\n                                                                                 class_mode = 'binary',\n                                                                                  target_size=(96,96),\n                                                                                  batch_size=16,\n                                                                                  shuffle=False)\n"
'G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n  logits=logits_fake, labels=labels_fake))\n\nG_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n  logits=logits_fake, labels=tf.ones_like(logits_fake)))\n'
"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npipeline = Pipeline([\n    ('poly', PolynomialFeatures(degree=5, include_bias=False)),\n    ('linreg', LinearRegression(normalize=True))\n    ])\n\npipeline.fit(X_train, y_train)\npipeline.predict(np.array([4, 5, 6, 7]).reshape(1, -1))\n"
"cv = CountVectorizer()\ntrain=['Hi this is stack overflow']\ncv.fit(train)\ncv.get_feature_names()\n\ntest=['Hi that is not stack overflow']\ncv.fit(test)\ncv.get_feature_names()\n"
"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata = pd.read_csv('classifications.csv',\n                    encoding='latin1',\n                    error_bad_lines=False,\n                    delimiter=';')\n\ndata.columns = ['desc', 'value', 'label']\ndata['label'] = data['label'].astype('category')\ndata.info()\nvectorizer = TfidfVectorizer()\nvectors    = vectorizer.fit_transform(data['desc'])\nprint('Shape: ',vectors.shape)\n\nclf = RandomForestClassifier(random_state=42)\n\nclf.fit(vectors,data['label'])\nprint('Score: {}'.format(clf.score(vectors,data['label'])))\nclf.predict(vectorizer.transform(data['desc']))\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4 entries, 0 to 3\nData columns (total 3 columns):\ndesc     4 non-null object\nvalue    4 non-null float64\nlabel    4 non-null category\ndtypes: category(1), float64(1), object(1)\nmemory usage: 340.0+ bytes\nShape:  (4, 14)\nScore: 1.0\narray(['Restaurants', 'Bars', 'Supermarket', 'Salary'], dtype=object)\n\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('vectorizer',TfidfVectorizer()),\n                     ('classifier',RandomForestClassifier())])\n"
'[Some trace info] - Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.\n'
"hyperparameter_dictionary = {'boosting_type': 'goss', 'num_leaves': 25, 'n_estimators': 184}\nmodel = lightgbm.LGBMClassifier(**hyperparameter_dictionary)\n\nprint(model)\n\nLGBMClassifier(boosting_type='goss', ... n_estimators=184, n_jobs=-1, num_leaves=25,...)\n"
'import tensorflow as tf\nimport numpy as np\n\n\n# Teach how to multiply\ndef generate_data(how_many):\n    data = np.random.rand(how_many, 2)\n    answers = data[:, 0] * data[:, 1]\n    return data, answers\n\n\nsess = tf.InteractiveSession()\n\ninput_data = tf.placeholder(tf.float32, shape=[None, 2])\ncorrect_answers = tf.placeholder(tf.float32, shape=[None])\n\nweights_1 = tf.Variable(tf.truncated_normal([2, 1], stddev=.1))\nbias_1 = tf.Variable(.0)\n\noutput_layer = tf.matmul(input_data, weights_1) + bias_1\n\nmean_squared = tf.reduce_mean(tf.square(correct_answers - tf.squeeze(output_layer)))\noptimizer = tf.train.GradientDescentOptimizer(.1).minimize(mean_squared)\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(1000):\n    x, y = generate_data(100)\n    sess.run(optimizer, feed_dict={input_data: x, correct_answers: y})\n\nerror = tf.reduce_sum(tf.abs(tf.squeeze(output_layer) - correct_answers))\n\nx, y = generate_data(100)\nprint("Total Error: ", error.eval(feed_dict={input_data: x, correct_answers: y}))\n'
'with tf.Graph().as_default():\n    #define model here\n'
'nps = np.array(all_s)\nnpd = [dd for dd in all_d]\nbox=sns.boxplot(data=nps.T) \nbox.set_xticklabels(npd)\nplt.show()\n'
'x1,x2,x3,z\n5.5,0.5,4.5,2\n7.4,1.1,3.6,0\n5.9,0.2,3.4,2\n9.9,0.1,0.8,0\n6.9,-0.1,0.6,2\n6.8,-0.3,5.1,2\n4.1,0.3,5.1,1\n1.3,-0.2,1.8,1\n4.5,0.4,2.0,0\n0.5,0.0,2.3,1\n5.9,-0.1,4.4,0\n9.3,-0.2,3.2,0\n1.0,0.1,2.8,1\n0.4,0.1,4.3,1\n2.7,-0.5,4.2,1\n\nlibrary("caret")\nmy.dataframe &lt;- read.csv("myExample.csv", header = T, sep =",")\nfit &lt;-  train(z ~ .,  data = my.dataframe, method = "lm")\nfit\n\nlibrary("caret")\nmy.dataframe &lt;- read.csv("myExample.csv", header = T, sep =",")\nfit &lt;-  train(z ~ .,  data = my.dataframe, method = "rf")\nfit\n'
'import numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nData = np.array([-108.604150711185, -131.880188127745,-18.3017441809734, 32.011639982337, -71.6651360870381, -107.587087751331, 21.316311739316, -36.015324564807, 138.22302265079, 47.9322592065447, -129.007749732555, -150.41808326425, -141.00589707504, -105.912063885407, 76.2956568174239, 141.457541434218, -20.6676395937811, -226.505644333494, -151.229861588686, -160.18717733968, -107.01667849677, -7.52794131287047, -93.1147621027003, 5.59630172385392, 38.741091785708, -32.9061390503546, -78.5031246062325, -9.64080356337477, -54.1430873201472, -108.127067430103, -12.2589074567133, 129.212940940854, 132.670728015743, 107.075153550768, 167.176831103164, -20.6839530330714, 102.677911281291, -109.423698849103, -154.454318421757, 140.52342226202, 110.184351332211, -16.6842057565239, -11.1688984829787, 178.441845032635, 37.0689292040101, 166.610506783818, -79.2764182099804, 99.1136693164655, 82.0929274697289, 15.1752041486536, 178.489001782771, 145.332200036106, -185.977800430997, -90.5440753976243, 78.0459300120412, 144.297553387967, 99.5945824957091, 110.803195137024, 81.3094331750562,-396.825240330405, -166.038928089807, -78.863983688682, 138.309908804212, -148.647304302406, -2.23135233624276, 129.411511929621, -111.664324254549, -96.4151180340831, 129.219227225386, 90.7050615157428, 141.986869866474, 93.0147970463941, 142.807435791073, -75.8426755946232, 122.537973092667, 117.078515092191, 134.166968023265, 90.8512172789568, 146.367129646428, 125.539182526718, -70.485058023267, -46.967575223949, 116.210349687502, -91.2992704167832, 104.052231138142, -114.580693287221, -82.9991067628608, -111.649187979413])\nData = scaler.fit_transform(Data)\nClass = np.array([0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0])\ndf_training = np.array([Data, Class])\n\nData = np.array([133.75999742845, 22.9386702890105, -126.959902277009, -116.317935595297, -33.9418594804197, -49.0102540773413, -159.266630498512, -8.92296705690401, 114.328300224712, 66.0706175847251, -154.385344188283, 70.7868284982941, -28.334490887314, 118.755307949047, 154.362286178401, 101.331675190569, 96.2196681290104, 99.5694296232446, 210.160787371823, 65.8474210711036, -125.475676456606, 66.7541385125748, -161.001356357477, -40.1416817172267, 38.6877489907967, -7.12706914419719, -10.3967176519225, -80.6831091111636, 128.604227270616, 75.4219966516171, 184.951786958864, 90.9170782990185, 66.7190886024699, 81.377280661573, -82.4053965286415, -65.6718687269108, 61.1679518726262, 190.532649096311, 199.917670153196, 104.558442558929, 113.747065157369, 106.640501329133,80.593201532054, 75.0176280888154, 155.538654396817, 30.0548798029353, 116.900219512636, 131.431417509576, 33.3308447581156, -121.191534016935, -80.4203785670198, 157.737407847885, 66.5956228628815, 50.8340706561446, -113.713450848071, -18.7787225270887, 113.832326071127, -45.5884280143408, 221.782395098832, 70.1660982367319, 235.005982636939, 80.8180320055801, -74.7107276814795, 133.925782624001, 97.9261686360971, -127.954532027281, 58.9295075974962, 96.1702797891484, -49.6048543914143, -42.1842037639683, -235.694708213157, 13.4862841916787, 126.396462591781, 214.297316240176, 125.148658464391, 84.8887673204376, 78.2717096234718, 139.677936314095, -168.649300541479, 103.40253638232, 69.2727189156141, 153.017155534869, -238.07168745534, -166.929968475244, 113.414489211719,85.5520123243496, 120.582346886614, -214.850084749638, 96.8090523924549])\nData = scaler.fit_transform(Data)\nClass = np.array([1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1])\ndf_test = np.array([Data, Class])\n\n# train model                                                                                                                                                 \nclf = SVC(verbose=True, gamma=0.01, kernel=\'rbf\', C=1)                                                     \n\n# make predictions\nclf.fit(df_training[0].reshape(88,1), df_training[1].reshape(88,1))\nTrainingPredictions = clf.predict(df_training[0].reshape(88,1))\nTestPredictions = clf.predict(df_test[0].reshape(89,1))\nSkill = np.sum(TestPredictions==df_test[1])/float(len(TestPredictions))\nprint("Skill: "+str(Skill)) #value is 0.84\n'
'cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n'
"tf_1d_input = tf.Variable([0.1, 1.0, 2.0, 3.0]) \ntf_4d_reshape = tf.reshape(tf_1d_input, [1, 2, 2, 1])\nw = tf.zeros([3, 3, 1, 10])\ntf_conv_layer = tf.nn.conv2d(tf_4d_reshape, w, [1, 1, 1, 1], 'SAME') # no error anymore\n"
'mlp.fit(X_train, y_train.values.reshape(len(y_train), 1))\n'
'X = []\nfor x in range(100):\n    X.append([[x], [x+1], [x+2], [x+3]])\nX = np.array(X)\n\n\nnp.random.shuffle(X)\n\ntrainX = X[:30, 0:-1, :]\ntestX = X[30:, 0:-1, :]\n\ntrainY = X[:30, -1, :]\ntestY = X[30:, -1, :]\n\nlastDense = X.shape[2]\n\nin_out_neurons = trainX.shape[1]\nhidden_neurons = 100\n\nmodel = Sequential()\nmodel.add(LSTM(in_out_neurons, return_sequences=False, input_shape=( in_out_neurons,1)))\nmodel.add(Dense(hidden_neurons))\nmodel.add(Dense(lastDense))\nmodel.add(Activation("linear"))\nmodel.compile(loss="mse", optimizer="adam", lr=.1)\n\nmodel.fit(trainX, trainY,validation_split=0.05, epochs=2000, batch_size=trainX.shape[0])\n\nprint (\'train\')\nprint (trainX)\nprint (model.predict(trainX))\n\nprint (\'test\')\nprint (testX)\nprint (model.predict(testX))\n'
'cross_entropy = tf.reduce_mean(\n   tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n\nprediction = tf.softmax(y_conv)\n'
'from sklearn.datasets import load_iris\nfrom sklearn.datasets import dump_svmlight_file\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n""" only keep first two classes """\nindices = y&lt;=1\nX = X[indices]\ny = y[indices]\n\n""" transform to +1 / -1 targets (0 -&gt; -1) """\ny[y==0] = -1\n\ndump_svmlight_file(X, y, \'my_dataset\', zero_based=False)  # 1-based!!!\n\n./svm_learn my_dataset my_output -v3\nScanning examples...done\nReading examples into memory...100..OK. (100 examples read)\nSetting default regularization parameter C=0.0199\nOptimizing...............done. (16 iterations)\nOptimization finished (0 misclassified, maxdiff=0.00057).\nRuntime in cpu-seconds: 0.00\nNumber of SV: 32 (including 28 at upper bound)\nL1 loss: loss=4.89469\nNorm of weight vector: |w|=0.69732\nNorm of longest example vector: |x|=9.13674\nEstimated VCdim of classifier: VCdim&lt;=31.50739\nComputing XiAlpha-estimates...done\nRuntime for XiAlpha-estimates in cpu-seconds: 0.00\nXiAlpha-estimate of the error: error&lt;=30.00% (rho=1.00,depth=0)\nXiAlpha-estimate of the recall: recall=&gt;70.00% (rho=1.00,depth=0)\nXiAlpha-estimate of the precision: precision=&gt;70.00% (rho=1.00,depth=0)\nNumber of kernel evaluations: 1291\nWriting model file...done \n'
"def compute_precision(predictions, labels):\n    '''Compute classification precision with a fixed threshold on distances.\n    '''\n    return labels[predictions.ravel() &lt; 0.5].mean()\n\n\ndef compute_accuracy(predictions, labels):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return np.mean(np.equal(predictions.ravel() &lt; 0.5, labels))\n"
"clf.set_params(n_estimators = len(clf.estimators_) + 40, warm_start = True)\nclf.fit(X, Y)\njoblib.dump(clf, 'classifier/classifier.pkl')\n"
"with open('dataset/s01.dat', 'rb') as f:\n     x = cPickle.load(f, encoding='latin1')\n"
'from sklearn.externals.joblib import Memory\nfrom sklearn.datasets import load_svmlight_file\nmem = Memory("./mycache")\n\n@mem.cache\ndef get_data():\n    data = load_svmlight_file("mysvmlightfile")\n    return data[0], data[1]\n\nX, y = get_data()\n'
'def make_dict(list1,list2):\n    d = {}\n    for k,v in zip(list1,list2):\n        d[k] = v\n    return d\n\ndf2[\'word\'].apply(lambda x : (x.split(" "), [i for i in reversed(range(1,len(x.split(" "))+1))])).apply(lambda y : make_dict(y[0],y[1])) \n\nIn [1]: df2[\'word\'].apply(lambda x : (x.split(" "), [i for i in reversed(range(1,len(x.split(" "))+1))]))\nOut[1]:\n0        ([one, two, four], [3, 2, 1])\n1       ([five, six, nine], [3, 2, 1])\n2    ([eight, eleven, ten], [3, 2, 1])\n\nIn [2]: def make_dict(list1,list2):\n    ...:     d = {}\n    ...:     for k,v in zip(list1,list2):\n    ...:         d[k] = v\n    ...:     return d\n\nIn [3]: df2[\'word\'].apply(lambda x : (x.split(" "), [i for i in reversed(range(1,len(x.split(" "))+1))])).apply(lambda y : make_dict(y[0],y[1]))\nOut[3]:\n0        {\'one\': 3, \'two\': 2, \'four\': 1}\n1       {\'five\': 3, \'six\': 2, \'nine\': 1}\n2    {\'eight\': 3, \'eleven\': 2, \'ten\': 1}\nName: word, dtype: object\n'
'pipeline.fit(X_train, y_train, mlp__validation_split=0.3)\n'
"import numpy as np\nimport tensorflow as tf\n\nx = tf.placeholder(dtype=tf.float32, shape=[None, 2], name='x')\nlayer = tf.nn.relu(x)\nslice = layer[:, 0]\nactivation = tf.log(1 + tf.exp(slice))\n\nwith tf.Session() as session:\n  session.run(tf.global_variables_initializer())\n  layer_val, slice_val, activ_val = session.run([layer, slice, activation],\n                                                feed_dict={x: np.random.randn(10, 2)})\n  print layer_val[:, 0]\n  print slice_val\n  print activ_val\n"
'data = np.array([[1,2] , [1,5], [2,3], [2,3], [2,3], [2,4]])\ndf1 = pd.DataFrame(data[:-2,:], columns=columns) # your data\ndf2 = pd.DataFrame(data, columns=columns) # my data\n\n# your method applied to my data\nprint 1 / df2.groupby(\'A\')[\'B\'].nunique()\n\nA\n1    0.5\n2    0.5\nName: B, dtype: float64\n\n# information gain method\n\ndef entropy(data):\n    """Compute entropy of a set of values"""\n    bin_cnt  = np.bincount(data)\n    bin_prob = bin_cnt / np.sum(bin_cnt)\n    entropy = - np.sum(bin_prob * np.ma.log2(bin_prob))\n    return entropy\n\n# using your data\nprint entropy(df1[\'B\']) - df1.groupby(\'A\')[\'B\'].apply(entropy)\n\nA\n1    0.5\n2    1.5\nName: B, dtype: float64\n\n# Using my data\nprint entropy(df2[\'B\']) - df2.groupby(\'A\')[\'B\'].apply(entropy)\n\nA\n1    0.792481\n2    0.981203\nName: B, dtype: float64\n'
'               filter = tf.zeros([3, 16, 16])\n                W = tf.Variable(tf.truncated_normal(filter, stddev=0.1), name="W")\n                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name="b")\n                conv = tf.nn.conv1d(\n                    input_values,\n                    W,\n                    strides=2,\n                    padding="VALID",\n                    name="conv")\n                # nonlinearity operation\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding=\'VALID\',\n                    name="pool")\n                pooled_outputs.append(pooled)\n'
'import xgboost as xgb\n\ndtrain = xgb.DMatrix(X_train_np, label=y_train_np)\ndtest = xgb.DMatrix(X_test_np, label=y_test_np)\n\n# Here we set eval_metric to be \'auc\' as well as other hypter parameters of xgboost\nparam0 = [\n    (\'max_depth\', 4),\n    (\'eta\', 0.1),\n    (\'objective\', \'binary:logistic\'),\n    (\'min_child_weight\', 4),\n    (\'silent\', 1),\n    (\'eval_metric\', \'auc\'),\n    (\'subsample\', 0.75),\n    (\'colsample_bytree\', 0.75),\n    (\'gamma\', 1),\n]\n\nwatchlist = [(dtrain, "trn"), (dtest, "tst")]\nn_estimators = 100\n\n# This is the same as fitting\nmodel = xgb.train(param0, dtrain, n_estimators , evals=watchlist)\n'
'def custom_loss(y_true, y_pred):\n    bs = 1 # batch size\n    l = 4 # label number\n    c = K.constant([[x for x in range(l)] for y in range(bs)], shape=((bs, l)))\n\n    truths = tf.multiply(y_true, c)\n    truths = K.sum(truths, axis=1)\n    truths = K.concatenate(list(truths for i in range(l)))\n    truths = K.reshape(truths, ((bs,l)))\n\n    distances = tf.add(truths, -c)\n    sqdist = tf.multiply(distances, distances)\n\n    out = tf.multiply(y_pred, sqdist)\n    out = K.sum(out, axis=1)\n\n    return K.mean(out)\n\ny_true = K.constant([0,0,1,0])\ny_pred = K.constant([1,0,0,0])\nprint(custom_loss(y_true, y_pred)) # tf.Tensor(4.0, shape=(), dtype=float32)\n'
'confusion = tf.confusion_matrix(\n     labels = tf.argmax( mnist.test.labels, 1 ),\n     predictions = tf.argmax( pred, 1 ) )\n\nprint(confusion.eval({x:mnist.test.images, y:mnist.test.labels}))\n'
'import pandas as pd\nprobsd = pd.DataFrame(clf.predict_proba(Xtest_tfidf))\ntop_100_class_0_tweets = probsd.sort_values(0, ascending=False).head(100).index\n'
"model.add(YourLastLayer(..., activation='tanh'))\nmodel.add(Lambda(lambda x: x * 5))\n"
'output_tensor = tf.concat([\n    x[:,0:self.idx1],\n    x_left,\n    x[:, self.idx1+1:self.idx2],\n    y_right,\n    x[:, (self.idx2+1):]\n], axis = 1)\n\nprint("self.idx1: %s" % self.idx1)\nprint("self.idx2: %s" % self.idx2)\nprint("x[:,0:self.idx1]: %s" % x[:,0:self.idx1].shape)\nprint("x_left: %s" % x_left.shape)\nprint("x[:, self.idx1+1:self.idx2]: %s" %\n      x[:, self.idx1+1:self.idx2].shape)\nprint("x_right.shape: %s" % x_right.shape)\nprint("y_right: %s" % y_right.shape)\nprint("x[:, (self.idx2+1):]: %s" % x[:, (self.idx2+1):].shape)\nprint("output_tensor.shape: %s" % output_tensor.shape)\n\nself.idx1: 0\nself.idx2: 1\nx[:,0:self.idx1]: (1000, 0)\nx_left: (1000, 1)\nx[:, self.idx1+1:self.idx2]: (1000, 0)\nx_right.shape: (1000, 1)\ny_right: (1000, 1)\nx[:, (self.idx2+1):]: (1000, 0)\noutput_tensor.shape: (1000, 2)\n\nself.idx1: 1\nself.idx2: 0\nx[:,0:self.idx1]: (1000, 1)\nx_left: (1000, 1)\nx[:, self.idx1+1:self.idx2]: (1000, 0)\nx_right.shape: (1000, 1)\ny_right: (1000, 1)\nx[:, (self.idx2+1):]: (1000, 1)\noutput_tensor.shape: (1000, 4)\n\nself.idx1: 0\nself.idx2: 1\nx[:,0:self.idx1]: (1000, 0)\nx_left: (1000, 1)\nx[:, self.idx1+1:self.idx2]: (1000, 0)\nx_right.shape: (1000, 1)\ny_right: (1000, 1)\nx[:, (self.idx2+1):]: (1000, 2)\noutput_tensor.shape: (1000, 4)\n\nself.idx1: 0\nself.idx2: 1\nx[:,0:self.idx1]: (1000, 0)\nx_left: (1000, 1)\nx[:, self.idx1+1:self.idx2]: (1000, 0)\nx_right.shape: (1000, 1)\ny_right: (1000, 1)\nx[:, (self.idx2+1):]: (1000, 2)\noutput_tensor.shape: (1000, 4)\n'
'df[\'counts\'] = df.groupby([\'PatientNumber\',\'year\'])["month"].transform(\'nunique\')\n'
"from sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.datasets import make_regression\nimport numpy as np\n\n# dummy data:\nX, y = make_regression(n_samples=20, n_features=5, n_targets=1)\nX.shape\n# (20, 5)\n\nkernel = RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\ngp = GaussianProcessRegressor(kernel=kernel)\ngp.fit(X, y)\n# GaussianProcessRegressor(alpha=1e-10, copy_X_train=True,\n#             kernel=RBF((length_scale=1), n_restarts_optimizer=0,\n#             normalize_y=False, optimizer='fmin_l_bfgs_b',\n#             random_state=None)\n\nkernel = RBF(length_scale=[1.0, 2.0], length_scale_bounds=[(1e-1, 10.0), (1e-2, 1.0)])\n"
'model.add(LSTM(num_units,input_shape=(None, data_dim));\n\n# if\nx = [t0,t1,t2,t3,t4]\n#then \ny = [t1,t2,t3,t4]\n\ny = x[1:]\n\nx = x[:-1]\n'
'c1_pix, _, _ = tf.split(tf.equal(op_img,[[[255,255,255]]]), 3, axis=-1)\nc2_pix = tf.logical_not(c1_pix)\nnew_op_img = tf.concat([c1_pix, c2_pix], -1)\n'
'sess.run(train_step,feed_dict={X: x_train[l:t], Y_: y_train[l:t]})\n'
'import numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nmnist = input_data.read_data_sets("MNIST_data/", one_hot=False)\n\ndigitDict = {}\n\nfor i in range(10):\n    mask = (mnist.train.labels == i)\n    digitDict[i] = mnist.train.images[mask]\n\nfor i in range(10):\n    print("Digit {0} matrix shape: {1}".format(i,digitDict[i].shape))\n'
"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnum_points = 200\na = 0.22\nb = 0.78\n# No need to create everything in a loop, np.random.normal takes a size parameter\nx_points = np.random.normal(0.0, 0.5, 200)\ny_points = a*x_points + b + np.random.normal(0.0, 0.1, 200)\nplt.plot(x_points, y_points, 'o', label='Input Data')\nplt.title('Linear Regression')\n\nA = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\nB = tf.Variable(tf.zeros([1]))\nY = tf.add(tf.multiply(A, x_points), B)\n\n# Always see whether the API you are using provides you with the implementation you need. It is more stable, and it will save you a lot of trouble with debugging like right now\ncost_function = tf.losses.mean_squared_error(Y, y_points)\noptimizer = tf.train.GradientDescentOptimizer(0.5)\nlinear_reg = optimizer.minimize(cost_function)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for step in range(0, 21):\n        sess.run(linear_reg)\n        if (step % 5) == 0:\n            plt.plot(x_points, y_points, 'o', label='step ={}'.format(step))\n            plt.plot(x_points, sess.run(A)*x_points + sess.run(B))\n            plt.legend()\n            plt.show()\n"
"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.DataFrame({\n        'Country': ['AB', 'CD', 'EF', 'FG']*20,\n        'ColumnA' : [1]*20*4,'ColumnB' : [10]*20*4, 'Label': [1,0,1,0]*20\n    })\n\ndf['Country_Code'] = df['Country'].astype('category').cat.codes\n\nX = df.loc[:, df.columns.drop(['Label','Country'])]\ny = df['Label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=df.Country_Code)\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nlm_predictions = lm.predict(X_test)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import preprocessing\n\ndf = pd.DataFrame({\n        'Country': ['AB', 'CD', 'EF', 'FG']*20,\n        'ColumnA' : [1]*20*4,'ColumnB' : [10]*20*4, 'Label': [1,0,1,0]*20\n    })\n\n# Train-Validation \nle = preprocessing.LabelEncoder()\ndf['Country_Code'] = le.fit_transform(df['Country'])\nX = df.loc[:, df.columns.drop(['Label','Country'])]\ny = df['Label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=df.Country_Code)\nlm = LinearRegression()\nlm.fit(X_train,y_train)\n\n# Test\ntest_df = pd.DataFrame({'Country': ['AB'], 'ColumnA' : [1],'ColumnB' : [10] })\ntest_df['Country_Code'] = le.transform(test_df['Country'])\nprint (lm.predict(test_df.loc[:, test_df.columns.drop(['Country'])]))\n"
'import pickle\n# To save\npickle.dump(model, "model.p")\n# To load again\nwith open(\'model.p\', \'r\') as fp:\n    model = pickle.load(fp)\n'
'3056/3056 [==============================] - 2s 576us/step - loss: 0.4731 - acc: 0.9876 - val_loss: 54.2266 - val_acc: 0.4088\n'
'model.fit_generator(batch_generator(X, y, 10),\n                    verbose = 1,\n                    steps_per_epoch = int(len(X) / batch_size),\n                    epochs = 20,\n                    validation_data = (X_test, y_test),\n                    callbacks = callbacks_list)\n'
'from PIL import Image\nimg = Image.open("image_file_path").convert(\'L\').resize((28, 28), Image.ANTIALIAS)\nimg = np.array(img)\nmodel.predict(img[None,:,:])\n'
"# Iterates through the columns and fixes any NaNs\ndef remove_nan(df):\n    replace_dict = {}\n\n    for col in df.columns:\n\n        # If there are any NaN values in this column\n        if pd.isna(df[col]).any():\n\n            # Replace NaN in object columns with 'N/A'\n            if df[col].dtypes == 'object':\n                replace_dict[col] = 'N/A'\n\n            # Replace NaN in float columns with 0\n            elif df[col].dtypes == 'float64':\n                replace_dict[col] = 0\n\n    df = df.fillna(replace_dict)\n\n    return df\n\n| House Type |\n| ---------- |\n| Mansion    |\n| Ranch      |\n\n| House Type |\n| ---------- |\n| Mansion    |\n| Duplex     |\n\n# Fits an sklearn one hot encoder\ndef train_one_hot_encoder(df, categorical_columns):\n    # take one-hot encoding of categorical columns\n    categorical_df = df[categorical_columns]\n    enc = OneHotEncoder(sparse=False, handle_unknown='ignore')\n    return enc.fit(categorical_df)\n\n# One hot encodes the given dataframe\ndef one_hot_encode(df, categorical_columns, encoder):\n    # Get dataframe with only categorical columns\n    categorical_df = df[categorical_columns]\n    # Get one hot encoding\n    ohe_df = pd.DataFrame(encoder.transform(categorical_df), columns=encoder.get_feature_names())\n    # Get float columns\n    float_df = df.drop(categorical_columns, axis=1)\n    # Return the combined array\n    return pd.concat([float_df, ohe_df], axis=1)\n\ndef feature_selection_and_engineering(df, encoder=None):\n    df = remove_nan(df)\n    categorical_columns = get_categorical_columns(df)\n    # If there is no encoder, train one\n    if encoder == None:\n        encoder = train_one_hot_encoder(df, categorical_columns)\n    # Encode Data\n    df = one_hot_encode(df, categorical_columns, encoder)\n    # Return the encoded data AND encoder\n    return df, encoder\n"
'Linear(in_features=784, out_features=256, bias=True)\n\nY = MX + B\n'
'tweet_a = Input(shape=(11,))\ntweet_b = Input(shape=(1,))\ntweetx = model(tweet_a)\ntweety = offset(tweet_b)\n\ntweet_a = Input(batch_shape=(860, 11))\ntweet_b = Input(batch_shape=(860, 1))\ntweetx = model(tweet_a)\ntweety = offset(tweet_b)\n'
'import os, argparse\n\nimport tensorflow as tf\n\n# The original freeze_graph function\n# from tensorflow.python.tools.freeze_graph import freeze_graph \n\ndir = os.path.dirname(os.path.realpath(__file__))\n\ndef freeze_graph(model_dir, output_node_names):\n    """Extract the sub graph defined by the output nodes and convert \n    all its variables into constant \n    Args:\n        model_dir: the root folder containing the checkpoint state file\n        output_node_names: a string, containing all the output node\'s names, \n                            comma separated\n    """\n    if not tf.gfile.Exists(model_dir):\n        raise AssertionError(\n            "Export directory doesn\'t exists. Please specify an export "\n            "directory: %s" % model_dir)\n\n    if not output_node_names:\n        print("You need to supply the name of a node to --output_node_names.")\n        return -1\n\n    # We retrieve our checkpoint fullpath\n    checkpoint = tf.train.get_checkpoint_state(model_dir)\n    input_checkpoint = checkpoint.model_checkpoint_path\n\n    # We precise the file fullname of our freezed graph\n    absolute_model_dir = "/".join(input_checkpoint.split(\'/\')[:-1])\n    output_graph = absolute_model_dir + "/frozen_model.pb"\n\n    # We clear devices to allow TensorFlow to control on which device it will load operations\n    clear_devices = True\n\n    # We start a session using a temporary fresh Graph\n    with tf.Session(graph=tf.Graph()) as sess:\n        # We import the meta graph in the current default Graph\n        saver = tf.train.import_meta_graph(input_checkpoint + \'.meta\', clear_devices=clear_devices)\n\n        # We restore the weights\n        saver.restore(sess, input_checkpoint)\n\n        # We use a built-in TF helper to export variables to constants\n        output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess, # The session is used to retrieve the weights\n            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n            output_node_names.split(",") # The output node names are used to select the usefull nodes\n        ) \n\n        # Finally we serialize and dump the output graph to the filesystem\n        with tf.gfile.GFile(output_graph, "wb") as f:\n            f.write(output_graph_def.SerializeToString())\n        print("%d ops in the final graph." % len(output_graph_def.node))\n\n    return output_graph_def\n'
"import pandas as pd\nimport numpy as np\nimport pickle\ndf = pd.read_csv('hiring.csv')\ndf['experience'].fillna(0, inplace=True)\n# Call the Get_dummies function\nX = df.drop('salary', axis=1)\ny = df.salary\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.33, random_state=0) \n\nfrom sklearn.tree import DecisionTreeRegressor\ntree = DecisionTreeTreeRegressor (max_depth = 4, random_state = 42)\ntree.fit(X_train, y_train)\npred_tree = tree.predict(X_test)\n\n# save the model to disk\nfilename = 'finalized_model.sav'\npickle.dump(tree, open(filename, 'wb'))\n\nfrom flask import Flask, make_response, request\nimport io\nfrom io import StringIO\nimport csv\nimport pandas as pd\nimport numpy as np\nimport pickle\n\n\n\napp = Flask(__name__)\n\ndef transform(text_file_contents):\n    return text_file_contents.replace(&quot;=&quot;, &quot;,&quot;)\n\n\n@app.route('/')\ndef form():\n    return &quot;&quot;&quot;\n        &lt;html&gt;\n            &lt;body&gt;\n                &lt;h1&gt;Let's TRY to Predict..&lt;/h1&gt;\n                &lt;/br&gt;\n                &lt;/br&gt;\n                &lt;p&gt; Insert your CSV file and then download the Result\n                &lt;form action=&quot;/transform&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt;\n                    &lt;input type=&quot;file&quot; name=&quot;data_file&quot; class=&quot;btn btn-block&quot;/&gt;\n                    &lt;/br&gt;\n                    &lt;/br&gt;\n                    &lt;button type=&quot;submit&quot; class=&quot;btn btn-primary btn-block btn-large&quot;&gt;Predict&lt;/button&gt;\n                &lt;/form&gt;\n            &lt;/body&gt;\n        &lt;/html&gt;\n    &quot;&quot;&quot;\n@app.route('/transform', methods=[&quot;POST&quot;])\ndef transform_view():\n    f = request.files['data_file']\n    if not f:\n        return &quot;No file&quot;\n\n    stream = io.StringIO(f.stream.read().decode(&quot;UTF8&quot;), newline=None)\n    csv_input = csv.reader(stream)\n    #print(&quot;file contents: &quot;, file_contents)\n    #print(type(file_contents))\n    print(csv_input)\n    for row in csv_input:\n        print(row)\n\n    stream.seek(0)\n    result = transform(stream.read())\n\n    df = pd.read_csv(StringIO(result))\n    \n\n    # load the model from disk\n    loaded_model = pickle.load(open(filename, 'rb'))\n    df['prediction'] = loaded_model.predict(df)\n\n    \n\n    response = make_response(df.to_csv())\n    response.headers[&quot;Content-Disposition&quot;] = &quot;attachment; filename=result.csv&quot;\n    return response\n\nif __name__ == &quot;__main__&quot;:\n    app.run(debug=False,port=9000)\n"
'temp_1 = theta_1 - alpha * (1 / m) * (matrix_x.transpose() * (theta_0 + theta_1 * matrix_x - matrix_y)).sum()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsat=np.random.rand(40,1)\nrand_a=np.random.randint(500)\nrand_b=np.random.randint(400)\ngpa=rand_a*sat+rand_b\ntheta_0 = 0.01\ntheta_1 = 0.01\nalpha = 0.1\ncost = 0\nm = len(gpa)\n\ndef calculateCost(matrix_x,matrix_y,m):\n    global theta_0,theta_1\n    cost = (1 / 2 * m) * ((theta_0 + (theta_1 * matrix_x) - matrix_y) ** 2).sum()\n    return cost\n\ndef gradDescent(alpha,matrix_x,matrix_y,num_iter=10000,eps=0.5):\n    global theta_0,theta_1,m,cost\n    cost = calculateCost(sat,gpa,m)\n    cost_hist=[cost]\n    for i in range(num_iter):\n        theta_0 -= alpha * (1 / m) * (theta_0 + theta_1 * matrix_x - matrix_y).sum()\n        theta_1 -= alpha * (1 / m) * (matrix_x.transpose().dot(theta_0 + theta_1 * matrix_x - matrix_y)).sum()\n        cost = calculateCost(sat,gpa,m)\n        cost_hist.append(cost)\n        if cost&lt;eps:\n            return cost_hist\n\nif __name__=="__main__":\n\n    print("init_cost==",cost)\n    cost_hist=gradDescent(alpha,sat,gpa)\n    print("final_cost,num_iters",cost,len(cost_hist))\n    print(rand_b,theta_0,rand_a,theta_1)\n    plt.plot(cost_hist,linewidth=5,color="r");plt.show()\n'
'label=["something","anything","nothing"]\ndate=[\'31st feb\',\'32nd dec\',\'1st jan\']\noutputList={i:j for i,j in zip(label,date)}\nprint(outputList)\n\n{\'something\': \'31st feb\', \'anything\': \'32nd dec\', \'nothing\': \'1st jan\'}\n\nlabel = olabel.append(output)\ndate = ldate.append(ld)\n\nolabel.append(output)\nlabel=olabel[:]\n'
'Repeat 20 time:\n     read next 100 datasets/datapoints\n     do the data processing\n     train the model for 1 epoch\n     save the model\n'
'from sklearn.decomposition import PCA\nfrom mlxtend.plotting import plot_decision_regions\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.svm import SVC\n\npc1 = [-0.114704, -0.036623, -0.266696, -0.304520]\npc2 = [0.318144, 0.402758, 0.102101, -0.044354]\nyvar = [0, 0, 1, 1]\n\n\nimport numpy as np    \ndf = np.column_stack((pc1, pc2))\n\n\n\nclf = SVC(C=1, gamma=0.0001, kernel=\'linear\')\nX_train = np.array(df)\ny_train = np.array(yvar)\nclf.fit(X_train, y_train)\nplot_decision_regions((X_train), (y_train), clf=clf, legend=2)\n\nfrom sklearn.decomposition import PCA\nfrom mlxtend.plotting import plot_decision_regions\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.svm import SVC\n\n\n\npc1 = [\n0.519179,\n0.271661,\n0.160372,\n0.131858,\n-0.082872,\n-0.018304,\n-0.075480,\n-0.120394,\n-0.079285,\n-0.061470,\n-0.114704,\n-0.036623,\n-0.266696,\n-0.304520,\n-0.341065,\n-0.335393,\n-0.294246,\n-0.112002,\n-0.008648,\n-0.016432,\n0.025505,\n0.065414,\n0.058254,\n0.080844,\n0.146013,\n0.072719,\n0.076515,\n0.073930,\n0.084932,\n0.127504,\n0.410069,\n0.444208,\n0.359892,\n0.351449,\n0.340579,\n0.195910,\n0.169974,\n0.168284,\n0.163418,\n0.222996,\n0.131592,\n0.035192,\n-0.005788,\n-0.146251,\n-0.165629,\n-0.157875,\n-0.144255,\n-0.115826,\n-0.145774,\n-0.218346,\n-0.154941,\n-0.173926,\n-0.191553,\n-0.209128\n]\n\npc2 = [\n0.247208,\n0.378146,\n0.395769,\n0.377220,\n0.099886,\n0.125293,\n0.129186,\n0.103077,\n0.315473,\n0.373005,\n0.318144,\n0.402758,\n0.102101,\n-0.044354,\n-0.091845,\n-0.158577,\n-0.172631,\n0.107467,\n0.039244,\n-0.011859,\n-0.003516,\n-0.144414,\n-0.199284,\n-0.227434,\n-0.177407,\n-0.215493,\n-0.218327,\n-0.205280,\n-0.213145,\n-0.119456,\n-0.070637,\n-0.054756,\n-0.039921,\n0.039005,\n-0.061595,\n-0.088828,\n0.014353,\n-0.034547,\n0.009783,\n-0.020889,\n0.197540,\n0.160503,\n0.010568,\n-0.078299,\n-0.054383,\n-0.065957,\n-0.038511,\n-0.080849,\n-0.064944,\n-0.008935,\n-0.066568,\n-0.109107,\n-0.059816,\n-0.118813\n]\n\nyvar = [\n1,\n1,\n1,\n0,\n1,\n1,\n1,\n1,\n0,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n0,\n1,\n0,\n0,\n1,\n1,\n0,\n1,\n1,\n0,\n1,\n1,\n0,\n0,\n1,\n1,\n1,\n1,\n1,\n0,\n1,\n0,\n1,\n1,\n1,\n0,\n1,\n0,\n1,\n0,\n1,\n1,\n0,\n0,\n1,\n1\n]\n\npc1 = [i * 10 for i in pc1]\npc2 = [i * 10 for i in pc2]\n\nimport numpy as np\ndf = np.column_stack((pc1, pc2))\n\n#df = x_var[[\'pc1\',\'pc2\']].join(y_var["y-var"])\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n\n#clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n#clf = AdaBoostClassifier()\n#clf = QuadraticDiscriminantAnalysis()\n#clf = KNeighborsClassifier(3)\n#clf = DecisionTreeClassifier(max_depth=20)\n#clf = SVC(C=1, gamma=0.25)\nclf = SVC(C=100, gamma=0.5)\n\nX_train = np.array(df)\ny_train = np.array(yvar)\nclf.fit(X_train, y_train)\nplot_decision_regions((X_train), (y_train), clf=clf, legend=2)\n'
'print(inputs.shape)\n# output TensorShape([Dimension(None), Dimension(256), Dimension(256), Dimension(1)])\n\n# inputs.shape.as_list()\n# output [None, 256, 256, 1]\n\nx = InceptionV3(include_top = False, weights = None, input_shape=inputs.shape.as_list()[1:])(x)\n'
'disp = plot_confusion_matrix(classifier, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize,\n                                   values_format="d")\n'
'# one hot encoding\nenc = OneHotEncoder()\nenc.fit(y)\ny = enc.transform(y).toarray()\n\nfit(self, X, y, sample_weight=None)[source]\n\n    Fit Gaussian Naive Bayes according to X, y\n\n    Parameters\n\n        Xarray-like, shape (n_samples, n_features)\n\n            Training vectors, where n_samples is the number of samples and n_features is the number of features.\n        yarray-like, shape (n_samples,)\n\n            Target values.\n        sample_weightarray-like, shape (n_samples,), optional (default=None)\n\n            Weights applied to individual samples (1. for unweighted).\n\n            New in version 0.17: Gaussian Naive Bayes supports fitting with sample_weight.\n\n    Returns\n\n        selfobject\n'
"from sklearn.feature_extraction.text import CountVectorizer\nimport lda\n\nl = [' '.join(i) for i in [A,B,C,D,E,F]]\nvec = CountVectorizer(analyzer='word', ngram_range=(1,1))\n\nX = vec.fit_transform(l)\n\nmodel = lda.LDA(n_topics=2, random_state=1)\nmodel.fit(X)\n\ndoc_topic = model.doc_topic_\n\nfor i in range(len(l)):\n    print(f'Cluster {i}: Topic ', doc_topic[i].argmax())\n\nCluster 0: Topic  1 # -&gt; A\nCluster 1: Topic  0\nCluster 2: Topic  1 #\xa0-&gt; C\nCluster 3: Topic  1 # -&gt; D\nCluster 4: Topic  0\nCluster 5: Topic  0\n"
'evaluate(train_x,  train_y, batch_size=&lt;batch size&gt;)\n'
'#modifiying the labels for two classes. binary prediction.  1[1,2,3] &amp; 2[4]   \ny_train_copy = np.copy(y_train)\ny_train_copy [y_train_copy &lt;4] = 1\ny_train_copy [y_train_copy &gt;=4] = 2\nclf1 = SGDClassifier(max_iter=1000, tol=1e-3)\nclf1.fit(X_train, y_train_copy )\n\n#training with classes 1 ,2, 3. filtering X based on Y\nX_train = X_train[Y_train&lt;4] \nY_train = Y_train[Y_train&lt;4]\nclf2 = SGDClassifier(max_iter=1000, tol=1e-3)\nclf2.fit(X_train, y_train)\n\npredicted = clf1.predict(X_test)\n#make sure you get 1 &amp; 2 filled in predicted list\n\nfurther_prediction_X_test = X_test[predicted == 1] #filtering all thats predicted as 1 by previous model\nfurther_prediction = clf2.predict(further_repdiction_X_test)\npredicted[predicted==2] = 4 #rolling back to original label\npredicted[predicted==1] = further_prediction #replacing the predicted classes from second model for which first model predicted as 1. (replacing with 1, 2, 3) \n'
'# reliance on global/upper scope variables is a bad practice\ndef h(a, b, c, value):\n    return a*(value**2)+b*value+c\n\n# don\'t repeat yourself - define it in one place rather than 3\nlr = 0.00000046*0.20  # learning rate\nfor i in range(1,300000000):\n\n    # first issue: a1, b1 and c1 need to be reset\n    a1 = b1 = c1 = 0\n\n    # hardcoding magic constants (including data shape) is a bad practice\n    for x_, y_ in range zip(x, y):\n        diff = h(a, b, c, x_)-y_\n        c1 += diff\n        b1 += diff * x[j]\n        a1 += diff * x[j]**2\n\n    # second issue: this needs to be done every "batch"\n    a -= lr * a1\n    b -= lr * b1\n    c -= lr * c1\n'
"classifier.add(Dense(units=4, \n                     kernel_initializer='uniform', \n                     activation='relu', \n                     input_dim=X.shape[1]))\n\n     Pclass  SibSp  Parch   Age     Fare  Sex_female  Sex_male\n0         3      1      0  22.0   7.2500           0         1\n1         1      1      0  38.0  71.2833           1         0\n2         3      0      0  26.0   7.9250           1         0\n3         1      1      0  35.0  53.1000           1         0\n4         3      0      0  35.0   8.0500           0         1\n..      ...    ...    ...   ...      ...         ...       ...\n886       2      0      0  27.0  13.0000           0         1\n887       1      0      0  19.0  30.0000           1         0\n888       3      1      2   NaN  23.4500           1         0\n889       1      0      0  26.0  30.0000           0         1\n890       3      0      0  32.0   7.7500           0         1\n"
'model.summary()\n'
"from sklearn.linear_model import PoissonRegressor\n\nprice = df.loc[:,'regularMarketPrice']\nfeatures = df.loc[:,feature_list]\n\n# \nX_train, X_test, y_train, y_test = train_test_split(features, price, test_size = 0.15, random_state = 1)\n\nif len(X_train.shape) &lt; 2:\n    X_train = np.array(X_train).reshape(-1,1)\n    X_test = np.array(X_test).reshape(-1,1)\n\n# \nmodel = PoissonRegressor()\nmodel.fit(X_train,y_train)\n\n# \nprint('Train Score:', model.score(X_train,y_train))\nprint('Test Score:', model.score(X_test,y_test))\n\n\n# \ny_predicted = model.predict(X_test)\n"
'for filename in word_count_dict["Britain"]:\n    if filename == \'total\': continue\n    print("Britain appears in {} {} times".format(filename, word_count_dict["Britain"][filename]))\n\nword_count_dict["Britain"].keys()\n\nif file not in word_count_dict[word]:\n    word_count_dict[word][file] = 0\n    word_count_dict[word][file] += 1              \n    word_count_dict[word]["total"] += 1        \n\nif file not in word_count_dict[word]:\n    word_count_dict[word][file] = 0\nword_count_dict[word][file] += 1              \nword_count_dict[word]["total"] += 1        \n\nfor word, counts in word_count_dict.iteritems():\n    print(\'Total counts for word {}: \'.format(word, counts[\'total\']))\n    for filename, count in counts.iteritems():\n        if filename == \'total\': continue\n        print("{} appears in {} {} times".format(word, filename, count))\n'
'from sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import BernoulliNB\n\n# generate some artificial data\nX, y = make_classification(n_samples=1000, n_features=50, weights=[0.1, 0.9])\n\n\n# your estimator\nestimator = BernoulliNB()\nestimator.fit(X, y)\n\n# generate predictions\nestimator.predict(X)\nOut[164]: array([1, 1, 1, ..., 0, 1, 1])\n\n# to get confidence on the prediction\nestimator.predict_proba(X)\n\nOut[163]: \narray([[ 0.0043,  0.9957],\n       [ 0.0046,  0.9954],\n       [ 0.0071,  0.9929],\n       ..., \n       [ 0.8392,  0.1608],\n       [ 0.0018,  0.9982],\n       [ 0.0339,  0.9661]])\n'
"target_values = training_set['IC50']\ntraining_data = training_set.drop('IC50')\n\nclf = LinearRegression()\nclf.fit(training_data, target_values)\n\ntest_data = test_set.drop('IC50')\n\npredicted_values = clf.predict(test_data)\n"
"np.random.seed(14)  # so you can reproduce\ndf = pd.DataFrame(np.random.randn(10, 1), columns=['a'])\ndf.head()\n\n---------\n a\n---------\n 1.331587\n 1.331587\n 0.715279\n-1.545400\n-0.008384\n 0.621336\n\ndf['a_new'] = df.shift(periods=1).fillna(0.0)\ndf.head()\n\n---------------------\n a           a_new\n---------------------\n 1.331587    0.000000\n 0.715279    1.331587\n-1.545400    0.715279\n-0.008384   -1.545400\n 0.621336   -0.008384\n\ndf['a_flags'] = [1 if x &gt; y else -1 for x, y in zip(df.a, df.a_new)]\ndf.head()\n\n-------------------------------\n a           a_new       a_flag\n-------------------------------\n 1.331587    0.000000    1\n 0.715279    1.331587   -1\n-1.545400    0.715279   -1 \n-0.008384   -1.545400    1\n 0.621336   -0.008384    1\n"
'nn.fit(X_train, y_train)\n\ny_train = [0,0,0,1,2,]\nX_train = [[ 7.1  3.   5.9  2.1]\n           [ 5.9  3.   4.2  1.5]\n           [ 5.5  2.4  3.7  1. ]\n           [ 6.1  2.8  4.7  1.2]\n           [ 5.   2.3  3.3  1. ]]\n\n    nn = Classifier(\n    layers=[\n            Layer("Maxout", units=100, pieces=2),\n            Layer("Softmax",units=1)],   -----&gt; if units is 3 that\'s ok!\n            learning_rate=0.001,\n            n_iter=25)\n    "Mismatch between dataset size and units in output layer."\n    AssertionError: Mismatch between dataset size and units in output layer.\n'
'Indexer          (0)  (1)    (2)      (3)\nPassengerID    Pclass Sex    Age      Fare\n1                 3    1    22.00    7.2500\n2                 1    0    38.00   71.2833\n3                 3    0    26.00    7.9250\n4                 1    0    35.00   53.1000\n5                 3    1    35.00    8.0500\n'
'{\n    "splitting": {\n        "percentBegin": 0,\n        "percentEnd": 70,\n        "complement": false,\n        "strategy": "sequential"\n    }\n}\n'
'my_list = [1, 2, 3, 4]\nmy_list[-1]\n4\n\nmy_list[:-1]\n[1, 2, 3]\n'
"def modified_cv(input_train_len, input_test_len):\n    yield (np.array(range(input_train_len)), \n           np.array(range(input_train_len, input_train_len + input_test_len)))\n\ninput_train_len = len(input_train)\ninput_test_len = len(input_test)\ndata = np.concatenate((input_train, input_test), axis=0)\ntarget = np.concatenate((target_train, target_test), axis=0)\ngrid = GridSearchCV(estimator=model, \n                    param_grid=param_grid,\n                    cv=modified_cv(input_train_len, input_test_len), \n                    scoring='neg_mean_squared_error')\ngrid_result = grid.fit(data, target)\n"
'&gt;&gt;&gt; xx, yy = list(zip(*sorted(set(zip(x, y)))))\n&gt;&gt;&gt; for k in range(15, 25):\n    print(k, interpolate.lagrange(xx[:k], yy[:k])(1))\n\n\n15 1.99999915221\n16 1.99998531246\n17 1.99992345466\n18 1.99993904792\n19 2.00236333472\n20 2.01589034207\n21 -1.04477498867\n22 -20.8148132927\n23 -172.983956978\n24 4185.90603781\n'
'x2[:, 0:2], x2[:, -2:] = x2[:, -2:].copy(), x2[:, 0:2].copy()\n\nOut[117]:\narray([[ 2,  3,  0,  1,  8,  9],\n       [12, 13,  6,  7,  4,  5]])\n'
'LabelList.append(os.path.splitext(File)[0])\n\ndata, labels = ReadImages(TRAIN_DIR)\nmodel.fit(np.array(data), np.array(labels), epochs=10, batch_size=32)\n\nclasses = ["nonPdr", "another_class"]\nLabelList.append(classes.index[os.path.splitext(File)[0]])\n'
'progress = dict()\n\nhistory=myModel.fit(X_train, y_train, evals_result=progress eval_metric=eval_metric, eval_set=eval_set)\n\nprint(progress)\n'
'Total: 40\nTotal correct predicts: 8\n'
"trained.columns[~trained.columns.isin(tests.columns)] #gives columns present in  trained but not is tests\n\ntrained.columns[trained.columns.isin(tests.columns)] #gives columns present in both trained &amp; tests df's\n"
'import pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.read_excel("Book1.xlsx")\n\ndf2 = pd.DataFrame()\n\nfor column in df:\n\n    X = df["Row Labels"]\n\n    Y = df[column]\n    y1 =Y.values.reshape(-1,1)\n    x1 =X.values.reshape(-1,1)\n    regressor = LinearRegression()\n    regressor.fit(x1, y1)\n    y_new = []\n    y_i = []\n    for i in range(12,24):\n        y_new.append(regressor.predict([[i]]))\n        y_i.append(i)\n\n    df2[column] = y_new\n'
'&gt;&gt;&gt; import numpy as np  # first I import numpy as "np"\n# I generate 10 random values and I store them in "importance"\n&gt;&gt;&gt; importance=np.random.rand(10)\n# here I just want to see the content of "importance"\n&gt;&gt;&gt; importance\narray([0.77609076, 0.97746829, 0.56946118, 0.23986983, 0.93655692,\n       0.22003531, 0.7711095 , 0.36083248, 0.58277805, 0.57865248])\n# here there is your error that I reproduce for teaching purpose\n&gt;&gt;&gt;importance.replace(".", ",")\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nAttributeError: \'numpy.ndarray\' object has no attribute \'replace\'\n\n&gt;&gt;&gt; imp_astr=[str(i) for i in importance]\n&gt;&gt;&gt; imp_astr\n[\'0.7760907642658763\', \'0.9774682868805988\', \'0.569461184647781\', \'0.23986982589422634\', \'0.9365569207431337\', \'0.22003531170279356\', \'0.7711094966708247\', \'0.3608324767276052\', \'0.5827780487688116\', \'0.5786524781334242\']\n# at the end, for each string, you can use the "replace" function\n&gt;&gt;&gt; imp_astr=[i.replace(".", ",") for i in imp_astr]\n&gt;&gt;&gt; imp_astr\n[\'0,7760907642658763\', \'0,9774682868805988\', \'0,569461184647781\', \'0,23986982589422634\', \'0,9365569207431337\', \'0,22003531170279356\', \'0,7711094966708247\', \'0,3608324767276052\', \'0,5827780487688116\', \'0,5786524781334242\']\n'
"datascaled = sc.transform(predictcrop)\npredictions = clf.predict(datascaled)\n\nfrom sklearn.externals.joblib import dump, load\ndump(sc, 'scaler.bin', compress=True)\n\nsc=load('scaler.bin')\n"
'!unzip "/content/drive/My Drive/darknet.zip"\n'
"results_in_splits = []\n\nfor k, v in cv.cv_results_.items():\n    if 'split' in k:\n        print('\\t-&gt;', k)\n        results_in_splits.append(v[best_model_idx])\n    else:\n        print(k)\n\nprint('\\n')\nprint(sum(results_in_splits) / len(results_in_splits))\nprint(cv.best_score_)\n\nmean_fit_time\nstd_fit_time\nmean_score_time\nstd_score_time\nparam_subsample\nparam_n_estimators\nparam_min_child_weight\nparam_max_depth\nparam_gamma\nparam_colsample_bytree\nparams\n    -&gt; split0_test_score\n    -&gt; split1_test_score\n    -&gt; split2_test_score\n    -&gt; split3_test_score\n    -&gt; split4_test_score\n    -&gt; split5_test_score\n    -&gt; split6_test_score\n    -&gt; split7_test_score\n    -&gt; split8_test_score\n    -&gt; split9_test_score\n    -&gt; split10_test_score\n    -&gt; split11_test_score\n    -&gt; split12_test_score\nmean_test_score\nstd_test_score\nrank_test_score\n\n\n0.8926320979964705\n0.8926320979964705\n"
"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nX_train = np.random.random((1000, 10))\ny_labels = np.array([0] * 500 + [1] * 500)\n\npcaObj1 = PCA(n_components=1)\nX_PCA = pcaObj1.fit_transform(X_train)\n\nplt.scatter(range(len(y_labels)), X_PCA, c=['red' if i==0 else 'green' for i in y_labels])\nplt.show()\n\n\n&gt;&gt;&gt; X_PCA.shape\n(1000, 1)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\nX_train = np.random.random((1000, 10))\ny_labels = np.array([0] * 500 + [1] * 500)\n\npcaObj1 = PCA(n_components=2)\nX_PCA = pcaObj1.fit_transform(X_train)\n\nplt.scatter(X_PCA[:,0], X_PCA[:,1], c=['red' if i==0 else 'green' for i in y_labels])\nplt.show()\n"
"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import average_precision_score    \nfrom sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_classification\n\nscorer = make_scorer(average_precision_score, average = 'weighted')\n\nX, y = make_classification(n_features=10, random_state=0, n_classes=3, n_samples=1000, n_informative=8)\ny = preprocessing.label_binarize(y, classes=[0, 1, 2])\nclf = OneVsRestClassifier(linear_model.LogisticRegression())\n\ncross_val_score(clf, X, y, cv=3,scoring=scorer)\narray([0.586501  , 0.54517146, 0.596331  ])  \n"
'# Prepare the training dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n\n# Prepare the validation dataset\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nval_dataset = val_dataset.batch(64)\n\nmodel.fit(train_dataset, epochs=3, validation_data=val_dataset)\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_val = x_train[-10000:]\ny_val = y_train[-10000:]\nx_train = x_train[:-10000]\ny_train = y_train[:-10000]\n'
"model=tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10,activation='relu',input_shape=[4],name='layer1'),\n    tf.keras.layers.Dense(4,activation='softmax',name='layer2'),\n    ])\n\n### Error ###\nobs=tf.constant([1,2,3,4],dtype=tf.float32) \npred=model(obs)\n\n### OK ###\nobs=tf.constant([[1,2,3,4]],dtype=tf.float32)\npred=model(obs)\n"
"# Importing Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# import data Set\n\ndataset = pd.read_csv('Position_Salaries.csv')\nX = dataset.iloc[:, 1:-1].values\ny = dataset.iloc[:, -1].values\n\ny = y.reshape(-1, 1)\n\n# Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX = sc_X.fit_transform(X)\ny = sc_y.fit_transform(y)\n\n# Building Model on training dataset\n\nfrom sklearn.svm import SVR\nregressor = SVR(kernel='rbf')\nregressor.fit(X,y)\n\nregressor.get_params()\n\n{'C': 1.0,\n 'cache_size': 200,\n 'coef0': 0.0,\n 'degree': 3,\n 'epsilon': 0.1,\n 'gamma': 'scale',\n 'kernel': 'rbf',\n 'max_iter': -1,\n 'shrinking': True,\n 'tol': 0.001,\n 'verbose': False}\n"
"wine_classes = [wine_data_frame[wine_data_frame['class'] == x] for x in range(3)]\n\nimport pandas as pd\nfrom sklearn.datasets import load_wine\n\nwine_data = load_wine()\nwine_data_frame = pd.DataFrame(data=wine_data['data'], columns=wine_data['feature_names'])\nwine_data_frame['class'] = wine_data['target']\n\nwine_classes = [wine_data_frame[wine_data_frame['class'] == x] for x in range(3)]\n\ntesting_data = []\nfor wine_class in wine_classes:\n    row = wine_class.sample()\n    testing_data.append(row)\n    wine_data_frame = wine_data_frame.drop(row.index)\n\n[   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  od280/od315_of_diluted_wines  proline  class\n2    13.16        2.36  2.67               18.6      101.0            2.8        3.24                   0.3             2.81             5.68  1.03                          3.17   1185.0      0,     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  od280/od315_of_diluted_wines  proline  class\n91     12.0        1.51  2.42               22.0       86.0           1.45        1.25                   0.5             1.63              3.6  1.05                          2.65    450.0      1,      alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  od280/od315_of_diluted_wines  proline  class\n150     13.5        3.12  2.62               24.0      123.0            1.4        1.57                  0.22             1.25              8.6  0.59                           1.3    500.0      2]\n\n     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  od280/od315_of_diluted_wines  proline  class\n0      14.23        1.71  2.43               15.6      127.0           2.80        3.06                  0.28             2.29             5.64  1.04                          3.92   1065.0      0\n1      13.20        1.78  2.14               11.2      100.0           2.65        2.76                  0.26             1.28             4.38  1.05                          3.40   1050.0      0\n3      14.37        1.95  2.50               16.8      113.0           3.85        3.49                  0.24             2.18             7.80  0.86                          3.45   1480.0      0\n4      13.24        2.59  2.87               21.0      118.0           2.80        2.69                  0.39             1.82             4.32  1.04                          2.93    735.0      0\n5      14.20        1.76  2.45               15.2      112.0           3.27        3.39                  0.34             1.97             6.75  1.05                          2.85   1450.0      0\n..       ...         ...   ...                ...        ...            ...         ...                   ...              ...              ...   ...                           ...      ...    ...\n173    13.71        5.65  2.45               20.5       95.0           1.68        0.61                  0.52             1.06             7.70  0.64                          1.74    740.0      2\n174    13.40        3.91  2.48               23.0      102.0           1.80        0.75                  0.43             1.41             7.30  0.70                          1.56    750.0      2\n175    13.27        4.28  2.26               20.0      120.0           1.59        0.69                  0.43             1.35            10.20  0.59                          1.56    835.0      2\n176    13.17        2.59  2.37               20.0      120.0           1.65        0.68                  0.53             1.46             9.30  0.60                          1.62    840.0      2\n177    14.13        4.10  2.74               24.5       96.0           2.05        0.76                  0.56             1.35             9.20  0.61                          1.60    560.0      2\n\n[175 rows x 14 columns]\n"
"vect = CountVectorizer(stop_words='english', \n                       max_features=10000, \n                       ngram_range=(1,2))\nrf_classifier = RandomForestClassifier(n_estimators=100)\npipe = Pipeline([(&quot;prep&quot;, prep),(&quot;CV&quot;, vect), (&quot;RF&quot;, rf_classifier)])    \npipe.fit(X_train_dtm, y_train)\n\nrf_predictions = pipe.predict(X_test_dtm)\nprint(&quot;RF Accuracy:&quot;)\nmetrics.accuracy_score(y_test, rf_predictions)\n\nwith open('PII_model.pickle', 'wb') as picklefile:\n    pickle.dump(pipe, picklefile)\n"
"import tensorflow as tf\nimport matplotlib.pyplot as plt\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\ninpTensor = tf.keras.layers.Input(x_train.shape[1:],)\n\nhidden1Out = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)(inpTensor)\nhidden2Out = tf.keras.layers.Dense(units=128, activation=tf.nn.relu)(hidden1Out)\npooling = tf.keras.layers.GlobalMaxPool2D()(hidden2Out) #&lt;== also GlobalAvgPool2D or Flatten are ok\nfinalOut = tf.keras.layers.Dense(units=10, activation=tf.nn.softmax)(pooling)\n\nmodel = tf.keras.Model(inputs=inpTensor, outputs=finalOut)\n\nmodel.summary()\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train,y_train, epochs = 4)\n"
"def scan_files(root):\n    pdftext = ''\n    for path, subdirs, files in os.walk(root):\n        for name in files:\n            if name.endswith('.pdf'):\n                #print(name)\n                pdf = PyPDF2.PdfFileReader(os.path.join(path,name))\n                numPages = pdf.getNumPages()\n                \n                pages = ''                    \n\n                for p in range(0, numPages):\n                    page = pdf.getPage(p)\n                    pages += page.extractText()\n                    pages = pages.replace('\\n', '')\n\n                pdftext += pages\n\n    return pdftext\n"
'result = test_set.map(lambda image, label: image)\n# You can iterate and check what you have received at the end.\n# I expect only the images.\nfor image in result.take(1):\n    print(image)\n'
'for b, (X_train, y_train) in enumerate(train_loader):\n\nX_train, y_train = X_train.cuda(), y_train.cuda()\n'
'y_spl = UnivariateSpline(ks,inertias,s=0,k=4)\nx_range = np.linspace(ks[0],ks[-1],1000)\n\ny_spl_1d = y_spl.derivative(n=1)\n\nplt.plot(x_range,y_spl_1d(x_range))\n'
"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf \n\ntf.compat.v1.debugging.set_log_device_placement(True)\nprint(tf.config.list_physical_devices('GPU'))\n\nmodel = Sequential()\nmodel.add(Dense(1024, input_dim=8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n\nX = np.random.randn(10,8) \ny = np.random.randn(10) \n\nmodel.fit(X, y, epochs=2)\nmodel.save(&quot;model.h5&quot;)\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nEpoch 1/2\n1/1 [==============================] - 0s 1ms/step - loss: 0.6570 - accuracy: 0.0000e+00\nEpoch 2/2\n1/1 [==============================] - 0s 983us/step - loss: 0.6242 - accuracy: 0.0000e+00\n&lt;tensorflow.python.keras.callbacks.History at 0x7fcad09366a0&gt;\n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf \n\n\nfrom keras.models import load_model\ntf.compat.v1.debugging.set_log_device_placement(True)\nprint(tf.config.list_physical_devices('GPU'))\n\nmodel = load_model('model.h5')\nmodel.predict(X)\n\n[]\narray([[0.4464949 ],\n       [0.43229908],\n       [0.49823508],\n       [0.4367126 ],\n       [0.47648385],\n       [0.48096564],\n       [0.47863394],\n       [0.5031184 ],\n       [0.45698297],\n       [0.45885688]], dtype=float32)\n"
'model.predict(np.expand_dims(test_images[0],0))\n'
"df = pd.DataFrame({'APP': ['A', 'B'], 'Size': ['5M','6K']})\ndf['Scale'] = df['Size'].str[-1]\ndf['Size'] = df['Size'].str[:-1].astype(int)\ndf.loc[df['Scale'] == 'K', 'Size'] = df.loc[df['Scale'] == 'K', 'Size'] / 1000\ndf = df.drop('Scale', axis=1)\ndf\n"
"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import SGDClassifier\n\nimport pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('./smsspamcollection//SMSSpamCollection', sep='\\t', names=[&quot;label&quot;, &quot;message&quot;])\ndf['label'][df['label']=='ham'] = np.random.choice(['hamA','hamB'],np.sum(df['label']=='ham'))\nX_train = df['message']\ny_train = df['label']\n\ndf['label'].value_counts()\n\nhamB    2425\nhamA    2400\nspam     747\n\ncv = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n\npipeline_sgd = Pipeline([\n     ('vect', CountVectorizer()),\n     ('tfdif', TfidfTransformer()),\n     ('nb', CalibratedClassifierCV(base_estimator = SGDClassifier(), cv=cv)),\n])\n\nmodel = pipeline_sgd.fit(X_train, y_train)\n\nn_top_labels = 3\nprobas = model.predict_proba(X_train[:5])\ntop_n_lables_idx = np.argsort(-probas)\ntop_n_probs = np.round(-np.sort(-probas),3)\ntop_n_labels = [model.classes_[i] for i in top_n_lables_idx]\n\nresults = list(zip(top_n_labels, top_n_probs))\n\npd.DataFrame(results)\n\n    0   1\n0   [hamB, hamA, spam]  [0.608, 0.38, 0.012]\n1   [hamA, hamB, spam]  [0.605, 0.391, 0.004]\n2   [spam, hamB, hamA]  [0.603, 0.212, 0.185]\n3   [hamB, hamA, spam]  [0.521, 0.478, 0.001]\n4   [hamB, hamA, spam]  [0.645, 0.352, 0.003]\n"
'model.add(tf.keras.layers.Input(shape=(150,150,3)))\n\nmodel = Sequential()\n\nmodel.add(tf.keras.layers.Input(shape=(150,50)))\n\nmodel.add(Conv2D(filters=16, kernel_size=2, strides=(1,1), activation=&quot;relu&quot;))\n\nmodel.fit(images,labels, batch_size=32, shuffle=True, epochs=1)\n\n'
"from sklearn.datasets import make_blobs\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nScaler =  StandardScaler()\n\nX, y = make_blobs(n_samples=5000, centers=3, shuffle=False,random_state=42)\nX = np.concatenate((X,np.random.randint(0,363175645.191632,size=(5000,35))),axis=1)\ny = (y==1).astype('int')\n\nX_scaled = Scaler.fit_transform(X)\n\nplt.scatter(x=X_scaled[:,0],y=X_scaled[:,1],c=['k' if i else 'b' for i in y])\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n\nsvc = SVC(kernel=&quot;linear&quot;)\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),scoring='accuracy')\nrfecv.fit(X_scaled, y)\n\nrfecv.ranking_\n\narray([ 1,  2, 17, 28, 33, 22, 23, 26,  6, 19, 20,  4, 10, 25,  3, 27, 11,\n        8, 18,  5, 29, 14,  7, 21,  9, 13, 24, 30, 35, 31, 32, 34, 16, 36,\n       37, 12, 15])\n"
'layer1 = tf.keras.layers.Conv1D(filters=256, kernel_size=1)(layer0)\n'
"def train_NN(self, trained_data_list, test_size=0.25):\n\n    X = np.array([[7, 4], [3,7], [8,5], [2,3]])\n    y = np.array([0, 1, 1, 0])\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    self.clf = MLPClassifier(solver='lbfgs', alpha=1e-10, hidden_layer_sizes=(2, 2), activation = 'logistic', random_state=5, max_iter=10000, learning_rate_init = 0.1)\n\n    self.clf.fit(X_train, y_train)\n\n    print(self.clf.predict(np.atleast_2d([7,4])))  #returns 0\n    print(self.clf.predict(np.atleast_2d([3,7])))  #returns 1 \n    print(self.clf.predict(np.atleast_2d([8,5])))  #returns 1 \n    print(self.clf.predict(np.atleast_2d([2,3])))  #returns 0 \n"
"import numpy as np\nimport tensorflow as tf \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n# train set / data \nx_train = x_train.astype('float32') / 255\n\n# validation set / data \nx_test = x_test.astype('float32') / 255\n\n# train set / target \ny_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n# validation set / target \ny_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n\nprint(x_train.shape, y_train.shape)\nprint(x_test.shape, y_test.shape)  \n# (50000, 32, 32, 3) (50000, 10)\n# (10000, 32, 32, 3) (10000, 10)\n\ninput = tf.keras.Input(shape=(32,32,3))\n# Block 1\nx = tf.keras.layers.Conv2D(32, 3, strides=2, activation=&quot;relu&quot;)(input)\nx = tf.keras.layers.MaxPooling2D(3)(x)\n\n# Now that we apply global max pooling.\ngap = tf.keras.layers.GlobalMaxPooling2D()(x)\n\n# Finally, we add a classification layer.\noutput = tf.keras.layers.Dense(10, activation='softmax')(gap)\n\n# bind all\nfunc_model = tf.keras.Model(input, output)\n\nfrom sklearn.metrics import f1_score, confusion_matrix\n\nclass Metrics(tf.keras.callbacks.Callback):\n    def __init__(self,val_x, val_y, batch_size = 20):\n        super().__init__()\n        self.val_x = val_x\n        self.val_y = val_y\n        self.batch_size = batch_size\n    def on_train_begin(self, logs=None):\n        self.confusion = []\n        self.precision = []\n        self.recall = []\n        self.f1s = []\n\n    def on_epoch_end(self, epoch, logs=None):\n        x    = self.val_x\n        targ = self.val_y\n\n        score   = np.asarray(self.model.predict(x))\n        predict = np.round(np.asarray(self.model.predict(x)))\n        self.f1s.append(f1_score(targ, predict, average='micro'))\n        self.confusion.append(confusion_matrix(targ.argmax(axis=1), predict.argmax(axis=1)))\n        print(&quot;\\nAt epoch {} f1_score {}:&quot;.format(epoch, self.f1s[-1]))\n        print('\\nAt epoch {} cm {}'.format(epoch, self.confusion[-1]))\n        return \n\n# compile\nprint('\\nFunctional API')\nfunc_model.compile(\n          loss      = 'categorical_crossentropy',\n          optimizer = tf.keras.optimizers.Adam()\n          )\n\nmetrics = Metrics(x_test, y_test)\n\n# fit \nfunc_model.fit(x_train, y_train, \n               validation_data=(x_test, y_test), \n               callbacks= [metrics] , \n               batch_size=128, epochs=3)\n\nTrain on 50000 samples, validate on 10000 samples\nEpoch 1/3\n49920/50000 [============================&gt;.] - ETA: 0s - loss: 1.7155\nAt epoch 0 f1_score 0.14721851357365376:\nAt epoch 0 cm [[919  11   2   0   0   2   3   6  46  11]\n [859  98   0   0   0   0   2   2   5  34]\n [942   7   9   0   1   6  27   6   2   0]\n [973   6   1   1   0   8   5   3   2   1]\n [949   1   0   0  22   1  22   3   1   1]\n [958   2   0   0   0  26   5   6   2   1]\n [831   2   3   0   0   1 158   1   3   1]\n [916   5   1   0   1   6   2  56   1  12]\n [750  11   0   0   0   0   1   4 227   7]\n [835  27   0   0   0   1   4   4   9 120]]\n50000/50000 [==============================] - 10s 206us/sample - loss: 1.7154 - val_loss: 1.7113\nEpoch 2/3\n49664/50000 [============================&gt;.] - ETA: 0s - loss: 1.7102\nAt epoch 1 f1_score 0.16048514677447706:\nAt epoch 1 cm [[896  10   2   0   0   2   2   5  69  14]\n [845  89   0   0   0   1   2   2  11  50]\n [941   6  12   0   2   7  23   5   4   0]\n [967   5   1   0   0  13   6   2   5   1]\n [946   0   1   0  27   1  20   2   2   1]\n [945   2   1   0   0  38   5   5   2   2]\n [840   2   4   0   0   1 148   1   3   1]\n [910   4   1   0   1  13   1  51   1  18]\n [694   6   0   0   0   0   0   3 290   7]\n [803  23   0   0   1   1   4   4  13 151]]\n50000/50000 [==============================] - 10s 198us/sample - loss: 1.7102 - val_loss: 1.7080\nEpoch 3/3\n50000/50000 [==============================] - ETA: 0s - loss: 1.7059\nAt epoch 2 f1_score 0.16229953553588644:\nAt epoch 2 cm [[899   9   2   0   0   2   3   5  65  15]\n [861  72   0   0   0   0   2   1  11  53]\n [935   4  10   0   1   5  36   6   3   0]\n [972   4   1   2   0   6   8   1   4   2]\n [949   0   1   0  10   0  35   2   2   1]\n [963   2   1   0   0  19   6   5   2   2]\n [796   2   4   0   0   0 194   0   3   1]\n [917   3   1   0   0   3   3  53   1  19]\n [702   4   0   0   0   0   1   3 281   9]\n [798  20   0   0   0   1   5   4  13 159]]\n50000/50000 [==============================] - 10s 196us/sample - loss: 1.7059 - val_loss: 1.7067\n\nimport tensorflow.keras.backend as K\n\ndef f1_metric(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val\n\n# compile\nprint('\\nFunctional API')\nfunc_model.compile(\n          metrics=['accuracy', f1_metric],\n          loss      = 'categorical_crossentropy',\n          optimizer = tf.keras.optimizers.Adam()\n          )\n\nEpoch 1/3\n50000/50000 [==============================] - ETA: 5s - loss: 2.2136 - accuracy: 0.1976 - f1_metric: 0.0000e+00 - val_loss: 2.1119 - val_accuracy: 0.2443 - val_f1_metric: 0.0000e+00\nEpoch 2/3\n50000/50000 [==============================] -  ETA: 7s - loss: 2.0456 - accuracy: 0.2546 - f1_metric: 4.3617e-04 - val_loss: 1.9909 - val_accuracy: 0.2829 - val_f1_metric: 0.0022\n"
'import tensorflow as tf\nfrom time import time\n\ndataset = tf.data.Dataset.range(1, 100)\nt1 = time()\ndataset = dataset.map(lambda x: x + 1)\nt2 = time()\nprint(&quot;Time taken for map : &quot;, t2-t1)\n\nt3 = time()\nds = dataset.take(50)\nt4 = time()\nlist(ds.as_numpy_iterator())\nprint(&quot;Time taken for take() : &quot;,t4-t3) \n\nTime taken for map :  0.013489961624145508\nTime taken for take() :  0.0005645751953125\n\ndataset = tf.data.Dataset.range(1, 100)\nt1 = time()\ndataset = dataset.map(lambda x: x + 1)\nt2 = time()\nprint(&quot;Time taken for map : &quot;, t2-t1)\n\nt3 = time()\nds = dataset.take(50)\nlist(ds.as_numpy_iterator())\nt4 = time()\nprint(&quot;Time taken for take() after some operation : &quot;,t4-t3)\n\nTime taken for map :  0.00974416732788086\nTime taken for take() after some operation :  0.017722606658935547 \n'
'            Actual_price  predict_price  Error          Relative Error\n4928          162000         165994  -3994.343750         2.5%\n11272         31000          50525   -19525.128906        62,9%\n7894          110000         117209  -7209.609375         6,5%\n4382          59500          75478   -15978.164062        26,5%\n345           500000         482369   17630.968750        3,5%\n...             ...            ...           ...\n3348          42750          38110    4639.328125         10,8%\n8993          74000          96511   -22511.226562        30%\n8270          83750          74911    8838.210938         10%\n2757          77500          89780   -12280.585938        15%\n6538          95000          92607    2392.765625         2,5%\n'
'rlronp=f.keras.callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.5, patience=1)\n    \n\nestop=tf.keras.callbacks.EarlyStopping(monitor=&quot;val_loss&quot;,patience=3,restore_best_weights=True)\ncallbacks=[rlronp, estop]\n'
"labels = kM.predict(V1_V2)\n\nplt.scatter(normed_V1, normed_V2, s=10, c=labels, cmap='coolwarm') # instead of c=bn_class\n"
"def sparse_one_hot(y):\n\n  m = len(y)\n  n_classes = len(tf.unique(tf.squeeze(y))[0])\n\n  dim2 = tf.range(m, dtype='int64')[:, None]\n  indices = tf.concat([y, dim2], axis=1)\n\n  ones = tf.ones(shape=(m, ), dtype='float32')\n\n  sparse_y = tf.sparse.SparseTensor(indices, ones, dense_shape=(m, n_classes))\n  \n  return sparse_y\n\nimport tensorflow as tf\n\ny = tf.random.uniform(shape=(10, 1), minval=0, maxval=4, dtype=tf.int64)\n\nsparse_y = sparse_one_hot(y) # sparse_y.values, sparse_y.indices\n\n# set sparse=True, for Input\n# tf.keras.Input(..., sparse=True, ...)\n"
"# For storing \nimport pickle\nwith open('pickle_file_name.pkl','w') as file_object: \n    pickle.dump(&lt;your_data_variable&gt;, file_object)  \n  \n# For loading \nwith open('pickle_file_name.pkl','r') as file_object:\n    your_data_variable= pickle.load(file_object) \n"
'from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_jobs=10)\n'
'def read_tensor_from_image_file(file_name,\n                            input_height=299,\n                            input_width=299,\n                            input_mean=0,\n                            input_std=255):\ninput_name = "file_reader"\noutput_name = "normalized"\n\nif type(file_name) is str:\n    file_reader = tf.read_file(file_name, input_name)\n    if file_name.endswith(".png"):\n        image_reader = tf.image.decode_png(file_reader, channels = 3,\n                                           name=\'png_reader\')\n    elif file_name.endswith(".gif"):\n        image_reader = tf.squeeze(tf.image.decode_gif(file_reader,\n                                                      name=\'gif_reader\'))\n    elif file_name.endswith(".bmp"):\n        image_reader = tf.image.decode_bmp(file_reader, name=\'bmp_reader\')\n    else:\n        image_reader = tf.image.decode_jpeg(file_reader, channels = 3,\n                                            name=\'jpeg_reader\')\n    float_caster = tf.cast(image_reader, tf.float32)\n    dims_expander = tf.expand_dims(float_caster, 0);\n    resized = tf.image.resize_bilinear(dims_expander, [input_height,\n                                                       input_width])\n    normalized = tf.divide(tf.subtract(resized, [input_mean]), \n                           [input_std])\n    sess = tf.Session()\n    result = sess.run(normalized)\n\nelif type(file_name) is np.ndarray:\n    resized = cv2.resize(file_name, (input_width, input_height),\n                            interpolation=cv2.INTER_LINEAR)\n    normalized = (resized - input_mean) / input_std\n    result = normalized\n    result = array(result).reshape(1, 224, 224, 3)\n\nreturn result\n'
"model = Sequential()\nmodel.add(LSTM(10, input_shape=(6,4), dropout=0.5))\nmodel.add(Dense(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(data.shape) # (1000, 6, 4)\nmodel.fit(data, target)\n\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape=(6,4)))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(10, input_shape=(6,4)))\nmodel.add(Dense(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(data.shape) # (1000, 6, 4)\nmodel.fit(data, target)\n"
'x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n'
'import numpy as np\nfrom sklearn import linear_model, datasets\n\narc = lambda r,c: r-c\nT1 = np.array([[arc(r,c) for c in xrange(4)] for r in xrange(5)])\nT2 = np.array([[arc(r,c) for c in xrange(1)] for r in xrange(5)])\nT3 = np.array([0,0,1,1,1])\n\nX = np.concatenate((T1,T2), axis=1)\nY = T3\nlogreg = linear_model.LogisticRegression(C=1e5)\n\n# we create an instance of Neighbours Classifier and fit the data.\n# using T1 and T2 as features, and T3 as target\nlogreg.fit(X, Y)\n\nX_test = np.array([[1, 0, -1, -1, 1],\n                   [0, 1, 2, 3, 4,]])\n\nprint logreg.predict(X_test)\n'
'bias = bias + (c-g)\n'
' fo = open ("stt.text", "r")\n y_list = [] # make new list to store all the data\n for line in fo.readlines():\n     a = line.find(\'/\')\n     str1 = line[0:9]\n     str2 = line[10:23]\n     y = str2 + \' \' + str1\n     y_list.append(y) # store all the data to the newly created list\n     print(y)\n fo.close()\n fo = open ("newstt.text", "w")\n for lines in y_list:\n     fo.write(lines+"\\n") # write all the data from the list\n fo.close()\n'
'np.dot((np.dot(X_test[0],reg.coefs_[0]) +reg.intercepts_[0] ),reg.coefs_[1]) + reg.intercepts_[1]\n'
'np.reshape(train_dataset, (6999, -1))\n\nnp.reshape(train_dataset, (train_dataset.shape[0], -1))\n'
'checkpoint_file = tf.train.latest_checkpoint("saved_models")\nimgSize = 48\n\ngraph = tf.Graph()\nwith graph.as_default():\n     sess = tf.Session()\n     with sess.as_default():\n         saver = tf.train.import_meta_graph("{}.meta".format(checkpoint_file))\n         saver.restore(sess,checkpoint_file)\n         inp = graph.get_operation_by_name("Reshaping_data/inp").outputs[0]\n         prediction=graph.get_operation_by_name("out").outputs[0]\n\n         img = imread(imagepath, flatten=True)\n         img = imresize(img, [imgSize, imgSize])\n         img = img.astype(\'float32\')\n\n         img_mean = np.mean(img)\n         img_var = np.std(img)\n         img = (img - img_mean)/img_var\n\n         #img = (48, 48)\n         img = np.expand_dims(img, axis=2)\n         #img = (48, 48, 1)\n         img = np.expand_dims(img, axis=0)\n         #img = (1, 48, 48, 1)\n\n         #inp expects (?, 48, 48, 1)\n         res = sess.run(prediction,feed_dict={inp:img})\n         print(res)\n         print(res.argmax(axis=1))\n'
'pred = sqlContext.createDataFrame([("What are liver enzymes ?" ,)], ["sentence"])\n\nprediction = model.transform(pred)\n'
'n=5\nxs = np.random.choice(list("qwertyuiopasdfghjklzxcvbnm"),3*n).reshape((-1,3))\nxs\narray([[\'z\', \'h\', \'d\'],\n       [\'g\', \'k\', \'y\'],\n       [\'t\', \'c\', \'o\'],\n       [\'f\', \'b\', \'s\'],\n       [\'x\', \'n\', \'z\']],\n      dtype=\'&lt;U1\')\n\nnp.unique(xs, return_inverse=True)[1].reshape((-1,3))\narray([[13,  5,  2],\n       [ 4,  6, 12],\n       [10,  1,  8],\n       [ 3,  0,  9],\n       [11,  7, 13]])\n\nn = 1000000\nxs = np.random.choice(list("qwertyuiopasdfghjklzxcvbnm"),3*n).reshape((-1,3))\n\n%timeit np.unique(xs, return_inverse=True)[1].reshape((-1,3))\n849 ms ± 39.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\nlabels = np.unique(xs, return_inverse=True)[1]\ndic = dict(zip(xs.flatten(),labels))\n\nys = np.reshape([dic[v] for list in xs for v in list], (-1,3))\nys\narray([[13,  5,  2],\n       [ 4,  6, 12],\n       [10,  1,  8],\n       [ 3,  0,  9],\n       [11,  7, 13]])\n\nreverse_dic = dict(zip(labels, xs.flatten()))\nnp.reshape([reverse_dic[v] for list in ys for v in list], (-1,3))\narray([[\'z\', \'h\', \'d\'],\n       [\'g\', \'k\', \'y\'],\n       [\'t\', \'c\', \'o\'],\n       [\'f\', \'b\', \'s\'],\n       [\'x\', \'n\', \'z\']],\n      dtype=\'&lt;U1\')\n\nlabels = np.unique(xs, return_inverse=True)[1]\ndic = dict(zip(xs.flatten(),labels))\nnp.vectorize(dic.get)(xs)\narray([[13,  5,  2],\n       [ 4,  6, 12],\n       [10,  1,  8],\n       [ 3,  0,  9],\n       [11,  7, 13]])\n\nreverse_dic = dict(zip(labels, xs.flatten()))\nnp.vectorize(reverse_dic.get)(ys)\narray([[\'z\', \'h\', \'d\'],\n       [\'g\', \'k\', \'y\'],\n       [\'t\', \'c\', \'o\'],\n       [\'f\', \'b\', \'s\'],\n       [\'x\', \'n\', \'z\']],\n      dtype=\'&lt;U1\')\n'
"import pandas as pd\ndf = pd.DataFrame({'Values': [0, 1, 2, 3, 4]},\n                  index = [pd.Timestamp('20130101 09:00:00'),\n                           pd.Timestamp('20130101 09:00:02'),\n                           pd.Timestamp('20130101 09:00:03'),\n                           pd.Timestamp('20130101 09:00:05'),\n                           pd.Timestamp('20130101 09:00:06')])\npattern = [1,2,3]\nprint df.iloc[[int(df.index.get_indexer_for((df[df.Values==i].index))) for i in pattern]]\n"
'for col_num in selector.ranking_ :\n    print(yourDataFrame.columns[col_num])\n\nmask = selector.get_support() #list of booleans\n    new_features = [] #becomes the list of your K best features in the following loop\n\n    for bool, feature in zip(mask, feature_names):\n        if bool:\n            new_features.append(feature)\n'
"def conditions(row):\n     if row.Age &lt;= 18:\n         return 1\n     elif condition:\n         ...\n\ndf['isChild'] = df.apply(conditions, axis=1)\n"
'import re\nmyfile = open("file.txt", "r")\nfor i in myfile:\n    match = re.match(r\'\\d{4}$\', i[:-1])\n    print(match.string)\n'
'w3 += a3.T.dot(a4delta)\n\n w3 -= addBias(a3).T.dot(a4delta) * step\n\na3delta = a4delta.dot(w3.T) * (1 * (1 - a3)) \n\na3delta = a4delta.dot(w3.T) * (a3 * (1 - a3))\n\nep = 0.12\nw = np.random.random((39, 39)) * 2 * ep - ep\n\n# Weights have different shapes to account for bias node\nw = np.random.random((39, 39)) * 2 * ep - ep\nw2 = np.random.random((40, 39))* 2 * ep - ep\nw3 = np.random.random((40, 1)) * 2 * ep - ep\n\nep = 0.12\nw = np.random.random((39, 39)) * 2 * ep - ep\nw2 = np.random.random((40, 39))* 2 * ep - ep\nw3 = np.random.random((40, 1)) * 2 * ep - ep\n\ndef addBias(mat):\n    return np.hstack((np.ones((mat.shape[0], 1)), mat))\n\nstep = -.1\nfor j in range(200):\n    # Forward prop\n    a2 = 1/(1 + np.exp(- addBias(x).dot(w)))\n    a3 = 1/(1 + np.exp(- addBias(a2).dot(w2)))\n    a4 = 1/(1 + np.exp(- addBias(a3).dot(w3)))\n\n    # Back prop\n    a4delta = (y - a4) \n    # need to remove bias nodes here\n    a3delta = a4delta.dot(w3[1:,:].T) * (a3 * (1 - a3))\n    a2delta = a3delta.dot(w2[1:,:].T) * (a2 * (1 - a2))\n\n    # Gradient Descent\n    # Multiply gradient by step then subtract\n    w3 -= addBias(a3).T.dot(a4delta) * step\n    w2 -= addBias(a2).T.dot(a3delta) * step \n    w -= addBias(x).T.dot(a2delta) * step\nprint(np.rint(a4))\n'
"#Write\nimport pickle \nx = [(20, {21, 23}), (30, {22, 24, 26, 28})]\n\nwith open('pickle.txt','wb') as f:\n    pickle.dump(x,f)\n\n#Read\nwith open('pickle.txt','rb') as f:\n    y = pickle.load(f)\n"
's = \'"Wireless Internet","Air conditioning",Kitchen,Heating,"Family/kid friendly",Essentials,"Hair dryer",Iron,"translation missing: en.hosting_amenity_50"\'\n\nprint(s.replace(\'"\',\'\').split(","))\n\n[\'Wireless Internet\', \'Air conditioning\', \'Kitchen\', \'Heating\', \'Family/kid friendly\', \'Essentials\', \'Hair dryer\', \'Iron\', \'translation missing: en.hosting_amenity_50\']\n\ns = \'x = {"Wireless Internet","Air conditioning",Kitchen,Heating,"Family/kid friendly",Essentials,"Hair dryer",Iron,"translation missing: en.hosting_amenity_50"}\'\n\nprint(s.replace(\'"\',\'\').split("{")[1].rstrip(\'}\').split(","))\n'
"image_data = gfile.FastGFile(image_path, 'rb').read()\n\ndef read_images(image_paths):\n    with tf.Graph().as_default(), tf.Session() as sess:\n        file_name = tf.placeholder(tf.string)\n        jpeg_data = tf.read_file(jpeg_name)\n        decoded_image = tf.image.decode_jpeg(jpeg_data, channels=3)\n        images = []\n        for path in image_paths:\n            images.append(sess.run(decoded_image, feed_dict={file_name: path}))\n        return images\n\ndef extract_features(image_paths):\n    images = read_images(image_paths)\n    with tf.Session() as sess:\n        flattened_tensor = sess.graph.get_tensor_by_name('fc7/relu:0')\n        return sess.run(flattened_tensor, {'input:0': images})\n"
'headings = dct[\'alcohol\'] + dct[\'tobacco\'] + dct[\'onset\']\n\n#print(\'my headings:\'+ str(headings))\n\nl1 = [\'Heavy drinking, Low consumption, Former Smoker, Gradual\', \'Low consumption, No consumption, Current on-and-off smoker, Sudden\', \'Heavy drinking, Current on-and-off smoker\']\n\n\nmlb = MultiLabelBinarizer()  # pass sparse_output=True if you\'d like\ndataMatrix = mlb.fit_transform(headings.split(\', \') for headings in l1)\n\nprint("My Classes: ")\nprint(mlb.classes_)\nprint(dataMatrix)\n'
'prob = np.dot(X_train,w)&gt;=0  \n\npred = [int(i) for i in prob]  \nz0= [1 if y_val[i,0]!=pred[i] else 0 for i in range(len(pred)) ]\nerror = sum(z0)/len(z0)\n'
"model.add(Conv2D(32,kernel_size=(3, 3),padding='same',input_shape=(500,366,3)))\n"
'spark = SparkSession \\\n.builder \\\n.appName("myApp") \\\n.config("spark.mongodb.input.uri", "mongodb://127.0.0.1/newumc.classification_data") \\\n.config("spark.mongodb.output.uri", "mongodb://127.0.0.1/newumc.classification_data") \\\n.getOrCreate()\ndf = spark.read.format("com.mongodb.spark.sql.DefaultSource").load()\nfield1 = df[1]\nfield2 = df[2]\n'
"gcloud ml-engine jobs submit training my_training_job \\\n--runtime-version 1.2 \\\n--job-dir=gs://myapp.appspot.com/train \\\n--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz \\\n--module-name object_detection.train \\\n--region us-central1 \\\n--config object_detection/samples/cloud/cloud.yml \\\n-- \\\n-- train_dir=gs://myapp.appspot.com/train \\\n-- pipeline_config_path=gs://myapp.appspot.com/data/ssd_mobilenet_v1_coco.config\n\nmodels/research/setup.py\n\nREQUIRED_PACKAGES = ['Pillow&gt;=1.0', 'matplotlib']\n\npython setup.py sdist\n(cd slim &amp;&amp; python setup.py sdist)\n"
'from functools import partial\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics.pairwise import chi2_kernel\n\n# import the iris dataset (http://en.wikipedia.org/wiki/Iris_flower_data_set)\niris = datasets.load_iris()\ntrain_features = iris.data[:, :2]  # Here we only use the first two features.\ntrain_labels = iris.target\n\nmy_chi2_kernel = partial(chi2_kernel, gamma=1)\n\nclassifier = svm.SVC(kernel=my_chi2_kernel)\n\nclassifier = classifier.fit(train_features, train_labels)\n\nprint("Train Accuracy : " + str(classifier.score(train_features, train_labels)))\n\ndef my_chi2_kernel(X):\n    gamma = 1\n    nom = np.power(X[:, np.newaxis] - X, 2)\n    denom = X[:, np.newaxis] + X\n    # NOTE: We need to fix some entries, since division by 0 is an issue here.\n    #       So we take all the index of would be 0 denominator, and fix them.\n    zero_denom_idx = denom == 0\n    nom[zero_denom_idx] = 0\n    denom[zero_denom_idx] = 1\n\n    return np.exp(-gamma * np.sum(nom / denom, axis=len(X.shape)))\n'
'...\nfc_0 = tf.nn.relu(tf.matmul(x, W_fc0) + b_fc0)\nfc_0 = tf.nn.dropout(fc_0, keep_prob=0.3)\nfc_1 = tf.nn.relu(tf.matmul(fc_0, W_fc1) + b_fc1)\nfc_1 = tf.nn.dropout(fc_1, keep_prob=0.3)\n...\n'
"KNN_ = neighbors.KNeighborsRegressor(n_neighbors=20, weights='distance').fit(X, y)\n\nyhat = KNN_.predict(X)\n"
'for idx in range(0,len(x)):\n    print idx,x[idx],transform_binarize[idx]\n\n278 [1L] [1.]\n279 [0L] [0.]\n280 [2L] [1.]\n281 [0L] [0.]\n282 [3L] [1.]\n283 [0L] [0.]\n284 [2L] [1.]\n285 [4L] [1.]\n286 [2L] [1.]\n287 [0L] [0.]\n288 [0L] [0.]\n289 [0L] [0.]\n290 [1L] [1.]\n291 [0L] [0.]\n292 [2L] [1.]\n293 [2L] [1.]\n294 [1L] [1.]\n295 [0L] [0.]\n296 [3L] [1.]\n297 [1L] [1.]\n298 [1L] [1.]\n299 [2L] [1.]\n300 [3L] [1.]\n301 [1L] [1.]\n302 [0L] [0.]     #&lt;--- I think you missed this row while reading your dataset\n'
'from sklearn.neighbors import KNeighborsRegressor\n# define the no. of nearest neighbors k\ntrain_size, train_scores, validation_scores = learning_curve(estimator = KNeighborsRegressor(n_neighbors=k), [...])\n\nfrom sklearn.neighbors import KNeighborsClassifier\n# define the no. of nearest neighbors k\ntrain_size, train_scores, validation_scores = learning_curve(estimator = KNeighborsClassifier(n_neighbors=k), [...])\n'
"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n\nmodel = Sequential()\nmodel.add(emb_layer)\nmodel.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5, return_sequences=True)\nmodel.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))\nmodel.add(Dense(2))\nmodel.add(Activation(‘sigmoid’))\n\nmodel = Sequential()\nmodel.add(emb_layer)\nmodel.add(Convolution1D(32, 3, padding=’same’))\nmodel.add(Activation(‘relu’))\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(32, dropout(0.5, recurrent_dropout=0.5, return_sequences=True))\nmodel.add(LSTM(64, dropout(0.5, recurrent_dropout=0.5))\nmodel.add(Dense(2))\nmodel.add(Activation(‘sigmoid’))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nfrom keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(patience=3)\n\nmodel.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping])\n"
'from sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nclf = KNeighborsClassifier()\nclf.fit(X, y)\n\n# here I am taking a single point only\ndistances, indices = clf.kneighbors(X[[0]],  n_neighbors=2)\n\nprint(distances, indices)\n\n#Output: array([[0., 0.]]), array([[17,  0]])\n'
"model_checkpoint = ModelCheckpoint('fcn32_weights.{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', save_best_only=True)\n"
'from keras.applications.inception_v3 import decode_predictions\n\npreds = model.predict(x)\nprint(\'Predicted:\', decode_predictions(preds, top=10))\n\n# Predicted: [(u\'n02504013\', u\'Indian_elephant\', 0.0042589349), ...]\n\ndef decode_predictions(preds, top=5, **kwargs):\n    """Decodes the prediction of an ImageNet model.\n    # Arguments\n        preds: Numpy tensor encoding a batch of predictions.\n        top: Integer, how many top-guesses to return.\n    # Returns\n        A list of lists of top class prediction tuples\n        `(class_name, class_description, score)`.\n        One list of tuples per sample in batch input.\n    # Raises\n        ValueError: In case of invalid shape of the `pred` array\n            (must be 2D).\n    """\n\nfrom keras.applications.imagenet_utils import decode_predictions\n'
"concat = Concatenate(axis=-1)([b1.output, b2.output])\n# or you can use the functional api as follows:\n#concat = concatenate([b1.output, b2.output], axis=-1)\n\nx = Dense(256, activation='relu', kernel_initializer='normal',\n          kernel_regularizer=keras.regularizers.l2(l2_lambda))(concat)\nx = Dropout(0.25)(x)\noutput = Dense(num_classes, activation='softmax')(x)\n\nmodel = Model([b1.input, b2.input], [output])\n"
'ys = []\nfor i in range(20):\n    ys.append(y_batch[:,i])\n\nyield(x_batch, ys)\n\ny_batch = np.zeros(shape=(32, 40))\n'
'model.add(Dropout(0.35, input_shape=(1200,)))\n'
'result1 = np.random.random((12,4))\nvectorOnes= np.ones((result1.shape[0],1))\n\n&gt;&gt;&gt; result1\narray([[0.24082843, 0.31800901, 0.01556211, 0.32774249],\n       [0.41475486, 0.90611202, 0.00791056, 0.49544814],\n       [0.22842928, 0.97168093, 0.1808639 , 0.32310355],\n       [0.99674441, 0.97379065, 0.7482597 , 0.9397243 ],\n       [0.37714731, 0.94101763, 0.73416157, 0.36625995],\n       [0.16470904, 0.97471554, 0.58262108, 0.67246731],\n       [0.40309562, 0.88545376, 0.40600242, 0.06040476],\n       [0.65425856, 0.15789502, 0.09350497, 0.49837995],\n       [0.65652148, 0.00545527, 0.68244463, 0.38962242],\n       [0.4012334 , 0.67545283, 0.09977628, 0.18019942],\n       [0.67110475, 0.45046098, 0.24962163, 0.71436953],\n       [0.32890942, 0.6090705 , 0.71712907, 0.35790405]])\n&gt;&gt;&gt; vectorOnes\narray([[1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [1.]])\n\nnew_results = np.concatenate((vectorOnes, result1),axis=1)\n&gt;&gt;&gt; new_results\narray([[1.        , 0.24082843, 0.31800901, 0.01556211, 0.32774249],\n       [1.        , 0.41475486, 0.90611202, 0.00791056, 0.49544814],\n       [1.        , 0.22842928, 0.97168093, 0.1808639 , 0.32310355],\n       [1.        , 0.99674441, 0.97379065, 0.7482597 , 0.9397243 ],\n       [1.        , 0.37714731, 0.94101763, 0.73416157, 0.36625995],\n       [1.        , 0.16470904, 0.97471554, 0.58262108, 0.67246731],\n       [1.        , 0.40309562, 0.88545376, 0.40600242, 0.06040476],\n       [1.        , 0.65425856, 0.15789502, 0.09350497, 0.49837995],\n       [1.        , 0.65652148, 0.00545527, 0.68244463, 0.38962242],\n       [1.        , 0.4012334 , 0.67545283, 0.09977628, 0.18019942],\n       [1.        , 0.67110475, 0.45046098, 0.24962163, 0.71436953],\n       [1.        , 0.32890942, 0.6090705 , 0.71712907, 0.35790405]])\n\n&gt;&gt;&gt; np.append(vectorOnes, result1, axis=1)\narray([[1.        , 0.24082843, 0.31800901, 0.01556211, 0.32774249],\n       [1.        , 0.41475486, 0.90611202, 0.00791056, 0.49544814],\n       [1.        , 0.22842928, 0.97168093, 0.1808639 , 0.32310355],\n       [1.        , 0.99674441, 0.97379065, 0.7482597 , 0.9397243 ],\n       [1.        , 0.37714731, 0.94101763, 0.73416157, 0.36625995],\n       [1.        , 0.16470904, 0.97471554, 0.58262108, 0.67246731],\n       [1.        , 0.40309562, 0.88545376, 0.40600242, 0.06040476],\n       [1.        , 0.65425856, 0.15789502, 0.09350497, 0.49837995],\n       [1.        , 0.65652148, 0.00545527, 0.68244463, 0.38962242],\n       [1.        , 0.4012334 , 0.67545283, 0.09977628, 0.18019942],\n       [1.        , 0.67110475, 0.45046098, 0.24962163, 0.71436953],\n       [1.        , 0.32890942, 0.6090705 , 0.71712907, 0.35790405]])\n'
'estimator.score(X_test,Y_test)\n'
'from pyspark.ml.evaluation import RegressionEvaluator\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=RegressionEvaluator(),\n                          numFolds=2)\n'
"main_input = Input(shape=(self.seq_len, 1), name='main_input')\n&lt;...&gt;\nauxiliary_input = Input(shape=(self.seq_len,1), name='aux_input')\n\nmain_input = Input(shape=(self.seq_len, ), name='main_input')\n&lt;...&gt;\nauxiliary_input = Input(shape=(self.seq_len, ), name='aux_input')\n"
'import keras.backend as K\n\ndef hardlim(x):\n   return K.cast(K.greater_equal(x,0), K.floatx())\n'
'num_classes = 4\nlabels = np.subtract(labels, 1) # assuming your labels start at 1 instead of 0\nlabels = np.squeeze(np.eye(num_classes)[np.array(labels).reshape(-1)])\n'
'a = np.random.rand(2, 3, 128)\nb = a.reshape(a.shape + (1, 1))\n'
'# Assuming you defined a graph, placeholders and logits layer.\n# Using cross entropy loss:\nlambda_ = 0.1\nxentropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\nys = tf.reduce_mean(xentropy)\nl2_norms = [tf.nn.l2_loss(v) for v in tf.trainable_variables()]\nl2_norm = tf.reduce_sum(l2_norms)\ncost = ys + lambda_*l2_norm\n# from here, define optimizer, train operation and train ... :-)\n'
"def parse(example_proto):\n    features = {\n        'label' : tf.FixedLenFeature((), tf.int64),\n        'image' : tf.FixedLenFeature((), tf.string)\n    }\n    parsed_features = tf.parse_single_example(example_proto, features)\n    img_shape = tf.stack([height, width, channel])\n    image = tf.decode_raw(parsed_features['image'], tf.float32)\n    image = tf.reshape(image, img_shape)\n    label = tf.cast(parsed['label'], tf.int32)\n    return image, label\n\ndef int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\nwriter = tf.python_io.TFRecordWriter('training.tfrecord')\n\nfor image_path, label in zip(X_train, y_train):\n    image = cv2.imread(image_path)\n    image = cv2.resize(image, (150, 150)) / 255.0\n    img_raw = image.tostring()\n    ex = tf.train.Example(features=tf.train.Features(feature={                                                                     \n                        'image': bytes_feature(img_raw),\n                        'label': int64_feature(label)\n                         }))\n    writer.write(ex.SerializeToString())\nwriter.close()\n"
"x = tf.placeholder(tf.float32, [None, input_dim])\npre_act_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\nact_1 = tf.nn.relu(pre_act_1)\npre_act_2 = tf.add(tf.matmul(act_1, weights['h2']), biases['b2'])\nact_2 = tf.nn.relu(pre_act_2)\n\npred = session.run(act_2, feed_dict={x: some_value_for_x})\n\na1, a2 = session.run(act_1, act_2, feed_dict={x: some_value_for_x})\n\npa1, a1, pa2, a2 = session.run(pre_act_1, act_1, pre_act_2, act_2, feed_dict={x: some_value_for_x})\n"
'X = tf.placeholder(tf.uint8, shape=[None, None, None, 3])\n\nim = Image.open(\'./sample/maltiz.png\')\nim3 = im.resize((300, 300))\n\nimage = np.asarray(im)[:,:,:3]\n\nmodel_path = \'models/ssd_mobilenet_v2_coco_2018_03_29/\'\n\nmeta_path = os.path.join(model_path, \'model.ckpt.meta\')\nmodel = tf.train.import_meta_graph(meta_path)\n\nsess = tf.Session()\nmodel.restore(sess, tf.train.latest_checkpoint(model_path))\n\ndata = np.array([image])\ndata = data.astype(np.uint8)\n\ngraph = tf.get_default_graph()\nX = graph.get_tensor_by_name(\'image_tensor:0\')\n\nfor i in graph.get_operations():\n    if "Relu" in i.name:\n        print(sess.run(i.values(), feed_dict = { X : data}))\n'
'Total:  print(1605/(1605+8902)) = 0.1527553059864852\n\nTrain:  print(1289/(1289+7116)) = 0.1533610945865556\nTest:   print(316/(316+1786)) = 0.15033301617507136\n\nTrain:  print(1284/(1284+7121)) = 0.15276621058893516\nTest:   print(321/(321+1781)) = 0.1527117031398668\n'
'import pandas as pd\nfrom sklearn import metrics\n\n\ndf = pd.read_csv("master.csv")\nlabels = list(df[\'Q3 Theme1\'])\nX = open(\'entire_dataset__resnet50_feature_vectors.txt\')\n#X_Data = X.read()\n\nfv = []\nfor line in X:\n    line = line.strip("\\n")\n    tmp_arr = line.split(\' \')\n    print(tmp_arr)\n    fv.append(tmp_arr)\n\nprint(fv)\nprint(\'Silhouette Score:\', metrics.silhouette_score(fv, labels,\n                                                    metric=\'cosine\'))\n'
"model = Sequential()\nmodel.add(Embedding(42, output_dim=64, input_length=40))\nmodel.add(LSTM(256,input_shape=(40,1),return_sequences=True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(42,activation='softmax'))\n"
'from sklearn.linear_model import LinearRegression\nX=np.array(df.drop("time",1))\ny=np.array(df["time"])\nclf=LinearRegression()\nclf.fit(X,y)\n'
'for images, steer in batch_generator(data_dir, X_train, y_train, 5, True):\n    # do something...\n'
'# out = self.relu(out) # this too\n'
'filepath="best_weight.hdf5"\n\nclassifier.load_weights(\'best_weight.hdf5\')\n'
"modelnet_path = '/modelnet10.npz'\ndata = np.load(modelnet_path)\nX, Y = data['X_train'], data['y_train']\nX_test, Y_test = data['X_test'], data['y_test']\nX = X.reshape(X.shape[0], 30, 30, 30, 1).astype('float32')\n\n...\ntrain_dataset = tf.data.Dataset.from_tensor_slices(X).batch(BATCH_SIZE)\n...\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for shape_batch in dataset:\n            train_step(shape_batch)\n\n        display.clear_output(wait=True)\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n    display.clear_output(wait=True)      \n\ntrain(X_test, EPOCHS)\n\n\ntrain(train_dataset , EPOCHS)\n"
"section     object\nLongitude   float\n\nfrom sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\nenc.fit(train.data['Section']() \ntrain.data['Section'] =enc.transform(train.data['Section']).toarray()\ntest.data['Section'] = enc.transform(test.data['Section']).toarray()\n"
'headlines = [" "*(max_len-len(i)) + i for i in headlines]\n'
'vocabulary = ["achtung", "suchen"]\n\nfor word in vocabulary:\n    df2[word] = 0\n\n    for index, row in df2.iterrows():\n        if word in row["title"].lower():\n            df2.set_value(index, word, 1)\n'
'res2 = tf.map_fn(lambda y: y*2, model.output)\n\n# Inital model that produces the output you want to map\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(2, input_shape=(2,)))\n\nres = tf.keras.layers.Lambda(lambda x: tf.map_fn(lambda y: y*2, x))(model.output)\n\nmodel2 = tf.keras.Model(inputs=model.inputs, outputs=res)\nprint(model2.predict(np.array([[1,2],[3,4]])))\n'
"    def loss(self,logits, labels):\n        z = tf.Variable(logits, trainable=False,dtype=tf.float32, name='z')\n        y = tf.Variable(labels, trainable=False,dtype=tf.float32, name='y')\n        m = tf.cast(tf.size(z), dtype=tf.float32)\n        cost = tf.divide(tf.reduce_sum(y*tf.math.log(z) + (1-y)*tf.math.log(1-z)),-m)\n        return cost\n\n    def loss(self,logits, labels):\n        z = logits\n        y = tf.constant(labels,dtype=tf.float32, name='y')\n        m = tf.cast(tf.size(z), dtype=tf.float32)\n        cost = (-1/m)*tf.reduce_sum(y*tf.math.log(z) + (1-y)*tf.math.log(1-z))\n        return cost\n"
"# initial (unscaled) x used here:\nx_train, x_test, y_train, y_test = train_test_split(x, predictions, test_size=.25)\nsc = StandardScaler()\nx_train_scaled = sc.fit_transform(x_train)\nx_test_scaled = sc.transform(x_test)\n\nclassifier = SVC(kernel='rbf')\nclassifier.fit(x_train_scaled, y_train) # no scaling for predictions or y_train\n"
"class CustomTransformer(BaseEstimator):\n\n    def __init__(self, percentile=.90):\n        self.percentile = percentile\n\n    def fit(self, X, y):\n        # Calculate thresholds for each column\n\n        # We have appended y as last column in X, so remove that\n        X_ = X.iloc[:,:-1].copy(deep=True)\n\n        thresholds = X_.loc[y == 9, :].quantile(q=self.percentile, interpolation='linear').to_dict()\n\n        # Store them for later use\n        self.thresholds = thresholds\n        return self\n\n    def transform(self, X):\n        # Create a copy of actual X, except the targets which are appended\n\n        # We have appended y as last column in X, so remove that\n        X_ = X.iloc[:,:-1].copy(deep=True)\n\n        # Use that here to get y\n        y =  X.iloc[:, -1].copy(deep=True)\n\n        # Replace values lower than the threshold for each column\n        for p in self.thresholds:\n            X_.loc[y != 9, p] = X_.loc[y != 9, p].apply(lambda x: 0 if x &lt; self.thresholds[p] else x)\n        return X_\n\n    def fit_transform(self, X, y):\n        return self.fit(X, y).transform(X)\n\n# We are appending the target into X\nexample_x = example[['feat1', 'feat2', 'target']]\nexample_y = example['target']\n"
'num_classes = 10\n\nxb = xb.view(-1, 64*8*8) # you get 750x4096\nout = self.linear(xb) # here an input of \n# input_size to linear layer = 4*4*64 # 1024\n# num_classes = 10 \n\nxb = xb.view(-1, 64*4*4) # you get 750x1024\nout = self.linear(xb) # M1 750x1024 M2 1024x10:\n# input_size = 4*4*64 # 1024\n# num_classes = 10 \n'
' Theta1_grad(:, 2:end) = Theta1_grad(:, 2:end) ./ m + ((lambda/m) * Theta1(:, 2:end));\n'
"&gt;&gt;&gt; def flatten_ingredients(d):\n...     # in-place version\n...     if isinstance(d.get('Ingredients'), list):\n...         for ingredient in d.pop('Ingredients'):\n...             d['Ingredients=%s' % ingredient] = True\n...     return d\n... \n&gt;&gt;&gt; RecipeData=[{'RecipeID':1,'Ingredients':['Flour','Milk'],'TimeToPrep':20}, {'RecipeID':2,'Ingredients':'Milk','TimeToPrep':5} ,{'RecipeID':3,'Ingredients':'Unobtainium','TimeToPrep':100}]\n&gt;&gt;&gt; map(flatten_ingredients, RecipeData)\n[{'Ingredients=Milk': True, 'RecipeID': 1, 'TimeToPrep': 20, 'Ingredients=Flour': True}, {'RecipeID': 2, 'TimeToPrep': 5, 'Ingredients': 'Milk'}, {'RecipeID': 3, 'TimeToPrep': 100, 'Ingredients': 'Unobtainium'}]\n\n&gt;&gt;&gt; from sklearn.feature_extraction import DictVectorizer\n&gt;&gt;&gt; dv = DictVectorizer()\n&gt;&gt;&gt; dv.fit_transform(flatten_ingredients(d) for d in RecipeData).toarray()\narray([[   1.,    1.,    0.,    1.,   20.],\n       [   0.,    1.,    0.,    2.,    5.],\n       [   0.,    0.,    1.,    3.,  100.]])\n&gt;&gt;&gt; dv.feature_names_\n['Ingredients=Flour', 'Ingredients=Milk', 'Ingredients=Unobtainium', 'RecipeID', 'TimeToPrep']\n"
"I had to do it on Linux Mint as I could not find a solution for windows. The modified answer is below.\n\n# Install compiler and tools\n$ sudo apt-get install build-essential libtool python-dev\n\n# Install cmake\n$ sudo apt-get install cmake\n\n$ wget https://github.com/libarchive/libarchive/releases/download/v3.4.1/libarchive-3.4.1.tar.xz\n$ tar -xJf libarchive-3.4.1.tar.xz\n\n# Configure using cmake...\n$ mkdir build\n$ cd build\n$ cmake -DCMAKE_INSTALL_PREFIX=/usr/local ../libarchive-3.4.1\n\n# Now compile and install...\n$ make\n$ sudo make install\n\n$ sudo sh -c 'echo /usr/local/lib &gt; /etc/ld.so.conf.d/libarchive3.conf'\n$ sudo ldconfig\n\n$pip install python-libarchive\n"
's = "Hello, how is it going ? I am tired actually, did not sleep   enough... That is bad for work, definitely"\ns = parse(s)\n\n#Create a list of all the tags you don\'t want\ndont_want = ["UH", "PRP"]\n\nsentence = parse(s).split(" ")\n\n#Go through all the words and look for any occurence of the tag you don\'t want\n#This is done through a nested list comprehension\n[word for word in sentence if not any(tag in word for tag in dont_want)]\n'
'def formData(dataset):\n\n    listOfTransaction = list()\n    listOfTransaction = dataset.values.tolist()\n    cleanedList = []\n    for transaction in listOfTransaction:\n        itemList = []\n        for item in transaction:\n            if False ==  pd.isnull(item):\n                itemList.append(item)\n        cleanedList.append(itemList)\n\n    return cleanedList\n\ndef formToPandas(result):\n    return pd.DataFrame(result)\n'
'cfr = RandomForestClassifier(n_estimators=100,n_jobs =5,{1:1,2:3,3:3,4:3})\n\ncfr = RandomForestClassifier(n_estimators=100,n_jobs=5,class_weight={1:1,2:3,3:3,4:3})\n'
'import numpy as np\nN = 100\n\nmean1, mean2, std1, std2 = 1, 5, 2, 3\nx0 = np.random.normal(loc=[mean1, mean2], scale=[std1, std2], size=(N, 2))\n\nmean3, mean4, std3, std4 = 2, -2, 1, 0.1\nx1 = np.random.normal(loc=[mean3, mean4], scale=[std3, std4], size=(N, 2))\n'
'L({(x_i,y_i)}, w) = SUM_i l(h(x_i|w), y_i) + C theta(w)\n\nL({(x_i,y_i)}, w) = SUM_i importance(y_i) * l(h(x_i|w), y_i) + C theta(w)\n'
'File "/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py", line 490, in mean\naxis = _normalize_axis(axis, ndim(x))\n\nFile "/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py", line 464, in prod\naxis = _normalize_axis(axis, ndim(x))\n'
'$ hdfs dfs -mkdir -p data/mllib\n$ hdfs dfs -put data/mllib/sample_libsvm_data.txt data/mllib\n'
'...\nconv = tf.layers.conv2d(...)\nflatten = tf.layers.flatten(...)\ninput_to_fc = tf.placeholder_with_default(flatten, shape=OUTPUT_SHAPE_OF_PREVIOUS_LAYER))\nfc = tf.layers.dense(input_to_fc, ...)\n'
'data = np.reshape(data, (-1, 2))\n\nlabels = np.reshape(labels, (-1, 2))\n'
'  X1_train = np.reshape(X1_train, (X1_train.shape[0], X1_train.shape[1] , 1))\n\n X2_train = np.reshape(X2_train, (X2_train.shape[0], X2_train.shape[1] , 1))\n\n\nnew_train  = np.dstack((X1_train, X2_train))\n\nnew_train  = np.dstack((X1_train, X2_train))\n'
'ConstantKernel(constant_value=1.0, constant_value_bounds=(1e-05, 100000.0))\n\nRBF(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))\n\nk(x_i, x_j) = exp(-1 / 2 d(x_i / length_scale, x_j / length_scale)^2)\n'
'import torch\n\ntorch.manual_seed(3515)\ntensor1 = torch.rand(2, 3)\ntensor2 = torch.rand(2, 2)\ntensor3 = torch.rand(2, 3)\npositions = torch.tensor([2, 0])\n\ntensor2.data.copy_(tensor1.data.index_select(1, positions))\ntensor3.data = tensor1.data.index_select(1, positions)\nprint(tensor2)\nprint(tensor3)\nprint(id(tensor1.data[0]))\nprint(id(tensor2.data[0]))\nprint(id(tensor3.data[0]))\n\ntensor([[ 0.5939,  0.8861],\n        [ 0.7525,  0.1537]])\ntensor([[ 0.5939,  0.8861],\n        [ 0.7525,  0.1537]])\n4583187080\n4583187008\n4583187080\n'
'for i in train.Tweet:\n    print i\nfor i in train.Affect_Dimension:\n    print i\n\nfor i in test.Tweet:\n    print i\nfor i in test.Affect_Dimension:\n    print i\n'
"LL = mu - 2*sigma # Lower limit \nUL = mu + 2*sigma # Upper limit\ndf['data'].clip(LL, UL)\n"
'global_step = tf.train.get_global_step()\n\nstep_value = sess.run(global_step)\n\nstep_value = global_step.eval(session=sess)\n'
'# Your example\n1 / 1 + np.exp(-30)\n\n# How it will be computed\n(1 / 1) + np.exp(-30)\n\n# What you actually wanted\n1 / (1 + np.exp(-30))\n\ndef sigmoid(z):\n    z = np.asarray(z)\n    return 1 / (1 + np.exp(-z))\n'
'#!/usr/bin/env python3\n\nimport numpy as np\n\ndata = np.array([0, 1, 2, 12, 13, 14, 15, 21, 22, 23])\nlabels = np.array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0])\nbounds = get_cluster_bounds(data, labels)\nprint(bounds) # {0: array([21,  2]), 1: array([12, 15])}\n\n#!/usr/bin/env python3\n\nimport numpy as np\n\n\ndef get_cluster_bounds(data: np.ndarray, labels: np.ndarray) -&gt; dict:\n    """\n    There are five ways in which the points of the cluster can be cyclically\n    considered. The points to be determined are marked with an arrow.\n\n    In the first case, the cluster data is distributed beyond the edge of\n    the cycle:\n         ↓B           ↓A\n    |#####____________#####|\n\n    In the second case, the data lies exactly at the beginning of the value\n    range, but without exceeding it.\n    ↓A        ↓B\n    |##########____________|\n\n    In the third case, the data lies exactly at the end of the value\n    range, but without exceeding it.\n                 ↓A       ↓B\n    |____________##########|\n\n    In the fourth, the data lies within the value range\n    without touching a border.\n            ↓A       ↓B\n    |_______##########_____|\n\n    In the fifth and simplest case, the data lies in the entire area without\n    another label existing.\n     ↓A                   ↓B\n    |######################|\n\n    Args:\n        data:      (n, 1) numpy array containing all data points.\n        labels:    (n, 1) numpy array containing all data labels.\n\n    Returns:\n        bounds:   A dictionary whose key is the index of the cluster and\n                  whose value specifies the start and end point of the\n                  cluster.\n    """\n\n    # Sort the data in ascending order.\n    shuffle = data.argsort()\n    data = data[shuffle]\n    labels = labels[shuffle]\n\n    # Get the number of unique clusters.\n    labels_unique = np.unique(labels)\n    num_clusters = labels_unique.size\n\n    bounds = {}\n\n    for c_index in range(num_clusters):\n        mask = labels == c_index\n        # Case 1 or 5\n        if mask[0] and mask[-1]:\n            # Case 5\n            if np.all(mask):\n                start = data[0]\n                end = data[-1]\n            # Case 1\n            else:\n                edges = np.where(np.invert(mask))[0]\n                start = data[edges[-1] + 1]\n                end = data[edges[0] - 1]\n\n        # Case 2\n        elif mask[0] and not mask[-1]:\n            edges = np.where(np.invert(mask))[0]\n            start = data[0]\n            end = data[edges[0] - 1]\n\n        # Case 3\n        elif not mask[0] and mask[-1]:\n            edges = np.where(np.invert(mask))[0]\n            start = data[edges[-1] + 1]\n            end = data[-1]\n\n        # Case 4\n        elif not mask[0] and not mask[-1]:\n            edges = np.where(mask)[0]\n            start = data[edges[0]]\n            end = data[edges[-1]]\n\n        else:\n            raise ValueError(\'This should not happen.\')\n\n        bounds[c_index] = np.array([start, end])\n\n    return bounds\n'
'best=SelectKBest(*args).fit(X_train,y_train)\nnew_train=best.transform(X_train)\nnew_test=best.transform(X_test)\n'
"x_test, y_test\n# (array([[2, 3, 4, 5]]), array([15]))\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n[...]\nprint(acc)\n# -88.65298209559413\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n[...]\nprint(acc)\n# -0.5210446297916358\n\ndata = pd.read_csv('example.csv', sep=';')\n"
"@solid\ndef load_raw_data(_):\n    yield Output('loaded_data')\n\n\n@solid\ndef process_data_into_table(_, raw_data):\n    yield Output(raw_data)\n\n\n@solid(\n    output_defs=[\n        OutputDefinition(name='fold_one', dagster_type=int, is_required=True),\n        OutputDefinition(name='fold_two', dagster_type=int, is_required=True),\n    ],\n)\ndef split_into_two_folds(_, table):\n    yield Output(1, 'fold_one')\n    yield Output(2, 'fold_two')\n\n\n@solid\ndef train_fold(_, fold):\n    yield Output('model')\n\n\n@solid\ndef evaluate_fold(_, model):\n    yield Output('compute_result')\n\n\n@composite_solid\ndef process_fold(fold):\n    return evaluate_fold(train_fold(fold))\n\n\n@solid\ndef summarize_results(context, fold_1_result, fold_2_result):\n    yield Output('summary_stats')\n\n\n@pipeline\ndef ml_pipeline():\n    fold_one, fold_two = split_into_two_folds(process_data_into_table(load_raw_data()))\n\n    process_fold_one = process_fold.alias('process_fold_one')\n    process_fold_two = process_fold.alias('process_fold_two')\n\n    summarize_results(process_fold_one(fold_one), process_fold_two(fold_two))\n"
'colors = ["green","red"]\n\nplt.scatter(X.iloc[:,0], X.iloc[:,1], c=np.array(colors)[labels], \n    s = 10, alpha=.1)\n\nplt.scatter(centroids[:, 0], centroids[:, 1], marker = "x", s=150, \n    linewidths = 5, zorder = 10, c=[\'green\', \'red\'])\nplt.show()\n\ndf = pd.DataFrame(list(\'dasdasas\'))\ndf[1]\n'
"import regex \n\ndef custom_analyzer(text):\n    words = regex.findall(r'\\w{2,}', text) # extract words of at least 2 letters\n    for w in words:\n        yield w\n\ncount_vect = CountVectorizer(analyzer = custom_analyzer)\nxv = count_vect.fit_transform(['she is a good girl','वो बहुत सुन्दर है','ఇది చాలా లాడిష్ మరియు బాల్య టీనేజ్ కుర్రాళ్ళు మాత్రమే దీనిని ఫన్నీగా చూడవచ్చు', 'దోపిడీ మరియు ఎక్కువగా లోతు లేదా అధునాతనత లేని నేరాలకు సంబంధించిన గ్రాఫిక్ చికిత్సను చూడటం భరించదగినది'])\ncount_vect.get_feature_names()\n"
"######Creating the model architecture by removing the last 5 layers from the network\nengineered_network = network.layers[-5].output\n\nengineered_network = Dropout(0.25)(engineered_network)\n\nengineered_network = Flatten()(engineered_network)\n\npredictions = Dense(7,activation='softmax')(engineered_network) \n"
'n = 100 \n\n# Create feature 1\nmean1 = 10 \nstandard_dev1 = 2\ncol1 = np.random.normal(loc=mean1,scale=standard_dev1,size=[n,1])\n\n# Create feature 2\nmean2 = 20\nstandard_dev2 = 4\ncol2 = np.random.normal(loc=mean2,scale=standard_dev2,size=[n,1])\n\ndata = np.concatenate([col1,col2],axis=1)\n\nprint(f"means of raw data: {data.mean(axis=0)}")\n&gt;&gt;&gt; \nmeans of raw data: [10.15783287 19.82541124]\n\n\nprint(f"standard devations of raw data: {data.std(axis=0)}")\n&gt;&gt;&gt;\nstandard devations of raw data: [2.00049111 3.87277793]\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nstandardized_data = StandardScaler().fit_transform(data)\n\nprint(f"means of standardized data: {standardized_data.mean(axis=0)}")\n&gt;&gt;&gt; \nmeans of standardized data: [-6.92779167e-16 -1.78745907e-15]\n\nprint(f"standard devations of standardized data: {standardized_data.std(axis=0)}")\n&gt;&gt;&gt; \nstandard devations of standardized data: [1. 1.]\n'
"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neural_network import MLPClassifier\nimport numpy as np\n\ndef hyperparameter_tune(clf, parameters, iterations, X, y):\n  randomSearch = RandomizedSearchCV(clf, param_distributions=parameters, n_jobs=-1, n_iter=iterations, cv=2) \n  randomSearch.fit(X,y)\n  params = randomSearch.best_params_\n  score = randomSearch.best_score_\n  return params, score\n\n\nparameters = {\n    'solver': ['sgd', 'adam', 'lbfgs'],\n    'activation': ['relu', 'tanh']\n}\nclf = MLPClassifier(batch_size=256, verbose=True, early_stopping=True)\n\nX_train_pca = np.random.randn(6,2)\ny = [0, 1, 0, 1, 0, 1]\nparameters_after_tuning, score_after_tuning = hyperparameter_tune(clf, parameters, 20, X_train_pca, y);\nprint(score_after_tuning)\n"
'class MyCallback(Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        lr = self.model.optimizer.lr\n        decay = self.model.optimizer.decay\n        iterations = self.model.optimizer.iterations\n        lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n        print(K.eval(lr_with_decay))\n\nlr = self.lr\nif self.initial_decay &gt; 0:\n    lr *= (1. / (1. + self.decay * K.cast(self.iterations, K.dtype(self.decay))))\n'
"features_1 = features_0.astype('float32')\n\nmodel = tf.keras.models.Sequential([\n    layers.Dense(units=16),\n    layers.Dense(units=8),\n    layers.Dense(units=1)\n])\nmodel(features_1)\nmodel.summary()\n"
'def fit(x, y, batch_size, epochs=1):\n    for epoch in range(epochs):\n        for batch_x, batch_y in batch(x, y, batch_size):\n            model.train_on_batch(batch_x, batch_y)\n'
"from sklearn.feature_extraction.text import CountVectorizer\ntext = ['this is a sentence', 'this is another sentence', 'not a sentence']\n\nvector = CountVectorizer(analyzer= 'word', tokenizer= None, max_features= 4500)\ndt = vector.fit_transform(text)\n\nprint(vector.vocabulary_) = {'this': 4, 'is': 1, 'sentence': 3, 'another': 0, 'not': 2}\n\ndata_features = vectorizer.fit_transform(text)\nprint(data_features.toarray())\n= [[0 1 0 1 1]\n [1 1 0 1 1]\n [0 0 1 1 0]]\n\n[0, 0, 0, 0, 0].\n\n[0            1(is)       0          1(sentence)           1(this)]\n[1(another)   1(is)       0          1(sentence)           1(this)]\n[0            0           1(not)     1(sentence)           0      ]\n"
"pg1 = {'kneighborsclassifier__n_neighbors': [3, 4, 5, 6, 7]}\npg2 = {'svc__C': [0.1, 1, 10, 100],\n       'svc__gamma': [0.001, 0.01, 0.1, 1, 10]}\npg3 = {'randomforestclassifier__n_estimators': [50, 100, 200, 300 ,400]}\npg4 = {'decisiontreeclassifier__max_depth': [12, 25, 50, 75, 100]}\npg5 = {'adaboostclassifier__n_estimators': [50, 100, 200, 300 ,400]}\npg6 = {'baggingclassifier__n_estimators': [50, 100, 200, 300, 400]}\n\nparam_grid_list = [pg1, pg2, pg3, pg4, pg5, pg6]\n\nfor m, p, mname in zip(models, param_grid_list, model_names):\n    pipe = make_pipeline(VarianceThreshold(threshold=1), \n                         MinMaxScaler(),\n                         SelectKBest(f_classif, k=20),  \n                         m)\n    grid = GridSearchCV(pipe, param_grid=p, cv=inner_cv)\n    grid.fit(X_train_test, y_train_test)\n    nested_score = cross_val_score(grid, X=X_train_test, y=y_train_test.values.ravel(), cv=outer_cv)\n    print(mname)\n    print(grid.best_params_)\n    print(grid.best_score_)\n    print('\\n')\n"
"import numpy as np\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]] # dummy data\nimputer = KNNImputer(n_neighbors=2)\nX_imp = imputer.fit_transform(X) # fit imputer &amp; transform training dta in 1 step\nX_imp\n# result:\narray([[1. , 2. , 4. ],\n       [3. , 4. , 3. ],\n       [5.5, 6. , 5. ],\n       [8. , 8. , 7. ]])\n\n# new (unseen - test) data with missing values:\n# we DON'T fit the imputer again\nX_new = np.array([[7, 3, 4], [np.nan, 8, 7]])\nX_new_imp = imputer.transform(X_new) # use the imputer already fitted with the training data\nX_new_imp\n# result:\narray([[7. , 3. , 4. ],\n       [5.5, 8. , 7. ]])\n"
"import numpy as np\nfrom tensorflow.keras.datasets import mnist\n\nclass_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\nclass NeuralNetwork():\n\n    correct = 0\n    epochs = 20\n    Lambda = 0\n    learningRate = 0.00001\n\n    def __init__(self, sizes, batchSize):\n        self.batchSize = batchSize\n        self.dimensions = sizes\n\n        self.secondLayerNeurons = np.empty(sizes[1])\n        self.outputNeurons = np.empty(sizes[2])\n\n        # Draw weights and biases from (-1, 1) by multiplying the (0, 1)\n        # values by 2 and subtracting 1. There are better ways of doing this,\n        # but this works just fine.\n        self.firstLayerWeights = np.random.rand(sizes[1], sizes[0]) * 2 - 1\n        self.secondLayerWeights = np.random.rand(sizes[2], sizes[1]) * 2 - 1\n        self.firstLayerBiases = np.random.rand(sizes[1]) * 2 - 1\n        self.secondLayerBiases = np.random.rand(sizes[2]) * 2 - 1\n\n        self.firstLayerWeightsSummations = np.zeros([sizes[1], sizes[0]])\n        self.secondLayerWeightsSummations = np.zeros([sizes[2], sizes[1]])\n        self.firstLayerBiasesSummations = np.zeros([sizes[1]])\n        self.secondLayerBiasesSummations = np.zeros([sizes[2]])\n\n        self.hiddenLayerErrors = np.empty(sizes[1])\n        self.outputLayerErrors = np.empty(sizes[2])\n\n    def sigmoid(self, x):\n        return 1/(1+np.exp(-x))\n\n    def sigmoidDerivative(self, x):\n        return np.multiply(x,(1-x))\n\n\n    def forwardProp(self, inputs):\n        for i in range (self.dimensions[1]):\n            self.secondLayerNeurons[i] = self.sigmoid(np.dot(self.firstLayerWeights[i], inputs)+self.firstLayerBiases[i])\n        for i in range (self.dimensions[2]):\n            self.outputNeurons[i] = self.sigmoid(np.dot(self.secondLayerWeights[i], self.secondLayerNeurons)+self.secondLayerBiases[i])\n\n    def backProp(self, inputs, correct_output):\n        self.outputLayerErrors = np.subtract(self.outputNeurons, correct_output)\n        self.hiddenLayerErrors = np.multiply(np.dot(self.secondLayerWeights.T, self.outputLayerErrors), self.sigmoidDerivative(self.secondLayerNeurons))\n\n        self.secondLayerBiasesSummations += self.outputLayerErrors\n        self.secondLayerWeightsSummations += np.outer(self.outputLayerErrors, self.secondLayerNeurons)\n\n        self.firstLayerBiasesSummations += self.hiddenLayerErrors\n        self.firstLayerWeightsSummations += np.outer(self.hiddenLayerErrors, inputs)\n\n    def train(self, trainImages, trainLabels):\n        size = str(self.batchSize)\n        err_sum = 0.0\n        err_count = 0\n        avg_err = 0.0\n\n        for m in range (self.batchSize):\n            correct_output = np.zeros([self.dimensions[2]])\n            correct_output[trainLabels[m]] = 1.0\n\n            self.forwardProp(trainImages[m].flatten())\n            self.backProp(trainImages[m].flatten(), correct_output)\n\n            if np.argmax(self.outputNeurons) == int(trainLabels[m]):\n                self.correct+=1\n\n            if m%150 == 0:\n                error = np.amax(np.absolute(self.outputLayerErrors))\n                err_sum += error\n                err_count += 1\n                avg_err = err_sum / err_count\n                accuracy = str(int((self.correct/(m+1))*100)) + '%'\n                percent = str(int((m/self.batchSize)*100)) + '%'\n                print (&quot;Progress: &quot; + percent + &quot; -- Accuracy: &quot; + accuracy + &quot; -- Error: &quot; + str(avg_err), end=&quot;\\r&quot;)\n\n        self.change()\n        print (size + '/' + size + &quot; -- &quot; + &quot; -- Accuracy: &quot; + accuracy + &quot; -- Error: &quot; + str(avg_err), end=&quot;\\r&quot;)\n        self.correct = 0\n\n    def change(self):\n\n        self.secondLayerBiases -= self.learningRate * self.secondLayerBiasesSummations\n        self.secondLayerWeights -= self.learningRate * self.secondLayerWeightsSummations\n        self.firstLayerBiases -= self.learningRate * self.firstLayerBiasesSummations\n        self.firstLayerWeights -= self.learningRate * self.firstLayerWeightsSummations\n\n        self.firstLayerSummations = np.zeros([self.dimensions[1], self.dimensions[0]])\n        self.secondLayerSummations = np.zeros([self.dimensions[2], self.dimensions[1]])\n        self.firstLayerBiasesSummations = np.zeros(self.dimensions[1])\n        self.secondLayerBiasesSummations = np.zeros(self.dimensions[2])\n\nif __name__ == &quot;__main__&quot;:\n\n    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n    train_images = train_images / 255 # Normalize image data\n\n    num_using = 60000 # Amount of data points to use. It's fast now, so we may as well use the full 60,000\n    bs = 1000 # Batch size. 60,000 is full batch. Consider trying mini-batch\n    neural_network = NeuralNetwork([784, 32, 10], bs)\n\n    for i in range (neural_network.epochs):\n        print (&quot;\\nEpoch&quot;, str(i+1) + &quot;/&quot; + str(neural_network.epochs))\n        for j in range(int(num_using / bs)):\n            print(&quot;Batch&quot;, str(j+1) + &quot;/&quot; + str(int(60000 / bs)))\n            neural_network.train(train_images[int(j * bs):int(j * bs) + bs], train_labels[int(j * bs):int(j * bs) + bs])\n"
'stages = stage_string + stage_one_hot + [assembler, rf]\npipeline = Pipeline(stages=stages)\n'
'last_layer_weights = old_model.layers[-1].get_weights()\nnew_model.layers[-1].set_weights(last_layer_weights)\n\n# Create\xa0an\xa0arbitrary\xa0model\xa0with\xa0some\xa0weights,\xa0for\xa0example\nmodel\xa0=\xa0Sequential(layers\xa0=\xa0[\n    Dense(70,\xa0input_shape\xa0=\xa0(100,)),\n    Dense(60),\n    Dense(50),\n    Dense(5)])\n\n#\xa0Save\xa0the\xa0weights\xa0of\xa0the\xa0model\nmodel.save_weights(“model.h5”)\n\n# Later, load in the model (we only really need the layer in question)\nold_model\xa0=\xa0Sequential(layers\xa0=\xa0[\n    Dense(70,\xa0input_shape\xa0=\xa0(100,)),\n    Dense(60),\n    Dense(50),\n    Dense(5)])\n\nold_model.load_weights(“model.h5”)\n\n# Create a new model with slightly different architecture (except for the layer in question, at least)\nnew_model\xa0=\xa0Sequential(layers\xa0=\xa0[\n    Dense(80,\xa0input_shape\xa0=\xa0(100,)),\n    Dense(60),\n    Dense(50),\n    Dense(5)])\n\n# Set the weights of the final layer of the new model to the weights of the final layer of the old model, but leaving other layers unchanged.\nnew_model.layers[-1].set_weights(old_model.layers[-1].get_weights())\n\n# Assert that the weights of the final layer is the same, but other are not.\nprint (np.all(new_model.layers[-1].get_weights()[0] == old_model.layers[-1].get_weights()[0]))\n&gt;&gt; True\n\nprint (np.all(new_model.layers[-2].get_weights()[0] == old_model.layers[-2].get_weights()[0]))\n&gt;&gt; False\n'
"import pandas as pd\nimport numpy as np\nfrom sklearn.base import TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.pipeline import make_pipeline\n\n\nclass CustomTransformer(TransformerMixin):\n    def __init__(self, some_stuff=None, column_names= []):\n        self.some_stuff = some_stuff\n        self.column_names = column_names\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        # do stuff on X, and return dataframe\n        # of the same shape - this gets messy\n        # if the preceding item is a numpy array\n        # and not a dataframe\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X, columns=self.column_names)\n        \n        X['str_len'] = X['my_str'].apply(lambda x: str(x)).str.len()\n        X['custom_func'] = X['val'].apply(lambda x: 1 if x &gt; 0.5 else -1)\n        return X\n\n\ndf = pd.DataFrame({\n    'my_str': [111, 2, 3333],\n    'val': [0, 1, 1]\n})\n\n# mixing this works as expected\nmy_pipeline = make_pipeline(StandardScaler(), CustomTransformer(column_names=[&quot;my_str&quot;, &quot;val&quot;]))\nmy_pipeline.fit_transform(df)\n\n# using this by itself works as well\nmy_pipeline = make_pipeline(CustomTransformer(column_names=[&quot;my_str&quot;, &quot;val&quot;]))\nmy_pipeline.fit_transform(df)\n\nIn [  ]: my_pipeline = make_pipeline(StandardScaler(), CustomTransformer(column_names=[&quot;my_str&quot;, &quot;val&quot;])) \n    ...: my_pipeline.fit_transform(df)                                                                                                                                                                                                  \nOut[  ]: \n     my_str       val  str_len  custom_func\n0 -0.671543 -1.414214       19           -1\n1 -0.742084  0.707107       18            1\n2  1.413627  0.707107       17            1\n\nIn [  ]: my_pipeline = make_pipeline(CustomTransformer(column_names=[&quot;my_str&quot;, &quot;val&quot;])) \n    ...: my_pipeline.fit_transform(df)                                                                                                                                                                                                  \nOut[  ]: \n   my_str  val  str_len  custom_func\n0     111    0        3           -1\n1       2    1        1            1\n2    3333    1        4            1\n\n\nfrom sklearn_pandas import DataFrameMapper\n\n# using sklearn-pandas\nstr_transformer = FunctionTransformer(lambda x: x.apply(lambda y: y.str.len()))\ncust_transformer = FunctionTransformer(lambda x: (x &gt; 0.5) *2 -1)\n\n\nmapper = DataFrameMapper([\n    (['my_str'], str_transformer),\n    (['val'], make_pipeline(StandardScaler(), cust_transformer))\n], input_df=True, df_out=True)\n\nmapper.fit_transform(df)\n\nIn [  ]: mapper.fit_transform(df)                                                                                                                                                                                                       \nOut[47]: \n   my_str  val\n0       3   -1\n1       2    1\n2       1    1\n\n"
"tf.data.experimental.make_csv_dataset(\n    file_pattern, \n    batch_size,\n    label_name=None, \n    select_columns=None, \n    shuffle=True,\n)\n\n# this is preprocessing step where you define preprocessing on images.\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# this is where you create iterator.\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\nmodel.fit(\n        train_generator,\n        steps_per_epoch=2000,\n        epochs=50,\n        validation_data=validation_generator,\n        validation_steps=800)\n"
'if (i+1)% batch_accumulation == 0:    \n    outputs_val = model(faces_val)\n\nloss = criterion(outputs[1], labels)\nloss = loss / batch_accumulation\n\nrunning_train_loss += loss.item()\n\noptimizer.zero_grad()\n'
'image_dataset = np.random.uniform(0, 1, (3000, 50, 120, 3))\nimage_dataset_y = np.random.uniform(0, embedding_dim, (3000,)).astype(np.int)\nval_dataset = np.random.uniform(0, 1, (300, 50, 120, 3))\nval_dataset_y = np.random.uniform(0, embedding_dim, (300,)).astype(np.int)\nmodel.fit(image_dataset, image_dataset_y , batch_size=30, epochs=10, validation_data=(val_dataset, val_dataset_y))\n'
"&gt;&gt;&gt; type(abalone_labels)\n&lt;class 'pandas.core.series.Series'&gt;\n\nabalone_model.fit(x = abalone_features, y = np.asarray(abalone_labels), epochs = 10)\n\nabalone_labels = np.array(abalone_labels)\n"
"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nds = tfds.load('iris', split='train', as_supervised=True)\n\ntrain = ds.take(125).shuffle(125).batch(1)\ntest = ds.skip(125).take(25).shuffle(25).batch(1)\n\nclass Dense(tf.Module):\n  def __init__(self, in_features, out_features, activation, name=None):\n    super().__init__(name=name)\n    self.activation = activation\n    self.w = tf.Variable(\n      tf.initializers.GlorotUniform()([in_features, out_features]), name='weights')\n    self.b = tf.Variable(tf.zeros([out_features]), name='biases')\n  def __call__(self, x):\n    y = tf.matmul(x, self.w) + self.b\n    return self.activation(y)\n\nclass SequentialModel(tf.Module):\n  def __init__(self, name):\n    super().__init__(name=name)\n    self.dense1 = Dense(in_features=4, out_features=16, activation=tf.nn.relu)\n    self.dense2 = Dense(in_features=16, out_features=32, activation=tf.nn.relu)\n    self.dense3 = Dense(in_features=32, out_features=3, activation=tf.nn.softmax)\n\n  def __call__(self, x):\n    x = self.dense1(x)\n    x = self.dense2(x)\n    x = self.dense3(x)\n    return x\n\nmodel = SequentialModel(name='sequential_model')\n\nloss_object = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n\ndef compute_loss(model, x, y):\n  out = model(x)\n  loss = loss_object(y_true=y, y_pred=out)\n  return loss, out\n\n\ndef get_grad(model, x, y):\n    with tf.GradientTape() as tape:\n        loss, out = compute_loss(model, x, y)\n        gradients = tape.gradient(loss, model.trainable_variables)\n    return loss, gradients, out\n\n\noptimizer = tf.optimizers.Adam()\n\nverbose = &quot;Epoch {:2d} Loss: {:.3f} TLoss: {:.3f} Acc: {:=7.2%} TAcc: {:=7.2%}&quot;\n\nfor epoch in range(1, 10 + 1):\n    train_loss = tf.constant(0.)\n    train_acc = tf.constant(0.)\n    test_loss = tf.constant(0.)\n    test_acc = tf.constant(0.)\n\n    for n_train, (x, y) in enumerate(train, 1):\n        loss_value, grads, out = get_grad(model, x, y)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        train_loss += loss_value\n        train_acc += tf.metrics.sparse_categorical_accuracy(y, out)[0]\n\n    for n_test, (x, y) in enumerate(test, 1):\n        loss_value, _, out = get_grad(model, x, y)\n        test_loss += loss_value\n        test_acc += tf.metrics.sparse_categorical_accuracy(y, out)[0]\n\n    print(verbose.format(epoch,\n                         tf.divide(train_loss, n_train),\n                         tf.divide(test_loss, n_test),\n                         tf.divide(train_acc, n_train),\n                         tf.divide(test_acc, n_test)))\n"
'_ , loss_result = sess.run([training_step, loss], \n                           feed_dict={\n                               image_tensor : image_batch,\n                               label_tensor : label_batch\n                           })\n'
'RMSE= np.sqrt(mean_squared_error(y_predict,y_test))\nRMSE\n'
"model.predict([cleaning_funcs('my bus departs in five minutes')])\n"
'ValueError: Incorrect number of features. Got 30 features, expected 29\n\nprint(df_kmeansA.head())\n\n===================\nShape of processed data: \n (12330, 29)\n'
"def parseCSV(filePath):\n      data = pd.read_csv(filePath)\n      data.to_sql('addresses', 'yoursqlconnection')\n"
"data.isnull().values.sum()\n663\n\ndata = data.fillna(data.mean())\ndata.isnull().values.sum()\n0\n\nalpha = 0.01\niters = 1000\ng,cost = gradientDescent(X,y,theta,iters,alpha)\nprint(g)\n[[ 3.37617073e-16 -1.03407822e-01 -7.06228959e-02  3.63774873e-01\n   2.51480688e-03  4.51868433e-01 -2.02133329e-01 -2.67955488e-01]]\nfinalCost = computeCost(X,y,g)\nprint(finalCost)\n0.21914950571622366\n\nfig, ax = plt.subplots()  \nax.plot(np.arange(iters), cost, 'r')  \nax.set_xlabel('Iterations')  \nax.set_ylabel('Cost')  \nax.set_title('Error vs. Training Epoch')\n\ndata = (data - data.mean())/data.std()\n"
'tf.Tensor(\n[[0.69813887]\n [0.67435524]\n [0.63301525]], shape=(3, 1), dtype=float64)\n'
'ones_subset = df.loc[df[&quot;label&quot;] == 1, :]\nnumber_of_1s = len(ones_subset)\n\nprint(number_of_1s)\n3\n\nzeros_subset = df.loc[df[&quot;label&quot;] == 0, :]\nsampled_zeros = zeros_subset.sample(number_of_1s)\n\nprint(sampled_zeros)\n\nclean_df = pd.concat([ones_subset, sampled_zeros], ignore_index=True)\n\nprint(clean_df)\n                                         id  label\n0  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n1  a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da      1\n2  7f6ccae485af121e0b6ee733022e226ee6b0c65f      1\n3  559e55a64c9ba828f700e948f6886f4cea919261      0\n4  f38a6374c348f90b587e046aac6079959adf3835      0\n5  068aba587a4950175d04c680d38943fd488d6a9d      0\n\ndownsampled_df = clean_df.groupby(&quot;label&quot;).sample(2)\n\nprint(downsampled_df)\n                                         id  label\n4  f38a6374c348f90b587e046aac6079959adf3835      0\n5  068aba587a4950175d04c680d38943fd488d6a9d      0\n1  a24ce148f6ffa7ef8eefb4efb12ebffe8dd700da      1\n0  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n'
'from keras.preprocessing import image\n    :\n    :\n    image = Image.open(foto)\n'
'rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;,factor=0.5, patience=1, \n                                          verbose=1)\nestop=tf.keras.callbacks.EarlyStopping(monitor=&quot;val_loss&quot;,patience=4,verbose=1,\n                                       restore_best_weights=True)\ncallbacks=[rlronp, estop]\n'
'c = np.sqrt(np.log(2 / (np.sqrt( 16 * delta + 1 ) -1 )))\n\nsigma = (c + np.sqrt(np.square(c) + epsilon) ) * s / (epsilon * np.sqrt(2))\n'
"from pulp import *\nimport numpy as np\nimport pandas as pd\n\n\n\ndateparse = lambda dates: pd.datetime.strptime(dates, '%m/%d/%Y')\nplayers = pd.read_csv(&quot;Date.csv&quot;, parse_dates=['DATE'], index_col='DATE',date_parser=dateparse)\nplayers[&quot;PG&quot;] = (players[&quot;POSITION&quot;] == 'PG').astype(float)\nplayers[&quot;SG&quot;] = (players[&quot;POSITION&quot;] == 'SG').astype(float)\nplayers[&quot;SF&quot;] = (players[&quot;POSITION&quot;] == 'SF').astype(float)\nplayers[&quot;PF&quot;] = (players[&quot;POSITION&quot;] == 'PF').astype(float)\nplayers[&quot;C&quot;] = (players[&quot;POSITION&quot;] == 'C').astype(float)\nplayers[&quot;SALARY&quot;] = players[&quot;SALARY&quot;].astype(float)\n\n\nplayers = players.reset_index()\ndate_list = list(set(players['DATE'])) #&lt;-- create the list of dates in the dataframe\ndate_list.sort()  #&lt;-- sort the dates\n\n\nfor var_date in date_list:\n    rows = []\n    model = LpProblem(&quot;problem&quot;, LpMaximize)\n    filter_players = players[players['DATE'] == var_date].reset_index(drop=True) #&lt;-- filter by date\n    total_points = {}\n    cost = {}\n    PG = {}\n    SG = {}\n    SF = {}\n    PF = {}\n    C = {}\n    number_of_players = {}\n    \n    for i, player in filter_players.iterrows():  #&lt;--then run on that filtered dataframe\n        var_name = 'x' + str(i) # Create variable name\n        decision_var = pulp.LpVariable(var_name, cat='Binary') # Initialize Variables\n    \n        total_points[decision_var] = player[&quot;POINTS&quot;] # Create PPG Dictionary\n        cost[decision_var] = player[&quot;SALARY&quot;] # Create Cost Dictionary\n        \n        # Create Dictionary for Player Types\n        PG[decision_var] = player[&quot;PG&quot;]\n        SG[decision_var] = player[&quot;SG&quot;]\n        SF[decision_var] = player[&quot;SF&quot;]\n        PF[decision_var] = player[&quot;PF&quot;]\n        C[decision_var] = player[&quot;C&quot;]\n        number_of_players[decision_var] = 1.0\n    \n    objective_function = pulp.LpAffineExpression(total_points)\n    model += objective_function\n    #Define cost constraint and add it to the model\n    total_cost = pulp.LpAffineExpression(cost)\n    model += (total_cost &lt;= 60000)\n    \n    PG_constraint = pulp.LpAffineExpression(PG)\n    SG_constraint = pulp.LpAffineExpression(SG)\n    SF_constraint = pulp.LpAffineExpression(SF)\n    PF_constraint = pulp.LpAffineExpression(PF)\n    C_constraint = pulp.LpAffineExpression(C)\n    total_players = pulp.LpAffineExpression(number_of_players)\n    \n    model += (PG_constraint &lt;= 2)\n    model += (SG_constraint &lt;= 2)\n    model += (SF_constraint &lt;= 2)\n    model += (PF_constraint &lt;= 2)\n    model += (C_constraint &lt;= 1)\n    model += (total_players &lt;= 9)\n    \n    model.solve()\n    \n    row = {}\n    for v in model.variables():\n        if v.varValue:\n            idx = int(v.name.replace('x',''))\n            player_name = filter_players.iloc[idx]['PLAYER']\n            position = filter_players.iloc[idx]['POSITION']\n            salary = filter_players.iloc[idx]['SALARY']\n            points = filter_players.iloc[idx]['POINTS']\n            row = {'DATE':var_date, 'PLAYER':player_name, 'POSITION':position, 'POINTS':points, 'SALARY':salary}\n            rows.append(row)\n            \n    lineup = pd.DataFrame(rows)\n    lineup.loc[9,'PLAYER'] = ''\n    lineup.loc[9,'POSITION'] = 'Total'\n    lineup.loc[9,'POINTS'] = lineup['POINTS'].sum(axis=0)\n    lineup.loc[9,'SALARY'] = lineup['SALARY'].sum(axis=0)   \n\n    print ('Lineup for: %s' %var_date.strftime('%m/%d/%Y'))   \n    print (lineup.iloc[:,1:])\n    print ('\\n\\n')\n\nLineup for: 01/25/2021\n                    PLAYER POSITION  POINTS   SALARY\n0             Daniel Theis       PF    44.1   4600.0\n1           Thaddeus Young       PF    44.3   4200.0\n2              Luka Doncic       PG    82.2  10700.0\n3             Delon Wright       PG    55.9   5600.0\n4  Shai Gilgeous-Alexander       SG    49.8   8200.0\n5          Carmelo Anthony       SF    39.7   4100.0\n6              Enes Kanter        C    49.9   6500.0\n7            Norman Powell       SG    42.2   5600.0\n8             LeBron James       SF    73.6   9800.0\n9                             Total   481.7  59300.0\n\n\n\nLineup for: 01/26/2021\n            PLAYER POSITION  POINTS   SALARY\n0     Terance Mann       SG    34.3   3500.0\n1     John Collins       PF    43.7   7400.0\n2       Trae Young       PG    49.1  10600.0\n3      David Nwaba       SG    34.3   5700.0\n4   Reggie Jackson       PG    47.4   5000.0\n5       RJ Barrett       SF    29.8   7500.0\n6    Royce O'Neale       PF    33.2   4500.0\n7      Rudy Gobert        C    55.8   8300.0\n8  De'Andre Hunter       SF    36.6   5600.0\n9                     Total   364.2  58100.0\n\n\n\nLineup for: 01/27/2021\n             PLAYER POSITION  POINTS   SALARY\n0  Precious Achiuwa       SF    35.7   3500.0\n1       Ben Simmons       PG    50.2   8200.0\n2   Khris Middleton       SF    48.5   7900.0\n3      Bradley Beal       SG    69.8  10300.0\n4        Chris Paul       PG    44.5   7400.0\n5     James Johnson       PF    34.9   4300.0\n6       Rudy Gobert        C    70.5   8000.0\n7    Taurean Prince       PF    36.9   4900.0\n8       Buddy Hield       SG    50.5   5500.0\n9                      Total   441.5  60000.0\n"
'model.add(Dense(1,activation=&quot;sigmoid&quot;))  # last layer\n\nmodel.compile(loss=&quot;binary_crossentropy&quot;,\n              optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;])\n'
'String = String.decode(\'utf-8\')\nString2 = String2.decode(\'utf-8\')\nbigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n                                     token_pattern=r\'\\b\\w+\\b\', min_df=1)\n\nX_train = bigram_vectorizer.fit_transform(np.array([String, String2]))\nprint type(X_train)\ny = np.array([1, 2])\nclf = SVC()\nclf.fit(X_train, y)\n\n#prepare test data\nprint(clf.predict(bigram_vectorizer.transform(np.array([X1, X2, ...]))))\n\nfrom sklearn.pipeline import Pipeline\n\nprint type(X_train) # Should be a list of texts length 100 in your case\ny_train = ... # Should be also a list of length 100\nclf = Pipeline([\n    (\'transformer\', CountVectorizer(...)),\n    (\'estimator\', SVC()),\n])\nclf.fit(X_train, y_train)\n\nX_test = np.array(["sometext"]) # array of test texts length = 1\nprint(clf.predict(X_test))\n'
"regressor.save('/tmp/tf_examples/my_model_1/')\n\nnew_regressor = TensorFlowEstimator.restore('/tmp/tf_examples/my_model_2')\n"
'x = tf.image.resize_images(x, [28, 28])\n\nx.get_shape().as_list() == [ None, width, height, channels] \n'
'X_train = np.array(["money deducted",\n                    "delivery is late",\n                    "something here",\n                    "payment problem"])\ny_labels = [(1, ), (2, ), (3, ), (1, )]\ny_train = MultiLabelBinarizer().fit_transform(y_labels)\ntarget_names = [\'cause1\', \'cause2\', \'cause48\']\nclassifier = Pipeline([\n    (\'vectorizer\', CountVectorizer()),\n    (\'tfidf\', TfidfTransformer()),\n    (\'clf\', OneVsRestClassifier(LinearSVC()))])\nclassifier.fit(X_train, y_train)\n\nmlb.fit_transform(y_labels)\n\nmlb.inverse_transform(classifier.predict(X_test))\n'
"scorer=make_scorer(f1_score, average='micro')\n\nscoring = 'f1_micro'\n"
'import glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimages = glob.glob(&lt;file pattern&gt;)\nimg_list = [plt.imread(image) for image in images]\nimg_array = np.stack(tuple(img_list))\n'
'pooled_outputs = [tf.reshape(out, [-1, 94, 1, self.max_length]) for out in pooled_outputs]\n'
'def getImage(filename):\n    # convert filenames to a queue for an input pipeline.\n    filenameQ = tf.train.string_input_producer([filename],num_epochs=None)\n\n    # object to read records\n    recordReader = tf.TFRecordReader()\n\n    # read the full set of features for a single example \n    key, fullExample = recordReader.read(filenameQ)\n\n    # parse the full example into its\' component features.\n    features = tf.parse_single_example(\n        fullExample,\n        features={\n            \'image/height\': tf.FixedLenFeature([], tf.int64),\n            \'image/width\': tf.FixedLenFeature([], tf.int64),\n            \'image/colorspace\': tf.FixedLenFeature([], dtype=tf.string,default_value=\'\'),\n            \'image/channels\':  tf.FixedLenFeature([], tf.int64),            \n            \'image/class/label\': tf.FixedLenFeature([],tf.int64),\n            \'image/class/text\': tf.FixedLenFeature([], dtype=tf.string,default_value=\'\'),\n            \'image/format\': tf.FixedLenFeature([], dtype=tf.string,default_value=\'\'),\n            \'image/filename\': tf.FixedLenFeature([], dtype=tf.string,default_value=\'\'),\n            \'image/encoded\': tf.FixedLenFeature([], dtype=tf.string, default_value=\'\')\n        })\n\n\n    # now we are going to manipulate the label and image features\n\n    label = features[\'image/class/label\']\n    image_buffer = features[\'image/encoded\']\n\n    # Decode the jpeg\n    with tf.name_scope(\'decode_jpeg\',[image_buffer], None):\n        # decode\n        image = tf.image.decode_jpeg(image_buffer, channels=3)\n\n        # and convert to single precision data type\n        image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n\n\n    # cast image into a single array, where each element corresponds to the greyscale\n    # value of a single pixel. \n    # the "1-.." part inverts the image, so that the background is black.\n    # re-define label as a "one-hot" vector \n    # it will be [0,1] or [1,0] here. \n    # This approach can easily be extended to more classes.\n    image=tf.reshape(image,[height,width,3])\n    label=tf.pack(tf.one_hot(label-1, nClass))\n\n    return label, image\n\n\nlabel, image = getImage("train-00000-of-00001")\nimageBatch, labelBatch = tf.train.shuffle_batch(\n    [image, label], batch_size=2,\n    capacity=20,\n    min_after_dequeue=10)\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n# start the threads used for reading files\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess,coord=coord)\nbatch_xs, batch_ys = sess.run([imageBatch, labelBatch])\nprint batch_xs\n\ncoord.request_stop()\ncoord.join(threads)\n'
'  dataSchema["dataFileContainsHeader"] = True,\n\n{"dataFileContainsHeader": [true], "attributes": [{"fieldName": "Var1", "fieldType": "CATEGORICAL"}, {"fieldName": "Var2", "fieldType": "CATEGORICAL"}, {"fieldName": "Var3", "fieldType": "NUMERIC"}, {"fieldName": "Var4", "fieldType": "CATEGORICAL"}, {"fieldName": "Var5", "fieldType": "CATEGORICAL"}, {"fieldName": "Var6", "fieldType": "CATEGORICAL"}], "version": "1.0", "dataFormat": "CSV", "targetFieldName": "Var6"}\n\n"dataFileContainsHeader": true\n'
"import numpy as np\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import StratifiedKFold\n\ndef rmse(a, b):\n    return np.sqrt(((a - b) ** 2).mean())\n\ndef incremental_learning2(X, y, n_splits=10, params = {}):    \n    # Initialize score arrays\n    sc_1, sc_2_v1, sc_2_v2 = (np.zeros(n_splits) for i in range(3))\n    # Create cross-validator\n    kfold = StratifiedKFold(n_splits=n_splits, random_state=0).split(X, y)\n    # Iterate through folds\n    for k, (train, test) in enumerate(kfold):\n        # Split data\n        X_test, y_test = X[test], y[test]    \n        splits = tts(X[train], y[train], test_size=0.5, random_state=0)\n        X_train_1, X_train_2, y_train_1, y_train_2 = splits\n        # Create data matrices\n        xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\n        xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\n        xg_test = xgb.DMatrix(X_test, label=y_test)    \n        # Fit models\n        model_1 = xgb.train(params, xg_train_1, 30)        \n        model_1.save_model('model_1.model')\n        model_2_v1 = xgb.train(params, xg_train_2, 30)\n        model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model='model_1.model')\n        # Make predictions and compute scores\n        preds = (m.predict(xg_test) for m in [model_1, model_2_v1, model_2_v2])\n        sc_1[k], sc_2_v1[k], sc_2_v2[k] = (rmse(p, y_test) for p in preds)\n    # Return scores\n    return sc_1, sc_2_v1, sc_2_v2\n\ndef display_results(a, b, c):\n    def hline(): \n        print('-'*50)\n    print('Cross-validation root mean square error\\n')    \n    print('Fold\\tmodel_v1\\tmodel_2_v1\\tmodel_2_v2')\n    hline()\n    for k, (ak, bk, ck) in enumerate(zip(a, b, c)):\n        print('%s\\t%.3f\\t\\t%.3f\\t\\t%.3f' % (k+1, ak, bk, ck))        \n    hline()\n    print('Avg\\t%.3f\\t\\t%.3f\\t\\t%.3f' % tuple(np.mean(s) for s in [a, b, c]))\n    print('Std\\t%.3f\\t\\t%.3f\\t\\t%.3f' % tuple(np.std(s) for s in [a, b, c]))\n\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=500000, centers=50, random_state=0)\n\nscores_1, scores_2_v1, scores2_v2 = incremental_learning2(X, y)\ndisplay_results(scores_1, scores_2_v1, scores2_v2)\n\nCross-validation root mean square error\n\nFold    model_v1        model_2_v1      model_2_v2\n--------------------------------------------------\n1       9.127           9.136           9.116\n2       9.168           9.155           9.128\n3       9.117           9.095           9.080\n4       9.107           9.113           9.089\n5       9.122           9.126           9.109\n6       9.096           9.099           9.084\n7       9.148           9.163           9.145\n8       9.089           9.090           9.069\n9       9.128           9.122           9.108\n10      9.185           9.162           9.160\n--------------------------------------------------\nAvg     9.129           9.126           9.109\nStd     0.029           0.026           0.028\n"
"# the original example:\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn import svm\nfrom sklearn.datasets import make_classification\nfrom sklearn.pipeline import make_pipeline\nimport pandas as pd\n\nX, y = make_classification(n_features=5, n_informative=5, n_redundant=0)\ndf_train = pd.DataFrame(X, columns=['A', 'B', 'C', 'D', 'E'])\ndf_train_y = pd.DataFrame(y)\n\nanova_filter = SelectKBest(f_regression, k=3)\nclf = svm.SVC(kernel='linear')\nanova_svm = make_pipeline(anova_filter, clf)\nf_reg_features = anova_svm.fit(df_train, df_train_y)\n\nimport eli5\nfeat_names = eli5.transform_feature_names(anova_filter, list(df.columns))\n\neli5.show_weights(anova_svm, feature_names=list(df.columns))\n"
'def batch_data(source, target, batch_size):\n\n   # Shuffle data\n   shuffle_indices = np.random.permutation(np.arange(len(target)))\n   source = source[shuffle_indices]\n   target = target[shuffle_indices]\n\n   for batch_i in range(0, len(source)//batch_size):\n      start_i = batch_i * batch_size\n      source_batch = source[start_i:start_i + batch_size]\n      target_batch = target[start_i:start_i + batch_size]\n\n      yield np.array(source_batch), np.array(target_batch)\n'
"# server.py\n\nfrom multiprocessing.managers import BaseManager\n# this represents the data structure you've already implemented.\nfrom ... import ExperienceTree\n\n# An important note: the way proxy objects work is by shared weak reference to\n# the object. If all of your workers die, it takes your proxy object with\n# it. Thus, if you have an instance, the instance is garbage-collected\n# once all references to it have been erased. I have chosen to sidestep \n# this in my code by using class variables and objects so that instances\n# are never used - you may define __init__, etc. if you so wish, but\n# just be aware of what will happen to your object once all workers are gone.\nclass ExperiencePool(object):\n\n    tree = ExperienceTree()\n\n    @classmethod\n    def update(cls, experience_object):\n        ''' Implement methods to update the tree with an experience object. '''\n        cls.tree.update(experience_object)\n\n    @classmethod\n    def sample(cls):\n        ''' Implement methods to sample the tree's experience objects. '''\n        return cls.tree.sample()\n\n# subclass base manager\nclass Server(BaseManager):\n    pass\n\n# register the class you just created - now you can access an instance of \n# ExperiencePool using Server.Shared_Experience_Pool().\nServer.register('Shared_Experience_Pool', ExperiencePool)\n\nif __name__ == '__main__':\n     # run the server on port 8080 of your own machine\n     with Server(('localhost', 8080), authkey=b'none') as server_process:\n         server_process.get_server().serve_forever()\n\n# client.py - you can always have a separate client file for a learner and a simulator.\n\nfrom multiprocessing.managers import BaseManager\nfrom server import ExperiencePool\n\nclass Server(BaseManager):\n     pass\n\nServer.register('Shared_Experience_Pool', ExperiencePool)\n\nif __name__ == '__main__':\n     # run the server on port 8080 of your own machine forever.\n     server_process = Server(('localhost', 8080), authkey=b'none')\n     server_process.connect()\n     experience_pool = server_process.Shared_Experience_Pool()\n     # now do your own thing and call `experience_call.sample()` or `update` whenever you want. \n"
"CountVectorizer(stop_words=['cat','dog', elephant'])\n"
"DataFrame.replace(\n        to_replace=None,\n        value=None,\n        inplace=False,\n        limit=None,\n        regex=False, \n        method='pad',\n        axis=None)\n"
"input_tensor = Input(shape=(224, 224, 3))\nt_model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\nt_output = Reshape((49, 512))(t_model.output)\ns_model = applications.VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\nfor layer in s_model.layers:\n    layer.name += '_1'\ns_output = Reshape((49, 512))(s_model.output)\nmerged = Dot(axes=1)([s_output, t_output])\n"
'xX = []\n\nfor index in range (len(X)):\n    if labels[index] == 0:\n        xX.append(X[index])\n\nX[labels == 0]\n\n[[  5.92321022e+06   1.42500000e+01   1.66852000e+03   1.49537378e+12]\n [  1.10927257e+06   7.47000000e+00   4.97500000e+01   1.49541180e+12]\n [  4.50160493e+06   4.21000000e+01   1.25270000e+03   1.49542090e+12]]\n'
"def parse_data_file(file_path):\n    games = []\n    current_game = []\n    with open(file_path, mode='r',) as file_reader:\n        for line in file_reader:\n            if len(line.strip()) == 0:\n                if len(current_game) &gt; 0:\n                    # A empty new line, so the current game has finished. Add the current game to the games.\n                    games.append(current_game)\n                    current_game = []\n            else:\n                current_game.append(line.strip().split())\n\n    return games\n\ndef parse_data_file(file_path):\n    games = []\n    current_game = []\n    with open(file_path, mode='r',) as file_reader:\n        for line in file_reader:\n            if len(line.strip()) == 0:\n                if len(current_game) &gt; 0:\n                    # A empty new line, so the current game has finished. Add the current game to the games.\n                    games.append(current_game)\n                    current_game = []\n            else:\n                current_game.append(map(lambda item: int(item), line.strip().split()))\n\n    return games\n"
'def column_for_treatment(design_info, factor, value):\n    column_name = "{}[T.{}]".format(factor, value)\n    return design_info.column_name_indexes[colum_name]\n\ncolumn_for_treatment(x.design_info, "a", "a2")\n'
'else:\n        self.w = None\n\nif not self.svm_model: # check not None\n    # pickle here\n'
'image_placeholder=tf.placeholder(tf.float32,shape=[batch_size,224,224,3])\nlabel_placeholder=tf.placeholder(tf.int64,shape=[batch_size,1])\n\nimage_placeholder=tf.placeholder(tf.float32,shape=image.shape)\nlabel_placeholder=tf.placeholder(tf.int64,shape=label.shape)\n'
'from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX, Y = cancer.data, cancer.target\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(test_size=0.1, random_state=23)\nfor train_index, valid_index in sss.split(X, Y):\n        X_train, X_valid = X[train_index], X[valid_index]\n        y_train, y_valid = Y[train_index], Y[valid_index]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,Y, test_size = 0.1, random_state=0)\n'
'import numpy as np\n\nfeatures = np.array(features)\nX = features.T\n\nclf.fit(X,....)\n'
'def featureExtraction(data):\n    vectorizer = TfidfVectorizer(min_df=10, max_df=0.75, ngram_range=(1,3))\n    tfidf_data = vectorizer.fit_transform(data)\n\n    # Here I am returning the vectorizer as well, which was used to generate the training data\n    return vectorizer, tfidf_data\n...\n...\ntfidf_vectorizer, tfidf_data = featureExtraction(data)\n...\n...\n\n# Now using the same vectorizer on test data\nX_test= tfidf_vectorizer.transform(Xnew)\n...\n'
"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\n\nclf = DecisionTreeClassifier(random_state=0)\n\niris = load_iris()\n\nclf.fit(iris.data,iris.target)\n#DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n#       max_features=None, max_leaf_nodes=None,\n#        min_impurity_decrease=0.0, min_impurity_split=None,\n#        min_samples_leaf=1, min_samples_split=2,\n#        min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n#        splitter='best')\n\n#Now extract the parameters\nparameters_dt = clf.get_params()\n\n#Now change the parameter you want\nparameters_dt['max_depth'] = 3\n\n#Now create a new classifier\nnew_clf = DecisionTreeClassifier(**parameters_dt)\n#DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n#        max_features=None, max_leaf_nodes=None,\n#        min_impurity_decrease=0.0, min_impurity_split=None,\n#        min_samples_leaf=1, min_samples_split=2,\n#        min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n#        splitter='best')\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=4,\n                        n_informative=2, n_redundant=0,\n                       random_state=0, shuffle=False)\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\n\nclf.fit(X, y)\n\nclf_list = clf.estimators_\nfor idx in range(0,len(clf_list)):\n    #Get the current Decision Tree in Random Forest\n    estimator = clf_list[idx]\n\n    #Get the params\n    temp_params = estimator.get_params()\n\n    #Change the params you want\n    temp_params['max_depth'] = 3\n\n    #Create a new decision tree\n    temp_decision_tree = DecisionTreeClassifier(**temp_params)\n\n    #Remove the old decision tree\n    clf.estimators_.pop(idx)\n\n    #Then insert the new decision tree at the current position\n    clf.estimators_.insert(idx, temp_decision_tree)\n"
'resized_input_tensor = tf.placeholder(tf.float32, [None, height, width, 3])\n\n{\n  "image": [\n    [\n      [0.0, 0.0, 0.0],\n        # ... (num entries = height)\n    ],\n    # (num entries = width)\n  ]\n}\n'
'from keras import activations\n\ndef the_softmax(axis):\n    def my_softmax(x):\n        return activations.softmax(x, axis=axis)\n    return my_softmax\n\n# sequential model\nmodel.add(Dense(..., activation=the_softmax(1)))\n\n# functional model\noutput = Dense(..., activation=the_softmax(1))(prev_layer_output)\n\nfrom keras import backend as K\n\ndef the_softmax(axis):\n    def my_softmax(x):\n        return K.softmax(x, axis=axis)\n    return my_softmax\n\n# sequential model\nmodel.add(Lambda(the_softmax(1)))\n\n# functional model\noutput = Lambda(the_softmax(1))(prev_layer_output)\n'
'pca.score(X_test_pca, y=None)\n\npca.score(X_test_std, y=None)\n'
'       for layer in model.layers:\n        layer.trainable = False\n'
"    if prediction_type == 'Probability':\n        predictions = np.transpose([1 - predictions, predictions])\n        return predictions\n\nprc_auc = sklearn.metrics.precision_recall_curve(y_test, y_score[:, 1])\n"
'from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ntransformer = ColumnTransformer(\n    transformers=[\n        ("OneHot",        # Just a name\n         OneHotEncoder(), # The transformer class\n         [0]              # The column(s) to be applied on.\n         )\n    ]\n)\n\nX = [\n    [\'a\', 0],\n    [\'b\', 1],\n    [\'a\', 2]\n]\n\nprint(transformer.fit_transform(X))\n\n&gt; [[1. 0.]\n&gt;  [0. 1.]\n&gt;  [1. 0.]]\n'
'print(model1.summary())\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 10)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                110       \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                110       \n=================================================================\nTotal params: 220\nTrainable params: 220\nNon-trainable params: 0\n'
'impot numpy as np\n\nlst = np.array(range(10))\nlst\nOut[56]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nlst = np.vstack((lst[:-1], lst[1:]))\n\n# Getting the list with shape (2, len(lst)-1)\nlst\nOut[60]: \narray([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [1, 2, 3, 4, 5, 6, 7, 8, 9]])\n\n# Getting the list with shape (len(lst)-1, 2)\nlst = lst.T\nlst\nOut[61]: \narray([[0, 1],\n       [1, 2],\n       [2, 3],\n       [3, 4],\n       [4, 5],\n       [5, 6],\n       [6, 7],\n       [7, 8],\n       [8, 9]])\n\nlst2 = lst[1:].copy()\n'
'train_data = train_data.reshape(100,1)  # 100 samples of shape 1\n\n# you may not need to do the following necessarily\ntrain_labels = train_labels.reshape(100,1)  # 100 labels of shape 1\n\n# alternative way using np.expand_dims\ntrain_data = np.expand_dims(train_data, axis=-1)\n\ntrain_data = np.asarray([[x for x in range(100)]])\n'
'train_data = data[0:515,0]\ntrain_labels = data[0:515,1]\n\ntest_data = data[515:586,0]\ntest_labels = data[515:586,1]\n'
'from sklearn.model_selection import train_test_split\n\ndef my_train_test_split(x,y):\n    # split data train 70 % and test 30 %\n    x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.3,random_state=42)\n    #normalization\n    x_train_N = (x_train-x_train.mean())/(x_train.max()-x_train.min())\n    x_test_N = (x_test-x_test.mean())/(x_test.max()-x_test.min())\n\nmy_train_test_split(data,data_y)\n'
"train_size = float(len(all_x)*0.7)\nvalid_size = float(len(all_x)*0.2)\ntest_size = float(len(x_prime)*0.1)\n\nimport numpy as np\nimport pandas as pd\n\ndef train_valid_test(df, train_split=.7, valid_split=.2, seed=None):    \n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n\n    training_max_index = int(train_split * len(df.index))\n    validate_max_index = int(valid_split * len(df.index)) + training_max_index\n\n    training = df.ix[perm[:training_max_index]]\n    validation = df.ix[perm[training_max_index:validate_max_index]]\n    test = df.ix[perm[validate_max_index:]]\n\n    return training, validation, test\n\nimport numpy as np\nimport pandas as pd\n\ndef train_valid_test(x_data, y_data, train_split=.7, valid_split=.2, seed=None):\n    if len(x_data.index) != len(y_data.index):\n        raise Exception('x_data and y_data must contain the same number of data points'\n\n    np.random.seed(seed)\n    perm = np.random.permutation(x_data.index)\n    x_data = x_data.reindex(perm)\n    y_data = y_data.reindex(perm)\n\n    training_max_index = int(train_split * len(x_data.index))\n    validate_max_index = int(valid_split * len(x_data.index)) + training_max_index\n\n    X_train, y_train = x_data[:training_max_index], y_data[:training_max_index]\n    X_valid, y_valid = x_data[:validate_max_index], y_data[:validate_max_index]\n    X_test, y_test = x_data[validate_max_index:], y_data[validate_max_index:]\n\n    return X_train, X_valid, X_test, y_train, y_valid, y_test\n"
'new_probs = tf.reduce_sum(probs * tf.one_hot(indices=actions, depth=4))\n\nnew_probs = tf.reduce_sum(probs * tf.one_hot(indices=actions, depth=4),1)\n'
'pre = neigh.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, pre)\n'
'regressor_OLS = sm.OLS(endog=Y, exog=X_opt)\nresults = regressor_OLS.fit()\n\nregressor_OLS = sm.OLS(endog=Y, exog=X_opt).fit()\n'
"    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(605, 700, 3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='rmsprop',\n          loss='binary_crossentropy',\n          metrics=['accuracy'])\n"
'results = model.evaluate(test_data, test_labels, verbose=0)\n'
'from sklearn.preprocessing import StandardScaler\n\nscale = StandardScaler()\ntraining_set = scale.fit_tranform(data_train)\ntest_set = scale.transform(data_test)\n'
'model = Sequential()\nmodel.add(Dense(20,input_dim = 2,activation = \'relu\'))\nmodel.add(Dense(20,activation = \'relu\'))\nmodel.add(Dense(1,activation = \'sigmoid\'))\n\nmodel = Sequential()\n\nmodel.add(Dense(2,input_dim = 2,activation = LeakyReLU(alpha=0.3)))\nmodel.add(Dense(1,activation = \'sigmoid\'))\n\noptimizer = Adam(lr=0.01)\nmodel.compile(loss = "binary_crossentropy" , metrics = [\'accuracy\'], optimizer=optimizer)\n\nmodel.fit(train,train_label, epochs = 500, verbose=2)\n'
' def LogLikelihood(self):\n     n = self.num_obs\n     k = self.num_features\n     residuals = self.residuals\n\n     ll = -(n * 1/2) * (1 + np.log(2 * np.pi)) - (n / 2) * np.log(residuals.dot(residuals) / n)\n\n     return ll\n\n  def AIC_BIC(self):\n    ll = self.LogLikelihood()\n    n = self.num_obs\n    k = self.num_features + 1\n\n    AIC = (-2 * ll) + (2 * k)\n    BIC = (-2 * ll) + (k * np.log(n))\n\n    return AIC, BIC\n'
'class SimpleModel(tf.Module):\n\ndef __init__(self):\n    super(SimpleModel, self).__init__()\n\n    self.layer1 = keras.layers.Flatten(input_shape=(28, 28))\n    self.layer2 = keras.layers.Dense(128, activation=\'relu\')\n    self.dropout = keras.layers.Dropout(0.5)\n    self.layer3 = keras.layers.Dense(10, activation=\'softmax\')\n\ndef __call__(self, x, training=False):\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    # if training:\n    #     x = self.dropout(x)\n    x = self.layer3(x)\n    return x\n\n@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\ndef predict(self, x):\n\n    return tf.argmax(self(x), axis=1)\n\n\ndef loss(m, x, y):\n    logits = m(x, True)\n    return tf.keras.losses.categorical_crossentropy(tf.reshape(tf.one_hot(y, 10), (y.size, 10)), logits)\n\n\ndef grad(m, x, y):\n    with tf.GradientTape() as tape:\n    lv = loss(m, x, y)\n    return lv, tape.gradient(lv, m.trainable_variables)\n\n\nfashion_mnist = keras.datasets.fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = \nfashion_mnist.load_data()\n\nclass_names = [\'T-shirt/top\', \'Trouser\', \'Pullover\', \'Dress\', \'Coat\',\n           \'Sandal\', \'Shirt\', \'Sneaker\', \'Bag\', \'Ankle boot\']\n\nmodel = SimpleModel()\n\n# model.compile(optimizer=\'adam\',\n#               loss=\'sparse_categorical_crossentropy\',\n#               metrics=[\'accuracy\'])\n#\n# model.fit(train_images, train_labels, epochs=10)\n\nepochs = 100\nbatch_size = 1000\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-5)\n\nfor epoch in range(epochs):\n\n    costs = []\n    for k in range(int(train_images.shape[0] / batch_size)):\n        start_index = k * batch_size\n        end_index = (k + 1) * batch_size\n        batch_x, batch_y = train_images[start_index:end_index, :, :], \n        train_labels[start_index:end_index]\n\n        loss_value, grads = grad(model, batch_x, np.reshape(batch_y, (1000, 1)))\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        costs.append(loss_value.numpy())\n    print("epoch %d of %d, cost %f" % (epoch, 10, np.mean(costs)))\n\nsignatures = {"serving_default": model.predict}\n\ntf.saved_model.save(model, \'tf_test\', signatures)\n\nThe given SavedModel SignatureDef contains the following input(s):\n    inputs[\'x\'] tensor_info:\n    dtype: DT_FLOAT\n    shape: (-1, 28, 28)\n    name: serving_default_x:0\nThe given SavedModel SignatureDef contains the following output(s):\n    outputs[\'output_0\'] tensor_info:\n    dtype: DT_INT64\n    shape: (-1)\n    name: StatefulPartitionedCall:0\nMethod name is: tensorflow/serving/predict\n'
"import io\nimport numpy as np    \nfrom PIL import Image\n\n# decode i'th image using: \nimg = Image.open(io.BytesIO(data_dir.data[i]))\nimg = np.asarray(img)\n\n# display i'th image\nimport matplotlib.pyplot as plt\n\nplt.imshow(img)\nplt.show()\n"
'X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n'
'output = model(b_x)[0]\n\noutput = model(b_x)\n'
"from keras import backend as K\n\ndef coeff_determination(y_true, y_pred):\n    SS_res =  K.sum(K.square( y_true-y_pred ))\n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n\nmodel.compile(optimizer='sgd', \n         loss='mse',\n         metrics = [coeff_determination])\n"
'import matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.measure import marching_cubes_lewiner\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n# mask is a currently stored binary 3D numpy array \nverts, faces, normals, values = marching_cubes_lewiner(mask)\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection="3d")\n\nax.set_xlim(np.min(verts[:,0]), np.max(verts[:,0]))\nax.set_ylim(np.min(verts[:,1]), np.max(verts[:,1])) \nax.set_zlim(np.min(verts[:,2]), np.max(verts[:,2]))\n\nmesh = Poly3DCollection(verts[faces])\nmesh.set_edgecolor(\'k\')\nax.add_collection3d(mesh)\nplt.tight_layout()\nplt.show()\n'
'params = {"C" : 0.01, "class_weight" : "balanced", "max_iter" : 10000,\n          "n_jobs" : -1, "penalty" : "l1", "random_state" : 42}\n\nfrom sklearn.linear_model import LogisticRegression\n\nLogisticRegression(**params)\nLogisticRegression(C=0.01, class_weight=\'balanced\', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=10000, multi_class=\'auto\', n_jobs=-1, penalty=\'l1\',\n                   random_state=42, solver=\'lbfgs\', tol=0.0001, verbose=0,\n                   warm_start=False)\n'
'import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nregr = DecisionTreeRegressor(max_depth=2)\nregr.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_pred = regr.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, s=20, edgecolor="black",\n            c="darkorange", label="data")\nplt.plot(X_test, y_pred, color="cornflowerblue",\n         label="max_depth=2", linewidth=2)\nplt.xlabel("data")\nplt.ylabel("target")\nplt.title("Decision Tree Regression")\nplt.legend()\nplt.show()\n\nplt.figure()\nplot_tree(regr, filled=True)\nplt.show()\n\nregr.predict(np.array([0]).reshape(1,-1))\n# array([0.05236068])\n'
'def func_to_apply(row) :\n      val = [row[\'B2\'],row[\'B3\'],row[\'B4\'],\n             row[\'B5\'],row[\'B6\'],row[\'B7\'],\n             row[\'B8\'],row[\'B11\'],row[\'B12\']]\n\n      corr = np.corrcoef(randompixels,val)\n      return corr\n\ndata["corr"] = data.apply(func_to_apply,axis=1)\ndata[data["corr"].max()==data["corr"]]["SOC"]\n\nfrom pandarallel import pandarallel\npandarallel.initialize()\n\ndata["corr"] = data.parallel_apply(func_to_apply,axis=1)\ndata[data["corr"].max()==data["corr"]]["SOC"]\n'
'model = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(224,224,3)),  # input shape required\n  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n  tf.kears.layers.Flatten(),\n  tf.keras.layers.Dense(2)\n])\n'
"import pandas as pd\n\n# save filepath to variable for easier access\nmelbourne_file_path = 'input/melb_data.csv'\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path)\n# print a summary of the data in Melbourne data\nprint(melbourne_data.describe())\n"
"import skimage\nfrom skimage import data, exposure, img_as_float\nfrom skimage.morphology import reconstruction\nimport matplotlib.pyplot as plt\n\nimg = skimage.io.imread('blemish.jpeg')\ngray_img = 255*skimage.color.rgb2gray(img) # multiply by 255 to get back in the [0;255] range\nimg_inten = exposure.rescale_intensity(gray_img,in_range=(50,100))\n\ndiliation_seed = img_inten.copy()\ndiliation_seed[1:-1,1:-1] = img_inten.min()\nmask = img_inten\n\neroded_img = reconstruction(diliation_seed,mask,method='dilation')\nplt.imshow(eroded_img,cmap='gray')\nplt.show()\n"
"model.compile('adam', 'mse')\n\nmodel.fit(X_train, y_train, sample_weight=weight_freq, ...)\n\ny_true = np.random.uniform(0,1, 1000) # just for example\nW_dict = {1:10,2:83,3:23,4:43,5:42,6:24,7:34,8:23,9:12,10:99} # just for example\n\ny_true_bin = np.digitize(a, bins=np.linspace(0,1, 11))\nweight_freq = [W_dict[i] for i in y_true_bin]\nweight_freq = np.asarray(weight_freq)\n"
'sequences = [torch.tensor([[5,6]]), torch.tensor([[2,1],[3,0]])]\npacked_seq = torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=False)\n\nsequences = torch.tensor([[2,1],[5,6],[3,0]])\nindexes = torch.tensor([1,0,1])\nnum_seq = np.unique(indexes)\nsequences = [sequences[indexes==seq_id] for seq_id in num_seq]\n'
'train_X, test_X, train_Y, test_Y  = train_test_split(X, Y, test_size=0.25, \n                                                     stratify=Y,\n                                                     random_state=42)\n'
'# import the necessary packages\nimport numpy as np\nimport cv2\nimport os\n\nmax_image = 1000\n\nclass SimpleDatasetLoader:\n    def __init__(self, preprocessors=None):\n        # store the image preprocessor\n        self.preprocessors = preprocessors\n\n        # if the preprocessors are None, initialize them as an\n        # empty list\n        if self.preprocessors is None:\n            self.preprocessors = []\n\n    def load(self, imagePaths, verbose=-1):\n        # initialize the list of features and labels\n        data = []\n        labels = []\n        cnt = 0\n\n        # loop over the input images\n        for (i, imagePath) in enumerate(imagePaths):\n            if cnt &gt;= max_image:\n                break\n            # load the image and extract the class label assuming\n            # that our path has the following format:\n            # /path/to/dataset/{class}/{image}.jpg\n            image = cv2.imread(imagePath)\n            label = imagePath.split(os.path.sep)[-2]\n\n            # check to see if our preprocessors are not None\n            if self.preprocessors is not None:\n                # loop over the preprocessors and apply each to\n                # the image\n                for p in self.preprocessors:\n                    image = p.preprocess(image)\n\n            # treat our processed image as a "feature vector"\n            # by updating the data list followed by the labels\n            data.append(image)\n            labels.append(label)\n\n            # show an update every `verbose` images\n            cnt += 1\n            if verbose &gt; 0 and i &gt; 0 and (i + 1) % verbose == 0:\n                print("[INFO] processed {}/{}".format(i + 1,\n                    len(imagePaths)))\n\n        # return a tuple of the data and labels\n        return (np.array(data, dtype=\'float32\')/255., np.array(labels))\n\nH = model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=4,\n    epochs=100, verbose=1)\n'
'X_test = np.array(X_test).reshape(1, -1)\nprint(X_test.shape)    \n# (1, 22)\n\ny_pred = SupportVectorRefModel.predict(X_test)\ny_pred\n# array([0.90156667])\n\nX_test,  y_test  = data.loc[1001], target.loc[1001] \n\nX_test,  y_test  = data.loc[1001:], target.loc[1001:]\nX_test.shape\n# (999, 22)\n\ny_pred = SupportVectorRefModel.predict(X_test)\ny_pred.shape\n# (999,)\n'
'print(outliers_df[model.labels_==-1]) \n   revenue  profit  country_CH  country_IN  country_JP\n0    11434    2345           0           1           0\n1    12132    3445           0           0           0\n2    21343    4545           1           0           0\n3    34423    3432           1           0           0\n4    43435    3234           0           0           1\n5    34345    3335           0           1           0\n'
'dE / dw_j = dE / dy * dy / dw_j\ndE / du_jk = dE / dz_j * dz_j / du_jk \n           = dE / dy * dy / dz_j * dz_j / du_jk\n\ndE / dw_1j = dE / dy_1 * dy_1 / dw_1j\ndE / dw_2j = dE / dy_2 * dy_2 / dw_2j\ndE / du_jk = dE / dz_j * dz_j / du_jk\n           = dE / dy_1 * dy_1 / dz_j * dz_j / du_jk + dE / dy_2 * dy_2 / dz_j * dz_j / du_jk\n'
'layer2_input = T.concatenate([layer2_input, self.f.flatten(2)])\n\nlayer2_input = T.concatenate([layer2_input, self.f.flatten(2)], axis=1)\n'
"$ python examples/document_classification_20newsgroups.py --all_categories\ndata loaded\n11314 documents - 22.055MB (training set)\n7532 documents - 13.801MB (test set)\n20 categories\n\nExtracting features from the training dataset using a sparse vectorizer\ndone in 2.849053s at 7.741MB/s\nn_samples: 11314, n_features: 129792\n\nExtracting features from the test dataset using the same vectorizer\ndone in 1.526641s at 9.040MB/s\nn_samples: 7532, n_features: 129792\n\n________________________________________________________________________________\nTraining: \nLinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n     random_state=None, tol=0.001, verbose=0)\ntrain time: 5.274s\ntest time:  0.033s\nf1-score:   0.860\ndimensionality: 129792\ndensity: 1.000000\n\n________________________________________________________________________________\nTraining: \nSGDClassifier(alpha=0.0001, class_weight=None, epsilon=0.1, eta0=0.0,\n       fit_intercept=True, l1_ratio=0.15, learning_rate='optimal',\n       loss='hinge', n_iter=50, n_jobs=1, penalty='l2', power_t=0.5,\n       random_state=None, shuffle=False, verbose=0, warm_start=False)\ntrain time: 3.521s\ntest time:  0.038s\nf1-score:   0.857\ndimensionality: 129792\ndensity: 0.390184\n\n________________________________________________________________________________\nTraining: \nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\ntrain time: 0.161s\ntest time:  0.036s\nf1-score:   0.836\ndimensionality: 129792\ndensity: 1.000000\n\n\n________________________________________________________________________________\nTraining: \nBernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\ntrain time: 0.167s\ntest time:  0.153s\nf1-score:   0.761\ndimensionality: 129792\ndensity: 1.000000\n"
"&gt;&gt;&gt; df = pd.DataFrame([dict1, dict2])\n&gt;&gt;&gt; df\nautoPay condition   country galleryURL  globalId    isMultiVariationListing itemId  listingInfo location    paymentMethod   postalCode  primaryCategory productId   returnsAccepted sellerInfo  sellingStatus   shippingInfo    title   topRatedListing viewItemURL\n0   true    {u'conditionId': u'3000', u'conditionDisplayNa...   US  http://thumbs4.ebaystatic.com/m/mO-HwGeodkgYX6...   EBAY-US false   201265198351    {u'listingType': u'StoreInventory', u'gift': u...   Dandridge,TN,USA    PayPal  37725   {u'categoryId': u'9355', u'categoryName': u'Ce...   {u'_type': u'ReferenceID', u'value': u'1825579...   true    {u'feedbackRatingStar': u'Turquoise', u'positi...   {u'currentPrice': {u'_currencyId': u'USD', u'v...   {u'expeditedShipping': u'true', u'shipToLocati...   Samsung Galaxy S5 SM-G900R4 (Latest Model) 16G...   false   http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G...\n1   false   {u'conditionId': u'3000', u'conditionDisplayNa...   US  http://thumbs4.ebaystatic.com/m/mO-HwGeodkgYX6...   EBAY-US false   201265198351    {u'listingType': u'StoreInventory', u'gift': u...   Dandridge,TN,USA    PayPal  37725   {u'categoryId': u'9355', u'categoryName': u'Ce...   {u'_type': u'ReferenceID', u'value': u'1825579...   true    {u'feedbackRatingStar': u'Turquoise', u'positi...   {u'currentPrice': {u'_currencyId': u'USD', u'v...   {u'expeditedShipping': u'true', u'shipToLocati...   Samsung Galaxy S5 SM-G900R4 (Latest Model) 16G...   false   http://www.ebay.com/itm/Samsung-Galaxy-S5-SM-G...\n"
"In [13]: df.drop_duplicates(subset=['title', 'year'])\n\nOut[13]:\n        title  votes  ranking  year\n0  Wonderland     19      7.9  1931\n1  Wonderland    120      7.1  1997\n2  Wonderland   3524      7.2  1999\n3  Wonderland  18169      6.6  2003\n4  Wonderland     17      8.7  2010\n5  Wonderland      6      8.5  2012\n"
"resultdict = df.T.to_dict()\n\nresultdict = df.to_dict(orient='index')\n\nIn [73]: df\nOut[73]:\n   Col1  Col2  Col3\na     1     2     3\nb     4     5     6\nc     7     8     9\n\nIn [74]: df.T.to_dict()\nOut[74]:\n{'a': {'Col1': 1, 'Col2': 2, 'Col3': 3},\n 'b': {'Col1': 4, 'Col2': 5, 'Col3': 6},\n 'c': {'Col1': 7, 'Col2': 8, 'Col3': 9}}\n\nclass Sample:\n    def __init__(self,D_key_value):\n        self.D_key_value = D_key_value \n\nresultdict = df.T.to_dict()\n\nresultdict = {k:Sample(v) for k,v in resultdict.items()}\n"
' [f(tuple(l)) for l in y_train]\n'
'x_t = (x1_t, x2_t, x3_t)\n\nx_{t+1} = (x1_{t+1}, x2_{t+1}, x3_{t+1})\n\nx_new = (x1_t, x2_t, x3_t, x1_{t+1}, x2_{t+1}, x3_{t+1})\n'
'In [608]:\nfrom sklearn.feature_extraction import FeatureHasher\nh = FeatureHasher(n_features=2**20,non_negative=True,input_type = \'string\')\ntest = [["unit_a","unit_b","unit_c"],["unit_c","unit_d","unit_c"],["unit_f","unit_aa"]]\ntest = [[\',\'.join(x) for x in test]] # join and reshape\nX_new = h.transform(test)\ntest,X_new.nonzero()\n\nOut[608]:\n([[\'unit_a,unit_b,unit_c\', \'unit_c,unit_d,unit_c\', \'unit_f,unit_aa\']],\n (array([0, 0, 0], dtype=int32), array([231648, 410028, 497899], dtype=int32)))\n\nIn [610]:\ntest = [["unit_a","unit_b","unit_c"],["unit_c","unit_d","unit_c"],["unit_f","unit_aa"]]\ntest_hash = [hash(tuple(x))%2**20 for x in test]\ntest_hash\n\nOut[610]:\n[736062, 345078, 521256]\n'
'var_types = np.array([cv2.CV_VAR_NUMERICAL] * var_n + [cv2.CV_VAR_ORDERED], np.uint8)\n'
"true_k = 4\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\n\nX2 = vectorizer.transform(documents2)\nX = np.vstack((X, X2))\nmodel.fit(X) # optimally you would start from the previous solution, but sklearn does not yet support it\n"
"In [35]: df\nOut[35]:\n   # Incidents Place  Month\n0            3     A      1\n1            5     B      1\n2            2     C      2\n3            2     B      2\n4            6     C      3\n5            3     A      1\n\nIn [36]: df.groupby('Place')['# Incidents'].sum().reset_index()\nOut[36]:\n  Place  # Incidents\n0     A            6\n1     B            7\n2     C            8\n\nIn [37]: df.groupby(['Place', 'Month'])['# Incidents'].sum().reset_index()\nOut[37]:\n  Place  Month  # Incidents\n0     A      1            6\n1     B      1            5\n2     B      2            2\n3     C      2            2\n4     C      3            6\n"
"numeric_cols = combined.columns[combined.dtypes != 'object']\ncombined.loc[:, numeric_cols] = combined[numeric_cols] / combined[numeric_cols].max()\n"
'input_state = tf.placeholder(tf.int64, shape=[None], name="input_state")\n'
'class ColumnNgram(BaseEstimator, TransformerMixin):\n    def __init__(self, colname, tokenizer, ngram_rg):\n        self.colname = colname\n        self.tokenizer = tokenizer\n        self.ngram_rg = ngram_rg\n        self.tfidf = None\n\n    def transform(self, df, y=None):\n        return self.tfidf.transform(df[self.colname].values)\n\n    def fit(self, df, y=None):\n        self.tfidf = TfidfVectorizer(tokenizer=self.tokenizer, ngram_range=self.ngram_rg)\n        self.tfidf.fit(df[self.colname].values)\n        return self\n'
'In [26]: import sklearn as sk\nIn [27]: from numpy import array\nIn [28]: model = sk.linear_model.LinearRegression()\nIn [29]: a = array(range(30)).reshape(10,3) # 10 samples, 3 features\nIn [30]: b = a**1.25 -0.25*a + 12           # 10 samples, 3 targets\nIn [31]: model.fit(a, b)\nOut[31]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)\nIn [32]: a[5], b[5], model.predict([a[5]])\nOut[32]: \n(array([15, 16, 17]),\n array([ 37.76984507,  40.        ,  42.26923414]),\n array([[ 39.47550026,  41.57922876,  43.75287898]]))\nIn [33]: \n'
'x.shape\n\nx = dataset.iloc[:,0]\nx = x.reshape((len(x),1))\n... \n'
"import numpy as np\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nnp.random.seed(42)\n\n# Generate some random data\n\nX_train = np.random.rand(1000, 3)\ny_train = np.random.randint(0, 2, len(X_train))\n\nX_test = np.random.rand(2000, 3)\ny_test = np.random.randint(0, 2, len(X_test))\n\n# Train a classifier\n\nclf = GaussianNB()\nclf.fit(X_train, y_train)\n\n# Test the classifier and print performance measure (here: accuracy)\n\ny_pred = clf.predict(X_test)\ny_random = np.random.randint(0, 2, len(X_test))\n\nprint('Accuracy (pred): {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('Accureary (random): {:.4f}'.format(accuracy_score(y_test, y_random)))\n\nAccuracy (pred): 0.5030\nAccureary (random): 0.5210\n"
'(-1, 0)\n\n(-1, 0, 1, 2, 3)\n'
"X = [({feature1:'val11', feature2:'val12' .... }, class1),\n     ({feature1:'val21', feature2:'val22' .... }, class2), \n     ...\n     ...                                                  ]\n\nfeature_names = cv.get_feature_names()\ntrain_set = []\nfor i, single_sample in enumerate(X):\n    single_feature_dict = {}\n    for j, single_feature in enumerate(single_sample):\n        single_feature_dict[feature_names[j]]=single_feature\n    train_set.append((single_feature_dict, y[i]))    \n\nnltk.NaiveBayesClassifier.train(train_set)\n"
'class Encoder(object):\n    def fit(self, X, *args, **kwargs):\n        return self\n    def transform(self, X):\n        return X*2\n\nclass Concatenator(object):\n    def fit(self, X, *args, **kwargs):\n        return self\n    def transform(self, Xs):\n        return np.concatenate(Xs, axis=0)\n\nclass MultiEncoder(Encoder):\n    def transform(self, Xs):\n        return list(map(super().transform, Xs))\n\npipe = sklearn.pipeline.Pipeline((\n    ("encoder", MultiEncoder()),\n    ("concatenator", Concatenator())\n))\n\npipe.fit_transform((\n    pd.DataFrame([[1,2],[3,4]]), \n    pd.DataFrame([[5,6],[7,8]])\n))\n'
'classifier.load_weights("saved_weights.h5")\nx = classifier.layers[-5].output   # use index of the layer directly\nx = GlobalAveragePooling2D()(x)\n'
'def generator():\n    ...\n    yield [input_samples1, input_samples2], targets\n\ndef generator():\n        ...\n        yield [in1, in2, ..., inM], [out1, out2, ..., outN]\n'
"output = Activation('tanh')(x)\n"
'import numpy as np\n\n# your comfusion matrix:\ncm =np.array([[1, 0, 1, 0],\n              [0, 1, 0, 0],\n              [1, 0, 0, 0],\n              [0, 0, 0, 1]])\n\n# true positives:\nTP = np.diag(cm)\nTP\n# array([1, 1, 0, 1])\n\n# false positives:\nFP = np.sum(cm, axis=0) - TP\nFP \n# array([1, 0, 1, 0])\n\n# false negatives\nFN = np.sum(cm, axis=1) - TP\nFN\n# array([1, 0, 1, 0])\n\nprecision = TP/(TP+FP)\nrecall = TP/(TP+FN)\n\nprecision\n# array([ 0.5,  1. ,  0. ,  1. ])\n\nrecall\n# array([ 0.5,  1. ,  0. ,  1. ])\n'
"extra_files = torch._C.ExtraFilesMap()\nextra_files['foo.txt'] = 'bar'\ntraced_script_module.save(serialized_model_path, _extra_files=extra_files)\n\nfiles = torch._C.ExtraFilesMap()\nfiles['foo.txt'] = ''\nloaded_model = torch.jit.load(serialized_model_path, _extra_files=files)\nprint(files)\n"
'x_new = x - x.min() / ( x.max() - x.min() )\n'
"from keras.layers import Dense, Activation\nfrom keras.models import Sequential\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=3))\nmodel.add(Dense(64, activation='relu')) # add more layers as necessary\nmodel.add(Dense(8))\nmodel.summary()  # use summary() to verify model architecture\n"
'from keras.models import Model\n\nx = Concatenate()([model.output, metadata_model.output])\nx = Dense(7)(x)\nout = Activation("softmax")(x)\n\nmerged_model = Model([model.input, metadata_model.input], out)\n\n# the rest is the same...\n'
"{'name': 'max_depth', 'type': 'discrete', 'domain': list(range(1, 51))}\n"
"losses_t = []\nlosses_v = []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    for i in range(3000):\n        _, loss_t = sess.run([train_op, loss], feed_dict = {x: X_tr, y: y_tr})\n        losses_t.append(loss_t)\n        loss_v = sess.run(lossv, feed_dict = {xv: X_val, yv: y_val})\n        losses_v.append(loss_v)\n        if(i % 20 == 0):\n            print('Training loss is: ', loss_t)\n            print('Validation loss is: ', loss_v)\n\n    W_value, b_value = sess.run([W, b])\n"
"tf.keras.models.load_model('model.h5', custom_objects = {'SimpleLayer': SimpleLayer})\n"
"theta0 = theta0 - learning_rate * 1/len(x) * sum (hypothesis(x) - y)\n# the update to theta1 is now using the updated version of theta0\ntheta1 = theta1 - learning_rate * 1/len(x) * sum((hypothesis(x) - y) * x)\n\n# modify to explicitly pass theta0/1\ndef hypothesis(x, theta0, theta1):\n    return theta0 + theta1 * x\n\n# explicitly pass y\ndef cost_function(x, y, theta0, theta1):\n    return 1 / (2 * len(x)) * sum((hypothesis(x, theta0, theta1) - y) ** 2)\n\nfor i in range(epochs):\n    print(f'{i}/ {epochs}')\n    # calculate hypothesis once\n    delta = hypothesis(x, theta0, theta1)\n    theta0 = theta0 - learning_rate * 1/len(x) * sum (delta - y)\n    theta1 = theta1 - learning_rate * 1/len(x) * sum((delta - y) * x)\n    print('\\ncost: {}\\ntheta0: {},\\ntheta1: {}'.format(cost_function(x, y, theta0, theta1))\n\n\n"
' import tensorflow\n model = tensorflow.keras.models.load_model("house_price.h5")\n y_pred=model.predict(X_test)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv(\'C:\\\\Users\\\\acer\\\\Downloads\\\\housepricedata.csv\')\ndataset.head()\n\nX=dataset.iloc[:,0:10]\ny=dataset.iloc[:,10]\n\nX.head()\nfrom sklearn.preprocessing import StandardScaler\nobj=StandardScaler()\nX=obj.fit_transform(X)\n\nfrom sklearn.model_selection import train_test_split\n X_train,X_test,y_train,y_test=train_test_split\n                                        (X,y,random_state=2020,test_size=0.25)\n\n  print(X_train.shape)\n  print(X_test.shape)\n  print(y_train.shape)\n  print(y_test.shape)\n\n  import keras\n  from keras.models import Sequential\n  from keras.layers import Dense\n  from keras.layers import Dropout\n  classifier = Sequential()\n\n    # Adding the input layer and the first hidden layer\n  classifier.add(Dense(units = 6, kernel_initializer = \'uniform\', activation = \n                                                      \'relu\', input_dim = 10))\n   # classifier.add(Dropout(p = 0.1))\n\n   # Adding the second hidden layer\n   classifier.add(Dense(units = 6, kernel_initializer = \'uniform\', activation \n                                                                   = \'relu\'))\n   # classifier.add(Dropout(p = 0.1))\n\n   # Adding the output layer\n   classifier.add(Dense(units = 1, kernel_initializer = \'uniform\', activation \n                                                               = \'sigmoid\'))\n\n       # Compiling the ANN\n  classifier.compile(optimizer = \'adam\', loss = \'binary_crossentropy\', metrics \n                                                          = [\'accuracy\'])\n\n  classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n  y_pred = classifier.predict(X_test)\n  y_pred = (y_pred &gt; 0.5)\n  print(y_pred)\n\n  classifier.save("house_price.h5")\n\n  import tensorflow\n  model = tensorflow.keras.models.load_model("house_price.h5")\n  y_pred=model.predict(X_test)\n  y_pred = (y_pred &gt; 0.5)\n  print(y_pred)\n\n  #True rep one\n\n  #false rep zero\n\n  #you can use replace function or map function of pandas  to get convert true \n into 1\n'
'train_X = train_X.reshape(-1, 28,28, 1)\ntest_X = test_X.reshape(-1, 28,28, 1)\n'
"df[df['Like'] == 1]['Style'].value_counts(normalize=True) * 100\n\ndf[df['Like'] == 1]['Typo'].value_counts(normalize=True) * 100\n\ndf[df['Like'] == 1]['Layout'].value_counts(normalize=True) * 100\n"
"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntrain = pd.DataFrame({'Text' :['I am a data scientist','Cricket is my favorite sport', 'I work on Python regularly', 'Python is very fast for data mining', 'I love playing cricket'],\n                      'Category' :['Data_Science','Cricket','Data_Science','Data_Science','Cricket']})\n\ntest = pd.DataFrame({'Text' :['I am new to data science field', 'I play cricket on weekends', 'I like writing Python codes'],\n                         'Category' :['Data_Science','Cricket','Data_Science']})\n\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(train['Text'])\nprint(vectorizer.get_feature_names())\n\n#['am', 'cricket', 'data', 'fast', 'favorite', 'for', 'is', 'love', 'mining', 'my', 'on', 'playing', 'python', 'regularly', 'scientist', 'sport', 'very', 'work']\n\nfeature_names = vectorizer.get_feature_names()\ndf= pd.DataFrame(X.toarray(),columns=feature_names)\n\nvectorizer_test = TfidfVectorizer()\nX_test = vectorizer_test.fit_transform(test['Text'])\nprint(vectorizer_test.get_feature_names())\n\n#['am', 'codes', 'cricket', 'data', 'field', 'like', 'new', 'on', 'play', 'python', 'science', 'to', 'weekends', 'writing']\nfeature_names_test = vectorizer_test.get_feature_names()\ndf_test= pd.DataFrame(X_test.toarray(),columns = feature_names_test)\n\nX_test_from_train = vectorizer.transform(test['Text'])\nfeature_names_test_from_train = vectorizer.get_feature_names()\ndf_test_from_train = pd.DataFrame(X_test_from_train.toarray(),columns = feature_names_test_from_train)\n"
'model.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'accuracy\'])\n\n# fit model\ntrain_y_ohe = pd.get_dummies(train_y)\nmodel.fit(train_X, train_y_ohe,epochs=1000,batch_size=20)\n\nloss, accuracy = model.evaluate(test_X, test_y_ohe, verbose=0)\nprint("Test fraction correct (Accuracy) = {:.2f}".format(accuracy))\n\n#Test fraction correct (Accuracy) = 0.97\n'
"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nx_train, x_test, y_train, y_test = train_test_split(features, results, test_size=0.3, random_state=0)\n\nclf = LogisticRegression()\nmodel = clf.fit(x_train, y_train)\n\ny_pred = clf.predict(x_test)\n\naccuracy_test = accuracy_score(y_test, y_pred)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\n\nclf = LogisticRegression()\n\n# shuffle data first:\nfeatures_s, results_s = shuffle(features, results)\naccuracy_cv = cross_val_score(clf, features_s, results_s, cv = 5, scoring='accuracy')\n\n# fit the model afterwards with the whole data, if satisfied with the performance:\nmodel = clf.fit(features, results)\n"
"import re\nstr = 'p!nk &amp; A$AP are 2 singers..... what? are the b0th rappers? ? ? NO!! #singer ##rapper @shady'\n\nstr = re.sub(r'(?&lt;=\\s)[\\W\\d](?=(\\s|$))', '', str)\nstr = re.sub(r'(?&lt;=\\w)\\W+(?=(\\s|$))', '', str)\nstr = re.sub(r'(\\W)\\1+(?=\\w)', r'\\1', str)\n\nprint(str)\n\np!nk A$AP are singers what are the b0th rappers NO #singer #rapper @shady\n"
'input_shape=(n_cols,) =&gt;&gt;  input_shape=(n_cols-1,)\n\nmodel.fit(xTrain, yTrain, validation_data=(xTrain, yTrain), epochs=50)\n'
"import sys\nfrom PIL import Image\nimport numpy as np\n\nimage_paths = ['image_path1.jpg', 'image_path2.jpg', 'image_path3.jpg']\nmin = sys.maxsize\nmax = -sys.maxsize\n\nfor image_path in image_paths:\n    image = Image.open(image_path)\n    np_image = np.asarray(image)\n    if min &gt; np_image.min()\n        min = np_image.min()\n    if max &lt; np_image.max()\n        max = np_image.max()\n\n    ...\n    pixels -= min\n    pixels /= (max - min)\n    ...\n"
'batch_size * number_of_data_in_fold = some_constant\n'
"import matplotlib.pyplot as plt\nfrom skimage.color import rgb2gray\nfrom skimage.transform import resize\nfrom skimage.io import imread, imshow\nimport pandas as pd\nimport numpy as np\nimport os\nfrom glob import glob\nimport warnings\nwarnings.simplefilter('ignore')\n\npath = &quot;C:/Users/IMA/Documents/ML/image/p1/*.png&quot;\np1 = glob(path)\np2 = os.listdir(&quot;C:/Users/IMA/Documents/ML/image/p2&quot;)\n\nlimit = 20\np1_image = [None]*limit\nj = 0\nfor i in p1:\n    if j &lt; limit:\n        p1_image[j] = imread(i)\n        j += 1\n    else:\n        break\n"
"embedding_size=300\nmax_len=40\nvocab_size=8256\n\nimage_model = Sequential([\n    Dense(embedding_size, input_shape=(2048,), activation='relu'),\n    RepeatVector(max_len)\n])\ncaption_model = Sequential([\n    Embedding(vocab_size, embedding_size, input_length=max_len),\n    LSTM(256, return_sequences=True),\n    TimeDistributed(Dense(300))\n])\n\nimage_in = Input(shape=(2048,))\ncaption_in = Input(shape=(max_len,))\nmerged = concatenate([image_model(image_in), caption_model(caption_in)],axis=-1)\nlatent = Bidirectional(LSTM(256, return_sequences=False))(merged)\nout = Dense(vocab_size, activation='softmax')(latent)\nfinal_model = Model([image_in, caption_in], out)\n\nfinal_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\nfinal_model.summary()\n"
'reg.partial_fit(X_minibatch, y_minibatch)\n\nfrom sklearn.linear_model import SGDRegressor\n \ndef iter_minibatches(chunksize):\n    # Provide chunks one by one\n    chunkstartmarker = 0\n    while chunkstartmarker &lt; numtrainingpoints:\n        chunkrows = range(chunkstartmarker,chunkstartmarker+chunksize)\n        X_chunk, y_chunk = getrows(chunkrows)\n        yield X_chunk, y_chunk\n        chunkstartmarker += chunksize\n \ndef main():\n    batcherator = iter_minibatches(chunksize=1000)\n    model = SGDRegressor()\n \n    for X_chunk, y_chunk in batcherator:\n        model.partial_fit(X_chunk, y_chunk)\n \n    y_predicted = model.predict(X_test)\n'
"import tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\nfrom tensorflow.keras import Model\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\nprint (len(x_train))\nx_train = x_train / 255.0\nx_test = x_test / 255.0\n\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)\n\nbase_model = ResNet50(weights= None, include_top=False, input_shape= (32,32,3))\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.4)(x)\npredictions = Dense(10 , activation= 'softmax')(x)\nmodel = Model(inputs = base_model.input, outputs = predictions)\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n\nhist = model.fit(x_train, y_train, )\n# the result for 2 epochs is shown below\n50000\nEpoch 1/2\n1563/1563 [==============================] - 58s 37ms/step - loss: 2.8654 - acc: 0.2537\nEpoch 2/2\n1563/1563 [==============================] - 51s 33ms/step - loss: 2.5331 - acc: 0.2748\n\nfor layer in base_model.layers:\n    layer.trainable = False\n#if you set batch_size=50, weights=&quot;imagenet&quot; with the base model frozen you get\n50000\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n94773248/94765736 [==============================] - 5s 0us/step\nEpoch 1/2\n1000/1000 [==============================] - 16s 16ms/step - loss: 2.5101 - acc: 0.1487\nEpoch 2/2\n1000/1000 [==============================] - 10s 10ms/step - loss: 2.1159 - acc: 0.2249\n"
'1/(2*length) * np.sum(np.square(predictions - y))\n'
"class_mode=None\n\nclass_mode='binary'\n"
"RegressionEvaluator(labelCol='PE')\n"
'history = model1.fit(train_images, train_labels, \n                  validation_data = (xvalid,yvalid), \n                  epochs=30, batch_size=64)\n'
"callback_es = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss')\n\nhistory = network.fit(train_img, train_label, epochs=50, batch_size=256, validation_split=0.2, callback = [callback_es])\n"
"from collections import Counter\n\nline = ''.join(df['event'].values)\n\nall_patterns = Counter()\nfor n in range(2, 6): # pattern sizes [2, 3, 4, 5]\n  all_patterns += Counter([line[i:i+n] for i in range(0, len(line)-n+1)])\n\nprint (all_patterns.most_common(10)) # Top 10 most common patterns\n# [('AB', 3), ('CA', 2), ('BD', 2), ('CAB', 2), ('ABD', 2), ('CABD', 2), ('BC', 1), ('DC', 1), ('ABC', 1), ('BCA', 1)]\n"
"return loaded_model\n\ndef loaded_model():\n   # load json and create model\n   json_file = open('model_num.json', 'r')\n   loaded_model_json = json_file.read()\n   json_file.close()\n   loaded_model = model_from_json(loaded_model_json)\n   # load weights into new model\n   loaded_model.load_weights(&quot;model_num.h5&quot;)\n   print(&quot;Loaded model from disk&quot;)\n   \n   return loaded_model\n"
'first, second = (5,6) # for example\n'
'batch_size=16\nstep_ep=data_size//batch_size\n\ndef generator():\n  while True:\n    for i in range(step_ep):\n       process your images\n       X=images\n       Y=labels\n       yield X,Y\n\nmodel.fit(generator(),epochs,steps_per_epoch=step_ep, \n          validation_data=val_generator,validation_steps=val_steps)\n'
'def split(r):\nmax_ig = 0\nmax_att = 0\nmax_att_val = 0\n\n# calculates gini for the rows provided\ncurr_gini = gini_index(r)\nno_att = len(r[0])\n\n# Goes through the different attributes\n\nfor c in range(no_att):\n\n    # Skip the label column (beer style)\n\n    if c == 3:\n        continue\n    column_vals = get_column(r, c)\n\n    i = 0\n    while i &lt; len(column_vals):\n        # value we want to check\n        att_val = r[i][c]\n\n        # Use the attribute value to fork the data to true and false streams\n        true, false = fork(r, c, att_val)\n\n        # Calculate the information gain\n        ig = gain(true, false, curr_gini)\n\n        # If this gain is the highest found then mark this as the best choice\n        if ig &gt; max_ig:\n            max_ig = ig\n            max_att = c\n            max_att_val = r[i][c]\n        i += 1\n\nreturn max_ig, max_att, max_att_val\n'
'mask = np.where(abs(arr) &lt; 0.00001, 0, 1)\npercent_zeros = np.mean(mask)\n'
'loss = tf.keras.losses.BinaryCrossentropy()(y_hat,y)\n\nloss = tf.keras.losses.binary_crossentropy(y_hat, y)\n'
'import os\nos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;-1&quot;\n'
'print ("A 12-inch pizza should cost: $%.2f" % model.predict(np.array([12]).reshape(1, -1)[0]))\n'
"conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='VALID')\n\nconv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='VALID')\n"
"from sklearn import metrics\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n\nclf = Pipeline([\n    ('transformer', OneHotEncoder()),\n    ('estimator', MultinomialNB()),\n])\n\nscores = cross_val_score(clf, dataFeatures.values, Y, cv=10, scoring='accuracy')\n"
"import pickle\nf = open('my_classifier.pickle', 'wb')\npickle.dump(classifier, f)\nf.close()\n\nimport pickle\nf = open('my_classifier.pickle', 'rb')\nclassifier = pickle.load(f)\nf.close()\n"
'import numpy as np\nfrom sklearn.model_selection import KFold\n\nX = ["a", "b", "c", "d"]\nkf = KFold(n_splits=2)\nfor train, test in kf.split(X):\n    print("%s %s" % (train, test))\n\n[2 3] [0 1] // these are indices of X\n[0 1] [2 3]\n\nfrom sklearn.model_selection import LeaveOneOut\n\nX = [1, 2, 3, 4]\nloo = LeaveOneOut()\nfor train, test in loo.split(X):\n    print("%s %s" % (train, test))\n\n[1 2 3] [0] // these are indices of X\n[0 2 3] [1]\n[0 1 3] [2]\n[0 1 2] [3]\n\nfrom sklearn.model_selection import LeavePOut\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\ny = np.array([1, 2, 3, 4])\nlpo = LeavePOut(2)\n\nfor train_index, test_index in lpo.split(X):\n    print("TRAIN:", train_index, "TEST:", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\nTRAIN: [2 3] TEST: [0 1]\nTRAIN: [1 3] TEST: [0 2]\nTRAIN: [1 2] TEST: [0 3]\nTRAIN: [0 3] TEST: [1 2]\nTRAIN: [0 2] TEST: [1 3]\nTRAIN: [0 1] TEST: [2 3]\n'
'from tensorflow.python import debug as tf_debug\nsess = tf_debug.LocalCLIDebugWrapperSession(sess)\n'
'import numpy as np\nimport pandas as pd\n\n# Origin DataFrame\ndf = pd.DataFrame(np.random.randn(6,4))\n\n# Copy data via flatten data matrix as an array\narray = df.values.flatten()\n\n# insert missing data by percent\n# Define the percent of missing data\npercent = 0.2\nsize = len(array)\n# generate a random list for indexing data which will be assigned NaN\nchosen = np.random.choice(size, int(size*percent))\narray[chosen] = np.nan\n\n# Create a new DataFrame with missing data\ndf2 = pd.DataFrame(np.reshape(array, (6,4)))\n'
'from sklearn import svm\nimport numpy as np\nclf = svm.SVC()\nnp.random.seed(seed=42)\nx=np.random.normal(loc=0.0, scale=1.0, size=[100,2])\ny=np.random.randint(2,size=100)\nclf.fit(x,y)\nprint(clf.score(x,y))\n'
'from sklearn.cross_validation import StratifiedKFold\n\ndef load_data():\n    # load your data using this function\n\ndef create model():\n    # create your model using this function\n\ndef train_and_evaluate__model(model, data[train], labels[train], data[test], labels[test)):\n    model.fit...\n    # fit and evaluate here.\n\nif __name__ == "__main__":\n    n_folds = 10\n    data, labels, header_info = load_data()\n    skf = StratifiedKFold(labels, n_folds=n_folds, shuffle=True)\n\n    for i, (train, test) in enumerate(skf):\n            print "Running Fold", i+1, "/", n_folds\n            model = None # Clearing the NN.\n            model = create_model()\n            train_and_evaluate_model(model, data[train], labels[train], data[test], labels[test))\n'
"movies.color = movies.color.map({'Color': 1, ' Black and White':0}).astype(int)\n"
'output, _ = tf.nn.dynamic_rnn(cell, X, dtype = tf.float32)\n'
"In [916]: alist=[sparse.random(1,10,.2, format='csr') for _ in range(3)]\nIn [917]: alist\nOut[917]: \n[&lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;,\n &lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;,\n &lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;]\n\nIn [918]: sparse.vstack(alist)\nOut[918]: \n&lt;3x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 6 stored elements in Compressed Sparse Row format&gt;\n\nIn [919]: np.array(alist)\nOut[919]: \narray([ &lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;,\n       &lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;,\n       &lt;1x10 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;], dtype=object)\n\nIn [920]: np.array(alist, float)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-920-52d4689fa7b3&gt; in &lt;module&gt;()\n----&gt; 1 np.array(alist, float)\n\nValueError: setting an array element with a sequence.\n"
"a = np.array([1,2,3])\nb = np.array([[4],[5],[6]])\n(a*b).shape #prints (3,3)\n\nX_1 = X[:,1].reshape((m,1))\n\n((hypothesis(X, theta) - y) * X[:, 1])\n\n# implementation of univariate linear regression\nimport numpy as np\n\n\ndef cost_function(hypothesis, y, m):\n  return (1 / (2 * m)) * ((hypothesis - y) ** 2).sum()\n\n\ndef hypothesis(X, theta):\n  return X.dot(theta)\n\n\ndef gradient_descent(X, y, theta, m, alpha):\n  X_1 = X[:,1]\n  X_1 = X_1.reshape((m,1))\n  for i in range(1500):\n    temp1 = theta[0][0] - alpha * (1 / m) * (hypothesis(X, theta) - y).sum()\n    temp2 = theta[1][0] - alpha * (1 / m) * ((hypothesis(X, theta) - y) * X_1).sum()\n    theta[0][0] = temp1\n    theta[1][0] = temp2\n\n  return theta\n\nif __name__ == '__main__':\n  data= np.random.normal(size=(100,2))\n\n  y = 30*data[:,0] + data[:, 1]\n  m = y.size\n  X = np.ones(shape=(m, 2))\n  y = y.reshape((m,1))\n  X[:, 1] = data[:, 0]\n  theta = np.zeros(shape=(2, 1))\n  alpha = 0.01\n\n  print(gradient_descent(X, y, theta, m, alpha))\n"
'log_likelihood, transition_params = \n    tf.contrib.crf.crf_log_likelihood(y_pred, y_true, sequence_lengths,  \n    transition_params=self.transition_params)\n'
'iter = 0\nfor param1 in [1,2,3,4]:\n for param2 in [2,3,4,5]:\n  plt.plot(..., label=str(param1)+"_"+str(param2)+"_"+str(iter))\n  iter += 1\nplt.legend()\n'
'def trainModel(df, weights, constantC, constantK, maxIter):\n#grab the values in a list\nx = df[\'X\'].values\ny = df[\'Y\'].values\nd = df[\'Class\'].values\nglobalErrorRate = 0\n\n#define some variables to keep track\nnumTurns = 0\n\nwhile numTurns &lt; maxIter:\n    localErrorRate = 0\n    successRate = 0\n    falsePosNeg = 0\n    truePosNeg = 0\n\n    for i in range(len(x)):\n        #calculate the discriminant\n        discriminant = weights[0] + (weights[1] * x[i]) + (weights[2] * y[i])\n\n        if(isPos(discriminant) and d[i] == 1):\n            truePosNeg += 1\n\n        elif(isPos(discriminant) == False and d[i] == -1):\n            truePosNeg += 1\n\n        else:\n            falsePosNeg += 1\n            weights = weightsUpdate(weights, constantC, constantK, d[i], x[i], y[i])\n\n    numTurns += 1 #increase number of turns by 1 iteration\n    print("Number of False Positive/Negative: " + str(falsePosNeg))\n    print("Number of True Positive/Negative: " + str(truePosNeg))\n    localErrorRate = falsePosNeg / len(x) * 100\n    successRate = truePosNeg / len(x) * 100\n    print("Error rate: " + str(localErrorRate) + "%")\n    print("Success rate: " + str(successRate) + "%")\n\n    if successRate == 100:\n        print("Trained Weight Values: " + str(weights))\n        break\n    else:\n        continue\n'
'    correctNNFL[\'NNbuckets\'] = pd.cut(correctNNFL[\'Score\'], np.linspace(0, 1, 11), labels=[\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\'])\n    correctNNFL[\'difference\'] = abs(correctNNFL[\'Score\'] - correctNNFL[\'Conf. Perc Uncreditworthy\'])\n    correctNNFL[\'diffBuckets\'] = pd.cut(correctNNFL[\'difference\'], np.linspace(0, 1, 11), labels=[\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\'])\n\n    correctNNFL.groupby([\'diffBuckets\']).count()\n    diffCoords = np.array(correctNNFL[[\'NNbuckets\', \'diffBuckets\']].groupby([\'diffBuckets\', \'NNbuckets\'])[\'diffBuckets\'].count().index.tolist())\n    diffIntersections = correctNNFL[[\'NNbuckets\', \'diffBuckets\']].groupby([\'diffBuckets\', \'NNbuckets\'])[\'diffBuckets\'].count().as_matrix()\n\n    diffGridC = np.zeros(100)\n    #diffGridC = np.arange(100)\n    diffGridC = diffGridC.reshape(10, 10)\n    diffGridC = diffGridC.astype(int)\n\n    for eachBucket in range(len(diffCoords)):\n        diffGridC[int(diffCoords[eachBucket][1])][int(diffCoords[eachBucket][0])] = int(diffIntersections[eachBucket])\n    print(diffGridC)\n    diffMapC = sns.heatmap(diffGridC, annot=True, fmt=\'d\', cmap="OrRd")\n    diffMapC.set(xlabel=\'Diff\', ylabel=\'NN\')\n    plt.show()\n'
'y_pred = clf.predict(features_test) #without labels_test!\naccuracy_score(labels_test, y_pred)\n'
'y_test_1 = dataset[:,15:16]\n\ny_test_1 = test_dataset_1[:,15:16]\n'
'importances = regr.feature_importances_\n'
"import pandas as pd\nfrom collections import Counter\n\ndf = &lt;your dataframe&gt;\n\ncount = Counter()\nfor row in df['traverse']:\n    nums = list(map(int, row.split('-&gt;')))\n    for index, value in enumerate(nums[:-1]):\n        count[value, nums[index + 1]] += 1\n\nfor key, value in sorted(count.items()):\n    print('{k[0]} to {k[1]} : {v}'.format(k=key, v=value))\n\n1 to 1 : 4\n1 to 2 : 2\n2 to 3 : 2\n3 to 1 : 1\n3 to 5 : 1\n3 to 13 : 1\n4 to 3 : 1\n4 to 4 : 1\n5 to 1 : 1\n5 to 5 : 1\n5 to 6 : 1\n6 to 3 : 1\n7 to 7 : 3\n7 to 8 : 1\n13 to 13 : 1\n13 to 15 : 1\n16 to 7 : 1\n"
'def create_tfrecord_for_prediction(batch_size, stoke_data, tfrecord_file):\n    def parse_line(stoke_data):\n        """Parse provided stroke data and ink (as np array) and classname."""\n        inkarray = json.loads(stoke_data)\n        stroke_lengths = [len(stroke[0]) for stroke in inkarray]\n        total_points = sum(stroke_lengths)\n        np_ink = np.zeros((total_points, 3), dtype=np.float32)\n        current_t = 0\n        for stroke in inkarray:\n            if len(stroke[0]) != len(stroke[1]):\n                print("Inconsistent number of x and y coordinates.")\n                return None\n            for i in [0, 1]:\n                np_ink[current_t:(current_t + len(stroke[0])), i] = stroke[i]\n            current_t += len(stroke[0])\n            np_ink[current_t - 1, 2] = 1  # stroke_end\n        # Preprocessing.\n        # 1. Size normalization.\n        lower = np.min(np_ink[:, 0:2], axis=0)\n        upper = np.max(np_ink[:, 0:2], axis=0)\n        scale = upper - lower\n        scale[scale == 0] = 1\n        np_ink[:, 0:2] = (np_ink[:, 0:2] - lower) / scale\n        # 2. Compute deltas.\n        #np_ink = np_ink[1:, 0:2] - np_ink[0:-1, 0:2]\n        #np_ink = np_ink[1:, :]\n        np_ink[1:, 0:2] -= np_ink[0:-1, 0:2]\n        np_ink = np_ink[1:, :]\n\n        features = {}\n        features["ink"] = tf.train.Feature(float_list=tf.train.FloatList(value=np_ink.flatten()))\n        features["shape"] = tf.train.Feature(int64_list=tf.train.Int64List(value=np_ink.shape))\n        f = tf.train.Features(feature=features)\n        ex = tf.train.Example(features=f)\n        return ex\n\n    if stoke_data is None:\n        print("Error: Stroke data cannot be none")\n        return\n\n    example = parse_line(stoke_data)\n\n    #Remove the file if it already exists\n    if tf.gfile.Exists(tfrecord_file):\n        tf.gfile.Remove(tfrecord_file)\n\n    writer = tf.python_io.TFRecordWriter(tfrecord_file)\n    for i in range(batch_size):\n        writer.write(example.SerializeToString())\n    writer.flush()\n    writer.close()\n\ndef get_classes():\n  classes = []\n  with tf.gfile.GFile(FLAGS.classes_file, "r") as f:\n    classes = [x.rstrip() for x in f]\n  return classes\n\ndef main(unused_args):\n  print("%s: I Starting application" % (datetime.now()))\n\n  estimator, train_spec, eval_spec = create_estimator_and_specs(\n      run_config=tf.estimator.RunConfig(\n          model_dir=FLAGS.model_dir,\n          save_checkpoints_secs=300,\n          save_summary_steps=100))\n  tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\n  if FLAGS.predict_for_data != None:\n      print("%s: I Starting prediction" % (datetime.now()))\n      class_names = get_classes()\n      create_tfrecord_for_prediction(FLAGS.batch_size, FLAGS.predict_for_data, FLAGS.predict_temp_file)\n      predict_results = estimator.predict(input_fn=get_input_fn(\n          mode=tf.estimator.ModeKeys.PREDICT,\n          tfrecord_pattern=FLAGS.predict_temp_file,\n          batch_size=FLAGS.batch_size))\n\n      #predict_results = estimator.predict(input_fn=predict_input_fn)\n      for idx, prediction in enumerate(predict_results):\n          index = prediction["class_index"]  # Get the predicted class (index)\n          probability = prediction["probabilities"][index]\n          class_name = class_names[index]\n          print("%s: Predicted Class is: %s with a probability of %f" % (datetime.now(), class_name, probability))\n          break #We care for only the first prediction, rest are all duplicates just to meet the batch size\n\n  # Build the model.\n  inks, lengths, labels = _get_input_tensors(features, labels)\n  convolved, lengths = _add_conv_layers(inks, lengths)\n  final_state = _add_rnn_layers(convolved, lengths)\n  logits = _add_fc_layers(final_state)\n\n  # Compute current predictions.\n  predictions = tf.argmax(logits, axis=1)\n\n  if mode == tf.estimator.ModeKeys.PREDICT:\n      preds = {\n          "class_index": predictions,\n          #"class_index": predictions[:, tf.newaxis],\n          "probabilities": tf.nn.softmax(logits),\n          "logits": logits\n      }\n      #preds = {"logits": logits, "predictions": predictions}\n\n      return tf.estimator.EstimatorSpec(mode, predictions=preds)\n      # Add the loss.\n  cross_entropy = tf.reduce_mean(\n      tf.nn.sparse_softmax_cross_entropy_with_logits(\n          labels=labels, logits=logits))\n\n  # Add the optimizer.\n  train_op = tf.contrib.layers.optimize_loss(\n      loss=cross_entropy,\n      global_step=tf.train.get_global_step(),\n      learning_rate=params.learning_rate,\n      optimizer="Adam",\n      # some gradient clipping stabilizes training in the beginning.\n      clip_gradients=params.gradient_clipping_norm,\n      summaries=["learning_rate", "loss", "gradients", "gradient_norm"])\n\n  return tf.estimator.EstimatorSpec(\n      mode=mode,\n      predictions={"logits": logits, "predictions": predictions},\n      loss=cross_entropy,\n      train_op=train_op,\n      eval_metric_ops={"accuracy": tf.metrics.accuracy(labels, predictions)})\n'
"git clone https://github.com/facebookresearch/fastText.git\ncd fastText\npip install .\n\nfastText.train_supervised(input, lr=0.1, dim=100, ws=5, epoch=5, minCount=1, minCountLabel=0, minn=0, maxn=0, neg=5, wordNgrams=1, loss='softmax', bucket=2000000, thread=12, lrUpdateRate=100, t=0.0001, label='__label__', verbose=2, pretrainedVectors='')\n"
'Dictionary with parameters names (string) as keys and distributions or \nlists of parameters to try. Distributions must\nprovide a rvs method for sampling (such as those from\nscipy.stats.distributions). If a list is given, it is sampled\nuniformly.\n\nparams_randomSearch = {\n      "learning_rate" : Real(10**-5, 10**0, "log-uniform", name=\'learning_rate\'),\n      "min_samples_leaf": np.arange(1,30,1),\n              ..\n              }\n'
'("root How do I delete my account?", {\n    \'heads\': [0, 4, 4, 4, 0, 6, 4, 4],  # index of token head\n    \'deps\': [\'ROOT\', \'-\', \'-\', \'-\', \'INTENT\', \'-\', \'OBJECT\', \'-\']\n})\n\nroot How do I delete my account?\n[(\'root\', \'ROOT\', \'root\'), (\'delete\', \'INTENT\', \'root\'), (\'account\', \'OBJECT\', \'delete\')]\n'
'model = Sequential()\nmodel.add(Dense(n, input_dim=3))\nmodel.add(Dense(m))\nmodel.add(Dense(1))\n'
"df.loc[df['xg_damage_grade'] == df['lr_damage_grade'], 'damage_grade'] = df['xg_damage_grade']\ndf.loc[df['xg_damage_grade'] != df['lr_damage_grade'], 'damage_grade'] = df['rf_damage_grade']\n\ndf['damage_grade'] = np.where(df['xg_damage_grade'] == df['lr_damage_grade'],\n                              df['xg_damage_grade']\n                              df['rf_damage_grade'])\n"
"df1['gender'] = df1['first_name'].map(lambda x: d.get_gender(x))\n"
'def linear(x):\n    """Linear (i.e. identity) activation function.\n    """\n    return x\n\ndf["linear"] = activations.linear(df["activation"])\n\ndf["selu"] = K.eval(activations.selu(df["activation"].values.reshape(-1,1)))\n\ndf["hard_sigmoid"] = K.eval(activations.hard_sigmoid(K.variable(df["activation"].values)))\n'
'from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \ntest_size=0.2)\n'
"row = data[i].split(',')\n"
"  for f in unf : \n    for f2 in f : \n      all_words.append(f2)\n\ndef get_all_words(sentences) :\n  unf = [s.split(' ') for s in sentences]\n  return list(set([word for sen in unf for word in sen]))\n\n  for a in [np.array(one_hot_encoded_df[s]) for s in s1.split(' ')] : \n    for aa in a : \n      flattened.append(aa)\n\n   1  2  is  sentence  this\n0  0  1   0         0     0\n1  0  0   0         0     1\n2  1  0   0         0     0\n3  0  0   1         0     0\n4  0  0   0         1     0\n\ndef get_one_hot(s , s1 , all_words) :\n  flattened = []\n  one_hot_encoded_df = pd.get_dummies(list(set(all_words)))\n  return one_hot_encoded_df[s1.split(' ')].T.sum().clip(0,1).values\n\n[0 1 1 1 1]\n[1 1 0 1 1]\n"
'import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\n\n# dummy data\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - 5*rng.rand(16)) # modify here - 5*\n\nestimator_1 = DecisionTreeRegressor(max_depth=2)\nestimator_1.fit(X, y)\n\nestimator_2 = DecisionTreeRegressor(max_depth=5)\nestimator_2.fit(X, y)\n\ny_pred_1 = estimator_1.predict(X)\ny_pred_2 = estimator_2.predict(X)\n\nnp.var(y) # true data\n# 11.238416688700267\n\nnp.var(y_pred_1) # max_depth=2\n# 1.7423865989859313\n\nnp.var(y_pred_2) # max_depth=5\n# 6.1398871265574595\n\nnp.mean(y)\n# -1.2561013675900665\n\nnp.mean(y_pred_1)\n# -1.2561013675900665\n\nnp.mean(y_pred_2)\n# -1.2561013675900665\n\nnp.unique(y_pred_1)\n# array([-11.74901949,  -1.9966201 ,  -0.71895532])\n'
'[20,4,5] , [200,1,5]\n\n[1,0.20,0.25] and [1,0.005,0.025]\n'
"inputs = Input(shape=(1,))\nembedding = Embedding(5, 3, input_length=1, name='cat_col')(inputs)\nmodel = Model(inputs, embedding)\n\nx = np.array([0,1,2,3,4]).reshape(5,1)\nlabels = np.zeros((5,1,3))\n\nprint (model.predict(x))\nprint (model.get_layer('cat_col').get_weights()[0])\n\nassert np.array_equal(model.predict(x).reshape(-1), model.get_layer('cat_col').get_weights()[0].reshape(-1))\n\n[[[-0.01862894,  0.0021644 ,  0.04706952]],\n [[-0.03891206,  0.01743075, -0.03666048]],\n [[-0.01799501,  0.01427511, -0.00056203]],\n [[ 0.03703432, -0.01952349,  0.04562894]],\n [[-0.02806044, -0.04623617, -0.01702447]]]\n\n[[-0.01862894,  0.0021644 ,  0.04706952],\n [-0.03891206,  0.01743075, -0.03666048],\n [-0.01799501,  0.01427511, -0.00056203],\n [ 0.03703432, -0.01952349,  0.04562894],\n [-0.02806044, -0.04623617, -0.01702447]]\n"
'X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n\npipe = Pipeline([\n    (\'oversample\', SMOTE(random_state=12)),\n\n    # Check out custom scikit-learn transformers\n    # You need to impletent your reshape logic in "transform()" method\n    (\'reshaper\', CustomReshaper(),\n    (\'clf\', clf)\n])\n\n# Remove the following two lines, so the data is 2-D while going to "RandomizedSearchCV". \n\n#    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n#    X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n\n\nfrom keras.layers import Reshape\n\ndef create_model(activation_1=\'relu\', activation_2=\'relu\',\n                 neurons_input = 1, neurons_hidden_1=1,\n                 optimizer=\'Adam\' ,):\n\n  model = Sequential()\n\n  # Add this before LSTM. The tuple denotes the last two dimensions of input\n  model.add(Reshape((1, X_train.shape[1])))\n  model.add(LSTM(neurons_input, \n                 activation=activation_1, \n\n                 # Since the data is 2-D, the following needs to be changed from "X_train.shape[1], X_train.shape[2]"\n                 input_shape=(1, X_train.shape[1]),\n                 kernel_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=42), \n                 bias_initializer=RandomNormal(mean=0.0, stddev=0.05, seed=42)))\n\n  model.add(Dense(2, activation=\'sigmoid\'))\n\n  model.compile (loss = \'sparse_categorical_crossentropy\', optimizer=optimizer)\n  return model\n'
'y = [1,2,3,4,6]\nidx = np.array([0,1])\nprint (y[idx])   # This will throw an error as list cannot be index this way\nprint (np.array(y)[idx]) # This is fine because it is a numpy array now \n\ny = np.array([i[1] for i in dataset])  #label for the data\n\ny = np.array([np.array(i[1]) for i in dataset])  #label for the data\n'
'probs=sigmoid(np.dot(X_bias,w))\n\nJ=loss(y,probs)\ndJ=gradient(X_bias,y,w)\n\nw=w-learning_rate*dJ\n\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nX, y = datasets.make_classification(\nn_samples = 200, n_features = 2, random_state = 333,\nn_informative =2, n_redundant = 0  , n_clusters_per_class= 1)\n\ndef sigmoid(s):\n    return 1 / (1 + np.exp(-s))\n\ndef loss(y, h):\n    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n\ndef gradient(X, y, w):\n    return -(y * X) / (1 + np.exp(-y * np.dot(X, w)))\n\n\nX_bias = np.append(np.ones((X.shape[0], 1)), X, axis=1)\ny = np.array([[1] if label == 0 else [0] for label in y])\nw = np.array([[np.random.uniform(-1, 1)] for _ in range(X.shape[1]+1)])\nmax_iter = 100\nlearning_rate = 0.1\nthreshold = 0.5\nfor _ in range(max_iter):\n    probs=sigmoid(np.dot(X_bias,w))\n    J=loss(y,probs)\n    dJ=gradient(X_bias,y,w)\n    w=w-learning_rate*dJ\nprobabilities = sigmoid(np.dot(X_bias, w))\npredictions = [[1] if p &gt; threshold else [0] for p in probabilities]\nprint("loss: %.2f, accuracy: %.2f" %\n(loss(y, probabilities), accuracy_score(y, predictions)))\n'
"clf_tree = DecisionTreeClassifier()\nclf_tree.fit(X_train, y_train) \nprint('Accuracy: %f' % clf_tree.score(X_test, y_test))\n\nprint((cross_val_score(clf_tree, X_test, y_test, cv=10, scoring='accuracy')))\n\nprint((cross_val_score(clf_tree, X_test, y_test, cv=10, scoring='accuracy')))\n\nprint((cross_val_score(DecisionTreeClassifier(), X_test, y_test, cv=10, scoring='accuracy')))\n"
"clf = GridSearchCV(XGBRegressor(), param_grid=tuned_parameters, cv=5,scoring='%s_macro' % score)\n"
'theta=theta-(alpha/m)*X.T.dot(X.dot(theta)-y)\n\nnp.c_[np.ones((m,1)),data]\n'
"import numpy as np\ntable = pd.pivot_table(data_frame, values=['conducted'], index=['name'], columns=['date'], aggfunc=np.sum).fillna(0.0)\n\ntable = pd.pivot_table(data_frame, values=['present'], index=['name'], columns=['date'], aggfunc='count').fillna(0.0)\n"
"df.groupby(['cusid', 'product']).mean().reset_index().groupby('product')['count'].mean()\n\nproduct\n1      1\n21     1\n30     2\n36     1\n78     1\n79     4\n99     1\n112    2\n228    1\n"
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# toy data\ny_true = [0, 1, 0, 1, 0, 0, 0, 1]\ny_pred =  [1, 1, 1, 0, 1, 1, 0, 1]\nclass_names=[0,1]\n\n# plot_confusion_matrix function\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    """\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    """\n    if not title:\n        if normalize:\n            title = \'Normalized confusion matrix\'\n        else:\n            title = \'Confusion matrix, without normalization\'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n\n    if normalize:\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\n        print("Normalized confusion matrix")\n    else:\n        print(\'Confusion matrix, without normalization\')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation=\'nearest\', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel=\'True label\',\n           xlabel=\'Predicted label\')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",\n             rotation_mode="anchor")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = \'.2f\' if normalize else \'d\'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha="center", va="center",\n                    color="white" if cm[i, j] &gt; thresh else "black")\n    fig.tight_layout()\n    return ax\n\nplot_confusion_matrix(y_true, y_pred, classes=class_names, normalize=True)\n# result:\nNormalized confusion matrix\n[[ 0.2         0.8       ]\n [ 0.33333333  0.66666667]]\n\n(0.67 + 0.2)/2\n# 0.435\n\nplot_confusion_matrix(y_true, y_pred, classes=class_names) # normalize=False by default\n# result\nConfusion matrix, without normalization\n[[1 4]\n [1 2]]\n\n(1+2)/(1+2+4+1)\n# 0.375\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)\n# 0.375\n'
"import numpy as np\nfrom sklearn.linear_model import Ridge\n\nalphas = np.logspace(-10, 10, 1000)\nsolution_norm = []\nresidual_norm = []\n\nfor alpha in alphas: \n    lm = Ridge(alpha=alpha)\n    lm.fit(X, y)\n    solution_norm += [(lm.coef_**2).sum()]\n    residual_norm += [((lm.predict(X) - y)**2).sum()]\n\nplt.loglog(residual_norm, solution_norm, 'k-')\nplt.show()\n"
"FeaturesDict({\n    'coarse_label': ClassLabel(shape=(), dtype=tf.int64, num_classes=20),\n    'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=100),\n})\n"
"import pandas as pd\nprediction = pd.DataFrame(to_save.T)\nprediction.index += 1\nprediction.to_csv('prediction.csv')\n"
"class PrinterCallback(tf.keras.callbacks.Callback):\n\n    # def on_train_batch_begin(self, batch, logs=None):\n    #     # Do something on begin of training batch\n\n    def on_epoch_end(self, epoch, logs=None):\n        print('EPOCH: {}, Train Loss: {}, Val Loss: {}'.format(epoch,\n                                                               logs['loss'],\n                                                               logs['val_loss']))\n\n    def on_epoch_begin(self, epoch, logs=None):\n        print('-'*50)\n        print('STARTING EPOCH: {}'.format(epoch))\n\n    # def on_train_batch_end(self, batch, logs=None):\n    #     # Do something on end of training batch\n    #\n"
"from sklearn.cluster import KMeans\n#here you select your columns\nX = df[['col1', 'col2', 'col3']]\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n#this will give you the groups back\nkmeans.predict(X)\n"
'import keras \n\nInput = keras.layers.Input(shape=(784,))\n\n#modelA\nHidden_A_1 = keras.layers.Dense(units=20)(Input)\nHidden_A_2 = keras.layers.Dense(units=20)(Hidden_A_1)\nOutput_A = keras.layers.Dense(units=1, activation=\'sigmoid\')(Hidden_A_2)\noptimizer_A = keras.optimizers.SGD(lr=0.00001, momentum=0.09, nesterov=True)\nmodel_A = keras.Model(inputs=Input, outputs=Output_A)\nmodel_A.compile(loss="binary_crossentropy",\n                   optimizer=optimizer_A,\n                   metrics=[\'accuracy\'])\n\n#modelB\nHidden1_B = keras.layers.Dense(units=10, activation=\'relu\')(Input)\nOutput_B = keras.layers.Dense(units=1, activation=\'sigmoid\')(Hidden1_B)\nmodel_B = keras.Model(inputs=Input, outputs=Output_B)\noptimizer_B = keras.optimizers.Adagrad()\nmodel_B.compile(loss="binary_crossentropy",\n                   optimizer=optimizer_B,\n                   metrics=[\'accuracy\'])\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nx_train = x_train.reshape(60000, 784)\nx_test = x_test.reshape(10000, 784)\nx_train = x_train.astype(\'float32\')\nx_test = x_test.astype(\'float32\')\n\nmodel_A.fit(x_train,y_train)\nmodel_B.fit(x_train,y_train)\n\nw = 0.8\noutput_modelC = model_A.predict(x_test) * w + model_B.predict(x_test) * (1 - w)\n\narray([[0.98165023],\n       [0.9918817 ],\n       [0.93426293],\n       ...,\n       [0.99940777],\n       [0.9960805 ],\n       [0.9992139 ]], dtype=float32)\n'
'#create our feature ad result sets\ny = data["quality"]\nX = data.drop(["quality"], axis=1)\n'
"label_encoders = {}\ncategorical_columns = [0, 1, 2, 3]  # I would recommend using columns names here if you're using pandas. If you're using numpy then stick with range(n) instead\n\nfor column in categorical_columns:\n    label_encoders[column] = LabelEncoder()\n    X[column] = label_encoders[column].fit_transform(X[column])  # if numpy instead of pandas use X[:, column] instead\n"
'def forward(self, x, depth, alpha):\n    """\n    forward pass of the Generator\n    :param x: input noise\n    :param depth: current depth from where output is required\n    :param alpha: value of alpha for fade-in effect\n    :return: y =&gt; output\n    """\n\n    # THOSE TWO LINES WERE ADDED\n    # We will pas tensors but unpack them here to `int` and `float`\n    depth = depth.item()\n    alpha = alpha.item()\n    # THOSE TWO LINES WERE ADDED\n    assert depth &lt; self.depth, "Requested output depth cannot be produced"\n\n    y = self.initial_block(x)\n\n    if depth &gt; 0:\n        for block in self.layers[: depth - 1]:\n            y = block(y)\n\n        residual = self.rgb_converters[depth - 1](self.temporaryUpsampler(y))\n        straight = self.rgb_converters[depth](self.layers[depth - 1](y))\n\n        out = (alpha * straight) + ((1 - alpha) * residual)\n\n    else:\n        out = self.rgb_converters[0](y)\n\n    return out\n\nimport torch\n\nfrom pro_gan_pytorch import PRO_GAN as pg\n\ngen = torch.nn.DataParallel(pg.Generator(depth=9))\ngen.load_state_dict(torch.load("GAN_GEN_SHADOW_8.pth"))\n\nmodule = gen.module.to("cpu")\n\n# Arguments like depth and alpha may need to be changed\ndummy_inputs = (torch.randn(1, 512), torch.tensor([5]), torch.tensor([0.1]))\ntorch.onnx.export(module, dummy_inputs, "GAN_GEN8.onnx", verbose=True)\n'
'outputs = net(inputs) \nprobabilities = torch.nn.functional.softmax(outputs, 1)\n\npredictions = probabilities &gt; 0.8\n\nthresholds = torch.tensor([0.1, 0.1, 0.8]).unsqueeze(0)\npredictions = probabilities &gt; thresholds\n'
'True positive (TP1)  = 12\nFalse positive (FP1) = 5\nFalse negative (FN1) = 10\n\nTrue positive (TP2)  = 50\nFalse positive (FP2) = 7\nFalse negative (FN2) = 7\n'
'np.subtract(y,prediction_function.reshape(y.shape))\n\nA = np.arange(5) #shape (5,)\nB = np.arange(5).reshape(5,1)  #shape (5,1)\nnp.subtract(A, B)\n\n[[ 0  1  2  3  4]\n [-1  0  1  2  3]\n [-2 -1  0  1  2]\n [-3 -2 -1  0  1]\n [-4 -3 -2 -1  0]]   \n\nnp.subtract(A, B.reshape(A.shape))\n\n[[0 0 0 0 0]]\n'
'sc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train.reshape(-1, 1))\ny_test = sc_y.transform(y_test.reshape(-1, 1))\n\n# model definition and fitting...\n\ny_pred_scaled = clf.predict(X_test) # get scaled predictions\ny_pred = sc_y.inverse_transform(y_pred_scaled) # transform back to original scale\n\nsc_x = StandardScaler()\nX_train = sc_x.fit_transform(X_train)\nX_test = sc_x.transform(X_test) # transform here\n'
'def generate_sample():\n    x = list(&quot;123456789&quot;)\n    y = list(&quot;2345&quot;)\n    while 1:\n        yield np.array(x).astype(np.float32),(np.array(y).astype(np.float32),np.array(y).astype(np.float32))\n\ndataset = tf.data.Dataset.from_generator(generate_sample,\n            output_signature=(\n                 tf.TensorSpec(shape=(9,), dtype=tf.float32),\n                 (tf.TensorSpec(shape=(4,), dtype=tf.float32),\n                 tf.TensorSpec(shape=(4,), dtype=tf.float32)),\n            ))\n'
"import numpy\nmyDtype = numpy.dtype([('name', numpy.str_), ('age', numpy.int32), ('score', numpy.float64)])\nmyData = numpy.empty(10, dtype=myDtype) # Create empty data sets\nprint myData['age'] # prints all ages\n\nwith open('myfile.txt', 'wb') as f:\n    numpy.ndarray.tofile(myData, f)\n\nwith open('myfile.txt', 'rb') as f:\n    loadedData = numpy.fromfile(f, dtype=myDtype)\n    print loadedData['age']\n"
'import numpy as np\nDictVectorizer(dtype=np.float32, sparse=False)\n'
'class Program():\n\n    def run(self):\n        while 1:\n            try:\n                self.do_something()\n            except KeyboardInterrupt:\n                break\n\n    def do_something(self):\n        print("Doing something")\n\n\n# usage:\n\na = Program()\na.run()\n\n# will print a lot of statements\n\n#\xa0if you hit CTRL+C it will stop\n# then you can run it again with a.run()\n'
'        parameters, a dict of parameter settings\n        mean_validation_score, the mean score over the cross-validation folds\n        cv_validation_scores, the list of scores for each fold\n'
'class PredictionTransformer(sklearn.base.BaseEstimator, sklearn.base.TransformerMixin):\n    def __init__(self, estimator):\n        self.estimator = estimator\n\n    def fit(self, X, y):\n        self.estimator.fit(X, y)\n        return self\n\n    def transform(self, X): \n        return self.estimator.predict_proba(X)\n'
"df.label.unique()\nOut[50]: array([  5.,   4.,   3.,   1.,   2.,  nan])\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\n\n# replace your own data file_path\ndf = pd.read_csv('data1.csv', header=0)\ndf[df.label.isnull()]\n\nOut[52]: \n                               id content  label\n900   Daewoo_DWD_M1051__Opinio...       5    NaN\n1463  Indesit_IWC_5105_B_it__O...       1    NaN\n\n\n# drop those two \ndf = df[df.label.notnull()]\n\nX = df.content.values\ny = df.label.values\n\ntransformer = TfidfVectorizer()\nX = transformer.fit_transform(X)\n\nestimator = SVC(kernel='linear', class_weight='auto', probability=True)\nestimator.fit(X, y)\n\nestimator.predict(X)\n\nOut[54]: array([ 4.,  4.,  4., ...,  2.,  2.,  3.])\n\nestimator.predict_proba(X)\n\nOut[55]: \narray([[ 0.0252,  0.0228,  0.0744,  0.3427,  0.535 ],\n       [ 0.002 ,  0.0122,  0.0604,  0.4961,  0.4292],\n       [ 0.0036,  0.0204,  0.1238,  0.5681,  0.2841],\n       ..., \n       [ 0.1494,  0.3341,  0.1586,  0.1316,  0.2263],\n       [ 0.0175,  0.1984,  0.0915,  0.3406,  0.3519],\n       [ 0.049 ,  0.0264,  0.2087,  0.3267,  0.3891]])\n"
'from skimage.util.shape import view_as_windows\nwindow_shape = (patch_size, patch_size, patch_size)\npatch_array = view_as_windows(A, window_shape)\n'
'def predict(x_test, y_test, ss):\n    prediction = nonlinear(np.dot(x_test,ss))\n    error = np.mean(np.abs(y_test - prediction))\n    print("P:",prediction,"\\nE:",error)\n'
"X = dataset['dataset_name'][:] #input\n\nfor key in dataset.keys():\n    print key\n"
'np.asarray(trainDataFeatures)\nnp.asarray(testDataFeatures)\n# replace the above two lines with these\ntrainDataFeatures = trainDataFeatures.toarray()\ntestDataFeatures = testDataFeatures.toarray()\n'
'arr = np.array([1, 2, 3])\narr.shape  # (3,)\narr = arr.reshape(arr.shape[0], 1, 1)\narr.shape  # (3, 1, 1)\n\nmodel.add(LSTM(128, input_shape=(arr.shape[1], arr.shape[2])))\n'
'km1 = KMeans(n_clusters=6, n_init=25, max_iter = 600, random_state=MYSEED)\n\nkm1 = KMeans(n_clusters=6, n_init=25, max_iter = 600, random_state=0)\n'
"import numpy as np\nimport cv2\ndef batch_generator(ids):\n    while True:\n        for start in range(0, len(ids), batch_size):\n            x_batch = []\n            y_batch = []\n            end = min(start + batch_size, len(ids))\n            ids_batch = ids[start:end]\n            for id in ids_batch:\n                img = cv2.imread(dpath+'train/{}.jpg'.format(id))\n                #img = cv2.resize(img, (224, 224), interpolation = cv2.INTER_AREA)\n                labelname=df_train.loc[df_train.id==id,'column_name'].values\n                labelnum=classes.index(labelname)\n                x_batch.append(img)\n                y_batch.append(labelnum)\n            x_batch = np.array(x_batch, np.float32) \n            y_batch = to_categorical(y_batch,120) \n            yield x_batch, y_batch\n\nmodel.fit_generator(generator=batch_generator(ids_train_split), \\\n               steps_per_epoch= \\ \n               np.ceil(float(len(ids_train_split)) / float(batch_size)),\\\n                epochs=epochs, verbose=1, callbacks=callbacks, \\\n                validation_data=batch_generator(ids_valid_split), \\\n                validation_steps=np.ceil(float(len(ids_valid_split)) / float(batch_size)))\n"
'print len(dset)\n'
'y_pred = grd.fit(X_train, y_train).predict_proba(X_test)[:,1] \n'
"df['Sex'] = df['Sex'].replace(['male', 'female'], [1,0])\n\ndf['Sex'].replace(['male', 'female'], [1,0], inplace=True)\n"
'def act(self, state, sess, episode):\n    if random.random() &lt; math.pow(2, -episode / 30):\n        return env.action_space.sample()\n\n    act_values = sess.run(self.model[3], feed_dict = { self.model[1]: state})\n    return np.argmax(act_values[0])\n'
"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nimport numpy as np\n\n#assuming all 4 columns correspond to 1 song\ndata_dim = 4\n#so one song would be 10x4 2D array \nnumber_of_notes_per_song = 10\nnsongs_train = 100\n#tunable parameter\nbatch_size = 32\nepochs = 5\n\n# I generated dummy data, but you have your own...\nx_train = np.random.random((nsongs_train, number_of_notes_per_song, data_dim)).reshape(nsongs_train*number_of_notes_per_song,data_dim)\n\n#this is a supervised learning problem, but your dataset has no labels..\n#we can use last note in each song as a label when training LSTM \nX = x_train[np.mod(np.arange(x_train.shape[0]),number_of_notes_per_song)!=0].reshape(nsongs_train,number_of_notes_per_song-1,data_dim)\ny = x_train[::number_of_notes_per_song].reshape(nsongs_train,data_dim) \n\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(number_of_notes_per_song-1, data_dim),return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(data_dim, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\nmodel.fit(X,y,batch_size=batch_size, epochs=epochs)\n\n#predict on unseen data, expects tensors of shape (None, number_of_notes_per_song-1, data_dim)\nmodel.predict(...)\n"
"dict_map = {'CERTIFIED':1 , 'CERTIFIED-WITHDRAWN': 1, 'DENIED': 0}\ndf['CASE_STATUS'] = df['CASE_STATUS'].map(dict_map)\ndf['CASE_STATUS'] = df['CASE_STATUS'].astype(int)\n"
'import spacy\nimport operator\nnlp = spacy.load("en")\n\ndoc = nlp("The cat is better than a dog and a wolf.")\nnns = [i.text for i in doc if i.tag_ == "NN"]\nfirst, third = list(operator.itemgetter(0, 2)(nns))\n\nlen(nns)\n'
'from keras.callbacks import Callback\nfrom keras import backed as K\n\nclass LRSchedule(Callback):\n    def __init__(self, schedule):\n        super(LRSchedule, self).__init__()\n        self.schedule = schedule\n\n    def on_train_begin(self, logs = {}):\n        self.epoch_counter = 0\n        self.schedule_index = 0\n\n    def on_epoch_end(self, epoch, logs = {}):\n        self.epoch_counter += 1\n\n        if len(self.schedule)  &gt; self.schedule_index + 1:\n            next_epoch = self.schedule[self.schedule_index + 1]\n            if self.epoch_counter == next_epoch:\n                K.set_value(self.model.optimizer.lr, self.model.optimizer.lr / 2.0)\n                self.schedule_index += 1\n\nlr_scheduler = LRSchedule([25, 35, 45])\nmodel.fit_generator(..., callbacks = [lr_scheduler])\n'
'import numpy as np\nidx = np.linalg.norm(\n    (np.array([[0, 1]]) -np.stack([fpr, tpr], axis=1)), \n    axis=1).argmax()\nmax_thresh = thresholds[idx]\n'
'import cv2\nimport numpy as np\n\nsrc = cv2.imread("/path.jpg")\ntarget_size = (64,64)\n\ndst = cv2.resize(src, target_size)\n\ndst = dst.reshape(target_size.shape[0] * target_size.shape[1])\n'
'next_price = model.predict(self.current_price)\n'
'X = np.arange(10)\ny = np.concatenate((np.ones(5), np.zeros(5)))\nX\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\ny\narray([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\nX_train\narray([5, 0, 7, 2, 9, 4, 3, 6])\nX_test\narray([8, 1])\nkf = Kfold(n_splits=5)\nfor train, test in kf.split(X_train):\n    print(train, test)\n[2 3 4 5 6 7] [0 1]\n[0 1 4 5 6 7] [2 3]\n[0 1 2 3 6 7] [4 5]\n[0 1 2 3 4 5 7] [6]\n[0 1 2 3 4 5 6] [7]\n'
'SAMPLE_SHAPE = (197,197,3)\n\nSAMPLE_SHAPE = (224,224,3)\n'
"model.compile(optimizer=Optimizer,\n              loss=['mse'],\n              metrics=['mse']\n              )\n\nmodel.compile(optimizer=Optimizer,\n              loss=['categorical_crossentropy'],\n              metrics=['acc']\n              )\n"
'from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33) // test set size is 0.33 \nclf = MLPClassifier()\nclf.fit(X_train, y_train) \nclf.predict(X_test, y_test) // predict on test set \n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=2)\nkf.get_n_splits(X)\nclf = MLPClassifier()\nfor train_index, test_index in kf.split(X):\n   X_train, X_test = X[train_index], X[test_index]\n   y_train, y_test = y[train_index], y[test_index]\n\n   clf.fit(X_train, y_train) \n   clf.predict(X_test, y_test) // predict on test set \n'
"# cnn_model function the same way as you defined it ...\n\nx = TimeDistributed(Lambda(cnn_model))(inputs)\n\ndef cnn_model():\n    input_frame = Input(shape=(config.IMAGE_H, config.IMAGE_W, config.N_CHANNELS))\n\n    x = Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu')(input_frame)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n\n    model = Model(input_frame, x)\n    return model\n\ninputs = Input(shape=(config.N_FRAMES_IN_SEQUENCE, config.IMAGE_H, config.IMAGE_W, config.N_CHANNELS))\n\nx = TimeDistributed(cnn_model())(inputs)\n"
'rf = RandomForestClassifier(max_features=.1)\n\nbase = DecisionTreeClassifier(max_features=.1)    \nrf = BaggingClassifier(base_estimator=base, max_samples=.25)\n'
"model = Sequential([\nConv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'),\nConv2D(64, (3, 3), activation='relu', padding='same'),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(128, (3, 3), activation='relu', padding='same'),\nConv2D(128, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(256, (3, 3), activation='relu', padding='same',),\nConv2D(256, (3, 3), activation='relu', padding='same',),\nConv2D(256, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nConv2D(512, (3, 3), activation='relu', padding='same',),\nMaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\nFlatten(),\nDense(4096, activation='relu'),\nDense(4096, activation='relu'),\nDense(1, activation='sigmoid')\n])\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\nfrom keras.layers import Dense\n\nvgg = VGG16(include_top=False, weights='imagenet', input_shape=(), pooling='avg')\nx = vgg.output\nx = Dense(1, activation='sigmoid')(x)\nmodel = Model(vgg.input, x)\nmodel.summary()\n"
'prediction = mymodel.predict(x)\n#return position of max\nMaxPosition=np.argmax(prediction)  \nprediction_label=classes[MaxPosition]\nprint(prediction_label) \n'
'import seaborn as sns\nimport pandas as pd\ntitanic = sns.load_dataset(\'titanic\')\ntitanic = titanic.copy()\ntitanic = titanic.dropna()\ntitanic[\'age\'].plot.hist(\n  bins = 50,\n  title = "Histogram of the age variable"\n)\n\n\nfrom scipy.stats import zscore\ntitanic["age_zscore"] = zscore(titanic["age"])\ntitanic["is_outlier"] = titanic["age_zscore"].apply(\n  lambda x: x &lt;= -2.5 or x &gt;= 2.5\n)\ntitanic[titanic["is_outlier"]]\n\n\nageAndFare = titanic[["age", "fare"]]\nageAndFare.plot.scatter(x = "age", y = "fare")\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nageAndFare = scaler.fit_transform(ageAndFare)\nageAndFare = pd.DataFrame(ageAndFare, columns = ["age", "fare"])\nageAndFare.plot.scatter(x = "age", y = "fare")\n\n\nfrom sklearn.cluster import DBSCAN\noutlier_detection = DBSCAN(\n  eps = 0.5,\n  metric="euclidean",\n  min_samples = 3,\n  n_jobs = -1)\nclusters = outlier_detection.fit_predict(ageAndFare)\n\nclusters\n\n\nfrom matplotlib import cm\ncmap = cm.get_cmap(\'Accent\')\nageAndFare.plot.scatter(\n  x = "age",\n  y = "fare",\n  c = clusters,\n  cmap = cmap,\n  colorbar = False\n)\n'
"clf = BaseEstimator('RandomForestClassifier')\nTraceback (most recent call last):\n  File &quot;/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py&quot;, line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File &quot;&lt;ipython-input-6-59dd1a62618a&gt;&quot;, line 1, in &lt;module&gt;\n    clf = BaseEstimator('RandomForestClassifier')\nTypeError: BaseEstimator() takes no arguments\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndef get_clf(name):\n    if name not in ('RandomForestClassifier', 'DecisionTreeClassifier'):\n        raise ValueError(f&quot;{name} is not a recognised option&quot;)    \n\n    classifiers = {\n        &quot;DecisionTreeClassifier&quot;: DecisionTreeClassifier,\n        &quot;RandomForestClassifier&quot;: RandomForestClassifier\n    }\n\n    classifier = classifiers[name]\n    return classifier()\n\ndef get_clf(name):\n    \n    if name not in ('RandomForestClassifier', 'DecisionTreeClassifier'):\n        raise ValueError(f&quot;{name} is not a recognised option&quot;)\n\n    if name == 'RandomForestClassifier':\n        return RandomForestClassifier()\n    elif name == 'DecisionTreeClassifier':\n        return DecisionTreeClassifier()\n    else: \n        raise RuntimeError()\n"
'curl localhost:5000/predict -d \'{"foo": "bar"}\' -H \'Content-Type: application/json\'\n'
'from livelossplot.tf_keras import PlotLossesCallback\n\nfrom livelossplot.inputs.tf_keras import PlotLossesCallback\n'
"# 3rd Fully Connected Layer\n# This is only the output of a hidden layer, you don't have to change this\nmodel.add(Dense(1000)) \nmodel.add(Activation(‘relu’))\n# Add Dropout\nmodel.add(Dropout(0.4))\n\n# Output Layer\n# This is what you want to change\nmodel.add(Dense(11))\nmodel.add(Activation(‘softmax’))\n"
"params =  [{'preprocess': [None, MaxAbsScaler(), MinMaxScaler(), StandardScaler()],\n            'model__gamma': ['scale', 'auto'],\n            'model__C': [1.0, 1.01, 1.015,3.0]\n          }]\n"
"import scipy.optimize as op\nimport numpy as np\nfrom pylab import scatter, show, legend, xlabel, ylabel\nfrom numpy import where\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\ndef sigmoid(z):\n    return 1/(1 + np.exp(-z))\n\n\ndef compute_gradient(theta, X, y, l):\n    m, n = X.shape\n    theta = theta.reshape((n, 1))\n    theta_r = theta[1:n, :]\n    y = y.reshape((m, 1))\n    h = sigmoid(X.dot(theta))\n    non_regularized_gradient = ((np.sum(((h-y)*X), axis=0))/m).reshape(n, 1)\n    reg = np.insert((l/m)*theta_r, 0, 0, axis=0)\n    grad = non_regularized_gradient + reg\n    return grad.flatten()\n\n\ndef compute_cost(theta, X, y, l):\n    h = sigmoid(X.dot(theta))\n    m, n = X.shape\n    theta = theta.reshape((n, 1))\n    theta_r = theta[1:n, :]\n    cost = np.sum((np.multiply(-y,np.log(h))-np.multiply((1-y),np.log(1-h))))/m\n    reg=(l/(2*m)) * np.sum(np.square(theta_r))\n    J = cost + reg\n    return J\n\n\ndef make_predictions(theta,X):\n    m, n = X.shape\n    return np.round(sigmoid(X.dot(theta.reshape(n, 1))))\n\n\ndata = np.loadtxt(open(&quot;ex2data2.txt&quot;, &quot;rb&quot;), delimiter=&quot;,&quot;, skiprows=1)\nnr, nc = data.shape\nX = data[:, 0:nc - 1]\ny = data[:, [nc - 1]]\npos = where(y == 1)\nneg = where(y == 0)\nscatter(X[pos, 0], X[pos, 1], marker='o', c='b')\nscatter(X[neg, 0], X[neg, 1], marker='x', c='r')\nxlabel('Equipment Test 1')\nylabel('Eguipment Test 2')\nlegend(['Nominal', 'Adverse'])\nshow()\nstoreX = X\npoly = PolynomialFeatures(6)\nX = poly.fit_transform(X)\nm, n = X.shape\ninitial_theta = np.zeros((n, 1))\nl = 1\n\nprint(&quot;Optimizing...&quot;)\nResult = op.minimize(fun=compute_cost, x0=initial_theta, args=(X, y, l), method='TNC', jac=compute_gradient)\noptimal_theta = Result.x\nprint(Result.x.shape)\nprint(&quot;optimal theta value&quot;)\nprint(optimal_theta)\np = make_predictions(optimal_theta, X)\nscore = np.mean(np.double(p == y))\nprint(&quot;Score:&quot;)\nprint(score)\n"
"img = cv.imread(&quot;Dilate.png&quot;)\n\ngray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\nedges = cv.Canny(gray, 150, 200, apertureSize=3)\ncv.imwrite(&quot;Canny.png&quot;, edges)\nelement = cv.getStructuringElement(cv.MORPH_RECT, (5, 3), (-1, -1))\ndilated = cv.dilate(edges, element)\ncv.imwrite(&quot;Eroded.png&quot;, dilated)\n\nminLineLength = 200\nmaxLineGap = 5\n\nlines = cv.HoughLinesP(dilated, cv.HOUGH_PROBABILISTIC, np.pi/180, 150, minLineLength, \n                       maxLineGap)\n\nfor x in range(0, len(lines)):\n    for x1, y1, x2, y2 in lines[x]:\n        pts = np.array([[x1, y1], [x2, y2]], np.int32)\n        cv.polylines(img, [pts], True, (0, 255, 0))\n\ncv.imwrite('dilate_final.png', img)\n"
'import random \n\nx = random.random()\ny = random.random()\n\nis_under = True\nfor i in range(1, len(xs)): # 0 is added to the tails of xs and ys\n    m = (y[1] - y[0])/(x[1]-x[0])\n    if y &gt; m * (x - x[0]) + y[0]:\n        is_under = False\n        break\n'
'* * * \n* * *\n* * *\n. . x\n'
'from module import *\n\nmodule.Class()\n\nfrom module import Class\n'
'cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv = cv_split, return_train_score=True)\n'
"pipeline = Pipeline(steps=[\n    ('pca', PCA(whiten=True)),\n    ('clf', MLPClassifier(batch_size=256, verbose=True, early_stopping=True))\n])\n\nparameters = {\n    'pca__n_components': [100, 200, 300, 400],\n    'clf__hidden_layer_sizes': [150, 200, 250, 300, 400],\n    'clf__solver': ['sgd', 'adam', 'lbfgs'],\n    'clf__activation': ['relu', 'tanh', 'identity', 'logistics']\n}\n\nparameters_after_tuning, score_after_tuning = tuning(pipeline, parameters, 20, X, y)\n"
"{\n  0: &quot;politics&quot;\n  1: &quot;sports&quot;,\n  2: &quot;weather&quot;,\n}\n\n0 The American government has launched ... today.\n1 FC Barcelona has won ... the country.\n2 The forecast looks ... okay.\n\nimport json\n\nwith open('labels.json') as json_file:\n    labels = json.load(json_file)\n    # This results in a Python dictionary where you can look-up a label given an index.\n\nwith open(raw.txt) as txt_file:\n    raw_texts = txt_file.readlines()\n    # This results in a list where you can retrieve the raw text by index like this: raw_texts[index].\n\nimport pandas as pd\n\ndata = pd.DataFrame(\n    {'label': labels.values(),\n     'text': raw_texts\n    })\n\n#    label      text\n# 0  politics   Sentence_1\n# 1  sports     Sentence_2\n# 2  weather    Sentence_3\n"
'tokenizer.fit_on_texts([shakespeare_text])\n\ntokenizer.fit_on_texts(shakespeare_text)\n'
"TRAINING SET LABEL FILE (train-labels-idx1-ubyte):\n[offset] [type]          [value]          [description]\n0000     32 bit integer  0x00000801(2049) magic number (MSB first)\n0004     32 bit integer  60000            number of items\n0008     unsigned byte   ??               label\n0009     unsigned byte   ??               label\n........\nxxxx     unsigned byte   ??               label\nThe labels values are 0 to 9.\n\nwith open('train-labels.idx1-ubyte', 'rb') as i:\n    magic, size = struct.unpack('&gt;II', i.read(8))\n    data_1 = np.fromfile(i, dtype=np.dtype(np.uint8)).newbyteorder(&quot;&gt;&quot;)   \n"
"# train and pickle\nsc = StandardScaler()\nX = sc.fit_transform(DataSource_train_independent.values)\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(X, y)\n\npickle.dump(sc, open('StandardScaler.pk', 'wb'))\npickle.dump(tree_reg, open('DecisionTree.pk', 'wb'))\n\n# load and predict\nsc = pickle.load(open('StandardScaler.pk', 'rb'))\nmodel = pickle.load(open('DecisionTree.pk', 'rb'))\n\nX_test = sc.transform(testdata.values)\npredictions = model.predict(X_test)\n\npipeline = Pipeline(steps=[('sc', StandardScaler()), \n                           ('tree_reg', DecisionTreeRegressor())])\n\npipeline.fit(X, y)\npipeline.predict(testdata.values)\n"
'  with tf.GradientTape() as t:\n    outputs = model(inputs)  # This line should be within context manager\n    current_loss = dummy_loss(outputs)\n'
'y_test = [[1], [2], [3]]\nimages = # fill in however you are getting your images into memory here\nclf.score(images, y_test)\n\n# or get the predictions by hand and do your own metric\npredictions = clf.predict(images)\nmse = np.mean(np.square(y_test - predictions))\n'
"# Create Model\n    self.model = Sequential()\n    self.model.add(LSTM(50, return_sequences=True, input_shape=(features_set.shape[1], features_set.shape[2])))\n    self.model.add(Dropout(0.2))\n    self.model.add(LSTM(50, return_sequences=True))\n    self.model.add(Dropout(0.2))\n    self.model.add(LSTM(50, return_sequences=True))\n    self.model.add(Dropout(0.2))\n    self.model.add(Dense(1))\n    self.model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n"
"adasyn = ADASYN(sampling_strategy='minority', random_state=8, n_neighbors=3)\n\nnew_data = data\nnew_classes = classes\n\nfor i in range(len(classes)-2):\n    new_data, new_classes = adasyn.fit_resample(data, classes)\n"
'accuracy_score(c_test, pred) # 0.743798\n\n1 - ((c_test != pred).sum() / X_test.shape[0]) # 0.743798\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\n# get your x and y as np array\nx = np.random.uniform(-1,1, 100)\ny = x - x**2 + x**3 - x**4 + np.random.normal(0, 0.1, 100)\n\ndef poly_fit(n):\n  ls_res=[]\n  ls_coeff = []\n  for i in range(2, n):\n    coeff, res, _, _, _ = np.polyfit(x, y, i, full=True)\n    ls_res.append(res)\n    ls_coeff.append(coeff)\n\n  # argmin should be taken from a penalized loss function\n  # add it here\n  return ls_coeff[np.argmin(ls_res)]\n\nplt.scatter(x, y, color='red')\ncoeff = poly_fit(6)\nplt.plot(np.sort(x), np.polyval(coeff, np.sort(x)), color='blue')\nplt.title('Polynomial Regression results')\nplt.xlabel('Position/level')\nplt.ylabel('Salary')\nplt.show()\n"
"y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n\nmodel.compile(loss=&quot;categorical_crossentropy&quot;, optimizer= &quot;adam&quot;, metrics=['accuracy'])\n"
'desired_width, desired_height = (100, 100)\nfinal_img = np.zeros((desired_height, desired_width, 3), dtype=&quot;uint8&quot;)\nimg_crop = crop_brain_contour(cv_img)\nh,w = img_crop.shape[:2]\n#Make sure h &lt; desired_height and w &lt; desired_width\nx1 = int(desired_width/2 - w/2)\ny1 = int(desired_height/2 - h/2)\nx2 = x1 + w\ny2 = y1 + h\n\nfinal_img[y1:y2, x1:x2, :] = img_crop\n\n# Write the final image\ncv2.imwrite(&quot;./extracted_data_1/image%04i.bmp&quot; %i,final_img)\n'
"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport numpy as np\n\nfig = plt.figure()\nax = fig.gca(projection='3d')   # Create the axes\n\n# Data\nX = np.linspace(-8, 8, 100)\nY = np.linspace(-4, 4, 100)\nX, Y = np.meshgrid(X, Y)\nZ = X**2 + Y**2\n\n# Plot the 3d surface\nsurface = ax.plot_surface(X, Y, Z,\n                          cmap=cm.coolwarm,\n                          rstride = 2,\n                          cstride = 2)\n\n# Set some labels\nax.set_xlabel('x-axis')\nax.set_ylabel('y-axis')\nax.set_zlabel('z-axis')\n\nplt.show()\n"
"pipe_steps = [('rfc',RandomForestClassifier(random_state=rs))]\n"
'x_data.resize((70000, 84, 1))\n'
"from sklearn.model_selection import train_test_split\n from sklearn.datasets import make_blobs\n \n#  Dummy dataset for example purpose\n X, y = make_blobs(n_samples=1000, centers=2, n_features=2, cluster_std=6.0)\n\n# first partition i.e. &quot;train-set&quot; and &quot;test-set&quot;\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=123)\n\n# second partition, we're splitting the &quot;train-set&quot; into 2 sets, thus creating a new partition of &quot;train-set&quot; and &quot;validation-set&quot;\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.9, random_state=123)\n\nprint(X_train.shape, X_test.shape, X_val.shape)  # output : ((810, 2), (100, 2), (90, 2))\n"
"&gt; df[df.index.isin(df[['view', 'video']].join(df[['width', 'height']]//2).drop_duplicates().index)\n       view                      video  left  width  top  height\n0   Endzone   57906_000718_Endzone.mp4   372   17.0  279      17\n1   Endzone   57906_000718_Endzone.mp4   851   20.0  273      14\n3   Endzone   57906_000718_Endzone.mp4   855   21.0  267      16\n5  Sideline  57906_000718_Sideline.mp4   763   18.0   98      26\n"
'length=len(x_train)\nb_max= 80 # set this based on  how much your  memory can hold\nbatch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and \n                  length/n&lt;=b_max],reverse=True)[0] \nsteps=int(length/batch_size)\n'
"import tensorflow as tf\n\nfor _ in range(5):\n    ds = tf.data.Dataset.range(1, 10).shuffle(4, seed=42).batch(3)\n    for i in ds:\n        print(i)\n    print()\n\ntf.Tensor([4 1 2], shape=(3,), dtype=int64)\ntf.Tensor([7 3 6], shape=(3,), dtype=int64)\ntf.Tensor([5 9 8], shape=(3,), dtype=int64)\n\ntf.Tensor([4 1 2], shape=(3,), dtype=int64)\ntf.Tensor([7 3 6], shape=(3,), dtype=int64)\ntf.Tensor([5 9 8], shape=(3,), dtype=int64)\n\ntf.Tensor([4 1 2], shape=(3,), dtype=int64)\ntf.Tensor([7 3 6], shape=(3,), dtype=int64)\ntf.Tensor([5 9 8], shape=(3,), dtype=int64)\n\ntf.Tensor([4 1 2], shape=(3,), dtype=int64)\ntf.Tensor([7 3 6], shape=(3,), dtype=int64)\ntf.Tensor([5 9 8], shape=(3,), dtype=int64)\n\ntf.Tensor([4 1 2], shape=(3,), dtype=int64)\ntf.Tensor([7 3 6], shape=(3,), dtype=int64)\ntf.Tensor([5 9 8], shape=(3,), dtype=int64)\n\nds = tf.data.Dataset.list_files(r'C:\\Users\\User\\Downloads\\*', shuffle=False)\n"
"def build_string_from_dict(d, sep='__'):\n    fd = _flatten_dict(d)\n    return sep.join(['{}={}'.format(k, _value2str(fd[k])) for k in sorted(fd.keys())])\n\ndef _flatten_dict(d, parent_key='', sep='_'):\n    # from http://stackoverflow.com/a/6027615/2476373\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(_flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\nflags_dict = vars(FLAGS)['__flags']\nprint build_string_from_dict(flags_dict)\n"
'# Note w[0] = w1, w[1] = w2. \nxs = [0, -b/w[0]]   # x-coordinate of the two points on line.\nys = [-b/w[1], 0]   # y-coordinate.\n'
"df.groupby('label', as_index=False).apply(lambda x: x.sample(2)) \\\n                                   .reset_index(level=0, drop=True)\nOut: \n           0         1         2         3         4  label\ns1  0.433731  0.886622  0.683993  0.125918  0.398787      1\ns1  0.719834  0.435971  0.935742  0.885779  0.460693      1\ns2  0.324877  0.962413  0.366274  0.980935  0.487806      2\ns2  0.600318  0.633574  0.453003  0.291159  0.223662      2\ns3  0.741116  0.167992  0.513374  0.485132  0.550467      3\ns3  0.301959  0.843531  0.654343  0.726779  0.594402      3\n\npd.concat(g.sample(2) for idx, g in df.groupby('label'))\n\n           0         1         2         3         4  label\ns1  0.442293  0.470318  0.559764  0.829743  0.146971      1\ns1  0.603235  0.218269  0.516422  0.295342  0.466475      1\ns2  0.569428  0.109494  0.035729  0.548579  0.760698      2\ns2  0.600318  0.633574  0.453003  0.291159  0.223662      2\ns3  0.412750  0.079504  0.433272  0.136108  0.740311      3\ns3  0.462627  0.025328  0.245863  0.931857  0.576927      3\n"
"data = np.array(data).reshape(-1,1)\nclf = mixture.GaussianMixture(n_components=1, covariance_type='full')\nclf.fit(data)\n\nx = np.array(np.linspace(-8000,8000,32000)).reshape(-1,1)\ny = clf.score_samples(x)\n\nplt.plot(x, y)\nplt.show()\n"
'n_folds = 10\nskf = list(StratifiedKFold(y, n_folds, shuffle=True, random_state=SEED)) \n'
'classifier = nltk.classify.NaiveBayesClassifier(training)\n\nprint(classifier.classify(get_word_features(["What", "is", "this"])))\n'
"import sklearn.metrics as sm\nimport numpy as np\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\n\ndata = 'C:/Users/seral_000/AllData.csv'\ndataframe = read_csv(data, names=None)\n\ndataframe = dataframe.sample(frac=1).reset_index(drop=True)\ndataset = dataframe.values\n\nX = dataset[:,0:-1]\nY = dataset[:,-1]\n\nX_train, X_test, Y_train, Y_test = train_test_split(\nX, Y, test_size=0.15, random_state=0)\n\nmodel = svm.SVC( C                        =   0.01,\n                 gamma                    =  'auto',\n                 kernel                   =  'linear',\n                 degree                   =   3,\n                 class_weight             =  'balanced',\n                 coef0                    =   0.0,\n                 decision_function_shape  =   None,\n                 probability              =   False,\n                 max_iter                 =  -1,\n                 tol                      =   0.001,\n                 cache_size               = 700,\n                 random_state             =   None,\n                 shrinking                =   True,\n                 verbose                  =   False\n                 )\n\nmodel.fit(X_train, Y_train) \n\nY_pred = model.predict(X_test)\nprint(Y_pred)\n\nACC = sm.accuracy_score(Y_test, Y_pred)\nprint ACC\n\ntarget_names = ['A', 'B', 'C']\nprint(sm.classification_report(Y_test, Y_pred, target_names=target_names))\n\n[ 1.  3.  1.  2.  3.  3.  2.  1.  1.  1.  1.  1.  2.  2.  3.  2.  3.  3.\n  2.  1.  2.  1.  1.  3.  2.  2.  2.  2.  1.  2.  2.  2.  2.  3.  3.  1.\n  3.  2.  2.  1.  2.  3.  1.  1.  2.  2.  3.  2.  1.  3.  1.  2.  1.  1.\n  1.  1.  1.  2.  1.  2.  1.  3.  3.  2.  3.  1.  1.  2.  2.  2.  2.  2.\n  2.  1.  3.  1.  1.  3.  3.  3.  1.  2.  2.  1.]\n\n0.52380952381\n\n                  precision recall    f1-score    N-supports\n\n          A       0.71      0.56      0.63        39\n          B       0.39      0.45      0.42        29\n          C       0.45      0.56      0.50        16\n\navg / total       0.55      0.52      0.53        84\n"
'np.sort(data_train.data.todense(), axis=1)\n'
'(arr == val).all(axis=-1).argmax()\n\nIn [977]: arr\nOut[977]: \narray([[1, 1, 1],\n       [1, 0, 1],\n       [0, 0, 1]])\n\nIn [978]: val\nOut[978]: array([1, 0, 1])\n\nIn [979]: (arr == val).all(axis=1).argmax()\nOut[979]: 1\n\n# https://stackoverflow.com/a/44999009/ @Divakar\ndef view1D(a): # a is array\n    a = np.ascontiguousarray(a)\n    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\n    return a.view(void_dt).ravel()\n\nout = (view1D(arr) == view1D(val[None])).argmax()\n\ndef first_match_index_along_axis(arr, val, axis):    \n    s = [None]*arr.ndim\n    s[axis] = Ellipsis\n    mask = val[np.s_[s]] == arr\n    idx = mask.all(axis=axis,keepdims=True).argmax()\n    shp = list(arr.shape)\n    del shp[axis]\n    return np.unravel_index(idx, shp)\n\nIn [74]: arr = np.random.randint(0,9,(4,5,6,7))\n\nIn [75]: first_match_index_along_axis(arr, arr[2,:,1,0], axis=1)\nOut[75]: (2, 1, 0)\n\nIn [76]: first_match_index_along_axis(arr, arr[2,1,3,:], axis=3)\nOut[76]: (2, 1, 3)\n'
'lda = models.LdaModel(corpus, num_topics=45, id2word=dictionary, update_every=5, chunksize=10000,  passes=1)\n'
"def time_converter(time_str):\n    hours = int(time_str[:2])\n    minutes = int(time_str[2:])\n    time_since_midnight = (hours * 60) + minutes\n    return time_since_midnight\n\ntime_converter('1940') # will return 1180 integer for 1180 minutes since midnight\n"
'refit : boolean, or string default=True\n        Refit an estimator using the best found parameters on the whole dataset.\n'
"import numpy as np\n\nclass_1 = [0.5, 0.4, 0.1, 0.0]\nclass_2 = [0.0, 0.4, 0.6, 0.0] \nclass_3 = [0.5, 0.4, 0.05, 0.05] \nclass_combined = np.array([class_1, class_2, class_3])\nclass_combined\n\n# VotingClassifier(voting='hard')\nhard_voting = np_matrix.argmax(axis=1)\nhard = np.bincount(voting).argmax()\n0\n\n# VotingClassifier(voting='soft')\nsoft_sum = class_combined.sum(axis=0)\nsoft = soft_sum.argmax()\n1\n"
'def transform(self, X):\n        """Return class labels or probabilities for X for each estimator.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n        Returns\n        -------\n        If `voting=\'soft\'` and `flatten_transform=True`:\n          array-like = (n_classifiers, n_samples * n_classes)\n          otherwise array-like = (n_classifiers, n_samples, n_classes)\n            Class probabilities calculated by each classifier.\n        If `voting=\'hard\'`:\n          array-like = [n_samples, n_classifiers]\n            Class labels predicted by each classifier.\n        """\n\nif(ratio&gt;threshold):\n     return 1\nelif(ratio&lt;(1-threshold)):\n     return 0\nelse:\n     #we don\'t make the prediction\n     return -1\n'
"valid_chars = {x:idx+1 for idx, x in enumerate(set(''.join(X1)))}\n"
"plt.xlim(60,200)\nplt.ylim(60,200)\n\n# create a figure object    \nfig = plt.figure()\n# create two axes within the figure and arrange them on the grid 1x2\nax1 = fig.add_Subplot(121)\n# ax2 is the second set of axes so it is 1x2, 2nd plot (hence 122)\n# they won't have the same limits this way because they are set up as separate objects, whereas in your example they are the same object that is being re-purposed each time!\nax2 = fig.add_Subplot(122)\n\nax1.plot(X1,Y1)\nax2.plot(X2,Y2)\n"
'inp_layer = Input((maxlen,))\nx = Embedding(max_features, embed_size, weights=[trainDataVecs])(x)\nx = LSTM(50, dropout=0.1)(x)\n'
"graph_def = tf.GraphDef()\nwith open('alexnet.pb', 'rb') as f:\n    graph_def.ParseFromString(f.read())\n\nwith tf.Graph().as_default() as graph:\n    importer.import_graph_def(graph_def, name='')\n\nnew_model = tf.GraphDef()\n\nwith tf.Session(graph=graph) as sess:    \n    for n in sess.graph_def.node:            \n        nn = new_model.node.add()\n        nn.CopyFrom(n)\n        if n.op.name == 'pool5':\n            break;\n\ntf.train.write_graph(new_model, '.', 'cut_model.pb', as_text=False)\n"
"from keras import backend as K\n\nclass CustomDense(Dense):\n    def __init__(self, units, **kwargs):\n        super(CustomDense, self).__init__(units, **kwargs)\n\n    def call(self, inputs):\n        output = K.dot(inputs, K.softmax(self.kernel, axis=-1))\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\nmodel = Sequential()\nmodel.add(CustomDense(2, use_bias=False, input_shape=(3,)))\n\nmodel.compile(loss='mse', optimizer='adam')\n\nimport numpy as np\n\nw = np.array([[2,3], [3,1], [1,-1]])\ninp = np.array([[2,1,3]])\n\nmodel.layers[0].set_weights([w])\nprint(model.predict(inp))\n\n# output\n[[4.0610714 1.9389288]]\n\nsoft_w = np.exp(w) / np.sum(np.exp(w), axis=-1, keepdims=True)\nprint(np.dot(inp, soft_w))\n\n[[4.06107115 1.93892885]]\n"
"X = df[['Hours Studied']]  # note the double brackets, shape (10, 1)\ny = df['Grade']\nmodel = LinearRegression().fit(X, y)\n\nmodel.predict([[5]])\narray([50.])\n"
'from keras import backend as K\n\nK.clear_session() \n'
'from sklearn.metrics import confusion_matrix\nconfusion_matrix(labels_test, pred)\n'
"icecream = ['Vanilla', 'Strawberry', 'Chocolate', 'Peach']\n\nicecream = ['vanilla', 'strawberry', 'chocolate', 'peach']\n\n                vanilla    strawberry    chocolate    peach    totalnum\nFranks Store       2            1            1          0        4.0\n"
'def cosine_distance(input):\n    jd = K.l2_normalize(input, axis=-1)\n    jt_six = K.l2_normalize(six_title_embedding, axis=-1)\n    jd = K.expand_dims(jd, axis=1)  # now it would have a shape of (None, 1, 100)\n    return jd * jt_six  # the result would be (None, 6, 100)\n'
'self._initial_state = cell.zero_state(config.batch_size, data_type())\nstate = self._initial_state\noutputs = []\nwith tf.variable_scope("RNN"):\n  for time_step in range(self.num_steps):\n    if time_step &gt; 0: tf.get_variable_scope().reuse_variables()\n    (cell_output, state) = cell(inputs[:, time_step, :], state)\n    outputs.append(cell_output)\noutput = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])\n'
"y=['Yearly Amount Spent']\n\ny\n# ['Yearly Amount Spent']\n\ny=customers['Yearly Amount Spent']\n"
'housing = stratTrainSet.drop("median_house_value", axis=1)\nhousing_labels = stratTestSet["median_house_value"].copy()\n\nhousing = stratTrainSet.drop("median_house_value", axis=1)\nhousing_labels = stratTrainSet["median_house_value"].copy()\n'
'estimators_ : ndarray of DecisionTreeRegressor,shape (n_estimators, loss_.K)\n\n    The collection of fitted sub-estimators. loss_.K is 1 for binary classification, otherwise n_classes.\n\n\nn_estimators_ : int\n\n    The number of estimators as selected by early stopping (if n_iter_no_change is specified). Otherwise it is set to n_estimators\n'
'precision_recall_fscore_support(y_test, pred)\n'
'grads_and_vars = opt.compute_gradients(loss)\ntrain_op = opt.apply_gradients(grads_and_vars)\n'
'# Invert the mapping dictionary you created\ninv_mapping_dict = {cat: {v: k for k, v in map_dict.items()} for cat, map_dict in mapping_dict.items()}\n\n# Assuming `predictions` is your resulting dataframe.\n# Replace the predictions with the inverted mapping dictionary.\npredictions.replace(inv_mapping_dict)\n'
'hidden_layer_sizes = (100, 50, 25)\n'
'X_train, X_test, y_train, y_test = \n        train_test_split(X_data, y_data, test_size=0.3, random_state=42, stratify=y_data)\n\nX_test, X_val, y_test, y_val = \n        train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n'
'    Iso_outliers = IsolationForest().fit(X_train)\n    Iso_outliers_train = Iso_outliers.predict(X_train)\n'
'print(classification_report(pred, ytest)) # wrong order of arguments\n\n             precision    recall  f1-score   support\n\n    class 0       0.00      0.00      0.00         0\n    class 1       1.00      0.63      0.77       171\n\navg / total       1.00      0.63      0.77       171\n\nprint(classification_report(ytest, pred)) # ytest first\n\n             precision    recall  f1-score   support\n\n    class 0       0.00      0.00      0.00        63\n    class 1       0.63      1.00      0.77       108\n\navg / total       0.40      0.63      0.49       171\n\npred\n# result:\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\nxtrain,xtest,ytrain,ytest = train_test_split(X,y,test_size=0.3,random_state=42)\n\nfrom sklearn.svm import SVC\nsvc=SVC()\nsvc.fit(xtrain, ytrain)\npred = svc.predict(xtest)\n\nprint(classification_report(ytest, pred))\n'
'IMG_SIZE = 150\n\nDATADIR = "/Users/path-to-data/"\nCATEGORIES = [\'green\', \'yellow\', \'red\']\n\ntraining_data = []\n\nfor category in CATEGORIES:\n    path = os.path.join(DATADIR, category)\n    class_num = CATEGORIES.index(category)\n    print(class_num)\n    for img in os.listdir(path):\n        try:\n            img_array = cv2.imread(os.path.join(path,img))\n            new_array = cv2.resize(img_array,(IMG_SIZE, IMG_SIZE))\n            training_data.append([new_array, class_num])\n        except Exception as e:\n            pass\n\nimport random\nrandom.shuffle(training_data)\n\nX = []\ny = []\n\nfor features, label in training_data:\n    X.append(features) \n    y.append(label)\n\n# X and y are currently of type list (list of python array), we will convert these to numpy array so we can feed it into our model.\nX = np.array(X)   # (112, 150, 150, 3)\ny = np.array(y)   # (112,)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation=\'relu\',input_shape=(150, 150, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation=\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation=\'relu\'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation=\'relu\'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation=\'softmax\'))\n\nmodel.compile(loss=\'sparse_categorical_crossentropy\',optimizer=\'Adam\',metrics=[\'sparse_categorical_accuracy\'])\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n'
"probabilities['other methode'] = proba[:,1]\nprobabilities['diff'] = probabilities[1]-probabilities['other method']\nprobabilities[probabilities['diff'] != 0]\n\nClasses 0    1        2     Y   other method diff\n20   1.0    0.0     0.0     0   0.1         -0.1\n36   1.0    0.0     0.0     0   0.1         -0.1\n41   1.0    0.0     0.0     0   0.1         -0.1\n50   0.0    1.0     0.0     1   0.9         0.1\n52   0.0    0.9     0.1     1   1.0         -0.1\n56   0.0    0.9     0.1     1   1.0         -0.1\n57   0.0    0.9     0.1     1   1.0         -0.1\n59   0.0    1.0     0.0     1   0.9         0.1\n60   0.0    0.9     0.1     1   1.0         -0.1\n68   0.0    0.9     0.1     1   1.0         -0.1\n... ... ... ... ... ... ...\n123  0.0    0.2     0.8     2   0.4         -0.2\n127  0.0    0.2     0.8     2   0.1         0.1\n129  0.0    0.1     0.9     2   0.6         -0.5\n133  0.0    0.1     0.9     2   0.9         -0.8\n134  0.0    0.2     0.8     2   0.6         -0.4\n137  0.0    0.0     1.0     2   0.1         -0.1\n138  0.0    0.3     0.7     2   0.6         -0.3\n141  0.0    0.0     1.0     2   0.1         -0.1\n142  0.0    0.0     1.0     2   0.1         -0.1\n146  0.0    0.0     1.0     2   0.1         -0.1\n\nclf.fit(X, y)\nclf.predict_proba(X)\n\ncross_val_predict(clf, X, y, cv=k_fold, method='predict_proba')\n\ndf = pd.DataFrame(index=['x1','x2',...,'x1000'],columns=['prediction_class_1']).fillna(0)\ndf['prediction_class_1'] = clf.predict(X) #clf trained and X the features values\nprint(df.sort_values('prediction_class_1'))\n"
"df['OFFENSE_CODE'].apply(str)\n\ndf['OFFENSE_CODE'] = df['OFFENSE_CODE'].apply(str)\n"
'x_train = np.expand_dims(x_train, axis=-1)\nx_dev = np.expand_dims(x_dev, axis=-1)\n\nmain_input = Input(shape=(294, 1)) # fix the input shape here as well \n'
"config = tensorflow.ConfigProto()\nconfig.gpu_options.allow_growth = True\nprint('############## Allowing Growth ###########')\nsession = tf.Session(config=config)\n\n# -------------------  start importing keras module ---------------------\nimport keras.backend.tensorflow_backend as K\nfrom keras.backend.tensorflow_backend import set_session\nset_session(tf.Session(config=config))\n"
'input_1 = Input(shape=(168, 5, ))\ndense_1 = Dense(units=64, input_shape=(None, 5,))(input_1)\n\ninput_2 = Input(shape=(168, 7, ))\nlstm_1 = LSTM(units=64, return_sequences=True, input_shape=(None, 7,))(input_2)\n\n'
'import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0.1, 0.5, -1.1, 1.2]) # continuous targets, i.e. regression problem\nskf = StratifiedKFold(n_splits=2)\n\nfor train_index, test_index in skf.split(X,y):\n    print("something")\n[...]\nValueError: Supported target types are: (\'binary\', \'multiclass\'). Got \'continuous\' instead.\n'
"layer6=Dropout(0.25)(layer6)\n\ninputs_1 = keras.Input(shape=(10081,1))\n\nlayer1 = Conv1D(64,14)(inputs_1)\nlayer2 = layers.MaxPool1D(5)(layer1)\nlayer3 = Conv1D(64, 14)(layer2)   \nlayer4 = layers.GlobalMaxPooling1D()(layer3)\n\ninputs_2 = keras.Input(shape=(84,))             \nlayer5 = layers.concatenate([layer4, inputs_2])\nlayer6 = Dense(128, activation='relu')(layer5)\nlayer6=Dropout(0.25)(layer6)\n\nlayer7 = Dense(2, activation='softmax')(layer6)\n\nmodel_2 = keras.models.Model(inputs = [inputs_1, inputs_2], output = [layer7])\nmodel_2.summary()\n"
"classifier.add(Dense(3923, activation='relu', kernel_initializer='random_normal', input_dim=13923)) \nclassifier.add(Dense(923, activation='relu', kernel_initializer='random_normal'))\nclassifier.add(Dense(23, activation='relu', kernel_initializer='random_normal')) \nclassifier.add(Dense(8, activation='sigmoid', kernel_initializer='random_normal'))\n"
'    def Return_BestHyperParameters(self):\n        gp_result = gp_minimize(func=lambda x: self.fitness(x),\n                                dimensions=dimensions,\n                                n_calls=12)\n        return gp_result\n\n\n\nfrom skopt.space import Integer, Categorical, Real\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\nimport tensorflow\nimport keras.backend as K\nimport GetPrediction\nimport Model\n\ndim_learning_rate = Real(low=1e-4, high=1e-2, prior=\'log-uniform\',\n                         name=\'learning_rate\')\ndim_num_dense_layers = Integer(low=1, high=5, name=\'num_dense_layers\')\ndim_num_input_nodes = Integer(low=16, high=128, name=\'num_input_nodes\')\ndim_num_dense_nodes = Integer(low=8, high=64, name=\'num_dense_nodes\')\ndim_dropout = Real(low=0.01, high=2, name=\'dropout\')\ndim_activation = Categorical(categories=[\'relu\', \'sigmoid\'],\n                             name=\'activation\')\ndim_batch_size = Integer(low=1, high=128, name=\'batch_size\')\ndim_adam_decay = Real(low=1e-6, high=1e-2, name="adam_decay")\n\ndimensions = [dim_learning_rate,\n              dim_num_dense_layers,\n              dim_num_input_nodes,\n              dim_num_dense_nodes,\n              dim_dropout,\n              dim_activation,\n              dim_batch_size,\n              dim_adam_decay\n              ]\ndefault_parameters = [1e-3, 1, 512, 13, 0.5, \'relu\', 64, 1e-3]\n\n\ndef fitness_wrapper(_STOCK, _INTERVALL, _TYPE):\n\n    @use_named_args(dimensions=dimensions)\n    def fitness(self, learning_rate, num_dense_layers, num_input_nodes,\n                num_dense_nodes, dropout, activation, batch_size, rms_decay):\n        model = Model.Tuning_Model(learning_rate=learning_rate,\n                                   num_dense_layers=num_dense_layers,\n                                   num_input_nodes=num_input_nodes,\n                                   num_dense_nodes=num_dense_nodes,\n                                   dropout=dropout,\n                                   activation=activation,\n                                   rms_decay=rms_decay\n                                   )\n\n        Train_Closing, \\\n        Train_Volume, \\\n        Train_Labels, \\\n        Test_Closing, \\\n        Test_Volume, \\\n        Test_Labels, \\\n        ClosingData_scaled, \\\n        VolumeData_scaled = GetPrediction.Return_Data(self.stock, self.interval, self._type)\n\n        # named blackbox becuase it represents the structure\n        blackbox = model.fit(\n            [\n                Train_Closing,\n                Train_Volume\n            ],\n            [\n                Train_Labels\n            ],\n            validation_data=(\n                [\n                    Test_Closing,\n                    Test_Volume\n                ],\n                [\n                    Test_Labels\n                ]\n            ),\n            epochs=250,\n            batch_size=batch_size\n        )\n        # return the validation accuracy for the last epoch.\n        accuracy = blackbox.history[\'val_mae\'][-1]\n\n        # Delete the Keras model with these hyper-parameters from memory.\n        del model\n\n        # Clear the Keras session, otherwise it will keep adding new\n        # models to the same TensorFlow graph each time we create\n        # a model with a different set of hyper-parameters.\n        K.clear_session()\n        tensorflow.reset_default_graph()\n\n        # the optimizer aims for the lowest score, so we return our negative accuracy\n        return -accuracy\n\n    return fitness\n\n\ndef Return_BestHyperParameters(_STOCK, _INTERVALL, _TYPE):\n    gp_result = gp_minimize(func=fitness_wrapper(_STOCK, _INTERVALL, _TYPE),\n                            dimensions=dimensions,\n                            n_calls=12)\n    return gp_result\n\n\nif __name__ == \'__main__\':\n    print(Return_BestHyperParameters(_STOCK="DJI", _INTERVALL="", _TYPE="Daily"))\n'
'Model: "sequential_4"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntime_distributed_12 (TimeDis (None, None, 20, 20, 32)  2080      \n_________________________________________________________________\ntime_distributed_13 (TimeDis (None, None, 9, 9, 64)    32832     \n_________________________________________________________________\ntime_distributed_14 (TimeDis (None, None, 7, 7, 64)    36928     \n_________________________________________________________________\nconv_lst_m2d_3 (ConvLSTM2D)  (None, 7, 7, 512)         10618880  \n_________________________________________________________________\ntime_distributed_15 (TimeDis (None, 7, 7, 4)           2052      \n=================================================================\nTotal params: 10,692,772\nTrainable params: 10,692,772\nNon-trainable params: 0\n_________________________________________________________________\n'
" bigram_token  = list(bigram.vocab.keys())\n type(bigram_token[0])\n\n #op\n bytes\n\ncntr = Counter()\nfor key in bigram.vocab.keys():\n    if len(key.decode('utf-8').split(b'_')) &gt; 1: # here added .decode('utf-8')\n       cntr[key] += bigram.vocab[key]\n"
'y_train_categorical = lb.fit_transform(y_train)\ny_test_categorical = lb.fit_transform(y_test)\n\ny_train_categorical = lb.fit_transform(y_train)\ny_test_categorical = lb.transform(y_test) # transform only\n'
"data1=pd.read_csv('score.csv')\n\ny_pred2 = pd.DataFrame(clf.predict(data1[data1.columns.difference(['ID'])]),columns = ['Predicted'], index = data1.index)\n\npred = pd.concat([data1['ID'], y_pred2['Predicted']], axis = 1)\n"
'def compute_output_shape(self, input_shape):\n    return [\n        [input_shape[0], None, None, 2],\n        [input_shape[0], None, None, 3],\n        [input_shape[0], None, None, 1]\n    ]\n'
'loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True) \n'
'# invert scaling for forecast\ninv_yhat = concatenate((yhat, test_X[:, -7:]), axis=1)\ninv_yhat = scaler.inverse_transform(inv_yhat)\ninv_yhat = inv_yhat[:,0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = concatenate((test_y, test_X[:, -7:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# invert scaling for forecast\npred_scaler = MinMaxScaler(feature_range=(0, 1)).fit(dataset.values[:,0].reshape(-1, 1))\ninv_yhat = pred_scaler.inverse_transform(yhat)\n# invert scaling for actual\ninv_y = pred_scaler.inverse_transform(test_y)\n'
"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n"
'xb = xb.view(-1, 32*32) \n\nxb = xb.view(-1, 32*32*3) \n'
'dw.add_(weight_decay, w)\n\na = torch.FloatTensor([0, 1.0, 2.0, 3.0])\nb = torch.FloatTensor([0, 4.0, 5.0, 6.0])\nc = 1.0\nz = a.add(b, c)\n\nTypeError: add() takes 1 positional argument but 2 were given\n\nz = a.add(b, alpha=c)\n'
"import os\nfrom collections import Counter,defaultdict\nd2 = defaultdict(dict)\nword_list = ['vs', 'mln', 'money']\nfor fil in d.values():\n    with open(fil[0]) as f:\n       path, name = os.path.split(fil[0])\n       words_c = Counter([word for line in f for word in line.split()])\n       for word in word_list:\n           d2[word][name] = words_c[word]\n\nd2['vs']['5.txt']\n"
'&lt;w, x&gt;\n\n&lt;w, x&gt; + b\n'
'import cPickle as pickle\nfrom sklearn.externals import joblib\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\ntrain_x = [0,1,2,6,\'true\',\'false\']\nle.fit_transform(train_x)\n\n# Save your encoding\njoblib.dump(le, \'/path/to/save/model\')\n# OR\npickle.dump(le, open( \'/path/to/model\', "wb" ) )\n\n# Load those encodings\nle = joblib.load(\'/path/to/save/model\') \n# OR\nle = pickle.load( open( \'/path/to/model\', "rb" ) )\n\n# Then use as normal\nnew_x = [0,0,0,2,2,2,\'false\']\nle.transform(new_x)\n# array([0, 0, 0, 1, 1, 1, 3])\n'
'from sklearn.externals import joblib\njoblib.dump(clf, \'filename.pkl\') \n\nclf = joblib.load(\'filename.pkl\')\n\nfrom pathlib import Path\n\nmy_file = Path("/path/to/file")\nif my_file.is_file():\n'
"vocabulary = ['Football', 'Europe'] # Put your targer words in here\nhot_encoder = [0] * len(vocabulary)\nbinary_bag = dict(zip(vocabulary, hot_encoder))\nwith open('text.txt', 'r') as f: #Put your text sample in here\n    words = [word for line in f for word in line.split()]\n    for word in words:\n        if word in vocabulary and binary_bag[word] == 0:\n            binary_bag[word] = 1\n"
'x=x.reshape(-1,1)\n'
'hidden_dims = 250\nmodel.add(Dense(hidden_dims))\n'
'h_pool2_flat = tf.reshape(h_pool2, [-1, 8*8*50])\n\nW_fc1 = weight_variable([8 * 8* 50, 500])\n'
'[0.53419214, 0.55088341, 0.53190422, 0.52382213, 0.53469753, 0.53098464,\n0.51968938, 0.53249627, 0.52852863, 0.52497149, 0.52816379, 0.5457474,\n0.52565753, 0.5276686, 0.52042121, 0.52128422, 0.52535951, 0.52730507]\n\n[0.43440229, 0.48104468, 0.49194154, 0.4766106, 0.50065982, 0.47388917,\n0.51052755, 0.50618082, 0.48478326, 0.4846094, 0.50018799, 0.4800632,\n0.4181695, 0.48307362, 0.5063237, 0.50420266, 0.47321039, 0.44235682]\n'
'set.seed(12345)\n#getting training data set sizes of .20 (in this case 20 out of 100)\ntrain.x&lt;-sample(1:100, 20)\ntrain.y&lt;-sample(1:100, 20)\n\n#simulating random data\nx&lt;-rnorm(100)\ny&lt;-rnorm(100)\n\n#sub-setting the x data\ntraining.x.data&lt;-x[train]\ntesting.x.data&lt;-x[-train]\n\n#sub-setting the y data\ntraining.y.data&lt;-y[train]\ntesting.y.data&lt;-y[-train]\n'
'X, X_pred, y = scale(df_data), scale(df_test), df_target\n\nscaler = StandardScaler()\nscaler.fit(df_data)\nX = scaler.transform(df_data)\nX_pred = scaler.transform(df_test)\n\nLogReg.fit(X, y)\nnovel = LogReg.predict(X_pred)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression \nX = np.array([[6], [8], [10], [14], [18]]).reshape(-1,1)\ny = [7,9,13,17.5,18]\n\n# In[2]:\nmodel = LinearRegression()  # Create an instance of the estimator\nmodel.fit(X, y)  # Fit the model on the training data \n\n# Predict the price of a pizza with a diameter that has never been                              seen before \n#test_pizza = np.array([[12]])  \npredicted_price = model.predict(X)\n\n\nplt.figure()\nplt.title(\'pizza thing\')\nplt.xlabel(\'x-axis diam\')\nplt.ylabel(\'y-axis price dollar dollar\')\nplt.plot(X,y, \'k.\')\nplt.plot(X,predicted_price)\nplt.axis([0,25,0,25])\nplt.grid(True)\nplt.show()\n\nprint(\'A 12" pizza should cost: $%.2f\' % predicted_price)\n'
'    # Initialise network\n    self.weights1 = (np.random.rand(self.nin+1,self.nhidden)-0.5)*2/np.sqrt(self.nin)\n    self.weights2 = (np.random.rand(self.nhidden+1,self.nout)-0.5)*2/np.sqrt(self.nhidden)\n\nfrom sklearn.preprocessing import StandardScaler\nfor i in range(apk_train_data.shape[1]-1):\n    scaler = StandardScaler().fit(apk_train_data[:,i].copy())\n    apk_train_data[:,i] = scaler.transform(apk_train_data[:,i].copy())\n    apk_test_data[:,i] = scaler.transform(apk_test_data[:,i].copy())\n\np.mlptrain(apk_train_data[:, 0:7], apk_train_data[:, 7:], 0.0001, 100000)\np.confmat(apk_test_data[:, 0:7], apk_test_data[:, 7:])\n# &gt;&gt; Percentage Correct:  71.4285714286\np.confmat(apk_train_data[:,0:7], apk_train_data[:,7:])\n# &gt;&gt; Percentage Correct: 88.8888888889\n'
'truncated_normal(mean=0.0, std=1.0)\n'
'df["lag_1"] = df["val3"].shift(1)\ndf["lag_2"] = df["val3"].shift(2)\ndf["lag_3"] = df["val3"].shift(3)\n\n[[var1_lag3, var1_lag2, var1_lag1],\n [var2_lag3, var2_lag2, var2_lag1],\n [var3_lag3, var3_lag2, var3_lag1]\n]\n'
"import pandas as pd\nbreakfast = [['Apple,Banana'],['Apple,Yogurt'],['Banana,Oatmeal']]\nbreakfast = [i[0].split(',') for i in breakfast]\n\n\ncolumns=list(set([j for i in breakfast for j in i]))\ncolumns.sort()\nvalues = [[1 if j in i else 0 for j in columns]for i in breakfast]\ndf = pd.DataFrame(values, columns=columns)\nprint(df) \n\n   Apple  Banana  Oatmeal  Yogurt\n0      1       1        0       0\n1      1       0        0       1\n2      0       1        1       0\n"
'mo.train_on_batch(x=[np.array([[i,i]])], y=[i])\n\nnp.array([[i,i]]).shape   # it is (1,2)\n'
'Conv2D(8, 16, 3, 1, 1)\n\nConv2D(1, 8, 1)\n'
'onehot_enc.fit(reviews_tokens)` \n'
"model = Sequential()\nmodel.add(Flatten(input_shape=(LENGTH, LENGTH)))\nmodel.add(Dense(24, activation='relu'))\n# the rest of the model\n\ntraining_data = np.reshape(training_data, (num_boards, LENGTH*LENGTH))\n\nmodel = Sequential() \nmodel.add(Dense(24,input_shape=(LENGTH*LENGTH,), activation='relu'))\n"
"import numpy as np\n\n# Define functions\ndef custom_fit(estimators, X, y):\n    for clf in estimators:\n        clf.fit(X, y)\n\ndef custom_predict(estimators, X, voting = 'soft', weights = None):\n\n    if voting == 'hard':\n        pred = np.asarray([clf.predict(X) for clf in estimators]).T\n        pred = np.apply_along_axis(lambda x:\n                                   np.argmax(np.bincount(x, weights=weights)),\n                                   axis=1,\n                                   arr=pred.astype('int'))\n    else:\n        pred = np.asarray([clf.predict_proba(X) for clf in estimators])\n        pred = np.average(pred, axis=0, weights=weights)\n        pred = np.argmax(pred, axis=1)\n\n    return pred\n\n\n# Use them\nestimators=[svm_isotonic, lr_isotonic]\ncustom_fit(estimators, X_val, y_val)\n\ncustom_predict(estimators, X_test)    \n"
"vocab = set(j for i in df['Ingredients'] for j in i) \n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(vocabulary=vocab, analyzer=lambda x: x)\n\nX = cv.fit_transform(df['Ingredients'])\n\ndf['Ingredients'] = df['Ingredients {ArrayOf&lt;string&gt;} '].apply(lambda x: [i.strip() for i in x.replace('[','').replace(']','').split(',')])\n\nX.todense()\n\nmatrix([[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1],\n        [0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n        [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)\n\ncv.get_feature_names()\n\n['Basil',\n 'Beef',\n 'Chicken',\n 'Coriander Seeds',\n 'Cumin',\n 'Garlic',\n 'Ginger',\n 'Onion',\n 'Oregano',\n 'Paprika',\n 'Spaghetti',\n 'Tomato']\n"
"from keras import losses\n\ndef loss_A(y_true, y_pred):\n    return losses.mean_squared_error(y_true[:,0], y_pred[:,0])\n\n#...\nmodel.compile(loss=loss_A, optimizer='adam')\n"
"#train test split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\n\n#Decision tree model\ndectree = DecisionTreeClassifier(class_weight='balanced', random_state=2018)\n"
"model.compile(optimizer='sparse_categorical_crossentropy', ...)\n\nfrom keras.utils import to_categorical\n\nlabels = to_categorical(np.array(labels), num_classes=10)\n"
"model = Sequential()\nmodel.add(TimeDistributed(Conv2D(5, (3,3), padding='same'), input_shape=(10, 100, 100, 3)))\n\nmodel.summary()\n\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntime_distributed_2 (TimeDist (None, 10, 100, 100, 5)   140       \n=================================================================\nTotal params: 140\nTrainable params: 140\nNon-trainable params: 0\n_________________________________________________________________\n"
'import keras.backend as K\n\n\ndef custom_loss(y_true, y_pred):\n    centers = K.constant([[-2.5, -1], [-1.25, -2], [.5, -1], [1.5, .25]])\n\n    # Expand dimensions to enable implicit broadcasting\n    y_pred_r = y_pred[:, None, :]  # Shape: (batch_size, 1, 2)\n    centers_r = centers[None, :, :]  # Shape: (1, nb_centers, 2)\n\n    # Compute minimum distance to centers for each element\n    distances = K.sqrt(K.sum(K.square(y_pred_r - centers_r), axis=-1))  # Shape=(batch_size, nb_centers)\n    min_distances = K.min(distances, axis=-1)  # Shape=(batch_size,)\n\n    # Output average of minimum distances\n    return K.mean(min_distances)\n'
'from sklearn.feature_extraction.text import TfidfVectorizer\nwords = [\'project\', \'management\', \'uml theory\', \'wireframe\']\nmod_tfidf = TfidfVectorizer()\nmod_tfidf.fit_transform(words)\n&lt;4x5 sparse matrix of type \'&lt;class \'numpy.float64\'&gt;\'\n    with 5 stored elements in Compressed Sparse Row format&gt;\n\nmod_tfidf.transform(words + ["dummy"])\n&lt;5x5 sparse matrix of type \'&lt;class \'numpy.float64\'&gt;\'\n    with 5 stored elements in Compressed Sparse Row format&gt;\n\nmod_tfidf.fit(words)\nmod_tfidf.transform(document_list)\n\nmod_tfidf = TfidfVectorizer(vocabulary=words)\n\nmod_tfidf.get_feature_names()\n'
"model = Sequential()\nmodel.add(LSTM(4, input_shape=(300000, 4), return_sequences=False))\nmodel.add(Dense(2, activation='softmax')) # 2 neurons because you have 2 classes\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n"
"# A sample dataframe with 5 columns\ndf = pd.DataFrame(np.random.randn(100,5))\n# Firsts 0, 1 will be retained and rest of the columns will be made as row\n# with their corresponding value. Finally we drop the variable axis\ndf = df.melt([0,1],value_name='A').drop('variable', axis=1)\n"
"from sklearn.svm import OneClassSVM\nX=[[0], [0.44], [0.45], [0.46], [1]]\nclf = OneClassSVM(gamma='auto', verbose=True)\nclf.fit(X)\n\n[LibSVM]\n\nOneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma='auto', kernel='rbf',\n            max_iter=-1, nu=0.5, random_state=None, shrinking=True, tol=0.001,\n            verbose=True)\n"
"import matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Load data from https://www.openml.org/d/554\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True)\nX = X / 255.\n\n# rescale the data, use the traditional train/test split\nX_train, X_test = X[:60000], X[60000:]\ny_train, y_test = y[:60000], y[60000:]\n\n# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\nmlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n                    solver='sgd', verbose=None, tol=1e-4, random_state=1,\n                    learning_rate_init=.1)\n\nmlp.fit(X_train, y_train)\n\nprint(y_train.shape)\nprint(mlp.predict_proba(X_test[:10]).sum(axis=1))\n\nenc = OneHotEncoder(handle_unknown='ignore')\n\nenc.fit(y_train.reshape(-1, 1))\n\ny_train_transformed = enc.transform(y_train.reshape(-1, 1)).toarray()\ny_test_transformed = enc.transform(y_test.reshape(-1, 1)).toarray()\n\n# mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n#                     solver='sgd', verbose=10, tol=1e-4, random_state=1)\nmlp_new = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n                    solver='sgd', verbose=None, tol=1e-4, random_state=1,\n                    learning_rate_init=.1)\n\nmlp_new.fit(X_train, y_train_transformed)\n\nprint(y_train_transformed.shape)\nprint(mlp_new.predict_proba(X_test[:10]).sum(axis=1))\n"
'model.add(mean_layer)\n'
"df_ = df.select_dtypes(exclude=['int', 'float'])\nfor col in df_.columns:\n    print(df_[col].unique()) # to print categories name only\n    print(df_[col].value_counts()) # to print count of every category\n"
'yaxis = [cat_to_name[str(i)] for i in axes[1][0].cpu()]\nxaxis = axes[0][0].cpu().numpy() \n'
'y_pred2 = clf.predict_proba(features2)[0]\nsomeclass = y_pred2.index(max(y_pred2)) # returns the class index *Maximum predicted value\nitsprob = max(y_pred2) # returns the Portability *Maximum predicted value\n'
"\n# user_filter filters the dataframe to all the rows where\n# 'userID' is NOT in the 'users' column (the value of which\n# is a list type)\nuser_filter = df['users'].map(lambda u: userID not in u)\n\n# cuisine_filter filters the dataframe to only the rows\n# where 'cuisine' exists in the 'cuisines' column (the value\n# of which is a list type)\ncuisine_filter = df['cuisines'].map(lambda c: cuisine in c)\n\n# Display the result, filtering by the weight assigned\ndf[user_filter &amp; cuisine_filter]\n\n"
"img = digits_with_zeros[0]\nimg = cv2.resize(img, dsize=(28, 28))\nimg = np.array(img).reshape(-1, 28,28,1)\n\nmodel = load_model('final_model.h5')\ndigit = model.predict_classes(img)\n"
"from keras.utils import to_categorical\nlabels = to_categorical(labels)\n\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical\n\n\nlabels = to_categorical(labels)\n\n# Splitting the data to training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(NDVI, labels, test_size=0.15, random_state=42)\n\n# Building the model\nmodel = Sequential([\n  Dense(128, activation='relu', input_shape=(17,),name=&quot;layer1&quot;),\n  Dense(64, activation='relu', name=&quot;layer2&quot;),\n  Dense(24, activation='softmax', name=&quot;layer3&quot;),\n])\nprint(model.summary())\n\n\n# Compiling the model\nmodel.compile(\n  optimizer='adam',                              # gradient-based optimizer\n  loss='categorical_crossentropy',               # (&gt;2 classes)\n  metrics=['accuracy'],\n)\n\n# Training the model\nmodel.fit(\n  X_train, # training data\n  y_train, # training targets\n  epochs=5,\n  batch_size=32,\n)\n"
'from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n'
'old_shape = images.shape\nN = images.shape[0]\nimages = np.reshape(images, (N, -1)) \nimages_two = np.reshape(images_two, (N, -1))\n\n## to reshape\n\nimages.reshape(old_shape)\n'
"import lightgbm\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Create some train and test data\nX, y = make_classification(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Define pipeline with scaler and lightgbm model\npipe = Pipeline([('scaler', StandardScaler()), ('lightgbm', lightgbm.LGBMClassifier())])\n\n# Train pipeline\npipe.fit(X_train, y_train)\n\n# Make predictions with pipeline (with lightgbm)\nprint(&quot;Predictions:&quot;, pipe.predict(X_test))\n\n# Evaluate pipeline performance\nprint(&quot;Performance score:&quot;, pipe.score(X_test, y_test))\n\nPredictions: [1 0 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0]\nPerformance score: 0.84\n"
"def transformation(inputs, labels):\n    tf.print('With transformation!')\n    return inputs, labels\n\ndef no_transformation(inputs, labels):\n    tf.print('No transformation!')\n    return inputs, labels\n\ndata_with_transform = data.take(4).map(transformation).batch(4)\ndata_no_transform = data.take(4).map(no_transformation).batch(4)\n\nif epoch &lt; 1:\n    ds = data_with_transform\nelse:\n    ds = data_no_transform\n\nfor X_train, y_train in ds:\n    train_step(X_train, y_train)\n\nimport tensorflow_datasets as tfds\nimport tensorflow as tf\n\ndata, info = tfds.load('iris', split='train', as_supervised=True,\n                       with_info=True)\n\ndef transformation(inputs, labels):\n    tf.print('With transformation!')\n    return inputs, labels\n\ndef no_transformation(inputs, labels):\n    tf.print('No transformation!')\n    return inputs, labels\n\ndata_with_transform = data.take(4).map(transformation).batch(4)\ndata_no_transform = data.take(4).map(no_transformation).batch(4)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(8, activation='relu'),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(info.features['label'].num_classes)\n])\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\ntrain_loss = tf.keras.metrics.Mean()\ntrain_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n\nopt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\n\n@tf.function\ndef train_step(inputs, labels):\n    with tf.GradientTape() as tape:\n        logits = model(inputs)\n        loss = loss_object(labels, logits)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    opt.apply_gradients(zip(gradients, model.trainable_variables))\n    train_loss(loss)\n    train_acc(labels, logits)\n\n\ndef main(epochs=5):\n\n    for epoch in range(epochs):\n\n        train_loss.reset_states()\n        train_acc.reset_states()\n\n        if epoch &lt; 1:\n            ds = data_with_transform\n        else:\n            ds = data_no_transform\n\n        for X_train, y_train in ds:\n            train_step(X_train, y_train)\n\nif __name__ == '__main__':\n    main()\n\nWith transformation!\nWith transformation!\nWith transformation!\nWith transformation!\n\nNo transformation!\nNo transformation!\nNo transformation!\nNo transformation!\n\nNo transformation!\nNo transformation!\nNo transformation!\nNo transformation!\n\nNo transformation!\nNo transformation!\nNo transformation!\nNo transformation!\n\nNo transformation!\nNo transformation!\nNo transformation!\nNo transformation!\n"
"pipeline = Pipeline(steps=[('prep',col_transform), ('m', LogisticRegression(max_iter=1000, C=c))])\npipeline.fit(X_train,y_train)       # &lt;- here is the problem\ny_hat = pipeline.predict(X_train)\nlr_array.append(f1_score(y_train,y_hat,pos_label=&quot;bad&quot;))\n"
'dataset_train = train_dataset.map(format_example).batch(1)\ndataset_test =test_dataset.map(format_example).batch(1)\n\nimport tensorflow as tf\n\nimages = tf.random.uniform(shape=(10, 224, 224, 3), maxval=256, dtype=tf.int32)\n\nds = tf.data.Dataset.from_tensor_slices(images)\n\nfor pic in ds:\n    print(pic.shape)\n    break\n\n(224, 224, 3)\n\nfor pic in ds.batch(4):\n    print(pic.shape)\n    break\n\n(4, 224, 224, 3)\n'
"def Gaussian_Kernel(x, mu, sigma):\n  p = (1./(math.sqrt(2. * math.pi * (sigma**2)))) * torch.exp((-1.) * (((x**2) - mu)/(2. * (sigma**2))))\n  return p\n\nclass MEE(torch.nn.Module):\n  def __init__(self):\n    super(MEE, self).__init__()\n\n  def forward(self, output, target, mu, variance):\n    error = output - target\n\n    error_diff = []\n    for i in range(0, error.size(0)):\n      for j in range(0, error.size(0)):\n        error_diff.append(error[i] - error[j]) # Assuming that's the desired operation\n\n    error_diff = torch.cat(error_diff)\n    kernel = Gaussian_Kernel(error_diff, mu, variance*(2**0.5))\n    loss = (1./(target.size(0)**2))*torch.sum(kernel)\n    \n    return loss\n"
'a2=tf.keras.layers.Conv2D(10, (3,3), input_shape=input_shape[1:])\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nstd = [[0.5, 0], [0, 0.5]]\nX1 = np.vstack((\n    np.random.multivariate_normal([2, -2], std, size=200),\n    np.random.multivariate_normal([-2, 2], std, size=200)\n))\ny1 = np.zeros(X1.shape[0])\n\nX2 = np.vstack((\n    np.random.multivariate_normal([2, 2], std, size=200),\n    np.random.multivariate_normal([-2, -2], std, size=200)\n))\ny2 = np.ones(X2.shape[0])\n\nX = np.vstack((X1, X2))\ny = np.hstack((y1, y2))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\nsvc_classifier = SVC(kernel='rbf', gamma='auto')\nsvc_classifier.fit(X_train, y_train)\n"
"import numpy as np\nimport pandas as pd\nimport sklearn.datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# load data\ndata_arr = sklearn.datasets.load_diabetes(as_frame=True).data.values\n\n# extract features and target\nX_raw = data_arr[:, 1:]\ny_raw = data_arr[:, :1]\n\n# add bias\nX = np.hstack((np.ones(y_raw.shape), X_raw))\ny = y_raw\n\n# do gradient descent\nlearning_rate = 0.001\niterations = 1000000\n\nobservations = X.shape[0]\nfeatures = X.shape[1]\n\nw = np.ones((features, 1))\n\nfor i in range(iterations):\n    w -= 2 * learning_rate * X.T.dot(X.dot(w) - y)\n\n# exclude the intercept as X already contains a column of ones\nreg = LinearRegression(fit_intercept=False).fit(X, y)\n\n# compare the estimated coefficients\nres = pd.DataFrame({\n    'manual': [format(x, '.6f') for x in w.flatten()],\n    'sklearn': [format(x, '.6f') for x in reg.coef_.flatten()]\n})\n\nres\n#       manual    sklearn\n# 0  -0.000000  -0.000000\n# 1   0.101424   0.101424\n# 2  -0.006468  -0.006468\n# 3   0.208211   0.208211\n# 4  -0.128653  -0.128653\n# 5   0.236556   0.236556\n# 6   0.132544   0.132544\n# 7  -0.039359  -0.039359\n# 8   0.177129   0.177129\n# 9   0.145396   0.145396\n\n# compare the RMSE\nprint(format(mean_squared_error(y, X.dot(w), squared=False), '.6f'))\n# 0.043111\n\nprint(format(mean_squared_error(y, reg.predict(X), squared=False), '.6f'))\n# 0.043111\n"
'lvl_of_interest =  np.array([6.5])\nlvl_of_interest = np.reshape(lvl_of_interest,(1,-1))\nlinear_regressor.predict(lvl_of_interest)\n'
'from datetime import datetime\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\nsampleTime = "23/02/2017 at 11:30 PM"\ntimeFormatted = datetime.strptime(sampleTime, "%d/%m/%Y at %I:%M %p")\nprint(timeFormatted)\n\nplt.scatter(timeFormatted, 0)\n\n# reshape X labels\nplt.gcf().autofmt_xdate()\nshowFormat = mdates.DateFormatter(\'%Y-%m-%d %H:%M\')\nplt.gca().xaxis.set_major_formatter(showFormat)\n\nplt.show()\n'
'import tensorflow as tf\n\nmax_output_length = 35\n\ninp = tf.keras.layers.Input(shape=(None, 10))\nx = tf.keras.layers.LSTM(20)(inp)\nx = tf.keras.layers.RepeatVector(max_output_length)(x)\nout = tf.keras.layers.LSTM(30, return_sequences=True)(x)\n\nmodel = tf.keras.Model(inp, out)\nmodel.summary()\n\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, None, 10)]        0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 20)                2480      \n_________________________________________________________________\nrepeat_vector (RepeatVector) (None, 35, 20)            0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 35, 30)            6120      \n=================================================================\nTotal params: 8,600\nTrainable params: 8,600\nNon-trainable params: 0\n_________________________________________________________________\n'
"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\n\ndef tfidf_vectorizer(corpus):\n\n    terms = list(set(' '.join([i for i in corpus]).split()))\n\n    terms.sort()\n\n    mat = np.zeros((len(corpus), len(terms)))\n\n    for i in range(len(corpus)):\n\n        tf = Counter(corpus[i].split())\n\n        for j in range(len(terms)):\n\n            df = len([document for document in corpus if terms[j] in document])\n\n            idf = 1.0 + np.log((len(corpus) + 1) / (df + 1))\n\n            mat[i, j] = tf[terms[j]] * idf\n\n    return (terms, mat)\n\ncorpus = ['this is the first document',\n          'this document is the second document',\n          'this one is the third']\n\n# manual calculation\nvectorizer_1 = tfidf_vectorizer(corpus)\n\nterms_1 = vectorizer_1[0]\nmatrix_1 = vectorizer_1[1]\n\n# scikit-learn calculation\nvectorizer_2 = TfidfVectorizer(norm=None).fit(corpus)\n\nterms_2 = vectorizer_2.get_feature_names()\nmatrix_2 = vectorizer_2.transform(corpus).toarray()\n\nprint(pd.DataFrame(data=matrix_1, columns=terms_1))\n\n   document     first   is       one    second  the     third  this\n0  1.287682  1.693147  1.0  0.000000  0.000000  1.0  0.000000   1.0\n1  2.575364  0.000000  1.0  0.000000  1.693147  1.0  0.000000   1.0\n2  0.000000  0.000000  1.0  1.693147  0.000000  1.0  1.693147   1.0\n\nprint(pd.DataFrame(data=matrix_2, columns=terms_2))\n\n   document     first   is       one    second  the     third  this\n0  1.287682  1.693147  1.0  0.000000  0.000000  1.0  0.000000   1.0\n1  2.575364  0.000000  1.0  0.000000  1.693147  1.0  0.000000   1.0\n2  0.000000  0.000000  1.0  1.693147  0.000000  1.0  1.693147   1.0\n"
'# Create an Image Data Generator to input later into our model.\ndata_generation = ImageDataGenerator(\n    # Use Inception ResNet v2\n    preprocessing_function=tf.keras.applications.inception_resnet_v2.preprocess_input\n)\n\n'
'my_y = np.array([2,5,6,10]).reshape(-1, 1)\nmy_x = np.array([19,23,22,30]).reshape(-1, 1)\n\nlm = sk.LinearRegression()\nlm = lm.fit(my_x, my_y)\nresult = lm.score(my_x, my_y)\nprint(result)\n\n0.9302407516147975\n'
'reg.predict([[1750, 1]])\n\n&gt;&gt;&gt; array([1.88])\n'
'# Custom loss\ndef loss(target_y, predicted_y):\n  return tf.reduce_mean(tf.square(target_y - predicted_y))\n\n# Define a training loop\ndef train(model, inputs, outputs, learning_rate):\n  with tf.GradientTape() as t:\n    current_loss = loss(outputs, model(inputs))\n  dW, db = t.gradient(current_loss, [model.W, model.b])\n  model.W.assign_sub(learning_rate * dW)\n  model.b.assign_sub(learning_rate * db)\n\nmodel = Model()\n\nWs, bs = [], []\nepochs = range(10)\nfor epoch in epochs:\n  Ws.append(model.W.numpy())\n  bs.append(model.b.numpy())\n  current_loss = loss(outputs, model(inputs))\n\n  train(model, inputs, outputs, learning_rate=0.1)\n  print(\'Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f\' %\n        (epoch, Ws[-1], bs[-1], current_loss))\n\n#Create the model \nmodel = tf.keras.Sequential([\n  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n  tf.keras.layers.Dense(3)\n])\n\n# You can define your own loss function here\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ndef loss(model, x, y, training):\n  # training=training is needed only if there are layers with different\n  # behavior during training versus inference (e.g. Dropout).\n  y_ = model(x, training=training)\n\n  return loss_object(y_true=y, y_pred=y_)\n\n# Create here your gradient and optimizor\ndef grad(model, inputs, targets):\n  with tf.GradientTape() as tape:\n    loss_value = loss(model, inputs, targets, training=True)\n  return loss_value, tape.gradient(loss_value, model.trainable_variables)\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n\n# Training loop \ntrain_loss_results = []\ntrain_accuracy_results = []\n\nnum_epochs = 201\n\nfor epoch in range(num_epochs):\n  epoch_loss_avg = tf.keras.metrics.Mean()\n  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n\n  # Training loop - using batches of 32\n  for x, y in train_dataset:\n    # Optimize the model\n    loss_value, grads = grad(model, x, y)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    # Track progress\n    epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n    # Compare predicted label to actual label\n    # training=True is needed only if there are layers with different\n    # behavior during training versus inference (e.g. Dropout).\n    epoch_accuracy.update_state(y, model(x, training=True))\n\n  # End epoch\n  train_loss_results.append(epoch_loss_avg.result())\n  train_accuracy_results.append(epoch_accuracy.result())\n\n  if epoch % 50 == 0:\n    print("Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}".format(epoch,\n                                                                epoch_loss_avg.result(),\n                                                                epoch_accuracy.result()))\n'
'datagen = image.ImageDataGenerator(featurewise_center=True,\n                               featurewise_std_normalization=True)\ndatagen.mean = np.array([111.1, 222.2, 333.3], dtype=np.float32).reshape((1,1,3))\ndatagen.std = np.array([5.3, 4.2, 6.3], dtype=np.float32).reshape((1,1,3))\n'
'a -&gt; b\na b -&gt; c\na b c -&gt; d\n'
'def word_features(name):\n    features = {}\n    features["firstletter"] = name[0].lower()\n    features["lastletter"] = name[-1].lower()\n    for letter in \'abcdefghijklmnopqrstuvwxyz\':\n        features["count(%s)" % letter] = name.lower().count(letter)\n        features["has(%s)" % letter] = (letter in name.lower())\n    return features\n\n&gt;&gt; words = [(\'Rock\', \'Physical Object\'), (\'Play\', \'Action\'), ... ]\n&gt;&gt;&gt; featuresets = [(word_features(n), g) for (n,g) in words]\n&gt;&gt;&gt; train_set, test_set = featuresets[500:], featuresets[:500]\n&gt;&gt;&gt; classifier = nltk.NaiveBayesClassifier.train(train_set)\n\nnltk.classify.accuracy(classifier, test_set)\n\nclassifier.classify(word_features(\'Gold\'))\n'
'def Cost(theta, X, Y): \n  m = Y.size\n  Y = Y.flatten()\n  for i in range(X.shape[0]):\n  X[i]=X[i].flatten()\n\n  h = Sigmoid(X.dot(theta.T))\n  a = (-Y.T.dot(log(h)))\n  b = ((1.0 - Y.T).dot(log(1.0-h)))\n\nJ = (1.0/m) * ((-Y.T.dot(log(h))) - ((1.0 - Y.T).dot(log(1.0-h))))\n\nif math.isnan(b):\n  #case of overflow\n  b = -999999\n\nJ =  (a-b)/m\n\nreturn J/m \n'
'def show_most_informative_features(self, n=10):\n    strlist = []\n    # Determine the most relevant features, and display them.\n    cpdist = self._feature_probdist\n    # print(\'Most Informative Features\')\n    strlist.append(\'Most Informative Features\')\n\n    for (fname, fval) in self.most_informative_features(n):\n            def labelprob(l):\n                return cpdist[l,fname].prob(fval)\n            labels = sorted([l for l in self._labels\n                     if fval in cpdist[l,fname].samples()],\n                    key=labelprob)\n            if len(labels) == 1: continue\n            l0 = labels[0]\n            l1 = labels[-1]\n            if cpdist[l0,fname].prob(fval) == 0:\n                ratio = \'INF\'\n            else:\n                ratio = \'%8.1f\' % (cpdist[l1,fname].prob(fval) /\n                          cpdist[l0,fname].prob(fval))\n            # print((\'%24s = %-14r %6s : %-6s = %s : 1.0\' %\n            #      (fname, fval, ("%s" % l1)[:6], ("%s" % l0)[:6], ratio)))\n            strlist.append((\'%24s = %-14r %6s : %-6s = %s : 1.0\' %\n                          (fname, fval, ("%s" % l1)[:6], ("%s" % l0)[:6], ratio)))\n\n    return strlist\n\n# Useage\nlist = show_most_informative_features(classifier, 100)\nfile.writelines(list)\n'
"import ast\nimport glob\nimport os\n\ndef my_load_files(folder, pattern):\n    pathname = os.path.join(folder, pattern)\n    for filename in glob.glob(pathname):\n        with open(filename) as file:\n            yield ast.literal_eval(file.read())\n\ntext_folder = 'C:/Users/username/Desktop/Samples'\nprint [[' '.join(x) for x in sample]\n                        for sample in my_load_files(text_folder, 'File_*')]\n\nprint [[' '.join(x) for x in sample[:-1]]\n                        for sample in my_load_files(text_folder, 'File_*')]\n"
'result = clf.predict([140,85])\n\nfrom sklearn import neighbors\n\n# Define your X and y here\n\nclf = neighbors.KNeighborsClassifier(5)\nclf.fit(X, y)\nresult = clf.predict([140,85])\n'
'f(y) = 1/(1+e(-y)) = 1/(1+exp(-(-0.475347))) = 0.38335\n'
'embed_sequence(ids,\n              vocab_size=None,\n              embed_dim=None,\n              unique=False,\n              initializer=None,\n              regularizer=None,\n              trainable=True,\n              scope=None,\n              reuse=None)\n'
"h = tf.nn.conv2D(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\nW = [[0,1,0],[0,1,0],[0,1,0]]\n\nW = tf.Variable(tf.truncated_normal(shape, stddev=0.1)))\n\nW = tf.Variable(tf.truncated_normal(shape, stddev=0.1)),trainable=False)\n"
'train_data=np.delete(iris.data,test_index)\n\ntrain_data=np.delete(iris.data,test_index, axis=0)\n\nThe axis along which to delete the subarray defined by obj. \n\nIf axis is None, obj is applied to the flattened array.\n'
"# the initial encoding\nlevels=['level_1', 'level_2', 'level_3']\ndf_original = pd.DataFrame({'levels': levels, 'A': [0,1,2], 'B': [3,4,5]})\ndummies = pd.get_dummies(df_original.levels)\ndf = df_original.drop('levels', axis=1).join(dummies)\n# remember the levels and their order\ndummy_columns = list(dummies.columns)\n\n# encoding another dataframe\nnew_levels=['level_1', 'level_2', 'level_2']\nnew_df_original = pd.DataFrame({'levels': new_levels, 'A': [5,6,7], 'B': [8,9,7]})\n# this is where I use the remembered information\nnew_dummies = pd.get_dummies(new_df_original.levels). \\\n    reindex(columns=dummy_columns).fillna(0).astype(int)\nnew_df = new_df_original.drop('levels', axis=1).join(new_dummies)\nprint(new_df)\n\n   A  B  level_1  level_2  level_3\n0  5  8        1        0        0\n1  6  9        0        1        0\n2  7  7        0        1        0\n"
'def extraer_pesos(red, session):\n    pesos = []\n    biases = []\n    for i in range(1, len(red)):\n        pesos.append(session.run(tf.get_default_graph().get_tensor_by_name(\n            os.path.split(red[i].name)[0] + \'/kernel:0\')).tolist())\n        biases.append(session.run(tf.get_default_graph().get_tensor_by_name(\n            os.path.split(red[i].name)[0] + \'/bias:0\')).tolist())\n    return pesos, biases\n\n\ndef create_network(layout, funciones, pesos,biases):\n    capas = [(tf.placeholder("float", [None, layout[0]])]\n    for i in range(0, len(pesos)):\n        #There\'s an element already, so capas[i] is the previous layer\n        capas.append(tf.layers.dense(capas[i], layout[i+1], activation=funciones[i],\n                                            kernel_initializer=tf.constant_initializer(pesos[i], dtype=tf.float32),\n                                            bias_initializer=tf.constant_initializer(biases[i], dtype=tf.float32),\n                                             name="layer"+str(i)))\n\n    return capas\n'
"import itertools\nfrom nltk.metrics import edit_distance\n\nMAGIC_THRESHOLD = 1\n\ncity_names = df['city'].value_counts()\n\nfor name_a, name_b in itertools.combinations(city_names.index, 2):\n    if edit_distance(name_a, name_b) &lt;= MAGIC_THRESHOLD:\n        count_a, count_b = city_names[name_a], city_names[name_b]\n        main_name, mistake = (name_a, name_b) if count_a &gt; count_b else (name_b, name_a)\n        df = df.replace({mistake: main_name})\n\nprint(df['city'].value_counts())\n\nDelhi     4\nMumbai    3\n"
"check = [isinstance(v, k) for k in (list, tuple, np.ndarray)]\n\nparams = {'max_depth': np.arange(1,10)}\n\nparams = {'max_depth': [x for x in range(1,10)]}\n"
'from datetime import datetime\n\nt_str = "2:50"\n\nt_delta = datetime.strptime(t_str, "%H:%M") - datetime(1900, 1, 1)\nseconds = t_delta.total_seconds()\nhours = seconds/60**2\n\nprint(seconds)\n# 10200.0\n\nfrom datetime import timedelta  \n\nh, m = map(int, t_str.split(sep=\':\'))\nt_delta = timedelta(hours=h, minutes=m)\n\n# Get total number of seconds\nseconds = t_delta.total_seconds()\n'
'tflite_convert \\\n  --output_file=test2.lite \\\n  --graph_def_file=frozen_inference_graph_3mbvoc.pb \\\n  --input_arrays=sub_2 \\\n  --output_arrays=ResizeBilinear_2 \\\n  --input_shapes=1,450,600,3 \\\n  --inference_input_type=QUANTIZED_UINT8 \\\n  --inference_type=FLOAT \\\n  --mean_values=128 \\\n  --std_dev_values=128\n'
'&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n&gt;&gt;&gt; Y = np.array([1, 1, 1, 2, 2, 2])\n&gt;&gt;&gt; from sklearn.naive_bayes import GaussianNB\n&gt;&gt;&gt; clf = GaussianNB()\n&gt;&gt;&gt; clf.fit(X, Y)\nGaussianNB(priors=None, var_smoothing=1e-09)\n&gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))\n[1]\n&gt;&gt;&gt; clf_pf = GaussianNB()\n&gt;&gt;&gt; clf_pf.partial_fit(X, Y, np.unique(Y))\nGaussianNB(priors=None, var_smoothing=1e-09)\n&gt;&gt;&gt; print(clf_pf.predict([[-0.8, -1]]))\n[1]\n'
"dir(su)\n\n&gt;&gt;&gt; dir(su)\n['Bunch', 'DataConversionWarning', 'IS_PYPY', 'Memory', 'Parallel', 'Sequence', \n'_IS_32BIT', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', \n'__loader__', '__name__', '__package__', '__path__', '__spec__', '_joblib', \n'_show_versions', 'as_float_array', 'assert_all_finite', 'axis0_safe_slice', \n'check_X_y', 'check_array', 'check_consistent_length', 'check_random_state', \n'check_symmetric', 'class_weight', 'column_or_1d', 'compute_class_weight', \n'compute_sample_weight', 'cpu_count', 'delayed', 'deprecate', 'deprecated', \n'deprecation', 'effective_n_jobs', 'fixes', 'gen_batches', 'gen_even_slices', \n'get_chunk_n_rows', 'get_config', 'hash', 'indexable', 'indices_to_mask', \n'is_scalar_nan', 'issparse', 'msg', 'murmurhash', 'murmurhash3_32', 'np', \n'numbers', 'parallel_backend', 'platform', 'register_parallel_backend', \n'resample', 'safe_indexing', 'safe_mask', 'safe_sqr', 'shuffle', 'struct', \n'tosequence', 'validation', 'warnings']\n\npip install -U scikit-learn\n"
"resid = y_test- y_pred.to_frame('price')\n"
"last_layer = pre_trained_model.get_layer('mixed7')\nlast_output = last_layer.output\n\n\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(1024, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense  (1, activation='sigmoid')(x)           \n\nmodel = Model( pre_trained_model.input, x) \n"
'model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding="same", activation="relu",input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2),strides=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), padding="same", activation="relu"))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), padding="same", activation="relu"))\n\n#Upsampling\nmodel.add(UpSampling2D(size=(2,2),interpolation=\'nearest\'))\nmodel.add(UpSampling2D(size=(2,2),interpolation=\'nearest\'))\n\nmodel.add(Dense(8, activation=\'relu\'))\nmodel.summary()\n\n\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 2004, 2752, 32)    1760      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 1002, 1376, 32)    0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 1002, 1376, 64)    18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 501, 688, 64)      0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 501, 688, 128)     73856     \n_________________________________________________________________\nup_sampling2d_1 (UpSampling2 (None, 1002, 1376, 128)   0         \n_________________________________________________________________\nup_sampling2d_2 (UpSampling2 (None, 2004, 2752, 128)   0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 2004, 2752, 8)     1032      \n=================================================================\nTotal params: 95,144\nTrainable params: 95,144\nNon-trainable params: 0\n\nif x_train3d.shape[2] % 2:\n    x_train3d_adj =  x_train3d_adj[:,:,:-1,:]\n    y_train3d_adj =  y_train3d_adj[:,:,:-1,:]\n'
'1.91629073 =&gt; 5/2\n1.22314355 =&gt; 5/4\n1.51082562 =&gt; 5/3\n1 =&gt; 5/5\n'
"from sklearn import datasets\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, f1_score\n\niris = datasets.load_iris()\nX, y = iris.data, iris.target\nmodel = OneVsRestClassifier(LinearSVC(random_state=0))\nmodel.fit(X, y)\nyhat = model.predict(X)\n\nprint('Accuracy:', accuracy_score(y, yhat))\nprint('F1:', f1_score(y, yhat, average='micro'))\n"
'Mean Squared Error: mean_squared_error, MSE or mse\nMean Absolute Error: mean_absolute_error, MAE, mae\nMean Absolute Percentage Error: mean_absolute_percentage_error, MAPE, mape\nCosine Proximity: cosine_proximity, cosine\n'
"model = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(1)\n])\n"
"model = Sequential()\nmodel.add(Conv1D(200, kernel_size=3, activation = useSomething, input_shape=(25,3)))\nmodel.add(LSTM(200))\nmodel.add(Dense(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel = Sequential()\nmodel.add(LSTM(200, return_sequences=True, input_shape=(25,3)))\nmodel.add(Conv1D(200, kernel_size=3, activation = useSomething))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel = Sequential()\nmodel.add(Conv1D(15, kernel_size=3, activation = useSomething, input_shape=(25,3)))\nmodel.add(LSTM(30, return_sequences=True))\nmodel.add(Conv1D(70, kernel_size=3, activation = useSomething))\n............\nmodel.add(LSTM(100))\nmodel.add(Dense(100))\nmodel.add(Dense(1, activation='sigmoid'))\n\ninputs = Input((25,3))\n\nside1 = Bidirectional(LSTM(100, return_sequences=True))(inputs) #200 total units\nside2 = Conv1D(200, activation = 'tanh', padding = 'same')(inputs) #same activation \n                                                                   #same length\n\nmerged = Add()([side1, side2]) \n     #or Concatenate()([side1, side2]) if different number of units/channels/features\n\noutputs = Conv1D(200)(merged)\noutputs = GlobalMaxPooling1D()(outputs)\noutputs = Dense(100)(outputs)\noutputs = Dense(1, activation='sigmoid')(outputs)\n\nmodel = Model(inputs, outputs)\n"
"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, UpSampling2D\nimport numpy as np\n\ninput = np.random.rand(10, 30, 30)\n\ninput = input[..., None] # keras needs 4D input, so add 1 dimension\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), input_shape=(30, 30, 1), activation='relu', padding='same'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\nmodel.add(UpSampling2D((2, 2)))\nmodel.add(Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n\nmodel.fit(input, input, batch_size=8, epochs=1)\n"
"e=df.groupby(['Country','City'])['Rating'].mean()\n\npd.DataFrame(e)\n"
"fig = plt.figure(figsize=(16,8))\nplt.title('Stock Price History')\nplt.plot(df['Date'], df['Close'])\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Stock Price', fontsize = 18)\nax = plt.axes()\nax.set_xticks(ax.get_xticks()[::250])\nplt.show()\n"
"prediction = pd.DataFrame(model.predict_classes([X_train_1,X_train_2])) \nprediction[prediction['label_1'] == '1'].shape[0]/921\n\nnp.argmax(model.predict([X_train_1,X_train_2]), axis=1)\n"
'robo_response = robo_response+return_response\n\nrobo_response = robo_response+user_response\n'
'import contractions\ncontractions.fix("you\'re happy now")\n# "you are happy now"\n\nfrom contractions import contractions_dict\n\n{..., \'you’ll\': \'you will\', ...}\n'
"keras.layers.Dense(1) # linear activation by default\n\nkeras.layers.Dense(1, activation='sigmoid')\n\nloss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n"
'key = lambda e: -abs(e[1]))\n\n&gt;&gt;&gt; X_crime = [3,1,2]\n&gt;&gt;&gt; linlasso_coef_ = [9,10,11]\n\n&gt;&gt;&gt; def abs(n):\n...    return n*n\n\n&gt;&gt;&gt; list(zip(list(X_crime), linlasso_coef_))\n[(3, 9), (1, 10), (2, 11)]\n&gt;&gt;&gt; sorted(list(zip(list(X_crime), linlasso_coef_)))\n[(1, 10), (2, 11), (3, 9)]\n\n&gt;&gt;&gt; \n&gt;&gt;&gt; e = (1,10)\n&gt;&gt;&gt; e[1]\n10\n&gt;&gt;&gt; abs(e[1])\n100\n&gt;&gt;&gt; -abs(e[1])\n-100\n\n&gt;&gt;&gt; lambda e: -abs(e[1])\n&lt;function &lt;lambda&gt; at 0x00000213E08538B0&gt;\n\n&gt;&gt;&gt; key = lambda e: -abs(e[1])\n&gt;&gt;&gt; key\n&lt;function &lt;lambda&gt; at 0x00000213E0853F70&gt;\n\n&gt;&gt;&gt; key(e)\n-100\n&gt;&gt;&gt;\n'
"X, y = df['data'], df['label']\nmetrics = []\n\nskf = StratifiedKFold(n_splits=5)\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    vect = CountVectorizer(ngram_range=(1,2), max_features=1000 , stop_words=&quot;English&quot;)\n    X_train_dtm = vect.fit_transform(X_train)\n    X_test_dtm = vect.transform(X_test)\n    nb = MultinomialNB()\n    nb.fit(X_train_dtm, y_train)\n    y_pred_class = nb.predict(X_test_dtm)\n\n    metrics.append(accuracy_score(y_test, y_pred_class))\n\nmetrics = numpy.array(metrics)\nprint('Mean accuracy: ', numpy.mean(metrics, axis=0))\nprint('Std for accuracy: ', numpy.std(metrics, axis=0))\n"
"x = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \n              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=np.float32)\nx = x.reshape(1, x.shape[0], x.shape[1])\n\nmodel = Sequential()\nmodel.add(layers.Conv1D(filters=10, kernel_size=2)) \nmodel.add(layers.MaxPooling1D(pool_size=2, strides=1))\nmodel.add(layers.Dense(1))\nmodel.add(layers.Activation(activation='softmax'))\nmodel.build((None, None, 10))\n\nmodel.predict(x)\n"
'start_xy = np.array(start_xy)\nstart_xy = start_xy.reshape(*start_xy.shape, 1)\n\nmodel.add(LSTM(64, return_sequences=False, input_shape=start_xy.shape[1:]))\n'
'# y_score typically has many tied values. Here we extract\n# the indices associated with the distinct values. We also\n# concatenate a value for the end of the curve.\ndistinct_value_indices = np.where(np.diff(y_score))[0]\nthreshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n# Attempt to drop thresholds corresponding to points in between and\n# collinear with other points. These are always suboptimal and do not\n# appear on a plotted ROC curve (and thus do not affect the AUC).\n# Here np.diff(_, 2) is used as a &quot;second derivative&quot; to tell if there\n# is a corner at the point. Both fps and tps must be tested to handle\n# thresholds with multiple data points (which are combined in\n# _binary_clf_curve). This keeps all cases where the point should be kept,\n# but does not drop more complicated cases like fps = [1, 3, 7],\n# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\nif drop_intermediate and len(fps) &gt; 2:\n    optimal_idxs = np.where(np.r_[True,\n                                  np.logical_or(np.diff(fps, 2),\n                                                np.diff(tps, 2)),\n                                  True])[0]\n    fps = fps[optimal_idxs]\n    tps = tps[optimal_idxs]\n    thresholds = thresholds[optimal_idxs]\n'
"import tensorflow as tf\nimport numpy as np\n\nds_x = tf.data.Dataset.from_tensor_slices(np.random.randn(5, 5).astype(np.float32))\n\nds_y = tf.data.Dataset.from_tensor_slices({'l1': np.arange(5), 'l2':np.arange(5)})\n\nds = tf.data.Dataset.zip((ds_x, ds_y)).batch(2)\n\ninput_ = tf.keras.Input(shape=[5])\nx = tf.keras.layers.Dense(30, activation='relu')(input_)\nx1 = tf.keras.layers.Dense(5, activation='softmax')(x)\nx2 = tf.keras.layers.Dense(5, activation='softmax')(x)\nmodel = tf.keras.Model(inputs=input_, outputs={'l1':x1, 'l2':x2})\n\ndef model_loss(y, y_):\n    res = 3 * tf.losses.SparseCategoricalCrossentropy()(y['l1'], y_['l1'])\n    res += tf.losses.SparseCategoricalCrossentropy()(y['l2'], y_['l2'])\n    return res\n\ntrain_loss = tf.keras.metrics.Mean()\noptimizer = tf.keras.optimizers.Adam()\n\nfor i in range(25):\n    for x, y in ds:\n        with tf.GradientTape() as tape:\n            out = model(x)\n            loss = model_loss(y, out)\n            \n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        train_loss(loss)\n    print(f'Epoch {i} Loss: {train_loss.result():=4.4f}')\n    train_loss.reset_states()\n\nEpoch 0 Loss: 6.4170\nEpoch 1 Loss: 6.3396\nEpoch 2 Loss: 6.2737\nEpoch 11 Loss: 5.7191\nEpoch 12 Loss: 5.6608\nEpoch 19 Loss: 5.2646\nEpoch 24 Loss: 4.9896\n"
"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmean_squared_error(df_test['Field1'], df_results['Field1_f'])  # MSE\n\nmean_squared_error(df_test['Field1'], df_results['Field1_f'],\n                                      squared=False)           # RMSE\n\nmean_absolute_error(df_test['Field1'], df_results['Field1_f']) # MAE\n"
'import tensorflow as tf\nY = tf.concat((np.copy(X[1:,:]),np.zeros((1,12))), axis=0)\nX = tf.convert_to_tensor(X)\n'
'testingInputs = testingInputs.reshape(testingInputs.shape[0], 1, testingInputs.shape[1])\n'
'X = X.dropna()\n'
'import pandas as pd\n#importing data\ndj = pd.read_json(&quot;frames2.json&quot;)\ndtext = dj[[&quot;user_id&quot;,&quot;turns&quot;]]\n#Saving text records in a list\nlist_ = []\nfor record in dtext[&quot;turns&quot;].values:\n  for r  in record:\n    list_.append(r[&quot;text&quot;])\n#Exporting the csv\nout = pd.Series(list_,name=&quot;text&quot;)\nout.to_csv(&quot;text.csv&quot;)\n'
'from sklearn.metrics import plot_confusion_matrix\n'
'predicted = [[index, x] for index, x in enumerate(rf.predict(test), start = 1)]\n'
"number of documents above the threshold tagged 1\n------------------------------------------------\n    number of documents above the threshold\n\nnumber of documents above the threshold tagged 1\n------------------------------------------------\n         number of documents tagged 1\n\n1.0 1\n1.0 1\n1.0 1\n1.0 1\n0.99    1\n0.99    1\n0.99    1\n0.989   1\n0.944   1\n0.944   1 TH=0.944 #1's=10; #0's=0\n0.941   1\n0.941   1\n0.941   1\n0.941   1\n0.941   0 TH=0.941 #1's=14; #0's=1\n0.934   0\n0.933   0\n0.933   1 TH=0.933 #1's=15; #0's=3\n0.88    1 TH=0.880 #1's=16; #0's=3\n0.784   0\n0.727   0\n0.727   0\n0.714   0\n0.714   1\n0.714   0\n0.714   0 TH=0.714 #1's=17; #0's=9\n0.711   0\n0.711   0\n0.707   0\n0.707   0\n0.696   0\n0.696   0\n0.696   0\n0.696   0\n\nTH = 0.944\n    precision = 10/10       = 1.000\n    recall = 10/17          = 0.588\nTH = 0.941\n    precision = 14/15       = 0.933\n    recall = 14/17          = 0.824\nTH = 0.933\n    precision = 15/18       = 0.833\n    recall = 15/17          = 0.882\nTH = 0.880\n    precision = 16/19       = 0.842\n    recall = 16/17          = 0.941\nTH = 0.714\n    precision = 17/26       = 0.654\n    recall = 17/17          = 1.000\n\nF1 = 2 * precision * recall\n         ------------------\n         precision + recall\n\nTH = 0.944   F1 = 2*1.000*0.588/1.000+0.588 = 0.741\nTH = 0.941   F1 = 2*0.933*0.824/0.933+0.824 = 0.875\nTH = 0.933   F1 = 2*0.833*0.882/0.833+0.882 = 0.857\nTH = 0.880   F1 = 2*0.842*0.941/0.842+0.941 = 0.889\nTH = 0.714   F1 = 2*0.654*1.000/0.654+1.000 = 0.791\n"
"import pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\ndf = pd.read_csv('/home/Jian/Downloads/data.csv', index_col=[0])\n\n# get part of the data\nx = df.loc[df.House==1, 'Daylight']\n# hp-filter, set parameter lamb=129600 following the suggestions for monthly data\nx_smoothed, x_trend = sm.tsa.filters.hpfilter(x, lamb=129600)\nfig, axes = plt.subplots(figsize=(12,4), ncols=3)\naxes[0].plot(x)\naxes[0].set_title('raw x')\naxes[1].plot(x_trend)\naxes[1].set_title('trend')\naxes[2].plot(x_smoothed)\naxes[2].set_title('smoothed x')\n"
'pred = clf.predict(features)\nfor sample, label in zip(features, pred):\n  print sample, label  \n\nvectorizer = CountVectorizer( ... )\n\n... = vectorizer.fit_transform( ... ) \n\npred = clf.predict(features)\nfor sample, label in zip(features, pred):\n  print vectorizer.inverse_transform(np.array([sample])), label  \n'
"skflow.TensorFlowDNNRegressor.__init__(\n     hidden_units,\n     n_classes=0,\n     tf_master='',\n     batch_size=32,\n     steps=200,\n     optimizer='SGD',\n     learning_rate=0.1,\n     tf_random_seed=42,\n     continue_training=False,\n     config_addon=None,\n     verbose=1,\n     max_to_keep=5,\n     keep_checkpoint_every_n_hours=10000)\n"
'fit(X)\n\nfit(X, y=None, sample_weight=None)\n'
'from sklearn.linear_model import LogisticRegression as expit\nimport numpy as np\n\ndef constructVariations(X, deg):\n\n    features = np.zeros((len(X), 27)) \n    spot = 0\n\n    for i in range(1, deg + 1):\n        for j in range(i + 1):\n\n            features[:, spot] = X[:,0]**(i - j) * X[:,1]**(j)\n            spot += 1\n\n    return features\n\nif __name__ == \'__main__\':\n    data = np.loadtxt("ex2points.txt", delimiter = ",")\n    X,Y = np.split(data, [len(data[0,:]) - 1], 1)\n    rawX = np.copy(X)    \n    X = constructVariations(X, 6)\n\n    oneArray = np.ones((len(X),1))\n    X = np.hstack((oneArray, X))\n    trial = expit(solver = \'sag\')\n    trial = trial.fit(X = X,y = np.ravel(Y))\n    print(trial.coef_)\n\n    from matplotlib import pyplot as plt\n\n    h = 0.01\n    x_min, x_max = rawX[:, 0].min() - 1, rawX[:, 0].max() + 1\n    y_min, y_max = rawX[:, 1].min() - 1, rawX[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    data = constructVariations(np.c_[xx.ravel(), yy.ravel()], 6)\n    oneArray = np.ones((len(data),1))\n    data = np.hstack((oneArray, data))\n    Z = trial.predict(data)\n    Z = Z.reshape(xx.shape)\n\n    plt.figure()\n    plt.scatter(rawX[:, 0], rawX[:, 1], c=Y, linewidth=0, s=50)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n    plt.show()\n\ndef thetaFunc(y, theta, x):\n\n    deg = 6\n\n    spot = 0\n    sum = theta[spot]\n\n    spot += 1\n    for i in range(1, deg + 1):\n        for j in range(i + 1):\n            sum += theta[spot] * x**(i - j) * y**(j)\n            spot += 1\n    return sum\n\nxx,yy = np.meshgrid(x,y)\nfor i in range(len(x)):\n     for j in range(len(y)):\n         z[i][j] = thetaFunc(yy[i][j], theta, xx[i][j])\nz -= trial.intercept_\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression as expit\n\ndef thetaFunc(y, theta, x):\n\n    deg = 6\n\n    spot = 0\n    sum = theta[spot]\n\n    spot += 1\n    for i in range(1, deg + 1):\n        for j in range(i + 1):\n            sum += theta[spot] * x**(i - j) * y**(j)\n            spot += 1\n    return np.exp(-sum)\n\n\ndef constructVariations(X, deg):\n\n    features = np.zeros((len(X), 27)) \n    spot = 0\n\n    for i in range(1, deg + 1):\n        for j in range(i + 1):\n\n            features[:, spot] = X[:,0]**(i - j) * X[:,1]**(j)\n            spot += 1\n\n    return features\n\nif __name__ == \'__main__\':\n    data = np.loadtxt("ex2points.txt", delimiter = ",")\n    X,Y = np.split(data, [len(data[0,:]) - 1], 1)\n\n    X = constructVariations(X, 6)\n    rawX = np.copy(X)\n\n    oneArray = np.ones((len(X),1))\n    X = np.hstack((oneArray, X))\n    trial = expit(solver = \'sag\')\n    trial = trial.fit(X = X,y = np.ravel(Y))\n\n    from matplotlib import pyplot as plt\n\n    theta = trial.coef_.ravel()\n\n    x = np.linspace(-1, 1.5, 100)\n    y = np.linspace(-1,1.5,100)\n    z = np.empty((100,100))\n\n\n    xx,yy = np.meshgrid(x,y)\n    for i in range(len(x)):\n         for j in range(len(y)):\n             z[i][j] = thetaFunc(yy[i][j], theta, xx[i][j])\n    z -= trial.intercept_\n\n    plt.contour(xx,yy,z &gt; 1,cmap=plt.cm.Paired, alpha=0.8)\n    plt.scatter(rawX[:, 0], rawX[:, 1], c=Y, linewidth=0, s=50)\n    plt.show()\n'
'partial_fit(X, y[, classes, sample_weight]) Fit linear model with Stochastic Gradient Descent.\n'
'    def reshape_data(self, X, n):\n    """\n    Reshape a data set of N time series samples of T time steps each\n    Args:\n        data: Time series data of shape (N,T,D)\n        n: int, length of time window used to predict x[t+1]\n\n    Returns:\n\n    """\n    N,T,D = X.shape\n\n    x = np.zeros((N*(T-n),n,D))\n    y = np.zeros((N*(T-n),D))\n\n    for i in range(T-n):\n        x[N*i:N*(i+1),:,:] = X[:,i:i+n,:]\n        y[N*i:N*(i+1),:] = X[:,i+n,:]\n\n    return x,y\n'
"new = df['Has_Arrears'] == 1\na = df[new]\ndf = df.append([a]*35, ignore_index=True)\n"
"X = pickle.load(open('dataset.pkl', 'rb')).astype('float32')\nY = pickle.load(open('dataset.pkl', 'rb')).astype('float32')\n"
"In [62]: M=(sparse.rand(10,3,.3,'csr')*10).astype(int)\nIn [63]: M\nOut[63]: \n&lt;10x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 9 stored elements in Compressed Sparse Row format&gt;\nIn [64]: M.A\nOut[64]: \narray([[0, 7, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 5],\n       [0, 0, 2],\n       [0, 0, 6],\n       [0, 4, 4],\n       [7, 1, 0],\n       [0, 0, 2]])\n\nIn [65]: np.array_split(M.A, 3)\nOut[65]: \n[array([[0, 7, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]]), array([[0, 0, 5],\n        [0, 0, 2],\n        [0, 0, 6]]), array([[0, 4, 4],\n        [7, 1, 0],\n        [0, 0, 2]])]\n\nIn [143]: alist = [M[0:4,:], M[4:7,:], M[7:10]]\nIn [144]: alist\nOut[144]: \n[&lt;4x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 1 stored elements in Compressed Sparse Row format&gt;,\n &lt;3x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 3 stored elements in Compressed Sparse Row format&gt;,\n &lt;3x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 5 stored elements in Compressed Sparse Row format&gt;]\nIn [145]: [m.A for m in alist]\nOut[145]: \n[array([[0, 7, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]], dtype=int32), array([[0, 0, 5],\n        [0, 0, 2],\n        [0, 0, 6]], dtype=int32), array([[0, 4, 4],\n        [7, 1, 0],\n        [0, 0, 2]], dtype=int32)]\n\nIn [146]: idx = [0,4,7,10]\nIn [149]: alist = []\nIn [150]: for i in range(len(idx)-1):\n     ...:     alist.append(M[idx[i]:idx[i+1]])   \n\nIn [160]: [M[i:i+5,:] for i in range(0,M.shape[0],5)]\nOut[160]: \n[&lt;5x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 2 stored elements in Compressed Sparse Row format&gt;,\n &lt;5x3 sparse matrix of type '&lt;class 'numpy.int32'&gt;'\n    with 7 stored elements in Compressed Sparse Row format&gt;]\n"
'row = tf.placeholder(tf.int32, shape=[10])\n\ncondition = tf.logical_and(\n    tf.equal(row[0], tf.reduce_max(row)),\n    tf.less(row[0], 4))\n\nsess = tf.Session()\n\nprint sess.run(condition, feed_dict={row: [6, -2, -2, -2, -1, -2, -3, -3, -6, -6]}) \nprint sess.run(condition, feed_dict={row: [1, -6, -7, -7, -7, -7, -7, -6, -6, -6]})\nprint sess.run(condition, feed_dict={row: [5, -3, -3, -4, -4, -4, -4, -3, -3, -3]})\n\n# Prints the following:\n# False\n# True\n# False\n\ndef swap_first_two(x):\n  swapped_first_two = tf.stack([x[1], x[0]])\n  rest = x[2:]\n  return tf.concat([swapped_first_two, rest], 0)\n\nmaybe_swapped = tf.cond(condition, lambda: swap_first_two(row), lambda: row)\n\nprint sess.run(maybe_swapped, feed_dict={row: [6, -2, -2, -2, -1, -2, -3, -3, -6, -6]}) \nprint sess.run(maybe_swapped, feed_dict={row: [1, -6, -7, -7, -7, -7, -7, -6, -6, -6]})\nprint sess.run(maybe_swapped, feed_dict={row: [5, -3, -3, -4, -4, -4, -4, -3, -3, -3]})\n\n# Prints the following:\n# [ 6 -2 -2 -2 -1 -2 -3 -3 -6 -6]\n# [-6  1 -7 -7 -7 -7 -7 -6 -6 -6]\n# [ 5 -3 -3 -4 -4 -4 -4 -3 -3 -3]\n\nmatrix = tf.constant([\n    [6, -2, -2, -2, -1, -2, -3, -3, -6, -6],\n    [1, -6, -7, -7, -7, -7, -7, -6, -6, -6],\n    [5, -3, -3, -4, -4, -4, -4, -3, -3, -3],\n])\n\ndef row_function(row):\n  condition = tf.logical_and(\n      tf.equal(row[0], tf.reduce_max(row)),\n      tf.less(row[0], 4))\n\n  def swap_first_two(x):\n    swapped_first_two = tf.stack([x[1], x[0]])\n    rest = x[2:]\n    return tf.concat([swapped_first_two, rest], 0)\n\n  maybe_swapped = tf.cond(condition, lambda: swap_first_two(row), lambda: row)\n\n  return maybe_swapped\n\nresult = tf.map_fn(row_function, matrix)\n\nprint sess.run(result)\n\n# Prints the following:\n# [[ 6 -2 -2 -2 -1 -2 -3 -3 -6 -6]\n#  [-6  1 -7 -7 -7 -7 -7 -6 -6 -6]\n#  [ 5 -3 -3 -4 -4 -4 -4 -3 -3 -3]]\n'
"pipe = Pipeline([('vectorizer', DictVectorizer()),\n                 ('scaler', StandardScaler(with_mean=False)),\n                 ('mutual_info', feat_sel),\n                 ('logistregress', clf)])\n\ny_pred = model_selection.cross_val_predict(pipe, instances, y, cv=10)\n"
'from os import path\nfile_path = "project/Train/ball/01.jpg"\ndir_name = path.basename(path.dirname(file_path))  # "ball"\n'
"def fit(self, X, y=None):\n    if self.strategy == 'most_frequent':\n        self.fills = pd.DataFrame(X).mode(axis=0).squeeze()\n\n        # Removed .values from the below line\n        self.statistics_ = self.fills\n        return self\n"
'cross_validation.train_test_split(x, y, test_size=0.2)\n'
"transformed_trans = tf.feature_column.categorical_column_with_hash_bucket('trans', 300)\n"
'variables = locals()\nfor i in list(range(1,11)):\n    variables["y{0}".format(i)]= variables["train{0}".format(i)][\'Labels\']\n    variables["x{0}".format(i)]= variables["train{0}".format(i)].drop(\'Labels\',1)\n    clf.fit(variables["x{0}".format(i)], variables["y{0}".format(i)])\n    variables["output{0}".format(i)]= clf.predict(variables["x{0}".format(i)], variables["y{0}".format(i)])\n\nvariables = locals()\nfor i in list(range(1,11)):\n    y= variables["train{0}".format(i)][\'Labels\']\n    x= variables["train{0}".format(i)].drop(\'Labels\',1)\n    clf.fit(x,y)\n    variables["output{0}".format(i)]= clf.predict(x,y)\n'
'train_X = (train_X - numpy.mean(train_X)) / (numpy.amax(train_X) - numpy.amin(train_X))\ntrain_Y = (train_Y - numpy.mean(train_Y)) / (numpy.amax(train_Y) - numpy.amin(train_Y))`\n'
"import pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n#importing dataset\ndataset = pd.read_csv('Salary_Data.csv')\nx = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 1].values\n\n#spliting the dataset into training and test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, \ntest_size=1/3, random_state=0)\n"
'TypeError: can\'t multiply sequence by non-int of type \'float\'\n\n&gt;&gt;&gt; [1, 0] * 3\n[1, 0, 1, 0, 1, 0]\n\n&gt;&gt;&gt; [1, 0] * 3.14\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nTypeError: can\'t multiply sequence by non-int of type \'float\'\n'
"def as_int_list(line):\n    return [int(i) for i in line.strip().split()]\n\n\ndef read_test_case(filehandle):\n    n, c, d = tuple(as_int_list(fh.readline()))\n    m = []\n    while len(m) &lt; n:\n        m.append(as_int_list(fh.readline()))\n    yield (n, c, d, m)\n\n\nif __name__ == '__main__':\n    localfile = 'testcases.txt'\n\n    no = 0\n    with open(localfile, 'r') as fh:\n        while no &lt; 5:\n            case = read_test_case(fh).next()\n            print(case)\n            no += 1\n"
"vectorizer.fit_transform(test['message'].values)\n\nvectorizer.transform(test['message'].values)\n"
'import numpy as np\nfrom sklearn.naive_bayes import BernoulliNB\n\nfeature_word_list = ["cat", "dog", "fridge", "car", "rabbit"]\nfeature1 = [0, 1, 0, 0, 0]\nfeature2 = [1, 0, 0, 0, 0]\nfeature3 = [0, 0, 1, 0, 0]\nfeature4 = [0, 0, 0, 1, 0]\nfeature5 = [1, 1, 0, 0, 1]\nclass_name_list = [1, 1, 0, 0, 1]\n\ntrain_features = np.array([feature1,feature2,feature3,feature4,feature5]).T\n\nclf = BernoulliNB()\nclf.fit(train_features, class_name_list)\n\ntest_data = ["this is about dog and cats","not animal related sentence"]\ntest_feature_list = []\nfor test_instance in test_data:\n  test_feature = [1 if feature_word in test_instance else 0 for feature_word in feature_word_list]\n  test_feature_list.append(test_feature) \n\ntest_feature_matrix = np.array(test_feature_list)\nprint(test_feature_matrix)\n\n[[1 1 0 0 0]\n [0 0 0 0 0]]\n\npredicted_label_list = clf.predict(test_feature_matrix)\nprint(predicted_label_list)\n\n[1 0]\n'
'&gt;&gt;&gt; a = [currentLane, offRoad, collision, lane1, lane2, lane3, reward, a_action]\n&gt;&gt;&gt; a = np.array(a) # convert to a numpy array\n&gt;&gt;&gt; a = np.expand_dims(a, 0) # change shape from (8,) to (1,8)\n&gt;&gt;&gt; model.predict(a) # voila!\n'
'sess.run(tf.reduce_mean(tf.abs(tf.where(tf.is_nan(x), tf.zeros_like(x), x)-y)))\n'
'from sklearn.metrics import make_scorer, f1_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\n\ndef relabeled_f1_score(y_true, y_pred):\n    y_pred_c = y_pred.copy()\n    y_pred_c[y_pred_c == 1] = 0\n    y_pred_c[y_pred_c == -1] = 1\n    return f1_score(y_true=y_true, y_pred=y_pred_c)\n\nn_samples = 1000\nn_features = 40\n\nX, _ = make_blobs(n_samples=n_samples, n_features=n_features)\ny = np.random.choice([0, 1], n_samples)  # 1 = outlier, 0 = inliner\n\nparam_grid = {\n    "n_estimators": [50, 200, 800],\n    "max_samples": [1000, 4000, 16000, 64000, 120000],\n    "max_features": [1, 5, 15, 30],\n    "contamination": [0.001, 0.1, 0.2, 0.5]\n}\n\ncustom_scorer = make_scorer(score_func=relabeled_f1_score, greater_is_better=True)\nmy_rs = RandomizedSearchCV(IsolationForest(), param_distributions=param_grid, scoring=custom_scorer, verbose=3)\n\nmy_rs.fit(X, y)\n'
'from sklearn.metrics import accuracy_score\n\naccuracy_score(val_predictions, val_y)\n'
"print(new_model.layers[2].get_layer('bn5c_branch2c').trainable) # output: False\n\n# save it\nnew_model.save('my_new_model.hd5')\n\n# load it again\nnew_model = load_model('my_new_model.hd5')\n\nprint(new_model.layers[2].get_layer('bn5c_branch2c').trainable) # output: False\n"
'for i in range(10000):\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n'
'from keras.models import Model\nfrom keras.layers import Input, Dense, concatenate\n\nwords_in = Input((10,))\nwords = Dense(10, activation=\'softmax\')(words_in)\n\ntexts_in = Input((10,))\ntexts = Dense(10, activation=\'softmax\')(texts_in)\n\nconcat = concatenate([words, texts])\n\ncbow = Model(inputs=[words_in, texts_in], output=concat)\n\ncbow.compile(loss="categorical_crossentropy", optimizer="adagrad")\n'
"model.add(LSTM(100, activation = 'relu', return_sequences=True))\n\nmodel.add(Conv1D(64, kernel_size = 5, activation = 'relu'))\nmodel.add(MaxPooling1D())\nmodel.add(LSTM(100, activation = 'relu'))\n"
'model.fit(X_train, [Y_train[:,0:1], Y_train[:,1:]], ...)\n'
'def tree_path(instance, values, left, right, threshold, features, node, depth):\n    spacer = \'    \' * depth\n    if (threshold[node] != _tree.TREE_UNDEFINED):\n        if instance[features[node]] &lt;= threshold[node]:\n            path = f\'{spacer}{features[node]} ({round(instance[features[node]], 2)}) &lt;= {round(threshold[node], 2)}\'\n            next_node = left[node]\n        else:\n            path = f\'{spacer}{features[node]} ({round(instance[features[node]], 2)}) &gt; {round(threshold[node], 2)}\'\n            next_node = right[node]\n        return path + \'\\n\' + tree_path(instance, values, left, right, threshold, features, next_node, depth+1)\n    else:\n        target = values[node]\n        for i, v in zip(np.nonzero(target)[1],\n                        target[np.nonzero(target)]):\n            target_count = int(v)\n            return spacer + "==&gt; " + str(round(target[0][0], 2)) + \\\n                   " ( " + str(target_count) + " examples )"\n\ndef get_path_code(tree, feature_names, instance):\n    left      = tree.tree_.children_left\n    right     = tree.tree_.children_right\n    threshold = tree.tree_.threshold\n    features  = [feature_names[i] for i in tree.tree_.feature]\n    values = tree.tree_.value\n    return tree_path(instance, values, left, right, threshold, features, 0, 0)\n\n# print the decision path of the first intance of a panda dataframe df\nprint(get_path_code(tree, df.columns, df.iloc[0]))\n'
'from keras import backend as K\nfrom keras.losses import the_loss_function   # import the suitable loss function\n\ny = Input(shape=labels_shape)\n\n# this is the gradient of loss with respect to inputs given some input data\ngrads = K.gradients(the_loss_function(y, model.output), model.inputs)\nfunc = K.function(model.inputs + [y, K.learning_phase()], grads)\n\n# usage in test mode = 0\nout = func([input_data_array, input_labels_array, 0])\n\n# usage in train mode = 1\nout = func([input_data_array, input_labels_array, 1])\n'
"from keras import metrics\nmodel.compile(loss='mse', optimizer='adam', metrics=[metrics.mean_squared_error, metrics.mean_absolute_error])\n"
"import numpy as np\n\ndef pad_txt_data(arr):\n  paded_arr = []\n  prefered_len = len(max(arr, key=len))\n\n  for each_arr in arr:\n    if len(each_arr) &lt; prefered_len:\n      print('padding array with zero')\n      while len(each_arr) &lt; prefered_len:\n          each_arr.insert(0, np.zeros(3))\n      paded_arr.append(each_arr)\n  return np.array(paded_arr)\n\n# your_arr = [shape(16, 3), shape(32, 3), . .. .]\n# loop through your_arr and prepare a single array with all the arrays and pass this array to padding function.\n\ninterm_arr = []\ndef input_prep():\n  for each_arr in your_arr:\n    interm_arr.append(each_arr)\n  final_arr = pad_txt_data(interm_arr)\n"
'cols = [\'merk\',\'ukuran\',\'bahan\',\'harga\']\ncth_data[\'combined\'] = cth_data[cols].apply(lambda row: \'_\'.join(row.values.astype(str)), axis=1)\n\nx = cth_data["combined"]\n'
"nltk.download('all')\n"
"X_train = np.random.random((300,2))\ny_train = np.random.random((300,20))\n\nclassifier = Sequential()\nclassifier.add(Dense(units = 6, init = 'uniform', activation = 'relu', input_dim = 2))\nclassifier.add(Dense(units = 6, init = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 20, init = 'uniform', activation = 'sigmoid'))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.summary()\n\nclassifier.fit(X_train, y_train, batch_size=10, nb_epoch=100)\n\n"
'#bash command\npython3 export_inference_graph.py \\\n--input_type image_tensor \\\n--pipeline_config_path PATH_TO_PIPELINE.config \\\n--trained_checkpoint_prefix PATH_TO/model.ckpt-NUMBER \\\n--output_directory PATH_TO_NEW_DIR \n'
"classifier.add(Dense(units=16, kernel_initializer='uniform', activation='relu', input_dim=11))\nclassifier.add(keras.layers.Dropout(0.5))\nclassifier.add(Dense(units=8, kernel_initializer='uniform', activation='relu'))\n\nfrom keras import regularizers\nmodel.add(Dense(64, input_dim=64,\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))\n"
"print(best.best_params_)\n\nprint(best.best_estimator_)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import datasets\nfrom sklearn.model_selection import RandomizedSearchCV\n\niris = datasets.load_iris()\n\nparameters = {\n'bootstrap': [True, False],\n'max_depth': [80, 90, 100, 110],\n'min_samples_leaf': [3, 4, 5]\n}\n\nmodel = RandomForestClassifier()\nclf = RandomizedSearchCV(model, parameters, cv=5, return_train_score=True, iid=True, n_iter = 4)\nclf.fit(iris.data, iris.target)\n\nRandomizedSearchCV(cv=5, error_score='raise-deprecating',\n          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n          fit_params=None, iid=True, n_iter=4, n_jobs=None,\n          param_distributions={'max_depth': [80, 90, 100, 110], 'bootstrap': [True, False], 'min_samples_leaf': [3, 4, 5]},\n          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n          return_train_score=True, scoring=None, verbose=0)\n\nclf.best_params_\n# {'bootstrap': True, 'max_depth': 90, 'min_samples_leaf': 5}\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=90, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=5, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n"
"import numpy as np\nfrom sklearn.externals import joblib\nmodel = joblib.load('SVM_LINEAR')\nmodel.predict(np.asarray([46.8,11,7,0.686563,6.540829e-08,1.133174e-09]))\n\nimport numpy as np\nfrom sklearn.externals import joblib\nmodel = joblib.load('SVM_LINEAR')\nmodel.predict([[46.8,11,7,0.686563,6.540829e-08,1.133174e-09]])\n"
'# taking examples for your X_train, X_val, y_train and y_val\nimport numpy as np\nX_train = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nX_val = np.array([[1, 2, 3], [4, 5, 6]])\ny_train = np.array([10, 11, 12])\ny_val = np.array([13, 14])\n\ndata = np.concatenate((X_train, X_val), axis=0)\ntarget = np.concatenate((y_train, y_val))\n'
'pd.Series(abs(svc.coef_[0])).sort_values(ascending=False).head(10).plot.barh()\n'
"import numpy as np\nimport pandas as pd\n\n# create dataframe \ndf = pd.DataFrame(np.array([['Male', 'Yes', 'Forceps'], ['Female', 'No', 'Forceps and ventouse'],\n                            ['Female','missing','None'], ['Male','Yes','Ventouse']]), \n                    columns=['gender', 'diabetes', 'assistance'])\n\ndf.head()\n\n# encode categorical data \nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(handle_unknown='ignore')\nresults = ohe.fit_transform(df)\ndf_results = pd.DataFrame.sparse.from_spmatrix(results)\ndf_results.columns = ohe.get_feature_names(df.columns)\ndf_results\n\n   gender_Female  gender_Male  diabetes_No  diabetes_Yes  diabetes_missing  assistance_Forceps  assistance_Forceps and ventouse  assistance_None  assistance_Ventouse\n0            0.0          1.0          0.0           1.0               0.0                 1.0                              0.0              0.0                  0.0\n1            1.0          0.0          1.0           0.0               0.0                 0.0                              1.0              0.0                  0.0\n2            1.0          0.0          0.0           0.0               1.0                 0.0                              0.0              1.0                  0.0\n3            0.0          1.0          0.0           1.0               0.0                 0.0                              0.0              0.0                  1.0\n\nprint(type(results))\n&lt;class 'scipy.sparse.csr.csr_matrix'&gt;\n"
"import numpy as np\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.svm import SVR\n\n\nX = np.asarray([0, 5, 10, 15, 20, 25]).reshape(-1, 1)\ny = np.asarray([[0.2, 45], [0.4, 47], [0.7, 51], [1.1, 60], [1.5, 90], [1.9, 100]])\n\nregressor = MultiOutputRegressor(SVR(kernel='rbf', C=1e3, gamma=0.1))\n\nregressor.fit(X, y)\n"
't = X_train,\ntype(t)\n# tuple\n\nclass Train():\n    def __init__(self, X_train, X_test, y_train, y_test):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.X_test = X_test\n        self.y_test = y_test\n        \n    def train(self):\n        self.cls = LogisticRegression() \n        self.cls.fit(self.X_train, self.y_train)\n'
"for j in bentonite:\n    if len(j)&lt;5:\n        print('Insufficient data')\n        continue\n    for i in j:\n        ...\n\nfor j in bentonite:\n    if len(j)&lt;5:\n        print('Insufficient data')\n    else:\n        for i in j:\n            ...\n"
"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nm_array = np.array([229, 230, 231, 230, 230])[:, np.newaxis]\nfaces = np.array(['Face 1', 'Face2', 'Face 3', 'Face 4', 'Face 5'])\n\nX_train, X_test, y_train, y_test = train_test_split(m_array, faces, test_size = 0.20)\n\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)\n"
"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport json\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\nwith open('/Users/aus10/Desktop/PGA/Data_Cleanup/Combined_Player_Stats.json') as json_file:\n    players_data = json.load(json_file)\n\nfor obj in players_data:\n    obj['Scrambling_List'] = [i for i in obj['Scrambling_List'] if i]\n\nfor obj in players_data:\n    def create_2d_lst(lst):\n        try:\n            if len(lst) &lt; 1:\n                return [0, 0]\n            else:\n                return [[i, j] for i, j in enumerate(lst)]\n        except:\n                pass\n    try:     \n        scrambling = create_2d_lst(obj['Scrambling_List'])\n        total_putts_GIR = create_2d_lst(obj['Total_Putts_GIR_List'])\n        SG_Putting = create_2d_lst(obj['SG_Putting_List'])\n    except Exception:\n        pass\n\n    data = scrambling\n    X = np.array(data)[:,0].reshape(-1,1)\n    y = np.array(data)[:,1].reshape(-1,1)\n\n    poly_reg = PolynomialFeatures(degree=4)\n\n    X_poly = poly_reg.fit_transform(X)\n\n    pol_reg = LinearRegression()\n    pol_reg.fit(X_poly, y)\n\n    predicted_y = poly_reg.fit_transform(X)\n    m = pol_reg.coef_\n    c = pol_reg.intercept_\n\n    prediction = pol_reg.predict(poly_reg.fit_transform([[len(X)+1]]))\n\n    def viz_polymonial():\n        plt.scatter(X, y, color='red')\n        plt.plot(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')\n        plt.plot(len(X)+1, pol_reg.predict(poly_reg.fit_transform([[len(X)+1]])), marker='x', color='green')\n        plt.title('Projected Scrambling Percentage')\n        plt.xlabel('Tournaments')\n        plt.ylabel('Scrambling Percentage')\n        plt.show()\n        return\n\n    viz_polymonial()\n\n    print(obj['Name'], prediction)\n"
' for n in range(n_episodes): \n    ss = env.reset()\n    states_total = 16\n    data = [[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]]\n    def encode(data, states_total):\n        targets = np.array(data).reshape(-1)\n        return np.eye(states_total)[targets]\n    m = encode(data,states_total)\n    s = m[ss]\n    #print(s)\n    #print(len(s))\n    done=False\n    r_sum = 0\n    while not done: \n        #env.render()\n        qvals_s = model.predict(s.reshape(1,-1))\n        if np.random.random() &lt; epsilon:  a = env.action_space.sample()\n        else:                             a = np.argmax(qvals_s); \n        sprime, r, done, info = env.step(a)\n        r_sum += r\n        q = encode(data,states_total)\n        sprime = q[sprime]\n        if len(replay_memory) &gt; mem_max_size:\n            replay_memory.pop(0)\n        replay_memory.append({&quot;s&quot;:s,&quot;a&quot;:a,&quot;r&quot;:r,&quot;sprime&quot;:sprime,&quot;done&quot;:done})\n        #s = n[sprime]\n        s=sprime\n        model=replay(replay_memory, minibatch_size = minibatch_size)\n    if epsilon &gt; 0.001:      epsilon -= 0.001\n    r_sums.append(r_sum)\n    print(r_sum)\n    print(epsilon)\n    if n % 100 == 0: print(n)\n'
"from aif360.datasets import StandardDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n\ndataset = StandardDataset(df, \n                          label_name='income', \n                          favorable_classes=[1], \n                          protected_attribute_names=['gender'], \n                          privileged_classes=[[1]])\n\ndef fair_metrics(dataset, y_pred):\n    dataset_pred = dataset.copy()\n    dataset_pred.labels = y_pred\n        \n    attr = dataset_pred.protected_attribute_names[0]\n    \n    idx = dataset_pred.protected_attribute_names.index(attr)\n    privileged_groups =  [{attr:dataset_pred.privileged_protected_attributes[idx][0]}] \n    unprivileged_groups = [{attr:dataset_pred.unprivileged_protected_attributes[idx][0]}] \n\n    classified_metric = ClassificationMetric(dataset, dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n    metric_pred = BinaryLabelDatasetMetric(dataset_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n\n    result = {'statistical_parity_difference': metric_pred.statistical_parity_difference(),\n             'disparate_impact': metric_pred.disparate_impact(),\n             'equal_opportunity_difference': classified_metric.equal_opportunity_difference()}\n        \n    return result\n\n\nfair_metrics(dataset, y_pred)\n\n{'statistical_parity_difference': -0.6666666666666667,\n 'disparate_impact': 0.3333333333333333,\n 'equal_opportunity_difference': 0.0}\n"
"classify = Conv2D(num_classes, (1, 1), activation='sigmoid')(up0b)\n\nmodel.compile(optimizer=RMSprop(lr=learning_rate), loss=make_loss('bce_dice'), metrics=[dice_coef, 'accuracy'])\n"
"linear_svm_sgd.fit(xtrain_bow, ytrain_bow)\ncalibrated_clf= CalibratedClassifierCV(linear_svm_sgd,cv=3, method='sigmoid')    \n#fit the model on train and predict its probability \nclf_model=calibrated_clf.fit(xtrain_bow,ytrain_bow)\npredictl1=clf_model.predict_proba(xtrain_bow)\n"
"x = Conv2D(32, (3, 3) , padding='SAME')(model_input)\nx = LeakyReLU(alpha=0.3)(x)\nx = BatchNormalization()(x)\nx = Conv2D(32, (3, 3) , padding='SAME')(x)\nx = LeakyReLU(alpha=0.3)(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Dropout(0.25)(x)\n\nx = Flatten()(x)\nx = Dense(256)(x)\nx = LeakyReLU(alpha=0.3)(x)\nx = Dense(128)(x)\nx = LeakyReLU(alpha=0.3)(x)\nx = Dense(2)(x)\nx = Activation('softmax')(x)\n"
'eli5.show_weights(perm, feature_names = X.columns.tolist(), top=None)\n'
'logreg = LogisticRegression(verbose=1)\n\nrf = RandomForestRegressor(verbose=1)\n'
"model.add(Dense(2)) # final layer\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')\n"
'return X[self.columns].values\n'
'[1, 0, 0, 0]\n[0, 1, 0, 0]\n[0, 0, 1, 0]\n...\n\n1\n2\n5\n3\n...\n'
'@_dispatch.add_dispatch_list\n@tf_export(\'math.add\', \'add\')\ndef add(x, y, name=None):\n  r"""Returns x + y element-wise.\n\n  *NOTE*: `math.add` supports broadcasting. `AddN` does not. More about broadcasting\n  [here](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n\n  Args:\n    x: A `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `uint8`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`, `string`.\n    y: A `Tensor`. Must have the same type as `x`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor`. Has the same type as `x`.\n  """\n  _ctx = _context._context or _context.context()\n  if _ctx is not None and _ctx._thread_local_data.is_eager:\n    try:\n      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n        _ctx._context_handle, _ctx._thread_local_data.device_name, "Add",\n        name, _ctx._post_execution_callbacks, x, y)\n      return _result\n    except _core._FallbackException:\n      try:\n        return add_eager_fallback(\n            x, y, name=name, ctx=_ctx)\n      except _core._SymbolicException:\n        pass  # Add nodes to the TensorFlow graph.\n      except (TypeError, ValueError):\n        result = _dispatch.dispatch(\n              add, x=x, y=y, name=name)\n        if result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n          return result\n        raise\n    except _core._NotOkStatusException as e:\n      if name is not None:\n        message = e.message + " name: " + name\n      else:\n        message = e.message\n      _six.raise_from(_core._status_to_exception(e.code, message), None)\n  # Add nodes to the TensorFlow graph.\n  try:\n    _, _, _op = _op_def_lib._apply_op_helper(\n        "Add", x=x, y=y, name=name)\n  except (TypeError, ValueError):\n    result = _dispatch.dispatch(\n          add, x=x, y=y, name=name)\n    if result is not _dispatch.OpDispatcher.NOT_SUPPORTED:\n      return result\n    raise\n  _result = _op.outputs[:]\n  _inputs_flat = _op.inputs\n  _attrs = ("T", _op._get_attr_type("T"))\n  _execute.record_gradient(\n      "Add", _inputs_flat, _attrs, _result, name)\n  _result, = _result\n  return _result\n'
"pipeline.named_steps['classifier'].best_params_\n"
"from sklearn.svm import SVC\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\ndef make_meshgrid(x, y, h=.02):\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    return xx, yy\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\nmodel = svm.SVC(kernel='linear')\nclf = model.fit(X, y)\n\nfig, ax = plt.subplots()\n# title for the plots\ntitle = ('Decision surface of linear SVC ')\n# Set-up grid for plotting.\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nplot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\nax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\nax.set_ylabel('y label here')\nax.set_xlabel('x label here')\nax.set_xticks(())\nax.set_yticks(())\nax.set_title(title)\nax.legend()\nplt.show()\n"
'knn = KNeighborsClassifier(n_neighbors=0)\n\nk_range = list(range(1))\n'
"% of non events = 3/5\n% of events = 1/4\n\nln(% of events / % of non events ) = ln(5/12) = -0.8754687373538999\n\nwoe = WOEEncoder(cols=['cat'], random_state=42, regularization=0)\nX = df['cat']\ny = df.target\nencoded_df = woe.fit_transform(X, y)\n\n0   -0.875469\n1   0.916291\n2   -0.875469\n3   0.916291\n4   -0.875469\n5   -0.875469\n6   0.916291\n7   0.223144\n8   0.223144\n"
"train = your_data[your_data['year_column'] &lt; 2019]\ntest = your_data[your_data['year_column'] == 2019]\n\nX_train = train.loc[:, train.columns != 'column_of_interest']\ny_train = train['column_of_interest']\nX_test = test.loc[:, test.columns != 'column_of_interest']\ny_train = test['column_of_interest']\n"
'## Explore the data (line 27)\ndata = pd.read_table(\'u.data\', header=None)  # header=None avoid getting the columns automatically\ndata.columns = [\'userID\', \'itemID\',\n                \'rating\', \'timestamp\']       # Manually set the columns.\ndata = data.drop(\'timestamp\', axis=1)        # Continue with regular work.\n\n...\n\n## Load user information (line 75)\nusers_info = pd.read_table(\'u.user\', sep=\'|\', header=None)\nusers_info.columns = [\'useID\', \'age\', \'gender\',\n                      \'occupation\' \'zipcode\']\nusers_info = users_info.set_index(\'userID\')\n\n...\n\n## Load movie information (line 88)\nmovies_info = pd.read_table(\'u.item\', sep=\'|\', header=None)\nmovies_info.columns = [\'movieID\', \'movie title\', \'release date\',\n                       \'video release date\', \'IMDb URL\', \'unknown\',\n                       \'Action\', \'Adventure\', \'Animation\', "Children\'s",\n                       \'Comedy\', \'Crime\', \'Documentary\', \'Drama\',\n                       \'Fantasy\', \'Film-Noir\', \'Horror\', \'Musical\',\n                       \'Mystery\', \'Romance\', \'Sci-Fi\',\' Thriller\',\n                       \'War\', \'Western\']\nmovies_info = movies_info.set_index(\'movieID\')#.drop(low_count_movies)\n'
"def logistic_predictions(weights_and_W, inputs):\n    '''\n    Here, :arg weights_and_W: is an array of the form [weights W.ravel()]\n    '''\n    # Outputs probability of a label being true according to logistic model.\n    weights = weights_and_W[:inputs.shape[1]]\n    W_raveled = weights_and_W[inputs.shape[1]:]\n    n_W = len(W_raveled)\n    W = W_raveled.reshape(inputs.shape[1], n_W/inputs.shape[1])\n\n    return sigmoid(np.dot(np.dot(inputs, W), weights))\n\ndef training_loss(weights_and_W):\n    # Training loss is the negative log-likelihood of the training labels.\n    preds = logistic_predictions(weights_and_W, inputs)\n    label_probabilities = preds * targets + (1 - preds) * (1 - targets)\n    return -np.sum(np.log(label_probabilities))\n"
'# Example 1\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n\n# Example 2\nsess = tf.Session()\n'
'[0.58348329, -0.99979492,  0.08571431, -0.99996706, -0.52397444, 0.99959056]\n'
" from keras.preprocessing.text import Tokenizer\n\n samples = ['The cat say on the mat.', 'The dog ate my homework.']\n\n tokenizer = Tokenizer(num_words=1000)\n tokenizer.fit_on_texts(samples)\n\n one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n\n word_index = tokenizer.word_index\n print('Found %s unique tokens.' % len(word_index))\n"
" def __getitem__(self, index):\n        return self.X_train[index], self.y_train[index]\n\nfor epoch in range(num_epochs):    \n    for batch_id, (x, y) in enumerate(dataloader):\n           x = Variable(x)\n           y = Variable(y)\n           # then do whatever you want to do\n\nfor epoch in range(num_epochs):    \n    for batch_id, data in enumerate(dataloader):\n        # data will be dict \n        x = Variable(data['X'])\n        y = Variable(data['y'])\n        # then do whatever you want to do\n"
'y_pred = np.rint(sess.run(final_output, feed_dict={X_data: X_test}))\n\nscore = sklearn.metrics.precision_score(y_test, y_pred)\n'
" def mnist_bytescale(image):\n    # Use float for rescaling\n    img_temp = image.astype(np.float32)\n    #Re-zero the data\n    img_temp -= img_temp.min()\n    #Re-scale and invert\n    img_temp /= (img_temp.max()-img_temp.min())\n    img_temp *= 255\n    return 255 - img_temp.astype('uint')\n"
'linear_svc_model = clf.fit(train_vectors, train_labels)\nmodel_object = []\nmodel_object.append(linear_svc_model)\nmodel_object.append(vectorizer)\n'
'class MyModel:\n    def __init__(self):\n        self.sess = tf.Session()\n        init.run()\n\n    def train_model(self, X_data, y_data):\n        # Creating placeholders X and y\n        for epoch in range(200):\n           for (xh, yh) in zip(X_data, y_data):\n               sess.run(optimizer, feed_dict={X: xh, y: yh})\n\n    def make_pred(self, date):\n        # Use the previous model to generate a prediction yp\n        return sess.run(yp, feed_dict={X: date})\n'
'  P(A|B) = P(A)  or P(B|A) = P(B)\n'
"from keras.models import load_model\nfrom sklearn.externals import joblib\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('estimator', KerasClassifier(build_model))\n])\n\npipeline.fit(X_train, y_train)\n# save keras model\npipeline.named_steps['estimator'].model.save('keras_model.h5')\n\n# save pipeline\npipeline.named_steps['estimator'].model = None\njoblib.dump(pipeline, 'sklearn_pipeline.pkl')\n\n# deserialize\npipeline = joblib.load('sklearn_pipeline.pkl')\n\n# add keras model\npipeline.named_steps['estimator'].model = load_model('keras_model.h5')\n"
'X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=123) \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\n#function\ndef train_test_rmse(x,y):\n    x = Iris_data[x]\n    y = Iris_data[y]\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2,random_state=123)\n    linreg = LinearRegression()\n    linreg.fit(X_train, y_train)\n    y_pred = linreg.predict(X_test)\n    print(accuracy_score(y_test, y_pred))  # or you can save it in variable and return it \n    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n'
"def Manhattan_d(a,b,c,d, df):\n    return df.apply(lambda row:abs(row['A']-a)+abs(row['B']-b)+abs(row['C']-c)+abs(row['D']-d), axis=1).idxmin()\n"
" kmeans_model = KMeans(n_clusters=clusters, init='k-means++', max_iter=iterations, random_state = 'int number need to given') \n"
"filter = [i.shape == most_common for i in df['c_matrix']]\ndf = df[filter]\n\ndf = df[[i.shape == most_common for i in df['c_matrix']]]\n"
'char_limit=4\ndf[df[\'sentence\'].apply(lambda x : len("".join(x))&gt;=char_limit)]\n'
'def loss(true, pred):\n    return keras.backend.mean(keras.backend.square(true-pred), axis=-1)\n'
"cluster_grid = {\n    'kmeans__n_clusters': range(2,100)\n}\n# adding n_jobs to run in parallel\ngrid = GridSearchCV(pipeline, cluster_grid, n_jobs=-1)\n\npipeline = Pipeline([\n    ('kmeans', KMeans(),\n    ('log_reg', LogisticRegression())\n])\ncluster_grid = {\n    'kmeans__n_clusters': range(2,100)\n}\n# adding n_jobs to run in parallel\ngrid = GridSearchCV(pipeline, cluster_grid, n_jobs=-1)\n"
'input_array = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14])\ninput_array_for_prediction = np.expand_dims(input_array,axis=0)\nprint(model.predict(input_array_for_prediction))\n'
"for item in C:\n    print(item)\n\nc_values = []\ndegrees = []\nfor item in C:\n    c_values.append(item['C'])\n    degrees.append(item['degree'])  \n"
"from sklearn.svm import SVC\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=100, n_features=4,\n                            n_informative=2, n_redundant=0,\n                            random_state=0, shuffle=False)\nclf = BaggingClassifier(base_estimator=SVC(),\n                         n_estimators=3, random_state=0)\n\nclf.estimators_\n# AttributeError: 'BaggingClassifier' object has no attribute 'estimators_'\n\nclf.fit(X, y)\nclf.estimators_\n# result:\n[SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n     max_iter=-1, probability=False, random_state=2087557356, shrinking=True,\n     tol=0.001, verbose=False),\n SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n     max_iter=-1, probability=False, random_state=132990059, shrinking=True,\n     tol=0.001, verbose=False),\n SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n     decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n     max_iter=-1, probability=False, random_state=1109697837, shrinking=True,\n     tol=0.001, verbose=False)]\n"
"from tensorflow import keras\nfrom keras import Model     # Wrong! DON'T mix keras and tf.keras!\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense\n"
'# Kürzen der Datasets aufgrund der Rechenzeit\n# train_test_split unnötig\nX_train = X_tr[:5000]\ny_train = y_tr[:5000]\nX_test = X_te[:5000]\ny_test = y_te[:5000]\n'
"ax.plot(X_test.iloc[:, 0], X_test.iloc[:, 1], y_pred, color='red')\n\nfrom sklearn.model_selection import train_test_split\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\n\n# generate some data as an example.\nnp.random.seed(1)\nn = 20\nX = pd.DataFrame(np.random.uniform(size=(n, 2)), columns=['foo', 'bar'])\nY = X['foo'] + 2*X['bar'] + np.random.normal(scale=0.2, size=n)\n\nX_train, X_test, y_train,y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X['foo'], X['bar'], Y, label='data')\n\nfor x0, x1, yt, yp in zip(X_test['foo'], X_test['bar'], y_test, y_pred):\n    ax.plot([x0, x0], [x1, x1], [yt, yp], color='red')\n\nax.scatter(X_test['foo'], X_test['bar'], y_pred, color='red', marker='s', label='prediction') \n\nax.set_xlabel('X0')\nax.set_ylabel('X1')\nax.set_zlabel('y')\nax.legend()\nfig.show()\n"
"print(X_train_credit_balance[0])\n\narray([30., 30.], dtype=float32)\n\ncredit_list = []\n\ndef convert_credit_rows(row):\n  credit_list.append(np.asarray([row['A'], row['B'], row['C'], row['D']], dtype=np.float32))\n\n X_train_credit_balance = np.array(credit_list)\n\n[[1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]\n [1. 2. 3.]]\n"
"def gen_model():\n   model = tf.keras.Sequential()\n   model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu', input_shape=(IMAGE_DIM, IMAGE_DIM, CHAN_COUNT)))\n   model.add(tf.keras.layers.MaxPool2D())\n   model.add(tf.keras.layers.Conv2D(filters=64, kernel_size = (5, 5), activation='relu'))\n   model.add(tf.keras.layers.MaxPool2D())\n   model.add(tf.keras.layers.Conv2D(filters=128, kernel_size = (5, 5), activation='relu'))\n   model.add(tf.keras.layers.MaxPool2D())\n   model.add(tf.keras.layers.Flatten())\n   model.add(tf.keras.layers.Dense(256, activation='relu'))\n   model.add(tf.keras.layers.Dropout(0.1))\n   model.add(tf.keras.layers.Dense(128, activation='relu'))\n   model.add(tf.keras.layers.Dropout(0.1))\n   model.add(tf.keras.layers.Dense(64, activation='relu'))\n   model.add(tf.keras.layers.Dropout(0.1))\n   model.add(tf.keras.layers.Dense(16, activation='relu'))\n   model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n   model.compile(loss=keras.losses.MeanSquaredError(),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[keras.metrics.MeanAbsoluteError()])\n   return model\n"
"X_train=train.drop(['Station','StationIndex','dayofyear'],axis=1)\nY_train=train['Rainfall']\nX_test=test.drop(['Station','StationIndex','dayofyear'],axis=1)\nY_test=test['Rainfall']\n\nX_train=train.drop(['Station','StationIndex','dayofyear','Rainfall'],axis=1)\nY_train=train['Rainfall']\nX_test=test.drop(['Station','StationIndex','dayofyear','Rainfall'],axis=1)\nY_test=test['Rainfall']\n\nfrom sklearn import svm\nmodel = svm.SVC(gamma='auto',kernel='linear')\nmodel.fit(X_train, Y_train)\nprint('Accuracy on training set: {:.2f}%'.format(100*model.score(X_train, Y_train)))\nprint('Accuracy on testing set: {:.2f}%'.format(100*model.score(X_test, Y_test)))\n"
'output = self.net_input(X_train_std).reshape(-1, 1)\n\nself.w_[1:] += self.eta * np.sum(errors * X_train_std, axis=0)\n'
"# Convert a collection of text documents to a vector of term/token counts. \ncnt_vect_for_new_data = count_vector.transform(df['new_data'])\n\n#RUN Prediction\ndf['NEW_DATA_PREDICTION'] = naive_bayes.predict(cnt_vect_for_new_data)\n"
'from sklearn.datasets import load_svmlight_file\ndef get_data(dn):\n    # load_svmlight_file loads dataset into sparse CSR matrix\n    X,Y = load_svmlight_file(dn)\n    print(type(X)) # you will get numpy.ndarray\n    return X,Y\n\n# X, Y = get_data(dn) uncomment this code and pass the dn parameter you want.\n# convert X to ndarray\nX = X.toarray()\nprint(type(X))\n\n# As you are going to implement logistic regression, you have to convert the \nlabels into 0 and 1 \nY = np.where(Y == -1, 0, 1)\n'
"X = houses_preprocessed.drop(columns=['price'])\ny = houses_preprocessed['price']\n\nimport seaborn as sns\n\nsns.clustermap(X.select_dtypes(&quot;number&quot;).corr(method=&quot;spearman&quot;),figsize=(6, 6))\n\nsns.pairplot(X[['bathrooms','sqft_above','sqft_living']])\n\nX = pd.get_dummies(X.drop(columns=['bathrooms','sqft_above']))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nreg = LinearRegression(fit_intercept=False)\nreg.fit(X_train, y_train)\n\nreg.score(X_test,y_test)\n0.7621069304476887\n\nres = pd.DataFrame({'coef':reg.coef_},index=X.columns)\nres.reindex(res.coef.abs().sort_values().index)\n\n\ncoef\nsqft_lot    -0.023554\nyr_built    54.699771\nsqft_basement   -100.401752\nsqft_living 278.836773\nstatezip_WA 98006   565.521930\n... ...\nstatezip_WA 98023   -342256.082284\nstatezip_WA 98070   -353819.063160\nstatezip_WA 98004   589945.748620\nwaterfront  621313.209967\nstatezip_WA 98039   816056.566554\n"
'   ID_Patient Exam_Name Exam_Result\n0           1    exam 0    result 0\n1           1    exam 1    result 1\n2           1    exam 2    result 2\n3           1    exam 3    result 3\n4           2    exam 0    result 4\n5           2    exam 1    result 5\n6           2    exam 2    result 6\n7           2    exam 3    result 7\n\nExam_Name     exam 0    exam 1    exam 2    exam 3\nID_Patient                                        \n1           result 0  result 1  result 2  result 3\n2           result 4  result 5  result 6  result 7\n\n   ID_Patient    exam 0    exam 1    exam 2    exam 3\n0           1  result 0  result 1  result 2  result 3\n1           2  result 4  result 5  result 6  result 7\n'
'(B, (N-kernel_size+1)//(stride) , filters)\n'
"model.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\ndef cnn_model():\n    model = Sequential()\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(CLASSNAME_SIZE, activation='softmax'))\n    return model\n\nmodel = cnn_model()\n\ndef cnn_model():\n    model = Sequential()\n    model.add(Conv2D(...))\n    # ... more conv/pool layers\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(CLASSNAME_SIZE, activation='softmax'))\n    return model\n"
"X = dataset.drop(columns=['Date','Result'])\ny = dataset.drop(columns=['Date', 'Open', 'High', 'Close'])\n"
'def extract_sift_feature(X, y):\n    images_descriptor = []\n    filter_images_descriptor = []\n    NoneType_index_list = []\n    sift = cv2.SIFT_create()\n\n    for i in range(len(X)):\n        _kp, des = sift.detectAndCompute(X[i], None)\n        images_descriptor.append(des)\n        #Check if there any image has 0 feature descriptor \n        if des is None:\n          NoneType_index_list.append(i)\n    images_descriptor = np.array(images_descriptor)\n\n    #Filter image any image has 0 feature descriptor \n    for i in range(len(images_descriptor)):\n        if images_descriptor[i] is not None:\n            filter_images_descriptor.append(images_descriptor[i])\n    filter_images_descriptor = np.array(filter_images_descriptor)\n\n    new_y = np.delete(y, NoneType_index_list)\n    return filter_images_descriptor, new_y\n'
"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.16, random_state=19, stratify=df['label'])\n"
'order = np.random.permutation(np.arange(2115))\n\ndata = data[order]\n'
"model = tf.keras.Sequential([\n                      \n                       tf.keras.layers.Dense(50,input_shape=(4,),activation=&quot;relu&quot;,kernel_initializer ='he_uniform'),\n                       tf.keras.layers.BatchNormalization(),\n                       tf.keras.layers.Dropout(0.1),\n                       tf.keras.layers.Dense(25,activation=&quot;relu&quot;,kernel_initializer ='he_uniform'),\n                       tf.keras.layers.BatchNormalization(),\n                       tf.keras.layers.Dropout(0.1),\n            \n                       tf.keras.layers.Dense(1),\n                    ])\n        \nopt = tf.keras.optimizers.RMSprop() # you can also use adam\nmodel.compile(loss='mse',optimizer=opt)\n\nEpoch 39/100\n32/32 [==============================] - 0s 5ms/step - loss: 0.0812 - val_loss: 0.0888\nEpoch 40/100\n32/32 [==============================] - 0s 5ms/step - loss: 0.0833 - val_loss: 0.0864\n\ny_hat = model.predict(X_test)\ny_hat[:10]\n\narray([[0.47433853],\n       [0.4804499 ],\n       [0.4659953 ],\n       [0.4893798 ],\n       [0.38975602],\n       [0.53456545],\n       [0.5105466 ],\n       [0.45408142],\n       [0.4651251 ],\n       [0.5104909 ]], dtype=float32)\n"
'svm = SVMClassifier([HueHistogramFeatureExtractor()])\n'
'def dtanh(x):\n    return 1 - np.tan(x)**2\n\ndef dtanh(x):\n   return 1 - np.tanh(x)**2\n'
"import numpy as np\n\nseries = [\n    [1,2,3,4],\n    [1,2,3],\n    [1],\n    [1,2,3,4,5,6,7,8]\n]\n\ndef make_square(jagged):\n    # Careful: this mutates the series list of list\n    max_cols = max(map(len, jagged))\n    for row in jagged:\n        row.extend([None] * (max_cols - len(row)))\n    return np.array(jagged, dtype=np.float)\n\n\nmake_square(series)\narray([[  1.,   2.,   3.,   4.,  nan,  nan,  nan,  nan],\n       [  1.,   2.,   3.,  nan,  nan,  nan,  nan,  nan],\n       [  1.,  nan,  nan,  nan,  nan,  nan,  nan,  nan],\n       [  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.]])\n\ndef wrapper(row1, row2):\n    # might have to fiddle a bit here, but i think this retrieves the indices.\n    i1, i2 = row1[0], row2[0]\n    return D[i1, i2]\n\n#!/usr/bin/env python2.7\n# encoding: utf-8\n'''\n'''\nfrom mlpy import dtw_std # I dont know if you are using this one: it doesnt matter.\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\n\n# Example data\nseries = [\n    [1, 2, 3, 4],\n    [1, 2, 3, 4],\n    [1, 2, 3, 4],\n    [1, 2, 3],\n\n    [1],\n\n    [1, 2, 3, 4, 5, 6, 7, 8],\n    [1, 2, 5, 6, 7, 8],\n    [1, 2, 4, 5, 6, 7, 8],\n]\n\n# I dont know.. these seemed to make sense to me!\ny = np.array([\n    0,\n    0,\n    0,\n    0,\n\n    1,\n\n    2,\n    2,\n    2\n])\n\n# Compute the distance matrix\nN = len(series)\nD = np.zeros((N, N))\n\nfor i in range(N):\n    for j in range(i+1, N):\n        D[i, j] = dtw_std(series[i], series[j])\n        D[j, i] = D[i, j]\n\nprint D\n\n# Create the fake data matrix: just the indices of the timeseries\nX = np.arange(N).reshape((N, 1))\n\n\n# Create the wrapper function that returns the correct distance\ndef wrapper(row1, row2):\n    # cast to int to prevent warnings: sklearn converts our integer indices to floats.\n    i1, i2 = int(row1[0]), int(row2[0])\n    return D[i1, i2]\n\n# Only the ball_tree algorith seems to accept a custom function\nknn = KNeighborsClassifier(weights='distance', algorithm='ball_tree', metric='pyfunc', func=wrapper)\nknn.fit(X, y)\nprint knn.kneighbors(X[0])\n# (array([[ 0.,  0.,  0.,  1.,  6.]]), array([[1, 2, 0, 3, 4]]))\nprint knn.kneighbors(X[0])\n# (array([[ 0.,  0.,  0.,  1.,  6.]]), array([[1, 2, 0, 3, 4]]))\n\nprint knn.predict(X)\n# [0 0 0 0 1 2 2 2]\n"
'import pandas as pd\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\ndf = pd.DataFrame(np.random.randint(0, 5, (10, 5)), columns=list("ABCDE"))\nneigh = NearestNeighbors(n_neighbors=3)\nneigh.fit(df.T) # Fit the data\nmodel = neigh.kneighbors(df.T, return_distance=False)\npd.DataFrame(df.columns[model])\n'
'centers = pca.inverse_transform(kmeans.cluster_centers_)\nprint(centers)\n\n[[ 6.82271303  3.13575974  5.47894833  1.91897312]\n [ 5.80425955  2.67855286  4.4229187   1.47741067]\n [ 5.03012829  3.42665848  1.46277424  0.23661913]]\n\nfor label in range(kmeans.n_clusters):\n    print(X[kmeans.labels_ == label].mean(0))\n\n[ 6.8372093   3.12093023  5.4627907   1.93953488]\n[ 5.80517241  2.67758621  4.43103448  1.45689655]\n[ 5.01632653  3.44081633  1.46734694  0.24285714]\n\nprint(KMeans(3).fit(X).cluster_centers_)\n\n[[ 6.85        3.07368421  5.74210526  2.07105263]\n [ 5.9016129   2.7483871   4.39354839  1.43387097]\n [ 5.006       3.418       1.464       0.244     ]]\n'
"&gt;&gt;&gt; float(u'3.9055320000e+06')\n3905532.0\n&gt;&gt;&gt; int(float(u'3.9055320000e+06'))\n3905532\n&gt;&gt;&gt; \n"
'&gt;&gt;&gt; print(tr_chunk[0][:5], "...")\n[(\'NN\', \'B-NP\'), (\'IN\', \'O\'), (\'DT\', \'B-NP\'), (\'NN\', \'I-NP\'), (\'VBZ\', \'O\')] ...\n\n&gt;&gt;&gt; words = "I sure love New York .".split()\n&gt;&gt;&gt; tags = [ t for (w, t) in nltk.pos_tag(words) ]\n&gt;&gt;&gt; u_chunker.tag(tags)\n'
"P_scores = []\nz1 = 1\n\n# This loop will execute 50 times\nwhile z1 &lt;= 50:\n   clf = RandomForestRegressor(n_estimators=z1, random_state=1)\n   clf.fit(X, Y)\n   kf = KFold(len(X), n_folds=5, random_state=1, shuffle=True)\n   R2 = cross_val_score(clf, X, Y, cv=kf, n_jobs=-1, scoring='r2')\n\n   # This will append index z1 and scores for that index as a tuple to the P_scores array\n   # Each R2 array will have 5 elements (number of folds)\n   # So total 250 scores (as you wanted)\n   P_scores.append((z1,R2))\n   z1 += 1\nprint(P_scores)\n"
"import numpy as np\n\nfd=fd.add_row_number()\ndf = fd.to_dataframe()\ndf_indexes = df[df.id.isin(indexes)]\ndf_indexes = df_indexes.drop(labels='id', axis=1)\nx_sub = np.array(df_indexes)\n"
"tfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,\n                            sublinear_tf=False)\n\ninput_list = ['A positive sentence', 'A negative sentence', ]\nclass_list = [0, 1]\n\ndf= pd.DataFrame({'text':input_list,'class': class_list})\n\nX = tfidf_vect.fit_transform(df['text'].values)\ny = df['class'].values\n\nprint(tfidf_vect.get_feature_names())\nprint()\nprint(X.todense())\n\n[u'negative', u'positive', u'sentence']\n\n[[ 0.          0.81480247  0.57973867]\n [ 0.81480247  0.          0.57973867]]\n\ntfidf_vect= TfidfVectorizer(use_idf=True, smooth_idf=True,\n                            sublinear_tf=False, ngram_range=(1, 2))\n\ninput_list = ['A positive sentence', 'A negative sentence', ]\nclass_list = [0, 1]\n\ndf= pd.DataFrame({'text':input_list,'class': class_list})\n\nX = tfidf_vect.fit_transform(df['text'].values)\ny = df['class'].values\n\nprint(tfidf_vect.get_feature_names())\nprint()\nprint(X.todense())\n\n[u'negative', u'negative sentence', u'positive', u'positive sentence', u'sentence']\n\n[[ 0.          0.          0.6316672   0.6316672   0.44943642]\n [ 0.6316672   0.6316672   0.          0.          0.44943642]]\n\nX = np.array(X.todense())\nmy_feature = np.array([[0.7, 1.2]])\nnp.concatenate((X, my_feature.T), axis=1)\n\narray([[ 0.        ,  0.        ,  0.6316672 ,  0.6316672 ,  0.44943642,\n     0.7       ],\n       [ 0.6316672 ,  0.6316672 ,  0.        ,  0.        ,  0.44943642,\n     1.2       ]])\n"
'Z = ifft(X.flatten())\n\nZ = ifft(X).flatten()\n'
"final_output = tf.add(tf.matmul(hidden_layer_3_output, output_layer['weights']), output_layer['bias']) \n\n'weights': tf.Variable(tf.random_normal(shape = [784, n_hidden_layer_1], stddev=0.005))\n"
'textdeskew input.jpg deskew.png\n\ntextcleaner -f 25 -o 10 -g -e normalize -s 1 deskew.png deskew_clean.png\n\nconvert input.jpg -deskew 40% input_deskew.png\n\nconvert input_deskew.png -negate -lat 25x25+10% -negate input_deskew_lat.png\n\nconvert input.jpg -deskew 40% -negate -lat 25x25+10% -negate input_deskew_lat.png\n'
'if np.linalg.norm(gradient) &lt; 1e-4:\n    return solution\n'
'import os\nos.listdir()\n'
'new_a = []\nnew_y = []\nfor x in a:\n    temp_a = []\n    sorted_labels = sorted(x, key=lambda x: x[1], reverse=True)\n    new_y.append(sorted_labels[0][0])\n    for z in x:\n        temp_a.append(z[1])\n    new_a.append(temp_a)\n'
'results = Orange.evaluation.testing.TestOnTestData(data, test, [lambda testdata: loadCF])\n'
"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nimport pandas as pd\n\n# create some dummy data\nX2 = np.asarray(np.random.normal(size=(1000,12)))\ny2 = np.asarray(np.random.choice(2,size=(1000)))\n\nlogreg = LogisticRegression()\nlogreg_scores = cross_val_score(logreg, X2, y2, cv=10, scoring='accuracy')\n\n# you need to fit the data using the fit function\nlogreg.fit(X2,y2)\n\n# creating some sample data again\ndataset2 = pd.DataFrame(np.asarray(np.random.normal(size=(1000,12))))\nOutcome = logreg.predict(dataset2) # predict the outcome\n\ndataset2.loc[:,'Outcome'] = Outcome # adding it to original data\n"
'classifier_nn.fit(x,y)\ntmp_classifier = deepcopy(classifier_nn)\nfor elem in step:\n    classifier_nn.partial_fit(new_x, new_y)\n    classifier_nn = tmp_classifier\n'
'(?, 1, 1, 1)    \n(?, 2, 1, 1)    \n(?, 4, 1, 1)    \n(?, 8, 1, 1)    \n\nmrg = Concatenate(axis=1)([max1,max2,max3,max4])\n'
" import pyshark\ncap = pyshark.FileCapture('/root/log.cap')\ncap\n&gt;&gt;&gt; &lt;FileCapture /root/log.cap&gt;\nprint cap[0]\nPacket (Length: 698)\nLayer ETH:\n        Destination: BLANKED\n        Source: BLANKED\n        Type: IP (0x0800)\nLayer IP:\n        Version: 4\n        Header Length: 20 bytes\n        Differentiated Services Field: 0x00 (DSCP 0x00: Default; ECN: 0x00: Not-ECT (Not ECN-Capable Transport))\n        Total Length: 684s\n        Identification: 0x254f (9551)\n        Flags: 0x00\n        Fragment offset: 0\n        Time to live: 1\n        Protocol: UDP (17)\n        Header checksum: 0xe148 [correct]\n        Source: BLANKED\n        Destination: BLANKED\n  ...\ndir(cap[0])\n['__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__', '__format__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_packet_string', 'bssgp', 'captured_length', 'eth', 'frame_info', 'gprs-ns', 'highest_layer', 'interface_captured', 'ip', 'layers', 'length', 'number', 'pretty_print', 'sniff_time', 'sniff_timestamp', 'transport_layer', 'udp']\ncap[0].layers\n[&lt;ETH Layer&gt;, &lt;IP Layer&gt;, &lt;UDP Layer&gt;, &lt;GPRS-NS Layer&gt;, &lt;BSSGP Layer&gt;]\n"
'from sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import chi2\nimport numpy\niris = load_iris()\nX, y = iris.data, iris.target\nselector = SelectPercentile(score_func=chi2, percentile=50)\nX_reduced = selector.fit_transform(X, y)\n'
"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.datasets import make_classification\n\nX_train, y_train = make_classification(n_features=4) # Put your training data here instead\n\n# parameters for random forest\nrfclf_params = {\n    'bootstrap': True, \n    'class_weight':None, \n    'criterion':'entropy',\n    'max_depth':None, \n    'max_features':'auto', \n    # ... fill in the rest you want here\n}\n\n# Fill in svm params here\nsvm_params = {\n    'probability':True\n}\n\n# KNeighbors params go here\nkneighbors_params = {\n\n}\n\nparams = [rfclf_params, svm_params, kneighbors_params]\nclassifiers = [RandomForestClassifier, SVC, KNeighborsClassifier]\n\ndef ensemble(classifiers, params, X_train, y_train, X_test):\n    best_preds = np.zeros((len(X_test), 2))\n    classes = np.unique(y_train)\n\n    for i in range(len(classifiers)):\n        # Construct the classifier by unpacking params \n        # store classifier instance\n        clf = classifiers[i](**params[i])\n        # Fit the classifier as usual and call predict_proba\n        clf.fit(X_train, y_train)\n        y_preds = clf.predict_proba(X_test)\n        # Take maximum probability for each class on each classifier \n        # This is done for every instance in X_test\n        # see the docs of np.maximum here: \n        # https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.maximum.html\n        best_preds = np.maximum(best_preds, y_preds)\n\n    # map the maximum probability for each instance back to its corresponding class\n    preds = np.array([classes[np.argmax(pred)] for pred in best_preds])\n    return preds\n\n# Test your predictions  \nfrom sklearn.metrics import accuracy_score, f1_score\ny_preds = ensemble(classifiers, params, X_train, y_train, X_train)\nprint(accuracy_score(y_train, y_preds), f1_score(y_train, y_preds))\n"
'x -= x.min()\nx /= x.ptp()\n'
"from keras.models import Model\nfrom keras.layers import Conv2D, Dense, Input, Embedding, multiply, Reshape, concatenate\n\nimg = Input(shape=(64, 64, 3))\nfeatures = Input(shape=(14,))\nembedded = Embedding(input_dim=14, output_dim=60*32)(features)\nembedded = Reshape(target_shape=(14, 60,32))(embedded)\n\nencoded = Conv2D(32, (3, 3), activation='relu')(img)\nencoded = Conv2D(32, (3, 3), activation='relu')(encoded)\n\nx = concatenate([embedded, encoded], axis=1)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\n\nmodel = Model([img, features], [main_output])\n"
'proba = self.predict_proba(X)\n'
"def custom_metric(y_true, y_pred):\n    max = K.max(y_pred)\n    return max\n\nmodel.compile(loss='mean_squared_error', optimizer=SGD(lr=0.005), \n              metrics=['accuracy', custom_metric])\n"
"from keras.preprocessing.sequence import TimeseriesGenerator\ntimeseries_generator = TimeseriesGenerator(data, targets, length, sampling_rate)\n\nmodel.fit_generator(timeseries_generator)\n\nmodel = Sequential()\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel = Sequential()\nmodel.add(Flatten())\nmodel.add(Dense(64, dropout=0.2, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n"
'dim = 5\n\ntf.reset_default_graph()\n\nlogits = tf.random_normal([2,3],dtype=tf.float32)\n\nlabels = tf.Variable([[ 0 , 0 ,1 ], [ 1, 0, 1]], dtype=tf.float32)\n\ncost_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,labels=labels)\nloss = tf.reduce_sum(tf.reduce_mean(cost_entropy, axis=0))\n\n\nwith tf.Session() as sess:\n    tf.global_variables_initializer().run()\n    print(sess.run(cost_entropy))\n    print(sess.run(loss))\n\n[[0.68918824 1.1838193  0.40476277]\n[1.5736797  0.6862114  0.9196656 ]]\n2.4358916\n'
'{\n    "image_data_format": "channels_last",\n    "floatx": "float32",\n    "backend": "tensorflow",\n    "epsilon": 1e-07\n}\n'
'# Three features per row\nfeatures = [[0,   1,  1],\n            [2,   3,  5],\n            [8,  13, 21],\n            [34, 55, 89]]\n\n# This changed.\n# Now a single label consists of a list of output values to be predicted\n# 5 outputs per row\nlabels = [[2,     3,   5,   8,  13], \n          [8,    13,  21,  34,  55], \n          [34,   55,  89, 144, 233], \n          [144, 233, 377, 610, 987]]\n\nclf = LinearRegression()\nclf.fit(features, labels)\ntest = [[144, 233, 377]]\nprint(clf.predict(test))\n\n# Output\n# array([[ 610.,  987., 1597., 2584., 4181.]])\n'
'File "C:/Users/Omar/Downloads/Kaggle Competition/Titanic.py", line 58, in &lt;module&gt;\n    print(accuracy_score(val_y, val_predictions)) \n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntitanic_model = DecisionTreeClassifier()\n'
"text = ['Construction workers needed for this company who has qualifications ']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntext = tokenizer.texts_to_sequences(text)\n\ntext = pad_sequences(twt, maxlen=100, dtype='int32', value=0)\nprint(text)\nsentiment = model.predict(text)[0]\ndisplay(sentiment)\n"
'model.fit(X_train[:, 1:], y)\nmodel.predict(X_test[:, 1:])\n'
'K.set_session(self.sess)\n\nclass KerasModel(object):\ndef __init__(self, seed, dim_size, optimizer, loss_func):\n    self.sess=tf.Session()\n\n    K.set_session(self.sess)\n'
'import numpy as np\nfrom keras.layers import Input, Lambda, Multiply, LSTM\nfrom keras.models import Model\nfrom keras.layers import add\n\n\nbatch_size   = 1\nnb_timesteps = 4\nnb_features  = 2\nhidden_layer = 2\n\nin1 = Input(shape=(nb_timesteps,nb_features))\n\nlstm=LSTM(hidden_layer,return_sequences=True)(in1)\n\n# Make two slices\nfactor1 = Lambda(lambda x: x[:, 0:nb_timesteps-1, :])(lstm)\nfactor2 = Lambda(lambda x: x[:, 1:nb_timesteps, :])(lstm)\n\n# Multiply them\nout = Multiply()([factor1,factor2])\n\n# set the two outputs so we can see both results\nmodel = Model(in1,[out,lstm])\n\na = np.arange(batch_size*nb_timesteps*nb_features).reshape([batch_size,nb_timesteps,nb_features])\n\nprediction = model.predict(a)\nout_, lstm_ = prediction[0], prediction[1]\n\n\nfor x in range(nb_timesteps-1):\n    assert all( out_[0,x] == lstm_[0,x]*lstm_[0,x+1])\n'
'def predict(self, sequence):\n  presprocessed = preprocess(sequence)\n  prediction = self.model.predict(preprocessed, batch_size=None, verbose=0, steps=None)\n'
"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\nfrom sklearn.svm import LinearSVC\n\nword_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\nfeatures = FeatureUnion([('words', word_vectorizer), ])\ncalibrated_svc = CalibratedClassifierCV(LinearSVC(), method='sigmoid', cv=3)\npipeline = make_pipeline(features, calibrated_svc)\npipeline.fit(train_x, train_y)\npredicted = pipeline.predict_proba(test_x)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nword_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\nfeatures = FeatureUnion([('words', word_vectorizer), ])\npipeline = make_pipeline(features, LogisticRegression())\npipeline.fit(train_x, train_y)\npredicted = pipeline.predict_proba(test_x)\n"
"import numpy as np\nfrom sklearn.cluster import KMeans\n\nnb_clust = 10\n# your data\nX = np.random.randn(7*1000).reshape( (1000,7) )   \n\n# your 6col centroids  \ncent_6cols = np.random.randn(6*nb_clust).reshape( (nb_clust,6) ) \n\n# artificially fix your centroids\nkm = KMeans( n_clusters=10 )\nkm.cluster_centers_ = cent_6cols\n\n# find the points laying on each cluster given your initialization\ninitial_prediction = km.predict(X[:,0:6])\n\n# For the 7th column you'll provide the average value \n# of the points laying on the cluster given by your partial centroids    \ncent_7cols = np.zeros( (nb_clust,7) )\ncent_7cols[:,0:6] = cent_6cols\nfor i in range(nb_clust):\n    init_7th = X[ np.where( initial_prediction == i ), 6].mean()\n    cent_7cols[i,6] =  init_7th\n\n# now you have initialized the 7th column with a Kmeans ++ alike \n# So now you can use the cent_7cols as your centroids\ntruekm = KMeans( n_clusters=10, init=cent_7cols )\n"
'def gradient_descent(x,b,y,m,alpha):\n    for i in range (iteration):\n        h = hypothesis(x,b,y)\n        #product = np.sum(h.dot(x))\n        xvalue = x[:,1]\n        product = h.dot(xvalue)\n        hsum = np.sum(h)\n        b = b - ((alpha/m)* np.array([hsum , product]) )\n        costlist.append(cost(x,b,y,m))\n\n    return b,cost(x,b,y,m)\n'
'input_shape = (50,1)# seq_length=50\n\n(None,50,1)\n'
'test_image /= 255.0\nresult = classifier.predict(test_image)\n\nloss, acc = evaluate_generator(test_set)\n'
"nlp_input = Input(shape=(20860,))\nmeta_input = Input(shape=(35,))\nemb = Embedding(output_dim=32, input_dim=20859)(nlp_input)\nnlp_output = Bidirectional(LSTM(128, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=regularizers.l2(0.01)))(emb)\nx = Concatenate()([nlp_output, meta_input])\nlayer1 = Dense(32, activation='relu')(x)\nlayer2 = Dense(1, activation='sigmoid')(layer1)\nmodel = Model(inputs=[nlp_input , meta_input], outputs=layer2)\noptimizer=adam(lr=0.00001)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics = ['binary_accuracy'])\n\nconcatenated = keras.layers.Lambda(lambda x: keras.backend.concatenate(x))([input1, input2])\n"
"def computeCost(X,y,theta):\n    '''\n     Using Mean Absolute Error\n\n     X:(100,3)\n     y: (100,1)\n     theta:(3,1)\n     Returns 1D matrix of predictions\n     Cost = ( log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n     '''\n    m = len(y)\n    # calculate the prediction\n    predictions = sigmoid(np.dot(X,theta))\n\n    # error for when label is of class1\n    class1_cost= -y * np.log(predictions)\n    # error for when label is of class1\n    class2_cost= (1-y)*np.log(1-predictions)\n    # total cost\n    cost = class1_cost-class2_cost\n    # averaging cost\n    cost =cost.sum() / m\n    return cost\n\nimport numpy as np\n\nlabel_type1= np.random.rand(100,1)\nlabel_type2= np.random.rand(100,)\npredictions= np.random.rand(100,1)\nprint(label_type1.shape, label_type2.shape, predictions.shape)\n\n# When you mutiply (100,1) with (100,1) --&gt; (100,1)\nprint((label_type1 * predictions).shape)\n\n# When you do a dot product (100,1) with (100,1) --&gt; Error, for which you have to take a transpose which isn't relavant to the context!\n# print( np.dot(label_type1,predictions).shape) # error: shapes (100,1) and (100,1) not aligned: 1 (dim 1) != 100 (dim 0)\nprint( np.dot(label_type1.T,predictions).shape) # \nprint('*'*5)\n\n# When you mutiply (100,) with (100,1) --&gt; (100,100) !\nprint((label_type2 * predictions).shape) # \n\n# When you  do a dot product (100,) with (100,1) --&gt; (1,) !\nprint(np.dot(label_type2, predictions).shape) \nprint('*'*5)\n\n# what you are doin\nlabel_type1_addDim = np.reshape(label_type2,(-1,1))\nprint(label_type1_transpose.shape)\n"
'x0^3 x1     Degree = 4\nx0^3 x1^2   Degree = 5\nx0^2 x1^2   Degree = 4 \n'
'from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(categories = "auto", handle_unknown = \'ignore\')\nX_train_encoded = encoder.fit_transform(X_train)\nX_test_encoded = encoder.transform(X_test)\n'
"housing_data = pd.DataFrame({'age': [1,5,1,10], 'size':[0,1,2,0], \n                             'price':[190,100,50,100]\n                            })\n\nfeature_arr = housing_data.drop('price', axis=1).values\ntarget_values = housing_data['price']\n"
'import os\nimport requests\nimport io #codecs\nfrom nltk.util import everygrams\nfrom nltk.lm.preprocessing import pad_both_ends\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.lm import Laplace\nfrom nltk import word_tokenize, sent_tokenize \n\n"""\nfileTest = open("AaronPressman.txt","r");\nwith io.open(\'AaronPressman.txt\', encoding=\'utf8\') as fin:\n        textTest = fin.read()\nif os.path.isfile(\'AaronPressmanEdited.txt\'):\n    with io.open(\'AaronPressmanEdited.txt\', encoding=\'utf8\') as fin:\n        text = fin.read()\n"""\ntextTest = "This is an ant. This is a cat"\ntext = "This is an orange. This is a mango"\n\nn = 2\n# Tokenize the text.\ntokenized_text = [list(map(str.lower, word_tokenize(sent))) \n                for sent in sent_tokenize(text)]\ntrain_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n\ntokenized_text = [list(map(str.lower, word_tokenize(sent))) \n                for sent in sent_tokenize(textTest)]\ntest_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n\nmodel = Laplace(1) \nmodel.fit(train_data, padded_sents)\n\ns = 0\nfor i, test in enumerate(test_data):\n    p = model.perplexity(test)\n    s += p\n\nprint ("Perplexity: {0}".format(s/(i+1)))\n'
'Y_batch_placeholder = tf.placeholder(tf.float32 ,[None, 2] )\nX_batch_placeholder = tf.placeholder(tf.float32 ,[None, 1000, 48])\n\n\nlogits = cnn_model_fn(X_batch_placeholder, MODE)\nprediction = tf.nn.softmax(logits)\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y_batch_placeholder))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss)\ncorrect_predict = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y_batch_placeholder, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n_, los, acc = sess.run([train_op, loss, accuracy],\n                                       feed_dict={X_batch_placeholder: X_batch, Y_batch_placeholder: Y_batch})\n'
'from numpy.random import seed\nseed(1234)\nfrom tensorflow import set_random_seed\nset_random_seed(5678)\n'
'pip install --upgrade ibm-watson\n\nfrom ibm_watson import VisualRecognitionV3\n'
"serialize(NB_pipeline, 'model.pickle')\n\nNB_pipeline = deserialize('model.pickle')\n"
"from tensorflow import keras\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\n\nX_train = np.random.rand(100, 1, 10)\ny_train = np.random.randint(0, 10, 100)\ny_train = keras.utils.to_categorical(y_train)\n\nassert X_train.ndim == 3\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(10), input_shape=(X_train.shape[1:])))\nmodel.add(Bidirectional(LSTM(8)))\n\nmodel.add(Dense(8, activation='tanh'))\nmodel.add(Dense(8, activation='tanh'))\nmodel.add(Dense(y_train.shape[-1], activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=&quot;adam&quot;)\n\nhistory = model.fit(X_train, y_train, epochs=1, batch_size=8)\n\nTrain on 100 samples\n  8/100 [=&gt;............................] - ETA: 0s - loss: 2.2984\n 80/100 [=======================&gt;......] - ETA: 0s - loss: 2.2863\n100/100 [==============================] - 0s 950us/sample - loss: 2.2984\n"
"df4 = pd.read_csv('Election-Results-2018 - Parlimen_Results_By_Candidate.csv')\ndf4['Votes for Candidate'] = df4['Votes for Candidate'].str.replace(',','').astype(float)\ndf4['Total Votes Cast'] = df4['Total Votes Cast'].str.replace(',','').astype(float)\ndf4['% of total Votes'] = df4['% of total Votes'].str.replace('%','').astype(float)\n\n# Step 1 - import the model \nfrom sklearn.linear_model import LogisticRegression\n\n# Step 2 - Define your training data\ncolumns = ['Candidate Party', 'Votes for Candidate']\n\n# Step 3 - create training dataset\nX = df4[columns]\ny = df4['New Results']\n"
"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\n\nencoder = LabelEncoder()\nencoder.fit(y_train)\nencoded_y = encoder.transform(y_train)\ny_train = np_utils.to_categorical(encoded_y)\n\narray([[1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.]], dtype=float32)\n\nmodel.add(Dense(2, activation='softmax'))\n"
"param_grids = {\n                    'learning_rate'    : [0.01, 0.05, 0.07, 0.1, 0.3, 0.5 ],\n                    'n_estimators'     : [50,60,70,80,90,100],\n                    'max_depth'        : [1, 2, 3, 4],\n                    'min_samples_leaf' : [1,2,3,5,10,15],\n                    'min_samples_split': [2,3,4,5,10],  \n                    'criterion' : ['friedman_mse', 'mse']\n    }\n"
"from sagemaker.xgboost.model import XGBoostPredictor\n    \npredictor = XGBoostPredictor(endpoint_name='your-endpoint')\npredictor.predict('&lt;payload&gt;')\n"
"ind = tf.argsort(y_pred,axis=-1,direction='ASCENDING',stable=False,name=None)[-2:]\n\nindices.shape=[1,11,1]\n\ntf.scatter_nd([[1,2],[1,3]],[1,1],shape=(2,10))\n#&lt;tf.Tensor: shape=(2, 10), dtype=int32, \n#numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n#             [0, 0, 1, 1, 0, 0, 0, 0, 0, 0]])&gt;\n\nN = tf.shape(y_pred)[0]\n# Shape: [N,2]\nind = tf.argsort(y_pred,axis=-1,direction='ASCENDING',stable=False,name=None)[:,-2:]\n\n# Shape: [N,2,1]\nind = ind[...,tf.newaxis]\n\n# Dummy range to add index for batch axis\n# Shape : [N,1,1]\nr = tf.range(N)[:,tf.newaxis,tf.newaxis]\n\n# Shape : [N,2,1]\nr = tf.repeat(r,2,axis=1)\n\n# Shape : [N,2,2]\nind = tf.concat([r,ind],axis=-1)\n\n# Shape : [N,2]\nupdates = tf.ones((N,2))\n\ny_pred1 = tf.scatter_nd(ind, updates, tf.shape(y_pred))\n\nsecond_max = tf.sort(y_pred,axis=-1,direction='ASCENDING')[:,-2,tf.newaxis]\ny_pred1 = tf.cast(y_pred&gt;=second_max,tf.int32)\n"
'X_train = my_training_data[:,1:]\nY_train = my_training_data[:,0]\n\nclf.fit(X_train.tolist(), Y_train.tolist())\n'
'prec, rec, fbeta, supp = precision_recall_fscore_support(y_test, y_pred, labels=classifier.classes_)\n'
'T.train(X,y)\n\nlistofbirthweights = np.array([[3430., 3815., 3405., 2190.]]).T/5000.\n\nIn [199]: X.shape\nOut[199]: (51, 13)\n'
'# create a placeholder       \nnum_placeholder = tf.placeholder(tf.int32)\n\n# create an identity operation\n# because same Tensor can not be both fed and fetched\nnum_op = tf.identity(num_placeholder)\n\nwith tf.Session() as sess:\n    # feed value to placeholder and fetch it\n    num = sess.run([num_op], feed_dict={num_placeholder: 5})\n\nnum_placeholder = tf.placeholder(tf.int32)\nones_op = tf.ones(shape=[num_placeholder], dtype=tf.int32)\nwith tf.Session() as sess:\n    ones = sess.run([ones_op], feed_dict={num_placeholder: 5})\n'
'lb = preprocessing.MultiLabelBinarizer()\nY = lb.fit_transform(y_train_text)\n\nclassifier = Pipeline([\n    (\'vectorizer\', CountVectorizer(stop_words="english")),\n    (\'tfidf\', TfidfTransformer()),\n    (\'clf\', OneVsRestClassifier(LinearSVC()))])\n\nclassifier.fit(X_train, Y)\npredicted = classifier.predict(X_test)\nx = classifier.decision_function(X_test)\npredicted_all = sp.sign(x - x.max(1).reshape(x.shape[0], 1) + 1e-20)\npredicted_all = (predicted_all + 1)/2\nfor i in range(0, len(predicted)):\n    #if we never came up with a prediction, use our "forced" single prediction\n    if (all(v == 0 for v in predicted[i])):\n        predicted[i] = predicted_all[i]\nall_labels = lb.inverse_transform(predicted)\n'
'create_default_packages:\n  - tensorflow\n  - PACKAGE_NAME\nchannels:\n  - conda-forge\n  - defaults\n'
'    validation_data=get_next_batch(X_val, y_val),\n    validation_steps=len(X_val)\n'
'a = np.empty((number_of_examples, number_of_features))\n'
"model = Sequential()\nmodel.add(Convolution2D(32, kernel_size=(3, 3),\n                        data_format='channels_first',\n                        activation='relu', input_shape=(1, 28, 28)))\nmodel.output_shape\n\n(None, 32, 26, 26)\n"
'for i in range(9):\n    ti0 = time.time()\n    df_features_train, df_features_test = randomizer(dataFiltered)\n    rsmeMaeStep = rsmeMaeAnalysis(df_features_train,df_features_test)\n    rsmeMaeAllpd = rsmeMaeAllpd.append(rsmeMaeStep.toPandas())\n    print(rsmeMaeAllpd)\n    ti1 = time.time()\n    print "Time for loop", i, ":", ti1-ti0\n\nfor i in range(10):\n    ti0 = time.time()\n    df_features_train, df_features_test = randomizer(dataFiltered)\n    rsmeMaeStep = rsmeMaeAnalysis(df_features_train,df_features_test)\n    rsmeMaeAll = rsmeMaeAll.union(rsmeMaeStep)\n    rsmeMaeAll.show(80,False)\n    ti1 = time.time()\n    print "Time for loop", i, ":", ti1-ti0\n'
"dataset = tf.data.Dataset.from_tensor_slices(([i for i in t_df['feature']], t_df['label']))\n"
"from keras import backend as K\nfrom keras.layers import Layer\n\nclass MyLayer(Layer):\n\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Create a trainable weight variable for this layer.\n        self.kernel = self.add_weight(name='kernel', \n                                  shape=(input_shape[1], self.output_dim),\n                                  initializer='uniform',\n                                  trainable=True)\n        super(MyLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, x):\n        return K.dot(x, self.kernel)\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], self.output_dim)\n\nfrom custom.layers import MyLayer\n\nmodel = keras.Sequential()\nmodel.add(MyLayer())\n"
"params = [{'gdb__n_estimators':[10,20]}]\n"
'activation: Activation function to use (see activations). If you don\'t specify anything, no activation is applied (ie. "linear" activation: a(x) = x).\n'
"dataframe.loc[0,'T'] = thres\ndataframe.loc[0,'TN'] = tn\ndataframe.loc[0,'FP'] = fp\ndataframe.loc[0,'FN'] = fn\ndataframe.loc[0,'TP'] = tp \n"
'clf.fit(xTrain.astype("U"), xTest.astype("U"))\n\nclf.fit(xTrain.astype("U"), yTrain.astype("U"))\n'
"# load your already trained model\nmodel_x = load_model('saved_model')\n\ninp = layers.Input(shape=(2,16))\n# this makes the input shape as `(16,2)`\nx = layers.Permute((2,1))(inp)\n# this would apply `model_x` on each of the 16 segments; the output shape would be (None, 16, 8)\nx = layers.TimeDistributed(model_x)(x)\n# flatten to make it have a shape of (None, 128)\nout = layers.Flatten()(x)\n\nfinal_model = Model(inp, out)\n"
'def dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = tf.reshape(y_true, [-1])\n    y_pred_f = tf.reshape(y_pred, [-1])\n    intersection = tf.reduce_sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)\n    return score\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = np.ravel(y_true)\n    y_pred_f = np.ravel(y_pred)\n    intersection = np.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n    return 1 - score\n'
"parameters = {'C': [1, 10], \n          'gamma': [0.001, 0.01, 1]}\nmodel = SVC()\ngrid = GridSearchCV(estimator=model, param_grid=parameters)\ngrid.fit(vectors, newsgroups_train.target)\nprint(grid)\n# summarize the results of the grid search\nprint(grid.best_score_)\nprint(grid.best_estimator_)\n"
"def load_img(p):\n    img = Image.open(p).convert('RGB')\n    im_transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n    img = im_transform(img).unsqueeze(0).cuda()\n\n    return img\n\ndef to_img(t):\n    img = t.cpu().clone().detach()\n    img = img.numpy().squeeze()\n    img = img.transpose(1, 2, 0)\n    img = img.clip(0, 1)\n\n    return img\n\ndef train(...):\n...\n    style = load_img(style_img)\n...\n            if i % 5 == 0:\n                plt.title('Iter#{:04d}'.format(i))\n                plt.imshow(to_img(img))\n                plt.pause(1e-3)\n...\n&lt;etc.&gt;\n...\n"
"num_classes=9\nY_train = keras.utils.to_categorical(Y_train, num_classes)\n\nmodel.add(Dense(num_classes)) \nmodel.add(Activation('softmax')) \n"
'from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\niris = load_iris()\nclf = DecisionTreeClassifier()\n\ntrain_X, val_X, train_y, val_y = train_test_split(iris.data, iris.label, random_state = 0)\nclf.fit(train_X, train_Y)\n\npred = clf.predict(val_X)\nprint(accuracy_score(val_y, pred))\n'
'import numpy as np\ndef models(array):\n    if (array[2] - array[1]) == (array[-1] - array[-2]):\n        desc = "Model is linear"\n        val = array[-1] + (array[2] - array[1])\n    elif (array[2] / array[1]) == (array[-1] / array[-2]):\n        desc = "Model is exponential"\n        val = array[-1] * (array[2]/array[1])\n    else: \n        desc = "Model is not linear nor exponential"\n        val = np.nan\n    return desc, val\n\nc = [1,2,3,4,5]\nprint(models(c))\n\n(\'Model is linear\', 6)\n\nc = [2,4,8,16,32]\nprint(models(c))\n\n(\'Model is exponential\', 64.0)\n'
"model.add(Dense(N))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n"
'temp = np.array([[radius, texture, perimeter, area, smoothness]]) # use double brackets\nscaler = StandardScaler()\nprint(log.predict(scaler.fit_transform(temp)))\n'
'X = df.iloc[:,:-1].to_numpy()\ny =  df.iloc[:,-1:].to_numpy()\n\ndef compute_cost(X, y, params):\n    n_samples = len(y)\n    h = X @ params  # dot product\n    return (1/(2*n_samples))*np.sum((h-y)**2)\n\ndef gradient_descent(X, y, params, learning_rate, n_iters):\n    n_samples = len(y)\n    J_history = np.zeros((n_iters,1))\n\n    for i in range(n_iters):\n        params = params - (learning_rate/n_samples) * X.T @ (X @ params - y) \n        J_history[i] = compute_cost(X, y, params)\n\n    return (J_history, params)\n\nn_samples = len(y)\nmu = np.mean(X, 0)\nsigma = np.std(X, 0)\n\nX = (X-mu) / sigma # normalize\n\nX = np.hstack((np.ones((n_samples,1)),X))  # add bias term\nn_features = np.size(X,1)\n\nparams = np.zeros((n_features,1))  # initial guess\n\nn_iters = 1500\nlearning_rate = 0.01\n\ninitial_cost = compute_cost(X, y, params)\n\nprint("Initial cost is: ", initial_cost, "\\n")\n\n(J_history, optimal_params) = gradient_descent(X, y, params, learning_rate, n_iters)\n\nprint("Optimal parameters are: \\n", optimal_params, "\\n")\n\nprint("Final cost is: ", J_history[-1])\n\nplt.plot(range(len(J_history)), J_history, \'r\')\n\nplt.title("Convergence Graph of Cost Function")\nplt.xlabel("Number of Iterations")\nplt.ylabel("Cost")\nplt.show()\n'
'import pandas as pd\n\ndummy_data1 = {\n        \'C1\': [\'11\', \'12\', \'13\', \'14\', \'15\', \'16\', \'17\', \'18\', \'19\', \'20\'],\n        \'C2\': [\'A\', \'E\', \'I\', \'M\', \'Q\', \'A\', \'E\', \'I\', \'M\', \'Q\', ],\n        \'C3\': [\'B\', \'F\', \'J\', \'N\', \'R\', \'B\', \'F\', \'J\', \'N\', \'R\', ],\n        \'C4\': [\'C\', \'G\', \'K\', \'O\', \'S\', \'C\', \'G\', \'K\', \'O\', \'S\', ],\n        \'C5\': [\'D\', \'H\', \'L\', \'P\', \'T\', \'D\', \'H\', \'L\', \'P\', \'T\', ]}\n\ndf1 = pd.DataFrame(dummy_data1, columns = [\'C1\', \'C2\', \'C3\', \'C4\', \'C5\'])\n\ndummy_data2 = {\n        \'attribute\': [\'C1\', \'C2\', \'C4\', \'C5\', \'C3\', ],\n        \'missing_or_unknown\': [\'X1\', \'X2\', \'X4\', \'X5\', \'X3\', ]}\n\ndf2 = pd.DataFrame(dummy_data2, columns = [\'attribute\', \'missing_or_unknown\'])\n\ndf2_transposed = df2.transpose()\nprint("df2_transposed=\\n", df2_transposed)\ndf2_transposed.columns = df2_transposed.iloc[0]\ndf2_transposed = df2_transposed.drop(df2_transposed.index[0])\nprint("df2_transposed with HEADER Replaced=\\n", df2_transposed)\n\ndf_new = pd.concat([df2_transposed, df1])\nprint("df_new=\\n", df_new)\n'
'from sklearn.cluster import KMeans\nfrom sklearn import datasets\nimport numpy as np\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nkm = KMeans(n_clusters=3)\nkm.fit(X)\n\ndef ClusterIndices(clustNum, labels_array): #numpy \n    return np.where(labels_array == clustNum)[0]\n\nClusterIndices(1, km.labels_)\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n   17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n   34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n\nX[ClusterIndicesNumpy(1,km.labels_)]\narray([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],...[4.8, 3. , 1.4, 0.3],\n   [5.1, 3.8, 1.6, 0.2],\n   [4.6, 3.2, 1.4, 0.2],\n   [5.3, 3.7, 1.5, 0.2],\n   [5. , 3.3, 1.4, 0.2]])\n'
'x = UpSampling2D((2, 2))(x) #say here the shape is (19,30) after upsampling but you need (20,30)\nx = ZeroPadding2D(((1, 0), (0, 0)))(x) # change to ZeroPadding2D(((0, 0), (0, 1))) if you want second dimension to increase by 1\n'
"tuned_parameters = [{'alphas': alphas}]\n\ntuned_parameters = [{'alpha': alphas}]\n"
'pipeline:\n   ...&lt;other components&gt;\n    - DucklingHTTPExtractor\n      dimensions: ["email"]\n\n\ndocker run -p 8000:8000 rasa/duckling\n'
'pima = pd.read_csv("diabetes.csv", header=None, names=col_names)\n\npima = pd.read_csv("diabetes.csv") # This will import the data file with the header names from the csv, which you can change later if required.\n\npima = pima.iloc[1:]\n'
"model = Sequential()\nmodel.add(layers.Dense(32, activation='relu' , input_shape=input_shape))\n# model.add(Dropout(0.1))\nmodel.add(layers.Dense(20, activation='relu' ))\n# model.add(Dropout(0.1))\nmodel.add(layers.Dense(15, activation='relu' ))\n# model.add(Dropout(0.1))\nmodel.add(layers.Dense(4)) # default activation='linear'\n\nmodel.compile(loss='mean_squared_error',\n              optimizer=keras.optimizers.Adam())\n"
'from sklearn.metrics import r2_score\n\ny_pred = model.predict(X)\nr_squared = r2_score(y, y_pred)\nadjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n'
'from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\nclf = RandomForestClassifier(random_state = 0, class_weight="balanced")\nselector = RFE(clf, 5, step=1)\n\ncv_acc = []\n\nfor train_index, val_index in k_fold.split(X, y):\n    selector.fit(X[train_index], y[train_index])\n    pred = selector.predict(X[val_index])\n    acc = accuracy_score(y[val_index], pred)\n    cv_acc.append(acc)\n\ncv_acc\n# result:\n[1.0,\n 0.9333333333333333,\n 0.9333333333333333,\n 1.0,\n 0.9333333333333333,\n 0.9333333333333333,\n 0.8666666666666667,\n 1.0,\n 0.8666666666666667,\n 0.9333333333333333]\n'
'sns.regplot(x, y, lowess=True, ax=plt.gca())\n'
"{'Product A':0, 'Product B':1, 'Product C':2, 'NO Product':3)}\n\nrf = RandomForestClassifier(n_estimators = 100, class_weight = {0:1,1:1,2:2,3:1})\n"
"import scipy.io\nimport math\nimport numpy as np\n\ndataset = scipy.io.loadmat('dataset.mat')\n\ndata = dataset['hog_features_train'].astype('float64') # Size is [2000, 324]\nbias_term = np.ones(shape=(2000,1))\ndata = np.concatenate(( bias_term , data), axis=1) # add bias term as an extra 1 in data features\n\nlabels = dataset['superclass_labels_train'].astype('float16') # Size is [2000, 1]\nNUMBER_OF_FEATURES = data.shape[1] # Is 325\n\n# Initialize weights with last weight as bias\nw = np.random.normal(0, 0.01, NUMBER_OF_FEATURES)\n\n# linear(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ +\ndef linear(w, observation):\n    return np.matmul(observation,w)\n\n# sigmoid(𝐱) = 1 / (1 + exp(−𝐱)\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# prob(𝐱) = 1 / (1 + exp(−linear(𝐱))\ndef prob(w, observation):\n    return sigmoid(linear(w, observation))\n\n# LLF = Σᵢ(𝑦ᵢ log(prob(𝐱ᵢ)) + (1 − 𝑦ᵢ) log(1 − prob(𝐱ᵢ)))\ndef log_likelyhood(w, data, labels):\n    return np.sum(prob(w, data))\n\n# NOTE: d/dw(log(1/(1 + e^(-w * x + b)))) =  x / (1 + e^(wx+b))\ndef gradient(w, data, labels):\n    #Initialze gradient vector\n    denom = (np.exp(linear(w, data)) + 1)\n    denom = np.expand_dims(denom, axis=1) # reshape from (2000,) to (2000, 1) for broadcasting\n    gradient = np.zeros_like(w)\n    gradient[1:] = np.sum((data[:, 1:] * labels) / denom, axis=0)\n    gradient[0]  = np.sum(-1 / denom)\n    return gradient\n\nLEARNING_RATE = 0.0001\nEPOCH = 1000\n\n# Calculate the LLF\nlikelyhood = log_likelyhood(w, data, labels)\nprint('likelyhood at the beginning: ', likelyhood)\n\n# Gradient ascent algorithm\nfor i in range(EPOCH):\n    gradient1 = gradient(w, data, labels)\n    w += gradient1 * LEARNING_RATE\n\n    likelyhood = log_likelyhood(w, data, labels)\n    print('likelyhood after epoch', i + 1, ': ', likelyhood)\n"
"from keras.datasets import mnist\nfrom keras.layers import Conv2D, Dense, GlobalAveragePooling2D\nfrom keras.models import Model, Input\nfrom keras.utils import to_categorical\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train_resized = x_train.reshape((60000, 28, 28, 1))\nx_test_resized = x_test.reshape((10000, 28, 28, 1))\ny_train_hot_encoded = to_categorical(y_train)\ny_test_hot_encoded = to_categorical(y_test)\n\ninputs = Input(shape=(28,28, 1))\n\nx = Conv2D(64, (3,3), activation='relu')(inputs)\nx = Conv2D(64, (3,3), activation='relu', name='final_conv')(x)\nx = GlobalAveragePooling2D()(x)\npredictions = Dense(10, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(x_train_resized, y_train_hot_encoded, epochs=1, batch_size=256, shuffle=True, validation_split=0.3)\n\n\nimport numpy as np\nimport cv2\nimport io\nimport requests\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n# Using Keras implementation from tensorflow\nfrom tensorflow.python.keras import applications\nfrom tensorflow.python.keras.preprocessing.image import load_img, img_to_array \nfrom tensorflow.python.keras import backend as K\n\n\n# Get the layer of the last conv layer\nfianlconv = model.get_layer('final_conv')\n# Get the weights matrix of the last layer\nweight_softmax = model.layers[-1].get_weights()[0]\n\n# Function to generate Class Activation Mapping\nHEIGHT = 28\nWIDTH = 28\n\ndef returnCAM(feature_conv, weight_softmax, class_idx):\n    size_upsample = (WIDTH, HEIGHT)\n\n    # Keras default is channels last, hence nc is in last\n    bz, h, w, nc = feature_conv.shape\n\n    output_cam = []\n    for idx in class_idx:\n        cam = np.dot(weight_softmax[:, idx], np.transpose(feature_conv.reshape(h*w, nc)))\n        cam = cam.reshape(h, w)\n        cam = cam - np.min(cam)\n        cam_img = cam / np.max(cam)\n        cam_img = np.uint8(255 * cam_img)\n\n        output_cam.append(cv2.resize(cam_img, size_upsample))\n\n    return output_cam\n\nx = x_test_resized[0,:,:,0]\n\nplt.imshow(x)\nplt.show()\n\nclasses = {1:'1', 2: '2', 3: '3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9', 0:'0'}\n\nprobs_extractor = K.function([model.input], [model.output])\nfeatures_conv_extractor = K.function([model.input], [fianlconv.output])\n\nprobs = probs_extractor([np.expand_dims(x, 0).reshape(1,28,28,1)])[0]\n\nfeatures_blob = features_conv_extractor([np.expand_dims(x, 0).reshape(1,28,28,1)])[0]\n\nfeatures_blobs = []\nfeatures_blobs.append(features_blob)\n\nidx = np.argsort(probs)\nprobs = np.sort(probs)\n\nfor i in range(-1, -6, -1):\n    print('{:.3f} -&gt; {}'.format(probs[0, i], classes[idx[0, i]]))\n\n\nCAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0, -1]])\n\nheatmap = cv2.applyColorMap(cv2.resize(CAMs[0], (28, 28)), cv2.COLORMAP_JET)\n\nresult = heatmap[:,:,0] * 0.3 + x * 0.5\n\nprint(result.shape)\n\nplt.imshow(result)\nplt.show()\n"
'y = array([False, False, True, False, False, True, False, False, False, False, False, ...])\n\ndata[y]\n\nfrom sklearn import datasets\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nX, y = datasets.load_breast_cancer(return_X_y=True) # 2 classes - binary classification\nknn = KNeighborsClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nknn.fit(X_train, y_train)\nprediction = knn.predict(X_test).astype(bool)\n\npred_tested_true = X_test[prediction]\n'
'import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\n\nip = Input((2,2,3))\nmx = tf.keras.backend.max(ip, axis = -1)\na1 = tf.expand_dims(mx, -1)\ncat = Concatenate(axis = -1)([a1, a1, a1])\nip_add1 = tf.math.add(ip, 1)\nsub = Subtract()([ip_add1, cat])\nneg2zero = Activation(\'relu\')(sub)\n\nmodel = Model(ip, neg2zero)\n\nx = np.transpose(np.array([\n  [[0, 1],\n  [9, 3]],\n\n [[2, 5],\n  [5, 4]],\n\n [[4, 2],\n  [8, 7]]], dtype = np.float32)) # your matrix need to be transposed as it had wrong order\n\nprint(x)\nprint(x.shape)\ny = model(x)\nprint(y)\n\n[[[0 2 4]\n  [9 5 8]]\n\n [[1 5 2]\n  [3 4 7]]]\n(2, 2, 3)\nWARNING:tensorflow:Model was constructed with shape (None, 2, 2, 3) for input Tensor("input_20:0", shape=(None, 2, 2, 3), dtype=float32), but it was called on an input with incompatible shape (2, 2, 3).\ntf.Tensor(\n[[[0. 0. 1.]\n  [1. 0. 0.]]\n\n [[0. 1. 0.]\n  [0. 0. 1.]]], shape=(2, 2, 3), dtype=float32)\n'
"X = np.random.randint(0,10, (1000,100))\ny = np.random.randint(0,3, 1000)\n\nmodel = Sequential([\n    Dense(128, input_dim = 100),\n    Dense(3, activation='softmax'),\n])\nmodel.summary()\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(X, y, epochs=3)\n\nX = np.random.randint(0,10, (1000,100))\ny = pd.get_dummies(np.random.randint(0,3, 1000)).values\n\nmodel = Sequential([\n    Dense(128, input_dim = 100),\n    Dense(3, activation='softmax'),\n])\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory = model.fit(X, y, epochs=3)\n"
"import pandas as pd            #importing the package        \ndf = pd.read_csv(path)         #df is a variable containing the data-frame of the csv file\nydf = pd.get_dummies(df['label']) #'label' is the title of the the column\n                                  #in the csv you want to one hot encode\n\nimport numpy as np\nvector = np.arange(5)    # vector = [0 1 2 3 4]\n\none_hot = (vector == 0).astype(np.int)  #[1 0 0 0 0]\none_hot = (vector == 2).astype(np.int)  #[0 0 1 0 0]\none_hot = (vector == 4).astype(np.int)  #[0 0 0 0 1]\n\nvector = np.arange(no_of_different_labels)\n\n# transform labels into one hot representation\ny_train_one_hot = (vector == y_train).astype(np.float)\n# make sure you y_train is of size (m,1) and not (m,) for broadcasting to work\n"
'test_pred = model.predict(ImageDataGenerator().flow(X_train, y=None, batch_size=batch_size, shuffle=False), steps=math.ceil(nb_train_samples/batch_size))\n'
'import torch.nn.functional as F\n...\nprob = F.softmax(output, dim=1)\n...\n'
'import numpy as np\n\nX_tr= np.array([10.8204,  7.67418, 7.83013, 8.30996, 8.1567,  6.94831, 14.8673, 7.69338, 7.67702, 12.7542, 11.847])\ny_tr= np.array([1965.21,  854.386, 909.126, 1094.06, 1012.6,  607.299, 2294.55, 866.316, 822.948, 2255.32, 2124.67])\nX_te= np.array([7.62022, 13.1943, 7.76752, 8.36949, 7.86459, 7.16032, 12.7035, 8.99822, 6.32853, 9.22345, 11.4751])\n\nimport sklearn.gaussian_process as gp\n\nkernel = gp.kernels.ConstantKernel(1.0, (1e-1, 1e3)) * gp.kernels.RBF(10.0, (1e-3, 1e3))\n\nmodel = gp.GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, alpha=0.1, normalize_y=True)\n\n# data reshape\nX_tr = X_tr.reshape(-1,1)\ny_tr = y_tr\n\nmodel.fit(X_tr, y_tr)\nparams = model.kernel_.get_params()\n\nX_te = X_te.reshape(-1,1)\n\ny_pred, std = model.predict(X_te, return_std=True)\n'
"le = LabelEncoder()\ny = le.fit_transform(train_df[y])\n# Train your model and predict into a variable 'y_pred'\ny_pred = le.inverse_transform(y_pred)\n"
'for filename in audio_files:\n    ...\n    Features = sc.fit_transform(Features)\n\nFeatures = sc.transform(Features)\n'
'inputs = Input(7)\nx = keras.layers.Reshape((7, 1))(inputs)\n'
'network = LeNet()\n\ntorch.manual_seed(42)\nnetwork = LeNet()\n'
"group1/\n    cats/\n        breeds_5_cats001.jpeg\n        breeds_5_cats002.jpeg\n    dogs/\n        breeds_4_dogs001.jpeg\n        breeds_4_dogs002.jpeg\ngroup2/\n    cats/\n        breeds_5_cats001.jpeg\n        breeds_5_cats002.jpeg\n    dogs/\n        breeds_4_dogs001.jpeg\n        breeds_4_dogs002.jpeg\ngroup3/\n    cats/\n        breeds_5_cats001.jpeg\n        breeds_5_cats002.jpeg\n    dogs/\n        breeds_4_dogs001.jpeg\n        breeds_4_dogs002.jpeg\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\nimport os\nfrom glob2 import glob\nfrom itertools import groupby\nfrom itertools import accumulate\nimport cv2\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nimport tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')\nos.chdir('c:/users/nicol/documents/datasets/catsanddogs')\n\nfilenames = glob('*/*/*.jpg')\n\ngroups = [list(v) for k, v in groupby(sorted(filenames), key=lambda x: x.split(os.sep)[0])]\nlengths = [0] + list(accumulate(map(len, groups)))\ngroups = [i for s in groups for i in s]\n\n['group1\\\\cats\\\\cat.4001.jpg',\n 'group1\\\\cats\\\\cat.4002.jpg',\n 'group1\\\\cats\\\\cat.4003.jpg',\n 'group1\\\\cats\\\\cat.4004.jpg',\n 'group1\\\\cats\\\\cat.4005.jpg',\n 'group1\\\\cats\\\\cat.4006.jpg',\n 'group1\\\\cats\\\\cat.4007.jpg',\n 'group1\\\\cats\\\\cat.4008.jpg',\n 'group1\\\\cats\\\\cat.4009.jpg',\n 'group1\\\\cats\\\\cat.4010.jpg']\n\nimages = list()\n\nfor image in filenames:\n    array = cv2.imread(image)/255\n    resized = cv2.resize(array, (32, 32))\n    images.append(resized)\n\nX = np.array(images).astype(np.float32)\n\ny = np.array(list(map(lambda x: x.split(os.sep)[1] == 'cats', groups))).astype(int)\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dropout(0.25))\n    model.add(Dense(64))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.summary()\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['mse', 'accuracy'])\n    return model\n\n\nkeras_clf = KerasClassifier(build_fn=build_model, epochs=1, batch_size=16, verbose=0)\n\ndef three_fold_cv():\n    i = 1\n    while i &lt;= 3:\n        min_length = lengths[i - 1]\n        max_length = lengths[i]\n        idx = np.arange(min_length, max_length, dtype=int)\n        yield idx, idx\n        i += 1\n\ntfc = three_fold_cv()\naccuracies = cross_val_score(estimator=keras_clf, scoring=&quot;accuracy&quot;, X=X, y=y, cv=tfc)\n\nprint(accuracies)\n\n[0.648 0.666 0.73 ]\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\nimport os\nfrom glob2 import glob\nfrom itertools import groupby\nfrom itertools import accumulate\nimport cv2\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nimport tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')\nos.chdir('c:/users/nicol/documents/datasets/catsanddogs')\n\nfilenames = glob('*/*/*.jpg')\n\ngroups = [list(v) for k, v in groupby(sorted(filenames), key=lambda x: x.split(os.sep)[0])]\nlengths = [0] + list(accumulate(map(len, groups)))\ngroups = [i for s in groups for i in s]\n\n\nimages = list()\n\nfor image in filenames:\n    array = cv2.imread(image)/255\n    resized = cv2.resize(array, (32, 32))\n    images.append(resized)\n\nX = np.array(images).astype(np.float32)\n\ny = np.array(list(map(lambda x: x.split(os.sep)[1] == 'cats', groups))).astype(int)\n\n\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dropout(0.25))\n    model.add(Dense(64))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.summary()\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['mse', 'accuracy'])\n    return model\n\n\nkeras_clf = KerasClassifier(build_fn=build_model, epochs=1, batch_size=16, verbose=0)\n\ndef three_fold_cv():\n    i = 1\n    while i &lt;= 3:\n        min_length = lengths[i - 1]\n        max_length = lengths[i]\n        idx = np.arange(min_length, max_length, dtype=int)\n        yield idx, idx\n        i += 1\n\ntfc = three_fold_cv()\naccuracies = cross_val_score(estimator=keras_clf, scoring=&quot;accuracy&quot;, X=X, y=y, cv=tfc)\n\nprint(accuracies)\n\n[0.648 0.666 0.73 ]\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\nfrom itertools import accumulate\nimport tensorflow as tf\n\n# Here's your dataset.\n(xtrain, ytrain), (_, _) = tf.keras.datasets.mnist.load_data()\n\n# You have three groups, as you wanted. They are 20,000 each.\nx_group1, y_group1 = xtrain[:20_000], ytrain[:20_000]\nx_group2, y_group2 = xtrain[20_000:40_000:], ytrain[20_000:40_000:]\nx_group3, y_group3 = xtrain[40_000:60_000], ytrain[40_000:60_000]\n\n# You need the accumulated lengths of the datasets: [0, 20000, 40000, 60000]\nlengths = [0] + list(accumulate(map(len, [y_group1, y_group2, y_group3])))\n\n# Now you need all three in a single dataset.\nX = np.concatenate([x_group1, x_group2, x_group3], axis=0)[..., np.newaxis]\ny = np.concatenate([y_group1, y_group2, y_group3], axis=0)\n\n\n# KerasClassifier needs a model building function.\ndef build_model():\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dropout(0.25))\n    model.add(Dense(64))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.summary()\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['mse', 'accuracy'])\n    return model\n\n\n# Creating the KerasClassifier.\nkeras_clf = KerasClassifier(build_fn=build_model, epochs=1, batch_size=16, verbose=0)\n\n\n# Creating the custom Cross-validation splitter. Splits are based on `lengths`.\ndef three_fold_cv():\n    i = 1\n    while i &lt;= 3:\n        min_length = lengths[i - 1]\n        max_length = lengths[i]\n        idx = np.arange(min_length, max_length, dtype=int)\n        yield idx, idx\n        i += 1\n\naccuracies = cross_val_score(estimator=keras_clf, scoring=&quot;accuracy&quot;, X=X, y=y, cv=three_fold_cv())\n\nprint(accuracies)\n"
"img  = tf.image.random_contrast(img, .2, .5)\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\nfrom glob2 import glob\nfrom collections import deque\n\ngroup1 = glob('group1\\\\*\\\\*.jpg')\ngroup2 = glob('group2\\\\*\\\\*.jpg')\ngroup3 = glob('group3\\\\*\\\\*.jpg')\n\ngroups = [group1, group2, group3]\n\nassert all(map(len, groups))\n\ndef load(file_path):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, size=(100, 100))\n    img  = tf.image.random_contrast(img, .2, .5)\n    label = tf.strings.split(file_path, os.sep)[1]\n    label = tf.cast(tf.equal(label, 'dogs'), tf.int32)\n    return img, label\n\naccuracies_on_test_set = {}\n\nfor i in range(len(groups)):\n    d = deque(groups)\n    d.rotate(i)\n    train1, train2, test1 = d\n    train_ds = tf.data.Dataset.from_tensor_slices(train1 + train2).\\\n        shuffle(len(train1) + len(train2)).map(load).batch(4)\n    test_ds = tf.data.Dataset.from_tensor_slices(test1).\\\n        shuffle(len(test1)).map(load).batch(4)\n\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dropout(0.25))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['mse', 'accuracy'])\n\n    model.fit(train_ds, validation_data=test_ds, epochs=5, verbose=0)\n    loss, mse, accuracy = model.evaluate(test_ds, verbose=0)\n    accuracies_on_test_set[f'epoch_{i + 1}_accuracy'] = accuracy\n\nprint(accuracies_on_test_set)\n\n{'epoch_1_accuracy': 0.915, 'epoch_2_accuracy': 0.95, 'epoch_3_accuracy': 0.9}\n\ngroup1/\n    dogs/\n        dog001.jpg\n    cats/\n        cat001.jpg\ngroup2/\n    dogs/\n        dog001.jpg\n    cats/\n        cat001.jpg\ngroup3/\n    dogs/\n        dog001.jpg\n    cats/\n        cat001.jpg\n"
'svr_reg = SVR(kernel=&quot;sigmoid&quot;)\n'
"def preprocess(filepath):\n    category = tf.strings.split(filepath, os.sep)[0]\n    read_file = tf.io.read_file(filepath)\n    decode = tf.image.decode_jpeg(read_file, channels=3)\n    resize = tf.image.resize(decode, (200, 200))\n    image = tf.expand_dims(resize, 0)\n    if tf.equal(category, 'tf_astronauts'):\n        image = tf.image.flip_up_down(image)\n        image = tf.image.flip_left_right(image)\n    # image = tf.image.convert_image_dtype(image, tf.float32)\n    # category = tf.cast(tf.equal(category, 'tf_astronauts'), tf.int32)\n    return image, category\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport cv2\nfrom skimage import data\nfrom glob2 import glob\nimport os\n\ncat = data.chelsea()\nastronaut = data.astronaut()\n\nfor category, picture in zip(['tf_cats', 'tf_astronauts'], [cat, astronaut]):\n    os.makedirs(category, exist_ok=True)\n    for i in range(5):\n        cv2.imwrite(os.path.join(category, category + f'_{i}.jpg'),\n                    cv2.cvtColor(picture, cv2.COLOR_RGB2BGR))\n\nfiles = glob('tf_*\\\\*.jpg')\n\n['tf_astronauts\\\\tf_astronauts_0.jpg',\n 'tf_astronauts\\\\tf_astronauts_1.jpg',\n 'tf_astronauts\\\\tf_astronauts_2.jpg',\n 'tf_astronauts\\\\tf_astronauts_3.jpg',\n 'tf_astronauts\\\\tf_astronauts_4.jpg',\n 'tf_cats\\\\tf_cats_0.jpg',\n 'tf_cats\\\\tf_cats_1.jpg',\n 'tf_cats\\\\tf_cats_2.jpg',\n 'tf_cats\\\\tf_cats_3.jpg',\n 'tf_cats\\\\tf_cats_4.jpg']\n\ndef preprocess(filepath):\n    category = tf.strings.split(filepath, os.sep)[0]\n    read_file = tf.io.read_file(filepath)\n    decode = tf.image.decode_jpeg(read_file, channels=3)\n    resize = tf.image.resize(decode, (200, 200))\n    image = tf.expand_dims(resize, 0)\n    if tf.equal(category, 'tf_astronauts'):\n        image = tf.image.flip_up_down(image)\n        image = tf.image.flip_left_right(image)\n    # image = tf.image.convert_image_dtype(image, tf.float32)\n    # category = tf.cast(tf.equal(category, 'tf_astronauts'), tf.int32)\n    return image, category\n\ntrain = tf.data.Dataset.from_tensor_slices(files).\\\n    shuffle(10).take(4).map(preprocess).batch(4)\n\nfig = plt.figure()\nplt.subplots_adjust(wspace=.1, hspace=.2)\nimages, labels = next(iter(train))\nfor index, (image, label) in enumerate(zip(images, labels)):\n    ax = plt.subplot(2, 2, index + 1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(label.numpy().decode())\n    ax.imshow(image[0].numpy().astype(int))\nplt.show()\n"
"model.add(Dense(30, activation='softmax'))\n\nmodel.compile(loss=&quot;categorical_crossentropy&quot;,optimizer=adadelta,metrics=['accuracy'])\n"
"import tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import EfficientNetB0\n\nclass Net(tf.keras.Model):\n    def __init__(self, idim, gdim):\n        super(Net, self).__init__()\n        # image input\n        self.efnet  = EfficientNetB0(input_shape=(idim), include_top = False, weights = 'imagenet')\n        self.gap    = KL.GlobalAveragePooling2D()\n        self.bn     = KL.BatchNormalization()\n        self.denseA = KL.Dense(784, activation='relu', name = 'denseA')\n        \n        # meta information input\n        #self.gender = KL.Input(shape=(gdim), name='gender', dtype='float32')\n        self.gmeta  = KL.Dense(100, kernel_regularizer=tf.keras.regularizers.l2(l=0.01), activation='relu')\n        \n        self.cat    = KL.Concatenate()\n        self.out    = KL.Dense(1, activation='linear')\n    \n    def call(self, inputs, training=False):\n        print(inputs[0])\n        print(inputs[1])\n        \n        # image data \n        x     = self.efnet(inputs[0])\n        x_gap = self.gap(x)\n        bn    = self.bn(x_gap)\n        den_A = self.denseA(bn)\n        \n        # tabular feature \n        #x2    = self.gender(inputs[1])\n        x2    = inputs[1]\n        x3    = self.gmeta(x2)\n        \n        # cat\n        out   = self.cat([den_A, x3]) # brackets removed\n        y     = self.out(out)\n        return y\n\nidim = (224, 224, 3) # image dimension\ngdim = 2 # let's say, we've 2 feature column\nmodel = Net(idim, gdim)\nmodel.build(input_shape=[(None, *idim), (None, gdim)])\n"
"class terminate_on_plateau(keras.callbacks.Callback):\n    \n    def __init__(self):\n        self.patience = 10\n        self.val_loss = deque([],self.patience)\n        self.std_threshold = 1e-2\n        \n    def on_epoch_end(self,epoch,logs=None):\n        val_loss,val_mae = model.evaluate(x_val,y_val)\n        self.val_loss.append(val_loss)\n        if len(self.val_loss) &gt;= self.patience:\n            std = np.std(self.val_loss)\n            if std &lt; self.std_threshold:\n                print('\\n\\n EarlyStopping on std invoked! \\n\\n')\n                # clear the deque\n                self.val_loss = deque([],self.patience)\n                model.stop_training = True\n\nfrom collections import deque\nimport numpy as np\n\nimport tensorflow as tf \nfrom tensorflow import keras \nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input,Dense\n\nx = np.linspace(0,10,1000)\nnp.random.shuffle(x)\ny = np.sin(x) + x\n\nx_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.3)\n\ninput_x = Input(shape=(1,))\ny = Dense(10,activation='relu')(input_x)\ny = Dense(10,activation='relu')(y)\ny = Dense(1,activation='relu')(y)\nmodel = Model(inputs=input_x,outputs=y)\n\nadamopt = tf.keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\nclass terminate_on_plateau(keras.callbacks.Callback):\n    \n    def __init__(self):\n        self.patience = 10\n        self.val_loss = deque([],self.patience)\n        self.std_threshold = 1e-2\n        \n    def on_epoch_end(self,epoch,logs=None):\n        val_loss,val_mae = model.evaluate(x_val,y_val)\n        self.val_loss.append(val_loss)\n        if len(self.val_loss) &gt;= self.patience:\n            std = np.std(self.val_loss)\n            if std &lt; self.std_threshold:\n                print('\\n\\n EarlyStopping on std invoked! \\n\\n')\n                # clear the deque\n                self.val_loss = deque([],self.patience)\n                model.stop_training = True\n    \nmodel.compile(loss='mse',optimizer=adamopt,metrics=['mae'])\nhistory = model.fit(x_train,y_train,\n                    batch_size=8,\n                    epochs=100,\n                    validation_data=(x_val, y_val),\n                    verbose=1,\n                    callbacks=[terminate_on_plateau()])\n"
'def batch_generator(items, batch_size):\n    l = list(items)\n    for i in range(0, len(l), batch_size):\n        yield l[i:i+batch_size]\n\ndef batch_generator(items, batch_size):\n    res = []\n    for item in items:\n        res.append(item)\n        if len(res) == batch_size:\n            yield(res)\n            res = []\n    if len(res) &gt; 0:\n        yield(res)\n'
'    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ncriterion = nn.CrossEntropyLoss()\n'
'y_new = [y for y in X if y not in boot]\n\nmodels_mse=[]\nfor _ in range(100):\n    train_idx = np.random.randint(0,len(X),size=(len(X),))\n    test_idx = np.array([i for i in range(len(X)) if i not in train_idx])\n    X_train, Y_train, X_test, Y_test = X[train_idx], Y[train_idx], X[test_idx],Y[test_idx]\n    model = LogisticRegression()\n    model.fit(X_train, Y_train)\n    Y_pred = model.predict(X_test)\n    mse = MSE(Y_test, Y_pred) # replace by appropriate API/function\n    models_mse.append(mse)\n\nprint(&quot;Bootstrapped MSE={}&quot;.format(sum(models_mse)/100))\n'
"import numpy as np\nimport tensorflow as tf\n\ninput_1 = np.array(([1, 2, 3], [4, 5, 6]))\ninput_2 = np.array((['A'], ['B']))\n\ntf.data.Dataset.from_tensor_slices((input_1, input_2))\n\n&lt;TensorSliceDataset shapes: ((3,), (1,)), types: (tf.int32, tf.string)&gt;\n\nds = tf.data.Dataset.from_tensor_slices((input_1, input_2))\n\nnext(iter(ds))\n\n(&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])&gt;,\n &lt;tf.Tensor: shape=(1,), dtype=string, numpy=array([b'A'], dtype=object)&gt;)\n"
"insert = np.array([[6.1, 3.0, 4.6, 1.4]]).astype('float') \n"
'pip install sockeye==2.2.8\n'
'def prepare(file):\n    img_array = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n    new_array = cv2.resize(img_array, (1000, 754)) # shape is (1000,754)\n    # converting to RGB\n    array_color = cv2.cvtColor(new_array, cv2.COLOR_GRAY2RGB) # shape is (1000,754,3)\n    array_with_batch_dim = np.expand_dims(array_color, axis=0) # shape is (1,1000,754,3)\n    return array_with_batch_dim\n\ndef prepare(file):\n    img_array = cv2.imread(file)\n    new_array = cv2.resize(img_array, (1000, 754)) # shape is (1000,754, 3)\n    # converting to RGB\n    array_with_batch_dim = np.expand_dims(new_array, axis=0) # shape is (1,1000,754,3)\n    return array_with_batch_dim\n'
'dict = (keyword:{file:count})\n\nimport os\n# returns the next word in the file\ndef words_generator(fileobj):\n    for line in fileobj:\n        for word in line.split():\n            yield word\nword_count_dict = {}\nfor dirpath, dnames, fnames in os.walk("./"):\n    for file in fnames:\n        f = open(file,"r")\n        words = words_generator(f)\n        for word in words:\n            if word not in word_count_dict:\n                  word_count_dict[word] = {"total":0}\n            if file not in word_count_dict[word]:\n                  word_count_dict[word][file] = 0\n            word_count_dict[word][file] += 1              \n            word_count_dict[word]["total"] += 1\n\nword_count_dict["Britain"]["total"]\n\nsum([word_count_dict["Britain"][file] if file in word_count_dict else 0 for file in ["74.txt", "75.txt"]])\n\n[file for key in word_count_dict["Britain"]]\n'
"features = pipeline.named_steps['tfidf'].get_feature_names()\nprint(features[pipeline.named_steps['l1'].coef_ != 0])\n"
'd_2 = (y - a_2)# * sigmoid(z_2,derivate=True)\nd_1 = (d_2.dot(weights_2))[:,1:]*sigmoid(z_1,derivate=True)\n\nsumError = np.mean((y-a_2)**2)\n'
"predict_proba(['Hello'])\n"
'result_matrix = cv.cv_results_\n'
'from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus = ["stack over flow stack over flow text vectorization scikit", "stack over flow"]\n\nvectorizer = TfidfVectorizer()\nx = vectorizer.fit_transform(corpus) # corpus is a collection of documents\n\nprint(vectorizer.vocabulary_) # vocabulary terms and their index\nprint(x) # tf-idf weights for each terms belong to a particular document\n\n{\'vectorization\': 5, \'text\': 4, \'over\': 1, \'flow\': 0, \'stack\': 3, \'scikit\': 2}\n  (0, 2)    0.33195438857 # first document, word = scikit\n  (0, 5)    0.33195438857 # word = vectorization\n  (0, 4)    0.33195438857 # word = text\n  (0, 0)    0.472376562969 # word = flow\n  (0, 1)    0.472376562969 # word = over\n  (0, 3)    0.472376562969 # word = stack\n  (1, 0)    0.57735026919 # second document\n  (1, 1)    0.57735026919\n  (1, 3)    0.57735026919\n\ncx = x.tocoo()\ndoc_id = -1\nfor i,j,v in zip(cx.row, cx.col, cx.data):\n    if doc_id == -1:\n        print(str(j) + \':\' + "{:.4f}".format(v), end=\' \')\n    else:\n        if doc_id != i:\n            print()\n        print(str(j) + \':\' + "{:.4f}".format(v), end=\' \')\n    doc_id = i\n\n2:0.3320 5:0.3320 4:0.3320 0:0.4724 1:0.4724 3:0.4724 \n0:0.5774 1:0.5774 3:0.5774\n'
'with open(datafilename, encoding="utf8") as f:\n   # Reading file as list of lines\n   data = f.readlines()\n\n   # Removing useless whitespaces\n   data = [line.rstrip() for line in data]\n\n   # Joining lines together\n   data = \'\'.join(data)\n\n# Loading dataframe from json str\ndf = pandas.read_json(datafile)\n'
'warm_start : bool, default: False\n\nWhen set to True, reuse the solution of the previous call to fit as initialization,\notherwise, just erase the previous solution. Useless for liblinear solver.\n'
"from keras.models import Sequential\nfrom keras.layers import Dense\nimport matplotlib.pyplot as plt\nimport numpy\n\nnumpy.random.seed(12)\n\nhistory = 96\n\n\ndef generate_data():\n    data = numpy.loadtxt('datos_horarios.csv', delimiter=',', dtype=float)\n    # Normalize data.\n    data[:, -1] /= numpy.max(data[:, -1])\n    train_test_data = []\n    for i in xrange(data.shape[0] - history - 1):\n        # Include the reference value here, will be extracted later.\n        train_test_data.append(data[i:i+history+1, -1].flatten())\n    return numpy.array(train_test_data)\n\ntrain_test_data = generate_data()\n# Shuffle data set in order to randomly select training and test data.\nnumpy.random.shuffle(train_test_data)\n\nn_samples = train_test_data.shape[0]\nn_train_samples = int(0.8 * n_samples)\n\ntrain_data = train_test_data[:n_train_samples, :-1]\ntrain_data_reference = train_test_data[:n_train_samples, -1][:, None]\n\ntest_data = train_test_data[n_train_samples:, :-1]\ntest_data_reference = train_test_data[n_train_samples:, -1]\n\nprint 'Tranining data: ', train_data\nprint 'Reference values: ', train_data_reference\n\n\nmodel = Sequential()\nmodel.add(Dense(history, input_dim=history, activation='sigmoid'))\nmodel.add(Dense(history/2, activation='sigmoid'))\nmodel.add(Dense(history/4, activation='sigmoid'))\nmodel.add(Dense(history/8, activation='sigmoid'))\nmodel.add(Dense(history/16, activation='sigmoid'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='mean_squared_error', metrics=['accuracy'])\nmodel.summary()\n\nmodel.fit(train_data, train_data_reference, shuffle=True, nb_epoch=200, batch_size=10)\n\n# Use the complete data set to see the network performance.\n# Regenerate data set because it was shuffled before.\ntrain_test_data = generate_data()\ntest_data_predicted = model.predict(train_test_data[:, :-1]).flatten()\ntest_data_reference = train_test_data[:, -1]\n\nrelative_deviation = test_data_predicted/test_data_reference - 1.0\nprint 'Relative deviation: ', relative_deviation\n\nplt.figure()\nplt.plot(range(len(test_data_reference)), test_data_reference, 'b-', label='reference')\nplt.plot(range(len(test_data_predicted)), test_data_predicted, 'r--', label='predicted')\nplt.xlabel('test case #')\nplt.ylabel('predictions')\nplt.title('Reference values vs predicted values')\nplt.legend()\n\nplt.figure()\nplt.plot(range(len(test_data_predicted)), relative_deviation, 'bx', label='relative deviation')\nplt.xlabel('test case #')\nplt.ylabel('relative deviation')\nplt.title('Relative deviation of predicted values (predicted / reference - 1)')\nplt.legend()\n\nplt.show()\n"
'"-0.9148","-1.7609","0.8441","-3.0872"\n"-2.4155","-0.7446","0.7238","-1.5506"\n\n"-0.2695","0.2271","0.7103","-4.7732"\n"-0.5421","-0.6235","0.2131","-3.3143"\n\nimport os\nimport glob\nimport csv\nimport pandas as pd\n\nfiles = glob.glob(r\'D:\\temp\\.data\\43055344\\*.csv\')\n\npd.concat(\n    [pd.read_csv(files[0], header=None).assign(f=os.path.basename(os.path.splitext(f)[0]))\n     for f in files\n    ],\n    ignore_index=True\n).to_csv(r\'d:/temp/out.csv\', header=None, index=False, quoting=csv.QUOTE_ALL)\n\n"-0.9148","-1.7609","0.8441","-3.0872","a"\n"-2.4155","-0.7446","0.7238","-1.5506","a"\n"-0.9148","-1.7609","0.8441","-3.0872","b"\n"-2.4155","-0.7446","0.7238","-1.5506","b"\n'
"CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n                       [0], [0], [0], [''], [0.0]]\nlabel_tensor = features.pop(LABEL_COLUMN)\n"
"X = PCAinput          #pandas DataFrame without 'label'\ny = inputs['label']   #pandas Series only the labels\nclf = svm.SVC()\nclf.fit(X,y)\n\nclf.predict(PCAinput)\n\n['Defeat', 'Defeat', 'Defeat', 'Win', 'Draw', 'Defeat', 'Win',\n   'Defeat', 'Defeat', 'Win', 'Win', 'Defeat', 'Win', 'Draw', 'Defeat',\n   'Defeat', 'Win', 'Defeat', 'Win', 'Defeat', 'Win', 'Win', 'Win',\n   'Win', 'Draw', 'Draw', 'Win', 'Defeat', 'Win', 'Win']\n"
'ax1.scatter(p1[:,0], p1[:,1])\n'
"clf.best_estimator_.named_steps['scale'].scale_ \n"
'classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])\n'
'from hmmlearn import hmm\n# Setting the HMM structure. n_component is the number of hidden states\nmode = hmm.MultinomialHMM(n_components=2)\n# Training the model with your data\nmodel.fit(your_data) \n# Predicting the states for the observation sequence X (with Viterbi)\nZ = model.predict(your_data)\n'
'import numpy as np\n\ncur_x = 1 # Initial value\ngamma = 1e-2 # step size multiplier\nprecision = 1e-10\nprev_step_size = cur_x\n\n# test function\ndef foo_func(x):\n    y = (np.sin(x) + x**2)**2\n    return y\n\n# Iteration loop until a certain error measure\n# is smaller than a maximal error\nwhile (prev_step_size &gt; precision):\n    prev_x = cur_x\n    cur_x += -gamma * foo_func(prev_x)\n    prev_step_size = abs(cur_x - prev_x)\n\nprint("The local minimum occurs at %f" % cur_x)\n'
'&gt;&gt;&gt; le = preprocessing.LabelEncoder()\n&gt;&gt;&gt; le.fit(["paris", "paris", "tokyo", "amsterdam"])\nLabelEncoder()\n&gt;&gt;&gt; list(le.classes_)\n[\'amsterdam\', \'paris\', \'tokyo\']\n&gt;&gt;&gt; le.transform(["tokyo", "tokyo", "paris"]) \narray([2, 2, 1]...)\n&gt;&gt;&gt; list(le.inverse_transform([2, 2, 1]))\n[\'tokyo\', \'tokyo\', \'paris\']\n'
"def __getstate__(self):\n    try:\n        state = super(BaseEstimator, self).__getstate__()\n    except AttributeError:\n        state = self.__dict__.copy()\n\n    if type(self).__module__.startswith('sklearn.'):\n        return dict(state.items(), _sklearn_version=__version__)\n    else:\n        return state\n\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\niris = datasets.load_iris()\nX = iris.data[:100, :2]\ny = iris.target[:100]\nmodel = SVC()\nmodel.fit(X,y)\nimport pickle\nwith open('mymodel','wb') as f:\n    pickle.dump(model,f)\n"
'import math\n\npredict_size_validation = int(math.ceil(nb_validation_samples / float(batch_size)))\n\nimport math\nfrom __future__ import division\n\npredict_size_validation = int(math.ceil(nb_validation_samples / batch_size))\n\npredict_size_validation = nb_validation_samples // batch_size\nif nb_validation_samples % batch_size != 0:\n    predict_size_validation += 1\n'
'import re\npattern = r"(\\w+)(\\t)(.*)(\\b)"\n'
'import numpy as np\n\nX_amount = data["Amount"].as_matrix().reshape(-1, 1)\nX_train = X_train.toarray()\nX_train = np.hstack((X_train, X_amount))\nX_test_amount = test_data["Amount"].as_matrix().reshape(-1, 1)\nX_test = X_test.toarray()\nX_test = np.hstack((X_test, X_test_amount)) \n\nimport scipy as sp\n\nX_amount = data["Amount"].as_matrix().reshape(-1, 1)\nX_train = sp.sparse.hstack((X_train, X_amount))\nX_test_amount = test_data["Amount"].as_matrix().reshape(-1, 1)\nX_test = sp.sparse.hstack((X_test, X_test_amount)) \n'
'from sklearn.model_selection import cross_val_predict, cross_val_score, KFold\nfrom sklearn.linear_model import LassoCV, Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_boston\n\nX, y = load_boston(return_X_y=True)\nkf = KFold(10, random_state=0, shuffle=False)\n\nfor train_i, test_i in kf.split(X):\n    print(train_i, test_i)\n\n(array([ 51,  52,  53, ... , 505], dtype=int64), array([ 0,  1,  2, ... , 50], dtype=int64)) ... (array([ ... ]))\n\nmodel_lassocv = LassoCV(cv=kf).fit(X, y)\nmodel_lassocv.mse_path_\n\n  array([[  41.74173819,   29.78409579,   32.00672122,  191.68560655,\n           103.46618603,  172.62108062,   16.92365434,  181.06822315,\n           116.83656233,   35.92813347], [ ... ], ... ])\n\n  lasso = Lasso(alpha = model_lassocv.alphas_[0])\n  lasso.fit(X[51:506], y[51:506])\n  predict = lasso.predict(X[0:51])\n  mean_squared_error(y_pred=predict, y_true=y[0:51])\n'
'input_labels=sess.run(tf.one_hot(cur_labels_batch,depth=2,on_value=1,off_value=0))\n\none_hot = tf.one_hot(cur_labels_batch,depth=2,on_value=1,off_value=0)\n# other necessary code here\nwhile training:\n    # ....\n    other_ops_result, input_labels = sess.run([other_ops, one_hot])\n'
'state_size = 10\nnum_layers = 3\n\nX = tf.placeholder(tf.float32, [None, 100, 10])\n\n# the second dimension is size 2 and represents\n# c, m ( the cell and hidden state ) \n# set the batch_size to None\nstate_placeholder = tf.placeholder(tf.float32, [num_layers, 2, \n                                    None, state_size])\n# l is number of layers placeholders \nl = tf.unstack(state_placeholder, axis=0)\n\nthen we create a tuple of LSTMStateTuple for each layer\nrnn_tuple_state = tuple(\n         [rnn.LSTMStateTuple(l[idx][0],l[idx][1])\n          for idx in range(num_layers)]\n)\n\n# I had to set resuse = True here : tf.__version__ 1.7.0\ncells  = [rnn.LSTMCell(10, reuse=True)] * num_layers\nmc = rnn.MultiRNNCell(cells, state_is_tuple=True)\n\noutputs, state = tf.nn.dynamic_rnn(cell=mc,\n                                   inputs=X,\n                                   initial_state=rnn_tuple_state,\n                                   dtype=tf.float32)\n'
'X = mnist.train.images\ny = mnist.train.labels\n\ndef flip_images(X_imgs):\n\n    X_flip = []\n    tf.reset_default_graph()\n    X = tf.placeholder(tf.float32, shape = (28, 28, 1))\n\n    input_d = tf.reshape(X_imgs, [-1, 28, 28, 1])\n    tf_img1 = tf.image.flip_left_right(X)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())        \n        for img_ind in range(input_d.shape[0]):\n            img = input_d[img_ind].eval()\n            flipped_imgs = sess.run([tf_img1], feed_dict={X: img})\n            X_flip.extend(flipped_imgs)\n    X_flip = np.array(X_flip, dtype = np.float32)\n    return X_flip\n\nflip = flip_images(X)\n'
'from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state=0)\n'
'print(encoder) # Tensor("model_1/conv2d_3/Sigmoid:0", shape=(?, 14, 28, 1), dtype=float32)\n'
'load_data = pd.read_csv(\'data.txt\',sep = ",",header = None)\nfeature_vale = load_data[0]\ny = np.matrix(load_data[1])\nm = len(feature_vale)\n#print(m)\n#plt.scatter(load_data[0],load_data[1])\ndf = pd.DataFrame(pd.Series(1,index= range(0,m)))\ndf[1] = load_data[0]\nX = np.matrix(df)\nrow_theta = np.zeros(2,dtype = int)\ntheta = np.array([row_theta]) # Transpose the array\nprint(theta.T)\nprediction = np.matmul(X,theta.T)\nerror = (prediction-y)\nerror_df = pd.DataFrame(error)\nsquared_error = np.square(error_df)\nprint(squared_error)\n'
"from sklearn.neural_network import MLPClassifier\nimport numpy as np\nX = [[[0., 0.], [0., 0.]], [[1., 1.], [1., 1.]]]\ny = [0, 1]\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n            hidden_layer_sizes=(5, 2), random_state=1)\nX_new = []\nfor i in X:\n    temp =[] \n    for j in i:\n        for k in j:\n            temp.append(k)\n    X_new.append(temp)\nclf.fit(X_new,y)\n"
"import pandas as pd\ndf=pd.DataFrame({'Gender':['M','F','I']}) \npd.get_dummies(df)\n\nGender_F    Gender_I    Gender_M\n0           0           1\n1           0           0\n0           1           0\n"
"df['predictions'] = y_pred_class\n"
'imputer.transform\n\nimputer.transform()\n'
"model.add(Dense(1, activation = 'linear'))\n"
"iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')\nfeature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\nX = iris_df[feature_cols] \ny = iris_df['species']\npredictions= {}\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=6)\n\nk = 3\nmy_knn_for_cs4661 = KNeighborsClassifier(n_neighbors=k)\n\nfor col in feature_cols:\n    my_knn_for_cs4661.fit(X_train[col].values.reshape(-1,1), y_train)\n    y_predict = my_knn_for_cs4661.predict(X_test[col].values.reshape(-1,1))\n    predictions[col] = accuracy_score(y_test, y_predict)\n\n\nprint(predictions)\n"
"func(estimator, X, y)\n\ndef accuracy_2_cv(estimator, X, y_labels):\n    n=2\n    y_probs = estimator.predict_proba(X)\n    class_names = estimator.classes_\n    cons_preds = [top_n_consolidation(y_labels[i], y_probs[i,:], class_names, n) for i in range(y_probs.shape[0])]\n    return accuracy_score(y_true=y_labels, y_pred=cons_preds)\n\ncustom_scoring = {'acc'       : 'accuracy',\n                  'acc2'      : accuracy_2_cv}\n"
'def __init__(self):\n    self.w = None  # define the prop here...\n    ....\n\ndef fit(self, x, y):\n    """Fit a model to data x with targets y"""\n    ...\n    # model weights w are calculated here\n    self.w = your_computed_value\n\ndef predict(self, x):\n    """Predict the target variable of the data x using trained weights w"""\n    ...\n    # predicted y values, y_pred, are calulated here\n    do_something_here(self.w)\n    return y_pred\n'
'inputs = Input(input_shape)\nx = inputs\n\n# if the input is a single image,\n# reshape it to a sequence of length one\nif len(input_shape) == 3:\n    x = Reshape((1,) + input_shape)(x)\n\n# the rest is the same\n'
'model.predict([192, 25, 651])\n\nmodel.predict([[[192, 25, 651]]])\n\nmodel.predict(np.array([[192, 25, 651]]))\n'
'training_iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\n\ninput_data, output_data = training_iterator.get_next()\ntrain_init = training_iterator.make_initializer(dataset)\n'
"C=[1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3, 1e4]\nfor i in C:                                             # 1st change\n    logisticl2 = LogisticRegression(penalty='l2',C=i)   # 2nd change\n    logisticl2.fit(X_train,Y_train)\n    probs = logisticl2.predict_proba(X_test)\n"
'./fasttext supervised -input ~/PycharmProjects/Pcat/input.txt -output model\n'
'from keras.layers import concatenate\n\nup6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)\n'
"classifier = Sequential()\n\nclassifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = \n'relu'))\n\nclassifier.add(MaxPooling2D(pool_size = (2, 2)))\n\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Flatten())\n\nclassifier.add(Dense(units = 128, activation = 'relu'))\n\nclassifier.add(Dense(units = 64, activation = 'relu'))\n\nclassifier.add(Dense(units = 4, activation = 'softmax'))\n"
'import weka.core.jvm as jvm\nfrom weka.core.converters import Loader\nfrom weka.classifiers import Classifier\n\ndef get_weka_prob(inst):\n    dist = c.distribution_for_instance(inst)\n    p = dist[next((i for i, x in enumerate(inst.class_attribute.values) if x == \'DONE\'), -1)]\n    return p\n\njvm.start()\n\nloader = Loader(classname="weka.core.converters.CSVLoader")\ndata = loader.load_file(r\'.\\recs_csv\\df.csv\')\ndata.class_is_last()\n\ndatatst = loader.load_file(r\'.\\recs_csv\\dftst.csv\')\ndatatst.class_is_last()\n\nc = Classifier("weka.classifiers.trees.J48", options=["-C", "0.1"])\n\nc.build_classifier(data)\nprint(c)\nprobstst = [get_weka_prob(inst) for inst in datatst]\n\njvm.stop()\n'
'LSTM(..., return_sequences=False)\n'
'import pandas as pd\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n[...]\n\ndf = pd.DataFrame({\'BMI\': data[\'BMI\'], \'Target\': data[\'Y\']}).sort_values(\'BMI\')\n\nmodel = LinearRegression(fit_intercept=True)\nmodel.fit(data[[\'BMI\']], data[\'Y\'])\n\nx_test = np.linspace(data[\'BMI\'].min(), data[\'BMI\'].max())\ny_pred = model.predict(x_test[:, np.newaxis])\n\nplt.scatter(df[\'BMI\'].values, df[\'Target\'].values)\nplt.plot(x_test, y_pred, linestyle="-", color="red")\nplt.show()\n'
'exog_params = model_fit.params[model.k_trend:model.k_trend + model.k_exog]\n'
'tensorflow.python.framework.errors_impl.NotFoundError: Failed to create a directory: model_checkpoints_5000/XXX; No such file or directory\n'
'def cal_miou(pred_mask, sample_mask):\n    tp = np.sum(cv2.bitwise_and(pred_mask, sample_mask))\n    fp = np.sum(cv2.bitwise_and(pred_mask, cv2.bitwise_not(sample_mask)))\n    fn = np.sum(cv2.bitwise_and(cv2.bitwise_not(pred_mask), sample_mask))\n    return tp/(tp+fp+fn)\n'
'import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\n\ntrain_data = np.array([\n    [-30,-60,-90,0,0],\n    [-50,-50,-50,10,0],\n    [-90,-60,-30,20,0],\n    [-50,-50,-95,0,10],\n    [-60,-30,-60,10,10],\n    [-95,-50,-50,20,10],\n])\n\ntest_data_x = np.array([\n    [-35,-50,-90],\n])\n\nx = train_data[:, :3]\ny = train_data[:, 3:]\nforest = RandomForestRegressor(n_estimators=100, random_state=1)\nclassifier = MultiOutputRegressor(forest, n_jobs=-1)\nclassifier.fit(x,y)\nprint(classifier.predict(test_data_x))\n'
'import numpy as np\n\nclass EW_learner():\n    #p is the probability distribution (from which we chose action C or D)\n    #pp are the potential payoffs (if 1 action was taken consistently)\n    #s is my real payoff or score (i.e. negated total prison years)\n    ##pp[0] and action = 0 is cooperate (C) =&gt; (0=C)\n    ##pp[1] and action = 1 is defect (D) =&gt; (1=D)\n    #ps is a list of the probability distribution at each round\n    #ss is a list of the scores at each round\n    def __init__(self, lr, p=[0.5,0.5], pp=[0,0], s=0):\n        self.lr = lr\n        self.p = p\n        self.pp = pp\n        self.s = s\n        self.ps = []\n        self.ss = []\n\n[0, -5, -6, -7, -8, -9, -10, -11, -12, -13]\n'
"space = {\n\n    'warm_start' : hp.choice('warm_start', [True, False]),\n    'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n    'tol' : hp.uniform('tol', 0.00001, 0.0001),\n    'C' : hp.uniform('C', 0.05, 3),\n    'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']),\n    'max_iter' : hp.choice('max_iter', range(5,1000))\n}\n"
"df['CC_count'] = df.CC.str.count(';').add(1)\n\nprint(df)\n                                               CC                    Body  \\\n0  indiejesse.d@gmail.com; pamelasilvera69@gmail....  conference meeting ...   \n\n   CC_count  \n0         3\n"
'from sklearn.metrics import jaccard_score, accuracy_score\n\nprint(a)\narray([[1, 1, 1, 1, 2, 0, 1, 0],\n       [2, 1, 1, 0, 1, 1, 0, 1]])\n\naccuracy_score(a[0,:], a[1,:])\n# 0.25\n\n(a[0,:] == a[1,:]).sum()/a.shape[1]\n# 0.25\n'
'|+|+|+|+|-|\n^  \n\n|+|+|+|+|-|\n  ^  \n\n|+|+|+|+|-|\n        ^\n\n|+|+|+|+|-|\n          ^\n'
"2014-21-3 XYZ\n2014-22-3 XYZ\n2014-23-3 XYZ\n\nlabel = ['up','down','up','up','down']\n\nmap_label = {'up':1,'down':0}\n\n[map_label[l] for l in label]\n# ===&gt; [1, 0, 1, 1, 0]\n\nn_sample = 10\ntimestemp = 5\nn_features = 3\nX = np.random.uniform(0,1, (n_sample, timestemp, n_features))\ny = np.random.randint(0,2, n_sample)\n\ninp = Input((timestemp,n_features))\nx = LSTM(8, activation='relu')(inp)\nout = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inp, out)\nmodel.compile('adam', 'binary_crossentropy')\nmodel.fit(X,y, epochs=3)\n\npreds = model.predict(X) # (n_samples, 1)\ny_classes = ((preds &gt; 0.5)+0).ravel() # Up == 1 and Down == 0\n\nn_sample = 10\ntimestemp = 5\nn_features = 3\nX = np.random.uniform(0,1, (n_sample, timestemp, n_features))\ny = np.random.randint(0,2, n_sample)\n\ninp = Input((timestemp,n_features))\nx = LSTM(8, activation='relu')(inp)\nout = Dense(2, activation='softmax')(x)\n\nmodel = Model(inp, out)\nmodel.compile('adam', 'sparse_categorical_crossentropy')\nmodel.fit(X,y, epochs=3)\n\npreds = model.predict(X) # (n_samples, n_class)\ny_classes = np.argmax(preds , axis=1)  # Up == 1 and Down == 0\n"
'confusion_matrix = confusion_matrix.reindex(confusion_matrix.index, axis=1, fill_value=0)\n'
"model = tf.keras.models.load_model('covid_model.hdf5')\n\ndef import_and_predict(image_data, model):\n    size = (160,160)    \n    image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\n    image = image.convert('RGB')\n    image = np.asarray(image)\n    image = (image.astype(np.float32) / 255.0)\n\n    img_reshape = image[np.newaxis,...]\n\n    prediction = model.predict(img_reshape)\n    \n    return prediction\n"
'import numpy as np\nfrom sklearn.metrics import r2_score\n\nx = np.array([2.3])\ny = np.array([2.1]) # exact values do not matter\n\nr2_score(x, y)\n\nUndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n  warnings.warn(msg, UndefinedMetricWarning)\n\nnan\n\nR^2 = 1 - (total_sum_squares)/(residual_sum_squares)\n'
'df = pd.read_csv(r\'C:\\Users\\WELCOME\\Desktop\\FinalYearPaper\\ConferencePaper\\NewTrain.csv\', \'rU\', delimiter=" ",header=None)\n'
'booster: string\nSpecify which booster to use: gbtree, gblinear or dart.\n'
'import tensorflow as tf\n\nxx = (\n    [178.72, 218.38, 171.1],\n    [211.57, 215.63, 173.13],\n    [196.25, 196.69, 116.91],\n    [121.88, 132.07, 85.02],\n    [117.04, 135.44, 112.54],\n    [118.13, 124.04, 97.98],\n    [116.73, 125.88, 99.04],\n    [118.75, 125.01, 110.16],\n    [109.69, 111.72, 69.07],\n    [76.57, 96.88, 67.38],\n    [91.69, 128.43, 87.57],\n    [117.57, 146.43, 117.57]\n)\n\nyy = (212.09, 195.58, 127.6, 116.5, 117.95, 117.55, 117.55,\n      110.39, 74.33, 91.08, 121.75, 127.3)\n\nx = tf.placeholder(tf.float32, [None, 3])\ny = tf.placeholder(tf.float32, [None])\n\n\ndef neuralnetwork(data, n1=5, n2=5):\n    hl1 = {\'weights\': tf.Variable(tf.random_normal([3, n1])), \'biases\':\n           tf.Variable(tf.random_normal([n1]))}\n\n    hl2 = {\'weights\': tf.Variable(tf.random_normal([n1, n2])),\n           \'biases\': tf.Variable(tf.random_normal([n2]))}\n\n    op = {\'weights\': tf.Variable(tf.random_normal([n2, 1])), \'biases\':\n          tf.Variable(tf.random_normal([1]))}\n\n    l1 = tf.add(tf.matmul(data, hl1[\'weights\']), hl1[\'biases\'])\n    l1 = tf.nn.relu(l1)\n    l2 = tf.add(tf.matmul(l1, hl2[\'weights\']), hl2[\'biases\'])\n    l2 = tf.nn.relu(l2)\n    output = tf.matmul(l2, op[\'weights\']) + op[\'biases\']\n    return output\n\n\nN_EPOCHS = 100\nif __name__ == \'__main__\':\n    pred = neuralnetwork(x)\n    loss = tf.reduce_mean(tf.squared_difference(pred, y))\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for epoch in range(N_EPOCHS):\n            epoch_loss = sess.run([train, loss], feed_dict={x: xx, y: yy})[1]\n            print("Epoch", epoch, " completed out of", N_EPOCHS, "loss:",\n                  epoch_loss)\n\npredictions = sess.run(pred, feed_dict={x: xx, y: yy})\nprint("Predictions:", predictions)\n'
"# second lstm\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(TimeDistributed(Dense(256)))\nmodel.add(Dropout(0.3))\nmodel.add(TimeDistributed(Dense(n_vocab_string_notes, activation='softmax')))\n"
'data = np.array(Ytrain)\nYtrain1 = pd.Series(data)\nN1 = len(Ytrain1)\nT1 = np.zeros((N1, K))\nfor i in range(N1): \n   print(i, Ytrain1[i]) # Prints fine \n   T1[i, Ytrain1[i]] = 1 \n'
'sess = K.get_session()\nwith sess.as_default():\n    output = model_output.eval()\n    print(output)\n'
'import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({\n    "category": ["a", "a", "a", "a", "b", "b", "b", "b"],\n    "weight": [1.0, 1.0, np.nan, 2.0, np.nan, 3.0, 3.0, 3.0]\n})\nprint df\n\ndf["weight"].fillna(df.groupby("category")["weight"].transform("median"), inplace=True)\nprint df\n\n  category  weight\n0        a     1.0\n1        a     1.0\n2        a     NaN\n3        a     2.0\n4        b     NaN\n5        b     3.0\n6        b     3.0\n7        b     3.0\n\n  category  weight\n0        a     1.0\n1        a     1.0\n2        a     1.0\n3        a     2.0\n4        b     3.0\n5        b     3.0\n6        b     3.0\n7        b     3.0\n'
'my_output = sigmoid(np.dot(my_input, weights))\n\nmy_input = [0.3,-0.1,0.1]\nprediction: [1.]\nmy_input = [0.5,.3,0]\nprediction: [1.]\nmy_input = [0.0,-.4,0.0]\nprediction: [2.25648121e-13]\n'
"loss_and_val_loss = []\nfor i in range(...):\n    # ...\n    loss_values = history_dict['loss']\n    val_loss_values = history_dict['val_loss']\n    loss_and_val_loss.append((loss_values, val_loss_values))\n# ...\n\n# (Example data with two trials, each with 3 epochs)\nloss_and_val_loss = [\n    ([1, 2, 3], [4, 5, 6]),\n    ([7, 8, 9], [10, 11, 12]),\n]\nlosses, val_losses = zip(*loss_and_val_loss)\nlosses_df = pd.DataFrame(losses)\nval_losses_df = pd.DataFrame(val_losses)\n"
'import pandas as pd\nimport numpy as np\nimport pylab as pl\nfrom sklearn.metrics import roc_curve, auc\n\ndf = pd.read_csv(\'filename.csv\')\n\ny_test = np.array(df)[:,0]\nprobas = np.array(df)[:,1]\n\n# Compute ROC curve and area the curve\nfpr, tpr, thresholds = roc_curve(y_test, probas)\nroc_auc = auc(fpr, tpr)\nprint("Area under the ROC curve : %f" % roc_auc)\n\n# Plot ROC curve\npl.clf()\npl.plot(fpr, tpr, label=\'ROC curve (area = %0.2f)\' % roc_auc)\npl.plot([0, 1], [0, 1], \'k--\')\npl.xlim([0.0, 1.0])\npl.ylim([0.0, 1.0])\npl.xlabel(\'False Positive Rate\')\npl.ylabel(\'True Positive Rate\')\npl.title(\'Receiver operating characteristic\')\npl.legend(loc="lower right")\npl.show()\n'
'-1 3:1 11:1 14:1 19:1 39:1 42:1 55:1 64:1 67:1 73:1 75:1 76:1 80:1 83:1 \n-1 3:1 6:1 17:1 27:1 35:1 40:1 57:1 63:1 69:1 73:1 74:1 76:1 81:1 103:1 \n-1 4:1 6:1 15:1 21:1 35:1 40:1 57:1 63:1 67:1 73:1 74:1 77:1 80:1 83:1 \n-1 5:1 6:1 15:1 22:1 36:1 41:1 47:1 66:1 67:1 72:1 74:1 76:1 80:1 83:1 \n-1 2:1 6:1 16:1 22:1 36:1 40:1 54:1 63:1 67:1 73:1 75:1 76:1 80:1 83:1 \n-1 2:1 6:1 14:1 20:1 37:1 41:1 47:1 64:1 67:1 73:1 74:1 76:1 82:1 83:1 \n\n&gt;&gt;&gt; from sklearn.datasets import load_svmlight_file\n&gt;&gt;&gt; X_train, y_train = load_svmlight_file(&quot;/path/to/train_dataset.txt&quot;)\n'
'page_vecs[:-1, :]\n'
"&gt;&gt;&gt; from sklearn.feature_extraction.text import CountVectorizer\n&gt;&gt;&gt; corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n&gt;&gt;&gt; vectorizer = CountVectorizer()\n&gt;&gt;&gt; X = vectorizer.fit_transform(corpus)\n&gt;&gt;&gt; print(vectorizer.get_feature_names())\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n&gt;&gt;&gt; print(X.toarray())  \n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n"
'from sklearn.datasets import load_iris\nfrom sklearn import tree\n\niris = load_iris()\nclf = tree.DecisionTreeClassifier(random_state=0).fit(iris.data, iris.target)\n\n\ntree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names)\n'
'image = tf.expand_dims(image, axis=0)   # the shape would be (1, 224, 224, 3)\nprint(model.predict_classes(image)[0])\n'
"df['assigned_label'] = df.groupby('Id')['label']\\\n                         .transform(lambda x: x.mode()[0] if len(x.mode()) == 1 else 'R')\n\n   Id_hour Id hour label assigned_label\n0      A_1  A    1     H              L\n1      A_2  A    2     L              L\n2      A_3  A    3     L              L\n3      A_4  A    4     L              L\n4      B_1  B    1     H              H\n5      B_2  B    2     H              H\n6      B_3  B    3     H              H\n7      B_4  B    4     L              H\n8      C_1  C    1     H              R\n9      C_2  C    2     H              R\n10     C_3  C    3     L              R\n11     C_4  C    4     L              R\n\u200b\n"
"count = CountVectorizer(lowercase = False, token_pattern = '[a-zA-Z0-9$&amp;+,:;=?@#|&lt;&gt;.^*()%!-]+')\n\n['#good', '@friend', 'Hello', 'a', 'day', 'good', 'is', 'this']\n"
'from keras.layers import TimeDistributed\n\ninp = Input(shape=(3, 96, 96, 3))\nout = TimeDistributed(the_trained_face_rec_model)(inp)\n\nmodel = Model(inp, out)\n'
'for root, dirs, files in os.walk(folder_path)\n    for filename in files:\n        img= os.path.join(root, filename)\n'
"import numpy as np\nfrom tensorflow.python.keras.layers import Input, Dense\nfrom tensorflow.python.keras.models import Model\n\nX=np.random.random(size=(100,1))\ny=np.random.randint(0,100,size=(100,3)).astype(float)   #Regression\n\ninput1 = Input(shape=(1,))\nl1 = Dense(10, activation='relu')(input1)\nl2 = Dense(50, activation='relu')(l1)\nl3 = Dense(50, activation='relu')(l2)\nout = Dense(3)(l3)\n\nmodel = Model(inputs=input1, outputs=[out])\nmodel.compile(\n    optimizer='adam',\n    loss=['mean_squared_error']\n    )\n\nhistory = model.fit(X, [y], epochs=10, batch_size=64)\n"
'git clone https://github.com/udacity/ud120-projects.git\n\ncd ud120-projects/k_means/\n'
"from sklearn.model_selection import train_test_split\n#features\nX = pd.get_dummies(df[['Name','Location']])\n\n#Target\ny = df['Age']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n\nfrom sklearn.svm import SVC\n\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)\ny_pred = svclassifier.predict(X_test)\n"
"selected = train[ (label == 0) | (label == 1) ]\n\nimport numpy as np\n\ntrain = np.array(['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1'])\nlabel = np.array([0, 0, 1, 2])\n\nselected = train[ (label == 0) | (label == 1) ]\n\nprint(selected)\n\nselected = train['item'][ (label['val'] == 0) | (label['val'] == 1) ]\n\nimport pandas as pd\n\ntrain = pd.DataFrame({'item': ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1']})\nlabel = pd.DataFrame({'val': [0, 0, 1, 2]})\n\nselected = train['item'][ (label['val'] == 0) | (label['val'] == 1) ]\n\nprint(selected)\n\nimport pandas as pd\n\ndf = pd.DataFrame({\n    'train': ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1'],\n    'label': [0, 0, 1, 2]\n})\n\nselected = df['train'][ (df['label'] == 0) | (df['label'] == 1) ]\n\nprint(selected)\n\ntrain = ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1']\nlabel = [0, 0, 1, 2]\n\nselected = []\n\nfor t, l in zip(train, label):\n    if l in (0, 1):\n        selected.append(t)\n\nprint(selected)\n\ntrain = ['digit0-1', 'digit0-2', 'digit1-1', 'digit2-1']\nlabel = [0, 0, 1, 2]\n\nselected = [t for t, l in zip(train, label) if l in (0, 1)]\n\nprint(selected)\n"
'padded_images = np.ones((7,29*29+1))\npadded_images[:,1:] = images.reshape(7,29*29)\n'
"from sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nscoring = ['precision_macro', 'recall_macro']\nclf = svm.SVC(kernel='linear', C=1, random_state=0)\nscores = cross_validate(clf, iris.data, iris.target, scoring=scoring,cv=5)\n"
"# get all sixth elements\nplabel = [x[6] for x in dataset]\nle.fit(plabel)\nplabel = le.transform(plabel)\n# replace sixth elements from each list inside 'dataset' with encoded label\ndataset_encoded = [[plabel[i] if indx==6 else elem for indx, elem in enumerate(x)] for i,x in enumerate(dataset)]\n&gt;&gt;&gt; dataset_encoded\n\n[[32.3, 33.5, 34.2, 35.3, 35.3, 35.7, 2],\n [52.3, 52.5, 53.2, 54.8, 55.3, 55.3, 3],\n [100.3, 110.2, 112.3, 132.5, 142.3, 153.5, 1],\n [153.5, 142.3, 132.5, 112.3, 110.2, 0, 0],\n [33.2, 34.5, 34.6, 35.3, 35.3, 35.8, 2],\n [33.2, 35.2, 35.4, 36.0, 36.2, 42.3, 4]]\n"
'embedding_dictionary[word]\n\nembedding_dictionary.vectors\n'
'np.random.seed(42)\n\nnp.random.shuffle(array)\n'
'conda create -c conda-forge -c powerai -n gymenv gym swig pip pystan\n'
'import tkinter as Tk\nfrom tkinter import filedialog\nfrom tensorflow import keras\nimport vector_build\n\nmodel = keras.models.load_model("anti_virus_model.h5")\n\n\ndef predict_file(fname):\n    print(fname)  # Debugging\n    pe = vector_build.encode_pe(fname)\n    print(pe)  # Debugging\n    result = model.predict(pe)\n    print(result)  # Debugging\n    return result\n\n\ndef browse_file():\n    fname = filedialog.askopenfilename(filetypes=(("exe files", "*.exe"),))\n    result = predict_file(fname)\n    # TODO: Do something with `result`\n\n\ndef ui_main():\n    root = Tk.Tk()\n    root.wm_title("Browser")\n    broButton = Tk.Button(master=root, text="Browse", width=80, height=25, command=browse_file)\n    broButton.pack(side=Tk.LEFT, padx=2, pady=2)\n\n    Tk.mainloop()\n\n\nif True:  # First make this branch work correctly,\n    predict_file("C:/Windows/Calc.exe")\nelse:  # ... then switch to this.\n    ui_main()\n'
"col1, col2\n1       2\n4       8\n16      32\n64      128\n\ncol1, col2, col1_next, col2_next\n1       2       4           8\n4       8       16          32\n16      32      64          128\n\ncol1, col2, col1_next, col2_next\n64     128      ?          ?\n\nimport mindsdb\nimport pandas as pd\n\n\npredictor = mindsdb.Predictor(name='example')\npredictor.learn(from_data='&lt;your data source&gt;', to_predict=['col1_next', 'col2_next'])\n\npredictor.predict(when={'col1': 64, 'col2':128}) # This will yield a prediction for col1_next and col2_next\n"
"res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),\n                       param_grid=param_grid,\n                       cv=LeaveOneOut(),\n                       verbose=20,\n                       n_jobs=-1, \n                       scoring = 'neg_root_mean_squared_error')\n"
"import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# some sample data\nX = pd.DataFrame({\n    'col1': ['obj1', 'obj2', 'obj3'],\n    'col2': [np.nan, 'oj3', 'oj1'],\n    'col3': ['jo3', 'jo1', np.nan]\n}).astype('category')\ny = pd.Series([0, 1, 1])\n\npipeline = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='missing'),\n    OneHotEncoder(handle_unknown='ignore', sparse=False)\n)\n\nZ = pipeline.fit_transform(X, y)\n"
"inp_enc = tf.keras.layers.Input((1,))\nout_enc = tf.keras.layers.Dense(units=32)(inp_enc)\nencoder = tf.keras.Model(inp_enc, out_enc)\n    \nout_dec = tf.keras.layers.Dense(units=1)(encoder.output)\ndecoder = tf.keras.Model(encoder.input, out_dec)\n\nvae = tf.keras.Model(encoder.input, decoder.output)\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def on_epoch_end(self, epoch, logs=None):\n        get_output = tf.keras.backend.function(\n            inputs = self.model.layers[0].input,\n            outputs = self.model.layers[1].output\n        )\n        print(&quot;\\nPrediction:&quot;, get_output(X))\n\nX = np.random.random_sample((8,1))\ny = X\nvae.compile(optimizer='adam', loss='mse')\nvae.fit(X, y, epochs=2, callbacks=[CustomCallback()])\n"
'x_train = feature_layer(dict(train)).numpy()\nx_test = feature_layer(dict(test)).numpy()\n\nmodel.fit(x_train, y_train)\n'
'model = Flatten()(model)\n'
'`image_list = tf.convert_to_tensor(image_list)\n\nmask_list = tf.convert_to_tensor(mask_list)`\n'
'import re\n\npattern = &quot;\\\\d{1,3}((?P&lt;separator&gt;[,.])(?P&lt;floating&gt;\\\\d+))?&quot;\n\nstrings = (\n    &quot;60&quot;,\n    &quot;60.00&quot;,\n    &quot;60,000&quot;,\n    &quot;60.0000&quot;,\n    &quot;60.00%&quot;,\n    &quot;60.00 %&quot;,\n    &quot;100%&quot;,\n    &quot;5&quot;,\n    &quot;% 60&quot;,\n    &quot;% 60,000&quot;\n)\n\ndef randomize(match):\n    from random import uniform\n\n    integer, floating = divmod(uniform(0, 100), 1)\n\n    def get_chars():\n        yield str(int(integer))\n        if match.group(&quot;separator&quot;) is not None:\n            yield match.group(&quot;separator&quot;)\n            precision = len(match.group(&quot;floating&quot;))\n            yield f&quot;{{:.{precision}f}}&quot;.format(floating)[2:]\n    return &quot;&quot;.join(get_chars())\n        \n    \n\nfor string in strings:\n    print(re.sub(pattern, randomize, string))\n\n29\n95.08\n51,507\n9.1783\n0.80%\n6.56 %\n16%\n22\n% 27\n% 93,174\n&gt;&gt;&gt; \n'
"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu')])\n\nx = layers.Flatten()(last_output)\nx = layers.Dense(1024, activation='relu')(x)\nx = layers.Dense(1,activation='sigmoid')(x) \n"
'print(np.array(hash(&quot;8/6P1/5k1K/6r1/8/8/8/8 b - - 0 83&quot;)).shape)\n# outputs: ()\n\nprint(np.array([hash(&quot;8/6P1/5k1K/6r1/8/8/8/8 b - - 0 83&quot;)]).shape)\n# outputs: (1,)\n'
"import pandas as pd\nimport numpy as np\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX_train = pd.DataFrame(np.random.standard_normal((1000,5)), columns=[f&quot;x{i}&quot; for i\nin range(5)])\nvif=pd.DataFrame()\nvif['VIF']=[variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])]\nvif['Predictors']=X_train.columns\n\nprint(vif)\n\n        VIF Predictors\n0  1.002882         x0\n1  1.004265         x1\n2  1.001945         x2\n3  1.004227         x3\n4  1.003989         x4\n"
'from sklearn.metrics import confusion_matrix\n\nY = np.random.choice([0,1],size=(1,10))\npred = np.random.choice([0,1],size=(1,10))\n\nconfusion_matrix(Y, pred)\nValueError: multilabel-indicator is not supported\n\nconfusion_matrix(Y.ravel(), pred.ravel())\n'
'from sklearn.datasets import load_boston\n\nA,b = load_boston(return_X_y=True)\nn_samples = A.shape[0]\nn_features = A.shape[1]\n\ndef grad_linreg(x):\n    &quot;&quot;&quot;Least-squares gradient&quot;&quot;&quot;\n    grad = (1. / n_samples) * np.dot(A.T, np.dot(A, x) - b)\n    return grad\n\ndef loss_linreg(x):\n    &quot;&quot;&quot;Least-squares loss&quot;&quot;&quot;\n    f = (1. / (2. * n_samples)) * sum((b - np.dot(A, x)) ** 2)\n    return f\n\nfrom scipy.optimize import check_grad\nfrom numpy.random import randn\n\ncheck_grad(loss_linreg,grad_linreg,randn(n_features))\ncheck_grad(loss_linreg,grad_linreg,randn(n_features))\ncheck_grad(loss_linreg,grad_linreg,randn(n_features))\ncheck_grad(loss_linreg,grad_linreg,randn(n_features))\n'
'jpgfile = Image.open(&quot;63.jpg&quot;) \njpgfile = jpgfile.resize((32, 32) # resize image to 32*32\nimg_as_matrix = numpy.array(jpgfile)  # convert to numpy array\nimg_as_matrix = img_as_matrix.reshape(img_as_matrix.shape[0]*img_as_matrix.shape[1]*img_as_matrix.shape[2],1).T  # Reshape and transpose image as the train images\n# Here the second dim is 1, since there is only 1 image instead of X.shape[3] images \n\nvalue = clf.predict(img_as_matrix)\n'
"df = pd.DataFrame(['25m 34s', '1m', '22s'], columns=['game_length'])\ndf['game_length_seconds'] = pd.to_timedelta(df['game_length']).apply(lambda x: x.seconds)\n\n&gt;&gt;&gt; df\n\n  game_length  game_length_seconds\n0     25m 34s                 1534\n1          1m                   60\n2         22s                   22\n"
'tesla_model = DecisionTreeRegressor(random_state = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ntesla_model.fit(X_train, X_test)\nmae = mean_absolute_error(y_test,tesla_model.predict(X_test))\n'
'x_train.select_dtypes(include=np.number).columns\n\nfeature_importances = {x_train.select_dtypes(include=np.number).columns[x]:grid_search.best_estimator_.feature_importances_[x] for x in range(len(grid_search.best_estimator_.feature_importances_))}\n'
'##For the first df\nX_train_df1 = dfs_split[0][0]\nX_test_df1 = dfs_split[0][1]\ny_train_df1 = dfs_split[0][2]\ny_test_df1 = dfs_split[0][3]\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.ion()\n\nx = [1,2,3,4,5]\ny = [1,2,3,4,5]\n\ndef Gradient_Descent(x, y, learning_rate, iterations):\n  theta_1=0\n  theta_0=0\n  m = x.shape[0]\n  for i in range(iterations):\n      theta_0, theta_1 = perform_cal(theta_0, theta_1, m, learning_rate)\n      ax.clear()\n      ax.plot(x, y, linestyle='None', marker='o')\n      ax.plot(x, theta_0 + theta_1*x)\n      fig.canvas.draw()\n\ndef mean_error(a, b, factor, m, theta_0, theta_1):\n  sum_mean = 0\n  for i in range(m):\n    sum_mean += (theta_0 + theta_1 * a[i]) - b[i]  # h(x) = (theta0 + theta1 * x) - y \n    if factor:\n      sum_mean *= a[i]\n  print(sum_mean)\n  return sum_mean\n\ndef perform_cal(theta_0, theta_1, m, learning_rate):\n  temp_0 = theta_0 - learning_rate * ((1 / m) * mean_error(x, y, False, m, theta_0, theta_1))\n  temp_1 = theta_1 - learning_rate * ((1 / m) * mean_error(x, y, True, m, theta_0, theta_1))\n  return temp_0 , temp_1\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n\n\n\nx = np.array(x)\ny = np.array(y)\nGradient_Descent(x,y, 0.01, 100)\n"
'import numpy as np\nimport pandas as pd\n\nxs = pd.DataFrame(np.random.random(10000))\ny = pd.Series(np.random.randint(0, 2, size=10000))\n\nsorted_idxs = np.argsort(xs[0].values)\nsorted_values = xs[0].values[sorted_idxs]\nsorted_targets = y.values[sorted_idxs]\nsorted_targets_less = np.insert(np.cumsum(sorted_targets), 0, 0)[:-1]\n\nunsorted_idxs = np.argsort(sorted_idxs)\ntargets_less = sorted_targets_less[unsorted_idxs]\n\nfor i, target_less_value in enumerate(targets_less):\n    assert target_less_value == y.values[np.where(xs.values &lt; xs.values[i])[0]].sum()\n'
'&gt;&gt;&gt; estimator = GridSearchCV(pipe, dict(pca__n_components=n_components,\n...                                     logistic__C=Cs),\n...                          verbose=1)\n&gt;&gt;&gt; estimator.fit(X_digits, y_digits)\nFitting 3 folds for each of 9 candidates, totalling 27 fits\n[...snip...]\n'
'&gt;&gt;&gt; ds=[\'rockatr1\',\'rockatr2\',\'rockatr\',\'rocktype\']\n&gt;&gt;&gt; X,y=ds\nTraceback (most recent call last):\n  File "&lt;stdin&gt;", line 1, in &lt;module&gt;\nValueError: too many values to unpack\n\n&gt;&gt;&gt; X,y=ds[:-1],ds[-1]\n&gt;&gt;&gt; X\n[\'rockatr1\', \'rockatr2\', \'rockatr\']\n&gt;&gt;&gt; y\n\'rocktype\'\n'
'dot_product = T.dot(x, w)\nf = theano.function([x], dot_product)\n\nprint f(rng.randn(feats))\n'
'data = np.array([x[0] for x in data])\n\ntarget = np.array([x[1] for x in data])\n'
'return [o_t[0], s_t1, s_t2]\n\nreturn o_t[0], s_t1, s_t2\n'
'model.fit\n'
'def loss_function(y_pred, y_true):\n\nreturn theano.tensor.net.binary_crossentropy(y_pred * v, y_true * v)\n'
"articletitles.append(e.title)\n\narticletitles.append(' '.join([e.title, ', from', feed]))\n"
'result = clf.predict(validationX)\nclf.score(result, validationY)\n\nclf.score(validationX, validationY)\n'
'# Import\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Instanciate your classifier\nneigh = KNeighborsClassifier(n_neighbors=4) #k=4 or whatever you want\n# Fit your classifier\nneigh.fit(X, y) # Where X is your training set and y is the training_output\n# Get the neighbors\nneigh.kneighbors(X_test, return_distance=False) # Where X_test is the sample or array of samples from which you want to get the k-nearest neighbors\n'
'import pandas as pd\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn import tree\nfrom sklearn.multiclass import OneVsRestClassifier\n\ndf = pd.DataFrame([[0, 0, 1, "Tree", "Flower"], [1,1,0,\'Tree\',\'Water\'], [0,1,0,\'Tree\',\'NA\'], [2,1,0,\'Water\',\'Wood\'], [1,1,0,\'Flower\',\'NA\'], [1,1,1,\'Tree\',\'Flower\'], [2,2,0,\'Flower\',\'NA\'] ], columns=(\'feature1\', \'feature2\', \'feature3\', \'outcome1\', \'outcome2\'))\n\n# Binarize your classes\noutcomes = zip(list(df[\'outcome1\']), list(df[\'outcome2\']))\nMLB = MultiLabelBinarizer()\nY = MLB.fit_transform(outcomes)\n\n# Extract your data\nX = df[[\'feature1\', \'feature2\', \'feature3\']]\n\n# Define base classifier and meta-classifier\nmy_tree_one = tree.DecisionTreeClassifier()\nclf = OneVsRestClassifier(my_tree_one)\n\n# Train your classifier and output your predictions\nclf.fit(X, Y)\npredictions = clf.predict(X)  # predict on a new X\nprint MLB.inverse_transform(predictions)\n\nthreshold = 0.6\npredictions = []\n\nprobabilities = clf.predict_proba(X)\nfor probability_row in probabilities:\n    predictions.append([1 if p &gt; threshold else 0 for p in probability_row])\npredictions = np.asarray(predictions)\n'
"output = tf.add(tf.matmul( l3, output_layer['weights']), output_layer['biases'])\noutput = tf.nn.relu(output)\n\noutput = tf.add(tf.matmul( l3, output_layer['weights']), output_layer['biases'])\n"
'with tf.name_scope("foo"):\n   a = tf.constant(1, name="bar")\n'
'table=table.dropna(inplace=True)\n\ninplace : boolean, default False\n    If True, do operation inplace and return None.\n'
'y = Lambda(lambda x: x[:,0,:,:], output_shape=(1,) + input_shape[2:])(x)\n'
'         _, c = sess.run([optimizer, loss], \n                      feed_dict={x:batch_x, y: batch_y})\n\n         _, c = sess.run([optimizer, loss], \n                      feed_dict={x:batch_x, y_: batch_y})\n'
'import numpy as np\nx = np.arange(10)                # Shape: (10,)\ny = np.arange(10).reshape(10,1)  # Shape: (10, 1)\ndifference = x-y                 # Shape: (10, 10)\n'
'import tensorflow as tf\n\n# define model\n...\n\n# load checkpoint\n...\n\n# assemble the list of weights to add noise\nlist_of_weights = [ ... ] \n\n\ndef add_random_noise(w, mean=0.0, stddev=1.0):\n    variables_shape = tf.shape(w)\n    noise = tf.random_normal(\n        variables_shape,\n        mean=mean,\n        stddev=stddev,\n        dtype=tf.float32,\n    )\n    return tf.assign_add(w, noise)\n\n\nsess = tf.Session()\nfor w in list_of_weights:\n    sess.run(add_random_noise(w))\n\n# continue experiments\n...\n'
"# What model to download.\nMODEL_NAME = 'ssd_mobilenet_v1_coco_11_06_2017'\n"
'new_data_h = [new_timeInMinutes, new_season, new_ext_temp]\n'
"import keras\nfrom keras.models import Model\n\nmodel_base = keras.applications.vgg16.VGG16(include_top=False, input_shape=(*IMG_SIZE, 3), weights='imagenet')\noutput = model_base.output\n# Add any other layers you want to `output` here...\noutput = Dense(len(categories), activation='softmax')(output)\nmodel = Model(model_base.input, output)\nfor layer in model_base.layers:\n   layer.trainable = False\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nreturn model\n"
"for row_index, answer in enumerate(data['ChooseTransport']):\n    for i in range(7):\n        name = 'ChooseTransport_' + str(i)\n        data.loc[row_index, name] = int(answer[i])\n\ndata.drop(columns=['ChooseTransport'], inplace=True)\n\ndata.corr()  \n\nimport seaborn as sns\nsns.heatmap(data.corr())\n"
'text here and then blank line\n\nanother text\n\nand this is it\n'
"epoch = tf.Variable(0, trainable=False) # 0 is initial value\n# increment by 1 when the next op is run\nepoch_incr_op = tf.assign_add(epoch, 1, name='incr_epoch')\n\n# Define any operations that depend on 'epoch'\n# Note we need to cast the integer 'epoch' to float to use in tf.pow\ngrad_W = grad_W + tf.random_normal(grad_W.shape, 0.0,\n                                  1.0/tf.pow(1+tf.cast(epoch, tf.float32), 0.55))\n\n# Training loop\nwhile running_epoch:\n    _, _, cost_ = sess.run([new_W, new_b ,cost], \n       feed_dict = {X_: X_train_tr, Y: labels_, learning_rate: learning_r})\n\n# At end of epoch, increment epoch counter\nsess.run(epoch_incr_op)\n"
"#Encoding the train data\nX_uncoded=pd.get_dummies(X_uncoded,columns=One_hot_encoded_predictors)\nX=X_uncoded\nprint(X.shape)\n(1460, 158)\n#Encoding the test data\nX_uncoded_test=pd.get_dummies(X_uncoded_test,columns=One_hot_encoded_predictors)\nprint(X_uncoded_test.shape)\n(1459, 151)\n\nimport pandas as pd\ns = pd.Series(list('abca'))\npd.get_dummies(s)\n   a  b  c\n0  1  0  0\n1  0  1  0\n2  0  0  1\n3  1  0  0\n"
'import tensorflow as tf\n\nbatch_size = 3\nlayer_1 = tf.ones((batch_size, 2))\n\noutput_1 = layer_1[:, None, 0]\noutput_2 = tf.sigmoid(layer_1[:, None, 1])\n\noutput = tf.concat([output_1, output_2], axis=-1)\n\nwith tf.Session() as sess:\n    print(sess.run(output))\n'
'# train classifier model \n\nfrom sklearn.externals import joblib\nfrom sklearn.naive_bayes import MultinomialNB\n\nclf = MultinomialNB()\nclf.fit(X, y)\n\njoblib.dump(clf, \'filename.pkl\') \n\n\n# flask backend\n\nclassifier = joblib.load("filename.pkl")\n\n@app.route("/predict", methods=["POST"])\ndef predict():\n\n    # get vect\n\n    result = classifier.predict(vect)\n\n    return result\n'
'def new_tail():\n    my_list = list(range(6))\n    my_list[-1] = "new last element"\n    for elem in my_list:\n        yield elem\n\nfor item in new_tail():\n    print(item)\n\n0\n1\n2\n3\n4\nnew last element\n'
'from keras.utils import to_categorical\n\ny_ = to_categorical(y_)\n'
'import shap\nimport xgboost as xgb\n\n# Assume X_train and y_train are both features and labels of data samples\n\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names, weight=weights_trn)\n\n# Train your xgboost model\nbst = xgb.train(params0, dtrain, num_boost_round=2500, evals=watchlist, early_stopping_rounds=200)\n\n# "explainer" object of shap\nexplainer = shap.TreeExplainer(bst)\n\n# "Values you explain, I took them from my training set but you can "explain" here what ever you want\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values, X_test)\nshap.summary_plot(shap_values, X_test, plot_type="bar")\n'
'train_sizes=np.linspace(.1, 1.0, 10)\n\n...\nn_max_training_samples = len(cv_iter[0][0])\ntrain_sizes_abs = _translate_train_sizes(train_sizes, n_max_training_samples)\n...\n...\n\n...\nelse:\n    train_test_proportions = []\n    for train, test in cv_iter:\n        for n_train_samples in train_sizes_abs:\n            train_test_proportions.append((train[:n_train_samples], test))\n...\n...\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv,\n                                                        n_jobs=n_jobs, \n                                                        train_sizes=train_sizes, \n                                                        shuffle=True) \n'
'keras.layers.Flatten(input_shape=(100, 100, 1))\n\nimage_paths, lbls = ["selfies-data/1", "selfies-data/2"], [0., 1.]\n\nlabels = []\nfile_names = []\nfor d, l in zip(image_paths, lbls):\n    # get the list all the images file names\n    name = [os.path.join(d,f) for f in os.listdir(d)]\n    file_names.extend(name)\n    labels.extend([l] * len(name))\n\nfile_names = tf.convert_to_tensor(file_names, dtype=tf.string)\nlabels = tf.convert_to_tensor(labels)\n\ndataset = tf.data.Dataset.from_tensor_slices((file_names, labels))\n\n# the rest is the same \n\nlabels = tf.expand_dims(labels, axis=-1)\n\n       # ... \n       keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\nmodel.compile(optimizer=tf.train.AdamOptimizer(),\n              loss=\'binary_crossentropy\',\n              metrics=[\'accuracy\'])\n'
'out = Lambda(lambda x: K.expand_dims(x, axis=1))(out)\n\nfrom keras.layers import concatenate, Concatenate\n\n# use functional interface\nout_put = concatenate(branch_outputs)\n\n# or use layer class\nout_put = Concatenate()(branch_outputs)\n'
"train_data['close'] = (train_data['close'] - 10)/(2000 - 10) \n"
"df = pd.DataFrame({'a':[1,2,3,4]})\n\n#slow, working only with unique values\ndf['b'] = df['a'].apply(lambda x: df.loc[df.a != x, 'a'].mean())\n#faster\ndf['b1'] = (df['a'].sum() - df['a']) / (len(df) - 1)\nprint (df)\n   a         b        b1\n0  1  3.000000  3.000000\n1  2  2.666667  2.666667\n2  3  2.333333  2.333333\n3  4  2.000000  2.000000\n"
'label1 = np.ones( (r1.shape[0],1) )\nlabel2 = np.ones( (r2.shape[0],1) ) * -1\ndata = np.concatenate((r1, r2))\nlabels = np.concatenate((label1, label2))\n\nr1 = np.append(label1, r1, axis=1)\nr2 = np.append(label2, r2, axis=1)\ndata = np.concatenate((r1,r2))\nnp.random.shuffle(data)\nlabels = data[:,0] #extracts labels in shape of (len(labels),)which is a rank 1 array \nlabels = np.reshape(labels,(len(labels),1)) #fix the shape to a 1D array    \nR = data[:,(1,2)] #extracting inputs\n'
'seq_fun = K.function([model.layers[0].input, model.layers[1].input, model.layers[2].input], \n                      [model.layers[3].get_output_at(i)])  # i must be 1, 2 or 3\n\nseq_fun = K.function(model.inputs, [model.layers[3].get_output_at(i)])\n'
'&gt;&gt;&gt; reg.coef_\n[0. 0. 0. 0.]\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\n\n# f_learn =  np.array([[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]])\n# r_learn =  np.array([6,7,8,9])\nf_learn = np.arange(20.).reshape(5, 4)\nf_learn += np.random.randn(5, 4)\nr_learn = f_learn[:, 0] + 2 * f_learn[:, 1] + 3 * f_learn[:, 2] + 4 * f_learn[:, 3]\n\nreg = linear_model.LinearRegression()\nreg.fit(f_learn, r_learn)\nprint(reg.coef_)\n\nx_1 = np.linspace(0, 20, 10)\nx_2 = np.linspace(0, 20, 10)\nX_1, X_2 = np.meshgrid(x_1, x_2)\n\nx_3 = np.full( (10,10), 5).ravel()\nx_4 = np.full( (10,10), 2).ravel()\n\npredict_matrix = np.vstack([X_1.ravel(), X_2.ravel(), x_3, x_4])\nprediction = reg.predict(predict_matrix.T)\n\nprediction_plot = prediction.reshape(X_1.shape)\n\nplt.figure()\ncp = plt.contourf(X_1, X_2, prediction_plot, 10)\nplt.colorbar(cp)\nplt.show()\n'
"inputs = [keras.layers.Input(name='ref', dtype=tf.float32, shape=(height * width,)), \n          keras.layers.Input(name='ltg', dtype=tf.float32, shape=(height * width,))]\n\njson_input = keras.layers.concatenate(inputs, axis=0)\n\n# ...\nserving_model = keras.Model(inputs, model_output)\n\nfrom keras.layers import Reshape\n\ninputs = [keras.layers.Input(name='ref', dtype=tf.float32, shape=(height * width,)), \n          keras.layers.Input(name='ltg', dtype=tf.float32, shape=(height * width,))]\n\nreshape_layer = Reshape((height, width, 1))\nr_in1 = reshape_layer(inputs[0])\nr_in2 = reshape_layer(inputs[1])\nimg = concatenate([r_in1, r_in2])\n\noutput = model(img)\n\nserving_model = keras.Model(inputs, output)\n\nreturn tf.reshape(stacked, [height, width, n_channels])\n"
'batch_size = tf.shape(layers)[0]\npadding_tensor = tf.ones([batch_size, 1])\n\nconcat = concatenate([layers, padding_tensor])\n'
"model = applications.VGG16(include_top=False, weights='imagenet')\nfeatures = model.predict(img/255.0)\nmodel_out = resnet_50.predict(features)\n"
'def binarise_number(number, max_number=None):\n    if max_number is None:\n        return [int(x) for x in format(number, "0b")]\n    n_number = format(number, "0&gt;%db" % len(binarise_number(max_number, None)))\n    return [int(x) for x in n_number]\n\ndef revert_binarise_number(n_number):\n    str_number = \'0b\' + \'\'.join(str(int(x)) for x in n_number)\n    number = int(str_number, base=2)\n    return number\n\nprint(rcrp_label)\n[[0 1 0]\n [0 1 0]\n [0 1 0]\n ...\n [0 0 0]\n [0 1 0]\n [1 0 0]]\n\nprint(rcwp_label)\n[[0 0 1]\n [0 0 0]\n [0 0 0]\n ...\n [0 1 0]\n [0 0 0]\n [0 0 0]]\n'
"inputs = Input(shape=(150, 150, 3))\n\nx = Flatten()(inputs)\n\npredictions = Dense(1, activation='sigmoid')(x)\n"
"import tensorflow as tf\nfrom tensorflow.python.keras.models import Model\n\nx = Input(shape=(None, None, 3), name='image_input')\nresize_x = tf.image.resize_images(x, [32,32])\nvgg_model = load_vgg()(resize_x)\nmodel = Model(inputs=x, outputs=vgg_model.output)\nmodel.compile(...)\nmodel.predict(...)\n"
'import numpy as np\n\npreds = model.predict(test_data)\npred_class = np.argmax(preds, axis=-1)\n'
'samples, x, y = xtrain.shape\nnew_dataset = xtrain.reshape((samples,x*y))\n'
"# balance the classes by over-sampling the training data\nos = SMOTE(random_state=0)\nos_X_train, os_y_train = os.fit_sample(X_train, y_train.ravel())\nos_X_train = pd.DataFrame(data=os_X_train, columns=X_train.columns)\n# critically important to have the categorical variables from float back to int\nos_X_train['attrcat1'] = os_X_train['attrcat1'].astype(int)\nos_X_train['attrcat2'] = os_X_train['attrcat2'].astype(int)\n"
"input_2 = Input(shape=(500,))\nmodel_2 = Dense(32)(input_2 )\nmodel_2 = Dense(4)(model_2)\n\ninput_1 = Input(shape=(None, 2048))\nmodel_1 = LSTM(32, dropout_U = 0.2, dropout_W = 0.2, return_sequences=True)(input_1 )\nmodel_1 = LSTM(16, dropout_U = 0.2, dropout_W = 0.2, return_sequences=False)(model_1)\n\nmerged = concatenate([model_2, model_1])\nmerged = Dense(8, activation='softmax')(merged)\n\nmodel = Model(inputs=[input_2 , input_1], outputs=merged)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
'from keras.layers import Reshape\n\ndef kaggle_LSTM_model():\n    model = Sequential()\n    model.add(Reshape((28,28), input_shape=x_train.shape[1:]))\n    # the rest is the same...\n'
'array = np.zeros((800, 4096))\npaths = [...] # set paths here\n\nfor i, path in enumerate(paths):\n    array[i] = mpimg.imread(path).reshape(4096)\n'
"x_text = data[data['Age'] != None].drop(columns='Age')\ny_test = data[data['Age'] != None]['Age']\n"
'# create Parameter dict storing model parameters\np1 = net1.collect_params()\np2 = net2.collect_params()\np3 = net3.collect_params()\n\nfor k1, k2, k3 in zip(p1, p2, p3):\n    p3[k3].set_data(0.5*(p1[k1].data() + p2[k2].data()))\n'
'Data = pd.read_csv("Data.csv")    \nX = Data.drop([\'name of the target column\'],axis=1).values\ny = Data[\'name of the target column\'].values\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n\nData = pd.read_csv("Data.csv")\nX = Data.iloc[:,:-1]\ny = Data.iloc[:,-1]\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n'
'import numpy as np\nnp.array([0, 1, 2]).shape\n\n## (3,)\n\nnp.array([[0, 1, 2], [3, 4, 5]]).shape\n\n## (2, 3)\n'
"pca_review_df = pd.DataFrame(data= pca_review, columns= ['Component1','Component2'])\n\npca_review = pca_a.fit_transform(R.toarray())\n"
'&gt;&gt;&gt; print(featureB[:2])\n[[11, 36, 60, 85, 110, 134, 159, 183, 208, 232, 257, 286, 310, 335],\n[11, 41, 69, 98, 127, 155, 184, 213, 241, 270, 299, 327, 356]]\n\npoints = []\nfor x in training_feature:\n    for y in x[1]:\n        points.append([x[0], y])\npoints = np.array(points)\n\npoints.shape # (80, 2)\n\nplt.scatter(points[:, 0], points[:, 1])\n'
"from nltk.corpus import brown\nfrom collections import Counter\nimport numpy as np\n\ntext = '\\n  '.join([' '.join([w for w in s]) for s in brown.sents()])\n\nunigrams = Counter(text)\nbigrams = Counter(text[i:(i+2)] for i in range(len(text)-2))\ntrigrams = Counter(text[i:(i+3)] for i in range(len(text)-3))\n\nweights = [0.001, 0.01, 0.989]\n\ndef strangeness(text):\n    r = 0\n    text = '  ' + text + '\\n'\n    for i in range(2, len(text)):\n        char = text[i]\n        context1 = text[(i-1):i]\n        context2 = text[(i-2):i]\n        num = unigrams[char] * weights[0] + bigrams[context1+char] * weights[1] + trigrams[context2+char] * weights[2] \n        den = sum(unigrams.values()) * weights[0] + unigrams[context1] * weights[1] + bigrams[context2] * weights[2]\n        r -= np.log(num / den)\n    return r / (len(text) - 2)\n\nt1 = '128, 127, h4rugz4sx383a6n64hpo, tt, t66, t65, asdfds'.split(', ')\nt2 = 'Michael, sara, jose colmenares, Dimitar, Jose Rafael, Morgan, Eduardo Medina, Luis R. Mendez, Hikaru, SELENIA, Zhang Ming, Xuting Liu, Chen Zheng'.split(', ')\nfor t in t1 + t2:\n    print('{:20} -&gt; {:9.5}'.format(t, strangeness(t)))\n\n128                  -&gt;    5.5528\n127                  -&gt;    5.6572\nh4rugz4sx383a6n64hpo -&gt;    5.9016\ntt                   -&gt;    4.9392\nt66                  -&gt;    6.9673\nt65                  -&gt;    6.8501\nasdfds               -&gt;    3.9776\nMichael              -&gt;    3.3598\nsara                 -&gt;    3.8171\njose colmenares      -&gt;    2.9539\nDimitar              -&gt;    3.4602\nJose Rafael          -&gt;    3.4604\nMorgan               -&gt;    3.3628\nEduardo Medina       -&gt;    3.2586\nLuis R. Mendez       -&gt;     3.566\nHikaru               -&gt;    3.8936\nSELENIA              -&gt;    6.1829\nZhang Ming           -&gt;    3.4809\nXuting Liu           -&gt;    3.7161\nChen Zheng           -&gt;    3.6212\n"
'Y= dependent variable, your target  \nX= regressors, your features  \nε= your errors\n'
"def costFunction(X,y,theta):\n    J = 0.0\n    m = y.size\n    print(y.shape)\n    print(X.shape)\n    print(theta.shape)\n    J = -1/m * np.sum(((1-y)*np.log(1-sigmoid(np.dot(X,theta))))+((y)*np.log(sigmoid(np.dot(X,theta)))))\n    grad = 1/m*np.dot(X.T, (sigmoid(np.dot(X, theta))-y))\n    return J, grad\n\nfrom scipy import optimize\nimport numpy as np\n\ndef sigmoid(t):\n    return 1./(1. + np.exp(t))\n\nX = np.random.random(size=(1000,3))\nY = np.random.random(size=(1000))\n\ndef costFunction(theta, x,y):\n    J = 0.0\n    m = y.size\n    J = -1/m * np.sum(((1-y)*np.log(1-sigmoid(np.dot(x,theta))))+((y)*np.log(sigmoid(np.dot(x,theta)))))\n    grad = 1/m*np.dot(x.T, (sigmoid(np.dot(x, theta))-y))\n    return J, grad\n\n#To check the function :\nprint(X[:,:3].shape)\nJ,grad = costFunction(theta=np.asarray([0,0,0]), x=X[:,:3],y=Y)\nprint(J)\nprint( grad)\n\noptions = {'maxiter' : 400}\ninitial_theta = np.zeros(3)\nx = X[:,:3]\nres = optimize.minimize(costFunction,\n                        x0 = initial_theta,\n                        args=(x, Y),\n                        jac=True,\n                        method='TNC',\n                        options=options)\n\ncost = res.fun\nthetaresult = res.x\nprint(cost)\nprint(thetaresult)\n"
'X_train, X_labels = data[train_index], response[train_index]\ny_test, y_labels = data[test_index], response[test_index]\nmodel = LogisticRegression().fit(X_train, X_labels)\npred = model.predict(y_test)\nacc = sklearn.metrics.accuracy_score(y_labels,pred,normalize=True)\n'
"model.add(Flatten())\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# probability array\nprobabilities = model.predict(image)[0]\n\n# get predicted classes\npred_classes = np.argmax(probabilities)\n"
'from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX.iloc[:, 1] = le.fit_transform(X.values[:, 1].astype(str))\n\n   Index  Ship_Mode\n0      0          0\n1      1          0\n2      2          1\n3      3          1\n4      4          0\n5      5          2\n'
"model = Sequential()\n\nmodel.add(Flatten())\nmodel.add(BatchNormalization(axis = 1, momentum = 0.99))\nmodel.add(Dense(inputdim, activation = tfnn.relu))\nmodel.add(BatchNormalization(axis = 1, momentum = 0.99))\nmodel.add(Dense(128, activation = tfnn.relu))\nmodel.add(BatchNormalization(axis = 1, momentum = 0.99))\nmodel.add(Dense(10, activation = tfnn.softmax))\n\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nEpoch 1/4\n60000/60000 [==============================] - 68s 1ms/sample - loss: 0.2045 - acc: 0.9374\nEpoch 2/4\n60000/60000 [==============================] - 55s 916us/sample - loss: 0.1007 - acc: 0.9689\n"
' train_datagen.flow_from_directory(\'dataset/training_set\',\n                                                 target_size = (64, 64),\n                                                 batch_size = 32,\n                                                 class_mode = \'binary\',\n                                                 color_mode = "grayscale")\n'
"from sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nX, y = make_regression(n_targets=2)\nprint('Feature vector:', X.shape)\nprint('Target vector:', y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\nprint('Build and fit a regressor model...')\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)\n\nprint('Done. Score', score)\n\nFeature vector: (100, 100)\nTarget vector: (100, 2)\nBuild and fit a regressor model...\nDone. Score 0.4405974071273537\n"
"import numpy as np\nimport pandas as pd\nimport sklearn\nsklearn.__version__\n# '0.21.3'\n\n# Importing the dataset\ndataset = pd.read_csv('50_Startups.csv')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 4].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder = LabelEncoder()\nX[:, 3] = labelencoder.fit_transform(X[:, 3])\nonehotencoder = OneHotEncoder(categorical_features = [3])\nX = onehotencoder.fit_transform(X).toarray()\n\n# Avoiding the Dummy Variable Trap\nX = X[:, 1:]\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split  # model_selection here, due to newer version of scikit_learn\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n# FutureWarning here, irrelevant to the issue\n\ny_train\n# result:\narray([ 96778.92,  96479.51, 105733.54,  96712.8 , 124266.9 , 155752.6 ,\n       132602.65,  64926.08,  35673.41, 101004.64, 129917.04,  99937.59,\n        97427.84, 126992.93,  71498.49, 118474.03,  69758.98, 152211.77,\n       134307.35, 107404.34, 156991.12, 125370.37,  78239.91,  14681.4 ,\n       191792.06, 141585.52,  89949.14, 108552.04, 156122.51, 108733.99,\n        90708.19, 111313.02, 122776.86, 149759.96,  81005.76,  49490.75,\n       182901.99, 192261.83,  42559.73,  65200.33])\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nsc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train.reshape(-1,1))  # reshape here\n\ny_train\n# result\narray([[-0.31304376],\n       [-0.32044287],\n       [-0.09175449],\n       [-0.31467774],\n       [ 0.3662475 ],\n       [ 1.14433163],\n       [ 0.57224308],\n       [-1.10020076],\n       [-1.82310158],\n       [-0.20861649],\n       [ 0.50587547],\n       [-0.23498575],\n       [-0.29700745],\n       [ 0.43361398],\n       [-0.93778138],\n       [ 0.22309235],\n       [-0.98076868],\n       [ 1.05682957],\n       [ 0.61437014],\n       [-0.05046517],\n       [ 1.17493831],\n       [ 0.39351679],\n       [-0.77118537],\n       [-2.34186247],\n       [ 2.03494965],\n       [ 0.79423047],\n       [-0.48182335],\n       [-0.02210286],\n       [ 1.15347296],\n       [-0.01760646],\n       [-0.46306547],\n       [ 0.04612731],\n       [ 0.32942519],\n       [ 0.9962397 ],\n       [-0.70283485],\n       [-1.4816433 ],\n       [ 1.81525556],\n       [ 2.04655875],\n       [-1.65292476],\n       [-1.09342341]])\n\ndataset.iloc[:, 3].values\n# result:\narray(['New York', 'California', 'Florida', 'New York', 'Florida',\n       'New York', 'California', 'Florida', 'New York', 'California',\n       'Florida', 'California', 'Florida', 'California', 'Florida',\n       'New York', 'California', 'New York', 'Florida', 'New York',\n       'California', 'New York', 'Florida', 'Florida', 'New York',\n       'California', 'Florida', 'New York', 'Florida', 'New York',\n       'Florida', 'New York', 'California', 'Florida', 'California',\n       'New York', 'Florida', 'California', 'New York', 'California',\n       'California', 'Florida', 'California', 'New York', 'California',\n       'New York', 'Florida', 'California', 'New York', 'California'],\n      dtype=object)\n\ny_train\n# result:\narray(['Florida', 'New York', 'Florida', 'California', 'Florida',\n       'Florida', 'Florida', 'New York', 'New York', 'New York',\n       'New York', 'Florida', 'California', 'California', 'California',\n       'California', 'New York', 'New York', 'California', 'California',\n       'New York', 'New York', 'California', 'California', 'California',\n       'Florida', 'California', 'New York', 'California', 'Florida',\n       'Florida', 'New York', 'New York', 'California', 'California',\n       'Florida', 'New York', 'New York', 'California', 'California'],\n      dtype=object)\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-18-4a9512e0c95c&gt; in &lt;module&gt;\n      5 X_test = sc_X.transform(X_test)\n      6 sc_y = StandardScaler()\n----&gt; 7 y_train = sc_y.fit_transform(y_train.reshape(-1,1))\n\n[...]\nValueError: could not convert string to float: 'Florida'\n"
'import numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nX = np.array([[3],[1],[4],[3]])\nknn.fit(X, [1,0,1,1])\n\nl = knn.kneighbors([[3]], n_neighbors=3, return_distance=False)\nX[l].ravel()\n'
"inputs = Input(shape=(29,3))\noutputs = LSTM(150)(inputs)\noutputs = Dense(100)(outputs)\noutputs = Dropout(0.2)(outputs)\noutputs = Dense(1, activation='sigmoid')(outputs)\n\nmodel = Model(inputs, outputs)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
'# Function that runs the requested algorithm and returns the accuracy metrics.\n# Passing the sklearn model as an argument along with cv values and training data.\ndef fit_ml_algo(algo, X_train, y_train, cv):\n\n# One Pass\nmodel = algo.fit(X_train, y_train)\nacc = round(model.score(X_train, y_train) * 100, 2)\n\n# Cross Validation \ntrain_pred = model_selection.cross_val_predict(algo, \n                                              X_train, \n                                              y_train, \n                                              cv=cv, \n                                              n_jobs = -1)\n# Cross-validation accuracy metric\nacc_cv = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\n\nreturn train_pred, acc, acc_cv\n'
'k = 2\na = torch.tensor(np.array([10.,20]), requires_grad=True).float()\nb = torch.tensor(np.array([10.,20]), requires_grad=True).float()\n\nbatch_size = a.size()[0]\nuniform_samples = Uniform(torch.tensor([0.]), torch.tensor([1.])).rsample(torch.tensor([batch_size,k])).view(-1,k)\nexp_a = 1/a\nexp_b = 1/b\nkm = (1 - uniform_samples**exp_b)**exp_a\n\nsticks = torch.zeros(batch_size,k)\nremaining_sticks = torch.ones_like(km[:,0])\nfor i in range(0,k-1):\n    sticks[:,i] = remaining_sticks * km[:,i]\n    remaining_sticks = remaining_sticks * (1-km[:,i])\nsticks[:,k-1] = remaining_sticks\nlatent_variables = sticks\nlatent_variables = torch.sum(latent_variables)\n\nlatent_variables.backward()\n'
"def define_model():\n    model = Sequential()\n    model.add(Dense(512, input_dim=77, kernel_initializer='RandomNormal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, kernel_initializer='RandomNormal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(512, kernel_initializer='RandomNormal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, kernel_initializer='RandomNormal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(512, kernel_initializer='RandomNormal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, kernel_initializer='RandomNormal', activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1))\n\nmodel=define_model()\n# Compile model\nmodel.compile(loss='mean_absolute_error', optimizer='adam')\n\n\n# evaluate model\nhistory = model.fit(scaler.transform(X_train_high), y_train_high,\n                    batch_size=128,\n                    epochs=5)\nresults = model.evaluate(scaler.transform(X_train_high), y_train_high, batch_size=128)\nprint('High test loss, test acc:', results)\n\nmodel=define_model()\n\nmodel.compile(loss='mean_absolute_error', optimizer='adam')\n# evaluate model\nhistory = model.fit(scaler.transform(X_train_medium), y_train_medium,\n                    batch_size=128,\n                    epochs=5)\nresults = model.evaluate(scaler.transform(X_train_medium), y_train_medium, batch_size=128)\nprint(' Medium test loss, test acc:', results)\n"
"df = df.explode('reference')\n\ndf = df['reference'].apply(pd.Series).merge(df, left_index=True, right_index=True, how ='outer')\n"
'normTrainFeaturesDf = np.random.rand(100, 10)\nnormTestFeaturesDf = np.random.rand(10, 10)\ntrainLabelsDf = np.random.rand(100)\n\n&gt;&gt;&gt; linear_model_preds = linearModel.predict(np.array(normTestFeaturesDf))\n&gt;&gt;&gt; nn_model_preds = nnModel.predict(normTestFeaturesDf).flatten()\n\n&gt;&gt;&gt; print(linear_model_preds)\n&gt;&gt;&gt; print(nn_model_preds)\n[0.46030349 0.69676376 0.43064266 0.4583325  0.50750268 0.51753189\n 0.47254946 0.50654825 0.52998559 0.35908762]\n[0.46030346 0.69676375 0.43064266 0.45833248 0.5075026  0.5175319\n 0.47254944 0.50654817 0.52998555 0.3590876 ]\n\n&gt;&gt;&gt; np.allclose(linear_model_preds, nn_model_preds)\nTrue\n'
"import locale\nlocale.setlocale(locale.LC_ALL, 'fr_FR')\n...\n    somma = locale.atof(a) + somma\n"
'x = AveragePooling3D((4, 4, 4), strides=(1, 1, 1), padding="same", data_format="channels_last")(x)\nx = Dropout(0.5)(x)\nx = Flatten()(x)\n\nx = Dense(nb_classes, activation=\'softmax\')(x)\n'
'self.W_hidden = w_hidden_old - learning_rate * self.W_hidden\n\nself.W_hidden = w_hidden_old - learning_rate * self.Partials_W_hidden\n\nclass Neural_Net:\n    """\n    """\n    def __init__(self, activation_function, learning_rate, runs):\n        self.activation_function = activation_function\n\n        self.Data = pd.read_csv(r"U:\\19_035_Machine_Learning_Workshop\\2_Workshopinhalt\\Weitere\\Neural Networks\\AirQualityUCI\\AirQualityUCI.csv", sep=\';\', decimal=b\',\').iloc[:, :-2].dropna()\n        self.X_train = np.linspace(0,5,1000)\n        self.y_train = np.sin(self.X_train)\n        plt.plot(self.X_train, self.y_train)\n        self.y_pred = None\n        self.W_input = np.random.randn(1, 3)\n        self.Partials_W_input = np.random.randn(1, 3)\n        self.W_hidden = np.random.randn(3,3)\n        self.Partials_W_hidden = np.random.randn(3,3)\n        self.W_output = np.random.randn(3,1)\n        self.Partials_W_output = np.random.randn(3,1)\n        self.Activations = np.zeros((3,2))\n        self.Partials = np.zeros((3,2))\n        self.Output_Gradient = None\n        self.Loss = 0\n        self.learning_rate = learning_rate\n        self.runs = runs\n        self.Losses = []\n        self.i = 0\n\n    def apply_activation_function(self, activation_vector):\n        # print(\'activation: \', 1/(1+np.exp(-activation_vector)))\n        return 1/(1+np.exp(-activation_vector))\n\n    def forward_pass(self, training_instance):\n        for layer in range(len(self.Activations[0])):\n            # For the first layer between X and the first hidden layer\n            if layer == 0:\n                pre_activation_first = self.W_input.T @ training_instance.reshape(1,1)\n                # print(\'pre activation: \', pre_activation)\n                # Apply the activation function\n                self.Activations[:,0] = self.apply_activation_function(pre_activation_first).ravel()\n            else:\n                pre_activation_hidden = self.W_hidden.T @ self.Activations[:, layer-1]\n                self.Activations[:, layer] = self.apply_activation_function(pre_activation_hidden)\n                # print(\'Activations: \', self.Activations)\n        output = self.W_output.T @ self.Activations[:, -1].reshape(-1,1)\n        # print(\'output: \', output)\n        return output\n\n    def backpropagation(self, y_true, training_instance):\n        if self.activation_function == \'sigmoid\':\n            pass\n        if self.activation_function == \'linear\':\n            # Calculate the ouput gradient\n            self.Output_Gradient = -(y_true-self.y_pred)\n            # print(\'Output Gradient: \', self.Output_Gradient)\n\n            # Calculate the partial gradients of the Error with respect to the pre acitvation values in the nodes\n            self.Partials[:, 1] = ((self.Activations[:, 1]*(1-self.Activations[:, 1])).reshape(-1,1)*(self.W_output @ self.Output_Gradient)).ravel()\n            self.Partials[:, 0] = self.Activations[:, 0]*(1-self.Activations[:, 0])*(self.W_hidden @ self.Partials[:, 1])\n            # print(\'Partials: \', self.Partials)\n\n            # Calculate the Gradients with respect to the weights\n            self.Partials_W_output = self.Output_Gradient * self.Activations[:, -1]\n            # print(\'Partials_W_output: \', self.Partials_W_output)\n            self.Partials_W_hidden = self.Partials[:, -1].reshape(3,1) * self.Activations[:, 0].reshape(1,3)\n            # print(\'Partials_W_hidden: \',self.Partials_W_hidden)\n            self.Partials_W_input = (self.Partials[:, 0].reshape(3,1) * training_instance.T).T\n            # print(\'Partials_W_input: \', self.Partials_W_input)\n\n    def weight_update(self, training_instance, learning_rate):\n\n        # Output Layer weights\n        w_output_old = self.W_output.copy()\n        self.W_output = w_output_old - learning_rate*self.Partials_W_output.reshape(-1,1)\n\n        # Hidden Layer weights\n        w_hidden_old = self.W_hidden.copy()\n        self.W_hidden = w_hidden_old - learning_rate * self.Partials_W_hidden\n        # print(\'W_hidden new: \', self.W_hidden)\n\n        # Input Layer weights\n        w_input_old = self.W_input.copy()\n        self.W_input = w_input_old - learning_rate * self.Partials_W_input\n        # print(\'W_input new: \', self.W_input)\n\n\n    def train_model(self):\n        # print(\'Initially predicted Value: \', self.make_prediction(self.X_test[0]))\n        # print(\'True value: \', self.y_test[0])\n        for _ in range(self.runs):\n            for instance in range(len(self.X_train)):\n                # forward pass\n                self.y_pred = self.forward_pass(self.X_train[instance])\n\n                # Calculate loss\n                self.Loss = self.calc_loss(self.y_pred, self.y_train[instance])\n                # print(\'Loss: \', self.Loss)\n\n                # Calculate backpropagation\n                self.backpropagation(self.y_train[instance], self.X_train[instance])\n\n                # Update weights\n                self.weight_update(self.X_train[instance], self.learning_rate)\n\n        # print(self.Losses)\n        # plt.plot(range(len(self.Losses)), self.Losses)\n        # plt.show()\n\n        # Make predictions\n        predictions = []\n        for i in np.linspace(0,5,1000):\n            predictions.append(self.make_prediction(i)[0])\n        plt.plot(np.linspace(0,5,1000), predictions)\n\n\n    def make_prediction(self, X_new):\n        return self.forward_pass(X_new)\n\n    def calc_loss(self, y_pred, y_true):\n        loss = (1/2)*(y_true-y_pred)**2\n        self.Losses.append(loss[0])\n        return (1/2)*(y_true-y_pred)**2\n\n    def accuracy(self):\n        pass\n\nNeural_Net(\'linear\', 0.1, 1500).train_model()\n'
"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndf = pd.DataFrame({'X1':[100,120,140,200,230,400,500,540,600,625],\n                       'X2':[14,15,22,24,23,31,33,35,40,40],\n                       'Y':[0,0,0,0,1,1,1,1,1,1]})\n\nX = df[['X1','X2']]\ny = df['Y']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4,random_state=42) \n\nX_train, X_test, y_train, y_test\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train) #This is where the training is taking place\ny_pred_logreg = logreg.predict(X_test) #Making predictions to test the model on test data\nprint('Logistic Regression Train accuracy %s' % logreg.score(X_train, y_train)) #Train accuracy\n#Logistic Regression Train accuracy 0.8333333333333334\nprint('Logistic Regression Test accuracy %s' % accuracy_score(y_pred_logreg, y_test)) #Test accuracy\n#Logistic Regression Test accuracy 0.5\nprint(confusion_matrix(y_test, y_pred_logreg)) #Confusion matrix\nprint(classification_report(y_test, y_pred_logreg)) #Classification Report\n"
'loss_op = 1.0 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True)\n'
"import numpy as np\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\npreds = np.array([-1.7059733, -0.914219, 2.6422875, -0.50430596])\nsigmoid(preds)\n# array([0.15368673, 0.28613728, 0.93353404, 0.37652929])\n\nfinal_preds = [1 if x&gt;0.5 else 0 for x in preds]\nfinal_preds\n# [0, 0, 1, 0]\n\nDense(1, activation='sigmoid')\n"
"grid = GridSearchCV(pipeline, param_grid=parameters,scoring='neg_mean_absolute_error', cv=None, refit=True)\n"
'import tensorflow.compat.v1 as tf1\ntf1.disable_v2_behavior() \nimport tensorflow as tf2 #Tensorflow 1.15.2\nfrom tensorflow.contrib.rnn import LSTMCell\n\n\nLSTMCell(num_units=num_nodes[li],\n                            state_is_tuple=True,\n                            initializer= tf.contrib.layers.xavier_initializer()\n                           )\n\nimport tensorflow.compat.v1 as tf1\ntf1.disable_v2_behavior() \nimport tensorflow as tf2 #Tensorflow 2.x\n\ntf2.compat.v1.nn.rnn_cell.LSTMCell(num_units=10,\n                            state_is_tuple=True,\n                            initializer= tf2.initializers.GlorotUniform()\n                           ) \n'
'df = pd.DataFrame({"x":[\'a\', \'b\', \'c\', \'a\', \'b\']})\n\nvalue_dict = {\'a\':\'A\', \'b\':\'A\', \'c\':\'B\'}\ndf[\'x\'] = df[\'x\'].replace(value_dict)\n'
'# Classification with Tensorflow 2.0\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport seaborn as sns\nsns.set(style="darkgrid")\n\ncols = [\'price\', \'maint\', \'doors\', \'persons\', \'lug_capacity\', \'safety\', \'output\']\ncars = pd.read_csv(r\'C:\\\\your_path\\\\cars_dataset.csv\', names=cols, header=None)\n\ncars.head()\n\n\nprice = pd.get_dummies(cars.price, prefix=\'price\')\nmaint = pd.get_dummies(cars.maint, prefix=\'maint\')\n\ndoors = pd.get_dummies(cars.doors, prefix=\'doors\')\npersons = pd.get_dummies(cars.persons, prefix=\'persons\')\n\nlug_capacity = pd.get_dummies(cars.lug_capacity, prefix=\'lug_capacity\')\nsafety = pd.get_dummies(cars.safety, prefix=\'safety\')\n\nlabels = pd.get_dummies(cars.output, prefix=\'condition\')\n\n# To create our feature set, we can merge the first six columns horizontally:\n\nX = pd.concat([price, maint, doors, persons, lug_capacity, safety] , axis=1)\n\n# Let\'s see how our label column looks now:\n\nlabels.head()\n\n\ny = labels.values\n\n# The final step before we can train our TensorFlow 2.0 classification model is to divide the dataset into training and test sets:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n# Model Training\n\n# To train the model, let\'s import the TensorFlow 2.0 classes. Execute the following script:\n\nfrom tensorflow.keras.layers import Input, Dense, Activation,Dropout\nfrom tensorflow.keras.models import Model\n\n\n# The next step is to create our classification model:\ninput_layer = Input(shape=(X.shape[1],))\ndense_layer_1 = Dense(15, activation=\'relu\')(input_layer)\ndense_layer_2 = Dense(10, activation=\'relu\')(dense_layer_1)\noutput = Dense(y.shape[1], activation=\'softmax\')(dense_layer_2)\n\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics=[\'acc\'])\n\n\n# The following script shows the model summary:\n\nprint(model.summary())\n\n\n# Result:\n\n# Model: "model"\n# Layer (type)                 Output Shape              Param #   \n\n\n# Finally, to train the model execute the following script:\nhistory = model.fit(X_train, y_train, batch_size=8, epochs=50, verbose=1, validation_split=0.2)\n\n\n# Result:\n\n# Train on 7625 samples, validate on 1907 samples\n# Epoch 1/50\n# - 4s 492us/sample - loss: 3.0998 - acc: 0.2658 - val_loss: 12.4542 - val_acc: 0.0834\n\n\n# Let\'s finally evaluate the performance of our classification model on the test set:\n\nscore = model.evaluate(X_test, y_test, verbose=1)\n\nprint("Test Score:", score[0])\nprint("Test Accuracy:", score[1])\n\n\n# Result: \n\n\n\n# Regression with TensorFlow 2.0\n\npetrol_cons = pd.read_csv(r\'C:\\\\your_path\\\\gas_consumption.csv\')\n\n# Let\'s print the first five rows of the dataset via the head() function:\n\npetrol_cons.head()\n\n\nX = petrol_cons.iloc[:, 0:4].values\ny = petrol_cons.iloc[:, 4].values\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n# Model Training\n\n# The next step is to train our model. This is process is quite similar to training the classification. The only change will be in the loss function and the number of nodes in the output dense layer. Since now we are predicting a single continuous value, the output layer will only have 1 node.\n\ninput_layer = Input(shape=(X.shape[1],))\ndense_layer_1 = Dense(100, activation=\'relu\')(input_layer)\ndense_layer_2 = Dense(50, activation=\'relu\')(dense_layer_1)\ndense_layer_3 = Dense(25, activation=\'relu\')(dense_layer_2)\noutput = Dense(1)(dense_layer_3)\n\nmodel = Model(inputs=input_layer, outputs=output)\nmodel.compile(loss="mean_squared_error" , optimizer="adam", metrics=["mean_squared_error"])\n\n\n# Finally, we can train the model with the following script:\n\nhistory = model.fit(X_train, y_train, batch_size=2, epochs=100, verbose=1, validation_split=0.2)\n\n\n# Result:\n\n# Train on 30 samples, validate on 8 samples\n# Epoch 1/100\n\n\n# To evaluate the performance of a regression model on test set, one of the most commonly used metrics is root mean squared error. We can find mean squared error between the predicted and actual values via the mean_squared_error class of the sklearn.metrics module. We can then take square root of the resultant mean squared error. Look at the following script:\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\npred_train = model.predict(X_train)\nprint(np.sqrt(mean_squared_error(y_train,pred_train)))\n\n\n# Result:\n\n# 57.398156439652396\n\n\npred = model.predict(X_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))\n\n\n# Result:\n\n# 86.61012708343948\n\n\n# https://stackabuse.com/tensorflow-2-0-solving-classification-and-regression-problems/\n# datasets:\n# https://www.kaggle.com/elikplim/car-evaluation-data-set\n\n\n# for OLS analysis\nimport statsmodels.api as sm\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())\n\n# Results:\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.987\nModel:                            OLS   Adj. R-squared (uncentered):              0.986\nMethod:                 Least Squares   F-statistic:                              867.8\nDate:                Thu, 09 Apr 2020   Prob (F-statistic):                    3.17e-41\nTime:                        13:13:11   Log-Likelihood:                         -269.00\nNo. Observations:                  48   AIC:                                      546.0\nDf Residuals:                      44   BIC:                                      553.5\nDf Model:                           4                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1           -14.2390      8.414     -1.692      0.098     -31.196       2.718\nx2            -0.0594      0.017     -3.404      0.001      -0.095      -0.024\nx3             0.0012      0.003      0.404      0.688      -0.005       0.007\nx4          1630.8913    130.969     12.452      0.000    1366.941    1894.842\n==============================================================================\nOmnibus:                        9.750   Durbin-Watson:                   2.226\nProb(Omnibus):                  0.008   Jarque-Bera (JB):                9.310\nSkew:                           0.880   Prob(JB):                      0.00952\nKurtosis:                       4.247   Cond. No.                     1.00e+05\n==============================================================================\n'
'sklearn.naive_bayes.MultinomialNB\nsklearn.naive_bayes.BernoulliNB\nsklearn.linear_model.Perceptron\nsklearn.linear_model.SGDClassifier\nsklearn.linear_model.PassiveAggressiveClassifier\n\nsklearn.linear_model.SGDRegressor\nsklearn.linear_model.PassiveAggressiveRegressor\n\nsklearn.cluster.MiniBatchKMeans\n\nsklearn.decomposition.MiniBatchDictionaryLearning\nsklearn.cluster.MiniBatchKMeans\n'
"h5f = h5py.File('myfile.hdf5', mode='r')\n\n# This returns a h5py object:\nfoo_ds = h5f['foo']\n# You can slice to get elements like this:\nfoo_slice1 = foo_ds[0,:,:,:] # first row\nfoo_slice2 = foo_ds[-1,:,:,:] # last row\n\n# This is the recommended method to get a Numpy array of the entire dataset:\nfoo_arr = h5f['foo'][:]\n# or, referencing h5py dataset object above\nfoo_arr = foo_ds[:] \n# you can also create an array with a slice\nfoo_slice1 = h5f['foo'][0,:,:,:] \n# is the same as (from above):\nfoo_slice1 = foo_ds[0,:,:,:] \n"
'# Imports\nimport tensorflow\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\n\n# Configuration options\nfeature_vector_length = 784\nnum_classes = 60000\n\n# Load the data\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\n# Reshape the data - MLPs do not understand such things as \'2D\'.\n# Reshape to 28 x 28 pixels = 784 features\nX_train = X_train.reshape(X_train.shape[0], feature_vector_length)\nX_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n\n# Convert into greyscale\nX_train = X_train.astype(\'float32\')\nX_test = X_test.astype(\'float32\')\nX_train /= 255\nX_test /= 255\n\n# Convert target classes to categorical ones\nY_train = to_categorical(Y_train, num_classes)\nY_test = to_categorical(Y_test, num_classes)\n\n# Imports\nimport tensorflow\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n# Configuration options\nfeature_vector_length = 784\nnum_classes = 60000\n\n# Load the data\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n\n# Visualize one sample\nimport matplotlib.pyplot as plt\nplt.imshow(X_train[0], cmap=\'Greys\')\nplt.show()\n\n# Set the input shape\ninput_shape = (feature_vector_length,)\nprint(f\'Feature shape: {input_shape}\')\nX_train=np.array(X_train,dtype="float32")\nX_test=np.array(X_train,dtype="float32")\n# Create the model\nmodel = Sequential()\nmodel.add(Flatten())\nmodel.add(Dense(350, input_shape=input_shape, activation=\'relu\'))\nmodel.add(Dense(50, activation=\'relu\'))\nmodel.add(Dense(num_classes, activation=\'softmax\'))\n\n# Configure the model and start training\n\nmodel.compile(loss=\'categorical_crossentropy\', optimizer=\'adam\', metrics= \n[\'accuracy\'])\nmodel.fit(X_train, Y_train, epochs=10, batch_size=250, verbose=1, \nvalidation_split=0.2)\n\n\n'
"&gt;&gt;&gt;import numpy as np\n&gt;&gt;&gt;a = np.array([[1,2,3], [4,5,6]])\n&gt;&gt;&gt;a\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n&gt;&gt;&gt;a[0,0] = np.nan\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-43-8e6dceaece5a&gt; in &lt;module&gt;\n----&gt; 1 a[0,0] = np.nan\n\nValueError: cannot convert float NaN to integer\n\n&gt;&gt;&gt;a = a.astype('float32')\n&gt;&gt;&gt;a[0,0] = np.nan\n&gt;&gt;&gt;a\narray([[nan,  2.,  3.],\n   [ 4.,  5.,  6.]], dtype=float32)\n"
'num_classes = 200\nmodel_a.fc2 = nn.Linear(num_classes, 256).cuda()\n'
"X,y = data.drop('G3'), data['G3']\n"
'%timeit -n 10 -r 10 a @ b\n'
'random_forest.fit(X_train, y_train)\npred = random_forest.predict(X_test) \n'
"# Import packages\nimport os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nimport sys\n\n# This is needed since the notebook is stored in the object_detection folder.\nsys.path.append(&quot;..&quot;)\n\n# Import utilites\nfrom utils import label_map_util\nfrom utils import visualization_utils as vis_util\n\n# Name of the directory containing the object detection module we're using\nMODEL_NAME = 'inference_graph'\nIMAGE_NAME = 'test1.jpg'\n\n# Grab path to current working directory\nCWD_PATH = os.getcwd()\n\n# Path to frozen detection graph .pb file, which contains the model that is used\n# for object detection.\nPATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')\n\n# Path to label map file\nPATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')\n\n# Path to image\nPATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)\n\n# Number of classes the object detector can identify\nNUM_CLASSES = 6\n\n# Load the label map.\n# Label maps map indices to category names, so that when our convolution\n# network predicts `5`, we know that this corresponds to `king`.\n# Here we use internal utility functions, but anything that returns a\n# dictionary mapping integers to appropriate string labels would be fine\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\n\n# Load the Tensorflow model into memory.\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\n    od_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n        serialized_graph = fid.read()\n        od_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(od_graph_def, name='')\n\n    sess = tf.Session(graph=detection_graph)\n\n# Define input and output tensors (i.e. data) for the object detection classifier\n\n# Input tensor is the image\nimage_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n\n# Output tensors are the detection boxes, scores, and classes\n# Each box represents a part of the image where a particular object was detected\ndetection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n\n# Each score represents level of confidence for each of the objects.\n# The score is shown on the result image, together with the class label.\ndetection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\ndetection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n\n# Number of objects detected\nnum_detections = detection_graph.get_tensor_by_name('num_detections:0')\n\n# Load image using OpenCV and\n# expand image dimensions to have shape: [1, None, None, 3]\n# i.e. a single-column array, where each item in the column has the pixel RGB value\nimage = cv2.imread(PATH_TO_IMAGE)\nimage_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_expanded = np.expand_dims(image_rgb, axis=0)\n\n# Perform the actual detection by running the model with the image as input\n(boxes, scores, classes, num) = sess.run(\n    [detection_boxes, detection_scores, detection_classes, num_detections],\n    feed_dict={image_tensor: image_expanded})\n\n# Draw the results of the detection (aka 'visulaize the results')\n\nvis_util.visualize_boxes_and_labels_on_image_array(\n    image,\n    np.squeeze(boxes),\n    np.squeeze(classes).astype(np.int32),\n    np.squeeze(scores),\n    category_index,\n    use_normalized_coordinates=True,\n    line_thickness=8,\n    min_score_thresh=0.60)\n\n# All the results have been drawn on image. Now display the image.\ncv2.imshow('Object detector', image)\n\n# Press any key to close the image\ncv2.waitKey(0)\n\n# Clean up\ncv2.destroyAllWindows()\n"
'def init_py_script\n  parsed_output = JSON.parse(`cat #{file.path} | python3 lib/some_python_script.py`)\nend\n'
"def process_image(image_path):\n    image = Image.open(image_path)\n    new_image = image.resize((224,224))\n    np_image = asarray(new_image)\n    np_image = np_image.astype('uint8')\n\n    return [np_image]\n"
"df =  df['Tickets']\n\ndf =  df[['Tickets']]\n"
"mobile = tf.keras.applications.mobilenet.MobileNet( include_top=False,\n                                                           input_shape=(224, 224,3),\n                                                           pooling='max', weights='imagenet',\n                                                           alpha=1, depth_multiplier=1,dropout=.5)                                                          \nx=mobile.layers[-1].output\nx=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\npredictions=Dense (4, activation='softmax')(x)\nmodel = Model(inputs=mobile.input, outputs=predictions)    \nfor layer in model.layers:\n    layer.trainable=True\nmodel.compile(Adamax(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\ncheckpoint=tf.keras.callbacks.ModelCheckpoint(filepath=save_loc, monitor='val_loss', verbose=0, save_best_only=True,\n    save_weights_only=False, mode='auto', save_freq='epoch', options=None)\nlr_adjust=tf.keras.callbacks.ReduceLROnPlateau( monitor=&quot;val_loss&quot;, factor=0.5, patience=1, verbose=0, mode=&quot;auto&quot;,\n    min_delta=0.00001,  cooldown=0,  min_lr=0) \ncallbacks=[checkpoint, lr_adjust]\n\n\n  [1]: http://httphttps://keras.io/api/applications/mobilenet/s://\n  [2]: https://keras.io/api/callbacks/reduce_lr_on_plateau/\n  [3]: https://keras.io/api/callbacks/model_checkpoint/\n"
'from sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_boston\nimport numpy as np\n\nX, y = load_boston(return_X_y=True)\nreg = LinearRegression().fit(X, y)\n\nybar = reg.predict(X)\nymean = y.mean()\n\n1 - sum((y-ybar)**2) / sum((y-ymean)**2)\n0.7406426641094095\n\nreg.score(X, y)\n0.7406426641094095\n\nnp.mean(abs(y-ybar)/y)\n0.16417298806489977\n'
"def nvidia_model():\n  model = Sequential()\n  model.add(Conv2D(24,(5,5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu'))\n  model.add(Conv2D(36, (5,5), strides=(2, 2), activation='elu'))\n  model.add(Conv2D(48, (5,5), strides=(2, 2), activation='elu'))\n  model.add(Conv2D(64, (3,3), activation='elu'))\n  \n  model.add(Conv2D(64, (3,3), activation='elu'))\n#   model.add(Dropout(0.5))\n  \n  \n  model.add(Flatten())\n  \n  model.add(Dense(100, activation = 'elu'))\n#   model.add(Dropout(0.5))\n  \n  model.add(Dense(50, activation = 'elu'))\n#   model.add(Dropout(0.5))\n  \n  model.add(Dense(10, activation = 'elu'))\n#   model.add(Dropout(0.5))\n\n  model.add(Dense(1))\n  \n  optimizer = Adam(lr=1e-3)\n  model.compile(loss='mse', optimizer=optimizer)\n  return model\nmodel = nvidia_model()\nprint(model.summary())\n"
'print(len(train_dataloader.sampler), len(valid_dataloader.sampler))\n'
"df['A'] = df.A.str.get_dummies().drop('d', axis=1).to_numpy().tolist()\ndf\nOut[237]: \n           A\n0  [1, 0, 0]\n1  [0, 1, 0]\n2  [1, 0, 0]\n3  [0, 0, 1]\n4  [0, 1, 0]\n5  [0, 0, 0]\n6  [1, 0, 0]\n"
"import requests\n\nurl = 'https://www.nasdaq.com/api/v1/historical/AMZN/stocks/2010-10-26/2020-10-26'\nheaders = { &quot;user-agent&quot;:&quot;Mozilla&quot;} \n\nr = requests.get(url, headers=headers)\n\nopen('HistoricalQuotes_2020_10_26.csv', 'wb').write(r.content)\n"
"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.Input(shape=(128,128,3)))\nmodel.add(tf.keras.layers.Conv2D(16, 5, activation=&quot;tanh&quot;, padding=&quot;same&quot;))\nmodel.add(tf.keras.layers.MaxPool2D(2)) # 64x54\nmodel.add(tf.keras.layers.Conv2D(16, 5, activation=&quot;tanh&quot;, padding=&quot;same&quot;))\nmodel.add(tf.keras.layers.MaxPool2D(2)) # 32x32\nmodel.add(tf.keras.layers.Conv2D(16, 5, activation=&quot;tanh&quot;, padding=&quot;same&quot;))\nmodel.add(tf.keras.layers.MaxPool2D(2)) # 16x16\nmodel.add(tf.keras.layers.Conv2D(16, 5, activation=&quot;tanh&quot;, padding=&quot;same&quot;))\nmodel.add(tf.keras.layers.MaxPool2D(2)) # 8x8\nmodel.add(tf.keras.layers.Conv2D(16, 5, activation=&quot;tanh&quot;, padding=&quot;same&quot;))\nmodel.add(tf.keras.layers.MaxPool2D(2)) # 4x4\nmodel.add(tf.keras.layers.Conv2D(16, 4, activation=&quot;tanh&quot;, padding=&quot;valid&quot;))\nmodel.add(tf.keras.layers.Conv2D(2, 1, activation=&quot;softmax&quot;, padding=&quot;same&quot;))\nmodel.add(tf.keras.layers.Flatten())\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.fit(INPUT, OUTPUT, epochs=10)\n\n2) model.add(tf.keras.Input(shape=(500,5)))\n3) model.add(tf.keras.layers.Dense(500*5, activation=&quot;relu&quot;))\n4) model.add(tf.keras.layers.Dense(1,activation=&quot;softmax&quot;))\n"
'print(y_train.unique())\n'
"crop = frame[y:y + h, x:x + w] #Cropping the original image to use only desired coordinates\nif (crop.shape) &gt; (300, 300):\n   crop = cv.resize(crop, (300, 300)) #resize only if the crop is larger than required size\nimg_blob = cv.dnn.blobFromImage(crop) #This model requires 300*300 images since I'm using it with res10_300x300_ssd_iter_140000\n"
"model.add(Dense(16, input_dim=5, activation='relu'))\n"
'X = X.reshape(1, *X.shape)\n\nX.unsqueeze(0)\n'
"tf.keras.layers.Dense(1, activation='sigmoid')\n\ntf.keras.layers.Dense(2, activation='softmax') \n"
' !python train_frcnn.py -o simple -p training_annotation.txt\n'
'cv = 10\n'
"import numpy as np\n\n# I'll use numbers instead of words,\n# but same exact concept\npoints_list = [[0,1,2],\n               [0,3],\n               [1,4],\n               [0,2]]\n\nscores = np.zeros((5,5))\n\nfor points in points_list:\n    temp = np.array(points)[:, np.newaxis]       \n    scores[temp, points] += 1\n\n&gt;&gt;&gt; scores\narray([[ 3.,  1.,  2.,  1.,  0.],\n       [ 1.,  2.,  1.,  0.,  1.],\n       [ 2.,  1.,  2.,  0.,  0.],\n       [ 1.,  0.,  0.,  1.,  0.],\n       [ 0.,  1.,  0.,  0.,  1.]])\n\nimport numpy as np\n\n# I'll use numbers instead of words,\n# but same exact concept\npoints_list = [[0,1,2],\n               [0,3],\n               [1,4],\n               [0,2],\n               [0,1,2,3],\n               [0,1,2,4]]\n\nscores = np.zeros((5,5))\n\nfor points in points_list:\n    temp = np.array(points)[:, np.newaxis]       \n    scores[temp, points] += 1\n\n\ndiag = scores.diagonal()\n\nkey_col = (scores/diag)[:, 0]\nkey_col[0] = 0\n\npoints_2 = np.where(key_col &gt; 0.5)[0]      # suppose 0.5 is the threshold \ntemp_2 = np.array(points_2)[:, np.newaxis] # step 1: we identified the points that are\n                                           # close to 0\ninner_scores = scores[temp_2, points_2]    # step 1: we are checking if those points are\n                                           # are close to each other\n\n&gt;&gt;&gt; scores\narray([[ 5.,  3.,  4.,  2.,  1.], # We identified that 1 and 2 are close to 0\n       [ 3.,  4.,  3.,  1.,  2.],\n       [ 4.,  3.,  4.,  1.,  1.],\n       [ 2.,  1.,  1.,  2.,  0.],\n       [ 1.,  2.,  1.,  0.,  2.]])\n&gt;&gt;&gt; inner_scores\narray([[ 4.,  3.],                # Testing to see whether 1 and 2 are close together\n       [ 3.,  4.]])               # Since they are, we can conclude that (0,1,2) occur \n                                  # together\n"
"x=list(reader)\nresult=numpy.array(x).astype('float')\n"
'import numpy as np\nfrom scipy.optimize import fmin_bfgs\nimport io\ndata = np.loadtxt(\'ex2data1.txt\',delimiter=",")\nm,n = data.shape\nX = np.array(np.column_stack((np.ones(m),data[:,:-1])))\ny = np.array(data[:,2].reshape(m,1))\ntheta = np.array(np.zeros(n).reshape(n,1))\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\ndef hypothesis(X,theta):\n    return sigmoid( X.dot(theta) )\n\ndef cost(theta):\n    h = hypothesis(X,theta)\n    cost = (-y.T.dot(np.log(h))-(1-y).T.dot(np.log(1-h)))/m\n    r = cost[0]\n    if np.isnan(r):\n        return np.inf\n    return r\n\ndef gradient(theta):\n    theta = theta.reshape(-1, 1)\n    h = hypothesis(X,theta)\n    grad = ((h-y).T.dot(X)).T/m\n    return grad.flatten()\n\ndef fmin():\n    initial_theta=np.zeros(n)\n    theta=fmin_bfgs(cost,initial_theta,fprime=gradient)\n    return theta\n\ntheta = fmin()\n'
'X.append(np.asarray(im, dtype=np.uint8))\n\narray([255, 255, 255, ..., 255, 255, 255], dtype=uint8)\n\narray([\n   [255, 255, 255, ..., 255, 255, 255],\n   [255, 255, 255, ..., 255, 255, 255],\n   [255,   0,   0, ...,   0,   0,   0],\n   ..., \n   [255,   0,   0, ...,   0,   0,   0],\n   [255, 255, 255, ..., 255, 255, 255],\n   [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)\n\nX.append(np.asarray(im, dtype=np.uint8).ravel())\n'
'Distance Calculation Metric : levenshtein distance\nClustering Algorithm : DBSCAN\n\nQuestion 2 : \nIs it even worth considering to create such large similarity matrix and store is in disk and use it later to calculate distances.\n\n3. I would really appreciate if someone could provide the big O complexity for the distance based approach. \n'
'def auc(y_true, y_val, plot=False):  \n#check input\nif len(y_true) != len(y_val):\n    raise ValueError(\'Label vector (y_true) and corresponding value vector (y_val) must have the same length.\\n\')\n#empty arrays, true positive and false positive numbers\ntp = []\nfp = []\n#count 1\'s and -1\'s in y_true\ncond_positive = list(y_true).count(1)\ncond_negative = list(y_true).count(-1)\n#all possibly relevant bias parameters stored in a list\nbias_set = sorted(list(set(y_val)), key=float, reverse=True)\nbias_set.append(min(bias_set)*0.9)\n\n#initialize y_pred array full of negative predictions (-1)\ny_pred = np.ones(len(y_true))*(-1)\n\n#the computation time is mainly influenced by this for loop\n#for a contamination rate of 1% it already takes ~8s to terminate\nfor bias in bias_set:\n    #"lower values tend to correspond to label −1"\n    #indices of values which exceed the bias\n    posIdx = np.where(y_val &gt; bias)\n    #set predicted values to 1\n    y_pred[posIdx] = 1\n    #the following function simply calculates results which enable a distinction \n    #between the cases of true positive and  false positive\n    results = np.asarray(y_true) + 2*np.asarray(y_pred)\n    #append the amount of tp\'s and fp\'s\n    tp.append(float(list(results).count(3)))\n    fp.append(float(list(results).count(1)))\n\n#calculate false positive/negative rate\ntpr = np.asarray(tp)/cond_positive\nfpr = np.asarray(fp)/cond_negative\n#optional scatterplot\nif plot == True:\n    plt.scatter(fpr,tpr)\n    plt.show()\n#calculate AUC\nAUC = np.trapz(tpr,fpr)\n\nreturn AUC\n'
"The price for Ron95 in next month will be RM [ 'B']\n"
"movie_groupby = movie_data.groupby('movie_id').agg(lambda v: np.sum(v['textvec']))\n"
'h(x) = &lt;q, [1 x]&gt; = q0 + q1*x0 + q2*x1 + q3*x2 + q4*x3\n\ndef h(x):\n   a = np.ones(x.shape[0], x.shape[1] + 1)\n   a[:, 1:5] = x\n   return a.dot(q)\n'
'random_state: RandomState or an int seed (None by default)\n'
'for probs, pred in zip(clf.predict_proba(x), clf.predict(x)):\n  print probs[pred]\n\nfor probs, truth in zip(clf.predict_proba(x), y):\n  print probs[truth]\n'
"additional_data\n      |\n      |-&gt; sports.cricket\n                |\n                |-&gt; file1.txt\n                |-&gt; file2.txt\n                |-&gt; ...\n      |\n      |-&gt; cooking.french\n                |\n                |-&gt; file1.txt\n                |-&gt; ...\n       ...\n\nimport os\n\nfrom sklearn import cross_validation\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.datasets import load_files\nfrom sklearn.externals import joblib\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n# Note if you have a pre-defined training/testing split in your additional data, you would merge them with the corresponding 'train' and 'test' subsets of 20news\nnews_data = fetch_20newsgroups(subset='all')\nadditional_data = load_files(container_path='/path/to/additional_data', encoding='utf-8')\n\n# Both data objects are of type `Bunch` and therefore can be relatively straightforwardly merged\n\n# Merge the two data files\n'''\nThe Bunch object contains the following attributes: `dict_keys(['target_names', 'description', 'DESCR', 'target', 'data', 'filenames'])`\nThe interesting ones for our purposes are 'data' and 'filenames'\n'''\nall_filenames = np.concatenate((news_data.filenames, additional_data.filenames)) # filenames is a numpy array\nall_data = news_data.data + additional_data.data # data is a standard python list\n\nmerged_data_path = '/path/to/merged_data'\n\n'''\nThe 20newsgroups data has a filename a la '/path/to/scikit_learn_data/20news_home/20news-bydate-test/rec.sport.hockey/54367'\nSo depending on whether you want to keep the sub directory structure of the train/test splits or not, \nyou would either need the last 2 or 3 parts of the path\n'''\nfor content, f in zip(all_data, all_filenames):\n    # extract sub path\n    sub_path, filename = f.split(os.sep)[-2:]\n\n    # Create output directory if not exists\n    p = os.path.join(merged_data_path, sub_path)\n    if (not os.path.exists(p)):\n        os.makedirs(p)\n\n    # Write data to file\n    with open(os.path.join(p, filename), 'w') as out_file:\n        out_file.write(content)\n\n# Now that everything is stored at `merged_data_path`, we can use `load_files` to fetch the dataset again, which now includes everything from 20newsgroups and your additional data\nall_data = load_files(container_path=merged_data_path)\n\n'''\nall_data is yet another `Bunch` object:\n    * `data` contains the data\n    * `target_names` contains the label names\n    * `target contains` the labels in numeric format\n    * `filenames` contains the paths of each individual document\n\nthus, running a classifier over the data is straightforward\n'''\nvec = CountVectorizer()\nX = vec.fit_transform(all_data.data)\n\n# We want to create a train/test split for learning and evaluating a classifier (supposing we haven't created a pre-defined train/test split encoded in the directory structure)\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, all_data.target, test_size=0.2)\n\n# Create &amp; fit the MNB model\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n\n# Evaluate Accuracy\ny_predicted = mnb.predict(X_test)\n\nprint('Accuracy: {}'.format(accuracy_score(y_test, y_predicted)))\n\n# Alternatively, the vectorisation and learning can be packaged into a pipeline and serialised for later use\npipeline = Pipeline([('vec', CountVectorizer()), ('mnb', MultinomialNB())])\n\n# Run the vectorizer and train the classifier on all available data\npipeline.fit(all_data.data, all_data.target)\n\n# Serialise the classifier to disk\njoblib.dump(pipeline, '/path/to/model_zoo/mnb_pipeline.joblib')\n\n# If you get some more data later on, you can deserialise the model and run them through the pipeline again\np = joblib.load('/path/to/model_zoo/mnb_pipeline.joblib')\n\ndocs_new = ['God is love', 'OpenGL on the GPU is fast']\n\ny_predicted = p.predict(docs_new)\nprint('Predicted labels: {}'.format(np.array(all_data.target_names)[y_predicted]))    \n"
'def layer(input):\n    hidden_weights = tf.Variable( tf.truncated_normal([n*n, 1]) )\n    hidden_layer = tf.nn.matmul( tf.matmul( input, hidden_weights), tf_ph_unit)\n    return hidden_layer\n\ninput = tf_input\nfor i in range(10):\n    input = layer(input)\n'
'session.run(tf.initialize_all_variables())\n\nsession.run(tf.assign(b,tf.ones((1,2))))\n'
"# pre-pad short sequences, equalize frame dimensions, and equalize sequence lengths\nif len(frames) &lt; self_frames_per_sequence:\n    frames = [frames[0]]*(self_frames_per_sequence - len(frames)) + frames\nframes = frames[0:self_frames_per_sequence]\nframes = [cv2.resize(frame, (self_columns, self_rows)).astype('float32') for frame in frames]\nframes = np.asarray(frames)\n\nrotated_frames = random_rotation(frames, rg=45)\nshifted_frames = random_shift(rotated_frames, wrg=0.25, hrg=0.25)\nsheared_frames = random_shear(shifted_frames, intensity=0.79)\nzoomed_frames = random_zoom(sheared_frames, zoom_range=(1.25, 1.25))\n"
'# Assuming that y contains values from 0 to N-1 where N is number of classes\nbatchX = (np.arange(max(batchX.flatten())+1) = batchX[:,:,None]).astype(int)\n'
'import matplotlib.pyplot as plt\nimport numpy as np; np.random.seed(1)\n\n\ndef modified_bland_altman_plot(predicted, truth):\n    predicted = np.asarray(predicted)\n    truth = np.asarray(truth) \n    diff = predicted - truth\n\n    fig, ax = plt.subplots()\n    ax.scatter(truth, diff, s=9, c=truth, cmap="rainbow")\n    ax.set_xlabel(\'truth\')\n    ax.set_ylabel(\'difference from truth\')\n    ax.set_title("Modified Bland-Altman Plot")\n\n    # Plot a horizontal line at 0\n    ax.axhline(0, ls=":", c=".2")\n\n    return ax\n\nx = np.random.rayleigh(scale=10, size=201)\ny = np.random.normal(size=len(x))+10-x/10.\n\nmodified_bland_altman_plot(y, x)\n\nplt.show()\n'
'df = pd.DataFrame(\n    data=[{ "Criteria1" : 5, "Criteria2" : 2, "Criteria3" : 1 },\n          { "Criteria1" : 0, "Criteria2" : 1, "Criteria3" : 5 },\n          { "Criteria1" : 4, "Criteria2" : 2, "Criteria3" : 1 }],\n    index=["item1", "item2", "item3"])\n\n       Criteria1  Criteria2  Criteria3\nitem1          5          2          1\nitem2          0          1          5\nitem3          4          2          1\n\n&gt;&gt; from sklearn.metrics.pairwise import cosine_similarity\n&gt;&gt; cosine_similarity(pd.Series(useritem), df)\narray([[ 0.98524468,  0.40967325,  0.98765833]])\n'
'    dataset = pandas.read_csv(url, names=names)\nNameError: name \'pandas\' is not defined\n\nimport pandas\nurl = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"\nnames = [\'sepal-length\', \'sepal-width\', \'petal-length\', \'petal-width\', \'class\']\ndataset = pandas.read_csv(url, names=names)\n\ndataset.head(3)\n\n   sepal-length  sepal-width  petal-length  petal-width        class\n0           5.1          3.5           1.4          0.2  Iris-setosa\n1           4.9          3.0           1.4          0.2  Iris-setosa\n2           4.7          3.2           1.3          0.2  Iris-setosa\n'
"pre_proc = ['Normalizer','MaxAbsScaler','MinMaxScaler','KernelCenterer', ...\n\npre_proc = [preprocessing.Normalizer, preprocessing.MaxAbsScalar, preprocessing.MinMaxScalar, preprocessing.KernelCenterer, preprocessing.StandardScaler]\n"
"plt.plot(x_df,y_df,'go', X[:, 0], new_h,'bo')\n\nplt.plot(x_df,y_df,'go', x_df.ix[:, 0], n,'bo')\n"
"dt = DecisionTreeClassifier()\ndt = dt.fit([[1],[2],[3]], [[2],[3],[4]])   #It should be dt.fit not clf.fit\n\ndot_data = export_graphviz(dt, out_file=None, \n                     feature_names=['1' , '2', '3','4'],  \n                     class_names=['true' , 'false','something_else'],  \n                     filled=True, rounded=True,  \n                     special_characters=True)  \n"
'sel = np.logical_or(ytrain_old == 2, ytrain_old == 3)\nXtrain = Xtrain_old.reshape((-1,500))[sel]\nytrain = ytrain_old[sel]\n'
"def get_next_input():\n    embedded_value = encoder_inputs_embedded[:, time, :]\n    embedded_value.set_shape([batch_size, input_embedding_size])\n    return embedded_value\n\nimport tensorflow as tf\nimport numpy as np\n\nbatch_size, max_time, input_embedding_size = 5, 10, 16\nvocab_size, num_units = 50, 64\n\nencoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\nencoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.int32, name='encoder_inputs_length')\n\nembeddings = tf.Variable(tf.random_uniform([vocab_size + 2, input_embedding_size], -1.0, 1.0),\n                         dtype=tf.float32, name='embeddings')\nencoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)\n\ncell = tf.contrib.rnn.LSTMCell(num_units)\nW = tf.Variable(tf.random_uniform([num_units, vocab_size], -1, 1), dtype=tf.float32, name='W_reader')\nb = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32, name='b_reader')\ngo_time_slice = tf.ones([batch_size], dtype=tf.int32, name='GO') * 1\ngo_step_embedded = tf.nn.embedding_lookup(embeddings, go_time_slice)\n\n\nwith tf.variable_scope('ReaderNetwork'):\n    def loop_fn_initial():\n        init_elements_finished = (0 &gt;= encoder_inputs_length)\n        init_input = go_step_embedded\n        init_cell_state = cell.zero_state(batch_size, tf.float32)\n        init_cell_output = None\n        init_loop_state = None\n        return (init_elements_finished, init_input,\n                init_cell_state, init_cell_output, init_loop_state)\n\n    def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n        def get_next_input():\n            embedded_value = encoder_inputs_embedded[:, time, :]\n            embedded_value.set_shape([batch_size, input_embedding_size])\n            return embedded_value\n\n        elements_finished = (time &gt;= encoder_inputs_length)\n        finished = tf.reduce_all(elements_finished)  # boolean scalar\n        next_input = tf.cond(finished,\n                             true_fn=lambda: tf.zeros([batch_size, input_embedding_size], dtype=tf.float32),\n                             false_fn=get_next_input)\n        state = previous_state\n        output = previous_output\n        loop_state = None\n        return elements_finished, next_input, state, output, loop_state\n\n\n    def loop_fn(time, previous_output, previous_state, previous_loop_state):\n        if previous_state is None:  # time = 0\n            return loop_fn_initial()\n        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n\nreader_loop = loop_fn\nencoder_outputs_ta, encoder_final_state, _ = tf.nn.raw_rnn(cell, loop_fn=reader_loop)\noutputs = encoder_outputs_ta.stack()\n\n\ndef next_batch():\n    return {\n        encoder_inputs: np.random.randint(0, vocab_size, (batch_size, max_time)),\n        encoder_inputs_length: [max_time] * batch_size\n    }\n\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as s:\n    s.run(init)\n    outs = s.run([outputs], feed_dict=next_batch())\n    print len(outs), outs[0].shape\n"
"pipeline = Pipeline([\n    ('clf', OneVsRestClassifier(SVC(), n_jobs=1)),\n])\n\nparameters = [\n\n    {'clf__estimator__kernel': ['rbf'],\n     'clf__estimator__gamma': [1e-3, 1e-4],\n     'clf__estimator__C': [1, 10]\n    },\n\n    {'clf__estimator__kernel': ['poly'],\n     'clf__estimator__C': [1, 10]\n    }\n     ]\n\ngrid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=3, verbose=10)\n"
'from sklean.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nX_vec = tfidf.fit_transform(X)\nX_vec.toarray()\narray([[ 0.        ,  1.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.6191303 ,  0.78528828,  0.        ],\n       [ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.6191303 ,  0.        ,  0.78528828]])\ncross_validation.cross_val_score(clf, X_vec, y, cv=2)\narray([ 0.5,  0.5])\n'
'alphas = tf.Variable(...)\n# Define some loss and training_op here\n\ndef get_alphas(sess, weights, filters):\n  for some_epochs:\n    sess.run(training_op)\n\n# The rest of the training...\n'
"substitutions = {\n    layer_input : C.placeholder(name='cloned_layer_input')\n}\ncloned_layer = layer_root.clone(clone_method, substitutions)\n"
'import numpy as np\n\nX = np.random.random(size=(50000, 3072))\ny = np.random.random(size=50000)\n\nperm = np.random.permutation(X.shape[0])  # assuming X.shape[0] == y.shape[0]\nX_perm = X[perm]  # views!!!\ny_perm = y[perm]\n'
'if self.fit_intercept:\n    self.intercept_ = parameters[-2]\nelse:\n    self.intercept_ = 0.0\nself.coef_ = parameters[:X.shape[1]]\n'
'gradient = X.T.dot(error) / X.shape[0]\nW += - 0.44 * gradient\n\nloss = np.sum(error ** 2)\nlossHistory.append(loss)\n'
"&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; l = tf.layers.Conv2DTranspose(filters=64, kernel_size=1, padding='SAME', strides=[2, 2])\n&gt;&gt;&gt; l(tf.ones([12, 7, 7, 128]))\n&lt;tf.Tensor 'conv2d_transpose/BiasAdd:0' shape=(12, 14, 14, 64) dtype=float32&gt;\n&gt;&gt;&gt; l.kernel\n&lt;tf.Variable 'conv2d_transpose/kernel:0' shape=(1, 1, 64, 128) dtype=float32_ref&gt;\n&gt;&gt;&gt; \n"
'np.random.seed(0)\nm = np.random.randint(0, 10, (5, 5))\nprint(m)\n\n[[5 0 3 3 7]\n [9 3 5 2 4]\n [7 6 8 8 1]\n [6 7 7 8 1]\n [5 9 8 9 4]]\n\nindicator = np.array([0,1,1,0,1])\ngroup1 = np.where(indicator == 0)[0] # [0 3]\ngroup2 = np.where(indicator == 1)[0] # [1 2 4]\n\n&gt;&gt;&gt; m[group1, :][:, group1]\n[[5 3]\n [6 8]]\n&gt;&gt;&gt; m[group2, :][:, group2]\n[[3 5 4]\n [6 8 1]\n [9 8 4]]\n'
"saver = tf.train.Saver()\nwith tf.Session() as sess:\n    saver.restore(sess, 'path to your saved file C:x/y/z/model/model.ckpt')\n"
' print([v.name for v in tf.all_variables()])\n\nsess.run(tf.initialize_all_variables())\ngradients_and_vars = sess.run([variable for grad,variable in grads_and_vars], feed_dict={x:data, y:labels})\nprint(gradients_and_vars)\n'
"output = Dense(3, activation='softmax')(x)\nmodel.compile(optimizer=Optimizer,\n          loss='categorical_crossentropy')\n\noutput = Dense(3)(x)\nmodel.compile(optimizer=Optimizer,\n          loss='mse')\n\nmodel.compile(optimizer=Optimizer,\n              loss='categorical_crossentropy',\n              metrics=['mse']\n              )\n\nmodel.compile(optimizer=Optimizer,\n              loss='mse')\n"
'return {\n        payload: [[...msg.payload.user_code,msg.payload.datetime]]\n};\n'
"from keras import models, layers\n\nn_features = 20\n\nmodel_input = layers.Input(shape=(12, None, n_features))\nx = layers.TimeDistributed(layers.LSTM(64, return_sequences=True))(model_input)\nmodel_output = layers.Dense(n_features, activation='softmax')(x)\n\nmodel = models.Model([model_input], [model_output])\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\nmodel.summary()\n\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 12, None, 20)      0         \n_________________________________________________________________\ntime_distributed_1 (TimeDist (None, 12, None, 64)      21760     \n_________________________________________________________________\ndense_1 (Dense)              (None, 12, None, 20)      1300      \n=================================================================\nTotal params: 23,060\nTrainable params: 23,060\nNon-trainable params: 0\n_________________________________________________________________\n"
"model = LogisticRegression()\nmodel.fit(X_train, Y_train)\n\nimport pickle\nfilename = 'finalized_model.sav'\npickle.dump(model, open(filename, 'wb'))\n\nloaded_model = pickle.load(open(filename, 'rb'))\n\nresult = loaded_model.score(X_test, Y_test)\nprint(result)\n"
"log_loss = log_loss(ytest, soft)\n\nfrom sklearn.metrics import log_loss\nprint(log_loss)\n&gt;&gt;&gt; &lt;function log_loss at 0x7f9f692db1b8&gt;\n\nlog_loss = log_loss(ytest, soft)\nprint(log_loss)\n&gt;&gt;&gt; 0.11895972559889094\nlog_loss = log_loss(ytest, soft)\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-40-b423b2324b92&gt; in &lt;module&gt;()\n----&gt; 1 log_loss = log_loss(ytest, soft)\n\nTypeError: 'numpy.float64' object is not callable\n\nfrom sklearn.metrics import log_loss\n...\nloss = log_loss(ytest, soft)\n\nfrom sklearn import metrics\n...\nloss = metrics.log_loss(ytest, soft)\n"
'model.add(Lambda(lambda x: K.l2_normalize(x, axis=-1)))  \n'
'Conv2D(..., input_shape=input_shape1, ...)\n\ninput_shape1=(img_rows, img_cols, 3)\n'
"def train(dataVar, dfNew):\n    ret = {}\n    index = 0\n    for k, i in enumerate(dfNew['Sentence_txt'].values):\n        if k in kFoldsTrain1:\n            dataVar = dataVar.append({index:i}, ignore_index=True)\n\n    ret['x'] = count_vect.fit_transform(dataVar[0].values)\n    ret['y'] = [i for k,i in enumerate(dfNew['Sentence_Polarity'].values) if k in kFoldsTrain1]\n    ret['y'] = np.asarray(Y_train1)\n\n    return ret\n\n#KFOLD-1\nkfold1 = train(pd.DataFrame(columns=['Sentence_txt']), dfNew)\n\n#KFOLD-2\nkfold2 = train(pd.DataFrame(columns=['Sentence_txt']), dfNew)\n"
"filename = 'finalized_model.sav' \n\nfilename = './finalized_model.sav'\n\nimport os\nfilename = os.path.abspath(__file__) + os.sep + 'finalized_model.sav'\n"
'import numpy, scipy, matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom scipy.optimize import differential_evolution\nimport warnings\n\ndata = [(1, 1), (2, 3), (3, 5), (4, 8), (5, 9), (6, 9), (7, 9)]\n\n# data to float arrays\nxData = []\nyData = []\nfor d in data:\n    xData.append(float(d[0]))\n    yData.append(float(d[1]))\n\n\ndef func(x, a, b, c): #sigmoidal curve fitting function\n    return  a / (1.0 + numpy.exp(-1.0 * (x - b) / c))\n\n\n# function for genetic algorithm to minimize (sum of squared error)\ndef sumOfSquaredError(parameterTuple):\n    warnings.filterwarnings("ignore") # do not print warnings by genetic algorithm\n    val = func(xData, *parameterTuple)\n    return numpy.sum((yData - val) ** 2.0)\n\n\ndef generate_Initial_Parameters():\n    # min and max used for bounds\n    maxX = max(xData)\n    minX = min(xData)\n    maxY = max(yData)\n    minY = min(yData)\n\n    minXY = min(minX, minY)\n    maxXY = min(maxX, maxY)\n\n    parameterBounds = []\n    parameterBounds.append([minXY, maxXY]) # search bounds for a\n    parameterBounds.append([minXY, maxXY]) # search bounds for b\n    parameterBounds.append([minXY, maxXY]) # search bounds for c\n\n    # "seed" the numpy random number generator for repeatable results\n    result = differential_evolution(sumOfSquaredError, parameterBounds, seed=3)\n    return result.x\n\n# by default, differential_evolution completes by calling curve_fit() using parameter bounds\ngeneticParameters = generate_Initial_Parameters()\n\n# now call curve_fit without passing bounds from the genetic algorithm,\n# just in case the best fit parameters are aoutside those bounds\nfittedParameters, pcov = curve_fit(func, xData, yData, geneticParameters)\nprint(\'Fitted parameters:\', fittedParameters)\nprint()\n\nmodelPredictions = func(xData, *fittedParameters) \n\nabsError = modelPredictions - yData\n\nSE = numpy.square(absError) # squared errors\nMSE = numpy.mean(SE) # mean squared errors\nRMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\nRsquared = 1.0 - (numpy.var(absError) / numpy.var(yData))\n\nprint()\nprint(\'RMSE:\', RMSE)\nprint(\'R-squared:\', Rsquared)\n\nprint()\n\n\n##########################################################\n# graphics output section\ndef ModelAndScatterPlot(graphWidth, graphHeight):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    # first the raw data as a scatter plot\n    axes.plot(xData, yData,  \'D\')\n\n    # create data for the fitted equation plot\n    xModel = numpy.linspace(min(xData), max(xData))\n    yModel = func(xModel, *fittedParameters)\n\n    # now the model as a line plot\n    axes.plot(xModel, yModel)\n\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot\n\ngraphWidth = 800\ngraphHeight = 600\nModelAndScatterPlot(graphWidth, graphHeight)\n'
'gray_data = data[:,:,0]\n\nsmall_data = gray_data[:28,:28]\n\nfinal_data = small_data.reshape(1,28,28)\n'
"#Remove duplicates from Beers dataset\n        df_beers_noduplicates = df_beers.drop_duplicates(subset='name', keep='first', inplace=False)\n        df_beers_merged = pd.merge(df_tastingprofiles, df_beers_noduplicates, on='beerID')\n        df_beers = df_beers_merged.drop(['malty', 'sweet', 'sour', 'hoppy', 'bitter', 'fruity'], axis=1)\n        df_tastingprofiles = df_beers_merged.drop(['name'], axis=1)\n"
"train_features = train[['F1','F2','F3','F4','F5','X','Y','Z','C1','C2']]\ntrain_label = train['LABEL']\n\ntest_features = test[['F1','F2','F3','F4','F5','X','Y','Z','C1','C2']]\ntest_label = test['LABEL']\n\nmodel.fit(train_features.values, train_label.values)\n"
"len(image_datasets['train'].classes)\n"
'for i in centroids.keys():\n  plt.scatter(*centroids[i], color=colmap[i], s=4) # change the s= parameter\n'
"nb_mul.predict(tfidf.transform(test['Tweets']))\n"
'X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size = 0.2, random_state = 0) \n\n\nx_test, x_val, y_test, y_val = train_test_split(X_train, y_train, test_size=0.25)\n'
"x\narray([[165349, 136898, 471784, 'New York'],\n       [162598, 151378, 443899, 'California'],\n       [153442, 101146, 407935, 'Florida'],\n       [144372, 118672, 383200, 'New York']], dtype=object)\n"
"np.mean(all_importances, axis=0, dtype=np.float64) / np.sum(all_importances)\n\nest = [estimator for estimator in model.estimators_]\n\nest[0]\nOut[57]: \nExtraTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n          max_features='auto', max_leaf_nodes=None,\n          min_impurity_decrease=0.0, min_impurity_split=None,\n          min_samples_leaf=1, min_samples_split=2,\n          min_weight_fraction_leaf=0.0, random_state=1045388471,\n          splitter='random')\n\nlen(est)\nOut[58]: 10\n\n[x.feature_importances_ for x in est]\nOut[59]: \n[array([0., 0., 1.]),\n array([0., 0., 1.]),\n array([0., 0., 1.]),\n array([0.33333333, 0.        , 0.66666667]),\n array([0.11111111, 0.88888889, 0.        ]),\n array([0., 1., 0.]),\n array([0., 0., 1.]),\n array([0., 1., 0.]),\n array([0., 0., 1.]),\n array([0.33333333, 0.66666667, 0.        ])]\n\nfeature_importances_[node.feature] += node.weighted_n_node_samples * node.impurity -\n                                      left.weighted_n_node_samples * left.impurity -\n                                      right.weighted_n_node_samples * right.impurity\n\nest[0].tree_.feature\nOut[60]: array([ 2,  2, -2, -2, -2], dtype=int64)\n\nest[0].tree_.weighted_n_node_samples\nOut[61]: array([4., 2., 1., 1., 2.])\n\nest[0].tree_.impurity\nOut[62]: array([0.375, 0.5  , 0.   , 0.   , 0.   ])\n\nest[3].tree_.compute_feature_importances(normalize=False)\nOut[63]: array([0.125, 0.   , 0.25 ])\n\nest[3].tree_.compute_feature_importances()\nOut[64]: array([0.33333333, 0.        , 0.66666667])\n"
'test_gen = generator(float_data, lookback=lookback, delay=delay, \n                     min_index=300001, max_index=None, step=step, \n                     batch_size=1)    # "reset" the generator\n\npred = model.predict_generator(test_gen, steps=test_steps)\n\ntest_gen = generator(float_data, lookback=lookback, delay=delay, \n                     min_index=300001, max_index=None, step=step, \n                     batch_size=1)    # "reset" the generator\n\ntruth = []\npred = []\n\nfor i in range(test_steps):\n    x, y = next(test_gen)\n    pred.append(model.pred(x))\n    truth.append(y) \n\npred = np.concatenate(pred)\ntruth = np.concatenate(truth)\n'
'import cv2\nimport tkinter as tk\nfrom PIL import Image, ImageTk\nimport tensorflow as tf\n\n# initializing window and image properties\nHEIGHT = 200\nWIDTH = 200\nIMAGE_HEIGHT = 200\nIMAGE_WIDTH = 200\n\n# loading mnist dataset\nmnist = tf.keras.datasets.mnist\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\n\ndef imageShow(index):\n    root = tk.Tk()\n    # resizing image into larger image\n    img_array = cv2.resize(x_train[index], (IMAGE_HEIGHT,IMAGE_WIDTH), interpolation = cv2.INTER_AREA)\n    img =  ImageTk.PhotoImage(image=Image.fromarray(img_array))\n    canvas = tk.Canvas(root,width=WIDTH,height=HEIGHT)\n    canvas.pack()\n    canvas.create_image(IMAGE_HEIGHT/2,IMAGE_WIDTH/2, image=img)\n    root.mainloop()\n\nimageShow(5)\n'
'for i in range(2):\n    if i==1:\n        aux_df = df\n    else:\n        aux_df = df2\n    .\n    .\n    .\n'
'import numpy as np\n\nfirst_row = np.array([-15, 128, 985, 18, -15])\n\nlm.predict(first_row)\n\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n\nlm.predict(first_row.reshape(1, -1))\n'
'Y = P(scoring a goal by football player in a match)\nFeature vector = [weight, height] # height and weight are highly correlated\n\nlog(P(goal)/P(1-goal)) =  0.55*weight- 0.12*height + bias\n\n# how would you interpret the negative coefficient of height now?\n'
'for step, (data, label) in enumerate(train_data_loader):\n    neural_net_model(data)\n\n# Forward pass\noutputs = model(images)\n'
'sklearn_model = RandomForestClassifier()\nsklearn_model.fit(x,y)\nmodel_object = jsonpickle.pickler.Pickler.flatten(sklearn_model)\nmodel = jsonpickle.unpickler.Unpickler.restore(model_object)\nmodel.predict(new_x)\n'
"# Instead of this:\nPath + os.path.sep + File \n\n# Do this:\nos.path.join(Path, File)\n\n# This might not work depending on the filename:\nFile.split('.')[0] # what if the file is named 2019.08.19_file.txt?\n\n# Try:\nos.path.splitext(File)[0]\n"
'sc_X.fit_transform(X.reshape[-1,1])\n'
'D = [\n [10, 1],\n [11, 1],\n [13, 1],\n [14, 1],\n [16, 0],\n [15, 0],\n [14, 0],\n [16, 0],\n]\n\nD = [\n [10, 0.67, 25, ..., 1],\n [16, 0.15, 20.5, ..., 0],\n [...]\n]\n'
'shuffle_index = np.arange(60000) #Rather "not_shuffled_index"\n\nnp.random.seed(1) #Or any number\nshuffle_index = np.random.permutation(60000) #Will be the same for a given seed\n'
'train \n   - class1_folder[class11.jpg, class12.jpg, ...]\n   - class2_folder[class21.jpg, class22.jpg, ...]\n   - and so on ...\n\nimport cv2\nimport numpy as np\nimport os\nimport glob\nimport mahotas as mt\nfrom sklearn.svm import LinearSVC\n\n# function to extract haralick textures from an image\ndef extract_features(image):\n    # calculate haralick texture features for 4 types of adjacency\n    textures = mt.features.haralick(image)\n\n    # take the mean of it and return it\n    ht_mean = textures.mean(axis = 0).reshape(1, -1)\n    return ht_mean\n\n# load the training dataset\ntrain_path  = "C:\\\\dataset\\\\train"\ntrain_names = os.listdir(train_path)\n\n# empty list to hold feature vectors and train labels\ntrain_features = []\ntrain_labels   = []\n\n# loop over the training dataset\nprint ("[STATUS] Started extracting haralick textures..")\nfor train_name in train_names:\n    cur_path = train_path + "\\\\" + train_name\n    print(cur_path)\n    cur_label = train_name\n    i = 1\n\n    for file in glob.glob(cur_path + "\\*.jpg"):\n        print ("Processing Image - {} in {}".format(i, cur_label))\n        # read the training image\n        #print(file)\n        image = cv2.imread(file)\n        #print(image)\n\n        # convert the image to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # extract haralick texture from the image\n        features = extract_features(gray)\n        #print(features.reshape(1, -1))\n        # append the feature vector and label\n        train_features.append(features.reshape(1, -1)[0])\n        train_labels.append(cur_label)\n\n        # show loop update\n        i += 1\n\n# have a look at the size of our feature vector and labels\nprint ("Training features: {}".format(np.array(train_features).shape))\nprint ("Training labels: {}".format(np.array(train_labels).shape))\n\n# create the classifier\nprint ("[STATUS] Creating the classifier..")\nclf_svm = LinearSVC(random_state = 9)\n\n# fit the training data and labels\nprint ("[STATUS] Fitting data/label to model..")\nprint(train_features)\nclf_svm.fit(train_features, train_labels)\n\n# loop over the test images\ntest_path = "C:\\\\dataset\\\\test"\nfor file in glob.glob(test_path + "\\*.jpg"): \n    # read the input image\n    image = cv2.imread(file)\n\n    # convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # extract haralick texture from the image\n    features = extract_features(gray)\n\n    # evaluate the model and predict label\n    prediction = clf_svm.predict(features)\n\n    # show the label\n    cv2.putText(image, str(prediction), (20,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 3)\n    print ("Prediction - {}".format(prediction))\n\n    # display the output image\n    cv2.imshow("Test_Image", image)\n    cv2.waitKey(0)\ncv2.destroyAllWindows()\n'
"import numpy as np\nimport pandas as pd\n\ndata = np.array(['a','b','c','d'])\ns = pd.Series(data)  # create dummy series\n\n1    b\n2    c\n3    d\n"
'size,time,class_label\n100,150,label1\n200,250,label2\n240,180,label1\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# loading data\ndata = pd.read_csv("sample_data.csv", error_bad_lines=True,\n    warn_bad_lines=True)\n\n# Dividing into dependent and independent features\nY = data.class_label_col.values\nX = data.drop("class_label_col", axis=1).values\n\n# encode the class column values\nlabel_encoded_Y = preprocessing.LabelEncoder().fit_transform(list(Y))\n\n# split training and testing data\nx_train,x_test,y_train,y_test=train_test_split(X,label_encoded_Y,\ntrain_size=0.8,\ntest_size=0.2)\n\n# Now use the whichever trainig algo you want\nclf = SVC(gamma=\'auto\')\nclf.fit(x_train, y_train) \n\n# Using the predictor\ny_pred = clf.predict(x_test)\n'
'import joblib\n\n# example for saving python object as pkl\njoblib.dump(vectorizer, "vectorizer.pkl")\n\n# loading pickled vectorizer\nvectorizer = joblib.load("vectorizer.pkl")\n'
"seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   input_shape=(None, 150, 150, 2),\n                   padding='same', return_sequences=True))\n\nimport keras\nfrom keras.models import Model\nfrom keras.layers import Dense, ConvLSTM2D, Input, Concatenate, BatchNormalization\n\ninput1 = Input(shape=[150, 150, 1])\nnn1 = ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   input_shape=(None, 150, 150, 1),\n                   padding='same', return_sequences=True)(input1)\nnn1 = BatchNormalization()(nn1)\n... # and so on\n\n# compute 2nd stream\ninput2 = Input(shape=[150, 150, 1])\nnn2 = ConvLSTM2D(filters=40, kernel_size=(3, 3),\n                   input_shape=(None, 150, 150, 1),\n                   padding='same', return_sequences=True)(input2)\nnn2 = BatchNormalization()(nn2)\n... # and so on\n\n# finally, join the nn1 and nn2 streams into nn\nnn = Concatenate()([nn1, nn2])\n# and work with this new nn layer\nnn = Dense(64, activation='relu')(nn)\n\n# finally, compile the model...\nmodel = Model([input1, input2], nn)\n\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n\n# and fit it, note how it now receives two inputs\nmodel.fit([pix_train_1, \n           pix_train_2], \n           y, ...\n         )\n"
'model.fit(X_Train,X_Test)\n\nmodel.fit(X_train,Y_train)\n'
"dense3=Dense(1, activation='softmax')(do2)\n\ndense3=Dense(2, activation='softmax')(do2)\n"
"shared_layer = layers.Dense(784, activation='relu')\n\nvisible = Input(shape=(28, 28, 1))\nflat = layers.Flatten()(visible)\nhidden = shared_layer(flat)\nhidden2 = shared_layer(hidden)\noutput = layers.Dense(10, activation='softmax')(hidden2)\n\nnew_model = Model(inputs=visible, outputs=output)\n"
"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, \n                               bootstrap = True,\n                               max_features = 'sqrt')\n# model.fit(X, y...\n"
"XFIT = np.array([x_train, XX_train])\nYFIT = np.array([y_train, yy_train])\n\nimport numpy as np\n\nx_train = np.random.random((6400, 2))\ny_train = np.random.randint(2, size=(6400,1))\n\nxx_train = np.array([x_train, x_train])\nyy_train = np.array([y_train, y_train])\n\nprint(xx_train.shape)\n(2, 6400, 2)\n\nprint(yy_train.shape)\n(2, 6400, 1)\n\nxx_train = np.vstack([x_train, x_train])\nyy_train = np.vstack([y_train, y_train])\n\nprint(xx_train.shape)\n(12800, 2)\n\nprint(yy_train.shape)\n(12800, 1)\n\nInputs = Input(shape=(2, ))\nhidden1 = Dense(units=100, activation=&quot;sigmoid&quot;)(Inputs)\nhidden2 = Dense(units=100, activation='relu')(hidden1)\npredictions = Dense(units=1, activation='sigmoid')(hidden2)\n\nmodel = Model([Inputs], outputs=predictions)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(xx_train, yy_train, batch_size=10, epochs=5)\n\nTrain on 12800 samples\nEpoch 1/5\n12800/12800 [==============================] - 3s 216us/sample - loss: 0.6978 - acc: 0.5047\nEpoch 2/5\n12800/12800 [==============================] - 2s 186us/sample - loss: 0.6952 - acc: 0.5018\nEpoch 3/5\n12800/12800 [==============================] - 3s 196us/sample - loss: 0.6942 - acc: 0.4962\nEpoch 4/5\n12800/12800 [==============================] - 3s 217us/sample - loss: 0.6938 - acc: 0.4898\nEpoch 5/5\n12800/12800 [==============================] - 3s 217us/sample - loss: 0.6933 - acc: 0.5002\n"
"# 'string_labels' are the labels in string format \n# provied in a list-like structure (eg. pd.Series)\nnumerical_labels = pd.Categorical(string_labels).codes\n\n# the above 'numerical_labels' is an array of dtype 'int8'\n# to convert it into pd.Series of dtype 'int32' or 'int64'\nnumerical_labels = pd.Series(numerical_labels, dtype=np.int64)\n\n\n"
"n_boxes = len(d['text']) # make the boxes around till the lenght text, put n_boxes in for loop\nfor i in range(n_boxes):\n    if int(d['conf'][i]) &gt; 10:\n        if re.match(date_pattern, d['text'][i]):\n            (x, y, w, h) = (d['left'][i], d['top'][i], d['width'][i], d['height'][i])\n\n            # crop ROI and dump to a file\n            cropped = gray[y : y+h, x : x+w]\n            imwrite('crop_' + str(i) + '.png', cropped)                \n\n            detect_img = cv2.rectangle(result, (x, y), (x + w, y + h), (0, 300, 0), 2) \n"
"val = 'col2'\np = df.columns.get_loc(val)\n\n#possible solution for dummies, be free use your solution\n#df2 = pd.get_dummies(df[val])\ndf = pd.concat([df.iloc[:, :p], df2, df.iloc[:, p:]], axis=1).drop(val, axis=1)\nprint (df)\n\n   col1  A  B  C  col3\n0     4  1  0  0  0.50\n1     5  0  1  0  0.78\n2     6  0  0  1  0.55\n3     7  1  0  0  0.78\n\nval = 'col2'\np = df.columns.get_loc(val)\n#possible solution for dummies, be free use your solution\n#df2 = pd.get_dummies(df[[val]])\ndf = pd.concat([df.iloc[:, :p], df2, df.iloc[:, p:]], axis=1).drop(val, axis=1)\nprint (df)\n\n   col1  col2_A  col2_B  col2_C  col3\n0     4       1       0       0  0.50\n1     5       0       1       0  0.78\n2     6       0       0       1  0.55\n3     7       1       0       0  0.78\n\nval = 'col2'\np = df.columns.get_loc(val)\n#possible solution for dummies, be free use your solution\n#df2 = pd.get_dummies(df.pop(val))\ndf = pd.concat([df.iloc[:, :p], df2, df.iloc[:, p:]], axis=1)\nprint (df)\n\n   col1  A  B  C  col3\n0     4  1  0  0  0.50\n1     5  0  1  0  0.78\n2     6  0  0  1  0.55\n3     7  1  0  0  0.78\n"
'def compute_abs_difference_matrix(Y):\n    abs_difference_matrix = np.array([])\n    n_samples = Y.shape[0]\n\n    # compute the absolute difference matrix\n\n    # and remember to return the matrix\n    # INSERT YOUR CODE HERE\n\n    for i in range(n_samples):\n        for j in range(n_samples):\n            abs_difference_matrix[i, j] = abs(Y[i] - Y[j])\n\n    return abs_difference_matrix\n'
"import pickle    \npkl = 'rf.pkl'\nwith open(pkl,'wb') as file:\n    pickle.dump(rf,file)\n\nwith open(pkl,'rb') as file:\n    pkl_model = pickle.load(file)\n"
"# you can declare number of splits here\nkfold = model_selection.KFold(n_splits=5, random_state=42)\n# your model goes here. \nmodel = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n# this will fit your model 5 times and use 1/5 as test data and 4/5 as training data\nresults = model_selection.cross_val_score(model, X_train,  y_train, cv=kfold)\n"
"import android.content.pm.PackageManager;\nimport android.media.Image;\nimport android.os.Bundle;\nimport android.util.Log;\nimport android.widget.Toast;\n\nimport androidx.annotation.NonNull;\nimport androidx.annotation.Nullable;\nimport androidx.appcompat.app.AppCompatActivity;\nimport androidx.camera.core.Camera;\nimport androidx.camera.core.CameraSelector;\nimport androidx.camera.core.ExperimentalGetImage;\nimport androidx.camera.core.ImageAnalysis;\nimport androidx.camera.core.ImageProxy;\nimport androidx.camera.core.Preview;\nimport androidx.camera.lifecycle.ProcessCameraProvider;\nimport androidx.camera.view.PreviewView;\nimport androidx.core.app.ActivityCompat;\nimport androidx.core.content.ContextCompat;\n\nimport com.google.common.util.concurrent.ListenableFuture;\nimport com.google.mlkit.common.model.LocalModel;\nimport com.google.mlkit.vision.common.InputImage;\nimport com.google.mlkit.vision.objects.DetectedObject;\nimport com.google.mlkit.vision.objects.ObjectDetection;\nimport com.google.mlkit.vision.objects.ObjectDetector;\nimport com.google.mlkit.vision.objects.custom.CustomObjectDetectorOptions;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.util.ArrayList;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.NoSuchElementException;\nimport java.util.concurrent.ExecutionException;\n\npublic class ActivityExample extends AppCompatActivity {\n    private ListenableFuture&lt;ProcessCameraProvider&gt; cameraProviderFuture;\n    private ObjectDetector objectDetector;\n    private PreviewView prevView;\n    private List&lt;String&gt; labels;\n\n    private int REQUEST_CODE_PERMISSIONS = 101;\n    private String[] REQUIRED_PERMISSIONS =\n            new String[]{&quot;android.permission.CAMERA&quot;};\n\n    @Override\n    protected void onCreate(@Nullable Bundle savedInstanceState) {\n        super.onCreate(savedInstanceState);\n\n        setContentView(R.layout.activity_fullscreen);\n        prevView = findViewById(R.id.viewFinder);\n\n        prepareObjectDetector();\n        prepareLabels();\n\n        if (allPermissionsGranted()) {\n            startCamera();\n        } else {\n            ActivityCompat.requestPermissions(this, REQUIRED_PERMISSIONS, REQUEST_CODE_PERMISSIONS);\n        }\n    }\n\n    private void prepareLabels() {\n        try {\n            InputStreamReader reader = new InputStreamReader(getAssets().open(&quot;labels_mobilenet_quant_v1_224.txt&quot;));\n            labels = readLines(reader);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n\n    private List&lt;String&gt; readLines(InputStreamReader reader) {\n        BufferedReader bufferedReader = new BufferedReader(reader, 8 * 1024);\n        Iterator&lt;String&gt; iterator = new LinesSequence(bufferedReader);\n\n        ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();\n\n        while (iterator.hasNext()) {\n            list.add(iterator.next());\n        }\n\n        return list;\n    }\n\n    private void prepareObjectDetector() {\n        CustomObjectDetectorOptions options = new CustomObjectDetectorOptions.Builder(loadModel(&quot;mobilenet_v1_1.0_224_quant.tflite&quot;))\n                .setDetectorMode(CustomObjectDetectorOptions.SINGLE_IMAGE_MODE)\n                .enableMultipleObjects()\n                .enableClassification()\n                .setClassificationConfidenceThreshold(0.5f)\n                .setMaxPerObjectLabelCount(3)\n                .build();\n        objectDetector = ObjectDetection.getClient(options);\n    }\n\n    private LocalModel loadModel(String assetFileName) {\n        return new LocalModel.Builder()\n                .setAssetFilePath(assetFileName)\n                .build();\n    }\n\n    private void startCamera() {\n        cameraProviderFuture = ProcessCameraProvider.getInstance(this);\n        cameraProviderFuture.addListener(() -&gt; {\n            try {\n                ProcessCameraProvider cameraProvider = cameraProviderFuture.get();\n                bindPreview(cameraProvider);\n            } catch (ExecutionException e) {\n                // No errors need to be handled for this Future.\n                // This should never be reached.\n            } catch (InterruptedException e) {\n            }\n        }, ContextCompat.getMainExecutor(this));\n    }\n\n    private void bindPreview(ProcessCameraProvider cameraProvider) {\n        Preview preview = new Preview.Builder().build();\n        CameraSelector cameraSelector = new CameraSelector.Builder()\n                .requireLensFacing(CameraSelector.LENS_FACING_BACK)\n                .build();\n        ImageAnalysis imageAnalysis = new ImageAnalysis.Builder()\n                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n                .build();\n        YourAnalyzer yourAnalyzer = new YourAnalyzer();\n        yourAnalyzer.setObjectDetector(objectDetector, labels);\n        imageAnalysis.setAnalyzer(\n                ContextCompat.getMainExecutor(this),\n                yourAnalyzer);\n\n        Camera camera =\n                cameraProvider.bindToLifecycle(\n                        this,\n                        cameraSelector,\n                        preview,\n                        imageAnalysis\n                );\n\n        preview.setSurfaceProvider(prevView.createSurfaceProvider(camera.getCameraInfo()));\n    }\n\n    private Boolean allPermissionsGranted() {\n        for (String permission : REQUIRED_PERMISSIONS) {\n            if (ContextCompat.checkSelfPermission(\n                    this,\n                    permission\n            ) != PackageManager.PERMISSION_GRANTED\n            ) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    @Override\n    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {\n        if (requestCode == REQUEST_CODE_PERMISSIONS) {\n            if (allPermissionsGranted()) {\n                startCamera();\n            } else {\n                Toast.makeText(this, &quot;Permissions not granted by the user.&quot;, Toast.LENGTH_SHORT)\n                        .show();\n                finish();\n            }\n        }\n    }\n\n    private static class YourAnalyzer implements ImageAnalysis.Analyzer {\n        private ObjectDetector objectDetector;\n        private List&lt;String&gt; labels;\n\n        public void setObjectDetector(ObjectDetector objectDetector, List&lt;String&gt; labels) {\n            this.objectDetector = objectDetector;\n            this.labels = labels;\n        }\n\n        @Override\n        @ExperimentalGetImage\n        public void analyze(@NonNull ImageProxy imageProxy) {\n            Image mediaImage = imageProxy.getImage();\n            if (mediaImage != null) {\n                InputImage image = InputImage.fromMediaImage(\n                        mediaImage,\n                        imageProxy.getImageInfo().getRotationDegrees()\n                );\n                objectDetector\n                        .process(image)\n                        .addOnFailureListener(e -&gt; imageProxy.close())\n                        .addOnSuccessListener(detectedObjects -&gt; {\n                            // list of detectedObjects has all the information you need\n                            StringBuilder builder = new StringBuilder();\n                            for (DetectedObject detectedObject : detectedObjects) {\n                                for (DetectedObject.Label label : detectedObject.getLabels()) {\n                                    builder.append(labels.get(label.getIndex()));\n                                    builder.append(&quot;\\n&quot;);\n                                }\n                            }\n                            Log.d(&quot;OBJECTS DETECTED&quot;, builder.toString().trim());\n                            imageProxy.close();\n                        });\n            }\n        }\n    }\n\n\n    static class LinesSequence implements Iterator&lt;String&gt; {\n        private BufferedReader reader;\n        private String nextValue;\n        private Boolean done = false;\n\n        public LinesSequence(BufferedReader reader) {\n            this.reader = reader;\n        }\n\n        @Override\n        public boolean hasNext() {\n            if (nextValue == null &amp;&amp; !done) {\n                try {\n                    nextValue = reader.readLine();\n                } catch (IOException e) {\n                    e.printStackTrace();\n                    nextValue = null;\n                }\n                if (nextValue == null) done = true;\n            }\n            return nextValue != null;\n        }\n\n        @Override\n        public String next() {\n            if (!hasNext()) {\n                throw new NoSuchElementException();\n            }\n            String answer = nextValue;\n            nextValue = null;\n            return answer;\n        }\n    }\n}\n\n&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;\n&lt;androidx.camera.view.PreviewView\n    xmlns:android=&quot;http://schemas.android.com/apk/res/android&quot;\n    android:id=&quot;@+id/viewFinder&quot;\n    android:layout_width=&quot;match_parent&quot;\n    android:layout_height=&quot;match_parent&quot; /&gt;\n\nandroid {\n    ...\n    aaptOptions {\n        noCompress &quot;tflite&quot;  // Your model\\'s file extension: &quot;tflite&quot;, &quot;lite&quot;, etc.\n    }\n    compileOptions {\n        sourceCompatibility JavaVersion.VERSION_1_8\n        targetCompatibility JavaVersion.VERSION_1_8\n    }\n}\n\n\ndependencies {\n    ...\n    \n    implementation 'com.google.mlkit:object-detection-custom:16.0.0'\n    def camerax_version = &quot;1.0.0-beta03&quot;\n    // CameraX core library using camera2 implementation\n    implementation &quot;androidx.camera:camera-camera2:$camerax_version&quot;\n    // CameraX Lifecycle Library\n    implementation &quot;androidx.camera:camera-lifecycle:$camerax_version&quot;\n    // CameraX View class\n    implementation &quot;androidx.camera:camera-view:1.0.0-alpha10&quot;\n}\n"
'vect.fit(X)\n'
"for ax,col in zip(axes.flat, chef_num.columns[1:]):\n    sns.scatterplot(x=chef_num[col], y=chef_num['REVENUE'], ax=ax)\n"
"# load 'pandas' library\nimport pandas as pd\n\n# One-hot encode categorical variable\none_hot_column_name = pd.get_dummies(dataset_name['column_to_encode']\n\n# Drop original categorical variable after it has been encoded\ndataset_name = dataset_name.drop('categorical_column', axis = 1)\n\n# join codings together\ndataset_name = dataset_name.join([one_hot_column_name])\n"
"from PIL import Image\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train = X_train/255\nX_test = X_test/255\nX_train_processed = np.reshape(X_train,[X_train.shape[0],X_train.shape[1],X_train.shape[2],1])\nX_test_processed = np.reshape(X_test,[X_test.shape[0],X_test.shape[1],X_test.shape[2],1])\nX_train_processed = np.pad(X_train_processed, ((0,0),(2,2),(2,2),(0,0)), 'constant')\nX_test_processed = np.pad(X_test_processed, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n\nX_train_processed = tf.image.resize(\n    images = X_train_processed,\n    size = np.array([32,32])\n\n)\nX_test_processed = tf.image.resize(\n    images = X_test_processed,\n    size = np.array([32,32])\n\n)\nY_train_processed = tf.one_hot(y_train,10)\nY_test_processed = tf.one_hot(y_test,10)\n\nLnet = tf.keras.Sequential()\n#First Layer\nLnet.add(\n    tf.keras.layers.Conv2D(\n        filters = 6, \n        kernel_size = (5,5), \n        strides = (1,1), \n        padding = 'valid', \n        activation = 'relu', \n        #kernel_initializer = keras.initializers.glorot_normal(seed=0)\n    )\n)\nLnet.add(\n    tf.keras.layers.AveragePooling2D(\n        pool_size = (2,2),\n        strides = (2,2),\n        padding = 'valid'\n    )\n)\n#Second Layer\nLnet.add(\n    tf.keras.layers.Conv2D(\n        filters = 16, \n        kernel_size = (5,5), \n        strides = (1,1), \n        padding = 'valid', \n        activation = 'relu'#, \n        #kernel_initializer = keras.initializers.glorot_normal(seed=0)\n    )\n)\nLnet.add(\n    tf.keras.layers.AveragePooling2D(\n        pool_size = (2,2),\n        strides = (2,2),\n        padding = 'valid'\n    )\n)\nLnet.add(tf.keras.layers.Flatten())\nLnet.add(\n    tf.keras.layers.Dense(\n        units = 120,\n        activation = 'relu'\n    )\n)\nLnet.add(tf.keras.layers.Flatten())\nLnet.add(\n    tf.keras.layers.Dense(\n        units = 84,\n        activation = 'relu'\n    )\n)\nLnet.add(\n    tf.keras.layers.Dense(\n        units = 10,\n        activation = 'softmax'\n    )\n)\n\nLnet.compile(\n    loss = tf.keras.losses.categorical_crossentropy,\n    optimizer = 'Adam',\n    metrics = ['accuracy']\n)\n\nLnet.fit(\n    x = X_train_processed,\n    y = Y_train_processed,\n    batch_size = 128,\n    epochs = 10,\n)\n\nscore = Lnet.evaluate(\n    x = X_test_processed,\n    y = Y_test_processed\n)\nprint(score[1])\n\nEpoch 9/10\n469/469 [==============================] - 30s 64ms/step - loss: 0.0260 - accuracy: 0.9916\nEpoch 10/10\n469/469 [==============================] - 30s 64ms/step - loss: 0.0239 - accuracy: 0.9922\n313/313 [==============================] - 3s 10ms/step - loss: 0.0394 - accuracy: 0.9876\n0.9876000285148621\n"
'rf_pipe = Pipeline(stages=[va, rf])\n\n....\n\n# Train the model and calculate the AUC using a BinaryClassificationEvaluator\nrf_pipe = Pipeline(stages=[va, rf])\nrf_pipeline = rf_pipe.fit(training_df)\n\n...\n\ncrossValidator = CrossValidator(estimator=**rf_pipe**, \n                          estimatorParamMaps=paramGrid, \n                          evaluator=bce, \n                          numFolds=3)\n\nmodel = crossValidator.fit(training_df)\n'
'df = pd.DataFrame({\'People\':[10,12,11,13,15,18]})\n\nlog_df_People = np.log(df.People)\n\nimport tensorflow as tf\nX = log_df_People.to_numpy()[:-1]\nY = log_df_People.shift(-1).to_numpy()[:-1]\n\nmodel = tf.keras.Sequential()\nmodel.add = tf.keras.layers.LSTM(100, activation="relu", input_shape=(2,))\nmodel.add = tf.keras.layers.Dropout(rate=0.2)\nmodel.add = tf.keras.layers.Dense(1, activation=\'relu\')\nmodel.compile(optimizer=\'adam\', loss=\'mean_absolute_error\',\n              metrics=[\'accuracy\'])\n\nmodel.fit(X,Y,epochs=100)\n'
'import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler as SC\n\n@tf.function\ndef trainingData(X0, X1, y, batchSize=1):\n    dataset = tf.data.Dataset.from_tensor_slices(({\'data0\' : X0, \'data1\':X1}, y))\n    return dataset.repeat()\n\ndef main():\n    X, y = np.asarray([[1.0,2.5] for i in range(200)]), np.asarray([1 for i in range(200)])\n    y = np.expand_dims(np.asarray(np.where(y == -1, np.zeros(shape=y.shape), y)),1)  \n    sc = SC(with_mean=True, with_std=True)\n    X = sc.fit_transform(X)\n    BATCH_SIZE = 1\n    EPOCHS = 10\n    N_SAMPLES = 400\n\n    inputFeatureColumns = [tf.feature_column.numeric_column(key=\'data0\', shape=(1,)), tf.feature_column.numeric_column(key=\'data1\', shape=(1,))]\n\n    estimator = tf.estimator.DNNClassifier(hidden_units=[32, 16], feature_columns=inputFeatureColumns, n_classes=2, \n                                            activation_fn=tf.nn.sigmoid, optimizer=\'SGD\')\n    estimator.train(input_fn=lambda: trainingData(np.expand_dims(X[:,0],1),np.expand_dims(X[:,1],1), y, BATCH_SIZE), steps=EPOCHS * N_SAMPLES / BATCH_SIZE)    \n\nif __name__ == "__main__":\n    main()\n'
"# define\nlog_reg_best = GridSearchCV(LogisticRegression(solver = 'liblinear'), parameters_log_reg, return_train_score = True)\n# fit\nlog_reg_best.fit(part_train_set_features, part_train_set_survived)\n# get best estimator\nfinal_log_reg = log_reg_best.best_estimator_\n"
"stop_set = set(stopwords.words())\nlemmatizer = WordNetLemmatizer()\n\ndef tokenize(text):\n    text = re.sub('[^a-zA-Z0-9]', ' ', text)\n    words = word_tokenize(text)\n    words = [lemmatizer.lemmatize(w.lower().strip()) for w in words if w not in stop_set]\n    return words \n\nfrom functools import lru_cache\n\nstop_set = set(stopwords.words())\nlemmatizer = WordNetLemmatizer()\n\n@lru_cache(maxsize=10000)\ndef lemmatize(word):\n    return lemmatizer.lemmatize(w.lower().strip())\n\ndef tokenize(text):\n    text = re.sub('[^a-zA-Z0-9]+', ' ', text)\n    words = [lemmatize(w) for w in word_tokenize(text)]\n    return [w for w in words if w not in stop_set]\n"
'X=BTC_cleanData[-1:] # this has one more column compared to X_train and X_test\nprint(regressor.predict(X))\n\nimport pandas as pd\nimport numpy as np\nimport talib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport investpy\nfrom investpy.crypto import get_crypto_historical_data\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n#Import open, high, low, close, volume and Return data from csv using investpy\nBTC = get_crypto_historical_data(crypto=\'bitcoin\', from_date=\'01/01/2014\', to_date=\'06/08/2020\')\n\n#Convert Data from Int to Float\nBTC.Volume = BTC.Volume.astype(float)\nBTC.High = BTC.High.astype(float)\nBTC.Low = BTC.Low.astype(float)\nBTC.Close = BTC.Close.astype(float)\n\n#Drop Unnecessary Columns\ndel BTC[\'Currency\']\n\n#Select Indicators as Features\nBTC[\'AD\'] = talib.AD(BTC[\'High\'].values, BTC[\'Low\'].values, BTC[\'Close\'].values, BTC[\'Volume\'].values)\n\n\n#Create forward looking columns using shift\nBTC[\'NextDayPrice\'] = BTC[\'Close\'].shift(-1)\n\n#Copy dataframe and clean data \nBTC_cleanData = BTC.copy()\nBTC_cleanData.dropna(inplace=True)\n#BTC_cleanData.to_csv(\'C:/Users/Admin/Desktop/BTCdata.csv\')\n\n#Split Data into Training and Testing Set\n#separate the features and targets into separate datasets.\n#split the data into training and testing sets using a 70/30 split \n#Using splicing, separate the features from the target into individual data sets.  \nX_all = BTC_cleanData.iloc[:, BTC_cleanData.columns != \'NextDayPrice\']  # feature values for all days\ny_all = BTC_cleanData[\'NextDayPrice\']  # corresponding targets/labels\nprint (X_all.head())  # print the first 5 rows\n\n#Split the data into training and testing sets using the given feature as the target\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.30, random_state=42)\n\n#Create a decision tree regressor and fit it to the training set\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\n\nprint ("Training set: {} samples".format(X_train.shape[0]))\nprint ("Test set: {} samples".format(X_test.shape[0]))\n\n#Evaluate Model (out of sample Accuracy and Mean Squared Error)\nscores = cross_val_score(regressor, X_test, y_test, cv=10)\nprint ("accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() / 2))    \n\nmse = mean_squared_error(y_test, regressor.predict(X_test))\nprint("MSE: %.4f" % mse)\n\n#Evaluate Model (In sample Accuracy and Mean Squared Error)\ntrainscores = cross_val_score(regressor, X_train, y_train, cv=10)\nprint ("accuracy: %0.2f (+/- %0.2f)" % (trainscores.mean(), trainscores.std() / 2))    \n\nmse = mean_squared_error(y_train, regressor.predict(X_train))\nprint("MSE: %.4f" % mse)\nprint(regressor.predict(X_train))\n\n#Predict Next Day Price\nprint(regressor.predict(X_test))\n'
"plt.plot(x, lin_reg2.predict(poly_reg.fit_transform(x)), color = 'blue')\n"
"y_pred2 = model.predict(X)\ndata['prediction'] = y_pred2\n\ny_new = model.predict(X_new)\ndata_new['prediction'] = y_new\n"
"class_weight={\n    'A': 0.5,\n    'B': 1.0,\n    'C': 1.0\n}\n"
"new_generator = new_datagen.flow_from_directory(\n    new_DIR,\n    target_size=(150,150),\n    class_mode='categorical',\n  batch_size=126,\n  shuffle=False\n)\n\nprint(&quot;model evaluate output &quot;, model.evaluate(new_generator))\n\nprint (np.argmax(model.predict(new_generator), axis=1))\n\nprint (model.predict_classes(new_generator))\n\ntest_datagen = ImageDataGenerator(rescale = 1./255)\n       \ntest_generator = test_datagen.flow_from_directory(\n    &quot;/tmp/rps-validation-set/&quot;,\n    target_size=(150,150),\n    class_mode=None,\n  batch_size=126,\n  shuffle=False\n)\n\nmodel.predict(test_generator)\n"
'xTrain = xTrain.reshape(2519025, 6)\n'
"&gt;&gt;&gt; x = [1, 2, 3]\n&gt;&gt;&gt; 2 * x\n[1, 2, 3, 1, 2, 3]\n&gt;&gt;&gt; 2.0 * x\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\nTypeError: can't multiply sequence by non-int of type 'float'\n\n&gt;&gt;&gt; [2.0 * xx for xx in x]\n[2.0, 4.0, 6.0]\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; 2.0 * np.array(x)\narray([2., 4., 6.])\n"
"random.shuffle(target)  # Requires `import shuffle`\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\npca = PCA(n_components=2)\npca_res = pca.fit_transform(data)\nplt.scatter(pca_res[:, 0], pca_res[:, 1], color=list(map(' rgbyc'.__getitem__, target)))\n\ndataset = np.asarray(data)[:, 1:2]\n\nimport seaborn as sns\nsns.boxplot(x=target, y=dataset[:, 1])  # Original dataset used here\n"
'# If you wanna check how it looks\nprint(df)\n\n   f0  f1  f2\n0  55  82  83\n1  54  35  46\n2  27  48  39\n'
'pred = model.predict(some_data)\n\nfrom sklearn.metrics import r2_score\n\nr2_score(correct_targets, pred)\n'
"def save_checkpoint(model, optimizer, save_path, epoch):\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch\n    }, save_path)\n\ndef load_checkpoint(model, optimizer, load_path):\n    checkpoint = torch.load(load_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    \n    return model, optimizer, epoch\n"
'x_min, y_min, x_max, y_max = annotation[&quot;bbox&quot;]\n\nx,y,w,h = label\nplt.imshow(frame[y:h, x:w])\n'
"generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    validation_split=0.2\n)\n\ntrain_ds = generator.flow_from_directory(\n  data_dir,\n  subset=&quot;training&quot;,\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,\n  class_mode='input'\n)\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, UpSampling2D, MaxPool2D\nfrom tensorflow.keras.layers import Activation, Dense, Input, BatchNormalization\nfrom tensorflow.keras import Model, Sequential\n\ndef create_block(input, chs):\n    x = input\n    for i in range(2):\n        x = Conv2D(chs, 3, padding=&quot;same&quot;)(x)\n        x = Activation(&quot;relu&quot;)(x)\n        x = BatchNormalization()(x)\n    return x\ninput_img = Input(shape=(328, 328, 3))\n\nblock1 = create_block(input_img, 32)\nx = MaxPool2D(2)(block1)\nblock2 = create_block(x, 64)\n\n#Middle\nx = MaxPool2D(2)(block2)\nmiddle = create_block(x, 128)\n\n# Decoder\nblock3 = create_block(middle, 64)\nup1 = UpSampling2D((2,2))(block3)\nblock4 = create_block(up1, 32)\nup2 = UpSampling2D((2,2))(block4)\n\n# output\nx = Conv2D(3, 1)(up2)\noutput = Activation(&quot;sigmoid&quot;)(x)\n\nX = np.random.rand(8, 328, 328, 3).astype(np.float32)\n\nautoencoder = Model(input_img, output)\nautoencoder.compile('adam', loss='binary_crossentropy')\nautoencoder.summary()\n\ngenerator = tf.keras.preprocessing.image.ImageDataGenerator()\n\ntrain_ds = generator.flow(\n  x=X,\n  y=X\n)\n\nhistory = autoencoder.fit(train_ds)\n\nTrain for 1 steps\n1/1 [==============================] - 9s 9s/step - loss: 0.8827\n"
'opt = keras.optimizers.Adam(0.001, clipnorm=1.)\nmodel.compile(loss=[&quot;mse&quot;, &quot;mse&quot;], loss_weights=[0.9, 0.1], optimizer=opt)\n\nEpoch 1/20\n363/363 [==============================] - 1s 2ms/step - loss: 1547.7197 - main_output_loss: 967.1940 - aux_output_loss: 6772.4609 - val_loss: 19.9807 - val_main_output_loss: 20.0967 - val_aux_output_loss: 18.9365\nEpoch 2/20\n363/363 [==============================] - 1s 2ms/step - loss: 13.2916 - main_output_loss: 14.0150 - aux_output_loss: 6.7812 - val_loss: 14.6868 - val_main_output_loss: 14.5820 - val_aux_output_loss: 15.6298\nEpoch 3/20\n363/363 [==============================] - 1s 2ms/step - loss: 11.0539 - main_output_loss: 11.6683 - aux_output_loss: 5.5244 - val_loss: 10.5564 - val_main_output_loss: 10.2116 - val_aux_output_loss: 13.6594\nEpoch 4/20\n363/363 [==============================] - 1s 1ms/step - loss: 7.4646 - main_output_loss: 7.7688 - aux_output_loss: 4.7269 - val_loss: 13.2672 - val_main_output_loss: 11.5239 - val_aux_output_loss: 28.9570\nEpoch 5/20\n363/363 [==============================] - 1s 2ms/step - loss: 5.6873 - main_output_loss: 5.8091 - aux_output_loss: 4.5909 - val_loss: 5.0464 - val_main_output_loss: 4.5089 - val_aux_output_loss: 9.8839\n\nopt = keras.optimizers.SGD(0.001, clipnorm=1.)\nmodel.compile(loss=[&quot;mse&quot;, &quot;mse&quot;], loss_weights=[0.9, 0.1], optimizer=opt)\n'
'x = model.summary()\n\nx == None\n\nTrue\n'
"model.add(layers.Dense(1, activation='sigmoid'))\n"
"def tweet2vec(tokens, size, tfidf):\n    # ------------- size is the dimension of word2vec model (200) ---------------\n    vec = np.zeros(size).reshape(1, size)\n    count = 0\n    for word in tokens:\n        try:\n            vec += model[word] * tfidf[word]\n            count += 1\n        except KeyError:\n            continue\n    if count != 0:\n        vec /= count\n    return vec\n\nX = np.concatenate([tweet2vec(token, 200, w) for token in tqdm(map(lambda token: token, dataset_token),\n                                                               desc='creating tweet vectors',\n                                                               total=len(dataset_token))]\n"
"model.compile(optimizer=&quot;rmsprop&quot;, loss='mse')\n"
'0.4285714285714286\n'
"df['role'] = df['device'].str[0]\n"
'def process_path(filename):\n  label = 1\n\n  image = tf.io.read_file(filename)\n  print(image.shape)\n  image = tf.image.decode_jpeg(image, channels=3)\n  print(image.shape)\n  image = tf.image.rgb_to_grayscale(image)\n  print(image.shape)\n  image = tf.image.convert_image_dtype(image, tf.float32)\n  print(image.shape)\n  image = tf.image.resize(image, [224, 224])\n  print(image.shape)\n\n  image, label = (lambda x,y : (rotate_image(x), y))(image, label)\n  print(image.shape)\n\n&gt;&gt;&gt; ds = ds.map(process_path)\n()\n(None, None, 3)\n(None, None, 1)\n(None, None, 1)\n(224, 224, 1)\n(224, 224, 1)\n\ndef tf_random_rotate_image(image, label):\n  im_shape = image.shape\n  [image,] = tf.py_function(random_rotate_image, [image], [tf.float32])\n  # the shape is set explicitly because tensorflow can not ensure\n  # that the shape is not modified during the execution of the function\n  image.set_shape(im_shape)\n  return image, label\n\ndef not_shape_preserving(image, label):\n    im_shape = image.shape\n    # this function does not preserve the shape\n    [image,] = tf.py_function(lambda x: 1., [image], [tf.float32])\n    image.set_shape(im_shape)\n    return image, label\n\nIncompatible shapes at component 0: expected [224,224,1] but got [].\n'
"model.compile(loss='categorical_crossentropy',metrics=\n   ['accuracy'],optimizer=tf.keras.optimizers.RMSprop(lr=0.001))\n"
'test_input_ids = np.array(test_input_ids, dtype=np.int32)\ntest_attention_mask = np.array(test_attention_mask, dtype=np.int32)\ntest_token_type_id = np.array(test_token_type_id, dtype=np.int32)\nids = np.expand_dims(test_input_ids, axis=0)\natm = np.expand_dims(test_attention_mask, axis=0)\ntok = np.expand_dims(test_token_type_id, axis=0)\nmodel(ids,atm.tok) works fine\n'
'array([[  0.,  10.,   0.,   0.],\n       [  1.,  11.,  11.,   1.],\n       [  2.,  12.,  24.,   4.],\n       [  3.,  13.,  39.,   9.],\n       [  4.,  14.,  56.,  16.],\n       [  5.,  15.,  75.,  25.],\n       [  6.,  16.,  96.,  36.],\n       [  7.,  17., 119.,  49.],\n       [  8.,  18., 144.,  64.],\n       [  9.,  19., 171.,  81.]])\n'
"dr = 0.5 # dropout rate \nmodel = models.Sequential() \nmodel.add(Reshape(([1]+in_shp), input_shape=in_shp))\n# You could drop the ZerpPadding2D as it adds rows and columns of zeros around image tensor\nmodel.add(ZeroPadding2D((0, 2)))\n# Start with lower filters count then increas as you go\nmodel.add(Conv2D(32, (1, 3),padding='valid', activation=&quot;relu&quot;, name=&quot;conv1&quot;,  kernel_initializer='glorot_uniform',data_format=&quot;channels_first&quot;))\nmodel.add(MaxPooling2D(2))\nmodel.add(Dropout(dr)) \n#model.add(ZeroPadding2D((0, 2))) # no need \n# Increase filters count\nmodel.add(Conv2D(64, (2, 3), padding=&quot;valid&quot;, activation=&quot;relu&quot;, name=&quot;conv2&quot;,  kernel_initializer='glorot_uniform',data_format=&quot;channels_first&quot;))\nmodel.add(MaxPooling2D(2))\nmodel.add(Conv2D(64, (2, 3), padding=&quot;valid&quot;, activation=&quot;relu&quot;, name=&quot;conv2&quot;,  kernel_initializer='glorot_uniform',data_format=&quot;channels_first&quot;))\nmodel.add(MaxPooling2D(2))\nmodel.add(Dropout(dr))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu',  kernel_initializer='he_normal', name=&quot;dense1&quot;)\nmodel.add(Dropout(dr))\n# Add one more Dense \nmodel.add(Dense(256, activation='relu',  kernel_initializer='he_normal', name=&quot;dense1&quot;)\nmodel.add(Dense( len(classes),  kernel_initializer='he_normal', name=&quot;dense2&quot; ))\nmodel.add(Activation('softmax'))\nmodel.add(Reshape([len(classes)]))\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam',metrics=['accuracy'])\nmodel.summary()\n"
'def tt_split(X, y, test_size=0.2):\n\n    i = int((1 - test_size) * X.shape[0]) \n    o = np.random.permutation(X.shape[0])\n    \n    X_train, X_test = np.split(np.take(X,o,axis=0), [i])\n    y_train, y_test = np.split(np.take(y,o), [i])\n    return X_train, X_test, y_train, y_test\n\nX = np.random.normal(0,1,(50,10))\ny = np.random.normal(0,1,(50,))\nX_train, X_test, y_train, y_test = tt_split(X,y)\n[X_train.shape,y_train.shape]\n[(40, 10), (40,)]\n\nX = pd.DataFrame(np.random.normal(0,1,(50,10)))\ny = pd.Series(np.random.normal(0,1,50))\nX_train, X_test, y_train, y_test = tt_split(X,y)\n[X_train.shape,y_train.shape]\n[(40, 10), (40,)]\n'
'    def forward(self, x):\n        x = x.view(1, 500, -1) \n\n        ...\n\nnet = Simple1DCNN4()\n\ninput_try = np.random.uniform(-10, 10, 5000)\n'
"clf=svm.SVC(kernel='linear',max_iter=100)\n"
'import cv2\nimg = cv2.imread("\'d:/Emmanu/project-data/b1.jpg\'")\ncrop_img = img[200:400, 100:300] # Crop from x, y, w, h -&gt; 100, 200, 300, 400\n# NOTE: its img[y: y + h, x: x + w] and *not* img[x: x + w, y: y + h]\ncv2.imshow("cropped", crop_img)\ncv2.waitKey(0)\n'
'# x = input data, size(&lt;points&gt;, &lt;dimensions&gt;)\n\n# fit the full model\nmax_components = x.shape[1] # as many components as input dimensions\npca = PCA(n_components=max_components)\npca.fit(x)\n\n# transform the data (contains all components)\ny_all = pca.transform(x)\n\n# keep only the top k components (with greatest variance)\nk = 2\ny = y_all[:, 0:k]\n\n# Calculate fraction of variance explained\n# for each choice of number of components\nr2 = pca.explained_variance_.cumsum() / x.var(0).sum()\n'
'for category in categories:\n    category_probability[category] = total_words_in_category[category] / sum(total_words_in_category.values())\n\nfor category in categories:\n    category_probability[category] = total_objects_in_category[category] / sum(total_objects_in_category.values())\n\nPROD_i P(x) = exp [ log [ PROD_i P_i(x) ] ] = exp [ SUM_i log P_i(X) ]\n'
"self.mean_ = self.df.groupby(['X']).mean()\n\nself.mean_ = self.df.groupby(['X'].mean())\n\nreturn self.mean_.ix[X].values\n\nreturn self.df['y']['X']\n"
'import numpy as np\nx = np.random.rand(100, 200)\n# Select the first 2 columns\ny = x[:, :2]\n# Get the row length\nprint (y.shape[0])\n# Get the column length\nprint (y.shape[1])\n# Number of dimensions\nprint (len(y.shape))\n'
'import datetime\nimport io\nimport pandas as pd\nimport numpy as np\ndf_string=\'[{"id":100,"vehicle_type":"Car","time":"2017-04-06 01:39:43","zone":"A","type":"Checked"},{"id":101,"vehicle_type":"Truck","time":"2017-04-06 02:35:45","zone":"B","type":"Unchecked"},{"id":102,"vehicle_type":"Truck","time":"2017-04-05 03:20:12","zone":"A","type":"Checked"},{"id":103,"vehicle_type":"Car","time":"2017-04-04 10:05:04","zone":"C","type":"Unchecked"}]\'\ndf = pd.read_json(io.StringIO(df_string))\ndf[\'zone\'] = pd.Categorical(df.zone)\ndf[\'vehicle_type\'] = pd.Categorical(df.vehicle_type)\ndf[\'type\'] = pd.Categorical(df.type)\ndf[\'zone_int\'] = df.zone.cat.codes\ndf[\'vehicle_type_int\'] = df.vehicle_type.cat.codes\ndf[\'type_int\'] = df.type.cat.codes\ndf.head()\n\nimport datetime\nimport io\nimport math\nimport pandas as pd\n#Taken from http://stackoverflow.com/questions/13071384/python-ceil-a-datetime-to-next-quarter-of-an-hour\ndef ceil_dt(dt, num_seconds=900):\n    nsecs = dt.minute*60 + dt.second + dt.microsecond*1e-6  \n    delta = math.ceil(nsecs / num_seconds) * num_seconds - nsecs\n    return dt + datetime.timedelta(seconds=delta)\n\ndf_string=\'[{"id":100,"vehicle_type":"Car","time":"2017-04-06 01:39:43","zone":"A","type":"Checked"},{"id":101,"vehicle_type":"Truck","time":"2017-04-06 02:35:45","zone":"B","type":"Unchecked"},{"id":102,"vehicle_type":"Truck","time":"2017-04-05 03:20:12","zone":"A","type":"Checked"},{"id":103,"vehicle_type":"Car","time":"2017-04-04 10:05:04","zone":"C","type":"Unchecked"}]\'\ndf = pd.read_json(io.StringIO(df_string))\ndf[\'zone\'] = pd.Categorical(df.zone)\ndf[\'vehicle_type\'] = pd.Categorical(df.vehicle_type)\ndf[\'type\'] = pd.Categorical(df.type)\ndf[\'zone_int\'] = df.zone.cat.codes\ndf[\'vehicle_type_int\'] = df.vehicle_type.cat.codes\ndf[\'type_int\'] = df.type.cat.codes\ndf[\'time\'] = pd.to_datetime(df.time)\ndf[\'dayofweek\'] = df.time.dt.dayofweek\ndf[\'month_int\'] = df.time.dt.month\ndf[\'year_int\'] = df.time.dt.year\ndf[\'day\'] = df.time.dt.day\ndf[\'date\'] = df.time.apply(lambda x: x.date())\ndf[\'month\'] = df.date.apply(lambda x: datetime.date(x.year, x.month, 1))\ndf[\'year\'] = df.date.apply(lambda x: datetime.date(x.year, 1, 1))\ndf[\'hour\'] = df.time.dt.hour\ndf[\'mins\']  = df.time.dt.minute\ndf[\'seconds\'] = df.time.dt.second\ndf[\'time_interval_3hour\'] = df.hour.apply(lambda x : math.floor(x/3)+1)\ndf[\'time_interval_6hour\'] = df.hour.apply(lambda x : math.floor(x/6)+1)\ndf[\'time_interval_12hour\'] = df.hour.apply(lambda x : math.floor(x/12)+1)\ndf[\'weekend\']  = df.dayofweek.apply(lambda x:  x&gt;4)\n\ndf[\'ceil_quarter_an_hour\'] =df.time.apply(lambda x : ceil_dt(x))\ndf[\'ceil_half_an_hour\'] =df.time.apply(lambda x : ceil_dt(x, num_seconds=1800))\ndf.head()\n'
'def MSE2(self, y):\n    loc = T.eq(y,1).nonezeros()[0]\n    S = T.clip(self.input2[loc],0,1)\n    self.input2 = T.set_subtensor(self.input2[loc], S)\n    return T.mean((y - self.input2) ** 2)\n'
'evaluator(expected, predicted)\n\nevaluator(gs.best_estimator_, test_data, expected)\n'
"inputs = Input(batch_shape=(batch_size, look_back, 1))\n\nlstm = LSTM(4, stateful=True)(inputs)\ndense = Dense(1)(lstm)\n\nmodel = Model(inputs=inputs, outputs=dense)\n\nmodel.compile(loss='mse', optimizer='adam')\nfor i in range(100):\n    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n    model.reset_states()\n"
'def relu(x, deriv=False):#RELU\n    if (deriv == True):\n        mask = x &gt; 0\n        x[mask] = 1\n        x[~mask] = 0\n    else: # HERE YOU WERE MISSING "ELSE"\n        return np.maximum(0,x)\n'
'tf.cast(training_set, tf.float32) #error occurs with/without casts\n\ntraining_set = tf.cast(training_set, tf.float32)\n'
'def decoding_layer(dec_input, encoder_state,\n               target_sequence_length, max_target_sequence_length,\n               rnn_size,\n               num_layers, target_vocab_to_int, target_vocab_size,\n               batch_size, keep_prob, decoding_embedding_size , encoder_outputs):\n"""\nCreate decoding layer\n:param dec_input: Decoder input\n:param encoder_state: Encoder state\n:param target_sequence_length: The lengths of each sequence in the target batch\n:param max_target_sequence_length: Maximum length of target sequences\n:param rnn_size: RNN Size\n:param num_layers: Number of layers\n:param target_vocab_to_int: Dictionary to go from the target words to an id\n:param target_vocab_size: Size of target vocabulary\n:param batch_size: The size of the batch\n:param keep_prob: Dropout keep probability\n:param decoding_embedding_size: Decoding embedding size\n:return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)\n"""\n# 1. Decoder Embedding\ndec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\ndec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n# 2. Construct the decoder cell\ndef create_cell(rnn_size):\n    lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n                                        initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))\n    drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n    return drop\n\n\ndec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])\n#dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  \n\n#attention details \nattention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) \n\nattn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)\n\nattn_zero = attn_cell.zero_state(batch_size , tf.float32 )\n\nattn_zero = attn_zero.clone(cell_state = encoder_state)\n\n#new_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())\n\n"""out_cell = tf.contrib.rnn.OutputProjectionWrapper(\n            attn_cell, target_vocab_size, reuse=True\n        )"""\n#end of attention \n#tensor_util.make_tensor_proto(attn_cell)\noutput_layer = Dense(target_vocab_size,\n                     kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n\nwith tf.variable_scope("decode"):\n    train_decoder_out = decoding_layer_train(attn_zero, attn_cell, dec_embed_input, \n                     target_sequence_length, max_target_sequence_length, output_layer, keep_prob)\n\nwith tf.variable_scope("decode", reuse=True):\n    infer_decoder_out = decoding_layer_infer(attn_zero, attn_cell, dec_embeddings, \n                         target_vocab_to_int[\'&lt;GO&gt;\'], target_vocab_to_int[\'&lt;EOS&gt;\'], max_target_sequence_length, \n                         target_vocab_size, output_layer, batch_size, keep_prob)\n\nreturn (train_decoder_out, infer_decoder_out)\n'
"def parse_patch_multi1(patch1):\n\n    # Structure for python .nrrd reader\n    data_1 , option = patch1\n\n    # Uses itertools to combine single float32 value with np.array values\n    return [i for i in itertools.chain(np.median(data_1), 0) my_own_function1(data_1)]\n\n\ndef parse_patch_multi2(patch2):\n\n    # Structure for python .nrrd reader\n    data_2 = patch2\n\n    # Uses itertools to combine single float32 value with np.array values\n    return [i for i in itertools.chain(my_own_function2(data_2)]\n\n# Directories\ndir_01 = '/Users/FK/Documents/image/01/'\ndir_02 = '/Users/FK/Documents/image/02/'\n\n# Method 01 patch data\nfile_dir_1 = Path(dir_01)\nfiles_1 = file_dir_1.glob('*.nrrd')\npatches_1 = read_patches_multi1(files_1)\n\n# Method 02 patch data\nfile_dir_2 = Path(dir_02)\nfiles_2 = file_dir_2.glob('*.nrrd')\npatches_2 = read_patches_multi2(files_2)\n\ntraining_file_multi1 = np.array([parse_patch_multi1(patch1) for (patch1) in patches_1], dtype=np.float32)\ntraining_file_multi2 = np.array([parse_patch_multi2(patch2) for (patch2) in patches_1], dtype=np.float32)\n\ntraining_file_combined= np.concatenate((training_file_multi1, training_file_multi2), axis=1)\n"
'output_feed = [prediction, y]\npreds, probs = sess.run(output_feed, print(sess.run(prediction, feed_dict={i: d for i,d in zip(X, unknowndata.T)}))\n'
'X_train, X_test, y_train, y_test = train_test_split(vector_data.reshape(-1, 1), \n             vector_target, random_state=0)\n'
'[{k: v for k, v in zip([table[word_id] for word_id in indices[indptr[i]:indptr[i + 1]]],data[indptr[i]:indptr[i + 1]].tolist())} for i in range(num_doc)]\n\nfinal_list = []\nfor i in range(num_doc):\n    new_list = []\n    for word_id in indices[indptr[i]:indptr[i + 1]]:\n        new_list.append(table[word_id])\n\n    new_dict = {}\n    for k, v in zip(new_list, data[indptr[i]:indptr[i + 1]].tolist()):\n        new_dict[k] = v\n    final_list.append(new_dict)\n'
'prediction=tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2), b_fc2)\n\nprediction=tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n'
'values[:, 1] = encoder.fit_transform(values[:, 1])\n'
'new_data = np.concatenate((X, f.squeeze()), axis=1)\n'
"def make_numpy_input_fn(x, y, batch_size):\n  def input_fn():\n    features, labels = tf.train.shuffle_batch(\n                             [tf.constant(x), tf.constant(y)],\n                             batch_size=batch_size, \n                             capacity=50*batch_size,\n                             min_after_dequeue=20*batch_size,\n                             enqueue_many=True)\n    features = {'x': features}\n    return features, labels\n  return input_fn\n"
'import numpy as np \nnp.round(np.clip(clf_LR.predict(X_predict), 0, 1))  # floats\nnp.round(np.clip(clf_LR.predict(X_predict), 0, 1)).astype(bool)  # binary\n'
'# WRONG! The result of `append` is `None`, not the list\ntest_accuracy_2 = test_accuracy_2.append(test_accuracy_1)\n\n# OK. Just collect the values in the list\ntest_accuracy_2.append(test_accuracy_1)\n'
'from sklearn import svm\n\n\ndata = [\n    # year, month, geo, type, count\n    [2016, 1, 1, \'A\', 50],\n    [2016, 1, 1, \'B\', 20],\n    [2016, 1, 2, \'A\', 10],\n    [2016, 1, 2, \'B\', 18],\n    [2016, 2, 1, \'A\', 62],\n    [2016, 2, 1, \'B\', 29],\n    [2016, 2, 2, \'A\', 14],\n    [2016, 2, 2, \'B\', 22],\n    [2016, 3, 1, \'A\', 59],\n    [2016, 3, 1, \'B\', 27],\n    [2016, 3, 2, \'A\', 16],\n    [2016, 3, 2, \'B\', 23],\n]\n\nX = []\ny = []\n\nfor year, month, geo, t, count in data:\n    for i in range(count):\n        X.append([year, month, geo])\n        y.append(t)\n\nclf = svm.SVC(probability=True)\n\nclf.fit(X, y)\n\ntest = [\n    [year, month, geo]\n    for year in [2016, 2017]\n    for month in range(1, 13)\n    for geo in [1, 2]\n]\n\nprediction = clf.predict_proba(test)\n\nfor (year, month, geo), proba in zip(test, prediction):\n    s = " ".join("%s=%.2f" % (cls, p)\n                 for cls, p in zip(clf.classes_, proba))\n    print("%d-%02d geo=%d: %s" % (year, month, geo, s))\n\n2016-01 geo=1: A=0.69 B=0.31\n2016-01 geo=2: A=0.39 B=0.61\n2016-02 geo=1: A=0.69 B=0.31\n2016-02 geo=2: A=0.39 B=0.61\n2016-03 geo=1: A=0.69 B=0.31\n2016-03 geo=2: A=0.39 B=0.61\n2016-04 geo=1: A=0.65 B=0.35\n2016-04 geo=2: A=0.43 B=0.57\n2016-05 geo=1: A=0.59 B=0.41\n2016-05 geo=2: A=0.50 B=0.50\n2016-06 geo=1: A=0.55 B=0.45\n2016-06 geo=2: A=0.54 B=0.46\n2016-07 geo=1: A=0.55 B=0.45\n2016-07 geo=2: A=0.54 B=0.46\n2016-08 geo=1: A=0.55 B=0.45\n2016-08 geo=2: A=0.54 B=0.46\n2016-09 geo=1: A=0.55 B=0.45\n2016-09 geo=2: A=0.55 B=0.45\n2016-10 geo=1: A=0.55 B=0.45\n2016-10 geo=2: A=0.55 B=0.45\n2016-11 geo=1: A=0.55 B=0.45\n2016-11 geo=2: A=0.55 B=0.45\n2016-12 geo=1: A=0.55 B=0.45\n2016-12 geo=2: A=0.55 B=0.45\n2017-01 geo=1: A=0.65 B=0.35\n2017-01 geo=2: A=0.43 B=0.57\n2017-02 geo=1: A=0.65 B=0.35\n2017-02 geo=2: A=0.43 B=0.57\n2017-03 geo=1: A=0.65 B=0.35\n2017-03 geo=2: A=0.43 B=0.57\n2017-04 geo=1: A=0.62 B=0.38\n2017-04 geo=2: A=0.46 B=0.54\n2017-05 geo=1: A=0.58 B=0.42\n2017-05 geo=2: A=0.51 B=0.49\n2017-06 geo=1: A=0.55 B=0.45\n2017-06 geo=2: A=0.54 B=0.46\n2017-07 geo=1: A=0.55 B=0.45\n2017-07 geo=2: A=0.54 B=0.46\n2017-08 geo=1: A=0.55 B=0.45\n2017-08 geo=2: A=0.54 B=0.46\n2017-09 geo=1: A=0.55 B=0.45\n2017-09 geo=2: A=0.55 B=0.45\n2017-10 geo=1: A=0.55 B=0.45\n2017-10 geo=2: A=0.55 B=0.45\n2017-11 geo=1: A=0.55 B=0.45\n2017-11 geo=2: A=0.55 B=0.45\n2017-12 geo=1: A=0.55 B=0.45\n2017-12 geo=2: A=0.55 B=0.45\n'
'train_set = ISPRS_dataset(train_ids, cache=CACHE)\ntrain_loader = torch.utils.data.DataLoader(train_set,batch_size=BATCH_SIZE)\n'
"import tensorflow as tf\nimport numpy as np\n\ntrainingDataSet_ = np.loadtxt('/data/optdigits.tra', delimiter=',');\ntrainingDataSet = tf.convert_to_tensor(trainingDataSet_, np.int32)\n\n# store labels of each sample\ny = trainingDataSet[:, 64]\n\n# remove lables from features\nx = trainingDataSet[:, :64]\n"
'int_mat = np.array([[0,0],[0,1],[0,2]])\n\narray([[0, 0],\n   [0, 1],\n   [0, 2]])\n\nfrom sklearn.preprocessing import StandardScaler\n\nssc = StandardScaler()\nint_scaled = ssc.fit_transform(int_mat)\ninverse_scaling = ssc.inverse_transform(int_scaled)\n\narray([[ 0.        , -1.22474487],\n       [ 0.        ,  0.        ],\n       [ 0.        ,  1.22474487]])\n\narray([[0.00000000e+00, 1.11022302e-16],\n       [0.00000000e+00, 1.00000000e+00],\n       [0.00000000e+00, 2.00000000e+00]])\n'
"import tensorflow as tf\nimport numpy as np\n\n\ndef read_data():\n    n_train = 100\n    n_test = 50\n    height = 20\n    width = 30\n    channels = 3\n    trainX = (np.random.random(\n        size=(n_train, height, width, channels)) * 255).astype(np.uint8)\n    testX = (np.random.random(\n            size=(n_test, height, width, channels))*255).astype(np.uint8)\n    trainY = (np.random.random(size=(n_train,))*10).astype(np.int32)\n    testY = (np.random.random(size=(n_test,))*10).astype(np.int32)\n    return trainX, testX, trainY, testY\n\n\ntrainX, testX, trainY, testY = read_data()\n# trainX [num_image, height, width, channels], these are numpy arrays\n\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY))\ntest_dataset = tf.data.Dataset.from_tensor_slices((testX, testY))\n\n\ndef map_single(x, y):\n    print('Map single:')\n    print('x shape: %s' % str(x.shape))\n    print('y shape: %s' % str(y.shape))\n    x = tf.image.per_image_standardization(x)\n    # Consider: x = tf.image.random_flip_left_right(x)\n    return x, y\n\n\ndef map_batch(x, y):\n    print('Map batch:')\n    print('x shape: %s' % str(x.shape))\n    print('y shape: %s' % str(y.shape))\n    # Note: this flips ALL images left to right. Not sure this is what you want\n    # UPDATE: looks like tf documentation is wrong and you need a 3D tensor?\n    # return tf.image.flip_left_right(x), y\n    return x, y\n\n\nbatch_size = 32\ntrain_dataset = train_dataset.repeat().shuffle(100)\ntrain_dataset = train_dataset.map(map_single, num_parallel_calls=8)\ntrain_dataset = train_dataset.batch(batch_size)\ntrain_dataset = train_dataset.map(map_batch)\ntrain_dataset = train_dataset.prefetch(2)\n\ntest_dataset = test_dataset.map(\n        map_single, num_parallel_calls=8).batch(batch_size).map(map_batch)\ntest_dataset = test_dataset.prefetch(2)\n\n\niterator = tf.data.Iterator.from_structure(train_dataset.output_types, \n                 train_dataset.output_shapes)\nfeatures, labels = iterator.get_next()\ntrain_init_op = iterator.make_initializer(train_dataset)\ntest_init_op = iterator.make_initializer(test_dataset)\n\n\nwith tf.Session() as sess:\n    sess.run(train_init_op)\n    feat, lab = sess.run((features, labels))\n\n    print(feat.shape)\n    print(lab.shape)\n\n    sess.run(test_init_op)\n    feat, lab = sess.run((features, labels))\n\n    print(feat.shape)\n    print(lab.shape)    \n"
'from tensorflow.contrib.learn.python.learn import monitors as monitor_lib\n\nest = tf.Estimator.DNNRegressor(...)\n\nvalidation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n    input_fn=eval_input_fn,\n    every_n_steps=100,\n)\nlist_of_monitors_and_hooks = [validation_monitor]\nhooks = monitor_lib.replace_monitors_with_hooks(list_of_monitors_and_hooks, est)\n\nest.train(\n    input_fn=input_fn_train,\n    steps=1000,\n    hooks=hooks\n)\n'
'import numpy as np\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\ndef sigmoid(x):\n    return 1.0/(1+np.asmatrix(np.exp(-x)))\n\ndef graD(X,y,alpha,s0,numda):\n    m=np.size(X,0)\n    X0=X[:,0]\n    X1=X[:,1:]\n\n    theta=np.asmatrix(np.zeros(np.size(X,1))).T\n    s=100\n    lit=0\n\n    s_history = []\n\n    while s&gt;s0 and lit&lt;=10000:\n        theta0=theta[0]\n        theta1=theta[1:]\n\n        sig = sigmoid(X*theta)\n\n        theta0_temp = theta0 - (float(alpha)/m)*X0.T*(sig-y)\n        theta1_temp = theta1 - (float(alpha)/m)*X1.T*(sig-y)   \n        theta=np.vstack((np.asmatrix(theta0_temp),np.asmatrix(theta1_temp)))\n\n        lit+=1\n\n        # calculating the cost function\n        part1 = np.multiply(y, np.log(sig))\n        part2 = np.multiply((1 - y), np.log(1 - sig))\n\n        s = (-part1 - part2).sum()/m        \n\n        s_history.append(s)\n\n    plt.plot(s_history)\n    plt.title("Cost function")\n    plt.grid()\n    plt.show()\n\n    print theta\n    print (-theta[0]/theta[1])  \n\n    return theta\n\n# Main module\n_f = loadtxt(\'data/ex2data2_n_1.txt\', delimiter=\',\')\n\n_X, _y = _f[:,[0]], _f[:,[1]]\n\n_m = np.shape(_X)[0]\n\n# add a column of 1\n_X = np.hstack((np.matrix(np.ones((_m, 1))),_X))\n_y = np.matrix(_y)\n\n_alpha = 0.01  \n\n_n= np.shape(_X)[1]\n\n_w = np.matrix(np.zeros((_n, 1)))\n\ngraD(_X, _y, _alpha, 0.1, 0.1)\n\ntheta = \n[[-5.51133636]\n [ 0.77301063]]\n\n-theta0/theta1 = \n[[ 7.12970317]]    #this number is the decision boundary for the 1-D case\n'
'new_word_counts = util.bagOfWords(["a place to listen to music it s making its way to the us"]) \n\nfiles = sklearn.datasets.load_files(dir_path)\n\n# Split in train/test\nX_train, X_test, y_train, y_test = sklearn.cross_validation.train_test_split(files.data, file.target)\n\n# Fit and tranform with X_train\ncount_vector = sklearn.feature_extraction.text.CountVectorizer()\nword_counts = count_vector.fit_transform(X_train)\ntf_transformer = sklearn.feature_extraction.text.TfidfTransformer(use_idf=True)\nX_train = tf_transformer.fit_transform(word_counts)\n\nclf = sklearn.svm.LinearSVC()\n\nclf.fit(X_train, y_train)\n\n# Transform X_test\ntest_word_counts = count_vector.transform(X_test) \nready_to_be_predicted = tf_transformer.transform(test_word_counts)\nX_test = clf.predict(ready_to_be_predicted)\n\n# Test example\nnew_word_counts = count_vector.transform["a place to listen to music it smaking its way to the us"]) \n\nready_to_be_predicted = tf_transformer.transform(new_word_counts)\npredicted = clf.predict(ready_to_be_predicted)\n'
'import boto3\nimport json\n\nlambda_client = boto3.client(\'lambda\')\na=[1,2,3]\nx = {"list" : a}\ninvoke_response = lambda_client.invoke(FunctionName="name_of_other_lambda",\n                                       InvocationType=\'RequestResponse\',\n                                       Payload=json.dumps(x))\nprint (invoke_response[\'Payload\'].read())\n'
'getattr(x_all,\'class\').value_counts().plot(kind=\'bar\')\n\nwith open("network_train.csv") as infile, open(\'new_network_train.csv\',\'w\') as outfile: \nfor line in infile: \n    outfile.write(line.replace(\'class\',\'class1\'))\n\nwith open("network_test.csv") as infile, open(\'new_network_test.csv\',\'w\') as outfile: \nfor line in infile: \n    outfile.write(line.replace(\'class\',\'class1\'))\n'
"# convert time\ndf['Time'] = pd.to_datetime(df['Time'], unit='s')\n\n# get output\ndf['count'] = df.groupby(['Device_Id', pd.Grouper(key='Time', freq='10min')]).cumcount()+1\n\nprint(df)\n\n  Device_Id            Msg                Time  count\n0    ABC123      connected 2018-04-26 06:37:57      1\n1    ABC123      connected 2018-04-26 06:37:59      2\n2    XYZ123  device failed 2018-04-26 06:40:14      1\n3    ABC123      connected 2018-04-26 06:47:59      1\n4    XVZ123  device failed 2018-04-26 06:48:20      1\n5    PQR123          error 2018-04-26 06:48:45      1\n6    ABC123      connected 2018-04-26 06:49:05      2\n"
'import numpy\nskf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\nscores = cross_val_score(svc, X, y, cv=skf, n_jobs=-1)\nmax_score_split = numpy.argmax(scores)\n\ntrain_indices, test_indices = k_fold.split(X)[max_score_split]\nX_train = X[train_indices]\ny_train = y[train_indices]\nX_test = X[test_indices]\ny_test = y[test_indices]\nmodel.fit(X_train, y_train) # this is your model object that should have been created before\n\nmodel.predict_proba(X_test)\n'
'x,y,correct,zeros\n1,1,1.0,0\n2,2,0.0, 0\n1,2,0.0,0\n3,1,1.0,0\n3,1,1.0,0\n4,2,0.0, 0\n5,2,0.0,0\n6,1,1.0,0\n7,1,1.0,0\n8,2,0.0, 0\n9,2,0.0,0\n10,1,1.0,0\n11,1,1.0,0\n12,1,1.0,0\n13,1,1.0,0\n14,1,1.0,0\n15,1,1.0,0\n16,1,1.0,0\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\ndata = pd.read_csv("./test.csv")\nX = data[["x","y"]]\ny = data[["correct"]]\nknn = LogisticRegression()\nscores = cross_val_score(knn, X, y.values.ravel(), cv=3, scoring="accuracy")\nscores.mean()\n'
'  svd.transform(X)\n\n  svd.inverse_transform(X)\n'
'classifier.fit(X_train, y_train)\n\npredicted = classifier.predict(test_tfidf)\n\nclassifier.fit(train_tfidf, y_train)\n'
"decoder_outputs = decoder_gru(decoder_inputs, initial_state=state_h)\n\ninputs = Input(shape=(None, 26))\ngru = GRU(64, return_sequences=True)(inputs)\noutputs = Dense(39, activation='softmax')(gru)\n\nmodel = Model(inputs, outputs)\n\ninputs = Input(shape=(None, 26))\ngru = GRU(256, return_sequences=True)(inputs)\ngru = GRU(128, return_sequences=True)(gru)\ngru = GRU(64, return_sequences=True)(gru)\noutputs = Dense(39, activation='softmax')(gru)\n\nmodel = Model(inputs, outputs)\n"
'model.compile(...., loss_weights=[1.0, 0.0]) \n'
'global graph\ngraph = tf.get_default_graph()\n\nwith graph.as_default():\n        prediction = mode.predict(...)\n'
'input = Input( ... ) \nmodel1 = load_model("feature1.h5")\nmodel2 = load_model("feature2.h5")\nmodel3 = load_model("feature3.h5")\n\nm1 = model1(input)\nm2 = model2(input)\nm3 = model3(input)\nmerged_model = merge([m1,m2,m3]) \n...\n'
'def compute_output_shape(self, input_shape):\n    return [(input_shape[0], self.output_dim), (input_shape[0], self.output_dim)]\n'
"model.compile(..., metrics=['categorical_accuracy'])\n\nmodel.compile(..., metrics=['accuracy'])\n"
'class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    def __init__(self,  **kwargs):\n        super().__init__(**kwargs)\n\nclass TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n    def __init__(self, num_words=8000, **kwargs):\n        super().__init__(num_words, **kwargs)\n'
'idx = int(len(train_X) * 0.2)  # 0.2 is the value of validation split\n# if train_X and train_y are numpy arrays\nval_X = train_X[idx:]   \nval_y = train_y[idx:]\n\n# if train_X and train_y are pandas dataframes\nval_X = train_X.iloc[idx:]\nval_y = train_y.iloc[idx:]\n'
'shared_layer = net.layers[0] # you want the first layer, so index = 0\nshared_layer.trainable = False\n\ninp = Input(the_shape_of_one_input_sample) # e.g. (2048,)\nx = shared_layer(inp)\nx = Dense(800, ...)(x)\nout = Dense(1, ...)(x)\n\nmodel = Model(inp, out)\n\n# the rest is the same...\n'
"x_train = num_converter_flatten(distribution_train)\ny_train = num_converter_flatten(probs_train)\n\nx_train = tf.keras.utils.normalize(x_train, axis=1)\ny_train = np.array(y_train)#tf.keras.utils.normalize(y_train, axis=1)\n\nmodel.add(tf.keras.layers.Dense(80, activation=tf.nn.sigmoid))\n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n"
"from skimage.io import imread\nfrom skimage.transform import resize\nfrom skimage import img_as_float\n\n#place holder for images\ntrain_Images = numpy.zeros((sample_counts, height, width, channels))\n\nfor index,file in enumerate(ListOfFileNames):\n    img = io.imread(file)\n    #In case you want to resize images use the line blow\n    img = resize(img, (height,width,channels), mode='reflect', anti_aliasing=True)\n    train_Images[index] = img_as_float(img)\n"
'predictions = model.predict(testData)\n'
'decoder.layers\nOut:\n[&lt;keras.engine.input_layer.InputLayer at 0x7f8a44d805c0&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e58400&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e746d8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e14940&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e2dba8&gt;]\n\nautoencoder.layers\nOut:[&lt;keras.engine.input_layer.InputLayer at 0x7f8a44e91c18&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e91c50&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e91ef0&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e89080&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e89da0&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e58400&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e746d8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e14940&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44e2dba8&gt;]\n\ndecoder.layers\nOut:\n[&lt;keras.engine.input_layer.InputLayer at 0x7f8a41de05f8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41ee4828&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41eaceb8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41e50ac8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41e5d780&gt;]\n\nautoencoder.layers\nOut:\n[&lt;keras.engine.input_layer.InputLayer at 0x7f8a41da3940&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41da3978&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41da3a90&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41da3b70&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a44720cf8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41ee4828&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41eaceb8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41e50ac8&gt;,\n &lt;keras.layers.core.Dense at 0x7f8a41e5d780&gt;]\n'
'# perform reshaping prior to passing to Keras\nfeatures = Input(shape=(None, 10))\n'
'folds &lt;- 3\nset.seed(42)\ncvIndex &lt;- createFolds(your_data, folds, returnTrain = T)\n\nfit.control &lt;- trainControl(method = "cv",\n                            index = cvIndex,\n                            number = folds,\n                            classProbs = TRUE, \n                            summaryFunction = twoClassSummary,\n                            allowParallel = FALSE)\n\nsearch.grid &lt;- expand.grid(.mtry = c(seq.int(1:sqrt(length(your_data)))+1))\n\nrfCaret &lt;- train(your_data_x, your_data_y, method = "rf", \n                     metric = \'ROC\', ntree = 500,\n                     trControl = fit.control, tuneGrid = search.grid,\n)\n\npreProcess(yourData, method = c("center", "scale"))\n\nfolds &lt;- 3\nset.seed(42)\ncvIndex &lt;- createFolds(dataset$Outcome, folds, returnTrain = T)\n\nfit.control &lt;- trainControl(method = "cv",\n                            index = cvIndex,\n                            number = folds,\n                            classProbs = TRUE, \n                            summaryFunction = twoClassSummary,\n                            allowParallel = FALSE)\n\n\nSVMCaret &lt;- train(Outcome ~ ., data = dataset, method = "svmLinear", \n                 metric = \'ROC\', \n                 trControl = fit.control)\n'
'def convert_tensor_from_camera(image)\n    image = image / 255\n    return [image]\n'
'df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\n'
'alg = GaussianNB()                       # initiating model\nalg = alg.fit(data_train, target_train)  # fitting model with train data\npred = alg.predict(***data_test***)      # testing with test data\nnew_pred = alg.predict(new_data)         # test with new data`     \n'
'X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=30, stratify=y)\n'
'import tensorflow as tf\n\na = [[0.462, 0.465, 0.492, 0.58],\n     [0.435, 0.385, 0.462, 0.467],\n     [0.586, 0.449, 0.627, 0.616],\n     [0.431, 0.671, 0.494, 0.8],\n     [0.519, 0.282, 0.586, 0.449]]\n\nb = [[0.465, 0.467, 0.491, 0.586],\n     [0.432, 0.488, 0.464, 0.59],\n     [0.585, 0.442, 0.633, 0.625],\n     [0.429, 0.664, 0.493, 0.805],\n     [0.524, 0.502, 0.594, 0.642]]\n\nmetric = tf.keras.metrics.MeanIoU(num_classes=1)\nprint(metric(a, b))\n&gt;&gt; tf.Tensor(1.0, shape=(), dtype=float32)\n'
"ct = make_column_transformer((OneHotEncoder(), cat_cols), remainder = 'passthrough')\n\nX_test_transformed = ohe.transform(X_test)\n"
'&gt;&gt;&gt; x = torch.FloatTensor([1,2,3])\n&gt;&gt;&gt; x.requires_grad = True\n&gt;&gt;&gt; x.sum().backward() # backward pass\n\n&gt;&gt;&gt; x.is_leaf\nTrue\n&gt;&gt;&gt; x.grad\ntensor([1., 1., 1.])\n\n&gt;&gt;&gt; x = torch.FloatTensor([1,2,3])\n&gt;&gt;&gt; x.requires_grad=True\n&gt;&gt;&gt; z = x.sum()\n&gt;&gt;&gt; z.backward()\n\n&gt;&gt;&gt; z.is_leaf\nFalse\n&gt;&gt;&gt; z.grad\nNone\n\n&gt;&gt;&gt; z.retain_grad()\n&gt;&gt;&gt; z = x.sum()\n&gt;&gt;&gt; z.retain_grad()\n&gt;&gt;&gt; z.backward()\n\n&gt;&gt;&gt; z.is_leaf\nFalse\n&gt;&gt;&gt; z.grad\ntensor(1.)\n'
'names = \'gary gary2 Jesus jesus2 shop tech hockey hockey2\'.split()\ndocs_new = [extract_message("C:\\\\Users\\\\Cody\\\\Documents\\\\Emails\\\\%s.html" % name)\n            for name in names]\n\nfor name, category in zip(names, predicted):\n    print(\'%r ---&gt; %s\' % (name, news.target_names[category]))\n    if (news.target_names[category] == \'comp.sys.ibm.pc.hardware\'):\n        computer_emails.append(name)\n'
'&gt;&gt;&gt; from keras.datasets import mnist\nUsing Theano backend.\nUsing gpu device 0: GeForce GT 730 (CNMeM is enabled with initial size: 70.0% of memory, cuDNN not available)\n&gt;&gt;&gt; (x_train, y_train), (x_test, y_test) = mnist.load_data()\nDownloading data from https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\n15253504/15296311 [============================&gt;.] - ETA: 0s&gt;&gt;&gt;\n&gt;&gt;&gt; x_train.shape\n(60000, 28, 28)\n&gt;&gt;&gt; y_train.shape\n(60000,)\n\n&gt;&gt;&gt; import numpy\n&gt;&gt;&gt; y_train_new = numpy.zeros([60000, 10])\n&gt;&gt;&gt; for i in range(0, 10):\n...     y_train_new[:, i] = (y_train == i).astype(numpy.int32)\n'
'newdata = img_data.reshape(800*800, 4)\ngmm = GaussianMixture(n_components=3, covariance_type="tied")\ngmm = gmm.fit(newdata)\n\ncluster = gmm.predict(newdata)\ncluster = cluster.reshape(800, 800)\nimshow(cluster)\n'
'prices = c(104.285380,101.347495,101.357033,102.778283,106.727258,106.841721,104.209071,105.134314,104.733694,101.891194,\n       101.099492,103.703526,104.495229,107.213726,107.766964,107.881427,104.104147,109.989455,113.413808,111.754094,\n       113.156266,113.175344,114.043355,114.405821,113.886963,114.643464,116.845936,119.584662,121.097665,121.691375,\n       122.409572,123.257045,123.003282,124.003971,127.360347,126.565541,123.328865,124.884959,123.012858,123.616144,\n       123.874695,123.089466,121.049785,121.231728,121.748831,119.230352,117.056607,119.172896,118.349363,119.651694,\n       121.653071,123.022434,122.088777,120.561411,121.815862,121.317912,118.148267,118.971800,118.023780,121.011481,\n       119.153744,118.981376,120.006005,121.949926,120.666746,120.274132,121.193425,121.710527,121.471128,120.944449,\n       121.404096,120.819962,119.460175,122.189325,121.528583,123.166074,124.171550,124.755684,127.025188,125.023811,\n       123.185225,119.843213,123.482080,123.242681,120.465651,119.709150,119.948549,122.715809,121.465765,121.028250,\n       121.167678,123.994700,123.821617,125.187049,125.071660,125.062044,126.340935,127.446743,124.638953,126.970765,\n       126.715948,125.273590,125.518791,124.965887,125.119739,124.388944,123.706228,122.888892,122.523495,123.927390,\n       123.648534,122.283102,122.042709,122.696577,122.408106,122.965818,121.735006,122.706193,122.148482,123.186979,\n       122.600420,121.879241,119.744552,120.605159,121.735006,121.581154,121.158062,120.859975,117.859871,115.455941,\n       118.542587,120.831128,120.783049,121.946551,123.571608,124.638953,126.994804,125.725529,120.408036,120.350342,\n       119.715705,118.052185,118.638744,118.263731,117.667556,116.638674,113.888579,110.234605,110.965400,110.705776,\n       111.582500,115.639343,109.621692,111.312044,111.225111,112.007502,113.166600,112.529097,111.089883,108.810324,\n       102.155170,99.605154,100.204021,105.951215,109.071121,109.428509,108.916574,104.048363,108.510890,106.608038,\n       105.545531,108.481913,106.395536,108.733051,110.317151,111.379658,112.316595,112.442164,110.037036,109.583056,\n       111.283066,109.534760,110.423402,111.080224,110.800109,108.607482,105.342689,106.202353,105.844965,106.617697,\n       107.004063,107.515998,107.004063,105.767692,108.298389,107.796113,107.979637,106.453491,108.047251,107.255201,\n       107.921682,109.892149,109.882489,111.563182,115.021157,111.350680,110.645562,115.204681,116.421734,115.426842,\n       117.049579,118.392201,117.841629,116.798441,117.436526,116.961193,113.274931,112.634686,112.256359,108.977526,\n       110.757603,110.287119,113.779367,115.224769,115.729205,114.225599,115.321776,114.497218,114.283803,114.759136,\n       113.827870,112.799597,111.751923,115.467287,114.739735,114.691232,112.159352,112.692890,109.792384,109.113336,\n       107.182899,108.007458,105.718095,102.856393,104.117482,104.020475,105.359170,104.796530,103.622747,105.485279,\n       104.107781,102.109440,102.196746,99.635764,97.685926,93.563134,94.057869,95.580877,96.968075,94.474998,96.541245,\n       94.222780,93.766848,93.892957,93.417623,98.384375,96.463639,96.997177,90.623825,91.273771,94.426495,93.543732,\n       91.652098,93.466127,93.708644,91.696830,92.662368,92.642862,91.940652,91.384737,91.667571,94.252091,95.695522,\n       93.881481,93.666917,94.486161,92.350274,93.725434,94.369126,94.515420,94.300856,98.045972,98.260536,98.992004,\n       100.464693,99.352862,98.533617,98.621394,98.670158,99.733225,99.986801,101.995899,103.351553,103.185754,103.302789,\n       103.293036,104.083021,103.507600,103.058966,102.590827,105.019300,106.852847,106.296931,107.272222,108.374300,\n       107.096670,108.218254,105.858050,105.975085,106.326190,107.711103,109.271568,109.330085,107.135681,104.824242,\n       104.268327,104.482891,103.351553,103.068719,102.483545,101.771582,95.402934,92.486815,91.423748,91.326219,92.828167,\n       91.862629,90.936103,90.981767,91.050455,91.668644,90.775704,88.646385,88.823011,92.120021,91.737332,92.787273,\n       92.434021,93.434899,94.622215,96.064657,97.752412,98.527602,98.468727,97.987913,96.614159,95.888032,96.084282,\n       96.780972,97.173473,97.085160,97.781850,96.977222,95.515156,95.632906,95.318905,95.721219,93.542837,93.317149,\n       94.111964,93.758713,94.298402,91.649019,90.314515,91.835457,92.630272,93.807775,94.092339,93.209211,93.739088,\n       94.141401,94.867529,95.161904,95.593656,95.053967,96.937972,96.928160,97.958475,97.997725,98.086038,97.565974,\n       96.810409,95.515156,94.857716,101.019984,102.383926,102.256363,104.061868,102.521301,103.806742,103.885243,106.032880,\n       106.910897,107.344972,106.545878,106.476821,106.723455,108.005951,107.907298,107.749452,107.611337,107.887567,\n       107.049012,107.384434,106.575474,106.121668,105.500150,105.381766,104.572806,104.671460,105.292978,106.279514,\n       106.249917,106.901031,104.099269,101.741448,104.020346,106.496551,110.265119,114.013955,113.372707,112.050749,\n       112.040883,112.021153,113.076746,111.192462,111.360173,111.567346,112.415767,110.669598,111.527885,111.005021,\n       111.478558,111.527885,112.356575,112.524286,114.487492,114.734126,115.760124,115.404971,116.046219,115.967296,\n       115.888373,115.543086,115.483894,115.030087,116.065950,116.657871,114.033686,112.938631,112.188864,112.011287,\n       109.988889,110.087542,108.351239,107.931825,109.488725,110.133301,109.954803,106.890586,107.525246,104.827942,\n       106.216260,109.072229,109.032563,109.141645,110.797711,110.867126,110.301883,110.857210,110.639046,110.529963,\n       109.597807,108.576401,108.982980,108.199572,109.032563,110.103551,111.184456,112.999187,112.354610,114.228840,\n       114.228840,114.853583,115.002331,115.666741,115.974154,116.083236,115.319661,115.547742,116.281568,115.785740,\n       115.755990,114.853583,115.180830,115.051914,115.636991,116.926144,117.997132,118.116131,118.750791,118.254963,\n       118.046715,118.998705,118.988788,118.780540,118.998705,119.078037,118.968955,120.863018,120.922517,120.932434,\n       120.615104,120.337440,127.675694,127.457529,128.002940,129.202844,130.432497,130.938241,131.315071,131.581537,\n       132.746769,134.469718,134.957721,134.793393,135.166865,136.142871,136.551200,135.973564,136.103034,136.371934,\n       136.431689,139.220278,138.393660,139.210318,138.772112,138.951378,138.433497,138.114801,138.572927,138.632682,\n       138.423538,139.887547,140.116610,139.419462,140.883471,139.270074,140.843634,140.345672,140.066813,140.305835,\n       143.213935,143.532630,143.343405,143.074505,143.114342,144.179981,143.433038,143.074505,142.755809,142.586502,\n       141.052778,141.222086,140.475142,141.251963,140.624531,140.106650,141.859477,141.690170,143.054587,143.950919,\n       143.065343,143.203975,143.064546,146.002523,146.908814,146.460648,145.932808,148.352905,152.376439,153.332527,\n       152.635380,153.322568,156.100000,155.700000,155.470000,150.250000,152.540000,152.960000,153.990000,153.800000,\n       153.340000,153.870000,153.610000,153.670000,152.760000,153.180000,155.450000,153.930000,154.450000,155.370000,\n       154.990000,148.980000,145.320000,146.590000)\n\nv.train = prices[1:500]\nv.test = prices[-(1:500)]\n\nsampleSize = 30\n\ngetJoinedLaggedData = function(dataVector){\n  data.len = length(dataVector)\n  result = NULL\n  for (i in seq(1, data.len - sampleSize + 1, by = 1)) {\n    result = rbind(result, dataVector[seq(i, i + sampleSize - 1, by = 1)])\n  }\n  list(input=matrix(result[,-sampleSize], ncol = sampleSize - 1), output = matrix(result[,sampleSize]))\n}\n\nm.train = getJoinedLaggedData(v.train)\n\n#build the model\nindata = tf$placeholder(tf$float32, shape(471, sampleSize - 1))\noutData = tf$placeholder(tf$float32, shape(471, 1))\n\nw1 &lt;- tf$Variable(tf$truncated_normal(shape(sampleSize - 1, 300),stddev = 1.0 / sqrt(sampleSize)))\nb1 = tf$Variable(tf$zeros(shape(300)))\nl1 &lt;- tf$matmul(indata, w1) + b1\n\nw2 &lt;- tf$Variable(tf$truncated_normal(shape(300, 50),stddev = 1.0 / sqrt(300)))\nb2 &lt;- tf$Variable(tf$zeros(shape(50)))\nl2 &lt;- tf$matmul(l1, w2) + b2\n\nw3 &lt;- tf$Variable(tf$truncated_normal(shape(50, 1),stddev = 1.0 / sqrt(50)))\nb3 &lt;- tf$Variable(tf$zeros(shape(1)))\npred &lt;- tf$matmul(l2, w3) + b3\n\n#loss function\nloss &lt;- tf$reduce_mean(tf$abs(tf$sub(x = pred, y = outData)))\n\n#trainer\noptimizer &lt;- tf$train$GradientDescentOptimizer(0.00003)\ntrain.op &lt;- optimizer$minimize(loss)\n\n#run the model\ninit = tf$initialize_all_variables()\n\nsess = tf$Session()\nsess$run(init)\n\nfor (i in 1:1000){\n  values = sess$run(list(train.op, loss, pred), feed_dict = dict(indata = m.train$input, outData = m.train$output))\n  loss_value = values[[2]]\n  pred_value = values[[3]]\n  print(loss_value)\n}\n\nmyLoss = mean(abs(pred_value - m.train$output))\nprint(sprintf("Tensorflow Loss: %f, My Loss: %f", loss_value, myLoss))\n'
'from sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# create the tuple data\nx_train = tuple(range(32383))\nx_train = np.asarray(x_train)\n\n#same for y_train\ny_train=tuple(range(32383))\ny_train = np.asarray(y_train)\n\n#convert tuples to nparray and reshape the x_train\nx_train = x_train.reshape(32383,1)\n\n#check if shape if (32383,)\ny_train.shape\n\n#create the model\nlg = LogisticRegression()\n\n#Fit the model\nlg.fit(x_train, y_train)\n'
'python samples\\your_folder_name\\your_python_file_name.py train --dataset="location_of_custom_dataset" --weights=coco\n\n[{\n    "filename": "000dfce9-f14c-4a25-89b6-226316f557f3.jpeg",\n    "regions": {\n        "0": {\n            "region_attributes": {\n                "object_name": "Cat"\n            },\n            "shape_attributes": {\n                "all_points_x": [75.30864197530865, 80.0925925925926, 80.0925925925926, 75.30864197530865],\n                "all_points_y": [11.672189112257607, 11.672189112257607, 17.72093488703078, 17.72093488703078],\n                "name": "polygon"\n            }\n        },\n        "1": {\n            "region_attributes": {\n                "object_name": "Cat"\n            },\n            "shape_attributes": {\n                "all_points_x": [80.40123456790124, 84.64506172839506, 84.64506172839506, 80.40123456790124],\n                "all_points_y": [8.114103362391036, 8.114103362391036, 12.205901974737595, 12.205901974737595],\n                "name": "polygon"\n            }\n        }\n    },\n    "width": 504,\n    "height": 495\n}]\n\ndef load_mask(self, image_id):\n    """Generate instance masks for an image.\n    Returns:\n    masks: A bool array of shape [height, width, instance count] with\n        one mask per instance.\n    class_ids: a 1D array of class IDs of the instance masks.\n    """\n    # If not your dataset image, delegate to parent class.\n    image_info = self.image_info[image_id]\n    if image_info["source"] != "name_of_your_project":   //change your project name\n        return super(self.__class__, self).load_mask(image_id)\n\n    # Convert polygons to a bitmap mask of shape\n    # [height, width, instance_count]\n    info = self.image_info[image_id]\n    mask = np.zeros([info["height"], info["width"], len(info["polygons"])], dtype=np.uint8)\n    class_id =  np.zeros([mask.shape[-1]], dtype=np.int32)\n\n    for i, p in enumerate(info["polygons"]):\n        # Get indexes of pixels inside the polygon and set them to 1\n        rr, cc = skimage.draw.polygon(p[\'all_points_y\'], p[\'all_points_x\'])\n        # print(rr.shape, cc.shape, i, np.ones([mask.shape[-1]], dtype=np.int32).shape, info[\'classes\'][i])\n\n        class_id[i] = self.class_dict[info[\'classes\'][i]]\n        mask[rr, cc, i] = 1\n\n\n    # Return mask, and array of class IDs of each instance. Since we have\n    # one class ID only, we return an array of 1s\n    return mask.astype(np.bool), class_id\n'
'import pandas as pd\nimport numpy as np\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV\n\n# Based on the following which has more examples:\n# http://nbviewer.jupyter.org/github/michelleful/SingaporeRoadnameOrigins/blob/master/notebooks/04%20Adding%20features%20with%20Pipelines.ipynb\n# http://michelleful.github.io/code-blog//2015/06/18/classifying-roads/\n# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n# https://stackoverflow.com/questions/49466193/how-to-add-a-feature-to-a-vectorized-data-set/49501769#49501769\n\n# Load ANSI file into pandas dataframe.\ndf = pd.read_csv(r\'c:/race.txt\', encoding = \'latin1\', usecols=[\'LAST_NAME\', \'RACE\'])\n\n# Convert last name to lower case.\ndf[\'LAST_NAME\'] = df[\'LAST_NAME\'].str.lower()\n\n# Remove the last name spaces.\n# df[\'LAST_NAME\'] = df[\'LAST_NAME\'].str.replace(\' \', \'\')\n\n# Remove all rows where race is NOT in African, Coloured, White, Indian.\ndf = df.drop(df[~df[\'RACE\'].isin([\'African\', \'Coloured\', \'White\', \'Indian\'])].index)\n\n# Returns a column from the dataframe named df as a numpy array of type string.\nclass TextExtractor(BaseEstimator, TransformerMixin):\n    """Adapted from code by @zacstewart\n       https://github.com/zacstewart/kaggle_seeclickfix/blob/master/estimator.py\n       Also see Zac Stewart\'s excellent blogpost on pipelines:\n       http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n       """\n\n    def __init__(self, column_name):\n        self.column_name = column_name\n\n    def transform(self, df):\n        # Select the relevant column and return it as a numpy array.\n        # Set the array type to be string.\n        return np.asarray(df[self.column_name]).astype(str)\n\n    def fit(self, *_):\n        return self\n\nclass Apply(BaseEstimator, TransformerMixin):\n    """Takes in a function and applies it element-wise to every element in the numpy array it\'s supplied with."""\n\n    def __init__(self, fn):\n        self.fn = np.vectorize(fn)\n\n    def transform(self, data):\n        # Note: reshaping is necessary because otherwise sklearn\n        # interprets the 1-d array as a single sample.\n        return self.fn(data.reshape(data.size, 1))\n\n    def fit(self, *_):\n        return self\n\nclass AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n    """Takes in dataframe, extracts last name column, outputs average word length"""\n\n    def __init__(self):\n        pass\n\n    def average_word_length(self, name):\n        """Helper code to compute average word length of a name"""\n        return np.mean([len(word) for word in name.split()])\n\n    def transform(self, df, y=None):\n        """The workhorse of this feature extractor"""\n        return df[\'LAST_NAME\'].apply(self.average_word_length)\n\n    def fit(self, df, y=None):\n        """Returns self unless something different happens in train and test"""\n        return self\n\n# Let\'s pick the same random 10% of the data to train with.\nrandom.seed(1965)\ntrain_test_set = df.loc[random.sample(list(df.index.values), int(len(df) / 10))]\n\n# X = train_test_set[[\'road_name\', \'has_malay_road_tag\']]\nX = train_test_set[[\'LAST_NAME\']]\ny = train_test_set[\'RACE\']\n\nvect = CountVectorizer(ngram_range=(1,4), analyzer=\'char\')\nclf = LinearSVC()\n\npipeline = Pipeline([\n    (\'name_extractor\', TextExtractor(\'LAST_NAME\')),    # Extract names from df.\n    (\'text_features\', FeatureUnion([\n        (\'vect\', vect),    # Extract ngrams from names.\n        (\'num_words\', Apply(lambda s: len(s.split()))),    # Number of words.\n        (\'ave_word_length\', Apply(lambda s: np.mean([len(w) for w in s.split()]))), # Average word length.\n    ])),\n    (\'clf\' , clf),     # Feed the output through a classifier.\n])\n\ndef run_experiment(X, y, pipeline, num_expts=100):\n    scores = list()\n    for i in range(num_expts):\n        X_train, X_test, y_train, y_true = train_test_split(X, y)\n        model = pipeline.fit(X_train, y_train)  # Train the classifier.\n        y_test = model.predict(X_test)          # Apply the model to the test data.\n        score = accuracy_score(y_test, y_true)  # Compare the results to the gold standard.\n        scores.append(score)\n\n    print(sum(scores) / num_expts)\n\n# Run x times (num_expts) and get the average accuracy.\nrun_experiment(X, y, pipeline, 100)\n'
'import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams[\'figure.figsize\'] = (20, 6)\n\ndf1 = pd.read_csv("TrainData.csv")\ndf2 = pd.read_csv("TestData.csv")\n\n\ntrain_data_X = np.asanyarray(df1[\'ENGINE SIZE\'])\ntrain_data_Y = np.asanyarray(df1[\'CO2 EMISSIONS\'])\ntest_data_X = np.asanyarray(df2[\'ENGINE SIZE\'])\ntest_data_Y = np.asanyarray(df2[\'CO2 EMISSIONS\'])\n\nW = tf.Variable(20.0, name= \'Weight\')\nb = tf.Variable(30.0, name= \'Bias\')\nX = tf.placeholder(tf.float32, name= \'Input\')\nY = tf.placeholder(tf.float32, name= \'Output\')\nprediction= W*X + b\n\n\nloss = tf.reduce_mean(tf.square(prediction - Y))\noptimizer = tf.train.GradientDescentOptimizer(0.05)\ntrain = optimizer.minimize(loss)\nloss_values = []\ntrain_data = []\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init)\n    for step in range(100):\n        for (x,y) in zip(train_data_X,train_data_Y):\n            _, loss_val, a_val, b_val = sess.run([train, loss, W, b], feed_dict={X:x, Y:y})\n            loss_values.append(loss_val)\n        if step % 5 == 0:\n            print(step, loss_val, a_val, b_val)\n            train_data.append([a_val, b_val])\n\nplt.plot(loss_values, \'ro\')\nplt.show()\n\nn_samples = train_data_X.shape[0]\nloss = tf.reduce_sum(tf.pow(prediction - Y, 2)) / (2 * n_samples)\n\ntrain_data_X = np.asanyarray(df1[[\'ENGINE SIZE\',\'MILEAGE\']])\ntrain_data_Y = np.asanyarray(df1[\'CO2 EMISSIONS\'])\ntest_data_X = np.asanyarray(df2[[\'ENGINE SIZE\',\'MILEAGE\']])\ntest_data_Y = np.asanyarray(df2[\'CO2 EMISSIONS\'])\n'
'als = ALS(rank=5, maxIter=5, alpha = 1.0, implicitPrefs=True, seed=0)\nmodel = als.fit(ratings)\n'
"results = model.evals_result_['valid_0']['l1']\nbest_perf = min(results)\nnum_boost = results.index(best_perf)\nprint('with boost', num_boost, 'perf', best_perf)    \nmodel = lgb.LGBMRegressor(**lgb_params, n_estimators=num_boost+1, n_jobs=-1)\nmodel.fit(X_train, y_train, verbose=-1)\n"
'D = [{"sentence":"This is sentence one", \n       "keyword":"key 1"},\n     {"sentence":"This is sentence one", \n       "keyword":"key 2"},\n     {"sentence":"This is sentence one", \n       "keyword":"key 3"},\n     {"sentence":"This is sentence one", \n       "keyword":"key 2"},\n     {"sentence":"This is sentence one", \n       "keyword":"key 1"}]\n\n[[1. 0. 0. 0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 1.]\n [0. 0. 1. 0. 0. 0. 1. 0.]\n [0. 1. 0. 0. 1. 0. 0. 0.]\n [1. 0. 0. 1. 0. 0. 0. 0.]]\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nsentences = [d[\'sentence\'] for d in D]\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(sentences)\n'
'p0 = (10,10)\np1 = (400,400)\nis_p0_ok = cv2.pointPolygonTest(approx, p0, False) &lt; 0\nis_p1_ok = cv2.pointPolygonTest(approx, p1, False) &lt; 0\nprint(is_p0_ok)\n&gt;&gt;&gt; True\nprint(is_p1_ok)\n&gt;&gt;&gt; False\n'
'errors = abs(predictions - test_labels)\n\naccuracy = 100 - mape\n'
"# _____________ FIRST MAX POOLING LAYER _____________________\nA_pool1 = tf.nn.max_pool(A_conv1)\n\n# _____________ FIRST DROPOUT LAYER _____________________\nA_out1 = tf.nn.dropout(x=A_pool1, rate=dropout_prob)\n\n# _____________ SECOND CONVOLUTIONAL LAYER _____________________\nA_conv2 = tf.nn.relu(tf.nn.conv2d(A_out1, W_conv2))\n\nconv2 = maxpool2d(conv2, k=2)\nA_out1 = tf.nn.dropout(x=conv2, rate=0.5)\nconv3 = conv2d(A_out1, weights['wc3'], biases['bc3'])\n\narray([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\nx2 = np.reshape(x, (2,5))\n\n&gt;&gt;&gt; x2\narray([[0, 1, 2, 3, 4],\n   [5, 6, 7, 8, 9]])\n\nnp.roll(x2, 1, axis=1)\narray([[4, 0, 1, 2, 3],\n       [9, 5, 6, 7, 8]])\n"
'model = torch.nn.Sequential(Model1(), Model2())\n\n        model1 = Model1()\n        model2 = Model2()\n        y1 = model1(X)\n        y2 = model2(y1)\n        loss = criterion(y2, y)\n        loss.backward()\n'
"model = Sequential()\n# Adding the input layer and the first hidden layer\nmodel.add(Dense(16, activation = 'relu', input_dim = 243))\n# Adding the output layer\nmodel.add(Dense(units = 1))\nmodel.compile(optimizer = 'adam',loss = 'mean_squared_error')\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40) # Model stop training after 40 epoch where validation loss didnt decrease\nmc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True) #You save model weight at the epoch where validation loss is minimal\ntrain = model.fit((train_X, train_label, batch_size=batch_size),epochs=1000,verbose=1,validation_data=(valid_X, valid_label),callbacks=[es,mc])#you can run for 1000 epoch btw model will stop after 40 epoch without better validation loss\n\n"
'import numpy as np\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nxx = [[1],[1],[1],[2],[2],[3]]\nyy = [1,1,1,0,0,0]\n\nmlb = MultiLabelBinarizer()\nxxtransformed =  mlb.fit_transform(xx)\nprint(xxtransformed)\n# [[1 0 0]\n# [1 0 0]\n# [1 0 0]\n# [0 1 0]\n# [0 1 0]\n# [0 0 1]]\n\nclf = MultinomialNB()\nclf.fit(xxtransformed,yy)\nprint(mlb.transform(np.array([[1]])))\n#[[1 0 0]]\nprint(clf.predict(mlb.transform(np.array([[1]]))))\n#[1]\n'
'if hsv_frame[i][j][1]&gt;= low_red and hsv_frame[i][j][1]&lt;=upper_red:\n\nif (hsv_frame[i][j][1]&gt;= low_red).all() and (hsv_frame[i][j][1]&lt;=upper_red).all():\n'
'dcc.Dropdown(\n        id=\'dropdown-5\',\n        options=[{\'label\': \'Logistic\', \'value\': \'log_clf\'},\n                 {\'label\': \'RFE\', \'value\': \'rfe_selector\'}],\n        value=\'log_clf\',\n        style={\'width\': \'150px\', \'height\': \'35px\', \'fontSize\': \'10pt\'}\n    )], style={})\n\nmodels = {\'Logistic\':log_clf, \'RFE\':rfe_selector}\n""""i jumped some line of code"""\ndcc.Dropdown(\n        id=\'dropdown-5\',\n        options=[{\'label\': v, \'value\': v} for v in [\'Logistic\',\'RFE\']],\n        value=\'Logistic\',\n        style={\'width\': \'150px\', \'height\': \'35px\', \'fontSize\': \'10pt\'}\n    )\n\n# function to run selected model\ndef ClassTrainEval(model):\n    fitModel = model.fit(X_train, y_train)\n    y_pred = fitModel.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    return fitModel, y_pred, y_score, cm #Note that y_score is never defined so you need to remove this \n\n# dash callback\n@app.callback(Output(\'conf_matrix\', \'figure\'), [Input(\'dropdown-5\', \'value\')])\ndef update_cm_matix(model):\n    for model in models: #&lt;-------No loop needed\n        ClassTrainEval(model) #&lt;-------Here You need to assigne the output\n    return {\'data\': [go.Heatmap(x=class_names, y=class_names, z=cm, showscale=True, colorscale=\'blues\')],\n            \'layout\': dict(width=350, height=280, margin={\'t\': 10},\n                           xaxis=dict(title=\'Predicted class\', tickvals=[0, 1]),\n                           yaxis=dict(title=\'True class\', tickvals=[0, 1], autorange=\'reversed\'))}\n\n@app.callback(Output(\'conf_matrix\', \'figure\'), [Input(\'dropdown-5\', \'value\')])\ndef update_cm_matix(v):\n    model = models[v]\n    fitModel, y_pred, cm =  ClassTrainEval(model)\n    return {\'data\': [go.Heatmap(x=class_names, y=class_names, z=cm, showscale=True, colorscale=\'blues\')],\n            \'layout\': dict(width=350, height=280, margin={\'t\': 10},\n                           xaxis=dict(title=\'Predicted class\', tickvals=[0, 1]),\n                           yaxis=dict(title=\'True class\', tickvals=[0, 1], autorange=\'reversed\'))}\n'
"df=df.rename_axis(index='Date')\n\ndf=df.reset_index()\n\ndf['Date']\n\ndf.index\n"
"encoded = ohe.fit_transform(arr.reshape(-1, 1))\nprint(encoded)\n\nprint(ohe.categories_) # this is the line you're looking for\n"
"forall grid\n  forall crimes\n    if crime.cell == grid.cell\n      do something\n\ngridIdxToCrimes = {} // to a grid_index you associate all the crimes\n\nfor crime_row in crime.iterrows():\n  grid_index = crime_row['grid']\n  if grid_index not in gridIdxToCrimes:\n    gridIdxToCrimes[grid_index] = []\n  gridIdxToCrimes[grid_index].push(crime_row)\n\nforall grid_index, grid_row in grid.iterrows():\n  topIndex = grid_row['top ']\n  if topIndex in gridIdxToCrimes:\n    # you get all the crimes above your current grid\n    near += count(gridIdxToCrimes[topIndex])\n\n"
'y_val = train_data[:10000]\ny_train = train_data[10000:]\n\ny_val = train_labels[:10000]\ny_train = train_labels[10000:]\n'
"model.add(keras.layers.Dense(units = 1, activation = 'linear')\n"
"[['d', 2, 6, 7], ['c', 14, 15], ['a', 3, 5, 6, 9], ['b', 14, 15, 56, 4, 76, 44, 91, 100]]\n"
"embedding_layer = Embedding(412457, 400, weights=[embeddings_dictionary.vectors], input_length=maxlen , trainable=False)\n\nmaxlen = 400\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n"
'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom  matplotlib.ticker import MaxNLocator #imported this to set the tick locators correct\nimport seaborn as sns\n%matplotlib inline\n\ndic = {\'Year\': {0: 2004, 1: 2009, 2: 2014, 3: 2019}, \'BJP\': {0: 40.67, 1: 35.2, 2: 46.63, 3: 56.56}, \'INC\': {0: 54.81, 1: 57.11, 2: 15.22, 3: 22.63}, \'AAP\': {0: 0.0, 1: 0.0, 2: 33.08, 3: 18.2}, \'Total_Polling\': {0: 47.09, 1: 51.81, 2: 65.1, 3: 60.59}} \nparl_data = pd.DataFrame(dic)\n\nfig, ax = plt.subplots(figsize = (15,6)) # assigned fig and ax in one line and added size to figure for better visibility\nplt.plot(parl_data[\'BJP\'], marker=\'o\', label=\'BJP\', color=\'red\')\nplt.plot(parl_data[\'INC\'], marker=\'o\', label=\'INC\', color=\'blue\')\nplt.plot(parl_data[\'AAP\'], marker=\'o\', label=\'AAP\', color=\'brown\')\nplt.plot(parl_data[\'Total_Polling\'], marker=\'o\', label=\'Total Polling\', color=\'green\', linestyle=\'dashed\')\nplt.legend()\n\nfor i,j in parl_data.BJP.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\nfor i,j in parl_data.INC.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\nfor i,j in parl_data.AAP.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\nfor i,j in parl_data.Total_Polling.items():\n    ax.annotate(str(j), xy=(i, j), size = 15) # increased the font size as it was looking cluttered\n\n\nax.set_alpha(0.8)\nax.set_title("\\n\\nParty-wise vote share in Lok Sabha Polls (Delhi) 2019\\n", fontsize=15)\nax.set_ylabel("Parliament Polling Percentage\\n", fontsize=15);\nax.set_xlabel("Election Year\\n", fontsize=15)\nplt.yticks(np.arange(0,100,10))\n\n##I Added from here\nax.xaxis.set_major_locator(MaxNLocator(4)) # To set the locators correct\nax.set_xticklabels([\'0\',\'2004\',\'2009\',\'2014\',\'2019\']) # To set the labels correct.\n\nplt.tick_params( left = False,bottom = False, labelsize= 13) #removed tick lines\nplt.legend(frameon = False, loc = "best", ncol=4, fontsize = 14) # removed the frame around the legend and spread them along a line\nplt.box(False) # Removed the box - 4 lines - you may keep if you like. Comment this line out\nplt.style.use(\'bmh\') #used the style bmh for better look n feel (my view) you may remove or keep as you like\n\n\nplt.show();\n\nwidth = .35\nind = np.arange(len(parl_data))\n\nfig, ax = plt.subplots(figsize = (15,6)) # assigned fig and ax in one line and added size to figure for better visibility\n\n#plot bars for bjp,inc &amp; aap\nbjp = plt.bar(ind,parl_data[\'BJP\'],width/2, label=\'BJP\', color=\'red\')\ninc = plt.bar(ind+width/2,parl_data[\'INC\'],width/2, label=\'INC\', color=\'green\')\naap = plt.bar(ind+width,parl_data[\'AAP\'],width/2, label=\'AAP\', color=\'orange\')\n\n#Make a line plot for Total_Polling\nplt.plot(parl_data[\'Total_Polling\'],\'bo-\',label=\'Total Polling\', color = \'darkred\')\n\ndef anotate_bars(bars):\n    \'\'\'\n    This function helps annotate the bars with data labels in the desired position.\n    \'\'\'\n    for bar in bars:\n        h = bar.get_height()\n        ax.text(bar.get_x()+bar.get_width()/2., 0.75*h, h,ha=\'center\', va=\'bottom\', color = \'white\', fontweight=\'bold\')\n\nanotate_bars(bjp)\nanotate_bars(inc)\nanotate_bars(aap)\n\nfor x,y in parl_data[\'Total_Polling\'].items():\n    ax.text(x, y+4, y,ha=\'center\', va=\'bottom\', color = \'black\', fontweight=\'bold\')\n\n\nax.set_title("\\n\\nParty-wise vote share in Lok Sabha Polls (Delhi) 2019\\n", fontsize=15)\nax.set_ylabel("Parliament Polling Percentage\\n", fontsize=15);\nax.set_xlabel("Election Year\\n", fontsize=15)\nplt.yticks(np.arange(0,100,10))\n\nax.xaxis.set_major_locator(MaxNLocator(4)) # To set the locators correct\nax.set_xticklabels([\'0\',\'2004\',\'2009\',\'2014\',\'2019\']) # To set the labels correct.\n\nplt.tick_params( left = False,bottom = False, labelsize= 13) #removed tick lines\nplt.legend(frameon = False, loc = "best", ncol=4, fontsize = 14) # removed the frame around the legend and spread them along a line\nplt.style.use(\'bmh\') #used the style bmh for better look n feel (my view) you may remove or keep as you like\n\n\nplt.show();\n'
'img = cv2.imread("a400m.png", 1)  # 0 means grayscale\n\nimg = cv2.resize(img, (32, 32)) #give a shape of (32, 32, 3)\n\nimg = np.expand_dims(img, 0) #give a shape of (1, 32, 32, 3), 0 means first dim\n'
'from scipy.stats import skewnorm\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sbs\n\nfig, ax = plt.subplots(1, 3, figsize=(10, 5))\ndata = skewnorm.rvs(size=1000, a=5)\nsbs.distplot(data, ax=ax[0])\nsbs.distplot(data, ax=ax[1])\nax[1].set_xscale("log")\nsbs.distplot(np.log(data), ax=ax[2])\nax[2].set_xticklabels([round(d, 1) for d in np.exp(ax[2].get_xticks())]);\n'
"df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], columns=['dogs', 'cats'])\n\ndf\n   dogs  cats\n0   0.2   0.3\n1   0.0   0.6\n2   0.6   0.0\n3   0.2   0.1\n\ndf.values.reshape(-1,1)\narray([[0.2],\n       [0.3],\n       [0. ],\n       [0.6],\n       [0.6],\n       [0. ],\n       [0.2],\n       [0.1]])\n\n"
"total=[train,test]\nfor i in range(2):\n    total[i].loc[dataset['Fare'] &lt;= 18, 'Fare'] = 9\n    total[i].loc[(dataset['Fare'] &gt; 18) &amp; (dataset['Fare'] &lt;= 37.0042), 'Fare'] = 11\n    total[i].loc[(dataset['Fare'] &gt; 37.0042) &amp; (dataset['Fare'] &lt;= 63.3583), 'Fare'] = 22\n    total[i].loc[(dataset['Fare'] &gt; 63.3583) &amp; (dataset['Fare'] &lt;= 93.5), 'Fare'] = 33\n    total[i].loc[(dataset['Fare'] &gt; 93.5) &amp; (dataset['Fare'] &lt;= 120), 'Fare'] = 44\n    total[i].loc[(dataset['Fare'] &gt; 120) &amp; (dataset['Fare'] &lt;= 164.8667), 'Fare'] = 55\n    total[i].loc[(dataset['Fare'] &gt; 164.8667) &amp; (dataset['Fare'] &lt;= 263), 'Fare'] = 66\n    total[i].loc[dataset['Fare'] &gt; 263, 'Fare'] =77\n# no need for this statement! total=[train,test]\n"
'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\npoly_features = PolynomialFeatures(degree=degree)\nlinreg = LinearRegression()\nX_train_poly = poly_features.fit_transform(X_train)\nlinreg.fit(X_train_poly, y_train)\n\nX_test_poly = poly_features.transform(X_test)\nprint(mean_squared_error(linreg.predict(X_test_poly), y_test))\n'
'(previous_layer_nodes + 1) * (layer_nodes)\n\ndense   : (44+1)*60 = 2700\ndense_1 : (60+1)*55 = 3355\ndense_2 : (55+1)*50 = 2800\ndense_3 : (50+1)*45 = 2295\ndense_4 : (45+1)*30 = 1380\ndense_5 : (30+1)*20 = 620\ndense_6 : (20+1)*1  = 21\n'
"import pandas as pd \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndf  = pd.DataFrame([['how are you','car'],['good mrng have a nice day','bike'],['today is my best working day','cycle'],['hello','bike']], columns = ['text','vehicle']) \n\npreprocess = make_column_transformer((CountVectorizer(), 'text'),(OneHotEncoder(), ['vehicle']))\n"
'import numpy as np\nfrom sklearn.model_selection import KFold, cross_val_score\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\ndef cv_f1(model, X, y):\n  score = np.mean(cross_val_score(model, X, y,\n                                scoring="f1",\n                                cv=kfolds))\n  return (score)\n\n\nmodel = ....\n\nscore_f1 = cv_f1(model, X_train, y_train)\n'
'TitleString ## The corpus\ntitleVector = TfidfVectorizer()\ntitleVectorArray = titleVector.fit_transform(TitleString).toarray()\nArticleString ## The corpus\nArticleVector = TfidfVectorizer()\nArticleVectorArray = ArticleVector.fit_transform(ArticleString).toarray()\nmodel = MultinomialNB()\nmodel.fit(np.concatenate(ArticleVectorArray, titleVectorArray), label_train)\n'
'n_ahead=input("How many values do you want to predict ?");\nn_ahead=int(n_ahead)\n# Making the prediction list \ndef predict_ahead(n_ahead):\n   yhat = []\n   for _ in range(n_ahead):\n   # Making the prediction\n       fc = regressor.predict(X_train)\n       yhat.append(fc)\n\n   # Creating a new input matrix for forecasting\n       X_train = np.append(X_train, fc)\n\n   # Ommitting the first variable\n       X_train = np.delete(X_train, 0)\n\n   # Reshaping for the next iteration\n       X_train = np.reshape(X_train, (1, len(X_train), 1))\n\n   return yhat \np=predict_ahead(n_ahead)\nprint(p)\n'
'predictions = []\nlast_x = (the last x value in your data)\nwhile len(predictions) &lt; #_of_predictions_you_want:\n    p = model.predict(last_x)\n    predictions.append(p)\n    last_x = np.roll(x, -1)\n    last_x[-1] = p\n'
"Dense(1,activation='sigmoid'),])\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nDense(2,activation='softmax'),])\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n"
'from sklearn.datasets import make_regression\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX, y = make_regression(n_samples=2000, n_features=4, n_informative=2,\n                      random_state=42, shuffle=False)\n\nrf = RandomForestRegressor(max_depth=2, random_state=0)\nkf = KFold(n_splits=5)\n\nrf_preds = cross_val_predict(rf, X, y, cv=kf, n_jobs=5) \nprint("RMSE Score using cv preds: {:0.5f}".format(mean_squared_error(y, rf_preds, squared=False)))\n\nscores = cross_val_score(rf, X, y, cv=kf, scoring=\'neg_root_mean_squared_error\', n_jobs=5)\nprint("RMSE Score using cv_score: {:0.5f}".format(scores.mean() * -1))\n\nRMSE Score using cv preds: 15.16839\nRMSE Score using cv_score: 15.16031\n\nRMSE = square_root([(y[1] - y_pred[1])^2 + (y[2] - y_pred[2])^2 + ... + (y[n] - y_pred[n])^2]/n)\n\nRMSE = (RMSE[1] + RMSE[2] + ... + RMSE[k])/k\n\nimport numpy as np\nRMSE__cv_score = []\n\nfor train_index, val_index in kf.split(X):\n    rf.fit(X[train_index], y[train_index])\n    pred = rf.predict(X[val_index])\n    err = mean_squared_error(y[val_index], pred, squared=False)\n    RMSE__cv_score.append(err)\n\nprint("RMSE Score using manual cv_score: {:0.5f}".format(np.mean(RMSE__cv_score)))\n\nRMSE Score using manual cv_score: 15.16031\n'
'km = KMeans(n_clusters=7).fit(dummies)\nclosest, _ = pairwise_distances_argmin_min(km.cluster_centers_, dummies)\nclosest\n'
"def mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred), axis=0)\n\nimport numpy as np\n\n# 2-output data\ny_true = np.array([[0.5, 1], [-1, 1], [7, -6]])\ny_pred = np.array([[0, 2], [-1, 2], [8, -5]])\nmae(y_true, y_pred)\n# array([0.5, 1. ])\n\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(y_true, y_pred, multioutput='raw_values')\n# array([0.5, 1. ])\n"
'cv2.imwrite(os.path.join(mainPath,filename),image)\n'
"df.loc[df[&quot;species&quot;] == &quot;setosa&quot;, &quot;species&quot;] = 0\ndf.loc[df[&quot;species&quot;] == &quot;versicolor&quot;, &quot;species&quot;] = 1\ndf.loc[df[&quot;species&quot;] == &quot;virginica&quot;, &quot;species&quot;] = 2\n\ndf['species'] = df['species'].map({'setosa':0, 'versicolor':1, 'virginica':2})\n"
'from itertools import product\n\ninitstate = 0\nStateSpace = {coord: initstate for coord in product(range(int(GRIDWIDTH)), range(int(GRIDHEIGHT)))}\n'
'model = PCA(n_components=16)\nmodel.fit(X_train)\nX_train = model.transform(X_train)\nX_validation = model.transform(X_validation)\n'
'.result {\nmargin: center\nwidth: 100%;\nwhite-space: pre-line;\nborder: 1px solid #ccc;\n}\n'
"model.add(layers.Dense(64,activation='relu', name='features'))\n\nmodel.get_layer('features').get_weights()[0]\n"
"from sklearn import linear_model\nimport numpy as np\nimport pandas as pd\n\nKitchenQual_df = pd.DataFrame(np.random.normal(0,1,(2000,6)))\nKitchenQual_df.columns = [&quot;OverallQual&quot;, &quot;YearBuilt&quot;, &quot;YearRemodAdd&quot;, &quot;GarageCars&quot;, &quot;GarageArea&quot;,&quot;dummy_KitchenQual&quot;]\n\nKitchenQual_X = KitchenQual_df[[&quot;OverallQual&quot;, &quot;YearBuilt&quot;, &quot;YearRemodAdd&quot;, &quot;GarageCars&quot;, &quot;GarageArea&quot;]]\nKitchenQual_Y = KitchenQual_df[&quot;dummy_KitchenQual&quot;]\n\nregr_KitchenQual = linear_model.LinearRegression()\nregr_KitchenQual.fit(KitchenQual_X, KitchenQual_Y)\n\npred = regr_KitchenQual.predict(KitchenQual_df[[&quot;OverallQual&quot;, &quot;YearBuilt&quot;, &quot;YearRemodAdd&quot;, &quot;GarageCars&quot;, &quot;GarageArea&quot;]].loc[[1555]])\n\n&quot;a&quot; + np.array(['b','c'])\n&quot;a&quot; + np.array([1,2])\n\nUFuncTypeError: ufunc 'add' did not contain a loop with signature matching types (dtype('&lt;U1'), dtype('&lt;U1')) -&gt; dtype('&lt;U1')\n\nprint(&quot;Predicted missing KitchenQual value: &quot; + str(pred[0]))\n\nPredicted missing KitchenQual value: -0.11176904834490986\n"
"# Fake batch dimension required to fit network's input dimension\nimage = loader(image).unsqueeze(0)\n\nplt.imshow(np.squeeze(image))\n\nplt.imshow(image[0])\n"
"model.add(layers.Dense(1, input_dim=width, activation='sigmoid'))\n"
"reg_log = LogisticRegression(solver=&quot;saga&quot;,max_iter=1000)\n\npen = ['l1', 'l2']\nC = [0.1,0.001]\n\nparameters= dict(C=C, penalty=pen)\n\nModel = GridSearchCV(reg_log, parameters, cv=5)\n\nBest_model = Model.fit(X, y)\n\nres = pd.DataFrame(Best_model.cv_results_)\nres[['param_C','param_penalty','mean_test_score']]\n\n    param_C param_penalty   mean_test_score\n0   0.1 l1  0.753333\n1   0.1 l2  0.833333\n2   0.001   l1  0.333333\n3   0.001   l2  0.700000\n"
"from sklearn.tree import DecisionTreeRegressor #&lt;---- FIRST ISSUE\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\ndata = pd.read_csv('housing.csv')\n\ndata = data.dropna() #&lt;--- SECOND ISSUE\n\nprices = data['median_house_value']\nfeatures = data.drop(['median_house_value', 'ocean_proximity'], axis = 1)\n\nX_train, X_test, y_train, y_test = train_test_split(features, prices, test_size=0.2, random_state=42)\n\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)\nscore = accuracy_score(y_test, predictions) #&lt;----- THIRD ISSUE\nscore\n"
"import cv2\nimport numpy as np\nfrom google.colab.patches import cv2_imshow\n\n# Read image and convert to grayscale\nimg = cv2.imread('1.jpg', 0)\n#gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n\ns1, s2 = img.shape\nnew_img = np.zeros([s1,s2],dtype=np.uint8)\nnew_img.fill(255)\n\n\nret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)\ncv2_imshow(thresh1)\n\n\nkernel = np.ones((3,3),np.uint8)\nerosion = cv2.erode(thresh1,kernel,iterations = 1)\ncv2_imshow(erosion)\n\n\n# Get lines with probabilistic hough lines\nfound_lines = cv2.HoughLinesP(erosion, np.pi/180, np.pi/180, 10, minLineLength=4, maxLineGap=4)\n\n# Loop through found lines and draw each line on original image\nfor line in found_lines:\n    x1, y1, x2, y2 = line[0]\n    cv2.line(new_img, (x1, y1), (x2, y2), (0, 0, 255), 1)\n    cv2_imshow(new_img)\n    #cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 1)\n\n# Show image, wait until keypress, close windows.\nprint(&quot;ORIGINAL IMAGE:&quot;)\ncv2_imshow(img)\n#cv2.imshow('image', img)\n#cv2.waitKey(0)\n#cv2.destroyAllWindows()\n"
'clf.fit(x_train, y_train, sample_weight=[c_w[i] for i in all_together])\n'
'FPR=np.array([1.0,0.6437673130193906,0.22105263157894736,0.03102493074792244,0.00221606648199446,0.0])\nTPR=np.array([1.0,0.9407831900668577,0.7172874880611271,0.3638968481375358,0.10315186246418338,0.0])\n'
"metrics=[&quot;accuracy&quot;, keras.metrics.Recall(name='recall')]\n\n# Fit the model\nhistory = model.fit(.....)\n# and then you can see what is available in the history with:\nprint(history.history.keys())\n"
'for epoch in range(n_epochs):\n    batch_losses = []\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.view([batch_size, -1, n_features]).to(device) # &lt;---\n        y_batch = y_batch.to(device)\n        loss = train_step(x_batch, y_batch)\n        batch_losses.append(loss)\n    training_loss = np.mean(batch_losses)\n    train_losses.append(training_loss)    \n    with torch.no_grad():\n        batch_val_losses = []\n        for x_val, y_val in val_loader:\n            x_val = x_val.view([batch_size, -1, n_features]).to(device) # &lt;---\n            y_val = y_val.to(device)        \n            model.eval()\n            yhat = model(x_val)\n            val_loss = criterion(y_val, yhat).item()\n            batch_val_losses.append(val_loss)\n        validation_loss = np.mean(batch_val_losses)\n        val_losses.append(validation_loss)\n    \n    print(f&quot;[{epoch+1}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}&quot;)\n\n[1] Training loss: 0.0235    Validation loss: 0.0173\n[2] Training loss: 0.0149    Validation loss: 0.0086\n[3] Training loss: 0.0083    Validation loss: 0.0074\n[4] Training loss: 0.0079    Validation loss: 0.0069\n[5] Training loss: 0.0076    Validation loss: 0.0069\n\n                          ...\n\n[96] Training loss: 0.0025   Validation loss: 0.0028\n[97] Training loss: 0.0024   Validation loss: 0.0027\n[98] Training loss: 0.0027   Validation loss: 0.0033\n[99] Training loss: 0.0027   Validation loss: 0.0030\n[100] Training loss: 0.0023  Validation loss: 0.0028\n'
'all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\nword_features = all_words.keys()[:2000]\n\ndef getFeatures(text, word_features):\n    text = text.lower()\n    f = {word: word in text for word in word_features}\n    return f\n\n\ndef FindFeaturesForList(MovieList):\n    featureSet = []\n    splitter = re.compile(\'\\\\W*\')\n    all_words = nltk.FreqDist(\n        s.lower()\n        for w in MovieList\n        for s in RemoveStopWords(splitter.split(w[\'imdb\'][\'plot\']))\n        if len(s) &gt; 5 and len(s) &lt; 15)\n    word_features = all_words.keys()[:2000]\n    for w in MovieList:\n        print w[\'imdb\'][\'title\']\n        try:\n            for genre in w[\'imdb\'][\'genres\']:\n                featureSet.append(\n                    (getFeatures(w[\'imdb\'][\'plot\'], word_features), genre))\n        except:\n            print "Error when retriving genre, skipping element"\n\n    return featureSet\n'
"g = f.transform([['as'],['bs']])\n\n&gt;&gt;&gt; g.nonzero()\n(array([0, 1], dtype=int32), array([494108, 335425], dtype=int32))\n"
'file=open(\'corpus.txt\',\'r\')\ndata=file.readlines()\nfile.close()\n\ndiccionario = {}\n\nfor linea in data:\n    linea.decode(\'latin_1\').encode(\'UTF-8\') # para los acentos\n    palabra_tag = linea.split(\'\\n\')\n    cadena = str(palabra_tag[0])\n    if(diccionario.has_key(cadena)):\n        aux = diccionario.get(cadena)\n        aux += 1\n        diccionario.update({cadena:aux})\n    else:\n        diccionario.update({cadena:1})\n\noutfile = open(\'lexic.txt\', \'w\')\noutfile.write(\'Palabra\\tTag\\tApariciones\\n\')\n\nfor key, value in diccionario.iteritems() :\n    s = str(value)\n    outfile.write(key +" "+s+\'\\n\')\noutfile.close()\n'
'from keras.utils.np_utils import to_categorical\n\nlabels_train = to_categorical(labels_train)\n'
'import numpy as np\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nX = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5,5], [6,6], [7,7], [8,8], [9,9], [10,10]])\ny = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\nsss = StratifiedShuffleSplit(y, 1, test_size=0.2, random_state=0)\n\nfor train_index, test_index in sss:\n    print("TRAIN:", train_index, "TEST:", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n&gt;&gt;&gt; TRAIN: [0 6 3 9 2 5 1 7] TEST: [4 8]\n'
'sss = StratifiedShuffleSplit(n_iter=1, test_size=0.2)\ntrain_idx, test_idx = next(sss.split(dataset.data, dataset.target))\n'
'X_poly            = polyFeatures(X, p)\nX_norm, mu, sigma = featureNormalize(X_poly)\nX_poly            = c_[ones((m, 1)), X_poly]\n\nX_poly            = polyFeatures(X, p)\nX_poly, mu, sigma = featureNormalize(X_poly)\nX_poly            = c_[ones((m, 1)), X_poly]\n\ncost, theta = trainLinearReg(X_poly, y, lamda)\n'
"def merge_feature_vector(traffic):\n    result = []\n    for id, data in enumerate(traffic):\n        result.extend(json.loads(data['http_body%s' % id]))\n    return result\n\ndef encode_traffic(traffic):\n    result = {}\n    for id, data in enumerate(traffic):\n        result['html_body%s' % id] = data['html_body%s' % id]\n    return result\n...\nfeatures = [encode_traffic(traffic) for traffic in train]\n\nh = FeatureHasher(n_features=10)\nfeatures = h.fit_transform(features)\nfeatures.toarray()\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.train(features)\n"
'import numpy as np\n\ngen = np.random.RandomState(0)\n\nn_total = 1000\nn_train = 800\n\ntrain_idx = gen.choice(n_total, size=n_train)\ntest_idx = np.setdiff1d(np.arange(n_total), train_idx)\n\nprint(test_idx.size)\n# 439\n\nn_test = 200\ntest_idx2 = gen.choice(test_idx, size=n_test)\n\nidx = gen.permutation(n_total)\ntrain_idx, test_idx = idx[:n_train], idx[n_train:]\n'
'xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))\ncoord1, coord2 = xx1.ravel(), xx2.ravel()\n\ncoord1, coord2 = zip(*itertools.product(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)))\n\n&gt;&gt;&gt; xx1, xx2 = np.meshgrid(np.arange(3), np.arange(2))\n&gt;&gt;&gt; coord1, coord2 = xx1.ravel(), xx2.ravel()\n&gt;&gt;&gt; coord1\narray([0, 1, 2, 0, 1, 2])\n&gt;&gt;&gt; coord2\narray([0, 0, 0, 1, 1, 1])\n&gt;&gt;&gt; coord1, coord2 = zip(*itertools.product(np.arange(3), np.arange(2)))\n&gt;&gt;&gt; coord1\n(0, 0, 1, 1, 2, 2)\n&gt;&gt;&gt; coord2\n(0, 1, 0, 1, 0, 1)\n'
'# Construct model\npred = multilayer_perceptron(x, weights, biases)\n\n# Construct model\nmodel pred = tf.nn.sigmoid(multilayer_perceptron(x, weights, biases))\n'
'def ask(self, seed): \n\nif not seed:\n    seed = self.getInitial()\n\ndef ask(self, seed=False):\n'
'Slope:\n [ 0.69314718]\nIntercept:\n 4.4408920985e-16\n\nIn [17]: np.exp(0.69314718*1.1 + 4.4408920985e-16)\nOut[17]: 2.1435469237522917\n'
"BATCH_SIZE = ...\nfor step in range(201):\n    # N.B. You'll need extra code to handle the cases where start_index and/or end_index\n    # wrap around the end of x_data and y_data.\n    start_index = step * BATCH_SIZE\n    end_index = (step + 1) * BATCH_SIZE\n    start_index = sess.run(train, {xs: x_data[start_index:end_index,:],\n                                   ys: y_data[start_index:end_index,:]})\n"
'import numpy as np\nfrom sklearn import decomposition\nnp.random.seed(0)\ndata = np.random.randn(100, 100)\nmdl = decomposition.PCA(n_components=5)\nmdl_fit = mdl.fit(data)\ndata_transformed = mdl_fit.transform(data)\ndata_transformed_manual = np.dot(data - mdl_fit.mean_, mdl.components_.T)\n\nnp.all(data_transformed == data_transformed_manual)\n\nTrue\n'
'import numpy as np\n\ndef sortRadially(a):\n    X, Y = np.indices(a.shape, dtype="float")\n    c = int(a.shape[0]/2)\n    d = np.sqrt((c-X)**2 + (c-Y)**2)\n    fd = d.flatten()\n    fX = X.flatten()\n    fY = Y.flatten()\n    argD = fd.argsort()\n    nX = fX[argD].astype(int)\n    nY = fY[argD].astype(int)\n    fa = a.flatten()\n    sa = a.copy()\n    fa.sort()\n    for i in range(nX.shape[0]):\n        a[nX[i], nY[i]] = fa[i]\n    return a\n\na = np.array([[ 2,  2,  1,],\n              [ 3,  3,  3,],\n              [ 2,  3,  2,]])\n\nmyown = np.random.randint(0, 100, (9, 9))\n\nprint("Your test:")\nprint(sortRadially(a))\nprint("")\nprint("My test:")\nprint(sortRadially(myown))\n\nYour test:\n[[3 2 3]\n [2 1 2]\n [3 2 3]]\n\nMy test:\n[[97 95 91 78 60 73 84 92 98]\n [93 78 55 44 30 34 55 80 92]\n [84 45 22 15 10 17 23 44 88]\n [71 42 12  7  2  7 16 42 73]\n [69 28 10  2  0  1 10 28 66]\n [72 44 13  5  1  3 14 38 77]\n [87 49 19 14  8 19 24 52 91]\n [95 83 48 43 33 39 52 79 94]\n [98 94 83 73 67 71 91 96 99]]\n'
' boots = bootstrap(stat_data, func=estimator,\n                                      n_boot=n_boot,\n                                      units=unit_data)\n confint.append(utils.ci(boots, ci))\n'
"import csv\nimport tensorflow as tf\nimport numpy as np\n\nwith open('train.csv', 'rt') as f:\n    reader = csv.reader(f)\n    your_list = list(reader)\n\ndef toFloatNoFail( data ) :\n    try :\n        return float(data)\n    except :\n        return 0\n\ndata = [ [ toFloatNoFail(x) for x in row ] for row in your_list[1:] ]\ndata = np.array( data ).astype( float )\nx_train = data[:,:-1]\nprint x_train.shape\ny_train = data[:,-1:]\nprint y_train.shape\n\n\nnum_features = np.shape(x_train)[1]\n\n# Input\ntf_train_dataset = tf.constant(x_train, dtype=tf.float32)\ntf_train_labels = tf.constant(y_train, dtype=tf.float32)\n\n# Variables\nweights = tf.Variable(tf.truncated_normal( [num_features, 1] , dtype=tf.float32))\nbiases = tf.Variable(tf.constant(0.0, dtype=tf.float32 ))\n\ntrain_prediction = tf.matmul(tf_train_dataset, weights) + biases\n\nloss = tf.reduce_mean( tf.square( tf.log(tf_train_labels) - tf.log(train_prediction) ))\n\noptimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\nnum_steps = 10001\n\ndef accuracy(prediction, labels):\n    return ((prediction - labels) ** 2).mean(axis=None)\n\n\nwith tf.Session() as session:\n    tf.global_variables_initializer().run()\n    print('Initialized')\n    for step in range(num_steps):\n        _, l, predictions = session.run([optimizer, loss, train_prediction])\n        if (step % 1000 == 0):\n            print('Loss at step %d: %f' % (step, l))\n\nloss = tf.reduce_mean( tf.square( tf.log(tf_train_labels) - tf.log(train_prediction) ))\n"
'import nltk\nsentence = "I want to check if a sentence has a specific parts of speech tag structure."\ntagged = nltk.pos_tag(nltk.word_tokenize(sentence))\ngrammar = r"""\nNP: \n{&lt;NNS&gt;&lt;IN&gt;&lt;NN&gt;&lt;NN&gt;&lt;NN&gt;}\n{&lt;PRP&gt;&lt;VBP&gt;}\n"""\n\ncp = nltk.RegexpParser(grammar)\nresult = cp.parse(tagged)\nprint result\n\n(S\n  (NP I/PRP want/VBP)\n  to/TO\n  check/VB\n  if/IN\n  a/DT\n  sentence/NN\n  has/VBZ\n  a/DT\n  specific/JJ\n  (NP parts/NNS of/IN speech/NN tag/NN structure/NN)\n  ./.)\n'
'# Declared outside of session\ntrain = optimizer.minimize(eval(func_seq))\n\n...\n\n# Declared in session\nsess.run(train)\nprint("Generation", x, ": ", sess.run(eval(func_seq)))\n'
"random_search = RandomizedSearchCV(LogReg, n_jobs=3, param_distributions = params_grid, n_iter = 3, cv = logo, scoring = 'accuracy')\n\nLogisticRegression(n_jobs=3)\n"
'import argparse\nparser  = argparser.ArgumentParser()\n\n# Input Arguments\nparser.add_argument(\n      \'--eval-file\',\n      help=\'local paths to evaluation data\',\n      nargs=\'+\',\n      required=True\n  )\nargs = parser.parse_args()\n\nimport json\njson.loads(Data) \n\nx_raw = ["key", "Value"] \nx_test = np.array(list(vocab_processor.transform(x_raw)))\n'
'print("decision id node %s : (For sample number [%s, %s] (= %s) %s %s)"\n      % (node_id,\n         sample_id,\n         X_train.columns[feature[node_id]],\n         new_X[sample_id, feature[node_id]],\n         threshold_sign,\n         threshold[node_id]))\n'
'ssl_verify: C:\\\\conda_config\\\\certificates\\\\ca_cert.cer\n'
'from sklearn import datasets\nfrom sklearn import tree\nfrom sklearn.model_selection import StratifiedKFold\n\n# load the digits dataset\ndigits = datasets.load_digits()\n\n# separate features and labels\nX_digits = digits.data\ny_digits = digits.target\n\n# split data into training and testing sets\nk_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\nfor train_index, test_index in k_fold.split(X_digits, y_digits):\n        train_features, test_features = X_digits[train_index], X_digits[test_index]\n        train_labels, test_labels = y_digits[train_index], y_digits[test_index]\n\n# fit to model\nclf = tree.DecisionTreeClassifier()\nclf = clf.fit(train_features, train_labels)\n\n# predict on the testing features\nprint(clf.predict(test_features))\n'
"from sklearn.preprocessing import OneHotEncoder\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame([[1,2],[3,0],[2,3]], columns=['colA', 'colB'])\nprint(df)\n\nn_values = df.max().values + 1\nenc = OneHotEncoder(sparse=False, n_values=n_values, dtype=int)\nenc.fit(df) \n\nencoded_columns = [\n    '{}_{}'.format(col_name, i)\n    for col_name, n_value in zip(df.columns, n_values)\n    for i in range(n_value)\n]\n\none_hot = enc.transform(df)\nrank_hot = np.zeros_like(one_hot)\n\nfor col_start, col_end in zip(enc.feature_indices_[:-1], enc.feature_indices_[1:]):\n    one_hot_col_reversed = one_hot[:, col_start: col_end][:, ::-1]\n    rank_hot[:, col_start: col_end] = np.cumsum(one_hot_col_reversed, axis=1)[:, ::-1]\n\nencoded_df = pd.DataFrame(rank_hot, columns=encoded_columns)\n\nprint(encoded_df)\n&gt;&gt;    colA_0  colA_1  colA_2  colA_3  colB_0  colB_1  colB_2  colB_3\n0       1       1       0       0       1       1       1       0\n1       1       1       1       1       1       0       0       0\n2       1       1       1       0       1       1       1       1\n"
"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer\n\nmy_scoring_func = make_scorer(f1_score)\nparameters = {'n_estimators':[10,30,50,80], 'max_features':[0.1, 0.2, 0.3,0.4], 'contamination' : [0.1, 0.2, 0.3]}\niso_for =  IsolationForest(max_samples='auto')\nclf = GridSearchCV(iso_for, parameters,  scoring=my_scoring_func)\n"
'siamese_network.compile(optimizer=Adam(**lr=.0001**), \n                        loss=identity_loss)\n\nsiamese_network.compile(optimizer=Adam(**lr=.00004**, **clipnorm=1.**),\n                        loss=identity_loss)\n'
'from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier()\nclf.fit(X_train,y_train)\nclf.predict(X_test)\n'
'import numpy as np\nimport matplotlib.pyplot as plt\n\nx = [0.2, 1.3, 2.1, 2.9, 3.3]\ny = [3.3, 3.9, 4.8, 5.5, 6.9]\n\n(m, b) = np.polyfit(x, y, 1)\nprint(m, b)\n\nyp = np.polyval([m, b], x)\nplt.plot(x, yp)\nplt.grid(True)\nplt.scatter(x,y)\n'
'import tensorflow as tf\n#Process 1  repeat the tensor in 2D.\n#e.g. [1,2,3,4]  --&gt; [[1,1,2,2],[1,1,2,2],[3,3,4,4],[3,3,4,4]]\n\n# assuming  m x n idenity matrix e.g. [1,2,3],[4,5,6]] , m=2,n=3\nid_matrix_size=2  # size of identity matrix (e.g. 2x2 3x3 ...)\nm=2\nn=3\nmytensor=tf.constant([[1,2,3],[4,5,6] ],dtype = tf.float32)\n\n# similar to np.repeat in x-dimension.\nflatten_mytensor=tf.reshape(mytensor,[-1,1])  \na=tf.tile(flatten_mytensor,  [1, id_matrix_size])\nb=tf.reshape( a, [m,-1])\n\n# tile in y-dimension\nc=tf.tile(b,[1,id_matrix_size])\nd=tf.reshape(c,[id_matrix_size*m,id_matrix_size*n])\n\n#Process 2  tile identity matrix in 2D. \nidentity_matrix=tf.eye(id_matrix_size) # identity matrix \nidentity_matrix_2D= tf.tile(identity_matrix,[m,n])\n\n#Process 3  elementwise multiply\noutput = tf.multiply(d,identity_matrix_2D )\n\nwith tf.Session() as sess:\n    print(sess.run(output) )\n#output :\n#[[1. 0. 2. 0. 3. 0.]\n# [0. 1. 0. 2. 0. 3.]\n# [4. 0. 5. 0. 6. 0.]\n# [0. 4. 0. 5. 0. 6.]]\n\ndef Expand_tensor(mytensor,id_matrix_size):\n    m=mytensor.shape[0]    \n    n=mytensor.shape[1]\n    # similar to np.repeat in x-dimension.\n    flatten_mytensor=tf.reshape(mytensor,[-1,1])  \n    a=tf.tile(flatten_mytensor,  [1, id_matrix_size])\n    b=tf.reshape( a, [m,-1])\n\n    # tile in y-dimension\n    c=tf.tile(b,[1,id_matrix_size])\n    d=tf.reshape(c,[id_matrix_size*m,id_matrix_size*n])\n\n\n    # tile identity matrix in 2D. \n    identity_matrix=tf.eye(id_matrix_size) # identity matrix \n    identity_matrix_2D= tf.tile(identity_matrix,[m,n])\n\n    #elementwise multiply\n    output = tf.multiply(d,identity_matrix_2D )\n    return output\n\nmytensor=tf.constant([[1,2,3],[4,5,6] ],dtype = tf.float32)\nwith tf.Session() as sess:\n    print(sess.run(Expand_tensor(mytensor,2)) )\n'
'le_hair = preprocessing.LabelEncoder()\nle_beard = preprocessing.LabelEncoder()                      \nle_scarf = preprocessing.LabelEncoder()                                            \ndframe["hair"] = le_hair.fit_transform(dframe["hair"])          \ndframe["beard"] = le_beard.fit_transform(dframe["beard"])         \ndframe["scarf"] = le_scarf.fit_transform(dframe["scarf"])  \n'
'for i in range(TRAINING_ITERATIONS):\n    with tf.GradientTape() as tape:\n        COST = tf.reduce_sum(LABELS - thanouseEyes(init))\n    GRADIENTS = tape.gradient(COST, VARIABLES)\n    optimizer.apply_gradients(zip(GRADIENTS, VARIABLES))\n'
'print(len(Y))\nprint(len(np.unique(Y)))\n\n621\n593\n\nX = 5 * list(X)\nY = 5 * list(Y)\n\nLR: 0.015700 (0.000403)\nKNN: 0.028583 (0.000403)\nCART: 0.018519 (0.001610)\nNB: 0.018519 (0.001610)\nSVM: 0.010870 (0.000403)\n\n0100000000 256\n0100000000 675\n0100000000 912\n\n0100000000 112\n0100000000 745\n0100000000 312\n\n0100000000\n\n{256: 0.333, 675: 0.333, 912: 0.333}\n\n0100000000 112 at this label: 0.00\n0100000000 745 at this label: 0.00\n0100000000 312 at this label: 0.00\n'
"&gt;&gt;&gt; inpu.shape\n(3,3)  # three samples of size 3\n\ninp = layers.Input((3,3))  # don't forget to set the correct input shape\nx = Flatten()(inp)\n# pass x to other Dense layers\n"
'mean = train_X_2.mean(axis=0)\ntrain_X_2 -= mean\nstd = train_X_2.std(axis=0)\ntrain_X_2 /= std\n'
'tf.keras.layers.Dense(4, activation=tf.nn.softmax)\n\nlabelids = {"john_": 0, "erin_": 1, "scott_": 2, "colin_": 3}\n'
'X_train, X_train, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n\nfrom sklearn.datasets import fetch_openml\nX, y = fetch_openml("mnist_784", version=1, return_X_y=True)\n'
'softmax_output = np.array(  # dummy values, not softmax\n    [[1, 2, 3], \n     [4, 5, 6],\n     [7, 8, 9],\n     [10, 11, 12]]\n)\nnum_train = 4  # length of the array\ny = [2, 1, 0, 2]  # a labels; values for indexing along the second axis\nsoftmax_output[range(num_train), list(y)]\nOut:\n[3, 5, 7, 12]\n'
'x_train = (x_train-x_train.min())/(x_train.max()-x_train.min())\nx_test = (x_test-x_test.min())/(x_test.max()-x_test.min())\n'
'arr = [cv2.imread( join(mypath,onlyfiles[n])).ravel() for n in range(0, len(onlyfiles))]\nX = np.vstack[arr]\ntsne = TSNE(n_components=2).fit_transform(X)\n'
"&gt;&gt;&gt; df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],'C': [1, 2, 3]})\n   A  B  C\n0  a  b  1\n1  b  a  2\n2  a  c  3\n\n&gt;&gt;&gt; pd.get_dummies(df, drop_first=True)\n   C  A_b  B_b  B_c\n0  1    0    1    0\n1  2    1    0    0\n2  3    0    0    1\n\n"
'from keras.callbacks import EarlyStopping\n\nclass DelayedEarlyStopping(EarlyStopping):\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch &gt;= 100:\n            super().on_epoch_end(epoch, logs=logs)\n'
'return self.get_data()\n'
"df['ID_int'] = df['id'].rank(method='dense').astype(int)\ndf['ID_int2'] = pd.factorize(df['id'])[0]\n\n  id  ID_int  ID_int2\n0  a       2        0\n1  b       3        1\n2  c       4        2\n3  a       2        0\n4  b       3        1\n5  c       4        2\n6  A       1        3\n7  b       3        1\n"
"classifier.add(Dense(units = 14, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\ny_pred = np.argmax(classifier.predict(X_test), axis=1)\n\ny_pred = classifier.predict_classes(X_test)\n"
'Ndf_class_0_records = trainData[trainData[\'DIED\'] == 0]\nNdf_class_1_records = trainData[trainData[\'DIED\'] == 1]\nNdf_class_0_record_counts = len(Ndf_class_0_records) ##### CHANGED THIS\nNdf_class_1_record_counts = len(Ndf_class_1_records) ##### CHANGED THIS\nX_smote = trainData.drop("DIED", axis=1)\ny_smote = trainData["DIED"]\nsmt = SMOTE(ratio={0:Ndf_class_0_record_counts, 1:Ndf_class_1_record_counts*2})\nX_smote_res, y_smote_res = smt.fit_sample(X_smote, y_smote)\n'
"img = imageio.imread(i)\nimg_aug = seq(images=img)\niaa.imshow(img_aug)\nprint(img_aug)\n\nfrom PIL import Image\nim = Image.fromarray(img_aug)\nim.save('img_aug.png')`\n"
"export_graphviz(clf_fit.best_estimator_, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\n"
'# check core SDK version number\nprint("Azure ML SDK Version: ", azureml.core.VERSION)\n'
"import dataframe as df\ndf = pd.DataFrame({'x': your_x_numpy_array, 'y': your_y_numpy_array})\ndf.sort_values(by=[your_index_of_choice],inplace = True)\n"
'ep = 120\ndf_split = np.array_split(df, 10)\ntest_part = 0\nacc = []\nf1 = []\nprec = []\nrecalls = []\nwhile test_part &lt; 10:\n    model = build_model()\n    train_x = []\n    train_y = []\n    test_x = []\n    test_y = []\n    print("CV Fold, with test partition i = " , test_part)\n\n    for i in range(10):\n        #on first iter that isnt a test part then set the train set to this \n        if len(train_x) == 0 and not i == test_part:\n            train_x = df_split[i][[\'start-sin\', \'start-cos\', \'start-sin-lag\', \'start-cos-lag\', \'prev-close-sin\', \'prev-close-cos\', \'prev-length\', \'state-lag\', \'monday\', \'tuesday\', \'wednesday\', \'thursday\', \'friday\', \'saturday\', \'sunday\']]\n            train_y = df_split[i][[\'wait-categ-none\', \'wait-categ-short\', \'wait-categ-medium\', \'wait-categ-long\']]\n            #terminate immediately\n            continue\n        #if current is not a test partition then concat with previous version\n        if not i == test_part:\n            train_x = pd.concat([train_x, df_split[i][[\'start-sin\', \'start-cos\', \'start-sin-lag\', \'start-cos-lag\', \'prev-close-sin\', \'prev-close-cos\', \'prev-length\', \'state-lag\', \'monday\', \'tuesday\', \'wednesday\', \'thursday\', \'friday\', \'saturday\', \'sunday\']]], axis=0)\n            train_y = pd.concat([train_y, df_split[i][[\'wait-categ-none\', \'wait-categ-short\', \'wait-categ-medium\', \'wait-categ-long\']]], axis=0)\n\n        #set this to test partition\n        else:\n            test_x = df_split[i][[\'start-sin\', \'start-cos\', \'start-sin-lag\', \'start-cos-lag\', \'prev-close-sin\', \'prev-close-cos\', \'prev-length\', \'state-lag\', \'monday\', \'tuesday\', \'wednesday\', \'thursday\', \'friday\', \'saturday\', \'sunday\']]\n            test_y = df_split[i][[\'wait-categ-none\', \'wait-categ-short\', \'wait-categ-medium\', \'wait-categ-long\']]\n    #enforce\n    train_y = train_y.replace(False, 0)\n    train_y = train_y.replace(True, 1)\n    test_y = test_y.replace(False, 0)\n    test_y = test_y.replace(True, 1)\n    #fit\n    model.fit(train_x, train_y, epochs=ep, verbose=1)\n    pred = model.predict(test_x)\n    #score\n    loss, accuracy, f1_score, precision, recall = model.evaluate(test_x, test_y, verbose=0)\n    #save\n    acc.append(accuracy)\n    f1.append(f1_score)\n    prec.append(precision)\n    recalls.append(recall)\n    test_part += 1\nprint("CV finished.\\n")\n\nprint("Mean Accuracy")\nprint(sum(acc)/len(acc))\nprint("Mean F1 score")\nprint(sum(f1)/len(f1))\nprint("Mean Precision")\nprint(sum(prec)/len(prec))\nprint("Mean Recall rate")\nprint(sum(recalls)/len(recalls))\n'
'L = Feedforward(2,10)\nf = Feedforward(3,9)\nL_opt = Adam(L.parameters(), lr=...)\nf_opt = Adam(f.parameters(), lr=...)\nfor (x,t,y) in dataset:\n    L.zero_grad()\n    f.zero_grad()\n    y_pred = L(t)*f(x)\n    loss = (y-y_pred)**2\n    loss.backward()\n    L_opt.step()\n    f_opt.step()\n\nclass ProductModel(t.nn.Module):\n    def __init__(self, L, f):\n        self.L = L\n        self.f = f\n    def forward(self, x,t):\n        return self.L(t)*self.f(x)\n'
'input_shape=(4,)\n'
'model.add(Concatenate([char_model, word_model]))\n'
"idg1 = ImageDataGenerator(**idg1_configs)\nidg2 = ImageDataGenerator(**idg2_configs)\n\ng1 = idg1.flow_from_directory('idg1_dir',...)\ng2 = idg2.flow_from_directory('idg2_dir',...)\n\ndef combine_gen(*gens):\n    while True:\n        for g in gens:\n            yield next(g)\n\n# ...\nmodel.fit_generator(combine_gen(g1, g2), steps_per_epoch=len(g1)+len(g2), ...)\n\nclass CombinedGen():\n    def __init__(self, *gens):\n        self.gens = gens\n\n    def generate(self):\n        while True:\n            for g in self.gens:\n                yield next(g)\n\n    def __len__(self):\n        return sum([len(g) for g in self.gens])\n\n# usage:\ncg = CombinedGen(g1, g2)\nmodel.fit_generator(cg.generate(), ...) # no need to set `steps_per_epoch`\n"
"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras import datasets, layers, models\nfrom keras.models import Model, Input, Sequential\nimport matplotlib.pyplot as plt\n\n\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images / 255.0, test_images / 255.0\ntrain_labels, test_labels = tf.keras.utils.to_categorical(train_labels, 10) , tf.keras.utils.to_categorical(test_labels, 10)\ninput_shape = train_images[0,:,:,:].shape\n\n\ninput = layers.Input(shape=input_shape)\n\nx = layers.Conv2D(32, (3, 3), activation='relu',padding='valid')(input)\nx = layers.MaxPooling2D((2,2))(x)\nx = layers.Conv2D(64, (3, 3), activation='relu')(x)\nx = layers.MaxPooling2D((2,2))(x)\nx = layers.Conv2D(64, (3, 3), activation='relu')(x)\n\nx = layers.Flatten()(x)\nx = layers.Dense(64, activation='relu')(x)\nx = layers.Dense(10, activation='softmax')(x)\n\nmodel = Model(input, x, name='Functional')\n\nmodel.summary()\n\nmodel.compile(optimizer='adam',\n              loss=loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=['accuracy'])\n\nhistory = model.fit(train_images, train_labels, epochs=10, \n                    validation_data=(test_images, test_labels))\n\nconv2d_16 (Conv2D)           (None, 30, 30, 32)        896       \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 13, 13, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_18 (Conv2D)           (None, 4, 4, 64)          36928     \n_________________________________________________________________\nflatten_6 (Flatten)          (None, 1024)              0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 64)                65600     \n_________________________________________________________________\ndense_12 (Dense)             (None, 10)                650       \n=================================================================\nTotal params: 122,570\nTrainable params: 122,570\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 50000 samples, validate on 10000 samples\nEpoch 1/10\n50000/50000 [==============================] - 15s 305us/step - loss: 1.4870 - accuracy: 0.4600 - val_loss: 1.2874 - val_accuracy: 0.5488\nEpoch 2/10\n50000/50000 [==============================] - 15s 301us/step - loss: 1.1365 - accuracy: 0.5989 - val_loss: 1.0789 - val_accuracy: 0.6191\nEpoch 3/10\n50000/50000 [==============================] - 15s 301us/step - loss: 0.9869 - accuracy: 0.6547 - val_loss: 0.9506 - val_accuracy: 0.6700\nEpoch 4/10\n50000/50000 [==============================] - 15s 301us/step - loss: 0.8896 - accuracy: 0.6907 - val_loss: 0.9509 - val_accuracy: 0.6695\nEpoch 5/10\n50000/50000 [==============================] - 16s 311us/step - loss: 0.8135 - accuracy: 0.7151 - val_loss: 0.8688 - val_accuracy: 0.7046\nEpoch 6/10\n50000/50000 [==============================] - 15s 303us/step - loss: 0.7566 - accuracy: 0.7351 - val_loss: 0.8411 - val_accuracy: 0.7141\n"
"generator = datagen.flow_from_directory(directory,target_size=(224, 224),batch_size=batch_size,class_mode='sparse')\n\nlabels = np.zeros(shape=(sample_count),2)\n"
'import tensorflow as tf\nimport numpy as np\n\nindices_tf = tf.placeholder(shape=(None,None),dtype=tf.int32)\nweights_tf = tf.placeholder(shape=(None,None),dtype=tf.float32)\n\n# The returned index counts from 0\nresult = tf.bincount(indices_tf,weights_tf)\n\nindices_data = np.array([1, 2, 3, 4, 5, 3])\nweights_data = np.array([0, 1, 0.5, 0.1, 0.6, 0.5])\n\nwith tf.Session() as sess:\n    print(sess.run(result, feed_dict={indices_tf:[indices_data],weights_tf:[weights_data]}))\n    print(sess.run(result, feed_dict={indices_tf: [indices_data]*2, weights_tf: [weights_data]*2}))\n\n# print\n[0.  0.  1.  1.  0.1 0.6]\n[0.  0.  2.  2.  0.2 1.2]\n'
'import numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, Conv2D, Reshape, TimeDistributed, Flatten, Conv1D,ConvLSTM2D, MaxPooling1D\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\n\nsess = tf.Session(config=config)\ndef create_dataset(signal_data, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(signal_data) - look_back):\n        dataX.append(signal_data[i:(i + look_back), :])\n        dataY.append(signal_data[i + look_back, -1])\n    return np.array(dataX), np.array(dataY)\n\n\nlook_back = 20\n\n\n\ndf = pd.read_csv(\'kospi.csv\')\nsignal_data = df[["Open", "Low", "High", "Volume", "Close"]].values.astype(\'float32\')\n\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nsignal_data = scaler.fit_transform(signal_data)\n\n\n\ntrain_size = int(len(signal_data) * 0.80)\ntest_size = len(signal_data) - train_size - int(len(signal_data) * 0.05)\nval_size = len(signal_data) - train_size - test_size\ntrain = signal_data[0:train_size]\nval = signal_data[train_size:train_size+val_size]\ntest = signal_data[train_size+val_size:len(signal_data)]\n\n\n\nx_train, y_train = create_dataset(train, look_back)\nx_val, y_val = create_dataset(val, look_back)\nx_test, y_test = create_dataset(test, look_back)\n\n\n\n\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(None, 5),return_sequences=True))\nmodel.add(Dropout(0.3))\n\nmodel.add(LSTM(128, input_shape=(None, 5)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(128))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(1))\n\n\n\n\nmodel.compile(loss=\'mean_squared_error\', optimizer=\'adam\', metrics=[\'accuracy\'])\n\n\nmodel.summary()\nhist = model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=2, validation_data=(x_val, y_val))\n\ntrainScore = model.evaluate(x_train, y_train, verbose=0)\nmodel.reset_states()\nprint(\'Train Score: \', trainScore)\nvalScore = model.evaluate(x_val, y_val, verbose=0)\nmodel.reset_states()\nprint(\'Validataion Score: \', valScore)\ntestScore = model.evaluate(x_test, y_test, verbose=0)\nmodel.reset_states()\nprint(\'Test Score: \', testScore)\n\n\n\np = model.predict(x_test)\n\n\nprint(mean_squared_error(y_test, p))\n\nimport matplotlib.pyplot as pplt\n\npplt.plot(y_test)\npplt.plot(p)\npplt.legend([\'testY\', \'p\'], loc=\'upper right\')\npplt.show()\n'
'import torch.nn as nn\nprojection = nn.Linear(2, 1, bias=True)\n'
'from pyspark.ml.feature import Tokenizer\n\ndf = spark.createDataFrame([\n    (0, \'Hello and good day\'),\n    (1, \'This is a simple demostration\'),\n    (2, \'Natural and unnatural language processing\')\n    ], [\'id\', \'sentence\'])\n\ndf.show(truncate=False)\n# +---+-----------------------------------------+\n# |id |sentence                                 |\n# +---+-----------------------------------------+\n# |0  |Hello and good day                       |\n# |1  |This is a simple demostration            |\n# |2  |Natural and unnatural language processing|\n# +---+-----------------------------------------+\n\ntokenizer = Tokenizer(inputCol="sentence", outputCol="words")\ntokenized = tokenizer.transform(df)\n\ntokenized.select(\'words\').show(truncate=False)\n# +-----------------------------------------------+\n# |words                                          |\n# +-----------------------------------------------+\n# |[hello, and, good, day]                        |\n# |[this, is, a, simple, demostration]            |\n# |[natural, and, unnatural, language, processing]|\n# +-----------------------------------------------+\n'
'x_train = x_train[y_train[:, 0] &lt; 5]\n'
'model.fit(train_inputs.toarray().astype(float), ...)\n'
'tf.nn.softmax_cross_entropy_with_logits(target, prediction)\n'
'   Other Features    Winner     Loser\n0               2   John D.  Jason S.\n1               4  Jason S.   Eric N.\n\n   Other Features  win Jason S.  win John D.  los Eric N.  los Jason S.\n0               2             0            1            0             1\n1               4             1            0            1             0\n'
"param_grid = {'base_estimator__svc__C': np.linspace(0.1, 2, 20),\n    'base_estimator__svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n"
'test_image = test_image[np.newaxis, ...]\n'
"df.romantic = (df.romantic == 'yes').astype(int)\ndf.internet = (df.internet == 'yes').astype(int)\n"
'$ heroku buildpacks:add heroku/jvm\n'
"import time\n\nstart_time = time.time()\nclassifier.fit(X_train, y_train)\nelapsed_time = time.time() - start_time\nprint(f'{elapsed_time:.2f}s elapsed during training')\n\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([('tree', DecisionTreeClassifier())], verbose=3)\npipe.set_params(tree__random_state=17).fit(X_train, y_train)\n"
'import numpy as np\nX_train = np.concatenate((X_train, X_train2), axis = -1)\nprint(X_train.shape)\n\n\nX_train = np.concatenate([X_train, X_train], axis = -1)\nX_val = np.concatenate([X_val, X_val], axis = -1) # you missed this\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_val.shape)\nprint(Y_val.shape)\n\nhistory = model.fit(X_train, Y_train, validation_data=(X_val,Y_val), batch_size=5,epochs = 2,shuffle=True) #,callbacks = callbacks\n'
'encoder = preprocessing.LabelEncoder()\ntrain_y = encoder.fit_transform(train_y)\nvalid_y = encoder.fit_transform(valid_y)\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\ny_train = le.fit_transform(["class1", "class2", "class3"])\ny_valid = le.fit_transform(["class2", "class3"])\nprint(y_train)\nprint(y_valid)\n\n[0 1 2]\n[0 1]\n\nresult = result[:1000] #shorten df - was :600\n\n# Encode the labels before splitting\nencoder = preprocessing.LabelEncoder()\ny_encoded = encoder.fit_transform(result[\'type\'])\n\n# CARE that I changed the target from result[\'type\'] to y_encoded\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(result[\'post\'], y_encoded, test_size=0.30, random_state=1)\n\ndef tokenizersplit(str):\n    return str.split()\n\n.\n.\n.\n'
'prob_y = []\nprob_x = []\n\nfor line in open(data_file_name):\n    line = line.split(None, 1)\n    # In case an instance with all zero features\n    if len(line) == 1: line += [\'\']\n    label, features = line\n    xi = {}\n    for e in features.split():\n        ind, val = e.split(":")\n        xi[int(ind)] = float(val)\n    prob_y += [float(label)]\n    prob_x += [xi]\n\nreturn (prob_y, prob_x)\n'
'from sklearn.ensemble import RandomForestClassifier\n\nn = 5\nX = [df.A.iloc[i:i+n] for i in df.index[:-n+1]] \nlabels = (df.B &gt; 0)[n-1:]\n\nmodel = RandomForestClassifier()\nmodel.fit(X, labels)\nmodel.predict(X)\n'
'pred = knn.predict([3,5,4,2])\n\npred = knn.predict_proba([3,5,4,2])\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import cross_val_score\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier()\nscores = cross_val_score(knn, X, y, cv=10)\nprint(score.mean())\n\n0.96666666666666679\n'
"train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nmodel.fit_generator(\n        train_generator,\n        steps_per_epoch=2000,\n        epochs=50,\n        validation_data=validation_generator,\n        validation_steps=800)\n"
'import numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.corpus import stopwords\nimport string\nimport spacy\nnlp = spacy.load(\'en\')\n\n# to remove punctuations\ntranslator = str.maketrans(\'\', \'\', string.punctuation)\n\n# some sample documents\nresumes = ["Executive Administrative Assistant with over 10 years of experience providing thorough and skillful support to senior executives.",\n"Experienced Administrative Assistant, successful in project management and systems administration.",\n"10 years of administrative experience in educational settings; particular skill in establishing rapport with people from diverse backgrounds.",\n"Ten years as an administrative support professional in a corporation that provides confidential case work.",\n"A highly organized and detail-oriented Executive Assistant with over 15 years\' experience providing thorough and skillful administrative support to senior executives.",\n"More than 20 years as a knowledgeable and effective psychologist working with individuals, groups, and facilities, with particular emphasis on geriatrics and the multiple psychopathologies within that population.",\n"Ten years as a sales professional with management experience in the fashion industry.",\n"More than 6 years as a librarian, with 15 years\' experience as an active participant in school-related events and support organizations.",\n"Energetic sales professional with a knack for matching customers with optimal products and services to meet their specific needs. Consistently received excellent feedback from customers.",\n"More than six years of senior software engineering experience, with strong analytical skills and a broad range of computer expertise.",\n"Software Developer/Programmer with history of productivity and successful project outcomes."]\n\njob_doc = ["""Executive Administrative with a knack for matching and effective psychologist with particular emphasis on geriatrics"""]\n\n# combine the two\n_all = resumes+job_doc\n\n# convert each to spacy document\ndocs= [nlp(document) for document in _all]\n\n# lemmatizae words, remove stopwords, remove punctuations\ndocs_pp = [\' \'.join([token.lemma_.translate(translator) for token in docs if not token.is_stop]) for docs in docs]\n\n# get tfidf matrix\ntfidf_vec = TfidfVectorizer()\ntfidf_matrix = tfidf_vec.fit_transform(docs_pp).todense()\n\n# calculate similarity\ncosine_similarity(tfidf_matrix[-1,], tfidf_matrix[:-1,])\n'
'import numpy as np\n\nar = [\n[1,1,1,1],\n[2,2,2,2],\n[1,1,1,1],\n[4,4,4,4]\n]\n\n\ndef insert_row(ar,positions):\n    ar = np.array(ar)\n    v = ar\n    count = 0\n    for i in positions:\n            a = np.add(ar[i,:],ar[i+1,:])/2\n            v = np.vstack([v[0:i+count+1,:],a,v[i+count+1:,:]])\n            count+=1\n    return v\n\n    print(insert_row(ar, [1,2]))\n\n     [[ 1.   1.   1.   1. ]\n     [ 2.   2.   2.   2. ]\n     [ 1.5  1.5  1.5  1.5]\n     [ 1.   1.   1.   1. ]\n     [ 2.5  2.5  2.5  2.5]\n     [ 4.   4.   4.   4. ]]\n\nprint(insert_row(ar, [0]))\n\n[[ 1.   1.   1.   1. ]\n [ 1.5  1.5  1.5  1.5]\n [ 2.   2.   2.   2. ]\n [ 1.   1.   1.   1. ]\n [ 4.   4.   4.   4. ]]\n'
'&gt;&gt;&gt; x = [0,2,0,5,0,6,77,8,9]\n&gt;&gt;&gt; list(filter((0).__ne__, x))\n[2, 5, 6, 77, 8, 9]\n'
"model_new.add(Dense(n_x_new,input_dim=200 ,kernel_initializer='glorot_uniform', activation='sigmoid'))\n"
"    im = mpimg.imread('dva-test.png')\n    im = im[:, :, 0]\n    im = im.ravel()\n    for j in range(len(im)):\n        if im[j] == 0:\n            im[j] = 1\n        elif im[j] == 1:\n            im[j] = 0\n\n     global prediction\n     prediction = cls.predict(np.array([im], dtype=float), as_iterable=False)\n"
'import numpy as np\nimages_list = np.stack(image_list) # assuming all the images have similar shape\n                                   # i.e. (height, width, 3), images_list has now\n                                   # shape (num_images, height, width, channel)\n\nimages = tf.placeholder(tf.float32, [None, height, width, channel])\n...\nsess.run(train_op, feed_dict={images:images_list})\n'
'preds_classes = model.predict_classes(X_test)\n\nimport numpy as np\n\nprobs = model.predict(X_test)\nclasses = np.argmax(probs, axis=-1)\n'
'"one": {A}\n"two": {A,B}\n"three": {A,B}\n"four": {B,C}\n"five": {C}\n\nStep 1: The counter will be all zeros initially  \n\nStep 2: After looking at the values for "four" the counter will look like below  \nB:1, C:1  \n\nStep 3: After looking at the values for "five" the counter will look like below  \nB:1, C:2  \n\nStep 4: After looking at the values for "six" the counter will look like below   \nB:1, C:2  \n\nStep 5: Choose the set with the maximum value. In this case it will be C.  \n'
'm_samples = orig_image_matrix.shape[0]\nimage_matrix = orig_image_matrix.reshape(m_samples, -1)\n'
'prediction = model.predict(test_X)\nprobs = prediction.max(1)\n'
'def load_housing_data(housing_path=HOUSING_PATH):\n    # missing function call to fetch the data\n    fetch_housing_data()\n    csv_path = os.path.join(housing_path, "housing.csv")\n    return pd.read_csv(csv_path)\n\n'
'import umap\n\nreduced = umap.UMAP().fit_transform(pima)\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nreduced = pca.fit_tranform(pima)\n'
"data.loc[data['Minutes'] == '0', 'Minutes'] = '00'\n"
'# load a dataset and regression function\nfrom sklearn import linear_model,datasets\nimport pandas as pd\n# I use boston dataset to show you \nfull_data = datasets.load_boston()\n\n# get a regressor, fit intercept\nreg = linear_model.LinearRegression(fit_intercept=True)\n# data is our explanatory, target is our response\nreg.fit(full_data[\'data\'],full_data[\'target\'])\n\n# we have 1 intercept and  11 variables\' coef\nreg.intercept_,reg.coef_\n\n# get the name of features\nfull_data.feature_names\n# append to get a new list\ncoef = np.append(reg.intercept_,reg.coef_)\nfeature_names = np.append([\'Intercept\'], full_data.feature_names)\n# output a dataframe contains coefficients you want\npd.DataFrame({"feature_names":feature_names,"coef":coef})\n\n   feature_names       coef\n0      Intercept  36.459488\n1           CRIM  -0.108011\n2             ZN   0.046420\n3          INDUS   0.020559\n4           CHAS   2.686734\n5            NOX -17.766611\n6             RM   3.809865\n7            AGE   0.000692\n8            DIS  -1.475567\n9            RAD   0.306049\n10           TAX  -0.012335\n11       PTRATIO  -0.952747\n12             B   0.009312\n13         LSTAT  -0.524758\n'
'estimator.fit(X_train, y_train)\ny_pred = estimator.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)\n'
"silhouette_score(df, preds, metric = 'euclidean')\n"
'#Create a sample DataFrame\ndf=pd.DataFrame([["1/1/2016 12:00:20 AM", 1],\n                 ["1/2/2016 5:03:20 AM", 2],\n                 ["1/2/2016 5:06:20 AM", 3],\n                 ["1/2/2016 5:07:20 AM", 4],\n                 ["1/2/2016 6:06:20 AM", 5],\n                 ["1/3/2016 00:00:20 AM", 6]]\n        ,columns=[\'date\',\'event_id\'])\n\n#We convert the date column into datetime and set as the index\ndf[\'date\'] = pd.to_datetime(df[\'date\'])\ndf.index = df.date\ndel df[\'date\']\n\n\n#This is where the magic happens.    \ndf.resample(\'6H\', label = \'right\').count()\n\n           date     \n2016-01-01 06:00:00     1\n2016-01-01 12:00:00     0\n2016-01-01 18:00:00     0\n2016-01-02 00:00:00     0\n2016-01-02 06:00:00     3\n2016-01-02 12:00:00     1\n2016-01-02 18:00:00     0\n2016-01-03 00:00:00     0\n2016-01-03 06:00:00     1\n'
"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Generating x data range from -1 to 4 with a step of 0.01\nx = np.arange(-1, 4, 0.01)\n\n# Simulating y data with an inflection point as y(x) = x³ - 5x² + 2x\ny = x**3 - 5*x**2 + 2*x\n\n# Plotting your curve\nplt.plot(x, y, label=&quot;y(x)&quot;)\n\n# Computing y 1st derivative of your curve with a step of 0.01 and plotting it\ny_1prime = np.gradient(y, 0.01)\nplt.plot(x, y_1prime, label=&quot;y'(x)&quot;)\n\n# Computing y 2nd derivative of your curve with a step of 0.01 and plotting it\ny_2prime = np.gradient(y_1prime, 0.01)\nplt.plot(x, y_2prime, label=&quot;y''(x)&quot;)\n\n# Finding the index of the zero (or zeroes) of your curve\nx_zero_index = np.where(np.diff(np.sign(y_2prime)))[0]\n\n# Finding the x value of the zero of your curve\nx_zero_value = x[x_zero_index][0]\n\n# Finding the y value corresponding to the x value of the zero\ny_zero_value = y[x_zero_index][0]\n\n# Reporting\nprint(f'The inflection point of your curve is {y_zero_value:.3f}.')\n"
"def label_encoder(column):\nvalues = ['one', 'two', 'three', 'four', 'five'] \nnew_row = []\nfor row in column:\n    for i, ii in enumerate(values):\n        if row == ii:\n            new_row.append(i)\n        else:\n            continue\nreturn new_row\n\ndef label_encoder(column):\nvalues = ['one', 'two', 'three', 'four', 'five'] \nnew_row = [i for row in column for (i, ii) in enumerate(values) if row==ii]\nreturn new_row\n"
'lr = LinearRegression()\nrg = Ridge()\npac = PassiveAggressiveClassifier()\nptn = Perceptron()\nestimators = [lr,rg,pac,ptn]\n'
'acc = model.evaluate(x_test,y_test)\n'
"&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(data=dict(a=['abc]', 'def']))\n&gt;&gt;&gt; df\n      a\n0  abc]\n1   def\n&gt;&gt;&gt; df['a'].str.replace(']', '')\n0    abc\n1    def\nName: a, dtype: object\n"
"from sklearn.feature_extraction.text import TfidfVectorizer\n\ndf = pd.DataFrame({&quot;blurb&quot;:[&quot;this is a sentence&quot;, &quot;this is, well, another one&quot;]})\nvectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b')\ndf[&quot;tf_idf&quot;] = list(vectorizer.fit_transform(df[&quot;blurb&quot;].values.astype(&quot;U&quot;)).toarray())\nvocab = sorted(vectorizer.vocabulary_.keys())\ndf[&quot;tf_idf_dic&quot;] = df[&quot;tf_idf&quot;].apply(lambda x: {k:v for k,v in dict(zip(vocab,x)).items() if v!=0})\n"
"for x in root.winfo_children():\n    if x.winfo_class() == 'Entry':\n        if x.get() != '':\n            # push all to the array\n            print(float(x.get()))\n            ty.append(float(x.get()))\n"
"df['Feature'].str.get_dummies(sep=',').add_prefix('feature_')\n\n   feature_option1  feature_option2  feature_option3  feature_option4\n0                1                0                1                0\n1                0                0                0                1\n2                0                1                1                0\n"
"y_train = df1['Y column']\nX_train = df1.drop('Y Column', axis = 1)\n\nX_test = df2\n"
'    /// &lt;summary&gt;\n    /// Creates a Jpeg image file drawing the given text and saves the file on give disk location.\n    /// &lt;/summary&gt;\n    /// &lt;param name="text"&gt;text which is to be draw in image&lt;/param&gt;\n    /// &lt;param name="font"&gt;font is to be used for the text&lt;/param&gt;\n    /// &lt;param name="textColor"&gt;color to be used for the text&lt;/param&gt;\n    /// &lt;param name="backColor"&gt;background color to be used for the image&lt;/param&gt;\n    /// &lt;param name="filename"&gt;filename with complete path where you want to save the output jpeg file.&lt;/param&gt;\n    private static void DrawText(String text, Font font, Color textColor, Color backColor, string filename)\n    {\n        //first, create a dummy bitmap just to get a graphics object\n        Image image = new Bitmap(1, 1);\n        Graphics drawing = Graphics.FromImage(image);\n\n        //measure the string to see how big the image needs to be\n        SizeF textSize = drawing.MeasureString(text, font);\n\n        //free up the dummy image and old graphics object\n        image.Dispose();\n        drawing.Dispose();\n\n        //create a new image of the right size\n        image = new Bitmap((int)textSize.Width, (int)textSize.Height);\n\n        drawing = Graphics.FromImage(image);\n\n        //paint the background\n        drawing.Clear(backColor);\n\n        //create a brush for the text\n        Brush textBrush = new SolidBrush(textColor);\n\n        drawing.DrawString(text, font, textBrush, 0, 0);\n\n        drawing.Save();\n\n        textBrush.Dispose();\n        drawing.Dispose();\n\n        image.Save(filename, ImageFormat.Jpeg);\n    }\n\n    /// &lt;summary&gt;\n    /// Merges two Jpeg images vertically\n    /// &lt;/summary&gt;\n    /// &lt;param name="inputJpeg1"&gt;filename with complete path of the first jpeg file.&lt;/param&gt;\n    /// &lt;param name="inputJpeg2"&gt;filname with complete path of the second jpeg file.&lt;/param&gt;\n    /// &lt;param name="outputJpeg"&gt;filename with complete path where you want to save the output jpeg file.&lt;/param&gt;\n    private void MergeJpeg(string inputJpeg1, string inputJpeg2, string outputJpeg)\n    {\n\n        Image image1 = Image.FromFile(inputJpeg1);\n        Image image2 = Image.FromFile(inputJpeg2);\n\n        int width = Math.Max(image1.Width, image2.Width);\n        int height = image1.Height + image2.Height;\n\n        Bitmap outputImage = new Bitmap(width, height);\n        Graphics graphics = Graphics.FromImage(outputImage);\n\n        graphics.Clear(Color.Black);\n        graphics.DrawImage(image1, new Point(0, 0));\n        graphics.DrawImage(image2, new Point(0, image1.Height));\n\n        graphics.Dispose();\n        image1.Dispose();\n        image2.Dispose();\n\n        outputImage.Save(outputJpeg, System.Drawing.Imaging.ImageFormat.Jpeg);\n        outputImage.Dispose();\n    }\n\n    var inputFile = @"c:\\inputImage.jpg";\n    var outputFile = @"c:\\outputImage.jpg";\n    Bitmap bitmap = null;\n\n    //Open file in read moad and create a stream and using that stream create a bitmap.\n    using (var stream = File.OpenRead(inputFile))\n    {\n        bitmap = (Bitmap)Bitmap.FromStream(stream);\n    }\n\n    using (bitmap)\n    using (var graphics = Graphics.FromImage(bitmap))\n    using (var font = new Font("Arial", 20, FontStyle.Regular))\n    {\n        //Draw Title in the existing jpeg file at coordinates 0,0 these coordinates can be changed as per your need.\n        graphics.DrawString("Title ", font, Brushes.Red, 0, 0);\n\n        //Save the updated file.\n        bitmap.Save(outputFile);\n    }\n'
"rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=KFold(len(y),n_folds=10),\nscoring='accuracy')\n"
'import matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn import svm\n\nboston = datasets.load_boston()\n\nX, y = boston.data, boston.target\nreg = svm.SVR(gamma=0.001, C=100.)\nreg.fit(X, y)\n\npredictions_training_set = reg.predict(X)\n'
"from sklearn.datasets import load_mlcomp\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Load mlcomp data using sklearn\ntrain_data = load_mlcomp(name_or_id=523, set_='train', mlcomp_root='/somewhere/on/your/computer')\ntest_data = load_mlcomp(name_or_id=523, set_='test', mlcomp_root='/somewhere/on/your/computer')\n# if you had the environment variable `MLCOMP_DATASETS_HOME` set, you wouldn't need to explicitly pass anything to `mlcomp_root`\n\n# `data` is a standard `Bunch` object, so you can now straightforwardly go on and vectorize the dataset,...\nvec = CountVectorizer(decode_error='replace')\nX_train = vec.fit_transform(train_data.data)\nX_test = vec.transform(test_data.data)\n\n# ...train a classifier... \nmnb = MultinomialNB()\nmnb.fit(X_train, train_data.target)\n\n# ...and evaluate it.\nprint('Accuracy: {}'.format(accuracy_score(test_data.target, mnb.predict(X_test))))\n"
"import cv2\nimport numpy as np\n\n\ndef getsamples(img):\n    x, y, z = img.shape\n    samples = np.empty([x * y, z])\n    index = 0\n    for i in range(x):\n        for j in range(y):\n            samples[index] = img[i, j]\n            index += 1\n    return samples\n\n\ndef EMSegmentation(img, no_of_clusters=2):\n    output = img.copy()\n    colors = np.array([[0, 11, 111], [22, 22, 22]])\n    samples = getsamples(img)\n    em = cv2.ml.EM_create()\n    em.setClustersNumber(no_of_clusters)\n    em.trainEM(samples)\n    means = em.getMeans()\n    covs = em.getCovs()  # Known bug: https://github.com/opencv/opencv/pull/4232\n    x, y, z = img.shape\n    distance = [0] * no_of_clusters\n    for i in range(x):\n        for j in range(y):\n            for k in range(no_of_clusters):\n                diff = img[i, j] - means[k]\n                distance[k] = abs(np.dot(np.dot(diff, covs[k]), diff.T))\n            output[i][j] = colors[distance.index(max(distance))]\n    return output\n\n\nimg = cv2.imread('dinosaur.jpg')\noutput = EMSegmentation(img)\ncv2.imshow('image', img)\ncv2.imshow('EM', output)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n"
'from sklearn.feature_selection import SelectKBest, chi2\n\nX = # your dataframe with n columns\ny = # target values - encoded if categorical\n# instanciate your selector\nselector = SelectKBest(chi2, k=...) # k &lt; n, try something like int(round(n/10.))\n# Fit it to your data\nselector.fit(X, y) # returns the selector itself but fitted\n# You can transform your data using the fit_transform method if you want\n\n# Now at this step you have reduce the dimensionality of your feature space. You can now perform a classification\n'
'from sklearn.feature_extraction.text import CountVectorizer\n\n#min_df is the minimum number of students \n#that have to have a piece of software installed to be included in \n#the feature set\n\nvectorizer = CountVectorizer(min_df=1)\nX = vectorizer.fit_transform(data)\n'
"import fasttext\n\nmodel = fasttext.load_model('wiki.en.bin')  # the name of the pretrained model\n\nclassifier = fasttext.supervised('train.txt', 'model', label_prefix='__label__')\n\nresult = classifier.test('test.txt')\nprint ('P@1:', result.precision)\nprint ('R@1:', result.recall)\nprint ('Number of examples:', result.nexamples)\n"
'import sys, os, numpy as np\nsys.path.insert(0, os.environ[\'CAFFE_ROOT\']+\'/python\')\nimport caffe\nclass classWeightHLayer(caffe.Layer):\n  def setup(self,bottom,top):\n    assert len(bottom)==1, "expecting exactly one input"\n    assert len(top)==1, "producing exactly one output"\n    # you might want to get L - the number of labels as a parameter...\n\n  def reshape(self,bottom,top):\n    top[0].reshape(1,1,L,L) # reshape the output to the size of H\n\n  def forward(self,bottom,top):         \n    labels = bottom[0].data\n    H = np.zeros((1,1,L,L), dtype=\'f4\') \n    # do your magic here...\n    top[0].data[...] = H    \n\n  def backward(self, top, propagate_down, bottom):\n    # no back-prop for input layers\n    pass\n'
"Train = pd.read_csv('Dataset/train.csv', delimiter=';')  \nTest = pd.read_csv('Dataset/test.csv', delimiter=';')\n"
"train_generator = train_datagen.flow_from_directory(\n    'your/data/here',\n    target_size=(150, 150),\n    batch_size=32,\n    class_mode='binary')\n"
'gen_logo = logo.split(df, groups=groups)  # create your generator\nlassoCV = linear_model.LassoCV(eps=0.0001, n_alphas=400, max_iter=200000, cv=gen_logo, normalize=False, random_state=9)  # pass it to the cv argument\nlassoCV.fit(df, y)  # now fit\n'
'schema = StructType([\n    StructField("Freq_Hz", IntegerType(), False),\n    StructField("AoA_Deg", IntegerType(), False),\n    StructField("Chord_m", DoubleType(), False),\n    StructField("V_inf_mps", DoubleType(), False),\n    StructField("displ_thick_m", DoubleType(), False),\n])\n'
'import matplotlib.pyplot as plt\nimport numpy as np\n\ndef generateData(goal):\n    x=[_ for _ in range(20)]\n    y=[10+np.random.exponential()*5 for _ in range(100)]\n    return x,y\n\ndef drawHistogram(data,nBins):\n    plt.figure()\n    plt.hist(diffs,nBins)\n\nsweep=np.linspace(10,20,4)\n\nfor goal in sweep:\n    for gw2 in sweep:\n\n        diffs=[]\n\n        for i in range(10):\n            data=generateData(goal)\n            diffs.append(goal-np.mean(data[1]))\n\n        #generate plot\n        plt.figure(1)\n        clr=(abs(np.mean(diffs))/goal,0,0)\n        plt.plot([goal], [gw2], marker="s", mew=\'1\', ms=\'35\', color=clr)\n\n        drawHistogram(diffs,5) ##Comment this line out to see what the final graph should look like\n\nplt.draw()\nplt.show()\n'
'theta -= ((alpha * 1) / m) * np.dot(X, (hip(X, theta) - y).T)\n'
'hypothesis = np.dot(x.astype(float), theta)\n'
'n_epoch = 100\n# Assuming of course that you have more than n_epoch samples in each of your sets\ntrainSamplesByEpoch = int(len(train_X) / n_epoch)      \nfor epoch in range(1,100):\n    train_X_current = train_X[0:epoch*trainSamplesByEpoch] \n    train_y_current = train_y[0:epoch*trainSamplesByEpoch] \n    # Train your network with train_X_current, train_y_current \n    # Compute the train accuracy with train_X_current, train_y_current  \n    # Compute the test accuracy with test_X, test_y\n'
"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=0.33)\n\npipe = Pipeline([\n    ('scale', StandardScaler()),\n    ('clf', LogisticRegression())\n])\n\nparam_grid = [\n    {\n        'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n        'clf__C': np.logspace(-3, 1, 5),\n    },\n]\n\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\ngrid.fit(X_train, y_train)\n"
'transformer = GaussianRandomProjection(n_components=8) #Set your desired dimension of the output\nX_new = transformer.fit_transform(wine_data.values[:, :11])\n'
"df1['Name']=df.loc[df1.index]['Name'].values \n\ndf1['Name']=df.loc[pol_ids.index]['Name'].values \n"
"import spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\ndoc = nlp('Its John and he is working at Google')\nprint([(X.text, X.label_) for X in doc.ents])\n\n[('John', 'PERSON'), ('Google', 'ORG')]\n\npip install spacy\npython -m spacy download en \n"
'kstream.map(lambda k, v: json.loads(v))\n'
"model.save('my_model.h5', include_optimizer=False)\n"
'Y=Y[np.isfinite(X[27])] # Remove NAN values from my label Y\n\nX=X[np.isfinite(X[27])] # remove rows with NaN labels\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.3)\n'
'import renom as rm\nimport numpy as np\n\n\nx = np.array([[1, -1]])\narray([[ 1, -1]])\nrm.elu(x)\nelu([[ 1.  , -0.00632121]])\n\n\n# instantiation\nactivation = rm.Elu()\nactivation(x)\nelu([[ 1.  , -0.00632121]])\n'
'# Feature to be predicted (y)\ny = df[\'predicted\'] #target variable\n\n#Training data\nX = df.drop("predicted",1)\n# here 1 is the axis which means drop a column\n\n# Perform a 70% train and 30% test data split\nX_train, X_test, y_train, y_test = ____(X, y, ____=____)\n'
'import numpy as np\n\ntraining_data = np.asarray(training_data)\nassert(training_data.shape = (1500,1800)) \n'
"# Data Partition\nX_train, X_valid, Y_train, Y_valid = model_selection.train_test_split(X, Y, test_size=0.2, random_state=21)\n\n# Cross validation on multiple model to see which models gives the best results\nprint('Start cross val')\ncv_score = cross_val_score(model, X_train, Y_train, scoring=metric, cv=5)\n# Then visualize the score you just obtain using mean, std or plot\nprint('Mean CV-score : ' + str(cv_score.mean()))\n\n# Then I tune the hyper parameters of the best (or top-n best) model using an other cross-val\nfor param in my_param:\n    model = model_with_param\n    cv_score = cross_val_score(model, X_train, Y_train, scoring=metric, cv=5)\n    print('Mean CV-score with param: ' + str(cv_score.mean()))\n\n# Now I have best parameters for the model, I can train the final model\nmodel = model_with_best_parameters\nmodel.fit(X_train, y_train)\n\n# And finally test your tuned model on the test set\ny_pred = model.predict(X_test)\nplot_or_print_metric(y_pred, y_test)\n"
"test_phrase = 'That film entertained me a lot'\nexample = vectorizer.transform([test_phrase])\nprint(test_phrase + ' was classified as ' + str(classifier.predict(example)))\n"
'textdata_with_label_113 = textData[clusterer.labels_ == 113]\n'
'import re\n\nstr = "This is what I\'m trying to do, but I can\'t figure out how."\nres = re.sub(r\'(?&lt;=\\w)(?=[,.!;:])\', \' \', str)\nprint res\n'
"size = 50\nX = np.arange(1, size, 1)\nY = np.arange(1, size, 1)\nU, V = np.meshgrid(X, Y)\n\n# Normalize the arrows:\nU = U / np.sqrt(U**2 + V**2)\nV = V / np.sqrt(U**2 + V**2)\n\n# create angles field\ndata = []\nfor i in np.linspace(0, 180, Y.shape[0]):\n    data.append([i]*X.shape[0])\n\nangle = np.array(data)\n\n# set starting parameters\nx_start_position = 2\ny_start_position = 2\nstep_length = 1.0\npoint_angle = angle[x_start_position, y_start_position]\nline_coordinates = [[x_start_position, y_start_position]]\n\n# collect line points for each step\nwhile x_start_position &gt;= 2:\n\n    # calculate tep based on angle\n    x_step = step_length * np.cos(point_angle*np.pi/180)\n    y_step = step_length * np.sin(point_angle*np.pi/180)\n\n    # calculate new position\n    x_new_position = x_start_position + x_step\n    y_new_position = y_start_position + y_step\n\n    # get array index of new position\n    x_new_index = int(x_new_position)\n    y_new_index = int(y_new_position)\n\n    # get new angle\n    point_angle = angle[y_new_index, x_new_index]\n\n    # update start position\n    x_start_position = x_new_position\n    y_start_position = y_new_position\n\n    # collect results\n    line_coordinates.append([x_new_position, y_new_position])\n\n# set line coordinates\nline_data = np.array(line_coordinates)\nx_line = line_data[:,0]\ny_line = line_data[:,1]\n\n# plot field\nplt.quiver(X, Y, U, V, color='black', angles=angle, width=0.005)\n# plot line\nplt.plot(x_line, y_line, '-', color='red')\nplt.show()\n"
'from sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#Load boston housing dataset as an example\nboston = load_boston()\n\n\nX = boston["data"]\nY = boston["target"]\nnames = boston["feature_names"]\nreg = RandomForestRegressor()\nreg.fit(X, Y)\nprint("Features sorted by their score:")\nprint(sorted(zip(map(lambda x: round(x, 4), reg.feature_importances_), names), \n             reverse=True))\n\n\nboston_pd = pd.DataFrame(boston.data)\nprint(boston_pd.head())\n\nboston_pd.columns = boston.feature_names\nprint(boston_pd.head())\n\n# correlations\nboston_pd.corr()\nimport seaborn as sn\nimport matplotlib.pyplot as plt\ncorrMatrix = boston_pd.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()\n\nfeatures = boston.feature_names\nimportances = reg.feature_importances_\nindices = np.argsort(importances)\n\nplt.title(\'Feature Importances\')\nplt.barh(range(len(indices)), importances[indices], color=\'#8f63f4\', align=\'center\')\nplt.yticks(range(len(indices)), features[indices])\nplt.xlabel(\'Relative Importance\')\nplt.show()\n'
"import numpy as np\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\n\nX, labels_true = load_iris(return_X_y=True) \nX = StandardScaler().fit_transform(X)\n\n# Compute DBSCAN\ndb = DBSCAN(eps=0.5,min_samples=5) # default parameter values\ndb.fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint('Estimated number of noise points: %d' % n_noise_)\nprint(&quot;Homogeneity: %0.3f&quot; % metrics.homogeneity_score(labels_true, labels))\nprint(&quot;Completeness: %0.3f&quot; % metrics.completeness_score(labels_true, labels))\nprint(&quot;V-measure: %0.3f&quot; % metrics.v_measure_score(labels_true, labels))\nprint(&quot;Adjusted Rand Index: %0.3f&quot;\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(&quot;Adjusted Mutual Information: %0.3f&quot;\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(&quot;Silhouette Coefficient: %0.3f&quot;\n      % metrics.silhouette_score(X, labels))\n\nEstimated number of clusters: 2\nEstimated number of noise points: 17\nHomogeneity: 0.560\nCompleteness: 0.657\nV-measure: 0.604\nAdjusted Rand Index: 0.521\nAdjusted Mutual Information: 0.599\nSilhouette Coefficient: 0.486\n\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask &amp; core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = X[class_member_mask &amp; ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()\n"
"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3))\n"
'class_mapping = train_data_gen.class_indices\n'
'    X   Y\n1   0   1\n2   1   2\n3   2   1\n4   3   2\n5   4   0\n6   5   2\n\nŷ = 0.2X + 1.2\n'
'from sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))\n'
"X_over, y_over = oversample.fit_resample(df['Features'].values.reshape(-1,1), df['Class'])\n\nCounter({'Positive': 10, 'Negative': 10})\n"
'y = [int(x * 100) for x in np.random.normal(0, 1, 100)]\n'
"SVC(kernel='rbf', class_weight='balanced', probability=True)\n\ny_pred = clf.predict_proba(X_test_pca)\n"
'from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nx_train = sc_X.fit_transform(x_train)\n'
'[0, 0] [[0.01937993]]\n[0, 1] [[0.99732982]]\n[1, 0] [[0.97613504]]\n[1, 1] [[0.01860087]]\n'
'optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=tfa.optimizers.CyclicalLearningRate)\n'
'model.fit(train_set, y_train)\n\npred_train = model.predict(train_set)\npred_test = model.predict(test_set)\n\nprint(&quot;Accuracy train: &quot;, accuracy_score(y_train, pred_train))\nprint(&quot;Accuracy test: &quot;, accuracy_score(y_test, pred_test))\n'
'rfc = RandomForestClassifier()\ndtc = DecisionTreeClassifier()\n# etc\nclfs = [rfc,dtc]\nfor clf in clfs:\n     clf.fit(x_train, y_train)\n     # etc\n'
"df_train = df_train[..., None]\ndf_validation = df_validation[..., None]\n\ndf_train = np.random.normal(size=(17980, 380))\ndf_validation = np.random.normal(size=(17980, 380))\n\ndf_train = df_train[..., None]\ndf_validation = df_validation[..., None]\n\ny_train = np.random.normal(size=(17980, 1))\ny_validation = np.random.normal(size=(17980, 1))\n\n#train,test = train_test_split(df, test_size=0.20, random_state=0)\n\n\n    \nbatch_size=32\nepochs=5\n    \nmodel = Sequential()\n\nmodel.add((Conv1D(filters=5, kernel_size=2, activation='relu', padding='same')))\nmodel.add((MaxPooling1D(pool_size=2)))\nmodel.add(LSTM(50, return_sequences=True))\nmodel.add(LSTM(10))\nmodel.add(Dense(1))\n\nadam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n\nmodel.compile(optimizer=adam, loss='mse', metrics=['mae', 'mape', 'acc'])\ncallbacks = [EarlyStopping('val_loss', patience=3)]\n\nmodel.fit(df_train, df_validation, batch_size=batch_size)\n\nprint(model.summary())\n"
'print(X.shape, y.shape) # gives (280, 34) (280,)\ny = y.reshape(-1, 1)\n'
'screen\n\nscreen -ls \n\nscreen -r &lt;Screen_name&gt;\n'
"print('Building model...')\nmodel2 = Sequential()\nmodel2.add(LSTM(128, input_shape=(maxlen, len(chars)),return_sequences=True))\nmodel2.add(Dropout(0.5))\nmodel2.add(LSTM(128, return_sequences=True))\nmodel2.add(Dropout(0.5))\nmodel2.add(LSTM(128))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(len(chars), activation='softmax'))\n\n# optimizer = RMSprop(lr=0.01)\noptimizer = Adam()\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\nprint('model built')\n"
'n_features = X_train.shape[1]\nn_desired_features = n_features / 5\nbuckets = np.random.random_integers(0, n_desired_features-1, size=n_features)\nX_new = np.zeros((X_train.shape[0], n_desired_features), dtype=X_train.dtype)\nfor i in range(n_features):\n    X_new[:,buckets[i]] += X_train[:,i]\n\nM = coo_matrix((repeat(1,n_features), (range(n_features), buckets)),\n               shape=(n_features,n_desired_features))\nX_new = X_train.dot(M)\n'
'X, y = # get training data from examples\nfor t in range(m):\n    classifiers[t].fit(X, y, sample_weights=_D)\n    # update _D based on results\n\nfor t in range(m):\n    chosen_examples_indexes = np.array([np.random.random() &lt; _d for _d in _D])\n    training_examples = examples[chosen_examples_indexes]\n    X, y = # get training data from training_examples\n    classifiers[t].fit(X, y)\n    # update _D based on results\n'
'from nltk.classify import NaiveBayesClassifier\nnb = NaiveBayesClassifier.train(train_set)\n'
'from sklearn.preprocessing import LabelEncoder\nle = {}\nfor i in range(len(X)):\n    le[i] = LabelEncoder()\n    X.iloc[:,i] = le[i].fit_transform(X.iloc[:,i])\n# do stuff\nfor i in range(len(X)):\n    X.iloc[:,i] = le[i].inverse_transform(X.iloc[:,i])\n'
'    # Find the best parameters by comparing on the mean validation score:\n    # note that `sorted` is deterministic in the way it breaks ties\n    best = sorted(grid_scores, key=lambda x: x.mean_validation_score,\n                  reverse=True)[0]\n'
"with open('file.txt') as file:\n    head = file.readline()\n    n, classes, dim = map(int, head.split())\n    print(n, classes, dim)\n\n    train_y = []\n    train_x = []\n\n    for line in file:\n        line = line.strip()\n        if line:\n            data = line.split()\n            labels = data[0]\n            print('labels:', labels)\n            train_y.append(labels)\n\n            data = data[1:]\n            data = [el.split(':')[1] for el in data]  # remove index\n            data = [float(el) for el in data]  # convert to float\n            print('data', len(data), data)\n            train_x.append(data)\n\n43907 120 101\n11,31,65,67\n120 [0.38088, 0.49408, 0.54001, 0.42293, 0.15832, 0.32698, 0.39086, 0.52712, 0.25405, 0.22373, 0.04029, 0.14113, 0.11225, 0.26317, 0.14702, 0.47241, 0.59261, 0.65314, 0.49987, 0.19652, 0.40389, 0.4824, 0.61922, 0.32035, 0.28125, 0.05475, 0.18046, 0.13996, 0.31993, 0.18122, 0.36429, 0.40721, 0.36893, 0.42766, 0.21139, 0.36434, 0.37071, 0.40911, 0.2893, 0.24305, 0.06312, 0.19359, 0.15876, 0.31605, 0.19741, 0.65617, 0.67876, 0.65083, 0.67464, 0.49243, 0.62389, 0.61062, 0.67822, 0.57477, 0.52307, 0.2068, 0.49629, 0.42922, 0.58661, 0.47155, 0.28448, 0.43247, 0.49807, 0.40814, 0.10271, 0.30303, 0.3095, 0.44486, 0.19173, 0.17489, 0.03414, 0.1531, 0.06832, 0.21702, 0.09969, 0.40986, 0.56192, 0.61203, 0.51447, 0.14602, 0.39881, 0.38329, 0.54849, 0.28294, 0.25271, 0.05101, 0.22311, 0.09811, 0.29967, 0.14487, 0.30849, 0.35848, 0.35208, 0.39469, 0.15751, 0.33937, 0.32156, 0.34137, 0.24797, 0.20607, 0.061, 0.21679, 0.11239, 0.27365, 0.15274, 0.59808, 0.62169, 0.60721, 0.64402, 0.39495, 0.59365, 0.55153, 0.57439, 0.51103, 0.464, 0.20203, 0.49234, 0.31798, 0.54781, 0.39378]\n31,33,67\n120 [0.44957, 0.46049, 0.45347, 0.41078, 0.23176, 0.40215, 0.34959, 0.53646, 0.31812, 0.30162, 0.06384, 0.22034, 0.18436, 0.30923, 0.21698, 0.51332, 0.51775, 0.52954, 0.4794, 0.26883, 0.46433, 0.41179, 0.63374, 0.36232, 0.35489, 0.07848, 0.26079, 0.22042, 0.35629, 0.25343, 0.39923, 0.37127, 0.33754, 0.39948, 0.27279, 0.41442, 0.33539, 0.41463, 0.32862, 0.29632, 0.08851, 0.26424, 0.22165, 0.35063, 0.25661, 0.66258, 0.59286, 0.56515, 0.62638, 0.5606, 0.66977, 0.56707, 0.67373, 0.56618, 0.56082, 0.3007, 0.56459, 0.50736, 0.61847, 0.52117, 0.3571, 0.43548, 0.50553, 0.44414, 0.14728, 0.36831, 0.30534, 0.50123, 0.24166, 0.23336, 0.04939, 0.21594, 0.10365, 0.27122, 0.14674, 0.4167, 0.4962, 0.5864, 0.50466, 0.17836, 0.42506, 0.3666, 0.56851, 0.28405, 0.28237, 0.0633, 0.26014, 0.12727, 0.31983, 0.17963, 0.3498, 0.35115, 0.35862, 0.40972, 0.19611, 0.38029, 0.31352, 0.37822, 0.27504, 0.24851, 0.07654, 0.26602, 0.14537, 0.31114, 0.19209, 0.61895, 0.59779, 0.60175, 0.64685, 0.41488, 0.62746, 0.53956, 0.63861, 0.49637, 0.48099, 0.19959, 0.53508, 0.32383, 0.57149, 0.39756]\n"
'df.groupby(df[1].str[:16]).head(1)\nOut[426]: \n                                  1        3         4         5        6   \\\n0   2018-03-14 23:00:02.815000-05:00  81.6381  0.002791  165.7452  29.6716   \n5   2018-03-14 23:15:01.853000-05:00  81.6381  0.002791  162.1682  29.5647   \n13  2018-03-14 23:30:01.834000-05:00  81.6872  0.002786  159.4302  29.4285   \n21  2018-03-14 23:45:01.819000-05:00  81.8787  0.002791  168.4906  29.1521   \n         7     8      9        10  \n0   91.1426  70.0  100.0  63.5461  \n5   91.1426  70.0  100.0  63.4788  \n13  90.8571  70.0  100.0  63.4151  \n21  90.3730  70.0  100.0  63.3431  \n'
"df1 = pd.DataFrame(np.random.randint(0,10,size=(10, 2)), columns=list('AB'))\ndf2 = pd.DataFrame(np.random.randint(0,10,size=(10, 2)), columns=list('AB'))\ndf3 = pd.DataFrame(np.random.randint(0,10,size=(10, 2)), columns=list('AB'))\n\ndef plot_me(*kwargs):\n    plt.figure(figsize=(13,9))\n    lab_ind = 0\n    for i in kwargs:\n\n        plt.plot(i['A'], i['B'], label = lab_ind)\n        lab_ind += 1\n    plt.legend()\n    plt.show()\n\nX = np.random.randint(0,50 ,size=(50, 2))\ny = np.random.randint(0,2 ,size=(50, 1))\n\nmodel=LinearRegression().fit(X,y)\npredictions=model.predict(X)\n\nres_df = pd.DataFrame(predictions,columns = ['Value'])\n\n    Value\n0   0.420395\n1   0.459389\n2   0.369648\n3   0.416058\n4   0.644088\n5   0.362072\n6   0.363157\n7   0.468943\n.      .\n.      .\n"
"from sklearn import linear_model\nfrom sklearn import preprocessing\nfrom random import randint\nimport numpy as np\n\n# Lets generate some random data\nx=[[randint(0,100),randint(0,100)] for i in range(1000)]\n\n# and compute the deterministic addition function\nY=[i[0]+i[1] for i in x]\n\n# First, we create an instance of the regressor\nreg_add=linear_model.LinearRegression()\n\n# then, fit it to the data\nreg_add.fit(x, Y)\n\n# and finally, test it on some sample\nsample = [[56, 23]]\nprint('Addition: X={}, Y_hat={}'.format(sample,reg_add.predict(sample)))\n\nAddition: X=[[56, 23]], Y_hat=[79.]\n\n# Again, lets generate some random data\nx=[[randint(0,100),randint(0,100)] for i in range(1000)]\n\n# And compute the multiplication of all coordinates for each sample\nY=np.array([i[0]*i[1] for i in x])\n\n# Lets create an instance of the processor, using polynomial features of degree=2\npp = preprocessing.PolynomialFeatures(2)\n\n# transform the original data\nx2 = pp.fit_transform(x)\n\n# Then, create a linear regressor,\nreg_mult=linear_model.LinearRegression()\n\n# Fit it to the processed data and the results\nreg_mult.fit(x2, Y)\n\n# and test it on a new example.\nsample = [[2, 4]]\nprint('Multiplication: X={}, Y_hat={}'.format(sample,reg_mult.predict(pp.transform(sample))))\n\nMultiplication: X=[[2, 4]], Y_hat=[8.]\n"
'pip install xgboost\n'
'Z = Z.reshape(xx1.shape)\nplt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap),\nplt.xlim(xx1.min(), xx1.max())\nplt.ylim(xx2.min(), xx2.max())\n'
'import numpy as np\n\na = np.zeros((299, 299, 3, 50))\nprint(a.shape)  # (299, 299, 3, 50) H x W x C x M\nb = np.transpose(a, [3, 0, 1, 2])\nprint(b.shape)  # (50, 299, 299, 3)\n\nimport tensorflow as tf\n\na = tf.zeros((299, 299, 3, 50), dtype=tf.int32)\nprint(a.shape.as_list())  # [299, 299, 3, 50]\nb = tf.transpose(a, [3, 0, 1, 2])\nprint(b.shape.as_list())  # [50, 299, 299, 3]\n'
" import pandas as pd\n import requests\n\n # Preparing dummy data \n links = ['https://google.com', 'http://thisisinvalid.de', 'http://docs.python-requests.org/en/master/api/broken']\n df = pd.DataFrame(links, columns=['links'])\n\n # Update as you need\n def is_broken(link):\n     try:\n         response = requests.get(link)\n         if response.status_code == 404:\n             return True\n         return False\n     except Exception as e:\n         return True\n\n df.ix[:, 'is_broken'] = df.ix[:, 'links'].map(lambda link: is_broken(link))\n"
'words = set(nltk.corpus.words.words())\n\ndef clean_sent(sent):\n    return " ".join(w for w in nltk.wordpunct_tokenize(sent) \\\n     if w.lower() in words or not w.isalpha())\n\ndf[\'Clean\'] = df[\'Chats\'].apply(clean_sent)\n\ndf[\'Chats\'] = df[\'Chats\'].apply(clean_sent)\n'
"def kmeans(dataSet, k, c):\n    # 1. Randomly choose clusters\n    rng = np.random.RandomState(c)\n    p = rng.permutation(dataSet.shape[0])[:k]\n    centers = dataSet[p]\n\n    while True:\n        labels = pairwise_distances_argmin(dataSet, centers)\n        new_centers = np.array([dataSet[labels == i].mean(0) for i in range(k)]\n        if np.all(centers == new_centers):\n            break\n        centers = new_centers\n    cluster_data = [dataSet[labels == i] for i in range(k)]\n    l = []\n    covs = []\n    for i in range(k):\n        l.append(len(cluster_data[i]) * 1.0 / len(dataSet))\n        covs.append(np.cov(np.array(cluster_data[i]).T))\n    return centers, l, covs, cluster_data\n\n\nreturn new_mu, new_covs, cluster_data\n\n\nclass gaussian_Mix_Model:\n\n    def __init__(self, k = 8, eps = 0.0000001):\n        self.k = k ## number of clusters\n        self.eps = eps ## threshold to stop `epsilon`\n\n\n    def calculate_Exp_Maxim(self, X, max_iters = 1000):\n\n        # n = number of data-points, d = dimension of data points        \n        n, d = X.shape\n\n        mu, Cov = [], []\n        for i in range(1,k):\n            new_mu, new_covs, cluster_data = kmeans(dataSet, k, c)\n            # Initialize new         \n            mu[k] = new_mu\n            Cov[k]= new_cov\n\n            # initialize the weights\n            w = [1./self.k] * self.k\n\n            R = np.zeros((n, self.k))\n\n            ### LLhoods\n            LLhoods = []\n\n            P = lambda mu, s: np.linalg.det(s) ** -.5 ** (2 * np.pi) ** (-X.shape[1]/2.) \\\n                * np.exp(-.5 * np.einsum('ij, ij -&gt; i',\\\n                        X - mu, np.dot(np.linalg.inv(s) , (X - mu).T).T ) ) \n\n            # Iterate till max_iters iterations        \n            while len(LLhoods) &lt; max_iters:\n\n            # Expectation Calcultion \n\n            ## membership for each of K Clusters\n            for k in range(self.k):\n                R[:, k] = w[k] * P(mu[k], Cov[k])\n\n            # Finding the log likelihood\n            LLhood = np.sum(np.log(np.sum(R, axis = 1)))\n\n            # Now store the log likelihood to the list. \n            LLhoods.append(LLhood)\n\n            # Number of data points to each clusters\n            R = (R.T / np.sum(R, axis = 1)).T                   \n            N_ks = np.sum(R, axis = 0)\n\n\n            # Maximization and calculating the new parameters. \n            for k in range(self.k):\n\n                # Calculate the new means\n                mu[k] = 1. / N_ks[k] * np.sum(R[:, k] * X.T, axis = 1).T\n                x_mu = np.matrix(X - mu[k])\n\n                # Calculate new cov\n                Cov[k] = np.array(1 / N_ks[k] * np.dot(np.multiply(x_mu.T,  R[:, k]), x_mu))\n\n                # Calculate new PiK\n                w[k] = 1. / n * N_ks[k]\n            # check for convergence\n            if (np.abs(LLhood - LLhoods[-2]) &lt; self.eps) and (iteration &lt; max_iters): break\n            else:\n                Continue\n\n    from collections import namedtuple\n    self.params = namedtuple('params', ['mu', 'Cov', 'w', 'LLhoods', 'num_iters'])\n    self.params.mu = mu\n    self.params.Cov = Cov\n    self.params.w = w\n    self.params.LLhoods = LLhoods\n    self.params.num_iters = len(LLhoods)       \n\n    return self.params\n\n# Call the GMM to find the model \ngmm = gaussian_Mix_Model(3, 0.000001)\nparams = gmm.fit_EM(X, max_iters= 150)\n\n# Plotting of Log-Likelihood VS Iterations. \nplt.plot(LLhoods[0])\nplt.savefig('Dataset_2A_GMM_Class_1_K_16.png')\nplt.clf()\nplt.plot(LLhoods[1])\nplt.savefig('Dataset_2A_GMM_Class_2_K_16.png')\nplt.clf()\nplt.plot(LLhoods[2])\nplt.savefig('Dataset_2A_GMM_Class_3_K_16.png')\nplt.clf()\n"
"input_holder = tf.placeholder(shape=(batch_size, None, num_features), dtype=..., name='inputs')\ntarget_holder = tf.placeholder(shape=(batch_size, num_features), dtype=..., name='targets')\n\ninput_holder = tf.placeholder(shape=(None, N, num_features), dtype=..., name='inputs')\ntarget_holder = tf.placeholder(shape=(None, num_features), dtype=..., name='targets')\n"
"x = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(350, activation='relu')(x)\nx = Dropout(0.4)(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\n# this is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\nlen(base_model.layers)\n# 311\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\nmodel.compile(optimizer = keras.optimizers.Adam(),\n                    loss='binary_crossentropy',\n                    metrics=['accuracy'])\n"
'inp = Input(shape=...)\nencoder = Embedding(...)(inp)  # call embedding layer on inputs\nencoder = LSTM(...)(encoder)   # call lstm layer on the output of embedding layer\n\nencoder_last = Lambda(lambda x: x[:,-1,:])(encoder)\n'
'X = dataset.iloc[:,0].values\n\nX = dataset.iloc[:,0].values.reshape(-1, 1)\n\nX = dataset.iloc[:,[0]].values\n'
'import numpy as np \nimport cv2\n# load image\nimg = cv2.imread("fn2.JPG")\n# enlarge image\nimg = cv2.resize(img,None,fx=4, fy=4, interpolation = cv2.INTER_CUBIC)\n# convert to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n# create mask using threshold\nret,mask = cv2.threshold(gray,200,255,cv2.THRESH_BINARY)\n# find contours in mask\nim, contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n# draw contour on image\nfor cnt in contours:\n    if cv2.contourArea(cnt) &lt; 3000 and cv2.contourArea(cnt) &gt; 200:\n        cv2.drawContours(img, [cnt], 0, (255,0,0), 2)\n\n#show images\ncv2.imshow("Mask", mask)\ncv2.imshow("Image", img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n'
'from tensorflow.python import keras\nfrom keras.layers import *\nfrom keras.models import Model, Sequential\n\nIMG_FEATURES_SIZE = 10\nMAX_SENTENCE = 80\nVOCABULARY_SIZE = 1000\nEMB_SIZE = 100\n\nembedding_matrix = np.zeros((VOCABULARY_SIZE, EMB_SIZE))\n\nLSTM_CELLS_CAPTION = 256\nLSTM_CELLS_MERGED = 1000\n\nimage_pre = Sequential()\nimage_pre.add(Dense(100, input_shape=(IMG_FEATURES_SIZE,), activation=\'relu\', name=\'fc_image\'))\nimage_pre.add(RepeatVector(MAX_SENTENCE,name=\'repeat_image\'))\n\ncaption_model = Sequential()\ncaption_model.add(Embedding(VOCABULARY_SIZE, EMB_SIZE,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SENTENCE,\n                            trainable=False, name="embedding"))\ncaption_model.add(LSTM(EMB_SIZE, return_sequences=True, name="lstm_caption"))\ncaption_model.add(TimeDistributed(Dense(100, name="td_caption")))\n\nmerge = Concatenate(axis=1,name="merge_models")([image_pre.output, caption_model.output])\nlstm = Bidirectional(LSTM(256,return_sequences=False, name="lstm_merged"),name="bidirectional_lstm")(merge)\noutput = Dense(VOCABULARY_SIZE, name="fc_merged", activation=\'softmax\')(lstm)\n\npredictive = Model([image_pre.input, caption_model.input], output)\npredictive.compile(\'sgd\', \'binary_crossentropy\')\npredictive.summary()\n'
"x = x - df_dx*lr\n\nx = x - dpf_dx*lr\ny = y - dpf_dy*lr\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d.axes3d import Axes3D\nfrom pylab import meshgrid\nfrom scipy.optimize import fmin\nimport math\n\ndef z_func(a):\n x, y = a\n return ((x-1)**2+(y-2)**2)\n \nx = np.arange(-3.0,3.0,0.1)\ny = np.arange(-3.0,3.0,0.1)\nX,Y = meshgrid(x, y) # grid of point\nZ = z_func((X, Y)) # evaluation of the function on the grid\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nsurf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,linewidth=0, antialiased=False)\nplt.show()\n\nfmin(z_func,np.array([10,10]))\n\ndef gradient_decent(x,y,lr):\n    while True:\n        d_x = 2*(x-1)\n        d_y = 2*(y-2)\n        \n        x -= d_x*lr\n        y -= d_y*lr\n        \n        if d_x &lt; 0.0001 and d_y &lt; 0.0001:\n            break\n    return x,y\n\nprint (gradient_decent(10,10,0.1)\n"
"from scipy.spatial import distance\ndef inverse_euc(a,b):\n    return 1/distance.euclidean(a, b)\n\nClassifier = KNeighborsClassifier(algorithm='ball_tree',n_neighbors=3, p=2, metric=inverse_euc)\n"
"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import Ridge, RidgeCV\n\nforest = pd.read_csv('forestfires.csv')\n#Coulmn ve row feaute adlarimi duzenledim\nforest.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),(1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)\nforest.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace=True)\n# iloc indeksin sırasıyla, loc indeksin kendisiyle işlem yapmaya olanak verir.Burada indeksledim\nX = forest.iloc[:,0:12].values\ny = forest.iloc[:,12].values\n# 30 -70 olarak train test setlerimi ayirdim\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n#x-y axis trainler arasina linear regressyon kurdum\nlr = LinearRegression()\n\n# The cross validation algorithms:\nlasso_cv = LassoCV()  # LassoCV will try to find the best alpha for you\n# ElasticNetCV will try to find the best alpha for you, for a given set of combinations of Ridge and Alpha\nenet_cv = ElasticNetCV()\nridge_cv = RidgeCV()\n\nlr.fit(X_train, y_train)\n\nlasso_cv.fit(X_train, y_train)\nenet_cv.fit(X_train, y_train)\nridge_cv.fit(X_train, y_train)\n\n#ridge regression modeli kurdum\nrr = Ridge(alpha=0.01)\nrr.fit(X_train, y_train)\nrr100 = Ridge(alpha=100)\n\nprint('LassoCV alpha:', lasso_cv.alpha_)\nprint('RidgeCV alpha:', ridge_cv.alpha_)\nprint('ElasticNetCV alpha:', enet_cv.alpha_, 'ElasticNetCV l1_ratio:', enet_cv.l1_ratio_)\nridge_alpha = ridge_cv.alpha_\nenet_alpha, enet_l1ratio = enet_cv.alpha_, enet_cv.l1_ratio_\n\nenet_new_l1ratios = [enet_l1ratio * mult for mult in [.9, .95, 1, 1.05, 1.1]]\nridge_new_alphas = [ridge_alpha * mult for mult in [.9, .95, 1, 1.05, 1.1]]\n\n# fit Enet and Ridge again:\nenet_cv = ElasticNetCV(l1_ratio=enet_new_l1ratios)\nridge_cv = RidgeCV(alphas=ridge_new_alphas)\n\nenet_cv.fit(X_train, y_train)\nridge_cv.fit(X_train, y_train)\n"
'rfe = RFE(log_reg,n_features)\nrfe.fit_transform(X_train,y_train)\nX_test = rfe.transform(X_test)\npredictions = rfe.predict(X_test)\n'
'let myURLString = "https://www.server.com/script.py"\n\nif let\n    url = URL(string: myURLString),\n    contents = try? String(contentsOfURL: url) {\n        print(contents)\n    }\n'
'x = [[each[:-1]] for each in data]\ny = [[each[-1]] for each in data]\n'
"from sklearn.metrics import confusion_matrix \ny_pred = clf.predict(X_test)\n## Obtaining confusion matrix below\nCM = confusion_matrix(y_pred, y_actual)\n\nimport glob\nimport os\n\nimport numpy as np\n\ndef obtain_y_pred(test_folder):\n    label = {'F':1, 'NF':-1}\n    test_images = glob.glob(os.path.join(test_folder, '*.jpg'))\n    y_test_ = []\n    for image in test_images:\n        y_test_.append(label[image.split('/')[-1].split('.')[0].split('_')[-1]])\n\n    return np.array(y_test_)\n"
"import quandl\ndf = quandl.get('WIKI/GOOGL')\n\ndf['HL_PCT'] = (df['Adj. High'] - df['Adj. Close']) / df['Adj. Close'] * 100.00\ndf['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open'] * 100.00\n\nprint(df.head())\n"
'sklearn.tree.DecisionTreeClassifier\nsklearn.tree.ExtraTreeClassifier\nsklearn.ensemble.ExtraTreesClassifier\nsklearn.neighbors.KNeighborsClassifier\nsklearn.neighbors.RadiusNeighborsClassifier\nsklearn.ensemble.RandomForestClassifier\n\n### Dummy Example only to test functionality\nnp.random.seed(0)\nX = np.random.randn(10,2)\ny1 = (X[:,[0]]&gt;.5).astype(int) # make dummy y1\ny2 = (X[:,[1]]&lt;.5).astype(int) # make dummy y2\ny = np.hstack([y1,y2]) # y has 2 columns\nprint("X = ",X,sep="\\n",end="\\n\\n")\nprint("y = ",y,sep="\\n",end="\\n\\n")\nrfc = RandomForestClassifier().fit(X, y) # use the same api for multi column y!\nout = rfc.predict(X)\nprint("Output = ",out,sep="\\n")\n\nX = \n[[ 1.76405235  0.40015721]\n [ 0.97873798  2.2408932 ]\n [ 1.86755799 -0.97727788]\n [ 0.95008842 -0.15135721]\n [-0.10321885  0.4105985 ]\n [ 0.14404357  1.45427351]\n [ 0.76103773  0.12167502]\n [ 0.44386323  0.33367433]\n [ 1.49407907 -0.20515826]\n [ 0.3130677  -0.85409574]]\n\ny = \n[[1 1]\n [1 0]\n [1 1]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [0 1]\n [1 1]\n [0 1]]\n\nOutput = \n[[1 1]\n [1 0]\n [1 1]\n [1 1]\n [0 1]\n [0 0]\n [1 1]\n [0 1]\n [1 1]\n [0 1]]\n'
'import numpy as np\n\nfrequency = {key:value for key,value in islice(frequency.items(), 0, n)}\ns = np.fromiter(frequency.values(), dtype=float)\n'
'r2_score(y_pred, y_true)\n'
'Out[18]: \ncol1    int32\ncol2    int32\ncol3    object\ncol4    float64\ndtype: object\n'
'import tensorflow as tf\n(train_imgs, train_labels ) , (test_imgs , test_labels ) = tf.keras.datasets.mnist.load_data()\n\ntrain_x = train_imgs.reshape(60000, 28*28)\ntest_x = test_imgs.reshape(10000, 28*28)\n\n# you need to 1-hot encode your labels\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\ndef one_hot_encode(labels, universe=None):\n    """\n    This one hot encoder works with categorical and numeric data, both.\n    `universe=` you use, if labels don\'t contain all categories/numbers.\n    Or if you want the 1hot encoding to be in a special order (not the\n    automatic alphabetic order).\n    `labels=` are the categories/numbers as labels to be 1hot encoded.\n    """\n    if universe is None:\n        universe = sorted(list(set(labels)))\n    nums = LabelEncoder().fit(universe).transform(labels)\n    one_hot = OneHotEncoder(sparse=False).fit(np.array(universe).reshape(-1, 1))\\\n              .transform(np.array(nums).reshape(-1, 1))\n    return one_hot\n\ntrain_labels_1h = one_hot_encode(train_labels)\ntest_labels_1h  = one_hot_encode(test_labels)\n\n# define epochs\nepochs = 5\n\n# define model\ndef create_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(512,activation=tf.nn.relu,input_shape = (784,)))\n    model.add(tf.keras.layers.Dense(256,activation=tf.nn.relu) )\n    model.add(tf.keras.layers.Dense(10,activation=tf.nn.softmax))\n    model.compile(loss=\'categorical_crossentropy\' , optimizer=\'adam\')\n    return model\n\n# "instanciate" a model\nmodel = create_model()\n\n# then run\nmodel.fit(train_x, train_labels_1h, epochs=epochs)\n\n'
"model.add(Dense(units=100, activation='relu'))\n\nmodel.add(Flatten())\n"
"from sklearn.model_selection import RepeatedStratifiedKFold\n\nX = a[:,:-1]\ny = a[:,-1]\n\nrskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=2)\n\nfor train_index, test_index in rskf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print(f'\\nClass 1: {((y_train==1).sum()/len(y_train))*100:.0f}%') \n    print(f'\\nShape of train: {X_train.shape[0]}')\n    print(f'Shape of test: {X_test.shape[0]}')\n\nClass 1: 73%\n\nShape of train: 33\nShape of test: 4\n\nClass 1: 73%\n\nShape of train: 33\nShape of test: 4\n\nClass 1: 73%\n\nShape of train: 33\nShape of test: 4\n\nClass 1: 73%\n\nShape of train: 33\nShape of test: 4\n...\n"
"df['Order_Date'] = pd.to_datetime(df['Order_Date'])\ndf.sort_values(['ID', 'Order_Date', 'Product'], inplace = True)\n\ndf['next_order_date'] = df['Order_Date'].shift(-1)\ndf['next_product'] = df['Product'].shift(-1)\n\ndf['days'] = df.apply(lambda row: row['next_order_date'] - row['Order_Date'] if ((row['next_product'] == 'P2') &amp; (row['Product'] == 'P1')) else np.nan, axis = 1)\n\n    ID  Order_Date  Product next_oder_date  days\n7   C-22    2018-01-15  P1  2018-09-04  232 days\n6   C-22    2018-09-04  P2  2018-09-22  NaT\n5   C-22    2018-09-22  P2  2019-09-05  NaT\n8   C-22    2019-09-05  P2  2018-07-25  NaT\n1   C-87    2018-07-25  P1  2018-08-02  8 days\n3   C-87    2018-08-02  P2  2018-11-20  NaT\n0   C-87    2018-11-20  P2  2019-07-19  NaT\n2   C-87    2019-07-19  P1  2019-12-09  143 days\n4   C-87    2019-12-09  P1  NaT         NaT\n"
"X = np.asarray([1,2,3,4,4,4,4,5])\n\ninp = Input((1,), dtype='int32')\nx = Lambda(lambda x: tf.one_hot(x[:,0], len(set(X))))(inp)\nout = Dense(20)(x)\n\nmodel = Model(inp,out)\nmodel.compile('adam','mse')\nprint(model.summary())\n\nmodel.fit(X, np.random.uniform(0,1, (len(X),20)), epochs=3)\n"
"csv = 'id, disease\\n'\nfor disease in os.listdir(root_data_path):\n    for file in os.listdir(os.path.join(root_data_path, disease)):\n        csv += f'{file},{disease}\\n'\n\nwith open('diseases_ids.csv', 'w+') as output_csv:\n    output_csv.write(csv)\n"
'\ndef cost_fxn(alpha, beta, y_actual):\n    cost = 0\n    y_predict = y_pred(alpha, beta, x_pred)\n    for i in range(100):\n        sq_error = (y_actual[i] - y_predict[i])**2\n        cost += sq_error\n    return cost/(2*100)\n\n\ndef cost_fxn(alpha, beta, y_actual):\n    cost = 0\n    y_predict = y_pred(alpha, beta, x)\n    for i in range(100):\n        sq_error = (y_actual[i] - y_predict[i])**2\n        cost += sq_error\n    return cost/(2*100)\n'
'import numpy as np\nimport scipy\n\n\nclass MultinomialNaiveBayes:\n    def __init__(self, alpha: float = 1.0):\n        self.alpha = alpha\n\n    def fit(self, X, y):\n        # Calculate priors from data\n        self.log_priors_ = np.log(np.bincount(y) / y.shape[0])\n\n        # Get indices where data belongs to separate class, creating a slicing mask.\n        class_indices = np.array(\n            np.ma.make_mask([y == current for current in range(len(self.log_priors_))])\n        )\n        # Divide dataset based on class indices\n        class_datasets = np.array([X[indices] for indices in class_indices])\n\n        # Calculate how often each class occurs and add alpha smoothing.\n        # Reshape into [n_classes, features]\n        classes_metrics = (\n            np.array([dataset.sum(axis=0) for dataset in class_datasets]).reshape(\n                len(self.log_priors_), -1\n            )\n            + self.alpha\n        )\n\n        # Calculate log likelihoods\n        self.log_likelihoods_ = np.log(\n            classes_metrics / classes_metrics.sum(axis=1)[:, np.newaxis]\n        )\n\n        return self\n\n    def predict(self, X):\n        # Return most likely class\n        return np.argmax(\n            scipy.sparse.csr_matrix.dot(X, self.log_likelihoods_.T) + self.log_priors_,\n            axis=1,\n        )\n'
'import numpy as np\n\ntweets_emotion = np.array([[3.1052819e-01, 2.7634043e-01, 1.6270137e-03, 7.7674150e-01],\n                           [5.0230421e-02, 7.7430069e-01, 7.7313791e-09, 2.0278792e-01],\n                           [9.9952579e-01, 1.3450404e-03, 5.8804121e-20, 3.2991991e-07],\n                           [3.9727339e-01, 2.8888196e-01, 1.9649005e-02, 2.1239746e-01],\n                           [1.2528910e-01, 3.2127723e-01, 3.2503495e-03, 5.5401272e-01],\n                           [5.8543805e-02, 4.5720499e-05, 2.9060062e-12, 9.3766922e-01]])\n\ntweets_emotion_class = np.argmax(tweets_emotion, axis=1)\ntweets_emotion_class\n# array([3, 1, 0, 0, 3, 3])\n'
"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\ndataframe = pd.read_csv(&quot;car-sales.csv&quot;)\ndf.head()\ny = dataframe[&quot;Price&quot;].str.replace(&quot;[\\$\\.\\,]&quot; , &quot;&quot;).astype(int)\nx = dataframe.drop(&quot;Price&quot;, axis=1)\ncat_features = [&quot;Make&quot;, &quot;Colour&quot;, &quot;Odometer&quot;, &quot;Doors&quot;, ]\noneencoder = OneHotEncoder()\ntransformer = ColumnTransformer([(&quot;onehot&quot;, oneencoder, cat_features)], remainder=&quot;passthrough&quot;)\ntransformered_x = transformer.fit_transform(x)\ntransformered_x = pd.get_dummies(dataframe[cat_features])\n\nx_train, x_test, y_train, y_test = train_test_split(transformered_x, y, test_size=.2, random_state=3)\n\nforest = RandomForestRegressor(n_estimators=200, criterion=&quot;mse&quot;, min_samples_leaf=3, min_samples_split=3, max_depth=10)\n\nforest.fit(x_train, y_train)\n\n# Explained variance score: 1 is perfect prediction\nprint('Score: %.2f' % forest.score(x_test, y_test, sample_weight=None))\nprint(forest.score(x_test, y_test))\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the residual sum of \nsquares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares \n((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it \ncan be negative (because the model can be arbitrarily worse). A constant model \nthat always predicts the expected value of y, disregarding the input features, \nwould get a R^2 score of 0.0.\n"
'import tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Sequential\nimport numpy as np\n\n(x,y),(x2,y2) = mnist.load_data()\n\ny = tf.keras.utils.to_categorical(y)  \ny2 = tf.keras.utils.to_categorical(y2)  \n\ndef pca(x):\n    x = np.reshape(x, (x.shape[0], 784)).astype(&quot;float32&quot;) / 255.\n    mean = x.mean(axis=0)\n    #print(mean)\n    #print(mean[:,None])\n    x -= mean[None, :]\n    \n    s, u, v = tf.linalg.svd(x)\n    s = tf.linalg.diag(s)\n    \n    k = 32 # DIM_REDUCED\n    projM = v[:, 0:k] #tf.matmul(u[:,0:k], s[0:k,0:k])  \n    return mean, projM\n\ndef apply_pca(mean, projM, x):\n    x = np.reshape(x, (x.shape[0], 784)).astype(&quot;float32&quot;) / 255.\n    #print(mean)\n    #print(mean[:,None])\n    x -= mean[None, :]\n\n    return tf.matmul(x, projM)\n   \nmean, projM = pca(x) \n\nx = apply_pca(mean, projM, x) \nx2 = apply_pca(mean, projM, x2) \n\n## BUILD A SUPER SIMPLE CLASSIFIC. NET\nmodel = Sequential()\nmodel.add(layers.Dense(32, activation=&quot;relu&quot;, input_shape=(32,)))\nmodel.add(layers.Dense(16, activation=&quot;relu&quot;))\nmodel.add(layers.Dense(10, activation=&quot;softmax&quot;))\n\nmodel.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;adam&quot;, metrics = [&quot;acc&quot;])\n\nmodel.fit(x,y, epochs = 5, verbose = 1, batch_size = 64, validation_data = (x2,y2))\n'
"assert len(result.predict()) == len(df)\nax = df.plot(figsize=(15,5))\nresult.predict().plot(ax=ax, label=&quot;predicted&quot;)\nplt.legend()\n\nfuture_dt = pd.Series(pd.date_range('2012-06-01', periods=60, freq='D'))\nfuture_dt_df = pd.DataFrame(index=future_dt, columns=df.columns)\nfuture_df = pd.concat([df, future_dt_df])\nfuture_df['forecast'] = result.predict(0,len(future_df)-1).values\n\n# Lets plot it\nax = future_df['sample_data'][len(df)-10:].plot(figsize=(15,5))\nfuture_df['forecast'][len(df)-10:].plot(ax=ax, label=&quot;predicted&quot;)\nplt.legend()\n\nfuture_dt = pd.Series(pd.date_range('2012-06-01', periods=60, freq='D'))\nfuture_dt = pd.DataFrame(index=future_dt)\nfuture_dt['forecast'] = result.forecast(60).values\n\nax = future_df['sample_data'][len(df)-10:].plot(figsize=(15,5))\nfuture_dt['forecast'].plot(figsize=(15,5))\nplt.legend()\n"
"data[data['4'] == 0]\n\ndata[data['4'] == 1]\n"
'tf.io.decode_png( contents, channels=0, dtype=tf.dtypes.uint8, name=None)\n'
"classes=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nwith tf.Session() as sess:\n    sess.run(init)\n    idxs = sess.run(tf.argmax(y_pred, 1), feed_dict={x:ch.test_images, y_true:ch.test_labels, hold_prob:1.0}))\n    labels = [classes[idx] for idx in idxs]\n    print(labels)\n"
'conda config --set ssl_verify no\n\npip --default-timeout=900 install tensorflow \n'
'    def partialFitChunks(self, chunks):\n\n        """ MBKmean partial fit vectorized chunks."""\n\n        for e, each_chunk in enumerate(chunks):\n            if self.verbose:\n                print \'current chunkID:\', e\n                print each_chunk\n                print each_chunk.tolist()[:10]\n\n            self.kmeans.partial_fit(each_chunk)\n            if self.verbose:\n                print \'no. of label:\', len(self.kmeans.labels_.tolist() or [])\n                print \'clustered docs:\', self.kmeans.counts_\n                print \'total docs processed:\', sum(self.kmeans.counts_.tolist())\n\n            predicted = self.kmeans.predict(each_chunk)\n            if self.verbose:\n                predicted_tolist = predicted.tolist()\n                print \'Total predicted docs:\', len(predicted_tolist)\n                counter = Counter(predicted_tolist)\n                print \'By Cluster:\',sortByIndex(counter.items(), 0, True)\n'
'y = Xorg[:,weightind]\nX = np.delete(X, weightind, 1)\n\nXimp = Imputer().fit_transform(X)\n'
'from __future__ import division\nimport numpy as np\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\ndef log_loss(y,ypred):\n    return -(y*np.log(ypred) + (1-y)*np.log(1-ypred)).mean()\n\nclass LogisticRegressionModel:\n\n    def __init__(self, n):\n        self.n = n\n        self.theta = np.zeros((1,n+1))\n        print(self.theta)\n\n\n    def SGD(self, trainingSet, epochs, minibatchsize, eta):\n        m = len(trainingSet)\n        X = np.ones((self.n+1,m))\n        Y = np.zeros((1,m))\n\n        for i, (xi, yi) in enumerate(trainingSet):\n            X[1:,i] = xi\n            Y[:,i] = yi\n\n        for epoch in xrange(epochs):\n            H = sigmoid(self.theta.dot(X))\n            derSum = (H-Y).dot(X.T)\n\n            self.theta -= eta * derSum/m\n\n            print(log_loss(Y,H))\n\n\n    def evaluate(self, testSet):\n        mtest = len(testSet)\n        X = np.ones((self.n+1,mtest))\n        Y = np.zeros((1,mtest))\n        for i, (xi, yi) in enumerate(testSet):\n            X[1:,i] = xi\n            Y[:,i] = yi\n\n        H = sigmoid(self.theta.dot(X))\n        H = (H &gt;= 0.5)\n        print((H == Y).mean() * 100)\n'
"df['**laebl**'] = df['forecast_col'].shift(-forecast_out)\n\ndf['label'] = df['forecast_col'].shift(-forecast_out)\n"
"X1 = vectorizer.fit_transform(train['question'])\n\n# The following line is changed\nY1 = vectorizer.transform(test['testing'])\n"
'x_train = np.array([[np.inf, 1, 1, np.nan], [2, 2, np.nan, 2,], [10, np.nan, 10, 10]])\n\nprint(x_train)\n\nx_train = x_train.T\nfor column in x_train:\n    # median = column[int(len(column)/2)] \n    median = np.nanmedian(column)\n    # column[column == np.nan] = median\n    column[np.isnan(column)] = median\n    column[column == np.inf] = 0\n    column[column == -np.inf] = 0\nx_train = x_train.T\n\nprint(x_train)\n\n[[ inf   1.   1.  nan]\n [  2.   2.  nan   2.]\n [ 10.  nan  10.  10.]]\n\n[[  0.    1.    1.    6. ]\n [  2.    2.    5.5   2. ]\n [ 10.    1.5  10.   10. ]]\n'
'spark.version\n# u\'2.2.0\'\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.classification import DecisionTreeClassifier\n\nfeatures_list = [[140, 0], [150, 0], [160, 1], [170, 1]]\nlabels = [0, 0, 1, 1]\n\ndd = [(labels[i], Vectors.dense(features_list[i])) for i in range(len(labels))]\ndd\n# [(0, DenseVector([140.0, 0.0])), \n#  (0, DenseVector([150.0, 0.0])), \n#  (1, DenseVector([160.0, 1.0])), \n#  (1, DenseVector([170.0, 1.0]))]\n\ndf = spark.createDataFrame(sc.parallelize(dd),schema=["label", "features"])\n\ndt = DecisionTreeClassifier(maxDepth=2, labelCol="label")\nmodel = dt.fit(df)\n\n# predict on the training set\nmodel.transform(df).show()  # \'transform\' instead of \'predict\' in Spark ML\n# +-----+-----------+-------------+-----------+----------+\n# |label|   features|rawPrediction|probability|prediction|      \n# +-----+-----------+-------------+-----------+----------+\n# |    0|[140.0,0.0]|    [2.0,0.0]|   [1.0,0.0]|      0.0|\n# |    0|[150.0,0.0]|    [2.0,0.0]|   [1.0,0.0]|      0.0| \n# |    1|[160.0,1.0]|    [0.0,2.0]|   [0.0,1.0]|      1.0| \n# |    1|[170.0,1.0]|    [0.0,2.0]|   [0.0,1.0]|      1.0|     \n# +-----+-----------+-------------+-----------+----------+\n\n# predict on a test set:\ntest = spark.createDataFrame([(Vectors.dense(180, 1),)], ["features"])\nmodel.transform(test).show() \n# +-----------+-------------+-----------+----------+\n# |   features|rawPrediction|probability|prediction|\n# +-----------+-------------+-----------+----------+ \n# |[180.0,1.0]|    [0.0,2.0]|  [0.0,1.0]|       1.0| \n# +-----------+-------------+-----------+----------+     \n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nconf = SparkConf()\nsc = SparkContext(conf=conf)\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\n'
"mean_results = [res.mean() for res in results]\nrects1 = plt.bar(index, mean_results,  bar_width,\n                 alpha=opacity,\n                 #  color='b',\n                 label='mean')\n"
'sudo rmmod nvidia_uvm\n'
"x_val = x_val.reshape(10000, 784).astype('float32') / 255.0\n\ntrain_rand_idxs = np.random.choice(50000, 700)\n\nx_val = x_val[train_rand_idxs]\n"
"&gt;&gt;&gt; x = pandas.DataFrame({'x': [1, 2, 3], 'y': [1., 2., 3.]})\n&gt;&gt;&gt; y = pandas.DataFrame(x)\n&gt;&gt;&gt; x\n   x    y\n0  1  1.0\n1  2  2.0\n2  3  3.0\n&gt;&gt;&gt; y\n   x    y\n0  1  1.0\n1  2  2.0\n2  3  3.0\n&gt;&gt;&gt; y.iloc[0, 0] = 2\n&gt;&gt;&gt; x\n   x    y\n0  2  1.0\n1  2  2.0\n2  3  3.0\n\n&gt;&gt;&gt; x = pandas.DataFrame({'x': [1, 2, 3], 'y': [1., 2., 3.]})\n&gt;&gt;&gt; numpy.log1p(x)\n          x         y\n0  0.693147  0.693147\n1  1.098612  1.098612\n2  1.386294  1.386294\n"
'words = phrase.split() \n// words : [\'This\', \'is\', \'good\'] \n\nlen(words) \n// number of words : 3 \n\nif "word" in dictionary_words:\n   print "Word is available"\n\n&gt;&gt;&gt; import enchant\n&gt;&gt;&gt; help(enchant)\n'
'from scipy.stats import multivariate_normal as mvn\nv = mvn(np.mean(c4[test_index], axis=0), X_train_cov + np.eye(30))\n'
'model1 = pca.transform(np.array(list(model1.values())))\n\nfor key, value in model1.items():\n    ...\n'
"costcla.sampling.cost_sampling(X, y, cost_mat, method='RejectionSampling', oversampling_norm=0.1, max_wc=97.5)\n"
'X_test_inversed = pca.inverse_transform(X_test_reduced)\n'
"df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\n"
"In[2]: a = [['23,24,-23,12,32,54,64,12,4,0,13,10']]\nIn[3]: result = []\nIn[4]: for sublist in a:\n  ...:     for elem in sublist:\n  ...:         result.append([int(b) for b in elem.split(',')])\n  ...: \nIn[5]: result\nOut[5]: [[23, 24, -23, 12, 32, 54, 64, 12, 4, 0, 13, 10]]\n"
'nor_features = (features-m_features) / (std_features + 1e-7)\nnor_labels = (labels-m_labels) / (std_labels + 1e-7)\n'
'Number of cases: 150K\nNumber of Classes: 64\nNumber of Features: 99\n\nimport time\nimport pandas as pd\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\n\ndef getDataset(path, x_attr, y_attr, mapping=None):\n    """\n    Extract dataset from CSV file\n    :param path: location of csv file\n    :param x_attr: list of Features Names\n    :param y_attr: Y header name in CSV file\n    :param mapping: dictionary of the classes integers\n    :return: tuple, (X, Y)\n    """\n    df = pd.read_csv(path)\n    if mapping is not None:\n        df.replace(mapping, inplace=True)\n    X = np.array(df[x_attr]).reshape(len(df), len(x_attr))\n    Y = np.array(df[y_attr])\n    return X, Y\n\ndef run(X_data, Y_data):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n    train_index, test_index = next(sss.split(X_data, Y_data))\n    X_train, X_test = X_data[train_index], X_data[test_index]\n    Y_train, Y_test = Y_data[train_index], Y_data[test_index]\n    clf = GaussianNB()\n    print("Start Modeling...")\n    now = time.time()\n    clf.fit(X_train, Y_train)\n    print(\'Elapsed Time:\', time.time() - now)\n    print("Finished Modeling...")\n    print(clf.score(X_test, Y_test))\n\nX, Y = getDataset("data.csv", [str(x) for x in range(1,98)] + ["99"], "98")\nrun(X, Y)\n\nStart Modeling...\nElapsed Time: 0.12834382057189941\nFinished Modeling...\n0.6888808876959838\n\nimport time\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport numpy as np\n\ndef getDataset(path, x_attr, y_attr, mapping=None):\n    """\n    Extract dataset from CSV file\n    :param path: location of csv file\n    :param x_attr: list of Features Names\n    :param y_attr: Y header name in CSV file\n    :param mapping: dictionary of the classes integers\n    :return: tuple, (X, Y)\n    """\n    df = pd.read_csv(path)\n    if mapping is not None:\n        df.replace(mapping, inplace=True)\n    X = np.array(df[x_attr]).reshape(len(df), len(x_attr))\n    Y = np.array(df[y_attr])\n    return X, Y\n\ndef run(X_data, Y_data):\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n    train_index, test_index = next(sss.split(X_data, Y_data))\n    X_train, X_test = X_data[train_index], X_data[test_index]\n    Y_train, Y_test = Y_data[train_index], Y_data[test_index]\n    clf = LogisticRegression(random_state=0, C=10)\n    print("Start Modeling...")\n    now = time.time()\n    clf.fit(X_train, Y_train)\n    print(\'Elapsed Time:\', time.time() - now)\n    print("Finished Modeling...")\n    print(clf.score(X_test, Y_test))\n\nX, Y = getDataset("data.csv", [str(x) for x in range(1,98)] + ["99"], "98")\nrun(X, Y)\n\nStart Modeling...\nElapsed Time: 80.22028756141663\nFinished Modeling...\n0.9141762953749468\n'
'df[df["x"] == "Hold centralized"]["x"].count()\n'
"* Use Seperate Graph.\n* Use Seperate Session.\n* reset the default Graph using tf.reset_default_graph()\n* for keras Use K.clear\n\nfrom keras.models import load_model\nwith tf.Session(graph=K.get_session().graph) as session:\n    session.run(tf.global_variables_initializer())\n    model = load_model('model.h5')\n    predictions = model.predict(input)\n"
"import glob\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\ndata = []\nfor i in glob.glob('path/to/my/data/**/*.png', recursive=True):\n    data.append(cv2.imread(i))\n\ndata = np.stack(data) # array of shape [num_images, height, width, channel]\n\ndef get_batch(data, batch_size):\n    data_size = data.shape[0]\n    indexes = list(range(data_size))\n    np.random.shuffle(indexes)\n    for i in range(0, data_size, batch_size):\n        yield data[indexes[i:i+batch_size]]\n\nimages = tf.placeholder(tf.float32, [None, height, width, channel])\nmy_net = build_network(images)\n\n...\n\nfor epoch in range(max_epochs):\n    for batch_images in get_batch(data, batch_size):\n        sess.run(train_op, feed_dict={images: batch_images})\n"
"from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n\nfor model in [RandomForestClassifier, BaggingClassifier]:\n    for n in [5, 10, 20]:\n        clf = model(random_state=12345, n_estimators=n)\n        print(clf)\n\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=5, n_jobs=1,\n            oob_score=False, random_state=12345, verbose=0,\n            warm_start=False)\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n            oob_score=False, random_state=12345, verbose=0,\n            warm_start=False)\nRandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=1,\n            oob_score=False, random_state=12345, verbose=0,\n            warm_start=False)\nBaggingClassifier(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=5, n_jobs=1, oob_score=False, random_state=12345,\n         verbose=0, warm_start=False)\nBaggingClassifier(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=10, n_jobs=1, oob_score=False, random_state=12345,\n         verbose=0, warm_start=False)\nBaggingClassifier(base_estimator=None, bootstrap=True,\n         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n         n_estimators=20, n_jobs=1, oob_score=False, random_state=12345,\n         verbose=0, warm_start=False)\n"
'model=best_model.fit(X_train,Y_train)\n'
"ls, cs = np.unique(labels,return_counts=True)\ndic = dict(zip(ls,cs))\nidx = [i for i,label in enumerate(labels) if dic[label] &lt;100 and label &gt;= 0]\n\nplt.scatter(only_xy[idx, 0], only_xy[idx, 1],\n        c=clustering.labels_[idx], cmap='rainbow')\n"
"model = keras.Sequential([\n    keras.layers.Flatten(input_shape=(28, 28)),\n    keras.layers.Dense(128, activation=tf.nn.sigmoid),\n    keras.layers.Dense(10, activation=tf.nn.sigmoid)])\n\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n"
'model.feature_importances_\n'
'import numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score\n\ny_true = np.array([0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1])\ny_predicted = np.array([0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,1,1,1])\n\nprint(confusion_matrix(y_true, y_predicted))\nprint("Accuracy score : %.3f" % accuracy_score(y_true, y_predicted))\nprint("Balanced accuracy score : %.3f" % balanced_accuracy_score(y_true, y_predicted))\n\n[[9 1]\n [2 7]]\nAccuracy score : 0.842\nBalanced accuracy score : 0.839\n'
'a = np.ones((16755, 1)).astype(int)\n\nm = x.shape[0]\na = np.ones((m, 1)).astype(int)\nx = np.append(arr = a,values = x, axis = 1)\n'
'if label in ["car", "buses", "trucks"]:\n    label="vehicles"\n'
'&lt;form method="post" action="{{ url_for(\'\') }}" enctype="multipart/form-data"&gt;\n\ndef upload_image():\n    try:\n        if request.method == \'POST\':\n            ALLOWED_EXTENSIONS = [".png", ".jpg", ".jpeg", ".gif"]\n\n            file = request.files[\'image\']\n            if file and any(split_filename(file.filename)[1] == s for s in ALLOWED_EXTENSIONS):\n                folder = app.config[\'UPLOAD_FOLDER\']\n                pathName = app.config[\'IMAGE_PATH\'] + datetime.utcnow().strftime(\n                    \'%Y\\\\%m\\\\\')\n                if not os.path.exists(os.path.join(folder + pathName)):\n                    os.makedirs(folder + pathName)\n                filename = str(uuid.uuid4()) + split_filename(file.filename)[1]\n                file.save(os.path.join(folder + pathName, filename))\n                path = pathlib.PureWindowsPath(pathName + filename).as_posix()\n                return url_for(\'main.get_file\', path=path, _external=True)\n            else:\n                return \'Please Choose PNG, JPG, JPEG, GIF Image, Not \' +  split_filename(file.filename)[1], 404\n    except Exception as error:\n        return error.__str__()\n\n@mn.route(\'/file/&lt;path:path&gt;\', methods=[\'GET\'])\ndef get_file(path):\n    try:\n        return send_file(os.path.join(app.config[\'UPLOAD_FOLDER\'], path))\n    except :\n        return send_file(os.path.join(app.config[\'UPLOAD_FOLDER\'], \'404.png\'))\n'
"import pandas as pd\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\n\nconcrete_data = pd.read_csv('https://cocl.us/concrete_data')\n\nn_cols = concrete_data.shape[1]\nmodel = Sequential()\nmodel.add(Dense(units=10, activation='relu', input_shape=(n_cols-1,)))           \nmodel.add(Dense(units=1))\nmodel.compile(loss='mean_squared_error',\n          optimizer='adam')\n\n\ny = concrete_data.Cement\nx = concrete_data.drop('Cement', axis=1)\nxTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.3)\n\nmodel.fit(xTrain, yTrain, epochs=50)\n"
"import numpy as np\nimport catboost\nfrom catboost import CatBoostRegressor\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\ny = boston['target']\nX = boston['data']\n\nmodel = CatBoostRegressor(depth=2, verbose=False, iterations=5).fit(X, y)\n\nmodel.plot_tree(tree_idx=0)\n\nmodel.plot_tree(tree_idx=4)\n"
"model.add(Dense(32,input_shape=(n,),activation='relu')\n\ninputs = Input(shape=(n,))                 # input layer\nx = Dense(32, activation='relu')(inputs)   # 1st hidden layer\n"
"import pandas as pd\n# Dictionary of items\nd = {'words' : [ [ 'cheap', 'expensive'], ['excited'], ['hot', 'summer'], ['money'], ['rain'] ], \n     'category': ['price', 'entertainment', 'weather', 'price', 'weather']}\n# Convert dictionary to dataframe\ndf = pd.DataFrame(d)\n# Unpack the list of 'words' by joining with ','\ndf.words = df.words.str.join(',')\n# Groupby and aggregate to get the unique 'words' for each 'category'\nnew_df = df.groupby('category').agg({'words':'unique'})\n# Since the groupby results in a list of items, unpack by joining with ','\nnew_df.words = new_df.words.str.join(',')\n# reset_index() to convert the groupby object to a dataframe\n# This is optional. If not used, 'category' will the index of the dataframe.\nnew_df.reset_index(inplace=True)\nnew_df\n"
"model.add(layers.Dense(7 , activation='softmax'))\n"
'new_testdata_tfidf= tfidf.transform([new_sentence])\n'
'model = DecisionTreeClassifier\n\nmodel = DecisionTreeClassifier()\n'
"import noisereduce\n\ntemp = noisereduce.reduce_noise(noise_clip=noise_clip,audio_clip=temp,verbose=True)\n\n signal, fs = librosa.load(path)\n signln = len(signal)\n avg_energy = np.sum(signal ** 2) / float(signln) #avg_energy of acual signal\nf_d = 0.02 #frame duration\nperc = 0.01\n\nflag = True\nj = 0\nf_length = fs * f_d #frame length is `frame per second(fs) * frame_duration(f_d)` \nsignln = len(signal)\nretsig = []\nnoise = signal[0:441] # just considering first little part as noise\navg_energy = np.sum(signal ** 2) / float(signln)\nwhile j &lt; signln:\n      subsig = signal[int(j): int(j) + int(f_length)]\n      average_energy = np.sum(subsig ** 2) / float(len(subsig)) # avg energy of current frame\n      if average_energy &lt;= avg_energy: #if enegy of the current frame is less than actual signal then then we can confirm that this frame as silence or noise part\n            if flag: #to get first noise or silence appearing on the signal \n                  noise = subsig #if you want to get all the noise frame, then just create a list and append it(noise_list.append(subsig)) and also don't use the flag condition\n                  flag = False\n            \n      else: # if avg energy of current frame is grater than actual signal energy then this frame contain the data \n           retsig.append(subsig) # so you need to add that frame to new variable\n      j += f_length\n"
"from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\npipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n\n// then \npipe.fit...\npipe.score...\npipe.predict...\n"
'predictions = svc_1.predict(new_face_image) \nprint (&quot;Confusion Matrix:&quot;)\nprint (metrics.confusion_matrix(y_test, predictions))\n\npredictions = svc_1.predict(new_face_image) \n# change this to what you expect but shape=(30,)\nexpected=np.ones(len(new_face_image))\nprint (&quot;Confusion Matrix:&quot;)\nprint (metrics.confusion_matrix(expected, predictions))\n\npredictions = svc_1.predict(x_test) \nprint (&quot;Confusion Matrix:&quot;)\nprint (metrics.confusion_matrix(y_test, predictions))\n'
'import numpy as np\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\n\n# dummy data\nrng = np.random.RandomState(1)  # for reproducibility\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\nestimator = DecisionTreeRegressor(max_depth=3)\nestimator.fit(X, y)\n\nimport graphviz \ndot_data = export_graphviz(estimator, out_file=None) \n\ngraph = graphviz.Source(dot_data) \ngraph\n\non_leaf = estimator.apply(X)\non_leaf\n# result:\narray([ 3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  6,  6,  6,  6,  6,  6,\n        6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n        6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n       10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 13, 13, 13,\n       13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14])\n\nlen(np.unique(on_leaf))\n# 8\n\nleaves = []\nfor i in np.unique(on_leaf):\n  leaves.append(y[np.argwhere(on_leaf==i)]) \n\nlen(leaves)\n# 8\n\nleaves[0]\n# array([[-1.1493464]])\n\nleaves[1]\n# result:\narray([[ 0.09131401],\n       [ 0.09668352],\n       [ 0.13651039],\n       [ 0.19403525],\n       [-0.12383814],\n       [ 0.26365828],\n       [ 0.41252216],\n       [ 0.44546446],\n       [ 0.47215529],\n       [-0.26319138]])\n\nlen(leaves[1])\n# 10\n\nleaves[1].mean()\n# 0.17253138570808904\n\nleaves[7]\n# result:\narray([[-0.99994398],\n       [-0.99703245],\n       [-0.99170146],\n       [-0.9732277 ]])\n\nleaves[7].mean()\n# -0.9904763973694366\n\non_leaf = estimator.apply(X)\n\nleaves = []\nfor i in np.unique(on_leaf):\n  leaves.append(y[np.argwhere(on_leaf==i)]) \n'
"from sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np\nimport mglearn\nimport matplotlib.pyplot as plt\n\ndata = fetch_california_housing()\n\nc = np.array([1 if y &gt; np.median(data['target']) else 0 for y in data['target']])\n\nX_train, X_test, c_train, c_test = train_test_split(data['data'], c, random_state=0)\n\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n\nfor counter in range(3):\n\n    clf = KNeighborsClassifier(n_neighbors=counter+1)\n\n    # fit the model to the training set\n    clf.fit(X_train, c_train)\n\n    # extract the model predictions on the test set\n    c_pred = clf.predict(X_test)\n\n    # plot the model predictions\n    plt.tight_layout()\n    mglearn.discrete_scatter(X_test[:,0], X_test[:,1], c_pred, ax=ax[counter])\n    plt.legend([&quot;Class 0&quot;, &quot;Class 1&quot;], loc=4)\n    plt.xlabel(&quot;First feature&quot;)\n    plt.ylabel(&quot;Second feature&quot;)\n"
"logs.get('acc')\n\nlogs.get('accuracy')\n"
'from collections import Counter\ncounter = Counter()\nfor s in sentences:\n  words = s.split(" ")\n  for i in range(len(words)):\n    counter.add(words[i])\n    if i &gt; 0: counter.add((words[i-1], words[i]))\n'
"import numpy as np\nfrom sklearn import decomposition\nfrom sklearn import linear_model\n\nfeatures_train = np.loadtxt('C:\\\\Users\\\\bruker\\\\Desktop\\\\Data\\\\CFD_features.txt', delimiter=',')\nratings_train = np.loadtxt('C:\\\\Users\\\\bruker\\\\Desktop\\\\Data\\\\CFD_ratings.txt', delimiter=',')\n\npca = decomposition.PCA(n_components=13)\npca.fit(features_train)\nfeatures_train = pca.transform(features_train)\nregr = linear_model.LinearRegression()\nregr.fit(features_train, ratings_train)\n\nfeatures_test = np.loadtxt('C:\\\\Users\\\\bruker\\\\Desktop\\\\Data\\\\CFD_features_Test.txt', delimiter=',')\n\nfeatures_test = pca.transform(features_test)\npredictions = regr.predict(features_test)\n"
'import tensorflow as tf\nimport numpy as np\nimport random\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport random\nimport pickle\nfrom collections import Counter\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nhm_lines = 100000\n\ndef create_lexicon(pos,neg):\n    lexicon = []\n    with open(pos,\'r\') as f:\n        contents = f.readlines()\n        for l in contents[:hm_lines]:\n            all_words = word_tokenize(l)\n            lexicon += list(all_words)\n\n    with open(neg,\'r\') as f:\n        contents = f.readlines()\n        for l in contents[:hm_lines]:\n            all_words = word_tokenize(l)\n            lexicon += list(all_words)\n\n    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n    w_counts = Counter(lexicon)\n    l2 = []\n    for w in w_counts:\n        #print(w_counts[w])\n        if 1000 &gt; w_counts[w] &gt; 50:\n            l2.append(w)\n    print(len(l2))\n    return l2\n\n\ndef sample_handling(sample,lexicon,classification):\n    featureset = []\n    with open(sample,\'r\') as f:\n        contents = f.readlines()\n        for l in contents[:hm_lines]:\n            current_words = word_tokenize(l.lower())\n            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n            features = np.zeros(len(lexicon))\n            for word in current_words:\n                if word.lower() in lexicon:\n                    index_value = lexicon.index(word.lower())\n                    features[index_value] += 1\n            features = list(features)\n            featureset.append([features,classification])\n    return featureset\n\ndef create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n    lexicon = create_lexicon(pos,neg)\n    features = []\n    features += sample_handling(\'pos.txt\',lexicon,[1,0])\n    features += sample_handling(\'neg.txt\',lexicon,[0,1])\n    random.shuffle(features)\n    features = np.array(features)\n\n    testing_size = int(test_size*len(features))\n\n    train_x = list(features[:,0][:-testing_size])\n    train_y = list(features[:,1][:-testing_size])\n    test_x = list(features[:,0][-testing_size:])\n    test_y = list(features[:,1][-testing_size:])\n\n    return train_x,train_y,test_x,test_y\n\ntrain_x,train_y,test_x,test_y = create_feature_sets_and_labels(\'pos.txt\',\'neg.txt\')\nn_nodes_hl1 = 2\n\n\nn_classes = 2\n\nbatch_size = 100\n\nx = tf.placeholder(\'float\',[None,len(train_x[0])])\ny = tf.placeholder(\'float\')\n\n#(input_data*weights) + biases\ndef neural_network_model(data):\n    hidden_1_layer = {\'weights\': tf.Variable(tf.random_normal([len(train_x[0]),n_nodes_hl1])),\n                      \'biases\': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n    output = tf.matmul(data,hidden_1_layer[\'weights\']) + hidden_1_layer[\'biases\']\n    return output\n\n\n\n\ndef train_neural_network(x):\n    prediction = neural_network_model(x)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n\n    hm_epochs = 1\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n\n        for epoch in range(hm_epochs):\n            epoch_loss=0\n            i=0\n            while i &lt; len(train_x):\n                start = i\n                end = i + batch_size\n                batch_x = np.array(train_x[start:end])\n                batch_y = np.array(train_y[start:end])\n\n                _,c = sess.run([optimizer,cost] , feed_dict = {x: batch_x , y : batch_y})\n                epoch_loss+= c\n                i+= batch_size\n            print("Epoch",epoch , \'completed out of \' ,hm_epochs, \' loss: \', epoch_loss )\n\n\n\n        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n        accuracy = tf.reduce_mean(tf.cast(correct, \'float\'))\n        print(\'Accuracy: \', accuracy.eval({x:test_x , y: test_y}))\n\ntrain_neural_network(x)\n\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport random\nimport pickle\nfrom collections import Counter\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nhm_lines = 100000\n\ndef create_lexicon(pos,neg):\n    lexicon = []\n    with open(pos,\'r\') as f:\n        contents = f.readlines()\n        for l in contents[:hm_lines]:\n            all_words = word_tokenize(l)\n            lexicon += list(all_words)\n\n    with open(neg,\'r\') as f:\n        contents = f.readlines()\n        for l in contents[:hm_lines]:\n            all_words = word_tokenize(l)\n            lexicon += list(all_words)\n\n    lexicon = [lemmatizer.lemmatize(i) for i in lexicon]\n    w_counts = Counter(lexicon)\n    l2 = []\n    for w in w_counts:\n        #print(w_counts[w])\n        if 1000 &gt; w_counts[w] &gt; 50:\n            l2.append(w)\n    print(len(l2))\n    return l2\n\n\ndef sample_handling(sample,lexicon,classification):\n    featureset = []\n    with open(sample,\'r\') as f:\n        contents = f.readlines()\n        for l in contents[:hm_lines]:\n            current_words = word_tokenize(l.lower())\n            current_words = [lemmatizer.lemmatize(i) for i in current_words]\n            features = np.zeros(len(lexicon))\n            for word in current_words:\n                if word.lower() in lexicon:\n                    index_value = lexicon.index(word.lower())\n                    features[index_value] += 1\n            features = list(features)\n            featureset.append([features,classification])\n    return featureset\n\ndef create_feature_sets_and_labels(pos,neg,test_size = 0.1):\n    lexicon = create_lexicon(pos,neg)\n    features = []\n    features += sample_handling(\'pos.txt\',lexicon,[1,0])\n    features += sample_handling(\'neg.txt\',lexicon,[0,1])\n    random.shuffle(features)\n    features = np.array(features)\n\n    testing_size = int(test_size*len(features))\n\n    train_x = list(features[:,0][:-testing_size])\n    train_y = list(features[:,1][:-testing_size])\n    test_x = list(features[:,0][-testing_size:])\n    test_y = list(features[:,1][-testing_size:])\n\n    return train_x,train_y,test_x,test_y\n\ntrain_x,train_y,test_x,test_y = create_feature_sets_and_labels(\'pos.txt\',\'neg.txt\')\nn_nodes_hl1 = 2\n\n\nn_classes = 2\n\nbatch_size = 100\n\nx = tf.placeholder(\'float\',[None,len(train_x[0])])\ny = tf.placeholder(\'float\')\n\nimport tensorflow as tf\n\nimport numpy as np\ntrain_x,train_y,test_x,test_y = create_feature_sets_and_labels(\'pos.txt\',\'neg.txt\')\nn_nodes_hl1 = 4\nn_nodes_hl2 = 3\nn_nodes_hl3 = 2\n\nn_classes = 2\n\nbatch_size = 100\n\nx = tf.placeholder(\'float\',[None,len(train_x[0])])\ny = tf.placeholder(\'float\')\n\ndef neural_network_model(data):\n    hidden_1_layer = {\'weights\': tf.Variable(tf.random_normal([len(train_x[0]),n_nodes_hl1])),\n                      \'biases\': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n\n    hidden_2_layer = {\'weights\': tf.Variable(tf.random_normal([n_nodes_hl1,n_nodes_hl2])),\n                      \'biases\': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n\n    hidden_3_layer = {\'weights\': tf.Variable(tf.random_normal([n_nodes_hl2,n_nodes_hl3])),\n                      \'biases\': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n\n    output_layer = {\'weights\': tf.Variable(tf.random_normal([n_nodes_hl3,n_classes])),\n                      \'biases\': tf.Variable(tf.random_normal([n_classes]))}\n\n    l1= tf.add(tf.matmul(data, hidden_1_layer[\'weights\']) , hidden_1_layer[\'biases\'])\n    l1 = tf.nn.relu(l1)\n\n    l2= tf.add(tf.matmul(l1, hidden_2_layer[\'weights\']) , hidden_2_layer[\'biases\'])\n    l2 = tf.nn.relu(l2)\n\n    l3= tf.add(tf.matmul(l2, hidden_3_layer[\'weights\']) , hidden_3_layer[\'biases\'])\n    l3 = tf.nn.relu(l3)\n\n    output = tf.matmul(l3, output_layer[\'weights\']) + output_layer[\'biases\']\n    return output\n\ndef train_neural_network(x):\n    prediction = neural_network_model(x)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n\n    hm_epochs = 10\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n\n        for epoch in range(hm_epochs):\n            epoch_loss=0\n            i=0\n            while i &lt; len(train_x):\n                start = i\n                end = i + batch_size\n                batch_x = np.array(train_x[start:end])\n                batch_y = np.array(train_y[start:end])\n\n                _,c = sess.run([optimizer,cost] , feed_dict = {x: batch_x , y : batch_y})\n                epoch_loss+= c\n                i+= batch_size\n            print("Epoch",epoch , \'completed out of \' ,hm_epochs, \' loss: \', epoch_loss )\n\n\n\n        correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n        accuracy = tf.reduce_mean(tf.cast(correct, \'float\'))\n        print(\'Accuracy: \', accuracy.eval({x:test_x , y: test_y}))\n\n\n\ntrain_neural_network(x)\n'
"data = pd.DataFrame(['A','B','C'],columns=['TYPE'])\n\ndata\nOut[24]: \n  TYPE\n0    A\n1    B\n2    C\n\n#Use get_dummies on pandas dataframe. \n\nnew_data = pd.get_dummies(data,columns=['TYPE'],prefix='TYPE_')\n\nnew_data\nOut[26]: \n   TYPE__A  TYPE__B  TYPE__C\n0      1.0      0.0      0.0\n1      0.0      1.0      0.0\n2      0.0      0.0      1.0\n"
'import quandl\n'
'sklearn.tree.DecisionTreeClassifier\nsklearn.tree.ExtraTreeClassifier\nsklearn.ensemble.ExtraTreesClassifier\nsklearn.neighbors.KNeighborsClassifier\n...\n...\n'
'def sigmoid_prime(z):\n    return sigmoid(z)*(1-sigmoid(z))\n'
'clf.predict_proba([[8,8]])\n'
'X_train = rtrain_df.drop("damage_grade", axis=1) \nY_train = rtrain_df["damage_grade"] \nX_test = rtest_df.drop("building_id", axis=1).copy() \nX_train.shape, Y_train.shape, X_test.shape\n\nimport keras \nfrom keras.models import Sequential \nfrom keras.layers import Dense, Dropout, Activation \nfrom keras.optimizers import SGD\n\nfrom keras.utils import np_utils\nY_train_cat = np_utils.to_categorical(Y_train) # converts into 5 categorical features\n\nmodel = Sequential() \nmodel.add(Dense(64, activation=\'relu\', input_dim=46))\nmodel.add(Dropout(0.5)) \nmodel.add(Dense(64, activation=\'relu\')) \nmodel.add(Dropout(0.5)) \nmodel.add(Dense(5, activation=\'softmax\')) \n\n# last Dense layer is the output layer that\'ll produce the probabilities for the 5 \n# outputs\n\nmodel.compile(optimizer=\'rmsprop\', loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\nmodel.fit(X_train, Y_train_cat, epochs=20, batch_size=128)\n\nimport numpy as np\n\npredictions = model.predict(X_test)\nresult = np.argmax(predictions,axis=1) # sets the output with max probability to 1\n'
'import numpy, scipy, matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom scipy.optimize import differential_evolution\nimport warnings\n\nxData = numpy.array([19.1647, 18.0189, 16.9550, 15.7683, 14.7044, 13.6269, 12.6040, 11.4309, 10.2987, 9.23465, 8.18440, 7.89789, 7.62498, 7.36571, 7.01106, 6.71094, 6.46548, 6.27436, 6.16543, 6.05569, 5.91904, 5.78247, 5.53661, 4.85425, 4.29468, 3.74888, 3.16206, 2.58882, 1.93371, 1.52426, 1.14211, 0.719035, 0.377708, 0.0226971, -0.223181, -0.537231, -0.878491, -1.27484, -1.45266, -1.57583, -1.61717])\nyData = numpy.array([0.644557, 0.641059, 0.637555, 0.634059, 0.634135, 0.631825, 0.631899, 0.627209, 0.622516, 0.617818, 0.616103, 0.613736, 0.610175, 0.606613, 0.605445, 0.603676, 0.604887, 0.600127, 0.604909, 0.588207, 0.581056, 0.576292, 0.566761, 0.555472, 0.545367, 0.538842, 0.529336, 0.518635, 0.506747, 0.499018, 0.491885, 0.484754, 0.475230, 0.464514, 0.454387, 0.444861, 0.437128, 0.415076, 0.401363, 0.390034, 0.378698])\n\n\ndef func(x, B0, B1, B2, B3, B4, B2p1, B2p, p):\n    returnVal = B0 # start with B0 and add the other terms\n\n    returnVal += B1 * numpy.sin(2.0 * numpy.pi * x)\n    returnVal += B2 * numpy.cos(2.0 * numpy.pi * x)\n\n    returnVal += B3 * numpy.sin(2.0 * numpy.pi * 2.0 * x)\n    returnVal += B4 * numpy.cos(2.0 * numpy.pi * 2.0 * x)\n\n    returnVal += B2p1 * numpy.sin(2.0 * numpy.pi * p * x)\n    returnVal += B2p * numpy.cos(2.0 * numpy.pi * p * x)\n\n    return  returnVal\n\n\n# function for genetic algorithm to minimize (sum of squared error)\ndef sumOfSquaredError(parameterTuple):\n    warnings.filterwarnings("ignore") # do not print warnings by genetic algorithm\n    val = func(xData, *parameterTuple)\n    return numpy.sum((yData - val) ** 2.0)\n\n\ndef generate_Initial_Parameters():\n    parameterBounds = []\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B0\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B1\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B2\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B3\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B4\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B2p1\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for B2p\n    parameterBounds.append([-1.0, 1.0]) # seach bounds for p\n\n    # "seed" the numpy random number generator for repeatable results\n    result = differential_evolution(sumOfSquaredError, parameterBounds, seed=3)\n    return result.x\n\n# generate initial parameter values\ngeneticParameters = generate_Initial_Parameters()\n\n# curve fit the test data\nfittedParameters, pcov = curve_fit(func, xData, yData, geneticParameters)\n\nprint(\'Parameters\', fittedParameters)\nprint()\n\nmodelPredictions = func(xData, *fittedParameters) \n\nabsError = modelPredictions - yData\n\nSE = numpy.square(absError) # squared errors\nMSE = numpy.mean(SE) # mean squared errors\nRMSE = numpy.sqrt(MSE) # Root Mean Squared Error, RMSE\nRsquared = 1.0 - (numpy.var(absError) / numpy.var(yData))\nprint(\'RMSE:\', RMSE)\nprint(\'R-squared:\', Rsquared)\n\nprint()\n\n\n##########################################################\n# graphics output section\ndef ModelAndScatterPlot(graphWidth, graphHeight):\n    f = plt.figure(figsize=(graphWidth/100.0, graphHeight/100.0), dpi=100)\n    axes = f.add_subplot(111)\n\n    # first the raw data as a scatter plot\n    axes.plot(xData, yData,  \'D\')\n\n    # create data for the fitted equation plot\n    xModel = numpy.linspace(min(xData), max(xData))\n    yModel = func(xModel, *fittedParameters)\n\n    # now the model as a line plot\n    axes.plot(xModel, yModel)\n\n    axes.set_xlabel(\'X Data\') # X axis data label\n    axes.set_ylabel(\'Y Data\') # Y axis data label\n\n    plt.show()\n    plt.close(\'all\') # clean up after using pyplot\n\ngraphWidth = 800\ngraphHeight = 600\nModelAndScatterPlot(graphWidth, graphHeight)\n'
"pandas.get_dummies(data) \n\npandas.get_dummies(df_trainFeautres['workclass'])\n"
"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train /= 255.\nX_test /= 255.\n"
'import numpy as np\nfrom sklearn import metrics\n\ny = np.array([0, 1, 1, 2, 2]) # 3-class problem\nscores = np.array([0.05, 0.1, 0.4, 0.35, 0.8])\nfpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)  # works OK, no error\n'
'KMeans(n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, random_state=None)\n\nLogisticRegression(C=1.0, fit_intercept=True, random_state=None)\n\nKMeans(n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, random_state=0)\n\nLogisticRegression(C=1.0, fit_intercept=True, random_state=0)\n'
"logisticR = LogisticRegression(random_state=0, max_iter=800, \n    solver='saga', multi_class='multinomial')\n"
'tfr = tfr[None, :, :]\n'
'for i in range(len(kmeans.cluster_centers)):\n  print("Cluster", i)\n  print("Center:", kmeans.cluster_centers_[i])\n  print("Size:", sum(kmeans.labels_ == i))\n'
"loss_list = []\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    for t in range(100):\n        loss_val, _ = sess.run([loss, train])\n        loss_list.append(loss_val)\n\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\n\ntf.reset_default_graph()\n\nx_train = [[0.,0.],[1.,1.],[1.,0.],[0.,1.]]\ny_train = [[0.],[0.],[1.],[1.]]\n\nx_test =  [[0.,0.],[.5,.5],[.5,0.],[0.,.5]]\ny_test = [[0.],[0.],[2.],[2.]]\n\n# use placeholder instead so you can have different inputs\nx = tf.placeholder('float32', [None, 2])\ny = tf.placeholder('float32',)\n\n# Layer 1 = the 2x3 hidden sigmoid\nm1 = tf.Variable(tf.random_uniform([2,3], minval=0.1, maxval=0.9, dtype=tf.float32))\nb1 = tf.Variable(tf.random_uniform([3], minval=0.1, maxval=0.9, dtype=tf.float32))\nh1 = tf.sigmoid(tf.matmul(x, m1) + b1)\n# Layer 2 = the 3x1 sigmoid output\nm2 = tf.Variable(tf.random_uniform([3,1], minval=0.1, maxval=0.9, dtype=tf.float32))\nb2 = tf.Variable(tf.random_uniform([1], minval=0.1, maxval=0.9, dtype=tf.float32))\ny_out = tf.sigmoid(tf.matmul(h1, m2) + b2)\n### loss\n# loss : sum of the squares of y0 - y_out\nloss = tf.reduce_sum(tf.square(y - y_out))\n# training step : gradient decent (1.0) to minimize loss\ntrain = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n\n# the two feed dictionaries\nfeeddict_train = {x: x_train, y: y_train}\nfeeddict_test = {x: x_test, y: y_test}\n\n### training\n# run 500 times using all the X and Y\n# print out the loss and any other interesting info\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    train_loss, test_loss = [], []\n    for step in range(500):\n        loss_train, _ = sess.run([loss, train], feed_dict=feeddict_train)\n        train_loss.append(loss_train)\n\n        # insert whatever logic is needed \n\n        # under the same tensorflow graph (in the session), use another feed dictionary \n        loss_test = sess.run(loss, feed_dict=feeddict_test)\n        test_loss.append(loss_test)\n\nplt.plot(train_loss, 'r', label='train_loss')\nplt.plot(test_loss, 'b', label='test_loss')\nplt.legend(loc='best')\n"
"#for saving the model\nmodel_data = pd.DataFrame(columns=['user','model'])\ntemp_model = RandomForestClassifier().fit(X,y)\nnew = pd.DataFrame({'user':[user_id],'model':[temp_model]})\nmodel_data = model_data.append(new)\npacked_model = jsonpickle.pickler.Pickler.flatten(model_data)\n\n#for loading the model\nunpacked_model = jsonpickle.unpickler.Unpickler.restore(packed_model) #this should be in the begining of your flask file - loaded into the memory\nuser_model=unpacked_model.at(user_id,'model') #this should be inside every api call\n"
'ROI = 255 - image[y:y+h, x:x+w]\n\nimport cv2\nfrom imutils import contours\n\nimage = cv2.imread(\'1.png\')\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nthresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n\ncnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncnts = cnts[0] if len(cnts) == 2 else cnts[1]\ncnts, _ = contours.sort_contours(cnts, method="left-to-right")\n\nROI_number = 0\nfor c in cnts:\n    area = cv2.contourArea(c)\n    if area &gt; 10:\n        x,y,w,h = cv2.boundingRect(c)\n        ROI = 255 - image[y:y+h, x:x+w]\n        cv2.imwrite(\'ROI_{}.png\'.format(ROI_number), ROI)\n        cv2.rectangle(image, (x, y), (x + w, y + h), (36,255,12), 1)\n        ROI_number += 1\ncv2.imshow(\'thresh\', thresh)\ncv2.imshow(\'image\', image)\ncv2.waitKey()\n'
"import pandas as pd\nimport ast\ndata = pd.read_csv('injury.txt',sep=';',converters={'Injury_Type': ast.literal_eval, 'Organ': ast.literal_eval})\n\ndata\nInjury_Type Organ   PositionGroup   Age speed   daysmissing\n0   [back]  [back]  LW  30.9295 5.239167    20\n1   [torn]  [biceps]    CB  26.2600 4.530000    10\n2   [torn]  [ACL]   LB  26.1500 4.440000    5\n\n\ndata['Injury_Type']\ndata['Injury_Type_String'] = [' '.join(l) for l in data['Injury_Type']]\n\ndata['Organ']\ndata['Organ_String'] = [' '.join(l) for l in data['Organ']]\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\nX = vectorizer.fit_transform(list(data['Organ_String']))\nOrgan_df = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\nOrgan_df\nacl back    biceps\n0   0   1   0\n1   0   0   1\n2   1   0   0\n\nY = vectorizer.fit_transform(list(data['Injury_Type_String']))\nInjury_Type_df = pd.DataFrame(Y.toarray(),columns=vectorizer.get_feature_names())\nInjury_Type_df\nback    torn\n0   1   0\n1   0   1\n2   0   1\n\n\ndata = pd.concat([data,Organ_df],axis=1)\ndata = pd.concat([data,Injury_Type_df],axis=1)\n\ndel data['Injury_Type']\ndel data['Injury_Type_String']\ndel data['Organ']\ndel data['Organ_String']\n\nprint(data)\n  PositionGroup      Age     speed  daysmissing  acl  back  biceps  back  torn\n0            LW  30.9295  5.239167           20    0     1       0     1     0\n1            CB  26.2600  4.530000           10    0     0       1     0     1\n2            LB  26.1500  4.440000            5    1     0       0     0     1\n\nPositionGroup_df = pd.get_dummies(data['PositionGroup'])\ndata = pd.concat([data,PositionGroup_df],axis=1)\n\ndel data['PositionGroup']\n\n\nprint(data)\n       Age     speed  daysmissing  acl  back  biceps  back  torn  CB  LB  LW\n0  30.9295  5.239167           20    0     1       0     1     0   0   0   1\n1  26.2600  4.530000           10    0     0       1     0     1   1   0   0\n2  26.1500  4.440000            5    1     0       0     0     1   0   1   0\n\ny = data.pop('daysmissing')\nX = data\n\nfrom sklearn.ensemble import RandomForestRegressor\nregr = RandomForestRegressor()\nregr.fit(X,y)\n"
"from tensorflow.keras import layers, models\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (5, 5), activation='relu', padding='same', input_shape=(800, 800, 1)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel.add(layers.Conv2DTranspose(64, (3, 3), strides=(2,2), padding='same', activation='relu'))\nmodel.add(layers.Conv2DTranspose(1, (3, 3), strides=(2,2), padding='same', activation='relu'))\n\nmodel.summary()\n"
'model.evaluate([[50,32],[16,18]],  [82,34], verbose=2)\n\nmodel.evaluate([[[50,32],[16,18]]],  [82,34], verbose=2)\n'
"for i in range(0,60):\n    records.append([df.columns[j]+'='+str(df.values[i,j]) for j in range(0,5)])\n"
"test_dictionary=[\n     {'winner':'ross','loser:'chandler'},\n     {'winner':'rachael','loser:'phoebe'},\n     {'winner':'joey','loser:'monica'},\n     {'winner':'gunther','loser:'chandler'}\n]\n\nfor contest in test_dictionary:\n    print (contest)\n\nfor line_number, contect in test_dictionary:\n    print (line_number,contest)\n\nfor line_number, contect in test_dictionary[:-1]:\n    print (line_number,contest)\n    print (line_number+1,test_dictionary[line_number+1])\n\nfor line_number in range(len(test_dictionary)-1]:\n    print (line_number,test_dictionary[line_number])\n    print (line_number+1,test_dictionary[line_number+1])\n"
'for image_x, image_y in zip(training_low_imgs, training_high_imgs):\n'
'preds=clf.best_estimator_.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nprint confusion_matrix(y_test, preds)\n'
'logdir = pathlib.Path(tempfile.mkdtemp())/"tensorboard_logs"\nshutil.rmtree(logdir, ignore_errors=True)\n'
"import numpy as np\nimport scipy.spatial\nfrom collections import Counter\n\n# loading the Iris-Flower dataset from Sklearn\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\niris = datasets.load_iris()\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 42, test_size = 0.2)\n\nclass KNN:\n    def __init__(self, k):\n        self.k = k\n\n    def fit(self, X, y):\n        self.X_train = X\n        self.y_train = y\n\n    def distance(self, X1, X2):\n        distance = scipy.spatial.distance.euclidean(X1, X2)\n\n    def predict(self, X_test):\n        final_output = []\n        for i in range(len(X_test)):\n            d = []\n            votes = []\n            for j in range(len(X_train)):\n                dist = scipy.spatial.distance.euclidean(X_train[j] , X_test[i])\n                d.append([dist, j])\n            d.sort()\n            d = d[0:self.k]\n            for d, j in d:\n                votes.append(y_train[j])\n            ans = Counter(votes).most_common(1)[0][0]\n            final_output.append(ans)\n\n        return final_output\n\n    def score(self, X_test, y_test):\n        predictions = self.predict(X_test)\n        return (predictions == y_test).sum() / len(y_test)\n\nclf = KNN(3)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)\nfor i in prediction:\n    print(i, end= ' ')\n\nprediction == y_test\n\nclf.score(X_test, y_test)\n\n\n# Result:\n# 1.0\n"
"X_train,X_test, y_train,y_test = train_test_split(data_df['Date'],data_df['Temperature'],test_size=0.2, random_state=0)\n"
'for model,i in zip([LinearRegression(),RandomForestRegressor()],[1,2]):\n\nfor i, model in enumerate((LinearRegression(),RandomForestRegressor()), start=1):\n'
'model.fit(x_train, y_train, batch_size = 64, epochs = 10)\n\nmodel.evaluate(x_test, y_test, batch_size = 128)\n'
'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\ndatasets = train_test_split(list_1, list_2)\nlist_1_train = datasets[0]\nlist_1_test = datasets[1]\nlist_2_train = datasets[2]\nlist_2_test = datasets[3]\n\n[list_1_train, list_1_test, list_2_train, list_2_test] = train_test_split(list_1, list_2)\n\nlist_1_train, list_1_test, list_2_train, list_2_test = train_test_split(list_1, list_2)\n'
'    with torch.no_grad():\n        for i in tqdm(range(len(test_X))):\n            real_class = torch.argmax(test_y[i])\n            net_out = net(test_X[i].view(-1, 1, 50, 50))[0]\n            predicted_class = torch.argmax(net_out)\n            if predicted_class == real_class:\n                correct += 1\n            total += 1\n\n     net_out = net(test_X[i].view(-1, 1, 50, 50))[0]\n\n     net_out = net(test_X[i].view(-1, 1, 50, 50).to(device)[0]\n'
'class LoadDataset(Dataset):\n    def __init__(self, data, label):\n        self.data = data\n        self.label = label\n    def __len__(self):\n        dlen = len(self.data)\n        llen = len(self.label)  # different here\n        return min(dlen, llen)  # different here\n    def __getitem__(self, index):\n        return self.data[index], self.label[index]  # different here\n'
"import pandas as pd\n\ngmm.fit(X_train)\ncluster_train = gmm.predict(X_train)\ncluster_test = gmm.predict(X_test)\n\nX_train['cluster_label'] = pd.Series(cluster_train, index=X_train.index)\nX_test['cluster_label'] = pd.Series(cluster_test, index=X_test.index)\n\nmodel_gmm_knn.fit(X_train, Y_train)\n"
'def call(self, x, training=False):\n    x = self.d1(x)\n    if training:\n        x = self.d2(x, training=training)\n    return self.d3(x)\n'
'from sklearn.model_selection import train_test_split\npercent=.1 specify the percentof data you want to use, in this case 10%\nX_data, X_dummy, y_labels, y_dummy=train_test_split(X,y,train_size=percent,randon_state=123, shuffle=True)\n\npercent=.1\nnew_data=pandas.data.sample(n=None, frac=percent, replace=False, weights=None, random_state=123, axis=0)\n'
"import datetime\n\ns = '10/29/2020 8:30'\ndate = datetime.datetime.strptime(s, &quot;%m/%d/%Y %H:%M&quot;)\ndate = date.timestamp()\n\nimport pandas as pd\n\ns = '10/29/2020 8:30'\ndf = pd.DataFrame({'Date':[s, s, s]})\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Date'] = df['Date'].astype('int64')\n\n                  Date\n0  1603960200000000000\n1  1603960200000000000\n2  1603960200000000000\n"
'from sklearn import svm\nX = [[1, 1, 0], [0, 0, 2]] # training data features\ny = [0, 1] # class labels\nclf = svm.SVC()\nclf.fit(X, y)  \n\nclf.predict([[0, 1, 0]])\n'
'sv = clf.support_vectors_ \n\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\ny = np.array([1, 1, 2, 2])\nfrom sklearn.svm import SVC\nclf = SVC()\nclf.fit(X, y) \n\nprint(clf.support_vectors_)\n'
'import numpy as np\nprint(np.mean(x_train))\n'
"from sklearn.metrics import f1_score\ny_true = [0, 1, 2, 0, 1, 2]\ny_pred = [0, 1, 1, 0, 0, 0]\nf1_score(y_true, y_pred, average='macro')\n\n# Output:  0.38888888888888884\n"
'model = joblib.load("model.sav")\n\nmodel.classes_\n'
"from nltk.corpus import stopwords\nw = stopwords.words('english')\n#lets say data is a string which has your sentence\nfor word in w:\n   if word in data:\n       data.replace(word,'')\n"
"import pandas as pd\ndf = pd.DataFrame({'vals': [[1,2,3,4], [2,3], [1,2,3], [2,3],\n                            [1,2,3], [1,2,3,4], [1], [2], [2,2], [2,1,3]]})\n\ndf.groupby(df.vals.apply(tuple)).groups\n#{(1,): Int64Index([6], dtype='int64'),\n# (1, 2, 3): Int64Index([2, 4], dtype='int64'),\n# (1, 2, 3, 4): Int64Index([0, 5], dtype='int64'),\n# (2,): Int64Index([7], dtype='int64'),\n# (2, 1, 3): Int64Index([9], dtype='int64'),\n# (2, 2): Int64Index([8], dtype='int64'),\n# (2, 3): Int64Index([1, 3], dtype='int64')}\n\ndf.reset_index().groupby(df.vals.apply(tuple))['index'].apply(list).sort_values().tolist()\n#[[0, 5], [1, 3], [2, 4], [6], [7], [8], [9]]\n"
'def workflow(EV_data, get_split, train_model, get_rmse,n_train = 250,n_test = 50,look_back = 1):\n    gX_train, gY_train, gX_test, gY_test, scaler, start_point = get_split(EV_data, n_train, n_test)\n\n    model = train_model(gX_train, gY_train, gX_test, gY_test)\n\n    RMSE, predictions = get_rmse(model, gX_test, gY_test, scaler, start_point, EV_data, n_train)\n\n    return RMSE, predictions\n'
'dist = np.sqrt(np.sum(np.power(float(testpoint)-float(trainpoint), 2)))\n'
"df.columns[      \n    (df == 1)        # mask \n    .any(axis=0)     # mask\n]\n\nimport pandas as pd\n\ndata = {'foo':[0,0,0,0], 'bar':[0, 1, 0, 0], 'baz':[0,0,0,0], 'spam':[0,1,0,1]}\ndf = pd.DataFrame(data, index=['a','b','c','d'])\n\nprint(df)\n\n   foo  bar  baz  spam\na    0    0    0     0\nb    0    1    0     1\nc    0    0    0     0\nd    0    0    0     1\n\n# group our df by index and creates a dict with lists of df's as values\ndf_dict = dict(\n    list(\n        df.groupby(df.index)\n    )\n)\n\nfor k, v in df_dict.items():               # k: name of index, v: is a df\n    check = v.columns[(v == 1).any()]\n    if len(check) &gt; 0:\n        print((k, check.to_list()))\n\n('b', ['bar', 'spam'])\n('d', ['spam'])\n"
'train_sizes, train_scores, test_scores, fit_times\n\ntrain_sizes, train_scores, test_scores, fit_times, _\n'
'import matplotlib.pyplot as plt\nfrom sklearn import *\nmodel = ensemble.RandomForestClassifier()\nmodel.fit(features, labels)\nmodel.feature_importances_\nimportances = np.sort(model.feature_importances_)[::-1]\ncumsum = np.cumsum(importances)\nplt.bar(range(len(importances)), importances)\nplt.plot(cumsum)\n'
'def get_parent(self):\n    rand = random.uniform(0, self.fitness_sum)\n    running_sum = 0\n    for dot in self.dots:\n        running_sum += dot.fitness\n        if running_sum &gt;= rand:\n            return dot\n    return self.dots[0]  # search failed, return 1st dot\n'
"X_reduced = PCA(n_components='mle').fit_transform(self.X)\n"
"X = tfidf.fit_transform([s] + list_s) # now X will have 3 rows\n\nfrom scipy.spatial.distance import cosine\ncosine(X[0].toarray(), X[1].toarray()) # cosine between s and 1st sentence\n\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ndef stem(text):\n    text_stem = [stemmer.stem(token) for token in text.split(' ')]\n    text_stem_join = ' '.join(text_stem)\n    return text_stem_join\n\nlist_s_stem = list(map(stem, list_s)) # map stem function to list of documents\n"
"import pandas as pd\ndf = pd.read_excel(File,sheetname=Sheet_Name)\n\nprint(df['column_name'])\n"
'import random\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import r2_score\nimport pandas as pd\nimport numpy as np\n\ninputs = [\'A\', \'B\', \'C\']\n\n# create some random data similar to yours\ndf = pd.DataFrame({\'input\': [random.choice(inputs) for _ in range(5000)], \'output\': [int(abs(n) * 100) for n in np.random.randn(5000)]})\n\n# one-hot-encode the categorical variable \'input\' for use in classification\ndummies = pd.get_dummies(df[\'input\'])\n\n# merge the one-hot-encoded dummies back with the original data\ndf = df.join(dummies)\n\n# our feature  matrix (input values as dummies)\nX = df[[\'A\', \'B\', \'C\']]\n\n# our outcome variable\ny = df[\'output\']\n\n# split the dataset into train and test objects so we can gauge the accuracy of our classifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80, random_state = 100)\n\n# our model instance\nmodel = LogisticRegression()\n\n# train the classifier\nmodel.fit(X_train, y_train)\n\n# use trained model from above to predict the class of "new" data\npredicted = model.predict(X_test)\n\n# let\'s see how well the classifier performed\nprint(r2_score(y_test, predicted))\n'
'from random import random\nimport math\n\ndef rand_cluster(n,c,r):\n    """returns n random points in disk of radius r centered at c"""\n    x,y = c\n    points = []\n    for i in range(n):\n        theta = 2*math.pi*random()\n        s = r*random()\n        points.append((x+s*math.cos(theta), y+s*math.sin(theta)))\n    return points\n\ndef rand_clusters(k,n,r, a,b,c,d):\n    """return k clusters of n points each in random disks of radius r\nwhere the centers of the disk are chosen randomly in [a,b]x[c,d]"""\n    clusters = []\n    for _ in range(k):\n        x = a + (b-a)*random()\n        y = c + (d-c)*random()\n        clusters.extend(rand_cluster(n,(x,y),r))\n    return clusters\n\nclusters = rand_clusters(4,50,0.3,0,1,0,1)\n'
"import numpy as np\nf=open('decision_tree_data.txt','r')\nx_train=[]\ny_train=[]\n\nfor line in f:\n    line=np.asarray(line.split(),dtype=np.float32)\n    x_train.append(line[:-1])\n    y_train.append(line[-1])\n\nx_train=np.asmatrix(x_train)\ny_train=np.reshape(y_train,(len(y_train),1))\n"
'a, b = y.reshape(y.shape[0]//2, 2).T\nprint(a)\n#array([0, 1, 2])\nprint(b)\n#array([0, 1, 2])\n'
'-&gt; Dataset\n    -&gt; natural_scenes\n    -&gt; artificial_images\n'
'    376         # avoid division by zero in normalization\n    377         std[std == 0] = 1.\n--&gt; 378         fac = 1. / (n_samples - n_classes)\n    379 \n    380         # 2) Within variance scaling\n\nZeroDivisionError: float division by zero\n\nimport numpy as np\n\nfrom sklearn.model_selection import cross_val_predict, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nX = np.array([[1, 2], [2, 4], [3, 2], [4, 4], [5, 2], [6, 4], [7, 2], [8, 4], [1, 2], [2, 4], [3, 2], [4, 4], [5, 2], [6, 4], [7, 2], [8, 4]])\nY = np.array([1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8])\n\nXtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y,train_size=0.5,random_state=1)\n\nclf = LinearDiscriminantAnalysis()\nclf.fit(X, Y)\n\n# without cross-valdidation\nprediction = clf.predict(Xtest)\n\n# with cross-valdidation\ncv = KFold(n_splits=2)\nprediction_cv = cross_val_predict(clf, X, Y, cv=cv)\n'
'Table 1 = Unprocessed Source Data\nTable 2 = Unique Names\nTable 3 = All derivations of Unique Names\n'
'for i in range(len(y_test)):\n    if result[i] == y_test[i]:\n        print("CORRECT: ", X_test[i])\n    else\n        print("INCORRECT: ", X_test[i])\n'
'def score(self, X, Y):\n    X_ = np.append(np.ones((X.shape[0],1)), X, axis = 1)\n    prediction = np.dot(X_, self.betas)\n    Y_mean = np.mean(Y)\n    ssr = np.sum((prediction - Y)**2)\n    ssto = np.sum((Y - Y_mean)**2)\n    return 1 - ssr / ssto\n\n/home/ely/anaconda/envs/py36-keras/lib/python3.6/site-packages/sklearn/utils/validation.py:444: \nDataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n'
'    #This will download the retrain script Which will retrain inception_v3\n    git clone --recursive https://github.com/daniel-kukiela/nmt-chatbot\n\n    cd nmt-chatbot\n\n    #This will install all the requirements like tensorFlow etc.\n    pip install -r requirements.txt\n\n    #This will start training your model based on your data set\n    python train.py\n'
'import sklearn\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn import svm\n\ndigits = datasets.load_digits()\n\nclf = svm.SVC(gamma=0.01,C= 100)\n\nx = digits.data\ny = digits.target\n\nclf.fit(x,y)\nprint ("prediction:",clf.predict(digits.data[-1].reshape(1, -1)))\nplt.imshow(digits.images[-1],cmap = plt.cm.gray_r, interpolation ="nearest")\nplt.show()\n'
'            test1   test2   test3   class\nperson1       3       0       1       A\nperson2       5       7       9       C\n\ndef dot(v1, v2):\n    return sum([x1*x2 for x1, x2 in zip(v1, v2)])\n\np1 = [3, 0, 1]\np2 = [5, 7, 9]\ncosine = dot(p1, p2) / ((dot(p1, p1) ** 0.5) * (dot(p2, p2) ** 0.5))\n'
'ap.add_argument("-s", "--source", required=True, help="Path to the source of shapes")\n'
"'''\n### N-Gram &amp; TD-IDF &amp; Cosine Similarity\nUsing n-gram on 'from column' with TF-IDF to predict the 'to column'.\nAdding to the df a 'cosine_similarity' feature with the numeric result.\n'''\ndef add_prediction_by_ngram_tfidf_cosine( from_column_name,ngram_range=(2,4) ):\n    global df\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.metrics.pairwise import cosine_similarity\n    vectorizer = TfidfVectorizer( analyzer='char',ngram_range=ngram_range )\n    vectorizer.fit(df.FromColumn)\n\n    w = from_column_name\n    vec_word = vectorizer.transform([w])\n\n    df['vec'] = df.FromColumn.apply(lambda x : vectorizer.transform([x]))\n    df['cosine_similarity'] = df.vec.apply(lambda x : cosine_similarity(x,vec_word)[0][0])\n\n    df = df.drop(['vec'],axis=1)\n"
'import ...\n\ndef get_dataset(path):\n    data = pd.read_csv(path)\n\n    data[\'Sex\'] = pd.factorize(data.Sex)[0]\n\n    filtered_titanic_data = data.dropna(axis=0)\n\n    return filtered_titanic_data\n\ntrain_path = "C:\\\\Users\\\\Omar\\\\Downloads\\\\Titanic Data\\\\train.csv"\ntest_path = "C:\\\\Users\\\\Omar\\\\Downloads\\\\Titanic Data\\\\test.csv"\n\ntrain_data = get_dataset(train_path)\ntest_data = get_dataset(test_path)\n\ncolumns_of_interest = [\'Pclass\', \'Sex\', \'Age\']\n\nx = train_data[columns_of_interest]\ny = train_data.Survived\n\ntrain_x, val_x, train_y, val_y = train_test_split(x, y, random_state=0)\n\ntitanic_model = DecisionTreeRegressor()\ntitanic_model.fit(train_x, train_y)\n\nval_predictions = titanic_model.predict(val_x)\n\nprint(val_predictions)\nprint(accuracy_score(val_y, val_predictions))\n\ntext_x = test_data[columns_of_interest]\ntest_predictions = titanic_model.predict(test_x)\n'
'data = {\n    "Alien Predator": [\'great\',\'17th\', \'abigail\', \'by\', \'century\', \'is\'],\n    "Shark Exorcist": [\'demonic\', \'devil\', \'great\', \'hell\', \'holy\', \'nun\'],\n    "Jurassic Shark": [\'abandoned\', \'an\', \'and\', \'beautiful\', \'abigail\',]\n}\n\nresult = {}\nfor movie_name, keywords in data.items():\n    for keyword in keywords:\n        result.setdefault(keyword, []).append(movie_name)\nprint(result)\n\n{\n\'great\': [\'Alien Predator\', \'Shark Exorcist\'], \n\'17th\': [\'Alien Predator\'], \n\'abigail\': [\'Alien Predator\', \'Jurassic Shark\'], \n\'by\': [\'Alien Predator\'], \n\'century\': [\'Alien Predator\'], \n\'is\': [\'Alien Predator\'], \n\'demonic\': [\'Shark Exorcist\'], \n\'devil\': [\'Shark Exorcist\'], \n\'hell\': [\'Shark Exorcist\'], \n\'holy\': [\'Shark Exorcist\'], \n\'nun\': [\'Shark Exorcist\'], \n\'abandoned\': [\'Jurassic Shark\'], \n\'an\': [\'Jurassic Shark\'],\n\'and\': [\'Jurassic Shark\'], \n\'beautiful\': [\'Jurassic Shark\']\n}\n'
'from sklearn.[family] import [Model]\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import SelectKBest\n'
"def create_model():\n    model = Sequential()\n    # Adding the input layer\n    model.add(Dense(26,activation='relu',input_shape=(n_cols,)))\n    # Adding the hidden layer\n    model.add(Dense(60,activation='relu'))\n    model.add(Dense(60,activation='relu'))\n    model.add(Dense(60,activation='relu'))\n    # Adding the output layer\n    model.add(Dense(1))  # change to 1 unit\n    # Compiling the RNN\n    model.compile(optimizer='adam', loss='mean_squared_error') # dismiss accuracy\n    return model\n\nkf = KFold(n_splits = 5, shuffle = True)\nscores = []\nfor i in range(5):\n    result = next(kf.split(data_input), None)\n    input_train = data_input[result[0]]\n    input_test = data_input[result[1]]\n    output_train = data_output[result[0]]\n    output_test = data_output[result[1]]\n    # Fitting the RNN to the Training set\n    model = create_model()  # move create_model here\n    model.fit(input_train, output_train, epochs=10000, batch_size=200 ,verbose=2)  # increase the epochs\n    predictions = model.predict(input_test) \n    scores.append(model.evaluate(input_test, output_test))\n\nprint('Loss from each Iteration: ', scores)\nprint('Average K-Fold Loss :' , np.mean(scores))\n"
'from gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\nmodel = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\nmodel.infer_vector([\'theis is a sentence1\', \'here is another sentence\', \'this represents the third sentence\']).tolist()\n\nfrom sklearn import svm\nclf = svm.SVC(gamma=0.001, C=100.0)\nd = pd.DataFrame({\'vectors\':[[1,2,3], [3,6,5], [9,2,4], [1,2,7]], "targets": [\'class1\', \'class1\', \'class2\', \'class2\']})\nd\n&gt;&gt;&gt;\n      vectors   targets\n0   [1, 2, 3]   class1\n1   [3, 6, 5]   class1\n2   [9, 2, 4]   class2\n3   [1, 2, 7]   class2\n\nclf.fit(X = d.vectors.values.tolist(), y =d.targets)\n\n&gt;&gt;&gt;\nSVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\ndecision_function_shape=\'ovr\', degree=3, gamma=0.001, kernel=\'rbf\',\nmax_iter=-1, probability=False, random_state=None, shrinking=True,\ntol=0.001, verbose=False)\n'
'print("Processed feature columns ({} total features):\\n{}".format(len(X_all.columns), list(X_all.columns)))\n'
'theta0 = 0\ntheta1 = 0\n\ndef hyp(x): return theta0 + theta1*x\n\ndef cost(hyp, x, y):\n    total1 = 0\n    total2 = 0\n\n    for i in range(1, len(x)):\n        total1 += hyp(x[i]) - y[i]\n        total2 += (hyp(x[i]) - y[i]) * x[i]\n\nreturn total1 / len(x), total2 / len(x)\n\nfor i in range(50):\n    s1, s2 = cost(hyp, x, y)\n    theta1 = theta1 - alpha * s2\n    theta0 = theta0 - alpha * s1\n'
'dataset[4:6, 4:7]\n\ndataset[4:, 4:][:2, :3]\n\nIn [11]: %timeit dataset[4:6, 4:7]                                              \n216 ns ± 0.896 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n\nIn [12]: %timeit dataset[4:, 4:][:2, :3]                                        \n419 ns ± 11.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n'
"df['age'] = [['87.5'] if x == ['+75'] else x for x in df.age]\n\ndf\n"
'from sklearn.metrics import confusion_matrix\ny_true = ...\ny_pred = ...\nconfusion_matrix( y_true, y_pred )\n'
'z = np.array([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1]).astype(float)\n# z.shape is (14,)\nz = np.expand_dims(z, axis=0)\n# z.shape is now (1, 14)\n\nbestmodel.predict(z)\n'
"X_train = data1.drop('result', axis = 1)\ny_train = data1.result\nX_test = data2.drop('result', axis = 1)\ny_test = data2.result\n\n# store these data transform objects\npf2 = PolynomialFeatures(degree=2, include_bias=False)\npf3 = PolynomialFeatures(degree=3, include_bias=False)\n\n# then apply the transform to the training set\nx2_ = pf2.fit_transform(X_train)\nx3_ = pf3.fit_transform(X_train)\n\nmodel2 = LinearRegression().fit(x2_, y_train)\nmodel3 = LinearRegression().fit(x3_, y_train)\n\nr_sq2 = model2.score(x2_, y_train)\nr_sq3 = model3.score(x3_, y_train)\n\ny_pred2 = model2.predict(x2_)\ny_pred3 = model3.predict(x3_)\n\n# now apply the fitted transform to the test set\nx2_test = pf2.transform(X_test)\nx3_test = pf3.transform(X_test)\n\n# apply trained model to transformed test data\ny2_test_pred = model2.predict(x2_test)\ny3_test_pred = model3.predict(x3_test)\n\n# compute the model accuracy for the test data\nr_sq2_test = model2.score(x2_test, y_test)\nr_sq3_test = model3.score(x3_test, y_test)\n"
'from sklearn import tree\n\nclf = tree.DecisionTreeClassifier()\n\n# [height, weight, shoe_size]\nX = [[181, 80, 44], [177, 70, 43], [160, 60, 38], [154, 54, 37], [166, 65, 40],\n     [190, 90, 47], [175, 64, 39],\n     [177, 70, 40], [159, 55, 37], [171, 75, 42], [181, 85, 43]]\n\nY = [\'male\', \'male\', \'female\', \'female\', \'male\', \'male\', \'female\', \'female\',\n     \'female\', \'male\', \'male\']\n\nclf = clf.fit(X, Y)\nprediction = clf.predict([[160, 60, 22]])\nprint(prediction)\n\nimport graphviz\ndot_data = tree.export_graphviz(clf, out_file=None)\ngraph = graphviz.Source(dot_data)\ngraph.render("gender")\n'
"X.shape\n\n(49,) \n\nX=data[['reck']]\ny=data[['price']]\n\nX.shape\n\n(49,1)\n"
"import torch\ndim = 2\nA = torch.rand(dim, dim, requires_grad=False)\nb = torch.rand(dim, 1,  requires_grad=False)\nx = torch.autograd.Variable(torch.rand(dim, 1), requires_grad=True)\nstop_loss = 1e-2\nstep_size = stop_loss / 3.0\nprint('Loss before: %s' % (torch.norm(torch.matmul(A, x) - b)))\nfor i in range(1000*1000):\n    Δ = torch.matmul(A, x) - b\n    L = torch.norm(Δ, p=2)\n    L.backward()\n    x.data -= step_size * x.grad.data # step\n    x.grad.data.zero_()\n    if i % 10000 == 0: print('Loss is %s at iteration %i' % (L, i))\n    if abs(L) &lt; stop_loss:\n        print('It took %s iterations to achieve %s loss.' % (i, step_size))\n        break\nprint('Loss after: %s' % (torch.norm(torch.matmul(A, x) - b)))\n"
"import pandas as pd\nfrom io import StringIO\n\ncsv = StringIO('''2001,1,,a,a\n        2001,2,,b,b\n        2001,3,,c,c\n        2005,1,,a,a\n        2005,1,,c,c''')\ndf = pd.read_csv(csv, header=None )\nprint(df)\n\n      0  1   2  3  4\n0  2001  1 NaN  a  a\n1  2001  2 NaN  b  b\n2  2001  3 NaN  c  c\n3  2005  1 NaN  a  a\n4  2005  1 NaN  c  c\n\ndf_new = df.dropna(how='all', axis=1)\n\ndf_new = df_new.T.drop_duplicates().T\ndf_new.columns = range(len(df_new.columns))\nprint(df_new)\n"
'import numpy as np\nmax_ = 3\nnrows = 1000\nncols = 25\n\nnp.random.seed(7)\n\nX = np.zeros((nrows,ncols))\n\ndata = np.random.randint(100, size=(nrows, ncols))\n\n# number of max non-zeros to be generated for each column\nvmax = np.random.randint(low=0, high=4, size=(nrows,))\n\nfor i in range(nrows):\n\n  if vmax[i]&gt;0:\n      #index for setting non-zeros\n      col = np.random.randint(low=0, high=ncols, size=(1,vmax[i]))\n\n      #set non-zeros elements\n      X[i][col] = data[i][col]\n\nprint(X)\n\n[[ 0. 68. 25. ...  0.  0.  0.]\n [ 0.  0.  0. ...  0.  0.  0.]\n [ 0.  0.  0. ...  0.  0.  0.]\n ...\n [ 0.  0.  0. ...  0.  0.  0.]\n [88.  0.  0. ...  0.  0.  0.]\n [ 0.  0.  0. ...  0.  0.  0.]]\n'
'from flask import Flask\nfrom flask_restful import Resource, Api\n\napp = Flask(__name__)\napi = Api(app)\n\nclass MyApi(Resource):\n    def get(self, date):\n        return {\'date\': \'if present\'}\n\napi.add_resource(MyApi, \'/\')\n\nif __name__ == \'__main__\':\n    app.run()\n\ncurl http://localhost:5000/ -d "data=base_64_image_content" -X PUT\n\nimport re\nimport json\nfrom google.protobuf.json_format import MessageToJson\nfrom google.cloud import vision\nfrom flask import Response\n\n\ndef detect_text(request):\n    """Responds to any HTTP request.\n    Args:\n        request (flask.Request): HTTP request object.\n    Returns:\n        The response text or any set of values that can be turned into a\n        Response object using\n        `make_response &lt;http://flask.pocoo.org/docs/0.12/api/#flask.Flask.make_response&gt;`.\n    """\n    client = vision.ImageAnnotatorClient()\n    image = vision.types.Image(content=request.data)\n    response = client.text_detection(image=image)\n    serialized = MessageToJson(response)\n    annotations = json.loads(serialized)\n\n    full_text = annotations[\'textAnnotations\'][0][\'description\']\n    annotations = json.dumps(annotations)\n\n    r = Response(response=annotations, status=200, mimetype="application/json")\n    return r\n\ndef post_image(path, URL):\n    headers = {\'content-type\': \'image/jpeg\'}\n    img = open(path, \'rb\').read()\n    response = requests.post(URL, data=img, headers=headers)\n    return response\n'
"df = pd.DataFrame({'Col1':list('abcdeafgbfhi')})\nsearch_str = 'b'\nidx_list = list(df[(df['Col1']==search_str)].index.values)\nprint(df[idx_list[0]:idx_list[1]])\n\n  Col1\n1    b\n2    c\n3    d\n4    e\n5    a\n6    f\n7    g\n"
'def calc_coefficients(X,Y):\n    X=np.mat(X)\n    Y = np.mat(Y)\n    return np.dot((np.dot(np.transpose(X),X))**(-1),np.transpose(np.dot(Y,X)))\n\ndef score_r2(y_pred,y_true):\n    ss_tot=np.power(y_true-y_true.mean(),2).sum()\n    ss_res = np.power(y_true -y_pred,2).sum()\n    return 1 -ss_res/ss_tot\n\n\nX = np.ones(shape=(506,11))\nX[:,1:] = data.values\n\nB=calc_coefficients(X,y)\n##### Coeffcients \nB[:]\nmatrix([[ 2.26053646e+01],\n        [-9.64973063e-02],\n        [ 5.28108077e-02],\n        [ 2.38029890e+00],\n        [ 3.94059598e+00],\n        [-1.05476566e+00],\n        [ 2.82595310e-01],\n        [-1.57226536e-02],\n        [-7.56519964e-01],\n        [ 1.02392192e-02],\n        [-5.70698610e-01]])\n\n#### Intercept \nB[0]\nmatrix([[22.60536463]])\n\ny_pred = np.dot(np.transpose(B),np.transpose(X))\n##### First 5 rows predicted\nnp.array(y_pred)[0][:5]\n\narray([30.42657776, 24.80818347, 30.69339701, 29.35761397, 28.6004966 ])\n\n##### First 5 rows Ground Truth\ny[:5]\n\narray([24. , 21.6, 34.7, 33.4, 36.2])\n\n### R^2 score\nscore_r2(y_pred,y)\n\n0.7278959820021539\n'
"def predict_price(dates,price):\n    date_index = np.where(date_format.columns == dates)[0][0]\n\n    x = np.zeros(len(date_index.columns))\n    if date_index &gt;= 0:\n        x[date_index] = 1\n\n    return prediction.predict([x])[0]\n\npredict_price('Feb 20, 2018', 1000)\n\ndef predict_price(dates,price):\n    date_index = np.where(date_format.columns == dates)[0][0]\n\n    x = np.zeros(len(date_format.columns)) # HERE IS THE CHANGE\n    if date_index &gt;= 0:\n        x[date_index] = 1\n\n    return prediction.predict([x])[0]\n\npredict_price('Feb 20, 2018', 1000)\n"
'model.add(keras.layers.Flatten(x_train))\n\nmodel.fit(x_train,y_train,epochs=3)\n\nmodel.add(keras.layers.Flatten())\n\n# wrong\nmodel.add(keras.layers.Dense(128,activation= keras.nn.relu))\n# right\nmodel.add(keras.layers.Dense(128,activation= keras.backend.relu))\n'
"import matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\n\nY=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1]\n\npredictions=[0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n       1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n       0, 0, 1, 0]\n\nprint('Precsion score: '+ str(precision_score(Y, predictions)))\nprint('Recall score: '+ str(recall_score(Y, predictions)))\nprint('F1 score: '+ str(f1_score(Y, predictions)))\nprint('ROC score: ' + str(roc_auc_score(Y, predictions)))\nprint('Confusion matrix: ')\nprint(confusion_matrix(Y, predictions))\n\nfpr, tpr, threshold = roc_curve(Y, predictions)\nroc_auc = auc(fpr, tpr)\n\nprint(fpr, tpr, threshold, roc_auc)\n\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic(ROC Curve)')\n\nPrecsion score: 0.9179487179487179\nRecall score: 0.895\nF1 score: 0.9063291139240507\nROC score: 0.9075\nConfusion matrix: \n[[184  16]\n [ 21 179]]\n[0.   0.08 1.  ] [0.    0.895 1.   ] [2 1 0] 0.9075\n"
'from keras.model import load_model\n# save model,assuming &quot;model&quot; is the name of the your instance,give a name as parameter\nmodel.save(&quot;name.h5&quot;)\n#load model\nmodel = load_model(&quot;name.h5&quot;)\n'
'def logreg(clf, xtrain, ytrain):\n    ...\n    return clf_hyper\ndef run(fold, model, vectorizer):\n    ...\n    #return clf_hyper and exec the func\n    return hyperparameter.logreg(clf,xtrain,ytrain) \n    \n'
"import pandas as pd\nfrom tabulate import tabulate\n\nfilepath     = &quot;SO.txt&quot;\n\ncolList = ['Name', 'Code', 'Bday', 'Address', 'Phone', 'Email', 'Info']\ndf_full = pd.DataFrame(columns = colList)\n                       \nwith open(filepath) as fp:\n    contents = fp.read()\n    #print(contents)\n    groups = [[line.split(&quot;#&quot;)[1].strip() for line in group.split(&quot;\\n&quot;) if line != &quot;&quot;] for group in contents.split(&quot;\\n\\n&quot;)]\n    #print(groups)\n    for groupInd, group in enumerate(groups):\n        df_temp  = pd.DataFrame(columns = colList, index = [groupInd])\n        #If first line of each group contains at least a number, then the above code returns True \n        if not(any(chr.isdigit() for chr in group[0])):\n            df_temp.Name    = group[0]\n            df_temp.Code    = group[1]\n            df_temp.Bday    = group[2]\n            \n            #####\n            #Concatenate a list of address and phone lines into one string\n            temp = ' '.join(group[3:-2]).split('Tp')\n            df_temp.Address = temp[0]\n            #Extract digit string means remove commas, dots, ...        \n            df_temp.Phone   = ''.join(filter(lambda i: i.isdigit(), temp[1]))\n            #####\n\n            df_temp.Email   = group[-2]\n            df_temp.Info    = group[-1]\n        \n            df_full = pd.concat([df_full, df_temp], axis=0)\n            \n    print(tabulate(df_full, headers='keys', tablefmt='psql'))  \n            \n\n+----+-----------+-------------------+----------------------+------------------------------------------------------------------------------+--------------+--------------------+-------------------+\n|    | Name      | Code              | Bday                 | Address                                                                      |        Phone | Email              | Info              |\n|----+-----------+-------------------+----------------------+------------------------------------------------------------------------------+--------------+--------------------+-------------------|\n|  0 | Jon Doe   | 27212000-C        | Calorina, 06/03 1993 | South Calorina Jaka Km 1 Num 009.006 Calorina. 11710,                        | 108437347343 | joe.st'a gmail.com | 20-09-2016 Akn    |\n|  2 | Jenny Doe | 5641141 2/E.15263 | Zimbabwe, 05/06/1993 | Mujair Street Iv No.185 Mujair, 15116.                                       |     04545454 | jenny@gmail.com    | 22-09-2016/T Info |\n|  3 | Igor Kart | 36412777/E,15264  | Kongo, 30/10/1994    | Kp. Pintu Air Kel. Pabuaran Kec.Boj onggede Kab.Bogor RT 04/09 Bogor, 16320. |    107262626 | igor.@gmail.com    | 22-09-2016T Info  |\n+----+-----------+-------------------+----------------------+------------------------------------------------------------------------------+--------------+--------------------+-------------------+\n"
"import pprint\nnp.random.seed(4)\ndf = pd.DataFrame(np.random.standard_normal((1000, 5)))\ndf.columns = list(&quot;ABCDE&quot;)\ndf_cor = df.corr(method='pearson')\n\n          A         B         C         D         E\n0  0.050562  0.499951 -0.995909  0.693599 -0.418302\n1 -1.584577 -0.647707  0.598575  0.332250 -1.147477\n2  0.618670 -0.087987  0.425072  0.332253 -1.156816\n3  0.350997 -0.606887  1.546979  0.723342  0.046136\n4 -0.982992  0.054433  0.159893 -1.208948  2.223360\n\n          A         B         C         D         E\nA  1.000000 -0.008658 -0.015977 -0.001219 -0.008043\nB -0.008658  1.000000  0.037419 -0.055335  0.057751\nC -0.015977  0.037419  1.000000  0.000049  0.057091\nD -0.001219 -0.055335  0.000049  1.000000 -0.017879\nE -0.008043  0.057751  0.057091 -0.017879  1.000000\n\n# Checking for correlations &gt; Absulute 0.05. Here i `0.05`, change it to `0.5` at your end.\n\ndf_cor[df_cor.abs() &gt; .05].dropna(axis=1, how='all').replace(1., np.nan).dropna(how='all', axis=1).dropna(how='all', axis=0).apply(lambda x:x.dropna().to_dict() ,axis=1).to_dict()\n\n{'B': {'D': -0.0553348494117175, 'E': 0.057751329924049855},\n 'C': {'E': 0.057091148280687266},\n 'D': {'B': -0.0553348494117175},\n 'E': {'B': 0.057751329924049855, 'C': 0.057091148280687266}}\n\ndf_cor[df_cor.abs() &gt; .05].replace(1, np.nan)\n\n    A         B         C         D         E\nA NaN       NaN       NaN       NaN       NaN\nB NaN       NaN       NaN -0.055335  0.057751\nC NaN       NaN       NaN       NaN  0.057091\nD NaN -0.055335       NaN       NaN       NaN\nE NaN  0.057751  0.057091       NaN       NaN\n\ndf_cor[df_cor.abs() &gt; .05].replace(1, np.nan).dropna(how='all', axis=1)\n\n          B         C         D         E\nA       NaN       NaN       NaN       NaN\nB       NaN       NaN -0.055335  0.057751\nC       NaN       NaN       NaN  0.057091\nD -0.055335       NaN       NaN       NaN\nE  0.057751  0.057091       NaN       NaN\n\n"
'!apt install swig cmake libopenmpi-dev zlib1g-dev\n!pip install stable-baselines[mpi]==2.8.0 box2d box2d-kengz\n'
'           |   Keyword1     Keyword2\n ------------------------------------\n Movie A   |  Superhero     Fight\n Movie B   |  Fight         Superhero\n\n          |   Superhero     Fight     StackOverflow    ...\n------------------------------------------------------------\nMovie A   |      1            1            0           ...\nMovie B   |      1            1            1           ...\n'
' data.replace([np.inf, -np.inf], 0, inplace=True)\n'
'clf.partial_fit(x1, y1)\n# get x2, y2\n# update accuracy if needed\nclf.partial_fit(x2, y2)\n'
'model.add(tf.keras.layers.Flatten(input_shape=(100,)))\n'
'lines = [line.split(\',\') for line in """\\\nevent,rack,role,dc\nnetwork,north,mobile,africa\nnetwork,east,mobile,asia\noom,south,desktop,europe\ncpu,east,web,northamerica\noom,north,mobile,europe\ncpu,south,web,northamerica\ncpu,west,web,northamerica\n""".splitlines()]\n\nfor line in lines:\n    print line\n\n[\'event\', \'rack\', \'role\', \'dc\']\n[\'network\', \'north\', \'mobile\', \'africa\']\n[\'network\', \'east\', \'mobile\', \'asia\']\n[\'oom\', \'south\', \'desktop\', \'europe\']\n[\'cpu\', \'east\', \'web\', \'northamerica\']\n[\'oom\', \'north\', \'mobile\', \'europe\']\n[\'cpu\', \'south\', \'web\', \'northamerica\']\n[\'cpu\', \'west\', \'web\', \'northamerica\']\n\ndef find_combinations(line):\n    combinations = []\n    for i in range(2**len(line)):\n        bits = bin(i)[2:].zfill(len(line))\n        if bits.count(\'1\') &lt; 2:  # skip numbers with less than two 1-bits\n            continue\n        combination = set()\n        for bit, word in zip(bits, line):\n            if bit == \'1\':\n                combination.add(word)\n        combinations.append(\'-\'.join(sorted(combination)))\n    return combinations\n\nfrom collections import defaultdict\ncounter = defaultdict(int)\nfor line in lines:\n    for c in find_combinations(line):\n        counter[c] += 1\n\nfor combination_freq in sorted(counter.items(), key=lambda item: item[1], reverse=True):\n    print combination_freq\n\n(\'cpu-northamerica\', 3)\n(\'northamerica-web\', 3)\n(\'cpu-northamerica-web\', 3)\n(\'cpu-web\', 3)\n(\'mobile-north\', 2)\n(\'mobile-network\', 2)\n(\'europe-oom\', 2)\n(\'east-network\', 1)\n(\'asia-east-mobile\', 1)\n(\'asia-east-network\', 1)\n(\'cpu-south-web\', 1)\n(\'east-northamerica-web\', 1)\n(\'europe-north\', 1)\n(\'cpu-east\', 1)\n...etc.\n'
"i = 0\nfor c in contours:\n    # get the bounding rect\n    x, y, w, h = cv2.boundingRect(c)\n    # to save the images\n    cv2.imwrite('img_{}.jpg'.format(i), image[y:y+h,x:x+w])\n    i += 1\n"
"from sklearn.datasets import make_regression\n\n# simulate a dataset with 500 factors, but only 5 out of them are truely \n# informative factors, all the rest 495 are noises. assume y is your response\n# variable 'Sales', and X are your possible factors\nX, y = make_regression(n_samples=1000, n_features=500, n_informative=5, noise=5)\n\nX.shape\nOut[273]: (1000, 500)\ny.shape\nOut[274]: (1000,)\n\nfrom sklearn.feature_selection import f_regression\n# regressing Sales on each of factor individually, get p-values\n_, p_values = f_regression(X, y)\n# select significant factors p &lt; 0.05\nmask = p_values &lt; 0.05\nX_informative = X[:, mask]\n\nX_informative.shape\nOut[286]: (1000, 38)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(n_estimators=100)\n# fit our model\ngbr.fit(X_informative, y)\n# generate predictions\ngbr_preds = gbr.predict(X_informative)\n\n# calculate erros and plot it\ngbr_error = y - gbr_preds\n\nfig, ax = plt.subplots()\nax.hist(y, label='y', alpha=0.5)\nax.hist(gbr_error, label='errors in predictions', alpha=0.4)\nax.legend(loc='best')\n"
'  tup=(ages_train[i],net_worths_train[i],value)\n\n  tup=(ages_train[i][0],net_worths_train[i][0],value[0])\n'
"    network = self._output_layer\n    prediction = lasagne.layers.get_output(network)\n    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)\n    loss = loss.mean()\n\n    params = lasagne.layers.get_all_params(network, trainable=True)\n    updates = lasagne.updates.sgd(loss, params, self.LEARN_RATE)\n    givens = {\n        states: current_states,\n        expected: agents_expected_reward,\n        real_rewards: rewards\n    }\n    train_fn = theano.function([input_var, target_var], loss,\n                                    updates=updates, on_unused_input='warn',\n                                    givens=givens,\n                                    allow_input_downcast='True')\n    train_fn(current_states, agents_expected_reward)\n"
'String document = "Header: blah blah \\n Header: blah blah"\n\nString[] sections = document.split("\\n");\nString[] headers = new String[sections.length];\nString[] bodies = new String[sections.length];;\n\nfor(int i = 0; i &lt; sections.length; i++){\n      headers[i] = sections[i].split(":")[0];\n      bodies[i] = sections[i].substring(headers[i].length() + 2);\n}\n'
'predictions2 = lm.predict(xtest2)\n\npredictions2 = lm2.predict(xtest2)\n'
'def euc(a,b):\n    return distance.euclidean\n\ndef euc(a,b):\n    return distance.euclidean(a, b)\n'
"X.shape(1 -1)\n\nX = pd.Series()\nX.shape(1 -1)\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-105-1bb3332dc7d5&gt; in &lt;module&gt;()\n      1 X = pd.Series()\n----&gt; 2 X.shape(1 -1)\n\nTypeError: 'tuple' object is not callable\n"
"X_test.shape\n# (360, 64)\n\npred.shape\n# (360,)\n\npred[0]\n# 8\n\ny_test[0]\n# 8\n\nplt.imshow(X_test[0].reshape(8,8), cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()\n"
'train_array.shape\n\ntrain_array.reshape(28, 28, 1, len(train_x))\n'
'p = Augmentor.Pipeline("C:\\\\Users\\\\Diganta\\\\Desktop\\\\Courses and Projects\\\\Projects\\\\Bennet")\np.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\np.zoom(probability=0.5, min_factor=1.1, max_factor=1.5)\np.sample(100)\n\np.sample(0)\n\np.process()\n'
'0 -&gt; 0 0 1\n1 -&gt; 0 1 0\n2 -&gt; 1 0 0\n'
'Cycle                   C_Start_Day  C_Start_Month  C_End_Day  C_End_Month  C_Num_Days\n10th June to 11th July  10           6              11         7            1\n20th June to 21st July  20           6              21         7            1\n17th June to 18th July  17           6              18         7            1\n'
"from keras.datasets import mnist\nfrom keras import models\nfrom keras import layers\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\ntrain_images = train_images.rashape((60000, 28*28))\ntrain_labels = train_images.astype('float32') / 255\ntest_images = test_images.rashape((10000, 28*28))\ntest_labels = test_images.astype('float32') / 255\n\nnetwork = models.Sequential()\nnetwork.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\nnetwork.add(layers.Dense(10, activation='softmax'))\nnetwork.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\nnetwork.fit(train_images, train_labels, epochs=5, batch_size=128)\n"
"data = pd.read_csv('filename.csv')\nx = data[:,:-1]\ny = data[:,-1]\nregressor = linear_model.LinearRegression()\nregressor.fit(x,y)\n"
'import numpy as np\nfrom sklearn.svm import SVR\n\n# Data: 200 instances of 5 features each\nX = randint(1, 100, size=(200, 5))\ny = randint(0, 2, size=200)\n\nreg = SVR()\nreg.fit(X, y)\n\ny_test = np.array([[0, 1, 2, 3, 4]])    # Input to .predict must be 2-dimensional\nreg.predict(y_test)\n'
'tf.reset_default_graph()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nc = a + b\nd = a\n\nwith tf.Session() as sess:\n    print(c.eval(feed_dict={a:1.0}))\n# Error because in order to evaluate c we must have the value for b.\n\nwith tf.Session() as sess:\n    print(d.eval(feed_dict={a:1.0}))\n# It works because d is not dependent on b.\n\noutputs = tf.identity(outputs[:,n_steps-1,:], name="prediction")\n\nX = graph.get_tensor_by_name(\'input:0\')\nprediction = graph.get_tensor_by_name(\'prediction:0\')\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())   \nsess.run(prediction, feed_dict={X: x_test})\n'
"import json\na= '[[1,11,1],[7,7,77],[5,6,7]]'\na = json.loads(a)\n\nimport pandas as pd\ndf = pd.DataFrame.from_records(a, columns=['col1','col2','col3'])\n\ndf['col2'] = pd.Categorical(df['col2'])\n"
'    import tensorflow as tf\n'
'keras.callbacks.callbacks.LearningRateScheduler(schedule, verbose=0)\n'
"def naive_bayes_eval(test_sms_file, f):\n    ...\n    f=open(test_sms_file, 'r')\n"
"import pandas as pd\nimport numpy as np\n\n# Inputs\nfeature_cols = list(range(1, 13))\nn_samples = 1000 \n\n# Data prep\ndf = pd.DataFrame({col: np.random.choice([0, 1], size=n_samples, p=[0.99, 0.01]) for col in feature_cols})\ndf['ID'] = '1234'\ndf['TimeStamp'] = pd.date_range(end='2019-12-04', freq='30s', periods=n_samples)\n\n# Compressed df\ncomp_df = df.loc[df.groupby('ID')[feature_cols].diff().sum(axis=1) != 0, :]\n\n# Results\nn_comp = len(df.index)-len(comp_df.index)\nprint('Data compressed by {} rows ({}%)'.format(str(n_comp), str(round(n_comp/len(df.index)*100, 2))))\n"
'model.predict(x) &gt; 0.5\n'
'import itertools\nimport pandas as pd\n\ndfs_in_list = [df1, df2, df3, df4, df5]\n\ncombinations = []\nfor length in range(2, len(dfs_in_list)):\n    combinations.extend(list(itertools.combinations(dfs_in_list, length)))\n\n\nfor c in combinations:\n    pd.concat(c, axis=1)\n'
"plt.scatter(k_true, k_pred)\nplt.plot(k_pred, predictor[0], '-k');\n"
"import numpy as np\nimport pandas as pd\ndf = pd.DataFrame({'id':np.arange(1,6),\n                   'lat':np.array([43,44,45,47,48]),\n                   'lon':np.array([16,5,12,13,17]),\n                   'values':[[171,172,142,169,178,180],[27,150,151,162,159,165],\n                             [151,153,152,37],[171.222,127,180,172.56],[np.nan]]\n                  })\n\n    id  lat     lon     values\n0   1   43  16  [171, 172, 142, 169, 178, 180]\n1   2   44  5   [27, 150, 151, 162, 159, 165]\n2   3   45  12  [151, 153, 152, 37]\n3   4   47  13  [171.222, 127, 180, 172.56]\n4   5   48  17  [nan]\n\ndef func(x):\n    x = np.array(x)\n    x_mean = np.mean(x)\n    x_sd = np.std(x)\n    return(x[abs(x - x_mean)&gt;2*x_sd])\n\nnewdf =df.copy()\nnewdf['outlier_values'] = newdf['values'].apply(func)\nnewdf\n\nid  lat     lon     values  outlier_values\n0   1   43  16  [171, 172, 142, 169, 178, 180]  [142]\n1   2   44  5   [27, 150, 151, 162, 159, 165]   [27]\n2   3   45  12  [151, 153, 152, 37]     []\n3   4   47  13  [171.222, 127, 180, 172.56]     []\n4   5   48  17  [nan]   []\n"
"df = df.append(pd.concat([df.iloc[[0]]]*200))\ndf = df.append(pd.concat([df.iloc[[1]]]*2000))\ndf = df.append(pd.concat([df.iloc[[2]]]*100))\ndf = df.append(pd.concat([df.iloc[[3]]]*300))\n\ndf.Class.value_counts()\n\nOrange        2001\nWatermelon     301\nApple          201\nBanana         101\nName: Class, dtype: int64\n\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import OneHotEncoder\nfrom imblearn.pipeline import Pipeline\n\nX = df.drop(['Class'],1)\ny = df.Class\n\nsteps = [('onehot', OneHotEncoder()), ('smt', SMOTE())]\npipeline = Pipeline(steps=steps)\n\nX, y = pipeline.fit_resample(X, y)\n\npd.Series(y.to_numpy()).value_counts()\n\nBanana        2001\nOrange        2001\nWatermelon    2001\nApple         2001\ndtype: int64\n\nsmote_samples = {'Apple':1000, 'Orange':2001, 'Banana':1000, 'Watermelon':1000}\n\nsteps = [('onehot', OneHotEncoder()), ('smt', SMOTE(smote_samples))]\npipeline = Pipeline(steps=steps)\n\nX, y = pipeline.fit_resample(X, y)\npd.Series(y.to_numpy()).value_counts()\n\nOrange        2001\nBanana        1000\nApple         1000\nWatermelon    1000\ndtype: int64\n"
"TypeError: '&lt;' not supported between instances of 'str' and 'float'\n\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\n# Making a list of missing value types\nmissing_values = [&quot;?&quot;]\ndf = pd.read_csv('D:\\\\data.csv', na_values=missing_values)\n\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, 24]\n\nX.iloc[:, 4] = X.iloc[:, 4].fillna('NaN') # &lt;-- add this line\n\nX.iloc[:, 4] = LabelEncoder().fit_transform(X.iloc[:, 4])\n"
'np.array([[2.1, -1.9, 5.5],\n          [-1.5, 2.4, 3.5],\n          [0.5, -7.9, 5.6],\n          [5.9, 2.3, -5.8]])\n'
'MSE Loss = sum((h - y) ** 2) / 2m\n\nGradient wrt b1 will be sum[(h - y) . x)] / m:\n\nhypothesis: h = b0 + b1.x\n\nfor b0 = 0, b1 = 1:\n\nh = x\n\ninput(x)        : [  1,  1,  2]\n\nprediction(h)   : [  1,  1,  2]\n\nGround truth(y) : [ 22,  3,  3]\n\nh - y           : [-21, -2, -1]\n\n(h - y). x      : [-21, -2, -2]\n\ngradient(b1)    : (-21 - 2 - 2) / 3 = -25 / 3 = -8.3333\n\n \n\n \n'
'def predict(self, X_test):\n        predictions = []\n        for row in X_test:\n            label = self.closest(row)\n            predictions.append(label)\n        return predictions\n\ndef predict(self, X_test):\n        predictions = []\n        for row in X_test.iterrows():\n            label = self.closest(list(row[1]))\n            predictions.append(label)\n        return predictions\n\ndef closest(self, row):\n        best_dist = euc(row, self.X_train[0])\n        best_index = 0\n        for i in range(1, len(self.X_train)):\n            dist = euc(row, self.X_train[i])\n            if dist &lt; best_dist:\n                best_dist = dist\n                best_index = i\n        return self.Y_train[best_index]\n\ndef closest(self, row):\n    best_dist = euc(row, self.X_train.iloc[0])\n    best_index = 0\n    for i in range(1, len(self.X_train.index)):\n        dist = euc(row, list(self.X_train.iloc[i]))\n        if dist &lt; best_dist:\n            best_dist = dist\n            best_index = i\n    return self.Y_train.iloc[best_index]\n'
'G -&gt; [1.0, 0.2, 0.1, 0.2]\nA -&gt; [0.2, 0.5, 0.7, 0.1]\nT -&gt; [0.1, 0.2, 1.0, 0.5]\nC -&gt; [0.4, 0.4, 0.5, 0.8]\n'
"try:\n    label = label_img(img)\nexcept IndexError:\n    print(img)\n    continue\n\nif len(img.split('.')) &lt; 2:\n    continue\nlable = label_img(img)\n"
'class DQN:\n    def __init__(self, env):\n        pass\n\n    def get_model(self):\n        model = Sequential()\n\n        model.add(layers.GRU(self.input_units, input_shape=self.shape, return_sequences=True))\n        for layer in range(self.hidden_layers):\n            model.add(layers.GRU(self.hidden_units, return_sequences=True))\n        model.add(layers.GRU(self.ouput_units, return_sequences=True))\n        print(model.summary())\n\ntarget = DQN()\ntarget.get_model()\n'
'emp_length&lt;-c("account","accountant","accounting","account specialist","Data Scientist","Data Science Expert")\n\ncluster&lt;-kmeans(stringdistmatrix(emp_length,emp_length,method="jw"),centers=2)\ncluster_n&lt;-cluster$cluster\n\ncbind(emp_length,cluster_n)\n     emp_length            cluster_n\n[1,] "account"             "2"      \n[2,] "accountant"          "2"      \n[3,] "accounting"          "2"      \n[4,] "account specialist"  "2"      \n[5,] "Data Scientist"      "1"      \n[6,] "Data Science Expert" "1" \n'
"data = [[timestamp_1, 0],\n        [timestamp_2, 1],\n        [timestamp_3, 2],\n        [timestamp_4, 3],\n        #continues...\n         ]]\n\nlist_time = []\nlist_numbers = []\n\nlist_time = [i[0] for i in data]\nlist_numbers = [i[1] for i in data]\n\nlist_numbers = np.reshape(list_number, (len(list_numbers), 1))\n\nsvr_lin = SVR(kernel='linear', C=1e3)\n\nsvr_lin.fit(list_numbers, list_time)\n\nplt.scatter(list_numbers, list_time)\nplt.plot(list_numbers, svr_lin.predict(list_numbers)) #in a different colour\nplt.show\n\n'''x is out of sample and starting point I have kept in sample however, starting \npoint does not need to be the 0th element.'''\n\nfor i in range(0,x):\n    print(svr_lin.predict(i))\n"
'import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom collections import defaultdict\n\ndata = {\n    "State" : ["California", "New York", "Alaska", "Arizona", "Alaska", "Arizona"],\n    "Month" : ["January", "February", "May", "February", "January", "February" ],\n    "Number" : ["1000", "750", "500", "25000", "2000", "1"]\n}\ndf = pd.DataFrame(data)\n\nd = defaultdict(MultiLabelBinarizer)  # dict of Features =&gt; model\n\nlist_encoded = []  # store single matrices\nfor column in df:\n    d[column].fit(df[column])\n    list_encoded.append(d[column].transform(df[column]))\nmerged = np.hstack(list_encoded) # matrix of 6 x 32\n'
"In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame({'Name':['A','B','C','D','E'],\n                      'sal_amt':[4500,50000,2000,3000,5000],\n                      'sal_md':['M','Y','W','B','M']})\n\nIn [3]: multiplier_dict = {'M':12, 'Y':1, 'W':52, 'B':26}\n\nIn [4]: df['sal_multiplier'] = df.sal_md.map(multiplier_dict)\n\nIn [5]: df['sal_annual'] = df.sal_amt*df.sal_multiplier\n\nIn [6]: df.head()\nOut[6]:\n  Name  sal_amt sal_md  sal_multiplier  sal_annual\n0    A     4500      M              12       54000\n1    B    50000      Y               1       50000\n2    C     2000      W              52      104000\n3    D     3000      B              26       78000\n4    E     5000      M              12       60000\n"
'features, _ = fo.feature_extraction(0)\n...\nnot enough values to unpack (expected 2, got 0)\n'
"# save the model to disk\n\n\nfilename = 'model.sav'\n\npickle.dump(model, open(filename, 'wb'))\n\n# load the model from disk\n\nloaded_model = pickle.load(open(filename, 'rb'))\n\nresult = loaded_model.predict(new_point)\n\n\n[[x11,x12,x13],\n\n[x21,x22,x23]]\n"
"import numpy as np\nfrom src.io.psee_loader import PSEELoader\n\n# open a file\nvideo = PSEELoader(&quot;some_file_td.dat&quot;)\nprint(video)  # show some metadata\nvideo.event_count()  # number of events in the file\nvideo.total_time()  # duration of the file in mus\n\n# let's read some Events , there are two ways by number of events or by time slices\nevents = video.load_n_events(10)  # this loads the 10 next events\nevents\n\n# let's randomly drop some events\nnp.random.choice(events, len(events)//2)\n\n# let's now try to read 10ms worth of events\nevents = video.load_delta_t(10000)\n"
"from sklearn.linear_model import LinearRegression\n\nX = [[1,2,3], [4, 5, 6], [7,8,9]]\ny = [30, 20, 10]\n\nlr = LinearRegression().fit(X, y)\n\npred = lr.predict([[13,14,15], [16,17,18]])\n\nprint(pred)\n\nprint(type(pred))\n\n[-10. -20.]\n\n&lt;class 'numpy.ndarray'&gt;\n"
'    import numpy as np\n    def my_svm(dataset, label):\n        rate = 1 # rate for gradient descent\n        epochs = 10000 # no of iterations\n        weights = np.zeros(dataset.shape[1]) # Create an array for storing the weights\n\n        # Min. the objective function(Hinge loss) by gradient descent\n        for epoch in range(1,epochs):\n            for n, data in enumerate(dataset):\n                if (label[n] * np.dot(dataset[n], weights)) &lt; 1:\n                    weights = weights + rate * ( (dataset[n] * label[n]) + (-2  *(1/epoch)* weights) )\n                else:\n                    weights = weights + rate * (-2  * (1/epoch) * weights)\n\n        return weights\n\n    def predict(test_data,weights):\n        results = []\n        for data in test_data:\n            result = np.dot(data,weights)\n            results.append(-1 if result &lt; 0 else 1)\n        return results\n\n    dataset = np.array([\n        [-2, 4,-1], #x_cood,y_cood,bias\n        [4, 1, -1],\n        [0, 2, -1],\n        [1, 6, -1],\n        [2, 5, -1],\n        [6, 2, -1]\n        ])\n    label = np.array([-1,-1,-1,1,1,1])\n\n    weights = my_svm(dataset,label)\n\n    test_data = np.array([\n                [0,3,-1], #Should belong to -1\n                [4,5,-1]  #Should belong to 1\n                ])\n    predict(test_data, weights)\n    &gt;Out[10]: [-1, 1]\n'
"correct = [['*','*'],['*','PER','*','GPE','ORG'],['GPE','*','*','*','ORG']]\npredicted = [['PER','*'],['*','ORG','*','GPE','ORG'],['PER','*','*','*','MISC']]\ntarget_names = ['PER','ORG','MISC','LOC','GPE'] # leave out '*'\n\ncorrect_flat = [item for sublist in correct for item in sublist]\npredicted_flat = [item for sublist in predicted for item in sublist]\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(correct_flat, predicted_flat, target_names=target_names))\n\n             precision    recall  f1-score   support\n\n        PER       1.00      0.86      0.92         7\n        ORG       1.00      0.50      0.67         2\n       MISC       0.00      0.00      0.00         0\n        LOC       0.50      0.50      0.50         2\n        GPE       0.00      0.00      0.00         1\n\navg / total       0.83      0.67      0.73        12\n\nUndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n"
"# open the input file for reading and create a new output file for writing\nreadfile = open('input.txt', 'r')\nwritefile = open('newfile.txt', 'w')\n\n# read each line in the input file\nfor line in readfile:\n\n    # remove the trailing carriage return\n    line = line.strip()\n\n    # write the prefix symbol, the input line, the postfix symbol,\n    # and a carriage return to the output file\n    writefile.write('(s)' + line + '(/s)' + '\\n')\n"
"from tensorflow.keras.layers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nimport numpy as np\n\n# put your data right here:\nnum_classes = 3 # here I'm assuming 3 classes, e.g. dog, cat and bird\nx_train = np.zeros((100, 128, 128, 3)) # I'm just using zeros to simplify the example\ny_train = np.zeros((100, num_classes))\n\nmodel = Sequential()\n# put your conv layer / conv blocks here:\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(128, 128, 3)))\nmodel.add(Flatten())\nmodel.add(Dense(units=num_classes, activation='sigmoid'))\n    \nmodel.compile(\n        loss=&quot;binary_crossentropy&quot;,\n        optimizer=Adam(0.005),\n        metrics=[&quot;accuracy&quot;])\ntraining_history = model.fit(x=x_train, y=y_train, epochs=5)\n"
'import numpy as np\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer(min_df=1)\n\ntfidf = vect.fit_transform(["My name is Ankit",\n                             "Ankit name is very famous",\n                             "Ankit like his name",\n                             "India has a lot of beautiful cities"])\n\nprint ((tfidf * tfidf.T).A)\n'
"df['age'].apply(lambda x: np.mean([int(x.split('-')[0]), int(x.split('-')[1])]) if '+' not in x else x[:-1])\n\nmapping = {'1-17': 9, '18-25': 21.5, '55+': 55}\ndf['age'].apply(lambda x: mapping[x])\n"
'f = open(\'sample_text.txt\', \'r\')\ndata = f.read()\nparagraphs = data.split("\\n\\n")\nparagraphs[:] = (value for value in paragraphs if value != \'\\t\')\n'
